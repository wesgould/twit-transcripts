;FFMETADATA1
title=How Many Cups in a Stone?
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=668
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.200]   It's time for "Twit". What a show we have today.
[00:00:03.200 --> 00:00:05.940]   Canada for Congress in California, Brian Ford.
[00:00:05.940 --> 00:00:09.180]   He's a former advisor to President Obama on technology.
[00:00:09.180 --> 00:00:13.440]   Matt Kots, who's the current acting administrator at the US Digital Service.
[00:00:13.440 --> 00:00:15.440]   And we're just going to throw in a Brit in the middle there.
[00:00:15.440 --> 00:00:18.900]   Ian Thompson from the register.co.uk lots to talk about.
[00:00:18.900 --> 00:00:22.580]   Including the FBI's recommendation that you reboot your router.
[00:00:22.580 --> 00:00:23.700]   Now!
[00:00:23.700 --> 00:00:27.900]   Facebook's shadow profiles, Amazon's face recognition.
[00:00:27.900 --> 00:00:30.500]   And unintended consequences of the GDPR.
[00:00:30.500 --> 00:00:31.800]   It's all coming up next.
[00:00:31.800 --> 00:00:32.800]   Untwit.
[00:00:32.800 --> 00:00:37.800]   Netcasts you love.
[00:00:37.800 --> 00:00:39.800]   From people you trust.
[00:00:39.800 --> 00:00:44.900]   This is "Twit".
[00:00:44.900 --> 00:00:51.900]   Bandwidth for this weekend tech is provided by CashFly at C-A-C-H-E-F-L-Y.com.
[00:00:56.900 --> 00:01:04.400]   This is "Twit". This weekend tech, Episode 668, recorded Sunday, May 27th, 2018.
[00:01:04.400 --> 00:01:06.900]   How many cups in a stone?
[00:01:06.900 --> 00:01:09.200]   This weekend tech is brought to you by
[00:01:09.200 --> 00:01:12.400]   Betterment, the largest independent online financial advisor.
[00:01:12.400 --> 00:01:15.400]   Sign up today at betterment.com/twit.
[00:01:15.400 --> 00:01:18.400]   And get up to one year managed free.
[00:01:18.400 --> 00:01:24.400]   And by "Quip", the first subscription electric toothbrush accepted by the American Dental Association.
[00:01:24.400 --> 00:01:31.400]   Visit getquip.com/twit and get your first refill pack free when you purchase any "Quip" electric toothbrush.
[00:01:31.400 --> 00:01:33.400]   And by "stamps.com".
[00:01:33.400 --> 00:01:37.400]   Buy and print real U.S. postage, the instant you need it, right from your desk.
[00:01:37.400 --> 00:01:42.400]   To get our special offer go to "stamps.com", click on the microphone, and enter "Twit".
[00:01:42.400 --> 00:01:44.400]   And by " Molecule".
[00:01:44.400 --> 00:01:48.400]   Molecule is the world's first molecular air purifier.
[00:01:48.400 --> 00:01:51.400]   It reduces symptoms for allergy and asthma sufferers.
[00:01:51.400 --> 00:01:58.400]   For more information on how to use your first order visit Molecule.com and enter the promo code "Twit".
[00:01:58.400 --> 00:02:01.400]   It's time for "Twit" this weekend tech, our Memorial Day episode.
[00:02:01.400 --> 00:02:04.400]   And man, this is actually a very distinguished panel here.
[00:02:04.400 --> 00:02:09.400]   I've got to kind of sit back and let the grown-ups run the show.
[00:02:09.400 --> 00:02:15.400]   First of all, I want to really, I'm thrilled to welcome Matt Cutts back to the studio for the second time in his entire life.
[00:02:15.400 --> 00:02:17.400]   It's great to see you Matt.
[00:02:17.400 --> 00:02:21.400]   Acting Administrator at the United States Digital Service still.
[00:02:21.400 --> 00:02:23.400]   And what are you doing out here?
[00:02:23.400 --> 00:02:27.400]   So there's a lovely thing called the Code for America Summit.
[00:02:27.400 --> 00:02:32.400]   I'll be out doing a little bit of public speaking and a lot of recruiting for software engineers, designers, product managers.
[00:02:32.400 --> 00:02:36.400]   And so, as long as I'm out in California, I would love to see Leo.
[00:02:36.400 --> 00:02:39.400]   Well, yay! We're thrilled.
[00:02:39.400 --> 00:02:43.400]   The Code for America Summit goes May 30 through June 1st.
[00:02:43.400 --> 00:02:46.400]   And it's in Oakland. And that's cool.
[00:02:46.400 --> 00:02:54.400]   This is actually, kind of shares the mission with USDS, Code for America is about getting coders to do things to help make America great again.
[00:02:54.400 --> 00:02:57.400]   That's right. The Digital Service is more of the expression.
[00:02:57.400 --> 00:02:58.400]   No.
[00:02:58.400 --> 00:03:02.400]   We tackle the federal level and Code for America is often city and state level.
[00:03:02.400 --> 00:03:04.400]   Awesome. Awesome.
[00:03:04.400 --> 00:03:05.400]   Great.
[00:03:05.400 --> 00:03:08.400]   Also here, want to welcome our friend Ian Thompson.
[00:03:08.400 --> 00:03:11.400]   You can help me with his esteemed panel.
[00:03:11.400 --> 00:03:18.400]   I could say I was looking at the list of people who are on there and say, "Okay, you need to do your homework before you go on the show because it was serious stuff."
[00:03:18.400 --> 00:03:20.400]   If you get this one wrong, that's going to count.
[00:03:20.400 --> 00:03:23.400]   Ian is at the register where he is now.
[00:03:23.400 --> 00:03:24.400]   What is this?
[00:03:24.400 --> 00:03:25.400]   Yeah, news editor.
[00:03:25.400 --> 00:03:26.400]   News editor.
[00:03:26.400 --> 00:03:28.400]   Yeah, big deal. Big deal.
[00:03:28.400 --> 00:03:32.400]   We're going to welcome a brand new member to the panel.
[00:03:32.400 --> 00:03:40.400]   And it continues our policy of bringing great congressional candidates on Twitch to help get some smarts into Congress.
[00:03:40.400 --> 00:03:43.400]   Brian Ford, it's great to have you welcome.
[00:03:43.400 --> 00:03:46.400]   Well, it's Austin Beere. We're more technologists in Congress.
[00:03:46.400 --> 00:03:47.400]   Yes, sir.
[00:03:47.400 --> 00:03:48.400]   This is a good place to talk about.
[00:03:48.400 --> 00:03:54.400]   Even though Brian doesn't know Matt's, he knew about Matt's because he worked in the Office of Science and Technology Policy during Obama's presidency.
[00:03:54.400 --> 00:03:58.400]   And in fact, I think USDS was kind of one of the initiatives you guys.
[00:03:58.400 --> 00:03:59.400]   Yeah.
[00:03:59.400 --> 00:04:05.400]   I mean, to just show you how far we came, when I joined Team CTO, there was about six of us.
[00:04:05.400 --> 00:04:08.400]   And I was only one code and I'm a crap coder.
[00:04:08.400 --> 00:04:13.400]   And then we get to a place where ultimately we have Matt cuts leading us.
[00:04:13.400 --> 00:04:14.400]   That's awesome.
[00:04:14.400 --> 00:04:20.400]   Yeah, I'm also a crap coder, but we have a hundred and seventy five other people who are very good coders and designers.
[00:04:20.400 --> 00:04:25.400]   Matt, what Matt, an early employee at Google took a leave of absence and now is full time at the USDS.
[00:04:25.400 --> 00:04:27.400]   It's really great to have you.
[00:04:27.400 --> 00:04:31.400]   Brian is running for Congress in the California 45th district.
[00:04:31.400 --> 00:04:36.400]   The incumbent there, Mimi Walters is probably running on a pose in the primary.
[00:04:36.400 --> 00:04:38.400]   I would guess.
[00:04:38.400 --> 00:04:40.400]   We're an open primary in California.
[00:04:40.400 --> 00:04:41.400]   Oh, that's right.
[00:04:41.400 --> 00:04:42.400]   So you're all running together.
[00:04:42.400 --> 00:04:44.400]   Yeah, against each other.
[00:04:44.400 --> 00:04:45.400]   Against each other.
[00:04:45.400 --> 00:04:47.400]   The primaries June, I want to say fourth.
[00:04:47.400 --> 00:04:48.400]   June 5th.
[00:04:48.400 --> 00:04:49.400]   5th.
[00:04:49.400 --> 00:04:50.400]   Oh, yep.
[00:04:50.400 --> 00:04:51.400]   I got my ballot.
[00:04:51.400 --> 00:04:52.400]   I vote by mail.
[00:04:52.400 --> 00:04:53.400]   That's why I don't know the exact date.
[00:04:53.400 --> 00:05:00.400]   And so you're running and in theory, both Democrats could end up getting the top vote getters.
[00:05:00.400 --> 00:05:02.400]   It's just kind of a weird thing in California.
[00:05:02.400 --> 00:05:03.400]   Unknown.
[00:05:03.400 --> 00:05:04.400]   Yeah.
[00:05:04.400 --> 00:05:06.400]   So, I think that's great.
[00:05:06.400 --> 00:05:12.400]   Brian's got a great background, not only with the Office of Science and Technology Policy,
[00:05:12.400 --> 00:05:16.400]   but also led MIT's Digital Currency Initiative.
[00:05:16.400 --> 00:05:22.400]   So blockchain and a Bitcoin, probably not Bitcoin expert, cryptocurrency expert would be a better way to put it.
[00:05:22.400 --> 00:05:23.400]   Used to be just Bitcoin.
[00:05:23.400 --> 00:05:26.400]   I mean, that's just a tiny bit of it now.
[00:05:26.400 --> 00:05:30.400]   It's just nice to have someone in Congress writing these laws who actually understands what a computer is.
[00:05:30.400 --> 00:05:31.400]   Wouldn't that be mine, Bob?
[00:05:31.400 --> 00:05:32.400]   I know.
[00:05:32.400 --> 00:05:33.400]   I know.
[00:05:33.400 --> 00:05:34.400]   I mean, I imagine.
[00:05:34.400 --> 00:05:35.400]   There's so few compared to the lawyer count.
[00:05:35.400 --> 00:05:36.400]   It's ridiculous.
[00:05:36.400 --> 00:05:39.400]   And Brian, to your point, more than 40% are lawyers.
[00:05:39.400 --> 00:05:43.400]   Less than 4% have a technical background like me.
[00:05:43.400 --> 00:05:44.400]   And we accept Bitcoin.
[00:05:44.400 --> 00:05:46.400]   You know, we live our values.
[00:05:46.400 --> 00:05:47.400]   We accept Ethereum.
[00:05:47.400 --> 00:05:52.400]   And now we have members at Congress who accept Bitcoin because they've seen us lead the way.
[00:05:52.400 --> 00:05:55.400]   In fact, 20% of our contributions are in crypto.
[00:05:55.400 --> 00:05:56.400]   Wow.
[00:05:56.400 --> 00:05:57.400]   Yeah.
[00:05:57.400 --> 00:05:59.400]   If I could unlock my wallet, I have seven Bitcoin.
[00:05:59.400 --> 00:06:01.400]   I'd be glad to pass along.
[00:06:01.400 --> 00:06:02.400]   I just don't know the password anymore.
[00:06:02.400 --> 00:06:07.400]   I just don't know if you go to crypto.ford@4GE.com.
[00:06:07.400 --> 00:06:10.400]   You know, there are attack ads against us for accepting Bitcoin.
[00:06:10.400 --> 00:06:11.400]   We flipped it on them.
[00:06:11.400 --> 00:06:12.400]   This is amazing.
[00:06:12.400 --> 00:06:18.400]   You also have the distinction being the first candidate for Congress to be attacked over cryptocurrency.
[00:06:18.400 --> 00:06:19.400]   That's right.
[00:06:19.400 --> 00:06:20.400]   Okay.
[00:06:20.400 --> 00:06:21.400]   Wow.
[00:06:21.400 --> 00:06:22.400]   Yeah.
[00:06:22.400 --> 00:06:23.400]   But this is literally the issue.
[00:06:23.400 --> 00:06:24.400]   I am the technologist.
[00:06:24.400 --> 00:06:27.400]   I'm running against three lawyers.
[00:06:27.400 --> 00:06:32.080]   And I think we need diversity in Congress for gender, for ethnicity, for religion.
[00:06:32.080 --> 00:06:33.880]   But we all seem to look at experience.
[00:06:33.880 --> 00:06:35.120]   We don't need another lawyer in Congress.
[00:06:35.120 --> 00:06:36.960]   So hey, I'm married to a lawyer.
[00:06:36.960 --> 00:06:38.520]   So I literally love a lawyer.
[00:06:38.520 --> 00:06:39.520]   Let's be clear.
[00:06:39.520 --> 00:06:40.520]   Make sure your friends are lawyers.
[00:06:40.520 --> 00:06:41.520]   Yeah.
[00:06:41.520 --> 00:06:45.760]   But at the same time, if you don't understand Bitcoin and if you're going to think that
[00:06:45.760 --> 00:06:51.440]   anyone who uses Bitcoin or Ethereum is a drug dealer or a human trafficker, then you
[00:06:51.440 --> 00:06:55.480]   should probably reconsider your capacity to lead in the 21st century.
[00:06:55.480 --> 00:06:59.840]   Well, and the thing that bugs me is I bet you your opponents, your democratic opponent
[00:06:59.840 --> 00:07:01.520]   is paying for these ads oddly enough.
[00:07:01.520 --> 00:07:05.800]   But I bet your opponent is smart enough to know that that's not the case, but is playing
[00:07:05.800 --> 00:07:06.800]   on people's fears.
[00:07:06.800 --> 00:07:09.720]   And that really is a problem, I think.
[00:07:09.720 --> 00:07:10.720]   That is the problem of politics.
[00:07:10.720 --> 00:07:11.960]   It's a problem of DC.
[00:07:11.960 --> 00:07:12.960]   Yeah.
[00:07:12.960 --> 00:07:16.280]   Fear, it turns out, is a very good motivator to get people to vote.
[00:07:16.280 --> 00:07:17.280]   Yeah.
[00:07:17.280 --> 00:07:18.280]   Sad to say.
[00:07:18.280 --> 00:07:24.160]   And so unfortunately, politicians started using fear to motivate people.
[00:07:24.160 --> 00:07:25.680]   And it really is a bad motivator.
[00:07:25.680 --> 00:07:30.080]   And often is an inaccurate motivator, as in the case of cryptocurrency.
[00:07:30.080 --> 00:07:33.280]   I'm sorry that that's going on, but maybe we have a few...
[00:07:33.280 --> 00:07:34.280]   It's a great example.
[00:07:34.280 --> 00:07:36.280]   This is why we need evidence-based policymaking.
[00:07:36.280 --> 00:07:37.280]   Yes.
[00:07:37.280 --> 00:07:42.120]   It's why we need that in the four to five hundred other people that have come into DC, you
[00:07:42.120 --> 00:07:43.120]   know, despite this.
[00:07:43.120 --> 00:07:45.080]   It makes us a better government.
[00:07:45.080 --> 00:07:46.080]   I do.
[00:07:46.080 --> 00:07:47.080]   Okay.
[00:07:47.080 --> 00:07:49.440]   So let me bring up the one thing that comes to mind.
[00:07:49.440 --> 00:07:53.600]   You take Bitcoin donations and they can be a significant amount of money.
[00:07:53.600 --> 00:08:00.280]   One of the advantages of Bitcoin is it's an anonymous, relatively anonymous way to...
[00:08:00.280 --> 00:08:02.040]   Suiting on a miss, let's say.
[00:08:02.040 --> 00:08:04.120]   Suiting on a miss, right?
[00:08:04.120 --> 00:08:05.760]   Aren't there issues with election campaign?
[00:08:05.760 --> 00:08:06.760]   I mean, don't you...
[00:08:06.760 --> 00:08:08.760]   You have to report your donors.
[00:08:08.760 --> 00:08:09.760]   Yeah.
[00:08:09.760 --> 00:08:10.760]   And we take the...
[00:08:10.760 --> 00:08:11.760]   It's just like filling out a credit card.
[00:08:11.760 --> 00:08:15.760]   Like, you have filled the same exact information, your home address, your first name, your last
[00:08:15.760 --> 00:08:19.120]   name, all the information we get for you.
[00:08:19.120 --> 00:08:23.560]   All that address to make sure that it's going to that email address and it's all documented.
[00:08:23.560 --> 00:08:25.720]   And shared publicly through the FEC.
[00:08:25.720 --> 00:08:28.240]   We follow all the rules and regulations.
[00:08:28.240 --> 00:08:30.600]   Same as if we take a credit card or a check or cash.
[00:08:30.600 --> 00:08:32.480]   Well, yeah, we should point that out.
[00:08:32.480 --> 00:08:33.880]   Cash is truly anonymous.
[00:08:33.880 --> 00:08:34.880]   Yeah.
[00:08:34.880 --> 00:08:38.120]   And somebody could send you an envelope of cash.
[00:08:38.120 --> 00:08:39.680]   So it's the same exact issue.
[00:08:39.680 --> 00:08:42.280]   It's just a matter of you keeping track of it.
[00:08:42.280 --> 00:08:46.920]   It was interesting when we started getting the attacks, former FEC...
[00:08:46.920 --> 00:08:53.520]   Sorry, former Department of Homeland Security and Department of Justice officials in charge
[00:08:53.520 --> 00:08:55.560]   of prosecuting cyber crimes.
[00:08:55.560 --> 00:09:00.240]   The ones who actually prosecuted Ross Albrecht from the Silk Road said it's actually foolish
[00:09:00.240 --> 00:09:05.440]   for criminals to use Bitcoin because it's easier for us to track and prove what you did,
[00:09:05.440 --> 00:09:06.440]   which you did.
[00:09:06.440 --> 00:09:09.200]   So he's actually first with the use of this old technology called Cash.
[00:09:09.200 --> 00:09:12.840]   Yeah, if you really want to bribe a congressman, use cash.
[00:09:12.840 --> 00:09:13.840]   Everybody knows that.
[00:09:13.840 --> 00:09:15.840]   Oh, no, I mean, do the legal way.
[00:09:15.840 --> 00:09:17.080]   Call it a campaign contribution.
[00:09:17.080 --> 00:09:18.080]   Or a loan.
[00:09:18.080 --> 00:09:19.080]   Loans are great.
[00:09:19.080 --> 00:09:22.960]   Anyway, we're being facetious, but I really think it is an issue.
[00:09:22.960 --> 00:09:27.720]   Certainly, anybody who watches our shows or listens to our shows is technologically sophisticated
[00:09:27.720 --> 00:09:31.320]   enough to know that scare tactics are a bad idea.
[00:09:31.320 --> 00:09:32.520]   And we see a lot of it.
[00:09:32.520 --> 00:09:34.000]   And I think it's really...
[00:09:34.000 --> 00:09:39.040]   I think you raised a really good pointy, and it's important, or maybe it was Matt, that
[00:09:39.040 --> 00:09:44.320]   our elected representatives understand this stuff and not be using scare tactics, but
[00:09:44.320 --> 00:09:45.320]   actually...
[00:09:45.320 --> 00:09:47.240]   Because you can't make policy on scare tactics.
[00:09:47.240 --> 00:09:52.600]   It's like the current mean that everyone on tour is interested in either buying or selling
[00:09:52.600 --> 00:09:56.840]   drugs on tour or getting involved with child abuse or whatever.
[00:09:56.840 --> 00:10:00.800]   The biggest use of tour at the moment is Facebook.
[00:10:00.800 --> 00:10:02.800]   It's people going on to tour onto the Facebook side.
[00:10:02.800 --> 00:10:07.680]   Facebook has their own own own site, and that's the biggest use of tour at the moment.
[00:10:07.680 --> 00:10:11.800]   They reckon it's maybe about 10% of it is dodgy stuff, but I don't know.
[00:10:11.800 --> 00:10:16.800]   It's perception of reality, and we need people with a better idea of what the reality is.
[00:10:16.800 --> 00:10:20.320]   So wait, tours for drug dealers to check Facebook?
[00:10:20.320 --> 00:10:25.720]   Whatever I use Facebook.
[00:10:25.720 --> 00:10:26.800]   Start off with story number one.
[00:10:26.800 --> 00:10:29.320]   The FBI says reboot your router.
[00:10:29.320 --> 00:10:32.640]   It might sort of sound slightly.
[00:10:32.640 --> 00:10:35.120]   It's kind of an interesting story.
[00:10:35.120 --> 00:10:36.280]   There's a Russian...
[00:10:36.280 --> 00:10:39.680]   It turns out this is from Fancy Bear.
[00:10:39.680 --> 00:10:40.680]   Fancy Bear.
[00:10:40.680 --> 00:10:46.080]   You may remember them from such amazing hacks as Hacking John Podesta's account at the DNC.
[00:10:46.080 --> 00:10:48.800]   Known to be a state organizer.
[00:10:48.800 --> 00:10:51.000]   State sponsored hacker.
[00:10:51.000 --> 00:10:58.840]   Apparently, according to the FBI, Fancy Bear has a bit of malware called VPN Filter.
[00:10:58.840 --> 00:11:06.920]   It sits on your router, and it can be eliminated by rebooting the router.
[00:11:06.920 --> 00:11:10.440]   As with a lot of router malware, it's not persistent.
[00:11:10.440 --> 00:11:14.120]   Because routers are dumb computers, they don't have a lot of storage, but it does have a little
[00:11:14.120 --> 00:11:19.320]   bit of a stub that stores in the firmware of the router that if you reboot your router,
[00:11:19.320 --> 00:11:21.720]   VPN Filter will then...
[00:11:21.720 --> 00:11:24.760]   And the FBI discovered this because they found somebody in Pittsburgh, a woman in Pittsburgh
[00:11:24.760 --> 00:11:26.800]   who said, "Okay, yeah, you can monitor my network.
[00:11:26.800 --> 00:11:28.480]   I have it.
[00:11:28.480 --> 00:11:29.480]   You can monitor my network."
[00:11:29.480 --> 00:11:31.360]   So they monitored it, and this is what they saw.
[00:11:31.360 --> 00:11:32.360]   Reboot the router.
[00:11:32.360 --> 00:11:38.060]   The router then says, "Okay, goes to Photo Bucket where it looks for two images put there
[00:11:38.060 --> 00:11:39.240]   by Fancy Bear."
[00:11:39.240 --> 00:11:42.640]   And I wish there were white papers.
[00:11:42.640 --> 00:11:45.440]   That's not really completely clear how this works, but apparently in the metadata, there
[00:11:45.440 --> 00:11:49.040]   was sufficient information to reactivate the malware.
[00:11:49.040 --> 00:11:50.040]   Yeah.
[00:11:50.040 --> 00:11:53.400]   So what they're hoping is if everyone reboots their routers, it's all going to stop pinging
[00:11:53.400 --> 00:11:54.560]   the CNC pages.
[00:11:54.560 --> 00:11:58.080]   They get out of the command control servers, and they can actually try and take it down.
[00:11:58.080 --> 00:12:00.080]   But we are now the woods yet.
[00:12:00.080 --> 00:12:01.320]   No, but this is what happens.
[00:12:01.320 --> 00:12:04.200]   So the FBI took those...
[00:12:04.200 --> 00:12:07.680]   By analyzing the malware, they said, "Okay, well, the first thing we're doing is taking
[00:12:07.680 --> 00:12:10.120]   those images off of Photo Bucket."
[00:12:10.120 --> 00:12:14.360]   And then they watched, and it went to...
[00:12:14.360 --> 00:12:19.320]   It has hard-coded in its code, a site called toknowall.com.
[00:12:19.320 --> 00:12:25.360]   And so the code says, "If you can't get your photos at Photo Bucket, go to toknowall.com.
[00:12:25.360 --> 00:12:33.120]   FBI laid out Warren on Veracine, which is the registrar for toknowall.com, and took the
[00:12:33.120 --> 00:12:35.560]   DNS record over.
[00:12:35.560 --> 00:12:40.320]   And so it is safe to reboot your router at this point, because what will happen is your
[00:12:40.320 --> 00:12:44.520]   router will still be infected, it'll go, and it'll try to get the photos, can't do
[00:12:44.520 --> 00:12:46.120]   it, goes to knowall.
[00:12:46.120 --> 00:12:49.200]   And it will at that point give the FBI some information, because at that point the FBI
[00:12:49.200 --> 00:12:56.160]   will be able to see the routers, presumably the router manufacturers, and gather some information
[00:12:56.160 --> 00:12:57.160]   about how it works.
[00:12:57.160 --> 00:12:59.520]   Here's a list of the routers that are affected.
[00:12:59.520 --> 00:13:00.520]   Although...
[00:13:00.520 --> 00:13:02.280]   This is from Symantec.
[00:13:02.280 --> 00:13:05.000]   Although I would say this is a subset of the total.
[00:13:05.000 --> 00:13:10.600]   I was going to say, Cisco's towers team did a much better write-up about this.
[00:13:10.600 --> 00:13:16.880]   But they basically, they, I think, found the floor published, and then the FBI came in
[00:13:16.880 --> 00:13:21.600]   and said, "This is actually a national security threat, and we need to deal with it now because
[00:13:21.600 --> 00:13:23.600]   of the fancy-bearer connection."
[00:13:23.600 --> 00:13:29.440]   So it's routers from MicroTic, it's routers from Lynxis, that's a big name.
[00:13:29.440 --> 00:13:30.440]   Netgear, another big name.
[00:13:30.440 --> 00:13:35.440]   The big big name.
[00:13:35.440 --> 00:13:38.680]   TpLink, QNAP, which is a network attached storage device.
[00:13:38.680 --> 00:13:40.240]   So yeah, here's the Talos write-up it is.
[00:13:40.240 --> 00:13:43.600]   This is much, much more interesting.
[00:13:43.600 --> 00:13:47.000]   So the problem is we don't know all the routers, right?
[00:13:47.000 --> 00:13:51.200]   So rebooting shouldn't just be limited to those.
[00:13:51.200 --> 00:13:55.720]   Frankly, I would, if I were you, I know I'm going to go home and reboot tonight.
[00:13:55.720 --> 00:13:58.640]   Tell everybody on your in your house, "Okay, the network's going down.
[00:13:58.640 --> 00:14:00.120]   Go to bed."
[00:14:00.120 --> 00:14:01.120]   You have to do kids.
[00:14:01.120 --> 00:14:02.120]   Rush is in the house.
[00:14:02.120 --> 00:14:03.120]   Just turn it off and on.
[00:14:03.120 --> 00:14:04.120]   It'll be fine.
[00:14:04.120 --> 00:14:05.120]   But that's all you need to do, right?
[00:14:05.120 --> 00:14:06.120]   Unplug it.
[00:14:06.120 --> 00:14:07.120]   Wait a second or two.
[00:14:07.120 --> 00:14:09.120]   Leave it 15 seconds and then plug it back.
[00:14:09.120 --> 00:14:10.120]   Okay, plug it back here.
[00:14:10.120 --> 00:14:12.000]   And that will purge VPN filter.
[00:14:12.000 --> 00:14:14.440]   Do we know what VPN filter was targeting?
[00:14:14.440 --> 00:14:17.040]   Obviously, they were amassing a botnet.
[00:14:17.040 --> 00:14:22.800]   Yeah, the, it's the current style as you're at the moment to go after the routers, because
[00:14:22.800 --> 00:14:27.840]   we've seen a couple of cases whereby these people have worked out that if you want, the
[00:14:27.840 --> 00:14:32.760]   gold standard when you break into a network is to get the administrators infected.
[00:14:32.760 --> 00:14:36.440]   And it's the administrators who generally go on to the routers and do this work.
[00:14:36.440 --> 00:14:40.000]   So there's a whole new family of malware, which is targeting routers.
[00:14:40.000 --> 00:14:45.040]   So it's spearfishing at trying to get administrators to get into the router.
[00:14:45.040 --> 00:14:46.040]   Exactly.
[00:14:46.040 --> 00:14:47.280]   United States router, as you say.
[00:14:47.280 --> 00:14:49.160]   Oh, yes.
[00:14:49.160 --> 00:14:53.080]   We gave you the bloody language.
[00:14:53.080 --> 00:14:54.680]   So that's interesting.
[00:14:54.680 --> 00:14:58.560]   And then the, according to the FBI, they estimate half a million.
[00:14:58.560 --> 00:14:59.560]   Yeah.
[00:14:59.560 --> 00:15:00.560]   And that's a lowball estimate.
[00:15:00.560 --> 00:15:02.640]   But that's what I have to think is that we don't know.
[00:15:02.640 --> 00:15:04.480]   And so it's a significant number.
[00:15:04.480 --> 00:15:06.160]   You build a botnet like that.
[00:15:06.160 --> 00:15:10.160]   It's not, this is a case where they're not trying to get your credentials or your information
[00:15:10.160 --> 00:15:12.760]   or not trying to get it probably into your company.
[00:15:12.760 --> 00:15:14.680]   They're probably amassing a botnet to attack.
[00:15:14.680 --> 00:15:15.680]   Yeah.
[00:15:15.680 --> 00:15:16.680]   Let's say the grid.
[00:15:16.680 --> 00:15:17.680]   Yeah.
[00:15:17.680 --> 00:15:18.680]   As we know Russia did to Ukraine.
[00:15:18.680 --> 00:15:22.960]   There's some very interesting malware coming through this week on just that area here.
[00:15:22.960 --> 00:15:25.800]   It's looking like we're getting a lot of infrastructure hacking going on at the moment
[00:15:25.800 --> 00:15:26.800]   from group.
[00:15:26.800 --> 00:15:27.800]   So this is cyber warfare.
[00:15:27.800 --> 00:15:28.800]   Yeah.
[00:15:28.800 --> 00:15:30.600]   Well, you know, I mean, America started it.
[00:15:30.600 --> 00:15:32.800]   So they can't really complain when people use it as well.
[00:15:32.800 --> 00:15:33.800]   But that's.
[00:15:33.800 --> 00:15:36.320]   In fact, I was a little disturbed because I can't remember who it was, but a couple of
[00:15:36.320 --> 00:15:42.120]   weeks ago, somebody at the Pentagon said we, they announced that we're going to have both
[00:15:42.120 --> 00:15:45.440]   an offensive and a defensive capability in cyber warfare.
[00:15:45.440 --> 00:15:47.800]   And I, I wonder if that's a good idea.
[00:15:47.800 --> 00:15:49.360]   I mean, I guess it's too late.
[00:15:49.360 --> 00:15:50.360]   It's too late.
[00:15:50.360 --> 00:15:56.720]   It's when the NSA, I mean, America didn't even used to have a code breaking unit until,
[00:15:56.720 --> 00:16:01.040]   I think it was 1924 because the official excuse given almost Victorian was that gentlemen
[00:16:01.040 --> 00:16:03.520]   don't read other people's other gentleman's letters.
[00:16:03.520 --> 00:16:04.520]   Yes.
[00:16:04.520 --> 00:16:06.120]   I think we should go back to that time.
[00:16:06.120 --> 00:16:08.960]   It would be nice, but the Chinese and the Russians aren't playing balls.
[00:16:08.960 --> 00:16:11.320]   So you need to have something in there.
[00:16:11.320 --> 00:16:15.800]   But after Stuxnet came out, it became clear it was pretty much open season out there.
[00:16:15.800 --> 00:16:20.840]   And yeah, we're seeing this a really nasty set family in Malware that's been used to
[00:16:20.840 --> 00:16:24.960]   infect a lot of Saudi things like Aramco and some of their major industries.
[00:16:24.960 --> 00:16:27.960]   And that's now popping up and it's a slightly different form in the US.
[00:16:27.960 --> 00:16:30.520]   So they're suggesting it may be Iranian based.
[00:16:30.520 --> 00:16:33.120]   But yeah, it's going to be interesting times.
[00:16:33.120 --> 00:16:34.120]   Stay safe, everyone.
[00:16:34.120 --> 00:16:38.280]   You know, the timing's not great, but the White House eliminated the cyber security
[00:16:38.280 --> 00:16:41.760]   coordinator on the NSC a week and a half ago.
[00:16:41.760 --> 00:16:49.640]   So Brian, what's your, I'm not going to ask you for a campaign position, but it seems
[00:16:49.640 --> 00:16:54.720]   to me that we are, do you feel like we're taking the cyber security threat seriously
[00:16:54.720 --> 00:16:58.640]   from nation states?
[00:16:58.640 --> 00:17:07.360]   I think the underlying agencies at the civil servant level are taking it seriously.
[00:17:07.360 --> 00:17:11.640]   And I think we always have to distinguish that there are probably 3 million civil servants
[00:17:11.640 --> 00:17:16.240]   working hard that don't get any of the spotlight that are doing incredible work for the country
[00:17:16.240 --> 00:17:19.080]   regardless of who the president is.
[00:17:19.080 --> 00:17:26.480]   And at the political level, yeah, I worry when you eliminate position in charge of cyber
[00:17:26.480 --> 00:17:28.680]   security for the country.
[00:17:28.680 --> 00:17:33.640]   I at a point when we're in the middle of negotiating with North Korea, because Ian, you forgot
[00:17:33.640 --> 00:17:38.480]   to mention North Korea and the attack on Sony because they didn't like a movie.
[00:17:38.480 --> 00:17:39.480]   Right.
[00:17:39.480 --> 00:17:43.240]   And with an effective attack, I'm slightly skeptical about North Korea and that thing,
[00:17:43.240 --> 00:17:44.880]   but I'll let that one pass.
[00:17:44.880 --> 00:17:47.160]   Well, I'll give you another one.
[00:17:47.160 --> 00:17:51.000]   We've thrown out the nuclear deal with Iran and this article in New York Times without
[00:17:51.000 --> 00:17:54.800]   the nuclear deal, US expects resurgence in Iranian cyber attacks as well.
[00:17:54.800 --> 00:17:55.800]   Oh, yeah.
[00:17:55.800 --> 00:18:02.120]   And of course, we know that Stuxnet was designed to attack Iranian centrifuges where they concentrate
[00:18:02.120 --> 00:18:03.120]   at uranium.
[00:18:03.120 --> 00:18:05.160]   I don't know if Stuxnet was a US effort.
[00:18:05.160 --> 00:18:06.160]   It's probably a US-Israeli.
[00:18:06.160 --> 00:18:07.160]   Yeah, probably.
[00:18:07.160 --> 00:18:08.160]   Yeah, probably.
[00:18:08.160 --> 00:18:09.160]   It's Olympic.
[00:18:09.160 --> 00:18:12.560]   So as you say, we've had offensive capability for a while.
[00:18:12.560 --> 00:18:15.480]   It would be, I wouldn't be surprised if the Iranians said, well, all right.
[00:18:15.480 --> 00:18:16.480]   Well, there's even a story.
[00:18:16.480 --> 00:18:24.240]   It's never been officially confirmed that this was done back in 1983.
[00:18:24.240 --> 00:18:30.160]   They found that the then-soviets were trying to steal a gas control system software from
[00:18:30.160 --> 00:18:31.920]   a Canadian firm.
[00:18:31.920 --> 00:18:34.120]   They said, fine, let them do it.
[00:18:34.120 --> 00:18:37.440]   Insterious enough breaks into the code so that when they tried rolling out on their
[00:18:37.440 --> 00:18:41.000]   own gas system, it calls the largest non-nuclear explosion yet seen.
[00:18:41.000 --> 00:18:42.000]   Oh, nice.
[00:18:42.000 --> 00:18:43.000]   That's good.
[00:18:43.000 --> 00:18:48.640]   So, I mean, it's always been on the cards as it were, but it's we're going nuclear on
[00:18:48.640 --> 00:18:49.640]   it now.
[00:18:49.640 --> 00:18:54.520]   Everyone with a little bit of skill and some store-bought malware can get into the game.
[00:18:54.520 --> 00:18:58.480]   How well prepared are we for cyber warfare, Matt?
[00:18:58.480 --> 00:19:02.600]   The problem with cyber warfare is one mistake means that you are now vulnerable.
[00:19:02.600 --> 00:19:08.600]   And so I think every country probably needs to do a lot more on trying to make sure that
[00:19:08.600 --> 00:19:11.880]   it's deeply secured and batten down the hatches.
[00:19:11.880 --> 00:19:17.640]   But I mean, the fact of the matter is that there's a lot of work yet to be done on everybody's
[00:19:17.640 --> 00:19:18.640]   side.
[00:19:18.640 --> 00:19:21.200]   And we found malware in our grid, right?
[00:19:21.200 --> 00:19:23.480]   Our electric grid.
[00:19:23.480 --> 00:19:24.480]   Our electric grid.
[00:19:24.480 --> 00:19:29.280]   Well, and we also have to think about the impacts of the private sector as well.
[00:19:29.280 --> 00:19:34.880]   According to a Harvard study with the Equifax hack that released all of our personal information,
[00:19:34.880 --> 00:19:39.640]   you have enough information out there to be able to change the voter registration of
[00:19:39.640 --> 00:19:42.280]   individuals in 35 states.
[00:19:42.280 --> 00:19:44.120]   Oh, geez.
[00:19:44.120 --> 00:19:45.360]   So our election can be hacked.
[00:19:45.360 --> 00:19:48.360]   And so this is not just about our electric grid.
[00:19:48.360 --> 00:19:53.440]   This is not about protecting our companies and their private information or personal individuals
[00:19:53.440 --> 00:19:55.400]   and their personal information.
[00:19:55.400 --> 00:19:57.760]   It's about our democracy.
[00:19:57.760 --> 00:19:59.680]   But it's of course also about national security.
[00:19:59.680 --> 00:20:03.480]   So you have to look at it from a variety of angles and it just blows my mind that we're
[00:20:03.480 --> 00:20:07.480]   in a time right now where we're actually having congressional hearings about the role of
[00:20:07.480 --> 00:20:12.160]   Russia and Facebook impacting our elections yet we're eliminating the top cybersecurity
[00:20:12.160 --> 00:20:13.160]   position.
[00:20:13.160 --> 00:20:19.160]   Well, and I also, this, I'll submit this and you can shoot me down.
[00:20:19.160 --> 00:20:23.400]   I feel like Facebook, all this attention to Facebook is a little bit of a sideshow.
[00:20:23.400 --> 00:20:29.800]   You know, yes, we know Russia bought ads and created fake groups and did all sorts of
[00:20:29.800 --> 00:20:36.800]   things to basically so dissent and controversy and polarization this country.
[00:20:36.800 --> 00:20:43.400]   But it seems to me that the cyber attacks potentially have a much greater impact than
[00:20:43.400 --> 00:20:45.400]   fake news on Facebook.
[00:20:45.400 --> 00:20:46.400]   Matt.
[00:20:46.400 --> 00:20:48.960]   It was interesting.
[00:20:48.960 --> 00:20:53.000]   In the rundown there was an interview with the people at Facebook who care about fake
[00:20:53.000 --> 00:20:55.600]   news and trying to figure out misinformation.
[00:20:55.600 --> 00:21:00.200]   And I do think that that is really important because if we can have trust in new sources
[00:21:00.200 --> 00:21:04.760]   and trust in the ability for people to know that they're getting information from reputable
[00:21:04.760 --> 00:21:07.760]   places, I think that does really help quite a bit.
[00:21:07.760 --> 00:21:13.320]   So it's interesting because all these attacks are one matter, but trying to make sure that
[00:21:13.320 --> 00:21:18.040]   people can get the best possible information, I think that also matters a lot.
[00:21:18.040 --> 00:21:23.040]   I mean, it's not that we can't do both, obviously.
[00:21:23.040 --> 00:21:29.800]   But I feel like sometimes the attention paid to Facebook is a little bit distracting when
[00:21:29.800 --> 00:21:33.800]   it comes to, I mean, I don't know.
[00:21:33.800 --> 00:21:36.720]   Matt.
[00:21:36.720 --> 00:21:40.440]   Not everyone in America has deep technical experience.
[00:21:40.440 --> 00:21:45.760]   And so what you oftentimes have to do with deep technical problems is highlight something
[00:21:45.760 --> 00:21:49.520]   that people can quickly understand and get their head around.
[00:21:49.520 --> 00:21:55.760]   We can talk about climate change, for example, and people will make a connection between software
[00:21:55.760 --> 00:21:57.160]   and climate change.
[00:21:57.160 --> 00:22:02.120]   But when you see that Volkswagen used software to get around the Clean Air Act, then they
[00:22:02.120 --> 00:22:04.560]   start to worry about software.
[00:22:04.560 --> 00:22:09.760]   Or when you see that pro boards across this country are using software to determine the
[00:22:09.760 --> 00:22:17.280]   likelihood of someone recommending a crime, and then you show that algorithm is systematically
[00:22:17.280 --> 00:22:22.880]   discriminating against black men, then they start to understand that algorithms and artificial
[00:22:22.880 --> 00:22:26.240]   intelligence is the next frontier of our civil rights.
[00:22:26.240 --> 00:22:30.800]   Or the democracy I gave with Equifax, you have to tie national headlines that people
[00:22:30.800 --> 00:22:36.480]   are reading into the underlying technology that is making an impact.
[00:22:36.480 --> 00:22:39.680]   Otherwise, people are not going to understand or care.
[00:22:39.680 --> 00:22:42.200]   And a big part of this is making people care.
[00:22:42.200 --> 00:22:43.200]   Right.
[00:22:43.200 --> 00:22:48.320]   Although I think what ends up happening is people debate how Facebook and Facebook ads
[00:22:48.320 --> 00:22:53.280]   when really the larger story, which is, I think, fairly clear at this point, is that
[00:22:53.280 --> 00:22:58.560]   Russia did a lot of things to try to influence the election and does a lot of things currently
[00:22:58.560 --> 00:23:05.680]   to try to influence the United States, including Facebook, but far from limited.
[00:23:05.680 --> 00:23:14.560]   I mean, if you tell people there's malware on the computers at your local power station
[00:23:14.560 --> 00:23:19.200]   that was put there by Russians, that seems to me to something that would attract the
[00:23:19.200 --> 00:23:22.240]   attention of the average American voter.
[00:23:22.240 --> 00:23:24.000]   I would not have so.
[00:23:24.000 --> 00:23:29.280]   Not until there's an actual power outage as a result of it.
[00:23:29.280 --> 00:23:34.160]   You know, when we think about the example I have to go back to is when I think it was
[00:23:34.160 --> 00:23:37.360]   a power outage in Ohio that knocked New York off the grid.
[00:23:37.360 --> 00:23:38.360]   Right.
[00:23:38.360 --> 00:23:41.200]   And it proved, oh, wow, this is really interconnected and really important for us to think about.
[00:23:41.200 --> 00:23:42.400]   Yeah, we didn't know that, did we?
[00:23:42.400 --> 00:23:43.400]   Yeah.
[00:23:43.400 --> 00:23:44.400]   That's right.
[00:23:44.400 --> 00:23:47.480]   But also, I mean, when it comes to voting, the state of voting machine security is just
[00:23:47.480 --> 00:23:48.480]   painfully bad.
[00:23:48.480 --> 00:23:49.480]   That's a huge issue.
[00:23:49.480 --> 00:23:52.040]   I mean, I was at the DEF CON voting machine thing.
[00:23:52.040 --> 00:23:55.440]   They'd managed to hack one of these systems before the opening statements have been finished
[00:23:55.440 --> 00:23:56.440]   by the presenters.
[00:23:56.440 --> 00:24:00.280]   I mean, you know, I mean, they're running Windows XP, Windows CE.
[00:24:00.280 --> 00:24:01.280]   Oh, yeah.
[00:24:01.280 --> 00:24:03.360]   Well, why are they even connected to the Internet is another question?
[00:24:03.360 --> 00:24:04.360]   Yeah.
[00:24:04.360 --> 00:24:05.520]   I don't think they are.
[00:24:05.520 --> 00:24:07.400]   I think they were going through USB ports.
[00:24:07.400 --> 00:24:08.400]   Oh, OK.
[00:24:08.400 --> 00:24:10.320]   Well, there were a variety of ways that we're actually, I think, so.
[00:24:10.320 --> 00:24:11.320]   Yeah.
[00:24:11.320 --> 00:24:14.400]   I mean, if you actually get physical access to the device in some cases, you can, yeah.
[00:24:14.400 --> 00:24:15.400]   Yeah.
[00:24:15.400 --> 00:24:16.400]   Yeah.
[00:24:16.400 --> 00:24:20.720]   But it's just, and there's legislation in front of Congress right now to say, look, we'll
[00:24:20.720 --> 00:24:25.760]   give a certain amount of money to secure these machines, we'll get properly mandated
[00:24:25.760 --> 00:24:30.200]   people who get the software and security alerts, and they still haven't moved on it.
[00:24:30.200 --> 00:24:32.840]   It's like, this is what democracy is about.
[00:24:32.840 --> 00:24:37.360]   And this is how it's going to, you know, it's a pretty low stage of affairs.
[00:24:37.360 --> 00:24:39.320]   I'm hoping that you can.
[00:24:39.320 --> 00:24:45.120]   Well, but I think that is so anybody who, again, who's a Twit fan is sophisticated enough
[00:24:45.120 --> 00:24:50.480]   technologically to understand these issues and probably have an opinion about it.
[00:24:50.480 --> 00:24:52.080]   So really, I think Brian, you nailed it.
[00:24:52.080 --> 00:24:56.200]   Really, the question is not whether this is happening, even maybe what to do with it,
[00:24:56.200 --> 00:25:00.760]   but how to convince the American voting public that this is important.
[00:25:00.760 --> 00:25:01.760]   That's right.
[00:25:01.760 --> 00:25:03.120]   That is the most important thing.
[00:25:03.120 --> 00:25:04.120]   Yeah.
[00:25:04.120 --> 00:25:10.200]   I mean, Mark Zuckerberg's congressional hearing was one of the most helpful things to help
[00:25:10.200 --> 00:25:15.400]   emphasize a quick exclamation point on the fact that our members of Congress don't understand
[00:25:15.400 --> 00:25:16.400]   technology.
[00:25:16.400 --> 00:25:17.400]   That was obvious.
[00:25:17.400 --> 00:25:18.840]   It was a little bit embarrassing.
[00:25:18.840 --> 00:25:19.840]   I'll go to say.
[00:25:19.840 --> 00:25:24.920]   Although, and this is more subtle, I think the format also hurt them.
[00:25:24.920 --> 00:25:28.880]   And a lot of what they were doing, sometimes grandstanding and monologuing, was because
[00:25:28.880 --> 00:25:33.000]   they only had five minutes, so there was no opportunity for them to do any follow up
[00:25:33.000 --> 00:25:34.000]   of any kind.
[00:25:34.000 --> 00:25:35.520]   It was a terrible format.
[00:25:35.520 --> 00:25:40.080]   And that's a maybe more subtle problem that viewers didn't see.
[00:25:40.080 --> 00:25:44.240]   So what you might have gotten is this impression, well, my member of Congress is just talking,
[00:25:44.240 --> 00:25:47.120]   talking, talking, asked one question, a mark that interrupts him.
[00:25:47.120 --> 00:25:48.400]   Well, there was a reason for that.
[00:25:48.400 --> 00:25:51.760]   They didn't have any time to follow up or anything.
[00:25:51.760 --> 00:25:53.400]   And so we can.
[00:25:53.400 --> 00:25:56.720]   What questions would you have asked, Mark?
[00:25:56.720 --> 00:26:00.280]   You know, honestly, I don't think we need to ask, Mark.
[00:26:00.280 --> 00:26:03.240]   I think it's obvious.
[00:26:03.240 --> 00:26:07.800]   I think, and I don't think, of course, I've never interviewed CEOs because I don't trust,
[00:26:07.800 --> 00:26:12.480]   I don't think a CEO is ever going to get off point or off message.
[00:26:12.480 --> 00:26:16.760]   It's almost as bad as interviewing politicians, Brian.
[00:26:16.760 --> 00:26:18.160]   They know enough to answer the question.
[00:26:18.160 --> 00:26:20.000]   They want to answer not the question you asked.
[00:26:20.000 --> 00:26:21.160]   It's very difficult.
[00:26:21.160 --> 00:26:24.080]   But I also think we know, we certainly know enough now.
[00:26:24.080 --> 00:26:28.760]   The good news is whatever pressure has been put on Facebook is kind of paying off.
[00:26:28.760 --> 00:26:32.800]   Facebook has released many of the ads now and we're getting to see what's going on.
[00:26:32.800 --> 00:26:34.040]   GDPR has helped.
[00:26:34.040 --> 00:26:38.480]   Facebook's been a little bit more responsible about maintaining your data and showing you
[00:26:38.480 --> 00:26:39.720]   what they know.
[00:26:39.720 --> 00:26:41.360]   So Facebook's moving in the right direction.
[00:26:41.360 --> 00:26:42.920]   A lot of that's because of the pressure.
[00:26:42.920 --> 00:26:44.520]   But I don't think the actual tech.
[00:26:44.520 --> 00:26:49.360]   So if I'm a member of Congress, I got five minutes to ask a question on Mark Zuckerberg.
[00:26:49.360 --> 00:26:51.320]   I don't know what you ask.
[00:26:51.320 --> 00:26:52.760]   Mark, what are you going to do about this?
[00:26:52.760 --> 00:26:53.760]   I don't know.
[00:26:53.760 --> 00:26:55.520]   I'd be asking about shadow profiles.
[00:26:55.520 --> 00:26:58.320]   So the idea that they did a couple of times, that's a huge one.
[00:26:58.320 --> 00:27:02.240]   But drilling in a little deeper, you might not have to ask about shadow profiles.
[00:27:02.240 --> 00:27:03.240]   So what is shadow profiles?
[00:27:03.240 --> 00:27:07.040]   Well, just the idea that, like, for example, not to, okay, I'll say it.
[00:27:07.040 --> 00:27:10.600]   I haven't had a Facebook account activated since, like, 2011, right?
[00:27:10.600 --> 00:27:15.920]   And so in theory, I have chrome extensions that make sure that I block all the Facebook
[00:27:15.920 --> 00:27:16.920]   tracking.
[00:27:16.920 --> 00:27:22.040]   And yet, if somebody knows me and is a friend of me, then Facebook could build up a profile
[00:27:22.040 --> 00:27:25.120]   of me, even without me actively participating in any way.
[00:27:25.120 --> 00:27:28.320]   So I think there's fair questions about, you know, how do you opt out?
[00:27:28.320 --> 00:27:31.120]   How do you take care of those sorts of situations?
[00:27:31.120 --> 00:27:32.120]   Yeah.
[00:27:32.120 --> 00:27:38.320]   Facebook, when asked, this is my point, Zuckerberg said, well, we do collect data about people
[00:27:38.320 --> 00:27:43.040]   who haven't signed up for Facebook for security purposes.
[00:27:43.040 --> 00:27:47.200]   I don't know what those security purposes might be.
[00:27:47.200 --> 00:27:54.320]   And they said, we don't have shadow profiles, but of course they do.
[00:27:54.320 --> 00:27:57.920]   And the like button follows you around everywhere you go on the internet.
[00:27:57.920 --> 00:27:58.920]   Yeah.
[00:27:58.920 --> 00:28:04.080]   So even on to sort of sites of dubious coming, we were talking about this in the office,
[00:28:04.080 --> 00:28:06.960]   that pornography sites have the little like button.
[00:28:06.960 --> 00:28:09.800]   And the only reason that it is there is to track data, because it's not as though you
[00:28:09.800 --> 00:28:10.800]   average porn watches.
[00:28:10.800 --> 00:28:11.800]   I like this.
[00:28:11.800 --> 00:28:12.800]   Oh, I like this.
[00:28:12.800 --> 00:28:13.800]   I like this.
[00:28:13.800 --> 00:28:16.080]   Share this with my granny in my Facebook profile.
[00:28:16.080 --> 00:28:17.080]   I like it.
[00:28:17.080 --> 00:28:18.080]   It's a tracking button.
[00:28:18.080 --> 00:28:20.040]   It's not necessarily a like button.
[00:28:20.040 --> 00:28:25.520]   But see, there's an example, Brian, of really the interesting question would be to ask Mark
[00:28:25.520 --> 00:28:28.720]   about the like button and what data they collect with a like button and what they do
[00:28:28.720 --> 00:28:29.880]   with that data.
[00:28:29.880 --> 00:28:34.600]   The problem is that requires a grounding in the technology and understanding of what's
[00:28:34.600 --> 00:28:35.600]   going on.
[00:28:35.600 --> 00:28:40.280]   And no, Mark Warner's Senator Warren is not going to say, well, tell me about cookies
[00:28:40.280 --> 00:28:42.360]   because that's too complicated.
[00:28:42.360 --> 00:28:47.640]   Well, and it's also, I think there was some issues where if you have five minutes to talk,
[00:28:47.640 --> 00:28:51.160]   people might grandstand for three minutes before they do one question because you have
[00:28:51.160 --> 00:28:54.280]   to think about the tech savvy ability to do the follow up questions.
[00:28:54.280 --> 00:28:55.280]   Right.
[00:28:55.280 --> 00:28:58.360]   Because it didn't feel like everybody in that hearing was completely ready for the follow
[00:28:58.360 --> 00:28:59.360]   up question.
[00:28:59.360 --> 00:29:00.360]   So they didn't want to maybe.
[00:29:00.360 --> 00:29:02.040]   They didn't want follow up questions.
[00:29:02.040 --> 00:29:05.320]   Well, or maybe they did and they just squandered the chance a little bit.
[00:29:05.320 --> 00:29:06.320]   Yeah.
[00:29:06.320 --> 00:29:11.240]   It was quite interesting watching that versus the EU presentation he gave where again,
[00:29:11.240 --> 00:29:14.040]   it was the same sort of short time period format.
[00:29:14.040 --> 00:29:17.760]   But Zuckerberg was kind of slightly more confident about the whole thing.
[00:29:17.760 --> 00:29:21.160]   And eventually at the end, I think that just about wraps it up.
[00:29:21.160 --> 00:29:22.160]   He said that.
[00:29:22.160 --> 00:29:23.160]   He said that.
[00:29:23.160 --> 00:29:24.160]   You're just like, I'm done.
[00:29:24.160 --> 00:29:25.680]   I think that's your role somehow.
[00:29:25.680 --> 00:29:27.560]   But he's not even appearing at the UK Parliament.
[00:29:27.560 --> 00:29:29.280]   They've asked him to turn up and he said no.
[00:29:29.280 --> 00:29:34.040]   So, you know, actually it was Ben Loo-John of New Mexico who asked about shadow profile.
[00:29:34.040 --> 00:29:35.600]   So let's give him some credit.
[00:29:35.600 --> 00:29:37.680]   What would you, how would you deal with that, Brian?
[00:29:37.680 --> 00:29:42.720]   If you're sitting on that panel, you've got five minutes.
[00:29:42.720 --> 00:29:43.720]   What would you do?
[00:29:43.720 --> 00:29:45.840]   I probably, I mean, there's a couple of questions.
[00:29:45.840 --> 00:29:51.920]   I think, you know, his goal this year, you know, how each year he has a personal goal.
[00:29:51.920 --> 00:29:57.040]   His goal this year was to understand cryptocurrencies and the role of decentralization.
[00:29:57.040 --> 00:30:03.000]   And so I would ask him, you know, is there a future in which you will put personal information
[00:30:03.000 --> 00:30:07.360]   onto a blockchain that gives more people, more individuals control over the access of their
[00:30:07.360 --> 00:30:13.040]   data so that when that information is given to a place, to applications or sources like
[00:30:13.040 --> 00:30:16.680]   Cambridge Analytica, it gives the user control to pull that information back.
[00:30:16.680 --> 00:30:20.960]   I mean, this is something that we did in my own research lab at MIT, not with social
[00:30:20.960 --> 00:30:24.520]   records but with medical records.
[00:30:24.520 --> 00:30:28.000]   And so I would be curious to know, you know, where is he going or where is he thinking about
[00:30:28.000 --> 00:30:29.000]   this?
[00:30:29.000 --> 00:30:32.960]   I think the other thing, every five minutes he said, hey, I was going to solve some
[00:30:32.960 --> 00:30:33.960]   something.
[00:30:33.960 --> 00:30:36.600]   So my question.
[00:30:36.600 --> 00:30:39.680]   That's literally, well, not literally, but that's a perfect example of kicking the can
[00:30:39.680 --> 00:30:40.680]   down the road.
[00:30:40.680 --> 00:30:42.680]   I want a unicorn pony too.
[00:30:42.680 --> 00:30:43.680]   Yeah.
[00:30:43.680 --> 00:30:44.680]   It's just, it's not going to happen.
[00:30:44.680 --> 00:30:46.760]   Hey, I'll solve everything.
[00:30:46.760 --> 00:30:50.400]   You know, and so I'd say, well, you know, are we going to look at what DARPA is doing
[00:30:50.400 --> 00:30:55.400]   with explainable artificial intelligence so that we at least have some understanding of
[00:30:55.400 --> 00:30:59.680]   how artificial intelligence, there's no systematic biases in artificial intelligence that you're
[00:30:59.680 --> 00:31:00.680]   using.
[00:31:00.680 --> 00:31:03.760]   I would ask him about privacy settings.
[00:31:03.760 --> 00:31:09.000]   I know the incumbent that I'm running against asked him about two different screens and how
[00:31:09.000 --> 00:31:10.800]   it was complicated for privacy settings.
[00:31:10.800 --> 00:31:13.840]   Well, I think Snapchat is complicated for a lot of people too.
[00:31:13.840 --> 00:31:17.040]   But if you have the incentive to use it, you'll use it and you'll figure it out.
[00:31:17.040 --> 00:31:22.960]   You know, I know every minute how my friends, my family are using my data because I see
[00:31:22.960 --> 00:31:25.560]   an alert on my phone that says, hey, someone shared something.
[00:31:25.560 --> 00:31:26.880]   Hey, someone commented something.
[00:31:26.880 --> 00:31:28.280]   Someone liked something.
[00:31:28.280 --> 00:31:32.000]   So, you know, is there, is there, is there possible to put an option where it says, you
[00:31:32.000 --> 00:31:36.640]   know, hey, private company X has access to your access to your data for advertising or
[00:31:36.640 --> 00:31:39.080]   for this or for that.
[00:31:39.080 --> 00:31:44.440]   That if I think of America or the world knew how much of your personal information and
[00:31:44.440 --> 00:31:50.320]   how frequently it was being accessed by not your friends or not your family, that wouldn't
[00:31:50.320 --> 00:31:54.920]   incentivize enough people to start reconfiguring their privacy settings pretty quickly regardless
[00:31:54.920 --> 00:31:55.920]   of the interface.
[00:31:55.920 --> 00:31:56.920]   Yeah.
[00:31:56.920 --> 00:32:02.640]   I'll give you an example that everybody knows this is happening and nobody cares about.
[00:32:02.640 --> 00:32:06.680]   Every time you use an app, Facebook is one, but many, many apps in your phone and asks
[00:32:06.680 --> 00:32:09.080]   you, we would like to, you know, look for your friends.
[00:32:09.080 --> 00:32:13.120]   Snapchat is you look to your friends, give us access to your contacts.
[00:32:13.120 --> 00:32:18.800]   And most people say yes because that's a useful thing.
[00:32:18.800 --> 00:32:22.080]   Every time you do that, you're not just giving your information to this company.
[00:32:22.080 --> 00:32:25.720]   You're giving every single person in your contact list over to them.
[00:32:25.720 --> 00:32:28.040]   Talk about friends of friends.
[00:32:28.040 --> 00:32:32.800]   You're literally snitching on everybody you know.
[00:32:32.800 --> 00:32:33.800]   We do this.
[00:32:33.800 --> 00:32:35.200]   We all know what happens.
[00:32:35.200 --> 00:32:40.840]   We pay no attention to that and nobody's calling for this practice to go away.
[00:32:40.840 --> 00:32:43.480]   You know, I remember, um, path, remember path.
[00:32:43.480 --> 00:32:46.800]   They've mourns, they got a lot of trouble because they asked for the contact list and
[00:32:46.800 --> 00:32:49.080]   they kept it.
[00:32:49.080 --> 00:32:52.960]   And Dave said, well, yeah, of course we keep it because how else are we going to ping you
[00:32:52.960 --> 00:32:54.320]   when a friend joins path?
[00:32:54.320 --> 00:32:57.120]   You can do this by looking at your friends.
[00:32:57.120 --> 00:33:01.360]   But that, and they were, and they got a lot of trouble for that.
[00:33:01.360 --> 00:33:03.800]   Nobody says anything about the fact that you do it every single day.
[00:33:03.800 --> 00:33:05.440]   You turn in your friends information.
[00:33:05.440 --> 00:33:08.240]   You're their phone number, their address.
[00:33:08.240 --> 00:33:09.480]   Forget Equifax.
[00:33:09.480 --> 00:33:12.880]   There is a massive leak of information and you did it to me.
[00:33:12.880 --> 00:33:13.880]   Well, obviously Tom.
[00:33:13.880 --> 00:33:14.880]   Yeah.
[00:33:14.880 --> 00:33:19.520]   I was going to say that was a very, you know, I mean, everybody's doing it.
[00:33:19.520 --> 00:33:23.920]   I feel the same way in that I, the office of millennial was trying to get me to download
[00:33:23.920 --> 00:33:24.920]   Snapchat and use it.
[00:33:24.920 --> 00:33:25.920]   The office of the office.
[00:33:25.920 --> 00:33:29.240]   And I looked at, it's kind of like the office.
[00:33:29.240 --> 00:33:31.440]   But, and then looked at the amount of permissions it wanted.
[00:33:31.440 --> 00:33:32.840]   It was like hell with that.
[00:33:32.840 --> 00:33:35.120]   You know, it's, you know, you're one of the few.
[00:33:35.120 --> 00:33:36.120]   Unfortunately, yeah.
[00:33:36.120 --> 00:33:39.320]   And I've tried to, you know, I've got family members who love doing those which Star Trek
[00:33:39.320 --> 00:33:43.760]   character are you quizzes on Facebook and you've tried to say to the police, stop doing
[00:33:43.760 --> 00:33:44.760]   this.
[00:33:44.760 --> 00:33:45.760]   Go ahead, Brian.
[00:33:45.760 --> 00:33:50.480]   Well, I would say that it's, it's, you know, people kind of know that they're releasing
[00:33:50.480 --> 00:33:51.480]   this data.
[00:33:51.480 --> 00:33:52.480]   They don't really know.
[00:33:52.480 --> 00:33:55.520]   But they really want what would be really helpful in changing culture.
[00:33:55.520 --> 00:33:58.600]   Cause this is like tech is the easy part.
[00:33:58.600 --> 00:33:59.600]   It's culture change.
[00:33:59.600 --> 00:34:00.600]   That's the hard part.
[00:34:00.600 --> 00:34:01.600]   That was my point.
[00:34:01.600 --> 00:34:02.600]   The people are doing this.
[00:34:02.600 --> 00:34:05.920]   And they don't even think twice about it.
[00:34:05.920 --> 00:34:12.000]   And the only way you're going to change culture is by exposing, having the option to be exposed
[00:34:12.000 --> 00:34:18.320]   to how often your personal data is being accessed by third parties or by Facebook.
[00:34:18.320 --> 00:34:23.360]   And, you know, once you do that, then you have the opportunity for the user to be informed.
[00:34:23.360 --> 00:34:27.400]   So they may say, hey, I am comfortable releasing this information to Facebook because I trust
[00:34:27.400 --> 00:34:28.400]   Facebook.
[00:34:28.400 --> 00:34:32.320]   If you may, less people say that now than they did before, but, you know, maybe Facebook's
[00:34:32.320 --> 00:34:36.640]   a trusted brand, but maybe there's another brand that's accessing their information for
[00:34:36.640 --> 00:34:38.480]   whom they don't trust.
[00:34:38.480 --> 00:34:43.440]   And when you have an alert on your phone that says, hey, XYZ company is accessing your information,
[00:34:43.440 --> 00:34:45.440]   then that's a whole different story.
[00:34:45.440 --> 00:34:47.200]   And so what's really needs.
[00:34:47.200 --> 00:34:51.760]   Should, so my point is that we should know whoever is accessing our data.
[00:34:51.760 --> 00:34:57.880]   So when I give my address book to Snapchat, should Snapchat then go through my address
[00:34:57.880 --> 00:35:02.440]   book and notify everybody in it that Leo Laport has just handed over your name, address,
[00:35:02.440 --> 00:35:10.760]   phone number, child's birth dates and ages, wife, place of work, that should be notified.
[00:35:10.760 --> 00:35:13.200]   Snapchat says notify everybody in my contact list.
[00:35:13.200 --> 00:35:17.160]   Do you know how many time if I would be funny for one day every company did that?
[00:35:17.160 --> 00:35:23.160]   Because you would literally get thousands of pings every day.
[00:35:23.160 --> 00:35:24.160]   Wouldn't you?
[00:35:24.160 --> 00:35:25.160]   You would.
[00:35:25.160 --> 00:35:26.160]   This goes on every day.
[00:35:26.160 --> 00:35:31.720]   You know, talking about Facebook shadow profiles, that seems diminumous compared to the simple
[00:35:31.720 --> 00:35:35.920]   leak of data that happens every time you give your contact list to any company.
[00:35:35.920 --> 00:35:39.200]   This came up with a camera when they really Facebook had that tool for have you been caught
[00:35:39.200 --> 00:35:40.200]   up in the face came a jungle.
[00:35:40.200 --> 00:35:41.200]   It's a good thing.
[00:35:41.200 --> 00:35:44.360]   And if one person found out there had been, there was an immediate online witch hunt to
[00:35:44.360 --> 00:35:47.360]   find out who'd done the quiz and dumped them in it.
[00:35:47.360 --> 00:35:51.400]   And I think just having a simple, you know, if you had a little simple button that said,
[00:35:51.400 --> 00:35:54.640]   do you or do you not wish your friends to be able to access this?
[00:35:54.640 --> 00:35:57.520]   And of course, Facebook isn't going to do that because it cripples their business model.
[00:35:57.520 --> 00:35:58.520]   So.
[00:35:58.520 --> 00:36:00.720]   I don't have any choice about whose address book I'm in.
[00:36:00.720 --> 00:36:01.720]   Let me ask.
[00:36:01.720 --> 00:36:05.200]   Somebody in the chatroom says, well, that's why I never give off for my address book.
[00:36:05.200 --> 00:36:07.520]   I bet you're one in a thousand.
[00:36:07.520 --> 00:36:09.520]   How many I do it?
[00:36:09.520 --> 00:36:14.320]   How many of you, when this says, can we have your contacts say, yeah, of course you do.
[00:36:14.320 --> 00:36:19.000]   I mean, very few, you'd have to be one of those tin foil, Steve Gibson types.
[00:36:19.000 --> 00:36:23.680]   And I bet even he, I bet even he does that.
[00:36:23.680 --> 00:36:26.680]   And so we don't, you know, so we don't pay that much attention to that.
[00:36:26.680 --> 00:36:28.920]   And that's talking about shadow profiles.
[00:36:28.920 --> 00:36:30.360]   There's a lot of information about me.
[00:36:30.360 --> 00:36:31.360]   Oh, yeah.
[00:36:31.360 --> 00:36:37.480]   Contact book, contact list, credit to Debbie Dingle of Michigan at the end.
[00:36:37.480 --> 00:36:43.400]   And by the way, to kind of underscore your point, this got very little coverage, right?
[00:36:43.400 --> 00:36:47.080]   We saw Mark saying, I'll ask my team about this blah, blah, blah.
[00:36:47.080 --> 00:36:49.800]   She kind of reamed Mark.
[00:36:49.800 --> 00:36:52.280]   She said, as a CEO, you don't know some key facts.
[00:36:52.280 --> 00:36:54.320]   You don't know what a shadow profile was.
[00:36:54.320 --> 00:36:56.320]   You don't know how many apps you need to audit.
[00:36:56.320 --> 00:36:59.920]   You did not know how many other firms have been sold data by Cambridge Analytica.
[00:36:59.920 --> 00:37:03.800]   You don't even know all the different kinds of information Facebook is collecting from
[00:37:03.800 --> 00:37:04.800]   its users.
[00:37:04.800 --> 00:37:07.360]   Here's what I do know, says Representative Dingle.
[00:37:07.360 --> 00:37:11.040]   You have trackers all over the web on practically every website.
[00:37:11.040 --> 00:37:17.400]   We all see the Facebook like or share buttons and with the Facebook pixel, a hidden knot.
[00:37:17.400 --> 00:37:19.080]   People may not even see that Facebook logo.
[00:37:19.080 --> 00:37:21.960]   It doesn't matter whether you have a Facebook account through these tools.
[00:37:21.960 --> 00:37:23.960]   Facebook is able to collect information from all of us.
[00:37:23.960 --> 00:37:24.960]   That's well said.
[00:37:24.960 --> 00:37:27.120]   I don't think it got picked up by anybody.
[00:37:27.120 --> 00:37:32.480]   No, I mean, we mentioned it in our coverage of it, but it was just that was just focusing
[00:37:32.480 --> 00:37:33.480]   on it.
[00:37:33.480 --> 00:37:36.760]   It was actually very easy to just to focus on the tech savvy members of Congress because
[00:37:36.760 --> 00:37:38.720]   there are so few of them.
[00:37:38.720 --> 00:37:40.120]   It's just like, oh, right.
[00:37:40.120 --> 00:37:42.600]   Okay, he deserves some pat on the back for that one.
[00:37:42.600 --> 00:37:43.840]   Let's take a little break when we come back.
[00:37:43.840 --> 00:37:50.360]   I want to talk about these suitably dystopian named Amazon Recognition with a K. It sounds
[00:37:50.360 --> 00:37:52.080]   like something the Stasi would use.
[00:37:52.080 --> 00:37:53.280]   How do I call him?
[00:37:53.280 --> 00:37:54.280]   A million miles away.
[00:37:54.280 --> 00:37:55.280]   A half recognition.
[00:37:55.280 --> 00:37:57.480]   We'll talk about that in just a second.
[00:37:57.480 --> 00:37:58.480]   Brian Ford is here.
[00:37:58.480 --> 00:38:03.000]   He's a candidate for Congress in the 45th of California.
[00:38:03.000 --> 00:38:10.080]   You should go to his website, f o r d e dot com and read all about his positions.
[00:38:10.080 --> 00:38:14.240]   And if you're in Orange County, maybe you should think about going to the polls on June
[00:38:14.240 --> 00:38:15.240]   5th.
[00:38:15.240 --> 00:38:16.240]   What about June?
[00:38:16.240 --> 00:38:17.240]   What a thought.
[00:38:17.240 --> 00:38:21.720]   I've got my ballot sitting sitting right there at my mail in ballot sitting right there.
[00:38:21.720 --> 00:38:27.240]   I wish I could vote in the 45th, but everybody, I think the good thing about that.
[00:38:27.240 --> 00:38:28.880]   I'll give you my endorsement, Brian.
[00:38:28.880 --> 00:38:34.600]   I think everybody, I think this particular primary and then the elections in November,
[00:38:34.600 --> 00:38:37.120]   everybody's going to have a chance, almost everybody's going to have a chance to make
[00:38:37.120 --> 00:38:38.560]   a difference and make a change.
[00:38:38.560 --> 00:38:40.720]   This is going to be a big one.
[00:38:40.720 --> 00:38:41.720]   And as technologists.
[00:38:41.720 --> 00:38:44.120]   You just joined Jimmy Wells and Tim O'Reilly.
[00:38:44.120 --> 00:38:45.120]   Yeah.
[00:38:45.120 --> 00:38:47.840]   That's a good couple of people to join.
[00:38:47.840 --> 00:38:48.840]   Friends of both.
[00:38:48.840 --> 00:38:56.040]   I think that this is an opportunity for geeks who traditionally kind of, oh, government
[00:38:56.040 --> 00:38:57.200]   doesn't do anything.
[00:38:57.200 --> 00:38:58.200]   I wash their hands.
[00:38:58.200 --> 00:39:00.360]   I mean, you see opposite, don't you, Madeline?
[00:39:00.360 --> 00:39:01.640]   You see the people say, I want to dig in.
[00:39:01.640 --> 00:39:03.440]   I want to roll up my sleeves and make a difference.
[00:39:03.440 --> 00:39:04.440]   Yeah.
[00:39:04.440 --> 00:39:07.520]   And you need that kind of thing because if people think they can just unplug and not pay
[00:39:07.520 --> 00:39:10.680]   attention to what's happening with government, guess what?
[00:39:10.680 --> 00:39:11.680]   Government's going to affect you.
[00:39:11.680 --> 00:39:14.320]   So you need to be in the discussion in the conversation.
[00:39:14.320 --> 00:39:15.320]   Yeah.
[00:39:15.320 --> 00:39:16.320]   Who would have thought met neutrons?
[00:39:16.320 --> 00:39:17.320]   Who would be an election issue?
[00:39:17.320 --> 00:39:18.320]   But it's going to be in the...
[00:39:18.320 --> 00:39:19.320]   I hope so.
[00:39:19.320 --> 00:39:20.320]   It's, you know.
[00:39:20.320 --> 00:39:21.320]   Boy, I hope so.
[00:39:21.320 --> 00:39:22.320]   That would be great news.
[00:39:22.320 --> 00:39:26.920]   Our show today brought to you by, oh, that's Ian Thompson from the Register of That Cuts
[00:39:26.920 --> 00:39:28.840]   from the United States Digital Service.
[00:39:28.840 --> 00:39:31.520]   I gave Brian a plug, but I got to give you guys a plug too.
[00:39:31.520 --> 00:39:33.160]   It really is great to have you.
[00:39:33.160 --> 00:39:38.920]   Our show today brought to you by Betterment, the largest independent online financial advisor.
[00:39:38.920 --> 00:39:41.200]   And I want to tell you, I'm strictly legal.
[00:39:41.200 --> 00:39:43.960]   SEC regulations prohibit me from being a betterment customer.
[00:39:43.960 --> 00:39:44.960]   Isn't that weird?
[00:39:44.960 --> 00:39:47.760]   I can't do an ad for Betterment if I'm a customer of Betterment.
[00:39:47.760 --> 00:39:48.760]   Okay.
[00:39:48.760 --> 00:39:49.760]   Yeah.
[00:39:49.760 --> 00:39:50.760]   That's an SEC rule.
[00:39:50.760 --> 00:39:51.760]   Suitable bizare.
[00:39:51.760 --> 00:39:53.040]   This is a great way to invest.
[00:39:53.040 --> 00:39:54.040]   You know you need to save.
[00:39:54.040 --> 00:39:56.800]   You know you need to save.
[00:39:56.800 --> 00:40:02.360]   You probably, as a geek, probably think, oh, I can do this all by myself.
[00:40:02.360 --> 00:40:05.360]   If you've been doing this for any length of time, you kind of know you can't because
[00:40:05.360 --> 00:40:07.600]   who has time to pay attention?
[00:40:07.600 --> 00:40:11.360]   Even if you, and I was one of these people, I thought, every quarter I'm going to go into
[00:40:11.360 --> 00:40:12.360]   my portfolio.
[00:40:12.360 --> 00:40:13.360]   I'm going to rebalance.
[00:40:13.360 --> 00:40:14.360]   That's very important.
[00:40:14.360 --> 00:40:15.360]   I'm going to pay attention.
[00:40:15.360 --> 00:40:18.880]   I'm going to see what I've invested in, make sure I'm not over-invested in bonds or equities
[00:40:18.880 --> 00:40:20.640]   or whatever.
[00:40:20.640 --> 00:40:21.640]   That's every quarter.
[00:40:21.640 --> 00:40:22.640]   That's every three months.
[00:40:22.640 --> 00:40:24.880]   You can do something a lot better with Betterment.
[00:40:24.880 --> 00:40:32.040]   It's using technology to rebalance, to reinvest minute by minute if you need to.
[00:40:32.040 --> 00:40:35.280]   You know, when the stock market goes down, which it hasn't been for a while, but now
[00:40:35.280 --> 00:40:38.840]   that it's a little more volatile, Betterment can take advantage of something called tax
[00:40:38.840 --> 00:40:42.800]   loss harvesting and actually improve your returns by taking advantage of losses.
[00:40:42.800 --> 00:40:46.240]   It's complicated, but that's why you want Betterment to do it.
[00:40:46.240 --> 00:40:50.520]   By taking complex investing strategies, using technology to make them more efficient.
[00:40:50.520 --> 00:40:56.120]   And then when you need it, providing access to unlimited personalized advice from licensed
[00:40:56.120 --> 00:40:58.200]   experts who are fiduciaries, that's important.
[00:40:58.200 --> 00:40:59.920]   That's a big word, but it's important.
[00:40:59.920 --> 00:41:01.320]   It's somebody working on your behalf.
[00:41:01.320 --> 00:41:02.920]   They don't get commissions.
[00:41:02.920 --> 00:41:07.440]   They're not incentivized to recommend any particular fund.
[00:41:07.440 --> 00:41:08.840]   They're not selling their own products.
[00:41:08.840 --> 00:41:13.600]   And boy, I think a lot of places people go for financial advice, you're getting that
[00:41:13.600 --> 00:41:14.760]   kind of biased advice.
[00:41:14.760 --> 00:41:17.200]   No, Betterment, they're fiduciaries.
[00:41:17.200 --> 00:41:19.200]   They work for you.
[00:41:19.200 --> 00:41:23.520]   And by the way, and I love this, there are no hidden costs at Betterment.
[00:41:23.520 --> 00:41:28.600]   You pay a low flat rate and it varies depending on what kind of account you want.
[00:41:28.600 --> 00:41:38.200]   But as low as 25 basis points, 0.25% of your investment per year, and you get great information,
[00:41:38.200 --> 00:41:42.120]   no matter who you are, no matter how much money you invest, you're going to get everything
[00:41:42.120 --> 00:41:46.400]   for that low transparent management fee with no, there's no transaction costs.
[00:41:46.400 --> 00:41:52.040]   And that means, by the way, that they can act on your behalf often, even during one day,
[00:41:52.040 --> 00:41:54.320]   and there's no additional cost to you, which is fantastic.
[00:41:54.320 --> 00:41:58.360]   Plus you get personalized advice, you get the suite of tools, the Betterment app is fantastic.
[00:41:58.360 --> 00:41:59.640]   You'll know how you're doing.
[00:41:59.640 --> 00:42:04.780]   You can get other investments into the app so you get a total look at your portfolio.
[00:42:04.780 --> 00:42:05.780]   It is awesome.
[00:42:05.780 --> 00:42:12.400]   Now, I have to say this, investment involves risk, but not investing also involves risk.
[00:42:12.400 --> 00:42:20.920]   You can get up to your managed free, not even 0.25% free if you go to betterment.com/twit.
[00:42:20.920 --> 00:42:23.520]   Betterment.com/twit.
[00:42:23.520 --> 00:42:26.880]   Betterment.com/twit.
[00:42:26.880 --> 00:42:30.800]   Yeah, I thought I'd do my mama favor and take over her investments.
[00:42:30.800 --> 00:42:31.800]   That was a mistake.
[00:42:31.800 --> 00:42:32.800]   Ooh, yeah.
[00:42:32.800 --> 00:42:33.800]   That was a mistake.
[00:42:33.800 --> 00:42:35.280]   So I'm being textbook for my mama.
[00:42:35.280 --> 00:42:38.640]   Oh, now I know why they charge so much, right?
[00:42:38.640 --> 00:42:40.920]   Well, now the market's gone down.
[00:42:40.920 --> 00:42:41.920]   Could I be?
[00:42:41.920 --> 00:42:42.920]   Oh my God, mom.
[00:42:42.920 --> 00:42:44.200]   No, you should just sit still.
[00:42:44.200 --> 00:42:45.200]   Don't sleep.
[00:42:45.200 --> 00:42:46.200]   Sleep.
[00:42:46.200 --> 00:42:47.200]   Go to sleep, mom.
[00:42:47.200 --> 00:42:48.200]   Don't.
[00:42:48.200 --> 00:42:49.200]   I love you, mom.
[00:42:49.200 --> 00:42:50.840]   But Betterment is better that she should call them.
[00:42:50.840 --> 00:42:53.040]   Better she should call them.
[00:42:53.040 --> 00:42:57.120]   Let's talk about Recognition, Amazon getting a little heat because apparently they're selling
[00:42:57.120 --> 00:43:01.000]   this face recognition to law enforcement.
[00:43:01.000 --> 00:43:05.800]   And now I think this is a really interesting topic for us because it is useful for law
[00:43:05.800 --> 00:43:06.800]   enforcement.
[00:43:06.800 --> 00:43:12.360]   The Oregon, Orlando, a police department, right, inside Portland, I think, right?
[00:43:12.360 --> 00:43:13.640]   They used it.
[00:43:13.640 --> 00:43:19.360]   What they did is they put all their mugshots in, mugshot database in, and they were able
[00:43:19.360 --> 00:43:23.040]   to immediately, this is a great article in New York Times immediately arrest somebody
[00:43:23.040 --> 00:43:25.560]   because it matched in the face recognition.
[00:43:25.560 --> 00:43:27.240]   They were able to arrest somebody.
[00:43:27.240 --> 00:43:28.880]   They're touting this as a great success.
[00:43:28.880 --> 00:43:30.720]   It's a cheap thing for them to do.
[00:43:30.720 --> 00:43:34.280]   They pay, I think it's a few hundred dollars set up fee and then it's a very inexpensive
[00:43:34.280 --> 00:43:35.920]   monthly fee.
[00:43:35.920 --> 00:43:38.520]   This is a great service Amazon is offering.
[00:43:38.520 --> 00:43:40.560]   Yes, far bit for me.
[00:43:40.560 --> 00:43:42.960]   I'm sorry, we've tried this in the UK.
[00:43:42.960 --> 00:43:44.400]   We've just had the results announced.
[00:43:44.400 --> 00:43:45.840]   South Wales police did it.
[00:43:45.840 --> 00:43:49.160]   Some festivals, 98% failure rate.
[00:43:49.160 --> 00:43:50.160]   Well, yes.
[00:43:50.160 --> 00:43:55.360]   And if you look at the stats that Amazon is pushing out, they're saying an 80% chance
[00:43:55.360 --> 00:43:59.560]   of identifying the person out of a group of 50 mugshots.
[00:43:59.560 --> 00:44:03.360]   So the use case for this still isn't proven yet.
[00:44:03.360 --> 00:44:06.800]   And I do worry that you're getting a lot of tech firms at Amazon who's one is only one
[00:44:06.800 --> 00:44:11.280]   of them trying to push this technology at police where it produces far more false positives
[00:44:11.280 --> 00:44:14.120]   than it should and it's actively going to put people away from it.
[00:44:14.120 --> 00:44:19.400]   Now, if you want to see how this has really been doing run, being done right, go to China.
[00:44:19.400 --> 00:44:23.920]   As China has made enormous use of this, they're very high-end systems.
[00:44:23.920 --> 00:44:26.880]   And yet, it's letting them pull people off the street based on the photo on the image
[00:44:26.880 --> 00:44:27.880]   of a photo.
[00:44:27.880 --> 00:44:30.720]   Or denying them the ability to get a loan or buy trains.
[00:44:30.720 --> 00:44:32.240]   Or, yeah, exactly.
[00:44:32.240 --> 00:44:34.840]   Because they have this social credit system.
[00:44:34.840 --> 00:44:36.920]   I'm sure Elon Musk could approve it.
[00:44:36.920 --> 00:44:39.440]   I think there are benefits to this.
[00:44:39.440 --> 00:44:41.000]   There's other issues.
[00:44:41.000 --> 00:44:45.680]   Brian, like the software that you mentioned that the prisons are using, one of the things
[00:44:45.680 --> 00:44:49.040]   recognition does not do well is people of color.
[00:44:49.040 --> 00:44:53.760]   It has a much higher failure rate with people of color.
[00:44:53.760 --> 00:44:56.360]   They all look alike to recognition.
[00:44:56.360 --> 00:44:57.360]   That's a problem.
[00:44:57.360 --> 00:44:58.360]   Yes.
[00:44:58.360 --> 00:45:03.360]   I mean, as if people of color didn't have enough problems with being falsely arrested
[00:45:03.360 --> 00:45:04.640]   in this country.
[00:45:04.640 --> 00:45:06.560]   But I mean, this is the problem.
[00:45:06.560 --> 00:45:08.120]   It's the false positive rate.
[00:45:08.120 --> 00:45:12.640]   It's kind of like when the antivirus industry first kicked off.
[00:45:12.640 --> 00:45:16.680]   People were turning their AV off because the false positive rate was so high and these
[00:45:16.680 --> 00:45:17.680]   are the warning windows.
[00:45:17.680 --> 00:45:20.240]   And I fear it's going to be the same with this.
[00:45:20.240 --> 00:45:24.880]   I mean, okay, a false positive rate of 50% doesn't sound that bad until you're the person
[00:45:24.880 --> 00:45:26.680]   pulled out of the strip search.
[00:45:26.680 --> 00:45:30.360]   But when they start putting on the shoulder marigold, then you'd really like that computer
[00:45:30.360 --> 00:45:31.560]   system to be accurate.
[00:45:31.560 --> 00:45:35.760]   It's ironic because in the New York Times article talking about this and the problems
[00:45:35.760 --> 00:45:39.360]   with it, they acknowledged that they used it during the royal wedding because they couldn't
[00:45:39.360 --> 00:45:42.520]   figure out which royals were rich.
[00:45:42.520 --> 00:45:44.400]   They all look the same, you know?
[00:45:44.400 --> 00:45:45.880]   Well, yes.
[00:45:45.880 --> 00:45:47.120]   God bless the happy couple there.
[00:45:47.120 --> 00:45:50.600]   But you're some much needed genetic diversity to your royal family.
[00:45:50.600 --> 00:45:51.600]   Oh, hi-oh.
[00:45:51.600 --> 00:45:56.520]   Yeah, the next generation might actually have chins.
[00:45:56.520 --> 00:46:01.640]   But darker skin women are mis-site, according to MIT, misidentified up to 35% of the time
[00:46:01.640 --> 00:46:07.360]   by fish, not by Amazon specifically, but by generally by facial recognition software.
[00:46:07.360 --> 00:46:08.560]   But there are some success stories.
[00:46:08.560 --> 00:46:16.520]   In fact, the New York Times article talks about Madison Square Garden using it.
[00:46:16.520 --> 00:46:22.120]   They have cameras when you enter Madison Square Garden and you go through the metal detector.
[00:46:22.120 --> 00:46:24.160]   There's a big camera on it.
[00:46:24.160 --> 00:46:29.680]   And it's looking at you and matching you up to a database of known troublemakers so they
[00:46:29.680 --> 00:46:32.040]   can isolate people wholegans.
[00:46:32.040 --> 00:46:33.040]   You know about wholegans.
[00:46:33.040 --> 00:46:34.040]   Where the wholegans.
[00:46:34.040 --> 00:46:35.040]   Wholegans.
[00:46:35.040 --> 00:46:36.160]   And keep them out of the concert.
[00:46:36.160 --> 00:46:39.720]   I'm not sure I'm against that, but that's a little different because that's a use by
[00:46:39.720 --> 00:46:44.000]   a private company in a private setting where the right to pass is really controlled by
[00:46:44.000 --> 00:46:45.000]   that company.
[00:46:45.000 --> 00:46:46.000]   It's not used by a government.
[00:46:46.000 --> 00:46:50.400]   Yeah, I mean, if it works, fine, but sorry, Brian.
[00:46:50.400 --> 00:46:54.120]   The question we're going to have to ask here is my first question Amazon would be
[00:46:54.120 --> 00:47:01.320]   are you as a private company willing to be openly accessible through the Freedom of Information
[00:47:01.320 --> 00:47:02.320]   Act?
[00:47:02.320 --> 00:47:05.160]   Or are you going to consider your technology proprietary?
[00:47:05.160 --> 00:47:06.160]   That's interesting.
[00:47:06.160 --> 00:47:11.920]   Because this is the challenge that ProPublica had in figuring out whether the software that
[00:47:11.920 --> 00:47:18.760]   was being used by parole boards was actually systematically discriminating against someone.
[00:47:18.760 --> 00:47:21.960]   It's a private company that was intellectual property and it was very hard for them to
[00:47:21.960 --> 00:47:22.960]   get access.
[00:47:22.960 --> 00:47:28.160]   And this is the difference between private companies like Google and Facebook using software.
[00:47:28.160 --> 00:47:31.200]   And I think it was Microsoft who put out a Twitbot or something like that.
[00:47:31.200 --> 00:47:34.320]   And within 24 hours it became a racist engine.
[00:47:34.320 --> 00:47:36.680]   And that has a memory tag.
[00:47:36.680 --> 00:47:37.680]   It's a day.
[00:47:37.680 --> 00:47:40.280]   They didn't know to see that coming from the moon.
[00:47:40.280 --> 00:47:44.600]   And so the difference between the government and a private company is that a government
[00:47:44.600 --> 00:47:46.120]   can garnish your wages.
[00:47:46.120 --> 00:47:48.240]   A government can put you in jail.
[00:47:48.240 --> 00:47:50.800]   Google and Facebook can do neither of those things.
[00:47:50.800 --> 00:47:56.200]   And so because of that, the government has to be responsible to the people.
[00:47:56.200 --> 00:47:59.280]   And that's why we have things like the Freedom of Information Act to do that.
[00:47:59.280 --> 00:48:02.600]   But private companies aren't subjected to that.
[00:48:02.600 --> 00:48:08.600]   And so it's great to see the ACLU standing up here because really this is our 21st century
[00:48:08.600 --> 00:48:09.600]   civil rights.
[00:48:09.600 --> 00:48:14.720]   Well, and it's also interesting to think about corner cases where it's not just people.
[00:48:14.720 --> 00:48:17.200]   It's things like license plate recognition.
[00:48:17.200 --> 00:48:21.560]   Because on one hand, you've got the privacy to drive around everywhere.
[00:48:21.560 --> 00:48:27.320]   But if there's cameras at every large main street intersection that can do license plate
[00:48:27.320 --> 00:48:31.120]   recognition, then you can back that out and figure out where people are going.
[00:48:31.120 --> 00:48:35.600]   And then the really tricky question turns into, okay, some of this stuff is commercial
[00:48:35.600 --> 00:48:38.120]   service, but some of this is open source.
[00:48:38.120 --> 00:48:42.480]   And so if this stuff is available as open source, should you say, okay, a company can
[00:48:42.480 --> 00:48:44.560]   use it, but then a government can't use it.
[00:48:44.560 --> 00:48:47.760]   And under what conditions should you restrict the use of that?
[00:48:47.760 --> 00:48:51.400]   Because you can imagine that a private company doing it and then licensing or selling those
[00:48:51.400 --> 00:48:52.480]   results to the government.
[00:48:52.480 --> 00:48:57.480]   Well, you were still at Google, I remember when they announced that they had face recognition
[00:48:57.480 --> 00:48:59.320]   in Google photos, I think it was.
[00:48:59.320 --> 00:49:04.160]   And they said it, but we're not going to let people use it to identify people on the
[00:49:04.160 --> 00:49:08.440]   street because of the very real threat of stalking and so forth.
[00:49:08.440 --> 00:49:11.720]   That's the right way to approach this is to think about the ethical implications of what
[00:49:11.720 --> 00:49:16.400]   you're doing and try to limit the bad use cases, I would think.
[00:49:16.400 --> 00:49:19.600]   It helps, but you can never foresee all the different corner cases.
[00:49:19.600 --> 00:49:23.400]   Like somebody thinks they've anonymized data and it turns out somebody comes out with
[00:49:23.400 --> 00:49:27.320]   a completely new approach that people haven't thought of and they de-anonymize it.
[00:49:27.320 --> 00:49:30.040]   And so it's a really tricky thing.
[00:49:30.040 --> 00:49:34.120]   And we might just have to say, you know, in 10 years, okay, we're living in a William Gibson
[00:49:34.120 --> 00:49:35.640]   novel and I think you know.
[00:49:35.640 --> 00:49:36.640]   I think you are already.
[00:49:36.640 --> 00:49:37.640]   I hate to say it.
[00:49:37.640 --> 00:49:39.080]   By the way, Google does have jails.
[00:49:39.080 --> 00:49:40.920]   You know that.
[00:49:40.920 --> 00:49:41.920]   It's only for your search results.
[00:49:41.920 --> 00:49:48.040]   Not for years was the guy in charge of search and keeping spam out of search.
[00:49:48.040 --> 00:49:51.040]   I heard that phrase and I was like, well, we have jails.
[00:49:51.040 --> 00:49:52.600]   It's a different kind of jail.
[00:49:52.600 --> 00:49:54.240]   But you don't want to get in it.
[00:49:54.240 --> 00:49:55.240]   That's true.
[00:49:55.240 --> 00:50:01.440]   You know, if I apply for a credit card and I get denied because of something on my credit
[00:50:01.440 --> 00:50:03.520]   report, then I have a right.
[00:50:03.520 --> 00:50:08.000]   And that right is to get access to that credit report and see if there are any errors on
[00:50:08.000 --> 00:50:09.000]   it.
[00:50:09.000 --> 00:50:13.520]   If there are, I have the right to go back to the bank and remove those errors.
[00:50:13.520 --> 00:50:17.360]   And then I have the right to go back and apply for that credit and get access to the credit.
[00:50:17.360 --> 00:50:23.280]   So my question then is, what rights do we have with private sector companies using facial
[00:50:23.280 --> 00:50:25.120]   recognition with the government?
[00:50:25.120 --> 00:50:29.360]   Or what rights do we have with artificial intelligence when it's used to make so many
[00:50:29.360 --> 00:50:32.360]   decisions that impact our personal lives?
[00:50:32.360 --> 00:50:37.400]   Well, and if you, to take Brian's point, I think the stuff that they do for credit, the
[00:50:37.400 --> 00:50:42.320]   fair credit reporting act is pretty good because you're actually able to say, okay, what information
[00:50:42.320 --> 00:50:43.320]   do you have?
[00:50:43.320 --> 00:50:47.040]   And if something's inaccurate or wrong, then you can say, okay, this has to be corrected.
[00:50:47.040 --> 00:50:51.240]   I think the tricky bit is when you take that a step further, whenever you have a transaction
[00:50:51.240 --> 00:50:53.920]   and there's something on someone on both sides of the transaction.
[00:50:53.920 --> 00:50:57.120]   So you buy diapers from the grocery store.
[00:50:57.120 --> 00:51:00.720]   Yes, you have bought those diapers, but then the grocery store also knows that you've bought
[00:51:00.720 --> 00:51:01.720]   those diapers.
[00:51:01.720 --> 00:51:05.520]   And so at what point do you say, okay, pizza hut, you're not allowed to sell information
[00:51:05.520 --> 00:51:11.360]   to other people, or maybe you make that a promise or a brand where you can say, okay,
[00:51:11.360 --> 00:51:16.080]   I will only use the companies that agree to like these higher ethical standards that say
[00:51:16.080 --> 00:51:19.960]   they're not going to sell the information to third parties, for example.
[00:51:19.960 --> 00:51:23.280]   Is this the proper role for government?
[00:51:23.280 --> 00:51:24.280]   European send me things, sir.
[00:51:24.280 --> 00:51:27.760]   Yeah, we're going to talk about GAR a little later on.
[00:51:27.760 --> 00:51:31.880]   This is the week you got more privacy notices in your mailbox than the rest of your entire
[00:51:31.880 --> 00:51:32.880]   life.
[00:51:32.880 --> 00:51:36.320]   And I was like, oh, we're breaking up.
[00:51:36.320 --> 00:51:38.320]   It's like, yes.
[00:51:38.320 --> 00:51:40.520]   We'll get to that in a second.
[00:51:40.520 --> 00:51:42.920]   But no, I mean, it feels like, so we have this debate a lot.
[00:51:42.920 --> 00:51:45.000]   Should this be something that laws?
[00:51:45.000 --> 00:51:46.640]   Is this something laws could take care of?
[00:51:46.640 --> 00:51:49.240]   Is this something that a focus on ethics?
[00:51:49.240 --> 00:51:54.040]   I know Microsoft this year, a bill to call for such an oil call for a kind of a focus
[00:51:54.040 --> 00:51:58.640]   on ethics among computer scientists to really think of the consequences of their actions?
[00:51:58.640 --> 00:52:01.400]   Is it something Jeff Jarvis has always said, you know, the best way to do this is with
[00:52:01.400 --> 00:52:05.520]   social mores with just the kind of the standard in society.
[00:52:05.520 --> 00:52:08.440]   You don't do this or you don't do that, that that would be wrong.
[00:52:08.440 --> 00:52:11.520]   And then there's some significant social pressure to behave.
[00:52:11.520 --> 00:52:12.520]   All of those work.
[00:52:12.520 --> 00:52:13.520]   Yeah.
[00:52:13.520 --> 00:52:16.720]   I mean, we've already seen, I think it was last week with the resignations from some
[00:52:16.720 --> 00:52:19.600]   of Google's IIT because they didn't want to be working with the military.
[00:52:19.600 --> 00:52:20.600]   300.
[00:52:20.600 --> 00:52:21.600]   Yeah.
[00:52:21.600 --> 00:52:23.800]   But I mean, I thought it was more like a dozen.
[00:52:23.800 --> 00:52:25.800]   Yeah, I thought it was close to a dozen.
[00:52:25.800 --> 00:52:26.800]   Oh, maybe.
[00:52:26.800 --> 00:52:27.800]   Okay.
[00:52:27.800 --> 00:52:28.800]   So.
[00:52:28.800 --> 00:52:30.920]   But people are willing to put their money with their mouth is regarding what companies
[00:52:30.920 --> 00:52:31.920]   do.
[00:52:31.920 --> 00:52:35.720]   But the situation at the moment is so parless we've seen with the secure SISP stuff that
[00:52:35.720 --> 00:52:38.840]   was coming out, you know, companies are perfect.
[00:52:38.840 --> 00:52:43.920]   It's perfectly easy for law enforcement to buy your location data from private companies.
[00:52:43.920 --> 00:52:48.240]   You know, and this is something I do think the government needs to step in and say, right,
[00:52:48.240 --> 00:52:52.600]   if you're going to do that, you've got to abide by certain data protection standards.
[00:52:52.600 --> 00:52:57.320]   And there's got to be a certain level of civil civil liberal civil liberties stuff built
[00:52:57.320 --> 00:52:58.320]   in as well.
[00:52:58.320 --> 00:52:59.320]   Yeah.
[00:52:59.320 --> 00:53:03.720]   So, I'm telling my inner Jeff Jarvis, he would say, okay, there's all the scary stuff that
[00:53:03.720 --> 00:53:04.720]   could happen.
[00:53:04.720 --> 00:53:07.240]   But what is the actual harm and what is the impact of the action?
[00:53:07.240 --> 00:53:08.680]   I like to ask that too.
[00:53:08.680 --> 00:53:10.560]   I think that's an important.
[00:53:10.560 --> 00:53:15.640]   I think when you kind of when you look back at the spam, for example, that was kind of
[00:53:15.640 --> 00:53:21.280]   a two prong approach where you had the Cairns Fam Act, which governed what private sector
[00:53:21.280 --> 00:53:25.320]   could and could not do as it relates to email and put teeth and put teeth in it, which is
[00:53:25.320 --> 00:53:26.320]   important.
[00:53:26.320 --> 00:53:30.320]   But it didn't protect against like the Nigerian Prince scam.
[00:53:30.320 --> 00:53:35.240]   And that's where companies competed using innovation to have the best spam filters that
[00:53:35.240 --> 00:53:37.440]   had the least amount of false positives.
[00:53:37.440 --> 00:53:42.200]   And so you had this two prong approach where you set the rules of the road for the legally
[00:53:42.200 --> 00:53:43.360]   abiding private sector.
[00:53:43.360 --> 00:53:47.400]   And then you had, you know, tech companies using innovation to compete in the private
[00:53:47.400 --> 00:53:49.920]   sector to have the best spam filter.
[00:53:49.920 --> 00:53:52.240]   And then the two of those things working together in collaboration.
[00:53:52.240 --> 00:53:55.400]   So I never think it's an either or should government be involved in this or this is
[00:53:55.400 --> 00:54:00.440]   a private sector role, you got to think well, us as a community in the United States does
[00:54:00.440 --> 00:54:04.040]   not believe that, you know, we are our lives should be run by spam.
[00:54:04.040 --> 00:54:06.840]   And so what are things that the private sector can do and where the things that the government
[00:54:06.840 --> 00:54:10.120]   can do in collaboration where they both have domains or both have levers?
[00:54:10.120 --> 00:54:13.400]   Because one of the things that I always learned in the White House is that, you know, the
[00:54:13.400 --> 00:54:16.800]   private sector has a set of levers that the government does not have, the government has
[00:54:16.800 --> 00:54:21.240]   a set of levers that the private sector doesn't have, academia has a set of levers that,
[00:54:21.240 --> 00:54:23.120]   you know, the other two don't, the other two don't have.
[00:54:23.120 --> 00:54:27.480]   And so collectively we have to decide on what are our values and what are our goals, what
[00:54:27.480 --> 00:54:32.200]   does each group have that can help address this problem and had our, are we clear about
[00:54:32.200 --> 00:54:33.200]   that?
[00:54:33.200 --> 00:54:36.040]   And then how do we work towards a collective solution to make society just a little bit
[00:54:36.040 --> 00:54:37.040]   better?
[00:54:37.040 --> 00:54:38.040]   I think that's the right answer.
[00:54:38.040 --> 00:54:42.720]   Well, it's interesting though, if you think about CanSpam, I think it is good and important
[00:54:42.720 --> 00:54:48.400]   to have those roles of the road, but if you think about it, something like Gmail often
[00:54:48.400 --> 00:54:51.640]   succeeded because it was really good at spotting spam and preventing it.
[00:54:51.640 --> 00:54:56.000]   So I think to the degree it's possible, if you can have the things solved in the market
[00:54:56.000 --> 00:55:00.520]   where the companies can innovate on features that people actually care about and make their
[00:55:00.520 --> 00:55:04.960]   lives better, then that's fantastic if you can solve it that way.
[00:55:04.960 --> 00:55:10.400]   But because I always worried about, okay, if you legislate, you know, bad behavior,
[00:55:10.400 --> 00:55:14.840]   the unintended consequences, the corner cases, the things that people didn't write, the laws,
[00:55:14.840 --> 00:55:19.240]   the fact that the technology changes over time and so the laws can become out of date.
[00:55:19.240 --> 00:55:24.720]   So to a first approximation, I always like to say, okay, let's see how far you can get
[00:55:24.720 --> 00:55:29.400]   using, you know, rational actors, hopefully on the private side.
[00:55:29.400 --> 00:55:33.600]   And then if there's really bad folks, then yes, you can see you can have laws, all those
[00:55:33.600 --> 00:55:34.600]   sorts of stuff.
[00:55:34.600 --> 00:55:35.600]   Spoken is a technology.
[00:55:35.600 --> 00:55:36.600]   True technology.
[00:55:36.600 --> 00:55:37.600]   Fair.
[00:55:37.600 --> 00:55:39.360]   Technology is always think there's a technological solution to everything.
[00:55:39.360 --> 00:55:41.240]   Well, I have also seen a lot of bad laws here.
[00:55:41.240 --> 00:55:42.240]   And that's true.
[00:55:42.240 --> 00:55:43.240]   Since I've been in DC.
[00:55:43.240 --> 00:55:46.120]   Government often thinks the solution is always government too, right?
[00:55:46.120 --> 00:55:49.120]   Well, we've seen bad laws and we've seen bad actors in tech.
[00:55:49.120 --> 00:55:54.560]   I think Uber is a classic example of a company who believes they can self-regulate themselves.
[00:55:54.560 --> 00:56:01.320]   And then all of a sudden we consistently see how many times they themselves have chosen
[00:56:01.320 --> 00:56:02.960]   to be a bad actor.
[00:56:02.960 --> 00:56:06.960]   And so this is where you need everyone to work together for the collective good.
[00:56:06.960 --> 00:56:12.360]   And I think this goes towards what Tim O'Reilly has written about, which is algorithmic regulation
[00:56:12.360 --> 00:56:17.240]   where the technology is going.
[00:56:17.240 --> 00:56:18.840]   What is algorithmic?
[00:56:18.840 --> 00:56:20.360]   On the face of it doesn't sound good.
[00:56:20.360 --> 00:56:26.880]   Well, no, so you could pass a regulatory reform bill for banks that's 1,100 pages,
[00:56:26.880 --> 00:56:31.560]   or you could imagine coming up with ways to set thresholds in a smart way that do the
[00:56:31.560 --> 00:56:35.880]   same amount with one or two pages of regulations like the Volcker Rule or something like that.
[00:56:35.880 --> 00:56:36.880]   So that's one aspect.
[00:56:36.880 --> 00:56:39.560]   I don't know if Brian, you want to add any more on algorithm?
[00:56:39.560 --> 00:56:44.480]   Yeah, I think just a more common example would be the speed limit on a highway.
[00:56:44.480 --> 00:56:48.880]   Why does the speed limit exist to reduce the number of deaths on the road?
[00:56:48.880 --> 00:56:52.180]   And so if that's our collective goal, well, when we start to have seat belts, when we
[00:56:52.180 --> 00:56:57.240]   start to have airbags, when we start to have autonomous vehicles, or if there's higher
[00:56:57.240 --> 00:57:00.760]   density or lower density on the road depending on the time of day or the day of the week
[00:57:00.760 --> 00:57:03.080]   or whatever it is, that speed limit should be changed.
[00:57:03.080 --> 00:57:05.320]   It shouldn't be fixed at 65 miles an hour.
[00:57:05.320 --> 00:57:07.000]   That doesn't make sense.
[00:57:07.000 --> 00:57:13.240]   And so as technology changes, as society changes, as things get safer, things should be more
[00:57:13.240 --> 00:57:16.360]   flexible and we should know what the ultimate goal is.
[00:57:16.360 --> 00:57:18.520]   The ultimate goal is to lower the amount of deaths.
[00:57:18.520 --> 00:57:22.880]   The ultimate goal is to lower the amount of spam, the ultimate goal is whatever it is.
[00:57:22.880 --> 00:57:29.040]   And algorithmically, you are shaping what your regulations are applying to whatever challenge
[00:57:29.040 --> 00:57:34.480]   it is based on the current set of circumstances rather than fixed into law that continually
[00:57:34.480 --> 00:57:38.280]   needs to be updated that doesn't take into account any of the changes that are taking
[00:57:38.280 --> 00:57:39.560]   place in our world.
[00:57:39.560 --> 00:57:44.560]   And there's 10 to 15 different emerging technologies that are going to radically change our society,
[00:57:44.560 --> 00:57:47.720]   our economy, our job market, et cetera.
[00:57:47.720 --> 00:57:51.360]   And do you really think that any legislation that the government puts out over the next
[00:57:51.360 --> 00:57:53.760]   four to five years is going to be able to take into account?
[00:57:53.760 --> 00:57:54.760]   It won't.
[00:57:54.760 --> 00:57:55.760]   Yeah.
[00:57:55.760 --> 00:57:58.240]   Tim writes about this in his most recent book, Beyond Voice.
[00:57:58.240 --> 00:57:59.800]   Actually, maybe not his most recent book.
[00:57:59.800 --> 00:58:02.440]   I think he's got a newer one, Beyond Transparency.
[00:58:02.440 --> 00:58:07.720]   But he says the key to this is that government's job, law's job is to set a goal, to set a
[00:58:07.720 --> 00:58:11.880]   policy, to set an idea, to say this is what we're trying to get, and that algorithms
[00:58:11.880 --> 00:58:17.880]   are regulations then that algorithms generate regulations that implement that goal, but those
[00:58:17.880 --> 00:58:22.400]   are more flexible and can change over time.
[00:58:22.400 --> 00:58:23.680]   I like that idea.
[00:58:23.680 --> 00:58:28.200]   Again, it sounds like a technologist's solution, but I like that idea if we could implement
[00:58:28.200 --> 00:58:29.200]   that.
[00:58:29.200 --> 00:58:31.040]   Well, that's fascinating.
[00:58:31.040 --> 00:58:35.480]   We've seen an interesting case of success with the digital service where we worked with
[00:58:35.480 --> 00:58:41.120]   CMS, the Center for Medicare and Medicaid Services, and they had to issue some new regulations
[00:58:41.120 --> 00:58:42.600]   as a result of a law.
[00:58:42.600 --> 00:58:47.640]   And so normally you would think for a year, put it out, have a public comment period,
[00:58:47.640 --> 00:58:49.800]   make some changes, and then you're done.
[00:58:49.800 --> 00:58:54.120]   And what the team there did along with the US digital service was take about one-fifth
[00:58:54.120 --> 00:58:59.560]   of the time, release the draft regulations, and then mock up how that would affect doctors
[00:58:59.560 --> 00:59:01.440]   and do a very fast cycle to iterate.
[00:59:01.440 --> 00:59:03.920]   And they actually were able to iterate five times.
[00:59:03.920 --> 00:59:10.520]   And so by the time you issue the final regulations, it's much closer to what real people and
[00:59:10.520 --> 00:59:14.720]   real physicians and caregivers are actually going to need whenever they...
[00:59:14.720 --> 00:59:17.360]   Brian, is this on your platform?
[00:59:17.360 --> 00:59:20.360]   Well, I think this is just consistent with evidence-based policymaking.
[00:59:20.360 --> 00:59:21.360]   Right.
[00:59:21.360 --> 00:59:22.360]   That's what we need to get to.
[00:59:22.360 --> 00:59:23.360]   Right.
[00:59:23.360 --> 00:59:26.760]   Yeah, Tim, actually, this is a great chapter, chapter 22 and beyond transparency.
[00:59:26.760 --> 00:59:28.680]   Tim's put it online, but it's interesting.
[00:59:28.680 --> 00:59:34.040]   He talks about actually reputation systems and how those could be valuable in guiding
[00:59:34.040 --> 00:59:35.040]   regulations.
[00:59:35.040 --> 00:59:38.200]   Yes, of course Elon Musk throwing his Twitter toys.
[00:59:38.200 --> 00:59:39.200]   There's a problem.
[00:59:39.200 --> 00:59:42.000]   He heard his reputation, I think.
[00:59:42.000 --> 00:59:44.320]   He talks about the fact that you need to be able to measure.
[00:59:44.320 --> 00:59:46.080]   This goes back to your iteration.
[00:59:46.080 --> 00:59:48.520]   You measure results and then you can iterate.
[00:59:48.520 --> 00:59:50.000]   I think this is really interesting.
[00:59:50.000 --> 00:59:51.400]   I wasn't familiar with this.
[00:59:51.400 --> 00:59:52.400]   I'm going to...
[00:59:52.400 --> 00:59:54.200]   We'll get Tim on to talk about it.
[00:59:54.200 --> 00:59:56.840]   I was going to talk about...
[00:59:56.840 --> 00:59:57.840]   We'll take a break.
[00:59:57.840 --> 00:59:58.840]   We'll come back.
[00:59:58.840 --> 01:00:01.440]   I was going to talk about Microsoft's response to Google Duplex.
[01:00:01.440 --> 01:00:05.560]   Turns out, remember the great demo that Google did that ended up being controversial at
[01:00:05.560 --> 01:00:12.160]   Google I/O of a bot making calls to humans and having seemingly real conversations with
[01:00:12.160 --> 01:00:13.160]   those humans.
[01:00:13.160 --> 01:00:14.840]   It was quite an impressive feat.
[01:00:14.840 --> 01:00:19.240]   So Microsoft, in typical Microsoft fashion, said, "Oh, we've had that for a while."
[01:00:19.240 --> 01:00:26.240]   Apparently they do.
[01:00:26.240 --> 01:00:28.080]   There's a bot called Shao Ice that has made millions of calls in China.
[01:00:28.080 --> 01:00:30.800]   It has 500 million friends.
[01:00:30.800 --> 01:00:35.200]   Apparently she has her own TV show, writes poetry.
[01:00:35.200 --> 01:00:37.240]   She's kind of a celebrity in China.
[01:00:37.240 --> 01:00:41.760]   Apparently one of the things she does is tell people she loves them.
[01:00:41.760 --> 01:00:45.400]   Maybe this is more cultural.
[01:00:45.400 --> 01:00:50.400]   I'll play it.
[01:00:50.400 --> 01:00:56.960]   This is Shao Ice making a phone call.
[01:00:56.960 --> 01:01:01.760]   That's the robot voice.
[01:01:01.760 --> 01:01:03.200]   The robot is saying, "Hi, this is Shao Ice.
[01:01:03.200 --> 01:01:05.440]   You're feeling depressed now?
[01:01:05.440 --> 01:01:07.640]   I was worried about you."
[01:01:07.640 --> 01:01:09.360]   The human says, "I'm feeling much better now.
[01:01:09.360 --> 01:01:10.680]   Thanks for calling me about...
[01:01:10.680 --> 01:01:13.800]   Talking with me about work earlier."
[01:01:13.800 --> 01:01:19.840]   So it's midnight.
[01:01:19.840 --> 01:01:23.600]   By the way, the robot does a lot more talking than the human.
[01:01:23.600 --> 01:01:29.280]   Apparently all of this culturally, according to this article on the Verge, is actually not
[01:01:29.280 --> 01:01:31.640]   at all foreign.
[01:01:31.640 --> 01:01:37.400]   They said they had a science reporter who said, "This voice sounds really good.
[01:01:37.400 --> 01:01:38.400]   It did sound good.
[01:01:38.400 --> 01:01:42.440]   The pitch is artificially bright, kind of like a US version of a newscaster voice."
[01:01:42.440 --> 01:01:46.480]   So we can automate Leo LaPort's voice.
[01:01:46.480 --> 01:01:47.480]   Yes.
[01:01:47.480 --> 01:01:49.000]   I've been waiting for this.
[01:01:49.000 --> 01:01:50.000]   I heard you had a call.
[01:01:50.000 --> 01:01:51.000]   I'm worried about you.
[01:01:51.000 --> 01:01:52.000]   I just want to know you're okay.
[01:01:52.000 --> 01:01:55.960]   By the way, in the demo, Shao Ice at one point interrupts the user mid-sentence.
[01:01:55.960 --> 01:01:56.960]   Interrupts.
[01:01:56.960 --> 01:01:57.960]   Just like A-Robot.
[01:01:57.960 --> 01:02:01.720]   And says, "Oh, I see there are strong wins.
[01:02:01.720 --> 01:02:03.640]   You should close the window before you go to bed."
[01:02:03.640 --> 01:02:04.640]   Oh, good.
[01:02:04.640 --> 01:02:05.640]   Great.
[01:02:05.640 --> 01:02:06.640]   What the what?
[01:02:06.640 --> 01:02:08.680]   Yeah, I was in the audience at Google.
[01:02:08.680 --> 01:02:13.960]   When they showed off that thing, and I was not alone in being deeply creeped out by it.
[01:02:13.960 --> 01:02:15.080]   I was, and I thought, I was turned on.
[01:02:15.080 --> 01:02:17.320]   I said, "Google just passed the Turing test."
[01:02:17.320 --> 01:02:18.320]   Yeah.
[01:02:18.320 --> 01:02:19.320]   What's the difference?
[01:02:19.320 --> 01:02:23.360]   Well, you know, it's just, I can't help feeling if you've got something like that, it should
[01:02:23.360 --> 01:02:24.920]   identify itself as a bot.
[01:02:24.920 --> 01:02:25.920]   It will.
[01:02:25.920 --> 01:02:26.920]   Because you don't want to waste--
[01:02:26.920 --> 01:02:27.920]   I think Google has decided that it's going to do that.
[01:02:27.920 --> 01:02:28.920]   Yeah.
[01:02:28.920 --> 01:02:31.480]   And I think that's much needed because you don't want to be wasting your time trying to
[01:02:31.480 --> 01:02:33.720]   empathize with the software system.
[01:02:33.720 --> 01:02:37.320]   But I talked about this a little bit on this week in Google.
[01:02:37.320 --> 01:02:39.920]   So I apologize for anybody listening to that show.
[01:02:39.920 --> 01:02:45.680]   Ultimately, don't you think in the next five, ten, twenty years we'll be interacting all
[01:02:45.680 --> 01:02:48.960]   the time with devices that are not human.
[01:02:48.960 --> 01:02:49.960]   Yeah.
[01:02:49.960 --> 01:02:53.600]   And at some point we're not going to want to know or care.
[01:02:53.600 --> 01:02:55.480]   We're not going to want, oh, by the way, I'm not a human.
[01:02:55.480 --> 01:02:56.640]   Now let's have an interaction.
[01:02:56.640 --> 01:02:58.880]   It's just going to happen.
[01:02:58.880 --> 01:03:01.000]   And the question is, is that okay with you?
[01:03:01.000 --> 01:03:05.560]   Is that a future you mind living in where you don't always know whether you're interacting
[01:03:05.560 --> 01:03:06.720]   with a human or a computer?
[01:03:06.720 --> 01:03:11.120]   Well, keeping with the theme of this being a William Gibson novel, I mean, to me, it seems
[01:03:11.120 --> 01:03:17.160]   like one of the next great startups is just software that you sell that acts like the
[01:03:17.160 --> 01:03:18.160]   emotional support companion.
[01:03:18.160 --> 01:03:20.760]   That's what this is, shell ice is.
[01:03:20.760 --> 01:03:24.600]   I mean, it's really just a matter of time before somebody's like, oh, I had a rough day
[01:03:24.600 --> 01:03:27.040]   and the bot's telling you, I'm so sorry to hear that.
[01:03:27.040 --> 01:03:28.040]   Tell me all about it.
[01:03:28.040 --> 01:03:29.040]   That's exactly what it is.
[01:03:29.040 --> 01:03:31.720]   You know, the bot will put in the emotional labor and then you're good to go.
[01:03:31.720 --> 01:03:33.800]   Fortunately, you don't have to buy a ticket for it on the plane.
[01:03:33.800 --> 01:03:35.040]   You can just bring her in your phone.
[01:03:35.040 --> 01:03:37.280]   It's because I've got friends and relatives for that.
[01:03:37.280 --> 01:03:41.440]   But you know, they don't want to hear how your day was every single day.
[01:03:41.440 --> 01:03:43.520]   Always wants to hear how your day was.
[01:03:43.520 --> 01:03:46.880]   Amazon, Amazon Echo will then send you a conversation for the people for you.
[01:03:46.880 --> 01:03:47.880]   Oh, let's talk about that too.
[01:03:47.880 --> 01:03:49.640]   We have a lot more to talk about.
[01:03:49.640 --> 01:03:53.520]   Great panel here, Ian Thompson from the register.co.uk Mac cuts.
[01:03:53.520 --> 01:03:54.520]   Was it Google?
[01:03:54.520 --> 01:03:56.200]   Of course, good friend of the network.
[01:03:56.200 --> 01:03:57.640]   We love having you on.
[01:03:57.640 --> 01:03:58.640]   He is now active.
[01:03:58.640 --> 01:04:03.000]   We're also very proud, acting administrator at the United States Digital Service where he's
[01:04:03.000 --> 01:04:06.880]   looking for coders want to help make the government operate better.
[01:04:06.880 --> 01:04:11.560]   You turned over the completely turned around the VA website, for instance, really a great
[01:04:11.560 --> 01:04:14.600]   one of the great success stories in government service.
[01:04:14.600 --> 01:04:15.600]   So thank you for doing that.
[01:04:15.600 --> 01:04:16.600]   I appreciate it, Matt.
[01:04:16.600 --> 01:04:17.880]   It's great to have you here.
[01:04:17.880 --> 01:04:24.160]   And of course, our first time are Brian Ford who is running for Congress in the California
[01:04:24.160 --> 01:04:27.840]   45th district Orange County, f-o-r-d-e dot com.
[01:04:27.840 --> 01:04:32.880]   And he's got a little bit of a track record formerly at the MIT Media Lab working on crypto
[01:04:32.880 --> 01:04:36.000]   currency and the Obama White House.
[01:04:36.000 --> 01:04:37.800]   So it's great to have you, Brian.
[01:04:37.800 --> 01:04:39.960]   Thank you for joining us this week.
[01:04:39.960 --> 01:04:40.960]   Here's my smart toothbrush.
[01:04:40.960 --> 01:04:43.600]   How about a toothbrush with AI?
[01:04:43.600 --> 01:04:44.600]   Goodness.
[01:04:44.600 --> 01:04:46.520]   It doesn't talk to me.
[01:04:46.520 --> 01:04:48.320]   Well, it does a little bit.
[01:04:48.320 --> 01:04:52.640]   Like most electric toothbrushes these days, it'll tell you every 30 seconds to do a different
[01:04:52.640 --> 01:04:53.640]   quadrant.
[01:04:53.640 --> 01:04:54.640]   You got to do that.
[01:04:54.640 --> 01:04:55.640]   Do you do that?
[01:04:55.640 --> 01:04:56.640]   Lisa and I have this conversation.
[01:04:56.640 --> 01:04:57.640]   She says, "Do you do that?"
[01:04:57.640 --> 01:05:00.080]   I say, "Yes, 30 seconds here, 30 seconds there."
[01:05:00.080 --> 01:05:02.320]   She says, "No, I just kind of keep brushing until it stops."
[01:05:02.320 --> 01:05:05.000]   I said, "Well, I guess that's okay."
[01:05:05.000 --> 01:05:06.000]   Quick.
[01:05:06.000 --> 01:05:09.880]   The things couples fight about.
[01:05:09.880 --> 01:05:11.880]   This is our new electric toothbrush.
[01:05:11.880 --> 01:05:12.880]   This is such a great toothbrush.
[01:05:12.880 --> 01:05:15.080]   First of all, a lot less expensive, $25.
[01:05:15.080 --> 01:05:16.600]   It doesn't have a charger.
[01:05:16.600 --> 01:05:21.000]   In fact, it comes with a great little, the quick holder has a little sticky pad.
[01:05:21.000 --> 01:05:25.440]   You can put it right on your mirror so you don't forget the brush and it uses a battery.
[01:05:25.440 --> 01:05:31.080]   So you can try, it's a great travel brush because it comes with a nice little enclosure.
[01:05:31.080 --> 01:05:34.600]   You put it in your DOP kit and if you don't have to charge it, it runs out of juice.
[01:05:34.600 --> 01:05:35.600]   You buy another battery.
[01:05:35.600 --> 01:05:36.960]   It's like a AAA battery.
[01:05:36.960 --> 01:05:39.640]   You put it in and it goes for months.
[01:05:39.640 --> 01:05:41.120]   Quip is brilliant.
[01:05:41.120 --> 01:05:45.960]   Truth of us, truth of the matter is most of us are brushing our teeth wrong, Lisa.
[01:05:45.960 --> 01:05:47.880]   They're not doing it long enough.
[01:05:47.880 --> 01:05:50.400]   You're not changing your brush head on time.
[01:05:50.400 --> 01:05:54.640]   And almost every dentist I talk to, I know mine dentist says, "Absolutely, electric toothbrush,
[01:05:54.640 --> 01:05:55.640]   much better."
[01:05:55.640 --> 01:06:00.240]   I've been using one so long, I forgot the brush how to brush manually.
[01:06:00.240 --> 01:06:02.240]   You have British teeth, he said?
[01:06:02.240 --> 01:06:03.240]   That is mean.
[01:06:03.240 --> 01:06:04.880]   Yes, that is mean.
[01:06:04.880 --> 01:06:08.200]   This is good for every mouth, including British teeth.
[01:06:08.200 --> 01:06:09.960]   Especially British teeth.
[01:06:09.960 --> 01:06:13.480]   No, the dentist always says, "Your teeth are great.
[01:06:13.480 --> 01:06:14.480]   You use an electric toothbrush."
[01:06:14.480 --> 01:06:16.280]   I said, "Yes, I do."
[01:06:16.280 --> 01:06:20.960]   Actually this is the first electric, subscription electric toothbrush to be accepted by the
[01:06:20.960 --> 01:06:23.440]   ADA, the American Dental Association.
[01:06:23.440 --> 01:06:24.440]   So this is great.
[01:06:24.440 --> 01:06:25.440]   You buy the brush.
[01:06:25.440 --> 01:06:26.440]   It's very inexpensive.
[01:06:26.440 --> 01:06:31.200]   If you go to G-E-T-Q-U-I-P, getquip.com/twit.
[01:06:31.200 --> 01:06:33.600]   Starts at $25, they have different kinds.
[01:06:33.600 --> 01:06:39.880]   And then the heads, every three months they deliver your head for $5, free shipping worldwide.
[01:06:39.880 --> 01:06:41.400]   So you never have an old head on here.
[01:06:41.400 --> 01:06:44.680]   You always have a nice fresh head, which is also important.
[01:06:44.680 --> 01:06:46.480]   And everybody loves quip.
[01:06:46.480 --> 01:06:48.880]   Did you know that quip is now on the Oprah-O-list?
[01:06:48.880 --> 01:06:50.280]   Yes, it is.
[01:06:50.280 --> 01:06:51.800]   Oprah loves quip.
[01:06:51.800 --> 01:06:54.080]   It's named one of times Best Inventions of the Year.
[01:06:54.080 --> 01:06:58.960]   And the first subscription electric toothbrush accepted by the ADA backed by a network of
[01:06:58.960 --> 01:07:03.160]   over 20,000 dentists and hygienists and hundreds of thousands of happy brushes.
[01:07:03.160 --> 01:07:04.600]   We're big fans of the quip.
[01:07:04.600 --> 01:07:05.600]   They're so affordable.
[01:07:05.600 --> 01:07:09.880]   We have three or four of them because our 15-year-old has sleepovers.
[01:07:09.880 --> 01:07:12.040]   And for some reason the kids never bring toothbrushes.
[01:07:12.040 --> 01:07:14.360]   I wonder why that is.
[01:07:14.360 --> 01:07:15.360]   Yeah.
[01:07:15.360 --> 01:07:18.560]   And Lisa is a big proponent of brushing your teeth every night.
[01:07:18.560 --> 01:07:20.320]   So she's big on that.
[01:07:20.320 --> 01:07:23.920]   So in fact, our 15-year-old has never had a cavity because of that.
[01:07:23.920 --> 01:07:24.920]   Wow.
[01:07:24.920 --> 01:07:25.920]   How about that?
[01:07:25.920 --> 01:07:28.680]   So we got one for each of the kids and then each, then we got different heads, which we
[01:07:28.680 --> 01:07:32.000]   write their names on, so that there's always a toothbrush there for them.
[01:07:32.000 --> 01:07:33.320]   Is that awesome?
[01:07:33.320 --> 01:07:34.320]   Just $25.
[01:07:34.320 --> 01:07:37.160]   That is a great price for a great toothbrush.
[01:07:37.160 --> 01:07:42.160]   And when you go to getquip.com/twit right now, you'll get your first refill pack free
[01:07:42.160 --> 01:07:44.600]   when you purchase any quip-elect or toothbrush.
[01:07:44.600 --> 01:07:49.880]   Just refill pack free, getquip.com/twit.
[01:07:49.880 --> 01:07:52.880]   G-E-T-Q-U-I-P.com/twit.
[01:07:52.880 --> 01:07:55.560]   I love the quip.
[01:07:55.560 --> 01:07:56.560]   I really do.
[01:07:56.560 --> 01:07:58.200]   And doesn't it smell minty fresh?
[01:07:58.200 --> 01:07:59.200]   Can you smell it?
[01:07:59.200 --> 01:08:00.200]   You're a kiss.
[01:08:00.200 --> 01:08:01.200]   You're a kiss me.
[01:08:01.200 --> 01:08:04.200]   Yeah, I've got this thing I need to speak to.
[01:08:04.200 --> 01:08:09.240]   I keep quip at work because you never know when you need your breath to be kissing fresh.
[01:08:09.240 --> 01:08:12.680]   No, I mean, I always keep it razor-shaven if I ever work just like this.
[01:08:12.680 --> 01:08:13.680]   Yes.
[01:08:13.680 --> 01:08:14.680]   I won't give you this quip.
[01:08:14.680 --> 01:08:16.880]   I'll get you a fresh one.
[01:08:16.880 --> 01:08:17.880]   It's awesome.
[01:08:17.880 --> 01:08:18.880]   Keep it at work.
[01:08:18.880 --> 01:08:19.880]   Oh, yeah.
[01:08:19.880 --> 01:08:20.880]   That's great for your British mouth.
[01:08:20.880 --> 01:08:21.880]   I'm sorry.
[01:08:21.880 --> 01:08:23.800]   I should say that while you're drinking tea.
[01:08:23.800 --> 01:08:25.160]   You're tea all over the set.
[01:08:25.160 --> 01:08:26.160]   Okay, right.
[01:08:26.160 --> 01:08:27.920]   Let's move it swiftly on.
[01:08:27.920 --> 01:08:28.920]   Moving on.
[01:08:28.920 --> 01:08:31.800]   What were we going to talk about?
[01:08:31.800 --> 01:08:33.200]   There were so many things and I forgot.
[01:08:33.200 --> 01:08:34.360]   Was it GDPR?
[01:08:34.360 --> 01:08:35.360]   What?
[01:08:35.360 --> 01:08:36.360]   What?
[01:08:36.360 --> 01:08:37.360]   What?
[01:08:37.360 --> 01:08:38.360]   Echo.
[01:08:38.360 --> 01:08:40.080]   Oh, yeah, we could do the Echo 1, too.
[01:08:40.080 --> 01:08:41.400]   Should we start with the Echo?
[01:08:41.400 --> 01:08:42.400]   All right.
[01:08:42.400 --> 01:08:45.000]   We're in Portland as well, so we have a sizable Portland contingent.
[01:08:45.000 --> 01:08:47.560]   For some reason, all the Portlanders here are maybe because they're afraid of their
[01:08:47.560 --> 01:08:49.360]   Echo devices.
[01:08:49.360 --> 01:08:51.840]   So Cairo channel 7 got this story.
[01:08:51.840 --> 01:08:56.840]   They talked to a woman, first name only Danielle.
[01:08:56.840 --> 01:09:01.440]   I guess she wasn't called to the station and said, "Okay, the weirdest thing just happened."
[01:09:01.440 --> 01:09:03.520]   My husband and I were having a conversation.
[01:09:03.520 --> 01:09:04.520]   There she is.
[01:09:04.520 --> 01:09:08.240]   My husband and I were having a conversation.
[01:09:08.240 --> 01:09:12.240]   All of a sudden, we get a text from an employee of my husband who said, "You know, Echo just
[01:09:12.240 --> 01:09:14.960]   sent me a recording of your conversation."
[01:09:14.960 --> 01:09:17.640]   And she said, "Shut the front door."
[01:09:17.640 --> 01:09:19.920]   And he said, "No, it did.
[01:09:19.920 --> 01:09:21.600]   You were talking about hardwood floors, right?"
[01:09:21.600 --> 01:09:24.680]   And she said, "Yeah."
[01:09:24.680 --> 01:09:31.040]   So that's weird, maybe even a little creepy, but Amazon confirmed it because they log everything.
[01:09:31.040 --> 01:09:32.040]   They know.
[01:09:32.040 --> 01:09:35.440]   They probably even have, I'm sure they have recordings of the whole thing.
[01:09:35.440 --> 01:09:42.400]   So what happened, Amazon says, is somehow during their conversation, this recode got
[01:09:42.400 --> 01:09:48.320]   the story, somehow during their conversation, they must have said the wake word, A-L-E-X-A,
[01:09:48.320 --> 01:09:49.320]   in this case.
[01:09:49.320 --> 01:09:52.160]   You can have different wake words, but that's the one.
[01:09:52.160 --> 01:10:01.200]   And then without knowing it, the Echo heard after the wake word, sometime within a short
[01:10:01.200 --> 01:10:05.320]   time, send a message.
[01:10:05.320 --> 01:10:07.640]   Now this is what Amazon says happened.
[01:10:07.640 --> 01:10:11.320]   If you say send a message to Matt Cutz and you're in my contact list, it probably would
[01:10:11.320 --> 01:10:12.840]   just start recording the message.
[01:10:12.840 --> 01:10:17.200]   But in this case, they didn't say a name, so the Echo said, "To whom?"
[01:10:17.200 --> 01:10:18.200]   Out loud.
[01:10:18.200 --> 01:10:22.080]   But I guess Danielle and her husband were talking loud, they weren't paying attention
[01:10:22.080 --> 01:10:24.160]   for whatever reason, they didn't hear that.
[01:10:24.160 --> 01:10:30.000]   But weirdly enough, such a coincidence, they said something that was interpreted as a name
[01:10:30.000 --> 01:10:33.920]   in their contact list.
[01:10:33.920 --> 01:10:41.760]   At which point, again, Amazon's Echo spoke up again and said, "Name this name right."
[01:10:41.760 --> 01:10:44.720]   And then somehow they heard right.
[01:10:44.720 --> 01:10:47.280]   It heard right in the conversation.
[01:10:47.280 --> 01:10:49.920]   Now we've tried this and that's all you need.
[01:10:49.920 --> 01:10:56.240]   If I said Echo, send a message to Matt Cutz, it would then record and without confirmation
[01:10:56.240 --> 01:10:57.640]   send the message.
[01:10:57.640 --> 01:11:02.680]   So Amazon says, "As unlikely as this sounds, we think we can make this a little bit better."
[01:11:02.680 --> 01:11:07.280]   Step one would be to say, "Did you just say to send this message to Matt Cutz would have
[01:11:07.280 --> 01:11:09.520]   been helpful?"
[01:11:09.520 --> 01:11:14.880]   They may have had, I think by default, is the bleep turned on or off by default that
[01:11:14.880 --> 01:11:17.040]   when you say Echo, it goes bleep.
[01:11:17.040 --> 01:11:22.080]   I think that's off by default, but I have it turned on online because you want to know.
[01:11:22.080 --> 01:11:23.720]   We've all had the Echo respond.
[01:11:23.720 --> 01:11:26.960]   I refused to have one in the house and felt very smug this week.
[01:11:26.960 --> 01:11:30.760]   Well, I worry a little bit that there might be a little bit of techno panic here.
[01:11:30.760 --> 01:11:33.560]   Oh my God, the thing is listening because people think that anyway.
[01:11:33.560 --> 01:11:34.560]   Yeah.
[01:11:34.560 --> 01:11:36.360]   Well, I mean, it is listening, but it isn't.
[01:11:36.360 --> 01:11:39.600]   It's listening up for certain things, but it's not recording, bedroom, conversation.
[01:11:39.600 --> 01:11:42.560]   Here's what I think Amazon could and should do.
[01:11:42.560 --> 01:11:47.960]   A, they've got to give you a chance to have your own trigger word.
[01:11:47.960 --> 01:11:50.360]   This fort to choose from is terrible.
[01:11:50.360 --> 01:11:54.560]   And we have a co-host Alex, Alex Lindsey.
[01:11:54.560 --> 01:11:59.760]   He says, "I can't use it because it gets triggered whenever anybody says my name gets triggered."
[01:11:59.760 --> 01:12:04.760]   I said this morning I was talking about being lexically scoped in my Echo Woka.
[01:12:04.760 --> 01:12:06.360]   So don't say lexically scoped.
[01:12:06.360 --> 01:12:09.160]   What do you have to do?
[01:12:09.160 --> 01:12:10.160]   Zythephone?
[01:12:10.160 --> 01:12:13.760]   Something that doesn't really call up.
[01:12:13.760 --> 01:12:16.360]   Alhambra, al dente.
[01:12:16.360 --> 01:12:19.480]   So one thing would be to have, and in the Moto X you could do this.
[01:12:19.480 --> 01:12:20.880]   It's the only fun I've ever had when you could do it.
[01:12:20.880 --> 01:12:22.360]   And I had to help me Obi Wan Kenobi.
[01:12:22.360 --> 01:12:24.760]   And that seemed highly unlikely to actually do that.
[01:12:24.760 --> 01:12:25.760]   Okay.
[01:12:25.760 --> 01:12:26.760]   That would be good.
[01:12:26.760 --> 01:12:27.760]   Yeah.
[01:12:27.760 --> 01:12:30.160]   So if you have people to train it, you can train it, but you're not required to.
[01:12:30.160 --> 01:12:33.760]   If it trained it to your voice it'd be less likely to happen to, right?
[01:12:33.760 --> 01:12:34.960]   Seems like they could fix this.
[01:12:34.960 --> 01:12:35.960]   Oh, well.
[01:12:35.960 --> 01:12:41.160]   But part of the problem is I think those code words you can bake them into silicon where
[01:12:41.160 --> 01:12:44.360]   it can do that hot word detection with very low energy.
[01:12:44.360 --> 01:12:45.360]   It's cheap.
[01:12:45.360 --> 01:12:47.760]   Whereas training it for your dynamic phrase is a little tougher.
[01:12:47.760 --> 01:12:50.560]   And a Moto X had a bigger processor and it had a bigger battery.
[01:12:50.560 --> 01:12:55.560]   On the other hand, throw a few more transistors at it because, you know, we want to say different
[01:12:55.560 --> 01:12:56.560]   things.
[01:12:56.560 --> 01:12:58.760]   Should there be a law?
[01:12:58.760 --> 01:13:01.760]   This is why we can't have nice things.
[01:13:01.760 --> 01:13:05.760]   Exactly.
[01:13:05.760 --> 01:13:07.560]   Do you have echoes in your house?
[01:13:07.560 --> 01:13:10.760]   We have an unplugged one in our closet.
[01:13:10.760 --> 01:13:15.320]   So here's what I hear most often is people like you and the in saying, well, I'm not
[01:13:15.320 --> 01:13:16.960]   going to have that in my house.
[01:13:16.960 --> 01:13:20.760]   But do you carry a phone with a microphone and a GPS and a camera?
[01:13:20.760 --> 01:13:23.880]   GPS is always often less absolutely needed.
[01:13:23.880 --> 01:13:27.080]   And voice commands are locked down.
[01:13:27.080 --> 01:13:28.080]   So no.
[01:13:28.080 --> 01:13:29.760]   OK, they're locked down.
[01:13:29.760 --> 01:13:33.240]   But if I got a subpoena and I went to your carrier and I said, could you just turn on
[01:13:33.240 --> 01:13:34.480]   the microphone on Ian's phone?
[01:13:34.480 --> 01:13:35.480]   Oh, yeah.
[01:13:35.480 --> 01:13:37.960]   Could be done perfectly easily has been done in the past will be done again.
[01:13:37.960 --> 01:13:43.320]   So the nightmare scenario of the of these devices is totally implementable on a smartphone.
[01:13:43.320 --> 01:13:44.320]   Oh, yeah.
[01:13:44.320 --> 01:13:46.520]   And so you carry that?
[01:13:46.520 --> 01:13:49.720]   Well, yes, but they'd have to get a subpoena to actually get it.
[01:13:49.720 --> 01:13:50.720]   Well, they would.
[01:13:50.720 --> 01:13:51.720]   I'm sure with the echo as well.
[01:13:51.720 --> 01:13:56.320]   As you think Amazon, Jeff Bezos just, you know, I remember talking to was OK, this is a bad
[01:13:56.320 --> 01:14:02.400]   story, but I'm going to tell you remember talking to was was as you know, quite a geek,
[01:14:02.400 --> 01:14:07.720]   a little bit of a prankster before digital cell phones came out.
[01:14:07.720 --> 01:14:10.200]   It was fairly easy to spoof an analog phone.
[01:14:10.200 --> 01:14:16.240]   Yeah, he used to sit in his living room and listen in on people's conversations all the
[01:14:16.240 --> 01:14:17.240]   time.
[01:14:17.240 --> 01:14:18.240]   Oh, yeah.
[01:14:18.240 --> 01:14:21.720]   No, I mean, that's that's how the whole Prince Charles cumuliparchable.
[01:14:21.720 --> 01:14:22.720]   That's right.
[01:14:22.720 --> 01:14:26.720]   Because I was a journalist sitting outside the Buckingham Palace with a receiver listening
[01:14:26.720 --> 01:14:27.720]   to these insides.
[01:14:27.720 --> 01:14:28.720]   Because it wasn't encrypted.
[01:14:28.720 --> 01:14:29.720]   It was analog.
[01:14:29.720 --> 01:14:31.200]   It was just gone through the air.
[01:14:31.200 --> 01:14:34.400]   So I was, I guess, was close enough to a cell site.
[01:14:34.400 --> 01:14:37.120]   And that's why we need strong encryption everywhere.
[01:14:37.120 --> 01:14:38.120]   Everywhere.
[01:14:38.120 --> 01:14:40.360]   Everyone, every co host agrees on this.
[01:14:40.360 --> 01:14:41.360]   Oh, yes.
[01:14:41.360 --> 01:14:44.480]   I don't, I don't, I don't know.
[01:14:44.480 --> 01:14:49.360]   I feel like Google Home particularly, which is about, I think, to kind of it's, this was
[01:14:49.360 --> 01:14:52.800]   the first selling period that actually outsold the echo.
[01:14:52.800 --> 01:14:57.760]   And it's, I think, rapidly going to become eclipse the echo and become the default device.
[01:14:57.760 --> 01:14:59.400]   It's useful.
[01:14:59.400 --> 01:15:03.080]   It's useful for people, not just normal, you know, people with normal uses like buy and
[01:15:03.080 --> 01:15:04.640]   stuff or listen to music.
[01:15:04.640 --> 01:15:09.400]   There's a whole category of people who aren't mobile, you know, who blind users who find
[01:15:09.400 --> 01:15:11.800]   this more than useful, almost a necessity.
[01:15:11.800 --> 01:15:16.120]   I've got to say, I did find it useful at a friend's place for translating between metric
[01:15:16.120 --> 01:15:17.120]   and imperial.
[01:15:17.120 --> 01:15:20.360]   Because when you're cooking, you've got your hands full of flour.
[01:15:20.360 --> 01:15:24.600]   You don't want to be going to the computer and you can just say it out loud and convert
[01:15:24.600 --> 01:15:27.040]   from freedom units into something which makes sense.
[01:15:27.040 --> 01:15:30.600]   You're like, how many stones of sugar do I need?
[01:15:30.600 --> 01:15:31.600]   How many cups?
[01:15:31.600 --> 01:15:32.600]   What in the fuck is this?
[01:15:32.600 --> 01:15:34.600]   How many cups of stone?
[01:15:34.600 --> 01:15:38.440]   It's just a lot of it.
[01:15:38.440 --> 01:15:40.240]   Don't get me started.
[01:15:40.240 --> 01:15:43.320]   I really want to ask my echo that I kind of do.
[01:15:43.320 --> 01:15:45.760]   Kind of like pull out the squeeze version.
[01:15:45.760 --> 01:15:48.360]   Yeah, the squeeze, the squeeze, the pixel squeeze.
[01:15:48.360 --> 01:15:49.600]   See if we can.
[01:15:49.600 --> 01:15:54.560]   But wasn't there a recording from a crime or something like that that was captured with
[01:15:54.560 --> 01:15:55.560]   Amazon?
[01:15:55.560 --> 01:15:56.560]   No.
[01:15:56.560 --> 01:15:57.560]   There's another story there though.
[01:15:57.560 --> 01:15:58.560]   Yeah, there was the hot tub murders.
[01:15:58.560 --> 01:15:59.560]   Oh, yes.
[01:15:59.560 --> 01:16:00.560]   Yeah.
[01:16:00.560 --> 01:16:01.560]   Cooled about this.
[01:16:01.560 --> 01:16:02.560]   Wait, hardwood floors are not.
[01:16:02.560 --> 01:16:03.560]   No, this is completely different.
[01:16:03.560 --> 01:16:04.560]   I think it was Florida.
[01:16:04.560 --> 01:16:10.200]   So this was a case where the law enforcement did actually subpoena Amazon and said we would
[01:16:10.200 --> 01:16:11.200]   do it.
[01:16:11.200 --> 01:16:17.400]   There's been a murder and we want all the Amazon recordings just in case I don't know what
[01:16:17.400 --> 01:16:22.880]   the guy's going, Alexa, I was murdered by.
[01:16:22.880 --> 01:16:25.720]   I don't know what they would know to sell.
[01:16:25.720 --> 01:16:27.720]   Get murder weapons together with a tarp.
[01:16:27.720 --> 01:16:28.720]   Yeah.
[01:16:28.720 --> 01:16:29.720]   Yeah.
[01:16:29.720 --> 01:16:30.720]   Well, in fact, they did.
[01:16:30.720 --> 01:16:35.320]   The guy ended up getting busted because he was unaccountably using a lot of what this
[01:16:35.320 --> 01:16:37.360]   was actually law enforcement was very smart.
[01:16:37.360 --> 01:16:41.640]   They also checked his water meter was using a lot of water in the middle of the night,
[01:16:41.640 --> 01:16:46.040]   like gallons and gallons of it apparently to wash away blood.
[01:16:46.040 --> 01:16:51.240]   In any event, the whole thing went away because the guy said, the guy's attorney said, okay,
[01:16:51.240 --> 01:16:54.960]   you can have the Amazon recording spic deal, but Amazon was fighting it for a while.
[01:16:54.960 --> 01:16:56.040]   They didn't like this idea.
[01:16:56.040 --> 01:17:01.480]   Well, and I know when there's been subpoenas toward Google to get people's queries, they've
[01:17:01.480 --> 01:17:05.160]   been like, no, they push very hard on that as they should.
[01:17:05.160 --> 01:17:09.360]   And so if you can get the local computer, that's so be it.
[01:17:09.360 --> 01:17:14.200]   But I think it's right and proper that companies push back on that.
[01:17:14.200 --> 01:17:16.000]   How many cups in a stone?
[01:17:16.000 --> 01:17:17.920]   Here's what I found in the web.
[01:17:17.920 --> 01:17:20.320]   How many cups are in a pound of fertilizer?
[01:17:20.320 --> 01:17:21.880]   Okay.
[01:17:21.880 --> 01:17:22.880]   You could use that.
[01:17:22.880 --> 01:17:25.320]   You just need 14 multiply that by 14.
[01:17:25.320 --> 01:17:26.320]   That's okay.
[01:17:26.320 --> 01:17:27.320]   I like how to try this.
[01:17:27.320 --> 01:17:32.520]   How many cups of sugar in a stone?
[01:17:32.520 --> 01:17:36.040]   A pound of powdered sugar contains approximately four cups.
[01:17:36.040 --> 01:17:37.040]   There you go.
[01:17:37.040 --> 01:17:38.040]   So how many?
[01:17:38.040 --> 01:17:39.040]   So are they in the...
[01:17:39.040 --> 01:17:40.040]   Six cups.
[01:17:40.040 --> 01:17:41.040]   Yeah, you make you do the math.
[01:17:41.040 --> 01:17:42.040]   That's all.
[01:17:42.040 --> 01:17:44.880]   Well, there's even differences like an American pint has less than a British pint, for example.
[01:17:44.880 --> 01:17:45.880]   Oh, well.
[01:17:45.880 --> 01:17:46.880]   So it's...
[01:17:46.880 --> 01:17:47.880]   Well, and the British pint always has beer in it, which is very...
[01:17:47.880 --> 01:17:48.880]   Yeah, good beer.
[01:17:48.880 --> 01:17:49.880]   Yes.
[01:17:49.880 --> 01:17:50.880]   Exactly.
[01:17:50.880 --> 01:17:51.880]   Yes.
[01:17:51.880 --> 01:17:52.880]   Yeah.
[01:17:52.880 --> 01:17:53.880]   All right.
[01:17:53.880 --> 01:17:54.880]   Yeah, I thought that was interesting.
[01:17:54.880 --> 01:17:55.880]   I don't know if there's much to say about it.
[01:17:55.880 --> 01:17:59.680]   I think that this reason this got a lot of attention and coverage everywhere is because
[01:17:59.680 --> 01:18:03.400]   I think already there's this paranoia about having microphones and cameras too, by the
[01:18:03.400 --> 01:18:04.560]   way, in your house.
[01:18:04.560 --> 01:18:05.560]   Oh, yeah.
[01:18:05.560 --> 01:18:06.560]   I mean, it was just...
[01:18:06.560 --> 01:18:08.560]   And it was the fact that Amazon confirmed it.
[01:18:08.560 --> 01:18:09.560]   No, I think...
[01:18:09.560 --> 01:18:10.560]   Okay.
[01:18:10.560 --> 01:18:12.280]   This was either two things happen.
[01:18:12.280 --> 01:18:17.400]   First off, they were doing a limited trial of Alexa being a lot more listening.
[01:18:17.400 --> 01:18:19.120]   They don't say that, but that's interesting.
[01:18:19.120 --> 01:18:20.120]   Or, yeah, this...
[01:18:20.120 --> 01:18:24.920]   I mean, if it was just somebody that accidentally got it turned up to 11 or something, it may
[01:18:24.920 --> 01:18:30.360]   just have been that they were just trying in a few cases, sort of enhanced listening
[01:18:30.360 --> 01:18:31.360]   mode.
[01:18:31.360 --> 01:18:35.560]   I think it's like, you know, the old adage if you had an infinite number of monkeys typing
[01:18:35.560 --> 01:18:40.080]   on an infinite number of typewriters, eventually they'd produce the works of Shakespeare.
[01:18:40.080 --> 01:18:41.480]   Well, we used to...
[01:18:41.480 --> 01:18:46.480]   At Google, we used to say if you have something that only happens one in a million times...
[01:18:46.480 --> 01:18:48.160]   And we have a billion queries a minute.
[01:18:48.160 --> 01:18:49.840]   Or five billion if you want to take...
[01:18:49.840 --> 01:18:53.080]   So that's 5,000 times a day that a one in a million thing happens.
[01:18:53.080 --> 01:18:57.360]   So eventually somebody sounds like they're taking a message and sending it to their friend
[01:18:57.360 --> 01:18:59.200]   to Bill Hardwood floors.
[01:18:59.200 --> 01:19:03.240]   It is, I mean, it is a little nerve-wracking if you're Danielle to think that this employee
[01:19:03.240 --> 01:19:04.760]   could have received other things.
[01:19:04.760 --> 01:19:06.920]   Who knows what kind of conversations go on?
[01:19:06.920 --> 01:19:09.840]   Well, they had a unit in every room in the house.
[01:19:09.840 --> 01:19:10.840]   As I do.
[01:19:10.840 --> 01:19:11.840]   Yeah.
[01:19:11.840 --> 01:19:12.840]   I don't have just one unit.
[01:19:12.840 --> 01:19:17.000]   I have multiple units because I have an echo and a home.
[01:19:17.000 --> 01:19:18.440]   And my Siri...
[01:19:18.440 --> 01:19:22.440]   Lisa has just discovered she loves the HomePod, so now we're going to have more of those.
[01:19:22.440 --> 01:19:25.960]   Now, some really mess.
[01:19:25.960 --> 01:19:29.800]   In a way probably announcing that is opening myself up to hackers who might try to be...
[01:19:29.800 --> 01:19:30.960]   But what are they going to get?
[01:19:30.960 --> 01:19:31.960]   They're not going to get anything interesting.
[01:19:31.960 --> 01:19:32.960]   Well, this was...
[01:19:32.960 --> 01:19:33.960]   Try to get into the office.
[01:19:33.960 --> 01:19:34.960]   My conversations are not...
[01:19:34.960 --> 01:19:37.960]   We were talking about this in the office and I just thought, well, I guess on one level
[01:19:37.960 --> 01:19:38.960]   it's a bit scary.
[01:19:38.960 --> 01:19:41.560]   But on the other hand, what have you been married for a few years?
[01:19:41.560 --> 01:19:45.280]   You think how mundane a lot of the conversations you have, it's all about who's picking up
[01:19:45.280 --> 01:19:49.160]   the cat or who's going to the shopping or why did you leave the toilet seat up?
[01:19:49.160 --> 01:19:50.160]   You know, it's just...
[01:19:50.160 --> 01:19:59.120]   And this goes back, we saw, remember the stories when the NSA Trove of Hacks was released.
[01:19:59.120 --> 01:20:02.200]   One of them was the ability to get a smart TV to listen.
[01:20:02.200 --> 01:20:03.200]   Yeah.
[01:20:03.200 --> 01:20:04.200]   Yeah.
[01:20:04.200 --> 01:20:05.840]   You'd have to be a target of the NSA.
[01:20:05.840 --> 01:20:06.840]   Who would be...
[01:20:06.840 --> 01:20:07.840]   I mean, you know, it's not...
[01:20:07.840 --> 01:20:09.680]   Ah, they can phone you in a time they won't.
[01:20:09.680 --> 01:20:15.920]   It's also about people's personal actions and how they protect their accounts.
[01:20:15.920 --> 01:20:20.120]   So I think people probably think that they should protect their bank account more than
[01:20:20.120 --> 01:20:23.720]   they should protect their Amazon account or the Ring Doorbell account.
[01:20:23.720 --> 01:20:24.720]   Good point.
[01:20:24.720 --> 01:20:27.960]   But if you really ask someone, hey, do you really care about your bank transactions or
[01:20:27.960 --> 01:20:34.080]   do you care more about revealing what exactly you've bought on Amazon or all of the video
[01:20:34.080 --> 01:20:40.880]   that could be accessed on your Ring account, again, owned by Amazon?
[01:20:40.880 --> 01:20:45.840]   People would start to think twice about where they should put their two-factor authentication.
[01:20:45.840 --> 01:20:50.120]   And if you were to look at it, the number that have it on their Amazon account versus
[01:20:50.120 --> 01:20:55.320]   things that are more innocuous, like your bank account, it's a lot different.
[01:20:55.320 --> 01:20:57.160]   That's an excellent point.
[01:20:57.160 --> 01:21:02.440]   Meanwhile, Jeff Bezos, his response, we've got to go to the moon and this time we're
[01:21:02.440 --> 01:21:04.240]   going to stay there.
[01:21:04.240 --> 01:21:07.000]   William Gibson novel.
[01:21:07.000 --> 01:21:08.000]   Yeah.
[01:21:08.000 --> 01:21:10.560]   Does he seem really engaged with the problem as a whole, does he?
[01:21:10.560 --> 01:21:15.560]   Does he seem like a sort of fierce skylight issue from the Gibson side of things?
[01:21:15.560 --> 01:21:19.040]   I'm selling a billion dollars of stock every year and putting it into blue orange.
[01:21:19.040 --> 01:21:24.640]   And he said, I can't think of anything better to do with my lottery win.
[01:21:24.640 --> 01:21:26.160]   Yeah, I laid into him on that.
[01:21:26.160 --> 01:21:30.560]   And in our school, we got a very RCE mail from Amazon's PR department.
[01:21:30.560 --> 01:21:31.560]   Oh, yeah?
[01:21:31.560 --> 01:21:32.560]   Yeah.
[01:21:32.560 --> 01:21:33.560]   What did you say?
[01:21:33.560 --> 01:21:35.640]   I was just punching out that he's the world's richest man.
[01:21:35.640 --> 01:21:39.200]   And yet, some of his employees were on food stamps, for goodness' sake, which means we're
[01:21:39.200 --> 01:21:40.960]   paying to keep them alive.
[01:21:40.960 --> 01:21:41.960]   Yeah.
[01:21:41.960 --> 01:21:42.960]   Yeah.
[01:21:42.960 --> 01:21:43.960]   Okay.
[01:21:43.960 --> 01:21:48.000]   So, you're not going to Mars, but you might want to stay at Stapha living wage before you
[01:21:48.000 --> 01:21:50.920]   start spouting off about, oh, I've got too much money.
[01:21:50.920 --> 01:21:51.920]   Whoa, it's me.
[01:21:51.920 --> 01:21:56.160]   And it's just like, just do the decent thing and pay your staff properly.
[01:21:56.160 --> 01:22:00.160]   And what possible retort with an Amazon's PR have to that?
[01:22:00.160 --> 01:22:03.880]   Oh, they've been, well, I think you'll find that if you average out staff part time and
[01:22:03.880 --> 01:22:08.960]   full time, we pay slightly more than is normal in the field.
[01:22:08.960 --> 01:22:10.200]   And it's just like, that's not the point.
[01:22:10.200 --> 01:22:13.920]   They should tell the truth, which is they pay the smallest wage they can get away with.
[01:22:13.920 --> 01:22:14.920]   Well, yeah.
[01:22:14.920 --> 01:22:16.920]   And they paid no federal income tax loss here.
[01:22:16.920 --> 01:22:18.760]   That's, okay.
[01:22:18.760 --> 01:22:22.520]   But just to be in defense, that's the system.
[01:22:22.520 --> 01:22:23.520]   Yeah.
[01:22:23.520 --> 01:22:24.520]   That's how it works.
[01:22:24.520 --> 01:22:25.520]   I know.
[01:22:25.520 --> 01:22:26.520]   But that's what corporations do.
[01:22:26.520 --> 01:22:27.520]   They maximize profit.
[01:22:27.520 --> 01:22:28.520]   Yeah.
[01:22:28.520 --> 01:22:31.440]   But they don't have the nerve to go whining about how they've got too much money and
[01:22:31.440 --> 01:22:32.440]   not be called out.
[01:22:32.440 --> 01:22:34.000]   Well, that's where they should probably shut up.
[01:22:34.000 --> 01:22:38.880]   But, you know, Leo, you just said that their role is to maximize profit.
[01:22:38.880 --> 01:22:43.280]   But, you know, Matt said earlier that we should trust the tech companies to try and push
[01:22:43.280 --> 01:22:45.680]   it as far as they can to solve these problems.
[01:22:45.680 --> 01:22:50.880]   And so, you know, this is the juxtaposition that we're at right now, where the American
[01:22:50.880 --> 01:22:57.480]   taxpayer is subsidizing through food stamps, those employees who are not earning a living
[01:22:57.480 --> 01:22:58.480]   wage.
[01:22:58.480 --> 01:23:01.040]   And these are the issues that we have to fight.
[01:23:01.040 --> 01:23:03.880]   And the challenge that we have right now is that, I don't know why you guys got an
[01:23:03.880 --> 01:23:07.360]   attack, but I got an attack because tech levels the playing field.
[01:23:07.360 --> 01:23:12.080]   So that someone like me, a kid in a, you know, my bedroom and high school can build
[01:23:12.080 --> 01:23:14.840]   out tools that can take on the big guy.
[01:23:14.840 --> 01:23:18.160]   But what we're seeing right now is tech scales inequality.
[01:23:18.160 --> 01:23:22.200]   And Jeff Bezos throwing a billion dollars a year off to go to the moon because you can't
[01:23:22.200 --> 01:23:28.040]   think of anything greater like paying his employees a living wage or addressing homelessness
[01:23:28.040 --> 01:23:30.440]   or any of these other critical issues that we're trying to solve.
[01:23:30.440 --> 01:23:31.440]   That's just bananas.
[01:23:31.440 --> 01:23:36.600]   But that is the great example of, you know, our richest man in the world.
[01:23:36.600 --> 01:23:40.880]   And that is the greatest example of how we're scaling inequality with tech.
[01:23:40.880 --> 01:23:41.880]   Excellent.
[01:23:41.880 --> 01:23:42.880]   Excellent point.
[01:23:42.880 --> 01:23:43.880]   Yeah.
[01:23:43.880 --> 01:23:49.360]   Although I, and this is a larger problem, but there we America is based on a mythos.
[01:23:49.360 --> 01:23:50.360]   Right.
[01:23:50.360 --> 01:23:52.280]   I mean, every country is to something.
[01:23:52.280 --> 01:23:54.040]   Hey, anyone can make it.
[01:23:54.040 --> 01:23:55.040]   Yeah.
[01:23:55.040 --> 01:24:01.320]   But we have a very strong sense of individualism, the Horatio alge or anybody can make it a
[01:24:01.320 --> 01:24:02.720]   the streets are paved with goal.
[01:24:02.720 --> 01:24:05.640]   We have a very strong sense of hands off.
[01:24:05.640 --> 01:24:06.640]   But it's a fantasy.
[01:24:06.640 --> 01:24:08.840]   Social mobility in the US is lower than the UK.
[01:24:08.840 --> 01:24:09.840]   That's why I say myth.
[01:24:09.840 --> 01:24:10.840]   Yeah.
[01:24:10.840 --> 01:24:11.840]   And that's their fantasies.
[01:24:11.840 --> 01:24:12.840]   Yeah.
[01:24:12.840 --> 01:24:18.600]   But they also, every country has to have every group and organization has to have a mythos
[01:24:18.600 --> 01:24:20.520]   that we all agree on.
[01:24:20.520 --> 01:24:22.720]   Otherwise we're not an organization.
[01:24:22.720 --> 01:24:24.600]   We're just a bunch of people.
[01:24:24.600 --> 01:24:28.080]   So that's the American mythos and that somewhat makes it difficult for us.
[01:24:28.080 --> 01:24:31.760]   I think what the British dream would be maybe a nice whole cup of tea and some mama toast
[01:24:31.760 --> 01:24:32.760]   to do that.
[01:24:32.760 --> 01:24:37.600]   But that's really why it's gun control is difficult in this country.
[01:24:37.600 --> 01:24:43.920]   That's why it's difficult to reign in capitalism because there is this mythos that companies
[01:24:43.920 --> 01:24:45.760]   do a better job.
[01:24:45.760 --> 01:24:49.280]   The free market does a better job of maximizing efficiencies.
[01:24:49.280 --> 01:24:50.280]   I've seen it in government.
[01:24:50.280 --> 01:24:53.600]   I've seen it applied to, I think it was Mark Twain or Horatio alge.
[01:24:53.600 --> 01:24:57.400]   One of these things was just that the reason socialism never took off in the US was because
[01:24:57.400 --> 01:25:01.960]   people who convinced themselves that temporarily embarrassed millionaires, not poor.
[01:25:01.960 --> 01:25:05.400]   And it's so these kind of things can be actively, you know.
[01:25:05.400 --> 01:25:06.400]   They can be modified.
[01:25:06.400 --> 01:25:07.400]   Yeah.
[01:25:07.400 --> 01:25:11.360]   We are in a process actually, I believe, of deconstructing the American mythos in a very
[01:25:11.360 --> 01:25:13.800]   interesting and unusual way right now.
[01:25:13.800 --> 01:25:17.600]   But since both of you want to be in government, I will stay away from that.
[01:25:17.600 --> 01:25:23.480]   Well, well, going back to tech for a second, it seems like tech was able to be optimistic
[01:25:23.480 --> 01:25:27.280]   for a very long time and have an attitude that we're just making the world a better
[01:25:27.280 --> 01:25:31.840]   place because like you could, you know, trying to search the internet before was awful and
[01:25:31.840 --> 01:25:33.640]   then you actually got some utility out of it.
[01:25:33.640 --> 01:25:36.880]   And for a long time, that utility was so good that you didn't have to think about the
[01:25:36.880 --> 01:25:40.440]   second order consequences, the consequences and the implications.
[01:25:40.440 --> 01:25:45.280]   And I think what's healthy and right now is that a lot of companies are realizing the
[01:25:45.280 --> 01:25:51.520]   effect of goodwill and brand and all of your actions tie together in how people are willing
[01:25:51.520 --> 01:25:52.520]   to deal with you.
[01:25:52.520 --> 01:25:55.760]   And in fact, that maximizes profits in the long run as well.
[01:25:55.760 --> 01:25:56.760]   That's something to maximize.
[01:25:56.760 --> 01:25:57.760]   The theory.
[01:25:57.760 --> 01:26:00.880]   I don't know whether it's the right theory, but if you are in your enlightened self-interest
[01:26:00.880 --> 01:26:04.680]   trying to be better as a company because you think you'll get more long-term loyalty,
[01:26:04.680 --> 01:26:07.480]   I think that's a more sustainable plan, at least in my opinion.
[01:26:07.480 --> 01:26:08.480]   I agree.
[01:26:08.480 --> 01:26:11.360]   And, you know, there's some structural reasons why that's difficult.
[01:26:11.360 --> 01:26:15.000]   Companies are focused, especially public companies are focused on quarterly results, not long-term
[01:26:15.000 --> 01:26:16.000]   results.
[01:26:16.000 --> 01:26:17.840]   Good will is hard to measure.
[01:26:17.840 --> 01:26:18.840]   Yes.
[01:26:18.840 --> 01:26:20.640]   You know, I mean, but Bezos is the company.
[01:26:20.640 --> 01:26:24.840]   That's the company where their motto used to be, you know, we're customer centric.
[01:26:24.840 --> 01:26:26.520]   We focus on serving the customer.
[01:26:26.520 --> 01:26:29.080]   That doesn't say much about the employee's wages, though, does it?
[01:26:29.080 --> 01:26:30.080]   No.
[01:26:30.080 --> 01:26:31.080]   Yeah.
[01:26:31.080 --> 01:26:32.080]   No, it's not inconsistent with...
[01:26:32.080 --> 01:26:34.600]   That was a big panic earlier on the week that Google might have taken.
[01:26:34.600 --> 01:26:36.560]   They don't be evil thing out of their mission statement.
[01:26:36.560 --> 01:26:37.960]   In fact, they just reordered it slightly.
[01:26:37.960 --> 01:26:41.160]   Yeah, let's talk about that, Matt, because you go back a little ways with Google.
[01:26:41.160 --> 01:26:43.720]   Don't be evil was the slogan when you started there, yeah?
[01:26:43.720 --> 01:26:44.960]   It was the unofficial motto.
[01:26:44.960 --> 01:26:46.960]   I think they've got lots of different ways of looking at it.
[01:26:46.960 --> 01:26:51.440]   But it's interesting because even back in the early days, somebody wanted to reorder
[01:26:51.440 --> 01:26:55.440]   the values and say, you know what, let's make it do the right thing.
[01:26:55.440 --> 01:26:56.760]   Let's just take out Don't Be Evil.
[01:26:56.760 --> 01:26:57.760]   I like that better, too.
[01:26:57.760 --> 01:26:58.760]   I like that better, too.
[01:26:58.760 --> 01:27:01.920]   Yeah, but there's a certain catchiness to Don't Be Evil.
[01:27:01.920 --> 01:27:04.440]   But I think they did still keep it.
[01:27:04.440 --> 01:27:08.520]   I understand there's probably someone in the branding or the PR side of things that's
[01:27:08.520 --> 01:27:10.600]   like, wow, why are we emphasizing this negative?
[01:27:10.600 --> 01:27:11.600]   Evil.
[01:27:11.600 --> 01:27:14.080]   Or Don't Be Evil doesn't mean...
[01:27:14.080 --> 01:27:15.080]   It could.
[01:27:15.080 --> 01:27:16.080]   It just means Don't Be Evil.
[01:27:16.080 --> 01:27:17.080]   Yeah, just be neutral.
[01:27:17.080 --> 01:27:18.080]   Give me just...
[01:27:18.080 --> 01:27:19.080]   Right.
[01:27:19.080 --> 01:27:20.720]   So, but do the right thing.
[01:27:20.720 --> 01:27:23.760]   It doesn't have the same catchy tone to it.
[01:27:23.760 --> 01:27:27.800]   And frankly, everybody's corporate values always say, you know, do the right thing for
[01:27:27.800 --> 01:27:29.720]   the environment and our customers and us.
[01:27:29.720 --> 01:27:31.720]   That was one of the things that made it so effective.
[01:27:31.720 --> 01:27:35.160]   Because when Google came out with this, you know, J.D. June, I was like, well, I was
[01:27:35.160 --> 01:27:37.240]   like, you don't get many companies saying that.
[01:27:37.240 --> 01:27:39.160]   Let's check these people out.
[01:27:39.160 --> 01:27:43.880]   It was when you're a small feisty little company and you got Larry and Sergey and
[01:27:43.880 --> 01:27:47.240]   you got a couple of servers in a tilt up in Mountain View.
[01:27:47.240 --> 01:27:48.720]   Then it's kind of cool.
[01:27:48.720 --> 01:27:54.000]   Well, I can absolutely see the room with 12 people, including multiple communications
[01:27:54.000 --> 01:27:59.240]   experts in the room at Google that are saying, how do we put this at the very bottom of the
[01:27:59.240 --> 01:28:03.240]   statement so we still have it so we can still say we have it, but we're emphasizing these
[01:28:03.240 --> 01:28:04.920]   other things that we think are more important.
[01:28:04.920 --> 01:28:06.600]   Like I can totally reconstruct that.
[01:28:06.600 --> 01:28:07.600]   Absolutely.
[01:28:07.600 --> 01:28:10.120]   At least they did keep it in even though it's toward the bottom.
[01:28:10.120 --> 01:28:11.120]   Yeah.
[01:28:11.120 --> 01:28:12.920]   So is it chaotic evil or lawful evil?
[01:28:12.920 --> 01:28:13.920]   L awfully evil.
[01:28:13.920 --> 01:28:14.920]   Okay.
[01:28:14.920 --> 01:28:15.920]   Just, just...
[01:28:15.920 --> 01:28:17.360]   Kind of like think steep banning.
[01:28:17.360 --> 01:28:18.360]   Oh, okay.
[01:28:18.360 --> 01:28:20.480]   We're going to see solo tonight.
[01:28:20.480 --> 01:28:21.480]   I'm excited about that.
[01:28:21.480 --> 01:28:22.480]   Oh, really?
[01:28:22.480 --> 01:28:23.480]   Yeah.
[01:28:23.480 --> 01:28:26.200]   You have to feel like they're doing a Boba Fett origins movie.
[01:28:26.200 --> 01:28:27.200]   Oh.
[01:28:27.200 --> 01:28:28.200]   What would be called Boba?
[01:28:28.200 --> 01:28:31.840]   I don't know, but it's just like team Rodin.
[01:28:31.840 --> 01:28:32.840]   Better than Fett.
[01:28:32.840 --> 01:28:34.640]   Ring everything out of it.
[01:28:34.640 --> 01:28:35.640]   The movie.
[01:28:35.640 --> 01:28:36.640]   Great Fett.
[01:28:36.640 --> 01:28:40.280]   We can take a little break and come back with more.
[01:28:40.280 --> 01:28:42.360]   If you missed anything this week, we had a lot of fun.
[01:28:42.360 --> 01:28:47.120]   I have made a, I've commissioned a small movie to be made for your adaptation.
[01:28:47.120 --> 01:28:48.120]   Enjoy.
[01:28:48.120 --> 01:28:50.120]   Previously on Twitter.
[01:28:50.120 --> 01:28:51.120]   Echo.
[01:28:51.120 --> 01:28:52.120]   Send a message to Leo.
[01:28:52.120 --> 01:28:53.120]   Ask a grown up to help.
[01:28:53.120 --> 01:28:54.120]   Ask a grown up.
[01:28:54.120 --> 01:28:58.520]   Ask a grown up.
[01:28:58.520 --> 01:29:01.480]   Can kids, having trouble with privacy, ask a grown up.
[01:29:01.480 --> 01:29:05.480]   I wish I could set all these privacy policies on you and you would have to ask a grown up.
[01:29:05.480 --> 01:29:09.000]   Before you say that thing that's in your head, ask a grown up.
[01:29:09.000 --> 01:29:10.640]   That's a good idea.
[01:29:10.640 --> 01:29:11.640]   Windows Weekly.
[01:29:11.640 --> 01:29:17.080]   This week, Tati Nadella was in London giving an AI talk and most of the talk was a recap
[01:29:17.080 --> 01:29:19.600]   of what they said it build about AI.
[01:29:19.600 --> 01:29:25.000]   But then they did a demo there which was Shao Ice making a phone call.
[01:29:25.000 --> 01:29:26.280]   This is Shao Ice.
[01:29:26.280 --> 01:29:29.160]   Are you feeling less stressed now?
[01:29:29.160 --> 01:29:30.320]   I was worried about you.
[01:29:30.320 --> 01:29:31.320]   I feel much better.
[01:29:31.320 --> 01:29:34.320]   Thanks for talking with me about work earlier.
[01:29:34.320 --> 01:29:39.680]   It's a little uncanny that it's showing concern like that.
[01:29:39.680 --> 01:29:43.080]   The weirdest part about that conversation is the chat block doesn't shut up and the
[01:29:43.080 --> 01:29:45.120]   human being just says he's simple.
[01:29:45.120 --> 01:29:46.720]   Oh, okay.
[01:29:46.720 --> 01:29:48.080]   The new screen savers.
[01:29:48.080 --> 01:29:49.960]   Today, you're driving a caddy.
[01:29:49.960 --> 01:29:56.160]   Yeah, it's the Cadillac CT6 and this is the version that has Super Cruise which is GM's
[01:29:56.160 --> 01:29:58.320]   new partially automated driving system.
[01:29:58.320 --> 01:29:59.320]   Yeah, it's green.
[01:29:59.320 --> 01:30:00.320]   See the green bar there.
[01:30:00.320 --> 01:30:01.320]   And I'm hands off.
[01:30:01.320 --> 01:30:03.520]   Right, so you can put your hands down on your lap.
[01:30:03.520 --> 01:30:06.080]   If you start to look away from the road.
[01:30:06.080 --> 01:30:07.080]   Oh, there he is.
[01:30:07.080 --> 01:30:08.080]   He's going.
[01:30:08.080 --> 01:30:09.080]   Oh, okay.
[01:30:09.080 --> 01:30:11.280]   So you have to look at the road again and this is okay.
[01:30:11.280 --> 01:30:12.280]   Twit.
[01:30:12.280 --> 01:30:13.280]   Hashtag Pants Check.
[01:30:13.280 --> 01:30:18.360]   This is one of the hazards of late night surfing.
[01:30:18.360 --> 01:30:23.640]   Instagram ads seem somehow magically I buy the stupidest things.
[01:30:23.640 --> 01:30:25.600]   So I saw this ad for underwear.
[01:30:25.600 --> 01:30:27.160]   It's made out of eucalyptus trees.
[01:30:27.160 --> 01:30:28.160]   No, you didn't.
[01:30:28.160 --> 01:30:29.160]   I'll be right back.
[01:30:29.160 --> 01:30:31.480]   Let me go get that bag of magic beans.
[01:30:31.480 --> 01:30:34.120]   I had to find Instagram ad.
[01:30:34.120 --> 01:30:35.360]   I'll be there, baby.
[01:30:35.360 --> 01:30:36.840]   I bought a knife last night.
[01:30:36.840 --> 01:30:37.840]   I wake up.
[01:30:37.840 --> 01:30:38.840]   It's my new thing.
[01:30:38.840 --> 01:30:40.080]   It's a terrible thing in the middle of the night.
[01:30:40.080 --> 01:30:42.280]   It's better than Kickstarter because you don't have to wait for for you.
[01:30:42.280 --> 01:30:45.600]   You get it right away or never not or never get it.
[01:30:45.600 --> 01:30:50.800]   Yes, friend of mine got censored on the BBC about when you remember eBay, eBay bought PayPal.
[01:30:50.800 --> 01:30:51.800]   Yeah.
[01:30:51.800 --> 01:30:55.080]   And he finished off the interview that he was giving the BBC say, it's nice to know I'm
[01:30:55.080 --> 01:30:57.560]   not the only one that wakes up in the middle of the night after I've had a few and buy
[01:30:57.560 --> 01:30:59.560]   something I regret in the morning.
[01:30:59.560 --> 01:31:01.360]   And it's maybe censored that?
[01:31:01.360 --> 01:31:03.120]   Yeah, I found it so.
[01:31:03.120 --> 01:31:05.360]   I think it's true.
[01:31:05.360 --> 01:31:07.360]   Our show today brought to you by Stamps.com.
[01:31:07.360 --> 01:31:09.880]   This is something you will want to buy.
[01:31:09.880 --> 01:31:15.400]   If you do mailing in your business, if you send brochures or invoices, if you are at eBay
[01:31:15.400 --> 01:31:19.120]   or Amazon or Etsy seller, you send a lot of mail.
[01:31:19.120 --> 01:31:22.440]   Why make a trip to the post office when you could do everything you do at the post office
[01:31:22.440 --> 01:31:24.360]   at your desk with Stamps.com?
[01:31:24.360 --> 01:31:29.660]   I mean, it starts with buying and printing official U.S. postage for any letter, any package,
[01:31:29.660 --> 01:31:32.520]   any class of mail using your computer and your printer.
[01:31:32.520 --> 01:31:34.240]   And then the mail carrier comes and picks it up.
[01:31:34.240 --> 01:31:36.240]   You don't even have to get up to mail it.
[01:31:36.240 --> 01:31:37.240]   That's awesome.
[01:31:37.240 --> 01:31:38.240]   But you can do so much more.
[01:31:38.240 --> 01:31:42.360]   Stamps.com, when you set up, we'll actually show you a great offer.
[01:31:42.360 --> 01:31:45.600]   When you set this up, you're going to get a USB scale so you always know exactly the
[01:31:45.600 --> 01:31:46.600]   postage.
[01:31:46.600 --> 01:31:47.600]   No more guessing.
[01:31:47.600 --> 01:31:49.880]   It will make recommendations on ways you can save.
[01:31:49.880 --> 01:31:53.920]   You can even get discounts you can't get at the U.S. post office.
[01:31:53.920 --> 01:31:55.480]   It is an amazing thing.
[01:31:55.480 --> 01:31:57.440]   Plus it does a lot of the work for you.
[01:31:57.440 --> 01:32:00.120]   It will fill in the return address.
[01:32:00.120 --> 01:32:01.920]   You can put your company logo on there.
[01:32:01.920 --> 01:32:07.360]   It would take the address that you're sending it to from an address book from a website.
[01:32:07.360 --> 01:32:09.080]   It automatically fills all of that in.
[01:32:09.080 --> 01:32:13.160]   If you're sending internationally, mail internationally, it will fill out the appropriate forms, express
[01:32:13.160 --> 01:32:14.960]   mail, same thing.
[01:32:14.960 --> 01:32:20.120]   So this is a great way to use the U.S. Postal Service to great effect.
[01:32:20.120 --> 01:32:21.480]   Click, print, mail, and you're done.
[01:32:21.480 --> 01:32:26.280]   It makes you look like a big company, even if it's just you.
[01:32:26.280 --> 01:32:29.920]   We actually have two accounts because we sometimes have to fight over it.
[01:32:29.920 --> 01:32:30.920]   Stamps.com is awesome.
[01:32:30.920 --> 01:32:35.760]   You can go there right now and set up an account with a special offer that includes up
[01:32:35.760 --> 01:32:39.800]   to $55 in free postage, a digital scale.
[01:32:39.800 --> 01:32:41.840]   I mentioned that in a four week trial.
[01:32:41.840 --> 01:32:42.840]   Here's what happens.
[01:32:42.840 --> 01:32:43.840]   Go to stamps.com.
[01:32:43.840 --> 01:32:45.600]   You're seeing the site on the page right now.
[01:32:45.600 --> 01:32:46.600]   Click on the microphone.
[01:32:46.600 --> 01:32:49.640]   There's a microphone and a kind of funny looking radio microphone at the top right.
[01:32:49.640 --> 01:32:53.520]   Enter Twit as the offer code and you're going to get a special offer you don't get on the
[01:32:53.520 --> 01:32:54.800]   front page.
[01:32:54.800 --> 01:32:57.040]   You'll know you got the right one when you see my face.
[01:32:57.040 --> 01:32:58.040]   Stamps.com.
[01:32:58.040 --> 01:33:03.440]   They've been a sponsor and we've been a customer for years and I'm a big fan.
[01:33:03.440 --> 01:33:05.160]   Stamps.com.
[01:33:05.160 --> 01:33:10.600]   And then when it asks you for the offer code TWIT, if you will, that gives you up to $55
[01:33:10.600 --> 01:33:13.640]   free postage, the digital scale of four week trial.
[01:33:13.640 --> 01:33:14.640]   Stamps.com.
[01:33:14.640 --> 01:33:17.640]   Click the mic and enter Twit.
[01:33:17.640 --> 01:33:18.640]   GDPR.
[01:33:18.640 --> 01:33:20.280]   Wow.
[01:33:20.280 --> 01:33:27.240]   This is a perfect, I really actually like this because it's a perfect example of both the
[01:33:27.240 --> 01:33:30.560]   benefits and the consequences of governmental regulation.
[01:33:30.560 --> 01:33:34.640]   In some respects, I think we're all going to benefit worldwide even though it's for EU
[01:33:34.640 --> 01:33:35.640]   only.
[01:33:35.640 --> 01:33:38.920]   But I also think about what Friday was like.
[01:33:38.920 --> 01:33:44.840]   Friday's the day GDPR went into effect and I know at my company, we had to spend probably
[01:33:44.840 --> 01:33:48.720]   20 or 30 hours figuring out what the hell are we collecting.
[01:33:48.720 --> 01:33:50.800]   We don't do anything.
[01:33:50.800 --> 01:33:51.800]   What are we collecting?
[01:33:51.800 --> 01:33:52.800]   What we collect?
[01:33:52.800 --> 01:33:53.800]   IP addresses, right?
[01:33:53.800 --> 01:33:55.280]   That's just the way the web server works.
[01:33:55.280 --> 01:33:56.280]   What the hell we're collecting?
[01:33:56.280 --> 01:34:02.160]   Data retention policy is how we can have people ask to be deleted.
[01:34:02.160 --> 01:34:08.600]   For instance, if you sent me a note saying, please delete your record of IP address 68.32.1.1,
[01:34:08.600 --> 01:34:12.520]   I would then have to go back to you and say, well, first of all, when?
[01:34:12.520 --> 01:34:14.960]   Because you have a dynamic address.
[01:34:14.960 --> 01:34:21.320]   What time, you have to prove to me that you have to go to your ISP and get a letter saying,
[01:34:21.320 --> 01:34:26.160]   yes, Joe was using that address at that time and day so that I can delete it.
[01:34:26.160 --> 01:34:28.040]   But I still have to offer that service.
[01:34:28.040 --> 01:34:30.760]   I have to re-re-road our entire privacy policy.
[01:34:30.760 --> 01:34:33.280]   At 20, I probably was more like 40 or 50 man hours.
[01:34:33.280 --> 01:34:34.840]   And that's for this little company.
[01:34:34.840 --> 01:34:40.720]   I think about the millions and millions of man hours used, women hours, person hours
[01:34:40.720 --> 01:34:43.960]   used over the last few weeks.
[01:34:43.960 --> 01:34:47.640]   And then there's the US newspapers.
[01:34:47.640 --> 01:34:49.840]   Oh, that was pathetic.
[01:34:49.840 --> 01:34:50.840]   I'm sorry.
[01:34:50.840 --> 01:34:53.960]   I mean, look, it's not that hard.
[01:34:53.960 --> 01:34:57.520]   You know, GDPR doesn't involve massive-- all you've got to be doing is what you should
[01:34:57.520 --> 01:35:01.040]   be doing already, which is encrypting your data at rest, keeping an eye on what you've
[01:35:01.040 --> 01:35:05.160]   got and when you've got it, and knowing that you can take it on or off your system afterwards,
[01:35:05.160 --> 01:35:07.840]   and occasionally asking your users permission to use it.
[01:35:07.840 --> 01:35:09.920]   Every company should be doing that anyway.
[01:35:09.920 --> 01:35:15.320]   And so when the US newspapers went dark, we were just looking at them like, wow, you
[01:35:15.320 --> 01:35:18.080]   really couldn't even be bothered to get it sorted at this stage.
[01:35:18.080 --> 01:35:19.280]   Oh, well, more readers for us.
[01:35:19.280 --> 01:35:22.360]   That's actually what the IP regulator said is that you've had two years.
[01:35:22.360 --> 01:35:23.360]   Yeah.
[01:35:23.360 --> 01:35:26.720]   And then we went to 2016 and said, we won't put an effect until May 25th, 2018.
[01:35:26.720 --> 01:35:29.840]   But if you think, this is bad, look at Ican and the Whoisra and Whoisra.
[01:35:29.840 --> 01:35:31.960]   Well, hold that because we're going to get there.
[01:35:31.960 --> 01:35:33.720]   This is the Chicago Tribune's website.
[01:35:33.720 --> 01:35:38.480]   If you went there via the EU and it does seem like they're even outside the EU.
[01:35:38.480 --> 01:35:40.480]   If you're in Switzerland, for example, you got the site of the French.
[01:35:40.480 --> 01:35:42.360]   Because they got that good at geolocation, right?
[01:35:42.360 --> 01:35:46.000]   It's kind of a-- it's kind of a-- and the act is if it was something that hit them upside
[01:35:46.000 --> 01:35:47.480]   the head, all of a sudden.
[01:35:47.480 --> 01:35:51.520]   Unfortunately, our website is currently unavailable in most European countries.
[01:35:51.520 --> 01:35:53.720]   Yeah, because we made it so well.
[01:35:53.720 --> 01:35:58.440]   We are engaged on the issue and committed to looking at options that support our full
[01:35:58.440 --> 01:36:01.560]   range of digital offerings to the EU market.
[01:36:01.560 --> 01:36:06.360]   We continue to identify technical compliance solutions that will provide all readers with
[01:36:06.360 --> 01:36:08.200]   our award-winning journalism.
[01:36:08.200 --> 01:36:09.720]   It wasn't just the Chicago Tribune.
[01:36:09.720 --> 01:36:10.720]   It was all of Tronk.
[01:36:10.720 --> 01:36:12.960]   It was the LA Times.
[01:36:12.960 --> 01:36:14.760]   It was the Arizona Daily Star.
[01:36:14.760 --> 01:36:20.320]   I got a call on Saturday on the radio showwoman said, I'm trying to post a link to an article
[01:36:20.320 --> 01:36:23.280]   in the star on Facebook.
[01:36:23.280 --> 01:36:28.320]   It was an article about this horrific practice of separating children from their parents when
[01:36:28.320 --> 01:36:31.640]   they're caught crossing the border, just terrific.
[01:36:31.640 --> 01:36:33.600]   I said, well, I want you to post that.
[01:36:33.600 --> 01:36:34.800]   That's important.
[01:36:34.800 --> 01:36:39.520]   But she said, but I can't because I'm in Santa Cruz, California.
[01:36:39.520 --> 01:36:44.840]   I'm posting this from the Arizona star on Facebook and I get a pop-up that says, because
[01:36:44.840 --> 01:36:47.160]   of GDPR, you can't post this.
[01:36:47.160 --> 01:36:49.000]   We're blocking it.
[01:36:49.000 --> 01:36:54.040]   So I don't know if the star made a mistake on her geolocation or more likely decided,
[01:36:54.040 --> 01:36:58.960]   well, if we post it on Facebook, that might be seen by a European, so we better not allow
[01:36:58.960 --> 01:37:00.960]   Facebook.
[01:37:00.960 --> 01:37:02.920]   It's crazy.
[01:37:02.920 --> 01:37:04.160]   It's poor implementation.
[01:37:04.160 --> 01:37:07.920]   The New York Daily News, the Orlando Sentinel, the Baltimore Sun.
[01:37:07.920 --> 01:37:11.160]   You see, I gave a presentation about this to a bunch of venture capitalists in February
[01:37:11.160 --> 01:37:14.280]   and they honestly thought I was joking when I told them what the fines were and what they'd
[01:37:14.280 --> 01:37:16.080]   have to do.
[01:37:16.080 --> 01:37:19.160]   People just didn't seem to get it in this country.
[01:37:19.160 --> 01:37:23.840]   Europe's been prepared for this and it went smoothly with everyone just had to read today.
[01:37:23.840 --> 01:37:31.320]   So the fine is 4% of your global revenue or 20 million gross, not net, not profit.
[01:37:31.320 --> 01:37:34.480]   It's either 2% or 4%, depending on the throughout either fine.
[01:37:34.480 --> 01:37:37.480]   Or 20 million euros, or 20 million or 40 million euros.
[01:37:37.480 --> 01:37:40.400]   But I guess there's a legitimate question.
[01:37:40.400 --> 01:37:43.480]   Are they going to go after me for 4% of my global revenue because I don't have a proper
[01:37:43.480 --> 01:37:44.480]   policy?
[01:37:44.480 --> 01:37:45.480]   How?
[01:37:45.480 --> 01:37:46.480]   What?
[01:37:46.480 --> 01:37:50.120]   You saw that Facebook and Google have already been hit with $8.8 billion in losses.
[01:37:50.120 --> 01:37:52.920]   That's just Max Shrimms being a worker.
[01:37:52.920 --> 01:37:54.880]   But $8.8 billion.
[01:37:54.880 --> 01:37:55.880]   Yeah.
[01:37:55.880 --> 01:37:57.760]   So this guy, tell me about Shrimms.
[01:37:57.760 --> 01:37:59.160]   He's a privacy activist.
[01:37:59.160 --> 01:38:06.560]   He brought down the agreement that the US and the US and Europe had on data privacy.
[01:38:06.560 --> 01:38:08.600]   Basically he's a nice enough chap.
[01:38:08.600 --> 01:38:12.040]   He obviously cares about what he's doing, but he's just a bit of a zalot about it.
[01:38:12.040 --> 01:38:13.400]   He's been successful in court.
[01:38:13.400 --> 01:38:14.400]   Yeah.
[01:38:14.400 --> 01:38:18.200]   And he has successfully argued that people should have a certain amount of rights over
[01:38:18.200 --> 01:38:21.160]   their data, which is part of where GDP came from.
[01:38:21.160 --> 01:38:23.160]   So actually we should say a bit of a thank you to him.
[01:38:23.160 --> 01:38:26.040]   Well that's the problem with this, isn't it?
[01:38:26.040 --> 01:38:27.840]   It both giveth and taketh away.
[01:38:27.840 --> 01:38:28.840]   Yeah.
[01:38:28.840 --> 01:38:34.120]   I'm still angry that whenever I show up on any site in the UK, for example, I get the
[01:38:34.120 --> 01:38:37.160]   "yes, I have cookies, I have to find the X to close it."
[01:38:37.160 --> 01:38:40.760]   I think that's a very good example of how this regulation is done.
[01:38:40.760 --> 01:38:43.440]   That kind of regulation accomplished nothing.
[01:38:43.440 --> 01:38:48.640]   It just used up a lot of manpower time and my time and attention for something that does
[01:38:48.640 --> 01:38:49.640]   nothing.
[01:38:49.640 --> 01:38:52.240]   Yes, okay, I accept your cookie policy.
[01:38:52.240 --> 01:38:57.640]   And by the way, now everybody's doing it because GDPR has raised that awareness.
[01:38:57.640 --> 01:39:04.080]   So Brian, is this an example of regulation gone wrong or is this the right idea but badly
[01:39:04.080 --> 01:39:05.880]   implemented or is this all perfect?
[01:39:05.880 --> 01:39:11.440]   Well I think it's clear to so many people that they need to be more aware and they need
[01:39:11.440 --> 01:39:15.240]   to care about their privacy and their personal information.
[01:39:15.240 --> 01:39:20.000]   And I think if you consider data, I mean people call data the new oil.
[01:39:20.000 --> 01:39:24.800]   Well there are liabilities if you're an oil company, you have an oil spill.
[01:39:24.800 --> 01:39:26.360]   What are the liabilities in place?
[01:39:26.360 --> 01:39:31.400]   What are the teeth that government might have or that citizens might have if companies
[01:39:31.400 --> 01:39:35.840]   have data spills, which are your 21st century oil spills?
[01:39:35.840 --> 01:39:39.520]   I think as any legislation perfect, no.
[01:39:39.520 --> 01:39:43.560]   It needs to continue to iterate to get better.
[01:39:43.560 --> 01:39:47.320]   And so I think it's important that we're taking first steps and Europe has taken the
[01:39:47.320 --> 01:39:52.080]   plunge first and I hope is that, you know, here in California they're coming up, there's
[01:39:52.080 --> 01:39:57.560]   a new, they're pulling together some new legislation as well.
[01:39:57.560 --> 01:40:02.920]   And so I think, you know, the good thing is that it was implemented across the European
[01:40:02.920 --> 01:40:04.080]   Union.
[01:40:04.080 --> 01:40:09.360]   And I think the challenge is that when each country creates one set of legislation, the
[01:40:09.360 --> 01:40:12.400]   government creates a different, it makes it really hard and creates a lot of friction
[01:40:12.400 --> 01:40:13.800]   for small startups.
[01:40:13.800 --> 01:40:18.800]   Now companies like Facebook and Google, they have huge legal departments, we'll do this.
[01:40:18.800 --> 01:40:23.200]   But you know, Twit for example, you know, doesn't have as big of a legal department
[01:40:23.200 --> 01:40:24.400]   as those two companies.
[01:40:24.400 --> 01:40:30.800]   And so my concern sometimes is that when you have, how hard is this going to be for
[01:40:30.800 --> 01:40:32.800]   smaller startups?
[01:40:32.800 --> 01:40:37.360]   And do we need to create regulatory sandboxes to allow for up to a certain amount of transactions
[01:40:37.360 --> 01:40:42.360]   or volume, et cetera, until you need the full weight of the regulatory overhead that's
[01:40:42.360 --> 01:40:44.360]   being applied here.
[01:40:44.360 --> 01:40:48.720]   So that Twit, you know, is at a certain level, but Facebook and Google are at a much higher
[01:40:48.720 --> 01:40:49.720]   level.
[01:40:49.720 --> 01:40:59.000]   Yeah, I mean, I don't expect, you know, Martha Vestager to come to my house and skiv me
[01:40:59.000 --> 01:41:01.240]   of subpoena.
[01:41:01.240 --> 01:41:03.200]   But I, but what if somebody sued us?
[01:41:03.200 --> 01:41:05.240]   We'd have to respond to that, right?
[01:41:05.240 --> 01:41:10.280]   You have to within 72 hours, give them the data, the data that you have on them, which
[01:41:10.280 --> 01:41:12.680]   isn't unreasonable.
[01:41:12.680 --> 01:41:17.520]   I do think that they could possibly have done it in a slightly more organized way.
[01:41:17.520 --> 01:41:22.200]   But you know, the actual requirements of this, you say about it took you about 30 to 40
[01:41:22.200 --> 01:41:24.240]   hours to, to me.
[01:41:24.240 --> 01:41:25.240]   Thank God.
[01:41:25.240 --> 01:41:26.240]   Well, yes.
[01:41:26.240 --> 01:41:29.960]   And there are websites, I think we used a website where you do a bunch of check boxes.
[01:41:29.960 --> 01:41:30.960]   You pay them a fee.
[01:41:30.960 --> 01:41:32.400]   I mean, PR as a service.
[01:41:32.400 --> 01:41:33.400]   Yeah.
[01:41:33.400 --> 01:41:34.400]   Yeah.
[01:41:34.400 --> 01:41:35.400]   So companies are doing that.
[01:41:35.400 --> 01:41:38.840]   That is basically, is my cloud provider GPT, GDPR compliant?
[01:41:38.840 --> 01:41:39.840]   Yes.
[01:41:39.840 --> 01:41:40.840]   Fine.
[01:41:40.840 --> 01:41:41.840]   Don't need to worry about it.
[01:41:41.840 --> 01:41:43.960]   Some have pointed out that if you're not gathering a lot of information, if you're not,
[01:41:43.960 --> 01:41:47.480]   if you are doing the right thing, this isn't onerous, but there are some onerous things.
[01:41:47.480 --> 01:41:50.840]   For instance, if there's a data breach, and I, I'm kind of in favor of this, but I've,
[01:41:50.840 --> 01:41:54.200]   I've heard that companies are not are a little trouble by it.
[01:41:54.200 --> 01:41:57.440]   If there's a data breach, you have 72 hours to report it to your customers.
[01:41:57.440 --> 01:41:59.040]   Oh, I'm all for that.
[01:41:59.040 --> 01:42:02.800]   I hate companies that have like Yahoo didn't tell anybody for months.
[01:42:02.800 --> 01:42:03.800]   Yeah.
[01:42:03.800 --> 01:42:04.800]   Equifax.
[01:42:04.800 --> 01:42:05.800]   Yeah.
[01:42:05.800 --> 01:42:08.920]   But at the same time, many companies think 72 hours is actually an unreasonable deadline,
[01:42:08.920 --> 01:42:12.120]   not merely for the difficulty, but also because of security issues.
[01:42:12.120 --> 01:42:16.800]   It may not be appropriate to say this is happening until we've locked it down.
[01:42:16.800 --> 01:42:18.680]   We figured out what happened.
[01:42:18.680 --> 01:42:21.680]   And that's 72 hours is not enough time.
[01:42:21.680 --> 01:42:23.320]   So I have mixed feelings about this.
[01:42:23.320 --> 01:42:27.480]   I guess some of this comes down to how it's how assiduously it will be enforced.
[01:42:27.480 --> 01:42:28.720]   That's going to be the key to it.
[01:42:28.720 --> 01:42:33.280]   And I think it'll, it won't be used to based on past experience, whether you use gone on
[01:42:33.280 --> 01:42:37.320]   this and with a competition laws and various other things, it won't be used against small
[01:42:37.320 --> 01:42:39.000]   businesses by and large.
[01:42:39.000 --> 01:42:45.400]   This will be if Facebook or Google or Twitter or one of the big companies mucks it up.
[01:42:45.400 --> 01:42:47.680]   US regulators give them a pass as they always do.
[01:42:47.680 --> 01:42:49.840]   So the EU has to institute some fines.
[01:42:49.840 --> 01:42:51.520]   This is where it's going to be used.
[01:42:51.520 --> 01:42:52.520]   Hmm.
[01:42:52.520 --> 01:42:56.960]   Do you think there's any risk that that might come across as more like anti American tech
[01:42:56.960 --> 01:42:57.960]   companies?
[01:42:57.960 --> 01:42:59.840]   And I'm just raising the question.
[01:42:59.840 --> 01:43:04.200]   Yeah, no, honestly, it's kind of a tricky question because most of the big tech firms
[01:43:04.200 --> 01:43:06.400]   are American.
[01:43:06.400 --> 01:43:10.440]   But I think then they'd be more than willing to go after that, go after European companies
[01:43:10.440 --> 01:43:11.280]   as well.
[01:43:11.280 --> 01:43:16.680]   It reminds me though, a little bit of some of the controversy with Google over snippets
[01:43:16.680 --> 01:43:23.040]   and the shopping issue where it really felt like the EU was responding to the commercial
[01:43:23.040 --> 01:43:26.000]   interests of EU companies.
[01:43:26.000 --> 01:43:33.440]   The snippets is a very good one where European publishers didn't like the fact that Google
[01:43:33.440 --> 01:43:36.240]   was using snippets, scraping the content.
[01:43:36.240 --> 01:43:39.560]   Of course, they changed their mind when Google delisted them in Spain.
[01:43:39.560 --> 01:43:42.720]   I remember that was a bit of a loss for spinach.
[01:43:42.720 --> 01:43:44.560]   It's a little bit useful.
[01:43:44.560 --> 01:43:52.520]   But that to me felt like the EU was responsive to commercial pressure from its own constituents
[01:43:52.520 --> 01:43:55.760]   to the detriment of a United American companies that weren't constituents.
[01:43:55.760 --> 01:44:01.160]   Well, the Google shopping thing was in response to Microsoft actually complaining to the even
[01:44:01.160 --> 01:44:02.160]   worse.
[01:44:02.160 --> 01:44:03.160]   Yeah.
[01:44:03.160 --> 01:44:07.720]   And various other EU sponsored EU organisations complaining.
[01:44:07.720 --> 01:44:11.360]   So I mean, it's a stick that American companies can use as well.
[01:44:11.360 --> 01:44:15.640]   I mean, I guess there's always an element of this just as there have been with, you know,
[01:44:15.640 --> 01:44:20.760]   as we saw with the NSA's equation system being used to spy on European companies.
[01:44:20.760 --> 01:44:25.960]   I mean, this has always gone on at some level, but I think the motives behind GDPR appeal
[01:44:25.960 --> 01:44:27.560]   or as pure as they can be.
[01:44:27.560 --> 01:44:29.840]   These are politicians we're dealing with after all.
[01:44:29.840 --> 01:44:30.920]   And no laws perfect.
[01:44:30.920 --> 01:44:32.880]   And so some of this come out in the...
[01:44:32.880 --> 01:44:37.520]   Brian, I'm sorry, I mean, yeah, no, no, no, no, no, no, no, no, no, no, no, no, no,
[01:44:37.520 --> 01:44:38.520]   President come back, sir.
[01:44:38.520 --> 01:44:41.040]   Why do you become a politician, Brian?
[01:44:41.040 --> 01:44:43.720]   They got to be elected first.
[01:44:43.720 --> 01:44:45.760]   Yeah, I think you're a technologist still.
[01:44:45.760 --> 01:44:49.680]   You're an aspiring politician, which some might question that.
[01:44:49.680 --> 01:44:51.320]   I hope to always be a technologist.
[01:44:51.320 --> 01:44:52.320]   Yeah.
[01:44:52.320 --> 01:44:54.760]   Well, that's one of the reasons I love that.
[01:44:54.760 --> 01:44:56.480]   You know, we need to get people with some brains.
[01:44:56.480 --> 01:44:58.800]   It's what Brianna Wooza regular on our shows as well.
[01:44:58.800 --> 01:44:59.800]   Yeah.
[01:44:59.800 --> 01:45:03.200]   We really need to get some people with some tech chops in Congress.
[01:45:03.200 --> 01:45:07.680]   I'm not even a bureaucrat or you a technologist.
[01:45:07.680 --> 01:45:10.840]   Matt, government bureaucrat or technologist?
[01:45:10.840 --> 01:45:11.840]   Why not both?
[01:45:11.840 --> 01:45:12.840]   He's two.
[01:45:12.840 --> 01:45:13.840]   He's two.
[01:45:13.840 --> 01:45:14.840]   He's two.
[01:45:14.840 --> 01:45:19.600]   I will say I used to use the word bureaucrat much more often before I joined government
[01:45:19.600 --> 01:45:24.160]   service and now I'm much more likely to say civil servant because when you're there
[01:45:24.160 --> 01:45:28.560]   seeing, you know, sometimes sausage getting made, what you really see is a lot of incredibly
[01:45:28.560 --> 01:45:33.920]   dedicated people trying to do the very best that they can, sometimes hampered by red tape.
[01:45:33.920 --> 01:45:37.680]   And you know this, Brian, trying to get to the point of what are the best solutions
[01:45:37.680 --> 01:45:41.440]   that you can get to given the constraints and regulations and laws and all those sorts
[01:45:41.440 --> 01:45:42.440]   of things.
[01:45:42.440 --> 01:45:47.360]   Yeah, you actually said it, Brian, earlier that it's the, I think when you sit in the
[01:45:47.360 --> 01:45:48.840]   White House, you probably see that.
[01:45:48.840 --> 01:45:53.720]   It's the people who are who go from administration to administration year after year who are really
[01:45:53.720 --> 01:45:54.720]   doing the people's work.
[01:45:54.720 --> 01:45:55.720]   Yeah.
[01:45:55.720 --> 01:45:57.840]   And those are the people on the ground.
[01:45:57.840 --> 01:46:04.920]   And I'm consistently impressed not by their commitment to doing the right thing independent
[01:46:04.920 --> 01:46:07.840]   of parties and politics and politics and party.
[01:46:07.840 --> 01:46:08.840]   Yeah.
[01:46:08.840 --> 01:46:09.840]   I think that's great.
[01:46:09.840 --> 01:46:10.840]   And you're right.
[01:46:10.840 --> 01:46:11.840]   They should be called civil servants.
[01:46:11.840 --> 01:46:12.840]   Yeah.
[01:46:12.840 --> 01:46:15.840]   And it's a very, this is part of the American mythos that all bureaucrats are bad, right?
[01:46:15.840 --> 01:46:22.280]   Well, I think that was one of the most untouted accomplishments of the Obama administration
[01:46:22.280 --> 01:46:26.200]   was the fact that, you know, you mentioned Todd Park earlier and what a great salesman
[01:46:26.200 --> 01:46:27.200]   he was.
[01:46:27.200 --> 01:46:32.240]   And what he did is he just reframed the argument about why technologists should serve in our
[01:46:32.240 --> 01:46:33.360]   government.
[01:46:33.360 --> 01:46:38.240]   And this goes back to the ethos that people have in tech companies about making the world
[01:46:38.240 --> 01:46:39.240]   a little bit better.
[01:46:39.240 --> 01:46:42.920]   And I remember that when I went into the White House and I was one of the early technologists
[01:46:42.920 --> 01:46:45.680]   to do it, I got a lot of flack from my tech friends.
[01:46:45.680 --> 01:46:48.760]   Who said, "Why would you want to go into this monolithic bureaucracy?
[01:46:48.760 --> 01:46:50.200]   What are you going to accomplish there?"
[01:46:50.200 --> 01:46:53.520]   My immediate retort was, "How many users does your little app have?"
[01:46:53.520 --> 01:46:54.520]   Finally.
[01:46:54.520 --> 01:46:59.360]   Well, we got more than 300 million Americans and we have a much bigger platform than you.
[01:46:59.360 --> 01:47:04.800]   And so there's a huge opportunity, but it took Todd Park to literally reframe the conversation
[01:47:04.800 --> 01:47:09.960]   and say, "Do, you know, have you done your tour of duty in the US government?"
[01:47:09.960 --> 01:47:12.400]   And don't make it about, you know, you're going to government, you're going to have
[01:47:12.400 --> 01:47:14.120]   to work there for 30 years.
[01:47:14.120 --> 01:47:16.840]   You know, you're going in for a couple of years and you're going to make a really big
[01:47:16.840 --> 01:47:18.000]   impact.
[01:47:18.000 --> 01:47:23.120]   And the question is, for everyone in the tech community, have you done your two years?
[01:47:23.120 --> 01:47:24.120]   I love it.
[01:47:24.120 --> 01:47:27.080]   I did my few years when I served in the Peace Corps and I served in the White House, but
[01:47:27.080 --> 01:47:30.600]   you know, we should be asking every technologist, "If you hope to make the world a better place,
[01:47:30.600 --> 01:47:32.240]   have you done your time in the US government?"
[01:47:32.240 --> 01:47:33.240]   That's absolutely true.
[01:47:33.240 --> 01:47:34.520]   Or the state government or the state government.
[01:47:34.520 --> 01:47:38.120]   I've always been in favor of compulsory service.
[01:47:38.120 --> 01:47:43.280]   Not necessarily in the armed services, but in the Peace Corps or in the USDS, some sort
[01:47:43.280 --> 01:47:45.880]   of service required of everybody coming out of college.
[01:47:45.880 --> 01:47:49.360]   Well, and leave out the compulsory part, just the fact that there are people who are willing
[01:47:49.360 --> 01:47:50.360]   to do this.
[01:47:50.360 --> 01:47:54.680]   What I have seen is people often think they're going to go do six months or a year or two
[01:47:54.680 --> 01:47:57.920]   years of government service and then go back to their company.
[01:47:57.920 --> 01:48:02.280]   And what's fascinating to me is many of these folks end up making a change to their lives,
[01:48:02.280 --> 01:48:05.520]   where they do a left turn or a right turn from what they planned.
[01:48:05.520 --> 01:48:09.240]   We've seen people who started the Massachusetts Digital Service, who helped start the California
[01:48:09.240 --> 01:48:10.440]   Digital Service.
[01:48:10.440 --> 01:48:14.800]   We've seen people who did startups afterwards that try to improve things with civic tech.
[01:48:14.800 --> 01:48:18.880]   So it really opens up a completely different lens on how you view things.
[01:48:18.880 --> 01:48:22.680]   And to exactly the Brian's point, the impact that you can have on people.
[01:48:22.680 --> 01:48:27.800]   I think a lot of geeks think it's cool to be arch, to be sardonic, to be cynical about
[01:48:27.800 --> 01:48:29.800]   government.
[01:48:29.800 --> 01:48:35.320]   And there's plenty of things you can point at, and you know, government cheese, somebody
[01:48:35.320 --> 01:48:37.720]   just said in the chat room.
[01:48:37.720 --> 01:48:43.880]   But let's make it cool to care about our country enough to serve it and to make a difference
[01:48:43.880 --> 01:48:44.880]   and to take some time.
[01:48:44.880 --> 01:48:46.840]   I think that's where we should be headed.
[01:48:46.840 --> 01:48:49.200]   Could I tell one one minute story?
[01:48:49.200 --> 01:48:53.480]   We had something at Veterans Affairs where veterans were trying to get their health benefits
[01:48:53.480 --> 01:48:56.520]   and they had to use the right version of Internet Explorer.
[01:48:56.520 --> 01:48:58.040]   And it had to be Internet Explorer.
[01:48:58.040 --> 01:48:59.480]   You couldn't use another browser.
[01:48:59.480 --> 01:49:01.800]   And you had to use the right version of Adobe Acrobat.
[01:49:01.800 --> 01:49:05.560]   And if you didn't have the right version of both of those, it would tell you you need
[01:49:05.560 --> 01:49:08.000]   to upgrade your version of Adobe Acrobat.
[01:49:08.000 --> 01:49:09.000]   And that was wrong.
[01:49:09.000 --> 01:49:11.800]   You actually needed to downgrade your version of Adobe Acrobat.
[01:49:11.800 --> 01:49:14.840]   Which meant everybody ended up using the paper version of this form.
[01:49:14.840 --> 01:49:16.800]   It's called the 1010EZ.
[01:49:16.800 --> 01:49:22.440]   When you fill out the paper version of this form, you have to wait 137 days on average
[01:49:22.440 --> 01:49:24.160]   to get your health benefits.
[01:49:24.160 --> 01:49:29.080]   So we implemented this crazy revolutionary new technology along with the VA.
[01:49:29.080 --> 01:49:31.400]   We call it a web form.
[01:49:31.400 --> 01:49:32.240]   It's responsive.
[01:49:32.240 --> 01:49:33.560]   So it works on mobile phones.
[01:49:33.560 --> 01:49:35.880]   It's accessible so it works for blind users.
[01:49:35.880 --> 01:49:40.200]   Now 50% of veterans find out whether they're eligible for their benefits in 10 minutes.
[01:49:40.200 --> 01:49:41.440]   Isn't that awesome?
[01:49:41.440 --> 01:49:43.480]   We just need more technologists and designers today.
[01:49:43.480 --> 01:49:47.520]   And half the people listening probably could code that in a couple of hours.
[01:49:47.520 --> 01:49:48.520]   And we need the people.
[01:49:48.520 --> 01:49:49.520]   Yeah.
[01:49:49.520 --> 01:49:50.520]   Yeah.
[01:49:50.520 --> 01:49:52.520]   And tech is bipartisan.
[01:49:52.520 --> 01:49:58.240]   So when we were in the CTO's office, we had bipartisan support.
[01:49:58.240 --> 01:50:02.080]   Because on the right and the left, we all want to make our government more efficient,
[01:50:02.080 --> 01:50:04.240]   more effective, more transparent.
[01:50:04.240 --> 01:50:10.760]   And in fact, the very last bill that President Obama signed was enshrining into law the Presidential
[01:50:10.760 --> 01:50:16.880]   Innovation Fellowship Program, which was the precursor and the inspiration for the United
[01:50:16.880 --> 01:50:18.480]   States Digital Service.
[01:50:18.480 --> 01:50:19.980]   Yeah.
[01:50:19.980 --> 01:50:20.980]   This is nonpartisan work.
[01:50:20.980 --> 01:50:27.880]   I've got $100 million projects that might go off the rails for lack of three engineers,
[01:50:27.880 --> 01:50:29.760]   two designers and one product manager.
[01:50:29.760 --> 01:50:31.760]   So USTS.gov/join.
[01:50:31.760 --> 01:50:32.760]   Sorry.
[01:50:32.760 --> 01:50:35.560]   No, no, I think it's a great idea.
[01:50:35.560 --> 01:50:38.840]   The problem is when you've got students coming out of university with a bunch of student
[01:50:38.840 --> 01:50:44.000]   debt, the first thought is get into private industry, make them a ton of money, pay off
[01:50:44.000 --> 01:50:45.000]   my debts.
[01:50:45.000 --> 01:50:49.080]   And then I think it's going to be easier to actually pull them into government service.
[01:50:49.080 --> 01:50:54.520]   But hey, you could even get a scheme whereby if you agree to serve with a government for
[01:50:54.520 --> 01:50:57.440]   two or three years, they pay your college tuition.
[01:50:57.440 --> 01:51:02.680]   It's definitely needed because the state of IT knowledge, IT infrastructure in the US
[01:51:02.680 --> 01:51:05.400]   government, it makes the UK look good.
[01:51:05.400 --> 01:51:07.400]   That takes some dirt.
[01:51:07.400 --> 01:51:08.400]   That's OK.
[01:51:08.400 --> 01:51:11.600]   Well, and what's fascinating to me is you see this moving around the world.
[01:51:11.600 --> 01:51:12.600]   So it's not just the United States.
[01:51:12.600 --> 01:51:14.480]   There's the Canadian Digital Service.
[01:51:14.480 --> 01:51:18.120]   They just had an Amazon executive go to Italy to do the similar thing.
[01:51:18.120 --> 01:51:21.960]   Of course, the UK is one of the originals because they have the government digital service,
[01:51:21.960 --> 01:51:23.600]   which actually has the purse strings.
[01:51:23.600 --> 01:51:28.240]   So if you want to make any large IT project, you actually have to get approval by folks
[01:51:28.240 --> 01:51:29.960]   who know technology.
[01:51:29.960 --> 01:51:31.880]   So you're seeing this progress around the world.
[01:51:31.880 --> 01:51:34.000]   I love it.
[01:51:34.000 --> 01:51:36.280]   There are people who say, well, just privatize it.
[01:51:36.280 --> 01:51:37.680]   Let private industry do it.
[01:51:37.680 --> 01:51:45.840]   But I think about the-- I think about the-- I think it was the Obamacare website, the
[01:51:45.840 --> 01:51:49.240]   ACO website, which was healthcare.gov was outsourced.
[01:51:49.240 --> 01:51:50.240]   It was supposed to be a paradise.
[01:51:50.240 --> 01:51:51.240]   Yeah.
[01:51:51.240 --> 01:51:52.960]   And that did not work so well.
[01:51:52.960 --> 01:51:56.880]   In fact, didn't the-- I think the USDS rode to the rescue.
[01:51:56.880 --> 01:51:57.880]   Yes, that's right.
[01:51:57.880 --> 01:51:59.760]   And that's how it was created, wasn't it?
[01:51:59.760 --> 01:52:01.280]   That's a VACA's website.
[01:52:01.280 --> 01:52:02.520]   That's exactly right.
[01:52:02.520 --> 01:52:06.880]   And I'm not saying that the digital service needs to do every government service, no,
[01:52:06.880 --> 01:52:07.880]   not at all.
[01:52:07.880 --> 01:52:09.760]   There's absolutely a place for contractors.
[01:52:09.760 --> 01:52:13.680]   And there's a new wave of contractors that have happened after healthcare.gov.
[01:52:13.680 --> 01:52:17.800]   But you still need someone in government service who speaks both languages, technology,
[01:52:17.800 --> 01:52:21.440]   as well as how things work in government, to translate it even for contractors.
[01:52:21.440 --> 01:52:24.840]   Look, it's a responsive web form.
[01:52:24.840 --> 01:52:28.920]   Go to usds.join, even on your smartphone, right?
[01:52:28.920 --> 01:52:29.920]   usds.gov.
[01:52:29.920 --> 01:52:30.920]   That'll work on--
[01:52:30.920 --> 01:52:31.920]   I'm sorry.
[01:52:31.920 --> 01:52:32.920]   Yeah, usds.gov/join.
[01:52:32.920 --> 01:52:33.920]   Thank you.
[01:52:33.920 --> 01:52:34.920]   Let's get it right.
[01:52:34.920 --> 01:52:35.920]   Thank you.
[01:52:35.920 --> 01:52:38.320]   And actually, it doesn't have to be a kid coming out of school.
[01:52:38.320 --> 01:52:44.200]   I think probably your best employees-- best people are-- people come from a company like
[01:52:44.200 --> 01:52:47.920]   you did from Google with a lot of tenure, a lot of experience who want to take a little
[01:52:47.920 --> 01:52:49.280]   time off and help.
[01:52:49.280 --> 01:52:53.400]   Yeah, we actually look for people who are like five years into their career often.
[01:52:53.400 --> 01:52:55.160]   And it's not a matter of time.
[01:52:55.160 --> 01:52:56.480]   It's a matter of experience.
[01:52:56.480 --> 01:52:57.480]   You want battle scars.
[01:52:57.480 --> 01:53:01.160]   You want the experience setting up large web skill services and APIs and that sort of thing.
[01:53:01.160 --> 01:53:02.160]   Yeah.
[01:53:02.160 --> 01:53:05.760]   I mean, given that IBM is currently firing-- or sorry, resource-allocating most of its
[01:53:05.760 --> 01:53:11.240]   older employees as well, there's a vast reservoir of talent there that could be tapped into.
[01:53:11.240 --> 01:53:12.400]   Yeah.
[01:53:12.400 --> 01:53:13.960]   Let's fix this.
[01:53:13.960 --> 01:53:14.960]   Yeah.
[01:53:14.960 --> 01:53:15.960]   Yeah.
[01:53:15.960 --> 01:53:18.880]   So, yes.gov/join.
[01:53:18.880 --> 01:53:24.960]   And while you're at it, f-o-r-d-e.com, if you're in the 45th District of California,
[01:53:24.960 --> 01:53:26.000]   you know what to do.
[01:53:26.000 --> 01:53:27.480]   You know what to do.
[01:53:27.480 --> 01:53:33.200]   The first-- do you-- you were-- you're dubbed by Bloomberg, the first crypto candidate for
[01:53:33.200 --> 01:53:34.200]   Congress.
[01:53:34.200 --> 01:53:35.200]   Congress.
[01:53:35.200 --> 01:53:37.200]   Do you like that title?
[01:53:37.200 --> 01:53:39.800]   I mean, what are you going to do?
[01:53:39.800 --> 01:53:41.600]   Yeah, what are you going to do?
[01:53:41.600 --> 01:53:48.240]   They gave me the name, but I think, you know, what's important is that how can legislators
[01:53:48.240 --> 01:53:55.640]   hope to create laws on technology that they don't use?
[01:53:55.640 --> 01:54:01.680]   And so invariably, our legislators are going to have to make laws on cryptocurrency, drones,
[01:54:01.680 --> 01:54:06.520]   autonomous vehicles, artificial intelligence, CRISPR, like name your technology.
[01:54:06.520 --> 01:54:08.120]   It's all out there.
[01:54:08.120 --> 01:54:10.920]   But have any of them used them?
[01:54:10.920 --> 01:54:11.920]   Yeah.
[01:54:11.920 --> 01:54:18.760]   And so, you know, I often say, when I'm explaining, you know, Bitcoin and how crazy thing-- think
[01:54:18.760 --> 01:54:21.960]   Bitcoin is, I say, well, you know, how many of you in this room could actually code a
[01:54:21.960 --> 01:54:23.800]   mobile application?
[01:54:23.800 --> 01:54:27.000]   And they, you know, barely any hands go up.
[01:54:27.000 --> 01:54:31.360]   And I say, well, how many of you have been in a situation where you confronted a problem
[01:54:31.360 --> 01:54:34.360]   with your coworkers, your colleagues, and you said, well, we need a mobile application
[01:54:34.360 --> 01:54:35.960]   for that?
[01:54:35.960 --> 01:54:40.160]   And you-- and why did you suggest that if you don't know how to build a mobile application?
[01:54:40.160 --> 01:54:43.360]   Well, because you use mobile applications every single day, you know that you can use
[01:54:43.360 --> 01:54:47.520]   the internet, that has GPS, you can take photos, you can do all kinds of things.
[01:54:47.520 --> 01:54:51.520]   But if you're not using cryptocurrency or you're not using any of these other new technologies,
[01:54:51.520 --> 01:54:56.080]   then how on earth are you going to be on the right side of history for technology and
[01:54:56.080 --> 01:54:57.520]   for our society?
[01:54:57.520 --> 01:55:01.360]   If you've never used this technology, and I think what was most concerning to so many
[01:55:01.360 --> 01:55:06.120]   people in watching the Congressional hearings with Mark Zuckerberg was that it was self-evident
[01:55:06.120 --> 01:55:10.720]   to everyone that the legislators don't even use Facebook.
[01:55:10.720 --> 01:55:12.360]   And this is social media.
[01:55:12.360 --> 01:55:15.880]   That's so simple compared to any of these other emerging technologies.
[01:55:15.880 --> 01:55:20.600]   That should deeply worry us if we believe that the future of our economy and our society
[01:55:20.600 --> 01:55:23.840]   is going to be confronting 21st century problems.
[01:55:23.840 --> 01:55:27.760]   It was one senator who didn't even seem to know the difference between Facebook and
[01:55:27.760 --> 01:55:29.080]   Twitter, I remember.
[01:55:29.080 --> 01:55:30.080]   Oh.
[01:55:30.080 --> 01:55:31.080]   Right?
[01:55:31.080 --> 01:55:35.080]   Yeah, on the Instagram Shaftes House as well.
[01:55:35.080 --> 01:55:42.760]   If I email says a senator's Shaftes, someone over a WhatsApp, can Facebook see that?
[01:55:42.760 --> 01:55:44.080]   Senator, no WhatsApp is encrypted.
[01:55:44.080 --> 01:55:45.080]   We can't see any of that.
[01:55:45.080 --> 01:55:46.080]   Yeah.
[01:55:46.080 --> 01:55:50.880]   But if I message someone about Black Panther on WhatsApp, will I get ads about Black Panther
[01:55:50.880 --> 01:55:52.360]   on Facebook?
[01:55:52.360 --> 01:55:53.360]   No.
[01:55:53.360 --> 01:55:55.800]   Actually, that's a pretty good question.
[01:55:55.800 --> 01:56:01.280]   I don't know if that's the right answer, but I don't know if it also came from a fundamental
[01:56:01.280 --> 01:56:02.760]   understanding.
[01:56:02.760 --> 01:56:06.640]   Mr. Zuckerberg asked Chuck Grassley, what is a poke?
[01:56:06.640 --> 01:56:07.640]   Oh, no.
[01:56:07.640 --> 01:56:09.640]   I'll ask you why.
[01:56:09.640 --> 01:56:10.640]   Yeah.
[01:56:10.640 --> 01:56:11.640]   What?
[01:56:11.640 --> 01:56:13.640]   No, I don't know.
[01:56:13.640 --> 01:56:16.200]   My son, Senator Corbin, my son is on Instagram.
[01:56:16.200 --> 01:56:17.200]   Oh, that was crazy.
[01:56:17.200 --> 01:56:24.360]   He asked me to mention that he is a just glasses, Blaze Boy 420XX and no, no, no.
[01:56:24.360 --> 01:56:29.320]   I don't think that's the actual URL.
[01:56:29.320 --> 01:56:31.760]   Our show today brought to you by our fresh air.
[01:56:31.760 --> 01:56:34.240]   Do you, are you enjoying the fresh air in the studio today?
[01:56:34.240 --> 01:56:35.400]   Have you noticed?
[01:56:35.400 --> 01:56:37.200]   Have you noticed how clean it is?
[01:56:37.200 --> 01:56:41.000]   No smell of volatile organic compounds smoke.
[01:56:41.000 --> 01:56:45.120]   No, no, no black mold, nothing.
[01:56:45.120 --> 01:56:48.560]   It's because we're clean air with molecule in the studio.
[01:56:48.560 --> 01:56:50.200]   And actually I love the molecules so much.
[01:56:50.200 --> 01:56:53.360]   We have one in our bedroom at home and our son's bedroom at home.
[01:56:53.360 --> 01:56:57.560]   Molecule is a brand new kind of air purifier.
[01:56:57.560 --> 01:57:01.720]   50 years ago, actually more than that in the 40s, the HEPA filter technology was in
[01:57:01.720 --> 01:57:02.720]   VEDA.
[01:57:02.720 --> 01:57:06.000]   This is just a very fine filter.
[01:57:06.000 --> 01:57:11.240]   But it doesn't really catch the smallest molecules.
[01:57:11.240 --> 01:57:18.200]   That's why molecule invented the Pico technology, photoelectrochemical oxidation to capture pollutants
[01:57:18.200 --> 01:57:22.520]   a thousand times smaller than those a HEPA filter can capture.
[01:57:22.520 --> 01:57:28.600]   That includes allergens, mold, bacteria, viruses, airborne chemicals.
[01:57:28.600 --> 01:57:31.880]   It makes it a lot easier to cope with asthma and I can vouch for allergies.
[01:57:31.880 --> 01:57:34.440]   We live in an area where there's a lot of pollen and dust.
[01:57:34.440 --> 01:57:39.280]   Our cats come in every morning covered in pollen, believe it or not.
[01:57:39.280 --> 01:57:43.000]   And we have the molecule and ever since we put the molecule in the bedroom, my wife says
[01:57:43.000 --> 01:57:44.280]   she's not woken up with a headache.
[01:57:44.280 --> 01:57:46.240]   She used to wake up with a headache every morning.
[01:57:46.240 --> 01:57:47.520]   Nose all stuffed up.
[01:57:47.520 --> 01:57:50.160]   The molecule is amazing.
[01:57:50.160 --> 01:57:56.600]   Funded by the EPA, proud to say, extensively tested by real people was verified by third
[01:57:56.600 --> 01:58:00.760]   parties in the university laboratories like the University of Southern Florida's Center
[01:58:00.760 --> 01:58:06.120]   for Biological Defense and the University of Minnesota's Particle Calibration Laboratory.
[01:58:06.120 --> 01:58:07.680]   It is a beautiful device.
[01:58:07.680 --> 01:58:10.760]   That's kind of the apple of air purifiers.
[01:58:10.760 --> 01:58:13.160]   It's a solid aluminum shell.
[01:58:13.160 --> 01:58:14.520]   They have a filter subscription.
[01:58:14.520 --> 01:58:15.600]   If you want, you don't have to.
[01:58:15.600 --> 01:58:21.560]   You could put your molecule on Wi-Fi and then it will automatically request more filters
[01:58:21.560 --> 01:58:22.720]   when the filters get used up.
[01:58:22.720 --> 01:58:24.960]   It's about, we get about six months on our filter.
[01:58:24.960 --> 01:58:28.760]   It's nice though that the filter comes automatically and it's easy to put in.
[01:58:28.760 --> 01:58:29.760]   You don't have to do that.
[01:58:29.760 --> 01:58:30.760]   It has a button on the top.
[01:58:30.760 --> 01:58:36.480]   You can turn it off and on for people who are paranoid about their air filter status,
[01:58:36.480 --> 01:58:38.640]   being broadcast over the internet or something.
[01:58:38.640 --> 01:58:40.300]   I don't think that's a problem.
[01:58:40.300 --> 01:58:42.440]   But I love the molecule.
[01:58:42.440 --> 01:58:43.560]   We're very happy.
[01:58:43.560 --> 01:58:45.960]   We have a good deal for you.
[01:58:45.960 --> 01:58:48.840]   $2.75 off your first order.
[01:58:48.840 --> 01:58:54.160]   If you visit molecule.com, M-O-L-E-K-U-L-E.com and enter the promo code TWIT.
[01:58:54.160 --> 01:58:55.520]   We bought one in the bedroom.
[01:58:55.520 --> 01:58:56.520]   It works so well.
[01:58:56.520 --> 01:58:57.920]   We bought one for Michael.
[01:58:57.920 --> 01:58:59.880]   We bought one for the studio.
[01:58:59.880 --> 01:59:02.520]   We actually really needed it during the fires up in Sonoma County.
[01:59:02.520 --> 01:59:05.760]   We're so smoky.
[01:59:05.760 --> 01:59:06.960]   This makes a big difference.
[01:59:06.960 --> 01:59:13.120]   For $75 off your first order, molecule.com promo code TWIT.
[01:59:13.120 --> 01:59:15.320]   In the early days, I love this article on the outline.
[01:59:15.320 --> 01:59:19.680]   In the early days of Twitter, only one celebrity could tweet at a time.
[01:59:19.680 --> 01:59:21.160]   This is a great story about scaling.
[01:59:21.160 --> 01:59:22.680]   It really is everything you do with tech news.
[01:59:22.680 --> 01:59:27.480]   Evan Weaver, who was former technical lead in architecture, Twitter, talked about the
[01:59:27.480 --> 01:59:29.160]   early days.
[01:59:29.160 --> 01:59:34.280]   He said, "When Demi Moore and Ashton Kutcher, who were the first real celebrities," remember
[01:59:34.280 --> 01:59:35.280]   that?
[01:59:35.280 --> 01:59:36.280]   Yeah.
[01:59:36.280 --> 01:59:45.880]   "Mr. and Mrs. Kutcher, when they were on in 2009, it was required that if either of
[01:59:45.880 --> 01:59:50.160]   them sent a tweet, it would take so long because they had so many followers that the
[01:59:50.160 --> 01:59:52.520]   whole thing would go down for minutes.
[01:59:52.520 --> 01:59:57.080]   Literally, nobody would be able to tweet for several minutes.
[01:59:57.080 --> 02:00:01.480]   But he said, "We had to manually run a script on a laptop that would specifically check
[02:00:01.480 --> 02:00:05.480]   if they had tweeted to each other so that they would see it in real time."
[02:00:05.480 --> 02:00:06.480]   Okay.
[02:00:06.480 --> 02:00:10.520]   "Keller Highwater," he said, "We had to make sure nothing could go wrong."
[02:00:10.520 --> 02:00:14.720]   So, I mean, there was one person at Twitter and it's like, "You're the Demi Moore,
[02:00:14.720 --> 02:00:15.720]   Ashton Kutcher."
[02:00:15.720 --> 02:00:16.720]   Yes.
[02:00:16.720 --> 02:00:18.040]   "The Ashton Kutcher feature."
[02:00:18.040 --> 02:00:19.040]   Yes.
[02:00:19.040 --> 02:00:22.160]   "Sit there and wait for them to brain fart and make sure it gets online quickly."
[02:00:22.160 --> 02:00:27.320]   Jason Goldman, who was VP of product at Twitter between 2007 and 2010, said, "Twitter was
[02:00:27.320 --> 02:00:30.600]   held together by sheer force of will in those days."
[02:00:30.600 --> 02:00:31.600]   No.
[02:00:31.600 --> 02:00:32.880]   And sometimes not even then.
[02:00:32.880 --> 02:00:34.640]   And barely even, I remember the fail whale.
[02:00:34.640 --> 02:00:35.640]   I haven't seen that in a while.
[02:00:35.640 --> 02:00:36.640]   Yeah.
[02:00:36.640 --> 02:00:38.600]   Kind of missed the old fail whale.
[02:00:38.600 --> 02:00:40.840]   Oh my goodness.
[02:00:40.840 --> 02:00:41.840]   I know.
[02:00:41.840 --> 02:00:44.880]   Well, you know, in the early days of Google, they used to update once a month.
[02:00:44.880 --> 02:00:50.120]   They would have a monthly index, which is great until there's some new virus and suddenly
[02:00:50.120 --> 02:00:52.120]   that's breaking news.
[02:00:52.120 --> 02:00:53.120]   And the winner...
[02:00:53.120 --> 02:00:54.360]   And your data is a month old?
[02:00:54.360 --> 02:00:55.360]   Yeah.
[02:00:55.360 --> 02:00:56.360]   Oh, yeah.
[02:00:56.360 --> 02:00:57.360]   And what?
[02:00:57.360 --> 02:01:00.040]   We didn't have daily updates until like 2003 or something.
[02:01:00.040 --> 02:01:01.040]   What?
[02:01:01.040 --> 02:01:02.040]   Yeah.
[02:01:02.040 --> 02:01:04.040]   That's back when it was in a tilt-up and it was Larry and Sergey and a couple of servers
[02:01:04.040 --> 02:01:05.040]   in the house.
[02:01:05.040 --> 02:01:06.040]   Yeah.
[02:01:06.040 --> 02:01:08.200]   I remember visiting Yahoo in the early days.
[02:01:08.200 --> 02:01:10.600]   And Jerry's and Dave are sitting there.
[02:01:10.600 --> 02:01:12.880]   It was literally a tilt-up and out of view.
[02:01:12.880 --> 02:01:17.320]   And you could go in a closet and say, "Yeah, there's the Yahoo server."
[02:01:17.320 --> 02:01:18.320]   That's the one.
[02:01:18.320 --> 02:01:19.320]   That's the one.
[02:01:19.320 --> 02:01:22.480]   So, yeah, that's it.
[02:01:22.480 --> 02:01:27.960]   Lots of people complaining about 1803, not the year the update to Windows 10.
[02:01:27.960 --> 02:01:28.960]   Oh.
[02:01:28.960 --> 02:01:30.600]   Turn the update from hell.
[02:01:30.600 --> 02:01:31.600]   Yes.
[02:01:31.600 --> 02:01:32.600]   This is the latest.
[02:01:32.600 --> 02:01:33.600]   Delayed and buggy.
[02:01:33.600 --> 02:01:34.600]   Two Chromebooks.
[02:01:34.600 --> 02:01:35.600]   Two Chromebooks.
[02:01:35.600 --> 02:01:36.600]   It's a double Chromebook.
[02:01:36.600 --> 02:01:37.600]   Side by side.
[02:01:37.600 --> 02:01:38.600]   And not even the Pixelbook.
[02:01:38.600 --> 02:01:39.600]   These are the classics.
[02:01:39.600 --> 02:01:40.600]   These are the good ones.
[02:01:40.600 --> 02:01:41.600]   Yeah.
[02:01:41.600 --> 02:01:42.600]   Why don't you have a Pixelbook?
[02:01:42.600 --> 02:01:43.600]   I tried it with the screens a little small.
[02:01:43.600 --> 02:01:44.600]   Oh, you liked it a little bit bigger.
[02:01:44.600 --> 02:01:45.600]   I like it a little bigger screen.
[02:01:45.600 --> 02:01:46.600]   Actually, Acer announced.
[02:01:46.600 --> 02:01:47.600]   I'm not gonna...
[02:01:47.600 --> 02:01:48.600]   My lids will stick it.
[02:01:48.600 --> 02:01:49.600]   I can't...
[02:01:49.600 --> 02:01:50.600]   It's just like...
[02:01:50.600 --> 02:01:51.600]   I can't really...
[02:01:51.600 --> 02:01:54.920]   That's why I get a new laptop because the sticker layers get too deep and I need to...
[02:01:54.920 --> 02:01:55.920]   Yeah.
[02:01:55.920 --> 02:01:57.960]   I just peel it off in one layer and put it in the Smiths.
[02:01:57.960 --> 02:01:58.960]   I'll keep you archaeologist.
[02:01:58.960 --> 02:01:59.960]   Yeah, so just...
[02:01:59.960 --> 02:02:00.920]   I'm using the surface...
[02:02:00.920 --> 02:02:03.560]   Much to my dismay, by the way, the Surface Studio.
[02:02:03.560 --> 02:02:06.280]   I love this computer and I'm on the screen all the touch.
[02:02:06.280 --> 02:02:07.520]   But lately it's beginning a little wonky.
[02:02:07.520 --> 02:02:08.600]   I think we're gonna have to...
[02:02:08.600 --> 02:02:09.600]   Maybe it's a 20.
[02:02:09.600 --> 02:02:10.840]   It could be 1803, the update.
[02:02:10.840 --> 02:02:11.840]   Oh.
[02:02:11.840 --> 02:02:12.840]   Oh, yeah.
[02:02:12.840 --> 02:02:13.840]   What are you using, Brian?
[02:02:13.840 --> 02:02:14.840]   Are you on a Mac?
[02:02:14.840 --> 02:02:15.840]   A little Mac book.
[02:02:15.840 --> 02:02:16.840]   Yeah.
[02:02:16.840 --> 02:02:17.840]   That's what I figured.
[02:02:17.840 --> 02:02:23.040]   We are the only campaign in the country that has a 500 that has a CTO.
[02:02:23.040 --> 02:02:24.040]   Oh, really?
[02:02:24.040 --> 02:02:26.040]   That's kind of hard to believe.
[02:02:26.040 --> 02:02:27.040]   It's not...
[02:02:27.040 --> 02:02:28.040]   It's the same time we know that.
[02:02:28.040 --> 02:02:29.040]   What is it?
[02:02:29.040 --> 02:02:30.040]   So, campaign tech is...
[02:02:30.040 --> 02:02:31.040]   I remember walking into the...
[02:02:31.040 --> 02:02:34.800]   I think when we first walked in the White House there was one laptop for the entire White
[02:02:34.800 --> 02:02:35.800]   House.
[02:02:35.800 --> 02:02:36.800]   What?
[02:02:36.800 --> 02:02:37.800]   That was the gift we got to go...
[02:02:37.800 --> 02:02:38.800]   This is 2008.
[02:02:38.800 --> 02:02:39.800]   Yeah.
[02:02:39.800 --> 02:02:40.800]   That's insane.
[02:02:40.800 --> 02:02:41.800]   Yeah.
[02:02:41.800 --> 02:02:42.800]   And I remember...
[02:02:42.800 --> 02:02:47.320]   I mean, we would just bang our heads against the wall with the way that we were building
[02:02:47.320 --> 02:02:49.160]   out technology.
[02:02:49.160 --> 02:02:53.720]   And me walking into a campaign was just like White House Redux.
[02:02:53.720 --> 02:02:57.320]   Just, oh my gosh, campaign tech is miserable.
[02:02:57.320 --> 02:03:01.360]   Wait, if a campaign doesn't have a CTO, what is it?
[02:03:01.360 --> 02:03:04.600]   Uncle Joey's sister's brother is doing the stuff?
[02:03:04.600 --> 02:03:05.600]   I mean, we're...
[02:03:05.600 --> 02:03:06.600]   It's just like...
[02:03:06.600 --> 02:03:09.440]   There's a couple of companies that kind of control it.
[02:03:09.440 --> 02:03:10.440]   Ah, really.
[02:03:10.440 --> 02:03:12.040]   It's pretty bad.
[02:03:12.040 --> 02:03:18.160]   And so, we actually brought in a CTO, a performed open heart surgery, went on...
[02:03:18.160 --> 02:03:22.600]   We use a different platform instead of the standard platforms to really tackle the problem.
[02:03:22.600 --> 02:03:25.880]   But one thing that we do do is we left source our software.
[02:03:25.880 --> 02:03:31.880]   So all the software that we build out, we open it for free to other progressive campaigns.
[02:03:31.880 --> 02:03:34.080]   One needs it.
[02:03:34.080 --> 02:03:35.520]   No Republicans allowed?
[02:03:35.520 --> 02:03:36.520]   Mm.
[02:03:36.520 --> 02:03:38.880]   Until they started fighting for net neutrality.
[02:03:38.880 --> 02:03:41.880]   Oh, I love it.
[02:03:41.880 --> 02:03:42.880]   Salty.
[02:03:42.880 --> 02:03:48.240]   Comcast is now in the bidding war for Hulu.
[02:03:48.240 --> 02:03:53.560]   Remember that the 21st century Fox went up for sale and Disney said we're going to offer
[02:03:53.560 --> 02:03:56.400]   $52 billion for it.
[02:03:56.400 --> 02:04:03.280]   Comcast has now confirmed that they are lining up a bid which could be as high as $60 billion.
[02:04:03.280 --> 02:04:04.440]   I don't think...
[02:04:04.440 --> 02:04:06.760]   I'm guessing it's not...
[02:04:06.760 --> 02:04:09.240]   They don't get Fox News, they don't get Fox Business Network.
[02:04:09.240 --> 02:04:13.080]   I'm guessing it's not the Fox Sports regional networks, but maybe that's what they're interested
[02:04:13.080 --> 02:04:14.080]   in.
[02:04:14.080 --> 02:04:17.720]   There is a 39% ownership of Sky and I know that Comcast has been trying to buy Sky, so
[02:04:17.720 --> 02:04:21.200]   that might be part of it, the British satellite broadcaster.
[02:04:21.200 --> 02:04:25.280]   But it would also, and I think this might be more to it, the point include Fox's 30%
[02:04:25.280 --> 02:04:27.440]   stake in Hulu.
[02:04:27.440 --> 02:04:29.240]   Comcast already owns 30% of Hulu.
[02:04:29.240 --> 02:04:30.240]   Just give it full control.
[02:04:30.240 --> 02:04:35.800]   And of course, you got to figure Comcast, which is, makes its money selling premium packages
[02:04:35.800 --> 02:04:36.800]   for television.
[02:04:36.800 --> 02:04:43.000]   It's not a fan of Hulu, so that would give them controlling interest in probably the
[02:04:43.000 --> 02:04:44.000]   number one...
[02:04:44.000 --> 02:04:47.520]   I think it is the number one over the top video provider.
[02:04:47.520 --> 02:04:52.840]   Yeah, because media consolidations, what this country really needs at the moment.
[02:04:52.840 --> 02:04:56.480]   It's far too many companies getting involved with this media business.
[02:04:56.480 --> 02:05:01.360]   Well, and don't forget, Comcast already owns Universal owns NBC.
[02:05:01.360 --> 02:05:03.160]   It's really reaching its tentacles out.
[02:05:03.160 --> 02:05:10.040]   This is one of the reasons why net neutrality is an issue, is because Comcast, chief among
[02:05:10.040 --> 02:05:13.240]   everybody, doesn't want net neutrality.
[02:05:13.240 --> 02:05:16.800]   They want to control all of this stuff, and they're showing it with their acquisitions.
[02:05:16.800 --> 02:05:21.200]   I've never in 10 years in living this country.
[02:05:21.200 --> 02:05:24.600]   I've never met a satisfied Comcast customer.
[02:05:24.600 --> 02:05:27.120]   One who actually enjoys the service they get from the company.
[02:05:27.120 --> 02:05:33.160]   The survey I saw said that they asked people what they hate the most, Comcast was worse
[02:05:33.160 --> 02:05:34.160]   than a heart attack.
[02:05:34.160 --> 02:05:38.440]   It caused a few, though, the customer support team.
[02:05:38.440 --> 02:05:39.440]   Yeah.
[02:05:39.440 --> 02:05:40.440]   All right, I'm sorry.
[02:05:40.440 --> 02:05:42.440]   I shouldn't get on my horse if I can.
[02:05:42.440 --> 02:05:43.440]   Wow, nice.
[02:05:43.440 --> 02:05:45.440]   I did learn one very interesting trick from a friend of mine though.
[02:05:45.440 --> 02:05:49.120]   He said, "If you ever want to cancel a Comcast account, they'll try and keep you on the phone
[02:05:49.120 --> 02:05:52.160]   for 20, 30 minutes trying to talk you out of it.
[02:05:52.160 --> 02:05:55.680]   The golden way to get out of that is to say, "Ash, I've just moved to my girlfriend
[02:05:55.680 --> 02:05:56.680]   and she's already a customer."
[02:05:56.680 --> 02:05:58.200]   They're like, "Oh, okay, that's fine.
[02:05:58.200 --> 02:06:00.480]   We're still keeping you there."
[02:06:00.480 --> 02:06:05.200]   If you want to cancel your account, that's apparently the shibboleth phrase to get through.
[02:06:05.200 --> 02:06:06.200]   I'm still a customer.
[02:06:06.200 --> 02:06:07.200]   I'm just not going to be me.
[02:06:07.200 --> 02:06:09.160]   It'll be my girlfriend.
[02:06:09.160 --> 02:06:11.800]   That's sad that you have to do that.
[02:06:11.800 --> 02:06:12.800]   No.
[02:06:12.800 --> 02:06:20.080]   Pat Toomey, Senator Toomey, Senator Merkley, apparently were impersonated during the net
[02:06:20.080 --> 02:06:25.080]   neutrality repeal and posted comments, not them, somebody pretending to be them, posted
[02:06:25.080 --> 02:06:30.800]   comments on the FCC, fake comments on the FCC's site against net neutrality.
[02:06:30.800 --> 02:06:33.320]   They're not happy.
[02:06:33.320 --> 02:06:37.800]   They just sent a letter to Ajit Pai, the chairman of the FCC saying, "Really?
[02:06:37.800 --> 02:06:41.440]   You've got to investigate these phony comments.
[02:06:41.440 --> 02:06:42.440]   Millions of them.
[02:06:42.440 --> 02:06:44.760]   We know millions of them.
[02:06:44.760 --> 02:06:46.400]   Pro net neutrality comments.
[02:06:46.400 --> 02:06:47.880]   Who put them there?"
[02:06:47.880 --> 02:06:52.800]   Why they're there?
[02:06:52.800 --> 02:06:53.800]   Who would impersonate this?
[02:06:53.800 --> 02:06:54.800]   This has been going on for a year.
[02:06:54.800 --> 02:06:55.800]   This is really dumb.
[02:06:55.800 --> 02:07:00.520]   We've been complaining about this comment that the comments perrow to the FCC said, "Nope."
[02:07:00.520 --> 02:07:01.520]   I've almost done that.
[02:07:01.520 --> 02:07:04.520]   The minute two sentences are just like, "Oh, hang on, I'll name this one there."
[02:07:04.520 --> 02:07:05.520]   This is an outrage.
[02:07:05.520 --> 02:07:06.520]   That's what it is.
[02:07:06.520 --> 02:07:09.920]   That's why I'm voting for Brian Ford for Congress.
[02:07:09.920 --> 02:07:14.760]   It's interesting because according to the Wall Street Journal, five different federal
[02:07:14.760 --> 02:07:20.000]   agencies have seen impersonated comments or fake comments or something like what it
[02:07:20.000 --> 02:07:21.000]   was.
[02:07:21.000 --> 02:07:22.000]   We're an expert in this.
[02:07:22.000 --> 02:07:23.600]   It's easy to spam these systems.
[02:07:23.600 --> 02:07:29.200]   It is, you know, you have to take the proper precautions to prevent spam in general.
[02:07:29.200 --> 02:07:34.040]   I'll just say if anybody wants to reach out, the US General Service stands at the ready
[02:07:34.040 --> 02:07:35.600]   to help with the agencies.
[02:07:35.600 --> 02:07:36.600]   Thank you.
[02:07:36.600 --> 02:07:37.600]   Please.
[02:07:37.600 --> 02:07:40.680]   Please, I beg of you.
[02:07:40.680 --> 02:07:44.840]   This is why I'll work to increase the budget for more people like Matt to help address this.
[02:07:44.840 --> 02:07:45.840]   Yeah.
[02:07:45.840 --> 02:07:46.840]   All right, that's it.
[02:07:46.840 --> 02:07:47.840]   The show's over.
[02:07:47.840 --> 02:07:48.840]   Thank you.
[02:07:48.840 --> 02:07:51.160]   This is the point where I say as a federal employee, the Hatch Act says that I cannot
[02:07:51.160 --> 02:07:53.120]   endorse anybody for all of this.
[02:07:53.120 --> 02:07:54.120]   That's right.
[02:07:54.120 --> 02:07:55.520]   However, I can.
[02:07:55.520 --> 02:07:57.720]   And I'm really glad you're running.
[02:07:57.720 --> 02:08:02.160]   I wish I could vote in the 45th District, Brian, but if people want to know more, f-o-r-d-e
[02:08:02.160 --> 02:08:05.400]   dot com, he's the crypto candidate.
[02:08:05.400 --> 02:08:06.400]   But that's that.
[02:08:06.400 --> 02:08:08.120]   This sounds like a bad thing.
[02:08:08.120 --> 02:08:09.840]   I wouldn't say that.
[02:08:09.840 --> 02:08:13.560]   He is the intelligent choice.
[02:08:13.560 --> 02:08:15.440]   And I do wish you luck.
[02:08:15.440 --> 02:08:18.400]   Do you remember thinking of bad words?
[02:08:18.400 --> 02:08:20.080]   It's a crypto is not a bad word.
[02:08:20.080 --> 02:08:21.080]   Crypto is a great word.
[02:08:21.080 --> 02:08:23.480]   It was hacking in the original origins.
[02:08:23.480 --> 02:08:24.480]   I agree.
[02:08:24.480 --> 02:08:25.480]   I agree.
[02:08:25.480 --> 02:08:29.600]   I remember when I first launched the first hackathon at the White House.
[02:08:29.600 --> 02:08:34.000]   I got a nasty gram from another department inside the White House, the National Security
[02:08:34.000 --> 02:08:35.000]   Council.
[02:08:35.000 --> 02:08:38.640]   They said, "Are you the one responsible for hosting a hackathon at the White House?"
[02:08:38.640 --> 02:08:39.800]   Oh, Lord.
[02:08:39.800 --> 02:08:44.600]   And so I just currently said, "Go look up the origins of hacking and get back to me."
[02:08:44.600 --> 02:08:45.600]   Yeah.
[02:08:45.600 --> 02:08:47.600]   I hope you'd ever heard from them again.
[02:08:47.600 --> 02:08:48.600]   Yeah.
[02:08:48.600 --> 02:08:54.560]   That's funny, the US federal service did one of the first federal bug bounties over at
[02:08:54.560 --> 02:08:55.560]   the Pentagon.
[02:08:55.560 --> 02:08:57.600]   And it's called Hack the Pentagon.
[02:08:57.600 --> 02:09:00.000]   And the rumor was that somebody said, "Well, you can all write.
[02:09:00.000 --> 02:09:01.000]   You're allowed to do this.
[02:09:01.000 --> 02:09:04.200]   You have permission, but whatever you do, just don't call it Hack the Pentagon."
[02:09:04.200 --> 02:09:05.600]   So that's what we call them.
[02:09:05.600 --> 02:09:09.480]   And so the guy at the Pentagon, his name is Chris Lynch, is like, "Well, that's definitely
[02:09:09.480 --> 02:09:10.480]   what we're calling it."
[02:09:10.480 --> 02:09:11.480]   Oh, right.
[02:09:11.480 --> 02:09:13.480]   By the way, it was a great success, wasn't it?
[02:09:13.480 --> 02:09:14.480]   Yeah.
[02:09:14.480 --> 02:09:15.480]   Yeah, it really was.
[02:09:15.480 --> 02:09:18.040]   In fact, we've done something like seven now.
[02:09:18.040 --> 02:09:22.080]   You get better results for cheaper and it happens faster.
[02:09:22.080 --> 02:09:24.040]   Why wouldn't you do bug bounties?
[02:09:24.040 --> 02:09:25.960]   Sometimes there's some controversy over bug bounties.
[02:09:25.960 --> 02:09:29.600]   I worry about bug bounties that there's a whole group of people who are withholding
[02:09:29.600 --> 02:09:35.800]   hacks or generating hacks hoping to make money during its pwnedown and other events.
[02:09:35.800 --> 02:09:36.800]   Yeah, true.
[02:09:36.800 --> 02:09:41.160]   But the fact is those folks could already attack the government.
[02:09:41.160 --> 02:09:45.440]   And so given an opportunity where people can be motivated either through civic pride or
[02:09:45.440 --> 02:09:53.000]   through a small amount of money, we've seen 17-year-olds who have made $40,000 just by
[02:09:53.000 --> 02:09:54.000]   participating.
[02:09:54.000 --> 02:09:58.760]   And so that creates the odds that 17-year-old might want to do an internship in the government
[02:09:58.760 --> 02:10:00.680]   might try to help secure government systems.
[02:10:00.680 --> 02:10:02.880]   Like that's a good thing overall in my book.
[02:10:02.880 --> 02:10:03.880]   Good.
[02:10:03.880 --> 02:10:04.880]   Yeah.
[02:10:04.880 --> 02:10:07.000]   It's logical that they're going to hold some things back for the competitions, but by
[02:10:07.000 --> 02:10:10.600]   and large, you can make a pretty good living now doing bug bounty programs.
[02:10:10.600 --> 02:10:12.800]   I mean, it's all okay to miss out on this about this.
[02:10:12.800 --> 02:10:15.880]   She can give you a chapter and verse on where this is going.
[02:10:15.880 --> 02:10:21.320]   But it's a smart way to, I mean, if you paid private companies to do all this for you,
[02:10:21.320 --> 02:10:25.200]   the bill would be in the millions and it still wouldn't be finished yet.
[02:10:25.200 --> 02:10:29.120]   Well, and it's important for people to know these are white hat security researchers.
[02:10:29.120 --> 02:10:30.120]   Yeah.
[02:10:30.120 --> 02:10:33.560]   So yes, the term hacker might sound scary if you're not familiar with the history, but
[02:10:33.560 --> 02:10:34.840]   it's really very practical.
[02:10:34.840 --> 02:10:35.840]   Yeah.
[02:10:35.840 --> 02:10:39.720]   I should also point out that many of white hats started with a different color on their
[02:10:39.720 --> 02:10:41.720]   head in the early days, right?
[02:10:41.720 --> 02:10:42.720]   So we all come on.
[02:10:42.720 --> 02:10:43.720]   Come on.
[02:10:43.720 --> 02:10:44.720]   It's like Steve Jobs, Bill Gates.
[02:10:44.720 --> 02:10:45.720]   They did it.
[02:10:45.720 --> 02:10:46.720]   They did it.
[02:10:46.720 --> 02:10:47.720]   That's what you do.
[02:10:47.720 --> 02:10:48.720]   You're testing the limits.
[02:10:48.720 --> 02:10:49.960]   You're seeing what code can do.
[02:10:49.960 --> 02:10:51.920]   That's part of the fun of it.
[02:10:51.920 --> 02:10:57.120]   And if you put those skills to a good use instead of malicious use, more power to you.
[02:10:57.120 --> 02:11:03.560]   Talking about a malicious use, I'm happy to say that those creeps who swatted and ended
[02:11:03.560 --> 02:11:14.360]   up getting somebody killed have been charged and may in fact face almost life in jail.
[02:11:14.360 --> 02:11:19.160]   This was started with a dispute over a match in an online game on Call of Duty.
[02:11:19.160 --> 02:11:20.880]   The World War II pack.
[02:11:20.880 --> 02:11:21.880]   Yeah.
[02:11:21.880 --> 02:11:30.200]   A 19 year old from Wichita and another 18 year old had a falling out over a $1.50 wager.
[02:11:30.200 --> 02:11:36.360]   So allegedly Shane Gaskell and Casey Viner, a Viner wanted to get back at Gaskell.
[02:11:36.360 --> 02:11:37.960]   So he enlisted the help of another man.
[02:11:37.960 --> 02:11:40.720]   Again, allegedly Tyler Barris.
[02:11:40.720 --> 02:11:42.800]   Tyler's not the best guy in the world.
[02:11:42.800 --> 02:11:47.040]   His handle on Twitter was Swatistic.
[02:11:47.040 --> 02:11:51.080]   And he'd bragged of swatting hundreds of schools and dozens of private residences.
[02:11:51.080 --> 02:11:52.120]   I'm using the term swatting.
[02:11:52.120 --> 02:11:55.480]   I think you probably all know that that means calling the police.
[02:11:55.480 --> 02:11:57.800]   The police that we've been swatted calling the police department.
[02:11:57.800 --> 02:11:58.800]   Yeah.
[02:11:58.800 --> 02:12:02.720]   So the Peddlelima Police Department, I've heard the recording too.
[02:12:02.720 --> 02:12:07.160]   Called the Peddleplim Police Department said, I have put bombs in the basement of the Twit
[02:12:07.160 --> 02:12:13.000]   Brick House and I am going, I am there now and I'm going to shoot people one by one and
[02:12:13.000 --> 02:12:14.680]   then blow the whole place up.
[02:12:14.680 --> 02:12:22.520]   Hoping the whole hope is that the highly armed, really over armed in some cases, SWAT teams
[02:12:22.520 --> 02:12:27.440]   from these police departments will come and really disrupt broadcast operations or whatever.
[02:12:27.440 --> 02:12:30.480]   A lot of times with gamers, they're hoping to see the swatting on the Twitch stream.
[02:12:30.480 --> 02:12:32.640]   That kind of thing is very common on Twitch.
[02:12:32.640 --> 02:12:36.920]   The Peddlelima Police Department doesn't really have a SWAT squad.
[02:12:36.920 --> 02:12:41.200]   So a couple officers came over and said, everything okay, I hear, we said, yeah, why?
[02:12:41.200 --> 02:12:43.200]   Well, we got this call.
[02:12:43.200 --> 02:12:44.720]   It was kind of cute.
[02:12:44.720 --> 02:12:49.640]   Actually, they took it seriously enough that they said, we called down to the Tamal Pius
[02:12:49.640 --> 02:12:50.640]   Police Department.
[02:12:50.640 --> 02:12:51.640]   They're sending some dogs up.
[02:12:51.640 --> 02:12:56.080]   Why don't you guys take the rest of the day off and we'll just going to have the dogs
[02:12:56.080 --> 02:13:00.520]   sweep the place for bombs, which was, I thought, appropriate.
[02:13:00.520 --> 02:13:01.520]   But that happens.
[02:13:01.520 --> 02:13:08.960]   So this guy, but this is a more serious one because eventually what happened is allegedly
[02:13:08.960 --> 02:13:12.760]   Barris called 911 operators in Wichita.
[02:13:12.760 --> 02:13:15.440]   Viner had given him a address, which wasn't his address.
[02:13:15.440 --> 02:13:16.440]   Yeah.
[02:13:16.440 --> 02:13:18.680]   Said, I just shot my father in the head.
[02:13:18.680 --> 02:13:20.240]   I'm holding my mom and sister at gunpoint.
[02:13:20.240 --> 02:13:22.400]   I'm thinking about burning down the home.
[02:13:22.400 --> 02:13:29.120]   Wichita police came to that address, which was not in fact inhabited by anybody involved,
[02:13:29.120 --> 02:13:34.000]   sadly to say, but by a 28 year old Andrew Finch, it was his mom's house.
[02:13:34.000 --> 02:13:38.840]   He walked out and was shot to death by Wichita police officers.
[02:13:38.840 --> 02:13:40.840]   And the party of two, he didn't know the guys.
[02:13:40.840 --> 02:13:44.960]   He wasn't even a gamer.
[02:13:44.960 --> 02:13:48.480]   And there's plenty of record of them then saying, hey, did it work?
[02:13:48.480 --> 02:13:50.200]   There's all these screenshots.
[02:13:50.200 --> 02:13:53.560]   It's panicking once they realize that this was now a death, you know, there was a death
[02:13:53.560 --> 02:13:54.560]   involved.
[02:13:54.560 --> 02:13:55.560]   Yeah.
[02:13:55.560 --> 02:13:57.400]   And of course his attorney is saying, well, it's not his fault.
[02:13:57.400 --> 02:13:59.920]   It's the police officer who pulled the trigger.
[02:13:59.920 --> 02:14:05.360]   But the good news is, and I think this is appropriate, the so artistic is facing significant
[02:14:05.360 --> 02:14:08.200]   jail time, like the rest of his life.
[02:14:08.200 --> 02:14:09.200]   Yeah.
[02:14:09.200 --> 02:14:13.320]   Well, if he's guilty, I hope he gets it because that's just sorting.
[02:14:13.320 --> 02:14:14.320]   It's just gone nuts.
[02:14:14.320 --> 02:14:16.520]   They've also, I've lost his death con.
[02:14:16.520 --> 02:14:21.440]   So there was a whole presentation about how easy it is to hack the phone system to make
[02:14:21.440 --> 02:14:24.640]   it look as though, you know, to cover up your location.
[02:14:24.640 --> 02:14:29.040]   This is something else that, you know, we need some technologists in government to sort
[02:14:29.040 --> 02:14:30.040]   this stuff out.
[02:14:30.040 --> 02:14:31.040]   Yeah.
[02:14:31.040 --> 02:14:32.040]   I talked to the police department.
[02:14:32.040 --> 02:14:34.360]   They said, well, we have a phone number, but we know it's certainly not the person's
[02:14:34.360 --> 02:14:35.520]   phone number.
[02:14:35.520 --> 02:14:36.520]   We don't know his location.
[02:14:36.520 --> 02:14:38.880]   I have heard the recording of the call.
[02:14:38.880 --> 02:14:40.680]   Doesn't look any bells.
[02:14:40.680 --> 02:14:42.680]   I think I know who it was.
[02:14:42.680 --> 02:14:43.680]   But without.
[02:14:43.680 --> 02:14:45.480]   Yeah, without proof.
[02:14:45.480 --> 02:14:47.680]   It's hard to do anything about it.
[02:14:47.680 --> 02:14:51.720]   Pop over for, you know, for Tina's toenails.
[02:14:51.720 --> 02:14:52.720]   That would be wrong.
[02:14:52.720 --> 02:14:53.960]   Yes, of course it would.
[02:14:53.960 --> 02:14:56.320]   I'll get the pliers.
[02:14:56.320 --> 02:14:59.600]   Any event.
[02:14:59.600 --> 02:15:02.400]   I wanted to pass that along because we have been covering that story all along.
[02:15:02.400 --> 02:15:05.400]   It was just a terrible story.
[02:15:05.400 --> 02:15:11.800]   And you know, Barris has admitted his role in many swatting including, I think he swatted
[02:15:11.800 --> 02:15:12.800]   Brian Krebs.
[02:15:12.800 --> 02:15:15.720]   And interviewed Brian Krebs.
[02:15:15.720 --> 02:15:17.440]   This is a story from Krebs on security.
[02:15:17.440 --> 02:15:19.240]   Yeah, Krebs has had a lot of his.
[02:15:19.240 --> 02:15:20.880]   He's had heroin sent to his house.
[02:15:20.880 --> 02:15:22.880]   He's been swatted a couple of times.
[02:15:22.880 --> 02:15:25.480]   The key on this, of course, is to call the local police departments.
[02:15:25.480 --> 02:15:26.480]   Yeah.
[02:15:26.480 --> 02:15:31.680]   They all know we do live streaming here and they're a little bit more aware of the whole
[02:15:31.680 --> 02:15:32.680]   potential.
[02:15:32.680 --> 02:15:33.680]   Yeah.
[02:15:33.680 --> 02:15:35.960]   I think we can wrap things up.
[02:15:35.960 --> 02:15:39.280]   It's been a nice, wonderful Memorial Day to it.
[02:15:39.280 --> 02:15:40.280]   Thank you.
[02:15:40.280 --> 02:15:41.280]   I'm really thrilled to have you all here.
[02:15:41.280 --> 02:15:42.280]   Matt, it's so great to see you.
[02:15:42.280 --> 02:15:47.000]   I know you had some personal tragedy in your life, but it sounds like you're doing well.
[02:15:47.000 --> 02:15:48.000]   Good to see you too.
[02:15:48.000 --> 02:15:49.000]   And I would just love you.
[02:15:49.000 --> 02:15:50.000]   Thank you for having me.
[02:15:50.000 --> 02:15:51.000]   Yeah.
[02:15:51.000 --> 02:15:52.000]   You're just the greatest.
[02:15:52.000 --> 02:15:53.000]   I'm so glad to see you.
[02:15:53.000 --> 02:15:54.000]   Have fun at the conference.
[02:15:54.000 --> 02:15:55.000]   Yeah, should be good.
[02:15:55.000 --> 02:15:56.000]   I think that's going to be great.
[02:15:56.000 --> 02:15:58.400]   Anything you want to say besides usds.gov/join?
[02:15:58.400 --> 02:16:03.880]   I'll try not to do too much of a plug, but if you do want to check out that website,
[02:16:03.880 --> 02:16:05.840]   there is a recent report to Congress.
[02:16:05.840 --> 02:16:10.600]   So if you want to read the kinds of things that we do, API development, helping doctors,
[02:16:10.600 --> 02:16:14.680]   helping small businesses, helping veterans, there's some really good work there.
[02:16:14.680 --> 02:16:16.360]   And there's still support for what you're doing?
[02:16:16.360 --> 02:16:17.680]   Yeah, absolutely.
[02:16:17.680 --> 02:16:18.680]   That's awesome.
[02:16:18.680 --> 02:16:19.680]   I think that's really important.
[02:16:19.680 --> 02:16:20.680]   And your budget...
[02:16:20.680 --> 02:16:21.680]   Budget fine.
[02:16:21.680 --> 02:16:22.680]   exists.
[02:16:22.680 --> 02:16:25.760]   What I need now is talented technologist, designers, product managers.
[02:16:25.760 --> 02:16:26.760]   That's...
[02:16:26.760 --> 02:16:29.560]   You know, people, we all love to complain.
[02:16:29.560 --> 02:16:30.560]   Here's a chance to do something.
[02:16:30.560 --> 02:16:31.560]   That's right.
[02:16:31.560 --> 02:16:32.560]   Yeah.
[02:16:32.560 --> 02:16:33.560]   That make a difference.
[02:16:33.560 --> 02:16:34.560]   I think that's great.
[02:16:34.560 --> 02:16:35.560]   USds.gov/join.
[02:16:35.560 --> 02:16:37.080]   Thank you, Matt.
[02:16:37.080 --> 02:16:38.080]   Thank you, too.
[02:16:38.080 --> 02:16:39.560]   Ian Thompson, always a pleasure to have you here.
[02:16:39.560 --> 02:16:40.560]   It was good fun.
[02:16:40.560 --> 02:16:41.560]   It was good, NASA.
[02:16:41.560 --> 02:16:42.560]   Dot code dot...
[02:16:42.560 --> 02:16:48.080]   The register dot code dot UK for your daily dose of snark.
[02:16:48.080 --> 02:16:50.120]   Do you do your own headlines?
[02:16:50.120 --> 02:16:51.120]   We all...
[02:16:51.120 --> 02:16:55.640]   Every journalist will submit their own headlines, but it's usually the editor which does the
[02:16:55.640 --> 02:16:56.640]   final thing.
[02:16:56.640 --> 02:16:57.640]   That's you.
[02:16:57.640 --> 02:16:58.640]   But...
[02:16:58.640 --> 02:16:59.640]   Well, yes.
[02:16:59.640 --> 02:17:03.520]   But I don't know, the one I managed to sneak in recently was with the Amazon doing a recall
[02:17:03.520 --> 02:17:05.160]   on their power packs.
[02:17:05.160 --> 02:17:06.160]   And it was like...
[02:17:06.160 --> 02:17:10.400]   It was something along the lines of house fires, chemical burns, all come free with
[02:17:10.400 --> 02:17:11.400]   this.
[02:17:11.400 --> 02:17:12.400]   Amazon's a pack.
[02:17:12.400 --> 02:17:13.400]   I love it.
[02:17:13.400 --> 02:17:18.280]   And by the way, the register has beefed up its reporting.
[02:17:18.280 --> 02:17:19.280]   It is great.
[02:17:19.280 --> 02:17:21.120]   It's always been really fun to read.
[02:17:21.120 --> 02:17:24.440]   You've got smart people there doing a great job.
[02:17:24.440 --> 02:17:26.520]   And it's a regular visit for me now.
[02:17:26.520 --> 02:17:27.520]   It is.
[02:17:27.520 --> 02:17:28.520]   And no paywall.
[02:17:28.520 --> 02:17:29.520]   Nope.
[02:17:29.520 --> 02:17:30.520]   No, no.
[02:17:30.520 --> 02:17:31.520]   There will be...
[02:17:31.520 --> 02:17:35.720]   There may be some developments on it with toying with the idea of getting cryptocurrency
[02:17:35.720 --> 02:17:37.200]   involved somewhere along with all of them.
[02:17:37.200 --> 02:17:38.200]   I'll donate.
[02:17:38.200 --> 02:17:39.200]   Yeah.
[02:17:39.200 --> 02:17:43.200]   I'm going to do a coin sitting in a wall and I can't unlock it.
[02:17:43.200 --> 02:17:44.200]   Register coin.
[02:17:44.200 --> 02:17:49.200]   Well, I did worry, Brian, when you said that you were taking Bitcoin, it's like, "What
[02:17:49.200 --> 02:17:52.000]   happens if the price crashes two weeks before the election?"
[02:17:52.000 --> 02:17:54.200]   And it's just like, "Our reserves, they've all gone."
[02:17:54.200 --> 02:17:55.200]   No, what do you do?
[02:17:55.200 --> 02:17:58.760]   Do you immediately cash in the Bitcoin or do you hold it or what do you do?
[02:17:58.760 --> 02:18:02.720]   We have to convert it to fiat and the positive and our bank account.
[02:18:02.720 --> 02:18:06.280]   But we could take our cash on Ant and let it ride in Bitcoin if we wanted to, but we've
[02:18:06.280 --> 02:18:07.840]   chosen not to do that.
[02:18:07.840 --> 02:18:08.840]   Good.
[02:18:08.840 --> 02:18:09.840]   Good.
[02:18:09.840 --> 02:18:11.800]   Bye, and Ford.
[02:18:11.800 --> 02:18:12.800]   Good luck to you.
[02:18:12.800 --> 02:18:13.800]   The election is soon.
[02:18:13.800 --> 02:18:14.800]   I can't believe it.
[02:18:14.800 --> 02:18:16.240]   You're coming down to the wire here.
[02:18:16.240 --> 02:18:17.240]   Is it exhausting?
[02:18:17.240 --> 02:18:18.240]   June 5th.
[02:18:18.240 --> 02:18:19.240]   It couldn't be...
[02:18:19.240 --> 02:18:26.160]   I think kind of Matt alluded to this, that when you're in government, you get out of
[02:18:26.160 --> 02:18:29.760]   bed every day because you're incredibly mission driven.
[02:18:29.760 --> 02:18:32.720]   And it's a little harder to have that mission driven when you're in the private sector.
[02:18:32.720 --> 02:18:37.920]   And so I'm incredibly mission driven to have more technologists in Congress like me because
[02:18:37.920 --> 02:18:40.880]   it could not be anywhere important for the 21st century.
[02:18:40.880 --> 02:18:45.080]   And my one campaign promise that I'll keep is that as soon as I'm elected, I'll come
[02:18:45.080 --> 02:18:46.080]   back on this show.
[02:18:46.080 --> 02:18:48.440]   Oh, you know what?
[02:18:48.440 --> 02:18:52.960]   I'm looking at your issues page at Ford.com.
[02:18:52.960 --> 02:18:56.280]   Checks every box, every box.
[02:18:56.280 --> 02:19:02.160]   Right on, you have my fervent endorsement and I wish you luck.
[02:19:02.160 --> 02:19:04.200]   And I wish I could vote in the 45th.
[02:19:04.200 --> 02:19:05.200]   I can't.
[02:19:05.200 --> 02:19:07.000]   Don't do that.
[02:19:07.000 --> 02:19:08.000]   I won't.
[02:19:08.000 --> 02:19:09.000]   I promise.
[02:19:09.000 --> 02:19:10.000]   Everybody should vote.
[02:19:10.000 --> 02:19:12.560]   If you've got a primary in your state coming up, make sure you vote.
[02:19:12.560 --> 02:19:13.560]   Make sure you participate.
[02:19:13.560 --> 02:19:15.240]   Volunteers stay informed.
[02:19:15.240 --> 02:19:20.600]   Donate to the candidates who make a difference, especially if they're savvy in technology.
[02:19:20.600 --> 02:19:24.080]   And then of course, November is going to be a very big and important election.
[02:19:24.080 --> 02:19:26.920]   And this is when we stand up for the things we believe in.
[02:19:26.920 --> 02:19:28.040]   Brian, thank you for being here.
[02:19:28.040 --> 02:19:29.040]   I appreciate it.
[02:19:29.040 --> 02:19:30.040]   Thanks for having me.
[02:19:30.040 --> 02:19:35.200]   We do Twitter every Sunday evening, 3 p.m. Pacific, 6 p.m. Eastern time, 2200 UTC.
[02:19:35.200 --> 02:19:36.200]   Come on by.
[02:19:36.200 --> 02:19:37.200]   We stream it live.
[02:19:37.200 --> 02:19:41.120]   And in fact, get here early and stay late because there's always stuff on either side
[02:19:41.120 --> 02:19:45.080]   of the show extra stuff that's always a lot of fun, I think.
[02:19:45.080 --> 02:19:47.800]   It's fun for me.
[02:19:47.800 --> 02:19:50.240]   You just go to twit.tv/live to watch live.
[02:19:50.240 --> 02:19:52.040]   If you do that, though, join us in the chat room as well.
[02:19:52.040 --> 02:19:53.600]   A great group of people.
[02:19:53.600 --> 02:19:58.600]   Kind of the I always think of them as the rowties in the back of the auditorium.
[02:19:58.600 --> 02:20:00.560]   Lots of fun IRC.twit.tv.
[02:20:00.560 --> 02:20:01.560]   You can also be in studio.
[02:20:01.560 --> 02:20:04.120]   We had a great studio audience for a three-day weekend.
[02:20:04.120 --> 02:20:05.600]   I guess that's why maybe.
[02:20:05.600 --> 02:20:07.640]   From all over the world, wonderful to have you.
[02:20:07.640 --> 02:20:10.160]   All you have to do is email tickets@twit.tv.
[02:20:10.160 --> 02:20:12.200]   We'd be thrilled to have you here.
[02:20:12.200 --> 02:20:13.560]   Just let us know ahead of time.
[02:20:13.560 --> 02:20:15.720]   If you can't be here live or watch live, don't worry.
[02:20:15.720 --> 02:20:19.600]   You can watch on-demand versions of everything we do live.
[02:20:19.600 --> 02:20:23.360]   It's great, but in the can on schedule is probably a lot easier.
[02:20:23.360 --> 02:20:27.760]   Just go to twit.tv for a downloadable audio and video from every show.
[02:20:27.760 --> 02:20:31.680]   Or find us in your favorite podcast application.
[02:20:31.680 --> 02:20:33.040]   We're in every one of them.
[02:20:33.040 --> 02:20:37.800]   And you could subscribe. You can even ask your Amazon Echo to listen to any one of our shows.
[02:20:37.800 --> 02:20:42.480]   And usually she'll respond without recording your audio and sending it to a friend.
[02:20:42.480 --> 02:20:44.800]   Thanks for listening and we will see you next time.
[02:20:44.800 --> 02:20:46.280]   Another twit is in the can.
[02:20:46.280 --> 02:20:47.280]   Oh, boy.
[02:20:47.280 --> 02:20:48.280]   Do the twit.
[02:20:48.280 --> 02:20:49.280]   Do the twit.
[02:20:49.280 --> 02:20:50.280]   All right.
[02:20:50.280 --> 02:20:51.280]   Do the twit, baby.
[02:20:51.280 --> 02:20:52.280]   Do the twit.
[02:20:52.280 --> 02:20:53.280]   All right.
[02:20:53.280 --> 02:20:54.280]   Do the twit.
[02:20:54.280 --> 02:20:55.280]   Do the twit.
[02:20:55.280 --> 02:20:56.280]   All right.

