;FFMETADATA1
title=Gradually Then Suddenly
artist=Leo Laporte, Alex Stamos, Jeff Jarvis, Brianna Wu
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-05-01
track=925
language=English
genre=Podcast
comment=Bluesky, the end of Twitter, Flipper Zero, Supreme Court and AI
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:01.760]   It's time for Twit this week in Tech.
[00:00:01.760 --> 00:00:05.280]   Boy, is this a show you're going to want to stay tuned for what a panel.
[00:00:05.280 --> 00:00:07.120]   Jeff Jarvis is here from this week in Google.
[00:00:07.120 --> 00:00:13.400]   Brianna Wu found a rebellion pack former congressional candidate Alex Stamos who has had
[00:00:13.400 --> 00:00:20.160]   gigs at Facebook and Google and Zoom and is now a security and trust expert professor
[00:00:20.160 --> 00:00:23.360]   at Stanford and the director of Stanford's internet observatory.
[00:00:23.360 --> 00:00:27.400]   We're going to talk about the rise of the Twitter replacements, particularly about blue
[00:00:27.400 --> 00:00:30.640]   sky trust and safety issues across the board.
[00:00:30.640 --> 00:00:36.480]   The disaster it is looming with a 2024 election and then we're going to celebrate the 30th
[00:00:36.480 --> 00:00:42.760]   birthday of the worldwide web all that and more coming up next on Twit.
[00:00:42.760 --> 00:00:56.120]   This is Twit.
[00:00:56.120 --> 00:00:57.520]   This is Twit.
[00:00:57.520 --> 00:01:05.360]   This week in Tech, episode 925 recorded Sunday, April 30, 2023.
[00:01:05.360 --> 00:01:11.840]   Gradually, then suddenly, this week in Tech is brought to you by Stamps.com.
[00:01:11.840 --> 00:01:15.600]   Set up your business for success when you start today.
[00:01:15.600 --> 00:01:18.800]   Sign up for the promo code Twit and you'll get a special offer that includes a four-week
[00:01:18.800 --> 00:01:22.360]   trial, free postage and a free digital scale.
[00:01:22.360 --> 00:01:27.400]   Just go to Stamps.com, click the microphone at the top of the page and enter the code
[00:01:27.400 --> 00:01:28.800]   Twit.
[00:01:28.800 --> 00:01:30.760]   And by ACI Learning.
[00:01:30.760 --> 00:01:34.360]   Oh, if you love IT, bro, you will love ACI Learning.
[00:01:34.360 --> 00:01:39.440]   ACI Learning offers fully customizable training for your team and formats for all types of
[00:01:39.440 --> 00:01:43.440]   learners across audit, cybersecurity and IT.
[00:01:43.440 --> 00:01:48.600]   From entry-level training to putting people on the moon, ACI Learning has you covered.
[00:01:48.600 --> 00:01:54.400]   Visit go.acilurning.com/twit to learn more.
[00:01:54.400 --> 00:01:56.160]   And by Noom.
[00:01:56.160 --> 00:02:01.760]   Stop chasing health trends and build sustainable, healthy habits with Noom's psychology-based
[00:02:01.760 --> 00:02:02.760]   approach.
[00:02:02.760 --> 00:02:06.680]   Check out Noom's first-ever book, The Noom Mindset, a deep dive into the psychology of
[00:02:06.680 --> 00:02:10.080]   behavior change available to buy now wherever books are sold.
[00:02:10.080 --> 00:02:15.560]   Don't forget to sign up for your trial at noom.com/twits.
[00:02:15.560 --> 00:02:17.600]   And by Lookout.
[00:02:17.600 --> 00:02:22.560]   Whether on a device or in the cloud, your business data is always on the move.
[00:02:22.560 --> 00:02:28.840]   Minimize risk, increase visibility and ensure compliance with Lookout's unified platform.
[00:02:28.840 --> 00:02:31.080]   Visit Lookout.com today.
[00:02:31.080 --> 00:02:41.280]   It's time for Twit this week at Tech, the show where we cover the week's Tech News.
[00:02:41.280 --> 00:02:42.920]   I have the best panel ever.
[00:02:42.920 --> 00:02:47.960]   I'm so excited to say hello to Alex Stamos, who is here from the Internet Observatory.
[00:02:47.960 --> 00:02:53.840]   It sounds like you got a telescope and you're looking at the Stanford Internet Observatory.
[00:02:53.840 --> 00:02:55.680]   What do you observe there, Alex?
[00:02:55.680 --> 00:02:59.680]   We are a multidisciplinary group that looks at the abuse of the Internet and different
[00:02:59.680 --> 00:03:02.080]   kind of technical and policy indications.
[00:03:02.080 --> 00:03:03.080]   Abuse.
[00:03:03.080 --> 00:03:04.080]   Abuse.
[00:03:04.080 --> 00:03:06.600]   Alex is a very well-known security guru.
[00:03:06.600 --> 00:03:09.800]   We had so much fun with you on this week in Google a few months ago.
[00:03:09.800 --> 00:03:12.720]   We said, "We've got to get Alex back in."
[00:03:12.720 --> 00:03:17.680]   And if we're going to get Alex back in, we better have Mr. Jeff Jarvis here to join
[00:03:17.680 --> 00:03:18.680]   him.
[00:03:18.680 --> 00:03:19.680]   Hello, Jeff, buzzmachine.com.
[00:03:19.680 --> 00:03:20.680]   Oh, oh.
[00:03:20.680 --> 00:03:23.960]   Alex is wearing a real, you know, man's t-shirt.
[00:03:23.960 --> 00:03:26.560]   I, however, and we're...
[00:03:26.560 --> 00:03:28.440]   Oh, a Twit 10.
[00:03:28.440 --> 00:03:32.440]   Well, that was almost a decade ago, the 10th anniversary of Twit.
[00:03:32.440 --> 00:03:34.240]   It was a long time ago.
[00:03:34.240 --> 00:03:35.240]   We are now in...
[00:03:35.240 --> 00:03:36.680]   Oh, is this a set of prizes?
[00:03:36.680 --> 00:03:40.280]   We had our Aintit's birthday while I was gone a couple of weeks ago.
[00:03:40.280 --> 00:03:41.360]   Oh, old enough to drink.
[00:03:41.360 --> 00:03:42.360]   Yeah.
[00:03:42.360 --> 00:03:44.480]   Well, two years will do the 20th and I'll get you a t-shirt.
[00:03:44.480 --> 00:03:45.480]   How about that?
[00:03:45.480 --> 00:03:46.480]   How about that?
[00:03:46.480 --> 00:03:50.520]   And yeah, Alex, we should mention, is wearing a gold...
[00:03:50.520 --> 00:03:53.280]   I'm sorry, Sacramento Kings shirt because...
[00:03:53.280 --> 00:03:55.480]   And we have a big screen just for Alex.
[00:03:55.480 --> 00:04:00.400]   One of the conditions of his appearance here was that he could watch the final, Game 7
[00:04:00.400 --> 00:04:04.200]   of the NBA semifinals, the Warriors and the Kings.
[00:04:04.200 --> 00:04:06.560]   It's a Bay Area X, Travis Ganson.
[00:04:06.560 --> 00:04:07.560]   Yeah, NorCal.
[00:04:07.560 --> 00:04:11.800]   And you know, frankly, we're not much farther from the Kings than we are from the Warriors.
[00:04:11.800 --> 00:04:12.960]   I could go either way on this.
[00:04:12.960 --> 00:04:14.160]   Yeah, no, and here at Petaluma.
[00:04:14.160 --> 00:04:15.160]   Yeah.
[00:04:15.160 --> 00:04:16.160]   No, it's beautiful.
[00:04:16.160 --> 00:04:17.160]   Yeah, I grew up in Sacramento.
[00:04:17.160 --> 00:04:22.280]   My dad had season tickets since like 1987 or so, so that means I've seen about 150 wins,
[00:04:22.280 --> 00:04:23.280]   300 losses.
[00:04:23.280 --> 00:04:27.640]   Yeah, the Warriors went through that period too, didn't they?
[00:04:27.640 --> 00:04:30.320]   Yeah, which I saw too when I was at Gallup, we'd go to Warriors games.
[00:04:30.320 --> 00:04:31.320]   It was like 10 bucks.
[00:04:31.320 --> 00:04:32.320]   Yeah, terrible.
[00:04:32.320 --> 00:04:33.320]   Yeah, terrible.
[00:04:33.320 --> 00:04:34.320]   Yeah, terrible.
[00:04:34.320 --> 00:04:35.320]   But, you know, it was a good time.
[00:04:35.320 --> 00:04:37.640]   It's great to see that and it's great to see that they're going to go play.
[00:04:37.640 --> 00:04:41.320]   Whoever wins plays the Lakers, I know either one of these teams will see other to beat
[00:04:41.320 --> 00:04:42.320]   the Lakers.
[00:04:42.320 --> 00:04:43.320]   Yeah, we could take...
[00:04:43.320 --> 00:04:44.320]   Let's both go down.
[00:04:44.320 --> 00:04:45.320]   Yeah.
[00:04:45.320 --> 00:04:46.320]   Beat the heck out of them.
[00:04:46.320 --> 00:04:48.560]   So, if Alex leaps to his feet at any point, you'll know.
[00:04:48.560 --> 00:04:50.320]   I have a victory hat, so you'll see.
[00:04:50.320 --> 00:04:51.320]   Oh, good.
[00:04:51.320 --> 00:04:52.320]   No spoilers.
[00:04:52.320 --> 00:04:55.120]   Also, hey, that's not as if that weren't enough.
[00:04:55.120 --> 00:05:02.720]   We've also got Brianna Wu with us, executive director of Rebellion Pack, a game developer,
[00:05:02.720 --> 00:05:07.520]   a speed runner, and now an advocate for Blue Sky.
[00:05:07.520 --> 00:05:08.520]   It's good to see.
[00:05:08.520 --> 00:05:09.520]   There we go.
[00:05:09.520 --> 00:05:10.520]   Good to be on the show.
[00:05:10.520 --> 00:05:16.000]   I'm the one causing all the problems on the internet that Alex has to investigate and
[00:05:16.000 --> 00:05:17.000]   work professionally.
[00:05:17.000 --> 00:05:18.000]   I woke up today.
[00:05:18.000 --> 00:05:19.560]   Appreciate the help, Brianna.
[00:05:19.560 --> 00:05:23.440]   I woke up today to post from Brianna.
[00:05:23.440 --> 00:05:25.840]   Blue Sky is saying I'm a guest on Twitter today.
[00:05:25.840 --> 00:05:27.120]   I'm sure Blue Sky will come up.
[00:05:27.120 --> 00:05:31.920]   I want to convince him and his massive audience of Blue Sky is a place worth spending time.
[00:05:31.920 --> 00:05:33.600]   Can you help by following him and saying hello?
[00:05:33.600 --> 00:05:35.080]   I'm bear with you, Brianna.
[00:05:35.080 --> 00:05:36.080]   He already...
[00:05:36.080 --> 00:05:38.200]   I asked him, he needed to be like, "No, no, I've been there.
[00:05:38.200 --> 00:05:39.200]   It's nothing."
[00:05:39.200 --> 00:05:40.200]   Wrong.
[00:05:40.200 --> 00:05:41.200]   Good to usual thing.
[00:05:41.200 --> 00:05:42.200]   All right.
[00:05:42.200 --> 00:05:43.200]   What is...
[00:05:43.200 --> 00:05:44.840]   So, Brianna explained what Blue Sky is before we go too much.
[00:05:44.840 --> 00:05:45.840]   They're farther down the road.
[00:05:45.840 --> 00:05:46.840]   Blue Sky is mastered on...
[00:05:46.840 --> 00:05:47.840]   Let's just admit it.
[00:05:47.840 --> 00:05:48.840]   It's mastered.
[00:05:48.840 --> 00:05:49.840]   No, it's not.
[00:05:49.840 --> 00:05:55.640]   Oh, you know, and I have to say, one of the things that hurts me a little bit is all of
[00:05:55.640 --> 00:05:56.640]   this.
[00:05:56.640 --> 00:06:02.800]   This week, it was all Blue Sky, because I think it was the blue check thing finally set off
[00:06:02.800 --> 00:06:08.360]   a number of well-known Twitter users to the point where they said, "I'm out of here."
[00:06:08.360 --> 00:06:14.400]   Blue Sky, which was created by Jack Dorsey a few years ago as a federated replacement
[00:06:14.400 --> 00:06:17.160]   in effect for Twitter, attracted them.
[00:06:17.160 --> 00:06:20.520]   And all of a sudden, all the conversations about Blue Sky, and here I am on our little
[00:06:20.520 --> 00:06:21.720]   Mastodon instance.
[00:06:21.720 --> 00:06:24.080]   I'm a big Mastodon user and fanfare.
[00:06:24.080 --> 00:06:25.920]   I still love Mastodon, Lee.
[00:06:25.920 --> 00:06:31.160]   And I'm saying, "How come all of the love go into Blue Sky all of a sudden?"
[00:06:31.160 --> 00:06:32.160]   But Blue Sky isn't Mastodon.
[00:06:32.160 --> 00:06:33.160]   No.
[00:06:33.160 --> 00:06:35.160]   I think it's important to say Blue Sky is not the same.
[00:06:35.160 --> 00:06:36.160]   Sure.
[00:06:36.160 --> 00:06:38.120]   I hear what you're saying.
[00:06:38.120 --> 00:06:41.160]   I feel like that the experience is very similar to it.
[00:06:41.160 --> 00:06:46.240]   Like the one thing that Blue Sky does better is it's not asking to choose between nine trillion
[00:06:46.240 --> 00:06:47.240]   different servers.
[00:06:47.240 --> 00:06:48.240]   Not yet, but it will.
[00:06:48.240 --> 00:06:49.240]   Yeah, but it will.
[00:06:49.240 --> 00:06:50.240]   Water, Brianna?
[00:06:50.240 --> 00:06:55.800]   You will have to choose eventually between what you want your metadata data to be as far
[00:06:55.800 --> 00:07:01.200]   as how trolls get labeled and what the moderation you want is.
[00:07:01.200 --> 00:07:05.800]   They're basically the way they are going to off-board moderation is very similar to
[00:07:05.800 --> 00:07:06.800]   Mastodon.
[00:07:06.800 --> 00:07:10.360]   You're going to leave it up to individuals and go through and label like we approved
[00:07:10.360 --> 00:07:11.560]   this person.
[00:07:11.560 --> 00:07:12.560]   This is a troll.
[00:07:12.560 --> 00:07:13.800]   This is a harasser.
[00:07:13.800 --> 00:07:19.960]   This person is banned and you can subscribe to whichever one you find best.
[00:07:19.960 --> 00:07:24.800]   I think that when it comes to Blue Sky, it's a really, really, really great conversation
[00:07:24.800 --> 00:07:26.080]   right now.
[00:07:26.080 --> 00:07:31.480]   I don't think it's going to be a great conversation a year from now with the approach that they're
[00:07:31.480 --> 00:07:35.360]   taking to moderation, in my view.
[00:07:35.360 --> 00:07:37.320]   Just added blocking on Friday.
[00:07:37.320 --> 00:07:39.560]   So at least you can block somebody who's really annoying.
[00:07:39.560 --> 00:07:43.600]   They launched with no trust in safety features, no trust in safety team.
[00:07:43.600 --> 00:07:48.320]   It was, they kind of passed backwards.
[00:07:48.320 --> 00:07:52.440]   In which the CEO has now admitted that she understands that you're really getting when
[00:07:52.440 --> 00:07:56.000]   you buy into a social network, the product is the moderation.
[00:07:56.000 --> 00:07:57.000]   It is the community.
[00:07:57.000 --> 00:07:58.000]   Yes.
[00:07:58.000 --> 00:08:01.200]   It is not anybody can put up a nice little white sheet that random people can comment
[00:08:01.200 --> 00:08:05.840]   on and that you consolidate all of these different tweets or skits or whatever you want to call
[00:08:05.840 --> 00:08:06.840]   them.
[00:08:06.840 --> 00:08:09.840]   The hard part is making it a community people want to actually stick around.
[00:08:09.840 --> 00:08:13.720]   So one of the reasons it's a hot eye right now is because it's invite only still.
[00:08:13.720 --> 00:08:14.720]   Yes.
[00:08:14.720 --> 00:08:15.720]   It's just like Clubhouse in the early days.
[00:08:15.720 --> 00:08:16.720]   Yeah.
[00:08:16.720 --> 00:08:17.720]   And Google Gmail in the early days.
[00:08:17.720 --> 00:08:18.720]   I don't love to remember that.
[00:08:18.720 --> 00:08:19.720]   Yeah.
[00:08:19.720 --> 00:08:21.720]   That's a good way to get people wanting in.
[00:08:21.720 --> 00:08:22.720]   Yeah.
[00:08:22.720 --> 00:08:23.720]   No, nope.
[00:08:23.720 --> 00:08:25.680]   You got to have an invite.
[00:08:25.680 --> 00:08:28.080]   It also I think wins because it looks a lot like Twitter.
[00:08:28.080 --> 00:08:29.080]   Right.
[00:08:29.080 --> 00:08:30.080]   I mean.
[00:08:30.080 --> 00:08:31.080]   Yes.
[00:08:31.080 --> 00:08:33.040]   Oh, and it's got two things that Macedon has intentionally kept out, which is really good
[00:08:33.040 --> 00:08:38.000]   search, full text search, not just based upon hashtags and basically quote tweets, right?
[00:08:38.000 --> 00:08:43.320]   Yeah, if I press the repost button, I either get a choice between real posts.
[00:08:43.320 --> 00:08:44.320]   Right.
[00:08:44.320 --> 00:08:45.320]   Which is the decision.
[00:08:45.320 --> 00:08:49.400]   Like it's this religious thing among some of the founders of Macedon of the key developers.
[00:08:49.400 --> 00:08:54.360]   They say that quote, quote tweeting is drives abuse, which is true, but it's a feature that
[00:08:54.360 --> 00:08:55.760]   lots of people want.
[00:08:55.760 --> 00:09:00.160]   And they say the same thing about full text search, which I think is actually pretty foolish.
[00:09:00.160 --> 00:09:03.120]   Oh, I'm going to change it as mine on both.
[00:09:03.120 --> 00:09:04.120]   Yeah.
[00:09:04.120 --> 00:09:05.120]   He's going to have to point out.
[00:09:05.120 --> 00:09:07.600]   He's going to have to because blue skies about their ether lunch on that.
[00:09:07.600 --> 00:09:08.760]   I get to give people a crib sheet.
[00:09:08.760 --> 00:09:11.360]   Oh, again, it's the creator of Massedon.
[00:09:11.360 --> 00:09:13.400]   Massedon does not stand alone.
[00:09:13.400 --> 00:09:21.240]   Massedon sits on a protocol called activity pub, which many other apps use as well.
[00:09:21.240 --> 00:09:26.040]   And activity pub is the back one of something called the Fediverse, which is a federated
[00:09:26.040 --> 00:09:31.240]   universe of social apps, which include pixel fed, which is for pictures.
[00:09:31.240 --> 00:09:33.800]   There's a peer tube, which is a YouTube clone.
[00:09:33.800 --> 00:09:37.560]   So there are a variety of things that use this activity pub.
[00:09:37.560 --> 00:09:38.800]   Blue skies following the same road.
[00:09:38.800 --> 00:09:39.800]   In fact, it was a little weird.
[00:09:39.800 --> 00:09:45.440]   Jack Dorsey gave them, I think, $10 million a couple of years ago to start as a public
[00:09:45.440 --> 00:09:49.200]   benefit corporation, not owned by Twitter.
[00:09:49.200 --> 00:09:50.840]   Jack is on the board.
[00:09:50.840 --> 00:09:51.840]   It's fully independent.
[00:09:51.840 --> 00:09:52.840]   But it's fully independent.
[00:09:52.840 --> 00:09:55.840]   Again, it's a public benefit corporation.
[00:09:55.840 --> 00:10:04.080]   And his walking papers, his mandate was create a federated Twitter, something decentralized
[00:10:04.080 --> 00:10:05.120]   that no one can own.
[00:10:05.120 --> 00:10:06.360]   And he was kind of prescient.
[00:10:06.360 --> 00:10:10.840]   Actually, the most interesting story about blue sky this week is eject or see went on
[00:10:10.840 --> 00:10:17.160]   blue sky to retract his statements about Elon Musk.
[00:10:17.160 --> 00:10:20.480]   When Elon first bought Twitter, he said the best possible person to own it.
[00:10:20.480 --> 00:10:22.720]   He didn't mean the light of consciousness.
[00:10:22.720 --> 00:10:23.720]   Yeah.
[00:10:23.720 --> 00:10:27.960]   The light of consciousness did not totally penetrate the business.
[00:10:27.960 --> 00:10:28.960]   Yeah.
[00:10:28.960 --> 00:10:33.440]   He was Jack was probably as disappointed as everybody else was by Elon's stewardship
[00:10:33.440 --> 00:10:37.520]   and was fairly outspoken on blue sky about that.
[00:10:37.520 --> 00:10:41.400]   I'll see if I can find that because blue sky does have search.
[00:10:41.400 --> 00:10:42.400]   Yeah.
[00:10:42.400 --> 00:10:43.400]   All right.
[00:10:43.400 --> 00:10:47.200]   So Leah, I do want to push back a little bit.
[00:10:47.200 --> 00:10:51.760]   And look, I really like, I think for those of us that have been out here enjoying blue
[00:10:51.760 --> 00:10:56.320]   sky, we're definitely getting a lot of pushback from master done people.
[00:10:56.320 --> 00:10:57.320]   And I get it.
[00:10:57.320 --> 00:10:58.320]   I love master.
[00:10:58.320 --> 00:11:00.280]   I've got 15,000 people there.
[00:11:00.280 --> 00:11:04.160]   My engagement is roughly equal to what I get on Twitter with 10 times that amount.
[00:11:04.160 --> 00:11:06.400]   I enjoy master done.
[00:11:06.400 --> 00:11:11.920]   I think what you're seeing with blue sky is not just this ephemeral love for it because
[00:11:11.920 --> 00:11:13.960]   it is invite only.
[00:11:13.960 --> 00:11:20.920]   I think the reality is a ton of people that are just at their wits and with harassment
[00:11:20.920 --> 00:11:24.520]   have moved over to blue sky and have made it their home.
[00:11:24.520 --> 00:11:29.280]   And I think it doesn't take, I think there's a critical mass of journalists that can leave
[00:11:29.280 --> 00:11:33.680]   Twitter and make blue sky their home and start posting there.
[00:11:33.680 --> 00:11:39.880]   It's going to have a really negative impact on Twitter as a vehicle to experience news
[00:11:39.880 --> 00:11:40.880]   actively.
[00:11:40.880 --> 00:11:44.680]   But Alex, it sounds like you disagree that this is the place to go if you're being harassed
[00:11:44.680 --> 00:11:45.680]   on Twitter.
[00:11:45.680 --> 00:11:48.080]   Well, I think blue sky has not sold on their long term vision.
[00:11:48.080 --> 00:11:49.080]   Right.
[00:11:49.080 --> 00:11:51.440]   I mean, blue sky has been better because it is invite only.
[00:11:51.440 --> 00:11:54.200]   It has had a much smaller network of people on it.
[00:11:54.200 --> 00:11:55.680]   There hasn't yet been a lot of hurt.
[00:11:55.680 --> 00:11:56.680]   Right.
[00:11:56.680 --> 00:11:57.680]   The trolls have not been for the most part.
[00:11:57.680 --> 00:12:00.440]   Blue sky invite codes are going for like 300 bucks on a day.
[00:12:00.440 --> 00:12:06.480]   So it's a big investment to go buy invite code and then go burn it after just to get
[00:12:06.480 --> 00:12:09.480]   a couple of nasty tweets at you.
[00:12:09.480 --> 00:12:13.560]   But structurally, there's nothing about blue sky that makes it better than mastodon.
[00:12:13.560 --> 00:12:17.520]   In fact, I would say, I mean, activity pub, it has a longer history than AT protocol and
[00:12:17.520 --> 00:12:18.920]   people have been working on it.
[00:12:18.920 --> 00:12:22.480]   In both cases, mastodon and blue sky, people have not figured out how are you going to
[00:12:22.480 --> 00:12:24.680]   do moderation in a distributed fashion.
[00:12:24.680 --> 00:12:30.040]   Right. Like the protocols themselves don't do a lot about the kind of metadata around
[00:12:30.040 --> 00:12:32.920]   moderation that is actually used inside of the big companies.
[00:12:32.920 --> 00:12:37.200]   And if you're a mastodon owner right now, moderation is a big pain in the butt.
[00:12:37.200 --> 00:12:41.520]   And the tools do not really exist to do almost anything from an automated perspective.
[00:12:41.520 --> 00:12:45.920]   And so I think, yes, it feels a little bit right now.
[00:12:45.920 --> 00:12:49.160]   It's because the trolls have bought their blue checks on Twitter and they're running rampant
[00:12:49.160 --> 00:12:50.160]   on Twitter.
[00:12:50.160 --> 00:12:55.640]   But there's absolutely nothing that says that you won't end up with fully abusive instances
[00:12:55.640 --> 00:12:58.600]   federating with blue sky just as you've had with mastodon.
[00:12:58.600 --> 00:13:02.680]   The good news on mastodon is whoever runs the instance, and I run a mastodon instance
[00:13:02.680 --> 00:13:03.680]   called twit.social.
[00:13:03.680 --> 00:13:04.680]   Right.
[00:13:04.680 --> 00:13:07.400]   Has kind of infinite control to moderate it.
[00:13:07.400 --> 00:13:11.320]   You don't have the tools, but with a small instance like mine, it's about five or six
[00:13:11.320 --> 00:13:12.400]   thousand people.
[00:13:12.400 --> 00:13:16.960]   I rely on the users to report when they report, I review and I boot boot them.
[00:13:16.960 --> 00:13:19.880]   Usually that's no more than a few a day.
[00:13:19.880 --> 00:13:21.320]   And it's invite only.
[00:13:21.320 --> 00:13:25.280]   So you have to apply to get in and it's not invite exactly, but you have to apply to get
[00:13:25.280 --> 00:13:26.280]   in.
[00:13:26.280 --> 00:13:27.280]   You're approval.
[00:13:27.280 --> 00:13:28.280]   Yeah.
[00:13:28.280 --> 00:13:30.280]   And so it hasn't been an issue.
[00:13:30.280 --> 00:13:36.360]   And then we also, so, and then I also have to remind users, you can block anybody on mastodon,
[00:13:36.360 --> 00:13:39.160]   which is nice, block them on any instance anywhere.
[00:13:39.160 --> 00:13:41.040]   You as a user can block any instance.
[00:13:41.040 --> 00:13:42.040]   Yep.
[00:13:42.040 --> 00:13:43.280]   And as an administrator, I can block an instance.
[00:13:43.280 --> 00:13:46.480]   So if there is an instance and there are quite a few on mastodon that's a, there are
[00:13:46.480 --> 00:13:49.760]   nasty, nasty incidents instances or.
[00:13:49.760 --> 00:13:56.360]   It's hard to say, Nazi instances, you can block them whole hog, which I do.
[00:13:56.360 --> 00:14:01.760]   And so, so right now, if you're on Twitch, social, your experience is probably very benign.
[00:14:01.760 --> 00:14:02.760]   Probably.
[00:14:02.760 --> 00:14:03.760]   Yeah.
[00:14:03.760 --> 00:14:04.760]   Yeah.
[00:14:04.760 --> 00:14:07.760]   Maybe not a mastodon, that social one of the big instances.
[00:14:07.760 --> 00:14:08.760]   But that's the argument.
[00:14:08.760 --> 00:14:11.160]   The argument is a small instance, right?
[00:14:11.160 --> 00:14:14.960]   That can be handled by individuals and moderators, right?
[00:14:14.960 --> 00:14:20.560]   Leo, I'm not, I don't use, as long as I don't use the instances feed, I'm a mastodon social
[00:14:20.560 --> 00:14:24.240]   just because I was, you know, a dork and went to the first place available.
[00:14:24.240 --> 00:14:25.760]   It's not bad at all.
[00:14:25.760 --> 00:14:31.800]   What's really interesting that I've noticed in my long experience of two days on Blue
[00:14:31.800 --> 00:14:38.840]   Sky is that Black Twitter has adopted Blue Sky far more than mastodon.
[00:14:38.840 --> 00:14:39.840]   Yep.
[00:14:39.840 --> 00:14:40.840]   That's good.
[00:14:40.840 --> 00:14:44.760]   I mean, that's not good for mastodon, but I'm glad that they've found out that there's
[00:14:44.760 --> 00:14:45.760]   a lot of people who don't know what they're doing.
[00:14:45.760 --> 00:14:46.760]   Well, we'll see.
[00:14:46.760 --> 00:14:53.080]   But it was a smart thing where the devs valued that and they found some people to give the
[00:14:53.080 --> 00:14:56.520]   invites to, to give the invites to the people who matter.
[00:14:56.520 --> 00:14:58.280]   And there's a lot of people there.
[00:14:58.280 --> 00:15:06.160]   Sedettes there, Dr. A is there, a lot of folks are there who were not on mastodon.
[00:15:06.160 --> 00:15:12.160]   And Nastodon didn't give the best reception when I held the Black Twitter Summit in February.
[00:15:12.160 --> 00:15:18.360]   You know, one of the things we talked about was that the geeks of mastodon said to Black
[00:15:18.360 --> 00:15:22.240]   Twitter, "Well, these are our rules and make your own instance."
[00:15:22.240 --> 00:15:27.200]   And it was their way to say, "F you," to a community they weren't valuing sufficiently.
[00:15:27.200 --> 00:15:30.720]   So we'll see what happens on Blue Sky, but it's, I think, a very positive sign because
[00:15:30.720 --> 00:15:35.840]   the you can make, I think, a very good argument that the social things that work are the ones
[00:15:35.840 --> 00:15:42.840]   that are pushed into new uses by communities like Black Twitter.
[00:15:42.840 --> 00:15:48.480]   If I channel the founders of Blue Sky and I'll let you respond to this, Alex, they said,
[00:15:48.480 --> 00:15:52.880]   "Well, we've got to solve the issues of distributed ID.
[00:15:52.880 --> 00:15:57.640]   We've got to solve these issues of federation before we start putting in moderation tools,
[00:15:57.640 --> 00:15:59.600]   before we start putting in blocking."
[00:15:59.600 --> 00:16:04.160]   Because of course, if you don't have a solid framework for federation, then you don't
[00:16:04.160 --> 00:16:05.480]   even know what blocking means.
[00:16:05.480 --> 00:16:06.480]   Is blocking local?
[00:16:06.480 --> 00:16:07.480]   Is blocking federated?
[00:16:07.480 --> 00:16:08.480]   How do you federate it?
[00:16:08.480 --> 00:16:09.480]   What do you say?
[00:16:09.480 --> 00:16:10.480]   Yeah.
[00:16:10.480 --> 00:16:13.840]   And so one of the things they have done that is, I think, probably better, they're protocols
[00:16:13.840 --> 00:16:15.840]   called AT Proto.
[00:16:15.840 --> 00:16:19.640]   One of the things they have done better is it's very easily portable.
[00:16:19.640 --> 00:16:24.520]   Your identity on Blue Sky, mine's leo-loport.me.
[00:16:24.520 --> 00:16:31.160]   It doesn't say Blue Sky, and I can easily move that because it's a public key crypto-backed
[00:16:31.160 --> 00:16:35.640]   DID to another instance if I don't like an instance.
[00:16:35.640 --> 00:16:39.600]   So I think that that, you know, it's a mast on you can move, but it's a manual process.
[00:16:39.600 --> 00:16:40.600]   Right.
[00:16:40.600 --> 00:16:42.920]   It's a real pain, and you need the server to do the work for you.
[00:16:42.920 --> 00:16:43.920]   That's right.
[00:16:43.920 --> 00:16:45.880]   And that's a big issue because if the server goes dark...
[00:16:45.880 --> 00:16:46.880]   Right.
[00:16:46.880 --> 00:16:47.880]   Which is, there's been a bunch.
[00:16:47.880 --> 00:16:51.320]   Which I think both Blue Sky, all of these federated platforms are going to end up in
[00:16:51.320 --> 00:16:53.800]   really serious moderation politics.
[00:16:53.800 --> 00:16:54.800]   Right.
[00:16:54.800 --> 00:16:59.680]   That you see, if you look at admin block or feta block hashtags and mastodon, it is full
[00:16:59.680 --> 00:17:05.080]   of people turning their little personal fights into folks into blocking entire instances.
[00:17:05.080 --> 00:17:08.160]   And then hundreds or thousands of people complaining, "Why can't I follow my friends
[00:17:08.160 --> 00:17:09.320]   anymore?"
[00:17:09.320 --> 00:17:13.880]   Because these two people, and then also well-meaning people who are trying to run communities that
[00:17:13.880 --> 00:17:18.440]   are open and helpful and that don't have the kind of abuse you see on Twitter, are being
[00:17:18.440 --> 00:17:24.240]   driven away from hosting mastodon because of the abuse they get if they slightly deviate
[00:17:24.240 --> 00:17:30.440]   from the conventional wisdom of what people want or the most maximal kind of...
[00:17:30.440 --> 00:17:32.080]   So how would you architect it now?
[00:17:32.080 --> 00:17:34.640]   And we should say Alex has trust in Stafec's standings.
[00:17:34.640 --> 00:17:36.640]   You've done this before.
[00:17:36.640 --> 00:17:39.120]   You are the guy, if anybody, talk about it.
[00:17:39.120 --> 00:17:45.720]   Yeah, I mean, you wouldn't know, but I'm actually 22 years old.
[00:17:45.720 --> 00:17:48.200]   I wouldn't tell myself, "I've done a lot of trust in safety work.
[00:17:48.200 --> 00:17:50.080]   I'm teaching trust in safety class at Stanford right now.
[00:17:50.080 --> 00:17:54.080]   I kind of wish the Blue Sky folks had taken my class because this is exactly why I teach
[00:17:54.080 --> 00:17:58.960]   in class at Stanford is you should not be launching any kind of social product without
[00:17:58.960 --> 00:18:00.600]   having done a lot of the basic stuff.
[00:18:00.600 --> 00:18:02.680]   It just feels Blue Sky, they've launched...
[00:18:02.680 --> 00:18:05.320]   I think they've launched six months early based upon what's going on.
[00:18:05.320 --> 00:18:06.840]   That's the feeling I get as well.
[00:18:06.840 --> 00:18:10.920]   This is so early, but have they launched it still invite only?
[00:18:10.920 --> 00:18:14.520]   Yeah, well, it would be better be a little under the radar for a little longer.
[00:18:14.520 --> 00:18:16.680]   But what happened is the invite only wasn't working.
[00:18:16.680 --> 00:18:20.240]   The reason all this abuse came up is the initial invites didn't have enough entropy.
[00:18:20.240 --> 00:18:24.000]   And so it looks like there's no rate limiting on the API.
[00:18:24.000 --> 00:18:29.000]   So you had trolls just foreseen the invites and then walking themselves in so they didn't
[00:18:29.000 --> 00:18:32.440]   have to really burn anything of consequence to go abuse people.
[00:18:32.440 --> 00:18:36.960]   So yes, I think there's a number of interesting security and safety issues in Blue Sky.
[00:18:36.960 --> 00:18:42.240]   But in the long run, even if they build out a team, how do you do community management
[00:18:42.240 --> 00:18:45.440]   of rules in a federated world is going to be a fascinating problem?
[00:18:45.440 --> 00:18:49.360]   I also think we're going to need better tools for folks because our team's doing a bunch
[00:18:49.360 --> 00:18:50.360]   of work on this.
[00:18:50.360 --> 00:18:53.400]   I don't want to preview it too much, but we'll be publishing a paper probably in two, three
[00:18:53.400 --> 00:18:56.800]   weeks from now where we talk about child safety issues on Mastodon.
[00:18:56.800 --> 00:18:58.400]   And it's a really serious issue.
[00:18:58.400 --> 00:19:03.720]   It's a serious issue that you have to think about if you're a Mastodon owner.
[00:19:03.720 --> 00:19:08.920]   Because when your people subscribe to other folks content, that content gets pulled down
[00:19:08.920 --> 00:19:09.920]   and stored on your server.
[00:19:09.920 --> 00:19:12.040]   Yeah, we should explain how this works.
[00:19:12.040 --> 00:19:13.960]   So here's my Mastodon instance.
[00:19:13.960 --> 00:19:15.480]   There's a number of timelines.
[00:19:15.480 --> 00:19:18.880]   There's the local timeline, which is just people posting on Twitch social.
[00:19:18.880 --> 00:19:22.200]   Here's the people I follow kind of you like your normal Twitter timeline.
[00:19:22.200 --> 00:19:24.360]   But there's this federated timeline.
[00:19:24.360 --> 00:19:29.240]   This is a timeline of everybody followed by anybody on my server, which means if somebody
[00:19:29.240 --> 00:19:34.720]   on my server follows and it's somebody that has child porn, that is now on my server.
[00:19:34.720 --> 00:19:35.720]   Right.
[00:19:35.720 --> 00:19:39.600]   It gets fetched by your server and stored in your blob storage, which means I'm responsible
[00:19:39.600 --> 00:19:40.600]   for it.
[00:19:40.600 --> 00:19:44.280]   And you don't have the tools that Twitter has that Facebook hasn't such to go automatically
[00:19:44.280 --> 00:19:47.680]   check that image against known hash lists and such.
[00:19:47.680 --> 00:19:49.480]   But it's a solvable problem.
[00:19:49.480 --> 00:19:54.120]   But there's so much thinking about the fun parts and not a lot of people are spending
[00:19:54.120 --> 00:19:55.200]   a lot of time on this side.
[00:19:55.200 --> 00:19:57.600]   This is Silicon Valley writ large, right?
[00:19:57.600 --> 00:20:00.840]   I mean, it's, you know, move fast and break things.
[00:20:00.840 --> 00:20:05.320]   And you know, this comes up a lot in the crypto field as well, which is if they just consulted
[00:20:05.320 --> 00:20:08.920]   crypto experts, they wouldn't have done it this way.
[00:20:08.920 --> 00:20:12.240]   If they just consulted trust and safety experts, they wouldn't have done this.
[00:20:12.240 --> 00:20:13.240]   They would never have launched.
[00:20:13.240 --> 00:20:14.240]   I mean, this is the flip side.
[00:20:14.240 --> 00:20:15.240]   You should have the problem.
[00:20:15.240 --> 00:20:18.680]   And say people is we don't want you to do any like lawyers.
[00:20:18.680 --> 00:20:21.520]   If you if lawyers ran the world, nothing would ever happen.
[00:20:21.520 --> 00:20:22.520]   Wow.
[00:20:22.520 --> 00:20:25.160]   Not that bad shots fired.
[00:20:25.160 --> 00:20:26.160]   Yeah.
[00:20:26.160 --> 00:20:31.600]   But if I could just chime in on here for a second, I really want to back up what you're
[00:20:31.600 --> 00:20:32.600]   saying, Alex.
[00:20:32.600 --> 00:20:37.680]   And I think that's the fundamental flaw that I see with blue sky because their vision of
[00:20:37.680 --> 00:20:44.080]   the future is someone who does not have any resources behind them from blue skies.
[00:20:44.080 --> 00:20:46.600]   Even apparently is going to go out there.
[00:20:46.600 --> 00:20:52.440]   They're going to be writing all this metadata that you or I or some user can subscribe to
[00:20:52.440 --> 00:20:58.040]   to basically put a label on users and put a label on activity, like put a label on skits,
[00:20:58.040 --> 00:20:59.040]   right?
[00:20:59.040 --> 00:21:00.440]   This is not funded.
[00:21:00.440 --> 00:21:06.080]   And I was talking, you know, I was talking to the former head of Twitter, trusted safety
[00:21:06.080 --> 00:21:12.120]   about this yesterday on on blue sky where, you know, this is incredibly expensive work
[00:21:12.120 --> 00:21:13.600]   to do correctly.
[00:21:13.600 --> 00:21:18.760]   This is the entire reason Elon Musk has like automated a lot of this because it is so expensive
[00:21:18.760 --> 00:21:20.240]   to do trust and safety.
[00:21:20.240 --> 00:21:22.000]   Well, you do have to have oversight.
[00:21:22.000 --> 00:21:23.000]   You have to have transparency.
[00:21:23.000 --> 00:21:26.600]   You have to have the ability to appeal, right?
[00:21:26.600 --> 00:21:32.320]   So this is the part of it that I am deeply, deeply, deeply skeptical with blue sky.
[00:21:32.320 --> 00:21:37.840]   I mean, I suppose theoretically that you get enough users over there, enough of the, you
[00:21:37.840 --> 00:21:43.160]   know, major users of Twitter and someone comes along and they offer to do trust and safety
[00:21:43.160 --> 00:21:47.520]   as a product that I can subscribe to for $8 a month.
[00:21:47.520 --> 00:21:53.440]   Maybe that works like I could kind of see that working, but it's just there, their entire
[00:21:53.440 --> 00:21:57.400]   paradigm here in my view is solving the wrong problem.
[00:21:57.400 --> 00:22:03.120]   What I do think they've done right is they've gotten all the right people from Twitter,
[00:22:03.120 --> 00:22:08.040]   off Twitter, talking to each other on this service, enjoying what it's like to have a
[00:22:08.040 --> 00:22:13.440]   conversation without endless harassment and death threats with adults there.
[00:22:13.440 --> 00:22:16.080]   And we're really getting emotionally invested in it.
[00:22:16.080 --> 00:22:19.120]   So maybe they can turn this ship around by share.
[00:22:19.120 --> 00:22:22.880]   Alex is appraisal that this is work they should have been done, you know, six months
[00:22:22.880 --> 00:22:25.480]   ago and really ideally prior to the MVP.
[00:22:25.480 --> 00:22:30.120]   I didn't solve the plan at this way, but I have somehow put together the perfect panel
[00:22:30.120 --> 00:22:32.160]   for this, for this topic.
[00:22:32.160 --> 00:22:36.980]   Brianna Wu, one of the victims of gamer gate and have still a very, despite that, area
[00:22:36.980 --> 00:22:43.000]   active Twitter user, Alex, who's obviously the guy to talk to for trust and safety.
[00:22:43.000 --> 00:22:47.800]   Jeff is also very active in Twitter, has reached out to various Twitter communities.
[00:22:47.800 --> 00:22:51.920]   You just did a seminar for school on black Twitter.
[00:22:51.920 --> 00:22:53.560]   So this boy, I couldn't have put together.
[00:22:53.560 --> 00:22:58.280]   Now I didn't know blue sky was going to be the topic of the day, but it sure ended up
[00:22:58.280 --> 00:23:00.040]   being the perfect panel to discuss it.
[00:23:00.040 --> 00:23:01.040]   A lot of invites went out.
[00:23:01.040 --> 00:23:05.480]   I saw I, to both Brianna and Alex's point, I before we got on and I lost it.
[00:23:05.480 --> 00:23:10.300]   There was a, what are we going to call a thread, a skeet thread, a, I don't know, a do they
[00:23:10.300 --> 00:23:11.980]   have to read a spread.
[00:23:11.980 --> 00:23:13.900]   Okay, I like spread.
[00:23:13.900 --> 00:23:19.020]   We're going to go with I saw a spread saying that that I'm sure Alex, you're a little more
[00:23:19.020 --> 00:23:20.020]   about this.
[00:23:20.020 --> 00:23:24.420]   And I will the intelligence on some of the bad places where the bad people go.
[00:23:24.420 --> 00:23:30.220]   They're, you know, army on the border of Crimea here at blue sky, planning to come in and
[00:23:30.220 --> 00:23:31.980]   to attack trans people.
[00:23:31.980 --> 00:23:32.980]   Yeah.
[00:23:32.980 --> 00:23:36.180]   And when they're going to have search to be able to do that, they're going to have quote
[00:23:36.180 --> 00:23:37.180]   tweets to be able to do that.
[00:23:37.180 --> 00:23:40.720]   And they're blocking is going to be fairly limited.
[00:23:40.720 --> 00:23:45.320]   And so how with no staff, how does blue sky react?
[00:23:45.320 --> 00:23:48.580]   And very importantly, how does the community of blue sky react?
[00:23:48.580 --> 00:23:50.960]   How do we, well, who do we report to?
[00:23:50.960 --> 00:23:51.960]   What do we do?
[00:23:51.960 --> 00:23:53.920]   What happens when we see it happening?
[00:23:53.920 --> 00:23:58.920]   It's going to be a very crucial moment that I'm rooting for blue sky, but I'm worried
[00:23:58.920 --> 00:24:00.120]   for them at this moment.
[00:24:00.120 --> 00:24:01.120]   Yeah.
[00:24:01.120 --> 00:24:02.120]   Yeah.
[00:24:02.120 --> 00:24:08.720]   And the problem they have versus Macedon is the AT network right now is 99% one company.
[00:24:08.720 --> 00:24:09.720]   Right.
[00:24:09.720 --> 00:24:12.240]   So that handful of people is not even a big company.
[00:24:12.240 --> 00:24:13.240]   Right.
[00:24:13.240 --> 00:24:16.580]   So that kind of attack has happened multiple times in the Fediverse, but the responsibility
[00:24:16.580 --> 00:24:18.800]   for dealing with it is distributed across.
[00:24:18.800 --> 00:24:25.380]   You know, Masson, the social is the biggest, but not anywhere near 99% of the user base.
[00:24:25.380 --> 00:24:28.100]   And so I do think they're cruising for a brazil.
[00:24:28.100 --> 00:24:30.320]   They're going to have to hire some folks pretty fast.
[00:24:30.320 --> 00:24:32.320]   And so I think that's the only for them.
[00:24:32.320 --> 00:24:35.120]   Unfortunately, Google and Facebook and a bunch of other companies have been hiring, have
[00:24:35.120 --> 00:24:36.920]   been firing really good trust and safety people.
[00:24:36.920 --> 00:24:38.760]   Twitter is a good time to be hiring.
[00:24:38.760 --> 00:24:41.560]   So it is a great time to build a trust and safety team.
[00:24:41.560 --> 00:24:45.920]   Folks who you could never have hired out of these companies before because they would
[00:24:45.920 --> 00:24:48.200]   have been way too expensive are totally available now.
[00:24:48.200 --> 00:24:49.200]   Message Alex at cybervillains.com.
[00:24:49.200 --> 00:24:50.200]   No, no, no, no.
[00:24:50.200 --> 00:24:51.200]   He's got some records.
[00:24:51.200 --> 00:24:53.760]   No, but you can, I'm sure you know some names.
[00:24:53.760 --> 00:24:54.760]   Yeah.
[00:24:54.760 --> 00:24:58.080]   I'm sure you can go off this, but you all came to my, my black Twitter summit.
[00:24:58.080 --> 00:25:02.320]   They've been talking since then and others have been talking about the need to create
[00:25:02.320 --> 00:25:05.680]   these structures for mastodon and activity pub.
[00:25:05.680 --> 00:25:09.080]   But now there's going to be pulling in both directions because I think there's going to
[00:25:09.080 --> 00:25:11.160]   be an urgent need for blue sky as well.
[00:25:11.160 --> 00:25:14.640]   Let me step back as a user.
[00:25:14.640 --> 00:25:19.000]   So I'm watching all this with, with interest, but it, but kind of, I guess because I run
[00:25:19.000 --> 00:25:23.200]   a mastodon instance, I'm not, I do have a little dog in this time, but as a user looking
[00:25:23.200 --> 00:25:26.440]   at the culture, looking at the content and so forth.
[00:25:26.440 --> 00:25:31.960]   Let me, is it over for Twitter, first of all, or is Twitter worth trying to save?
[00:25:31.960 --> 00:25:36.400]   Oh, I mean, we're acting as if, oh yeah, what Twitter's done.
[00:25:36.400 --> 00:25:37.400]   What's next?
[00:25:37.400 --> 00:25:38.720]   But is it over?
[00:25:38.720 --> 00:25:41.160]   I think that I actually do.
[00:25:41.160 --> 00:25:46.320]   I think it, what's the quote where you go bankrupt slowly and then quickly?
[00:25:46.320 --> 00:25:48.440]   I do think that's Twitter's fate.
[00:25:48.440 --> 00:25:54.320]   I mean, you know, under Elon Musk, I, I don't think I'm the only person has this experience.
[00:25:54.320 --> 00:26:00.120]   Twitter is a remarkably bad place to spend time and I was there for gamergate.
[00:26:00.120 --> 00:26:02.280]   So it's gotten worse.
[00:26:02.280 --> 00:26:03.280]   It's worse now.
[00:26:03.280 --> 00:26:05.720]   Every, every Tuesday is like gamergate now.
[00:26:05.720 --> 00:26:07.920]   It's, it's so bad.
[00:26:07.920 --> 00:26:14.400]   You know, the, the thing is you can't tell anyone who's real in every single conversation.
[00:26:14.400 --> 00:26:20.880]   You've got a bunch of crypto jerkstorves warming their way in their harassment is crazy.
[00:26:20.880 --> 00:26:23.040]   You know, death threats, rape threats.
[00:26:23.040 --> 00:26:24.360]   They're just 90 years.
[00:26:24.360 --> 00:26:27.760]   Here's the argument people would use is that I've heard people use this.
[00:26:27.760 --> 00:26:32.560]   Well, don't ever look at the four U tab just following just the people you follow.
[00:26:32.560 --> 00:26:33.560]   That's not full of it.
[00:26:33.560 --> 00:26:34.560]   It doesn't.
[00:26:34.560 --> 00:26:38.200]   It doesn't help because they rise to the top of your mentions.
[00:26:38.200 --> 00:26:43.240]   Like even when I'm looking through my individual users, it's all the blue checks that are there.
[00:26:43.240 --> 00:26:44.240]   Yeah.
[00:26:44.240 --> 00:26:46.320]   I stopped looking at replies about 10 years ago though.
[00:26:46.320 --> 00:26:48.320]   So I mean, fair enough.
[00:26:48.320 --> 00:26:49.320]   That's a problem.
[00:26:49.320 --> 00:26:55.400]   It is, I think the Elon made a really fatal mistake here with de-certifying all the journalists.
[00:26:55.400 --> 00:27:01.640]   The only value that Twitter has in my mind is it is the best place to talk about crazy
[00:27:01.640 --> 00:27:06.560]   events as they are happening, like the Will Smith slap, right?
[00:27:06.560 --> 00:27:11.760]   So if you have a place with a critical mass of those journalists to talk about this stuff,
[00:27:11.760 --> 00:27:15.120]   that is something blue sky can definitely become the home up.
[00:27:15.120 --> 00:27:19.360]   So Twitter doesn't have to get all the, like Twitter doesn't have to hemorrhage all their
[00:27:19.360 --> 00:27:20.360]   users.
[00:27:20.360 --> 00:27:23.320]   They just have to hemorrhage the most important power users.
[00:27:23.320 --> 00:27:24.320]   And I've used that.
[00:27:24.320 --> 00:27:25.320]   It's already a blue sky.
[00:27:25.320 --> 00:27:26.320]   Well, okay.
[00:27:26.320 --> 00:27:27.320]   So wait a minute.
[00:27:27.320 --> 00:27:28.320]   You've conflated two different things.
[00:27:28.320 --> 00:27:32.200]   So one of the reasons Twitter was good is so that you could see what people were saying
[00:27:32.200 --> 00:27:34.760]   about the slap, not just journalists.
[00:27:34.760 --> 00:27:39.520]   You could see what the zeitgeist was, what people were saying.
[00:27:39.520 --> 00:27:41.080]   So that's one thing.
[00:27:41.080 --> 00:27:50.960]   And there's also the important voices, the, I don't know what, the verified voices.
[00:27:50.960 --> 00:27:52.960]   That's another thing.
[00:27:52.960 --> 00:27:55.520]   People aren't leaving Twitter or are they?
[00:27:55.520 --> 00:27:56.920]   Some verified voices have.
[00:27:56.920 --> 00:28:01.400]   I mean, that's what happened this week in blue sky was a lot of well known people said,
[00:28:01.400 --> 00:28:02.400]   that's it.
[00:28:02.400 --> 00:28:03.400]   I'm done.
[00:28:03.400 --> 00:28:04.400]   Right.
[00:28:04.400 --> 00:28:13.600]   But I also noticed AOC Alexandria Ocasio-Cortez, the member of Congress from New York was on
[00:28:13.600 --> 00:28:14.600]   blue sky.
[00:28:14.600 --> 00:28:19.000]   But she said, but this is my personal account because I can't move my account as AOC rep
[00:28:19.000 --> 00:28:22.080]   AOC over yet, my government account over yet.
[00:28:22.080 --> 00:28:25.280]   It's still on Twitter because it hasn't, I don't know.
[00:28:25.280 --> 00:28:26.480]   So that hasn't been approved.
[00:28:26.480 --> 00:28:27.480]   There's some sort of process.
[00:28:27.480 --> 00:28:31.120]   Well, she was saying, I mean, one of the problems is there's not basic security stuff
[00:28:31.120 --> 00:28:32.120]   on blue sky, right?
[00:28:32.120 --> 00:28:33.120]   So it doesn't have multi-factor authentic.
[00:28:33.120 --> 00:28:34.120]   There's no two factors.
[00:28:34.120 --> 00:28:35.880]   It doesn't have verification itself.
[00:28:35.880 --> 00:28:40.320]   So the problem that Twitter has created for themselves, the problem that blue sky and
[00:28:40.320 --> 00:28:41.320]   mast on are facing.
[00:28:41.320 --> 00:28:42.760]   They just embraced it.
[00:28:42.760 --> 00:28:43.760]   Right.
[00:28:43.760 --> 00:28:50.520]   From a historical perspective, Twitter had the best understood, the most recognized brand
[00:28:50.520 --> 00:28:53.480]   value in the blue check mark of what it meant to be verified.
[00:28:53.480 --> 00:28:54.640]   And they threw that away.
[00:28:54.640 --> 00:28:57.360]   They burned all of that brand value.
[00:28:57.360 --> 00:29:00.080]   They burned it down to the ground.
[00:29:00.080 --> 00:29:01.080]   At the moment, that-
[00:29:01.080 --> 00:29:02.320]   Worse, worse, Alex.
[00:29:02.320 --> 00:29:03.320]   It became a negative.
[00:29:03.320 --> 00:29:04.320]   Right.
[00:29:04.320 --> 00:29:05.320]   It became a negative.
[00:29:05.320 --> 00:29:06.320]   Yes.
[00:29:06.320 --> 00:29:07.320]   I'd say it's the mark of the eight bucks schmuck.
[00:29:07.320 --> 00:29:10.680]   If you have the blue check, not just me, Liz, it says that you're an idiot.
[00:29:10.680 --> 00:29:13.120]   It says you're an idiot or you're a faker.
[00:29:13.120 --> 00:29:15.880]   And we're at a historical moment.
[00:29:15.880 --> 00:29:21.720]   We're entering a time in which any kind of thing that human beings can generate, text,
[00:29:21.720 --> 00:29:27.400]   video, audio, all of that stuff is now easily fakable using large language models.
[00:29:27.400 --> 00:29:28.400]   Oh, yeah.
[00:29:28.400 --> 00:29:29.400]   Yeah.
[00:29:29.400 --> 00:29:34.400]   So as a result, like, from a to throw away identity as something that you can use as trustworthy
[00:29:34.400 --> 00:29:39.680]   in your platform, at the moment that it has actually become economical to run tens of
[00:29:39.680 --> 00:29:44.760]   thousands of accounts that look like they're legitimately human is incredibly stupid.
[00:29:44.760 --> 00:29:49.320]   And I think this is going to be the story over the next two years or so is what platform
[00:29:49.320 --> 00:29:54.000]   is going to be able to give you trust in the identity of the people who are joining as
[00:29:54.000 --> 00:29:56.080]   well as the possibility that they're not-
[00:29:56.080 --> 00:29:59.800]   But the positive identity is this Leo LaPorte who's saying this.
[00:29:59.800 --> 00:30:03.600]   And there's also kind of the negative identity of, well, if I'm talking to somebody, is it
[00:30:03.600 --> 00:30:07.760]   unlikely that they're part of a botnet of 10,000, 20,000 fake accounts that are being
[00:30:07.760 --> 00:30:13.040]   driven out of St. Petersburg or Tehran or, you know, Saudi Arabia or a variety of other
[00:30:13.040 --> 00:30:15.000]   places that love to manipulate the internet?
[00:30:15.000 --> 00:30:16.520]   And that's what's gone away at Twitter.
[00:30:16.520 --> 00:30:20.160]   Partially because the blue check thing, partially because all of the people who do anti-influenced
[00:30:20.160 --> 00:30:23.000]   operation stuff have either been fired or quit.
[00:30:23.000 --> 00:30:27.320]   And so I think one of the things Breonna is talking about here is you do not know when
[00:30:27.320 --> 00:30:31.800]   you're interacting with people whether or not they're part of massive manipulatory
[00:30:31.800 --> 00:30:32.800]   brain networks.
[00:30:32.800 --> 00:30:33.800]   Okay.
[00:30:33.800 --> 00:30:34.800]   But you all know about that.
[00:30:34.800 --> 00:30:38.640]   But I would submit that the average user doesn't neither knows nor cares.
[00:30:38.640 --> 00:30:40.600]   Well, they don't know whether it's fake or not.
[00:30:40.600 --> 00:30:41.600]   I mean, they're not- They don't care.
[00:30:41.600 --> 00:30:44.600]   They're not following the literature on what the Islamic Revolutionary Guard Corps is doing
[00:30:44.600 --> 00:30:45.600]   on Twitter.
[00:30:45.600 --> 00:30:46.600]   Right.
[00:30:46.600 --> 00:30:49.080]   But what they do feel is that if they make a political statement that is disagreeable
[00:30:49.080 --> 00:30:52.640]   by somebody, they will end up with 500 people sending them what looks like death threats
[00:30:52.640 --> 00:30:53.840]   or calling them a schmuck.
[00:30:53.840 --> 00:30:54.840]   That's pretty bad.
[00:30:54.840 --> 00:30:55.840]   Right.
[00:30:55.840 --> 00:30:59.160]   And that experience that everybody is now having the gamer gate experience.
[00:30:59.160 --> 00:31:00.160]   Everybody gets it.
[00:31:00.160 --> 00:31:01.160]   Yeah.
[00:31:01.160 --> 00:31:03.000]   Makes the value of the platform go.
[00:31:03.000 --> 00:31:06.800]   But so maybe you don't say anything that's ch- that's political and just talk about the
[00:31:06.800 --> 00:31:07.800]   Warriors game.
[00:31:07.800 --> 00:31:08.800]   Yes.
[00:31:08.800 --> 00:31:10.080]   And then it's okay, right?
[00:31:10.080 --> 00:31:11.320]   Is it good for something like that?
[00:31:11.320 --> 00:31:13.240]   You want to talk about the King's Warriors?
[00:31:13.240 --> 00:31:14.240]   Is it a good place to play?
[00:31:14.240 --> 00:31:15.440]   That's not what people are on Twitter on, right?
[00:31:15.440 --> 00:31:17.280]   I mean, that's the Pinterest model, right?
[00:31:17.280 --> 00:31:19.800]   It's like this is a fun place where people don't have serious conversation.
[00:31:19.800 --> 00:31:22.600]   But Twitter was a place for serious political conversation.
[00:31:22.600 --> 00:31:23.600]   That's out.
[00:31:23.600 --> 00:31:24.600]   And it's not useful.
[00:31:24.600 --> 00:31:25.840]   Clearly that's out, right?
[00:31:25.840 --> 00:31:27.320]   That's gone.
[00:31:27.320 --> 00:31:29.720]   Can you go there to get news?
[00:31:29.720 --> 00:31:31.120]   Not trustworthy news, right?
[00:31:31.120 --> 00:31:32.120]   Like...
[00:31:32.120 --> 00:31:35.120]   Again, though, I think most people don't know whether it's trustworthy or not, right?
[00:31:35.120 --> 00:31:36.480]   I think most people aren't aware of that.
[00:31:36.480 --> 00:31:37.480]   Yeah.
[00:31:37.480 --> 00:31:39.320]   Which is a problem for the polity, obviously.
[00:31:39.320 --> 00:31:40.920]   I think it's brought...
[00:31:40.920 --> 00:31:43.400]   2024 is going to be a disaster.
[00:31:43.400 --> 00:31:44.400]   It's the election year.
[00:31:44.400 --> 00:31:46.920]   It is going to be by far worse than 2016 in that...
[00:31:46.920 --> 00:31:49.160]   Because all the troll farms are going to be out in force.
[00:31:49.160 --> 00:31:50.320]   Troll farms are out in force.
[00:31:50.320 --> 00:31:53.120]   They're going to have large language models behind them.
[00:31:53.120 --> 00:31:55.040]   Not the ones that are being hosted at Google or...
[00:31:55.040 --> 00:31:57.040]   No, but that automates their process, right?
[00:31:57.040 --> 00:31:59.360]   So they can create an onslaught.
[00:31:59.360 --> 00:32:01.280]   They have an army now.
[00:32:01.280 --> 00:32:05.960]   And Twitter has completely given up on preventing these kinds of botnets, right?
[00:32:05.960 --> 00:32:07.440]   Or it's encouraging.
[00:32:07.440 --> 00:32:08.760]   Or it's encouraging them.
[00:32:08.760 --> 00:32:12.240]   For eight bucks, you can verify all these accounts and get yourself raised up.
[00:32:12.240 --> 00:32:16.160]   That is totally economically viable for the professionals who manipulate social...
[00:32:16.160 --> 00:32:17.160]   For Russia or China.
[00:32:17.160 --> 00:32:18.160]   For Russia or China.
[00:32:18.160 --> 00:32:23.320]   For more domestic groups who are renting out this capability, it's not clear it's illegal
[00:32:23.320 --> 00:32:28.040]   for an American politician to hire a local troll farm to go troll their opponents.
[00:32:28.040 --> 00:32:29.960]   OMG, it's not.
[00:32:29.960 --> 00:32:34.720]   You probably can't use campaign funds for it, but it's an interesting question.
[00:32:34.720 --> 00:32:35.840]   Can you use pack funds?
[00:32:35.840 --> 00:32:36.840]   One of the things that...
[00:32:36.840 --> 00:32:38.840]   That's a good question.
[00:32:38.840 --> 00:32:39.840]   Can you use pack funds?
[00:32:39.840 --> 00:32:41.840]   You know that might...
[00:32:41.840 --> 00:32:45.120]   So what do we know?
[00:32:45.120 --> 00:32:48.760]   Well, I lost the train of thought.
[00:32:48.760 --> 00:32:56.160]   One of the things that became an issue on Twitter...
[00:32:56.160 --> 00:33:01.360]   So is Elon just going to run it into the ground?
[00:33:01.360 --> 00:33:05.600]   And I mean, he's got to be close to bankruptcy at this point already.
[00:33:05.600 --> 00:33:09.400]   So can we just grieve Twitter and move on?
[00:33:09.400 --> 00:33:11.400]   Is that what we should do?
[00:33:11.400 --> 00:33:15.800]   Right now, Twitter isn't the best position company to take advantage of the...
[00:33:15.800 --> 00:33:18.280]   He could just switch it all back on.
[00:33:18.280 --> 00:33:19.280]   So one option is...
[00:33:19.280 --> 00:33:22.760]   I don't think as long as he's running it, the people are going to come back.
[00:33:22.760 --> 00:33:27.200]   But if he sold it for a humongous loss or if he let it go into receivership and it's
[00:33:27.200 --> 00:33:29.600]   being run by the banks to which it will be.
[00:33:29.600 --> 00:33:31.480]   I think Twitter could turn it all around.
[00:33:31.480 --> 00:33:35.440]   Rehire the trust and safety team, get you all wrapped back in.
[00:33:35.440 --> 00:33:39.160]   They've got all the software that they had before.
[00:33:39.160 --> 00:33:43.760]   Nick Masnick, Mox Elon, saying he's speedrunning the moderation curve and doing a bad job of
[00:33:43.760 --> 00:33:44.760]   it.
[00:33:44.760 --> 00:33:45.760]   But you can rewind it.
[00:33:45.760 --> 00:33:46.760]   Right, you could put it all back.
[00:33:46.760 --> 00:33:50.560]   And Twitter had over 16 years kind of a refined...
[00:33:50.560 --> 00:33:53.640]   They'd certainly gone through this long enough that they had pretty refined.
[00:33:53.640 --> 00:33:56.720]   You think their system was good enough?
[00:33:56.720 --> 00:33:57.720]   It was rough.
[00:33:57.720 --> 00:34:02.480]   I mean, Twitter, Mark Zuckerberg, you know, a clown car that drove into a gold mine,
[00:34:02.480 --> 00:34:03.480]   certainly.
[00:34:03.480 --> 00:34:05.480]   Is it like an accurate description of Twitter?
[00:34:05.480 --> 00:34:09.840]   And if we're just the same, Twitter had some of the best people, but they were never well
[00:34:09.840 --> 00:34:14.840]   resourced compared to YouTube and Facebook and other companies like that.
[00:34:14.840 --> 00:34:15.840]   Yeah.
[00:34:15.840 --> 00:34:18.520]   So let's talk like pragmatically.
[00:34:18.520 --> 00:34:20.600]   Let's take a break and then we're going to talk pragmatically.
[00:34:20.600 --> 00:34:21.600]   I love this.
[00:34:21.600 --> 00:34:22.600]   This is a great conversation.
[00:34:22.600 --> 00:34:24.960]   I'm sorry if you have no interest in this.
[00:34:24.960 --> 00:34:29.520]   But honestly, the future of social is kind of hanging by a threat at this point.
[00:34:29.520 --> 00:34:30.520]   A lot of people...
[00:34:30.520 --> 00:34:32.920]   A lot of people have said social is over.
[00:34:32.920 --> 00:34:33.960]   You know, like it's over.
[00:34:33.960 --> 00:34:35.280]   It's just forget about it.
[00:34:35.280 --> 00:34:42.840]   I hate to give it up because it's been a valuable way to communicate, to have people be heard.
[00:34:42.840 --> 00:34:46.640]   But if we can't solve these trust and safety issues, if we can't solve the troll farm issues,
[00:34:46.640 --> 00:34:47.640]   maybe it is over.
[00:34:47.640 --> 00:34:49.880]   Maybe it's too potent a weapon for that actors.
[00:34:49.880 --> 00:34:54.280]   Yeah, they'd be very sad if we go back to you have to own a television station or a newspaper.
[00:34:54.280 --> 00:34:55.840]   Yeah, we don't want to go back to that.
[00:34:55.840 --> 00:34:56.840]   Yeah.
[00:34:56.840 --> 00:34:59.800]   A lot of the television station owners and the newspaper publishers want that.
[00:34:59.800 --> 00:35:00.800]   They love that idea.
[00:35:00.800 --> 00:35:01.800]   Yeah, they do like that.
[00:35:01.800 --> 00:35:02.800]   Yeah.
[00:35:02.800 --> 00:35:04.560]   What about podcast network owners?
[00:35:04.560 --> 00:35:05.560]   What about them?
[00:35:05.560 --> 00:35:06.560]   Nobody ever mentions?
[00:35:06.560 --> 00:35:07.560]   Big podcast.
[00:35:07.560 --> 00:35:10.880]   Big, it's going to be big someday.
[00:35:10.880 --> 00:35:12.360]   I promise you.
[00:35:12.360 --> 00:35:14.320]   Great panel for this.
[00:35:14.320 --> 00:35:15.760]   Brianna Wu, it's wonderful to see you.
[00:35:15.760 --> 00:35:17.080]   Wonderful to have you back.
[00:35:17.080 --> 00:35:20.440]   rebellionpack.com.
[00:35:20.440 --> 00:35:23.360]   That is your Frank Wu memorial mug.
[00:35:23.360 --> 00:35:24.360]   I love it.
[00:35:24.360 --> 00:35:25.360]   It's beautiful.
[00:35:25.360 --> 00:35:26.520]   You married him for his mug.
[00:35:26.520 --> 00:35:28.560]   I married him just for this mug right now.
[00:35:28.560 --> 00:35:29.560]   That is a great mug.
[00:35:29.560 --> 00:35:30.560]   You've loved that.
[00:35:30.560 --> 00:35:32.800]   15 years, but it was for...
[00:35:32.800 --> 00:35:35.720]   You got the mug.
[00:35:35.720 --> 00:35:37.120]   Say hi to Frank.
[00:35:37.120 --> 00:35:41.480]   I think some of those illustrations, are they woos?
[00:35:41.480 --> 00:35:46.960]   No, that's all Capcom CBS 2 are, but we're going to be out there for live show in a month.
[00:35:46.960 --> 00:35:48.640]   So maybe we can all go get dinner.
[00:35:48.640 --> 00:35:49.640]   For a live to it?
[00:35:49.640 --> 00:35:50.640]   Yes.
[00:35:50.640 --> 00:35:51.640]   Fantastic.
[00:35:51.640 --> 00:35:52.640]   Can't wait.
[00:35:52.640 --> 00:35:54.920]   Great to have Alex Stamos in studio.
[00:35:54.920 --> 00:35:56.680]   We don't see people in studio very often.
[00:35:56.680 --> 00:36:00.240]   I'll try not to breathe on you, Alex.
[00:36:00.240 --> 00:36:07.480]   Alex works with Chris Krebs, the legendary, two great names with the Krebs Stamos group.
[00:36:07.480 --> 00:36:09.880]   What do you do at the Krebs Stamos group?
[00:36:09.880 --> 00:36:14.640]   We work with companies to help them deal with their big picture cyber risk, mostly geopolitical
[00:36:14.640 --> 00:36:15.640]   cyber risk.
[00:36:15.640 --> 00:36:20.320]   So again, I couldn't have put together a better panel for this dang show completely
[00:36:20.320 --> 00:36:21.320]   inadvertently.
[00:36:21.320 --> 00:36:25.840]   And Jeff Jarvis, I think you all know him from buzzmachine.com and his new book.
[00:36:25.840 --> 00:36:27.200]   The grown-up's table today.
[00:36:27.200 --> 00:36:28.680]   I won't listen to what that happened.
[00:36:28.680 --> 00:36:33.080]   You see him, of course, every Wednesday on the...
[00:36:33.080 --> 00:36:34.080]   What is that show?
[00:36:34.080 --> 00:36:35.080]   Twig.
[00:36:35.080 --> 00:36:36.080]   It's called Twig.
[00:36:36.080 --> 00:36:37.080]   This week in...
[00:36:37.080 --> 00:36:38.080]   Twig.
[00:36:38.080 --> 00:36:39.080]   Yeah.
[00:36:39.080 --> 00:36:40.080]   Yeah.
[00:36:40.080 --> 00:36:41.080]   Soon you forget.
[00:36:41.080 --> 00:36:42.560]   He's also the author of the Gutenberg parentheses.
[00:36:42.560 --> 00:36:46.000]   And we're getting closer and closer to the release.
[00:36:46.000 --> 00:36:47.000]   Yes, June.
[00:36:47.000 --> 00:36:50.160]   But you can order it now for discounts.
[00:36:50.160 --> 00:36:51.160]   Let's go.
[00:36:51.160 --> 00:36:52.560]   I'm giving you a chance to plug it.
[00:36:52.560 --> 00:36:57.840]   It's the word parenthesis.parenthys.com.
[00:36:57.840 --> 00:37:01.040]   It's hard to spell.
[00:37:01.040 --> 00:37:02.040]   I got it.
[00:37:02.040 --> 00:37:04.480]   And there's a lot of letters.
[00:37:04.480 --> 00:37:07.000]   The age of print and its lessons for the age of internet.
[00:37:07.000 --> 00:37:08.680]   It couldn't be better.
[00:37:08.680 --> 00:37:09.680]   All three of you.
[00:37:09.680 --> 00:37:12.040]   It's a wonderful blue spree, by the way, publishes this.
[00:37:12.040 --> 00:37:15.840]   And is that deal, the Barnes and Noble deal still good?
[00:37:15.840 --> 00:37:17.280]   No, it was too Friday.
[00:37:17.280 --> 00:37:18.280]   Oh, shoot.
[00:37:18.280 --> 00:37:20.640]   But you get a discount on Bloomsbury.
[00:37:20.640 --> 00:37:28.200]   So my favorite blackwells in the UK, I love, because I can buy English, British books and
[00:37:28.200 --> 00:37:30.760]   American books at a discount with free shipping.
[00:37:30.760 --> 00:37:31.760]   To the US.
[00:37:31.760 --> 00:37:33.280]   Of course, we'll double link to indie books as well.
[00:37:33.280 --> 00:37:34.280]   Yeah.
[00:37:34.280 --> 00:37:35.280]   Let's give them all a plug.
[00:37:35.280 --> 00:37:37.200]   We want to keep those independent bookstores alive.
[00:37:37.200 --> 00:37:40.040]   It's good to have all three of you here.
[00:37:40.040 --> 00:37:44.320]   You know, also I'd like to keep alive the United States Postal Service friends.
[00:37:44.320 --> 00:37:46.760]   You always...
[00:37:46.760 --> 00:37:48.960]   You never know what's going to happen to the USPS.
[00:37:48.960 --> 00:37:51.320]   If gosh started, it's an important part of Democracy.
[00:37:51.320 --> 00:37:53.800]   Ben Franklin started it and it's still going.
[00:37:53.800 --> 00:37:54.840]   But I got to tell you one thing.
[00:37:54.840 --> 00:38:00.200]   You do not have to actually go to the post office to get the services of the US Postal
[00:38:00.200 --> 00:38:01.200]   Service.
[00:38:01.200 --> 00:38:06.520]   In fact, here at Twit for the last, I don't know, 15 years, we've been using stamps.com.
[00:38:06.520 --> 00:38:11.320]   For the last 25 years, stamps.com has been helping businesses save time and money because
[00:38:11.320 --> 00:38:17.200]   with stamps.com, you can print real US postage from your computer with your printer.
[00:38:17.200 --> 00:38:19.640]   No postage meter necessary.
[00:38:19.640 --> 00:38:25.560]   You can even tell stamps.com, have them come and get it and the uniformed employee of the
[00:38:25.560 --> 00:38:30.680]   federal government will come and pick up your package and send it on its way.
[00:38:30.680 --> 00:38:34.800]   You could focus on your business because stamps.com has your postage needs covered.
[00:38:34.800 --> 00:38:35.800]   Plus you get discounts.
[00:38:35.800 --> 00:38:37.160]   You can't get it the post office.
[00:38:37.160 --> 00:38:38.160]   Great rates too.
[00:38:38.160 --> 00:38:40.600]   They've been a partner here.
[00:38:40.600 --> 00:38:43.640]   We've been advertising on our shows since 2012.
[00:38:43.640 --> 00:38:44.640]   That's 11 years now.
[00:38:44.640 --> 00:38:45.640]   I got to ask.
[00:38:45.640 --> 00:38:46.640]   You haven't tried them yet.
[00:38:46.640 --> 00:38:47.640]   What are you waiting for?
[00:38:47.640 --> 00:38:48.640]   Oh, I know.
[00:38:48.640 --> 00:38:50.880]   How about if I make Sweet in the Pot?
[00:38:50.880 --> 00:38:55.080]   Because now stamps.com also works with UPS.
[00:38:55.080 --> 00:38:56.320]   Yes.
[00:38:56.320 --> 00:38:59.760]   So now really all your shipping needs are handled.
[00:38:59.760 --> 00:39:05.920]   Stamps.com has huge carrier discounts up to 84% off US Postal Service and UPS rates.
[00:39:05.920 --> 00:39:09.080]   They've negotiated a very sweet deal with UPS.
[00:39:09.080 --> 00:39:10.760]   It'll save you a lot of money.
[00:39:10.760 --> 00:39:14.320]   And again, you use your computer, print those labels.
[00:39:14.320 --> 00:39:17.200]   They'll even send you a free scale so you get exactly the...
[00:39:17.200 --> 00:39:20.400]   You don't pay one penny more for shipping than you need to.
[00:39:20.400 --> 00:39:21.520]   They'll suggest...
[00:39:21.520 --> 00:39:24.680]   If you put a book on there, they'll say, "Have you thought about media rates?"
[00:39:24.680 --> 00:39:26.960]   They'll suggest better rates.
[00:39:26.960 --> 00:39:27.960]   It's so great.
[00:39:27.960 --> 00:39:30.640]   And if you sell products online, it is the most professional way.
[00:39:30.640 --> 00:39:36.880]   If you're an Etsy or eBay or Amazon seller, it seamlessly connects with every major marketplace
[00:39:36.880 --> 00:39:37.880]   and shopping cart.
[00:39:37.880 --> 00:39:39.040]   So it automatically...
[00:39:39.040 --> 00:39:44.280]   You don't know typos because it's going to take that address directly from the site.
[00:39:44.280 --> 00:39:46.960]   Your return dress is automatically filled in your logo too.
[00:39:46.960 --> 00:39:50.560]   If you want to look so professional, you always have exactly the right...
[00:39:50.560 --> 00:39:53.760]   I can't tell you how many times I've gotten packages from Etsy.
[00:39:53.760 --> 00:39:59.240]   Brown paper, twine, licked stamps placed on the package.
[00:39:59.240 --> 00:40:02.200]   And it's not unusual for it to be postage due.
[00:40:02.200 --> 00:40:04.200]   See, that's a bad impression.
[00:40:04.200 --> 00:40:06.240]   Do it right with stamps.com.
[00:40:06.240 --> 00:40:10.240]   They automatically tell you your best, cheapest, fastest shipping options.
[00:40:10.240 --> 00:40:11.800]   You save money.
[00:40:11.800 --> 00:40:17.440]   For 25 years now, stamps.com has been indispensable for over 1 million businesses, including ours.
[00:40:17.440 --> 00:40:19.000]   You get access to the postal service.
[00:40:19.000 --> 00:40:20.880]   You get access to UPS.
[00:40:20.880 --> 00:40:23.760]   No lines, no traffic, no waiting any time of the day or night.
[00:40:23.760 --> 00:40:25.640]   Set your business up for success.
[00:40:25.640 --> 00:40:26.640]   Get started with stamps.com.
[00:40:26.640 --> 00:40:29.220]   We have a really great offer for you.
[00:40:29.220 --> 00:40:31.080]   Use the promo code TWIT.
[00:40:31.080 --> 00:40:35.360]   The special offer, the TWIT offer, just click the link up in the right, the microphone,
[00:40:35.360 --> 00:40:36.360]   enter TWIT.
[00:40:36.360 --> 00:40:38.720]   You get four weeks, a four week trial.
[00:40:38.720 --> 00:40:44.040]   You get free postage to use over a period of time and that digital scale, no long-term
[00:40:44.040 --> 00:40:46.720]   commitments, no contracts.
[00:40:46.720 --> 00:40:47.720]   Stamps.com.
[00:40:47.720 --> 00:40:50.320]   Just click that microphone at the top of the page.
[00:40:50.320 --> 00:40:51.440]   Remember the offer code TWIT though.
[00:40:51.440 --> 00:40:53.600]   That's very important so they know you saw it here.
[00:40:53.600 --> 00:40:55.080]   Thank you, stamps are supporting TWIT.
[00:40:55.080 --> 00:40:57.640]   Thank you for supporting TWIT.
[00:40:57.640 --> 00:40:58.640]   Stamps.com.
[00:40:58.640 --> 00:41:06.840]   All right, I didn't say put a pin in it, but I probably should have Brianna because I interrupted
[00:41:06.840 --> 00:41:07.840]   you.
[00:41:07.840 --> 00:41:08.840]   No worries.
[00:41:08.840 --> 00:41:09.840]   And this is...
[00:41:09.840 --> 00:41:10.840]   And this is...
[00:41:10.840 --> 00:41:14.640]   There couldn't be a better time to talk about the future of Twitter, the alternatives to
[00:41:14.640 --> 00:41:17.040]   Twitter, and there couldn't be a better panel to do this.
[00:41:17.040 --> 00:41:19.320]   So continue with your thought.
[00:41:19.320 --> 00:41:23.480]   So I would love to get some consensus from everyone here.
[00:41:23.480 --> 00:41:29.760]   I think we all agree that Blue Sky has a better chance to make it than most social media
[00:41:29.760 --> 00:41:30.760]   networks.
[00:41:30.760 --> 00:41:35.160]   I think if you want to run the numbers, it's probably safest about unfailier for all of
[00:41:35.160 --> 00:41:36.800]   these.
[00:41:36.800 --> 00:41:39.840]   But I think they have a better chance than average.
[00:41:39.840 --> 00:41:45.080]   So if they did want to win in the long run, Alex, I'd really love to know your opinion
[00:41:45.080 --> 00:41:46.080]   here.
[00:41:46.080 --> 00:41:48.000]   This is what I think they need to do.
[00:41:48.000 --> 00:41:53.240]   I know they want to do the mastodon thing where you add some metadata to your server and
[00:41:53.240 --> 00:41:55.080]   verify yourself that way.
[00:41:55.080 --> 00:41:56.920]   I think this is a losing idea.
[00:41:56.920 --> 00:42:03.600]   I think they should just commit to identifying and manually reviewing and re-verifying everybody
[00:42:03.600 --> 00:42:05.680]   that was formally verified at Twitter.
[00:42:05.680 --> 00:42:10.720]   I think especially going into an election, I think that would be a huge draw for all of
[00:42:10.720 --> 00:42:12.480]   those users.
[00:42:12.480 --> 00:42:17.280]   And I think it would very much be worth the money because the users you want are the power
[00:42:17.280 --> 00:42:18.280]   users.
[00:42:18.280 --> 00:42:23.400]   I saw one study that said it was like 1/100th of a hundredth of the Twitter users that generated
[00:42:23.400 --> 00:42:25.560]   like 90% of the content.
[00:42:25.560 --> 00:42:27.160]   We're the nut jobs that are on there.
[00:42:27.160 --> 00:42:29.160]   Twin flowers in the day.
[00:42:29.160 --> 00:42:30.840]   They want us over there.
[00:42:30.840 --> 00:42:32.600]   So I think that's the first thing.
[00:42:32.600 --> 00:42:38.640]   The second thing is, look, I understand the Jack and Jay and the crypto people, they've
[00:42:38.640 --> 00:42:49.080]   got this idea for a federated modular idea to harassment and trust and safety.
[00:42:49.080 --> 00:42:52.920]   If they're going to stick with that, I think at the very least they need to invest in a
[00:42:52.920 --> 00:42:58.920]   very strong default option that they themselves are funding.
[00:42:58.920 --> 00:43:02.680]   If you have a problem with the way they run it, I think you should be able to.
[00:43:02.680 --> 00:43:04.920]   So like a main site, a main site.
[00:43:04.920 --> 00:43:05.920]   A main site.
[00:43:05.920 --> 00:43:06.920]   Like a main site.
[00:43:06.920 --> 00:43:07.920]   But a main site.
[00:43:07.920 --> 00:43:09.920]   Massed on that social or a main blue sky dot app.
[00:43:09.920 --> 00:43:10.920]   That kind of thing.
[00:43:10.920 --> 00:43:12.080]   With their own moderation policies.
[00:43:12.080 --> 00:43:13.920]   That's going to happen anyway.
[00:43:13.920 --> 00:43:15.440]   That's what happened to mass.
[00:43:15.440 --> 00:43:16.440]   Exactly.
[00:43:16.440 --> 00:43:17.440]   Yeah.
[00:43:17.440 --> 00:43:18.440]   So create a baseline.
[00:43:18.440 --> 00:43:23.520]   Like really go out there, add the transparency that really ended up biting Twitter in the butt.
[00:43:23.520 --> 00:43:24.520]   But really commit to that.
[00:43:24.520 --> 00:43:25.880]   But does that solve anything?
[00:43:25.880 --> 00:43:31.240]   I think if anything we've learned, I hope we've learned that I look at T2 and a bunch
[00:43:31.240 --> 00:43:36.080]   of other Twitter clones, we don't want to go with a centralized site anymore.
[00:43:36.080 --> 00:43:37.440]   A single owner centralized site.
[00:43:37.440 --> 00:43:40.640]   Or is that easier and better for trust and safety?
[00:43:40.640 --> 00:43:43.880]   Nobody has figured out a good way to do a truly distributed trust and safety.
[00:43:43.880 --> 00:43:45.920]   I mean, the best example would be email.
[00:43:45.920 --> 00:43:47.800]   Email is truly federated.
[00:43:47.800 --> 00:43:53.240]   And the way that works is you rely upon whoever holds your mailbox to effectively do the trust
[00:43:53.240 --> 00:43:57.120]   and safety work spam filtering, filtering anything horrible that happens.
[00:43:57.120 --> 00:44:00.720]   That being said, Brianna knows this as much better than I do.
[00:44:00.720 --> 00:44:02.280]   If you're at all in the public.
[00:44:02.280 --> 00:44:03.800]   Oh, we get horrible.
[00:44:03.800 --> 00:44:04.800]   We all get horribly harassed.
[00:44:04.800 --> 00:44:05.800]   Right.
[00:44:05.800 --> 00:44:06.800]   You get horrible harassment on email.
[00:44:06.800 --> 00:44:07.800]   But you can ignore it.
[00:44:07.800 --> 00:44:09.280]   And you can ignore it.
[00:44:09.280 --> 00:44:11.440]   But you can make that same statement around social.
[00:44:11.440 --> 00:44:16.360]   The difference is that nasty poison pen letter that comes to me is not public.
[00:44:16.360 --> 00:44:17.360]   Right.
[00:44:17.360 --> 00:44:18.360]   Somebody does that on Twitter.
[00:44:18.360 --> 00:44:19.360]   It's public.
[00:44:19.360 --> 00:44:20.360]   All right.
[00:44:20.360 --> 00:44:21.360]   That's a good point.
[00:44:21.360 --> 00:44:22.360]   And that does bring in lots of other people.
[00:44:22.360 --> 00:44:23.360]   The plan is good.
[00:44:23.360 --> 00:44:28.400]   I think one BSky is going to have to have the main kind of BSky.app.
[00:44:28.400 --> 00:44:29.400]   They're going to have to do trust and safety.
[00:44:29.400 --> 00:44:33.120]   They cannot wash their hands and say we're distributed and it's not a responsibility.
[00:44:33.120 --> 00:44:36.640]   And second, somebody's going to have to step and do verification.
[00:44:36.640 --> 00:44:37.840]   Verification is expensive.
[00:44:37.840 --> 00:44:38.840]   Right.
[00:44:38.840 --> 00:44:43.680]   So to take photographs of people's IDs and to say this account belongs to this in real
[00:44:43.680 --> 00:44:47.280]   life person, either you have to pay a decent amount of money.
[00:44:47.280 --> 00:44:51.520]   And I think it's like something like three to four dollars per account that you're trying
[00:44:51.520 --> 00:44:54.960]   to get verified through a service or you have to build that in house or buy it.
[00:44:54.960 --> 00:45:00.680]   When I was at Facebook, we bought a company that did ML verification of identifiers.
[00:45:00.680 --> 00:45:04.040]   And that was like, I think a four hundred million dollar or something purchase in that
[00:45:04.040 --> 00:45:05.040]   range.
[00:45:05.040 --> 00:45:06.040]   Right.
[00:45:06.040 --> 00:45:07.040]   So somebody's going to have to do that.
[00:45:07.040 --> 00:45:10.960]   But I think people who do that are going to be in a good place for the next couple of
[00:45:10.960 --> 00:45:11.960]   years.
[00:45:11.960 --> 00:45:15.440]   What about the way Mastodon does verification where it's kind of the burden is on the user.
[00:45:15.440 --> 00:45:20.920]   You know, I've verified my, if you go to my profile Mastodon, you could see the green
[00:45:20.920 --> 00:45:26.280]   highlight says, well, he owns Twit, he owns Leo.fm and I'm using key oxide to verify the
[00:45:26.280 --> 00:45:27.280]   other side.
[00:45:27.280 --> 00:45:28.520]   That's one way of doing it, right?
[00:45:28.520 --> 00:45:30.840]   It's distributed as no burden on the Mastodon.
[00:45:30.840 --> 00:45:34.800]   I think like almost just like every other product feature of Mastodon, it works great
[00:45:34.800 --> 00:45:35.800]   for nerds.
[00:45:35.800 --> 00:45:36.800]   Right.
[00:45:36.800 --> 00:45:37.800]   Yeah.
[00:45:37.800 --> 00:45:40.120]   It is not a feature that I think is realistic.
[00:45:40.120 --> 00:45:41.120]   The nerdy solution.
[00:45:41.120 --> 00:45:42.880]   Talking of the keg of nerds here, but yes.
[00:45:42.880 --> 00:45:43.880]   Yeah.
[00:45:43.880 --> 00:45:44.880]   I thought it was really easy.
[00:45:44.880 --> 00:45:45.880]   Yeah.
[00:45:45.880 --> 00:45:48.760]   It was pretty easy for you, but like realistically, I mean, how many people are going to be able
[00:45:48.760 --> 00:45:50.240]   to verify here's to that.
[00:45:50.240 --> 00:45:51.240]   Here's my well known domain.
[00:45:51.240 --> 00:45:54.840]   Well, as an example, Washington Post has been trying to deal with how do I verify our
[00:45:54.840 --> 00:45:58.800]   journalists using rel.me and it's, you know, it's not obvious.
[00:45:58.800 --> 00:45:59.800]   It's complicated.
[00:45:59.800 --> 00:46:00.800]   Yeah.
[00:46:00.800 --> 00:46:01.800]   Keybase had like a really cool.
[00:46:01.800 --> 00:46:02.800]   I love keybase.
[00:46:02.800 --> 00:46:06.000]   That's why I'm on key oxide is I miss keybase.
[00:46:06.000 --> 00:46:07.000]   Yeah.
[00:46:07.000 --> 00:46:10.520]   And so it's, I think there is a product opportunity here for one of these companies to do a key
[00:46:10.520 --> 00:46:13.320]   base level strength of verification.
[00:46:13.320 --> 00:46:16.320]   Keybase is still around.
[00:46:16.320 --> 00:46:17.320]   You hired away.
[00:46:17.320 --> 00:46:18.320]   You hired away.
[00:46:18.320 --> 00:46:19.320]   These people.
[00:46:19.320 --> 00:46:20.320]   It was my fault.
[00:46:20.320 --> 00:46:22.040]   I helped zoom by keybase.
[00:46:22.040 --> 00:46:23.040]   You're right.
[00:46:23.040 --> 00:46:25.240]   The key base service is still running effectively.
[00:46:25.240 --> 00:46:27.680]   They run it as a charity.
[00:46:27.680 --> 00:46:30.840]   It would be a cool thing for zoom to do to like donate the source code.
[00:46:30.840 --> 00:46:34.240]   I think that would be in here for them to do because then somebody else, maybe Apache
[00:46:34.240 --> 00:46:37.880]   or, you know, Mozilla or somebody else could pick up that code and use it.
[00:46:37.880 --> 00:46:40.160]   These would be some of the trust.
[00:46:40.160 --> 00:46:41.160]   It's got to be some of the trust.
[00:46:41.160 --> 00:46:44.760]   I mean, the whole idea of like the certificate authorities and that has had its own problems
[00:46:44.760 --> 00:46:45.760]   as well, right?
[00:46:45.760 --> 00:46:46.760]   Right.
[00:46:46.760 --> 00:46:50.480]   The case like system is that the level of trust you have in that intermediary is limited.
[00:46:50.480 --> 00:46:51.480]   It is not infinite.
[00:46:51.480 --> 00:46:54.920]   But the certificate authorities, it is close to infinite based upon outworks right now.
[00:46:54.920 --> 00:46:55.920]   Right.
[00:46:55.920 --> 00:46:56.920]   Right.
[00:46:56.920 --> 00:46:57.920]   Right.
[00:46:57.920 --> 00:46:58.920]   Leo, can I add into Brianna's challenge here?
[00:46:58.920 --> 00:47:01.000]   By the way, keybase is at least as geeky as well.
[00:47:01.000 --> 00:47:02.000]   Me.
[00:47:02.000 --> 00:47:03.000]   I mean, it's worse.
[00:47:03.000 --> 00:47:04.000]   Oh, yeah.
[00:47:04.000 --> 00:47:05.000]   Yeah.
[00:47:05.000 --> 00:47:06.000]   I'm not saying it's a usual solution right now.
[00:47:06.000 --> 00:47:07.000]   Might as well do no.
[00:47:07.000 --> 00:47:08.000]   Sure.
[00:47:08.000 --> 00:47:11.560]   Somebody could make up something that would be easy to do that would somehow verify your
[00:47:11.560 --> 00:47:12.560]   identity.
[00:47:12.560 --> 00:47:13.560]   It's not a big document.
[00:47:13.560 --> 00:47:17.960]   It's a big document because I have to say that's a showstopper for some people as well.
[00:47:17.960 --> 00:47:21.240]   I don't want to send anybody my for the vulnerable drivers license.
[00:47:21.240 --> 00:47:22.240]   Right.
[00:47:22.240 --> 00:47:23.240]   It is.
[00:47:23.240 --> 00:47:24.240]   And that is the flip side here, right?
[00:47:24.240 --> 00:47:29.960]   Like you can have an I you can have a verification of a pseudo identity of this is the real drill,
[00:47:29.960 --> 00:47:30.960]   right?
[00:47:30.960 --> 00:47:34.520]   That doesn't have to be but with the breaking up of Twitter, a lot of that's gone away.
[00:47:34.520 --> 00:47:35.520]   That's a shame.
[00:47:35.520 --> 00:47:37.120]   That is a real loss actually.
[00:47:37.120 --> 00:47:38.120]   Yeah.
[00:47:38.120 --> 00:47:40.040]   Neil Stevenson talked about that in his book, Fall.
[00:47:40.040 --> 00:47:44.600]   He had a similar system because he realized people were going to want to have multiple
[00:47:44.600 --> 00:47:45.600]   identities.
[00:47:45.600 --> 00:47:46.600]   No one should have to have.
[00:47:46.600 --> 00:47:48.400]   I shouldn't just have to be Leo Leport.
[00:47:48.400 --> 00:47:51.720]   I could be a dev knowle as well and other characters.
[00:47:51.720 --> 00:47:53.400]   And it would be verified.
[00:47:53.400 --> 00:47:57.960]   And the root cert would be Leo Leport, but maybe it would be anonymous so that I could
[00:47:57.960 --> 00:48:00.680]   have some anonymous but it would somehow verify out there.
[00:48:00.680 --> 00:48:02.120]   And then we need something like that.
[00:48:02.120 --> 00:48:03.120]   That's sure.
[00:48:03.120 --> 00:48:07.400]   I mean, yes, you could build like some kind of incredibly complex system like that.
[00:48:07.400 --> 00:48:09.480]   It's easier if you're a science fiction author.
[00:48:09.480 --> 00:48:14.560]   This is where Blue Sky has an opportunity over Mastodon is because Blue Sky is mostly in
[00:48:14.560 --> 00:48:15.560]   house right now.
[00:48:15.560 --> 00:48:16.760]   It is not really federated.
[00:48:16.760 --> 00:48:20.200]   Does the DID help with that the distributed identity?
[00:48:20.200 --> 00:48:21.200]   Might.
[00:48:21.200 --> 00:48:22.200]   I have to look into it more.
[00:48:22.200 --> 00:48:23.200]   Yeah.
[00:48:23.200 --> 00:48:26.840]   I would ask Brianna because I think she spent a lot more time looking at this.
[00:48:26.840 --> 00:48:32.560]   The two things of the AT protocol that really distinguished it are this decentralized database,
[00:48:32.560 --> 00:48:34.080]   but it is a common form.
[00:48:34.080 --> 00:48:38.400]   Unlike Mastodon where it's some sort of message passing thing, there is a standard for how
[00:48:38.400 --> 00:48:43.320]   your data is stored and then this distributed identity, which is across all instances.
[00:48:43.320 --> 00:48:45.320]   Have you looked into AT protocol?
[00:48:45.320 --> 00:48:46.320]   Brianna?
[00:48:46.320 --> 00:48:47.320]   I have not.
[00:48:47.320 --> 00:48:48.320]   No.
[00:48:48.320 --> 00:48:49.320]   Another goal.
[00:48:49.320 --> 00:48:54.280]   One of the downsides with the way that they're doing it is you have to basically your block
[00:48:54.280 --> 00:48:55.640]   list becomes public.
[00:48:55.640 --> 00:49:00.320]   There's a whole lot of information that becomes public just by the nature of how they're doing.
[00:49:00.320 --> 00:49:02.440]   So they're committed to full transparency.
[00:49:02.440 --> 00:49:04.080]   I did want to hear from Jeff though.
[00:49:04.080 --> 00:49:06.240]   I know you've had some thoughts about that.
[00:49:06.240 --> 00:49:07.240]   Thank you.
[00:49:07.240 --> 00:49:12.720]   I like your view of just wholesale take the Twitter verifications as a starter kit.
[00:49:12.720 --> 00:49:13.880]   I think that's brilliant.
[00:49:13.880 --> 00:49:14.880]   But that's lost.
[00:49:14.880 --> 00:49:15.880]   We've lost that.
[00:49:15.880 --> 00:49:22.440]   Well, no, no, because there were services that you bought the plaque Leo that said you
[00:49:22.440 --> 00:49:23.440]   have a plaque.
[00:49:23.440 --> 00:49:24.440]   I have a plaque.
[00:49:24.440 --> 00:49:25.440]   Some verified.
[00:49:25.440 --> 00:49:26.440]   Right.
[00:49:26.440 --> 00:49:27.440]   That's .1.
[00:49:27.440 --> 00:49:35.360]   .2 is that I think we might have to enter into capitalism here.
[00:49:35.360 --> 00:49:38.400]   What I see is a WordPress like model.
[00:49:38.400 --> 00:49:45.400]   And this is where I think Jack was headed that the base unit of either activity pub or
[00:49:45.400 --> 00:49:49.400]   AT as protocols are open because of protocols.
[00:49:49.400 --> 00:49:51.360]   But then there are commercial entities on top.
[00:49:51.360 --> 00:49:56.200]   This is where it goes to what Alex was saying that I can pay someone for a moderation service.
[00:49:56.200 --> 00:49:58.600]   I can pay someone for a verification service.
[00:49:58.600 --> 00:50:04.440]   I can pay for the Disney find view of the web or whatever that I think we're going to
[00:50:04.440 --> 00:50:07.360]   get like WordPress.com a top WordPress.org.
[00:50:07.360 --> 00:50:10.600]   I think we're going to have to see investment in here because.
[00:50:10.600 --> 00:50:14.840]   Blue Sky does not have the money right now to build a trust and safety team like Twitter
[00:50:14.840 --> 00:50:15.840]   had.
[00:50:15.840 --> 00:50:17.800]   It just it's not there.
[00:50:17.800 --> 00:50:20.400]   So how do we get and neither does Mastodon.
[00:50:20.400 --> 00:50:21.600]   So how do we get the investment in there?
[00:50:21.600 --> 00:50:25.280]   I think we've got to find ways to bring in some commercial entities.
[00:50:25.280 --> 00:50:26.280]   Is that ad supported?
[00:50:26.280 --> 00:50:30.760]   We get back into that whole mess of attention based economies and all that crap.
[00:50:30.760 --> 00:50:34.680]   Is it paid for by users and then it's a matter of privilege?
[00:50:34.680 --> 00:50:36.040]   That's a problem too.
[00:50:36.040 --> 00:50:39.840]   And by the way, one of the things that came out at the Black Twitter summit, you know,
[00:50:39.840 --> 00:50:46.600]   to your point a minute ago, what a verification was that we had a couple of sex workers there
[00:50:46.600 --> 00:50:52.800]   who are the leading edge of harassment online by officialdom and the world.
[00:50:52.800 --> 00:50:55.080]   And they don't want their names verified.
[00:50:55.080 --> 00:50:58.760]   They don't want their names out there for very good reason.
[00:50:58.760 --> 00:51:04.040]   So you got to have ways, I think as Alex said, where you can verify an identity that isn't
[00:51:04.040 --> 00:51:08.280]   a an official identity, a government identity.
[00:51:08.280 --> 00:51:10.600]   And that's important as well.
[00:51:10.600 --> 00:51:14.520]   We it's ironic because we need it now more than ever.
[00:51:14.520 --> 00:51:15.520]   Yeah.
[00:51:15.520 --> 00:51:16.760]   You know, and it's also hard.
[00:51:16.760 --> 00:51:19.560]   Yeah, or the never now more than ever.
[00:51:19.560 --> 00:51:24.400]   I started a Elon if you're listening, it's an opportunity.
[00:51:24.400 --> 00:51:30.160]   Back to maybe we could get some people together to buy to buy it back, you know, get a couple
[00:51:30.160 --> 00:51:32.720]   of billionaires because they've got all that data still.
[00:51:32.720 --> 00:51:34.000]   They could just turn back.
[00:51:34.000 --> 00:51:35.000]   They haven't thrown it out.
[00:51:35.000 --> 00:51:36.000]   They haven't thrown it out.
[00:51:36.000 --> 00:51:37.000]   They could federate tomorrow.
[00:51:37.000 --> 00:51:38.000]   Right.
[00:51:38.000 --> 00:51:42.640]   They could re-verify all the people who they've actually looked at IDs and create a new color.
[00:51:42.640 --> 00:51:46.000]   You can let Twitter blue people keep their blue check mark and you go upgrade to the
[00:51:46.000 --> 00:51:48.400]   gold or whatever you want to call it.
[00:51:48.400 --> 00:51:50.440]   But Musk would have to admit that he was wrong.
[00:51:50.440 --> 00:51:53.160]   And that does not seem something that he does too often.
[00:51:53.160 --> 00:51:55.280]   Unless they go bankrupt.
[00:51:55.280 --> 00:51:59.640]   Who are because if it goes bankrupt, the bankers get it, not the shareholders.
[00:51:59.640 --> 00:52:04.240]   Who are the bankholders, the debt holders?
[00:52:04.240 --> 00:52:09.960]   Well, there's the Saudi sovereign fund, which is a significant stake.
[00:52:09.960 --> 00:52:13.000]   I don't know where they're a legion C. Sly.
[00:52:13.000 --> 00:52:15.400]   Yeah, we have to look up.
[00:52:15.400 --> 00:52:19.880]   I don't know if it's public what, you know, what would happen.
[00:52:19.880 --> 00:52:22.040]   And who who's like the food of control?
[00:52:22.040 --> 00:52:23.040]   Yeah, who would have control.
[00:52:23.040 --> 00:52:25.200]   But yeah, I mean, you could see it going.
[00:52:25.200 --> 00:52:29.720]   I mean, that's one of his ways out is he can just miss a payment or two.
[00:52:29.720 --> 00:52:34.560]   I'm not sure how many payments he has to miss before they're able to effectively foreclose,
[00:52:34.560 --> 00:52:38.280]   allow the company to go into receivership in those banks and the banks hire a professional
[00:52:38.280 --> 00:52:39.280]   manager.
[00:52:39.280 --> 00:52:40.280]   What does that do to his Tesla stocks?
[00:52:40.280 --> 00:52:42.040]   It's so much of that has intertwined now.
[00:52:42.040 --> 00:52:45.920]   Well, it's probably better for a Tesla stock in that if he wants Twitter to continue to
[00:52:45.920 --> 00:52:49.920]   exist now, he has to sell Tesla stock to take that cash and pour it into Twitter to pay
[00:52:49.920 --> 00:52:52.120]   out of banks.
[00:52:52.120 --> 00:52:57.800]   And so I expect that Tesla shareholders would be happy to see him stopping to burn his reputation
[00:52:57.800 --> 00:53:01.720]   in this way and to reduce the chance that he's going to have to do future sales.
[00:53:01.720 --> 00:53:04.760]   So Larry Allison has a billion.
[00:53:04.760 --> 00:53:09.360]   Carter holding, which is the Qatar's sovereign wealth fund.
[00:53:09.360 --> 00:53:12.360]   Of course, Prince Salaweed bin Talal.
[00:53:12.360 --> 00:53:17.600]   He has 35 million shares kept his shares, 13 billion from bank loans, including Morgan
[00:53:17.600 --> 00:53:18.600]   Stanley, be of age.
[00:53:18.600 --> 00:53:19.600]   I'm not asking about shares.
[00:53:19.600 --> 00:53:21.320]   I'm asking about debt holders.
[00:53:21.320 --> 00:53:22.320]   Debt holders.
[00:53:22.320 --> 00:53:29.320]   Morgan Stanley, be of a Mitsubishi, UFJ financial group, Mizuho, Barclays, and two French banks,
[00:53:29.320 --> 00:53:33.800]   BNP Parabas and Societe, Genoa.
[00:53:33.800 --> 00:53:35.760]   Morgan Stanley, three and a half billion.
[00:53:35.760 --> 00:53:38.360]   So Morgan Stanley, probably the largest debtor.
[00:53:38.360 --> 00:53:43.120]   So if they have, if they have sanity and they come in and they hire Alex and they hire
[00:53:43.120 --> 00:53:46.480]   some other smart people, I would say just hire this would run.
[00:53:46.480 --> 00:53:47.480]   Just hire this panel.
[00:53:47.480 --> 00:53:48.480]   You'd be set.
[00:53:48.480 --> 00:53:52.200]   And you end up with Twitter for the cost of 13 billion, not 45 billion.
[00:53:52.200 --> 00:53:53.600]   And so it's a much better deal.
[00:53:53.600 --> 00:53:59.520]   I don't think it's worth 13 now either, but more closer to 13 than 45.
[00:53:59.520 --> 00:54:00.520]   Would you?
[00:54:00.520 --> 00:54:03.840]   So okay, so the lenders have first dibs.
[00:54:03.840 --> 00:54:04.840]   So you would just write off.
[00:54:04.840 --> 00:54:09.200]   I think if going to bankruptcy, there'd be a reshuffling in that shareholders would
[00:54:09.200 --> 00:54:12.200]   probably lose the share of their equity.
[00:54:12.200 --> 00:54:14.120]   Jack, Jack himself loses a billion bucks.
[00:54:14.120 --> 00:54:15.120]   Yeah.
[00:54:15.120 --> 00:54:16.120]   He left it there.
[00:54:16.120 --> 00:54:17.120]   Yeah.
[00:54:17.120 --> 00:54:21.680]   If banks get control, we're planning the demise of Twitter.
[00:54:21.680 --> 00:54:22.680]   Yeah.
[00:54:22.680 --> 00:54:24.640]   It seems unlikely.
[00:54:24.640 --> 00:54:26.760]   I mean, Elon's still the richest guy in the world.
[00:54:26.760 --> 00:54:31.480]   He probably can as the number of people have shown is that he can effectively run it indefinitely,
[00:54:31.480 --> 00:54:32.840]   depending on exactly what happens.
[00:54:32.840 --> 00:54:35.680]   And his cost, he just doesn't want it to be out of his pocket.
[00:54:35.680 --> 00:54:37.920]   Brianna, could you imagine trusting Twitter again?
[00:54:37.920 --> 00:54:39.360]   Could you imagine it being fixed?
[00:54:39.360 --> 00:54:40.360]   Under Elon Musk.
[00:54:40.360 --> 00:54:41.360]   No, I know that.
[00:54:41.360 --> 00:54:43.480]   That's stipulated your honor.
[00:54:43.480 --> 00:54:48.400]   So could there be a future version of Twitter that fix these mistakes?
[00:54:48.400 --> 00:54:49.400]   I suppose.
[00:54:49.400 --> 00:54:51.440]   I think that's theoretically possible.
[00:54:51.440 --> 00:54:52.440]   Yeah.
[00:54:52.440 --> 00:54:54.400]   So where would you put your bets now?
[00:54:54.400 --> 00:54:58.760]   Because I've heard you go back and forth a little bit here between Mastodon, slash
[00:54:58.760 --> 00:55:01.800]   activity pub, blue sky and Twitter.
[00:55:01.800 --> 00:55:02.800]   Sure.
[00:55:02.800 --> 00:55:04.920]   Who has the best shot the next two years?
[00:55:04.920 --> 00:55:06.560]   I just want to be honest.
[00:55:06.560 --> 00:55:09.760]   And I know, look, I enjoy Mastodon a lot.
[00:55:09.760 --> 00:55:13.920]   I Leo, if you get me an invite, I'm happy to change my instance over to Twitter.
[00:55:13.920 --> 00:55:14.920]   I would love to do that.
[00:55:14.920 --> 00:55:16.120]   I've really enjoyed it.
[00:55:16.120 --> 00:55:20.240]   I don't think there's a future where normal people sign up for Mastodon.
[00:55:20.240 --> 00:55:23.120]   I just don't think that's going to happen.
[00:55:23.120 --> 00:55:30.640]   Would it help if I showed you this plaque that said that I am who I say I am because
[00:55:30.640 --> 00:55:32.080]   I have a plaque to prove it.
[00:55:32.080 --> 00:55:36.280]   This screenshot this and make that your that should be your favorite.
[00:55:36.280 --> 00:55:37.280]   Every platform.
[00:55:37.280 --> 00:55:39.080]   This is now you're verified.
[00:55:39.080 --> 00:55:40.960]   How much did the plaque cost you Leo?
[00:55:40.960 --> 00:55:45.640]   It was like 40 bucks, but it was worth every penny of it.
[00:55:45.640 --> 00:55:49.520]   It says in honor of Leo LaPorte who had a verified Twitter account before they were
[00:55:49.520 --> 00:55:52.800]   available for purchase of every 2022.
[00:55:52.800 --> 00:55:55.360]   I love that.
[00:55:55.360 --> 00:55:58.440]   I just took a video of my thing.
[00:55:58.440 --> 00:55:59.440]   That's a whole thing.
[00:55:59.440 --> 00:56:06.480]   So I think Mastodon is going to continue to be have an outsized impact and geek culture.
[00:56:06.480 --> 00:56:09.720]   I think that you're going to run up against discord.
[00:56:09.720 --> 00:56:14.800]   Just being honest, I talk to more people on discord on a daily basis than I do on Mastodon.
[00:56:14.800 --> 00:56:16.720]   Actually, I do too.
[00:56:16.720 --> 00:56:20.000]   We have our own discord server as well, but I do too.
[00:56:20.000 --> 00:56:22.640]   So I don't think Mastodon is going to win.
[00:56:22.640 --> 00:56:25.080]   I do think it's going to get percentage of it.
[00:56:25.080 --> 00:56:27.800]   I think blue sky has a lot of things up against it.
[00:56:27.800 --> 00:56:32.920]   I think it's got a better shot than most of these social media networks.
[00:56:32.920 --> 00:56:40.160]   I think the reality is Twitter is going to keep limping along in this broken state until
[00:56:40.160 --> 00:56:42.960]   people settle on going somewhere else.
[00:56:42.960 --> 00:56:45.280]   I just don't think this is tenable.
[00:56:45.280 --> 00:56:50.120]   I'm not the only person on this panel that finds it a almost useless place to spend time
[00:56:50.120 --> 00:56:51.120]   in this forum.
[00:56:51.120 --> 00:56:52.120]   Yeah.
[00:56:52.120 --> 00:56:53.360]   Well, it's going to continue to exist.
[00:56:53.360 --> 00:56:57.720]   It is going to become the most important 8chan copy in the 2024.
[00:56:57.720 --> 00:56:58.720]   Exactly.
[00:56:58.720 --> 00:57:01.440]   It is becoming a troll site.
[00:57:01.440 --> 00:57:06.200]   And what you'll see is because the blue check marks inevitably you have this cycle of you
[00:57:06.200 --> 00:57:12.920]   pay for the 8 bucks, you get much better algorithmic reach, you get a lot more push that's only
[00:57:12.920 --> 00:57:13.920]   coming from run sides.
[00:57:13.920 --> 00:57:17.480]   But at least in the American context, it is going to become more and more specifically
[00:57:17.480 --> 00:57:19.720]   political and radical.
[00:57:19.720 --> 00:57:23.200]   And the remaining trust and safety people there are going to have to make a decision
[00:57:23.200 --> 00:57:24.200]   of whether or not.
[00:57:24.200 --> 00:57:28.120]   I think there's still a couple of good people who are hanging on because they believe it's
[00:57:28.120 --> 00:57:32.920]   better to try to make things not as bad as possible.
[00:57:32.920 --> 00:57:35.320]   And they're going to have to decide whether or not they want to be part of that because
[00:57:35.320 --> 00:57:38.120]   it is going towards a Chan territory for sure.
[00:57:38.120 --> 00:57:39.320]   Do you know people?
[00:57:39.320 --> 00:57:44.560]   I've seen a few people I know who actually bought the check and I'm kind of shocked and
[00:57:44.560 --> 00:57:46.040]   sad for them.
[00:57:46.040 --> 00:57:49.800]   Do you know people who've actually bought it?
[00:57:49.800 --> 00:57:50.800]   Not I do.
[00:57:50.800 --> 00:57:51.800]   Yeah.
[00:57:51.800 --> 00:57:52.800]   My wife.
[00:57:52.800 --> 00:57:53.800]   I do.
[00:57:53.800 --> 00:57:54.800]   Well, business.
[00:57:54.800 --> 00:57:55.800]   Is this right?
[00:57:55.800 --> 00:57:56.800]   Yeah, for her it's different.
[00:57:56.800 --> 00:57:58.400]   It is like Joanna Stern.
[00:57:58.400 --> 00:57:59.400]   I love Joanna.
[00:57:59.400 --> 00:58:02.320]   I was disappointed to see that she paid for it.
[00:58:02.320 --> 00:58:06.720]   So it has become the check of a check of doom.
[00:58:06.720 --> 00:58:08.760]   It is not something you are proud of.
[00:58:08.760 --> 00:58:10.400]   I don't want to wear the cone of shame.
[00:58:10.400 --> 00:58:12.160]   Yeah, it's the cone of shame.
[00:58:12.160 --> 00:58:13.640]   He fucks muck.
[00:58:13.640 --> 00:58:14.640]   What a loss though.
[00:58:14.640 --> 00:58:15.640]   What a loss.
[00:58:15.640 --> 00:58:21.920]   I mean, we've said this before, but it's a shame and I, you know, we didn't appreciate
[00:58:21.920 --> 00:58:23.080]   it enough when we had it.
[00:58:23.080 --> 00:58:24.080]   Yeah.
[00:58:24.080 --> 00:58:25.080]   And now more than ever.
[00:58:25.080 --> 00:58:29.880]   No, it's a cesspool and now we don't want to cesspool looks like it wasn't a cesspool.
[00:58:29.880 --> 00:58:30.880]   Yeah.
[00:58:30.880 --> 00:58:34.220]   I, you know, I will always use Mastin on that until, you know, we get taken down for child
[00:58:34.220 --> 00:58:38.640]   porn, but until then you'll be in jail so you can't use that.
[00:58:38.640 --> 00:58:40.200]   I'll be in jail so I won't be able to use anything.
[00:58:40.200 --> 00:58:45.740]   But no, I will, I like Mastin on as a community, but I'm not looking for it to replace Twitter.
[00:58:45.740 --> 00:58:47.520]   And I don't think anything I've seen it.
[00:58:47.520 --> 00:58:52.560]   Well, actually, blue sky comes the closest I've seen to replacing Twitter at this post
[00:58:52.560 --> 00:58:56.520]   news once wanted to, but no, well, yeah.
[00:58:56.520 --> 00:59:02.400]   Uh, if let's say everybody, see, it's not going to happen though, because of these trust
[00:59:02.400 --> 00:59:08.040]   and safety issues, which at first didn't rear their ugly heads, but increasingly will
[00:59:08.040 --> 00:59:10.400]   over time on blue skies only going to get worse.
[00:59:10.400 --> 00:59:12.520]   The interesting question is it can blue side.
[00:59:12.520 --> 00:59:13.720]   They've got money, right?
[00:59:13.720 --> 00:59:14.720]   They have hackers.
[00:59:14.720 --> 00:59:18.280]   They could definitely raise more with the amount of, are they going to be able to hire
[00:59:18.280 --> 00:59:20.360]   people quickly enough to deal with that?
[00:59:20.360 --> 00:59:21.360]   And it's possible.
[00:59:21.360 --> 00:59:22.360]   I mean, it's not impossible.
[00:59:22.360 --> 00:59:26.560]   I just haven't seen anything out of them that has indicated they understand how much of a
[00:59:26.560 --> 00:59:28.080]   how urgent it is for them to.
[00:59:28.080 --> 00:59:29.080]   Well, they will soon.
[00:59:29.080 --> 00:59:30.080]   Yeah.
[00:59:30.080 --> 00:59:31.080]   Right.
[00:59:31.080 --> 00:59:32.080]   Because it'll happen.
[00:59:32.080 --> 00:59:35.720]   And then they'll say, Oh my God, um, I don't, let's hope it's not too late.
[00:59:35.720 --> 00:59:40.960]   Um, maybe Elon will sell them the blue check, the old blue check database.
[00:59:40.960 --> 00:59:44.680]   How much are you like this Leo outside?
[00:59:44.680 --> 00:59:45.920]   I think, isn't that public?
[00:59:45.920 --> 00:59:47.640]   It must because right.
[00:59:47.640 --> 00:59:54.360]   We saw that thing where only 28 people had signed up for the blue after the, the apocalypse,
[00:59:54.360 --> 00:59:56.640]   which means that those four hundred forty nine thousand.
[00:59:56.640 --> 00:59:57.640]   Yeah.
[00:59:57.640 --> 00:59:58.640]   So we have a copy.
[00:59:58.640 --> 00:59:59.640]   We know who they are.
[00:59:59.640 --> 01:00:02.600]   Like that doesn't help you at all because it doesn't mean that an account on blue skies
[01:00:02.600 --> 01:00:03.600]   controlled by the same.
[01:00:03.600 --> 01:00:04.600]   Right.
[01:00:04.600 --> 01:00:08.360]   So you could use that to build some kind of bridge where somebody has to tweet something.
[01:00:08.360 --> 01:00:11.440]   I mean, that's one of the, the, the things that has not happened that blue sky hasn't
[01:00:11.440 --> 01:00:16.920]   tried yet is you could do a key based like verification of I am so and so on blue sky.
[01:00:16.920 --> 01:00:23.120]   I expect Elon we very quickly to block any kind of immediately block that.
[01:00:23.120 --> 01:00:29.760]   That's the thing about a centralized network owned by an individual is you can't, you can't
[01:00:29.760 --> 01:00:30.760]   use it.
[01:00:30.760 --> 01:00:32.360]   It's no longer in your control.
[01:00:32.360 --> 01:00:37.120]   And even though Twitter wouldn't exist with that, all of us posting on Twitter, but it's
[01:00:37.120 --> 01:00:38.920]   no longer controlled by us.
[01:00:38.920 --> 01:00:41.600]   Never probably was, but it's certainly not now.
[01:00:41.600 --> 01:00:48.120]   So if you were going to start from scratch Alex, last question you probably been asked
[01:00:48.120 --> 01:00:51.360]   this before, what would you do if you were going to start from scratch?
[01:00:51.360 --> 01:00:58.000]   Presuming that we need somewhere, we need a some sort of public entity that is the, you
[01:00:58.000 --> 01:01:00.800]   know, you know, the speaker's corner.
[01:01:00.800 --> 01:01:05.400]   The, I mean, there's a lot of different things that this fulfills.
[01:01:05.400 --> 01:01:06.400]   What would you do?
[01:01:06.400 --> 01:01:07.960]   Would you, how would you start such a thing?
[01:01:07.960 --> 01:01:10.080]   You would start with trust and safety, I guess.
[01:01:10.080 --> 01:01:14.600]   No, you have to have functionality first, but I think you have to take into account the
[01:01:14.600 --> 01:01:17.960]   community people join is the product, not just the.
[01:01:17.960 --> 01:01:18.960]   The people are the product.
[01:01:18.960 --> 01:01:20.480]   The content is the best.
[01:01:20.480 --> 01:01:23.240]   And honestly, I think Blue Sky is doing the right thing by making it look like Twitter
[01:01:23.240 --> 01:01:26.360]   because that eases the adoption.
[01:01:26.360 --> 01:01:27.760]   Yeah.
[01:01:27.760 --> 01:01:29.000]   People know how to use it right out.
[01:01:29.000 --> 01:01:30.480]   It looks exactly like Twitter.
[01:01:30.480 --> 01:01:31.480]   Yeah.
[01:01:31.480 --> 01:01:35.320]   It's like if I go back and forth, the only difference is that little blue bird.
[01:01:35.320 --> 01:01:36.320]   Yeah.
[01:01:36.320 --> 01:01:40.680]   It's otherwise is pretty much the same exact thing, which is fine.
[01:01:40.680 --> 01:01:44.440]   Two point of, I'm sure Blue Sky has IP lawyers, but.
[01:01:44.440 --> 01:01:45.960]   I'm a little confused, to be honest.
[01:01:45.960 --> 01:01:50.080]   When I go back and forth, I can't, I actually did lose track of which one I'm on.
[01:01:50.080 --> 01:01:57.120]   Can design be copyrighted?
[01:01:57.120 --> 01:01:58.600]   Certain things can be.
[01:01:58.600 --> 01:02:02.240]   This was the lawsuit between Microsoft and Apple over the trash can.
[01:02:02.240 --> 01:02:03.240]   I can't.
[01:02:03.240 --> 01:02:04.240]   Right.
[01:02:04.240 --> 01:02:05.960]   There's a trash can.
[01:02:05.960 --> 01:02:09.560]   Apple suits a bunch of people for making rectangular pieces of glass.
[01:02:09.560 --> 01:02:10.560]   Right.
[01:02:10.560 --> 01:02:11.560]   Right.
[01:02:11.560 --> 01:02:12.880]   So there is certainly a case law on this.
[01:02:12.880 --> 01:02:14.240]   I can't remember exactly what it was.
[01:02:14.240 --> 01:02:15.240]   I think it's patents.
[01:02:15.240 --> 01:02:16.480]   It's not copyright, but yeah.
[01:02:16.480 --> 01:02:17.480]   Yeah.
[01:02:17.480 --> 01:02:18.480]   Yeah.
[01:02:18.480 --> 01:02:19.480]   You could use it.
[01:02:19.480 --> 01:02:21.040]   I'd like Blue Sky to use serif type, a nice serif.
[01:02:21.040 --> 01:02:22.040]   Yeah.
[01:02:22.040 --> 01:02:23.040]   Make it a serif and then you're done.
[01:02:23.040 --> 01:02:26.320]   Well, that's post then, which I think it sounds like post is kind of they've had their
[01:02:26.320 --> 01:02:27.520]   time and it's passed.
[01:02:27.520 --> 01:02:28.520]   Yeah.
[01:02:28.520 --> 01:02:29.520]   I think we're done on post.
[01:02:29.520 --> 01:02:32.040]   What about Noster and OSTR?
[01:02:32.040 --> 01:02:34.480]   This is a decentralized network with a chance of working.
[01:02:34.480 --> 01:02:35.880]   Jack is very active on.
[01:02:35.880 --> 01:02:38.200]   Jack's also active on this.
[01:02:38.200 --> 01:02:39.200]   Yeah.
[01:02:39.200 --> 01:02:41.880]   So this is its own protocol though.
[01:02:41.880 --> 01:02:43.520]   This is not a teapro.
[01:02:43.520 --> 01:02:46.880]   Uh, normal people are not going to use that.
[01:02:46.880 --> 01:02:47.880]   I'm sorry.
[01:02:47.880 --> 01:02:48.880]   I could tell you right now.
[01:02:48.880 --> 01:02:49.880]   I don't even know.
[01:02:49.880 --> 01:02:51.600]   Have you done a Leo?
[01:02:51.600 --> 01:02:52.600]   No.
[01:02:52.600 --> 01:02:54.600]   You're the only person I know who could probably figure it out.
[01:02:54.600 --> 01:02:58.480]   Mike Masnick wrote a, did a write up on all three.
[01:02:58.480 --> 01:03:04.640]   Masted on, uh, Noster and, uh, Blue Sky.
[01:03:04.640 --> 01:03:08.480]   He didn't pick one, but he talked about the pros and cons on each.
[01:03:08.480 --> 01:03:13.640]   But I think Alex is really the first to bring up this kind of intractable trust and safety
[01:03:13.640 --> 01:03:14.640]   problem.
[01:03:14.640 --> 01:03:16.360]   And, and I, it's not technology.
[01:03:16.360 --> 01:03:18.800]   I think, I think Alex is so right.
[01:03:18.800 --> 01:03:21.040]   Alex, you're incredibly quotable.
[01:03:21.040 --> 01:03:24.360]   Um, you're, you're born for Twitter at all.
[01:03:24.360 --> 01:03:26.280]   Um, but, but you're right.
[01:03:26.280 --> 01:03:29.920]   It's, you're buying into your trusting moderation.
[01:03:29.920 --> 01:03:31.200]   You're trusting the community.
[01:03:31.200 --> 01:03:32.200]   It's the humanity.
[01:03:32.200 --> 01:03:35.880]   The technology, the technology is now as commodified as can be.
[01:03:35.880 --> 01:03:39.800]   Um, it's how well are you protected?
[01:03:39.800 --> 01:03:44.520]   How well can you find people who care about what kind of good conversation can you have
[01:03:44.520 --> 01:03:45.520]   there?
[01:03:45.520 --> 01:03:46.520]   That's everything.
[01:03:46.520 --> 01:03:53.780]   The technology helps that happen, but yeah, here's Masnick's article six months in thoughts
[01:03:53.780 --> 01:03:56.760]   on the current post Twitter diaspora options.
[01:03:56.760 --> 01:03:57.760]   Whoo.
[01:03:57.760 --> 01:04:01.080]   Whatever the diaspora speaking, speaking of way.
[01:04:01.080 --> 01:04:02.960]   How does Masnick write so fast?
[01:04:02.960 --> 01:04:04.560]   Hey, he's amazing, isn't he?
[01:04:04.560 --> 01:04:05.560]   It's unbelievable.
[01:04:05.560 --> 01:04:08.160]   And, but, and so fast and so accurately.
[01:04:08.160 --> 01:04:10.440]   I mean, I think he's, I always am very impressed.
[01:04:10.440 --> 01:04:14.080]   By the way, he writes off, uh, post dot news in this.
[01:04:14.080 --> 01:04:18.680]   It says, it just focused too much on news content to be actually all that useful.
[01:04:18.680 --> 01:04:25.600]   T two is nice and works and looks like Twitter, but it's just another centralized clone.
[01:04:25.600 --> 01:04:32.320]   Uh, you know, I would have loved to do is look at activity pub versus AT protocol and
[01:04:32.320 --> 01:04:37.320]   kind of compare the two of those protocols and like, and like a protocol, maybe.
[01:04:37.320 --> 01:04:39.120]   I'm pretty sure protocols.
[01:04:39.120 --> 01:04:40.120]   Yeah.
[01:04:40.120 --> 01:04:41.120]   Yeah.
[01:04:41.120 --> 01:04:42.120]   Right.
[01:04:42.120 --> 01:04:44.720]   The key thing is, is they have incompatible namespaces, right?
[01:04:44.720 --> 01:04:48.120]   And that's, that's a problem here is it is unfortunate.
[01:04:48.120 --> 01:04:51.280]   It would have been nice because activity pub's been around for a while.
[01:04:51.280 --> 01:04:55.480]   I feel like blue sky could have lived within the activity pub overall framework.
[01:04:55.480 --> 01:05:01.800]   And, and I agree, become like the best, uh, Fediverse host, uh, without splitting the
[01:05:01.800 --> 01:05:02.800]   namespace apart.
[01:05:02.800 --> 01:05:04.560]   Uh, that would have been, I think a smarter move.
[01:05:04.560 --> 01:05:06.280]   But that's effectively what Mozilla's doing, right?
[01:05:06.280 --> 01:05:09.960]   I think Mozilla has an interesting position here as somebody who's trusted in the tech
[01:05:09.960 --> 01:05:16.040]   space who has money, who has actual employees, you know, unlike a lot of these hosts, um,
[01:05:16.040 --> 01:05:20.680]   that they could be kind of 800 pound gorilla and build from both a moderation perspective
[01:05:20.680 --> 01:05:23.320]   and a community perspective, something that people attract people to massive on.
[01:05:23.320 --> 01:05:24.560]   Alex, I'm confused.
[01:05:24.560 --> 01:05:26.920]   I, I, you could confuse me there for a second.
[01:05:26.920 --> 01:05:28.400]   How is it a different namespace?
[01:05:28.400 --> 01:05:33.960]   If I can be, if I could have a name at B Sky Dot social and at Twitter dot social.
[01:05:33.960 --> 01:05:34.960]   Yeah.
[01:05:34.960 --> 01:05:37.080]   Doesn't that qualification make it a different namespace?
[01:05:37.080 --> 01:05:38.080]   Aren't there?
[01:05:38.080 --> 01:05:39.080]   Yeah.
[01:05:39.080 --> 01:05:40.080]   I'm saying, but there's no interactivity between them, right?
[01:05:40.080 --> 01:05:42.280]   So it's, oh, no interactivity, but you could build that.
[01:05:42.280 --> 01:05:43.280]   Well, that's the nature of it, right?
[01:05:43.280 --> 01:05:44.280]   You can't share it.
[01:05:44.280 --> 01:05:45.280]   It is it.
[01:05:45.280 --> 01:05:46.280]   Mm.
[01:05:46.280 --> 01:05:48.720]   It's not, you'd have to build a mapping.
[01:05:48.720 --> 01:05:49.720]   Right.
[01:05:49.720 --> 01:05:55.160]   So like the AT protocol has this whole cryptographic identity that's not recognized.
[01:05:55.160 --> 01:05:56.160]   I see.
[01:05:56.160 --> 01:05:59.640]   So there's no mechanism that says like you're showing the Alex at cyber villains.
[01:05:59.640 --> 01:06:01.240]   That means nothing on blue sky.
[01:06:01.240 --> 01:06:05.680]   If somebody else registered that, there's no verification other than ownership of domain.
[01:06:05.680 --> 01:06:06.680]   Right.
[01:06:06.680 --> 01:06:07.680]   Got it.
[01:06:07.680 --> 01:06:12.920]   So I, I genuinely, I, I mean this respectfully, I totally understand like this point of view.
[01:06:12.920 --> 01:06:14.880]   I understand this being the priority.
[01:06:14.880 --> 01:06:18.040]   I just, I really think we're all missing the bus here.
[01:06:18.040 --> 01:06:19.360]   It's not technology.
[01:06:19.360 --> 01:06:21.480]   It's the user experience.
[01:06:21.480 --> 01:06:22.480]   It's the policies.
[01:06:22.480 --> 01:06:26.320]   Uh, Alex, you mentioned the 2024 election.
[01:06:26.320 --> 01:06:29.160]   This information is going to be huge.
[01:06:29.160 --> 01:06:32.600]   It's having policies in place to address that.
[01:06:32.600 --> 01:06:34.800]   It's figuring out bad actors.
[01:06:34.800 --> 01:06:40.200]   That is 90% of what a social media network needs to be thinking about.
[01:06:40.200 --> 01:06:42.960]   And all the rest of this stuff, like as a geek, I love it.
[01:06:42.960 --> 01:06:46.240]   I just think it's the wrong problem to be thinking about here.
[01:06:46.240 --> 01:06:49.720]   It sounds like the real, uh, nut is verification.
[01:06:49.720 --> 01:06:56.960]   Like that, but that is funny how often authentication comes up in every context, uh, on the internet.
[01:06:56.960 --> 01:06:57.960]   Yeah.
[01:06:57.960 --> 01:07:02.960]   To prove you are who you say you are is kind of the nut when it comes to finance, when
[01:07:02.960 --> 01:07:09.400]   it comes to crypto, when it comes to, uh, uh, uh, security when it comes to tweeting everything.
[01:07:09.400 --> 01:07:13.920]   Uh, we need a good, uh, we need a, you, you cover elections as well.
[01:07:13.920 --> 01:07:16.840]   That's part of what, uh, the crab stamos group does, right?
[01:07:16.840 --> 01:07:18.920]   Not an application, but Stanford, we study elections.
[01:07:18.920 --> 01:07:20.120]   You study at Stanford.
[01:07:20.120 --> 01:07:21.120]   Okay.
[01:07:21.120 --> 01:07:24.840]   That's also, uh, an issue we don't want to have to show a driver's license, uh, at a
[01:07:24.840 --> 01:07:29.600]   polling booth, but authentication, if you're going to do, for instance, internet voting,
[01:07:29.600 --> 01:07:31.800]   you'd need authentication of some kind that works.
[01:07:31.800 --> 01:07:33.400]   Well, but you run the reasons it almost.
[01:07:33.400 --> 01:07:34.400]   You can't do it.
[01:07:34.400 --> 01:07:41.920]   I, I think there's really something here and I think Jay and the blue sky team have, have
[01:07:41.920 --> 01:07:45.960]   inadvertently fallen into something that I think is important.
[01:07:45.960 --> 01:07:49.560]   And Joshua Topolsky was in fact talking to them about this.
[01:07:49.560 --> 01:07:55.600]   The fact that it's invite only seems to have resulted in something.
[01:07:55.600 --> 01:08:00.760]   There's much, much, much easier to manage with the a holes getting in there and doing
[01:08:00.760 --> 01:08:02.120]   what they do.
[01:08:02.120 --> 01:08:07.480]   And when you have the way you get into a social media network being just an email address
[01:08:07.480 --> 01:08:11.880]   that anyone can sign up for, that's really good for growth hacking and getting a lot
[01:08:11.880 --> 01:08:14.000]   of people in there quickly.
[01:08:14.000 --> 01:08:16.920]   I don't think it's really good for, for trust.
[01:08:16.920 --> 01:08:23.080]   Like when I had the blue sky team was nice enough to give me a few codes and I'm like,
[01:08:23.080 --> 01:08:25.800]   really thinking I'm like, who do I want to go to?
[01:08:25.800 --> 01:08:26.800]   Yeah.
[01:08:26.800 --> 01:08:32.680]   And I back channel it to James essay Corey, the Star Trek Picard season three team, you
[01:08:32.680 --> 01:08:34.840]   know, so you only did sci-fi, basically.
[01:08:34.840 --> 01:08:40.400]   If it's, I mean, I really thought through everyone I was giving that to because in a
[01:08:40.400 --> 01:08:45.400]   certain way, I was putting my stamp of your, your, yeah, that's exactly right.
[01:08:45.400 --> 01:08:46.400]   Yeah.
[01:08:46.400 --> 01:08:52.280]   And more to that point, Jay and the blue sky team had said, when people join and they
[01:08:52.280 --> 01:08:56.880]   are disruptive and they end up having to ban them right away, they do look at who gave
[01:08:56.880 --> 01:08:59.800]   them the code and what's the chain of trust.
[01:08:59.800 --> 01:09:02.240]   That's what the GP was all based on.
[01:09:02.240 --> 01:09:03.240]   Exactly.
[01:09:03.240 --> 01:09:09.120]   So maybe if there's a social media network where the way you form an account is you have
[01:09:09.120 --> 01:09:12.360]   to like Italy generates a friend of a friend.
[01:09:12.360 --> 01:09:13.360]   Yeah.
[01:09:13.360 --> 01:09:14.360]   Right.
[01:09:14.360 --> 01:09:15.360]   Yeah.
[01:09:15.360 --> 01:09:16.360]   Maybe that's an interesting view.
[01:09:16.360 --> 01:09:20.960]   Maybe you could do it if you go to a Ivy League school, you get in.
[01:09:20.960 --> 01:09:22.360]   No, I like that.
[01:09:22.360 --> 01:09:23.360]   I like that.
[01:09:23.360 --> 01:09:24.360]   I would tell him this.
[01:09:24.360 --> 01:09:25.360]   I would tell him like that.
[01:09:25.360 --> 01:09:26.360]   I know.
[01:09:26.360 --> 01:09:32.360]   Well, so I think Brianna is correct about why blue skies okay right now, but nothing
[01:09:32.360 --> 01:09:34.800]   that requires an invite code is really federated.
[01:09:34.800 --> 01:09:35.800]   Right.
[01:09:35.800 --> 01:09:41.400]   Like this is going to be if blue sky really is wants to live in a federated world, then
[01:09:41.400 --> 01:09:44.320]   they're not going to be able to gatekeep for too much longer.
[01:09:44.320 --> 01:09:45.320]   So challenging.
[01:09:45.320 --> 01:09:49.320]   Is this is why app.net failed by the way, I love app.net.
[01:09:49.320 --> 01:09:52.920]   That was the social media network that launched my career.
[01:09:52.920 --> 01:09:58.400]   It failed because Dalton had this crazy vision about developers all developing their own
[01:09:58.400 --> 01:10:01.720]   client and they wouldn't have to invest in trusted safety.
[01:10:01.720 --> 01:10:03.680]   Eventually the whole thing just exploded.
[01:10:03.680 --> 01:10:06.640]   You had the coolest people there.
[01:10:06.640 --> 01:10:11.520]   Everyone there has gone on to work for Google or have a massive media career or do awesome
[01:10:11.520 --> 01:10:12.520]   stuff.
[01:10:12.520 --> 01:10:16.720]   Everyone on app.net ended up being important, but it failed because they had this vision
[01:10:16.720 --> 01:10:21.480]   that wasn't important and was going the wrong way and it all blew up.
[01:10:21.480 --> 01:10:24.440]   And I really feel that's going to happen again with blue skies.
[01:10:24.440 --> 01:10:27.440]   Say in a way it lives on through a mastodon, right?
[01:10:27.440 --> 01:10:28.760]   Yeah, to a degree.
[01:10:28.760 --> 01:10:33.640]   You know, I wasn't directly responsible for activity pub, but I know identical.
[01:10:33.640 --> 01:10:39.720]   I mean, there was a whole chain, GNU social activity pub evolved out of a bunch of, I'm
[01:10:39.720 --> 01:10:42.280]   sure app.net was kind of informed it a little bit.
[01:10:42.280 --> 01:10:43.960]   I'm a little distracted by this.
[01:10:43.960 --> 01:10:48.120]   Brianna, is your left bicep way larger than your right because you've been holding that
[01:10:48.120 --> 01:10:49.120]   mug?
[01:10:49.120 --> 01:10:50.120]   No.
[01:10:50.120 --> 01:10:51.600]   It's resting on my chair.
[01:10:51.600 --> 01:10:52.600]   It's a titanium mug.
[01:10:52.600 --> 01:10:53.600]   It's very light.
[01:10:53.600 --> 01:10:55.800]   It's just part of a work out routine.
[01:10:55.800 --> 01:10:57.720]   It's me flexing.
[01:10:57.720 --> 01:10:58.720]   It's just right here.
[01:10:58.720 --> 01:10:59.720]   I don't think I can.
[01:10:59.720 --> 01:11:00.720]   I'll go with my chair.
[01:11:00.720 --> 01:11:01.720]   I just want to.
[01:11:01.720 --> 01:11:02.720]   What's in the mug?
[01:11:02.720 --> 01:11:03.720]   I don't know.
[01:11:03.720 --> 01:11:05.760]   It is throat coat tea so I can talk.
[01:11:05.760 --> 01:11:06.760]   Nice.
[01:11:06.760 --> 01:11:07.760]   That's nice.
[01:11:07.760 --> 01:11:10.560]   I've been coughing a lot lately too with allergy season out here.
[01:11:10.560 --> 01:11:12.440]   Oh, it was here.
[01:11:12.440 --> 01:11:13.440]   It's bad.
[01:11:13.440 --> 01:11:18.720]   We have a super bloom because of all the rain and nobody can breathe basically in the entire
[01:11:18.720 --> 01:11:19.720]   state.
[01:11:19.720 --> 01:11:21.280]   I have given myself a gold check.
[01:11:21.280 --> 01:11:24.120]   I don't know if you've noticed, but in my shot now I have a gold check.
[01:11:24.120 --> 01:11:25.120]   So I am.
[01:11:25.120 --> 01:11:26.120]   Right.
[01:11:26.120 --> 01:11:27.120]   Who the hell are we?
[01:11:27.120 --> 01:11:28.360]   You have no idea.
[01:11:28.360 --> 01:11:29.360]   I have a gold check.
[01:11:29.360 --> 01:11:31.760]   I'm the guy who walked into a student in the blue.
[01:11:31.760 --> 01:11:32.760]   You got the golden pineapple.
[01:11:32.760 --> 01:11:33.760]   So that's good.
[01:11:33.760 --> 01:11:34.760]   That's close.
[01:11:34.760 --> 01:11:35.760]   Yeah.
[01:11:35.760 --> 01:11:36.760]   Is that a Wi-Fi pineapple?
[01:11:36.760 --> 01:11:37.760]   That is a Wi-Fi pineapple.
[01:11:37.760 --> 01:11:38.760]   I have that.
[01:11:38.760 --> 01:11:41.200]   My voted sticker in the Stanford Internet Observatory.
[01:11:41.200 --> 01:11:42.480]   Now let me ask you about that.
[01:11:42.480 --> 01:11:45.040]   Well, we'll take a break, but I want to ask you about Wi-Fi pineapple because I have
[01:11:45.040 --> 01:11:47.000]   very mixed feelings about this thing.
[01:11:47.000 --> 01:11:48.480]   Yeah, you should.
[01:11:48.480 --> 01:11:49.480]   Yeah.
[01:11:49.480 --> 01:11:50.480]   Okay.
[01:11:50.480 --> 01:11:51.480]   All right.
[01:11:51.480 --> 01:11:52.480]   There you go.
[01:11:52.480 --> 01:11:53.480]   And there was another device that was looked like a gaming joystick.
[01:11:53.480 --> 01:11:54.480]   Flippers.
[01:11:54.480 --> 01:11:55.480]   Yeah.
[01:11:55.480 --> 01:11:56.480]   Flippers.
[01:11:56.480 --> 01:11:57.480]   Yeah.
[01:11:57.480 --> 01:11:58.480]   Talking about a bunch of stuff I showed to my students.
[01:11:58.480 --> 01:11:59.480]   All right.
[01:11:59.480 --> 01:12:00.480]   Well, we'll talk about that.
[01:12:00.480 --> 01:12:01.480]   Yeah.
[01:12:01.480 --> 01:12:02.480]   Wow.
[01:12:02.480 --> 01:12:03.480]   Great panel.
[01:12:03.480 --> 01:12:10.400]   And you know, Alex, I'm sorry that you had to be here during what was clearly a superior
[01:12:10.400 --> 01:12:14.680]   team beating that inferior team.
[01:12:14.680 --> 01:12:19.060]   But I just say together we'll go down to the plate.
[01:12:19.060 --> 01:12:21.120]   Where's the hat?
[01:12:21.120 --> 01:12:22.700]   Oh, 1951 champions.
[01:12:22.700 --> 01:12:23.700]   Yeah, no.
[01:12:23.700 --> 01:12:24.700]   That's good.
[01:12:24.700 --> 01:12:25.700]   First night.
[01:12:25.700 --> 01:12:26.700]   That's really good.
[01:12:26.700 --> 01:12:28.960]   1951 champions.
[01:12:28.960 --> 01:12:30.400]   We still love you.
[01:12:30.400 --> 01:12:31.400]   That's awesome.
[01:12:31.400 --> 01:12:32.440]   I'm glad you brought that hat.
[01:12:32.440 --> 01:12:33.600]   That's an awesome hat.
[01:12:33.600 --> 01:12:34.600]   Wow.
[01:12:34.600 --> 01:12:37.480]   I didn't realize the Kings have been around since 1951.
[01:12:37.480 --> 01:12:38.480]   Yeah, it's Kansas City.
[01:12:38.480 --> 01:12:40.000]   So they moved back to the 1980s.
[01:12:40.000 --> 01:12:42.600]   Oh, were they Kings or what were they?
[01:12:42.600 --> 01:12:43.840]   They're the Kansas City Royals.
[01:12:43.840 --> 01:12:45.840]   Oh, and then they renamed to the Kings and then they were the--
[01:12:45.840 --> 01:12:48.200]   Because of the baseball, Kansas City Royals, which confused--
[01:12:48.200 --> 01:12:50.480]   Yeah, I wonder if it was the same ownership or something.
[01:12:50.480 --> 01:12:51.480]   Yeah.
[01:12:51.480 --> 01:12:53.800]   Yeah, I think they were the-- or yeah, I don't know if they're called the Royals, but I think
[01:12:53.800 --> 01:12:55.920]   there was a relationship with the baseball team.
[01:12:55.920 --> 01:12:56.920]   Okay.
[01:12:56.920 --> 01:12:57.920]   Nice.
[01:12:57.920 --> 01:12:59.920]   Our show-- no spoilers.
[01:12:59.920 --> 01:13:00.920]   We don't know what happened.
[01:13:00.920 --> 01:13:01.920]   Something happened.
[01:13:01.920 --> 01:13:02.920]   We don't know what happened.
[01:13:02.920 --> 01:13:06.760]   Actually, truthfully, if you cared, you were watching a game like this.
[01:13:06.760 --> 01:13:08.640]   Yes, you saw it.
[01:13:08.640 --> 01:13:10.440]   I think we could put some spoilers out.
[01:13:10.440 --> 01:13:11.440]   I don't know.
[01:13:11.440 --> 01:13:13.440]   I feel like the overlap between us and ESPN.
[01:13:13.440 --> 01:13:18.840]   And there's a Venn diagram with a very little slice in the middle there.
[01:13:18.840 --> 01:13:19.840]   It's amped.
[01:13:19.840 --> 01:13:20.840]   That's the only--
[01:13:20.840 --> 01:13:22.080]   It's an Alex, apparently.
[01:13:22.080 --> 01:13:23.080]   Yeah.
[01:13:23.080 --> 01:13:27.040]   Our show today brought to you-- you may have noticed whenever you see a wide shot, whenever
[01:13:27.040 --> 01:13:31.440]   you see our studio, by our studio sponsors, the great folks at ACI Learning.
[01:13:31.440 --> 01:13:34.080]   You may say, well, who is this ACI Learning where?
[01:13:34.080 --> 01:13:35.080]   When they're at home?
[01:13:35.080 --> 01:13:37.440]   Well, you remember the name ITPro, right?
[01:13:37.440 --> 01:13:44.920]   For many years since they started in 2013, ITPro merged with ACI Learning, and there was
[01:13:44.920 --> 01:13:46.280]   a good reason for it.
[01:13:46.280 --> 01:13:50.760]   Now they are bringing you all the benefits of ITPro plus.
[01:13:50.760 --> 01:13:54.360]   You get auditPro, you get expanded practice labs.
[01:13:54.360 --> 01:13:59.480]   You even get in-person study in their hubs so much more.
[01:13:59.480 --> 01:14:02.040]   All the benefits of ITPro plus more.
[01:14:02.040 --> 01:14:06.160]   And you know ITPro brings you engaging and entertaining IT training.
[01:14:06.160 --> 01:14:11.680]   Well, now that it's part of ACI Learning, they've expanded their production capabilities.
[01:14:11.680 --> 01:14:16.120]   They now have those studios in Gainesville are hot, man.
[01:14:16.120 --> 01:14:19.520]   They are fair, fired up, bringing you fresh content.
[01:14:19.520 --> 01:14:23.520]   You need to do that in the IT space because everything changes, new versions of software,
[01:14:23.520 --> 01:14:25.840]   the test change companies come and go.
[01:14:25.840 --> 01:14:30.000]   But ITPro and ACI Learning will always have the latest content.
[01:14:30.000 --> 01:14:34.200]   So you at any stage of your development, whether you're just getting into IT, whether
[01:14:34.200 --> 01:14:39.760]   you've got a team that needs to keep up on cybersecurity, you can get what you need if
[01:14:39.760 --> 01:14:44.320]   you're an ITPro and you say, I need to know more about a subject.
[01:14:44.320 --> 01:14:47.080]   ACI Learning and ITPro have you covered.
[01:14:47.080 --> 01:14:51.600]   6,800 hours of content, new content added every single day.
[01:14:51.600 --> 01:14:57.360]   Your team can get team training for CompTIA certs for Microsoft, for Cisco, for Linux,
[01:14:57.360 --> 01:15:00.680]   Apple Security, Cloud and a whole lot more.
[01:15:00.680 --> 01:15:06.680]   And of course, one of the main things companies want their IT team to dig deep on to get better
[01:15:06.680 --> 01:15:07.680]   at is cybersecurity.
[01:15:07.680 --> 01:15:10.280]   It's really important these days.
[01:15:10.280 --> 01:15:14.600]   CompTIA courses from ITPro and ACI Learning make it easy to level up those employees in
[01:15:14.600 --> 01:15:16.320]   cybersecurity.
[01:15:16.320 --> 01:15:18.680]   Those certs are more than just proving you have a skill set.
[01:15:18.680 --> 01:15:23.560]   It lets your customers see you're committed to keeping your organization up to date.
[01:15:23.560 --> 01:15:25.840]   And ACI Learning is with you every step of the way.
[01:15:25.840 --> 01:15:31.400]   You can fully customize the training for your team, their team, interface their platform,
[01:15:31.400 --> 01:15:34.440]   lets you track results of individuals and teams.
[01:15:34.440 --> 01:15:37.120]   You can manage your seats, assign and unassign team members.
[01:15:37.120 --> 01:15:39.280]   You can access monthly usage reports.
[01:15:39.280 --> 01:15:42.840]   You can get great visual reports, which makes it easy to show the higher ups that you're
[01:15:42.840 --> 01:15:44.320]   getting the value.
[01:15:44.320 --> 01:15:47.680]   It means your team will appreciate you offering this to them.
[01:15:47.680 --> 01:15:51.960]   You will use it and will get better and learn because of it.
[01:15:51.960 --> 01:15:55.200]   You get all the reporting you need so you can justify it to the higher ups.
[01:15:55.200 --> 01:15:57.200]   They get all the training they need.
[01:15:57.200 --> 01:15:58.440]   It's a win all around.
[01:15:58.440 --> 01:16:02.600]   For teams from 2 to 1,000 people volume discount started 5 seats.
[01:16:02.600 --> 01:16:05.200]   You can even get a free 2 week trial for training for your team.
[01:16:05.200 --> 01:16:09.320]   Plus, they're always doing events, whether you're an individual or a team.
[01:16:09.320 --> 01:16:12.680]   They're always doing events to help you learn more to get better.
[01:16:12.680 --> 01:16:17.600]   Coming up on May, May 18th, about 3 weeks from now Thursday.
[01:16:17.600 --> 01:16:22.160]   2pm Eastern, the All Things Cybersecurity webinar.
[01:16:22.160 --> 01:16:24.360]   The special guest is Naomi Buckwalter.
[01:16:24.360 --> 01:16:27.880]   She's Director of Product Security for Contrast Security.
[01:16:27.880 --> 01:16:32.520]   And founder and Executive Director of the Cybersecurity Gatebreakers Foundation.
[01:16:32.520 --> 01:16:35.720]   She'll talk about what it takes to be a security architect.
[01:16:35.720 --> 01:16:40.400]   She's got tips for advancing your cybersecurity career, how to bridge the knowledge gap in
[01:16:40.400 --> 01:16:41.400]   cybersecurity.
[01:16:41.400 --> 01:16:46.720]   If you go live Thursday, May, 10th, 2pm, you'll be able to ask questions.
[01:16:46.720 --> 01:16:48.000]   But of course, it'll be online.
[01:16:48.000 --> 01:16:51.160]   It's free to anyone who wants to find out more.
[01:16:51.160 --> 01:16:54.160]   Visit go.acilearning.com/twit.
[01:16:54.160 --> 01:16:58.600]   From entry level training to putting people on the moon, ACI learning.
[01:16:58.600 --> 01:17:02.480]   As you've covered, maintain your company's competitive edge with ACI learning.
[01:17:02.480 --> 01:17:05.120]   Visit go.acilearning.com/twit.
[01:17:05.120 --> 01:17:09.160]   Go.gio.acilearning.com/twit.
[01:17:09.160 --> 01:17:12.240]   And if you're an individual and you want to get started with a standard or premium IT
[01:17:12.240 --> 01:17:19.040]   pro membership as an individual offer code TWIT30, we'll get you 30% off, TWIT30.
[01:17:19.040 --> 01:17:22.640]   And of course, if you've got a team, team discounts start at just five seats.
[01:17:22.640 --> 01:17:24.120]   So you're going to get a discount too.
[01:17:24.120 --> 01:17:28.640]   Go.acilearning.com/twit to take advantage of it.
[01:17:28.640 --> 01:17:33.520]   It is really a great opportunity for both you as an individual to get an IT, for an IT
[01:17:33.520 --> 01:17:36.720]   professional, to step up to level up your career.
[01:17:36.720 --> 01:17:40.560]   And of course, if you've got an IT team, you know they need to stay on top of stuff.
[01:17:40.560 --> 01:17:43.280]   This is a rapidly changing world.
[01:17:43.280 --> 01:17:45.360]   It's a scary world out there.
[01:17:45.360 --> 01:17:47.360]   Go.acilearning.com/twit.
[01:17:47.360 --> 01:17:48.920]   We thank you so much for their support.
[01:17:48.920 --> 01:17:53.840]   You use that TWIT30 offer code or go to that /twit webpage.
[01:17:53.840 --> 01:17:55.080]   You're letting them know you're sending them a signal.
[01:17:55.080 --> 01:17:56.640]   I saw it on TWIT that really helps us.
[01:17:56.640 --> 01:17:58.560]   So please do that for us.
[01:17:58.560 --> 01:17:59.560]   Would you?
[01:17:59.560 --> 01:18:04.360]   Go.acilearning.com/twit.
[01:18:04.360 --> 01:18:09.360]   Brianna Wu is here, a heartbroken Alex Stamos is here.
[01:18:09.360 --> 01:18:11.960]   I'm sorry Alex.
[01:18:11.960 --> 01:18:13.680]   I would have celebrated with you.
[01:18:13.680 --> 01:18:14.680]   I really would have.
[01:18:14.680 --> 01:18:15.680]   Was it close?
[01:18:15.680 --> 01:18:16.680]   Was it close?
[01:18:16.680 --> 01:18:18.600]   It was close until the end.
[01:18:18.600 --> 01:18:21.560]   When Steph Curry has 50 points, you know you're in trouble.
[01:18:21.560 --> 01:18:23.880]   The Kings lost slowly and then suddenly.
[01:18:23.880 --> 01:18:25.480]   Yeah, just like bankruptcy.
[01:18:25.480 --> 01:18:26.480]   Just like.
[01:18:26.480 --> 01:18:30.800]   And the fall of the Roman Empire, it's all the same.
[01:18:30.800 --> 01:18:34.520]   And of course, Jeff Jarvis is here normally on TWIT, but it's great to have all three
[01:18:34.520 --> 01:18:37.120]   of you here for that conversation.
[01:18:37.120 --> 01:18:40.280]   So tell me what the Wi-Fi pineapple is, Alex.
[01:18:40.280 --> 01:18:44.680]   Wi-Fi pineapple is a hardware device you can buy that runs its own operating system.
[01:18:44.680 --> 01:18:47.440]   So it's a little box with a bunch of antennas popping out of it.
[01:18:47.440 --> 01:18:51.760]   You can hook up to your computer and it has a nice little web interface and lets you do
[01:18:51.760 --> 01:18:57.760]   lots of really interesting and mostly illegal stuff with Wi-Fi.
[01:18:57.760 --> 01:18:59.920]   I know the guy who sells it.
[01:18:59.920 --> 01:19:01.840]   Is his hack five been?
[01:19:01.840 --> 01:19:02.840]   Yeah, good friend.
[01:19:02.840 --> 01:19:03.840]   He's a good friend.
[01:19:03.840 --> 01:19:04.840]   The hack five.
[01:19:04.840 --> 01:19:09.840]   And I have mixed feelings about this.
[01:19:09.840 --> 01:19:11.480]   They say it's for pen testing.
[01:19:11.480 --> 01:19:14.320]   And it is used for that and I have used it for that.
[01:19:14.320 --> 01:19:16.320]   I use it mostly for educational purposes.
[01:19:16.320 --> 01:19:21.160]   So when I teach Wi-Fi in my fall classes about cybersecurity, my spring class is about
[01:19:21.160 --> 01:19:22.160]   trust and safety.
[01:19:22.160 --> 01:19:26.400]   But in the fall, I teach a cyber class and I do a demo where I intercept people's connections
[01:19:26.400 --> 01:19:30.200]   and pull up one of the interesting things you can do with it is.
[01:19:30.200 --> 01:19:32.640]   You're the fun professor.
[01:19:32.640 --> 01:19:38.080]   Yes, my ratings, my reviews are really good until I get fired.
[01:19:38.080 --> 01:19:41.840]   That's the, I think that's the pleasure.
[01:19:41.840 --> 01:19:44.920]   So one of the cool things it does is, you know, some of the people don't really understand
[01:19:44.920 --> 01:19:50.560]   is that when you add a device to a Wi-Fi network and it remembers it, it will beacon
[01:19:50.560 --> 01:19:51.560]   for that.
[01:19:51.560 --> 01:19:52.560]   It will look for the beacon in the future.
[01:19:52.560 --> 01:19:56.600]   So your computer is effectively constantly saying, hey, anybody here Starbucks?
[01:19:56.600 --> 01:19:59.120]   Hey, anybody here is the name of my home network and such.
[01:19:59.120 --> 01:20:02.600]   So like one of the fun demos I do is while I'm giving the Wi-Fi lecture, I'm sniffing
[01:20:02.600 --> 01:20:06.160]   in the background, of course, all the students, there's about 200 students in that class.
[01:20:06.160 --> 01:20:07.560]   They all have laptops open.
[01:20:07.560 --> 01:20:09.720]   90% of them are probably not paying attention.
[01:20:09.720 --> 01:20:12.240]   Something again, I can figure out with the Wi-Fi pineapple.
[01:20:12.240 --> 01:20:17.960]   I see you browsing your, your, your blue space page.
[01:20:17.960 --> 01:20:19.520]   I was taking a talk today.
[01:20:19.520 --> 01:20:20.520]   Yeah, exactly.
[01:20:20.520 --> 01:20:24.280]   And when they, you know, at the end that I show, you know, whose network is this, whose
[01:20:24.280 --> 01:20:25.280]   network is this?
[01:20:25.280 --> 01:20:27.720]   And you have people raising their hand of like, that's my parents network.
[01:20:27.720 --> 01:20:29.080]   That's the hotel I just went to and such.
[01:20:29.080 --> 01:20:30.480]   So it does all kinds of interesting stuff.
[01:20:30.480 --> 01:20:33.880]   Like one of the things you use it for is to pretend to be wireless network.
[01:20:33.880 --> 01:20:39.000]   So it has a radio that you can push perhaps a little bit beyond what the FCC says is a
[01:20:39.000 --> 01:20:42.960]   acceptable level of power output in the unregulated spectrum.
[01:20:42.960 --> 01:20:47.960]   And so what you can do is if you're in a public network, you can have it broadcast in
[01:20:47.960 --> 01:20:54.520]   a higher decibel level and take over and other and people will associate to it.
[01:20:54.520 --> 01:21:00.600]   And then it will route all that traffic over, you can have it go over like a GSM card or
[01:21:00.600 --> 01:21:04.720]   LTE or over a hard wire if you have it.
[01:21:04.720 --> 01:21:06.000]   And then you can sniff all of that traffic.
[01:21:06.000 --> 01:21:07.560]   You watch it as it goes out to the internet.
[01:21:07.560 --> 01:21:09.640]   They still think they're on the internet, but they're going through you.
[01:21:09.640 --> 01:21:12.920]   All stuff you can do with like a properly configured Linux laptop and such, but like
[01:21:12.920 --> 01:21:14.160]   this just makes it all easy.
[01:21:14.160 --> 01:21:17.240]   Because it has its own CPU, you have your computer attached, you tell it what to do
[01:21:17.240 --> 01:21:19.080]   and then you can walk away and you can leave it there.
[01:21:19.080 --> 01:21:24.960]   So often we have used it for penetration tests to you a good place for it, especially if
[01:21:24.960 --> 01:21:28.720]   you have a battery pack attached to it is the restrooms in the lobby.
[01:21:28.720 --> 01:21:32.280]   So if you can use a restroom in a lobby of a building and they have a drop ceiling, you
[01:21:32.280 --> 01:21:35.360]   can go put in the drop ceiling and let it take over Wi-Fi.
[01:21:35.360 --> 01:21:37.080]   So this is my mixed feelings about this.
[01:21:37.080 --> 01:21:43.400]   And you know, I've never talked to Darren about it, but it's 120 bucks.
[01:21:43.400 --> 01:21:44.400]   Yes.
[01:21:44.400 --> 01:21:45.480]   A script kitty could use this.
[01:21:45.480 --> 01:21:49.720]   And that's my problem is if you're going to do it with a configured Linux laptop, you
[01:21:49.720 --> 01:21:51.200]   know what you're doing.
[01:21:51.200 --> 01:21:52.200]   Not necessarily, but yes.
[01:21:52.200 --> 01:21:56.920]   It is one of these interesting tools where effectively almost anything you do with its
[01:21:56.920 --> 01:21:59.080]   illegal unless you're doing it in a fair day cage, right?
[01:21:59.080 --> 01:22:01.480]   So doing most of the stuff.
[01:22:01.480 --> 01:22:05.040]   But it's legal to sell it, even though anything you could do with it would be illegal.
[01:22:05.040 --> 01:22:06.040]   Yes.
[01:22:06.040 --> 01:22:07.040]   Isn't that funny?
[01:22:07.040 --> 01:22:08.040]   Freedom, man.
[01:22:08.040 --> 01:22:09.040]   Freedom.
[01:22:09.040 --> 01:22:13.400]   So Shira Ovid at the Washington Post had a piece today saying five things you shouldn't
[01:22:13.400 --> 01:22:14.640]   worry about.
[01:22:14.640 --> 01:22:17.840]   Everyone was using Wi-Fi at a public space.
[01:22:17.840 --> 01:22:18.840]   Okay.
[01:22:18.840 --> 01:22:19.840]   That's changed a little bit.
[01:22:19.840 --> 01:22:24.200]   I mean, since the days of fire sheep where you were sending unencrypted traffic and somebody
[01:22:24.200 --> 01:22:29.080]   could impersonate you, but this thing, as Alex just explained, but let me say it.
[01:22:29.080 --> 01:22:33.960]   So Jeff, you were just at a hotel somewhere and using their Wi-Fi, you still have that
[01:22:33.960 --> 01:22:36.600]   in your list of Wi-Fi's that you've accessed.
[01:22:36.600 --> 01:22:42.320]   The pineapple can impersonate it and can be stronger than the coffee shop Wi-Fi.
[01:22:42.320 --> 01:22:48.040]   So your laptop without any talking to you will say, "Oh, hey, we're back at the hotel.
[01:22:48.040 --> 01:22:49.680]   Let's check in.
[01:22:49.680 --> 01:22:50.680]   It's a better signal."
[01:22:50.680 --> 01:22:54.480]   I mean, things are better now in that HTTPS has become pretty much ubiquitous.
[01:22:54.480 --> 01:22:55.480]   Yeah.
[01:22:55.480 --> 01:22:56.480]   Thanks to Google, HTTPS everywhere.
[01:22:56.480 --> 01:23:03.560]   Thanks to HTTPS Anywhere, like plug-in makers, thanks to Let's Encrypt, the EFF's project
[01:23:03.560 --> 01:23:04.560]   to give up.
[01:23:04.560 --> 01:23:05.560]   Which makes it easier to be SSL.
[01:23:05.560 --> 01:23:07.040]   Honestly, thanks to Ed Snowden.
[01:23:07.040 --> 01:23:09.080]   I mean, we don't want to do a whole Snowden thing.
[01:23:09.080 --> 01:23:10.240]   I have mixed feelings here.
[01:23:10.240 --> 01:23:14.160]   But there was a massive move to Encrypt and I saw a bunch of that.
[01:23:14.160 --> 01:23:15.520]   I was the C so at Yahoo.
[01:23:15.520 --> 01:23:19.360]   Yahoo would not have done all the work necessary, which was very expensive and very difficult
[01:23:19.360 --> 01:23:23.440]   to encrypt all of Yahoo's sites to HTTPS all the time if it wasn't for the Snowden disclosure.
[01:23:23.440 --> 01:23:24.440]   Oh, really interesting.
[01:23:24.440 --> 01:23:25.440]   Yeah.
[01:23:25.440 --> 01:23:30.240]   It turned out it was very difficult for anybody who wasn't Google and therefore in a fully
[01:23:30.240 --> 01:23:33.200]   contained ecosystem because there's a huge ecosystem problem.
[01:23:33.200 --> 01:23:37.880]   Every bit of JavaScript you pulled in, every analytics platform you used, all that stuff
[01:23:37.880 --> 01:23:38.880]   had to go to HTTPS.
[01:23:38.880 --> 01:23:41.600]   It was all distributed on a bunch of servers.
[01:23:41.600 --> 01:23:43.600]   You may not even have owned all the servers.
[01:23:43.600 --> 01:23:44.600]   Right.
[01:23:44.600 --> 01:23:45.600]   Right.
[01:23:45.600 --> 01:23:48.360]   And so it took years to get there and it basically happened because of the Snowden disclosure
[01:23:48.360 --> 01:23:50.360]   because it turned out that it had to take a little bit more time.
[01:23:50.360 --> 01:23:51.360]   I thought it was fire sheep.
[01:23:51.360 --> 01:23:55.240]   Fire sheep seemed like the tipping point when any idiot could go into a coffee shop and
[01:23:55.240 --> 01:23:56.240]   steal your face.
[01:23:56.240 --> 01:23:59.480]   But you could have done for a long time before that using a variety of tools.
[01:23:59.480 --> 01:24:00.480]   Oh, yeah.
[01:24:00.480 --> 01:24:03.440]   But the tools of like you had to be running Linux, you had to tweak the kernel a little
[01:24:03.440 --> 01:24:04.440]   bit.
[01:24:04.440 --> 01:24:05.440]   See, that's my point with the pineapple.
[01:24:05.440 --> 01:24:06.480]   As soon as it gets easy.
[01:24:06.480 --> 01:24:07.480]   Yeah.
[01:24:07.480 --> 01:24:11.920]   So and then there's this flipper zero, which is something more recent, the multi tool device
[01:24:11.920 --> 01:24:12.920]   for geeks.
[01:24:12.920 --> 01:24:13.920]   Yes, it's fun.
[01:24:13.920 --> 01:24:16.400]   Do you own it sounds like you might own one for you.
[01:24:16.400 --> 01:24:21.240]   Well, I don't know one, but I've certainly looked at the coverage of this.
[01:24:21.240 --> 01:24:27.920]   Before my comments, I really want to stress that we have criminalized white hat hacking
[01:24:27.920 --> 01:24:28.920]   like in the States.
[01:24:28.920 --> 01:24:29.920]   That's true.
[01:24:29.920 --> 01:24:30.920]   But it always has been.
[01:24:30.920 --> 01:24:37.920]   My friend, Russell Schwartz was working at Intel, did a little freelance pen testing at Intel
[01:24:37.920 --> 01:24:39.760]   and got thrown in jail for it.
[01:24:39.760 --> 01:24:45.080]   100% are you remember when the AT&T thing happened a few years ago with the iPhone, right?
[01:24:45.080 --> 01:24:51.560]   And people basically they tried to cover up by basically getting people charged.
[01:24:51.560 --> 01:24:52.560]   Right.
[01:24:52.560 --> 01:24:56.880]   Basically pen testing and trying to report vulnerabilities.
[01:24:56.880 --> 01:24:59.000]   We've seen that leveled levied politically.
[01:24:59.000 --> 01:25:05.440]   So just my blanket comment here is look as a policy, I'm 100% white hat hacking.
[01:25:05.440 --> 01:25:07.840]   I think we need ways to indemnify people.
[01:25:07.840 --> 01:25:12.120]   They're out there doing what I consider public service that's in the interest of national
[01:25:12.120 --> 01:25:13.440]   security.
[01:25:13.440 --> 01:25:19.080]   That said, you know, if you look at the pineapple zero and some of the things you can do with
[01:25:19.080 --> 01:25:25.760]   it, you know, you have people with no training whatsoever that can go unlock cars.
[01:25:25.760 --> 01:25:30.680]   They can change the price of gas at a gas station.
[01:25:30.680 --> 01:25:31.680]   Oh, wait a minute.
[01:25:31.680 --> 01:25:32.680]   I want to know more about that.
[01:25:32.680 --> 01:25:33.680]   That's cool.
[01:25:33.680 --> 01:25:36.000]   This was reported by something I saw.
[01:25:36.000 --> 01:25:39.880]   But the bottom line with this is like an individual gas station.
[01:25:39.880 --> 01:25:42.040]   They don't have a pen testing department.
[01:25:42.040 --> 01:25:43.040]   Yeah.
[01:25:43.040 --> 01:25:44.040]   Right.
[01:25:44.040 --> 01:25:47.700]   So what the deal is you buy a tank of gas and then tell them, Hey, by the way, I've
[01:25:47.700 --> 01:25:49.700]   just found my vulnerability.
[01:25:49.700 --> 01:25:50.700]   Oh, 100%.
[01:25:50.700 --> 01:25:51.700]   And you're bummed.
[01:25:51.700 --> 01:25:52.700]   Don't give it to you.
[01:25:52.700 --> 01:25:59.520]   No, but it's, it's, I, I get that this is a tool that can be used for good things, but
[01:25:59.520 --> 01:26:05.320]   I also think it is made in a way that like introduces these vulnerabilities to people.
[01:26:05.320 --> 01:26:07.520]   I have no real way to act on it.
[01:26:07.520 --> 01:26:11.040]   So the flipper zero was, I think it was a kick starter.
[01:26:11.040 --> 01:26:12.040]   It was somehow fun.
[01:26:12.040 --> 01:26:13.680]   That's the one I'm thinking of.
[01:26:13.680 --> 01:26:14.680]   Correct.
[01:26:14.680 --> 01:26:15.680]   Yeah.
[01:26:15.680 --> 01:26:18.400]   And it's a really kind of an IOT.
[01:26:18.400 --> 01:26:23.460]   It's kind of designed for the, not for Wi-Fi, but for the, you know, the sub one gigahertz
[01:26:23.460 --> 01:26:24.460]   radio.
[01:26:24.460 --> 01:26:25.460]   Yeah, it's effectively.
[01:26:25.460 --> 01:26:26.460]   So it's not, it doesn't do Wi-Fi.
[01:26:26.460 --> 01:26:28.620]   Like you said, sub one gigahertz.
[01:26:28.620 --> 01:26:35.700]   There's a ton of spectrum used for IOT systems, Zigbee, Laura is one.
[01:26:35.700 --> 01:26:39.660]   So there's a, there's a bunch of standards that people use for their gardening systems
[01:26:39.660 --> 01:26:40.860]   and their home alarm systems.
[01:26:40.860 --> 01:26:46.180]   Or opening the parking garage or the garage door or garage door.
[01:26:46.180 --> 01:26:47.180]   Or your doorbell.
[01:26:47.180 --> 01:26:49.920]   Or your car ID on your cards.
[01:26:49.920 --> 01:26:52.840]   This is effectively a super cheap version of the USRP, right?
[01:26:52.840 --> 01:26:55.540]   So like we've had software defined radio for a while.
[01:26:55.540 --> 01:26:57.280]   They've often been very expensive.
[01:26:57.280 --> 01:27:00.360]   What these folks did is they built a software defined radio platform.
[01:27:00.360 --> 01:27:06.000]   They limited its frequency range to make it cheaper and then put like a cool little
[01:27:06.000 --> 01:27:08.000]   GUI on it and credit community.
[01:27:08.000 --> 01:27:11.200]   So there's this community of people that you can download programs onto that a little
[01:27:11.200 --> 01:27:12.520]   SD card and pop it in.
[01:27:12.520 --> 01:27:13.880]   So I use the flipper zero.
[01:27:13.880 --> 01:27:15.360]   I demonstrate to my Stanford students.
[01:27:15.360 --> 01:27:17.060]   I copy one of their badges.
[01:27:17.060 --> 01:27:18.060]   Yeah.
[01:27:18.060 --> 01:27:20.060]   So you can go around the campus as them.
[01:27:20.060 --> 01:27:21.060]   Yeah.
[01:27:21.060 --> 01:27:26.080]   I'm like, professor, and it is like, honestly, this more points up the, the, the flaw in the,
[01:27:26.080 --> 01:27:28.620]   the badges than it does anything else.
[01:27:28.620 --> 01:27:31.220]   Well, actually, can I say something about that really quickly?
[01:27:31.220 --> 01:27:36.080]   When I ran for Congress, one of the things I really got a crash course in is the way
[01:27:36.080 --> 01:27:41.940]   that large data and things like this, um, I always say misused by police departments,
[01:27:41.940 --> 01:27:47.540]   but there's certainly asymmetric defense that can be done by police departments, right?
[01:27:47.540 --> 01:27:53.140]   Because they do have the power to go and like Google, uh, your entire, like history of your
[01:27:53.140 --> 01:27:58.020]   Android phone and find out everywhere you've been where the defense, um, your defense that
[01:27:58.020 --> 01:28:01.380]   may be prohibitively, uh, you know, expensive.
[01:28:01.380 --> 01:28:06.260]   So it's really easy to see something like this, like someone stealing your badge and
[01:28:06.260 --> 01:28:09.540]   then like make it look like you're part of a crime, right?
[01:28:09.540 --> 01:28:12.500]   And how do you like go and prove your innocence there?
[01:28:12.500 --> 01:28:15.860]   I mean, it's, it's very easy to imagine scenarios.
[01:28:15.860 --> 01:28:20.460]   And there's not a clear policy direction here forward because we don't want to criminalize
[01:28:20.460 --> 01:28:24.140]   like, you know, pen testing and looking for vulnerabilities.
[01:28:24.140 --> 01:28:29.220]   At the same time, this is something has a tremendous, um, capability for misuse.
[01:28:29.220 --> 01:28:32.460]   And I, I truly don't know where to go from here.
[01:28:32.460 --> 01:28:34.220]   It's only a hundred sixty nine bucks.
[01:28:34.220 --> 01:28:35.460]   I'm ordering one right now.
[01:28:35.460 --> 01:28:38.820]   Cause I think we got some, some fun, some fun with it.
[01:28:38.820 --> 01:28:45.700]   Um, should these be illegal?
[01:28:45.700 --> 01:28:53.380]   No, no, no, but again, it's not something, I mean, the flippers are interesting because
[01:28:53.380 --> 01:28:55.340]   it doesn't have a humongous amount of range.
[01:28:55.340 --> 01:28:58.500]   Um, you have to be, it's kind of, you have to be proximate.
[01:28:58.500 --> 01:28:59.740]   You have to be reasonably proximate.
[01:28:59.740 --> 01:29:00.740]   Yeah.
[01:29:00.740 --> 01:29:04.860]   Um, but it still certainly could be used, uh, to steal cars and such.
[01:29:04.860 --> 01:29:10.140]   So, uh, geez, Louise, well, Hyundai's particularly, those are really easy to steal.
[01:29:10.140 --> 01:29:11.140]   I hear.
[01:29:11.140 --> 01:29:12.140]   Yeah.
[01:29:12.140 --> 01:29:13.140]   I don't know.
[01:29:13.140 --> 01:29:14.140]   Yeah.
[01:29:14.140 --> 01:29:15.140]   I, I drove through the city.
[01:29:15.140 --> 01:29:17.220]   So, you know, I, I cut off a couple of catalytic converters because it's what you do.
[01:29:17.220 --> 01:29:18.220]   Yeah.
[01:29:18.220 --> 01:29:19.220]   I just pick, pick them up and take them with you.
[01:29:19.220 --> 01:29:20.940]   You never know when you might need an expert.
[01:29:20.940 --> 01:29:21.940]   Right.
[01:29:21.940 --> 01:29:22.940]   Yeah.
[01:29:22.940 --> 01:29:23.940]   Wow.
[01:29:23.940 --> 01:29:24.940]   It's an interesting time we live in.
[01:29:24.940 --> 01:29:29.660]   Um, fortunately, the good news is most people are honest.
[01:29:29.660 --> 01:29:31.260]   Yes.
[01:29:31.260 --> 01:29:32.420]   It's only that small.
[01:29:32.420 --> 01:29:36.980]   I don't, I don't know, one tenth to one percent that of the larceny in their hearts.
[01:29:36.980 --> 01:29:38.140]   Ruin it for everybody.
[01:29:38.140 --> 01:29:40.660]   And they ruin it for everybody.
[01:29:40.660 --> 01:29:42.100]   So only sell to the good people.
[01:29:42.100 --> 01:29:44.060]   Just like you only let the good people on the blue sky.
[01:29:44.060 --> 01:29:45.060]   Exactly.
[01:29:45.060 --> 01:29:47.060]   Or you don't need trust and safety.
[01:29:47.060 --> 01:29:48.060]   Yeah.
[01:29:48.060 --> 01:29:53.260]   Hey, speaking of trusted safety, online safety bill is coming to the UK.
[01:29:53.260 --> 01:29:55.980]   It's in front of a parliament right now.
[01:29:55.980 --> 01:30:00.940]   Uh, if it passes, uh, websites will have to do age checks.
[01:30:00.940 --> 01:30:01.940]   Yeah.
[01:30:01.940 --> 01:30:04.820]   Age requirements, it's got a bunch of requirements on age checks.
[01:30:04.820 --> 01:30:08.580]   It was being pushed by a coalition of companies that sell age check services.
[01:30:08.580 --> 01:30:10.620]   Uh, so yeah, not shocking.
[01:30:10.620 --> 01:30:12.020]   Uh, color be shocked.
[01:30:12.020 --> 01:30:17.140]   At what point they were saying, you'd have to go into a pub and to prove that you were
[01:30:17.140 --> 01:30:18.140]   a over 13.
[01:30:18.140 --> 01:30:19.140]   Right.
[01:30:19.140 --> 01:30:20.900]   Like an older version.
[01:30:20.900 --> 01:30:23.140]   I'm not totally sure what the plan is here.
[01:30:23.140 --> 01:30:25.100]   I think that now you can do an online one.
[01:30:25.100 --> 01:30:26.100]   Yeah.
[01:30:26.100 --> 01:30:30.260]   An older version of the UK law, which failed was you had to go into like a pub or I think,
[01:30:30.260 --> 01:30:31.900]   um, maybe the post office.
[01:30:31.900 --> 01:30:32.900]   The store, yeah.
[01:30:32.900 --> 01:30:34.260]   Any place it does ID checks already.
[01:30:34.260 --> 01:30:35.260]   Yeah.
[01:30:35.260 --> 01:30:36.260]   And then they'd give you like a number.
[01:30:36.260 --> 01:30:37.260]   Um, excuse me.
[01:30:37.260 --> 01:30:38.260]   I'd like to see porn.
[01:30:38.260 --> 01:30:40.660]   Hey, Joe, this guy was a porn.
[01:30:40.660 --> 01:30:41.660]   Oh, I see porn.
[01:30:41.660 --> 01:30:42.660]   Right.
[01:30:42.660 --> 01:30:43.660]   Right.
[01:30:43.660 --> 01:30:44.660]   All right.
[01:30:44.660 --> 01:30:45.660]   Show me your idea.
[01:30:45.660 --> 01:30:46.660]   Governor.
[01:30:46.660 --> 01:30:47.660]   Right.
[01:30:47.660 --> 01:30:48.660]   I'll take the scotch, Harper rose and porn.
[01:30:48.660 --> 01:30:49.660]   Yeah.
[01:30:49.660 --> 01:30:50.660]   Porn, porn hub, uh, log in.
[01:30:50.660 --> 01:30:51.660]   Yeah.
[01:30:51.660 --> 01:30:53.180]   So it's got a lot of scary stuff in it.
[01:30:53.180 --> 01:30:56.140]   I think a number of people have not been paying attention to Europe.
[01:30:56.140 --> 01:30:58.620]   Uh, obviously the UK is not part of the EU anymore.
[01:30:58.620 --> 01:31:00.540]   They're going their own way on child safety.
[01:31:00.540 --> 01:31:03.880]   The digital between the digital services act and the online safety bill, there's a huge
[01:31:03.880 --> 01:31:08.140]   number of requirements for American companies that are kicking in already this year and that
[01:31:08.140 --> 01:31:09.700]   there will be more if this one passes.
[01:31:09.700 --> 01:31:12.860]   Um, and some of them have some encryption too.
[01:31:12.860 --> 01:31:15.820]   Some of them reasonable and some of them are not like in the one of the real downsides
[01:31:15.820 --> 01:31:20.300]   of the UK encryption, the UK bill is it's not clear the end end encryption will work
[01:31:20.300 --> 01:31:26.140]   with, uh, the requirements, um, that there's a huge one because, uh, and by the way, as
[01:31:26.140 --> 01:31:30.300]   an example, and we're going to see more of this Wikipedia has said, if this passes,
[01:31:30.300 --> 01:31:31.900]   we're not going to do age checks.
[01:31:31.900 --> 01:31:37.900]   Uh, and the, and now of course, does that mean there's no Wikipedia in the UK or as one
[01:31:37.900 --> 01:31:44.460]   government official said, oh, don't worry because only sites posing a high risk to children
[01:31:44.460 --> 01:31:51.460]   will be need age verification, but the trans entry and see what the GOP says about that
[01:31:51.460 --> 01:31:52.460]   with schools.
[01:31:52.460 --> 01:31:53.460]   Yeah.
[01:31:53.460 --> 01:31:55.180]   There's no, there's no safety anymore on speech in that way.
[01:31:55.180 --> 01:31:56.180]   Yeah.
[01:31:56.180 --> 01:31:57.180]   Yeah.
[01:31:57.180 --> 01:32:00.420]   Let's talk about the digit because so this is not yet law online safety bill.
[01:32:00.420 --> 01:32:01.860]   We're going to keep watching it.
[01:32:01.860 --> 01:32:08.420]   Uh, it's expected it will be however, I do think this is, this is really emblematic of
[01:32:08.420 --> 01:32:13.580]   what's happened in the United States as we've really seeded our willingness to, you know,
[01:32:13.580 --> 01:32:18.500]   take a an active role in technology and how it shapes our lives.
[01:32:18.500 --> 01:32:22.700]   We've just made the collective decision to, to set it out, you know, uh, except for your
[01:32:22.700 --> 01:32:27.620]   partner, Alex Krebs, who I closely watched is excellent jobs.
[01:32:27.620 --> 01:32:28.620]   Chris, right?
[01:32:28.620 --> 01:32:29.620]   Yeah.
[01:32:29.620 --> 01:32:30.620]   Yes, Chris.
[01:32:30.620 --> 01:32:35.860]   Uh, but a largely we've sat out and kind of interest in shaping how technology is going
[01:32:35.860 --> 01:32:36.860]   to face our lives.
[01:32:36.860 --> 01:32:40.140]   And you know, I think you can look at what Europe is doing.
[01:32:40.140 --> 01:32:42.500]   I don't agree with much of GDPR.
[01:32:42.500 --> 01:32:47.100]   I don't agree with large parts of this, but the reality is if we are not going to take
[01:32:47.100 --> 01:32:51.660]   an active role in this, other people are going to do it and they're going to make decisions.
[01:32:51.660 --> 01:32:54.540]   It affects all of us that we don't necessarily agree with.
[01:32:54.540 --> 01:33:01.220]   Blue Sky is going to run head first into with their decentralized idea of how moderation
[01:33:01.220 --> 01:33:02.220]   works.
[01:33:02.220 --> 01:33:05.260]   They're going to run head first into Germany's laws on hate speech.
[01:33:05.260 --> 01:33:06.940]   I don't know how they get around them.
[01:33:06.940 --> 01:33:11.980]   So this is, it's part of a larger symptom of us just being uninterested in doing the
[01:33:11.980 --> 01:33:14.820]   job of governance very much to our consternation.
[01:33:14.820 --> 01:33:21.340]   The Digital Services Act, which is the law in the EU requires companies to do risk management,
[01:33:21.340 --> 01:33:26.220]   conduct external and independent auditing, share data with authorities and researchers
[01:33:26.220 --> 01:33:29.380]   and adopt a code of conduct by August.
[01:33:29.380 --> 01:33:34.300]   They announced this week EU industry chief, Terry Britton said on Tuesday that there are
[01:33:34.300 --> 01:33:38.740]   19 US companies that would be subject to this because it's the law.
[01:33:38.740 --> 01:33:41.100]   They're called the Vlops very large on the Vlops.
[01:33:41.100 --> 01:33:47.340]   You have to be big, including five alphabet subsidiaries, two meta platforms, two Microsoft
[01:33:47.340 --> 01:33:54.060]   businesses, Twitter, yes, Alibaba and AliExpress, yes, Google Maps, Google Play, Google Search
[01:33:54.060 --> 01:33:59.660]   Google Shopping, YouTube, Facebook, Instagram, Amazon's marketplace.
[01:33:59.660 --> 01:34:04.620]   Very importantly, Apple and Google's app stores who will vary in all likelihood by August
[01:34:04.620 --> 01:34:08.260]   have to have alternative payment methods.
[01:34:08.260 --> 01:34:12.860]   Apple has already indicated that, well, if we do it, it's only going to be for the EU.
[01:34:12.860 --> 01:34:13.860]   Right.
[01:34:13.860 --> 01:34:15.340]   That's the Digital Markets Act, which is in parallel.
[01:34:15.340 --> 01:34:16.340]   Yeah.
[01:34:16.340 --> 01:34:17.340]   That's right.
[01:34:17.340 --> 01:34:19.340]   Digital Services, Digital Markets, yeah.
[01:34:19.340 --> 01:34:20.340]   Yep.
[01:34:20.340 --> 01:34:21.340]   Yep.
[01:34:21.340 --> 01:34:23.700]   So where does encryption stand globally?
[01:34:23.700 --> 01:34:28.340]   I know in the UK and Australia, it's really on the verge of being a little bit right.
[01:34:28.340 --> 01:34:29.340]   Right.
[01:34:29.340 --> 01:34:32.140]   Especially in the UK and on the verge of end-end encryption effectively.
[01:34:32.140 --> 01:34:37.780]   So the Digital Services Act, there is a big push from civil society to get rid of proposed
[01:34:37.780 --> 01:34:40.500]   parts of the Digital Services Act that would have made end-end encryption hard.
[01:34:40.500 --> 01:34:43.020]   So we're okay in Europe for now.
[01:34:43.020 --> 01:34:47.020]   India has been in a fight with WhatsApp that is going to the Indian Supreme Court.
[01:34:47.020 --> 01:34:51.940]   This is because Modi wants to be able to control what news Indians get.
[01:34:51.940 --> 01:34:52.940]   Yes.
[01:34:52.940 --> 01:34:55.700]   And WhatsApp is by far the most important platform.
[01:34:55.700 --> 01:34:58.820]   So India is a fascinating example in that the most important social platform there is
[01:34:58.820 --> 01:35:02.300]   in the encrypted, which is not true, pretty much anywhere else.
[01:35:02.300 --> 01:35:03.300]   I mean, I guess--
[01:35:03.300 --> 01:35:04.300]   Except Brazil.
[01:35:04.300 --> 01:35:05.300]   Brazil, another-- you're right.
[01:35:05.300 --> 01:35:06.780]   Another developing countries.
[01:35:06.780 --> 01:35:09.980]   And Brazil just went back and forth on WhatsApp.
[01:35:09.980 --> 01:35:10.980]   They banned it.
[01:35:10.980 --> 01:35:11.980]   They banned it.
[01:35:11.980 --> 01:35:16.540]   So one of the things in Brazil is that individual judges have a huge amount of power.
[01:35:16.540 --> 01:35:22.060]   So you end up in a situation where you have an individual judge ban WhatsApp or order the
[01:35:22.060 --> 01:35:26.660]   head of Facebook, Brazil, who's a sales guy who has nothing to do with encryption, order
[01:35:26.660 --> 01:35:27.660]   him arrested.
[01:35:27.660 --> 01:35:28.660]   He's been arrested a couple of times.
[01:35:28.660 --> 01:35:31.660]   I think he has a bag packed in case he has to spend the weekend.
[01:35:31.660 --> 01:35:32.660]   Poor guy.
[01:35:32.660 --> 01:35:33.660]   So--
[01:35:33.660 --> 01:35:39.740]   Oh, actually Telegram was banned is not WhatsApp, but Telegram was banned in Brazil.
[01:35:39.740 --> 01:35:43.060]   And then another judge said, oh, never mind.
[01:35:43.060 --> 01:35:44.060]   Yeah.
[01:35:44.060 --> 01:35:48.140]   And this is a constant thing in Brazil is the legal system.
[01:35:48.140 --> 01:35:51.380]   It comes to eventual consistency, but there's a big back and forth.
[01:35:51.380 --> 01:35:52.380]   Yeah.
[01:35:52.380 --> 01:35:52.660]   And they were--
[01:35:52.660 --> 01:36:02.140]   Telegram was banned because the messaging company denied requests to reveal the personal
[01:36:02.140 --> 01:36:05.740]   data of users who had been sharing stream estate messages.
[01:36:05.740 --> 01:36:10.740]   So protecting the identities of their users, they got banned.
[01:36:10.740 --> 01:36:13.220]   And then another judge says, well, that's draconian.
[01:36:13.220 --> 01:36:15.540]   We don't have to ban them.
[01:36:15.540 --> 01:36:19.300]   Telegram has said we might have to leave Brazil.
[01:36:19.300 --> 01:36:20.780]   But that's the problem with all this.
[01:36:20.780 --> 01:36:22.460]   The UK one, though, like you said, is end in encryption.
[01:36:22.460 --> 01:36:25.540]   And you've had WhatsApp and the signal both say we will not comply.
[01:36:25.540 --> 01:36:28.460]   So you're going to end up-- Wikipedia is not the important one.
[01:36:28.460 --> 01:36:29.460]   WhatsApp and the signal are--
[01:36:29.460 --> 01:36:30.460]   That's huge.
[01:36:30.460 --> 01:36:32.820]   --on a solution course with the UK.
[01:36:32.820 --> 01:36:38.220]   So ultimately, if the UK insists what's happened, signals say, bye-bye.
[01:36:38.220 --> 01:36:39.220]   We don't.
[01:36:39.220 --> 01:36:41.460]   How do you say goodbye to a country?
[01:36:41.460 --> 01:36:42.460]   You could block them.
[01:36:42.460 --> 01:36:44.540]   I mean, WhatsApp is based upon phone numbers.
[01:36:44.540 --> 01:36:48.140]   You could block all the plus 4/4 numbers.
[01:36:48.140 --> 01:36:52.860]   You could go into the App Store and delist yourself.
[01:36:52.860 --> 01:36:54.740]   You could geo-block based upon IP address.
[01:36:54.740 --> 01:36:56.820]   There's a variety of options.
[01:36:56.820 --> 01:36:58.780]   You certainly can technically do it.
[01:36:58.780 --> 01:37:02.740]   It is something that Facebook has threatened before but never done.
[01:37:02.740 --> 01:37:04.820]   I think Google has threatened it but has never done.
[01:37:04.820 --> 01:37:09.020]   And this might be the change because this is not a situation.
[01:37:09.020 --> 01:37:12.380]   In other situations, the companies have threatened it because of a law they don't like.
[01:37:12.380 --> 01:37:16.260]   But it's been something often that they can follow without breaking the entire world.
[01:37:16.260 --> 01:37:19.020]   In this case, if you break encryption there, you break it everywhere.
[01:37:19.020 --> 01:37:20.620]   WhatsApp would have to rebuild their app.
[01:37:20.620 --> 01:37:22.700]   They'd have to build back doors.
[01:37:22.700 --> 01:37:24.820]   And so there's realistically no way to do it.
[01:37:24.820 --> 01:37:27.300]   And in doing so, they'd almost certainly break the law everywhere else.
[01:37:27.300 --> 01:37:30.580]   Because they've made a bunch of promises in Europe, in the United States,
[01:37:30.580 --> 01:37:33.660]   and such under which they're being held to.
[01:37:33.660 --> 01:37:40.700]   And so I don't think any good way for them to follow the UK law without breaking the law
[01:37:40.700 --> 01:37:43.620]   in a variety of other important jurisdictions.
[01:37:43.620 --> 01:37:45.740]   I agree with that.
[01:37:45.740 --> 01:37:49.820]   I also want to say that the fight is here beyond services.
[01:37:49.820 --> 01:37:52.100]   It actually comes down to hardware as well.
[01:37:52.100 --> 01:37:59.980]   I was on a BBC panel recently talking about when it comes to basically wireless communication
[01:37:59.980 --> 01:38:07.900]   protocols, when we move to a new protocol, China has some very different views on encryption
[01:38:07.900 --> 01:38:14.180]   and privacy than we do here in the United States and the government's ability to get
[01:38:14.180 --> 01:38:16.420]   in there and look at things.
[01:38:16.420 --> 01:38:21.660]   The tendency is always going to be for governments to agree with the protocol that allows intelligence
[01:38:21.660 --> 01:38:24.140]   and law enforcement to look at that.
[01:38:24.140 --> 01:38:30.060]   So when we are seeding a role with this, China ends up building a lot of these things.
[01:38:30.060 --> 01:38:33.820]   So it's going to have long-term effects on all of this.
[01:38:33.820 --> 01:38:40.260]   So it's just a very, very troubling shift in public policy, one I think could really
[01:38:40.260 --> 01:38:41.260]   get dystopian.
[01:38:41.260 --> 01:38:46.860]   Joe in our Discord chat says, "I love to dunk on meta as much as anyone, but Facebook
[01:38:46.860 --> 01:38:51.340]   enabling end-to-end encryption for WhatsApp was the biggest privacy win in history."
[01:38:51.340 --> 01:38:52.340]   Yep.
[01:38:52.340 --> 01:38:53.340]   That's true.
[01:38:53.340 --> 01:38:56.340]   I've been saying that.
[01:38:56.340 --> 01:39:00.700]   I've said that to European parliamentarians, do not like to hear that.
[01:39:00.700 --> 01:39:04.860]   But it is absolutely true.
[01:39:04.860 --> 01:39:06.620]   Basically there's a 90-day period.
[01:39:06.620 --> 01:39:11.780]   Over that 90-day period, at the time about a billion people ended up with full protection
[01:39:11.780 --> 01:39:13.700]   of all of the content that they're sending to one another.
[01:39:13.700 --> 01:39:14.700]   It's amazing.
[01:39:14.700 --> 01:39:19.460]   What a difference something simple like that can make.
[01:39:19.460 --> 01:39:20.460]   Yeah.
[01:39:20.460 --> 01:39:25.660]   Well, the battle, we've been talking about this for years now because the FBI has been
[01:39:25.660 --> 01:39:28.860]   demanding back doors and so forth.
[01:39:28.860 --> 01:39:31.860]   There doesn't seem to be any resolution.
[01:39:31.860 --> 01:39:37.900]   It seems to be just two opposing parties and with no good answer in between.
[01:39:37.900 --> 01:39:42.700]   There's no way to do this that doesn't jeopardize other people's security and privacy.
[01:39:42.700 --> 01:39:47.940]   There are options for end-to-end encrypted products to provide more safety for people.
[01:39:47.940 --> 01:39:51.780]   And unfortunately, folks in the government side never consider those.
[01:39:51.780 --> 01:39:52.780]   What are those?
[01:39:52.780 --> 01:39:54.580]   Is it a key escrow system of some kind or?
[01:39:54.580 --> 01:39:55.580]   Yeah.
[01:39:55.580 --> 01:39:59.340]   So without key escrow, which is really a type of backdoor, you can have better reporting
[01:39:59.340 --> 01:40:00.340]   systems, right?
[01:40:00.340 --> 01:40:03.260]   So that's something that WhatsApp has invested in that they still could do a lot of work
[01:40:03.260 --> 01:40:07.940]   on is helping people who are the targets of abuse to report that and to deal with it.
[01:40:07.940 --> 01:40:08.940]   But how do they report it?
[01:40:08.940 --> 01:40:10.300]   Because only they can see it.
[01:40:10.300 --> 01:40:13.100]   But they can report, they see it and then they can report it.
[01:40:13.100 --> 01:40:14.300]   The unencrypted version of it.
[01:40:14.300 --> 01:40:15.300]   Unencrypted version.
[01:40:15.300 --> 01:40:18.340]   And what you can do is what we did with Facebook Messenger when we shipped.
[01:40:18.340 --> 01:40:19.860]   Have a hash of something.
[01:40:19.860 --> 01:40:20.860]   Exactly.
[01:40:20.860 --> 01:40:21.860]   It's called a "franking code."
[01:40:21.860 --> 01:40:23.620]   So there's a mechanism so somebody can't fake it.
[01:40:23.620 --> 01:40:27.660]   So you can't create fake C-Sam or something and send, oh my God, the send perficit.
[01:40:27.660 --> 01:40:28.660]   I'm getting this.
[01:40:28.660 --> 01:40:29.660]   I'm getting this.
[01:40:29.660 --> 01:40:31.300]   You can cryptographically verify that it came from that person.
[01:40:31.300 --> 01:40:33.500]   The server can say, oh yeah, it's the same message.
[01:40:33.500 --> 01:40:37.220]   Right, because it's signed with the public key effectively of the person who sent it.
[01:40:37.220 --> 01:40:39.020]   So every message is signed and encrypted.
[01:40:39.020 --> 01:40:40.300]   Well, that's a good way to handle it.
[01:40:40.300 --> 01:40:41.300]   Yeah.
[01:40:41.300 --> 01:40:42.300]   And so you can do that.
[01:40:42.300 --> 01:40:45.860]   One of the things I'd like to see companies invest in, and I've been advocating this,
[01:40:45.860 --> 01:40:51.140]   we ran a number of events at Stanford where we brought people together to talk about this.
[01:40:51.140 --> 01:40:54.700]   And unfortunately, companies have not made more direction here.
[01:40:54.700 --> 01:40:58.100]   It's on pushing a bunch of the classifiers and other trusted safety functionality into
[01:40:58.100 --> 01:41:00.780]   the client, right, where it has to be decrypted.
[01:41:00.780 --> 01:41:05.260]   So if you are getting a bunch of death threats or say you're a woman online, like a woman
[01:41:05.260 --> 01:41:10.780]   who, if there's any way to contact her, she will get unwanted pictures of male genitalia,
[01:41:10.780 --> 01:41:11.780]   we'll say, right?
[01:41:11.780 --> 01:41:14.340]   That is a common thing that happens.
[01:41:14.340 --> 01:41:19.140]   And so you could have a classifier that if a man you don't know sends you an image that
[01:41:19.140 --> 01:41:20.940]   it's like, oh, I know what this is.
[01:41:20.940 --> 01:41:21.940]   And it blurs it.
[01:41:21.940 --> 01:41:24.620]   And it says, did you want this person to send you male genitalia?
[01:41:24.620 --> 01:41:28.980]   And you say no, and it automatically blocks the person and reports it to the platform.
[01:41:28.980 --> 01:41:29.980]   It does not have to.
[01:41:29.980 --> 01:41:30.980]   Wow, we need that button.
[01:41:30.980 --> 01:41:31.980]   Yeah.
[01:41:31.980 --> 01:41:32.980]   That's a good button.
[01:41:32.980 --> 01:41:33.980]   You start that company, please?
[01:41:33.980 --> 01:41:34.980]   Yeah.
[01:41:34.980 --> 01:41:37.720]   Well, the promise is the people who have to implement that would be Apple and what's
[01:41:37.720 --> 01:41:38.720]   it?
[01:41:38.720 --> 01:41:39.720]   Yeah, the people who do the messaging service.
[01:41:39.720 --> 01:41:41.720]   It has to be built into those products.
[01:41:41.720 --> 01:41:43.440]   Because it has to be in the app, the client side.
[01:41:43.440 --> 01:41:44.440]   And Apple did a credit.
[01:41:44.440 --> 01:41:45.440]   Yeah.
[01:41:45.440 --> 01:41:49.600]   When they announced a bunch of effectively very complicated back doors, the other thing
[01:41:49.600 --> 01:41:51.360]   they announced was doing some of that stuff.
[01:41:51.360 --> 01:41:52.800]   And that is something they've kept on doing.
[01:41:52.800 --> 01:41:54.040]   And they are implementing that now.
[01:41:54.040 --> 01:41:55.040]   They are implementing that.
[01:41:55.040 --> 01:41:56.040]   For parents, apparently.
[01:41:56.040 --> 01:41:57.040]   Yes.
[01:41:57.040 --> 01:41:58.040]   Yeah.
[01:41:58.040 --> 01:41:59.040]   And so now they've started on the package.
[01:41:59.040 --> 01:42:02.480]   I do have to push back on that a little bit because they were talking about doing it
[01:42:02.480 --> 01:42:06.400]   in a way that I mean, well, they were talking about two different things.
[01:42:06.400 --> 01:42:11.240]   There was the CSAM scanner, which they have now abandoned.
[01:42:11.240 --> 01:42:15.640]   And I think that a lot of privacy advocates, including you, Alex, said that's not going
[01:42:15.640 --> 01:42:17.140]   to work.
[01:42:17.140 --> 01:42:19.680]   But this parental thing is very different, right?
[01:42:19.680 --> 01:42:21.640]   I want to hear Brianna's objection.
[01:42:21.640 --> 01:42:25.080]   Well, I just want to make sure, Brianna, you're making the distinction between the two because
[01:42:25.080 --> 01:42:27.400]   I agree with you about the first.
[01:42:27.400 --> 01:42:30.840]   I was talking about the Apple system where they came forward and you're right, they
[01:42:30.840 --> 01:42:31.840]   did abandon it.
[01:42:31.840 --> 01:42:37.120]   But they were looking at basically scanning your photos and then basically alerting parents.
[01:42:37.120 --> 01:42:39.160]   Well, they do do that.
[01:42:39.160 --> 01:42:45.040]   If your child was looking at sexual imagery, which I can tell you is a queer kid in Mississippi,
[01:42:45.040 --> 01:42:47.400]   that could have gotten me killed as a child.
[01:42:47.400 --> 01:42:48.400]   Right.
[01:42:48.400 --> 01:42:49.400]   So Apple moderated a little bit.
[01:42:49.400 --> 01:42:51.520]   So now it says to the kid.
[01:42:51.520 --> 01:42:55.160]   So my understanding is there's no notification of parents anymore.
[01:42:55.160 --> 01:43:00.000]   What you can do is you could say this account basically can't send or receive naked photos
[01:43:00.000 --> 01:43:02.920]   in I'm search and that is a client side classifier.
[01:43:02.920 --> 01:43:07.960]   It is a new duty classifier does not either classify for C. Sam or check against hash lists.
[01:43:07.960 --> 01:43:13.040]   And it basically tells some that it tells the kid you can't send a nude, right?
[01:43:13.040 --> 01:43:16.440]   Yeah, or receive one with that.
[01:43:16.440 --> 01:43:19.640]   But then that's in messaging that doesn't mean you can't go on a website or right.
[01:43:19.640 --> 01:43:21.280]   But the same kind of idea can be extended.
[01:43:21.280 --> 01:43:25.280]   I mean, that's just one purpose can be extended to if you're an adult and somebody sends you
[01:43:25.280 --> 01:43:27.640]   a death threat that that's classified on the client side.
[01:43:27.640 --> 01:43:31.280]   You have no relationship with that person and you don't have to see that content for it
[01:43:31.280 --> 01:43:34.880]   to say to you, hey, somebody sent you something hateful.
[01:43:34.880 --> 01:43:37.040]   Would you like to report it without seeing it?
[01:43:37.040 --> 01:43:40.440]   I have to say I have a lot more experience with diet.
[01:43:40.440 --> 01:43:43.360]   I was literally swatted this week and I saw that.
[01:43:43.360 --> 01:43:44.360]   I'm sorry about that.
[01:43:44.360 --> 01:43:45.360]   Geez.
[01:43:45.360 --> 01:43:46.360]   It's fine.
[01:43:46.360 --> 01:43:47.680]   But I'm saying death threats.
[01:43:47.680 --> 01:43:54.000]   I have no faith that machine language is ever going to be really able to crack down a harassment
[01:43:54.000 --> 01:43:55.960]   because it's so context dependent.
[01:43:55.960 --> 01:43:56.960]   Yeah.
[01:43:56.960 --> 01:44:01.880]   Because there are just a million ways to talk around it or to not trip those things.
[01:44:01.880 --> 01:44:07.160]   I think every single social media network, including Facebook has tried to automate these
[01:44:07.160 --> 01:44:13.040]   processes with machine learning, some kind of context analysis and all these various
[01:44:13.040 --> 01:44:14.040]   ways.
[01:44:14.040 --> 01:44:19.920]   It saves money, but I think it really comes at the expense of accuracy in my view.
[01:44:19.920 --> 01:44:23.520]   I think you have to work on the reporting side for end to end, that you have human beings
[01:44:23.520 --> 01:44:27.400]   look at the reports and then you ban people from the end of the encrypted network.
[01:44:27.400 --> 01:44:28.400]   Yeah.
[01:44:28.400 --> 01:44:33.960]   Horasters have learned that an overt threat is actionable, but you can couch it in such
[01:44:33.960 --> 01:44:39.560]   a way that it's not actionable, but the recipient knows perfectly well what you're saying.
[01:44:39.560 --> 01:44:42.200]   Those things are hard to detect because it's nuanced.
[01:44:42.200 --> 01:44:43.480]   So there are things you can do.
[01:44:43.480 --> 01:44:48.520]   The problem, the hardest thing to stop in any of these is a conspiracy between consenting
[01:44:48.520 --> 01:44:49.520]   adults.
[01:44:49.520 --> 01:44:53.160]   If you have adults who want to do something illegal, use the end of encryption, whether
[01:44:53.160 --> 01:44:57.000]   that they're planning the terrorist attack or they're trading CSAM.
[01:44:57.000 --> 01:45:01.480]   That is the hardest thing because there's not a participant in the conversation who will
[01:45:01.480 --> 01:45:02.480]   report it, right?
[01:45:02.480 --> 01:45:04.480]   Because they're both part of the conspiracy.
[01:45:04.480 --> 01:45:06.360]   And that's what the UK wants.
[01:45:06.360 --> 01:45:10.480]   And I think that's just something that we can't solve while also providing privacy.
[01:45:10.480 --> 01:45:13.920]   And so I think we have to choose to provide people privacy and we can focus on the other
[01:45:13.920 --> 01:45:17.600]   kinds of abuse types where there's a victim who is part of the conversation and protect
[01:45:17.600 --> 01:45:19.160]   them.
[01:45:19.160 --> 01:45:20.160]   So what do you think, Brianna?
[01:45:20.160 --> 01:45:28.200]   I would like to hear your opinion of what Apple has implemented protecting kids because there
[01:45:28.200 --> 01:45:30.080]   was less pushback on that.
[01:45:30.080 --> 01:45:31.080]   Yeah.
[01:45:31.080 --> 01:45:33.280]   I'm only hearing it here, like this implementation.
[01:45:33.280 --> 01:45:36.400]   I need to read more about it from the way you've described it.
[01:45:36.400 --> 01:45:37.600]   It makes sense to me.
[01:45:37.600 --> 01:45:41.840]   But I think we all know with these kinds of policies, the details really, really, really,
[01:45:41.840 --> 01:45:43.280]   really matter.
[01:45:43.280 --> 01:45:45.160]   And it also really matters.
[01:45:45.160 --> 01:45:49.760]   You know, there's auditing or oversight or reporting stuff to government agencies.
[01:45:49.760 --> 01:45:53.360]   I mean, there's a lot of stuff here to critically think through.
[01:45:53.360 --> 01:45:55.800]   So you can read about it.
[01:45:55.800 --> 01:45:58.520]   They call it communication safety and messages.
[01:45:58.520 --> 01:46:01.840]   Apple, as usual, has a pretty good white paper on it.
[01:46:01.840 --> 01:46:06.840]   But basically they're saying messages now includes tools that warn children, warn children
[01:46:06.840 --> 01:46:11.440]   directly and provide helpful resources if they receive or attempt to send photos that
[01:46:11.440 --> 01:46:13.080]   may contain nudity.
[01:46:13.080 --> 01:46:14.080]   Right.
[01:46:14.080 --> 01:46:19.000]   It blurs the photo before it's viewed on your child's advice, provides guidance and age-appropriate
[01:46:19.000 --> 01:46:23.720]   resources to help them make a safe choice, including contacting someone they trust if
[01:46:23.720 --> 01:46:24.720]   they choose.
[01:46:24.720 --> 01:46:25.720]   Yeah.
[01:46:25.720 --> 01:46:27.720]   They doesn't block it.
[01:46:27.720 --> 01:46:29.000]   Yeah, I think it's kind of...
[01:46:29.000 --> 01:46:33.960]   Apple has, I think, been very responsive to the concerns they've heard and are trying to
[01:46:33.960 --> 01:46:36.960]   find a way that works.
[01:46:36.960 --> 01:46:39.600]   And it seems like a good system.
[01:46:39.600 --> 01:46:42.880]   You know, parents have to turn it on, too.
[01:46:42.880 --> 01:46:44.120]   It's not going to just happen.
[01:46:44.120 --> 01:46:48.640]   One of the problems is when they had their first version of all this, they kind of did
[01:46:48.640 --> 01:46:51.400]   a very appily thing, which is they did it all in house.
[01:46:51.400 --> 01:46:53.160]   They have the smartest people in the world there.
[01:46:53.160 --> 01:46:55.080]   We can figure this out from first principles.
[01:46:55.080 --> 01:46:56.640]   And because they didn't want...
[01:46:56.640 --> 01:46:58.520]   They never work with anybody on that side of Apple, right?
[01:46:58.520 --> 01:47:01.560]   Like in trust and safety and cybersecurity, it is extremely hard to work with anybody
[01:47:01.560 --> 01:47:02.560]   at Apple.
[01:47:02.560 --> 01:47:04.680]   And so they didn't talk to anybody.
[01:47:04.680 --> 01:47:08.000]   They came out, they just busted through the wall like the Kool-Aid man.
[01:47:08.000 --> 01:47:11.560]   But like, here's a C-Sam scanning tool and all this stuff.
[01:47:11.560 --> 01:47:12.840]   And it was not a great idea.
[01:47:12.840 --> 01:47:15.160]   Once again, if there's a moral here, consult experts.
[01:47:15.160 --> 01:47:16.160]   Right.
[01:47:16.160 --> 01:47:17.160]   Right.
[01:47:17.160 --> 01:47:18.160]   And so they did a whole listening tour.
[01:47:18.160 --> 01:47:19.160]   I know they talked to a lot of folks.
[01:47:19.160 --> 01:47:20.560]   They actually visited us at Stanford and we chatted with them.
[01:47:20.560 --> 01:47:25.000]   And I know they talked to a lot of child safety advocates and a variety of advocates
[01:47:25.000 --> 01:47:26.000]   for different equities.
[01:47:26.000 --> 01:47:28.600]   And I think this is the compromise they came up with.
[01:47:28.600 --> 01:47:29.600]   Here's Brianna.
[01:47:29.600 --> 01:47:31.600]   Did they report things to law enforcement?
[01:47:31.600 --> 01:47:32.600]   No.
[01:47:32.600 --> 01:47:33.600]   No.
[01:47:33.600 --> 01:47:34.600]   Not anymore.
[01:47:34.600 --> 01:47:37.240]   So like if you look at the Nickmic reports, it's actually kind of stunningly small, which
[01:47:37.240 --> 01:47:40.920]   is one of the things they're trying to deal with is that Facebook reports about 22 million
[01:47:40.920 --> 01:47:42.720]   pieces of C-Sam per year.
[01:47:42.720 --> 01:47:45.560]   And the last one I saw, I think Apple had like 200.
[01:47:45.560 --> 01:47:48.120]   Which it's hard to think about.
[01:47:48.120 --> 01:47:51.560]   There's not 200 bad anything on a Billy and user network.
[01:47:51.560 --> 01:47:52.560]   Right.
[01:47:52.560 --> 01:47:55.680]   Like the first step for something bad happening is 10,000.
[01:47:55.680 --> 01:47:57.200]   Right.
[01:47:57.200 --> 01:48:01.480]   And so what that 200 is is actually a big question for a lot of folks.
[01:48:01.480 --> 01:48:03.160]   Yeah, where those come from?
[01:48:03.160 --> 01:48:06.960]   One of the theories is it's C-Sam that's been sent via the iCloud email, right, which
[01:48:06.960 --> 01:48:08.400]   would be a silly way to do it.
[01:48:08.400 --> 01:48:11.960]   And therefore that I've heard a couple of theses of like what services does Apple have
[01:48:11.960 --> 01:48:15.120]   that are unencrypted that people could move images around.
[01:48:15.120 --> 01:48:18.680]   This just for you, Brianna, is what the screen looks like.
[01:48:18.680 --> 01:48:20.560]   This is from Apple's website.
[01:48:20.560 --> 01:48:27.120]   If I were to send or you were to send a child a sensitive photo, the photo is blurred, then
[01:48:27.120 --> 01:48:30.600]   there's a message that pops up says you're not alone and can always get help from someone
[01:48:30.600 --> 01:48:32.480]   you trusted with trained professionals.
[01:48:32.480 --> 01:48:36.640]   You can also block this person and you're given a choice of message, a grown up, ways
[01:48:36.640 --> 01:48:39.400]   to get help, block contact or cancel.
[01:48:39.400 --> 01:48:40.400]   And so if you wish.
[01:48:40.400 --> 01:48:41.400]   Or view the photo.
[01:48:41.400 --> 01:48:42.400]   Yeah.
[01:48:42.400 --> 01:48:44.880]   If you wish, you can cancel and view the photo.
[01:48:44.880 --> 01:48:48.480]   If you said, Oh, no, no, I know what that is and I want to see it.
[01:48:48.480 --> 01:48:50.560]   But I think this is a I like this.
[01:48:50.560 --> 01:48:51.800]   Yeah, that makes a long sense.
[01:48:51.800 --> 01:48:54.480]   Yeah, I think this is a good way to do it.
[01:48:54.480 --> 01:48:55.480]   And I think it solves that problem.
[01:48:55.480 --> 01:48:59.280]   It doesn't solve the C-Sam issue, but that's a very difficult issue.
[01:48:59.280 --> 01:49:05.040]   It's very difficult and it's the mass trading of C-Sam is is mostly not an end-end encrypted
[01:49:05.040 --> 01:49:08.000]   messenger issue because what's happened?
[01:49:08.000 --> 01:49:11.000]   I message are not the best ways to move huge men to C-Sam.
[01:49:11.000 --> 01:49:16.800]   The best way to do that is via encrypted like locker, the mega's, a variety of dark websites
[01:49:16.800 --> 01:49:20.200]   and, you know, tor hidden services and such.
[01:49:20.200 --> 01:49:26.960]   So it's yeah, I think, but the one of the problems is law enforcement, like they, their
[01:49:26.960 --> 01:49:29.680]   hammer is lawful access to content, right?
[01:49:29.680 --> 01:49:32.840]   Like that is all they understand is I have access to your text messages.
[01:49:32.840 --> 01:49:34.080]   I could prosecute you.
[01:49:34.080 --> 01:49:38.680]   So everything's an ale, even if there's other more subtle technical solutions in place.
[01:49:38.680 --> 01:49:39.680]   Right.
[01:49:39.680 --> 01:49:44.720]   Actually, I encourage you to read the entire Apple page because I to me it really there's
[01:49:44.720 --> 01:49:49.000]   more images and there's just so much that they've done and it is done right.
[01:49:49.000 --> 01:49:53.080]   And I've gives me and it encourages me that they're spending this energy to do it right
[01:49:53.080 --> 01:49:55.680]   that they could continue to do it right in other areas as well.
[01:49:55.680 --> 01:49:59.480]   But to go back to our original topic, I don't think this is compatible with the UK on lunch
[01:49:59.480 --> 01:50:00.480]   health safety law.
[01:50:00.480 --> 01:50:01.480]   That's a problem.
[01:50:01.480 --> 01:50:02.480]   That would be considered enough.
[01:50:02.480 --> 01:50:03.680]   Yeah, see, that's a problem.
[01:50:03.680 --> 01:50:05.920]   Let's take a little break.
[01:50:05.920 --> 01:50:06.920]   Great panel.
[01:50:06.920 --> 01:50:08.320]   Couldn't be better.
[01:50:08.320 --> 01:50:11.400]   Brianna Wu, rebellion pack.com.
[01:50:11.400 --> 01:50:13.800]   What is rebellion pack?
[01:50:13.800 --> 01:50:19.280]   We use a large array of tech tools to win elections.
[01:50:19.280 --> 01:50:21.680]   So we do micro targeting.
[01:50:21.680 --> 01:50:28.360]   We use large standard figure out people that they believe in causes, but they may need
[01:50:28.360 --> 01:50:32.000]   a little bit more of a push to get out there and actually vote.
[01:50:32.000 --> 01:50:39.120]   So we use basically all the data we can to activate them and we win elections that way.
[01:50:39.120 --> 01:50:41.520]   It's going to get busy for you in a couple of years, isn't it?
[01:50:41.520 --> 01:50:43.520]   It's going to be very, very busy.
[01:50:43.520 --> 01:50:44.520]   Wow.
[01:50:44.520 --> 01:50:45.520]   Wow.
[01:50:45.520 --> 01:50:46.520]   I was really proud.
[01:50:46.520 --> 01:50:51.840]   We helped win the Wisconsin scotter's race.
[01:50:51.840 --> 01:50:57.280]   This was the most that ever been spent in any spring court race in history.
[01:50:57.280 --> 01:50:58.640]   There's a lot on the line.
[01:50:58.640 --> 01:51:04.760]   We just airdrop volunteers out there to go out on campus and make calls.
[01:51:04.760 --> 01:51:07.240]   We were in Europe during the election.
[01:51:07.240 --> 01:51:11.680]   So it was a little hard to follow the story, but we definitely were following the story.
[01:51:11.680 --> 01:51:14.320]   Big, big election, very big election.
[01:51:14.320 --> 01:51:16.440]   We were very proud of that one.
[01:51:16.440 --> 01:51:17.440]   Yeah.
[01:51:17.440 --> 01:51:18.440]   Jeff Jarvis is also here.
[01:51:18.440 --> 01:51:22.000]   Your book, The Gutenberg Parenthesis, comes out next month.
[01:51:22.000 --> 01:51:23.000]   June.
[01:51:23.000 --> 01:51:24.000]   June.
[01:51:24.000 --> 01:51:26.880]   So I guess it's almost almost next month.
[01:51:26.880 --> 01:51:30.240]   The Gutenberg parent parenthesis.com.
[01:51:30.240 --> 01:51:32.160]   As we said, buy it from Blackwell's.
[01:51:32.160 --> 01:51:35.080]   That's one of Jeff's preferences.
[01:51:35.080 --> 01:51:37.920]   One of many.
[01:51:37.920 --> 01:51:40.080]   Just buy it.
[01:51:40.080 --> 01:51:41.080]   That's where you really come.
[01:51:41.080 --> 01:51:42.080]   Thank you.
[01:51:42.080 --> 01:51:43.800]   This week in tech is brought to you by Noom.
[01:51:43.800 --> 01:51:49.240]   So one of the other things we did while we were overseas is eat a lot of cornetos and
[01:51:49.240 --> 01:51:52.000]   croissants and panel chocolos.
[01:51:52.000 --> 01:51:53.880]   I had some kachio pepe.
[01:51:53.880 --> 01:51:56.640]   I had some pasta carbonara.
[01:51:56.640 --> 01:52:02.160]   And we got back and both Lisa and I are going over to this scale going, "Oh boy."
[01:52:02.160 --> 01:52:07.600]   I mean, when you go away to Europe for three weeks and you eat all this great food and
[01:52:07.600 --> 01:52:12.640]   you're on a cruise ship where there apparently is a meal at every, they even had a midnight
[01:52:12.640 --> 01:52:18.920]   buffet called Death by Chocolate, you realize that there are many opportunities.
[01:52:18.920 --> 01:52:20.160]   So Lisa gets on the scale.
[01:52:20.160 --> 01:52:21.360]   She said, "Oh, I lost two pounds."
[01:52:21.360 --> 01:52:22.360]   And I'm going, "Why?"
[01:52:22.360 --> 01:52:23.360]   I got it.
[01:52:23.360 --> 01:52:25.360]   And then I looked in and I said, "I've gained no weight."
[01:52:25.360 --> 01:52:27.480]   And in both cases, I think we can credit Noom.
[01:52:27.480 --> 01:52:28.480]   We can really thank Noom.
[01:52:28.480 --> 01:52:31.280]   "Briana, you did Noom and lost 100 pounds."
[01:52:31.280 --> 01:52:35.560]   Noom, now I should not promise anything like that.
[01:52:35.560 --> 01:52:39.440]   Noom, first time Noomers lose an average of 15 pounds in 16 weeks.
[01:52:39.440 --> 01:52:44.040]   And I have to say, I did not follow any particular prescriptive diet in Europe.
[01:52:44.040 --> 01:52:45.320]   I said, "I'm going to eat what I want."
[01:52:45.320 --> 01:52:49.600]   But one of the things that works about Noom is it's not a diet.
[01:52:49.600 --> 01:52:51.960]   It's a psychology-based approach.
[01:52:51.960 --> 01:52:56.320]   Believe me, I have done every fad diet known to man.
[01:52:56.320 --> 01:53:00.280]   And the problem with fad diets is when you go off them, you gain the weight right back.
[01:53:00.280 --> 01:53:02.040]   Noom's a little different.
[01:53:02.040 --> 01:53:07.240]   Noom uses psychology, not fads, to help you make intentional and sustainable choices that
[01:53:07.240 --> 01:53:10.960]   are aligned with your values and your weight loss goals.
[01:53:10.960 --> 01:53:12.040]   What does that mean?
[01:53:12.040 --> 01:53:14.400]   It means you learn about why you eat.
[01:53:14.400 --> 01:53:18.440]   You know, Lisa and I both have this fog eating thing where we eat without even being aware
[01:53:18.440 --> 01:53:19.440]   of it.
[01:53:19.440 --> 01:53:21.760]   I get home from work and I start stuffing my mouth.
[01:53:21.760 --> 01:53:24.240]   We're watching TV and make a big thing a popcorn.
[01:53:24.240 --> 01:53:26.400]   And it's not that I can't do that anymore.
[01:53:26.400 --> 01:53:29.440]   I still love my popcorn.
[01:53:29.440 --> 01:53:33.040]   But Noom helps us be aware.
[01:53:33.040 --> 01:53:38.200]   And when you're aware, suddenly it's a lot easier to make good choices, even when you're
[01:53:38.200 --> 01:53:41.440]   faced with kachoy pepep.
[01:53:41.440 --> 01:53:44.320]   Every journey is different with Noom's psychology-based approach.
[01:53:44.320 --> 01:53:48.240]   Your daily lessons will be personalized to you and to your goals.
[01:53:48.240 --> 01:53:49.360]   There are no bad foods.
[01:53:49.360 --> 01:53:51.160]   I loved that.
[01:53:51.160 --> 01:53:52.160]   There's no forbidden.
[01:53:52.160 --> 01:53:53.920]   And time off is fine.
[01:53:53.920 --> 01:54:00.600]   They use scientific principles like cognitive behavioral therapy to create sustained, long-term
[01:54:00.600 --> 01:54:03.160]   changes in your relationship with food.
[01:54:03.160 --> 01:54:04.160]   And that's what's key.
[01:54:04.160 --> 01:54:05.640]   It's not a diet.
[01:54:05.640 --> 01:54:08.200]   It's nourishing, not restrictive.
[01:54:08.200 --> 01:54:11.200]   It focuses on progress, not perfection.
[01:54:11.200 --> 01:54:14.480]   They've got all kinds of levels of support.
[01:54:14.480 --> 01:54:15.800]   You can log your food.
[01:54:15.800 --> 01:54:17.040]   You can do the weigh-ins.
[01:54:17.040 --> 01:54:18.520]   You can have an individual coach.
[01:54:18.520 --> 01:54:25.200]   You can have a group, five-minute daily check-ins, personal coaching, whatever works for you.
[01:54:25.200 --> 01:54:26.880]   The lessons are great.
[01:54:26.880 --> 01:54:29.640]   You can really learn a lot from those lessons.
[01:54:29.640 --> 01:54:34.080]   I spent about 15 minutes a day because I wanted to learn more.
[01:54:34.080 --> 01:54:38.120]   First-time numerous, as I said, loads an average of 15 pounds of 16 weeks.
[01:54:38.120 --> 01:54:39.320]   This is the key to me.
[01:54:39.320 --> 01:54:44.680]   95% of Noom customers say Noom is a good long-term solution.
[01:54:44.680 --> 01:54:46.000]   Would you agree, Brian?
[01:54:46.000 --> 01:54:48.000]   You lost a lot of weight and you've kept it off.
[01:54:48.000 --> 01:54:49.000]   You look great.
[01:54:49.000 --> 01:54:50.000]   Very, very strongly.
[01:54:50.000 --> 01:54:54.680]   I think people can look at me and go back two years and look at what I look like on those
[01:54:54.680 --> 01:54:56.680]   Twitch shows.
[01:54:56.680 --> 01:54:58.560]   Someone showed me a picture of it.
[01:54:58.560 --> 01:55:00.560]   I did not even recognize myself.
[01:55:00.560 --> 01:55:06.160]   I've kept over 100 pounds off for two years with Noom's.
[01:55:06.160 --> 01:55:07.320]   It is not hard.
[01:55:07.320 --> 01:55:09.920]   It changes the way that you think about food.
[01:55:09.920 --> 01:55:13.160]   I was down in Disney for the last two weeks.
[01:55:13.160 --> 01:55:18.240]   The way it works is it changes.
[01:55:18.240 --> 01:55:21.760]   It makes you aware of what you're eating.
[01:55:21.760 --> 01:55:25.120]   You make very deliberate choice.
[01:55:25.120 --> 01:55:29.280]   If I'm going to have the caramel popcorn, I love it.
[01:55:29.280 --> 01:55:30.720]   I can do that.
[01:55:30.720 --> 01:55:36.040]   I just need to not make other crazy choices during the day that make my calorie budget
[01:55:36.040 --> 01:55:37.040]   explode.
[01:55:37.040 --> 01:55:42.080]   It gets to a point, your body doesn't want the garbage.
[01:55:42.080 --> 01:55:48.880]   I've not had it in two years because it doesn't sound like something tastes good to me anymore.
[01:55:48.880 --> 01:55:49.920]   I love Noom.
[01:55:49.920 --> 01:55:51.400]   I still use it.
[01:55:51.400 --> 01:55:52.320]   I pay for it.
[01:55:52.320 --> 01:55:53.320]   It's great.
[01:55:53.320 --> 01:55:55.080]   It endures a thousand percent.
[01:55:55.080 --> 01:55:56.080]   Yeah, me too.
[01:55:56.080 --> 01:55:57.080]   Lisa, too.
[01:55:57.080 --> 01:55:59.400]   I know I leave food on my plate when I didn't use to do that.
[01:55:59.400 --> 01:56:01.920]   I used to be a clean plate club, right?
[01:56:01.920 --> 01:56:07.680]   If I want that Monte Cristo sandwich at the Pirates of the Caribbean, I will eat half
[01:56:07.680 --> 01:56:10.160]   of it maybe or a quarter of it and then leave the rest.
[01:56:10.160 --> 01:56:11.160]   It's okay.
[01:56:11.160 --> 01:56:14.320]   I'm eating Lisa and I both do now because of Noom as we put our forks down.
[01:56:14.320 --> 01:56:16.400]   We turn off the TV, turn off the phones.
[01:56:16.400 --> 01:56:18.880]   We put our forks down, close our eyes and taste the food.
[01:56:18.880 --> 01:56:20.480]   I didn't use to taste it.
[01:56:20.480 --> 01:56:21.480]   Believe it or not.
[01:56:21.480 --> 01:56:26.960]   Stop chasing health trends, build sustainable healthy habits with Noom's psychology based
[01:56:26.960 --> 01:56:28.440]   approach.
[01:56:28.440 --> 01:56:32.560]   Sign up for your trial today at Noom.com/twit.
[01:56:32.560 --> 01:56:36.040]   Noom and double ohm.com/twit.
[01:56:36.040 --> 01:56:37.040]   Sign up for a trial today.
[01:56:37.040 --> 01:56:39.800]   Check out Noom's first ever book, The Noom Mindset, by the way.
[01:56:39.800 --> 01:56:44.320]   It just came out a deep dive into the psychology of behavior change and it's available wherever
[01:56:44.320 --> 01:56:47.720]   you get your books, even Blackwells.
[01:56:47.720 --> 01:56:50.160]   N double ohm.com/twit.
[01:56:50.160 --> 01:56:53.360]   Brianna and I and Lisa all are believers.
[01:56:53.360 --> 01:56:59.080]   One of our chatters lost 60 pounds on Noom and has kept it off.
[01:56:59.080 --> 01:57:01.200]   It's just, it works.
[01:57:01.200 --> 01:57:02.200]   That's all I can say.
[01:57:02.200 --> 01:57:03.200]   It works.
[01:57:03.200 --> 01:57:04.800]   And we thank Noom so much for their support.
[01:57:04.800 --> 01:57:05.800]   You support us too.
[01:57:05.800 --> 01:57:06.800]   It's important.
[01:57:06.800 --> 01:57:10.200]   Noom.com/twit.
[01:57:10.200 --> 01:57:13.680]   You know, we haven't talked about all day and I find it very refreshing.
[01:57:13.680 --> 01:57:15.680]   We haven't mentioned AI.
[01:57:15.680 --> 01:57:18.320]   Well, a little bit.
[01:57:18.320 --> 01:57:19.320]   You talked about the AI.
[01:57:19.320 --> 01:57:20.320]   That's why I'm a lot.
[01:57:20.320 --> 01:57:21.320]   It's over.
[01:57:21.320 --> 01:57:22.320]   I think AI is over.
[01:57:22.320 --> 01:57:23.320]   It's over.
[01:57:23.320 --> 01:57:24.320]   That was a big deal.
[01:57:24.320 --> 01:57:25.320]   Well, I wasn't over.
[01:57:25.320 --> 01:57:28.720]   I'm glad we had because last week was RSA, the big security group in San Francisco and
[01:57:28.720 --> 01:57:30.360]   it's all everybody was talking about.
[01:57:30.360 --> 01:57:31.360]   It was AI.
[01:57:31.360 --> 01:57:32.360]   It was AI.
[01:57:32.360 --> 01:57:36.440]   Well, I, one of the things I don't like about Twitter now is there's a disproportionate amount
[01:57:36.440 --> 01:57:37.440]   of AI.
[01:57:37.440 --> 01:57:38.840]   It was like when blockchain was big.
[01:57:38.840 --> 01:57:42.320]   It was, there's a disproportionate amount of attention paid to it.
[01:57:42.320 --> 01:57:43.320]   AI bros.
[01:57:43.320 --> 01:57:44.320]   It's all AI bros now.
[01:57:44.320 --> 01:57:46.520]   We have three bros and FTE bros.
[01:57:46.520 --> 01:57:47.520]   Yeah.
[01:57:47.520 --> 01:57:48.520]   Crypto bros.
[01:57:48.520 --> 01:57:53.080]   But that doesn't, the problem is that gives it kudos that it probably doesn't deserve.
[01:57:53.080 --> 01:57:56.840]   Well, this is what's so difficult is, and I, it's a question I ask and I've been asking
[01:57:56.840 --> 01:57:59.760]   since the chat GPT went public.
[01:57:59.760 --> 01:58:06.320]   Actually even before that with Dolly too and stable diffusion is, you know, we, did
[01:58:06.320 --> 01:58:07.320]   we talk, we did.
[01:58:07.320 --> 01:58:08.480]   We talked last week.
[01:58:08.480 --> 01:58:11.920]   I thought about Jaren Lanier's and we talked on Twig about it about Jaren Lanier's.
[01:58:11.920 --> 01:58:17.080]   I thought very good article in the New Yorker saying there's no such thing as AI.
[01:58:17.080 --> 01:58:22.120]   What AI really, what we're seeing anyway, the AI we're seeing today is really more of
[01:58:22.120 --> 01:58:25.160]   a collaboration between humans.
[01:58:25.160 --> 01:58:29.320]   Everything it says, everything it draws is based on stuff.
[01:58:29.320 --> 01:58:34.000]   It's basically mashing together from humans.
[01:58:34.000 --> 01:58:39.000]   And so if you think of it that way instead of as some sort of evil intelligence about
[01:58:39.000 --> 01:58:44.080]   to take over the world, it's a little bit more less, less intimidating and a little bit
[01:58:44.080 --> 01:58:47.920]   more frankly realistic as to what it can achieve.
[01:58:47.920 --> 01:58:54.760]   When I was in Italy, I went to open AI and it was blocked in Italy.
[01:58:54.760 --> 01:58:59.200]   The Italian regulator was concerned about privacy.
[01:58:59.200 --> 01:59:05.240]   That GPT is now back after meeting watchdog demands, CHAP GPT and open AI had always
[01:59:05.240 --> 01:59:07.520]   said, well, chat TPT doesn't say anything.
[01:59:07.520 --> 01:59:10.280]   Open AI actually does.
[01:59:10.280 --> 01:59:12.080]   It says a lot, but it's meaningless.
[01:59:12.080 --> 01:59:15.080]   Open AI had said, well, no, we're very careful about privacy.
[01:59:15.080 --> 01:59:19.200]   I think they reassured the regulators.
[01:59:19.200 --> 01:59:22.760]   They fulfilled according to the AP a raft of conditions.
[01:59:22.760 --> 01:59:26.520]   The Italian data protection authority wanted satisfied.
[01:59:26.520 --> 01:59:33.360]   And so the ban was lifted, but it ain't over until it's over.
[01:59:33.360 --> 01:59:37.600]   I think the, I don't know if it's a digital markets act, but the EU is.
[01:59:37.600 --> 01:59:39.960]   There's this new AI act that they put together very quickly.
[01:59:39.960 --> 01:59:40.960]   Oh, yeah.
[01:59:40.960 --> 01:59:41.960]   They have a new one.
[01:59:41.960 --> 01:59:42.960]   Oh, great.
[01:59:42.960 --> 01:59:48.380]   So these are folks focused on risky usages.
[01:59:48.380 --> 01:59:51.760]   And there's our favorite Margaret A. the Vastaker.
[01:59:51.760 --> 01:59:53.600]   Add it again.
[01:59:53.600 --> 01:59:58.040]   "The use of artificial intelligence that threaten people's safety or rights such as live, facial
[01:59:58.040 --> 02:00:03.320]   scanning should be banned or tightly controlled," you officials said Wednesday, as they outlined
[02:00:03.320 --> 02:00:08.960]   an ambitious package of proposed regulations to rein in the rapidly expanding technology.
[02:00:08.960 --> 02:00:11.200]   I wouldn't disagree with them on face recognition.
[02:00:11.200 --> 02:00:12.200]   Right.
[02:00:12.200 --> 02:00:17.000]   Which in the US we only have that in Illinois right now, which is how you end up with a
[02:00:17.000 --> 02:00:19.440]   number of facial facial recognition companies being able to work.
[02:00:19.440 --> 02:00:22.720]   And even we end up as taxpayers paying for them because a bunch of police departments
[02:00:22.720 --> 02:00:26.680]   end up paying those facial recognition companies, which there's been a number of stories of
[02:00:26.680 --> 02:00:31.560]   people being misrecognized and arrested.
[02:00:31.560 --> 02:00:33.520]   Some pretty sad stories there.
[02:00:33.520 --> 02:00:38.800]   But yeah, the interesting thing I think for Europe and AI companies is Europe has such
[02:00:38.800 --> 02:00:42.880]   a big focus on effectively the right to be forgotten.
[02:00:42.880 --> 02:00:48.800]   And so how do you train a large language model on a petabytes of data and then ignore some
[02:00:48.800 --> 02:00:52.120]   little part of it that says that somebody was arrested at some point, which they have
[02:00:52.120 --> 02:00:54.080]   the right to remove that from the Google index.
[02:00:54.080 --> 02:00:56.720]   Do you have the right to remove that from AI?
[02:00:56.720 --> 02:00:59.560]   And I think that's where a bunch of these fights are going to be.
[02:00:59.560 --> 02:01:03.800]   Is where is it practical to retrain or maybe what they'll end up doing?
[02:01:03.800 --> 02:01:08.560]   It's just like certain people, if I said, you know, I want the right to be forgotten from
[02:01:08.560 --> 02:01:12.360]   OpenAI, maybe Alex Stammos, it just becomes a list of words.
[02:01:12.360 --> 02:01:13.360]   Unperson.
[02:01:13.360 --> 02:01:14.360]   I'm a non person, right?
[02:01:14.360 --> 02:01:18.440]   Like they, instead of retraining the entire thing, which would take the power output of
[02:01:18.440 --> 02:01:24.080]   Bolivia for a week or whatever, they just say that these are certain people that you
[02:01:24.080 --> 02:01:25.560]   can't talk about.
[02:01:25.560 --> 02:01:31.760]   So the EU says they're taking a four level risk based approach to balance because they
[02:01:31.760 --> 02:01:34.760]   want to balance privacy rights against the innovation.
[02:01:34.760 --> 02:01:40.440]   And I don't think any country wants to say no AI because that's clearly something that's
[02:01:40.440 --> 02:01:41.440]   happening.
[02:01:41.440 --> 02:01:42.440]   I mean, there's a good quote.
[02:01:42.440 --> 02:01:47.280]   I forget who said it, but the British Empire did not win the Industrial Revolution by outline
[02:01:47.280 --> 02:01:48.280]   steam.
[02:01:48.280 --> 02:01:49.280]   Yeah.
[02:01:49.280 --> 02:01:50.280]   Yeah.
[02:01:50.280 --> 02:01:51.280]   I think people want to be careful.
[02:01:51.280 --> 02:01:55.320]   The Chinese have been much more aggressive actually in the use of AI.
[02:01:55.320 --> 02:01:56.960]   And I think that that's been controversial.
[02:01:56.960 --> 02:01:57.960]   Yeah.
[02:01:57.960 --> 02:01:58.960]   Usand control both.
[02:01:58.960 --> 02:02:00.440]   Both in the use by the control.
[02:02:00.440 --> 02:02:01.440]   Yeah.
[02:02:01.440 --> 02:02:02.440]   Right.
[02:02:02.440 --> 02:02:03.440]   And their regulations.
[02:02:03.440 --> 02:02:04.440]   They control it.
[02:02:04.440 --> 02:02:05.440]   Yeah.
[02:02:05.440 --> 02:02:10.240]   Unlimited AI, but we control it.
[02:02:10.240 --> 02:02:14.440]   So the EU, I think I have to say the things they've talked about makes sense.
[02:02:14.440 --> 02:02:22.240]   Case recognition, the use of AI systems to filter out school job or loan applicants.
[02:02:22.240 --> 02:02:24.440]   That's clearly fraught with peril.
[02:02:24.440 --> 02:02:29.680]   They would ban AI outright in a few cases considered to risky such as government social
[02:02:29.680 --> 02:02:32.640]   scoring systems that judge people based on their behavior.
[02:02:32.640 --> 02:02:37.840]   That's something they have done or at least tried it in certain cases in China.
[02:02:37.840 --> 02:02:39.280]   That's clearly something we don't want.
[02:02:39.280 --> 02:02:40.280]   Yeah.
[02:02:40.280 --> 02:02:41.280]   Yeah.
[02:02:41.280 --> 02:02:42.280]   The problem there is not the technology.
[02:02:42.280 --> 02:02:44.280]   It's the bad judgment of the government official who would use it badly.
[02:02:44.280 --> 02:02:45.280]   Yeah.
[02:02:45.280 --> 02:02:46.280]   Yeah.
[02:02:46.280 --> 02:02:47.280]   Well, that's often the case, isn't it?
[02:02:47.280 --> 02:02:48.280]   Right?
[02:02:48.280 --> 02:02:49.280]   Yeah.
[02:02:49.280 --> 02:02:52.200]   Which is also much more reasonable than some of people who've reeked out and said like,
[02:02:52.200 --> 02:02:56.360]   you have to have a moratorium on people building large language models and you know, get ready
[02:02:56.360 --> 02:02:57.360]   to go.
[02:02:57.360 --> 02:02:58.360]   Let's take six months off.
[02:02:58.360 --> 02:02:59.360]   Right.
[02:02:59.360 --> 02:03:01.440]   Let's take six months off so we can catch up and bomb data centers.
[02:03:01.440 --> 02:03:05.200]   You know, like the actual application of the AI is where is where the rubber hits the
[02:03:05.200 --> 02:03:07.440]   road and that seems a totally reasonable place.
[02:03:07.440 --> 02:03:08.680]   I love the problem with AI.
[02:03:08.680 --> 02:03:09.680]   You just did it.
[02:03:09.680 --> 02:03:11.560]   You just said it to Jeff as people.
[02:03:11.560 --> 02:03:12.560]   It's how it's used.
[02:03:12.560 --> 02:03:13.560]   Right.
[02:03:13.560 --> 02:03:14.560]   Yeah.
[02:03:14.560 --> 02:03:19.760]   Unacceptable uses also in the you would include manipulating behavior, exploiting children's
[02:03:19.760 --> 02:03:20.760]   vulnerabilities.
[02:03:20.760 --> 02:03:21.760]   Let's stop them.
[02:03:21.760 --> 02:03:22.760]   Let's not break them.
[02:03:22.760 --> 02:03:23.760]   Yeah.
[02:03:23.760 --> 02:03:27.320]   Manipulating behavior like like all media don't manipulate behavior like all advertising
[02:03:27.320 --> 02:03:28.720]   is manipulating behavior.
[02:03:28.720 --> 02:03:30.320]   All politics is manipulating behavior.
[02:03:30.320 --> 02:03:32.120]   Oh, does that mean?
[02:03:32.120 --> 02:03:33.600]   Yeah, but you don't want AI to do that.
[02:03:33.600 --> 02:03:35.360]   You want real humans doing that.
[02:03:35.360 --> 02:03:36.360]   Right.
[02:03:36.360 --> 02:03:39.400]   It's an important job and you want to take the job from the European politics.
[02:03:39.400 --> 02:03:43.000]   Well, we don't want what we don't want is Russian troll farms generating hundreds of
[02:03:43.000 --> 02:03:44.840]   thousands of AI generated accounts.
[02:03:44.840 --> 02:03:47.200]   Yes, because they really care about the law.
[02:03:47.200 --> 02:03:48.200]   Yeah.
[02:03:48.200 --> 02:03:49.200]   If there's anybody that's not going to stop them.
[02:03:49.200 --> 02:03:52.960]   If there's anybody who really cares about EU law, it's a Russian troll.
[02:03:52.960 --> 02:03:58.960]   I love it when you see chat GPT error messages in replies to tweets.
[02:03:58.960 --> 02:03:59.960]   That's always hysterical.
[02:03:59.960 --> 02:04:02.360]   You know, they say, sorry, I can't talk about that.
[02:04:02.360 --> 02:04:03.360]   Yeah.
[02:04:03.360 --> 02:04:05.040]   So you've written up a good point though, talking about chat GPT.
[02:04:05.040 --> 02:04:08.840]   I think there's a there's a lot of smart stuff here in the EU regulation.
[02:04:08.840 --> 02:04:12.880]   The problem is the EU and everybody else is focusing on open AI, Microsoft Google.
[02:04:12.880 --> 02:04:15.800]   Those people have teams that more or less are thinking about these things.
[02:04:15.800 --> 02:04:17.480]   They're better than the others.
[02:04:17.480 --> 02:04:22.760]   And yes, and the problem is not necessarily there will be uses of their platforms that
[02:04:22.760 --> 02:04:25.360]   can be harmful, but they will at least react to that.
[02:04:25.360 --> 02:04:28.400]   A ton of this stuff you can just run on your Nvidia card at home.
[02:04:28.400 --> 02:04:29.400]   Right.
[02:04:29.400 --> 02:04:35.080]   Art team at Sanford, we did we have a pre print out where we tested GPT three generate disinformation
[02:04:35.080 --> 02:04:39.160]   against real Russian and Iranian disinformation.
[02:04:39.160 --> 02:04:43.760]   And now we're regenerating it with stuff just running on my RTX 4090 at home, which is great
[02:04:43.760 --> 02:04:45.000]   because now I can write off my.
[02:04:45.000 --> 02:04:47.040]   I was just going to say, good deal.
[02:04:47.040 --> 02:04:48.040]   Yeah.
[02:04:48.040 --> 02:04:49.960]   How did you get one?
[02:04:49.960 --> 02:04:52.880]   Yeah, yeah, easier to get than they used to be.
[02:04:52.880 --> 02:04:53.880]   I have a truck.
[02:04:53.880 --> 02:04:54.880]   Yeah.
[02:04:54.880 --> 02:04:57.320]   It turns out you have that flipper zero.
[02:04:57.320 --> 02:04:58.760]   Oh, there you go.
[02:04:58.760 --> 02:05:01.120]   Just open those pod bay doors.
[02:05:01.120 --> 02:05:05.920]   So there's also this thing auto GPT has become very popular now.
[02:05:05.920 --> 02:05:13.680]   It's a GitHub repository that basically merges chat GPT with if this than that.
[02:05:13.680 --> 02:05:17.000]   And suddenly you've got a guy with agency.
[02:05:17.000 --> 02:05:19.120]   Now I'm scared to be now.
[02:05:19.120 --> 02:05:23.040]   I'm actually I mean, automated home stuff is already so janky like, what I'm going to
[02:05:23.040 --> 02:05:26.400]   throw into here is this humongous large language model that says totally unpredictable.
[02:05:26.400 --> 02:05:28.960]   So the lights off for all of the garage door.
[02:05:28.960 --> 02:05:29.960]   Yeah.
[02:05:29.960 --> 02:05:31.480]   I'm sorry, Dave.
[02:05:31.480 --> 02:05:32.480]   I can't.
[02:05:32.480 --> 02:05:33.480]   I was.
[02:05:33.480 --> 02:05:38.520]   I was so confused with something Nate Silver said on Twitter yesterday.
[02:05:38.520 --> 02:05:45.920]   So he was arguing that these kinds of a large language model searches were better than conventional
[02:05:45.920 --> 02:05:48.160]   search now.
[02:05:48.160 --> 02:05:53.320]   And I had to say, I spent a lot of time using the new being, which overall it's excellent,
[02:05:53.320 --> 02:05:58.080]   like start asking at programming questions or how to write sample code for a language.
[02:05:58.080 --> 02:06:00.480]   You know, it's really impressive stuff.
[02:06:00.480 --> 02:06:05.560]   But if you drill down and start trying to solve real world problems, like I was trying
[02:06:05.560 --> 02:06:10.760]   to get it to get me information about how to score in one of my pinball machines, it
[02:06:10.760 --> 02:06:15.520]   just starts like hallucinating and mixing facts together.
[02:06:15.520 --> 02:06:16.720]   It's not even a nation.
[02:06:16.720 --> 02:06:19.520]   It's just all it is is a word assembler.
[02:06:19.520 --> 02:06:20.520]   Right.
[02:06:20.520 --> 02:06:21.520]   100% right.
[02:06:21.520 --> 02:06:22.520]   We all know that.
[02:06:22.520 --> 02:06:27.720]   100% it's irresponsible in my view for Microsoft to have started using this with its
[02:06:27.720 --> 02:06:28.720]   search engine.
[02:06:28.720 --> 02:06:30.400]   It's irresponsible for Google to consider it.
[02:06:30.400 --> 02:06:32.240]   You're going to hear your thoughts on this.
[02:06:32.240 --> 02:06:35.200]   It's irresponsible for Kevin Roost to act like it fell in love with him when he knows
[02:06:35.200 --> 02:06:36.200]   better.
[02:06:36.200 --> 02:06:38.280]   Well, that generates link clicks.
[02:06:38.280 --> 02:06:41.080]   I mean, I think that that's the problem is the relation.
[02:06:41.080 --> 02:06:42.080]   Yeah.
[02:06:42.080 --> 02:06:46.040]   It's a fascinating technology that can do interesting things, but its relationship to
[02:06:46.040 --> 02:06:47.840]   fact is not built in, right?
[02:06:47.840 --> 02:06:50.960]   I entirely share your assessment on this.
[02:06:50.960 --> 02:06:55.840]   And I don't think it's better than normal search currently.
[02:06:55.840 --> 02:06:58.560]   And it's understanding underlying technology.
[02:06:58.560 --> 02:07:04.400]   I think there are non-trivial like hurdles for it to get over to like stop hallucinating,
[02:07:04.400 --> 02:07:07.360]   show facts clearly, show sourcing clearly.
[02:07:07.360 --> 02:07:11.960]   And it's not better right now than just clicking on article from the New York Times and knowing
[02:07:11.960 --> 02:07:13.920]   that I can trust that.
[02:07:13.920 --> 02:07:20.160]   So I would submit there are some uses for AI in search.
[02:07:20.160 --> 02:07:21.440]   It's just that it shouldn't be.
[02:07:21.440 --> 02:07:24.080]   I think AI chat is more of the problem.
[02:07:24.080 --> 02:07:29.000]   And AI seems to be very good at summarizing existing content.
[02:07:29.000 --> 02:07:32.720]   So you feed it a PDF at some, and you don't see a lot of hallucination there because it's
[02:07:32.720 --> 02:07:38.200]   working off an actual database of content.
[02:07:38.200 --> 02:07:39.200]   That's fair.
[02:07:39.200 --> 02:07:40.200]   And I think so.
[02:07:40.200 --> 02:07:48.680]   I think AI could reasonably replace a spider in going out and summarizing websites and
[02:07:48.680 --> 02:07:52.680]   then making a better search index, couldn't it?
[02:07:52.680 --> 02:08:00.200]   I use, among other things, I use NIVA as I've mentioned before as a search engine.
[02:08:00.200 --> 02:08:06.400]   NIVA like another variety of other search engines gives you an AI generated summary of
[02:08:06.400 --> 02:08:07.560]   your answer first.
[02:08:07.560 --> 02:08:09.840]   And I often find that quite useful.
[02:08:09.840 --> 02:08:11.920]   Now we've talked about this on Twig before.
[02:08:11.920 --> 02:08:16.400]   It provides you with footnotes to refer to the sources.
[02:08:16.400 --> 02:08:17.400]   So it doesn't seem to--
[02:08:17.400 --> 02:08:20.480]   More sources it finds after the fact, which is all correct.
[02:08:20.480 --> 02:08:21.480]   That's correct.
[02:08:21.480 --> 02:08:25.040]   Yeah, I'm not sure actually.
[02:08:25.040 --> 02:08:27.040]   That's an interesting question.
[02:08:27.040 --> 02:08:28.040]   Right.
[02:08:28.040 --> 02:08:32.920]   But I mean, I have to agree with Brianna and JJ here.
[02:08:32.920 --> 02:08:39.760]   This was, I think, irresponsible for Microsoft to roll out chat GPT connected to search because
[02:08:39.760 --> 02:08:44.200]   people believe that search should be real.
[02:08:44.200 --> 02:08:48.960]   They have a level of belief in looking at the product of what it returns is accurate.
[02:08:48.960 --> 02:08:51.840]   Because everybody will have shown, if you ask about something it doesn't know about,
[02:08:51.840 --> 02:08:53.680]   it will just come up with BS.
[02:08:53.680 --> 02:08:54.680]   And that can include--
[02:08:54.680 --> 02:08:55.680]   It's crude.
[02:08:55.680 --> 02:08:56.680]   It lying about you, right?
[02:08:56.680 --> 02:08:57.680]   Like, you can make--
[02:08:57.680 --> 02:08:59.320]   Oh, it's saying people are dead all the time, yeah.
[02:08:59.320 --> 02:09:00.320]   Right, right.
[02:09:00.320 --> 02:09:02.240]   Or you can make it-- you know, why was this person canceled?
[02:09:02.240 --> 02:09:03.240]   And it will create an--
[02:09:03.240 --> 02:09:04.720]   Oh, let me make up why.
[02:09:04.720 --> 02:09:08.120]   Of why this person was canceled, why this person was fired from their job.
[02:09:08.120 --> 02:09:11.040]   Based upon things that happened to other folks.
[02:09:11.040 --> 02:09:15.160]   And so there's a big difference to me for like what OpenAI did, which is like they built
[02:09:15.160 --> 02:09:21.360]   a tool, a game, a playground, a sandbox that you can play in, is very different than attaching
[02:09:21.360 --> 02:09:22.360]   this and saying this is part of--
[02:09:22.360 --> 02:09:23.360]   Yes.
[02:09:23.360 --> 02:09:24.360]   Yeah.
[02:09:24.360 --> 02:09:31.600]   There's a really interesting video out there where someone gets basically chat GPT to code.
[02:09:31.600 --> 02:09:34.400]   What was the name of the Flappy Bird?
[02:09:34.400 --> 02:09:35.400]   Flappy Bird.
[02:09:35.400 --> 02:09:36.400]   Right.
[02:09:36.400 --> 02:09:40.160]   It creates the AI, another AI program, creates all the art assets.
[02:09:40.160 --> 02:09:44.840]   It goes through and it like creates all the steps that you're going to need to create
[02:09:44.840 --> 02:09:47.960]   a playable version of this game.
[02:09:47.960 --> 02:09:52.280]   I think it's really good for creative things like that.
[02:09:52.280 --> 02:09:55.600]   I don't think it's really good at sourcing in my experience.
[02:09:55.600 --> 02:10:02.280]   You know, the use case, you described like go through my email and tell me like what all
[02:10:02.280 --> 02:10:05.520]   the emails from Jason are saying.
[02:10:05.520 --> 02:10:08.680]   That is a-- that's limited with the sources it's doing.
[02:10:08.680 --> 02:10:11.360]   That's something I think it's really going to be good at.
[02:10:11.360 --> 02:10:17.080]   I think this broad declaration that's going to be better from search, I invite your audience,
[02:10:17.080 --> 02:10:22.440]   make your own assessment, go to Bing, find some subject you know a lot about and start
[02:10:22.440 --> 02:10:27.240]   asking questions and see if those answers are accurate because that has not been-- it's
[02:10:27.240 --> 02:10:30.440]   been a very confidence-shaking exercise.
[02:10:30.440 --> 02:10:31.440]   Here's what--
[02:10:31.440 --> 02:10:34.880]   I even think it's a mistake to call it hallucination because that is part of the effort from war
[02:10:34.880 --> 02:10:35.880]   civilization.
[02:10:35.880 --> 02:10:37.320]   I know, I agree.
[02:10:37.320 --> 02:10:38.320]   And--
[02:10:38.320 --> 02:10:39.320]   Word order.
[02:10:39.320 --> 02:10:41.040]   Yeah, word salad.
[02:10:41.040 --> 02:10:42.200]   Yeah, yeah, yeah.
[02:10:42.200 --> 02:10:43.200]   It's word--
[02:10:43.200 --> 02:10:48.240]   If we said-- rather than acting as if it writes, it doesn't write, it assembles words.
[02:10:48.240 --> 02:10:49.240]   Yeah.
[02:10:49.240 --> 02:10:50.240]   So it does.
[02:10:50.240 --> 02:10:59.080]   Here is Neva's AI summary of who you are, Brianna Wu, born July 6, 1977 American video
[02:10:59.080 --> 02:11:01.040]   game developer and computer programmer.
[02:11:01.040 --> 02:11:05.160]   That's from Wikipedia, Kofana giant space cat and independent video game development
[02:11:05.160 --> 02:11:09.120]   studio with Amanda Warner and Boston, also blogger and podcasting matters relating
[02:11:09.120 --> 02:11:10.320]   to the video game industry.
[02:11:10.320 --> 02:11:13.400]   Well, I can vouch for the fact that you're not limited to the video game industry.
[02:11:13.400 --> 02:11:15.280]   But other than that, it's not too bad.
[02:11:15.280 --> 02:11:17.320]   But all of these things are sourced just from Wikipedia.
[02:11:17.320 --> 02:11:18.320]   Yeah, notice that.
[02:11:18.320 --> 02:11:19.320]   That's just why we're--
[02:11:19.320 --> 02:11:21.520]   And if Wikipedia were wrong, that would be wrong.
[02:11:21.520 --> 02:11:26.840]   Which I have to say, one of the problems with Wikipedia is Brianna is dealt with more
[02:11:26.840 --> 02:11:30.000]   abuse than I have, so she's probably better protected.
[02:11:30.000 --> 02:11:31.000]   I have randos.
[02:11:31.000 --> 02:11:32.000]   Yeah.
[02:11:32.000 --> 02:11:34.280]   I'm-- we're in some political issues right now.
[02:11:34.280 --> 02:11:38.520]   Well, don't like it when you study people lying about the election.
[02:11:38.520 --> 02:11:39.520]   Right.
[02:11:39.520 --> 02:11:43.360]   And so as a result, Wikipedia effectively lets anonymous people just use their IP addresses
[02:11:43.360 --> 02:11:46.120]   without authenticating to edit my Wikipedia page, which I'm--
[02:11:46.120 --> 02:11:47.680]   So it's not a definitive source.
[02:11:47.680 --> 02:11:48.680]   It is not--
[02:11:48.680 --> 02:11:49.920]   I have a story about that.
[02:11:49.920 --> 02:11:50.440]   Yeah.
[02:11:50.440 --> 02:11:53.840]   So if you're super famous, if you're Joe Biden, there's a ton of people watching.
[02:11:53.840 --> 02:11:54.360]   Watch.
[02:11:54.360 --> 02:11:56.280]   If anything, your accounts locked or whatever.
[02:11:56.280 --> 02:11:58.880]   If you're not notable enough, they delete you as not notable.
[02:11:58.880 --> 02:12:02.640]   I'm in that horrible middle where I'm notable enough that I can't argue that they delete
[02:12:02.640 --> 02:12:03.640]   the total page.
[02:12:03.640 --> 02:12:05.240]   But nobody's paying attention.
[02:12:05.240 --> 02:12:07.600]   And so I'm pretty sure I know who one of these people are.
[02:12:07.600 --> 02:12:08.600]   People don't like me.
[02:12:08.600 --> 02:12:09.600]   They can just go.
[02:12:09.600 --> 02:12:10.600]   They don't have to log in.
[02:12:10.600 --> 02:12:12.640]   They just write a random IP address.
[02:12:12.640 --> 02:12:13.640]   And they can change stuff.
[02:12:13.640 --> 02:12:17.280]   And then if I go in and say, this isn't true, they turned off my account.
[02:12:17.280 --> 02:12:18.280]   Right.
[02:12:18.280 --> 02:12:19.280]   We're doing that.
[02:12:19.280 --> 02:12:20.280]   Right.
[02:12:20.280 --> 02:12:22.360]   And so it's like, Wikipedia is actually a great example of what you have to be super
[02:12:22.360 --> 02:12:26.760]   careful pulling from if you're AI of anything it has to do with individuals.
[02:12:26.760 --> 02:12:27.760]   Yeah.
[02:12:27.760 --> 02:12:31.920]   So a student came out of class from one day and saw me in the hallway and said, "Professor
[02:12:31.920 --> 02:12:36.480]   is Professor Jarvis, I just saw that you're polyamorous and your wife's okay with it."
[02:12:36.480 --> 02:12:37.480]   Oh, Lord.
[02:12:37.480 --> 02:12:38.480]   I said, "What?"
[02:12:38.480 --> 02:12:39.480]   What?
[02:12:39.480 --> 02:12:45.120]   And my wife is absolutely not okay with it.
[02:12:45.120 --> 02:12:46.800]   It's in Wikipedia.
[02:12:46.800 --> 02:12:50.560]   I said, "Somebody hates me."
[02:12:50.560 --> 02:12:51.560]   The first Wiki divorce.
[02:12:51.560 --> 02:12:52.560]   Wow.
[02:12:52.560 --> 02:12:53.560]   That was neat.
[02:12:53.560 --> 02:12:54.560]   Yeah.
[02:12:54.560 --> 02:12:55.560]   Yeah.
[02:12:55.560 --> 02:12:58.160]   Jimmy Wales has co-responded my divorce here exactly.
[02:12:58.160 --> 02:13:00.400]   The Supreme Court has weighed in.
[02:13:00.400 --> 02:13:01.400]   Oh, no.
[02:13:01.400 --> 02:13:03.640]   Brianna, you've gone through a million times more.
[02:13:03.640 --> 02:13:10.760]   I mean, I think people's marriages should probably be left out of the public, right?
[02:13:10.760 --> 02:13:11.760]   That's just me.
[02:13:11.760 --> 02:13:13.760]   I don't know.
[02:13:13.760 --> 02:13:14.760]   Oh, Lord.
[02:13:14.760 --> 02:13:16.240]   Well, it's in Wikipedia.
[02:13:16.240 --> 02:13:17.240]   It must be true.
[02:13:17.240 --> 02:13:18.240]   That's true.
[02:13:18.240 --> 02:13:22.880]   So, Leo, I don't want to spend time on it, but I just wanted to plug something in line
[02:13:22.880 --> 02:13:23.880]   1782.
[02:13:23.880 --> 02:13:28.360]   The New York Times had a pretty good explainer of large language models.
[02:13:28.360 --> 02:13:34.200]   They just took a set of Jane Austen, and then they took screenshots of it, learning
[02:13:34.200 --> 02:13:37.600]   words, and then learning Jane Austen.
[02:13:37.600 --> 02:13:38.640]   How many steps it went through?
[02:13:38.640 --> 02:13:40.440]   Very, very fast, very quickly.
[02:13:40.440 --> 02:13:45.920]   Well, I think it's a pretty good explainer that gets people to understand what this is
[02:13:45.920 --> 02:13:46.920]   doing and how it's doing.
[02:13:46.920 --> 02:13:48.440]   Well, at first, you don't even know what a word is.
[02:13:48.440 --> 02:13:50.040]   It's just the way of the characters.
[02:13:50.040 --> 02:13:51.760]   So it's garbage.
[02:13:51.760 --> 02:13:52.760]   It's gobbledygook.
[02:13:52.760 --> 02:13:53.760]   Leech garbage, right?
[02:13:53.760 --> 02:13:54.760]   Yeah.
[02:13:54.760 --> 02:13:58.800]   Now, as you continue, it will get smarter.
[02:13:58.800 --> 02:13:59.800]   Yeah.
[02:13:59.800 --> 02:14:01.800]   What, for 250 rounds?
[02:14:01.800 --> 02:14:02.960]   Not much smarter.
[02:14:02.960 --> 02:14:05.160]   Two words come out.
[02:14:05.160 --> 02:14:09.000]   250 rounds in a matter of seconds.
[02:14:09.000 --> 02:14:10.240]   Yeah, that's the thing.
[02:14:10.240 --> 02:14:11.240]   That's what's interesting.
[02:14:11.240 --> 02:14:16.120]   So when you see that delay, it's actually doing this.
[02:14:16.120 --> 02:14:18.320]   It's going through stuff.
[02:14:18.320 --> 02:14:25.640]   Of course, I also refer our more computer literate readers and listeners to Stephen Wolfram's
[02:14:25.640 --> 02:14:28.440]   excellent article on how it works.
[02:14:28.440 --> 02:14:29.440]   It's a little bit more mathematical.
[02:14:29.440 --> 02:14:33.760]   If you're trying to show your aunt what this stuff is, a tea potty.
[02:14:33.760 --> 02:14:36.120]   Not Aunt Pruitt and Aunt Martha.
[02:14:36.120 --> 02:14:37.880]   No, no, you're Aunt.
[02:14:37.880 --> 02:14:40.360]   You're Aunt.
[02:14:40.360 --> 02:14:42.320]   So this is actually a great piece.
[02:14:42.320 --> 02:14:44.880]   It's from the upshot.
[02:14:44.880 --> 02:14:51.200]   I'm not sure I would always trust the New York Times take on AI as Kevin Ruse is present.
[02:14:51.200 --> 02:14:52.200]   Yes.
[02:14:52.200 --> 02:14:53.440]   But this was well done.
[02:14:53.440 --> 02:14:56.000]   I think this is very well done.
[02:14:56.000 --> 02:15:00.360]   If you want to get really nerdy, I recommend my colleague Andrew at Stanford CS has a Coursera
[02:15:00.360 --> 02:15:01.520]   course on this.
[02:15:01.520 --> 02:15:02.520]   Oh, nice.
[02:15:02.520 --> 02:15:03.520]   Yeah, it's quite good.
[02:15:03.520 --> 02:15:04.520]   Andrew Ng.
[02:15:04.520 --> 02:15:05.520]   Yeah, and MG.
[02:15:05.520 --> 02:15:07.520]   And then Coursera.
[02:15:07.520 --> 02:15:10.960]   And I'm going to search for it right now.
[02:15:10.960 --> 02:15:12.520]   It's just chat GBT.
[02:15:12.520 --> 02:15:14.840]   What is the best online course about AI?
[02:15:14.840 --> 02:15:17.520]   Is it the AI for everyone course?
[02:15:17.520 --> 02:15:18.520]   Is that?
[02:15:18.520 --> 02:15:19.520]   So is it an AI for everyone?
[02:15:19.520 --> 02:15:21.640]   Then he has a deep one.
[02:15:21.640 --> 02:15:22.640]   That's an engineering one.
[02:15:22.640 --> 02:15:24.080]   Ah, nice.
[02:15:24.080 --> 02:15:27.720]   So depending on how geeky you want to get, you can also.
[02:15:27.720 --> 02:15:30.120]   Well, it's basically a copy of what you see at Stanford.
[02:15:30.120 --> 02:15:33.600]   And for all the people that talk about, we need to put, you know, have a pause or put
[02:15:33.600 --> 02:15:35.560]   the genie back in the bottle.
[02:15:35.560 --> 02:15:36.560]   That genie is out.
[02:15:36.560 --> 02:15:38.200]   Oh, I did my math right.
[02:15:38.200 --> 02:15:43.720]   I think this quarter, just this quarter, 15% of Stanford's undergraduates are taking
[02:15:43.720 --> 02:15:46.880]   the intro to AI class, not 15% of students.
[02:15:46.880 --> 02:15:47.880]   Of all.
[02:15:47.880 --> 02:15:48.880]   All percent of all.
[02:15:48.880 --> 02:15:51.840]   Yeah, one five, 15% just this quarter.
[02:15:51.840 --> 02:15:56.760]   And so, you know, that's the kind of knowledge distribution that, you know, you can't put
[02:15:56.760 --> 02:15:58.720]   it back in through regulation and such.
[02:15:58.720 --> 02:16:02.320]   You have to regulate the impacts, and mitigate the dangers.
[02:16:02.320 --> 02:16:04.720]   You're not going to be able to stop people from doing more work in this space.
[02:16:04.720 --> 02:16:05.720]   Yeah.
[02:16:05.720 --> 02:16:07.560]   And it's lemmings over the tensor chip.
[02:16:07.560 --> 02:16:08.560]   Geez.
[02:16:08.560 --> 02:16:13.560]   So help me because I, I'm so overwhelmed.
[02:16:13.560 --> 02:16:16.640]   It's not the first time technology topics have overwhelmed me.
[02:16:16.640 --> 02:16:21.400]   In fact, it's kind of been a story of my 40 years covering this stuff.
[02:16:21.400 --> 02:16:25.760]   It's, it's, it's like drinking from a hose.
[02:16:25.760 --> 02:16:30.840]   But in this case, I'm so overwhelmed by the stories and the information on AI that have
[02:16:30.840 --> 02:16:34.880]   kind of started to tune it out that I try to understand it.
[02:16:34.880 --> 02:16:40.840]   But I am not, for instance, going to product hunt and taking a look at all the new AI startups
[02:16:40.840 --> 02:16:42.720]   every day.
[02:16:42.720 --> 02:16:44.600]   Am I missing out by not doing that?
[02:16:44.600 --> 02:16:47.120]   Is something magical about to happen?
[02:16:47.120 --> 02:16:51.760]   I think you're probably going to be blindsided for 2024.
[02:16:51.760 --> 02:16:52.760]   So.
[02:16:52.760 --> 02:16:55.960]   Well, I'll be interesting for the, for the presidential election, you think.
[02:16:55.960 --> 02:16:56.960]   Yeah.
[02:16:56.960 --> 02:17:01.440]   I think that I think it's going to play a major role in disinformation and, you know, creating
[02:17:01.440 --> 02:17:04.160]   false sentiment this time around.
[02:17:04.160 --> 02:17:09.040]   But I just feel like it's impossible to keep up at this point with what's happening in
[02:17:09.040 --> 02:17:10.040]   AI.
[02:17:10.040 --> 02:17:11.040]   Am I wrong, Alex?
[02:17:11.040 --> 02:17:14.160]   I mean, there's a lot of, there's, there's a lot to follow.
[02:17:14.160 --> 02:17:15.400]   There's a lot going on.
[02:17:15.400 --> 02:17:16.840]   I think Brandon's right.
[02:17:16.840 --> 02:17:17.840]   2024 is going to be best.
[02:17:17.840 --> 02:17:20.320]   The RNC already had a AI generated ad.
[02:17:20.320 --> 02:17:21.320]   Now.
[02:17:21.320 --> 02:17:22.560]   Yeah, you got a little trouble for it.
[02:17:22.560 --> 02:17:23.560]   Yeah.
[02:17:23.560 --> 02:17:24.560]   And they didn't really do anything.
[02:17:24.560 --> 02:17:28.320]   They obviously did it because they wanted the story to be that used at AI generated that.
[02:17:28.320 --> 02:17:33.400]   So they got a huge amount of free play for, for this ad of people criticizing it.
[02:17:33.400 --> 02:17:36.160]   Not, they're not, I don't think they're the folks who invented that little political
[02:17:36.160 --> 02:17:37.160]   trick, right?
[02:17:37.160 --> 02:17:41.560]   Of having a controversial ad that you put $10 behind, but you get a million dollars in
[02:17:41.560 --> 02:17:43.400]   free airplay.
[02:17:43.400 --> 02:17:48.920]   But you know, it's not going to be next time declared by the RNC with a little disclaimer
[02:17:48.920 --> 02:17:51.880]   of, on the bottom as well as who paid for it.
[02:17:51.880 --> 02:17:57.360]   There's no reason why you can't do that on a individualized basis.
[02:17:57.360 --> 02:18:01.480]   And you can have, you know, those kinds of ads be going out to much, much smaller groups.
[02:18:01.480 --> 02:18:06.720]   And so one of the fascinating questions will be a number of companies have been kind of
[02:18:06.720 --> 02:18:09.360]   diluting their standards around political advertising.
[02:18:09.360 --> 02:18:13.920]   Twitter, for example, through a lot of them, Facebook is opening up to allow, are they
[02:18:13.920 --> 02:18:16.320]   going to allow AI generated political advertising?
[02:18:16.320 --> 02:18:22.360]   Because if so, if you can break people up into 100 person groups and then AI generate
[02:18:22.360 --> 02:18:28.160]   50, 100 different advertisements, both the voice and the video, and then do A/B testing,
[02:18:28.160 --> 02:18:32.200]   that's the kind of thing that you could build upon what people did in 2016 and 2020 and
[02:18:32.200 --> 02:18:33.720]   really shoot their stratosphere.
[02:18:33.720 --> 02:18:37.520]   Because you no longer have to have somebody creating the art, creating the script.
[02:18:37.520 --> 02:18:42.400]   If it's all automated, you could test 10,000, 100,000 ads and then find the 10 that work
[02:18:42.400 --> 02:18:45.360]   in those 10 different segments and then put all your money behind them.
[02:18:45.360 --> 02:18:47.360]   Somebody is definitely working on that now, I'm sure.
[02:18:47.360 --> 02:18:49.360]   But it's a really good idea.
[02:18:49.360 --> 02:18:50.360]   I do this.
[02:18:50.360 --> 02:18:51.360]   I guess it's going to go to the third.
[02:18:51.360 --> 02:18:52.360]   Oh, God.
[02:18:52.360 --> 02:18:53.360]   Really?
[02:18:53.360 --> 02:18:54.360]   Here it is.
[02:18:54.360 --> 02:18:55.360]   Yeah.
[02:18:55.360 --> 02:18:56.360]   You gave it to rebellion, Peck.
[02:18:56.360 --> 02:18:57.360]   Great.
[02:18:57.360 --> 02:18:58.360]   This video is now going to be shown at a Senate hearing.
[02:18:58.360 --> 02:19:00.800]   But that is, this is like the beginning of upworthy.
[02:19:00.800 --> 02:19:04.400]   It was the best of intentions that ruined every headline on the Internet.
[02:19:04.400 --> 02:19:06.880]   That is something we learned in 2016.
[02:19:06.880 --> 02:19:13.320]   I remember very well, we learned how you can make many, many small buys on Facebook and
[02:19:13.320 --> 02:19:17.320]   really do it as a human, but really fine tune.
[02:19:17.320 --> 02:19:18.320]   And fine tune.
[02:19:18.320 --> 02:19:19.320]   Right.
[02:19:19.320 --> 02:19:21.320]   So that's one of the things that Trump campaign did, the Hillary campaign didn't.
[02:19:21.320 --> 02:19:22.320]   That's right.
[02:19:22.320 --> 02:19:25.320]   Like if you rule back to 2016, we talked about 2016 last time I was here, so I don't want
[02:19:25.320 --> 02:19:26.320]   to do too much.
[02:19:26.320 --> 02:19:27.320]   That's okay.
[02:19:27.320 --> 02:19:32.360]   If your thesis is Facebook through the 2016 election to Trump, it is not Cambridge Analytica.
[02:19:32.360 --> 02:19:33.880]   It is not the Russians.
[02:19:33.880 --> 02:19:39.760]   It is the Trump campaign's proper use of the Facebook advertising platform helped by Facebook
[02:19:39.760 --> 02:19:40.760]   sales engineers.
[02:19:40.760 --> 02:19:41.760]   Right.
[02:19:41.760 --> 02:19:43.760]   They actually had intended engineers in the campaign.
[02:19:43.760 --> 02:19:46.840]   Which was offered to both sides, but only the Trump people, the Hillary people made these
[02:19:46.840 --> 02:19:50.320]   beautiful videos that they showed to everybody in the country.
[02:19:50.320 --> 02:19:54.960]   And Trump's team made these much cheaper ads and then tested them against the only-
[02:19:54.960 --> 02:19:57.160]   And they'd show them at 10 people at a time.
[02:19:57.160 --> 02:19:58.160]   You get 100 is the middle.
[02:19:58.160 --> 02:19:59.160]   100.
[02:19:59.160 --> 02:20:01.520]   So you can show it to 100 people, you test it for 100 people.
[02:20:01.520 --> 02:20:06.840]   And then if the 10 ads you test to 100 people, the one that works, you show it to the 10,000
[02:20:06.840 --> 02:20:12.720]   people using lookalike audiences of the other unemployed steel workers in Michigan, for example.
[02:20:12.720 --> 02:20:15.520]   So that, but that had humans, right?
[02:20:15.520 --> 02:20:17.160]   Human beings assembled all those-
[02:20:17.160 --> 02:20:18.160]   Imagine AI now.
[02:20:18.160 --> 02:20:19.160]   Imagine AI doing this.
[02:20:19.160 --> 02:20:22.680]   Was it Brad Parscale who was really the wizard behind that?
[02:20:22.680 --> 02:20:25.440]   Uh, I mean, he was the guy running that team.
[02:20:25.440 --> 02:20:26.440]   Yeah.
[02:20:26.440 --> 02:20:30.200]   I have benefited, I have benefited having nothing to do with that personally.
[02:20:30.200 --> 02:20:32.240]   So everything I know I've read afterwards.
[02:20:32.240 --> 02:20:34.440]   So I can't speak as to what he personally did.
[02:20:34.440 --> 02:20:35.440]   Yeah.
[02:20:35.440 --> 02:20:36.440]   Yeah.
[02:20:36.440 --> 02:20:41.480]   I do have to say, and this is the reality of this, it's a good scale.
[02:20:41.480 --> 02:20:47.760]   But as I'm thinking through it, the problem with doing ads on Facebook is a lot of them
[02:20:47.760 --> 02:20:51.720]   get reported and a lot of them get taken down.
[02:20:51.720 --> 02:20:53.720]   You're speaking from experience.
[02:20:53.720 --> 02:20:58.120]   I am, yeah, they get because when you put something out there that people don't like,
[02:20:58.120 --> 02:21:03.240]   they report it, that triggers Facebook's algorithm, they take it down, even if it's 100%
[02:21:03.240 --> 02:21:09.080]   compliant, and then you're waiting for a human to step in and then your news cycle has gone
[02:21:09.080 --> 02:21:10.080]   by.
[02:21:10.080 --> 02:21:17.440]   The problem is if you've started putting stuff out, the AI had done, eventually the AI is
[02:21:17.440 --> 02:21:22.760]   going to turn out stuff that is going to violate the Facebook terms of service.
[02:21:22.760 --> 02:21:27.640]   It's definitely going to get reported and your entire account is going to be banned.
[02:21:27.640 --> 02:21:33.720]   So I think this is where Facebook's aggressive policies on political advertising as much of
[02:21:33.720 --> 02:21:35.840]   the headaches as they cause me.
[02:21:35.840 --> 02:21:39.800]   I do think they would make this a very difficult scam to pull off.
[02:21:39.800 --> 02:21:43.920]   Is Facebook going to be the place to do this in 2024 or is it over Facebook?
[02:21:43.920 --> 02:21:44.920]   We don't stick to questions.
[02:21:44.920 --> 02:21:47.880]   Facebook is the only place worth political advertising.
[02:21:47.880 --> 02:21:50.120]   Twitter does not matter for elections.
[02:21:50.120 --> 02:21:51.120]   Yeah.
[02:21:51.120 --> 02:21:56.600]   It does not matter when you're talking about linking the databases of who the known voters
[02:21:56.600 --> 02:21:57.600]   are.
[02:21:57.600 --> 02:21:59.120]   Facebook is the best place to do it.
[02:21:59.120 --> 02:22:02.600]   And 80% of the people who actually vote are on Facebook.
[02:22:02.600 --> 02:22:03.600]   That's an important.
[02:22:03.600 --> 02:22:04.600]   No offense.
[02:22:04.600 --> 02:22:05.600]   No, but that's an important number.
[02:22:05.600 --> 02:22:09.320]   80% of the people who actually vote are on Facebook today.
[02:22:09.320 --> 02:22:12.600]   If you're trying to reach the North, it's still the place to go.
[02:22:12.600 --> 02:22:13.600]   Yeah.
[02:22:13.600 --> 02:22:14.600]   That is correct.
[02:22:14.600 --> 02:22:15.760]   So that's the place to watch.
[02:22:15.760 --> 02:22:18.080]   Do you feel Alex, you used to work at Facebook.
[02:22:18.080 --> 02:22:19.080]   Today's the most.
[02:22:19.080 --> 02:22:20.080]   You're not there anymore.
[02:22:20.080 --> 02:22:24.560]   But yeah, after they tried to lay it all on your shoulders.
[02:22:24.560 --> 02:22:29.320]   But no, do you feel like they understand how important this is going to be and that they
[02:22:29.320 --> 02:22:31.600]   are going to do what needs to be done?
[02:22:31.600 --> 02:22:32.600]   Well, yes.
[02:22:32.600 --> 02:22:33.880]   There's people there who understand it.
[02:22:33.880 --> 02:22:38.320]   The problem is they've had round after round of layoffs in this last layoff.
[02:22:38.320 --> 02:22:41.480]   Unfortunately, included a bunch of people who worked on influence operations.
[02:22:41.480 --> 02:22:45.080]   So the team that I helped start, I did a little bit to start.
[02:22:45.080 --> 02:22:52.440]   And then that really grew for the 2018, 2020, 2022 elections is being decimated by layoffs.
[02:22:52.440 --> 02:22:59.760]   And so one of my fears here is Elon Musk has created kind of a permission structure for
[02:22:59.760 --> 02:23:05.080]   Mark Zuckerberg to do like half as well on everything integrity related because he's
[02:23:05.080 --> 02:23:07.240]   still 10 times above what Musk is doing.
[02:23:07.240 --> 02:23:08.240]   Right?
[02:23:08.240 --> 02:23:12.000]   Yeah, he's lowered the bar and he's lowered the bar and created a structure in which as
[02:23:12.000 --> 02:23:16.160]   long as you're not out there retweeting disinformation yourself, right?
[02:23:16.160 --> 02:23:17.800]   And then like defaming people.
[02:23:17.800 --> 02:23:18.800]   You're okay.
[02:23:18.800 --> 02:23:19.800]   You're okay.
[02:23:19.800 --> 02:23:20.800]   My employees pedophiles.
[02:23:20.800 --> 02:23:21.800]   Yeah.
[02:23:21.800 --> 02:23:22.800]   You're doing better than Mark.
[02:23:22.800 --> 02:23:27.880]   And so in this new age of efficiency, I'm really afraid that as he pours all this money
[02:23:27.880 --> 02:23:34.000]   into VR, which I think is just a stupid investment that the place he's cutting is on integrity.
[02:23:34.000 --> 02:23:37.160]   And so I don't know how good that team will be for 2024.
[02:23:37.160 --> 02:23:38.160]   So Brianna, good luck.
[02:23:38.160 --> 02:23:40.200]   I mean, maybe you'll get a lot more ads approved.
[02:23:40.200 --> 02:23:44.480]   Well, you know, what you've decided, Brianna, is to fight fire with fire.
[02:23:44.480 --> 02:23:45.480]   I think you learned the lesson.
[02:23:45.480 --> 02:23:49.040]   I believe in, I don't believe in unilateral disarmament.
[02:23:49.040 --> 02:23:53.480]   I mean, I think we have a larger discussion about how elections are funded and the tools
[02:23:53.480 --> 02:23:55.880]   that we use until those policies are passed.
[02:23:55.880 --> 02:23:59.680]   I think you fight with the same tools your enemy does if you have access to them.
[02:23:59.680 --> 02:24:02.840]   So and rather win to be morally pure.
[02:24:02.840 --> 02:24:03.840]   That's how I feel.
[02:24:03.840 --> 02:24:04.840]   Yeah.
[02:24:04.840 --> 02:24:07.800]   I think that's a fascinating conversation.
[02:24:07.800 --> 02:24:10.280]   But we are going to defer that for a moment.
[02:24:10.280 --> 02:24:13.080]   Our last ad and then last thoughts.
[02:24:13.080 --> 02:24:14.600]   I wish we'd go for another five hours.
[02:24:14.600 --> 02:24:16.520]   There's so much to talk about here.
[02:24:16.520 --> 02:24:20.840]   And you just opened a whole Pandora's box, but you'll come back before 2024, I hope.
[02:24:20.840 --> 02:24:21.840]   Sure.
[02:24:21.840 --> 02:24:22.840]   Both of you.
[02:24:22.840 --> 02:24:23.840]   All right.
[02:24:23.840 --> 02:24:25.680]   One more question for us real quick on that.
[02:24:25.680 --> 02:24:30.600]   What proportion of the friends and colleagues you had a Facebook or gone from Facebook?
[02:24:30.600 --> 02:24:35.040]   So I'm not saying these people aren't replaced, but unfortunately, like every child safety
[02:24:35.040 --> 02:24:38.240]   person I worked with, most of them are gone.
[02:24:38.240 --> 02:24:41.640]   I know at least three or four people were fired from the influence ops and that threatened
[02:24:41.640 --> 02:24:43.200]   tell team.
[02:24:43.200 --> 02:24:47.640]   So there are still good people who are trying it, but just proportionally, I know that these
[02:24:47.640 --> 02:24:49.040]   teams have been.
[02:24:49.040 --> 02:24:51.040]   This is where a lot of the savings are coming from.
[02:24:51.040 --> 02:24:54.440]   Like they just, you know, they cut a bunch of money.
[02:24:54.440 --> 02:24:58.680]   They're rewarded in the stock market because without revenue going up profitability went
[02:24:58.680 --> 02:24:59.880]   back up.
[02:24:59.880 --> 02:25:03.000]   So I think Mark's getting a positive signal on this and we'll continue cutting.
[02:25:03.000 --> 02:25:05.840]   But most recent quarterly results, absolutely.
[02:25:05.840 --> 02:25:07.960]   And trust and safety doesn't make you money.
[02:25:07.960 --> 02:25:09.480]   And unfortunately, it doesn't make you money.
[02:25:09.480 --> 02:25:13.640]   And if Elon's going to do none of it, then all you have to do is some of it, right?
[02:25:13.640 --> 02:25:15.240]   You still look better than Elon.
[02:25:15.240 --> 02:25:19.120]   The other thing that honestly is happening is you now have a politicization of things
[02:25:19.120 --> 02:25:20.600]   that used to be agreed upon.
[02:25:20.600 --> 02:25:24.320]   Like Russian agents should not be able to run political ads in the United States.
[02:25:24.320 --> 02:25:25.600]   Used to be a bipartisan position.
[02:25:25.600 --> 02:25:27.960]   That's no longer effectively a bipartisan position.
[02:25:27.960 --> 02:25:31.440]   And so if you're going to be in a situation where you're going to get yelled at in a house
[02:25:31.440 --> 02:25:36.640]   hearing, because you've done the basic things to try to keep bots and trolls off your platform,
[02:25:36.640 --> 02:25:40.080]   then you might as well decide this is a politically dangerous place where it is better for us
[02:25:40.080 --> 02:25:41.600]   to quote, quote, quote, be neutral.
[02:25:41.600 --> 02:25:43.720]   From my perspective, neutrality is not being neutral.
[02:25:43.720 --> 02:25:45.040]   You were choosing the side.
[02:25:45.040 --> 02:25:47.480]   You're choosing the side of the trolls and the bad actors.
[02:25:47.480 --> 02:25:50.200]   But I don't think that's how they see it necessarily inside the company.
[02:25:50.200 --> 02:25:51.200]   What is often the case?
[02:25:51.200 --> 02:25:53.720]   I just kind of say something about that and add on to it.
[02:25:53.720 --> 02:25:58.880]   I think this is why it's so important that those of us that work in the political sphere,
[02:25:58.880 --> 02:26:04.040]   I think is never been more important for us to constantly reiterate that our highest
[02:26:04.040 --> 02:26:10.400]   principle here is democracy, free and fair elections, democracy, democracy, democracy.
[02:26:10.400 --> 02:26:15.840]   Every single time I get a chance nowadays to go talk to a Republican on a show, I really
[02:26:15.840 --> 02:26:19.440]   try to make that the point of view that we're coming back around and agreeing on because
[02:26:19.440 --> 02:26:21.200]   you're absolutely correct.
[02:26:21.200 --> 02:26:22.480]   This has been weaponized.
[02:26:22.480 --> 02:26:26.400]   I really think it's terribly bad for the country.
[02:26:26.400 --> 02:26:30.440]   If we agree on nothing else, we should agree their elections are fair.
[02:26:30.440 --> 02:26:31.440]   Do they agree?
[02:26:31.440 --> 02:26:33.200]   Brianna, do they agree?
[02:26:33.200 --> 02:26:37.840]   There's no one out loud is going to say, oh, no, I don't believe in democracy.
[02:26:37.840 --> 02:26:43.360]   This shocked me when I studied years ago, studied and I'm not making God was rule about
[02:26:43.360 --> 02:26:47.760]   to be hit when I studied World War II Germany.
[02:26:47.760 --> 02:26:48.960]   Democracy was an enemy.
[02:26:48.960 --> 02:26:50.880]   Democracy was something that you did not want.
[02:26:50.880 --> 02:26:51.880]   It was set out loud.
[02:26:51.880 --> 02:26:53.200]   That was the point.
[02:26:53.200 --> 02:27:01.440]   On certain places I think in this country now, we're getting to that point where major public
[02:27:01.440 --> 02:27:07.360]   radio station did some research recently and they found their audience that the word democracy
[02:27:07.360 --> 02:27:10.120]   is a turn off.
[02:27:10.120 --> 02:27:12.080]   I hear what you're saying.
[02:27:12.080 --> 02:27:18.080]   I think that is why it's really up to us that are reasonable actors in this space in both
[02:27:18.080 --> 02:27:24.160]   parties to constantly hammer this point home because I think you're right, there are some
[02:27:24.160 --> 02:27:30.320]   people that are so frustrated they would throw all of this out the window and I think that
[02:27:30.320 --> 02:27:34.160]   it's really important for the adults to model different kinds of behavior here.
[02:27:34.160 --> 02:27:38.880]   There's also a kind of a weasel word that people who want to eliminate democracy use
[02:27:38.880 --> 02:27:45.440]   called a liberalism where they say, well, this is a we're in a governing system that hides
[02:27:45.440 --> 02:27:51.840]   its non-democratic practices behind formally democratic institutions and procedures.
[02:27:51.840 --> 02:27:55.440]   This is a great conspiracy theory where you say, well, it looks democratic and I'm all
[02:27:55.440 --> 02:28:00.480]   for real democracy but we don't live in a real democracy.
[02:28:00.480 --> 02:28:05.000]   We live in an illiberal democracy and that's one way to get around.
[02:28:05.000 --> 02:28:08.040]   I'm sorry, I shouldn't provide them with weapons, Brianna, but that's one way to get
[02:28:08.040 --> 02:28:09.040]   around.
[02:28:09.040 --> 02:28:11.720]   That's something around what you're proposing.
[02:28:11.720 --> 02:28:17.600]   You wouldn't think anybody be against democracy but this way they can say that.
[02:28:17.600 --> 02:28:20.080]   They also couch it in.
[02:28:20.080 --> 02:28:24.480]   Do you want, when people talk about direct election of the president and the national
[02:28:24.480 --> 02:28:29.400]   popular vote compact, well, then you have California and New York making all the decisions.
[02:28:29.400 --> 02:28:37.200]   They couch it in these very not so secret racial terms of these horrible places.
[02:28:37.200 --> 02:28:42.000]   People actually live.
[02:28:42.000 --> 02:28:47.720]   We only have one senator right now for 47 million people.
[02:28:47.720 --> 02:28:49.240]   That's a different kind of worm still.
[02:28:49.240 --> 02:28:54.700]   Oh Lord, let's take a break and we'll get back with a final thoughts with a sterling
[02:28:54.700 --> 02:29:00.360]   sterling panel from rebellionpacked.org.
[02:29:00.360 --> 02:29:05.760]   If you want to find out more, the wonderful Brianna, I'm sorry.com.org.
[02:29:05.760 --> 02:29:08.760]   That's it.
[02:29:08.760 --> 02:29:13.720]   She is on Twitter and blue sky and Mastodon is Brianna Wu.
[02:29:13.720 --> 02:29:15.520]   I think you can search for it in all three platforms.
[02:29:15.520 --> 02:29:16.960]   I'm only Brianna Wu.
[02:29:16.960 --> 02:29:19.400]   I'm only Brianna on blue sky.
[02:29:19.400 --> 02:29:21.480]   Oh, thanks for complicating matters.
[02:29:21.480 --> 02:29:22.920]   Yeah, sorry.
[02:29:22.920 --> 02:29:27.320]   Only Brianna, you know, follow me and I'll point you to her.
[02:29:27.320 --> 02:29:30.200]   Also, great to have Alex Stamos here.
[02:29:30.200 --> 02:29:36.440]   He is the director of the Stanford Internet Observatory, Stanford IO, and also a principal
[02:29:36.440 --> 02:29:40.120]   in the Krebs Stamos group.
[02:29:40.120 --> 02:29:43.080]   Great place to go if you're looking for help along these lines.
[02:29:43.080 --> 02:29:45.320]   This is a troubled time.
[02:29:45.320 --> 02:29:50.680]   I feel like if you were a dark horse candidate from 2024 and you listen to this show, you
[02:29:50.680 --> 02:29:57.640]   might see an opportunity to create a campaign out of nothing.
[02:29:57.640 --> 02:29:58.840]   Mike Pence, here he comes.
[02:29:58.840 --> 02:30:02.040]   I'm not saying that Mike Pence should do anything about it.
[02:30:02.040 --> 02:30:09.160]   That's Jeff Jarvis, aka Jacks Javros from buzzmachine.com, our superhero.
[02:30:09.160 --> 02:30:10.160]   Great joke.
[02:30:10.160 --> 02:30:16.280]   Goodmoorprethices.com, our show they brought to you by lookout that one thing the pandemic
[02:30:16.280 --> 02:30:20.840]   has really changed is people are not at the office as much as they used to be, right?
[02:30:20.840 --> 02:30:24.440]   Hybrid work is here, remote work is here, business has changed forever.
[02:30:24.440 --> 02:30:29.280]   The boundaries to where we work or even how we work have disappeared, which means the
[02:30:29.280 --> 02:30:31.600]   boundaries to your data have shifted.
[02:30:31.600 --> 02:30:32.600]   It's always on the move.
[02:30:32.600 --> 02:30:33.600]   It's on a device.
[02:30:33.600 --> 02:30:34.760]   It's in the cloud.
[02:30:34.760 --> 02:30:35.760]   It's across networks.
[02:30:35.760 --> 02:30:37.960]   It's in the local coffee shop.
[02:30:37.960 --> 02:30:39.920]   Now that's nice for your workforce.
[02:30:39.920 --> 02:30:46.840]   It can be a challenge for IT security, especially if IT is struggling with multiple point tools
[02:30:46.840 --> 02:30:51.720]   in a variety of different platforms that are all incompatible and you spend a lot of your
[02:30:51.720 --> 02:30:55.720]   cycles just trying to get stuff to work together, you need lookout.
[02:30:55.720 --> 02:31:00.200]   Lookout helps you control your data and free your workforce with lookout.
[02:31:00.200 --> 02:31:03.120]   You gain complete visibility into all your data.
[02:31:03.120 --> 02:31:06.320]   You can minimize risk from external and internal threats.
[02:31:06.320 --> 02:31:07.480]   You can ensure compliance.
[02:31:07.480 --> 02:31:09.520]   That's getting more and more important.
[02:31:09.520 --> 02:31:13.520]   And by seamlessly securing hybrid work, no matter where your data is, you don't have
[02:31:13.520 --> 02:31:16.480]   to sacrifice productivity for security.
[02:31:16.480 --> 02:31:20.120]   Your IT people will love it because their work is a lot simpler.
[02:31:20.120 --> 02:31:22.440]   They don't have that complex multi-tool environment.
[02:31:22.440 --> 02:31:25.400]   It's a single unified platform.
[02:31:25.400 --> 02:31:30.240]   Lookout reduces IT complexity, giving you more time to focus on whatever else comes your
[02:31:30.240 --> 02:31:31.240]   way.
[02:31:31.240 --> 02:31:32.240]   Good data protection.
[02:31:32.240 --> 02:31:33.960]   It does not have to be a cage.
[02:31:33.960 --> 02:31:38.320]   It can be a springboard letting you and your organization bound toward a future of your
[02:31:38.320 --> 02:31:39.320]   making.
[02:31:39.320 --> 02:31:46.920]   Visit lookout.com today to learn how to safeguard data, secure hybrid work, and reduce IT complexity.
[02:31:46.920 --> 02:31:47.920]   Lookout.com.
[02:31:47.920 --> 02:31:51.280]   Thank you for supporting our show lookout.
[02:31:51.280 --> 02:31:52.720]   We appreciate it.
[02:31:52.720 --> 02:31:54.160]   And you check it out.
[02:31:54.160 --> 02:31:55.160]   Lookout.com.
[02:31:55.160 --> 02:32:04.680]   Well, there's a bunch of little things Supreme Court has rejected a computer scientist lawsuit.
[02:32:04.680 --> 02:32:11.560]   Oh, before I do that, they are telling me that I should probably mention we have made
[02:32:11.560 --> 02:32:17.760]   a is this going to be AI Leo again, a fabulous promotional announcement with my
[02:32:17.760 --> 02:32:21.080]   alter ego watch.
[02:32:21.080 --> 02:32:22.080]   And don't be sentiment.
[02:32:22.080 --> 02:32:25.480]   These little blue lights that go down the stairway and I can't not see them.
[02:32:25.480 --> 02:32:28.840]   I spend so much time energy not looking at them that I don't see the movie anymore.
[02:32:28.840 --> 02:32:29.840]   And I understand that.
[02:32:29.840 --> 02:32:33.960]   Once I took ayahuasca, I could not avoid seeing the machine elves everywhere.
[02:32:33.960 --> 02:32:34.960]   So it happens.
[02:32:34.960 --> 02:32:37.680]   And once you once once seen, you can't unsee it.
[02:32:37.680 --> 02:32:39.200]   I'm not saying it back.
[02:32:39.200 --> 02:32:44.320]   Hi, I'm a she know previously on Twitter.
[02:32:44.320 --> 02:32:50.440]   News Weekly start with the protecting kids on social media act, which is here to protect
[02:32:50.440 --> 02:32:52.600]   kids from social media, apparently.
[02:32:52.600 --> 02:32:54.720]   The basic thing is it's age verification.
[02:32:54.720 --> 02:32:57.440]   They want parents to verify age of teenage users.
[02:32:57.440 --> 02:33:01.400]   The big difference with this as opposed to many other of the state level bills is that
[02:33:01.400 --> 02:33:06.160]   they create an age verification system through it or they create the thing that will create
[02:33:06.160 --> 02:33:07.160]   it.
[02:33:07.160 --> 02:33:08.160]   All about Android.
[02:33:08.160 --> 02:33:11.280]   Google authenticators been around since 2013, I believe.
[02:33:11.280 --> 02:33:12.600]   So a very long time.
[02:33:12.600 --> 02:33:18.200]   Well they finally added it to factor authentication codes now sync with the Google account.
[02:33:18.200 --> 02:33:22.800]   When you set up a new device, once you log into that new device, the authenticator will
[02:33:22.800 --> 02:33:26.440]   be automatically set up with your account.
[02:33:26.440 --> 02:33:33.320]   Mac Break Weekly, where some Utah college kids essayed a very challenging hike that got
[02:33:33.320 --> 02:33:36.360]   them to a canyon they could not get out of.
[02:33:36.360 --> 02:33:43.240]   They didn't have a cell signal they had iPhones and were able to use their iPhones to save
[02:33:43.240 --> 02:33:44.240]   themselves.
[02:33:44.240 --> 02:33:45.760]   That's a really great story.
[02:33:45.760 --> 02:33:47.480]   I'm sure Apple will turn it into a commercial.
[02:33:47.480 --> 02:33:48.480]   Twit.
[02:33:48.480 --> 02:33:49.480]   Those poor kids.
[02:33:49.480 --> 02:33:51.320]   They're going to have to go down into the pit again.
[02:33:51.320 --> 02:33:57.960]   That's the only bad thing to do the end.
[02:33:57.960 --> 02:33:58.960]   Hello.
[02:33:58.960 --> 02:34:04.880]   The video is stuck down here with the machine elves and no one brought their iPhone.
[02:34:04.880 --> 02:34:13.640]   We have, by the way, we have an AI Leo in our Discord and I have been informed that
[02:34:13.640 --> 02:34:18.600]   AI Leo, if you mention him in our Discord, will automatically heart you.
[02:34:18.600 --> 02:34:25.520]   So he's kind of starting to get his own little own little world of its own.
[02:34:25.520 --> 02:34:28.240]   Can you take Iowaska Leo?
[02:34:28.240 --> 02:34:29.240]   No.
[02:34:29.240 --> 02:34:34.680]   Oh, but I have friends who have and they insist the machine elves are all around us.
[02:34:34.680 --> 02:34:39.120]   All right.
[02:34:39.120 --> 02:34:43.560]   Supreme Court back to the Supreme Court rejects computer scientists lawsuit over AI generated
[02:34:43.560 --> 02:34:45.640]   inventions.
[02:34:45.640 --> 02:34:52.520]   The trademark office refuses to issue patents for inventions that Steven Thawler's he's
[02:34:52.520 --> 02:34:55.400]   a computer scientist artificial intelligence system created.
[02:34:55.400 --> 02:34:59.440]   He sued the justices said, no, no, the lower court was correct.
[02:34:59.440 --> 02:35:02.760]   You can't or no, I'm sorry, the patent and trademark office was correct.
[02:35:02.760 --> 02:35:05.240]   You cannot patent that.
[02:35:05.240 --> 02:35:07.560]   So I guess that's a victory for humans.
[02:35:07.560 --> 02:35:08.560]   Yeah.
[02:35:08.560 --> 02:35:09.560]   I don't know.
[02:35:09.560 --> 02:35:10.560]   Supreme Court.
[02:35:10.560 --> 02:35:12.280]   I mean, this is what my husband does.
[02:35:12.280 --> 02:35:17.120]   So that that sounds very much in line with patent law and, you know, how novelty works.
[02:35:17.120 --> 02:35:22.240]   Like they've traditionally really frowned at computer generated algorithms ending up
[02:35:22.240 --> 02:35:23.240]   with patents.
[02:35:23.240 --> 02:35:27.960]   Well, and they're kind of agreeing with a Jaren Lanier that it's really just a generative
[02:35:27.960 --> 02:35:30.320]   based on other things humans have done.
[02:35:30.320 --> 02:35:32.480]   So it doesn't, it isn't novel in that sense.
[02:35:32.480 --> 02:35:33.480]   Yeah.
[02:35:33.480 --> 02:35:38.920]   It doesn't raise the novel, the novelty and the creativity argument of that.
[02:35:38.920 --> 02:35:39.920]   Yeah.
[02:35:39.920 --> 02:35:40.920]   Yeah.
[02:35:40.920 --> 02:35:41.920]   Yeah.
[02:35:41.920 --> 02:35:44.720]   The Supreme Court will be reviewing and this is going to be more interesting.
[02:35:44.720 --> 02:35:48.680]   They agreed on Monday to consider whether the First Amendment bars government officials
[02:35:48.680 --> 02:35:53.040]   from blocking critics on social platforms like Facebook and Twitter.
[02:35:53.040 --> 02:35:57.800]   And what's weird is I thought this had been decided already with Trump back in 2021.
[02:35:57.800 --> 02:36:02.080]   It turns out that it was mooted because Trump got out of office.
[02:36:02.080 --> 02:36:10.320]   So the Supreme Court did not have to decide that Trump had blocked some people on Twitter.
[02:36:10.320 --> 02:36:15.480]   A lower court said that violated their free speech rights and he had to unblock them.
[02:36:15.480 --> 02:36:17.000]   Supreme Court never did decide that.
[02:36:17.000 --> 02:36:22.040]   So now they're going to have a chance to decide that they've got actually two competing decisions
[02:36:22.040 --> 02:36:23.760]   from lower courts.
[02:36:23.760 --> 02:36:26.040]   One said, yes, one said no.
[02:36:26.040 --> 02:36:27.040]   So we'll see.
[02:36:27.040 --> 02:36:28.880]   We'll see where they go with this one.
[02:36:28.880 --> 02:36:33.200]   I mean, this is interesting is it's actually about school district level folks.
[02:36:33.200 --> 02:36:34.520]   You're not talking about the president.
[02:36:34.520 --> 02:36:35.520]   Local.
[02:36:35.520 --> 02:36:36.520]   And maybe.
[02:36:36.520 --> 02:36:37.520]   Yeah.
[02:36:37.520 --> 02:36:42.840]   I actually, I hope they come up with something a little more nuanced where they allow people
[02:36:42.840 --> 02:36:44.680]   to block on their personal accounts.
[02:36:44.680 --> 02:36:50.280]   But perhaps if you have like an official account, you can't because you kind of abuse people
[02:36:50.280 --> 02:36:54.240]   get if they're just on the local city council or these days if they're on the school board
[02:36:54.240 --> 02:36:57.480]   is spectacular and I can't, they should not have to deal with abuse.
[02:36:57.480 --> 02:37:01.680]   There needs to be a way to handle abuse without blocking legitimate feedback.
[02:37:01.680 --> 02:37:02.680]   Right.
[02:37:02.680 --> 02:37:05.600]   Whether you agree with it or not, it's feedback.
[02:37:05.600 --> 02:37:07.520]   So yeah, this is a, yeah, that's good.
[02:37:07.520 --> 02:37:12.000]   This is an appropriate thing for the Supreme Court to look at, I guess.
[02:37:12.000 --> 02:37:13.480]   Yeah.
[02:37:13.480 --> 02:37:15.000]   What else?
[02:37:15.000 --> 02:37:19.080]   Just kind of trying to have so many stories.
[02:37:19.080 --> 02:37:21.320]   Alex, you put in a couple of stories.
[02:37:21.320 --> 02:37:25.880]   One is that Twitter is complying with more government demands under Elon Musk than they
[02:37:25.880 --> 02:37:26.880]   did.
[02:37:26.880 --> 02:37:27.880]   Good thing.
[02:37:27.880 --> 02:37:32.560]   No, no, this is more demands for both data and for censorship in the numbers have gone
[02:37:32.560 --> 02:37:37.280]   up, especially in India, which is, you know, something I've, I've pointed out from the
[02:37:37.280 --> 02:37:41.680]   beginning is you might not know this, but Elon Musk owns a car company as well.
[02:37:41.680 --> 02:37:44.920]   And that car company really wants to sell those cars in India.
[02:37:44.920 --> 02:37:46.280]   Now the world's most populous.
[02:37:46.280 --> 02:37:48.000]   And China is a big market forum too.
[02:37:48.000 --> 02:37:49.680]   And China is a huge market for him.
[02:37:49.680 --> 02:37:53.360]   And so, you know, you end up with a situation where they have a huge amount leverage over
[02:37:53.360 --> 02:37:55.760]   him for his other purposes.
[02:37:55.760 --> 02:38:00.000]   Imagine if Mark Zuckerberg, all of his money was actually tied up like in a Chinese pharmaceutical
[02:38:00.000 --> 02:38:01.000]   company, right?
[02:38:01.000 --> 02:38:04.400]   Like, that's effectively what you've got here.
[02:38:04.400 --> 02:38:09.640]   And you know, Twitter, despite all the talk about free speech has actually complied in
[02:38:09.640 --> 02:38:13.760]   a much larger number of times of both turning over data and censorship on behalf of a number
[02:38:13.760 --> 02:38:14.760]   of countries.
[02:38:14.760 --> 02:38:18.880]   And India is the most interesting one here because India is going into an election year
[02:38:18.880 --> 02:38:20.600]   as well in 2024.
[02:38:20.600 --> 02:38:27.200]   Modi is consolidating power using a variety of different legal means and whether or not
[02:38:27.200 --> 02:38:30.920]   American companies allow that to happen on their platforms is something they're going
[02:38:30.920 --> 02:38:32.440]   to have to really consider.
[02:38:32.440 --> 02:38:34.040]   Obviously there's huge downside.
[02:38:34.040 --> 02:38:35.840]   He already blocked TikTok.
[02:38:35.840 --> 02:38:40.160]   So he has demonstrated that he is willing to take very aggressive action against companies
[02:38:40.160 --> 02:38:41.160]   that may come angry.
[02:38:41.160 --> 02:38:43.440]   And it looks like Twitter has heard that message.
[02:38:43.440 --> 02:38:45.000]   Also, Erdogan and Turkey.
[02:38:45.000 --> 02:38:46.000]   Erdogan and Turkey.
[02:38:46.000 --> 02:38:47.000]   Yeah.
[02:38:47.000 --> 02:38:48.000]   Obviously Putin.
[02:38:48.000 --> 02:38:50.880]   China is not that relevant because China actually just blocks these entire platforms.
[02:38:50.880 --> 02:38:51.880]   The whole things block.
[02:38:51.880 --> 02:38:52.880]   The whole things block.
[02:38:52.880 --> 02:38:57.000]   But it's really interesting as we move towards a more authoritarian governments all over the
[02:38:57.000 --> 02:38:58.000]   world.
[02:38:58.000 --> 02:38:59.000]   There is going to be more and more pressure.
[02:38:59.000 --> 02:39:03.840]   In fact, it's not merely that Twitter has not refused any of these requests, but they're
[02:39:03.840 --> 02:39:07.040]   getting a lot more than they ever got before since Elon took over.
[02:39:07.040 --> 02:39:09.400]   Yes, they're getting more requests.
[02:39:09.400 --> 02:39:12.000]   I think people see it as a new opening.
[02:39:12.000 --> 02:39:13.800]   Countries see it as a new opening.
[02:39:13.800 --> 02:39:17.800]   Twitter has a long history of pushing back against government requests for data and for
[02:39:17.800 --> 02:39:18.800]   censorship.
[02:39:18.800 --> 02:39:22.040]   They fought this over and over again around the world.
[02:39:22.040 --> 02:39:25.320]   And all the people that used to fight that have apparently been fun.
[02:39:25.320 --> 02:39:27.400]   So it's worth adding.
[02:39:27.400 --> 02:39:29.120]   This is probably not ideological.
[02:39:29.120 --> 02:39:35.320]   I mean, it's very expensive to hire lawyers to do all this work to stand up to this kind
[02:39:35.320 --> 02:39:37.640]   of government demand for data.
[02:39:37.640 --> 02:39:43.520]   I look at when the Twitter files were released, I do believe Twitter's chief counsel quit.
[02:39:43.520 --> 02:39:46.840]   I know they've lost legal resources across the board.
[02:39:46.840 --> 02:39:52.480]   And you can say a lot to criticize Apple, but they sure went toe to toe with the FBI and
[02:39:52.480 --> 02:39:56.960]   dedicated like they put both their reputation and a lot of money on the line.
[02:39:56.960 --> 02:39:59.600]   They're doing the right thing for user privacy here.
[02:39:59.600 --> 02:40:05.920]   And I think when you have a steward like Elon Musk, who his bottom line is not to any principle.
[02:40:05.920 --> 02:40:08.000]   It's to stop Twitter from hemorrhaging money.
[02:40:08.000 --> 02:40:10.320]   I think it's entirely unsurprising.
[02:40:10.320 --> 02:40:11.320]   This is the outcome we're saying.
[02:40:11.320 --> 02:40:13.800]   He's also a bit of an authoritarian though, don't you?
[02:40:13.800 --> 02:40:16.680]   Fair, fair, fair, fair.
[02:40:16.680 --> 02:40:22.720]   I would think in this case, I personally would not attribute this to a conscious decision,
[02:40:22.720 --> 02:40:27.400]   but rather him just not caring about the details, which is a pattern that's happened
[02:40:27.400 --> 02:40:31.440]   to his companies for a long time.
[02:40:31.440 --> 02:40:33.040]   Let's end with a happy note.
[02:40:33.040 --> 02:40:35.080]   I know I had a really dig for this one.
[02:40:35.080 --> 02:40:38.280]   This is the wrong panel for a happy note.
[02:40:38.280 --> 02:40:41.480]   I feel like it's a cherry panel.
[02:40:41.480 --> 02:40:45.880]   No, this is a realistic panel, a realistic panel.
[02:40:45.880 --> 02:40:46.880]   How about this?
[02:40:46.880 --> 02:40:52.920]   It was this day, 30 years ago, that CERN, which owned the copyright to the World Wide
[02:40:52.920 --> 02:40:59.680]   Web because Tim Berners-Lea physicist at the Swiss Particle Accelerator Lab, invented
[02:40:59.680 --> 02:41:08.000]   and released the World Wide Web, was on this day that CERN said, "Okay, it's public domain.
[02:41:08.000 --> 02:41:10.720]   Let everybody develop web pages."
[02:41:10.720 --> 02:41:19.240]   And literally approved this April 30th in 1993.
[02:41:19.240 --> 02:41:21.280]   This is the day the web was born.
[02:41:21.280 --> 02:41:24.080]   Happy birthday, World Wide Web.
[02:41:24.080 --> 02:41:25.080]   Happy birthday.
[02:41:25.080 --> 02:41:26.920]   You have been an unmitigated success.
[02:41:26.920 --> 02:41:28.920]   It doesn't seem bad.
[02:41:28.920 --> 02:41:32.320]   It's a bad thing because it worked out really well.
[02:41:32.320 --> 02:41:33.320]   Oh, come on.
[02:41:33.320 --> 02:41:34.320]   It's a PDF part.
[02:41:34.320 --> 02:41:35.320]   Oh, yeah.
[02:41:35.320 --> 02:41:38.560]   You didn't know what the spire that you're going to extend was that the University of
[02:41:38.560 --> 02:41:42.000]   Minnesota was going to try to basically charge for gopher.
[02:41:42.000 --> 02:41:44.160]   Oh, you're kidding.
[02:41:44.160 --> 02:41:49.440]   And Berners-Lea thought that was ridiculous and said, "No, no, no, no."
[02:41:49.440 --> 02:41:52.400]   And so the contrast created it.
[02:41:52.400 --> 02:41:57.080]   So it was on this day that CERN gave it all up, no conditions.
[02:41:57.080 --> 02:41:58.080]   Wow.
[02:41:58.080 --> 02:41:59.080]   Well, thank you.
[02:41:59.080 --> 02:42:01.680]   I remember my CS course the year this happened.
[02:42:01.680 --> 02:42:08.200]   I was taking one at the college level because there was obviously no computer science program
[02:42:08.200 --> 02:42:10.440]   at my high school in Mississippi.
[02:42:10.440 --> 02:42:16.120]   And our final when this happened was to create your own web page.
[02:42:16.120 --> 02:42:21.160]   I remember staying up all night trying to figure out how to create my own web page.
[02:42:21.160 --> 02:42:22.160]   It was Voltron.
[02:42:22.160 --> 02:42:24.240]   It was a bunch of pictures of Voltron.
[02:42:24.240 --> 02:42:25.240]   It was so bad.
[02:42:25.240 --> 02:42:26.240]   How old were you?
[02:42:26.240 --> 02:42:27.240]   But I did it.
[02:42:27.240 --> 02:42:31.240]   Oh, gosh, I would have been like 15 or 16.
[02:42:31.240 --> 02:42:33.640]   Yeah, that's awesome.
[02:42:33.640 --> 02:42:34.640]   That's awesome.
[02:42:34.640 --> 02:42:37.280]   The first real use of the World Wide Web.
[02:42:37.280 --> 02:42:38.280]   A Voltron.
[02:42:38.280 --> 02:42:41.120]   What I love too is in the CERN document.
[02:42:41.120 --> 02:42:45.920]   In no event will CERN be liable to anyone for any damages arising out of the use of
[02:42:45.920 --> 02:42:46.920]   this software.
[02:42:46.920 --> 02:42:49.120]   What a fault, man.
[02:42:49.120 --> 02:42:51.120]   See, they knew.
[02:42:51.120 --> 02:42:52.880]   They had a premonition.
[02:42:52.880 --> 02:42:53.880]   They knew.
[02:42:53.880 --> 02:42:59.120]   Well, as Jeff will say again and again, there's been a lot of good that's come out of the
[02:42:59.120 --> 02:43:00.120]   World Wide Web.
[02:43:00.120 --> 02:43:03.560]   Frankly, there's a lot of good that's come out of the Twitter and Facebook and all of
[02:43:03.560 --> 02:43:08.280]   those places, and so it's easy to focus on the problems, but it's also important to
[02:43:08.280 --> 02:43:11.200]   remember that even Wikipedia has its uses.
[02:43:11.200 --> 02:43:14.120]   Hey, it's really great to have you.
[02:43:14.120 --> 02:43:16.440]   Brianna Wu, rebellionpack.com.
[02:43:16.440 --> 02:43:19.680]   Anything else you would like to plug?
[02:43:19.680 --> 02:43:21.640]   How's your speed running going?
[02:43:21.640 --> 02:43:23.320]   I'm not doing much speed running.
[02:43:23.320 --> 02:43:27.080]   I want to give out a shout out to the Dedham Police Department.
[02:43:27.080 --> 02:43:28.080]   Oh, yeah.
[02:43:28.080 --> 02:43:29.720]   I saw that picture.
[02:43:29.720 --> 02:43:36.520]   I got, I got swatted last week and this often goes really, really wrong.
[02:43:36.520 --> 02:43:42.040]   They did literally everything correctly that you could ask law enforcement to do in this
[02:43:42.040 --> 02:43:43.040]   situation.
[02:43:43.040 --> 02:43:44.680]   Yeah, I think that's really cool.
[02:43:44.680 --> 02:43:48.960]   Like, Dedham's not the biggest town and they handled this extremely skillful.
[02:43:48.960 --> 02:43:51.200]   Did they know about you ahead of time?
[02:43:51.200 --> 02:43:52.760]   I mean, were they?
[02:43:52.760 --> 02:43:56.160]   I mean, I ran for Congress, so I assume they kind of got it.
[02:43:56.160 --> 02:43:57.160]   Yeah.
[02:43:57.160 --> 02:44:00.760]   I don't like a bulletin board where you might see this and it might be a spot.
[02:44:00.760 --> 02:44:03.160]   It's a full bit and a few times for speeding too, so.
[02:44:03.160 --> 02:44:05.160]   Well, I knew about you from that too.
[02:44:05.160 --> 02:44:06.160]   Yeah.
[02:44:06.160 --> 02:44:07.240]   But no, I mean, I'm not.
[02:44:07.240 --> 02:44:09.400]   That woman driving the Boxster, you know her.
[02:44:09.400 --> 02:44:10.400]   Well done, you know.
[02:44:10.400 --> 02:44:12.120]   Oh, geez, she's a good person.
[02:44:12.120 --> 02:44:13.120]   Hard.
[02:44:13.120 --> 02:44:15.120]   Normally doesn't go very fast.
[02:44:15.120 --> 02:44:16.120]   Yeah.
[02:44:16.120 --> 02:44:17.880]   Well, yeah, good on them.
[02:44:17.880 --> 02:44:23.640]   We've been swatted once too and a law enforcement department that doesn't come in with tanks
[02:44:23.640 --> 02:44:26.680]   and guns ablazing is a blessing in a case like that.
[02:44:26.680 --> 02:44:28.280]   Yeah, I really appreciate it.
[02:44:28.280 --> 02:44:31.680]   Brianna, what's the, what's the quick tip to law enforcement agencies out there about
[02:44:31.680 --> 02:44:32.920]   what they should do?
[02:44:32.920 --> 02:44:37.760]   So the number one thing is if you're getting the claim through an email address that you
[02:44:37.760 --> 02:44:43.200]   cannot verify, most police departments have tools to, you know, look up your Google address
[02:44:43.200 --> 02:44:46.560]   or something like that, which is a whole privacy discussion we can have.
[02:44:46.560 --> 02:44:51.800]   But, you know, ultimately your Google, like a Gmail account is tied to a database and
[02:44:51.800 --> 02:44:56.160]   they can get a sense of who you are often for swatting.
[02:44:56.160 --> 02:44:58.920]   They use anonymous services like proton mint.
[02:44:58.920 --> 02:45:02.680]   So if this happens, that's pretty much a giveaway, isn't it?
[02:45:02.680 --> 02:45:03.680]   Yeah.
[02:45:03.680 --> 02:45:04.680]   Right.
[02:45:04.680 --> 02:45:05.680]   You should be very suspicious about that.
[02:45:05.680 --> 02:45:09.400]   You know, I would say if your major target out there, like I was unsurprised that I got
[02:45:09.400 --> 02:45:10.400]   swatted.
[02:45:10.400 --> 02:45:14.400]   Yeah, I had reached out to the police chief in town and had talked to them about some
[02:45:14.400 --> 02:45:18.720]   of the threats I had ahead of time, which I don't know if it made a difference with
[02:45:18.720 --> 02:45:19.720]   certain.
[02:45:19.720 --> 02:45:20.720]   It's more on your behalf.
[02:45:20.720 --> 02:45:24.280]   Just be proactive in police departments.
[02:45:24.280 --> 02:45:29.120]   They really need training about frankly being more skeptical in these situations.
[02:45:29.120 --> 02:45:30.120]   Yeah.
[02:45:30.120 --> 02:45:34.960]   Here's the tweet and a picture of the very nice people at the Dedham police department.
[02:45:34.960 --> 02:45:35.960]   Police department.
[02:45:35.960 --> 02:45:36.960]   Well done.
[02:45:36.960 --> 02:45:39.120]   That's all five of their police cars.
[02:45:39.120 --> 02:45:42.960]   So it helps that they don't have tanks.
[02:45:42.960 --> 02:45:45.760]   You know, that makes a little bit of a difference.
[02:45:45.760 --> 02:45:50.560]   Yeah, when we're, see, it's kind of lazy to swat somebody by email.
[02:45:50.560 --> 02:45:53.920]   It doesn't seem like they really, their heart is in it.
[02:45:53.920 --> 02:45:55.920]   We were swatted with a phone call.
[02:45:55.920 --> 02:45:59.400]   And so it's a little more credible than somebody calls and says, well, I'm not sure I should
[02:45:59.400 --> 02:46:00.680]   give them a roadmap for that.
[02:46:00.680 --> 02:46:03.000]   But no, thanks to the pedal in the police.
[02:46:03.000 --> 02:46:06.680]   I think they realized that there was although we did have to leave the building and have
[02:46:06.680 --> 02:46:08.720]   a bomb dogs sweep it.
[02:46:08.720 --> 02:46:10.760]   Oh, I'm sorry.
[02:46:10.760 --> 02:46:11.760]   Yeah.
[02:46:11.760 --> 02:46:12.760]   Yeah.
[02:46:12.760 --> 02:46:17.760]   I had to get the bomb dogs from Marin.
[02:46:17.760 --> 02:46:20.240]   It took a little while.
[02:46:20.240 --> 02:46:23.520]   Only chicken dogs and bomb chickens.
[02:46:23.520 --> 02:46:25.120]   I'm glad you're okay, Brianna.
[02:46:25.120 --> 02:46:26.120]   Oh, I'm fine.
[02:46:26.120 --> 02:46:28.320]   And yeah, shout out to the Dedham PD.
[02:46:28.320 --> 02:46:30.760]   I think it's a good thing that Petaluma doesn't have bomb dogs.
[02:46:30.760 --> 02:46:31.760]   Yeah.
[02:46:31.760 --> 02:46:33.640]   I remember my admit weekend for Cal.
[02:46:33.640 --> 02:46:37.800]   They brought out that the UCP was very proud that they're one, the only university police
[02:46:37.800 --> 02:46:39.200]   department to have a bomb robot.
[02:46:39.200 --> 02:46:40.200]   Yeah.
[02:46:40.200 --> 02:46:41.200]   Thanks to the Unibombers at the time.
[02:46:41.200 --> 02:46:42.200]   That's something to be a problem of.
[02:46:42.200 --> 02:46:44.200]   I was like, no, I don't see that as a positive.
[02:46:44.200 --> 02:46:45.200]   Wonderful.
[02:46:45.200 --> 02:46:46.200]   So that's good.
[02:46:46.200 --> 02:46:47.200]   Yeah.
[02:46:47.200 --> 02:46:48.200]   Yeah.
[02:46:48.200 --> 02:46:53.120]   Alex, same as you are a really a huge asset to the community.
[02:46:53.120 --> 02:46:55.280]   We're very grateful when you spend time.
[02:46:55.280 --> 02:46:59.480]   Came all the way down for Sacramento, missed the second half of the Warriors Kings game.
[02:46:59.480 --> 02:47:00.920]   Just to be here, I really am grateful.
[02:47:00.920 --> 02:47:01.920]   Thank you.
[02:47:01.920 --> 02:47:03.760]   Well, as far as I know Sacramento won, that's going to be my.
[02:47:03.760 --> 02:47:04.760]   That's good news.
[02:47:04.760 --> 02:47:06.760]   That's going to be my internal monologue here.
[02:47:06.760 --> 02:47:08.280]   I don't have to check.
[02:47:08.280 --> 02:47:10.840]   Stanford in an observatory Stanford IO.
[02:47:10.840 --> 02:47:13.640]   If you're lucky enough to be a Stanford student, take some classes.
[02:47:13.640 --> 02:47:14.640]   Yeah, T.T.
[02:47:14.640 --> 02:47:16.640]   Prost and safety now, it's too late to sign up.
[02:47:16.640 --> 02:47:19.160]   I'd love to have you down on June 7th.
[02:47:19.160 --> 02:47:25.080]   My students, 36 of project teams, build these trust and safety response.
[02:47:25.080 --> 02:47:26.480]   Oh, let's cover it.
[02:47:26.480 --> 02:47:27.480]   Oh, we'll bring it.
[02:47:27.480 --> 02:47:28.480]   I would.
[02:47:28.480 --> 02:47:29.480]   I would.
[02:47:29.480 --> 02:47:30.480]   I would have to be a guest judge if you want.
[02:47:30.480 --> 02:47:31.480]   Oh, I'll be a judge too.
[02:47:31.480 --> 02:47:32.480]   I think that would be fun to cover.
[02:47:32.480 --> 02:47:33.480]   Yeah.
[02:47:33.480 --> 02:47:34.480]   Fascinating.
[02:47:34.480 --> 02:47:37.520]   So we do it on Discord because Discord doesn't catch any of the Trust Safety problems.
[02:47:37.520 --> 02:47:38.520]   So you can.
[02:47:38.520 --> 02:47:40.240]   Oh, so it's easy to create them on Discord.
[02:47:40.240 --> 02:47:41.240]   Yeah, it's what you're saying.
[02:47:41.240 --> 02:47:42.240]   Right.
[02:47:42.240 --> 02:47:44.400]   You don't have to worry about Discord catching anything.
[02:47:44.400 --> 02:47:45.400]   That's true.
[02:47:45.400 --> 02:47:49.080]   But if I'm going to plug something, it'll be with my colleague at Stanford, Evelyn Dewek,
[02:47:49.080 --> 02:47:50.080]   who's a law professor.
[02:47:50.080 --> 02:47:55.120]   We have a podcast called moderated content every Monday about this kind of stuff.
[02:47:55.120 --> 02:47:58.400]   So it is it is a brilliant sandwich.
[02:47:58.400 --> 02:48:00.120]   A brilliant sandwich.
[02:48:00.120 --> 02:48:01.120]   Yum yum.
[02:48:01.120 --> 02:48:06.000]   Moderated content wherever finer podcasts are aggregated.
[02:48:06.000 --> 02:48:07.640]   You can get it on iTunes or Spotify.
[02:48:07.640 --> 02:48:10.200]   There's an RSS feed as well.
[02:48:10.200 --> 02:48:12.600]   So if you like this kind of conversation, I like TikTok.
[02:48:12.600 --> 02:48:13.680]   Boom, that's good.
[02:48:13.680 --> 02:48:16.520]   You got some good titles in there.
[02:48:16.520 --> 02:48:17.520]   That's great.
[02:48:17.520 --> 02:48:19.600]   We had pretty good timing on creating this podcast.
[02:48:19.600 --> 02:48:20.600]   I kind of got.
[02:48:20.600 --> 02:48:21.600]   Yeah, no kidding.
[02:48:21.600 --> 02:48:22.600]   No kidding.
[02:48:22.600 --> 02:48:23.600]   Moderated content.
[02:48:23.600 --> 02:48:26.000]   And of course, probably don't need a plug.
[02:48:26.000 --> 02:48:30.840]   Cribs, Damos group is the place to go to get help if you need it.
[02:48:30.840 --> 02:48:36.280]   Yeah, because a lot of people get their enterprise management consulting tips from podcasts.
[02:48:36.280 --> 02:48:37.280]   So.
[02:48:37.280 --> 02:48:41.160]   Should we do KSI group slash twit and should I put something there?
[02:48:41.160 --> 02:48:42.160]   Yeah, sure.
[02:48:42.160 --> 02:48:44.000]   Yeah, maybe that would be the thing to do.
[02:48:44.000 --> 02:48:50.000]   No, actually just go to Ks.group and you don't need a referral code.
[02:48:50.000 --> 02:48:52.440]   Biddling a better, safer, more secure technological future.
[02:48:52.440 --> 02:48:54.320]   I think we're in favor.
[02:48:54.320 --> 02:48:55.320]   We're in favor.
[02:48:55.320 --> 02:48:57.200]   We're behind you on that.
[02:48:57.200 --> 02:48:58.200]   That's important stuff.
[02:48:58.200 --> 02:48:59.200]   Thanks.
[02:48:59.200 --> 02:49:03.480]   Jeff Jarvis, I want to see you this Wednesday on this week in Google.
[02:49:03.480 --> 02:49:06.240]   He's at buzzmachine.com and his book's coming out in June.
[02:49:06.240 --> 02:49:10.600]   Gutenberg parenthesis.com to pre-order it.
[02:49:10.600 --> 02:49:13.480]   And of course, you teach a few classes too in these kinds of things.
[02:49:13.480 --> 02:49:14.480]   No one again.
[02:49:14.480 --> 02:49:21.080]   Professor of journalism at the fabulous Craig Newmark School at the City University of New
[02:49:21.080 --> 02:49:22.080]   York.
[02:49:22.080 --> 02:49:25.520]   And then the following week we'll be together watching Google I/O.
[02:49:25.520 --> 02:49:27.760]   Yes, I'm excited about that.
[02:49:27.760 --> 02:49:28.760]   What date is that?
[02:49:28.760 --> 02:49:29.760]   That's May 10th.
[02:49:29.760 --> 02:49:30.760]   May 10th.
[02:49:30.760 --> 02:49:31.760]   May 10th.
[02:49:31.760 --> 02:49:33.640]   Jeff and I will cover the keynote.
[02:49:33.640 --> 02:49:35.400]   Jason Howell's going down for Google I/O.
[02:49:35.400 --> 02:49:38.080]   They're going to do a couple of shows down there all about Android.
[02:49:38.080 --> 02:49:42.520]   And I think they've got four or five Google folks joining them to talk about Android and
[02:49:42.520 --> 02:49:44.360]   other things that Google's up to.
[02:49:44.360 --> 02:49:50.240]   So we'll have some good coverage of Google I/O coming up in a couple of weeks.
[02:49:50.240 --> 02:49:54.360]   We do Twitch every Sunday afternoon, 2 to 5 p.m. Pacific.
[02:49:54.360 --> 02:49:55.520]   I'm sorry, 2 to 5 p.m.
[02:49:55.520 --> 02:49:57.120]   Yeah, Pacific.
[02:49:57.120 --> 02:50:01.160]   That would be 5 to 8 p.m. Eastern time.
[02:50:01.160 --> 02:50:02.920]   And then I guess 2100 UTC.
[02:50:02.920 --> 02:50:08.920]   If you want to tune in and watch live, the live streams, audio and video are at Twitch.tv/live.
[02:50:08.920 --> 02:50:14.600]   If you're watching live chat live at IRC.Twit.tv, you can also join the club Twitch Discord.
[02:50:14.600 --> 02:50:16.000]   Wouldn't hurt you.
[02:50:16.000 --> 02:50:17.000]   Wouldn't hurt you.
[02:50:17.000 --> 02:50:18.000]   It's only seven bucks a month.
[02:50:18.000 --> 02:50:21.000]   You get ad-free versions of all the shows.
[02:50:21.000 --> 02:50:23.880]   You get the special shows we don't put out in public.
[02:50:23.880 --> 02:50:28.240]   A lot of shows that we're working on, like Scott Wilkinson's home theater geeks, Hands-on
[02:50:28.240 --> 02:50:32.680]   Mac, Hands-on Windows, the Entitled Linux Show.
[02:50:32.680 --> 02:50:41.560]   And you get AI Leo who will send you a heart if you just but say hello.
[02:50:41.560 --> 02:50:44.880]   That is Twitch.tv/ I don't know what that means.
[02:50:44.880 --> 02:50:45.880]   That animated gift.
[02:50:45.880 --> 02:50:47.040]   There are lots of them on there though.
[02:50:47.040 --> 02:50:48.040]   Twitch.tv/club-twit.
[02:50:48.040 --> 02:50:50.600]   And it really helps us.
[02:50:50.600 --> 02:50:54.440]   It really smooths out the bumps between advertisers.
[02:50:54.440 --> 02:50:57.640]   It helps us produce new content and keep the lights on.
[02:50:57.640 --> 02:51:04.160]   So it's a great way to support your favorite podcasts and get some great content too.
[02:51:04.160 --> 02:51:06.600]   Twitch.tv/club-twit.
[02:51:06.600 --> 02:51:10.680]   After the fact you get the free versions of the show, ad supported at our website this
[02:51:10.680 --> 02:51:12.880]   weekend tech is Twitch.tv.
[02:51:12.880 --> 02:51:16.880]   Twitch.tv also has links to the YouTube channel for this weekend tech.
[02:51:16.880 --> 02:51:21.480]   It also has links to the various podcast players you can use.
[02:51:21.480 --> 02:51:23.560]   If you subscribe in one of them, you'll get it automatically.
[02:51:23.560 --> 02:51:27.600]   The minutes available audio or video.
[02:51:27.600 --> 02:51:28.600]   That's about it for this week.
[02:51:28.600 --> 02:51:32.360]   What a great show I thank you all for being here and we'll see you next time.
[02:51:32.360 --> 02:51:33.360]   Another Twitch.
[02:51:33.360 --> 02:51:34.360]   This is the case.
[02:51:34.360 --> 02:51:35.360]   Bye bye.
[02:51:35.360 --> 02:51:36.360]   [Music]
[02:51:36.360 --> 02:51:36.360]   [Music]
[02:51:36.360 --> 02:51:43.360]   [Music]

