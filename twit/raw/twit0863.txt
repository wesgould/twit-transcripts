;FFMETADATA1
title=An API For Truth
artist=Leo Laporte, Amy Webb, Paris Martineau
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-02-21
track=863
language=English
genre=Podcast
comment=Android Privacy Sandbox, Truth Social, Cyber Warfare, NUDGE Act
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.000]   It's time for Twit this week in tech.
[00:00:02.000 --> 00:00:07.000]   Two of my favorite people here today, they always are my favorite people, but I love Paris Martin-O from the information.
[00:00:07.000 --> 00:00:14.000]   Amy Webb is here from our favorite futurist to talk about her new book, the Genesis Machine.
[00:00:14.000 --> 00:00:19.000]   We'll also talk about Facebook's pivot and its strategy going forward.
[00:00:19.000 --> 00:00:23.000]   The war in Ukraine could mean cyber warfare on our shores.
[00:00:23.000 --> 00:00:26.000]   We'll talk about the plans for defense.
[00:00:26.000 --> 00:00:30.000]   And then it's the new Senate Nudge Act.
[00:00:30.000 --> 00:00:36.000]   Or as I call it, the Nudge Act to keep platforms from spreading misinformation.
[00:00:36.000 --> 00:00:42.000]   Will it work? That and a whole lot more coming up, including a cameo from Paris's cat.
[00:00:42.000 --> 00:00:44.000]   Next on Twit.
[00:00:44.000 --> 00:00:47.000]   Podcasts you love.
[00:00:47.000 --> 00:00:49.000]   From people you trust.
[00:00:49.000 --> 00:00:52.000]   This is Twit.
[00:00:52.000 --> 00:00:56.000]   This is Twit.
[00:00:56.000 --> 00:01:08.000]   This week in tech, episode 863, recorded Sunday, February 20th, 2022.
[00:01:08.000 --> 00:01:11.000]   An API for truth.
[00:01:11.000 --> 00:01:15.000]   This week in tech is brought to you by New Relic.
[00:01:15.000 --> 00:01:18.000]   That next 9 p.m. call is just waiting to happen.
[00:01:18.000 --> 00:01:26.000]   Get New Relic before it does and you can get access to the whole New Relic platform and 100 GB of data free forever.
[00:01:26.000 --> 00:01:28.000]   No credit card required.
[00:01:28.000 --> 00:01:32.000]   Sign up at New Relic.com/Twit.
[00:01:32.000 --> 00:01:34.000]   And by Wealthfront.
[00:01:34.000 --> 00:01:44.000]   To start building your wealth and get your first $5,000 managed for free for life, go to wealthfront.com/Twit.
[00:01:44.000 --> 00:01:52.000]   And by ZipRecruiter. According to research, 90% of employers plan to enhance their employees' experiences here.
[00:01:52.000 --> 00:01:56.000]   And if you need to add more employees, there's ZipRecruiter.
[00:01:56.000 --> 00:02:02.000]   ZipRecruiter's technology finds qualified candidates for your job and you can invite your top choices to apply.
[00:02:02.000 --> 00:02:08.000]   Try ZipRecruiter for free today at ziprecruiter.com/Twit.
[00:02:08.000 --> 00:02:11.000]   And by Stamps.com.
[00:02:11.000 --> 00:02:14.000]   Stop overpaying for shipping with Stamps.com.
[00:02:14.000 --> 00:02:21.000]   Sign up with a promo code TWIT for a special offer that includes a four-week trial, lots of free postage and a digital scale.
[00:02:21.000 --> 00:02:29.000]   No long-term commitments or contracts. Just go to Stamps.com, click the microphone at the top of the page and enter the code TWIT.
[00:02:29.000 --> 00:02:34.000]   [Music]
[00:02:34.000 --> 00:02:39.000]   It's time for TWIT this week at Tech. The show will be covering the week's tech news.
[00:02:39.000 --> 00:02:45.000]   I have the great pleasure to welcome one of my favorite panels on board here.
[00:02:45.000 --> 00:02:49.000]   Paris Martynote joins us from the information and Brooklyn.
[00:02:49.000 --> 00:02:53.000]   Hello, Paris. Good to see you. I'm glad to see you.
[00:02:53.000 --> 00:02:57.000]   You still have your bedazzled mannequin.
[00:02:57.000 --> 00:03:02.000]   I know. I was like, I have to make sure that it's in the frame this time so that people can see.
[00:03:02.000 --> 00:03:07.000]   You can also see above my other shoulder, I've got a lamp. It's like a little human torso.
[00:03:07.000 --> 00:03:11.000]   Oh my God. We got parts all about.
[00:03:11.000 --> 00:03:15.000]   It's kind of a well-reliquarium there in beautiful downtown Brooklyn.
[00:03:15.000 --> 00:03:19.000]   Is it also looks like a can of tomatoes behind you, but I must be...
[00:03:19.000 --> 00:03:21.000]   Yeah, the good kind of tomatoes.
[00:03:21.000 --> 00:03:25.000]   That is a tomato can that is empty in where I keep my masks.
[00:03:25.000 --> 00:03:31.000]   Because I both got a big tomato can because I was making a dinner party the week.
[00:03:31.000 --> 00:03:35.000]   I was like, this is a gorgeous can. Gotta keep this. What should I put in it?
[00:03:35.000 --> 00:03:39.000]   It's only where San Marzano masks. It's the only kind.
[00:03:39.000 --> 00:03:41.000]   I mean, yeah, it's that or bust.
[00:03:41.000 --> 00:03:47.000]   That's right. Also with the Amy Webb who immediately recognized the San Marzano tomatoes.
[00:03:47.000 --> 00:03:51.000]   Are you sick of me yet? You were here yesterday.
[00:03:51.000 --> 00:03:53.000]   I'm always happy to see you.
[00:03:53.000 --> 00:03:59.000]   We did a great triangulation. It's on a twit news feed or on a triangulation feed
[00:03:59.000 --> 00:04:03.000]   about Amy's new book, The Genesis Machine, which people are already.
[00:04:03.000 --> 00:04:05.000]   I'm seeing raves everywhere.
[00:04:05.000 --> 00:04:09.000]   If somebody in our discord just said how much they loved the Genesis Machine.
[00:04:09.000 --> 00:04:11.000]   Highly recommended.
[00:04:11.000 --> 00:04:15.000]   It's about... well, tell us what's it about?
[00:04:15.000 --> 00:04:21.000]   The Genesis Machine is about a currently obscure, but pretty soon
[00:04:21.000 --> 00:04:25.000]   it will be well-known area of science called synthetic biology.
[00:04:25.000 --> 00:04:29.000]   So this is basically where scientists engineer or re-engineer organisms
[00:04:29.000 --> 00:04:34.000]   to give them new and improved abilities for all different types of purposes.
[00:04:34.000 --> 00:04:40.000]   So there's a ton of opportunity and a ton of really horrifying risk on the horizon.
[00:04:40.000 --> 00:04:45.000]   And despite what, you know, it may sound like, oh, this is going to be kind of a heady read or a thick.
[00:04:45.000 --> 00:04:48.000]   It's very enjoyable. Real page turner. In fact, I love this.
[00:04:48.000 --> 00:04:51.000]   The middle part where you kind of have some sci-fi scenarios
[00:04:51.000 --> 00:04:55.000]   about what the next decades might look like. It's really good.
[00:04:55.000 --> 00:04:59.000]   And because Amy is an optimist, she keeps telling me that.
[00:04:59.000 --> 00:05:03.000]   It has a happy ending. Well, not happy, but at least...
[00:05:03.000 --> 00:05:05.000]   I don't know. It has a hopeful ending.
[00:05:05.000 --> 00:05:09.000]   Yeah. No, there's some pretty horrifically terrifying scenarios.
[00:05:09.000 --> 00:05:13.000]   The middle section are all written a little bit like science fiction,
[00:05:13.000 --> 00:05:16.000]   but they're all rooted in fact. So there's speculative scenarios.
[00:05:16.000 --> 00:05:17.000]   Yeah.
[00:05:17.000 --> 00:05:19.000]   One involves Elon Musk.
[00:05:19.000 --> 00:05:20.000]   So...
[00:05:20.000 --> 00:05:23.000]   Yes. He has a cult-like group
[00:05:23.000 --> 00:05:26.000]   building underground shelters on Mars.
[00:05:26.000 --> 00:05:28.000]   The E.S.T. years.
[00:05:28.000 --> 00:05:33.000]   There's also another horrifying point at which we've discovered that we can live forever.
[00:05:33.000 --> 00:05:37.000]   And Ted Cruz is now in his 50/50 year in the Senate.
[00:05:37.000 --> 00:05:40.000]   (laughs)
[00:05:40.000 --> 00:05:43.000]   You know, I was thinking about that the other day.
[00:05:43.000 --> 00:05:45.000]   Reading the book, by the way, I love it.
[00:05:45.000 --> 00:05:48.000]   "Genesis Machine" again. "Public Affairs" books. It's on Audible. It's on Amazon.
[00:05:48.000 --> 00:05:50.000]   It's on your bookstore.
[00:05:50.000 --> 00:05:54.000]   I was thinking, "What I want to live forever."
[00:05:54.000 --> 00:05:58.000]   And I think, no, there's a... Even if you can, you know, prolong life,
[00:05:58.000 --> 00:06:01.000]   it's good to get out of the way and let the...
[00:06:01.000 --> 00:06:02.000]   Yeah.
[00:06:02.000 --> 00:06:03.000]   Let the youngs take...
[00:06:03.000 --> 00:06:06.000]   I was thinking about this the other day, and I was like,
[00:06:06.000 --> 00:06:09.000]   you know, living forever as a concept sounds great, but the only thing
[00:06:09.000 --> 00:06:12.000]   and realize that you have to work forever as well.
[00:06:12.000 --> 00:06:13.000]   (laughs)
[00:06:13.000 --> 00:06:16.000]   Ooh. No retirement for you. That's another thing in the book.
[00:06:16.000 --> 00:06:20.000]   The retirement age keeps getting pushed out farther and farther and farther.
[00:06:20.000 --> 00:06:21.000]   Right.
[00:06:21.000 --> 00:06:25.000]   Yeah. I have... I'll have enough money to live for 20 years when I retire,
[00:06:25.000 --> 00:06:31.000]   and that better be it. Otherwise, I'm going to be in Alpo and living at the mission.
[00:06:31.000 --> 00:06:33.000]   So...
[00:06:33.000 --> 00:06:36.000]   What should we talk about this week?
[00:06:36.000 --> 00:06:37.000]   Google...
[00:06:37.000 --> 00:06:39.000]   We could talk about this week in tech news.
[00:06:39.000 --> 00:06:41.000]   We could do that.
[00:06:41.000 --> 00:06:45.000]   I said the G word. Let's talk about a big tech company, Google.
[00:06:45.000 --> 00:06:51.000]   Have said that they are going to follow an Apple's lead
[00:06:51.000 --> 00:06:57.000]   and allow Android users to start blocking digital advertising tokens.
[00:06:57.000 --> 00:07:00.000]   In fact, they have a whole initiative called Privacy Sandbox.
[00:07:00.000 --> 00:07:02.000]   That's what Flock was part of.
[00:07:02.000 --> 00:07:04.000]   That's what the new topics is part of.
[00:07:04.000 --> 00:07:08.000]   But on mobile, they want to...
[00:07:08.000 --> 00:07:10.000]   This is from the keyword blog,
[00:07:10.000 --> 00:07:13.000]   limit sharing of user data with third parties,
[00:07:13.000 --> 00:07:19.000]   operate without cross-app identifiers, including advertising ID.
[00:07:19.000 --> 00:07:23.000]   This is what Facebook says is going to cost them billions of dollars next year,
[00:07:23.000 --> 00:07:25.000]   thanks to Apple.
[00:07:25.000 --> 00:07:28.000]   But they didn't say...
[00:07:28.000 --> 00:07:31.000]   And the Google blog is when.
[00:07:31.000 --> 00:07:35.000]   And Google, remember, makes money in advertising.
[00:07:35.000 --> 00:07:40.000]   But it does seem like if both Google and Apple on mobile make it hard for Facebook
[00:07:40.000 --> 00:07:45.000]   and others to track you, that's both a good thing for consumers and a bad thing.
[00:07:45.000 --> 00:07:47.000]   Should Facebook be worried about this, Paris?
[00:07:47.000 --> 00:07:49.000]   What do you think?
[00:07:49.000 --> 00:07:57.000]   I mean, I think ultimately having both of the major cell phone providing companies,
[00:07:57.000 --> 00:08:02.000]   making these shifts, even if Google's the final product ends up being
[00:08:02.000 --> 00:08:04.000]   just a fraction of what Apple says.
[00:08:04.000 --> 00:08:07.000]   It's going to end up being bad for companies like Facebook
[00:08:07.000 --> 00:08:14.000]   who predominantly just rely on being able to track users across a variety of platforms.
[00:08:14.000 --> 00:08:19.000]   I think it's going to mean that a lot of these advertising dependent companies
[00:08:19.000 --> 00:08:29.000]   are going to have to find other ways to target users or other services to kind of line their coffers.
[00:08:29.000 --> 00:08:32.000]   I don't know if it's related, but on the day of the announcement,
[00:08:32.000 --> 00:08:36.000]   Meta fell 3% snap, 3.4% on the stock exchange.
[00:08:36.000 --> 00:08:40.000]   Pinterest down by a percent, Twitter by 3.6%.
[00:08:40.000 --> 00:08:43.000]   This is funny because these companies are making a lot of money.
[00:08:43.000 --> 00:08:47.000]   Although Google did include a statement from Snap and its announcement,
[00:08:47.000 --> 00:08:51.000]   Snap said it's made privacy a priority and places it at the center of how we design products
[00:08:51.000 --> 00:08:56.000]   so we're excited to collaborate with Google to develop a new privacy preserving standard for Android.
[00:08:56.000 --> 00:08:59.000]   It's funny because Snap said kind of the same thing about Apple.
[00:08:59.000 --> 00:09:06.000]   We're going to figure out a way to survive this, whereas Facebook said it's going to cost us $10 billion next year.
[00:09:06.000 --> 00:09:13.000]   Amy, privacy clearly is something people want is in our future.
[00:09:13.000 --> 00:09:17.000]   Our company is going to find a way around it, I guess, is the question.
[00:09:17.000 --> 00:09:23.000]   Right. So I think what's happening is that the big tech players are trying to find somebody,
[00:09:23.000 --> 00:09:28.000]   one of the entities to focus the spotlight of Washington, DC on.
[00:09:28.000 --> 00:09:37.000]   From my point of view, $10 billion is not insignificant given that Facebook's last earnings report
[00:09:37.000 --> 00:09:42.000]   was pretty catastrophic and the price has dropped a staggering amount in one day
[00:09:42.000 --> 00:09:45.000]   and they lost a ton of market shares.
[00:09:45.000 --> 00:09:52.000]   I think what's happening is if Apple preemptively strikes, which they have in Google Follows Suit,
[00:09:52.000 --> 00:09:58.000]   that leaves lawmakers a giant boogie man left and that is Facebook.
[00:09:58.000 --> 00:10:03.000]   Google will figure out a way to earn revenue off of other services.
[00:10:03.000 --> 00:10:11.000]   People may figure out advertisers might still figure out a way around some of the privacy measures,
[00:10:11.000 --> 00:10:14.000]   but Facebook doesn't have a phone in the market.
[00:10:14.000 --> 00:10:16.000]   They don't have an operating system in the market.
[00:10:16.000 --> 00:10:20.000]   That puts them at a pretty big disadvantage compared to Apple and Google
[00:10:20.000 --> 00:10:25.000]   can control in a way that Facebook kind of gets disintermediated from all of this.
[00:10:25.000 --> 00:10:28.000]   Maybe they do.
[00:10:28.000 --> 00:10:33.000]   I think Facebook just was blaming Apple because they wanted to blame Apple.
[00:10:33.000 --> 00:10:37.000]   We now have heard, by the way, that Mark Zuckerberg has spent $500 billion,
[00:10:37.000 --> 00:10:40.000]   half a trillion renaming Facebook to meta.
[00:10:40.000 --> 00:10:43.000]   I don't know where that number comes from, but that seems like an honor.
[00:10:43.000 --> 00:10:44.000]   Well, it was hiring.
[00:10:44.000 --> 00:10:46.000]   They went through this insane hiring.
[00:10:46.000 --> 00:10:51.000]   It's a lot of money.
[00:10:51.000 --> 00:10:54.000]   Well, thousands of people dedicated and moved around.
[00:10:54.000 --> 00:10:56.000]   I think these two things are connected.
[00:10:56.000 --> 00:11:04.000]   I think that obviously it seems like Facebook is seeing the writing is on the wall for the current social media-based platform space
[00:11:04.000 --> 00:11:11.000]   and that they think the future or whatever their next step is going to be is going to be in ARV or is going to be in the metaverse.
[00:11:11.000 --> 00:11:20.000]   Whether or not that pans out, we'll see whether or not it pans out in the way that Facebook thinks is going to work for it is debatable.
[00:11:20.000 --> 00:11:31.000]   But I think that part of the reason why they have spent this amount of money is because they realize they're going to have to have something else besides kind of an advertising-based social media platform.
[00:11:31.000 --> 00:11:35.000]   This number, by the way, comes from an intelligence, sir.
[00:11:35.000 --> 00:11:49.000]   And I think you're right. I think this is more than just the name change. It's a repositioning of a company that's been incredibly successful in advance of what they see as headwinds from the privacy advocates.
[00:11:49.000 --> 00:12:02.000]   But I would guess that Google and Facebook are really an Amazon if you throw them in and Microsoft, if they wanted to, are really in a good position even if third-party tracking is turned off because they have first-party data.
[00:12:02.000 --> 00:12:05.000]   Doesn't that kind of protect them?
[00:12:05.000 --> 00:12:07.000]   I think in terms of rep an Amazon.
[00:12:07.000 --> 00:12:13.000]   Facebook knows everything about you as a first-party. They don't need to track you. They know it all.
[00:12:13.000 --> 00:12:17.000]   Maybe I'm misunderstanding how this stuff works.
[00:12:17.000 --> 00:12:25.000]   Similarly, Google has all these search signals that's why search ads work so well. Right, Amy?
[00:12:25.000 --> 00:12:38.000]   Yeah. So again, I think this depends on one of the things that there's a little bit of information as asymmetry here. We don't know what plans there are to change advertising.
[00:12:38.000 --> 00:12:44.000]   I can tell you that I looked at a patent from SNAP. Oh my gosh, it must have been 2015, 2016.
[00:12:44.000 --> 00:12:48.000]   Or I don't know. Time is a vortex. I've lost a sense of what your would have been.
[00:12:48.000 --> 00:12:58.000]   Oh, why? Like pre-COVID. Let's say that. Pre-COVID. I remember seeing a patent that SNAP was building a visual auctioning system.
[00:12:58.000 --> 00:13:05.000]   So it was like AR. So if you took a photo, it would recognize what was in the photo like a can of Coca-Cola or something.
[00:13:05.000 --> 00:13:10.000]   And then it would collect all of the data from that image and build.
[00:13:10.000 --> 00:13:18.000]   It was building what looked like a competitor to Google, but for visual advertising. So visual information versus, you know.
[00:13:18.000 --> 00:13:29.000]   So we don't know what's planned. It's possible that Facebook is going all in on metaverse technologies because it opens up a new avenue to revenue through different types of...
[00:13:29.000 --> 00:13:36.000]   Not necessarily even advertising. I mean, look how much Fortnite makes on downloadable content and costumes and dances and things.
[00:13:36.000 --> 00:13:44.000]   You basically Facebook's creating a new world with it and owns the economy of a whole new world that probably does Trump advertising revenue.
[00:13:44.000 --> 00:13:51.000]   I figured the stock market, the fact that the stock market punished Facebook so badly after their results two weeks ago.
[00:13:51.000 --> 00:13:58.000]   And it hasn't bounced back. I thought, oh, that's fine. So what Mark Zuckerberg's with 50 billion less or whatever.
[00:13:58.000 --> 00:14:05.000]   It'll all come back on Monday. And it didn't. It tells me that at least the market believes this is problematic for Facebook.
[00:14:05.000 --> 00:14:12.000]   I think part of what, again, this is my outside perspective and based on what I'm seeing with some of the folks we talked to.
[00:14:12.000 --> 00:14:15.000]   I think people have forgotten that Mark Zuckerberg is very, very smart.
[00:14:15.000 --> 00:14:25.000]   And I think what's happening is that this feels like dumb decisions or silly sort of trendy shiny object decision making processes.
[00:14:25.000 --> 00:14:35.000]   And there's been so much momentum now and that it's almost like buzzy and ridiculous sounding.
[00:14:35.000 --> 00:14:41.000]   And he's a very smart person. My hunch is that there's much more going on behind the scenes.
[00:14:41.000 --> 00:14:47.000]   And for whatever reason, I think some of this has to do with the staving off some of the regulatory stuff that's coming.
[00:14:47.000 --> 00:14:53.000]   There was this push in October. But pre-October 2021, nobody was talking about this.
[00:14:53.000 --> 00:14:59.000]   And suddenly on that one fateful day, a guy makes a press conference and here we are.
[00:14:59.000 --> 00:15:00.000]   Six months later.
[00:15:00.000 --> 00:15:06.000]   And obviously the stock market, financial markets are volatile, unpredictable.
[00:15:06.000 --> 00:15:12.000]   Don't necessarily move in a rational way. Yet I kind of a believer in the wisdom of crowds.
[00:15:12.000 --> 00:15:21.000]   With a signal this strong, it's pretty clear that stock market feels, investors feel like, Facebook has a problem.
[00:15:21.000 --> 00:15:27.000]   And they're not giving them credit for being able to build a metaverse that they can recoup.
[00:15:27.000 --> 00:15:32.000]   In fact, you see a lot of articles saying, "Ah, the metaverse, we already have that or we had it.
[00:15:32.000 --> 00:15:35.000]   It's not a new idea. They're never going to be able to do it."
[00:15:35.000 --> 00:15:43.000]   Paris, are you are you champing it a bit to where VR, a visor and launching the metaverse?
[00:15:43.000 --> 00:15:47.000]   I have no interest partially because I wear glasses and I have to put it on.
[00:15:47.000 --> 00:15:48.000]   Me too.
[00:15:48.000 --> 00:15:51.000]   Yeah.
[00:15:51.000 --> 00:15:52.000]   Uncomfortable.
[00:15:52.000 --> 00:15:57.000]   I don't know. I look at enough screens, but I also know that it seems like quite a lot of people are interested in this.
[00:15:57.000 --> 00:16:01.000]   There were Oculus sales, especially if the holidays were...
[00:16:01.000 --> 00:16:02.000]   Number one app.
[00:16:02.000 --> 00:16:05.000]   Definitely. I mean, not only one of my numbers.
[00:16:05.000 --> 00:16:06.000]   Yeah.
[00:16:06.000 --> 00:16:11.000]   High relative to what? Oculus has sold a lot, but...
[00:16:11.000 --> 00:16:13.000]   It was the number one app to...
[00:16:13.000 --> 00:16:15.000]   I mean, Christmas Day was the number one app on the app.
[00:16:15.000 --> 00:16:18.000]   No, you're my defending Facebook's decisions.
[00:16:18.000 --> 00:16:22.000]   But I think that you said previously, Amy, is smart.
[00:16:22.000 --> 00:16:29.000]   Obviously, Facebook now meta didn't make this decision in a vacuum just on a whim.
[00:16:29.000 --> 00:16:36.000]   This was something that a lot of smart people put a lot of time and effort and thought into.
[00:16:36.000 --> 00:16:41.000]   And perhaps in five years, 10 years, we're all going to be looking back and being like,
[00:16:41.000 --> 00:16:46.000]   "Wow, I can't believe we thought that this whole VR/AR thing was just absolute nonsense."
[00:16:46.000 --> 00:16:51.000]   But I think that whatever that version of the product ends up being is probably going to be
[00:16:51.000 --> 00:16:55.000]   rather different than what we are thinking of as the metaverse today.
[00:16:55.000 --> 00:16:57.000]   Because frankly, I mean, it looks a little ridiculous.
[00:16:57.000 --> 00:17:00.000]   It looks like Second Life, but with a weird goggle.
[00:17:00.000 --> 00:17:04.000]   But I don't know. Maybe there'll be something to it.
[00:17:04.000 --> 00:17:07.000]   Chris Cox, who's the chief product officer at meta,
[00:17:07.000 --> 00:17:15.000]   told an all-hands meeting that the horizon platform has now 300,000 users,
[00:17:15.000 --> 00:17:18.000]   which is a 10x growth in three months.
[00:17:18.000 --> 00:17:21.000]   Yeah, but that's 300,000 users.
[00:17:21.000 --> 00:17:23.000]   Yeah, it's still not even a part of it.
[00:17:23.000 --> 00:17:25.000]   I mean, that's the thing. And it's also not brand new technology.
[00:17:25.000 --> 00:17:27.000]   None of this is brand new, right?
[00:17:27.000 --> 00:17:31.000]   So if they were up 10x in the past year or the past three years, great.
[00:17:31.000 --> 00:17:34.000]   But that is not the case here.
[00:17:34.000 --> 00:17:39.000]   I also just want to put a quick plug-in for distinguishing between AR and VR.
[00:17:39.000 --> 00:17:42.000]   These are totally different technologies and they get lumped together.
[00:17:42.000 --> 00:17:46.000]   Virtual reality struggles to find use cases and business cases,
[00:17:46.000 --> 00:17:49.000]   because for one thing, if you work glasses like we do,
[00:17:49.000 --> 00:17:52.000]   it makes it really challenging to put on the headset.
[00:17:52.000 --> 00:17:56.000]   But secondly, you've got to be in a place where you can absolutely trust that
[00:17:56.000 --> 00:17:58.000]   you're doing sensory deprivation.
[00:17:58.000 --> 00:18:01.000]   So that means you have to be in a place where you feel totally safe.
[00:18:01.000 --> 00:18:03.000]   It is totally private.
[00:18:03.000 --> 00:18:09.000]   Not a lot of people have a luxury to be in a space like that all the time.
[00:18:09.000 --> 00:18:12.000]   So I don't know.
[00:18:12.000 --> 00:18:17.000]   I'm not sure why Facebook is going so quickly on VR when extended reality
[00:18:17.000 --> 00:18:22.000]   and just straight up heads up display on regular glasses where the computer is.
[00:18:22.000 --> 00:18:24.000]   Yeah, I mean, right.
[00:18:24.000 --> 00:18:27.000]   It seems like AR has a lot more applicable use.
[00:18:27.000 --> 00:18:32.000]   It seems like there's already kind of growing use cases for that.
[00:18:32.000 --> 00:18:36.000]   I think that the total VR headset move is perplexing,
[00:18:36.000 --> 00:18:39.000]   but maybe there is some other use case that we're not seeing.
[00:18:39.000 --> 00:18:40.000]   Yeah.
[00:18:40.000 --> 00:18:45.000]   I'll tell you what I think, because this is the tweet from Horizon Worlds.
[00:18:45.000 --> 00:18:48.000]   And with the video, associated video, it's time.
[00:18:48.000 --> 00:18:50.000]   Ten thousand worlds have been created.
[00:18:50.000 --> 00:18:52.000]   Drop in and play or just hang out.
[00:18:52.000 --> 00:18:54.000]   The possibilities are endless.
[00:18:54.000 --> 00:18:55.000]   I agree with you.
[00:18:55.000 --> 00:19:00.000]   VR is also a problematic issue, but the thing that you can't do in AR
[00:19:00.000 --> 00:19:05.000]   is create a world where you can sell outfits and build, build,
[00:19:05.000 --> 00:19:08.000]   I mean, look at Second Life, the whole economy.
[00:19:08.000 --> 00:19:11.000]   The whole Linden dollar economy really exploded.
[00:19:11.000 --> 00:19:14.000]   Now, admittedly, Second Life is no longer, I mean, it's around,
[00:19:14.000 --> 00:19:16.000]   but it's no longer a driving force.
[00:19:16.000 --> 00:19:17.000]   It is around, yeah.
[00:19:17.000 --> 00:19:18.000]   People still do it.
[00:19:18.000 --> 00:19:21.000]   But it's, I think, if I'm Mark, I'm looking at that.
[00:19:21.000 --> 00:19:26.000]   I'm looking at how well Fortnite and others have done with as a free product
[00:19:26.000 --> 00:19:30.000]   that would downloadable content and thinking, this could be the future of Meta,
[00:19:30.000 --> 00:19:34.000]   and it could be really big if we, you know, if we create a world that everybody wants
[00:19:34.000 --> 00:19:42.000]   to be in and we control the economy, I can see how they could say there's a big upside to that.
[00:19:42.000 --> 00:19:45.000]   I think that upside but with limited use cases.
[00:19:45.000 --> 00:19:46.000]   Yeah.
[00:19:46.000 --> 00:19:47.000]   Sorry.
[00:19:47.000 --> 00:19:48.000]   Yeah.
[00:19:48.000 --> 00:19:51.000]   And it's very different than what Facebook's kind of core product is already.
[00:19:51.000 --> 00:19:52.000]   Oh, it's a complete pivot.
[00:19:52.000 --> 00:19:53.000]   It's a complete pivot.
[00:19:53.000 --> 00:19:58.000]   It's going to be a complete pivot, and I think that it will, I guess, have to see if these
[00:19:58.000 --> 00:20:03.000]   thousands of hires that they've made is enough to transition a company so radically.
[00:20:03.000 --> 00:20:04.000]   Yeah.
[00:20:04.000 --> 00:20:09.000]   Because, I mean, that is closer to building like a triple A game than running a social media
[00:20:09.000 --> 00:20:10.000]   network.
[00:20:10.000 --> 00:20:11.000]   I mean, it is social.
[00:20:11.000 --> 00:20:13.000]   You're in there with your friends.
[00:20:13.000 --> 00:20:17.000]   I guess World of Warcraft social, but it's not social in the way that Facebook's social,
[00:20:17.000 --> 00:20:22.000]   where you follow Granny and Granny gets to see the grandkids and you see what she's up to.
[00:20:22.000 --> 00:20:24.000]   That's what Facebook is, right?
[00:20:24.000 --> 00:20:28.000]   And it's news and it's a very different thing.
[00:20:28.000 --> 00:20:32.000]   It almost feels like Mark says, I don't think Facebook as it stands is the future.
[00:20:32.000 --> 00:20:36.000]   I think this is the future and we're going to just leave Facebook behind.
[00:20:36.000 --> 00:20:39.000]   I mean, I think users have been saying that about Facebook for a long time.
[00:20:39.000 --> 00:20:40.000]   They're aging out, right?
[00:20:40.000 --> 00:20:41.000]   Yeah.
[00:20:41.000 --> 00:20:46.000]   In the most recent earnings update you saw that daily active users had dropped for the Facebook
[00:20:46.000 --> 00:20:48.000]   platform, which is not everyone.
[00:20:48.000 --> 00:20:49.000]   Yeah.
[00:20:49.000 --> 00:20:50.000]   Yeah.
[00:20:50.000 --> 00:20:56.000]   I had a really interesting conversation a couple days ago with Philip Rosdale, who's one of the
[00:20:56.000 --> 00:20:58.000]   co-founders of Second Life.
[00:20:58.000 --> 00:21:01.000]   We were getting ready for a meeting.
[00:21:01.000 --> 00:21:03.000]   He had some interesting stuff to say about Oculus.
[00:21:03.000 --> 00:21:07.000]   He's gone back to run Second Life, as an island.
[00:21:07.000 --> 00:21:08.000]   Yeah.
[00:21:08.000 --> 00:21:13.000]   They were trying to do something with Oculus that sounded like, and, you know, again,
[00:21:13.000 --> 00:21:18.000]   I think that the use cases are just, they're just not there.
[00:21:18.000 --> 00:21:24.000]   I get what's plausible, but I think what's much more probable and practical is AR.
[00:21:24.000 --> 00:21:31.000]   I mean, I don't know why everybody's not going all in on digital glasses.
[00:21:31.000 --> 00:21:38.000]   Like, there's so much data showing that we're all becoming much more nearsighted, which makes
[00:21:38.000 --> 00:21:42.000]   sense because we're staring mostly at screens and screens up close to our faces.
[00:21:42.000 --> 00:21:47.000]   So we know that there's going to be a total addressable market of like enormous proportions
[00:21:47.000 --> 00:21:48.000]   pretty soon.
[00:21:48.000 --> 00:21:53.000]   So if everybody's going to need to wear some type of corrective lens, and if we know that
[00:21:53.000 --> 00:21:58.000]   augmented reality and sort of spatial computing at the moment, it's more practical.
[00:21:58.000 --> 00:22:00.000]   It's a little bit easier than all the VR stuff.
[00:22:00.000 --> 00:22:02.000]   There's more business use cases.
[00:22:02.000 --> 00:22:04.000]   There's more daily use cases.
[00:22:04.000 --> 00:22:05.000]   Then why wouldn't you?
[00:22:05.000 --> 00:22:10.000]   I just don't understand why people, like, why didn't Facebook become, become, why didn't
[00:22:10.000 --> 00:22:13.000]   Facebook lean in more to AR?
[00:22:13.000 --> 00:22:15.000]   They've got this ridiculous pair of glasses.
[00:22:15.000 --> 00:22:18.000]   I think people are often, yeah, I mean, the Ray Bann glasses.
[00:22:18.000 --> 00:22:22.680]   I think people are a bit scared off by the failure of like Google Glass and kind of these
[00:22:22.680 --> 00:22:23.680]   more probable.
[00:22:23.680 --> 00:22:24.680]   But none of those were AR.
[00:22:24.680 --> 00:22:26.000]   Those were not real AR.
[00:22:26.000 --> 00:22:27.000]   Right.
[00:22:27.000 --> 00:22:28.000]   Right.
[00:22:28.000 --> 00:22:29.800]   Nor are the Facebook glasses.
[00:22:29.800 --> 00:22:31.440]   Apple, apparently, we don't know.
[00:22:31.440 --> 00:22:36.440]   Apple is going to do its rumor to VR headset this year in AR headset in years to come.
[00:22:36.440 --> 00:22:41.240]   I would love it if they had functional AR in glasses that I can also say.
[00:22:41.240 --> 00:22:45.240]   Even those glasses you're wearing right now with a heads up display.
[00:22:45.240 --> 00:22:50.600]   Now maybe they're looking at tweets while I also make eye contact with the camera.
[00:22:50.600 --> 00:22:53.920]   You wouldn't be the first one of our panelists to do that.
[00:22:53.920 --> 00:22:56.000]   I don't want to name names, but there are those.
[00:22:56.000 --> 00:22:58.360]   I have done it before on the show.
[00:22:58.360 --> 00:23:02.600]   So I know because I see tweets coming in from our panelists.
[00:23:02.600 --> 00:23:04.800]   I'm going, we're doing something.
[00:23:04.800 --> 00:23:07.360]   I mean, you just got to keep that.
[00:23:07.360 --> 00:23:08.360]   Stay plugged in.
[00:23:08.360 --> 00:23:11.800]   The truth is anybody does our shows is completely ADD.
[00:23:11.800 --> 00:23:13.840]   They need multiple stimuli.
[00:23:13.840 --> 00:23:14.840]   And that's fine.
[00:23:14.840 --> 00:23:15.840]   I understand.
[00:23:15.840 --> 00:23:16.840]   You got to be tweeting.
[00:23:16.840 --> 00:23:17.840]   I got 18 things going on.
[00:23:17.840 --> 00:23:20.200]   I was just saying you have two phones sitting on your desk.
[00:23:20.200 --> 00:23:22.080]   You're looking at a computer and you're surrounded by two of these.
[00:23:22.080 --> 00:23:23.080]   I got a chat room.
[00:23:23.080 --> 00:23:24.480]   I got two chat rooms going.
[00:23:24.480 --> 00:23:25.480]   But that's my job.
[00:23:25.480 --> 00:23:27.920]   Yeah, you heard it.
[00:23:27.920 --> 00:23:28.920]   Word.
[00:23:28.920 --> 00:23:29.920]   I'm totally ADD.
[00:23:29.920 --> 00:23:31.960]   No, that's no, I totally.
[00:23:31.960 --> 00:23:36.280]   So in Microsoft, the rumor is, according to Business Insider, not going to go forward
[00:23:36.280 --> 00:23:43.400]   with HoloLens 3, they haven't found a real market for HoloLens, which is AR.
[00:23:43.400 --> 00:23:46.160]   Magic Leap has stumbled.
[00:23:46.160 --> 00:23:48.120]   Is it because AR is hard to do?
[00:23:48.120 --> 00:23:49.240]   I agree with both of you.
[00:23:49.240 --> 00:23:50.240]   I can talk about that.
[00:23:50.240 --> 00:23:53.200]   I love this vision of augmented reality.
[00:23:53.200 --> 00:23:59.440]   The same one that was in Damon and FreedomTM, Daniel Suarez's books, where you're walking
[00:23:59.440 --> 00:24:06.160]   around and you have this rich data set along with seeing the world around you.
[00:24:06.160 --> 00:24:09.320]   Is it harder to do, Amy, than we thought?
[00:24:09.320 --> 00:24:10.320]   Or what?
[00:24:10.320 --> 00:24:13.040]   Why is HoloLens dying?
[00:24:13.040 --> 00:24:15.080]   What's going on?
[00:24:15.080 --> 00:24:16.080]   Right.
[00:24:16.080 --> 00:24:18.640]   So it's hard to do in a compact way.
[00:24:18.640 --> 00:24:24.520]   So I've actually got an original Magic Leap in the room behind me.
[00:24:24.520 --> 00:24:26.080]   I've got HoloLens.
[00:24:26.080 --> 00:24:29.120]   I've obviously tried and played with all of these things.
[00:24:29.120 --> 00:24:31.880]   HoloLens is an incredible experience.
[00:24:31.880 --> 00:24:34.040]   You can actually wear it with glasses on.
[00:24:34.040 --> 00:24:37.200]   The problem is it's an enormous clunky headset.
[00:24:37.200 --> 00:24:41.920]   Magic Leap, I thought, was spectacular.
[00:24:41.920 --> 00:24:44.720]   I think the technology is spectacular.
[00:24:44.720 --> 00:24:47.320]   It was a huge, again, another really big clunky headset.
[00:24:47.320 --> 00:24:55.400]   You had to wear a pack that heated up and it didn't have a long battery life.
[00:24:55.400 --> 00:24:58.760]   Here's what I would say, though.
[00:24:58.760 --> 00:25:07.240]   I'm concerned that humanity is failing the Marshmallow test.
[00:25:07.240 --> 00:25:09.040]   We have no patience.
[00:25:09.040 --> 00:25:16.400]   This is basic technology that still needs many years.
[00:25:16.400 --> 00:25:21.080]   Because it's much more functional, it's much more practical, there was something called
[00:25:21.080 --> 00:25:25.080]   mine meld that a guy at MIT, it was an app.
[00:25:25.080 --> 00:25:27.200]   It was the greatest thing I have ever seen.
[00:25:27.200 --> 00:25:32.160]   To this day, it was the most impressive, most amazing thing I've ever seen.
[00:25:32.160 --> 00:25:34.440]   It was an app that you ran on your iPad.
[00:25:34.440 --> 00:25:40.680]   While two people were talking, it would automatically pull up and define everything.
[00:25:40.680 --> 00:25:42.760]   All the stuff that you would be like, "You're sitting in a meeting.
[00:25:42.760 --> 00:25:44.360]   I don't know what half of these acronyms are."
[00:25:44.360 --> 00:25:46.440]   We kind of have that right now.
[00:25:46.440 --> 00:25:48.440]   Think of the progress we've made.
[00:25:48.440 --> 00:25:52.440]   It used to be you'd have to go to the library to find out how old John Glenn was when he
[00:25:52.440 --> 00:25:53.440]   died.
[00:25:53.440 --> 00:25:55.440]   Now you just ask Alexa.
[00:25:55.440 --> 00:25:56.440]   We have all that already.
[00:25:56.440 --> 00:26:01.440]   No, this was different.
[00:26:01.440 --> 00:26:03.120]   It was amazing.
[00:26:03.120 --> 00:26:04.960]   It was just a amount of names.
[00:26:04.960 --> 00:26:06.520]   We're getting more and more like that.
[00:26:06.520 --> 00:26:10.280]   This is instant information on your fingertips.
[00:26:10.280 --> 00:26:19.480]   Imagine that overlaid when I wanted it to be there on my glasses.
[00:26:19.480 --> 00:26:20.320]   That would be amazing.
[00:26:20.320 --> 00:26:24.960]   If I was when the world opens up again and I go back to conferences, I would love to
[00:26:24.960 --> 00:26:30.960]   be able to sit in a conference and hear that conversation and not miss anything because
[00:26:30.960 --> 00:26:35.400]   I've got it in my field of view or better yet, I would love to speak at a conference
[00:26:35.400 --> 00:26:36.400]   and feet.
[00:26:36.400 --> 00:26:39.240]   I used to live tweet my own speeches.
[00:26:39.240 --> 00:26:40.240]   I used to program.
[00:26:40.240 --> 00:26:44.520]   I had a little script that ran and it would automatically tweet out the stuff that I knew
[00:26:44.520 --> 00:26:46.520]   people were going to be looking up or.
[00:26:46.520 --> 00:26:50.800]   I've been speeches with Twitter behind me as people are commenting, which is a very brave
[00:26:50.800 --> 00:26:51.800]   and dumb do.
[00:26:51.800 --> 00:26:52.800]   No, no, no.
[00:26:52.800 --> 00:26:53.800]   I wouldn't do that.
[00:26:53.800 --> 00:26:54.800]   That's insane.
[00:26:54.800 --> 00:26:55.800]   This was me.
[00:26:55.800 --> 00:26:59.080]   I had a little Apple script running.
[00:26:59.080 --> 00:27:00.080]   It was badass.
[00:27:00.080 --> 00:27:01.280]   It was awesome.
[00:27:01.280 --> 00:27:06.520]   But imagine if I could pre-program a speech and feed that stuff right into the field of
[00:27:06.520 --> 00:27:08.000]   vision.
[00:27:08.000 --> 00:27:11.440]   This just supercharges our productivity and what we can do.
[00:27:11.440 --> 00:27:17.160]   Roni Abovitz, who I think very, very highly of, he was the founder of Magic Leap.
[00:27:17.160 --> 00:27:19.560]   This is the kind of stuff he was working on.
[00:27:19.560 --> 00:27:22.160]   Man, investors, they took on a lot of money.
[00:27:22.160 --> 00:27:26.960]   So this is the marshmallow test, which is we're not good at delayed gratification.
[00:27:26.960 --> 00:27:27.960]   We are not.
[00:27:27.960 --> 00:27:28.960]   Well, the problem is-
[00:27:28.960 --> 00:27:33.600]   This is a couple of toddlers with a marshmallow and you get two marshmallows if you don't
[00:27:33.600 --> 00:27:37.080]   eat it right away and humans just eat it away.
[00:27:37.080 --> 00:27:39.520]   Yeah, and it's technology, it's science.
[00:27:39.520 --> 00:27:43.800]   It's just like general patience with other people.
[00:27:43.800 --> 00:27:49.720]   This stuff is very useful, especially when we get to a point where we've got functional
[00:27:49.720 --> 00:27:50.720]   digital twins.
[00:27:50.720 --> 00:27:52.320]   You're going to want to use?
[00:27:52.320 --> 00:27:53.440]   We talked about this before.
[00:27:53.440 --> 00:28:00.560]   There are companies like Apple that have the resources to be patient and to develop something.
[00:28:00.560 --> 00:28:01.560]   No startup.
[00:28:01.560 --> 00:28:05.400]   This is why Magic Leap, the investors just didn't have the patience to give Magic Leap
[00:28:05.400 --> 00:28:06.480]   that timeframe.
[00:28:06.480 --> 00:28:10.560]   But there are companies like Microsoft, apparently they're not as patient maybe.
[00:28:10.560 --> 00:28:13.720]   I don't know, they dispute the fact that HoloLens 3 is dead.
[00:28:13.720 --> 00:28:15.360]   They said it's doing great.
[00:28:15.360 --> 00:28:18.480]   It is terrific technology.
[00:28:18.480 --> 00:28:21.640]   But again, you have to think through where are the use cases for that.
[00:28:21.640 --> 00:28:27.120]   And outside of gaming, again and again, I think this has been the problem.
[00:28:27.120 --> 00:28:32.120]   These companies don't quite understand what the better markets are beyond the ones that
[00:28:32.120 --> 00:28:33.400]   they've already imagined.
[00:28:33.400 --> 00:28:35.840]   And so they give up.
[00:28:35.840 --> 00:28:41.760]   Yeah, 70 Microsoft employees, according to the Wall Street Journal, left HoloLens, 40
[00:28:41.760 --> 00:28:43.560]   of them join Metta.
[00:28:43.560 --> 00:28:46.320]   So this is very typical in Silicon Valley.
[00:28:46.320 --> 00:28:49.760]   There's a lot of movement back and forth and who knows they might go back to Microsoft
[00:28:49.760 --> 00:28:50.760]   later.
[00:28:50.760 --> 00:28:51.760]   I think it's interesting.
[00:28:51.760 --> 00:28:55.920]   The companies that have the resources and the patients like Apple, like Microsoft, Google,
[00:28:55.920 --> 00:29:00.000]   no, Google has no patience.
[00:29:00.000 --> 00:29:02.760]   They may come up with this down the road.
[00:29:02.760 --> 00:29:06.440]   I think Metta, I think Mark is, I will stipulate.
[00:29:06.440 --> 00:29:07.840]   He's very smart.
[00:29:07.840 --> 00:29:12.640]   And I think he saw the writing on the wall and he said this, it's not so much Apple pulling
[00:29:12.640 --> 00:29:13.640]   IDFA.
[00:29:13.640 --> 00:29:17.680]   It's not so much Google's, you know, security and privacy vault.
[00:29:17.680 --> 00:29:22.480]   It's really just the consumers don't want ad tracking.
[00:29:22.480 --> 00:29:23.760]   And that's just become common.
[00:29:23.760 --> 00:29:29.600]   It's just, it's a common sentiment even among people who are not technologically that sophisticated.
[00:29:29.600 --> 00:29:35.640]   So I think he probably looked at that and said, yeah, we need a different model.
[00:29:35.640 --> 00:29:39.880]   And he has the money and he has the wherewithal and I hope he has the patience to pivot.
[00:29:39.880 --> 00:29:44.560]   They bought Oculus so clearly they thought about this years ago.
[00:29:44.560 --> 00:29:46.680]   And maybe it's going to take, what's the timeframe, Amy?
[00:29:46.680 --> 00:29:53.920]   You think 10 years before we have the technology to wear lightweight, you know, battery powered,
[00:29:53.920 --> 00:29:58.280]   enough processing power, you need all day battery life, right?
[00:29:58.280 --> 00:30:01.720]   You can't, you don't have to charge your glasses at lunch.
[00:30:01.720 --> 00:30:02.720]   Right.
[00:30:02.720 --> 00:30:06.720]   I mean, I guess, listen, we're already wearing plenty of other wearables.
[00:30:06.720 --> 00:30:11.880]   The problem actually has less to do with the battery than it does refracting the light
[00:30:11.880 --> 00:30:14.680]   and making it so that you can see.
[00:30:14.680 --> 00:30:16.240]   There are other issues as well.
[00:30:16.240 --> 00:30:17.240]   Okay.
[00:30:17.240 --> 00:30:22.720]   There are other issues, but I don't understand what the end depends on who you talk to.
[00:30:22.720 --> 00:30:27.920]   You know, yes or no, or maybe.
[00:30:27.920 --> 00:30:29.720]   Paris is wearing right now.
[00:30:29.720 --> 00:30:32.920]   Paris is wearing those earbuds, as Apple earbuds.
[00:30:32.920 --> 00:30:33.920]   I am.
[00:30:33.920 --> 00:30:34.920]   You're special.
[00:30:34.920 --> 00:30:36.600]   I mean, special audio.
[00:30:36.600 --> 00:30:42.280]   We already are kind of in this semi, you know, we're halfway there, I guess.
[00:30:42.280 --> 00:30:48.360]   And I could, and I totally can see how somebody like Mark or Tim Cook might say, I see this
[00:30:48.360 --> 00:30:49.360]   coming.
[00:30:49.360 --> 00:30:55.040]   We need to solve these very real technology hurdles, but we think this is going to be
[00:30:55.040 --> 00:30:57.400]   the next big thing if we can do that.
[00:30:57.400 --> 00:31:04.240]   And I think for a company like Facebook, obviously a large part of this is they had
[00:31:04.240 --> 00:31:05.240]   to rebrand.
[00:31:05.240 --> 00:31:10.520]   I mean, I have noticed this just even scrolling through apps like Instagram lately from that
[00:31:10.520 --> 00:31:11.520]   I follow.
[00:31:11.520 --> 00:31:17.360]   I know from meta, but I follow an influencer who normally, I think when all of the Facebook
[00:31:17.360 --> 00:31:22.360]   stuff is going down, shared some like memes or whatever, very anti Facebook.
[00:31:22.360 --> 00:31:27.160]   But of course now they're all promoting the Ray ban meta glasses, but I think that part
[00:31:27.160 --> 00:31:33.880]   of the re-band is to make these sort of technologies that Facebook wants to corner.
[00:31:33.880 --> 00:31:40.920]   Very palatable to a younger generation that might have kind of negative associations with
[00:31:40.920 --> 00:31:45.880]   the brand name Facebook, because they're all tagging at Ray then at meta.
[00:31:45.880 --> 00:31:50.800]   That seems a bit more innocuous than a Facebook product that is going to be a camera you wear
[00:31:50.800 --> 00:31:52.280]   around all the time.
[00:31:52.280 --> 00:31:56.520]   If you could, you know, I think all would be forgiven.
[00:31:56.520 --> 00:31:59.400]   All would be forgiven if you made a cool enough product.
[00:31:59.400 --> 00:32:01.400]   That's all that man.
[00:32:01.400 --> 00:32:03.640]   Is that is that your blanket statement?
[00:32:03.640 --> 00:32:04.640]   Yes.
[00:32:04.640 --> 00:32:05.640]   All is forgiven.
[00:32:05.640 --> 00:32:06.640]   All is forgiven.
[00:32:06.640 --> 00:32:11.640]   You know, I won't, I don't have a Facebook account, so I can't use an Oculus Quest.
[00:32:11.640 --> 00:32:15.800]   And it's not quite cool enough for me to bite the bullet and make a Facebook account to do
[00:32:15.800 --> 00:32:17.240]   it.
[00:32:17.240 --> 00:32:23.800]   But if there really were a metaverse and maybe we even got legs in this metaverse and all
[00:32:23.800 --> 00:32:29.160]   my friends were in there and I were missing all the fun, I don't think people, AR memories
[00:32:29.160 --> 00:32:30.000]   aren't that long.
[00:32:30.000 --> 00:32:33.600]   I think at five years, if there's something cool enough, people will fly.
[00:32:33.600 --> 00:32:36.840]   They won't care what then if it's Mark Zuckerberg.
[00:32:36.840 --> 00:32:37.840]   I don't think.
[00:32:37.840 --> 00:32:41.840]   I think I'd be curious to know whether or not people want that type of kinetic experience
[00:32:41.840 --> 00:32:47.880]   because I think the other thing we're, I don't know, I've screwed around a lot with VR and
[00:32:47.880 --> 00:32:51.400]   I sometimes I just want to be a blob on my couch and play Zelda.
[00:32:51.400 --> 00:32:55.320]   I don't want a full body digital experience, you know.
[00:32:55.320 --> 00:32:56.320]   No, that's a, no, no.
[00:32:56.320 --> 00:33:00.320]   I want to have to climb a giant tower while I'm playing Zelda.
[00:33:00.320 --> 00:33:02.800]   I just want to do it in Zelda.
[00:33:02.800 --> 00:33:03.800]   I want to hit.
[00:33:03.800 --> 00:33:04.800]   Yeah.
[00:33:04.800 --> 00:33:05.800]   I just want to go up, up, up, up, up, up, up.
[00:33:05.800 --> 00:33:11.400]   No, I think that that's an unknown and I think that there are unknowns and it's not
[00:33:11.400 --> 00:33:12.400]   a sure thing.
[00:33:12.400 --> 00:33:13.400]   It's a gamble.
[00:33:13.400 --> 00:33:20.640]   But I think I would, I would bet Apple and Facebook bought into this Facebook to the
[00:33:20.640 --> 00:33:26.000]   tune of half a trillion dollars have bought into this notion that yeah, it's a gamble.
[00:33:26.000 --> 00:33:29.080]   People might not, you know, flock to it.
[00:33:29.080 --> 00:33:32.720]   But if we build it, we believe they will come.
[00:33:32.720 --> 00:33:36.280]   And I think that's a good, that's a good point, Leo, because why therefore why are we
[00:33:36.280 --> 00:33:41.360]   chastising Zuckerberg for defining the future and working backwards?
[00:33:41.360 --> 00:33:45.160]   That is the thing that we praise and we continue to praise Steve Jobs for.
[00:33:45.160 --> 00:33:46.160]   Right.
[00:33:46.160 --> 00:33:47.160]   Right.
[00:33:47.160 --> 00:33:48.160]   I think it's got to know they needed.
[00:33:48.160 --> 00:33:49.680]   No, I think he's got to see his how.
[00:33:49.680 --> 00:33:54.160]   I think to take an enterprise like Facebook that's, I mean, look, they did not have an
[00:33:54.160 --> 00:33:56.440]   unprofitable quarter.
[00:33:56.440 --> 00:33:59.880]   You know, there were some, a couple of negative numbers that stock market punished him for,
[00:33:59.880 --> 00:34:02.560]   but really that's a juggernaut.
[00:34:02.560 --> 00:34:10.440]   And yet he is saying, yeah, I, you know, I see farther and I see problems.
[00:34:10.440 --> 00:34:14.560]   So we're going to spend half a trillion dollars and change everything.
[00:34:14.560 --> 00:34:17.680]   I mean, that's a very gutsy thing to do.
[00:34:17.680 --> 00:34:19.960]   I'm curious to hear Paris's take on this.
[00:34:19.960 --> 00:34:24.160]   We haven't heard very much from Cheryl Sandberg this entire time, right?
[00:34:24.160 --> 00:34:25.680]   I'm sure that's purposeful.
[00:34:25.680 --> 00:34:31.720]   I mean, I think that with all, I think in the same sense that, you know, one, there was
[00:34:31.720 --> 00:34:34.760]   all that attention on, oh, is there a rift in their relationship?
[00:34:34.760 --> 00:34:35.760]   Right.
[00:34:35.760 --> 00:34:40.720]   And I think that part of this is that she is working behind the scenes and they've realized
[00:34:40.720 --> 00:34:44.600]   that most press for them ends up being negative press.
[00:34:44.600 --> 00:34:49.680]   I think the Times has had some really interesting reporting, probably not recently, probably
[00:34:49.680 --> 00:34:54.680]   a couple of months ago, at the very least on kind of the very calculated way in which
[00:34:54.680 --> 00:34:56.680]   Facebook handles PR.
[00:34:56.680 --> 00:35:01.760]   I mean, part of the reason why people have been laughing at the goofy Mark Zuckerberg
[00:35:01.760 --> 00:35:04.920]   launching meta is because they have crafted this right.
[00:35:04.920 --> 00:35:10.440]   They've carefully crafted this image of him as the goofy guy who wears a lot of sunscreen
[00:35:10.440 --> 00:35:15.880]   and goes windsurfing or the person who posts about smoking meats in his backyard.
[00:35:15.880 --> 00:35:18.360]   That's too kind of soften his image.
[00:35:18.360 --> 00:35:21.640]   Oh, obviously Cheryl Sandberg hasn't had that.
[00:35:21.640 --> 00:35:26.160]   They're making him the friendly doofus so that we're not afraid of yet.
[00:35:26.160 --> 00:35:27.640]   Yeah, I mean, it's completely purposeful.
[00:35:27.640 --> 00:35:31.560]   That's the only reason why you're going to have a lot of images of someone surfing on
[00:35:31.560 --> 00:35:37.040]   a windboard with a bunch of sunscreen on the face for all these big things that we do,
[00:35:37.040 --> 00:35:38.040]   you know?
[00:35:38.040 --> 00:35:42.520]   So something happened this week that actually might be Jermaine, which is they moved, they
[00:35:42.520 --> 00:35:45.720]   basically created a triumph for it.
[00:35:45.720 --> 00:35:49.360]   So Cheryl Sandberg is in charge of operations.
[00:35:49.360 --> 00:35:54.120]   Mark is in charge of the vision.
[00:35:54.120 --> 00:35:59.640]   And now Nick Clegg, who was the former deputy prime minister at the UK, who was in charge
[00:35:59.640 --> 00:36:01.880]   of global affairs and communications.
[00:36:01.880 --> 00:36:04.800]   Now he is president of global affairs.
[00:36:04.800 --> 00:36:07.000]   He's a new president.
[00:36:07.000 --> 00:36:15.000]   And it's essentially a triumvirate of Sheryl Sandberg, Mark Zuckerberg and Nick Clegg.
[00:36:15.000 --> 00:36:22.160]   And Cheryl runs the business, Mark's in charge of product, I guess is the best thing to describe
[00:36:22.160 --> 00:36:23.160]   it as.
[00:36:23.160 --> 00:36:29.400]   And Nick's in charge of relationship with the outside world government, but also in marketing.
[00:36:29.400 --> 00:36:30.400]   And that sounds-
[00:36:30.400 --> 00:36:31.400]   We'll have to see if Nick-
[00:36:31.400 --> 00:36:32.400]   Oh, sorry.
[00:36:32.400 --> 00:36:33.400]   He's good at this.
[00:36:33.400 --> 00:36:39.560]   And I think that sounds to me like a streamlined, like this is their burden for action.
[00:36:39.560 --> 00:36:41.640]   This is structure, the streamline.
[00:36:41.640 --> 00:36:48.200]   And similar to kind of what Microsoft did as it was facing a lot of regulatory scrutiny,
[00:36:48.200 --> 00:36:52.760]   when it elevated its general counsel, Brad Smith, to president in charge of dealing with
[00:36:52.760 --> 00:36:55.520]   government and governmental issues.
[00:36:55.520 --> 00:37:00.340]   Clegg is there to kind of be there in front of the firing squad and handle all this public
[00:37:00.340 --> 00:37:06.320]   scrutiny and let people like Mark Zuckerberg, and I guess to a lesser extent, Cheryl Sandberg
[00:37:06.320 --> 00:37:07.560]   kind of fade into the background.
[00:37:07.560 --> 00:37:09.760]   Do their job.
[00:37:09.760 --> 00:37:11.640]   So he'll take all the heat.
[00:37:11.640 --> 00:37:14.280]   Well, this is what Mark wrote in his post.
[00:37:14.280 --> 00:37:20.840]   We need a senior leader at the level of myself for our products and Cheryl for our business
[00:37:20.840 --> 00:37:25.240]   who can lead and represent us for all our policy issues globally.
[00:37:25.240 --> 00:37:28.320]   So he's elevated Nick to equal.
[00:37:28.320 --> 00:37:33.000]   Nick will now lead our company on all our policy matters, including, but I would say
[00:37:33.000 --> 00:37:37.000]   not limited to how we interact with governments as they consider adopting new policies and
[00:37:37.000 --> 00:37:43.240]   regulations, as well as how we make the case publicly for our products and work.
[00:37:43.240 --> 00:37:44.240]   Is that a sense?
[00:37:44.240 --> 00:37:45.560]   Let me translate this.
[00:37:45.560 --> 00:37:46.560]   Yes, please.
[00:37:46.560 --> 00:37:51.040]   This is, Clegg will be the guy who's going to sit in front of Congress and the European
[00:37:51.040 --> 00:37:53.200]   Commission and get yelled at.
[00:37:53.200 --> 00:37:54.560]   He's going to be the person.
[00:37:54.560 --> 00:37:56.760]   We painted a target on his photos.
[00:37:56.760 --> 00:38:00.520]   I mean, you know, he's probably getting paid quite a lot.
[00:38:00.520 --> 00:38:05.800]   So it's probably a complete gold laid target, but it means that you're not going to have
[00:38:05.800 --> 00:38:11.560]   any more photos of Mark Zuckerberg sitting on his little high chair at Congress because
[00:38:11.560 --> 00:38:12.960]   he has better things to do.
[00:38:12.960 --> 00:38:13.960]   That's right.
[00:38:13.960 --> 00:38:18.040]   But another way to look at this is so that that was what I was going to say and then
[00:38:18.040 --> 00:38:23.880]   the Brad Smith's position, you know, how he wound up president of Microsoft, but Brad
[00:38:23.880 --> 00:38:32.480]   Smith is an incredibly smart, gifted, you know, technology diplomat, whatever that that
[00:38:32.480 --> 00:38:35.880]   really official role, you know, job title is for that now.
[00:38:35.880 --> 00:38:41.360]   I don't think that Nick Clegg is anywhere close to that status.
[00:38:41.360 --> 00:38:44.800]   And for that reason, you know, this might be a huge missed opportunity for Facebook because
[00:38:44.800 --> 00:38:51.800]   it's not enough to stave off criticism and to deflect the attention away from Mark.
[00:38:51.800 --> 00:38:58.600]   It could have been an opportunity to start building forward and to become an indispensable
[00:38:58.600 --> 00:39:04.320]   part of policy and decision making the way that that Brad has done for Microsoft.
[00:39:04.320 --> 00:39:06.720]   When you were sitting with the issue, go ahead.
[00:39:06.720 --> 00:39:09.320]   I think part of the issue, my cat has joined us here.
[00:39:09.320 --> 00:39:10.320]   Yeah.
[00:39:10.320 --> 00:39:11.320]   I don't know why she's doing this.
[00:39:11.320 --> 00:39:12.320]   Always welcome.
[00:39:12.320 --> 00:39:13.320]   Yep.
[00:39:13.320 --> 00:39:17.720]   Well, I think part of the issue is that when you are a company like Facebook at this moment
[00:39:17.720 --> 00:39:25.400]   in time, the number of people who are truly gifted at, you know, policy, political matters
[00:39:25.400 --> 00:39:31.600]   and technically gifted that are also going to take a job as being the target for Facebook.
[00:39:31.600 --> 00:39:32.840]   That's a very small list.
[00:39:32.840 --> 00:39:36.120]   It's probably only going to be internal people like Nick Clegg.
[00:39:36.120 --> 00:39:40.280]   Actually, according to what's your cat's name?
[00:39:40.280 --> 00:39:41.280]   Ismo.
[00:39:41.280 --> 00:39:44.720]   She's trying to sit on all of my keyboards.
[00:39:44.720 --> 00:39:46.720]   Ismo.
[00:39:46.720 --> 00:39:53.120]   Two years ago, three years ago, Facebook tried to hire Brad Smith, by the way, 2018.
[00:39:53.120 --> 00:39:57.040]   So I think this has been in the works.
[00:39:57.040 --> 00:39:58.600]   I've come around to the...
[00:39:58.600 --> 00:40:04.400]   I was very skeptical when Zuckerberg told everybody, "Look, this Apple thing is going
[00:40:04.400 --> 00:40:05.400]   to kill us.
[00:40:05.400 --> 00:40:06.680]   It's going to cost us $10 billion."
[00:40:06.680 --> 00:40:08.680]   I thought that was misdirection.
[00:40:08.680 --> 00:40:14.200]   I think, you know, Snap has said, and I'm sure Facebook has ways of tracking people without
[00:40:14.200 --> 00:40:18.160]   the help of one little bit of information from an iPhone.
[00:40:18.160 --> 00:40:23.080]   However, now, as I think about it, I think maybe Mark has realized that it's going to
[00:40:23.080 --> 00:40:26.400]   be a much more serious problem than just that.
[00:40:26.400 --> 00:40:29.360]   Google's kind of confirming that with their decision to do this.
[00:40:29.360 --> 00:40:30.360]   And that Mark has pivoted.
[00:40:30.360 --> 00:40:36.280]   Now, as I think about it, I think maybe this is a very interesting move that could fail
[00:40:36.280 --> 00:40:39.120]   spectacularly, admittedly.
[00:40:39.120 --> 00:40:41.840]   But he doesn't want to be mispaced.
[00:40:41.840 --> 00:40:42.840]   And you know what?
[00:40:42.840 --> 00:40:47.040]   You're going to go through a few years of the market not believing of pundits like us
[00:40:47.040 --> 00:40:50.160]   saying, "Oh, Facebook's had it."
[00:40:50.160 --> 00:40:57.080]   But if you're Mark, if we grant that he's smart, this is a very interesting strategy.
[00:40:57.080 --> 00:41:03.080]   It's worthwhile noting that Facebook is a very different company outside of developed
[00:41:03.080 --> 00:41:04.080]   economies.
[00:41:04.080 --> 00:41:11.000]   So in maturing economies or developing economies, Facebook serves a different purpose in society.
[00:41:11.000 --> 00:41:14.320]   In some cases, it is the way that people get online.
[00:41:14.320 --> 00:41:17.280]   Facebook is also what's up.
[00:41:17.280 --> 00:41:18.760]   I think we need to...
[00:41:18.760 --> 00:41:19.760]   It's WhatsApp and it's Instagram.
[00:41:19.760 --> 00:41:22.960]   It is the way that people communicate with everyone.
[00:41:22.960 --> 00:41:26.240]   Yeah, it's the way to do it with that WhatsApp.
[00:41:26.240 --> 00:41:27.240]   Yeah.
[00:41:27.240 --> 00:41:28.240]   Right.
[00:41:28.240 --> 00:41:33.280]   So you've got small businesses, parts of the economy in emerging markets like Brazil,
[00:41:33.280 --> 00:41:36.520]   like India, Facebook is a core part of the economy.
[00:41:36.520 --> 00:41:41.640]   So from our vantage point in the United States, like, yeah, we may not need to post whatever
[00:41:41.640 --> 00:41:44.760]   messages to our high school people or whatever it might be.
[00:41:44.760 --> 00:41:46.560]   And maybe metaverse seems silly.
[00:41:46.560 --> 00:41:48.200]   But there's a pretty...
[00:41:48.200 --> 00:41:49.840]   There's a huge market outside.
[00:41:49.840 --> 00:41:54.480]   And so I'd be curious to know with the pivot how it is...
[00:41:54.480 --> 00:41:58.120]   I'd be curious to know what that long-term roadmap looks like outside of the US where...
[00:41:58.120 --> 00:42:00.560]   I don't know, Google...
[00:42:00.560 --> 00:42:04.160]   I was just about to say like, where there may not be enough compute or enough resources,
[00:42:04.160 --> 00:42:09.920]   but then I changed what I was going to say because Google is funding a huge pipe to Africa
[00:42:09.920 --> 00:42:14.440]   to get parts of the region online and with like higher speed broadband.
[00:42:14.440 --> 00:42:15.440]   So...
[00:42:15.440 --> 00:42:19.760]   And remember, Facebook's internet.org plans, which India said, "No, thank you.
[00:42:19.760 --> 00:42:22.520]   We don't need any more colonialists running our internet."
[00:42:22.520 --> 00:42:26.240]   But for a while, it was there thinking, "Oh, we'll actually provide internet to people
[00:42:26.240 --> 00:42:27.440]   who don't have it.
[00:42:27.440 --> 00:42:30.640]   It'll be Facebook internet, but we'll provide this kind of internet to people who don't
[00:42:30.640 --> 00:42:31.640]   have it."
[00:42:31.640 --> 00:42:32.640]   Right.
[00:42:32.640 --> 00:42:33.640]   Facebook internet.
[00:42:33.640 --> 00:42:34.640]   I wonder if eventually...
[00:42:34.640 --> 00:42:35.640]   Yeah.
[00:42:35.640 --> 00:42:36.640]   Yeah.
[00:42:36.640 --> 00:42:40.840]   I think I wonder if eventually we'll see something like that, but with providing Oculus headsets
[00:42:40.840 --> 00:42:48.480]   or whatever medium they end up choosing to be like the basis of their VR world or metaverse,
[00:42:48.480 --> 00:42:52.080]   what have you, because I mean, I think that that would be the main barrier to entry as
[00:42:52.080 --> 00:42:56.880]   far as kind of a global expansion of these products is that most people probably are
[00:42:56.880 --> 00:43:03.440]   not going to be able to purchase an Oculus headset in the developing world.
[00:43:03.440 --> 00:43:06.440]   When you sat with Philip Rosdale, did you ask him about all of this, Amy?
[00:43:06.440 --> 00:43:08.840]   Did you get a chance to kind of pick his brain?
[00:43:08.840 --> 00:43:09.840]   Yeah.
[00:43:09.840 --> 00:43:10.840]   So...
[00:43:10.840 --> 00:43:13.040]   He was first, in a way.
[00:43:13.040 --> 00:43:14.040]   Yeah.
[00:43:14.040 --> 00:43:15.040]   I don't know.
[00:43:15.040 --> 00:43:19.440]   I don't have his permission to repeat what we talked about, so I feel like I probably
[00:43:19.440 --> 00:43:20.440]   shouldn't.
[00:43:20.440 --> 00:43:22.240]   I don't know that it was off the record, but...
[00:43:22.240 --> 00:43:23.240]   Right.
[00:43:23.240 --> 00:43:27.040]   I think generally speaking, I think Oculus...
[00:43:27.040 --> 00:43:32.840]   I think Oculus is one of the few head-mounted displays that are out, that are functional,
[00:43:32.840 --> 00:43:37.320]   that are great, that have some type of ecosystem, but the market's not quite there.
[00:43:37.320 --> 00:43:42.680]   And I think part of what they were facing, I think part of what every company faces is,
[00:43:42.680 --> 00:43:47.520]   you've got maybe a great product that probably needs a little bit more time, and you've got
[00:43:47.520 --> 00:43:49.000]   a tiny market size.
[00:43:49.000 --> 00:43:55.200]   So are you going to throw all of your effort into developing for a product that's just
[00:43:55.200 --> 00:43:58.640]   like probably not going to reach critical mass, or are you going to try and do something
[00:43:58.640 --> 00:43:59.640]   else?
[00:43:59.640 --> 00:44:01.960]   If it's me, I'm going to try and do something else.
[00:44:01.960 --> 00:44:08.840]   In a way, Second Life was too early, the technology wasn't ready for it.
[00:44:08.840 --> 00:44:13.280]   Yeah, and somewhat ironically, I remember Reuters doing...
[00:44:13.280 --> 00:44:15.200]   I remember hearing about the first...
[00:44:15.200 --> 00:44:19.880]   I was probably still a journalist at that point when Reuters did its big press conference
[00:44:19.880 --> 00:44:20.880]   inside of Second Life.
[00:44:20.880 --> 00:44:21.880]   Do you remember that?
[00:44:21.880 --> 00:44:22.880]   Yeah.
[00:44:22.880 --> 00:44:23.880]   I think they were the first ones.
[00:44:23.880 --> 00:44:24.880]   Yeah.
[00:44:24.880 --> 00:44:26.680]   Consulates opened in Second Life.
[00:44:26.680 --> 00:44:27.680]   Right.
[00:44:27.680 --> 00:44:30.120]   And now the city of Seoul, everybody somehow has forgotten this.
[00:44:30.120 --> 00:44:36.320]   So the city of Seoul is opening up a metaverse for city services.
[00:44:36.320 --> 00:44:41.480]   Why not just do what Estonia has done, which is just like, make the digital stuff work.
[00:44:41.480 --> 00:44:43.280]   Estonia doesn't need a metaverse.
[00:44:43.280 --> 00:44:48.840]   Estonia has best in class, hyper-futuristic digital services and tools for its 4 million
[00:44:48.840 --> 00:44:49.840]   residents.
[00:44:49.840 --> 00:44:55.000]   So it's a teeny tiny little country in North Central Europe, but they've done amazing things
[00:44:55.000 --> 00:44:57.400]   without needing a metaverse to support all of it.
[00:44:57.400 --> 00:44:58.400]   So we'll see.
[00:44:58.400 --> 00:45:05.280]   I don't need to be super down on the metaverse, but I also think I'm concerned because I'm
[00:45:05.280 --> 00:45:09.400]   seeing all of this money and time and effort and resources being thrown at this.
[00:45:09.400 --> 00:45:15.800]   And I think that what people are forgetting is the metaverse, like many other huge things
[00:45:15.800 --> 00:45:18.840]   as they are being built is still picks and shovels stage.
[00:45:18.840 --> 00:45:23.400]   I feel like people are putting the cart before the horse.
[00:45:23.400 --> 00:45:25.880]   There's infrastructure that has to get built.
[00:45:25.880 --> 00:45:33.600]   I saw the most asinine graphic from a professional from Gartner.
[00:45:33.600 --> 00:45:35.160]   I'll just say who it was.
[00:45:35.160 --> 00:45:38.480]   It was the dumbest, it was the most ridiculous chart that I've ever seen.
[00:45:38.480 --> 00:45:40.040]   And it was like, this is the metaverse.
[00:45:40.040 --> 00:45:43.160]   And it was just like, it just had random words on it.
[00:45:43.160 --> 00:45:47.720]   Like, like, maybe they use GPT-3 to do it.
[00:45:47.720 --> 00:45:48.720]   Maybe.
[00:45:48.720 --> 00:45:52.200]   That makes more sense for sure.
[00:45:52.200 --> 00:45:53.200]   But that's my point.
[00:45:53.200 --> 00:45:55.200]   Why does that thing exist?
[00:45:55.200 --> 00:45:58.760]   Because decision makers at companies feel like they're being driven by fear right now.
[00:45:58.760 --> 00:46:01.880]   They feel like they've missed it and they don't know what it is.
[00:46:01.880 --> 00:46:04.560]   And the thing is that it isn't a thing yet.
[00:46:04.560 --> 00:46:06.760]   It's in the process of being built.
[00:46:06.760 --> 00:46:07.760]   So.
[00:46:07.760 --> 00:46:14.680]   If you advise companies all the time on strategy and so forth, Facebook's in a unique position
[00:46:14.680 --> 00:46:19.040]   to kind of, and I think really what they've done is they've propped up this Nick Clegg
[00:46:19.040 --> 00:46:21.000]   doll on the ramparts.
[00:46:21.000 --> 00:46:23.120]   And they've said, go ahead, Nick.
[00:46:23.120 --> 00:46:29.840]   We'll be down here working on stuff out of sight, out of mind, one hopes.
[00:46:29.840 --> 00:46:37.520]   If you were advising a company on that has cash flow, right, that has a lot of resources,
[00:46:37.520 --> 00:46:39.520]   very smart people.
[00:46:39.520 --> 00:46:42.440]   And I mean, they're looking for the next big thing, right?
[00:46:42.440 --> 00:46:44.120]   I mean, that's what you do in Silicon Valley.
[00:46:44.120 --> 00:46:46.040]   And it's historically very hard to do.
[00:46:46.040 --> 00:46:48.280]   There's the innovators dilemma.
[00:46:48.280 --> 00:46:51.560]   And if you're advising a company saying, well, the innovators dilemma, you know, you
[00:46:51.560 --> 00:46:58.760]   have to be willing to burn your bridges behind you, burn the Cortezes, burn the ships.
[00:46:58.760 --> 00:47:01.960]   Isn't that what he's doing?
[00:47:01.960 --> 00:47:02.960]   I don't know.
[00:47:02.960 --> 00:47:06.400]   I honestly, I'm trying to, again, I'm trying to think through what's the.
[00:47:06.400 --> 00:47:10.200]   Would you ever advise a company to do that?
[00:47:10.200 --> 00:47:13.400]   What we do is long range forecasting.
[00:47:13.400 --> 00:47:18.000]   So 10 years from now, what are the factors in play and how do you work backwards?
[00:47:18.000 --> 00:47:19.000]   Right.
[00:47:19.000 --> 00:47:20.640]   So I, you know, we, we've done that.
[00:47:20.640 --> 00:47:22.720]   I bet he's done that.
[00:47:22.720 --> 00:47:26.960]   Maybe I know that Facebook doesn't have a central foresight division.
[00:47:26.960 --> 00:47:29.240]   No, it's got Mark Zuckerberg.
[00:47:29.240 --> 00:47:31.240]   That's his job.
[00:47:31.240 --> 00:47:36.120]   So if that's what they're planning, I, again, I just would be very curious to know what are
[00:47:36.120 --> 00:47:42.760]   those defined use cases and how do they improve upon what exists?
[00:47:42.760 --> 00:47:47.200]   Because if there's not some type of improvement, then it, you, I think you go the way of foresquare.
[00:47:47.200 --> 00:47:51.200]   Goala and scavenger that had no vowels.
[00:47:51.200 --> 00:47:55.120]   You know, did, I mean, we're still making a lot of money selling.
[00:47:55.120 --> 00:48:00.480]   They are, but they are, but what they are not doing is game of the gamification layer
[00:48:00.480 --> 00:48:01.800]   in the badges, right?
[00:48:01.800 --> 00:48:04.960]   That was the shiny object and people, this is my point.
[00:48:04.960 --> 00:48:08.680]   If you understand what to track and you have to do that using data, not your instinct and
[00:48:08.680 --> 00:48:12.080]   gut, then you would have known that location based services or what matter.
[00:48:12.080 --> 00:48:15.640]   That was the long term longitudinal trend, not the badges.
[00:48:15.640 --> 00:48:20.760]   So I'm having a hard time as an outsider thinking about what's going on at Facebook and what
[00:48:20.760 --> 00:48:25.760]   they think the longitudinal trend is that's driving the decision making around the metaverse,
[00:48:25.760 --> 00:48:29.960]   because it's definitely not the cartoon avatars that we see bouncing around today.
[00:48:29.960 --> 00:48:30.960]   No.
[00:48:30.960 --> 00:48:35.160]   So if I'm them, I'm starting to do some of the things they've done, which is kind of get
[00:48:35.160 --> 00:48:39.040]   people used to the idea, knowing full well that what we're showing them is not going
[00:48:39.040 --> 00:48:41.120]   to be what the final product is.
[00:48:41.120 --> 00:48:47.200]   Let's kind of get them used to it, start to build demand, continue the existing product
[00:48:47.200 --> 00:48:52.560]   to the degree that he was smart because he's picking a product that meshes nicely with
[00:48:52.560 --> 00:48:53.560]   existing product.
[00:48:53.560 --> 00:48:59.960]   It's not a case of cannibalizing the blue Facebook to make meta.
[00:48:59.960 --> 00:49:04.920]   In fact, he could probably easily transition people once he's got something.
[00:49:04.920 --> 00:49:06.920]   So they've got a lot of technology problems to solve.
[00:49:06.920 --> 00:49:08.440]   So that's what they're doing.
[00:49:08.440 --> 00:49:10.440]   Let's solve these problems.
[00:49:10.440 --> 00:49:11.440]   Let's start building up on this.
[00:49:11.440 --> 00:49:16.360]   What Apple has been doing for the last five years now that we look back in hindsight, they
[00:49:16.360 --> 00:49:20.440]   have been slowly adding technologies so that when the time comes in the year next year
[00:49:20.440 --> 00:49:26.200]   or two to introduce some sort of augmented reality headset, they've got everything in
[00:49:26.200 --> 00:49:27.200]   place.
[00:49:27.200 --> 00:49:31.160]   They've got developers in place and they've even got their users somewhat accustomed to
[00:49:31.160 --> 00:49:32.680]   the idea.
[00:49:32.680 --> 00:49:35.160]   I think this is a new kind of 4D chess.
[00:49:35.160 --> 00:49:39.280]   I'm going to give them credit.
[00:49:39.280 --> 00:49:46.080]   They're thinking about this and we don't know what they're in your heart of hearts what
[00:49:46.080 --> 00:49:47.080]   they're thinking.
[00:49:47.080 --> 00:49:48.600]   But I'm going to give them some credit.
[00:49:48.600 --> 00:49:51.520]   They might have some idea here what they're doing.
[00:49:51.520 --> 00:49:54.920]   Aren't they also working on some brainless sheen interface stuff too?
[00:49:54.920 --> 00:49:55.920]   That's Elon.
[00:49:55.920 --> 00:49:57.560]   That's neural link.
[00:49:57.560 --> 00:49:59.760]   Yes, but it is neural link.
[00:49:59.760 --> 00:50:01.360]   Didn't some it is.
[00:50:01.360 --> 00:50:03.160]   Past subjects die?
[00:50:03.160 --> 00:50:04.400]   They're animals though, not people.
[00:50:04.400 --> 00:50:05.400]   Monkeys, not people.
[00:50:05.400 --> 00:50:06.400]   Neural link.
[00:50:06.400 --> 00:50:07.400]   Monkeys.
[00:50:07.400 --> 00:50:08.400]   You know what?
[00:50:08.400 --> 00:50:10.400]   You've got to kill a few monkeys.
[00:50:10.400 --> 00:50:11.400]   Make a problem.
[00:50:11.400 --> 00:50:12.400]   Make a brag.
[00:50:12.400 --> 00:50:13.400]   A brain.
[00:50:13.400 --> 00:50:16.040]   You've got to have them at a verse without some dead monkeys.
[00:50:16.040 --> 00:50:19.360]   I thought Facebook is invested in or doing some type of brain.
[00:50:19.360 --> 00:50:20.360]   I think they are.
[00:50:20.360 --> 00:50:22.360]   I think it's a division of Facebook.
[00:50:22.360 --> 00:50:24.080]   Anyhow, that might make sense.
[00:50:24.080 --> 00:50:30.160]   If he's gone out 10, 15 years and is thinking through, is there a way to think, you know,
[00:50:30.160 --> 00:50:31.720]   to sort of have a human interface?
[00:50:31.720 --> 00:50:33.400]   That's what you need is a neural interface.
[00:50:33.400 --> 00:50:34.640]   Yeah, you need a jacket.
[00:50:34.640 --> 00:50:39.280]   Then maybe the metaverse stuff starts to make a little bit more sense.
[00:50:39.280 --> 00:50:45.600]   That's, again, like forget the cartoon avatars, but maybe it's more thinking, you know, to
[00:50:45.600 --> 00:50:46.600]   people.
[00:50:46.600 --> 00:50:49.080]   So maybe that's what's on the horizon, possibly.
[00:50:49.080 --> 00:50:51.840]   That's what, by the way, I just read that that's what William Gibson wanted to call
[00:50:51.840 --> 00:50:53.040]   "neuromancer."
[00:50:53.040 --> 00:50:57.600]   Jack Inne, but his publishers wouldn't let him do it for obvious.
[00:50:57.600 --> 00:51:01.240]   No, that would be.
[00:51:01.240 --> 00:51:03.640]   The reviews write themselves.
[00:51:03.640 --> 00:51:04.640]   Let's take a little break.
[00:51:04.640 --> 00:51:06.680]   I think this is the best panel in a while.
[00:51:06.680 --> 00:51:11.240]   I'm going to only give them tough, brainy issues to discuss.
[00:51:11.240 --> 00:51:12.560]   Paris Martin knows here.
[00:51:12.560 --> 00:51:16.160]   She writes it the information about Amazon and other big tech.
[00:51:16.160 --> 00:51:18.360]   Wonderful to have you, Paris.
[00:51:18.360 --> 00:51:20.400]   I've not killed any monkeys.
[00:51:20.400 --> 00:51:22.120]   No monkeys have died in the making of this program.
[00:51:22.120 --> 00:51:25.160]   No monkeys have died in the making of this podcast.
[00:51:25.160 --> 00:51:28.520]   No, or that bedazzled mannequin behind you.
[00:51:28.520 --> 00:51:29.520]   Absolutely.
[00:51:29.520 --> 00:51:36.480]   Also, Amy Webb, author of a brand new book, The Genesis Machine, which is a must buy,
[00:51:36.480 --> 00:51:40.200]   a must read, just fantastic.
[00:51:40.200 --> 00:51:41.760]   You know, it's funny.
[00:51:41.760 --> 00:51:48.160]   I've known you for a while now, Amy, and I guess familiarity breeds not contempt, but
[00:51:48.160 --> 00:51:49.160]   just kind of complacency.
[00:51:49.160 --> 00:51:50.160]   I don't have contempt.
[00:51:50.160 --> 00:51:55.800]   I don't have contempt for you, but I just, you know, you're a...
[00:51:55.800 --> 00:51:56.800]   It's Amy.
[00:51:56.800 --> 00:51:57.800]   She's great.
[00:51:57.800 --> 00:52:00.280]   And then I read your bio in the back of the book.
[00:52:00.280 --> 00:52:03.600]   She's in a professor at the Stern School at NYU.
[00:52:03.600 --> 00:52:05.640]   And I mean, you're a fellow of all the...
[00:52:05.640 --> 00:52:07.720]   You're just like some big shot.
[00:52:07.720 --> 00:52:09.320]   And I just kind of lost sight of all that.
[00:52:09.320 --> 00:52:11.040]   And I just think, well, it's Amy.
[00:52:11.040 --> 00:52:13.160]   So big shot over here.
[00:52:13.160 --> 00:52:15.920]   She's a super big brain.
[00:52:15.920 --> 00:52:18.560]   And it's great to have her from the Future Today Institute.
[00:52:18.560 --> 00:52:21.080]   They're working on their annual report.
[00:52:21.080 --> 00:52:24.080]   I can't wait that comes out next month.
[00:52:24.080 --> 00:52:26.280]   And some very interesting things in that.
[00:52:26.280 --> 00:52:27.280]   We'll talk about that.
[00:52:27.280 --> 00:52:28.480]   I have you on when that comes out.
[00:52:28.480 --> 00:52:29.480]   That's fun.
[00:52:29.480 --> 00:52:32.480]   I'll show today brought to you by New Relic.
[00:52:32.480 --> 00:52:33.800]   You know that routine.
[00:52:33.800 --> 00:52:39.640]   If you're a software engineer, if you run a network, if you're in DevOps, you're in bed.
[00:52:39.640 --> 00:52:40.640]   It's late at night.
[00:52:40.640 --> 00:52:41.640]   You're tired.
[00:52:41.640 --> 00:52:43.320]   Suddenly the phone rings.
[00:52:43.320 --> 00:52:44.800]   The buzzer goes off.
[00:52:44.800 --> 00:52:47.760]   The teletype chatters, whatever it is that you've got alerting you.
[00:52:47.760 --> 00:52:49.040]   And there's a problem.
[00:52:49.040 --> 00:52:50.040]   The network's down.
[00:52:50.040 --> 00:52:51.040]   Something's broken.
[00:52:51.040 --> 00:52:52.280]   Your software is not working.
[00:52:52.280 --> 00:52:53.280]   Is it...
[00:52:53.280 --> 00:52:54.280]   Then your mind starts racing, right?
[00:52:54.280 --> 00:52:56.080]   Three in the morning, you're going, is it the back end?
[00:52:56.080 --> 00:52:57.080]   Is it the front end?
[00:52:57.080 --> 00:52:59.080]   Is it that commit I just pushed yesterday?
[00:52:59.080 --> 00:53:01.440]   Oh my God, what happened?
[00:53:01.440 --> 00:53:03.600]   And now you've got your whole team running around.
[00:53:03.600 --> 00:53:06.480]   They're firing up tool after tool messaging each other.
[00:53:06.480 --> 00:53:10.080]   Do we have slow running queries?
[00:53:10.080 --> 00:53:11.640]   Is the cloud provider down?
[00:53:11.640 --> 00:53:13.840]   What's going on?
[00:53:13.840 --> 00:53:15.000]   That's the worst.
[00:53:15.000 --> 00:53:16.000]   And we've all been...
[00:53:16.000 --> 00:53:18.600]   I know you know what that's like.
[00:53:18.600 --> 00:53:23.080]   And part of that is because organizations are not implementing observability for their
[00:53:23.080 --> 00:53:24.560]   networks and systems.
[00:53:24.560 --> 00:53:28.800]   New Relic did a survey, only half of all the organizations they talked to are actually
[00:53:28.800 --> 00:53:30.960]   putting in observability.
[00:53:30.960 --> 00:53:34.000]   And I think it's so important.
[00:53:34.000 --> 00:53:41.440]   Maintaining network observability is so important to keeping your stack running, to keeping your
[00:53:41.440 --> 00:53:43.640]   nerves from going crazy.
[00:53:43.640 --> 00:53:45.800]   New Relic's the solution.
[00:53:45.800 --> 00:53:48.880]   New Relic engineers love it.
[00:53:48.880 --> 00:53:50.800]   Combine 16 different monitoring products.
[00:53:50.800 --> 00:53:54.520]   I know you know the name New Relic, but maybe you haven't checked them out lately.
[00:53:54.520 --> 00:53:57.400]   These are products that you would normally buy separately.
[00:53:57.400 --> 00:54:03.320]   But now it's in one software stack that gives you observability across the entire enterprise.
[00:54:03.320 --> 00:54:05.840]   You get application monitoring, APM.
[00:54:05.840 --> 00:54:09.600]   Unified monitoring for all your apps, all your microservices.
[00:54:09.600 --> 00:54:11.600]   Use Kubernetes, you'll love PIXI.
[00:54:11.600 --> 00:54:13.640]   Observability.
[00:54:13.640 --> 00:54:15.920]   You get distributed tracing.
[00:54:15.920 --> 00:54:20.240]   So you can see all your traces, no management headaches, fine and fixed issues fast.
[00:54:20.240 --> 00:54:22.680]   You get network performance monitoring.
[00:54:22.680 --> 00:54:25.920]   So you don't have to check 12 different data silos.
[00:54:25.920 --> 00:54:28.960]   You can stop guessing where the issues start.
[00:54:28.960 --> 00:54:31.560]   You get a system-wide correlated view.
[00:54:31.560 --> 00:54:34.000]   You got to go to new relic.com and take a look at this.
[00:54:34.000 --> 00:54:36.200]   New Relic.com/twit.
[00:54:36.200 --> 00:54:39.600]   That's just four of the 16 tools.
[00:54:39.600 --> 00:54:43.600]   And more importantly, you can pinpoint an issue down to the line of code.
[00:54:43.600 --> 00:54:48.080]   So you know exactly what happened and you can resolve it quickly and go back to bed.
[00:54:48.080 --> 00:54:54.000]   That's why the Dev and Ops teams at DoorDash and GitHub and Epic Games and more than 14,000
[00:54:54.000 --> 00:54:58.200]   other companies use New Relic to debug and improve software.
[00:54:58.200 --> 00:55:00.160]   And here's the best part.
[00:55:00.160 --> 00:55:01.160]   Here's the best part.
[00:55:01.160 --> 00:55:02.680]   It's free.
[00:55:02.680 --> 00:55:03.880]   It's free.
[00:55:03.880 --> 00:55:07.960]   Whether you run a cloud native startup or a Fortune 500 company, in five minutes you'll
[00:55:07.960 --> 00:55:11.040]   have New Relic up and running before that next 3am call.
[00:55:11.040 --> 00:55:12.560]   Just waiting to happen.
[00:55:12.560 --> 00:55:13.880]   So you get New Relic.
[00:55:13.880 --> 00:55:16.880]   Go to new relic.com/twit.
[00:55:16.880 --> 00:55:22.360]   You can get access to the whole New Relic platform and 100GB of data free for no credit
[00:55:22.360 --> 00:55:26.560]   card required because they know you're going to love New Relic and you're then going to
[00:55:26.560 --> 00:55:29.120]   go to the boss and say we have to have this everywhere.
[00:55:29.120 --> 00:55:32.120]   We have to go all in.
[00:55:32.120 --> 00:55:35.720]   There's no reason not to do this right now.
[00:55:35.720 --> 00:55:37.240]   You can do it while you're listening.
[00:55:37.240 --> 00:55:39.160]   Any W.R.E.L.I.C.
[00:55:39.160 --> 00:55:42.160]   New Relic.com/twit.
[00:55:42.160 --> 00:55:46.840]   The next time the phone rings at 3am, you can calmly go to the New Relic dashboard.
[00:55:46.840 --> 00:55:48.760]   Say, here's the problem.
[00:55:48.760 --> 00:55:51.120]   Let's fix this and be back to bed in no time.
[00:55:51.120 --> 00:55:54.640]   New Relic.com/twit.
[00:55:54.640 --> 00:55:55.640]   You want this.
[00:55:55.640 --> 00:55:56.640]   You know you need this.
[00:55:56.640 --> 00:55:57.880]   You've got to have it.
[00:55:57.880 --> 00:55:58.880]   Erelik.
[00:55:58.880 --> 00:56:01.360]   The Nujact.
[00:56:01.360 --> 00:56:03.080]   I like to call it the Nujact.
[00:56:03.080 --> 00:56:08.840]   This is a new bill introduced by Senator Klobuchar that might actually be another one
[00:56:08.840 --> 00:56:13.720]   of those headwinds blowing against Facebook.
[00:56:13.720 --> 00:56:21.400]   The Social Media Nudge Act, which is sponsored by Senator Klobuchar and Cynthia Loomis of
[00:56:21.400 --> 00:56:25.440]   Wyoming, is kind of interesting.
[00:56:25.440 --> 00:56:27.480]   I might say cookie.
[00:56:27.480 --> 00:56:32.120]   It would direct the National Science Foundation and the National Academy of Sciences Engineering
[00:56:32.120 --> 00:56:42.560]   and Medicine to study "content neutral ways to slow down the spread of misinformation."
[00:56:42.560 --> 00:56:48.560]   The FTC would get the recommendations, codify them, mandate that Facebook and Twitter and
[00:56:48.560 --> 00:56:53.760]   other social media platforms, put them into practice.
[00:56:53.760 --> 00:56:59.920]   Is it even conceivable that the NSF and the National Academy of Sciences Engineering and
[00:56:59.920 --> 00:57:05.840]   Medicine and ASM could come up with best practices to add friction to content sharing online
[00:57:05.840 --> 00:57:09.120]   that would make any sense at all?
[00:57:09.120 --> 00:57:10.120]   This makes me very mad.
[00:57:10.120 --> 00:57:12.120]   Let me tell you why.
[00:57:12.120 --> 00:57:13.120]   Good.
[00:57:13.120 --> 00:57:19.720]   Because in 2014, so this is eight years ago, no, 2013, sorry.
[00:57:19.720 --> 00:57:21.040]   I know where you're going with this.
[00:57:21.040 --> 00:57:23.320]   I know exactly where you're going with this.
[00:57:23.320 --> 00:57:30.880]   In 2013, I had more than one meeting at State Department with some folks, some higher-level
[00:57:30.880 --> 00:57:32.480]   folks and lower-level folks.
[00:57:32.480 --> 00:57:38.440]   I said, "Hey, there's a Twitter thing and there's misinformation bots and here's what
[00:57:38.440 --> 00:57:43.800]   a bot is and here's what a botnet is and here's how some of this stuff works."
[00:57:43.800 --> 00:57:50.640]   This is probably going to be sometime soon a channel for misinformation and that's something
[00:57:50.640 --> 00:57:54.080]   you ought to be paying attention to.
[00:57:54.080 --> 00:57:56.440]   Nobody cared.
[00:57:56.440 --> 00:58:02.760]   I think I'm a pretty effective communicator and I failed repeatedly to get anybody at
[00:58:02.760 --> 00:58:12.360]   all excited about the coming onslaught of misinformation and also ways for us to use.
[00:58:12.360 --> 00:58:19.520]   To think through how on Twitter and Facebook botnets might pop up and how they would work.
[00:58:19.520 --> 00:58:20.520]   I don't know.
[00:58:20.520 --> 00:58:25.000]   I think it's like 2022 and I feel like everybody's super frickin' late to the party on this
[00:58:25.000 --> 00:58:29.080]   one and it makes me really upset.
[00:58:29.080 --> 00:58:32.920]   I thought you were going to talk about the other thing that makes you really upset, which
[00:58:32.920 --> 00:58:36.600]   is the abandonment of the US Office of Technology Assessment.
[00:58:36.600 --> 00:58:38.000]   Well, there's that.
[00:58:38.000 --> 00:58:39.960]   Listen, we don't have a list.
[00:58:39.960 --> 00:58:43.560]   We don't have a long list of things that piss Amy off.
[00:58:43.560 --> 00:58:46.760]   I think Senator Klobuchar is great.
[00:58:46.760 --> 00:58:49.640]   Her heart is in the right place.
[00:58:49.640 --> 00:58:53.240]   But I also think like what the hell has everybody been doing?
[00:58:53.240 --> 00:58:59.720]   Yeah, so I know I was not the only one like shopping this around the hill.
[00:58:59.720 --> 00:59:05.920]   We don't have an OSTP office of, or we don't have an OSTP, oh my God, so many acronyms.
[00:59:05.920 --> 00:59:09.880]   We don't have the Office of Tech Assessment.
[00:59:09.880 --> 00:59:15.640]   The OSTP finally has a new person in charge, but they're, you know, we've just like kicked
[00:59:15.640 --> 00:59:17.640]   the can down the road too many times.
[00:59:17.640 --> 00:59:19.440]   So I don't know.
[00:59:19.440 --> 00:59:20.440]   It's just the Office of Science.
[00:59:20.440 --> 00:59:21.440]   Super duper hope.
[00:59:21.440 --> 00:59:26.040]   Technology policy kind of take the place of the old Office of Science.
[00:59:26.040 --> 00:59:34.160]   No, no, because OST was supposed to advise Congress, right?
[00:59:34.160 --> 00:59:40.400]   It was supposed to be like the GAO, a nonpartisan technology group of technology experts that
[00:59:40.400 --> 00:59:43.600]   would help Congress understand these difficult issues.
[00:59:43.600 --> 00:59:44.600]   Right.
[00:59:44.600 --> 00:59:51.320]   So I'm actually a fellow at the GAO on foresight, but the GAO's mandate is about auditing.
[00:59:51.320 --> 00:59:53.680]   It's not about, it's not really about foresight.
[00:59:53.680 --> 00:59:56.480]   But it's nonpartisan, right?
[00:59:56.480 --> 01:00:01.840]   It's supposed to be a trusting, a trusted authority that is without an extra grind,
[01:00:01.840 --> 01:00:02.840]   in other words.
[01:00:02.840 --> 01:00:03.840]   Correct.
[01:00:03.840 --> 01:00:07.080]   Yes, no, but everybody else has an extra right with them because they're auditors.
[01:00:07.080 --> 01:00:09.880]   Like nobody wants, right?
[01:00:09.880 --> 01:00:13.680]   So they are doing work and they're trying to become a central hub for foresight, but
[01:00:13.680 --> 01:00:17.680]   like nobody wants GAO coming knocking on the door and saying, "Hey, let's talk about your
[01:00:17.680 --> 01:00:18.680]   long-term plans."
[01:00:18.680 --> 01:00:19.680]   Right.
[01:00:19.680 --> 01:00:20.680]   So we're both auditing.
[01:00:20.680 --> 01:00:28.000]   The Office of Technology Assessment was the group that was responsible for doing research
[01:00:28.000 --> 01:00:33.160]   abs, you know, without politics involved on thorny areas over long periods of time.
[01:00:33.160 --> 01:00:37.240]   And when they were around, they produced a lot of terrific research.
[01:00:37.240 --> 01:00:43.800]   We are in this situation in, you know, we are highly polarized, "Oh, did you see this?"
[01:00:43.800 --> 01:00:48.560]   The Times, like late last night published maybe Para saw this.
[01:00:48.560 --> 01:00:51.160]   They think they know one of the originators of QAnon.
[01:00:51.160 --> 01:00:52.160]   Oh.
[01:00:52.160 --> 01:00:56.760]   They know the two originators of QAnon, which I believe has already been.
[01:00:56.760 --> 01:00:57.760]   Is it the father and son?
[01:00:57.760 --> 01:00:58.760]   One of them is running for...
[01:00:58.760 --> 01:00:59.760]   One of them is Ron Watkins.
[01:00:59.760 --> 01:01:00.760]   Ron Watkins.
[01:01:00.760 --> 01:01:01.760]   We always thought it was him.
[01:01:01.760 --> 01:01:02.760]   Yeah.
[01:01:02.760 --> 01:01:03.760]   Yeah.
[01:01:03.760 --> 01:01:12.240]   Who has been in charge of kind of the platform behind it, but the other one is I'm forgetting
[01:01:12.240 --> 01:01:18.240]   his name, but he was an original like common-durn poster on the original forum.
[01:01:18.240 --> 01:01:25.440]   Watkins, according to the documentary, kind of took over the QAnon account when it moved
[01:01:25.440 --> 01:01:30.040]   off 4chan to 8chan Watkins platform.
[01:01:30.040 --> 01:01:34.440]   So this, I guess this other guy is the forerunner who...
[01:01:34.440 --> 01:01:37.840]   And this is based on linguistic analysis.
[01:01:37.840 --> 01:01:38.840]   Interesting.
[01:01:38.840 --> 01:01:39.840]   Right.
[01:01:39.840 --> 01:01:46.040]   But again, my point is one of them is running for Congress in Arizona.
[01:01:46.040 --> 01:01:47.040]   Paul Ferlberg.
[01:01:47.040 --> 01:01:48.720]   How do we...
[01:01:48.720 --> 01:01:52.440]   It's inexcusable that we have gotten ourselves to this place, I think.
[01:01:52.440 --> 01:01:57.280]   Like, I don't care what your political beliefs are, just the fact that politics has come...
[01:01:57.280 --> 01:02:01.000]   Like, the problem that I have is just that politics is so down.
[01:02:01.000 --> 01:02:03.040]   Oh, Ron Watkins is running for Congress.
[01:02:03.040 --> 01:02:04.040]   Oh, my God.
[01:02:04.040 --> 01:02:05.040]   Yeah.
[01:02:05.040 --> 01:02:06.040]   In Arizona.
[01:02:06.040 --> 01:02:07.040]   And like...
[01:02:07.040 --> 01:02:10.040]   No, but this is the point.
[01:02:10.040 --> 01:02:14.360]   Like, we are in a situation that was avoidable.
[01:02:14.360 --> 01:02:15.360]   I keep hoping...
[01:02:15.360 --> 01:02:19.320]   You have a reason Ferlberg is not running for Congress, he's from South Africa.
[01:02:19.320 --> 01:02:20.320]   I keep...
[01:02:20.320 --> 01:02:21.320]   That would put a...
[01:02:21.320 --> 01:02:22.320]   Yeah, it makes it hard.
[01:02:22.320 --> 01:02:23.880]   You know, it's a long-distance relationship.
[01:02:23.880 --> 01:02:26.120]   You know, it's never work.
[01:02:26.120 --> 01:02:29.960]   I keep thinking that we're all going to come to our senses.
[01:02:29.960 --> 01:02:32.920]   Oh, that's the first mistake, Leah.
[01:02:32.920 --> 01:02:35.640]   Our senses are long gone.
[01:02:35.640 --> 01:02:37.720]   And then people will just go, "What?
[01:02:37.720 --> 01:02:38.720]   Oh, what?
[01:02:38.720 --> 01:02:41.480]   Oh, no, that was nuts.
[01:02:41.480 --> 01:02:42.480]   Pizza gate?
[01:02:42.480 --> 01:02:43.480]   What?
[01:02:43.480 --> 01:02:44.480]   No, no, no.
[01:02:44.480 --> 01:02:46.840]   What can I elect Ron Watkins?"
[01:02:46.840 --> 01:02:50.600]   But Klobuchar, I think part of this legislation is in response.
[01:02:50.600 --> 01:02:52.320]   I don't think it's just about the platforms.
[01:02:52.320 --> 01:02:53.320]   It's about...
[01:02:53.320 --> 01:02:56.680]   It's this sort of bigger picture thing.
[01:02:56.680 --> 01:02:59.440]   And again, how did we get to this point?
[01:02:59.440 --> 01:03:02.240]   I think we got to this point because we just didn't...
[01:03:02.240 --> 01:03:04.920]   There wasn't a plan and we didn't have a process in place.
[01:03:04.920 --> 01:03:10.360]   And I'm not saying we need to plan everything, but it would be good at least to run and develop
[01:03:10.360 --> 01:03:13.440]   some scenarios and then work backwards from those.
[01:03:13.440 --> 01:03:16.880]   I think it's a big mistake to blame the platforms, honestly.
[01:03:16.880 --> 01:03:19.720]   I've come around to this.
[01:03:19.720 --> 01:03:22.640]   They're just a place where people express themselves.
[01:03:22.640 --> 01:03:26.240]   The problem lies in the people, not the platforms.
[01:03:26.240 --> 01:03:30.480]   I mean, this is the gunmaker defense, correct?
[01:03:30.480 --> 01:03:34.600]   Yeah, but the case part of it is that you have platforms...
[01:03:34.600 --> 01:03:39.120]   Well, I mean, you have platforms that are incentivized to...
[01:03:39.120 --> 01:03:40.320]   I guess you're right.
[01:03:40.320 --> 01:03:43.760]   Like, have people on them as much as possible?
[01:03:43.760 --> 01:03:50.280]   Facebook for a long time was built specifically to have its algorithm kind of feed you things
[01:03:50.280 --> 01:03:54.080]   that got more emoji reactions beyond just the like.
[01:03:54.080 --> 01:03:59.680]   And it ended up being that some of the emoji reactions that were weighted even higher
[01:03:59.680 --> 01:04:02.040]   ended up being things like the angry reaction.
[01:04:02.040 --> 01:04:06.800]   So of course, that ends up meaning in practice that people in their feeds are increasingly
[01:04:06.800 --> 01:04:12.400]   seeing content that makes them extremely emotional in a negative way and then generates
[01:04:12.400 --> 01:04:14.640]   comments calling that out.
[01:04:14.640 --> 01:04:16.240]   And frankly, so do the news.
[01:04:16.240 --> 01:04:17.240]   So do the news.
[01:04:17.240 --> 01:04:20.640]   So the 24 hour news networks do exactly the same thing.
[01:04:20.640 --> 01:04:22.880]   Your local news at 11 does exactly the same.
[01:04:22.880 --> 01:04:25.320]   They know what drives engagement.
[01:04:25.320 --> 01:04:26.320]   They always...
[01:04:26.320 --> 01:04:27.320]   Absolutely.
[01:04:27.320 --> 01:04:30.760]   I mean, it's a problem bigger than social networks, but I mean, the thing that I guess
[01:04:30.760 --> 01:04:35.400]   to circle back to original point of view, the thing that makes me angry about this nudge
[01:04:35.400 --> 01:04:43.600]   fact or possible plan is that I don't think it's likely that a government agency reviewing
[01:04:43.600 --> 01:04:48.760]   these social networks to come up with a list of best practices for how to slow the sharing
[01:04:48.760 --> 01:04:54.320]   of content is ever going to result in any actionable change for these platforms.
[01:04:54.320 --> 01:04:59.200]   I mean, one, it seems unlikely that this bill would pass whatsoever given the amount of
[01:04:59.200 --> 01:05:04.120]   money in tech lobbying and the amount that companies like Facebook have to gain from
[01:05:04.120 --> 01:05:06.720]   something like this not being codified.
[01:05:06.720 --> 01:05:08.880]   But two, it just...
[01:05:08.880 --> 01:05:09.880]   We have...
[01:05:09.880 --> 01:05:13.320]   I've been stunned over the last couple of years watching all of these congressional
[01:05:13.320 --> 01:05:21.600]   hearings relating to tech at just how little the people in power seem to understand about
[01:05:21.600 --> 01:05:23.480]   how technology actually works.
[01:05:23.480 --> 01:05:24.480]   You know?
[01:05:24.480 --> 01:05:29.000]   I mean, this is how we get quotes like Mark Zuckerberg, will you commit to ending Finsta,
[01:05:29.000 --> 01:05:32.280]   which is what happened in a recent congressional hearing?
[01:05:32.280 --> 01:05:38.960]   I mean, it doesn't seem likely that this is going to produce something actionable and
[01:05:38.960 --> 01:05:39.960]   helpful.
[01:05:39.960 --> 01:05:45.280]   It almost feels like witchcraft like they're saying, "Well, we need to conduct a study
[01:05:45.280 --> 01:05:48.920]   and they're going to come up with a magic silver bullet and then we're going to force
[01:05:48.920 --> 01:05:52.160]   these platforms to adhere to it."
[01:05:52.160 --> 01:05:54.720]   It's almost a way of saying, "We don't know what to do.
[01:05:54.720 --> 01:05:56.360]   No, they do know what to do.
[01:05:56.360 --> 01:05:58.440]   They can't because we have a First Amendment."
[01:05:58.440 --> 01:06:01.080]   So this is the central problem here.
[01:06:01.080 --> 01:06:05.480]   Clearly, I don't think anybody among us thinks that we're in a good situation.
[01:06:05.480 --> 01:06:09.800]   I mean, maybe people are making money in the current situation or they people who love chaos,
[01:06:09.800 --> 01:06:14.400]   but I think the average person is like, "Yeah, this is not great."
[01:06:14.400 --> 01:06:22.200]   But if any of our lawmakers come in and attempt to impose any restrictions, if I'm one of
[01:06:22.200 --> 01:06:28.760]   the platforms, I'm going to mount a First Amendment freedom of speech defense and nobody wants
[01:06:28.760 --> 01:06:30.600]   to be in court on this.
[01:06:30.600 --> 01:06:39.960]   So the problem is that there's no way really to incent the platforms to change.
[01:06:39.960 --> 01:06:45.000]   That's the problem in our current setup.
[01:06:45.000 --> 01:06:48.080]   It's politically in expedient.
[01:06:48.080 --> 01:06:52.240]   But you want to mean as much noise about as you can just so that people think you're
[01:06:52.240 --> 01:06:53.600]   doing the right thing.
[01:06:53.600 --> 01:06:57.560]   In some way, it all ties back to our original topic of advertising because the reason why
[01:06:57.560 --> 01:07:06.120]   these platforms are set up to incentivize polarizing content or things that keep people
[01:07:06.120 --> 01:07:11.320]   engaged or in the comments, regardless of what the tone of that is, is because they need eyeballs
[01:07:11.320 --> 01:07:12.920]   on ads.
[01:07:12.920 --> 01:07:18.280]   And I think that if the bottom falls out on advertising because of more stringent privacy
[01:07:18.280 --> 01:07:22.360]   protections, that could be a step in the right direction.
[01:07:22.360 --> 01:07:28.600]   But it's obviously not as simple as passing a law that fixes the problem because the problem
[01:07:28.600 --> 01:07:31.240]   is very vast.
[01:07:31.240 --> 01:07:35.000]   There was a house bill, a similar house bill called Protecting Americans from Dangerous
[01:07:35.000 --> 01:07:36.560]   Algorithms Act.
[01:07:36.560 --> 01:07:41.400]   And I would agree that algorithms seem to be the problem, whether it's YouTube forcing
[01:07:41.400 --> 01:07:48.200]   people to have more and more extreme content or the Facebook news feed now called The Feed,
[01:07:48.200 --> 01:07:52.200]   not showing you the stuff from your family, but showing you the stuff that's going to
[01:07:52.200 --> 01:07:55.560]   make you the most angry or most upset.
[01:07:55.560 --> 01:07:57.080]   Twitter is an outrage engine.
[01:07:57.080 --> 01:08:02.440]   I don't think it's algorithmically driven to be that, but it's just kind of naturally
[01:08:02.440 --> 01:08:04.680]   become that.
[01:08:04.680 --> 01:08:06.920]   But I just think algorithms could be part of the problem.
[01:08:06.920 --> 01:08:11.320]   What if you had just a chronological feed for everything with that solvent?
[01:08:11.320 --> 01:08:17.720]   This has been my problem with all of these fits and starts.
[01:08:17.720 --> 01:08:19.080]   We're fighting yesterday's war.
[01:08:19.080 --> 01:08:20.080]   Right?
[01:08:20.080 --> 01:08:24.520]   So we're on the precipice of automated versioning at scale.
[01:08:24.520 --> 01:08:29.400]   So there's already some researchers at Virginia Tech, they created this algorithm that can
[01:08:29.400 --> 01:08:33.000]   break down an image into individual parts.
[01:08:33.000 --> 01:08:37.960]   And then they use a generative adversarial network again to sort of move things around.
[01:08:37.960 --> 01:08:42.360]   Really what this allows somebody to do is to take one picture of somebody modeling clothing
[01:08:42.360 --> 01:08:48.600]   and then reposition them and put different clothing on them immediately without any real
[01:08:48.600 --> 01:08:50.440]   rendering time or anything else.
[01:08:50.440 --> 01:08:57.200]   We've got deep fake dubs where you can already translate at scale just about anything in
[01:08:57.200 --> 01:09:00.840]   near real time and there are startups that are doing this.
[01:09:00.840 --> 01:09:12.000]   So my point is if you try to legislate this at the level of code, then it immediately
[01:09:12.000 --> 01:09:16.800]   becomes outdated the moment that it eventually gets passed, which is going to take forever
[01:09:16.800 --> 01:09:17.800]   anyways.
[01:09:17.800 --> 01:09:20.920]   We just need a different approach and punitive regulation, I don't think is the right way
[01:09:20.920 --> 01:09:23.760]   to do it because the market's going to win out.
[01:09:23.760 --> 01:09:25.280]   So there's got to be a different way.
[01:09:25.280 --> 01:09:30.640]   The other way I think to do it is to incentivize the companies to change by giving them economic
[01:09:30.640 --> 01:09:34.360]   incentives versus threatening them with punishments.
[01:09:34.360 --> 01:09:39.200]   But I know that's not a super popular viewpoint.
[01:09:39.200 --> 01:09:46.720]   Well the good news is tomorrow Trump's Truth Social will launch and all of this will become
[01:09:46.720 --> 01:09:47.720]   moot.
[01:09:47.720 --> 01:09:49.200]   How do you join it?
[01:09:49.200 --> 01:09:53.360]   If you have to be a $5 a week, you can join anybody who can join as long as you're willing
[01:09:53.360 --> 01:09:55.360]   to spend $5 a week.
[01:09:55.360 --> 01:09:56.360]   Yeah.
[01:09:56.360 --> 01:09:57.360]   Oh wait.
[01:09:57.360 --> 01:09:59.160]   That's more expensive than Twitter blue even.
[01:09:59.160 --> 01:10:00.760]   Oh yeah, lots more.
[01:10:00.760 --> 01:10:02.760]   Twitter blue is $3 a month, right?
[01:10:02.760 --> 01:10:04.240]   Or $5 a month, whatever it is.
[01:10:04.240 --> 01:10:05.240]   $5 a week.
[01:10:05.240 --> 01:10:06.680]   Oh my God, I thought you said $5 a month.
[01:10:06.680 --> 01:10:09.880]   That is quite a lot.
[01:10:09.880 --> 01:10:12.960]   Well the truth does not come cheaply.
[01:10:12.960 --> 01:10:17.000]   The truth is expensive.
[01:10:17.000 --> 01:10:20.160]   You may remember- I mean forget about freedom of speech.
[01:10:20.160 --> 01:10:23.760]   You may remember when they first launched this, there were some complaints from the
[01:10:23.760 --> 01:10:27.600]   open source community because it looked like they had just forked mastodon without giving
[01:10:27.600 --> 01:10:29.200]   credit.
[01:10:29.200 --> 01:10:35.560]   They immediately responded by posting a page that says, "We love open source.
[01:10:35.560 --> 01:10:37.320]   Open source is not big tech."
[01:10:37.320 --> 01:10:39.000]   I guess that's true.
[01:10:39.000 --> 01:10:42.480]   And here's the source code so they're compliant.
[01:10:42.480 --> 01:10:44.000]   So I guess it's still running on mastodon.
[01:10:44.000 --> 01:10:47.840]   We run our own mastodon instance, twitter.social.
[01:10:47.840 --> 01:10:52.800]   What would be, although I might rename it twitter.truth because I think that's a better name.
[01:10:52.800 --> 01:10:57.040]   But what I think is interesting about mastodon and I'm curious if they're going to allow
[01:10:57.040 --> 01:11:00.560]   this, is that you confederate mastodon instances.
[01:11:00.560 --> 01:11:05.080]   Unlike twitter, which is a silo, imagine a twitter but there's thousands of people running
[01:11:05.080 --> 01:11:08.080]   twitter servers, all of which can talk to each other.
[01:11:08.080 --> 01:11:14.760]   I'm hoping truth social is on the mastodon fetivers because then we could just, you know,
[01:11:14.760 --> 01:11:16.360]   we could join it with that.
[01:11:16.360 --> 01:11:17.360]   I mean to pay anything.
[01:11:17.360 --> 01:11:19.880]   We could just, you know, but I bet they don't do that.
[01:11:19.880 --> 01:11:20.880]   Is there a truth?
[01:11:20.880 --> 01:11:22.480]   Is there an API for truth?
[01:11:22.480 --> 01:11:28.440]   Oh yes, a well-known API for truth.
[01:11:28.440 --> 01:11:29.600]   There is from mastodon.
[01:11:29.600 --> 01:11:30.600]   I don't know.
[01:11:30.600 --> 01:11:31.600]   They might modify enough.
[01:11:31.600 --> 01:11:37.560]   They're set for app store release in the Apple app store on February 21st, which is,
[01:11:37.560 --> 01:11:42.360]   of course, President's Day in the United States.
[01:11:42.360 --> 01:11:43.840]   We shall see.
[01:11:43.840 --> 01:11:44.840]   We shall see.
[01:11:44.840 --> 01:11:50.200]   I find it really funny that the retweets on this platform are called retruths.
[01:11:50.200 --> 01:11:55.480]   If I recall correctly, the tweets or post version of this are just truths.
[01:11:55.480 --> 01:12:00.920]   It seems incredibly confusing.
[01:12:00.920 --> 01:12:04.640]   I would, I would, I don't know how I'm not going to pay a five bucks a week, but I would
[01:12:04.640 --> 01:12:12.760]   love to just, you know, if you, I guess Gab and Parler were not enough and we now need
[01:12:12.760 --> 01:12:13.760]   truth social.
[01:12:13.760 --> 01:12:18.960]   By the way, Devin Nunes, Congressman Devin Nunes, who retired to run this is in charge.
[01:12:18.960 --> 01:12:23.040]   So do they take, I'm assuming they take check, right?
[01:12:23.040 --> 01:12:24.640]   I can write a check every week.
[01:12:24.640 --> 01:12:26.440]   You can give her five dollars.
[01:12:26.440 --> 01:12:28.480]   Yeah, totally going to do that.
[01:12:28.480 --> 01:12:29.480]   Yeah, right.
[01:12:29.480 --> 01:12:30.480]   Send them pennies.
[01:12:30.480 --> 01:12:32.160]   Five, five, no, I just like.
[01:12:32.160 --> 01:12:33.160]   Trump branded states.
[01:12:33.160 --> 01:12:34.160]   Five thousand pennies.
[01:12:34.160 --> 01:12:35.160]   Yeah.
[01:12:35.160 --> 01:12:36.160]   There you go.
[01:12:36.160 --> 01:12:37.160]   No, a pain, Melania's NFT.
[01:12:37.160 --> 01:12:38.160]   I'll just.
[01:12:38.160 --> 01:12:39.160]   Oh, very important.
[01:12:39.160 --> 01:12:40.160]   Yeah.
[01:12:40.160 --> 01:12:41.160]   Yeah.
[01:12:41.160 --> 01:12:44.200]   Is, I read that she bought her own NFT.
[01:12:44.200 --> 01:12:45.200]   Is that what I read?
[01:12:45.200 --> 01:12:46.200]   Is that true?
[01:12:46.200 --> 01:12:49.360]   Well, but okay, that's we can make fun of this.
[01:12:49.360 --> 01:12:53.320]   However, let's also just note that that is part of the pyramid scheme that's happening
[01:12:53.320 --> 01:12:54.320]   right now.
[01:12:54.320 --> 01:12:55.320]   Yeah, people are buying their own.
[01:12:55.320 --> 01:12:56.320]   Which is, yeah.
[01:12:56.320 --> 01:12:57.320]   Right, right, right.
[01:12:57.320 --> 01:12:59.320]   So it's not like they're original.
[01:12:59.320 --> 01:13:05.640]   You know, I mean, to on the pyramid scheme level, there was recently a made off coin was
[01:13:05.640 --> 01:13:10.360]   the name of it in the, you know, much like Bernie made off.
[01:13:10.360 --> 01:13:11.360]   Sure.
[01:13:11.360 --> 01:13:12.360]   Was the name of this coin.
[01:13:12.360 --> 01:13:17.280]   Of course, turns out to be a scam couple weeks, months after people had invested and made
[01:13:17.280 --> 01:13:19.840]   off coin, they made off with it.
[01:13:19.840 --> 01:13:22.440]   What a surprise.
[01:13:22.440 --> 01:13:30.600]   You Bloomberg apparently realized that the entity that purchased Melania's Trump's NFT
[01:13:30.600 --> 01:13:35.240]   was the same wallet number that created it.
[01:13:35.240 --> 01:13:37.640]   So I think that's going to be a problem going forward.
[01:13:37.640 --> 01:13:42.120]   I don't think people understand that their wallets are, can be traced in our public.
[01:13:42.120 --> 01:13:43.120]   Yeah.
[01:13:43.120 --> 01:13:44.120]   So, yeah.
[01:13:44.120 --> 01:13:51.240]   I mean that crypto, that couple that stole all the money from the cryptocurrency exchange
[01:13:51.240 --> 01:13:56.720]   currently seriously didn't realize that their transactions were clearly going to be traced.
[01:13:56.720 --> 01:13:57.720]   Allegedly.
[01:13:57.720 --> 01:13:58.720]   Rosalcon, I allegedly.
[01:13:58.720 --> 01:13:59.720]   Yeah.
[01:13:59.720 --> 01:14:02.480]   The famous rap star.
[01:14:02.480 --> 01:14:03.480]   Wrestlecon.
[01:14:03.480 --> 01:14:07.000]   Is this a real rap star?
[01:14:07.000 --> 01:14:08.000]   Oh, no, no.
[01:14:08.000 --> 01:14:10.000]   I like the lead team.
[01:14:10.000 --> 01:14:11.000]   Oh my God.
[01:14:11.000 --> 01:14:12.000]   I don't know.
[01:14:12.000 --> 01:14:13.000]   I don't know.
[01:14:13.000 --> 01:14:15.600]   I'm going to have to play a razzle con video for Amy.
[01:14:15.600 --> 01:14:17.280]   There's a real thing.
[01:14:17.280 --> 01:14:18.280]   So it's a couple.
[01:14:18.280 --> 01:14:19.560]   Oh, I'm so happy for you.
[01:14:19.560 --> 01:14:22.320]   Can we do a rush chaser after that?
[01:14:22.320 --> 01:14:23.320]   Yeah.
[01:14:23.320 --> 01:14:24.320]   Yeah.
[01:14:24.320 --> 01:14:25.320]   Whatever it is.
[01:14:25.320 --> 01:14:29.720]   The crocodile of Wall Street rapper accused of laundering billions of dollars in crypto.
[01:14:29.720 --> 01:14:34.520]   Heather Morgan and her husband, Ilya Dutch Lichtenstein, were arrested.
[01:14:34.520 --> 01:14:35.520]   Is this real?
[01:14:35.520 --> 01:14:36.520]   Yes.
[01:14:36.520 --> 01:14:37.520]   No.
[01:14:37.520 --> 01:14:38.520]   Yes.
[01:14:38.520 --> 01:14:45.440]   This is the cryptocurrency that was stolen years ago.
[01:14:45.440 --> 01:14:47.720]   I think this is from 2016.
[01:14:47.720 --> 01:14:49.120]   I missed all of this.
[01:14:49.120 --> 01:14:51.880]   It was at the time it was only worth millions.
[01:14:51.880 --> 01:14:57.360]   But of course, by the time they ended up starting to sell it, it's worth, I think, $4.2
[01:14:57.360 --> 01:14:58.360]   billion.
[01:14:58.360 --> 01:15:00.240]   Let me see if I can find.
[01:15:00.240 --> 01:15:06.480]   Razzle con has pulled down a lot of her wraps, but I think there was one on Twitter.
[01:15:06.480 --> 01:15:09.160]   If I can find it.
[01:15:09.160 --> 01:15:10.680]   Oh, my God.
[01:15:10.680 --> 01:15:13.920]   So there were some incredible details.
[01:15:13.920 --> 01:15:16.120]   It's kind of hard to describe.
[01:15:16.120 --> 01:15:19.000]   Here's a little bit of Razzle con right now.
[01:15:19.000 --> 01:15:20.000]   Juicy, Juicy.
[01:15:20.000 --> 01:15:21.000]   I'm motherf*cking fancy.
[01:15:21.000 --> 01:15:24.960]   Razz, boys on payroll, cuz I'm so damn wealthy.
[01:15:24.960 --> 01:15:26.960]   Just stay healthy.
[01:15:26.960 --> 01:15:35.840]   F*ck we heard about Razzle con, sprinkle showers wherever I think.
[01:15:35.840 --> 01:15:36.840]   Is that not her?
[01:15:36.840 --> 01:15:38.400]   I don't know who that is.
[01:15:38.400 --> 01:15:40.400]   Yeah, it says it's Razzle con.
[01:15:40.400 --> 01:15:43.160]   I believe that is Razzle con.
[01:15:43.160 --> 01:15:46.720]   She was also a contributor to, what was it?
[01:15:46.720 --> 01:15:47.720]   Was it Forbes?
[01:15:47.720 --> 01:15:48.720]   Oh, yeah.
[01:15:48.720 --> 01:15:49.720]   She contributed to Forbes.
[01:15:49.720 --> 01:15:50.720]   Yeah.
[01:15:50.720 --> 01:15:53.560]   A Forbes contributor and had written a number of articles.
[01:15:53.560 --> 01:15:54.560]   That's on print.
[01:15:54.560 --> 01:15:55.560]   It is totally on print.
[01:15:55.560 --> 01:16:00.320]   About how to protect yourself from fraud related to cryptocurrency, which I think is
[01:16:00.320 --> 01:16:02.400]   quite ironic on that she.
[01:16:02.400 --> 01:16:03.400]   Well, she knew.
[01:16:03.400 --> 01:16:05.360]   She was an expert.
[01:16:05.360 --> 01:16:10.120]   Is she like seriously trying to rap or is this like a parody?
[01:16:10.120 --> 01:16:11.200]   Here's the best known.
[01:16:11.200 --> 01:16:14.360]   Here's your one and a half million views.
[01:16:14.360 --> 01:16:16.800]   Her best known rap, Razzle con.
[01:16:16.800 --> 01:16:18.600]   Turkish Martha Stark.
[01:16:18.600 --> 01:16:20.600]   Keep going up in her skirt.
[01:16:20.600 --> 01:16:22.600]   Been trapping with dessert.
[01:16:22.600 --> 01:16:24.600]   Gone poppy from that dirt.
[01:16:24.600 --> 01:16:26.600]   Rebellion-shone Turk.
[01:16:26.600 --> 01:16:28.400]   Conbases with a Burke.
[01:16:28.400 --> 01:16:30.800]   I don't know what the sound effect is.
[01:16:30.800 --> 01:16:32.520]   I thought she'd come alert.
[01:16:32.520 --> 01:16:34.000]   It's not a mistake.
[01:16:34.000 --> 01:16:36.000]   Turkish mr.
[01:16:36.000 --> 01:16:38.840]   Turkish Martha Stewart.
[01:16:38.840 --> 01:16:41.600]   She is apparently the Turkish Martha Stewart.
[01:16:41.600 --> 01:16:43.640]   I think is the-
[01:16:43.640 --> 01:16:45.840]   Where does she from?
[01:16:45.840 --> 01:16:47.160]   Where does she hail from?
[01:16:47.160 --> 01:16:51.120]   I think Rikers Island now, but before I'm not sure.
[01:16:51.120 --> 01:16:52.120]   Oh my God.
[01:16:52.120 --> 01:16:57.680]   She recommended rapping as a form of self-care.
[01:16:57.680 --> 01:17:02.880]   There were also some fantastic details in a recent New York Times article about the couple,
[01:17:02.880 --> 01:17:12.280]   specifically that when the government went to kind of raid their apartment after the arrest,
[01:17:12.280 --> 01:17:16.920]   when I guess when they were executing a search warrant, agents found more than 50 electronic
[01:17:16.920 --> 01:17:22.360]   devices throughout their apartment, including a bag labeled burner phone that had more than
[01:17:22.360 --> 01:17:23.800]   $40,000 in cash.
[01:17:23.800 --> 01:17:28.560]   If you're gonna have burner phones, you gotta keep them separate.
[01:17:28.560 --> 01:17:33.840]   They had at least two holodeck books whose pages had secret compartments in them.
[01:17:33.840 --> 01:17:41.160]   Then, as the agents were searching, Ms. Razzlecon had asked if she could go get their cat,
[01:17:41.160 --> 01:17:42.720]   which was hiding under the bed.
[01:17:42.720 --> 01:17:45.080]   The agents were like, "Of course, you can get the cat."
[01:17:45.080 --> 01:17:51.200]   As she kneels down to get the cat, she grabs a phone allegedly off the nightstand and starts
[01:17:51.200 --> 01:17:59.120]   hitting it rapidly to try and wipe it or engage encryption.
[01:17:59.120 --> 01:18:02.680]   Apparently the agents had to grab it from her, which I think is a hell of a move.
[01:18:02.680 --> 01:18:04.680]   This is amazing.
[01:18:04.680 --> 01:18:07.120]   It's one of those stories.
[01:18:07.120 --> 01:18:12.080]   Now of course, there's a story about open sea.
[01:18:12.080 --> 01:18:16.680]   Users complaining about missing NFTs.
[01:18:16.680 --> 01:18:21.480]   Is this the Super Mega Mayflower yacht, one of those?
[01:18:21.480 --> 01:18:22.480]   Yes.
[01:18:22.480 --> 01:18:24.880]   Like the $650,000 cartoon boat that somebody purchased?
[01:18:24.880 --> 01:18:28.880]   We are actively investigating rumors of an exploit associated with open sea related smart
[01:18:28.880 --> 01:18:29.880]   contracts.
[01:18:29.880 --> 01:18:31.840]   OpenSea posted to Twitter last night.
[01:18:31.840 --> 01:18:35.920]   There appears to be a phishing attack originating outside of OpenSea's website.
[01:18:35.920 --> 01:18:41.000]   Do not click links other than OpenSea.io.
[01:18:41.000 --> 01:18:48.960]   32 users so far, according to OpenSea CEO Devin Finser last night, signed a malicious payload
[01:18:48.960 --> 01:18:55.280]   from an attacker as a result some of their NFTs were stolen.
[01:18:55.280 --> 01:18:57.280]   Be careful out there.
[01:18:57.280 --> 01:18:58.280]   It's in the word.
[01:18:58.280 --> 01:18:59.280]   This is interesting.
[01:18:59.280 --> 01:19:04.480]   I wonder if who in the federal government, who in the FBI is, right?
[01:19:04.480 --> 01:19:05.640]   There'd be an FBI case.
[01:19:05.640 --> 01:19:06.640]   Who's in charge of?
[01:19:06.640 --> 01:19:10.000]   That would be grand larceny if it's above a certain amount.
[01:19:10.000 --> 01:19:11.000]   But it's digital.
[01:19:11.000 --> 01:19:12.000]   So who's in charge?
[01:19:12.000 --> 01:19:13.360]   What's it really worth?
[01:19:13.360 --> 01:19:14.360]   Right?
[01:19:14.360 --> 01:19:19.360]   I'm more like, do we suddenly, like, this is in your book?
[01:19:19.360 --> 01:19:20.360]   This is in your book.
[01:19:20.360 --> 01:19:24.720]   It's a chapter from your book where they're trying to figure out who's responsible for
[01:19:24.720 --> 01:19:25.720]   what was that for?
[01:19:25.720 --> 01:19:27.440]   That was for cyber attack?
[01:19:27.440 --> 01:19:29.920]   That's if there's a cyber biological attack.
[01:19:29.920 --> 01:19:33.760]   But actually now that I'm hearing you guys talk, like, I wonder which division is in
[01:19:33.760 --> 01:19:39.760]   charge of investigating the theft of an NFT if it's above $250,000, which I think is what
[01:19:39.760 --> 01:19:40.760]   qualifies as a leader.
[01:19:40.760 --> 01:19:45.600]   I feel like it's probably the same division that was investigating the theft of the alleged
[01:19:45.600 --> 01:19:48.600]   deaths of Raazal Khan and Herbo.
[01:19:48.600 --> 01:19:51.080]   Because there was a, I'm trying to remember the specific name.
[01:19:51.080 --> 01:19:52.320]   I mean, that sounds funny.
[01:19:52.320 --> 01:19:58.920]   But there was a specific division that had to trace back the crypto transactions from,
[01:19:58.920 --> 01:20:02.920]   like, wallet to wallet to wallet to try and pin it on the issue.
[01:20:02.920 --> 01:20:05.680]   I bet the FBI is pretty good at that by now.
[01:20:05.680 --> 01:20:06.680]   Yeah.
[01:20:06.680 --> 01:20:07.680]   That'd be my guess.
[01:20:07.680 --> 01:20:08.680]   Yeah.
[01:20:08.680 --> 01:20:09.680]   Yeah.
[01:20:09.680 --> 01:20:10.680]   I don't know.
[01:20:10.680 --> 01:20:11.680]   I don't know.
[01:20:11.680 --> 01:20:16.400]   In a series of tweets, Finzer dispelled rumors, the hack was worth $200 million.
[01:20:16.400 --> 01:20:22.120]   He said the hacker has $1.7 million of ETH in his wallet from selling some of the stolen
[01:20:22.120 --> 01:20:23.120]   NFTs.
[01:20:23.120 --> 01:20:28.520]   So yes, it is grand larceny.
[01:20:28.520 --> 01:20:31.440]   Let's take a little break.
[01:20:31.440 --> 01:20:38.240]   We are having a good time talking about the world, the weird modern world with two who
[01:20:38.240 --> 01:20:41.920]   know, Amy Webb, futurist.
[01:20:41.920 --> 01:20:44.560]   She's been living in this present for many years now.
[01:20:44.560 --> 01:20:49.160]   And she's our tour guide future today, Institute at Amy Webb on the Twitter and author of the
[01:20:49.160 --> 01:20:56.240]   brand new book, the Genesis machine, Paris Martenau writes about Amazon and big check
[01:20:56.240 --> 01:20:57.800]   at the in for.
[01:20:57.800 --> 01:21:01.720]   Scare quotes, scare quotes.
[01:21:01.720 --> 01:21:03.720]   Our show today brought to you by wealth front.
[01:21:03.720 --> 01:21:04.720]   They're sensible.
[01:21:04.720 --> 01:21:06.360]   They're, it's not.
[01:21:06.360 --> 01:21:09.240]   They're not talking diamond hands and stonks taking into the moon.
[01:21:09.240 --> 01:21:10.760]   We're not talking about meme stocks.
[01:21:10.760 --> 01:21:12.600]   We're talking and you know what?
[01:21:12.600 --> 01:21:14.800]   No, not casting dispersions on that.
[01:21:14.800 --> 01:21:16.200]   If that's what you want to do, that's fine.
[01:21:16.200 --> 01:21:17.200]   That's fun.
[01:21:17.200 --> 01:21:20.000]   A lot of investment apps make it easy to start trading.
[01:21:20.000 --> 01:21:25.120]   You saw all those ads on the Super Bowl, but just because it's easy to do doesn't mean,
[01:21:25.120 --> 01:21:27.080]   you know what you're doing.
[01:21:27.080 --> 01:21:33.720]   Wealthfront makes it easy to invest and easy to grow your savings with a diversified portfolio
[01:21:33.720 --> 01:21:35.760]   that balances your other riskier bets.
[01:21:35.760 --> 01:21:41.280]   You can start investing in no time, use Wealthfront's classic portfolio or if you feel like it and
[01:21:41.280 --> 01:21:45.000]   it's easy to do, make your own portfolio with things you care about.
[01:21:45.000 --> 01:21:46.800]   They have socially responsible funds.
[01:21:46.800 --> 01:21:49.280]   I'm a big fan of those technology funds.
[01:21:49.280 --> 01:21:50.720]   Yes, you can do crypto.
[01:21:50.720 --> 01:21:55.200]   They've got crypto trusts, hundreds of other investments in every area.
[01:21:55.200 --> 01:21:58.360]   But the thing is, Wealthfront was designed by financial experts to help you turn your
[01:21:58.360 --> 01:22:03.240]   good ideas into great investments without the hassle of doing everything yourself.
[01:22:03.240 --> 01:22:07.160]   If you don't want to, for instance, spend lots of time trying to figure out how to lower
[01:22:07.160 --> 01:22:09.280]   your tax bill, don't worry.
[01:22:09.280 --> 01:22:12.280]   Wealthfront does tax loss harvesting automatically.
[01:22:12.280 --> 01:22:16.960]   All the experts say you've got to pay attention and rebalance your portfolio regularly, right?
[01:22:16.960 --> 01:22:18.160]   You know what rebalancing is.
[01:22:18.160 --> 01:22:19.440]   Would you know how to do it?
[01:22:19.440 --> 01:22:20.440]   Don't worry.
[01:22:20.440 --> 01:22:22.200]   Wealthfront does it for you automatically.
[01:22:22.200 --> 01:22:25.600]   Wealthfront is trusted with over $28 billion in assets.
[01:22:25.600 --> 01:22:28.840]   They're helping nearly half a million people build their wealth.
[01:22:28.840 --> 01:22:35.080]   It's easy to get started, $500 opens the account, grow your wealth the easy way, let
[01:22:35.080 --> 01:22:37.160]   Wealthfront do the work for you.
[01:22:37.160 --> 01:22:40.480]   4.9 out of 5 stars on the Apple App Store, by the way.
[01:22:40.480 --> 01:22:41.880]   That's how you'll know you got the right one.
[01:22:41.880 --> 01:22:43.360]   Wealthfront.
[01:22:43.360 --> 01:22:44.720]   Start building your wealth right now.
[01:22:44.720 --> 01:22:52.160]   Get your first $5,000 managed free forever at wealthfront.com/twit.
[01:22:52.160 --> 01:22:59.160]   I want you to read all about it.
[01:22:59.160 --> 01:23:01.960]   W-E-A-L-T-H Wealthfront, fr-o-n-t.com/twit.
[01:23:01.960 --> 01:23:05.280]   Start building your wealth, go to wealthfront.com/twit today.
[01:23:05.280 --> 01:23:07.440]   We thank you so much for supporting Twit.
[01:23:07.440 --> 01:23:08.920]   You support us when you use that address.
[01:23:08.920 --> 01:23:15.160]   I know you could just go to Wealthfront, but try to go to the one we mentioned, wealthfront.com/twit.
[01:23:15.160 --> 01:23:19.240]   That way they'll know you saw it here.
[01:23:19.240 --> 01:23:24.880]   That's becoming a big problem in podcasting in general.
[01:23:24.880 --> 01:23:30.560]   An advertiser's really want to know this, but podcasting is not designed around tracking.
[01:23:30.560 --> 01:23:31.720]   It's RSS.
[01:23:31.720 --> 01:23:34.600]   You're anonymous to podcasters.
[01:23:34.600 --> 01:23:36.240]   That's why we do an annual survey.
[01:23:36.240 --> 01:23:38.720]   In fact, you have like eight more days to do it.
[01:23:38.720 --> 01:23:40.840]   It runs out at the end of the month.
[01:23:40.840 --> 01:23:41.840]   Twit.tv/survey22.
[01:23:41.840 --> 01:23:46.680]   We're used to asking you, it's completely voluntary.
[01:23:46.680 --> 01:23:52.480]   We also have been using, for some advertisers who request it, a firm called Chartable, which
[01:23:52.480 --> 01:23:57.920]   does what they do is, and they do it in a privacy forward way, which is the reason we
[01:23:57.920 --> 01:24:00.200]   use them.
[01:24:00.200 --> 01:24:03.600]   They get our IP addresses of people who download the show.
[01:24:03.600 --> 01:24:09.240]   They get IP addresses from the sponsor's website, the page you go to, like wealthfront.com/twit.
[01:24:09.240 --> 01:24:13.480]   And then without divulging that information to anybody else, they say, "This is, you've
[01:24:13.480 --> 01:24:15.480]   got 36% match or whatever.
[01:24:15.480 --> 01:24:18.200]   This is how many people Twit sent to your website."
[01:24:18.200 --> 01:24:22.720]   And yet, it's privacy respecting, which is great.
[01:24:22.720 --> 01:24:27.720]   Unfortunately, Chartable and the other company that we've used, PodSights, both were just
[01:24:27.720 --> 01:24:30.200]   purchased by Spotify.
[01:24:30.200 --> 01:24:37.640]   So more and more podcasting is moving into this realm where it's not RSS, where you get
[01:24:37.640 --> 01:24:42.680]   it in an app called Spotify, where they know everything about you and they have all the
[01:24:42.680 --> 01:24:45.480]   analytics they could possibly want.
[01:24:45.480 --> 01:24:51.400]   And it makes me worry a little bit about the future of open podcasting of RSS.
[01:24:51.400 --> 01:24:58.160]   I know this day is coming at some point, but I know the day is coming where I will no longer
[01:24:58.160 --> 01:25:03.120]   be able to skip through podcast ads to my heart's content, and it's going to be a dark
[01:25:03.120 --> 01:25:04.120]   day.
[01:25:04.120 --> 01:25:10.200]   I'm going to have to keep downloading my podcast directly because, you know, I never skip.
[01:25:10.200 --> 01:25:11.200]   In Spotify, can you do it?
[01:25:11.200 --> 01:25:12.200]   No, no, you can.
[01:25:12.200 --> 01:25:13.200]   I don't know.
[01:25:13.200 --> 01:25:14.200]   I listen to everyone.
[01:25:14.200 --> 01:25:16.720]   No, no, but can you skip in Spotify?
[01:25:16.720 --> 01:25:17.880]   Do they have a skip button?
[01:25:17.880 --> 01:25:18.880]   Probably not, right?
[01:25:18.880 --> 01:25:19.880]   I used to.
[01:25:19.880 --> 01:25:20.880]   I think it's called.
[01:25:20.880 --> 01:25:21.880]   I think it's called.
[01:25:21.880 --> 01:25:24.120]   Mine is called Castor, does that sound right?
[01:25:24.120 --> 01:25:25.880]   It was this thing called Pocket Casts.
[01:25:25.880 --> 01:25:27.880]   I love Pocket Casts, and yes, you can.
[01:25:27.880 --> 01:25:28.880]   That's what I use.
[01:25:28.880 --> 01:25:29.880]   But that's the point.
[01:25:29.880 --> 01:25:31.600]   Pocket Casts uses RSS feeds.
[01:25:31.600 --> 01:25:36.840]   You cannot, for instance, listen to the Joe Rogan experience on Pocket Casts because Spotify,
[01:25:36.840 --> 01:25:39.840]   and turns out, by the way, they spent 200 million.
[01:25:39.840 --> 01:25:44.360]   Not what we had been earlier reported 100 million for the Joe Rogan experience.
[01:25:44.360 --> 01:25:47.240]   And by the way, it's been a great investment for them.
[01:25:47.240 --> 01:25:52.000]   Joe Rogan is a million dollars to buy one add on Joe Rogan, and you have to agree to
[01:25:52.000 --> 01:25:54.640]   buy ads on other Spotify shows as well.
[01:25:54.640 --> 01:25:56.720]   So it's a money maker, right?
[01:25:56.720 --> 01:25:57.720]   That's 200 ads.
[01:25:57.720 --> 01:25:58.720]   It's nothing.
[01:25:58.720 --> 01:26:03.000]   You know, that's a year's worth of advertising.
[01:26:03.000 --> 01:26:06.400]   But the whole idea is it's exclusive, so you have to have Spotify.
[01:26:06.400 --> 01:26:11.680]   You don't have to be a paid member, but you have to have that app.
[01:26:11.680 --> 01:26:17.520]   And that's the thing I think that's bad for RSS podcasting, traditional.
[01:26:17.520 --> 01:26:22.280]   You won't be able to use podcast to listen to some podcasts anymore.
[01:26:22.280 --> 01:26:26.720]   I was just reading, I forget where it was, the Spotify's plans for how it's attempting
[01:26:26.720 --> 01:26:34.320]   to expand because at some point, the market mechanics for the music alone aren't enough
[01:26:34.320 --> 01:26:38.440]   to sustain it because the margins are too low and just that the money doesn't really
[01:26:38.440 --> 01:26:40.440]   make sense over a long period of time.
[01:26:40.440 --> 01:26:41.440]   Oh, gosh, yes.
[01:26:41.440 --> 01:26:43.320]   This is why Spotify's doing it.
[01:26:43.320 --> 01:26:47.680]   The worst possible business you could be in is streaming music because you have to pay
[01:26:47.680 --> 01:26:50.000]   so much to the labels.
[01:26:50.000 --> 01:26:52.560]   I mean, they're not even paying much to the labels.
[01:26:52.560 --> 01:26:54.960]   Well, no, see, now that's a common misconception.
[01:26:54.960 --> 01:26:55.960]   They're paying a lot to the labels.
[01:26:55.960 --> 01:26:58.280]   Two thirds of the money they make goes to the labels.
[01:26:58.280 --> 01:26:59.440]   You're talking about the artists.
[01:26:59.440 --> 01:27:00.440]   Oh, yes.
[01:27:00.440 --> 01:27:01.440]   Very different.
[01:27:01.440 --> 01:27:02.440]   Sorry.
[01:27:02.440 --> 01:27:04.440]   People who made the music.
[01:27:04.440 --> 01:27:05.440]   Yeah, yeah.
[01:27:05.440 --> 01:27:06.960]   You're talking about people who make the music.
[01:27:06.960 --> 01:27:07.960]   Oh, who cares about that?
[01:27:07.960 --> 01:27:08.960]   No, you're right.
[01:27:08.960 --> 01:27:12.400]   Spotify pays one of the lowest fees per stream, but most of that money doesn't go to the
[01:27:12.400 --> 01:27:13.400]   artists.
[01:27:13.400 --> 01:27:15.640]   It goes to the label.
[01:27:15.640 --> 01:27:19.120]   They are also, I think this is kind of a double-ender because not only they're buying
[01:27:19.120 --> 01:27:24.120]   podcasts like Rogan, they're buying things like Charitable and POD sites and Megaphone,
[01:27:24.120 --> 01:27:26.720]   which is a direct ad insertion platform and anchor FM.
[01:27:26.720 --> 01:27:29.800]   They're buying the backend as well, which means advertisers.
[01:27:29.800 --> 01:27:30.800]   Yeah.
[01:27:30.800 --> 01:27:39.720]   They literally, this is a direct shot at companies like mine because if I don't have access to
[01:27:39.720 --> 01:27:44.120]   those metrics, advertisers just say, no, no, we're not going to buy your show because
[01:27:44.120 --> 01:27:45.920]   we have to buy Spotify.
[01:27:45.920 --> 01:27:50.200]   Does Nielsen have, doesn't Nielsen do that type of rating?
[01:27:50.200 --> 01:27:55.400]   Nobody does anything like this, unfortunately.
[01:27:55.400 --> 01:27:57.160]   Spotify, look, it's good business.
[01:27:57.160 --> 01:28:00.160]   I understand completely why they're doing this.
[01:28:00.160 --> 01:28:05.880]   They need audio programming that doesn't cost as much as streaming music does and podcasts
[01:28:05.880 --> 01:28:08.560]   are a good deal from their point of view.
[01:28:08.560 --> 01:28:13.320]   By the way, they won't even have to buy many podcasts just because they own the backend,
[01:28:13.320 --> 01:28:17.840]   everybody's going to have to go to them to sell ads.
[01:28:17.840 --> 01:28:25.920]   Currently almost looks like 3.5 million podcasts are on Spotify, not exclusive, but on Spotify.
[01:28:25.920 --> 01:28:26.920]   That's a big...
[01:28:26.920 --> 01:28:32.720]   What that number is for podcasts with a regular audience of a thousand listeners.
[01:28:32.720 --> 01:28:34.760]   Oh, yeah, that's a good question.
[01:28:34.760 --> 01:28:35.760]   Yeah.
[01:28:35.760 --> 01:28:36.760]   Yeah.
[01:28:36.760 --> 01:28:40.640]   I mean, how much do someone uploading something?
[01:28:40.640 --> 01:28:42.560]   This is just distribution.
[01:28:42.560 --> 01:28:48.800]   At some point, I'm trying to think of a company that is in the distribution business that
[01:28:48.800 --> 01:28:54.440]   has not at some point faced a problem because at some point, there's another disruption
[01:28:54.440 --> 01:28:59.760]   that comes around and they find it hard to compete.
[01:28:59.760 --> 01:29:03.920]   Or you wind up with market, you wind up with over saturation.
[01:29:03.920 --> 01:29:04.920]   I don't know.
[01:29:04.920 --> 01:29:06.760]   I'm trying to play this forward five years.
[01:29:06.760 --> 01:29:09.680]   Please do and let me know what you figure out because I'm...
[01:29:09.680 --> 01:29:12.080]   I mean, at this point...
[01:29:12.080 --> 01:29:22.160]   I think what happens is that I would be curious to know how they pivot and they're going to
[01:29:22.160 --> 01:29:27.080]   have to at some point, I would think, because just the traditional distribution models over
[01:29:27.080 --> 01:29:30.600]   and over and over again tend not to work out.
[01:29:30.600 --> 01:29:31.600]   You digital...
[01:29:31.600 --> 01:29:34.400]   Especially advertising based like a distribution model.
[01:29:34.400 --> 01:29:35.400]   Right.
[01:29:35.400 --> 01:29:39.680]   I mean, I think the fact that this is all, again, based on bringing in ad dollars, it's
[01:29:39.680 --> 01:29:45.280]   on almost certainty that at some point, much like advertising has kind of collapsed for
[01:29:45.280 --> 01:29:49.680]   digital media, and of course, print media, that at some point that is going to come to
[01:29:49.680 --> 01:29:51.880]   audio content.
[01:29:51.880 --> 01:29:53.640]   And it'll move to whatever the next thing is.
[01:29:53.640 --> 01:29:57.560]   Maybe that's VR glasses or something.
[01:29:57.560 --> 01:29:59.040]   But I'm curious as to what...
[01:29:59.040 --> 01:30:01.800]   Yeah, like you said, Amy, what is the 10-year plan for this?
[01:30:01.800 --> 01:30:03.800]   Well, and if Meta...
[01:30:03.800 --> 01:30:04.800]   Again, we don't...
[01:30:04.800 --> 01:30:07.360]   Going back to what we were talking about earlier with Meta, I don't know what happens
[01:30:07.360 --> 01:30:12.360]   with that, but one would think that if there's a ton of money, advertisers are fickle and
[01:30:12.360 --> 01:30:16.920]   they go where they think they're going to have the greatest impact.
[01:30:16.920 --> 01:30:22.560]   And if Spotify is pretty much ad driven, the ad driven model for distribution just over
[01:30:22.560 --> 01:30:27.200]   and over again, you don't have enough control over that ecosystem.
[01:30:27.200 --> 01:30:29.520]   And you at some point become vulnerable.
[01:30:29.520 --> 01:30:32.360]   So I'd be curious to see what happens.
[01:30:32.360 --> 01:30:36.800]   Amazon iHeart, which does a lot of podcasts.
[01:30:36.800 --> 01:30:41.840]   They're the biggest audio company because they own a lot of radio stations.
[01:30:41.840 --> 01:30:44.400]   Apple II is getting into podcasting.
[01:30:44.400 --> 01:30:47.320]   So I hope there's some competition.
[01:30:47.320 --> 01:30:48.320]   But I also kind of bemoan.
[01:30:48.320 --> 01:30:53.280]   I mean, the same thing happened to blogging to some degree where the small independent
[01:30:53.280 --> 01:30:57.760]   blogs kind of just disappeared because they couldn't compete against the big companies
[01:30:57.760 --> 01:30:59.240]   who are moving into the blogging space.
[01:30:59.240 --> 01:31:06.480]   I like the idea, of course, I'm in the business of democratized kind of free media, ad supported
[01:31:06.480 --> 01:31:09.000]   free media that anybody can listen to, anybody can make.
[01:31:09.000 --> 01:31:10.000]   I like that idea.
[01:31:10.000 --> 01:31:16.920]   I don't like to see it kind of moved into silos, but I fear that's where it's going.
[01:31:16.920 --> 01:31:22.320]   Speaking of silos, I just can't wait to hear the roar of horror when people realize that
[01:31:22.320 --> 01:31:29.280]   they can no longer use Microsoft Windows without signing in to Microsoft.
[01:31:29.280 --> 01:31:33.840]   This was released in a dev version of Windows 11 Pro.
[01:31:33.840 --> 01:31:36.040]   For a long time, it's only Windows 11 Home.
[01:31:36.040 --> 01:31:37.400]   They required a Microsoft account.
[01:31:37.400 --> 01:31:41.040]   You would tell people, you can get Windows 11 Pro or you can disconnect from the internet,
[01:31:41.040 --> 01:31:43.120]   still get a local account.
[01:31:43.120 --> 01:31:47.320]   Looks like if Microsoft goes ahead with this, it's in the dev channel right now.
[01:31:47.320 --> 01:31:54.480]   It will be the first desktop operating system to require internet connectivity and an account
[01:31:54.480 --> 01:31:57.680]   with a company for even basic functionality.
[01:31:57.680 --> 01:31:58.680]   Max don't do it.
[01:31:58.680 --> 01:32:02.000]   What do you do if you're on a plane?
[01:32:02.000 --> 01:32:04.760]   Yeah, well, you don't install Windows.
[01:32:04.760 --> 01:32:05.760]   You wait until you...
[01:32:05.760 --> 01:32:06.760]   Yeah!
[01:32:06.760 --> 01:32:11.320]   You can use it on a plane, but you can't install it on a plane.
[01:32:11.320 --> 01:32:14.640]   But I think there are still people in the world who don't have internet who might want
[01:32:14.640 --> 01:32:18.840]   to use a computer.
[01:32:18.840 --> 01:32:19.840]   I don't know.
[01:32:19.840 --> 01:32:22.720]   Or there are times when you don't want to have the internet on.
[01:32:22.720 --> 01:32:24.160]   I mean, we do some sensitive...
[01:32:24.160 --> 01:32:25.160]   Absolutely.
[01:32:25.160 --> 01:32:28.800]   Sometimes we do some sensitive work and we don't even want to be on like a VPN.
[01:32:28.800 --> 01:32:33.800]   We need to be completely off on a machine that doesn't ever connect anywhere.
[01:32:33.800 --> 01:32:36.720]   It's a really interesting move on Microsoft's part.
[01:32:36.720 --> 01:32:40.880]   They'll probably get away with it, but there's going to be...
[01:32:40.880 --> 01:32:41.880]   Just get ready.
[01:32:41.880 --> 01:32:45.000]   We talked about it on Windows Weekly and Paul and Mary Jo were kind of...
[01:32:45.000 --> 01:32:46.000]   Are people going to care?
[01:32:46.000 --> 01:32:47.920]   And I said, "Paul, they're going to care."
[01:32:47.920 --> 01:32:49.880]   You ain't there nothing yet.
[01:32:49.880 --> 01:32:51.920]   People will care.
[01:32:51.920 --> 01:32:53.600]   I was a die-hard Windows user.
[01:32:53.600 --> 01:32:54.600]   I remember until...
[01:32:54.600 --> 01:32:59.160]   I think Windows 8 came out and that was when I jumped into the world of Macbooks where
[01:32:59.160 --> 01:33:00.160]   I've stayed sensitive.
[01:33:00.160 --> 01:33:01.880]   Never looked back, did you?
[01:33:01.880 --> 01:33:02.880]   Never looked back.
[01:33:02.880 --> 01:33:05.320]   I just couldn't get over all the big boxes.
[01:33:05.320 --> 01:33:09.720]   But this is the sort of thing that I mean, and I think if I was still using the operating
[01:33:09.720 --> 01:33:12.800]   system, would definitely turn me away.
[01:33:12.800 --> 01:33:13.800]   Right.
[01:33:13.800 --> 01:33:15.680]   So, the Super Bowl was last week.
[01:33:15.680 --> 01:33:17.040]   I know Amy watched.
[01:33:17.040 --> 01:33:19.800]   Actually, I don't know that you watched.
[01:33:19.800 --> 01:33:22.240]   I don't know that I would say I watched it.
[01:33:22.240 --> 01:33:28.480]   We did the opposite of what Paris was talking about, about like fast forwarding through ads.
[01:33:28.480 --> 01:33:30.640]   We fast forwarded the game and then just landed on the ads.
[01:33:30.640 --> 01:33:31.640]   Yeah, I think that's what I said.
[01:33:31.640 --> 01:33:32.640]   Some people do.
[01:33:32.640 --> 01:33:33.640]   They watch the ads.
[01:33:33.640 --> 01:33:34.640]   Yeah.
[01:33:34.640 --> 01:33:39.120]   So, we watched the last two minutes of the game and didn't care.
[01:33:39.120 --> 01:33:40.800]   So what did you think about?
[01:33:40.800 --> 01:33:45.000]   We were doing the show and I'm looking and I'm saying, "Oh, I think we've lost the feed,
[01:33:45.000 --> 01:33:46.000]   John.
[01:33:46.000 --> 01:33:47.440]   There's a bouncing QR code."
[01:33:47.440 --> 01:33:48.440]   QR code.
[01:33:48.440 --> 01:33:50.040]   What is that all about?
[01:33:50.040 --> 01:33:59.400]   It turned out it was about Coinbase spending, I think it was $12 million for 60 seconds of
[01:33:59.400 --> 01:34:03.320]   ad time on the Super Bowl with a bouncing QR code.
[01:34:03.320 --> 01:34:04.520]   So here's the funny part.
[01:34:04.520 --> 01:34:05.520]   It worked.
[01:34:05.520 --> 01:34:06.520]   Yeah.
[01:34:06.520 --> 01:34:14.360]   Crypto apps shot up in the App Store after Super Bowl started with crypto ads.
[01:34:14.360 --> 01:34:17.520]   I mean, there were quite a few.
[01:34:17.520 --> 01:34:22.040]   I didn't watch Super Bowl, but I heard that there was I think like six or seven crypto
[01:34:22.040 --> 01:34:23.040]   ads.
[01:34:23.040 --> 01:34:24.040]   Yes.
[01:34:24.040 --> 01:34:28.000]   And Coinbase for a while that servers were down, they couldn't keep up.
[01:34:28.000 --> 01:34:34.140]   They went from 186th place on the App Store to number two on the App Store after that
[01:34:34.140 --> 01:34:36.200]   ad because that's what the QR code was.
[01:34:36.200 --> 01:34:38.440]   You scan it and you get a link.
[01:34:38.440 --> 01:34:43.440]   And as you pointed out, Amy, this is probably not the most secure thing in the world to do.
[01:34:43.440 --> 01:34:48.080]   No, I thought it was, listen, they're not targeting my dad.
[01:34:48.080 --> 01:34:52.840]   They're targeting the people who know how to open up a phone and scan on that thing.
[01:34:52.840 --> 01:34:55.600]   I think it was super smart and annoying.
[01:34:55.600 --> 01:34:57.680]   I mean, I think it was exactly what they wanted it to be.
[01:34:57.680 --> 01:35:05.680]   But yeah, there were, I thought that the whole night was kind of a bizarre near future dystopian
[01:35:05.680 --> 01:35:06.680]   hellscape.
[01:35:06.680 --> 01:35:10.560]   It's just, you know, you're the futurist.
[01:35:10.560 --> 01:35:12.560]   You should have been comfortable in that.
[01:35:12.560 --> 01:35:13.560]   Did it?
[01:35:13.560 --> 01:35:17.680]   No, what I was, what I was comfortable with, I just remember the first ad that I saw was
[01:35:17.680 --> 01:35:23.240]   that Uber eats ad were like, like, like, eat diapers, Greg's drinking dish soap.
[01:35:23.240 --> 01:35:25.160]   Cousin Greg is right.
[01:35:25.160 --> 01:35:27.920]   And like, what is the message that Uber is trying to tell me?
[01:35:27.920 --> 01:35:30.400]   I think what they're trying to tell me.
[01:35:30.400 --> 01:35:35.680]   I think what the message there is, like, technology has created this learned helplessness where
[01:35:35.680 --> 01:35:38.120]   humans have forgotten how to eat and we're just like,
[01:35:38.120 --> 01:35:39.280]   It says eat on the bag.
[01:35:39.280 --> 01:35:40.840]   I'm going to eat it.
[01:35:40.840 --> 01:35:45.640]   It was a very strange campaign, especially since you can't do a car commercial without
[01:35:45.640 --> 01:35:48.760]   saying professional driver on a closed track.
[01:35:48.760 --> 01:35:50.600]   Do not attempt.
[01:35:50.600 --> 01:35:54.920]   Nevertheless, they've got somebody eating light bulbs in their commercial and see now they
[01:35:54.920 --> 01:35:59.880]   did have actually had a series of very funny disclaimers, including do not eat light bulbs.
[01:35:59.880 --> 01:36:03.920]   These are not real light bulbs, but they were very fine print.
[01:36:03.920 --> 01:36:09.120]   I could just see the next TikTok fad trying to eat the stuff Uber eats delivers.
[01:36:09.120 --> 01:36:10.520]   That's not out of the at a ball.
[01:36:10.520 --> 01:36:11.520]   Oh, yeah.
[01:36:11.520 --> 01:36:12.520]   Maybe that was their plan.
[01:36:12.520 --> 01:36:14.360]   Someone's out there crunching on light bulbs.
[01:36:14.360 --> 01:36:15.360]   Light bulbs right now.
[01:36:15.360 --> 01:36:16.560]   Do not eat lot bulbs, please.
[01:36:16.560 --> 01:36:17.560]   Yeah.
[01:36:17.560 --> 01:36:18.560]   Yeah.
[01:36:18.560 --> 01:36:23.920]   And then you just see Larry David's ad for FTX, which is another crypto exchange.
[01:36:23.920 --> 01:36:24.920]   And I'm just curious.
[01:36:24.920 --> 01:36:29.240]   You were earlier in this thing, bemoaning, saying advertising is dead.
[01:36:29.240 --> 01:36:32.400]   And now we just are having a segment where we're talking about super bowl advertising
[01:36:32.400 --> 01:36:34.000]   is not dead.
[01:36:34.000 --> 01:36:37.360]   It's six and a half million dollars for 30 seconds.
[01:36:37.360 --> 01:36:41.640]   But that's the point is that in fact, these ads apparently work, which kind of blows me
[01:36:41.640 --> 01:36:42.840]   away.
[01:36:42.840 --> 01:36:46.880]   The most expensive ads you can buy in media, I think.
[01:36:46.880 --> 01:36:47.880]   Yeah.
[01:36:47.880 --> 01:36:50.040]   I don't know.
[01:36:50.040 --> 01:36:58.760]   I did not. I thought the ads were really, I mean, they were clever, but they were horrific.
[01:36:58.760 --> 01:37:01.080]   The one was Scarlett Johansson and Colin Jost.
[01:37:01.080 --> 01:37:04.160]   And the and the and the echo reading your mind.
[01:37:04.160 --> 01:37:05.160]   Yeah.
[01:37:05.160 --> 01:37:09.200]   Right. And that was just, I don't know, or the animatronic dog.
[01:37:09.200 --> 01:37:11.600]   I don't even remember what commercial this was, but there was the animatronic.
[01:37:11.600 --> 01:37:12.680]   Oh, maybe it was a face.
[01:37:12.680 --> 01:37:13.680]   That was Facebook.
[01:37:13.680 --> 01:37:15.160]   That was the animatronic dog.
[01:37:15.160 --> 01:37:16.160]   The animatronic.
[01:37:16.160 --> 01:37:17.160]   Yeah.
[01:37:17.160 --> 01:37:18.160]   Yeah.
[01:37:18.160 --> 01:37:19.160]   Yeah.
[01:37:19.160 --> 01:37:20.160]   Electric view.
[01:37:20.160 --> 01:37:24.160]   Right. But there was, there was another animatronic, like Chuck E. Cheese style dog.
[01:37:24.160 --> 01:37:25.160]   Oh.
[01:37:25.160 --> 01:37:27.280]   And like, like nobody cared anymore.
[01:37:27.280 --> 01:37:29.200]   And he was like thrown in the garbage.
[01:37:29.200 --> 01:37:34.960]   And you know, so like, but he was able to live on and see his friend, he had to find
[01:37:34.960 --> 01:37:36.920]   compassion in the metaverse.
[01:37:36.920 --> 01:37:42.200]   I feel like if we're okay with this, with this hellscape that these companies are showing
[01:37:42.200 --> 01:37:45.880]   us, like that is, that is very concerning to me.
[01:37:45.880 --> 01:37:46.960]   Very concerning.
[01:37:46.960 --> 01:37:52.560]   I can't, I can't believe I missed the Facebook ad here.
[01:37:52.560 --> 01:37:57.060]   Let's just, let's just watch a little bit of this Facebook ad.
[01:37:57.060 --> 01:38:00.600]   This is a Chuck E. Cheese kind of animatronic.
[01:38:00.600 --> 01:38:03.120]   The breast rock gets shut down.
[01:38:03.120 --> 01:38:09.120]   The animatronics get carried out to the front closing forever, which, you know, must have
[01:38:09.120 --> 01:38:10.840]   happened to a few Chuck E. Cheese's.
[01:38:10.840 --> 01:38:12.560]   He's in a pawn shop.
[01:38:12.560 --> 01:38:17.760]   Now he's become a part of a miniature golf course or a karaoke bar.
[01:38:17.760 --> 01:38:19.760]   And he's just so sad.
[01:38:19.760 --> 01:38:22.560]   He's somehow got on the back of a car.
[01:38:22.560 --> 01:38:25.760]   He's staring at a man, pumping gas.
[01:38:25.760 --> 01:38:26.760]   Yeah.
[01:38:26.760 --> 01:38:27.840]   Oh, and it just fell off a truck.
[01:38:27.840 --> 01:38:29.000]   And now he's in the desert.
[01:38:29.000 --> 01:38:31.200]   God, this is distressing.
[01:38:31.200 --> 01:38:32.320]   This is, I didn't see this.
[01:38:32.320 --> 01:38:33.320]   This is so depressing.
[01:38:33.320 --> 01:38:34.320]   There are tears like.
[01:38:34.320 --> 01:38:35.320]   Yeah.
[01:38:35.320 --> 01:38:37.120]   It's like going down his eyes, he's being crushed.
[01:38:37.120 --> 01:38:38.120]   And wait a minute.
[01:38:38.120 --> 01:38:40.120]   Paris Martin is saving him.
[01:38:40.120 --> 01:38:41.120]   That's true.
[01:38:41.120 --> 01:38:42.120]   You would look so good.
[01:38:42.120 --> 01:38:43.120]   But dazzled.
[01:38:43.120 --> 01:38:47.920]   Now he's now pointing the way to the Bosworth Space Center Space Cafe.
[01:38:47.920 --> 01:38:48.920]   Right.
[01:38:48.920 --> 01:38:49.920]   There's an Oculus.
[01:38:49.920 --> 01:38:50.920]   There's an Oculus.
[01:38:50.920 --> 01:38:52.840]   It's weird in the Oculus.
[01:38:52.840 --> 01:38:53.840]   It's weird in the Oculus.
[01:38:53.840 --> 01:38:55.840]   It's a spacewalk with meta quests.
[01:38:55.840 --> 01:39:01.000]   Suddenly he has no legs in the metaverse and all his dis-
[01:39:01.000 --> 01:39:05.640]   I love a disclaimer that says screens are simulated, not indicative of what they match.
[01:39:05.640 --> 01:39:07.640]   That should really say that, shouldn't it?
[01:39:07.640 --> 01:39:08.640]   Yeah.
[01:39:08.640 --> 01:39:10.760]   So he's in fact just saying what it was.
[01:39:10.760 --> 01:39:14.560]   But you guys, what message is this sending that like in the real world your friends will
[01:39:14.560 --> 01:39:16.120]   forget and discard you.
[01:39:16.120 --> 01:39:19.520]   But if you plug into the metaverse, it's a constant party.
[01:39:19.520 --> 01:39:20.520]   I don't know.
[01:39:20.520 --> 01:39:21.760]   It's a constant variety of presents.
[01:39:21.760 --> 01:39:22.760]   It's very, very impressive.
[01:39:22.760 --> 01:39:23.760]   It's very grim.
[01:39:23.760 --> 01:39:29.160]   I mean, I think it's very, very disturbing to me.
[01:39:29.160 --> 01:39:30.160]   Wow.
[01:39:30.160 --> 01:39:31.920]   I hadn't seen that.
[01:39:31.920 --> 01:39:33.720]   Yeah, I don't know what the message is.
[01:39:33.720 --> 01:39:34.720]   Yeah.
[01:39:34.720 --> 01:39:36.160]   You're useless.
[01:39:36.160 --> 01:39:37.160]   You're discarded.
[01:39:37.160 --> 01:39:38.160]   No one loves you.
[01:39:38.160 --> 01:39:41.120]   Thank God you've got friends in the metaverse.
[01:39:41.120 --> 01:39:46.560]   Yeah, the message to me is that the multiverse is real and I'm in the wrong dimension.
[01:39:46.560 --> 01:39:47.560]   Right.
[01:39:47.560 --> 01:39:49.560]   I want to go to the wherever it forked.
[01:39:49.560 --> 01:39:51.680]   I want to go back in the other direction.
[01:39:51.680 --> 01:39:53.880]   Holy cow.
[01:39:53.880 --> 01:39:55.720]   That's depressing as hell.
[01:39:55.720 --> 01:39:56.880]   I missed that one.
[01:39:56.880 --> 01:39:59.360]   I mean, it's very much ready player one energy.
[01:39:59.360 --> 01:40:00.360]   Yeah.
[01:40:00.360 --> 01:40:06.160]   It's like you're going to be in a desolate environment in awful circumstances, perhaps
[01:40:06.160 --> 01:40:09.280]   alone, but yet you'll have your oculus.
[01:40:09.280 --> 01:40:14.680]   Do you think, and you'll have to forgive me because I'm an old guy, I'm a boomer.
[01:40:14.680 --> 01:40:20.880]   But do you think Paris, this actually speaks to millennials and Gen Z folks?
[01:40:20.880 --> 01:40:24.880]   I think of the anti work crowd, the great resignation.
[01:40:24.880 --> 01:40:31.240]   People who say, you know, I can't make a living by a house and have three square meals.
[01:40:31.240 --> 01:40:34.920]   I can't even pay rent on what I jobs I'm getting.
[01:40:34.920 --> 01:40:37.720]   This is a dystopian world already for them.
[01:40:37.720 --> 01:40:39.960]   Is that who this is aimed at?
[01:40:39.960 --> 01:40:46.680]   I mean, I don't think necessarily because I think that also part of the anti work crowd
[01:40:46.680 --> 01:40:56.200]   or kind of Gen Z disillusionment is also rooted in perhaps just skepticism of like performative
[01:40:56.200 --> 01:40:59.080]   sincerity and advertising and corporate goals.
[01:40:59.080 --> 01:41:00.080]   Very much so.
[01:41:00.080 --> 01:41:03.560]   I mean, I think you look at this and the first thing I think is, Oh my God.
[01:41:03.560 --> 01:41:05.640]   I mean, this looks like a dystopian nightmare.
[01:41:05.640 --> 01:41:06.640]   Yeah.
[01:41:06.640 --> 01:41:11.400]   I mean, in a way, we're already for I think some people were already living in that.
[01:41:11.400 --> 01:41:12.400]   This is it.
[01:41:12.400 --> 01:41:17.960]   I mean, but I think that this at also is probably not targeted to Gen Z demographics for the
[01:41:17.960 --> 01:41:20.600]   same, you know, or like an astalgic.
[01:41:20.600 --> 01:41:22.920]   Obviously, I mean, it's an astalgic ad.
[01:41:22.920 --> 01:41:28.360]   It is to have people think that this is what the cool kids are doing and that then you should
[01:41:28.360 --> 01:41:29.360]   get into it as well.
[01:41:29.360 --> 01:41:35.200]   I mean, I was just looking it up, I guess with Nielsen had found that Sunday's Super Bowl
[01:41:35.200 --> 01:41:42.800]   adveraged a 26.5 rating in adults 18 to 49, which is down 11% from last year.
[01:41:42.800 --> 01:41:47.600]   And that the younger, younger adults, I think they had 117 million.
[01:41:47.600 --> 01:41:49.480]   Oh, wait, this is last year.
[01:41:49.480 --> 01:41:50.480]   I'm sorry.
[01:41:50.480 --> 01:41:52.520]   This was the biggest Super Bowl in years.
[01:41:52.520 --> 01:41:53.520]   Yeah.
[01:41:53.520 --> 01:41:54.520]   Yeah.
[01:41:54.520 --> 01:41:55.520]   What is the demo?
[01:41:55.520 --> 01:42:01.520]   Let me find the ratings for the 18 to 30 demographic for this year.
[01:42:01.520 --> 01:42:03.920]   Now we're getting some reportage.
[01:42:03.920 --> 01:42:07.840]   Now we're seeing a reporter at work looking for those numbers.
[01:42:07.840 --> 01:42:09.560]   A lot of Googling folks.
[01:42:09.560 --> 01:42:15.240]   The Super Bowl viewing figures were up 16% 112.3 million people.
[01:42:15.240 --> 01:42:20.280]   The most watched single US telecast in two years.
[01:42:20.280 --> 01:42:23.200]   I mean, it says years is not that long.
[01:42:23.200 --> 01:42:25.480]   No, no, especially because last year was down 11%.
[01:42:25.480 --> 01:42:26.480]   From the year prior.
[01:42:26.480 --> 01:42:27.480]   Right.
[01:42:27.480 --> 01:42:28.480]   Right.
[01:42:28.480 --> 01:42:31.520]   Is this because the teams or I don't follow?
[01:42:31.520 --> 01:42:37.040]   I don't think Cincinnati is a big media market, but LA sure is.
[01:42:37.040 --> 01:42:42.440]   I think the team, some Super Bowl, according to a study delivered 170 million worth of brand
[01:42:42.440 --> 01:42:45.000]   exposure for sponsors.
[01:42:45.000 --> 01:42:46.520]   Get ready for this.
[01:42:46.520 --> 01:42:50.880]   75 minutes of advertising in that game.
[01:42:50.880 --> 01:42:56.320]   There's only 11 minutes on average of action in an NFL game.
[01:42:56.320 --> 01:42:59.400]   Nike got 46 minutes of in-game exposure.
[01:42:59.400 --> 01:43:01.400]   All those swooshes.
[01:43:01.400 --> 01:43:04.840]   Pepsi and Bose also scored well.
[01:43:04.840 --> 01:43:08.640]   So far, his minute of exposure was valued by Hive at three and a half million.
[01:43:08.640 --> 01:43:13.600]   See, that's half what they paid.
[01:43:13.600 --> 01:43:15.040]   But the demographics are interesting.
[01:43:15.040 --> 01:43:17.560]   I think that's what I agree with you, Parris.
[01:43:17.560 --> 01:43:20.520]   That's what I'd like to know is do younger people.
[01:43:20.520 --> 01:43:21.520]   Are they watching?
[01:43:21.520 --> 01:43:25.760]   I mean, yeah, because I was able to find the specific breakdowns.
[01:43:25.760 --> 01:43:27.440]   I guess it's from last year.
[01:43:27.440 --> 01:43:33.120]   But even then, that was found the demographic of 18 to 34.
[01:43:33.120 --> 01:43:38.920]   Only it was a 20.95, which is down 13% from the year prior.
[01:43:38.920 --> 01:43:43.240]   And I mean, I would assume that it's probably a similar trend this year.
[01:43:43.240 --> 01:43:52.960]   It seems like the notion of gathering around to watch ads seems a little off to me.
[01:43:52.960 --> 01:43:54.560]   I didn't watch the Super Bowl.
[01:43:54.560 --> 01:43:58.400]   So I'm probably not the right person to make this proclamation.
[01:43:58.400 --> 01:44:06.240]   But it seems very, I don't know, not in line with the current culture.
[01:44:06.240 --> 01:44:11.440]   Here from at age, the average age of a Super Bowl viewers 55.
[01:44:11.440 --> 01:44:12.440]   That tracks.
[01:44:12.440 --> 01:44:13.440]   Yeah.
[01:44:13.440 --> 01:44:14.440]   Yeah.
[01:44:14.440 --> 01:44:20.600]   By contrast, the most coveted demographic for advertisers, the one you were speaking
[01:44:20.600 --> 01:44:23.120]   of 18 to 34.
[01:44:23.120 --> 01:44:28.120]   But I have to say, there are probably not a lot of big events that can draw a young
[01:44:28.120 --> 01:44:30.080]   audience better than the Super Bowl.
[01:44:30.080 --> 01:44:32.000]   I mean, it may not be the best.
[01:44:32.000 --> 01:44:38.000]   Well, I think the question is, are there big events five years from now that draw that
[01:44:38.000 --> 01:44:39.200]   type of critical mass?
[01:44:39.200 --> 01:44:42.480]   I think we're like a third of the country.
[01:44:42.480 --> 01:44:43.480]   I don't know.
[01:44:43.480 --> 01:44:44.480]   I don't think so.
[01:44:44.480 --> 01:44:46.640]   I think we're pretty diffuse now and what we pay attention to.
[01:44:46.640 --> 01:44:50.880]   And I just, I wonder if going forward, we have these like moments where a third of the
[01:44:50.880 --> 01:44:53.360]   country is all doing the same thing at the same time.
[01:44:53.360 --> 01:44:55.360]   Those are going to be a similar.
[01:44:55.360 --> 01:44:56.360]   I think there's a power in.
[01:44:56.360 --> 01:44:58.200]   I think it's going to be similar.
[01:44:58.200 --> 01:44:59.200]   Yeah.
[01:44:59.200 --> 01:45:00.200]   Yeah.
[01:45:00.200 --> 01:45:01.200]   Collective experience.
[01:45:01.200 --> 01:45:03.040]   We're all fragmented now.
[01:45:03.040 --> 01:45:04.040]   Right.
[01:45:04.040 --> 01:45:06.400]   I mean, I think it's kind of the micro influencer trend.
[01:45:06.400 --> 01:45:09.440]   I think we're going to start seeing that in advertising more often, which is specifically
[01:45:09.440 --> 01:45:15.880]   targeting like key demographics, even if it isn't, you know, 30% of the US, it is a
[01:45:15.880 --> 01:45:20.840]   specific demographic that is interested in a product like that.
[01:45:20.840 --> 01:45:25.360]   And it might be a much smaller sample size, but then you can kind of spread those ad dollars
[01:45:25.360 --> 01:45:26.360]   around.
[01:45:26.360 --> 01:45:29.360]   So Spotify's doing the right thing, right?
[01:45:29.360 --> 01:45:30.360]   I guess.
[01:45:30.360 --> 01:45:32.520]   Even so, but I don't know.
[01:45:32.520 --> 01:45:33.520]   Spotify is.
[01:45:33.520 --> 01:45:39.640]   If Rogan has 11 million listeners, which is he claims that's a large number, but that
[01:45:39.640 --> 01:45:44.200]   is a far cry from 130 million people all tuning into the same thing at the same time.
[01:45:44.200 --> 01:45:50.240]   And I just, I do think it's worth foregrounding that for just a moment because while we, you
[01:45:50.240 --> 01:45:55.440]   know, not everybody may want to watch the, the Super Bowl, it is we don't have a lot
[01:45:55.440 --> 01:46:00.760]   of moments in time when we are all doing the same thing, you know, we're all thinking about
[01:46:00.760 --> 01:46:02.680]   the same thing at the same time.
[01:46:02.680 --> 01:46:07.080]   And I think that that is actually important in society, that there are these moments where
[01:46:07.080 --> 01:46:14.000]   we come together, even if it's fleeting, so that we have these sort of common milestones
[01:46:14.000 --> 01:46:15.000]   in society.
[01:46:15.000 --> 01:46:19.200]   And I don't know, I'm not sure if that's going to be possible going forward, maybe.
[01:46:19.200 --> 01:46:21.640]   And I think that's part of the reason.
[01:46:21.640 --> 01:46:22.640]   Go ahead.
[01:46:22.640 --> 01:46:26.800]   I think that's part of the reason we have the polarization we see today.
[01:46:26.800 --> 01:46:31.760]   And it seems like a lot of different people are living in separate realities because
[01:46:31.760 --> 01:46:34.360]   we all live in separate information ecosystems.
[01:46:34.360 --> 01:46:39.560]   I mean, for some people, it might be a bachelor Monday.
[01:46:39.560 --> 01:46:42.640]   Some people, it is, they're looking forward to Super Bowl Sunday.
[01:46:42.640 --> 01:46:46.440]   Some people are watching some Twitch streamer.
[01:46:46.440 --> 01:46:47.520]   It's radically different.
[01:46:47.520 --> 01:46:52.920]   And I mean, I think that fracturing is going to have cascading effects with our society.
[01:46:52.920 --> 01:46:53.920]   Yeah.
[01:46:53.920 --> 01:46:56.000]   And it's, I think that that's a good point.
[01:46:56.000 --> 01:46:59.040]   I'm just looking at the IRC where I like to hang out during these shows.
[01:46:59.040 --> 01:47:02.640]   And Phoenix is mentioning the Winter Olympics on ratings.
[01:47:02.640 --> 01:47:06.960]   And I think the Winter Olympics also tend to get lower ratings, but I think this has
[01:47:06.960 --> 01:47:08.960]   been like really low ranked.
[01:47:08.960 --> 01:47:09.960]   They tanked.
[01:47:09.960 --> 01:47:10.960]   Yeah.
[01:47:10.960 --> 01:47:11.960]   Yeah.
[01:47:11.960 --> 01:47:12.960]   So I don't know.
[01:47:12.960 --> 01:47:13.960]   I don't.
[01:47:13.960 --> 01:47:14.960]   I don't.
[01:47:14.960 --> 01:47:15.960]   Yes.
[01:47:15.960 --> 01:47:17.600]   Sports are the one event where you can get a whole bunch of people together.
[01:47:17.600 --> 01:47:19.560]   I think we are moving into a niche world.
[01:47:19.560 --> 01:47:21.000]   I don't know if that's a bad thing.
[01:47:21.000 --> 01:47:25.600]   I mean, look, I'm a niche broadcaster.
[01:47:25.600 --> 01:47:30.160]   I like it that we can reach a niche audience.
[01:47:30.160 --> 01:47:31.160]   Yeah.
[01:47:31.160 --> 01:47:37.080]   I guess what I'm saying is I think it's good for people to have shared experiences.
[01:47:37.080 --> 01:47:39.720]   And I guess the point that I'm making is we have shared places.
[01:47:39.720 --> 01:47:43.760]   Maybe that's an old fashioned point of view, though.
[01:47:43.760 --> 01:47:48.880]   It may be, but humans haven't evolved beyond packs and tribalism.
[01:47:48.880 --> 01:47:53.280]   And if that's where we still are, which I think is fine, then we should acknowledge
[01:47:53.280 --> 01:47:56.720]   that those shared experiences are still kind of relevant.
[01:47:56.720 --> 01:48:02.440]   And we're so, I worry that like the absence of those shared experiences is maybe not
[01:48:02.440 --> 01:48:05.600]   great going forward, but I don't know the thing that would galvanize everybody.
[01:48:05.600 --> 01:48:09.080]   I mean, mass media is new.
[01:48:09.080 --> 01:48:13.680]   Let's remind ourselves it's only a few hundred, maybe a hundred years old.
[01:48:13.680 --> 01:48:16.520]   It's not a, it's a relatively new phenomenon.
[01:48:16.520 --> 01:48:20.240]   We didn't have those mass moments before mass media.
[01:48:20.240 --> 01:48:21.240]   Did we?
[01:48:21.240 --> 01:48:22.240]   Do we have a moment?
[01:48:22.240 --> 01:48:26.360]   I think in that case, our world was a lot smaller.
[01:48:26.360 --> 01:48:31.920]   I mean, the population of demographic you'd be thinking of to have a shared experience
[01:48:31.920 --> 01:48:38.320]   would be your town or your street or your, you know, school or church or community because
[01:48:38.320 --> 01:48:40.440]   of mass media and communication.
[01:48:40.440 --> 01:48:47.840]   Both the population that we're thinking of as in the word us is much faster, but yet
[01:48:47.840 --> 01:48:52.680]   also the sort of things that everyone is focusing on are we have almost endless possibilities.
[01:48:52.680 --> 01:48:53.680]   Yeah.
[01:48:53.680 --> 01:48:55.440]   Let's take a break.
[01:48:55.440 --> 01:48:59.080]   When we come back, I got a question on the radio show and I asked you.
[01:48:59.080 --> 01:49:01.760]   It was a very nice woman called.
[01:49:01.760 --> 01:49:07.600]   She said, I'm not very techno literate, literate, but I'm just curious what you think cyberwar
[01:49:07.600 --> 01:49:10.320]   fair is going to look like.
[01:49:10.320 --> 01:49:15.800]   And I thought, you know, I think this might be more timely because we're, I don't know
[01:49:15.800 --> 01:49:22.040]   if we're on the brink of a war in Ukraine, but already it's pretty clear the Russians
[01:49:22.040 --> 01:49:26.440]   are using DDoS attacks against the financial system in Ukraine.
[01:49:26.440 --> 01:49:33.800]   And there may be in fact a cyber front if there is a war in Ukraine.
[01:49:33.800 --> 01:49:36.160]   What would a cyberwar look like in the United States?
[01:49:36.160 --> 01:49:37.320]   We're going to take a break.
[01:49:37.320 --> 01:49:43.520]   I can ask the two of you very smart people, Paris, Martin, no, and Amy Webb are here today.
[01:49:43.520 --> 01:49:46.200]   It's great to have you.
[01:49:46.200 --> 01:49:51.160]   I try to reassure the nice person and say, don't worry, it's going to be all right.
[01:49:51.160 --> 01:49:54.000]   Maybe you don't feel the same.
[01:49:54.000 --> 01:49:56.320]   Our show today brought to you by zip recruiter.
[01:49:56.320 --> 01:49:58.680]   I'll tell you one good thing.
[01:49:58.680 --> 01:49:59.840]   People are hiring.
[01:49:59.840 --> 01:50:01.240]   There are lots of jobs out there.
[01:50:01.240 --> 01:50:07.960]   In fact, according to the latest research, 90% of employers are not only hiring, but want
[01:50:07.960 --> 01:50:09.600]   to make it a better experience.
[01:50:09.600 --> 01:50:12.600]   They know it's that's a top priority for 2022.
[01:50:12.600 --> 01:50:16.840]   You know, it's not given them snacks, although we do that, but it's also making them feel
[01:50:16.840 --> 01:50:22.800]   more valued, making them feel more important, focusing on company culture, offering more
[01:50:22.800 --> 01:50:28.040]   learning opportunities, allowing for flexibility and work schedules, being more empathetic,
[01:50:28.040 --> 01:50:32.240]   allowing them to connect, understanding their situation.
[01:50:32.240 --> 01:50:35.840]   I think that's what we're going to need to do going forward.
[01:50:35.840 --> 01:50:40.580]   And if you need to add more employees to your team, can I recommend zipper recruiter?
[01:50:40.580 --> 01:50:46.480]   Right now you can try zip recruiter for free at ziprecruiter.com/twitsiprecruiter's technology,
[01:50:46.480 --> 01:50:50.760]   finds the right candidates for your job, and proactively presents them to you.
[01:50:50.760 --> 01:50:52.720]   You can easily review those candidates.
[01:50:52.720 --> 01:50:57.640]   You can actually invite your top choices to apply, which works really well.
[01:50:57.640 --> 01:51:01.240]   When you get invited by a company to apply, you're going to apply faster.
[01:51:01.240 --> 01:51:02.600]   You're going to be there for the interviews.
[01:51:02.600 --> 01:51:05.120]   You really, it's flattering, right?
[01:51:05.120 --> 01:51:10.280]   Don't wonder zip recruiter is rated the number one hiring site in the US, according to G2
[01:51:10.280 --> 01:51:11.280]   meetings.
[01:51:11.280 --> 01:51:14.500]   Find the right employees right now with zip recruiter.
[01:51:14.500 --> 01:51:17.660]   We've had great experiences with zipper recruiter.
[01:51:17.660 --> 01:51:18.660]   It's one of the best ways.
[01:51:18.660 --> 01:51:21.820]   In fact, it is the best way we've found a hire.
[01:51:21.820 --> 01:51:22.820]   Try it for free.
[01:51:22.820 --> 01:51:25.860]   We've got an exclusive address just for you listeners.
[01:51:25.860 --> 01:51:29.220]   ziprecruiter.com/twit.
[01:51:29.220 --> 01:51:33.780]   Get a free trial ziprecruiter.com/twit.
[01:51:33.780 --> 01:51:35.740]   It is absolutely the best way to hire.
[01:51:35.740 --> 01:51:37.380]   I'll vouch for that.
[01:51:37.380 --> 01:51:40.860]   Ziprecruiter.
[01:51:40.860 --> 01:51:49.200]   I told the very nice caller not to be afraid, but the first thing, if I were attacking the
[01:51:49.200 --> 01:51:55.140]   US, I'd go after as a financial system, then I'd go after the electric power grid, and then
[01:51:55.140 --> 01:51:57.200]   I'd go after our food supply system.
[01:51:57.200 --> 01:51:59.400]   And Bob's your uncle.
[01:51:59.400 --> 01:52:00.400]   The war's over.
[01:52:00.400 --> 01:52:01.400]   Amy, am I crazy?
[01:52:01.400 --> 01:52:03.440]   You're not crazy.
[01:52:03.440 --> 01:52:10.520]   I was talking to one of the FDA commissioners, gosh, maybe six weeks ago, and we were just
[01:52:10.520 --> 01:52:13.680]   talking about the global supply of food.
[01:52:13.680 --> 01:52:19.680]   The US has not made shoring up our -- there's an intersection between cybersecurity and our
[01:52:19.680 --> 01:52:24.240]   food supply, and that just hasn't been a top priority.
[01:52:24.240 --> 01:52:32.480]   And I recognize that we can't inflate the number of government employees by 4x or 5x,
[01:52:32.480 --> 01:52:38.080]   but the truth is that we are probably less secure than we should be.
[01:52:38.080 --> 01:52:43.440]   And partially, I think that's because our federal government tends to make the states
[01:52:43.440 --> 01:52:47.520]   like leave it up to the states to do things, and then the states leave it up to the local
[01:52:47.520 --> 01:52:48.920]   cities to do things.
[01:52:48.920 --> 01:52:50.680]   We just don't have this big coordinated plan.
[01:52:50.680 --> 01:52:57.040]   So you saw a lot of -- I mean, because of COVID, like ransomware is way up in local
[01:52:57.040 --> 01:53:01.400]   governments and hospitals and things like that.
[01:53:01.400 --> 01:53:05.720]   We know that there have been critical infrastructure breaks.
[01:53:05.720 --> 01:53:09.520]   It's -- I don't know, Andy Greenberg, who's at Wired, who I think does really tremendous
[01:53:09.520 --> 01:53:10.520]   reporting.
[01:53:10.520 --> 01:53:11.520]   He's great.
[01:53:11.520 --> 01:53:18.440]   You know, he -- I guess in January, had a story about maybe Russia has already started
[01:53:18.440 --> 01:53:19.600]   something.
[01:53:19.600 --> 01:53:25.920]   You know, they -- Russia definitely sent malware to Ukrainian government computers, and it
[01:53:25.920 --> 01:53:31.000]   was a master -- it was a malware that overwrites the master boot record, and then it runs
[01:53:31.000 --> 01:53:36.360]   this big long file corruption program and overwrites file types and directories, and
[01:53:36.360 --> 01:53:38.600]   it was pretty brutal.
[01:53:38.600 --> 01:53:46.320]   And you know, if they're wiping out sensitive data at a government level, that's -- I don't
[01:53:46.320 --> 01:53:47.320]   know.
[01:53:47.320 --> 01:53:48.320]   I mean, this is part of the problem.
[01:53:48.320 --> 01:53:50.680]   Like, I actually -- for a couple of years -- I don't remember where I wrote this.
[01:53:50.680 --> 01:53:51.960]   It might have been Wired.
[01:53:51.960 --> 01:53:57.760]   We don't really have like -- we don't have an agreement upon what constitutes a digital
[01:53:57.760 --> 01:54:03.200]   act of war or a cyber act of war, and so I just -- I think we're in kind of a tricky
[01:54:03.200 --> 01:54:04.840]   situation right now.
[01:54:04.840 --> 01:54:09.040]   That's actually one of the things I told the caller was we need a Geneva Convention, a
[01:54:09.040 --> 01:54:10.040]   Geneva Accord.
[01:54:10.040 --> 01:54:13.400]   You know, there's certain things the international community agrees are beyond the pale, even
[01:54:13.400 --> 01:54:20.200]   in warfare, things like chemical warfare, and we need that for cyber warfare, because
[01:54:20.200 --> 01:54:22.560]   we're all vulnerable.
[01:54:22.560 --> 01:54:27.960]   You know, I think one of the things maybe protecting the U.S. is the fact that we have
[01:54:27.960 --> 01:54:29.840]   our own hackers.
[01:54:29.840 --> 01:54:32.200]   And if you do this to us, we can do it to you.
[01:54:32.200 --> 01:54:35.640]   I don't know what there is to protect Ukraine.
[01:54:35.640 --> 01:54:40.760]   This is from the Harvard Business Review this week, the cyber security risks of an escalating
[01:54:40.760 --> 01:54:44.880]   Russia-Ukraine conflict.
[01:54:44.880 --> 01:54:48.440]   The implications for business of conflict in Ukraine, whether conventional cyber or
[01:54:48.440 --> 01:54:55.480]   hybrid, will be felt far beyond the region's borders.
[01:54:55.480 --> 01:55:00.560]   Russia's formidable cyber forces preparing to unleash a new wave of cyber attacks on
[01:55:00.560 --> 01:55:07.880]   Ukrainian and Western energy, finance, and communications infrastructure.
[01:55:07.880 --> 01:55:14.320]   This is a way that a war in Ukraine could spiral out of control and have global implications.
[01:55:14.320 --> 01:55:19.280]   The Harvard Business Review said conflict in Ukraine presents the most acute cyber risk
[01:55:19.280 --> 01:55:23.760]   U.S. and Western corporations have ever faced.
[01:55:23.760 --> 01:55:28.320]   SISA, the Cyber Security Infrastructure Security Agency, recently issued a warning on the
[01:55:28.320 --> 01:55:35.880]   risk of Russian cyber attacks spilling over onto U.S. networks.
[01:55:35.880 --> 01:55:42.400]   And I wish I knew more about what we're up to.
[01:55:42.400 --> 01:55:47.040]   Imagine we have our own hackers.
[01:55:47.040 --> 01:55:52.640]   I find it interesting that when we hear government officials talking, the story that we're seeing
[01:55:52.640 --> 01:55:56.520]   is very much boots on the ground, troops moving around.
[01:55:56.520 --> 01:55:57.520]   We don't need government.
[01:55:57.520 --> 01:56:01.720]   We get commercial satellites that show us exactly where troops and things are.
[01:56:01.720 --> 01:56:03.600]   I mean, anybody can take a look.
[01:56:03.600 --> 01:56:07.040]   CNN had a great picture of a bridge built in from Belarus.
[01:56:07.040 --> 01:56:08.040]   Yeah.
[01:56:08.040 --> 01:56:11.400]   I mean, there's no mystery in that regard.
[01:56:11.400 --> 01:56:15.920]   There's not, but algorithmic warfare is far less costly.
[01:56:15.920 --> 01:56:17.080]   It's less visible.
[01:56:17.080 --> 01:56:18.240]   It's easier to cloak.
[01:56:18.240 --> 01:56:19.880]   It can be far more damaging.
[01:56:19.880 --> 01:56:24.080]   So I don't know.
[01:56:24.080 --> 01:56:31.040]   I wonder if the visual elements that we're seeing are cover for the digital conflict that's
[01:56:31.040 --> 01:56:32.680]   maybe already underway.
[01:56:32.680 --> 01:56:34.160]   And that has me concerned.
[01:56:34.160 --> 01:56:43.680]   And I mean, as far as risk goes to the U.S., obviously, I mean, the aggressive aspects of
[01:56:43.680 --> 01:56:44.840]   this or something risky.
[01:56:44.840 --> 01:56:52.080]   But I think the pandemic has shown us how deficient the U.S., many of the U.S. government's systems
[01:56:52.080 --> 01:56:53.080]   are.
[01:56:53.080 --> 01:56:54.560]   I mean, what was it?
[01:56:54.560 --> 01:57:00.960]   Millions of lines of a cobalt code are still running on mainframes used in banks here or
[01:57:00.960 --> 01:57:07.120]   government agencies like the VA, the Department of Justice, the Social Security Administration.
[01:57:07.120 --> 01:57:12.440]   I mean, this is part of the reason why a lot of unemployment systems crashed.
[01:57:12.440 --> 01:57:16.160]   And we're really difficult to fix during the early days of the pandemic.
[01:57:16.160 --> 01:57:20.000]   We have this very shaky infrastructure already.
[01:57:20.000 --> 01:57:26.920]   It feels like it would just take not even the largest attack to cause a lot of widespread
[01:57:26.920 --> 01:57:30.400]   destruction that would be very difficult to fix.
[01:57:30.400 --> 01:57:32.360]   I'm actually worried about cobalt.
[01:57:32.360 --> 01:57:35.360]   There's like nobody who really knows how to program it.
[01:57:35.360 --> 01:57:39.600]   Like, yes, it's a legacy system, but it's I don't even know if there are.
[01:57:39.600 --> 01:57:43.040]   Sometimes those legacy systems are more robust and reliable, frankly.
[01:57:43.040 --> 01:57:44.040]   Yeah, yeah.
[01:57:44.040 --> 01:57:45.360]   They've been running for a long time.
[01:57:45.360 --> 01:57:50.880]   The president on Tuesday said if Russia attacks the United States or its allies through asymmetric
[01:57:50.880 --> 01:57:56.480]   means like disruptive cyber attacks against our companies or critical infrastructure,
[01:57:56.480 --> 01:57:59.280]   we are prepared to respond.
[01:57:59.280 --> 01:58:03.520]   He didn't go into great detail, but he did say the US and its NATO allies are boosting
[01:58:03.520 --> 01:58:06.400]   their collective defenses in cyberspace.
[01:58:06.400 --> 01:58:13.920]   This might be, I mean, this might be the first real cyber war we've ever seen.
[01:58:13.920 --> 01:58:16.760]   And I don't know if we're prepared for something like that.
[01:58:16.760 --> 01:58:17.920]   I hope we are.
[01:58:17.920 --> 01:58:21.720]   Well, we were in process to being prepared.
[01:58:21.720 --> 01:58:28.560]   Our military doesn't, has just a couple of months ago, there are these new centers that
[01:58:28.560 --> 01:58:29.560]   have popped up.
[01:58:29.560 --> 01:58:33.200]   I have to take a look and see what they're called exactly, but to try to get more coordination
[01:58:33.200 --> 01:58:37.880]   between the different departments and to get the cyber people and the data people all
[01:58:37.880 --> 01:58:38.880]   talking about their--
[01:58:38.880 --> 01:58:39.880]   I hope they're doing that.
[01:58:39.880 --> 01:58:40.880]   I hope so.
[01:58:40.880 --> 01:58:46.680]   We were just in the process of organizing all of that.
[01:58:46.680 --> 01:58:51.280]   This is probably not the best possible time for something like this.
[01:58:51.280 --> 01:58:58.560]   You're saying the last four years have not been the optimum for preparing for cyber warfare.
[01:58:58.560 --> 01:58:59.560]   Right.
[01:58:59.560 --> 01:59:02.640]   So there's a new artificial intelligence and data accelerator.
[01:59:02.640 --> 01:59:06.720]   It's part of the joint artificial intelligence centers project.
[01:59:06.720 --> 01:59:12.040]   They're trying to boost-- what's happening is they're trying to coordinate AI efforts
[01:59:12.040 --> 01:59:17.400]   across all of DoDino, but that had just started to be in progress.
[01:59:17.400 --> 01:59:26.600]   Yeah, it's estimated 75% of all ransomware is originated in Russia.
[01:59:26.600 --> 01:59:28.880]   Russia would like to say, well, this is just bad actors.
[01:59:28.880 --> 01:59:32.520]   We have nothing to do with it, but I don't know if that's the case.
[01:59:32.520 --> 01:59:36.120]   I don't know if that's the case.
[01:59:36.120 --> 01:59:41.560]   We don't even know if, as you said, is that an act of war?
[01:59:41.560 --> 01:59:42.560]   Right.
[01:59:42.560 --> 01:59:44.560]   So this is part of the issue.
[01:59:44.560 --> 01:59:46.960]   You need to have some of these terms defined.
[01:59:46.960 --> 01:59:47.960]   Yeah.
[01:59:47.960 --> 01:59:52.880]   And because also, let me ask this question a different way.
[01:59:52.880 --> 02:00:01.040]   So if it's true that, let's say, Russia is screwing around with Ukraine and in the process
[02:00:01.040 --> 02:00:06.520]   somehow attacks the United States in some more meaningful way than it has.
[02:00:06.520 --> 02:00:10.680]   Like at that point do our NATO allies-- who do we have to convince?
[02:00:10.680 --> 02:00:14.800]   How does NATO decide that there has been a significant enough of an attack that they
[02:00:14.800 --> 02:00:16.720]   are willing to retaliate?
[02:00:16.720 --> 02:00:20.040]   And if it was a digital attack, what are they retaliating with?
[02:00:20.040 --> 02:00:21.600]   You know what I mean?
[02:00:21.600 --> 02:00:27.000]   I feel like we've got asymmetry in how-- I don't know, how we're thinking through this.
[02:00:27.000 --> 02:00:33.760]   That being said, I know folks at DOD who are very sharp and my assumption is that somebody
[02:00:33.760 --> 02:00:36.360]   somewhere is thinking about it.
[02:00:36.360 --> 02:00:41.840]   I think the operative here is thinking versus we have thought through.
[02:00:41.840 --> 02:00:47.120]   Wednesday, US officials warned that Russian state-sponsored cyber actors have regularly
[02:00:47.120 --> 02:00:54.000]   targeted US-cleared defense contractors since at least January 2020.
[02:00:54.000 --> 02:01:02.840]   And this is the director of CISA, Jen Easterly, using the early warning system called Twitter.
[02:01:02.840 --> 02:01:08.040]   Every organization in the US is at risk from cyber threats that can disrupt essential services.
[02:01:08.040 --> 02:01:12.320]   As we know, the Russians have used cyber as a key component-- I don't like the use of
[02:01:12.320 --> 02:01:14.240]   cyber in this and now.
[02:01:14.240 --> 02:01:18.040]   Have you cyber as a key component of their forced projection to include disabling or
[02:01:18.040 --> 02:01:23.600]   destroying critical infrastructure while there are no specific credible threats to the US
[02:01:23.600 --> 02:01:24.600]   homeland at this time.
[02:01:24.600 --> 02:01:30.520]   We are mindful of the potential for Russia to consider escalating its destabilizing action
[02:01:30.520 --> 02:01:35.880]   in ways that may affect our critical infrastructure to include cascading impacts as we saw with
[02:01:35.880 --> 02:01:40.280]   not Peccia, which was a Russian originating ransomware.
[02:01:40.280 --> 02:01:43.640]   All organizations must adopt a heightened posture of vigilance.
[02:01:43.640 --> 02:01:45.200]   The time to act is now.
[02:01:45.200 --> 02:01:46.200]   This is on Twitter.
[02:01:46.200 --> 02:01:49.280]   And by the way, she's dressed in a superhero outfit.
[02:01:49.280 --> 02:01:52.800]   I'm a little nervous to be honest with you.
[02:01:52.800 --> 02:01:57.800]   We're urging all orgs to put shields up to reduce the likelihood of cyber intrusion quickly
[02:01:57.800 --> 02:02:02.960]   detect a potential intrusion, ensure you're prepared to respond, maximum resilience.
[02:02:02.960 --> 02:02:05.960]   I'm glad we've got a motto.
[02:02:05.960 --> 02:02:07.720]   I think that was intended for us.
[02:02:07.720 --> 02:02:09.440]   I think that's public posturing.
[02:02:09.440 --> 02:02:11.600]   All of this is public posturing at this point.
[02:02:11.600 --> 02:02:13.960]   That's what, you know.
[02:02:13.960 --> 02:02:17.800]   If you are a country like Russia, you can feel the pretty big army they have.
[02:02:17.800 --> 02:02:21.960]   They're on the ground right outside Ukraine.
[02:02:21.960 --> 02:02:30.040]   But you don't need a giant force of hackers to really wreak havoc in a world that is ill-prepared
[02:02:30.040 --> 02:02:31.680]   for this.
[02:02:31.680 --> 02:02:35.360]   I hope this doesn't become the first cyber war.
[02:02:35.360 --> 02:02:37.240]   I fear that it might.
[02:02:37.240 --> 02:02:40.960]   I fear that it already is in progress to be perfectly honest.
[02:02:40.960 --> 02:02:41.960]   That's right.
[02:02:41.960 --> 02:02:42.960]   Yeah, that's right.
[02:02:42.960 --> 02:02:46.720]   Yeah, I feel like it would be naive to say that the first cyber war is coming.
[02:02:46.720 --> 02:02:49.040]   You know, it's here.
[02:02:49.040 --> 02:02:51.800]   It's here and probably has been.
[02:02:51.800 --> 02:02:56.200]   Is now the time we start talking about prepping?
[02:02:56.200 --> 02:03:00.360]   I'm taking a big hole in the backyard.
[02:03:00.360 --> 02:03:01.360]   How do you?
[02:03:01.360 --> 02:03:02.360]   Leah, what's in your basement?
[02:03:02.360 --> 02:03:03.720]   What's in my basement?
[02:03:03.720 --> 02:03:04.960]   How do you prep for this?
[02:03:04.960 --> 02:03:08.160]   So this is the conversation I was having with this woman.
[02:03:08.160 --> 02:03:11.480]   Should you go and plant your victory garden now so that you have food?
[02:03:11.480 --> 02:03:13.760]   Because we only have five days of food supply.
[02:03:13.760 --> 02:03:18.480]   What would you do if the grocery store shelves were empty in a week?
[02:03:18.480 --> 02:03:19.920]   What would you do?
[02:03:19.920 --> 02:03:20.920]   Where would you go?
[02:03:20.920 --> 02:03:22.760]   What would I do?
[02:03:22.760 --> 02:03:24.160]   I would be fine.
[02:03:24.160 --> 02:03:26.120]   My husband's going to kill me for saying this.
[02:03:26.120 --> 02:03:33.920]   I would be fine because I am somebody who has supplies shoved away in our basement.
[02:03:33.920 --> 02:03:34.920]   I know what this sounds like.
[02:03:34.920 --> 02:03:35.920]   I know what this is.
[02:03:35.920 --> 02:03:36.920]   It's a tuna fish in the basement.
[02:03:36.920 --> 02:03:37.920]   I understand.
[02:03:37.920 --> 02:03:39.080]   No, I actually said that too.
[02:03:39.080 --> 02:03:40.080]   We got a big pantry.
[02:03:40.080 --> 02:03:41.960]   We got lots of staples.
[02:03:41.960 --> 02:03:44.320]   I still have a lot of beans left over from here.
[02:03:44.320 --> 02:03:45.800]   It is quarantined.
[02:03:45.800 --> 02:03:46.800]   Beans are good.
[02:03:46.800 --> 02:03:47.800]   Beans are good.
[02:03:47.800 --> 02:03:49.560]   You could live on beans for a while.
[02:03:49.560 --> 02:03:50.560]   And tomatoes?
[02:03:50.560 --> 02:03:51.560]   Leo is a side note.
[02:03:51.560 --> 02:03:52.560]   We had that.
[02:03:52.560 --> 02:03:53.920]   You had that screenshot of Twitter going up.
[02:03:53.920 --> 02:03:56.320]   Do you have an NFT profile photo?
[02:03:56.320 --> 02:03:58.360]   Is that for you and his T-guy?
[02:03:58.360 --> 02:03:59.360]   No.
[02:03:59.360 --> 02:04:00.600]   That's what it starts, Leo.
[02:04:00.600 --> 02:04:01.600]   It starts the joke.
[02:04:01.600 --> 02:04:02.600]   It started the...
[02:04:02.600 --> 02:04:03.600]   No, it started the...
[02:04:03.600 --> 02:04:07.720]   So you noticed my hexagonal avatar there.
[02:04:07.720 --> 02:04:09.360]   It's terrifying.
[02:04:09.360 --> 02:04:12.080]   You know, this was the thing.
[02:04:12.080 --> 02:04:13.120]   I am Twitter blue.
[02:04:13.120 --> 02:04:18.320]   I admit, but one of the features of Twitter blue is you could get a hexagonal.
[02:04:18.320 --> 02:04:19.320]   What was it?
[02:04:19.320 --> 02:04:22.320]   You had to have an NFT to have a hexagonal Twitter?
[02:04:22.320 --> 02:04:29.880]   Yeah, if you have some, I guess, an NFT or can connect it to your wallet, then you can
[02:04:29.880 --> 02:04:37.200]   get your profile photo to be a hexagonal profile that is your NFT.
[02:04:37.200 --> 02:04:41.880]   So all I did is I went to a site and I just edited it so it's an X-agonal.
[02:04:41.880 --> 02:04:47.320]   You should put your face onto a board Ape is my pitch.
[02:04:47.320 --> 02:04:48.720]   I think that would be fun.
[02:04:48.720 --> 02:04:52.280]   And that's why I picked this particular avatar because it is the closest I'm going to get
[02:04:52.280 --> 02:04:54.800]   to a board Ape.
[02:04:54.800 --> 02:04:56.280]   This is actually from Mad magazine.
[02:04:56.280 --> 02:04:57.640]   I don't know where it's a right click.
[02:04:57.640 --> 02:04:59.480]   It's a right click lifestyle now.
[02:04:59.480 --> 02:05:03.120]   It's a right click life for us.
[02:05:03.120 --> 02:05:04.680]   This is from Mad magazine.
[02:05:04.680 --> 02:05:05.680]   I was...
[02:05:05.680 --> 02:05:08.560]   Lisa and I were in a Mad magazine at one point because...
[02:05:08.560 --> 02:05:09.560]   Oh, wow.
[02:05:09.560 --> 02:05:10.560]   Yeah.
[02:05:10.560 --> 02:05:11.560]   Before the fall.
[02:05:11.560 --> 02:05:12.560]   Yeah.
[02:05:12.560 --> 02:05:13.560]   So that should be an NFT, shouldn't it?
[02:05:13.560 --> 02:05:14.560]   That should be a...
[02:05:14.560 --> 02:05:18.200]   I mean, magazines, if you think about it, they're kind of like the original NFT.
[02:05:18.200 --> 02:05:19.200]   Yeah.
[02:05:19.200 --> 02:05:22.200]   And I bet you, Amy, you can figure out where this picture of me with a donkey.
[02:05:22.200 --> 02:05:23.560]   What I was going to say is that Petra?
[02:05:23.560 --> 02:05:24.560]   That's Petra.
[02:05:24.560 --> 02:05:25.560]   Jordan, yeah.
[02:05:25.560 --> 02:05:26.560]   Yeah.
[02:05:26.560 --> 02:05:27.560]   Yeah.
[02:05:27.560 --> 02:05:28.560]   Lisa and I were in Petra.
[02:05:28.560 --> 02:05:34.720]   I guess that was the last big trip we took before the pandemic in October of 2019.
[02:05:34.720 --> 02:05:36.120]   Beautiful.
[02:05:36.120 --> 02:05:37.120]   And I know that this...
[02:05:37.120 --> 02:05:38.120]   I've read your...
[02:05:38.120 --> 02:05:42.840]   I saw your TED talk about how this was your goal in life was to get to Petra.
[02:05:42.840 --> 02:05:44.840]   It's an amazing, amazing place.
[02:05:44.840 --> 02:05:45.840]   Yeah.
[02:05:45.840 --> 02:05:49.200]   It's pretty tell how much the internet has warped my brain because when you said Petra, the
[02:05:49.200 --> 02:05:55.400]   first thing that came to my head was Petra, the World Wonder in Civilizations, Revolutions
[02:05:55.400 --> 02:06:02.840]   6, a video game and not Petra, the actual thing that that World Wonder is based off of
[02:06:02.840 --> 02:06:03.840]   the video game.
[02:06:03.840 --> 02:06:05.840]   I'm sure Sid Meier was thinking of Petra, Jordan.
[02:06:05.840 --> 02:06:08.760]   I mean, it's quite literally Petra, but in there.
[02:06:08.760 --> 02:06:09.760]   Oh, wow.
[02:06:09.760 --> 02:06:10.760]   So is this simulation like this?
[02:06:10.760 --> 02:06:11.760]   You can say, "I've been there.
[02:06:11.760 --> 02:06:12.760]   I've been there."
[02:06:12.760 --> 02:06:13.760]   Civ 6.
[02:06:13.760 --> 02:06:17.800]   So my daughter's name is Petra because of that place.
[02:06:17.800 --> 02:06:22.800]   And I tried to play Civ 6 with her and we couldn't get...
[02:06:22.800 --> 02:06:24.200]   It was too hard to explain.
[02:06:24.200 --> 02:06:26.720]   I couldn't get her going on her own.
[02:06:26.720 --> 02:06:28.200]   I think it's kind of boring.
[02:06:28.200 --> 02:06:29.680]   Yeah, the tutorial is like...
[02:06:29.680 --> 02:06:30.680]   The tutorial is a whole game.
[02:06:30.680 --> 02:06:31.680]   It was too much.
[02:06:31.680 --> 02:06:32.680]   It was pretty though.
[02:06:32.680 --> 02:06:33.680]   That's the thing though.
[02:06:33.680 --> 02:06:35.880]   I thought it was going to be too much for me at first and then I was like, "Okay, let
[02:06:35.880 --> 02:06:37.080]   me try this tutorial."
[02:06:37.080 --> 02:06:39.280]   And then months of my life went by.
[02:06:39.280 --> 02:06:40.280]   Should I get into it?
[02:06:40.280 --> 02:06:41.280]   Should I...
[02:06:41.280 --> 02:06:46.040]   Because I love like the age of empires and all that stuff, but Civ was turn based, which
[02:06:46.040 --> 02:06:47.760]   kind of turned me off.
[02:06:47.760 --> 02:06:49.520]   I love turn based.
[02:06:49.520 --> 02:06:50.520]   You don't mind that.
[02:06:50.520 --> 02:06:51.520]   Civ 6.
[02:06:51.520 --> 02:06:52.520]   Fantastic.
[02:06:52.520 --> 02:06:55.440]   I like it because I'm a very anxious person and I want to be able to sit there and think
[02:06:55.440 --> 02:06:57.840]   about my move for as long as I need.
[02:06:57.840 --> 02:06:58.840]   There's no age.
[02:06:58.840 --> 02:07:00.640]   This is also why I love the fire emblem games.
[02:07:00.640 --> 02:07:01.640]   Fantastic.
[02:07:01.640 --> 02:07:02.640]   Yeah.
[02:07:02.640 --> 02:07:03.640]   Okay.
[02:07:03.640 --> 02:07:05.840]   I have like 10 minutes a day of free time at this point.
[02:07:05.840 --> 02:07:08.720]   So that's not even...
[02:07:08.720 --> 02:07:14.440]   That amount of time isn't enough to read the instructions per turn in Civ 6.
[02:07:14.440 --> 02:07:15.440]   So yeah.
[02:07:15.440 --> 02:07:16.440]   That's fair.
[02:07:16.440 --> 02:07:17.440]   I'm playing like...
[02:07:17.440 --> 02:07:18.440]   I'm playing nothing.
[02:07:18.440 --> 02:07:19.440]   I've got no game.
[02:07:19.440 --> 02:07:24.040]   I mean, I like to play Zelda, but my family keeps playing without me with my character.
[02:07:24.040 --> 02:07:25.040]   And then I...
[02:07:25.040 --> 02:07:26.040]   Wow, that's so emotional.
[02:07:26.040 --> 02:07:27.040]   It's like, "Great.
[02:07:27.040 --> 02:07:31.440]   I don't know where the hell I am or what this tool is or what that Boba Fett thing is.
[02:07:31.440 --> 02:07:32.440]   I don't know what's going on."
[02:07:32.440 --> 02:07:36.360]   But thank goodness they did because it gave you a great opening when you were on coast
[02:07:36.360 --> 02:07:37.760]   to coast.
[02:07:37.760 --> 02:07:38.760]   And so...
[02:07:38.760 --> 02:07:39.760]   Yeah.
[02:07:39.760 --> 02:07:40.760]   Yeah.
[02:07:40.760 --> 02:07:44.000]   And if you don't know what we're talking about, listen to the triangulation we did yesterday
[02:07:44.000 --> 02:07:45.760]   with Amy Webb because she tells that story.
[02:07:45.760 --> 02:07:51.080]   It actually turned out to your benefit, you actually did a great job changing somebody's
[02:07:51.080 --> 02:07:53.920]   mind about COVID vaccinations because of it.
[02:07:53.920 --> 02:07:54.920]   So good to go on.
[02:07:54.920 --> 02:07:55.920]   Yeah.
[02:07:55.920 --> 02:07:56.920]   My husband's actually now using...
[02:07:56.920 --> 02:07:59.920]   He's using when people come in and talk about vaccines.
[02:07:59.920 --> 02:08:01.320]   He's a eye doctor.
[02:08:01.320 --> 02:08:04.200]   This is the story he's now using them to help them understand.
[02:08:04.200 --> 02:08:05.200]   So...
[02:08:05.200 --> 02:08:06.200]   So you start out...
[02:08:06.200 --> 02:08:08.280]   Well, it's kind of like Zelda.
[02:08:08.280 --> 02:08:09.280]   That would be good.
[02:08:09.280 --> 02:08:10.280]   Yeah.
[02:08:10.280 --> 02:08:11.280]   What?
[02:08:11.280 --> 02:08:12.280]   Huh?
[02:08:12.280 --> 02:08:13.280]   That'd get my attention.
[02:08:13.280 --> 02:08:15.280]   It's like a video game versus Zelda.
[02:08:15.280 --> 02:08:16.280]   Yeah.
[02:08:16.280 --> 02:08:19.280]   It's kind of like Zelda.
[02:08:19.280 --> 02:08:20.280]   One more break.
[02:08:20.280 --> 02:08:21.280]   We're going to wrap things up.
[02:08:21.280 --> 02:08:22.280]   So much.
[02:08:22.280 --> 02:08:26.080]   I want to talk with you about, but I've taken a lot of your time.
[02:08:26.080 --> 02:08:27.800]   So nice to have you both here.
[02:08:27.800 --> 02:08:29.840]   Paris Martin, the information.
[02:08:29.840 --> 02:08:31.760]   What are you working on more Amazon stuff?
[02:08:31.760 --> 02:08:33.680]   We should talk about the unionization move.
[02:08:33.680 --> 02:08:34.680]   Yeah.
[02:08:34.680 --> 02:08:37.440]   I mean, there's a lot of stuff going on with Amazon just keeps a bit bigger.
[02:08:37.440 --> 02:08:39.040]   When we come back and do that.
[02:08:39.040 --> 02:08:40.040]   Yeah.
[02:08:40.040 --> 02:08:43.680]   Our box is now starting to unionize too, which I thought was very interesting.
[02:08:43.680 --> 02:08:46.160]   There's a big movement, a foot.
[02:08:46.160 --> 02:08:48.960]   Also Amy Webb, futurist.
[02:08:48.960 --> 02:08:50.800]   And I should just...
[02:08:50.800 --> 02:08:53.240]   Oh, and there's the cat.
[02:08:53.240 --> 02:08:55.280]   She keeps trying to hop on my keyboard.
[02:08:55.280 --> 02:08:57.760]   She really wants to get online.
[02:08:57.760 --> 02:08:59.960]   I'm telling her the internet's a bad face.
[02:08:59.960 --> 02:09:00.960]   Hey, where's your cat?
[02:09:00.960 --> 02:09:01.960]   Wait a minute.
[02:09:01.960 --> 02:09:02.960]   You don't have a cat.
[02:09:02.960 --> 02:09:04.440]   You have like a Gila monster or something.
[02:09:04.440 --> 02:09:05.440]   What do you have?
[02:09:05.440 --> 02:09:06.440]   No, I have...
[02:09:06.440 --> 02:09:07.440]   I have...
[02:09:07.440 --> 02:09:08.440]   I have...
[02:09:08.440 --> 02:09:09.440]   I have a fish.
[02:09:09.440 --> 02:09:10.440]   Its name is Fish.
[02:09:10.440 --> 02:09:13.640]   You can get the fish in the shot.
[02:09:13.640 --> 02:09:14.640]   It's been...
[02:09:14.640 --> 02:09:15.640]   That fish...
[02:09:15.640 --> 02:09:17.640]   The cat would like the fish.
[02:09:17.640 --> 02:09:18.800]   Fun fact.
[02:09:18.800 --> 02:09:22.800]   So goldfish apparently live for like 30 years and they just keep growing.
[02:09:22.800 --> 02:09:27.560]   So currently we have a fish named Fish that we got at a carnival or something stupid.
[02:09:27.560 --> 02:09:29.040]   And it's like that big and it has...
[02:09:29.040 --> 02:09:30.040]   Oh yeah.
[02:09:30.040 --> 02:09:31.040]   You can't put anything else in the tank.
[02:09:31.040 --> 02:09:32.040]   I need a koi pond now.
[02:09:32.040 --> 02:09:33.040]   It's a monster.
[02:09:33.040 --> 02:09:34.040]   You got a koi.
[02:09:34.040 --> 02:09:36.240]   Yeah, if you put it in a koi pond, it'll get like this big.
[02:09:36.240 --> 02:09:37.240]   Yeah.
[02:09:37.240 --> 02:09:38.240]   Yeah.
[02:09:38.240 --> 02:09:42.800]   And then forgets us but then knows us again and we got a whole thing and yeah...
[02:09:42.800 --> 02:09:43.960]   Just ask the cat.
[02:09:43.960 --> 02:09:47.680]   Them's good eating.
[02:09:47.680 --> 02:09:51.960]   Our show today is by Stamps.com.
[02:09:51.960 --> 02:09:54.440]   Oh, I love Stamps.com.
[02:09:54.440 --> 02:09:58.600]   I have Stamps, actual Stamps with my head on them.
[02:09:58.600 --> 02:10:00.080]   But I made a mistake when I made them.
[02:10:00.080 --> 02:10:01.320]   I didn't make forever stamps.
[02:10:01.320 --> 02:10:05.480]   So every couple of months I have to go to Debbie and I say, "Could you print me some more
[02:10:05.480 --> 02:10:07.960]   stamps, penny stamps, two cent stamps?"
[02:10:07.960 --> 02:10:11.040]   So I can still use the picture with my head on it.
[02:10:11.040 --> 02:10:15.320]   And then if you ever get an email or not email a snail mail from me, you'll see a bunch of
[02:10:15.320 --> 02:10:18.640]   one cent and two cent stamps to bring the value up.
[02:10:18.640 --> 02:10:19.640]   That's the problem, right?
[02:10:19.640 --> 02:10:21.600]   The postage changes.
[02:10:21.600 --> 02:10:23.840]   There's always this mystery.
[02:10:23.840 --> 02:10:26.600]   Stamps.com solves this problem.
[02:10:26.600 --> 02:10:29.000]   Not a trip to the post office.
[02:10:29.000 --> 02:10:30.960]   You don't need to do that with Stamps.com.
[02:10:30.960 --> 02:10:31.960]   Skip the trip.
[02:10:31.960 --> 02:10:33.680]   Focus on your small business.
[02:10:33.680 --> 02:10:39.280]   And when you need postage, you use your own computer, your own printer.
[02:10:39.280 --> 02:10:42.320]   You spend less time at the post office, more time making customers happy.
[02:10:42.320 --> 02:10:45.400]   You can even mail from Stamps.com.
[02:10:45.400 --> 02:10:48.480]   Just print the label on any package, any envelope.
[02:10:48.480 --> 02:10:53.640]   And the post mail carrier, a uniformed employee of the federal government will come to your
[02:10:53.640 --> 02:10:56.760]   home, come to your office, pick it up.
[02:10:56.760 --> 02:10:58.960]   We've been using Stamps.com for more than a decade.
[02:10:58.960 --> 02:11:01.760]   They've been a partner of the show and advertisers since 2012.
[02:11:01.760 --> 02:11:05.360]   In fact, I've been telling you about Stamps.com for 10 years.
[02:11:05.360 --> 02:11:06.360]   Why?
[02:11:06.360 --> 02:11:07.360]   What is keeping you?
[02:11:07.360 --> 02:11:08.360]   Stamps.com saves you time.
[02:11:08.360 --> 02:11:09.360]   It saves you money.
[02:11:09.360 --> 02:11:10.360]   It saves you stress.
[02:11:10.360 --> 02:11:12.400]   And now there's something new with Stamps.com.
[02:11:12.400 --> 02:11:16.680]   Not only do you get access to all the United States Postal Service has to offer from your
[02:11:16.680 --> 02:11:20.000]   own desk, they now support UPS.
[02:11:20.000 --> 02:11:25.400]   So this is your all in one shipping partner.
[02:11:25.400 --> 02:11:26.400]   You'll get discounts.
[02:11:26.400 --> 02:11:27.640]   You can't get anywhere else.
[02:11:27.640 --> 02:11:33.160]   Up to 40% off postal service rates, up to 76% off UPS rates.
[02:11:33.160 --> 02:11:37.640]   Whether you're in office setting invoices, a side hustle, Etsy shop, a full blown warehouse
[02:11:37.640 --> 02:11:41.560]   shipping out orders, Stamps.com makes your life easier.
[02:11:41.560 --> 02:11:44.760]   No special supplies, no special anchor equipment.
[02:11:44.760 --> 02:11:48.120]   Just your computer, your printer, you're up and running in minutes printing official
[02:11:48.120 --> 02:11:51.640]   postage for any letter, any package, anywhere you want to send.
[02:11:51.640 --> 02:11:53.600]   And we always have postage.
[02:11:53.600 --> 02:11:58.440]   In fact, we just get the little, you know, stick them labels, print the postage right
[02:11:58.440 --> 02:11:59.440]   on that.
[02:11:59.440 --> 02:12:00.720]   It's amazing.
[02:12:00.720 --> 02:12:02.040]   Stop overpaying for shipping.
[02:12:02.040 --> 02:12:03.200]   Get the discounts.
[02:12:03.200 --> 02:12:06.920]   Get the convenience of Stamps.com.
[02:12:06.920 --> 02:12:08.200]   When you sign up today, here's what you do.
[02:12:08.200 --> 02:12:09.200]   You go to Stamps.com.
[02:12:09.200 --> 02:12:10.520]   We're in the right corner.
[02:12:10.520 --> 02:12:12.800]   You say, "Hurt us on radio or podcast.
[02:12:12.800 --> 02:12:14.360]   Click that."
[02:12:14.360 --> 02:12:15.640]   Sometimes there's a microphone, sometimes not.
[02:12:15.640 --> 02:12:18.240]   The promo code to enter is TWIT.
[02:12:18.240 --> 02:12:19.560]   Couldn't be easier.
[02:12:19.560 --> 02:12:20.560]   TWIT.
[02:12:20.560 --> 02:12:24.520]   You'll get a special offer, includes a four week trial of Stamps.com.
[02:12:24.520 --> 02:12:28.320]   You'll get a bunch of free postage you can use over the life of your account.
[02:12:28.320 --> 02:12:33.560]   A digital scale so that you never have to guess what the postage is.
[02:12:33.560 --> 02:12:40.320]   And there are no long term commitments, no contracts, just convenience and savings. Stamps.com.
[02:12:40.320 --> 02:12:41.800]   Click the top of the home page.
[02:12:41.800 --> 02:12:42.800]   Offer code, TWIT.
[02:12:42.800 --> 02:12:44.640]   Please use that offer code.
[02:12:44.640 --> 02:12:45.640]   We love them.
[02:12:45.640 --> 02:12:46.920]   Couldn't not be happier.
[02:12:46.920 --> 02:12:48.560]   We've been using them.
[02:12:48.560 --> 02:12:50.920]   As I said, more than a decade.
[02:12:50.920 --> 02:12:52.560]   Stamps.com.
[02:12:52.560 --> 02:12:58.280]   How to fund week this week on TWIT.tv.
[02:12:58.280 --> 02:13:00.800]   We've made a little, I don't know, movie for you.
[02:13:00.800 --> 02:13:01.800]   Watch this.
[02:13:01.800 --> 02:13:03.800]   Shucks, I meant to send you a bottle of this.
[02:13:03.800 --> 02:13:04.800]   I will do this.
[02:13:04.800 --> 02:13:05.800]   A bottle of what?
[02:13:05.800 --> 02:13:06.800]   A bottle of bourbon.
[02:13:06.800 --> 02:13:11.640]   A genetically synthesized bourbon.
[02:13:11.640 --> 02:13:12.640]   Yes.
[02:13:12.640 --> 02:13:14.920]   Previously on TWIT.
[02:13:14.920 --> 02:13:16.080]   TWIT events.
[02:13:16.080 --> 02:13:21.640]   I am very concerned that 10 years from now, the arguments we're having today about Big
[02:13:21.640 --> 02:13:25.680]   Tech are going to become arguments about Big Bio.
[02:13:25.680 --> 02:13:32.960]   And AI is one thing, but we're talking about biology, which tends to, you know, it tends
[02:13:32.960 --> 02:13:35.000]   to do what it wants.
[02:13:35.000 --> 02:13:37.800]   And this is on a planetary scale.
[02:13:37.800 --> 02:13:38.800]   All about Android.
[02:13:38.800 --> 02:13:44.480]   Lottie allows motion designers to create vector animations, export them as a file,
[02:13:44.480 --> 02:13:46.080]   just a JSON file.
[02:13:46.080 --> 02:13:50.600]   And then there are players on Android and Web and iOS and basically every other platform.
[02:13:50.600 --> 02:13:53.160]   So you can just drop in the file and play it.
[02:13:53.160 --> 02:13:59.760]   If you set up a Google Home device or a Chromecast device, you might remember these bouncing circles
[02:13:59.760 --> 02:14:02.760]   and boxes and it's almost always Lottie.
[02:14:02.760 --> 02:14:03.760]   Tech News Weekly.
[02:14:03.760 --> 02:14:07.800]   It's label printers you can't DRM any cartridge because one doesn't exist, right?
[02:14:07.800 --> 02:14:09.560]   It's just like a heating element.
[02:14:09.560 --> 02:14:16.440]   And until now, the Dymobran, they are relying on RFID chips to identify their own reams
[02:14:16.440 --> 02:14:18.440]   of thermal paper.
[02:14:18.440 --> 02:14:23.360]   You just wish that you could sort of push that idea out there that he has of like, look,
[02:14:23.360 --> 02:14:24.720]   get rid of it, get a different one.
[02:14:24.720 --> 02:14:25.960]   You're going to save money in the long run.
[02:14:25.960 --> 02:14:28.360]   Like I want to tell everyone out there who has a Dymobenter.
[02:14:28.360 --> 02:14:32.840]   Yeah, not to be a revolutionary or anything, but TWIT.
[02:14:32.840 --> 02:14:36.040]   Viva la label maker.
[02:14:36.040 --> 02:14:37.040]   There it is.
[02:14:37.040 --> 02:14:38.840]   Viva la label maker.
[02:14:38.840 --> 02:14:39.840]   Nice.
[02:14:39.840 --> 02:14:45.480]   Now that we're back, I think we have to issue a correction given to me by someone on one
[02:14:45.480 --> 02:14:48.520]   of our faithful listeners here on TWIT on Twitter.
[02:14:48.520 --> 02:14:55.200]   They sent me, fells name is Michael Soder, a Snopes article entitled, "Will Truth Social
[02:14:55.200 --> 02:14:57.440]   Cost Users 499 a Week?"
[02:14:57.440 --> 02:14:58.440]   Not so?
[02:14:58.440 --> 02:14:59.440]   No.
[02:14:59.440 --> 02:15:00.440]   Oh.
[02:15:00.440 --> 02:15:01.840]   We were spreading the false about truth.
[02:15:01.840 --> 02:15:03.280]   How much is truth?
[02:15:03.280 --> 02:15:04.960]   Free, I think.
[02:15:04.960 --> 02:15:11.080]   Well, boy, I really have to look that up because I saw that in a lot of media.
[02:15:11.080 --> 02:15:12.080]   I know.
[02:15:12.080 --> 02:15:18.480]   Apparently news circulated on social media about being, it was just fully wrong.
[02:15:18.480 --> 02:15:19.480]   Yeah.
[02:15:19.480 --> 02:15:20.480]   Okay.
[02:15:20.480 --> 02:15:21.480]   Daily got.
[02:15:21.480 --> 02:15:22.480]   Right.
[02:15:22.480 --> 02:15:23.480]   A big thing about it.
[02:15:23.480 --> 02:15:24.480]   Okay.
[02:15:24.480 --> 02:15:25.480]   Yeah.
[02:15:25.480 --> 02:15:26.480]   Okay.
[02:15:26.480 --> 02:15:27.480]   So hey, good news.
[02:15:27.480 --> 02:15:28.480]   It's free.
[02:15:28.480 --> 02:15:29.480]   I will say it's not entirely your fault, Leo.
[02:15:29.480 --> 02:15:30.480]   The New York Post had spread through.
[02:15:30.480 --> 02:15:32.760]   Well, there is my, that is my source for all.
[02:15:32.760 --> 02:15:36.360]   I mean, yeah, clearly that's where you're getting all of your information.
[02:15:36.360 --> 02:15:37.360]   So.
[02:15:37.360 --> 02:15:38.360]   Yeah.
[02:15:38.360 --> 02:15:39.360]   Okay.
[02:15:39.360 --> 02:15:40.360]   Good.
[02:15:40.360 --> 02:15:41.360]   Well, I stand corrected.
[02:15:41.360 --> 02:15:42.360]   I apologize.
[02:15:42.360 --> 02:15:43.840]   I guess I am going to sign up.
[02:15:43.840 --> 02:15:44.840]   If it's free, that's how you use it.
[02:15:44.840 --> 02:15:47.360]   I guess now we're going to have to start sending some, yes.
[02:15:47.360 --> 02:15:48.360]   It's true.
[02:15:48.360 --> 02:15:50.680]   It's, y'all, it's not free, right?
[02:15:50.680 --> 02:15:52.880]   I mean, if it's free, then...
[02:15:52.880 --> 02:15:54.040]   We'll find out tomorrow.
[02:15:54.040 --> 02:15:55.040]   Right.
[02:15:55.040 --> 02:15:56.960]   Then you are, you are still paying.
[02:15:56.960 --> 02:15:58.520]   You're the customer.
[02:15:58.520 --> 02:15:59.800]   Or are you the product?
[02:15:59.800 --> 02:16:00.800]   That's the question.
[02:16:00.800 --> 02:16:01.800]   Yeah.
[02:16:01.800 --> 02:16:04.320]   So, suppose we'll find out.
[02:16:04.320 --> 02:16:10.680]   Amazon has been trying, I mean, workers, I feel, they've been trying to unionize forever.
[02:16:10.680 --> 02:16:16.960]   Now New York's Amazon workers, an article by Paris Martineau and the information, will
[02:16:16.960 --> 02:16:19.760]   be voting.
[02:16:19.760 --> 02:16:22.760]   But Amazon is playing tough on this, right?
[02:16:22.760 --> 02:16:24.800]   They do not want a union.
[02:16:24.800 --> 02:16:31.080]   I mean, they've been playing tough enough that their first kind of big union election,
[02:16:31.080 --> 02:16:35.720]   which happened in Bessemer last year, they're having to have a duo for it because...
[02:16:35.720 --> 02:16:37.880]   The LRB through the vote out.
[02:16:37.880 --> 02:16:42.000]   Yeah, they said Amazon interfered too much with that election.
[02:16:42.000 --> 02:16:44.040]   So actually, I mean, it's really interesting.
[02:16:44.040 --> 02:16:48.160]   Right now, the second election is underway.
[02:16:48.160 --> 02:16:51.520]   It's a mail-in election for workers at Bessemer.
[02:16:51.520 --> 02:16:56.600]   Votes will start being, like, they will start being counted towards the end of March, right
[02:16:56.600 --> 02:17:03.320]   around the same time that this other warehouse here in New York is voting in person.
[02:17:03.320 --> 02:17:06.560]   So I mean, March, April, going to be a big time for Amazon.
[02:17:06.560 --> 02:17:08.960]   Could be first two unions.
[02:17:08.960 --> 02:17:12.400]   Calling the Pinkertons, Jeff, that's what I did.
[02:17:12.400 --> 02:17:15.480]   I mean, I think they did at one point literally enlist the Pinkertons.
[02:17:15.480 --> 02:17:16.720]   No, really?
[02:17:16.720 --> 02:17:17.720]   No.
[02:17:17.720 --> 02:17:19.560]   I'm pretty sure.
[02:17:19.560 --> 02:17:20.560]   No.
[02:17:20.560 --> 02:17:21.560]   We double checked.
[02:17:21.560 --> 02:17:22.840]   That's call snopes on that one.
[02:17:22.840 --> 02:17:30.000]   The Pinkertons, very famously, famous union-busting agency in the '20s, right?
[02:17:30.000 --> 02:17:33.000]   They busted, was it GM?
[02:17:33.000 --> 02:17:38.800]   I mean, yeah, they specifically, yeah, are Pinkertons spies that would bust unions, infiltrate them.
[02:17:38.800 --> 02:17:40.360]   Motherboard had reported this.
[02:17:40.360 --> 02:17:45.800]   Oh, Amazon hires Pinkertons spies to track warehouse workers.
[02:17:45.800 --> 02:17:50.440]   Well, what's so new is old as new again.
[02:17:50.440 --> 02:17:53.000]   That's kind of amazing.
[02:17:53.000 --> 02:17:54.000]   That's amazing.
[02:17:54.000 --> 02:17:57.800]   What's the origin of the word Pinkerton?
[02:17:57.800 --> 02:18:00.040]   Was there like a person with the last name, Pinkerton?
[02:18:00.040 --> 02:18:01.040]   Mr. Pinkerton?
[02:18:01.040 --> 02:18:03.200]   I think the last name was Pinkerton.
[02:18:03.200 --> 02:18:08.040]   What's amazing is it's still, the Pinkertons are still around.
[02:18:08.040 --> 02:18:10.400]   I mean, clearly, Amazon's enlisting them.
[02:18:10.400 --> 02:18:11.400]   Yeah.
[02:18:11.400 --> 02:18:16.800]   They were established in the United States by Scotsman Alan Pinkerton in the 1850s.
[02:18:16.800 --> 02:18:20.280]   I think you should do the rest of the broadcast and that.
[02:18:20.280 --> 02:18:21.280]   Oh, I shall.
[02:18:21.280 --> 02:18:22.280]   I like that accident.
[02:18:22.280 --> 02:18:27.200]   He became famous when he claimed to have foiled a plot to assassinate President-elect
[02:18:27.200 --> 02:18:32.120]   the Lincoln who later hired Pinkerton agents for his security.
[02:18:32.120 --> 02:18:34.200]   Now do Irish.
[02:18:34.200 --> 02:18:39.920]   Turn in the labor strikes in the late 19th, 20th and 21st centuries.
[02:18:39.920 --> 02:18:43.400]   Businessmen hired Pinkerton to infiltrate unions.
[02:18:43.400 --> 02:18:48.760]   The homestead strike of 19, 1892.
[02:18:48.760 --> 02:18:49.760]   That's amazing.
[02:18:49.760 --> 02:18:50.760]   Andrew Carnegie.
[02:18:50.760 --> 02:18:52.840]   You know why I got it?
[02:18:52.840 --> 02:18:55.320]   It's a combination of the Lucky Charms.
[02:18:55.320 --> 02:18:57.920]   The Lucky Charms in there.
[02:18:57.920 --> 02:18:59.320]   And the Irish Spring.
[02:18:59.320 --> 02:19:01.400]   And I like it too.
[02:19:01.400 --> 02:19:04.560]   So I just brought those two together.
[02:19:04.560 --> 02:19:06.960]   And I've never met an Irishman, but actually when Lisa-
[02:19:06.960 --> 02:19:10.280]   I'd say some Irish listeners are throwing their phones in the morning.
[02:19:10.280 --> 02:19:13.120]   Oh, I hate it when I do those accidents.
[02:19:13.120 --> 02:19:18.000]   When Lisa and I got married, we had a wonderful grass priest Irishman.
[02:19:18.000 --> 02:19:21.720]   He'd been a priest Catholic priest until he fell in love and got married.
[02:19:21.720 --> 02:19:25.720]   But he was still acting as a minister.
[02:19:25.720 --> 02:19:27.480]   His name was Peter.
[02:19:27.480 --> 02:19:29.680]   And he married us and it was a beautiful thing.
[02:19:29.680 --> 02:19:35.000]   He said, when you marry, when you wear your red and ring, you're holding your wife's heart
[02:19:35.000 --> 02:19:36.000]   in your hand.
[02:19:36.000 --> 02:19:38.240]   So you treat that with love there.
[02:19:38.240 --> 02:19:39.880]   It was beautiful.
[02:19:39.880 --> 02:19:42.680]   I felt like the Lucky Charms leprechaun was marrying us.
[02:19:42.680 --> 02:19:44.720]   It was a beautiful thing.
[02:19:44.720 --> 02:19:45.720]   That's cool.
[02:19:45.720 --> 02:19:46.720]   Yeah.
[02:19:46.720 --> 02:19:48.720]   Is that a priest made of grass?
[02:19:48.720 --> 02:19:52.520]   It's a term for a priest who is no longer.
[02:19:52.520 --> 02:19:56.360]   So you're never not a priest apparently.
[02:19:56.360 --> 02:19:58.880]   That's fun.
[02:19:58.880 --> 02:20:00.360]   Yeah.
[02:20:00.360 --> 02:20:07.120]   You may be defrocked, but once a priest always, Father Robert explained this to me once.
[02:20:07.120 --> 02:20:08.720]   And I have to ask him.
[02:20:08.720 --> 02:20:10.120]   By the way, I'm on the information.
[02:20:10.120 --> 02:20:11.120]   I'm looking.
[02:20:11.120 --> 02:20:13.000]   Most popular story.
[02:20:13.000 --> 02:20:15.600]   The digital reinvention of Playboy.
[02:20:15.600 --> 02:20:19.160]   This is for a weekend section, which has been fantastic.
[02:20:19.160 --> 02:20:21.280]   I love the weekend section because it's lighter.
[02:20:21.280 --> 02:20:22.280]   It's wonderful.
[02:20:22.280 --> 02:20:23.280]   I love Jessica's column.
[02:20:23.280 --> 02:20:25.000]   It's like little magazine-y stories.
[02:20:25.000 --> 02:20:26.000]   Yeah.
[02:20:26.000 --> 02:20:29.480]   Apparently, Playboy is pivoting.
[02:20:29.480 --> 02:20:35.840]   And I love the headline where NFTs are the magazine and the metaverse is the mansion.
[02:20:35.840 --> 02:20:37.840]   Oh my God.
[02:20:37.840 --> 02:20:39.640]   Oh boy.
[02:20:39.640 --> 02:20:43.600]   I say no to that.
[02:20:43.600 --> 02:20:44.600]   You say no?
[02:20:44.600 --> 02:20:45.600]   Okay.
[02:20:45.600 --> 02:20:48.360]   I say no.
[02:20:48.360 --> 02:20:54.080]   There is apparently, they realize that they're losing all their business to only fans.
[02:20:54.080 --> 02:21:01.160]   So they've created kind of a Playboy only fans featuring celebrities as much as young
[02:21:01.160 --> 02:21:02.840]   women.
[02:21:02.840 --> 02:21:07.040]   And then, but I can't wait to hear about the meta mansion.
[02:21:07.040 --> 02:21:08.040]   No.
[02:21:08.040 --> 02:21:11.080]   I just drove past the old mansion.
[02:21:11.080 --> 02:21:14.040]   You're a Chicago in, right?
[02:21:14.040 --> 02:21:15.040]   Me?
[02:21:15.040 --> 02:21:16.040]   Yeah.
[02:21:16.040 --> 02:21:18.840]   But I was in LA a couple of weeks ago and we drove past the Hugh-
[02:21:18.840 --> 02:21:19.320]   Oh, the LA-
[02:21:19.320 --> 02:21:20.320]   Playboy.
[02:21:20.320 --> 02:21:22.920]   Because the original mansion was in Chicago.
[02:21:22.920 --> 02:21:28.080]   My parents once, they were, I don't know why, but they went to the Playboy Club, which
[02:21:28.080 --> 02:21:31.840]   was in some, I don't know, my dad's got a story, like some building somewhere.
[02:21:31.840 --> 02:21:38.320]   I just, I for one don't really want to go anywhere where the bunnies don't have legs.
[02:21:38.320 --> 02:21:39.320]   But that's just me.
[02:21:39.320 --> 02:21:41.400]   That just might be me.
[02:21:41.400 --> 02:21:42.400]   I don't know.
[02:21:42.400 --> 02:21:43.400]   Yeah.
[02:21:43.400 --> 02:21:47.000]   They're going to have to solve that, aren't they?
[02:21:47.000 --> 02:21:48.000]   In the meta.
[02:21:48.000 --> 02:21:49.760]   I mean, you could, I guess.
[02:21:49.760 --> 02:21:54.120]   I was like, half of your metaverse is, you know, bunnies just top up.
[02:21:54.120 --> 02:21:55.600]   The other half is just legs down.
[02:21:55.600 --> 02:21:56.600]   Just legs.
[02:21:56.600 --> 02:21:57.600]   It'll be very terrifying.
[02:21:57.600 --> 02:22:02.800]   But then you could actually, you could have somebody, it's nothing but legs, basically.
[02:22:02.800 --> 02:22:04.160]   This actually begs the question.
[02:22:04.160 --> 02:22:06.680]   So there's already been pretty bad.
[02:22:06.680 --> 02:22:09.880]   I don't, I want to keep this safe because I'm not sure who's listening, but like, there's
[02:22:09.880 --> 02:22:12.880]   been some pretty horrific things that have already taken place.
[02:22:12.880 --> 02:22:15.360]   I mean, they have to be safe environments.
[02:22:15.360 --> 02:22:16.360]   They have to be safe environments.
[02:22:16.360 --> 02:22:17.360]   And the metaverse.
[02:22:17.360 --> 02:22:20.640]   Ask Philip about what happened to Second Life.
[02:22:20.640 --> 02:22:23.160]   Because Second Life, I was shocked.
[02:22:23.160 --> 02:22:28.520]   I have, I have a character in Second Life, my, my Second Life avatar.
[02:22:28.520 --> 02:22:30.800]   You know, when you join Second Life, this is God.
[02:22:30.800 --> 02:22:32.280]   This is 10, 15 years ago.
[02:22:32.280 --> 02:22:33.280]   Yeah.
[02:22:33.280 --> 02:22:34.280]   It's been a long ago.
[02:22:34.280 --> 02:22:37.560]   They would kind of spin up an automatic name and I, and I liked mine so much.
[02:22:37.560 --> 02:22:41.200]   I kept it, Pruneface Spatula.
[02:22:41.200 --> 02:22:42.200]   And it's very good.
[02:22:42.200 --> 02:22:43.200]   Isn't that good?
[02:22:43.200 --> 02:22:44.200]   I mean, you would keep it.
[02:22:44.200 --> 02:22:45.200]   That's a password, basically.
[02:22:45.200 --> 02:22:46.280]   It's awesome.
[02:22:46.280 --> 02:22:51.200]   So my character, so I end using a while and I still, I still have a login.
[02:22:51.200 --> 02:22:57.360]   So I went in and it's, it's, it's just, I don't know.
[02:22:57.360 --> 02:22:58.360]   You're right.
[02:22:58.360 --> 02:22:59.360]   It's not family friendly.
[02:22:59.360 --> 02:23:09.320]   People are wearing outfits set kind of, because you could always buy a body in Second Life.
[02:23:09.320 --> 02:23:12.080]   But they're not like, there are whole King Islands now.
[02:23:12.080 --> 02:23:13.080]   They're a whole King.
[02:23:13.080 --> 02:23:14.080]   Not now.
[02:23:14.080 --> 02:23:17.160]   Thank you for saying it because I couldn't.
[02:23:17.160 --> 02:23:18.160]   It's gotten, there's a lot of.
[02:23:18.160 --> 02:23:19.880]   Listen, we've got to talk about the King Island.
[02:23:19.880 --> 02:23:22.880]   Well, if you go to the marketplace, this is, this is not too.
[02:23:22.880 --> 02:23:27.960]   I was going to say we earlier had opened up Second Life briefly and it was exclusively
[02:23:27.960 --> 02:23:29.760]   a bit risky clothes.
[02:23:29.760 --> 02:23:30.760]   Yeah.
[02:23:30.760 --> 02:23:38.040]   Here's the, for only $1,299, Linden dollars, you can get the, so this, these are the kind
[02:23:38.040 --> 02:23:43.640]   of costumes people are wearing, but this, you know, obviously it's still going on.
[02:23:43.640 --> 02:23:49.640]   Why is, why is the metaverse and virtual reality and the future always dark and raining
[02:23:49.640 --> 02:23:52.440]   and with women have to wear like pleather?
[02:23:52.440 --> 02:23:54.240]   Like, how did that, how did that become?
[02:23:54.240 --> 02:23:57.120]   Oh, because you can't wear a pleather in the rain in real life.
[02:23:57.120 --> 02:23:58.120]   It just would be inconvenient.
[02:23:58.120 --> 02:23:59.880]   So, you know, that's fantasy.
[02:23:59.880 --> 02:24:01.520]   That's high fantasy right there.
[02:24:01.520 --> 02:24:03.440]   You know, why not athleisure?
[02:24:03.440 --> 02:24:05.160]   Yeah, athleisure.
[02:24:05.160 --> 02:24:06.160]   Right.
[02:24:06.160 --> 02:24:07.160]   Yeah.
[02:24:07.160 --> 02:24:08.160]   Let them be comfortable.
[02:24:08.160 --> 02:24:09.160]   Yes.
[02:24:09.160 --> 02:24:11.680]   Let our, let me be kind to our avatars, right?
[02:24:11.680 --> 02:24:12.680]   Avatar rights.
[02:24:12.680 --> 02:24:15.120]   I blame two things, neuromancer.
[02:24:15.120 --> 02:24:16.640]   Let's not constrict them.
[02:24:16.640 --> 02:24:17.880]   William Gibson's neuromancer.
[02:24:17.880 --> 02:24:22.720]   Remember her mirror shades that she couldn't even take off because they had, you know, cyber
[02:24:22.720 --> 02:24:28.120]   powers and then I blame the matrix because everybody in the matrix is apparently dressed
[02:24:28.120 --> 02:24:29.120]   in latex.
[02:24:29.120 --> 02:24:34.680]   Well, I deep fake myself into my, my cyber parents, Neo and Trinity.
[02:24:34.680 --> 02:24:37.920]   I put my face on Trinity, but she, that's different.
[02:24:37.920 --> 02:24:39.680]   That was not hyper sexualized.
[02:24:39.680 --> 02:24:40.680]   I do.
[02:24:40.680 --> 02:24:44.240]   I totally want them to be like my mom and dad, but that's not hyper sexualized.
[02:24:44.240 --> 02:24:47.360]   You put your mom and dad's faces on Neo and Trinity?
[02:24:47.360 --> 02:24:48.360]   Sorry.
[02:24:48.360 --> 02:24:53.320]   Now I put my own face on Trinity and I posted it somewhat accidentally to our work Slack
[02:24:53.320 --> 02:24:54.320]   channel.
[02:24:54.320 --> 02:24:57.840]   And everybody's like, wow, what are you doing today?
[02:24:57.840 --> 02:25:01.560]   I want Neo and Trinity to be your mom and dad, but you are Trinity.
[02:25:01.560 --> 02:25:02.560]   I do.
[02:25:02.560 --> 02:25:03.560]   I don't know.
[02:25:03.560 --> 02:25:06.360]   Second life, I've got second life probably has a kink island for that specific.
[02:25:06.360 --> 02:25:07.840]   This is, this is not a kink thing.
[02:25:07.840 --> 02:25:11.080]   I think this is me just trying to get in touch with my roots.
[02:25:11.080 --> 02:25:14.240]   Trinity was, Trinity was pretty sexy, but you're right.
[02:25:14.240 --> 02:25:15.240]   She was more.
[02:25:15.240 --> 02:25:16.240]   No.
[02:25:16.240 --> 02:25:17.240]   She had like a.
[02:25:17.240 --> 02:25:19.920]   She was totally not sexualized.
[02:25:19.920 --> 02:25:20.920]   I don't think so.
[02:25:20.920 --> 02:25:24.160]   No, she had a rubber trench coat, but okay.
[02:25:24.160 --> 02:25:25.160]   I mean, Neil in the.
[02:25:25.160 --> 02:25:27.920]   Yeah, they all had rubber trench coats.
[02:25:27.920 --> 02:25:29.400]   First of all, it wasn't rubber.
[02:25:29.400 --> 02:25:32.120]   Was it weather?
[02:25:32.120 --> 02:25:34.480]   Stop messing with my favorite movie.
[02:25:34.480 --> 02:25:36.520]   It doesn't look like my second favorite movie.
[02:25:36.520 --> 02:25:39.440]   What are your thoughts on the new Matrix movie though?
[02:25:39.440 --> 02:25:41.560]   I have deep thoughts on that.
[02:25:41.560 --> 02:25:43.560]   I, I, I kind of love it.
[02:25:43.560 --> 02:25:44.560]   Did you?
[02:25:44.560 --> 02:25:45.560]   I know it's a apology.
[02:25:45.560 --> 02:25:49.640]   I went in there expecting it to be corny and fond and that's exactly what it was.
[02:25:49.640 --> 02:25:51.360]   And it's exactly what you expected.
[02:25:51.360 --> 02:25:52.360]   Yeah.
[02:25:52.360 --> 02:25:53.360]   It was wonderful.
[02:25:53.360 --> 02:25:54.360]   Um, it was.
[02:25:54.360 --> 02:25:56.600]   I love the first one so much.
[02:25:56.600 --> 02:25:59.040]   No sequel ever lived up to the first one.
[02:25:59.040 --> 02:26:00.040]   That's really the truth.
[02:26:00.040 --> 02:26:02.360]   No, those, those second two were, were not great.
[02:26:02.360 --> 02:26:09.080]   The third one I know, if you know the backstory behind what the Waukauci sisters went through,
[02:26:09.080 --> 02:26:14.480]   which was pretty horrific and awful, then a lot of it makes sense.
[02:26:14.480 --> 02:26:17.960]   That being said, they could, there were so many stupid technical problems with that
[02:26:17.960 --> 02:26:22.720]   movie that they could have addressed and they were like, like janky, silly choices made
[02:26:22.720 --> 02:26:26.520]   to represent technology that they should have not used.
[02:26:26.520 --> 02:26:27.360]   So that was a problem.
[02:26:27.360 --> 02:26:28.760]   Did you like Sensei?
[02:26:28.760 --> 02:26:32.280]   They're, they're kind of short lived Netflix series.
[02:26:32.280 --> 02:26:33.800]   I did not see it.
[02:26:33.800 --> 02:26:34.800]   I didn't see it.
[02:26:34.800 --> 02:26:35.800]   I think you should watch it.
[02:26:35.800 --> 02:26:36.800]   All right.
[02:26:36.800 --> 02:26:40.600]   I think it's still on Netflix or is it Amazon or Netflix?
[02:26:40.600 --> 02:26:42.600]   I, uh, Netflix.
[02:26:42.600 --> 02:26:50.160]   Um, it was created by the Waukauci, Waukauci sisters and it's kind of beautiful in an interesting
[02:26:50.160 --> 02:26:51.160]   way.
[02:26:51.160 --> 02:26:52.160]   I don't know.
[02:26:52.160 --> 02:26:53.160]   I liked it anyway.
[02:26:53.160 --> 02:26:55.000]   Uh, let's see.
[02:26:55.000 --> 02:26:56.000]   How did we get there?
[02:26:56.000 --> 02:26:59.600]   Oh, I remember I was looking at the information and I got distracted, which is.
[02:26:59.600 --> 02:27:01.400]   Oh, we're talking about Playboy Mansion.
[02:27:01.400 --> 02:27:02.400]   Playboy Mansion.
[02:27:02.400 --> 02:27:06.200]   That's the, yes, let's like, it's not perfect there.
[02:27:06.200 --> 02:27:11.800]   Um, Playboy Mansion, King, Ireland, I think that's how we got to now.
[02:27:11.800 --> 02:27:12.800]   Yeah.
[02:27:12.800 --> 02:27:17.200]   And then we got to leather, you know, it all, it'll, and then, and then pleather.
[02:27:17.200 --> 02:27:19.160]   They keep pulling me in.
[02:27:19.160 --> 02:27:20.160]   Stop it.
[02:27:20.160 --> 02:27:21.160]   Sorry.
[02:27:21.160 --> 02:27:23.960]   We'll go back to your great NFT.
[02:27:23.960 --> 02:27:24.960]   Yeah.
[02:27:24.960 --> 02:27:25.960]   No, no, I'm done.
[02:27:25.960 --> 02:27:26.960]   I'm bored.
[02:27:26.960 --> 02:27:27.960]   I mean, I know it's all right.
[02:27:27.960 --> 02:27:28.960]   Never mind.
[02:27:28.960 --> 02:27:29.960]   Never mind.
[02:27:29.960 --> 02:27:38.760]   Do not make a video of your cat purring because it could get taken down.
[02:27:38.760 --> 02:27:39.760]   Why?
[02:27:39.760 --> 02:27:40.760]   Yes.
[02:27:40.760 --> 02:27:43.280]   And right at the IP, somebody copyrighted it.
[02:27:43.280 --> 02:27:48.680]   So of course, of course, this is a YouTube content ID nightmare again.
[02:27:48.680 --> 02:27:56.560]   Um, a user on YouTube last March named Digihaven uploaded a one hour video, uh, looping
[02:27:56.560 --> 02:28:01.880]   his cat phantom purring a year later.
[02:28:01.880 --> 02:28:03.560]   It never went, never went viral.
[02:28:03.560 --> 02:28:09.840]   It was not, you know, a big download or anything, but a year later, EMI music and PRS and other
[02:28:09.840 --> 02:28:12.960]   music publishing company took it down.
[02:28:12.960 --> 02:28:14.200]   Well, they didn't take it down.
[02:28:14.200 --> 02:28:18.920]   They said, uh, they demonetize it and put their ads on it because they claimed the rights
[02:28:18.920 --> 02:28:21.720]   to his kitty cats purring.
[02:28:21.720 --> 02:28:26.840]   I hate terrible world where you can't even make money from your cat's purse.
[02:28:26.840 --> 02:28:29.080]   How, so wait.
[02:28:29.080 --> 02:28:35.920]   So the copyright, like what Jack, sorry, what, what Jack Hole in the USPTO granted.
[02:28:35.920 --> 02:28:38.080]   I think Jack Hole's actually worse.
[02:28:38.080 --> 02:28:39.080]   Worse.
[02:28:39.080 --> 02:28:40.080]   Okay.
[02:28:40.080 --> 02:28:41.080]   What Jack Hole.
[02:28:41.080 --> 02:28:44.080]   What orange furbin in the USPTO?
[02:28:44.080 --> 02:28:45.080]   Yes.
[02:28:45.080 --> 02:28:46.080]   Yeah.
[02:28:46.080 --> 02:28:53.880]   What metamate at that, that's, I, I accept at USPTO granted a patent on like, I don't
[02:28:53.880 --> 02:28:58.200]   think I think it's a, it's just a flaw in the content ID system.
[02:28:58.200 --> 02:29:05.640]   It's got to be unless some rap artist sampled it, which is, I guess possible.
[02:29:05.640 --> 02:29:09.040]   Um, did Raza, Allah, whatever that will be.
[02:29:09.040 --> 02:29:10.040]   Yeah.
[02:29:10.040 --> 02:29:12.800]   Razzle, razzle, Kat, Kat, rap.
[02:29:12.800 --> 02:29:17.000]   This show is degenerating.
[02:29:17.000 --> 02:29:22.320]   I'm trying to think about what other sounds we could copyright to cause the most havoc.
[02:29:22.320 --> 02:29:23.320]   Oh, that's a good idea.
[02:29:23.320 --> 02:29:27.720]   Maybe somebody could trademark hit that bell or like and subscribe, but you'd devastate
[02:29:27.720 --> 02:29:29.400]   the whole YouTube ecosystem.
[02:29:29.400 --> 02:29:30.640]   If you, so, uh, John is.
[02:29:30.640 --> 02:29:31.800]   No, no, no, I got it.
[02:29:31.800 --> 02:29:32.800]   I got it.
[02:29:32.800 --> 02:29:33.800]   It's Hey guys.
[02:29:33.800 --> 02:29:34.800]   Hey guys.
[02:29:34.800 --> 02:29:35.800]   Hey guys.
[02:29:35.800 --> 02:29:36.800]   trademark.
[02:29:36.800 --> 02:29:37.800]   Hey guys.
[02:29:37.800 --> 02:29:38.800]   Oh my God.
[02:29:38.800 --> 02:29:39.800]   Oh my God.
[02:29:39.800 --> 02:29:40.800]   Oh, I claim it.
[02:29:40.800 --> 02:29:41.800]   Quick.
[02:29:41.800 --> 02:29:42.800]   Would you work on that?
[02:29:42.800 --> 02:29:44.400]   We would own it, man.
[02:29:44.400 --> 02:29:45.400]   We'd own YouTube.
[02:29:45.400 --> 02:29:46.960]   We could take them all down, man.
[02:29:46.960 --> 02:29:53.000]   Amy gets Hey guys, uh, I'll get like and subscribe and Leo, you can get hit that bell.
[02:29:53.000 --> 02:29:54.800]   Hit that smash that bell.
[02:29:54.800 --> 02:29:55.800]   Okay.
[02:29:55.800 --> 02:29:58.200]   We're getting the big bucks guys.
[02:29:58.200 --> 02:29:59.200]   This is it.
[02:29:59.200 --> 02:30:00.200]   It's all over.
[02:30:00.200 --> 02:30:01.200]   It's all retire.
[02:30:01.200 --> 02:30:03.720]   I'm going to end up realizing now we can just.
[02:30:03.720 --> 02:30:05.880]   I don't need that spotify money.
[02:30:05.880 --> 02:30:06.880]   I got YouTube.
[02:30:06.880 --> 02:30:11.160]   Paris, Marna, you're fantastic.
[02:30:11.160 --> 02:30:12.760]   I love your Santa.
[02:30:12.760 --> 02:30:15.120]   Your sequined mannequin or Santa Quinn.
[02:30:15.120 --> 02:30:17.240]   I will call Santa Quinn.
[02:30:17.240 --> 02:30:20.440]   I mean, we could, you know, I have a new word.
[02:30:20.440 --> 02:30:28.840]   I love your, I love your Marzano tomato masks and your disembodied body in the fireplace.
[02:30:28.840 --> 02:30:29.840]   I don't know.
[02:30:29.840 --> 02:30:32.200]   Uh, just it's all very important.
[02:30:32.200 --> 02:30:33.200]   It's light.
[02:30:33.200 --> 02:30:35.200]   It's the world of Brooklyn living.
[02:30:35.200 --> 02:30:36.200]   That's what we say.
[02:30:36.200 --> 02:30:39.000]   It's truly, I mean, Brooklyn, there's just bodies everywhere.
[02:30:39.000 --> 02:30:40.000]   No heads.
[02:30:40.000 --> 02:30:42.240]   Really terrifying in the river.
[02:30:42.240 --> 02:30:43.800]   They're, they're, they're happy.
[02:30:43.800 --> 02:30:45.760]   Paris, you're fantastic.
[02:30:45.760 --> 02:30:48.320]   Read her stuff on the information.
[02:30:48.320 --> 02:30:51.520]   Is there anything else you want to plug anything at all?
[02:30:51.520 --> 02:30:54.560]   I think last time I was here, I plugged owning a printer.
[02:30:54.560 --> 02:30:56.880]   So you know, we'll read up that.
[02:30:56.880 --> 02:30:58.120]   Uh, we'll also plug.
[02:30:58.120 --> 02:31:03.280]   If you're out there, you work for Amazon or used to get my DMS.
[02:31:03.280 --> 02:31:04.600]   Hit me up on signal.
[02:31:04.600 --> 02:31:05.600]   That's chat.
[02:31:05.600 --> 02:31:06.600]   What's your signal idea?
[02:31:06.600 --> 02:31:08.760]   You want to give it out or it's probably on the.
[02:31:08.760 --> 02:31:09.760]   Signal.
[02:31:09.760 --> 02:31:10.760]   Go ahead.
[02:31:10.760 --> 02:31:11.760]   Do it.
[02:31:11.760 --> 02:31:12.760]   Yeah.
[02:31:12.760 --> 02:31:13.760]   Hold on a second.
[02:31:13.760 --> 02:31:14.760]   Let me put up.
[02:31:14.760 --> 02:31:20.800]   Uh, my signal is two, six, seven, seven, nine, seven, eight, six, five, five.
[02:31:20.800 --> 02:31:22.200]   And that's on WhatsApp too.
[02:31:22.200 --> 02:31:25.120]   The tip line is open.
[02:31:25.120 --> 02:31:26.880]   Let's get her some good tips.
[02:31:26.880 --> 02:31:27.880]   Get her a story.
[02:31:27.880 --> 02:31:28.880]   Yeah.
[02:31:28.880 --> 02:31:29.880]   Or we can just, you know, we can just chat.
[02:31:29.880 --> 02:31:31.920]   We can just chat about your experience.
[02:31:31.920 --> 02:31:33.320]   Have you been, I'm curious.
[02:31:33.320 --> 02:31:36.600]   Have you been watching the Finding Anna?
[02:31:36.600 --> 02:31:38.640]   What is it, the name of it?
[02:31:38.640 --> 02:31:39.640]   Inventing Anna.
[02:31:39.640 --> 02:31:42.920]   I tried and frankly was not a fan.
[02:31:42.920 --> 02:31:46.600]   I mean, I thought it was just a bit over heavy handed.
[02:31:46.600 --> 02:31:51.000]   I think they made some weird choices because they weren't able to get the rights to her
[02:31:51.000 --> 02:31:53.200]   life story or like the story of the.
[02:31:53.200 --> 02:31:54.200]   Oh, they did.
[02:31:54.200 --> 02:31:57.840]   Netflix page $320,000 for the rights.
[02:31:57.840 --> 02:32:03.280]   They got the rights to like, they got the rights to the magazine story.
[02:32:03.280 --> 02:32:04.880]   And then paid her for her inclusion.
[02:32:04.880 --> 02:32:10.280]   But that's the reason why they had to focus on the making of the New York magazine article
[02:32:10.280 --> 02:32:14.320]   instead of specifically what happened with the crime, which I think just kind of throws
[02:32:14.320 --> 02:32:15.320]   it out of whack.
[02:32:15.320 --> 02:32:16.320]   No, you're right.
[02:32:16.320 --> 02:32:17.320]   I have that.
[02:32:17.320 --> 02:32:18.320]   That's a good point.
[02:32:18.320 --> 02:32:19.320]   Now you're ruining it for me, but that's a good.
[02:32:19.320 --> 02:32:20.320]   I know.
[02:32:20.320 --> 02:32:21.320]   Don't, don't listen to me.
[02:32:21.320 --> 02:32:22.320]   Enjoy your content, everybody.
[02:32:22.320 --> 02:32:23.320]   I just love that actress.
[02:32:23.320 --> 02:32:27.320]   And I just think it's a great story and made me think of you because it's all about a feature
[02:32:27.320 --> 02:32:32.960]   magazine writer who's been forced, unfortunately, to cover a meaningless Me Too story.
[02:32:32.960 --> 02:32:38.000]   And, you know, two years late and instead really wants to cover the story of the scam
[02:32:38.000 --> 02:32:42.640]   artist who took high society in New York for a ride.
[02:32:42.640 --> 02:32:44.880]   And it's a fascinating story, but you're right.
[02:32:44.880 --> 02:32:47.600]   It's not quite the story that should have been told.
[02:32:47.600 --> 02:32:48.600]   Is it?
[02:32:48.600 --> 02:32:50.360]   Do you love all stories focused on journalists?
[02:32:50.360 --> 02:32:51.360]   Yes.
[02:32:51.360 --> 02:32:52.360]   That's fun.
[02:32:52.360 --> 02:32:55.840]   And they made a fake New York magazine called Manhattan Manhattan with the same type face
[02:32:55.840 --> 02:32:58.160]   and everything.
[02:32:58.160 --> 02:33:00.640]   Same red logo when you go into the building.
[02:33:00.640 --> 02:33:01.640]   Yeah.
[02:33:01.640 --> 02:33:02.640]   Cool.
[02:33:02.640 --> 02:33:03.640]   All right.
[02:33:03.640 --> 02:33:04.640]   Anyway, great to have you.
[02:33:04.640 --> 02:33:05.800]   It's great to see you.
[02:33:05.800 --> 02:33:06.800]   Thank you, Paris.
[02:33:06.800 --> 02:33:07.800]   Great to be here as well.
[02:33:07.800 --> 02:33:08.800]   Thank you.
[02:33:08.800 --> 02:33:09.800]   Amy Webb's new book.
[02:33:09.800 --> 02:33:10.800]   Gotta read it.
[02:33:10.800 --> 02:33:11.800]   Must buy.
[02:33:11.800 --> 02:33:12.800]   Get it on Audible.
[02:33:12.800 --> 02:33:17.360]   Get it, you know, in a bookstore, go to your favorite local independent bookstore and
[02:33:17.360 --> 02:33:20.480]   buy a copy of the Genesis machine.
[02:33:20.480 --> 02:33:22.560]   You will not put it down.
[02:33:22.560 --> 02:33:25.920]   It is really good and you are fantastic.
[02:33:25.920 --> 02:33:29.000]   This is her fourth hour with me this weekend.
[02:33:29.000 --> 02:33:32.320]   And I'm sure she's sick to death.
[02:33:32.320 --> 02:33:33.320]   Can I take?
[02:33:33.320 --> 02:33:34.800]   Can I not stick to that?
[02:33:34.800 --> 02:33:36.600]   Can I just say one thing really, really quickly?
[02:33:36.600 --> 02:33:37.920]   Of course, of course.
[02:33:37.920 --> 02:33:41.280]   So we talked a little bit about cyber war.
[02:33:41.280 --> 02:33:44.760]   I don't know what actually is happening, but I know that the people who work in the
[02:33:44.760 --> 02:33:49.440]   industry get very little thanks until something bad happens and then they get criticism.
[02:33:49.440 --> 02:33:54.320]   So I just wanted to shout out say thanks to everybody working in InfoSec and all the
[02:33:54.320 --> 02:33:57.160]   people who are in IT who are keeping things running.
[02:33:57.160 --> 02:33:58.160]   I agree.
[02:33:58.160 --> 02:33:59.160]   Thanks.
[02:33:59.160 --> 02:34:00.160]   So just wanted to say thanks.
[02:34:00.160 --> 02:34:01.600]   You know what a lot of them listen to this show.
[02:34:01.600 --> 02:34:06.240]   I talk to them a lot and I know how hard it is.
[02:34:06.240 --> 02:34:09.320]   It's really, really tough, but I think they do a very good job.
[02:34:09.320 --> 02:34:15.960]   The fact that we are still in a functioning country is just testament to that.
[02:34:15.960 --> 02:34:16.960]   Yeah.
[02:34:16.960 --> 02:34:21.040]   So I don't know what's ahead for you guys over the next, you know, however long, but
[02:34:21.040 --> 02:34:25.240]   I just want to say like I know that nobody tells you thank you and then the only time
[02:34:25.240 --> 02:34:29.600]   anybody really pays attention is when something goes wrong and that's right on.
[02:34:29.600 --> 02:34:30.600]   Right on.
[02:34:30.600 --> 02:34:33.520]   What is your best guess about what will happen in Ukraine?
[02:34:33.520 --> 02:34:36.400]   Is Putin going to back down?
[02:34:36.400 --> 02:34:37.400]   I don't know.
[02:34:37.400 --> 02:34:42.160]   This is, I did a little bit of work in Ukraine and it was served on a presidential commission
[02:34:42.160 --> 02:34:46.960]   involving the US and Russia at one point.
[02:34:46.960 --> 02:34:53.140]   There's a part of me that wonders if what we're seeing is, is I wonder if what's really
[02:34:53.140 --> 02:34:54.560]   happening is digital.
[02:34:54.560 --> 02:34:55.560]   It's not physical.
[02:34:55.560 --> 02:34:56.560]   Yeah.
[02:34:56.560 --> 02:34:57.800]   I think it's very complicated.
[02:34:57.800 --> 02:35:02.220]   I think a lot of people, we didn't spend a lot of time, but we did bring it up.
[02:35:02.220 --> 02:35:06.080]   I think a lot of people think that this could easily spill into a cyber war that would not
[02:35:06.080 --> 02:35:08.600]   be pleasant for anybody.
[02:35:08.600 --> 02:35:11.800]   So we'll be watching with.
[02:35:11.800 --> 02:35:15.480]   Thank you in advance to everybody works in IT.
[02:35:15.480 --> 02:35:17.800]   We appreciate you.
[02:35:17.800 --> 02:35:22.560]   It always seems to happen on a holiday, you know, you know, Christmas Eve and suddenly
[02:35:22.560 --> 02:35:26.520]   you've got to go in and fix all your email servers.
[02:35:26.520 --> 02:35:27.720]   Thank you, Amy.
[02:35:27.720 --> 02:35:34.120]   Of course, future today Institute is your business and keep an eye out for this year's
[02:35:34.120 --> 02:35:37.760]   new report, which is out in next month.
[02:35:37.760 --> 02:35:40.160]   I'm sure we'll have you on to talk about it.
[02:35:40.160 --> 02:35:41.160]   That'd be awesome.
[02:35:41.160 --> 02:35:42.320]   Yeah, we got a lot of good stuff.
[02:35:42.320 --> 02:35:45.200]   Always available for free on the website, which is great.
[02:35:45.200 --> 02:35:51.560]   It's a great website to play with and have fun at ftifuturedodayinstitute.
[02:35:51.560 --> 02:35:55.160]   Yeah, I broke the, I was screwing around.
[02:35:55.160 --> 02:35:56.160]   Is it broken?
[02:35:56.160 --> 02:35:57.160]   It's broken.
[02:35:57.160 --> 02:35:58.160]   I broke the internet.
[02:35:58.160 --> 02:36:02.240]   No, she used to have this really great thing.
[02:36:02.240 --> 02:36:03.240]   It's just going to be.
[02:36:03.240 --> 02:36:06.640]   Yeah, we've got a little script that feeds all the signals we're tracking into sort of
[02:36:06.640 --> 02:36:12.600]   this, this like network map that I broke the code on because I'm whatever.
[02:36:12.600 --> 02:36:14.680]   I'm not, I should not be touching it, but I did.
[02:36:14.680 --> 02:36:18.360]   You actually were in there messing with the code and you broke it?
[02:36:18.360 --> 02:36:19.360]   Yes.
[02:36:19.360 --> 02:36:20.360]   I was trying to upgrade.
[02:36:20.360 --> 02:36:21.360]   Care quotes.
[02:36:21.360 --> 02:36:24.120]   Can you, can you, do you have it on GitHub?
[02:36:24.120 --> 02:36:25.120]   Can you roll it back?
[02:36:25.120 --> 02:36:30.200]   No, it's a custom built little plugin thing that we made and I just, I don't have time,
[02:36:30.200 --> 02:36:31.200]   like whatever.
[02:36:31.200 --> 02:36:33.360]   I just need to farm it out and have somebody else fix it.
[02:36:33.360 --> 02:36:34.360]   It's my fault.
[02:36:34.360 --> 02:36:35.800]   I've kept, kept you too long.
[02:36:35.800 --> 02:36:40.800]   So futuretodayinstitute.com.
[02:36:40.800 --> 02:36:44.640]   Very, very good work and very, very nice to have you both.
[02:36:44.640 --> 02:36:46.000]   I don't want this show to end.
[02:36:46.000 --> 02:36:47.000]   Can you tell?
[02:36:47.000 --> 02:36:50.240]   But it's going to have to because we're at a time and that's.
[02:36:50.240 --> 02:36:55.280]   That's not, I'll say it's very nice to finally have the show on a Sunday night before a holiday
[02:36:55.280 --> 02:37:00.320]   because normally we end this and I'm feeling all ant and like crap, I got to go to bed.
[02:37:00.320 --> 02:37:01.480]   You know, I got to get prepared for the week.
[02:37:01.480 --> 02:37:02.760]   I got a whole day tomorrow.
[02:37:02.760 --> 02:37:03.760]   Hit the clubs.
[02:37:03.760 --> 02:37:05.000]   Hit the clubs.
[02:37:05.000 --> 02:37:06.520]   The masks are off.
[02:37:06.520 --> 02:37:07.520]   The, it's all over.
[02:37:07.520 --> 02:37:08.520]   I guess.
[02:37:08.520 --> 02:37:09.520]   Yeah.
[02:37:09.520 --> 02:37:10.520]   Get on in there.
[02:37:10.520 --> 02:37:11.520]   What?
[02:37:11.520 --> 02:37:12.520]   Yeah.
[02:37:12.520 --> 02:37:13.520]   Yeah.
[02:37:13.520 --> 02:37:14.520]   It's like free, Rome freely.
[02:37:14.520 --> 02:37:15.520]   No mask.
[02:37:15.520 --> 02:37:16.520]   No, COVID card.
[02:37:16.520 --> 02:37:17.520]   The grocery store.
[02:37:17.520 --> 02:37:19.960]   I mean, no, we're still getting COVID carded everywhere.
[02:37:19.960 --> 02:37:21.240]   As we probably should for a while.
[02:37:21.240 --> 02:37:22.480]   As we probably should.
[02:37:22.480 --> 02:37:24.280]   But oh man.
[02:37:24.280 --> 02:37:27.240]   Do you think that it would be really nice if there were just a moment where it's like
[02:37:27.240 --> 02:37:28.240]   over?
[02:37:28.240 --> 02:37:29.240]   Oh, boom.
[02:37:29.240 --> 02:37:30.720]   Like, I mean, that would be fantastic.
[02:37:30.720 --> 02:37:34.480]   That would require more coordination than I think anyone is possible.
[02:37:34.480 --> 02:37:35.720]   Anyone is capable of.
[02:37:35.720 --> 02:37:36.880]   Yeah.
[02:37:36.880 --> 02:37:41.480]   We do Twitter on every Sunday afternoon, 230 Pacific, 530 Eastern.
[02:37:41.480 --> 02:37:43.360]   That's 2230 UTC.
[02:37:43.360 --> 02:37:45.720]   You can watch us do it live at live.twit.tv.
[02:37:45.720 --> 02:37:51.160]   We stream live all day and all night prerecorded shows, you know, post recorded shows if we
[02:37:51.160 --> 02:37:53.080]   don't have a live show, but it's always open.
[02:37:53.080 --> 02:37:54.040]   So is the chatroom.
[02:37:54.040 --> 02:37:57.000]   IRC.twit.tv.
[02:37:57.000 --> 02:38:00.360]   If you are a member of club twit, you also have exclusive access.
[02:38:00.360 --> 02:38:06.040]   You can go past the velvet rope into our club twit discord server.
[02:38:06.040 --> 02:38:08.080]   Club twit is a great way to support what we do.
[02:38:08.080 --> 02:38:10.760]   We really appreciate our thousands of members.
[02:38:10.760 --> 02:38:11.760]   Thank you.
[02:38:11.760 --> 02:38:12.760]   Thank you.
[02:38:12.760 --> 02:38:13.760]   Thank you.
[02:38:13.760 --> 02:38:17.080]   All of those a month gets you ad free versions of all of our shows.
[02:38:17.080 --> 02:38:22.840]   You get a special twit plus feed with lots of interesting additional shows and of course,
[02:38:22.840 --> 02:38:24.920]   access to the discord.
[02:38:24.920 --> 02:38:29.820]   The feed now has the untitled Linux show that gives fizz with Dick D Bartolo, Stacey
[02:38:29.820 --> 02:38:37.200]   Higginbotham's book club this week in space with Rod Pyle and Derek Malick from space.com.
[02:38:37.200 --> 02:38:39.880]   There's a lot of good content on there.
[02:38:39.880 --> 02:38:41.160]   What did we just do?
[02:38:41.160 --> 02:38:43.720]   Did we just do Owen JJ Stone?
[02:38:43.720 --> 02:38:45.040]   Is that coming up this week?
[02:38:45.040 --> 02:38:48.440]   Okay, Owen's going to be a guest on our fireside chats.
[02:38:48.440 --> 02:38:50.280]   There's a lot going on in the club.
[02:38:50.280 --> 02:38:54.440]   All you have to do is go to twit.tv/club twit to sign up.
[02:38:54.440 --> 02:38:56.360]   No contract month to month.
[02:38:56.360 --> 02:38:57.360]   Give it a try.
[02:38:57.360 --> 02:39:00.880]   See if you like it and it sure helps us do a lot of things like yesterday's triangulation,
[02:39:00.880 --> 02:39:03.160]   which was ad free thanks to the club.
[02:39:03.160 --> 02:39:05.040]   So we appreciate it.
[02:39:05.040 --> 02:39:10.160]   After the fact, all of our shows are still available to everybody all in sundry ad supported
[02:39:10.160 --> 02:39:12.120]   at twit.tv.
[02:39:12.120 --> 02:39:13.120]   That's the website.
[02:39:13.120 --> 02:39:14.440]   You can also go to youtube.
[02:39:14.440 --> 02:39:15.800]   Every show has its own youtube channel.
[02:39:15.800 --> 02:39:21.120]   The master channel is youtube.com/twit and there's links there to all the other channels.
[02:39:21.120 --> 02:39:30.280]   And of course, because we are a podcast proudly using RSS since 2004, you can subscribe
[02:39:30.280 --> 02:39:31.440]   in your favorite podcast player.
[02:39:31.440 --> 02:39:32.840]   You like pocket cast so do I.
[02:39:32.840 --> 02:39:36.080]   That's a good one overcast Google podcast Apple podcast.
[02:39:36.080 --> 02:39:38.160]   We're everywhere even Spotify.
[02:39:38.160 --> 02:39:42.280]   Just subscribe and that way you'll get it automatically the minute it's available.
[02:39:42.280 --> 02:39:44.320]   Thank you everybody for being here.
[02:39:44.320 --> 02:39:45.320]   Have a great week.
[02:39:45.320 --> 02:39:48.560]   Another twit is in the San Marzano can.
[02:39:48.560 --> 02:39:48.920]   Bye bye.
[02:39:48.920 --> 02:40:00.120]   for a quick video.

