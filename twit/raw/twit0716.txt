;FFMETADATA1
title=A Very Attractive Container of Nope
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=716
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.640]   Hey, a big twit coming up.
[00:00:02.640 --> 00:00:07.840]   We've got a great panel, Paris Martenot, Micah Sargent, and the return of the digital
[00:00:07.840 --> 00:00:10.160]   genser with Father Robert Ballis here.
[00:00:10.160 --> 00:00:17.320]   We're going to talk about Google's Thanos Easter Egg, the Galaxy Fold, Postmortem Amazon,
[00:00:17.320 --> 00:00:18.320]   doubling its profits.
[00:00:18.320 --> 00:00:25.160]   Microsoft, a trillion dollar company, and the end of a $16,000 laundry folding robot.
[00:00:25.160 --> 00:00:29.360]   It's all next on Twit.
[00:00:29.360 --> 00:00:31.360]   That casts you love.
[00:00:31.360 --> 00:00:36.360]   From people you trust.
[00:00:36.360 --> 00:00:44.360]   This is Twit.
[00:00:44.360 --> 00:00:52.160]   This is Twit, this week at Tech, Episode 716, recorded Sunday, April 28, 2019.
[00:00:52.160 --> 00:00:56.160]   A very attractive container of note.
[00:00:56.160 --> 00:00:59.240]   This week at Tech is brought to you by CashFly.
[00:00:59.240 --> 00:01:02.440]   Give your users the seamless online experience they want.
[00:01:02.440 --> 00:01:05.520]   Power your site or app with CashFly's CDN.
[00:01:05.520 --> 00:01:08.560]   And be 30% faster than the competition.
[00:01:08.560 --> 00:01:12.280]   Learn more at twit.cashfly.com.
[00:01:12.280 --> 00:01:17.680]   And by Captera, find the right tools to make an informed software decision for your business.
[00:01:17.680 --> 00:01:22.400]   Visit capeterra's free website at captera.com/twit.
[00:01:22.400 --> 00:01:23.400]   And by Quip.
[00:01:23.400 --> 00:01:28.840]   Get a fresh start every day with Quip, the first subscription electric toothbrush accepted
[00:01:28.840 --> 00:01:31.200]   by the American Dental Association.
[00:01:31.200 --> 00:01:36.600]   Visit getquip.com/twit to get your first refill pack free when you purchase any Quip
[00:01:36.600 --> 00:01:38.440]   electric toothbrush.
[00:01:38.440 --> 00:01:39.600]   And by Casper.
[00:01:39.600 --> 00:01:44.240]   A sleep brand that continues to revolutionize its line of products to create an exceptionally
[00:01:44.240 --> 00:01:47.320]   comfortable sleep experience one night at a time.
[00:01:47.320 --> 00:01:53.120]   Get $50 towards select mattresses by visiting casper.com/twit and using the promo code Twit
[00:01:53.120 --> 00:01:57.760]   One at checkout.
[00:01:57.760 --> 00:01:58.800]   It's time for Twit.
[00:01:58.800 --> 00:02:01.240]   This week in Tech, the show we cover the weeks.
[00:02:01.240 --> 00:02:04.480]   Tech news with wonderful people from our community.
[00:02:04.480 --> 00:02:08.640]   The tech experts like Father Robert Ballisare.
[00:02:08.640 --> 00:02:10.880]   He's back from his stint at the Vatican.
[00:02:10.880 --> 00:02:11.880]   No, temporarily.
[00:02:11.880 --> 00:02:12.880]   Temporarily.
[00:02:12.880 --> 00:02:13.880]   Digital Jesuit.
[00:02:13.880 --> 00:02:15.040]   He's at Jesuit.org.
[00:02:15.040 --> 00:02:18.280]   Padre.sj on the Twitter.
[00:02:18.280 --> 00:02:19.360]   And what brings you back here?
[00:02:19.360 --> 00:02:20.360]   You're getting a little medical treatment.
[00:02:20.360 --> 00:02:25.400]   Yeah, a little spot of surgery and it's not that I couldn't have done it in Italy.
[00:02:25.400 --> 00:02:30.040]   It's just that because I just registered on their healthcare system, it would take 18
[00:02:30.040 --> 00:02:31.040]   months for them to do it.
[00:02:31.040 --> 00:02:32.520]   Yeah, because you just moved there.
[00:02:32.520 --> 00:02:33.520]   Right.
[00:02:33.520 --> 00:02:34.520]   Right.
[00:02:34.520 --> 00:02:35.960]   So they said, just go home, take care of it and come back.
[00:02:35.960 --> 00:02:38.120]   And it's minor surgery, everything's minor surgery.
[00:02:38.120 --> 00:02:40.640]   I do have a suture in my eye, which is very annoying.
[00:02:40.640 --> 00:02:42.200]   It's eye surgery.
[00:02:42.200 --> 00:02:43.200]   Yeah.
[00:02:43.200 --> 00:02:46.600]   But you know, it's like everyone gets a suture at some point.
[00:02:46.600 --> 00:02:50.320]   Also the coffee chihuahua is in her house.
[00:02:50.320 --> 00:02:51.320]   It's me, the coffee chihuahua.
[00:02:51.320 --> 00:02:53.320]   I'm like a sergeant.
[00:02:53.320 --> 00:02:57.560]   Love to have him on chihuahua.coffee is literally his website.
[00:02:57.560 --> 00:03:00.920]   It would be coffee.chihuahua, but nobody registers that domain yet.
[00:03:00.920 --> 00:03:02.360]   No one has chihuahua for some reason.
[00:03:02.360 --> 00:03:03.360]   I know.
[00:03:03.360 --> 00:03:04.960]   I'll figure out how to have that top level domain.
[00:03:04.960 --> 00:03:06.480]   I think I'll figure that out.
[00:03:06.480 --> 00:03:07.480]   Chihuahua.coffee.
[00:03:07.480 --> 00:03:11.160]   I think it's great that you can get a coffee.coffee is pretty cool.
[00:03:11.160 --> 00:03:13.000]   I do like that coffee.
[00:03:13.000 --> 00:03:15.680]   I've heard it a lot faster because of the caffeine.
[00:03:15.680 --> 00:03:17.400]   You can get to the website a lot faster.
[00:03:17.400 --> 00:03:19.320]   Oh, interesting.
[00:03:19.320 --> 00:03:23.840]   And from a little place you might know called wired.com, Paris Martenau.
[00:03:23.840 --> 00:03:24.840]   She's staff right there.
[00:03:24.840 --> 00:03:25.840]   Does great work.
[00:03:25.840 --> 00:03:26.840]   Hi, Paris.
[00:03:26.840 --> 00:03:28.360]   Hey, thanks so much for having me, guys.
[00:03:28.360 --> 00:03:30.800]   You have a great view out your bedroom window.
[00:03:30.800 --> 00:03:31.800]   That is spectacular.
[00:03:31.800 --> 00:03:33.720]   Yeah, I'm here.
[00:03:33.720 --> 00:03:36.240]   I'm walked up over to the office today.
[00:03:36.240 --> 00:03:37.400]   Oh, you're at the office.
[00:03:37.400 --> 00:03:38.400]   Okay.
[00:03:38.400 --> 00:03:42.160]   Yeah, I have very slow cell phones or internet speeds in my apartment.
[00:03:42.160 --> 00:03:44.320]   So I think it should be a little easier to see you guys.
[00:03:44.320 --> 00:03:45.320]   It's very kind of you.
[00:03:45.320 --> 00:03:46.320]   I appreciate it.
[00:03:46.320 --> 00:03:48.320]   It's actually also better coffee here.
[00:03:48.320 --> 00:03:50.400]   That's important.
[00:03:50.400 --> 00:03:51.400]   Not coffee.
[00:03:51.400 --> 00:03:52.400]   So I got to try this.
[00:03:52.400 --> 00:03:53.400]   Not yet.
[00:03:53.400 --> 00:03:54.400]   I'm at google.com.
[00:03:54.400 --> 00:03:57.440]   And just no spoilers here, but I just going to type.
[00:03:57.440 --> 00:03:59.040]   You got to watch this carefully.
[00:03:59.040 --> 00:04:01.240]   Thanos into the search.
[00:04:01.240 --> 00:04:02.240]   Whoa, wait.
[00:04:02.240 --> 00:04:03.240]   What are they doing?
[00:04:03.240 --> 00:04:04.240]   Do they change it?
[00:04:04.240 --> 00:04:05.240]   Click on the glove.
[00:04:05.240 --> 00:04:07.200]   Click on the glove right here.
[00:04:07.200 --> 00:04:09.520]   Oh, it snapped.
[00:04:09.520 --> 00:04:10.520]   That's not good.
[00:04:10.520 --> 00:04:12.160]   And it deleted half of all the web pages.
[00:04:12.160 --> 00:04:14.320]   I believe that's not good.
[00:04:14.320 --> 00:04:18.160]   No, no, it's not good or bad.
[00:04:18.160 --> 00:04:19.160]   Oh, okay.
[00:04:19.160 --> 00:04:21.160]   I'm going to go.
[00:04:21.160 --> 00:04:22.960]   Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh.
[00:04:22.960 --> 00:04:23.960]   How about that?
[00:04:23.960 --> 00:04:24.960]   They're deleting the web pages.
[00:04:24.960 --> 00:04:25.960]   I've seen them.
[00:04:25.960 --> 00:04:26.960]   I've seen them.
[00:04:26.960 --> 00:04:27.960]   Oh, my God.
[00:04:27.960 --> 00:04:28.960]   That's cute.
[00:04:28.960 --> 00:04:29.960]   Alright, I like that.
[00:04:29.960 --> 00:04:32.640]   Oh, my God.
[00:04:32.640 --> 00:04:34.840]   This is, I know, see, he says click again, they'll come back.
[00:04:34.840 --> 00:04:36.260]   I don't want them to come back.
[00:04:36.260 --> 00:04:37.840]   I think the world is back.
[00:04:37.840 --> 00:04:38.840]   Is in balance now.
[00:04:38.840 --> 00:04:42.840]   There's only half as many Marvel based websites as there used to be.
[00:04:42.840 --> 00:04:43.920]   I have not seen it.
[00:04:43.920 --> 00:04:45.080]   There will be no spoilers here.
[00:04:45.080 --> 00:04:46.880]   Has anybody seen the end game?
[00:04:46.880 --> 00:04:48.120]   Ah, yeah.
[00:04:48.120 --> 00:04:49.820]   I have a lot of
[00:04:49.820 --> 00:04:50.800]   varsity and I have Father Robert.
[00:04:50.800 --> 00:04:52.920]   Paris doesn't even know what we're talking about.
[00:04:52.920 --> 00:04:55.360]   And Micah says, I don't really know either.
[00:04:55.360 --> 00:04:56.360]   No.
[00:04:56.360 --> 00:04:57.360]   Yeah.
[00:04:57.360 --> 00:04:58.360]   1.2 billion dollars already.
[00:04:58.360 --> 00:05:00.680]   It's only been open three days.
[00:05:00.680 --> 00:05:03.040]   I will say this and this is not a spoiler.
[00:05:03.040 --> 00:05:09.200]   But for all the comic book fans who loved the original series in graphic novel format,
[00:05:09.200 --> 00:05:11.600]   they went in a completely different direction, which is awesome.
[00:05:11.600 --> 00:05:13.080]   So you don't know what's going to happen.
[00:05:13.080 --> 00:05:16.720]   You can't say, well, I read the book and it said no, no, but it is whatsoever.
[00:05:16.720 --> 00:05:18.800]   So I've learned three hours long.
[00:05:18.800 --> 00:05:19.800]   Yes.
[00:05:19.800 --> 00:05:25.440]   So I recommend there are a number of articles online about when is best time to go pee.
[00:05:25.440 --> 00:05:26.440]   One word.
[00:05:26.440 --> 00:05:27.440]   Yeah.
[00:05:27.440 --> 00:05:28.440]   Gatherter.
[00:05:28.440 --> 00:05:33.400]   That is a word.
[00:05:33.400 --> 00:05:37.480]   Or as was it Dicty Wartolo told me, I suggest you go before the movie starts.
[00:05:37.480 --> 00:05:38.480]   Yeah.
[00:05:38.480 --> 00:05:39.480]   We were fine with that.
[00:05:39.480 --> 00:05:41.400]   You can't hold it for three hours, Leo.
[00:05:41.400 --> 00:05:45.320]   I went to the theater here in Petaluma, which with Brian Burnett and his wife.
[00:05:45.320 --> 00:05:46.320]   I love those seats.
[00:05:46.320 --> 00:05:48.080]   Yeah, we have an actual seats.
[00:05:48.080 --> 00:05:49.080]   Yeah.
[00:05:49.080 --> 00:05:50.080]   And you can.
[00:05:50.080 --> 00:05:51.080]   You can have a toilet build on.
[00:05:51.080 --> 00:05:52.080]   Yeah.
[00:05:52.080 --> 00:05:53.080]   They show.
[00:05:53.080 --> 00:05:54.080]   Almost.
[00:05:54.080 --> 00:05:55.080]   It's like a lazy boy recliner.
[00:05:55.080 --> 00:05:56.080]   That's the future.
[00:05:56.080 --> 00:05:57.080]   There's a button.
[00:05:57.080 --> 00:05:58.080]   The things go out.
[00:05:58.080 --> 00:05:59.080]   The seat goes back.
[00:05:59.080 --> 00:06:00.080]   There's a little table.
[00:06:00.080 --> 00:06:01.080]   This is the weirdest thing.
[00:06:01.080 --> 00:06:02.080]   Yeah.
[00:06:02.080 --> 00:06:03.640]   You can swing in front of you for your food.
[00:06:03.640 --> 00:06:07.560]   But the only bad thing is almost universally in that theater when you do press the button
[00:06:07.560 --> 00:06:09.880]   to make the seat goes out, it goes.
[00:06:09.880 --> 00:06:10.880]   It does.
[00:06:10.880 --> 00:06:11.880]   It does.
[00:06:11.880 --> 00:06:12.880]   It's really embarrassing.
[00:06:12.880 --> 00:06:13.880]   You got it.
[00:06:13.880 --> 00:06:14.880]   No, that was the seat.
[00:06:14.880 --> 00:06:18.800]   It's very embarrassing.
[00:06:18.800 --> 00:06:19.800]   It's, am I wrong?
[00:06:19.800 --> 00:06:20.800]   No, no, you're perfect.
[00:06:20.800 --> 00:06:23.480]   And you know what those little slide out tables are a perfect place to put a little
[00:06:23.480 --> 00:06:25.720]   tripod for your camera so you can take the film?
[00:06:25.720 --> 00:06:26.720]   Exactly.
[00:06:26.720 --> 00:06:27.720]   It's very small.
[00:06:27.720 --> 00:06:28.720]   And they don't mind if you do that.
[00:06:28.720 --> 00:06:29.720]   No, no.
[00:06:29.720 --> 00:06:30.720]   It's not.
[00:06:30.720 --> 00:06:31.720]   Yeah.
[00:06:31.720 --> 00:06:32.720]   They encourage it.
[00:06:32.720 --> 00:06:33.720]   Yeah.
[00:06:33.720 --> 00:06:34.720]   All right.
[00:06:34.720 --> 00:06:36.640]   That's the last you're going to hear about the Avengers in this entire episode.
[00:06:36.640 --> 00:06:38.160]   I just had to get that in.
[00:06:38.160 --> 00:06:39.880]   We might talk a little bit about the Game of Thrones.
[00:06:39.880 --> 00:06:40.880]   Nope.
[00:06:40.880 --> 00:06:41.880]   Nope, nope, no, no spoilers.
[00:06:41.880 --> 00:06:43.280]   Although this is the big battle.
[00:06:43.280 --> 00:06:44.600]   Do you follow that Paris?
[00:06:44.600 --> 00:06:50.440]   Um, I followed most of Game of Thrones up until like the last season or two whenever
[00:06:50.440 --> 00:06:51.640]   it diverged from the books.
[00:06:51.640 --> 00:06:52.640]   I got disappointed.
[00:06:52.640 --> 00:06:53.640]   Oh, wow.
[00:06:53.640 --> 00:06:55.920]   And that's a higher geek.
[00:06:55.920 --> 00:06:56.920]   That's like metagame.
[00:06:56.920 --> 00:06:57.920]   I respect that.
[00:06:57.920 --> 00:06:58.920]   Yeah.
[00:06:58.920 --> 00:06:59.920]   That's why.
[00:06:59.920 --> 00:07:02.760]   Because I feel like the dot, like it went from being like this very like long buildup
[00:07:02.760 --> 00:07:06.680]   show, you know, where there was, it would take like seven episodes for somebody to travel
[00:07:06.680 --> 00:07:11.160]   across the world and then all of these plot lines would kind of perfectly align to total
[00:07:11.160 --> 00:07:12.160]   fan service.
[00:07:12.160 --> 00:07:14.200]   Maybe it's changed in the past couple of seasons.
[00:07:14.200 --> 00:07:15.920]   Please don't @ me on Twitter.
[00:07:15.920 --> 00:07:19.960]   That is like the ultimate geek kind of humble wreck.
[00:07:19.960 --> 00:07:22.960]   Well, I stopped watching when it diverged books.
[00:07:22.960 --> 00:07:28.440]   Well, you know, Game of Thrones is one of the very, very, very few series that I actually
[00:07:28.440 --> 00:07:33.240]   enjoyed it in a on screen version more than the book version.
[00:07:33.240 --> 00:07:35.120]   It's almost universally, it's the other way around.
[00:07:35.120 --> 00:07:39.120]   Part of the problem though is I don't believe George RR Martin has finished the books.
[00:07:39.120 --> 00:07:40.120]   No.
[00:07:40.120 --> 00:07:41.120]   Oh, yeah.
[00:07:41.120 --> 00:07:42.120]   No, they can't wait.
[00:07:42.120 --> 00:07:43.120]   Yeah.
[00:07:43.120 --> 00:07:47.120]   It's like, dude, we're getting here the episode season six.
[00:07:47.120 --> 00:07:49.480]   We got you got to finish the book, dudes.
[00:07:49.480 --> 00:07:50.480]   Oh, more.
[00:07:50.480 --> 00:07:52.240]   We work on those for the rest of time.
[00:07:52.240 --> 00:07:55.160]   No, the last book is going to end with the final chapter of the last book.
[00:07:55.160 --> 00:07:56.160]   I already know what it is.
[00:07:56.160 --> 00:08:02.000]   It's Jon Snow wakes up and the entire thing's been a dream and he actually lives on a space
[00:08:02.000 --> 00:08:03.000]   station.
[00:08:03.000 --> 00:08:04.000]   It's Westworld.
[00:08:04.000 --> 00:08:05.000]   It's just a tie in from H.C.
[00:08:05.000 --> 00:08:06.000]   Series.
[00:08:06.000 --> 00:08:07.000]   Day.
[00:08:07.000 --> 00:08:08.000]   Yeah.
[00:08:08.000 --> 00:08:09.000]   Spoilers.
[00:08:09.000 --> 00:08:10.000]   Sorry.
[00:08:10.000 --> 00:08:11.000]   Sorry.
[00:08:11.000 --> 00:08:12.000]   Sorry.
[00:08:12.000 --> 00:08:13.000]   I'm going to be a chorus.
[00:08:13.000 --> 00:08:14.000]   Oh my god.
[00:08:14.000 --> 00:08:20.440]   This is a big night tonight though because I believe this is the longest battle in all
[00:08:20.440 --> 00:08:22.640]   of like movie and television or something.
[00:08:22.640 --> 00:08:26.880]   This is a battle that's going to go for three weeks and they're losing at least three
[00:08:26.880 --> 00:08:27.880]   main characters.
[00:08:27.880 --> 00:08:28.880]   Yeah.
[00:08:28.880 --> 00:08:32.880]   So the last last week it was like everybody's kind of basically saying goodbye.
[00:08:32.880 --> 00:08:33.880]   Right.
[00:08:33.880 --> 00:08:34.880]   Exactly.
[00:08:34.880 --> 00:08:36.200]   So honestly better than Thanos can do.
[00:08:36.200 --> 00:08:37.600]   It's more than half.
[00:08:37.600 --> 00:08:39.200]   All right.
[00:08:39.200 --> 00:08:41.960]   The other cool thing about that is when they shot it, they actually shot it out of
[00:08:41.960 --> 00:08:46.680]   sequence so that you couldn't tell who was dying by who was leaving the set.
[00:08:46.680 --> 00:08:48.120]   Oh, that's really clever.
[00:08:48.120 --> 00:08:49.400]   That's actually that's a good idea.
[00:08:49.400 --> 00:08:53.440]   I'll tell you one person who we know died the galaxy fold.
[00:08:53.440 --> 00:08:54.440]   Beautiful.
[00:08:54.440 --> 00:09:02.220]   I was prepared to go Friday against all best advice because as we know the galaxy fold
[00:09:02.220 --> 00:09:11.840]   failed in the hands of Mark Gurman, Mark has Brownlee, Steve Kovac, Dieter Bone, at least
[00:09:11.840 --> 00:09:12.840]   four, maybe more.
[00:09:12.840 --> 00:09:15.240]   Not just failed, but failed in different ways.
[00:09:15.240 --> 00:09:17.840]   So it wasn't the same defect that was causing it.
[00:09:17.840 --> 00:09:22.040]   It was there's a design problem with the phone, not just failed in different ways, but failed
[00:09:22.040 --> 00:09:24.600]   in different ways in two days.
[00:09:24.600 --> 00:09:25.600]   Exactly.
[00:09:25.600 --> 00:09:26.600]   Not really.
[00:09:26.600 --> 00:09:32.480]   And there were so many tech like I don't know people who were supposed to be reviewing
[00:09:32.480 --> 00:09:37.200]   this that just acts there was a like small film over it that looked like a screen protector.
[00:09:37.200 --> 00:09:41.920]   But when you pulled it off the total phone collapsed because that's apparently supposed
[00:09:41.920 --> 00:09:43.600]   to just be kept on the entire time.
[00:09:43.600 --> 00:09:44.600]   It was a tiny film.
[00:09:44.600 --> 00:09:49.000]   And of course they didn't apparently in the box or tell the reviewers at any point don't
[00:09:49.000 --> 00:09:50.000]   take the film off.
[00:09:50.000 --> 00:09:51.640]   They just thought they wouldn't.
[00:09:51.640 --> 00:09:55.520]   But apparently it's like in the instruction somewhere in the book that you know people
[00:09:55.520 --> 00:09:56.520]   are definitely reading.
[00:09:56.520 --> 00:10:00.720]   That makes sense to me because for us Asians we always keep the plastic on everything.
[00:10:00.720 --> 00:10:02.720]   So I mean our phones are still working fine.
[00:10:02.720 --> 00:10:04.160]   I don't know what's the problem.
[00:10:04.160 --> 00:10:05.160]   That's true.
[00:10:05.160 --> 00:10:06.160]   That's true.
[00:10:06.160 --> 00:10:10.200]   That's a stereotype but that may well be what we're thinking South Korea is like why would
[00:10:10.200 --> 00:10:11.440]   anybody remove that?
[00:10:11.440 --> 00:10:12.540]   I know.
[00:10:12.540 --> 00:10:13.540]   It's plastic.
[00:10:13.540 --> 00:10:14.540]   It comes from the factory.
[00:10:14.540 --> 00:10:15.540]   Why would you?
[00:10:15.540 --> 00:10:17.320]   My Samsung Galaxy's 10 came with a screen protector.
[00:10:17.320 --> 00:10:19.880]   I don't like screen protectors but it's pre-installed, factory installed.
[00:10:19.880 --> 00:10:23.480]   I immediately peeled it off so I understand why reviewers who had already looked at the
[00:10:23.480 --> 00:10:29.000]   S10 said oh yeah this is like the S10 and they just anyway last week when we convened
[00:10:29.000 --> 00:10:31.400]   there was some question about whether Samsung would still sell it.
[00:10:31.400 --> 00:10:33.080]   I was I actually pledged.
[00:10:33.080 --> 00:10:38.560]   I'm going to go buy it because I was really curious before it broke in the few hours before
[00:10:38.560 --> 00:10:40.200]   they lost the phone.
[00:10:40.200 --> 00:10:43.040]   Many reviewers reported this is actually better than you think.
[00:10:43.040 --> 00:10:46.440]   This is a very interesting way for a phone to work.
[00:10:46.440 --> 00:10:51.240]   A small screen on the front that you could just do notifications or make calls but when
[00:10:51.240 --> 00:10:53.680]   you want more you've got more by opening it up.
[00:10:53.680 --> 00:10:59.200]   I think I remember one reviewer saying I get it now and so I was prepared because I knew
[00:10:59.200 --> 00:11:02.200]   that they take it back if it broke but I guess I'm not going to get the chance.
[00:11:02.200 --> 00:11:07.880]   The real question is if we'll ever get the chance to buy a Galaxy Fold.
[00:11:07.880 --> 00:11:14.560]   AT&T weirdly sent out an email saying oh yeah June 13th we're going to have it.
[00:11:14.560 --> 00:11:16.520]   Samsung said no.
[00:11:16.520 --> 00:11:20.120]   But now I'm thinking maybe June 13th maybe Samsung's smart and saying we're not going
[00:11:20.120 --> 00:11:22.240]   to say a day until we figure out what's going on here.
[00:11:22.240 --> 00:11:25.920]   I mean it's still an interesting tech and it's still something I'd love to get my hands
[00:11:25.920 --> 00:11:30.840]   on but I mean the problem with the phone is what everyone thought was going to be with
[00:11:30.840 --> 00:11:32.920]   the phone which is durability of a folding screen.
[00:11:32.920 --> 00:11:34.160]   It works great in the lab.
[00:11:34.160 --> 00:11:36.800]   It works great when you have someone there who's telling you how to use it.
[00:11:36.800 --> 00:11:40.240]   Now take that out and misuse it a thousand ways and will it still work and of course
[00:11:40.240 --> 00:11:41.480]   not.
[00:11:41.480 --> 00:11:47.440]   But taking that misuse it in one way it won't work.
[00:11:47.440 --> 00:11:50.040]   Dale Poco on our chatroom said you know what they should have done is put another film
[00:11:50.040 --> 00:11:52.920]   on top of that film saying don't take off the next film.
[00:11:52.920 --> 00:11:55.400]   Yeah then you get to peel one thing.
[00:11:55.400 --> 00:11:57.120]   You get some peeling.
[00:11:57.120 --> 00:11:58.120]   Yeah.
[00:11:58.120 --> 00:12:01.680]   One of the most interesting developments with this though that I think is the whole I fix
[00:12:01.680 --> 00:12:06.080]   it saga so I fix it for I guess those who don't know is this website where they do really
[00:12:06.080 --> 00:12:12.120]   good tear downs of phones devices all sorts of things and they did a good tear down of
[00:12:12.120 --> 00:12:17.120]   it and one of my colleagues Louise Metzaki's had written about it specifically because they
[00:12:17.120 --> 00:12:21.760]   found some interesting things kind of like that there was a kind of notch hole in it
[00:12:21.760 --> 00:12:25.680]   that maybe food or something could have fallen in eventually and messed up the phone but
[00:12:25.680 --> 00:12:30.400]   within a couple days of posting this review and tear down they took it down from their
[00:12:30.400 --> 00:12:37.800]   main site and said that the person that I guess they had gotten it from had asked them
[00:12:37.800 --> 00:12:43.640]   to remove it that they can't really say anything more but that it is no longer going to be
[00:12:43.640 --> 00:12:47.760]   a published story which I think is incredibly odd.
[00:12:47.760 --> 00:12:53.280]   Yeah and certainly among tech journalist Twitter I was seeing lots of blow back on that you
[00:12:53.280 --> 00:12:57.560]   know you have like your first blow back where everybody saying this phone is breaking it's
[00:12:57.560 --> 00:13:00.120]   it's going to get into people's hands and it's going to fail.
[00:13:00.120 --> 00:13:07.240]   I saw so much criticism of Samsung and whoever this person was you know the partnership there
[00:13:07.240 --> 00:13:14.080]   essentially pulling this this excellent piece for my fix it and that's just like I think
[00:13:14.080 --> 00:13:22.120]   that Samsung has potentially done some harm for themselves in not just one well another
[00:13:22.120 --> 00:13:27.800]   way but also in terms of their relationship with journalists and particularly those who
[00:13:27.800 --> 00:13:35.400]   consider sort of the traditional aspects of journalism and how we don't want to censor
[00:13:35.400 --> 00:13:41.360]   these good pieces by saying oh no we're pulling that because we are taking back the phone not
[00:13:41.360 --> 00:13:42.360]   a good look.
[00:13:42.360 --> 00:13:48.080]   Yeah I fix it who by the way we should mention is a sponsor of some of our shows I fix it
[00:13:48.080 --> 00:13:54.240]   wrote we're under no obligation to remove our analysis legal or otherwise but out of
[00:13:54.240 --> 00:13:57.720]   respect for the partner that provided us with the phone and we don't know who that
[00:13:57.720 --> 00:14:03.440]   is and that's who Samsung contacted by the way not I fix it but the partner whom we consider
[00:14:03.440 --> 00:14:06.800]   an ally in making devices more repairable we're choosing to withdraw our story until
[00:14:06.800 --> 00:14:11.560]   we could purchase a galaxy full of retail that may be a while but it had the Streisand
[00:14:11.560 --> 00:14:17.520]   effect because yes something on the internet doesn't go away just because you take the
[00:14:17.520 --> 00:14:24.680]   link down it's already archive.org 30 replies to that tweet with the link the archive link
[00:14:24.680 --> 00:14:30.880]   yeah and the big two you accomplished except to attract more attention and the big ticket
[00:14:30.880 --> 00:14:36.000]   items that were that it got a two out of ten on the repairability scale and the big big
[00:14:36.000 --> 00:14:40.920]   item was the fact that they found there was no real way to keep dust from ending up within
[00:14:40.920 --> 00:14:45.840]   the OLED enclosure and then that's what's causing a lot of the problems you get any dust
[00:14:45.840 --> 00:14:50.360]   in there whatsoever when you fold it close the OLEDs tend to fold inward on themselves
[00:14:50.360 --> 00:14:54.120]   and if there's any any particles whatsoever you will cause damage every time you close
[00:14:54.120 --> 00:14:58.720]   it so I mean that was the big takeaway from the tear down which is interesting system
[00:14:58.720 --> 00:15:03.040]   of course not very repairable everything is glued into everything else but the screen
[00:15:03.040 --> 00:15:10.600]   seems designed to fail it's not it can't last all that screens notoriously oh yes fragile
[00:15:10.600 --> 00:15:15.280]   right right so that's the problem and only only it's screen can be bent in this way because
[00:15:15.280 --> 00:15:20.400]   it's not glass it's plastic but in order to keep that flexibility you need a way to keep
[00:15:20.400 --> 00:15:25.160]   contaminants out of the enclosure in which the OLED is sandwiched and they didn't because
[00:15:25.160 --> 00:15:31.280]   there's a visible gap right I'm just I'm a little now I'm just curious what you all
[00:15:31.280 --> 00:15:37.920]   think I have Samsung's the number one manufacturer of phones in the world number two goes back
[00:15:37.920 --> 00:15:45.240]   at fourth between Huawei and Apple but Samsung is big I am a big fan of the notes I was from
[00:15:45.240 --> 00:15:52.160]   day one I think this s10 is the best phone I've ever had so I have I feel bad for Samsung
[00:15:52.160 --> 00:15:56.840]   and I'm wondering do you guys feel bad for Samsung or do you think it serves them right
[00:15:56.840 --> 00:16:03.660]   I mean I don't I don't go ahead your turn I was just gonna say I don't feel bad for
[00:16:03.660 --> 00:16:11.400]   Samsung but I feel bad for or well okay I can't feel bad for Samsung because I feel bad that
[00:16:11.400 --> 00:16:19.080]   the company has tried a new thing and it is not you know this to be innovative to try
[00:16:19.080 --> 00:16:23.320]   something new and for something like folding phones which we've seen these reviewers say
[00:16:23.320 --> 00:16:29.240]   there is something to this I feel bad that it failed in that way but when we see when
[00:16:29.240 --> 00:16:32.560]   you know I fix it takes it apart and sees that there are all of these reasons why it
[00:16:32.560 --> 00:16:37.200]   failed that's the part where I don't feel as bad for the company itself and we're like
[00:16:37.200 --> 00:16:41.640]   a little bit of shot in front of comes in but in terms of a company stepping out and
[00:16:41.640 --> 00:16:46.360]   trying something new yeah it's kind of like a person who doesn't normally dance getting
[00:16:46.360 --> 00:16:51.600]   out on the dance floor and yes and you're yeah you're a current yeah you're encouraging
[00:16:51.600 --> 00:16:56.960]   them and you know they're fighting their lower lip exactly dancing if there's dust
[00:16:56.960 --> 00:17:02.440]   getting into the outfit they're wearing you know and it's it's it's okay and I'm glad that
[00:17:02.440 --> 00:17:10.240]   they tried it and so Samsung you keep dancing honey how about you Paris what what's your
[00:17:10.240 --> 00:17:14.640]   reaction I agree I'm happy that they tried this but I'm also I mean Samsung is a large
[00:17:14.640 --> 00:17:20.240]   company I'm more confused than anything that they would allow this to get into the get to
[00:17:20.240 --> 00:17:24.240]   the stage where they're giving phones to tech reviewers and the sense we go in to roll it
[00:17:24.240 --> 00:17:32.280]   out soon while the phone is clearly I mean as the I fix it review showed has some serious
[00:17:32.280 --> 00:17:38.280]   issues with it that I mean if a team of just like bloggers taking it apart could tell that
[00:17:38.280 --> 00:17:42.520]   and I feel like all of Samsung probably should have seen this coming before they handed it
[00:17:42.520 --> 00:17:48.240]   out to a bunch of people who have the power to point out all of these obvious mistakes
[00:17:48.240 --> 00:17:52.920]   you do wonder how this happened well I mean they had some interesting technology they thought
[00:17:52.920 --> 00:17:56.960]   they've been working on this for at least five years but you feel like they never gave
[00:17:56.960 --> 00:18:02.160]   it to a human well I mean they gave it to the machine that tested the opening and closing
[00:18:02.160 --> 00:18:09.000]   was like very carefully balancing the pressure along all the edges and you know give it to
[00:18:09.000 --> 00:18:15.200]   a three year old yes that's exactly right why is there no child trying this device out
[00:18:15.200 --> 00:18:21.000]   to see if it passes because no child should have a $2,000 phone okay that's right yeah
[00:18:21.000 --> 00:18:26.400]   but I've been here's the thing this is not going to affect Samsung all that much because
[00:18:26.400 --> 00:18:31.880]   they they're billions of phones don't typically come from the top end that we deal with yes
[00:18:31.880 --> 00:18:37.960]   they sell all of the almost feature phones right so this is this I'm I'm with you Mike
[00:18:37.960 --> 00:18:43.520]   I think this is a chance for us to see what might be possible I want Samsung to keep doing
[00:18:43.520 --> 00:18:48.880]   this even if it does make mean more mistakes $2,000 was a crazy price point which meant
[00:18:48.880 --> 00:18:52.880]   that very few people were actually going to buy it except as status symbols so you know
[00:18:52.880 --> 00:18:58.680]   yeah should they have waited in hindsight maybe a tiny bit but it looked like an interesting
[00:18:58.680 --> 00:19:05.560]   enough product that people would pay two grand to get one I would have you're the source
[00:19:05.560 --> 00:19:11.120]   aren't you I fix a person to hand it over your phone I didn't get one yeah I you know
[00:19:11.120 --> 00:19:17.280]   it's pockets they're just full of galaxy this makes me actually you know sometimes I feel
[00:19:17.280 --> 00:19:23.280]   bad that I don't get offered the you know pre-release stuff I usually turn it down because
[00:19:23.280 --> 00:19:27.760]   I don't want pre-production stuff for sure sometimes it is and you can't really judge
[00:19:27.760 --> 00:19:33.760]   a product based on its pre-production you pay for access though yeah you do because they
[00:19:33.760 --> 00:19:39.920]   harass you they don't mean to with the publicist call you up and say how's that review going
[00:19:39.920 --> 00:19:46.680]   you like it and then after you're good hey what you don't what's the matter with you
[00:19:46.680 --> 00:19:51.680]   and yeah I prefer not to deal with it it's actually even a step worse than that now I
[00:19:51.680 --> 00:19:56.840]   I have people who are getting into this field and their reviews are basically given to them
[00:19:56.840 --> 00:20:03.840]   by the PR people yeah well they want the free they want the free stuff right and they don't
[00:20:03.840 --> 00:20:09.560]   want to do the work of actually reviewing it so it's a win win and I feel like that's
[00:20:09.560 --> 00:20:15.320]   one of the major issues with the review based world is that I mean I there was what the
[00:20:15.320 --> 00:20:22.000]   scandal with one of the iPhone 10 kind of came out I believe whenever they did not give
[00:20:22.000 --> 00:20:28.320]   iPhone 10 pre-release models to certain outlets that had written poorly about Apple in years
[00:20:28.320 --> 00:20:32.520]   past of course I guess that's Apple's prerogative they can do what they want but it's ridiculous
[00:20:32.520 --> 00:20:39.360]   that this whole market is kind of it depends on the quality of your review and how much
[00:20:39.360 --> 00:20:43.520]   Apple or Samsung or whatever company you're talking about is actually going to like what
[00:20:43.520 --> 00:20:48.080]   you produce I'm sure there's I'm sure there's always an undercurrent of that but Apple has
[00:20:48.080 --> 00:20:55.400]   never is surprisingly unabashed at at and Microsoft in its heyday in the 90s was very
[00:20:55.400 --> 00:21:00.560]   much like this in punishing the people the journalists it didn't like and rewarding the
[00:21:00.560 --> 00:21:06.320]   journalists it did like to Vorek always tells the story of walking by somebody's office at
[00:21:06.320 --> 00:21:10.400]   Microsoft a big big executive and there was a blackboard with this with basically an
[00:21:10.400 --> 00:21:14.360]   enemies list these are the people we don't like and Dorek was very proud that he was
[00:21:14.360 --> 00:21:21.480]   on that list Microsoft never was afraid to do that and Apple now is really wielding its
[00:21:21.480 --> 00:21:28.080]   power I think to do that it isn't it's not your right Paris is there right but it's also
[00:21:28.080 --> 00:21:33.200]   wrong but to be clear this is this is not unique to the tech world I mean this is every
[00:21:33.200 --> 00:21:38.160]   journalistic venue on the planet car magazines are notorious car magazines or you want to
[00:21:38.160 --> 00:21:41.600]   be in the financial markets well you have to play nice with the banks to get the information
[00:21:41.600 --> 00:21:45.480]   and then and then you have to decide when you're going to spend that because Washington you
[00:21:45.480 --> 00:21:50.040]   write that be you don't get access if you're going to play nice you want to be a White House
[00:21:50.040 --> 00:21:52.880]   reporter there are certain rules you're going to have to follow you want to follow the belt
[00:21:52.880 --> 00:21:57.000]   way well you have to play by their ball so it's not just a tech thing we just notice it
[00:21:57.000 --> 00:22:04.520]   more because especially now it's the influencer culture yeah I mean it's it's fire festival
[00:22:04.520 --> 00:22:11.040]   in the Silicon Valley yeah we got Trey Radcliffe just wrote a new book about how influencers
[00:22:11.040 --> 00:22:14.920]   have ruined Instagram Trey is probably most of our viewers know because he's been a regular
[00:22:14.920 --> 00:22:19.600]   on our network as a fabulous photographer who was able to use Instagram and other platforms
[00:22:19.600 --> 00:22:28.720]   like Google+ and Facebook to build a great career but he says the influencers on Instagram
[00:22:28.720 --> 00:22:34.000]   have really just rooted not just real influencers but phony influencers there's so much money
[00:22:34.000 --> 00:22:41.240]   to be made as an influencer on Instagram Paris Martenow might have written a nice story oh
[00:22:41.240 --> 00:22:46.400]   I remember that yeah he's following along in what you wrote exactly I forgot we talked
[00:22:46.400 --> 00:22:50.520]   about that last time you were on yeah there's a there's a print piece about that very topic
[00:22:50.520 --> 00:22:57.400]   in this issue of wired the May issue on these is that a different piece than the than the
[00:22:57.400 --> 00:23:02.240]   one we talked about it is different it's it's more kind of about the icon like the spawn
[00:23:02.240 --> 00:23:07.920]   con economy in the influencer world and how I guess because you just mentioned fake influencers
[00:23:07.920 --> 00:23:14.400]   are particularly damaging for the Instagram world and it kind of deals with that in the
[00:23:14.400 --> 00:23:22.840]   sense that the rise of spawn con this is the online version inside the weird and booming
[00:23:22.840 --> 00:23:30.200]   industry of online influence and part of the problem is people are buying followers yeah
[00:23:30.200 --> 00:23:38.920]   I mean basically part of the issue is that the metrics and ways in which we view and value
[00:23:38.920 --> 00:23:44.880]   content or ideas online we kind of view them as the equivalent as their real world equivalence
[00:23:44.880 --> 00:23:49.560]   like oh if this tweet or Instagram post has thousands and thousands of likes and comments
[00:23:49.560 --> 00:23:55.120]   then that's valuable content and people agree with that idea or maybe like that post but
[00:23:55.120 --> 00:24:01.160]   that isn't always true because all of these metrics can be very easily bought and often
[00:24:01.160 --> 00:24:06.520]   are and another them hold the same weight as their real world equivalence nor are there
[00:24:06.520 --> 00:24:12.840]   any real ways to check whether let's say these 10,000 likes are real or fake or the
[00:24:12.840 --> 00:24:17.160]   first thousand were fake and that's what boosted them to get 9,000 more real ones and
[00:24:17.160 --> 00:24:21.480]   it's like how do you even begin to balance the way to content and that and the value
[00:24:21.480 --> 00:24:27.440]   of content or ideas online you can't really. There was an interesting paper by I believe
[00:24:27.440 --> 00:24:34.720]   it was MIT's Media Lab where they were able to show what manipulated metrics look like
[00:24:34.720 --> 00:24:39.360]   versus organic metrics because in an organic metric let's say you've got a tweet or an
[00:24:39.360 --> 00:24:44.320]   Instagram post that's particularly popular you get a big boom at the start and then it
[00:24:44.320 --> 00:24:49.280]   just kind of tapers off and it does sort of just decline into obscurity when you've got
[00:24:49.280 --> 00:24:54.960]   a boosted post what you see is it actually starts low and then it goes high and then
[00:24:54.960 --> 00:25:01.080]   every time it starts to decline it goes back up. But the problem with that is you can't
[00:25:01.080 --> 00:25:05.480]   just look at that chart and say well they're cheating and they're not because you get the
[00:25:05.480 --> 00:25:10.320]   same thing if you pay for it through Twitter or Facebook so the only difference is do I
[00:25:10.320 --> 00:25:17.800]   pay Facebook or do I pay my army of bots to get that continually boosted post.
[00:25:17.800 --> 00:25:20.000]   And part of the issue, oh sorry.
[00:25:20.000 --> 00:25:21.280]   No please please continue.
[00:25:21.280 --> 00:25:25.760]   Oh sorry and part of the issue is now that the fakers and the people who kind of sell
[00:25:25.760 --> 00:25:31.800]   the it's called like social like SMM panels is often where people kind of get these fake
[00:25:31.800 --> 00:25:37.680]   likes and everything like that. They're now offering services that specifically exist
[00:25:37.680 --> 00:25:43.480]   to kind of counter that narrative where one place that I often kind of go to check the
[00:25:43.480 --> 00:25:48.880]   temperature of the online fake economy is this is called social media SM garden.
[00:25:48.880 --> 00:25:56.400]   And I've seen recently is they offer a slow likes or slow YouTube views, no drops which
[00:25:56.400 --> 00:26:02.120]   means essentially that you would get if you buy 10,000 or 100,000 views or YouTube video
[00:26:02.120 --> 00:26:06.440]   they'll trickle them in every hour on the hour for a couple of days.
[00:26:06.440 --> 00:26:08.600]   Right and they watch all the way through.
[00:26:08.600 --> 00:26:14.320]   You know, there are the other types where, oh my gosh, I got a million views but 90%
[00:26:14.320 --> 00:26:17.200]   of them dropped after the first 15 seconds.
[00:26:17.200 --> 00:26:21.240]   And you can actually buy full engagement because you can tell them, no, I want you to
[00:26:21.240 --> 00:26:25.600]   actually download and be active the entire time of the video.
[00:26:25.600 --> 00:26:31.040]   So yeah, it's a cat and mouse game and there's a part of me that says we kind of did this,
[00:26:31.040 --> 00:26:34.840]   right? I mean, this is our industry. This is this is Twit. This is online where we
[00:26:34.840 --> 00:26:36.040]   this is not Twitter.
[00:26:36.040 --> 00:26:43.640]   But in the sense that one of the big selling points when when you started this this revolution
[00:26:43.640 --> 00:26:48.440]   of where advertising dollars go is we said, look, we can tell you where your advertising
[00:26:48.440 --> 00:26:53.080]   money is being spent. We can show you the engagement. We can show you much better metrics
[00:26:53.080 --> 00:26:56.160]   than you can get off of the old Nielsen system.
[00:26:56.160 --> 00:27:03.120]   So we created a system that looked more honest, but now it's being gained in and there's no
[00:27:03.120 --> 00:27:04.240]   easy fix for that.
[00:27:04.240 --> 00:27:09.800]   So my question is though, who is really harmed by this fake? So it's not really readers because
[00:27:09.800 --> 00:27:14.240]   if you don't really have a million followers, they're not nobody's really seeing except
[00:27:14.240 --> 00:27:19.120]   a bunch of bots, you're trying to trick Samsung. You're trying to trick the companies to give
[00:27:19.120 --> 00:27:20.360]   you money, right?
[00:27:20.360 --> 00:27:27.120]   Yes and no, because yes, they're fake, but and Paris, you alluded to this. It might be
[00:27:27.120 --> 00:27:31.240]   fake at the beginning, but because of the way that search engines are weighed.
[00:27:31.240 --> 00:27:32.240]   Self.
[00:27:32.240 --> 00:27:37.200]   So since you got so much fake popularity, now you're actually getting organic views. Well,
[00:27:37.200 --> 00:27:41.840]   okay, now do we consider is that fair game now? Is that table stakes? Do I have to pay
[00:27:41.840 --> 00:27:46.320]   a little bit to get my post boosted above the noise in order to give it a chance to get
[00:27:46.320 --> 00:27:47.320]   popular?
[00:27:47.320 --> 00:27:48.320]   Yeah.
[00:27:48.320 --> 00:27:50.560]   And I don't know the answer to that.
[00:27:50.560 --> 00:27:55.960]   Yeah, and I think of the where it where that ends up leading to is it plays into the content
[00:27:55.960 --> 00:27:59.760]   cycle. Like, let's say one thing that I think about which is a total hypothetical, but I
[00:27:59.760 --> 00:28:03.880]   think it's a good example of this. You remember after the parkland shooting, there was a large
[00:28:03.880 --> 00:28:09.400]   news cycle about how one of the top the top video on YouTube was about the crisis act
[00:28:09.400 --> 00:28:13.520]   or kind of conspiracy. And everyone, there were a lot of articles and reactions written
[00:28:13.520 --> 00:28:17.840]   to that. And it was kind of the trending topic on Twitter for a while. And I was thinking
[00:28:17.840 --> 00:28:21.160]   at the time, I was like, how do we even know if all those views are correct? Like, what
[00:28:21.160 --> 00:28:26.640]   if if somebody hypothetically buys a lot of views for a YouTube video or tweet that ends
[00:28:26.640 --> 00:28:33.040]   up becoming popular and it's, let's say, a polarizing exposing like a polarizing viewpoint.
[00:28:33.040 --> 00:28:38.920]   And then people start using that tweet or account or, I don't know, article or video
[00:28:38.920 --> 00:28:43.240]   as a way to say, oh, this is what the right of the left is doing. This is the emblematic
[00:28:43.240 --> 00:28:47.400]   of a certain viewpoint. We should react to that in this way. And this whole content cycle
[00:28:47.400 --> 00:28:52.520]   starts up that was originally predicated and alive. But because now people are engaging
[00:28:52.520 --> 00:28:58.600]   with it, it kind of becomes a partial truth in a way. And I mean, there's no really,
[00:28:58.600 --> 00:29:02.760]   there's no way to ensure that we are not participating in that or no way really to
[00:29:02.760 --> 00:29:05.800]   counteract against it. It's just something that we're privy to.
[00:29:05.800 --> 00:29:12.040]   So Paris, when you said social media garden, I created an account and logged in. How does
[00:29:12.040 --> 00:29:18.920]   this even exist? Tell me what this thing does. Oh, so, so go over on the left. I can't really
[00:29:18.920 --> 00:29:26.920]   see any money in it yet. So they want me to deposit funds, obviously, right? And then
[00:29:26.920 --> 00:29:29.760]   and then what? There is go over to like it's social media
[00:29:29.760 --> 00:29:35.320]   dot garden slash pricing. Oh, pricing it in here. Yeah, I got it right here. Yeah. So
[00:29:35.320 --> 00:29:39.520]   there that'll show you the pricing table of the things you can buy. How is this evenly?
[00:29:39.520 --> 00:29:48.800]   I can buy high retention, slow views, 300 to 600 a day for $2.80 and 80 cents per thousand
[00:29:48.800 --> 00:29:58.840]   up to 10. I can buy 10,000 views. That's a deal on YouTube. Is it only you? Oh, no,
[00:29:58.840 --> 00:30:03.880]   wait a minute. Instagram tabs for all of the sound cloud. Facebook.
[00:30:03.880 --> 00:30:11.280]   I can buy sound cloud followers, plays, likes, reposts, downloads. So what do they have?
[00:30:11.280 --> 00:30:18.240]   A panel of humans in Asia or somewhere that are doing this? I'm not sure in the case of
[00:30:18.240 --> 00:30:22.280]   SM gardens, specifically, but it could either be bots. It could be. I don't know if you've
[00:30:22.280 --> 00:30:26.840]   ever seen their particularly dramatic videos of these things called Chinese click farms
[00:30:26.840 --> 00:30:31.360]   where they have a wall of smartphones, right? Walls and walls of phones that are automated
[00:30:31.360 --> 00:30:36.320]   to click and watch things or in some cases, if it's a particularly advanced operation,
[00:30:36.320 --> 00:30:41.880]   they'll have like 100 people in a room just, you know, watching clicking videos of manning
[00:30:41.880 --> 00:30:45.680]   machines. And the idea of slow quizzes, you don't want to trigger something. There it
[00:30:45.680 --> 00:30:50.920]   is. You don't want to trigger any thing on YouTube that says, wait a minute, they got
[00:30:50.920 --> 00:30:56.520]   too many likes and too few minutes. It looks more organic. They maybe even do it in spurts
[00:30:56.520 --> 00:31:02.200]   and stuff like real people would. Is it full? I mean, YouTube must know about this. Oh, definitely.
[00:31:02.200 --> 00:31:07.520]   I mean, it's a constant kind of back and forth in YouTube in particular. Viewbodding is often
[00:31:07.520 --> 00:31:12.320]   what this is called or just buying fake views is something that YouTube has been fighting
[00:31:12.320 --> 00:31:16.760]   against since the beginning. And it's interesting, I guess, to look at this pricing table now
[00:31:16.760 --> 00:31:23.000]   because I was looking at it in January, February when I was writing the print piece, which
[00:31:23.000 --> 00:31:27.520]   had mentioned this and there was maybe like five different YouTube options available
[00:31:27.520 --> 00:31:34.160]   then because I guess YouTube had cracked down on a specific type of fake viewbodding. But
[00:31:34.160 --> 00:31:38.480]   now there are like 15 or whatever available on there. So I guess, I mean, this kind of
[00:31:38.480 --> 00:31:43.880]   cycle and back and forth happens, where YouTube will crack down and win for a bit. And then
[00:31:43.880 --> 00:31:49.360]   the spammers and fakers will adapt their methods and come back.
[00:31:49.360 --> 00:31:56.920]   So the cheese Amazon on there as well? No, twitches. I don't see where I can buy views
[00:31:56.920 --> 00:32:02.520]   for a for priest book. Is that the Vatican social?
[00:32:02.520 --> 00:32:10.700]   It's useless for me then. It's one of the holy site. So the cheapest thing on here is
[00:32:10.700 --> 00:32:22.300]   mobile views on YouTube, only $2.60 per thousand. And it takes 300 to 600 a day. So
[00:32:22.300 --> 00:32:27.220]   it builds up slowly. That's pretty cheap. Do you think this is? I mean, honestly, do
[00:32:27.220 --> 00:32:33.500]   you think big YouTube stars are using this? Who's this for? Yeah. Why wouldn't they, I
[00:32:33.500 --> 00:32:38.700]   guess? Well, certainly wannabes. But yeah, but if you make more money, if you're charging
[00:32:38.700 --> 00:32:47.780]   as I would imagine, CPMs cost per thousand impressions or adding a few thousand impressions.
[00:32:47.780 --> 00:32:54.460]   Look, it's only $2.60 per thousand. If you're charging twice that per thousand as a CPM,
[00:32:54.460 --> 00:33:02.660]   it's worth it. Yeah. And I mean, that's one big issue that a lot of companies have in
[00:33:02.660 --> 00:33:08.780]   regards to influencer marketing is there was some study, I believe, from maybe it was clear
[00:33:08.780 --> 00:33:14.700]   cloud or one of these social media marketing and analytics firms. They said that companies,
[00:33:14.700 --> 00:33:18.700]   their number one problem right now is trying to figure out whether influencers actually
[00:33:18.700 --> 00:33:22.540]   have genuine stats versus fake stats, because they don't know how much you pay them for
[00:33:22.540 --> 00:33:29.660]   any campaigns. So that's who it hurts. I mean, in the long run, I would imagine, and I would
[00:33:29.660 --> 00:33:34.940]   hope, since it'd be good for me, brands would turn away from this and go to places where
[00:33:34.940 --> 00:33:39.860]   they have, you know, they can really measure. You would hope that, but I mean, put yourself
[00:33:39.860 --> 00:33:46.300]   into the mind of an advertiser, you have to assume that if it's this easy to cheat, everyone's
[00:33:46.300 --> 00:33:51.420]   cheating. Right. And so therefore you just take 30% off of whatever quoted metric they
[00:33:51.420 --> 00:33:57.020]   give you. And that's what you pitch. Honestly, advertisers don't have much better stats for
[00:33:57.020 --> 00:34:02.380]   TV or straight, you either. So there those poor have all I feel, by the way, this is why
[00:34:02.380 --> 00:34:08.820]   Google, this is why Google and Facebook have 80% of all digital ad sales. Right. But there's
[00:34:08.820 --> 00:34:14.220]   a big problem with Click fraud there too. Yeah, and actually, yes, let's clear that
[00:34:14.220 --> 00:34:18.420]   up. This is not a digital phenomenon. And this was when Oh, yeah, it's been going on
[00:34:18.420 --> 00:34:22.620]   for years. Does every newspaper that get printed actually get read? No, but they charge
[00:34:22.620 --> 00:34:26.820]   on print magazines and newspapers. No, Toria is for doing counting what they call
[00:34:26.820 --> 00:34:33.020]   pass along numbers where well, TV guide, we know has through, you know, a million subscribers,
[00:34:33.020 --> 00:34:41.580]   but we figure each TV guy gets read by 18 people. So it's exactly how they do it. It's
[00:34:41.580 --> 00:34:47.460]   kind of amazing. Or we know by watching 0.01% of the households of the United States,
[00:34:47.460 --> 00:34:52.380]   we're watching. That's not right either. We had a problem with that on tech TV in the
[00:34:52.380 --> 00:34:59.220]   old days because we had just small audience that out of the, you know, whatever 1000 Nielsen
[00:34:59.220 --> 00:35:06.240]   families, maybe one person was watching maybe two if that day that person to got sick,
[00:35:06.240 --> 00:35:12.460]   a raise guy got in half. So it wasn't a very good system. It's not a good system for small
[00:35:12.460 --> 00:35:13.460]   networks.
[00:35:13.460 --> 00:35:20.340]   Wait, question, did any of us know a Nielsen family? I never knew a Nielsen family. No,
[00:35:20.340 --> 00:35:24.700]   I'm not supposed to know Nielsen families. I would get in trouble if I knew a Nielsen
[00:35:24.700 --> 00:35:28.500]   family. Do we even know how many of you need someone to introduce yourself as a Nielsen
[00:35:28.500 --> 00:35:29.500]   family?
[00:35:29.500 --> 00:35:32.420]   One of my anybody let's ask our chair room. There's a few hundred people in there. Anybody
[00:35:32.420 --> 00:35:36.940]   ever? I know people have done arbitron ratings. Were you but the people, John, John was a
[00:35:36.940 --> 00:35:41.740]   Nielsen family had a people meter in your house. You got a diary. Yeah, that's the old radio
[00:35:41.740 --> 00:35:46.220]   way of doing it was a. Yeah, that's the old system that the radio used was you'd actually
[00:35:46.220 --> 00:35:50.460]   write down. And that favored a certain kind of programming. It's one of the reasons Rush
[00:35:50.460 --> 00:35:55.220]   Limbaugh early on got very, very good ratings. His fans were so devout. They would just at
[00:35:55.220 --> 00:35:58.300]   the end of the week, nobody writes it in the ratings to get at the end of the week. Oh,
[00:35:58.300 --> 00:36:01.940]   I got a mail. I said, I better write in what I listened to. They just write Rush Limbaugh,
[00:36:01.940 --> 00:36:02.940]   Rush Limbaugh.
[00:36:02.940 --> 00:36:06.100]   Have you one of the chat room is talking about a diary or a book? That's the old way. Now
[00:36:06.100 --> 00:36:10.060]   they have these people meters, which are a little more reliable because they record all
[00:36:10.060 --> 00:36:16.020]   the time. You were a people meter house? Your brother. I mean, he wears a little thing
[00:36:16.020 --> 00:36:20.980]   in the round. What? It's easy enough to know exactly what everyone in America is listening
[00:36:20.980 --> 00:36:25.580]   to since our phones are listening to us 24/7. I mean, they can just get from now. Oh,
[00:36:25.580 --> 00:36:34.420]   don't do that. Honestly, many TVs have microphones. Yes. They have also many TVs had a system
[00:36:34.420 --> 00:36:38.740]   to look at the pixels of what you're watching to and figure out what those pixels mean,
[00:36:38.740 --> 00:36:44.180]   what show you're watching. So your TVs are phoning home. Look at your, your privacy agreement
[00:36:44.180 --> 00:36:48.100]   or your license agreement of your television. In most cases, modern TVs are doing that.
[00:36:48.100 --> 00:36:52.140]   That's why I don't plug in a smart TV ever. It doesn't get any network. Don't give it
[00:36:52.140 --> 00:36:56.780]   a network. No, that's probably a good idea. All right. Let's take a little break and
[00:36:56.780 --> 00:37:02.860]   we'll come back. Good. Good panel day from Wired Magazine and wired.com Paris Martino.
[00:37:02.860 --> 00:37:06.620]   I'm sorry I forgot that you wrote that great influencers. You are that you know all about
[00:37:06.620 --> 00:37:13.620]   this subject. Great to have you, Paris. Buying. I'm going to start buying hits on social
[00:37:13.620 --> 00:37:20.620]   social. Yeah, next week, you're waiting to go up 100,000. No, I pledge we will never
[00:37:20.620 --> 00:37:27.580]   do that. By the way, Paris, I did a keynote in Croatia and one part relied very heavily
[00:37:27.580 --> 00:37:32.500]   on your reporting. So thank you very much for that. Oh, thank you. I love you in Croatia,
[00:37:32.500 --> 00:37:36.140]   Paris. Wow. That sounds a little.
[00:37:36.140 --> 00:37:43.140]   Is it split to Brovnik? Zagreb. Zagreb. They love you in Zagreb. Zagreb loves Paris.
[00:37:43.140 --> 00:37:47.220]   Oh, whoa. That's a hashtag.
[00:37:47.220 --> 00:37:53.420]   Hashtag Zagreb loves Paris. Everybody loves Micah Sargent, Chihuahua.coffee. And we're
[00:37:53.420 --> 00:37:57.780]   going to what? I was just going to say I'm kind of concerned now. I think maybe everybody
[00:37:57.780 --> 00:38:03.620]   in my life. I have to see if they're buying like, oh, I got 12 hugs today. No, you probably
[00:38:03.620 --> 00:38:07.500]   bought those. I just don't. I can't believe any metric anymore. It's kind of scary. It
[00:38:07.500 --> 00:38:13.100]   kind of makes you think the whole world is lie. Everyone's lying and everything's
[00:38:13.100 --> 00:38:20.260]   a lie. Yeah. And father, Robert Ballasier, the digital Jesuit on loan from God. Yes. And
[00:38:20.260 --> 00:38:27.140]   I think I'm going to change my Twitter name to Chihuahua SJ. That is an awesome man. I
[00:38:27.140 --> 00:38:32.420]   show today literally brought to you by our content delivery network, cash fly. You know
[00:38:32.420 --> 00:38:39.980]   what a CDN is? Well, you're using one. You're soaking in it. A CDN gets content to you faster
[00:38:39.980 --> 00:38:45.340]   by moving the content to servers closer to you. Cash fly is amazing with servers all
[00:38:45.340 --> 00:38:50.660]   over the world. Whenever you download a Twitter episode, audio or video, you're getting it
[00:38:50.660 --> 00:38:56.260]   from the server nearest you through cash fly. That means it's faster about 10 times faster
[00:38:56.260 --> 00:39:01.020]   than the traditional you come to our website and get it. And it's and cash fly works so
[00:39:01.020 --> 00:39:06.660]   well, it's even 30% faster than other major CDNs. It delivers rich media content right
[00:39:06.660 --> 00:39:11.380]   to our users without a hitch. And and you know what? I hear this all the time for people
[00:39:11.380 --> 00:39:17.020]   when I'm, you know, downloading podcasts, some other podcast, sometimes a little slow
[00:39:17.020 --> 00:39:22.180]   to come in. Sometimes I don't get it. Twit always comes in fast. You can thank cash fly
[00:39:22.180 --> 00:39:27.260]   for that. No matter what industry you're in, if your website is tied to company revenue,
[00:39:27.260 --> 00:39:32.540]   if your downloads are important to you, give your customer the fastest downloads they
[00:39:32.540 --> 00:39:39.460]   need with cash fly. LG uses cash fly, Microsoft uses cash fly Adobe and ours Technica uses
[00:39:39.460 --> 00:39:44.180]   cash fly. We've been using it for a decade. And I'm not once, not once have I got a 3am
[00:39:44.180 --> 00:39:51.460]   call saying the CDNs down. Never get that we we go down, but the CDN is always up. In
[00:39:51.460 --> 00:39:56.660]   fact, 100% uptime SLA. And the reason they can do that is because they have all those
[00:39:56.660 --> 00:40:00.780]   servers. If one goes down, they still got another to serve your content. No billing
[00:40:00.780 --> 00:40:06.060]   spikes either. This I really like when we sat down with cash fly, they tailored a plan
[00:40:06.060 --> 00:40:11.420]   on our yearly usage, not our daily monthly weekly usage, because we have very spiky downloads.
[00:40:11.420 --> 00:40:16.500]   As you might imagine, Sunday is a big day for Twit, right? So I don't want to pay a lot
[00:40:16.500 --> 00:40:20.060]   more on Sunday or have to call and make sure that I haven't gone over my limit. Don't
[00:40:20.060 --> 00:40:23.780]   have to worry about it. They smooth it all out. In fact, on average customers who switched
[00:40:23.780 --> 00:40:30.260]   to cash fly save more than 20%, 20%. Look at all the companies that use cash fly. Right
[00:40:30.260 --> 00:40:36.060]   now you can call cash fly and get a complimentary detailed analysis of your current CDN bill
[00:40:36.060 --> 00:40:41.340]   and usage trends and see how much you can save with cash fly. It's very easy. It's
[00:40:41.340 --> 00:40:45.740]   a really simple thing to do. And I promise you, no hard sell. They're nice people that
[00:40:45.740 --> 00:40:51.220]   will give you the information you need to make the right choice. Twit.cash fly.com.
[00:40:51.220 --> 00:40:58.900]   Twit.cash fly.com. We really couldn't do Twit without cash fly. Twit.cash fly.com. I was
[00:40:58.900 --> 00:41:02.580]   talking about BitTorrent on the radio show earlier and it reminded me in the early days
[00:41:02.580 --> 00:41:07.980]   that's how we got Twit out, was we would do a BitTorrent feed. We'd ask our listeners
[00:41:07.980 --> 00:41:14.660]   to seed it all over the world and then they would get that from... That was a funky system.
[00:41:14.660 --> 00:41:20.220]   The long comes cash fly and then I don't have to worry. So an interesting thing happening
[00:41:20.220 --> 00:41:27.700]   in Apple land and the plot has thickened. Today the New York Times, or maybe it was yesterday,
[00:41:27.700 --> 00:41:36.660]   wrote an article, Apple cracks down on apps that buy iPhone addiction. What? According
[00:41:36.660 --> 00:41:42.340]   to the New York Times, at least 11 of the 17 most downloaded screen time and parental
[00:41:42.340 --> 00:41:51.100]   control apps are removed or restricted from the iOS store. In some cases, Apple forced
[00:41:51.100 --> 00:41:55.460]   companies to remove features that allowed parents to control their children's devices
[00:41:55.460 --> 00:42:00.100]   or that block children's access to certain apps and adult content. In other cases, they
[00:42:00.100 --> 00:42:06.980]   just yanked the app. The example the Times gives is our PACT, which is the number one
[00:42:06.980 --> 00:42:11.700]   parental control iPhone app, 3 million downloads. Their CEO says they yanked us out of the blue
[00:42:11.700 --> 00:42:17.660]   with no warning in February. 80% of our PACT's revenue comes from the app store. They're
[00:42:17.660 --> 00:42:25.220]   systematically killing the industry, said Amir Masavi and the chief executive of our PACT.
[00:42:25.220 --> 00:42:31.620]   So why is Apple doing this? I think your answer to this question depends a lot on how you feel
[00:42:31.620 --> 00:42:32.620]   about Apple.
[00:42:32.620 --> 00:42:38.580]   Is it skeptical or do you write along with them?
[00:42:38.580 --> 00:42:41.940]   If you hate Apple, then you're going to say, "Oh, it's any competitive because they have
[00:42:41.940 --> 00:42:48.860]   screen time." Screen time is not as draconian as many of these apps. Apple, yes, pays lip
[00:42:48.860 --> 00:42:54.860]   service to you not using too much of your iPhone, but they don't want you to do too good a job.
[00:42:54.860 --> 00:43:00.140]   But yeah, I don't buy it because in fact, Phil Schiller had such a good response that
[00:43:00.140 --> 00:43:08.820]   I feel like, "Okay, maybe I'm on the side of Apple on this one." So Schiller emailed, he
[00:43:08.820 --> 00:43:16.780]   got many emails I think from Apple fans. Mac Rumor's reader Zachary Robinson emailed Tim
[00:43:16.780 --> 00:43:24.020]   Cook to express concern and got this response from Phil Schiller, the chief of marketing.
[00:43:24.020 --> 00:43:29.500]   So take it as you will at Apple. Thank you for being a fan of Apple on your email. I
[00:43:29.500 --> 00:43:33.860]   would like to assure you the App Store team acted extremely responsibly in this matter,
[00:43:33.860 --> 00:43:41.260]   helping to protect our children from technologies that could be used to violate their privacy
[00:43:41.260 --> 00:43:47.100]   and security. So apparently some of these apps were using something Apple calls MDM
[00:43:47.100 --> 00:43:53.780]   or mobile device management. Installing an MDM profile is a method to limit and control
[00:43:53.780 --> 00:43:58.700]   use of these devices. I'm not familiar with this. Maybe you are, Micah, because you cover
[00:43:58.700 --> 00:44:05.500]   this beat. Yeah. It's what Facebook has done in the past with they, they, we probably talked
[00:44:05.500 --> 00:44:10.060]   to it's probably been talked about on the show, but Facebook was letting these people
[00:44:10.060 --> 00:44:16.060]   install these apps and it installed a profile. And then it was tracking everything that the
[00:44:16.060 --> 00:44:20.220]   person did on the phone and then they were giving like $20 gift cards or whatever. Facebook
[00:44:20.220 --> 00:44:25.020]   got in a lot of trouble for that. This is the same thing. It installs a profile onto your
[00:44:25.020 --> 00:44:30.700]   device and that profile gives access to these apps to not only know everything that you're
[00:44:30.700 --> 00:44:37.540]   doing, but then they can be controlled that way. And I find it fascinating because I remember
[00:44:37.540 --> 00:44:43.700]   when the App Store guidelines said that you could not create an app that recreated the
[00:44:43.700 --> 00:44:48.860]   behavior of an app that Apple makes. And that's what I thought was going on.
[00:44:48.860 --> 00:44:52.020]   The argument. Yeah, that's what I thought was going on. We already do this. You can't
[00:44:52.020 --> 00:44:56.460]   do it. But remember, Apple allows other browsers. They have Safari on their iPhone.
[00:44:56.460 --> 00:45:02.420]   Other homes, kit apps, their other, yeah, Apple's own fart app that was recreated by
[00:45:02.420 --> 00:45:07.260]   other people. Yeah, it's a shame because Apple's fart app was the best, but it was the best.
[00:45:07.260 --> 00:45:12.340]   You're being silly. Yes, everyone. Don't look for Apple's fart app. It does not exist.
[00:45:12.340 --> 00:45:16.180]   Clearly, Apple enforces that rule selectively, but I thought, well, maybe that was what they
[00:45:16.180 --> 00:45:21.980]   were doing. On the other hand, MDM is not and Schiller says this is intended for enterprises.
[00:45:21.980 --> 00:45:29.140]   MDM has been an enterprise feature for as long back as I can remember. I mean, since it's
[00:45:29.140 --> 00:45:35.140]   inception. Because companies can reasonably control their employees' devices, especially
[00:45:35.140 --> 00:45:40.580]   if they give them to the employees. So when I connected to the Vatican's Exchange server,
[00:45:40.580 --> 00:45:46.380]   it installed a script, an MDM script, so that I can remotely wipe the device. Wow. And this
[00:45:46.380 --> 00:45:49.700]   goes above all the other security that I have on the phone. So no matter what they try
[00:45:49.700 --> 00:45:53.540]   to do with it, if I should lose this phone, I could log into my Exchange server and say,
[00:45:53.540 --> 00:45:56.900]   kill that device. Interesting. Interesting. Yeah. So it's an easy
[00:45:56.900 --> 00:46:02.900]   green feature, but here's, and here's where I think I actually have sympathy for Apple.
[00:46:02.900 --> 00:46:09.180]   If you have software that is using MDM, it has to be enterprise-grade security because
[00:46:09.180 --> 00:46:13.660]   what you're giving that software, the ability to do is anything, literally anything from
[00:46:13.660 --> 00:46:17.380]   the device. They point that out. Not only are these companies getting unprecedented access
[00:46:17.380 --> 00:46:23.260]   to these kids' phones, but hackers could also... Right. Because if they have not done their
[00:46:23.260 --> 00:46:27.380]   due diligence, the developer, and they have not made the software so that it's secure,
[00:46:27.380 --> 00:46:32.340]   it means that I now have another vector that I can attack someone's phone, and now I own
[00:46:32.340 --> 00:46:37.300]   anything that phone has MDM privileges over. Because when most of the software, the way
[00:46:37.300 --> 00:46:41.180]   it works is you have the parent phone and you have the child phone, and the child phone
[00:46:41.180 --> 00:46:47.260]   is slave to the parent phone. So if I own the parent phone, I now own any phone that
[00:46:47.260 --> 00:46:54.820]   phone has MDM access to. So is the times at fault for writing that...
[00:46:54.820 --> 00:47:01.060]   They did contact Apple and they got a very anodyne response from an Apple spokesperson.
[00:47:01.060 --> 00:47:05.460]   We treat all apps the same, including those that compete with our own services. Our incentives
[00:47:05.460 --> 00:47:10.460]   to have a vibrant app ecosystem didn't mention anything about MDM. That seems like a fumble
[00:47:10.460 --> 00:47:16.940]   on Apple's part, but maybe the times should have pushed a little harder to get something
[00:47:16.940 --> 00:47:20.220]   a little less namby-pamby.
[00:47:20.220 --> 00:47:24.060]   It sounds a bit like they had a sensational story because it sounded like Apple was doing
[00:47:24.060 --> 00:47:29.860]   a very Apple thing again, whereas I think a lot of security and researchers would say,
[00:47:29.860 --> 00:47:31.980]   maybe this isn't a feature you want out in the open.
[00:47:31.980 --> 00:47:38.740]   If Schiller is accurate and they were using MDM, Apple has an obligation to do something
[00:47:38.740 --> 00:47:44.820]   about it, as they did with the Facebook app.
[00:47:44.820 --> 00:47:50.460]   So anyway, I thought you'd want to get both sides of the story because I knew I was going
[00:47:50.460 --> 00:47:56.580]   to talk about the time story and then I saw Phil Schiller's response. And I have to say,
[00:47:56.580 --> 00:47:57.580]   I think maybe...
[00:47:57.580 --> 00:48:05.700]   I think there's something to be said too for the discussion of... I think it was a Black
[00:48:05.700 --> 00:48:11.500]   Mirror episode. And the Black Mirror episode was about installing this little thing into
[00:48:11.500 --> 00:48:15.940]   a child's head and then you could filter out things that scared them and then the parent
[00:48:15.940 --> 00:48:23.180]   could watch over their iPad device and filter things out and see things. And the mom put
[00:48:23.180 --> 00:48:29.460]   it away, but then she ended up seeing it and interacted with her child's life in a very
[00:48:29.460 --> 00:48:32.540]   controlling way when the child was older and was able to make responsible decisions on
[00:48:32.540 --> 00:48:39.540]   their own. And I think that there's not an insignificant amount of discussion to have
[00:48:39.540 --> 00:48:46.860]   about the sort of surveillance state parenting that can sometimes come with technology.
[00:48:46.860 --> 00:48:52.940]   And mind you, I say this is a person who does not have children and it's a whole different
[00:48:52.940 --> 00:48:53.940]   aspect of computing.
[00:48:53.940 --> 00:48:57.940]   You're actually closer and aged to those kids that are getting monitored than you are
[00:48:57.940 --> 00:49:00.820]   for the parents of those kids that are getting monitored.
[00:49:00.820 --> 00:49:05.460]   So you're kind of like, "Free the teenager man."
[00:49:05.460 --> 00:49:06.460]   And I understand that.
[00:49:06.460 --> 00:49:07.460]   I'm just...
[00:49:07.460 --> 00:49:10.660]   As the only one with parents on this entire... with the children on this entire panel, you
[00:49:10.660 --> 00:49:14.860]   all have parents I presume. As the only one with children on this entire panel, I will
[00:49:14.860 --> 00:49:21.500]   say I never second guess a parent for doing whatever it is they feel like they need to
[00:49:21.500 --> 00:49:29.260]   do to responsibly raise their children. But I do agree with you 100% Micah. Sometimes
[00:49:29.260 --> 00:49:35.060]   the tighter you grip, the more they want to slip through your grip. And it's not necessarily
[00:49:35.060 --> 00:49:38.700]   the best way to handle this. It's a tough thing. I'd hate to be raising kids in this
[00:49:38.700 --> 00:49:42.420]   era. My kids now are adults. Thank goodness.
[00:49:42.420 --> 00:49:47.660]   Because I feel bad for kids who have access to any kind of awful stuff.
[00:49:47.660 --> 00:49:48.660]   Yeah.
[00:49:48.660 --> 00:49:53.700]   And I don't... I think parents that want to find a way to protect them, I understand.
[00:49:53.700 --> 00:49:54.700]   I really do.
[00:49:54.700 --> 00:50:00.580]   Let me go one step further in defense of Apple, which you will not normally find me in.
[00:50:00.580 --> 00:50:07.860]   But get rid of all the MDM talk, get rid of all the child parenting app talk and say,
[00:50:07.860 --> 00:50:11.820]   would you want Apple to get rid of poorly written back doors?
[00:50:11.820 --> 00:50:12.820]   Yes.
[00:50:12.820 --> 00:50:14.500]   And answer absolutely yes. I would just say yes.
[00:50:14.500 --> 00:50:15.500]   No question.
[00:50:15.500 --> 00:50:20.260]   That's essentially what they did. They saw this as really poorly written back doors to
[00:50:20.260 --> 00:50:22.260]   children's iPhones.
[00:50:22.260 --> 00:50:27.500]   And between the two, between having a device that may be kind of a tech savvy person might
[00:50:27.500 --> 00:50:31.580]   be able to know what their child is using or having a device that I know is locked down
[00:50:31.580 --> 00:50:34.700]   as much as possible so that it doesn't get infected. So really bad people don't have
[00:50:34.700 --> 00:50:40.620]   access to it. I'm always going to choose to get rid of that back door. Right? I mean,
[00:50:40.620 --> 00:50:42.300]   that should be a no brainer.
[00:50:42.300 --> 00:50:48.580]   I think both Paris and Micah are of a generation where you kind of... you were in high school
[00:50:48.580 --> 00:50:52.340]   certainly when you had internet when you were in high school, right?
[00:50:52.340 --> 00:50:53.340]   For you. Yeah.
[00:50:53.340 --> 00:50:58.620]   And even before then, what did your parents do?
[00:50:58.620 --> 00:50:59.620]   Paris you first.
[00:50:59.620 --> 00:51:07.100]   My parents really had no idea how to use the internet in the same way that I did. I remember
[00:51:07.100 --> 00:51:14.500]   having a argument with my parents for many years in high school that they thought Google
[00:51:14.500 --> 00:51:19.780]   Chrome was a virus and I was trying to face them that it was not. And I still bring this
[00:51:19.780 --> 00:51:22.900]   up to this day and they're like, "Oh no, we definitely didn't say that." I'm like,
[00:51:22.900 --> 00:51:27.540]   "I printed out sheets of research showing Google Chrome's out of virus." But I mean,
[00:51:27.540 --> 00:51:31.500]   I think it was very different in the sense that I was always the one who was fixing the
[00:51:31.500 --> 00:51:34.700]   tech problems as a kid and they were not.
[00:51:34.700 --> 00:51:37.460]   So they didn't have a chance. That's all you're saying, Paris.
[00:51:37.460 --> 00:51:42.300]   No, it was a chance to police my internet usage. But I don't know. Now I'm increasingly
[00:51:42.300 --> 00:51:47.380]   concerned as more and more tech companies roll out products specifically designed for
[00:51:47.380 --> 00:51:52.500]   kids. Which I guess is good. It's in there are more safety measures, but it's also like
[00:51:52.500 --> 00:51:57.500]   you're feeding children into this kind of content cycle. The first thing that kind of
[00:51:57.500 --> 00:52:02.420]   comes to mind is Caroline Haskins, a reporter at Motherboard, I think I tweeted this out
[00:52:02.420 --> 00:52:08.540]   the other day, but she was looking at the permissions on YouTube kids and to sign up,
[00:52:08.540 --> 00:52:12.220]   you have to enter in the child. The child has to enter in their age. And the age limit is
[00:52:12.220 --> 00:52:17.140]   from like, you could enter in the age of zero to the age of like 13. I'm like, it is
[00:52:17.140 --> 00:52:22.420]   insane. You could make an account for a zero year old child, hypothetically, in this world
[00:52:22.420 --> 00:52:28.620]   that, I don't know, Google is kind of setting up these sort of walled gardens for children
[00:52:28.620 --> 00:52:33.780]   of any conceivable age to consume content in quite a lot.
[00:52:33.780 --> 00:52:38.420]   I think this is probably just lazy programming on the part of whoever, you know, whatever
[00:52:38.420 --> 00:52:48.700]   code monkey had to write this login page, he said, I will start at zero. It is pretty
[00:52:48.700 --> 00:52:54.380]   funny. I would think it should start at 13, right? Because of COPPA, but maybe not.
[00:52:54.380 --> 00:53:00.460]   No, because I mean, I think that's the thing is that a YouTube kids and Facebook kids or
[00:53:00.460 --> 00:53:05.660]   a messenger kids exist kind of to specifically capture the under 13.
[00:53:05.660 --> 00:53:10.620]   You're right. They want, they want two year olds. They want one year olds. They want zero
[00:53:10.620 --> 00:53:16.300]   year olds. They do. You want to put a tablet in front of your newborn.
[00:53:16.300 --> 00:53:21.220]   Actually that's absolutely true. And I just saw a study from pediatricians who said, under
[00:53:21.220 --> 00:53:27.420]   no circumstances, should a child in the age of one be looking at any screen of any kind.
[00:53:27.420 --> 00:53:32.660]   So it was a lot easier when I was a child to do responsible parenthood because any mature
[00:53:32.660 --> 00:53:36.940]   content that you would download over 14 for modem would take like an hour and a half.
[00:53:36.940 --> 00:53:40.980]   So while my parents had to do was pick up the phone every 45 minutes, would reset my
[00:53:40.980 --> 00:53:48.260]   connection and have to start over. So there you go. Good parenting.
[00:53:48.260 --> 00:53:52.820]   When I was a kid, I just said go out and play with the rocks. We're not it's not time for
[00:53:52.820 --> 00:53:53.820]   dinner yet.
[00:53:53.820 --> 00:53:59.660]   Did you have that? I'm on the phone. I do remember that. But that was an adult. I get
[00:53:59.660 --> 00:54:06.340]   on copuser playing Dungeons, you know, the colossal cave and my wife would pick up the
[00:54:06.340 --> 00:54:17.460]   phone. Oh, that negotiated horribly. Mike, like what did your folks do?
[00:54:17.460 --> 00:54:21.780]   I was going to say that was certainly my experience in elementary school was the me wanting to
[00:54:21.780 --> 00:54:27.660]   be on the internet to most of the time it was like upload little stop motion videos that
[00:54:27.660 --> 00:54:32.540]   I had made this horrible crappy webcam that I had. And I would just take a photo with
[00:54:32.540 --> 00:54:39.700]   webcam. Wait, wait, was it a Logitech quick cam? It was circular. Yes. Yeah. It had a
[00:54:39.700 --> 00:54:45.900]   focus ring. Did you hook it up to the parallel port of your computer? Yes. Yes. That was it.
[00:54:45.900 --> 00:54:51.620]   They had that on TV. They never worked. That's what I was rocking. And I see it wasn't
[00:54:51.620 --> 00:54:56.260]   doing any video. It was just single frame. So maybe that's why mine worked so well. But
[00:54:56.260 --> 00:54:59.980]   I was just going to say my experience was mirrored with Paris in terms of high school
[00:54:59.980 --> 00:55:06.220]   or middle school on is that they would come to me for the tech problems. And so I could
[00:55:06.220 --> 00:55:10.540]   I could have and did do, you know, a bunch of things online that they would know the
[00:55:10.540 --> 00:55:17.980]   first thing about at the time. And so now I think about folks growing up. I mean, literally
[00:55:17.980 --> 00:55:23.740]   like, I'm trying to think of sort of brands that I love and there aren't many brands that
[00:55:23.740 --> 00:55:31.260]   I love. And I think about the access that folks have now to these things that play such
[00:55:31.260 --> 00:55:35.420]   a huge role in their life. And so if you are zero and you grow up watching YouTube kids
[00:55:35.420 --> 00:55:42.540]   and you you tie that like dopamine cycle of watching a show and, you know, falling in
[00:55:42.540 --> 00:55:48.820]   love with the little things that dance around on the screen, you get the cycle built in.
[00:55:48.820 --> 00:55:54.380]   You know that comes from YouTube. You're 13 and you get to graduate to regular YouTube.
[00:55:54.380 --> 00:55:58.260]   You suddenly into Minecraft and all these other things. You're never going to not be
[00:55:58.260 --> 00:56:01.420]   on YouTube. You're never going to not be on Netflix. You're never going to. You know,
[00:56:01.420 --> 00:56:07.380]   I mean, these these services, they're so our connection with everything is so much now
[00:56:07.380 --> 00:56:14.020]   that it's you almost don't need like advertising because these things are are again, building
[00:56:14.020 --> 00:56:19.140]   themselves into I think our psychological reward centers now. And so, you know, you
[00:56:19.140 --> 00:56:23.380]   got to have your YouTube, you got to have these things and they make you feel good.
[00:56:23.380 --> 00:56:26.740]   And that's what's horrifying to me is that I can't think growing up like what that would
[00:56:26.740 --> 00:56:33.780]   have been. You know, as I'm trying to think of it, I guess like Gerber baby food, what
[00:56:33.780 --> 00:56:37.620]   it made me happy, but like then I stopped eating it. So there's not really a brand in
[00:56:37.620 --> 00:56:42.540]   that way. But now you start from zero with your Samsung phone or our official, your Apple
[00:56:42.540 --> 00:56:46.580]   camp, then you're going to keep going to those products over and over again or those services
[00:56:46.580 --> 00:56:47.580]   online.
[00:56:47.580 --> 00:56:53.860]   I wonder. I mean, we are going to have in a few years a generation that grew up watching
[00:56:53.860 --> 00:56:54.860]   Minecraft videos on YouTube.
[00:56:54.860 --> 00:57:00.740]   In a couple of years, well, the generation, they'll think Minecraft is passing. Well,
[00:57:00.740 --> 00:57:05.460]   that's the good news is everything there. You know, there's a time for things where out,
[00:57:05.460 --> 00:57:13.300]   but still, I wonder. I mean, honestly, is that going to be problematic or yes? You think
[00:57:13.300 --> 00:57:18.420]   so? Absolutely. Or maybe it's just we don't we're old and we don't get the new the kids
[00:57:18.420 --> 00:57:23.340]   today. Well, no, because I'm a I'm the oldest of minecraft and it's not just YouTube and
[00:57:23.340 --> 00:57:28.180]   it's not just whatever is the latest and the greatest unboxing video. It's the fact that
[00:57:28.180 --> 00:57:33.980]   when we talk about the always connected generation, it's no longer a figure of speech. I have
[00:57:33.980 --> 00:57:38.620]   watched kids fall asleep with their phones on their chest. Oh, yeah, that's pretty common.
[00:57:38.620 --> 00:57:42.780]   And then as they're waking up before they're even really conscious, the first thing they
[00:57:42.780 --> 00:57:46.580]   do is they grab their phone again. And I'm thinking, as you have a problem with that,
[00:57:46.580 --> 00:57:49.940]   that's a major problem. I do that every day. I know, but that's a problem. That's how I
[00:57:49.940 --> 00:57:54.780]   turn off my alarm. Yeah, exactly. That affect your sleep cycles. Oh, God, you should see
[00:57:54.780 --> 00:58:00.180]   my sleep cycle. The sleep cycle you track in your phone. Yeah. Well, or, you know, matter
[00:58:00.180 --> 00:58:05.820]   fact, yes, at least I've grown enough to know that my Samsung phone every morning sends me
[00:58:05.820 --> 00:58:10.420]   a notification saying, did you go to sleep at 830 and wake up at 530? Because that's when
[00:58:10.420 --> 00:58:16.500]   we saw you last. No, no, no, it is. It's accurate. It knows somehow exactly when I went to sleep.
[00:58:16.500 --> 00:58:20.500]   I guess because that's when the phone fell on my chest. The only thing that I've done
[00:58:20.500 --> 00:58:25.460]   that's mature in the last year is if I wake up at five o'clock in the morning, really
[00:58:25.460 --> 00:58:30.300]   quick because I have to do something, I don't reach for my phone because if I know, if I
[00:58:30.300 --> 00:58:33.420]   reach for my phone thinking, Oh, let me just check what email I have and I'll go back to
[00:58:33.420 --> 00:58:38.300]   bed in five minutes, I'll be up for the next three hours. But you know, youngsters, young
[00:58:38.300 --> 00:58:42.220]   adults, they don't have that yet. It's once they start looking at the screen, they're
[00:58:42.220 --> 00:58:45.540]   looking at it until they fall asleep. A lot of parents that I know actually will take
[00:58:45.540 --> 00:58:50.420]   away the devices and put them somewhere, not in the bedroom otherwise. Jason and Meg
[00:58:50.420 --> 00:58:56.340]   can do that. They have like a device thing. Carson, you do that too with Zach? Yeah. And
[00:58:56.340 --> 00:59:03.140]   Alex, I think that's probably what I would do if I had kids today. I feel cruel though.
[00:59:03.140 --> 00:59:09.140]   And I again, I want to just emphasize, I worry that it's just not us not understanding the
[00:59:09.140 --> 00:59:14.340]   culture that those kids are growing up in. So I mean, is there anything wrong with them
[00:59:14.340 --> 00:59:19.020]   growing up watching Minecraft videos on YouTube or playing Fortnite to the wee hours of the
[00:59:19.020 --> 00:59:23.220]   morning or Instagramming or any of this stuff? This is their generation with, isn't this
[00:59:23.220 --> 00:59:29.900]   what it's going to be like in their adult life too? Yeah, I mean, I think so. And yeah,
[00:59:29.900 --> 00:59:33.900]   this kind of bumps up against something that I think about a lot, which is my oldest gripe
[00:59:33.900 --> 00:59:40.700]   when it comes to like tech news coverage is the genre that is like, the teens are using
[00:59:40.700 --> 00:59:48.140]   popular social media service X to do thing. Why that? Oh my God. No, but I mean, like
[00:59:48.140 --> 00:59:55.820]   the teens are talking about politics on Instagram. The teens use YouTube to express social behavior
[00:59:55.820 --> 01:00:00.860]   is I'm like, okay, yes. Yes. People who grew up online are going to have social behaviors
[01:00:00.860 --> 01:00:08.260]   online that are the same as offline. Who the hell cares? Except this, except this. It's
[01:00:08.260 --> 01:00:11.460]   not just a means of communication because if it was just a means of communication, we
[01:00:11.460 --> 01:00:16.260]   could look at it like any other revolution, like the industrial revolution. It hasn't
[01:00:16.260 --> 01:00:21.340]   just changed how people get information. It's changed how people relate to one another.
[01:00:21.340 --> 01:00:26.020]   And that is incredibly difficult to quantify what that changes because now that people
[01:00:26.020 --> 01:00:31.180]   are accustomed to having relationships with people that they've never met in person,
[01:00:31.180 --> 01:00:35.900]   that changes what kind of dynamic you have and what kind of interpersonal development
[01:00:35.900 --> 01:00:41.380]   you have. If I can grow up to the age of 40 without ever really having to deal with
[01:00:41.380 --> 01:00:45.980]   another human being, well, you have to deal with other human beings. You go to school,
[01:00:45.980 --> 01:00:50.060]   you go to church, you go places, you buy groceries. It's not like they're never doing
[01:00:50.060 --> 01:00:54.780]   that. It's actually, I would submit it's expanded their horizons. They're getting to
[01:00:54.780 --> 01:00:58.020]   know people from all over the world. They're, they're getting.
[01:00:58.020 --> 01:00:59.660]   Yeah, without having to be physically next.
[01:00:59.660 --> 01:01:04.220]   Yes, I would say it's not, it's not either or I would hope that kids are getting up and
[01:01:04.220 --> 01:01:09.580]   going somewhere once in a while. But, you know, if you replace all social interaction
[01:01:09.580 --> 01:01:13.780]   with online social interaction, clearly you're going to be stunted. But I don't think that's
[01:01:13.780 --> 01:01:17.780]   the case for the vast majority of people. They're, they're getting both and so in a
[01:01:17.780 --> 01:01:20.020]   way they've broadened their horizons. Don't you think?
[01:01:20.020 --> 01:01:25.020]   Yes, and that's why, I mean, I'm not a Luddite. On Twitter, I posted a while back, there
[01:01:25.020 --> 01:01:29.220]   was a, there was a Christian group that was doing a huge meeting and they wanted me to
[01:01:29.220 --> 01:01:31.260]   keynote and I said, oh sure, I love keynote.
[01:01:31.260 --> 01:01:32.260]   I'll Skype in.
[01:01:32.260 --> 01:01:37.340]   Well, no, they wanted to fly me back and I do my, my schfield and we're setting up the
[01:01:37.340 --> 01:01:42.340]   terms and they say, oh, what we really want is we want a keynote on, on why the technology
[01:01:42.340 --> 01:01:44.580]   is bad and evil. The devil's technology.
[01:01:44.580 --> 01:01:51.300]   I haven't looked at my bio, but okay. And I get it. They're scared because they see
[01:01:51.300 --> 01:01:55.700]   this as an addiction and it absolutely is an addiction if you let it be an addiction.
[01:01:55.700 --> 01:02:00.500]   I think they're scared as all parents have been some time immemorial because it's something
[01:02:00.500 --> 01:02:06.580]   new. Turn off that rock and me on music. Stop watching Howdy Doody, whatever it is. It's
[01:02:06.580 --> 01:02:11.260]   something new. You can literally go back and Jeff Jarvis will bring this up if you ask
[01:02:11.260 --> 01:02:16.940]   him at the drop of a hat to the days of the, when people's, the printing press came out,
[01:02:16.940 --> 01:02:20.740]   people started getting, being able to afford books and they started reading books. People
[01:02:20.740 --> 01:02:25.380]   were writing screeds against reading like they're not experiencing the world. They're
[01:02:25.380 --> 01:02:30.620]   reading. It's on the page. One of their likes. It's the same thing. And so I, my only fear
[01:02:30.620 --> 01:02:36.100]   is that we don't understand it. And so we condemn it just as you were saying. And maybe
[01:02:36.100 --> 01:02:42.380]   it's not so bad. I don't know. I mean, we're running a big experiment. That's for you.
[01:02:42.380 --> 01:02:46.840]   Keep happening. Yeah. Paris, go ahead.
[01:02:46.840 --> 01:02:51.620]   Oh, I was saying, I think the issues that we have with technology are not technology
[01:02:51.620 --> 01:02:57.060]   replacing online interaction. It is that when interacting in the same way as you would
[01:02:57.060 --> 01:03:05.820]   offline, online, there are extra consequences and ramifications that are powered by algorithms
[01:03:05.820 --> 01:03:10.500]   and increased polarization and a variety of other effects that you have to watch out
[01:03:10.500 --> 01:03:14.180]   for. But it's not just the use of technology in this case is bad.
[01:03:14.180 --> 01:03:18.900]   You can't make it blanket. And yet, I also understand the fear that we are running a
[01:03:18.900 --> 01:03:24.660]   mass experiment on our children. And we don't know how it's going to turn out. And we won't
[01:03:24.660 --> 01:03:29.300]   find out for another 15 years. And when we do, we may not like the results we get. I
[01:03:29.300 --> 01:03:35.220]   don't know. We did this large symposium of educators, high school, higher education,
[01:03:35.220 --> 01:03:38.780]   university, post grad, et cetera, et cetera, hundreds of teachers, professionals from around
[01:03:38.780 --> 01:03:45.380]   the world. And one of the common themes was how education has changed. It used to be education
[01:03:45.380 --> 01:03:50.060]   was about teaching you how to get the information that you wanted to have. That's now incredibly
[01:03:50.060 --> 01:03:52.060]   easy. How to use the library and the card.
[01:03:52.060 --> 01:03:53.700]   For a size. Yeah.
[01:03:53.700 --> 01:03:58.340]   And if education wants to stay relevant, what it has to do, it has to change from how
[01:03:58.340 --> 01:04:02.140]   to get you the information to how do I critique the information? How do I know a good source
[01:04:02.140 --> 01:04:04.860]   from a bad source? How do I synthesize new ideas?
[01:04:04.860 --> 01:04:10.500]   The illiteracy, I think is going to be right now, there are certain classes, depending
[01:04:10.500 --> 01:04:14.940]   on where you are, where the curriculum decides sort of you have to take this class in order
[01:04:14.940 --> 01:04:18.500]   to graduate. And I think media literacy is going to be such an important thing. And
[01:04:18.500 --> 01:04:23.580]   not just media literacy, but tech literacy, the whole kit and kaboodle.
[01:04:23.580 --> 01:04:31.260]   I do hope, and maybe it's a futile hope, but I do hope that this, the whole fake news cycle,
[01:04:31.260 --> 01:04:36.780]   the whole 2016 election influenced by the Russians. All of this, that there will be
[01:04:36.780 --> 01:04:43.260]   a reaction to this that ends up the right response is not let's shut down Facebook or
[01:04:43.260 --> 01:04:49.580]   let's find ways to block fake news. The right response is to teach kids literacy, to teach
[01:04:49.580 --> 01:04:52.660]   them how to understand what they're seeing and to teach them that there are people trying
[01:04:52.660 --> 01:04:57.100]   to influence you. And so you should be, you should think about what you're receiving,
[01:04:57.100 --> 01:05:03.580]   who's writing it and why, and what their purpose is. And if people can understand that deeply,
[01:05:03.580 --> 01:05:06.740]   that's going to be more valuable than shutting down Facebook.
[01:05:06.740 --> 01:05:07.740]   Oh, yes.
[01:05:07.740 --> 01:05:08.740]   I hope.
[01:05:08.740 --> 01:05:13.580]   Yeah. And I think that the, I mean, the prime example of that is that if there have been
[01:05:13.580 --> 01:05:16.780]   blanking on the names right now, but I know there have been certain studies where if you
[01:05:16.780 --> 01:05:22.620]   look at young adults or kids versus, I don't know, boomers or people who have less media
[01:05:22.620 --> 01:05:29.540]   literacy or time online, people perhaps would say in like the 50 or 60 plus era, more often
[01:05:29.540 --> 01:05:33.940]   succumb to fake news or polluted information than people who've grown up online.
[01:05:33.940 --> 01:05:38.780]   It is. It's the baby boomers that is sharing this crap. Not the teenagers. They know it's
[01:05:38.780 --> 01:05:39.780]   crap.
[01:05:39.780 --> 01:05:40.780]   They aren't on Facebook.
[01:05:40.780 --> 01:05:41.780]   Right. They're not even.
[01:05:41.780 --> 01:05:43.220]   You're following the meme account on Instagram.
[01:05:43.220 --> 01:05:50.260]   Right. No, that's what I read the same study. It said boomers share false stories eight times
[01:05:50.260 --> 01:05:57.900]   more than people under 25. That it's my generation that's being suckered by all this stuff.
[01:05:57.900 --> 01:06:01.820]   And there was that incredibly powerful Washington Post piece that I guess in blank in the name
[01:06:01.820 --> 01:06:07.580]   of right now where they followed some people, I think they're in their 70s, something that
[01:06:07.580 --> 01:06:13.140]   shared posts from this one definitively fake news site that was, I guess, right leaning,
[01:06:13.140 --> 01:06:18.300]   but it was actually run by kind of a liberal, trollish person who was just creating satirical
[01:06:18.300 --> 01:06:23.300]   news. It was a right leaning bent kind of it was supposed to be as a joke, but even after
[01:06:23.300 --> 01:06:26.700]   they told the people who were sharing this news, Hey, this is false. It was made by this
[01:06:26.700 --> 01:06:30.660]   person who's doing it as a joke. If you look on the site, it literally says this is a joke.
[01:06:30.660 --> 01:06:31.660]   Says it.
[01:06:31.660 --> 01:06:36.860]   They shared it anyway because they it's still conformed with their beliefs. And I think
[01:06:36.860 --> 01:06:41.020]   that's why this issue is so much larger than just media literacy. It is also about looking
[01:06:41.020 --> 01:06:47.660]   at these sort of societal issues that we have in confronting these like internal and
[01:06:47.660 --> 01:06:53.900]   external beliefs that are held. And in addition to that also confronting the platform issue
[01:06:53.900 --> 01:06:57.140]   of all this in regards to polarization.
[01:06:57.140 --> 01:07:01.380]   This is the business insider article from earlier this year. Baby Boomer share nearly
[01:07:01.380 --> 01:07:07.500]   seven times as many fake news articles on Facebook as adults under 30. So maybe there's
[01:07:07.500 --> 01:07:13.140]   maybe there's hope. Maybe there's hope for the future. And I think Paris, I have to say
[01:07:13.140 --> 01:07:18.820]   you're a poster child for parents not knowing anything about technology and letting the
[01:07:18.820 --> 01:07:25.540]   kid get into it. I mean, you turn into and I say this in the most positive way of geek.
[01:07:25.540 --> 01:07:28.060]   Right? Yeah, certainly.
[01:07:28.060 --> 01:07:30.980]   And that's a good thing.
[01:07:30.980 --> 01:07:35.300]   And love my parents to death and they'll never read or hear this, which is why I will say
[01:07:35.300 --> 01:07:41.940]   it. But they are definitely the sect of adults that fall for fake news at all times. And
[01:07:41.940 --> 01:07:46.020]   that is the only thing they share and it has changed their beliefs radically. But I think
[01:07:46.020 --> 01:07:52.460]   because of that, me and at least my sister who's still in the Gen Z kind of stage of
[01:07:52.460 --> 01:08:01.220]   being obsessed with being online, it has resulted in us adopting technologically driven strategies
[01:08:01.220 --> 01:08:03.300]   to make sure all our information is correct.
[01:08:03.300 --> 01:08:08.380]   That's really encouraging. I think that's really great. And honestly, it's easy it is
[01:08:08.380 --> 01:08:13.300]   for us baby boomers to dis millennials. I think actually we're you guys are going to
[01:08:13.300 --> 01:08:16.940]   do a better job than we did. I hope anyway, it's up to you now.
[01:08:16.940 --> 01:08:21.660]   Well, we're in that transition time, right? Because I mean, when I was growing up, the
[01:08:21.660 --> 01:08:26.060]   whole idea was two primary sources for anything that you wanted to say was true, you found
[01:08:26.060 --> 01:08:31.180]   two primary sources and then a couple of facts to back it up. Well, it's really easy to find
[01:08:31.180 --> 01:08:37.060]   two primary sources now that are both fake or were both built off of the same fake kernel.
[01:08:37.060 --> 01:08:40.780]   So what do I what do I tell a student now? Because I'm actually still dealing with this.
[01:08:40.780 --> 01:08:44.140]   This is really interesting because Jesuits of course have a long history of education
[01:08:44.140 --> 01:08:52.740]   and of really hard headed education of really being demanding and rigorous in their requirements
[01:08:52.740 --> 01:08:58.540]   for their students. I mean, this is and going back to many, many centuries of this. Yes.
[01:08:58.540 --> 01:09:03.580]   John McLaughlin of the McLaughlin group, famous Jesuit, a good friend of mine was educated
[01:09:03.580 --> 01:09:12.820]   by him and he was absolutely strict in his requirements as he should be. And I so I
[01:09:12.820 --> 01:09:18.180]   wonder, do you see any change in Jesuit doctrine and education in the internet era?
[01:09:18.180 --> 01:09:23.740]   Yes. And that's that was that the reason why we had that big symposium, which was they
[01:09:23.740 --> 01:09:28.740]   said, look, the universities need to change their focus. We've been focused for so long
[01:09:28.740 --> 01:09:32.860]   and pumping up PhDs. We just want to get more people with more doctors who have published
[01:09:32.860 --> 01:09:36.100]   more things that will never be read because they were in a journal somewhere on a dusty
[01:09:36.100 --> 01:09:40.220]   shelf. And we actually have to make them useful again. And one of the ways that people said,
[01:09:40.220 --> 01:09:46.140]   look, we can make it useful if we actually start teaching people how to critique the
[01:09:46.140 --> 01:09:51.260]   information that they're receiving. So the the years of education where it was just about
[01:09:51.260 --> 01:09:56.420]   dumping as much info into someone's brain as possible are over. Now it's all about,
[01:09:56.420 --> 01:10:02.820]   I need to I need to make you a critical consumer of information. And that's really very different.
[01:10:02.820 --> 01:10:06.820]   It's so amazing that this organization went in with the Jesuits founded 1600s.
[01:10:06.820 --> 01:10:14.860]   Oh, well, we go back a little earlier, but we were kind of extinct in 1773.
[01:10:14.860 --> 01:10:19.420]   Okay. And then came back. And we came back. Okay. But I just think it's amazing that this
[01:10:19.420 --> 01:10:24.420]   organization has been around for so long is actually the rigorous intellectual branch
[01:10:24.420 --> 01:10:28.100]   of the Catholic churches in many, many ways. And it's great to hear,
[01:10:28.100 --> 01:10:32.620]   which gets us in trouble, by the way, a lot, a lot. It's great to hear that you that they're
[01:10:32.620 --> 01:10:36.060]   forward thinking enough to say, yeah, we've got to rethink how they're not hidebound.
[01:10:36.060 --> 01:10:40.900]   They're not saying, well, it was good enough in 1922. It's good enough today. No, we got
[01:10:40.900 --> 01:10:45.860]   to rethink how we do this stuff. And I think we absolutely, absolutely do. Let's take a
[01:10:45.860 --> 01:10:49.980]   little break. Lots more to talk about. What a panel. I love it when Father Roberts in town.
[01:10:49.980 --> 01:10:55.900]   We're glad to get you. I hope we can keep you. Just, you know, okay, can I can you tell
[01:10:55.900 --> 01:11:01.340]   this on the air? This story that you told me? Oh, which one about the basement? Can you
[01:11:01.340 --> 01:11:05.980]   say that on the air? I can say this. Right before I left, there was a group of us who
[01:11:05.980 --> 01:11:12.220]   were playing a round of Dungeons and Dragons in a crypt underneath the Vatican as you do
[01:11:12.220 --> 01:11:15.900]   in Rome, because it's, it sets the ambiance. Dungeons and Dragons. I just want to say that
[01:11:15.900 --> 01:11:21.820]   again. Dungeons and Dragons in a crypt beneath the Vatican. Yes. And someone, a very, very
[01:11:21.820 --> 01:11:27.060]   nice man who wears an awesome hat came in. And he wanted to know what we were doing.
[01:11:27.060 --> 01:11:33.700]   We had to explain. Well, this is Dungeons and Dragons. And the game master explains the
[01:11:33.700 --> 01:11:40.220]   scenario and says, would you like to play? And he says, I do not think that I would.
[01:11:40.220 --> 01:11:47.180]   And that was the, and then he walked out. Oh, what a story. And the people who know me,
[01:11:47.180 --> 01:11:53.980]   they know who I'm talking about. Oh, it's awesome. Paris Marten, from Wired, great to
[01:11:53.980 --> 01:12:01.180]   have you and our resident millennial. We love to have you to Michael Sergeant Chihuahua.coffier.
[01:12:01.180 --> 01:12:07.980]   Are you excited about Harry Potter Wizard's unite? Are you going to play it? Oh, I hear
[01:12:07.980 --> 01:12:17.740]   you. Ah, that's not the answer. Go all of those. They're fun. You don't do those. It's,
[01:12:17.740 --> 01:12:24.340]   it's just, it's, it's not for me, but I'm excited to see Renee get excited about it.
[01:12:24.340 --> 01:12:29.580]   I am really excited. Renee Richie. It's, it's rolled out now. It's the, it's based on,
[01:12:29.580 --> 01:12:33.100]   so it all started with Ingress, which you were the first father, Robert, to ever play
[01:12:33.100 --> 01:12:38.420]   around here. I still play. You were really into it. And then Ingress from Niantic, which
[01:12:38.420 --> 01:12:43.980]   was a Google company, spawned Pokemon Go, which became a huge phenomenon in 2016, July
[01:12:43.980 --> 01:12:49.220]   of 2016. Did you still play? I still play it. Nice. As do I. I'm, I'm, I'm going to keep
[01:12:49.220 --> 01:12:53.660]   plant until I get to level 40. I still have a one and a half million points to go. I have
[01:12:53.660 --> 01:12:59.780]   a ballissee that's still guarding. I love, you know, I still have a slacking that is stuck
[01:12:59.780 --> 01:13:06.340]   in a gym in Kauai. I put this gym in the national tropical botanical gardens. Apparently nobody
[01:13:06.340 --> 01:13:10.180]   plays Pokemon Go there. It's been there for weeks. I'm never getting my slacking back,
[01:13:10.180 --> 01:13:15.460]   but that's another story. And then Warner Brothers, which who owns the right to the Harry Potter
[01:13:15.460 --> 01:13:20.020]   movies, and I guess the right to do Harry Potter games is partnering with Niantic to
[01:13:20.020 --> 01:13:26.180]   do what, if you thought Pokemon Go was a phenomenon, I personally feel that when Wizards Unite,
[01:13:26.180 --> 01:13:30.580]   which is the Harry Potter game comes out and I think it's imminent, it's going to dwarf
[01:13:30.580 --> 01:13:35.780]   as long as they give me a separate wand. So I don't have to wave my phone. If I can cast
[01:13:35.780 --> 01:13:42.500]   there is a one and you have the same spells. Flippendo. Totally sold. Expecto Patronum.
[01:13:42.500 --> 01:13:45.580]   I will be running around the Vatican with a wand. So fun. I'm sure that's going to turn
[01:13:45.580 --> 01:13:50.780]   out really nice is they've piggybacked on each game because all in the ingress stops
[01:13:50.780 --> 01:13:54.820]   and spots were what was used in Pokemon Go. They're going to be reused in Harry Potter.
[01:13:54.820 --> 01:13:58.780]   So of course all the same places that were ingress. What do they call them in ingress?
[01:13:58.780 --> 01:14:03.980]   They're portals in ingress. They're they're what a poky stops in Pokemon because yeah,
[01:14:03.980 --> 01:14:06.900]   once they got that data set, why wouldn't they reuse? No, no, it's great. And it's based
[01:14:06.900 --> 01:14:10.620]   on real maps. So it's the whole idea is you walk around, you have to walk around and
[01:14:10.620 --> 01:14:15.820]   visit places to play the game, which is they billions of miles. People have walked playing
[01:14:15.820 --> 01:14:19.580]   Pokemon Go over the last. I'm going to ask them if we can do something special for St.
[01:14:19.580 --> 01:14:24.180]   Peters because that would be awesome. But now there are some priests who think Harry
[01:14:24.180 --> 01:14:28.620]   Potter is satanic. Yes, they burned those books. Yes. And then you heard what I did. No,
[01:14:28.620 --> 01:14:34.700]   I sent them e-copies of Harry Potter books. You can't burn these baby. Burn them on a
[01:14:34.700 --> 01:14:41.540]   desk. I had a coupon so they got the whole series. That's I want to know if they burnt
[01:14:41.540 --> 01:14:47.740]   their tablets. Oh my God, you're evil. I mean, good in a good way. Let's take a break
[01:14:47.740 --> 01:14:53.980]   more to come with a superb panel. I showed they brought to you by Capetera. Who doesn't
[01:14:53.980 --> 01:14:59.340]   have had we've all had this experience. I certainly did of going to work going to a new job and
[01:14:59.340 --> 01:15:04.420]   they're using software that's based on Windows 98 or that requires Internet Explorer. You
[01:15:04.420 --> 01:15:09.100]   know, old line of business software, some high school kid wrote for the boss 15 years
[01:15:09.100 --> 01:15:15.500]   ago. The kids long gone. He's you know, he's running a tractor in Indiana or somewhere.
[01:15:15.500 --> 01:15:18.900]   And but the boss can't get new software. He says, I like this software and you still
[01:15:18.900 --> 01:15:22.820]   run on this stuff. It's insecure. It's inadequate. It crashes all the time. Who doesn't have
[01:15:22.820 --> 01:15:30.060]   that experience? I got a I got a solution for you. You could show the boss this capetera.com
[01:15:30.060 --> 01:15:35.580]   capetera is a is the leading free directory of software solutions for your business. I
[01:15:35.580 --> 01:15:40.900]   guarantee you there is a newer better program out there for you. Many of them running off
[01:15:40.900 --> 01:15:47.820]   the web. But but the point is no matter what your business and there are 700 categories,
[01:15:47.820 --> 01:15:53.100]   thousands of businesses, everything from a content management system to email marketing,
[01:15:53.100 --> 01:15:57.660]   the IT service to running a yoga studio to workflow management. I go on and look at all
[01:15:57.660 --> 01:16:03.220]   these categories. And and if you need software for your business, you don't have to, you
[01:16:03.220 --> 01:16:06.740]   know, your line of business software doesn't have to be out old and out of date. It can
[01:16:06.740 --> 01:16:10.740]   but it's all there and it makes it really easy. Pick a category, Carson any category.
[01:16:10.740 --> 01:16:14.620]   I don't care. Just click on one there. What are we going to get here? Computer repair
[01:16:14.620 --> 01:16:21.300]   shop software. How many packages are there? In there's hundreds, but now you can narrow
[01:16:21.300 --> 01:16:25.060]   it down. You see that on the left, you could filter it by I want ones with reviews that
[01:16:25.060 --> 01:16:28.660]   are four star or better. I want something that can run online or I can want something
[01:16:28.660 --> 01:16:33.460]   that runs on disk. I maybe I need to manage accounts. Whatever it is you need, you click
[01:16:33.460 --> 01:16:39.460]   the boxes, filter it out, narrow those hundreds of packages down to the few that do everything
[01:16:39.460 --> 01:16:44.260]   you want. You can build a chart, a comparison chart of four different ones. So you can
[01:16:44.260 --> 01:16:50.540]   look side by side to see what which they do. But here's the magic. Kaptera has 850,000
[01:16:50.540 --> 01:16:55.620]   reviews of products with 30,000 fresh reviews every month. And these are real reviews. Kaptera
[01:16:55.620 --> 01:17:01.100]   is very careful to vet them of people actually use the programs. So you can find out what
[01:17:01.100 --> 01:17:07.380]   people really think about the software you're thinking of buying filter the results, compare
[01:17:07.380 --> 01:17:13.820]   them side by side, read the reviews and pick a better program for your business. Kaptera
[01:17:13.820 --> 01:17:18.420]   believes software makes the world a better place because software can help every organization
[01:17:18.420 --> 01:17:23.340]   become a more efficient, effective version of itself. Non-profits too. And here's the
[01:17:23.340 --> 01:17:28.900]   best part. I think people would pay for this five, $10, $20 a month. But Kaptera is absolutely
[01:17:28.900 --> 01:17:34.060]   free free forever. They're not free me. I'm free free no matter what kind of software
[01:17:34.060 --> 01:17:39.100]   your business needs. Kaptera makes it easy to discover the right solution at no cost
[01:17:39.100 --> 01:17:48.420]   to you. Go to Kaptera, C-A-P-T-E-R-R-A, Kaptera.com/Twit. Kaptera is software selection simplified.
[01:17:48.420 --> 01:17:54.420]   Stop suffering with old software. Get in the new stuff. Kaptera.com/Twit. We thank them
[01:17:54.420 --> 01:17:58.620]   for supporting this week in tech. And I thank you for using that URL because that tells
[01:17:58.620 --> 01:18:07.540]   them you heard it here. Kaptera.com/Twit. I love this article in Gizmodo. Don't forget
[01:18:07.540 --> 01:18:15.980]   to cancel Apple News Plus. I forgot. I got charged. I wanted to cancel. Yeah. It launched
[01:18:15.980 --> 01:18:22.060]   March 25th. I did the same thing. But you know what? Apple popped up a thing saying,
[01:18:22.060 --> 01:18:26.900]   do you want to renew? Which I'm really grateful to. This is their magazine rack. They pay
[01:18:26.900 --> 01:18:32.820]   $10 a month for hundreds of magazines. And I found a thing. Quiet is on there. Wired's
[01:18:32.820 --> 01:18:38.060]   on there. I didn't use it that much. So I did cancel because you know what I prefer to
[01:18:38.060 --> 01:18:42.540]   do when I do this? I buy Wired. I subscribe to Wired. There's a handful of newspapers
[01:18:42.540 --> 01:18:46.580]   and magazines. I actually want to give them money directly. Instead of whatever it is
[01:18:46.580 --> 01:18:53.500]   you get from Apple, probably 50% or less of the subscription fee. But I think for most
[01:18:53.500 --> 01:18:57.260]   people, they don't want a bunch of different subscriptions. So maybe Apple News is a good
[01:18:57.260 --> 01:19:03.820]   solution. But yeah, April 25th, three days ago, you didn't cancel. You just paid Apple
[01:19:03.820 --> 01:19:08.740]   $10. It's like the star subscription on my Amazon Prime account. My dad uses it. And
[01:19:08.740 --> 01:19:12.620]   he keeps forgetting that every once in a while, if it says stars, every time he clicks it,
[01:19:12.620 --> 01:19:16.820]   it starts the one week for you to try. Oh crap. And then a week later, I get building statements.
[01:19:16.820 --> 01:19:22.540]   Oh, we noticed a week's cancel. Oh, he keeps he resubscribes doing it. It's like the nighttime
[01:19:22.540 --> 01:19:27.940]   he's done it in two years. Well, you're going to be glad you're an Amazon Prime subscriber.
[01:19:27.940 --> 01:19:33.460]   You see this Amazon Prime wants to offer one day delivery. I'm so excited. I'm dropping
[01:19:33.460 --> 01:19:39.260]   out. Oh, yeah. But it's going to take a while. Yeah. They don't do one data, Italy, unfortunately.
[01:19:39.260 --> 01:19:45.220]   Yeah, they had announced this on their earning call. I guess Friday, Thursday is my thought.
[01:19:45.220 --> 01:19:50.300]   And it was funny because literally then afterwards, every single question that was asked by the
[01:19:50.300 --> 01:19:54.740]   investors was, oh, yeah, both that one day delivery. I mean, they've just said where
[01:19:54.740 --> 01:20:03.100]   Amazon, I guess they said, were they're trying to, over the next year, change entirely globally.
[01:20:03.100 --> 01:20:08.940]   Their two day Prime shipping offer from around the globe to be one day to all zip codes anywhere.
[01:20:08.940 --> 01:20:15.220]   And they're going to spend at least $800 million trying to transfer over their logistic services
[01:20:15.220 --> 01:20:18.740]   and their whole network to work for one day.
[01:20:18.740 --> 01:20:25.340]   I can't wait to tell my grandchildren. I remember when it took two days to get me an Amazon.
[01:20:25.340 --> 01:20:32.180]   Oh, that's, you know, ultimately they want it same day because honestly, the only impediment
[01:20:32.180 --> 01:20:36.300]   to ordering something on Amazon as opposed to the store down the block is you can get
[01:20:36.300 --> 01:20:39.860]   it right now at the store down the block. But if you could get it same day, maybe a few
[01:20:39.860 --> 01:20:41.660]   hours later, but it's not even time.
[01:20:41.660 --> 01:20:46.340]   God, it's the reason why you Amazon now. Yeah, it's effort. I have to lock that block
[01:20:46.340 --> 01:20:50.260]   to that store down there. I can just push a button and it comes. I don't even push a
[01:20:50.260 --> 01:20:55.260]   button. I just tell Echo, I say, Echo, I need batteries. The last time you ordered batteries,
[01:20:55.260 --> 01:20:59.380]   it was these. You want to order them again? Yes. What's your number? And you give it your
[01:20:59.380 --> 01:21:06.300]   code and it orders them and they come the next day. Can I do an Amazon PSA just for
[01:21:06.300 --> 01:21:11.580]   if there's an Amazon developer currently listening to the show? Look, Amazon, I understand
[01:21:11.580 --> 01:21:15.220]   that you want to do that whole thing. If you choose slower delivery, I'll give you a dollar
[01:21:15.220 --> 01:21:19.380]   for digital PSA. Yeah, they say you have an Amazon delivery day. Right. Like Mondays,
[01:21:19.380 --> 01:21:22.980]   my Amazon delivery day. But if you could wait, we'll give you a dollar. I'll never do that.
[01:21:22.980 --> 01:21:28.740]   I always do that. I always do that. And here's the thing though. I want to know. I have,
[01:21:28.740 --> 01:21:34.780]   if I'm ordering like 12 things, I have to make 12 separate orders so I can collect the $1
[01:21:34.780 --> 01:21:39.780]   on every single item versus putting them all in one box. If you let me put them all in
[01:21:39.780 --> 01:21:44.580]   one box and get credit for each one of the items in the box, that's 11 less boxes you
[01:21:44.580 --> 01:21:49.780]   have to send stuff in. I'm just saying. I always choose to have it shipped. So this is weird
[01:21:49.780 --> 01:21:55.220]   online. It is difficult to find there's a delivery option where you can say, Hey, I
[01:21:55.220 --> 01:22:00.900]   want you to ship this in as few boxes as possible. Yes. But I can't find it online. I have to
[01:22:00.900 --> 01:22:05.380]   always go to my mobile device and do it that way. And that's because I just, I don't know,
[01:22:05.380 --> 01:22:11.380]   I have this thing about people like neighbors seeing me with 12 boxes and I feel weird and
[01:22:11.380 --> 01:22:16.900]   like, I don't know, I just, it makes me feel bad. I'm like, I feel like I'm flaunting wealth or
[01:22:16.900 --> 01:22:20.420]   something. And usually it's like small things anyway, but it makes you feel weird. It does look
[01:22:20.420 --> 01:22:28.980]   like my God, this man has money. There's 18 boxes. What's inside of there? And so I find this
[01:22:28.980 --> 01:22:34.420]   interesting and I'm curious, all of you, your experiences with Amazon delivery, I have not had
[01:22:34.420 --> 01:22:40.980]   any issues. If it says two day shipping, I think once in my whole history of being an Amazon customer,
[01:22:41.300 --> 01:22:48.100]   I've had it. It took three days instead of the two days. But I saw several people on Twitter
[01:22:48.100 --> 01:22:53.060]   complain saying, Oh, great, they're going to switch it to one day delivery. This is going to cause a
[01:22:53.060 --> 01:22:58.740]   bunch of issues for me. I wish they would just improve the quality and this and that and the other.
[01:22:58.740 --> 01:23:03.540]   A lot of people apparently have issues with whomever is delivering their packages, where they don't put
[01:23:03.540 --> 01:23:09.060]   them on their porch or they, it doesn't end up showing up or they say I knocked and no one answered.
[01:23:09.060 --> 01:23:12.900]   And then they're like, I was home all day because I worked from home and I didn't hear a knock.
[01:23:12.900 --> 01:23:17.380]   I've not had any issues personally. Most of the time Amazon chooses to use the postal service,
[01:23:17.380 --> 01:23:24.580]   the US Postal Service to deliver mine occasionally. It's UPS, but I'm curious for you all, if you
[01:23:24.580 --> 01:23:30.740]   order two day, does it arrive in two days? Two days or less for me. Oh, that's just come one.
[01:23:30.740 --> 01:23:36.020]   Michael, you live in a big city, a big urban metropolitan region. And so it's easy to deliver to you.
[01:23:37.300 --> 01:23:43.060]   Right? I don't. I live in the Midwest. I know. Okay, that was the joke. Okay, I was like,
[01:23:43.060 --> 01:23:47.860]   wait, you know, I don't. Maybe you maybe you live next door to an Amazon.
[01:23:47.860 --> 01:23:54.500]   That's probably what it is. I need to look around. There's many a time where our Amazon deliveries
[01:23:54.500 --> 01:24:01.300]   have been delayed. I ordered before I left for vacation. I bought a new thinkpad and I ordered a
[01:24:01.300 --> 01:24:06.980]   one terabyte M.2 drive from Western Digital. I'm in Hawaii and I got an email saying,
[01:24:07.380 --> 01:24:13.060]   we don't know what happened. The carrier lost it. And it was supposed to come the next day.
[01:24:13.060 --> 01:24:18.340]   I thought that's amazing. And it didn't come at all. And then it turns out you can't just check
[01:24:18.340 --> 01:24:23.940]   a box saying, well, I want my money back or send me another one. No, you have to go. You have to go
[01:24:23.940 --> 01:24:30.980]   to the Amazon like messenger thing. This happens. So I live in Brooklyn. So most of my Amazon
[01:24:30.980 --> 01:24:35.620]   deliveries, probably not out of 10, get there in like one day or I often do Amazon now because
[01:24:35.620 --> 01:24:39.140]   I'm a heathen who needs products immediately. I'll get them with an hour or two.
[01:24:39.140 --> 01:24:42.740]   So such a good name. It's a two year old name. Now, mommy.
[01:24:42.740 --> 01:24:48.980]   They order my Whole Foods groceries and they arrive in one and a half hours during a snowstorm.
[01:24:48.980 --> 01:24:53.460]   And I'm like, I hate myself, but I love this. But one of every 10 times I probably make an
[01:24:53.460 --> 01:24:57.940]   Amazon order, it'll get, I think the post, US Postal Service hasn't been dead against me
[01:24:57.940 --> 01:25:02.740]   because my stuff will just get lost somehow or never arrived or it'll say it got delivered
[01:25:02.740 --> 01:25:07.540]   within by check the tracking thing. It'll be in like, I don't know, upstate New York somewhere,
[01:25:07.540 --> 01:25:12.420]   never got taken from its facility. So I just message the Amazon little chat line.
[01:25:12.420 --> 01:25:14.740]   Yeah, but you can't do it. That's the only way you can do it, right?
[01:25:14.740 --> 01:25:19.300]   Is you have to. Yeah, there used to be a button you could click that says, my package got lost
[01:25:19.300 --> 01:25:25.300]   or more often for in New York, my package got taken from my doorstep by a neighbor. But you know,
[01:25:25.300 --> 01:25:30.180]   now they have never moved into a big city. There is there is a more effective way to
[01:25:30.180 --> 01:25:33.060]   complain to Amazon than going through their help screen though.
[01:25:33.060 --> 01:25:36.580]   What? Twitter. Seriously, if you they respond to tweets,
[01:25:36.580 --> 01:25:39.380]   Oh, but I can't. I can't be that person. So quickly. Yeah.
[01:25:39.380 --> 01:25:43.300]   So quickly. You need to set up another account that's just like your first name,
[01:25:43.300 --> 01:25:47.300]   underscore Amazon complaints and then no one will know. It'll be here.
[01:25:47.300 --> 01:25:51.460]   Okay, that's acceptable because my my one thing that annoys me is when I see people with like
[01:25:51.460 --> 01:25:56.500]   blue check marks on Twitter, using it to like complain about their services or complain about
[01:25:56.500 --> 01:26:00.100]   their Amazon. Actually, I'm like, come on, you know that you're going to get a great
[01:26:00.100 --> 01:26:04.740]   you know that you're just using the fact that you're just doing it for that.
[01:26:04.740 --> 01:26:11.220]   That's the version of having 12 packages on your porch. Well, my my results are skewed because
[01:26:11.220 --> 01:26:16.260]   when I was living in San Francisco, I lived at a campus and we had a security office and a central
[01:26:16.260 --> 01:26:21.700]   receiving. So it makes it a lot easier for me to get packages. When I was living in DC,
[01:26:21.700 --> 01:26:27.380]   they would leave packages on the porch and probably one out of every three packages got taken.
[01:26:27.380 --> 01:26:35.380]   Now, I don't know why, but this only happens once in a while, but I ordered something on Amazon
[01:26:35.380 --> 01:26:39.300]   for my mom. I ordered a bucket of popcorn because she likes the bucket of popcorn.
[01:26:39.300 --> 01:26:42.660]   Why didn't you go to the corner store, Leo? Because she lives in Rhode Island.
[01:26:42.660 --> 01:26:50.980]   So I got a I got a notice saying, oh, we're only five stops away. And I click so I texted her the map
[01:26:51.540 --> 01:26:54.340]   and then shortly thereafter, I got a picture of the box.
[01:26:54.340 --> 01:27:02.500]   How do you think about this photo? Wow. I want that. Is that wild? What is that?
[01:27:02.500 --> 01:27:07.780]   It's a picture of a box of popcorn on my bus. How do you get that? I want that.
[01:27:07.780 --> 01:27:11.940]   I don't think I applied for it. It doesn't happen all the time. But when it does,
[01:27:11.940 --> 01:27:16.420]   it seems like it was delivered by an Amazon Flex employee then, which is like Amazon's
[01:27:16.420 --> 01:27:22.020]   contract workers. So that's to reassure you that that guy driving the 56 Chevy,
[01:27:22.020 --> 01:27:27.140]   because he is responsible for it. Right. Okay. I wish they would have done this with my hard drive.
[01:27:27.140 --> 01:27:32.660]   Boys, they have to take photos of the packages and send it in. So that means somebody sketched
[01:27:32.660 --> 01:27:38.020]   delivered. I just saw a new thing where Amazon's requiring that the drivers take a selfie of
[01:27:38.020 --> 01:27:42.260]   themselves to confirm their identity as well. That's right. So somebody else isn't doing your
[01:27:42.260 --> 01:27:46.260]   flex. I want to be one of these flex drivers, and I'll take pictures of the packages as I
[01:27:46.260 --> 01:27:55.780]   throw them over fences and trees across the live photo. Our office manager has a ring
[01:27:55.780 --> 01:27:59.620]   camera on her front door because she works for a twit. We all have ring cameras. And she has a
[01:27:59.620 --> 01:28:06.660]   video of the mailman driving by in the truck. Not even stopping throwing the packing boxes on the
[01:28:06.660 --> 01:28:13.300]   door. So she called the post office and sent them the video. She said, this person did this. They
[01:28:13.300 --> 01:28:18.500]   said, well, you don't know who that was. You don't know who that was.
[01:28:18.500 --> 01:28:21.940]   So I'm going to know that was in the USP.
[01:28:21.940 --> 01:28:27.460]   Some of the people just take the trucks and they do rounds. It's weird. No, that was.
[01:28:27.460 --> 01:28:32.500]   That makes you really appreciate Amazon. Now you do raise an interesting point. And so the chat
[01:28:32.500 --> 01:28:37.940]   room this one day delivery is going to put a lot of stress on already highly stressed Amazon
[01:28:37.940 --> 01:28:45.940]   employees working in the fulfillment centers, working to delivery. I mean, I'm so torn because
[01:28:45.940 --> 01:28:52.260]   I've been an employee all my life. Now I'm an employer at twit. So I kind of understand both
[01:28:52.260 --> 01:28:58.820]   sides of it. One is is an employee. It really sucks if this is your job and you're treated badly.
[01:28:58.820 --> 01:29:04.100]   But as an employer, I also say, but they could just get another job. Why are they working in a
[01:29:04.100 --> 01:29:09.700]   job that sucks? So maybe it's the only job they could get. Maybe there are not a lot of jobs in
[01:29:09.700 --> 01:29:15.460]   that town. Maybe they have face tattoos. I don't know what it is. But so should I feel bad for
[01:29:15.460 --> 01:29:21.700]   Amazon employees? You should, but you should not tip them on the invoice. You should hand them
[01:29:21.700 --> 01:29:25.220]   cash because if you tip them on the invoice, Amazon will take part of that. Wait a minute,
[01:29:25.220 --> 01:29:31.780]   you can tip Amazon employees. Yes. Whenever you're doing Amazon now or like the direct delivery ones,
[01:29:31.780 --> 01:29:35.540]   they give you an option to tip. So that's interesting. Oh, I thought you meant like roll up to the
[01:29:35.540 --> 01:29:42.740]   Amazon warehouse and knock on the door and say, go make it a brain, baby. Sorry, you can't pee,
[01:29:42.740 --> 01:29:53.220]   but here's a book. One word, Catherine. That's that word. That word is a word. Say the secret
[01:29:53.220 --> 01:30:01.620]   word and win $100. No, I feel bad. I feel like that's a tough job. But I don't know.
[01:30:01.620 --> 01:30:07.860]   Robits. It is. It's better to have that job than the no job and let the robots do it.
[01:30:07.860 --> 01:30:13.220]   Amazon is, it seems like they're thinking a little bit about the environmental impact of
[01:30:13.220 --> 01:30:18.260]   thousands of trucks driving around all the time delivering us our razor blades.
[01:30:18.260 --> 01:30:22.900]   Especially if you get them all in separate packages so you get that dollar deal.
[01:30:22.900 --> 01:30:30.020]   I have never had to pay for any electronic entertainment on Amazon because of that.
[01:30:30.020 --> 01:30:34.740]   Thank you. Where do you get the digital dollars from? Well, if you agree to not get second day
[01:30:34.740 --> 01:30:40.660]   here, right? Yeah. And by the way, when you check that, it's normally the same delivery time.
[01:30:40.660 --> 01:30:46.580]   Oh, I almost always is the same delivery time. Yeah. Amazon. Yeah, because they go
[01:30:46.580 --> 01:30:51.620]   quote it later sometimes and I've chosen to do that. Oh, because for me, it varied. Sometimes I would
[01:30:51.620 --> 01:30:56.100]   get deals on Amazon pantry and I'd get deals on some other things as well. And so I'd sometimes
[01:30:56.100 --> 01:30:59.860]   click that expecting, okay, this isn't going to arrive for a while. That's fine. And then it would
[01:30:59.860 --> 01:31:04.100]   still arrive the same day. And then because I'm weird again, I don't know if it's a Midwestern
[01:31:04.100 --> 01:31:08.740]   thing. But then I'm like, I feel guilty that I got this credit. No, no, so I've got these 12
[01:31:08.740 --> 01:31:13.060]   packages on my porch and I'm already guilty about that. And then I'm like, wow, they delivered at
[01:31:13.060 --> 01:31:18.340]   the same day that they said they were going to if I just did two day shipping. But now I've also
[01:31:18.340 --> 01:31:23.220]   got this Amazon pantry deal. I know it makes no sense. I don't make sense. But that's how I feel
[01:31:23.220 --> 01:31:28.420]   about it. Do you want the super, super secret tip for that super secret tip? If you're ordering
[01:31:28.420 --> 01:31:34.500]   12 things, you order 11 of them as give me the bonus, but take longer. Order one is second day
[01:31:34.500 --> 01:31:40.260]   air. They will put all of them into the second day air box. That's brilliant. That 100% of the time
[01:31:40.260 --> 01:31:43.940]   that happens. It's just saying. I'm going to start doing that.
[01:31:43.940 --> 01:31:48.820]   So you should, okay. And then you get digital bucks and you can give that to the Amazon employee.
[01:31:48.820 --> 01:31:53.700]   Yes. And then Amazon will take 40% of them. So, yeah. Oh, that's not good.
[01:31:53.700 --> 01:32:03.300]   Amazon did throw out $700 million to invest in Rivian. This is the electric truck company.
[01:32:03.300 --> 01:32:08.660]   The plans to launch an electric pickup and electric SUV in 2020. In the United States,
[01:32:08.660 --> 01:32:16.580]   Amazon joined a round of investing. It also included auto makers. I think,
[01:32:16.580 --> 01:32:23.300]   let me see who else invested in it. I can't find the rest of them. But still,
[01:32:23.300 --> 01:32:28.660]   that's obviously Amazon looking at electric trucks to replace its diesel vehicles. That would make
[01:32:28.660 --> 01:32:35.140]   me feel a little bit better about deliveries from Amazon. And I've always thought that
[01:32:35.140 --> 01:32:40.180]   there is far more of a market for electric vehicles, industrial electric vehicles,
[01:32:40.180 --> 01:32:44.820]   than there are for consumer electric vehicles. Consumer electric vehicles are great. They're
[01:32:44.820 --> 01:32:50.500]   fun to show off. But when you talk about actual savings, it's the industrial stuff that fits what
[01:32:50.500 --> 01:32:56.260]   electric vehicles are good at. Electric vehicles are great at getting efficiency for short travels
[01:32:56.260 --> 01:33:01.940]   that don't go really, really fast that require a lot of torque. That's a delivery vehicle. That's
[01:33:01.940 --> 01:33:06.900]   a truck. So, I mean, Amazon's looking at this and it's a very forward way of saying,
[01:33:06.900 --> 01:33:11.140]   we want to own everything that's part of our core strategy. And if our core strategy is this
[01:33:11.140 --> 01:33:15.540]   one day delivery, and if we can have more electric vehicles on the road than anybody else, we essentially
[01:33:15.540 --> 01:33:22.020]   replace USPS and UPS and FedEx. That's amazing. I don't know why, but for some reason, I went to
[01:33:22.020 --> 01:33:30.420]   Amazon and they offered me six bags of shaved magnesium. That seems dangerous. I don't know.
[01:33:30.420 --> 01:33:36.980]   I just, it seems like it's probably, well, they say it's emergency fire starters.
[01:33:36.980 --> 01:33:39.300]   I don't know if it's a good way. What do they know?
[01:33:39.300 --> 01:33:46.820]   Yeah. What do they know about your future plans? What do you plan about this week? What are the
[01:33:46.820 --> 01:33:52.740]   algorithms telling them? I should sell this overnight. Like I need my six bags of magnesium,
[01:33:52.740 --> 01:33:58.980]   shaved magnesium tomorrow. If you get magnesium wet, doesn't it like burst into flames?
[01:33:58.980 --> 01:34:00.420]   I feel like this is dangerous.
[01:34:00.420 --> 01:34:02.500]   Oh, it's sodium.
[01:34:02.500 --> 01:34:06.260]   Glad you're not a chemistry teacher. I would have blown myself off.
[01:34:06.260 --> 01:34:14.020]   She's Louise. Amazon, by the way, did have its quarterly results. Their profit more than doubled.
[01:34:14.020 --> 01:34:17.620]   70% rise in sales.
[01:34:17.620 --> 01:34:22.980]   Oh, there's this, there's some magnesium spheres I could buy.
[01:34:24.180 --> 01:34:31.220]   They also set in fire. Yeah, they, yeah. 3.56 billion dollars quarterly profit. That's not as
[01:34:31.220 --> 01:34:36.740]   much as some of the other tech giants. But what's interesting is Amazon didn't take a profit for
[01:34:36.740 --> 01:34:42.340]   a long, long time, right? It feels like it's almost, they can do what they want. Like,
[01:34:42.340 --> 01:34:45.700]   should we make profit this quarter? What do you think? I don't know, Jeff, what do you think?
[01:34:45.700 --> 01:34:48.660]   Because they make this a profit from AWS.
[01:34:49.460 --> 01:34:54.420]   It all comes from AWS. The amount of money that they make from AWS allows them to
[01:34:54.420 --> 01:34:58.820]   operate at a loss when it comes to e-commerce or logistics or any of the other
[01:34:58.820 --> 01:35:04.900]   spheres of Amazon, because they are making so much money handover fist from their web services
[01:35:04.900 --> 01:35:11.140]   business. Yeah, the retail department for Amazon is actually a pittance compared to AWS.
[01:35:11.140 --> 01:35:15.700]   It's just enough to justify having all that infrastructure.
[01:35:16.420 --> 01:35:22.180]   Sluggis is a Wall Street Journal. Sluggis retail sales overseas and flat performance from Whole Foods
[01:35:22.180 --> 01:35:30.420]   drag down revenue growth for a fat fourth straight quarter. But I mean, it still rose 17%. So it's
[01:35:30.420 --> 01:35:38.900]   not like really dragging it down. It was 43% a year ago. So stock is up. Let me just see what they
[01:35:38.900 --> 01:35:45.140]   talk. Do they dove divide up their revenue by division or do they not say that?
[01:35:46.020 --> 01:35:47.700]   They probably don't say it.
[01:35:47.700 --> 01:35:55.460]   Although advertising business, revenue derived from advertising rose 34. Did you realize Amazon
[01:35:55.460 --> 01:36:02.740]   makes 34%? They make $2.72 billion a quarter on advertising? Wow.
[01:36:02.740 --> 01:36:09.540]   Amazon is the secret, I guess, 34 advertising giant out there. They're trying to eventually
[01:36:09.540 --> 01:36:14.740]   compete with Facebook and Google. But you're right, Pera, sales rose 41%.
[01:36:15.300 --> 01:36:25.940]   For Amazon web services, $7.7 billion and operating income up 59%, $2.22 billion.
[01:36:25.940 --> 01:36:30.820]   And they'll raise even more if they end up winning the Pentagon's Jedi Cloud contract.
[01:36:30.820 --> 01:36:31.140]   Yes.
[01:36:31.140 --> 01:36:35.780]   Yeah. That's a $10 billion contract. That's a significant
[01:36:35.780 --> 01:36:38.340]   but it's spread out over a few years.
[01:36:38.340 --> 01:36:44.900]   You got to remember on the advertising story for Amazon, that number includes a lot of the
[01:36:44.900 --> 01:36:50.020]   technologies that's not Amazon selling ads directly. They're enabling the metric gathering on ads.
[01:36:50.020 --> 01:36:51.300]   Oh, really?
[01:36:51.300 --> 01:36:56.260]   Right. So they sell some great technology, some great AI technologies that allow you to
[01:36:56.260 --> 01:37:01.780]   better target advertising and to better target your CRM models. And that just
[01:37:01.780 --> 01:37:03.940]   grouped into the advertising profit section.
[01:37:04.580 --> 01:37:09.220]   So it's not just those sponsored products at the top of every single stinkin' search I do
[01:37:09.220 --> 01:37:14.420]   that I always end up buying because it also happens to be the Amazon choice for the said product.
[01:37:14.420 --> 01:37:21.380]   No, it's, for example, if I set up a service using AWS, I automatically have the ability to
[01:37:21.380 --> 01:37:28.180]   quickly add AI to my service so that I can gather better quality information on my users.
[01:37:28.180 --> 01:37:32.500]   That's counted as advertising revenue because ultimately that's what they're doing. They're
[01:37:32.500 --> 01:37:34.020]   just figuring out who they should pitch to.
[01:37:34.020 --> 01:37:44.740]   Did you know that Amazon is the highest rated tech company for reputation quotient?
[01:37:44.740 --> 01:37:51.540]   So this is a really interesting story from Axios. They do a, it's called the Axios Harris
[01:37:51.540 --> 01:38:00.900]   poll. Every, I guess every year they survey 18,000 Americans to ask them which companies have
[01:38:00.900 --> 01:38:08.100]   the highest reputation quotient. But what really surprised me is that of all the tech companies,
[01:38:08.100 --> 01:38:12.500]   let me go back to this graph. Amazon has the highest, it's number two.
[01:38:12.500 --> 01:38:15.220]   And if you look at the reputation for what?
[01:38:15.220 --> 01:38:25.060]   Be in a good company, not reputation for sucking. Let me see, how do they define reputation?
[01:38:26.980 --> 01:38:29.780]   Well, if you look at that graph of Facebook being all the way down there.
[01:38:29.780 --> 01:38:36.900]   So if you look at the graph, which this is seven years of results of these polls,
[01:38:36.900 --> 01:38:41.940]   Microsoft, and this was the story Axios published, has actually come up quite a bit.
[01:38:41.940 --> 01:38:47.940]   They're among the big tech companies, among the fangs, they're number two. It's Amazon,
[01:38:47.940 --> 01:38:55.220]   Microsoft number nine, Netflix 24, Apple, oddly enough, number 32 on the list below Amazon,
[01:38:55.220 --> 01:39:02.500]   Microsoft and Netflix. And just above Google, but nobody, but nobody is as bad as Facebook.
[01:39:02.500 --> 01:39:09.700]   And really Facebook is plummeting their 90 number 94 out of the top 100 now.
[01:39:09.700 --> 01:39:18.340]   Companies untouched by scandal. Companies that consumers can trust. I don't know.
[01:39:19.140 --> 01:39:26.340]   So like trust and trust. Yeah. I think it's because most people assume when you say the word Amazon,
[01:39:26.340 --> 01:39:30.580]   they think of the place they go online. It's an online marketplace to buy goods and get
[01:39:30.580 --> 01:39:35.300]   stuck to their house. They don't realize that Amazon isn't just the everything store.
[01:39:35.300 --> 01:39:40.820]   They're the largest cloud computing provider. They have home security systems, fashion designers,
[01:39:40.820 --> 01:39:48.020]   books, they own IMDb. They have like as much physical spaces, like 38 pentagons put together.
[01:39:48.020 --> 01:39:53.780]   Yeah. And do so many other things. Yeah. From Twitch. I mean, there are so many
[01:39:53.780 --> 01:39:58.420]   different words of Amazon. Number one is weggments. What is what is well, good old Weggman.
[01:39:58.420 --> 01:40:01.700]   Is it a grocery store? I think it's a grocery store. Don't they sell wigs?
[01:40:01.700 --> 01:40:08.580]   No. Why is that in the thing? It's not just tech. It's all companies.
[01:40:08.580 --> 01:40:15.220]   So the top 10 are Weggman's Amazon Patagonia. Oh, and drill down because they're number one for
[01:40:15.220 --> 01:40:21.220]   ethics. There's a category called ethics. Oh, so affinity ethics growth products and services,
[01:40:21.220 --> 01:40:27.380]   citizenship, vision, culture, character, and trajectory. Wow. I've never been more buzzed
[01:40:27.380 --> 01:40:34.260]   than that buzzword collection. Yeah. That's good. That's more. Number five trajectory.
[01:40:34.260 --> 01:40:39.220]   Publix and other grocery store. There's six Samsung's in that list. Number seven,
[01:40:39.220 --> 01:40:44.500]   above Procter and Gamble, Microsoft, Sony, UPS. Go down to Apple. I want to see what hurt them.
[01:40:44.500 --> 01:40:48.580]   What was the thing that dragged it down? So you apparently figured this. Oh, I see.
[01:40:48.580 --> 01:40:55.060]   You could see. So Apple's products and services are very high. Vision is high.
[01:40:55.060 --> 01:40:58.980]   Trajectory is high. But the lowest is citizenship. Ouch.
[01:40:58.980 --> 01:41:06.500]   Right down there with general electric and character. What people don't like Apple. I think it's just
[01:41:06.500 --> 01:41:09.300]   representing how they don't like Apple. Shall we kick a puppy?
[01:41:10.180 --> 01:41:15.380]   Kick that. Oh, look of Comcast. Oh, no. Are they even on the list? I want to see what they go for
[01:41:15.380 --> 01:41:21.220]   Amazon was outstanding in growth, vision, and trajectory. But it's only weak point and wasn't
[01:41:21.220 --> 01:41:28.580]   very weak is citizenship. There's numbers six and ethics. And ethics. Yeah, but not terrible.
[01:41:28.580 --> 01:41:32.340]   So where is Comcast? Probably I don't sort of go down at the bottom. I don't think the list.
[01:41:32.340 --> 01:41:36.420]   The US government is one of them. The US government is below the Trump organization in
[01:41:36.420 --> 01:41:41.380]   Philip Morris. You can't get lower than that. Philip Morris.
[01:41:41.380 --> 01:41:47.140]   Sears. Sears. Wells Fargo. Sears. Trump organization. Philip Morrison, US government.
[01:41:47.140 --> 01:41:48.580]   The bottom five. Look at that.
[01:41:48.580 --> 01:41:55.540]   Earned that spot. Dish you earned that spot. Oh, 91. That's horrible. And you deserve it.
[01:41:55.540 --> 01:41:59.780]   Here's Comcast. Number 91. Let's just see what hurt Comcast. Everything.
[01:41:59.780 --> 01:42:04.100]   Sucked along. So Comcast, you were great except for the everything.
[01:42:04.980 --> 01:42:11.140]   There were there were category with citizenship. Character followed by character affinity ethics.
[01:42:11.140 --> 01:42:16.260]   Wow. Twitter is low too. Have you seen if you look down to the next one, the biggest declines.
[01:42:16.260 --> 01:42:21.780]   Facebook, Tesla, McDonald's, Target, Nike, Chick-fil-a, Google, Comcast, and Sears. Excuse me.
[01:42:21.780 --> 01:42:26.340]   It's Chick-fil-a. I'm sorry, Chick-fil-a. Chick-fil-a. I've never actually eaten there.
[01:42:26.340 --> 01:42:37.300]   Benicchile. Google Big Drop 2. But Facebook 43 spots. Wow. Tesla Big Drop.
[01:42:37.300 --> 01:42:44.340]   Yeah. I bet there are 39. But Amazon, big winner here. So people like Amazon. They like that one
[01:42:44.340 --> 01:42:49.060]   day delivery thing. Hey, Comcast, good news. You're more or less stable at the bottom.
[01:42:50.500 --> 01:42:56.980]   Right. They only dropped 13 places. Even Sears, which I don't even think exists anymore.
[01:42:56.980 --> 01:43:00.820]   Right. Is there Sears? Even Sears beats the US government.
[01:43:00.820 --> 01:43:04.980]   Back in my day, that was a she. You know, you could order a house by mail.
[01:43:04.980 --> 01:43:11.060]   Build it yourself. One man. All right.
[01:43:11.060 --> 01:43:12.660]   Yep. Let's talk about.
[01:43:12.660 --> 01:43:13.780]   Wakemans Facebook.
[01:43:13.780 --> 01:43:20.420]   Wegmans. To Chatham, must know Wegmans, right? Chatham, where's Wegmans?
[01:43:20.420 --> 01:43:21.780]   Where's my nearest Wegmans?
[01:43:21.780 --> 01:43:26.740]   Was it William Wegman was the guy who took pictures of his dogs.
[01:43:26.740 --> 01:43:30.340]   What? You remember? Oh, the Weimariners?
[01:43:30.340 --> 01:43:36.180]   Yeah, the Weimariners. Wegmans. Get great meal help and more at Wegmans.com.
[01:43:36.180 --> 01:43:40.900]   Make it a meal. Apparently they have a brisk arm of the Wegmans.
[01:43:40.900 --> 01:43:48.260]   Wegmans in California. Yeah. Let's see where the stores primarily are.
[01:43:48.260 --> 01:43:52.500]   Maryland, Pennsylvania, New York. There's one in Brooklyn.
[01:43:52.500 --> 01:43:54.740]   There's one. Go ahead. Go on over.
[01:43:54.740 --> 01:43:58.420]   Hi, these two are Wegmans. Yeah.
[01:43:58.420 --> 01:44:00.980]   Why are they number one? That's so strange.
[01:44:00.980 --> 01:44:03.140]   Can you report live from the Wegmans? The number one.
[01:44:03.140 --> 01:44:05.540]   I'm standing outside a Wegmans store.
[01:44:05.540 --> 01:44:08.980]   Ask everyone. They just massage my feet.
[01:44:08.980 --> 01:44:11.940]   You trust Wegmans. Why do you trust Wegmans? They're very ugly.
[01:44:11.940 --> 01:44:16.820]   Can we have a best of for the day?
[01:44:16.820 --> 01:44:18.340]   All right. So we're going to take a little break.
[01:44:18.340 --> 01:44:21.780]   If you need to take a break yourself, my fine panel, Paris,
[01:44:21.780 --> 01:44:26.660]   Micah, Robert, go ahead because we are going to sit back,
[01:44:26.660 --> 01:44:32.660]   no spoilers and watch a movie about what happened this week on Twitch.
[01:44:32.660 --> 01:44:34.500]   Previously on Twitch.
[01:44:34.500 --> 01:44:40.340]   It's time again to talk to the most fabulous, amazing iOS expert in the world.
[01:44:40.340 --> 01:44:43.780]   My mom. Do you celebrate Mother's Day, mama?
[01:44:43.780 --> 01:44:46.180]   Will you feel bad if I don't send you something?
[01:44:46.180 --> 01:44:49.140]   Yes. You're egging me on.
[01:44:49.140 --> 01:44:51.860]   Tech news weekly.
[01:44:51.860 --> 01:44:55.620]   Now, sometimes Galaxy Fold just can't catch a break these days.
[01:44:55.620 --> 01:45:00.180]   The talented folks at iFixit have been patiently tearing down the device to get to the heart.
[01:45:00.180 --> 01:45:02.660]   So we have a copy of the catch as well.
[01:45:02.660 --> 01:45:08.260]   It's so close to launch, I have to assume they had tens, if not hundreds of thousands of these
[01:45:08.260 --> 01:45:13.380]   manufactured and ready to go. I certainly can't imagine they'll be able to physically alter the
[01:45:13.380 --> 01:45:17.060]   hardware in the kind of time frame that people are probably hoping for.
[01:45:17.060 --> 01:45:23.620]   With the recall of all these devices, you then had to let this go back to Samsung in pieces, right?
[01:45:23.620 --> 01:45:26.100]   You know, it didn't go back in pieces. It went back in one non-working piece.
[01:45:26.100 --> 01:45:26.820]   Let's put it that way.
[01:45:26.820 --> 01:45:32.260]   This week in Google, Google walk out organizers are saying they're facing
[01:45:32.260 --> 01:45:34.900]   retaliation for organizing those walkouts.
[01:45:34.900 --> 01:45:37.540]   They've been, some of them have been demoted.
[01:45:37.540 --> 01:45:40.340]   But is that illegal is my question.
[01:45:40.340 --> 01:45:41.620]   It's not good PR.
[01:45:41.620 --> 01:45:43.940]   It's not good PR, but it's not illegal.
[01:45:43.940 --> 01:45:44.900]   Right, Karsten?
[01:45:44.900 --> 01:45:47.300]   You're absolutely positively right.
[01:45:47.300 --> 01:45:50.660]   There's nothing wrong with anything you have ever said.
[01:45:50.660 --> 01:45:51.540]   That's right.
[01:45:51.540 --> 01:45:54.820]   To it, the happiest place on earth.
[01:45:54.820 --> 01:45:56.340]   Or else.
[01:45:56.340 --> 01:46:00.900]   It's in our contract.
[01:46:00.900 --> 01:46:01.780]   You better be happy.
[01:46:01.780 --> 01:46:02.660]   That's all I'm saying.
[01:46:02.660 --> 01:46:04.260]   I'll show you today, Brat, you.
[01:46:04.260 --> 01:46:09.940]   I tell you what, Twit does have the best dental health of any podcast network because
[01:46:09.940 --> 01:46:12.260]   we are a Qwip network.
[01:46:12.260 --> 01:46:16.340]   We use the electric toothbrush design to make brushing your teeth more simple,
[01:46:16.340 --> 01:46:18.420]   more affordable, even enjoyable.
[01:46:18.420 --> 01:46:22.340]   Created by dentists, loved by people, and very affordable.
[01:46:22.340 --> 01:46:27.300]   If you've always wanted to brush better, but maybe didn't want to spend hundreds of dollars
[01:46:27.300 --> 01:46:30.580]   for an electric toothbrush, check out Qwip.
[01:46:30.980 --> 01:46:34.180]   I think it is absolutely the best toothbrush I've ever used.
[01:46:34.180 --> 01:46:38.100]   In fact, we use it for travel too because, well, let me explain it.
[01:46:38.100 --> 01:46:43.300]   So it's gentle and all dentists say people are brushing too hard.
[01:46:43.300 --> 01:46:46.020]   Sometimes electric toothbrushes are too abrasive.
[01:46:46.020 --> 01:46:50.660]   Qwip is an effective gentle clean for both your teeth and your gums.
[01:46:50.660 --> 01:46:53.940]   Of course, it has a two minute timer that pulses every 30 seconds.
[01:46:53.940 --> 01:46:57.780]   So you can brush each of the four quadrants of your mouth for 30 seconds each,
[01:46:57.780 --> 01:46:59.140]   as a dentist recommends.
[01:47:00.340 --> 01:47:03.860]   By the way, about 90% of us don't do that or don't clean evenly.
[01:47:03.860 --> 01:47:05.380]   So it's very helpful for that.
[01:47:05.380 --> 01:47:09.780]   I love the cover, which you can use either as a stand or there's a little sticky strip on the back.
[01:47:09.780 --> 01:47:10.980]   I'm at it right to the mirror that way.
[01:47:10.980 --> 01:47:11.780]   I never forget.
[01:47:11.780 --> 01:47:14.820]   Hey, I'm looking at this toothbrush in my mirror.
[01:47:14.820 --> 01:47:19.540]   It's time to brush, declutters your sink, and because it doesn't have a charger,
[01:47:19.540 --> 01:47:23.380]   it runs in a AAA battery, which lasts three months.
[01:47:23.380 --> 01:47:25.700]   I love it for travel.
[01:47:25.700 --> 01:47:26.980]   I don't have to bring a charger with me.
[01:47:26.980 --> 01:47:31.460]   It's very compact. That stand doubles as a cover, so you flip it around, cover it up.
[01:47:31.460 --> 01:47:35.780]   There's no easier way to travel, no wires, no clunky charger.
[01:47:35.780 --> 01:47:39.380]   And even if you do run out, you just go out and get a AAA battery,
[01:47:39.380 --> 01:47:41.060]   you're good to go for another three months.
[01:47:41.060 --> 01:47:46.340]   Brush heads, batteries, even toothpaste, if you want to deliver automatically on a
[01:47:46.340 --> 01:47:49.380]   dentist recommended schedule every three months for just $5.
[01:47:49.380 --> 01:47:50.980]   Well, that's for the brush heads.
[01:47:50.980 --> 01:47:56.260]   That's important too, because about three out of four of us use brushes
[01:47:56.260 --> 01:47:58.900]   that are worn out, old, and ineffective.
[01:47:58.900 --> 01:48:03.140]   Quip, one of the first lecture toothbrushes accepted by the American Dental Association,
[01:48:03.140 --> 01:48:06.580]   and they have thousands of five-star verified reviews.
[01:48:06.580 --> 01:48:07.780]   I love Quip.
[01:48:07.780 --> 01:48:08.580]   We use Quip.
[01:48:08.580 --> 01:48:10.820]   They're backed by 20,000 dental professionals.
[01:48:10.820 --> 01:48:12.660]   And I love it because it starts at $25.
[01:48:12.660 --> 01:48:15.060]   So it's affordable.
[01:48:15.060 --> 01:48:19.380]   The whole family can have a quip or you can get one quip with four heads and share it around.
[01:48:19.380 --> 01:48:23.060]   $25 for the starters said that couldn't be a better price.
[01:48:23.060 --> 01:48:26.500]   And if you want, you get fancy, they have copper and gold and all that stuff.
[01:48:26.500 --> 01:48:32.100]   If you go to getquip.com/twit, even your first refill pack is free when you purchase any
[01:48:32.100 --> 01:48:34.100]   quip electric toothbrush.
[01:48:34.100 --> 01:48:35.300]   That's a great deal.
[01:48:35.300 --> 01:48:36.900]   What a good Mother's Day gift.
[01:48:36.900 --> 01:48:38.580]   Maybe I should send that to mom.
[01:48:38.580 --> 01:48:42.580]   Get your first refill pack free at getquip.com/twit.
[01:48:42.580 --> 01:48:43.860]   I did send it to my daughter.
[01:48:43.860 --> 01:48:44.660]   I was very proud.
[01:48:44.660 --> 01:48:46.740]   She said, "Dad, I think I ought to brush better."
[01:48:46.740 --> 01:48:48.180]   I said, "It's on its way.
[01:48:48.180 --> 01:48:49.700]   The quip is on its way."
[01:48:49.700 --> 01:48:54.100]   G-E-T-Q-U-I-P.com/twit.
[01:48:54.100 --> 01:48:55.460]   We thank quip for their support.
[01:48:55.460 --> 01:48:59.380]   We thank you for supporting us by using that special URL.
[01:48:59.380 --> 01:49:03.700]   Getquip.com/twit.
[01:49:03.700 --> 01:49:05.780]   Facebook.
[01:49:05.780 --> 01:49:07.460]   Facebook.
[01:49:07.460 --> 01:49:09.620]   -Text number 94.
[01:49:09.620 --> 01:49:11.620]   -And falling.
[01:49:11.620 --> 01:49:15.780]   -And you know, next year, I'd be surprised if it's on the list at all.
[01:49:15.780 --> 01:49:19.380]   So the latest thing, and now the state of New York is investigating,
[01:49:19.380 --> 01:49:22.660]   is this email password thing.
[01:49:22.660 --> 01:49:23.460]   Have you seen this?
[01:49:23.460 --> 01:49:30.580]   The New York Attorney General is going to investigate Facebook over harvesting email.
[01:49:30.580 --> 01:49:36.580]   The unauthorized collection of 1.5 million users' email address books,
[01:49:36.580 --> 01:49:40.740]   which Facebook says, "It was unintentional.
[01:49:40.740 --> 01:49:43.380]   It was unintentional."
[01:49:43.940 --> 01:49:49.540]   So it started with a tweet from E-Sushi saying that Facebook was asking some users
[01:49:49.540 --> 01:49:53.780]   to enter their email passwords when they opened up new accounts.
[01:49:53.780 --> 01:50:01.620]   Because we have to verify that you're real.
[01:50:01.620 --> 01:50:06.180]   So this is the pop-up according to a business insider.
[01:50:06.180 --> 01:50:07.380]   This is the pop-up.
[01:50:07.380 --> 01:50:09.380]   Confirm your email address.
[01:50:09.380 --> 01:50:12.820]   To continue using Facebook, you need to confirm your email address.
[01:50:12.820 --> 01:50:15.940]   Because you signed up with Stephen Mercantile at Yandex.com,
[01:50:15.940 --> 01:50:18.420]   you could do this automatically through Yandex.com.
[01:50:18.420 --> 01:50:21.700]   Now, normal website, what would they do?
[01:50:21.700 --> 01:50:23.940]   They'd send you an email, right?
[01:50:23.940 --> 01:50:27.220]   To that address, let's say, click this link to verify.
[01:50:27.220 --> 01:50:28.660]   That should be sufficient.
[01:50:28.660 --> 01:50:30.180]   No, what did Facebook do?
[01:50:30.180 --> 01:50:31.860]   Here's your email.
[01:50:31.860 --> 01:50:33.620]   Give us your password.
[01:50:33.620 --> 01:50:34.500]   -Amazing.
[01:50:34.500 --> 01:50:36.420]   -We promise we won't save it.
[01:50:36.420 --> 01:50:36.820]   -It does.
[01:50:36.820 --> 01:50:38.100]   It says we won't save it.
[01:50:38.100 --> 01:50:40.420]   What the what?
[01:50:42.180 --> 01:50:45.220]   Well, maybe they didn't save it, but what did they do with the password?
[01:50:45.220 --> 01:50:47.940]   They connected and downloaded your contacts.
[01:50:47.940 --> 01:50:50.100]   -That's so groady.
[01:50:50.100 --> 01:50:50.820]   -It's not a bug.
[01:50:50.820 --> 01:50:52.100]   That's a feature.
[01:50:52.100 --> 01:50:54.900]   -And look what you'd see briefly, by the way,
[01:50:54.900 --> 01:50:58.340]   after you gave them the password and clicked the link.
[01:50:58.340 --> 01:51:00.900]   Importing contacts.
[01:51:00.900 --> 01:51:03.860]   So at least they told you.
[01:51:03.860 --> 01:51:06.260]   What the hell?
[01:51:06.260 --> 01:51:09.140]   That was in it first.
[01:51:09.140 --> 01:51:12.020]   -I stabbed you, but at least I told you that I stabbed you.
[01:51:12.500 --> 01:51:13.140]   It's fine.
[01:51:13.140 --> 01:51:16.420]   -I told you after the fact when you accused me of stabbing you,
[01:51:16.420 --> 01:51:17.780]   I was like, "Oh, yeah, I definitely did it.
[01:51:17.780 --> 01:51:18.900]   I'm sorry.
[01:51:18.900 --> 01:51:19.700]   Won't do it again."
[01:51:19.700 --> 01:51:23.620]   -Facebook says it was unintentionally uploaded to Facebook.
[01:51:23.620 --> 01:51:24.580]   We're deleting it.
[01:51:24.580 --> 01:51:27.700]   That makes no sense.
[01:51:27.700 --> 01:51:29.700]   -Somebody had to write that program,
[01:51:29.700 --> 01:51:31.380]   and that's a lot of lines of code.
[01:51:31.380 --> 01:51:35.540]   That's not some code monkey doing an authentication login screen.
[01:51:35.540 --> 01:51:36.740]   -And we went through a Q&A process.
[01:51:36.740 --> 01:51:37.780]   -Somebody approved it.
[01:51:37.780 --> 01:51:38.260]   -It's a review.
[01:51:40.260 --> 01:51:42.980]   It was definitely a couple of team meetings for one.
[01:51:42.980 --> 01:51:44.020]   -Okay, here's the thing.
[01:51:44.020 --> 01:51:45.540]   I love to rally against Facebook
[01:51:45.540 --> 01:51:48.660]   because they've done so many head-slappingly bad moves
[01:51:48.660 --> 01:51:52.020]   in the last couple of years, but we're enabling them.
[01:51:52.020 --> 01:51:54.340]   By we, I mean, the internet community
[01:51:54.340 --> 01:51:55.620]   that continues to use Facebook,
[01:51:55.620 --> 01:51:58.100]   because they've done this so many times,
[01:51:58.100 --> 01:51:59.460]   and then they draw back a little bit,
[01:51:59.460 --> 01:52:01.700]   and then they push again, and then they draw back a little bit.
[01:52:01.700 --> 01:52:04.340]   And if you look at where our privacy policy was
[01:52:04.340 --> 01:52:07.380]   with Facebook eight years ago versus where it is now,
[01:52:07.380 --> 01:52:10.580]   it is so unrecognizable because they know
[01:52:10.580 --> 01:52:12.580]   that all they have to do is apologize,
[01:52:12.580 --> 01:52:16.260]   maybe pay a tiny little fine, and then they'll push again,
[01:52:16.260 --> 01:52:17.380]   and again, and again.
[01:52:17.380 --> 01:52:19.300]   And that's why this will never stop.
[01:52:19.300 --> 01:52:20.980]   Until people finally say,
[01:52:20.980 --> 01:52:23.380]   "No, no, no, you finally push past the line
[01:52:23.380 --> 01:52:26.100]   at which we will continue to use your service,
[01:52:26.100 --> 01:52:28.420]   they will never stop because it makes money."
[01:52:28.420 --> 01:52:31.620]   -Facebook, it might not even be a tiny little fine,
[01:52:31.620 --> 01:52:33.300]   but Facebook makes so much money.
[01:52:33.300 --> 01:52:35.380]   For instance, in their quarterly results this week,
[01:52:35.380 --> 01:52:38.500]   they announced, "We're gonna set aside $3 billion.
[01:52:38.500 --> 01:52:40.420]   We expect a pretty big fine from the EU
[01:52:40.420 --> 01:52:42.100]   in the $3 billion to $5 billion range."
[01:52:42.100 --> 01:52:44.580]   So we're just gonna take a little bit of pocket change
[01:52:44.580 --> 01:52:47.780]   we made this year, and we're just gonna put that aside.
[01:52:47.780 --> 01:52:51.540]   $3 billion to $5 billion, but come on,
[01:52:51.540 --> 01:52:53.140]   they're making more than that a quarter.
[01:52:53.140 --> 01:52:55.220]   It's pocket change.
[01:52:55.220 --> 01:52:55.780]   -Oh, it's bad.
[01:52:55.780 --> 01:52:58.180]   Okay, no, that's a serious slap on the wrist, but still.
[01:52:58.180 --> 01:52:59.380]   -But it is, but it isn't,
[01:52:59.380 --> 01:53:00.980]   because $3 billion to, again,
[01:53:00.980 --> 01:53:03.780]   moved up the privacy goal post further just a little bit.
[01:53:03.780 --> 01:53:06.020]   -Yeah, that's actually the real point,
[01:53:06.020 --> 01:53:08.820]   which is they're getting something for that $3 billion.
[01:53:08.820 --> 01:53:12.500]   And it's worth it, because they're not only moving
[01:53:12.500 --> 01:53:15.300]   the privacy goal post, but whatever it was that they did
[01:53:15.300 --> 01:53:18.340]   to get the fine, I bet you they made more money off of it.
[01:53:18.340 --> 01:53:19.140]   -Exactly.
[01:53:19.140 --> 01:53:20.180]   -Then the fine.
[01:53:20.180 --> 01:53:21.940]   And so it's a profit thing.
[01:53:21.940 --> 01:53:27.460]   What more can you say about Facebook?
[01:53:27.460 --> 01:53:30.740]   These guys, are they scum?
[01:53:30.740 --> 01:53:34.020]   Are they, what, so I asked this question on this weekend,
[01:53:34.020 --> 01:53:35.540]   Google, we talked about the same story.
[01:53:35.540 --> 01:53:38.180]   Is Facebook immoral?
[01:53:38.180 --> 01:53:39.380]   Are they amoral?
[01:53:39.380 --> 01:53:42.100]   Are they just move fast and break things?
[01:53:42.100 --> 01:53:43.860]   Geeks, Paris, what do you think?
[01:53:43.860 --> 01:53:47.620]   -I think at a certain point, a company gets so big that
[01:53:47.620 --> 01:53:50.580]   it is impossible even.
[01:53:50.580 --> 01:53:52.820]   I mean, I think the thing that always sticks out to me
[01:53:52.820 --> 01:53:55.300]   is during the suck hearing this last year.
[01:53:55.300 --> 01:53:59.700]   There were certain times where Mark Zuckerberg said something,
[01:53:59.700 --> 01:54:02.020]   where he's like, "Oh, I didn't know that X was happening
[01:54:02.020 --> 01:54:02.820]   in regards to Facebook."
[01:54:02.820 --> 01:54:05.460]   I didn't know that we were doing lie.
[01:54:05.460 --> 01:54:08.660]   And it's like, at a certain point, your company gets so large
[01:54:08.660 --> 01:54:12.900]   that there is not one person or five people or 10 people responsible.
[01:54:12.900 --> 01:54:18.100]   It is a giant beast that cannot be contained by any man or woman.
[01:54:18.100 --> 01:54:20.980]   And that is terrifying in a note of its own right.
[01:54:20.980 --> 01:54:23.940]   -So you're kind of giving them a pass.
[01:54:23.940 --> 01:54:24.900]   You're saying, "Well, it's just a really big--"
[01:54:24.900 --> 01:54:26.820]   -No, I don't think they're giving it a pass.
[01:54:26.820 --> 01:54:29.220]   I think that it's still just as bad, but I think that
[01:54:30.020 --> 01:54:32.820]   there is a certain kind of--
[01:54:32.820 --> 01:54:33.780]   -They're uncovering over it.
[01:54:33.780 --> 01:54:35.940]   -I feel like schools have thought, "Yeah, we're there saying,
[01:54:35.940 --> 01:54:38.580]   'Oh, we're blaming it on suck or share on.'
[01:54:38.580 --> 01:54:38.900]   -Yeah.
[01:54:38.900 --> 01:54:41.300]   -Well, we're blaming it on a certain person that it is--
[01:54:41.300 --> 01:54:45.380]   I think blaming it on morality, being good versus evil,
[01:54:45.380 --> 01:54:46.740]   makes the problem too simple.
[01:54:46.740 --> 01:54:49.460]   The fact is that it has grown to such a size
[01:54:49.460 --> 01:54:52.180]   that none of the choices are even, at a certain point,
[01:54:52.180 --> 01:54:53.860]   to the choices of one person.
[01:54:53.860 --> 01:54:56.420]   But it is just this large ever-moving,
[01:54:56.980 --> 01:55:00.020]   ever-moving fast and ever-breaking things of other beast
[01:55:00.020 --> 01:55:03.380]   that is very difficult to contain
[01:55:03.380 --> 01:55:07.300]   and even more difficult to have actionable solutions to things.
[01:55:07.300 --> 01:55:08.260]   And that's how we should--
[01:55:08.260 --> 01:55:11.940]   -Is it because they have two billion users that you can't--
[01:55:11.940 --> 01:55:12.580]   you just can't.
[01:55:12.580 --> 01:55:15.140]   But I mean, still, right?
[01:55:15.140 --> 01:55:16.100]   -I think it does.
[01:55:16.100 --> 01:55:20.100]   -Writing an authentication process that requires a user's password
[01:55:20.100 --> 01:55:21.860]   to their email.
[01:55:21.860 --> 01:55:25.380]   No one thinks that's a good idea.
[01:55:25.380 --> 01:55:26.500]   -How is that?
[01:55:26.500 --> 01:55:30.420]   -And then to use it not for what it's advertised for,
[01:55:30.420 --> 01:55:33.460]   but to then start importing context when that was not--
[01:55:33.460 --> 01:55:34.740]   yeah, there was no mention.
[01:55:34.740 --> 01:55:36.660]   It didn't have anything to do with importing context.
[01:55:36.660 --> 01:55:40.100]   You're supposed to be authenticating my login or whatever.
[01:55:40.100 --> 01:55:42.820]   Why are you now importing my contacts?
[01:55:42.820 --> 01:55:45.620]   That's not what you said was going to happen.
[01:55:45.620 --> 01:55:46.500]   Why did this happen?
[01:55:46.500 --> 01:55:50.820]   And then to go with the unintentional thing, that's just so--
[01:55:50.820 --> 01:55:55.620]   it was intended that you can't say it's unintentional
[01:55:55.620 --> 01:55:57.940]   when that makes no sense at all.
[01:55:57.940 --> 01:56:02.180]   -I think unintentional is Facebook code for--
[01:56:02.180 --> 01:56:05.780]   we've gotten too big barberches and we do not know how to control
[01:56:05.780 --> 01:56:07.940]   all parts of our body.
[01:56:07.940 --> 01:56:08.180]   -Yeah.
[01:56:08.180 --> 01:56:10.340]   -And that is an even scarier reality, of course.
[01:56:10.340 --> 01:56:13.140]   So that's why they would not use that term of phrase.
[01:56:13.140 --> 01:56:13.620]   -That's right.
[01:56:13.620 --> 01:56:15.540]   -I guess to make matters worse during the same week,
[01:56:15.540 --> 01:56:19.300]   or maybe it was last week, you know, last month they had had
[01:56:19.300 --> 01:56:21.060]   that whole plain text password,
[01:56:21.060 --> 01:56:22.900]   snafu on Instagram where they had said,
[01:56:22.900 --> 01:56:27.060]   "Oh, we had had some thousands of users password stored in plain text."
[01:56:27.060 --> 01:56:30.020]   And of course, on Mueller report day right in the morning,
[01:56:30.020 --> 01:56:32.340]   they updated that PR post to say,
[01:56:32.340 --> 01:56:34.260]   "Oh, yeah, it wasn't thousands.
[01:56:34.260 --> 01:56:37.460]   It was accidentally millions of Instagram passwords that were exposed."
[01:56:37.460 --> 01:56:37.700]   -But they--
[01:56:37.700 --> 01:56:38.020]   -Sorry.
[01:56:38.020 --> 01:56:40.100]   -But all they did was change thousands to millions.
[01:56:40.100 --> 01:56:41.220]   They didn't tell anybody--
[01:56:41.220 --> 01:56:42.740]   -No, of course not.
[01:56:42.740 --> 01:56:43.700]   They just changed it.
[01:56:43.700 --> 01:56:44.820]   -They didn't say we got to--
[01:56:44.820 --> 01:56:45.460]   they didn't say anything.
[01:56:45.460 --> 01:56:47.380]   They just changed thousands to millions.
[01:56:47.380 --> 01:56:48.340]   Of course, everybody noticed.
[01:56:48.340 --> 01:56:48.980]   Thank goodness.
[01:56:48.980 --> 01:56:51.140]   That's pretty bad.
[01:56:51.140 --> 01:56:54.020]   It's internally stored in plain text.
[01:56:54.020 --> 01:56:56.260]   That's a-- that you could say that was a mistake.
[01:56:56.260 --> 01:56:57.060]   Somebody blew it.
[01:56:57.060 --> 01:56:58.180]   It's bad.
[01:56:58.180 --> 01:56:59.220]   It's a bad mistake.
[01:56:59.220 --> 01:57:01.460]   And it's by the way that following on the heels of them
[01:57:01.460 --> 01:57:04.020]   doing the same thing with Facebook passwords not so long ago.
[01:57:04.020 --> 01:57:07.060]   I don't-- I mean, I don't have a Facebook account.
[01:57:07.060 --> 01:57:09.700]   I think you'd be nuts to have a Facebook account at this point.
[01:57:09.700 --> 01:57:10.100]   -But here--
[01:57:10.100 --> 01:57:11.860]   -Yeah, I wish they didn't know Instagram.
[01:57:11.860 --> 01:57:12.820]   -This is a lot of internet you have here.
[01:57:12.820 --> 01:57:13.700]   -Here's the thing.
[01:57:13.700 --> 01:57:15.700]   We know that this is bad.
[01:57:15.700 --> 01:57:18.180]   And most of our listeners know that this is bad.
[01:57:19.140 --> 01:57:23.140]   But Facebook is counting on is that the vast majority,
[01:57:23.140 --> 01:57:25.780]   the general public, more than 90% of the people
[01:57:25.780 --> 01:57:28.420]   are still in that mindset that says,
[01:57:28.420 --> 01:57:30.260]   "Well, I don't care if Facebook has that information.
[01:57:30.260 --> 01:57:31.300]   Why does that affect me?
[01:57:31.300 --> 01:57:32.820]   I don't care if they have my contacts.
[01:57:32.820 --> 01:57:33.940]   I don't care if they have all my email
[01:57:33.940 --> 01:57:35.380]   because I like what they do."
[01:57:35.380 --> 01:57:39.140]   That's the problem is people say this to me all the time.
[01:57:39.140 --> 01:57:40.820]   Well, you can say I quit Facebook.
[01:57:40.820 --> 01:57:41.300]   -Right.
[01:57:41.300 --> 01:57:42.340]   -Because you don't need Facebook.
[01:57:42.340 --> 01:57:44.100]   But there's a lot of us that we need Facebook.
[01:57:44.100 --> 01:57:46.020]   This is how we keep in touch with family, with friends.
[01:57:46.020 --> 01:57:48.660]   In some countries, it's the internet.
[01:57:48.660 --> 01:57:50.500]   It's you because we haven't.
[01:57:50.500 --> 01:57:54.260]   So it's unreasonable for you to say quit Facebook.
[01:57:54.260 --> 01:57:57.780]   Why isn't there a better Facebook somewhere?
[01:57:57.780 --> 01:57:59.140]   Why isn't somebody come along?
[01:57:59.140 --> 01:58:01.380]   -Because there wasn't a better Facebook.
[01:58:01.380 --> 01:58:02.900]   It was Instagram that they bought it.
[01:58:02.900 --> 01:58:04.420]   -Yeah.
[01:58:04.420 --> 01:58:04.820]   -Preach.
[01:58:04.820 --> 01:58:06.020]   Yeah.
[01:58:06.020 --> 01:58:09.380]   Instagram has gone downhill since they bought it though, right?
[01:58:09.380 --> 01:58:10.420]   For what I created.
[01:58:10.420 --> 01:58:11.940]   So I got off Instagram at the same.
[01:58:11.940 --> 01:58:14.260]   I decided I'm not going to use what's up Instagram or Facebook.
[01:58:14.260 --> 01:58:15.860]   All at the same time in August.
[01:58:15.860 --> 01:58:17.620]   My wife when we were in Hawaii said,
[01:58:17.620 --> 01:58:18.980]   "Please create an Instagram account.
[01:58:18.980 --> 01:58:19.860]   Just follow me.
[01:58:19.860 --> 01:58:22.020]   Make it private.
[01:58:22.020 --> 01:58:25.300]   Just follow me so that I don't have to tell you all my pictures."
[01:58:25.300 --> 01:58:26.500]   Because that's where I put them.
[01:58:26.500 --> 01:58:28.660]   She has a private Instagram account.
[01:58:28.660 --> 01:58:29.860]   So I use it that way.
[01:58:29.860 --> 01:58:31.540]   Nevertheless, even just signing up for that,
[01:58:31.540 --> 01:58:35.140]   it asked me no fewer than five times for access to my contact list.
[01:58:35.140 --> 01:58:36.580]   I think it's going to keep doing that.
[01:58:36.580 --> 01:58:38.900]   And we should really point out.
[01:58:38.900 --> 01:58:40.420]   -It hates it when you turn off notifications.
[01:58:40.420 --> 01:58:41.140]   -Oh, I told.
[01:58:41.140 --> 01:58:43.540]   -So you turn off notifications that will remind you 50,000 times.
[01:58:43.540 --> 01:58:45.060]   -And it's constantly suggesting,
[01:58:45.060 --> 01:58:46.260]   because I only follow one person.
[01:58:46.260 --> 01:58:48.020]   My wife, then I follow more people.
[01:58:48.020 --> 01:58:48.900]   They follow more people.
[01:58:48.900 --> 01:58:52.180]   And it's kind of, my feet is filled with all of these
[01:58:52.180 --> 01:58:56.100]   implications from Facebook, not pictures that I want to see.
[01:58:56.100 --> 01:58:57.380]   But why don't you do this?
[01:58:57.380 --> 01:58:58.020]   Why don't you do that?
[01:58:58.020 --> 01:58:58.740]   What's wrong with you?
[01:58:58.740 --> 01:58:59.460]   I do.
[01:58:59.460 --> 01:59:02.340]   So that, but what's interesting is only following one person.
[01:59:02.340 --> 01:59:03.460]   I haven't seen one ad yet.
[01:59:03.460 --> 01:59:04.500]   I don't know why that is.
[01:59:04.500 --> 01:59:06.500]   -Wow, that is interesting.
[01:59:06.500 --> 01:59:06.820]   -Wow.
[01:59:06.820 --> 01:59:10.820]   -Maybe I just, maybe when there's more pictures, I'll see them.
[01:59:10.820 --> 01:59:12.340]   I don't know if you figured out the secret.
[01:59:12.340 --> 01:59:13.220]   You unlock the company.
[01:59:13.220 --> 01:59:15.380]   -I think if you have fewer than nine pictures in your feet,
[01:59:15.380 --> 01:59:16.740]   because it adds every ninth picture,
[01:59:16.740 --> 01:59:19.060]   if you have fewer than nine pictures from your feet,
[01:59:19.060 --> 01:59:19.860]   you don't get an ad.
[01:59:19.860 --> 01:59:23.220]   -Well, I feel like I have more ads than every nine pictures.
[01:59:23.220 --> 01:59:23.700]   -Oh, man.
[01:59:23.700 --> 01:59:26.420]   And part of the reasons I got rid of Facebook is,
[01:59:26.420 --> 01:59:29.140]   I mean, Instagram is because I kept buying Instagram.
[01:59:29.140 --> 01:59:31.620]   That was terrible.
[01:59:31.620 --> 01:59:33.380]   Oh, those underpants look great.
[01:59:33.380 --> 01:59:34.180]   I'll buy those.
[01:59:34.180 --> 01:59:35.620]   I mean, constantly.
[01:59:35.620 --> 01:59:38.420]   -I feel like seven sizes smaller and like,
[01:59:38.420 --> 01:59:39.620]   made of one layer.
[01:59:39.620 --> 01:59:39.780]   -They do have good ads.
[01:59:39.780 --> 01:59:40.740]   I will say, yeah.
[01:59:40.740 --> 01:59:42.820]   -If the ads are so well-heard of ads.
[01:59:42.820 --> 01:59:43.620]   -If the ads are so well-heard of ads.
[01:59:43.620 --> 01:59:44.660]   -If the ads are so well-heard of ads.
[01:59:44.660 --> 01:59:45.140]   -What do you get?
[01:59:45.140 --> 01:59:48.980]   -I get the weirdest ads because I have very weird internet habits
[01:59:48.980 --> 01:59:50.180]   because I use it for work.
[01:59:50.180 --> 01:59:52.900]   And all of my ads, right now,
[01:59:52.900 --> 01:59:56.020]   all of my ads recently have been for medical-related stuff.
[01:59:56.020 --> 01:59:59.060]   I get random prescription medication ads.
[01:59:59.060 --> 02:00:01.220]   I get ads for fake doctors.
[02:00:01.220 --> 02:00:04.180]   I get ads for teletherapy services that are definitely illegal.
[02:00:04.180 --> 02:00:08.340]   As well as ads for medications for migraines
[02:00:08.340 --> 02:00:10.340]   that if you look in the mind text,
[02:00:10.340 --> 02:00:12.980]   they're definitely just some fake herbal thing.
[02:00:14.020 --> 02:00:15.380]   My Instagram ads are very strange.
[02:00:15.380 --> 02:00:18.900]   -What are you doing that you're getting those ads?
[02:00:18.900 --> 02:00:22.260]   -I think it's that I oftentimes will click on them
[02:00:22.260 --> 02:00:24.900]   or take screenshots then because I think, I don't know.
[02:00:24.900 --> 02:00:26.660]   -You're re-sharing those ads.
[02:00:26.660 --> 02:00:27.460]   -Relaying.
[02:00:27.460 --> 02:00:29.460]   I'm working on stories related to kind of like,
[02:00:29.460 --> 02:00:32.580]   the medical industries take over of Instagram.
[02:00:32.580 --> 02:00:35.460]   So I guess my engagement with them
[02:00:35.460 --> 02:00:37.540]   are the fact that I'll sometimes screenshot them.
[02:00:37.540 --> 02:00:38.900]   Means show me more and more,
[02:00:38.900 --> 02:00:42.980]   which I guess works because it's better fodder for my reporting.
[02:00:42.980 --> 02:00:45.700]   -But it looks like I quit Instagram in the nick of time.
[02:00:45.700 --> 02:00:49.060]   I'm buying drugs all the time.
[02:00:49.060 --> 02:00:50.500]   -I can't get you.
[02:00:50.500 --> 02:00:52.180]   -It goes from underpants to drugs.
[02:00:52.180 --> 02:00:56.580]   -I get ads for restaurants and yoga places in Seattle and Portland.
[02:00:56.580 --> 02:01:02.180]   This is a side effect of you fuzzing your social media presence.
[02:01:02.180 --> 02:01:05.460]   -I actually have devices in both those cities that are fuzzing.
[02:01:05.460 --> 02:01:07.700]   And so all my ads get messed up.
[02:01:07.700 --> 02:01:08.340]   -Wait a minute.
[02:01:08.340 --> 02:01:09.780]   What did you buy a storage locker?
[02:01:10.420 --> 02:01:13.220]   -Set up an iPhone, plug it in and now you remotely access it.
[02:01:13.220 --> 02:01:15.460]   How do you do, how do you have a phone in those cities?
[02:01:15.460 --> 02:01:19.700]   -The Society of Jesus in the United States has 65 high schools,
[02:01:19.700 --> 02:01:23.540]   28 colleges and universities and over 400 parishes and research centers.
[02:01:23.540 --> 02:01:27.700]   So pretty much every state, every city, I've got something running somewhere.
[02:01:27.700 --> 02:01:29.620]   -What do you give it to a teenager?
[02:01:29.620 --> 02:01:30.580]   Here's five bucks, kid.
[02:01:30.580 --> 02:01:31.780]   Keep the network going.
[02:01:31.780 --> 02:01:33.780]   -No, I have the keys to the data centers.
[02:01:33.780 --> 02:01:38.900]   -These guys deeper than anybody really understands.
[02:01:38.900 --> 02:01:39.460]   -It's fun stuff.
[02:01:39.460 --> 02:01:41.460]   -He's kind of like Opus Day.
[02:01:41.460 --> 02:01:42.500]   -Oh no.
[02:01:42.500 --> 02:01:45.140]   -He's like, "Oh no."
[02:01:45.140 --> 02:01:47.620]   -Look, when Opus Day talks about buying new notebooks,
[02:01:47.620 --> 02:01:49.220]   they actually mean notebooks with papers.
[02:01:49.220 --> 02:01:51.060]   -Physical notebooks, you actually.
[02:01:51.060 --> 02:01:51.940]   We need vellum.
[02:01:51.940 --> 02:01:52.660]   Can we get vellum?
[02:01:52.660 --> 02:01:53.540]   Where can we get vellum?
[02:01:53.540 --> 02:01:57.140]   -Oh, anyway, what do you get?
[02:01:57.140 --> 02:01:58.820]   And what does it make this complete?
[02:01:58.820 --> 02:02:00.900]   Micah, in your Facebook ads, what do you get?
[02:02:00.900 --> 02:02:05.700]   -On Instagram, I get like just cool products, things that I want.
[02:02:05.700 --> 02:02:12.180]   It'll be some weird lamp, but I'm like, that's literally the lamp that's perfect for me.
[02:02:12.180 --> 02:02:17.940]   Or some new cup to put hot beverages in, or occasionally, yeah, like underpants,
[02:02:17.940 --> 02:02:21.380]   or a new pair of pants, or a shirt, or something.
[02:02:21.380 --> 02:02:24.500]   And I'm like, how do you know that this is exactly what I want?
[02:02:24.500 --> 02:02:28.580]   So I have to be careful too, because Instagram has my number for sure.
[02:02:28.580 --> 02:02:33.220]   -I might need to stop fuzzing my data just so I could see this magic.
[02:02:33.220 --> 02:02:34.260]   -That's a nice ad.
[02:02:34.260 --> 02:02:36.260]   That might actually be eye opening for me.
[02:02:36.260 --> 02:02:37.860]   I have never received.
[02:02:37.860 --> 02:02:40.180]   -Facebook ads, terrible Instagram ads.
[02:02:40.180 --> 02:02:40.740]   Great.
[02:02:40.740 --> 02:02:41.380]   That is great.
[02:02:41.380 --> 02:02:41.380]   -Great.
[02:02:41.380 --> 02:02:44.820]   Which is weird, because it's the same company, and they have the same data set.
[02:02:44.820 --> 02:02:47.540]   So why would there not be better ads on Facebook?
[02:02:47.540 --> 02:02:53.460]   -Maybe it's a better job of understanding what we like.
[02:02:53.460 --> 02:03:00.260]   Like with Facebook, for my use of it, that's different versus who I follow,
[02:03:00.260 --> 02:03:02.580]   and what I, in the Discovery tab.
[02:03:02.580 --> 02:03:05.300]   I think about this a lot when I'm in the Discovery tab.
[02:03:05.300 --> 02:03:09.620]   I wonder, does it know how long I linger on a section of the Discovery tab?
[02:03:09.620 --> 02:03:09.940]   -Oh, absolutely.
[02:03:09.940 --> 02:03:11.220]   -I'm sure it does.
[02:03:11.220 --> 02:03:14.980]   And to tap on a video is going to take me more, because actually,
[02:03:14.980 --> 02:03:21.380]   Megan Moroni was tweeting the other day, she keeps getting these ads for a mushroom coffee.
[02:03:21.380 --> 02:03:25.060]   First of all, why would you ever do that to yourself?
[02:03:25.060 --> 02:03:28.180]   But apparently she's bought it, but she was asking kind of like,
[02:03:28.180 --> 02:03:31.140]   "She's gotten it," and then somebody else she knows has gotten those ads.
[02:03:31.140 --> 02:03:33.700]   How come they both were getting those ads?
[02:03:33.700 --> 02:03:37.140]   And I think it depends on who you're following, and what you like.
[02:03:37.140 --> 02:03:39.860]   And yes, of course, how they developed a profile of you.
[02:03:39.860 --> 02:03:45.220]   But maybe just for me, Instagram has a better understanding of what I actually am interested in,
[02:03:45.220 --> 02:03:51.140]   and like, and my age group, and all of those things that apply as well.
[02:03:51.140 --> 02:03:51.620]   I don't know.
[02:03:51.620 --> 02:03:54.340]   -I think the difference is that on Instagram,
[02:03:54.340 --> 02:03:56.660]   the ads are specifically designed in a way that, like,
[02:03:56.660 --> 02:03:59.460]   Instagram is a visual platform, and it is supposed to be the things,
[02:03:59.460 --> 02:04:01.540]   and they're looking at our visually appealing and artistic,
[02:04:01.540 --> 02:04:05.460]   and so all of the ads also fit into that profile as to not break up the feed.
[02:04:05.460 --> 02:04:06.900]   -That's a good point.
[02:04:06.900 --> 02:04:12.500]   -I actually, when Megan tweeted that, I mentioned that a couple of months ago,
[02:04:12.500 --> 02:04:17.220]   somebody just out of the blue mailed me a mushroom beverage.
[02:04:17.220 --> 02:04:19.300]   -No, I'll call MUD.
[02:04:19.300 --> 02:04:20.260]   -Don't do it.
[02:04:20.260 --> 02:04:22.660]   -MUD/WTR.
[02:04:22.660 --> 02:04:23.540]   -Yeah, MUD water.
[02:04:23.540 --> 02:04:24.420]   -No.
[02:04:24.420 --> 02:04:27.380]   -It's an amalgam of organic, earth-grown ingredients,
[02:04:27.380 --> 02:04:29.300]   loaded by cultures old and young.
[02:04:29.300 --> 02:04:33.780]   It has racy, cordyceps, chock, lion's mane, cacao.
[02:04:33.780 --> 02:04:34.900]   -Lions mane?
[02:04:34.900 --> 02:04:36.020]   -That's a mushroom.
[02:04:36.020 --> 02:04:37.780]   Cinnamon, not real lion's mane.
[02:04:37.780 --> 02:04:39.620]   -That is a very attractive container of nope.
[02:04:39.620 --> 02:04:43.860]   I actually subscribed for a few months.
[02:04:43.860 --> 02:04:45.540]   -No, you tried it.
[02:04:45.540 --> 02:04:46.100]   -Did you like it?
[02:04:46.100 --> 02:04:46.500]   -No, I did.
[02:04:46.500 --> 02:04:46.900]   I tried.
[02:04:46.900 --> 02:04:47.940]   I subscribed to it.
[02:04:47.940 --> 02:04:52.500]   I don't know what I, again, like I said, and I shouldn't admit this in public,
[02:04:52.500 --> 02:04:54.900]   because it's going to be a problem, but I am a sucker,
[02:04:55.860 --> 02:04:57.620]   and I just fall for all that stuff.
[02:04:57.620 --> 02:04:59.620]   -Is that just like another soillent type thing?
[02:04:59.620 --> 02:05:01.540]   -It's a proprietary mushroom blend.
[02:05:01.540 --> 02:05:06.340]   -They don't like those words together.
[02:05:06.340 --> 02:05:10.740]   -Who should not be used next to each other?
[02:05:10.740 --> 02:05:12.900]   -By the way, we didn't mention this, we were talking about Microsoft,
[02:05:12.900 --> 02:05:18.020]   but they are the now the third US company to get it to a trillion dollar market cap.
[02:05:18.020 --> 02:05:19.700]   Is that not amazing?
[02:05:19.700 --> 02:05:20.580]   First it was Apple.
[02:05:21.940 --> 02:05:27.700]   August 2nd Amazon on September 4th, Microsoft, because they had a great fourth quarter,
[02:05:27.700 --> 02:05:28.660]   their results look good.
[02:05:28.660 --> 02:05:30.340]   What a turnaround for this company.
[02:05:30.340 --> 02:05:34.500]   What an amazing story worth a trillion dollars.
[02:05:34.500 --> 02:05:36.340]   -Which is why Amazon is trying to copy it.
[02:05:36.340 --> 02:05:41.940]   I mean, Microsoft did what Amazon did, which is they deprecated the traditional products
[02:05:41.940 --> 02:05:42.900]   that they had offered.
[02:05:42.900 --> 02:05:44.420]   -They pivoted the whole time.
[02:05:44.420 --> 02:05:45.300]   -Oh, yeah, absolutely.
[02:05:45.300 --> 02:05:49.060]   It's a hard pivot to services and cloud, and that's where they've made their money,
[02:05:49.060 --> 02:05:50.660]   and that's why they continue to grow.
[02:05:50.660 --> 02:05:54.900]   Amazon, we know that worked there, and Apple is looking at that and starting to realize
[02:05:54.900 --> 02:05:58.340]   their product line is looking a little old, it's looking a little too traditional,
[02:05:58.340 --> 02:06:00.980]   and they want some of that servicing cloud money.
[02:06:00.980 --> 02:06:01.220]   -Yeah.
[02:06:01.220 --> 02:06:07.940]   Let's take a break and we'll wrap it up with some quick hits from the bottom of the barrel.
[02:06:07.940 --> 02:06:11.380]   With the mud water.
[02:06:11.380 --> 02:06:12.340]   -With the mud water.
[02:06:12.340 --> 02:06:14.820]   -Wait, wait, wait, the mud water wasn't the bottom of the barrel?
[02:06:14.820 --> 02:06:16.340]   -Oh, it's fair, fair.
[02:06:16.340 --> 02:06:17.540]   -We will go even lower.
[02:06:18.340 --> 02:06:21.940]   I'm sure they brought to you by, "Oh, we just got the new Casper mattress.
[02:06:21.940 --> 02:06:24.100]   Darn it, I wanted to bring you a video.
[02:06:24.100 --> 02:06:25.700]   I will bring you a video with it."
[02:06:25.700 --> 02:06:26.820]   These guys are amazing.
[02:06:26.820 --> 02:06:30.900]   Casper is an online retailer of premium mattresses.
[02:06:30.900 --> 02:06:35.060]   They were the first to realize the mattress markup in the store is crazy.
[02:06:35.060 --> 02:06:37.540]   Why don't we just sell direct?
[02:06:37.540 --> 02:06:39.860]   We can manufacture it here in the US.
[02:06:39.860 --> 02:06:45.220]   We can innovate, make amazing mattresses, and by selling it directly to customers
[02:06:45.220 --> 02:06:48.500]   instead of through a store, we can make the price so much lower.
[02:06:48.500 --> 02:06:51.940]   They literally revolutionize the mattress industry.
[02:06:51.940 --> 02:06:56.820]   Of course, they realize one of the problems with that is people think they like to go to a mattress
[02:06:56.820 --> 02:06:59.060]   store and lie on the mattress before they buy it.
[02:06:59.060 --> 02:07:03.220]   But I would submit having fallen to this many times.
[02:07:03.220 --> 02:07:07.860]   You can't tell how good a mattress is going to be by lying in the middle of the day on a
[02:07:07.860 --> 02:07:10.580]   straightly, a straightly, straightly, store while a shop girl looks at you.
[02:07:10.580 --> 02:07:13.300]   Like, "Don't get your feet on my mattress."
[02:07:13.940 --> 02:07:15.620]   You can't test a mattress that way.
[02:07:15.620 --> 02:07:17.140]   Casper has so much of a better way.
[02:07:17.140 --> 02:07:22.020]   They now they send it to you and you have free delivery and painless returns
[02:07:22.020 --> 02:07:22.980]   within 100 days.
[02:07:22.980 --> 02:07:25.060]   You have 100 nights to sleep on that mattress.
[02:07:25.060 --> 02:07:29.620]   Really, if you like it, this new hybrid mattress is amazing.
[02:07:29.620 --> 02:07:33.380]   All of the Casper mattress is amazing.
[02:07:33.380 --> 02:07:38.660]   The original Casper mattress combines multiple supportive memory foams for a sleep surface
[02:07:38.660 --> 02:07:40.660]   with just the right sink, just the right bounce.
[02:07:42.260 --> 02:07:43.700]   All of the mattresses are breathable.
[02:07:43.700 --> 02:07:48.020]   You sleep cool, really important to be able to regulate your temperature through the
[02:07:48.020 --> 02:07:49.460]   night. That's what keeps you from waking up.
[02:07:49.460 --> 02:07:55.700]   Long lasting comfort and support, and it's risk free to buy it online.
[02:07:55.700 --> 02:07:58.180]   They just came out with this new hybrid.
[02:07:58.180 --> 02:07:58.980]   Take a look at that.
[02:07:58.980 --> 02:08:05.300]   I don't know how they do what they do, but they make mattresses that are
[02:08:05.300 --> 02:08:09.940]   both things that I want because I want a soft mattress because I want to be comfortable.
[02:08:09.940 --> 02:08:14.420]   I want to hit my hips to dig into it or anything, but I also want firm because I want firm support
[02:08:14.420 --> 02:08:19.860]   for my back and somehow Casper makes a mattress that's both and they keep innovating.
[02:08:19.860 --> 02:08:20.820]   They keep inventing.
[02:08:20.820 --> 02:08:23.460]   There's a bunch of different kinds. Go to the website, look at it.
[02:08:23.460 --> 02:08:28.820]   All the Casper mattresses uphold the highest environmental production standards.
[02:08:28.820 --> 02:08:35.140]   They're made in the USA with free shipping and, yes, free returns to the US and Canada.
[02:08:35.860 --> 02:08:41.300]   You could pick the perfect mattress for you at Casper.com/twit. In fact, you'll save $50
[02:08:41.300 --> 02:08:46.180]   towards select mattresses if you use the promo code TWIT1 at checkout.
[02:08:46.180 --> 02:08:52.740]   I just love these mattresses. Casper.com/twit, promo code TWIT1 at checkout,
[02:08:52.740 --> 02:08:58.900]   terms and conditions apply. We have two new Casper sitting in our entryway of our house,
[02:08:58.900 --> 02:09:04.100]   and I will promise you for the next time I will do a video because I love to show the unboxing.
[02:09:04.100 --> 02:09:08.660]   They're so compact that we just got two California Kings, which is the biggest they make.
[02:09:08.660 --> 02:09:14.580]   It's in a box. Not that big. You open it up and it goes, "You can get it up.
[02:09:14.580 --> 02:09:21.540]   Apartment stairs easily." It's amazing. Love our Casper's Casper.com/twit.
[02:09:21.540 --> 02:09:30.820]   I got to feel for Hertz because I know the feeling when you get a brand new website
[02:09:30.820 --> 02:09:38.180]   and the cost keeps going up and up and up. Actually, Hertz is so upset about the website that Accenture,
[02:09:38.180 --> 02:09:45.700]   yeah, that Accenture pretended to be making for them, that they're demanding $32 million.
[02:09:45.700 --> 02:09:52.260]   They never got a site or a mobile app. It wasn't responsive design in the first place.
[02:09:52.260 --> 02:09:57.380]   None of the things Accenture said they would do, they did, and then they said, "Well, we'll do it,
[02:09:57.380 --> 02:10:05.140]   but you have to pay more." It is a textbook story of how a website design can go wrong.
[02:10:05.140 --> 02:10:11.860]   I know exactly how they feel. The code, as Hertz says, was terrible in a security nightmare.
[02:10:11.860 --> 02:10:18.100]   Accenture's developers wrote the code for the customer-facing e-commerce website in a way
[02:10:18.100 --> 02:10:24.260]   that created serious security vulnerabilities and performance problems. The defects in the front-end
[02:10:24.260 --> 02:10:29.140]   development code were so pervasive that all of Accenture's work on that component had to be
[02:10:29.140 --> 02:10:36.020]   scrapped. It was built on Angular 2. Accenture said, "Oh, we could speed it up. We need to use
[02:10:36.020 --> 02:10:40.740]   something called rapid, but it hurts. You got to buy the licenses for it. Hertz bought the licenses,
[02:10:40.740 --> 02:10:43.620]   and then it turned out Accenture didn't know how to use rapid."
[02:10:43.620 --> 02:10:53.140]   The quick fix took longer than it would have done without it. Accenture admitted that they had
[02:10:53.140 --> 02:10:57.380]   spent a good deal of time fighting through the integration of rapid and Earths. I could just go
[02:10:57.380 --> 02:11:02.660]   on and on and on." A spokesperson for Accenture told the Register, "We believe the allegations
[02:11:02.660 --> 02:11:06.260]   in this lawsuit are without merit, and we intend to defend our position."
[02:11:06.260 --> 02:11:15.620]   But I tell you, I read this and I thought, "Oh boy, I have been there. I feel for you, Hertz."
[02:11:16.660 --> 02:11:24.100]   Big, big news, I think, in the Academy Awards arena. Remember, after the last Oscars,
[02:11:24.100 --> 02:11:29.540]   when Roma came perilously close to winning a best picture, even though it was a Netflix
[02:11:29.540 --> 02:11:35.940]   streaming movie, but what a great movie. It did win a couple of Oscars, but it didn't get the big
[02:11:35.940 --> 02:11:40.420]   Oscar for Best Picture. After that, Steven Spielberg said, "We ought to change the rules,
[02:11:40.420 --> 02:11:44.260]   and no movie that is not from a theater should be eligible for an Oscar."
[02:11:44.260 --> 02:11:49.140]   I'm surprised here. What was one of the major partners for Apple's announcement?
[02:11:49.140 --> 02:11:54.500]   I forget when their new streaming service? Not Netflix. He was Steven Spielberg, wasn't he?
[02:11:54.500 --> 02:12:02.020]   Oh, yeah, weirdly enough. Steven Spielberg, but he was only doing TV shows. He's doing his TV
[02:12:02.020 --> 02:12:10.020]   show. Not a movie. Spielberg wanted to change the rules, and weirdly, the Department of Justice
[02:12:10.020 --> 02:12:16.180]   got involved, which is very weird, and wrote a letter to the Academy saying, "If you change
[02:12:16.180 --> 02:12:23.940]   the rules, you'll be in violation of competition law. If you vote to make streaming movies ineligible
[02:12:23.940 --> 02:12:31.940]   for Oscar nominations, I have to think that somebody like maybe Jeff Bezos at Amazon said,
[02:12:32.500 --> 02:12:39.940]   it would be very kind of you to write them a note." Here's the good news. The Academy of Motion
[02:12:39.940 --> 02:12:45.300]   Pictures Arts and Sciences announced that he will allow streaming services like Netflix, Hulu,
[02:12:45.300 --> 02:12:53.220]   and Amazon Prime to be eligible for Academy Award nominations. However, they still have to run
[02:12:53.220 --> 02:12:58.900]   in a Los Angeles County commercial theater for at least seven days, with at least three
[02:12:58.900 --> 02:13:02.660]   screenings a day for paid admission to be eligible. That's what Roman did.
[02:13:02.660 --> 02:13:05.460]   That's really easy. You just buy a theater out from a week.
[02:13:05.460 --> 02:13:13.380]   So you are eligible as long as you do that LA theater thing, and they decided not to change
[02:13:13.380 --> 02:13:20.980]   the rule that would make Amazon, Netflix, Hulu ineligible. So obviously, that's the right answer,
[02:13:20.980 --> 02:13:25.060]   right? Part of me wants to think that there's someone at the DOJ who said, "Okay, let's just
[02:13:25.060 --> 02:13:29.140]   nip this in the bud before it gets crazy and just say, 'Look, you have to maintain
[02:13:29.140 --> 02:13:33.460]   an openness and otherwise you're going to be in antitrust territory.'"
[02:13:33.460 --> 02:13:38.580]   The other part of me thinks the people at the DOJ, which has just been beat up the last couple
[02:13:38.580 --> 02:13:45.540]   of years, wanted an easy win. I said, "Look, this is pretty clear by law, and this makes us look
[02:13:45.540 --> 02:13:48.420]   good and it makes us look like we're looking at the heroes."
[02:13:48.420 --> 02:13:55.140]   Yeah. I mean, Netflix in January joined the Motion Picture Association of America.
[02:13:55.140 --> 02:13:58.020]   I'm pretty sure they have some poll from that.
[02:13:58.020 --> 02:14:04.500]   Well, more people watch movies on Netflix than watch in theaters in the United States.
[02:14:04.500 --> 02:14:05.940]   Oh, definitely.
[02:14:05.940 --> 02:14:09.300]   You can't really cut them out, can you?
[02:14:09.300 --> 02:14:16.660]   I just like the headline in this story from the next web.com. You can't take my door.
[02:14:17.700 --> 02:14:21.300]   It's both country music and AI's greatest achievement.
[02:14:21.300 --> 02:14:23.940]   It is an AI...
[02:14:23.940 --> 02:14:24.740]   I'm intrigued.
[02:14:24.740 --> 02:14:25.300]   Yeah, right?
[02:14:25.300 --> 02:14:26.340]   It is an AI...
[02:14:26.340 --> 02:14:27.300]   Oh, it's botnik.
[02:14:27.300 --> 02:14:28.580]   It's a botnik.
[02:14:28.580 --> 02:14:34.420]   It's an AI that generated a song performed by humans, Elle, Brian, and Timothy Joyce,
[02:14:34.420 --> 02:14:38.500]   written by a recursive neural network. You want to hear it?
[02:14:38.500 --> 02:14:39.700]   Yes.
[02:14:39.700 --> 02:14:42.020]   You can't take my door.
[02:14:42.020 --> 02:14:44.020]   [Music]
[02:14:44.020 --> 02:15:08.660]   I mean, this is what most country songs sound like to me.
[02:15:09.860 --> 02:15:10.420]   No.
[02:15:10.420 --> 02:15:21.300]   Oh my god. That's the best song I ever heard.
[02:15:21.300 --> 02:15:22.020]   I love it.
[02:15:22.020 --> 02:15:24.740]   But to be clear, the lyrics were created by an AI.
[02:15:24.740 --> 02:15:25.940]   Yeah, not the song.
[02:15:25.940 --> 02:15:28.420]   Yeah, it was sung by real people.
[02:15:28.420 --> 02:15:33.860]   But if I did it like this, did it only create the lyrics or did it also create the music for it?
[02:15:33.860 --> 02:15:36.020]   Like the chords or something.
[02:15:36.020 --> 02:15:39.780]   Was it just the lyrics and they figured out how to make it work as a song?
[02:15:39.780 --> 02:15:43.700]   It looks like it was written by the recursive neural network.
[02:15:43.700 --> 02:15:44.340]   So I guess...
[02:15:44.340 --> 02:15:47.060]   But did they do lyrics and music or just lyrics?
[02:15:47.060 --> 02:15:48.420]   It sounds like they just sung it.
[02:15:48.420 --> 02:15:52.260]   I think they just did lyrics and then it has to be a little same in format.
[02:15:52.260 --> 02:15:52.660]   Yeah.
[02:15:52.660 --> 02:15:54.180]   Yeah.
[02:15:54.180 --> 02:15:59.540]   Botnik is really interesting because it's this studio that kind of works in the edge of comedy and tech.
[02:15:59.540 --> 02:16:03.460]   They have predictive keyboards as well as neural networks where they kind of pair
[02:16:03.940 --> 02:16:07.140]   comedians with AI and they end up...
[02:16:07.140 --> 02:16:13.620]   A lot of them popular kind of viral, a bot made ex-tweets come from botnik.
[02:16:13.620 --> 02:16:14.580]   I don't know if you've seen...
[02:16:14.580 --> 02:16:19.860]   There was one like an AI-generated Harry Potter script or maybe like Seinfeld scripts or a
[02:16:19.860 --> 02:16:20.900]   Coachella lineup.
[02:16:20.900 --> 02:16:23.460]   Botnik does a lot of really cool stuff like this.
[02:16:23.460 --> 02:16:24.580]   And I think this...
[02:16:24.580 --> 02:16:29.140]   I'm not sure if it's this song in particular, but they have a thing called the songularity,
[02:16:29.140 --> 02:16:31.460]   which is an entire album created by AI.
[02:16:31.460 --> 02:16:37.860]   So I just asked Botnik because you could do this on their website to create a Jerry Seinfeld monologue.
[02:16:37.860 --> 02:16:41.380]   I think it's doing it right now.
[02:16:41.380 --> 02:16:43.220]   So this is a predictive text keyboard.
[02:16:43.220 --> 02:16:45.700]   So you have to click one word and then I think it'll...
[02:16:45.700 --> 02:16:49.940]   Like it generates all these words and then you can
[02:16:49.940 --> 02:16:53.220]   use this to kind of lead the album.
[02:16:53.220 --> 02:16:53.220]   Ahhh.
[02:16:53.220 --> 02:16:55.380]   Yeah.
[02:16:55.380 --> 02:16:56.660]   Oh, so I'm doing it.
[02:16:56.660 --> 02:16:58.180]   The other night he died.
[02:16:58.180 --> 02:16:59.940]   Whatever.
[02:17:00.500 --> 02:17:02.020]   [laughter]
[02:17:02.020 --> 02:17:03.300]   I can't have had enough.
[02:17:03.300 --> 02:17:04.260]   This is poetry.
[02:17:04.260 --> 02:17:04.580]   Yeah.
[02:17:04.580 --> 02:17:08.420]   It's not quite as good as you can't leave my door.
[02:17:08.420 --> 02:17:09.540]   You can't take my door.
[02:17:09.540 --> 02:17:11.380]   I don't want to love you anymore.
[02:17:11.380 --> 02:17:13.860]   Won't let my heart be my face.
[02:17:13.860 --> 02:17:17.620]   Barbed whiskey good and whiskey straight.
[02:17:17.620 --> 02:17:19.700]   No, you can't take my door.
[02:17:19.700 --> 02:17:20.340]   That is awesome.
[02:17:20.340 --> 02:17:21.620]   I keep those in my heart.
[02:17:21.620 --> 02:17:22.980]   Yeah, those are good lyrics.
[02:17:22.980 --> 02:17:28.340]   It's whimsical, but this is actually important because one of the big failings of AI neural networks
[02:17:28.340 --> 02:17:30.660]   has been it's not good at ending things.
[02:17:30.660 --> 02:17:32.180]   I'll give you an example.
[02:17:32.180 --> 02:17:34.260]   You're going back to MIT and their Media Lab.
[02:17:34.260 --> 02:17:39.300]   They tried to train an AI to look at thousands of YouTube videos
[02:17:39.300 --> 02:17:41.300]   to see if it could complete the video.
[02:17:41.300 --> 02:17:44.580]   So could it understand that there's actually a narrative going on?
[02:17:44.580 --> 02:17:45.540]   It would give it...
[02:17:45.540 --> 02:17:46.500]   You understand the context?
[02:17:46.500 --> 02:17:47.060]   Can you see what...
[02:17:47.060 --> 02:17:47.380]   Yeah.
[02:17:47.380 --> 02:17:47.380]   Yeah.
[02:17:47.380 --> 02:17:47.860]   Yeah.
[02:17:47.860 --> 02:17:51.780]   So for example, one of the things that they had a video of a train
[02:17:51.780 --> 02:17:53.940]   coming into a station and then leaving the station.
[02:17:53.940 --> 02:17:57.540]   And the AI was able to actually reproduce
[02:17:58.100 --> 02:17:59.140]   the train.
[02:17:59.140 --> 02:18:02.020]   And it knew that there was a train and there were cars and they were connected
[02:18:02.020 --> 02:18:03.620]   and they were all moving in this direction.
[02:18:03.620 --> 02:18:05.220]   But the thing is it never stopped.
[02:18:05.220 --> 02:18:08.340]   It never knew that at some point the train had to end.
[02:18:08.340 --> 02:18:10.340]   And they could never get the AI to understand that.
[02:18:10.340 --> 02:18:14.980]   If this was generated by an AI, this is an AI that actually understands
[02:18:14.980 --> 02:18:17.780]   that there's a certain cadence to the song and then it ends.
[02:18:17.780 --> 02:18:18.500]   Yeah.
[02:18:18.500 --> 02:18:20.180]   So it's pretty good.
[02:18:20.180 --> 02:18:23.460]   Small and it's funny, but that's actually still a very big advancement.
[02:18:23.460 --> 02:18:25.860]   I actually think my chatroom can beat your AI.
[02:18:26.580 --> 02:18:27.220]   Probably.
[02:18:27.220 --> 02:18:28.180]   Probably.
[02:18:28.180 --> 02:18:29.940]   Chatroom already wrote a second lyric.
[02:18:29.940 --> 02:18:31.140]   "You can't take my door.
[02:18:31.140 --> 02:18:33.700]   I want my rumba to wash my kitchen floor."
[02:18:33.700 --> 02:18:38.180]   Any other lyrics you want to add?
[02:18:38.180 --> 02:18:43.860]   I'll be glad to write that up and send it to the Country Music Awards.
[02:18:43.860 --> 02:18:48.820]   How long before we see the Emmys or the CMA Awards say,
[02:18:48.820 --> 02:18:54.260]   "You can't submit bot written songs for our approval?"
[02:18:54.260 --> 02:18:55.460]   Sad news.
[02:18:55.460 --> 02:19:02.740]   You may remember the $16,000 AI-powered Japanese laundry robot.
[02:19:02.740 --> 02:19:04.420]   Did you see that at CES?
[02:19:04.420 --> 02:19:06.180]   Every year for the last five years.
[02:19:06.180 --> 02:19:06.660]   Yeah.
[02:19:06.660 --> 02:19:07.860]   That was part of the problem.
[02:19:07.860 --> 02:19:10.900]   They never really did get it working.
[02:19:10.900 --> 02:19:13.140]   There was an example of the,
[02:19:13.140 --> 02:19:20.500]   this is from I think a CNET of a demo of the Londroid from,
[02:19:20.500 --> 02:19:22.020]   I mean, it looked cool, the idea.
[02:19:22.020 --> 02:19:24.500]   But in order to make this work, they actually,
[02:19:24.500 --> 02:19:27.460]   actually she's telling the story how it ate her shirt.
[02:19:27.460 --> 02:19:28.580]   But let me see if I can find the,
[02:19:28.580 --> 02:19:29.380]   Oh no.
[02:19:29.380 --> 02:19:30.020]   Yeah.
[02:19:30.020 --> 02:19:30.100]   Yeah.
[02:19:30.100 --> 02:19:35.220]   Let me, in any event, nobody apparently wanted to spend that much money
[02:19:35.220 --> 02:19:38.820]   on a laundry folding device that will eat your shirt.
[02:19:38.820 --> 02:19:39.700]   Oh, wait, hold on.
[02:19:39.700 --> 02:19:42.020]   See, this is not the company that went bankrupt.
[02:19:42.020 --> 02:19:44.740]   So this is Foldymate, which we have covered on 12.
[02:19:44.740 --> 02:19:46.420]   Foldymate is still around.
[02:19:46.420 --> 02:19:47.460]   It's still around.
[02:19:47.460 --> 02:19:51.380]   But it's a, I would guess it's, it looks cool.
[02:19:51.380 --> 02:19:54.020]   It looks fun, but it's expensive.
[02:19:54.020 --> 02:19:56.500]   And what he showed this on twig this week,
[02:19:56.500 --> 02:20:02.100]   there's also a laundry robot that is human powered by tele,
[02:20:02.100 --> 02:20:03.540]   it's tele laundry.
[02:20:03.540 --> 02:20:07.780]   So people somewhere, somewhere else will fold your laundry.
[02:20:07.780 --> 02:20:12.820]   My laundry folding is also human powered.
[02:20:12.820 --> 02:20:15.380]   I just fold the laundry with my hands.
[02:20:15.380 --> 02:20:15.700]   Yeah.
[02:20:15.700 --> 02:20:17.060]   It's like Uber for laundry.
[02:20:17.060 --> 02:20:19.780]   I mean, at $16,000, you could just throw away your laundry
[02:20:19.780 --> 02:20:22.660]   and buy new clothes and probably come out equally.
[02:20:22.660 --> 02:20:25.540]   This is a little bit inside baseball,
[02:20:25.540 --> 02:20:28.740]   but I thought I'd mention it to people who have been asking me.
[02:20:28.740 --> 02:20:31.700]   There's a new podcast app out called Luminary.
[02:20:31.700 --> 02:20:33.780]   They raised $100 million.
[02:20:33.780 --> 02:20:37.620]   And there was a bit of a to do over Luminary this week,
[02:20:37.620 --> 02:20:39.620]   because when they launched on Tuesday,
[02:20:39.620 --> 02:20:41.300]   it turned out that they were
[02:20:41.300 --> 02:20:46.580]   caching or proxying the requests to download the podcast
[02:20:46.580 --> 02:20:48.900]   for the podcast on their app so that
[02:20:49.860 --> 02:20:51.220]   they didn't understand this podcast.
[02:20:51.220 --> 02:20:54.340]   That works would be unable to count downloads.
[02:20:54.340 --> 02:20:56.340]   All the downloads would look like they were coming from
[02:20:56.340 --> 02:20:59.780]   a few, a handful of IP addresses in the Luminary network.
[02:20:59.780 --> 02:21:03.060]   And it also turned out some complained that the proxying
[02:21:03.060 --> 02:21:08.020]   was also stripping out links to Patreon sites and other
[02:21:08.020 --> 02:21:09.540]   show note links.
[02:21:09.540 --> 02:21:13.540]   And a number of podcasts, including Joe Rogan,
[02:21:13.540 --> 02:21:17.380]   I think the Daily, a number of podcasts said,
[02:21:17.380 --> 02:21:24.020]   the Gimlet Media shows said, no, you can't put our shows on Luminary.
[02:21:24.020 --> 02:21:29.700]   That may backfire also, because then Luminary on those shows
[02:21:29.700 --> 02:21:33.380]   put a big banner saying, well, these shows won't let us,
[02:21:33.380 --> 02:21:37.060]   won't let you listen to it on the Luminary app.
[02:21:37.060 --> 02:21:40.340]   But here's some other shows you might like better.
[02:21:40.340 --> 02:21:40.740]   Wow.
[02:21:40.740 --> 02:21:43.060]   We declined.
[02:21:43.060 --> 02:21:46.420]   We decided after looking at this just to stay out of it entirely.
[02:21:46.420 --> 02:21:48.660]   We're going to, if you want to listen to our shows on Luminary,
[02:21:48.660 --> 02:21:50.180]   be our guests.
[02:21:50.180 --> 02:21:52.340]   I'm not going to weigh in on this unless they,
[02:21:52.340 --> 02:21:55.700]   you know, do something else awful because they've gone back to
[02:21:55.700 --> 02:21:59.780]   letting us count the downloads, which is really what's most important to us.
[02:21:59.780 --> 02:22:05.300]   So, but a big, a big battle going on over that.
[02:22:05.300 --> 02:22:09.460]   And I think part of the problem was that they raised $100 million for a podcast app.
[02:22:09.460 --> 02:22:12.580]   But subscribe away if you'd like.
[02:22:12.580 --> 02:22:15.300]   I think it's a good time to call it.
[02:22:15.300 --> 02:22:15.940]   What do you think?
[02:22:16.820 --> 02:22:17.780]   One word.
[02:22:17.780 --> 02:22:18.660]   What?
[02:22:18.660 --> 02:22:19.940]   Oh, no, not a game.
[02:22:19.940 --> 02:22:26.340]   I just, I was thinking maybe we should be longer than Avengers Endgame, but
[02:22:26.340 --> 02:22:28.820]   it's a pretty high bar.
[02:22:28.820 --> 02:22:30.180]   Well, we'd have to go.
[02:22:30.180 --> 02:22:32.020]   We're going to have an after the credits scene.
[02:22:32.020 --> 02:22:32.660]   Yes, right.
[02:22:32.660 --> 02:22:34.580]   We are definitely listening to the tone.
[02:22:34.580 --> 02:22:37.620]   Everyone stay for the full credits and listen for the little musical.
[02:22:37.620 --> 02:22:42.100]   In the early days of Twit, 15 years ago, I always had after the credits bloopers and fun stuff.
[02:22:42.100 --> 02:22:42.420]   Yes.
[02:22:42.420 --> 02:22:42.820]   Always.
[02:22:42.820 --> 02:22:45.700]   What happened?
[02:22:45.700 --> 02:22:46.500]   Yeah, what happened?
[02:22:46.500 --> 02:22:48.500]   We stopped making mistakes.
[02:22:48.500 --> 02:22:50.420]   Where'd the fun go?
[02:22:50.420 --> 02:22:52.500]   You're perfect now.
[02:22:52.500 --> 02:22:53.940]   Oh, we have a lot of fun.
[02:22:53.940 --> 02:22:56.020]   I always enjoyed that part of the show myself.
[02:22:56.020 --> 02:22:57.860]   Thank you, Paris Martyno.
[02:22:57.860 --> 02:22:58.820]   So great to have you.
[02:22:58.820 --> 02:23:01.940]   You find Paris's work at Wired Magazine, Wired.com.
[02:23:01.940 --> 02:23:06.980]   And she just wrote another story inside the weird and booming industry of,
[02:23:06.980 --> 02:23:09.860]   and actually that's not new, but it's a great one to read.
[02:23:09.860 --> 02:23:10.500]   Or is it new?
[02:23:10.500 --> 02:23:13.540]   This is your new one inside the weird and booming industry of,
[02:23:13.540 --> 02:23:15.220]   yeah, of online.
[02:23:15.220 --> 02:23:20.820]   Yeah, the newest one is about egg donation, which also came out last week,
[02:23:20.820 --> 02:23:22.260]   but check both of it now.
[02:23:22.260 --> 02:23:24.820]   Inside the wild answer.
[02:23:24.820 --> 02:23:28.420]   Quietly lucrative business of donating human eggs.
[02:23:28.420 --> 02:23:33.220]   And then you have a picture of a goose laying a golden egg.
[02:23:33.220 --> 02:23:34.100]   A golden egg.
[02:23:34.100 --> 02:23:35.620]   Oh, a golden egg.
[02:23:35.620 --> 02:23:36.660]   Okay, I get it.
[02:23:36.660 --> 02:23:37.940]   Nice.
[02:23:37.940 --> 02:23:41.700]   That's actually what babies look like in the womb.
[02:23:41.700 --> 02:23:42.740]   It's just a golden egg.
[02:23:42.740 --> 02:23:43.620]   Yeah, that's an extra.
[02:23:43.620 --> 02:23:44.100]   Yeah.
[02:23:44.100 --> 02:23:47.220]   And now you know why Paris's Instagram ads are so damn weird.
[02:23:47.220 --> 02:23:48.740]   Is that before or after the quick?
[02:23:48.740 --> 02:23:50.020]   I can't remember.
[02:23:50.020 --> 02:23:51.380]   It is always.
[02:23:51.380 --> 02:23:52.180]   It's somewhere in between.
[02:23:52.180 --> 02:23:52.660]   Yeah.
[02:23:52.660 --> 02:23:53.860]   Always a pleasure to have you on.
[02:23:53.860 --> 02:23:56.820]   Thank you for, it looks like it's a little darker in Brooklyn.
[02:23:56.820 --> 02:23:59.060]   Thank you for coming in to work and joining us today.
[02:23:59.060 --> 02:24:00.100]   We really appreciate it.
[02:24:00.100 --> 02:24:01.780]   Thanks so much for having me.
[02:24:01.780 --> 02:24:03.620]   @Parismartyno on the Twitter.
[02:24:03.620 --> 02:24:07.460]   Send her some good leads, some scoops.
[02:24:07.460 --> 02:24:10.100]   And me or Weird Instagram ads, please.
[02:24:10.100 --> 02:24:11.220]   Or Weird Instagram ads.
[02:24:11.220 --> 02:24:12.580]   I think that's your next story.
[02:24:12.580 --> 02:24:14.580]   Weird Instagram ads that people actually buy.
[02:24:14.580 --> 02:24:18.100]   Like underwear and lamps.
[02:24:18.100 --> 02:24:19.860]   That's what lamps.
[02:24:19.860 --> 02:24:21.300]   Lamps, what are they?
[02:24:21.300 --> 02:24:22.100]   Too many lamps.
[02:24:22.100 --> 02:24:23.060]   Weird lamps.
[02:24:23.060 --> 02:24:25.460]   Apparently, Micah has a thing for lamps.
[02:24:25.460 --> 02:24:28.500]   Micah Sergeant, he's at Chihuahua.coffee.
[02:24:28.500 --> 02:24:31.380]   Tell us about your podcast, Micah.
[02:24:31.380 --> 02:24:34.820]   All right, well, I've got a few, but I'll talk about clockwise,
[02:24:34.820 --> 02:24:36.260]   which is on the relay of him network.
[02:24:36.260 --> 02:24:38.100]   It's a 30 minute show.
[02:24:38.100 --> 02:24:39.380]   We have two people on.
[02:24:40.020 --> 02:24:43.460]   Four of us talk about a topic that we bring to the table.
[02:24:43.460 --> 02:24:44.900]   And then if you're into the office,
[02:24:44.900 --> 02:24:48.420]   then you can check out somehow I manage.
[02:24:48.420 --> 02:24:50.660]   That's a podcast on the incomparable network,
[02:24:50.660 --> 02:24:52.020]   which is Jason Snell's network,
[02:24:52.020 --> 02:24:56.740]   where Tiffany Arment and I talk about the office.
[02:24:56.740 --> 02:24:59.460]   And in fact, I'll be recording an episode with her this evening.
[02:24:59.460 --> 02:25:03.460]   So we are, we've been doing it for more than a year now.
[02:25:03.460 --> 02:25:06.100]   And we're just going through every episode of the office.
[02:25:06.100 --> 02:25:09.460]   So if you are in season four, episode four,
[02:25:09.460 --> 02:25:10.260]   for tonight.
[02:25:10.260 --> 02:25:13.220]   Yes, season four, episode four is this evening.
[02:25:13.220 --> 02:25:18.820]   So if you understand why that says lunch party and the one before,
[02:25:18.820 --> 02:25:22.100]   if you can hear that quote in your head, where are the turtles?
[02:25:22.100 --> 02:25:23.300]   This is a show for you.
[02:25:23.300 --> 02:25:23.860]   Absolutely.
[02:25:23.860 --> 02:25:26.980]   Probably the greatest sitcom ever made.
[02:25:26.980 --> 02:25:27.860]   I think it really frets on.
[02:25:27.860 --> 02:25:29.140]   Amen, hands down.
[02:25:29.140 --> 02:25:32.580]   Now, do you recommend, I think I asked you this before,
[02:25:32.580 --> 02:25:35.060]   you don't listen to the podcast while you're watching this show.
[02:25:35.060 --> 02:25:36.580]   Correct.
[02:25:36.580 --> 02:25:38.660]   Yeah, it's not a watch along.
[02:25:38.660 --> 02:25:41.780]   Do you watch the show first, then the podcast, the podcast first, then the show.
[02:25:41.780 --> 02:25:43.780]   You can do either of those.
[02:25:43.780 --> 02:25:47.780]   There are some people who are watching the office for the first time with us.
[02:25:47.780 --> 02:25:51.380]   So the watch, the episode, come and check out the show.
[02:25:51.380 --> 02:25:54.740]   And it's funny because most of them end up going,
[02:25:54.740 --> 02:25:57.620]   I couldn't wait for a new episode of the podcast.
[02:25:57.620 --> 02:26:01.300]   I ended up watching three seasons of the office of two nights.
[02:26:01.300 --> 02:26:03.220]   The office is ultimately binge worthy.
[02:26:03.220 --> 02:26:07.300]   How many office episodes are you about halfway through, right?
[02:26:08.500 --> 02:26:11.460]   Season nine is the last season.
[02:26:11.460 --> 02:26:14.180]   So we're almost exactly halfway through, aren't you?
[02:26:14.180 --> 02:26:15.220]   Well, it gets you right.
[02:26:15.220 --> 02:26:17.460]   Yeah, yeah, yeah, we're getting there.
[02:26:17.460 --> 02:26:18.100]   We're getting there.
[02:26:18.100 --> 02:26:20.820]   And we don't know what we're going to do after we get all the way through.
[02:26:20.820 --> 02:26:23.140]   We might change up the theme a little bit.
[02:26:23.140 --> 02:26:25.060]   So we're going to have to.
[02:26:25.060 --> 02:26:27.060]   I think once you run that episode.
[02:26:27.060 --> 02:26:28.260]   Yeah, you can't just go back.
[02:26:28.260 --> 02:26:28.980]   Kind of done.
[02:26:28.980 --> 02:26:30.420]   Just think.
[02:26:30.420 --> 02:26:30.740]   It's true.
[02:26:30.740 --> 02:26:31.300]   I'm just guessing.
[02:26:31.300 --> 02:26:35.540]   And next time you're in the crypts of the Vatican,
[02:26:35.540 --> 02:26:39.300]   make sure you look up Father Roberts Dungeons and Dragons game.
[02:26:39.300 --> 02:26:40.500]   I was just a replacement.
[02:26:40.500 --> 02:26:43.380]   I was I was filling in for a player who wasn't there that week.
[02:26:43.380 --> 02:26:44.580]   Were the other players priests?
[02:26:44.580 --> 02:26:47.540]   Two months, seniors, a priest and a bishop.
[02:26:47.540 --> 02:26:49.060]   What was your class?
[02:26:49.060 --> 02:26:51.140]   I was a Paladin.
[02:26:51.140 --> 02:26:53.220]   I thought they all be clerics.
[02:26:53.220 --> 02:26:54.420]   Where are they all clerics?
[02:26:54.420 --> 02:26:56.340]   Then would be boring.
[02:26:56.340 --> 02:26:56.820]   I guess they would be.
[02:26:56.820 --> 02:26:58.340]   But your clerics wandering around the countryside,
[02:26:58.340 --> 02:26:59.940]   getting killed every other turn.
[02:26:59.940 --> 02:27:01.220]   You need a Paladin or two.
[02:27:01.220 --> 02:27:01.860]   Is it illegal?
[02:27:01.860 --> 02:27:02.820]   You were chaotic.
[02:27:02.820 --> 02:27:03.780]   Good or good.
[02:27:03.780 --> 02:27:04.980]   No, I was chaotic neutral.
[02:27:04.980 --> 02:27:06.020]   Chaotic neutral.
[02:27:06.020 --> 02:27:07.540]   The best way to play D&D.
[02:27:07.540 --> 02:27:08.260]   That's the way to be.
[02:27:08.260 --> 02:27:09.620]   Is it the best way to play D&D?
[02:27:09.620 --> 02:27:12.740]   My parents just come with you being a human being.
[02:27:12.740 --> 02:27:13.700]   Why is that Paris?
[02:27:13.700 --> 02:27:14.500]   Explain that to me.
[02:27:14.500 --> 02:27:16.180]   Oh, I don't know.
[02:27:16.180 --> 02:27:17.620]   Because I think then chaotic neutral.
[02:27:17.620 --> 02:27:20.500]   You get more choices.
[02:27:20.500 --> 02:27:21.700]   You aren't held to be like,
[02:27:21.700 --> 02:27:23.940]   oh, I'm just going to make all the good choices all the time.
[02:27:23.940 --> 02:27:26.020]   And you obviously aren't playing an evil character.
[02:27:26.020 --> 02:27:27.780]   You're unpredictable.
[02:27:27.780 --> 02:27:28.740]   No one will know.
[02:27:28.740 --> 02:27:29.220]   Yeah.
[02:27:29.220 --> 02:27:29.700]   What you're going to do.
[02:27:29.700 --> 02:27:31.460]   Do you have a name for your,
[02:27:31.460 --> 02:27:33.380]   do you use the same name all the time or different names?
[02:27:33.940 --> 02:27:37.940]   Oh, my current character is a mall elf,
[02:27:37.940 --> 02:27:40.260]   an elf that grew up in the crawl space of a mall.
[02:27:40.260 --> 02:27:41.220]   It's a druid.
[02:27:41.220 --> 02:27:44.180]   And the character is named after Claire's.
[02:27:44.180 --> 02:27:46.180]   So it's Claire's shadow's name.
[02:27:46.180 --> 02:27:46.660]   Has a.
[02:27:46.660 --> 02:27:48.740]   That sounds like the best.
[02:27:48.740 --> 02:27:49.220]   Hey.
[02:27:49.220 --> 02:27:52.980]   Is it a tabletop D&D game that you go to or is it online?
[02:27:52.980 --> 02:27:54.660]   It's the idea.
[02:27:54.660 --> 02:27:55.220]   It's good too.
[02:27:55.220 --> 02:27:56.580]   I mean, we just made it one of our friends.
[02:27:56.580 --> 02:27:57.380]   How is this all black?
[02:27:57.380 --> 02:27:57.620]   Oh my God.
[02:27:57.620 --> 02:27:58.500]   I'm so jealous.
[02:27:58.500 --> 02:28:00.100]   If you're chaotic neutral,
[02:28:00.100 --> 02:28:02.660]   it means you're not held to the good evil.
[02:28:02.660 --> 02:28:03.300]   Yeah.
[02:28:03.300 --> 02:28:04.820]   Idea of the game master.
[02:28:04.820 --> 02:28:05.860]   You can be whatever you want.
[02:28:05.860 --> 02:28:06.340]   You can be whatever you want.
[02:28:06.340 --> 02:28:08.100]   Oh, of the game master, exactly.
[02:28:08.100 --> 02:28:08.340]   Yeah.
[02:28:08.340 --> 02:28:10.980]   Well, of course, it depends on the game master.
[02:28:10.980 --> 02:28:13.620]   If you're, if you're dungeon master's a jerk, then yeah.
[02:28:13.620 --> 02:28:16.900]   Did your paladin grow up under a mall?
[02:28:16.900 --> 02:28:19.540]   Yeah.
[02:28:19.540 --> 02:28:20.980]   I didn't do the most of my dirt.
[02:28:20.980 --> 02:28:23.060]   I would run down for my,
[02:28:23.060 --> 02:28:24.180]   because I was only there for a week.
[02:28:24.180 --> 02:28:25.220]   I was just quick.
[02:28:25.220 --> 02:28:25.700]   Yeah.
[02:28:25.700 --> 02:28:25.860]   Yeah.
[02:28:25.860 --> 02:28:29.060]   Father Robert, where can we,
[02:28:29.060 --> 02:28:30.260]   what's going on with you?
[02:28:30.260 --> 02:28:31.780]   Where can we catch up with what you're,
[02:28:31.780 --> 02:28:32.420]   what you're up to?
[02:28:32.420 --> 02:28:33.860]   Well, you got to find me on Twitter,
[02:28:33.860 --> 02:28:35.300]   twitter.com/padreastj,
[02:28:35.300 --> 02:28:36.420]   or on a priest book,
[02:28:36.420 --> 02:28:38.020]   Padreastj.
[02:28:38.020 --> 02:28:40.740]   Do you have to be a priest to use a priest book?
[02:28:40.740 --> 02:28:42.180]   It only works in the Vatican,
[02:28:42.180 --> 02:28:43.780]   so you're probably going to want to use Twitter instead.
[02:28:43.780 --> 02:28:46.580]   But no, definitely follow me on Twitter
[02:28:46.580 --> 02:28:48.580]   because I've actually done
[02:28:48.580 --> 02:28:50.820]   three test episodes of Padre's Corner
[02:28:50.820 --> 02:28:52.500]   coming from my new studio in the Vatican.
[02:28:52.500 --> 02:28:54.020]   And that's,
[02:28:54.020 --> 02:28:55.060]   that's going to be awesome.
[02:28:55.060 --> 02:28:55.060]   Awesome.
[02:28:55.060 --> 02:28:55.860]   You have lives.
[02:28:55.860 --> 02:28:57.460]   I just, I don't want to really kick it off
[02:28:57.460 --> 02:28:59.460]   until I know I can do it every single week.
[02:28:59.460 --> 02:29:01.460]   Oh, so excited.
[02:29:01.460 --> 02:29:02.260]   That'll be fun.
[02:29:02.260 --> 02:29:04.500]   The return of Padre's Corner.
[02:29:04.500 --> 02:29:06.100]   A little bit of fun tech stuff,
[02:29:06.100 --> 02:29:07.540]   some, some maker stuff,
[02:29:07.540 --> 02:29:08.660]   some review stuff,
[02:29:08.660 --> 02:29:09.380]   and of course,
[02:29:09.380 --> 02:29:12.340]   whatever I can see in the Vatican.
[02:29:12.340 --> 02:29:12.900]   I'm going to be,
[02:29:12.900 --> 02:29:13.300]   I've,
[02:29:13.300 --> 02:29:15.300]   I've brought over some cameras
[02:29:15.300 --> 02:29:16.580]   that are very easy to hide.
[02:29:16.580 --> 02:29:18.500]   Oh, jeez.
[02:29:18.500 --> 02:29:21.140]   God, this is not going to end well.
[02:29:21.140 --> 02:29:23.460]   Oh my God.
[02:29:23.460 --> 02:29:24.820]   Well, the faster I get fired,
[02:29:24.820 --> 02:29:25.940]   the faster I can come back to California.
[02:29:25.940 --> 02:29:26.980]   Okay, good.
[02:29:26.980 --> 02:29:27.940]   All right, good.
[02:29:27.940 --> 02:29:29.460]   We do Twitch Sundays
[02:29:29.460 --> 02:29:30.580]   right after the radio show
[02:29:30.580 --> 02:29:31.540]   that usually means between
[02:29:31.540 --> 02:29:33.780]   two 15 and 230 Pacific time.
[02:29:33.780 --> 02:29:35.540]   Let's make it 230 Pacific.
[02:29:35.540 --> 02:29:36.980]   That's 530 Eastern.
[02:29:36.980 --> 02:29:39.300]   That would be 2130 UTC.
[02:29:39.300 --> 02:29:40.580]   I'd love it if you'd come by and,
[02:29:40.580 --> 02:29:42.500]   you could watch us in studio.
[02:29:42.500 --> 02:29:43.540]   If you want to be in studio,
[02:29:43.540 --> 02:29:44.660]   we have a nice audience today.
[02:29:44.660 --> 02:29:46.340]   All you have to do is email tickets
[02:29:46.340 --> 02:29:47.620]   at twit.tv.
[02:29:47.620 --> 02:29:49.140]   But there's a live stream
[02:29:49.140 --> 02:29:50.260]   you can listen to or watch.
[02:29:50.260 --> 02:29:51.300]   We've got audio and video
[02:29:51.300 --> 02:29:53.140]   at twit.tv/live.
[02:29:53.140 --> 02:29:57.460]   You could even ask your vocal device,
[02:29:57.460 --> 02:29:58.740]   your Amazon Echo,
[02:29:58.740 --> 02:30:00.020]   or you could say,
[02:30:00.020 --> 02:30:02.980]   listen, play Twitch live.
[02:30:02.980 --> 02:30:04.100]   And it will actually play
[02:30:04.100 --> 02:30:05.780]   the audio stream that's going on right now,
[02:30:05.780 --> 02:30:06.500]   which is kind of fun.
[02:30:06.500 --> 02:30:08.020]   Or you can ask for any podcast.
[02:30:08.020 --> 02:30:12.260]   Just say, hey, Echo, play this week in tech.
[02:30:12.260 --> 02:30:14.420]   And it will play the most recent episode.
[02:30:14.420 --> 02:30:16.180]   You can download any episode
[02:30:16.180 --> 02:30:18.260]   from our website, twit.tv.
[02:30:18.260 --> 02:30:19.540]   Best thing to do is subscribe.
[02:30:19.540 --> 02:30:20.420]   And you could subscribe with
[02:30:20.420 --> 02:30:23.060]   whatever podcatcher you like the best.
[02:30:23.060 --> 02:30:24.420]   If it's luminary, that's fine.
[02:30:24.420 --> 02:30:26.420]   iTunes, Google, pocketcast,
[02:30:26.420 --> 02:30:29.220]   stitcher, slacker, Spotify.
[02:30:29.220 --> 02:30:30.980]   The best thing that's subscribing is
[02:30:30.980 --> 02:30:32.500]   you'll get it the minute it's available
[02:30:32.500 --> 02:30:34.820]   and plenty of time for your Monday morning commute.
[02:30:34.820 --> 02:30:36.180]   I do appreciate your listening.
[02:30:36.180 --> 02:30:37.060]   And we'll see you next time.
[02:30:37.060 --> 02:30:38.660]   Another twit is in the can.
[02:30:38.660 --> 02:30:39.620]   - Amazing. - Bye-bye.
[02:30:39.620 --> 02:30:42.220]   (upbeat music)
[02:30:42.220 --> 02:30:43.220]   - I wanna twit.
[02:30:43.220 --> 02:30:44.100]   - Do it the twit.
[02:30:44.100 --> 02:30:45.380]   - Why did I buy that?
[02:30:45.380 --> 02:30:46.740]   They sent me a can.
[02:30:46.740 --> 02:30:48.500]   I tried it and then I said,
[02:30:48.500 --> 02:30:49.780]   oh yeah, I'm subscribing to this.
[02:30:49.780 --> 02:30:50.660]   - Wait, wait, wait.
[02:30:50.660 --> 02:30:51.780]   So wait. - Oh, as I think.
[02:30:51.780 --> 02:30:52.820]   - They just sent you a can
[02:30:52.820 --> 02:30:53.940]   or you saw it on Instagram.
[02:30:53.940 --> 02:30:55.460]   - No, I randomly got a can in the mail.
[02:30:55.460 --> 02:30:57.060]   I wasn't even an Instagram ad.
[02:30:57.060 --> 02:30:59.060]   I randomly got a can in the mail
[02:30:59.060 --> 02:31:00.740]   and I said, this is good.
[02:31:00.740 --> 02:31:01.940]   Not taste good.
[02:31:01.940 --> 02:31:05.300]   - So you just tasted something
[02:31:05.300 --> 02:31:06.660]   that randomly came in the mail?
[02:31:06.660 --> 02:31:07.540]   - Yes.
[02:31:07.540 --> 02:31:08.900]   - So stupid.
[02:31:08.900 --> 02:31:10.180]   Oh my Leo.
[02:31:10.180 --> 02:31:16.180]   - He would be the easiest person.
[02:31:16.180 --> 02:31:17.060]   - I keep hoping somebody.
[02:31:17.060 --> 02:31:20.420]   - I keep hoping somebody put acid in my food.
[02:31:20.420 --> 02:31:21.300]   - My food.
[02:31:21.300 --> 02:31:23.060]   (sighs)
[02:31:23.060 --> 02:31:24.340]   - Um, editors note.
[02:31:24.340 --> 02:31:24.980]   - Yes.
[02:31:24.980 --> 02:31:26.660]   - Please put that after the credits.
[02:31:26.660 --> 02:31:27.700]   - Service page two.
[02:31:27.700 --> 02:31:28.660]   Service page two.
[02:31:28.660 --> 02:31:29.500]   Service page two.
[02:31:29.500 --> 02:31:30.900]   - Now we got a blooper.
[02:31:30.900 --> 02:31:33.780]   Now we got a blooper.

