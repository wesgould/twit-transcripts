;FFMETADATA1
title=Future Proof This Kid
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=633
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:05.920]   Hey, I'm Becky Worley and I'm sitting in for Leo Laport on this week in tech and we have
[00:00:05.920 --> 00:00:07.960]   a phenomenal panel.
[00:00:07.960 --> 00:00:09.400]   It's an all-robert panel.
[00:00:09.400 --> 00:00:11.720]   We have Rob Reed, the science fiction writer.
[00:00:11.720 --> 00:00:18.200]   We have Robert Scoble, who needs no introduction, and we have Bob Sullivan, who is a security
[00:00:18.200 --> 00:00:21.400]   writer and journalist, par excellence.
[00:00:21.400 --> 00:00:24.280]   We are going to talk about Apple's new gear.
[00:00:24.280 --> 00:00:26.120]   We're going to talk about Google.
[00:00:26.120 --> 00:00:27.760]   Why did they buy HTC?
[00:00:27.760 --> 00:00:28.760]   It's like the double rainbow.
[00:00:28.760 --> 00:00:29.760]   What does it mean?
[00:00:29.760 --> 00:00:32.440]   And then the Equifax hack.
[00:00:32.440 --> 00:00:34.440]   What's new and what you should do.
[00:00:34.440 --> 00:00:36.760]   That's all coming up on This Week in Tech.
[00:00:36.760 --> 00:00:42.320]   NetCasts you love.
[00:00:42.320 --> 00:00:47.520]   From people you trust.
[00:00:47.520 --> 00:00:49.760]   This is Twit.
[00:00:49.760 --> 00:00:56.000]   Bandwidth for This Week in Tech is provided by CashFly at CACHEFLY.com.
[00:00:56.000 --> 00:01:03.440]   This is Twit.
[00:01:03.440 --> 00:01:09.040]   It's episode 633 recorded September 24, 2017.
[00:01:09.040 --> 00:01:12.400]   Future proof this kid.
[00:01:12.400 --> 00:01:18.320]   This Week in Tech is brought to you by Betterment, the largest independent online financial advisor.
[00:01:18.320 --> 00:01:23.880]   For one low, transparent fee, Betterment gives you personalized advice and invests your money.
[00:01:23.880 --> 00:01:30.320]   For a limited time, you can get up to one year managed free, learn more at betterment.com/twit.
[00:01:30.320 --> 00:01:32.960]   And buy rocket mortgage from quick and loans.
[00:01:32.960 --> 00:01:34.520]   Home plays a big role in your life.
[00:01:34.520 --> 00:01:37.000]   That's why quick and loans created rocket mortgage.
[00:01:37.000 --> 00:01:41.160]   It lets you apply simply and understand the entire mortgage process fully so you can be
[00:01:41.160 --> 00:01:44.160]   confident you're getting the right mortgage for you.
[00:01:44.160 --> 00:01:48.760]   Get started at rocketmortgage.com/twit2.
[00:01:48.760 --> 00:01:54.800]   Come by Blue Apron #1 Fresh Ingredient Recipe Delivery Service in the Country.
[00:01:54.800 --> 00:01:58.640]   Check out this week's menu and get three meals free with your first purchase and free shipping
[00:01:58.640 --> 00:02:02.960]   by going to Blue Apron.com/twit.
[00:02:02.960 --> 00:02:04.720]   And buy Kaptera.
[00:02:04.720 --> 00:02:06.720]   Find software solutions for your business needs.
[00:02:06.720 --> 00:02:12.480]   Kaptera is a free website with over 400 categories of business software and thousands of ratings
[00:02:12.480 --> 00:02:15.560]   and reviews from software users just like you.
[00:02:15.560 --> 00:02:18.360]   Kaptera.com/twit.
[00:02:18.360 --> 00:02:22.920]   Welcome to this week in tech.
[00:02:22.920 --> 00:02:26.520]   I am Becky Worley, consumer and technology correspondent for ABC News.
[00:02:26.520 --> 00:02:27.520]   Good morning America.
[00:02:27.520 --> 00:02:30.280]   Also an F-O-L.
[00:02:30.280 --> 00:02:31.440]   That's a friend of Leo.
[00:02:31.440 --> 00:02:33.400]   Leo's on vacation this week.
[00:02:33.400 --> 00:02:36.280]   But we have an epic panel for you today.
[00:02:36.280 --> 00:02:37.800]   Amazing week of tech news to unpack.
[00:02:37.800 --> 00:02:39.840]   So I want to get right to it.
[00:02:39.840 --> 00:02:42.840]   Robert Scobel, you are here with us in studio.
[00:02:42.840 --> 00:02:43.840]   Thank you so much.
[00:02:43.840 --> 00:02:47.200]   I've got a few of the things that you have done recently.
[00:02:47.200 --> 00:02:49.800]   Scobelizer.com.
[00:02:49.800 --> 00:02:53.600]   You were recently in Singapore, is that right?
[00:02:53.600 --> 00:02:54.600]   And Russia.
[00:02:54.600 --> 00:02:55.600]   You're in Singapore law.
[00:02:55.600 --> 00:02:56.600]   You have to put law at the end of every week.
[00:02:56.600 --> 00:02:59.960]   In the last three weeks I've been in Russia, Canada and Singapore.
[00:02:59.960 --> 00:03:01.920]   Whoa.
[00:03:01.920 --> 00:03:04.280]   And is that in promotion of the book?
[00:03:04.280 --> 00:03:06.280]   It's the fourth transformation.
[00:03:06.280 --> 00:03:11.040]   The fourth transformation of augmented reality and artificial intelligence will change everything
[00:03:11.040 --> 00:03:12.040]   with Shell Israel.
[00:03:12.040 --> 00:03:13.040]   Is that what you were doing?
[00:03:13.040 --> 00:03:14.040]   Partly, yeah.
[00:03:14.040 --> 00:03:15.840]   Augmented reality is hot this year.
[00:03:15.840 --> 00:03:20.200]   Facebook, Google and Apple have all announced major strategies on augmented reality.
[00:03:20.200 --> 00:03:22.640]   So I'm doing a lot of speaking.
[00:03:22.640 --> 00:03:23.640]   Awesome.
[00:03:23.640 --> 00:03:24.640]   Okay, great.
[00:03:24.640 --> 00:03:26.400]   Well, we are stoked to have you here.
[00:03:26.400 --> 00:03:31.920]   Also in studio, Rob Reed, you are founder of listen.com, author of after on.
[00:03:31.920 --> 00:03:39.480]   I want to read this as the described as a tale of a diabolical social network with the
[00:03:39.480 --> 00:03:43.600]   means and perhaps the inclination to take full control of human society.
[00:03:43.600 --> 00:03:45.000]   Yeah, based on a true story.
[00:03:45.000 --> 00:03:46.000]   Oh, yeah.
[00:03:46.000 --> 00:03:47.440]   Did I say it with this enough gravitas?
[00:03:47.440 --> 00:03:48.840]   Yeah, that was lots of gravitas.
[00:03:48.840 --> 00:03:49.840]   I like that.
[00:03:49.840 --> 00:03:50.840]   Yeah.
[00:03:50.840 --> 00:03:51.840]   Yeah.
[00:03:51.840 --> 00:03:52.840]   It was like Jupiter grade gravitas.
[00:03:52.840 --> 00:03:55.800]   I've usually got like lunar gravitas, which is much sixth of Earth.
[00:03:55.800 --> 00:03:57.280]   So okay, I'm getting really geeky here.
[00:03:57.280 --> 00:03:58.280]   Yeah, you are.
[00:03:58.280 --> 00:03:59.280]   Okay.
[00:03:59.280 --> 00:04:00.880]   Well, I'm just honored because I'm sort of professionally perky.
[00:04:00.880 --> 00:04:02.440]   So to have any kind of gravitas.
[00:04:02.440 --> 00:04:03.440]   Gravitas.
[00:04:03.440 --> 00:04:05.200]   Well, really roll the R.
[00:04:05.200 --> 00:04:06.200]   Gravitas.
[00:04:06.200 --> 00:04:07.200]   Yes.
[00:04:07.200 --> 00:04:13.640]   I have been listening to the after on podcast, which you do with Tom Merritt and it is gripping.
[00:04:13.640 --> 00:04:15.200]   I am loving right now.
[00:04:15.200 --> 00:04:19.560]   I'm listening to the episode with James Barrett, the dangers of super intelligence.
[00:04:19.560 --> 00:04:20.720]   They are very real.
[00:04:20.720 --> 00:04:22.720]   So so good.
[00:04:22.720 --> 00:04:23.720]   Thank you.
[00:04:23.720 --> 00:04:27.280]   Via Skype from a part of the country where fall is falling.
[00:04:27.280 --> 00:04:28.280]   Washington DC.
[00:04:28.280 --> 00:04:29.280]   Welcome Bob Sullivan.
[00:04:29.280 --> 00:04:31.280]   So glad to have you.
[00:04:31.280 --> 00:04:35.280]   He is a syndicated columnist, consumer advocate, security journalist.
[00:04:35.280 --> 00:04:37.720]   I've been a huge fan for years.
[00:04:37.720 --> 00:04:44.200]   I don't want to date you or me, but you've been a contributor for CNBC, MSNBC, NBC, the
[00:04:44.200 --> 00:04:48.040]   network, a bunch of consumer advocacy books.
[00:04:48.040 --> 00:04:50.440]   Most recent one, is it getting unstuck, Bob?
[00:04:50.440 --> 00:04:52.040]   Is that the most recent one?
[00:04:52.040 --> 00:04:54.640]   Yes, it's the plateau effect getting unstuck.
[00:04:54.640 --> 00:04:59.960]   So how corporations, why corporations and people and musicians, we all follow this pattern
[00:04:59.960 --> 00:05:02.080]   of beginners luck followed by sophomore slump.
[00:05:02.080 --> 00:05:05.120]   And we're seeing that in computer security right now.
[00:05:05.120 --> 00:05:06.280]   And in a lot of places.
[00:05:06.280 --> 00:05:11.600]   I specifically requested that you come in this week because I found your take on the
[00:05:11.600 --> 00:05:17.120]   Equifax hack and what to do is a, you know, real practical consumer advice as some of
[00:05:17.120 --> 00:05:22.400]   the most level headed and slow to wait and let it kind of play out because it's such
[00:05:22.400 --> 00:05:26.880]   a mess that actually acting too quickly could be even more of a disaster than it already
[00:05:26.880 --> 00:05:27.880]   is.
[00:05:27.880 --> 00:05:32.040]   So we're going to get into that as we move through the show.
[00:05:32.040 --> 00:05:35.680]   So also Bob Sullivan, you can find all his stuff at Bob Sullivan.net.
[00:05:35.680 --> 00:05:40.840]   I want to make sure I get out there and he is @redtapecron at on Twitter.
[00:05:40.840 --> 00:05:45.960]   So I want to start today, you guys, with what is in my mind the freshest story of the week
[00:05:45.960 --> 00:05:50.200]   and one that happened at the end of the week and which may have a lot of portant as we move
[00:05:50.200 --> 00:05:56.120]   forward, which is Uber losing its license in London.
[00:05:56.120 --> 00:06:00.000]   London's transport regulator issued a statement saying Uber will not be issued with a private
[00:06:00.000 --> 00:06:04.440]   higher operator license after, I love all the British words that are in this, this is
[00:06:04.440 --> 00:06:11.160]   quote, "after the expiry of its current license on 30 September."
[00:06:11.160 --> 00:06:17.440]   Basically what they've cited is the transport for London, that's the transportation authority.
[00:06:17.440 --> 00:06:22.680]   They cite Uber's approach for reporting serious crime offenses, how medical certificates are
[00:06:22.680 --> 00:06:27.440]   obtained, how background checks are done and what really may be the most interesting aspect
[00:06:27.440 --> 00:06:34.560]   that they call out is Uber's software program to kind of obscure reported issues.
[00:06:34.560 --> 00:06:40.360]   It's called Grayball and the TFL says it could be used to block regulatory bodies from gaining
[00:06:40.360 --> 00:06:44.960]   full access to the app and prevent officials from figuring out what's going on on Uber
[00:06:44.960 --> 00:06:47.160]   that's not perfect and lovely.
[00:06:47.160 --> 00:06:55.360]   Do you guys think that this has a ripple effect that's going to play out in other Uber environments?
[00:06:55.360 --> 00:07:02.040]   Is this London just trying to get them to do what the black cabs have to do?
[00:07:02.040 --> 00:07:07.040]   I think London is just trying to get itself in the paper.
[00:07:07.040 --> 00:07:11.160]   Uber has obviously been banned from other places and other circumstances and both Uber
[00:07:11.160 --> 00:07:15.240]   and Lyft basically exiled themselves from Austin.
[00:07:15.240 --> 00:07:19.360]   But I think there's something about just the stature and the size of London and also the
[00:07:19.360 --> 00:07:20.360]   suddenness of this.
[00:07:20.360 --> 00:07:21.800]   It seemed to come out of nowhere.
[00:07:21.800 --> 00:07:24.480]   They're talking the expiry is 30 September.
[00:07:24.480 --> 00:07:25.480]   That is really soon.
[00:07:25.480 --> 00:07:26.480]   Right.
[00:07:26.480 --> 00:07:27.920]   Now they can appeal it fast.
[00:07:27.920 --> 00:07:28.920]   Right.
[00:07:28.920 --> 00:07:30.080]   And then they said they will.
[00:07:30.080 --> 00:07:31.880]   They said they're vigorously going to appeal it.
[00:07:31.880 --> 00:07:36.720]   It's not going away but it would change things considerably.
[00:07:36.720 --> 00:07:45.080]   I'm really curious if this is a monetary play, if this is a real safety issue or if this
[00:07:45.080 --> 00:07:47.440]   is just the issue of them obscuring their information.
[00:07:47.440 --> 00:07:50.360]   There's two sides to this that don't get reported.
[00:07:50.360 --> 00:07:57.120]   The other side is that regular taxi cabs have a security problem as well and they under
[00:07:57.120 --> 00:08:08.160]   report some of the offenses there and they weren't properly judged here against Uber.
[00:08:08.160 --> 00:08:15.040]   Uber, I mean I've used it all over the world from Russia to Singapore to China to South
[00:08:15.040 --> 00:08:17.560]   Africa and it works great.
[00:08:17.560 --> 00:08:18.880]   And consumers love it.
[00:08:18.880 --> 00:08:19.880]   I love it.
[00:08:19.880 --> 00:08:25.880]   And I'm bothered that I have to go back to using taxis in London.
[00:08:25.880 --> 00:08:31.640]   They're more expensive than Uber was and it's harder to get one.
[00:08:31.640 --> 00:08:34.720]   Yeah, it's great when you arrive at the airport.
[00:08:34.720 --> 00:08:36.800]   There's always a road them there.
[00:08:36.800 --> 00:08:42.240]   But if you're in a weird alley somewhere because you're at a friend's house, it's so
[00:08:42.240 --> 00:08:47.360]   nice to be able to pull out the Uber up and know where that taxi is and that it's on
[00:08:47.360 --> 00:08:48.360]   the way.
[00:08:48.360 --> 00:08:49.360]   Right.
[00:08:49.360 --> 00:08:54.400]   And it's harder to get other taxis to come and visit you.
[00:08:54.400 --> 00:08:58.720]   A lot of times I don't know the phone numbers to call to get a taxi to come and pick me
[00:08:58.720 --> 00:08:59.720]   up.
[00:08:59.720 --> 00:09:00.720]   Right.
[00:09:00.720 --> 00:09:02.240]   So now I have to ask and it's a pain in the ass.
[00:09:02.240 --> 00:09:06.360]   It seems like this is a real political question as well.
[00:09:06.360 --> 00:09:11.680]   It's like who's lobbying the transportation authority in London?
[00:09:11.680 --> 00:09:12.680]   Yeah.
[00:09:12.680 --> 00:09:13.680]   What's their incentive?
[00:09:13.680 --> 00:09:16.320]   And I went and looked and it's, you know, sitting on...
[00:09:16.320 --> 00:09:19.640]   In London, they do have amazing taxis.
[00:09:19.640 --> 00:09:20.640]   Oh, yeah.
[00:09:20.640 --> 00:09:22.360]   Everybody there is very educated.
[00:09:22.360 --> 00:09:23.960]   They know the city very well.
[00:09:23.960 --> 00:09:28.520]   They're generally polite and the taxis are all pre-consistent and pre-clean.
[00:09:28.520 --> 00:09:30.160]   That's not true in other cities.
[00:09:30.160 --> 00:09:33.560]   It's a very well-regarded profession there.
[00:09:33.560 --> 00:09:37.000]   There's a test that the cab drivers have to take called the knowledge or that's what
[00:09:37.000 --> 00:09:42.840]   the body of knowledge is referred to that is about navigating from waypoint to waypoint
[00:09:42.840 --> 00:09:44.840]   of which there are thousands in London.
[00:09:44.840 --> 00:09:47.560]   And one of my favorite things about this is they've put them in, they've put London
[00:09:47.560 --> 00:09:53.440]   cabbies and FMRIs and they have found the part of the brain that does this thing.
[00:09:53.440 --> 00:09:57.840]   It's probably part of the hippocampus because it's memory is literally larger and has greater
[00:09:57.840 --> 00:10:00.160]   blood flow than in a mere mortal.
[00:10:00.160 --> 00:10:02.480]   So all of that is amazing.
[00:10:02.480 --> 00:10:06.440]   But it's a very, very powerful lobby because it's an ancient profession and a very, very
[00:10:06.440 --> 00:10:10.640]   well established profession and black cabs are so expensive.
[00:10:10.640 --> 00:10:17.120]   I mean the price differential in San Francisco is appreciable between UberX and a taxi in
[00:10:17.120 --> 00:10:18.640]   London, it's yawning.
[00:10:18.640 --> 00:10:25.240]   Yeah, it's at least anecdotally, I experienced it's at least 2X what an Uber is.
[00:10:25.240 --> 00:10:31.280]   And I think that this really does come down to financial incentives in a lot of ways.
[00:10:31.280 --> 00:10:33.320]   I looked it up and listened to this.
[00:10:33.320 --> 00:10:39.080]   So these are the application fees that a black cab operator would have to pay.
[00:10:39.080 --> 00:10:40.600]   Now I agree with you completely.
[00:10:40.600 --> 00:10:42.720]   It's like PhD level taxi driving.
[00:10:42.720 --> 00:10:45.000]   So they are top shelf professionals.
[00:10:45.000 --> 00:10:46.000]   Listen to this.
[00:10:46.000 --> 00:10:52.080]   80 pounds for the application, 60 pounds for the online application.
[00:10:52.080 --> 00:10:56.720]   Then you have to go see your doctor and get a full medical.
[00:10:56.720 --> 00:10:58.240]   You have to take the knowledge.
[00:10:58.240 --> 00:11:00.000]   That's 200 pounds.
[00:11:00.000 --> 00:11:02.760]   You have to appear and defend your knowledge.
[00:11:02.760 --> 00:11:04.360]   That's 400 pounds.
[00:11:04.360 --> 00:11:05.640]   And then you get a license fee.
[00:11:05.640 --> 00:11:07.600]   That's 192 pounds.
[00:11:07.600 --> 00:11:13.200]   So we're talking about almost 800 pounds of city licensing fees that are lost for every
[00:11:13.200 --> 00:11:16.280]   Uber driver who replaces a black cab driver.
[00:11:16.280 --> 00:11:17.280]   Right.
[00:11:17.280 --> 00:11:20.600]   And the black cab drivers could make that up on a single ride from Heathrow to Nauti
[00:11:20.600 --> 00:11:21.600]   Hell.
[00:11:21.600 --> 00:11:24.600]   Oh, and by the way, the sitting for the knowledge may be free.
[00:11:24.600 --> 00:11:25.600]   Maybe free.
[00:11:25.600 --> 00:11:27.640]   The other thing, I don't know why I'm so freaked about.
[00:11:27.640 --> 00:11:31.440]   The other thing about the knowledge is many people have to sit for it multiple times.
[00:11:31.440 --> 00:11:35.160]   And it's not shameful to fail because it's almost unheard of to pass on on the first
[00:11:35.160 --> 00:11:36.160]   time through.
[00:11:36.160 --> 00:11:37.160]   That's the biggest one.
[00:11:37.160 --> 00:11:38.160]   100 pounds.
[00:11:38.160 --> 00:11:39.160]   400, 400.
[00:11:39.160 --> 00:11:40.160]   Yeah.
[00:11:40.160 --> 00:11:41.160]   Yeah.
[00:11:41.160 --> 00:11:43.920]   And if Uber drivers are displacing those people, I think the London, I think the black
[00:11:43.920 --> 00:11:45.160]   cabs are still pretty robust.
[00:11:45.160 --> 00:11:47.240]   I don't think they've been driven from the roads yet.
[00:11:47.240 --> 00:11:51.440]   But yeah, certainly if they were replacing them, that would be quite a bit of lost revenue.
[00:11:51.440 --> 00:11:54.680]   They're so symbolic in London during the Olympics.
[00:11:54.680 --> 00:11:59.040]   You remember they brought the Spice Girls in and they were each in a black cab as they
[00:11:59.040 --> 00:12:02.080]   went around the around the arena.
[00:12:02.080 --> 00:12:03.440]   It was it was cool to see.
[00:12:03.440 --> 00:12:07.120]   And you really realize this is a place where it's not just financial.
[00:12:07.120 --> 00:12:11.640]   It's not just as the mayor in London City Con says you have to who has to play by the
[00:12:11.640 --> 00:12:15.240]   rules, but it's also cultural.
[00:12:15.240 --> 00:12:16.640]   It is a cultural touch down.
[00:12:16.640 --> 00:12:19.960]   What do you guys think about this the software that they're using?
[00:12:19.960 --> 00:12:24.120]   It's called Grayball and it's been it's been brought up in the news before.
[00:12:24.120 --> 00:12:25.120]   Oh, yeah.
[00:12:25.120 --> 00:12:30.160]   Obviously because it's you know it profiles users in most cases, but this is when you
[00:12:30.160 --> 00:12:33.760]   know, they're really saying you cannot use this to obscure problems.
[00:12:33.760 --> 00:12:34.760]   Yeah, they say.
[00:12:34.760 --> 00:12:39.520]   I think they were specifically using it to avoid picking up regulators.
[00:12:39.520 --> 00:12:43.440]   So my recollection of its use in the US and I believe that Uber has basically copped to
[00:12:43.440 --> 00:12:44.880]   it and said, yeah, guilty is charged.
[00:12:44.880 --> 00:12:46.320]   We did that stuff.
[00:12:46.320 --> 00:12:49.960]   And I'm pretty sure that it was in certain cities where they felt they were being particularly
[00:12:49.960 --> 00:12:51.920]   targeted by regulators.
[00:12:51.920 --> 00:12:58.480]   And the purpose of it, if I recall correctly, was to refrain from picking up people from
[00:12:58.480 --> 00:13:01.920]   the local government who might be trying to monitor them.
[00:13:01.920 --> 00:13:07.040]   And so it wasn't as nefarious as it might sound on the surface, but it was all about
[00:13:07.040 --> 00:13:08.800]   evading the law.
[00:13:08.800 --> 00:13:11.680]   So it you know, it certainly doesn't look good.
[00:13:11.680 --> 00:13:15.960]   And I wouldn't I don't know if it was alleged that they were using it in London, but I think
[00:13:15.960 --> 00:13:20.560]   they were citing that is this is this is a company that doesn't exactly play by the rules.
[00:13:20.560 --> 00:13:27.400]   And the new CEO just said yesterday to his employees that this is what happens.
[00:13:27.400 --> 00:13:31.400]   The London ruling is what happens when you get a bad reputation.
[00:13:31.400 --> 00:13:34.400]   I think that's the the pile on effect.
[00:13:34.400 --> 00:13:40.320]   And that's why I wonder if if you feel like this will be the start of a snowball where
[00:13:40.320 --> 00:13:44.720]   other other cities feel like this is a chance to get their hooks into Uber and have more
[00:13:44.720 --> 00:13:50.600]   legislative and maybe more financial incentive hooks that they can call from because the
[00:13:50.600 --> 00:13:56.000]   reputation is that I think London is a pretty unique situation because of its strength of
[00:13:56.000 --> 00:14:00.080]   its educated taxi force.
[00:14:00.080 --> 00:14:02.200]   It isn't true in a lot of places in the world.
[00:14:02.200 --> 00:14:08.040]   And in a lot of other places in the world, Uber has become so important that it's really
[00:14:08.040 --> 00:14:12.040]   hard to get kicked them out now.
[00:14:12.040 --> 00:14:15.240]   Interesting to see what Lyft does too, right?
[00:14:15.240 --> 00:14:16.240]   Well, Lyft has never done that.
[00:14:16.240 --> 00:14:17.240]   They've never done that.
[00:14:17.240 --> 00:14:18.240]   Lyft is not international.
[00:14:18.240 --> 00:14:23.720]   And that's that's one reason I keep using Uber's because I fly international quite a
[00:14:23.720 --> 00:14:29.760]   bit and you need to and Lyft is great here in San Francisco, but it's not that great.
[00:14:29.760 --> 00:14:34.280]   If you travel overseas, it's not that great place where they simply do not exist.
[00:14:34.280 --> 00:14:35.280]   Yeah.
[00:14:35.280 --> 00:14:36.280]   Yeah.
[00:14:36.280 --> 00:14:37.280]   Okay.
[00:14:37.280 --> 00:14:38.280]   I have some strong feelings about Uber.
[00:14:38.280 --> 00:14:39.920]   I would love to jump in if I could for a second.
[00:14:39.920 --> 00:14:43.360]   I think it's a I think it's a big deal because of this.
[00:14:43.360 --> 00:14:47.600]   Uber has basically been able to kind of bully its way around around the world around local
[00:14:47.600 --> 00:14:52.280]   regulators, largely because consumers do find it so incredibly useful.
[00:14:52.280 --> 00:14:56.560]   And so when a city mayor steps up and says, no, we're going to regulate Uber and there's
[00:14:56.560 --> 00:14:57.560]   all of this out.
[00:14:57.560 --> 00:14:59.720]   So that has worked for them so far.
[00:14:59.720 --> 00:15:04.600]   If it doesn't work now in London, this will be sort of the first very public loss that
[00:15:04.600 --> 00:15:05.600]   Uber has.
[00:15:05.600 --> 00:15:08.160]   I think it will put the company back on its own, which is obviously it already is because
[00:15:08.160 --> 00:15:10.200]   of everything else that's happened.
[00:15:10.200 --> 00:15:12.800]   And I think we could see real substantial changes.
[00:15:12.800 --> 00:15:18.680]   I have a story I've read and rewritten on my site several times, which I call the six
[00:15:18.680 --> 00:15:23.560]   reasons that Uber nomics is terrible for transportation and Uber itself.
[00:15:23.560 --> 00:15:27.520]   So there's a lot of reasons why consumers love Uber today, but I think it's on a lot
[00:15:27.520 --> 00:15:31.160]   of unsustainable largely because it's kind of cheating the system and that's why the
[00:15:31.160 --> 00:15:35.200]   price is so cheap and there's this Ponzi scheme element to it.
[00:15:35.200 --> 00:15:38.960]   But I think we really could see something happen if Uber can't get past this London
[00:15:38.960 --> 00:15:41.120]   issue without some kind of major change.
[00:15:41.120 --> 00:15:44.160]   Of course, other cities, other governments are going to try to do the same thing.
[00:15:44.160 --> 00:15:48.240]   And before they had to back down, if London wins, I think they'll they won't back down
[00:15:48.240 --> 00:15:50.920]   as much because there's a story there.
[00:15:50.920 --> 00:15:51.920]   Yeah.
[00:15:51.920 --> 00:15:54.920]   I think that some of the folks in the chat room are agreeing.
[00:15:54.920 --> 00:16:00.640]   The manager 0440 says, "Let the market decide if the advantages are there for the black
[00:16:00.640 --> 00:16:04.000]   cab and the provisions of the London cab act, they'll make a living.
[00:16:04.000 --> 00:16:09.920]   A monopoly isn't required with all the advantages cited."
[00:16:09.920 --> 00:16:15.440]   It seems like there's, this is definitely a financial story as they go through this
[00:16:15.440 --> 00:16:19.560]   and try and figure out where the benefits are.
[00:16:19.560 --> 00:16:22.520]   I mean, 40,000 drivers in the city right now.
[00:16:22.520 --> 00:16:24.520]   And 3 million customers.
[00:16:24.520 --> 00:16:28.480]   And if at the end of the day, the 3 million customers, and those are by the way, they're
[00:16:28.480 --> 00:16:29.680]   counting them pretty generously.
[00:16:29.680 --> 00:16:33.920]   I think one article I saw said 3 million people have taken at least once in the last
[00:16:33.920 --> 00:16:35.320]   three months, which is not enough.
[00:16:35.320 --> 00:16:39.000]   Some of those people aren't necessarily hooked on it, but a pretty high percentage are.
[00:16:39.000 --> 00:16:44.800]   And if it is a different, a price differential of 2X, let's just say, imagine that it is.
[00:16:44.800 --> 00:16:50.760]   It kind of says, poorer folks don't get to have driving on demand anymore.
[00:16:50.760 --> 00:16:54.840]   And wealthier folks can afford their black cabs, but we're going to ban this aspect
[00:16:54.840 --> 00:16:56.480]   of transportation.
[00:16:56.480 --> 00:16:59.520]   And if you're below a certain income, that's a bummer for you.
[00:16:59.520 --> 00:17:02.680]   And oh, if you're one of these 40,000 drivers, that's too bad for you.
[00:17:02.680 --> 00:17:07.640]   Now that said, when Uber and Lyft exiled themselves from Austin, it was remarkable how
[00:17:07.640 --> 00:17:13.880]   fast local companies were up and running and providing very similar services.
[00:17:13.880 --> 00:17:16.440]   It was a matter of a single digit number of weeks.
[00:17:16.440 --> 00:17:17.440]   I've forgotten what-
[00:17:17.440 --> 00:17:19.960]   Were those pre-existing taxi companies that figured out how to-
[00:17:19.960 --> 00:17:22.520]   No, they weren't taxi companies.
[00:17:22.520 --> 00:17:23.520]   They were new ones.
[00:17:23.520 --> 00:17:25.760]   Yeah, they were new companies that got up and running.
[00:17:25.760 --> 00:17:26.760]   Yeah.
[00:17:26.760 --> 00:17:28.880]   I used them at South by Southwest and they were all right.
[00:17:28.880 --> 00:17:29.880]   They were all right.
[00:17:29.880 --> 00:17:30.880]   Yeah.
[00:17:30.880 --> 00:17:33.840]   But better than nothing, or better than a 2X price, or were they?
[00:17:33.840 --> 00:17:35.080]   I don't know, you were there.
[00:17:35.080 --> 00:17:36.600]   No, they were about the same as Uber.
[00:17:36.600 --> 00:17:38.840]   And they had a similar kind of app.
[00:17:38.840 --> 00:17:40.600]   The promise, they only worked in one city.
[00:17:40.600 --> 00:17:42.600]   So as soon as you left Austin, you-
[00:17:42.600 --> 00:17:43.600]   It was a band-aid.
[00:17:43.600 --> 00:17:44.600]   Yeah, definitely a band-aid.
[00:17:44.600 --> 00:17:46.440]   And now Uber and Lyft are back.
[00:17:46.440 --> 00:17:47.440]   Yep.
[00:17:47.440 --> 00:17:51.320]   Well, we can't ignore- Uber is half the price almost everywhere.
[00:17:51.320 --> 00:17:53.120]   In New York City, I'm in New York City a lot.
[00:17:53.120 --> 00:17:55.080]   And Uber is an amazing deal there.
[00:17:55.080 --> 00:17:56.960]   But it doesn't make any sense, does it?
[00:17:56.960 --> 00:18:01.520]   I mean, Uber isn't creating a 50% efficiency in its real cost.
[00:18:01.520 --> 00:18:03.600]   So something else is making Uber ride so cheap.
[00:18:03.600 --> 00:18:08.400]   And in the end, that is really why people use Uber because it has this huge price advantage.
[00:18:08.400 --> 00:18:14.520]   But my big concern with Uber is it can't continue to charge 50% less than everybody else.
[00:18:14.520 --> 00:18:17.200]   It does have some efficiency, by the way.
[00:18:17.200 --> 00:18:18.960]   I've talked to lots of drivers all over the world.
[00:18:18.960 --> 00:18:19.960]   Marginal gains.
[00:18:19.960 --> 00:18:22.160]   Well, here's what the Lyft-
[00:18:22.160 --> 00:18:23.920]   Car's only just finished.
[00:18:23.920 --> 00:18:24.920]   I just finished it.
[00:18:24.920 --> 00:18:27.560]   Car's dispatched where you are when you are.
[00:18:27.560 --> 00:18:29.360]   And there's a little bit of efficiency to that.
[00:18:29.360 --> 00:18:33.600]   But that's a very marginal gain compared to the main cost of moving people around, which is-
[00:18:33.600 --> 00:18:37.040]   It can be pretty significant gain though because the Lyft drivers-
[00:18:37.040 --> 00:18:40.760]   I take a lot of Lyft and Uber here in San Francisco and I talk to the drivers.
[00:18:40.760 --> 00:18:43.640]   And almost everybody's working for both here in San Francisco.
[00:18:43.640 --> 00:18:50.040]   And they say that Uber keeps them busier per hour and Lyft takes them further away to pick up a ride.
[00:18:50.040 --> 00:18:53.840]   And that further away can be maybe 10 minutes each way, right?
[00:18:53.840 --> 00:18:56.720]   That's significant out of an hour drive.
[00:18:56.720 --> 00:18:58.120]   So there's a network effect for the drive?
[00:18:58.120 --> 00:18:59.280]   There is a network effect.
[00:18:59.280 --> 00:19:04.760]   And a lot of times Uber doesn't even let them go back to Lyft because they keep them so busy.
[00:19:04.760 --> 00:19:06.120]   Yeah, yeah, yeah.
[00:19:06.120 --> 00:19:10.520]   In Singapore, my rides were almost always saying,
[00:19:10.520 --> 00:19:12.360]   "Your driver's dropping somebody off."
[00:19:12.360 --> 00:19:16.000]   And I would actually see them come into the hotel, drop somebody off,
[00:19:16.000 --> 00:19:19.240]   and then I would get in the car and off we go.
[00:19:19.240 --> 00:19:22.280]   They didn't even have time to quit the app and try something else.
[00:19:22.280 --> 00:19:28.480]   Well, one of the sources of the discount is the fact that Uber is burning billions and billions and billions of dollars per year.
[00:19:28.480 --> 00:19:33.040]   So to a certain extent, wealthy VCs are funding your ride.
[00:19:33.040 --> 00:19:37.760]   That's not permanently sustainable, but I certainly appreciate their generosity.
[00:19:37.760 --> 00:19:42.960]   But that's also, you would call that product dumping, that's antitrust behavior.
[00:19:42.960 --> 00:19:46.320]   And one of the consequences of this, here's the worst outcome.
[00:19:46.320 --> 00:19:49.680]   And by the way, I use Uber, the technology is great.
[00:19:49.680 --> 00:19:52.600]   Some kind of ride-sharing service is absolutely the future.
[00:19:52.600 --> 00:19:57.240]   So what I'm worried about is Uber will burn through that money, it will crash and burn.
[00:19:57.240 --> 00:20:02.760]   And in the meantime, it's going to take down, what's happening with New York City taxis right now is crazy.
[00:20:02.760 --> 00:20:05.760]   Those are the ones that you just listed the fees for London drivers.
[00:20:05.760 --> 00:20:09.760]   It used to cost a million dollars to become a taxi driver in New York City.
[00:20:09.760 --> 00:20:12.120]   People would get mortgages to get medallions.
[00:20:12.120 --> 00:20:18.880]   The value of medallions is cut down to about $300,000 now and people are declaring bankruptcy because they can't afford them.
[00:20:18.880 --> 00:20:21.880]   Octions for medallions are going almost unsold.
[00:20:21.880 --> 00:20:30.480]   What I'm saying is that yellow cabs in New York are in danger and Uber could bring down both itself and other forms of transportation if we don't regulate this properly.
[00:20:30.480 --> 00:20:35.560]   Wasn't there a gentleman in New York who owned hundreds of medallions?
[00:20:35.560 --> 00:20:36.320]   Oh, yeah.
[00:20:36.320 --> 00:20:42.280]   And was demanding a government bailout because he was a centimillionaire and it wasn't fair for a centimillionaire.
[00:20:42.280 --> 00:20:46.560]   I mean, I don't know exactly, I actually live in New York currently, but I've only been there for about a year and a half.
[00:20:46.560 --> 00:20:50.040]   I don't know the city system as well as I did in San Francisco.
[00:20:50.040 --> 00:20:54.880]   But I would say having talked to probably hundreds of cabbies over the years in San Francisco,
[00:20:54.880 --> 00:21:02.720]   the situation in San Francisco was almost futile, not futile, but FEUDAL, nights and shining armor futile.
[00:21:02.720 --> 00:21:06.480]   In that a very high percentage of the drivers didn't own a medallion.
[00:21:06.480 --> 00:21:14.440]   They were working for somebody who drove a cab some decades before, who owned dozens before the rules changed and for bad people owning multiple ones.
[00:21:14.440 --> 00:21:20.280]   And so that driver would have to be paying this obscene rent, defacto to rent the medallion.
[00:21:20.280 --> 00:21:23.560]   And so where did all those, where did the efficiency come from?
[00:21:23.560 --> 00:21:25.280]   Well, you don't have this feudal system.
[00:21:25.280 --> 00:21:30.320]   The surf doesn't have to pay the Lord, who owns hundreds of medallions in the case of our centimillionaire in New York,
[00:21:30.320 --> 00:21:36.600]   who's howling about how unfair it is that the price is going down, or the more one-off situations in San Francisco,
[00:21:36.600 --> 00:21:42.560]   where folks were paying several dozen, I don't remember what, it was close to a hundred bucks, a shift to the medallion holder.
[00:21:42.560 --> 00:21:48.680]   That's where all the pricing is going to somebody who, in many cases, people who have dozens or hundreds of medallions.
[00:21:48.680 --> 00:21:52.640]   I don't, they got a great run for decades and decades in San Francisco.
[00:21:52.640 --> 00:21:58.040]   They deliberately constricted the supply of medallions in order to keep the price of medallions high.
[00:21:58.040 --> 00:21:59.480]   That did not benefit the drivers.
[00:21:59.480 --> 00:22:01.080]   It benefited the medallion holders.
[00:22:01.080 --> 00:22:02.560]   It's artificial scarcity.
[00:22:02.560 --> 00:22:08.560]   And it punished the people in the city in San Francisco who found themselves unable to get a ride late at night,
[00:22:08.560 --> 00:22:12.400]   women are walking home at 2 a.m., people are doing stupid things like driving drunk,
[00:22:12.400 --> 00:22:16.240]   were refraining from going out and patronizing the bars in the restaurants of the city,
[00:22:16.240 --> 00:22:20.640]   because the medallion holders had this power that allowed them to constrain the supply.
[00:22:20.640 --> 00:22:23.400]   I don't think those guys deserve a free ride anymore.
[00:22:23.400 --> 00:22:24.080]   Yeah.
[00:22:24.080 --> 00:22:28.320]   No, if any industry was ripe for disruption, it was taxis.
[00:22:28.320 --> 00:22:30.000]   So this is a good thing.
[00:22:30.000 --> 00:22:33.840]   We just have to be careful as we don't have to burn the whole system down while we...
[00:22:33.840 --> 00:22:34.360]   Not all better.
[00:22:34.360 --> 00:22:34.880]   Yeah.
[00:22:34.880 --> 00:22:35.680]   I agree.
[00:22:35.680 --> 00:22:43.280]   And that kind of coexisting and figuring out how they find the medium of those two worlds existing is going to be really a test case.
[00:22:43.280 --> 00:22:47.080]   And we have even started talking about what happens when self-driving cars come along.
[00:22:47.080 --> 00:22:48.280]   Oh, that's a whole...
[00:22:48.280 --> 00:22:49.280]   That's the interesting thing.
[00:22:49.280 --> 00:22:49.880]   Yeah.
[00:22:49.880 --> 00:22:52.880]   Because I think that's what's going to tear apart Uber.
[00:22:52.880 --> 00:22:56.880]   Well, either that or they're going to be the biggest supply of rides on demand.
[00:22:56.880 --> 00:23:03.480]   I don't know that they have the talent anymore because they're PR problems to build the latest self-driving cars.
[00:23:03.480 --> 00:23:07.480]   And I don't know that they're going to get the distribution of cars with sensors.
[00:23:07.480 --> 00:23:11.280]   But don't they have terabytes of data from Google to help them with a talent issue?
[00:23:11.280 --> 00:23:12.080]   Well, maybe.
[00:23:12.080 --> 00:23:13.560]   But that's not enough.
[00:23:13.560 --> 00:23:17.600]   I think there's one guy who could tear apart Uber and it's Elon Musk.
[00:23:17.600 --> 00:23:17.920]   Oh, yeah.
[00:23:17.920 --> 00:23:19.000]   The Tesla system.
[00:23:19.000 --> 00:23:22.440]   The Tesla's new Model 3.
[00:23:22.440 --> 00:23:26.440]   That's going to put 400,000 cars with sensors out on the road.
[00:23:26.440 --> 00:23:30.640]   And that's going to be what enables Elon to get self-driving before anybody else.
[00:23:30.640 --> 00:23:35.240]   You're going to choose who you ride with based on who's AI is the best.
[00:23:35.240 --> 00:23:41.840]   Well, and Elon is more aggressive on self-driving technology than anybody else.
[00:23:41.840 --> 00:23:46.240]   Yeah, well, the idea is that you'll be able to, if you're a Tesla owner,
[00:23:46.240 --> 00:23:49.840]   you'll be able to let your car roll out and go to work for you all day.
[00:23:49.840 --> 00:23:50.240]   Right?
[00:23:50.240 --> 00:23:50.840]   I mean, when it's...
[00:23:50.840 --> 00:23:51.240]   Wow.
[00:23:51.240 --> 00:23:52.440]   That's what I'm hoping for.
[00:23:52.440 --> 00:23:56.640]   And then you get income from your car while you're at home, you know, playing whatever.
[00:23:56.640 --> 00:23:59.640]   It's very clear that he's aiming at Toyota.
[00:23:59.640 --> 00:24:01.640]   And he doesn't even care about Uber, I think.
[00:24:01.640 --> 00:24:03.240]   I think he's aiming at Toyota.
[00:24:03.240 --> 00:24:09.240]   And he's going to try with the solar panel, the battery in your garage, and the car that has no oil.
[00:24:09.240 --> 00:24:18.040]   He's going to try to reduce the cost per mile of transportation over Uber or over Toyota.
[00:24:18.040 --> 00:24:19.240]   And over car ownership.
[00:24:19.240 --> 00:24:29.240]   And that's going to be what really changes this whole model because I'm sorry people who work for Uber or for taxis in seven years,
[00:24:29.240 --> 00:24:30.640]   you do not have a job.
[00:24:30.640 --> 00:24:34.240]   And we should be accurate about that, and honest about that.
[00:24:34.240 --> 00:24:39.240]   We have at least 30 more days of Ubers in London though.
[00:24:39.240 --> 00:24:40.040]   So we can...
[00:24:40.040 --> 00:24:41.040]   Is that October 1, though?
[00:24:41.040 --> 00:24:42.040]   I mean, that's just a...
[00:24:42.040 --> 00:24:43.240]   I think they have like 20...
[00:24:43.240 --> 00:24:43.840]   I don't know.
[00:24:43.840 --> 00:24:45.840]   They have like 20-some days to talk it out.
[00:24:45.840 --> 00:24:48.440]   And it could be a negotiating tactic on the city government's part.
[00:24:48.440 --> 00:24:49.440]   I don't put that past...
[00:24:49.440 --> 00:24:51.440]   It could be like open strong.
[00:24:51.440 --> 00:24:52.440]   Yeah.
[00:24:52.440 --> 00:24:55.440]   I don't put that past CityCon in London.
[00:24:55.440 --> 00:24:57.440]   So, you know, they're not afraid of their...
[00:24:57.440 --> 00:24:58.440]   There's a lot of money involved.
[00:24:58.440 --> 00:25:00.440]   There's a lot of negotiations.
[00:25:00.440 --> 00:25:01.440]   A lot of negotiations.
[00:25:01.440 --> 00:25:02.440]   Right?
[00:25:02.440 --> 00:25:09.440]   While Uber is negotiating and working out in London, the iPhone 8 reviews and the teardowns are out, as is the phone.
[00:25:09.440 --> 00:25:15.440]   I just want to go through a couple of the things that have kind of come down over the last week as the phones come out.
[00:25:15.440 --> 00:25:18.440]   And the news is all the embargoes have been lifted.
[00:25:18.440 --> 00:25:28.440]   So, MacRumors lists in order of notable differences on the iPhone 8, the glass back, wireless charging, new colors, better speakers.
[00:25:28.440 --> 00:25:30.440]   I'm not super impressed yet.
[00:25:30.440 --> 00:25:33.440]   The A11 Bionic Chip, six-core processor.
[00:25:33.440 --> 00:25:37.440]   They say nothing noticeable on everyday processors, but processes.
[00:25:37.440 --> 00:25:40.440]   But it may impact augmented reality.
[00:25:40.440 --> 00:25:43.440]   I'm glad we have Robert Schoebel here to talk about that.
[00:25:43.440 --> 00:25:46.440]   What has gotten a lot of praise is the camera.
[00:25:46.440 --> 00:25:49.440]   Some say it's the best in any smartphone ever.
[00:25:49.440 --> 00:25:51.440]   12 megapixels, larger sensor.
[00:25:51.440 --> 00:25:54.440]   Flash is quad LED.
[00:25:54.440 --> 00:25:55.440]   Much better for night shots.
[00:25:55.440 --> 00:25:56.440]   I liked what Justin Robert Young said.
[00:25:56.440 --> 00:26:01.440]   He said, "You look less like a crazed raccoon against a pitch black background."
[00:26:01.440 --> 00:26:06.440]   Prices for the 8, 699 for the 64 gigabyte.
[00:26:06.440 --> 00:26:11.440]   Not a lot of lines are frenzy around the purchase you.
[00:26:11.440 --> 00:26:13.440]   There was a little line at Hillsdale.
[00:26:13.440 --> 00:26:15.440]   I went to Hillsdale on Friday night.
[00:26:15.440 --> 00:26:17.440]   There was probably about 40 people in line.
[00:26:17.440 --> 00:26:19.440]   So, there was a little line.
[00:26:19.440 --> 00:26:23.440]   But most of the stores around the world didn't have the mega lines that we've seen in past iPhone.
[00:26:23.440 --> 00:26:26.440]   So, let's start with Robert.
[00:26:26.440 --> 00:26:30.440]   What you thought was going to come, what actually came out, and what your reaction is.
[00:26:30.440 --> 00:26:36.440]   I thought Apple was going to be far more aggressive with augmented reality and with sensors.
[00:26:36.440 --> 00:26:44.440]   So, there's in the X, which hasn't shipped yet, there's a 3D sensor that sees your face and unlocks when you look at the phone.
[00:26:44.440 --> 00:26:51.440]   I thought they would be much more aggressive with that sensor and explain it to developers better and even open it up.
[00:26:51.440 --> 00:26:55.440]   But it seems like they've kept it to themselves.
[00:26:55.440 --> 00:26:59.440]   And the demos on the first day were pretty lackluster.
[00:26:59.440 --> 00:27:05.440]   Just didn't show off the technology in any really nice way.
[00:27:05.440 --> 00:27:08.440]   And I think they did this on purpose.
[00:27:08.440 --> 00:27:14.440]   They didn't show the new OLED screen off against the iPhone 8 screen because there's quite a bit of difference.
[00:27:14.440 --> 00:27:19.440]   OLED has better contrast, better color, better sharpness, better battery life.
[00:27:19.440 --> 00:27:22.440]   And that's why a lot of my friends and I are waiting for the X.
[00:27:22.440 --> 00:27:24.440]   Have you seen them side by side?
[00:27:24.440 --> 00:27:29.440]   I've seen OLED next to this screen and this screen is very similar to the screen.
[00:27:29.440 --> 00:27:31.440]   Yeah, it's quite a bit different.
[00:27:31.440 --> 00:27:33.440]   It's noticeable.
[00:27:33.440 --> 00:27:36.440]   And they didn't really show that off.
[00:27:36.440 --> 00:27:39.440]   Phil Schiller had a list of things that it did better.
[00:27:39.440 --> 00:27:44.440]   But he didn't really say, "Oh, look at the screen versus last year's screen."
[00:27:44.440 --> 00:27:47.440]   Not going to slam the product that he's also simultaneously released.
[00:27:47.440 --> 00:27:54.440]   Yeah, and I think that's an example of how conservative Apple is on selling things now.
[00:27:54.440 --> 00:28:01.440]   They know that the consumer who cares is going to figure it out and everybody else doesn't.
[00:28:01.440 --> 00:28:04.440]   You wanted a transparent phone and you got a mess.
[00:28:04.440 --> 00:28:07.440]   Well, yeah, that's a whole mess.
[00:28:07.440 --> 00:28:12.440]   My sources were telling me that there was a bunch of new stuff coming.
[00:28:12.440 --> 00:28:14.440]   And that was on the aggressive side.
[00:28:14.440 --> 00:28:21.440]   What actually shipped was on the most conservative side of all the predictions that I saw.
[00:28:21.440 --> 00:28:23.440]   Why would they do that?
[00:28:23.440 --> 00:28:30.440]   I'm hearing that there's a lot of fighting inside Apple at the executive level and that they just don't need to.
[00:28:30.440 --> 00:28:33.440]   They're the most profitable company right now.
[00:28:33.440 --> 00:28:37.440]   And if they do a good job of selling it, they're not going to sell a whole lot more.
[00:28:37.440 --> 00:28:41.440]   And if they don't do a good job, they're going to not sell a whole lot less.
[00:28:41.440 --> 00:28:45.440]   Most of us are going to have to update our phones every three years.
[00:28:45.440 --> 00:28:47.440]   And they know that.
[00:28:47.440 --> 00:28:52.440]   Let's go back to talking about the augmented reality.
[00:28:52.440 --> 00:28:54.440]   What excites you?
[00:28:54.440 --> 00:28:56.440]   Where's -- just give us a state of the state.
[00:28:56.440 --> 00:28:57.440]   That's your jam right now.
[00:28:57.440 --> 00:28:59.440]   The problem is I know too much.
[00:28:59.440 --> 00:29:02.440]   I have a Microsoft HoloLens.
[00:29:02.440 --> 00:29:06.440]   And that kind of augmented reality is really mind blowing.
[00:29:06.440 --> 00:29:13.440]   When you're wearing glasses or heads up displays like that, you can see aliens coming through your walls.
[00:29:13.440 --> 00:29:15.440]   And it's really mind blowing.
[00:29:15.440 --> 00:29:24.440]   The kind of augmented reality we're now getting on our iPhones and our iPads is fun.
[00:29:24.440 --> 00:29:28.440]   But we haven't yet seen it used for anything really crazy.
[00:29:28.440 --> 00:29:30.440]   That wasn't out before.
[00:29:30.440 --> 00:29:33.440]   In fact, somebody in the chat room was asking about the IKEA.
[00:29:33.440 --> 00:29:34.440]   Yeah, IKEA.
[00:29:34.440 --> 00:29:35.440]   It's great.
[00:29:35.440 --> 00:29:38.440]   But I saw that on Google Tango two years ago.
[00:29:38.440 --> 00:29:42.440]   And most of the app was built for Google Tango.
[00:29:42.440 --> 00:29:46.440]   So it's not something that's completely mind blowing out of the box.
[00:29:46.440 --> 00:29:56.440]   As a consumer reporter for a mainstream television network, I'm constantly fascinated by what my producers think is like,
[00:29:56.440 --> 00:29:58.440]   "Whoa, this is what's happening now."
[00:29:58.440 --> 00:30:00.440]   And they're really good at it.
[00:30:00.440 --> 00:30:05.440]   And they were desperate to make AR, "Wow, this is really cool."
[00:30:05.440 --> 00:30:10.440]   And the only thing they could come up with that they wanted to really check out was the IKEA app.
[00:30:10.440 --> 00:30:15.440]   These are the same people who were all about second life for like five years, right?
[00:30:15.440 --> 00:30:21.440]   I mean, I'm showing you a little dragon app here in this, the problem.
[00:30:21.440 --> 00:30:24.440]   Robert has two, he has two folders on his phone.
[00:30:24.440 --> 00:30:26.440]   It's like the keep and the dump.
[00:30:26.440 --> 00:30:28.440]   So as you come down here a little bit, there you go.
[00:30:28.440 --> 00:30:29.440]   There we go.
[00:30:29.440 --> 00:30:30.440]   And it's fun.
[00:30:30.440 --> 00:30:32.440]   You can see a little dragon.
[00:30:32.440 --> 00:30:34.440]   You can play some darts, right?
[00:30:34.440 --> 00:30:37.440]   Yep, I'm putting my hand there in front of your phone.
[00:30:37.440 --> 00:30:39.440]   So it senses tables really well.
[00:30:39.440 --> 00:30:43.440]   And it keeps the thing locked on the table really well, even though you shake it around.
[00:30:43.440 --> 00:30:44.440]   Which is pretty cool.
[00:30:44.440 --> 00:30:45.440]   It is very, very good.
[00:30:45.440 --> 00:30:47.440]   Yeah, that's the most impressive thing.
[00:30:47.440 --> 00:30:52.440]   This is, by the way, Apple bought a company called Mateo out of Germany.
[00:30:52.440 --> 00:30:55.440]   And this is Mateo's stuff.
[00:30:55.440 --> 00:30:57.440]   This is AR kit.
[00:30:57.440 --> 00:30:59.440]   Apple's AR platform.
[00:30:59.440 --> 00:31:01.440]   Yeah, he wants to be fed.
[00:31:01.440 --> 00:31:04.440]   So we've got a few of them.
[00:31:04.440 --> 00:31:06.440]   We've got a few of them, some donuts here.
[00:31:06.440 --> 00:31:08.440]   It's fun stuff like that, right?
[00:31:08.440 --> 00:31:09.440]   Fun.
[00:31:09.440 --> 00:31:14.440]   But the kind of thing that maybe a 13 year old plays, but I don't see a lot of utility in there.
[00:31:14.440 --> 00:31:21.440]   There is some apps that you can look at one app with a human heart and you can walk around the heart
[00:31:21.440 --> 00:31:23.440]   and learn all about it.
[00:31:23.440 --> 00:31:27.440]   That gives you a sense of where the value really is.
[00:31:27.440 --> 00:31:29.440]   Education is huge.
[00:31:29.440 --> 00:31:30.440]   Yeah, games are fun.
[00:31:30.440 --> 00:31:36.440]   But the kinds of games we're seeing are really pretty pedantic little fun games for tables.
[00:31:36.440 --> 00:31:41.440]   We haven't yet seen the full blown, you know, aliens coming through the walls kind of game.
[00:31:41.440 --> 00:31:46.440]   And it feels like this is a bet of goggles versus tablets or phones.
[00:31:46.440 --> 00:31:51.240]   And so, you know, the best use case I've ever seen for this, I went to a trade show about
[00:31:51.240 --> 00:31:56.240]   10 years ago where it was only for law enforcement and military.
[00:31:56.240 --> 00:32:04.240]   And they had this huge circular screen that you stepped inside of and you were given this gun
[00:32:04.240 --> 00:32:06.240]   that was tethered and had a laser.
[00:32:06.240 --> 00:32:12.240]   And then doors would open all around in the screen and you'd have to figure out who was the bad guy
[00:32:12.240 --> 00:32:18.240]   and needed to be shot and had a weapon and who was the lady holding the baby.
[00:32:18.240 --> 00:32:22.240]   And it was progressively harder and harder and it was heart stopping stuff.
[00:32:22.240 --> 00:32:26.240]   I mean, it was the most extreme experience I have ever had.
[00:32:26.240 --> 00:32:33.240]   And that was a million dollar installation made for the government or like a huge, maybe the state of California
[00:32:33.240 --> 00:32:35.240]   might have one of those.
[00:32:35.240 --> 00:32:37.240]   Now you can do that in a pair of goggles.
[00:32:37.240 --> 00:32:40.240]   Yeah, three thousand dollars for a hollow lens, right?
[00:32:40.240 --> 00:32:41.240]   Have you tried the meditue yet?
[00:32:41.240 --> 00:32:42.240]   I have.
[00:32:42.240 --> 00:32:44.240]   In fact, I just was in Singapore with a founder of Meda.
[00:32:44.240 --> 00:32:45.240]   Good more on it?
[00:32:45.240 --> 00:32:46.240]   Yeah, yeah.
[00:32:46.240 --> 00:32:51.240]   So we talked a lot about how the market is shaking up.
[00:32:51.240 --> 00:32:59.240]   Meda is doing really well with architects and car designers, people who need to see CAD drawings
[00:32:59.240 --> 00:33:03.240]   and see them in deep detail because it's tethered to a PC with a big...
[00:33:03.240 --> 00:33:04.240]   It's got power, yeah.
[00:33:04.240 --> 00:33:06.240]   And it's also a wider field of view, right?
[00:33:06.240 --> 00:33:07.240]   It's 90 degree.
[00:33:07.240 --> 00:33:12.240]   It's 90 degree where a hollow lens is about 20 per eye, so it's about this much.
[00:33:12.240 --> 00:33:16.240]   Hollow lens is almost like having an HDTV in front of you.
[00:33:16.240 --> 00:33:19.240]   And I find Meda to just be so much more enveloping.
[00:33:19.240 --> 00:33:24.240]   I did my very first podcast just a few weeks ago was with Maroam, my first podcast interview.
[00:33:24.240 --> 00:33:26.240]   We had like an hour long interview.
[00:33:26.240 --> 00:33:29.240]   And I was... which do you like better?
[00:33:29.240 --> 00:33:32.240]   Being an expert in AR, which I am not, and having tried both of you.
[00:33:32.240 --> 00:33:39.240]   The problem is I bias my life toward consumers and consumers need to walk into a shopping mall.
[00:33:39.240 --> 00:33:44.240]   We haven't yet seen a product that does that really well because the glasses are too big
[00:33:44.240 --> 00:33:46.240]   and too ugly and too expensive.
[00:33:46.240 --> 00:33:47.240]   Yeah.
[00:33:47.240 --> 00:33:49.240]   And so we know they're coming because they've started...
[00:33:49.240 --> 00:33:50.240]   Oh, of course.
[00:33:50.240 --> 00:33:52.240]   They've started coming out of the R&D labs.
[00:33:52.240 --> 00:33:56.240]   In fact, I might have a picture of one somewhere.
[00:33:56.240 --> 00:33:59.240]   They're coming in three, four, five ounces.
[00:33:59.240 --> 00:34:02.240]   The hollow lens is 1.28 pounds, right?
[00:34:02.240 --> 00:34:04.240]   And Meda's around there.
[00:34:04.240 --> 00:34:10.240]   Meda, though, is optimized for somebody who's going to sit at a desk and need to see their work in 3D.
[00:34:10.240 --> 00:34:11.240]   It's very big.
[00:34:11.240 --> 00:34:15.240]   Like a skyscraper or a car or something like that.
[00:34:15.240 --> 00:34:23.240]   And because it's tethered to a big PC with a big Nvidia card, they get about half a million polygons per second.
[00:34:23.240 --> 00:34:26.240]   So they get a lot more detail than what we're getting on our little iPhone.
[00:34:26.240 --> 00:34:27.240]   Oh, yeah.
[00:34:27.240 --> 00:34:33.240]   So this detail, the field of view, you liken this in the pre-show right.
[00:34:33.240 --> 00:34:41.240]   When you actually take AR onto a phone or a tablet, it's not like having that movie screen in front of you.
[00:34:41.240 --> 00:34:43.240]   It's like shining a flashlight.
[00:34:43.240 --> 00:34:52.240]   Yeah, I feel like in a sense I'm in a pitch-start room and I've got a flashlight with an incredibly narrow beam that can illuminate this layer of reality, which is the AR layer.
[00:34:52.240 --> 00:34:58.240]   But because it's this tiny little thing in my hand, I feel like it's a demo from the near future.
[00:34:58.240 --> 00:34:59.240]   Yeah.
[00:34:59.240 --> 00:35:00.240]   I'm entirely with Robert.
[00:35:00.240 --> 00:35:06.240]   I don't find it. I mean, I thought Pokemon Go was really cool, proof of concept, but I believe it is coming.
[00:35:06.240 --> 00:35:08.240]   And I know there's some new games coming.
[00:35:08.240 --> 00:35:09.240]   Yeah.
[00:35:09.240 --> 00:35:13.240]   The problem is Apple just didn't give developers enough time to do serious stuff.
[00:35:13.240 --> 00:35:14.240]   Yeah.
[00:35:14.240 --> 00:35:17.240]   And that's why you're seeing most of the apps are like measuring tape apps.
[00:35:17.240 --> 00:35:18.240]   Yeah.
[00:35:18.240 --> 00:35:19.240]   It's kind of me.
[00:35:19.240 --> 00:35:20.240]   Yeah.
[00:35:20.240 --> 00:35:23.240]   I don't have to carry a yard stick with me any more over stock.
[00:35:23.240 --> 00:35:25.240]   I'm tired of carrying a yard stick everywhere.
[00:35:25.240 --> 00:35:26.240]   Wait.
[00:35:26.240 --> 00:35:27.240]   What are you doing?
[00:35:27.240 --> 00:35:28.240]   None.
[00:35:28.240 --> 00:35:34.240]   Or the furniture app so you can aim it in your living room and see what your new couch might look like.
[00:35:34.240 --> 00:35:35.240]   That kind of thing.
[00:35:35.240 --> 00:35:38.240]   But they're car-finder over stock.
[00:35:38.240 --> 00:35:41.240]   Let's use see products in 3D.
[00:35:41.240 --> 00:35:47.240]   But so much more compelling when you have the headset on and you're enveloped with it.
[00:35:47.240 --> 00:35:48.240]   Yeah.
[00:35:48.240 --> 00:35:50.240]   And that to me was one of...
[00:35:50.240 --> 00:35:55.240]   Maron makes a lot of interesting trade-offs with meta because he made a very bold decision to say,
[00:35:55.240 --> 00:35:59.240]   "Okay, it's going to be a tethered experience because he does believe, as I'm sure you heard,
[00:35:59.240 --> 00:36:03.240]   traveling with him, that AR's first killer apps are all going to be about productivity.
[00:36:03.240 --> 00:36:05.240]   They're going to be business apps."
[00:36:05.240 --> 00:36:09.240]   And so he's not as concerned about the tethered than which would be a non-starter for a consumer.
[00:36:09.240 --> 00:36:12.240]   And being tethered allows that headset.
[00:36:12.240 --> 00:36:13.240]   It's really encumbering.
[00:36:13.240 --> 00:36:16.240]   But man, it gives a lot of horsepower.
[00:36:16.240 --> 00:36:17.240]   Yeah.
[00:36:17.240 --> 00:36:19.240]   Because if you put all that horsepower in your head, you'd be...
[00:36:19.240 --> 00:36:20.240]   Right.
[00:36:20.240 --> 00:36:26.240]   The biggest thing about marketing funding in China and Singapore last week is he's working toward
[00:36:26.240 --> 00:36:28.240]   a pair of glasses that will be consumer grade.
[00:36:28.240 --> 00:36:29.240]   Oh, yeah.
[00:36:29.240 --> 00:36:32.240]   And he's competing with Magic Leap, which got $1.4 billion.
[00:36:32.240 --> 00:36:33.240]   Yeah.
[00:36:33.240 --> 00:36:40.240]   Microsoft has spent billions of dollars and is about to announce a new Halloween strategy on October 3rd,
[00:36:40.240 --> 00:36:41.240]   I think it was.
[00:36:41.240 --> 00:36:42.240]   Oh, really?
[00:36:42.240 --> 00:36:43.240]   Yeah.
[00:36:43.240 --> 00:36:44.240]   And Google, of course, hasn't given it a rest.
[00:36:44.240 --> 00:36:46.240]   Google invested in Magic Leap.
[00:36:46.240 --> 00:36:47.240]   Of course.
[00:36:47.240 --> 00:36:49.240]   And then they have their own internal teams and they're working on a pair of glasses.
[00:36:49.240 --> 00:36:54.240]   And Amazon says that they're working on less-caled power, smart glasses.
[00:36:54.240 --> 00:36:56.240]   And Bob Sullivan, that really...
[00:36:56.240 --> 00:36:58.240]   You and I are the consumer reporters here.
[00:36:58.240 --> 00:37:03.240]   Do you think shopping is going to be the tipping point for AR in the consumer?
[00:37:03.240 --> 00:37:08.240]   Well, it all depends on how reasonably inexpensive the process is, right?
[00:37:08.240 --> 00:37:16.240]   I mean, there's forever the holy grail has been, could I somehow try something on and see if it looks good on me without going to a store?
[00:37:16.240 --> 00:37:18.240]   Or, you know, can I order something?
[00:37:18.240 --> 00:37:20.240]   And immediately get a feel for it.
[00:37:20.240 --> 00:37:22.240]   And that's going to be a killer app.
[00:37:22.240 --> 00:37:24.240]   But it has to feel really good.
[00:37:24.240 --> 00:37:28.240]   And at the moment, you know, so many of these experiments, when we actually...
[00:37:28.240 --> 00:37:35.240]   When they touch reality, they just fail miserably because they're just a little bit too much friction for the consumer to figure it out, make it work, and it doesn't look quite right.
[00:37:35.240 --> 00:37:39.240]   Maybe because I'm a TV producer at heart.
[00:37:39.240 --> 00:37:47.240]   When I try AR apps, I ultimately don't trust them because I don't trust the color temperature in the cameras.
[00:37:47.240 --> 00:37:53.240]   And therefore, I feel like an example is I've done a couple stories on the makeup apps.
[00:37:53.240 --> 00:37:56.240]   Like, L'Oreal has a very good makeup app to try different lipsticks.
[00:37:56.240 --> 00:37:57.240]   Sephora does too.
[00:37:57.240 --> 00:38:01.240]   But I don't trust the shade. It's all about shade, right?
[00:38:01.240 --> 00:38:05.240]   So why would I trust the shade if my skin doesn't look like it's the right tone?
[00:38:05.240 --> 00:38:14.240]   Sephora said they spent a lot of time making sure that their lipstick color in augmented reality matches the physical product when you buy it.
[00:38:14.240 --> 00:38:19.240]   And so there is a way to do it right, but you're right.
[00:38:19.240 --> 00:38:30.240]   Because of how augmented reality works, it either has a point cloud and then it converts that to polygons like in the HoloLens, and then it overlays video on those polygons.
[00:38:30.240 --> 00:38:32.240]   So polygons are little triangles, right?
[00:38:32.240 --> 00:38:38.240]   So think about if I want to take you and put you into augmented reality, I have to cut you up into little triangles.
[00:38:38.240 --> 00:38:46.240]   And that makes things look a little weird, and it often doesn't have the right color, because it's a lot of math going on.
[00:38:46.240 --> 00:38:47.240]   Yeah.
[00:38:47.240 --> 00:38:58.240]   Give us your big pronouncement on how this product launch, the rollout of AR in OS 11, is going to be seen when we look back in history.
[00:38:58.240 --> 00:39:02.240]   Is it a tipping point? Is it just a small marker along the way?
[00:39:02.240 --> 00:39:04.240]   It's a stepping stone.
[00:39:04.240 --> 00:39:22.240]   We know that in three years we're going to see a lot of glasses get unlocked because the companies are spending billions of dollars each, Apple, Facebook, Snap, Google, Huawei, Samsung, Microsoft, all have an Amazon, right?
[00:39:22.240 --> 00:39:25.240]   All have, well, Magic Leap 2, yeah.
[00:39:25.240 --> 00:39:34.240]   All have billion dollar efforts underway on the glasses, and we can see that they're coming sometime in the next five years, right?
[00:39:34.240 --> 00:39:37.240]   And Magic Leap is rumored to be coming in December.
[00:39:37.240 --> 00:39:38.240]   Really?
[00:39:38.240 --> 00:39:39.240]   Yeah.
[00:39:39.240 --> 00:39:42.240]   For about $2,000 with a developer preview, so that's a hint.
[00:39:42.240 --> 00:39:46.240]   And the Magic Leap is still tethered to a box that you'll have in your pocket.
[00:39:46.240 --> 00:39:49.240]   So it's still a little dorky.
[00:39:49.240 --> 00:39:58.240]   And we haven't yet escaped the dorky factor until we do, that's going to keep it out of mass market, right?
[00:39:58.240 --> 00:40:02.240]   This mass market that won't put up with dorky factor.
[00:40:02.240 --> 00:40:05.240]   Says the man who wore a Google Glass in the shower.
[00:40:05.240 --> 00:40:06.240]   I'll wear a Google Glass in the shower.
[00:40:06.240 --> 00:40:08.240]   Well, I work for a year, right?
[00:40:08.240 --> 00:40:11.240]   And I don't mind looking like a dork because I am a dork, right?
[00:40:11.240 --> 00:40:13.240]   Badgerbaunter, badgerbaunter.
[00:40:13.240 --> 00:40:15.240]   Badgerbaunter, I'm a nerd, right?
[00:40:15.240 --> 00:40:19.240]   And I'm not offended by that.
[00:40:19.240 --> 00:40:22.240]   A lot of people are.
[00:40:22.240 --> 00:40:24.240]   But I didn't wear my haul ins today.
[00:40:24.240 --> 00:40:25.240]   It's too heavy.
[00:40:25.240 --> 00:40:29.240]   It's just too heavy to wear for more than half an hour.
[00:40:29.240 --> 00:40:36.240]   I want to finish up with Apple releases this week by just talking a little bit about the Apple Watch 3.
[00:40:36.240 --> 00:40:39.240]   So the embargo, you got one on, okay?
[00:40:39.240 --> 00:40:40.240]   No, I don't have the three.
[00:40:40.240 --> 00:40:41.240]   I have the old one.
[00:40:41.240 --> 00:40:42.240]   The old one.
[00:40:42.240 --> 00:40:44.240]   I have the old original.
[00:40:44.240 --> 00:40:48.240]   It seems like it just came out.
[00:40:48.240 --> 00:40:50.240]   What do you mean it's old and original?
[00:40:50.240 --> 00:40:56.240]   Well, you're probably better off because the reviews have not been kind to the Apple Watch 3,
[00:40:56.240 --> 00:41:00.240]   which was supposed to have its own cellular connection.
[00:41:00.240 --> 00:41:03.240]   Didn't need your phone to get notifications.
[00:41:03.240 --> 00:41:05.240]   Which is great for runners, right?
[00:41:05.240 --> 00:41:11.240]   If you're going running, you don't want to carry your phone with you and wear your watch.
[00:41:11.240 --> 00:41:14.240]   And then you can still pick up calls from businesses or whatnot.
[00:41:14.240 --> 00:41:17.240]   I remember reading something and this goes back to my--
[00:41:17.240 --> 00:41:18.240]   Canting to them.
[00:41:18.240 --> 00:41:19.240]   Yeah.
[00:41:19.240 --> 00:41:21.240]   I'm on the run.
[00:41:21.240 --> 00:41:27.240]   I remember reading something just as a consumer reporter that women who are joggers today,
[00:41:27.240 --> 00:41:31.240]   95% will not run without their phone as a safety issue.
[00:41:31.240 --> 00:41:32.240]   Security measure.
[00:41:32.240 --> 00:41:33.240]   Yeah.
[00:41:33.240 --> 00:41:37.240]   And so I thought this is actually the killer app for runners.
[00:41:37.240 --> 00:41:44.240]   And the news is basically from-- well, we'll start with the Wall Street Journal's Joanna Stern.
[00:41:44.240 --> 00:41:46.240]   She says that it wasn't just her.
[00:41:46.240 --> 00:41:51.240]   It was also Jeffrey Fowler, quote, "experienced cellular connectivity issues on three separate
[00:41:51.240 --> 00:41:57.240]   pre-production models in two different states on two different 4G LTE carriers.
[00:41:57.240 --> 00:42:01.240]   On the AT&T connected models, the cellular connection dropped calls and were choppy.
[00:42:01.240 --> 00:42:04.240]   Siri often failed to connect.
[00:42:04.240 --> 00:42:07.240]   On the one that ran on T-Mobile dropped connections.
[00:42:07.240 --> 00:42:12.240]   They asked Apple about the issues and they said, "Hey, we haven't seen any of this in our testing.
[00:42:12.240 --> 00:42:14.240]   We're going to look into it."
[00:42:14.240 --> 00:42:16.240]   Then Lauren got at the verge, had similar issues.
[00:42:16.240 --> 00:42:21.240]   I think she was out on a surfboard and was woefully disappointed with her ability to call people
[00:42:21.240 --> 00:42:22.240]   and say, "Hey, I'm surfing."
[00:42:22.240 --> 00:42:26.240]   Yeah, if you can't call somebody from your watch when surfing.
[00:42:26.240 --> 00:42:27.240]   What is this country coming to?
[00:42:27.240 --> 00:42:31.240]   I mean, the Dick Tracy Promise has fallen.
[00:42:31.240 --> 00:42:32.240]   Absolutely.
[00:42:32.240 --> 00:42:33.240]   This is insane.
[00:42:33.240 --> 00:42:39.240]   It's like Stone Age led to Donner Party had it easier.
[00:42:39.240 --> 00:42:42.240]   Apple did issue a statement on this.
[00:42:42.240 --> 00:42:48.240]   They said they discovered when it tries to join unauthenticated Wi-Fi networks.
[00:42:48.240 --> 00:42:51.240]   It prevents the watch from using cellular connections.
[00:42:51.240 --> 00:42:56.240]   I'm not sure why it was trying to do that while Lauren Good was surfing, but they said
[00:42:56.240 --> 00:42:58.240]   they're investigating a fix.
[00:42:58.240 --> 00:43:05.240]   Does this, what's been thrown out there, is that this is the least fully cooked release
[00:43:05.240 --> 00:43:07.240]   Apple has ever made.
[00:43:07.240 --> 00:43:09.240]   Is that a strong, too strong?
[00:43:09.240 --> 00:43:13.240]   I'm seeing bugs on iOS 11.
[00:43:13.240 --> 00:43:17.240]   Sometimes my phone doesn't even light up for a couple minutes.
[00:43:17.240 --> 00:43:21.240]   I'm seeing a lot of complaints on Facebook of various problems.
[00:43:21.240 --> 00:43:27.240]   The watch shows that they're having some problem on testing things and getting them done
[00:43:27.240 --> 00:43:28.240]   properly.
[00:43:28.240 --> 00:43:34.240]   I assume they get fixed because we've been through this with Apple before.
[00:43:34.240 --> 00:43:37.240]   The first versions of iOS always have some bugs.
[00:43:37.240 --> 00:43:39.240]   I don't know what gate this is.
[00:43:39.240 --> 00:43:41.240]   Is this watch Wi-Fi cellular gate?
[00:43:41.240 --> 00:43:43.240]   That's just not sexy.
[00:43:43.240 --> 00:43:49.240]   I don't know how many people are going to go by the watch and want to use it this way.
[00:43:49.240 --> 00:43:50.240]   It's a problem and no fix.
[00:43:50.240 --> 00:43:54.240]   I've been waiting for it because that's what I wanted was I wanted to be able to go.
[00:43:54.240 --> 00:43:56.240]   You want to get rid of your phone?
[00:43:56.240 --> 00:43:58.240]   You want to go free phone free?
[00:43:58.240 --> 00:43:59.240]   Phone free.
[00:43:59.240 --> 00:44:02.240]   We don't have pockets.
[00:44:02.240 --> 00:44:08.240]   I put mine in my jog bra and then I'm worried it's going to get the water.
[00:44:08.240 --> 00:44:10.240]   We have problems.
[00:44:10.240 --> 00:44:13.240]   I can imagine the market slides.
[00:44:13.240 --> 00:44:17.240]   We have a user in the chat room who says LTE on a watch is fantastic.
[00:44:17.240 --> 00:44:20.240]   Some people are having a good time with it.
[00:44:20.240 --> 00:44:22.240]   That's always true when you hit bugs.
[00:44:22.240 --> 00:44:26.240]   I worked at Microsoft and we knew all of our software had bugs.
[00:44:26.240 --> 00:44:29.240]   They had bug lists on the internet.
[00:44:29.240 --> 00:44:34.240]   But when you have an obscure bug, it might hit a few dozen people.
[00:44:34.240 --> 00:44:37.240]   Those people will be very noisy on Facebook.
[00:44:37.240 --> 00:44:41.240]   I'm having a bug on iOS 11 on my phone right now.
[00:44:41.240 --> 00:44:47.240]   Not everybody is because it hasn't come up as an issue with a lot of people.
[00:44:47.240 --> 00:44:51.240]   Bob Sullivan, does this feel like a half-baked consumer product that we're not used to seeing?
[00:44:51.240 --> 00:44:54.240]   You're not used to seeing quite at this level from Apple?
[00:44:54.240 --> 00:44:59.240]   Well, when you combine this with the lack of lines at Apple Store, over the new phone,
[00:44:59.240 --> 00:45:02.240]   clearly you get the sense that maybe this is an Apple's greatest day.
[00:45:02.240 --> 00:45:07.240]   But I do have to say this issue of handing off between Wi-Fi networks and cell networks.
[00:45:07.240 --> 00:45:09.240]   I've never had a phone that didn't have that problem.
[00:45:09.240 --> 00:45:12.240]   I don't understand why it's such a difficult problem to solve.
[00:45:12.240 --> 00:45:17.240]   But lots of people are frustrated with trying to navigate from a cell connection when they walk between Wi-Fi and whatnot.
[00:45:17.240 --> 00:45:20.240]   Especially right now, when people are trying so desperately to not use up data,
[00:45:20.240 --> 00:45:23.240]   so they try to hop on Wi-Fi as often as they can.
[00:45:23.240 --> 00:45:26.240]   And you walk through neighborhoods and it connects to a hotspot disconnects.
[00:45:26.240 --> 00:45:28.240]   So I think this is a pretty typical problem.
[00:45:28.240 --> 00:45:31.240]   All right. We are going to take a quick break.
[00:45:31.240 --> 00:45:38.240]   When we come back, we're going to figure out why Google bought HTC or bought part of HTC?
[00:45:38.240 --> 00:45:39.240]   Or did they buy HTC?
[00:45:39.240 --> 00:45:41.240]   Or did they just acquire?
[00:45:41.240 --> 00:45:43.240]   How do you say that word? Acquire?
[00:45:43.240 --> 00:45:44.240]   Some of HTC?
[00:45:44.240 --> 00:45:46.240]   It's like the double rainbow.
[00:45:46.240 --> 00:45:48.240]   What does it mean?
[00:45:48.240 --> 00:45:52.240]   But first, here's Leo with information on one of our sponsors.
[00:45:52.240 --> 00:45:56.240]   Hey, Becky. So nice to see you. Let me interrupt just briefly.
[00:45:56.240 --> 00:46:02.240]   For a moment, we'll get back to Twitter in a second, but our show today brought to you by our sponsor, Betterment.
[00:46:02.240 --> 00:46:05.240]   I'm really interested in this.
[00:46:05.240 --> 00:46:07.240]   You know, unfortunately, I can't use it.
[00:46:07.240 --> 00:46:12.240]   But I tell everybody I know, including family members, to use it.
[00:46:12.240 --> 00:46:18.240]   One of my family members, a young person, my daughter, just starting out in life,
[00:46:18.240 --> 00:46:21.240]   she had all of her money in a CD.
[00:46:21.240 --> 00:46:25.240]   And every three months it would roll over and she's getting like less than 1%, right?
[00:46:25.240 --> 00:46:28.240]   She said, "Dad, help me invest it." And what did I tell her? Betterment.
[00:46:28.240 --> 00:46:35.240]   Now, the reason is, these days invest, first of all, you've got to have long-term savings.
[00:46:35.240 --> 00:46:38.240]   I'm proud of her at the age of 25 that she has savings.
[00:46:38.240 --> 00:46:46.240]   Really important, you're saving for retirement. At her age, she's probably saving, you know, for kids going to college.
[00:46:46.240 --> 00:46:50.240]   Maybe a house, rainy day fund. You've got to save.
[00:46:50.240 --> 00:46:54.240]   But you can't just put it in a mattress or, you know, it's practically as bad to put it in a CD.
[00:46:54.240 --> 00:46:58.240]   You've got to invest. But it's a challenge.
[00:46:58.240 --> 00:47:06.240]   If you don't have the time to do the research, and even if you do, it's hard to compete with the people who do this for a living.
[00:47:06.240 --> 00:47:10.240]   That's why I recommend and betterment to everybody.
[00:47:10.240 --> 00:47:14.240]   It's the largest independent online financial advisor.
[00:47:14.240 --> 00:47:18.240]   Betterment is designed to help improve your long-term returns.
[00:47:18.240 --> 00:47:24.240]   And I like this, lower taxes for retirement planning, for building wealth, for all your financial goals.
[00:47:24.240 --> 00:47:29.240]   270,000 customers to date. Many of them I have sent, I think.
[00:47:29.240 --> 00:47:36.240]   Based on what you share, betterment will, including risk tolerance, your plans, they ask to be a few questions.
[00:47:36.240 --> 00:47:42.240]   It made Taylor recommendations on decisions like how much to invest, how much risks to take on.
[00:47:42.240 --> 00:47:45.240]   You know, she's young, so she can take on more risk.
[00:47:45.240 --> 00:47:48.240]   She's probably, I think, those, she's kind of risk adverse.
[00:47:48.240 --> 00:47:52.240]   She doesn't want, so it balances it based on what you want.
[00:47:52.240 --> 00:47:56.240]   Features like betterment, she loved this socially responsible investing portfolio.
[00:47:56.240 --> 00:48:04.240]   Their SRI, I'll give you the flexibility to reduce your investment in companies that don't meet your social, environmental, and government's benchmarks.
[00:48:04.240 --> 00:48:06.240]   So you can feel good about your investments.
[00:48:06.240 --> 00:48:13.240]   Betterment gives you a clearer view of your net worth when you sink in outside accounts, like bank accounts and other investments.
[00:48:13.240 --> 00:48:19.240]   But best of all, their fees are transparent and low compared to traditional services.
[00:48:19.240 --> 00:48:22.240]   I'm not sure which one Abby's going to choose.
[00:48:22.240 --> 00:48:27.240]   You could get, you can, for .25% a year, that's all.
[00:48:27.240 --> 00:48:32.240]   You can get unlimited messaging access to their team of licensed financial experts through their mobile apps messaging feature.
[00:48:32.240 --> 00:48:34.240]   By the way, these are fiduciaries.
[00:48:34.240 --> 00:48:45.240]   These are people who are not paid, they're not on commission, they're not paid by the companies they're investing in, they're paid by you, by your fees, and they work in your, on your behalf alone.
[00:48:45.240 --> 00:48:47.240]   So you really can trust them.
[00:48:47.240 --> 00:48:57.240]   I had to kind of explain to Abby what fiduciary means and why this is so important that some financial advisors are not really acting on your behalf.
[00:48:57.240 --> 00:49:00.240]   But they are at betterment.
[00:49:00.240 --> 00:49:05.240]   If you want, and I think this is maybe something down the road she'd want, you can upgrade to .4%.
[00:49:05.240 --> 00:49:10.240]   That's all .4% for unlimited phone access to their team of certified financial planners.
[00:49:10.240 --> 00:49:14.240]   I think as she gets older, a financial planner will be really a value to her.
[00:49:14.240 --> 00:49:16.240]   And she doesn't want to listen to me.
[00:49:16.240 --> 00:49:18.240]   She wants, you know, she doesn't want to hear what dad has to say.
[00:49:18.240 --> 00:49:20.240]   Betterment is a fiduciary.
[00:49:20.240 --> 00:49:24.240]   So that means they don't get commissions for recommending funds.
[00:49:24.240 --> 00:49:26.240]   They don't have funds of their own.
[00:49:26.240 --> 00:49:28.240]   And they do what's right for you.
[00:49:28.240 --> 00:49:30.240]   They're working for you.
[00:49:30.240 --> 00:49:33.240]   Betterment cares about keeping your money and data secure too.
[00:49:33.240 --> 00:49:34.240]   They do it all right.
[00:49:34.240 --> 00:49:35.240]   I checked it out.
[00:49:35.240 --> 00:49:36.240]   Absolutely.
[00:49:36.240 --> 00:49:40.240]   Full data encryption, hash, salted, two-factor authentication.
[00:49:40.240 --> 00:49:42.240]   Very, very important.
[00:49:42.240 --> 00:49:46.240]   Your stuff is secure and private at betterment.
[00:49:46.240 --> 00:49:49.240]   Now, as I told Abby, investing involves risk.
[00:49:49.240 --> 00:49:51.240]   I don't want her to have, you know, "Oh, it's automatic.
[00:49:51.240 --> 00:49:52.240]   You're going to make money."
[00:49:52.240 --> 00:50:01.240]   But I have to say, with the right advice, doing it smart, if you start young, by the time you're my age, you'll have something to live on.
[00:50:01.240 --> 00:50:02.240]   I didn't start so young.
[00:50:02.240 --> 00:50:03.240]   I wish I had.
[00:50:03.240 --> 00:50:09.240]   And unfortunately, due to SEC regulations, because I do the ads for betterment, I can't use them.
[00:50:09.240 --> 00:50:11.240]   It kills me.
[00:50:11.240 --> 00:50:13.240]   But if I could, I would.
[00:50:13.240 --> 00:50:15.240]   And certainly members of my family do.
[00:50:15.240 --> 00:50:17.240]   Betterment.com/twit.
[00:50:17.240 --> 00:50:24.240]   You can get up to one year managed free if you want to know more, visit betterment.com/twit.
[00:50:24.240 --> 00:50:25.240]   Don't put this off.
[00:50:25.240 --> 00:50:28.240]   Now's the time to start planning for your future.
[00:50:28.240 --> 00:50:30.240]   Betterment.com/twit.
[00:50:30.240 --> 00:50:32.240]   Now, back to Warly.
[00:50:32.240 --> 00:50:33.240]   Hello, Warly.
[00:50:33.240 --> 00:50:34.240]   [laughs]
[00:50:34.240 --> 00:50:35.240]   Uh-oh.
[00:50:35.240 --> 00:50:37.240]   Leo's doing his Russian accent.
[00:50:37.240 --> 00:50:38.240]   We're in trouble.
[00:50:38.240 --> 00:50:40.240]   Yep, we are back with This Week in Tech.
[00:50:40.240 --> 00:50:41.240]   I'm Becky Warly.
[00:50:41.240 --> 00:50:51.240]   And joined by Rob Reed, Robert Scobel, and Bob Sullivan, you may have noticed this is the All Robert Show.
[00:50:51.240 --> 00:50:52.240]   Yep.
[00:50:52.240 --> 00:50:53.240]   You must be called Robert.
[00:50:53.240 --> 00:51:02.240]   Now, I want you to know, I thought about it, and just to really be progressive in how we book this show, next week, it's going to be the All Richard Show.
[00:51:02.240 --> 00:51:05.240]   We're going to have a Richard, a Rich, and a Dick.
[00:51:05.240 --> 00:51:06.240]   So that's how it's going to go.
[00:51:06.240 --> 00:51:07.240]   Wow.
[00:51:07.240 --> 00:51:08.240]   Yeah.
[00:51:08.240 --> 00:51:10.240]   So you guys, we're going to have to figure out who those people are.
[00:51:10.240 --> 00:51:11.240]   You got a week to do it.
[00:51:11.240 --> 00:51:12.240]   Yeah.
[00:51:12.240 --> 00:51:14.240]   I think Dick Deep Artolo is going to, we're going to figure it out.
[00:51:14.240 --> 00:51:15.240]   Yeah, you're going to do it.
[00:51:15.240 --> 00:51:16.240]   It's going to happen.
[00:51:16.240 --> 00:51:20.240]   I want to get us back into the swing of things by talking about Google.
[00:51:20.240 --> 00:51:26.240]   They announced this week it's acquiring a big portion of Taiwan hardware maker HTC.
[00:51:26.240 --> 00:51:31.240]   Google pays $1.1 billion, gets 2,000 new employees.
[00:51:31.240 --> 00:51:37.240]   HTC keeps about 2,000 employees, gets a ton of cash, which they need desperately.
[00:51:37.240 --> 00:51:44.240]   And this is the interesting part is that HTC is going to continue making its own HTC branded devices.
[00:51:44.240 --> 00:51:51.240]   Simultaneously, we know that Google is going to announce the Pixel 2 on October 4th.
[00:51:51.240 --> 00:51:55.240]   So I promise that this would be the double rainbow explanation.
[00:51:55.240 --> 00:51:57.240]   What does it mean?
[00:51:57.240 --> 00:52:03.240]   It means HTC has some more money to invest in its VR headset, which didn't go to Google.
[00:52:03.240 --> 00:52:07.240]   Google bought the Pixel team from last year.
[00:52:07.240 --> 00:52:11.240]   HTC made the first Pixel last year.
[00:52:11.240 --> 00:52:18.240]   And those people are going to Google, which gives you a hint to why Google bought this group of people at HTC.
[00:52:18.240 --> 00:52:26.240]   But this gives the other HTC the money to keep investing in VR until VR starts catching with consumers,
[00:52:26.240 --> 00:52:27.240]   but it has not.
[00:52:27.240 --> 00:52:30.240]   So that's the good side for HTC.
[00:52:30.240 --> 00:52:33.240]   But then let's look at it from the Google side.
[00:52:33.240 --> 00:52:41.240]   So HTC's stock price is down 96% from what it was at its all time high way, way back.
[00:52:41.240 --> 00:52:44.240]   It's bleeding cash, struggling.
[00:52:44.240 --> 00:52:52.240]   Do you think because Google felt held hostage by the fact that one of their best hardware manufacturers is struggling?
[00:52:52.240 --> 00:52:54.240]   Did they have to do this?
[00:52:54.240 --> 00:53:00.240]   No, I think Google is gearing up for a new age in consumer electronics of glasses,
[00:53:00.240 --> 00:53:08.240]   and they need more people who know how to build things, because Google knows it's going to be in a war with Apple
[00:53:08.240 --> 00:53:13.240]   and Microsoft and Amazon, and is gearing up for that war.
[00:53:13.240 --> 00:53:16.240]   And I think it's a smart purchase on that point.
[00:53:16.240 --> 00:53:22.240]   I don't think Google really cares about the business interests of HTC and whether it survives or not.
[00:53:22.240 --> 00:53:28.240]   Google has plenty of great partners with Samsung, LG is making the new Pixel 2 phone,
[00:53:28.240 --> 00:53:33.240]   and Huawei, I'm going to Huawei's launch of the Mate 9 or 10,
[00:53:33.240 --> 00:53:35.240]   coming up in a couple weeks in Germany.
[00:53:35.240 --> 00:53:39.240]   And so Google has plenty of Android partners who are doing real well,
[00:53:39.240 --> 00:53:42.240]   and that's why HTC has been in trouble, by the way.
[00:53:42.240 --> 00:53:47.240]   Samsung and Huawei have been taking it apart in the worldwide market.
[00:53:47.240 --> 00:53:50.240]   So Google wants to be in the hardware business.
[00:53:50.240 --> 00:53:53.240]   They, or they have to be in order to be competitive.
[00:53:53.240 --> 00:53:55.240]   They can't just be a service.
[00:53:55.240 --> 00:53:58.240]   And you're seeing all sorts of new products coming out, right?
[00:53:58.240 --> 00:54:03.240]   Amazon has the Echo, Google has Google Home, and a bunch of other products.
[00:54:03.240 --> 00:54:06.240]   I have a new Wi-Fi mesh network that comes from Google, right?
[00:54:06.240 --> 00:54:11.240]   So there's plenty of places to use engineers and supply chain experts
[00:54:11.240 --> 00:54:14.240]   who know how to get things built to build real product.
[00:54:14.240 --> 00:54:16.240]   So maybe they thought this was a fire sale.
[00:54:16.240 --> 00:54:18.240]   They could get a lot of good talent.
[00:54:18.240 --> 00:54:22.240]   Okay. $550,000 per employee.
[00:54:22.240 --> 00:54:24.240]   I did the math on my calculator.
[00:54:24.240 --> 00:54:29.240]   The 1.1 billion 2,000 employees, $550 grand.
[00:54:29.240 --> 00:54:33.240]   That's a big recruiting sum, but they are obviously coming in and integrated teams
[00:54:33.240 --> 00:54:35.240]   that know how to do stuff.
[00:54:35.240 --> 00:54:38.240]   I mean, do either or any of you, do any of the three of you think that Google
[00:54:38.240 --> 00:54:41.240]   now regrets losing or getting rid of Motorola?
[00:54:41.240 --> 00:54:45.240]   Because they're kind of, you know, they had a lot of these people good at making stuff.
[00:54:45.240 --> 00:54:50.240]   That was the first comment when we started talking about HGC's rumor
[00:54:50.240 --> 00:54:53.240]   that Google was going to be buying a piece of it was,
[00:54:53.240 --> 00:54:56.240]   "Oh, didn't they screw up Motorola before?"
[00:54:56.240 --> 00:54:58.240]   And it hasn't been that long, has it?
[00:54:58.240 --> 00:55:00.240]   That's what big companies do.
[00:55:00.240 --> 00:55:04.240]   They buy smaller companies and they screw it up, and then they try it again,
[00:55:04.240 --> 00:55:05.240]   and screw it up again.
[00:55:05.240 --> 00:55:08.240]   And we'll see if this actually makes something out of it.
[00:55:08.240 --> 00:55:13.240]   But you can tell Google is in a different space than it was back at the Motorola launch.
[00:55:13.240 --> 00:55:19.240]   Google has quite a few hardware products now, and we're entering a new era of competition
[00:55:19.240 --> 00:55:21.240]   among these big players.
[00:55:21.240 --> 00:55:25.240]   So, staffing up at this point makes a lot of sense to me.
[00:55:25.240 --> 00:55:28.240]   Let's hit some of those products just so we go through them.
[00:55:28.240 --> 00:55:41.240]   So, the Pixel 2 XL, we saw leaks in black and white, just black, maybe $849 for 64 gigabyte phone.
[00:55:41.240 --> 00:55:45.240]   That's coming from DroidLife, I think.
[00:55:45.240 --> 00:55:49.240]   Pixel 2, three colors, kind of blue is one of those colors.
[00:55:49.240 --> 00:55:51.240]   My oldest Davis reference, right?
[00:55:51.240 --> 00:55:55.240]   At it? Okay, that's very googly to do that, right?
[00:55:55.240 --> 00:56:03.240]   And the Pixel Book, Expensive, is a $1,200 Chromebook-ish?
[00:56:03.240 --> 00:56:07.240]   I mean, that's, that's spandy, but people are excited about it.
[00:56:07.240 --> 00:56:09.240]   Probably with a really nice old-led screen on it, right?
[00:56:09.240 --> 00:56:11.240]   Yeah, I think so.
[00:56:11.240 --> 00:56:19.240]   So, all this Google announcing all this October 4th in San Francisco,
[00:56:19.240 --> 00:56:25.240]   yeah, I think it's, they, they obviously are in the hardware business and feel like they have to compete.
[00:56:25.240 --> 00:56:31.240]   I'm going to move on to a couple more little sort of business-y stories just to get, move us through.
[00:56:31.240 --> 00:56:41.240]   Um, T-Mobile and Sprint, there are noises, Reuters reporting that they, the two companies are close to reaching tentative terms,
[00:56:41.240 --> 00:56:45.240]   deal rumored to be coming down the pike by the end of October.
[00:56:45.240 --> 00:56:50.240]   So, Sprint and T-Mobile are the number three and number four wireless carriers.
[00:56:50.240 --> 00:56:54.240]   If they combined into one, it would be 130 million subscribers.
[00:56:54.240 --> 00:57:00.240]   T-Mobile CEO, John Leger, he's a wild man, would remain the CEO of the new company.
[00:57:00.240 --> 00:57:10.240]   You know, they've had big regulatory issues all along, but you're going to remember that Sprint's major stockholder, SoftBank,
[00:57:10.240 --> 00:57:19.240]   SoftBank's owner, Masayoshi Sun, met with President Trump at the end of last year and then announced at the beginning of this year that we're going to have,
[00:57:19.240 --> 00:57:23.240]   this is going to be great, we're going to benefit from the deregulation.
[00:57:23.240 --> 00:57:29.240]   Sprint, joining with T-Mobile, Bob Sullivan, is this good for us?
[00:57:29.240 --> 00:57:37.240]   Anybody who likes their brand new, great, all-you-can-eat cell phone plan is going to really hate this.
[00:57:37.240 --> 00:57:42.240]   The best thing that happened to consumers was the last time a big cell phone company merger was blocked.
[00:57:42.240 --> 00:57:50.240]   And what we saw out of that was all of these incredibly aggressive pricing moves by T-Mobile that led the industry down and down into a situation where,
[00:57:50.240 --> 00:57:58.240]   you know, we all lost unlimited data plans now, we have them back and we have anti-trust lawyers in the,
[00:57:58.240 --> 00:58:00.240]   in Washington, D.C. I think to thank for that.
[00:58:00.240 --> 00:58:05.240]   So, I mean, of course, in order to stay competitive, you probably eventually figured these two were going to match up,
[00:58:05.240 --> 00:58:14.240]   but any time, as far as I'm concerned, any time a quadopoly goes to a triopoly, which it's bad for consumers,
[00:58:14.240 --> 00:58:17.240]   and ultimately we're going to get less choice and higher prices as a result.
[00:58:17.240 --> 00:58:20.240]   Is this going to go through, do you guys think?
[00:58:20.240 --> 00:58:23.240]   Over my pay grade.
[00:58:23.240 --> 00:58:27.240]   Wait a second, this is no business show.
[00:58:27.240 --> 00:58:34.240]   Well, I think it's less, it's more likely to go through under a Republican administration, which tends to be easier on antitrust,
[00:58:34.240 --> 00:58:39.240]   you know, and that it, that will, this decision will be made at the pinnacle of the DOJ, which is, you know,
[00:58:39.240 --> 00:58:41.240]   going to be political appointees.
[00:58:41.240 --> 00:58:43.240]   So, impossible to know.
[00:58:43.240 --> 00:58:47.240]   I think it would have been less likely to go through under Obama than, though, but we'll see.
[00:58:47.240 --> 00:58:48.240]   But Bob, what do you think?
[00:58:48.240 --> 00:58:50.240]   I think, you know, this stuff better than I do.
[00:58:50.240 --> 00:58:57.240]   I think this is, this is going to be another golden age of mergers and acquisitions like this because of the new environment in D.C.
[00:58:57.240 --> 00:59:00.240]   I don't think they'll have any trouble getting approval for this.
[00:59:00.240 --> 00:59:02.240]   And who becomes number three?
[00:59:02.240 --> 00:59:03.240]   Do they vault over AT&T?
[00:59:03.240 --> 00:59:06.240]   Is Verizon the largest or is AT&T the largest?
[00:59:06.240 --> 00:59:12.240]   They're surely going to become number two with 130 million subs because there are not 390 million Americans.
[00:59:12.240 --> 00:59:15.240]   Does anybody, I'll just carry, or does this make them number one?
[00:59:15.240 --> 00:59:16.240]   I don't know.
[00:59:16.240 --> 00:59:18.240]   Probably make some number one, doesn't it?
[00:59:18.240 --> 00:59:19.240]   Yeah.
[00:59:19.240 --> 00:59:20.240]   No idea.
[00:59:20.240 --> 00:59:22.240]   By subscriber, I do not know.
[00:59:22.240 --> 00:59:24.240]   This is where the Google is fascinating.
[00:59:24.240 --> 00:59:25.240]   This is where the Google is fascinating.
[00:59:25.240 --> 00:59:26.240]   We'll see how the, okay, all right.
[00:59:26.240 --> 00:59:27.240]   Verizon.
[00:59:27.240 --> 00:59:28.240]   Verizon's number one.
[00:59:28.240 --> 00:59:36.240]   140, as of March, 2017, total subs, 146 million, AT&T, 134.
[00:59:36.240 --> 00:59:39.240]   There are more subs than Americans.
[00:59:39.240 --> 00:59:44.240]   Well, they'll basically vault to number, tie for two or what it looks like.
[00:59:44.240 --> 00:59:49.240]   Well, do they count the watch sub and the phone sub?
[00:59:49.240 --> 00:59:50.240]   I don't know.
[00:59:50.240 --> 00:59:52.240]   Do you pay extra for the watch?
[00:59:52.240 --> 00:59:53.240]   Ten bucks a month.
[00:59:53.240 --> 00:59:54.240]   Ten bucks?
[00:59:54.240 --> 00:59:55.240]   Yeah, like the iPad.
[00:59:55.240 --> 00:59:56.240]   Yeah.
[00:59:56.240 --> 00:59:57.240]   That's not bad.
[00:59:57.240 --> 00:59:59.240]   See, piece of my running piece of mine for ten bucks a month.
[00:59:59.240 --> 01:00:00.240]   I'm going to pay that.
[01:00:00.240 --> 01:00:02.240]   You can't watch very many videos on your watch.
[01:00:02.240 --> 01:00:03.240]   That's right.
[01:00:03.240 --> 01:00:05.240]   Not when you're running, you can't.
[01:00:05.240 --> 01:00:08.240]   The battery issue sounded equally challenging.
[01:00:08.240 --> 01:00:13.240]   Now, as you guys may remember, I was Leo's TV producer when we first started at Tech TV.
[01:00:13.240 --> 01:00:18.240]   And my job was to time the show and make sure that we started on time and we ended on time.
[01:00:18.240 --> 01:00:26.240]   And I have been derelict in my duty today, filling in for him because I let our first segment go 44 minutes before we took an ad.
[01:00:26.240 --> 01:00:28.240]   So I am going to take a breath.
[01:00:28.240 --> 01:00:37.240]   Let Leo talk about one of our sponsors and then we are going to come back and we -- I'm so thrilled to have Bob Sullivan here because he's the king of all things Equifax.
[01:00:37.240 --> 01:00:45.240]   And if you wondered, what is the current state of this bleep show, he's going to fill us in and tell us what we should be doing.
[01:00:45.240 --> 01:00:47.240]   So Leo, take it away.
[01:00:47.240 --> 01:00:49.240]   Well, I'm more of this week in Tech in a moment.
[01:00:49.240 --> 01:00:50.240]   I'm sorry I'm not here.
[01:00:50.240 --> 01:00:51.240]   Wish I were.
[01:00:51.240 --> 01:00:53.240]   I'm going to wait too much fun.
[01:00:53.240 --> 01:00:54.240]   I will be back next week.
[01:00:54.240 --> 01:00:57.240]   Thank you, Becky, for holding down the fort.
[01:00:57.240 --> 01:00:59.240]   Our show today brought to you by Rocket Mortgage.
[01:00:59.240 --> 01:01:01.240]   You've heard me talk about them before.
[01:01:01.240 --> 01:01:06.240]   Rocket Mortgage comes from Quick and Loans, the best home loan company in the American country.
[01:01:06.240 --> 01:01:19.240]   I can say that without fear of contradiction, all you have to do is look at JD Power's Customer Satisfaction Awards number one year after year for both mortgage servicing and mortgage approval.
[01:01:19.240 --> 01:01:22.240]   And now they've taken it a step farther.
[01:01:22.240 --> 01:01:28.240]   I think this is a great company, Quick and Loans, making a product for you.
[01:01:28.240 --> 01:01:32.240]   So you the geek, us in other words.
[01:01:32.240 --> 01:01:40.240]   So if you're looking to get a home loan, unfortunately, I have to say, and I bought, I know, four houses in my life.
[01:01:40.240 --> 01:01:43.240]   Last time, about four years ago, Lisa and I bought our current house.
[01:01:43.240 --> 01:01:50.240]   And I didn't really, I just, I went to where the guy, the guy the realtor told us to go to, frankly.
[01:01:50.240 --> 01:01:55.240]   And it was that big bank, you know the one with a stagecoach and the horses.
[01:01:55.240 --> 01:01:56.240]   Yeah.
[01:01:56.240 --> 01:01:58.240]   And what a mistake.
[01:01:58.240 --> 01:02:04.240]   They, it was like, I don't know, like we were prisoners, like we don't trust you.
[01:02:04.240 --> 01:02:10.240]   It took more than a month, they asked for more and more information to the point we were going on vacation and they still wanted information.
[01:02:10.240 --> 01:02:14.240]   We were sending faxing information in from the boat on our cruise.
[01:02:14.240 --> 01:02:16.240]   This was four years ago.
[01:02:16.240 --> 01:02:17.240]   We almost lost the house.
[01:02:17.240 --> 01:02:18.240]   I am not kidding.
[01:02:18.240 --> 01:02:21.240]   It took us more than a month to get a home loan.
[01:02:21.240 --> 01:02:23.240]   Next time, Quick and Loans and Rocket Mortgage.
[01:02:23.240 --> 01:02:26.240]   Rocket's aptly named because it's fast.
[01:02:26.240 --> 01:02:31.240]   And it lifts the burden of getting a home loan because you don't have to go to the bank.
[01:02:31.240 --> 01:02:33.240]   You don't have to go to Quick and Loans even.
[01:02:33.240 --> 01:02:37.240]   You just go online to rocketmortgage.com/twit2.
[01:02:37.240 --> 01:02:41.240]   That's rocketmortgage.com/twit the number two.
[01:02:41.240 --> 01:02:43.240]   And you can do it all online and it's easy and it's fast.
[01:02:43.240 --> 01:02:53.240]   You don't have to go through boxes to find your check statements or your, you know, your pay stubs or anything like that because they have, of course, financial partners throughout the industry.
[01:02:53.240 --> 01:02:55.240]   They can get everything they need with your permission.
[01:02:55.240 --> 01:02:57.240]   All you have to do is give them permission.
[01:02:57.240 --> 01:03:03.240]   They will crunch the numbers and then tell you based on your income assets and credits what loans you qualify for.
[01:03:03.240 --> 01:03:06.240]   And this happens not in months or weeks or days but in minutes.
[01:03:06.240 --> 01:03:09.240]   Literally, minutes.
[01:03:09.240 --> 01:03:10.240]   It's completely transparent.
[01:03:10.240 --> 01:03:12.240]   You know exactly what's happening.
[01:03:12.240 --> 01:03:14.240]   You can understand the process fully.
[01:03:14.240 --> 01:03:21.240]   You get to choose the loan you want with absolute confidence because it's from Quick and Loans, the best lender in the country.
[01:03:21.240 --> 01:03:23.240]   I love this.
[01:03:23.240 --> 01:03:27.240]   You could, it's so fast you could do it in an open house and have your loan before you left.
[01:03:27.240 --> 01:03:29.240]   Say, look, we're approved.
[01:03:29.240 --> 01:03:32.240]   Rocketmortgage from Quick and Loans.
[01:03:32.240 --> 01:03:33.240]   Apply simply.
[01:03:33.240 --> 01:03:38.240]   Understand fully mortgage confidently at rocketmortgage.com/twit2.
[01:03:38.240 --> 01:03:41.240]   You're probably not buying a house this moment.
[01:03:41.240 --> 01:03:42.240]   Maybe you are.
[01:03:42.240 --> 01:03:47.240]   If you're not bookmark rocketmortgage.com/twit2 and use it when you do, you'll be glad.
[01:03:47.240 --> 01:03:48.240]   You can thank me later.
[01:03:48.240 --> 01:03:53.240]   Equal housing lender licensed in all 50 states and MLS consumer access.org number 30.
[01:03:53.240 --> 01:03:54.240]   30.
[01:03:54.240 --> 01:03:56.240]   Rocketmortgage from Quick and Loans.
[01:03:56.240 --> 01:03:57.240]   We thank them for their support.
[01:03:57.240 --> 01:03:58.240]   You can bet.
[01:03:58.240 --> 01:04:01.240]   Next time we buy or refi, we'll be going to rocketmortgage.
[01:04:01.240 --> 01:04:05.240]   Now back to Becky Wole.
[01:04:05.240 --> 01:04:06.240]   And Twit.
[01:04:06.240 --> 01:04:08.240]   Thank you, Leo.
[01:04:08.240 --> 01:04:09.240]   This week in tech continues.
[01:04:09.240 --> 01:04:10.240]   I am Becky Wole.
[01:04:10.240 --> 01:04:14.240]   As Leo just mentioned, I want to focus this block on security.
[01:04:14.240 --> 01:04:17.240]   It seems like it's been a huge week.
[01:04:17.240 --> 01:04:19.240]   We're going to talk about the SEC Edgar breach.
[01:04:19.240 --> 01:04:24.240]   We're going to talk about the sea cleaner breach and some interesting developments about
[01:04:24.240 --> 01:04:26.240]   who that was really targeting.
[01:04:26.240 --> 01:04:30.240]   But I want to start out with the Equifax breach.
[01:04:30.240 --> 01:04:34.240]   It's like a cascade of developments that just keeps getting worse.
[01:04:34.240 --> 01:04:40.240]   So Bob Sullivan, you posted, you have been continually posting and covering this story.
[01:04:40.240 --> 01:04:46.240]   And you sounded like a total voice of reason as the advice was flying willy nilly.
[01:04:46.240 --> 01:04:49.240]   So can you just give us a timeline of what we know now?
[01:04:49.240 --> 01:04:50.240]   What happened?
[01:04:50.240 --> 01:04:51.240]   When did it happen?
[01:04:51.240 --> 01:04:52.240]   Or why is that?
[01:04:52.240 --> 01:04:55.240]   What's the state of the state with Equifax right now?
[01:04:55.240 --> 01:04:56.240]   Sure.
[01:04:56.240 --> 01:05:02.240]   Well, just to bring everybody up to speed, I think everybody knows that 143 million consumers,
[01:05:02.240 --> 01:05:06.240]   not customers, consumers had their information exposed in this hack.
[01:05:06.240 --> 01:05:09.240]   And that's probably about three quarters of Americans who have a credit report.
[01:05:09.240 --> 01:05:11.240]   So we might as well just say everybody.
[01:05:11.240 --> 01:05:16.240]   And we learned about this two weeks ago, over the past few weeks, little pieces of what
[01:05:16.240 --> 01:05:18.240]   has actually happened have dribbled out.
[01:05:18.240 --> 01:05:20.240]   So we still don't know a lot of important things.
[01:05:20.240 --> 01:05:24.240]   The most important question of all for me is who did this and why?
[01:05:24.240 --> 01:05:27.240]   And Equifax has been strikingly silent on that issue.
[01:05:27.240 --> 01:05:31.240]   It's really important because I think the steps that people should take to protect themselves
[01:05:31.240 --> 01:05:36.240]   vary a lot based on who at least we suspect was responsible for this and what their intention
[01:05:36.240 --> 01:05:38.240]   with the data was.
[01:05:38.240 --> 01:05:41.240]   It could be anything from just a drive by hack.
[01:05:41.240 --> 01:05:44.240]   We now know that the way they broken was not complicated.
[01:05:44.240 --> 01:05:48.240]   It was at that point a fairly well known actively exploited attack.
[01:05:48.240 --> 01:05:53.240]   Which is horrific to think about that it was an easy hack with this much sensitive data.
[01:05:53.240 --> 01:05:55.240]   That is just horrifying.
[01:05:55.240 --> 01:05:57.240]   Well, I mean, I think it's un-unsuitable.
[01:05:57.240 --> 01:06:00.240]   We've seen all the steps that have happened in the way that they've handled this.
[01:06:00.240 --> 01:06:04.240]   That it's just clear that Equifax did not take any of this seriously, including the
[01:06:04.240 --> 01:06:05.240]   announcement itself.
[01:06:05.240 --> 01:06:09.240]   They came out with a website that was badly constructed.
[01:06:09.240 --> 01:06:13.240]   The latest news just this week, folks who went to that website and entered the information
[01:06:13.240 --> 01:06:19.240]   to see if they were exposed by the hack and were told come back and will contact you about
[01:06:19.240 --> 01:06:21.240]   your offer offer of a product.
[01:06:21.240 --> 01:06:25.240]   They're just now getting the first set of emails and those emails are coming from a brand new
[01:06:25.240 --> 01:06:27.240]   domain that was just registered a couple of weeks ago.
[01:06:27.240 --> 01:06:31.240]   So a lot of web browsers are flagging them as phishing emails.
[01:06:31.240 --> 01:06:32.240]   Of course they will.
[01:06:32.240 --> 01:06:33.240]   Okay.
[01:06:33.240 --> 01:06:34.240]   They look fake.
[01:06:34.240 --> 01:06:38.240]   So, and there's been one incident like this after another since this attack.
[01:06:38.240 --> 01:06:46.340]   I was completely flummoxed by this because I went to go look at what URL they were directing
[01:06:46.340 --> 01:06:47.740]   people to.
[01:06:47.740 --> 01:06:54.840]   So Equifax said, created this correct URL, Equifaxsecurity2017.com.
[01:06:54.840 --> 01:07:02.800]   And my first thought was, why would you not have Equifax.com/security2017?
[01:07:02.800 --> 01:07:06.800]   Or something that I could verify was the Equifax domain.
[01:07:06.800 --> 01:07:14.200]   I mean, and then someone registered an inversion of that URL, the opposite, instead of Equifax
[01:07:14.200 --> 01:07:15.200]  security blah, blah, blah.
[01:07:15.200 --> 01:07:19.040]   They registered Security Equifax2017.
[01:07:19.040 --> 01:07:21.480]   And whoops.
[01:07:21.480 --> 01:07:26.360]   Equifax's own Twitter account was directing people to the wrong URL.
[01:07:26.360 --> 01:07:28.720]   Even they confused themselves.
[01:07:28.720 --> 01:07:33.720]   If the stakes weren't so high, it would become a call.
[01:07:33.720 --> 01:07:34.720]   Yeah.
[01:07:34.720 --> 01:07:38.960]   It's just, again, it's a signal of a company that this would be the equivalent of cars
[01:07:38.960 --> 01:07:41.000]   rolling off of a assembly line.
[01:07:41.000 --> 01:07:46.080]   And before major incidents occur, you know, windows falling out and tires falling off.
[01:07:46.080 --> 01:07:48.080]   I mean, it's just obvious they weren't taking it seriously.
[01:07:48.080 --> 01:07:50.360]   They didn't train their employees well enough.
[01:07:50.360 --> 01:07:53.560]   People were calling who hadn't even, apparently the only people in the world who didn't know
[01:07:53.560 --> 01:07:56.720]   about this leak were Equifax customer service representatives.
[01:07:56.720 --> 01:08:01.600]   People were calling and getting the verbal equivalent of blank stares when they called.
[01:08:01.600 --> 01:08:05.160]   But this is the most important point to me because, of course, what's happened since
[01:08:05.160 --> 01:08:08.120]   is this absolute flurry manhouse of activity.
[01:08:08.120 --> 01:08:12.280]   A life lock set had 100,000 new signups within a couple of days.
[01:08:12.280 --> 01:08:16.800]   All sorts of people flooding the security fees freeze websites of the three credit
[01:08:16.800 --> 01:08:18.720]   bureaus and security freezes are great.
[01:08:18.720 --> 01:08:23.780]   But my recommendation has been to just wait for a minute because we know the data was
[01:08:23.780 --> 01:08:25.280]   stolen several months ago.
[01:08:25.280 --> 01:08:28.960]   So there's no urgency to do this yesterday, today or tomorrow.
[01:08:28.960 --> 01:08:30.960]   We can wait until we find out more about what's happened.
[01:08:30.960 --> 01:08:36.120]   And again, whether it was a prank or whether it was a nation state, whether it was a professional
[01:08:36.120 --> 01:08:41.800]   set of identity thieves, those, the truth behind that will affect the sensible choices
[01:08:41.800 --> 01:08:42.800]   that you make.
[01:08:42.800 --> 01:08:44.480]   But I want people to really think about this.
[01:08:44.480 --> 01:08:49.280]   We already know last year was there were more identity theft victims than ever.
[01:08:49.280 --> 01:08:52.760]   And people are kind of bored of this story because it's been going on for so long.
[01:08:52.760 --> 01:08:56.960]   But something like 15 million Americans had their identities stolen last year.
[01:08:56.960 --> 01:09:04.040]   Why that matters is how much has a typical consumer's odds gone up for being a likelihood
[01:09:04.040 --> 01:09:06.920]   of being a victim because of this Equifax incident.
[01:09:06.920 --> 01:09:10.720]   And I would argue one or two percent maybe.
[01:09:10.720 --> 01:09:14.880]   So in reality, the world is not a lot more dangerous today than it was two weeks before
[01:09:14.880 --> 01:09:16.000]   we knew about this incident.
[01:09:16.000 --> 01:09:17.640]   So I don't want people to overreact to it.
[01:09:17.640 --> 01:09:20.360]   I certainly don't want them paying for products that they don't need.
[01:09:20.360 --> 01:09:23.800]   I do believe that the end result of all of this has already been a couple of bills in
[01:09:23.800 --> 01:09:28.520]   Congress urging this will be that all these companies have to allow us to freeze our credit
[01:09:28.520 --> 01:09:30.120]   reports for free.
[01:09:30.120 --> 01:09:34.320]   And I think that and they'll also be forced to have a simple procedure for doing that,
[01:09:34.320 --> 01:09:37.200]   a simple procedure for thawing our credit reports.
[01:09:37.200 --> 01:09:41.800]   And so you can afford to weigh a little bit and see how that shakes out before you overreact.
[01:09:41.800 --> 01:09:46.000]   That seems so smart to me when I heard you say just take a minute.
[01:09:46.000 --> 01:09:49.960]   Consumer reports came out with their advice this week and they took a while to come out
[01:09:49.960 --> 01:09:53.280]   with some concrete advice.
[01:09:53.280 --> 01:10:02.680]   And they kind of did the you can do a freeze if you want, but they didn't advise doing
[01:10:02.680 --> 01:10:03.680]   it.
[01:10:03.680 --> 01:10:05.880]   They really just said you can if you want.
[01:10:05.880 --> 01:10:08.680]   I have a very practical question for you, though, Bob.
[01:10:08.680 --> 01:10:09.680]   My son is here.
[01:10:09.680 --> 01:10:10.680]   Finn, come here.
[01:10:10.680 --> 01:10:14.240]   He's nine.
[01:10:14.240 --> 01:10:17.240]   And I haven't told you this because it's kind of complicated.
[01:10:17.240 --> 01:10:23.640]   But I went to the site and I put in Finn has a sister who says twin.
[01:10:23.640 --> 01:10:27.200]   They have sequential.
[01:10:27.200 --> 01:10:28.200]   So security numbers.
[01:10:28.200 --> 01:10:29.200]   Oh gosh.
[01:10:29.200 --> 01:10:35.000]   I put them both in and one has apparently been compromised and the other one has it.
[01:10:35.000 --> 01:10:41.080]   And it just makes no sense to me that they would have sequential so security numbers
[01:10:41.080 --> 01:10:43.840]   and one would potentially be hacked and one wouldn't.
[01:10:43.840 --> 01:10:50.080]   How much can we trust the Equifax site that says did you know that?
[01:10:50.080 --> 01:10:51.960]   No.
[01:10:51.960 --> 01:10:52.960]   Don't worry.
[01:10:52.960 --> 01:10:54.120]   I check your credit all the time.
[01:10:54.120 --> 01:10:56.880]   I've been doing that since like you were six months old.
[01:10:56.880 --> 01:10:57.880]   So don't worry.
[01:10:57.880 --> 01:10:58.880]   I got you covered, buddy.
[01:10:58.880 --> 01:10:59.880]   Okay.
[01:10:59.880 --> 01:11:00.880]   You haven't taken out.
[01:11:00.880 --> 01:11:01.880]   You haven't opened any credit cards, have you?
[01:11:01.880 --> 01:11:02.880]   No.
[01:11:02.880 --> 01:11:03.880]   Okay.
[01:11:03.880 --> 01:11:04.880]   Good.
[01:11:04.880 --> 01:11:05.880]   All right.
[01:11:05.880 --> 01:11:06.880]   Well, I'm just saying I want to make sure I know that you haven't because you know if
[01:11:06.880 --> 01:11:10.160]   credit card ever was opened in your name in the next year, I would know.
[01:11:10.160 --> 01:11:12.080]   Oh, maybe it was from the Equifax hack.
[01:11:12.080 --> 01:11:13.080]   So I don't know.
[01:11:13.080 --> 01:11:16.200]   Should one of my children be worried?
[01:11:16.200 --> 01:11:21.200]   What is going on with their site that supposedly says some are compromised and some aren't?
[01:11:21.200 --> 01:11:25.320]   So I haven't gotten an answer and myself and a bunch of other journalists have been asking
[01:11:25.320 --> 01:11:27.640]   this question since the very beginning.
[01:11:27.640 --> 01:11:31.400]   So I can only presume by reading tea leaves from the outside, but I've heard so many stories
[01:11:31.400 --> 01:11:37.000]   just like the one you just shared that tell me that there was no live data behind that
[01:11:37.000 --> 01:11:39.640]   website that Equifax was sending people to.
[01:11:39.640 --> 01:11:44.400]   I mean, I don't know that this is my speculation because so many people either got a vague answer
[01:11:44.400 --> 01:11:48.720]   or got, you know, I've heard people get who receive different answers on different dates.
[01:11:48.720 --> 01:11:53.640]   I mean, I think that might have been, you know, essentially busy work that they gave
[01:11:53.640 --> 01:11:58.320]   people so that they could have give them something to do to try to reassure them and
[01:11:58.320 --> 01:12:02.280]   get them in the process of signing up for this product they want people to have that's
[01:12:02.280 --> 01:12:04.040]   going to be their free gift.
[01:12:04.040 --> 01:12:07.520]   But I am just convinced that they didn't necessarily have real data behind that.
[01:12:07.520 --> 01:12:09.480]   So I would not believe what they told you.
[01:12:09.480 --> 01:12:14.160]   And if somebody who went to that site was told, your data was not impacted, I wouldn't
[01:12:14.160 --> 01:12:17.080]   believe it.
[01:12:17.080 --> 01:12:24.680]   So now one of the big developments this week is that the stock sales by the corporate officers
[01:12:24.680 --> 01:12:30.760]   are being evaluated because apparently based on some of the data that we're seeing now,
[01:12:30.760 --> 01:12:40.560]   many of them sold stock before the hack was revealed and the DOJ is now said to be engaging
[01:12:40.560 --> 01:12:46.200]   in a criminal probe that that was insider trading or violation of the SEC rules.
[01:12:46.200 --> 01:12:52.360]   I also read somewhere that the head of security for Equifax made $2.3 million a year, which
[01:12:52.360 --> 01:12:53.800]   is again horrifying.
[01:12:53.800 --> 01:12:59.400]   That word keeps popping up and their chief information officer and their head of security
[01:12:59.400 --> 01:13:02.200]   are retiring.
[01:13:02.200 --> 01:13:05.200]   So this story just keeps rolling along.
[01:13:05.200 --> 01:13:11.200]   I think it's going to be made into some sort of a business school case of how not to handle
[01:13:11.200 --> 01:13:13.840]   a situation.
[01:13:13.840 --> 01:13:15.680]   It just keeps going sideways.
[01:13:15.680 --> 01:13:20.040]   Well, I mean, I'll tell you that I've been covering the credit bureaus for a long time
[01:13:20.040 --> 01:13:22.480]   and you won't show you know this as well.
[01:13:22.480 --> 01:13:23.480]   They're generally unpopular.
[01:13:23.480 --> 01:13:27.480]   Well, we didn't choose to participate.
[01:13:27.480 --> 01:13:29.560]   This is not something that you opt into.
[01:13:29.560 --> 01:13:31.240]   Yeah, and they have this history.
[01:13:31.240 --> 01:13:34.880]   I mean, if you could just go down the rap sheet for all of the bureaus at the Federal Trade
[01:13:34.880 --> 01:13:38.680]   Commission, you know, consent order after consent order for decades.
[01:13:38.680 --> 01:13:43.720]   So that's in the DNA of these companies is how do we sort of walk along the edge of
[01:13:43.720 --> 01:13:46.520]   the law and once in a while we walk along the wrong side of it.
[01:13:46.520 --> 01:13:51.560]   But I do think that the most important thing about this hack is what you just said.
[01:13:51.560 --> 01:13:56.160]   I'm old enough that I wrote about the very first choice point hack when the first time
[01:13:56.160 --> 01:14:00.160]   that Americans discovered that there were even companies like Choice Point that had data
[01:14:00.160 --> 01:14:01.160]   backgrounds on you.
[01:14:01.160 --> 01:14:04.600]   That story was on the front page of every newspaper in the country for a week just like
[01:14:04.600 --> 01:14:05.600]   this one.
[01:14:05.600 --> 01:14:09.120]   But the real story was who was Choice Point?
[01:14:09.120 --> 01:14:10.600]   Why do they have data about me?
[01:14:10.600 --> 01:14:11.840]   I never heard of them.
[01:14:11.840 --> 01:14:14.800]   But yet they know what block I live in, what kind of food I eat.
[01:14:14.800 --> 01:14:16.320]   And that's really what this is about.
[01:14:16.320 --> 01:14:20.560]   I mean, people have heard of Equifax before, but in the end, everybody is stopping and
[01:14:20.560 --> 01:14:22.320]   saying I never asked for this.
[01:14:22.320 --> 01:14:26.080]   How could you put me at such great risk when I have no business relationship with you?
[01:14:26.080 --> 01:14:27.080]   What so ever?
[01:14:27.080 --> 01:14:30.800]   I mean, lots of people like me were running around correcting others in the first day
[01:14:30.800 --> 01:14:37.080]   that this happened when people were saying 143 million customers were exposed by this
[01:14:37.080 --> 01:14:38.080]   attack.
[01:14:38.080 --> 01:14:39.080]   And that's not true.
[01:14:39.080 --> 01:14:41.680]   Equifax only has a few customers, banks and lenders.
[01:14:41.680 --> 01:14:42.680]   We're not their customers.
[01:14:42.680 --> 01:14:43.880]   We're their product.
[01:14:43.880 --> 01:14:46.760]   And we didn't ask for this and now they put us at risk and that's really where the anger
[01:14:46.760 --> 01:14:48.000]   comes from.
[01:14:48.000 --> 01:14:51.640]   The chat room makes such a great point that John Oliver needs to cover this.
[01:14:51.640 --> 01:14:54.800]   This is a perfect target for his diatribes.
[01:14:54.800 --> 01:14:58.200]   I want to move on in security and talk about Seacleaner for Windows.
[01:14:58.200 --> 01:14:59.200]   You've heard of it.
[01:14:59.200 --> 01:15:01.240]   It's a popular security app.
[01:15:01.240 --> 01:15:05.760]   It's supposed to clean temporary internet files, getting rid of malicious programs.
[01:15:05.760 --> 01:15:11.040]   Now Avast, which is a big, well-known security company, hosts, distributes the software.
[01:15:11.040 --> 01:15:14.800]   But this week it was found to be tainted with a backdoor.
[01:15:14.800 --> 01:15:20.160]   And it seems like they've served it to millions of its users.
[01:15:20.160 --> 01:15:24.240]   And this was all between August 15th and September 12th.
[01:15:24.240 --> 01:15:28.120]   Cisco Talos researchers found the activity.
[01:15:28.120 --> 01:15:33.920]   And what's really come to light at the end of this week is that even though there were
[01:15:33.920 --> 01:15:41.040]   700,000 some infected PCs, 20 of them belong to really connected companies that seem to
[01:15:41.040 --> 01:15:46.400]   be the target here or at least the phishing expedition.
[01:15:46.400 --> 01:15:52.720]   We're talking about Cisco, Microsoft, Gmail, VMware, Akamai, Sony, Samsung.
[01:15:52.720 --> 01:15:57.120]   And I think it leads to that underlying issue when we're talking about these big tech companies,
[01:15:57.120 --> 01:15:58.960]   when we're talking about security.
[01:15:58.960 --> 01:16:00.160]   Who can you trust?
[01:16:00.160 --> 01:16:05.400]   This is not like a random download from some, you know, Joe's download site.
[01:16:05.400 --> 01:16:10.480]   These are from the companies themselves with embedded malware in it.
[01:16:10.480 --> 01:16:12.840]   Does that shake you a little bit, Robert?
[01:16:12.840 --> 01:16:18.120]   Yeah, the truth is none of us or very few of us are very good at security.
[01:16:18.120 --> 01:16:21.480]   I was out of VC dinner that are night talking about this.
[01:16:21.480 --> 01:16:25.960]   And I was like, come on, be honest, how many of you use two-factor authentication on your
[01:16:25.960 --> 01:16:26.960]   iPhone?
[01:16:26.960 --> 01:16:29.040]   And like one out of four was using it.
[01:16:29.040 --> 01:16:32.720]   And these are people who are rich, have something to protect.
[01:16:32.720 --> 01:16:33.720]   Targets.
[01:16:33.720 --> 01:16:41.320]   Who are in the tech industry and should know better, but don't practice the best security
[01:16:41.320 --> 01:16:43.160]   of their, on their person.
[01:16:43.160 --> 01:16:45.920]   And that's how these things get in, right?
[01:16:45.920 --> 01:16:51.000]   Somebody clicks on an email at a company and all of a sudden it's spreading throughout
[01:16:51.000 --> 01:16:57.960]   the network because somebody didn't follow protocol and probably doesn't even know the
[01:16:57.960 --> 01:17:00.840]   proper protocol to be honest about it.
[01:17:00.840 --> 01:17:05.560]   When I worked at Rackspace, we saw this all the time and we'd have to go through and change
[01:17:05.560 --> 01:17:11.640]   lots of servers and do lots of things because somebody would get in through an employee
[01:17:11.640 --> 01:17:18.120]   clicking on a link and all of a sudden there's bad stuff going around, right?
[01:17:18.120 --> 01:17:21.360]   Security is hard because people are undisciplined.
[01:17:21.360 --> 01:17:27.440]   Organizations aren't incentivized by security and therefore it's the wetware that's the
[01:17:27.440 --> 01:17:28.440]   problem.
[01:17:28.440 --> 01:17:30.640]   Yeah, it's hard.
[01:17:30.640 --> 01:17:33.000]   It's a pain in the butt doing two-factor authentication.
[01:17:33.000 --> 01:17:34.560]   I'm setting up a new iPad.
[01:17:34.560 --> 01:17:35.560]   It's a pain.
[01:17:35.560 --> 01:17:37.800]   I know it's a pain.
[01:17:37.800 --> 01:17:40.120]   But that's the table stakes now.
[01:17:40.120 --> 01:17:46.840]   If you don't protect your stuff, you're going to be the guy or the girl who clicks on something
[01:17:46.840 --> 01:17:50.720]   and gets it access to your corporate network.
[01:17:50.720 --> 01:17:57.880]   Okay, so market forces, you are incentivized to practice good security because it would
[01:17:57.880 --> 01:18:01.880]   be disastrous for your brand if you lost access to your security?
[01:18:01.880 --> 01:18:06.240]   Is the person at Equifax who used admin and admin as a password?
[01:18:06.240 --> 01:18:13.200]   Is that person really going to pay anything to the damage that he or she has cost?
[01:18:13.200 --> 01:18:14.200]   Yeah.
[01:18:14.200 --> 01:18:18.520]   And you know, market forces didn't get a seat belt either.
[01:18:18.520 --> 01:18:20.600]   So it has to be more than that.
[01:18:20.600 --> 01:18:24.520]   We keep making products that are too easy for people to break.
[01:18:24.520 --> 01:18:25.880]   Human nature is what it is.
[01:18:25.880 --> 01:18:27.600]   That's a known quantity.
[01:18:27.600 --> 01:18:30.640]   I saw somebody in the chat room say this a moment ago and I agree.
[01:18:30.640 --> 01:18:32.560]   I wish I was picked up on the NIC.
[01:18:32.560 --> 01:18:37.080]   But part of the problem here is that cleaning the registry is hard.
[01:18:37.080 --> 01:18:39.160]   And Microsoft invented registries.
[01:18:39.160 --> 01:18:42.000]   Microsoft could do a better job of making them clean themselves.
[01:18:42.000 --> 01:18:46.360]   And a product like CC Cleaner could have been coming from Redmond and we theoretically wouldn't
[01:18:46.360 --> 01:18:47.360]   have had this problem.
[01:18:47.360 --> 01:18:48.360]   We've seen that cycle.
[01:18:48.360 --> 01:18:51.720]   Microsoft has delivered software with viruses in it too.
[01:18:51.720 --> 01:18:52.720]   So.
[01:18:52.720 --> 01:18:53.720]   Well, absolutely.
[01:18:53.720 --> 01:18:54.720]   Yeah.
[01:18:54.720 --> 01:18:56.800]   Even I worked at Microsoft.
[01:18:56.800 --> 01:19:04.080]   And they had to spend many hundreds of millions of dollars upgrading their employees and their
[01:19:04.080 --> 01:19:08.120]   systems to be more secure and get rid of the virus problems that were going around in the
[01:19:08.120 --> 01:19:09.120]   90s.
[01:19:09.120 --> 01:19:12.040]   I'm not saying to blame it on the fish employee.
[01:19:12.040 --> 01:19:13.640]   The chat rooms are here with me.
[01:19:13.640 --> 01:19:19.120]   I'm saying it's a deep problem that all of us are at blame for.
[01:19:19.120 --> 01:19:24.120]   And very few of us practice good security practices.
[01:19:24.120 --> 01:19:28.440]   How many of you go to Google and search on how to make a good password?
[01:19:28.440 --> 01:19:31.200]   How many of us use two-factor authentication?
[01:19:31.200 --> 01:19:36.480]   How many of us use a separate password for each service we're registering on?
[01:19:36.480 --> 01:19:38.320]   How many of us really think about security?
[01:19:38.320 --> 01:19:39.480]   I can tell you.
[01:19:39.480 --> 01:19:41.320]   Very few of us do.
[01:19:41.320 --> 01:19:45.400]   I do because I live in fear of my life being hacked, right?
[01:19:45.400 --> 01:19:47.240]   But very few people do.
[01:19:47.240 --> 01:19:51.840]   And that's in the tech industry amongst people who should know better.
[01:19:51.840 --> 01:19:56.600]   I've got a question of personal relevance for Bob, but I think it's going to interest
[01:19:56.600 --> 01:19:58.440]   a lot of viewers as well.
[01:19:58.440 --> 01:20:03.440]   So Bob, you had suggested that we not get carried away and freeze our...
[01:20:03.440 --> 01:20:06.360]   It's called freezing our credit, right?
[01:20:06.360 --> 01:20:08.480]   That's the main option that people are suggesting, yeah.
[01:20:08.480 --> 01:20:09.480]   Yeah.
[01:20:09.480 --> 01:20:12.440]   Well, I am one of those who did get carried away and froze my credit two nights ago.
[01:20:12.440 --> 01:20:18.600]   And it was actually a last straw thing because I've probably had four or five periods in
[01:20:18.600 --> 01:20:24.520]   the last 10 or 15 years where somebody has given me one free year of experience or
[01:20:24.520 --> 01:20:26.160]   Equifax.
[01:20:26.160 --> 01:20:27.840]   We gave away your social security number.
[01:20:27.840 --> 01:20:31.960]   So for 12 months, we're going to give you this product that when the 12 months are over
[01:20:31.960 --> 01:20:36.480]   is going to start costing you $15.99 if you're not really careful.
[01:20:36.480 --> 01:20:40.880]   And after having all these free years that have rolled into this expensive...
[01:20:40.880 --> 01:20:45.720]   The very idea of Equifax giving me a free product that I would then get addicted to and
[01:20:45.720 --> 01:20:50.480]   have to pay whatever for the rest of my life because they gave somebody my social security
[01:20:50.480 --> 01:20:53.000]   number was a little bit too much.
[01:20:53.000 --> 01:20:56.480]   So I decided to freeze my credit.
[01:20:56.480 --> 01:20:59.000]   My wife and I, we both have plenty of credit cards.
[01:20:59.000 --> 01:21:02.280]   We're not anticipating taking out any loans.
[01:21:02.280 --> 01:21:06.080]   So my first question to you is why not do that?
[01:21:06.080 --> 01:21:07.080]   If one is...
[01:21:07.080 --> 01:21:09.840]   I mean, not everybody's in that situation.
[01:21:09.840 --> 01:21:14.440]   We were doing it with some dread because we're like, "Oh my God, unfreezing it is probably...
[01:21:14.440 --> 01:21:18.440]   We're going to get punished by Equifax because you make our lives a hell living hell when
[01:21:18.440 --> 01:21:20.040]   we actually need to unfreeze it.
[01:21:20.040 --> 01:21:24.000]   Why are you suggesting that people keep their credit unfrozen?
[01:21:24.000 --> 01:21:28.960]   Because it seems if it is frozen and please tell me if this is false security, it becomes
[01:21:28.960 --> 01:21:29.960]   almost impossible.
[01:21:29.960 --> 01:21:33.080]   I mean, somebody could steal your identity but they can't do anything with it because
[01:21:33.080 --> 01:21:36.760]   they can't take out any credit or is that false security?
[01:21:36.760 --> 01:21:38.240]   No, you're right.
[01:21:38.240 --> 01:21:41.320]   And I, by the way, I'm in favor of security freezes.
[01:21:41.320 --> 01:21:44.960]   I've spent the last several years telling people that they're a good idea, especially
[01:21:44.960 --> 01:21:46.880]   someone who's in your spot.
[01:21:46.880 --> 01:21:52.440]   But as a somebody who sometimes has to give advice to millions of panicked people, my
[01:21:52.440 --> 01:21:56.640]   first piece of advice in this moment was don't do anything because you might do the wrong
[01:21:56.640 --> 01:21:57.960]   thing.
[01:21:57.960 --> 01:22:02.200]   The best example of that, I did a story about this, for several days last week, when you
[01:22:02.200 --> 01:22:08.240]   search TransUnion's website for security freeze, what you got was a one page saying their security
[01:22:08.240 --> 01:22:13.200]   freeze freeze website was broken and then a redirect to their identity theft product,
[01:22:13.200 --> 01:22:14.200]   which was...
[01:22:14.200 --> 01:22:15.200]   Didn't you get something with that?
[01:22:15.200 --> 01:22:16.200]   Oh, yeah, we can pull that.
[01:22:16.200 --> 01:22:17.200]   Yeah.
[01:22:17.200 --> 01:22:18.200]   But what I hear about...
[01:22:18.200 --> 01:22:22.040]   Well, actually, I can show you something of great, fabulous timing.
[01:22:22.040 --> 01:22:24.960]   I think it might be available, the screenshot.
[01:22:24.960 --> 01:22:30.040]   Okay, so this is what happened when I was trying to do my credit freeze on Equifax.
[01:22:30.040 --> 01:22:33.840]   I went through the whole process and I got this error message as a screenshot from my
[01:22:33.840 --> 01:22:34.840]   Mac.
[01:22:34.840 --> 01:22:38.840]   I said, "Sorry, we can't process your online request.
[01:22:38.840 --> 01:22:45.160]   To assist us in processing your request, please submit in writing the following things."
[01:22:45.160 --> 01:22:50.240]   And they listened in writing, so as to mail to them a bunch of stuff, including my social
[01:22:50.240 --> 01:22:55.120]   security number on a piece of paper that I mailed to them.
[01:22:55.120 --> 01:22:57.480]   And this is...
[01:22:57.480 --> 01:23:03.520]   I was so staggered by the audacity of this incompetence to basically say, "You got to
[01:23:03.520 --> 01:23:04.520]   mail us something.
[01:23:04.520 --> 01:23:08.400]   Here's our address and scribble your social security number."
[01:23:08.400 --> 01:23:09.400]   So I called them.
[01:23:09.400 --> 01:23:13.880]   It was a few minutes before 1 a.m. when their customer service was shut down.
[01:23:13.880 --> 01:23:17.080]   So I called them, now, to their credit, I got through to customer service very quickly
[01:23:17.080 --> 01:23:18.960]   and I explained what happened.
[01:23:18.960 --> 01:23:22.320]   And the woman I talked to said, "Oh, yeah, there's a problem with the site.
[01:23:22.320 --> 01:23:23.760]   Go to this other site."
[01:23:23.760 --> 01:23:29.840]   And she gives me a URL and it's basically an FTC site that has articles about identity
[01:23:29.840 --> 01:23:30.840]   theft.
[01:23:30.840 --> 01:23:31.840]   You can do it from that site.
[01:23:31.840 --> 01:23:33.320]   Go to this FTC site.
[01:23:33.320 --> 01:23:36.120]   You can do it there or mail this stuff in.
[01:23:36.120 --> 01:23:42.840]   Now, thank God, the next day I was like, "Okay, now the kid gloves are coming off."
[01:23:42.840 --> 01:23:47.080]   And actually, after a couple more attempts, I was able to finally get it done online.
[01:23:47.080 --> 01:23:52.040]   But the fact that I get this thing saying, "Mail us your social security number."
[01:23:52.040 --> 01:23:53.040]   Oh my God.
[01:23:53.040 --> 01:23:57.600]   And TransUnion was broken that night as well, but I got through with them the following
[01:23:57.600 --> 01:23:58.840]   day too.
[01:23:58.840 --> 01:24:01.480]   But Experian, boom, three minutes, I was frozen.
[01:24:01.480 --> 01:24:07.480]   Well, this is the kind of consumer horror that creates an outcry for different policies.
[01:24:07.480 --> 01:24:11.960]   I mean, aspiring in the chat room is saying, "Why isn't there a policy that these companies
[01:24:11.960 --> 01:24:14.480]   have security audits?
[01:24:14.480 --> 01:24:16.360]   Do you think that's where this is going?
[01:24:16.360 --> 01:24:20.280]   What is the outcome of this kind of a massive screw up?
[01:24:20.280 --> 01:24:21.280]   Yeah.
[01:24:21.280 --> 01:24:22.600]   And I'll jump in here.
[01:24:22.600 --> 01:24:28.200]   The Fortune 100 have security systems that let them walk around their networks and see
[01:24:28.200 --> 01:24:30.680]   anomalies in deep detail.
[01:24:30.680 --> 01:24:36.200]   And there is security software from Israel and other places to help secure these kinds of
[01:24:36.200 --> 01:24:39.760]   things, but it costs a lot of money and it needs to be used.
[01:24:39.760 --> 01:24:44.320]   And it needs to be used by somebody who really knows what they're doing, not a music history
[01:24:44.320 --> 01:24:45.320]   major.
[01:24:45.320 --> 01:24:46.320]   Right.
[01:24:46.320 --> 01:24:50.480]   But also, you're not going to figure out a fishing risk with a pen test, you know?
[01:24:50.480 --> 01:24:53.120]   And so that's where this gets so messy and ugly.
[01:24:53.120 --> 01:24:54.960]   But what do you think the outcome is, Bob?
[01:24:54.960 --> 01:24:55.960]   Sorry about that.
[01:24:55.960 --> 01:24:58.800]   Well, no, that's a really great point.
[01:24:58.800 --> 01:25:03.520]   And once again, we have to remember that the victims here are not consumers of the company.
[01:25:03.520 --> 01:25:08.400]   So the incentive that these bureaus have to take care of our data is, you know, on the
[01:25:08.400 --> 01:25:15.040]   list of priorities for the meeting quarterly estimates, this is going to be number 99 of
[01:25:15.040 --> 01:25:16.040]   100.
[01:25:16.040 --> 01:25:17.560]   And that's why we need rules.
[01:25:17.560 --> 01:25:22.200]   But the Fair Credit Reporting Act and all of its subsequent editions, like the Fact Act
[01:25:22.200 --> 01:25:27.840]   in 2003, there are hundreds of pages of regulations where, I mean, it's incredibly specific.
[01:25:27.840 --> 01:25:31.840]   I went back and found the first credit freeze law, which was passed by the state of California
[01:25:31.840 --> 01:25:32.840]   in 2002.
[01:25:32.840 --> 01:25:37.720]   And just as a plug for state legislators, we wouldn't have security freezes if it weren't
[01:25:37.720 --> 01:25:41.040]   for state legislatures that passed individual state laws.
[01:25:41.040 --> 01:25:44.680]   That's why in every single state of the union, there are slightly different rules for how
[01:25:44.680 --> 01:25:46.120]   we get a security freeze.
[01:25:46.120 --> 01:25:48.680]   But it even specifies how they have to do it.
[01:25:48.680 --> 01:25:51.680]   There has to be a 10-digit pin involved, incredibly specific.
[01:25:51.680 --> 01:25:55.240]   And that's the only way that you can get these companies to do things is to be that
[01:25:55.240 --> 01:25:56.240]   specific.
[01:25:56.240 --> 01:26:01.280]   It's very hard for Washington, D.C., or any state house to pass legislation that's going
[01:26:01.280 --> 01:26:06.240]   to be specific about security at a company like Equifax, to be specific about saying
[01:26:06.240 --> 01:26:11.320]   you have to use, say, a content inspection tool, which would have caught 143 million
[01:26:11.320 --> 01:26:15.680]   Social Security numbers flying by even like the basic off-the-shelf software should have
[01:26:15.680 --> 01:26:17.320]   caught that.
[01:26:17.320 --> 01:26:20.080]   But they didn't use it and they wouldn't invest in it because it's just a mere cost center
[01:26:20.080 --> 01:26:21.080]   to them.
[01:26:21.080 --> 01:26:24.400]   And so we need rules, but it's very, very hard to pass legislation that's effective that
[01:26:24.400 --> 01:26:25.400]   way.
[01:26:25.400 --> 01:26:30.920]   And Bob, when I can't take it anymore and I just have to get another frequent flyer credit
[01:26:30.920 --> 01:26:33.480]   card, what hoops am I going to have?
[01:26:33.480 --> 01:26:38.160]   Having frozen my credit now, what hoops am I going to have to jump through to unfreeze
[01:26:38.160 --> 01:26:39.160]   it?
[01:26:39.160 --> 01:26:41.120]   I did save the 10-digit pins.
[01:26:41.120 --> 01:26:43.200]   I have them all very carefully squared away.
[01:26:43.200 --> 01:26:44.200]   I won't say where.
[01:26:44.200 --> 01:26:45.880]   But I got all the pins.
[01:26:45.880 --> 01:26:47.160]   How hard is it to unfreeze?
[01:26:47.160 --> 01:26:50.160]   And I'm sure a lot of viewers are going to be interested in this as well because it's
[01:26:50.160 --> 01:26:52.760]   a consequence of freezing some time before thought.
[01:26:52.760 --> 01:26:53.760]   Yeah.
[01:26:53.760 --> 01:26:56.200]   There's a simple five-step process.
[01:26:56.200 --> 01:26:59.840]   And when you have all those things in place, it'll be fine.
[01:26:59.840 --> 01:27:01.680]   You'll be able to do it quickly.
[01:27:01.680 --> 01:27:04.200]   However, there are so many variables that can go wrong.
[01:27:04.200 --> 01:27:07.560]   And I know you just told me you put that 10-digit pin in a careful place.
[01:27:07.560 --> 01:27:11.720]   However, four years might go by and you'll have completely forgotten where that is.
[01:27:11.720 --> 01:27:12.720]   It's very, very hard.
[01:27:12.720 --> 01:27:16.920]   I hear from consumers all the time who can't find that pin.
[01:27:16.920 --> 01:27:21.560]   And then the good news is it's then difficult to get into your account and we want that,
[01:27:21.560 --> 01:27:22.560]   right?
[01:27:22.560 --> 01:27:26.120]   You know, Brian Krebs, the great security writer, he published a story just a couple
[01:27:26.120 --> 01:27:31.600]   of days ago concerned that some of the credit bureaus actually issuing people these pins
[01:27:31.600 --> 01:27:34.360]   without asking difficult questions.
[01:27:34.360 --> 01:27:39.520]   So this is going to be the next step in the cat and mouse game that we see.
[01:27:39.520 --> 01:27:42.640]   For the most part, everything I know about security freezes are, they work.
[01:27:42.640 --> 01:27:47.080]   I do not know of a single person who has complained to me that I had a security freeze on my
[01:27:47.080 --> 01:27:49.480]   report and my identity was stolen anyway.
[01:27:49.480 --> 01:27:50.480]   So they work.
[01:27:50.480 --> 01:27:52.840]   However, very few people have used them.
[01:27:52.840 --> 01:27:56.560]   And now that a lot of people are standing up for them, pin hacking is going to become
[01:27:56.560 --> 01:27:57.560]   a thing.
[01:27:57.560 --> 01:27:58.560]   Of course it is.
[01:27:58.560 --> 01:28:00.160]   So it'll just be another step.
[01:28:00.160 --> 01:28:02.080]   Identity thieves have to go through to commit crime.
[01:28:02.080 --> 01:28:05.440]   And now you're part of this larger, more homogeneous group and this is the process.
[01:28:05.440 --> 01:28:07.600]   So people need to watch out for that.
[01:28:07.600 --> 01:28:14.000]   Lest we think that these problems only happen to consumers, to large corporations.
[01:28:14.000 --> 01:28:18.320]   Oh no, the United States government also had a horrendous breach this week.
[01:28:18.320 --> 01:28:24.480]   It was disclosed that the Securities and Exchange Commission's Edgar database was hacked.
[01:28:24.480 --> 01:28:30.520]   This is where you'll find publicly accessible documents that tell you about the filings
[01:28:30.520 --> 01:28:34.160]   of any large publicly traded company.
[01:28:34.160 --> 01:28:41.400]   But it's also where companies do a couple of things that create knowledge that would
[01:28:41.400 --> 01:28:44.160]   be beneficial to someone who knew it before it was public.
[01:28:44.160 --> 01:28:50.480]   So there's a private part of the Edgar database where companies file information that's required
[01:28:50.480 --> 01:28:53.800]   by the SEC but is not required to be public.
[01:28:53.800 --> 01:28:58.960]   And it's where companies post dummy filings.
[01:28:58.960 --> 01:29:03.600]   And often they will create the draft of the filing and post it there to make sure that
[01:29:03.600 --> 01:29:06.080]   it formats correctly and looks right.
[01:29:06.080 --> 01:29:09.320]   And then they'll go live with it at the appropriate time.
[01:29:09.320 --> 01:29:15.040]   Well when the database was hacked a lot of that information was made available to traders
[01:29:15.040 --> 01:29:21.720]   and it's been disclosed that there have been some trades made that benefited those hackers.
[01:29:21.720 --> 01:29:26.240]   And you know I've been coming to security since the tech TV days and I used to think
[01:29:26.240 --> 01:29:31.920]   in a lot of ways as early and nascent viruses, hacks.
[01:29:31.920 --> 01:29:37.480]   It was graffiti but that it was going to get serious when it got financial.
[01:29:37.480 --> 01:29:41.360]   And then when ransomware hit I thought this is it.
[01:29:41.360 --> 01:29:45.000]   And now we're seeing way beyond that.
[01:29:45.000 --> 01:29:52.560]   And not only does this incentivize other hackers to go after big, big whale targets but it
[01:29:52.560 --> 01:29:59.040]   also undermines our faith in the systems that we depend on for our democracy.
[01:29:59.040 --> 01:30:04.120]   And this goes to fake news, this goes to when the markets are shaken.
[01:30:04.120 --> 01:30:08.840]   And I'm surprised that there wasn't a jitter set because it feels like this market is ready
[01:30:08.840 --> 01:30:10.480]   for some jitters anyway.
[01:30:10.480 --> 01:30:13.080]   So I was surprised that this didn't trigger anything.
[01:30:13.080 --> 01:30:17.360]   But do you think this is one more step in that erosion of faith?
[01:30:17.360 --> 01:30:22.840]   I think the personal step that I took of basically stepping out of the credit system.
[01:30:22.840 --> 01:30:28.200]   Because I, you know, that to me was pretty big and it would have been an unimaginable
[01:30:28.200 --> 01:30:33.840]   step for me to have taken, I don't know, 10 or 15 years ago but I really felt like on
[01:30:33.840 --> 01:30:39.080]   a certain level I was seceding from a certain part of the economy.
[01:30:39.080 --> 01:30:45.280]   Because I had absolutely no faith left in people whose job it was to secure us.
[01:30:45.280 --> 01:30:49.360]   And in fact, quite the opposite is getting back to a lot of the things that Bob was talking
[01:30:49.360 --> 01:30:50.360]   about.
[01:30:50.360 --> 01:30:52.240]   I'm the product, not the customer.
[01:30:52.240 --> 01:30:57.840]   And you know, their defense of me is completely not a priority.
[01:30:57.840 --> 01:31:03.160]   And yeah, that personal secession from something just knowing I can't get the JetBlue Visa
[01:31:03.160 --> 01:31:05.120]   card, which I really don't want.
[01:31:05.120 --> 01:31:06.920]   I really don't want it.
[01:31:06.920 --> 01:31:09.120]   Actually car by a house.
[01:31:09.120 --> 01:31:10.120]   Yeah.
[01:31:10.120 --> 01:31:15.960]   But taking that step of sort of quasi sequestering myself, and I didn't have, thank you Bob,
[01:31:15.960 --> 01:31:20.560]   you've put my mind at ease that when it comes time to thought, I'll be able to do it.
[01:31:20.560 --> 01:31:25.760]   But I did it without any notion that it was going to be easy to thought this thing because
[01:31:25.760 --> 01:31:28.160]   yeah, it's a last straw kind of thing.
[01:31:28.160 --> 01:31:31.480]   I'm sure every one of these things, it's like a drum beat.
[01:31:31.480 --> 01:31:34.560]   You know, when Anthem got hacked, I mean that in some ways that's even more intimate because
[01:31:34.560 --> 01:31:35.560]   that's your insurer.
[01:31:35.560 --> 01:31:37.640]   They've got your medical information.
[01:31:37.640 --> 01:31:39.320]   You're trusting them with your life.
[01:31:39.320 --> 01:31:42.760]   They handed out tens of millions of Social Security numbers.
[01:31:42.760 --> 01:31:43.920]   So yeah, I think it does.
[01:31:43.920 --> 01:31:45.760]   It has an eroding effect for sure.
[01:31:45.760 --> 01:31:47.880]   Robert, does it shake you at all?
[01:31:47.880 --> 01:31:50.760]   I mean, you're such an enthusiast of so many things.
[01:31:50.760 --> 01:31:53.240]   But when you see the downside attack, what's your thinking?
[01:31:53.240 --> 01:31:58.680]   I live in fear of having my life ripped apart by an identity thief or somebody hacking into
[01:31:58.680 --> 01:32:01.200]   my system because that's how I make my money, right?
[01:32:01.200 --> 01:32:04.760]   I'm on Facebook and Google and Twitter.
[01:32:04.760 --> 01:32:11.240]   And if they got in like they did with the wired reporter and tear my life apart, it
[01:32:11.240 --> 01:32:15.400]   would be really difficult to recover from that, right?
[01:32:15.400 --> 01:32:21.080]   Bob, the holy grail for me is if last pass gets hacked.
[01:32:21.080 --> 01:32:29.120]   And where's your thinking on what the best practices are for us as consumers?
[01:32:29.120 --> 01:32:33.680]   I mean, I hate to be dark about it, but this is what I think.
[01:32:33.680 --> 01:32:39.720]   You just have to assume it's going to happen to you and plan your recovery now, which conceptually
[01:32:39.720 --> 01:32:42.920]   comes, kind of falls in the category of backups.
[01:32:42.920 --> 01:32:46.160]   So there are lots of ways that a hacker can ruin your life and it doesn't mean draining
[01:32:46.160 --> 01:32:47.160]   your bank account.
[01:32:47.160 --> 01:32:50.120]   It could be, imagine what someone could do if they took over your social media accounts
[01:32:50.120 --> 01:32:54.680]   for a few hours when you weren't paying attention and said some of the wrong things or if you
[01:32:54.680 --> 01:32:55.680]   know.
[01:32:55.680 --> 01:32:57.960]   My entire soccer team would show up at the wrong field.
[01:32:57.960 --> 01:33:01.920]   If I would be drawn in quarter, I'm telling you, if they got my team snap password, it
[01:33:01.920 --> 01:33:04.240]   would be all over.
[01:33:04.240 --> 01:33:10.520]   That is terrible, but with some more imagination, some more terrible things could also happen.
[01:33:10.520 --> 01:33:14.320]   And there's lots of ways that this can go.
[01:33:14.320 --> 01:33:18.000]   But one of the most horrible things that can happen to someone is a ransomware attack
[01:33:18.000 --> 01:33:21.320]   or just some kind of file deletion of all your family photos.
[01:33:21.320 --> 01:33:23.920]   And I always bring this example up because it's so personal.
[01:33:23.920 --> 01:33:29.560]   Many, many people don't have a single photo of their baby that's not digital somewhere.
[01:33:29.560 --> 01:33:32.520]   And while we all say this, they don't have backups.
[01:33:32.520 --> 01:33:38.240]   And so having a backup for your most precious files is really the way to plan for this.
[01:33:38.240 --> 01:33:40.880]   So it's not about prevention, it's about recovery.
[01:33:40.880 --> 01:33:44.680]   And so use the picture metaphor and then think about your money, think about all the other
[01:33:44.680 --> 01:33:46.480]   parts of your life.
[01:33:46.480 --> 01:33:48.160]   Think like a hacker is what I usually say.
[01:33:48.160 --> 01:33:52.100]   If someone was to really mess with your life, what would they do and how could you plan
[01:33:52.100 --> 01:33:53.920]   for that and counter that and recover from that?
[01:33:53.920 --> 01:33:59.240]   By the way, if you use Google Assistant and you use Google Photos, watch this, show me
[01:33:59.240 --> 01:34:02.800]   all my photos of my children.
[01:34:02.800 --> 01:34:05.800]   And Google will pull up my photos of my children.
[01:34:05.800 --> 01:34:06.800]   There we go.
[01:34:06.800 --> 01:34:07.800]   You've got backup.
[01:34:07.800 --> 01:34:08.800]   Is that all clap?
[01:34:08.800 --> 01:34:12.280]   I upload it to Facebook, Google and Flickr.
[01:34:12.280 --> 01:34:14.320]   So I have three and app.
[01:34:14.320 --> 01:34:15.320]   That's four.
[01:34:15.320 --> 01:34:18.440]   So my photos are in four places.
[01:34:18.440 --> 01:34:19.440]   It's plan.
[01:34:19.440 --> 01:34:22.760]   They're going to work to take down my family photos.
[01:34:22.760 --> 01:34:26.600]   You'll probably those photos from my dead ends.
[01:34:26.600 --> 01:34:32.840]   We are going to take a break and come back and talk about Facebook, really algorithmically
[01:34:32.840 --> 01:34:41.320]   generated ads that can target people based on incredibly negative stereotypes and how
[01:34:41.320 --> 01:34:43.720]   Facebook and Google are handling this.
[01:34:43.720 --> 01:34:46.920]   And is this the beginning of the algorithms taking over?
[01:34:46.920 --> 01:34:49.760]   Rob Reed is going to have to talk me off the ledge here.
[01:34:49.760 --> 01:34:51.520]   So we'll take a break and be right back.
[01:34:51.520 --> 01:34:53.840]   Hey, we'll have more with Becky and the gang in just a bit.
[01:34:53.840 --> 01:34:56.840]   But I wanted to stop by and tell you about something we've been using at the house that
[01:34:56.840 --> 01:34:58.280]   we love.
[01:34:58.280 --> 01:34:59.600]   And we've been telling people about.
[01:34:59.600 --> 01:35:03.760]   In fact, many of the Twit team now uses Blue Apron.
[01:35:03.760 --> 01:35:05.040]   You know about Blue Apron, right?
[01:35:05.040 --> 01:35:06.280]   Oh, I love this.
[01:35:06.280 --> 01:35:11.120]   Blue Apron is the number one fresh ingredient and recipe delivery service in the country.
[01:35:11.120 --> 01:35:12.880]   Every week, by the way, it's not a subscription.
[01:35:12.880 --> 01:35:14.720]   So we choose, for instance, we're out of town.
[01:35:14.720 --> 01:35:18.680]   So we're not, you know, we just, we don't get it this week or the next week.
[01:35:18.680 --> 01:35:19.680]   You choose the day you get it.
[01:35:19.680 --> 01:35:20.920]   You choose the menu.
[01:35:20.920 --> 01:35:29.120]   They deliver you a box with up to three different meals for two or for family for you choose.
[01:35:29.120 --> 01:35:33.320]   All the ingredients you need fresh, delicious ingredients, never frozen, not even the meat
[01:35:33.320 --> 01:35:34.320]   or the fish.
[01:35:34.320 --> 01:35:37.080]   It's all fresh and refrigerated box.
[01:35:37.080 --> 01:35:39.280]   Precisely the right amount too.
[01:35:39.280 --> 01:35:42.160]   So if you need one celery rib, you get it.
[01:35:42.160 --> 01:35:46.160]   You don't get all stalked that you have to waste later, put in the fridge.
[01:35:46.160 --> 01:35:47.680]   You don't, you know, everything you need.
[01:35:47.680 --> 01:35:51.640]   If you need two teaspoons of soy sauce, it comes in a little cute little jar, a little
[01:35:51.640 --> 01:35:52.640]   bottle.
[01:35:52.640 --> 01:35:55.920]   That's, you don't have any extra, but that's good.
[01:35:55.920 --> 01:35:56.920]   For a couple of reasons.
[01:35:56.920 --> 01:35:57.920]   First of all, makes the recipe easy.
[01:35:57.920 --> 01:35:58.920]   You didn't use it all.
[01:35:58.920 --> 01:36:00.480]   You didn't do it right.
[01:36:00.480 --> 01:36:03.600]   I know I have to use this because it's sitting here.
[01:36:03.600 --> 01:36:06.040]   Plus there's no leftovers, no waste.
[01:36:06.040 --> 01:36:07.760]   And I have to say the food is so good.
[01:36:07.760 --> 01:36:09.480]   You probably won't have any leftovers to that either.
[01:36:09.480 --> 01:36:11.040]   It is amazing.
[01:36:11.040 --> 01:36:15.840]   Very, but, but good portions, $10 per person or less.
[01:36:15.840 --> 01:36:22.800]   There's seasonal recipes from 150 local farms and fisheries and ranchers all sustainable.
[01:36:22.800 --> 01:36:23.800]   The kind of food you would buy.
[01:36:23.800 --> 01:36:28.440]   And by the way, every time I get a blue apron box, I marvel at how perfect the produce
[01:36:28.440 --> 01:36:32.160]   and the fish and the meat are, it's perfect.
[01:36:32.160 --> 01:36:35.360]   It's like, well, I guess it is hand picked.
[01:36:35.360 --> 01:36:38.840]   And it's like, if I went to the store, I wouldn't get said it wouldn't be as good.
[01:36:38.840 --> 01:36:40.760]   They do a better job than me.
[01:36:40.760 --> 01:36:43.400]   Pick go to blue apron.com/twit.
[01:36:43.400 --> 01:36:44.400]   Look at the menu.
[01:36:44.400 --> 01:36:45.400]   See what's on the menu.
[01:36:45.400 --> 01:36:49.080]   Stuff like summer vegetable and egg paninis with Calabrian chili mayonnaise and caprésia
[01:36:49.080 --> 01:36:50.080]   salad.
[01:36:50.080 --> 01:36:51.080]   Oh, I ate these ads.
[01:36:51.080 --> 01:36:53.080]   They make me so hungry.
[01:36:53.080 --> 01:36:54.080]   So I'm sorry.
[01:36:54.080 --> 01:36:59.320]   Soi glaze, pork and rice cakes with bok choy and marinated green beans.
[01:36:59.320 --> 01:37:00.960]   These are skills too, by the way.
[01:37:00.960 --> 01:37:04.400]   These are things that you will now know how to do.
[01:37:04.400 --> 01:37:07.880]   And I can't tell every time we do blue apron, there's one or two things we go, oh, we're
[01:37:07.880 --> 01:37:08.880]   making this again.
[01:37:08.880 --> 01:37:11.360]   But now you know how you've done it.
[01:37:11.360 --> 01:37:12.720]   Skillet vegetable chili.
[01:37:12.720 --> 01:37:14.240]   By the way, these are, this is a vegetarian meal.
[01:37:14.240 --> 01:37:17.240]   Yeah, they have meals for all your dieter needs.
[01:37:17.240 --> 01:37:20.440]   Skillet vegetable chili with cornmeal and cheddar drop biscuits.
[01:37:20.440 --> 01:37:21.440]   No.
[01:37:21.440 --> 01:37:24.480]   That's mean.
[01:37:24.480 --> 01:37:28.640]   Garlic, butter, shrimp and corn with green beans, salad and roasted purple tomatoes.
[01:37:28.640 --> 01:37:30.080]   Look, now's the time.
[01:37:30.080 --> 01:37:31.080]   Go to blood.
[01:37:31.080 --> 01:37:34.840]   You sure to check out while you're there the blue apron market on the website.
[01:37:34.840 --> 01:37:36.080]   They've got all sorts of pantry items.
[01:37:36.080 --> 01:37:37.720]   We've been buying all sorts of things.
[01:37:37.720 --> 01:37:41.000]   Tools and kitchen appliances and things from them.
[01:37:41.000 --> 01:37:44.080]   They even have a wine selection, which is fantastic.
[01:37:44.080 --> 01:37:45.880]   Blueapron.com/twit.
[01:37:45.880 --> 01:37:47.640]   Every meal has a wine pairing they recommend.
[01:37:47.640 --> 01:37:52.640]   So you can, you know, if you, if I know Becky likes a little wine now, then you know exactly
[01:37:52.640 --> 01:37:54.080]   what wine pairs with us.
[01:37:54.080 --> 01:37:55.080]   Wonderful.
[01:37:55.080 --> 01:37:56.080]   Blueapron.com/twit.
[01:37:56.080 --> 01:37:57.880]   Get three meals free.
[01:37:57.880 --> 01:38:01.960]   Yes, three meals free with your first purchase and free shipping.
[01:38:01.960 --> 01:38:05.160]   You'll love how good it feels and tastes to create incredible home-cooked meals with
[01:38:05.160 --> 01:38:06.160]   blue apron.
[01:38:06.160 --> 01:38:11.600]   And if you have kids, get the family plan and get the kids to cook it with you.
[01:38:11.600 --> 01:38:12.600]   Trust me on this.
[01:38:12.600 --> 01:38:13.600]   They love it.
[01:38:13.600 --> 01:38:14.600]   You love it.
[01:38:14.600 --> 01:38:15.760]   The house smells incredible.
[01:38:15.760 --> 01:38:17.560]   And you're giving them a great skill.
[01:38:17.560 --> 01:38:22.000]   And what a great family experience you look back on and think, oh, those are the days.
[01:38:22.000 --> 01:38:23.000]   Blueapron.
[01:38:23.000 --> 01:38:24.400]   I wish I could do it now.
[01:38:24.400 --> 01:38:26.960]   Blueapron.com/twit.
[01:38:26.960 --> 01:38:29.200]   Blueapron is a better way to cook.
[01:38:29.200 --> 01:38:32.040]   Can we thank him for their support of this weekend deck?
[01:38:32.040 --> 01:38:34.760]   Back to Becky in the gang.
[01:38:34.760 --> 01:38:37.440]   I love how he cracks himself up when he's reading the ads.
[01:38:37.440 --> 01:38:42.880]   Like he's read quite a few ads in his days here at Twit, but he's, he's just, he's, he's
[01:38:42.880 --> 01:38:43.880]   his own humor.
[01:38:43.880 --> 01:38:44.880]   I love Leo.
[01:38:44.880 --> 01:38:46.960]   He's seriously one of my oldest friends.
[01:38:46.960 --> 01:38:50.160]   So it's, it's awesome to come do this show, but I miss him when I'm here and it makes
[01:38:50.160 --> 01:38:52.160]   me think of him a lot.
[01:38:52.160 --> 01:39:00.520]   The next story is complicated and Facebook, they're clarifying its, their advertising
[01:39:00.520 --> 01:39:05.600]   policy after last week it was discovered by ProPublica that advertisers were able to
[01:39:05.600 --> 01:39:12.600]   target users based on anti-Semitic terms like Jew haters.
[01:39:12.600 --> 01:39:19.240]   Peter did the research and it's caused a ton of fewer in how ads are targeted.
[01:39:19.240 --> 01:39:25.760]   CTO of Facebook, Cheryl Sandberg wrote that they're tightening their enforcement processes.
[01:39:25.760 --> 01:39:31.000]   They're making sure that ad targeting terms don't violate Facebook's community standards.
[01:39:31.000 --> 01:39:36.160]   And that includes targeting anyone based on negative opinions based on race, ethnicity,
[01:39:36.160 --> 01:39:41.640]   national origin, religious affiliation, sexual orientation, sex, gender, gender identity,
[01:39:41.640 --> 01:39:43.920]   disability, diseases.
[01:39:43.920 --> 01:39:49.480]   And this is where I think the key is they also promise to add more human review to the automated
[01:39:49.480 --> 01:39:56.760]   processes and they're creating a program to encourage people to report abuses.
[01:39:56.760 --> 01:40:04.200]   Alex Cantrowitz at BuzzFeed tried the same thing at Google and basically it allowed the
[01:40:04.200 --> 01:40:14.960]   same types of racist phrases.
[01:40:14.960 --> 01:40:20.520]   So he tried to do this not only with anti-Semitic terms but also with racist phrases.
[01:40:20.520 --> 01:40:28.280]   And I guess I would ask Rob Reed, is this the first or one of many steps along the path
[01:40:28.280 --> 01:40:31.160]   of algorithms dividing us as people?
[01:40:31.160 --> 01:40:37.000]   Well, you know, first of all, going to the issues, I don't know if the fact that it was
[01:40:37.000 --> 01:40:42.640]   possible for people to target ads to this, I haven't seen any suggestion that either
[01:40:42.640 --> 01:40:47.040]   of these companies is doing a rampant business in these actual keywords.
[01:40:47.040 --> 01:40:51.800]   And I also, I kind of got the feeling that these were just kind of like open fields where
[01:40:51.800 --> 01:40:58.800]   one could type in any random string of letters and numbers if they wanted to or any four
[01:40:58.800 --> 01:41:01.000]   English words strung together.
[01:41:01.000 --> 01:41:05.640]   So I'm glad that the companies reacted with horror.
[01:41:05.640 --> 01:41:10.040]   There was some autocomplete that was available which makes me nervous.
[01:41:10.040 --> 01:41:11.040]   That's right, there was.
[01:41:11.040 --> 01:41:14.520]   The autocomplete was kind of horrifying because that means people type those things in the
[01:41:14.520 --> 01:41:18.120]   past and the algorithm is recognizing it.
[01:41:18.120 --> 01:41:21.320]   So I mean, I think the companies appropriately reacted with horror.
[01:41:21.320 --> 01:41:28.040]   I don't know if we can necessarily be furious with them for not having anticipated every
[01:41:28.040 --> 01:41:32.400]   plausible offensive four word combination like black people ruin anything.
[01:41:32.400 --> 01:41:36.360]   It would so never enter my mind to say those four words.
[01:41:36.360 --> 01:41:38.680]   I mean, and most minds it wouldn't enter their minds.
[01:41:38.680 --> 01:41:43.160]   So it's not surprising that Google didn't deliberately say, oh, somebody might say this,
[01:41:43.160 --> 01:41:44.960]   let's block it.
[01:41:44.960 --> 01:41:48.960]   Nonetheless, you know, the autocomplete is kind of chilling because I think that's algorithmically
[01:41:48.960 --> 01:41:52.920]   generated thing that means lots of people did happen to type in that thing.
[01:41:52.920 --> 01:41:55.840]   Which is why everything always autocompletes not worth.
[01:41:55.840 --> 01:41:56.840]   How weird is that, right?
[01:41:56.840 --> 01:42:02.120]   You type in anybody's name and it immediately like George Washington net worth.
[01:42:02.120 --> 01:42:05.040]   What is that means of debt for 200 years?
[01:42:05.040 --> 01:42:06.440]   Oh, I get it.
[01:42:06.440 --> 01:42:08.560]   What is Rob Reed's death worth net worth?
[01:42:08.560 --> 01:42:11.040]   There we go.
[01:42:11.040 --> 01:42:13.440]   An Equifax will tell you for free.
[01:42:13.440 --> 01:42:20.880]   I can't remember who said it, but the overarching concept was that if you write algorithms,
[01:42:20.880 --> 01:42:23.040]   it is and you're responsible for them.
[01:42:23.040 --> 01:42:28.240]   It is your duty to think about the worst possible thing it could do.
[01:42:28.240 --> 01:42:32.400]   And so you're your point that I would never think that.
[01:42:32.400 --> 01:42:38.320]   And so I mean, it is an infinite set of possibilities and therefore it's a programming challenge.
[01:42:38.320 --> 01:42:43.360]   But the bigger issue in my mind is that this is a profit margin question, which is the reason
[01:42:43.360 --> 01:42:48.520]   why these companies are such profit monsters is because they have so little human oversight
[01:42:48.520 --> 01:42:50.680]   and they have a responsibility.
[01:42:50.680 --> 01:42:56.600]   And to these companies' credits, specifically Google, I think, well, I'll get to Twitter
[01:42:56.600 --> 01:43:03.120]   later, but Mark Zuckerberg also said this week that Facebook is going to stop allowing
[01:43:03.120 --> 01:43:07.120]   untraceable political ads that I come from a TV world.
[01:43:07.120 --> 01:43:13.520]   I've been in the TV business since 1993 and we have a duty because the FTC mandates that
[01:43:13.520 --> 01:43:21.040]   we have a lot of oversight on what ads we take, where they air and what standards they meet.
[01:43:21.040 --> 01:43:26.160]   And that is the beauty of the internet and we're really seeing the dark side.
[01:43:26.160 --> 01:43:27.160]   Yeah.
[01:43:27.160 --> 01:43:28.160]   Yeah.
[01:43:28.160 --> 01:43:32.760]   I think, well, that's partly a relic, I believe, of the fact that TV stations were using a
[01:43:32.760 --> 01:43:35.840]   public resource in the bandwidths and it was a finite resource.
[01:43:35.840 --> 01:43:41.120]   And because you were taking a public good, you fell under a certain level of licensing
[01:43:41.120 --> 01:43:43.400]   and restriction that the internet didn't fall under.
[01:43:43.400 --> 01:43:49.160]   So I think that might have been more of a TV station granted a monopoly, basically.
[01:43:49.160 --> 01:43:50.160]   Yeah.
[01:43:50.160 --> 01:43:51.160]   Precisely.
[01:43:51.160 --> 01:43:53.160]   More than that than necessarily a speech regulation.
[01:43:53.160 --> 01:43:54.160]   It was back in the day.
[01:43:54.160 --> 01:43:55.160]   It was back in the day.
[01:43:55.160 --> 01:43:56.160]   When you didn't have the...
[01:43:56.160 --> 01:43:58.360]   Like, when was the last time anybody listened to anything on terrestrial broadcasting?
[01:43:58.360 --> 01:43:59.360]   Although, I will say...
[01:43:59.360 --> 01:44:01.160]   Oh, Bob and I are like this.
[01:44:01.160 --> 01:44:02.160]   Oh, I pay check.
[01:44:02.160 --> 01:44:03.160]   No, no, no.
[01:44:03.160 --> 01:44:08.000]   When I go home, my wife and I are going to sit down and watch the Oakland Raiders on
[01:44:08.000 --> 01:44:12.440]   our HDTV receiver because we're cable cutters.
[01:44:12.440 --> 01:44:14.080]   This is how I get the ABC News.
[01:44:14.080 --> 01:44:17.280]   I use Google Assistant and I say, "Play ABC News."
[01:44:17.280 --> 01:44:19.320]   But you're not going to get the Raiders game.
[01:44:19.320 --> 01:44:22.040]   Well, you could watch the football on...
[01:44:22.040 --> 01:44:23.040]   There we go.
[01:44:23.040 --> 01:44:24.040]   Wasn't it on NFL Network?
[01:44:24.040 --> 01:44:25.040]   That's playing.
[01:44:25.040 --> 01:44:26.040]   No, we're cord cutters.
[01:44:26.040 --> 01:44:27.040]   Wow, that's fast.
[01:44:27.040 --> 01:44:28.040]   Yeah.
[01:44:28.040 --> 01:44:30.160]   Google Assistant is the app of the year.
[01:44:30.160 --> 01:44:31.160]   Wow.
[01:44:31.160 --> 01:44:32.160]   I think that's cool.
[01:44:32.160 --> 01:44:33.720]   You could get some football online this week.
[01:44:33.720 --> 01:44:35.800]   It was one of the first times where I really noticed that it was...
[01:44:35.800 --> 01:44:36.800]   I can't remember if it was space.
[01:44:36.800 --> 01:44:39.840]   I put a show that they were on Twitter last year.
[01:44:39.840 --> 01:44:42.240]   The local TV, it's a whole weird thing.
[01:44:42.240 --> 01:44:43.640]   Oh, the blackouts.
[01:44:43.640 --> 01:44:48.400]   But the thing with being able to know everything an algorithm to do is very, very problematic
[01:44:48.400 --> 01:44:54.520]   because particularly with a lot of the neural network programming that's going on, when
[01:44:54.520 --> 01:45:00.800]   you take a training set and a really sophisticated machine learning system and second on that
[01:45:00.800 --> 01:45:06.480]   and it starts making connections and it starts processing things, the programmers quite literally
[01:45:06.480 --> 01:45:09.520]   can't tell you exactly how it's working.
[01:45:09.520 --> 01:45:14.720]   They don't understand because it's gone through back propagation and then also in some places
[01:45:14.720 --> 01:45:19.880]   they might do genetic coding where the code is evolving itself and the output can be something
[01:45:19.880 --> 01:45:26.880]   that's radically efficient but is quite literally unknowable to the engineer who set that process
[01:45:26.880 --> 01:45:29.440]   in motion to how and why it works.
[01:45:29.440 --> 01:45:30.440]   Is that because it's...
[01:45:30.440 --> 01:45:31.440]   Is that because it's...
[01:45:31.440 --> 01:45:34.760]   Are you saying that's because it's learning or you're saying that's because the data sets
[01:45:34.760 --> 01:45:37.680]   are so infinite that it's hard to test every scenario.
[01:45:37.680 --> 01:45:43.000]   It's because in going through the data and finding this leads to an accurate outcome and
[01:45:43.000 --> 01:45:49.160]   that leads to an inaccurate outcome, it's shaping the pathways within itself in a semi-automated
[01:45:49.160 --> 01:45:53.760]   manner that gets... that gradually approaches more and more and more correctness.
[01:45:53.760 --> 01:45:58.400]   This is how image recognition has suddenly worked after not working for decades and decades
[01:45:58.400 --> 01:46:02.840]   and in a very different branch which is genetic programming.
[01:46:02.840 --> 01:46:10.600]   We might... we'll basically tell software to evolve itself and maybe it's trying to target
[01:46:10.600 --> 01:46:16.480]   a positive return in high velocity trading in the market and it knows how to... how scores
[01:46:16.480 --> 01:46:17.480]   kept.
[01:46:17.480 --> 01:46:21.600]   Scores kept if you make money you win and if you lose money you lose and programs could
[01:46:21.600 --> 01:46:27.920]   start evolving in a very, very rapid way where thousands or even more attempts at coming
[01:46:27.920 --> 01:46:33.320]   up with an algorithm that work are mutated and evolved and rejected on a way that's very...
[01:46:33.320 --> 01:46:35.080]   that's quite literally Darwinian.
[01:46:35.080 --> 01:46:38.040]   And again you might end up with an output that you really don't understand.
[01:46:38.040 --> 01:46:43.240]   Now the most arguably there's a guy named Nicholas Bostrom who is a leading theorist
[01:46:43.240 --> 01:46:48.960]   in super intelligence and super AI research and he is quite concerned about something that's
[01:46:48.960 --> 01:46:53.640]   become quite widespread which he refers to as neuromorphic programming.
[01:46:53.640 --> 01:46:57.920]   And a better term might be neuromomatic which is basically where researchers, particularly
[01:46:57.920 --> 01:47:03.680]   AI researchers are replicating the way the human brain does things because we know it
[01:47:03.680 --> 01:47:09.560]   works here and we can replicate it imperfectly in silicon but the problem is you end up creating
[01:47:09.560 --> 01:47:12.480]   something that you really truly don't understand.
[01:47:12.480 --> 01:47:16.360]   It's like this is how it works in nature and we can replicate it and we can speed it up
[01:47:16.360 --> 01:47:17.360]   by a million eggs.
[01:47:17.360 --> 01:47:26.240]   That's like a simpler, the CEO of Turi which Applebot gave a talk last year at the Data
[01:47:26.240 --> 01:47:32.920]   Science Conference and he said we trained AI on huskies and wolves and it kept throwing
[01:47:32.920 --> 01:47:40.160]   an error once in a while and they looked into how the algorithm learned about those two things
[01:47:40.160 --> 01:47:42.720]   and it learned on the snow in the picture.
[01:47:42.720 --> 01:47:45.040]   It learned on the wrong thing.
[01:47:45.040 --> 01:47:49.800]   You took the snow out of the picture because wolves are usually on grass and huskies are
[01:47:49.800 --> 01:47:54.120]   usually on snow and once in a while it would be incorrect.
[01:47:54.120 --> 01:47:58.400]   You take the snow out of the picture and it learned on the face of the dog or on the wolf
[01:47:58.400 --> 01:48:00.960]   and it got correct.
[01:48:00.960 --> 01:48:05.400]   And that's just a very simple way to demonstrate where we're going.
[01:48:05.400 --> 01:48:11.120]   Google Assistant is learning from our behavior.
[01:48:11.120 --> 01:48:15.360]   It switches words on me once in a while because it's trying to fit me into a search that our
[01:48:15.360 --> 01:48:17.520]   array was done.
[01:48:17.520 --> 01:48:20.920]   And we're going to get some complex systems.
[01:48:20.920 --> 01:48:23.200]   It's going to be very hard for anybody to look at.
[01:48:23.200 --> 01:48:24.200]   No, no, I've got to say.
[01:48:24.200 --> 01:48:25.200]   To figure out where it went wrong.
[01:48:25.200 --> 01:48:29.160]   As a science fiction writer I think this is awesome because there's all kinds of crazy
[01:48:29.160 --> 01:48:31.160]   stories that we get to tell around this stuff.
[01:48:31.160 --> 01:48:36.600]   But there is a genuine danger where we are definitely well past the point where all code
[01:48:36.600 --> 01:48:38.960]   is understood by its titular creator.
[01:48:38.960 --> 01:48:41.320]   We passed that many, many years ago.
[01:48:41.320 --> 01:48:43.680]   Finn, come here for a second.
[01:48:43.680 --> 01:48:51.120]   So my son's nine and you have studied artificial intelligence and you know so much more than
[01:48:51.120 --> 01:48:52.560]   I ever would.
[01:48:52.560 --> 01:48:54.600]   Tell him what he needs to know.
[01:48:54.600 --> 01:48:55.600]   What he needs to know.
[01:48:55.600 --> 01:48:58.320]   I actually studied Arabic in college.
[01:48:58.320 --> 01:49:01.240]   I can teach him how to say all kinds of great phrases in Arabic.
[01:49:01.240 --> 01:49:04.240]   Future proof this kid for the AI future that's coming.
[01:49:04.240 --> 01:49:09.240]   You know I think that I actually where is the bathroom to Arabic.
[01:49:09.240 --> 01:49:12.200]   Finn and come on if you're in Cairo.
[01:49:12.200 --> 01:49:13.200]   Wow.
[01:49:13.200 --> 01:49:15.080]   And I can't translate.
[01:49:15.080 --> 01:49:17.080]   So Arabic translation is definitely not the place to go.
[01:49:17.080 --> 01:49:21.560]   I actually by sheer happenstance I was in a group of people having breakfast in New York
[01:49:21.560 --> 01:49:28.960]   a couple of days ago and Andrew McAfee from MIT was there who has written co-authored one
[01:49:28.960 --> 01:49:35.240]   of the leading books raising the alarm of structural unemployment for many, many people
[01:49:35.240 --> 01:49:37.040]   because of automation.
[01:49:37.040 --> 01:49:38.680]   And I asked him that very question.
[01:49:38.680 --> 01:49:42.320]   I didn't actually cite Finn because we had not yet met at that point.
[01:49:42.320 --> 01:49:48.240]   But he said that the advice he gives to his own kids is really pushing into realms where
[01:49:48.240 --> 01:49:51.520]   it's an intensely human interaction.
[01:49:51.520 --> 01:49:56.840]   So it's going to be a long time before we want computers as therapists or masseuses or
[01:49:56.840 --> 01:50:03.360]   also things that are really kind of very, very high end creativity.
[01:50:03.360 --> 01:50:09.680]   And so he reassured me he said as a guy who writes 575 page long science fiction novels
[01:50:09.680 --> 01:50:12.360]   you'll be one of the last to go.
[01:50:12.360 --> 01:50:14.600]   And that had a chilling ring to it.
[01:50:14.600 --> 01:50:18.000]   But that was better than first to go.
[01:50:18.000 --> 01:50:21.840]   So what I'm saying, Finn is write some science fiction young man.
[01:50:21.840 --> 01:50:22.840]   The creative arts.
[01:50:22.840 --> 01:50:23.840]   The creative arts.
[01:50:23.840 --> 01:50:24.840]   Yeah.
[01:50:24.840 --> 01:50:27.520]   Google knows all facts, right?
[01:50:27.520 --> 01:50:29.240]   Why is the sky blue?
[01:50:29.240 --> 01:50:30.240]   Right?
[01:50:30.240 --> 01:50:31.240]   It'll give us an answer.
[01:50:31.240 --> 01:50:32.240]   What does it say?
[01:50:32.240 --> 01:50:33.240]   Do you see our math department?
[01:50:33.240 --> 01:50:38.200]   A clear cloudless daytime sky is blue because molecules in the air scatter blue light from
[01:50:38.200 --> 01:50:40.160]   the sun more than they scatter red light.
[01:50:40.160 --> 01:50:41.160]   Right.
[01:50:41.160 --> 01:50:42.160]   So learning things by.
[01:50:42.160 --> 01:50:43.160]   We see red and orange.
[01:50:43.160 --> 01:50:45.880]   We're going to know it's going to everything.
[01:50:45.880 --> 01:50:46.880]   It's going to everything.
[01:50:46.880 --> 01:50:49.680]   Never knows what to show.
[01:50:49.680 --> 01:50:52.400]   Google knows most facts.
[01:50:52.400 --> 01:50:57.480]   We have lots of interesting arguments at our house about which facts are incorrect, right?
[01:50:57.480 --> 01:51:00.240]   Who was the first James Bond?
[01:51:00.240 --> 01:51:03.800]   There's information from Wikipedia.
[01:51:03.800 --> 01:51:04.800]   Oh, that's different.
[01:51:04.800 --> 01:51:07.760]   It used to say Sean Connery, which is not correct.
[01:51:07.760 --> 01:51:09.200]   Whoa, maybe it learned.
[01:51:09.200 --> 01:51:15.480]   It's not correct because James Bond, there was a James Bond before the movie James Bond.
[01:51:15.480 --> 01:51:16.480]   If you think about it.
[01:51:16.480 --> 01:51:17.480]   The literary character.
[01:51:17.480 --> 01:51:19.480]   There was a stage play.
[01:51:19.480 --> 01:51:20.480]   Ah.
[01:51:20.480 --> 01:51:26.400]   I forget the guy's name, Brandon Word says it shows me inaccuracies in Google.
[01:51:26.400 --> 01:51:27.720]   So there's another clue.
[01:51:27.720 --> 01:51:29.360]   Anything that mandates subjectivity.
[01:51:29.360 --> 01:51:31.440]   So that's a very nuanced thing.
[01:51:31.440 --> 01:51:32.440]   So, okay.
[01:51:32.440 --> 01:51:39.280]   But where I would be as a kid is learn the framework of how to ask these systems what,
[01:51:39.280 --> 01:51:44.760]   you know, questions because, you know, knowing how things work makes your life more enjoyable.
[01:51:44.760 --> 01:51:49.880]   But you have to have the curiosity of being able to ask it for help and knowing when it's
[01:51:49.880 --> 01:51:51.680]   not giving you the right help.
[01:51:51.680 --> 01:51:52.680]   Yeah, yeah.
[01:51:52.680 --> 01:51:56.440]   Like when is the map taking it somewhere incorrect, right?
[01:51:56.440 --> 01:51:58.680]   And we all are figuring that out.
[01:51:58.680 --> 01:52:00.680]   So it's kind of the King Solomon roles.
[01:52:00.680 --> 01:52:06.840]   I mean, do you think that AI will take over the role of lawyers and judges?
[01:52:06.840 --> 01:52:08.480]   Lawyers before judges.
[01:52:08.480 --> 01:52:12.560]   I don't know if we're going to trust it on the judge front because there, you know,
[01:52:12.560 --> 01:52:17.560]   in early attempts that have been made with like parole boards and so forth.
[01:52:17.560 --> 01:52:21.040]   People have detected things that look like racial bias coming out of the AI.
[01:52:21.040 --> 01:52:26.040]   And I think that, you know, however explicable that might be when you try to crawl through
[01:52:26.040 --> 01:52:30.520]   the code that you don't really understand to figure out what was going on there.
[01:52:30.520 --> 01:52:33.760]   Humans will be highly uncomfortable with that kind of thing and will prefer to leave that
[01:52:33.760 --> 01:52:34.760]   role to humans.
[01:52:34.760 --> 01:52:40.840]   However, Dan Arielli, who's done a lot of amazing work in behavioral psychology, wrote
[01:52:40.840 --> 01:52:45.640]   a book called Predictably Unrational and did a bunch of other great work, has shown that
[01:52:45.640 --> 01:52:51.800]   human judges, their decisions and parole boards and so forth, are heavily, heavily influenced
[01:52:51.800 --> 01:52:57.600]   by how many hours that has been since they've eaten, taken an enormous amount of data, proximity
[01:52:57.600 --> 01:53:01.840]   to lunch leads to a great deal of leniency.
[01:53:01.840 --> 01:53:03.840]   It was way statistically significant.
[01:53:03.840 --> 01:53:06.000]   After lunch, not before lunch.
[01:53:06.000 --> 01:53:07.640]   I forget exactly what it was.
[01:53:07.640 --> 01:53:10.400]   It was the first thing in the morning and after lunch.
[01:53:10.400 --> 01:53:11.960]   There were times you shared it.
[01:53:11.960 --> 01:53:14.160]   You wanted to come up.
[01:53:14.160 --> 01:53:15.160]   And that's chilling.
[01:53:15.160 --> 01:53:19.080]   I mean, imagine that that's how justice gets meted out.
[01:53:19.080 --> 01:53:24.480]   But yeah, I think that we know justice is not justice for a lot of people.
[01:53:24.480 --> 01:53:27.760]   Society will probably be more comfortable with robotic lawyers than judges.
[01:53:27.760 --> 01:53:31.720]   So creativity, but some judgment oriented things.
[01:53:31.720 --> 01:53:36.000]   Where else do you think that the human interaction will prevail?
[01:53:36.000 --> 01:53:39.360]   I think being super enabled by computers.
[01:53:39.360 --> 01:53:45.040]   So Tim O'Reilly has got an amazing book coming out on October 10th and I've been lucky enough
[01:53:45.040 --> 01:53:46.040]   to read it.
[01:53:46.040 --> 01:53:48.160]   And it's called WTF.
[01:53:48.160 --> 01:53:49.160]   Where's the future?
[01:53:49.160 --> 01:53:50.160]   What's the future or something like that?
[01:53:50.160 --> 01:53:52.680]   We also know it stands for something else.
[01:53:52.680 --> 01:53:54.160]   But he made this really cool point.
[01:53:54.160 --> 01:54:00.720]   He calls them augmented workers, which are people who have been super empowered by their
[01:54:00.720 --> 01:54:02.440]   pairing with technology.
[01:54:02.440 --> 01:54:07.600]   And he makes the point that Amazon, since they brought in their famous Kiva robots, has
[01:54:07.600 --> 01:54:13.120]   been on a hiring, and these are the robots that automate Amazon's warehouses to a degree
[01:54:13.120 --> 01:54:15.560]   that a lot of folks find frightening.
[01:54:15.560 --> 01:54:20.720]   They have been on a hiring tear that unlike without precedent in the history of warehouse
[01:54:20.720 --> 01:54:25.400]   management in this country, since they brought in the Kiva robots, because the workers who
[01:54:25.400 --> 01:54:30.760]   are in there are now managing fleets of robots and every worker is wildly productive.
[01:54:30.760 --> 01:54:35.760]   And there is the famous statistic that's been attested to in multiple sources that after
[01:54:35.760 --> 01:54:39.880]   the rise of automatic teller machines in the early 80s, the number of people employed
[01:54:39.880 --> 01:54:42.680]   as tellers in the country has gone up and up and up.
[01:54:42.680 --> 01:54:45.600]   Because again, they're now able to do higher and higher level things.
[01:54:45.600 --> 01:54:52.440]   And so I think if you put yourself in a role where you are in a sense the computer's manager,
[01:54:52.440 --> 01:54:56.120]   you're letting it do the things that fall within its superpower, but you're doing those
[01:54:56.120 --> 01:54:58.480]   judgment-y things that fall outside of it.
[01:54:58.480 --> 01:55:01.680]   I mean, Gary Kasparov is very big into this.
[01:55:01.680 --> 01:55:09.800]   He and others call it the phenomenon the Centaur, which is sort of a human and AI team.
[01:55:09.800 --> 01:55:16.440]   You can pair a fairly mediocre chess player with some pretty lousy software, chess playing
[01:55:16.440 --> 01:55:21.560]   software, and those two things together can slaughter the best software in the world,
[01:55:21.560 --> 01:55:26.360]   which is now 20 years more evolved than the software that beat Gary Kasparov 20 years
[01:55:26.360 --> 01:55:27.360]   ago in 1997.
[01:55:27.360 --> 01:55:33.280]   I read this big blue at IBM says, "Yes, big blue beats a human at jeopardy, but what
[01:55:33.280 --> 01:55:37.520]   we didn't tell you was big blue and a human beats big blue."
[01:55:37.520 --> 01:55:38.520]   Every time.
[01:55:38.520 --> 01:55:41.920]   Because you go back and look at that Ken Jennings tape and everything that big blue or Watson
[01:55:41.920 --> 01:55:45.520]   got wrong were these thigh slappers.
[01:55:45.520 --> 01:55:47.760]   This really like, "Oh my God, how did it come up?
[01:55:47.760 --> 01:55:48.760]   It's so smart."
[01:55:48.760 --> 01:55:52.840]   But then it got, "And if, yeah, Ken Jennings were teamed up with Watson, it'd be unstoppable."
[01:55:52.840 --> 01:55:59.520]   And so I think that sort of Centaur pairing of machine wisdom with human wisdom will get
[01:55:59.520 --> 01:56:03.400]   us through at least the next few years until computers are bettered absolutely everything.
[01:56:03.400 --> 01:56:09.600]   So just as mixed reality is superseding a straight virtual reality in how we perceive
[01:56:09.600 --> 01:56:15.160]   things now, mixed AI might be a future that makes sense, at least in the near term.
[01:56:15.160 --> 01:56:16.800]   Yeah, a lot of smart people think so.
[01:56:16.800 --> 01:56:17.800]   That's very Kasparov being one.
[01:56:17.800 --> 01:56:20.040]   That's why I'm learning Google Assistant, right?
[01:56:20.040 --> 01:56:21.040]   Yeah.
[01:56:21.040 --> 01:56:22.040]   Can I jump in here, Becky?
[01:56:22.040 --> 01:56:23.360]   That's something important to say to your kids.
[01:56:23.360 --> 01:56:28.000]   So I did a story about six months ago that I titled "A Billion Useless People About This,"
[01:56:28.000 --> 01:56:31.640]   because that's the stark reality that some people, this is why people don't invite me
[01:56:31.640 --> 01:56:32.640]   to cocktail parties honestly.
[01:56:32.640 --> 01:56:34.640]   It's so much fun, Mom.
[01:56:34.640 --> 01:56:35.640]   Yeah.
[01:56:35.640 --> 01:56:37.240]   You're all laid off in 10 years.
[01:56:37.240 --> 01:56:38.240]   Yeah.
[01:56:38.240 --> 01:56:40.320]   Well, this is the stark, you know, you wonder what are these people going to do.
[01:56:40.320 --> 01:56:44.320]   But it was really based on this amazing study by these Oxford researchers.
[01:56:44.320 --> 01:56:50.120]   They ranked 700 professions and they're likely to be automatable in the near future.
[01:56:50.120 --> 01:56:55.720]   And the tellers are at the top, call service representatives are at the top, the jobs that
[01:56:55.720 --> 01:56:56.720]   you would expect.
[01:56:56.720 --> 01:57:01.200]   The bottom of that list, the least likely automatable jobs, there's the story, is really,
[01:57:01.200 --> 01:57:03.680]   really useful if you're a young person right now.
[01:57:03.680 --> 01:57:06.040]   It's populated by surprising things.
[01:57:06.040 --> 01:57:09.320]   You guys have already said the creative class stuff and working with machines.
[01:57:09.320 --> 01:57:11.520]   But empathy is another thing.
[01:57:11.520 --> 01:57:13.040]   Computers are not going to be good at empathy.
[01:57:13.040 --> 01:57:17.960]   So healthcare workers in general, we're going to need them, particularly geriatric healthcare.
[01:57:17.960 --> 01:57:23.240]   But anyone or one job you'll find on that list that's really promising are physical
[01:57:23.240 --> 01:57:28.240]   therapists, physical recovery therapists, both for athletes and for people who have other
[01:57:28.240 --> 01:57:29.240]   disabilities.
[01:57:29.240 --> 01:57:33.720]   But the key component for all of those things is the ability to empathize with other humans.
[01:57:33.720 --> 01:57:37.160]   And so whatever you can do to develop your empathy is going to increase your job prospects.
[01:57:37.160 --> 01:57:38.160]   Watch VR.
[01:57:38.160 --> 01:57:45.960]   VR is Stanford University is about to report on the study they did with people who are
[01:57:45.960 --> 01:57:49.840]   viewing homeless encampments on either a screen or in VR.
[01:57:49.840 --> 01:57:54.280]   And VR is way better at triggering your empathy.
[01:57:54.280 --> 01:57:55.280]   Yep.
[01:57:55.280 --> 01:57:56.840]   Well, this has given me some great advice.
[01:57:56.840 --> 01:57:57.840]   So here's what we're going to do.
[01:57:57.840 --> 01:58:02.480]   When you go home, I want you to fire up the Roomba and I want you to simultaneously get
[01:58:02.480 --> 01:58:03.680]   the vacuum cleaner out.
[01:58:03.680 --> 01:58:05.640]   And I want you to vacuum both of you.
[01:58:05.640 --> 01:58:06.640]   As a team.
[01:58:06.640 --> 01:58:08.120]   Yeah, I want you to work with that.
[01:58:08.120 --> 01:58:09.120]   Can we do that?
[01:58:09.120 --> 01:58:10.120]   That was a ploy.
[01:58:10.120 --> 01:58:11.120]   That was a ploy.
[01:58:11.120 --> 01:58:12.120]   What if I was a ploy?
[01:58:12.120 --> 01:58:13.120]   Because the Roomba doesn't get in the corners.
[01:58:13.120 --> 01:58:14.120]   Then vacuum around?
[01:58:14.120 --> 01:58:15.120]   Yeah.
[01:58:15.120 --> 01:58:16.120]   You could do that.
[01:58:16.120 --> 01:58:18.200]   And Robert just made a good choice.
[01:58:18.200 --> 01:58:19.360]   An observation.
[01:58:19.360 --> 01:58:23.520]   The Roomba is really bad underneath the table where you drop all the Cheerios.
[01:58:23.520 --> 01:58:24.520]   Yeah.
[01:58:24.520 --> 01:58:25.520]   So, yeah.
[01:58:25.520 --> 01:58:27.000]   Hey, we're going to take it.
[01:58:27.000 --> 01:58:28.760]   In the corner it pushes them all into the corner.
[01:58:28.760 --> 01:58:30.000]   So you've got to find the fun.
[01:58:30.000 --> 01:58:31.000]   See, AI with humans.
[01:58:31.000 --> 01:58:34.200]   I think I'm always looking for the upside.
[01:58:34.200 --> 01:58:35.200]   I think that's what happens too.
[01:58:35.200 --> 01:58:39.400]   And you have kids as you're like, I've got to figure out the upside of this.
[01:58:39.400 --> 01:58:41.160]   It really forces you to do that.
[01:58:41.160 --> 01:58:46.440]   But we're going to take a quick look at what has been happening, is going to happen on
[01:58:46.440 --> 01:58:47.520]   the Twit network.
[01:58:47.520 --> 01:58:51.840]   So take a look at all the amazing things that are happening here at Twit.
[01:58:51.840 --> 01:58:53.480]   Previously on Twit.
[01:58:53.480 --> 01:58:55.640]   I don't think I've ever looked like that in my life.
[01:58:55.640 --> 01:58:58.560]   But that's me on the KTLA website.
[01:58:58.560 --> 01:59:01.560]   Can you take that exact picture with the iPhone 8?
[01:59:01.560 --> 01:59:02.560]   Oh my gosh.
[01:59:02.560 --> 01:59:04.560]   This is going to be your next.
[01:59:04.560 --> 01:59:05.560]   That's pretty good, but it's funny.
[01:59:05.560 --> 01:59:07.480]   How does it make me handsome?
[01:59:07.480 --> 01:59:08.480]   Know how.
[01:59:08.480 --> 01:59:13.680]   I'm going to talk first about my very favorite feature and that's do not disturb while driving.
[01:59:13.680 --> 01:59:14.680]   Okay, yes.
[01:59:14.680 --> 01:59:19.520]   So you have to say, I'm not driving or close.
[01:59:19.520 --> 01:59:21.040]   You don't want to light your iPhone.
[01:59:21.040 --> 01:59:24.040]   So if you get a text message, you can auto reply.
[01:59:24.040 --> 01:59:25.480]   So I changed this a little bit.
[01:59:25.480 --> 01:59:31.680]   I love you too much to answer.
[01:59:31.680 --> 01:59:33.560]   No, it's not just an auto reply.
[01:59:33.560 --> 01:59:36.240]   It's also a moral judgment.
[01:59:36.240 --> 01:59:37.240]   Windows weekly.
[01:59:37.240 --> 01:59:39.040]   Mary Jo, I know you're not into it.
[01:59:39.040 --> 01:59:42.640]   However, the next time you come to the Brick House, we will have multiple machines here,
[01:59:42.640 --> 01:59:43.640]   all with Cuphead.
[01:59:43.640 --> 01:59:46.680]   Would you be willing to go a couple of rounds of co-op with me?
[01:59:46.680 --> 01:59:47.680]   Sure.
[01:59:47.680 --> 01:59:49.000]   You'll have to show me how to play, but I would try it.
[01:59:49.000 --> 01:59:52.680]   I think the problem is people who don't play a lot of games pick this thing up and they're
[01:59:52.680 --> 01:59:53.680]   like, oh, this is kind of hard.
[01:59:53.680 --> 01:59:56.880]   It's like, yeah, you've been using a fidget spinner for the past six months.
[01:59:56.880 --> 01:59:59.320]   Maybe you should do something slightly more complicated.
[01:59:59.320 --> 02:00:00.320]   Twit.
[02:00:00.320 --> 02:00:01.320]   Leo says hi.
[02:00:01.320 --> 02:00:03.520]   Listen, she's gone.
[02:00:03.520 --> 02:00:04.520]   Let's talk about her.
[02:00:04.520 --> 02:00:05.520]   Oh, yeah.
[02:00:05.520 --> 02:00:06.520]   Go ahead.
[02:00:06.520 --> 02:00:08.000]   She'll ever be able to play Cuphead.
[02:00:08.000 --> 02:00:09.960]   So this can be hilarious when I was.
[02:00:09.960 --> 02:00:11.280]   Actually, it's incredibly difficult to play.
[02:00:11.280 --> 02:00:13.600]   I kind of downplayed that because I didn't want it to be freaked out.
[02:00:13.600 --> 02:00:14.600]   Yeah, yeah.
[02:00:14.600 --> 02:00:16.440]   Editor, we had to cut this part out of the episode.
[02:00:16.440 --> 02:00:17.440]   Yeah, this never happened.
[02:00:17.440 --> 02:00:18.440]   This never happened.
[02:00:18.440 --> 02:00:19.440]   They'll develop into different.
[02:00:19.440 --> 02:00:20.440]   What do you think?
[02:00:20.440 --> 02:00:21.440]   But like, robot.
[02:00:21.440 --> 02:00:23.440]   Do you think artificial intelligence is good?
[02:00:23.440 --> 02:00:24.440]   No.
[02:00:24.440 --> 02:00:25.440]   No.
[02:00:25.440 --> 02:00:26.440]   How come?
[02:00:26.440 --> 02:00:31.480]   People would lose a ton of, there would be realist jobs.
[02:00:31.480 --> 02:00:34.400]   There's going to be way more jobs because of it.
[02:00:34.400 --> 02:00:39.240]   Because of what he said in the Amazon factory, just because you don't, you have an assistant
[02:00:39.240 --> 02:00:40.600]   that's driving your car.
[02:00:40.600 --> 02:00:44.160]   You're going to have a lot of new kinds of jobs coming.
[02:00:44.160 --> 02:00:48.520]   But it's hard to explain that to people who haven't seen mixed reality glasses.
[02:00:48.520 --> 02:00:53.600]   Because mixed reality glasses can teach you any job in real time on top of the job.
[02:00:53.600 --> 02:00:57.960]   They use them at Caterpillar to teach how to fix a million dollar tractor because it shows
[02:00:57.960 --> 02:01:00.760]   you on top of the tractor how to do it.
[02:01:00.760 --> 02:01:06.680]   That's going to unlock a whole raft of new jobs that looks sort of like TV jobs or Hollywood
[02:01:06.680 --> 02:01:07.680]   jobs.
[02:01:07.680 --> 02:01:08.680]   More fun jobs maybe.
[02:01:08.680 --> 02:01:09.680]   That sounds good.
[02:01:09.680 --> 02:01:10.680]   Yeah, better.
[02:01:10.680 --> 02:01:11.680]   Okay, good.
[02:01:11.680 --> 02:01:12.680]   I'm glad we figured this out.
[02:01:12.680 --> 02:01:16.840]   We're going to take a quick break and then we're going to come back with a consumer story
[02:01:16.840 --> 02:01:20.280]   that makes me sad, especially as an athlete.
[02:01:20.280 --> 02:01:22.880]   It's a story that I promoted and Bob I bet you did too.
[02:01:22.880 --> 02:01:24.120]   So we'll be right back.
[02:01:24.120 --> 02:01:25.120]   All right.
[02:01:25.120 --> 02:01:30.280]   One more word from our sponsor than back to the final section of the show.
[02:01:30.280 --> 02:01:33.560]   Thank you Becky Worley and company for filling in for me.
[02:01:33.560 --> 02:01:35.880]   Well, I'll be back next week.
[02:01:35.880 --> 02:01:37.800]   I'm sure I'm missing you guys terribly.
[02:01:37.800 --> 02:01:40.200]   Well, maybe not.
[02:01:40.200 --> 02:01:43.000]   Anyway, I hope you're having fun.
[02:01:43.000 --> 02:01:46.000]   I hope you will take a look at our sponsor today, Cap Terek.
[02:01:46.000 --> 02:01:48.200]   What Cap Terek is a great idea.
[02:01:48.200 --> 02:01:49.760]   This is free.
[02:01:49.760 --> 02:01:53.640]   There's no cost, no obligation, no salesman will call.
[02:01:53.640 --> 02:01:56.320]   What it is is a business software directory.
[02:01:56.320 --> 02:02:01.120]   So if you're one of those people that gets tasked by the boss with coming up with, oh,
[02:02:01.120 --> 02:02:07.960]   and the boss says, oh, Smithers, find that software to manage the business, whether it's
[02:02:07.960 --> 02:02:11.960]   auto repair or dentistry or a yoga studio.
[02:02:11.960 --> 02:02:14.040]   You have yoga studio software.
[02:02:14.040 --> 02:02:15.040]   Fine.
[02:02:15.040 --> 02:02:16.640]   It wouldn't be Smithers.
[02:02:16.640 --> 02:02:17.640]   It'd be shocking.
[02:02:17.640 --> 02:02:18.640]   Shockedy.
[02:02:18.640 --> 02:02:22.080]   We need software to manage the yoga studio.
[02:02:22.080 --> 02:02:25.920]   Well, go, go, don't shock, don't Google it.
[02:02:25.920 --> 02:02:27.960]   Because you're going to get random results.
[02:02:27.960 --> 02:02:30.200]   Go to Cap Terek.
[02:02:30.200 --> 02:02:33.840]   Whether you're a startup looking to keep better track of customers, a nonprofit, I hope they
[02:02:33.840 --> 02:02:38.280]   have a record fundraising year or a business that simply needs better payroll software,
[02:02:38.280 --> 02:02:41.480]   you'll can find the software you need at Cap Terek.
[02:02:41.480 --> 02:02:44.240]   Cap Terek has got you covered.
[02:02:44.240 --> 02:02:49.120]   CapTerek.com is the best easy to use software comparison site.
[02:02:49.120 --> 02:02:54.720]   Hundreds of categories, thousands of programs, hundreds of thousands of reviews by users
[02:02:54.720 --> 02:03:01.840]   like you, Shockedy, people who've used that software and know the pros and the cons, the
[02:03:01.840 --> 02:03:02.840]   good and the bad.
[02:03:02.840 --> 02:03:04.640]   And it's all there.
[02:03:04.640 --> 02:03:08.240]   All your homework is being done for you.
[02:03:08.240 --> 02:03:10.200]   And this is free.
[02:03:10.200 --> 02:03:11.200]   There's no obligation.
[02:03:11.200 --> 02:03:12.880]   There's no registration even.
[02:03:12.880 --> 02:03:15.560]   You're not going to get emails or phone calls or anything.
[02:03:15.560 --> 02:03:18.840]   You go there, pick the category you're looking for.
[02:03:18.840 --> 02:03:20.560]   You do yoga studios.
[02:03:20.560 --> 02:03:22.520]   And then you'll see, you know, it's amazing.
[02:03:22.520 --> 02:03:24.360]   Like a hundred different programs.
[02:03:24.360 --> 02:03:27.680]   So you want to narrow it down and say, well, I only want to see four star reviewed.
[02:03:27.680 --> 02:03:34.880]   I want stuff that runs on the web and can manage clients, classes and staff of 10 or
[02:03:34.880 --> 02:03:36.720]   whatever, narrow it down.
[02:03:36.720 --> 02:03:40.320]   It'll find all the software that fits your criteria.
[02:03:40.320 --> 02:03:44.400]   Then you look at it and the capsule reviews and the stars and you check the ones you want
[02:03:44.400 --> 02:03:48.160]   to know more about and you'll make a comparison chart for you that you can see exactly what
[02:03:48.160 --> 02:03:52.480]   all the features are, where it installs, how it installs, everything you need to know.
[02:03:52.480 --> 02:03:57.080]   And then read the reviews, vetted reviews from real people.
[02:03:57.080 --> 02:03:58.480]   I think this is all you need to do.
[02:03:58.480 --> 02:04:01.920]   This is the research that your boss wants you to do or if you're the boss that you want
[02:04:01.920 --> 02:04:05.040]   yourself to do before you try a program.
[02:04:05.040 --> 02:04:06.040]   It's awesome.
[02:04:06.040 --> 02:04:07.040]   Capetera.
[02:04:07.040 --> 02:04:11.760]   It's like a secret weapon for people who need to find business software.
[02:04:11.760 --> 02:04:13.280]   C A P but don't make it.
[02:04:13.280 --> 02:04:14.280]   Don't keep it a secret.
[02:04:14.280 --> 02:04:15.280]   Tell everybody.
[02:04:15.280 --> 02:04:16.280]   Capetera.
[02:04:16.280 --> 02:04:18.640]   C A P T E R R A dot com.
[02:04:18.640 --> 02:04:24.680]   Go to capetera.com/twit and join the millions of people who use it every month to find
[02:04:24.680 --> 02:04:27.840]   the software that will save your business every day.
[02:04:27.840 --> 02:04:30.040]   Capetera.com/twit.
[02:04:30.040 --> 02:04:33.560]   We thank you so much for making this week in tech possible.
[02:04:33.560 --> 02:04:36.920]   I'm back to Becky.
[02:04:36.920 --> 02:04:38.200]   Thank you for filling in for me, Becky.
[02:04:38.200 --> 02:04:39.280]   We'll see you soon.
[02:04:39.280 --> 02:04:40.920]   It is my pleasure, Leo.
[02:04:40.920 --> 02:04:42.720]   And thank you guys for talking to my kid.
[02:04:42.720 --> 02:04:49.640]   That was actually I really benefited from that and in the chat room, Rev Dan L P says,
[02:04:49.640 --> 02:04:57.360]   Robert's explanation of saving us from the robot overlords is like a warm blanket of logic.
[02:04:57.360 --> 02:04:59.000]   A warm blanket of logic.
[02:04:59.000 --> 02:05:01.560]   Oh, I feel all warmed up.
[02:05:01.560 --> 02:05:08.000]   What makes me not feel warm is that as a consumer reporter, I recommend things to people.
[02:05:08.000 --> 02:05:13.080]   All most of us either to our friends or publicly, we recommend apps to people.
[02:05:13.080 --> 02:05:19.440]   And I am an athlete and I've been so positive about getting people to be healthy and fit
[02:05:19.440 --> 02:05:20.440]   and working out.
[02:05:20.440 --> 02:05:25.640]   And I loved an app called PACT or our PACT or Jim PACT.
[02:05:25.640 --> 02:05:28.560]   These are all names that have been associated with it.
[02:05:28.560 --> 02:05:34.680]   And the premise of it was that you pledge that you will do something every day.
[02:05:34.680 --> 02:05:41.400]   You'll either walk 10,000 steps by four o'clock, you will check in at a gym that you've designated.
[02:05:41.400 --> 02:05:45.480]   They evolved the app so that you promise certain eating habits.
[02:05:45.480 --> 02:05:50.360]   You could tie it in with your Fitbit or any of your trackers.
[02:05:50.360 --> 02:05:58.160]   And in your pledge, you said, I will give $10, $5 was the minimum per week, but up to
[02:05:58.160 --> 02:06:00.400]   $50 or $100.
[02:06:00.400 --> 02:06:03.480]   If I don't do it, I lose that money.
[02:06:03.480 --> 02:06:09.280]   So you gave it your credit card and it was going to take the money if you didn't do it.
[02:06:09.280 --> 02:06:18.000]   And then if you did do it, you got a piece of the pie of all that money that people lost
[02:06:18.000 --> 02:06:19.880]   because they didn't keep their PACT.
[02:06:19.880 --> 02:06:22.120]   And they kept the VIG, right?
[02:06:22.120 --> 02:06:24.640]   You wondered what the stake was.
[02:06:24.640 --> 02:06:32.320]   That perhaps was my error in not knowing exactly what the rake was.
[02:06:32.320 --> 02:06:42.080]   Turns out, the PACT app owes nearly a million dollars to people, so wisely says on Gizmodo,
[02:06:42.080 --> 02:06:46.840]   owes nearly $1 million for not paying users to exercise.
[02:06:46.840 --> 02:06:50.240]   Bob, I mean, I'm just as a consumer reporter, I'm horrified.
[02:06:50.240 --> 02:06:51.240]   Can you absolve me?
[02:06:51.240 --> 02:06:54.480]   Can you tell me that this or are you going to tell me that I should have known this was
[02:06:54.480 --> 02:06:57.520]   always going to come and I should not recommend it to our viewers?
[02:06:57.520 --> 02:07:01.680]   I mean, I think the saving grace here is the amount they owe people.
[02:07:01.680 --> 02:07:07.080]   We're talking generally in the $25, $50, $75 range, which is much cheaper than a personal
[02:07:07.080 --> 02:07:08.080]   trainer.
[02:07:08.080 --> 02:07:10.400]   So perhaps it was actually a plus for them in the end.
[02:07:10.400 --> 02:07:11.400]   Okay, okay.
[02:07:11.400 --> 02:07:15.640]   I'm going to take that as partial, absulsion, absolution, absolution.
[02:07:15.640 --> 02:07:17.160]   But I am feeling guilty.
[02:07:17.160 --> 02:07:26.680]   On the other hand, I have tried but not recommended the watch that zaps you with 220 volts of
[02:07:26.680 --> 02:07:28.280]   electricity when you don't.
[02:07:28.280 --> 02:07:29.280]   Pavlov.
[02:07:29.280 --> 02:07:30.280]   Pavlov.
[02:07:30.280 --> 02:07:31.280]   Pavlov.
[02:07:31.280 --> 02:07:32.280]   You're right.
[02:07:32.280 --> 02:07:33.280]   Pavlov watch.
[02:07:33.280 --> 02:07:34.280]   Pavlov.
[02:07:34.280 --> 02:07:35.280]   There's something like that.
[02:07:35.280 --> 02:07:36.280]   Wait, when does it zap you exactly?
[02:07:36.280 --> 02:07:37.680]   So you can tell it.
[02:07:37.680 --> 02:07:40.440]   I need to do 10,000 steps by 4 p.m.
[02:07:40.440 --> 02:07:42.160]   Tell me about Pavlov.
[02:07:42.160 --> 02:07:46.000]   And if it doesn't come, if you don't do it by 4 p.m.
[02:07:46.000 --> 02:07:47.000]   Look at that.
[02:07:47.000 --> 02:07:48.000]   It will zap me.
[02:07:48.000 --> 02:07:49.000]   Unless you have it.
[02:07:49.000 --> 02:07:50.000]   Pavlov watch.
[02:07:50.000 --> 02:07:54.960]   And it hurts like, oh my God.
[02:07:54.960 --> 02:08:00.200]   I had such an interesting experience wearing it because it was like as the day wore on and
[02:08:00.200 --> 02:08:04.080]   I knew I wasn't accomplishing the goal that I had promised I needed to do that I could
[02:08:04.080 --> 02:08:07.120]   feel the watch heavier and heavier on my arm.
[02:08:07.120 --> 02:08:09.360]   Wow, it manipulates gravity.
[02:08:09.360 --> 02:08:10.360]   That is a sophisticated watch.
[02:08:10.360 --> 02:08:12.360]   It was a psychological gravity.
[02:08:12.360 --> 02:08:13.360]   Yeah, yeah.
[02:08:13.360 --> 02:08:14.360]   I'm going to go with it.
[02:08:14.360 --> 02:08:16.360]   It was a version of therapy, they call it.
[02:08:16.360 --> 02:08:17.360]   A version.
[02:08:17.360 --> 02:08:18.360]   The guy who invented it.
[02:08:18.360 --> 02:08:19.360]   Pavlov is the name.
[02:08:19.360 --> 02:08:20.360]   Pavlov.
[02:08:20.360 --> 02:08:21.360]   That's right.
[02:08:21.360 --> 02:08:28.080]   That he created it because he was procrastinating so badly that he posted on Facebook that if
[02:08:28.080 --> 02:08:33.120]   he did not finish his paper that he needed someone to come to his house and slap him.
[02:08:33.120 --> 02:08:35.200]   And he said it really motivated him.
[02:08:35.200 --> 02:08:37.760]   There's like a line of people at his door.
[02:08:37.760 --> 02:08:40.240]   He paid them.
[02:08:40.240 --> 02:08:48.240]   So I had such a great enjoyable time talking with all you three Roberts today.
[02:08:48.240 --> 02:08:49.960]   Thank you for your time.
[02:08:49.960 --> 02:08:51.880]   Let me just go through and ask you guys to tell people.
[02:08:51.880 --> 02:08:54.880]   We're going to start a union.
[02:08:54.880 --> 02:08:56.840]   I always want to go on strike.
[02:08:56.840 --> 02:08:57.840]   The Roberts union.
[02:08:57.840 --> 02:09:02.240]   But you know that those Richard Scabs will come in here and they will take over.
[02:09:02.240 --> 02:09:03.240]   We're better than Richard.
[02:09:03.240 --> 02:09:04.240]   We can take on Richard's.
[02:09:04.240 --> 02:09:06.240]   You're no dicks.
[02:09:06.240 --> 02:09:08.680]   Yeah, that'll be like the NFL scab.
[02:09:08.680 --> 02:09:10.280]   We know what he's going to watch it.
[02:09:10.280 --> 02:09:12.440]   You know, and the NFL strike breakers came in.
[02:09:12.440 --> 02:09:16.680]   Well, let's start with our first Robert Bob Sullivan to tell people where they can find
[02:09:16.680 --> 02:09:19.720]   you and what you're doing and what you're going to be doing.
[02:09:19.720 --> 02:09:20.720]   Sure.
[02:09:20.720 --> 02:09:22.880]   Well, I work for myself now at Bob Sullivan net.
[02:09:22.880 --> 02:09:27.080]   I'm still a correspondent for CNBC and a contributor to the Today Show occasionally.
[02:09:27.080 --> 02:09:30.840]   But the main thing that people want to find me, they can go to Bob Sullivan net and sign
[02:09:30.840 --> 02:09:32.160]   up for my newsletter.
[02:09:32.160 --> 02:09:35.480]   That's the safest way to make sure that you don't miss any of my stories.
[02:09:35.480 --> 02:09:41.040]   I really appreciate all your wisdom and practical advice about the Equifax hack this week, Bob.
[02:09:41.040 --> 02:09:43.800]   So thank you so very, very much.
[02:09:43.800 --> 02:09:44.800]   Rob Reed.
[02:09:44.800 --> 02:09:50.160]   Yes, I am a tech entrepreneur, turned science fiction author.
[02:09:50.160 --> 02:09:54.720]   So I'd like everybody to use Rhapsody, although they've renamed it Napster.
[02:09:54.720 --> 02:09:56.040]   That was a music service I started.
[02:09:56.040 --> 02:09:58.320]   So it became Listen, then Rhapsody.
[02:09:58.320 --> 02:10:01.080]   It was Listen.com was the name of the company I started.
[02:10:01.080 --> 02:10:02.960]   Rhapsody was our product.
[02:10:02.960 --> 02:10:06.760]   Then we sold it to Real Networks, who sold half of it to MTV.
[02:10:06.760 --> 02:10:12.200]   And at some point somebody bought the Napster brand out of whatever purgatory it had gone
[02:10:12.200 --> 02:10:16.080]   into and they glued it on top of Rhapsody about a year ago.
[02:10:16.080 --> 02:10:17.240]   But it's still Rhapsody to me.
[02:10:17.240 --> 02:10:18.680]   I'm a Spotify user now.
[02:10:18.680 --> 02:10:20.280]   I like that.
[02:10:20.280 --> 02:10:23.960]   But these days I write science fiction for Random House and I have a new novel that came
[02:10:23.960 --> 02:10:26.680]   out a month ago called After On.
[02:10:26.680 --> 02:10:32.080]   It's about a diabolical social media company set in present-day San Francisco, which it
[02:10:32.080 --> 02:10:35.400]   will sound like a spoiler, but you really see it coming from the very beginning.
[02:10:35.400 --> 02:10:37.640]   Attains consciousness.
[02:10:37.640 --> 02:10:41.960]   And rather than going all Terminator and trying to kill us all, it becomes something
[02:10:41.960 --> 02:10:46.840]   much more terrifying, which is a hyper-intelligence, super-empowered 14-year-old mean girl.
[02:10:46.840 --> 02:10:49.200]   That terrifies me.
[02:10:49.200 --> 02:10:50.520]   It is terrifying.
[02:10:50.520 --> 02:10:53.120]   And there's a certain amount of playfulness in the book, but it's a very serious rumination
[02:10:53.120 --> 02:10:58.520]   on artificial super-intelligence risk and synthetic biology, promise and peril, nihilistic
[02:10:58.520 --> 02:11:00.640]   terrorism, a bunch of other things.
[02:11:00.640 --> 02:11:05.160]   And as you know, I started a podcast in connection with the book.
[02:11:05.160 --> 02:11:06.160]   So good.
[02:11:06.160 --> 02:11:07.160]   Thank you.
[02:11:07.160 --> 02:11:08.160]   Thank you.
[02:11:08.160 --> 02:11:12.120]   What my original plan was was to do eight episodes that would go deep into the science
[02:11:12.120 --> 02:11:16.000]   and tech connected to the book because you could only go so deep in the novel without
[02:11:16.000 --> 02:11:18.040]   hijacking your story.
[02:11:18.040 --> 02:11:22.160]   And there's an episode on quantum computing with the venture capitalist Steve Gervits
[02:11:22.160 --> 02:11:23.800]   in long interview.
[02:11:23.800 --> 02:11:28.880]   I talked to Maron Gribbitz, who we were talking about who runs meta about augmented reality,
[02:11:28.880 --> 02:11:30.920]   which is a major thing in the book.
[02:11:30.920 --> 02:11:34.640]   I had a two-hour interview with Sam Harris, who's very outspoken about terrorism.
[02:11:34.640 --> 02:11:37.240]   Terrorism is a major subject.
[02:11:37.240 --> 02:11:42.280]   And I just decided actually a few days ago that episode eight goes up on Tuesday.
[02:11:42.280 --> 02:11:43.600]   I've been having so much fun.
[02:11:43.600 --> 02:11:45.840]   I'm going to keep it going indefinitely.
[02:11:45.840 --> 02:11:50.320]   And so it's going to graduate from its loose affiliation with the book.
[02:11:50.320 --> 02:11:52.400]   It's called the After On podcast.
[02:11:52.400 --> 02:11:55.720]   It's going to graduate in a week from its loose affiliation with the book.
[02:11:55.720 --> 02:12:02.520]   And the theme is going to be unhurried conversations with thinkers and founders and scientists,
[02:12:02.520 --> 02:12:03.680]   or maybe founders and thinkers.
[02:12:03.680 --> 02:12:07.200]   I haven't figured out the sequence yet, but it's going to be long interviews.
[02:12:07.200 --> 02:12:09.200]   Really good stuff.
[02:12:09.200 --> 02:12:12.920]   I'm a person who is an intuitar.
[02:12:12.920 --> 02:12:18.280]   And so I intuit after I listen to a tech podcast, I get a feeling like, "Oh, I'm so
[02:12:18.280 --> 02:12:22.840]   glad that I did that because I learned more and I feel knowledgeable and that was unproductive
[02:12:22.840 --> 02:12:24.160]   and blah, blah."
[02:12:24.160 --> 02:12:27.640]   But I get a different feel when I listen to the After On podcast, which is the way I
[02:12:27.640 --> 02:12:30.760]   feel after a sort of a Terry Gross interview with an entertainer.
[02:12:30.760 --> 02:12:35.120]   It's more like, "Wow, that was an amazing journey that I just went on."
[02:12:35.120 --> 02:12:39.600]   And it's a really different take on the genre of tech podcasting.
[02:12:39.600 --> 02:12:41.320]   It's not really tech podcasting.
[02:12:41.320 --> 02:12:43.560]   It's intelligence podcasting.
[02:12:43.560 --> 02:12:45.240]   And I just am really enjoying it.
[02:12:45.240 --> 02:12:46.240]   Thank you.
[02:12:46.240 --> 02:12:49.040]   I am delighted to hear that for something who's been in production as long.
[02:12:49.040 --> 02:12:54.080]   Rip onto your experience and get that on my playlist.
[02:12:54.080 --> 02:12:55.080]   So thank you so much.
[02:12:55.080 --> 02:12:56.080]   Thank you.
[02:12:56.080 --> 02:12:59.680]   Robert Skowbel, you are always doing so many things.
[02:12:59.680 --> 02:13:03.840]   What do you want to talk about that you want people to know is on your radar and that
[02:13:03.840 --> 02:13:05.080]   they need to pay attention to?
[02:13:05.080 --> 02:13:09.960]   Augmented reality is totally on my radar right now because Apple just shipped it last
[02:13:09.960 --> 02:13:14.600]   week and Google is coming with AR Core and a whole bunch of stuff is coming over the
[02:13:14.600 --> 02:13:17.320]   next three, four years.
[02:13:17.320 --> 02:13:19.160]   I write mostly on Facebook.
[02:13:19.160 --> 02:13:23.840]   I wrote a book called The Fourth Transformation about this stuff and we're doing corporate
[02:13:23.840 --> 02:13:26.480]   training coming up soon.
[02:13:26.480 --> 02:13:31.720]   And I'm going around the world and doing a lot of speeches and other kinds of work.
[02:13:31.720 --> 02:13:33.720]   What does corporate training for AR look like?
[02:13:33.720 --> 02:13:39.480]   How we're teaching business users, business decision makers what is coming and how to
[02:13:39.480 --> 02:13:41.400]   get ready for it.
[02:13:41.400 --> 02:13:44.120]   So this is going to affect every brand, right?
[02:13:44.120 --> 02:13:49.000]   You're going to eventually look at an iPhone and a menu is going to pop off that.
[02:13:49.000 --> 02:13:54.920]   I saw that already at I-fluence, which Google bot.
[02:13:54.920 --> 02:13:57.360]   So Google's working on that.
[02:13:57.360 --> 02:14:01.280]   And I bet that's sort of what Amazon's thinking about too because they want you to walk into
[02:14:01.280 --> 02:14:06.320]   a store and start looking at things and they'll let you buy things right by looking at them.
[02:14:06.320 --> 02:14:08.360]   You're such a tastemaker in the industry.
[02:14:08.360 --> 02:14:12.440]   And I just, there was a subliminal thread through the show, which is Google Assistant
[02:14:12.440 --> 02:14:16.920]   and I'm going to go take your advice and I'm going to go try it out.
[02:14:16.920 --> 02:14:21.640]   Keep in mind it's three feet away from my mouth and it understands me even in noisy situations,
[02:14:21.640 --> 02:14:23.520]   which is just amazing.
[02:14:23.520 --> 02:14:28.960]   And when I worked at Microsoft a decade ago, we saw natural language processing and it was
[02:14:28.960 --> 02:14:30.480]   70% accurate.
[02:14:30.480 --> 02:14:33.800]   And today it's fairly accurate now.
[02:14:33.800 --> 02:14:36.400]   How long have you been majoring in it?
[02:14:36.400 --> 02:14:38.040]   You said kind of playfully.
[02:14:38.040 --> 02:14:40.120]   It came out a couple months ago on iPhone.
[02:14:40.120 --> 02:14:41.120]   I've had iOS.
[02:14:41.120 --> 02:14:43.720]   It came out on Pixel last year.
[02:14:43.720 --> 02:14:46.440]   But ever since it came out, I've been forcing myself to use it.
[02:14:46.440 --> 02:14:48.520]   So I've used it for thousands of questions.
[02:14:48.520 --> 02:14:52.600]   And do you feel super empowered by, do you feel like a long-ended person?
[02:14:52.600 --> 02:14:58.280]   Yeah, because you can ask it all sorts of things really, really quickly.
[02:14:58.280 --> 02:15:01.280]   It's the weather.
[02:15:01.280 --> 02:15:03.160]   And it's movies.
[02:15:03.160 --> 02:15:06.800]   And I'll just keep going.
[02:15:06.800 --> 02:15:09.640]   You can ask it all sorts of things.
[02:15:09.640 --> 02:15:13.000]   What did Apple do in the stock market?
[02:15:13.000 --> 02:15:14.000]   Wow.
[02:15:14.000 --> 02:15:20.440]   And you can see it's accurate at seeing my voice, hearing my voice, and it's accurate
[02:15:20.440 --> 02:15:22.080]   at giving you an answer.
[02:15:22.080 --> 02:15:25.840]   This is so comforting because I've said for years that I/O is the revolution and it's
[02:15:25.840 --> 02:15:29.960]   not just the artificial intelligence there, but what you're saying is this the method
[02:15:29.960 --> 02:15:33.680]   of interaction because that was seconds and totally seamless.
[02:15:33.680 --> 02:15:34.680]   Yeah.
[02:15:34.680 --> 02:15:35.680]   So that is so cool.
[02:15:35.680 --> 02:15:39.280]   Yeah, it's pretty impressive as long as you have a connection.
[02:15:39.280 --> 02:15:42.280]   If you're in the middle of Canada like I was, no, it doesn't.
[02:15:42.280 --> 02:15:43.280]   El Bueno.
[02:15:43.280 --> 02:15:46.480]   Don't be trying that at the Calgary Stampede, folks.
[02:15:46.480 --> 02:15:47.480]   All right.
[02:15:47.480 --> 02:15:48.480]   Thank you, Roberts.
[02:15:48.480 --> 02:15:49.480]   Another Twit.
[02:15:49.480 --> 02:15:50.480]   Is it in the can?
[02:15:50.480 --> 02:15:51.480]   It's amazing.
[02:15:51.480 --> 02:16:00.680]   Do it the Twit.

