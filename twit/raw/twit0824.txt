;FFMETADATA1
title=Ask Shatner's Ghost
artist=Leo Laporte, Mike Elgan, Jason Howell, Iain Thomson
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-05-23
track=824
language=English
genre=Podcast
comment=Google IO recap, Windows 10X gets dumped, Bitcoin collapse, Twitter verified
encoded_by=Uniblab 5.2
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.640]   It's time for Twit this week at Tech, a very special Twit.
[00:00:03.640 --> 00:00:09.160]   The last time everybody was in studio was January 2020.
[00:00:09.160 --> 00:00:10.800]   And it was the same two guys.
[00:00:10.800 --> 00:00:12.240]   Mike Elgin's here.
[00:00:12.240 --> 00:00:13.560]   Ian Thompson's here.
[00:00:13.560 --> 00:00:18.080]   We're going to throw Jason Howell into the mix because there's a lot to talk about.
[00:00:18.080 --> 00:00:20.040]   Jason, of course, is an expert on Android.
[00:00:20.040 --> 00:00:23.440]   We'll be talking about Google I/O, all the Google announcements there.
[00:00:23.440 --> 00:00:27.280]   Mike Elgin makes a strong case for firing Sundar Pachai.
[00:00:27.280 --> 00:00:29.360]   I think it might happen sooner than later.
[00:00:29.360 --> 00:00:32.200]   We'll talk about Bitcoin's collapse.
[00:00:32.200 --> 00:00:34.760]   We'll talk about Twitter's blue check.
[00:00:34.760 --> 00:00:36.360]   Now you can get one too.
[00:00:36.360 --> 00:00:42.840]   It's all coming up next with a very special Mezcal fueled this week in Tech.
[00:00:42.840 --> 00:00:48.920]   Podcasts you love from people you trust.
[00:00:48.920 --> 00:00:58.880]   This is Twit.
[00:00:58.880 --> 00:01:01.560]   This is Twit this week in Tech.
[00:01:01.560 --> 00:01:07.360]   Episode 824 recorded Sunday, May 23, 2021.
[00:01:07.360 --> 00:01:09.440]   Ask Shatner's Ghost.
[00:01:09.440 --> 00:01:13.680]   This episode of This Week in Tech is brought to you by Mint Mobile.
[00:01:13.680 --> 00:01:19.800]   Mint Mobile's secret sauce is there the first company to sell wireless service online only
[00:01:19.800 --> 00:01:23.280]   to get your new wireless plan for just 15 bucks a month.
[00:01:23.280 --> 00:01:29.320]   And get the plan shipped to your door for free, go to mintmobile.com/twit.
[00:01:29.320 --> 00:01:31.360]   And by Udacity.
[00:01:31.360 --> 00:01:36.840]   Gain in demand tech skills in as little as three months with Udacity's part time online
[00:01:36.840 --> 00:01:38.160]   tech courses.
[00:01:38.160 --> 00:01:46.800]   Visit Udacity.com/twit and get 75% off any program with the offer code TWIT 75, offer
[00:01:46.800 --> 00:01:50.120]   ends June 30, 2021.
[00:01:50.120 --> 00:01:52.800]   And by ZipRecruiter.
[00:01:52.800 --> 00:01:58.280]   Finding someone to wear many hats or one very specific hat is no easy task.
[00:01:58.280 --> 00:02:03.760]   ZipRecruiter finds people with the right experience for your job and invites them to apply.
[00:02:03.760 --> 00:02:08.440]   Whether that's a civil engineer in New York or a mascot in Missouri.
[00:02:08.440 --> 00:02:13.880]   Try ZipRecruiter free at ziprecruiter.com/twit.
[00:02:13.880 --> 00:02:15.560]   And by Casper.
[00:02:15.560 --> 00:02:20.440]   When it comes to a better night's sleep, Casper's new cooling collection has you covered.
[00:02:20.440 --> 00:02:21.880]   Focus on tomorrow.
[00:02:21.880 --> 00:02:23.880]   Let Casper handle the rest.
[00:02:23.880 --> 00:02:31.280]   Explore Casper products, mattresses, sheets, pillows and more at Casper.com/twit1.
[00:02:31.280 --> 00:02:42.240]   And use the code TWIT1 for $100 off select mattresses.
[00:02:42.240 --> 00:02:43.760]   It's time for Twit This Week at Tech The Show.
[00:02:43.760 --> 00:02:49.200]   We talk about the week's tech news and this is actually a landmark episode of This Week
[00:02:49.200 --> 00:02:57.440]   in Tech because for the first time since February 2020, we are all in one place.
[00:02:57.440 --> 00:02:59.080]   All vaccinated, right?
[00:02:59.080 --> 00:03:00.080]   Yep.
[00:03:00.080 --> 00:03:01.080]   Safe.
[00:03:01.080 --> 00:03:03.800]   We're doing this according to the rules.
[00:03:03.800 --> 00:03:07.440]   But I am thrilled to have Mike Elgin sitting next to me.
[00:03:07.440 --> 00:03:08.640]   Not as well as I am Leo.
[00:03:08.640 --> 00:03:09.640]   This is fantastic.
[00:03:09.640 --> 00:03:10.640]   It's really great to see you.
[00:03:10.640 --> 00:03:11.640]   Thank you for being here.
[00:03:11.640 --> 00:03:12.640]   In real life.
[00:03:12.640 --> 00:03:16.160]   Ian Thompson is also here from the register.
[00:03:16.160 --> 00:03:17.160]   I'm the register.
[00:03:17.160 --> 00:03:19.600]   Look, it's a real Ian Thompson.
[00:03:19.600 --> 00:03:24.000]   And by the way, I should mention, Mike seems to have brought some spirits, so we'll be
[00:03:24.000 --> 00:03:26.360]   celebrating the occasion.
[00:03:26.360 --> 00:03:27.680]   Distrational, why not?
[00:03:27.680 --> 00:03:30.840]   And we figured we could get Jason Howell to come in.
[00:03:30.840 --> 00:03:33.800]   You've already been in once before for the Google I/O event.
[00:03:33.800 --> 00:03:37.200]   Yeah, that was kind of our first kind of step in the beginning.
[00:03:37.200 --> 00:03:38.200]   With the beginning.
[00:03:38.200 --> 00:03:41.600]   Like, what is it like to sit at the table with another person again?
[00:03:41.600 --> 00:03:44.000]   So that was for the special for the I/O.
[00:03:44.000 --> 00:03:44.520]   We--
[00:03:44.520 --> 00:03:46.760]   He's behind the camera, but you can't see him.
[00:03:46.760 --> 00:03:51.840]   But Michael O'Donnell is also here, our official photographer at photo.
[00:03:51.840 --> 00:03:55.800]   Come on, step into the limelight briefly there at photo.
[00:03:55.800 --> 00:03:56.800]   There we are.
[00:03:56.800 --> 00:03:57.800]   There it is.
[00:03:57.800 --> 00:04:03.720]   I've known Michael for more than 21 years now, 22 years, when he started coming to the
[00:04:03.720 --> 00:04:06.360]   screensavers in San Francisco.
[00:04:06.360 --> 00:04:07.360]   Taking pictures.
[00:04:07.360 --> 00:04:09.280]   We're in a pro at the time.
[00:04:09.280 --> 00:04:10.920]   You are now a very accomplished pro.
[00:04:10.920 --> 00:04:13.720]   You're a very Silicon Valley's official photographer, I think.
[00:04:13.720 --> 00:04:16.840]   So when Michael heard that we were all going to be in studio, he said, "Can I come?"
[00:04:16.840 --> 00:04:18.280]   And I said, "You betcha."
[00:04:18.280 --> 00:04:19.280]   Amazing.
[00:04:19.280 --> 00:04:20.280]   You betcha.
[00:04:20.280 --> 00:04:25.040]   So this is already posting on Twitter @photo.
[00:04:25.040 --> 00:04:26.760]   He's already-- nice picture of you, Jason.
[00:04:26.760 --> 00:04:27.760]   I like that.
[00:04:27.760 --> 00:04:28.760]   Hey, thank you.
[00:04:28.760 --> 00:04:29.760]   Look at that.
[00:04:29.760 --> 00:04:30.760]   Look at that.
[00:04:30.760 --> 00:04:31.760]   Look at that.
[00:04:31.760 --> 00:04:32.760]   Yeah, but I'll look at it.
[00:04:32.760 --> 00:04:33.760]   Yeah.
[00:04:33.760 --> 00:04:34.760]   Oh, there it is.
[00:04:34.760 --> 00:04:35.760]   And that is a--
[00:04:35.760 --> 00:04:36.760]   You're good.
[00:04:36.760 --> 00:04:37.760]   He is a pro.
[00:04:37.760 --> 00:04:38.760]   I'm not the old faithful geyser.
[00:04:38.760 --> 00:04:39.920]   Is that part of it?
[00:04:39.920 --> 00:04:41.680]   Are you calling me a geyser?
[00:04:41.680 --> 00:04:44.200]   All right, I do spew every 20 minutes.
[00:04:44.200 --> 00:04:51.640]   All right, ladies and gentlemen, before we get into the show, I think we should celebrate
[00:04:51.640 --> 00:04:52.640]   the reunion.
[00:04:52.640 --> 00:04:53.640]   Look at that one of you.
[00:04:53.640 --> 00:04:54.640]   This is one of his.
[00:04:54.640 --> 00:04:55.640]   Oh, man.
[00:04:55.640 --> 00:04:56.640]   Oh, that's hysterical.
[00:04:56.640 --> 00:04:57.640]   I don't know.
[00:04:57.640 --> 00:04:58.640]   Yeah, we can't really show it.
[00:04:58.640 --> 00:05:00.640]   Is that today or is that previous?
[00:05:00.640 --> 00:05:02.480]   That was posted up this previous.
[00:05:02.480 --> 00:05:04.320]   Yes, that was--
[00:05:04.320 --> 00:05:06.720]   So Mike, as you know, we talk about this all the time.
[00:05:06.720 --> 00:05:10.480]   Mike does these gastronomad adventures all over the world.
[00:05:10.480 --> 00:05:12.680]   You're going to be doing Provence next month.
[00:05:12.680 --> 00:05:13.680]   That's right.
[00:05:13.680 --> 00:05:14.680]   That's the first time--
[00:05:14.680 --> 00:05:15.680]   And that's the first one since the pandemic.
[00:05:15.680 --> 00:05:16.680]   Wow.
[00:05:16.680 --> 00:05:20.720]   That's the first time since Mexico City in January, I believe, of 2020.
[00:05:20.720 --> 00:05:21.720]   Oh, my goodness.
[00:05:21.720 --> 00:05:22.720]   That's got to be a hardship.
[00:05:22.720 --> 00:05:25.720]   I mean, I know travel companies all over the world are struggling.
[00:05:25.720 --> 00:05:26.720]   Yes.
[00:05:26.720 --> 00:05:27.720]   That's got to have been very tough.
[00:05:27.720 --> 00:05:29.440]   Very tough for everyone involved.
[00:05:29.440 --> 00:05:30.440]   Yeah.
[00:05:30.440 --> 00:05:34.240]   But-- and we're going, Lisa and I are going to Oaxaca in October.
[00:05:34.240 --> 00:05:35.240]   I can't wait.
[00:05:35.240 --> 00:05:36.240]   So excited.
[00:05:36.240 --> 00:05:40.440]   All sold out, but very excited that you're going to be going
[00:05:40.440 --> 00:05:42.000]   and Lisa's going to be going.
[00:05:42.000 --> 00:05:43.040]   You guys are going to love it.
[00:05:43.040 --> 00:05:45.040]   So you brought--
[00:05:45.040 --> 00:05:47.120]   and it's perfect because we should toast, I think,
[00:05:47.120 --> 00:05:49.000]   the return to Studio.
[00:05:49.000 --> 00:05:51.800]   You brought along the product of Oaxaca,
[00:05:51.800 --> 00:05:52.320]   which is--
[00:05:52.320 --> 00:05:52.320]   That's right.
[00:05:52.320 --> 00:05:53.640]   --they're famous for Mezcal.
[00:05:53.640 --> 00:05:54.000]   That's right.
[00:05:54.000 --> 00:05:58.320]   90% of the world's Mezcal-- Mexico's Mezcal-- is made in Oaxaca.
[00:05:58.320 --> 00:06:03.360]   The whole Oaxaca Valley is paved with agave plants,
[00:06:03.360 --> 00:06:04.800]   from which they make this stuff.
[00:06:04.800 --> 00:06:07.120]   And so this is a barrel-aged and really fancy.
[00:06:07.120 --> 00:06:08.080]   Agave is a cactus.
[00:06:08.080 --> 00:06:09.200]   Is that what they get aloe from?
[00:06:09.200 --> 00:06:10.400]   Is agave-- no.
[00:06:10.400 --> 00:06:11.400]   I don't know.
[00:06:11.400 --> 00:06:13.320]   They must be related, right?
[00:06:13.320 --> 00:06:15.400]   All right, I'm going to pass this down.
[00:06:15.400 --> 00:06:15.920]   OK.
[00:06:15.920 --> 00:06:17.920]   Anybody who wants can have--
[00:06:17.920 --> 00:06:21.120]   You've seen a shot glass before, haven't you, Ian?
[00:06:21.120 --> 00:06:22.360]   More than a few times.
[00:06:22.360 --> 00:06:24.120]   It's a little small for a pint glass,
[00:06:24.120 --> 00:06:26.400]   but it's unfried in particular because we've
[00:06:26.400 --> 00:06:27.120]   wanted to make it up.
[00:06:27.120 --> 00:06:29.120]   But yes.
[00:06:29.120 --> 00:06:33.880]   So I think a toast to a return to normal, the new normal.
[00:06:33.880 --> 00:06:38.000]   Yes, and thanks to Mike Elgin and the people of Oaxaca
[00:06:38.000 --> 00:06:39.880]   for our beverage.
[00:06:39.880 --> 00:06:40.480]   Thank you.
[00:06:40.480 --> 00:06:41.560]   A little long grill arm, guys.
[00:06:41.560 --> 00:06:43.320]   What do you say in Mexico?
[00:06:43.320 --> 00:06:43.840]   Salud.
[00:06:43.840 --> 00:06:44.840]   Salud.
[00:06:44.840 --> 00:06:45.240]   Salud.
[00:06:45.240 --> 00:06:46.400]   All right.
[00:06:46.400 --> 00:06:48.400]   And I'm sorry we don't have enough for the whole.
[00:06:48.400 --> 00:06:51.400]   I was going to say, we're not doing Dan and Warren, are we?
[00:06:51.400 --> 00:06:54.960]   We don't have enough for the whole chat room, but--
[00:06:54.960 --> 00:06:56.720]   Oh, that's really nice.
[00:06:56.720 --> 00:06:57.160]   That's nice.
[00:06:57.160 --> 00:06:59.840]   So you actually brought up the Gastronomad page.
[00:06:59.840 --> 00:07:02.480]   That is the guy who makes this stuff,
[00:07:02.480 --> 00:07:04.120]   on that picture that was there.
[00:07:04.120 --> 00:07:04.640]   Really?
[00:07:04.640 --> 00:07:06.360]   Yeah, and what's amazing about this
[00:07:06.360 --> 00:07:09.320]   is that they let the plant come through.
[00:07:09.320 --> 00:07:10.520]   It's not smokey.
[00:07:10.520 --> 00:07:13.000]   No, I was just going to say, in Mexico,
[00:07:13.000 --> 00:07:16.960]   I appreciate the mescal industry.
[00:07:16.960 --> 00:07:20.040]   When I was working as a waiter,
[00:07:20.040 --> 00:07:23.040]   had an introduction to a lot of different types of mescal.
[00:07:23.040 --> 00:07:24.520]   But what I always got in the way for me
[00:07:24.520 --> 00:07:26.200]   was the intense smokiness.
[00:07:26.200 --> 00:07:27.200]   This is not a smokey though.
[00:07:27.200 --> 00:07:28.200]   This doesn't have that smokey.
[00:07:28.200 --> 00:07:28.760]   Not at all.
[00:07:28.760 --> 00:07:29.240]   It has zero.
[00:07:29.240 --> 00:07:32.280]   It's cooked in steam ovens.
[00:07:32.280 --> 00:07:34.920]   And then they use wood fire to distill it.
[00:07:34.920 --> 00:07:37.280]   It doesn't taste like tequila, even though it really doesn't.
[00:07:37.280 --> 00:07:37.880]   Now it is.
[00:07:37.880 --> 00:07:38.320]   What is the deal?
[00:07:38.320 --> 00:07:40.720]   Tequila is a mescal type of mescal.
[00:07:40.720 --> 00:07:42.960]   It tastes almost scotchy.
[00:07:42.960 --> 00:07:44.520]   Yeah, exactly.
[00:07:44.520 --> 00:07:48.160]   It's probably a lot to do with the barrels.
[00:07:48.160 --> 00:07:49.760]   They age in American oak.
[00:07:49.760 --> 00:07:54.080]   But yeah, the variety is insane.
[00:07:54.080 --> 00:07:56.280]   The amount of different flavors they have in mescal.
[00:07:56.280 --> 00:07:57.600]   So it's just a wonderful beverage.
[00:07:57.600 --> 00:08:00.160]   Tequila's all made from one type of agave.
[00:08:00.160 --> 00:08:01.560]   Mescal's made from 50.
[00:08:01.560 --> 00:08:02.080]   Wow.
[00:08:02.080 --> 00:08:05.720]   So it speaks volumes about humanity's desire
[00:08:05.720 --> 00:08:08.680]   to get off its tree that they can get taken on a goviplan
[00:08:08.680 --> 00:08:10.840]   that I can turn into booze.
[00:08:10.840 --> 00:08:13.600]   Basically, I think early humans fermented everything.
[00:08:13.600 --> 00:08:14.760]   Right, I was going to say everything.
[00:08:14.760 --> 00:08:15.600]   Give it a shot.
[00:08:15.600 --> 00:08:16.440]   Give it a shot.
[00:08:16.440 --> 00:08:17.440]   Turned into booze.
[00:08:17.440 --> 00:08:17.960]   That's what happened.
[00:08:17.960 --> 00:08:20.120]   So I'm going to go out and play woolly mama tomorrow.
[00:08:20.120 --> 00:08:22.680]   Just give me a drink somehow.
[00:08:22.680 --> 00:08:27.080]   The indigenous Mexicans have been making polke,
[00:08:27.080 --> 00:08:31.160]   which is fermented agave sap for 10,000 years or something
[00:08:31.160 --> 00:08:32.160]   like that.
[00:08:32.160 --> 00:08:33.680]   And it was part of their ceremonies.
[00:08:33.680 --> 00:08:36.360]   Monda Zuma, of course, was really into it.
[00:08:36.360 --> 00:08:37.960]   And it's just been around forever.
[00:08:37.960 --> 00:08:40.720]   So you just still polke and you get mescal?
[00:08:40.720 --> 00:08:41.880]   More or less.
[00:08:41.880 --> 00:08:44.320]   It's more like a wine process, actually.
[00:08:44.320 --> 00:08:46.560]   They ferment the-- they cook the agave
[00:08:46.560 --> 00:08:49.280]   and then they ferment it in vats.
[00:08:49.280 --> 00:08:51.960]   And then they take the liquid and they distill that twice
[00:08:51.960 --> 00:08:52.840]   or three times.
[00:08:52.840 --> 00:08:53.960]   It's actually delicious.
[00:08:53.960 --> 00:08:54.960]   It is.
[00:08:54.960 --> 00:08:55.960]   It's super good.
[00:08:55.960 --> 00:08:58.120]   I'm not a huge tequila fan, but that is fantastic.
[00:08:58.120 --> 00:08:59.120]   Yeah.
[00:08:59.120 --> 00:09:01.840]   If you drank this, not knowing what it was,
[00:09:01.840 --> 00:09:03.520]   like if I drank this, not knowing what it was,
[00:09:03.520 --> 00:09:04.600]   I would not have had to jump to you.
[00:09:04.600 --> 00:09:05.600]   It's almost like you're a huge tequila.
[00:09:05.600 --> 00:09:07.400]   So I think it's a hard time.
[00:09:07.400 --> 00:09:08.320]   I don't know.
[00:09:08.320 --> 00:09:10.480]   I mean, tequila is usually like you drink it
[00:09:10.480 --> 00:09:11.960]   because you're drunk.
[00:09:11.960 --> 00:09:15.080]   But you could sip that quite happily for an evening.
[00:09:15.080 --> 00:09:16.000]   And I am drunk, don't it?
[00:09:16.000 --> 00:09:17.760]   So it happened that fast.
[00:09:17.760 --> 00:09:19.520]   It's not me.
[00:09:19.520 --> 00:09:21.040]   Beginning of the show.
[00:09:21.040 --> 00:09:22.480]   Let's get some tequila.
[00:09:22.480 --> 00:09:27.880]   I wish we'd had this on Tuesday when Google did its Google I/O.
[00:09:27.880 --> 00:09:28.880]   10 AM.
[00:09:28.880 --> 00:09:29.880]   10 AM.
[00:09:29.880 --> 00:09:30.880]   It would be not.
[00:09:30.880 --> 00:09:33.000]   It was a little supper to begin with.
[00:09:33.000 --> 00:09:37.560]   I was so hopeful that Google would announce a Pixel 6 phone
[00:09:37.560 --> 00:09:38.840]   or at least talk about because they're
[00:09:38.840 --> 00:09:41.800]   doing a system on a chip that's unique and different.
[00:09:41.800 --> 00:09:43.160]   I was hoping they would have something
[00:09:43.160 --> 00:09:45.720]   to say about Android 12.
[00:09:45.720 --> 00:09:47.480]   It was the most tepid.
[00:09:47.480 --> 00:09:50.200]   And I brought it up again something
[00:09:50.200 --> 00:09:52.520]   you said the last time you were here, which is that--
[00:09:52.520 --> 00:09:54.200]   and I hate to say it, but I think you might be right.
[00:09:54.200 --> 00:09:57.280]   Sundurpichai is a terrible CEO.
[00:09:57.280 --> 00:10:00.640]   He is because especially for a company like Google,
[00:10:00.640 --> 00:10:04.520]   where you have so many brilliant technologists coming up
[00:10:04.520 --> 00:10:06.760]   with all these different things, what you need is a forceful
[00:10:06.760 --> 00:10:09.640]   leader who can forge it all into a single vision going
[00:10:09.640 --> 00:10:11.120]   in a single direction.
[00:10:11.120 --> 00:10:14.160]   And Google just seems to-- he can't seem to heard the cats.
[00:10:14.160 --> 00:10:16.040]   Everybody's going in different directions.
[00:10:16.040 --> 00:10:18.880]   They do brilliant technology.
[00:10:18.880 --> 00:10:22.800]   And then one of the things that makes it very clear--
[00:10:22.800 --> 00:10:24.760]   it's unfair to compare people to Steve Jobs, right?
[00:10:24.760 --> 00:10:27.240]   Because Steve Jobs was a one and only type of person.
[00:10:27.240 --> 00:10:29.760]   He may not want another Steve Jobs to be on the internet.
[00:10:29.760 --> 00:10:31.800]   When you saw Steve Jobs presentation,
[00:10:31.800 --> 00:10:34.280]   you knew what the vision was and where they were going
[00:10:34.280 --> 00:10:35.040]   and what they were doing.
[00:10:35.040 --> 00:10:35.720]   And he led it there.
[00:10:35.720 --> 00:10:37.480]   With this, you have all this great technology.
[00:10:37.480 --> 00:10:39.360]   And I'm sure we'll get into the details
[00:10:39.360 --> 00:10:41.320]   of multiple times when this happened,
[00:10:41.320 --> 00:10:43.400]   where they have something great going on,
[00:10:43.400 --> 00:10:45.400]   but then the way they presented is like, oh,
[00:10:45.400 --> 00:10:46.520]   that's not the way to present it.
[00:10:46.520 --> 00:10:48.240]   You mean like talking to Pluto?
[00:10:48.240 --> 00:10:49.880]   Plock, that was horrible.
[00:10:49.880 --> 00:10:52.000]   That was just people hate the technology.
[00:10:52.000 --> 00:10:54.040]   It also buried the lead in effect,
[00:10:54.040 --> 00:10:57.120]   because it wasn't clear that what was happening here
[00:10:57.120 --> 00:11:01.960]   was somewhat spontaneously, an artificial intelligence
[00:11:01.960 --> 00:11:04.360]   was having a conversation, understanding context and doing
[00:11:04.360 --> 00:11:04.800]   that.
[00:11:04.800 --> 00:11:06.080]   It could have been great, but I don't
[00:11:06.080 --> 00:11:09.000]   want to know how a paper airplane feels.
[00:11:09.000 --> 00:11:10.960]   It's just a horrible example.
[00:11:10.960 --> 00:11:12.720]   How could they come up with an example?
[00:11:12.720 --> 00:11:15.440]   Also, they were talking about this stuff five years ago.
[00:11:15.440 --> 00:11:17.760]   I mean, I've tweeted out a picture
[00:11:17.760 --> 00:11:19.880]   when they first had it in the amphitheater.
[00:11:19.880 --> 00:11:23.440]   And they were doing exactly the same thing five years ago.
[00:11:23.440 --> 00:11:25.120]   And maybe it's got slightly better,
[00:11:25.120 --> 00:11:26.760]   but this is hardly inspiring.
[00:11:26.760 --> 00:11:27.760]   I like your example.
[00:11:27.760 --> 00:11:30.880]   I think he may be the bomber for Google.
[00:11:30.880 --> 00:11:31.160]   Yeah.
[00:11:31.160 --> 00:11:33.600]   And we're talking about Lambda, I believe it's called.
[00:11:33.600 --> 00:11:34.200]   And this is--
[00:11:34.200 --> 00:11:36.280]   L-A-M-D-A, yes.
[00:11:36.280 --> 00:11:40.560]   And it's a new speech engine way for artificial intelligence
[00:11:40.560 --> 00:11:42.840]   to understand sentence structure and syntax
[00:11:42.840 --> 00:11:43.640]   and all that kind of stuff.
[00:11:43.640 --> 00:11:46.880]   Brilliant technology, again, brilliant technology.
[00:11:46.880 --> 00:11:48.720]   But what they're failing to do is
[00:11:48.720 --> 00:11:52.480]   to realize that I don't think it's a good idea,
[00:11:52.480 --> 00:11:58.160]   nor do I think it's desirable for voice AI
[00:11:58.160 --> 00:12:00.880]   to be making jokes and sort of like--
[00:12:00.880 --> 00:12:03.280]   make-- say things that are not of any use,
[00:12:03.280 --> 00:12:05.200]   but make them sound more human.
[00:12:05.200 --> 00:12:08.000]   I want them to have-- to speak in a very human way,
[00:12:08.000 --> 00:12:09.840]   to understand what I say, to speak back to me
[00:12:09.840 --> 00:12:11.400]   in a way that's very natural.
[00:12:11.400 --> 00:12:14.160]   But give me the information and shut that F up.
[00:12:14.160 --> 00:12:16.160]   I don't want to hear like dad jokes
[00:12:16.160 --> 00:12:18.480]   and weird comments that--
[00:12:18.480 --> 00:12:19.480]   The words that were taught out--
[00:12:19.480 --> 00:12:22.520]   They deliberately put pauses into things
[00:12:22.520 --> 00:12:24.160]   to make it seem more human.
[00:12:24.160 --> 00:12:25.560]   And it's just like, hang on, you're supposed
[00:12:25.560 --> 00:12:27.160]   to be efficient, for goodness sake.
[00:12:27.160 --> 00:12:29.200]   Do the job, but don't try and pretend you're something
[00:12:29.200 --> 00:12:31.000]   you're not-- get the job done.
[00:12:31.000 --> 00:12:33.040]   Well, five to 10 years ago with this sort of stuff,
[00:12:33.040 --> 00:12:35.920]   it was so robotic that it was on the other side of things.
[00:12:35.920 --> 00:12:38.960]   Like, I can't concentrate on this amazing tool
[00:12:38.960 --> 00:12:40.840]   that I have the potential of using
[00:12:40.840 --> 00:12:43.000]   because I feel like I'm talking to a robot.
[00:12:43.000 --> 00:12:45.600]   So it's almost like they took that really to heart.
[00:12:45.600 --> 00:12:47.880]   They're like, well, let's give you a human then.
[00:12:47.880 --> 00:12:49.640]   And in so doing it, I don't know.
[00:12:49.640 --> 00:12:51.280]   It does kind of break the experience.
[00:12:51.280 --> 00:12:52.720]   I completely agree with you, Mike.
[00:12:52.720 --> 00:12:55.640]   I feel like when they're setting up this sort of technology,
[00:12:55.640 --> 00:12:58.120]   all that kept running through my mind is like,
[00:12:58.120 --> 00:13:02.080]   some time in the future when we have the Jetsons robot running
[00:13:02.080 --> 00:13:03.960]   around, it's like they're getting us ready
[00:13:03.960 --> 00:13:06.320]   or they think they're getting us ready for the future where
[00:13:06.320 --> 00:13:08.760]   we have these robots that we're talking to as if they're humans.
[00:13:08.760 --> 00:13:11.520]   And I don't know that I want to talk to a robot as if it's human.
[00:13:11.520 --> 00:13:14.560]   I want it to understand the thing that I'm asking it
[00:13:14.560 --> 00:13:15.520]   and give me the answer.
[00:13:15.520 --> 00:13:17.160]   More importantly, how old are you kids?
[00:13:17.160 --> 00:13:18.160]   How old are you kids?
[00:13:18.160 --> 00:13:19.160]   11 and 8.
[00:13:19.160 --> 00:13:21.160]   OK, so they're both girls?
[00:13:21.160 --> 00:13:22.160]   Yes.
[00:13:22.160 --> 00:13:26.160]   These girls are growing up in a world where AI is just sort of there.
[00:13:26.160 --> 00:13:27.160]   It's always been there.
[00:13:27.160 --> 00:13:31.160]   It wasn't there when we were kids, but for them, it's normal and natural.
[00:13:31.160 --> 00:13:35.160]   The worst possible thing is that they think that the natural language
[00:13:35.160 --> 00:13:41.160]   speech of these technologies means that the thing that they're talking to is human,
[00:13:41.160 --> 00:13:48.160]   has a spirit, has deserves respect, deserves to be obeyed to a certain extent.
[00:13:48.160 --> 00:13:49.680]   No, it's a toaster.
[00:13:49.680 --> 00:13:53.160]   When it's broken, we're going to throw it in a landfill and we're going to disassemble
[00:13:53.160 --> 00:13:54.160]   it and recycle it.
[00:13:54.160 --> 00:13:55.160]   It's like the refrigerator.
[00:13:55.160 --> 00:13:56.160]   It has no...
[00:13:56.160 --> 00:13:58.160]   This is what your son is trying to teach people.
[00:13:58.160 --> 00:13:59.160]   Exactly.
[00:13:59.160 --> 00:14:00.160]   Exactly.
[00:14:00.160 --> 00:14:05.560]   But there is a greater moral hazard and it ties back to something that they didn't mention
[00:14:05.560 --> 00:14:08.160]   there, the firing of two AI ethics researchers.
[00:14:08.160 --> 00:14:09.160]   Oh, yeah.
[00:14:09.160 --> 00:14:12.160]   The Harvard, Mitchell, and Tim Nick Gebru.
[00:14:12.160 --> 00:14:15.600]   And their papers, "Tochastic Perrets," when we were doing this on this week in Google
[00:14:15.600 --> 00:14:17.160]   on Wednesday, Kevin Marks was on it.
[00:14:17.160 --> 00:14:24.520]   He brought this paper up and he said, "One of the reasons he believes Google fired these
[00:14:24.520 --> 00:14:30.160]   two women is this paper basically said, 'This kind of research is very problematic.'"
[00:14:30.160 --> 00:14:32.160]   And it's kind of what you were saying, Mike Elgin.
[00:14:32.160 --> 00:14:35.160]   I'll read you a little bit from the paper.
[00:14:35.160 --> 00:14:39.720]   The past few years ever since processing capacity caught up with neural models have been heady
[00:14:39.720 --> 00:14:42.920]   times in the world of natural language processing.
[00:14:42.920 --> 00:14:43.920]   Exactly.
[00:14:43.920 --> 00:14:48.400]   As you said, neural approaches in general and large, transformer LMs, learning models,
[00:14:48.400 --> 00:14:54.280]   that's what GPT is and that's what this lambda is a successor to GPT, in particular,
[00:14:54.280 --> 00:15:00.560]   rapidly overtaken the leaderboards, wide variety of benchmarks, etc., etc.
[00:15:00.560 --> 00:15:10.160]   The problem that they associate with this is that humans assume a certain amount of authority
[00:15:10.160 --> 00:15:11.160]   in these responses.
[00:15:11.160 --> 00:15:15.880]   When you ask Google Assistant or even Alexa or Siri, when you ask them something and they
[00:15:15.880 --> 00:15:21.000]   come back with an answer, you assume there is an authority, that it's a correct answer.
[00:15:21.000 --> 00:15:26.040]   And they said, "This is a huge hazard for this stuff.
[00:15:26.040 --> 00:15:30.400]   I'll give you their more academic language."
[00:15:30.400 --> 00:15:35.200]   But I think we've identified a wide variety of costs and risks associated with a rush
[00:15:35.200 --> 00:15:39.400]   for ever larger learning models, including, and she mentioned environmental costs.
[00:15:39.400 --> 00:15:45.960]   Apparently, Google was mad at her for that, financial cost, opportunity cost.
[00:15:45.960 --> 00:15:49.400]   But there is also an issue.
[00:15:49.400 --> 00:15:53.800]   Let me see if I can find the exact sentence here.
[00:15:53.800 --> 00:16:00.280]   The risk of substantial harms, including stereotyping, denigration, increases in extremist ideology,
[00:16:00.280 --> 00:16:07.560]   wrongful arrest, should humans encounter, and they had pointed to an arrest based on
[00:16:07.560 --> 00:16:13.240]   face recognition where the officers assumed, or no, I'm sorry, based on automatic translation,
[00:16:13.240 --> 00:16:18.080]   where the officers assumed that the translation was accurate, it wasn't.
[00:16:18.080 --> 00:16:23.320]   Should humans encounter seemingly coherent LM output and take it for the words of some
[00:16:23.320 --> 00:16:28.400]   person or organization was accountability for what is said.
[00:16:28.400 --> 00:16:30.000]   There is no accountability.
[00:16:30.000 --> 00:16:34.480]   In fact, Google or anybody who's doing this would say, "Oh, no, well, that's the machine."
[00:16:34.480 --> 00:16:36.640]   And Google kind of referred to this at Google.
[00:16:36.640 --> 00:16:37.640]   What did he say?
[00:16:37.640 --> 00:16:41.680]   He gave an example of that the planet Pluto wanted to play basketball with you.
[00:16:41.680 --> 00:16:44.040]   I can't remember what it was exactly.
[00:16:44.040 --> 00:16:45.040]   He gave something dumb.
[00:16:45.040 --> 00:16:46.040]   A dumb example of dumb.
[00:16:46.040 --> 00:16:48.440]   We don't always get this right.
[00:16:48.440 --> 00:16:49.440]   Right.
[00:16:49.440 --> 00:16:51.640]   But really, there's a much larger hazard to this.
[00:16:51.640 --> 00:16:52.640]   We've already seen this.
[00:16:52.640 --> 00:16:59.320]   I mean, we're now at the point where the first internet generation, the people who, not those
[00:16:59.320 --> 00:17:04.560]   of us who were around before, but the people who grew up and the internet was always there.
[00:17:04.560 --> 00:17:06.880]   And they listened to it and they believe it.
[00:17:06.880 --> 00:17:10.840]   And in the last 20 years, we've seen what that brings.
[00:17:10.840 --> 00:17:16.120]   If we want to throw AI on autonomous bots into the thing, we're talking about really
[00:17:16.120 --> 00:17:19.240]   pushing that to the next, accelerating it to the next level.
[00:17:19.240 --> 00:17:21.400]   And that's deeply worrying.
[00:17:21.400 --> 00:17:26.120]   The authors of this paper, two from Google and one university researcher, are really,
[00:17:26.120 --> 00:17:27.480]   I mean, I haven't read the whole thing.
[00:17:27.480 --> 00:17:32.400]   Emily Cander and Angelina McMillan Major from University of Washington and the two Google
[00:17:32.400 --> 00:17:35.960]   AI researchers, Tim Nick Gevreu and Margaret Mitchell.
[00:17:35.960 --> 00:17:36.960]   But they're clear.
[00:17:36.960 --> 00:17:42.000]   I haven't read the whole thing, so I can't say that they are right with a capital R.
[00:17:42.000 --> 00:17:44.840]   But I can say that they're definitely onto something and they probably--
[00:17:44.840 --> 00:17:46.440]   And I take it to stuff, Google.
[00:17:46.440 --> 00:17:50.160]   They probably should have been put in charge instead of fired because this is really--
[00:17:50.160 --> 00:17:51.160]   Yeah, no question.
[00:17:51.160 --> 00:17:52.160]   That's kind of the final sentence.
[00:17:52.160 --> 00:17:58.400]   We call on the field to recognize that applications that aim to believably mimic humans bring risk
[00:17:58.400 --> 00:18:00.840]   of extreme harms.
[00:18:00.840 --> 00:18:05.520]   Working on synthetic human behavior is a bright line in ethical AI development where downstream
[00:18:05.520 --> 00:18:09.520]   effects need to be understood and modeled in order to block foreseeable harm to society
[00:18:09.520 --> 00:18:12.520]   and different social groups.
[00:18:12.520 --> 00:18:14.520]   This is exactly what you're talking about.
[00:18:14.520 --> 00:18:15.520]   They're working on.
[00:18:15.520 --> 00:18:16.520]   They're right on, yeah.
[00:18:16.520 --> 00:18:23.400]   What is it about Google when faced with this where they feel threatened and what happens
[00:18:23.400 --> 00:18:24.400]   happens to them?
[00:18:24.400 --> 00:18:27.440]   Because in the face of this, they fire these people and then they go on stage at Google
[00:18:27.440 --> 00:18:28.920]   I/O and they present--
[00:18:28.920 --> 00:18:29.920]   Right.
[00:18:29.920 --> 00:18:32.920]   They say, "But you could talk to Pluto."
[00:18:32.920 --> 00:18:33.920]   That's right.
[00:18:33.920 --> 00:18:34.920]   Yeah.
[00:18:34.920 --> 00:18:36.160]   Which is so tone-deaf.
[00:18:36.160 --> 00:18:41.840]   We must always remember that CEOs are not at liberty to speak the truth.
[00:18:41.840 --> 00:18:43.000]   They're not allowed to.
[00:18:43.000 --> 00:18:50.320]   They have to say what truth is in the interest of the company and paying lip service to diversity
[00:18:50.320 --> 00:18:55.480]   and inclusion and all that kind of stuff is what the Google needs to be said.
[00:18:55.480 --> 00:18:57.920]   But the truth is that there are all kinds of problems.
[00:18:57.920 --> 00:19:00.600]   There are all kinds of moral issues, ethical issues.
[00:19:00.600 --> 00:19:02.560]   Again, especially for children.
[00:19:02.560 --> 00:19:05.360]   Children are going to grow up in a world where everything is fake.
[00:19:05.360 --> 00:19:07.280]   Fake Instagram celebrities.
[00:19:07.280 --> 00:19:09.560]   Deep fake everything.
[00:19:09.560 --> 00:19:10.560]   There's another example of fake.
[00:19:10.560 --> 00:19:11.560]   A person talking to you.
[00:19:11.560 --> 00:19:13.800]   It's like fake person is what they're trying to create here.
[00:19:13.800 --> 00:19:15.800]   Google is not going to give up on this.
[00:19:15.800 --> 00:19:21.560]   As we saw with the power struggle that's going on with DeepMind this week, Google is
[00:19:21.560 --> 00:19:22.640]   not giving up on this.
[00:19:22.640 --> 00:19:25.480]   It recognizes there are financial advantages to it.
[00:19:25.480 --> 00:19:28.080]   It fits perfectly with that business model.
[00:19:28.080 --> 00:19:29.080]   They're all in.
[00:19:29.080 --> 00:19:32.240]   What's weird is that they did the right thing in the early days with assistance because
[00:19:32.240 --> 00:19:35.320]   they didn't make it too chatty, like Siri.
[00:19:35.320 --> 00:19:41.200]   Siri does those tired old comments that are just so annoying.
[00:19:41.200 --> 00:19:44.360]   The assistant was one of the few that was very neutral.
[00:19:44.360 --> 00:19:48.120]   I thought they had the right idea there.
[00:19:48.120 --> 00:19:55.520]   Google in their blog post in which they call Lambda a breakthrough conversation technology.
[00:19:55.520 --> 00:20:01.560]   The very last paragraph, they acknowledge it says responsibility first.
[00:20:01.560 --> 00:20:06.080]   Our highest priority when creating technologies like Lambda is working to ensure we minimize
[00:20:06.080 --> 00:20:11.080]   the risks of internalizing biases, mirroring hateful speech, replicating misleading information.
[00:20:11.080 --> 00:20:16.760]   They practically stole the paragraph from stochastic parrots, the original paper.
[00:20:16.760 --> 00:20:24.600]   At the presentation, it was more gee whiz, isn't this cool?
[00:20:24.600 --> 00:20:27.480]   When I watch this whole thing, and we'll talk a little bit about some of the other things
[00:20:27.480 --> 00:20:30.360]   because of course I'm sure you have something to say about Android 12.
[00:20:30.360 --> 00:20:32.880]   It's pretty material you as nice.
[00:20:32.880 --> 00:20:34.880]   They're pretty sure they love a compared to a lot of stuff.
[00:20:34.880 --> 00:20:37.320]   We're talking about now.
[00:20:37.320 --> 00:20:44.600]   I get the feeling that sooner Pichai, and you have to include the CFO Ruth Porat, who is
[00:20:44.600 --> 00:20:46.600]   the axe at Google.
[00:20:46.600 --> 00:20:50.200]   Someone who has to sit through far too many Google financials.
[00:20:50.200 --> 00:20:54.560]   She's obviously highly skilled but has all the charisma of a road accident.
[00:20:54.560 --> 00:20:56.000]   I'm watching sooner Pichai.
[00:20:56.000 --> 00:20:57.080]   I feel like he is also...
[00:20:57.080 --> 00:20:58.640]   I've found the same about the dark.
[00:20:58.640 --> 00:21:00.880]   He's a small man in this thing.
[00:21:00.880 --> 00:21:03.000]   He's not what needs to be.
[00:21:03.000 --> 00:21:09.360]   I really fear that what's happened at Google is they're looking at the great financial results
[00:21:09.360 --> 00:21:13.520]   they get from search and advertising against search.
[00:21:13.520 --> 00:21:16.120]   That really powers 90% of what they do.
[00:21:16.120 --> 00:21:17.120]   It's the most...
[00:21:17.120 --> 00:21:18.120]   E.C. 4%.
[00:21:18.120 --> 00:21:19.120]   94%.
[00:21:19.120 --> 00:21:21.080]   They're just the rest of it.
[00:21:21.080 --> 00:21:22.080]   They're just...
[00:21:22.080 --> 00:21:24.240]   It's on phoning it in.
[00:21:24.240 --> 00:21:26.200]   It's like, "Well, we got to advertise it."
[00:21:26.200 --> 00:21:29.120]   We've really felt the phoning it in for the past few years.
[00:21:29.120 --> 00:21:32.120]   That leads to some of the frustration going into Google I/O.
[00:21:32.120 --> 00:21:33.480]   I thought maybe E.C.
[00:21:33.480 --> 00:21:36.760]   Pichai is saying at the earnings call, "Something significant," whatever.
[00:21:36.760 --> 00:21:37.800]   Then we see what we saw.
[00:21:37.800 --> 00:21:45.160]   That whole first hour was Google showing off a little bit but also subdued about it.
[00:21:45.160 --> 00:21:48.840]   It was hard to get excited about the things they were getting excited about.
[00:21:48.840 --> 00:21:55.000]   Part of that I felt came from the energy that Sundar brought to this super low energy.
[00:21:55.000 --> 00:21:56.000]   It's like, "Come on, man.
[00:21:56.000 --> 00:21:59.480]   Get excited about the things you have going on so that I can get excited about it."
[00:21:59.480 --> 00:22:03.720]   Again, it's snatching a defeat from the jaws of victory.
[00:22:03.720 --> 00:22:07.800]   This technology is brilliant, but here we are crapping all over it.
[00:22:07.800 --> 00:22:08.800]   Why?
[00:22:08.800 --> 00:22:13.560]   Because Sundar Pichai talking to a planet is a little bit like Clint Eastwood talking to
[00:22:13.560 --> 00:22:14.560]   a chair.
[00:22:14.560 --> 00:22:15.560]   It's so spotty.
[00:22:15.560 --> 00:22:16.560]   You're just like, "Oh."
[00:22:16.560 --> 00:22:17.560]   Instead of going, "Wow, you're just freaking out."
[00:22:17.560 --> 00:22:18.560]   Well, and maybe that's Mr. X.
[00:22:18.560 --> 00:22:26.960]   I mean, maybe they're smart enough to say, "Well, we really don't want to talk about the real
[00:22:26.960 --> 00:22:28.560]   ethical issues here."
[00:22:28.560 --> 00:22:30.880]   Let's talk to a paper airplane and see how everybody else.
[00:22:30.880 --> 00:22:31.880]   Let's be fluffy about that.
[00:22:31.880 --> 00:22:33.760]   They did show some interesting stuff.
[00:22:33.760 --> 00:22:36.960]   We're going to get to some of those things.
[00:22:36.960 --> 00:22:42.880]   The number one thing I heard from the outside world was, "Oh, that Starline video conferencing
[00:22:42.880 --> 00:22:44.520]   thing was cool.
[00:22:44.520 --> 00:22:46.160]   This is light field projection.
[00:22:46.160 --> 00:22:50.160]   So it looks like you're almost as long as you don't move around too much."
[00:22:50.160 --> 00:22:55.880]   I'm flushing back to Silicon Valley and the hologram suite there.
[00:22:55.880 --> 00:22:56.880]   It's pretty much the same.
[00:22:56.880 --> 00:22:59.760]   I mean, they've been working on this for decades now.
[00:22:59.760 --> 00:23:02.480]   They still can't get a workable system out of it.
[00:23:02.480 --> 00:23:05.320]   I can only imagine what it costs, too.
[00:23:05.320 --> 00:23:06.640]   They have five or six of them.
[00:23:06.640 --> 00:23:09.280]   They've been showing them other people.
[00:23:09.280 --> 00:23:10.640]   That is a non-trivial thing.
[00:23:10.640 --> 00:23:12.320]   So my prediction for how...
[00:23:12.320 --> 00:23:17.920]   So there are certain parts of this that are 100% going to happen and parts of it that
[00:23:17.920 --> 00:23:18.920]   aren't necessarily.
[00:23:18.920 --> 00:23:23.320]   The part that's going to happen is the rendering of a sort of a deep fake version of the person
[00:23:23.320 --> 00:23:28.240]   in real time with everything they do and say is going to be reflected by the other person
[00:23:28.240 --> 00:23:29.560]   and you'll be able to make eye contact.
[00:23:29.560 --> 00:23:31.600]   Eye contact is absolutely everything.
[00:23:31.600 --> 00:23:32.600]   Absolutely.
[00:23:32.600 --> 00:23:37.040]   Everybody's like Zoom fatigue is caused primarily by who's looking at whom.
[00:23:37.040 --> 00:23:41.120]   When you do a Zoom call with 12 people, 12 people are looking at you for two hours.
[00:23:41.120 --> 00:23:42.120]   That's not good.
[00:23:42.120 --> 00:23:47.200]   No matter who's speaking, it's weird, our brains panic and it's Zoom fatigue.
[00:23:47.200 --> 00:23:48.200]   We need to be able to...
[00:23:48.200 --> 00:23:51.520]   When somebody else is talking, you need to see other people looking at the person who's
[00:23:51.520 --> 00:23:52.520]   talking.
[00:23:52.520 --> 00:23:53.720]   They're not looking at you anymore.
[00:23:53.720 --> 00:23:54.720]   All that kind of stuff.
[00:23:54.720 --> 00:23:58.800]   Apple is working on something called Bionic Virtual Meeting Room which is going to take
[00:23:58.800 --> 00:24:05.200]   place with all the devices but especially their upcoming augmented reality glasses where
[00:24:05.200 --> 00:24:14.360]   they'll do emojis instead of a lifelike 3D person which again increases the price to $20,000
[00:24:14.360 --> 00:24:16.360]   and brings it down to something simple.
[00:24:16.360 --> 00:24:21.520]   I think it's meetings are going to involve emojis or avatars of some kind that do the
[00:24:21.520 --> 00:24:27.360]   same thing that does but with holographic displays of the kind that we'll get with apple
[00:24:27.360 --> 00:24:31.160]   glasses or augmented reality glasses, we'll be able to...
[00:24:31.160 --> 00:24:35.560]   We could all be like the three of you, it could be holograms and when Jason's talking,
[00:24:35.560 --> 00:24:39.560]   we all look at Jason and I can make contact with Jason and then when Leo talks, then we
[00:24:39.560 --> 00:24:45.960]   all look at Leo and that will be psychologically satisfying and cheap, cheaper, not like this.
[00:24:45.960 --> 00:24:47.840]   It's a great technology but I don't see it ever happening.
[00:24:47.840 --> 00:24:52.520]   This is the only 3D technology that doesn't require glasses or goggles or visors because
[00:24:52.520 --> 00:24:54.360]   it's called Lightfield Projection.
[00:24:54.360 --> 00:24:58.600]   I actually saw it at CES in early 2020.
[00:24:58.600 --> 00:25:01.760]   A number of big companies have been showing it.
[00:25:01.760 --> 00:25:04.360]   It wasn't that impressive.
[00:25:04.360 --> 00:25:07.440]   You have to kind of stay in the same line of sight for it to work.
[00:25:07.440 --> 00:25:11.880]   In fact, Lauren Gutt, a wired who actually got the demo of it, said that herself is,
[00:25:11.880 --> 00:25:17.240]   if you move too much, you lose all the illusion.
[00:25:17.240 --> 00:25:24.840]   I would also imagine almost everything Google showed at I/O, it's lab level.
[00:25:24.840 --> 00:25:29.760]   It was a lot of that first hour it felt like.
[00:25:29.760 --> 00:25:30.760]   This was the problem.
[00:25:30.760 --> 00:25:34.120]   Everything they talked about was old or future.
[00:25:34.120 --> 00:25:36.440]   What have you done for me now?
[00:25:36.440 --> 00:25:40.000]   Well, there were two things they announced kind of now.
[00:25:40.000 --> 00:25:41.680]   Material you.
[00:25:41.680 --> 00:25:47.480]   I immediately put the public beta of Android 12 on my Pixel.
[00:25:47.480 --> 00:25:50.360]   I could say how you find it because I looked at that presentation.
[00:25:50.360 --> 00:25:53.920]   I was like, I don't want that anywhere near my phone.
[00:25:53.920 --> 00:25:54.920]   It's okay.
[00:25:54.920 --> 00:25:55.920]   Yeah.
[00:25:55.920 --> 00:26:02.360]   Mind you, what you get on Android 12 beta 1 is like very early.
[00:26:02.360 --> 00:26:06.960]   A lot of material you was not even implemented if you're putting a wall on your background.
[00:26:06.960 --> 00:26:07.960]   I'm not getting that yet.
[00:26:07.960 --> 00:26:11.920]   I'm doing any of the coloration and kind of shape shifting that's supposedly happening
[00:26:11.920 --> 00:26:12.920]   with material you.
[00:26:12.920 --> 00:26:19.080]   I'm just excited because at this point, it's been so many versions since Google seemed
[00:26:19.080 --> 00:26:22.560]   like it really gave a damn about changing this sort of thing.
[00:26:22.560 --> 00:26:25.760]   So seeing any sort of energy there has me at least.
[00:26:25.760 --> 00:26:26.760]   But Ty is pretty good.
[00:26:26.760 --> 00:26:28.280]   He's the designer who did material.
[00:26:28.280 --> 00:26:30.200]   Yeah, but yeah, Matias Duarte is awesome.
[00:26:30.200 --> 00:26:33.080]   So he came back and he's doing material you.
[00:26:33.080 --> 00:26:34.320]   It'll be nice.
[00:26:34.320 --> 00:26:38.840]   It is, it's really interesting how this is just fashion.
[00:26:38.840 --> 00:26:42.080]   Just the same way Levi's and wide ties come and go.
[00:26:42.080 --> 00:26:48.040]   You know, now we're back to flat, you know, color, no more skeuomorphism.
[00:26:48.040 --> 00:26:52.400]   That's what material design did in material use is the next step to those bright colors.
[00:26:52.400 --> 00:26:56.760]   And you might as well put daisies on it and stuff and next year it'll be something else.
[00:26:56.760 --> 00:26:59.080]   For this point, it doesn't change anything.
[00:26:59.080 --> 00:27:00.080]   It's just design.
[00:27:00.080 --> 00:27:04.280]   Yeah, I mean, for those of us for a certain age, I still hanker after the days of command
[00:27:04.280 --> 00:27:06.800]   online because it was just so clean.
[00:27:06.800 --> 00:27:08.480]   I still use the command line.
[00:27:08.480 --> 00:27:09.480]   Exactly.
[00:27:09.480 --> 00:27:12.080]   And you know, someone changes their UI.
[00:27:12.080 --> 00:27:14.400]   I find it very difficult to get excited about that.
[00:27:14.400 --> 00:27:15.400]   Yeah.
[00:27:15.400 --> 00:27:19.160]   I mean, if there was something going on under the under the hood, as it were, that made
[00:27:19.160 --> 00:27:22.600]   this a better system, I could get genuinely excited about it.
[00:27:22.600 --> 00:27:23.920]   But just pretty pictures.
[00:27:23.920 --> 00:27:27.080]   It's just like, this is the best you come up with.
[00:27:27.080 --> 00:27:29.280]   You know, it it it underwhelmed.
[00:27:29.280 --> 00:27:30.280]   Yeah.
[00:27:30.280 --> 00:27:35.600]   It reminds me I'm looking at in the 70s.
[00:27:35.600 --> 00:27:40.720]   So in the 60s, the hippies brought back, you know, flower power and their pictures of flowers.
[00:27:40.720 --> 00:27:48.600]   But it had migrated by the time the 70s came along, it had migrated to, you know, the
[00:27:48.600 --> 00:27:52.240]   stickies you put on your bathtub to keep from slipping of daisies.
[00:27:52.240 --> 00:27:53.240]   Yes.
[00:27:53.240 --> 00:27:58.120]   And it was the same kind of devalued, you know, feeling.
[00:27:58.120 --> 00:28:00.520]   And by the way, it looks a lot like material you.
[00:28:00.520 --> 00:28:02.280]   I hate to say it.
[00:28:02.280 --> 00:28:04.720]   But I feel like it's old as new.
[00:28:04.720 --> 00:28:07.000]   But that's what fashion and design does.
[00:28:07.000 --> 00:28:11.440]   And so I guess I'm looking for technological improvements.
[00:28:11.440 --> 00:28:15.200]   Google made the camera and the pixel the absolute best camera ever.
[00:28:15.200 --> 00:28:16.200]   And then they coasted from it.
[00:28:16.200 --> 00:28:18.360]   And then they totally coasted three years.
[00:28:18.360 --> 00:28:20.640]   Super curious to see if that changes this year.
[00:28:20.640 --> 00:28:23.320]   Yeah, I guess this wasn't the announcement for the pixels.
[00:28:23.320 --> 00:28:25.480]   So there was nothing to say about it.
[00:28:25.480 --> 00:28:26.480]   Right.
[00:28:26.480 --> 00:28:29.400]   And they announced Android 12 sort of.
[00:28:29.400 --> 00:28:33.280]   They had an as weird, creepy new features and photos that I thought.
[00:28:33.280 --> 00:28:36.840]   So speaking of deep fake, right, taking two images.
[00:28:36.840 --> 00:28:40.680]   So if you've got like a burst or, you know, the example they gave and I'm super familiar
[00:28:40.680 --> 00:28:44.680]   with this having kids, but I'm always taking like four or five shots to make sure I get
[00:28:44.680 --> 00:28:45.960]   the right one.
[00:28:45.960 --> 00:28:51.680]   And they're basically saying photos will take two of those shots and inter like basically
[00:28:51.680 --> 00:28:53.440]   create the frames in between them.
[00:28:53.440 --> 00:28:55.280]   It's a more this animation.
[00:28:55.280 --> 00:28:56.280]   We've seen this before.
[00:28:56.280 --> 00:28:58.920]   It's morphing and it's creepy as sin.
[00:28:58.920 --> 00:29:04.120]   I think we should also recognize that it takes up quite a lot of memory and Google is
[00:29:04.120 --> 00:29:07.320]   very keen on people to start paying for their memory allocation.
[00:29:07.320 --> 00:29:08.840]   Oh, yeah, that's something like this.
[00:29:08.840 --> 00:29:10.160]   They make no mention of that.
[00:29:10.160 --> 00:29:11.160]   Did they?
[00:29:11.160 --> 00:29:13.160]   I don't think so that June 15 is going to start paying.
[00:29:13.160 --> 00:29:15.000]   Oh, I don't think they mentioned that at all.
[00:29:15.000 --> 00:29:16.000]   No, amazing.
[00:29:16.000 --> 00:29:19.600]   You know, it's just like, so yeah, you're constantly getting Google photos like I made
[00:29:19.600 --> 00:29:20.760]   this photo book for you.
[00:29:20.760 --> 00:29:21.760]   I think we made this.
[00:29:21.760 --> 00:29:22.760]   We just like to say we got this collage.
[00:29:22.760 --> 00:29:23.760]   We might say.
[00:29:23.760 --> 00:29:26.360]   Well, oh, by the way, you're reaching your memory limit.
[00:29:26.360 --> 00:29:29.680]   Yeah, we're making it really hard to delete images on mass.
[00:29:29.680 --> 00:29:31.720]   That's only a dollar ninety nine a month.
[00:29:31.720 --> 00:29:34.280]   Yeah, a lot of dark patterns on Apple too.
[00:29:34.280 --> 00:29:38.680]   Like I have two annoying little red things on my on my settings in Apple that says you
[00:29:38.680 --> 00:29:40.440]   need to finish setting up your phone.
[00:29:40.440 --> 00:29:44.560]   One is give us money for iCloud and the other one's give us money through microtransactions
[00:29:44.560 --> 00:29:46.040]   by the windows of OneDrive.
[00:29:46.040 --> 00:29:47.680]   And we're so gonna go to the security center.
[00:29:47.680 --> 00:29:49.520]   It's just like you have an enabled OneDrive.
[00:29:49.520 --> 00:29:51.000]   Yes, there's a very good reason for that.
[00:29:51.000 --> 00:29:55.040]   By the way, is one of the reasons I've kind of used Linux more and more is because it's
[00:29:55.040 --> 00:29:57.400]   not castring you to buy up spot.
[00:29:57.400 --> 00:29:58.400]   There's no more.
[00:29:58.400 --> 00:29:59.400]   There's no upsell in it.
[00:29:59.400 --> 00:30:00.400]   Yeah.
[00:30:00.400 --> 00:30:01.400]   And I'm dreading the day.
[00:30:01.400 --> 00:30:04.760]   I start saying you just installed, you know, pop OS.
[00:30:04.760 --> 00:30:05.960]   Would you like to buy a computer?
[00:30:05.960 --> 00:30:07.760]   No, I just dreading this.
[00:30:07.760 --> 00:30:09.480]   But they added this new feature.
[00:30:09.480 --> 00:30:11.680]   They call it.
[00:30:11.680 --> 00:30:12.680]   What is it?
[00:30:12.680 --> 00:30:14.040]   Memories is part of memories part of memories.
[00:30:14.040 --> 00:30:15.040]   What is it?
[00:30:15.040 --> 00:30:16.040]   What is it?
[00:30:16.040 --> 00:30:17.040]   Personalized memories.
[00:30:17.040 --> 00:30:18.040]   There we go.
[00:30:18.040 --> 00:30:19.040]   It will notice, for instance, they use the example.
[00:30:19.040 --> 00:30:23.000]   There was an orange backpack in 153 of your photos from the last 10 years.
[00:30:23.000 --> 00:30:25.840]   So we've made a montage of orange backpacks.
[00:30:25.840 --> 00:30:28.320]   I fear what kinds of things it's gonna notice in my face.
[00:30:28.320 --> 00:30:32.280]   I've already had a really rough time with this because this wasn't Google.
[00:30:32.280 --> 00:30:36.360]   This was Facebook and then Google chimed in afterwards.
[00:30:36.360 --> 00:30:39.400]   But my mentor died about eight years ago.
[00:30:39.400 --> 00:30:40.400]   And it's gonna pull up pictures.
[00:30:40.400 --> 00:30:43.400]   And it was just like, remember where you were eight years ago?
[00:30:43.400 --> 00:30:45.160]   It's like, yeah, I was crying.
[00:30:45.160 --> 00:30:46.880]   I was out of the back of a church.
[00:30:46.880 --> 00:30:47.880]   Thank you very much.
[00:30:47.880 --> 00:30:49.160]   You'll love this new feature.
[00:30:49.160 --> 00:30:55.160]   It allows you to take Xs and former spouses and pass away mentors out of your images.
[00:30:55.160 --> 00:30:56.160]   They actually have a new feature.
[00:30:56.160 --> 00:30:58.720]   That's a fact you've got to edit that out.
[00:30:58.720 --> 00:31:00.600]   You know, it's just like...
[00:31:00.600 --> 00:31:03.880]   I think the one thing that I'm really glad they're adding is a locked folder feature
[00:31:03.880 --> 00:31:07.440]   where you have photos that you say no one else can see.
[00:31:07.440 --> 00:31:09.160]   It's passcode protected.
[00:31:09.160 --> 00:31:11.920]   They weren't completely clear on whether Google sees it.
[00:31:11.920 --> 00:31:12.920]   Yeah, right.
[00:31:12.920 --> 00:31:14.720]   Google might see it.
[00:31:14.720 --> 00:31:15.720]   That's right.
[00:31:15.720 --> 00:31:16.720]   But no one else.
[00:31:16.720 --> 00:31:17.720]   That's right.
[00:31:17.720 --> 00:31:19.760]   And what are people keeping secret and why?
[00:31:19.760 --> 00:31:25.280]   But this goes back to my previous theme of Google having fantastic technology and then
[00:31:25.280 --> 00:31:27.800]   presenting in a way that really misses the mark.
[00:31:27.800 --> 00:31:33.440]   So for example, they have this new feature that fills in between two still photos.
[00:31:33.440 --> 00:31:36.920]   That is actually a new photo medium that could be very, very cool.
[00:31:36.920 --> 00:31:42.200]   But they show it as a way to have their AI create pictures of your children that never
[00:31:42.200 --> 00:31:43.200]   existed.
[00:31:43.200 --> 00:31:44.200]   Your children.
[00:31:44.200 --> 00:31:51.880]   The fact that they're suggesting this is something to do with children is a problematic.
[00:31:51.880 --> 00:31:56.040]   They show their artsy, farty person saying, "I created this new kind of thing that looks
[00:31:56.040 --> 00:31:58.040]   eerie in a unique way."
[00:31:58.040 --> 00:31:59.840]   Look at this unique way.
[00:31:59.840 --> 00:32:00.840]   They're morphs.
[00:32:00.840 --> 00:32:04.800]   And I always thought morphs are kind of creepy where they're morphing between two photos.
[00:32:04.800 --> 00:32:06.120]   They're adding the frames.
[00:32:06.120 --> 00:32:07.120]   You're right.
[00:32:07.120 --> 00:32:09.440]   I think there's a place for it that's not your kids' photos.
[00:32:09.440 --> 00:32:11.200]   And the other one is the backpack.
[00:32:11.200 --> 00:32:12.560]   It's like, it's cool technology.
[00:32:12.560 --> 00:32:17.320]   It's like a backpack themed album.
[00:32:17.320 --> 00:32:19.680]   Like, why would I want a bad artist to come up with a better example?
[00:32:19.680 --> 00:32:20.680]   You owe the better example.
[00:32:20.680 --> 00:32:21.680]   You owe the better example.
[00:32:21.680 --> 00:32:24.040]   This orange backpack from REI 3995.
[00:32:24.040 --> 00:32:25.120]   Would you like to buy a new one?
[00:32:25.120 --> 00:32:27.120]   Or in the case of Ian, look at all the caskets.
[00:32:27.120 --> 00:32:28.120]   We did all the caskets.
[00:32:28.120 --> 00:32:29.120]   We did all the caskets.
[00:32:29.120 --> 00:32:30.120]   We got the funerals together.
[00:32:30.120 --> 00:32:31.120]   Oh, God.
[00:32:31.120 --> 00:32:32.120]   I'm sorry.
[00:32:32.120 --> 00:32:33.120]   I didn't invite him.
[00:32:33.120 --> 00:32:34.120]   No, no.
[00:32:34.120 --> 00:32:36.200]   But it's that kind of thing.
[00:32:36.200 --> 00:32:41.000]   You know, obviously things that are commented on or shared around a lot.
[00:32:41.000 --> 00:32:45.800]   Yeah, I can understand they want to remind you of it, but it's just, it's mindless.
[00:32:45.800 --> 00:32:49.920]   If there's a theme here, it's that AI has unintended consequences.
[00:32:49.920 --> 00:32:50.920]   Yes.
[00:32:50.920 --> 00:32:57.760]   It shouldn't be just kind of let to run free with our memories because you might have results.
[00:32:57.760 --> 00:32:59.200]   You don't fully appreciate it.
[00:32:59.200 --> 00:33:02.080]   I don't want to make this the Slam Google show.
[00:33:02.080 --> 00:33:04.560]   Was there anything they brought where back?
[00:33:04.560 --> 00:33:05.560]   Yeah.
[00:33:05.560 --> 00:33:06.560]   That show's called Twig.
[00:33:06.560 --> 00:33:07.560]   Yeah.
[00:33:07.560 --> 00:33:10.960]   We have a show for that.
[00:33:10.960 --> 00:33:14.040]   They brought, you know what?
[00:33:14.040 --> 00:33:17.800]   It's a little bit of a victory for them because Samsung, which was creating its own operating
[00:33:17.800 --> 00:33:21.680]   system, Tizen, what's your sense of this?
[00:33:21.680 --> 00:33:26.200]   When you talked about this in all about Android, Jason, was it that they've given up on Tizen?
[00:33:26.200 --> 00:33:27.200]   Yeah.
[00:33:27.200 --> 00:33:30.640]   My understanding is that Samsung, at least as far as their wearables is concerned, is
[00:33:30.640 --> 00:33:33.360]   saying, "Tizen, no more Google."
[00:33:33.360 --> 00:33:37.800]   Like you were right on this week in Google, you made the comment that this is really about
[00:33:37.800 --> 00:33:38.800]   where OS.
[00:33:38.800 --> 00:33:40.280]   This has nothing to do with Tizen.
[00:33:40.280 --> 00:33:43.240]   They're saying we're merging Tizen with your OS.
[00:33:43.240 --> 00:33:44.600]   We're merging efforts, though, right?
[00:33:44.600 --> 00:33:46.040]   It's still Wear OS.
[00:33:46.040 --> 00:33:50.640]   It's just Samsung, to my understanding, is working with Google to bring the things that
[00:33:50.640 --> 00:33:54.160]   worked with Tizen over onto Wear OS.
[00:33:54.160 --> 00:33:56.640]   They're creating a combined product as a result.
[00:33:56.640 --> 00:34:00.800]   It was my sense that they had to do that because the real chief feature of this new
[00:34:00.800 --> 00:34:04.280]   Wear, and it's not Wear OS, it's just Wear.
[00:34:04.280 --> 00:34:05.280]   Which is actually--
[00:34:05.280 --> 00:34:06.280]   Wear your down.
[00:34:06.280 --> 00:34:08.640]   I don't know what that necessarily is because even they see the new--
[00:34:08.640 --> 00:34:09.640]   Wear and tear.
[00:34:09.640 --> 00:34:11.760]   In some places, it's still Wear OS.
[00:34:11.760 --> 00:34:14.280]   Some places is Wear, so I don't even know what the milli-hand is there.
[00:34:14.280 --> 00:34:15.440]   I don't know either.
[00:34:15.440 --> 00:34:18.080]   But they have worn me down, clearly.
[00:34:18.080 --> 00:34:22.680]   But they said it's going to run Play Store apps.
[00:34:22.680 --> 00:34:24.800]   So that means it's really Wear OS.
[00:34:24.800 --> 00:34:25.800]   Absolutely.
[00:34:25.800 --> 00:34:29.000]   And that's the big part of the deal for Samsung, right?
[00:34:29.000 --> 00:34:30.000]   Like their watches.
[00:34:30.000 --> 00:34:31.000]   That's a capitulation that's saying--
[00:34:31.000 --> 00:34:32.400]   Never had access to the Wear--
[00:34:32.400 --> 00:34:33.400]   The Play Store.
[00:34:33.400 --> 00:34:34.400]   The Play Store.
[00:34:34.400 --> 00:34:39.200]   So therefore, they didn't have the wide supply of apps that other Wear OS devices had, even
[00:34:39.200 --> 00:34:40.200]   I know one was really using those.
[00:34:40.200 --> 00:34:45.480]   There is a rumor, it's pure rumor, that Samsung will be doing the chip in these new Wear watches
[00:34:45.480 --> 00:34:47.280]   instead of Qualcomm.
[00:34:47.280 --> 00:34:49.200]   Which maybe was the tit for tat.
[00:34:49.200 --> 00:34:50.200]   Okay, if you guys--
[00:34:50.200 --> 00:34:51.200]   Yeah, maybe so.
[00:34:51.200 --> 00:34:53.920]   If you guys will start using Wear, we'll start using your chips.
[00:34:53.920 --> 00:34:54.920]   I don't--
[00:34:54.920 --> 00:34:58.480]   To me, the chip that stands out is the bargaining chip that this turned out to be for Samsung.
[00:34:58.480 --> 00:35:01.240]   I mean, basically, they went off on their own.
[00:35:01.240 --> 00:35:05.200]   They slapped Google by saying, "We're not going to support your OS.
[00:35:05.200 --> 00:35:06.520]   We're going to do our own."
[00:35:06.520 --> 00:35:08.480]   They did it for a while.
[00:35:08.480 --> 00:35:09.480]   It had problems.
[00:35:09.480 --> 00:35:15.560]   So to a certain extent, this is a face-saving measure for Samsung and also a bargaining chip
[00:35:15.560 --> 00:35:18.600]   where they can say, "Okay, well, we'll cancel this.
[00:35:18.600 --> 00:35:22.160]   We'll go back to where we were, except are you going to do this list of things we want
[00:35:22.160 --> 00:35:25.520]   you to do, and you're also going to be a customer for our chips maybe?"
[00:35:25.520 --> 00:35:29.520]   But the point is that Samsung is getting a lot more than they would have gotten had
[00:35:29.520 --> 00:35:32.400]   they just gone along and never launched Tizen.
[00:35:32.400 --> 00:35:34.440]   You're right, Tizen was a good bargaining chip.
[00:35:34.440 --> 00:35:35.440]   That's it.
[00:35:35.440 --> 00:35:36.440]   I don't know if that was the intent.
[00:35:36.440 --> 00:35:37.440]   Yeah.
[00:35:37.440 --> 00:35:38.440]   I think it was probably--
[00:35:38.440 --> 00:35:39.440]   Exactly.
[00:35:39.440 --> 00:35:41.720]   I don't think it was the intent, but it turned out to be probably--
[00:35:41.720 --> 00:35:42.720]   Well, it was.
[00:35:42.720 --> 00:35:43.720]   It was.
[00:35:43.720 --> 00:35:47.120]   They were scared of the reaction, Google's Android dominance.
[00:35:47.120 --> 00:35:52.000]   So whether they thought Tizen would go on, they needed some leverage against that.
[00:35:52.000 --> 00:35:56.000]   It was weird because Tizen was the inferior operating system, but as Jason has often said
[00:35:56.000 --> 00:36:02.360]   on his other podcast, they generally have the best watches outside of the Apple Watch
[00:36:02.360 --> 00:36:03.360]   platform.
[00:36:03.360 --> 00:36:04.920]   So way ahead of it.
[00:36:04.920 --> 00:36:12.600]   It's a win for Google to get them back on board for Wear OS, but no doubt they gave up a
[00:36:12.600 --> 00:36:15.360]   lot more than they would have given up had they just gone along the whole time.
[00:36:15.360 --> 00:36:19.960]   The other thing that I want to point out is that especially for a tiny operating system
[00:36:19.960 --> 00:36:26.960]   like this, piling in all the stuff is not the way to win in that space.
[00:36:26.960 --> 00:36:29.800]   More features isn't better on a smartwatch.
[00:36:29.800 --> 00:36:31.640]   Better features is better.
[00:36:31.640 --> 00:36:32.640]   Better prioritization.
[00:36:32.640 --> 00:36:33.640]   Better design.
[00:36:33.640 --> 00:36:34.640]   So you have multiple cooks in the kitchen.
[00:36:34.640 --> 00:36:41.840]   You have several people, several organizations trying Fitbit, the Fitbit philosophy, Fitbit
[00:36:41.840 --> 00:36:44.920]   functionality, features, et cetera.
[00:36:44.920 --> 00:36:50.520]   Now all of it has to be represented in-- the reason Apple Watch is so great is that this
[00:36:50.520 --> 00:36:51.520]   is a singular vision.
[00:36:51.520 --> 00:36:53.760]   It's very clear about what's happening.
[00:36:53.760 --> 00:36:57.920]   You don't have multiple cooks in the kitchen, and so it's actually a really good experience.
[00:36:57.920 --> 00:37:04.840]   So I'm not certain that the number of features they've promised to cram into this thing is
[00:37:04.840 --> 00:37:08.320]   going to ruin it, but I suspect it might.
[00:37:08.320 --> 00:37:14.600]   Yeah, I'm also curious to see the effect of their change on this new effort compared
[00:37:14.600 --> 00:37:16.360]   to Wear OS.
[00:37:16.360 --> 00:37:20.800]   Wear OS similar to Android Auto, it's one UI.
[00:37:20.800 --> 00:37:25.880]   It's not Samsung's one UI, of course, but it's a singular UI that everybody has.
[00:37:25.880 --> 00:37:29.040]   If you've got an Android Auto device, it's got the same UI.
[00:37:29.040 --> 00:37:31.240]   If you've got Wear OS devices, they're all the same.
[00:37:31.240 --> 00:37:33.160]   The hardware is the only differentiator.
[00:37:33.160 --> 00:37:35.400]   Now they're opening that up.
[00:37:35.400 --> 00:37:38.440]   So I'm curious to see what kind of impact that has.
[00:37:38.440 --> 00:37:44.080]   That has me a little concerned because, hello, fragmentation once again, or the potential
[00:37:44.080 --> 00:37:49.120]   of that, because any of that customization ends up meaning that maybe these watches aren't
[00:37:49.120 --> 00:37:55.920]   updated as well as they would be if everything was locked into the same kind of direction
[00:37:55.920 --> 00:37:56.920]   as far as that's concerned.
[00:37:56.920 --> 00:38:02.640]   Maybe I'm coming from an Apple-centric point of view, but I also feel like Apple's totally
[00:38:02.640 --> 00:38:04.480]   dominant in the smart watch.
[00:38:04.480 --> 00:38:05.480]   Absolutely, right?
[00:38:05.480 --> 00:38:06.480]   Yes.
[00:38:06.480 --> 00:38:08.600]   The only hope Google has is this Fitbit acquisition.
[00:38:08.600 --> 00:38:10.400]   I think that they played off of that a little bit.
[00:38:10.400 --> 00:38:13.200]   Well, we now own Fitbit.
[00:38:13.200 --> 00:38:14.560]   But it's a different device.
[00:38:14.560 --> 00:38:20.680]   I mean, Fitbit is basically a dumb device at just certain things very, very well.
[00:38:20.680 --> 00:38:26.400]   Apple Watch, for all, I dislike the old smartwatch things.
[00:38:26.400 --> 00:38:28.120]   They do it really well.
[00:38:28.120 --> 00:38:32.760]   It's the only smartwatch I've ever seen which A, looks good and B, people are enthusiastic
[00:38:32.760 --> 00:38:33.760]   about.
[00:38:33.760 --> 00:38:34.760]   It's totally dominant in the market too.
[00:38:34.760 --> 00:38:35.760]   It's not even close.
[00:38:35.760 --> 00:38:36.760]   Not even close.
[00:38:36.760 --> 00:38:39.200]   But I mean, Apple has done this time and time again.
[00:38:39.200 --> 00:38:44.880]   MP3 players have been around for years until the iPod came out.
[00:38:44.880 --> 00:38:47.920]   And then it was just like, yeah, same with smartphones.
[00:38:47.920 --> 00:38:51.040]   They've been around even keyboardless smartphones.
[00:38:51.040 --> 00:38:52.040]   Smartish phones.
[00:38:52.040 --> 00:38:53.040]   Smartish phones.
[00:38:53.040 --> 00:38:54.040]   Yeah.
[00:38:54.040 --> 00:38:55.040]   But Apple made it work.
[00:38:55.040 --> 00:38:59.720]   It has to be said though that Apple's first few iterations of the Apple Watch were not great.
[00:38:59.720 --> 00:39:01.520]   They were clunky and chunky and clunky.
[00:39:01.520 --> 00:39:02.520]   Out of the gold one?
[00:39:02.520 --> 00:39:03.520]   Yeah.
[00:39:03.520 --> 00:39:04.520]   You never bought that.
[00:39:04.520 --> 00:39:09.040]   It must be just like, the minute you take it out of the box, it loses 90% of its value.
[00:39:09.040 --> 00:39:15.800]   But the reason it's so great now is that they just kept chipping away at all the little
[00:39:15.800 --> 00:39:17.920]   rough edges.
[00:39:17.920 --> 00:39:23.040]   They applied artificial, powerful artificial intelligence for the slightest little usability
[00:39:23.040 --> 00:39:25.960]   features which Apple is so good at.
[00:39:25.960 --> 00:39:29.200]   And they got it at the point where it's so great that that's actually the reason why
[00:39:29.200 --> 00:39:32.120]   I switched back to iPhone from Android.
[00:39:32.120 --> 00:39:33.880]   Because I wanted an Apple Watch.
[00:39:33.880 --> 00:39:34.880]   That was enough.
[00:39:34.880 --> 00:39:37.160]   And I think there are probably other people like me as well.
[00:39:37.160 --> 00:39:40.520]   There's potential for Google, but I don't think they have the vision to pull it off.
[00:39:40.520 --> 00:39:46.320]   At this point, they've become much more, again, I'm sorry to keep dissing Google.
[00:39:46.320 --> 00:39:48.880]   I love Google actually, but I'm sorry to keep dissing them.
[00:39:48.880 --> 00:39:50.440]   No, because we love them so much.
[00:39:50.440 --> 00:39:52.040]   We're so mad at them.
[00:39:52.040 --> 00:39:53.040]   Exactly.
[00:39:53.040 --> 00:39:57.240]   But I don't think they have the vision to chip away at all the little unrefinements like
[00:39:57.240 --> 00:39:59.200]   Apple did with the Apple Watch.
[00:39:59.200 --> 00:40:00.680]   And so I don't have a lot of hope that they could be better.
[00:40:00.680 --> 00:40:01.680]   Yeah.
[00:40:01.680 --> 00:40:04.480]   You need somebody like a Johnny Ive who's just going to focus on it again and again
[00:40:04.480 --> 00:40:05.920]   and again and again and refine it and refine it.
[00:40:05.920 --> 00:40:10.120]   To have the power to just overrule all the brilliant ideas that every little engineer
[00:40:10.120 --> 00:40:11.120]   and the company has done.
[00:40:11.120 --> 00:40:12.120]   Right.
[00:40:12.120 --> 00:40:13.120]   And we'll stay there.
[00:40:13.120 --> 00:40:14.120]   Yeah.
[00:40:14.120 --> 00:40:15.840]   Stay with Google and not get wooed out to Apple.
[00:40:15.840 --> 00:40:19.960]   Just the minute Apple recognizes this is a person that's smart enough to do some damage.
[00:40:19.960 --> 00:40:24.560]   Michael Quineola, who's in our Discord chat, says there is something good about Android
[00:40:24.560 --> 00:40:25.560]   where which is open source.
[00:40:25.560 --> 00:40:27.560]   That's what ties into both based on Linux.
[00:40:27.560 --> 00:40:28.560]   Yeah.
[00:40:28.560 --> 00:40:30.880]   So he said it's possible that something could be done with it.
[00:40:30.880 --> 00:40:34.040]   But that's going to be a small hobbyist thing.
[00:40:34.040 --> 00:40:35.040]   Yeah.
[00:40:35.040 --> 00:40:39.960]   And he says, I love this Fitbit is a Tamogotchi, but the stupid thing you're keeping alive
[00:40:39.960 --> 00:40:40.960]   is you.
[00:40:40.960 --> 00:40:41.960]   Bravo.
[00:40:41.960 --> 00:40:42.960]   Nice.
[00:40:42.960 --> 00:40:43.960]   It's good.
[00:40:43.960 --> 00:40:44.960]   Yeah.
[00:40:44.960 --> 00:40:45.960]   Yeah.
[00:40:45.960 --> 00:40:50.080]   One last bit from Google I/O and I thought of you when I saw this Mike Elgin.
[00:40:50.080 --> 00:40:51.840]   No, they didn't bring back Google+, darn it.
[00:40:51.840 --> 00:40:52.840]   I even thought they might.
[00:40:52.840 --> 00:40:53.840]   Yeah.
[00:40:53.840 --> 00:40:56.880]   We were sitting here thinking, you know, boy, because they brought back, they bringing
[00:40:56.880 --> 00:41:02.160]   back a lot of stuff, including apparently a Google reader like feature in Chrome.
[00:41:02.160 --> 00:41:03.160]   Yeah.
[00:41:03.160 --> 00:41:04.160]   It's this is to me.
[00:41:04.160 --> 00:41:05.160]   This is a really dumb idea.
[00:41:05.160 --> 00:41:08.920]   When they killed Google reader, I immediately went to Feedly.
[00:41:08.920 --> 00:41:12.760]   And if I wanted to, which I don't, I could put Feedly as my new tab.
[00:41:12.760 --> 00:41:14.160]   I'm going to use a Chromebook, of course.
[00:41:14.160 --> 00:41:15.640]   I use a Pixelbook.
[00:41:15.640 --> 00:41:21.200]   And that new tab, whatever you allocate to that new tab is everything if you're a Chromebook
[00:41:21.200 --> 00:41:22.560]   user.
[00:41:22.560 --> 00:41:25.360]   And Feedly doesn't even come close to making the cut.
[00:41:25.360 --> 00:41:29.540]   But if they did take Google reader or Feedly and made it the new tab, that's essentially
[00:41:29.540 --> 00:41:30.960]   what we're talking about here.
[00:41:30.960 --> 00:41:31.960]   Yeah.
[00:41:31.960 --> 00:41:33.400]   Which means it's kind of limited space.
[00:41:33.400 --> 00:41:35.280]   Because it's only on that new tab.
[00:41:35.280 --> 00:41:36.280]   Right.
[00:41:36.280 --> 00:41:37.280]   Yes.
[00:41:37.280 --> 00:41:39.840]   And it looks like the way they're going to do, by the way, it's only a mobile right now,
[00:41:39.840 --> 00:41:42.400]   but the way they're going to do it takes up so much space.
[00:41:42.400 --> 00:41:45.760]   It's not, you know, one of the reasons we use news readers is because we can have 100
[00:41:45.760 --> 00:41:47.640]   articles and scan through them quickly.
[00:41:47.640 --> 00:41:48.640]   Yeah, right.
[00:41:48.640 --> 00:41:49.640]   This is not that.
[00:41:49.640 --> 00:41:50.640]   Right.
[00:41:50.640 --> 00:41:51.800]   This is basically Google's card interface.
[00:41:51.800 --> 00:41:55.160]   And speaking of Google+, they killed Google reader partly because they said, we don't need
[00:41:55.160 --> 00:41:56.800]   Google reader if we have Google+, right?
[00:41:56.800 --> 00:41:57.800]   Yeah.
[00:41:57.800 --> 00:42:01.720]   And they killed Google+, and now they're like, oh, we should have a reader.
[00:42:01.720 --> 00:42:03.080]   I'm going to make a prediction.
[00:42:03.080 --> 00:42:06.160]   So Google, I don't think Sundar Pajai will be there in a year.
[00:42:06.160 --> 00:42:07.160]   Wow.
[00:42:07.160 --> 00:42:10.600]   If I'm the board, I think he would, I think he'll hold on longer, but I like you.
[00:42:10.600 --> 00:42:13.320]   If I'm the board, I'm, I'm thinking, I'm looking at this.
[00:42:13.320 --> 00:42:14.320]   Okay.
[00:42:14.320 --> 00:42:16.120]   We'll give him a year because of COVID.
[00:42:16.120 --> 00:42:18.520]   I'm looking at this thinking, we need leadership.
[00:42:18.520 --> 00:42:19.520]   We need leadership.
[00:42:19.520 --> 00:42:21.240]   Sundar's a really nice guy.
[00:42:21.240 --> 00:42:23.200]   I first met him when the Chromebooks came out.
[00:42:23.200 --> 00:42:24.200]   Yes.
[00:42:24.200 --> 00:42:26.520]   And he was a great leader at the Chrome OS division.
[00:42:26.520 --> 00:42:27.520]   Yes.
[00:42:27.520 --> 00:42:28.760]   That's about right.
[00:42:28.760 --> 00:42:32.280]   I have an overarching mega theory about this whole thing.
[00:42:32.280 --> 00:42:37.200]   And this is, and I say this, as a way to play devil's advocate with your position that he's
[00:42:37.200 --> 00:42:38.280]   going to be gone soon.
[00:42:38.280 --> 00:42:42.920]   So I think that the big war, the reason Google+ has gone is that you had a Vikkindotra who
[00:42:42.920 --> 00:42:44.760]   is all about the enthusiast user.
[00:42:44.760 --> 00:42:47.320]   He's like, we got to serve those people.
[00:42:47.320 --> 00:42:52.480]   And then, and then Sundar Pajai was like, no, it's all about the business, which is the
[00:42:52.480 --> 00:42:54.880]   future of Google business is business.
[00:42:54.880 --> 00:42:57.240]   We need to thrill business customers.
[00:42:57.240 --> 00:42:59.080]   And so they had a battle.
[00:42:59.080 --> 00:43:04.760]   Sundar Pajai won Vikkindotra left, and they've been screwing enthusiast users ever since.
[00:43:04.760 --> 00:43:06.160]   Oh, that's us.
[00:43:06.160 --> 00:43:07.160]   And that's us.
[00:43:07.160 --> 00:43:08.160]   That's what we're doing.
[00:43:08.160 --> 00:43:11.720]   We are on the Vikkindotra team mindset because that's who we are.
[00:43:11.720 --> 00:43:14.520]   No business is happily using Google Docs at this.
[00:43:14.520 --> 00:43:15.520]   That's true.
[00:43:15.520 --> 00:43:18.280]   I'm not saying they're succeeding at that, but that's the goal.
[00:43:18.280 --> 00:43:19.280]   That's what it's all for.
[00:43:19.280 --> 00:43:20.280]   I'm not quite.
[00:43:20.280 --> 00:43:24.480]   I'm not so sure about that because we use Google Docs and I have to say nobody I know
[00:43:24.480 --> 00:43:25.480]   is happy about it.
[00:43:25.480 --> 00:43:29.000]   Well, honestly, I've been thinking about this because since I've been working from
[00:43:29.000 --> 00:43:33.720]   home for the last year, I've been using a mix of Microsoft Word and Google Docs.
[00:43:33.720 --> 00:43:38.480]   And I'll be honest, Google Docs's spell checker in particular is superior to anything
[00:43:38.480 --> 00:43:39.480]   Microsoft has put out.
[00:43:39.480 --> 00:43:40.480]   Interesting.
[00:43:40.480 --> 00:43:41.480]   Yeah.
[00:43:41.480 --> 00:43:42.960]   I mean, OK, it gets some things wrong.
[00:43:42.960 --> 00:43:43.960]   It doesn't recognize certain things.
[00:43:43.960 --> 00:43:45.600]   So do you write in Google Docs?
[00:43:45.600 --> 00:43:47.160]   Is that your CMS?
[00:43:47.160 --> 00:43:48.160]   It is now.
[00:43:48.160 --> 00:43:49.160]   Yeah.
[00:43:49.160 --> 00:43:50.160]   No, our CMS is private.
[00:43:50.160 --> 00:43:51.160]   We have our own CMS.
[00:43:51.160 --> 00:43:54.160]   So the register has its own CMS, but you will write the articles first in docs.
[00:43:54.160 --> 00:44:00.520]   I would write the articles in docs, Word or Libra Office and then you never write straight
[00:44:00.520 --> 00:44:01.520]   to the CMS.
[00:44:01.520 --> 00:44:02.520]   No, no, everybody knows that.
[00:44:02.520 --> 00:44:03.520]   Yeah.
[00:44:03.520 --> 00:44:05.240]   Well, at least everybody's learned that.
[00:44:05.240 --> 00:44:06.240]   Right.
[00:44:06.240 --> 00:44:07.240]   Yeah, exactly.
[00:44:07.240 --> 00:44:08.240]   It's a great deal.
[00:44:08.240 --> 00:44:09.880]   It's a lot of work.
[00:44:09.880 --> 00:44:10.880]   But yeah, all right.
[00:44:10.880 --> 00:44:11.880]   I mean, depends where I am.
[00:44:11.880 --> 00:44:16.720]   If I'm offline, Libra Office, if I'm pushed to Microsoft Word because I've got to copy something
[00:44:16.720 --> 00:44:20.600]   from there, Google Docs usually a lot of the time, to be honest.
[00:44:20.600 --> 00:44:21.920]   It's actually interchangeable though.
[00:44:21.920 --> 00:44:24.640]   And that's part of the, it's really a commodity product at this point.
[00:44:24.640 --> 00:44:25.640]   So that's part of the problem.
[00:44:25.640 --> 00:44:26.640]   It's hard to distinguish.
[00:44:26.640 --> 00:44:29.160]   But I guess that's what Smart Canvas is trying to do.
[00:44:29.160 --> 00:44:30.840]   Which is the return of Google Wave.
[00:44:30.840 --> 00:44:32.040]   I'm sure you noted.
[00:44:32.040 --> 00:44:34.280]   But again, for business customers.
[00:44:34.280 --> 00:44:35.280]   Yeah.
[00:44:35.280 --> 00:44:36.280]   Right.
[00:44:36.280 --> 00:44:39.200]   I don't think, you know, those features are not going to be available to you unless
[00:44:39.200 --> 00:44:41.240]   you have the workspace.
[00:44:41.240 --> 00:44:42.760]   And there, you know what?
[00:44:42.760 --> 00:44:48.280]   I understand and from the point of view of the business, the business customers pay.
[00:44:48.280 --> 00:44:49.280]   Right?
[00:44:49.280 --> 00:44:50.280]   Yeah.
[00:44:50.280 --> 00:44:51.280]   That's right.
[00:44:51.280 --> 00:44:53.600]   My Gmail account has been closed.
[00:44:53.600 --> 00:44:54.600]   I can't get into it.
[00:44:54.600 --> 00:44:55.600]   How do I get some help?
[00:44:55.600 --> 00:44:56.600]   I said, well, it's free.
[00:44:56.600 --> 00:44:57.600]   Right.
[00:44:57.600 --> 00:44:59.240]   You're going to get that level of support.
[00:44:59.240 --> 00:45:00.240]   Right?
[00:45:00.240 --> 00:45:01.240]   Right?
[00:45:01.240 --> 00:45:04.480]   But for paid users, there's support.
[00:45:04.480 --> 00:45:05.480]   Yep.
[00:45:05.480 --> 00:45:08.880]   So maybe that's what's going on is they don't want to offer the free stuff.
[00:45:08.880 --> 00:45:10.840]   And as Jeff Jarvis will, will, will.
[00:45:10.840 --> 00:45:11.840]   I'm there.
[00:45:11.840 --> 00:45:14.640]   Verify the paid version of all that stuff is worse.
[00:45:14.640 --> 00:45:15.640]   In some ways.
[00:45:15.640 --> 00:45:16.640]   It has more problematic.
[00:45:16.640 --> 00:45:18.480]   And will not interoperate if you have both.
[00:45:18.480 --> 00:45:19.480]   Exactly.
[00:45:19.480 --> 00:45:21.240]   He hates that because he's got a personal account and a business account.
[00:45:21.240 --> 00:45:22.240]   Right.
[00:45:22.240 --> 00:45:23.240]   And never the twine shot me.
[00:45:23.240 --> 00:45:24.240]   Right.
[00:45:24.240 --> 00:45:29.880]   This is a part of a larger move, I think, towards Google to monetize itself rather than innovate.
[00:45:29.880 --> 00:45:33.520]   You've seen it with YouTube where you're suddenly being spammed with adverts and...
[00:45:33.520 --> 00:45:34.520]   Oh, it's horrible.
[00:45:34.520 --> 00:45:35.520]   Yeah.
[00:45:35.520 --> 00:45:36.520]   Exactly.
[00:45:36.520 --> 00:45:38.800]   You're really, if you aren't paying for YouTube premium, yeah.
[00:45:38.800 --> 00:45:41.920]   You are, you are spending a lot of time ignoring adverts.
[00:45:41.920 --> 00:45:42.920]   Yeah.
[00:45:42.920 --> 00:45:44.400]   And they're doing the same with Google storage.
[00:45:44.400 --> 00:45:49.720]   They're doing, it's moving from an innovative economy to a rent-seeking economy.
[00:45:49.720 --> 00:45:51.640]   And that never works long-term.
[00:45:51.640 --> 00:45:53.600]   Although that's what Apple's doing, isn't it?
[00:45:53.600 --> 00:45:56.960]   I mean, they've started to say, "Well, we want our services to support the company,
[00:45:56.960 --> 00:45:57.960]   the services are wrong."
[00:45:57.960 --> 00:46:01.560]   Well, yeah, but at the same time, they build products which excite people.
[00:46:01.560 --> 00:46:04.920]   It's also when you get the red dot thing saying, "Well, you haven't signed up for
[00:46:04.920 --> 00:46:05.920]   our cloud."
[00:46:05.920 --> 00:46:06.920]   That's right.
[00:46:06.920 --> 00:46:10.520]   That idea raising a public head like, you know, we need to keep, you know, a little
[00:46:10.520 --> 00:46:11.520]   useful hostel.
[00:46:11.520 --> 00:46:16.480]   I mean, me and my wife and my sons and their significant others, I calculated we spend
[00:46:16.480 --> 00:46:18.800]   about $50,000 a year on Apple stuff.
[00:46:18.800 --> 00:46:23.920]   And they harass me to pay for iCloud.
[00:46:23.920 --> 00:46:24.920]   And they want that.
[00:46:24.920 --> 00:46:25.920]   They want that.
[00:46:25.920 --> 00:46:26.920]   And they want that.
[00:46:26.920 --> 00:46:27.920]   And they want that.
[00:46:27.920 --> 00:46:29.400]   And it's just like, you know, how much money are they of mine?
[00:46:29.400 --> 00:46:30.400]   A couple of hours.
[00:46:30.400 --> 00:46:32.240]   A couple of pixels reacts in a thin pattern.
[00:46:32.240 --> 00:46:33.240]   That's it.
[00:46:33.240 --> 00:46:34.240]   Yeah.
[00:46:34.240 --> 00:46:39.240]   Oh, I'm embarrassed to admit I've got the new iPad coming on Thursday.
[00:46:39.240 --> 00:46:40.240]   Yeah.
[00:46:40.240 --> 00:46:42.240]   Oh, do the math.
[00:46:42.240 --> 00:46:43.720]   Have you done the math?
[00:46:43.720 --> 00:46:44.720]   Have you done the math?
[00:46:44.720 --> 00:46:47.280]   How much you spend on Apple products?
[00:46:47.280 --> 00:46:48.280]   It's going to be close to $50,000.
[00:46:48.280 --> 00:46:50.040]   It's got to be for me alone.
[00:46:50.040 --> 00:46:51.040]   Yes, yeah.
[00:46:51.040 --> 00:46:54.240]   It's just I actually have to force myself to kind of stop.
[00:46:54.240 --> 00:47:01.680]   I wish they would just be happy with the 50,000 instead of wanting 51,000.
[00:47:01.680 --> 00:47:03.600]   It's funny because I really love like you.
[00:47:03.600 --> 00:47:04.600]   I love Google.
[00:47:04.600 --> 00:47:05.840]   I love open source.
[00:47:05.840 --> 00:47:07.000]   I want to support Google.
[00:47:07.000 --> 00:47:13.200]   I want to support their, you know, what used to be their kind of culture.
[00:47:13.200 --> 00:47:16.320]   But I don't feel like I have a home at Google anymore.
[00:47:16.320 --> 00:47:18.080]   And that's really what Apple wants you to feel like.
[00:47:18.080 --> 00:47:19.440]   You have a home at Apple.
[00:47:19.440 --> 00:47:20.440]   Yes.
[00:47:20.440 --> 00:47:22.040]   And they need to do something.
[00:47:22.040 --> 00:47:23.360]   I think they need to do something product.
[00:47:23.360 --> 00:47:24.360]   That's why I pretty agree.
[00:47:24.360 --> 00:47:25.360]   Board is going to.
[00:47:25.360 --> 00:47:27.200]   It's funny because the board isn't here.
[00:47:27.200 --> 00:47:28.200]   They look at the search.
[00:47:28.200 --> 00:47:29.200]   Yeah.
[00:47:29.200 --> 00:47:30.200]   They go, "Yeah, you're doing great.
[00:47:30.200 --> 00:47:31.200]   I'll pick up the good work."
[00:47:31.200 --> 00:47:32.200]   Yeah.
[00:47:32.200 --> 00:47:33.200]   In fact, you should get rid of all that other stuff.
[00:47:33.200 --> 00:47:34.200]   Right.
[00:47:34.200 --> 00:47:35.200]   They may be saying that.
[00:47:35.200 --> 00:47:37.160]   But honestly, they should be thinking what the future is going to hold.
[00:47:37.160 --> 00:47:38.160]   Yes.
[00:47:38.160 --> 00:47:39.160]   They need to focus.
[00:47:39.160 --> 00:47:45.800]   I think the focus on your most enthusiastic users is a winning strategy.
[00:47:45.800 --> 00:47:47.800]   That's how Apple wins.
[00:47:47.800 --> 00:47:50.000]   And Google could because they have people.
[00:47:50.000 --> 00:47:51.000]   They do.
[00:47:51.000 --> 00:47:54.160]   They still do after all the times they've had the rug pulled out from under them.
[00:47:54.160 --> 00:47:55.160]   They still have them.
[00:47:55.160 --> 00:47:56.160]   So.
[00:47:56.160 --> 00:47:57.160]   Yeah.
[00:47:57.160 --> 00:47:58.160]   Yeah.
[00:47:58.160 --> 00:47:59.160]   I need to sleep.
[00:47:59.160 --> 00:48:00.160]   All right.
[00:48:00.160 --> 00:48:01.160]   We're going to take a little break before this.
[00:48:01.160 --> 00:48:04.160]   Mez Cal sets in because I don't think I'm going to be able to read any commercials after
[00:48:04.160 --> 00:48:05.160]   this.
[00:48:05.160 --> 00:48:06.160]   I got you.
[00:48:06.160 --> 00:48:08.160]   And you say, "I'll back up."
[00:48:08.160 --> 00:48:09.160]   Yeah.
[00:48:09.160 --> 00:48:10.160]   I'll do that.
[00:48:10.160 --> 00:48:11.160]   You can take over.
[00:48:11.160 --> 00:48:13.080]   You know who's really taking you for a ride.
[00:48:13.080 --> 00:48:16.560]   You know who's really rent seeking these days the big phone companies.
[00:48:16.560 --> 00:48:17.560]   Oh, yeah.
[00:48:17.560 --> 00:48:18.560]   The big cellular companies.
[00:48:18.560 --> 00:48:19.560]   Oh, yeah.
[00:48:19.560 --> 00:48:21.400]   Man, those hidden fees.
[00:48:21.400 --> 00:48:27.560]   You don't even get new features and new benefits, but your bill keeps going up, right?
[00:48:27.560 --> 00:48:30.760]   That's why I'm a fan of Mint Mobile.
[00:48:30.760 --> 00:48:36.240]   Mint Mobile offers premium wireless service starting at get this $15 a month.
[00:48:36.240 --> 00:48:39.080]   Now, see, we're all conditioned to go, "Oh, yeah.
[00:48:39.080 --> 00:48:40.080]   Sure.
[00:48:40.080 --> 00:48:41.080]   What's the catch?"
[00:48:41.080 --> 00:48:44.840]   The big phone companies have been doing to us all this time, but there isn't a catch.
[00:48:44.840 --> 00:48:48.960]   I've been using Mint Mobile for more than a year now, and it's great.
[00:48:48.960 --> 00:48:49.960]   They're secret sauces.
[00:48:49.960 --> 00:48:52.680]   They're the first companies to sell wireless service online only.
[00:48:52.680 --> 00:48:57.480]   No stores, no crazy overhead costs.
[00:48:57.480 --> 00:49:01.320]   They're saving money, but what's great is they're saving money and passing it down to
[00:49:01.320 --> 00:49:02.480]   you.
[00:49:02.480 --> 00:49:05.000]   Mint Mobile just saves big time.
[00:49:05.000 --> 00:49:06.000]   And look at...
[00:49:06.000 --> 00:49:11.040]   If you go to mintmobile.com/twit right now and look at the prices.
[00:49:11.040 --> 00:49:12.040]   It's very straightforward.
[00:49:12.040 --> 00:49:17.880]   Yes, they have an unlimited plan, but start with the trial offer $15 a month.
[00:49:17.880 --> 00:49:24.280]   All plans come with unlimited talk and text plus high-speed data delivered on the nation's
[00:49:24.280 --> 00:49:25.560]   largest 5G network.
[00:49:25.560 --> 00:49:28.920]   All right, your geeks, I'll tell you, it's T-Mobile.
[00:49:28.920 --> 00:49:29.920]   They are on MVNO.
[00:49:29.920 --> 00:49:31.160]   They resell T-Mobile.
[00:49:31.160 --> 00:49:34.440]   So if you're getting good T-Mobile service where you are, that's the service you'll get
[00:49:34.440 --> 00:49:36.480]   from Mint Mobile.
[00:49:36.480 --> 00:49:42.040]   Some wireless service, but when I compare my Mint Mobile bill with my T-Mobile bill, it's
[00:49:42.040 --> 00:49:45.800]   a lot less for Mint Mobile.
[00:49:45.800 --> 00:49:48.320]   You can use your own phone, bring your phone, sure, no problem.
[00:49:48.320 --> 00:49:51.400]   In fact, this is another example of rent seeking.
[00:49:51.400 --> 00:49:53.720]   You ask for a new SIM for any mobile carrier.
[00:49:53.720 --> 00:49:55.960]   They're going to say, "Oh, that'd be $15, please."
[00:49:55.960 --> 00:49:56.960]   Not from Mint Mobile.
[00:49:56.960 --> 00:50:00.440]   They will send you the SIM for free, put it in the phone.
[00:50:00.440 --> 00:50:04.880]   You can even port your number over for free along with all your existing contacts.
[00:50:04.880 --> 00:50:08.760]   And of course, if you're not 100% satisfied, Mint Mobile has you covered, seven-day money
[00:50:08.760 --> 00:50:11.400]   back guarantee.
[00:50:11.400 --> 00:50:16.480]   It's almost hard to believe, but I want to stand here and say it's true and I've got
[00:50:16.480 --> 00:50:17.480]   it.
[00:50:17.480 --> 00:50:21.360]   To get your new wireless plan for just $15 a month and get the plan shipped to your door
[00:50:21.360 --> 00:50:25.960]   for free, go to mintmobile.com/twit.
[00:50:25.960 --> 00:50:28.160]   Just go to mintmobile.com/twit.
[00:50:28.160 --> 00:50:31.240]   There's a little thing on there that you can check your current phone and see if it'll
[00:50:31.240 --> 00:50:32.240]   work.
[00:50:32.240 --> 00:50:34.160]   Almost all phones do.
[00:50:34.160 --> 00:50:35.720]   Mint Mobile.com/twit.
[00:50:35.720 --> 00:50:38.520]   They send you the SIM, you pop it in, you're good to go.
[00:50:38.520 --> 00:50:41.000]   Cut your wireless bill down to $15 a month.
[00:50:41.000 --> 00:50:45.240]   You have to ask, "Why am I paying 70, 80, 90, 100 bucks a month when I could do it with
[00:50:45.240 --> 00:50:48.760]   $15 a month, the same service?"
[00:50:48.760 --> 00:50:49.760]   It's nuts.
[00:50:49.760 --> 00:50:50.760]   You got to do it.
[00:50:50.760 --> 00:50:52.760]   $15 a month, mintmobile.com/twit.
[00:50:52.760 --> 00:51:00.000]   We thank them so much for their support of our all-in studio, Mezcal Powered.
[00:51:00.000 --> 00:51:01.000]   Yes.
[00:51:01.000 --> 00:51:02.000]   Mescallen.
[00:51:02.000 --> 00:51:03.000]   Mescallen.
[00:51:03.000 --> 00:51:05.200]   Minti Fresh.
[00:51:05.200 --> 00:51:11.320]   This week in tech.
[00:51:11.320 --> 00:51:14.040]   One other thing they did that was so bad.
[00:51:14.040 --> 00:51:19.040]   It should look like a good little assassination squad off.
[00:51:19.040 --> 00:51:20.400]   I like Michael Paney.
[00:51:20.400 --> 00:51:21.800]   I think he's a good actor.
[00:51:21.800 --> 00:51:27.400]   But folks, Google, please, you don't have to bring in a Hollywood shield to tell bad
[00:51:27.400 --> 00:51:30.080]   jokes.
[00:51:30.080 --> 00:51:33.680]   People who are watching Google I/O understand what quantum computing is.
[00:51:33.680 --> 00:51:37.480]   They might even be excited that you're building a quantum AI lab.
[00:51:37.480 --> 00:51:41.000]   You can tell them the stuff you don't need to have Michael Paney tell bad jokes.
[00:51:41.000 --> 00:51:42.000]   Why?
[00:51:42.000 --> 00:51:43.000]   Why?
[00:51:43.000 --> 00:51:44.000]   Yes, exactly.
[00:51:44.000 --> 00:51:45.000]   Why?
[00:51:45.000 --> 00:51:46.000]   Yeah, it was a little awkward.
[00:51:46.000 --> 00:51:48.280]   And I tell you, Santa Barbara is my hometown.
[00:51:48.280 --> 00:51:49.680]   That's where they're going to build this center.
[00:51:49.680 --> 00:51:54.520]   This is great news for computer scientists in Santa Barbara because there's a town where
[00:51:54.520 --> 00:51:57.440]   everybody wants to live there and there are no jobs.
[00:51:57.440 --> 00:52:00.440]   There's a small tech sector there.
[00:52:00.440 --> 00:52:01.440]   So there's this space there.
[00:52:01.440 --> 00:52:02.880]   There's a few other companies.
[00:52:02.880 --> 00:52:05.000]   But this is so good for Santa Barbara.
[00:52:05.000 --> 00:52:06.920]   It's a boutique-y kind of.
[00:52:06.920 --> 00:52:11.680]   You know, when they're competing with other quantum computing efforts.
[00:52:11.680 --> 00:52:13.880]   It's such a sort of come-out-like place.
[00:52:13.880 --> 00:52:15.360]   Yeah, very much so.
[00:52:15.360 --> 00:52:16.680]   It's a resort town.
[00:52:16.680 --> 00:52:18.280]   It's where Prince Harry lives.
[00:52:18.280 --> 00:52:19.280]   Monocino's.
[00:52:19.280 --> 00:52:20.280]   Oprah lives there.
[00:52:20.280 --> 00:52:21.280]   Oprah.
[00:52:21.280 --> 00:52:22.280]   Yeah.
[00:52:22.280 --> 00:52:23.280]   Ellen DeGeneres.
[00:52:23.280 --> 00:52:24.480]   It's a beautiful, very fancy.
[00:52:24.480 --> 00:52:25.480]   It's Hollywood's weekend.
[00:52:25.480 --> 00:52:27.480]   Yeah, yeah.
[00:52:27.480 --> 00:52:28.480]   Yeah.
[00:52:28.480 --> 00:52:29.480]   Yeah.
[00:52:29.480 --> 00:52:30.480]   Place.
[00:52:30.480 --> 00:52:31.480]   You know, so.
[00:52:31.480 --> 00:52:32.480]   Oh, how do you move there?
[00:52:32.480 --> 00:52:33.480]   That doesn't surprise me.
[00:52:33.480 --> 00:52:34.480]   Right.
[00:52:34.480 --> 00:52:35.480]   Yeah.
[00:52:35.480 --> 00:52:36.480]   Prince Monocino.
[00:52:36.480 --> 00:52:40.960]   But this is literally this will help them attract people because it's a great place to
[00:52:40.960 --> 00:52:42.160]   live and it's hard to work there.
[00:52:42.160 --> 00:52:44.920]   So they have got great computer science jobs.
[00:52:44.920 --> 00:52:48.640]   If they can make it work, I mean, there's been some considerable research now.
[00:52:48.640 --> 00:52:53.440]   I'm just like, oh, I mean, this is something which came up at a conference early this
[00:52:53.440 --> 00:52:54.440]   week.
[00:52:54.440 --> 00:53:01.240]   There's no real proof that they can make these things work on a lasting, sustainable basis.
[00:53:01.240 --> 00:53:05.880]   Now, personally, I'm sure they can, but I don't think it's going to be anywhere near
[00:53:05.880 --> 00:53:07.960]   it as easy as people are saying.
[00:53:07.960 --> 00:53:11.800]   And this is why I was kind of encouraged by the scale of that announcement because they
[00:53:11.800 --> 00:53:13.720]   weren't saying we're going to do this next year.
[00:53:13.720 --> 00:53:14.720]   They're saying.
[00:53:14.720 --> 00:53:15.720]   And they're doing it.
[00:53:15.720 --> 00:53:17.640]   Yeah, this is way down the road.
[00:53:17.640 --> 00:53:18.800]   They're being rational about it.
[00:53:18.800 --> 00:53:19.800]   Can I?
[00:53:19.800 --> 00:53:22.040]   Can I be the real cynic here?
[00:53:22.040 --> 00:53:23.760]   There's huge federal money.
[00:53:23.760 --> 00:53:24.760]   Oh, yes.
[00:53:24.760 --> 00:53:25.760]   Yes.
[00:53:25.760 --> 00:53:30.760]   The federal government has decided that, well, if there's going to be a quantum computing
[00:53:30.760 --> 00:53:33.280]   race, we better be in it.
[00:53:33.280 --> 00:53:35.840]   Even though I think it's very speculative.
[00:53:35.840 --> 00:53:38.760]   You know, I've been the real cynic on all of this.
[00:53:38.760 --> 00:53:42.480]   I just, I don't know if they're ever going to build a quantum computer that's stable.
[00:53:42.480 --> 00:53:46.960]   To articulate the cynical position, they get the money whether they can do it or not.
[00:53:46.960 --> 00:53:47.960]   Yeah.
[00:53:47.960 --> 00:53:48.960]   Because it's a government contract.
[00:53:48.960 --> 00:53:49.960]   Yeah.
[00:53:49.960 --> 00:53:51.240]   So I think really IBM's doing this.
[00:53:51.240 --> 00:53:52.520]   They say, wait, wait, we've got one.
[00:53:52.520 --> 00:53:53.520]   We're building one.
[00:53:53.520 --> 00:53:55.720]   But, you know, it's like four qubits.
[00:53:55.720 --> 00:53:56.720]   It's not.
[00:53:56.720 --> 00:54:00.400]   This is not going to solve any problems for some time to come.
[00:54:00.400 --> 00:54:06.800]   No, no, I mean, there was one of the talks I was listening to at RSA was they had a
[00:54:06.800 --> 00:54:11.120]   government university of Cambridge, one of their chief cryptologists in the crypto panel.
[00:54:11.120 --> 00:54:16.080]   And he was saying, look, you can do this stuff, but it's the equivalent of like an
[00:54:16.080 --> 00:54:20.240]   Altair 76 or, you know, the very earliest computers.
[00:54:20.240 --> 00:54:24.200]   It's dial and knob and you've got to do each individual thing.
[00:54:24.200 --> 00:54:26.880]   And that works for really basic stuff.
[00:54:26.880 --> 00:54:33.960]   But for complex stuff, it's just the presumption is it is at the same stage as the Altair was
[00:54:33.960 --> 00:54:36.560]   in what is going to be this massive revolution.
[00:54:36.560 --> 00:54:40.920]   I think we're closer to Colossus than Altair to be able to agree with him.
[00:54:40.920 --> 00:54:41.920]   I don't know.
[00:54:41.920 --> 00:54:42.920]   I'm just saying.
[00:54:42.920 --> 00:54:44.920]   And, you know, I'd love to be proven wrong.
[00:54:44.920 --> 00:54:46.840]   I mean, you know, there'll be fascinating and see.
[00:54:46.840 --> 00:54:47.840]   Oh, yeah.
[00:54:47.840 --> 00:54:50.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:54:50.840 --> 00:54:53.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:54:53.840 --> 00:54:56.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:54:56.840 --> 00:54:59.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:54:59.840 --> 00:55:02.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:02.840 --> 00:55:05.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:05.840 --> 00:55:08.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:08.840 --> 00:55:10.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:10.840 --> 00:55:12.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:12.840 --> 00:55:14.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:14.840 --> 00:55:15.840]   I mean, I'm not sure if I'm going to be able to do this.
[00:55:15.840 --> 00:55:16.840]   I have to do it sure.
[00:55:16.840 --> 00:55:17.840]   I have to drive home tonight.
[00:55:17.840 --> 00:55:18.840]   This is not a lie.
[00:55:18.840 --> 00:55:19.840]   Oh, no, no, no.
[00:55:19.840 --> 00:55:22.840]   Really, the real question is, which he and his and will be the company?
[00:55:22.840 --> 00:55:24.840]   Yeah, the title.
[00:55:24.840 --> 00:55:29.840]   Honestly, I'm up against some really strong competition here.
[00:55:29.840 --> 00:55:30.840]   So, you know, it's...
[00:55:30.840 --> 00:55:32.840]   Did I spell your name right?
[00:55:32.840 --> 00:55:33.840]   I-A-I-A-N.
[00:55:33.840 --> 00:55:34.840]   Ah, no, it's I-A-I-N.
[00:55:34.840 --> 00:55:37.840]   My parents have asked us if we've had words about this.
[00:55:37.840 --> 00:55:41.840]   Also, they just asked us if we had time, so then I'll be watching this.
[00:55:41.840 --> 00:55:42.840]   So it's like that.
[00:55:42.840 --> 00:55:44.840]   I-A-I-A-N.
[00:55:44.840 --> 00:55:45.840]   I'm fixing it right now.
[00:55:45.840 --> 00:55:46.840]   I had it wrong.
[00:55:46.840 --> 00:55:48.840]   Ah, don't worry about it.
[00:55:48.840 --> 00:55:50.840]   I still get messages to Lane.
[00:55:50.840 --> 00:55:51.840]   Lane Thompson.
[00:55:51.840 --> 00:55:54.840]   And I've never met a single American called Lane.
[00:55:54.840 --> 00:55:55.840]   I get, I get mail.
[00:55:55.840 --> 00:55:56.840]   Snail mail to Helen Leport.
[00:55:56.840 --> 00:55:57.840]   So it's okay.
[00:55:57.840 --> 00:55:58.840]   Well, I'll get it.
[00:55:58.840 --> 00:56:00.840]   Please, Helen's a reasonable Lane.
[00:56:00.840 --> 00:56:01.840]   I look pretty good in pumps.
[00:56:01.840 --> 00:56:02.840]   I'm not saying.
[00:56:02.840 --> 00:56:03.840]   All right.
[00:56:03.840 --> 00:56:06.840]   Moving along, we've dised Google enough now.
[00:56:06.840 --> 00:56:11.840]   Although I want to congratulate Rick Cloud, because I know Rick, he created Feedburner.
[00:56:11.840 --> 00:56:12.840]   Back in the day.
[00:56:12.840 --> 00:56:13.840]   Yeah.
[00:56:13.840 --> 00:56:17.840]   And he and I have been talking pretty much since Twit began.
[00:56:17.840 --> 00:56:20.840]   He, Feedburner was purchased by Google.
[00:56:20.840 --> 00:56:25.840]   He became a venture capitalistic Google Ventures partner at Google Ventures.
[00:56:25.840 --> 00:56:29.840]   He is now California's Chief Technology Innovation Officer.
[00:56:29.840 --> 00:56:30.840]   Yeah.
[00:56:30.840 --> 00:56:31.840]   So good guy.
[00:56:31.840 --> 00:56:32.840]   It's a great job, Todd.
[00:56:32.840 --> 00:56:33.840]   That's why I said.
[00:56:33.840 --> 00:56:34.840]   Yeah.
[00:56:34.840 --> 00:56:37.840]   Well, you know, I suspect that it's almost pro bono.
[00:56:37.840 --> 00:56:40.840]   You know, that he's doing it out of the goodness of his heart.
[00:56:40.840 --> 00:56:42.840]   He's post-economic at this point.
[00:56:42.840 --> 00:56:43.840]   Yeah.
[00:56:43.840 --> 00:56:44.840]   So, yeah.
[00:56:44.840 --> 00:56:45.840]   So good for you.
[00:56:45.840 --> 00:56:47.720]   It's a good person to choose.
[00:56:47.720 --> 00:56:52.160]   We certainly need to get technology working in government.
[00:56:52.160 --> 00:56:59.840]   And he's incredibly laudatory about the pace at which Sacramento, as a government, functions.
[00:56:59.840 --> 00:57:04.760]   He's saying that it's like very fast moving organization and so on, which is great to
[00:57:04.760 --> 00:57:05.760]   hear.
[00:57:05.760 --> 00:57:07.840]   I haven't heard that before, but that's great.
[00:57:07.840 --> 00:57:11.840]   You know, I will try to get Rick down here and we can talk to him because he's just up
[00:57:11.840 --> 00:57:13.840]   the way he wrote a piece in Sacramento.
[00:57:13.840 --> 00:57:17.840]   So he seems like the kind of guy who's just addicted to really interesting jobs.
[00:57:17.840 --> 00:57:18.840]   Smart guy.
[00:57:18.840 --> 00:57:19.840]   Yeah.
[00:57:19.840 --> 00:57:20.840]   Really nice guy.
[00:57:20.840 --> 00:57:23.840]   And Feedburner was a very early way to get podcasts out.
[00:57:23.840 --> 00:57:24.840]   So I was very happy to--
[00:57:24.840 --> 00:57:25.840]   Yeah, indeed.
[00:57:25.840 --> 00:57:26.840]   Yeah.
[00:57:26.840 --> 00:57:27.840]   You remember that?
[00:57:27.840 --> 00:57:28.840]   I do.
[00:57:28.840 --> 00:57:29.840]   The good old days.
[00:57:29.840 --> 00:57:30.840]   All right.
[00:57:30.840 --> 00:57:31.840]   Enough Google.
[00:57:31.840 --> 00:57:32.840]   I hope we haven't.
[00:57:32.840 --> 00:57:35.560]   I worry that maybe I was too negative.
[00:57:35.560 --> 00:57:38.560]   Say something nice about Google and shake hands.
[00:57:38.560 --> 00:57:40.560]   Don't say anything at all.
[00:57:40.560 --> 00:57:41.560]   Yeah.
[00:57:41.560 --> 00:57:43.240]   I like Sundar Pichai as a guy.
[00:57:43.240 --> 00:57:45.640]   I just feel like they need something.
[00:57:45.640 --> 00:57:47.560]   They feel like they lost a mojo.
[00:57:47.560 --> 00:57:48.560]   They need their mojo.
[00:57:48.560 --> 00:57:49.880]   Companies are like countries.
[00:57:49.880 --> 00:57:53.360]   They need a dictator with a vision.
[00:57:53.360 --> 00:57:56.880]   Like Steve Jobs, who's the ultimate model for that.
[00:57:56.880 --> 00:57:57.880]   He--
[00:57:57.880 --> 00:57:58.880]   Mark Zuckerberg?
[00:57:58.880 --> 00:57:59.880]   No.
[00:57:59.880 --> 00:58:00.880]   No.
[00:58:00.880 --> 00:58:01.880]   No.
[00:58:01.880 --> 00:58:02.880]   No, that's just as much as he was living in.
[00:58:02.880 --> 00:58:05.080]   You know who's really been good is such an Adele.
[00:58:05.080 --> 00:58:06.080]   He's really good.
[00:58:06.080 --> 00:58:07.080]   Yeah.
[00:58:07.080 --> 00:58:10.080]   That's an example of a leader who can bring to get--
[00:58:10.080 --> 00:58:13.480]   Well, I mean, easy act to follow after Ballmer, right?
[00:58:13.480 --> 00:58:15.120]   But I mean, he brought to get--
[00:58:15.120 --> 00:58:18.400]   He's also not-- he's more of a thoughtful, quiet engineering
[00:58:18.400 --> 00:58:19.400]   type.
[00:58:19.400 --> 00:58:22.800]   He's not the most dynamic speaker.
[00:58:22.800 --> 00:58:24.600]   But he's obviously a great leader.
[00:58:24.600 --> 00:58:26.240]   He knows the right thing when he sees it.
[00:58:26.240 --> 00:58:27.240]   And he says, let's do that.
[00:58:27.240 --> 00:58:31.120]   I mean, the first press conference I went to him with as CEO,
[00:58:31.120 --> 00:58:32.480]   I was kind of, yeah, OK.
[00:58:32.480 --> 00:58:35.000]   He's got the Cosmo Bill Gates.
[00:58:35.000 --> 00:58:39.560]   But at the same time, he speaks rationally clearly,
[00:58:39.560 --> 00:58:41.160]   and he lays out a clear policy.
[00:58:41.160 --> 00:58:42.880]   And you get the feeling this guy knows
[00:58:42.880 --> 00:58:45.040]   what he's talking about, which is something
[00:58:45.040 --> 00:58:48.160]   that had been missing from Microsoft for quite some time.
[00:58:48.160 --> 00:58:51.960]   And the way he has turned that company around from being--
[00:58:51.960 --> 00:58:54.560]   I mean, I don't want to be too strong, but a bit of a joke.
[00:58:54.560 --> 00:58:56.360]   I mean, in the turn of the century,
[00:58:56.360 --> 00:59:01.120]   to something now where everyone wants to work at Microsoft.
[00:59:01.120 --> 00:59:05.400]   It's a startling turnaround.
[00:59:05.400 --> 00:59:07.360]   Did you, at the register, do you guys
[00:59:07.360 --> 00:59:10.240]   cover the Bill Gates divorce at all?
[00:59:10.240 --> 00:59:11.120]   It's such a concept.
[00:59:11.120 --> 00:59:13.440]   It's such a people magazine fodder.
[00:59:13.440 --> 00:59:13.940]   Yeah.
[00:59:13.940 --> 00:59:15.920]   I think it's a little bit appropriate
[00:59:15.920 --> 00:59:18.560]   because it looks like it's turning out
[00:59:18.560 --> 00:59:20.800]   that the board wanted him out, that he
[00:59:20.800 --> 00:59:24.320]   was being investigated for questionable behavior
[00:59:24.320 --> 00:59:26.320]   for a relationship with a subordinate.
[00:59:26.320 --> 00:59:28.320]   To two decades prior.
[00:59:28.320 --> 00:59:32.400]   I mean, the behavior in question for which he may have been
[00:59:32.400 --> 00:59:35.080]   ousted took place in 2000.
[00:59:35.080 --> 00:59:38.720]   And then it came up again as part of the Me Too thing.
[00:59:38.720 --> 00:59:40.640]   In 2019, Microsoft supported-- this
[00:59:40.640 --> 00:59:42.480]   is the New York Times, board of directors,
[00:59:42.480 --> 00:59:45.480]   opened an investigation into one of these cases
[00:59:45.480 --> 00:59:47.720]   being notified that he had sought to initiate an intimate
[00:59:47.720 --> 00:59:49.760]   relationship with the company employee in the year 2000,
[00:59:49.760 --> 00:59:52.600]   20 years earlier.
[00:59:52.600 --> 00:59:55.360]   The board hired a law firm to investigate
[00:59:55.360 --> 00:59:59.120]   and the sense that the Times in the Wall Street Journal had
[00:59:59.120 --> 01:00:01.640]   was that Gates stepped down before the investigation
[01:00:01.640 --> 01:00:02.480]   could get underway.
[01:00:02.480 --> 01:00:04.280]   And then they didn't finish the investigation.
[01:00:04.280 --> 01:00:05.760]   So it sounds like-- I'll tell you what,
[01:00:05.760 --> 01:00:07.840]   I'll leave if you stop this investigation.
[01:00:07.840 --> 01:00:09.680]   That's what it sounds like to me.
[01:00:09.680 --> 01:00:10.240]   I don't know.
[01:00:10.240 --> 01:00:12.640]   I mean, I haven't personally written about this,
[01:00:12.640 --> 01:00:18.960]   but I know a fair number of people at Microsoft.
[01:00:18.960 --> 01:00:20.240]   I'm not sure I can say that.
[01:00:20.240 --> 01:00:22.280]   Ready on the bleeper, but Bill Gates
[01:00:22.280 --> 01:00:24.520]   was an notorious shackhound at Microsoft.
[01:00:24.520 --> 01:00:25.520]   We call it a woman.
[01:00:25.520 --> 01:00:28.120]   That's not profanity in the internet.
[01:00:28.120 --> 01:00:30.320]   Oh, it's funny.
[01:00:30.320 --> 01:00:31.320]   OK, fine.
[01:00:31.320 --> 01:00:32.040]   But I mean, eight's--
[01:00:32.040 --> 01:00:33.880]   It's very awesome how we have it.
[01:00:33.880 --> 01:00:35.200]   P-13, let's say.
[01:00:35.200 --> 01:00:36.240]   He wasn't a tour.
[01:00:36.240 --> 01:00:41.840]   I mean, I know for a fact that he was in a relationship
[01:00:41.840 --> 01:00:44.600]   with his head of PR at one stage.
[01:00:44.600 --> 01:00:45.680]   Well, he was famous for this.
[01:00:45.680 --> 01:00:46.720]   Yeah, he was famous.
[01:00:46.720 --> 01:00:49.800]   But my understanding also was that when they got married,
[01:00:49.800 --> 01:00:52.680]   it was a monogamish relationship.
[01:00:52.680 --> 01:00:56.560]   As in, you know, we are together in public,
[01:00:56.560 --> 01:00:59.360]   but there are excursions outside,
[01:00:59.360 --> 01:01:01.840]   and we both understand this and the rest of this.
[01:01:01.840 --> 01:01:04.560]   I mean, in terms of hissing on your own employees, yeah.
[01:01:04.560 --> 01:01:07.360]   Well, his wife was an employee.
[01:01:07.360 --> 01:01:09.560]   Yeah, she developed Microsoft Bob.
[01:01:09.560 --> 01:01:10.800]   Sorry.
[01:01:10.800 --> 01:01:12.240]   I still get amused by that.
[01:01:12.240 --> 01:01:13.440]   And he still married her.
[01:01:13.440 --> 01:01:15.040]   Let me marry her anyway.
[01:01:15.040 --> 01:01:15.920]   Questionable judgment.
[01:01:15.920 --> 01:01:16.440]   Yeah.
[01:01:16.440 --> 01:01:18.880]   Yeah, but I mean, come on.
[01:01:18.880 --> 01:01:20.000]   I don't know.
[01:01:20.000 --> 01:01:25.000]   I know very few people who haven't had a flinger work.
[01:01:25.000 --> 01:01:27.360]   I'm guilty of that myself.
[01:01:27.360 --> 01:01:30.240]   Admittedly, I was never the CEO, but you know, I mean,
[01:01:30.240 --> 01:01:31.000]   it happens.
[01:01:31.000 --> 01:01:32.640]   A lot of people meet those spouses.
[01:01:32.640 --> 01:01:34.480]   In the MeToo era, it is--
[01:01:34.480 --> 01:01:36.200]   It is suddenly a big problem, right?
[01:01:36.200 --> 01:01:38.960]   I don't think you can retroactively apply this stuff.
[01:01:38.960 --> 01:01:40.200]   I really don't.
[01:01:40.200 --> 01:01:41.080]   You know, I mean, it's--
[01:01:41.080 --> 01:01:42.240]   20 years ago.
[01:01:42.240 --> 01:01:43.040]   Right.
[01:01:43.040 --> 01:01:44.400]   And that's problematic.
[01:01:44.400 --> 01:01:45.000]   There's a lot of--
[01:01:45.000 --> 01:01:49.360]   I mean, there's a lot of Jeffrey Epstein throwing
[01:01:49.360 --> 01:01:51.560]   around around this because apparently--
[01:01:51.560 --> 01:01:54.440]   He continued to solicit donations from Jeffrey Epstein.
[01:01:54.440 --> 01:01:56.680]   Well, and also apparently, there was a disagreement
[01:01:56.680 --> 01:01:59.120]   between Melinda Gates and Bill Gates about his friendship.
[01:01:59.120 --> 01:02:01.440]   And she's like, you got to stop hanging out with this creep.
[01:02:01.440 --> 01:02:03.800]   And he's like, no, no, no, I'm going to go hang out with him.
[01:02:03.800 --> 01:02:04.840]   And so there--
[01:02:04.840 --> 01:02:08.520]   And she apparently went to solicit a divorce
[01:02:08.520 --> 01:02:11.360]   attorney a couple of years ago and so on.
[01:02:11.360 --> 01:02:13.640]   But I personally--
[01:02:13.640 --> 01:02:19.040]   we have to separate what is speculation and what is real.
[01:02:19.040 --> 01:02:21.840]   And most of this stuff is speculation.
[01:02:21.840 --> 01:02:25.120]   I tend to-- I've always been a strong Bill Gates critic
[01:02:25.120 --> 01:02:26.560]   since my Windows Magazine days.
[01:02:26.560 --> 01:02:29.400]   I actually have gotten drunk and partyed with Bill Gates
[01:02:29.400 --> 01:02:30.160]   back in the '90s.
[01:02:30.160 --> 01:02:31.680]   And he was an animal.
[01:02:31.680 --> 01:02:33.400]   But--
[01:02:33.400 --> 01:02:34.320]   But--
[01:02:34.320 --> 01:02:35.840]   Would you like to elaborate on that?
[01:02:35.840 --> 01:02:38.480]   Well, probably Devorax party of Condex.
[01:02:38.480 --> 01:02:39.040]   Oh, yeah.
[01:02:39.040 --> 01:02:40.120]   I've danced with Bill Gates.
[01:02:40.120 --> 01:02:41.600]   Yeah, yeah.
[01:02:41.600 --> 01:02:43.600]   It's-- well--
[01:02:43.600 --> 01:02:44.720]   Across from Bill Gates.
[01:02:44.720 --> 01:02:45.040]   He was--
[01:02:45.040 --> 01:02:46.640]   50% of you was dancing with me.
[01:02:46.640 --> 01:02:47.640]   Yeah, MeToo.
[01:02:47.640 --> 01:02:48.160]   MeToo.
[01:02:48.160 --> 01:02:48.680]   MeToo.
[01:02:48.680 --> 01:02:49.680]   Yeah, yeah.
[01:02:49.680 --> 01:02:52.320]   But the point is that--
[01:02:52.320 --> 01:02:54.160]   I think he was dancing with Spencer the cat,
[01:02:54.160 --> 01:02:54.800]   as I remember.
[01:02:54.800 --> 01:02:57.800]   But it was a crazy time, man.
[01:02:57.800 --> 01:03:00.000]   We were young with the drugs of Froben.
[01:03:00.000 --> 01:03:01.640]   Nobody remembers.
[01:03:01.640 --> 01:03:04.360]   What really matters is that the Bill and Melinda Gates
[01:03:04.360 --> 01:03:09.680]   Foundation is the world's largest philanthropy, I believe.
[01:03:09.680 --> 01:03:12.120]   And it's very important and they do important work.
[01:03:12.120 --> 01:03:14.600]   And I've always been a critic of Bill Gates
[01:03:14.600 --> 01:03:18.240]   because while they do great work as an organization,
[01:03:18.240 --> 01:03:22.160]   his individual ideas have always been often very bad.
[01:03:22.160 --> 01:03:26.040]   Like, he wants to sprinkle metal throughout the world's
[01:03:26.040 --> 01:03:27.840]   atmosphere to cool the planet.
[01:03:27.840 --> 01:03:29.800]   And he wants to put cameras in every classroom.
[01:03:29.800 --> 01:03:33.760]   And he wants to force GMO crops down down the throats
[01:03:33.760 --> 01:03:34.800]   of African farmers.
[01:03:34.800 --> 01:03:37.840]   And he has all kinds of ideas that I personally disagree with.
[01:03:37.840 --> 01:03:40.080]   I find his ideas often very disagreeable.
[01:03:40.080 --> 01:03:44.120]   But we don't know that Jeffrey Epstein's thing.
[01:03:44.120 --> 01:03:45.480]   I think that's a little unfair.
[01:03:45.480 --> 01:03:45.980]   Yeah.
[01:03:45.980 --> 01:03:47.080]   I think that's pushing it.
[01:03:47.080 --> 01:03:47.760]   And also--
[01:03:47.760 --> 01:03:48.680]   I take what you're saying.
[01:03:48.680 --> 01:03:51.240]   I mean, the big thing for me is polio.
[01:03:51.240 --> 01:03:51.960]   Yes.
[01:03:51.960 --> 01:03:55.000]   I mean, the one thing that the foundation has done
[01:03:55.000 --> 01:03:56.680]   is we're nearly there.
[01:03:56.680 --> 01:03:59.280]   Then there are a couple more provinces left to do.
[01:03:59.280 --> 01:04:01.040]   This has been a massive data science problem,
[01:04:01.040 --> 01:04:02.600]   massive vaccination problem.
[01:04:02.600 --> 01:04:04.240]   But we're at the point where we're
[01:04:04.240 --> 01:04:06.720]   killing polio in the same way we killed smallpox.
[01:04:06.720 --> 01:04:07.200]   Amazing.
[01:04:07.200 --> 01:04:10.040]   And the same way we could kill measles, rubella, mumps,
[01:04:10.040 --> 01:04:11.000]   the whole thing.
[01:04:11.000 --> 01:04:12.640]   You know, we could kill the stuff.
[01:04:12.640 --> 01:04:13.840]   He took the money and he did it.
[01:04:13.840 --> 01:04:15.040]   And yeah, you're right.
[01:04:15.040 --> 01:04:17.560]   A lot of his ideas are--
[01:04:17.560 --> 01:04:20.240]   I liked the nuclear power idea you had.
[01:04:20.240 --> 01:04:22.560]   I don't know if you saw the Netflix documentary about him.
[01:04:22.560 --> 01:04:24.160]   But some of those were just like--
[01:04:24.160 --> 01:04:25.040]   That was an absolutely--
[01:04:25.040 --> 01:04:26.040]   That was a puff piece.
[01:04:26.040 --> 01:04:27.040]   Oh, absolutely.
[01:04:27.040 --> 01:04:28.360]   It was--
[01:04:28.360 --> 01:04:29.320]   That was his--
[01:04:29.320 --> 01:04:31.120]   It was a slow hand job to the both of them.
[01:04:31.120 --> 01:04:32.440]   But I mean, it was just--
[01:04:32.440 --> 01:04:34.600]   it was very, very--
[01:04:34.600 --> 01:04:35.960]   isn't this lovely--
[01:04:35.960 --> 01:04:38.440]   I'll be graced and doing the rest of it.
[01:04:38.440 --> 01:04:40.840]   But at the same time, polio.
[01:04:40.840 --> 01:04:41.800]   Right.
[01:04:41.800 --> 01:04:43.080]   I want to-- along that line, but I'd
[01:04:43.080 --> 01:04:43.920]   like to change this up.
[01:04:43.920 --> 01:04:46.640]   So we all agree there's no room here
[01:04:46.640 --> 01:04:50.240]   for discussing the Bill Gates and Melinda Gates divorce.
[01:04:50.240 --> 01:04:51.720]   Moving on.
[01:04:51.720 --> 01:04:53.640]   Although we did it for about 50 hours.
[01:04:53.640 --> 01:04:54.160]   [LAUGHTER]
[01:04:54.160 --> 01:04:56.160]   Other than that, we did it very effectively.
[01:04:56.160 --> 01:04:57.160]   There was no more room.
[01:04:57.160 --> 01:04:58.880]   We dismissed it.
[01:04:58.880 --> 01:05:01.360]   We had a great interview on Friday.
[01:05:01.360 --> 01:05:03.600]   It's on our triangulation and our Twitter events feed
[01:05:03.600 --> 01:05:06.640]   with Andy Weir, the guy who wrote the Martian, his new book.
[01:05:06.640 --> 01:05:07.720]   I was so jealous.
[01:05:07.720 --> 01:05:08.640]   I was so optimistic.
[01:05:08.640 --> 01:05:09.160]   Fantastic.
[01:05:09.160 --> 01:05:10.280]   Project Hail Mary.
[01:05:10.280 --> 01:05:13.160]   But he said something interesting at the end of the interview,
[01:05:13.160 --> 01:05:15.800]   which actually I could have done a whole interview on.
[01:05:15.800 --> 01:05:17.960]   He said, you don't-- you have to--
[01:05:17.960 --> 01:05:20.040]   and remember, this guy is a science fiction author,
[01:05:20.040 --> 01:05:21.760]   but he's a former computer programmer.
[01:05:21.760 --> 01:05:23.880]   He really has-- if you've read the books,
[01:05:23.880 --> 01:05:25.240]   amazing mass--
[01:05:25.240 --> 01:05:26.320]   a grasp of science.
[01:05:26.320 --> 01:05:27.720]   And he's done a lot of research.
[01:05:27.720 --> 01:05:31.400]   He says, you have to understand what we did with the mRNA
[01:05:31.400 --> 01:05:37.080]   vaccines is going to change medicine forever.
[01:05:37.080 --> 01:05:40.600]   And furthermore, he says, this is the last pandemic.
[01:05:40.600 --> 01:05:45.000]   Because we now know how to do a genome assay of whatever
[01:05:45.000 --> 01:05:46.280]   virus is affecting us.
[01:05:46.280 --> 01:05:49.120]   Within weeks, create a vaccination form.
[01:05:49.120 --> 01:05:51.040]   Not just viruses, maybe cancer.
[01:05:51.040 --> 01:05:53.080]   Yeah, because it's genetically based.
[01:05:53.080 --> 01:05:55.560]   If you can attack the cancer directly instead
[01:05:55.560 --> 01:05:57.360]   of all the other cells around it,
[01:05:57.360 --> 01:05:58.880]   you can perhaps treat cancer.
[01:05:58.880 --> 01:05:59.960]   So he was in the opinion.
[01:05:59.960 --> 01:06:02.040]   This is the last pandemic.
[01:06:02.040 --> 01:06:05.480]   I hope he's right, but I fear Mother Nature is perfectly
[01:06:05.480 --> 01:06:08.560]   capable of throwing up some real zingers.
[01:06:08.560 --> 01:06:10.840]   But this is what we've been talking about for so long
[01:06:10.840 --> 01:06:15.880]   is treating the genome, treating genetic treatments
[01:06:15.880 --> 01:06:18.280]   for diseases where you're so targeted.
[01:06:18.280 --> 01:06:22.200]   Because even a single disease might manifest in a variety
[01:06:22.200 --> 01:06:23.560]   of ways in different people.
[01:06:23.560 --> 01:06:28.640]   If I can take the disease in you and make a vaccine that's
[01:06:28.640 --> 01:06:32.120]   purpose-built for you, it's amazing.
[01:06:32.120 --> 01:06:33.680]   The other thing is programmability.
[01:06:33.680 --> 01:06:37.640]   So we're looking at two wonderful things
[01:06:37.640 --> 01:06:39.680]   in the future of pandemic response.
[01:06:39.680 --> 01:06:44.880]   One is imagine a home device that can give you a test.
[01:06:44.880 --> 01:06:48.480]   And you can download the latest virus data,
[01:06:48.480 --> 01:06:51.480]   take a test in the home, and know if you're positive
[01:06:51.480 --> 01:06:52.880]   every morning when you're brushing your teeth,
[01:06:52.880 --> 01:06:55.480]   whether you're positive or negative for the current pandemic
[01:06:55.480 --> 01:06:56.320]   virus.
[01:06:56.320 --> 01:06:59.400]   And so the reason, I mean, just to be clear about the damage
[01:06:59.400 --> 01:07:02.600]   that this pandemic has caused, the reason we had to lock down
[01:07:02.600 --> 01:07:04.400]   for all of this time-- what is it?
[01:07:04.400 --> 01:07:06.800]   A year, year and a half, whatever it's been--
[01:07:06.800 --> 01:07:09.680]   was because we don't know who's positive and who's not.
[01:07:09.680 --> 01:07:11.160]   That's the only reason.
[01:07:11.160 --> 01:07:15.120]   If we knew for sure who was positive and who was negative,
[01:07:15.120 --> 01:07:16.760]   the positive people could stay home
[01:07:16.760 --> 01:07:18.160]   and do what we've all been doing.
[01:07:18.160 --> 01:07:20.320]   And the negative people could have been continuing
[01:07:20.320 --> 01:07:22.920]   to live their lives as normal.
[01:07:22.920 --> 01:07:24.120]   So that testing is huge.
[01:07:24.120 --> 01:07:26.560]   The other thing is something similar on the vaccine front.
[01:07:26.560 --> 01:07:30.920]   They'll be able to quickly program new vaccines for new
[01:07:30.920 --> 01:07:31.960]   variants.
[01:07:31.960 --> 01:07:35.400]   It'll take weeks instead of years to develop these things.
[01:07:35.400 --> 01:07:37.080]   And then they'll have to test them and then they'll roll out
[01:07:37.080 --> 01:07:39.000]   and be much quicker thanks to this technology.
[01:07:39.000 --> 01:07:39.880]   It's really--
[01:07:39.880 --> 01:07:40.800]   It's very exciting.
[01:07:40.800 --> 01:07:41.280]   He's right.
[01:07:41.280 --> 01:07:42.360]   It's huge.
[01:07:42.360 --> 01:07:44.520]   Although Whitfield Diffia, RSA this year,
[01:07:44.520 --> 01:07:48.280]   raised a very interesting point where this kind of technology
[01:07:48.280 --> 01:07:50.240]   where you have that kind of implanted technology
[01:07:50.240 --> 01:07:54.040]   and you have that kind of level of stuff poses huge privacy
[01:07:54.040 --> 01:07:54.960]   risks.
[01:07:54.960 --> 01:07:58.080]   And he was basically saying, in 10 years,
[01:07:58.080 --> 01:08:01.440]   the very concept of privacy is going to be gone.
[01:08:01.440 --> 01:08:03.000]   Genetic privacy, particularly.
[01:08:03.000 --> 01:08:06.320]   Yeah, because not because people are being forced to,
[01:08:06.320 --> 01:08:08.480]   because you won't be competitive unless you--
[01:08:08.480 --> 01:08:09.280]   You want to do it.
[01:08:09.280 --> 01:08:10.360]   Yeah, exactly.
[01:08:10.360 --> 01:08:11.840]   You won't be competitive unless you've
[01:08:11.840 --> 01:08:14.920]   got a virus monitor, a personal communicator
[01:08:14.920 --> 01:08:16.880]   which is built into you.
[01:08:16.880 --> 01:08:18.600]   You're going to have to do this stuff.
[01:08:18.600 --> 01:08:19.680]   And it's a matter of choice.
[01:08:19.680 --> 01:08:22.920]   It's a matter of if you don't do it, then, yeah,
[01:08:22.920 --> 01:08:25.080]   we always need someone to dig ditches.
[01:08:25.080 --> 01:08:25.600]   Right.
[01:08:25.600 --> 01:08:27.360]   Whitens the gap between the haves and the haves.
[01:08:27.360 --> 01:08:27.880]   Exactly.
[01:08:27.880 --> 01:08:29.000]   And you have that access.
[01:08:29.000 --> 01:08:30.000]   Yeah.
[01:08:30.000 --> 01:08:32.040]   And that's one of the things we talk about all the time
[01:08:32.040 --> 01:08:34.800]   in these shows is the hazards of new technologies.
[01:08:34.800 --> 01:08:37.120]   But it's also important to talk about the potential benefits.
[01:08:37.120 --> 01:08:38.160]   Yes, absolutely.
[01:08:38.160 --> 01:08:42.720]   And with eyes wide open, consider the risks
[01:08:42.720 --> 01:08:45.320]   and do something about it so that we can have these technologies
[01:08:45.320 --> 01:08:48.320]   and still be civilized.
[01:08:48.320 --> 01:08:48.600]   Right.
[01:08:48.600 --> 01:08:50.760]   And I think it'll take us a next leap forward.
[01:08:50.760 --> 01:08:53.960]   200 years ago, you had five kids and three of them
[01:08:53.960 --> 01:08:55.800]   died by the time they were 10.
[01:08:55.800 --> 01:08:57.000]   That was normal.
[01:08:57.000 --> 01:09:00.080]   Now we have kids and we all have a reasonable expectation
[01:09:00.080 --> 01:09:01.800]   that they'll all survive to adulthood.
[01:09:01.800 --> 01:09:02.800]   It's a miracle.
[01:09:02.800 --> 01:09:03.280]   I think--
[01:09:03.280 --> 01:09:04.760]   So stop having five kids, folks.
[01:09:04.760 --> 01:09:05.260]   Yeah.
[01:09:05.260 --> 01:09:06.960]   You don't have to do it anymore.
[01:09:06.960 --> 01:09:07.800]   You want the two kids?
[01:09:07.800 --> 01:09:09.800]   Just have two kids.
[01:09:09.800 --> 01:09:10.480]   That's dark.
[01:09:10.480 --> 01:09:11.000]   That's so dark.
[01:09:11.000 --> 01:09:12.160]   That was really dark.
[01:09:12.160 --> 01:09:12.680]   Sorry.
[01:09:12.680 --> 01:09:13.760]   But no judgment.
[01:09:13.760 --> 01:09:15.960]   I think this technology is going to be part of an--
[01:09:15.960 --> 01:09:18.400]   and also AI stuff, which--
[01:09:18.400 --> 01:09:20.640]   and not to lead into more Google News.
[01:09:20.640 --> 01:09:23.800]   But AI-based diagnoses and so on
[01:09:23.800 --> 01:09:26.280]   is going to take us to the next level where
[01:09:26.280 --> 01:09:29.160]   surviving to the age of 100 is going to be pretty normal.
[01:09:29.160 --> 01:09:29.880]   Well, that's--
[01:09:29.880 --> 01:09:32.560]   of course, that's been my plan all along.
[01:09:32.560 --> 01:09:33.800]   And that's what Ray Kurzweil said.
[01:09:33.800 --> 01:09:35.760]   I just want to live long enough to live forever.
[01:09:35.760 --> 01:09:36.760]   Yes.
[01:09:36.760 --> 01:09:37.760]   Exactly.
[01:09:37.760 --> 01:09:40.200]   Oh, I can just get to that next plateau.
[01:09:40.200 --> 01:09:40.960]   I don't know.
[01:09:40.960 --> 01:09:42.760]   If I could upload myself, I'd be fine.
[01:09:42.760 --> 01:09:43.920]   But honestly, I'm fine.
[01:09:43.920 --> 01:09:44.920]   Is it you?
[01:09:44.920 --> 01:09:45.600]   This is always to me.
[01:09:45.600 --> 01:09:47.000]   Well, this is the problem.
[01:09:47.000 --> 01:09:49.320]   The philosophical question is, OK, there's
[01:09:49.320 --> 01:09:51.480]   a duplicate of you.
[01:09:51.480 --> 01:09:53.200]   And actually, this was something Neil
[01:09:53.200 --> 01:09:55.520]   Stevenson wrote about in his last book, The Fall,
[01:09:55.520 --> 01:09:58.280]   is what you don't duplicate the sound--
[01:09:58.280 --> 01:09:59.760]   It's your current mind state.
[01:09:59.760 --> 01:10:04.120]   You don't do the connect-- what he called the connectome,
[01:10:04.120 --> 01:10:06.680]   which is the relationships between the axons
[01:10:06.680 --> 01:10:09.840]   and the dendrites and all the cells, not the cells themselves.
[01:10:09.840 --> 01:10:11.640]   You don't make a duplicate of your brain.
[01:10:11.640 --> 01:10:14.080]   You make a duplicate of the connections your brain has made.
[01:10:14.080 --> 01:10:16.520]   Neil Stevenson is very, very strong on this stuff
[01:10:16.520 --> 01:10:18.280]   because he thinks a lot about it.
[01:10:18.280 --> 01:10:18.760]   I did.
[01:10:18.760 --> 01:10:20.040]   But is that you?
[01:10:20.040 --> 01:10:21.720]   Or is it just some--
[01:10:21.720 --> 01:10:22.520]   I'm a wog, honestly.
[01:10:22.520 --> 01:10:24.320]   Honestly, I'm a wog.
[01:10:24.320 --> 01:10:26.040]   This has been-- lockdown's been hard
[01:10:26.040 --> 01:10:27.720]   because I haven't seen my family for two years.
[01:10:27.720 --> 01:10:31.920]   But my mom's 86, and she's still playing golf most weekends.
[01:10:31.920 --> 01:10:33.640]   Everything's perfectly fair enough.
[01:10:33.640 --> 01:10:36.920]   But the older we get, the longer we're going to be kept going,
[01:10:36.920 --> 01:10:39.960]   as long as you've got the money and the wear with all.
[01:10:39.960 --> 01:10:42.920]   You know, we all got to die sometime.
[01:10:42.920 --> 01:10:44.320]   It's still going to be a problem.
[01:10:44.320 --> 01:10:45.640]   And you'll probably--
[01:10:45.640 --> 01:10:47.080]   You'll probably be ready for it.
[01:10:47.080 --> 01:10:48.040]   It's an interesting idea.
[01:10:48.040 --> 01:10:50.440]   Uploading yourself seems like something you do for others
[01:10:50.440 --> 01:10:51.960]   now for yourself.
[01:10:51.960 --> 01:10:53.560]   I see it as an issue.
[01:10:53.560 --> 01:10:55.480]   Yeah, something I do to torture my children.
[01:10:55.480 --> 01:10:56.960]   That's right.
[01:10:56.960 --> 01:10:58.160]   To harm them?
[01:10:58.160 --> 01:10:59.360]   I mean, I'm inheriting you.
[01:10:59.360 --> 01:11:00.840]   I'm just trying to get the money.
[01:11:00.840 --> 01:11:01.880]   I'm uploading my money.
[01:11:01.880 --> 01:11:02.400]   Legality.
[01:11:02.400 --> 01:11:03.200]   I'm not gone.
[01:11:03.200 --> 01:11:05.480]   Are your children going to feel duty-bound once a week
[01:11:05.480 --> 01:11:08.640]   to open the interface and just be like, sort of--
[01:11:08.640 --> 01:11:09.760]   So how is it?
[01:11:09.760 --> 01:11:11.480]   How is it in cyberspace, Dad?
[01:11:11.480 --> 01:11:11.960]   It's great.
[01:11:11.960 --> 01:11:12.480]   Yeah.
[01:11:12.480 --> 01:11:12.800]   OK.
[01:11:12.800 --> 01:11:15.320]   You should read the fall because that actually comes up.
[01:11:15.320 --> 01:11:15.800]   Really?
[01:11:15.800 --> 01:11:17.160]   That actually comes up.
[01:11:17.160 --> 01:11:17.920]   It's fascinating.
[01:11:17.920 --> 01:11:21.560]   I love to hear them banks' idea where you back yourself up
[01:11:21.560 --> 01:11:24.640]   on a regular basis as like an insurance policy.
[01:11:24.640 --> 01:11:26.720]   And then you can choose which version you want.
[01:11:26.720 --> 01:11:29.120]   You can buy in 5'11" or in 5'12".
[01:11:29.120 --> 01:11:31.440]   Like Google Docs, you can go back several revisions.
[01:11:31.440 --> 01:11:32.560]   Yeah, it takes a while.
[01:11:32.560 --> 01:11:33.560]   Just like a--
[01:11:33.560 --> 01:11:35.160]   Yeah, I go for Ian 14.
[01:11:35.160 --> 01:11:38.440]   You're all the name 25 at this point because I was a complete--
[01:11:38.440 --> 01:11:40.920]   never mind.
[01:11:40.920 --> 01:11:41.880]   Let's take a little break.
[01:11:41.880 --> 01:11:42.480]   We have more.
[01:11:42.480 --> 01:11:45.840]   What a great-- it's so much fun to have an in-studio panel.
[01:11:45.840 --> 01:11:46.960]   So glad to have you, Mike.
[01:11:46.960 --> 01:11:50.160]   Elgin.elgin.com, gastronomad.net.
[01:11:50.160 --> 01:11:51.040]   So glad to be here.
[01:11:51.040 --> 01:11:52.840]   When do you leave country?
[01:11:52.840 --> 01:11:53.840]   Week and half.
[01:11:53.840 --> 01:11:55.240]   Are you looking forward to it?
[01:11:55.240 --> 01:11:56.480]   Yes, we're going to Spain.
[01:11:56.480 --> 01:11:58.600]   Then if they'll let us in, Portugal,
[01:11:58.600 --> 01:12:00.040]   and then we're going to Vermont.
[01:12:00.040 --> 01:12:02.040]   That's part of the problem with traveling right now.
[01:12:02.040 --> 01:12:03.040]   Will they let us in?
[01:12:03.040 --> 01:12:04.640]   What do I need to do to get in?
[01:12:04.640 --> 01:12:06.280]   Well, on the technological front,
[01:12:06.280 --> 01:12:09.160]   I think vaccine passports are coming.
[01:12:09.160 --> 01:12:10.880]   They'll call it health passes.
[01:12:10.880 --> 01:12:11.360]   But I think--
[01:12:11.360 --> 01:12:13.120]   Are there going to be apps or the pieces of paper?
[01:12:13.120 --> 01:12:14.440]   There'll be apps mostly.
[01:12:14.440 --> 01:12:16.120]   They also have a paper version for people
[01:12:16.120 --> 01:12:18.560]   who don't want to use the apps that QR code-based.
[01:12:18.560 --> 01:12:21.600]   But-- and Europe is leading the charge on this.
[01:12:21.600 --> 01:12:23.600]   But I think that health passports
[01:12:23.600 --> 01:12:25.800]   will be a permanent fixture of travel.
[01:12:25.800 --> 01:12:26.600]   You're going to have to be.
[01:12:26.600 --> 01:12:30.400]   Whatever, I don't have this, I don't have that.
[01:12:30.400 --> 01:12:31.240]   I've had this vaccine.
[01:12:31.240 --> 01:12:32.520]   That's another lesson we've learned.
[01:12:32.520 --> 01:12:34.040]   Is that air travel has made it possible
[01:12:34.040 --> 01:12:36.440]   for these plagues to spread--
[01:12:36.440 --> 01:12:37.440]   That's right.
[01:12:37.440 --> 01:12:38.760]   That's right.
[01:12:38.760 --> 01:12:41.960]   Although, I mean, I used to be an aviation journalist,
[01:12:41.960 --> 01:12:44.280]   the air filtration system they have on planes.
[01:12:44.280 --> 01:12:45.480]   It's not the best spread on the plane.
[01:12:45.480 --> 01:12:46.480]   It's not the best spread on the plane.
[01:12:46.480 --> 01:12:48.920]   It's when you go to the airport and then mingle with everyone
[01:12:48.920 --> 01:12:50.080]   and go out of the air.
[01:12:50.080 --> 01:12:51.720]   I'm not worried about the plane at all.
[01:12:51.720 --> 01:12:53.240]   No, on the plane itself, you'll
[01:12:53.240 --> 01:12:54.240]   be pretty good.
[01:12:54.240 --> 01:12:55.840]   It's pretty for them you would be on the street, initially.
[01:12:55.840 --> 01:12:56.840]   Yeah.
[01:12:56.840 --> 01:12:57.840]   That's true, in general.
[01:12:57.840 --> 01:13:01.520]   The most dangerous part of flying is the drive to the airport.
[01:13:01.520 --> 01:13:03.960]   Well, I was going to say, I mean, in terms of air crashes,
[01:13:03.960 --> 01:13:05.960]   the most dangerous is the first and last five minutes.
[01:13:05.960 --> 01:13:06.480]   Right.
[01:13:06.480 --> 01:13:07.840]   Especially in first.
[01:13:07.840 --> 01:13:10.800]   I mean, when I was doing it-- this is horrible.
[01:13:10.800 --> 01:13:12.920]   I've gone to-- when I was doing the aviation journalism
[01:13:12.920 --> 01:13:17.960]   job, got onto the aircraft for a 14-hour flight to Indonesia.
[01:13:17.960 --> 01:13:19.800]   And he looked at my shoes and he was,
[01:13:19.800 --> 01:13:22.040]   I mean, wearing running shoes, you should always
[01:13:22.040 --> 01:13:24.040]   wear leather shoes on a plane.
[01:13:24.040 --> 01:13:25.040]   It's like, well, why is that?
[01:13:25.040 --> 01:13:27.160]   It's like, if you've got to run through burning jet fuel,
[01:13:27.160 --> 01:13:28.640]   they're going to have to fry those.
[01:13:28.640 --> 01:13:29.640]   I'll fry your fish.
[01:13:29.640 --> 01:13:32.040]   It's like, I've got that information.
[01:13:32.040 --> 01:13:33.760]   And then he pulls the kiss to your Oscar
[01:13:33.760 --> 01:13:34.920]   by folder out the seat.
[01:13:34.920 --> 01:13:35.920]   Oh, Lord.
[01:13:35.920 --> 01:13:38.720]   And you know that picture with a plane on the sea,
[01:13:38.720 --> 01:13:40.520]   with all the little life rothering?
[01:13:40.520 --> 01:13:41.360]   You see that?
[01:13:41.360 --> 01:13:43.720]   Never happened in 100 years of aviation.
[01:13:43.720 --> 01:13:47.360]   You cannot land an aircraft on open water, you know?
[01:13:47.360 --> 01:13:48.520]   And keep it in more peace.
[01:13:48.520 --> 01:13:51.200]   It never-- I hope I don't ever get that.
[01:13:51.200 --> 01:13:52.360]   It's a slight attendant.
[01:13:52.360 --> 01:13:53.640]   That sounds like a scary person.
[01:13:53.640 --> 01:13:54.480]   This one was boss.
[01:13:54.480 --> 01:13:55.640]   I couldn't tell him to show you that.
[01:13:55.640 --> 01:13:56.640]   Oh, he knew that stuff.
[01:13:56.640 --> 01:13:57.480]   Oh, I was going to say that.
[01:13:57.480 --> 01:13:58.840]   He wore leather shoes.
[01:13:58.840 --> 01:14:01.920]   That was the last day of the job, by the way.
[01:14:01.920 --> 01:14:03.000]   Go to Mel's, on your feet.
[01:14:03.000 --> 01:14:05.720]   That's the intomson, the register.com.
[01:14:05.720 --> 01:14:07.120]   You now have a dot-com address.
[01:14:07.120 --> 01:14:08.360]   He's news editor.
[01:14:08.360 --> 01:14:10.600]   There's great to have you along.
[01:14:10.600 --> 01:14:11.520]   It's good to be back.
[01:14:11.520 --> 01:14:12.720]   You feel first.
[01:14:12.720 --> 01:14:13.920]   Almost normal again.
[01:14:13.920 --> 01:14:14.800]   Almost.
[01:14:14.800 --> 01:14:15.960]   Yeah.
[01:14:15.960 --> 01:14:19.200]   Already, he said 15 completely rude things.
[01:14:19.200 --> 01:14:21.920]   But no one knew because--
[01:14:21.920 --> 01:14:22.920]   Rude somewhere.
[01:14:22.920 --> 01:14:23.680]   Yeah, we're rude.
[01:14:23.680 --> 01:14:24.240]   Maybe not here.
[01:14:24.240 --> 01:14:25.720]   Yeah, not here.
[01:14:25.720 --> 01:14:29.640]   And of course, Jason Hall from all about Android and Tech News
[01:14:29.640 --> 01:14:33.880]   Weekly and our producer, who is producing as you do the show.
[01:14:33.880 --> 01:14:35.400]   How's that working out for you?
[01:14:35.400 --> 01:14:36.920]   You know, I've cut my job in half.
[01:14:36.920 --> 01:14:38.640]   I'm just numbering the stories.
[01:14:38.640 --> 01:14:39.920]   Have some more of this, Kel.
[01:14:39.920 --> 01:14:42.240]   And we'll see what happens.
[01:14:42.240 --> 01:14:43.000]   Oh, I filled up.
[01:14:43.000 --> 01:14:44.080]   I just take it by time.
[01:14:44.080 --> 01:14:44.920]   You've got to try this.
[01:14:44.920 --> 01:14:46.280]   It's so good.
[01:14:46.280 --> 01:14:47.480]   All right, OK.
[01:14:47.480 --> 01:14:49.960]   Arshad, are they brought to you by Udacity?
[01:14:49.960 --> 01:14:52.400]   Are you looking to take your tech skills to the next level?
[01:14:52.400 --> 01:14:55.720]   Udacity was started by a Googler, Sebastian Thrun.
[01:14:55.720 --> 01:14:57.960]   Thrun realized, working at Google,
[01:14:57.960 --> 01:15:00.080]   that you get people applying for jobs
[01:15:00.080 --> 01:15:02.600]   that have got great degrees from big universities,
[01:15:02.600 --> 01:15:04.560]   but they don't have the skills that you
[01:15:04.560 --> 01:15:06.520]   need to work at Google.
[01:15:06.520 --> 01:15:09.440]   He realized there were a lot of big companies, tech companies,
[01:15:09.440 --> 01:15:13.160]   especially, that needed people but couldn't
[01:15:13.160 --> 01:15:16.400]   get them from the traditional education system.
[01:15:16.400 --> 01:15:19.600]   And so he created Udacity, the world's fastest, most efficient
[01:15:19.600 --> 01:15:24.400]   way to master the skills top employers are looking for.
[01:15:24.400 --> 01:15:27.440]   And you can do it part time while you're doing another job.
[01:15:27.440 --> 01:15:29.360]   You can do it the night if you want.
[01:15:29.360 --> 01:15:30.800]   You could do it anywhere in the world.
[01:15:30.800 --> 01:15:36.280]   In fact, I love this 14 million people in over 240 countries
[01:15:36.280 --> 01:15:38.040]   now use Udacity.
[01:15:38.040 --> 01:15:39.800]   It is a massive success.
[01:15:39.800 --> 01:15:44.240]   This is exactly, to me, what's so great about technology.
[01:15:44.240 --> 01:15:47.760]   You can get a nano degree in just five or 10 hours a week
[01:15:47.760 --> 01:15:49.840]   for as little as three months, get
[01:15:49.840 --> 01:15:52.160]   a suite of employable skills.
[01:15:52.160 --> 01:15:54.400]   Udacity is partnering with the biggest companies
[01:15:54.400 --> 01:15:58.920]   in the industry, Microsoft, Google, IBM, Amazon.
[01:15:58.920 --> 01:16:02.080]   These are companies that know what they need,
[01:16:02.080 --> 01:16:04.680]   and they help design these nano degree programs
[01:16:04.680 --> 01:16:07.800]   so that they can get the brains they need.
[01:16:07.800 --> 01:16:10.120]   Now, Udacity, let me give you a little idea
[01:16:10.120 --> 01:16:12.600]   of what it's like to take a course at Udacity.
[01:16:12.600 --> 01:16:18.440]   There's IBM Watson, Facebook, DD, BMW also, AT&T.
[01:16:18.440 --> 01:16:21.440]   You'll, of course, like a lot of online systems,
[01:16:21.440 --> 01:16:23.640]   they have video courses that you can take
[01:16:23.640 --> 01:16:26.360]   at your leisure and your own time.
[01:16:26.360 --> 01:16:28.840]   But what I love about Udacity is they go beyond
[01:16:28.840 --> 01:16:29.840]   that online course.
[01:16:29.840 --> 01:16:32.600]   They have projects, and this is so important
[01:16:32.600 --> 01:16:35.400]   that they realized it's absolutely true.
[01:16:35.400 --> 01:16:37.320]   It's one thing to learn something in a classroom.
[01:16:37.320 --> 01:16:39.480]   It's another thing to actually do it.
[01:16:39.480 --> 01:16:42.440]   And so as part of your course offering,
[01:16:42.440 --> 01:16:43.880]   you're gonna be doing projects.
[01:16:43.880 --> 01:16:45.680]   Now, you're not doing it alone.
[01:16:45.680 --> 01:16:48.240]   The projects will be reviewed by qualified professionals
[01:16:48.240 --> 01:16:49.080]   every step of the way.
[01:16:49.080 --> 01:16:53.040]   You'll have human help, you've personalized code reviews,
[01:16:53.040 --> 01:16:55.480]   and you even have access to mentors 24/7.
[01:16:55.480 --> 01:16:56.880]   So it's really great.
[01:16:56.880 --> 01:16:59.800]   You are working with people who are already working
[01:16:59.800 --> 01:17:00.760]   in the field.
[01:17:00.760 --> 01:17:03.000]   You're doing something that's real.
[01:17:03.000 --> 01:17:06.000]   You're learning it, but you're also doing it.
[01:17:06.000 --> 01:17:07.240]   And I think that's so important.
[01:17:07.240 --> 01:17:09.920]   You'll get support courses too that are designed
[01:17:09.920 --> 01:17:11.640]   to help you develop important skills
[01:17:11.640 --> 01:17:12.680]   for getting that job.
[01:17:12.680 --> 01:17:15.680]   They'll help you polish your LinkedIn profile
[01:17:15.680 --> 01:17:17.440]   to show prospective employers.
[01:17:17.440 --> 01:17:19.160]   They have classes on networking
[01:17:19.160 --> 01:17:22.640]   and how to get noticed and land the job you want.
[01:17:22.640 --> 01:17:26.360]   So what are the most sought after Udacity nanodegrees
[01:17:26.360 --> 01:17:28.560]   for both consumers and business clients alike?
[01:17:28.560 --> 01:17:30.640]   The top among the top 10 data engineering,
[01:17:30.640 --> 01:17:33.640]   I guess you could probably figure that data analyst,
[01:17:33.640 --> 01:17:37.680]   project manager, C++, still huge digital marketing.
[01:17:37.680 --> 01:17:39.960]   I've taken the Python, some of the Python courses there.
[01:17:39.960 --> 01:17:41.480]   They're excellent.
[01:17:41.480 --> 01:17:43.080]   They're excellent.
[01:17:43.080 --> 01:17:46.840]   You can choose from the program you want to fit the technique
[01:17:46.840 --> 01:17:48.360]   that you really speaks to you.
[01:17:48.360 --> 01:17:50.640]   AI, cloud computing.
[01:17:50.640 --> 01:17:53.200]   I know some people want to learn how to build
[01:17:53.200 --> 01:17:54.080]   self-driving cars.
[01:17:54.080 --> 01:17:55.080]   That's hot right now.
[01:17:55.080 --> 01:17:58.280]   Yes, they have autonomous system courses,
[01:17:58.280 --> 01:18:00.040]   data science courses, of course,
[01:18:00.040 --> 01:18:02.680]   all the programming languages.
[01:18:02.680 --> 01:18:05.880]   It really is going to help you get a job, get that.
[01:18:05.880 --> 01:18:06.720]   I'll have an example.
[01:18:06.720 --> 01:18:08.760]   In fact, there's a bunch of them on the website.
[01:18:08.760 --> 01:18:13.760]   As an example, Francisco Gutierrez, he loved tech.
[01:18:13.760 --> 01:18:15.000]   There's probably many of you do,
[01:18:15.000 --> 01:18:17.840]   but he couldn't afford a four-year degree.
[01:18:17.840 --> 01:18:19.520]   He really wanted a job in tech.
[01:18:19.520 --> 01:18:23.400]   He participated in Udacity's Grow with Google Challenge.
[01:18:23.400 --> 01:18:25.200]   Was awarded a full scholarship
[01:18:25.200 --> 01:18:27.160]   for the mobile web specialist nanodegree.
[01:18:27.160 --> 01:18:28.880]   He got the nanodegree.
[01:18:28.880 --> 01:18:30.160]   After going through the program,
[01:18:30.160 --> 01:18:31.800]   he got an internship with Microsoft.
[01:18:31.800 --> 01:18:34.000]   He is now, they offered him a full-time job.
[01:18:34.000 --> 01:18:36.120]   He's now working full-time at Microsoft
[01:18:36.120 --> 01:18:37.400]   as a software engineer,
[01:18:37.400 --> 01:18:38.800]   all because of Udacity.
[01:18:38.800 --> 01:18:40.240]   His dreams are coming true.
[01:18:40.240 --> 01:18:42.400]   Your should too.
[01:18:42.400 --> 01:18:44.080]   They offer flexible payment options.
[01:18:44.080 --> 01:18:45.560]   You can learn at your own pace and schedule.
[01:18:45.560 --> 01:18:48.720]   They know a lot of the 14 million people
[01:18:48.720 --> 01:18:50.720]   who have taken Udacity courses are doing it
[01:18:50.720 --> 01:18:52.640]   while they have other jobs.
[01:18:52.640 --> 01:18:54.880]   We know technology is disrupting enterprises
[01:18:54.880 --> 01:18:56.560]   across every industry.
[01:18:56.560 --> 01:19:00.320]   If you have employees that you want to upskill,
[01:19:00.320 --> 01:19:02.160]   Udacity for enterprises for you.
[01:19:02.160 --> 01:19:03.800]   You can get them the training.
[01:19:03.800 --> 01:19:05.760]   You need to make them better employees.
[01:19:05.760 --> 01:19:06.600]   They love it too.
[01:19:06.600 --> 01:19:08.640]   It's a wonderful benefit for your company.
[01:19:08.640 --> 01:19:10.400]   Check out the website.
[01:19:10.400 --> 01:19:12.680]   Get the in-demand tech skills you need to advance
[01:19:12.680 --> 01:19:15.520]   your career, visit udacity.com/twit.
[01:19:15.520 --> 01:19:20.040]   Get 75% off, 75% off.
[01:19:20.040 --> 01:19:23.280]   Now you've got to do this by June 30th, 2021.
[01:19:23.280 --> 01:19:25.680]   So if you're on the fence, hop off the fence
[01:19:25.680 --> 01:19:28.360]   and go to udacity.com/twit.
[01:19:28.360 --> 01:19:29.720]   The offer code for this,
[01:19:29.720 --> 01:19:32.720]   twit 75, that is a huge discount.
[01:19:32.720 --> 01:19:37.600]   Twit 75 at udacity.com/twit.
[01:19:37.600 --> 01:19:40.080]   Thank you, Udacity, for all you're doing,
[01:19:40.080 --> 01:19:42.080]   really to help people get into this business
[01:19:42.080 --> 01:19:44.200]   and for supporting this week in tech.
[01:19:44.200 --> 01:19:45.080]   We really appreciate it.
[01:19:45.080 --> 01:19:48.760]   Thank you, dear listener, for using that address.
[01:19:48.760 --> 01:19:49.880]   They know that you saw it here.
[01:19:49.880 --> 01:19:54.880]   Udacity.com/twit offer code is twit 75.
[01:19:54.880 --> 01:19:59.920]   John, do we have a best of from this week?
[01:19:59.920 --> 01:20:01.880]   All right, let's see it.
[01:20:01.880 --> 01:20:04.000]   This is what you missed this week on Twit.
[01:20:04.000 --> 01:20:05.680]   Tell me about the process of making this
[01:20:05.680 --> 01:20:07.920]   'cause it has been a while since Artemis.
[01:20:07.920 --> 01:20:11.600]   Well, I got this big pile of buttons here, right?
[01:20:11.600 --> 01:20:12.440]   Yes.
[01:20:12.440 --> 01:20:13.280]   And if you hit these buttons,
[01:20:13.280 --> 01:20:14.120]   better show up.
[01:20:14.120 --> 01:20:15.000]   (laughing)
[01:20:15.000 --> 01:20:16.640]   And then if you do them in the right sequence,
[01:20:16.640 --> 01:20:17.840]   you get a book.
[01:20:17.840 --> 01:20:20.600]   Previously on Twit.
[01:20:20.600 --> 01:20:22.360]   Twit Live Events.
[01:20:22.360 --> 01:20:26.120]   He's got a notebook and it is a Corker Project,
[01:20:26.120 --> 01:20:28.600]   Hail, Mary, Andy Weir.
[01:20:28.600 --> 01:20:29.680]   Welcome back.
[01:20:29.680 --> 01:20:30.680]   It's great to be here.
[01:20:30.680 --> 01:20:32.880]   Boy, this really is an era of reboots, isn't it?
[01:20:32.880 --> 01:20:33.920]   (laughing)
[01:20:33.920 --> 01:20:35.200]   iOS today.
[01:20:35.200 --> 01:20:37.600]   We have got quite an exciting show for you
[01:20:37.600 --> 01:20:42.000]   because Rosemary Orchard has her iPad Pro, the new one,
[01:20:42.000 --> 01:20:46.080]   and we both have our new Siri remotes.
[01:20:46.080 --> 01:20:47.360]   All about Android.
[01:20:47.360 --> 01:20:50.080]   We found out at Google I/O,
[01:20:50.080 --> 01:20:52.600]   where OS is getting a new life.
[01:20:52.600 --> 01:20:55.400]   Google has aligned with Samsung.
[01:20:55.400 --> 01:20:58.320]   And so they're effectively combining Wear OS
[01:20:58.320 --> 01:21:02.040]   and Samsung's efforts with Tizen OS.
[01:21:02.040 --> 01:21:03.840]   Mac Break Weekly.
[01:21:03.840 --> 01:21:07.000]   Apple has announced lossless audio
[01:21:07.000 --> 01:21:11.840]   for their entire catalog, 75 million on free.
[01:21:11.840 --> 01:21:13.560]   The bad news is it doesn't work on any
[01:21:13.560 --> 01:21:16.600]   of their Bluetooth devices because Bluetooth sucks.
[01:21:16.600 --> 01:21:19.160]   If you are happy with spam,
[01:21:19.160 --> 01:21:21.000]   enjoy the heck out of that spam
[01:21:21.000 --> 01:21:23.320]   and stay away from A5WogU
[01:21:23.320 --> 01:21:25.680]   because A5WogU's acres are $400.
[01:21:25.680 --> 01:21:26.800]   It will ruin your life.
[01:21:26.800 --> 01:21:27.640]   I agree with you.
[01:21:27.640 --> 01:21:28.920]   It will ruin your life.
[01:21:28.920 --> 01:21:30.200]   It will be still.
[01:21:30.200 --> 01:21:32.720]   - To it, it's watch for dinner.
[01:21:32.720 --> 01:21:35.320]   - I hope it's, I hope it's better for dinner than to it.
[01:21:35.320 --> 01:21:37.680]   But if that's what you're eating, we're glad you're here.
[01:21:37.680 --> 01:21:39.280]   It's kind of a liquid dinner.
[01:21:39.280 --> 01:21:40.120]   - Yes.
[01:21:40.120 --> 01:21:40.960]   - He thinks to Mike Elgin.
[01:21:40.960 --> 01:21:42.560]   - Yeah, this is gonna have to be the last one though
[01:21:42.560 --> 01:21:44.240]   'cause I've got to drive back and I do it.
[01:21:44.240 --> 01:21:47.760]   - Oh, the last one, I'm on my first still.
[01:21:47.760 --> 01:21:48.920]   What do you mean the last one?
[01:21:48.920 --> 01:21:50.680]   - I actually topped you off.
[01:21:50.680 --> 01:21:51.520]   - Oh you did?
[01:21:51.520 --> 01:21:52.360]   - You were talking something.
[01:21:52.360 --> 01:21:54.320]   - You were talking to Adam or something.
[01:21:54.320 --> 01:21:55.920]   A little stronger than I thought.
[01:21:55.920 --> 01:21:58.040]   - Microsoft, you had this article in Microsoft
[01:21:58.040 --> 01:21:59.560]   is killing Windows 10X.
[01:21:59.560 --> 01:22:02.640]   - Oh yes, yeah I'm happy to talk about that.
[01:22:02.640 --> 01:22:03.920]   That was just like, what's--
[01:22:03.920 --> 01:22:05.240]   - I was excited about 10X.
[01:22:05.240 --> 01:22:07.480]   The idea was kind of a redesign of Windows
[01:22:07.480 --> 01:22:09.400]   that's containerized, more robust.
[01:22:09.400 --> 01:22:11.720]   - It's like, oh, it's like 10S.
[01:22:11.720 --> 01:22:14.360]   They cannot get Windows to run on anything
[01:22:14.360 --> 01:22:16.280]   smaller than all laptop.
[01:22:16.280 --> 01:22:19.080]   - They had initially, and I think it was a red herring,
[01:22:19.080 --> 01:22:21.600]   said this is gonna be for our dual screen devices.
[01:22:21.600 --> 01:22:22.440]   - Yeah.
[01:22:22.440 --> 01:22:23.440]   - The duo and the Neo, the duo,
[01:22:23.440 --> 01:22:24.640]   it's up being Android.
[01:22:24.640 --> 01:22:25.480]   - The other hairs.
[01:22:25.480 --> 01:22:26.320]   - I think they were just saying that--
[01:22:26.320 --> 01:22:27.160]   - I was still waiting for--
[01:22:27.160 --> 01:22:28.000]   - To lower it. - To lower it.
[01:22:28.000 --> 01:22:29.000]   - The Neo and I would have bought one of those.
[01:22:29.000 --> 01:22:30.120]   - The Neo looks cool.
[01:22:30.120 --> 01:22:30.960]   - Yeah.
[01:22:30.960 --> 01:22:33.840]   - But then they said, oh no, it's gonna be for other devices.
[01:22:33.840 --> 01:22:36.240]   And now they've said, they finally admitted it.
[01:22:36.240 --> 01:22:37.720]   - I mean, every time we look at,
[01:22:37.720 --> 01:22:39.680]   I don't know if you've still got a foldable phone.
[01:22:39.680 --> 01:22:41.320]   Every time we looked at a foldable phone,
[01:22:41.320 --> 01:22:43.960]   it just made my footage because--
[01:22:43.960 --> 01:22:45.520]   - Does it still work?
[01:22:45.520 --> 01:22:47.800]   I got very nervous about a foldable screen.
[01:22:47.800 --> 01:22:50.720]   The duo with two screens made sense.
[01:22:50.720 --> 01:22:52.280]   - Yeah, it's a much better plan,
[01:22:52.280 --> 01:22:53.560]   but it also doesn't make sense
[01:22:53.560 --> 01:22:54.840]   'cause the hinge isn't in the middle,
[01:22:54.840 --> 01:22:57.280]   so it's not one big screen.
[01:22:57.280 --> 01:22:58.680]   It's two smaller screens.
[01:22:58.680 --> 01:23:01.000]   Also, Android was poorly suited for it.
[01:23:01.000 --> 01:23:04.880]   The fold started to bubble up in the middle.
[01:23:04.880 --> 01:23:06.640]   I didn't even use it every day.
[01:23:06.640 --> 01:23:07.480]   - Yeah, I'm thankful too.
[01:23:07.480 --> 01:23:09.640]   - And so I had to peel the screen protect the fold too.
[01:23:09.640 --> 01:23:11.280]   I had to peel the screen protector off,
[01:23:11.280 --> 01:23:13.240]   which as you remember with a fold one was the end.
[01:23:13.240 --> 01:23:14.760]   - That was the big problem.
[01:23:14.760 --> 01:23:15.600]   - Yeah.
[01:23:15.600 --> 01:23:16.960]   - But in this case, it survived that.
[01:23:16.960 --> 01:23:18.680]   But now I'm very careful.
[01:23:18.680 --> 01:23:21.360]   I don't think of folding screens a good idea.
[01:23:21.360 --> 01:23:25.000]   - No, I generally don't feel, okay,
[01:23:25.000 --> 01:23:27.520]   a lot of testers got them and they test them out.
[01:23:27.520 --> 01:23:28.520]   Oh yeah, this is what it's going to do.
[01:23:28.520 --> 01:23:29.640]   - Yeah, but people review,
[01:23:29.640 --> 01:23:31.200]   this is the problem with tech reviews.
[01:23:31.200 --> 01:23:32.040]   - Yeah.
[01:23:32.040 --> 01:23:32.880]   - Two weeks.
[01:23:32.880 --> 01:23:33.720]   - Yeah, exactly.
[01:23:33.720 --> 01:23:35.120]   But after a year of use,
[01:23:35.120 --> 01:23:37.320]   a year of opening and closing the thing,
[01:23:37.320 --> 01:23:38.440]   that thing's toast.
[01:23:38.440 --> 01:23:39.280]   - Yeah.
[01:23:39.280 --> 01:23:40.120]   - You know, it's--
[01:23:40.120 --> 01:23:40.960]   - Yeah.
[01:23:40.960 --> 01:23:42.280]   - I don't know.
[01:23:42.280 --> 01:23:45.280]   Yeah, we were talking about Windows 10X and,
[01:23:45.280 --> 01:23:50.280]   as I say, it was yet another example of Microsoft
[01:23:50.760 --> 01:23:53.200]   trying to get Windows to run on something
[01:23:53.200 --> 01:23:55.040]   which wasn't a full computer.
[01:23:55.040 --> 01:23:57.440]   - See, I was hoping for more.
[01:23:57.440 --> 01:23:59.840]   I was hoping it was actually an attempt
[01:23:59.840 --> 01:24:03.920]   to take this awful code base that is 20 years old
[01:24:03.920 --> 01:24:08.160]   and is just spaghetti, literally tens of millions of lines
[01:24:08.160 --> 01:24:09.480]   of spaghetti code.
[01:24:09.480 --> 01:24:14.120]   Many of the lines are one liners just to make one program work.
[01:24:14.120 --> 01:24:16.040]   I mean, it's just a mess.
[01:24:16.040 --> 01:24:19.200]   And rationalize it, not start over
[01:24:19.200 --> 01:24:20.840]   'cause you can't do that.
[01:24:20.840 --> 01:24:22.800]   - But Microsoft really is the legacy company.
[01:24:22.800 --> 01:24:24.880]   And I think they weren't gonna do that.
[01:24:24.880 --> 01:24:27.520]   - This is the problem in that you had to have
[01:24:27.520 --> 01:24:29.720]   those kind of customers supported.
[01:24:29.720 --> 01:24:31.560]   I mean, this is why when they tried to move to Edge,
[01:24:31.560 --> 01:24:34.000]   it's like, well, hang on, we're gonna have to have IE.
[01:24:34.000 --> 01:24:36.640]   - They just finally took IE out, right?
[01:24:36.640 --> 01:24:37.480]   - Yeah, finally.
[01:24:37.480 --> 01:24:38.600]   It's just like they finally took it
[01:24:38.600 --> 01:24:39.920]   behind the chemical shed and shuffled it.
[01:24:39.920 --> 01:24:41.040]   - They still have, though, by the way,
[01:24:41.040 --> 01:24:42.760]   and I've used it on Edge, it's weird.
[01:24:42.760 --> 01:24:44.520]   And IE mode on Edge.
[01:24:44.520 --> 01:24:45.560]   - Oh, good grief.
[01:24:45.560 --> 01:24:46.800]   I know.
[01:24:46.800 --> 01:24:47.640]   - But I mean--
[01:24:47.640 --> 01:24:48.720]   - They cannot turn it off.
[01:24:48.720 --> 01:24:50.920]   - But you're right, they are the legacy company.
[01:24:50.920 --> 01:24:53.440]   And they've tried this with 10S,
[01:24:53.440 --> 01:24:56.880]   they've tried this with RT, and it just doesn't work.
[01:24:56.880 --> 01:24:58.640]   - It's like Intel with their processors.
[01:24:58.640 --> 01:25:01.160]   They can't get a small processor
[01:25:01.160 --> 01:25:04.240]   because they're so sold in to, you know,
[01:25:04.240 --> 01:25:05.160]   their current architect.
[01:25:05.160 --> 01:25:08.200]   - And this is where the innovators dilemma really can--
[01:25:08.200 --> 01:25:09.440]   - Right, put you out of business.
[01:25:09.440 --> 01:25:10.320]   - Exactly.
[01:25:10.320 --> 01:25:11.160]   Exactly.
[01:25:11.160 --> 01:25:14.320]   I mean, Microsoft owes its existence
[01:25:14.320 --> 01:25:16.960]   to the creation of the IBM PC.
[01:25:16.960 --> 01:25:19.240]   They made the IBM PC by spinning out,
[01:25:19.240 --> 01:25:21.680]   basically what was an autonomous company.
[01:25:21.680 --> 01:25:23.480]   They built the thing, they did the standards,
[01:25:23.480 --> 01:25:24.760]   they made a whole bunch of mistakes,
[01:25:24.760 --> 01:25:26.280]   they ended up losing control of it,
[01:25:26.280 --> 01:25:27.600]   et cetera, rest is history.
[01:25:27.600 --> 01:25:29.560]   But this is what Microsoft needs to do
[01:25:29.560 --> 01:25:32.520]   if they want to have, you know,
[01:25:32.520 --> 01:25:34.240]   something running on smaller devices.
[01:25:34.240 --> 01:25:36.080]   They need to forget about Windows,
[01:25:36.080 --> 01:25:38.160]   stop trying to, like, you know,
[01:25:38.160 --> 01:25:41.080]   bring in all this legacy stuff for these small devices,
[01:25:41.080 --> 01:25:42.560]   build something lean and stuff from there.
[01:25:42.560 --> 01:25:45.360]   - Was that Windows CE, was that the idea of CE?
[01:25:45.360 --> 01:25:46.200]   - A Win.
[01:25:46.200 --> 01:25:49.600]   - No, CE, CE was another Windows X type thing
[01:25:49.600 --> 01:25:51.480]   where they were trying to build in all this,
[01:25:51.480 --> 01:25:52.600]   like, stuff, bring along all this whole code.
[01:25:52.600 --> 01:25:54.960]   - Take Windows and make it fit into it.
[01:25:54.960 --> 01:25:56.040]   - It's a little, exactly.
[01:25:56.040 --> 01:25:56.880]   - Exactly.
[01:25:56.880 --> 01:25:58.720]   So it's, and the other thing, philosophically,
[01:25:58.720 --> 01:26:01.360]   can you kill something that doesn't exist?
[01:26:01.360 --> 01:26:03.240]   I mean, it basically never,
[01:26:03.240 --> 01:26:05.000]   it's like something they've been talking about.
[01:26:05.000 --> 01:26:06.720]   - They've never enough, yeah, yeah, yeah.
[01:26:06.720 --> 01:26:07.880]   - So it's not-- - They're never really, yeah.
[01:26:07.880 --> 01:26:09.680]   - They're not taking anything away from anyone
[01:26:09.680 --> 01:26:10.520]   in the real world.
[01:26:10.520 --> 01:26:11.960]   - Well, and you noted this too Ian
[01:26:11.960 --> 01:26:15.640]   and Paul Theron, Mary Jofoli, were absolutely--
[01:26:15.640 --> 01:26:17.720]   - Well, Mary Jofoli, she was fantastic about this.
[01:26:17.720 --> 01:26:19.160]   - Well, live it about this.
[01:26:19.160 --> 01:26:21.880]   This announcement, Mary Jo had the story the week before.
[01:26:21.880 --> 01:26:22.720]   - Yeah, nice to see you.
[01:26:22.720 --> 01:26:23.720]   - And then this week Mary Jo said,
[01:26:23.720 --> 01:26:24.880]   well, Microsoft confirmed it,
[01:26:24.880 --> 01:26:27.080]   but wail you hear how they confirmed it.
[01:26:27.080 --> 01:26:29.560]   There's a blog post by this poor guy, John Cable,
[01:26:29.560 --> 01:26:31.040]   who is always given this job.
[01:26:31.040 --> 01:26:34.120]   He's vice president, program management,
[01:26:34.120 --> 01:26:36.600]   Windows servicing in the livery.
[01:26:36.600 --> 01:26:39.040]   But he, I think he's got one of those,
[01:26:39.040 --> 01:26:40.640]   you know, he's the red stapler guy.
[01:26:40.640 --> 01:26:43.200]   He's got one of those corner offices.
[01:26:43.200 --> 01:26:45.080]   Where they go, "John."
[01:26:45.080 --> 01:26:45.920]   - Yes.
[01:26:45.920 --> 01:26:49.600]   - We need you to write a blog post, John.
[01:26:49.600 --> 01:26:50.440]   - Okay.
[01:26:50.440 --> 01:26:53.360]   - So the blog post is,
[01:26:53.360 --> 01:26:57.520]   how to get the Windows 10 May 2021 update?
[01:26:57.520 --> 01:26:59.320]   That's literally the blog post.
[01:26:59.320 --> 01:27:02.240]   - And then the second to last topic on the blog post.
[01:27:02.240 --> 01:27:03.520]   - At the very bottom!
[01:27:03.520 --> 01:27:05.840]   - It was pitiful.
[01:27:05.840 --> 01:27:09.280]   And, you know, I mean, all credit systems, Foley,
[01:27:09.280 --> 01:27:12.360]   she nailed them absolutely beautifully.
[01:27:12.360 --> 01:27:13.480]   - They buried this.
[01:27:13.480 --> 01:27:15.720]   They also released it as Google I/O.
[01:27:15.720 --> 01:27:17.120]   - Google I/O is beginning.
[01:27:17.120 --> 01:27:19.160]   - The first day of Google I/O is like,
[01:27:19.160 --> 01:27:20.640]   how could we bury this?
[01:27:20.640 --> 01:27:21.480]   You know?
[01:27:21.480 --> 01:27:25.720]   - So way down at the bottom, oh, by the way,
[01:27:25.720 --> 01:27:29.680]   and this is the subhead is our customer first focus, okay?
[01:27:29.680 --> 01:27:32.400]   - But you get the re-engage for burying it, I think.
[01:27:32.400 --> 01:27:35.000]   - Following a year long exploration
[01:27:35.000 --> 01:27:37.560]   and engaging in conversations with customers.
[01:27:37.560 --> 01:27:39.320]   Remember, customers come first.
[01:27:39.320 --> 01:27:41.560]   We realized the technology of Windows 10X
[01:27:41.560 --> 01:27:43.680]   could be useful in more ways
[01:27:43.680 --> 01:27:46.160]   and serve more customers than we originally imagined.
[01:27:46.160 --> 01:27:48.400]   Oh, this is good, this is good, right?
[01:27:48.400 --> 01:27:51.280]   We concluded the 10X technology shouldn't just be confined
[01:27:51.280 --> 01:27:53.440]   to a subset of customers.
[01:27:53.440 --> 01:27:55.680]   Instead, we're just gonna kill it.
[01:27:55.680 --> 01:27:58.000]   We are leveraging learnings from our,
[01:27:58.000 --> 01:28:00.880]   this is a, by the way, that was the title of the show.
[01:28:00.880 --> 01:28:03.760]   We are leveraging learnings from our journey thus far
[01:28:03.760 --> 01:28:06.080]   in accelerating the integration of key foundational
[01:28:06.080 --> 01:28:09.280]   10X technology into other parts of Windows
[01:28:09.280 --> 01:28:11.320]   and products at the company,
[01:28:11.320 --> 01:28:13.200]   but we're never gonna release.
[01:28:13.200 --> 01:28:15.040]   - It was just too good.
[01:28:15.040 --> 01:28:18.120]   You know, they just, they had to share it with people.
[01:28:18.120 --> 01:28:21.280]   And I seriously, I read that now just like,
[01:28:21.280 --> 01:28:23.960]   wow, somebody really earned their money
[01:28:23.960 --> 01:28:26.600]   in the marketing department that day.
[01:28:26.600 --> 01:28:29.360]   - I wanna meet John Cable, I just have to know.
[01:28:29.360 --> 01:28:31.520]   Why is it, apparently this is not the first time
[01:28:31.520 --> 01:28:34.280]   they've made him be the bearer of bad tidings.
[01:28:34.280 --> 01:28:35.800]   - You should take a stapler.
[01:28:35.800 --> 01:28:36.920]   (laughing)
[01:28:36.920 --> 01:28:37.760]   - Excuse me?
[01:28:37.760 --> 01:28:39.480]   - Do you have money?
[01:28:39.480 --> 01:28:40.480]   - What do you understand?
[01:28:40.480 --> 01:28:42.680]   - They're totally out of the rent.
[01:28:42.680 --> 01:28:44.480]   - They told me we're vice president now.
[01:28:44.480 --> 01:28:45.960]   - Who is he told us now?
[01:28:45.960 --> 01:28:48.120]   (laughing)
[01:28:48.120 --> 01:28:48.960]   - Poor guy.
[01:28:48.960 --> 01:28:50.240]   - To be the stalking horse from Microsoft,
[01:28:50.240 --> 01:28:52.480]   I mean, I'm sure it's a very profitable position,
[01:28:52.480 --> 01:28:54.880]   but it was just kind of like,
[01:28:54.880 --> 01:28:59.880]   are you excited about Microsoft Teams for consumers?
[01:28:59.880 --> 01:29:02.960]   - Ah, I've used Teams a couple of times.
[01:29:02.960 --> 01:29:05.400]   Honestly, it's about as much fun as pulling teeth.
[01:29:06.320 --> 01:29:08.600]   Microsoft Teams for personal use.
[01:29:08.600 --> 01:29:11.360]   Teams has been a big success in enterprise,
[01:29:11.360 --> 01:29:14.200]   but I love the idea of something called Teams
[01:29:14.200 --> 01:29:15.320]   for one person.
[01:29:15.320 --> 01:29:16.160]   (laughing)
[01:29:16.160 --> 01:29:18.120]   - But do people actually use it because they want to use it?
[01:29:18.120 --> 01:29:19.280]   Because they have to use it.
[01:29:19.280 --> 01:29:20.920]   - They have to, because the guy, every time.
[01:29:20.920 --> 01:29:24.400]   - Every time someone has said to me,
[01:29:24.400 --> 01:29:26.120]   let's get together with a video call.
[01:29:26.120 --> 01:29:29.320]   It's either been Zoom or Google Meet.
[01:29:29.320 --> 01:29:30.840]   No one has ever said to me,
[01:29:30.840 --> 01:29:32.640]   oh, well, you're like, yeah, let's get together for a team.
[01:29:32.640 --> 01:29:34.640]   - Yeah, this is how I want my family to look.
[01:29:34.640 --> 01:29:37.640]   When we meet together, this looks like the last supper.
[01:29:37.640 --> 01:29:38.480]   - Yeah.
[01:29:38.480 --> 01:29:39.640]   (laughing)
[01:29:39.640 --> 01:29:42.120]   - It's always way shorter than the table, too.
[01:29:42.120 --> 01:29:44.280]   - Which one would be training?
[01:29:44.280 --> 01:29:45.760]   - Yes.
[01:29:45.760 --> 01:29:48.600]   This is the so-called together mode,
[01:29:48.600 --> 01:29:51.640]   which, let's be fair, was designed for businesses.
[01:29:51.640 --> 01:29:52.480]   Right?
[01:29:52.480 --> 01:29:54.280]   But I don't know if I want my family to be.
[01:29:54.280 --> 01:29:57.680]   - It's designed for that kind of the wacky HR manager
[01:29:57.680 --> 01:29:59.920]   who thinks this is a really good idea.
[01:29:59.920 --> 01:30:02.160]   And this will open us up and allow us to share
[01:30:02.160 --> 01:30:05.760]   more interactively and a proactive and forward thinking, man.
[01:30:05.760 --> 01:30:07.160]   It's just rubbish.
[01:30:07.160 --> 01:30:09.040]   (laughing)
[01:30:09.040 --> 01:30:11.120]   - If I'm gonna have to sit through a video call,
[01:30:11.120 --> 01:30:13.120]   I want the person in front of me,
[01:30:13.120 --> 01:30:14.480]   or the group, or the person in front of us.
[01:30:14.480 --> 01:30:15.800]   - My favorite one.
[01:30:15.800 --> 01:30:17.920]   - Which they obviously added for teams for families
[01:30:17.920 --> 01:30:19.160]   is at the breakfast bar.
[01:30:19.160 --> 01:30:20.000]   - Right.
[01:30:20.000 --> 01:30:21.640]   - But everybody looks like they're four foot tall
[01:30:21.640 --> 01:30:24.640]   because it cuts you off at the shoulder.
[01:30:24.640 --> 01:30:25.480]   - Looks like a jury.
[01:30:25.480 --> 01:30:26.920]   - They record a sitcom, though,
[01:30:26.920 --> 01:30:29.240]   because they always do the open table, you know?
[01:30:29.240 --> 01:30:30.080]   - Yeah.
[01:30:30.080 --> 01:30:31.560]   - They're never facing each other at a sitcom table.
[01:30:31.560 --> 01:30:33.320]   - What's the name of the Friends Coffee Shop?
[01:30:33.320 --> 01:30:34.800]   It could be a better grand--
[01:30:34.800 --> 01:30:35.640]   - Central perk.
[01:30:35.640 --> 01:30:36.640]   - Central perk, yeah.
[01:30:36.640 --> 01:30:37.480]   - Yeah, yeah.
[01:30:37.480 --> 01:30:38.480]   - Go with that, Texas.
[01:30:38.480 --> 01:30:40.000]   - That is really ridiculous.
[01:30:40.000 --> 01:30:42.280]   Although they're doing the reboot now,
[01:30:42.280 --> 01:30:43.520]   it's like, who cares?
[01:30:43.520 --> 01:30:44.360]   It was a long time ago.
[01:30:44.360 --> 01:30:45.560]   - Were the same actors or--
[01:30:45.560 --> 01:30:46.560]   - Yeah, same actors, yeah.
[01:30:46.560 --> 01:30:47.560]   - But like they're really--
[01:30:47.560 --> 01:30:49.040]   - What is it, the nursing home now?
[01:30:49.040 --> 01:30:51.240]   - Well, well, I mean, if you look at the actors
[01:30:51.240 --> 01:30:53.200]   playing Joey, he looks like he's eating Joey.
[01:30:53.200 --> 01:30:55.460]   (laughing)
[01:30:55.460 --> 01:30:58.040]   - How you doin'?
[01:30:58.040 --> 01:30:58.880]   (laughing)
[01:30:58.880 --> 01:31:00.040]   - How you tastein'?
[01:31:01.000 --> 01:31:02.600]   - All right, so there we go.
[01:31:02.600 --> 01:31:04.000]   That's the Microsoft segment.
[01:31:04.000 --> 01:31:05.720]   I hope you enjoy it.
[01:31:05.720 --> 01:31:07.000]   - Always a positive.
[01:31:07.000 --> 01:31:09.160]   - Always, always much, much right.
[01:31:09.160 --> 01:31:10.000]   - That's the right side of life.
[01:31:10.000 --> 01:31:10.840]   - I do hope you don't--
[01:31:10.840 --> 01:31:12.120]   - We're gonna cut 'em all down today.
[01:31:12.120 --> 01:31:13.640]   - That's what we're doing today.
[01:31:13.640 --> 01:31:15.360]   - What else should we do, Mr. Producer?
[01:31:15.360 --> 01:31:17.200]   Where should we go with this?
[01:31:17.200 --> 01:31:18.040]   Where should we go?
[01:31:18.040 --> 01:31:19.200]   Apple versus Epic?
[01:31:19.200 --> 01:31:20.680]   Apple versus China?
[01:31:20.680 --> 01:31:22.440]   - Sure, do a little--
[01:31:22.440 --> 01:31:24.360]   - Why don't you focus a little bit on Apple?
[01:31:24.360 --> 01:31:25.200]   - Apple versus Apple.
[01:31:25.200 --> 01:31:26.040]   - This isn't a world.
[01:31:26.040 --> 01:31:26.880]   - We haven't seen the world.
[01:31:26.880 --> 01:31:27.720]   - Have we known?
[01:31:27.720 --> 01:31:28.560]   Should we dis a little--
[01:31:28.560 --> 01:31:30.800]   - We've got some Blue Origin's
[01:31:30.800 --> 01:31:32.320]   to sing to do later, but yeah, no, that's fine.
[01:31:32.320 --> 01:31:33.680]   - Oh yeah, we wanna talk Bezos.
[01:31:33.680 --> 01:31:34.680]   We can do that too.
[01:31:34.680 --> 01:31:38.200]   Evan Spiegel, CEO of Snap.
[01:31:38.200 --> 01:31:39.880]   I guess this was--
[01:31:39.880 --> 01:31:40.920]   - As Cheerleader.
[01:31:40.920 --> 01:31:42.560]   - Cheerleader.
[01:31:42.560 --> 01:31:44.920]   He welcomes our insights over Lawrence.
[01:31:44.920 --> 01:31:46.280]   - We're happy.
[01:31:46.280 --> 01:31:47.120]   - Yes.
[01:31:47.120 --> 01:31:48.040]   - To pay Apple 30%.
[01:31:48.040 --> 01:31:49.600]   We're happy with that.
[01:31:49.600 --> 01:31:50.680]   Well, he does have a point,
[01:31:50.680 --> 01:31:51.960]   with that Apple we wouldn't exist.
[01:31:51.960 --> 01:31:54.640]   It's true that the iPhone made Snapchat what it is, right?
[01:31:54.640 --> 01:31:57.160]   He sees the falling out between Apple and Facebook
[01:31:57.160 --> 01:31:58.760]   and things we could be,
[01:31:58.760 --> 01:32:01.600]   'cause they've always given a pass to Facebook and Twitter
[01:32:01.600 --> 01:32:04.120]   to the detriment of other social networks.
[01:32:04.120 --> 01:32:06.680]   And he sees that position opening,
[01:32:06.680 --> 01:32:10.800]   where he could be the favorite child of Apple,
[01:32:10.800 --> 01:32:12.800]   and that is a sweet place to be.
[01:32:12.800 --> 01:32:15.120]   I wish it was 35%.
[01:32:15.120 --> 01:32:16.720]   We wish we'd like to pay more.
[01:32:16.720 --> 01:32:17.560]   - Yeah.
[01:32:17.560 --> 01:32:19.200]   - I've never yet been business-building yours.
[01:32:19.200 --> 01:32:21.560]   Yes, I'm happy to give 30% of my turnover
[01:32:21.560 --> 01:32:22.960]   to a conduit.
[01:32:22.960 --> 01:32:25.480]   It's kind of like, yes, I'm fully in favor
[01:32:25.480 --> 01:32:26.680]   of free market competition.
[01:32:26.680 --> 01:32:28.280]   I love having competitors.
[01:32:28.280 --> 01:32:30.960]   I wouldn't want to be a monopolist in this market at all.
[01:32:30.960 --> 01:32:34.040]   - There are a lot of those smaller developers, especially,
[01:32:34.040 --> 01:32:35.800]   who say, you know, this is a good deal.
[01:32:35.800 --> 01:32:36.880]   I don't have to print it.
[01:32:36.880 --> 01:32:37.880]   I don't have to stock it.
[01:32:37.880 --> 01:32:39.240]   I don't have to get a distributor.
[01:32:39.240 --> 01:32:41.320]   They take 30%, which was less than Ingram.
[01:32:41.320 --> 01:32:44.000]   Micro took, when you had shrink-wrapped software,
[01:32:44.000 --> 01:32:45.200]   they would take 50%.
[01:32:45.200 --> 01:32:46.440]   It depends on what you're doing,
[01:32:46.440 --> 01:32:49.200]   what the marketplace is, who your competition is, et cetera.
[01:32:49.200 --> 01:32:53.360]   There are lots of small app makers who have been wiped out
[01:32:53.360 --> 01:32:55.440]   because Apple decided to build that functionality
[01:32:55.440 --> 01:32:56.800]   into the operating system.
[01:32:56.800 --> 01:32:57.640]   - Yeah, that's the rub.
[01:32:57.640 --> 01:33:00.480]   - There are lots of companies where they're taking money
[01:33:00.480 --> 01:33:04.400]   where the company is cross-platform on multiple devices,
[01:33:04.400 --> 01:33:07.080]   especially gaming companies and stuff like that,
[01:33:07.080 --> 01:33:08.920]   where Apple is just one component
[01:33:08.920 --> 01:33:11.000]   and then Apple really sticks it to them.
[01:33:11.000 --> 01:33:16.000]   But what was particularly irritating about his missive
[01:33:16.000 --> 01:33:20.520]   is he basically just went down the Apple bullet points,
[01:33:20.520 --> 01:33:22.080]   oh, and all the tools they give us
[01:33:22.080 --> 01:33:25.160]   and all the great technology they bring to market.
[01:33:25.160 --> 01:33:26.920]   - Usually that's somebody who's scared, right?
[01:33:26.920 --> 01:33:28.320]   - Exactly, right.
[01:33:28.320 --> 01:33:29.440]   - Don't mess with me, Apple.
[01:33:29.440 --> 01:33:34.440]   He actually was talking on tech check, a CNBC program.
[01:33:34.440 --> 01:33:38.800]   Barry Diller, same day, ripped into Apple
[01:33:38.800 --> 01:33:40.600]   for the cut it takes of transactions,
[01:33:40.600 --> 01:33:43.200]   saying his companies are, quote, "overcharged
[01:33:43.200 --> 01:33:47.240]   "in a disgusting manner by Apple."
[01:33:47.240 --> 01:33:49.480]   The idea they actually justify it by saying,
[01:33:49.480 --> 01:33:51.920]   "We spend all this money protecting our little app store."
[01:33:51.920 --> 01:33:54.000]   I mean, it's criminal.
[01:33:54.000 --> 01:33:56.600]   - Well, it will be criminal, wow.
[01:33:56.600 --> 01:34:00.360]   - I mean, they still get, you know, I mean, okay,
[01:34:00.360 --> 01:34:01.440]   when you go into the app store,
[01:34:01.440 --> 01:34:03.920]   you do get a better level of code checking
[01:34:03.920 --> 01:34:06.240]   than you would get with, say, the Google Play Store.
[01:34:06.240 --> 01:34:07.440]   You know, they are--
[01:34:07.440 --> 01:34:09.440]   - Not that bad stuff doesn't get through.
[01:34:09.440 --> 01:34:11.400]   - No, bad stuff gets into the iOS
[01:34:11.400 --> 01:34:13.240]   at a much lower rate than Google Play Store,
[01:34:13.240 --> 01:34:14.880]   but it still gets through.
[01:34:14.880 --> 01:34:15.720]   - Yeah.
[01:34:15.720 --> 01:34:19.440]   - But for Tim Cook to standing court today this week
[01:34:19.440 --> 01:34:21.520]   and say, "You know what?
[01:34:21.520 --> 01:34:25.680]   "I don't really know if our app store is profitable or so."
[01:34:25.680 --> 01:34:27.240]   I don't have--
[01:34:27.240 --> 01:34:29.280]   - We don't, they don't tell me those things.
[01:34:29.280 --> 01:34:30.680]   - We don't calculate those numbers.
[01:34:30.680 --> 01:34:35.680]   - This is a COO, which revolutionized Apple's entire content,
[01:34:35.680 --> 01:34:37.600]   it's entire Liberty Standing.
[01:34:37.600 --> 01:34:39.800]   - He knows to the penny exactly,
[01:34:39.800 --> 01:34:41.480]   how much they get from this.
[01:34:41.480 --> 01:34:43.040]   And for him to sit there and call,
[01:34:43.040 --> 01:34:47.280]   I'm presuming it's profitable, I really wouldn't know.
[01:34:47.280 --> 01:34:49.560]   - This is what I was saying earlier.
[01:34:49.560 --> 01:34:52.200]   CEOs are not at Liberty to speak the truth.
[01:34:52.200 --> 01:34:53.040]   - True.
[01:34:53.040 --> 01:34:54.440]   - They have to, they're going to court
[01:34:54.440 --> 01:34:56.520]   and they can't make you say something
[01:34:56.520 --> 01:34:58.600]   that you don't remember or you don't know.
[01:34:58.600 --> 01:35:01.080]   So by saying, "I don't remember it and I don't know,"
[01:35:01.080 --> 01:35:03.200]   is how you win court cases.
[01:35:03.200 --> 01:35:07.840]   And so he's just winning the court case by lying.
[01:35:07.840 --> 01:35:09.520]   - We know this from the,
[01:35:09.520 --> 01:35:12.680]   there's only two press members in the court.
[01:35:12.680 --> 01:35:13.520]   - Yeah.
[01:35:13.520 --> 01:35:14.280]   - One of them, Dorothy Atkins,
[01:35:14.280 --> 01:35:16.360]   ready for law 360, tweeted.
[01:35:16.360 --> 01:35:18.840]   - What a world we live in.
[01:35:18.840 --> 01:35:21.160]   - You just have a sketch artist drawing
[01:35:21.160 --> 01:35:23.240]   and he's just, you rush it to the press.
[01:35:23.240 --> 01:35:25.840]   Now she's tweeting the testimony.
[01:35:25.840 --> 01:35:27.760]   So these aren't obviously verbatim,
[01:35:27.760 --> 01:35:32.460]   but when Cook was questioned by his own counsel,
[01:35:32.460 --> 01:35:37.560]   he was, let's see, I'll read this.
[01:35:37.560 --> 01:35:39.280]   The question he had cooked by his own company's counsel
[01:35:39.280 --> 01:35:40.600]   was gentle.
[01:35:40.600 --> 01:35:41.680]   - Well done.
[01:35:41.680 --> 01:35:42.520]   (laughing)
[01:35:42.520 --> 01:35:43.360]   - Yeah.
[01:35:43.360 --> 01:35:44.600]   - And directed at reiterating the reasons
[01:35:44.600 --> 01:35:48.720]   why Apple's app store is superior and sufficient for iOS users
[01:35:48.720 --> 01:35:51.280]   while also asserting the presence of stiff competition.
[01:35:51.280 --> 01:35:54.160]   He admitted to a handful of conflicts with developers
[01:35:54.160 --> 01:35:56.680]   differing priorities or needing to improve discovery,
[01:35:56.680 --> 01:35:58.680]   but said the company works constantly
[01:35:58.680 --> 01:36:01.060]   to retain developers and users.
[01:36:01.060 --> 01:36:03.960]   Then he was asked about Apple's R&D numbers.
[01:36:03.960 --> 01:36:05.920]   Now, I don't know if this was by Apple's counsel,
[01:36:05.920 --> 01:36:07.880]   by Apple's counsel.
[01:36:07.880 --> 01:36:11.200]   We know Apple's R&D, they report it,
[01:36:11.200 --> 01:36:14.880]   was 15 to $20 billion annually for the last three years.
[01:36:14.880 --> 01:36:18.320]   But when asked, well, how much of that research
[01:36:18.320 --> 01:36:20.760]   and development goes to the app store, he said,
[01:36:20.760 --> 01:36:24.120]   I don't know, we don't allocate it like that.
[01:36:24.120 --> 01:36:24.960]   - I don't know.
[01:36:24.960 --> 01:36:29.040]   It's such a trans, I mean, it's literally--
[01:36:29.040 --> 01:36:29.880]   - I didn't do it.
[01:36:29.880 --> 01:36:30.720]   - Right.
[01:36:30.720 --> 01:36:31.560]   - You know, I mean--
[01:36:31.560 --> 01:36:32.400]   - Well, it's worse than that.
[01:36:32.400 --> 01:36:35.800]   So some, in several cases, he didn't say, I don't know.
[01:36:35.800 --> 01:36:40.680]   He said, you can't estimate what that is.
[01:36:40.680 --> 01:36:41.640]   Of course you can.
[01:36:41.640 --> 01:36:42.880]   Sure you can estimate.
[01:36:42.880 --> 01:36:43.720]   - You could guess.
[01:36:43.720 --> 01:36:45.320]   - You could either give or take $2 billion,
[01:36:45.320 --> 01:36:47.320]   whatever it is, you can estimate this.
[01:36:47.320 --> 01:36:48.160]   - They can estimate that.
[01:36:48.160 --> 01:36:49.640]   - They can actually not answering exactly.
[01:36:49.640 --> 01:36:53.800]   - Yeah, changing the color on an iPhone cover,
[01:36:53.800 --> 01:36:55.960]   they know exactly how much that costs per year.
[01:36:55.960 --> 01:36:56.800]   - You've got to think so.
[01:36:56.800 --> 01:36:59.360]   - And, you know, it's just make believes.
[01:36:59.360 --> 01:37:03.760]   - Not only do they know, but Tim Cook is such a detail guy.
[01:37:03.760 --> 01:37:06.400]   That his whole role, when Steve Jobs, the CEO,
[01:37:06.400 --> 01:37:09.760]   was to perform manufacturing and miracles.
[01:37:09.760 --> 01:37:13.280]   - And then this is, oh, I don't know so much anymore.
[01:37:13.280 --> 01:37:14.920]   When Epic's counsel stepped up,
[01:37:14.920 --> 01:37:17.960]   one of the things you asked initially right away, in fact,
[01:37:17.960 --> 01:37:20.520]   was, you know, our expert said you're making
[01:37:20.520 --> 01:37:23.000]   more than 78% profit on the app store.
[01:37:23.000 --> 01:37:24.520]   Is that accurate?
[01:37:24.520 --> 01:37:26.760]   Who do you think Tim Cook said?
[01:37:26.760 --> 01:37:27.600]   - I don't know.
[01:37:27.600 --> 01:37:31.440]   - It's a very symptoms, what's a truck type of thing?
[01:37:31.440 --> 01:37:32.280]   - Yeah.
[01:37:32.280 --> 01:37:34.240]   - Depends what your definition of is.
[01:37:34.240 --> 01:37:35.440]   - Yeah, exactly.
[01:37:35.440 --> 01:37:36.640]   - Yeah, I did not sleep with it.
[01:37:36.640 --> 01:37:41.640]   - Although when that information was divulged by Epic's
[01:37:41.640 --> 01:37:44.880]   witness, Apple said, well, he's, you know,
[01:37:44.880 --> 01:37:46.880]   he doesn't know he's just an expert witness.
[01:37:46.880 --> 01:37:50.120]   And your honor, would you mind if this were not made public?
[01:37:50.120 --> 01:37:50.960]   - Yeah.
[01:37:50.960 --> 01:37:53.880]   - So, right, it was wrong and it shouldn't know and should know.
[01:37:53.880 --> 01:37:55.360]   (laughing)
[01:37:55.360 --> 01:37:57.080]   He actually did that again.
[01:37:57.080 --> 01:37:59.600]   Epic's attorney asked Cook to break down
[01:37:59.600 --> 01:38:01.760]   the confidential income numbers
[01:38:01.760 --> 01:38:04.200]   that combined the Mac and iOS app stores.
[01:38:04.200 --> 01:38:05.840]   Apple objected.
[01:38:05.840 --> 01:38:07.760]   The attorney said, that's privileged information.
[01:38:07.760 --> 01:38:11.320]   We can only divulge that in a closed court.
[01:38:11.320 --> 01:38:14.360]   Because Apple doesn't want anybody to hear these numbers.
[01:38:14.360 --> 01:38:17.600]   - And Cook did say the iOS numbers are, quote,
[01:38:17.600 --> 01:38:18.440]   "a lot larger."
[01:38:18.440 --> 01:38:19.280]   - A lot larger.
[01:38:19.280 --> 01:38:20.440]   - A lot larger than the Mac numbers.
[01:38:20.440 --> 01:38:21.960]   - Probably north of 90%.
[01:38:21.960 --> 01:38:24.200]   - But this was the thing, they were willing to throw Mac OS
[01:38:24.200 --> 01:38:25.320]   under the bus.
[01:38:25.320 --> 01:38:27.120]   'Cause in earlier testimony this week,
[01:38:27.120 --> 01:38:30.960]   they said basically iOS, because we have such a lockdown
[01:38:30.960 --> 01:38:34.240]   on the store, malware instances are very slight there,
[01:38:34.240 --> 01:38:37.800]   but oh, Mac OS, yeah, we get a lot of malware there.
[01:38:37.800 --> 01:38:39.120]   And it was just basically, yeah,
[01:38:39.120 --> 01:38:41.800]   wait to throw your laptop customers under the bus.
[01:38:41.800 --> 01:38:43.160]   It was pretty shameful.
[01:38:43.160 --> 01:38:45.400]   - We should point out that this is not a jury trial.
[01:38:45.400 --> 01:38:47.800]   It's a bench trial, which means the only audience
[01:38:47.800 --> 01:38:49.280]   for this is the judge.
[01:38:49.280 --> 01:38:50.800]   She's pretty smart, Judge Rogers.
[01:38:50.800 --> 01:38:52.920]   Probably can see through all of this
[01:38:52.920 --> 01:38:54.800]   and it has the same, maybe, reaction as well.
[01:38:54.800 --> 01:38:56.640]   - I would have liked CCO there, but yeah.
[01:38:56.640 --> 01:38:58.000]   - Yeah, you lose CCO?
[01:38:58.000 --> 01:38:58.840]   - Yeah.
[01:38:58.840 --> 01:39:00.720]   - But Rogers has done a number of these cases.
[01:39:00.720 --> 01:39:02.800]   - True. - So she's...
[01:39:02.800 --> 01:39:03.640]   - We'll see.
[01:39:03.640 --> 01:39:07.840]   We'll see, you know, it'll be interesting.
[01:39:07.840 --> 01:39:09.880]   I mean, Epic is not exactly the poster child
[01:39:09.880 --> 01:39:11.760]   you'd like to have is the claim of this.
[01:39:11.760 --> 01:39:12.760]   - No, but I mean,
[01:39:12.760 --> 01:39:15.640]   I can kind of understand Apple's position in that...
[01:39:15.640 --> 01:39:16.480]   - Sure.
[01:39:16.480 --> 01:39:18.440]   - You know, we maintain the apps tour, 15%,
[01:39:18.440 --> 01:39:20.680]   is actually a reasonable commission, you know,
[01:39:20.680 --> 01:39:22.680]   for larger players.
[01:39:22.680 --> 01:39:25.040]   But the in-app purchases thing, I'm sorry,
[01:39:25.040 --> 01:39:28.040]   that's just, that's just,
[01:39:28.040 --> 01:39:30.120]   come back, say, run seeking again.
[01:39:30.120 --> 01:39:31.800]   - I'm always fascinated by the question
[01:39:31.800 --> 01:39:35.560]   of whether Apple is monopolistic because, you know,
[01:39:35.560 --> 01:39:38.400]   it's like in the smartphone world, no.
[01:39:38.400 --> 01:39:43.000]   In the Is The Apple universe, a separate from the rest
[01:39:43.000 --> 01:39:43.840]   of the Apple world?
[01:39:43.840 --> 01:39:45.560]   Yes, and are they a monopoly in them?
[01:39:45.560 --> 01:39:47.520]   Absolutely, they're in the mother of all men up there.
[01:39:47.520 --> 01:39:49.040]   They only... - Yeah.
[01:39:49.040 --> 01:39:50.360]   - Player. - You have to buy it
[01:39:50.360 --> 01:39:52.680]   in the app store. - So how do you make that decision?
[01:39:52.680 --> 01:39:55.760]   And from Apple's point of view, they could say,
[01:39:55.760 --> 01:39:59.080]   no, the iPhone is not a separate product from the app store.
[01:39:59.080 --> 01:40:02.240]   The app store and the iPhone and some other things as well,
[01:40:02.240 --> 01:40:03.720]   that's the product.
[01:40:03.720 --> 01:40:05.160]   Like, who's to decide?
[01:40:05.160 --> 01:40:10.160]   Who can decide whether a Toyota car is separate from the tires
[01:40:10.160 --> 01:40:17.400]   that are made for the car, or are separate from the service
[01:40:17.400 --> 01:40:19.120]   that they provide to customers?
[01:40:19.120 --> 01:40:21.120]   Like, how do you decide whether...
[01:40:21.120 --> 01:40:24.400]   - What Toyota said, you can only buy Toyota tires
[01:40:24.400 --> 01:40:26.200]   for your Toyota car, you know?
[01:40:26.200 --> 01:40:28.600]   - We made our wheels proprietary.
[01:40:28.600 --> 01:40:29.440]   I mean, this happens all the time.
[01:40:29.440 --> 01:40:31.000]   - And you can buy them from us.
[01:40:31.000 --> 01:40:32.880]   - You can't buy cake cups for anybody,
[01:40:32.880 --> 01:40:34.800]   but curig for your curig coffee machine,
[01:40:34.800 --> 01:40:36.600]   but it's just what they try to say.
[01:40:36.600 --> 01:40:39.240]   - But I mean, just some, with Android,
[01:40:39.240 --> 01:40:40.760]   they're doing exactly the same thing,
[01:40:40.760 --> 01:40:42.680]   and Google has weighed it back a bit.
[01:40:42.680 --> 01:40:45.080]   Do you feel it's the same way?
[01:40:45.080 --> 01:40:47.600]   - No, 'cause you can buy anything from a...
[01:40:47.600 --> 01:40:49.960]   - But I mean, you don't have to use the Google store.
[01:40:49.960 --> 01:40:51.080]   - No, it's right. - The app was selling
[01:40:51.080 --> 01:40:52.520]   an app developer.
[01:40:52.520 --> 01:40:55.960]   I mean, is Google as bad as Apple?
[01:40:55.960 --> 01:40:57.760]   - By enforcing development. - Yeah.
[01:40:57.760 --> 01:41:00.120]   - In ensuring that developers are transacting.
[01:41:00.120 --> 01:41:01.120]   - Are transacting through them?
[01:41:01.120 --> 01:41:02.440]   - No, no, no, they're much looser.
[01:41:02.440 --> 01:41:04.240]   - I mean, I think that it is a little bit looser,
[01:41:04.240 --> 01:41:06.640]   but I think it's almost one of those situations
[01:41:06.640 --> 01:41:09.120]   where I have a feeling that Google kind of wishes
[01:41:09.120 --> 01:41:11.040]   it was in Apple. - Oh, no, no, no, no,
[01:41:11.040 --> 01:41:12.880]   I'm sure that's true.
[01:41:12.880 --> 01:41:14.040]   I'm sure they wish that they were.
[01:41:14.040 --> 01:41:17.520]   But I think in general, the platform is just more open.
[01:41:17.520 --> 01:41:18.880]   It's definitely looser.
[01:41:18.880 --> 01:41:19.800]   - Actually, in the chatroom,
[01:41:19.800 --> 01:41:23.360]   Chemele is saying Toyota insists you buy Toyota oil filters,
[01:41:23.360 --> 01:41:25.640]   so I guess there is a little bit of past terms.
[01:41:25.640 --> 01:41:26.720]   - Oh, okay. - Well, okay.
[01:41:26.720 --> 01:41:27.560]   - This isn't quite that.
[01:41:27.560 --> 01:41:28.520]   This is more like Toyota saying,
[01:41:28.520 --> 01:41:31.440]   you have to give us 30% for any oil filters you sell.
[01:41:31.440 --> 01:41:32.640]   - This is like Toyota saying,
[01:41:32.640 --> 01:41:35.800]   you have to buy all your gasoline from us or something like that.
[01:41:35.800 --> 01:41:40.640]   But the other thing that needs to be stated,
[01:41:40.640 --> 01:41:42.760]   if you're gonna be realistic about the real world,
[01:41:42.760 --> 01:41:45.520]   is that not only does Apple have a monopoly
[01:41:45.520 --> 01:41:49.080]   of the iOS environment,
[01:41:49.080 --> 01:41:52.280]   but the iPhone buyers spend more money.
[01:41:52.280 --> 01:41:54.280]   Like if you're an app developer,
[01:41:54.280 --> 01:41:58.840]   your chances of getting an iOS Apple,
[01:41:58.840 --> 01:42:02.120]   an iPhone user to pay for your app,
[01:42:02.120 --> 01:42:04.800]   is much higher than it is on Android.
[01:42:04.800 --> 01:42:06.280]   That's just the economics of it.
[01:42:06.280 --> 01:42:08.040]   And so that's the other part of it
[01:42:08.040 --> 01:42:10.280]   that probably shouldn't really come into it.
[01:42:10.280 --> 01:42:12.720]   But really what's at issue here is that Apple,
[01:42:12.720 --> 01:42:16.040]   in fact, does have a monopoly on the segment of the market
[01:42:16.040 --> 01:42:18.040]   that spends the most money on apps.
[01:42:18.040 --> 01:42:21.800]   And that's a huge differentiator between them and Google.
[01:42:21.800 --> 01:42:24.920]   - They did, and this is your article on the register.
[01:42:24.920 --> 01:42:27.200]   They did really throw macOS onto the bus.
[01:42:27.200 --> 01:42:28.880]   - Oh, it was painful. - It was quite fatal.
[01:42:28.880 --> 01:42:31.920]   - Craig Federighi testifying.
[01:42:31.920 --> 01:42:36.360]   Judge Rogers said, "There are multiple stores in the Mac.
[01:42:36.360 --> 01:42:37.760]   So if that can happen in the Mac,
[01:42:37.760 --> 01:42:40.760]   why should we not allow multiple stores on the phone?"
[01:42:40.760 --> 01:42:42.800]   Federighi said, "Well, that's how we've done it on the Mac
[01:42:42.800 --> 01:42:45.960]   and it's regularly exploited on the Mac."
[01:42:45.960 --> 01:42:50.440]   He said, "We have a level of malware on the Mac.
[01:42:50.440 --> 01:42:54.160]   We don't find acceptable as much worse than on iOS."
[01:42:54.160 --> 01:42:56.520]   I guess that's not strictly untrue, but boy,
[01:42:56.520 --> 01:42:57.760]   that sure is like...
[01:42:57.760 --> 01:42:59.320]   - No, it wasn't. - You put it.
[01:42:59.320 --> 01:43:01.160]   - Yeah, I mean, it was pretty shameless,
[01:43:01.160 --> 01:43:03.000]   but also let's be fair about this.
[01:43:03.000 --> 01:43:06.040]   The amount of malware you get on Mac OS
[01:43:06.040 --> 01:43:08.080]   is much less than you get on Windows.
[01:43:08.080 --> 01:43:10.240]   - Sure. - Because people go where...
[01:43:10.240 --> 01:43:12.160]   - It's more locked in. - Well, also, it's more locked in.
[01:43:12.160 --> 01:43:14.440]   - It is more locked in. - But as Mike said,
[01:43:14.440 --> 01:43:17.240]   I don't understand why more malware writers aren't targeting
[01:43:17.240 --> 01:43:19.800]   Apple users because that's where the money is.
[01:43:19.800 --> 01:43:21.440]   They've got a lot of money. - It's harder.
[01:43:21.440 --> 01:43:22.640]   - Pay a lot of stuff. - And actually...
[01:43:22.640 --> 01:43:24.360]   - But yeah, Apple makes it harder.
[01:43:24.360 --> 01:43:26.840]   If you really... If Judge Rogers pays attention,
[01:43:26.840 --> 01:43:28.000]   judge if you're listening,
[01:43:28.000 --> 01:43:32.160]   what really Apple is doing is saying,
[01:43:32.160 --> 01:43:36.160]   the only way we can get it more secure
[01:43:36.160 --> 01:43:38.440]   is by locking it down on iOS.
[01:43:38.440 --> 01:43:40.440]   We don't lock it down as much on Mac OS.
[01:43:40.440 --> 01:43:43.440]   But in fact, in the global ecosystem,
[01:43:43.440 --> 01:43:46.160]   Mac OS is much more secure because it's much more locked down.
[01:43:46.160 --> 01:43:48.800]   You have a keeper, you have signing.
[01:43:48.800 --> 01:43:51.360]   It is possible, in fact, to make something more secure
[01:43:51.360 --> 01:43:54.200]   without going to the extreme of saying
[01:43:54.200 --> 01:43:55.840]   you have to buy everything from our store.
[01:43:55.840 --> 01:43:57.760]   - Absolutely. - And if Judge Rogers
[01:43:57.760 --> 01:43:59.840]   reads between the lines and doesn't take
[01:43:59.840 --> 01:44:03.000]   what Craig Federig is saying on face value,
[01:44:03.000 --> 01:44:05.760]   she should... Maybe the epic attorney will bring this up,
[01:44:05.760 --> 01:44:07.520]   but she should really consider that.
[01:44:07.520 --> 01:44:11.120]   It is not a requirement that you only are allowed
[01:44:11.120 --> 01:44:14.040]   to buy stuff through us to make it more secure.
[01:44:14.040 --> 01:44:15.920]   It's completely possible to make it more secure,
[01:44:15.920 --> 01:44:19.040]   and Apple has done so on Mac OS without that.
[01:44:19.040 --> 01:44:21.000]   And actually, I think they've done a pretty good job,
[01:44:21.000 --> 01:44:22.000]   to be honest with you.
[01:44:22.000 --> 01:44:23.840]   - Yeah, they've written some... - Disembrosicule codes
[01:44:23.840 --> 01:44:25.200]   and play for Craig Federig.
[01:44:25.200 --> 01:44:26.040]   - Craig Federig, he says.
[01:44:26.040 --> 01:44:27.880]   So I don't actually think that it's because
[01:44:27.880 --> 01:44:29.200]   there's no money there.
[01:44:29.200 --> 01:44:31.640]   I think it's just much more difficult for an exploit.
[01:44:31.640 --> 01:44:33.320]   There have been exploits, but it's hard.
[01:44:33.320 --> 01:44:34.400]   You have to get, you know,
[01:44:34.400 --> 01:44:35.640]   if just steal somebody's signing certificate.
[01:44:35.640 --> 01:44:39.120]   - I'm a very swift guy. - And they have a kill switch.
[01:44:39.120 --> 01:44:41.800]   - Yeah. - That works pretty well.
[01:44:41.800 --> 01:44:42.720]   All right, let's take a little break.
[01:44:42.720 --> 01:44:46.200]   Our show today brought to you by Zip Recruiter.
[01:44:46.200 --> 01:44:48.000]   Here we are back in studio.
[01:44:48.000 --> 01:44:51.200]   Maybe you're starting to hire back up.
[01:44:51.200 --> 01:44:53.800]   As things come back, are you hiring for spring?
[01:44:53.800 --> 01:44:55.400]   Can I make a recommendation?
[01:44:55.400 --> 01:44:56.720]   This is the platform we use.
[01:44:56.720 --> 01:44:58.840]   It's the best way to hire.
[01:44:58.840 --> 01:45:00.360]   What type of role you're hiring for?
[01:45:00.360 --> 01:45:02.440]   Maybe you want to hire somebody that has many...
[01:45:02.440 --> 01:45:03.720]   Where's many hats?
[01:45:03.720 --> 01:45:06.280]   You know, can do a lot in a small company like ours.
[01:45:06.280 --> 01:45:07.560]   That's almost always the case.
[01:45:07.560 --> 01:45:12.160]   Or maybe, you know, you just have a very specific position to fill.
[01:45:12.160 --> 01:45:14.960]   But you can't find somebody who has all, you know,
[01:45:14.960 --> 01:45:16.880]   the exact right fit for your company,
[01:45:16.880 --> 01:45:18.760]   all the things you're really looking for.
[01:45:18.760 --> 01:45:21.520]   Zip Recruiter can help you with both situations.
[01:45:21.520 --> 01:45:23.480]   And by the way, you could try it for free right now.
[01:45:23.480 --> 01:45:26.120]   At ziprecruiter.com/twit.
[01:45:26.120 --> 01:45:28.920]   Whether you need to hire a civil engineer in New York,
[01:45:28.920 --> 01:45:33.320]   at a turning in Colorado, or even a mascot in Missouri,
[01:45:33.320 --> 01:45:35.480]   if you see the ads you'll know what I'm talking about.
[01:45:35.480 --> 01:45:37.120]   Zip Recruiter is amazing.
[01:45:37.120 --> 01:45:39.320]   First thing that happens, you post on Zip Recruiter
[01:45:39.320 --> 01:45:41.880]   and it goes out to 100 plus job boards right away,
[01:45:41.880 --> 01:45:43.240]   including social networks.
[01:45:43.240 --> 01:45:46.120]   That's really great because you're going to cast the broadest net.
[01:45:46.120 --> 01:45:48.680]   You're going to reach the most possible employees.
[01:45:48.680 --> 01:45:50.920]   But Zip Recruiter goes a step farther
[01:45:50.920 --> 01:45:53.720]   because they already have millions of resumes on file.
[01:45:53.720 --> 01:45:56.120]   See, people come to Zip Recruiter looking for work.
[01:45:56.120 --> 01:45:59.880]   They use matching technology to look at the resumes on file,
[01:45:59.880 --> 01:46:01.720]   to look at what you're looking for.
[01:46:01.720 --> 01:46:03.960]   And then when they find people with the right experience
[01:46:03.960 --> 01:46:07.240]   for your job, they will actively invite them to apply.
[01:46:07.240 --> 01:46:09.400]   They'll say, "Hey, we just got a new listing.
[01:46:09.400 --> 01:46:11.960]   This might be just right for you. You should apply."
[01:46:11.960 --> 01:46:15.160]   That's why four out of five employers
[01:46:15.160 --> 01:46:17.800]   who post on Zip Recruiter get a quality candidate
[01:46:17.800 --> 01:46:19.000]   within the first day.
[01:46:19.000 --> 01:46:23.320]   In fact, our experience is usually within the first hour.
[01:46:23.320 --> 01:46:24.520]   I'll never forget Lisa.
[01:46:24.520 --> 01:46:25.720]   We posted a breakfast.
[01:46:25.720 --> 01:46:29.480]   We needed a bookkeeper and she said, "Oh, like before lunch,
[01:46:29.480 --> 01:46:32.440]   before within an hour, this is really good candidate.
[01:46:32.440 --> 01:46:34.120]   Oh, another one. Another one.
[01:46:34.120 --> 01:46:35.800]   It really works."
[01:46:35.800 --> 01:46:37.560]   Now, I want you to try it for free right now,
[01:46:37.560 --> 01:46:38.120]   a dollar.
[01:46:38.120 --> 01:46:40.040]   Only listeners can get it linked.
[01:46:40.040 --> 01:46:43.560]   ziprecruiter.com/twit.
[01:46:43.560 --> 01:46:46.520]   If you're in a hurry, if you've got a tough job to fill,
[01:46:46.520 --> 01:46:49.480]   if you just don't want a whole bunch of email coming to your inbox
[01:46:49.480 --> 01:46:51.400]   and a whole bunch of phone calls coming to your phone,
[01:46:51.400 --> 01:46:54.760]   you got to try Zip Recruiter, the easiest way to hire,
[01:46:54.760 --> 01:46:57.480]   the fastest way to hire, the most effective way to hire.
[01:46:57.480 --> 01:46:59.880]   Go to ziprecruiter.com/twit.
[01:46:59.880 --> 01:47:01.800]   Right now, you could try it for free.
[01:47:01.800 --> 01:47:07.240]   ziprecruiter.com/twit.
[01:47:07.240 --> 01:47:13.320]   One more Apple story that really kind of frosted me a little bit.
[01:47:13.320 --> 01:47:16.280]   Apple has announced they're going to do lossless audio.
[01:47:16.280 --> 01:47:17.240]   Yeah, an Apple music.
[01:47:17.240 --> 01:47:23.720]   As low res, as CD quality, as high res is the highest res music.
[01:47:23.720 --> 01:47:27.640]   But you can't use it on your AirPods and your AirPods Pro
[01:47:27.640 --> 01:47:29.960]   or on your AirPods Pro Max on your three and the four.
[01:47:29.960 --> 01:47:33.720]   How much is that $555 headphones?
[01:47:33.720 --> 01:47:34.440]   That's crazy.
[01:47:34.440 --> 01:47:39.800]   Even with a wire, it won't work because it uses Bluetooth.
[01:47:39.800 --> 01:47:43.640]   And there's just not enough bandwidth to do high res audio.
[01:47:44.360 --> 01:47:47.560]   There was a rumor that maybe Apple is somehow going to patch these.
[01:47:47.560 --> 01:47:51.960]   I don't think they can because it's just the bandwidth and Bluetooth.
[01:47:51.960 --> 01:47:57.240]   If you could maybe there's some secret radio built in that they never told anybody about,
[01:47:57.240 --> 01:47:58.120]   that they're going to enable.
[01:47:58.120 --> 01:47:59.560]   But I don't think so.
[01:47:59.560 --> 01:48:01.080]   I was there at the start of Bluetooth.
[01:48:01.080 --> 01:48:05.400]   This was purely a keyboard amount designed as a keyboard amount.
[01:48:05.400 --> 01:48:07.160]   It was never designed for high quality music.
[01:48:07.160 --> 01:48:09.720]   And then they went to low-power, which was great,
[01:48:09.720 --> 01:48:12.280]   but never designed for something like this.
[01:48:12.280 --> 01:48:16.120]   So yeah, I mean, Apple's kind of stuffed itself there,
[01:48:16.120 --> 01:48:18.840]   but you know, Apple owns Beats and Beats,
[01:48:18.840 --> 01:48:21.400]   the most expensive thing in those headphones is the packaging.
[01:48:21.400 --> 01:48:23.240]   So Neil Young is not impressed.
[01:48:23.240 --> 01:48:24.840]   Oh, I bet Neil Young's not happy.
[01:48:24.840 --> 01:48:25.320]   No, yeah.
[01:48:25.320 --> 01:48:29.480]   Apple did admit that they are going to update the HomePod and HomePod
[01:48:29.480 --> 01:48:30.760]   MIDI to support this.
[01:48:30.760 --> 01:48:31.800]   The MIDI, it won't matter.
[01:48:31.800 --> 01:48:34.680]   It's a good speaker for what it is, but it's not a good speaker.
[01:48:34.680 --> 01:48:38.360]   You need like five speakers to really get to be with this.
[01:48:38.360 --> 01:48:39.400]   My curiosity with this.
[01:48:39.400 --> 01:48:40.600]   But at least those use Wi-Fi.
[01:48:40.600 --> 01:48:43.560]   So there's enough bandwidth that they can do lossless over Wi-Fi.
[01:48:43.560 --> 01:48:46.040]   Are you even going to hear the difference in a speaker like that?
[01:48:46.040 --> 01:48:46.760]   Absolutely not.
[01:48:46.760 --> 01:48:47.800]   No, no.
[01:48:47.800 --> 01:48:49.720]   I'm excited because it will work with the Apple TV.
[01:48:49.720 --> 01:48:53.640]   And I actually have my Apple TV hooked up to a really good DAC
[01:48:53.640 --> 01:48:54.840]   and really good speakers.
[01:48:54.840 --> 01:48:57.560]   And so that should sound pretty good.
[01:48:57.560 --> 01:48:57.880]   Yeah.
[01:48:57.880 --> 01:48:59.400]   So I'm happy about that.
[01:48:59.400 --> 01:49:00.760]   CD quality is fine.
[01:49:00.760 --> 01:49:01.720]   I don't need more than this.
[01:49:01.720 --> 01:49:01.800]   Right.
[01:49:01.800 --> 01:49:03.000]   Well, but I'll be happy about that.
[01:49:03.000 --> 01:49:06.200]   And it's the good thing is they made it free.
[01:49:06.200 --> 01:49:06.920]   Right.
[01:49:06.920 --> 01:49:09.560]   So you don't pay extra for it, which immediately Amazon said,
[01:49:09.560 --> 01:49:10.120]   "Oh, whoops."
[01:49:10.120 --> 01:49:12.520]   Because they were charging 14 something.
[01:49:12.520 --> 01:49:13.080]   Yeah.
[01:49:13.080 --> 01:49:14.200]   Like five bucks more a month.
[01:49:14.200 --> 01:49:14.680]   Yeah.
[01:49:14.680 --> 01:49:15.880]   So now it's free.
[01:49:15.880 --> 01:49:16.760]   It's also free.
[01:49:16.760 --> 01:49:18.760]   It's part of the base model.
[01:49:18.760 --> 01:49:19.800]   I think that we can--
[01:49:19.800 --> 01:49:20.760]   It's vital going to do.
[01:49:20.760 --> 01:49:20.920]   Yeah.
[01:49:20.920 --> 01:49:21.960]   It's exactly what it is.
[01:49:21.960 --> 01:49:22.760]   It's easier to figure it out.
[01:49:22.760 --> 01:49:23.160]   I don't know.
[01:49:23.160 --> 01:49:23.160]   I don't know.
[01:49:23.160 --> 01:49:24.600]   It's actually a secret sauce.
[01:49:24.600 --> 01:49:25.160]   I don't know.
[01:49:25.160 --> 01:49:25.640]   Yeah.
[01:49:25.640 --> 01:49:27.080]   So what is the music?
[01:49:27.080 --> 01:49:29.800]   Like, I don't know that I've actually experienced music
[01:49:29.800 --> 01:49:32.040]   that you could find on a catalog like Apple Music.
[01:49:32.040 --> 01:49:34.680]   That is Dolby Atmos compatible.
[01:49:34.680 --> 01:49:35.240]   Oh, yeah.
[01:49:35.240 --> 01:49:36.120]   This is the other thing.
[01:49:36.120 --> 01:49:37.240]   This is space for--
[01:49:37.240 --> 01:49:37.720]   This is more--
[01:49:37.720 --> 01:49:38.120]   Yeah.
[01:49:38.120 --> 01:49:39.880]   And I mean very spatial.
[01:49:39.880 --> 01:49:42.600]   It's spatial audio.
[01:49:42.600 --> 01:49:44.680]   And I think it's kind of gimmicky.
[01:49:44.680 --> 01:49:46.280]   It's like quiet.
[01:49:46.280 --> 01:49:46.760]   Mostly it's--
[01:49:46.760 --> 01:49:47.320]   Back in the 70s.
[01:49:47.320 --> 01:49:48.920]   So the--
[01:49:48.920 --> 01:49:50.680]   Well, at least as initially announced
[01:49:50.680 --> 01:49:52.680]   on the AirPods Pro and the Pro Max,
[01:49:52.680 --> 01:49:55.160]   it's so that if you turn your head--
[01:49:55.160 --> 01:49:56.360]   This is weird, but--
[01:49:56.360 --> 01:49:56.520]   Ah.
[01:49:56.520 --> 01:49:58.120]   That sounds still comes from the TV.
[01:49:58.120 --> 01:49:59.480]   Mm.
[01:49:59.480 --> 01:50:00.040]   I see.
[01:50:00.040 --> 01:50:00.680]   OK.
[01:50:00.680 --> 01:50:02.280]   That's a weird effect.
[01:50:02.280 --> 01:50:03.880]   And that was the first thing.
[01:50:03.880 --> 01:50:05.960]   Now they're saying, well, we can also do Dolby Atmos,
[01:50:05.960 --> 01:50:08.360]   which means that if you're--
[01:50:08.360 --> 01:50:12.280]   And remember the SACD and DVD audio?
[01:50:12.280 --> 01:50:13.080]   Remember?
[01:50:13.080 --> 01:50:17.240]   Back in the day, I remember we had the leader of Flaming Lips,
[01:50:17.240 --> 01:50:17.800]   Wayne--
[01:50:17.800 --> 01:50:19.720]   I can't remember his name on call for help--
[01:50:19.720 --> 01:50:20.280]   back--
[01:50:20.280 --> 01:50:20.840]   Raincoin.
[01:50:20.840 --> 01:50:21.320]   That's right.
[01:50:21.320 --> 01:50:22.360]   And 20--
[01:50:22.360 --> 01:50:25.160]   Probably this was 2001 or 2000.
[01:50:25.160 --> 01:50:27.880]   And he was all excited because they could use this new DVD
[01:50:27.880 --> 01:50:31.720]   format to have multi-track quadrificonic sound.
[01:50:31.720 --> 01:50:32.680]   Yeah.
[01:50:32.680 --> 01:50:34.760]   And so he was mixing and stuff that way.
[01:50:34.760 --> 01:50:37.400]   But that format, nobody used it, it died.
[01:50:37.400 --> 01:50:38.360]   It's gone.
[01:50:38.360 --> 01:50:40.040]   Some of a few audio files used it.
[01:50:40.040 --> 01:50:42.040]   But I think it is a case that there are a lot of people
[01:50:42.040 --> 01:50:43.400]   with 5,1, and 7,1, so--
[01:50:43.400 --> 01:50:43.880]   Well, yeah.
[01:50:43.880 --> 01:50:45.000]   I mean, they have--
[01:50:45.000 --> 01:50:47.560]   --amos systems in their house for their home theater.
[01:50:47.560 --> 01:50:48.040]   So maybe--
[01:50:48.040 --> 01:50:49.800]   --just not as used to that being a thing
[01:50:49.800 --> 01:50:50.840]   that you listen to music.
[01:50:50.840 --> 01:50:53.080]   You're not remixing yellow gold in 5,1?
[01:50:53.080 --> 01:50:54.200]   Not yet.
[01:50:54.200 --> 01:50:55.880]   Although I might have to think about it.
[01:50:55.880 --> 01:50:57.320]   You should.
[01:50:57.320 --> 01:50:59.560]   I don't know that I have enough going on in my music
[01:50:59.560 --> 01:51:01.960]   to spread it across five speakers.
[01:51:01.960 --> 01:51:04.280]   Well, because it's Apple, I imagine they'll be--
[01:51:04.280 --> 01:51:06.680]   I'm sure Bono's working on it, of course.
[01:51:06.680 --> 01:51:07.560]   [LAUGHTER]
[01:51:07.560 --> 01:51:08.760]   Good grief.
[01:51:08.760 --> 01:51:10.800]   The dark side of the moon, it's probably going to sound
[01:51:10.800 --> 01:51:11.640]   pretty awesome.
[01:51:11.640 --> 01:51:12.920]   Absolutely going to do it.
[01:51:12.920 --> 01:51:15.640]   Let's just make Bono history, OK?
[01:51:15.640 --> 01:51:18.280]   Actually, I'm sure Dark Side of the Moon has an SACD.
[01:51:18.280 --> 01:51:19.320]   In fact, John would know that.
[01:51:19.320 --> 01:51:19.880]   Yeah.
[01:51:19.880 --> 01:51:22.280]   Well, I mean, I used Dark Side of the Moon as the example,
[01:51:22.280 --> 01:51:25.800]   because I remember Dark Side of the Moon had a quad version
[01:51:25.800 --> 01:51:26.840]   back in the 70s.
[01:51:26.840 --> 01:51:29.080]   --when Quadrificonic and DLS.
[01:51:29.080 --> 01:51:30.840]   Well, in Brian Eno, I'm being one.
[01:51:30.840 --> 01:51:33.080]   You know, he actually put on the album
[01:51:33.080 --> 01:51:35.400]   the position as to where you should put the bass--
[01:51:35.400 --> 01:51:36.120]   --the bass speed in the--
[01:51:36.120 --> 01:51:36.600]   I think it's--
[01:51:36.600 --> 01:51:37.600]   --right.
[01:51:37.600 --> 01:51:38.880]   --I'm not a musician.
[01:51:38.880 --> 01:51:41.080]   I do love music, but I think it's a little gimmicky.
[01:51:41.080 --> 01:51:41.480]   Yeah.
[01:51:41.480 --> 01:51:42.680]   I think you can get--
[01:51:42.680 --> 01:51:45.720]   look, if you go to a symphony, you're
[01:51:45.720 --> 01:51:47.000]   sitting in the audience.
[01:51:47.000 --> 01:51:49.640]   The sound stage is as wide as the proscenium arch
[01:51:49.640 --> 01:51:50.760]   and the symphony's there.
[01:51:50.760 --> 01:51:52.440]   I think stereo speakers can actually
[01:51:52.440 --> 01:51:53.800]   duplicate that pretty well.
[01:51:53.800 --> 01:51:53.960]   Right.
[01:51:53.960 --> 01:51:55.720]   There's no cellist above your head.
[01:51:55.720 --> 01:51:56.440]   Totally.
[01:51:56.440 --> 01:51:57.880]   Or in the back of the auditorium.
[01:51:57.880 --> 01:51:59.000]   I think that's fine.
[01:51:59.000 --> 01:51:59.240]   Yeah.
[01:51:59.240 --> 01:52:00.440]   I don't need--
[01:52:00.440 --> 01:52:02.040]   I don't need--
[01:52:02.040 --> 01:52:03.240]   --frying to sneak out on a--
[01:52:03.240 --> 01:52:05.160]   I think they should put the guy with the cymbals
[01:52:05.160 --> 01:52:07.200]   like in the back behind you.
[01:52:07.200 --> 01:52:08.200]   It is weird.
[01:52:08.200 --> 01:52:08.600]   It's really weird.
[01:52:08.600 --> 01:52:09.960]   Surround set in--
[01:52:09.960 --> 01:52:12.840]   like, when you listen to a ball game and the beer guy
[01:52:12.840 --> 01:52:14.680]   is behind you, it's kind of strange.
[01:52:14.680 --> 01:52:15.240]   Yeah.
[01:52:15.240 --> 01:52:18.240]   You see, I've-- well, I got an Dolby invite journalist
[01:52:18.240 --> 01:52:20.840]   to film screening as occasionally to show off
[01:52:20.840 --> 01:52:22.640]   the latest sound technology.
[01:52:22.640 --> 01:52:26.760]   And we went to see the first Star Trek reboot.
[01:52:26.760 --> 01:52:28.840]   It was at the downtown Dolby Theater down to there.
[01:52:28.840 --> 01:52:29.760]   Yeah, they're just like--
[01:52:29.760 --> 01:52:30.760]   --it's really nice.
[01:52:30.760 --> 01:52:32.080]   This is the Atmos thing.
[01:52:32.080 --> 01:52:34.160]   They explained about the number of speakers around there.
[01:52:34.160 --> 01:52:35.080]   And you're like--
[01:52:35.080 --> 01:52:35.600]   and it was.
[01:52:35.600 --> 01:52:37.040]   The sound was really good.
[01:52:37.040 --> 01:52:38.560]   The movie was--
[01:52:38.560 --> 01:52:39.080]   but--
[01:52:39.080 --> 01:52:41.480]   So there's the problem in a nutshell.
[01:52:41.480 --> 01:52:45.240]   I mean, it's like, yeah, we have 38 speakers positioned
[01:52:45.240 --> 01:52:46.000]   around the room.
[01:52:46.000 --> 01:52:47.600]   And you're like, this is never going
[01:52:47.600 --> 01:52:49.240]   to be a commercial system.
[01:52:49.240 --> 01:52:49.840]   Actually, but--
[01:52:49.840 --> 01:52:50.400]   --is something for cinemas.
[01:52:50.400 --> 01:52:51.800]   There are a lot of cinemas that do it.
[01:52:51.800 --> 01:52:52.400]   But you're not at all.
[01:52:52.400 --> 01:52:53.760]   No, no, no, cinemas will do it.
[01:52:53.760 --> 01:52:56.320]   But you know, on Discord, someone's
[01:52:56.320 --> 01:52:57.880]   saying, I've got a five in one.
[01:52:57.880 --> 01:53:00.600]   I'm presuming everyone who most people have a seven in one.
[01:53:00.600 --> 01:53:01.600]   Yeah, five, seven.
[01:53:01.600 --> 01:53:02.880]   Yeah, because we're tech enthusiasts.
[01:53:02.880 --> 01:53:05.280]   But for your average, bloke's on the street.
[01:53:05.280 --> 01:53:07.200]   Well, more people probably have a sound on TV.
[01:53:07.200 --> 01:53:08.520]   Yeah, it's like a stereo sound.
[01:53:08.520 --> 01:53:09.520]   Or maybe you're right.
[01:53:09.520 --> 01:53:10.120]   Or something.
[01:53:10.120 --> 01:53:10.520]   Exactly.
[01:53:10.520 --> 01:53:12.120]   It feels like a little gimmick.
[01:53:12.120 --> 01:53:14.000]   Anyway, I'm glad Apple's doing it.
[01:53:14.000 --> 01:53:15.600]   Well, I'm glad if they're doing it.
[01:53:15.600 --> 01:53:16.200]   It's no ex-charge.
[01:53:16.200 --> 01:53:16.720]   It will help, which--
[01:53:16.720 --> 01:53:18.360]   --because even if it's a gimmick.
[01:53:18.360 --> 01:53:19.160]   Yes, that's--
[01:53:19.160 --> 01:53:19.440]   No, it's simple.
[01:53:19.440 --> 01:53:20.440]   Some say hi, Riz.
[01:53:20.440 --> 01:53:22.240]   I mean, we've talked about this all week.
[01:53:22.240 --> 01:53:25.800]   And a number of people in our chat room say, oh, come on.
[01:53:25.800 --> 01:53:29.720]   Are you saying that a 384 kilobit AAC is going to sound
[01:53:29.720 --> 01:53:31.400]   any different than a CD quality?
[01:53:31.400 --> 01:53:31.960]   Yeah.
[01:53:31.960 --> 01:53:33.800]   I think you're probably-- most people won't.
[01:53:33.800 --> 01:53:34.320]   Certainly not.
[01:53:34.320 --> 01:53:34.320]   No.
[01:53:34.320 --> 01:53:36.680]   The majority of people are not going to notice the difference.
[01:53:36.680 --> 01:53:39.920]   And I-- as someone who cares a lot about music,
[01:53:39.920 --> 01:53:43.040]   I have long been on the side of like, no, but it matters.
[01:53:43.040 --> 01:53:45.240]   It matters because when you sit down
[01:53:45.240 --> 01:53:49.160]   and put the needle on a record with a good sound system,
[01:53:49.160 --> 01:53:51.600]   there is something visceral about that experience that
[01:53:51.600 --> 01:53:52.600]   is different.
[01:53:52.600 --> 01:53:54.640]   And I can't put my finger on what it is.
[01:53:54.640 --> 01:53:56.840]   At the same time, I was kind of reading through this stuff
[01:53:56.840 --> 01:53:58.200]   ahead of this show.
[01:53:58.200 --> 01:54:01.680]   And I can't remember if it was in a comment on a Verge article,
[01:54:01.680 --> 01:54:03.360]   but someone pointed out the fact that when it comes
[01:54:03.360 --> 01:54:06.800]   to high definition video, we don't scrutinize it in the same way
[01:54:06.800 --> 01:54:07.680]   that we do with audio.
[01:54:07.680 --> 01:54:11.400]   With high definition video, it's all compressed to some degree.
[01:54:11.400 --> 01:54:14.080]   Yet it's totally good enough for the video files,
[01:54:14.080 --> 01:54:15.280]   the spin of files.
[01:54:15.280 --> 01:54:17.320]   But with audio, for some reason, it's not.
[01:54:17.320 --> 01:54:19.600]   And there's a disconnection there.
[01:54:19.600 --> 01:54:22.360]   Like, it probably is just-- it probably is great.
[01:54:22.360 --> 01:54:25.080]   I think the opportunity here is for Leo,
[01:54:25.080 --> 01:54:27.960]   for Twitch shows to be surrounded.
[01:54:27.960 --> 01:54:29.280]   We did one.
[01:54:29.280 --> 01:54:31.000]   We did one many years ago.
[01:54:31.000 --> 01:54:33.560]   And like, it's like you're in the middle and all the guests
[01:54:33.560 --> 01:54:34.360]   are all around you.
[01:54:34.360 --> 01:54:36.840]   We use Dolby Audio to encode it.
[01:54:36.840 --> 01:54:40.120]   So Dolby had a thing that never came out in the US
[01:54:40.120 --> 01:54:43.760]   called Headphone Audio that was simulated because it
[01:54:43.760 --> 01:54:44.920]   was with headphones.
[01:54:44.920 --> 01:54:47.640]   But it was very much spatial.
[01:54:47.640 --> 01:54:51.240]   And we did-- in fact, I think we were at the 21st Amendment
[01:54:51.240 --> 01:54:52.480]   group pub, as I remember.
[01:54:52.480 --> 01:54:54.040]   Or we were at some group pub.
[01:54:54.040 --> 01:54:55.040]   We did.
[01:54:55.040 --> 01:54:57.240]   It was the 21st Amendment group pub and remember it.
[01:54:57.240 --> 01:54:58.280]   Yeah, right.
[01:54:58.280 --> 01:54:59.320]   They didn't have Mezcal.
[01:54:59.320 --> 01:55:01.240]   I can tell you that right now.
[01:55:01.240 --> 01:55:03.160]   We did a Mac break weekly that way.
[01:55:03.160 --> 01:55:04.120]   Alex Lindsey helped us.
[01:55:04.120 --> 01:55:05.400]   And we had special recording.
[01:55:05.400 --> 01:55:07.560]   And we had the Dolby engineers come out.
[01:55:07.560 --> 01:55:09.680]   We did one.
[01:55:09.680 --> 01:55:10.920]   Well, we have what I mean.
[01:55:10.920 --> 01:55:11.840]   It's not bad.
[01:55:11.840 --> 01:55:13.280]   It's a little scary.
[01:55:13.280 --> 01:55:14.120]   It's scary.
[01:55:14.120 --> 01:55:15.200]   Somebody chimes in.
[01:55:15.200 --> 01:55:15.760]   You're like, well--
[01:55:15.760 --> 01:55:21.440]   Yeah, and I honestly think that people, they just adjust.
[01:55:21.440 --> 01:55:22.440]   Yes.
[01:55:22.440 --> 01:55:24.840]   They know you're there and I'm here.
[01:55:24.840 --> 01:55:26.320]   I don't think--
[01:55:26.320 --> 01:55:28.560]   I think it's a great effort to do stuff.
[01:55:28.560 --> 01:55:32.800]   I think it matters an awful lot to a few people.
[01:55:32.800 --> 01:55:33.320]   Yeah.
[01:55:33.320 --> 01:55:35.120]   It doesn't matter what's all to a lot of people.
[01:55:35.120 --> 01:55:35.720]   There you go.
[01:55:35.720 --> 01:55:36.760]   That's exactly right.
[01:55:36.760 --> 01:55:37.440]   Yeah, right.
[01:55:37.440 --> 01:55:40.240]   Episode 23, by the way, Mac break weekly, episode 23.
[01:55:40.240 --> 01:55:43.600]   Dolby's around January 10, 2007.
[01:55:43.600 --> 01:55:44.800]   That's how long ago--
[01:55:44.800 --> 01:55:45.800]   14 years ago.
[01:55:45.800 --> 01:55:47.560]   --a final attack, it's happened.
[01:55:47.560 --> 01:55:48.600]   It's in Dudley.
[01:55:48.600 --> 01:55:49.120]   Right.
[01:55:49.120 --> 01:55:49.760]   It's in Dudley.
[01:55:49.760 --> 01:55:51.920]   It's in Dudley, yes.
[01:55:51.920 --> 01:55:55.400]   I would do it that way if there were a lot of people who'd hear it.
[01:55:55.400 --> 01:55:58.880]   So maybe now that Apple's doing it, maybe do
[01:55:58.880 --> 01:56:01.040]   a spatial audio versions of our show,
[01:56:01.040 --> 01:56:03.040]   it would just be panning everybody a little bit differently.
[01:56:03.040 --> 01:56:05.120]   Yeah, I mean, but again, all the work,
[01:56:05.120 --> 01:56:08.840]   like the extra work and effort for how much gain?
[01:56:08.840 --> 01:56:10.280]   Like, yeah.
[01:56:10.280 --> 01:56:11.320]   Pretty small.
[01:56:11.320 --> 01:56:13.240]   I had a little argument in our Discord
[01:56:13.240 --> 01:56:15.520]   with somebody who said, you're saying records
[01:56:15.520 --> 01:56:18.080]   sound better than digital.
[01:56:18.080 --> 01:56:19.720]   And I think I'm with you, Jason.
[01:56:19.720 --> 01:56:20.280]   I think they do.
[01:56:20.280 --> 01:56:22.480]   I mean, there's pops and clicks and they get damaged.
[01:56:22.480 --> 01:56:23.920]   They're imperfect.
[01:56:23.920 --> 01:56:27.240]   But you're hearing directly, especially
[01:56:27.240 --> 01:56:29.360]   if it's a direct to disc recording,
[01:56:29.360 --> 01:56:33.240]   you're hearing the sound wave come out of Miles Davis trumpet
[01:56:33.240 --> 01:56:35.280]   into the recording, into your ear.
[01:56:35.280 --> 01:56:37.240]   And also the reverberation off the wall,
[01:56:37.240 --> 01:56:39.520]   so you can hear the room better, I think.
[01:56:39.520 --> 01:56:41.000]   Spatialities better.
[01:56:41.000 --> 01:56:43.280]   I mean, I do kind of miss that--
[01:56:43.280 --> 01:56:44.280]   Shh.
[01:56:44.280 --> 01:56:44.800]   Yeah.
[01:56:44.800 --> 01:56:47.000]   There's no study there's an aesthetic about it
[01:56:47.000 --> 01:56:48.480]   that is really pleasing.
[01:56:48.480 --> 01:56:49.080]   Exactly.
[01:56:49.080 --> 01:56:51.080]   That could be just total nostalgia.
[01:56:51.080 --> 01:56:52.560]   I think-- I mean, I think--
[01:56:52.560 --> 01:56:56.280]   Honestly, if you told your children in 10 years time,
[01:56:56.280 --> 01:56:59.240]   it's like, yeah, we used to burn this onto a piece of plastic
[01:56:59.240 --> 01:57:03.480]   and take a diamond and rub it down the groove of the record.
[01:57:03.480 --> 01:57:04.480]   That's pretty crazy.
[01:57:04.480 --> 01:57:05.280]   And if they were in really advanced,
[01:57:05.280 --> 01:57:06.840]   you'd get double grooves on the record.
[01:57:06.840 --> 01:57:07.840]   Yeah, that's pretty crazy.
[01:57:07.840 --> 01:57:09.680]   So you got two tracks on the same disc.
[01:57:09.680 --> 01:57:10.280]   Yeah.
[01:57:10.280 --> 01:57:13.200]   I fear it's nostalgia, but I miss it terribly.
[01:57:13.200 --> 01:57:13.800]   Yeah.
[01:57:13.800 --> 01:57:14.280]   Yeah.
[01:57:14.280 --> 01:57:15.400]   Me too.
[01:57:15.400 --> 01:57:16.400]   Leaked emails.
[01:57:16.400 --> 01:57:18.280]   Do you know the app citizen, which
[01:57:18.280 --> 01:57:20.600]   got a lot of heat if you want to be paranoid,
[01:57:20.600 --> 01:57:21.400]   listen to citizens.
[01:57:21.400 --> 01:57:23.120]   Yeah, so it's like a neighborhood app,
[01:57:23.120 --> 01:57:26.000]   and it will tell you what crime is going on in your neighborhood,
[01:57:26.000 --> 01:57:29.800]   which is usually, unfortunately, there's a black male walking
[01:57:29.800 --> 01:57:33.280]   by, and I think he might be up to no good.
[01:57:33.280 --> 01:57:35.280]   It's like next door for a really sad people.
[01:57:35.280 --> 01:57:38.160]   Yeah, and it's a little racist often.
[01:57:38.160 --> 01:57:40.840]   And it's certainly designed to scare you.
[01:57:40.840 --> 01:57:43.000]   Now, this should really scare you.
[01:57:43.000 --> 01:57:44.800]   According to Motherboard, leaked emails
[01:57:44.800 --> 01:57:47.920]   show that citizen is testing an on demand security
[01:57:47.920 --> 01:57:50.640]   force so bad.
[01:57:50.640 --> 01:57:52.720]   Basically, vigilante army.
[01:57:52.720 --> 01:57:53.280]   Right.
[01:57:53.280 --> 01:57:53.680]   Right.
[01:57:53.680 --> 01:57:56.080]   It's like a-- it's like a--
[01:57:56.080 --> 01:57:59.000]   it's like, you know, best by having a tech support division
[01:57:59.000 --> 01:58:01.000]   with Volkswagen Beatles, coming out,
[01:58:01.000 --> 01:58:04.800]   except they're probably armed and have badges and, you know,
[01:58:04.800 --> 01:58:05.360]   criminal records.
[01:58:05.360 --> 01:58:06.800]   I mean, as C'mon Lovins says on--
[01:58:06.800 --> 01:58:07.680]   Possibly go wrong.
[01:58:07.680 --> 01:58:10.360]   As my Lovins says on Discord, and I agree with them
[01:58:10.360 --> 01:58:13.360]   this one, can you imagine what HOA is going to do with this?
[01:58:13.360 --> 01:58:13.680]   Yeah.
[01:58:13.680 --> 01:58:15.240]   You know, it's just like the ability
[01:58:15.240 --> 01:58:17.880]   to just swap your neighbors if you don't like them.
[01:58:17.880 --> 01:58:18.880]   Yeah.
[01:58:18.880 --> 01:58:20.400]   It's a horrific test.
[01:58:20.400 --> 01:58:21.320]   Thanks for the first.
[01:58:21.320 --> 01:58:21.960]   Yeah.
[01:58:21.960 --> 01:58:25.360]   And it really-- if you understand what citizen was
[01:58:25.360 --> 01:58:26.480]   all along doing--
[01:58:26.480 --> 01:58:27.000]   Mm-hmm.
[01:58:27.000 --> 01:58:28.640]   --because this was really what the point was.
[01:58:28.640 --> 01:58:29.280]   Yes.
[01:58:29.280 --> 01:58:31.160]   This is the natural next step.
[01:58:31.160 --> 01:58:31.600]   Yes.
[01:58:31.600 --> 01:58:32.680]   You know?
[01:58:32.680 --> 01:58:35.800]   And reality crime on an on a lap.
[01:58:35.800 --> 01:58:36.360]   But just--
[01:58:36.360 --> 01:58:38.080]   Yeah, it's kind of terrifying.
[01:58:38.080 --> 01:58:40.040]   It's pretty skilmy.
[01:58:40.040 --> 01:58:44.080]   I mean, my guy used to work with HOA on his phone
[01:58:44.080 --> 01:58:45.440]   because he lived in San Francisco,
[01:58:45.440 --> 01:58:47.120]   and he was just-- you know, he just moved here,
[01:58:47.120 --> 01:58:48.880]   and he's very curious about it.
[01:58:48.880 --> 01:58:51.680]   And it was constantly like, someone stabbed on the street.
[01:58:51.680 --> 01:58:52.680]   Someone stabbed--
[01:58:52.680 --> 01:58:53.200]   It's just like--
[01:58:53.200 --> 01:58:54.200]   It's like, welcome to San Francisco.
[01:58:54.200 --> 01:58:55.520]   Yeah, welcome to a sissy.
[01:58:55.520 --> 01:58:55.960]   You know?
[01:58:55.960 --> 01:58:59.920]   I mean, I was raised in a small town with like 2,000 people
[01:58:59.920 --> 01:59:00.760]   in it.
[01:59:00.760 --> 01:59:03.480]   And yeah, you could have the citizens and go, yeah,
[01:59:03.480 --> 01:59:05.600]   someone's got his face punched in on a Friday night
[01:59:05.600 --> 01:59:06.360]   outside the pub.
[01:59:06.360 --> 01:59:07.320]   It happens.
[01:59:07.320 --> 01:59:10.080]   Our local paper-- I mean, this is Petaloma.
[01:59:10.080 --> 01:59:13.120]   It's 55,000 people-- has stopped doing its police
[01:59:13.120 --> 01:59:16.720]   blotter, which was the best worry of the local paper
[01:59:16.720 --> 01:59:19.200]   was all of this kind of petty crime and--
[01:59:19.200 --> 01:59:20.200]   Oh, like a serious--
[01:59:20.200 --> 01:59:21.200]   --for the crime.
[01:59:21.200 --> 01:59:22.760]   I can see my email as crime.
[01:59:22.760 --> 01:59:25.440]   I think it was because they had the same fear
[01:59:25.440 --> 01:59:28.040]   that this was kind of stimulating,
[01:59:28.040 --> 01:59:29.720]   kind of an irrational paranoia.
[01:59:29.720 --> 01:59:30.560]   Right.
[01:59:30.560 --> 01:59:33.720]   And they were concerned about it.
[01:59:33.720 --> 01:59:36.160]   The police blotter, like Citizen,
[01:59:36.160 --> 01:59:41.320]   is biased towards the complaints, not the actual activity.
[01:59:41.320 --> 01:59:42.320]   Yeah.
[01:59:42.320 --> 01:59:43.800]   So like, you know, so-- I used to do this for these--
[01:59:43.800 --> 01:59:44.280]   It isn't.
[01:59:44.280 --> 01:59:45.280]   Yeah, that's exactly what it is.
[01:59:45.280 --> 01:59:47.280]   Yeah.
[01:59:47.280 --> 01:59:48.720]   No, I get it.
[01:59:48.720 --> 01:59:50.120]   So I get it.
[01:59:50.120 --> 01:59:52.480]   It comes up on email because I'm on the emergency response
[01:59:52.480 --> 01:59:54.360]   team for our community because that's
[01:59:54.360 --> 01:59:56.320]   what we're trying for the earthquake.
[01:59:56.320 --> 02:00:00.800]   But the stuff that you get-- it's just really like, yeah,
[02:00:00.800 --> 02:00:02.920]   a car window was broken in such and such.
[02:00:02.920 --> 02:00:04.080]   Nothing was stolen.
[02:00:04.080 --> 02:00:09.920]   And apps like that just amplify people's concern
[02:00:09.920 --> 02:00:11.960]   that they have no reason to be concerned about.
[02:00:11.960 --> 02:00:14.480]   Or perception that the world is actually a worse place
[02:00:14.480 --> 02:00:15.680]   than it actually is.
[02:00:15.680 --> 02:00:16.680]   Absolutely.
[02:00:16.680 --> 02:00:17.680]   You nailed it.
[02:00:17.680 --> 02:00:19.280]   Because my favorite thing about small town papers
[02:00:19.280 --> 02:00:21.960]   is the police blotter because it's almost always
[02:00:21.960 --> 02:00:23.360]   historically funny.
[02:00:23.360 --> 02:00:24.880]   But I actually admire this.
[02:00:24.880 --> 02:00:25.640]   This is what they said.
[02:00:25.640 --> 02:00:28.200]   We recognize that weekly report of names and charges
[02:00:28.200 --> 02:00:30.320]   can peak reader interest.
[02:00:30.320 --> 02:00:32.720]   We acknowledge that many of the crimes alleged in the weekly
[02:00:32.720 --> 02:00:35.360]   feature are troubling, including physical injury,
[02:00:35.360 --> 02:00:37.400]   proper loss damage, and drunken driving.
[02:00:37.400 --> 02:00:39.320]   We acknowledge that these reports are rightfully
[02:00:39.320 --> 02:00:41.160]   a matter of the public record.
[02:00:41.160 --> 02:00:44.160]   But not everyone arrested is guilty.
[02:00:44.160 --> 02:00:45.120]   Yeah.
[02:00:45.120 --> 02:00:49.000]   And this is a larger national movement among news outlets
[02:00:49.000 --> 02:00:52.000]   to acknowledge how our coverage choices can have lasting
[02:00:52.000 --> 02:00:54.360]   and disproportionate impacts on black, indigenous,
[02:00:54.360 --> 02:00:57.720]   and people of color, as well as under-resourced residents.
[02:00:57.720 --> 02:01:02.720]   And in fact, in this article on Motherboard about citizen,
[02:01:02.720 --> 02:01:05.680]   they point out the citizen offered a $30,000 bounty
[02:01:05.680 --> 02:01:06.560]   against a person.
[02:01:06.560 --> 02:01:08.640]   It falsely accused of starting a while.
[02:01:08.640 --> 02:01:09.720]   Wow.
[02:01:09.720 --> 02:01:11.760]   So there really are issues.
[02:01:11.760 --> 02:01:13.320]   Oh, absolutely.
[02:01:13.320 --> 02:01:15.600]   I think the best use of the police blotter
[02:01:15.600 --> 02:01:19.160]   is that the crime desk at any local paper
[02:01:19.160 --> 02:01:21.880]   should be obsessing over it looking for stories.
[02:01:21.880 --> 02:01:23.080]   But to publish it directly.
[02:01:23.080 --> 02:01:24.360]   It's very speaking, yeah, exactly.
[02:01:24.360 --> 02:01:25.200]   Yeah, yeah.
[02:01:25.200 --> 02:01:26.800]   No, I mean, this is it.
[02:01:26.800 --> 02:01:30.120]   It's kind of like we changed policy at the Redge
[02:01:30.120 --> 02:01:33.560]   a couple of years ago, about five years ago now.
[02:01:33.560 --> 02:01:36.200]   In the-- when there was a mass shooting,
[02:01:36.200 --> 02:01:37.600]   you didn't name the shooter.
[02:01:37.600 --> 02:01:38.120]   Right.
[02:01:38.120 --> 02:01:40.000]   Because that's what they're after.
[02:01:40.000 --> 02:01:41.000]   That's what they're writing about.
[02:01:41.000 --> 02:01:41.360]   The fame.
[02:01:41.360 --> 02:01:42.800]   So it's just like--
[02:01:42.800 --> 02:01:45.520]   I wish that would become the same thing.
[02:01:45.520 --> 02:01:47.360]   Yeah, I mean, we basically--
[02:01:47.360 --> 02:01:50.200]   I think our standard term now is someone whose name
[02:01:50.200 --> 02:01:52.800]   isn't worth remembering.
[02:01:52.800 --> 02:01:54.800]   Perfect.
[02:01:54.800 --> 02:01:56.800]   Because studies have shown--
[02:01:56.800 --> 02:01:58.000]   Don't give them the variety.
[02:01:58.000 --> 02:01:59.800]   Yeah, that's what they want.
[02:01:59.800 --> 02:02:02.520]   And we should do the same thing here.
[02:02:02.520 --> 02:02:04.920]   So your article, Blue Eye Origin,
[02:02:04.920 --> 02:02:07.720]   has set a price for your trip into space.
[02:02:07.720 --> 02:02:10.880]   I think the Blue Origin is not really a space flight.
[02:02:10.880 --> 02:02:11.920]   It's more like a--
[02:02:11.920 --> 02:02:13.600]   Harp into the lower atmosphere.
[02:02:13.600 --> 02:02:14.560]   This is the problem.
[02:02:14.560 --> 02:02:15.240]   Harp to atmosphere.
[02:02:15.240 --> 02:02:16.280]   Technically, it is.
[02:02:16.280 --> 02:02:19.360]   Because we've got this thing called the Carmen line, which
[02:02:19.360 --> 02:02:20.960]   is--
[02:02:20.960 --> 02:02:22.760]   62 miles above the sea, I see that.
[02:02:22.760 --> 02:02:25.680]   Yes, it's miles 100 kilometers in non-freedom units.
[02:02:25.680 --> 02:02:28.800]   And basically, once you get above that,
[02:02:28.800 --> 02:02:30.720]   that's the point where aircraft--
[02:02:30.720 --> 02:02:32.880]   the lift the aircraft can generate,
[02:02:32.880 --> 02:02:35.280]   won't get you anywhere beyond that.
[02:02:35.280 --> 02:02:37.160]   And therefore, you're considered to be in space.
[02:02:37.160 --> 02:02:38.560]   You're not in air any longer.
[02:02:38.560 --> 02:02:39.240]   Exactly.
[02:02:39.240 --> 02:02:40.520]   But let's be honest.
[02:02:40.520 --> 02:02:45.560]   I mean, a proper astronaut has to master two or three
[02:02:45.560 --> 02:02:48.640]   professional skills, which could earn them a shed load of money
[02:02:48.640 --> 02:02:51.720]   in the private sector, spend years waiting for a spot
[02:02:51.720 --> 02:02:54.000]   to get into space.
[02:02:54.000 --> 02:02:55.680]   They are astronauts.
[02:02:55.680 --> 02:02:59.040]   People that go to the ISS, like Tom Cruise later this year--
[02:02:59.040 --> 02:03:00.440]   He's gone to the ISS.
[02:03:00.440 --> 02:03:00.960]   Yes.
[02:03:00.960 --> 02:03:01.440]   He's going to film.
[02:03:01.440 --> 02:03:03.080]   And the Russians are going to beat him with an actor.
[02:03:03.080 --> 02:03:04.000]   The Russians have beat--
[02:03:04.000 --> 02:03:07.000]   Have they want to be the first to have an actor in space?
[02:03:07.000 --> 02:03:07.560]   Yeah.
[02:03:07.560 --> 02:03:08.240]   But even then--
[02:03:08.240 --> 02:03:11.880]   Is it for a movie or is he for a mission impossible, right?
[02:03:11.880 --> 02:03:12.880]   I think it's a mission possible.
[02:03:12.880 --> 02:03:13.720]   That's a great stunt.
[02:03:13.720 --> 02:03:13.920]   Yeah.
[02:03:13.920 --> 02:03:15.200]   That's actually a great stunt.
[02:03:15.200 --> 02:03:17.640]   But even then, he's got to spend three months getting the--
[02:03:17.640 --> 02:03:18.640]   What's the insurance that I'm getting?
[02:03:18.640 --> 02:03:20.120]   Getting the language skills.
[02:03:20.120 --> 02:03:21.120]   Oh, come on.
[02:03:21.120 --> 02:03:22.880]   Tom Cruise, the way he does his own stunts,
[02:03:22.880 --> 02:03:25.320]   he broke his ankle on one of those fins.
[02:03:25.320 --> 02:03:26.080]   Wow.
[02:03:26.080 --> 02:03:28.160]   His hair has its own insurance policy.
[02:03:28.160 --> 02:03:29.880]   But even then, he's going to have some training.
[02:03:29.880 --> 02:03:31.560]   He's going to have three months of training.
[02:03:31.560 --> 02:03:33.160]   This is truly ISS.
[02:03:33.160 --> 02:03:34.560]   This is basically--
[02:03:34.560 --> 02:03:37.240]   I have at least 1.4 million.
[02:03:37.240 --> 02:03:40.600]   I will spend three minutes in freefall
[02:03:40.600 --> 02:03:42.200]   and get a great view of the world.
[02:03:42.200 --> 02:03:45.120]   This is not going the far back comment for just a couple
[02:03:45.120 --> 02:03:46.200]   hundred thousand.
[02:03:46.200 --> 02:03:47.880]   Well, 7,000 to go in the moment.
[02:03:47.880 --> 02:03:48.400]   Is that all?
[02:03:48.400 --> 02:03:49.520]   7,000 bucks.
[02:03:49.520 --> 02:03:50.880]   We'll get you out of all of it.
[02:03:50.880 --> 02:03:51.360]   You want to hear yourself?
[02:03:51.360 --> 02:03:51.880]   Let's do it.
[02:03:51.880 --> 02:03:52.400]   What do you say?
[02:03:52.400 --> 02:03:53.320]   Yeah, let's do it.
[02:03:53.320 --> 02:03:55.760]   I'd be a friend and a heartbeat if I could just
[02:03:55.760 --> 02:03:56.680]   to fight with my woman better.
[02:03:56.680 --> 02:03:58.880]   I turned down the Blue Angels not once, not twice,
[02:03:58.880 --> 02:03:59.600]   but three times.
[02:03:59.600 --> 02:04:01.120]   So I am not the guy--
[02:04:01.120 --> 02:04:04.880]   And they deliver you back to Earth overnight.
[02:04:04.880 --> 02:04:05.560]   Exactly.
[02:04:05.560 --> 02:04:06.560]   Where is this Amazon, right?
[02:04:06.560 --> 02:04:10.520]   This is-- I mean, the whole Blue Origin thing.
[02:04:10.520 --> 02:04:11.840]   Only if you're a prime member.
[02:04:11.840 --> 02:04:13.320]   It's actually a perfect--
[02:04:13.320 --> 02:04:14.360]   Amazon Prime--
[02:04:14.360 --> 02:04:17.800]   Amazon said, told you guys that there was a lot of interest
[02:04:17.800 --> 02:04:21.360]   for this $1.4 million ride--
[02:04:21.360 --> 02:04:23.120]   50, $2 million for the bidding.
[02:04:23.120 --> 02:04:25.840]   50, 200 bidders.
[02:04:25.840 --> 02:04:27.200]   I bet they're all Bitcoin bros.
[02:04:27.200 --> 02:04:30.200]   And I wonder if the bidding is off now that Bitcoin's down.
[02:04:30.200 --> 02:04:32.280]   Yeah, a few people have sold out.
[02:04:32.280 --> 02:04:33.520]   Yeah, probably so.
[02:04:33.520 --> 02:04:34.360]   Wouldn't that be funny if--
[02:04:34.360 --> 02:04:38.000]   I mean, we should-- Oh, all these people reduce their offers now.
[02:04:38.000 --> 02:04:39.560]   But we should call these what they are.
[02:04:39.560 --> 02:04:41.840]   These are space tourists.
[02:04:41.840 --> 02:04:44.560]   You are not an astronaut, just because you happen
[02:04:44.560 --> 02:04:48.440]   to get beyond the 62-mile mark doesn't make you an astronaut.
[02:04:48.440 --> 02:04:49.440]   You're a passenger.
[02:04:49.440 --> 02:04:50.440]   Yeah.
[02:04:50.440 --> 02:04:53.360]   So I'd like to recommend a movie actually called Stowaway.
[02:04:53.360 --> 02:04:55.280]   I don't know if you guys have seen it, but this is a movie.
[02:04:55.280 --> 02:04:56.520]   I saw the tridefors.
[02:04:56.520 --> 02:04:58.680]   On a mission of Mars, there's somebody on board
[02:04:58.680 --> 02:05:00.120]   who is not a trained astronaut.
[02:05:00.120 --> 02:05:02.320]   And so it deals with this very same issue.
[02:05:02.320 --> 02:05:03.600]   Was it Seth Rogen?
[02:05:03.600 --> 02:05:04.600]   No, because I don't want it.
[02:05:04.600 --> 02:05:05.600]   OK.
[02:05:05.600 --> 02:05:06.840]   No, he's a little over-explanatory now.
[02:05:06.840 --> 02:05:08.200]   No, I saw the tridefors.
[02:05:08.200 --> 02:05:09.160]   It's great.
[02:05:09.160 --> 02:05:10.160]   It's a great science.
[02:05:10.160 --> 02:05:11.360]   You've seen it?
[02:05:11.360 --> 02:05:12.360]   I've seen it.
[02:05:12.360 --> 02:05:13.640]   The science is great.
[02:05:13.640 --> 02:05:14.640]   Everything is brilliant.
[02:05:14.640 --> 02:05:16.520]   And Kendrick is in it.
[02:05:16.520 --> 02:05:17.080]   Yeah.
[02:05:17.080 --> 02:05:18.640]   Is she the Stowaway?
[02:05:18.640 --> 02:05:19.080]   No.
[02:05:19.080 --> 02:05:19.520]   No.
[02:05:19.520 --> 02:05:20.960]   She's the super astronaut.
[02:05:20.960 --> 02:05:21.480]   She's like--
[02:05:21.480 --> 02:05:22.320]   Of course she is.
[02:05:22.320 --> 02:05:23.160]   Yeah.
[02:05:23.160 --> 02:05:23.680]   Yeah.
[02:05:23.680 --> 02:05:24.160]   But--
[02:05:24.160 --> 02:05:25.280]   Because I like to have to wear so much,
[02:05:25.280 --> 02:05:26.920]   because he actually gets the science right.
[02:05:26.920 --> 02:05:29.680]   If this gets that science right as well, I'm watching it.
[02:05:29.680 --> 02:05:30.600]   I mean, OK.
[02:05:30.600 --> 02:05:33.200]   So Andy has sold his new movie.
[02:05:33.200 --> 02:05:35.240]   The Martian was Matt Damon.
[02:05:35.240 --> 02:05:35.760]   Yeah.
[02:05:35.760 --> 02:05:36.760]   Right?
[02:05:36.760 --> 02:05:37.760]   As Mark--
[02:05:37.760 --> 02:05:38.760]   Oh, as you sold us once.
[02:05:38.760 --> 02:05:39.760]   --Watney.
[02:05:39.760 --> 02:05:41.680]   His Artemis is his second book.
[02:05:41.680 --> 02:05:43.760]   He hasn't sold that, but hasn't been made yet.
[02:05:43.760 --> 02:05:47.840]   The new book, Project Hail Mary, which is fantastic, sold fox.
[02:05:47.840 --> 02:05:50.280]   It's going to be done by the director's Lord and Miller, who
[02:05:50.280 --> 02:05:51.760]   did Spider-Man--
[02:05:51.760 --> 02:05:53.640]   Right.
[02:05:53.640 --> 02:05:56.400]   --into the multiverse, which was very good.
[02:05:56.400 --> 02:05:59.280]   And the Lego movie is not the first live action movie.
[02:05:59.280 --> 02:06:00.280]   Everything is sold.
[02:06:00.280 --> 02:06:01.760]   I'm told.
[02:06:01.760 --> 02:06:05.920]   But they've already signed Ryan Gosling to pay the lead.
[02:06:05.920 --> 02:06:08.680]   I hope he's better than Blade Runner then.
[02:06:08.680 --> 02:06:10.600]   Well, and Andy was funny, because I
[02:06:10.600 --> 02:06:12.440]   think he was responding to that when he said,
[02:06:12.440 --> 02:06:14.200]   you know, he really didn't have to act in Blade Runner.
[02:06:14.200 --> 02:06:16.840]   This is a role he'll have to act in.
[02:06:16.840 --> 02:06:17.400]   Yeah.
[02:06:17.400 --> 02:06:18.920]   You know, he's very good in some things.
[02:06:18.920 --> 02:06:22.000]   But I went to watch Blade Runner at the end of it.
[02:06:22.000 --> 02:06:26.080]   I was just kind of like, ah, you know, it's two and a half
[02:06:26.080 --> 02:06:26.080]   now.
[02:06:26.080 --> 02:06:27.560]   I like that first scene, though.
[02:06:27.560 --> 02:06:29.080]   I thought that first scene was pretty good.
[02:06:29.080 --> 02:06:31.000]   It was some marvelous scenes in their way.
[02:06:31.000 --> 02:06:32.440]   No, it's--
[02:06:32.440 --> 02:06:33.800]   Wasn't as good as the original.
[02:06:33.800 --> 02:06:34.240]   No.
[02:06:34.240 --> 02:06:34.680]   No.
[02:06:34.680 --> 02:06:36.720]   Nothing could be.
[02:06:36.720 --> 02:06:38.000]   Although which original?
[02:06:38.000 --> 02:06:39.840]   Was it the original, the rebooted original,
[02:06:39.840 --> 02:06:41.000]   the remastered original?
[02:06:41.000 --> 02:06:44.200]   With three extra minutes in from the director original.
[02:06:44.200 --> 02:06:45.200]   Yeah.
[02:06:45.200 --> 02:06:47.560]   Yeah, there were a few.
[02:06:47.560 --> 02:06:50.040]   Let's see.
[02:06:50.040 --> 02:06:54.120]   Just looking-- we are now in what I like to call the seeds
[02:06:54.120 --> 02:06:54.800]   in the stems.
[02:06:54.800 --> 02:06:57.480]   You call it the quick, quick hits.
[02:06:57.480 --> 02:06:58.920]   I don't know what you're talking about,
[02:06:58.920 --> 02:07:01.200]   but speaking of someone who's going for citizenship,
[02:07:01.200 --> 02:07:02.280]   I have no idea.
[02:07:02.280 --> 02:07:02.640]   Oh, really?
[02:07:02.640 --> 02:07:04.040]   When's that going to be?
[02:07:04.040 --> 02:07:06.320]   Well, soon the embassy reopens.
[02:07:06.320 --> 02:07:08.640]   You are US embassy in the UK is still closed.
[02:07:08.640 --> 02:07:10.600]   I'm--
[02:07:10.600 --> 02:07:11.760]   What do you have to do?
[02:07:11.760 --> 02:07:13.800]   Do you have to memorize the names of the presidents?
[02:07:13.800 --> 02:07:16.160]   Well, I've got to learn more about the US government
[02:07:16.160 --> 02:07:18.880]   than it seems most Americans citizens.
[02:07:18.880 --> 02:07:19.680]   I don't know--
[02:07:19.680 --> 02:07:22.160]   Can I remember-- recommend Schoolhouse Rock?
[02:07:22.160 --> 02:07:23.840]   I've watched many of those videos.
[02:07:23.840 --> 02:07:24.880]   That's how we know.
[02:07:24.880 --> 02:07:26.560]   If you ask most people, how many members
[02:07:26.560 --> 02:07:28.880]   of the House of Representatives there are?
[02:07:28.880 --> 02:07:29.880]   I think you get one.
[02:07:29.880 --> 02:07:30.720]   Are you supposed to know that?
[02:07:30.720 --> 02:07:32.080]   Is it 358?
[02:07:32.080 --> 02:07:33.800]   No, no, it's 400.
[02:07:33.800 --> 02:07:35.480]   400--
[02:07:35.480 --> 02:07:36.520]   I think it's full-thirty--
[02:07:36.520 --> 02:07:37.400]   I'm already a citizen.
[02:07:37.400 --> 02:07:39.720]   I don't have to know that.
[02:07:39.720 --> 02:07:40.920]   There's no more more than that.
[02:07:40.920 --> 02:07:42.760]   There's 100 senators I know that.
[02:07:42.760 --> 02:07:47.160]   And it's 538 for the Electoral College, so it'd be 438.
[02:07:47.160 --> 02:07:48.680]   Yeah.
[02:07:48.680 --> 02:07:51.160]   It's stuff like that.
[02:07:51.160 --> 02:07:53.040]   Although I've always loved The Simpsons.
[02:07:53.040 --> 02:07:54.040]   438.
[02:07:54.040 --> 02:07:55.080]   episode where APU becomes a citizen.
[02:07:55.080 --> 02:07:57.400]   It's just like, what was the cause of the Civil War?
[02:07:57.400 --> 02:07:58.980]   Actually, there were many multitudinous
[02:07:58.980 --> 02:08:01.440]   the conflict-- obviously, leaving slavery aside.
[02:08:01.440 --> 02:08:03.320]   There was the conflict between the industrialized--
[02:08:03.320 --> 02:08:04.120]   just say slavery.
[02:08:04.120 --> 02:08:06.680]   OK, fine.
[02:08:06.680 --> 02:08:08.520]   Short answer is please.
[02:08:08.520 --> 02:08:09.920]   Well, good luck.
[02:08:09.920 --> 02:08:11.080]   I hope you are.
[02:08:11.080 --> 02:08:12.320]   I mean, the only downside is that you're--
[02:08:12.320 --> 02:08:15.200]   I'll come to your swearing-in.
[02:08:15.200 --> 02:08:18.200]   If I do it and I get the swearing-in,
[02:08:18.200 --> 02:08:21.200]   I'm going the full Arnold Schwarzenegger, you know,
[02:08:21.200 --> 02:08:24.280]   top half of the red, white and blue, the whole thing,
[02:08:24.280 --> 02:08:26.320]   way with the flag.
[02:08:26.320 --> 02:08:28.920]   I mean, I feel my mother would disnair at me, but still.
[02:08:28.920 --> 02:08:31.840]   [LAUGHTER]
[02:08:31.840 --> 02:08:33.840]   Are you verified on Twitter?
[02:08:33.840 --> 02:08:35.160]   Twitter verified, anybody?
[02:08:35.160 --> 02:08:36.200]   Yes, Ian is.
[02:08:36.200 --> 02:08:36.720]   I am.
[02:08:36.720 --> 02:08:37.640]   Mike is.
[02:08:37.640 --> 02:08:38.480]   Jason, no.
[02:08:38.480 --> 02:08:39.440]   I put in for it.
[02:08:39.440 --> 02:08:40.400]   Jason, I am.
[02:08:40.400 --> 02:08:41.880]   So you're the only one on this panel,
[02:08:41.880 --> 02:08:43.920]   so you're going to have to leave now.
[02:08:43.920 --> 02:08:44.400]   OK.
[02:08:44.400 --> 02:08:48.040]   Twitter is now letting anyone apply for verification.
[02:08:48.040 --> 02:08:49.440]   This is the first time since 2017.
[02:08:49.440 --> 02:08:51.280]   It's kind of cheap, but it knows.
[02:08:51.280 --> 02:08:52.800]   Now I don't want the blue check.
[02:08:52.800 --> 02:08:54.600]   The blue thing is cheap, anyway.
[02:08:54.600 --> 02:08:55.160]   Yeah.
[02:08:55.160 --> 02:08:57.040]   I still don't sound like the game you want to be honest,
[02:08:57.040 --> 02:08:59.000]   but you know, it's just--
[02:08:59.000 --> 02:09:01.080]   it has been weird.
[02:09:01.080 --> 02:09:02.720]   To make the great for verification,
[02:09:02.720 --> 02:09:05.040]   you will have to meet their new criteria.
[02:09:05.040 --> 02:09:05.840]   I have a whole list.
[02:09:05.840 --> 02:09:08.240]   You have to have an account of public interest
[02:09:08.240 --> 02:09:11.600]   that falls in one of these six categories as well as being--
[02:09:11.600 --> 02:09:16.440]   and I think you are authentic, notable, and active.
[02:09:16.440 --> 02:09:17.240]   That's Jason.
[02:09:17.240 --> 02:09:18.840]   That's like my love life.
[02:09:18.840 --> 02:09:20.360]   You're the--
[02:09:20.360 --> 02:09:24.440]   You're the six categories of accounts that can qualify.
[02:09:24.440 --> 02:09:25.680]   Government.
[02:09:25.680 --> 02:09:26.240]   No.
[02:09:26.240 --> 02:09:28.480]   Companies, brands, and organizations.
[02:09:28.480 --> 02:09:31.560]   News organizations and journalists.
[02:09:31.560 --> 02:09:32.440]   Entertainment.
[02:09:32.440 --> 02:09:33.040]   Nope.
[02:09:33.040 --> 02:09:35.320]   Sports and gaming activists, organizers,
[02:09:35.320 --> 02:09:37.440]   and other influential individuals
[02:09:37.440 --> 02:09:39.240]   will be in your account settings tab.
[02:09:39.240 --> 02:09:39.720]   Yeah.
[02:09:39.720 --> 02:09:42.440]   Entertainment is a very wide category.
[02:09:42.440 --> 02:09:45.280]   I mean, you do get the feeling that influences--
[02:09:45.280 --> 02:09:46.600]   But they have different--
[02:09:46.600 --> 02:09:49.120]   Yeah, they have different subcategories
[02:09:49.120 --> 02:09:51.360]   within those as far as requirements are concerned.
[02:09:51.360 --> 02:09:53.800]   Because they have one for a useless Instagram nerd.
[02:09:53.800 --> 02:09:54.880]   That's a good question.
[02:09:54.880 --> 02:09:55.760]   Influencer.
[02:09:55.760 --> 02:09:56.920]   Are you an influencer?
[02:09:56.920 --> 02:09:57.360]   Right.
[02:09:57.360 --> 02:09:58.200]   Yeah.
[02:09:58.200 --> 02:09:59.840]   Prove that you've influenced people.
[02:09:59.840 --> 02:10:00.880]   Exactly.
[02:10:00.880 --> 02:10:04.320]   For the journalist-- so obviously, I put mine in through journalists.
[02:10:04.320 --> 02:10:07.720]   And they do have a section of it that includes podcasting.
[02:10:07.720 --> 02:10:08.200]   Oh, good.
[02:10:08.200 --> 02:10:10.880]   But in order to do it, you have to prove
[02:10:10.880 --> 02:10:14.560]   that you have a Wikipedia page.
[02:10:14.560 --> 02:10:15.080]   Really?
[02:10:15.080 --> 02:10:20.360]   So if you have a Wikipedia page, that's one type of proof.
[02:10:20.360 --> 02:10:25.200]   Or that you're linked to from three different sources or whatever.
[02:10:25.200 --> 02:10:27.320]   So they have different levels.
[02:10:27.320 --> 02:10:28.880]   I could say most of the journalists I know
[02:10:28.880 --> 02:10:31.960]   who have a Wikipedia page wrote to themselves.
[02:10:31.960 --> 02:10:32.520]   Not me.
[02:10:32.520 --> 02:10:33.360]   I mean--
[02:10:33.360 --> 02:10:33.680]   No, no.
[02:10:33.680 --> 02:10:35.440]   I mean, you're saying that you've been a rapper
[02:10:35.440 --> 02:10:36.600]   with a Wikipedia page.
[02:10:36.600 --> 02:10:37.600]   Yeah.
[02:10:37.600 --> 02:10:38.360]   No, no.
[02:10:38.360 --> 02:10:40.560]   I mean, I do know a couple of journalists who literally
[02:10:40.560 --> 02:10:42.360]   wrote their own Wikipedia pages.
[02:10:42.360 --> 02:10:42.880]   Yeah.
[02:10:42.880 --> 02:10:44.560]   And it was just like, wow.
[02:10:44.560 --> 02:10:48.320]   That's kind of ethically dodgy.
[02:10:48.320 --> 02:10:52.280]   But it's going to be interesting to see how it plays out.
[02:10:52.280 --> 02:10:54.560]   You just have a blue take at the very least.
[02:10:54.560 --> 02:10:55.040]   But--
[02:10:55.040 --> 02:10:56.200]   Well, thank you for saying that.
[02:10:56.200 --> 02:10:57.440]   We'll see if Twitter agrees.
[02:10:57.440 --> 02:11:02.320]   I ended up actually applying more for my writing at--
[02:11:02.320 --> 02:11:06.520]   the bugger than I did for Twit, primarily because the requirements
[02:11:06.520 --> 02:11:07.960]   that they had for the podcast--
[02:11:07.960 --> 02:11:09.480]   Yeah, they want-- I didn't satisfy.
[02:11:09.480 --> 02:11:12.800]   But with writing, I could provide three articles, which
[02:11:12.800 --> 02:11:13.800]   is what it required.
[02:11:13.800 --> 02:11:16.280]   Because you know what-- that's why the book costume requirements.
[02:11:16.280 --> 02:11:18.840]   Because I don't have a Wikipedia page.
[02:11:18.840 --> 02:11:23.320]   And I don't have three separate sources linking to me.
[02:11:23.320 --> 02:11:24.800]   Yeah, cool.
[02:11:24.800 --> 02:11:26.280]   Like reputable sources.
[02:11:26.280 --> 02:11:28.640]   So I don't know if that's like three articles written about you
[02:11:28.640 --> 02:11:29.520]   or what it is.
[02:11:29.520 --> 02:11:32.040]   But they have a certain level of requirement there.
[02:11:32.040 --> 02:11:33.040]   What do you care for?
[02:11:33.040 --> 02:11:35.480]   You have a blue check or not on Twitter.
[02:11:35.480 --> 02:11:37.960]   You know, I don't really care that much.
[02:11:37.960 --> 02:11:39.280]   But they keep me out.
[02:11:39.280 --> 02:11:40.680]   You're the only one that does it.
[02:11:40.680 --> 02:11:41.200]   So yeah.
[02:11:41.200 --> 02:11:42.360]   So I filled it out to see.
[02:11:42.360 --> 02:11:43.800]   You want to be one of the cool kids.
[02:11:43.800 --> 02:11:45.360]   When I applied years and years ago--
[02:11:45.360 --> 02:11:50.280]   My main argument was that I linked to 25 people who
[02:11:50.280 --> 02:11:52.280]   were stealing my identity, my picture, my name.
[02:11:52.280 --> 02:11:53.280]   That's a good argument.
[02:11:53.280 --> 02:11:53.760]   Yeah, that's right.
[02:11:53.760 --> 02:11:54.480]   And they gave it to me.
[02:11:54.480 --> 02:11:55.280]   But that was--
[02:11:55.280 --> 02:11:56.760]   No, I don't think I ever applied.
[02:11:56.760 --> 02:11:58.360]   I think it just happened.
[02:11:58.360 --> 02:11:59.240]   It just happened.
[02:11:59.240 --> 02:12:00.760]   I've been there so long.
[02:12:00.760 --> 02:12:02.680]   And it's funny because I wouldn't qualify.
[02:12:02.680 --> 02:12:04.160]   I'm not active at all.
[02:12:04.160 --> 02:12:05.400]   I've been treated in months.
[02:12:05.400 --> 02:12:07.200]   You qualify because you have a Wikipedia page.
[02:12:07.200 --> 02:12:07.680]   I do have one.
[02:12:07.680 --> 02:12:08.680]   You do have one.
[02:12:08.680 --> 02:12:09.560]   You know what I'm saying?
[02:12:09.560 --> 02:12:11.840]   Personally, I think that the way that auto work
[02:12:11.840 --> 02:12:15.720]   is that they should have a real names policy where
[02:12:15.720 --> 02:12:19.120]   if you demonstrate to Twitter who you really are,
[02:12:19.120 --> 02:12:22.760]   voluntarily, and then you can use any name you want for the public
[02:12:22.760 --> 02:12:24.120]   as long as they know that you are.
[02:12:24.120 --> 02:12:25.360]   That's actually a good idea.
[02:12:25.360 --> 02:12:29.760]   And it should be kind of a badge of authenticity
[02:12:29.760 --> 02:12:31.720]   that you're not like a troll or something like that.
[02:12:31.720 --> 02:12:33.520]   And then you could choose to say, I only want to see--
[02:12:33.520 --> 02:12:34.480]   But you can still be anonymous.
[02:12:34.480 --> 02:12:35.000]   Yeah.
[02:12:35.000 --> 02:12:37.360]   And I only want to see stuff from verified users.
[02:12:37.360 --> 02:12:38.680]   That would make Twitter amazing.
[02:12:38.680 --> 02:12:39.200]   Yes.
[02:12:39.200 --> 02:12:39.880]   Yeah.
[02:12:39.880 --> 02:12:41.160]   I've been still watching it for years.
[02:12:41.160 --> 02:12:44.800]   The ability to change your name is also very handy because--
[02:12:44.800 --> 02:12:48.960]   if mine did a tweet which went viral,
[02:12:48.960 --> 02:12:52.320]   the Sun newspaper, which is one of Britain's biggest tabloids,
[02:12:52.320 --> 02:12:55.400]   and a news international publication,
[02:12:55.400 --> 02:12:57.280]   contacted him saying, can we use the tweet?
[02:12:57.280 --> 02:12:58.440]   And he said, no.
[02:12:58.440 --> 02:13:00.400]   So they used it anyway.
[02:13:00.400 --> 02:13:04.480]   And he was able to change his name to Rupert Murdoch as a CU--
[02:13:04.480 --> 02:13:05.400]   Or you can do that.
[02:13:05.400 --> 02:13:05.760]   Yeah.
[02:13:05.760 --> 02:13:06.480]   You can do that.
[02:13:06.480 --> 02:13:08.360]   And that got taken down almost instantly.
[02:13:08.360 --> 02:13:08.840]   Yeah.
[02:13:08.840 --> 02:13:10.160]   You probably should see that.
[02:13:10.160 --> 02:13:10.960]   That's important.
[02:13:10.960 --> 02:13:12.440]   So you know.
[02:13:12.440 --> 02:13:15.760]   I think you can say, I only want to follow verified--
[02:13:15.760 --> 02:13:16.600]   I think you can.
[02:13:16.600 --> 02:13:17.400]   But the problem is that--
[02:13:17.400 --> 02:13:18.080]   The verified's meaningless.
[02:13:18.080 --> 02:13:20.080]   Right now, they're not regular people.
[02:13:20.080 --> 02:13:21.560]   They're just celebrities.
[02:13:21.560 --> 02:13:22.080]   Yeah.
[02:13:22.080 --> 02:13:22.640]   And Kim Kardashian.
[02:13:22.640 --> 02:13:24.040]   People with Wikipedia pages.
[02:13:24.040 --> 02:13:26.080]   Actually, I want to give a plug, Mike, to your son
[02:13:26.080 --> 02:13:31.760]   before we get to our next and final ad and wrap the show.
[02:13:31.760 --> 02:13:35.080]   Kevin Elgin created Chatterbox, which
[02:13:35.080 --> 02:13:38.920]   is, as you were talking about, a smart speaker that
[02:13:38.920 --> 02:13:40.000]   is private.
[02:13:40.000 --> 02:13:43.920]   But it helps kids understand exactly how smart speakers work.
[02:13:43.920 --> 02:13:45.320]   They actually build it themselves.
[02:13:45.320 --> 02:13:48.160]   They've got a curriculum for schools.
[02:13:48.160 --> 02:13:50.360]   It's appropriate for a K through 12.
[02:13:50.360 --> 02:13:53.880]   But even younger kids, honestly,
[02:13:53.880 --> 02:13:55.840]   should probably get some experience
[02:13:55.840 --> 02:13:59.120]   with how these devices are made and understanding
[02:13:59.120 --> 02:14:00.800]   what these devices do.
[02:14:00.800 --> 02:14:05.280]   Copa compliant, no data collection, very affordable.
[02:14:05.280 --> 02:14:07.640]   It's at hellochatterbox.com.
[02:14:07.640 --> 02:14:09.680]   And I really wanted to mention it,
[02:14:09.680 --> 02:14:14.000]   because Kevin's doing such a great thing with his startup.
[02:14:14.000 --> 02:14:17.040]   And I always like to give you a little bit of a plug.
[02:14:17.040 --> 02:14:21.240]   Meanwhile, speaking of plugs, let me talk about my mattress.
[02:14:21.240 --> 02:14:22.920]   I had a good night's sleep last night.
[02:14:22.920 --> 02:14:25.200]   Thank you, Casper.
[02:14:25.200 --> 02:14:28.600]   Casper mattresses help you sleep cool,
[02:14:28.600 --> 02:14:31.040]   because you shouldn't be worried about tomorrow.
[02:14:31.040 --> 02:14:33.320]   You should only worry about tonight.
[02:14:33.320 --> 02:14:36.560]   Now, Casper has a new cooling collection.
[02:14:36.560 --> 02:14:37.920]   I think this is such a good idea,
[02:14:37.920 --> 02:14:41.400]   because we know that good night's sleep
[02:14:41.400 --> 02:14:43.560]   relies on temperature regulation.
[02:14:43.560 --> 02:14:45.160]   And it's cooler than you think.
[02:14:45.160 --> 02:14:48.560]   And anybody who's slept hot knows it's no good.
[02:14:48.560 --> 02:14:50.640]   That's not comfortable.
[02:14:50.640 --> 02:14:54.440]   Casper has a cooling collection with everything you need
[02:14:54.440 --> 02:14:55.880]   to help you sleep cool all night
[02:14:55.880 --> 02:14:57.600]   with long summer nights coming.
[02:14:57.600 --> 02:14:58.440]   You're going to need this.
[02:14:58.440 --> 02:15:03.440]   Casper's mattress has a new thing called Snow Technology.
[02:15:03.440 --> 02:15:06.360]   They've got hyper light sheets.
[02:15:06.360 --> 02:15:08.440]   They even have a mattress protector now designed
[02:15:08.440 --> 02:15:09.920]   to keep you cool and comfortable.
[02:15:09.920 --> 02:15:10.920]   Tomorrow's a new day.
[02:15:10.920 --> 02:15:13.640]   Make the most of it with Casper's new cooling collection.
[02:15:13.640 --> 02:15:16.160]   Their Wave Hybrid Snow mattress
[02:15:16.160 --> 02:15:19.080]   actually keeps you cool for more than 12 hours.
[02:15:19.080 --> 02:15:19.920]   You know what?
[02:15:19.920 --> 02:15:21.720]   You might even want to stay in bed for 12 hours,
[02:15:21.720 --> 02:15:23.480]   pulling heat away from your body
[02:15:23.480 --> 02:15:25.680]   for sustained temperature regulation,
[02:15:25.680 --> 02:15:27.320]   a cool to the touch feeling,
[02:15:27.320 --> 02:15:29.920]   and of course, a much improved tomorrow.
[02:15:29.920 --> 02:15:31.680]   Bed or bedding makes for a better tomorrow.
[02:15:31.680 --> 02:15:34.040]   That's why Casper's hyper light sheets are designed
[02:15:34.040 --> 02:15:35.600]   with an innovative grid weave
[02:15:35.600 --> 02:15:39.000]   that lets air flow through for maximum breathability.
[02:15:39.000 --> 02:15:41.680]   The lightweight duvet provides optimal temperature control
[02:15:41.680 --> 02:15:43.520]   without sacrificing plush comfort
[02:15:43.520 --> 02:15:46.360]   and you'll love the breathable mattress protector,
[02:15:46.360 --> 02:15:48.800]   which approves the coolness of the bed even further
[02:15:48.800 --> 02:15:51.920]   by allowing the air to flow between you, your body,
[02:15:51.920 --> 02:15:52.760]   and your mattress.
[02:15:52.760 --> 02:15:57.280]   Even Sammy the Cat loves our Casper.
[02:15:57.280 --> 02:15:59.880]   All of the Casper products designed to work together
[02:15:59.880 --> 02:16:01.760]   to prevent overheating all night long
[02:16:01.760 --> 02:16:04.520]   because cooler sleep means better sleep
[02:16:04.520 --> 02:16:06.880]   and better sleep means better tomorrow.
[02:16:06.880 --> 02:16:11.200]   And as always with Casper free shipping, free returns.
[02:16:11.200 --> 02:16:14.240]   Everybody in my family now has Casper's,
[02:16:14.240 --> 02:16:15.080]   including my daughter,
[02:16:15.080 --> 02:16:16.680]   I just got her, she got a new apartment.
[02:16:16.680 --> 02:16:19.160]   I got her as a housewarming present.
[02:16:19.160 --> 02:16:20.800]   It's very affordable, don't tell her.
[02:16:20.800 --> 02:16:21.640]   It's very affordable.
[02:16:21.640 --> 02:16:24.200]   I got her the frame, the foundation,
[02:16:24.200 --> 02:16:25.920]   the mattress, I even got the Casper sheets,
[02:16:25.920 --> 02:16:27.960]   the Casper pillow, she's very happy.
[02:16:27.960 --> 02:16:29.360]   When it comes to a better night's sleep,
[02:16:29.360 --> 02:16:31.720]   Casper's new cooling collection,
[02:16:31.720 --> 02:16:34.600]   as you covered, focus on tomorrow,
[02:16:34.600 --> 02:16:36.520]   like Casper handle the rest.
[02:16:36.520 --> 02:16:38.920]   Explore Casper products, mattresses, sheets,
[02:16:38.920 --> 02:16:41.600]   pillows and more at Casper.com/twit1.
[02:16:41.600 --> 02:16:42.640]   Don't forget the offer code is
[02:16:42.640 --> 02:16:45.360]   Twit1 for $100 off select mattresses.
[02:16:45.360 --> 02:16:50.280]   Twit1, T-W-I-T1, $100 off select mattresses,
[02:16:50.280 --> 02:16:55.080]   exclusions apply, see Casper.com for more details.
[02:16:55.080 --> 02:16:57.160]   Interesting article from you,
[02:16:57.160 --> 02:16:59.040]   and this is not part of the seeds and the stems.
[02:16:59.040 --> 02:17:00.560]   I don't want you to feel bad.
[02:17:01.520 --> 02:17:03.960]   It comes like I was smoking something with this topic.
[02:17:03.960 --> 02:17:05.160]   It's actually fascinating.
[02:17:05.160 --> 02:17:07.320]   You talk about William Shatner,
[02:17:07.320 --> 02:17:10.360]   90 years old, going into a Los Angeles studio,
[02:17:10.360 --> 02:17:13.240]   a company called Storyfile,
[02:17:13.240 --> 02:17:16.840]   where they capture him with 3D cameras,
[02:17:16.840 --> 02:17:18.840]   and then it says,
[02:17:18.840 --> 02:17:22.320]   Shatner's ghost will be beamed to his family members,
[02:17:22.320 --> 02:17:24.240]   to fans via the internet,
[02:17:24.240 --> 02:17:27.360]   possibly to museums and entertainment venues,
[02:17:27.360 --> 02:17:31.200]   where people will be able to ask Shatner's ghost questions
[02:17:31.200 --> 02:17:33.400]   they will play the answers.
[02:17:33.400 --> 02:17:35.080]   So it's pre-recorded answers,
[02:17:35.080 --> 02:17:36.400]   or they're generating new answers.
[02:17:36.400 --> 02:17:37.240]   They're pre-recorded,
[02:17:37.240 --> 02:17:40.480]   they use AI to match the answers with the questions,
[02:17:40.480 --> 02:17:43.080]   so you'll be able to have a conversation with Shatner.
[02:17:43.080 --> 02:17:43.920]   Right?
[02:17:43.920 --> 02:17:46.720]   But the purpose of the story is not the Shatner
[02:17:46.720 --> 02:17:49.040]   and the Storyfile product.
[02:17:49.040 --> 02:17:50.680]   They're also democratizing it,
[02:17:50.680 --> 02:17:52.400]   making it available on phones and everything.
[02:17:52.400 --> 02:17:53.360]   Anybody can do this.
[02:17:53.360 --> 02:17:54.680]   You could have it on your gravestone.
[02:17:54.680 --> 02:17:56.000]   I could call it a mic.
[02:17:56.000 --> 02:17:56.840]   Exactly.
[02:17:56.840 --> 02:17:57.960]   Where's the Mescal?
[02:17:57.960 --> 02:17:59.120]   Where are you hiding the Mescal?
[02:17:59.120 --> 02:18:00.320]   Exactly.
[02:18:00.320 --> 02:18:05.320]   But what are you saying that he would really do this really well?
[02:18:05.320 --> 02:18:06.160]   He's done it.
[02:18:06.160 --> 02:18:07.000]   He's done it.
[02:18:07.000 --> 02:18:07.840]   Shatner comma.
[02:18:07.840 --> 02:18:08.680]   Yeah.
[02:18:08.680 --> 02:18:14.120]   But my point is I talk about a whole bunch
[02:18:14.120 --> 02:18:15.320]   of different technologies,
[02:18:15.320 --> 02:18:19.920]   because I believe that people will express their,
[02:18:19.920 --> 02:18:21.160]   sort of, day of the dead,
[02:18:21.160 --> 02:18:25.320]   like interacting with ancestors through technology.
[02:18:25.320 --> 02:18:26.160]   Through holograms.
[02:18:26.160 --> 02:18:27.000]   AI, holograms.
[02:18:27.000 --> 02:18:27.920]   It's already happening, in fact.
[02:18:27.920 --> 02:18:28.760]   There's already Japanese--
[02:18:28.760 --> 02:18:31.200]   Remember Tupac at Coachella, right?
[02:18:31.200 --> 02:18:35.280]   And I mentioned him, there's a dozen and a half celebrities
[02:18:35.280 --> 02:18:36.720]   that have been turned into holograms.
[02:18:36.720 --> 02:18:38.240]   Those aren't holograms, actually.
[02:18:38.240 --> 02:18:39.760]   You have to be in one place,
[02:18:39.760 --> 02:18:43.760]   looking directly at a 2D thing that simulates 3D.
[02:18:43.760 --> 02:18:45.280]   But there's that.
[02:18:45.280 --> 02:18:49.200]   There's the Deepnastalgia app, is another example.
[02:18:49.200 --> 02:18:49.560]   Yeah, right.
[02:18:49.560 --> 02:18:51.040]   There's a lot of--
[02:18:51.040 --> 02:18:53.680]   And so one of the things that I think is interesting
[02:18:53.680 --> 02:18:57.560]   about the-- let's go back to the Kardashian hologram.
[02:18:57.560 --> 02:19:01.480]   They felt the need to, by they, I mean Kanye West,
[02:19:01.480 --> 02:19:05.040]   felt the need to have Kim Kardashian's father, Robert
[02:19:05.040 --> 02:19:07.480]   Kardashian, say, I'm watching over you.
[02:19:07.480 --> 02:19:09.280]   I see you.
[02:19:09.280 --> 02:19:12.160]   And this is also what happens with this Japanese hologram.
[02:19:12.160 --> 02:19:17.240]   It's a company that gives you an AR representation
[02:19:17.240 --> 02:19:19.920]   of your deceased loved ones at their grave site.
[02:19:19.920 --> 02:19:21.720]   And they say, oh, we're watching over you.
[02:19:21.720 --> 02:19:22.320]   We're doing that.
[02:19:22.320 --> 02:19:25.040]   Those are the kinds of things a ghost would say, right?
[02:19:25.040 --> 02:19:27.600]   So we're going to be creating these digital ghosts.
[02:19:27.600 --> 02:19:28.920]   And so the point of this article is
[02:19:28.920 --> 02:19:30.440]   to point out that this is coming.
[02:19:30.440 --> 02:19:34.920]   We're going to have gravesite AR.
[02:19:34.920 --> 02:19:36.440]   We're going to have roadside where somebody
[02:19:36.440 --> 02:19:37.400]   got into an accident.
[02:19:37.400 --> 02:19:38.520]   Those little things that they do with--
[02:19:38.520 --> 02:19:39.520]   Yeah, yeah, yeah.
[02:19:39.520 --> 02:19:41.480]   They're going to be AR there.
[02:19:41.480 --> 02:19:43.400]   There's going to be AR of dead people everywhere.
[02:19:43.400 --> 02:19:45.080]   Ghosts, basically.
[02:19:45.080 --> 02:19:49.320]   Semi-transparent apparitions that represent the deceased.
[02:19:49.320 --> 02:19:53.440]   And I fear that just like the spiritualist movement
[02:19:53.440 --> 02:19:56.000]   of 100 years ago, which I detailed in the article,
[02:19:56.000 --> 02:19:58.160]   there's going to be a lot of fraud and hoaxes
[02:19:58.160 --> 02:20:00.160]   and people making money, basically
[02:20:00.160 --> 02:20:04.080]   tricking people who are unfamiliar with the technology
[02:20:04.080 --> 02:20:06.560]   into believing that they are, in fact, conjuring the dead.
[02:20:06.560 --> 02:20:09.840]   Conndoiled, the author Sherlock Holmes very famously believed.
[02:20:09.840 --> 02:20:12.600]   He believed his wife was a medium.
[02:20:12.600 --> 02:20:16.480]   And Harry Houdini went to Great Pains to debunk all of this.
[02:20:16.480 --> 02:20:17.480]   That's right.
[02:20:17.480 --> 02:20:19.720]   So it was a big craze that always happens during times
[02:20:19.720 --> 02:20:20.440]   of great trauma.
[02:20:20.440 --> 02:20:24.240]   And now we're having the technology of deep fakes,
[02:20:24.240 --> 02:20:27.480]   augmented reality, and AI, where we--
[02:20:27.480 --> 02:20:30.520]   it'll be a banality 20 years from now, 10 years from now,
[02:20:30.520 --> 02:20:33.320]   to have conversations with dead people like William Shatner,
[02:20:33.320 --> 02:20:34.240]   Tupac, et cetera.
[02:20:34.240 --> 02:20:36.320]   This is the Japanese company spot message
[02:20:36.320 --> 02:20:38.800]   that you refer to here in your article.
[02:20:38.800 --> 02:20:39.840]   It's AR.
[02:20:39.840 --> 02:20:41.520]   You'll hold up your phone and you'll
[02:20:41.520 --> 02:20:45.800]   see your past parents at their grave site speak to you.
[02:20:45.800 --> 02:20:49.640]   Imagine with Apple's coming AR glasses--
[02:20:49.640 --> 02:20:50.320]   Or snaps.
[02:20:50.320 --> 02:20:52.560]   --or snaps, right?
[02:20:52.560 --> 02:20:54.280]   As you come closer to the grave, you'll
[02:20:54.280 --> 02:20:55.840]   see them standing there from a distance.
[02:20:55.840 --> 02:20:56.520]   And you can walk around.
[02:20:56.520 --> 02:20:57.520]   Walk around, too.
[02:20:57.520 --> 02:20:57.840]   Yes.
[02:20:57.840 --> 02:20:58.720]   We're going there.
[02:20:58.720 --> 02:21:02.200]   And there seems to be a psychological need for us
[02:21:02.200 --> 02:21:04.640]   to interact with our deceased loved ones.
[02:21:04.640 --> 02:21:07.200]   We talked about Oaxaca and Day of the Dead.
[02:21:07.200 --> 02:21:08.680]   Day of the Dead, that's what it's all about.
[02:21:08.680 --> 02:21:11.360]   You go and have a mezcal with your deceased loved
[02:21:11.360 --> 02:21:12.720]   graves at the grave site.
[02:21:12.720 --> 02:21:15.400]   And also in the a frienda in your house,
[02:21:15.400 --> 02:21:15.720]   it's an alternative.
[02:21:15.720 --> 02:21:16.760]   You make a shrine.
[02:21:16.760 --> 02:21:17.240]   Exactly.
[02:21:17.240 --> 02:21:19.320]   But they will pour a shot of mezcal
[02:21:19.320 --> 02:21:21.840]   and put it in front of the photo of the dead loved one.
[02:21:21.840 --> 02:21:25.400]   And then it mysteriously disappears.
[02:21:25.400 --> 02:21:26.000]   Obviously--
[02:21:26.000 --> 02:21:26.000]   --obvious--
[02:21:26.000 --> 02:21:26.760]   --stantas cookies.
[02:21:26.760 --> 02:21:26.800]   --lady--
[02:21:26.800 --> 02:21:26.800]   --lady--
[02:21:26.800 --> 02:21:26.800]   --addening--
[02:21:26.800 --> 02:21:26.800]   --lady--
[02:21:26.800 --> 02:21:27.800]   --addening--
[02:21:27.800 --> 02:21:29.720]   --supernatural forces at work.
[02:21:29.720 --> 02:21:30.220]   Absolutely.
[02:21:30.220 --> 02:21:30.720]   It did remind--
[02:21:30.720 --> 02:21:30.720]   Wow.
[02:21:30.720 --> 02:21:32.040]   --reading your article.
[02:21:32.040 --> 02:21:35.920]   It did remind me of a story from the old American science
[02:21:35.920 --> 02:21:40.080]   fiction writer, Roger Zilazny, where a grandson comes
[02:21:40.080 --> 02:21:44.320]   and negotiates with the AI of his dead grandfather.
[02:21:44.320 --> 02:21:47.720]   There's also a fabulous Black Mirror episode.
[02:21:47.720 --> 02:21:48.220]   Yes.
[02:21:48.220 --> 02:21:48.720]   Right.
[02:21:48.720 --> 02:21:51.120]   Where you bring back your deceased partner--
[02:21:51.120 --> 02:21:51.920]   --superman.
[02:21:51.920 --> 02:21:52.420]   Yeah.
[02:21:52.420 --> 02:21:57.480]   What was happening in the fortress of solitude, dead old dad,
[02:21:57.480 --> 02:21:59.720]   was giving advice, guiding--
[02:21:59.720 --> 02:22:00.220]   --gerate the way--
[02:22:00.220 --> 02:22:01.280]   --was visiting.
[02:22:01.280 --> 02:22:03.120]   We're all going to have Superman's dad.
[02:22:03.120 --> 02:22:07.720]   And so technology will enable this.
[02:22:07.720 --> 02:22:08.220]   This is all I'm saying.
[02:22:08.220 --> 02:22:10.320]   --saying for my kids who are stuck with me, I'm sorry to say.
[02:22:10.320 --> 02:22:12.520]   But that's one of those things.
[02:22:12.520 --> 02:22:13.020]   Wow.
[02:22:13.020 --> 02:22:14.220]   That's really an interesting--
[02:22:14.220 --> 02:22:14.720]   --that's the answer.
[02:22:14.720 --> 02:22:15.120]   --I love that.
[02:22:15.120 --> 02:22:16.520]   It's common for exactly right.
[02:22:16.520 --> 02:22:19.480]   There's some people that would be capable, emotionally
[02:22:19.480 --> 02:22:23.020]   capable of maneuvering that sort of experience.
[02:22:23.020 --> 02:22:25.620]   There's a lot of people, though, that I can't imagine.
[02:22:25.620 --> 02:22:29.520]   That would be good for their mental state at that point.
[02:22:29.520 --> 02:22:31.620]   They might think they want that.
[02:22:31.620 --> 02:22:33.020]   But then when faced with the point--
[02:22:33.020 --> 02:22:34.920]   That's the point of the Black Mirror episode.
[02:22:34.920 --> 02:22:35.620]   Absolutely.
[02:22:35.620 --> 02:22:37.220]   You think you want that till you get it.
[02:22:37.220 --> 02:22:38.020]   Until you get it.
[02:22:38.020 --> 02:22:39.920]   But I'm slightly very concept of having it
[02:22:39.920 --> 02:22:40.420]   itself.
[02:22:40.420 --> 02:22:43.420]   It's just like, oh, you get together with your family
[02:22:43.420 --> 02:22:44.820]   and everything's great.
[02:22:44.820 --> 02:22:46.120]   I'm being with my family.
[02:22:46.120 --> 02:22:47.820]   After a week, we start arguing.
[02:22:47.820 --> 02:22:48.820]   This doesn't sound like hell.
[02:22:48.820 --> 02:22:50.120]   Well, it would do that, too.
[02:22:50.120 --> 02:22:50.720]   It would be right.
[02:22:50.720 --> 02:22:51.220]   Yeah.
[02:22:51.220 --> 02:22:52.220]   You did that point.
[02:22:52.220 --> 02:22:54.520]   I find, I think, psychologically, you're exactly right.
[02:22:54.520 --> 02:22:58.320]   And it may be-- we're talking all the time about AR.
[02:22:58.320 --> 02:23:01.820]   And it's hard to come up with an actual application that--
[02:23:01.820 --> 02:23:03.120]   You had those Google glasses.
[02:23:03.120 --> 02:23:04.120]   And right.
[02:23:04.120 --> 02:23:06.720]   You know, is there-- what is going to drive this?
[02:23:06.720 --> 02:23:08.820]   That might actually be what drives it.
[02:23:08.820 --> 02:23:12.420]   And it may not be a positive thing at all.
[02:23:12.420 --> 02:23:13.820]   Don't you think Apple's probably already
[02:23:13.820 --> 02:23:16.720]   got something in the lab that might be--
[02:23:16.720 --> 02:23:17.820]   Life sold, yes.
[02:23:17.820 --> 02:23:19.320]   Just take it--
[02:23:19.320 --> 02:23:22.420]   You're going to have app developers for these platforms.
[02:23:22.420 --> 02:23:24.620]   And there are going to be people-- whatever demand there
[02:23:24.620 --> 02:23:27.220]   is to see ghosts of dead--
[02:23:27.220 --> 02:23:28.820]   there's already a company in Florida.
[02:23:28.820 --> 02:23:32.320]   This guy will let you record your own 3D hologram.
[02:23:32.320 --> 02:23:34.820]   And you can do your own eulogy.
[02:23:34.820 --> 02:23:35.820]   If you don't trust me--
[02:23:35.820 --> 02:23:35.820]   I wish--
[02:23:35.820 --> 02:23:36.320]   I wish--
[02:23:36.320 --> 02:23:36.820]   I wish--
[02:23:36.820 --> 02:23:37.820]   --we were such a great guy.
[02:23:37.820 --> 02:23:37.820]   Yeah.
[02:23:37.820 --> 02:23:38.320]   Boy.
[02:23:38.320 --> 02:23:39.320]   Everybody loved me.
[02:23:39.320 --> 02:23:40.820]   I really missed myself.
[02:23:40.820 --> 02:23:41.620]   I really--
[02:23:41.620 --> 02:23:43.220]   I wish I could be here with you right now.
[02:23:43.220 --> 02:23:44.220]   I wish I could be here with you.
[02:23:44.220 --> 02:23:45.220]   Who better to do this--
[02:23:45.220 --> 02:23:46.220]   Who better than me to talk about me?
[02:23:46.220 --> 02:23:48.220]   I'm kind of like the unofficial obituary writer
[02:23:48.220 --> 02:23:50.220]   for the rage, because I do an awful lot of them.
[02:23:50.220 --> 02:23:53.220]   But it's just like the temptation to do your own obituary
[02:23:53.220 --> 02:23:56.220]   is like forge from the earliest--
[02:23:56.220 --> 02:23:57.220]   Yes.
[02:23:57.220 --> 02:23:58.220]   --in the--
[02:23:58.220 --> 02:24:00.220]   --to the idmissible man, the end Thompson.
[02:24:00.220 --> 02:24:02.220]   What is Bitcoin down to?
[02:24:02.220 --> 02:24:03.220]   30,000?
[02:24:03.220 --> 02:24:04.220]   I have a question, Bitcoin.
[02:24:04.220 --> 02:24:05.220]   Let's find out.
[02:24:05.220 --> 02:24:06.220]   Let's see.
[02:24:06.220 --> 02:24:07.220]   We are--
[02:24:07.220 --> 02:24:08.220]   It went as low as--
[02:24:08.220 --> 02:24:09.220]   34,918.
[02:24:09.220 --> 02:24:10.220]   Yeah.
[02:24:10.220 --> 02:24:11.220]   That means my wallet has lost.
[02:24:11.220 --> 02:24:12.220]   So by the dip, is that what they say?
[02:24:12.220 --> 02:24:15.220]   They say, this is the problem people have been buying the dip
[02:24:15.220 --> 02:24:16.220]   for the past week.
[02:24:16.220 --> 02:24:17.220]   Yeah.
[02:24:17.220 --> 02:24:18.220]   It keeps dipping.
[02:24:18.220 --> 02:24:20.220]   This is why you should never take my financial advice,
[02:24:20.220 --> 02:24:22.220]   because when it hit 2000, I was kind of like,
[02:24:22.220 --> 02:24:24.220]   oh, for goodness sake, get your money out now.
[02:24:24.220 --> 02:24:25.220]   You'll be--
[02:24:25.220 --> 02:24:26.220]   It's--
[02:24:26.220 --> 02:24:27.220]   I gave the same--
[02:24:27.220 --> 02:24:28.220]   When someone else came to do a--
[02:24:28.220 --> 02:24:30.220]   Duane versus investing Google's IPO and was like,
[02:24:30.220 --> 02:24:34.220]   ow, there'll be another better search engine along in a few years.
[02:24:34.220 --> 02:24:36.220]   Don't bother getting into that stuff.
[02:24:36.220 --> 02:24:39.220]   So yeah, don't listen to my investment advice.
[02:24:39.220 --> 02:24:41.220]   But it's been up and down like the Assyrian Empire.
[02:24:41.220 --> 02:24:42.220]   It's just--
[02:24:42.220 --> 02:24:43.220]   It's so stable.
[02:24:43.220 --> 02:24:44.220]   Yeah, you watch people--
[02:24:44.220 --> 02:24:45.220]   It's a secure reference.
[02:24:45.220 --> 02:24:48.220]   Did you say up and down like the Assyrian Empire?
[02:24:48.220 --> 02:24:49.220]   It's a Monty Python thing.
[02:24:49.220 --> 02:24:50.220]   Oh, okay.
[02:24:50.220 --> 02:24:53.220]   I thought you were a real historian there from--
[02:24:53.220 --> 02:24:54.220]   No, no.
[02:24:54.220 --> 02:24:55.220]   I can't claim credit.
[02:24:55.220 --> 02:24:57.220]   He's a Monty Python historian.
[02:24:57.220 --> 02:24:58.220]   That's all that matters.
[02:24:58.220 --> 02:25:00.220]   You know you have your historical roots.
[02:25:00.220 --> 02:25:02.220]   Monty Python's life of Brian.
[02:25:02.220 --> 02:25:03.220]   It's a wonderful film.
[02:25:03.220 --> 02:25:04.220]   Oh, I'm glad I had the monday.
[02:25:04.220 --> 02:25:05.220]   Yeah.
[02:25:05.220 --> 02:25:06.220]   Love that.
[02:25:06.220 --> 02:25:07.220]   You could probably see it on Netflix.
[02:25:07.220 --> 02:25:08.220]   Now, I'm going to go to the show.
[02:25:08.220 --> 02:25:11.220]   You could probably see it on Netflix.
[02:25:11.220 --> 02:25:14.220]   Netflix is now expanding their gaming efforts.
[02:25:14.220 --> 02:25:15.220]   They're looking--
[02:25:15.220 --> 02:25:17.220]   I think Netflix is looking at what Apple's doing.
[02:25:17.220 --> 02:25:21.220]   They want to do a subscription gaming service, kind of like Apple's arcade.
[02:25:21.220 --> 02:25:25.220]   They've already done games like based on Stranger Things and other things.
[02:25:25.220 --> 02:25:29.220]   But apparently, according to an exclusive article in the information,
[02:25:29.220 --> 02:25:35.220]   Nick Wingfield and Jessica Tunkle, they are going to expand their game efforts.
[02:25:35.220 --> 02:25:37.220]   Pretty soon an Netflix is going to own everything.
[02:25:37.220 --> 02:25:39.220]   I mean, like a controller?
[02:25:39.220 --> 02:25:43.220]   Like a controller hooked up to your TV via Bluetooth?
[02:25:43.220 --> 02:25:45.220]   Is that what we're talking about here?
[02:25:45.220 --> 02:25:49.220]   They're still figuring it out, according to one industry executive.
[02:25:49.220 --> 02:25:51.220]   You've got to remember this is the information.
[02:25:51.220 --> 02:25:53.220]   So it'll probably be a rumor that is going on.
[02:25:53.220 --> 02:25:56.220]   And then six months later, yes, Netflix is designed as a new game.
[02:25:56.220 --> 02:25:57.220]   They're usually an insight.
[02:25:57.220 --> 02:25:58.220]   Yeah, right.
[02:25:58.220 --> 02:25:59.220]   It could be.
[02:25:59.220 --> 02:26:02.220]   I feel like they've saturated the market for the Netflix and chill people.
[02:26:02.220 --> 02:26:04.220]   And then for the people who don't participate in that type of activity.
[02:26:04.220 --> 02:26:05.220]   For those who don't--
[02:26:05.220 --> 02:26:06.220]   Gaming.
[02:26:06.220 --> 02:26:12.220]   Gaming is becoming a streaming product, right?
[02:26:12.220 --> 02:26:19.220]   Well, I mean, the thing is, the gaming is no longer you buy a lot of hardware and you buy a game and you sit at home and play it.
[02:26:19.220 --> 02:26:25.220]   More and more, thanks to Google's Stadia and Microsoft's X-Cloud and G-Force Now from Nvidia.
[02:26:25.220 --> 02:26:27.220]   It's a streaming product, right?
[02:26:27.220 --> 02:26:30.220]   I mean, I'm not a big gamer.
[02:26:30.220 --> 02:26:35.220]   I still play Civilization II if I'm going to be elite, if I'm going to be gaming.
[02:26:35.220 --> 02:26:40.220]   But the Stadia and the rest of them don't seem to work that well so far.
[02:26:40.220 --> 02:26:41.220]   I mean, have you tried them?
[02:26:41.220 --> 02:26:42.220]   Because I'm genuinely curious.
[02:26:42.220 --> 02:26:44.220]   No, they're totally playable.
[02:26:44.220 --> 02:26:45.220]   Oh, OK.
[02:26:45.220 --> 02:26:46.220]   Yeah.
[02:26:46.220 --> 02:26:47.220]   You have to have the right bandwidth.
[02:26:47.220 --> 02:26:52.220]   Like, you definitely have to have a certain tier of bandwidth in your home for it to be dependable.
[02:26:52.220 --> 02:26:54.220]   But yeah, I found Stadia to be pretty darn responsive.
[02:26:54.220 --> 02:26:56.220]   When it first launched, it was pretty--
[02:26:56.220 --> 02:26:59.220]   It was the only way to play Cyberpunk 2017.
[02:26:59.220 --> 02:27:01.220]   It was there for a while.
[02:27:01.220 --> 02:27:06.220]   You don't want to do something where latency might really be the end of the line for you.
[02:27:06.220 --> 02:27:07.220]   Yeah, that's the thing.
[02:27:07.220 --> 02:27:09.220]   But SIV 2 would be fine for instance.
[02:27:09.220 --> 02:27:11.220]   There's no reason you should have to have a game.
[02:27:11.220 --> 02:27:12.220]   But first shooters, I mean--
[02:27:12.220 --> 02:27:13.220]   No, I found one counter-shoot games.
[02:27:13.220 --> 02:27:14.220]   I found one counter-shoot games.
[02:27:14.220 --> 02:27:15.220]   I found one counter-shoot games.
[02:27:15.220 --> 02:27:17.220]   That was just a-- it blew me away.
[02:27:17.220 --> 02:27:20.220]   Counter-Strike on broadband was just fantastic.
[02:27:20.220 --> 02:27:24.220]   I literally spent 10 hours playing the first time I was introduced to it.
[02:27:24.220 --> 02:27:28.220]   But it came down to latency and it came down to hardware and eventually you could get some
[02:27:28.220 --> 02:27:30.220]   up from any other second.
[02:27:30.220 --> 02:27:31.220]   Then you're in trouble.
[02:27:31.220 --> 02:27:32.220]   Yeah, then you're in trouble.
[02:27:32.220 --> 02:27:33.220]   So I'm curious.
[02:27:33.220 --> 02:27:40.220]   I think it'll work, but you need to get that bandwidth and the bandwidth needs to be reliable.
[02:27:40.220 --> 02:27:43.220]   Well, I mean, but in the future, it'll just keep getting better.
[02:27:43.220 --> 02:27:45.220]   Latency will go down over time.
[02:27:45.220 --> 02:27:49.220]   If they're looking at a long play of 15 years or something like that.
[02:27:49.220 --> 02:27:50.220]   Yeah.
[02:27:50.220 --> 02:27:54.220]   But clearly, they must be thinking, "What's that?"
[02:27:54.220 --> 02:27:56.220]   "Look what we did to Hollywood."
[02:27:56.220 --> 02:27:57.220]   Yeah.
[02:27:57.220 --> 02:27:58.220]   It came into town.
[02:27:58.220 --> 02:28:00.220]   What other industries can we just wrote?
[02:28:00.220 --> 02:28:04.980]   It makes a little company from Los Gatos, California that shipped DVDs to people and
[02:28:04.980 --> 02:28:07.220]   now we dominate the Oscars.
[02:28:07.220 --> 02:28:10.220]   What if we did that to gaming?
[02:28:10.220 --> 02:28:11.220]   Yeah.
[02:28:11.220 --> 02:28:14.340]   Everybody who focuses on the technology but really matters to the content, what people
[02:28:14.340 --> 02:28:15.500]   really love is the content.
[02:28:15.500 --> 02:28:21.300]   We could create original content plus drive others to come to our platform and make them
[02:28:21.300 --> 02:28:22.300]   available to us.
[02:28:22.300 --> 02:28:23.300]   It's historically difficult, though.
[02:28:23.300 --> 02:28:27.100]   Google created its own independent gaming lab to go along with Stadium.
[02:28:27.100 --> 02:28:28.300]   They've now fired everybody.
[02:28:28.300 --> 02:28:29.700]   They brought in really good people.
[02:28:29.700 --> 02:28:31.300]   They hired a lot of people.
[02:28:31.300 --> 02:28:32.300]   And it just didn't work.
[02:28:32.300 --> 02:28:34.780]   It's hard to do a game people want to play.
[02:28:34.780 --> 02:28:38.100]   I don't think Google could have done what Netflix did.
[02:28:38.100 --> 02:28:40.340]   I don't think Apple is doing what Netflix did.
[02:28:40.340 --> 02:28:41.580]   Nobody did what Netflix did.
[02:28:41.580 --> 02:28:44.620]   So I think they might be high in their own supply.
[02:28:44.620 --> 02:28:47.700]   I mean, they have a great track record for creating awesome content.
[02:28:47.700 --> 02:28:51.540]   But I do think the game industry is selling the seeds for its own destruction on this
[02:28:51.540 --> 02:28:55.940]   one because if you look at the amount of user resentment from loot boxes and the rest
[02:28:55.940 --> 02:29:00.580]   of it and the amount of government kickback, free to play has become the blight of the
[02:29:00.580 --> 02:29:01.580]   gaming industry.
[02:29:01.580 --> 02:29:02.580]   It's terrible.
[02:29:02.580 --> 02:29:03.580]   Yeah, exactly.
[02:29:03.580 --> 02:29:08.820]   So if Netflix could go in there and reorganize the gaming thing, so yeah, you don't have to
[02:29:08.820 --> 02:29:10.940]   buy all the extra stuff to get to the top levels.
[02:29:10.940 --> 02:29:14.660]   You can actually just play a game the way it used to be made.
[02:29:14.660 --> 02:29:15.660]   Right.
[02:29:15.660 --> 02:29:16.660]   That's what Apple Arcade is.
[02:29:16.660 --> 02:29:20.300]   It took all those games that were free to play, but you end up buying a lot of stuff in
[02:29:20.300 --> 02:29:21.300]   them.
[02:29:21.300 --> 02:29:22.300]   Yeah.
[02:29:22.300 --> 02:29:23.300]   It took them all and took that all away.
[02:29:23.300 --> 02:29:24.380]   You pay five bucks a month.
[02:29:24.380 --> 02:29:28.260]   They're not the greatest games, but there's more than a hundred of them.
[02:29:28.260 --> 02:29:30.620]   And there's no additional expense down there.
[02:29:30.620 --> 02:29:31.620]   So I think the...
[02:29:31.620 --> 02:29:32.620]   I think the...
[02:29:32.620 --> 02:29:36.060]   Yeah, and I think that's what's going to happen is that there's going to be a huge reaction
[02:29:36.060 --> 02:29:37.060]   to...
[02:29:37.060 --> 02:29:38.300]   I have a theory about what they're thinking.
[02:29:38.300 --> 02:29:41.580]   What if they turn their top shows into video games?
[02:29:41.580 --> 02:29:43.580]   That's what they did with Stranger Things.
[02:29:43.580 --> 02:29:44.580]   Exactly.
[02:29:44.580 --> 02:29:45.580]   Exactly.
[02:29:45.580 --> 02:29:46.580]   Black Mirror.
[02:29:46.580 --> 02:29:47.580]   Exactly.
[02:29:47.580 --> 02:29:48.580]   But what if they did that?
[02:29:48.580 --> 02:29:49.580]   Game of Thrones is just wasted.
[02:29:49.580 --> 02:29:50.580]   Scale, right.
[02:29:50.580 --> 02:29:51.580]   Yeah.
[02:29:51.580 --> 02:29:52.580]   That's probably what they're thinking.
[02:29:52.580 --> 02:29:56.980]   They're not a reflection of the problem Netflix in any company that's hugely successful like
[02:29:56.980 --> 02:29:58.580]   Netflix has, which is what's next.
[02:29:58.580 --> 02:29:59.580]   How do you keep growing?
[02:29:59.580 --> 02:30:00.580]   How do you keep growing?
[02:30:00.580 --> 02:30:01.580]   What's happening?
[02:30:01.580 --> 02:30:03.580]   I just want to work out what the Peaky Blenders game would be.
[02:30:03.580 --> 02:30:05.580]   Well, we're in Gold Splat.
[02:30:05.580 --> 02:30:08.580]   And you put a razor in it and go to town.
[02:30:08.580 --> 02:30:09.580]   I don't think...
[02:30:09.580 --> 02:30:10.580]   I might be a fun game.
[02:30:10.580 --> 02:30:11.580]   Hey, I...
[02:30:11.580 --> 02:30:14.980]   Honestly, I watched that series just because the accents are hilarious.
[02:30:14.980 --> 02:30:15.980]   Oh, I love them.
[02:30:15.980 --> 02:30:17.980]   And it might mean more to you than you do to a Espa-Dance.
[02:30:17.980 --> 02:30:19.980]   Oh, I was raised in the black country.
[02:30:19.980 --> 02:30:20.980]   Ah, you know.
[02:30:20.980 --> 02:30:21.980]   Are they accurate?
[02:30:21.980 --> 02:30:26.580]   Some of them are frankly hilarious.
[02:30:26.580 --> 02:30:29.580]   It's very easy to go from a bromi accent to a Liverpool accent.
[02:30:29.580 --> 02:30:30.580]   Yeah.
[02:30:30.580 --> 02:30:31.580]   And, you know, it's...
[02:30:31.580 --> 02:30:32.580]   I mean, too, it's funny.
[02:30:32.580 --> 02:30:34.660]   The main actors do it pretty well.
[02:30:34.660 --> 02:30:35.660]   But there are some...
[02:30:35.660 --> 02:30:38.900]   There's that occasional slip which is just like, "Ooh!"
[02:30:38.900 --> 02:30:42.980]   If my granny heard that, she'd be throwing things at the TV.
[02:30:42.980 --> 02:30:51.180]   Guys, I think we should all toast one another for a wonderful in-studio meat.
[02:30:51.180 --> 02:30:52.180]   Cheers.
[02:30:52.180 --> 02:30:53.180]   Cheers.
[02:30:53.180 --> 02:30:54.180]   Really, what is...
[02:30:54.180 --> 02:30:57.340]   I'm sorry we can't send some of this fabulous Mezcal to you.
[02:30:57.340 --> 02:30:59.580]   Should we plug this guy's Mezcal 'cause it really...
[02:30:59.580 --> 02:31:00.580]   Can you buy it?
[02:31:00.580 --> 02:31:03.780]   No, they don't export it.
[02:31:03.780 --> 02:31:04.780]   You have to go there.
[02:31:04.780 --> 02:31:05.780]   Okay, I will.
[02:31:05.780 --> 02:31:06.780]   Which you will.
[02:31:06.780 --> 02:31:07.780]   When can I be...
[02:31:07.780 --> 02:31:08.780]   Oh, October, I'll see in...
[02:31:08.780 --> 02:31:09.780]   I'll see you in Wauk.
[02:31:09.780 --> 02:31:10.780]   I think we're going to see this guy.
[02:31:10.780 --> 02:31:11.780]   It's like the best scotch.
[02:31:11.780 --> 02:31:12.780]   It's like the best scotch.
[02:31:12.780 --> 02:31:13.780]   They will not export it to the US.
[02:31:13.780 --> 02:31:14.780]   Absolutely.
[02:31:14.780 --> 02:31:15.780]   And you've got to...
[02:31:15.780 --> 02:31:16.780]   That's why you've got to travel for the alcohol.
[02:31:16.780 --> 02:31:17.780]   Well, thank you for bringing it.
[02:31:17.780 --> 02:31:19.580]   No, no, that was fantastic.
[02:31:19.580 --> 02:31:20.580]   Thank you.
[02:31:20.580 --> 02:31:24.180]   It's amazing for putting this together, bringing in a bunch of guys in the studio.
[02:31:24.180 --> 02:31:25.860]   I'm glad you saved a seat for yourself.
[02:31:25.860 --> 02:31:27.860]   It's really always a pleasure to have you on.
[02:31:27.860 --> 02:31:29.500]   All about Android.
[02:31:29.500 --> 02:31:31.820]   Every Tuesday about 5 p.m. Pacific.
[02:31:31.820 --> 02:31:34.220]   Tech News Weekly every Thursday around noon, right?
[02:31:34.220 --> 02:31:35.220]   Yeah.
[02:31:35.220 --> 02:31:36.220]   Yeah, 1130.
[02:31:36.220 --> 02:31:37.220]   1130.
[02:31:37.220 --> 02:31:38.220]   So yeah, about noon.
[02:31:38.220 --> 02:31:39.220]   11 a.m.
[02:31:39.220 --> 02:31:42.060]   We'll get started around 11.
[02:31:42.060 --> 02:31:45.300]   If it's anything like my show, noon is probably more accurate.
[02:31:45.300 --> 02:31:48.100]   Anyway, I'm so glad you could be here and thank you for producing.
[02:31:48.100 --> 02:31:49.100]   Thank you.
[02:31:49.100 --> 02:31:50.100]   It was pretty wonderful.
[02:31:50.100 --> 02:31:52.660]   And if there's anybody, I would say, "Oh, we've got to have him in."
[02:31:52.660 --> 02:31:53.660]   I think it was.
[02:31:53.660 --> 02:31:56.420]   In fact, Ian Thomas, we've got to have him in.
[02:31:56.420 --> 02:31:57.420]   Oh, that's right.
[02:31:57.420 --> 02:32:00.700]   Normally you would stop off at the English food store here, but that's long gone.
[02:32:00.700 --> 02:32:05.060]   I know, but the average British club in San Rafael has reopened.
[02:32:05.060 --> 02:32:08.940]   I would stop off there, but my wife isn't with me and she'd kill me if I went in there
[02:32:08.940 --> 02:32:11.140]   and that's what was wrong with outside.
[02:32:11.140 --> 02:32:14.740]   There's a Scotch egg waiting with your name.
[02:32:14.740 --> 02:32:17.540]   They wrote it there five years ago and it's still there.
[02:32:17.540 --> 02:32:19.980]   I'm so much Scotch eggs.
[02:32:19.980 --> 02:32:22.820]   No one does them properly over here.
[02:32:22.820 --> 02:32:23.820]   You know, it's just--
[02:32:23.820 --> 02:32:26.820]   Can we do a special for the Twit Club?
[02:32:26.820 --> 02:32:29.900]   Ian makes Scotch eggs, teaches us all how to do it.
[02:32:29.900 --> 02:32:33.100]   Well, I mean, there's a steak and ale pie in my fridge at the moment, which I cooked
[02:32:33.100 --> 02:32:34.100]   last night.
[02:32:34.100 --> 02:32:35.100]   Whoa!
[02:32:35.100 --> 02:32:36.900]   I think this might be a very nice--
[02:32:36.900 --> 02:32:41.820]   At some point, I may have to actually bring Scotch eggs up because you need to taste the
[02:32:41.820 --> 02:32:47.380]   shared joy of a hard-billed egg wrapped in pork meat and beetened breadcrumbs and deep
[02:32:47.380 --> 02:32:48.380]   fried.
[02:32:48.380 --> 02:32:49.380]   That sounds pretty good.
[02:32:49.380 --> 02:32:53.180]   It is pretty damn good.
[02:32:53.180 --> 02:32:57.740]   Ian, you'll find Ian at the register.com where he's a news editor and writes all those
[02:32:57.740 --> 02:32:59.780]   great headlines and fun bits.
[02:32:59.780 --> 02:33:01.380]   You're writing more, it looks like.
[02:33:01.380 --> 02:33:03.020]   I'm writing a bit more now, yeah.
[02:33:03.020 --> 02:33:05.780]   That'll change because we're actually hiring.
[02:33:05.780 --> 02:33:06.940]   We're hiring a lot.
[02:33:06.940 --> 02:33:11.460]   We've got a couple of people in the US, someone else in APAC.
[02:33:11.460 --> 02:33:13.300]   And in the UK as well.
[02:33:13.300 --> 02:33:15.500]   So we're actually expanding the red at the moment.
[02:33:15.500 --> 02:33:16.500]   Fantastic.
[02:33:16.500 --> 02:33:18.140]   It was after a year that looked fairly dodgy.
[02:33:18.140 --> 02:33:20.260]   We've actually come out of it pretty damn well.
[02:33:20.260 --> 02:33:21.420]   Yeah, I'm hopeful.
[02:33:21.420 --> 02:33:22.780]   I feel like-- I don't know.
[02:33:22.780 --> 02:33:24.460]   I just feel like the lid is lifting.
[02:33:24.460 --> 02:33:26.700]   And this is an example of it.
[02:33:26.700 --> 02:33:31.020]   You can get Michael O'Donnell in here taking pictures for the first time in a year.
[02:33:31.020 --> 02:33:37.940]   And of course, Mike Elgin, gastronomad.net heading out onto the road to Provence and then
[02:33:37.940 --> 02:33:38.940]   Morocco.
[02:33:38.940 --> 02:33:42.380]   And it's Europe just as soon as they'll let us in, which is very soon.
[02:33:42.380 --> 02:33:46.940]   But you'll keep writing and that's the beauty of Elgin.com is the blog, but also Fast Company
[02:33:46.940 --> 02:33:47.940]   and elsewhere.
[02:33:47.940 --> 02:33:52.100]   Yes, you can find it all, Elgin.com and find me on Twitter.
[02:33:52.100 --> 02:33:54.100]   I'm verified, Jason.
[02:33:54.100 --> 02:33:55.100]   Mike Elgin.
[02:33:55.100 --> 02:33:56.100]   Everybody's verified.
[02:33:56.100 --> 02:33:57.100]   He's verified.
[02:33:57.100 --> 02:33:58.100]   I mean, wow.
[02:33:58.100 --> 02:33:59.100]   I wonder why I want to be verified.
[02:33:59.100 --> 02:34:00.100]   I want to be part of the club.
[02:34:00.100 --> 02:34:01.100]   You can have my blue sheet.
[02:34:01.100 --> 02:34:02.100]   I don't know.
[02:34:02.100 --> 02:34:03.100]   That's tick mine.
[02:34:03.100 --> 02:34:05.620]   I'll give you mine.
[02:34:05.620 --> 02:34:06.620]   Thank you for being here, Mike.
[02:34:06.620 --> 02:34:07.620]   Thanks for inviting me.
[02:34:07.620 --> 02:34:11.660]   I guess the next time I'll see you, it will be in Oaxaca for the day of the day.
[02:34:11.660 --> 02:34:12.660]   I guess so.
[02:34:12.660 --> 02:34:13.660]   Yeah.
[02:34:13.660 --> 02:34:15.580]   That's going to be quite the positive.
[02:34:15.580 --> 02:34:16.580]   Wait.
[02:34:16.580 --> 02:34:17.580]   It's going to be epic.
[02:34:17.580 --> 02:34:24.500]   We do this week in tech every Sunday, right after the tech guy show, round about 230 Pacific
[02:34:24.500 --> 02:34:26.820]   530 Eastern 2130 UTC.
[02:34:26.820 --> 02:34:29.100]   If you want to tune in live, easy to do.
[02:34:29.100 --> 02:34:31.580]   There's a live stream at twitter.tv/live.
[02:34:31.580 --> 02:34:33.100]   There's audio and video there.
[02:34:33.100 --> 02:34:37.660]   If you're watching live, of course, you could join the chatters in IRC.
[02:34:37.660 --> 02:34:38.660]   IRC.
[02:34:38.660 --> 02:34:39.660]   .tv.
[02:34:39.660 --> 02:34:44.380]   There's also, of course, on demand shows for everything we do because it's a podcast.
[02:34:44.380 --> 02:34:47.660]   So you can download those from the website, twitter.tv.
[02:34:47.660 --> 02:34:51.180]   There's also a YouTube channel dedicated to this show and all of our shows each have
[02:34:51.180 --> 02:34:52.980]   their own YouTube channel.
[02:34:52.980 --> 02:34:56.540]   My favorite way for you to listen is to subscribe.
[02:34:56.540 --> 02:34:58.500]   There are plenty of podcast applications.
[02:34:58.500 --> 02:35:01.740]   If you do subscribe, you'll get it automatically the minute it's available.
[02:35:01.740 --> 02:35:05.780]   And if they allow reviews, we'd sure appreciate a five star.
[02:35:05.780 --> 02:35:06.980]   That would be nice.
[02:35:06.980 --> 02:35:11.420]   I also want to invite anybody who wants to support us further to join ClubTwit.
[02:35:11.420 --> 02:35:13.500]   There's a few benefits to the club.
[02:35:13.500 --> 02:35:16.940]   You can go to twit.tv/clubtwit to find out more.
[02:35:16.940 --> 02:35:18.140]   Seven dollars a month.
[02:35:18.140 --> 02:35:19.140]   That's it.
[02:35:19.140 --> 02:35:20.820]   There's no yearly plan, monthly planner.
[02:35:20.820 --> 02:35:22.140]   It's just month to month.
[02:35:22.140 --> 02:35:24.020]   You can cancel anytime automatically.
[02:35:24.020 --> 02:35:25.620]   It's very easy.
[02:35:25.620 --> 02:35:26.780]   But here's what you get.
[02:35:26.780 --> 02:35:29.060]   Add free versions of all of our shows.
[02:35:29.060 --> 02:35:30.220]   All of the ads cut out.
[02:35:30.220 --> 02:35:32.500]   I think it's done very seamlessly.
[02:35:32.500 --> 02:35:34.100]   Takes about an hour out of every show.
[02:35:34.100 --> 02:35:38.980]   I must say the shows are considerably shorter without the ads.
[02:35:38.980 --> 02:35:41.580]   But also you get the fabulous discord.
[02:35:41.580 --> 02:35:43.620]   We really have a lot of fun in there.
[02:35:43.620 --> 02:35:47.820]   I'm playing Valheim from time to time, like every night.
[02:35:47.820 --> 02:35:49.060]   We have a Linux show in there.
[02:35:49.060 --> 02:35:50.060]   We do a lot of stuff.
[02:35:50.060 --> 02:35:54.660]   I want to get a Scotch egg cook off going maybe in there.
[02:35:54.660 --> 02:35:55.660]   Ian is now in there.
[02:35:55.660 --> 02:36:00.060]   Hey, give me a deep fat fry or enough pork meat I can do.
[02:36:00.060 --> 02:36:02.740]   Pork meat is on me, my friend.
[02:36:02.740 --> 02:36:06.180]   And so the discord is fun.
[02:36:06.180 --> 02:36:07.860]   By the way, it's not just the shows.
[02:36:07.860 --> 02:36:10.660]   Every show has a discord channel inside our server.
[02:36:10.660 --> 02:36:17.740]   But also there's been great conversations on every area you can imagine from anime,
[02:36:17.740 --> 02:36:20.700]   to travel, to space, to sci-fi.
[02:36:20.700 --> 02:36:22.100]   Lisa has a channel in there.
[02:36:22.100 --> 02:36:24.620]   There's data science and comics.
[02:36:24.620 --> 02:36:25.620]   So it's a great community.
[02:36:25.620 --> 02:36:28.380]   I think this is the future, honestly, of social.
[02:36:28.380 --> 02:36:33.700]   It's these kinds of dedicated servers where you can go and hang out with people you like.
[02:36:33.700 --> 02:36:36.700]   Without having a deck of sprits filled off the third parties.
[02:36:36.700 --> 02:36:37.700]   Yeah, exactly.
[02:36:37.700 --> 02:36:38.700]   And yes, exactly that.
[02:36:38.700 --> 02:36:41.060]   In fact, it's one of the reasons people join Club Twitter's.
[02:36:41.060 --> 02:36:44.100]   There's no tracking at all in any of the shows.
[02:36:44.100 --> 02:36:49.860]   You'll get dedicated URLs just for you of all the shows and there's no tracking.
[02:36:49.860 --> 02:36:54.260]   And then finally, there's a Twit+ feed where we put things like our interviews with Andy
[02:36:54.260 --> 02:36:57.540]   Weir, but also behind the scenes stuff.
[02:36:57.540 --> 02:37:01.780]   And if we ever do a Scotch egg episode, it'll be there.
[02:37:01.780 --> 02:37:03.580]   In short, you'd have to be nuts.
[02:37:03.580 --> 02:37:05.420]   You'd have to join Club Twitter.
[02:37:05.420 --> 02:37:07.660]   Well, we thank you, all of you who have supported us.
[02:37:07.660 --> 02:37:08.660]   It really helps.
[02:37:08.660 --> 02:37:13.540]   And we were able to do that special triangulation with Andy Weir because of listeners like you.
[02:37:13.540 --> 02:37:14.540]   So thank you.
[02:37:14.540 --> 02:37:18.340]   Twit.tv/clubtwit.
[02:37:18.340 --> 02:37:19.340]   That's it for the show.
[02:37:19.340 --> 02:37:20.340]   Thank you for being here.
[02:37:20.340 --> 02:37:21.340]   I'll see you next week.
[02:37:21.340 --> 02:37:22.340]   Another Twit.
[02:37:22.340 --> 02:37:23.340]   It's in the can.
[02:37:23.340 --> 02:37:29.880]   Do the Twit.
[02:37:29.880 --> 02:37:33.540]   Do the Twit.
[02:37:33.540 --> 02:37:35.260]   Do the Twit.

