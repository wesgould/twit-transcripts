;FFMETADATA1
title=The Great Firewall of America
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=780
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:06.720]   It's time for Twit this week in Tech. Mike Elgin is here. Seth Rosenblatt, Alex Wilhelm joining us.
[00:00:06.720 --> 00:00:11.280]   And we've got the experts. We're going to talk about the Twitter hack. Who did it, how they did it,
[00:00:11.280 --> 00:00:19.520]   and why it's a big deal, whether the US should shut down TikTok. And why is that Amazon dash cart so
[00:00:19.520 --> 00:00:25.040]   very small? It's not for Americans. That's for sure. It's all next on Twit.
[00:00:26.000 --> 00:00:30.880]   This week at Tech comes to you from Twit's LastPass Studios. Stay in control when it comes to your
[00:00:30.880 --> 00:00:37.120]   company's access points and authentication. LastPass makes security simple for your remote workforce.
[00:00:37.120 --> 00:00:40.400]   Check out lazpass.com/twit to learn more.
[00:00:43.120 --> 00:00:56.240]   Podcasts you love from people you trust. This is Twit.
[00:00:56.240 --> 00:01:04.960]   This is Twit. This week in Tech, episode 780 recorded Sunday, July 19th, 2020, the great firewall
[00:01:04.960 --> 00:01:10.960]   of America. This episode of This Week in Tech is brought to you by Salesforce Service Cloud.
[00:01:10.960 --> 00:01:16.080]   Salesforce Service Cloud is the world's number one customer service platform that empowers
[00:01:16.080 --> 00:01:21.040]   organizations to deliver service from anywhere from home, in the office or in the field.
[00:01:21.040 --> 00:01:26.720]   Go to bit.ly/salesforceforceservice to find out more.
[00:01:26.720 --> 00:01:34.560]   And by Zendesk. Zendesk is a service-first CRM company that builds software designed to
[00:01:34.560 --> 00:01:39.440]   improve customer relationships. Right now, Zendesk is offering a complimentary six-month
[00:01:39.440 --> 00:01:44.480]   remote support bundle that comes with the essential tools your team needs to stay agile.
[00:01:44.480 --> 00:01:50.800]   And it takes hours, not weeks, to get up and running. Go to zendesk.com/twit to get started.
[00:01:50.800 --> 00:01:57.920]   And by ExpressVPN. ExpressVPN is an app that re-routs your internet connection through their
[00:01:57.920 --> 00:02:03.760]   secure servers so your ISP can't see the sites you visit. For three extra months free with a
[00:02:03.760 --> 00:02:12.560]   one-year package, go to expressvpn.com/twit. And by LastPass, give your IT Department a break
[00:02:12.560 --> 00:02:18.480]   and supply them with the tools that really protect your business. Visit LastPass.com/twit
[00:02:18.480 --> 00:02:19.920]   to find out how they can help you.
[00:02:19.920 --> 00:02:30.480]   It's time for our Twits! This week at Tech the Tech News Roundtable,
[00:02:30.480 --> 00:02:36.800]   where we discuss the week's Tech News with a rotating group of very smart technology
[00:02:36.800 --> 00:02:43.120]   journalists. And I love the panel today. Mike Elgin, my old friend is here from Elgin.com.
[00:02:43.120 --> 00:02:47.680]   Used to be a Gastronomad. Now he's grounded.
[00:02:47.680 --> 00:02:50.320]   Now I'm just a Gastropod.
[00:02:50.320 --> 00:02:51.120]   Just a Gastropod.
[00:02:51.120 --> 00:02:55.120]   Just staying in place and eating everything.
[00:02:55.120 --> 00:03:01.680]   Gastropod. Of course you will be getting back on the road I'm sure as soon as it's safe to do
[00:03:01.680 --> 00:03:08.160]   sleep. Unfortunately you're not allowed to go any of these places anymore. But France won't
[00:03:08.160 --> 00:03:10.480]   let me come anymore. Isn't that rotten?
[00:03:10.480 --> 00:03:10.800]   It's tragic.
[00:03:10.800 --> 00:03:16.720]   Just last night Lisa said when this is over, first place we're going, France.
[00:03:16.720 --> 00:03:20.160]   Yes, that's it. It's done. Done deal.
[00:03:20.720 --> 00:03:27.680]   Also with us from the ParallaxTH-Parallax.com. Seth Rosenblatt, he's editor and chief there.
[00:03:27.680 --> 00:03:28.160]   Hi everyone.
[00:03:28.160 --> 00:03:36.320]   Security guru, hello Seth. You're looking very wolverine today. I like the
[00:03:36.320 --> 00:03:38.880]   the mutton shops. That's a great look.
[00:03:38.880 --> 00:03:44.320]   I did a COVID beard for three months and I got sick of being woken up in the middle of the night
[00:03:44.320 --> 00:03:46.960]   by my mustache hairs tickling my nose.
[00:03:46.960 --> 00:03:48.720]   Yeah, it was quite rude.
[00:03:50.560 --> 00:03:57.840]   So the day before I proposed to my now fiance, I chopped this stuff down.
[00:03:57.840 --> 00:03:59.600]   Congratulations. Thank you.
[00:03:59.600 --> 00:04:00.320]   That's awesome.
[00:04:00.320 --> 00:04:01.360]   He said yes by the way.
[00:04:01.360 --> 00:04:03.840]   Oh that's wonderful. Congratulations.
[00:04:03.840 --> 00:04:06.800]   When are the nuptials? Have you set a date?
[00:04:06.800 --> 00:04:12.800]   We're aiming for August. We're just who knows how long shutdown in California is going to be.
[00:04:12.800 --> 00:04:15.840]   So we're just going to kind of crank this thing out.
[00:04:15.840 --> 00:04:20.960]   Well, I think if you could be quarantined with somebody for the last four months,
[00:04:20.960 --> 00:04:24.480]   and still like them, that's a very good test.
[00:04:24.480 --> 00:04:26.640]   Yeah, of her patience.
[00:04:26.640 --> 00:04:33.760]   Great. Yeah, she moved in for the right reasons just before the shutdown.
[00:04:33.760 --> 00:04:34.240]   Perfect.
[00:04:34.240 --> 00:04:36.960]   Oh, it wasn't even like forced. That's awesome.
[00:04:36.960 --> 00:04:39.920]   No, no. I mean, I didn't feel forced, but
[00:04:42.160 --> 00:04:46.320]   and it's been great. And we actually, it's worse than just living,
[00:04:46.320 --> 00:04:50.080]   it's worse than just sharing a bedroom together. Our office is shared.
[00:04:50.080 --> 00:04:53.920]   So I have a desk on one side and she has a desk on the other.
[00:04:53.920 --> 00:04:58.480]   And except for, you know, when I'm showering or running the dog,
[00:04:58.480 --> 00:05:01.200]   we're pretty much around each other 22 hours a day.
[00:05:01.200 --> 00:05:03.600]   Yeah, this is a stress test for any relationship.
[00:05:03.600 --> 00:05:04.960]   Yeah, but it's been great.
[00:05:04.960 --> 00:05:05.680]   It's been really good.
[00:05:05.680 --> 00:05:06.640]   I feel very fortunate.
[00:05:06.640 --> 00:05:07.200]   Very lucky.
[00:05:07.200 --> 00:05:08.960]   That I actually like my life.
[00:05:08.960 --> 00:05:12.880]   But you know, I feel for people who don't. That's it's going to be time.
[00:05:12.880 --> 00:05:17.200]   Yeah, I've heard of a lot of relationships that have come to an end because of
[00:05:17.200 --> 00:05:18.320]   the first thing.
[00:05:18.320 --> 00:05:22.240]   You're thinking of having a socially distanced wedding then, if you're thinking of August.
[00:05:22.240 --> 00:05:25.600]   It would be small. It would be a very small group.
[00:05:25.600 --> 00:05:33.280]   Beagley right now in San Francisco, you're allowed, I think a religious ceremony of 13 people total.
[00:05:34.720 --> 00:05:38.240]   So yeah, so it would be very small, but that's okay.
[00:05:38.240 --> 00:05:39.760]   We'd rather get this done.
[00:05:39.760 --> 00:05:41.520]   Excuse, actually.
[00:05:41.520 --> 00:05:47.840]   To be honest, any excuse to have a small wedding as far as I'm concerned.
[00:05:47.840 --> 00:05:51.600]   Hey, who is here? But Alex Wilhelm recently married.
[00:05:51.600 --> 00:05:53.440]   Still married, right Alex?
[00:05:53.440 --> 00:05:54.560]   Why is that?
[00:05:54.560 --> 00:05:54.800]   Yeah.
[00:05:54.800 --> 00:05:58.080]   No, she hasn't. We just hit the one year mark.
[00:05:58.080 --> 00:06:00.320]   But thank you.
[00:06:00.320 --> 00:06:02.480]   Speaking of changes, I still have half time on the west coast.
[00:06:02.480 --> 00:06:05.760]   So for the first six months of our marriage, I was only here half the time.
[00:06:05.760 --> 00:06:08.560]   And then I moved here full time and I was still going to travel a bunch.
[00:06:08.560 --> 00:06:09.440]   And then COVID happened.
[00:06:09.440 --> 00:06:12.720]   So we've also been learning how to be around each other all the time.
[00:06:12.720 --> 00:06:15.120]   And it took everything to some adjustments.
[00:06:15.120 --> 00:06:17.680]   But like Seth said, so far really happy with it.
[00:06:17.680 --> 00:06:21.680]   I'm also very lucky with who I'm with because she's tremendous.
[00:06:21.680 --> 00:06:23.280]   It's been a lot of fun.
[00:06:23.280 --> 00:06:26.560]   Well, she also is a psychiatrist.
[00:06:26.560 --> 00:06:28.480]   So very handy.
[00:06:29.200 --> 00:06:32.880]   It's a little intimidating sometimes because you don't want to complain too much about the wrong things.
[00:06:32.880 --> 00:06:34.320]   I'm crazy.
[00:06:34.320 --> 00:06:35.040]   You're not up there.
[00:06:35.040 --> 00:06:35.520]   Exactly.
[00:06:35.520 --> 00:06:38.720]   We can always 5150 you Alex.
[00:06:38.720 --> 00:06:40.560]   It's always a possibility.
[00:06:40.560 --> 00:06:43.280]   I know what that means because it's songs are band-rope.
[00:06:43.280 --> 00:06:44.400]   But I had to Google what that means.
[00:06:44.400 --> 00:06:45.200]   I know that joke.
[00:06:45.200 --> 00:06:48.960]   So you all are Twitter lovers.
[00:06:48.960 --> 00:06:52.880]   Alex actually has a very dangerous handle.
[00:06:52.880 --> 00:06:54.640]   It's only four letters long.
[00:06:54.640 --> 00:06:56.240]   Yes, sir.
[00:06:56.240 --> 00:07:01.440]   I am very grateful that I didn't go for @leo back in 2006 when I signed up for Twitter.
[00:07:01.440 --> 00:07:03.280]   I decided to be better to have my full name.
[00:07:03.280 --> 00:07:04.880]   You are at risk.
[00:07:04.880 --> 00:07:09.680]   And in fact, it looks like this Twitter hack that happened on Wednesday
[00:07:09.680 --> 00:07:20.080]   is involved with or was started by a group of hackers who are seeking the OG Twitter
[00:07:20.640 --> 00:07:25.680]   handles, the @6 which was Adrian Lamo's old handler.
[00:07:25.680 --> 00:07:26.480]   @y.
[00:07:26.480 --> 00:07:31.120]   Those are the ones that are @God @Fear.
[00:07:31.120 --> 00:07:34.400]   Those are the ones that are drawing big bucks.
[00:07:34.400 --> 00:07:40.880]   And some of these hackers are very good at using sim swapping or other techniques
[00:07:40.880 --> 00:07:46.000]   to get these handles.
[00:07:46.720 --> 00:07:50.560]   And it's those guys who have been reporting.
[00:07:50.560 --> 00:07:52.960]   They actually talked to the New York Times.
[00:07:52.960 --> 00:07:59.600]   Lucky 225 has a medium post about the Twitter hack.
[00:07:59.600 --> 00:08:05.680]   Lucky 225 actually owned Adrian Lamo's @6 account after Adrian passed away.
[00:08:05.680 --> 00:08:12.640]   And he noticed it had been hacked because the bad guys which in this case,
[00:08:13.360 --> 00:08:17.600]   they would change everything including verification, phone numbers, email addresses.
[00:08:17.600 --> 00:08:20.800]   So you might never know except that your password stopped working,
[00:08:20.800 --> 00:08:23.040]   but they forgot to change the Google voice number.
[00:08:23.040 --> 00:08:24.800]   He sent out a tweet.
[00:08:24.800 --> 00:08:32.240]   This was early Wednesday saying there's something going on and it's much more
[00:08:32.240 --> 00:08:36.880]   than just guessing a password or a sim hijacking.
[00:08:36.880 --> 00:08:40.640]   They somehow have really gotten into Twitter.
[00:08:40.640 --> 00:08:43.840]   And I kind of suspect that immediately too because when you see
[00:08:43.840 --> 00:08:47.760]   Barack Obama's account hacked, Joe Biden account hacked,
[00:08:47.760 --> 00:08:52.560]   you know that at least those two are certainly using strong two-factor authentication.
[00:08:52.560 --> 00:08:56.160]   Obama is the number one most followed person on Twitter.
[00:08:56.160 --> 00:09:00.800]   So Twitter likely has additional security on his account as they do on President Trump's note
[00:09:00.800 --> 00:09:02.560]   that President Trump did not get hacked.
[00:09:02.560 --> 00:09:08.480]   But Twitter immediately kind of went crazy.
[00:09:08.480 --> 00:09:13.120]   So the hacks that we all saw, I'll just recap for those of you
[00:09:13.120 --> 00:09:17.360]   stayed away from the news this week for a good reason.
[00:09:17.360 --> 00:09:24.320]   The hack essentially took was an account takeover and it would be displayed by a
[00:09:24.320 --> 00:09:27.120]   tweet that says, "Hey, something along the lines of,
[00:09:27.120 --> 00:09:30.960]   "Hey, I want to give back to my community.
[00:09:31.760 --> 00:09:39.120]   So for the next 24 hours, I'm going to double any Bitcoin sent to this address."
[00:09:39.120 --> 00:09:42.560]   And sure, I believe it.
[00:09:42.560 --> 00:09:43.600]   They must have meant that.
[00:09:43.600 --> 00:09:47.120]   And so the idea was people would send Bitcoin and guess what?
[00:09:47.120 --> 00:09:50.240]   It would not get doubled in shockingly.
[00:09:50.240 --> 00:09:56.240]   As soon as we saw this and we were doing this week in Google when it happened,
[00:09:57.280 --> 00:10:01.760]   it was immediately obvious somebody had gotten deep into the Twitter credentials.
[00:10:01.760 --> 00:10:04.400]   Either it was at a rogue Twitter employee, which has happened before
[00:10:04.400 --> 00:10:09.040]   a contractor leaving Twitter in 2017, disabled, depressed its account.
[00:10:09.040 --> 00:10:11.120]   So we know that's possible.
[00:10:11.120 --> 00:10:17.600]   It seems to be that, according to the New York Times, this is actually really shocking.
[00:10:17.600 --> 00:10:21.680]   Twitter had-- and Seth, I'll defer to you on this because you're the security guru and I'm sure
[00:10:21.680 --> 00:10:24.480]   you've been doing nothing over the last three days but writing about this.
[00:10:25.680 --> 00:10:34.000]   Twitter had apparently put the credentials for its Godmo dashboard in their Slack channel and pinned it.
[00:10:34.000 --> 00:10:37.520]   Boo hiss.
[00:10:37.520 --> 00:10:42.400]   At the Parallax, we actually ran a story, I think it was two years ago,
[00:10:42.400 --> 00:10:47.680]   on how you need to not use Slack for important trusted communication.
[00:10:47.680 --> 00:10:49.120]   Okay.
[00:10:49.120 --> 00:10:50.480]   I don't know what to tell you.
[00:10:53.120 --> 00:10:56.800]   We haven't confirmed that. Twitter hasn't confirmed it, but the New York Times reported it.
[00:10:56.800 --> 00:11:01.200]   I think it's fairly credible. They were talking to people who were on the periphery of this hack.
[00:11:01.200 --> 00:11:07.600]   These guys who steal-- who sim swap and steal single character Twitter handles.
[00:11:07.600 --> 00:11:10.720]   And short ones like @Alex, has anybody ever tried to hack you?
[00:11:10.720 --> 00:11:16.400]   Not that I've noticed, but I do run-- I'm not going to exactly detail what security I use,
[00:11:16.400 --> 00:11:20.880]   but my Twitter account was the first thing that I put on my password manager and I have other
[00:11:20.880 --> 00:11:25.840]   security stuff attached to it just out of general reasonable this.
[00:11:25.840 --> 00:11:28.880]   But I was most surprised that they got to Joe Biden's account and all of this,
[00:11:28.880 --> 00:11:31.280]   because when it was-- That has to be so secure, right?
[00:11:31.280 --> 00:11:35.040]   You would think so, right? Well, now after I heard about the Slack thing, I was less sure.
[00:11:35.040 --> 00:11:38.000]   But when they got Elon-- Well, but that's Twitter's fault.
[00:11:38.000 --> 00:11:39.520]   That's not Joe Biden's fault.
[00:11:39.520 --> 00:11:41.760]   Entirely. No, I'm not going to talk to a buddy about me.
[00:11:41.760 --> 00:11:47.360]   A Twitter has a Godmoad that is accessible apparently without severe--
[00:11:47.360 --> 00:11:51.200]   you should, for instance, it should be IP checked, it geo-located checked.
[00:11:51.200 --> 00:11:55.200]   It should only be accessed accessible to the most trusted Twitter employees,
[00:11:55.200 --> 00:11:58.240]   and it should not have its credentials posted in the Slack channel.
[00:11:58.240 --> 00:12:02.640]   One other-- There's one really serious issue that came out of this
[00:12:02.640 --> 00:12:10.000]   that I find to be very mind-boggling, which is that it does appear that some direct messages
[00:12:10.000 --> 00:12:11.120]   were talking about-- We're talking about--
[00:12:11.120 --> 00:12:11.680]   We're talking about--
[00:12:11.680 --> 00:12:12.160]   --counts that were hacked.
[00:12:12.160 --> 00:12:12.720]   Yeah.
[00:12:12.720 --> 00:12:19.840]   Although not the verified accounts, but the non-verified accounts were downloaded or otherwise accessed.
[00:12:19.840 --> 00:12:23.680]   And the problem there is that people like using a service.
[00:12:23.680 --> 00:12:25.840]   So you're in Twitter, you want to continue using Twitter,
[00:12:25.840 --> 00:12:29.200]   you don't want to have to hop out and use an end-to-end encrypted service,
[00:12:29.200 --> 00:12:32.640]   even though you may have signal on your phone or wire or whatever.
[00:12:32.640 --> 00:12:38.880]   And people still, including in the InfoSec world, still communicate
[00:12:40.160 --> 00:12:44.160]   very confidential information over Twitter DM.
[00:12:44.160 --> 00:12:45.440]   Don't be. Do not.
[00:12:45.440 --> 00:12:49.920]   It really just sort of beggars the mind.
[00:12:49.920 --> 00:12:50.480]   Yeah.
[00:12:50.480 --> 00:12:53.520]   Why on earth would you do this when you know it's not secure,
[00:12:53.520 --> 00:12:55.440]   when you know Twitter employees have access?
[00:12:55.440 --> 00:12:58.960]   You know, I was talking with somebody about it, and I said,
[00:12:58.960 --> 00:13:01.600]   "This is like, if Twitter is the public square,
[00:13:01.600 --> 00:13:03.920]   then you're going off into a corner of the public square
[00:13:03.920 --> 00:13:06.800]   and having a conversation. You think it's private,
[00:13:06.800 --> 00:13:08.480]   but everyone else is still around you.
[00:13:08.480 --> 00:13:11.360]   It's just that it looks like you're being a little bit more secretive."
[00:13:11.360 --> 00:13:13.840]   It's really mind-boxing.
[00:13:13.840 --> 00:13:18.560]   How much, Seth, you've covered break-ins and things like this for years.
[00:13:18.560 --> 00:13:24.320]   How much should we trust, for instance, these hackers,
[00:13:24.320 --> 00:13:28.400]   these SimSwapper hackers, telling the new, and PlugWalk Joe, for instance?
[00:13:28.400 --> 00:13:31.520]   How much, how accurate is this?
[00:13:31.520 --> 00:13:36.480]   Or I worry that this is disinformation, that it isn't some guy named Kirk,
[00:13:36.480 --> 00:13:39.840]   or that it maybe was some guy named Kirk, which is what they said,
[00:13:39.840 --> 00:13:44.640]   but he had other motives besides merely this stupid, spammy thing,
[00:13:44.640 --> 00:13:46.480]   which at best netted him 100,000.
[00:13:46.480 --> 00:13:50.960]   Although we don't even know because it's very typical when there's a Bitcoin address
[00:13:50.960 --> 00:13:54.720]   given for a hack like this, that the guys who control it will put money in
[00:13:54.720 --> 00:13:59.280]   and take money out to make it to obfuscate how much is going on there.
[00:13:59.280 --> 00:14:01.280]   How incredible is this?
[00:14:01.280 --> 00:14:04.880]   Or could it all be a smokescreen for something much more devious?
[00:14:05.760 --> 00:14:09.040]   I mean, it could be in the grand deal.
[00:14:09.040 --> 00:14:09.600]   We just don't know.
[00:14:09.600 --> 00:14:11.120]   We don't know.
[00:14:11.120 --> 00:14:17.920]   We do know, Joe Cox over at Vice Motherboard was the first person to report on SimSwapping
[00:14:17.920 --> 00:14:20.000]   in connection to the Twitter hack.
[00:14:20.000 --> 00:14:25.200]   The fact that the New York Times seems to have also confirmed it
[00:14:25.200 --> 00:14:28.560]   indicates that SimSwap, at least the SimSwapping community,
[00:14:28.560 --> 00:14:31.360]   and to some degree, was involved in this.
[00:14:32.880 --> 00:14:37.280]   What were the motives as they were reported by the New York Times
[00:14:37.280 --> 00:14:40.320]   or as they were reported by Joe Cox?
[00:14:40.320 --> 00:14:43.360]   Probably, possibly.
[00:14:43.360 --> 00:14:48.720]   This always puts me, stuff like this always puts me in mind of
[00:14:48.720 --> 00:14:56.480]   the Sony hack from the end of 2015, and nobody would believe that it was North Korea that did it.
[00:14:58.080 --> 00:15:04.800]   Everybody was very, very skeptical. North Korea's hacking division was not really on a lot of people's
[00:15:04.800 --> 00:15:07.600]   radar, and lo and behold, it turned out to be North Korea.
[00:15:07.600 --> 00:15:13.520]   Are the hackers that are talking to the media actually the people who were behind this,
[00:15:13.520 --> 00:15:16.160]   and their motives are what they describe them to be?
[00:15:16.160 --> 00:15:16.880]   It's possible.
[00:15:16.880 --> 00:15:22.880]   I think that a more paranoid stance is probably the better one to take.
[00:15:22.880 --> 00:15:25.520]   Don't trust Twitter DMs.
[00:15:25.520 --> 00:15:32.740]   30,000 games. I make sure that if you're doing sensitive communication, do it in a
[00:15:32.740 --> 00:15:35.440]   place. That's sensitive.
[00:15:35.440 --> 00:15:42.480]   And I've made my Twitter account private, but that's useless because if somebody has
[00:15:42.480 --> 00:15:46.920]   access to the God Mode dashboard, they could change anything. They could make it a public
[00:15:46.920 --> 00:15:53.880]   again. They could do anything they want. I use a UbeKey. I turned off SMS for a normal
[00:15:53.880 --> 00:16:00.480]   hacker. That would be good security. But if you've got the dashboard, it doesn't matter.
[00:16:00.480 --> 00:16:05.720]   If you're using services that are not end-end encrypted, which basically means that the
[00:16:05.720 --> 00:16:10.480]   only people that can see messages are at either end of the message, the person sending
[00:16:10.480 --> 00:16:17.600]   it and the person receiving it, then anyone who works for the service potentially has
[00:16:17.600 --> 00:16:23.160]   access to what's being communicated, whether it's photos or whatever. Facebook probably
[00:16:23.160 --> 00:16:29.880]   has very similar access that Twitter does. LinkedIn probably has very similar access.
[00:16:29.880 --> 00:16:34.000]   I mean, the thought of LinkedIn getting hacked in whatever's being said in those messages
[00:16:34.000 --> 00:16:39.720]   between people, I think is truly terrifying because that's ostensibly where so much business
[00:16:39.720 --> 00:16:40.720]   is being conducted.
[00:16:40.720 --> 00:16:46.220]   Yeah, I mean, there were a number of real concerns that come up. One is they could tweet
[00:16:46.220 --> 00:16:52.980]   as President Trump, you know, go to your basements because I'm about to launch a nuclear attack
[00:16:52.980 --> 00:16:58.860]   on Russia. They could. I mean, who knows how much chaos that would cause. But the point
[00:16:58.860 --> 00:17:05.540]   is the president uses Twitter for policy statements. Yes. And it's risky now that somebody could
[00:17:05.540 --> 00:17:10.780]   have impersonated him. I think it's also telling that somebody did not impersonate him. That's
[00:17:10.780 --> 00:17:18.700]   a little suspicious to me. Now the Kirk, this Kirk guy was a sim swapper. Apparently he
[00:17:18.700 --> 00:17:27.300]   sold at why for 1500 Bitcoin, that Bitcoin went to the same address that was later used
[00:17:27.300 --> 00:17:34.980]   in this scam to double your Bitcoin. So that ties him directly to this. We don't know who
[00:17:34.980 --> 00:17:40.180]   Kirk is. In fact, he has no history. The only made is a discord account where all the
[00:17:40.180 --> 00:17:48.460]   conversations going on a month ago. So who is Kirk? I don't know could be it. Vladimir.
[00:17:48.460 --> 00:17:56.460]   I don't know. It doesn't. It does seem like it was just a simple dumb Bitcoin scam that
[00:17:56.460 --> 00:18:02.500]   had best netted him $100,000 at best. What a way. Jesus. Nothing. You did all this work
[00:18:02.500 --> 00:18:05.940]   to get in there and you got a hundred. Well, for instance, you could make more of them
[00:18:05.940 --> 00:18:12.100]   if you took some time and stayed in the God mode and sold some accounts bit by bit. I
[00:18:12.100 --> 00:18:16.660]   would imagine there are people who would want Joe Biden's account for a lot of money or
[00:18:16.660 --> 00:18:25.940]   yes, right? It could easily have been a trial run for whether or not this way of getting
[00:18:25.940 --> 00:18:30.380]   at those accounts is effective. But then you busted it because by doing it, you've gone
[00:18:30.380 --> 00:18:37.620]   very public. Even I with no inside information immediately knew this was an inside job and
[00:18:37.620 --> 00:18:45.620]   almost either was a rugamploid or someone had got into the administrator accounts. That
[00:18:45.620 --> 00:18:50.060]   was obvious. So you've blown your cover now by doing this. There's all sorts of things
[00:18:50.060 --> 00:18:54.380]   they could. For instance, they could have just download DMs in the background with anybody
[00:18:54.380 --> 00:19:02.340]   known for years. Sure. Maybe they have been. We don't know how long they've had access
[00:19:02.340 --> 00:19:11.140]   for one thing. I hesitate to speculate during cases like this, which are rapidly unfolding
[00:19:11.140 --> 00:19:16.300]   and we don't yet. It hasn't even been five days. Absolutely. And Twitter's not been super
[00:19:16.300 --> 00:19:21.860]   forthcoming. No, nor should they. I don't think at this point. I think they've said
[00:19:21.860 --> 00:19:26.540]   what they need to say. And it's pretty clear that there is a lot of alignment between
[00:19:26.540 --> 00:19:33.700]   what's been reported and what Twitter's been saying. But I think that this is the kind
[00:19:33.700 --> 00:19:39.860]   of thing where there are so many moving parts and we don't know. I mean, the people who are
[00:19:39.860 --> 00:19:48.220]   behind this are certainly not on any law enforcement radar now. Until now. To me, the takeaways
[00:19:48.220 --> 00:19:53.780]   from this are twofold. First of all, we're wondering, okay, is this China or Russia or
[00:19:53.780 --> 00:19:58.620]   Iran or one of the usual suspects in terms of state sponsored hacking? And then they
[00:19:58.620 --> 00:20:06.380]   invented a Kirk to sort of be the face of it. The young naive face of this. I think
[00:20:06.380 --> 00:20:13.820]   is probably this hack itself is what it appears to be. Some dumb kids managed to stumble
[00:20:13.820 --> 00:20:18.620]   into Twitter and tried to make some money from it. And they could have made a lot more and
[00:20:18.620 --> 00:20:23.380]   they didn't do it that well. But my get. So the first thing that this tells me is that
[00:20:23.380 --> 00:20:28.820]   you know, the likelihood is that China, Russia, Iran, certainly the NSA and others have been
[00:20:28.820 --> 00:20:33.820]   probably exploiting the same hack and rummaging around in these accounts. Interesting.
[00:20:33.820 --> 00:20:38.100]   Invisibly behind the scenes for probably years. The other thing is that we talk about these
[00:20:38.100 --> 00:20:42.740]   risks. I know we're going to talk about TikTok later, but this is a public network when something
[00:20:42.740 --> 00:20:48.820]   like this happens and it goes public. The effect of it is visible to all of us, most
[00:20:48.820 --> 00:20:53.140]   of the effects are the fact that it exists is visible to all of us. But when this sort
[00:20:53.140 --> 00:20:58.620]   of thing happens within WhatsApp, for example, in what we were talking about, what if somebody
[00:20:58.620 --> 00:21:03.580]   took over Trump's account and said there's a nuclear war or whatever, that kind of thing
[00:21:03.580 --> 00:21:08.380]   actually happens in places like India through WhatsApp. People say, oh, there's, you know,
[00:21:08.380 --> 00:21:14.300]   they start riots and stuff like that through these false rumors within WhatsApp. And it's
[00:21:14.300 --> 00:21:19.220]   far more nefarious in those private networks where you can't see what other people are
[00:21:19.220 --> 00:21:24.780]   seeing, right? And it's certainly even worse in TikTok where you can algorithmically show
[00:21:24.780 --> 00:21:30.340]   a million people a million different messages. So I think, you know, all of these suspicions
[00:21:30.340 --> 00:21:37.100]   are our suspicions we should have, but we should be aware that these kinds of things,
[00:21:37.100 --> 00:21:41.420]   the same kinds of things can be happening in the secret behind the scenes networks like
[00:21:41.420 --> 00:21:44.300]   Facebook, et cetera, which is far more nefarious.
[00:21:44.300 --> 00:21:48.540]   It does. We know it does. Because it happens all the time. Right? Absolutely. Yes.
[00:21:48.540 --> 00:21:55.380]   So setting aside an issue like shouldn't Twitter have, you know, encrypted its DMs and which
[00:21:55.380 --> 00:22:01.300]   we know they've been, you know, has been debated internally there for years and Twitter has
[00:22:01.300 --> 00:22:07.060]   always refused to do it. I think there's a real issue here that it gets back to the
[00:22:07.060 --> 00:22:14.540]   Sony question, which is what role does the US government need to play in advising tech
[00:22:14.540 --> 00:22:21.180]   companies as to how to protect their, you know, their internal tools, how to protect
[00:22:21.180 --> 00:22:26.820]   their data and not just tech companies because it's media companies, it's financial institutions.
[00:22:26.820 --> 00:22:32.860]   We do know that there is communication, right? There's no way that they're not talking,
[00:22:32.860 --> 00:22:42.940]   but where it feels sort of like tech organizations are working going in one direction. The US
[00:22:42.940 --> 00:22:47.980]   government is going in another direction and there's not collaboration where there should
[00:22:47.980 --> 00:22:53.740]   be. Well, you don't want a nation state attacking a tech company without the tech company being
[00:22:53.740 --> 00:22:57.740]   aware that the crowd has targeted a nation. The US government is simultaneously going
[00:22:57.740 --> 00:23:03.700]   in two different directions, all multiple, all the time. The NSA, for instance, has a complete
[00:23:03.700 --> 00:23:08.940]   duality. On the one hand, their job, their responsibilities to protect American companies
[00:23:08.940 --> 00:23:15.020]   and Americans, I guess, peripherally from hacks by other companies, countries. On the
[00:23:15.020 --> 00:23:19.660]   other hand, their mission is to do those, perpetrate those hacks on other countries. And
[00:23:19.660 --> 00:23:26.860]   as we found out, in the process of which creating hacking tools that they, whoops, let escape
[00:23:26.860 --> 00:23:34.380]   and turned into ransomware technologies like Eternal Blue that were used against Americans.
[00:23:34.380 --> 00:23:41.580]   So even within the NSA, there are these conflicting goals, dualities.
[00:23:41.580 --> 00:23:49.740]   There's been talk for a long time of having a cyber czar. Some one person who's in charge
[00:23:49.740 --> 00:23:55.340]   wasn't really Giuliani. The cybers are not even joking. I know.
[00:23:55.340 --> 00:24:03.340]   You know, it's scary. It's really. Trump initially proposed that Barron Trump do it.
[00:24:03.340 --> 00:24:10.940]   Yeah. He knows that he's all over the cyber. So really, the questions for me have to do with
[00:24:10.940 --> 00:24:15.900]   Twitter at this point, because I think, I mean, again, we don't know exactly, but it sure looks
[00:24:15.900 --> 00:24:23.020]   like Twitter didn't do a very good job. Forget end-to-end encrypting DMs. They let some
[00:24:23.020 --> 00:24:32.300]   bozo get access to their God mode. That is a horrific breach of responsibility. And because,
[00:24:32.300 --> 00:24:38.220]   and honestly, I think it should diminish Twitter as a public utility. They have been a public
[00:24:38.220 --> 00:24:43.260]   utility, prop chief because of the president, but they have become this important public utility.
[00:24:43.260 --> 00:24:47.900]   But I think this should diminish their role there because pretty clearly they can't be trusted with
[00:24:47.900 --> 00:24:52.540]   it. I would like Twitter to make a much more clear. They did put a blog post up yesterday.
[00:24:52.540 --> 00:24:57.340]   As we head into the weekend, next week, we're focused on these core objectives,
[00:24:57.340 --> 00:25:00.940]   restoring access for all account owners. Oh, yeah. The first thing they did is they turned
[00:25:00.940 --> 00:25:06.860]   off verified accounts, which actually in turn, it makes no sense except that it won't be people
[00:25:06.860 --> 00:25:11.660]   from verified accounts who are sending out these big coins. It was very funny.
[00:25:12.300 --> 00:25:15.740]   That was one of the funniest hours on Twitter I've ever had. Well, all of a sudden none of us could
[00:25:15.740 --> 00:25:22.300]   tweet. And by the way, I saw article after article of saying this is how it's there was a wired
[00:25:22.300 --> 00:25:28.860]   Twitter is at its best when the verified accounts can't tweet. Man, this is great. It's like it used
[00:25:28.860 --> 00:25:35.900]   to be where they know these celebrities, some some some some verified accounts.
[00:25:35.900 --> 00:25:39.180]   There is a problem. Yeah, I don't know what to think about that. I don't know what that means.
[00:25:39.740 --> 00:25:45.180]   They crawl vera. There's some there's apparently a Twitter account that has tweeted every word.
[00:25:45.180 --> 00:25:51.580]   And so some verified accounts are actually retweeting word by word because you could still
[00:25:51.580 --> 00:25:59.100]   retweet word by word, making messages by retweeting one word at a time. I did not sink to that level.
[00:25:59.100 --> 00:26:04.540]   I was way worse than that. Like a random. That's actually rather innovative. I was just texting
[00:26:04.540 --> 00:26:07.740]   with my friends and I had them tweet out my texts for me and then I would retweet them when they
[00:26:07.740 --> 00:26:12.300]   would tweet my texts. Addits. We were I mean, we were having a fun. Remember when Twitter was
[00:26:12.300 --> 00:26:16.060]   fun? Leo back in the day. That's what it felt like. It felt like it's so the
[00:26:16.060 --> 00:26:20.460]   Twitter's first goal is to the president got involved. Twitter's first goal. And this was
[00:26:20.460 --> 00:26:26.220]   yesterday. We want to restore access for all the account owners who may still be locked out
[00:26:26.220 --> 00:26:33.900]   as a result of our remediation efforts to our second goal continuing our investigation into the
[00:26:33.900 --> 00:26:40.300]   incident and cooperation with law enforcement. Finally at three, they get to the most important
[00:26:40.300 --> 00:26:47.900]   point further secure further is a doobie as word securing our systems to protect future attacks
[00:26:47.900 --> 00:26:53.420]   and then rolling out additional company wide training to guard against social engineering
[00:26:53.420 --> 00:26:58.940]   tactics. This was back when they said it was social engineering like somebody took our engineer
[00:26:58.940 --> 00:27:04.220]   out and got them drunk and got the password. But I think I believe the slack story. So we don't know
[00:27:04.220 --> 00:27:12.860]   yet. But that sounds like exactly what some knit would do. So but what should be the consequence
[00:27:12.860 --> 00:27:17.980]   to Twitter? What is the result upshot of this? It's like saying what should the consequences be
[00:27:17.980 --> 00:27:24.620]   to Facebook for allowing the kinds of political discourse that have happened there. I mean,
[00:27:24.620 --> 00:27:28.060]   they're not going to face any punishment. Nothing's going to happen.
[00:27:28.060 --> 00:27:34.700]   But we can demand we should demand that they do a better job of security.
[00:27:34.700 --> 00:27:39.260]   I think embarrassment matters. I think the fact that there's some shame to this.
[00:27:39.260 --> 00:27:44.460]   My in-laws brought this up to me. We were having dinner and they're not Twitter users really at
[00:27:44.460 --> 00:27:47.980]   all. And they were like, did you hear about Twitter? I'm like, oh, yes, I did. Because I was there. I
[00:27:47.980 --> 00:27:53.500]   logged off to come see you. Here's what Twitter wrote. We are acutely aware of our responsibilities
[00:27:53.500 --> 00:27:59.500]   to the people who use our service and to society more generally. Unfortunately, that's the case.
[00:27:59.500 --> 00:28:06.140]   We're embarrassed. We're disappointed. And more than anything, we're sorry. We know we must work
[00:28:06.140 --> 00:28:11.100]   to regain your trust. And we will support all efforts to bring the perpetrators to justice.
[00:28:11.100 --> 00:28:16.300]   We hope that our openness and transparency throughout this process and the steps and works we will
[00:28:16.300 --> 00:28:20.620]   take to safeguard against other attacks in the future will be the start of making this right. I
[00:28:20.620 --> 00:28:27.340]   think that's completely sincere. So if we see, I mean, if Twitter is really sincere about this,
[00:28:27.340 --> 00:28:34.300]   then we will see the security team at Twitter being taken a lot more seriously. We will see
[00:28:34.300 --> 00:28:41.180]   security improvements being made to Twitter. And not unlike Zoom, where they went out and
[00:28:41.180 --> 00:28:45.260]   hired a bunch of security experts and brought them in and said, what do we need to do? And they laid
[00:28:45.260 --> 00:28:50.300]   out a series of steps. And for the most part, it seems like Zoom achieved that goal.
[00:28:50.860 --> 00:28:56.300]   Or Google in 2010, when they revealed they'd been hacked by China. And then Google suddenly
[00:28:56.300 --> 00:29:02.140]   started taking security significantly more seriously. We should see in the next three to six months,
[00:29:02.140 --> 00:29:09.580]   major steps taken by Twitter. And part of the problem is, is that I think that this is internally
[00:29:09.580 --> 00:29:17.340]   a culture issue. This is an issue where Twitter security team needs to be taken a lot more seriously.
[00:29:17.340 --> 00:29:22.540]   The products need to be updated with security in mind, as well as user experience and other
[00:29:22.540 --> 00:29:32.060]   very important things. But to not do so, I think is almost a criminal act within six months of this
[00:29:32.060 --> 00:29:38.620]   presidential election. >> I don't think it's that big of a deal really because Twitter is facing
[00:29:38.620 --> 00:29:44.300]   embarrassment. Probably there's an economic downfall. Really ultimately, Twitter is mostly a public
[00:29:44.300 --> 00:29:48.940]   thing. It's not like it's a financial service. And in the larger scheme of things, this is a case
[00:29:48.940 --> 00:29:55.340]   in which Twitter's probable negligence or alleged negligence led to some users being
[00:29:55.340 --> 00:30:02.460]   screwed in one way or another. And it was accidental. Whereas with Facebook, we learned that Facebook
[00:30:02.460 --> 00:30:09.740]   screws its users on purpose ten times a year. And so in the larger scheme of things, this is a
[00:30:09.740 --> 00:30:17.580]   relatively innocent thing. And I'm sure they'll fix it. But I just don't think this is that big
[00:30:17.580 --> 00:30:24.060]   of a deal. It only feels like a big deal because big deal users were affected. >> I feel like it's
[00:30:24.060 --> 00:30:29.900]   a big deal. Obviously, what happened is not a big deal. >> Like Alex. But what could have happened
[00:30:29.900 --> 00:30:35.340]   could have been a big deal. And the fact that the president of the United States uses Twitter as
[00:30:35.340 --> 00:30:41.260]   his, I mean, literally primary way. He could have press conferences. He could put out memos.
[00:30:41.260 --> 00:30:45.260]   He could write articles. There's a million ways a president could communicate. He has the bully
[00:30:45.260 --> 00:30:50.060]   pulpit. He chooses Twitter as the primary way he communicates with the outside world.
[00:30:50.060 --> 00:30:58.460]   Gives Twitter an unusual role, not a role it deserves. But it's a real, and as a result,
[00:30:58.460 --> 00:31:03.580]   I think there's a higher standard that has to be applied to Twitter. Maybe not. >> I disagree.
[00:31:03.580 --> 00:31:09.980]   I mean, what Trump has trained us to do is to not believe anything he tweets.
[00:31:09.980 --> 00:31:16.300]   >> Yeah, that's a good point. It's not a, you're very- >> That's not a Twitter problem,
[00:31:16.300 --> 00:31:22.620]   it's a president problem. But it's just the fact of the matter. As Roosevelt used radio and
[00:31:22.620 --> 00:31:28.300]   fireside chats, the president uses Twitter. And so, I mean, they're talking about,
[00:31:29.500 --> 00:31:35.420]   Republicans talk about anti-conservative bias on social media. This should concern them a little
[00:31:35.420 --> 00:31:41.260]   bit more. You could put words into the mouth of leaders, not just the president, but all kinds
[00:31:41.260 --> 00:31:47.580]   of leaders all over the world, if you had access to this tool. Do you think we would just ignore it?
[00:31:47.580 --> 00:31:53.180]   If the president said, I really decided I don't like China, so we're going to nuke them. Or we're
[00:31:53.180 --> 00:31:53.820]   going to- >> Yes.
[00:31:53.820 --> 00:31:59.020]   >> Or Elon Musk said, let's say you should, so one of the things they could have done is short
[00:31:59.020 --> 00:32:05.020]   Tesla and then have Elon tweet, you're right, this whole thing isn't working. We've lost
[00:32:05.020 --> 00:32:10.620]   billions of dollars, I'm going to shut it down. Or vice versa, buy Tesla and have Elon said,
[00:32:10.620 --> 00:32:17.420]   I have got the funds to buy Tesla, take it private, and I'm going to do so at $420 a
[00:32:17.420 --> 00:32:18.700]   share. Oh, wait a minute, he already did that.
[00:32:18.700 --> 00:32:24.060]   >> Yeah, he already did that. The thing though is, I think we've lost so much faith in the
[00:32:24.060 --> 00:32:26.940]   president's ability to tweet factual things. And if he tweeted out- >> Or he did.
[00:32:26.940 --> 00:32:31.660]   >> Yeah, if he tweeted out, we're going to nuke China right now. I would, well, one,
[00:32:31.660 --> 00:32:34.700]   I always have to double check that it's the actual president, not a parody account,
[00:32:34.700 --> 00:32:38.380]   so it's often very close. But I wouldn't believe it from his Twitter account, because I think
[00:32:38.380 --> 00:32:41.980]   he would just be confused about his own process. I don't think the president would be that on the ball.
[00:32:41.980 --> 00:32:46.540]   >> So if you agree with Mike, we definitely wouldn't believe it.
[00:32:46.540 --> 00:32:50.460]   >> Yeah, yeah. Okay. But there might be others who would. I mean, I-
[00:32:50.460 --> 00:32:54.540]   >> Yeah, no, I hear you. We're not, we're not the entire world here. I mean, but I think we are
[00:32:54.540 --> 00:32:57.980]   at some of the most online people that have ever existed. Like the four of us, it's pretty bad.
[00:32:57.980 --> 00:32:59.740]   But- >> What if he tweeted out, I'm going to give Kim
[00:32:59.740 --> 00:33:05.820]   John one day to dismantle all the nukes, or we're going in?
[00:33:05.820 --> 00:33:10.060]   Well, it's Trump, sorry. The next day he- >> He never mind, we best friends.
[00:33:10.060 --> 00:33:15.260]   >> Well, within, within 20 minutes, we would be able to identify that this was
[00:33:15.260 --> 00:33:19.660]   not tweeted by him and it was false, etc. This is the nature of Twitter because it's public.
[00:33:19.660 --> 00:33:22.380]   And again, I think we should really be worrying about the secret private-
[00:33:22.380 --> 00:33:25.580]   >> Behind the seats, I agree. >> Communications, that's the beauty of
[00:33:25.580 --> 00:33:30.780]   Twitter. This hack was clearly a hack within seconds. We all knew that there was a hack going on.
[00:33:30.780 --> 00:33:33.580]   >> Yeah. No, you're absolutely right. That's a very good point.
[00:33:33.580 --> 00:33:36.380]   All right, next TikTok, we're gonna take a break.
[00:33:36.380 --> 00:33:47.180]   Oh, I love the, I love tech. It's a never ending panoply of bizarness. Alex Wilhelm is here. He is,
[00:33:47.980 --> 00:33:53.180]   he likes to call himself a reporter. He's modest senior editor at TechCrunch.
[00:33:53.180 --> 00:33:56.220]   Long time, friend. It's great to see you in my old home town.
[00:33:56.220 --> 00:33:57.500]   >> Good to be here. >> Actually, my old house.
[00:33:57.500 --> 00:33:59.820]   >> Yes, sir. >> In the backyard,
[00:33:59.820 --> 00:34:03.340]   where I had a young lad with lounge about.
[00:34:03.340 --> 00:34:09.100]   >> Today it is so hot, it feels like it's the service of Mars or the sun, whatever the hotter
[00:34:09.100 --> 00:34:12.540]   than it is. But we'll get there. >> Yeah, it gets really cold and really hot.
[00:34:12.540 --> 00:34:16.620]   It's kind of the best of both worlds. Mike Elgin is here. He's grounded for the nonce
[00:34:16.620 --> 00:34:20.380]   elgin.com. Mike, you still do a mics list, which I love.
[00:34:20.380 --> 00:34:26.380]   >> Yep, mics list. Guess what, Leo? Mike's list is now 20 years old.
[00:34:26.380 --> 00:34:29.980]   >> What? That's amazing. >> Oh, yeah.
[00:34:29.980 --> 00:34:30.780]   >> That's really good. >> Yeah.
[00:34:30.780 --> 00:34:34.060]   And you have a cardboard robot head behind you.
[00:34:34.060 --> 00:34:37.740]   >> Yeah, one more year and mics list can drink alcohol.
[00:34:37.740 --> 00:34:39.740]   [laughter]
[00:34:39.740 --> 00:34:41.020]   >> 21.
[00:34:41.020 --> 00:34:43.900]   >> Great to have Seth Rosenblatt here.
[00:34:43.900 --> 00:34:46.940]   Perfect day, our security guru at the Parallax.
[00:34:46.940 --> 00:34:48.220]   >> Yeah, that worked out really well.
[00:34:48.220 --> 00:34:48.940]   >> Yeah.
[00:34:48.940 --> 00:34:54.220]   I think our producer has some ideas in his head. He's thinking things out.
[00:34:54.220 --> 00:34:56.620]   >> Oh, are you blaming your producer for this?
[00:34:56.620 --> 00:34:58.700]   >> Was it your-- how long have you been booked for this show?
[00:34:58.700 --> 00:35:02.140]   >> A couple of weeks. >> Okay, so you're right.
[00:35:02.140 --> 00:35:03.820]   >> A week? Something. >> It was just--
[00:35:03.820 --> 00:35:04.860]   >> I don't know. >> It was just a time.
[00:35:04.860 --> 00:35:07.980]   >> Time has no meaning anymore. >> Although there's almost always a security
[00:35:07.980 --> 00:35:10.460]   flaw in the world, isn't there? >> Yeah.
[00:35:10.460 --> 00:35:12.300]   >> There's plenty to talk about with Seth.
[00:35:12.300 --> 00:35:13.660]   Our show today brought to you by--
[00:35:13.660 --> 00:35:15.820]   We're so glad to have him back. Salesforce.
[00:35:15.820 --> 00:35:20.220]   We use Salesforce as our CRM,
[00:35:20.220 --> 00:35:22.220]   as our customer relationship management,
[00:35:22.220 --> 00:35:24.700]   as our sales team is using it all the time.
[00:35:24.700 --> 00:35:28.300]   I want to talk a little bit about something that we perhaps will use
[00:35:28.300 --> 00:35:31.820]   in future Salesforce service cloud.
[00:35:31.820 --> 00:35:36.380]   This is a way that you can serve your customers,
[00:35:36.380 --> 00:35:38.060]   super serve your customers.
[00:35:38.060 --> 00:35:40.780]   Service Cloud wants you to be prepared during these unknown times.
[00:35:40.780 --> 00:35:43.180]   They know you're trying to do more with less.
[00:35:43.180 --> 00:35:47.900]   You're hustling to adapt to new customer needs, new ways of business.
[00:35:47.900 --> 00:35:50.940]   You're innovating to respond to the current environment.
[00:35:50.940 --> 00:35:55.340]   That's what's so amazing, how flexible American business has been.
[00:35:55.340 --> 00:35:59.100]   Number one, though, you want satisfied customers.
[00:35:59.100 --> 00:36:02.780]   That means you need a platform that performs quickly and efficiently
[00:36:02.780 --> 00:36:07.100]   to meet their needs. That's Salesforce service cloud.
[00:36:07.100 --> 00:36:09.580]   The world's number one customer service platform,
[00:36:10.140 --> 00:36:15.180]   empowering organizations of all sizes to deliver service anywhere, anytime.
[00:36:15.180 --> 00:36:19.980]   It runs out of the box without any customization,
[00:36:19.980 --> 00:36:21.660]   but the sky's the limit.
[00:36:21.660 --> 00:36:23.660]   You can customize it to your heart's content,
[00:36:23.660 --> 00:36:26.540]   so you can make it the best solution for your customers.
[00:36:26.540 --> 00:36:29.020]   Many people just install it, run it as is,
[00:36:29.020 --> 00:36:32.620]   and as they use it, fine tune it to get it closer and closer
[00:36:32.620 --> 00:36:36.380]   to what they need to maintain their customer's relationship.
[00:36:36.380 --> 00:36:38.220]   Service Cloud, out of the box,
[00:36:38.220 --> 00:36:40.940]   allows you to rapidly respond to customer needs.
[00:36:40.940 --> 00:36:45.660]   Wherever those customers are, Facebook, WhatsApp, chat, text messaging,
[00:36:45.660 --> 00:36:49.660]   you could do it from the house, which is good,
[00:36:49.660 --> 00:36:51.740]   because that's where most of us are these days.
[00:36:51.740 --> 00:36:54.220]   You could do it in the field, you could do it in the office,
[00:36:54.220 --> 00:36:58.140]   you could provide instant support with self-service portals for your customers.
[00:36:58.140 --> 00:36:59.180]   Customers love this.
[00:36:59.180 --> 00:37:03.020]   If they can go to a website and get articles, account info,
[00:37:03.020 --> 00:37:04.300]   talk to other community members,
[00:37:04.300 --> 00:37:07.660]   and all of this is built into Salesforce Service Cloud.
[00:37:07.660 --> 00:37:08.700]   It uses AI.
[00:37:08.700 --> 00:37:11.260]   In fact, there's AI-powered chatbots.
[00:37:11.260 --> 00:37:15.340]   You can embed on your website to help resolve customers issues quickly.
[00:37:15.340 --> 00:37:17.900]   They're so good, you won't know.
[00:37:17.900 --> 00:37:19.900]   The customers don't know that it's a chatbot.
[00:37:19.900 --> 00:37:22.060]   They're so good, they get the job done,
[00:37:22.060 --> 00:37:27.660]   and release you and your employees to do the more personal stuff that you need to do too.
[00:37:27.660 --> 00:37:31.900]   Give your service team the tools to provide an unforgettable customer experience.
[00:37:31.900 --> 00:37:33.660]   You'll be resolving cases faster,
[00:37:33.660 --> 00:37:36.860]   using their suite of intelligent productivity tools.
[00:37:36.860 --> 00:37:40.940]   You'll give your agents a complete shared view of every customer and interaction.
[00:37:40.940 --> 00:37:42.940]   They love it, it just comes right up on the screen.
[00:37:42.940 --> 00:37:46.380]   You could personalize every customer conversation.
[00:37:46.380 --> 00:37:50.220]   You can always give your customer exactly the service they need and deserve.
[00:37:50.220 --> 00:37:52.700]   And it's so fast, it's so flexible.
[00:37:52.700 --> 00:37:58.380]   Salesforce Service Cloud lets trailblazers like you provide world-class service anytime,
[00:37:58.380 --> 00:38:02.780]   anywhere, whether it's messaging, chat, phone, self-service, or in-person.
[00:38:02.780 --> 00:38:05.340]   And if you're not yet a trailblazer, here's your chance
[00:38:05.340 --> 00:38:07.340]   to become a Salesforce Trailblazer.
[00:38:07.340 --> 00:38:12.060]   Over 150,000 companies keep their customers happy with Salesforce.
[00:38:12.060 --> 00:38:15.420]   By the way, we're all remote, right?
[00:38:15.420 --> 00:38:22.060]   You may be a mis-dream for some of the great training and events that Salesforce puts together.
[00:38:22.060 --> 00:38:23.740]   There is a virtual event coming up.
[00:38:23.740 --> 00:38:27.260]   We want to invite you to service changemakers.
[00:38:27.260 --> 00:38:27.820]   It's free.
[00:38:27.820 --> 00:38:32.860]   You'll be joining professionals from all over the country for online learning and sharing and
[00:38:32.860 --> 00:38:38.700]   networking. This is going to be a really fun event and a real informative event,
[00:38:38.700 --> 00:38:43.100]   whether you're new to Salesforce or a longtime service cloud user, everyone's welcome.
[00:38:43.100 --> 00:38:46.060]   It's July 28th, but do register right now, don't wait.
[00:38:46.060 --> 00:38:48.940]   Once you register, you'll get more details about it.
[00:38:48.940 --> 00:38:50.060]   And here's the site.
[00:38:50.060 --> 00:38:55.820]   Just go to bitly, bit.ly/service changemakers.
[00:38:55.820 --> 00:38:59.500]   That's a different address than the one you see on the screen.
[00:38:59.500 --> 00:39:00.700]   This is for the event.
[00:39:00.700 --> 00:39:02.700]   This is for the event.
[00:39:02.700 --> 00:39:07.420]   Service changemakers, bit.ly/service changemakers.
[00:39:07.420 --> 00:39:11.020]   And if you want to know more about Salesforce Service Cloud,
[00:39:11.020 --> 00:39:15.580]   you can find out at bit.ly/salesforceforservice.
[00:39:15.580 --> 00:39:21.500]   bit.ly/salesforceforservice.
[00:39:21.500 --> 00:39:25.660]   That's the address to find out how the world's number one service platform can help grow your
[00:39:25.660 --> 00:39:27.500]   business. We're big fans of Salesforce.
[00:39:27.500 --> 00:39:32.620]   And Service Cloud is a really great way for you to provide the service your customers need
[00:39:32.620 --> 00:39:40.380]   want and deserve Salesforce Service, bit.ly/salesforceforservice.
[00:39:40.380 --> 00:39:43.180]   Thank you, Salesforce. Welcome to the Twit family.
[00:39:43.180 --> 00:39:44.140]   We're very excited.
[00:39:44.140 --> 00:39:47.500]   TikTok hysteria.
[00:39:47.500 --> 00:39:50.460]   That's what Mike Masnick calls it at Tech Dirt.
[00:39:50.460 --> 00:39:53.740]   And I'm inclined to kind of go along with him.
[00:39:53.740 --> 00:39:55.180]   The drum beat has started.
[00:39:55.180 --> 00:39:57.740]   It started with Secretary of State Mike Pompeo saying,
[00:39:57.740 --> 00:40:02.860]   we're looking at banning TikTok in the United States.
[00:40:02.860 --> 00:40:05.100]   It has been banned in other places.
[00:40:05.100 --> 00:40:09.740]   It's been banned in India along with a lot of other Chinese applications.
[00:40:09.740 --> 00:40:11.260]   Of course, China does the same thing.
[00:40:11.260 --> 00:40:15.180]   Twitter and Facebook and other social media are banned in China.
[00:40:15.180 --> 00:40:19.580]   But we have a kind of a different style here in the United States of America.
[00:40:19.580 --> 00:40:23.420]   Amazon, remember, sent out that letter saying,
[00:40:23.420 --> 00:40:27.900]   oh, last week, stop using TikTok on your company phone.
[00:40:27.900 --> 00:40:32.940]   And then they retracted it because it turns out they have a lot of relationships with TikTok.
[00:40:32.940 --> 00:40:37.020]   For all we know, they use their Amazon Web Services.
[00:40:37.020 --> 00:40:41.340]   Where do you stand on the TikTok war?
[00:40:41.340 --> 00:40:45.740]   I'll give you one opposite opposing point of view from Mike Masnick's.
[00:40:45.740 --> 00:40:51.100]   Somebody I respect equally, I think is an equally astute observer, Ben Thompson,
[00:40:51.100 --> 00:40:53.660]   his trajectory wrote a long article.
[00:40:53.660 --> 00:40:55.660]   He's never written a short article that I know of.
[00:40:55.660 --> 00:41:03.180]   For a long article, he says, this is a prescription I don't come to lightly.
[00:41:03.180 --> 00:41:08.460]   Perhaps the most powerful argument against taking any sort of action is that we aren't China.
[00:41:08.460 --> 00:41:13.740]   And isn't blocking TikTok something China would do.
[00:41:13.740 --> 00:41:19.180]   But if China is on the offensive against Western liberalism with a lowercase L,
[00:41:19.180 --> 00:41:24.060]   not only within its borders, but within ours, it's in liberalism's interest to cut off a vector
[00:41:24.060 --> 00:41:28.620]   that has taken root precisely because it's so brilliantly engineered to give humans
[00:41:28.620 --> 00:41:29.900]   exactly what they want.
[00:41:29.900 --> 00:41:37.740]   He says, I would prefer Congress take the lead, but I think Apple and Google should ban TikTok,
[00:41:37.740 --> 00:41:45.180]   remove it from the App Store, and make it unusable if it's already installed on a device.
[00:41:45.180 --> 00:41:46.860]   So that's the extreme opposite.
[00:41:47.660 --> 00:41:52.300]   Mike Masnick says, TikTok's never demonstrably done anything wrong.
[00:41:52.300 --> 00:41:55.900]   This is xenophobia at its worst.
[00:41:55.900 --> 00:41:58.460]   All right, Alex, where do you stand?
[00:41:58.460 --> 00:41:59.980]   I'll start with Alex.
[00:41:59.980 --> 00:42:00.620]   We'll move around.
[00:42:00.620 --> 00:42:02.860]   So a couple of things.
[00:42:02.860 --> 00:42:07.900]   One, the conard that TikTok collects too much information about users is silly.
[00:42:07.900 --> 00:42:11.420]   Trump has been talking about this some and beyond that many social applications are
[00:42:11.420 --> 00:42:13.340]   essentially just vacuums for your information.
[00:42:13.340 --> 00:42:15.740]   Speasimally, bother me one way or the other.
[00:42:15.740 --> 00:42:19.500]   One thing that does though is the opacity of the TikTok algorithm.
[00:42:19.500 --> 00:42:23.740]   Now, we all know that TikTok has become very popular because it's an amazingly
[00:42:23.740 --> 00:42:28.220]   addictive thing to use because it just feeds you more and more stuff and then you watch it.
[00:42:28.220 --> 00:42:32.860]   Without really knowing how that recommendation engine works, we don't really know if they're
[00:42:32.860 --> 00:42:34.140]   pushing any particular narrative.
[00:42:34.140 --> 00:42:37.660]   We do know that there has been some rules in place on TikTok back in the day that were
[00:42:37.660 --> 00:42:42.140]   then exposed and then taken down that didn't show off people that were heavier in size and
[00:42:42.140 --> 00:42:43.980]   weren't a certain color and so forth.
[00:42:43.980 --> 00:42:46.620]   And so there's always been some issues with how TikTok operates.
[00:42:46.620 --> 00:42:51.340]   So if TikTok doesn't want to get banned, talking very publicly about how it's algorithm works,
[00:42:51.340 --> 00:42:53.180]   I think would be an intelligent thing to do.
[00:42:53.180 --> 00:42:58.540]   I'm hesitant to say ban it today, but I'm certainly sympathetic to the arguments that
[00:42:58.540 --> 00:42:59.420]   are in favor of it.
[00:42:59.420 --> 00:43:01.660]   I'm a little bit less than Ben, but more than Mike.
[00:43:01.660 --> 00:43:02.620]   I think it's where I land on this one.
[00:43:02.620 --> 00:43:02.620]   Yeah.
[00:43:02.620 --> 00:43:07.180]   And I think for a company like Wells Fargo, which it has to say, you can't use TikTok in your company
[00:43:07.180 --> 00:43:10.460]   phone is completely reasonable for a lot of reasons.
[00:43:10.460 --> 00:43:14.780]   For one thing, you don't want the teller behind the camera to be sending TikToks.
[00:43:14.780 --> 00:43:16.380]   But I think that that's reasonable.
[00:43:16.380 --> 00:43:21.900]   But whether I don't think there's would be any precedent for the United States government
[00:43:21.900 --> 00:43:25.500]   to say, we are going to prevent a Chinese app from being used, right, Mike?
[00:43:25.500 --> 00:43:27.020]   I don't think we've ever done that.
[00:43:27.020 --> 00:43:29.340]   I don't know if there's a precedent or not.
[00:43:29.340 --> 00:43:35.660]   I know that they have been apps banned because they've demonstrated some nefarious activity
[00:43:35.660 --> 00:43:36.700]   on apps.
[00:43:36.700 --> 00:43:36.700]   Yeah.
[00:43:36.700 --> 00:43:37.700]   Right.
[00:43:37.700 --> 00:43:43.540]   I think that so I will take the devil's advocate position sort of in the more extreme version
[00:43:43.540 --> 00:43:44.540]   of Ben.
[00:43:44.540 --> 00:43:47.380]   And I think first let's characterize what Ben pointed out, which I think he's right on
[00:43:47.380 --> 00:43:48.380]   everything.
[00:43:48.380 --> 00:43:53.020]   He points out China's national intelligence law, which means that by law, Chinese law,
[00:43:53.020 --> 00:43:56.900]   any Chinese company has to hand over any data they ask for.
[00:43:56.900 --> 00:43:58.940]   No questions asked.
[00:43:58.940 --> 00:44:02.540]   But I'm not concerned like Alex, I'm not concerned about the data they're collecting.
[00:44:02.540 --> 00:44:05.500]   Yeah, they could figure out where secret bases are.
[00:44:05.500 --> 00:44:08.460]   They could harvest it for intelligence.
[00:44:08.460 --> 00:44:10.580]   I don't think that's really the real concern.
[00:44:10.580 --> 00:44:15.540]   The real concern is what Ben Thompson highlighted, which is that TikTok is an amazing tool of
[00:44:15.540 --> 00:44:16.540]   propaganda.
[00:44:16.540 --> 00:44:22.300]   Here's an example that Ben trotted out, which is that so we all know that President Trump
[00:44:22.300 --> 00:44:25.100]   had a rally that didn't have the turnout he wanted.
[00:44:25.100 --> 00:44:28.900]   And we found out later that the initially had a million signups for the rally and then
[00:44:28.900 --> 00:44:31.220]   like 6,000 people showed up, something like that.
[00:44:31.220 --> 00:44:35.980]   And it turned out that a bunch of TikTok users were doing this grassroots effort to fake
[00:44:35.980 --> 00:44:37.780]   signups.
[00:44:37.780 --> 00:44:40.300]   TikTok and K-pop stands.
[00:44:40.300 --> 00:44:41.300]   Exactly.
[00:44:41.300 --> 00:44:43.380]   That was a good week.
[00:44:43.380 --> 00:44:51.620]   To what extent did TikTok, the company, facilitate the algorithmic growth of that movement?
[00:44:51.620 --> 00:44:56.580]   Yeah, because the woman who posted that first on TikTok, that post immediately, algorithmically
[00:44:56.580 --> 00:44:59.660]   got pushed to the tops that everybody saw it.
[00:44:59.660 --> 00:45:02.020]   Yeah, and so it's a little suspicious.
[00:45:02.020 --> 00:45:04.060]   It's not about Trump.
[00:45:04.060 --> 00:45:09.020]   It's about a Chinese company influencing American politics, which China has demonstrated as
[00:45:09.020 --> 00:45:12.100]   Ben points out, a strong willingness to do.
[00:45:12.100 --> 00:45:14.260]   China tries to censor globally.
[00:45:14.260 --> 00:45:16.660]   They try to control dissidents globally.
[00:45:16.660 --> 00:45:20.300]   They're increasingly meddling in other countries' activities.
[00:45:20.300 --> 00:45:23.220]   And TikTok is the fastest growing social network.
[00:45:23.220 --> 00:45:27.260]   If it is a social network, Ben Thompson says it isn't ever.
[00:45:27.260 --> 00:45:28.620]   And so here's another one.
[00:45:28.620 --> 00:45:31.660]   Chinese found evidence that they are sensing Hong Kong protests.
[00:45:31.660 --> 00:45:34.180]   Okay, that's sort of in Chinese, China, Ballywerk.
[00:45:34.180 --> 00:45:36.820]   They've been censoring the Houston Rockets.
[00:45:36.820 --> 00:45:43.420]   Okay, they're using TikTok to meddle in the world's perception about China, and this
[00:45:43.420 --> 00:45:45.460]   is an increasing trend.
[00:45:45.460 --> 00:45:52.500]   So his remedy is that TikTok be spun out into a non-Chinese company that does not beholden
[00:45:52.500 --> 00:45:55.100]   to the Chinese or beholden Chinese law and stuff like that.
[00:45:55.100 --> 00:45:58.940]   I would argue with that as well, argue in favor of that as well.
[00:45:58.940 --> 00:46:01.020]   But here's the problem, Aleo.
[00:46:01.020 --> 00:46:06.220]   So some network is going to be the global network that everybody in the world uses.
[00:46:06.220 --> 00:46:12.060]   And by banning all foreign social networks, China has made it a matter of law that the
[00:46:12.060 --> 00:46:16.860]   only possible global network must be a Chinese social network.
[00:46:16.860 --> 00:46:19.220]   It has to be Chinese, right?
[00:46:19.220 --> 00:46:22.260]   So Facebook can never be global because it's banned in China.
[00:46:22.260 --> 00:46:26.020]   But TikTok can be global because it's not banned in the United States.
[00:46:26.020 --> 00:46:29.060]   And for that reason, I think we should ban it.
[00:46:29.060 --> 00:46:34.780]   If China wants TikTok un-banned in the United States, then they can un-banned Facebook in
[00:46:34.780 --> 00:46:36.100]   China.
[00:46:36.100 --> 00:46:40.500]   Should mention the reason TikTok blocks the Houston Rockets is because their general
[00:46:40.500 --> 00:46:47.260]   manager had the temerity to tweet in support of Hong Kong protesters, at which point China
[00:46:47.260 --> 00:46:51.180]   said we're going to boycott the NBA, which makes a lot of money in China.
[00:46:51.180 --> 00:46:52.660]   The NBA's lots of money.
[00:46:52.660 --> 00:46:54.420]   Very big in China.
[00:46:54.420 --> 00:47:00.180]   At one point, the NBA said we could lose $400 million due to a Chinese boycott over this
[00:47:00.180 --> 00:47:03.340]   single tweet from the GM.
[00:47:03.340 --> 00:47:07.260]   But that's really a China problem, more than a TikTok problem.
[00:47:07.260 --> 00:47:10.540]   Well, the devil's in the details.
[00:47:10.540 --> 00:47:16.460]   So for example, the Chinese government is currently in a tussle with the government of
[00:47:16.460 --> 00:47:21.060]   Australia because government Australia wants to launch an inquiry into the origins of the
[00:47:21.060 --> 00:47:22.060]   coronavirus.
[00:47:22.060 --> 00:47:28.500]   How did they let it, did they cover up the spread of it, early days, etc, etc.
[00:47:28.500 --> 00:47:31.460]   And Australia absolutely has the right to do this.
[00:47:31.460 --> 00:47:35.420]   Australians died and are still dying from the coronavirus and they have the right to figure
[00:47:35.420 --> 00:47:36.620]   out what happened.
[00:47:36.620 --> 00:47:40.180]   And China is basically saying we're going to boycott Australia.
[00:47:40.180 --> 00:47:44.660]   We're going to have sanctions on you if you don't stop investigating.
[00:47:44.660 --> 00:47:52.660]   And TikTok could be a perfect way to sway Australian public opinion against the Australian
[00:47:52.660 --> 00:47:56.660]   government and in favor of the Chinese government in this sort of political war.
[00:47:56.660 --> 00:48:02.620]   This is the advantage that non-democratic countries like China have has over democratic
[00:48:02.620 --> 00:48:04.660]   countries like Australia.
[00:48:04.660 --> 00:48:10.860]   If you sway public opinion in China, it doesn't do you any good because it's a non-democratic
[00:48:10.860 --> 00:48:17.380]   country, but in Australia, you can sway public opinion in a way that favors Beijing's policies.
[00:48:17.380 --> 00:48:19.740]   And so that should be a concern to everyone.
[00:48:19.740 --> 00:48:21.380]   That's a really interesting point.
[00:48:21.380 --> 00:48:22.820]   I just want to say I'll get you.
[00:48:22.820 --> 00:48:28.100]   That's a really interesting point, a much more subtle point than the one the US government's
[00:48:28.100 --> 00:48:29.260]   driving home.
[00:48:29.260 --> 00:48:35.860]   The Trump campaign has actually been buying ads on Facebook, urging Facebook members to
[00:48:35.860 --> 00:48:42.420]   sign a petition banning TikTok saying the ad say TikTok has been caught red-handed by
[00:48:42.420 --> 00:48:44.940]   monitoring what's on your phone's clipboard.
[00:48:44.940 --> 00:48:49.980]   This of course is because in iOS 14, you see the little pop-up whenever anything looks
[00:48:49.980 --> 00:48:50.980]   at your clipboard.
[00:48:50.980 --> 00:48:55.460]   So you're in TikTok and you get these pop-ups every time you type a character.
[00:48:55.460 --> 00:49:00.260]   TikTok says, "Yeah, we did that because if there was a URL on your clipboard and you
[00:49:00.260 --> 00:49:04.060]   were posting, we wanted you to be able to quickly paste it in.
[00:49:04.060 --> 00:49:05.060]   We stopped doing that.
[00:49:05.060 --> 00:49:08.940]   By the way, Reddit did the same thing for the same reasons.
[00:49:08.940 --> 00:49:14.620]   A lot of different apps did that, including Microsoft's LinkedIn app.
[00:49:14.620 --> 00:49:20.940]   So I think it's a little bit disingenuous for this campaign, but they are trying to get
[00:49:20.940 --> 00:49:23.740]   people to sign a petition to ban TikTok.
[00:49:23.740 --> 00:49:28.420]   I think that's the president ginning up the usual xenophobia rather than an actual fear
[00:49:28.420 --> 00:49:29.420]   of TikTok.
[00:49:29.420 --> 00:49:31.860]   But I think what you said though is very subtle.
[00:49:31.860 --> 00:49:34.180]   That's not the TikTok spying on you.
[00:49:34.180 --> 00:49:38.060]   Like Masic's funny, he says, "Well, you got a bunch of crappy IoT devices from China,
[00:49:38.060 --> 00:49:39.060]   also spying on you."
[00:49:39.060 --> 00:49:41.660]   That's the least of the words.
[00:49:41.660 --> 00:49:46.260]   And what is China going to get if they spy on my TikTok?
[00:49:46.260 --> 00:49:48.700]   So Seth, you're the tiebreaker.
[00:49:48.700 --> 00:49:52.820]   You've got one for and one against.
[00:49:52.820 --> 00:49:55.260]   I'll say this.
[00:49:55.260 --> 00:50:07.500]   Anytime that any major government gets involved in banning technology or banning apps tends
[00:50:07.500 --> 00:50:11.500]   to go awry 10 years, 15 years down the line.
[00:50:11.500 --> 00:50:18.980]   So I'm very hesitant to see something like that for those reasons simply because it sets
[00:50:18.980 --> 00:50:25.500]   a very uncomfortable precedent as to what Canada cannot be allowed by a government.
[00:50:25.500 --> 00:50:32.860]   But I think that my kid on something really clever that hasn't quite been highlighted,
[00:50:32.860 --> 00:50:34.260]   which is TikTok itself.
[00:50:34.260 --> 00:50:36.500]   I mean, who really cares?
[00:50:36.500 --> 00:50:48.900]   The real issue I think is that in the way that we have yet to figure out how to manage
[00:50:48.900 --> 00:50:54.460]   a soft diplomacy in the era of global tech.
[00:50:54.460 --> 00:51:02.340]   And we don't really have a good way of dealing with countries, not just China, but say Russia
[00:51:02.340 --> 00:51:11.460]   or Iran or others, that do engage in technology in a way that we don't like.
[00:51:11.460 --> 00:51:20.500]   TikTok itself as an issue of controlling the message isn't too different, I think, from
[00:51:20.500 --> 00:51:24.460]   Voice of America on radio.
[00:51:24.460 --> 00:51:31.100]   But how are we, as a country, going to influence other countries and their use of technology?
[00:51:31.100 --> 00:51:35.900]   Is it just allowing Facebook and Google and Apple and whomever to go do business wherever
[00:51:35.900 --> 00:51:37.300]   they want?
[00:51:37.300 --> 00:51:44.780]   Or do we need to be a lot more thoughtful about how we do interact with the rest of the world?
[00:51:44.780 --> 00:51:52.740]   And I think TikTok, by and large, and how they behave is stuff that we really don't
[00:51:52.740 --> 00:51:57.380]   see any different from coming from any other country.
[00:51:57.380 --> 00:52:05.500]   I don't think if you go searching on what is it, Yandex, a Russian search engine, you're
[00:52:05.500 --> 00:52:10.380]   going to find Russian propaganda up at the top.
[00:52:10.380 --> 00:52:15.860]   I think that we need to be a lot more thoughtful about how we're going about things.
[00:52:15.860 --> 00:52:21.620]   And the US, for the past three and a half years, has completely seeded any interest in leading
[00:52:21.620 --> 00:52:23.500]   or guiding the rest of the world.
[00:52:23.500 --> 00:52:25.860]   And that's where the real problem is.
[00:52:25.860 --> 00:52:34.420]   It's also the case that whenever we're threatened by an authoritarian regime like China, they
[00:52:34.420 --> 00:52:38.900]   have advantages against an open nation like ours.
[00:52:38.900 --> 00:52:42.700]   And there's always the temptation to say, well, we should be less open so that we can
[00:52:42.700 --> 00:52:47.780]   fight against those that would undermine us.
[00:52:47.780 --> 00:52:49.380]   That's always the temptation.
[00:52:49.380 --> 00:52:56.580]   It's always the wrong way to go because we preserve our integrity and our authority as
[00:52:56.580 --> 00:53:02.980]   a nation by being open and democratic and not doing things like banning TikTok.
[00:53:02.980 --> 00:53:09.220]   Even though it presents a problem for us, we'd prefer to go that way than just to fight
[00:53:09.220 --> 00:53:12.660]   fire with fire, to become equally autocratic.
[00:53:12.660 --> 00:53:17.580]   Well, it would be very interesting to see a situation where the US government said, we'll
[00:53:17.580 --> 00:53:20.980]   allow TikTok as soon as Facebook is allowed into China.
[00:53:20.980 --> 00:53:25.780]   I mean, I don't know that that would be a good idea, but that would be certainly interesting
[00:53:25.780 --> 00:53:26.780]   to watch it.
[00:53:26.780 --> 00:53:27.780]   I could see that happening.
[00:53:27.780 --> 00:53:30.180]   Well, we're trying to say screw you.
[00:53:30.180 --> 00:53:31.180]   Right.
[00:53:31.180 --> 00:53:32.180]   It doesn't change.
[00:53:32.180 --> 00:53:34.660]   We just have a ban on TikTok in the United States.
[00:53:34.660 --> 00:53:35.660]   Right.
[00:53:35.660 --> 00:53:42.580]   No, but if the US does that and also gets, say, England, Germany, Australia, other places
[00:53:42.580 --> 00:53:50.620]   where TikTok wants to be big on board, you could find, I think, and this is what I was
[00:53:50.620 --> 00:53:55.980]   getting at when I was talking about diplomacy, you can get ways of pressuring countries that
[00:53:55.980 --> 00:53:59.340]   otherwise they wouldn't do.
[00:53:59.340 --> 00:54:04.900]   It's not too different, I think, from how Russia refuses to extradite any of its cyber
[00:54:04.900 --> 00:54:05.900]   criminals.
[00:54:05.900 --> 00:54:10.940]   We know that there are people hiding in Russia who are responsible for...
[00:54:10.940 --> 00:54:15.300]   Mueller, Mueller, Mueller, they decided 20 or 30 Russians for medical election.
[00:54:15.300 --> 00:54:17.060]   Yeah, I think it was a dozen.
[00:54:17.060 --> 00:54:18.060]   Yeah, whatever.
[00:54:18.060 --> 00:54:19.060]   We'll never see those guys.
[00:54:19.060 --> 00:54:20.060]   Yeah.
[00:54:20.060 --> 00:54:21.060]   No, exactly.
[00:54:21.060 --> 00:54:27.460]   And if there was a way to get other countries to pressure Russia, maybe Russia...
[00:54:27.460 --> 00:54:29.740]   I don't know, I mean, we could speculate for hours.
[00:54:29.740 --> 00:54:33.900]   But the point being is that without that kind of diplomacy, we're leaving, I think, a very
[00:54:33.900 --> 00:54:39.020]   crucial and important negotiating tool in the toolbox.
[00:54:39.020 --> 00:54:42.540]   And it needs to be used and implemented.
[00:54:42.540 --> 00:54:50.940]   This kind of freaking out because of nonsense over the clipboard is...
[00:54:50.940 --> 00:54:53.540]   We can do the argument at the end of the day.
[00:54:53.540 --> 00:54:54.540]   Mueller and then...
[00:54:54.540 --> 00:54:55.540]   Well, a lot of money is just...
[00:54:55.540 --> 00:54:56.540]   Mueller and then...
[00:54:56.540 --> 00:54:57.540]   Let's get the number right.
[00:54:57.540 --> 00:55:02.580]   26 Russian nationals, three Russian companies in his indictments.
[00:55:02.580 --> 00:55:05.700]   And of course, you'll never see any of them.
[00:55:05.700 --> 00:55:06.700]   Right.
[00:55:06.700 --> 00:55:07.700]   Yeah.
[00:55:07.700 --> 00:55:14.100]   The freaking out over clipboards and the freaking out over the spy capabilities is just people
[00:55:14.100 --> 00:55:18.260]   who are non-technical and who don't understand it, who are in Washington, for example.
[00:55:18.260 --> 00:55:20.620]   And I don't really don't trust the Trump administration not to...
[00:55:20.620 --> 00:55:21.820]   I don't think they understand it.
[00:55:21.820 --> 00:55:25.540]   I think they're just jumping on it because it's a good story, right?
[00:55:25.540 --> 00:55:26.380]   Convenient.
[00:55:26.380 --> 00:55:27.380]   Sounds good.
[00:55:27.380 --> 00:55:33.060]   But I think the largest problem here, the big, big Uber super problem here is that our impulses
[00:55:33.060 --> 00:55:38.140]   are based on a world where free speech is where publications or individuals say things
[00:55:38.140 --> 00:55:42.860]   in public and then we all look at what they're saying and we all make our decisions and it's
[00:55:42.860 --> 00:55:44.860]   the free market of ideas and all that kind of stuff.
[00:55:44.860 --> 00:55:47.260]   That's our impulses are based on that world.
[00:55:47.260 --> 00:55:49.660]   But what we're talking about here is something entirely different.
[00:55:49.660 --> 00:55:54.180]   The world of secret algorithms where each individual user can be fed a different message
[00:55:54.180 --> 00:55:56.180]   based on data that's been...
[00:55:56.180 --> 00:55:57.180]   It's true.
[00:55:57.180 --> 00:55:58.180]   It's extracted over years.
[00:55:58.180 --> 00:55:59.180]   But that's happening on Facebook.
[00:55:59.180 --> 00:56:00.180]   Yeah.
[00:56:00.180 --> 00:56:01.180]   It's happening every other person.
[00:56:01.180 --> 00:56:02.180]   Facebook, they're people.
[00:56:02.180 --> 00:56:03.180]   Yeah.
[00:56:03.180 --> 00:56:10.700]   Yes, but that's the larger problem that our laws don't adequately deal with our diplomacy,
[00:56:10.700 --> 00:56:12.140]   our policies.
[00:56:12.140 --> 00:56:15.900]   We basically have all these instincts that are based on another world and then we have
[00:56:15.900 --> 00:56:20.620]   the problems being caused by a new phenomenon that is...
[00:56:20.620 --> 00:56:25.260]   It's far more damaging and difficult of a problem than just the old arguments about
[00:56:25.260 --> 00:56:27.900]   free speech versus no free speech, etc.
[00:56:27.900 --> 00:56:33.620]   And when a foreign government can tweak the algorithms and speak directly, privately,
[00:56:33.620 --> 00:56:35.620]   into the ears of millions of idiosyncrasies...
[00:56:35.620 --> 00:56:36.620]   Interesting.
[00:56:36.620 --> 00:56:37.620]   I agree.
[00:56:37.620 --> 00:56:41.540]   That's a different thing than the voice of America where it's just a news report saying
[00:56:41.540 --> 00:56:43.660]   and everybody can see it.
[00:56:43.660 --> 00:56:50.620]   I think the problem is we have to figure out how to deal with this algorithmically generated
[00:56:50.620 --> 00:56:54.780]   world and to a certain extent my bias would be in favor of just saying, "You know what?
[00:56:54.780 --> 00:57:00.580]   This book or TikTok or whatever, no more algorithms until we figure this out."
[00:57:00.580 --> 00:57:01.580]   Because these algorithms are...
[00:57:01.580 --> 00:57:05.540]   I mean, this is what probably determined the election to those things.
[00:57:05.540 --> 00:57:07.180]   Well, would you do that by government fiat?
[00:57:07.180 --> 00:57:08.180]   You can't...
[00:57:08.180 --> 00:57:10.100]   The government's not going to say that or should they?
[00:57:10.100 --> 00:57:11.100]   You can't ban an algorithm.
[00:57:11.100 --> 00:57:14.180]   I think you've got a law saying that more...
[00:57:14.180 --> 00:57:16.860]   You can ban secrecy about an algorithm.
[00:57:16.860 --> 00:57:18.460]   I think there has to be more transparency.
[00:57:18.460 --> 00:57:22.060]   But then you've got to get...
[00:57:22.060 --> 00:57:25.500]   But then you've got to get Facebook and Twitter to open up their algorithms and good luck
[00:57:25.500 --> 00:57:26.500]   with that.
[00:57:26.500 --> 00:57:27.500]   Absolutely.
[00:57:27.500 --> 00:57:28.740]   Which I would like...
[00:57:28.740 --> 00:57:38.140]   I'd love to see independent audits of all kinds of social media and other tech algorithms
[00:57:38.140 --> 00:57:44.380]   because the problems that we've had with these algorithms, especially concerning facial recognition
[00:57:44.380 --> 00:57:49.100]   and other issues, has come about because nobody is auditing them or they're only being edited
[00:57:49.100 --> 00:57:50.100]   internally.
[00:57:50.100 --> 00:57:51.100]   We can...
[00:57:51.100 --> 00:57:56.740]   I mean, that's one of the things we talk about and have been for years on this show.
[00:57:56.740 --> 00:57:59.420]   You know, the YouTube recommendation engine.
[00:57:59.420 --> 00:58:04.300]   Clearly the number one problem on YouTube, the Facebook news feed algorithm.
[00:58:04.300 --> 00:58:06.660]   Clearly the number one problem on Facebook.
[00:58:06.660 --> 00:58:10.700]   But good luck getting those companies to abandon them because those are money makers.
[00:58:10.700 --> 00:58:11.940]   That's their secret sauce.
[00:58:11.940 --> 00:58:17.700]   That's like saying, "Congress is going to force Coca-Cola to give up its secret recipe."
[00:58:17.700 --> 00:58:20.020]   This is the thing that bothers me.
[00:58:20.020 --> 00:58:22.580]   Democracies are being compromised.
[00:58:22.580 --> 00:58:28.620]   But what's sacrosanct is that Mark Zuckerberg has to make billions and billions and billions
[00:58:28.620 --> 00:58:29.940]   and billions of dollars every quarter.
[00:58:29.940 --> 00:58:31.620]   That must be preserved.
[00:58:31.620 --> 00:58:37.940]   He must have the right to destroy democracy for his own personal profit.
[00:58:37.940 --> 00:58:39.260]   Why do we defend that?
[00:58:39.260 --> 00:58:40.820]   Yeah, no, I agree with you.
[00:58:40.820 --> 00:58:43.020]   The first thing we should defend is our democracy.
[00:58:43.020 --> 00:58:44.020]   Right.
[00:58:44.020 --> 00:58:45.020]   Yes.
[00:58:45.020 --> 00:58:46.020]   But like...
[00:58:46.020 --> 00:58:47.540]   It's per se, it's often fatalism.
[00:58:47.540 --> 00:58:50.460]   I don't think anyone here is saying that the world wouldn't be better if the algorithms
[00:58:50.460 --> 00:58:51.460]   weren't available.
[00:58:51.460 --> 00:58:52.460]   It would be.
[00:58:52.460 --> 00:58:53.460]   We agree if we could see them.
[00:58:53.460 --> 00:58:56.580]   We're saying that with Congress, except the donations as it does from corporations, we're
[00:58:56.580 --> 00:58:58.420]   not going to get it.
[00:58:58.420 --> 00:59:01.260]   The people in Congress are not going to be in favor of what you're saying.
[00:59:01.260 --> 00:59:05.140]   My question is, in a world where, as Seth pointed out, there's no diplomacy, no collective
[00:59:05.140 --> 00:59:08.980]   action, in a world like where we don't have the ability to force Facebook to do this because
[00:59:08.980 --> 00:59:12.100]   Congress is useless in the present and idiot, what the hell do we do?
[00:59:12.100 --> 00:59:15.860]   I want to know what's an actual step we can take that's more constructive because I
[00:59:15.860 --> 00:59:17.620]   agree with you guys both in principle.
[00:59:17.620 --> 00:59:19.460]   I just don't see a way forward from those two big ones.
[00:59:19.460 --> 00:59:20.460]   No, I think you're right.
[00:59:20.460 --> 00:59:21.460]   In a world we do that.
[00:59:21.460 --> 00:59:24.020]   But what's the pragmatic solution?
[00:59:24.020 --> 00:59:30.380]   We all advocate, we can't choose to not advocate things because the Senate is filled with bozos.
[00:59:30.380 --> 00:59:32.580]   We have to advocate them anyway.
[00:59:32.580 --> 00:59:33.580]   Right.
[00:59:33.580 --> 00:59:34.580]   Let's advocate them.
[00:59:34.580 --> 00:59:35.580]   Let's advocate them.
[00:59:35.580 --> 00:59:38.260]   Is there anything short of that that we could do now?
[00:59:38.260 --> 00:59:43.980]   Well, I mean, certainly, I think, you know, taking the corporate approach, which, you know,
[00:59:43.980 --> 00:59:51.140]   sort of where we're at in America in the year of our Lord 2020 and pressuring companies
[00:59:51.140 --> 00:59:58.780]   to ban apps that the government feels are risky on corporate devices is one way to do
[00:59:58.780 --> 00:59:59.780]   it.
[00:59:59.780 --> 01:00:04.140]   And especially because so much, even pre-COVID was done on your own devices.
[01:00:04.140 --> 01:00:08.420]   So much of people's work was done on devices that they had to pay for, but also had to
[01:00:08.420 --> 01:00:14.860]   be tied to work accounts and had, you know, work restrictions placed on how those devices
[01:00:14.860 --> 01:00:17.140]   were used.
[01:00:17.140 --> 01:00:24.020]   I don't think that you could, you would encounter a ton of problems in banning apps that way,
[01:00:24.020 --> 01:00:25.020]   going through corporations.
[01:00:25.020 --> 01:00:26.020]   Would it be okay?
[01:00:26.020 --> 01:00:27.020]   Okay.
[01:00:27.020 --> 01:00:28.020]   People off.
[01:00:28.020 --> 01:00:29.020]   Yeah.
[01:00:29.020 --> 01:00:31.860]   But it would, but it would, but that would send a message.
[01:00:31.860 --> 01:00:36.160]   I, I, and I say this really as a theory, I don't know that it's something that I would
[01:00:36.160 --> 01:00:41.920]   agree with, you know, long term, but, you know, if we're looking for what levers we can
[01:00:41.920 --> 01:00:47.240]   pull, that's one way of getting at it.
[01:00:47.240 --> 01:00:50.500]   Should we have a great firewall of America?
[01:00:50.500 --> 01:00:51.500]   No.
[01:00:51.500 --> 01:00:53.740]   Good Lord, no.
[01:00:53.740 --> 01:00:57.780]   We, I think we should do something.
[01:00:57.780 --> 01:01:02.680]   There's an asymmetric warfare in terms of disinformation that we need to do something
[01:01:02.680 --> 01:01:10.820]   about and probably a great firewall of America is not the solution, but at some point we
[01:01:10.820 --> 01:01:17.460]   really have a grim future if every totalitarian regime in the world can just shape public opinion.
[01:01:17.460 --> 01:01:18.460]   Willie Nilly.
[01:01:18.460 --> 01:01:23.180]   The, the, here's the other thing that, I mean, you know, the one thing is a national ban on
[01:01:23.180 --> 01:01:28.100]   certain types of social networks or the certain types of behaviors of certain social networks,
[01:01:28.100 --> 01:01:33.360]   but there's also institutional bans, as we're seeing with Wells Fargo, et cetera.
[01:01:33.360 --> 01:01:38.120]   Did you know, and I don't see anybody talking about this, did you know that there are lots
[01:01:38.120 --> 01:01:43.920]   and lots of teachers in public schools in the United States who are seriously trying
[01:01:43.920 --> 01:01:47.280]   to figure out how to make tick talk part of their curriculum?
[01:01:47.280 --> 01:01:48.280]   What?
[01:01:48.280 --> 01:01:50.920]   And to use tick talk is that's a bad idea.
[01:01:50.920 --> 01:01:53.560]   But you got to go where the kids are, man.
[01:01:53.560 --> 01:01:55.800]   You got to get that's what they're saying.
[01:01:55.800 --> 01:01:59.200]   It's like, no, no, no, that's a, that's a bad idea.
[01:01:59.200 --> 01:02:02.980]   So I would be in favor of things like no tick talk in schools.
[01:02:02.980 --> 01:02:05.240]   For example, that's fine.
[01:02:05.240 --> 01:02:08.060]   That just makes it more desirable to the kids.
[01:02:08.060 --> 01:02:11.420]   It's forbidden, a forbidden pleasure.
[01:02:11.420 --> 01:02:12.420]   Yeah.
[01:02:12.420 --> 01:02:13.420]   Yeah.
[01:02:13.420 --> 01:02:16.300]   Maybe if the teachers advocate it, they won't want anything to do with it.
[01:02:16.300 --> 01:02:17.300]   Yeah.
[01:02:17.300 --> 01:02:18.540]   That's, maybe the teacher should use it.
[01:02:18.540 --> 01:02:21.940]   That's the way to kill it is say it's now part of the curriculum.
[01:02:21.940 --> 01:02:27.740]   They're going to do this is that every social network eventually goes the way of the boomers.
[01:02:27.740 --> 01:02:31.660]   Like we all used to be on Facebook back before, you know, some people arrived and then now
[01:02:31.660 --> 01:02:33.500]   we're not on Facebook as much as we used to be.
[01:02:33.500 --> 01:02:37.180]   Twitter is showing remarkable resiliency over time, but I'm sure tick talk will also
[01:02:37.180 --> 01:02:40.760]   become taken over by brands and boring people and then it'll go away.
[01:02:40.760 --> 01:02:43.580]   The question is what happens in the short term and I think the impending election gives
[01:02:43.580 --> 01:02:48.380]   this entire conversation a lot more importance because I think we're all very scared about
[01:02:48.380 --> 01:02:50.220]   influence going into this.
[01:02:50.220 --> 01:02:54.300]   But I'll just say there are some politicians who are reasonable on these issues that have
[01:02:54.300 --> 01:02:55.300]   actual thoughts about them.
[01:02:55.300 --> 01:02:58.300]   Like Elizabeth Warren, I think is one of them, not saying that I agree with her entirely,
[01:02:58.300 --> 01:03:01.340]   but I maybe it was slightly too harsh on Congress earlier saying they're all idiots.
[01:03:01.340 --> 01:03:03.220]   I would just now say they're mostly idiots.
[01:03:03.220 --> 01:03:06.820]   But there are some of them who do understand tech issues and have reasonable positions
[01:03:06.820 --> 01:03:07.820]   on them.
[01:03:07.820 --> 01:03:11.900]   And maybe one thing we could do back to concrete steps is elevate those voices to a higher
[01:03:11.900 --> 01:03:14.900]   part of the discourse away from, you know, Trump, Facebook ads or whatever the hell
[01:03:14.900 --> 01:03:15.900]   we're at.
[01:03:15.900 --> 01:03:22.700]   Brock White and another one, one of the difficulties I think in the situation that Congress as
[01:03:22.700 --> 01:03:28.580]   a whole is facing both the House and Senate is that most of the more reasonable voices
[01:03:28.580 --> 01:03:32.660]   on tech are coming only from the Democratic Party.
[01:03:32.660 --> 01:03:39.980]   And when you have tech awareness being a political issue in that way, take for example,
[01:03:39.980 --> 01:03:43.940]   election security and voting machine security.
[01:03:43.940 --> 01:03:48.940]   Then you have an incredible knowledge gap that's also been politicized.
[01:03:48.940 --> 01:03:53.820]   And we've seen what happens there when you politicize masks.
[01:03:53.820 --> 01:04:00.060]   What we don't want are people who look at tech and say, oh, this is a Republican issue
[01:04:00.060 --> 01:04:08.500]   or this is a Democratic issue because then you wind up with a lot less wiser discourse.
[01:04:08.500 --> 01:04:14.660]   I also think that people that what's a good idea versus a bad idea.
[01:04:14.660 --> 01:04:19.140]   It has to be said that back in May, the initial TikTok hysteria in Washington was a clear
[01:04:19.140 --> 01:04:20.140]   bipartisan effort.
[01:04:20.140 --> 01:04:25.100]   I mean, it was both parties coming together to crap all over TikTok.
[01:04:25.100 --> 01:04:28.060]   And so that brings hope.
[01:04:28.060 --> 01:04:30.580]   Let's all come together to crap all over TikTok.
[01:04:30.580 --> 01:04:36.380]   Well, and you could by the way substitute Huawei for TikTok and a lot of this conversation.
[01:04:36.380 --> 01:04:40.780]   And that goes back to the Obama administration and the Department of Commerce.
[01:04:40.780 --> 01:04:48.700]   And I, for all of the presumed problems that Huawei presents, there's never been any real
[01:04:48.700 --> 01:04:49.700]   evidence.
[01:04:49.700 --> 01:04:55.060]   I mean, they're kind of a not a great company, but that they've spied on Americans that their
[01:04:55.060 --> 01:04:56.460]   equipment is spyware.
[01:04:56.460 --> 01:04:59.340]   There's never been any as far as I know, Seth.
[01:04:59.340 --> 01:05:02.740]   Well, I mean, that's the super micro story, right?
[01:05:02.740 --> 01:05:04.900]   Where this incredible story came out.
[01:05:04.900 --> 01:05:09.940]   It seemed, I mean, it falls in line with everyone's paranoia.
[01:05:09.940 --> 01:05:17.420]   But unlike with Twitter this week, where, you know, Vice ran a story and then New York
[01:05:17.420 --> 01:05:23.180]   Times ran a story a few days later confirming independently much of what the Vice story
[01:05:23.180 --> 01:05:27.500]   said, we've never seen a confirmation of super micro.
[01:05:27.500 --> 01:05:35.340]   And I have only seen rumors and supposition when it comes to Huawei.
[01:05:35.340 --> 01:05:40.300]   I'm coming to the conclusion that that Bloomberg super micro story was a plant.
[01:05:40.300 --> 01:05:49.640]   That Bloomberg fell for an NSA plot because most of the sources I suspect were from three
[01:05:49.640 --> 01:05:52.060]   letter agencies.
[01:05:52.060 --> 01:05:56.620]   Without a scribing motive, because who, you know, without talking with the folks, it's
[01:05:56.620 --> 01:05:57.620]   hard to say.
[01:05:57.620 --> 01:06:02.460]   I would presume that it's more an issue of a bunch of people trying to describe an elephant
[01:06:02.460 --> 01:06:08.340]   and just getting, you know, Bloomberg got played, but Bloomberg's never attracted, never
[01:06:08.340 --> 01:06:12.100]   explained and there's never been any evidence presented.
[01:06:12.100 --> 01:06:17.740]   But with Huawei, it would again, it would be not out of the line of reason to say, Oh,
[01:06:17.740 --> 01:06:23.220]   well, if you open up your devices so we can, you know, have them independently audited,
[01:06:23.220 --> 01:06:30.180]   you know, or again, using other forms of soft diplomacy would go, I think, a lot further.
[01:06:30.180 --> 01:06:38.120]   What if by dance, they can't be compelled by C. F. I. S. They could be compelled by C.
[01:06:38.120 --> 01:06:44.940]   F. I. S. The Committee on Foreign Investments in the U. S. to to discourage the musically
[01:06:44.940 --> 01:06:49.060]   acquisition, which was a big part of TikTok success because that was a lot of the lip
[01:06:49.060 --> 01:06:54.620]   thinking stuff. But what if by dance, seeing this as an economic problem said, all right,
[01:06:54.620 --> 01:06:59.220]   we're going to move out of China. We're going to become a Western company. We're just going
[01:06:59.220 --> 01:07:00.220]   to leave.
[01:07:00.220 --> 01:07:03.900]   It would come down to the terms of service to me, like, you know, do they still have that
[01:07:03.900 --> 01:07:08.420]   same thing that says, you know, information may be passed to the parent corporation that
[01:07:08.420 --> 01:07:09.420]   the answer is yes.
[01:07:09.420 --> 01:07:14.900]   I mean, the Chinese government requires that Apple store all the iCloud stuff from Chinese
[01:07:14.900 --> 01:07:20.100]   citizens on Chinese servers where the Chinese government has full access. So we're trying
[01:07:20.100 --> 01:07:21.100]   to crackles doing that.
[01:07:21.100 --> 01:07:26.060]   Yeah, but you're trying to create a firewall between U. S. users will global users and global
[01:07:26.060 --> 01:07:31.620]   data versus Chinese users. I don't think we can help Chinese individuals around the great
[01:07:31.620 --> 01:07:34.540]   firewall of China. I think there's not much we can do there, but we can try to think about
[01:07:34.540 --> 01:07:38.580]   our own borders and our own safety. That's why I think there's distinct issues there,
[01:07:38.580 --> 01:07:39.580]   Leo.
[01:07:39.580 --> 01:07:40.580]   All right.
[01:07:40.580 --> 01:07:46.140]   I think that I think a lot of this conversation gets around is, but hasn't quite hit, is that
[01:07:46.140 --> 01:07:52.860]   we're really talking about what role, what greater role can government play to positively
[01:07:52.860 --> 01:07:58.340]   impact tech? And what we've seen, and there was a story earlier this week on how Cisco
[01:07:58.340 --> 01:08:05.580]   was involved in the creation of China's great firewall. You know, should there be a ban on
[01:08:05.580 --> 01:08:15.260]   US-based companies working with countries that are authoritarian or in other ways are known
[01:08:15.260 --> 01:08:21.460]   to be bad actors in a space? I think this is really serious because we've gone so far
[01:08:21.460 --> 01:08:25.980]   towards the direction of supporting private companies and allowing private enterprise
[01:08:25.980 --> 01:08:33.380]   to rule everything from tech to who pays for our health care that now that we're discussing
[01:08:33.380 --> 01:08:40.300]   what role government can play to help people in these situations and help consumers and
[01:08:40.300 --> 01:08:45.060]   help the international relations, I think is really interesting.
[01:08:45.060 --> 01:08:51.380]   Well, along those lines, there has to be a lot more forethought, and I don't see that
[01:08:51.380 --> 01:08:54.180]   happening until we see changes in Washington.
[01:08:54.180 --> 01:08:59.020]   I think it's precisely forethought that would suggest that we should, in fact, ban Huawei
[01:08:59.020 --> 01:09:04.500]   as a provider of 5G equipment. Simply, for the same reason that the reason we built the
[01:09:04.500 --> 01:09:10.620]   national highway system, the reason Eisenhower really wanted that, was not because the Soviet
[01:09:10.620 --> 01:09:18.620]   Union in the time had demonstrated its attempts and desire to invade the United States, but
[01:09:18.620 --> 01:09:21.860]   that it was a possibility at some point in the future. And if that ever happened, you'd
[01:09:21.860 --> 01:09:25.460]   want to be able to get the military equipment across the country to wherever they were
[01:09:25.460 --> 01:09:34.740]   invading. And likewise, you know, if Huawei, 5G equipment is a firmware update away from
[01:09:34.740 --> 01:09:39.660]   becoming the ultimate spy tool or the ultimate cyber warfare tool.
[01:09:39.660 --> 01:09:44.940]   And so something like 5G equipment is not something that can be left in the hands of
[01:09:44.940 --> 01:09:49.900]   a company that is closely aligned with the Chinese Communist Party. Personally, that's
[01:09:49.900 --> 01:09:53.780]   what I think. And this is exactly what you're talking about, Seth. This is thinking about
[01:09:53.780 --> 01:10:00.300]   the long term. This is thinking from the way, for example, it's a military problem ultimately.
[01:10:00.300 --> 01:10:06.500]   If cyber war, especially if it's combined with actual military activity, is something
[01:10:06.500 --> 01:10:13.180]   that you want to make sure that you are prepared for. And so when it gets to networking equipment
[01:10:13.180 --> 01:10:16.660]   that everybody's relying on, I think that's too sensitive to be left to a company like
[01:10:16.660 --> 01:10:22.700]   Huawei, which, again, has demonstrated bad faith. Everything from stealing, what was
[01:10:22.700 --> 01:10:29.580]   it, T-Mobile's robot arm once to your name? I just saw a long list of transgressions.
[01:10:29.580 --> 01:10:36.420]   And so I would tend to lean. But of course, this is something that the Trump administration
[01:10:36.420 --> 01:10:40.300]   was very much, you know, they were very much against Huawei until it became clear that
[01:10:40.300 --> 01:10:41.300]   it was just a bargaining.
[01:10:41.300 --> 01:10:46.140]   That was just trade-out stuff. Yeah. Although now TSMC, the Taiwan chip maker says, we
[01:10:46.140 --> 01:10:51.100]   know we can no longer sell our chips to Huawei, which is going to have, you know, funny things
[01:10:51.100 --> 01:10:55.180]   Huawei's done economically great. They had a great quarter. Yeah. Because they're going
[01:10:55.180 --> 01:11:01.580]   to be all right. Because in China, people are patriotic, like in Huawei. But Huawei's
[01:11:01.580 --> 01:11:05.540]   great and should be allowed. Huawei phones are great. Yeah, they really are. You know,
[01:11:05.540 --> 01:11:09.660]   all that kind of stuff, I think should be completely allowed. I just think that networking
[01:11:09.660 --> 01:11:14.860]   equipment like 5G base stations from one is a special case. Yeah. And certainly not.
[01:11:14.860 --> 01:11:19.740]   Can you forgot about how 5G helps spread coronavirus? Oh my gosh, that's right. It's
[01:11:19.740 --> 01:11:24.660]   a giant conspiracy. We can't. We can't leave out the important Facebook memes. This is
[01:11:24.660 --> 01:11:29.300]   what the people want. We have to talk about it. Well, by the way, it's all going to be
[01:11:29.300 --> 01:11:35.940]   moot next month because Instagram is going to reveal its new TikTok competitor. Wheels,
[01:11:35.940 --> 01:11:41.580]   Star Wars, and August. That's it. It's it's it's all over for TikTok. Facebook has such
[01:11:41.580 --> 01:11:46.540]   a wonderful track record with succeeding with stolen ideas. Well, but you got to give Instagram
[01:11:46.540 --> 01:11:51.180]   credit. I think while they didn't put Snapchat out of business, they did with Instagram stories
[01:11:51.180 --> 01:11:57.540]   and filters kind of put a damper on Snapchat's growth, didn't they? Oh, no. It's true that
[01:11:57.540 --> 01:12:04.620]   they have had some success. Yeah. Facebook does so much. Yeah. Yeah. But Instagram's,
[01:12:04.620 --> 01:12:09.100]   you know, they've got the they got the Utes on their side. I think we'll be interested
[01:12:09.100 --> 01:12:15.140]   in this. Should we be pushing the kids of America to go to reels and say, you know,
[01:12:15.140 --> 01:12:20.020]   TikTok is not safe. It's Chinese reverse psychology. Tell them the opposite. Yeah. And
[01:12:20.020 --> 01:12:23.380]   then then they'll tell I think I think right now what I'm amazed by is how many TikToks
[01:12:23.380 --> 01:12:27.220]   I get sent in group texts. I have different friend groups around the US from, you know,
[01:12:27.220 --> 01:12:32.580]   I've moved around and people rarely send tweets now. They send mostly TikToks. And
[01:12:32.580 --> 01:12:36.660]   I'm shocked by the the only reason I don't have TikTok on my phone is not because I'm
[01:12:36.660 --> 01:12:40.620]   worried the Chinese government's following me around. I'm sure they are anyway. It's
[01:12:40.620 --> 01:12:45.180]   because it's too goddamn addictive. I spent too addictive. You start it and your hours
[01:12:45.180 --> 01:12:50.460]   later doom scrolling through, except it's the opposite. It's more like joy scrolling
[01:12:50.460 --> 01:12:57.620]   through TikTok. It's so I've never used TikTok. I don't have it on my phone. I don't touch
[01:12:57.620 --> 01:13:03.580]   it. All right. So interested. Well, all right. Let me just show you a couple of TikToks,
[01:13:03.580 --> 01:13:09.260]   Seth. We're going to prop. Okay. What is happening to my eyes? We're going to prop your eyes open
[01:13:09.260 --> 01:13:17.340]   here. And we're going to make you watch some TikTok stuff. This is a good example of a
[01:13:17.340 --> 01:13:23.260]   TikTok example of Chinese propaganda. Yeah. Yeah. This is TikTok. It's all about the shoes.
[01:13:23.260 --> 01:13:31.500]   There's one. There's a there's a monster truck. That's a good that's good. There's a dad working
[01:13:31.500 --> 01:13:38.700]   from home and surfing in his tie across to a zoom meeting across the pool. Here's a dog.
[01:13:38.700 --> 01:13:47.260]   There's a lot of nice pet TikToks. And you're not well, sorry about that. You're not actually
[01:13:47.260 --> 01:13:50.460]   getting the real benefit of because I'm not playing the music because I don't want to get us
[01:13:50.460 --> 01:13:55.820]   taken out. Thank you. But there's a there's music that goes along with it as well.
[01:13:55.820 --> 01:14:03.580]   There's a very popular TikTok where this guy has roped his 80 year old grandma into his TikToks.
[01:14:05.420 --> 01:14:11.420]   Let's see. Well, you get the idea. You get the idea. Yeah. Don't you find that compelling? Don't
[01:14:11.420 --> 01:14:18.060]   you? I mean, like, how is this different from vine? It's but it's fine. It's better than vine.
[01:14:18.060 --> 01:14:24.220]   It's far more algorithmically determined than vine. And it's they just really know how to get the
[01:14:24.220 --> 01:14:29.660]   stuff that makes your eyes are glued to the screen. Well, actually, bummed. And it's I had some
[01:14:29.660 --> 01:14:33.900]   TikToks. I wanted to show you that were on the top of my feed just minutes ago and now they're
[01:14:33.900 --> 01:14:38.540]   they got new stuff because it's constantly adjusted to give her her. You know, actually,
[01:14:38.540 --> 01:14:45.260]   who was it? Ben Thompson, somebody had a really good point comparing TikTok to Quibi because,
[01:14:45.260 --> 01:14:52.220]   of course, this was that very bad doomsday week for stop watching TikTok. I'm talking.
[01:14:52.220 --> 01:14:56.780]   This was that. See, that's the problem. Yeah.
[01:14:59.420 --> 01:15:09.660]   I was watching that person keep falling down. When Quibi first came out three months ago,
[01:15:09.660 --> 01:15:14.700]   they offered a three month subscription. This is the platform. Jeffrey Kadsenberg,
[01:15:14.700 --> 01:15:21.580]   Meg Whitman raised of almost $2 billion, hired big names to make seven minute videos because
[01:15:21.580 --> 01:15:29.500]   that's all we have attention for. Not just phone first, phone only. You couldn't put it on your TV.
[01:15:29.500 --> 01:15:35.900]   You couldn't send it. You just that's all it was. But that three months ran out earlier this month.
[01:15:35.900 --> 01:15:42.140]   And according, I think it was sensor tower of the millions of people who signed up
[01:15:42.140 --> 01:15:47.260]   for Quibi, only 72,000 kept their subscriptions. Ouch.
[01:15:49.740 --> 01:15:53.180]   And so I think it was Ben Thompson who explained what's going on.
[01:15:53.180 --> 01:15:57.180]   And I thought this was very astute. I'll give Ben lots of credit on this one. By the way,
[01:15:57.180 --> 01:16:01.340]   we try to get him on the show all the time. But because of the hour, he's in Taiwan and be very
[01:16:01.340 --> 01:16:07.660]   early morning. He's got the kids. Anyway, it's he can't really do it. But Ben, we love you. We
[01:16:07.660 --> 01:16:12.780]   quote you all the time. He said, Quibi is the Hollywood model, which is that there is a scarcity
[01:16:12.780 --> 01:16:18.700]   of content. So the Hollywood model is we're going to use executives and all this stuff to pick the
[01:16:18.700 --> 01:16:23.260]   best content we can find. Most of it will lose money. Enough of it will be big hits. It'll support
[01:16:23.260 --> 01:16:28.060]   the rest of the economy. That's how Hollywood works TV movies. That's how Hollywood works.
[01:16:28.060 --> 01:16:34.780]   What TikTok and YouTube did is exact opposite everybody throw everything on there. And then
[01:16:34.780 --> 01:16:40.300]   algorithmically, we'll pick the best stuff. There's an infinitude. There's no scarcity of content.
[01:16:40.300 --> 01:16:45.180]   There's an infinitude of content. And our algorithms will surface the best stuff.
[01:16:45.180 --> 01:16:49.260]   And I think that we can see that in the war between the Hollywood model
[01:16:49.260 --> 01:16:54.380]   and the Silicon Valley model, it's very clear Silicon Valley model has completely dominated.
[01:16:54.380 --> 01:16:59.660]   It's like my I always have this conversation with my wife. It's like, what is the better way to take
[01:16:59.660 --> 01:17:04.380]   photos? My wife likes to spend a lot of time setting up the photo, getting the lighting just
[01:17:04.380 --> 01:17:09.660]   right, and then taking a picture. I like to take 5,000 pictures. And just by sheer chance,
[01:17:09.660 --> 01:17:13.020]   one of them is going to be pretty good. That's the TikTok model, basically.
[01:17:13.020 --> 01:17:18.860]   Yeah. When I was learning to take photos, that was in freshman year of high school. That was
[01:17:18.860 --> 01:17:27.340]   one of our projects. Go out, take a point and shoot camera, a disposable one, and shoot three
[01:17:27.340 --> 01:17:33.660]   roles worth of film on it, and buy three of them, I guess, and shoot them. Develop them. And the class
[01:17:33.660 --> 01:17:42.780]   will talk about your photos. And sure enough, one per 36 shots was good. That's called spray and
[01:17:42.780 --> 01:17:49.340]   pray. Yeah, right. There's a lot of praying on those clips from TikTok that you showed me.
[01:17:49.340 --> 01:17:52.620]   Yeah, you spray and you pray. I still don't get it. I still don't get it.
[01:17:52.620 --> 01:18:00.380]   Oh, just put it on there and try it. He'll never stop. I also killed Instagram for the same reason,
[01:18:00.380 --> 01:18:05.420]   but also because the ads were so effective that I kept buying things in the middle of the night.
[01:18:05.420 --> 01:18:10.940]   The ads are so good on Instagram. Reddit. Oh my God. Oh, no. You know, Seth,
[01:18:10.940 --> 01:18:15.500]   Seth, you're in San Francisco, right? So you're in California. There are legal things you can
[01:18:15.500 --> 01:18:19.340]   purchase that would help you understand TikTok that you can consume. I'm just saying, like what?
[01:18:19.340 --> 01:18:25.740]   Oh, that kind of thing. Like, you can go down the dispensary, pick up. Yeah.
[01:18:25.740 --> 01:18:28.860]   Yeah. And that's a family show. So I'm being a bleak, but I'm like, it'll help.
[01:18:28.860 --> 01:18:32.860]   I don't have those things. I mean, I grew up here, you know,
[01:18:32.860 --> 01:18:39.820]   this, I am very familiar with these things. You have to have that you struggle with these
[01:18:39.820 --> 01:18:43.740]   things in Rhode Island. Here's an Instagram purchase. It's not legal in Rhode Island.
[01:18:43.740 --> 01:18:48.300]   It's legal in Massachusetts, which is about four minutes away. Yeah, it's not very far. Yeah.
[01:18:48.300 --> 01:18:57.100]   That's the beauty of Rhode Island. I never lived in Rhode Island. I'm aware how close you are to
[01:18:57.100 --> 01:19:04.300]   someplace Apple. As I grew up in Providence, once I realized as an adult that getting a Boston is
[01:19:04.300 --> 01:19:10.380]   like nothing. I got mad at my father. Why didn't he was a big baseball fan? And he would always say,
[01:19:10.380 --> 01:19:16.060]   Oh, we can't go to the Red Sox. The games is too far away. And it's like, Dad, I drive farther
[01:19:16.060 --> 01:19:22.380]   and go to the city. It's crazy. This is this is this is called the hygiene.
[01:19:22.380 --> 01:19:27.260]   I wanted those things. I've seen them in ads and I just roll past.
[01:19:27.260 --> 01:19:31.340]   Yes. You don't watch enough. You don't do enough Instagram. You see, I can hook on. I can.
[01:19:33.020 --> 01:19:39.340]   Leo, what is this? It's called a hygiene hand. It's for COVID. It's made out of copper,
[01:19:39.340 --> 01:19:44.060]   actually brass, but there's copper in it. And you can go to the ATM machine. You never have to
[01:19:44.060 --> 01:19:49.260]   touch it, which is a big, I don't like touching touch screens. You can press elevator buttons.
[01:19:49.260 --> 01:19:56.620]   You can press walk buttons. And the hook is very handy for grabbing things. This is the first time
[01:19:56.620 --> 01:20:01.340]   I'm talking about, well, but it's Leo. That's too nerdy for me. I've never seen that before in
[01:20:01.340 --> 01:20:07.180]   any Twitch show, but I think that's finally too much. So speaking about COVID, I've got a natural
[01:20:07.180 --> 01:20:12.060]   serious question that I've been wondering. And I've been digging around online and I can't find
[01:20:12.060 --> 01:20:16.380]   a reasonable answer except for one thing, which I'll hold off on saying because I want to hear
[01:20:16.380 --> 01:20:22.380]   what you guys think. Why is it that tech companies, especially Apple and Google and Samsung, aren't
[01:20:22.380 --> 01:20:28.620]   pushing contactless payments? Like, you know, like it's the thing that everybody's got to have.
[01:20:28.620 --> 01:20:34.940]   Well, Apple is Apple. In fact, I strap on my Apple watch before I go to the grocery store,
[01:20:34.940 --> 01:20:40.540]   because I can just wave it at the payment thing. Yeah. Apple wants everybody to do this.
[01:20:40.540 --> 01:20:47.420]   COVID or not, they're pushing that hard. But I'm not seeing the same kind of pressure from
[01:20:47.420 --> 01:20:51.340]   other companies that do have in it. Google has no attention span.
[01:20:51.340 --> 01:20:57.100]   I have no desire to pull out my credit card or cash ever again.
[01:20:57.100 --> 01:21:00.460]   Do you wear you don't wear an Apple watch though? You could use your phone, I guess.
[01:21:00.460 --> 01:21:05.340]   Yeah. Yeah. Yeah. I just use my phone. I'm happy using my phone. But I don't want I have no desire
[01:21:05.340 --> 01:21:12.540]   to deal with with cards or cash, you know, as little as possible. And yet, and yet I don't see
[01:21:12.540 --> 01:21:17.420]   ads for this. I don't see people, you know, really sort of pushing it even here in San Francisco when
[01:21:17.420 --> 01:21:24.860]   I go out to buy something, you know, not every place has it. It's crazy. It's really just, it seems
[01:21:24.860 --> 01:21:31.660]   like so obvious, reducing risk, the risk of infections.
[01:21:31.660 --> 01:21:39.100]   I went to a Costco in California somewhere and they had a perfect solution. Basically,
[01:21:39.100 --> 01:21:46.940]   the protocol was you get out of the car and you insert your Costco card into the gas tank to get
[01:21:46.940 --> 01:21:53.420]   gas, then you get back in your car, then they have full service. So the attendant touches
[01:21:54.140 --> 01:21:59.100]   only the gas stuff. So you touch your stuff and he they touch their stuff. You never touch the
[01:21:59.100 --> 01:22:06.060]   same stuff. Yeah, that worked. I get that was so masked up before I go to a gas station and I get
[01:22:06.060 --> 01:22:11.420]   hygiene hand everything because I don't want to touch anything. Right. I feel like that's the most
[01:22:11.420 --> 01:22:17.260]   dangerous thing I do. So you know, so it's not it's not that people don't have that on their devices.
[01:22:17.260 --> 01:22:21.100]   I mean, like I do know people who have phones that don't have NFC or have older
[01:22:21.100 --> 01:22:25.500]   iPhone. Touchless never has never worked at the gas station. I have to point out they have the
[01:22:25.500 --> 01:22:31.500]   little fish and the thing. It works. It works on my phone. So I can sit on my motorcycle,
[01:22:31.500 --> 01:22:36.620]   take out my phone, reach over, touch the contactless payment on the pump and start filling up.
[01:22:36.620 --> 01:22:41.820]   But but what we're but the problem I'm talking about is is that not enough endpoints have it.
[01:22:41.820 --> 01:22:46.060]   So you're not seeing it in restaurants, which should be using it. We're not seeing it in
[01:22:46.060 --> 01:22:55.180]   restaurants. Right. And I don't know in what I think is is that back when we when we started using
[01:22:55.180 --> 01:23:00.860]   chip cards, the US was very slow to adopt because credit card companies were blocking
[01:23:00.860 --> 01:23:09.180]   the adoption of of those cards because stores didn't want to pay for new readers. And so they
[01:23:09.180 --> 01:23:14.060]   you know, they they didn't want to pay for them either. The in whoever was going to bear the cost
[01:23:14.060 --> 01:23:18.540]   wasn't going to step up. Eventually, we got chip in signature, which was not chip in pin.
[01:23:18.540 --> 01:23:23.580]   And and we're and I'm wondering if it's a similar
[01:23:23.580 --> 01:23:31.340]   stopping point here where where we know the technology exists. We know that some stores have
[01:23:31.340 --> 01:23:36.140]   it and others don't, but it's not enough of them. And there needs to be this push to really get it
[01:23:36.140 --> 01:23:41.900]   out into as many places as possible. It seems like a no brainer, but it's not it's not happening.
[01:23:43.660 --> 01:23:48.060]   Let's take it. It's also nice. Go ahead. Yeah, I was just gonna say Leo, it's also a nice intro
[01:23:48.060 --> 01:23:55.820]   to the Amazon store concept. Yeah, because that's also yeah, they're expanding it in an interesting
[01:23:55.820 --> 01:24:00.540]   way, which we'll talk about in just a second. But first a word from our sponsor, Mike Elgin is
[01:24:00.540 --> 01:24:08.060]   here Seth Rosenblatt. It's great to have Alex Wilhelm. What a panel smart, funny, and damn good looking.
[01:24:08.060 --> 01:24:14.380]   Our show today brought. I don't know if you might self in any of this. Our show today brought to
[01:24:14.380 --> 01:24:19.420]   you by Zen desk. You know the name Zen desk. It's a service first CRM company that builds
[01:24:19.420 --> 01:24:24.460]   software designed to improve customer relationships. We know things are a little weird right now.
[01:24:24.460 --> 01:24:31.100]   Make it hard to keep up with what matters most your customers. Customers don't go away and Zen
[01:24:31.100 --> 01:24:37.100]   desk is here to help. They're offering. This is a fantastic deal. A six month complimentary remote
[01:24:37.100 --> 01:24:43.180]   support bundle. This is for free for six months. It comes with the essential tools your team needs
[01:24:43.180 --> 01:24:47.980]   to stay agile and keep connected with your customers, whether by email, phone, chat,
[01:24:47.980 --> 01:24:52.940]   community form, help center, social media. It's easy to get up and running and you could get it
[01:24:52.940 --> 01:24:59.340]   in a matter of hours one day and you're done. You're ready. Zen desk was built on a very simple
[01:24:59.340 --> 01:25:04.380]   concept to make customer service software that's easy to use and accessible to everyone with a
[01:25:04.380 --> 01:25:10.620]   single dynamic help desk interface. Very simple to implement but flexible enough that you can
[01:25:10.620 --> 01:25:15.820]   modify it as your needs change. So it works right out of the box or can be configured with your
[01:25:15.820 --> 01:25:20.540]   preferences. And by the way, one of the ways people configure it is with the integrations.
[01:25:20.540 --> 01:25:26.700]   Man, they have a lot of integrations. It works with everything. Zen desk is perfect.
[01:25:26.700 --> 01:25:31.180]   Whether you're a startup or a large enterprise, it'll scale with you as you get going to ensure
[01:25:31.180 --> 01:25:36.540]   the powerful, innovative customer experiences are within reach for every company, no matter the size,
[01:25:36.540 --> 01:25:42.060]   the industry, or the ambition. Just talk to Ben Chapman. He's director of client facing
[01:25:42.060 --> 01:25:47.500]   experience and analytics at Homebridge. He says, having all the customer data in a single place
[01:25:47.500 --> 01:25:52.780]   helps us to achieve a high and consistent level of customer satisfaction. We can collaborate better
[01:25:52.780 --> 01:25:57.340]   and we have the visibility to see and report on everything. Zen desk also improves security
[01:25:57.980 --> 01:26:04.140]   by allowing us to control access to sensitive content. Another important point. Zen desk works
[01:26:04.140 --> 01:26:10.060]   nearly everywhere. More than 150,000 customers across hundreds of industries and over 30 languages.
[01:26:10.060 --> 01:26:15.100]   Look, the customer journey differs for everybody. But no matter what your business need,
[01:26:15.100 --> 01:26:21.660]   Zen desk products are flexible enough to pave the path that's best for your organization.
[01:26:21.660 --> 01:26:28.060]   Simple enough, go to zendesk.com/twit to get six months remote support bundle zen
[01:26:28.060 --> 01:26:38.380]   zendesk.com/twit. We thank you so much for their support of this week in tech. I do not talk
[01:26:38.380 --> 01:26:45.580]   much on the shows. I just a little bit I have about the Earn It Act, which was this backhanded way to
[01:26:46.460 --> 01:26:53.740]   get rid of end-to-end encryption by tying it to your Section 230 protections or the new
[01:26:53.740 --> 01:27:04.540]   much more blatant act to eliminate encryption called lead or it's what L-A-E-D, the lawful
[01:27:04.540 --> 01:27:12.060]   access to encrypted data act. Both cases that they're intended to eliminate encryption.
[01:27:12.060 --> 01:27:15.980]   But the reason I don't spend a lot of time talking about it is I don't feel like they're
[01:27:15.980 --> 01:27:21.020]   very likely to get passed or am I just completely wrong? Seth, you probably write about these.
[01:27:21.020 --> 01:27:29.020]   Should I be worried about either of these? Earn it seems to have been kind of decapitated
[01:27:29.020 --> 01:27:40.300]   in committee. It seems to have been. It also wasn't until it was. I think one of the problems
[01:27:40.300 --> 01:27:52.220]   is that these laws which face uncertain opposition, it's not really clear who's opposing them except
[01:27:52.220 --> 01:27:56.700]   for the usual suspects in Congress who fight these kinds of things. People like you're on
[01:27:56.700 --> 01:28:06.220]   line. What feels more dangerous this time about Earn It and I don't know whether to call it lead
[01:28:06.220 --> 01:28:13.100]   or late or whatever is that they're tying it to the prosecution. Depends if it's present tense or
[01:28:13.100 --> 01:28:20.220]   past tense. They're tying it to the prosecution of CSAM of child sexual abuse material.
[01:28:20.220 --> 01:28:25.740]   Right, because think of the children. You never go wrong by saying think of the children.
[01:28:25.740 --> 01:28:30.220]   Who's in favor of child sex abuse? Not me.
[01:28:32.220 --> 01:28:35.580]   Well, we don't know what Trump's relationship with Jeffrey Epstein was.
[01:28:35.580 --> 01:28:40.380]   Good point, but nevertheless. I mean, I don't know that we want to go there yet.
[01:28:40.380 --> 01:28:48.140]   But the point being is that these bills, are they out of committee? Did they get voted on?
[01:28:48.140 --> 01:28:52.540]   Earn It is out of committee. I'm pretty sure.
[01:28:52.540 --> 01:28:55.820]   They were supposed to vote on it a couple of weeks ago, but I didn't hear anything.
[01:28:55.820 --> 01:29:00.380]   I think they voted it out of committee and it's moving to the full Senate floor,
[01:29:00.380 --> 01:29:07.580]   but it was also just before it got voted out of committee. A manager's bill was a writer or
[01:29:07.580 --> 01:29:12.300]   something was attached to it. And so it's now far more diluted than it was.
[01:29:12.300 --> 01:29:15.740]   They took off, for instance, they took out the penalties. They just said,
[01:29:15.740 --> 01:29:19.900]   they just said, you better. You better do it right.
[01:29:19.900 --> 01:29:29.420]   So what we saw is if everyone remembers soap or PIPA and PIPA, those were fought against and
[01:29:29.420 --> 01:29:35.180]   defeated, Cesta and Fosta, which is the next turn of the screw of those, did get through.
[01:29:35.180 --> 01:29:46.780]   And when you read the accounts by adult sex workers, they have been harmed by those bills,
[01:29:46.780 --> 01:29:56.140]   not helped. Earn It is really, or at least as it was, was really bad. Lead is just a straight-up
[01:29:56.140 --> 01:30:02.860]   attempt to force backdoor access. And that's not going to end well, ever,
[01:30:02.860 --> 01:30:10.140]   because encryption is simply math. And if you think of it, I don't know if you guys have seen
[01:30:10.140 --> 01:30:14.700]   these face masks that have been going around that have a hole in the middle. So you can-
[01:30:14.700 --> 01:30:16.540]   It's easy to breathe, really.
[01:30:16.540 --> 01:30:17.980]   You can figure straw.
[01:30:17.980 --> 01:30:19.100]   Yeah, it's good.
[01:30:19.100 --> 01:30:21.500]   You're very crassigarette or whatever.
[01:30:21.500 --> 01:30:21.820]   Yeah.
[01:30:22.460 --> 01:30:29.100]   Also terrible for coronavirus. So I tell people that these bills are what they want to do
[01:30:29.100 --> 01:30:33.740]   with by mandating a backdoor and encryption is like wearing a mask with a hole in the middle.
[01:30:33.740 --> 01:30:37.500]   It's not really going to be effective because at some point, something is going to get through it.
[01:30:37.500 --> 01:30:38.220]   The math is out there.
[01:30:38.220 --> 01:30:38.540]   Yeah.
[01:30:38.540 --> 01:30:45.980]   So what it will do is keep Apple and Facebook and Google from providing end-to-end encryption
[01:30:45.980 --> 01:30:46.780]   of any kind.
[01:30:47.980 --> 01:30:52.940]   And the argument is that, you know, end-to-end encryption is already in use,
[01:30:52.940 --> 01:30:58.540]   often enroll your own situation by cyber criminals. So they already have this.
[01:30:58.540 --> 01:31:06.060]   You're not really doing anything more to get at them when you're using encryption that's been
[01:31:06.060 --> 01:31:07.180]   intentionally backdoor.
[01:31:07.180 --> 01:31:10.460]   My real question is the reason I don't talk about it a lot is I just feel like
[01:31:10.460 --> 01:31:16.620]   it's-is this security theater or is there actually a chance?
[01:31:16.620 --> 01:31:18.940]   I think there might be a chance they'd get out of the Senate.
[01:31:18.940 --> 01:31:20.460]   Both of them are Senate bills.
[01:31:20.460 --> 01:31:22.140]   But I don't think it'll get out of the house.
[01:31:22.140 --> 01:31:23.580]   So I-
[01:31:23.580 --> 01:31:25.420]   Do we want to risk it?
[01:31:25.420 --> 01:31:32.300]   I mean, I think there have been enough bad tech legislation that's gotten through Congress
[01:31:32.300 --> 01:31:35.100]   and signed, you know-
[01:31:35.100 --> 01:31:36.860]   Are they going to wait till we're all busy?
[01:31:36.860 --> 01:31:37.580]   That people should-
[01:31:37.580 --> 01:31:41.740]   Worryed about something else and just kind of sneak it out like a pandemic?
[01:31:41.740 --> 01:31:42.220]   Yeah.
[01:31:42.220 --> 01:31:43.340]   Yeah, right.
[01:31:43.340 --> 01:31:44.540]   That seems likely to me.
[01:31:45.580 --> 01:31:50.940]   And another issue, and one that I feel is really important and not being discussed enough,
[01:31:50.940 --> 01:31:54.220]   is that CSAM is a serious problem online.
[01:31:54.220 --> 01:31:58.620]   It's definitely something that needs to be addressed more and better by,
[01:31:58.620 --> 01:32:00.860]   you know, by tech companies.
[01:32:00.860 --> 01:32:03.340]   And for too long it hasn't been.
[01:32:03.340 --> 01:32:07.260]   And part of the problem here is that, you know, now the people-
[01:32:07.260 --> 01:32:13.660]   the anti-encryption groups have, you know, a very compelling argument that
[01:32:13.660 --> 01:32:15.180]   CSAM needs to be stopped.
[01:32:15.180 --> 01:32:16.380]   Everybody agrees with that.
[01:32:16.380 --> 01:32:21.820]   And why didn't actually proposed a bill to sort of counter-earn it
[01:32:21.820 --> 01:32:27.900]   that would increase funding to national organizations for prosecuting CSAM,
[01:32:27.900 --> 01:32:31.180]   for tracking down these people, for throwing them in jail,
[01:32:31.180 --> 01:32:34.780]   and for getting aid to the victims?
[01:32:34.780 --> 01:32:39.100]   And fortunately it doesn't have a clever acronym,
[01:32:39.100 --> 01:32:41.340]   so it's- I can't remember it at the moment.
[01:32:42.220 --> 01:32:46.860]   But it's one of those things where this is a bill that may actually help prevent CSAM
[01:32:46.860 --> 01:32:50.300]   and it's not being discussed and it's not being
[01:32:50.300 --> 01:32:56.780]   talked about by other news organizations anywhere near as much as it should.
[01:32:56.780 --> 01:32:59.980]   Every time we talk about earn it, we should be talking about this other bill,
[01:32:59.980 --> 01:33:05.580]   because the idea that stopping CSAM is going to happen when you poke a hole in encryption is
[01:33:05.580 --> 01:33:06.540]   just silly.
[01:33:06.540 --> 01:33:08.140]   I mean, it's not how it works.
[01:33:08.140 --> 01:33:13.580]   And to follow along on Mike on your contention that it's the secret stuff you got to worry
[01:33:13.580 --> 01:33:16.380]   more about, I didn't even know this.
[01:33:16.380 --> 01:33:18.940]   Thank you, Seth, for putting this story in the rundown.
[01:33:18.940 --> 01:33:24.620]   Apparently, the president has issued what's called a presidential finding,
[01:33:24.620 --> 01:33:32.540]   aka a secret authorization, to give the CIA more sweeping powers in their international
[01:33:32.540 --> 01:33:39.580]   operations, who it targets, undoing a lot of restrictions that have been placed by other
[01:33:39.580 --> 01:33:42.060]   administrations, previous administrations.
[01:33:42.060 --> 01:33:47.980]   The CIA will now have easier time authorizing its own covert cyber operations.
[01:33:47.980 --> 01:33:49.980]   They don't have to get approval from the White House.
[01:33:49.980 --> 01:33:57.260]   This is what some are calling a very aggressive finding
[01:33:58.460 --> 01:34:06.540]   that lets the CIA take the fight offensively to Russia, China, Iran, North Korea.
[01:34:06.540 --> 01:34:11.100]   Those are specifically mentioned, but it could be any country.
[01:34:11.100 --> 01:34:14.700]   There wasn't a lot of coverage of this story at all.
[01:34:14.700 --> 01:34:22.220]   It's an abrogation of executive authority over foreign policy, basically.
[01:34:22.220 --> 01:34:22.860]   Sam, you do it.
[01:34:22.860 --> 01:34:23.420]   And you do it.
[01:34:23.420 --> 01:34:23.820]   I don't want to do it.
[01:34:23.820 --> 01:34:26.860]   Well, there are two ways to abrogate foreign policy.
[01:34:26.860 --> 01:34:31.820]   One is that you find out that Russians are paying bounties to kill American soldiers and you go,
[01:34:31.820 --> 01:34:34.060]   well, who cares, whatever, and you move on.
[01:34:34.060 --> 01:34:36.620]   That's one way to shirk your responsibility.
[01:34:36.620 --> 01:34:40.460]   The other one is the CIA comes to you and says, hey, we want to be able to make all our own
[01:34:40.460 --> 01:34:41.020]   decisions.
[01:34:41.020 --> 01:34:42.780]   That way, you don't have to deal with any of this stuff.
[01:34:42.780 --> 01:34:46.380]   We'll just make the decisions and act on our own more than we've been doing.
[01:34:46.380 --> 01:34:47.980]   Yeah, go ahead and do that.
[01:34:47.980 --> 01:34:50.300]   That's another way to shirk your responsibility.
[01:34:50.300 --> 01:35:00.060]   We do want the CIA in specific areas to take the fight, as it were, to other countries that are
[01:35:00.060 --> 01:35:03.340]   attacking us with asymmetrical warfare.
[01:35:03.340 --> 01:35:06.140]   We want to push back with asymmetrical warfare.
[01:35:06.140 --> 01:35:08.460]   Absolutely, but that needs to have-
[01:35:08.460 --> 01:35:10.540]   And he's called it the most-
[01:35:10.540 --> 01:35:11.100]   And he's called it the most-
[01:35:11.100 --> 01:35:11.580]   And he's called it the most-
[01:35:11.580 --> 01:35:12.620]   Billion oversight.
[01:35:12.620 --> 01:35:13.020]   Yes.
[01:35:13.020 --> 01:35:15.580]   Not just go ahead and do your thing, because that's-
[01:35:15.580 --> 01:35:17.020]   So to be clear, this happened-
[01:35:17.020 --> 01:35:18.460]   Right, that's how we can solve that stuff.
[01:35:18.460 --> 01:35:20.380]   Two years ago, but we're only now-
[01:35:20.380 --> 01:35:22.860]   Because it was very closely held.
[01:35:22.860 --> 01:35:24.060]   We're only now learning-
[01:35:24.060 --> 01:35:27.340]   This is a story from Yahoo News, Zach Dorfman, Kim Zetter,
[01:35:27.340 --> 01:35:29.340]   Jenna McLaughlin, and Sean Naylor.
[01:35:29.340 --> 01:35:34.220]   This is their scoop, because they've seen now this memo.
[01:35:34.220 --> 01:35:39.580]   And if this is accurate, this basically gives the CIA
[01:35:39.580 --> 01:35:43.820]   free hand to run cyber war without any oversight at all.
[01:35:45.740 --> 01:35:48.620]   There's never been a historical example of American three-letter agencies
[01:35:48.620 --> 01:35:50.300]   overstepping their authorities, so we're certainly-
[01:35:50.300 --> 01:35:50.940]   Right, we can tell.
[01:35:50.940 --> 01:35:51.100]   ...fying here, I know.
[01:35:51.100 --> 01:35:51.900]   I'm so concerned.
[01:35:51.900 --> 01:35:52.780]   Yeah, they're paid-
[01:35:52.780 --> 01:35:53.740]   Sarcasm, by the way.
[01:35:53.740 --> 01:35:54.140]   I know.
[01:35:54.140 --> 01:35:55.980]   Really?
[01:35:55.980 --> 01:35:57.500]   Just some people wouldn't get it, so I feel like-
[01:35:57.500 --> 01:36:01.020]   Yeah, well, so one of the interesting things in this story is that this is something
[01:36:01.020 --> 01:36:04.300]   that the CIA has been asking for since the Bush administration,
[01:36:04.300 --> 01:36:06.620]   the second Bush, not the first Bush.
[01:36:06.620 --> 01:36:10.620]   They asked for it under Obama, and both times they were denied.
[01:36:10.620 --> 01:36:14.940]   And they were apparently expecting to be denied again.
[01:36:15.740 --> 01:36:20.140]   Because at this point, if a Republican and a Democrat say,
[01:36:20.140 --> 01:36:22.140]   "No, why would anything change?"
[01:36:22.140 --> 01:36:24.940]   And Trump said yes and signed it.
[01:36:24.940 --> 01:36:27.260]   And one of the issues-
[01:36:27.260 --> 01:36:31.500]   I can't just see the CIA guy running home from the White House,
[01:36:31.500 --> 01:36:32.620]   and he signed it!
[01:36:32.620 --> 01:36:33.420]   He signed it!
[01:36:33.420 --> 01:36:34.140]   It worked!
[01:36:34.140 --> 01:36:34.700]   It worked!
[01:36:34.700 --> 01:36:38.780]   I can see the CIA going, "Just tell him I was against it."
[01:36:38.780 --> 01:36:40.060]   Yeah, Obama hated it.
[01:36:40.060 --> 01:36:41.740]   Well, that's just a bad idea.
[01:36:41.740 --> 01:36:44.060]   That may be exactly what happened.
[01:36:44.060 --> 01:36:45.820]   I mean, who knows about that?
[01:36:45.820 --> 01:36:50.620]   But one of the interesting things here is that it's not just going after
[01:36:50.620 --> 01:36:56.460]   countries and some kind of nebulous cyber warfare.
[01:36:56.460 --> 01:36:58.220]   It's going after critical infrastructure.
[01:36:58.220 --> 01:37:03.020]   See, and the real risk of that is the escalation of this kind of cyber warfare,
[01:37:03.020 --> 01:37:04.060]   because once-
[01:37:04.060 --> 01:37:06.940]   So if we're doing it, they can do it, and it just escalates.
[01:37:06.940 --> 01:37:12.620]   And honestly, you could talk about the devastation created by nuclear warfare.
[01:37:12.620 --> 01:37:18.220]   Fortunately, everybody on both sides kind of understands that and steps back a little bit from it.
[01:37:18.220 --> 01:37:23.020]   But I think you could devastate a country just as much, if not more so, with economic advantage.
[01:37:23.020 --> 01:37:26.780]   The hacking tools that we saw used in Stuxnet,
[01:37:26.780 --> 01:37:34.140]   which at the time we're cutting edge and use only by the Israelis and the US,
[01:37:34.140 --> 01:37:37.180]   since have been found in use elsewhere.
[01:37:37.180 --> 01:37:37.340]   That's right.
[01:37:37.340 --> 01:37:39.580]   In terms of code, I'm Stuxnet, made it-
[01:37:39.580 --> 01:37:39.580]   That's right.
[01:37:39.580 --> 01:37:41.420]   Have now made it into commercial malware.
[01:37:42.380 --> 01:37:49.980]   These things have a very clear trickle down effect and can be quite detrimental
[01:37:49.980 --> 01:37:57.580]   within a very short period of time, far beyond anything that they were originally intended for.
[01:37:57.580 --> 01:37:58.540]   Mike, what were you going to say?
[01:37:58.540 --> 01:38:07.020]   Well, I just think that what we need is to have our pushback against cyber warfare attacks on us
[01:38:07.020 --> 01:38:08.860]   to be part of a strategic program.
[01:38:08.860 --> 01:38:12.540]   For example, I'd love to see- I mean, we're just getting our bots handed to us in terms of
[01:38:12.540 --> 01:38:17.660]   disinformation for mostly Russia, but also China and other countries getting into the act as well,
[01:38:17.660 --> 01:38:22.540]   because everybody's noticed that the United States is just an easily-influenceable country.
[01:38:22.540 --> 01:38:29.420]   I would love to see a strategic vision that put a lot of wood behind the arrow of fighting
[01:38:29.420 --> 01:38:31.500]   disinformation with information.
[01:38:31.500 --> 01:38:31.980]   Yes.
[01:38:31.980 --> 01:38:33.020]   So something like that-
[01:38:33.020 --> 01:38:35.660]   Something fits more with the openness that I was talking about.
[01:38:35.660 --> 01:38:36.140]   Exactly.
[01:38:36.140 --> 01:38:42.060]   Let's pursue that as opposed to secret cyber warfare.
[01:38:42.060 --> 01:38:49.100]   And when you're going to attack infrastructure like a stuck-snet type of attack,
[01:38:49.100 --> 01:38:52.060]   I mean, stuck-snet, that was mostly Israeli technology.
[01:38:52.060 --> 01:38:53.420]   The US kind of went along with it.
[01:38:53.420 --> 01:38:58.620]   It was kind of like a Middle Eastern thing and going after a specific problem.
[01:38:58.620 --> 01:39:03.340]   Well, also it was seemed reasonable because they were enriching uranium for making the atomic bomb.
[01:39:03.340 --> 01:39:05.580]   So we wanted to stop the Iranians from doing that.
[01:39:05.580 --> 01:39:08.780]   The Israelis really wanted to stop the Iranians from doing that.
[01:39:08.780 --> 01:39:10.540]   They said, "Hey, you know what we could do?
[01:39:10.540 --> 01:39:14.780]   These are air-gapped centrifuges, but we could actually blow them up.
[01:39:14.780 --> 01:39:17.820]   If we could just get somebody to pick up a USB key and stick it in."
[01:39:17.820 --> 01:39:19.340]   And it worked.
[01:39:19.340 --> 01:39:25.100]   What you don't want is to be- just to have this low-level constant
[01:39:25.100 --> 01:39:29.100]   attack anything you can attack, because that leads to all kinds of other problems.
[01:39:29.100 --> 01:39:36.380]   You need to have a policy, a strategy, a vision for why we attack, when we attack, exactly.
[01:39:36.380 --> 01:39:39.820]   Because if the United States is going to be a global leader in these things,
[01:39:39.820 --> 01:39:46.620]   we need to basically try to- the best we can to set the boundaries and the rules for
[01:39:46.620 --> 01:39:49.340]   how these sort of low-level cyber attacks are going to happen.
[01:39:49.340 --> 01:39:53.980]   And because if we don't, it's just going to be a free-for-all and chaos.
[01:39:53.980 --> 01:39:56.700]   It's going to evolve in a bad way very quickly.
[01:39:56.700 --> 01:39:57.500]   And we don't want that.
[01:39:58.060 --> 01:40:02.860]   So we have an all-day talked about any of the big companies, Google,
[01:40:02.860 --> 01:40:08.060]   talked a little bit about Facebook, but Google, Apple, I've got a Google story that I don't
[01:40:08.060 --> 01:40:12.140]   understand, but you guys, you're so global, you're so smart, you'll understand it, and you can tell
[01:40:12.140 --> 01:40:15.340]   me why this thing with GEO is a big deal.
[01:40:15.340 --> 01:40:20.060]   But- and you're a finance guy, Alex, maybe you'll understand too.
[01:40:20.060 --> 01:40:22.620]   But first, a word from our sponsor.
[01:40:22.620 --> 01:40:30.140]   I was talking to a guy called the radio show saying, I use a VPN.
[01:40:30.140 --> 01:40:34.780]   A lot of people do at home to protect themselves against the snooping of their
[01:40:34.780 --> 01:40:36.140]   internet service providers.
[01:40:36.140 --> 01:40:37.980]   And I couldn't get on YouTube TV.
[01:40:37.980 --> 01:40:39.820]   How do they know I'm on a VPN?
[01:40:39.820 --> 01:40:45.340]   I said, there's one way that IP address has been used by other VPN customers.
[01:40:45.340 --> 01:40:49.340]   And as a result, they kind of know that you're on a VPN.
[01:40:49.340 --> 01:40:50.780]   There's no way to sniff the traffic.
[01:40:50.780 --> 01:40:52.220]   It's from the IP address.
[01:40:52.220 --> 01:41:00.060]   What you want is a VPN service that doesn't reuse those IP addresses in the same way.
[01:41:00.060 --> 01:41:05.420]   You want ExpressVPN, which works not only with YouTube TV, but with Hulu and Netflix and everything
[01:41:05.420 --> 01:41:10.060]   else. ExpressVPN is the only VPN I ever think about.
[01:41:10.060 --> 01:41:15.900]   Think about all the times you're at home surfing the net, and you searched for something that
[01:41:15.900 --> 01:41:20.700]   maybe you'd like to keep to yourself, the hemorrhoid cream or whatever.
[01:41:20.700 --> 01:41:24.300]   You're internet service providers sitting there going, he's looking for a hemorrhoid cream,
[01:41:24.300 --> 01:41:25.580]   quick set up some ads.
[01:41:25.580 --> 01:41:28.700]   And they aren't, by the way, legally entitled.
[01:41:28.700 --> 01:41:33.740]   They're actually explicitly entitled to sell that information to make a little extra money,
[01:41:33.740 --> 01:41:36.780]   because gosh knows you don't pay enough for your internet service.
[01:41:36.780 --> 01:41:42.300]   That's why you got even at home, even when it's not a security issue, when you're not trying to
[01:41:42.300 --> 01:41:46.940]   stay safe at the open Wi-Fi access point, you got to run a VPN.
[01:41:46.940 --> 01:41:49.980]   Incognito mode is not incognito.
[01:41:50.620 --> 01:41:53.980]   Private browsing is not private browsing.
[01:41:53.980 --> 01:42:00.220]   Whenever you're at home, whenever you're around town, you got to use ExpressVPN.
[01:42:00.220 --> 01:42:02.300]   You can trust ExpressVPN.
[01:42:02.300 --> 01:42:04.380]   They've had independent third party audits.
[01:42:04.380 --> 01:42:05.820]   They do not log.
[01:42:05.820 --> 01:42:09.580]   In fact, they go so far to prevent logging.
[01:42:09.580 --> 01:42:11.420]   They actually run something called trusted server.
[01:42:11.420 --> 01:42:14.700]   When you press the button, it's so easy to get used ExpressVPN.
[01:42:14.700 --> 01:42:19.500]   There's an app for every device, even smart TVs and streaming devices.
[01:42:19.500 --> 01:42:22.620]   You press the button, you get on the VPN server.
[01:42:22.620 --> 01:42:28.140]   That VPN server spins up in RAM only, a copy of the trusted server.
[01:42:28.140 --> 01:42:29.580]   That's what you're running through.
[01:42:29.580 --> 01:42:30.460]   It's sandboxed.
[01:42:30.460 --> 01:42:31.660]   It can't write to the hard drive.
[01:42:31.660 --> 01:42:33.500]   It cannot keep track of what you're doing.
[01:42:33.500 --> 01:42:36.140]   And then when you leave, when you log off, it disappears.
[01:42:36.140 --> 01:42:36.940]   It's RAM only.
[01:42:36.940 --> 01:42:41.900]   There is no trace, no trace of your visit to ExpressVPN and all.
[01:42:41.900 --> 01:42:44.060]   So your privacy is absolutely.
[01:42:44.620 --> 01:42:45.420]   Protected.
[01:42:45.420 --> 01:42:47.180]   I don't care what ISP you're using.
[01:42:47.180 --> 01:42:50.300]   I don't care if it's a big boys like Verizon or Comcast, a little one.
[01:42:50.300 --> 01:42:53.180]   They all can legally sell your information to ad companies.
[01:42:53.180 --> 01:42:56.780]   ExpressVPN keeps them from snooping on you.
[01:42:56.780 --> 01:43:02.060]   It reroutes your internet connection, your DNS, everything through their secure servers.
[01:43:02.060 --> 01:43:03.980]   The ISPs can't see the sites you visit.
[01:43:03.980 --> 01:43:07.340]   It's all encrypted with the best encryption available.
[01:43:07.340 --> 01:43:13.420]   And by the way, you can leave it on because ExpressVPN is awesome.
[01:43:13.420 --> 01:43:15.260]   And this is because they're a really good company.
[01:43:15.260 --> 01:43:17.900]   They have so many servers that they're fast.
[01:43:17.900 --> 01:43:19.260]   You can watch HD video.
[01:43:19.260 --> 01:43:20.300]   You can watch Netflix.
[01:43:20.300 --> 01:43:24.540]   I will go to England and watch Dr. Hu on Netflix in England.
[01:43:24.540 --> 01:43:25.980]   And it doesn't slow down.
[01:43:25.980 --> 01:43:26.940]   You can watch it in HD.
[01:43:26.940 --> 01:43:30.300]   Very few VPN providers can say that.
[01:43:30.300 --> 01:43:31.820]   So much so that I just leave it on.
[01:43:31.820 --> 01:43:32.860]   I don't even know it's on.
[01:43:32.860 --> 01:43:36.220]   Sometimes I'm surprised to go, "Oh, wait a minute, my VPN's on."
[01:43:36.220 --> 01:43:39.260]   Apparently I'm emerging on the internet from Belarus.
[01:43:39.260 --> 01:43:40.060]   I didn't know that.
[01:43:40.060 --> 01:43:40.860]   Sorry about that.
[01:43:41.820 --> 01:43:51.260]   All the devices you use, all of the places you go, ExpressVPN is the way to protect your privacy,
[01:43:51.260 --> 01:43:59.740]   to protect your security, to expand the TV shows you can watch, eliminate geographic restrictions,
[01:43:59.740 --> 01:44:01.340]   protect your online activity today.
[01:44:01.340 --> 01:44:03.740]   It's the number one VPN, not just me.
[01:44:03.740 --> 01:44:04.860]   It's not not the only one.
[01:44:04.860 --> 01:44:08.220]   CNET said it, Wired said it.
[01:44:08.220 --> 01:44:11.340]   If you look at the webpage, you'll see lots of other places
[01:44:11.340 --> 01:44:13.420]   rated very highly everywhere.
[01:44:13.420 --> 01:44:15.980]   Protect your online activity today.
[01:44:15.980 --> 01:44:17.980]   ExpressVPN.com/twit.
[01:44:17.980 --> 01:44:20.380]   Go to ExpressVPN.com/twit.
[01:44:20.380 --> 01:44:23.820]   The best deal, you'll get an extra three months when you sign up for a one-year package.
[01:44:23.820 --> 01:44:26.380]   It gets it down about seven bucks a month, a little less.
[01:44:26.380 --> 01:44:28.060]   That's the beauty of ExpressVPN.
[01:44:28.060 --> 01:44:29.740]   They charge a reasonable amount.
[01:44:29.740 --> 01:44:31.500]   I think it's a very fair amount.
[01:44:31.500 --> 01:44:33.580]   And that's how they can do it so well.
[01:44:33.580 --> 01:44:38.060]   ExpressVPN.com/twit.
[01:44:38.060 --> 01:44:41.020]   Thank you very much for your support, ExpressVPN.
[01:44:41.020 --> 01:44:42.940]   And for keeping us safe.
[01:44:42.940 --> 01:44:45.580]   Leo, I'd like to second what you said.
[01:44:45.580 --> 01:44:49.180]   And I'm glad you emphasized a couple of points because it's not often emphasized.
[01:44:49.180 --> 01:44:52.780]   VPNs have limited capacity.
[01:44:52.780 --> 01:44:57.180]   And everybody's suddenly working from home and some of the lesser VPNs are just groaning under
[01:44:57.180 --> 01:44:57.740]   the strain.
[01:44:57.740 --> 01:44:58.220]   Yeah.
[01:44:58.220 --> 01:45:01.820]   ExpressVPN, which I just used constantly.
[01:45:01.820 --> 01:45:02.780]   I never turned it off.
[01:45:02.780 --> 01:45:04.060]   It's just there.
[01:45:04.060 --> 01:45:06.940]   I forget about it because it's like no performance hit.
[01:45:06.940 --> 01:45:07.820]   Everything's fast.
[01:45:08.620 --> 01:45:12.220]   And like you say, you can pick a country, any country.
[01:45:12.220 --> 01:45:15.020]   So when you're abroad and you need to access your bank,
[01:45:15.020 --> 01:45:17.980]   which doesn't want you logging into a US bank from another country,
[01:45:17.980 --> 01:45:20.940]   you just decide to pop up in the US.
[01:45:20.940 --> 01:45:23.660]   If you're in the US, you want to access a foreign resource.
[01:45:23.660 --> 01:45:24.940]   You pop up in that country.
[01:45:24.940 --> 01:45:26.380]   So it's just like brilliant.
[01:45:26.380 --> 01:45:31.020]   And for those less technical people who are afraid or have tried other VPNs that are really slow,
[01:45:31.020 --> 01:45:33.260]   it's like, ExpressVPN is amazing.
[01:45:33.260 --> 01:45:37.020]   You just turn it on and say, "Always be running," and then you never have to think about it.
[01:45:37.020 --> 01:45:37.980]   I leave it on everywhere.
[01:45:38.540 --> 01:45:41.420]   So I mentioned this story and we probably should mention it.
[01:45:41.420 --> 01:45:48.140]   894 gigabytes of records left on an unsecured elastic search cluster,
[01:45:48.140 --> 01:45:53.740]   belonging to UFO VPN, which is a Hong Kong VPN, with multiple affiliates.
[01:45:53.740 --> 01:45:55.660]   I think there were seven other affiliates.
[01:45:55.660 --> 01:46:04.220]   UFO, fast, free VPN, super VPN, flash VPN, secure VPN, and rabbit VPN.
[01:46:04.780 --> 01:46:08.860]   They do white label VPN service, so you may actually be using them and not know it.
[01:46:08.860 --> 01:46:12.620]   They've all claimed that they don't log, but somehow,
[01:46:12.620 --> 01:46:21.100]   actually the total amount was 1.2 terabytes of data, including more than a billion log entries,
[01:46:21.100 --> 01:46:27.340]   many featuring highly sensitive information, just left hanging out on an elastic search cluster.
[01:46:27.340 --> 01:46:32.860]   Just they included websites, visited connection logs, people's names,
[01:46:32.860 --> 01:46:38.620]   subscribers, email, home address, plain text passwords, Bitcoin and PayPal information,
[01:46:38.620 --> 01:46:43.260]   messages to support desk device specifications, account info.
[01:46:43.260 --> 01:46:48.380]   And the thing that pisses me off is they all pretend to be no log VPNs.
[01:46:48.380 --> 01:46:59.420]   Speaking of the internet creaking under the load, CloudFlare was out for about half an hour.
[01:46:59.420 --> 01:47:01.100]   Did you notice any of that this week?
[01:47:01.660 --> 01:47:02.940]   This was on tweets.
[01:47:02.940 --> 01:47:07.180]   Yeah, July 17th. So that was Friday, right?
[01:47:07.180 --> 01:47:09.740]   They were out for 27 minutes.
[01:47:09.740 --> 01:47:12.940]   Affecting locations.
[01:47:12.940 --> 01:47:16.220]   The San Jose Dallas, Seattle, Los Angeles, Chicago, Washington, Richmond,
[01:47:16.220 --> 01:47:17.580]   and I can go on and on and on.
[01:47:17.580 --> 01:47:21.180]   It just shows how much traffic goes through CloudFlare.
[01:47:21.180 --> 01:47:24.460]   CloudFlare is widely used by sites for DDoS protection.
[01:47:24.460 --> 01:47:27.500]   For other reasons, we use it for authentication.
[01:47:27.500 --> 01:47:29.180]   They have a great authentication service.
[01:47:29.180 --> 01:47:29.980]   They're really good.
[01:47:29.980 --> 01:47:31.340]   I really love CloudFlare.
[01:47:32.140 --> 01:47:36.620]   And apparently an engineer,
[01:47:36.620 --> 01:47:42.780]   while working on a segment of the backbone from Newark to Chicago,
[01:47:42.780 --> 01:47:47.740]   updated the configuration on a router in Atlanta to alleviate congestion.
[01:47:47.740 --> 01:47:49.180]   Thanks to COVID.
[01:47:49.180 --> 01:47:52.700]   Unfortunately, they fat-fingered it.
[01:47:52.700 --> 01:47:59.180]   And there was an error that caused all the traffic across the backbone to be sent to Atlanta,
[01:48:00.620 --> 01:48:03.180]   which immediately overwhelmed the Atlanta router.
[01:48:03.180 --> 01:48:05.820]   This is John Graham coming running for a Cloudware,
[01:48:05.820 --> 01:48:10.140]   and caused CloudFlare network locations connected to the backbone to fail.
[01:48:10.140 --> 01:48:13.420]   So if you noticed, you wouldn't notice it as CloudFlare.
[01:48:13.420 --> 01:48:18.060]   You'd notice it as your site loading slowly, or in some cases, not at all.
[01:48:18.060 --> 01:48:27.180]   I don't, you know, it's too easy to fat-finger BGP tables.
[01:48:27.180 --> 01:48:29.900]   All of this stuff, we've seen this happen over and over again.
[01:48:30.620 --> 01:48:35.340]   There's got to be a better way to do this than have somebody typing in,
[01:48:35.340 --> 01:48:37.260]   you know, to Emacs.
[01:48:37.260 --> 01:48:39.020]   And let's get that right here.
[01:48:39.020 --> 01:48:40.140]   We'll just do that.
[01:48:40.140 --> 01:48:40.780]   Then make it a mess.
[01:48:40.780 --> 01:48:46.620]   On the other hand, it's just part of the reality of the distribution of the--
[01:48:46.620 --> 01:48:49.580]   The charming nature of the internet.
[01:48:49.580 --> 01:48:50.460]   Exactly.
[01:48:50.460 --> 01:48:51.900]   There is no internet.
[01:48:51.900 --> 01:48:53.340]   It's just a bunch of computers.
[01:48:53.340 --> 01:48:57.900]   And so this happens when AWS goes down or that has a problem.
[01:48:57.900 --> 01:49:01.740]   You know, basically there is no big gigantic ocean.
[01:49:01.740 --> 01:49:05.420]   It's just a bunch of little ponds that we're all accessing through.
[01:49:05.420 --> 01:49:09.740]   And one fat-finger attack, and you know, we have a bunch of problems.
[01:49:09.740 --> 01:49:13.980]   But the plus side is that it's not a central--
[01:49:13.980 --> 01:49:15.340]   The internet is not centralized.
[01:49:15.340 --> 01:49:16.860]   And they fixed it in 27 minutes.
[01:49:16.860 --> 01:49:19.100]   It is, though, that's part of the problem,
[01:49:19.100 --> 01:49:21.980]   because it is more centralized, because so many people use CloudFlare.
[01:49:21.980 --> 01:49:24.140]   And so many people use Amazon.
[01:49:24.140 --> 01:49:26.540]   So there is some centralization happening, which is not good.
[01:49:26.540 --> 01:49:28.300]   This is my favorite visualization.
[01:49:28.300 --> 01:49:31.740]   This is the CloudFlare traffic conditions.
[01:49:31.740 --> 01:49:34.700]   And what you're seeing-- green means normal traffic.
[01:49:34.700 --> 01:49:37.020]   You see the yellow, orange, and red.
[01:49:37.020 --> 01:49:39.260]   That means super huge amount of traffic.
[01:49:39.260 --> 01:49:40.540]   That's the Atlanta knock.
[01:49:40.540 --> 01:49:44.620]   And then all these white spaces are no traffic.
[01:49:44.620 --> 01:49:46.540]   So this chunk--
[01:49:46.540 --> 01:49:47.740]   I'd be not happening.
[01:49:47.740 --> 01:49:49.100]   That's nothing happening.
[01:49:49.100 --> 01:49:51.340]   This chunk got sent to there.
[01:49:54.380 --> 01:49:55.180]   I shouldn't laugh.
[01:49:55.180 --> 01:49:59.820]   And I'm John's a friend, and I sympathize.
[01:49:59.820 --> 01:50:03.260]   I can imagine what that board looked like as it lit up.
[01:50:03.260 --> 01:50:04.940]   Well, it's like the old joke.
[01:50:04.940 --> 01:50:08.460]   The two worst ideas in history are humans and the internet.
[01:50:08.460 --> 01:50:08.700]   Yeah.
[01:50:08.700 --> 01:50:12.860]   I had a third one, which is BGP, because it almost always--
[01:50:12.860 --> 01:50:13.660]   Yeah.
[01:50:13.660 --> 01:50:16.380]   It's the border gateway protocol that causes this.
[01:50:16.380 --> 01:50:23.180]   This was, in fact, you know, he changed the line that leaked all the BGP routes into the backbone.
[01:50:24.060 --> 01:50:24.780]   To Atlanta.
[01:50:24.780 --> 01:50:27.100]   And you know what?
[01:50:27.100 --> 01:50:27.580]   The financial--
[01:50:27.580 --> 01:50:29.740]   You want transparency.
[01:50:29.740 --> 01:50:31.260]   God bless you, CloudFlare.
[01:50:31.260 --> 01:50:31.820]   They're actually--
[01:50:31.820 --> 01:50:34.620]   This has got not only the picture--
[01:50:34.620 --> 01:50:36.620]   This is the text that the guy screwed up.
[01:50:36.620 --> 01:50:38.300]   This is good.
[01:50:38.300 --> 01:50:38.860]   Yeah.
[01:50:38.860 --> 01:50:40.940]   You added an ATL0.
[01:50:40.940 --> 01:50:44.380]   Yeah.
[01:50:44.380 --> 01:50:45.900]   BGP is very vulnerable.
[01:50:45.900 --> 01:50:46.300]   Go ahead.
[01:50:46.300 --> 01:50:47.020]   The financial--
[01:50:47.020 --> 01:50:50.780]   Oh, I was just thinking about the hit to the company's share price.
[01:50:50.780 --> 01:50:53.660]   I pulled that up when we were talking to the fat finger disaster.
[01:50:53.660 --> 01:50:56.780]   No, it was 1.3%, so effectively nothing.
[01:50:56.780 --> 01:50:59.980]   So the result of this sort of thing isn't really sharp in the financial market.
[01:50:59.980 --> 01:51:03.100]   So I think we'll keep seeing similar errors because the penalty,
[01:51:03.100 --> 01:51:05.980]   aside from embarrassment, is pretty minimal, it seems.
[01:51:05.980 --> 01:51:09.580]   And what it did prove is how many people use CloudFlare, so in a way in a--
[01:51:09.580 --> 01:51:10.140]   Very popular.
[01:51:10.140 --> 01:51:11.180]   Kind of negative way.
[01:51:11.180 --> 01:51:11.340]   Right.
[01:51:11.340 --> 01:51:12.780]   So--
[01:51:12.780 --> 01:51:23.340]   Has there been a study, Alex, of the impact of tech company mistakes on the stock for the company
[01:51:23.340 --> 01:51:23.980]   long term?
[01:51:23.980 --> 01:51:24.460]   That's interesting.
[01:51:24.460 --> 01:51:24.940]   Yeah.
[01:51:24.940 --> 01:51:25.500]   I mean, I just--
[01:51:25.500 --> 01:51:28.300]   In terms of time or something else?
[01:51:28.300 --> 01:51:29.100]   No, no, no, no, no.
[01:51:29.100 --> 01:51:34.620]   Just in terms of like when Elon Musk does something incredibly stupid,
[01:51:34.620 --> 01:51:42.300]   like what is the long term impact on Tesla stock or this latest thing with Twitter?
[01:51:42.300 --> 01:51:45.820]   Like is Twitter stock really going to be impacted?
[01:51:45.820 --> 01:51:50.060]   Because my gut tells me that they don't matter at all.
[01:51:50.060 --> 01:51:51.820]   That's correct.
[01:51:51.820 --> 01:51:56.380]   I think the main answer is the revenue stream unchanging means the share price doesn't change.
[01:51:56.380 --> 01:52:00.540]   Because the stock price is just today's estimate of the value of future cash flows.
[01:52:00.540 --> 01:52:03.020]   So if those cash flows don't change, nothing really matters.
[01:52:03.020 --> 01:52:06.860]   So if Twitter goes down for a day even, it'll make that quarter slightly worse.
[01:52:06.860 --> 01:52:09.420]   But the long term trajectory of the business doesn't really shift.
[01:52:09.420 --> 01:52:14.700]   So this is why Facebook's stock isn't really under duress during this advertiser kind of like
[01:52:14.700 --> 01:52:18.300]   blackout search, whatever you want to call it, because people presume they'll all come back.
[01:52:18.300 --> 01:52:19.980]   And in five years, we won't even recall this.
[01:52:19.980 --> 01:52:21.740]   So why is it shares down now?
[01:52:21.740 --> 01:52:27.100]   Well, I'm trying to make a stock market seem less dumb.
[01:52:27.100 --> 01:52:32.060]   I mean, Tesla at $1,500 a share makes that point that things can get a little wild.
[01:52:32.060 --> 01:52:37.580]   But I mean, I think that investors tend to look at short term downtime as that short term downtime,
[01:52:37.580 --> 01:52:39.980]   as opposed to a long financial change.
[01:52:39.980 --> 01:52:40.540]   Yeah.
[01:52:40.540 --> 01:52:44.860]   But investors are also emotional, especially more so than ever right now.
[01:52:46.700 --> 01:52:50.460]   Sometimes they reactedversely to something that doesn't seem like it's going to hit the bottom
[01:52:50.460 --> 01:52:51.340]   line, particularly.
[01:52:51.340 --> 01:52:53.660]   But I agree with that.
[01:52:53.660 --> 01:52:54.780]   But also they do the opposite.
[01:52:54.780 --> 01:52:56.700]   Sometimes they overpay for things like Tesla.
[01:52:56.700 --> 01:52:57.180]   Tesla.
[01:52:57.180 --> 01:52:57.660]   Yeah.
[01:52:57.660 --> 01:53:01.820]   Not giving you investment advice, but if you are a fundamentals-minded investor,
[01:53:01.820 --> 01:53:04.620]   it's an interesting share price trying to deduce the reasons behind.
[01:53:04.620 --> 01:53:07.260]   And I think the real reasons are optimism and retail investors.
[01:53:07.260 --> 01:53:08.140]   Yeah.
[01:53:08.140 --> 01:53:14.220]   But remember, for a long time, Amazon's P to E was in the hundreds, wasn't it?
[01:53:14.220 --> 01:53:15.260]   Yeah.
[01:53:15.260 --> 01:53:17.180]   It was it was abnormally high.
[01:53:17.180 --> 01:53:20.620]   And a lot of people said, well, you see, that's a those are stupid people.
[01:53:20.620 --> 01:53:23.580]   I don't think anybody got hurt by an Amazon stock.
[01:53:23.580 --> 01:53:27.660]   No, in fact, you would have just be able to buy yourself a house or two if you bought far enough
[01:53:27.660 --> 01:53:27.980]   back.
[01:53:27.980 --> 01:53:28.220]   Yeah.
[01:53:28.220 --> 01:53:33.740]   But there's always an example that makes any sort of caution appear to be bad advice.
[01:53:33.740 --> 01:53:33.980]   Oh, yeah.
[01:53:33.980 --> 01:53:37.180]   I don't think that that caution is apparently the wrong way to approach things.
[01:53:37.180 --> 01:53:40.700]   Cloudflare, though, you know, worth $10, $11 billion today, a 1% hit.
[01:53:40.700 --> 01:53:43.900]   I think everyone's kind of shrugging this off as one person making an error.
[01:53:43.900 --> 01:53:45.900]   By the way, I came back incidentally.
[01:53:45.900 --> 01:53:47.260]   They're now at 5%.
[01:53:47.260 --> 01:53:47.740]   Right.
[01:53:47.740 --> 01:53:52.540]   So if they did this every weekend for six months, they might listen to this.
[01:53:52.540 --> 01:53:53.020]   It's right.
[01:53:53.020 --> 01:53:56.620]   There's no pattern here other than, you know, Matthew Prince on Twitter having to go out and
[01:53:56.620 --> 01:53:57.660]   talk about things publicly.
[01:53:57.660 --> 01:53:59.740]   Quickly, Todd Swashner.
[01:53:59.740 --> 01:54:00.620]   Constantly.
[01:54:00.620 --> 01:54:02.140]   Yeah.
[01:54:02.140 --> 01:54:04.300]   See, now, maybe I was stupid, but I thought,
[01:54:04.300 --> 01:54:11.660]   dumb me, that the COVID crisis that the the shutting down of Main Street, America,
[01:54:11.660 --> 01:54:17.100]   that the bankruptcy, that the millions of people unemployed, the bread lines,
[01:54:17.100 --> 01:54:19.260]   I thought that'll probably hit the stock market.
[01:54:19.260 --> 01:54:22.780]   So I got out of the stock market back in, I think, February or March.
[01:54:22.780 --> 01:54:25.020]   That was a dumb move.
[01:54:25.020 --> 01:54:26.540]   I guess I was too cautious.
[01:54:26.540 --> 01:54:27.340]   Who knew?
[01:54:27.340 --> 01:54:28.380]   Bread lines.
[01:54:28.380 --> 01:54:30.380]   Leo, people are all lining up for bread.
[01:54:30.380 --> 01:54:32.060]   It's good news.
[01:54:32.060 --> 01:54:32.620]   It's good news.
[01:54:32.620 --> 01:54:33.900]   Buy bread stock.
[01:54:33.900 --> 01:54:40.300]   I do find it very funny as a market observer lately that every time there's a rumor of a
[01:54:40.300 --> 01:54:42.860]   vaccine somewhere out in the future shares just surge.
[01:54:42.860 --> 01:54:44.780]   People, they're bullish about America.
[01:54:44.780 --> 01:54:45.260]   Yep.
[01:54:45.260 --> 01:54:48.860]   And whenever there's a 1.3 million new unemployed this week, the stock market goes,
[01:54:48.860 --> 01:54:50.220]   yeah, you know, it's going to be fine.
[01:54:50.220 --> 01:54:54.460]   I've lost my ability to kind of figure out what's going on.
[01:54:54.460 --> 01:54:57.820]   So I'm just sitting back, you know, with the metaphorical martini watching.
[01:54:57.820 --> 01:54:58.140]   Yeah.
[01:54:58.140 --> 01:54:58.940]   Yeah.
[01:54:58.940 --> 01:54:59.740]   Well, I know.
[01:54:59.740 --> 01:55:03.100]   I mean, in a pile of cash in a big tax bill, that's me.
[01:55:03.100 --> 01:55:05.340]   You played yourself, Leo.
[01:55:05.340 --> 01:55:05.980]   No, I did.
[01:55:05.980 --> 01:55:07.180]   I flayed myself.
[01:55:07.180 --> 01:55:11.660]   I still, this way, I don't have to look at the returns all the time.
[01:55:11.660 --> 01:55:15.500]   And I should mention just to reassure everybody, I never buy individual stocks
[01:55:15.500 --> 01:55:17.340]   because I'm not smart enough to do that.
[01:55:17.340 --> 01:55:21.500]   I buy by the dumbest, I buy index stocks.
[01:55:21.500 --> 01:55:26.460]   I buy the dumbest thing I could buy, you know, so I don't have any, any dog in this hunt.
[01:55:26.460 --> 01:55:34.140]   So having established my pro bonos, or no, my bono fides, or some Latin phrase,
[01:55:36.300 --> 01:55:38.540]   in terms of investing, I am puzzled.
[01:55:38.540 --> 01:55:43.340]   And I don't even know if it's a big story, but Google just bought a big chunk,
[01:55:43.340 --> 01:55:47.260]   a giant chunk, and they're not the only American company to do so,
[01:55:47.260 --> 01:55:56.540]   in GEO, JIO, which is, I think, the most valuable company in India, right?
[01:55:56.540 --> 01:55:57.900]   Reliance.
[01:55:57.900 --> 01:56:01.340]   Well, the owner is the richest person in India,
[01:56:01.340 --> 01:56:08.460]   Mukesh, Ambani. And yeah, so this is a company that is really,
[01:56:08.460 --> 01:56:10.460]   it's like a four-year-old company or something like that.
[01:56:10.460 --> 01:56:14.220]   And they're just kicking butt in terms of offering what the Indian market really,
[01:56:14.220 --> 01:56:21.500]   really wants, which is reasonably good quality, very low priced internet connectivity.
[01:56:21.500 --> 01:56:23.420]   And they're just a rocket ship.
[01:56:23.420 --> 01:56:26.300]   And so Facebook invested in them.
[01:56:26.300 --> 01:56:27.580]   A bunch of American companies did.
[01:56:27.580 --> 01:56:28.380]   Now Google is.
[01:56:29.180 --> 01:56:33.900]   And, but to me, the most interesting thing is that they're actually going to work with Google
[01:56:33.900 --> 01:56:39.980]   to develop an India-specific, presumably India-specific version of Android that will facilitate
[01:56:39.980 --> 01:56:48.860]   the low-cost distribution of a version of Android that does access the Play Store and does
[01:56:48.860 --> 01:56:53.740]   support 5G. So this is actually probably a very good thing for the Indian market.
[01:56:53.740 --> 01:56:56.700]   It's not the investment because they've got plenty of investment.
[01:56:56.700 --> 01:57:01.260]   It's the partnership with Google that will result in a better version of Android.
[01:57:01.260 --> 01:57:07.820]   So Reliance Industries, which is India's most highly valued firm, 10% now owned by Facebook.
[01:57:07.820 --> 01:57:17.340]   They put in 5.7 billion in April. Google just put in 4.5 billion for a 7% 8% stock.
[01:57:17.340 --> 01:57:20.460]   Google makes more, well, I guess they both make sense.
[01:57:20.460 --> 01:57:25.660]   Google making their own version of Android for Geo makes a lot of sense, I think.
[01:57:26.540 --> 01:57:30.620]   I think it's about access, Leo. So, you know, when I think about companies putting a lot of money
[01:57:30.620 --> 01:57:34.300]   into Geo, which is the subsidiary of Reliance, it's just the kind of the mobile telecom version.
[01:57:34.300 --> 01:57:39.580]   It's a 360, 380 million subscribers. If you want to have access to the Indian market because you
[01:57:39.580 --> 01:57:44.140]   think that you need to keep growing your revenues globally, having a stake and therefore maybe a
[01:57:44.140 --> 01:57:48.780]   bit of a say or some protection from the most important wireless carrier is critical, especially
[01:57:48.780 --> 01:57:52.220]   after Facebook pulled the trigger on its deal back in April. Now everyone's scrambling to put
[01:57:52.220 --> 01:57:56.540]   money into the company. It's not just these two groups. It's many other people trying to get a
[01:57:56.540 --> 01:58:01.020]   piece of this for two reasons, upside and then of course access. I mean, if Facebook is not going
[01:58:01.020 --> 01:58:06.540]   to play particularly fair in India, if they can use its ownership to box out rivals,
[01:58:06.540 --> 01:58:10.780]   not everyone has to have a piece for safety. Also, we spend a lot of this show talking about China
[01:58:10.780 --> 01:58:15.180]   and the lack of access to that market. Well, India's 1.3 billion people. And so you probably
[01:58:15.180 --> 01:58:19.660]   want to make sure you have a big open door to all those folks. And so it's about access. It's
[01:58:19.660 --> 01:58:25.020]   about partnerships. It's about money as well. It's probably the most interesting story in tech today
[01:58:25.020 --> 01:58:28.380]   that isn't a crisis. I feel like everything else we talk about is not a crisis. This is just
[01:58:28.380 --> 01:58:33.260]   yeah, this is optimistic. This is about the future of having many more people on the internet and
[01:58:33.260 --> 01:58:38.540]   some capacity. And so in one way, it's fun. But I think it's also very strategic. And the money
[01:58:38.540 --> 01:58:46.700]   is used here to buy bargaining chips. Right now the geophones use KaiOS or KaiOS, which is kind of
[01:58:47.660 --> 01:58:51.980]   it's not a smartphone. It's not a feature phone. It's kind of in between. It does run apps,
[01:58:51.980 --> 01:58:58.300]   including WhatsApp and YouTube and Facebook and Google Maps. Google is an investor in KaiOS.
[01:58:58.300 --> 01:59:04.380]   So they have some stake in that. But it makes sense for Google to put what sounds like a lot of
[01:59:04.380 --> 01:59:12.140]   money, $4.5 billion into Reliance. But that's a pittance for Google, especially to get them into
[01:59:12.140 --> 01:59:17.980]   that market and to get Android on those KaiOS phones. So money will especially.
[01:59:17.980 --> 01:59:24.540]   This has been a long time desire of Google to get Android working on cheaper phones, on cheaper
[01:59:24.540 --> 01:59:30.060]   hardware and get it functioning as if Android normally would. And if they can pull this off with
[01:59:30.060 --> 01:59:36.540]   India, then suddenly a whole bunch of other markets, especially those in Africa, become more open to
[01:59:36.540 --> 01:59:44.540]   Google in a way that they're not open to Apple in a way that Facebook hasn't really been investing.
[01:59:44.540 --> 01:59:48.620]   The only other major investor that I'm aware of over there has been China.
[01:59:48.620 --> 01:59:55.980]   So if Google here is proxy for the US, that's probably a good thing geopolitically.
[01:59:55.980 --> 02:00:00.540]   And maybe Google was attracted because Geo has something called Geo Glass.
[02:00:02.380 --> 02:00:10.620]   Oh gosh, that's a throwback. Which by the way is their second like virtual reality/agmented
[02:00:10.620 --> 02:00:17.580]   reality product. They dumped the first one because it just wasn't going to work. And this one isn't
[02:00:17.580 --> 02:00:21.740]   going to work either. But it's clear that they have a lot of money to throw around because of all
[02:00:21.740 --> 02:00:26.860]   the investment. And it's clear that they know what is true, which is that augmented reality,
[02:00:26.860 --> 02:00:32.940]   virtual reality, etc. is going to be gigantic, including in India.
[02:00:32.940 --> 02:00:39.900]   Yeah. Walmart also a big investor in an Indian company. I think a lot of American companies see
[02:00:39.900 --> 02:00:46.220]   this as a big growth opportunity. So thank you for explaining that, Alex. I now understand.
[02:00:46.220 --> 02:00:50.460]   It's a cool moment, Leo. I'm glad we're talking about the story on this show because I do think
[02:00:50.460 --> 02:00:54.860]   we have an American focus here in the US and we forget that we are a relatively small country
[02:00:54.860 --> 02:00:59.100]   in terms of population compared to several. And so the story is that start over there could really
[02:00:59.100 --> 02:01:02.380]   impact us over here later on. So keep an eye on Geo if you're listening to the show.
[02:01:02.380 --> 02:01:11.180]   Apple joining a list of companies eliminating racist terms in their language.
[02:01:11.180 --> 02:01:18.540]   GitHub eliminated the master branch. The Linux kernel community has decided not to use Blacklist,
[02:01:18.540 --> 02:01:25.020]   Whitelist, Master and Slave in their terms. They're going to replace them with frankly terms that
[02:01:25.020 --> 02:01:31.340]   are more descriptive. I mean, Blacklist and Whitelist isn't nearly as good as Denylist and Allowlist.
[02:01:31.340 --> 02:01:35.500]   That makes a lot more sense. It tells you a lot more about what it is. We're just and the things
[02:01:35.500 --> 02:01:40.220]   we're so inured to it because we've used those terms for so long. We forget that they are offensive
[02:01:40.220 --> 02:01:46.540]   to a large portion of the world. And Blacklist, Whitelist is one thing, but Master and Slave is
[02:01:46.540 --> 02:01:54.300]   really pretty bad. But Host or Controller and Client, those are more descriptive. I'm happy to see
[02:01:54.300 --> 02:01:59.100]   that. Twitter's doing the same thing. I have a little more problem with the master branch
[02:01:59.100 --> 02:02:06.140]   because I don't think it's Master Slave. We talk about mastery and mastering something. Is that the
[02:02:06.140 --> 02:02:12.460]   same? I guess it's the same root. Different use. There are lots and lots of words that
[02:02:13.020 --> 02:02:19.420]   were associated with slavery. One of them is even more out there is plantation,
[02:02:19.420 --> 02:02:26.620]   which is being objected to. My own view is that this is one of the ways that language evolves.
[02:02:26.620 --> 02:02:33.020]   There are lots of us who have the luxury of not being offended by words like plantation and
[02:02:33.020 --> 02:02:38.860]   master, etc. But there are a lot of people for whom, lots and lots of people who hear those words,
[02:02:38.860 --> 02:02:45.740]   and they just cringe. And since the truth is, we don't care that much and they do care. I say,
[02:02:45.740 --> 02:02:50.700]   let's change it. Let's do it. Yeah. But as somebody who was born in Los Angeles County,
[02:02:50.700 --> 02:02:54.140]   let me give a shout out to Los Angeles County because this is like a trendy thing right now
[02:02:54.140 --> 02:03:00.220]   because of Black Lives Matter. But Los Angeles County was demanding that computer suppliers in
[02:03:00.220 --> 02:03:07.420]   2003 not use the words master and slave for their technology products in any way. And so,
[02:03:08.140 --> 02:03:12.060]   a shout for Los Angeles County that was 17 years ahead on this.
[02:03:12.060 --> 02:03:18.540]   You could not be a racist and kind of be unconscious about it. So I think it's okay to
[02:03:18.540 --> 02:03:24.220]   bring it up because it is hurtful to people, to a lot of people. And there's no reason to use
[02:03:24.220 --> 02:03:28.060]   hurtful language if you can find something that is not only not hurtful but actually
[02:03:28.060 --> 02:03:33.500]   technically more descriptive. And there's nothing, there's no reason to cling to these terms.
[02:03:33.500 --> 02:03:40.140]   Who cares? Right. Who cares? Exactly. So two points I want to bring up that are related to this.
[02:03:40.140 --> 02:03:48.220]   One is, is that, well, most of what I've seen of the InfoSec community's discussion of this on
[02:03:48.220 --> 02:03:54.700]   Twitter has been fairly reasonable with some debate about white and blacklist versus a
[02:03:54.700 --> 02:04:02.380]   lower block list and things like that. There was also one Google security person who was going to
[02:04:02.380 --> 02:04:06.940]   speak at Black Hat, which is the big angle. What are they going to call Black Hat now?
[02:04:06.940 --> 02:04:12.860]   Well, so that's really interesting because it looks like they're having to change things.
[02:04:12.860 --> 02:04:18.700]   Right. And so one Google executive said that he wasn't going to speak,
[02:04:18.700 --> 02:04:27.660]   which actually created a bit of backlash from some InfoSec people who pointed out that for one
[02:04:27.660 --> 02:04:35.180]   thing, nobody who is a bi-POC InfoSec person was really calling for that change at Black Hat.
[02:04:35.180 --> 02:04:43.420]   And for another, and really the bigger point is, is that while this changing of terminology
[02:04:43.420 --> 02:04:47.980]   is important and there are people who are arguing for it, what's really important is making sure that
[02:04:47.980 --> 02:04:55.900]   these tech companies start implementing much better diversity programs, have much better pay scales,
[02:04:56.540 --> 02:05:03.820]   and really these HR departments that are protecting companies and preventing them from moving forward
[02:05:03.820 --> 02:05:12.940]   need to get on the ball and really start diversifying these organizations and making them fair.
[02:05:12.940 --> 02:05:18.540]   It's not enough to just bring in by POC. You also have to make sure that the environment
[02:05:18.540 --> 02:05:25.500]   is going to set everybody up for success, not just the white men who've already been there.
[02:05:25.500 --> 02:05:33.260]   Absolutely. I want to nominate Bad Hat because in French, in French, Bad Hat is exactly what,
[02:05:33.260 --> 02:05:39.180]   this was a book I read to my child when they were a little Madeleine in the Bad Hat. It was a
[02:05:39.180 --> 02:05:44.940]   bad, a bratty kid. He was a bad hat. So now I don't know what you're going to call White Hat,
[02:05:44.940 --> 02:05:50.620]   good hat, good hat, fantastic hat. We just need a whole new name for that conference.
[02:05:51.420 --> 02:05:57.340]   I mean, the idea of White and Black as bad and good, that's really bad. That's bad.
[02:05:57.340 --> 02:06:00.540]   That's bad. That's always back much further, I think, than...
[02:06:00.540 --> 02:06:07.900]   Yeah. Yeah. It doesn't have race. It doesn't have race origins. It has more to do with the
[02:06:07.900 --> 02:06:15.420]   darkness of night. It was a black and stormy night, etc. But it doesn't matter because these,
[02:06:15.420 --> 02:06:22.540]   because words have connotations, words imply things. Yeah. I just wonder what they're thinking
[02:06:22.540 --> 02:06:29.100]   at Red Hat. I think Red Hat's okay. Unless you're a commie. Yeah.
[02:06:29.100 --> 02:06:34.460]   Or a blue hat. I mean, Microsoft has its blue hat hacker thing.
[02:06:34.460 --> 02:06:40.300]   Maybe we should ban hats. Who wears hats anymore, anyway?
[02:06:40.300 --> 02:06:44.300]   I did today because I was out in the sun, but I want to point out how broad this is going.
[02:06:44.300 --> 02:06:47.500]   Leads from Rhode Island, where I now currently live, and Rhode Island used to be called the
[02:06:47.500 --> 02:06:51.420]   state of Rhode Island and Providence Plantations. It officially still is my friend.
[02:06:51.420 --> 02:06:57.020]   It's not too clarified. Yes. There's been an executive order to pursue a name change, but I'm
[02:06:57.020 --> 02:07:03.260]   amazed at... Yes, Gina has signed that. By the way, Gina, great governor. You are lucky,
[02:07:03.260 --> 02:07:08.780]   because you are living in a state that is practically COVID-free thanks to a brilliant governor there.
[02:07:08.780 --> 02:07:13.420]   And do the fact that I've been at home for five months. Yes. You are no longer the super spreader.
[02:07:13.420 --> 02:07:16.780]   You once were. No, but I'm excited to see the name.
[02:07:16.780 --> 02:07:21.820]   That was his nickname. Alex super spreader Wilhelm, but I leave it and had to do with COVID.
[02:07:21.820 --> 02:07:23.420]   I'm being defamed.
[02:07:23.420 --> 02:07:29.580]   No, I'm just saying that as we talk about this inside of technology, it's happening all over
[02:07:29.580 --> 02:07:32.940]   the country in other ways as well. So it's not like we're singling out tech and saying tech is
[02:07:32.940 --> 02:07:36.940]   bad. Tech needs to change. Because of plantation. Yeah, plantation is pretty
[02:07:36.940 --> 02:07:41.740]   pretty onerous there. But there were no slaves to Providence in Rhode Island. But that's interesting.
[02:07:41.740 --> 02:07:47.100]   I'm not sure the history of that one. I don't think so. Yeah, I think there were.
[02:07:47.100 --> 02:07:51.260]   Were there? I think Roger Williams. I think Roger Williams is not into the slave thing. I
[02:07:51.260 --> 02:07:57.020]   said my memory of it. But you know what? Stand corrected if that's not true. It's difficult because
[02:07:57.020 --> 02:08:05.980]   I think I would love to see us for the future change all these terms. But I hope we're forgiving
[02:08:05.980 --> 02:08:13.180]   about the culture of the past. I'm reading the original Wizard of Oz right now. Yes. And in this
[02:08:13.180 --> 02:08:17.260]   book, which was written, I believe published, I believe in 1901. Are you reading that to Princess
[02:08:17.260 --> 02:08:22.540]   Squishy face or is this for your own? I started to, but she really likes the movie better. And
[02:08:22.540 --> 02:08:26.700]   she's bothered by the differences. So I kind of backed off on reading it to her. Maybe in the
[02:08:26.700 --> 02:08:34.620]   future, I will. Lots and lots of differences. But basically, it's very everybody is very color
[02:08:34.620 --> 02:08:39.980]   centric in terms of their clothing in that book. And so the way you know the difference between
[02:08:39.980 --> 02:08:45.180]   a good witch and a bad witch is that the good witch is all wear white. And the bad witch is all
[02:08:45.180 --> 02:08:51.580]   wear black. And this is the kind of thing that you'll find throughout culture and history,
[02:08:51.580 --> 02:08:57.820]   these references to black being associated with badness and white being associated with goodness.
[02:08:57.820 --> 02:09:03.660]   And so I hope we don't attack the past in this way in terms of language and culture.
[02:09:04.300 --> 02:09:11.180]   But I would like us to see us fix the future. And I do stand corrected in my memory. I actually,
[02:09:11.180 --> 02:09:15.900]   I think was blanking it out, but the slave trade was very active in Rhode Island.
[02:09:15.900 --> 02:09:21.180]   So part of the state's history. Yeah, I forgot all about that. In fact, Brown University,
[02:09:21.180 --> 02:09:28.300]   John Brown, the founder of Brown University, was a slave trader. So yeah, I forgot all about that.
[02:09:28.300 --> 02:09:33.900]   It was part of the triangle trade. Yeah, it's not not great. But we can always do better. I think,
[02:09:33.900 --> 02:09:37.180]   I think Mike makes a perfectly fine point. We don't need to go back and with a pair of
[02:09:37.180 --> 02:09:41.340]   scissors, take out all the words. We still read Mark Twain, which is one of his books as a pretty
[02:09:41.340 --> 02:09:47.260]   famous example of how language looks pretty bad in retrospect. But I think we can only not do that
[02:09:47.260 --> 02:09:51.580]   if we devote ourselves to making a better future and say, Okay, cool, we used to do that. Now we
[02:09:51.580 --> 02:09:56.540]   know better. Now we're going to do better. And I think it's a words are a starting point, but we
[02:09:56.540 --> 02:10:03.260]   got a lot more work to do another in a more concrete ways. And I think, for instance,
[02:10:03.260 --> 02:10:08.460]   diversity in these companies is really important. And it will help solve this problem.
[02:10:08.460 --> 02:10:15.980]   You know, it's funny. You're one of the places when I was at Tuedlio, we did a big
[02:10:15.980 --> 02:10:25.260]   show on diversity and gender in tech companies. And basically, our panel made it very clear that
[02:10:25.260 --> 02:10:32.220]   this all begins with education and universities and stuff like that. And I was very happy to hear
[02:10:32.220 --> 02:10:37.420]   what I thought was a big piece of news coming out from my own monitor, which is I went to UCLA,
[02:10:37.420 --> 02:10:43.900]   but the UC systems incoming freshman class is the majority of them are Hispanic.
[02:10:43.900 --> 02:10:49.740]   Awesome. And so this is like, you know, talk about putting your money where your mouth is.
[02:10:49.740 --> 02:10:53.740]   Everybody's always talking about, Oh, we need more diversity. We need this. We need that.
[02:10:53.740 --> 02:11:00.060]   The California State University system exists as a place to have to give a high quality university
[02:11:00.060 --> 02:11:08.860]   education to Californians who for one reason or another, being denied that education elsewhere.
[02:11:08.860 --> 02:11:14.140]   And in fact, you know, the in terms of the college age population in California,
[02:11:14.140 --> 02:11:19.100]   the majority is Hispanic now. And for the University of California to represent that majority,
[02:11:19.100 --> 02:11:27.100]   they don't quite meet them the same percentage as the population itself, but they're approaching it.
[02:11:27.100 --> 02:11:31.660]   That's a huge deal. And I hope that is also reflected in engineering schools as well,
[02:11:31.660 --> 02:11:36.620]   because because one of the big, we talk about who's underrepresented, who's overrepresented in
[02:11:36.620 --> 02:11:43.820]   technology, probably one of the biggest groups that's underrepresented is Hispanics in technology.
[02:11:43.820 --> 02:11:50.540]   I agree. Yeah. And so and so this is I think this is a big deal and technology industry,
[02:11:50.540 --> 02:11:54.540]   Silicon Valley, it can be done. You can do this. Look at the University of California.
[02:11:54.540 --> 02:11:59.500]   It's just a matter of time. It might have already happened before California. In California,
[02:11:59.500 --> 02:12:07.020]   anyway, whites are a minority. I don't know if it's happened yet, but I know it's demographically soon.
[02:12:08.780 --> 02:12:14.540]   Not that I bothers me in the least, honestly. Let's take a little break. When we come back,
[02:12:14.540 --> 02:12:20.380]   we're going to say farewell to a friend of the network and a friend of Geeks everywhere.
[02:12:20.380 --> 02:12:26.860]   But first word from our sponsor, the people who buy us this beautiful studio, the LastPass studios,
[02:12:26.860 --> 02:12:32.460]   of course, our friends at LastPass, keeping us safe for more than a decade. In fact,
[02:12:32.460 --> 02:12:37.180]   it's exactly a decade ago that Steve Gibson. So I've been using LastPass for a few years,
[02:12:37.180 --> 02:12:42.700]   and I've been singing its praises to Steve Gibson. And about it was exactly 10 years ago that he
[02:12:42.700 --> 02:12:47.260]   talked to Joe Seagrest, the creator of LastPass, Joe showed him the code,
[02:12:47.260 --> 02:12:51.260]   explained what they did. And Steve was super impressed that a whole show on it,
[02:12:51.260 --> 02:12:57.020]   a whole security now on it, and started using it. Steve's been using it ever since for a decade
[02:12:57.020 --> 02:13:04.060]   because it's done right. It's secure. It's done right. We started using LastPass enterprise
[02:13:04.060 --> 02:13:10.300]   in the business because we realized all well and good if people work here know about security
[02:13:10.300 --> 02:13:14.300]   and password. But we have a lot of people in the business office and others who maybe don't have
[02:13:14.300 --> 02:13:20.300]   the same understanding or maybe reusing passwords or maybe putting passwords on post-it notes on
[02:13:20.300 --> 02:13:26.620]   their screen. And those people have by virtue of their job access to our most important data,
[02:13:26.620 --> 02:13:33.100]   everything, our websites, our databases, our bank accounts. So we decided, you know,
[02:13:33.100 --> 02:13:39.820]   we better use LastPass at work. It is a challenging time for your IT department. Very big job right
[02:13:39.820 --> 02:13:44.860]   now. There's more devices. People are bringing their own. There's more applications and people
[02:13:44.860 --> 02:13:49.740]   are going home to work. So they're not even working on your company network anymore.
[02:13:49.740 --> 02:13:55.580]   Add that to new threats, new regulations, strong securities gotten harder than ever. Fortunately,
[02:13:55.580 --> 02:14:01.980]   LastPass is here to be your partner, to provide your IT with strong security, both easy to use
[02:14:02.540 --> 02:14:07.900]   and easy to manage. Both are important. If employees don't like LastPass, if they don't find it easy,
[02:14:07.900 --> 02:14:14.860]   then there's no point in using it. They do. They love it. If IT can't manage it and get centralized
[02:14:14.860 --> 02:14:20.380]   control, then you're not doing the security job you need to. LastPass secures every entry point
[02:14:20.380 --> 02:14:26.860]   from shadow IT to apps to mobile and cloud services. The key here is their access solutions give you
[02:14:26.860 --> 02:14:33.420]   the IT department visibility and control over every access point to your organization.
[02:14:33.420 --> 02:14:40.540]   With employees using their own devices, their own phones, they're at home, suddenly you lose
[02:14:40.540 --> 02:14:45.740]   track of who's using what where. But LastPass identity solves this. It offers a simple,
[02:14:45.740 --> 02:14:52.620]   integrated view across every access and authentication task. So you can see who's accessing what,
[02:14:52.620 --> 02:14:57.900]   when they're accessing it, where they are. That's the information you need to keep your stuff safe.
[02:14:57.900 --> 02:15:02.940]   Too bad Twitter didn't know about that. Increased security without increasing productivity.
[02:15:02.940 --> 02:15:07.100]   Your employees will love it. Things like single sign on, which makes, you don't even use it
[02:15:07.100 --> 02:15:12.300]   Passwords. It's just easy. Multi-factor authentication, adding contextual factors
[02:15:12.300 --> 02:15:18.620]   to fingerprint and face recognition. Things like geo location, IP addresses, reduces friction,
[02:15:19.500 --> 02:15:23.980]   and prevents threats better. It's kind of the holy grail in security. More convenient and
[02:15:23.980 --> 02:15:31.580]   more secure. Is that even possible? It is. And of course, passwords, which are the number one
[02:15:31.580 --> 02:15:37.180]   vulnerability in the world right now. LastPass's password management is the best. It secures all
[02:15:37.180 --> 02:15:42.620]   those passwords, generates them. You don't have to think about it. It lets employees do their work
[02:15:42.620 --> 02:15:46.860]   securely, whether the office or working from home. When employees want to share passwords,
[02:15:46.860 --> 02:15:52.620]   they don't have to make phone calls, send emails with your password in it. Text messages.
[02:15:52.620 --> 02:15:57.820]   LastPass has secure password sharing built right in. And of course, I don't need to say it,
[02:15:57.820 --> 02:16:05.180]   but LastPass follows the best security practices. 256 AES encryption. They never transmit the
[02:16:05.180 --> 02:16:10.540]   password back to LastPass. They don't know your password. Only you do data is only decrypted on
[02:16:10.540 --> 02:16:15.500]   your devices. And by the way, every device everywhere you want to be. Android, iOS,
[02:16:15.500 --> 02:16:22.380]   Mac, Windows, Linux, every browser. It is the best. It's what I use. It's what Steve uses. It's
[02:16:22.380 --> 02:16:27.260]   what we've recommended for years. Secure your business plus give your IT department the tools
[02:16:27.260 --> 02:16:33.740]   they need to keep your business safe. Visit LastPass.com/twit to find out how they can help your business
[02:16:33.740 --> 02:16:40.940]   stay productive and secure. No matter where your employees are working from, LastPass.com/twit,
[02:16:40.940 --> 02:16:48.540]   we thank them so much for their support of this weekend tech and the studio naming rights.
[02:16:48.540 --> 02:16:56.140]   I feel like we're a football stadium or something. It's kind of a shock.
[02:16:56.140 --> 02:17:06.380]   And I was very sad to see Grant Imahara, who was just, if you watch Mythbusters, of course,
[02:17:06.380 --> 02:17:14.220]   you know how great Grant was. Passed away suddenly from aneurysm at the age of 49.
[02:17:14.220 --> 02:17:22.460]   I will never forget Grant coming to the Brick House back in 2013 for a triangulation episode.
[02:17:22.460 --> 02:17:29.980]   First thing I knew is people were going, "Grandimahara is here. He's grandimahara." But he was so
[02:17:29.980 --> 02:17:37.180]   unassuming, so sweet, so normal. He was friendly, he said, "Hi to everybody." I didn't get to do
[02:17:37.180 --> 02:17:43.420]   the interview. I was jealous because I, as actor, got to do the interview. But I did get to talk to
[02:17:43.420 --> 02:17:50.540]   Grant and everybody just was so impressed by him. He was such a sweet guy and it's such a loss
[02:17:50.540 --> 02:17:56.940]   to the geek community. He was a professional engineer. I did not know this. I probably should.
[02:17:56.940 --> 02:18:04.300]   But he designed rebuilt R2-D2 for the prequels and was one of three operators during the movies.
[02:18:04.300 --> 02:18:10.220]   He was amazing. He built the robot that Craig Ferguson used.
[02:18:10.220 --> 02:18:18.140]   Jeff Peterson? Who's that? The robot's name was Jeff Peterson? That's a pretty mundane name for a robot.
[02:18:21.740 --> 02:18:28.300]   Yeah, Jeff, the robot that Craig Ferguson, GEO, oh well that makes it all the better.
[02:18:28.300 --> 02:18:34.780]   Jeff Peterson. And he would grant him a horror, a surprise shocking, passed away at the age of
[02:18:34.780 --> 02:18:42.540]   49. A really talented electrical engineer, roboticist, and of course, myth buster. He will be missed.
[02:18:44.860 --> 02:18:52.220]   I don't want to end with such a sad note. Oh, I could play a promo. That'll cheer us up.
[02:18:52.220 --> 02:18:57.180]   I forgot all about it. A little movie celebrating the week of the promo to cheer
[02:18:57.180 --> 02:19:02.700]   a little promo cheers me up. I don't know how you, before I always could. It was a house ad.
[02:19:02.700 --> 02:19:11.660]   Previously on Twitter. Happy birthday to Jeff Jarvis. Oh my god. It's a giant taco cake.
[02:19:11.660 --> 02:19:17.420]   Taco cake. Windows weekly. We are really stuck on the window side with a compatibility problem
[02:19:17.420 --> 02:19:21.980]   that we can't get past because its business customers rely on stuff from the 1980s or 90s or
[02:19:21.980 --> 02:19:27.500]   whatever it is. They just can't do it. And that is a problem arm does not solve. Tech news weekly.
[02:19:27.500 --> 02:19:33.180]   Twitter experienced its worst hijacking to date. What was interesting about this was that it was so
[02:19:33.180 --> 02:19:40.060]   big. I mean, it was like so many different high profile accounts that it immediately
[02:19:40.060 --> 02:19:48.060]   seemed like an inside job. Panzontech. I am going to show you my favorite dock for the Mac.
[02:19:48.060 --> 02:19:56.780]   It's the CalDigit Thunderbolt Station 3 plus and it is incredible. This week in Google.
[02:19:56.780 --> 02:20:02.860]   Instead of having one of those Amazon grab-and-go stores near you, what if every store had an
[02:20:02.860 --> 02:20:08.540]   Amazon grab-and-go shopping cart? It's called a dash cart. You don't have to scan things. You
[02:20:08.540 --> 02:20:13.740]   just throw them in. When you are done shopping, you take the cart through a special lane. It's
[02:20:13.740 --> 02:20:18.540]   just like fast track for your shopping and you're done. Twitter for help with the technology
[02:20:18.540 --> 02:20:25.180]   addiction problem call 1-800-TWET. I don't like these cuz they make my ears stick out.
[02:20:25.180 --> 02:20:33.980]   And I don't like to look weird. That's what I worded to the grocery store. Actually, I look
[02:20:33.980 --> 02:20:41.100]   even weird enough because I have a Dental 3D printed, what they call a fitter that was 3D
[02:20:41.100 --> 02:20:45.900]   modeled by my face that holds the mask in place so that it's sealed. So now I look like I'm wearing
[02:20:45.900 --> 02:20:52.220]   a muzzle. I look like a handball actor. But I'm safe. So did you want to talk about that grab-and-go
[02:20:52.220 --> 02:20:57.820]   cart? Mike, I thought you mentioned that earlier. Well, yeah, it's very interesting because, of
[02:20:57.820 --> 02:21:04.380]   course, Amazon owns they own Whole Foods, which is a pretty big grocery chain store. And there's
[02:21:04.380 --> 02:21:11.100]   intending to sort of roll out their concept stores that include, originally they're called
[02:21:11.100 --> 02:21:16.220]   Amazon Go, which are tiny little markets where you just go in and you shoplift and then they charge
[02:21:16.220 --> 02:21:22.300]   you anyway. Just grab whatever you're talking about. It's an interesting take. Okay. What's
[02:21:22.300 --> 02:21:27.820]   fascinating about it is that they have these carts that are, they basically have almost no
[02:21:27.820 --> 02:21:32.540]   capacity because almost all of the space of the car is taking off that look in the picture.
[02:21:32.540 --> 02:21:37.900]   Yeah. They're so small, in fact, that they're Woodland Hills, I think it is a store where they're
[02:21:37.900 --> 02:21:42.860]   going to roll this out near Los Angeles. They're going to offer both carts. So if you only want a
[02:21:42.860 --> 02:21:46.300]   couple of things, you can get the high-tech cart. But if you're going to get a normal shopping
[02:21:46.300 --> 02:21:51.820]   load, we'll just get the old-fashioned thing and do the checkout the normal way. But they're
[02:21:51.820 --> 02:21:57.740]   dash carts, they're called. Actually, I'll allow you the best part is they allow you to weigh the
[02:21:57.740 --> 02:22:04.700]   produce that you're buying in the cart. They have scans. Yeah. So that's pretty cool. And you can't
[02:22:04.700 --> 02:22:10.540]   fill them too high. Otherwise, the cameras and all the sensors can't tell what you're buying.
[02:22:10.540 --> 02:22:14.460]   That's the whole point. You put some products in this cart and it says, "Oh, we know what you're
[02:22:14.460 --> 02:22:19.660]   buying. We know who you are. We'll just charge you for this stuff when you walk out." So it's kind of
[02:22:19.660 --> 02:22:24.380]   a cool concept. But I think that they have many years to go before this is kind of a mainstream
[02:22:24.380 --> 02:22:28.780]   thing that can roll out into all the whole foods. And I just think that during a pandemic,
[02:22:28.780 --> 02:22:34.940]   it's actually kind of ideal. You sterilize the handle of the cart and then you never interact
[02:22:34.940 --> 02:22:40.060]   with a cashier, you never swapping credit cards, any of that stuff. It's all just charge your Amazon
[02:22:40.060 --> 02:22:48.060]   credit card the same when you use for your prime account or whatever. It's pretty cool that we're
[02:22:48.060 --> 02:22:54.140]   trying to figure out this technology. But like all of these things that require kind of AI and
[02:22:54.140 --> 02:22:59.180]   image recognition, which includes self-driving cars, it's going to take a lot longer for us to
[02:22:59.180 --> 02:23:03.740]   really nail this than we think it is. And I think that their little tiny carts that only allow you
[02:23:03.740 --> 02:23:08.780]   to get a couple of items is a perfect example of how you have a super aggressive company that's
[02:23:08.780 --> 02:23:15.180]   trying to make this mainstream. But they're really clearly not ready for the way Americans buy
[02:23:15.180 --> 02:23:20.060]   groceries, which is in large quantities. Yeah, I mean, nowadays, most Americans don't even use
[02:23:20.060 --> 02:23:27.820]   shopping carts. They use those giant pallet trucks. That's right. They load everything on.
[02:23:27.820 --> 02:23:33.100]   But I give Amazon a little more credit. They will evolve this quickly. I think that
[02:23:33.100 --> 02:23:38.060]   look, it's pretty clear Jeff Bezos wants to own every possible market.
[02:23:38.060 --> 02:23:43.740]   Yeah, but we've all used the sell checkouts that Safeway or those in Paris and
[02:23:43.740 --> 02:23:46.860]   sell checkouts. And they are the worst thing in the world. Yeah, used them once.
[02:23:46.860 --> 02:23:52.300]   And they're static. This has to work on wheels, which has to have a connection and cameras that
[02:23:52.300 --> 02:23:56.860]   don't get blocked and so many things. This to me looks like, I remember when Amazon showed off
[02:23:56.860 --> 02:24:00.780]   the drone deliveries that still haven't happened like five years ago and got an amazing press run
[02:24:00.780 --> 02:24:06.940]   out of that. This feels like that. Five years ago. Yeah, Charlie Rose. At least. Yeah, I got full.
[02:24:07.660 --> 02:24:14.300]   It's terrible when you try to buy alcohol just as one point because you still have to go interact
[02:24:14.300 --> 02:24:19.020]   with the cashier. Right. If you're trying to use it as anything that any normal adult would want
[02:24:19.020 --> 02:24:27.180]   to go do, except for our sober friends, it winds up being a bigger hurdle because I have to find
[02:24:27.180 --> 02:24:32.380]   cashier. I think it's telling the sexier. Well, yeah, but you just can't buy alcohol.
[02:24:33.340 --> 02:24:41.500]   I just want to buy alcohol with that interact. But honestly, if this cart were in Europe,
[02:24:41.500 --> 02:24:47.100]   the Europeans would say, well, it's so big. How am I supposed to fit it? It's tired. Well, I brought
[02:24:47.100 --> 02:24:52.860]   my ring bag. Europe needs it badly. I mean, I spent a lot of time in Europe and one of the biggest
[02:24:52.860 --> 02:24:59.820]   chains there is Car Four, which is a UK company. And in Car Four's, you have to actually wait in line
[02:24:59.820 --> 02:25:06.940]   to weigh your produce. Oh, that's silly. One item at a time. So if you buy six or seven or eight
[02:25:06.940 --> 02:25:11.900]   produce items, that's six or seven or eight times that you have to wait in line to wait a minute.
[02:25:11.900 --> 02:25:16.220]   You can only wait one thing at a time. Oh, no. I mean, you typically would like,
[02:25:16.220 --> 02:25:21.340]   you can't go with like a whole bunch of things. You get stink eye from people who were also waiting.
[02:25:21.340 --> 02:25:27.820]   See one item in line. Oh, less. Thank you. Go wait in line to check everything out.
[02:25:27.820 --> 02:25:31.820]   Cheese. The horribly they would love. They don't get grocery stores.
[02:25:31.820 --> 02:25:37.660]   That's because you're supposed to go out every night and carefully pick everything for that day's
[02:25:37.660 --> 02:25:42.460]   dinner. And that's it. And then you'll shop again tomorrow. You're not supposed to be
[02:25:42.460 --> 02:25:47.820]   yeah, that's not that works in little tiny French towns. And it's a beautiful thing. But when you're
[02:25:47.820 --> 02:25:53.980]   in Barcelona, where? Yeah. Yeah. Every time you go out the door, you're at risky getting pickpocketed.
[02:25:54.620 --> 02:26:01.020]   That's right. All right. Cheer you up. I got to cheer you up. Here are the new emoji arriving on
[02:26:01.020 --> 02:26:07.100]   iOS and Android later this year with the new versions of iOS and Android Googles bringing back
[02:26:07.100 --> 02:26:15.740]   the little mushy turtle that we all didn't miss. What's the first thing? That is either
[02:26:15.740 --> 02:26:24.060]   streamers or jellyfish. I don't know. Wedding bells. Aliens. Oh, it might be wedding bells.
[02:26:24.060 --> 02:26:32.780]   That's this is the this is the problem is the way this works. Friday was a world emoji day,
[02:26:32.780 --> 02:26:38.300]   which Jeremy Broog just made up. But more power to you, Jeremy. We love you.
[02:26:38.300 --> 02:26:44.460]   The problem is that every company, all the Unicode consortium does is give you a description
[02:26:44.460 --> 02:26:49.660]   and a reference that nobody uses. Then every company draws their own. So these are Google's
[02:26:49.660 --> 02:26:55.500]   images. Like what is I don't know. These are terrible. What is that a tamale? But what the hell's in it?
[02:26:55.500 --> 02:26:58.220]   That's a tamale. But what's in it? It looks like a little bit chocolate.
[02:26:58.220 --> 02:27:00.140]   Beans.
[02:27:00.140 --> 02:27:08.060]   Google's artists are challenged. Maybe they don't go outside. You know, the hug though. Look at Apple.
[02:27:08.060 --> 02:27:15.260]   Yeah. The hugs. It's nice except it's weirdly amorphous blue blobs that are hugging, not humans.
[02:27:15.260 --> 02:27:19.900]   Well, we're all out of shape because of COVID. They don't have any fingers. So I like apples.
[02:27:19.900 --> 02:27:25.420]   When you go to Apple, they look how nice these are. Those lungs look like they're really your lungs.
[02:27:25.420 --> 02:27:30.300]   And the and the tamale has actual tamale filling instead of, I don't know what,
[02:27:30.300 --> 02:27:36.460]   rat's circular things. Yeah. I'm worried about the beaver one. That's going to be used for a
[02:27:36.460 --> 02:27:43.340]   sexual reference. Look at this quarter. It says the crazy ones on it. Isn't that cool?
[02:27:44.140 --> 02:27:51.420]   Dota bird, Babushka doll, pinata, boomerang. A good looking ninja. Yeah, the beaver.
[02:27:51.420 --> 02:27:59.660]   Love the is the is the Italian one offensive? The jahann gesture. Hey,
[02:27:59.660 --> 02:28:07.580]   it's pinched fingers. What's suggestive about that? I think every Italian hand gesture,
[02:28:07.580 --> 02:28:12.460]   every single one means your wife is sleeping around on you. I think that's pretty much they all
[02:28:13.020 --> 02:28:15.100]   mean the same thing. Now it's offensively.
[02:28:15.100 --> 02:28:22.860]   You just made it worse. The sign of the cuckled every one of them is the sign of the cuckled.
[02:28:22.860 --> 02:28:27.340]   There's the Google turtle. Wait, wait, Leo, even the Boba T is the sign of the cuckled.
[02:28:27.340 --> 02:28:31.420]   They all are. That's basically all Italians think about is. I think that's what's
[02:28:31.420 --> 02:28:38.140]   going around on Google tamale. It's a Boba. It's a Boba tamale. Oh, I see. It's a Boba tamale.
[02:28:38.140 --> 02:28:43.580]   They're using the same circular objects. The hell are the Google people? There's a Boba.
[02:28:43.580 --> 02:28:48.780]   Yeah, they want reuse the Boba T. By the way, I've always thought these were like tapioca or
[02:28:48.780 --> 02:28:56.300]   something. They're cornstarch. That's disgusting. Okay, that's horrible. That pinched. That looks
[02:28:56.300 --> 02:29:02.860]   like that gourmet going. Yeah, fantastic. Google's is better than apples. Yeah, apples really does
[02:29:02.860 --> 02:29:09.580]   look like. I don't want to say. Much eternal though is lovely. I love that turtle. I use the turtle.
[02:29:09.580 --> 02:29:14.940]   Yeah, Mike, don't make us think it's sexually suggestive that I'll just be upsetting to me.
[02:29:14.940 --> 02:29:20.460]   Yes, it is. I'm a little cute turtle. The last half hour of a twit is always so interesting.
[02:29:20.460 --> 02:29:23.980]   It's like conversation, conversation, conversation. Hey, you're making a tally.
[02:29:23.980 --> 02:29:29.580]   You're way for the sleeping around. I'd like to see the whole range of Italian
[02:29:29.580 --> 02:29:37.740]   tangentures. The flicking below the chin. Oh, yeah. I love it. I love it. By the way,
[02:29:37.740 --> 02:29:45.100]   I'm a part Italian. I love Italians. If I could, I would be living in Italy. So I don't mean to say
[02:29:45.100 --> 02:29:51.740]   that in any way disparaging. I'll never forget though, we were in Portofino and we were trying to
[02:29:51.740 --> 02:29:58.060]   go from one town to another in a very thin road. Some guy in a truck was taking up too much of the
[02:29:58.060 --> 02:30:05.420]   road and our driver stopped the car, got out of the car, slammed the door and got in a fight with
[02:30:05.420 --> 02:30:11.420]   the driver of the truck in front of us. Like that's going to fix the traffic jam. And then
[02:30:11.420 --> 02:30:17.660]   there was a lot of this hands. It wasn't a fight though. They don't usually fight. They argue and
[02:30:17.660 --> 02:30:22.700]   scream at each other. They were yelling, but you're right. There was no fist, no, no, no punches thrown.
[02:30:22.700 --> 02:30:27.580]   We could use more argument in the US because in the US they tend to actually be fights where it's
[02:30:27.580 --> 02:30:32.140]   like drag people out. Well, you know, that's what we do. We go, we go passive aggressive
[02:30:32.140 --> 02:30:37.980]   until it's so suppressed for so long that all we could do is blow our stacks and kill somebody.
[02:30:37.980 --> 02:30:43.580]   When I'm here on, I weren't on honeymoon. We were in Rome and we got, we called a cab. The
[02:30:43.580 --> 02:30:49.900]   ACAB came to the hotel, picked this up. We drove like 20 miles and the cab we actually called
[02:30:49.900 --> 02:30:57.260]   was chasing our cab because we had called him and another cab is up. And they got out and they
[02:30:57.260 --> 02:31:01.340]   argued for like 20 minutes. And then our driver came back and said, you have to go with him.
[02:31:01.340 --> 02:31:12.700]   That's Italy. I love Italy. What year was that?
[02:31:12.700 --> 02:31:19.260]   That was 1987. Okay. I'm not trying to date you. I'm just curious the date of the story because
[02:31:19.260 --> 02:31:23.020]   it's an interesting angle. I wonder if that still happened today, like the Uber age, you know?
[02:31:23.020 --> 02:31:28.460]   Oh, it might. Uber has fixed a lot of things. All right.
[02:31:28.460 --> 02:31:32.780]   All out in Italy? Wait a minute. No, no, we're not allowed to leave for a while.
[02:31:32.780 --> 02:31:38.540]   Yeah. Yeah. I suppose I should do this story because it's a huge story and I accidentally let it
[02:31:38.540 --> 02:31:50.780]   go to the end. But this is Shrimms 2, which is a, this is why I didn't do this story. It's
[02:31:50.780 --> 02:32:00.700]   complicated. The court of justice in the European Union has struck down the flagship data flow
[02:32:00.700 --> 02:32:07.100]   arrangement between the United States and the EU called privacy shield. This was something that
[02:32:07.100 --> 02:32:14.620]   was supposed to cut through the problems with European customers of American companies data
[02:32:14.620 --> 02:32:21.260]   and where it's held. The idea was privacy shield would balance the requirements of US national
[02:32:21.260 --> 02:32:28.460]   security, public interest, law enforcement, and the privacy rights of citizens of the EU.
[02:32:28.460 --> 02:32:35.820]   And the court said, it's no good. You got to go back. So this has been going on. You know what?
[02:32:35.820 --> 02:32:42.460]   I'm sorry. I haven't brought it up. It's been going on since like 2013. Shrimms 2 succeeded.
[02:32:42.460 --> 02:32:49.340]   Shrimms 1. It's called Shrimms because of a privacy activist in Europe named Max Shrimms.
[02:32:49.340 --> 02:32:57.180]   It was his complaints that started this whole legal, snafu rolling. This is on tuck wrench and
[02:32:57.180 --> 02:33:04.140]   I don't understand it. It's complicated. It means that companies like Facebook and Microsoft and
[02:33:04.140 --> 02:33:11.740]   others are going to deal with this for some time to come. Yeah. I mean, one of the issues it raises,
[02:33:12.300 --> 02:33:19.500]   not directly, I think, but in sort of the long term is where companies can store their data and
[02:33:19.500 --> 02:33:27.900]   if they can store data wherever they have a site or if it has to be specifically in that country,
[02:33:27.900 --> 02:33:32.220]   the way that China mandates that its data must be accessible and so on.
[02:33:34.540 --> 02:33:44.300]   I think that this is going to put a big damper on how companies develop and it's going to lead to
[02:33:44.300 --> 02:33:52.940]   the ongoing and ever-growing bulkization of the internet. If Bulgaria, with, I don't know,
[02:33:52.940 --> 02:33:58.700]   5,000 people in it decides that you need to hold all of their data in Bulgaria,
[02:34:00.380 --> 02:34:07.100]   that sets a very bad precedent and I don't know how you get around that.
[02:34:07.100 --> 02:34:14.780]   All right. That's enough. I paid it lip service. That was your best shot, Leo.
[02:34:14.780 --> 02:34:20.380]   Two out of ten. Good job. I don't want to make the end of the show be the EU court decision docket.
[02:34:20.380 --> 02:34:24.860]   I could mention-- Could you stop at emojis? Yeah, let's go back to emojis, please.
[02:34:26.380 --> 02:34:31.180]   Apple won in court because remember they had a deal with Ireland and then
[02:34:31.180 --> 02:34:37.020]   Marta Vistaier in the EU said, "No, no, no. That's illegal. Apple, almost $14 billion
[02:34:37.020 --> 02:34:43.340]   to Ireland, which they're going to give to us." The court said, "No, no. That was a legit deal.
[02:34:43.340 --> 02:34:47.420]   Apple does not owe that tax. So big victory, big celebration in Cupertina."
[02:34:47.420 --> 02:34:52.700]   I think we should wrap this up because God knows it's been going on long enough.
[02:34:53.980 --> 02:34:59.580]   Thank you so much for being here. Seth is the man in charge, the founder, editor in chief
[02:34:59.580 --> 02:35:05.500]   of an excellent publication for anybody who cares about security, theparallax.com. Seth Rosenblatt
[02:35:05.500 --> 02:35:12.300]   is @SethR on the Twitter. Thanks for having me on again. Thank you, Wolverine. We appreciate it.
[02:35:12.300 --> 02:35:18.140]   You just need to get your eyebrows a little pointed up. Yeah, they're gray, but--
[02:35:18.700 --> 02:35:25.260]   That's not my problem. It's nice to see you, Seth. Thank you. Alex Wilhelm, my love to Eliza
[02:35:25.260 --> 02:35:33.180]   and your wonderful plantation there in Providence. That's the plantation I grew up on.
[02:35:33.180 --> 02:35:39.900]   Are you in the backyard? Is that where you built that? I'm in a small building in the backyard.
[02:35:39.900 --> 02:35:45.420]   Without the ACN, it's a greenhouse. So I have become sweat on this.
[02:35:45.420 --> 02:35:51.020]   I'm now liquid water. And your entire book collection is out there.
[02:35:51.020 --> 02:35:53.980]   So that's nice. Yeah, finally. We have a wall of built-in.
[02:35:53.980 --> 02:35:56.300]   So I put all the books in one place. Don't you love that?
[02:35:56.300 --> 02:36:00.620]   Nice to have you. Always my goal. Things going well in TechCrunch?
[02:36:00.620 --> 02:36:06.300]   Yes, sir. TC is doing lovely. I'm also on a podcast called Equity. If you're into the financial
[02:36:06.300 --> 02:36:09.900]   side of Tech, which is what we dork out about that. But as always, thanks for having me on the
[02:36:09.900 --> 02:36:13.900]   speaking tech. It's a pleasure. Always a pleasure. Mike Elgin, he's grounded for the knots, but that's
[02:36:13.900 --> 02:36:21.740]   good for us. We get to talk to him more often. Elgin.com at Mike Elgin. And when we get to travel again,
[02:36:21.740 --> 02:36:28.140]   gastronomad.net so you can go out and eat the foods of the worlds and the countries where they were
[02:36:28.140 --> 02:36:36.300]   created. We have full bookings for next year. They're not all full, but we have lots of
[02:36:36.300 --> 02:36:42.220]   experiences for next year. So if you want to plan next year's travel, check us out,
[02:36:42.220 --> 02:36:49.260]   gastronomad.net. And we added a second Provence experience in the summer next year.
[02:36:49.260 --> 02:36:53.420]   Oh boy, I love it. Because the first one filled up and so if you want to go to Provence.
[02:36:53.420 --> 02:36:58.540]   Provence, Morocco, Oaxaca, Barcelona. You're going back to Barcelona, huh?
[02:36:58.540 --> 02:37:04.140]   Oh, yeah. Absolutely. I miss Barcelona. I love that city so much. All of these cities are my
[02:37:04.140 --> 02:37:10.380]   favorites. Prosecco, Mexico City. We actually learned that our style of travel is actually very
[02:37:10.380 --> 02:37:14.940]   conducive to people who are concerned about viruses and things like that because we stay in a small
[02:37:14.940 --> 02:37:21.340]   group. In the case of Barcelona, we're actually in the Cava growing wine country outside of Barcelona
[02:37:21.340 --> 02:37:26.220]   where we stay to make surgical strikes into places. So it's actually very conducive to the
[02:37:26.220 --> 02:37:35.420]   post-pandemic world. When are you hoping to get back in the saddle? It'll be like March April,
[02:37:35.420 --> 02:37:41.420]   something like that. And we'll have Morocco experience, which is already booked. And we're
[02:37:41.420 --> 02:37:46.380]   going to do the whole lineup next year. We've postponed all of the ones that we're going to do
[02:37:46.380 --> 02:37:51.100]   this year and everybody's looking forward to doing it next year. It'll be fun. When you go back,
[02:37:51.100 --> 02:37:58.380]   people will be so happy to see you. Yes, we missed your money and your enthusiasm, Americans.
[02:37:58.380 --> 02:38:05.100]   Yeah. No, you'll be welcome with open arms. We really miss our friends. We've been
[02:38:05.100 --> 02:38:11.180]   trying to counsel our friends who help us on a lot of these things. We have a lot of friends who
[02:38:11.180 --> 02:38:16.060]   were wine makers and stuff like that who have suffered a bit through this thing. And we're all
[02:38:16.060 --> 02:38:21.980]   of us are looking forward to getting back together and having a great time with food and wine.
[02:38:21.980 --> 02:38:27.980]   Thank you all for being here. Man, this is fun to get together with friends and talk about things
[02:38:27.980 --> 02:38:34.300]   that may and may not matter. We do. Yeah, we're making good things.
[02:38:34.300 --> 02:38:39.900]   Yeah, that's all important or not. You know, those emojis make a difference.
[02:38:39.900 --> 02:38:47.500]   We do this show every Sunday afternoon around 2 30 Pacific 5 30 Eastern 21 30 UTC.
[02:38:47.500 --> 02:38:51.820]   You can watch us do it live at twit.tv/live. There's audio and video streams you can choose
[02:38:51.820 --> 02:38:57.180]   from your favorite there. After the fact, all the shows go up on the website at twit.tv
[02:38:57.180 --> 02:39:03.420]   or on our YouTube channels. We have a number of YouTube channels, but it all starts at youtube.com/twit.
[02:39:04.220 --> 02:39:10.140]   And of course, you can best thing to do be subscribe in your favorite podcast application.
[02:39:10.140 --> 02:39:17.260]   That way you get it automatically the minute it's available and it make make us feel good too.
[02:39:17.260 --> 02:39:23.100]   So thank you for your support. Now my friends, I say good night because another twit.
[02:39:23.100 --> 02:39:25.100]   This is amazing. Bye bye.
[02:39:25.100 --> 02:39:34.460]   [Music]
[02:39:34.460 --> 02:39:36.460]   You

