;FFMETADATA1
title=Can You Smell What Tim is Cooking?
artist=Leo Laporte, Christina Warren, Dan Gillmor, Larry Magid
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-06-05
track=930
language=English
genre=Podcast
comment=<p>WWDC preview, AI scientist statement, virtual kidnapping</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.760]   It's time for Treadt this week at Tech. What a great panel we have for you.
[00:00:03.760 --> 00:00:07.080]   Christina Warren's here, film girl from GitHub.
[00:00:07.080 --> 00:00:11.360]   We also have Dan Gilmore from the Walter Cronkite School of Journalism at ASU
[00:00:11.360 --> 00:00:17.920]   and from Connect Safely Larry Maggot. Larry Maggot is going to tell the terrifying story of how he very nearly got scammed
[00:00:17.920 --> 00:00:23.000]   by a virtual kidnapping scheme. I'll also show you what to do to avoid that.
[00:00:23.000 --> 00:00:26.560]   Then we're going to talk about Apple's big reveal tomorrow.
[00:00:26.560 --> 00:00:29.560]   Are you ready for a VR nerd helmet?
[00:00:29.560 --> 00:00:33.560]   Maybe, maybe not. Also AI, of course, in the news.
[00:00:33.560 --> 00:00:40.560]   We talk about that press release this week from AI scientists and leaders saying it's an extinction event.
[00:00:40.560 --> 00:00:44.160]   Watch out! Why would they do that? It's all coming up next.
[00:00:44.160 --> 00:00:45.160]   On Twitch.
[00:00:45.160 --> 00:00:47.560]   [Music]
[00:00:47.560 --> 00:00:49.560]   Podcasts you love.
[00:00:49.560 --> 00:00:51.560]   From people you trust.
[00:00:51.560 --> 00:00:53.560]   This is Twitch.
[00:00:53.560 --> 00:00:55.560]   [Music]
[00:00:55.560 --> 00:01:00.560]   [Music]
[00:01:00.560 --> 00:01:03.560]   This is Twitch. This week in Tech.
[00:01:03.560 --> 00:01:08.560]   Episode 930 recorded Sunday, June 4th, 2023.
[00:01:08.560 --> 00:01:11.560]   Can you smell what Tim is cooking?
[00:01:11.560 --> 00:01:15.560]   This week in Tech is brought to you by ExpressVPN.
[00:01:15.560 --> 00:01:20.560]   Using the internet without ExpressVPNs like forgetting to mute yourself on Zoom
[00:01:20.560 --> 00:01:23.560]   and then everyone hearing you trash talking your boss.
[00:01:23.560 --> 00:01:28.560]   Protect your online privacy by visiting ExpressVPN.com/twit.
[00:01:28.560 --> 00:01:31.560]   You can get three extra months free with a one-year package.
[00:01:31.560 --> 00:01:37.560]   Ambi-collide-collide is a device trust solution that ensures if a device isn't secure,
[00:01:37.560 --> 00:01:39.560]   it can't access your apps.
[00:01:39.560 --> 00:01:41.560]   It's zero trust for Okta.
[00:01:41.560 --> 00:01:45.560]   Visit collide.com/twit and book a demo today.
[00:01:45.560 --> 00:01:48.560]   And by AG1 by Athletic Greens.
[00:01:48.560 --> 00:01:51.560]   If you're looking for a simpler and cost-effective supplement routine,
[00:01:51.560 --> 00:01:57.560]   AG1 is giving away a free one-year supply of Vitamin D and five free travel packs
[00:01:57.560 --> 00:01:59.560]   with your first purchase of a subscription.
[00:01:59.560 --> 00:02:03.560]   Go to athleticgreens.com/twit.
[00:02:03.560 --> 00:02:07.560]   And by Cisco-Maraki with employees working in different locations,
[00:02:07.560 --> 00:02:12.560]   providing a unified work experience seems as easy as hurting cats.
[00:02:12.560 --> 00:02:15.560]   How do you rein in so many moving parts?
[00:02:15.560 --> 00:02:17.560]   The Maraki Cloud Managed Network.
[00:02:17.560 --> 00:02:18.560]   That's how.
[00:02:18.560 --> 00:02:21.560]   Now your organization can make hybrid work.
[00:02:21.560 --> 00:02:24.560]   Visit maraki.sisco.com/twit.
[00:02:24.560 --> 00:02:34.560]   It's time for Twit this week at Tech, the show we get together with the best journalists
[00:02:34.560 --> 00:02:41.560]   in the world to talk about the week's tech news so that you'll know what's next.
[00:02:41.560 --> 00:02:44.560]   Larry Maggot joins us from connect safely.org.
[00:02:44.560 --> 00:02:45.560]   You had a tale to tell.
[00:02:45.560 --> 00:02:47.560]   We'll talk about it.
[00:02:47.560 --> 00:02:50.560]   Boy, that was a scary story about your wife getting kidnapped,
[00:02:50.560 --> 00:02:51.560]   except she wasn't.
[00:02:51.560 --> 00:02:52.560]   She wasn't.
[00:02:52.560 --> 00:02:53.560]   She wasn't.
[00:02:53.560 --> 00:02:57.560]   Anyway, we'll talk about that when we come back later in the show,
[00:02:57.560 --> 00:02:59.560]   I guess we're not going away.
[00:02:59.560 --> 00:03:03.560]   Also with this legend, Dan Gilmore,
[00:03:03.560 --> 00:03:09.560]   founder of the ASU News CoLab at the Arizona State's Walter Cronkite
[00:03:09.560 --> 00:03:10.560]   School of Journalism.
[00:03:10.560 --> 00:03:12.560]   It's great to see you, Dan.
[00:03:12.560 --> 00:03:13.560]   Sure.
[00:03:13.560 --> 00:03:17.560]   You're my journalistic conscience.
[00:03:17.560 --> 00:03:20.560]   I always think of you.
[00:03:20.560 --> 00:03:23.560]   Would Dan approve that kind of thing?
[00:03:23.560 --> 00:03:26.560]   What would Dan say?
[00:03:26.560 --> 00:03:30.560]   And, yeah, you can say something now if you wish.
[00:03:30.560 --> 00:03:35.560]   No, I was going to say that sometimes questionable.
[00:03:35.560 --> 00:03:40.560]   And then at Zecharastina Warren, who has had a very busy couple of weeks,
[00:03:40.560 --> 00:03:44.560]   is, of course, developer advocate at Kit Hub.
[00:03:44.560 --> 00:03:46.560]   We love seeing you.
[00:03:46.560 --> 00:03:49.560]   And you were all over Microsoft's build broadcast.
[00:03:49.560 --> 00:03:50.560]   Yeah.
[00:03:50.560 --> 00:03:51.560]   Sure was.
[00:03:51.560 --> 00:03:52.560]   Yeah, it was a good time.
[00:03:52.560 --> 00:03:54.560]   It was good to be back with people again.
[00:03:54.560 --> 00:03:58.560]   And it was, they definitely, they had me on my feet a lot for sure.
[00:03:58.560 --> 00:03:59.560]   That goes without saying.
[00:03:59.560 --> 00:04:01.560]   You were working hard.
[00:04:01.560 --> 00:04:04.560]   I did want to ask you, though, I saw a Panos Panay come out dressed
[00:04:04.560 --> 00:04:16.560]   traditionally in black wearing his zebra Chuck 70 sneakers.
[00:04:16.560 --> 00:04:19.560]   They really stood out with the black outfit.
[00:04:19.560 --> 00:04:20.560]   Yeah.
[00:04:20.560 --> 00:04:21.560]   I imagine you noticed that.
[00:04:21.560 --> 00:04:22.560]   Is he a sneaker head?
[00:04:22.560 --> 00:04:23.560]   He is.
[00:04:23.560 --> 00:04:24.560]   He is.
[00:04:24.560 --> 00:04:25.560]   He's a massive sneaker head.
[00:04:25.560 --> 00:04:27.560]   That was actually the first time I ever met him in person.
[00:04:27.560 --> 00:04:31.560]   I think that I got cool points because I commented on his shoes.
[00:04:31.560 --> 00:04:36.560]   He was wearing a very specific, rare type of shoe.
[00:04:36.560 --> 00:04:39.560]   I was like, "Oh, I like your shoes."
[00:04:39.560 --> 00:04:41.560]   They were fleecy.
[00:04:41.560 --> 00:04:44.560]   They were like, "Fleece, well, sheep's zebra's or z-sheeps or something."
[00:04:44.560 --> 00:04:45.560]   Yeah.
[00:04:45.560 --> 00:04:46.560]   No, he is a shoe person.
[00:04:46.560 --> 00:04:48.560]   I think that his brother or someone works.
[00:04:48.560 --> 00:04:52.560]   I don't know if it's a Nike or if it's one of the other big shoe manufacturers.
[00:04:52.560 --> 00:04:55.560]   But he is a fantastic shoe collection.
[00:04:55.560 --> 00:04:59.560]   So people at home should always watch what Panos' shoes are no matter what
[00:04:59.560 --> 00:05:01.560]   he's at because they're always very, very good.
[00:05:01.560 --> 00:05:07.560]   This was a Rick Owens collaboration with a converse, the zebra print faux shearling
[00:05:07.560 --> 00:05:11.560]   with the signature exaggerated tongues.
[00:05:11.560 --> 00:05:13.560]   I didn't see the tongue they were tucked into his pants.
[00:05:13.560 --> 00:05:18.560]   But I didn't know the weird toes, the square toes with vents.
[00:05:18.560 --> 00:05:20.560]   And that's how we were able to track it down.
[00:05:20.560 --> 00:05:25.560]   I have to give credit to a tie in our marketing department who said, "Oh, it was easy.
[00:05:25.560 --> 00:05:29.560]   I just looked for vented toes, zebra print converse."
[00:05:29.560 --> 00:05:33.560]   And they came right up.
[00:05:33.560 --> 00:05:34.560]   They came right up.
[00:05:34.560 --> 00:05:35.560]   They're wild looking.
[00:05:35.560 --> 00:05:39.560]   Honestly, they were more interesting than Panos was.
[00:05:39.560 --> 00:05:42.560]   Panos got robbed.
[00:05:42.560 --> 00:05:45.560]   It turns out you may not.
[00:05:45.560 --> 00:05:49.560]   You don't have to say anything if you don't want to talk about the gossip at micro.
[00:05:49.560 --> 00:05:50.560]   No, we can talk about it.
[00:05:50.560 --> 00:05:53.560]   I mean, I'm missing the context here, I guess.
[00:05:53.560 --> 00:05:59.560]   So I think it was Paul Thoreau who did the legwork and got it confirmed by a couple of sources.
[00:05:59.560 --> 00:06:03.560]   The material Panos was going to give on day two.
[00:06:03.560 --> 00:06:04.560]   He did the day two keynote.
[00:06:04.560 --> 00:06:07.560]   We covered them both live, so that was fun.
[00:06:07.560 --> 00:06:11.560]   Was essentially stolen on day one.
[00:06:11.560 --> 00:06:16.560]   And so he was left with a keynote with nothing much to talk about.
[00:06:16.560 --> 00:06:18.560]   And it kind of felt that way, I'll be honest.
[00:06:18.560 --> 00:06:22.560]   He waited into the audience and was really pumped about how exciting things were.
[00:06:22.560 --> 00:06:25.560]   But it wasn't a whole lot to talk about.
[00:06:25.560 --> 00:06:29.560]   And I think it turned out that he kind of, it was unfortunate.
[00:06:29.560 --> 00:06:32.560]   Because he's got his very enthusiastic.
[00:06:32.560 --> 00:06:34.560]   He's a good speaker.
[00:06:34.560 --> 00:06:35.560]   He's a great speaker.
[00:06:35.560 --> 00:06:36.560]   He's a great speaker.
[00:06:36.560 --> 00:06:40.560]   Yeah, this is the weird part when you're hosting the show.
[00:06:40.560 --> 00:06:46.560]   Like there are so many different moving parts of the live production that I didn't know anything about who in what was supposed to say anything to make.
[00:06:46.560 --> 00:06:51.560]   So I'm sure that the people in the control room did, but I did not.
[00:06:51.560 --> 00:06:54.560]   So that was, I completely missed all of that.
[00:06:54.560 --> 00:07:00.560]   Well, I have my notes from the keynote and it recently only couple of lines.
[00:07:00.560 --> 00:07:04.560]   Panos, Panay, revival, meeting time, pumped.
[00:07:04.560 --> 00:07:06.560]   And what does it mean for us as people?
[00:07:06.560 --> 00:07:15.560]   That was the entire, but I do have to give a lot of credit to Stevie Batiche, who is a technical fellow at Microsoft.
[00:07:15.560 --> 00:07:22.560]   At Microsoft's Applied Sciences Group, who really gave, you tell me if you agree or disagree.
[00:07:22.560 --> 00:07:31.560]   I think the best rundown of what Microsoft's plans are with AI and how AI is going to integrate into Microsoft's products in the future.
[00:07:31.560 --> 00:07:32.560]   Yeah.
[00:07:32.560 --> 00:07:34.560]   How much of that did you get to see?
[00:07:34.560 --> 00:07:35.560]   I don't know if you got to see it.
[00:07:35.560 --> 00:07:39.560]   No, I have to catch up on all the stuff afterwards.
[00:07:39.560 --> 00:07:40.560]   Because yeah, you're backstage.
[00:07:40.560 --> 00:07:42.560]   You're behind the scenes.
[00:07:42.560 --> 00:07:45.560]   And this was the very end of the day two keynote.
[00:07:45.560 --> 00:07:47.560]   So you're probably, they're already probably gone.
[00:07:47.560 --> 00:07:49.560]   Let's, you know, powder, Christina's nose.
[00:07:49.560 --> 00:07:50.560]   Okay, get ready.
[00:07:50.560 --> 00:07:51.560]   You know, yeah.
[00:07:51.560 --> 00:07:52.560]   Yeah.
[00:07:52.560 --> 00:07:52.560]   Yeah.
[00:07:52.560 --> 00:07:56.560]   I was like watching as much as I could and then I watched after the event, but no, you're exactly right.
[00:07:56.560 --> 00:08:03.560]   Like during those things, when we're trying to go backstage and get places to sit up, like I literally had my phone like up and was like, you know, watching stuff.
[00:08:03.560 --> 00:08:06.560]   But what I could see in real time was, was more limited.
[00:08:06.560 --> 00:08:11.560]   Like there were some instances where I was to be backstage and interviewing people right after their, you know, talks.
[00:08:11.560 --> 00:08:16.560]   And I could see like the last, you know, five or ten minutes or so, but would miss the earlier part.
[00:08:16.560 --> 00:08:18.560]   And so I had to go back and watch a lot of stuff.
[00:08:18.560 --> 00:08:26.560]   AI has been the non-stop subject of this show for the last probably eight weeks.
[00:08:26.560 --> 00:08:29.560]   I mean, it is, it is all we talk about.
[00:08:29.560 --> 00:08:37.560]   And it was, of course, for Microsoft, the big, really the big reveal Microsoft is an investor, an open AI.
[00:08:37.560 --> 00:08:45.560]   To the tune of $10 billion, although that seems like a good deal of that is in kind donation of Azure time.
[00:08:45.560 --> 00:08:50.560]   Because of course, AI, these models take big cloud.
[00:08:50.560 --> 00:08:51.560]   Massive clusters.
[00:08:51.560 --> 00:08:53.560]   Like, to create.
[00:08:53.560 --> 00:08:54.560]   Yeah.
[00:08:54.560 --> 00:08:55.560]   No, it needs.
[00:08:55.560 --> 00:08:56.560]   No, exactly.
[00:08:56.560 --> 00:08:56.560]   No.
[00:08:56.560 --> 00:09:00.560]   And the GPU like usage is insane, especially when you're training the models.
[00:09:00.560 --> 00:09:01.560]   Yeah.
[00:09:01.560 --> 00:09:06.560]   When you're running them, I mean, they're working on ways to make that better, but even when you're running certain ones, like GPU,
[00:09:06.560 --> 00:09:12.560]   you're running certain ones, like GPT4, as example, is very GPU intensive, which is one of the reasons why it is constrained right now,
[00:09:12.560 --> 00:09:17.560]   both for people who want to pay for access to the API and also why it's more expensive.
[00:09:17.560 --> 00:09:23.560]   And so even if you pay for like, chat GPT plus, you're limited to the number of queries you can use.
[00:09:23.560 --> 00:09:25.560]   I've been 20 bucks a month.
[00:09:25.560 --> 00:09:27.560]   I can only 25 queries a day.
[00:09:27.560 --> 00:09:28.560]   Right.
[00:09:28.560 --> 00:09:29.560]   Right.
[00:09:29.560 --> 00:09:30.560]   I know it in the last three weeks.
[00:09:30.560 --> 00:09:31.560]   I've not done.
[00:09:31.560 --> 00:09:32.560]   I've done.
[00:09:32.560 --> 00:09:35.560]   One of those things where they were off quickly.
[00:09:35.560 --> 00:09:45.560]   Dan, it's also been a big part of your world, I imagine, because a number of journalistic entities, including CNET, have turned to AI to write their articles.
[00:09:45.560 --> 00:09:53.560]   And I think there's probably some concern among journalism students that they may not have a job when they graduate.
[00:09:53.560 --> 00:09:59.560]   Do you talk about that at all, Dan, with your students?
[00:09:59.560 --> 00:10:05.560]   I've been bringing it in. I'm not teaching journalism these days.
[00:10:05.560 --> 00:10:07.560]   It's more media literacy.
[00:10:07.560 --> 00:10:21.560]   Well, it's even more important there because the first thing, I mean, if you want to talk about the imminent hazards of AI, it's the disinformation pile that AI is going to create up to the 2024 elections.
[00:10:21.560 --> 00:10:27.560]   Yeah, the authenticity is kind of important.
[00:10:27.560 --> 00:10:50.560]   I guess I have so many thoughts about this, and I'm also trying really hard not to get caught up in the hype because I think a whole lot of what's going on now is what amounts to a transference of the cryptocurrency hype to and investments.
[00:10:50.560 --> 00:10:55.560]   I put that in quotes to AI.
[00:10:55.560 --> 00:11:02.560]   And we're going to see a whole bunch of new sleazy stuff and victims.
[00:11:02.560 --> 00:11:10.560]   It's like a repeat, but that wash rinse repeats the way it goes in tech.
[00:11:10.560 --> 00:11:19.560]   What do you make of Sam Altman being one of the many people who have a lot invested in AI going around calling it an existential threat?
[00:11:19.560 --> 00:11:26.560]   We're on a family show here.
[00:11:26.560 --> 00:11:31.560]   You can use initials if you want to say BS, for instance.
[00:11:31.560 --> 00:11:43.560]   Well, there's so many things I would say about the guy, but first of all, calling it open AI is one of the, that ought to go in quotes.
[00:11:43.560 --> 00:11:48.560]   Well, it was intended to be, wasn't it, when Elon Musk and Sam and others founded it?
[00:11:48.560 --> 00:11:55.560]   The whole premise was we can't let Google and Microsoft and Facebook own this space and do it in a closed fashion.
[00:11:55.560 --> 00:12:00.560]   We've got to create an AI open development, but that didn't happen.
[00:12:00.560 --> 00:12:13.560]   Well, this is either one of the great bait and switches of all time or that, you know, amazingly they changed their minds when a whole bunch of money shows up.
[00:12:13.560 --> 00:12:17.560]   And I got to go with both.
[00:12:17.560 --> 00:12:20.560]   I think this is really bothering me.
[00:12:20.560 --> 00:12:27.560]   And the idea of this guy running around the world that listening to or give me a break,
[00:12:27.560 --> 00:12:32.560]   and goes to Washington and says, regulate us before it's too late.
[00:12:32.560 --> 00:12:35.560]   What he means is regulate us before we have competition.
[00:12:35.560 --> 00:12:38.560]   So we are locked into place as the winner.
[00:12:38.560 --> 00:12:41.560]   That's what they're really doing with all this.
[00:12:41.560 --> 00:12:42.560]   Please regulate us.
[00:12:42.560 --> 00:12:54.560]   A few days ago, there was a, for the Center for AI Safety released a statement on AI risk signed by Sam Altman.
[00:12:54.560 --> 00:12:59.560]   And a lot of people I highly respect the statement was short and sweet mitigate.
[00:12:59.560 --> 00:13:02.560]   This is it mitigating the risk of extinction.
[00:13:02.560 --> 00:13:05.560]   By the way, extinction.
[00:13:05.560 --> 00:13:20.560]   They don't say of whom, but I'm presuming they mean our extinction from AI should be a global priority alongside other societal scale risks like pandemics and nuclear war.
[00:13:20.560 --> 00:13:30.560]   That's it signed by Jeffrey Hinton, the Google's retired Google scientist who was one of the fathers of large language models, Sam Altman's on this list.
[00:13:30.560 --> 00:13:41.560]   Bill Gates signs this pretty much a who's who of scientists, Bill McKibben, who is certainly up to date on extinction events.
[00:13:41.560 --> 00:13:47.560]   He was one of the first to talk about climate change as an extinction event.
[00:13:47.560 --> 00:13:55.560]   Lawrence Tribe from Harvard, Kevin Scott, the CTO of Microsoft, Peter Norvig from Stanford.
[00:13:55.560 --> 00:14:03.560]   I mean, some really rusty schwiker, the Apollo 9 astronaut are all worried about the extinction.
[00:14:03.560 --> 00:14:07.560]   I have to say, when it comes to Sam Altman, maybe I'm very cynical.
[00:14:07.560 --> 00:14:11.560]   I feels like maybe this is, look, that's not about regulatory capture.
[00:14:11.560 --> 00:14:15.560]   Maybe it is, but I think more it's about like, see this stuff really works.
[00:14:15.560 --> 00:14:23.560]   I mean, I think you're right, but I'm going to take a cynical approach to even that statement.
[00:14:23.560 --> 00:14:26.560]   I think that that has been, I think it's been misconstrued.
[00:14:26.560 --> 00:14:33.560]   I mean, I think that really that is just saying that the same way that the nuclear and many other technologies can be used for bad.
[00:14:33.560 --> 00:14:42.560]   This is a very sanguine statement basically saying that the AI, especially as it evolves, could be used for bad purposes.
[00:14:42.560 --> 00:14:45.560]   But there also should be a global priority.
[00:14:45.560 --> 00:14:50.560]   We should create a commission or something or something.
[00:14:50.560 --> 00:14:52.560]   I mean, we probably should.
[00:14:52.560 --> 00:14:53.560]   I don't think so.
[00:14:53.560 --> 00:14:57.560]   I am so underwhelmed by what AI is not AI.
[00:14:57.560 --> 00:14:59.560]   This is a lyson steroids.
[00:14:59.560 --> 00:15:02.560]   This is just a chatty BS.
[00:15:02.560 --> 00:15:07.560]   Well, the chat components are, I mean, some of the other stuff, some of the general part.
[00:15:07.560 --> 00:15:09.560]   I think you're genuinely interesting.
[00:15:09.560 --> 00:15:15.560]   I mean, I think that the core of the data is a little bit more complicated than the core of the data.
[00:15:15.560 --> 00:15:20.560]   I mean, I think that the core of the data is a little bit more complicated than the core of the data.
[00:15:20.560 --> 00:15:26.560]   I mean, I think that the core of the data is a little bit more complicated than the core of the data.
[00:15:26.560 --> 00:15:32.560]   I mean, I think that the core of the data is a little bit more complicated than the core of the data.
[00:15:32.560 --> 00:15:43.560]   You have, I mean, for instance, for several years now, MarketWatch has done, you know, just like auto-generated articles about the stock market that are very, very inconsequential.
[00:15:43.560 --> 00:15:51.560]   So I think that for some boilerplate things where if you're generally running the same block of code every day, but a different function is plugged in,
[00:15:51.560 --> 00:15:54.560]   why people are already automating things that way already, right?
[00:15:54.560 --> 00:16:00.560]   So I don't think that it's that much of a stretch to say that you could be running something that was AI-generated.
[00:16:00.560 --> 00:16:09.560]   But I think that whether it needs to be seen as this way, whereas this is like this existential threat right now, I would agree with you.
[00:16:09.560 --> 00:16:10.560]   Probably not.
[00:16:10.560 --> 00:16:21.560]   But I do feel like, I don't know, I personally think that a lot of the framing around that simple letter, which was in response to a very hysterical petition that people put out,
[00:16:21.560 --> 00:16:29.560]   which to me read as being like, "Hey, we missed out on this wave and now we want to stop other people from profiting off of it because we can't profit off of it."
[00:16:29.560 --> 00:16:31.560]   So let's pause everything for six months.
[00:16:31.560 --> 00:16:44.560]   I just read this as this should be treated the same way that we treat other types of technology that have had incredibly consequential, both positive and negative impacts on the world.
[00:16:44.560 --> 00:16:50.560]   I can't think of any technology beginning with the wheel and the fire that doesn't have positive and negative implications.
[00:16:50.560 --> 00:16:51.560]   Right.
[00:16:51.560 --> 00:16:53.560]   And it's really the question of how you use it.
[00:16:53.560 --> 00:17:00.560]   I am not particularly worried about dystopian notion that these machines are going to take over the world and kill their masters.
[00:17:00.560 --> 00:17:03.560]   I worry a lot about misinformation, disinformation.
[00:17:03.560 --> 00:17:22.560]   And I think going into the 2024 election, it may be, I wouldn't call it an extinction level event, but it could be very consequential if people are led to believe things that aren't true, including the ability to create photographic or recorded evidence of things that politicians may or may not have done that could be made.
[00:17:22.560 --> 00:17:25.560]   That could be basically totally fabricated.
[00:17:25.560 --> 00:17:27.560]   So how do we know whether to believe our eyes?
[00:17:27.560 --> 00:17:30.560]   When do we know when something's real and not real?
[00:17:30.560 --> 00:17:36.560]   It's really clearly hyperbole to say it's an extinction level of any kind, an existential level event.
[00:17:36.560 --> 00:17:37.560]   But I think there is a danger.
[00:17:37.560 --> 00:17:38.560]   I agree with you.
[00:17:38.560 --> 00:17:40.560]   I completely agree with you from disinformation.
[00:17:40.560 --> 00:17:49.560]   But to me, the real danger is not that overly persuasive that people will change their vote because they saw a tweet of the Pentagon on fire.
[00:17:49.560 --> 00:17:51.560]   That's not what the risk is.
[00:17:51.560 --> 00:17:53.560]   The risk is that people won't trust anything.
[00:17:53.560 --> 00:17:59.560]   This is the old Soviet Union technique of flooding the zone with BS.
[00:17:59.560 --> 00:18:06.560]   I was thinking about to undermine trust in our institutions and our media because I can't believe anything.
[00:18:06.560 --> 00:18:09.560]   So I'm just going to stop looking.
[00:18:09.560 --> 00:18:10.560]   I was thinking about this.
[00:18:10.560 --> 00:18:17.560]   So if you know in the news this week, there was this recording that apparently was on earth of Trump, essentially admitting that he had classified documents.
[00:18:17.560 --> 00:18:21.560]   If I were Trump's attorney, I would probably claim this is fake.
[00:18:21.560 --> 00:18:23.560]   How do we know this is really Donald Trump?
[00:18:23.560 --> 00:18:25.560]   It sounds like Donald Trump, but it could be fake.
[00:18:25.560 --> 00:18:27.560]   And of course it could be fake.
[00:18:27.560 --> 00:18:33.560]   But the only way you're real on it, Elon Musk has already done that.
[00:18:33.560 --> 00:18:34.560]   Of course he has.
[00:18:34.560 --> 00:18:40.560]   And they've already played the maybe it's fake so you can't trust it.
[00:18:40.560 --> 00:18:44.560]   It's a little more trust because it came from Trump's own people.
[00:18:44.560 --> 00:18:45.560]   But okay.
[00:18:45.560 --> 00:18:50.560]   Well, and the judge in the Musk case said, "Give me a break."
[00:18:50.560 --> 00:18:59.560]   And I think, but yeah, this is going to be a really big deal.
[00:18:59.560 --> 00:19:07.560]   The whole deep fake phenomenon that people have been talking about now gets more real and more confusing.
[00:19:07.560 --> 00:19:11.560]   It's a confusion factor, as you said, that's really the big one.
[00:19:11.560 --> 00:19:21.560]   We have to work harder on helping people be appropriately skeptical and think about context,
[00:19:21.560 --> 00:19:22.560]   think about all sorts of things.
[00:19:22.560 --> 00:19:29.560]   And we're going to have to come up with better ways to detect and to characterize what we're seeing.
[00:19:29.560 --> 00:19:30.560]   Yeah.
[00:19:30.560 --> 00:19:36.560]   I'm part of a project that is in full disclosure.
[00:19:36.560 --> 00:19:43.560]   It's a DARPA funded project that's working on some of these issues.
[00:19:43.560 --> 00:19:48.560]   And it's really difficult.
[00:19:48.560 --> 00:19:55.560]   But people are looking at ways to cater somebody you ought to bring on to this program some day
[00:19:55.560 --> 00:20:04.560]   from witness, that wonderful group that puts technology in the hands of people to record
[00:20:04.560 --> 00:20:09.560]   and save human rights abuses.
[00:20:09.560 --> 00:20:14.560]   Sam Gregory is what I'm talking about.
[00:20:14.560 --> 00:20:26.560]   And they and others, a bunch of folks in this area have been looking at ways, well, how can we maybe start thinking about provenance of information?
[00:20:26.560 --> 00:20:32.560]   How can we think about something that has a watermark from the second it was created?
[00:20:32.560 --> 00:20:33.560]   Yeah, that's interesting.
[00:20:33.560 --> 00:20:36.560]   That might be for NFTs.
[00:20:36.560 --> 00:20:48.560]   Tremendous, well, it's kind of sort of the NFT thing, but done more correctly and without scams attached.
[00:20:48.560 --> 00:20:58.560]   And it's really difficult because it creates a whole set of other problems, including kind of perpetual copyright and control.
[00:20:58.560 --> 00:21:03.560]   If by the person who created it, because you want that, no.
[00:21:03.560 --> 00:21:18.560]   How do you deal with the fact that every single bit of media created today is at some level being changed as it's not the original?
[00:21:18.560 --> 00:21:21.560]   What does the original, what does that even mean?
[00:21:21.560 --> 00:21:31.560]   And this is real AI stuff, when AI has changed it from the microsecond of creation into something that it wasn't.
[00:21:31.560 --> 00:21:40.560]   You take a photo with any modern camera and it's not what you saw with your eyes.
[00:21:40.560 --> 00:21:42.560]   It's already been manipulated.
[00:21:42.560 --> 00:21:47.560]   It's kind of a Heisenberg's media uncertainty principle.
[00:21:47.560 --> 00:21:50.560]   So we're always...
[00:21:50.560 --> 00:21:54.560]   But AI is going to be bad and we have to help people.
[00:21:54.560 --> 00:21:58.560]   But I think you make a point, though, that we've always had a certain amount of this.
[00:21:58.560 --> 00:22:04.560]   I mean, the only thing that really changes the quantity that AI can generate at speed.
[00:22:04.560 --> 00:22:08.560]   Because we've always had a certain amount of disinformation.
[00:22:08.560 --> 00:22:19.560]   Russia has famously had a troll farm building disinformation using media to disseminate it back in 2016.
[00:22:19.560 --> 00:22:23.560]   I mean, it's not new. It's just the quantity in the speed of it.
[00:22:23.560 --> 00:22:24.560]   That's new, yes?
[00:22:24.560 --> 00:22:30.560]   Well, when you talk about quantity, more people get misinformation watching Fox News than probably all of them.
[00:22:30.560 --> 00:22:33.560]   My point. Twitter bots put together.
[00:22:33.560 --> 00:22:41.560]   So I think it's a mistake to say, "Oh, this existential threat from AI when you got Fox News."
[00:22:41.560 --> 00:22:42.560]   Right.
[00:22:42.560 --> 00:22:44.560]   I mean, there's going to be some people watching.
[00:22:44.560 --> 00:22:47.560]   You see, what are you talking about, Fox News? This is fair and balanced.
[00:22:47.560 --> 00:22:49.560]   And maybe you feel it is.
[00:22:49.560 --> 00:22:55.560]   I think the Dan's larger point is absolutely well taken, which is all media is essentially a creation.
[00:22:55.560 --> 00:22:59.560]   Storytelling around what actually happened.
[00:22:59.560 --> 00:23:01.560]   That's the nature of it.
[00:23:01.560 --> 00:23:10.560]   And no media tells the entire story. I mean, it's just, if there's not enough space in any publication or broadcast to tell the entire story of anything.
[00:23:10.560 --> 00:23:17.560]   So even honest journalists have to pick and choose what do they decide to emphasize around a story or what stories to cover.
[00:23:17.560 --> 00:23:25.560]   And, you know, it creates this, you know, we can create a sense of urgency about something that exists, but essentially doesn't, isn't really an issue.
[00:23:25.560 --> 00:23:37.560]   So Fox, for example, will take a case where maybe an illegal immigrant created a committed a crime and make it look as if immigrants are on our major threat to American law and order.
[00:23:37.560 --> 00:23:40.560]   When in fact, yes, there are cases where immigrants commit a crime.
[00:23:40.560 --> 00:23:47.560]   But why would that crime featured on national news when there were thousands of crimes that took place that day that weren't featured on national news?
[00:23:47.560 --> 00:23:50.560]   And that is the process of journalism is picking and choosing, right?
[00:23:50.560 --> 00:23:51.560]   Right.
[00:23:51.560 --> 00:23:52.560]   Right.
[00:23:52.560 --> 00:23:53.560]   And you can say, well, go ahead.
[00:23:53.560 --> 00:23:54.560]   Go ahead.
[00:23:54.560 --> 00:23:55.560]   You're the journalist.
[00:23:55.560 --> 00:23:56.560]   Go ahead.
[00:23:56.560 --> 00:23:57.560]   Explain.
[00:23:57.560 --> 00:23:58.560]   I'm sorry.
[00:23:58.560 --> 00:24:04.560]   The idea that journalists do context is amusing to me because journalists don't do context as a rule.
[00:24:04.560 --> 00:24:05.560]   Should they?
[00:24:05.560 --> 00:24:06.560]   Of course.
[00:24:06.560 --> 00:24:07.560]   Yeah.
[00:24:07.560 --> 00:24:08.560]   But it's never been part of the.
[00:24:08.560 --> 00:24:10.560]   Except just the facts.
[00:24:10.560 --> 00:24:11.560]   Just the facts.
[00:24:11.560 --> 00:24:13.560]   Just never been part of what people do.
[00:24:13.560 --> 00:24:14.560]   Yeah.
[00:24:14.560 --> 00:24:15.560]   Well, a journalist picking two.
[00:24:15.560 --> 00:24:19.560]   So for example, if there was a random murder in Cincinnati, chances are it would make the national news.
[00:24:19.560 --> 00:24:22.560]   If some famous person were murdered, it would make the national news.
[00:24:22.560 --> 00:24:23.560]   It's still just a murder.
[00:24:23.560 --> 00:24:24.560]   I mean, it's not just a murder.
[00:24:24.560 --> 00:24:26.560]   But it's the same crime.
[00:24:26.560 --> 00:24:27.560]   Right.
[00:24:27.560 --> 00:24:30.560]   Because it's a famous person, it's now national news.
[00:24:30.560 --> 00:24:34.560]   Or because a person who ever committed the crime maybe elevates the national news.
[00:24:34.560 --> 00:24:42.560]   So that's the case where journalists do take some kind of, in one of many examples where journalists make some kind of contextual decision about what to emphasize.
[00:24:42.560 --> 00:24:44.560]   Well, let me ask this, Dan.
[00:24:44.560 --> 00:24:46.560]   You're at the Cronkite school.
[00:24:46.560 --> 00:24:55.560]   Clearly Walter Cronkite, who had 30 minutes a day to tell us what happened, was making decisions about what to report, what not to report.
[00:24:55.560 --> 00:24:58.560]   We trusted him.
[00:24:58.560 --> 00:25:03.560]   But perhaps we were deaf to the notion that he was in fact.
[00:25:03.560 --> 00:25:04.560]   Editorializing.
[00:25:04.560 --> 00:25:07.560]   I mean, that's the process, right?
[00:25:07.560 --> 00:25:12.560]   Was it just our naivete that we thought, well, we can trust the New York Times and Walter Cronkite and.
[00:25:12.560 --> 00:25:13.560]   Huntley and Brinkley.
[00:25:13.560 --> 00:25:15.560]   And we used to what we were a Huntley Brinkley family.
[00:25:15.560 --> 00:25:20.560]   But that maybe was our naivete at the time even.
[00:25:20.560 --> 00:25:28.560]   I mean, the Vietnam War didn't suddenly become a terrible war because Walter Cronkite suddenly noticed.
[00:25:28.560 --> 00:25:34.560]   I mean, if anything, you could argue that the government, which they did, I mean, they did this repeatedly with Edward R. Marrow.
[00:25:34.560 --> 00:25:36.560]   Sorry to interrupt, Dan.
[00:25:36.560 --> 00:25:44.560]   But like, you know, where they would intervene and try to, you know, take shift and take control over what the networks would allow.
[00:25:44.560 --> 00:25:46.560]   That the newsmen to say.
[00:25:46.560 --> 00:25:49.560]   And I'm sure that Cronkite faced some of those same challenges.
[00:25:49.560 --> 00:25:55.560]   But maybe now you're in fact to spread this information when there's only a handful of sources of information.
[00:25:55.560 --> 00:25:57.560]   And they're all trusted.
[00:25:57.560 --> 00:26:01.560]   Edward R. Marrow wore an army uniform while reporting.
[00:26:01.560 --> 00:26:04.560]   I mean, he was literally, you know, in bed with the military.
[00:26:04.560 --> 00:26:06.560]   I'm not saying he wasn't a great journalist.
[00:26:06.560 --> 00:26:09.560]   But clearly that had to have some influence on him.
[00:26:09.560 --> 00:26:10.560]   Yeah.
[00:26:10.560 --> 00:26:11.560]   What do you think?
[00:26:11.560 --> 00:26:12.560]   Those were more naivete.
[00:26:12.560 --> 00:26:23.560]   We were, yeah, I think the 50s and 60s were more naive times until the Vietnam War got to the point where it couldn't be hidden how bad it was.
[00:26:23.560 --> 00:26:38.560]   And, but can a lot of what in the sort of golden era of the three network model, they did try hard to do a good job and what they did.
[00:26:38.560 --> 00:26:42.560]   But a lot of the problems were in what they didn't cover.
[00:26:42.560 --> 00:26:47.560]   For example, they did not cover civil rights until they were forced.
[00:26:47.560 --> 00:26:59.560]   So that there was that there was a famous turning point with Cronkite where he basically said this is nuts about Vietnam.
[00:26:59.560 --> 00:27:00.560]   This is crazy.
[00:27:00.560 --> 00:27:01.560]   We can't win this thing.
[00:27:01.560 --> 00:27:05.560]   We're being lied to and kind of sort of said that on the air.
[00:27:05.560 --> 00:27:06.560]   Yeah.
[00:27:06.560 --> 00:27:17.560]   And that really, there's a famous story of Lyndon Johnson turning to somebody in the White House and saying, well, if we've lost Cronkite, we're done.
[00:27:17.560 --> 00:27:35.560]   And a lot of the rise of right wing media stems from the substantial part of the population, certainly not a majority, but a significant part that never trusted the three networks.
[00:27:35.560 --> 00:27:36.560]   Right.
[00:27:36.560 --> 00:27:57.560]   We're in New York liberals and we're advocating something different and from their perspective, better, leavening of the, what they believed was this entirely liberal driven agenda.
[00:27:57.560 --> 00:27:58.560]   Was it?
[00:27:58.560 --> 00:28:00.560]   I think it was 1974.
[00:28:00.560 --> 00:28:05.560]   It would be almost 60 years ago, 50 years ago.
[00:28:05.560 --> 00:28:11.560]   Here's Walter Cronkite and this was what changed the fate of the war, I think.
[00:28:11.560 --> 00:28:18.560]   Well, it seems now more certain than ever that the bloody experience of Vietnam is to end in a stalemate.
[00:28:18.560 --> 00:28:29.560]   But it is increasingly clear to this reporter that the only rational way out then will be to negotiate, not as victims, but as an honorable people who lived up to their own lives.
[00:28:29.560 --> 00:28:34.560]   Who lived up to their pledge to defend democracy and did the best they could.
[00:28:34.560 --> 00:28:39.560]   He's basically saying, let's get out of the war.
[00:28:39.560 --> 00:28:44.560]   And that was a watershed moment because he was so trusted.
[00:28:44.560 --> 00:28:58.560]   But I again ask the question, which is worse for disinformation when you have a hundred thousand news sources, all of which are untrusted, or three news sources of which are highly trusted.
[00:28:58.560 --> 00:29:02.560]   But maybe just as prone to disinformation.
[00:29:02.560 --> 00:29:06.560]   I mean, I know they're trying, but they're human.
[00:29:06.560 --> 00:29:10.560]   It seems to me that that's actually more risky.
[00:29:10.560 --> 00:29:23.560]   I agree with that if they're controlled by other influences in a more direct way.
[00:29:23.560 --> 00:29:37.560]   I mean, there were other media and there always have been media, smaller outlets and people who got at news that the big ones didn't.
[00:29:37.560 --> 00:29:43.560]   And it eventually would filter up into the public consciousness.
[00:29:43.560 --> 00:29:54.560]   And you know me, you know that I think that a hundred thousand outlets is at some level better.
[00:29:54.560 --> 00:29:59.560]   But we aren't doing a very good job of sorting it out and helping other people.
[00:29:59.560 --> 00:30:00.560]   Is that our fault?
[00:30:00.560 --> 00:30:01.560]   That's really the problem.
[00:30:01.560 --> 00:30:06.560]   As consumers, that's our job and that's what you're teaching basically.
[00:30:06.560 --> 00:30:12.560]   We all have to take some responsibility for knowing what we're talking about.
[00:30:12.560 --> 00:30:16.560]   And we're not generally doing that very well.
[00:30:16.560 --> 00:30:22.560]   It's difficult and it's going to be a process that's going to take a long time.
[00:30:22.560 --> 00:30:26.560]   But we don't really have an alternative unless you prefer censorship.
[00:30:26.560 --> 00:30:30.560]   And then you're in a worse position than you were.
[00:30:30.560 --> 00:30:39.560]   So disinformation is one way that AI could, I think extinction is such a strong word but will stipulate that's okay, extinction.
[00:30:39.560 --> 00:30:41.560]   Maybe that's one way.
[00:30:41.560 --> 00:30:42.560]   How well?
[00:30:42.560 --> 00:30:45.560]   It seems to me they've read too much science fiction.
[00:30:45.560 --> 00:30:54.560]   They've watched too many movies of people getting locked out of the pod bay and whopper declaring nuclear war.
[00:30:54.560 --> 00:31:01.560]   I mean, I don't know if AI poses a risk of extinction.
[00:31:01.560 --> 00:31:05.560]   And if so, how?
[00:31:05.560 --> 00:31:13.560]   On the front cover of Connect Safely, I reprinted my Mercury News column called AI makes a mistake but could it destroy us.
[00:31:13.560 --> 00:31:16.560]   And my conclusion is it's not going to destroy us.
[00:31:16.560 --> 00:31:28.560]   And I also think that we really do have to look at the possibility.
[00:31:28.560 --> 00:31:32.560]   And I think the benefits are enormous and I think the risk is substantial.
[00:31:32.560 --> 00:31:36.560]   But at the end of the day, I think we can manage this one.
[00:31:36.560 --> 00:31:38.560]   I'm not losing sleep over it.
[00:31:38.560 --> 00:31:42.560]   I lose a lot more sleep over climate change than I do over the AI.
[00:31:42.560 --> 00:31:47.560]   And I think that's an extinction event.
[00:31:47.560 --> 00:31:50.560]   Guess what's barreling down the pipe at us?
[00:31:50.560 --> 00:31:53.560]   Increasingly rapid speed.
[00:31:53.560 --> 00:31:57.560]   And I don't care how much you trust your favorite news source.
[00:31:57.560 --> 00:32:00.560]   No one's doing enough to stop it.
[00:32:00.560 --> 00:32:02.560]   I mean, that's an extinction event.
[00:32:02.560 --> 00:32:03.560]   Right, exactly.
[00:32:03.560 --> 00:32:08.560]   And I think that's headed right our way with no evidence that we're doing anything to stop it.
[00:32:08.560 --> 00:32:11.560]   More than one extinction event, I'm not saying that.
[00:32:11.560 --> 00:32:13.560]   There's a lot of other bad stuff happening.
[00:32:13.560 --> 00:32:17.560]   But AI does not seem to rise to that level of danger.
[00:32:17.560 --> 00:32:22.560]   And we have decades of evidence that climate change is coming and that it's extremely dangerous.
[00:32:22.560 --> 00:32:26.560]   I don't know what day it's going to, you know, and we continue to ignore it, exactly.
[00:32:26.560 --> 00:32:27.560]   Yeah.
[00:32:27.560 --> 00:32:28.560]   All right.
[00:32:28.560 --> 00:32:30.560]   Let's take a break.
[00:32:30.560 --> 00:32:33.560]   Enough AI, although there's a lot more AI stories.
[00:32:33.560 --> 00:32:37.560]   I guess I might have to throw them in.
[00:32:37.560 --> 00:32:41.560]   There was a chatbot designed to help eating disorders.
[00:32:41.560 --> 00:32:48.560]   They fired the entire team that was on this hotline.
[00:32:48.560 --> 00:33:01.560]   The NEDA, the National Eating Disorders Association, had a website where people would go to get information about eating disorders and what to do to fight them.
[00:33:01.560 --> 00:33:07.560]   They decided, "Yeah, we don't need people. Let's just get a bot named Tessa."
[00:33:07.560 --> 00:33:17.560]   Tessa started giving people dieting advice, which is exactly what you don't give somebody who's suffering from an eating disorder.
[00:33:17.560 --> 00:33:25.560]   That's just an example of hit to me over promoting the potential of AI.
[00:33:25.560 --> 00:33:28.560]   I don't think AI is all that.
[00:33:28.560 --> 00:33:34.560]   It may have potential, but potential something may happen in the future.
[00:33:34.560 --> 00:33:49.560]   NEDA was fooled by these stories and I think as a media organization, it's our responsibility to tell people, "No, a chatbot should not be giving advice on eating disorders. Why would you even think that?"
[00:33:49.560 --> 00:34:00.560]   It's the reason why I don't rely on autopilot on my Tesla, although I do believe we will reach a day someday when cars can safely drive themselves, which just aren't there yet.
[00:34:00.560 --> 00:34:04.560]   But we need to be experimenting and building it to get to that point.
[00:34:04.560 --> 00:34:17.560]   The question is, should we be drinking the Kool-Aid this quickly as opposed to simply supporting the fact that research and experimentation needs to happen with the appropriate warnings and safeguards?
[00:34:17.560 --> 00:34:26.560]   We have to think about the whole... The employment angle to it is really going to be key.
[00:34:26.560 --> 00:34:33.560]   A big business wants AI in substantial part to get rid of human labor.
[00:34:33.560 --> 00:34:46.560]   That is the fundamental driving force behind a lot of corporate adoption of AI.
[00:34:46.560 --> 00:35:00.560]   This is Nirvana. You can hire return on capital by getting rid of these annoying humans who need healthcare, who need money.
[00:35:00.560 --> 00:35:15.560]   If we don't really focus on that as part of this and put that not just in sort of by the way, they got the reason this idiot chatbot was giving bad advice is that they wanted to get rid of people.
[00:35:15.560 --> 00:35:17.560]   Right. Right. Right.
[00:35:17.560 --> 00:35:20.560]   But make that really clearer and louder.
[00:35:20.560 --> 00:35:27.560]   The other thing about we've been using AI in making decisions for now a number of years.
[00:35:27.560 --> 00:35:43.560]   In other contexts, it's now just starting to affect the white middle class in a profound way when people have been having decisions made about them and for them.
[00:35:43.560 --> 00:35:55.560]   With AI augmented tools for years, including redlining of mortgages, healthcare decisions, who gets a job, all sorts of stuff.
[00:35:55.560 --> 00:36:01.560]   Oh, and we didn't give a damn about any of them or we didn't give enough of a damn.
[00:36:01.560 --> 00:36:04.560]   And suddenly it's hitting us, "Oh gosh, this is really dangerous."
[00:36:04.560 --> 00:36:17.560]   Yeah. That's a really good point. All the job hunting sites have offered AI help in screening out candidates, which ended up, of course, to disproportionately affect people of color and other minorities.
[00:36:17.560 --> 00:36:20.560]   Yeah. I mean, this has been going on for years.
[00:36:20.560 --> 00:36:22.560]   Quite confused.
[00:36:22.560 --> 00:36:29.560]   Totally. I mean, if you substitute AI for automation, which I know they're not exactly interchangeable, but in many ways, that's how these things are being used.
[00:36:29.560 --> 00:36:40.560]   Yeah. I think Dan, I think you're exactly right. These things and these algorithms have been used in very negative ways, both for screening people for loans as a great example.
[00:36:40.560 --> 00:36:54.560]   For job stuff, it could potentially be bad. That's why it's important that there are ethics boards and hopefully, you know, bias parties looking at these things.
[00:36:54.560 --> 00:37:04.560]   I'm sure you have the best healthcare in the world because you work at Microsoft, but honestly, I know that our healthcare is very much influenced by, I wouldn't call it AI.
[00:37:04.560 --> 00:37:06.560]   Oh, no, but it's been a bit of business.
[00:37:06.560 --> 00:37:07.560]   It's actually a business.
[00:37:07.560 --> 00:37:14.560]   It's actuarial tables are an extinction level threat to mankind because they're just looking at statistics.
[00:37:14.560 --> 00:37:18.560]   It's the same thing. We can't get earthquake insurance in California.
[00:37:18.560 --> 00:37:28.560]   No, I mean, and look, my parents who are in their 70s, you know, are healthy. I see the way that they are treated by doctors based on certain things.
[00:37:28.560 --> 00:37:29.560]   Did me, they are.
[00:37:29.560 --> 00:37:37.560]   No, not at all, but they are using, but AI will be used in some diagnostics criteria. And in some cases, that might actually be useful, right?
[00:37:37.560 --> 00:37:47.560]   Like in some cases, I could see that that might actually be useful than the current model, which is that you have people who, if you are over a certain age, will immediately look at you or a certain gender or a certain race.
[00:37:47.560 --> 00:38:00.560]   Well, immediately dismiss any of your concerns and will just make a diagnostic based on, you know, whatever they plugged into something, whereas if something's more complex, potentially some of these systems could help.
[00:38:00.560 --> 00:38:07.560]   But no, you're not wrong. I mean, like, even when you have good healthcare, getting, you know, doctors to actually listen to you is very difficult.
[00:38:07.560 --> 00:38:08.560]   Yeah.
[00:38:08.560 --> 00:38:12.560]   Well, we have to do that. We have to ration healthcare. We don't have enough. It's expensive.
[00:38:12.560 --> 00:38:13.560]   Right.
[00:38:13.560 --> 00:38:18.560]   And we can't all get good healthcare, you know, only the rich, I guess.
[00:38:18.560 --> 00:38:34.560]   Well, if we took away that massive portion of the spending that's going to insurance companies for the sole purpose of not giving us healthcare, maybe we'd be able to afford more.
[00:38:34.560 --> 00:38:35.560]   I don't know. Call me crazy.
[00:38:35.560 --> 00:38:36.560]   Yeah.
[00:38:36.560 --> 00:38:47.560]   So the other thing about healthcare is, I mean, the more I've experimented with various tools to do diagnostics, the more skeptical idea about the accuracy of diagnostic testing.
[00:38:47.560 --> 00:39:04.560]   I mean, certain things are obvious. Like, you can't experience the cancer, maybe, but other things like sleep apnea. It's a lot of, I don't know, I wouldn't call it pseudoscience, but approximation of determining what constitutes a, and the only thing in what doesn't.
[00:39:04.560 --> 00:39:18.560]   Maybe just a lot we don't know. Obviously, the COVID situation was one where we went through three years of kind of guessing. We were talking earlier off camera about, you know, washing our hands and washing our vegetables and things that we were led to believe by very credible, knowledgeable, well-meaning
[00:39:18.560 --> 00:39:25.560]   scientists were the right thing to do, not because they were trying to fool us or manipulate us because they just didn't know.
[00:39:25.560 --> 00:39:27.560]   And there's still a lot we don't know.
[00:39:27.560 --> 00:39:28.560]   Yeah.
[00:39:28.560 --> 00:39:36.560]   And that's 30% of our healthcare spending goes to administrative costs. That's a significant portion.
[00:39:36.560 --> 00:39:49.560]   Larry, the scientific process is something to celebrate, of course. And yeah, we made a lot of mistakes early on and so did people. We should have, who should have done a better job.
[00:39:49.560 --> 00:40:02.560]   And I think that's a great thing to do with health organization being one of the top ones, but different than sort of willful misinformation along the way.
[00:40:02.560 --> 00:40:03.560]   There was, I think that-
[00:40:03.560 --> 00:40:18.560]   And I don't know for a fact, but I've seen it said that there was some bias also because the medical establishment had, for so many centuries, fought off the my asthma theory of illness that you got sick because of the fact that you're not going to be able to get sick.
[00:40:18.560 --> 00:40:30.560]   And I think that's a good thing to say, because of bad air, they were very, very reluctant to say that this disease was caused by aerosolized particles.
[00:40:30.560 --> 00:40:32.560]   And that's a bias, right? That's a-
[00:40:32.560 --> 00:40:37.560]   That's not scientific method. That's like, "Oh, no, we don't want to go there."
[00:40:37.560 --> 00:40:43.560]   And that's a good example of where the scientific method can be overruled by-
[00:40:43.560 --> 00:41:01.560]   I think it's part of the scientific method is studying and re-studying and re-investigating it. I mean, why are drugs pulled off the market besides the political ones like the abortion pill, but most drugs that are pulled off the market are because subsequent research has proven that they are less helpful than either they are more harmful than helpful.
[00:41:01.560 --> 00:41:10.560]   And that's part of the scientific process. And it didn't mean it was foolish for us to have taken those drugs when we believed they were helpful, but it just means that we had in-
[00:41:10.560 --> 00:41:22.560]   we were basing those decisions on what later turned out to be incomplete information. And that's what's sort of scary to me when I think about all of the decisions that I make as a healthcare consumer based on what is clearly incomplete information.
[00:41:22.560 --> 00:41:26.560]   So do you think this is a place AI could help? I do.
[00:41:26.560 --> 00:41:35.560]   Eventually. I think AI can make- is going to be great at making connections that we otherwise might not spot.
[00:41:35.560 --> 00:41:37.560]   That's what it seems to be.
[00:41:37.560 --> 00:42:01.560]   And that has been- we've seen things being used that way. The issue, as in that test, that you were just getting results from is drawing definitive conclusions from connections where the classic correlation is not causation.
[00:42:01.560 --> 00:42:11.560]   And we sorting out the difference. I think AI could give us all kinds of wonderful things to investigate further.
[00:42:11.560 --> 00:42:27.560]   Well, we know one area that's huge is protein folding, where AI is so much faster and so much more effective and has really been solving a problem that is going to have a significant impact on healthcare.
[00:42:27.560 --> 00:42:35.560]   This is from Science Magazine. The game has changed AI triumphs of protein folding.
[00:42:35.560 --> 00:42:37.560]   Remember folding at home where-
[00:42:37.560 --> 00:42:38.560]   Yeah.
[00:42:38.560 --> 00:42:44.560]   You could devote a portion of your computer, idle computer time to solving these folding problems?
[00:42:44.560 --> 00:42:50.560]   AI has already done more than all folding at home over the years put together.
[00:42:50.560 --> 00:43:01.560]   I think the SETI, which was one of the things that shut down last year actually, but yeah, that's a great example of where AI is very, very useful.
[00:43:01.560 --> 00:43:13.560]   So I'm not saying AI is useless. I'm saying that the large language models are for the most part seem to be bullshit generators.
[00:43:13.560 --> 00:43:29.560]   I think the large language model though, if they have the right database underlying them can be useful. And I think they can, in healthcare, they can be useful also in doing the things that a lot of doctors do today, which is to take a bunch of information and make a conclusion as to what the best course of action is.
[00:43:29.560 --> 00:43:30.560]   Yeah.
[00:43:30.560 --> 00:43:37.560]   And that's based essentially on actual intelligence. The doctor knows this and that and has this data and makes this conclusion.
[00:43:37.560 --> 00:43:46.560]   But with an AI system that has access to a huge amount of data assuming the data is accurate, it might be able to make a better recommendation than a typical human doctor could.
[00:43:46.560 --> 00:43:53.560]   Especially once you get out of the big cities, when you got the big universities, you get into places where you can't get the world's greatest doctors.
[00:43:53.560 --> 00:44:00.560]   If you can get AI that can simulate that kind of intelligent diagnosis, it could benefit me in kind.
[00:44:00.560 --> 00:44:02.560]   All right. I want to take a break.
[00:44:02.560 --> 00:44:03.560]   We're assuming it's accurate.
[00:44:03.560 --> 00:44:04.560]   Assuming it's accurate.
[00:44:04.560 --> 00:44:05.560]   Assuming it's accurate.
[00:44:05.560 --> 00:44:06.560]   Yeah.
[00:44:06.560 --> 00:44:07.560]   Of course.
[00:44:07.560 --> 00:44:11.560]   And that's maybe one of the biggest challenges is figuring out.
[00:44:11.560 --> 00:44:13.560]   But that's true with even current medical information.
[00:44:13.560 --> 00:44:14.560]   True with everything.
[00:44:14.560 --> 00:44:16.560]   But humans are making decisions for machines.
[00:44:16.560 --> 00:44:17.560]   Right.
[00:44:17.560 --> 00:44:26.560]   You've got to be like, look, look at misinformation. I voted for X Y V because I believe this and if this turns out to be false, you just made a human decision that was based on bad data.
[00:44:26.560 --> 00:44:30.560]   And frankly, that's how we could wind up with another Trump term.
[00:44:30.560 --> 00:44:36.560]   Maybe we let AI fill out the medical forms, but not decide who gets a liver. How about that?
[00:44:36.560 --> 00:44:37.560]   I would agree with that.
[00:44:37.560 --> 00:44:38.560]   How about that?
[00:44:38.560 --> 00:44:43.560]   Our show today brought to you by a great panel. Really good to have Dan Gilmore here from ASU.
[00:44:43.560 --> 00:44:47.560]   I love it that you're now working with people on media literacy.
[00:44:47.560 --> 00:44:51.560]   This is a very important area. I think that's fantastic.
[00:44:51.560 --> 00:44:56.560]   @DanGilmore on Mastodon, D-A-N-G-I-W-L-M-O-R.
[00:44:56.560 --> 00:45:01.560]   Great to have you, Larry Maggud, who is going to tell a horrific story in a little bit.
[00:45:01.560 --> 00:45:03.560]   The President and CEO of Connectsafly.org.
[00:45:03.560 --> 00:45:08.560]   Something that happened to you. I wanted to get you on as soon as I read that, but I knew we were having you as soon.
[00:45:08.560 --> 00:45:10.560]   So thank you for being here.
[00:45:10.560 --> 00:45:16.560]   And Christina Warren, who has by every right to be completely exhausted from a week at field.
[00:45:16.560 --> 00:45:20.560]   And then last week you were in Atlanta for what?
[00:45:20.560 --> 00:45:28.560]   For Render ATL, which is a big kind of tech and music conference that is focused on the southeast.
[00:45:28.560 --> 00:45:31.560]   And a lot of the black tech community is there, which is really great.
[00:45:31.560 --> 00:45:34.560]   And just lots of different people from lots of different backgrounds.
[00:45:34.560 --> 00:45:35.560]   It was a really, really good time.
[00:45:35.560 --> 00:45:37.560]   Were you there in an official capacity?
[00:45:37.560 --> 00:45:40.560]   Yeah, we had a booth for GitHub.
[00:45:40.560 --> 00:45:42.560]   We actually had a nail booth, which was really fun.
[00:45:42.560 --> 00:45:44.560]   So we were doing people's nails.
[00:45:44.560 --> 00:45:49.560]   We had nail wraps, basically, that had little octocats on them.
[00:45:49.560 --> 00:45:50.560]   Oh, man.
[00:45:50.560 --> 00:45:52.560]   Other colors too, which was really fun.
[00:45:52.560 --> 00:45:56.560]   But honestly, I went last year just in a personal capacity.
[00:45:56.560 --> 00:45:58.560]   And so there were a number of my colleagues.
[00:45:58.560 --> 00:46:01.560]   And we just had a really good time and we just wanted to support the community.
[00:46:01.560 --> 00:46:04.560]   What was the AI conversation there like?
[00:46:04.560 --> 00:46:06.560]   I imagine it was fast and furious.
[00:46:06.560 --> 00:46:14.560]   Yeah, I mean, so what was interesting was that last year at Render, there was a lot of Web3 and Blockchain Talk.
[00:46:14.560 --> 00:46:20.560]   And this year, interestingly, there wasn't as much AI as I was expecting.
[00:46:20.560 --> 00:46:25.560]   We were showing off GitHub Co-pilot, one of my colleagues, Roselle Scarlett, who's amazing.
[00:46:25.560 --> 00:46:33.560]   She gave a talk on GitHub Co-pilot and kind of showing what it can do in terms of helping you improve your productivity
[00:46:33.560 --> 00:46:37.560]   and get some coding things done faster.
[00:46:37.560 --> 00:46:42.560]   But I think that there is still kind of a question for a lot of people.
[00:46:42.560 --> 00:46:48.560]   It's not similar to the conversation we've been having here about what are the benefits, what are the challenges going to be.
[00:46:48.560 --> 00:46:53.560]   But I have to be honest, I mean, there is also, and some of this is going to be negative,
[00:46:53.560 --> 00:47:01.560]   but there is a massive opportunity right now because of the hype and the excitement and the fire around AI.
[00:47:01.560 --> 00:47:09.560]   You know, you see some really interesting things being built up, even if you don't believe that large language models are the future.
[00:47:09.560 --> 00:47:15.560]   There are still some really, even if it's not going to save humanity, there are still going to be interesting things you can do
[00:47:15.560 --> 00:47:19.560]   and can benefit from because of tools that people are building right now.
[00:47:19.560 --> 00:47:21.560]   And I personally find that exciting.
[00:47:21.560 --> 00:47:22.560]   Yeah.
[00:47:22.560 --> 00:47:24.560]   Render ATL, how fun.
[00:47:24.560 --> 00:47:25.560]   Yeah.
[00:47:25.560 --> 00:47:28.560]   I know you'd probably run asleep right now, but thank you for staying on that.
[00:47:28.560 --> 00:47:29.560]   No, I'm glad to be here.
[00:47:29.560 --> 00:47:30.560]   Obviously.
[00:47:30.560 --> 00:47:35.560]   Look, I'm not the one who was super early for, so like, it's fine.
[00:47:35.560 --> 00:47:40.560]   I know it's this is normal time for you. Are you going to be home for the next couple of weeks?
[00:47:40.560 --> 00:47:44.560]   I think so. I might be going out of town, but if I do, it's not for work.
[00:47:44.560 --> 00:47:45.560]   It'll be for like a concert.
[00:47:45.560 --> 00:47:47.560]   So I wonder who.
[00:47:47.560 --> 00:47:51.560]   No, no, actually, that's that's a little wild.
[00:47:51.560 --> 00:47:53.560]   This would be a Ben Folds actually.
[00:47:53.560 --> 00:47:58.560]   Oh, I've just got the new Ben Folds album. That is, he is so good.
[00:47:58.560 --> 00:47:59.560]   He's not good.
[00:47:59.560 --> 00:48:00.560]   Yeah.
[00:48:00.560 --> 00:48:03.560]   What happened to the five? Is it just the Ben Folds one now? What is the?
[00:48:03.560 --> 00:48:09.560]   He goes back and forth. And so like, I think someone did solo things, but I'm, she does it with the band.
[00:48:09.560 --> 00:48:10.560]   But yeah.
[00:48:10.560 --> 00:48:13.560]   I'm listening to it. I'm thinking this is like old people music.
[00:48:13.560 --> 00:48:17.560]   So I'm glad to know a young person likes Ben Folds.
[00:48:17.560 --> 00:48:23.560]   Michael likes Ben Folds too. But I think this is kind of nice and sweet and, you know.
[00:48:23.560 --> 00:48:31.560]   Yeah. Okay. I'm going to say this. I think the best album, one of my favorite albums of the 2000s was the album he did with William Shatner.
[00:48:31.560 --> 00:48:39.560]   I'm not even joking. I'm being completely serious. It's called Has been. It came out in 2005 and William Shatner produced by Ben Folds.
[00:48:39.560 --> 00:48:44.560]   It's a really, really good. So that's my free. That's my free. That's my free advice to people.
[00:48:44.560 --> 00:48:50.560]   That's your pick. I got something to listen to tonight. Wow, which is good because successions over and I'm really verclimped.
[00:48:50.560 --> 00:48:53.560]   Oh my God. I, we need to talk about that.
[00:48:53.560 --> 00:48:54.560]   We'll save that.
[00:48:54.560 --> 00:48:55.560]   I will save it. I will save it.
[00:48:55.560 --> 00:48:56.560]   I'm going to fight.
[00:48:56.560 --> 00:48:57.560]   I'm going to fight.
[00:48:57.560 --> 00:48:58.560]   I'm going to go.
[00:48:58.560 --> 00:49:03.560]   All right. So today brought to you by Express VPN.
[00:49:03.560 --> 00:49:12.560]   They're really good. These guys are coming up with analogies for using a VPN. This is the best yet.
[00:49:12.560 --> 00:49:25.560]   Using the internet without Express VPN is like forgetting. This is very timely actually to our early conversation for getting to mute yourself on zoom and having your coworkers hear something you don't want them to hear.
[00:49:25.560 --> 00:49:33.560]   Oh, it could just be a bit of harmless banter. But if it's about the boss and the boss over here's you. Well, now you got a problem.
[00:49:33.560 --> 00:49:46.560]   See privacy isn't, you know, we often hear them say, well, if you've got nothing to hide, what are you worried about? Everybody has something to hide, right? Everybody closes the bathroom door.
[00:49:46.560 --> 00:49:53.560]   You need a VPN. Internet service providers are not only know everywhere you go and what you do on the Internet.
[00:49:53.560 --> 00:50:05.560]   They're allowed to sell it on the marketers to data brokers, add companies that will then use your data to do all sorts of stuff target you follow you around.
[00:50:05.560 --> 00:50:15.560]   That's just one of many reasons a good VPN is important. The reason I use Express VPN is because it is literally the best VPN out there for several reasons.
[00:50:15.560 --> 00:50:26.560]   They do not track you in any way and they go the extra mile to prevent that. They run their own software called trusted server, which runs in RAM sandbox so it cannot write to the hard disk.
[00:50:26.560 --> 00:50:36.560]   You press that button in your Express VPN app. You launch the server. It's running in RAM. And as soon as you close it, as soon as you leave the server, it goes away with no trace left.
[00:50:36.560 --> 00:50:48.560]   But as if that weren't enough, they even run a special Debian distribution on all their servers that went every reboot wipes the drive and starts fresh and they do that every day.
[00:50:48.560 --> 00:50:58.560]   This is how you protect people's privacy. There is nothing on Express VPN servers, not a thing. Very few other VPN companies can say that.
[00:50:58.560 --> 00:51:12.560]   Now, Express VPM is not free, but there's another reason I like Express VPN, but they put that money into infrastructure. So they're fast, which means you can watch HD video on Express VPN, which means you can use their geographic
[00:51:12.560 --> 00:51:25.560]   data. So, you know, variability to say, let's say, I'm in now, I'm in England and I can watch Netflix England or I'm in Japan. I can watch Netflix Japan. That only works because they have enough bandwidth. So you can watch that HD video.
[00:51:25.560 --> 00:51:35.560]   It also requires that they rotate their IP addresses, fresh IP addresses all the time. So that your IP address is completely random and unrelated to you.
[00:51:35.560 --> 00:51:48.560]   How you stay safe. And of course, as you would expect with Express VPN, all your traffic is tunneled through an encrypted tunnel. Nobody at a coffee shop or anywhere or your ISP can see what you're doing. That's what you want.
[00:51:48.560 --> 00:51:59.560]   Rated number one by CNET, tech radar. It works everywhere. You do phones, laptops, they sell routers, very good routers with Express VPN built in so your whole house can be protected.
[00:51:59.560 --> 00:52:16.560]   It will also run on some existing routers. You can check Express VPN's website for more information. I think Express VPN is just head and shoulders above everybody else. If it's time to protect your online privacy, I want you to go to expressvpn.com/twit today.
[00:52:16.560 --> 00:52:31.560]   You'll get an extra three months free with a one year package. E X P R E S S VPN.com/twit. Please use that address so they know you saw it here. You'll also get that extra three months free with a one year package. That's your best deal again.
[00:52:31.560 --> 00:52:48.560]   You'll get less than seven bucks a month for rock solid privacy at blazing speeds with no compromises. Express VPN.com/twit. We thank them so much for their support of this week in tech.
[00:52:48.560 --> 00:52:55.180]   League, Micah Sargent, Jason Snell and others headed to Cupertino tomorrow morning bright
[00:52:55.180 --> 00:52:57.000]   and early.
[00:52:57.000 --> 00:53:01.560]   Because it's one of those nerd holidays Apple does every once in a while.
[00:53:01.560 --> 00:53:05.120]   WWDC the World Wide Developers Conference.
[00:53:05.120 --> 00:53:10.900]   And this one might be more important than a lot of Apple events in years past.
[00:53:10.900 --> 00:53:13.940]   It's expected, they will announce.
[00:53:13.940 --> 00:53:16.240]   We don't know what the name of it will be.
[00:53:16.240 --> 00:53:20.640]   Joanna Stern at the Wall Street Journal might call it their Apple nerd helmet.
[00:53:20.640 --> 00:53:27.080]   They're calling them nerd helmets from now on the VR spectacles.
[00:53:27.080 --> 00:53:29.080]   We don't or AR or mixed reality.
[00:53:29.080 --> 00:53:32.680]   We don't really know anything because Apple says nothing.
[00:53:32.680 --> 00:53:39.080]   But rumors have been pretty strong that Apple will announce this tomorrow.
[00:53:39.080 --> 00:53:40.080]   Are you excited?
[00:53:40.080 --> 00:53:41.760]   Christina, you're the youngest person here.
[00:53:41.760 --> 00:53:42.760]   This I know.
[00:53:42.760 --> 00:53:44.760]   Yeah, no, I am excited.
[00:53:44.760 --> 00:53:45.760]   Sorry.
[00:53:45.760 --> 00:53:46.760]   I'm excited.
[00:53:46.760 --> 00:53:47.760]   I'm okay.
[00:53:47.760 --> 00:53:53.440]   I'm like cautiously optimistic because the rumors we've been hearing about this for so long.
[00:53:53.440 --> 00:53:57.600]   I'm trying to wrap my mind around this because on the one hand.
[00:53:57.600 --> 00:54:02.760]   On the one hand we know this is a stupid category that nobody wants.
[00:54:02.760 --> 00:54:03.760]   Exactly.
[00:54:03.760 --> 00:54:07.440]   On the other hand, it's Apple and if anybody can make me spend $3,000 on something I don't
[00:54:07.440 --> 00:54:08.440]   want, it's them.
[00:54:08.440 --> 00:54:10.440]   So that's where I'm at.
[00:54:10.440 --> 00:54:11.440]   The max is.
[00:54:11.440 --> 00:54:12.440]   That's where I'm at.
[00:54:12.440 --> 00:54:13.440]   Yeah.
[00:54:13.440 --> 00:54:14.440]   Exactly.
[00:54:14.440 --> 00:54:19.040]   It seems on every level to be absolutely dumb and a complete waste of time.
[00:54:19.040 --> 00:54:22.080]   And on the other hand, I'm like, okay, but it's Apple.
[00:54:22.080 --> 00:54:23.520]   So that's what Mark Grumman said.
[00:54:23.520 --> 00:54:29.320]   He said, he says, if anyone can make this product category a success, it's Apple.
[00:54:29.320 --> 00:54:31.080]   That's fair.
[00:54:31.080 --> 00:54:37.840]   But I don't even know if Apple has the horsepower to make something that makes about 10% of
[00:54:37.840 --> 00:54:39.960]   its users nauseated.
[00:54:39.960 --> 00:54:42.400]   That seems a strike against it.
[00:54:42.400 --> 00:54:43.960]   Has no killer app.
[00:54:43.960 --> 00:54:54.600]   No real utility is very sexy the first time you use it, but has a very steep curve of loss
[00:54:54.600 --> 00:54:57.680]   of interest, shall we say.
[00:54:57.680 --> 00:55:00.600]   I just don't see this taken off.
[00:55:00.600 --> 00:55:02.000]   I really don't.
[00:55:02.000 --> 00:55:06.040]   And that would be a big deal if Apple were to put so and they've clearly put so much
[00:55:06.040 --> 00:55:09.880]   time, money, money, blood and treasure into this.
[00:55:09.880 --> 00:55:15.320]   If it were a flop, I think people would blame Tim Cook, to be honest.
[00:55:15.320 --> 00:55:17.240]   I think as I should.
[00:55:17.240 --> 00:55:22.600]   Well, yeah, I mean, look, we don't know, right?
[00:55:22.600 --> 00:55:23.600]   But I think you're right.
[00:55:23.600 --> 00:55:31.840]   I mean, I think that if this is not a success, I think this has to be at his feet, right?
[00:55:31.840 --> 00:55:33.920]   There have been products they've put out before.
[00:55:33.920 --> 00:55:37.200]   I think Apple TV is a good example where they've had to pivot.
[00:55:37.200 --> 00:55:39.440]   Apple Watch, I think, was more successful, right?
[00:55:39.440 --> 00:55:43.560]   Like it started out as a fashion object and then they had to pivot the fitness.
[00:55:43.560 --> 00:55:47.800]   And I would argue that it owns basically the entire smartwatch category.
[00:55:47.800 --> 00:55:49.280]   There are very few exceptions.
[00:55:49.280 --> 00:55:53.040]   I mean, it is they basically have the whole thing sewn up.
[00:55:53.040 --> 00:55:58.400]   So there you said you brought up Apple TV as an example of something.
[00:55:58.400 --> 00:56:02.880]   Well, I mean, Roku, Roku was an example of a company that nobody had ever heard of because
[00:56:02.880 --> 00:56:08.080]   they didn't exist until it hit the market way after Apple hit with a household name and
[00:56:08.080 --> 00:56:13.720]   it did much better in the TV market coming out with a product that was superior to what
[00:56:13.720 --> 00:56:14.720]   Apple came up.
[00:56:14.720 --> 00:56:17.200]   And Apple is still trying to catch up in that regard.
[00:56:17.200 --> 00:56:18.600]   So it's not inevitable.
[00:56:18.600 --> 00:56:24.520]   Now I have to admit, even though I've worked, I've written a number of guides for Reality
[00:56:24.520 --> 00:56:29.600]   Labs, which is the division of meta that does the virtual reality headsets, that there
[00:56:29.600 --> 00:56:34.440]   is a huge gap, an opportunity for someone to do this, right?
[00:56:34.440 --> 00:56:37.800]   Christina, you worked for a company that's had a device in the market for years.
[00:56:37.800 --> 00:56:42.720]   A pretty good device actually called Hollens, which I'm not sure because of price or whatever
[00:56:42.720 --> 00:56:46.480]   even it's just never become a big deal in the consumer market.
[00:56:46.480 --> 00:56:47.480]   But it's true.
[00:56:47.480 --> 00:56:48.480]   Yeah.
[00:56:48.480 --> 00:56:49.480]   I mean, they tried, right?
[00:56:49.480 --> 00:56:53.120]   They tried with the mixed reality headsets and it wasn't it didn't work.
[00:56:53.120 --> 00:56:55.760]   I mean, I think this has been the ongoing problem.
[00:56:55.760 --> 00:57:03.560]   There are very useful ways in industry to use, you know, AR, VR types of devices, even
[00:57:03.560 --> 00:57:07.400]   Google Glass, which I mean, they finally were tired, but they found success in the industry
[00:57:07.400 --> 00:57:08.400]   that way.
[00:57:08.400 --> 00:57:09.720]   Epson was doing interesting things.
[00:57:09.720 --> 00:57:11.520]   Hollens is a good example.
[00:57:11.520 --> 00:57:13.800]   But I still haven't seen the killer app for VR.
[00:57:13.800 --> 00:57:15.960]   I think for a lot of us, we thought it was gaining.
[00:57:15.960 --> 00:57:21.280]   If anybody was going to do that, I'd still think that that meta with the meta quest devices
[00:57:21.280 --> 00:57:26.880]   have probably come the closest at breaking through on that, you know, HTC and Steam have
[00:57:26.880 --> 00:57:30.520]   had the Vive headsets.
[00:57:30.520 --> 00:57:32.000]   But it's remained a niche thing.
[00:57:32.000 --> 00:57:39.560]   And so how do you define kind of a killer app and not only a killer app, but a killer
[00:57:39.560 --> 00:57:44.960]   app where people are going to be willing to spend the reported $3,000?
[00:57:44.960 --> 00:57:46.360]   I don't know.
[00:57:46.360 --> 00:57:48.520]   Those are my two big questions.
[00:57:48.520 --> 00:57:55.120]   The price is the real thing that has me going, you know, I don't know about this because
[00:57:55.120 --> 00:58:01.800]   if that price is to be believed, that just feels like that's an instant loss.
[00:58:01.800 --> 00:58:03.440]   I still know how you do that.
[00:58:03.440 --> 00:58:07.360]   One advantage of that price is you don't have to have a big installed base to justify
[00:58:07.360 --> 00:58:08.440]   the fact that you're doing it.
[00:58:08.440 --> 00:58:13.160]   In fact, I remember very early in the days of the Macintosh, Steve Jobs would compare
[00:58:13.160 --> 00:58:15.480]   the Mac to the Mercedes Benz and say, "Sure."
[00:58:15.480 --> 00:58:18.120]   Mercedes, the most popular car in the road, no.
[00:58:18.120 --> 00:58:19.800]   It's just, you know, a great car.
[00:58:19.800 --> 00:58:25.240]   And you know, they built a reputation based on small market share, but high value to those
[00:58:25.240 --> 00:58:28.200]   who were able to afford to buy the product.
[00:58:28.200 --> 00:58:32.880]   So it's conceivable that Apple could sell a small number of these, but still gain some
[00:58:32.880 --> 00:58:33.880]   kind of traction.
[00:58:33.880 --> 00:58:40.400]   Well, Apple has, you know, a small, less than half of the phone market globally.
[00:58:40.400 --> 00:58:43.160]   But easily the lion's share of profits.
[00:58:43.160 --> 00:58:44.160]   Right.
[00:58:44.160 --> 00:58:48.760]   And easily the lion's share of profits for developers, that would be my question, right?
[00:58:48.760 --> 00:58:52.480]   Because you're not wrong, you can absolutely make a profit having and have a very good
[00:58:52.480 --> 00:58:55.400]   business for a small niche device.
[00:58:55.400 --> 00:59:00.320]   My question would be in the era we're at where we pay, what we pay for applications,
[00:59:00.320 --> 00:59:05.120]   for your phone, for your tablet, even for your game console, are you going to be able
[00:59:05.120 --> 00:59:10.080]   to have enough volume where the people who could create these applications could dedicate
[00:59:10.080 --> 00:59:11.640]   the resources that you put into them?
[00:59:11.640 --> 00:59:13.240]   And that I'm not sure about, right?
[00:59:13.240 --> 00:59:19.720]   Because even big AAA, you know, game makers who have these massive budgets really struggle
[00:59:19.720 --> 00:59:21.480]   with that, what platform should we support?
[00:59:21.480 --> 00:59:23.640]   The Mac is actually a great example there.
[00:59:23.640 --> 00:59:25.920]   There are not real games on the Mac.
[00:59:25.920 --> 00:59:27.480]   The Mac is not a gaming platform.
[00:59:27.480 --> 00:59:30.360]   And it never has been and it probably never will be.
[00:59:30.360 --> 00:59:31.360]   Why?
[00:59:31.360 --> 00:59:33.720]   Because there's just not the market for it.
[00:59:33.720 --> 00:59:34.720]   And it's not worth the investment.
[00:59:34.720 --> 00:59:38.840]   The companies that have put the money into it have not gotten that investment back.
[00:59:38.840 --> 00:59:44.520]   Where you do see it as for casual types of games and things that tend to be laden with
[00:59:44.520 --> 00:59:49.160]   a lot of VINAP purchases and recurring subscriptions and things that Apple likes a whole lot,
[00:59:49.160 --> 00:59:53.720]   but not necessarily that the hardcore gamers want.
[00:59:53.720 --> 00:59:59.760]   But when you look at the multi-trillion dollar kind of gaming business, the big AAA titles
[00:59:59.760 --> 01:00:01.120]   aren't on Mac products.
[01:00:01.120 --> 01:00:05.760]   So they could be a success even with a very small clientele.
[01:00:05.760 --> 01:00:10.480]   I worry and that part like, okay, well, what about the people building on that platform,
[01:00:10.480 --> 01:00:11.480]   right?
[01:00:11.480 --> 01:00:16.040]   Because Mercedes can be profitable, but like, you know, Apple Apple can make money out
[01:00:16.040 --> 01:00:17.040]   of it.
[01:00:17.040 --> 01:00:18.040]   What about anybody else?
[01:00:18.040 --> 01:00:22.920]   Larry's when he brought up the early Mac, that was a really good point.
[01:00:22.920 --> 01:00:32.480]   And you think about the single application that carried the Mac into the high popularity
[01:00:32.480 --> 01:00:37.840]   where it was profitable for everybody in a desktop publishing.
[01:00:37.840 --> 01:00:43.520]   And that required a number of things to come together, including a laser printer that was
[01:00:43.520 --> 01:00:44.760]   affordable.
[01:00:44.760 --> 01:00:51.440]   And so maybe there are some if these rumors are true and I find the $3,000 thing, I just
[01:00:51.440 --> 01:00:52.440]   don't believe it.
[01:00:52.440 --> 01:00:55.040]   I just, I think that's absurd.
[01:00:55.040 --> 01:01:06.440]   But I think the, any seriously interesting application, if the headset is wonderfully
[01:01:06.440 --> 01:01:13.360]   better than anything that's come so far, is, will sell enough to get the next generation
[01:01:13.360 --> 01:01:14.360]   going.
[01:01:14.360 --> 01:01:20.400]   But, you know, the killer app for VR is better headsets, smaller, lighter, and that don't
[01:01:20.400 --> 01:01:21.680]   give you nausea.
[01:01:21.680 --> 01:01:22.680]   Right.
[01:01:22.680 --> 01:01:24.120]   You know, vertigo.
[01:01:24.120 --> 01:01:28.000]   And once all that's settled, I'll buy one.
[01:01:28.000 --> 01:01:34.640]   I want something that I can wear on an airplane and watch iMac's movies on an airplane.
[01:01:34.640 --> 01:01:35.640]   Totally.
[01:01:35.640 --> 01:01:37.720]   That's that, you know, like, you know, like, you know, that's the killer app.
[01:01:37.720 --> 01:01:42.800]   I have a lot of gadgets, including two VR headsets I've got with the Quest and the Quest
[01:01:42.800 --> 01:01:43.800]   Pro.
[01:01:43.800 --> 01:01:47.000]   Many of my gadgets have sit and draw us because I just don't get around to using them or I
[01:01:47.000 --> 01:01:48.960]   don't want to use them particularly.
[01:01:48.960 --> 01:01:51.160]   In the case of the VR headset, I avoid using it.
[01:01:51.160 --> 01:01:52.600]   I actually don't enjoy it.
[01:01:52.600 --> 01:01:54.280]   I mean, it's a negative.
[01:01:54.280 --> 01:01:56.400]   I can't wait to get it off.
[01:01:56.400 --> 01:01:59.720]   And that's not a very good sign for loving it.
[01:01:59.720 --> 01:02:01.160]   It's not even that I even love the product.
[01:02:01.160 --> 01:02:03.640]   I just, I really don't like it at all.
[01:02:03.640 --> 01:02:05.040]   I mean, it's a negative.
[01:02:05.040 --> 01:02:09.200]   I play Beat Saber Delizos songs and I think that's kind of fun, but...
[01:02:09.200 --> 01:02:10.200]   For how long?
[01:02:10.200 --> 01:02:11.200]   For how long?
[01:02:11.200 --> 01:02:12.200]   For how long?
[01:02:12.200 --> 01:02:13.400]   Not for how long is it for my life.
[01:02:13.400 --> 01:02:14.400]   Half hour sessions.
[01:02:14.400 --> 01:02:15.800]   No, no, no, no, no.
[01:02:15.800 --> 01:02:16.800]   And I have a pick it up lately.
[01:02:16.800 --> 01:02:21.320]   I got the Quest Pro because I want to say, well, what's the best in the market today?
[01:02:21.320 --> 01:02:22.320]   That was 16-hour bucks.
[01:02:22.320 --> 01:02:27.680]   It's now down to 1,000, which tells you a little bit about its sales figures.
[01:02:27.680 --> 01:02:33.280]   Meta just announced the Quest 3 for $4.99.99.
[01:02:33.280 --> 01:02:38.680]   Timing it, of course, the announcement perfectly to make sure that Apple didn't steal its thunder.
[01:02:38.680 --> 01:02:44.240]   But Meta's put a lot of money, billions, tens of billions of dollars into VR and yet
[01:02:44.240 --> 01:02:47.720]   to find a market for it.
[01:02:47.720 --> 01:02:53.760]   I just feel like Apple is sailing up to the new world in their boat and then they look
[01:02:53.760 --> 01:02:57.120]   and they say, "Oh, look, there's Google on the rocks.
[01:02:57.120 --> 01:03:00.320]   Oh, oh dear, there's Microsoft on the rocks.
[01:03:00.320 --> 01:03:02.520]   Oh, my.
[01:03:02.520 --> 01:03:03.800]   Here's Meta on the rocks.
[01:03:03.800 --> 01:03:04.800]   I don't know.
[01:03:04.800 --> 01:03:08.040]   Is there anywhere safe to land in this territory?"
[01:03:08.040 --> 01:03:10.720]   Now, I will be watching...
[01:03:10.720 --> 01:03:17.400]   We're going to stream it, of course, tomorrow morning, 10 AM Pacific, 1 PM Eastern at live.twit.tv.
[01:03:17.400 --> 01:03:22.680]   I will be watching very carefully the verbiage because I think what we really want to know
[01:03:22.680 --> 01:03:26.840]   is how Apple's positioning this.
[01:03:26.840 --> 01:03:34.000]   It's likely that it's a developer kit, not even a consumer product, and it won't be out
[01:03:34.000 --> 01:03:36.400]   according to rumors until this fall.
[01:03:36.400 --> 01:03:38.560]   It's likely it's the...
[01:03:38.560 --> 01:03:47.160]   I expect their positioning will be we are aiming at a product down the road like...
[01:03:47.160 --> 01:03:48.360]   I don't know.
[01:03:48.360 --> 01:03:50.520]   This is the question is, when can they do this?
[01:03:50.520 --> 01:03:54.040]   That is lightweight spectacles like, "Yes, won't nauseate you."
[01:03:54.040 --> 01:03:56.680]   Is more of an AR product.
[01:03:56.680 --> 01:04:01.080]   I'm sure Apple's doing this because they want to say, "Well, what's next after the iPhone?"
[01:04:01.080 --> 01:04:05.360]   I've seen some pundits say, "This is the next iPhone."
[01:04:05.360 --> 01:04:12.040]   The technologies aren't there yet for a long battery life, for a lightweight heads-up display
[01:04:12.040 --> 01:04:14.920]   that you can wear much like you would wear your spectacles.
[01:04:14.920 --> 01:04:18.720]   I think that's five years away, four years away.
[01:04:18.720 --> 01:04:20.440]   And that, by the way, has said the same thing.
[01:04:20.440 --> 01:04:27.760]   They are also going for developing a lightweight AR glasses, which I think is what most people
[01:04:27.760 --> 01:04:31.160]   think is the holy grail of this whole extended reality.
[01:04:31.160 --> 01:04:33.400]   Let's put it this way.
[01:04:33.400 --> 01:04:36.000]   It's the only thing we...
[01:04:36.000 --> 01:04:40.440]   Dan aside, watching his iMacs movie on an airplane, it's the only thing I can really
[01:04:40.440 --> 01:04:45.560]   see as a mass market for this is something you could wear in lieu of spectacles.
[01:04:45.560 --> 01:04:48.520]   They could even be corrective that have a heads-up display.
[01:04:48.520 --> 01:04:49.520]   They could...
[01:04:49.520 --> 01:04:53.600]   You're not going to get nauseous because you're still seeing the world.
[01:04:53.600 --> 01:04:56.080]   So that's going to solve the nausea problem.
[01:04:56.080 --> 01:04:57.680]   But you're getting additional information.
[01:04:57.680 --> 01:04:58.680]   It replaces your phone.
[01:04:58.680 --> 01:05:02.280]   You've got temple pieces that can give you audio.
[01:05:02.280 --> 01:05:07.240]   You have some way, I don't know how, of signaling, of choosing, of controlling it.
[01:05:07.240 --> 01:05:10.640]   I don't know if it's by blinking or where you look or maybe you have something in your
[01:05:10.640 --> 01:05:12.320]   pocket that you're clicking.
[01:05:12.320 --> 01:05:13.600]   Coach, the temple.
[01:05:13.600 --> 01:05:14.600]   It's a...
[01:05:14.600 --> 01:05:17.320]   You can imagine something like that.
[01:05:17.320 --> 01:05:19.800]   But the technologies are not there yet.
[01:05:19.800 --> 01:05:20.800]   Not yet.
[01:05:20.800 --> 01:05:22.040]   And it's like AI.
[01:05:22.040 --> 01:05:28.040]   It's like you're throwing your dart into the distance and hoping you can hit that target.
[01:05:28.040 --> 01:05:33.880]   You know, the thing about what you're saying, Leo, is actually very profound in a sense.
[01:05:33.880 --> 01:05:39.320]   You give Steve Jobs a lot of credit for inventing the iPod or for being the CEO of Apple when
[01:05:39.320 --> 01:05:40.760]   the iPod was invented.
[01:05:40.760 --> 01:05:44.080]   But really the credit goes to storage, batteries.
[01:05:44.080 --> 01:05:47.800]   There's a lot of technologies that made it possible to build the iPod.
[01:05:47.800 --> 01:05:52.320]   It wasn't that if he had had an idea for an iPod 20 years before that, he could have
[01:05:52.320 --> 01:05:54.920]   conceived of it, but he kind of built it accordingly.
[01:05:54.920 --> 01:05:57.720]   Well, the Newton was John Scully's vision for...
[01:05:57.720 --> 01:05:58.720]   Right.
[01:05:58.720 --> 01:05:59.720]   It wasn't it yet.
[01:05:59.720 --> 01:06:00.720]   Well, the net.
[01:06:00.720 --> 01:06:05.280]   This could be Apple's Newton for VR or AR or mixed reality.
[01:06:05.280 --> 01:06:12.160]   This could be that prototype, very clunky product, but you're kind of counting on a
[01:06:12.160 --> 01:06:13.480]   lot of technologies to develop.
[01:06:13.480 --> 01:06:18.040]   Now, you're right on the iPhone, but Apple also facilitated that.
[01:06:18.040 --> 01:06:26.560]   They basically built the Chinese ability to create those things with massive investment.
[01:06:26.560 --> 01:06:27.560]   So it's...
[01:06:27.560 --> 01:06:29.720]   That's true, but they didn't invent better batteries.
[01:06:29.720 --> 01:06:30.560]   None of it.
[01:06:30.560 --> 01:06:31.560]   It facilitated it, right?
[01:06:31.560 --> 01:06:32.560]   Yeah.
[01:06:32.560 --> 01:06:35.120]   That technology had to get here.
[01:06:35.120 --> 01:06:43.640]   So all of this is back to Moore's law and its equivalence continuing with things getting
[01:06:43.640 --> 01:06:50.520]   smaller and lighter and more powerful in exponential ways.
[01:06:50.520 --> 01:06:59.000]   So it isn't that hard to look a few years out and think about how quickly micro LED
[01:06:59.000 --> 01:07:07.120]   is coming along, which was a fantasy just several years ago.
[01:07:07.120 --> 01:07:12.640]   The people who do the hardware are constantly doing stuff that blows my mind.
[01:07:12.640 --> 01:07:18.800]   And it's really going to be the hardware people who make possible the software piece
[01:07:18.800 --> 01:07:20.280]   to be overlaid on it.
[01:07:20.280 --> 01:07:27.040]   And as Larry had said about the iPod originally, and I get...
[01:07:27.040 --> 01:07:34.160]   It may be closer than we think, but positioning this thing tomorrow is really going to be
[01:07:34.160 --> 01:07:38.360]   interesting to watch because the...
[01:07:38.360 --> 01:07:44.280]   If they position it properly, which is to say, "Hey, we're taking a long-term view here
[01:07:44.280 --> 01:07:49.920]   and make that the central point as opposed to all of the...
[01:07:49.920 --> 01:07:50.920]   Right.
[01:07:50.920 --> 01:08:00.040]   What you can do today, which isn't much, if they can get the public to understand...
[01:08:00.040 --> 01:08:06.280]   And I got to say, Facebook, Meta didn't do a great job of that.
[01:08:06.280 --> 01:08:11.440]   They sort of presented it as a here and now when, in fact, they were looking long-term
[01:08:11.440 --> 01:08:13.960]   and misplayed that.
[01:08:13.960 --> 01:08:20.640]   But if Apple and others in this market can really help the public understand that this
[01:08:20.640 --> 01:08:29.240]   is a play that feels long-term, but it genuinely is coming.
[01:08:29.240 --> 01:08:38.880]   And here are the building blocks and we can easily predict based on hardware improvements
[01:08:38.880 --> 01:08:44.920]   in the past that transfer into the future at the same kind of Moore's Law, et cetera,
[01:08:44.920 --> 01:08:46.920]   rate.
[01:08:46.920 --> 01:08:57.640]   The case can really be made for this product tomorrow as the beachhead that the public
[01:08:57.640 --> 01:08:59.960]   needs to understand where it's going to go.
[01:08:59.960 --> 01:09:03.560]   You know, there's really two audiences though.
[01:09:03.560 --> 01:09:10.680]   This is a developer conference and so, yes, they have to convince the public...
[01:09:10.680 --> 01:09:11.680]   I think they have to...
[01:09:11.680 --> 01:09:18.160]   You're right, downplay expectations for the public because they don't want people to say,
[01:09:18.160 --> 01:09:20.480]   "Well, I don't want that."
[01:09:20.480 --> 01:09:25.000]   They want people to think or to understand this is not what we're going to sell you.
[01:09:25.000 --> 01:09:27.240]   This is not for you.
[01:09:27.240 --> 01:09:28.960]   We're aiming down the road.
[01:09:28.960 --> 01:09:33.760]   And then they also have to convince developers, "This is something for you because down the
[01:09:33.760 --> 01:09:36.240]   road this is going to be a world beater."
[01:09:36.240 --> 01:09:38.920]   Christina, you talk to developers all the time.
[01:09:38.920 --> 01:09:42.080]   How do you position that message to developers?
[01:09:42.080 --> 01:09:44.640]   What if you were an Apple's shoes?
[01:09:44.640 --> 01:09:53.160]   I mean, I think you have to have some really good first party examples as I think how you
[01:09:53.160 --> 01:09:54.160]   would have to position that.
[01:09:54.160 --> 01:10:00.760]   I think you have to have some really good kind of first party examples of some things
[01:10:00.760 --> 01:10:06.560]   that why this is worth investing in and why this is worth building on even though the
[01:10:06.560 --> 01:10:10.360]   future that we are at right now might not be where we need to be.
[01:10:10.360 --> 01:10:11.440]   I think that's what you have to do.
[01:10:11.440 --> 01:10:17.880]   I think you have to parlay that this is the step and show some of the exciting things that
[01:10:17.880 --> 01:10:18.880]   you can do now.
[01:10:18.880 --> 01:10:19.880]   Right now.
[01:10:19.880 --> 01:10:26.680]   There is speculation that No Man's Sky, which is a PC game that has been adapted for
[01:10:26.680 --> 01:10:33.720]   Meta's Quest, will be there demonstrating their Mac version of it on this.
[01:10:33.720 --> 01:10:34.720]   So that would be gaming.
[01:10:34.720 --> 01:10:38.560]   But you'd have to do products if you'd have to do social.
[01:10:38.560 --> 01:10:39.560]   Yeah.
[01:10:39.560 --> 01:10:45.400]   I mean, look, I can say in my own mind, I'm trying to justify how I would spend $3,000.
[01:10:45.400 --> 01:10:47.800]   And Dan, I can't believe that price is real.
[01:10:47.800 --> 01:10:50.240]   Maybe if it's a developer kit, then that would actually make sense.
[01:10:50.240 --> 01:10:54.840]   But I can't think that that is what they would bring an actual consumer product out for.
[01:10:54.840 --> 01:10:59.400]   But in my mind, I have to think, okay, what if I could have the equivalent of four high
[01:10:59.400 --> 01:11:04.920]   resolution displays in my field of vision and I could control my Mac that way?
[01:11:04.920 --> 01:11:06.480]   That could be a productivity thing, right?
[01:11:06.480 --> 01:11:09.400]   That could be something that I could really enjoy where I go, okay, instead of having to
[01:11:09.400 --> 01:11:17.400]   have my 227 inch 5K monitors, I have this headset and I can have a really good job of
[01:11:17.400 --> 01:11:18.400]   working in that space.
[01:11:18.400 --> 01:11:22.160]   And that means that even when I'm on the run, on the go, when I'm working remotely, which
[01:11:22.160 --> 01:11:25.760]   is not uncommon, I have access to all of that.
[01:11:25.760 --> 01:11:30.200]   So maybe that would be a productivity thing.
[01:11:30.200 --> 01:11:36.040]   But yeah, I mean, gaming is obviously the very easy thing to kind of go out with this
[01:11:36.040 --> 01:11:39.360]   space.
[01:11:39.360 --> 01:11:42.760]   This is why I'm excited to watch tomorrow because one way or another, this is going
[01:11:42.760 --> 01:11:43.760]   to be interesting.
[01:11:43.760 --> 01:11:48.160]   Either we're all going to be convinced, even if it's only four of the 90 minutes or however
[01:11:48.160 --> 01:11:54.920]   long the keynote is, or we're going to walk away and go, still don't quite see it.
[01:11:54.920 --> 01:11:58.080]   And so either way, these two outcomes are really interesting.
[01:11:58.080 --> 01:12:01.600]   This will be a masterclass in the expectation setting.
[01:12:01.600 --> 01:12:03.080]   Yes.
[01:12:03.080 --> 01:12:08.880]   For two different audiences, consumer and developer, I don't know if it'll be Tim Cook,
[01:12:08.880 --> 01:12:10.920]   probably won't be Tim Cook, might be Avi.
[01:12:10.920 --> 01:12:13.640]   They have some very good technical people there.
[01:12:13.640 --> 01:12:22.360]   I'm not sure who it will be, but I will be deconstructing on the fly because I think
[01:12:22.360 --> 01:12:26.000]   there's going to be a lot between the lines as we listen to Apple position.
[01:12:26.000 --> 01:12:27.000]   Yes.
[01:12:27.000 --> 01:12:28.000]   No, I agree with you.
[01:12:28.000 --> 01:12:31.840]   I mean, what's interesting to me to look at as a comparison, we were talking about the
[01:12:31.840 --> 01:12:36.680]   Newton and we were talking about the TV and some other things, the iPad, which obviously
[01:12:36.680 --> 01:12:40.080]   Steve Jobs introduced so that goes a lot further.
[01:12:40.080 --> 01:12:43.920]   What was interesting about that device was that it did have this massive expectation
[01:12:43.920 --> 01:12:44.920]   behind it.
[01:12:44.920 --> 01:12:47.200]   We had all wanted a tablet Mac for a long time.
[01:12:47.200 --> 01:12:48.800]   A lot of us still do.
[01:12:48.800 --> 01:12:53.560]   I still, I went from being a long time detractor and saying we don't need touchscreen Macs to
[01:12:53.560 --> 01:12:55.360]   now I'm, I've come all over.
[01:12:55.360 --> 01:12:59.720]   I'm like, we need touchscreen Macs, even if that's not the primary interface, it's getting
[01:12:59.720 --> 01:13:01.200]   silly at this point.
[01:13:01.200 --> 01:13:06.120]   But what they did with the iPad, which, you know, that original hardware was not that
[01:13:06.120 --> 01:13:07.120]   great.
[01:13:07.120 --> 01:13:08.320]   It was actually fairly limited.
[01:13:08.320 --> 01:13:11.920]   It only had, I think, 256 megabytes of RAM.
[01:13:11.920 --> 01:13:17.520]   And, you know, the iPad 2 was a much better product and was, was really killer.
[01:13:17.520 --> 01:13:24.040]   But what they did show off and what you could do with it was so impressive that no one else
[01:13:24.040 --> 01:13:27.480]   has even really been able to meaningfully enter the tablet space.
[01:13:27.480 --> 01:13:30.840]   I mean, you have low-end Android tablets, but that's it.
[01:13:30.840 --> 01:13:33.880]   Everyone tries to come in at a higher point, can't do it.
[01:13:33.880 --> 01:13:40.080]   And the iPad, you know, here we are, what, 13 years later is it's fallen off.
[01:13:40.080 --> 01:13:44.520]   It's maybe not as, it's not the iPhone, but it still makes substantial amount of money.
[01:13:44.520 --> 01:13:50.360]   And so I, the way that they were able to kind of show off and get you to see how it could
[01:13:50.360 --> 01:13:54.640]   be useful and fit into your life and, and open up possibilities for developers to,
[01:13:54.640 --> 01:13:58.400]   you know, build things off of it was really interesting.
[01:13:58.400 --> 01:14:02.720]   And so my, my hope would be that they could, they could kind of capture what they did with,
[01:14:02.720 --> 01:14:08.480]   the iPad, even though that initial device, like even the one generation later was better.
[01:14:08.480 --> 01:14:12.720]   And two generations later, when they, not not the third one, which is the one that I bought,
[01:14:12.720 --> 01:14:17.360]   and then they changed the connector immediately and made the battery not as terrible.
[01:14:17.360 --> 01:14:23.120]   But the retina model, you know, they made these iterative changes on that device very quickly.
[01:14:23.120 --> 01:14:30.360]   And the literature, the promise that they showed off, you know, in 2010, but, but we all saw what it was
[01:14:30.640 --> 01:14:36.200]   when it came out. And that would be my, my, my, I think the best case scenario for them.
[01:14:36.200 --> 01:14:39.120]   You know, I think about about full self-driving.
[01:14:39.120 --> 01:14:43.440]   So you've got Tesla, which is out there basically every other week claiming that only tomorrow
[01:14:43.440 --> 01:14:45.000]   we're going to have it. It's going to be perfect.
[01:14:45.000 --> 01:14:47.520]   You'll be able to sit in the backseat, smoke a joint and do it.
[01:14:47.520 --> 01:14:53.240]   And then you have every other car company with their head down, working on towards full self-driving,
[01:14:53.240 --> 01:14:58.320]   driver assisted and, you know, Cadillac's doing a pretty good job, Ford's doing a pretty good job.
[01:14:59.440 --> 01:15:01.560]   Some of the European companies are doing a good job.
[01:15:01.560 --> 01:15:04.840]   None of them are making the kinds of claims that Elon Musk makes.
[01:15:04.840 --> 01:15:07.520]   And none of them are saying that we're going to deliver this tomorrow.
[01:15:07.520 --> 01:15:14.160]   But you know that eventually every car or everybody will be able to spend money and get a self-driving car
[01:15:14.160 --> 01:15:17.160]   because every, in these companies are invested in it.
[01:15:17.160 --> 01:15:19.920]   And I think that there is a lesson to be learned in this.
[01:15:19.920 --> 01:15:25.280]   Now, Musk has done well, but actually not that many people are suckers like me that have paid.
[01:15:25.280 --> 01:15:28.320]   In my case, I only paid seven grand for full self-driving.
[01:15:29.160 --> 01:15:30.520]   If he never get it.
[01:15:30.520 --> 01:15:35.320]   Well, no, I mean, I've got five thousand and never got it.
[01:15:35.320 --> 01:15:42.360]   Well, I've got the data, but my car will be in the junkyard by the time the full true self-driving is available.
[01:15:42.360 --> 01:15:44.800]   So we're suckers. Yeah, of course.
[01:15:44.800 --> 01:15:48.880]   So what puzzles me here is that maybe the difference between
[01:15:48.880 --> 01:15:53.080]   Tesla and these other companies is that their CEOs
[01:15:53.080 --> 01:15:56.640]   take seriously securities laws.
[01:15:56.840 --> 01:16:00.800]   And and actually think that they might get punished for
[01:16:00.800 --> 01:16:05.680]   bullshitting the public about things that are material to their
[01:16:05.680 --> 01:16:09.440]   results and but Elon is demeanory.
[01:16:09.440 --> 01:16:11.360]   Obviously not enforced against Elon.
[01:16:11.360 --> 01:16:13.360]   Yes, Elon is demonstrated. You don't have to.
[01:16:13.360 --> 01:16:17.040]   It's really, really interesting time when it comes to normative values.
[01:16:17.040 --> 01:16:20.760]   Donald Trump showed that you really can exceed the norm
[01:16:20.760 --> 01:16:24.640]   drastically and suffer no consequences, at least up to this point.
[01:16:24.920 --> 01:16:29.520]   Elon's done the same. I think somewhat taking a page from the Donald Trump playbook.
[01:16:29.520 --> 01:16:35.240]   I fear for a society where the norms are no longer enforced.
[01:16:35.240 --> 01:16:38.680]   I think that that's a serious society.
[01:16:38.680 --> 01:16:43.040]   I got an over the air update just a couple of days ago and it's forgotten a lot better.
[01:16:43.040 --> 01:16:44.960]   I mean, the FSD.
[01:16:44.960 --> 01:16:46.880]   Would you trust it though? Would you trust it?
[01:16:46.880 --> 01:16:48.560]   Oh, no, not completely.
[01:16:48.560 --> 01:16:50.320]   I always have my foot one inch over the brake.
[01:16:50.320 --> 01:16:51.800]   Yeah, me too. Yeah, they will.
[01:16:52.200 --> 01:16:55.440]   But I just want to make it more stressful and not. Yes.
[01:16:55.440 --> 01:16:56.600]   Oh, absolutely.
[01:16:56.600 --> 01:16:59.720]   I got to worry about driving the other guy driving in the car.
[01:16:59.720 --> 01:17:00.360]   Driving. Sure.
[01:17:00.360 --> 01:17:03.120]   They're in proof and they could screw up.
[01:17:03.120 --> 01:17:05.320]   I have. And you're paying your pain.
[01:17:05.320 --> 01:17:09.480]   Thousands of dollars to have more stress when you're in your car.
[01:17:09.480 --> 01:17:13.040]   Can you guys please explain that?
[01:17:13.040 --> 01:17:14.200]   I think I'm thinking there.
[01:17:14.200 --> 01:17:18.160]   I have a Ford Mocke which has their blue crews.
[01:17:18.160 --> 01:17:21.640]   It is hands free, but it is very limited.
[01:17:21.640 --> 01:17:22.880]   It's only on highways.
[01:17:22.880 --> 01:17:23.920]   They've mapped.
[01:17:23.920 --> 01:17:25.200]   It is not on local street.
[01:17:25.200 --> 01:17:29.320]   This is where Elon, I think, really overestimated the capability of the system.
[01:17:29.320 --> 01:17:31.760]   You should not be doing that on city streets, right?
[01:17:31.760 --> 01:17:35.520]   I do. But again, my foot one inch over the brake.
[01:17:35.520 --> 01:17:36.760]   Yeah, that would terrify me.
[01:17:36.760 --> 01:17:39.920]   Blue crews, I still have to pay attention, but I feel a lot more
[01:17:39.920 --> 01:17:44.160]   sanguine and it turns off and beeps at you and says, you need to take control.
[01:17:44.160 --> 01:17:48.560]   When it gets to a situation, there is a curve going down a meringue
[01:17:49.480 --> 01:17:52.960]   that the Tesla almost always would steer toward the divider.
[01:17:52.960 --> 01:17:55.160]   I would always have to say, no, no, no, no, no, no.
[01:17:55.160 --> 01:17:58.600]   And the blue crews turns off at that point, is take control.
[01:17:58.600 --> 01:18:01.000]   So there's something about that curve that cameras.
[01:18:01.000 --> 01:18:04.120]   Well, that's why I gave you that example, because that's an example of a mature
[01:18:04.120 --> 01:18:08.360]   company taking a position that will eventually be usable technology.
[01:18:08.360 --> 01:18:11.880]   But we're going to roll it out gradually as opposed to Elon who basically
[01:18:11.880 --> 01:18:13.240]   likes to blow things up.
[01:18:13.240 --> 01:18:17.240]   So my next car, I think, is going to be a BMW i5
[01:18:17.720 --> 01:18:18.960]   with full self driving.
[01:18:18.960 --> 01:18:24.800]   And they say, you're going to be able to change lanes by looking in the side mirror.
[01:18:24.800 --> 01:18:28.700]   How do you know that's the reason you're looking at the
[01:18:28.700 --> 01:18:30.000]   car? That's going to be a little scary.
[01:18:30.000 --> 01:18:32.880]   Yeah, I might turn that feature off.
[01:18:32.880 --> 01:18:37.960]   Yeah, that doesn't seem like anybody has thought that through super well.
[01:18:37.960 --> 01:18:40.400]   But it's been in the side here and I want to change lanes.
[01:18:40.400 --> 01:18:42.480]   OK, not looking for any other reason.
[01:18:42.480 --> 01:18:46.640]   No, if you leave the default on death, it'll change lanes just on its own.
[01:18:47.000 --> 01:18:48.560]   I'm driving along, having a great day.
[01:18:48.560 --> 01:18:52.440]   And all of a sudden I'm finding myself changing lanes for reasons that I can't
[01:18:52.440 --> 01:18:53.240]   quite understand.
[01:18:53.240 --> 01:18:56.440]   So in some ways, it may be better than what I've got.
[01:18:56.440 --> 01:18:57.640]   Oh, Lord.
[01:18:57.640 --> 01:19:00.240]   Anyway, tomorrow is going to be a big day.
[01:19:00.240 --> 01:19:03.120]   We're going to smell what Tim's cooking and.
[01:19:03.120 --> 01:19:05.760]   Oh, that's bad.
[01:19:05.760 --> 01:19:06.360]   And that's good.
[01:19:06.360 --> 01:19:07.560]   Yeah, we'll find out.
[01:19:07.560 --> 01:19:10.000]   I hope you will tune in for our live streaming coverage.
[01:19:10.000 --> 01:19:13.720]   Well, I was going to do it with Mike, a sergeant who got a last minute invite.
[01:19:13.720 --> 01:19:14.800]   So he's going down there.
[01:19:14.800 --> 01:19:16.200]   So it's a good job.
[01:19:16.200 --> 01:19:17.200]   Good job, Mike.
[01:19:17.200 --> 01:19:18.280]   Yeah, no kidding.
[01:19:18.280 --> 01:19:20.880]   He called his context and said, Hey, what about me?
[01:19:20.880 --> 01:19:23.160]   Yeah, I call my context.
[01:19:23.160 --> 01:19:24.120]   They go, who are you?
[01:19:24.120 --> 01:19:26.880]   So it's a different, it's a different experience for Micah.
[01:19:26.880 --> 01:19:31.440]   We will be here and we're going to open up the club to it stage.
[01:19:31.440 --> 01:19:33.960]   So if you're a member of club to it, get into the stage.
[01:19:33.960 --> 01:19:37.000]   So because I don't have a cohost, you're the cohost.
[01:19:37.000 --> 01:19:41.480]   I'd love to get the club to it members and your thoughts as we listen together
[01:19:41.480 --> 01:19:43.120]   to what Tim's cooking.
[01:19:44.560 --> 01:19:48.600]   That's tomorrow 10 AM Pacific 1 PM Eastern for our live coverage.
[01:19:48.600 --> 01:19:52.320]   I think, Christina, you've covered Apple for years.
[01:19:52.320 --> 01:19:55.040]   I think I might stick around for the state of the union.
[01:19:55.040 --> 01:19:58.360]   I think that second keynote in the afternoon might be.
[01:19:58.360 --> 01:19:59.280]   That's always my favorite.
[01:19:59.280 --> 01:19:59.720]   Yeah.
[01:19:59.720 --> 01:20:02.640]   So to be honest with you, that's always my favorite.
[01:20:02.640 --> 01:20:06.440]   Like the big keynote is always, you know, the big pomp and circumstance.
[01:20:06.440 --> 01:20:09.240]   And you get the product announcements and you get to see the features.
[01:20:09.240 --> 01:20:13.000]   And then the state of the union is when like you really get to nerd out and you
[01:20:13.000 --> 01:20:14.360]   really get to see, okay, what can I take?
[01:20:14.360 --> 01:20:18.400]   The advantage of what are the things that were glossed over that that are really
[01:20:18.400 --> 01:20:19.200]   going to be interesting?
[01:20:19.200 --> 01:20:23.840]   That's that's the one I think, especially with this rumored, you know, XROS or
[01:20:23.840 --> 01:20:27.320]   whatever it's going to be called that I think will really, really be important to
[01:20:27.320 --> 01:20:31.920]   see, especially for anybody who has a whether you you code or not.
[01:20:31.920 --> 01:20:35.600]   For those of us who do obviously think of a greater interest, but even if you
[01:20:35.600 --> 01:20:41.680]   don't just to kind of get an idea of this is what people will be able to build on
[01:20:41.680 --> 01:20:42.520]   going forward.
[01:20:42.560 --> 01:20:44.160]   And these are some of the new things.
[01:20:44.160 --> 01:20:47.000]   And so that's exciting.
[01:20:47.000 --> 01:20:52.320]   Yeah, it's fun to kind of, for me, it's always been fun to take the next step and
[01:20:52.320 --> 01:20:57.320]   to kind of try to understand what the what the strategy is.
[01:20:57.320 --> 01:20:59.120]   You know, I'm a chess player.
[01:20:59.120 --> 01:21:01.480]   I like to kind of think about strategy.
[01:21:01.480 --> 01:21:05.280]   And this is one where Apple's going to really have to step up, I think.
[01:21:05.280 --> 01:21:05.680]   Yeah.
[01:21:05.680 --> 01:21:06.480]   A lot different though.
[01:21:06.480 --> 01:21:08.000]   It was when Steve Jobs was doing it.
[01:21:08.000 --> 01:21:11.520]   I remember I would always go to these events and I always go in person and I'd
[01:21:11.520 --> 01:21:14.400]   walk away, say, Oh my God, I'm going to buy that the moment it comes out.
[01:21:14.400 --> 01:21:15.200]   It's so amazing.
[01:21:15.200 --> 01:21:17.160]   And then the next day I'll think about it.
[01:21:17.160 --> 01:21:19.280]   Say, well, maybe it wasn't as revolutionary.
[01:21:19.280 --> 01:21:21.920]   Famous reality distortion field.
[01:21:21.920 --> 01:21:22.080]   Right.
[01:21:22.080 --> 01:21:22.400]   It worked.
[01:21:22.400 --> 01:21:22.640]   Yep.
[01:21:22.640 --> 01:21:23.520]   We're done to work.
[01:21:23.520 --> 01:21:24.000]   Yeah.
[01:21:24.000 --> 01:21:25.720]   I still get that.
[01:21:25.720 --> 01:21:28.400]   I even got it during the Microsoft keynote.
[01:21:28.400 --> 01:21:32.600]   I get excited by new technology and I always have to kind of pinch myself and
[01:21:32.600 --> 01:21:35.680]   say about how realistic is this?
[01:21:35.680 --> 01:21:38.240]   You know, I worked for years with John C.
[01:21:38.240 --> 01:21:43.840]   Devorak, who was famous for being the fact he even had a column in the back of
[01:21:43.840 --> 01:21:46.120]   Mac user magazine called the anti editor.
[01:21:46.120 --> 01:21:48.400]   He was famous for whenever it is.
[01:21:48.400 --> 01:21:49.120]   I'm again it.
[01:21:49.120 --> 01:21:54.600]   And the funny thing is the technology, you know, you do that 100% of the time
[01:21:54.600 --> 01:21:56.280]   and you're right about 80% of the time.
[01:21:56.280 --> 01:21:59.040]   Most stuff isn't going to change the way.
[01:21:59.040 --> 01:21:59.480]   Right.
[01:21:59.480 --> 01:22:00.560]   It's true.
[01:22:00.560 --> 01:22:01.080]   It's true.
[01:22:01.080 --> 01:22:03.600]   But you know, but there are those times when it does, right?
[01:22:03.600 --> 01:22:07.400]   Like the iPhone, I think being like the most, you know, kind of recent example
[01:22:07.400 --> 01:22:12.480]   that I can kind of think of where it literally did, you know, change everything.
[01:22:12.480 --> 01:22:16.080]   And I don't know, even if it doesn't, it's still fun.
[01:22:16.080 --> 01:22:17.480]   I mean, we're enthusiasts.
[01:22:17.480 --> 01:22:21.760]   Like I think it's fun even to be like brought into kind of the bubble of
[01:22:21.760 --> 01:22:24.480]   believing that this is going to be this revolutionary thing.
[01:22:24.480 --> 01:22:25.280]   And then it's not.
[01:22:25.280 --> 01:22:29.560]   And then you have a whole bunch of like, like all of us do a bunch of, I have a
[01:22:29.560 --> 01:22:34.480]   very like, just like you have a bunch of gadgets that are in boxes and all over
[01:22:34.480 --> 01:22:35.880]   my, my office.
[01:22:35.880 --> 01:22:36.200]   Yeah.
[01:22:36.200 --> 01:22:36.880]   Don't work.
[01:22:36.880 --> 01:22:37.000]   Yeah.
[01:22:37.000 --> 01:22:37.680]   And it's okay.
[01:22:37.680 --> 01:22:39.120]   It's a reminder.
[01:22:39.120 --> 01:22:42.960]   It's like, okay, I remember when we thought that was going to be the next big thing.
[01:22:42.960 --> 01:22:44.200]   Every six months.
[01:22:44.200 --> 01:22:48.240]   I have for the staff at to a Leo's garage sale.
[01:22:48.240 --> 01:22:50.080]   I don't put prices on it.
[01:22:50.080 --> 01:22:54.200]   It's all free, but I pile up the conference from table with exactly that
[01:22:54.200 --> 01:22:58.200]   stuff, Christina, stuff that I Alexis O'Haney and during COVID,
[01:22:58.200 --> 01:23:03.840]   founded a company that was going to be live music performances in your house,
[01:23:03.840 --> 01:23:08.480]   since you can't go out and he sold these wooden, they were made of wood,
[01:23:08.480 --> 01:23:11.120]   wooden speakers, wooden, they were wooden.
[01:23:11.120 --> 01:23:16.920]   They didn't, they connected them to this box that was connected to the internet.
[01:23:16.920 --> 01:23:21.360]   And then at four or 15 every evening, you would sit in front of these wooden
[01:23:21.360 --> 01:23:23.800]   speakers and there would be a live concert.
[01:23:23.800 --> 01:23:27.520]   And the speakers were supposedly designed for the live concert.
[01:23:27.520 --> 01:23:28.560]   John, just a little heads up.
[01:23:28.560 --> 01:23:31.320]   That's going to be on the table at Leo's next yard sale.
[01:23:33.480 --> 01:23:35.280]   So many times I've fallen for these.
[01:23:35.280 --> 01:23:38.520]   Actually, maybe I should put the Quest Pro on there to come to think of it.
[01:23:38.520 --> 01:23:39.200]   That didn't really.
[01:23:39.200 --> 01:23:42.160]   I was better than the Quest 2.
[01:23:42.160 --> 01:23:42.800]   Yeah.
[01:23:42.800 --> 01:23:45.640]   No, you know, that's why I spent that much money on it.
[01:23:45.640 --> 01:23:48.200]   I am not buying the Apple device, so I don't care.
[01:23:48.200 --> 01:23:51.080]   I don't care how I don't.
[01:23:51.080 --> 01:23:53.680]   I just can't bring myself to do that.
[01:23:53.680 --> 01:23:54.840]   I'm going to, I'm going to wait.
[01:23:54.840 --> 01:23:56.240]   Jason Howles raising his hand.
[01:23:56.240 --> 01:23:57.960]   Jason, you want those wooden speakers.
[01:23:57.960 --> 01:23:59.800]   He's a pretty good at those.
[01:23:59.800 --> 01:24:02.840]   Let's take a little bit of a break.
[01:24:02.840 --> 01:24:05.840]   I'm kind of interested.
[01:24:05.840 --> 01:24:07.880]   I'm really interested in what's going to happen tomorrow.
[01:24:07.880 --> 01:24:14.800]   It's always fun to watch a company with a trillion dollar, two trillion dollar
[01:24:14.800 --> 01:24:18.960]   valuation and hundreds of billions of dollars in cash.
[01:24:18.960 --> 01:24:21.640]   Tell us what's next.
[01:24:21.640 --> 01:24:27.080]   It's notoriously hard for tech companies to figure out what's next.
[01:24:27.080 --> 01:24:30.120]   They more often fail than not, especially the incumbents.
[01:24:30.120 --> 01:24:31.680]   But on the other hand.
[01:24:32.760 --> 01:24:35.960]   Apple, Apple, something special out there.
[01:24:35.960 --> 01:24:39.040]   Oh, sorry, Jason.
[01:24:39.040 --> 01:24:40.200]   You don't want the wooden speakers.
[01:24:40.200 --> 01:24:41.400]   You want the Quest Pro.
[01:24:41.400 --> 01:24:43.560]   OK, well, you can borrow the Quest Pro.
[01:24:43.560 --> 01:24:44.560]   You don't have to.
[01:24:44.560 --> 01:24:46.240]   You don't have to take it home.
[01:24:46.240 --> 01:24:53.520]   Our show today brought to you by Colide K-O-L-I-D-E, a device trust solution
[01:24:53.520 --> 01:24:58.360]   that ensures unsecured devices can't access your apps.
[01:24:58.360 --> 01:25:02.120]   This is actually a really very, very good point.
[01:25:02.120 --> 01:25:04.600]   If you're an Octa user, you feel good, right?
[01:25:04.600 --> 01:25:12.480]   You're secure because with Octa, only known users can get into your network
[01:25:12.480 --> 01:25:14.320]   and your cloud into your apps.
[01:25:14.320 --> 01:25:19.400]   Octa really assures that the authentication is happening, but that zero trust
[01:25:19.400 --> 01:25:24.600]   architecture has a bit of a hole because your identity provider
[01:25:24.600 --> 01:25:26.880]   lets known devices log into apps.
[01:25:26.880 --> 01:25:31.480]   But just because a device is known, a user is known, doesn't mean that
[01:25:31.480 --> 01:25:33.120]   the device is in a secure state.
[01:25:33.120 --> 01:25:39.600]   Device compliance is kind of the unsung problem in all of this.
[01:25:39.600 --> 01:25:43.120]   Many of the devices in your fleet probably shouldn't be trusted.
[01:25:43.120 --> 01:25:46.880]   They could be running out of date versions of the operating system.
[01:25:46.880 --> 01:25:49.520]   Unencrypted credentials could be in their download folder.
[01:25:49.520 --> 01:25:52.600]   They could have browsers that are out of date and on and on and on.
[01:25:52.600 --> 01:25:59.600]   Remember the big password manager company that got hacked because one of their
[01:25:59.600 --> 01:26:05.360]   DevOps guys was working at home on a laptop on a network that was also running
[01:26:05.360 --> 01:26:09.840]   an old version of Plex, an unpached version of Plex with a known flaw.
[01:26:09.840 --> 01:26:13.640]   The hacker used that flaw to get into the network and then get into the DevOps
[01:26:13.640 --> 01:26:20.920]   guys laptop and steal his credentials and then log into the password store, right?
[01:26:20.920 --> 01:26:24.120]   So with DevOps credentials.
[01:26:24.120 --> 01:26:27.760]   And of course they were authenticated when they were logging in.
[01:26:27.760 --> 01:26:29.320]   But here's the problem.
[01:26:29.800 --> 01:26:35.200]   Again, a device that isn't secure is the problem.
[01:26:35.200 --> 01:26:38.040]   So here's how it works with Collide.
[01:26:38.040 --> 01:26:40.240]   You put the Collide agent on all the endpoints, right?
[01:26:40.240 --> 01:26:44.720]   If a device isn't compliant and you set the rules, what compliant means, you get
[01:26:44.720 --> 01:26:49.120]   to say and it's or it's not running the agent, it just can't access the organization.
[01:26:49.120 --> 01:26:50.040]   SaaS apps.
[01:26:50.040 --> 01:26:55.680]   The device user cannot log into your company's cloud apps until they've
[01:26:55.680 --> 01:26:57.240]   fixed the problem on their end.
[01:26:57.240 --> 01:26:57.800]   It's that simple.
[01:26:57.800 --> 01:26:59.520]   And by the way, they fix the problem.
[01:26:59.520 --> 01:27:00.920]   That's the other thing I love about Collide.
[01:27:00.920 --> 01:27:02.760]   It offloads your IT team.
[01:27:02.760 --> 01:27:05.640]   It makes your users part of your security plan.
[01:27:05.640 --> 01:27:07.280]   Here's an example.
[01:27:07.280 --> 01:27:08.840]   Employee doesn't have an up-to-date browser, right?
[01:27:08.840 --> 01:27:11.160]   I just I forgot to get the last version of Chrome.
[01:27:11.160 --> 01:27:13.000]   But that's a problem.
[01:27:13.000 --> 01:27:19.600]   So Collide says, dude, fix your Chrome or you can't get in and user
[01:27:19.600 --> 01:27:20.200]   remediates.
[01:27:20.200 --> 01:27:21.160]   It doesn't say dude.
[01:27:21.160 --> 01:27:22.920]   It says, here's the problem.
[01:27:22.920 --> 01:27:23.960]   Here's what's going on.
[01:27:23.960 --> 01:27:25.120]   Here's why you need to fix it.
[01:27:25.120 --> 01:27:26.000]   Here's how you fix it.
[01:27:26.200 --> 01:27:27.240]   The user fixes it.
[01:27:27.240 --> 01:27:33.480]   Your IT department doesn't get loaded up and you get your fleet to 100% compliance.
[01:27:33.480 --> 01:27:34.600]   That's great.
[01:27:34.600 --> 01:27:38.480]   Without Collide, IT teams just don't have a good way to solve these compliance
[01:27:38.480 --> 01:27:42.280]   issues or stop and secure devices from logging in with Collide.
[01:27:42.280 --> 01:27:45.800]   You set you enforce compliance across your entire fleet.
[01:27:45.800 --> 01:27:49.040]   And this is another thing I love about Collide completely cross platform,
[01:27:49.040 --> 01:27:51.960]   Windows, Mac, Linux, absolutely.
[01:27:52.800 --> 01:27:58.360]   Collide's unique because it makes device compliance part of the authentication process.
[01:27:58.360 --> 01:28:02.080]   So a user logs in with Octa, Collide alerts them to compliance issues,
[01:28:02.080 --> 01:28:05.920]   prevents unsecured devices from logging in and gets the users to help you fix it.
[01:28:05.920 --> 01:28:10.480]   It's security you feel good about because Collide puts transparency and respect for
[01:28:10.480 --> 01:28:12.720]   users at the center of their product.
[01:28:12.720 --> 01:28:15.160]   Everybody's happy to sum it up.
[01:28:15.160 --> 01:28:20.200]   Collide's method means fewer support tickets, less frustration, most importantly,
[01:28:20.200 --> 01:28:22.240]   100% fleet compliance.
[01:28:22.440 --> 01:28:26.640]   K-O-L-I-D-E, Collide.com/twit.
[01:28:26.640 --> 01:28:27.520]   You can learn more.
[01:28:27.520 --> 01:28:28.440]   You can book a demo.
[01:28:28.440 --> 01:28:32.600]   K-O-L-I-D-E, Collide.com/twit.
[01:28:32.600 --> 01:28:35.040]   These guys have been a sponsor for a long time.
[01:28:35.040 --> 01:28:36.240]   I really love this product.
[01:28:36.240 --> 01:28:37.840]   Collide.com/twit.
[01:28:37.840 --> 01:28:39.640]   We thank them for their support.
[01:28:39.640 --> 01:28:42.880]   I wish that the password company had been using them.
[01:28:42.880 --> 01:28:48.880]   Speaking of security.
[01:28:48.880 --> 01:28:51.800]   Oh, this is bad for gigabyte.
[01:28:51.800 --> 01:29:00.360]   Millions of PC motherboards sold with a firmware back door, Andy Greenberg writing wire.com.
[01:29:00.360 --> 01:29:03.200]   These are gigabyte motherboards.
[01:29:03.200 --> 01:29:08.360]   They've been selling them for years of a company called Eclipsium, which is a firmware
[01:29:08.360 --> 01:29:10.040]   focused cybersecurity company.
[01:29:10.040 --> 01:29:15.400]   Said they've discovered a mechanism in the firmware of the motherboards sold by gigabyte.
[01:29:15.400 --> 01:29:18.720]   These are, by the way, great motherboards.
[01:29:18.720 --> 01:29:23.800]   They're really good. They're often used in gaming PCs and high-end computing.
[01:29:23.800 --> 01:29:28.160]   Whenever a computer with the affected gigabyte motherboard restarts,
[01:29:28.160 --> 01:29:32.720]   code within the motherboards firmware invisibly initiates an updater program
[01:29:32.720 --> 01:29:39.360]   that runs on the computer and then downloads and executes another piece of software from the internet.
[01:29:39.360 --> 01:29:43.280]   Now, I think this was a firmware update strategy, right?
[01:29:45.440 --> 01:29:49.200]   You know, good idea, but researchers found it's insecure
[01:29:49.200 --> 01:29:53.040]   because the updater program is triggered from the computer's firmware
[01:29:53.040 --> 01:29:54.320]   outside the operating system.
[01:29:54.320 --> 01:29:55.920]   Very difficult to discover, right?
[01:29:55.920 --> 01:29:57.600]   Your operating system isn't even running.
[01:29:57.600 --> 01:29:59.960]   Very difficult for you to remove it.
[01:29:59.960 --> 01:30:03.200]   And it is hijackable.
[01:30:03.200 --> 01:30:08.760]   John Locatees, who works at Eclipsium, leads strategy and research.
[01:30:08.760 --> 01:30:12.360]   If you have one of these gigabyte machines, you have to worry about the fact that it's
[01:30:12.360 --> 01:30:16.880]   basically grabbing something from the internet and running it without you being involved.
[01:30:16.880 --> 01:30:18.240]   And it hasn't done this securely.
[01:30:18.240 --> 01:30:22.080]   Of course, it would have to be a supply side hack.
[01:30:22.080 --> 01:30:25.560]   They'd have to get into the gigabyte database.
[01:30:25.560 --> 01:30:28.400]   271 models of gigabyte motherboards affected.
[01:30:28.400 --> 01:30:35.720]   So just something to be aware of.
[01:30:35.720 --> 01:30:41.080]   I guess there's no commentary on that, except maybe you want to go.
[01:30:41.080 --> 01:30:42.080]   It's frustrating.
[01:30:42.080 --> 01:30:44.280]   Well, it's frustrating that these things happen.
[01:30:44.280 --> 01:30:48.520]   And on the one hand, you understand that they want to have these auto updating
[01:30:48.520 --> 01:30:51.160]   abilities within the bio.
[01:30:51.160 --> 01:30:51.360]   Right.
[01:30:51.360 --> 01:30:52.200]   Then that's a good idea.
[01:30:52.200 --> 01:30:55.200]   On the other hand, this is this is sort of the trade off.
[01:30:55.200 --> 01:30:56.920]   And so I don't know.
[01:30:56.920 --> 01:31:03.440]   There has to be a better solution to be able to do this because, ironically, I think
[01:31:03.440 --> 01:31:08.440]   that the reason that they have this backdoor is because, and it's just because
[01:31:08.440 --> 01:31:11.440]   their server, I assume, is not secure enough.
[01:31:11.440 --> 01:31:14.120]   But the reason they have this backdoor is they're trying to prevent people from
[01:31:14.120 --> 01:31:21.400]   having unpatched motherboards and having other exploits that can potentially be even worse.
[01:31:21.400 --> 01:31:24.040]   So it's a challenge for sure.
[01:31:24.040 --> 01:31:29.560]   But the number of devices that are impacted, I have a gigabyte motherboard.
[01:31:29.560 --> 01:31:32.760]   It is not on the list, but many others are.
[01:31:32.760 --> 01:31:34.240]   And so I had to have this question.
[01:31:34.240 --> 01:31:36.520]   So I had to, I made a point to turn off.
[01:31:36.720 --> 01:31:37.680]   That is the thing you can do.
[01:31:37.680 --> 01:31:42.160]   You can turn off the auto update feature and I made a point to turn that off so that,
[01:31:42.160 --> 01:31:46.360]   you know, this, this is not going to hopefully affect me.
[01:31:46.360 --> 01:31:48.720]   And if you haven't already been modified.
[01:31:48.720 --> 01:31:49.400]   Yeah.
[01:31:49.400 --> 01:31:50.600]   Unless it's sorry.
[01:31:50.600 --> 01:31:51.040]   Right.
[01:31:51.040 --> 01:31:51.800]   Yeah.
[01:31:51.800 --> 01:31:52.760]   Then you're not alive.
[01:31:52.760 --> 01:31:53.360]   There's no evidence.
[01:31:53.360 --> 01:31:53.760]   Right.
[01:31:53.760 --> 01:31:53.880]   Yeah.
[01:31:53.880 --> 01:31:56.400]   But then there's no evidence that this has actually been modified in a while.
[01:31:56.400 --> 01:31:58.640]   Like this is, this is at least right now, this is theoretical.
[01:31:58.640 --> 01:32:03.480]   So and I haven't had the computer hasn't been on in quite some time.
[01:32:03.480 --> 01:32:06.440]   So I'm not concerned personally, but yeah, that's a good point.
[01:32:06.440 --> 01:32:08.560]   I don't know what it's going to take for
[01:32:08.560 --> 01:32:15.560]   for companies to be held accountable for this kind of thing.
[01:32:15.560 --> 01:32:21.760]   But the good news is gigabyte has released a firmware update that they say it's a beta,
[01:32:21.760 --> 01:32:23.480]   but they think it will fix this.
[01:32:23.480 --> 01:32:29.400]   So if you have a gigabyte motherboard, you might want to check to see if there's a firmware update.
[01:32:29.400 --> 01:32:31.360]   It is hard for come.
[01:32:31.360 --> 01:32:35.120]   This is this is like that Bloomberg story, which by the way, has never been
[01:32:35.760 --> 01:32:37.880]   confirmed, but exactly.
[01:32:37.880 --> 01:32:39.800]   That's still a big asterisk on that.
[01:32:39.800 --> 01:32:40.160]   Yeah.
[01:32:40.160 --> 01:32:40.640]   Yeah.
[01:32:40.640 --> 01:32:45.240]   You know, we still we have this situation where
[01:32:45.240 --> 01:32:52.920]   technology companies uniquely in our world get away with
[01:32:52.920 --> 01:33:02.400]   putting out crap products that are insecure, that that break and have no liability.
[01:33:03.200 --> 01:33:06.920]   I don't understand how how they have evaded
[01:33:06.920 --> 01:33:12.760]   what would be standard liability questions in any other business.
[01:33:12.760 --> 01:33:13.400]   Yeah.
[01:33:13.400 --> 01:33:20.040]   It raises an interesting point.
[01:33:20.040 --> 01:33:22.880]   And and I've we talked about this on twig.
[01:33:22.880 --> 01:33:26.720]   There's a new book just came out, which by the way, I highly recommend
[01:33:26.720 --> 01:33:32.560]   by Scott Shapiro, who is a both a computer scientist
[01:33:32.560 --> 01:33:33.720]   and a philosopher.
[01:33:33.720 --> 01:33:40.640]   But his new book is called Fancy Bear Goes Fishing, the dark history of the
[01:33:40.640 --> 01:33:43.360]   information age and five extraordinary hacks.
[01:33:43.360 --> 01:33:46.800]   It starts with Robert Tapen Morris's very famous internet worm.
[01:33:46.800 --> 01:33:52.320]   The first time we saw a worm take the internet down way back and I think it was 1983.
[01:33:52.320 --> 01:33:56.360]   Highly recommend the book.
[01:33:56.360 --> 01:34:02.320]   They're very interesting description of the the mirai worm and the story behind it.
[01:34:02.320 --> 01:34:08.240]   And the kid who created it, he was a Rutgers student who was pissed off because only
[01:34:08.240 --> 01:34:12.320]   upper class computer science majors could get into the computer science classes.
[01:34:12.320 --> 01:34:17.040]   So he d tossed the entire system and brought it to its knees.
[01:34:17.040 --> 01:34:23.520]   He was kind of a wild child and eventually came up with the mirai botnet, which was a
[01:34:23.520 --> 01:34:30.680]   the very first, I think, DDoS as a service system where script kitties didn't know
[01:34:30.680 --> 01:34:35.480]   anything about DDoS and could rent the hardware to DDoS their favorite target.
[01:34:35.480 --> 01:34:38.000]   He eventually got caught the FBI caught him.
[01:34:38.000 --> 01:34:42.720]   They did they decided not to send him to jail, but instead to enlist his help as
[01:34:42.720 --> 01:34:45.400]   often happens in fighting other hackers.
[01:34:45.400 --> 01:34:50.200]   And he's now he says it reformed and is very grateful to the FBI agent who caught
[01:34:50.200 --> 01:34:52.920]   him for teaching him the right way.
[01:34:52.920 --> 01:34:57.600]   It's a it's a highly great story, but that's not why I'm bringing it up because
[01:34:58.280 --> 01:35:03.120]   Scott and I may not this may be beyond my pay grade, maybe yours to maybe, I don't
[01:35:03.120 --> 01:35:09.280]   know, maybe somebody will know, but Scott says because these are essentially our
[01:35:09.280 --> 01:35:14.520]   computers are essentially Turing machines, which means they're designed in a way
[01:35:14.520 --> 01:35:16.800]   that they can solve any problem, arbitrary problem.
[01:35:16.800 --> 01:35:19.280]   They have the general purpose hardware to do that.
[01:35:19.280 --> 01:35:21.960]   They cannot be secured.
[01:35:24.200 --> 01:35:29.640]   Jennifer Wilett's article in our technical is cyber security and unsolvable problems.
[01:35:29.640 --> 01:35:32.240]   Scott Sapero Shapiro says, yes.
[01:35:32.240 --> 01:35:37.000]   Any thoughts?
[01:35:37.000 --> 01:35:38.320]   Christina?
[01:35:38.320 --> 01:35:43.160]   I mean, yeah, I think I think that I don't think that is one of those things.
[01:35:43.160 --> 01:35:47.240]   It is one of those problems that I don't think that you can ever permanently solve,
[01:35:47.240 --> 01:35:47.640]   right?
[01:35:47.640 --> 01:35:50.240]   Like, I think that it's one of those that is always a moving target.
[01:35:50.560 --> 01:35:57.480]   And so possibly when we reach the age of quantum, yes, you could maybe have, you
[01:35:57.480 --> 01:36:02.920]   know, these systems that are they're not going to be hackable or, or, you know,
[01:36:02.920 --> 01:36:03.480]   whatnot.
[01:36:03.480 --> 01:36:05.480]   I don't really believe that.
[01:36:05.480 --> 01:36:09.320]   Yeah, I think that it is kind of a it's always kind of a moving target.
[01:36:09.320 --> 01:36:15.160]   And this is why good security funds, you know, firms rather can make so much money
[01:36:15.160 --> 01:36:21.800]   because this is an ongoing thing that the targets and the things that are easy
[01:36:21.800 --> 01:36:25.120]   exploits today are not necessarily going to be the same exploits tomorrow.
[01:36:25.120 --> 01:36:27.040]   And so you can't just rush on your laurels.
[01:36:27.040 --> 01:36:31.960]   So yeah, I mean, I think, yeah, I don't, I don't know if this ever goes away.
[01:36:31.960 --> 01:36:36.720]   I think this is why we have to be vigilant, but we have to kind of expect, or not
[01:36:36.720 --> 01:36:41.320]   expect what we have to have the awareness that there is the potential with any of
[01:36:41.320 --> 01:36:43.200]   the things that we use, that it could be compromised.
[01:36:43.200 --> 01:36:47.400]   And that's why it's important to run updates and to, you know, stay aware of
[01:36:47.400 --> 01:36:52.600]   things and to, you know, pay attention and to audit code and have third parties
[01:36:52.600 --> 01:36:55.400]   auditing, you know, your code as much as possible.
[01:36:55.400 --> 01:36:59.120]   Wouldn't it make sense to try to prevent the fall from this stuff?
[01:36:59.120 --> 01:36:59.520]   Yeah.
[01:36:59.520 --> 01:37:02.120]   I think part of the problem is social engineering.
[01:37:02.120 --> 01:37:05.840]   I mean, even if we could make perfect hardware and perfect software, which we
[01:37:05.840 --> 01:37:08.520]   can't know, we can't make perfect people.
[01:37:08.520 --> 01:37:10.760]   100% of the sucker born every minute.
[01:37:10.760 --> 01:37:11.480]   Yeah.
[01:37:11.480 --> 01:37:15.200]   And we're going to talk about later when we get into the situation, what happened
[01:37:15.200 --> 01:37:20.840]   to me, even very tech savvy people and be at risk of social engineering, because
[01:37:20.840 --> 01:37:23.600]   the social engineers are very, very smart.
[01:37:23.600 --> 01:37:24.120]   Yeah.
[01:37:24.120 --> 01:37:25.320]   And absolutely.
[01:37:25.320 --> 01:37:28.200]   And the thing is, is that the thing with social engineering is that, A, you're
[01:37:28.200 --> 01:37:31.560]   right, very tech savvy people can still be taken in that happens all the time.
[01:37:31.560 --> 01:37:36.760]   But B, many times the people who are socially engineered are not us.
[01:37:36.760 --> 01:37:40.520]   It is customer service representatives for companies that control access to our
[01:37:40.520 --> 01:37:44.640]   devices. And so, you know, that that's how most people have, you know, get
[01:37:44.640 --> 01:37:45.480]   simp shipped.
[01:37:45.480 --> 01:37:49.160]   It has something to do with whatever policies or security they have in place
[01:37:49.160 --> 01:37:49.960]   on their devices.
[01:37:49.960 --> 01:37:54.160]   It's just getting the customer service person from, you know, one of those
[01:37:54.160 --> 01:37:57.920]   companies to, to say enough things to, to getting them to pass things over.
[01:37:57.920 --> 01:38:00.960]   Sometimes without even having information from the user themselves.
[01:38:00.960 --> 01:38:02.000]   So you're right.
[01:38:02.000 --> 01:38:05.640]   It, there is the people problem that I don't think we can ever solve, even if
[01:38:05.640 --> 01:38:10.000]   we were able to build, you know, fully secure hardware devices.
[01:38:10.080 --> 01:38:13.640]   And to expand what you're saying is computer security is one area where you
[01:38:13.640 --> 01:38:15.360]   can be completely innocent.
[01:38:15.360 --> 01:38:19.120]   So for example, if you're only crying where to have shopped at target, right
[01:38:19.120 --> 01:38:23.040]   before their systems are hacked, your information was, with a bridge, you
[01:38:23.040 --> 01:38:24.280]   didn't say anything wrong.
[01:38:24.280 --> 01:38:27.720]   There was nothing you could have possibly done to protect yourself except
[01:38:27.720 --> 01:38:28.640]   live under a rock.
[01:38:28.640 --> 01:38:29.200]   Right.
[01:38:29.200 --> 01:38:30.560]   Yet you were vulnerable.
[01:38:30.560 --> 01:38:32.320]   And that's not true as most things.
[01:38:32.320 --> 01:38:32.880]   I mean, right.
[01:38:32.880 --> 01:38:36.520]   Like an airplane falling out of the sky and hitting you, at least if you die in a
[01:38:36.520 --> 01:38:39.360]   plane crash, probably you bought the ticket on the plane.
[01:38:39.360 --> 01:38:41.120]   Maybe I'm not saying you did anything wrong.
[01:38:41.120 --> 01:38:43.280]   But you were, you were at least involved.
[01:38:43.280 --> 01:38:46.840]   You, you, you had some agency in the decision to get on the plane to begin with.
[01:38:46.840 --> 01:38:48.560]   And you understood that there was a risk.
[01:38:48.560 --> 01:38:49.760]   No, you're exactly right.
[01:38:49.760 --> 01:38:52.640]   I mean, I, I, with a target hack, it was so funny.
[01:38:52.640 --> 01:38:53.960]   It was like targeted to pull away.
[01:38:53.960 --> 01:38:56.520]   They were like three of them in a row that I was victim of.
[01:38:56.520 --> 01:39:01.400]   And I had to change my, you know, credit card like three times and like a 14
[01:39:01.400 --> 01:39:04.720]   month period of time, which is very frustrating to then have to update all
[01:39:04.720 --> 01:39:05.840]   of your recurring charges.
[01:39:05.840 --> 01:39:06.640]   But you're right.
[01:39:06.640 --> 01:39:11.640]   You know, just by, and you could say, okay, we'll use cash in most circumstances.
[01:39:11.640 --> 01:39:14.680]   That is not a reasonable expectation for most people, right?
[01:39:14.680 --> 01:39:16.880]   That that is not an original thing that goes on.
[01:39:16.880 --> 01:39:18.640]   It is a possible many places.
[01:39:18.640 --> 01:39:20.440]   I can drink it an airplane with cash.
[01:39:20.440 --> 01:39:21.360]   Right.
[01:39:21.360 --> 01:39:22.080]   Absolutely.
[01:39:22.080 --> 01:39:22.840]   Yeah.
[01:39:22.840 --> 01:39:23.760]   Yeah.
[01:39:23.760 --> 01:39:32.680]   And the other difference is that target suffers essentially no liability, no
[01:39:32.680 --> 01:39:33.760]   accountability.
[01:39:34.000 --> 01:39:39.600]   Whereas if there's a plane crash, right, there's a lot of money changes hands.
[01:39:39.600 --> 01:39:40.480]   Yeah.
[01:39:40.480 --> 01:39:44.280]   And there's a lot of damage to the air to the carriers.
[01:39:44.280 --> 01:39:52.040]   We've, we, again, we've given people who do software, basically a complete
[01:39:52.040 --> 01:39:55.200]   pass on liability.
[01:39:55.200 --> 01:39:57.040]   And that doesn't seem right.
[01:39:57.040 --> 01:39:58.120]   You're right.
[01:39:58.120 --> 01:39:59.600]   And it's complicated.
[01:39:59.600 --> 01:40:04.760]   And I understand all these things about how software is, is different.
[01:40:04.760 --> 01:40:10.480]   But, and it's the supply chain of software is insane the whole.
[01:40:10.480 --> 01:40:10.960]   Mm hmm.
[01:40:10.960 --> 01:40:20.120]   But, but the somehow the technology industry has persuaded, it's pretty
[01:40:20.120 --> 01:40:21.040]   amazing that they've done it.
[01:40:21.040 --> 01:40:28.280]   They persuaded legislators that doesn't matter how badly they behave.
[01:40:29.280 --> 01:40:33.080]   They have no liability for what they do wrong.
[01:40:33.080 --> 01:40:35.600]   It's, it's something's got to change there.
[01:40:35.600 --> 01:40:40.360]   Actually, you, boy, you guys, you don't have to read the book now because you
[01:40:40.360 --> 01:40:42.200]   came to the exact conclusion.
[01:40:42.200 --> 01:40:45.760]   Sephiara said, I actually wrote this book three times the first time I just
[01:40:45.760 --> 01:40:47.760]   wrote about the code and the hackers.
[01:40:47.760 --> 01:40:51.440]   And then I realized, well, there's a human element to all of this.
[01:40:51.440 --> 01:40:52.360]   So he rewrote it.
[01:40:52.360 --> 01:40:54.200]   And then he said, I realized there's a third element.
[01:40:54.200 --> 01:40:57.160]   There's a philosophical element to this.
[01:40:57.680 --> 01:41:03.240]   Um, this is an interview from ours Technica, the interviewer, uh, talks about the
[01:41:03.240 --> 01:41:07.120]   science, and this goes back to our AI conversation, the scientific community
[01:41:07.120 --> 01:41:09.400]   and various disciplines has struggled with this in the past.
[01:41:09.400 --> 01:41:11.360]   There's an attitude of, we're just doing the research.
[01:41:11.360 --> 01:41:12.760]   It's just a tool.
[01:41:12.760 --> 01:41:14.280]   It's morally neutral.
[01:41:14.280 --> 01:41:23.360]   Uh, Scott says that in fact, you cannot talk about hacking in a neutral way.
[01:41:23.360 --> 01:41:24.920]   He says, I'm a philosopher.
[01:41:24.920 --> 01:41:26.480]   So my day job is teaching that.
[01:41:27.080 --> 01:41:31.280]   But it's a problem throughout all of STEM, the idea that tools are morally neutral.
[01:41:31.280 --> 01:41:36.320]   You're just making them and it's up to the end user to use it in the right way.
[01:41:36.320 --> 01:41:39.640]   He says, that's a reasonable attitude to have if you live in a culture that's
[01:41:39.640 --> 01:41:44.560]   doing the work of explaining why these tools ought to be used in one way, rather
[01:41:44.560 --> 01:41:45.440]   than another.
[01:41:45.440 --> 01:41:49.120]   But when we have a culture that doesn't do that, and this kind of is also what
[01:41:49.120 --> 01:41:54.280]   you teach, uh, Dan, then it becomes a very morally problematic activity.
[01:41:54.280 --> 01:41:56.800]   We're now seeing a lot of hand-ringing about AI.
[01:41:57.760 --> 01:42:00.880]   We always see hand-ringing about every single new technology.
[01:42:00.880 --> 01:42:04.960]   There's the techno-utopians and there's the techno-distopians.
[01:42:04.960 --> 01:42:09.040]   But a couple of years later, usually the cooler heads, uh, prevail.
[01:42:09.040 --> 01:42:14.840]   So he said, it's important when you're talking about hacking to, to, to include
[01:42:14.840 --> 01:42:18.880]   the fact that it's morally reprehensible, that it's wrong and it's bad.
[01:42:18.880 --> 01:42:22.440]   Uh, he says, you're not going to get rid of it because as long as, because
[01:42:22.440 --> 01:42:26.760]   humans, as long as you got humans, your cyber security,
[01:42:26.760 --> 01:42:31.880]   is not primarily a technological problem that requires primarily an engineering
[01:42:31.880 --> 01:42:32.360]   solution.
[01:42:32.360 --> 01:42:37.680]   It's a human problem that requires an understanding of human behavior, hacking is
[01:42:37.680 --> 01:42:38.400]   about humans.
[01:42:38.400 --> 01:42:41.880]   And he's calling for the death of solutionism.
[01:42:41.880 --> 01:42:44.760]   Cause we do kind of come from that point of view that, oh, we can fix this.
[01:42:44.760 --> 01:42:46.240]   We can solve this.
[01:42:46.240 --> 01:42:48.800]   So we can't go ahead.
[01:42:48.800 --> 01:42:52.400]   To dance point, I mean, I agree that legislators need to wake up and be more
[01:42:52.400 --> 01:42:53.520]   aware of what's going on.
[01:42:53.880 --> 01:42:59.560]   But there's also Dan, as you know, well-intentioned legislation that's bad.
[01:42:59.560 --> 01:43:03.720]   Communications, Decency Act was one example going back in 1998 or
[01:43:03.720 --> 01:43:09.560]   the Arkansas law that is going to essentially ban minors using social media
[01:43:09.560 --> 01:43:10.840]   without parental consent.
[01:43:10.840 --> 01:43:16.600]   Also a Utah law, there's so much tech, there was so much dystopian, or I mean,
[01:43:16.600 --> 01:43:20.240]   dystopian, but overregulation because the regulators don't understand what's
[01:43:20.240 --> 01:43:21.400]   effective and what isn't.
[01:43:21.400 --> 01:43:22.200]   Right.
[01:43:22.360 --> 01:43:24.520]   And so that's what Bob, that's what worries me.
[01:43:24.520 --> 01:43:28.840]   And whenever Washington or so, or now increasingly state legislators are getting
[01:43:28.840 --> 01:43:31.880]   their myths on technology, is they really don't understand what they're
[01:43:31.880 --> 01:43:34.080]   regulating and what the real dangers are.
[01:43:34.080 --> 01:43:39.120]   And one of the reasons why I started safe kids.com and later connect safely was,
[01:43:39.120 --> 01:43:43.960]   and again, we're very imperfect ourselves, but at least to try to create sort of a
[01:43:43.960 --> 01:43:49.280]   conversation around the relationship between industry, government and consumers
[01:43:49.280 --> 01:43:53.200]   as to how we can use this technology in a more safe manner.
[01:43:53.200 --> 01:43:58.440]   We will never claim that we'll ever reach absolute safety, but make it safer.
[01:43:58.440 --> 01:44:03.880]   And it's it requires a lot of, you know, I hate to say it takes the village
[01:44:03.880 --> 01:44:06.560]   because that's become trite, but it really does take a village.
[01:44:06.560 --> 01:44:10.240]   There's no question that that the
[01:44:10.240 --> 01:44:18.680]   problems that we can create by misguided regulation are serious
[01:44:18.680 --> 01:44:23.240]   and that we have to be careful as we do this stuff.
[01:44:23.240 --> 01:44:34.040]   But my only point in this context was that we have given a single collection of
[01:44:34.040 --> 01:44:47.600]   people in our economy an exemption from liability for egregious misconduct.
[01:44:48.440 --> 01:44:50.560]   And that doesn't seem right to me.
[01:44:50.560 --> 01:44:53.320]   I think we could we could start there.
[01:44:53.320 --> 01:44:56.720]   And I think you're right.
[01:44:56.720 --> 01:45:01.760]   I would point out and I completely agree with you is that the government itself is
[01:45:01.760 --> 01:45:03.880]   often the one who leaks information.
[01:45:03.880 --> 01:45:07.800]   There have been numerous government hacks of databases and whatnot.
[01:45:07.800 --> 01:45:11.400]   And they also should not be exempt from those things.
[01:45:11.400 --> 01:45:14.040]   It is an interesting thing.
[01:45:14.040 --> 01:45:18.040]   You know, I think about Equifax and the fact that most people who were
[01:45:18.040 --> 01:45:22.640]   impacted by that again, I was we weren't even compensated like $20.
[01:45:22.640 --> 01:45:23.720]   We didn't get anything.
[01:45:23.720 --> 01:45:26.840]   You know, they were invited to spend more money with Equifax.
[01:45:26.840 --> 01:45:28.200]   Exactly. Exactly.
[01:45:28.200 --> 01:45:28.720]   Exactly.
[01:45:28.720 --> 01:45:30.320]   And I was like, why would I give you any money?
[01:45:30.320 --> 01:45:32.680]   You know, why would I trust anything you've done?
[01:45:32.680 --> 01:45:36.760]   And in that case, and I think what's interesting too, if you didn't want to,
[01:45:36.760 --> 01:45:40.760]   you know, to to Larry's point, it's a difficult problem to solve.
[01:45:40.760 --> 01:45:44.880]   But I feel like there are some instances where it is flat out from any other
[01:45:44.880 --> 01:45:47.080]   circumstance, it would be pure negligence.
[01:45:47.280 --> 01:45:48.840]   Equifax being a great example.
[01:45:48.840 --> 01:45:52.920]   There are some supply chain types of attacks and other attacks where it can
[01:45:52.920 --> 01:45:56.920]   truly say, you know, this, this was not something that we could conceive of
[01:45:56.920 --> 01:46:00.640]   happening because we didn't have control over what ultimately impacted our systems.
[01:46:00.640 --> 01:46:04.400]   But there are other instances where, you know, you're running outdated systems
[01:46:04.400 --> 01:46:07.600]   and you're not holding things up the way that you should.
[01:46:07.600 --> 01:46:12.160]   You're not doing the basic due diligence in those companies or, or, you know,
[01:46:12.160 --> 01:46:15.640]   still get government contracts or the government themselves are still able to
[01:46:16.040 --> 01:46:17.640]   force us to use their systems.
[01:46:17.640 --> 01:46:19.680]   Um, and, and we have new recourse.
[01:46:19.680 --> 01:46:20.520]   And it's very interesting.
[01:46:20.520 --> 01:46:21.080]   I don't like that.
[01:46:21.080 --> 01:46:23.320]   There ought to be a corporate extinction.
[01:46:23.320 --> 01:46:29.440]   I mean, corporate extinction level events are common for, for this kind of stuff.
[01:46:29.440 --> 01:46:37.480]   And the fact that the credit agencies do what they do and are, are notoriously
[01:46:37.480 --> 01:46:43.360]   incompetent, willfully in competence, because they're, they don't see the point
[01:46:43.360 --> 01:46:44.960]   in spending the money to do it right.
[01:46:45.040 --> 01:46:47.920]   And they are, they have a point.
[01:46:47.920 --> 01:46:50.480]   No one does anything about it when they do it wrong.
[01:46:50.480 --> 01:46:55.960]   I guess there, if you're looking at their shareholder value,
[01:46:55.960 --> 01:46:58.640]   um, hey, screw the public.
[01:46:58.640 --> 01:47:01.200]   It's worth, you know, we make more money doing it that way.
[01:47:01.200 --> 01:47:02.920]   And nobody holds us accountable.
[01:47:02.920 --> 01:47:05.080]   It's, it's what, what a grift.
[01:47:05.080 --> 01:47:05.760]   It's amazing.
[01:47:05.760 --> 01:47:08.280]   The thing about it, you're, you're absolutely right.
[01:47:08.280 --> 01:47:12.400]   I don't think I know anybody who doesn't have a credit reporting, um, story to
[01:47:12.400 --> 01:47:16.520]   tell about an inaccurate report, maybe not getting a loan because of some, because
[01:47:16.520 --> 01:47:22.240]   of some inaccurate, but in aggregate, it serves lenders well, because, you know,
[01:47:22.240 --> 01:47:23.720]   they're going to make a lot of mistakes.
[01:47:23.720 --> 01:47:26.640]   But over the course of the millions of loans they give, they're still going to
[01:47:26.640 --> 01:47:27.880]   make money on those loans.
[01:47:27.880 --> 01:47:32.360]   And so, you know, again, it's not about you and me and the rest of the public.
[01:47:32.360 --> 01:47:33.880]   It's about the industry.
[01:47:33.880 --> 01:47:37.000]   And, and that's who these credit bureaus are serving.
[01:47:37.000 --> 01:47:38.240]   They're not serving the public at all.
[01:47:38.240 --> 01:47:41.720]   No, in fact, if you want a credit card, you have to give them permission.
[01:47:41.800 --> 01:47:42.280]   You have to.
[01:47:42.280 --> 01:47:42.920]   Yeah.
[01:47:42.920 --> 01:47:43.480]   Yeah.
[01:47:43.480 --> 01:47:46.440]   If you want to rent an apartment these days, you have to give them permission
[01:47:46.440 --> 01:47:47.560]   over and over.
[01:47:47.560 --> 01:47:51.080]   All of this stuff is, uh, you don't have a choice.
[01:47:51.080 --> 01:47:54.440]   This is the, by the way, in the Equifax with another example, we're
[01:47:54.440 --> 01:47:55.360]   innocent people.
[01:47:55.360 --> 01:47:55.840]   Yes.
[01:47:55.840 --> 01:47:59.080]   I never, I don't know anybody said, gee, I'm going to become an Equifax cuss.
[01:47:59.080 --> 01:48:01.880]   No, again, you don't have any, you know, it's just the matter.
[01:48:01.880 --> 01:48:02.360]   I'm different.
[01:48:02.360 --> 01:48:03.560]   Just by existing society.
[01:48:03.560 --> 01:48:04.000]   Yeah.
[01:48:04.000 --> 01:48:08.200]   I have to get a lot of alerts on all, uh, actually, there's more than the three.
[01:48:08.200 --> 01:48:11.280]   I think it's on, on five different credit reporting.
[01:48:11.280 --> 01:48:12.680]   I just, you get the free loan, right?
[01:48:12.680 --> 01:48:13.360]   Yeah.
[01:48:13.360 --> 01:48:14.080]   I'm, yeah.
[01:48:14.080 --> 01:48:17.880]   Now it's free used to be, they charge you to turn it off.
[01:48:17.880 --> 01:48:18.160]   Yeah.
[01:48:18.160 --> 01:48:19.280]   Exactly.
[01:48:19.280 --> 01:48:24.000]   The far thank goodness federal legislation has stopped that scam.
[01:48:24.000 --> 01:48:32.240]   It meant to anyone who's listening to this, please go freeze your credit right now.
[01:48:32.240 --> 01:48:32.680]   Yep.
[01:48:32.680 --> 01:48:34.920]   As soon as we're, no, wait, wait until the show is over.
[01:48:34.920 --> 01:48:41.240]   Then you go and freeze your credit so that, uh, at least you have, uh,
[01:48:41.240 --> 01:48:48.680]   partial protection and it's, it's easier to unfreeze when you need to have someone
[01:48:48.680 --> 01:48:51.800]   look at your credit report on a specific thing.
[01:48:51.800 --> 01:48:53.800]   It's easier than you might think.
[01:48:53.800 --> 01:48:54.480]   Yeah.
[01:48:54.480 --> 01:48:55.240]   This is the issue.
[01:48:55.240 --> 01:48:55.760]   This is the issue.
[01:48:55.760 --> 01:48:56.720]   This has good guides on this.
[01:48:56.720 --> 01:49:02.560]   It was easy for me to do, uh, because I'm old and I'm not buying a new house and I'm not, you know,
[01:49:02.560 --> 01:49:04.600]   it taken out loans and I'm not buying new credit cards.
[01:49:04.600 --> 01:49:08.600]   If you're young and you're, you know, you're getting started in the world, uh,
[01:49:08.600 --> 01:49:14.000]   freezing your credit, uh, is an inconvenience, but it's, but I think it's a worthy inconvenience.
[01:49:14.000 --> 01:49:14.440]   And, uh,
[01:49:14.440 --> 01:49:19.160]   So my daughter bought a car and the car salesmen tried to convince her that she should take
[01:49:19.160 --> 01:49:21.480]   out a loan in order to build her credit.
[01:49:21.480 --> 01:49:23.480]   Now, first of all, she already had good credit.
[01:49:23.480 --> 01:49:23.920]   Right.
[01:49:23.920 --> 01:49:25.600]   But they, they actually made this case.
[01:49:25.600 --> 01:49:28.840]   And apparently there's some truth to that, which surprised me when I actually did a little
[01:49:28.840 --> 01:49:32.720]   research, it seems absurd that your credit score would go up because you're in debt.
[01:49:32.720 --> 01:49:33.600]   It does.
[01:49:33.600 --> 01:49:34.080]   Yeah.
[01:49:34.080 --> 01:49:34.400]   It does.
[01:49:34.400 --> 01:49:35.200]   It does.
[01:49:35.200 --> 01:49:35.800]   Yeah.
[01:49:35.840 --> 01:49:40.080]   I've had the same thing where when I try to look at my credit for various things,
[01:49:40.080 --> 01:49:43.880]   like the thing I'll be dinged with is that I don't have enough, you know, um,
[01:49:43.880 --> 01:49:44.440]   outstanding.
[01:49:44.440 --> 01:49:48.200]   I don't have any outstanding loans or I don't have enough, you know, kind of open lines of credit.
[01:49:48.200 --> 01:49:49.600]   You know, you pay it off and whatnot.
[01:49:49.600 --> 01:49:52.560]   And it does seem counterintuitive, but yet that's.
[01:49:52.560 --> 01:49:54.880]   You're paying your bills on time.
[01:49:54.880 --> 01:49:55.480]   Exactly.
[01:49:55.480 --> 01:50:00.640]   Building your bills on time and they're like, Oh no, you know, this isn't that we,
[01:50:00.640 --> 01:50:01.920]   well, we don't know if you're trustworthy.
[01:50:01.920 --> 01:50:02.720]   Okay.
[01:50:02.720 --> 01:50:05.800]   You know, like Leo, I'm old and I just paid
[01:50:05.800 --> 01:50:08.680]   my bills on time and I don't worry about credit anymore.
[01:50:08.680 --> 01:50:13.200]   I've already, I've already withdrawn from the economic life of this country.
[01:50:13.200 --> 01:50:17.920]   Uh, but if you want to freeze your credit, uh, yes, consumer reports has a number of articles.
[01:50:17.920 --> 01:50:23.280]   The FTC actually has a whole page devoted, do not get fooled by credit locks.
[01:50:23.280 --> 01:50:29.520]   Do the credit freeze that really will protect you and the FTC, a consumer
[01:50:29.520 --> 01:50:32.000]   that FTC.gov has the details on how to do that.
[01:50:32.000 --> 01:50:33.080]   It's actually quite easy.
[01:50:33.680 --> 01:50:36.840]   And nowadays they can't charge you to unfreeze it.
[01:50:36.840 --> 01:50:41.400]   So, um, you know, you can freeze and unfreeze at will.
[01:50:41.400 --> 01:50:42.880]   They don't make it easy.
[01:50:42.880 --> 01:50:48.520]   They're doing every, they have crappy websites with complicated logins and all
[01:50:48.520 --> 01:50:49.120]   sorts of stuff.
[01:50:49.120 --> 01:50:53.360]   But once you get it down and you can log in, turn it on and off fairly quickly.
[01:50:53.360 --> 01:50:55.760]   It can't be any harder than I'm subscribing from a page.
[01:50:55.760 --> 01:50:57.240]   No, no, you're right.
[01:50:57.240 --> 01:50:58.200]   That's the heart of your right.
[01:50:58.200 --> 01:50:58.960]   It's close.
[01:50:58.960 --> 01:51:02.440]   But then by the way, why is it that when you get an email that you want to
[01:51:02.440 --> 01:51:07.240]   subscribe, that we're unsubscribe if in the tiniest possible print imaginable
[01:51:07.240 --> 01:51:09.200]   at the bottom of the page.
[01:51:09.200 --> 01:51:09.800]   Oh, yeah.
[01:51:09.800 --> 01:51:11.520]   Well, cause they don't want you to unsubscribe.
[01:51:11.520 --> 01:51:12.120]   What did you think?
[01:51:12.120 --> 01:51:14.160]   It is really hard to find that link.
[01:51:14.160 --> 01:51:18.640]   Web design to steer you to bad decisions is it's dark.
[01:51:18.640 --> 01:51:25.760]   I have a whole part of my media literacy teaching is in there.
[01:51:25.760 --> 01:51:30.240]   There's one section where I talk about how, uh, and there's lots of good
[01:51:30.240 --> 01:51:36.480]   articles about this web designers trying to trick you into making decisions,
[01:51:36.480 --> 01:51:38.800]   favoring the company.
[01:51:38.800 --> 01:51:48.040]   Uh, yeah, it's, you know, the, the language like, you know, yes, I, uh, yes.
[01:51:48.040 --> 01:51:52.600]   I want to be a miserable human being for the rest of my life.
[01:51:52.600 --> 01:51:59.000]   If I'm not getting your emails every morning, you know, that, that's a, that's
[01:51:59.000 --> 01:52:04.880]   the choices they give you and that's in, uh, you know, one kind of bold thing.
[01:52:04.880 --> 01:52:09.760]   And then the, or no, that's the, I forget how the, the, the one that they don't
[01:52:09.760 --> 01:52:10.520]   want you to quit.
[01:52:10.520 --> 01:52:10.880]   It's hidden.
[01:52:10.880 --> 01:52:12.600]   It's usually in gray on white.
[01:52:12.600 --> 01:52:13.400]   Yeah.
[01:52:13.400 --> 01:52:15.200]   Like gray on white dark patterns.
[01:52:15.200 --> 01:52:16.240]   Dark patterns.
[01:52:16.240 --> 01:52:16.600]   Dark patterns.
[01:52:16.600 --> 01:52:17.080]   It's called.
[01:52:17.080 --> 01:52:17.360]   Yep.
[01:52:17.360 --> 01:52:24.240]   There's a great website called deceptive dot design, deceptive dot design, which
[01:52:24.240 --> 01:52:28.600]   talks about dark patterns and has a whole hall of shame.
[01:52:28.600 --> 01:52:34.320]   So if you're looking, if you're looking for dark patterns, there are literally
[01:52:34.320 --> 01:52:42.680]   448 examples in here of misleading buttons that don't work, all sorts of stuff.
[01:52:42.680 --> 01:52:47.480]   Microsoft does add a little bit too, not to slam your company, but, uh,
[01:52:47.480 --> 01:52:49.080]   also a I work for GitHub.
[01:52:49.080 --> 01:52:50.600]   I mean, you are on the same.
[01:52:50.600 --> 01:52:51.000]   Yeah.
[01:52:51.000 --> 01:52:51.360]   Okay.
[01:52:51.360 --> 01:52:54.160]   Be I'm not here to defend anybody.
[01:52:54.160 --> 01:52:54.720]   And you're right.
[01:52:54.720 --> 01:52:57.440]   They, the systems, I call that out too.
[01:52:57.440 --> 01:52:57.840]   It's not.
[01:52:57.840 --> 01:52:59.000]   Oh, it's notorious.
[01:52:59.000 --> 01:53:02.760]   We talk about it as weekly when Microsoft policies, a cookie,
[01:53:02.760 --> 01:53:08.000]   a master's degree to figure out in cookie management, to figure out how to set your
[01:53:08.000 --> 01:53:08.440]   cookies.
[01:53:08.440 --> 01:53:10.080]   Why can't they make it really easy?
[01:53:10.080 --> 01:53:12.320]   Some sites do say only necessary cookies.
[01:53:12.320 --> 01:53:14.760]   And then of course they don't define what really are necessary.
[01:53:14.760 --> 01:53:15.360]   Right.
[01:53:15.360 --> 01:53:19.720]   Well, you have to set a cookie to say, don't show this.
[01:53:19.720 --> 01:53:21.280]   Don't exactly.
[01:53:21.280 --> 01:53:26.200]   I was going to say, I have in a, I was going to say in you block a pro,
[01:53:26.200 --> 01:53:29.400]   I have some sort of like cookie banner extension.
[01:53:29.400 --> 01:53:30.400]   Yeah.
[01:53:30.400 --> 01:53:30.760]   Yep.
[01:53:30.760 --> 01:53:31.320]   Exactly.
[01:53:31.320 --> 01:53:32.120]   I have that set.
[01:53:32.120 --> 01:53:32.880]   Yeah.
[01:53:32.880 --> 01:53:37.200]   There's a Firefox thing they do now that basically it's there.
[01:53:37.200 --> 01:53:39.160]   You watch it happen in real time.
[01:53:39.160 --> 01:53:46.400]   You go to a new place and it automatically brings up the dialogues,
[01:53:46.440 --> 01:53:49.560]   watches them and says it selects everything to say no.
[01:53:49.560 --> 01:53:50.760]   Oh, that's nice.
[01:53:50.760 --> 01:53:55.160]   And you watch it go through your screen, does it in front of you?
[01:53:55.160 --> 01:53:55.800]   It's wonderful.
[01:53:55.800 --> 01:53:57.280]   Is that an extension?
[01:53:57.280 --> 01:54:00.640]   Or is it built into Firefox?
[01:54:00.640 --> 01:54:02.200]   I think they build it in.
[01:54:02.200 --> 01:54:04.320]   And that's one of the recent versions.
[01:54:04.320 --> 01:54:06.480]   It's sort of exciting to.
[01:54:06.480 --> 01:54:09.400]   I am a very happy Firefox user.
[01:54:09.400 --> 01:54:11.600]   Yeah, I really like Firefox.
[01:54:11.600 --> 01:54:12.280]   I really do.
[01:54:13.640 --> 01:54:19.480]   I want to take a break and we come back, Larry, you mentioned your horrific story.
[01:54:19.480 --> 01:54:23.920]   We actually had you on about a month ago on Tech News Weekly to describe this,
[01:54:23.920 --> 01:54:25.840]   but we've never had you on Twitch to describe it.
[01:54:25.840 --> 01:54:28.600]   And I want to show the video that you've also made.
[01:54:28.600 --> 01:54:34.040]   You call it virtual kidnapping scams and it happened to you.
[01:54:34.040 --> 01:54:38.040]   So we'll talk about that when we come back and just a little bit.
[01:54:38.040 --> 01:54:41.080]   Great to have Larry Maggett here from connect safely.org.
[01:54:42.320 --> 01:54:45.760]   From the wonderful Cronkite School of Journalism.
[01:54:45.760 --> 01:54:47.960]   I just love that name in Arizona State.
[01:54:47.960 --> 01:54:55.080]   He now teaches Dan Gilmore Media Literacy, which is even more important at the ASU
[01:54:55.080 --> 01:54:56.360]   News CoLab.
[01:54:56.360 --> 01:54:58.520]   Thank you for being here, Dan.
[01:54:58.520 --> 01:54:59.280]   I appreciate it.
[01:54:59.280 --> 01:55:01.080]   It's great to have your intelligence.
[01:55:01.080 --> 01:55:03.760]   And of course, it's the wonderful film girl.
[01:55:03.760 --> 01:55:06.280]   They still call her that, I guess, around the office.
[01:55:06.840 --> 01:55:13.680]   Senior dev advocate for GitHub, Christina Warren, who I've known since she was knee
[01:55:13.680 --> 01:55:15.040]   high to a grasshopper.
[01:55:15.040 --> 01:55:19.120]   I was thinking the other day about when I first saw you and I think it was at a
[01:55:19.120 --> 01:55:20.560]   Macworld Expo.
[01:55:20.560 --> 01:55:21.960]   Yeah.
[01:55:21.960 --> 01:55:25.160]   Yeah, you were there for Mashable maybe or another.
[01:55:25.160 --> 01:55:27.080]   No, no, the unofficial Apple.
[01:55:27.080 --> 01:55:28.480]   Oh, yeah.
[01:55:28.480 --> 01:55:31.000]   To to to to to to to to to.
[01:55:31.000 --> 01:55:31.520]   Yeah.
[01:55:31.520 --> 01:55:35.320]   Uh, and you were sitting on the floor with your colleagues covering Macworld.
[01:55:35.320 --> 01:55:36.520]   And you were eating your lunch.
[01:55:36.520 --> 01:55:39.600]   I think I probably came up to you and said, Oh, film girl.
[01:55:39.600 --> 01:55:43.440]   I've been a fanboy ever since.
[01:55:43.440 --> 01:55:46.480]   Great to have you and your sneakers on the show.
[01:55:46.480 --> 01:55:48.880]   Our show today brought to you by.
[01:55:48.880 --> 01:55:51.240]   Well, I think I need it right now.
[01:55:51.240 --> 01:55:53.440]   Athletic Greens, we're going to take a little vitamin break.
[01:55:53.440 --> 01:55:55.200]   Everybody get together.
[01:55:55.200 --> 01:55:58.320]   Like many of you, I wanted to support my health.
[01:55:58.320 --> 01:56:01.920]   I have been taking literally my son.
[01:56:02.640 --> 01:56:07.520]   The other day I this was, I was visiting him at school this actually a few years ago.
[01:56:07.520 --> 01:56:12.880]   And I took a handful of vitamins and swallowed it and he was a guest.
[01:56:12.880 --> 01:56:14.040]   He said, what are you doing?
[01:56:14.040 --> 01:56:18.000]   I said, well, I want to, you know, I have to make sure I've got the proper nutrition.
[01:56:18.000 --> 01:56:19.680]   Well, it's a lot easier now.
[01:56:19.680 --> 01:56:22.560]   Thanks to AG one.
[01:56:22.560 --> 01:56:26.800]   It gives my body what it craves in one daily nutritional drink.
[01:56:26.800 --> 01:56:29.040]   You do it before you eat in the morning.
[01:56:29.040 --> 01:56:30.360]   It's actually a great way to start.
[01:56:30.360 --> 01:56:32.120]   AG one was founded in 2010.
[01:56:32.120 --> 01:56:34.680]   So part of millions of routines ever since.
[01:56:34.680 --> 01:56:38.000]   In fact, every time I talk about it, I hear from people saying, Oh, yeah, I've been
[01:56:38.000 --> 01:56:39.000]   doing this for years.
[01:56:39.000 --> 01:56:40.600]   Well, I just discovered it.
[01:56:40.600 --> 01:56:43.600]   It's the best all in one solution for daily nutrition.
[01:56:43.600 --> 01:56:48.760]   It saves you time, confusion and money each serving less than $3 a day.
[01:56:48.760 --> 01:56:50.960]   It's a lot less than I was paying for supplements.
[01:56:50.960 --> 01:56:53.640]   And it's not just vitamins, minerals.
[01:56:53.640 --> 01:56:56.200]   It's also pro and prebiotics.
[01:56:56.200 --> 01:56:58.040]   It supports your gut health.
[01:56:58.400 --> 01:57:04.360]   It's got everything you need, 75 high quality minerals, vitamins, pre and probiotics.
[01:57:04.360 --> 01:57:06.240]   You put it in 12 ounces of water.
[01:57:06.240 --> 01:57:09.480]   In fact, if you get the AG one kit, you'll get the shaker that I'm using here.
[01:57:09.480 --> 01:57:14.240]   You get a scoop, beautiful aluminum scoop and a canister to store your AG one in.
[01:57:14.240 --> 01:57:15.920]   It dissolves very easily.
[01:57:15.920 --> 01:57:16.960]   You don't have to use hot water.
[01:57:16.960 --> 01:57:17.960]   This is cold water.
[01:57:17.960 --> 01:57:21.400]   You shake it up and it's a seamless daily habit.
[01:57:21.400 --> 01:57:26.080]   One scoop of AG one in the morning is everything you need.
[01:57:26.320 --> 01:57:33.680]   And oh, by the way, I've tried other ones of these other, you know, drinks that you can't, you have to choke down.
[01:57:33.680 --> 01:57:34.880]   This is delicious.
[01:57:34.880 --> 01:57:35.880]   It's refreshing.
[01:57:35.880 --> 01:57:37.120]   I love it.
[01:57:37.120 --> 01:57:38.560]   Mm.
[01:57:38.560 --> 01:57:44.480]   If you're looking for a simpler cost effective supplement routine, look no farther.
[01:57:44.480 --> 01:57:46.520]   AG one by athletic greens.
[01:57:46.520 --> 01:57:53.120]   If you order right now, if you get a subscription, you'll get a free one year supply of vitamin D.
[01:57:53.960 --> 01:57:57.840]   And these travel packs that I was just using, these are super convenient.
[01:57:57.840 --> 01:58:01.360]   You get five free travel packs with your first purchase of a subscription.
[01:58:01.360 --> 01:58:09.040]   Athleticgreens.com/twit athleticgreens.com/twit.
[01:58:09.040 --> 01:58:10.720]   Thank you so much for their support.
[01:58:10.720 --> 01:58:15.680]   Thank you, athletic greens for supporting my my my dietary needs.
[01:58:15.680 --> 01:58:18.120]   Mm.
[01:58:18.120 --> 01:58:21.840]   I think I drink that just to prove that it tastes delicious.
[01:58:21.840 --> 01:58:22.760]   I mean, it really does.
[01:58:22.760 --> 01:58:23.760]   It's really, really good.
[01:58:24.720 --> 01:58:26.080]   Much better than that cup of coffee.
[01:58:26.080 --> 01:58:30.640]   I just had a g one athletic greens.com/twit.
[01:58:30.640 --> 01:58:32.040]   All right.
[01:58:32.040 --> 01:58:34.280]   I needed the fortification.
[01:58:34.280 --> 01:58:35.880]   Oh, Larry's gone for a moment.
[01:58:35.880 --> 01:58:36.560]   We'll just give him a.
[01:58:36.560 --> 01:58:37.960]   There he is.
[01:58:37.960 --> 01:58:44.320]   I needed the fortification, Larry, because I knew you were going to tell a horrific story.
[01:58:44.320 --> 01:58:49.160]   Tell us what happened in your own words, Mr.
[01:58:49.160 --> 01:58:54.960]   Megan. So one day, oh, I know, but I'm months ago, my cell phone rings and I pick it up.
[01:58:54.960 --> 01:58:58.720]   And there is a screaming or crying woman on the phone.
[01:58:58.720 --> 01:59:05.240]   And it, oh, I forgot to tell you the caller ID looks like it's from my wife's phone.
[01:59:05.240 --> 01:59:09.800]   It's actually it turned out later, I found out it with one digit off, but it looks like her phone number.
[01:59:09.800 --> 01:59:12.320]   And I hear this person screaming and crying.
[01:59:12.320 --> 01:59:15.280]   It's a female voice and I say, Patty, what's the matter?
[01:59:15.280 --> 01:59:17.760]   And she continues to cream and cream and crime.
[01:59:17.760 --> 01:59:19.240]   And this is very unlike her.
[01:59:19.240 --> 01:59:23.440]   She's not an hysterical person, but, you know, I was worried.
[01:59:23.440 --> 01:59:27.440]   And then all of a sudden she gets off the phone and up comes officer.
[01:59:27.440 --> 01:59:29.280]   So and so a quote, a police officer.
[01:59:29.280 --> 01:59:33.520]   The officer tells me that I'm with your wife.
[01:59:33.520 --> 01:59:37.400]   And before I can provide you with information, I need some information from you.
[01:59:37.400 --> 01:59:38.880]   And I say, come on, get to it.
[01:59:38.880 --> 01:59:41.880]   I mean, I'm really sucked into it at this point.
[01:59:41.880 --> 01:59:42.760]   Call her ID.
[01:59:42.760 --> 01:59:43.360]   Look like her.
[01:59:43.360 --> 01:59:46.920]   If I heard a voice in my head, I'd saw it with her anyway.
[01:59:46.920 --> 01:59:50.520]   So he asked me my name and her name, which I would never ordinarily give.
[01:59:50.520 --> 01:59:52.480]   I'm a fairly savvy guy, but I do.
[01:59:52.480 --> 01:59:55.360]   And the next thing he knows, actually, I'm not a police officer.
[01:59:55.360 --> 01:59:58.760]   I'm with a Mexican drug cartel and we have your wife.
[01:59:58.760 --> 02:00:06.720]   And, you know, and at that point, I put my cell phone on speaker and I still have a landline, believe it or not.
[02:00:06.720 --> 02:00:08.400]   It's an internet phone, a phone.
[02:00:08.400 --> 02:00:12.960]   And I called 911 and so they could hear the entire call.
[02:00:12.960 --> 02:00:14.520]   I didn't talk to the 911.
[02:00:14.520 --> 02:00:16.320]   I just called him, say, get over here.
[02:00:16.880 --> 02:00:20.760]   And we, the call went on with this person for 11 minutes.
[02:00:20.760 --> 02:00:27.400]   And at one point, he said, you need to get in the car right now and start driving to a Walmart in San Jose.
[02:00:27.400 --> 02:00:30.800]   But first, you need to go to the bank and get $5,000 in cash.
[02:00:30.800 --> 02:00:33.560]   I kept it.
[02:00:33.560 --> 02:00:35.360]   And I kind of played a little along with him.
[02:00:35.360 --> 02:00:38.520]   I mean, I wasn't sure it was a real situation.
[02:00:38.520 --> 02:00:42.120]   You didn't know what it was, but I was, I was emotionally really upset.
[02:00:42.120 --> 02:00:42.400]   Very.
[02:00:42.400 --> 02:00:44.960]   So they get to screaming, get you like all.
[02:00:44.960 --> 02:00:46.400]   My wife was in Sanford.
[02:00:46.400 --> 02:00:48.640]   She was in San Francisco that day, which is unusual.
[02:00:48.640 --> 02:00:52.320]   Normally, especially since COVID, she, you know, she's at Holland, I'm at home.
[02:00:52.320 --> 02:00:54.520]   So it's unusual for her not to be with me.
[02:00:54.520 --> 02:00:57.440]   And he said something about her being in San Francisco.
[02:00:57.440 --> 02:01:00.200]   I'm not sure he started with that where he got it for me.
[02:01:00.200 --> 02:01:02.840]   I mean, there's a lot I don't know about my own action.
[02:01:02.840 --> 02:01:06.520]   Anyway, long story short, the call goes on for 11 minutes.
[02:01:06.520 --> 02:01:10.360]   Finally, I think he came to the realization I was not going to comply.
[02:01:10.360 --> 02:01:14.600]   I didn't say I wouldn't comply, but I kept asking for questions and things like that.
[02:01:15.040 --> 02:01:16.200]   And he hung up.
[02:01:16.200 --> 02:01:19.680]   I later talked to the 911 operator who heard the entire thing.
[02:01:19.680 --> 02:01:23.880]   He said that they took it seriously so seriously that they sent the SFP
[02:01:23.880 --> 02:01:25.720]   D to check on my wife.
[02:01:25.720 --> 02:01:27.080]   And by the way, I knew where she was.
[02:01:27.080 --> 02:01:27.560]   Wow.
[02:01:27.560 --> 02:01:29.760]   Did I have, I track her with her permission.
[02:01:29.760 --> 02:01:31.960]   She and I track each other on Google Maps.
[02:01:31.960 --> 02:01:35.120]   So I knew she was at the Embarcadero Center, but that's a big place.
[02:01:35.120 --> 02:01:37.880]   Anyway, point is it was a virtual kidnapping.
[02:01:37.880 --> 02:01:42.960]   I didn't pay the ransom, but I have to tell you, it was very painful.
[02:01:42.960 --> 02:01:44.240]   It was, it was terrifying.
[02:01:44.640 --> 02:01:49.200]   I was so shaken that I actually got in, I got on the train and visited her in
[02:01:49.200 --> 02:01:49.840]   San Francisco.
[02:01:49.840 --> 02:01:52.480]   I needed to be with her because I was just so annoyed by this.
[02:01:52.480 --> 02:01:52.800]   Yeah.
[02:01:52.800 --> 02:01:57.880]   Gonna be at one point, one of my colleagues called me up and I said, I have to hang
[02:01:57.880 --> 02:01:58.280]   up right now.
[02:01:58.280 --> 02:01:59.320]   My wife's been kidnapped.
[02:01:59.320 --> 02:01:59.880]   Okay.
[02:01:59.880 --> 02:02:01.520]   I didn't think she might have been kidnapped.
[02:02:01.520 --> 02:02:03.440]   Oh, actually, believe it enough.
[02:02:03.440 --> 02:02:09.800]   Oh, well, based on this experience, as you saw, I wrote about it on for the Mercury
[02:02:09.800 --> 02:02:14.040]   news and also on Connect Safely and then Connect Safely just recently put together
[02:02:14.480 --> 02:02:17.400]   a guide to virtual kidnapping and a video.
[02:02:17.400 --> 02:02:18.920]   I think you said you might want to show the video.
[02:02:18.920 --> 02:02:20.920]   People need to be aware of that.
[02:02:20.920 --> 02:02:23.400]   And again, it's embarrassing for me to say this.
[02:02:23.400 --> 02:02:28.320]   And as I said at the end of my column, knowledge is power, but emotions can be
[02:02:28.320 --> 02:02:29.400]   stronger than knowledge.
[02:02:29.400 --> 02:02:33.120]   And despite everything I know about computer technology and security, I've
[02:02:33.120 --> 02:02:37.720]   written literally hundreds and hundreds of articles on various aspects of human
[02:02:37.720 --> 02:02:39.240]   engineering and computer security.
[02:02:39.240 --> 02:02:43.920]   I still, I wouldn't say I fell for it, but I was still heavily impacted by it.
[02:02:44.520 --> 02:02:48.880]   And I, you know, I think it's important to say that we're all vulnerable, you
[02:02:48.880 --> 02:02:50.760]   know, that it isn't on your part.
[02:02:50.760 --> 02:02:53.440]   I mean, of all people, you're not ignorant of all this stuff.
[02:02:53.440 --> 02:02:58.480]   You're very technologically literate, but they use emotion to the point where
[02:02:58.480 --> 02:02:59.360]   you stop thinking.
[02:02:59.360 --> 02:03:05.160]   Yeah, they cut off your frontal lobe and, and, and you're just acting on emotion.
[02:03:05.160 --> 02:03:08.160]   And, you know, we do dumb things.
[02:03:08.160 --> 02:03:09.600]   You didn't really fall for it.
[02:03:09.600 --> 02:03:10.680]   You didn't send them $5,000.
[02:03:10.680 --> 02:03:11.840]   I didn't take the five.
[02:03:11.840 --> 02:03:13.680]   I didn't give them the five grand, but believe me,
[02:03:13.680 --> 02:03:15.840]   it, you know, it hurts still.
[02:03:15.840 --> 02:03:20.680]   Um, and, you know, and as I said in a follow up column, it's going to get worse
[02:03:20.680 --> 02:03:25.080]   because now we've got the ability to recreate our loved one's voice or they
[02:03:25.080 --> 02:03:27.480]   have the way to recreate your loved one's voice.
[02:03:27.480 --> 02:03:29.280]   And I think it's going to get worse.
[02:03:29.280 --> 02:03:30.080]   It's yeah.
[02:03:30.080 --> 02:03:31.760]   Cause this was just a screaming woman.
[02:03:31.760 --> 02:03:35.320]   Obviously a scam, probably completely at random.
[02:03:35.320 --> 02:03:40.360]   Uh, it sounds like he knew a few things, but maybe that was just luck or, or it wasn't
[02:03:40.360 --> 02:03:41.560]   clear what he knew and what he didn't know.
[02:03:41.560 --> 02:03:42.000]   Yeah.
[02:03:42.000 --> 02:03:42.040]   Yeah.
[02:03:42.040 --> 02:03:42.560]   Yeah.
[02:03:42.920 --> 02:03:46.320]   I mean, one of the things I did, I changed all our passwords because maybe they
[02:03:46.320 --> 02:03:47.320]   hacked into our Google account.
[02:03:47.320 --> 02:03:48.120]   Yeah.
[02:03:48.120 --> 02:03:52.680]   Uh, should I play the v, I'll play the v, I'd like you to don't mind.
[02:03:52.680 --> 02:03:52.960]   Yeah.
[02:03:52.960 --> 02:03:56.160]   If you play it into the, we'll play it into the record as they say.
[02:03:56.160 --> 02:03:57.760]   This is just a couple of minutes.
[02:03:57.760 --> 02:04:05.560]   Virtual kidnapping scams are an insidious form of fraud, praying on our deepest fears.
[02:04:05.560 --> 02:04:07.920]   Let's talk through how this scam works.
[02:04:08.880 --> 02:04:13.280]   In simple terms, a virtual kidnapping scam is a social engineering scam aimed at
[02:04:13.280 --> 02:04:17.280]   extorting money from unsuspecting people by convincing them that a loved one has
[02:04:17.280 --> 02:04:18.000]   been kidnapped.
[02:04:18.000 --> 02:04:22.680]   These scammers employ techniques such as cloning phone numbers or even using
[02:04:22.680 --> 02:04:25.920]   artificial intelligence to mimic the voice of your loved one, making their
[02:04:25.920 --> 02:04:27.720]   calls incredibly convincing.
[02:04:27.720 --> 02:04:31.440]   Detecting a virtual kidnapping scam can be challenging, especially when
[02:04:31.440 --> 02:04:32.600]   emotions are running high.
[02:04:32.600 --> 02:04:36.560]   Here are some key steps to keep in mind if you suspect you're being targeted.
[02:04:37.000 --> 02:04:41.160]   The FBI says that the best course of action in most cases is simply to hang up the
[02:04:41.160 --> 02:04:41.520]   phone.
[02:04:41.520 --> 02:04:45.040]   But if you do get reeled in into conversation with a scammer, here are some
[02:04:45.040 --> 02:04:46.000]   tips on what to do.
[02:04:46.000 --> 02:04:48.240]   One, stay calm.
[02:04:48.240 --> 02:04:52.440]   Remember, scammers rely on fear and panic to manipulate their victims.
[02:04:52.440 --> 02:04:54.520]   Try to slow the situation down.
[02:04:54.520 --> 02:05:00.800]   Two, guard personal information, regardless of who the caller claims to be,
[02:05:00.800 --> 02:05:05.520]   avoid sharing any personally identifying details such as names, locations, or
[02:05:05.520 --> 02:05:06.800]   information about your loved one.
[02:05:07.640 --> 02:05:09.480]   Three, seek help.
[02:05:09.480 --> 02:05:12.320]   If possible, have someone nearby call 911.
[02:05:12.320 --> 02:05:17.240]   If you're alone, discreetly call 911 from another phone if possible and put
[02:05:17.240 --> 02:05:19.160]   the call on speaker as an operator to hear.
[02:05:19.160 --> 02:05:24.280]   Four, verify independently, reach out to your loved one directly, or have
[02:05:24.280 --> 02:05:27.080]   someone else call their phone or check their location if it's been shared.
[02:05:27.080 --> 02:05:28.960]   You want to confirm their safety.
[02:05:28.960 --> 02:05:32.520]   Five, ask to speak to your loved one.
[02:05:32.520 --> 02:05:36.720]   If the scammer insists on speaking for them, ask a question that only they
[02:05:36.720 --> 02:05:37.720]   would know the answer to.
[02:05:37.720 --> 02:05:41.360]   Consider establishing a code word or phrase with your family members as an
[02:05:41.360 --> 02:05:42.480]   additional security measure.
[02:05:42.480 --> 02:05:44.600]   Prevention is always the best defense.
[02:05:44.600 --> 02:05:48.360]   So let's explore proactive steps to protect ourselves from virtual kidnapping
[02:05:48.360 --> 02:05:52.120]   scams, discuss virtual kidnapping scams with your family members and loved ones,
[02:05:52.120 --> 02:05:54.800]   ensuring they're aware of the threat and how to respond.
[02:05:54.800 --> 02:06:00.400]   Avoid sharing real time location or travel plans on social media as this
[02:06:00.400 --> 02:06:02.520]   information can be exploited by scammers.
[02:06:03.360 --> 02:06:07.040]   Refrain from sharing your phone number or recordings of your voice online.
[02:06:07.040 --> 02:06:10.240]   Scammers can use this information to make their claims more convincing.
[02:06:10.240 --> 02:06:11.000]   I'm so intrigued.
[02:06:11.000 --> 02:06:14.960]   Utilize location sharing services with their permission and reciprocally
[02:06:14.960 --> 02:06:18.280]   share your cell phone location with loved ones using legitimate tracking
[02:06:18.280 --> 02:06:21.240]   services like Google Maps or Apple's Find My Friends.
[02:06:21.240 --> 02:06:22.840]   Note physical details.
[02:06:22.840 --> 02:06:26.000]   Make a mental note or written note of what your loved ones are wearing when
[02:06:26.000 --> 02:06:27.560]   they leave the house and where they're going.
[02:06:27.560 --> 02:06:30.360]   For more information, visit connect safely.org.
[02:06:30.360 --> 02:06:31.720]   Nice.
[02:06:32.360 --> 02:06:32.840]   That's good.
[02:06:32.840 --> 02:06:34.200]   That's what Knek loudly does.
[02:06:34.200 --> 02:06:35.000]   That's very well done.
[02:06:35.000 --> 02:06:35.480]   I like it.
[02:06:35.480 --> 02:06:36.080]   Very simple.
[02:06:36.080 --> 02:06:37.640]   Then by me, a director, Chris Lee.
[02:06:37.640 --> 02:06:40.880]   And I think if you want to share this and I recommend you share this,
[02:06:40.880 --> 02:06:45.160]   especially if you or people in your family are older because, you know,
[02:06:45.160 --> 02:06:50.400]   older people like me, the ones that are targeted off into this, you can find
[02:06:50.400 --> 02:06:54.720]   this now in the upper right corner on Connect Safely Connect Safely.org
[02:06:54.720 --> 02:06:59.240]   or go directly there with connect safely.org/virtual kidnapping.
[02:06:59.240 --> 02:07:01.040]   How widespread is this?
[02:07:01.360 --> 02:07:03.080]   It's very widespread and it's growing.
[02:07:03.080 --> 02:07:05.200]   And as I said, it's going to get a lot worse with AI.
[02:07:05.200 --> 02:07:08.800]   When we talked about the existential threat, this is the threat that I
[02:07:08.800 --> 02:07:13.640]   worry about, this kind of the use of AI to exploit people to commit crimes.
[02:07:13.640 --> 02:07:15.520]   I think it's going to get a lot worse.
[02:07:15.520 --> 02:07:19.080]   And in a sense, this was artificial emotional intelligence, but
[02:07:19.080 --> 02:07:24.280]   it's going to get a lot worse as they're able to clone people's voices.
[02:07:24.280 --> 02:07:27.720]   And I agree, Lee, you and I are screwed when it comes to, you know, having
[02:07:27.720 --> 02:07:28.880]   their voices on the internet.
[02:07:28.880 --> 02:07:30.880]   I mean, every, but I'm, but some are a lot of people.
[02:07:30.880 --> 02:07:33.000]   Everything he said in there, I'm like, Oh, yeah.
[02:07:33.000 --> 02:07:35.240]   OK, don't post where you are social.
[02:07:35.240 --> 02:07:37.560]   Oh, I'm dead meat, you know.
[02:07:37.560 --> 02:07:40.520]   But what we did do, I know this is a possibility.
[02:07:40.520 --> 02:07:44.440]   What I did do with my family members is set up a key word, you know,
[02:07:44.440 --> 02:07:47.160]   that I can use that they all know.
[02:07:47.160 --> 02:07:48.920]   One, by the way, quick pro script.
[02:07:48.920 --> 02:07:53.680]   So yesterday I get a WhatsApp message, which comes from the Congo.
[02:07:53.680 --> 02:07:55.960]   And I said, and he's talking about a kidnapping.
[02:07:55.960 --> 02:07:57.720]   And I said, ah, this is a scam.
[02:07:57.720 --> 02:07:59.280]   I'm not going to worry about it.
[02:07:59.280 --> 02:08:02.840]   Then a minute later, I get the same, I get a message and the person identifies
[02:08:02.840 --> 02:08:03.840]   himself.
[02:08:03.840 --> 02:08:05.200]   And it's a name that I know.
[02:08:05.200 --> 02:08:07.440]   I actually have a colleague that lives in the Congo.
[02:08:07.440 --> 02:08:09.840]   That's very active in African internet safety.
[02:08:09.840 --> 02:08:12.440]   So I said, can you prove to me you are who you say you are?
[02:08:12.440 --> 02:08:16.160]   And he sends me a photograph of him and me and my wife and his wife together.
[02:08:16.160 --> 02:08:17.640]   When we last saw each other.
[02:08:17.640 --> 02:08:19.880]   And it turned out it was a real kidnapping.
[02:08:19.880 --> 02:08:20.560]   Oh my.
[02:08:20.560 --> 02:08:22.360]   And I was almost going to write it off.
[02:08:22.360 --> 02:08:26.160]   But he needed my help to get ahold of people at meta to make sure that this
[02:08:26.160 --> 02:08:29.920]   person's account and luckily this person was rescued.
[02:08:29.920 --> 02:08:31.000]   I don't have all the details.
[02:08:31.000 --> 02:08:35.080]   But the point is that that was a rare case of a real kidnapping.
[02:08:35.080 --> 02:08:36.520]   And that's one of the problems.
[02:08:36.520 --> 02:08:40.560]   You don't want to be immune to the actual danger when you're convinced that it's
[02:08:40.560 --> 02:08:41.480]   all going to be a scam.
[02:08:41.480 --> 02:08:44.960]   I do have and Lisa and I both share our location.
[02:08:44.960 --> 02:08:47.240]   Apple lets you build that in.
[02:08:47.240 --> 02:08:50.440]   And so I can see on the front page of my phone.
[02:08:50.440 --> 02:08:53.720]   Well, you don't have to show it, but I guess.
[02:08:53.720 --> 02:08:54.960]   Yeah, don't show it.
[02:08:54.960 --> 02:08:58.960]   Because it's our home, but I can see the leases at home.
[02:08:58.960 --> 02:09:01.400]   And that, you know, that's pretty valuable.
[02:09:01.400 --> 02:09:02.880]   It's just like you have with yours.
[02:09:02.880 --> 02:09:06.160]   There was actually just an article, I think, in the New York Times about people
[02:09:06.160 --> 02:09:10.800]   sharing their location and the pros and cons of doing that with family members.
[02:09:10.800 --> 02:09:13.560]   The nice thing about Google Maps is it doesn't matter what operating, you know,
[02:09:13.560 --> 02:09:14.280]   you can use it.
[02:09:14.280 --> 02:09:17.600]   You can have a relative that's on iPhone and you're on Android or whatever.
[02:09:17.600 --> 02:09:19.520]   And so it goes across platforms.
[02:09:19.520 --> 02:09:20.360]   But yes, it is.
[02:09:20.360 --> 02:09:21.680]   We do do that.
[02:09:21.680 --> 02:09:23.200]   And it's very reassuring.
[02:09:23.200 --> 02:09:24.640]   And again, obviously with permission.
[02:09:25.320 --> 02:09:30.000]   Nobody's nobody's, you know, that's the problem is it looks a lot like a spousal
[02:09:30.000 --> 02:09:30.560]   stalking.
[02:09:30.560 --> 02:09:32.320]   It's well, right.
[02:09:32.320 --> 02:09:33.040]   Three on it.
[02:09:33.040 --> 02:09:34.000]   It has to be between.
[02:09:34.000 --> 02:09:37.160]   We trust one another and agree that this we're going to do this.
[02:09:37.160 --> 02:09:41.480]   But I'm really actually glad that Lisa allows me to track her.
[02:09:41.480 --> 02:09:42.760]   So speak.
[02:09:42.760 --> 02:09:45.480]   And I'm going to track her by doing.
[02:09:45.480 --> 02:09:52.920]   Yeah, there's an app called TICE T I C E that comes out of Europe, where data
[02:09:52.920 --> 02:10:02.680]   protection is privacy is better than the US that's they actively try not to do
[02:10:02.680 --> 02:10:07.680]   anything beyond the location sharing between two people.
[02:10:07.680 --> 02:10:08.080]   That's it.
[02:10:08.080 --> 02:10:11.120]   No collection of cloud data.
[02:10:11.120 --> 02:10:14.320]   No, none of the stuff that the big companies here do.
[02:10:14.320 --> 02:10:19.120]   So in this category, that's worth taking.
[02:10:19.120 --> 02:10:29.000]   Look, I have not fully taken a look, but people I know in the security area have
[02:10:29.000 --> 02:10:31.120]   said that this looks really good to them.
[02:10:31.120 --> 02:10:33.840]   Yeah, this looks great.
[02:10:33.840 --> 02:10:37.520]   And I trust Apple because Lisa and I are both on iPhones.
[02:10:37.520 --> 02:10:40.840]   I can use that system and I trust them not to.
[02:10:40.840 --> 02:10:43.360]   Well, I don't know what they're doing with it, but I don't think they're selling
[02:10:43.360 --> 02:10:44.680]   it to data brokers.
[02:10:44.680 --> 02:10:47.200]   Do you do this with your husband, Christina or no?
[02:10:47.800 --> 02:10:55.280]   I don't, but for various reasons, I don't, but that might change.
[02:10:55.280 --> 02:11:00.440]   I have, I have thought about using like find my friends with my mom before it
[02:11:00.440 --> 02:11:01.240]   workplaces.
[02:11:01.240 --> 02:11:06.200]   And honestly, like my concern is because at my age, like I'm not concerned about
[02:11:06.200 --> 02:11:12.160]   me being taken in by somebody, like, you know, calling me about my husband, being
[02:11:12.160 --> 02:11:15.280]   kidnapped or him being called about me.
[02:11:15.840 --> 02:11:19.800]   We would text one, like our method of communication is different, but I am
[02:11:19.800 --> 02:11:26.280]   concerned about people potentially targeting my parents and like, like, like
[02:11:26.280 --> 02:11:30.400]   everybody else on here, there's hundreds and hundreds and hundreds of hours of my
[02:11:30.400 --> 02:11:31.400]   voice on the internet.
[02:11:31.400 --> 02:11:35.200]   So the AI can be trained to mimic my voice extremely well.
[02:11:35.200 --> 02:11:40.880]   And I am concerned with that being used to potentially, you know, bait people
[02:11:40.880 --> 02:11:45.200]   in into meeting my parents into thinking that there was some sort of scenario
[02:11:45.200 --> 02:11:49.680]   happening and especially since I travel as much as I do, like that would be a not
[02:11:49.680 --> 02:11:52.760]   inconsequential thing for them to be like, well, I didn't even know that she was
[02:11:52.760 --> 02:11:55.520]   here. Well, okay, but she might have been right.
[02:11:55.520 --> 02:11:59.120]   Like there, there are just a lot of things that make me, I've been concerned
[02:11:59.120 --> 02:11:59.960]   about this for a while.
[02:11:59.960 --> 02:12:01.880]   I've heard of these scans before.
[02:12:01.880 --> 02:12:06.440]   I didn't know exactly how it worked, but they definitely play on, you know, your
[02:12:06.440 --> 02:12:08.160]   emotions as you were saying.
[02:12:08.160 --> 02:12:11.880]   And there are things that I've just done in my life and that if you're in any
[02:12:11.880 --> 02:12:15.880]   way sort of public, I think it makes you especially risky because there are
[02:12:15.880 --> 02:12:20.200]   things that we can't opt out of that, you know, could be used to target the
[02:12:20.200 --> 02:12:20.920]   people that we love.
[02:12:20.920 --> 02:12:22.160]   And that's what I worry about.
[02:12:22.160 --> 02:12:26.560]   Like I'm not as concerned with sharing my like location with my husband right
[02:12:26.560 --> 02:12:29.840]   now. That doesn't make it a ton of sense for us, but I understand respect
[02:12:29.840 --> 02:12:32.560]   other people who do it, but I have in certain ways.
[02:12:32.560 --> 02:12:37.640]   Like I even ironically, I post about my location when I'm at airports or things
[02:12:37.640 --> 02:12:42.920]   like that to let my mom know where I am because she would always worry.
[02:12:42.920 --> 02:12:45.800]   So there's like a catch 22 with all of it.
[02:12:45.800 --> 02:12:49.600]   I think it's important to point out that people your age are, I don't know if
[02:12:49.600 --> 02:12:53.880]   anybody is targeted, but people your age are victimized and people your age have
[02:12:53.880 --> 02:12:54.760]   paid the ransom.
[02:12:54.760 --> 02:12:58.280]   If that's the CNN did a story about virtual kidnapping recently.
[02:12:58.280 --> 02:13:01.760]   And I think the person they gave you that example was not too much older than you
[02:13:01.760 --> 02:13:01.920]   are.
[02:13:01.920 --> 02:13:05.480]   So I don't think anybody should be saying one just because they're not.
[02:13:05.480 --> 02:13:06.360]   No, I agree with that.
[02:13:06.360 --> 02:13:07.120]   No, I agree with that.
[02:13:07.120 --> 02:13:11.000]   I'm just saying like I tend to be like as a person, I tend to have like much like
[02:13:11.000 --> 02:13:13.960]   you, you know, I have my blinders up and you weren't taken in and like the same
[02:13:13.960 --> 02:13:14.720]   thing with fishing things.
[02:13:14.720 --> 02:13:18.560]   I've certainly been the victim of a fishing thing before and then immediately
[02:13:18.560 --> 02:13:20.440]   almost instantly recognized what it is.
[02:13:20.440 --> 02:13:24.960]   But my my fear is more not to say that I couldn't be taken in by something.
[02:13:24.960 --> 02:13:29.120]   My fear is more, okay, how could my voice or how could my information be
[02:13:29.120 --> 02:13:33.640]   weaponized to go after people who are less tech savvy than me using emotions
[02:13:33.640 --> 02:13:36.040]   and all and actually using the tech as well.
[02:13:36.480 --> 02:13:40.560]   I have to say that 20 minutes of sheer care was probably worth having paid a
[02:13:40.560 --> 02:13:41.720]   five thousand rams from it.
[02:13:41.720 --> 02:13:44.000]   Yeah, it was it was horrible.
[02:13:44.000 --> 02:13:48.120]   I think what you're and what you're describing is going to be used in a more
[02:13:48.120 --> 02:13:55.720]   in a lesser dire context, but more condensing because we all have gotten
[02:13:55.720 --> 02:14:02.920]   emails and texts from supposed friends overseas who had their wallet stolen
[02:14:02.920 --> 02:14:04.760]   and need to wire up some money.
[02:14:05.280 --> 02:14:07.960]   Well, their friend is going to call you, right?
[02:14:07.960 --> 02:14:10.800]   And it's going to sound exactly like your friend.
[02:14:10.800 --> 02:14:16.000]   And this is going to be have been well pre-programmed with lots of branching
[02:14:16.000 --> 02:14:22.360]   for whatever your responses to say the right thing and AI is going to make
[02:14:22.360 --> 02:14:23.920]   that even more convincing.
[02:14:23.920 --> 02:14:33.120]   That's going to be interesting in a lot of ways to see how how well the bad guys
[02:14:33.120 --> 02:14:37.840]   game it out for our responses to see if it's a real thing.
[02:14:37.840 --> 02:14:41.160]   That I mean, it's going to get this is a real arms race.
[02:14:41.160 --> 02:14:44.120]   And that's why it's important either have a code where at least say things
[02:14:44.120 --> 02:14:46.400]   and I don't know, maybe they'll get the old scam this as well.
[02:14:46.400 --> 02:14:47.640]   But where do we meet?
[02:14:47.640 --> 02:14:48.760]   Where did you go to school?
[02:14:48.760 --> 02:14:53.080]   I mean, questions that they'd have to be very, very sophisticated to be able
[02:14:53.080 --> 02:14:54.400]   to answer if they're scammers.
[02:14:54.400 --> 02:14:59.480]   I'm not saying I always ask what what what is Wikipedia say my birthday is?
[02:14:59.480 --> 02:15:02.640]   And if they I'm screwed.
[02:15:02.640 --> 02:15:03.640]   I'm deep trouble.
[02:15:03.640 --> 02:15:10.720]   I have to go to be sides deep tracks before I can get something people don't know about me.
[02:15:10.720 --> 02:15:13.720]   Well, no, every public figure takes that risk.
[02:15:13.720 --> 02:15:17.880]   And you know, on the fact is that it's public figures, you know, our family members
[02:15:17.880 --> 02:15:21.520]   know, I think that we, you know, not to believe anything.
[02:15:21.520 --> 02:15:29.040]   Alia G in our discord is saying thanks to leaks of what's app data.
[02:15:29.040 --> 02:15:32.240]   People are looking through WhatsApp data for people named mom.
[02:15:32.240 --> 02:15:39.240]   And and using that information to scam moms, you know, because yeah, I mean,
[02:15:39.240 --> 02:15:40.680]   that makes a lot of sense.
[02:15:40.680 --> 02:15:47.200]   And I just I worry that I've always worried about this.
[02:15:47.200 --> 02:15:50.840]   One of the things I used the radio show for is a way to kind of warn people
[02:15:50.840 --> 02:15:54.240]   against these scams, but it's just it's nonstop nowadays.
[02:15:54.240 --> 02:15:56.400]   I'm going to make it how sensitive to that.
[02:15:56.480 --> 02:15:59.920]   I'm amazing how many attractive women text me and send me WhatsApp.
[02:15:59.920 --> 02:16:01.760]   You know, I just think it's good.
[02:16:01.760 --> 02:16:02.760]   I know.
[02:16:02.760 --> 02:16:07.320]   Yeah, I mean, I just that how I figure they just, you know, I'm irresistible.
[02:16:07.320 --> 02:16:08.040]   What can I say?
[02:16:08.040 --> 02:16:11.440]   Apparently, I maybe have any young women are interested in older men.
[02:16:11.440 --> 02:16:16.520]   And I try to have these conversations with them because you know that there's
[02:16:16.520 --> 02:16:20.080]   cameras and like for what ever really you engage them.
[02:16:20.080 --> 02:16:20.800]   I do.
[02:16:20.800 --> 02:16:22.560]   I try to because it's fascinating.
[02:16:22.560 --> 02:16:23.600]   I know what they're doing.
[02:16:23.600 --> 02:16:32.120]   And weirdly, I have I've been I guess I'm not saying the right things because I weirdly
[02:16:32.120 --> 02:16:34.480]   they disengaged the conversation relatively quickly.
[02:16:34.480 --> 02:16:35.320]   Yeah, it's hard.
[02:16:35.320 --> 02:16:36.920]   It's hard to keep them online.
[02:16:36.920 --> 02:16:44.720]   So this will be this will be a really good thing to turn over to AI bots pretending
[02:16:44.720 --> 02:16:47.320]   to be you and engaging.
[02:16:47.320 --> 02:16:49.920]   Of course, it'll be it'll be a scammer bot.
[02:16:49.920 --> 02:16:50.840]   Right.
[02:16:50.840 --> 02:16:53.720]   And they can, you know, they can talk forever.
[02:16:53.720 --> 02:16:55.440]   Oh, yeah, I love that.
[02:16:55.440 --> 02:16:58.400]   Just never give you the AI bot access to your checking account.
[02:16:58.400 --> 02:16:59.080]   Otherwise.
[02:16:59.080 --> 02:17:04.560]   Now I be sucking it to how often you get the Nigerian money where right?
[02:17:04.560 --> 02:17:07.360]   What a world.
[02:17:07.360 --> 02:17:08.160]   What a world.
[02:17:08.160 --> 02:17:09.520]   What a world we live in.
[02:17:09.520 --> 02:17:10.360]   Let's take a little break.
[02:17:10.360 --> 02:17:11.320]   Finish this thing up.
[02:17:11.320 --> 02:17:12.840]   Great panel.
[02:17:12.840 --> 02:17:13.640]   Love you guys.
[02:17:13.640 --> 02:17:14.720]   It's so good to see you.
[02:17:16.200 --> 02:17:22.640]   I've asked both Christina and and Larry to plug something you your chance to plug
[02:17:22.640 --> 02:17:26.720]   something Dan Gilmore anything you want to promote anything you're up to.
[02:17:26.720 --> 02:17:28.080]   You want to tell the world about?
[02:17:28.080 --> 02:17:32.640]   I'm not quite ready.
[02:17:32.640 --> 02:17:35.240]   I've got a project in the works that I have not.
[02:17:35.240 --> 02:17:43.400]   I'm still trying to raise the research funding for it.
[02:17:43.400 --> 02:17:45.320]   So I can't really talk about it.
[02:17:45.320 --> 02:17:45.960]   Nice.
[02:17:46.520 --> 02:17:52.320]   But I bet if you go to Dan Gilmore.com, D A N G I double L M O R, you'll keep up
[02:17:52.320 --> 02:17:54.840]   on what Dan has been up to.
[02:17:54.840 --> 02:17:56.560]   And I'm sure he'll post it there.
[02:17:56.560 --> 02:17:57.040]   Yes.
[02:17:57.040 --> 02:17:59.520]   I'm shamefully lacks in my blood.
[02:17:59.520 --> 02:18:01.920]   Who is it?
[02:18:01.920 --> 02:18:02.360]   Right.
[02:18:02.360 --> 02:18:06.520]   Who is my my my master on feed is more likely.
[02:18:06.520 --> 02:18:07.000]   OK.
[02:18:07.000 --> 02:18:07.600]   Relevant.
[02:18:07.600 --> 02:18:08.400]   Fair enough.
[02:18:08.400 --> 02:18:13.040]   I only I only use I have a Twitter account still, but that's just to prevent
[02:18:13.680 --> 02:18:16.000]   somebody from taking my username.
[02:18:16.000 --> 02:18:20.480]   If I me too, I actually deleted everything on there.
[02:18:20.480 --> 02:18:23.120]   I keep it so I can still see stuff.
[02:18:23.120 --> 02:18:26.680]   So nobody will be Leo Laport and occasionally I'll DM people because
[02:18:26.680 --> 02:18:27.800]   there's still people.
[02:18:27.800 --> 02:18:30.480]   The only way I can reach them is DMing.
[02:18:30.480 --> 02:18:35.000]   And let me show you if you're on Mastodon, I'm on the on the advanced
[02:18:35.000 --> 02:18:36.080]   web interface right here.
[02:18:36.080 --> 02:18:39.800]   You just type in at Dan Gilmore and there he shows up right there.
[02:18:39.800 --> 02:18:40.920]   There's a lot of other ones.
[02:18:41.360 --> 02:18:46.880]   These are all retweet sites and you could tell because they don't have a icon.
[02:18:46.880 --> 02:18:48.560]   But this one at Mastodon.social.
[02:18:48.560 --> 02:18:50.280]   I think that's the real you.
[02:18:50.280 --> 02:18:51.440]   I'm already following you.
[02:18:51.440 --> 02:18:54.640]   Maybe this would be a good thing for you to add to your mast.
[02:18:54.640 --> 02:18:59.000]   I like to follow people like Dan who actually post interesting stuff.
[02:18:59.000 --> 02:19:01.720]   I follow you too, Larry and you too, Christina.
[02:19:01.720 --> 02:19:03.920]   You've been on you've been on Mastodon quite a bit.
[02:19:03.920 --> 02:19:05.960]   Yeah, I like you.
[02:19:05.960 --> 02:19:06.560]   Yeah, I like you.
[02:19:06.560 --> 02:19:07.440]   Yeah, I like Mastodon.
[02:19:07.440 --> 02:19:08.120]   I'm a Mastodon.
[02:19:08.120 --> 02:19:09.120]   I'm a blue sky.
[02:19:09.120 --> 02:19:10.280]   Are you a blue sky too?
[02:19:10.280 --> 02:19:10.720]   I think I.
[02:19:10.720 --> 02:19:11.360]   You're a blue sky.
[02:19:11.360 --> 02:19:11.760]   Yeah.
[02:19:11.760 --> 02:19:14.280]   Yeah, there's no underscore on blue skies.
[02:19:14.280 --> 02:19:16.440]   I'm just filmgirl.p sky.social.
[02:19:16.440 --> 02:19:22.360]   But I, yeah, I probably never going to be able to leave Twitter to be honest.
[02:19:22.360 --> 02:19:28.040]   But I mean, until it literally falls apart, which might be just matter of months, we'll see.
[02:19:28.040 --> 02:19:31.520]   But yeah, I've been spending a lot more time on the other platforms, which is really great.
[02:19:31.520 --> 02:19:34.720]   Why is that your Mastodon handle?
[02:19:34.720 --> 02:19:37.920]   Film underscore girl at Mastodon.social.
[02:19:37.920 --> 02:19:38.880]   OK.
[02:19:39.240 --> 02:19:43.960]   So again, just go to your Mastodon instance and at film underscore girl.
[02:19:43.960 --> 02:19:46.440]   And it should be able to find that pretty quickly.
[02:19:46.440 --> 02:19:47.200]   That's very, very, very far.
[02:19:47.200 --> 02:19:50.040]   Yeah, I've found that's the easiest way to follow people.
[02:19:50.040 --> 02:19:50.360]   Yeah.
[02:19:50.360 --> 02:19:58.160]   And I'm following you on blue sky to I, I, I still out the jury still out on blue sky.
[02:19:58.160 --> 02:19:59.960]   It's gotten a lot like Twitter.
[02:19:59.960 --> 02:20:02.280]   My fear here's my fear.
[02:20:02.280 --> 02:20:08.520]   You know, the whole idea of blue skies is going to be federated just like Mastodon is, which is a good thing, I think.
[02:20:09.160 --> 02:20:14.960]   We've learned from the fact that Twitter could be bought by a crazed evil genius billionaire.
[02:20:14.960 --> 02:20:20.800]   That it's probably good not to be centralized into a single company's social.
[02:20:20.800 --> 02:20:29.080]   But the problem I have at this point is because they haven't yet set up Federation and there's more than 100,000 people at the main blue sky instance.
[02:20:29.080 --> 02:20:33.680]   Federation maybe just, you know, dead on arrival with blue sky.
[02:20:33.680 --> 02:20:35.600]   I think it depends.
[02:20:35.600 --> 02:20:42.080]   Because I mean, like, I think that that I think you have a good default instance, then it can be good.
[02:20:42.080 --> 02:20:46.600]   I think that the way that search works, if search can work around, I mean, they're still trying to figure out the Federation aspects.
[02:20:46.600 --> 02:20:51.840]   But like if search can work across federations that you're part of, and that would be a big improvement, that's one of my biggest.
[02:20:51.840 --> 02:20:54.120]   I understand it's there for ideological reasons.
[02:20:54.120 --> 02:20:55.320]   I disagree with those reasons.
[02:20:55.320 --> 02:21:01.080]   I'm a Mastodon, but that has been one of my biggest kind of problems with Mastodon is the fact that search isn't there.
[02:21:01.120 --> 02:21:07.000]   And so, you know, like that, there are some clients that will cash things and do it, but that's a hack.
[02:21:07.000 --> 02:21:10.440]   And so I don't know, I think we'll see.
[02:21:10.440 --> 02:21:22.520]   I like the idea of being able to kind of exist in two different spaces because if that's the case, like, I'm not going to lose out on the benefits of being on the main instance, I wouldn't mind having my own server, which I thought I was doing with Mastodon.
[02:21:22.520 --> 02:21:25.120]   Mastodon, I can't do that because I can't bring in my post over.
[02:21:25.120 --> 02:21:29.400]   So you can migrate your username, but you can't take your post over.
[02:21:29.400 --> 02:21:36.000]   And the team seems very not interested in doing that at all, which is frustrating.
[02:21:36.000 --> 02:21:42.400]   But I think there are clients already being developed that will bring the post over for Mastodon.
[02:21:42.400 --> 02:21:45.000]   So yeah, Calkey, Calkey will do it.
[02:21:45.000 --> 02:21:57.040]   So there are some forks that do it, but other, but in terms of like the core Mastodon project, at least last of my checked, which was like two weeks ago, because people keep yelling at me about it because I keep yelling about it.
[02:21:57.200 --> 02:22:01.440]   They're like, no, you're wrong. And I'm like, I've read all the PRs and I've read the stuff.
[02:22:01.440 --> 02:22:03.160]   They don't care.
[02:22:03.160 --> 02:22:04.160]   So I should.
[02:22:04.160 --> 02:22:05.360]   I should agree.
[02:22:05.360 --> 02:22:05.880]   I agree.
[02:22:05.880 --> 02:22:12.040]   Has anybody written a really friendly user of guide to Mastodon for people who, you know, need to learn the basics of it?
[02:22:12.040 --> 02:22:13.680]   There are a couple.
[02:22:13.680 --> 02:22:16.600]   I'll have to dig them out, but same.
[02:22:16.600 --> 02:22:19.560]   It's and it's a really rapidly.
[02:22:19.560 --> 02:22:24.440]   It's a moving target in a lot of ways.
[02:22:25.200 --> 02:22:32.720]   I think I'll have to go find it, but I would go to Mastodon.help to start.
[02:22:32.720 --> 02:22:35.840]   This has been around since the beginning of Mastodon.
[02:22:35.840 --> 02:22:43.240]   It's it's look, it's not completely user friendly because there's a lot of information, but it does describe everything.
[02:22:43.240 --> 02:22:47.960]   And there have been a number of attempts to make videos that explain it.
[02:22:47.960 --> 02:22:52.360]   Honestly, don't tell anybody, but I don't want to make it that easy.
[02:22:53.080 --> 02:22:56.680]   I kind of like it that there's a barrier to entry.
[02:22:56.680 --> 02:23:05.120]   In fact, we are on Mastodon instance, which is a Twitter social is you have to apply, you know, you don't get in automatically.
[02:23:05.120 --> 02:23:10.680]   You have to say, I want to, you know, and you have to know something about Twitter, or I'm not going to let you in.
[02:23:10.680 --> 02:23:15.840]   And I think by doing that, we get a higher quality of people in there.
[02:23:15.840 --> 02:23:18.520]   Now, I admit there needs a perfect use case.
[02:23:18.520 --> 02:23:19.040]   Yeah.
[02:23:19.040 --> 02:23:22.960]   But you're describing it really ideal the way it ought to work.
[02:23:23.120 --> 02:23:27.520]   Yeah, but I also understand that, you know, there are movements, the Arab Spring,
[02:23:27.520 --> 02:23:35.400]   Black Twitter and so forth, where ease of entry is very important to getting to building critical mass in a community.
[02:23:35.400 --> 02:23:37.320]   And I understand that.
[02:23:37.320 --> 02:23:40.320]   And I hope solutions come along for that.
[02:23:40.320 --> 02:23:47.520]   You know, Mastodon, I could be a little prickly, but remember, Mastodon is just a part of the Fediverse, the activity pub based Fediverse.
[02:23:47.640 --> 02:23:51.840]   Kalki is another Fediverse clone.
[02:23:51.840 --> 02:23:54.520]   Uh, Miss Key was the Kalki's based on Miss Key.
[02:23:54.520 --> 02:23:55.680]   Then there are a lot of them.
[02:23:55.680 --> 02:23:59.000]   There's Pixel Fed, there's a, I can throw my, I can go on and on.
[02:23:59.000 --> 02:24:02.560]   So I think that these underlying protocols are what's key.
[02:24:02.560 --> 02:24:12.560]   I think AT proto, the underlying protocol for blue sky is, in fact, I note that you maintain a, a GitHub stars collection.
[02:24:12.560 --> 02:24:13.640]   Yes.
[02:24:13.800 --> 02:24:18.720]   For both, yeah, for both that I blue sky goodness and I've masked on goodness.
[02:24:18.720 --> 02:24:19.440]   Oh, that's awesome.
[02:24:19.440 --> 02:24:19.800]   Those.
[02:24:19.800 --> 02:24:32.480]   So I'm trying to basically have collections of repos that I find on GitHub of interesting things related to both projects and Mastodon should probably rename activity pub goodness, but honestly, like, mast on this for better work.
[02:24:32.480 --> 02:24:39.600]   Everybody knows the name Mastodon, but I kind of, I, I, I resent that a little bit because Mastodon is just one example.
[02:24:39.600 --> 02:24:40.040]   I agree.
[02:24:40.040 --> 02:24:43.160]   I do with activity pub.
[02:24:43.160 --> 02:24:45.760]   In the Fediverse is it can be a lot of different things.
[02:24:45.760 --> 02:24:47.400]   So no, I fully agree.
[02:24:47.400 --> 02:24:48.000]   I fully agree.
[02:24:48.000 --> 02:24:49.560]   I mean, I am too.
[02:24:49.560 --> 02:24:54.840]   I mean, look, I, I, I really think that that at proto, the at protocol is very interesting.
[02:24:54.840 --> 02:24:56.280]   And again, they're still working on things.
[02:24:56.280 --> 02:25:07.520]   But I think a lot of the decisions that they've made, again, the most primary one for me being just the way that account portability works, I think is superior to how it works in activity.
[02:25:07.520 --> 02:25:08.520]   Absolutely.
[02:25:08.520 --> 02:25:11.360]   And, and, and, and they were very thoughtful about that.
[02:25:11.360 --> 02:25:12.840]   That's the chief difference, frankly.
[02:25:12.840 --> 02:25:14.640]   It is.
[02:25:14.640 --> 02:25:19.880]   And, and, and people are working on bridges to make the two work better together, which I think is really, really good.
[02:25:19.880 --> 02:25:25.720]   Um, micro dot blog, uh, my, my friend, Manton's project that he's been working on for God.
[02:25:25.720 --> 02:25:29.920]   I don't remember how long ago the, the kickstarter was, but I backed it on kickstarter.
[02:25:29.920 --> 02:25:34.560]   Um, and, and, um, you know, because I wanted to support what it was doing.
[02:25:34.560 --> 02:25:35.920]   I don't even have an act at the county.
[02:25:35.920 --> 02:25:37.400]   It's a Fedvers client.
[02:25:37.400 --> 02:25:41.680]   You could post on a Mastodon via Mike and Blue Sky and Blue Sky.
[02:25:41.680 --> 02:25:43.480]   Oh, I didn't know he had a T proto.
[02:25:43.480 --> 02:25:44.080]   Yeah.
[02:25:44.080 --> 02:25:44.280]   Yeah.
[02:25:44.280 --> 02:25:48.920]   He added that several weeks ago, like very early on, which I thought was really interesting.
[02:25:48.920 --> 02:25:59.600]   And so that I, I, to me, that's kind of like the perfect sort of things you can cross post to Mastodon, Tumblr, Blue Sky, medium, LinkedIn, and Flickr, which is great.
[02:25:59.600 --> 02:26:05.040]   Um, and, and I love what, you know, what he and, and that team is, is building.
[02:26:05.520 --> 02:26:16.120]   Um, because I think to your point, you know, it's, it's easy to kind of do the, the Twitter clone, but if you can build these different types of experiences and stuff, then you really have something.
[02:26:16.120 --> 02:26:20.040]   Um, and for me, having that control over my user data is really important.
[02:26:20.040 --> 02:26:29.320]   Well, I'm going to say something that hopefully doesn't get me kicked off of a future guest on Twitch, but based on the conversation that I just heard, this is clearly not ready for prime time.
[02:26:29.320 --> 02:26:31.640]   And I, I say that was a great love and respect.
[02:26:31.640 --> 02:26:32.040]   Yeah.
[02:26:32.040 --> 02:26:37.160]   And for the three of you, the reality is the people who I write for wouldn't have a clue.
[02:26:37.160 --> 02:26:49.440]   Um, and if they're listening to this aspect of part of the, you know, they're going to be in translation and that somehow if Mastodon is to catch on or Blue Sky or anything else, it has to be made much more simple.
[02:26:49.440 --> 02:26:51.280]   Uh, it's not hard to sign up.
[02:26:51.280 --> 02:26:51.840]   I admit.
[02:26:51.840 --> 02:26:53.800]   Well, that's what Blue Sky is right now.
[02:26:53.800 --> 02:26:57.280]   Blue Sky right now to be just like Twitter is just like Twitter.
[02:26:57.320 --> 02:27:01.520]   And the thing is, and that team's goal is a little bit different than Mastodon.
[02:27:01.520 --> 02:27:08.320]   I'm not going to try to say one is better than the other because they're different, but they really don't want you to have to think about the Federation properties about it at all.
[02:27:08.320 --> 02:27:12.640]   And that was interesting for me when I thought, which, which Federation, which do I do?
[02:27:12.640 --> 02:27:12.960]   I do.
[02:27:12.960 --> 02:27:13.440]   I do.
[02:27:13.440 --> 02:27:13.960]   I do.
[02:27:13.960 --> 02:27:18.240]   No, it's, it's a very, and unfortunately a lot of people say it doesn't matter, but it does matter.
[02:27:18.240 --> 02:27:18.560]   Yeah.
[02:27:18.560 --> 02:27:22.560]   And so, um, I think that that is the biggest barrier to entry for Mastodon.
[02:27:22.800 --> 02:27:37.360]   But I do think that that is, I think the Blue Sky, which is still in private beta has going forward is that I was, I would venture to say that probably that a Mastodon, a vast majority of users know and care about the Federation aspects.
[02:27:37.360 --> 02:27:38.320]   That's important to them.
[02:27:38.320 --> 02:27:43.440]   I would say that I'm Blue Sky because I'm active on both the vast majority do not care.
[02:27:43.440 --> 02:27:44.800]   And so.
[02:27:44.800 --> 02:27:46.280]   Yeah.
[02:27:46.280 --> 02:27:46.800]   Can we keep them?
[02:27:46.800 --> 02:27:47.520]   I apologize.
[02:27:47.520 --> 02:27:48.080]   I started.
[02:27:48.080 --> 02:27:49.000]   I know you're fine.
[02:27:49.000 --> 02:27:52.720]   Um, keep in mind that.
[02:27:52.880 --> 02:28:02.720]   It's really since November that anyone beyond a very small community has cared at all about this.
[02:28:02.720 --> 02:28:07.160]   The progress in just a few months has been astonishing.
[02:28:07.160 --> 02:28:18.680]   And we're going to see more of the fact that Mozilla has set up an instance and they clearly plan for it to be or are working on it being something that will be.
[02:28:19.280 --> 02:28:24.400]   Yeah, usable in a fairly easy way by people.
[02:28:24.400 --> 02:28:27.920]   They've, they're, they're carefully managing that process.
[02:28:27.920 --> 02:28:33.000]   Um, it is quite a development, but you're not wrong, Larry.
[02:28:33.000 --> 02:28:35.720]   The thing is that this is the moment.
[02:28:35.720 --> 02:28:45.480]   This is really the moment when some leverage applied wisely could produce incredible downstream effects.
[02:28:45.480 --> 02:28:50.720]   And I'm, I've been begging my philanthropist friends to jump in.
[02:28:50.720 --> 02:28:51.520]   Yes.
[02:28:51.520 --> 02:29:05.480]   And to, to basically think about the de, the re decentralized internet as a mission and to look for ways to help that happen.
[02:29:05.480 --> 02:29:15.440]   So if you're, if you have a lot of money listening to this and are at a foundation or a charity, please think about
[02:29:15.440 --> 02:29:22.160]   this. This is, this is the moment when you could put in what's a relatively small, uh,
[02:29:22.160 --> 02:29:26.120]   uh, donation and, and, and investment.
[02:29:26.120 --> 02:29:28.440]   Somebody like Craig Newmark.
[02:29:28.440 --> 02:29:29.000]   Amazing.
[02:29:29.000 --> 02:29:29.200]   Yeah.
[02:29:29.200 --> 02:29:36.000]   But there are some really good people out there who care a lot about these things, uh, who could really make a big, uh, big, big difference.
[02:29:36.000 --> 02:29:40.320]   So you see another thing, Elon Musk has really changed the world, hasn't he?
[02:29:40.320 --> 02:29:40.680]   Yeah.
[02:29:40.680 --> 02:29:48.920]   And we're, we're so grateful to Elon for having let us know that a centralized social network is a bad idea.
[02:29:48.920 --> 02:29:50.400]   Um, I'm hopeful.
[02:29:50.400 --> 02:29:50.600]   Yeah.
[02:29:50.600 --> 02:29:52.360]   I know you've stayed on Twitter, Christina.
[02:29:52.360 --> 02:29:53.560]   And I, yeah.
[02:29:53.560 --> 02:30:04.600]   And I'm hopeful that Twitter will get through this because really it is, it has been a since 2006, a very valuable part of the conversation.
[02:30:04.600 --> 02:30:07.920]   Uh, and I, I agree if it's lost.
[02:30:07.920 --> 02:30:10.240]   And I don't think it's necessarily over.
[02:30:10.240 --> 02:30:17.520]   I think Elon, it seems highly likely, uh, you know, uh, was it fidelity just downgraded?
[02:30:17.520 --> 02:30:18.440]   Uh, yeah.
[02:30:18.440 --> 02:30:18.920]   Yeah.
[02:30:18.920 --> 02:30:26.560]   They declared their investment in Twitter, uh, not a total loss, but they basically said it's only really worth about 15 billion, which is a third.
[02:30:26.560 --> 02:30:26.920]   Right.
[02:30:26.920 --> 02:30:28.120]   What do you want to do?
[02:30:28.120 --> 02:30:29.280]   The third, yeah.
[02:30:29.280 --> 02:30:37.240]   And he, it was just, you know, ha, it was like, I guess 50% less than what he claimed or even he thought it was only worth half.
[02:30:37.240 --> 02:30:38.200]   Yeah.
[02:30:38.200 --> 02:30:41.840]   I think it's worth half and then it's, it's really worth, you know, half of that.
[02:30:41.840 --> 02:30:48.920]   So, you know, but it's also worth what snap is worth and what, uh, Pinterest are worth, which honestly is probably fair.
[02:30:48.920 --> 02:30:49.360]   That's been right.
[02:30:49.360 --> 02:30:51.440]   No, but that's also probably, that's also probably fair.
[02:30:51.440 --> 02:30:51.680]   Yeah.
[02:30:51.680 --> 02:31:02.680]   Um, but I mean, you know, like my, my expectation at this point, and I don't want to make predictions because I've been wrong on all of them would be the, well, he will, he will sell at some point.
[02:31:02.680 --> 02:31:05.080]   Either sell or the banks will take it.
[02:31:05.080 --> 02:31:05.800]   Yes.
[02:31:05.800 --> 02:31:07.360]   I just don't know who buys it at this point.
[02:31:07.360 --> 02:31:07.960]   It's the real thing.
[02:31:07.960 --> 02:31:08.360]   Yeah.
[02:31:08.360 --> 02:31:09.240]   I just don't who buys it.
[02:31:09.240 --> 02:31:13.360]   So you want to make a small fortune when he felt it because he bought it with a big fortune.
[02:31:13.360 --> 02:31:14.240]   Yes.
[02:31:14.240 --> 02:31:14.720]   That's right.
[02:31:14.720 --> 02:31:18.560]   Actually, he borrowed 13 billion from the banks.
[02:31:18.560 --> 02:31:27.520]   I think that's what's going to end up being the value of Twitter and that the banks will just, um, take it over when it goes bankrupt because I think that's what's going to happen.
[02:31:27.520 --> 02:31:30.480]   It's clearly not had in positive direction.
[02:31:30.480 --> 02:31:31.680]   So I was talking about it.
[02:31:31.680 --> 02:31:32.240]   Go ahead.
[02:31:32.560 --> 02:31:39.360]   If it matter what instance, I mean, I know you want us to join Twitter and, but, but from a standpoint of the user, does it matter that much?
[02:31:39.360 --> 02:31:42.520]   Which instance or community they join on, on Mastodon?
[02:31:42.520 --> 02:31:52.720]   You can follow anybody on any other Mastodon instance unless the instance you're on has, for some reason, blocked it or as we call it, defederated it.
[02:31:52.720 --> 02:32:00.640]   Uh, I have defederated a number of, in fact, you can even see when you sign up or go to an instance, what other instances are blocked.
[02:32:01.040 --> 02:32:11.120]   Uh, I defederated for instance, Russia just created its own Mastodon instance called, um, probta.ru.
[02:32:11.120 --> 02:32:18.280]   Uh, I instantly defederated it because it's clear that that was going to be a massive source of disinformation pumping into the network.
[02:32:18.280 --> 02:32:30.680]   This is by the way, one of the strengths of Federation is I as a, as a person who runs my own Mastodon server can say, no, nothing from probta.ru on my server, but Mastodon.
[02:32:30.680 --> 02:32:40.040]   But social, all of the normal big servers, if just as I did, I entered in your name, Dan and I found you and I added you and was no trouble at all.
[02:32:40.040 --> 02:32:44.200]   So I will see you in my home feed of people I follow.
[02:32:44.200 --> 02:32:48.760]   But what's cool about having an instance, uh, there's a local timeline.
[02:32:48.760 --> 02:32:53.440]   The people on our Twitter, social local timeline are all Twitter listeners.
[02:32:53.440 --> 02:32:58.360]   They're all, you know, which makes it, it has a community of its very own.
[02:32:58.560 --> 02:33:01.120]   So this is the home, which is people I follow.
[02:33:01.120 --> 02:33:03.080]   This is the local timeline.
[02:33:03.080 --> 02:33:04.840]   I'm running the advanced interface.
[02:33:04.840 --> 02:33:05.640]   Not everybody runs that.
[02:33:05.640 --> 02:33:07.080]   A lot of people like it to look like Twitter.
[02:33:07.080 --> 02:33:09.840]   And this is the, what they call the federated timeline.
[02:33:09.840 --> 02:33:15.960]   That's interesting because that's everybody followed by anybody on the local server.
[02:33:15.960 --> 02:33:20.880]   So that also has a friends of friends, local con context to it.
[02:33:20.880 --> 02:33:28.320]   So yes, the server that you follow definitely impacts what the locals look like, what the
[02:33:28.320 --> 02:33:32.640]   federated timeline looks like, but you're in your, can I be at Larry
[02:33:32.640 --> 02:33:35.120]   Maggot on, on your server as well as.
[02:33:35.120 --> 02:33:39.160]   No, not, not unless you want to maintain two accounts because there, there,
[02:33:39.160 --> 02:33:41.760]   there's nothing to stop you from doing that.
[02:33:41.760 --> 02:33:45.200]   I have multiple accounts, but you want to have, you could see where out Larry
[02:33:45.200 --> 02:33:45.480]   Maggot.
[02:33:45.480 --> 02:33:49.920]   I want, you want to have one primary account, uh, for discoverability, but also
[02:33:49.920 --> 02:33:52.680]   because you, it's non portable, the content you post.
[02:33:52.680 --> 02:33:58.280]   So you really kind of, if you can light at the right place, uh, and stay there, um,
[02:33:58.280 --> 02:33:59.680]   you know, micro blogs interesting.
[02:33:59.680 --> 02:34:03.600]   I don't, I, I'm, uh, been paying three bucks a month forever to met on.
[02:34:03.600 --> 02:34:06.560]   So I really should take a look at using micro blog to post.
[02:34:06.560 --> 02:34:11.640]   Uh, I'm at Leo at Leo dot social is my Mastodon or rather my
[02:34:11.640 --> 02:34:14.000]   Fediverse name at, uh, microblog.
[02:34:14.000 --> 02:34:16.360]   And I'm at Leo at twit.social.
[02:34:16.360 --> 02:34:20.240]   So I have those too, but I only post on the twit.social one.
[02:34:20.240 --> 02:34:21.000]   I don't know.
[02:34:21.000 --> 02:34:25.400]   I, I think let a thousand flowers bloom has happened.
[02:34:26.600 --> 02:34:30.480]   Uh, does there have to be one that we all follow?
[02:34:30.480 --> 02:34:32.000]   Well, there's an advantage to that.
[02:34:32.000 --> 02:34:34.120]   There's also a disadvantage to it.
[02:34:34.120 --> 02:34:39.720]   Well, I like the idea that, you know, you can decide, you know, you don't want
[02:34:39.720 --> 02:34:41.560]   dot R you on your, on your server.
[02:34:41.560 --> 02:34:42.960]   And that's your right to do that.
[02:34:42.960 --> 02:34:44.920]   You're not really censoring them.
[02:34:44.920 --> 02:34:48.480]   You're simply saying that they're, I'm not saying they can't exist.
[02:34:48.480 --> 02:34:48.920]   Right.
[02:34:48.920 --> 02:34:49.400]   You are.
[02:34:49.400 --> 02:34:51.200]   You're not saying that they can't exist.
[02:34:51.200 --> 02:34:52.960]   You're saying they can't exist on my server.
[02:34:52.960 --> 02:34:53.280]   Right.
[02:34:53.440 --> 02:34:59.200]   So if you want to go somewhere else and get them, his, his servers, his living
[02:34:59.200 --> 02:35:01.600]   room, he's not going to invite thugs in.
[02:35:01.600 --> 02:35:02.120]   Right.
[02:35:02.120 --> 02:35:02.560]   Yeah.
[02:35:02.560 --> 02:35:07.840]   And I blacked Nazis and I block, you know, you know, if there are griefing
[02:35:07.840 --> 02:35:11.600]   servers, which there are in the Fediverse, those, those are block, I block
[02:35:11.600 --> 02:35:13.720]   about 20 servers.
[02:35:13.720 --> 02:35:16.280]   And whenever there's a problem, I'll consider it and so forth.
[02:35:16.280 --> 02:35:21.640]   But so far, because, and by the way, I'm one person, I might spend 15, 20 minutes
[02:35:21.640 --> 02:35:23.880]   a day at most moderating.
[02:35:23.880 --> 02:35:26.440]   I don't have to spend a lot of time because we have a tight-knit group.
[02:35:26.440 --> 02:35:32.880]   Um, there is a, there are very good, uh, tools for an individual to block a feed
[02:35:32.880 --> 02:35:34.600]   or block a server themselves.
[02:35:34.600 --> 02:35:38.760]   So if I didn't block Profta, but you wanted to, you could do that for yourself
[02:35:38.760 --> 02:35:41.000]   on your, on your feed.
[02:35:41.000 --> 02:35:42.960]   I mean, I think there's pretty good tools.
[02:35:42.960 --> 02:35:46.560]   There's no quote tweet and there's no global search.
[02:35:47.560 --> 02:35:53.120]   I think the, the lack of quote tweets, which is less crucial, but the lack of
[02:35:53.120 --> 02:35:54.520]   search is a big deal to me.
[02:35:54.520 --> 02:35:54.840]   Yeah.
[02:35:54.840 --> 02:35:56.800]   That, that's got to be fixed.
[02:35:56.800 --> 02:36:00.920]   And it's, and it's really improving quickly though.
[02:36:00.920 --> 02:36:06.200]   And the people are doing forks that are still at it completely interoperable, that
[02:36:06.200 --> 02:36:07.800]   do everything I want.
[02:36:07.800 --> 02:36:13.560]   I may, I may end up on Calkey or something like that, but this is really a wonderful,
[02:36:13.840 --> 02:36:18.240]   uh, fervent, the going on and, and, and, yeah.
[02:36:18.240 --> 02:36:18.800]   Yeah.
[02:36:18.800 --> 02:36:21.000]   I think something important is happening.
[02:36:21.000 --> 02:36:25.720]   Larry, if you want some, some personal advice on it, I've been doing
[02:36:25.720 --> 02:36:29.960]   Mastodon now for six months and it's pretty great.
[02:36:29.960 --> 02:36:35.600]   And then I have a third, the number of followers I had at Twitter and 10 times
[02:36:35.600 --> 02:36:36.480]   the engagement.
[02:36:36.480 --> 02:36:39.440]   I had a serious engagement.
[02:36:39.440 --> 02:36:41.840]   It's, it's, it's astonishing.
[02:36:42.480 --> 02:36:43.440]   Yeah, I'm on Newfy.
[02:36:43.440 --> 02:36:47.280]   I just picked out a kind of a random, but, um, that's fine.
[02:36:47.280 --> 02:36:48.280]   I haven't done much yet.
[02:36:48.280 --> 02:36:48.840]   Yeah.
[02:36:48.840 --> 02:36:50.800]   Just stick, stay there.
[02:36:50.800 --> 02:36:52.280]   That's your community.
[02:36:52.280 --> 02:36:55.800]   Uh, and, uh, it's a bunch of news people.
[02:36:55.800 --> 02:36:57.400]   So I think that's fine.
[02:36:57.400 --> 02:36:58.400]   Yeah.
[02:36:58.400 --> 02:37:00.680]   Uh, how about Reddit?
[02:37:00.680 --> 02:37:05.760]   Uh, as long as we're talking social, uh, Reddit, which is owned by the massive
[02:37:05.760 --> 02:37:09.520]   magazine publisher, Conde Nast, the people who published the New Yorker, Vanity Fair,
[02:37:10.120 --> 02:37:13.920]   a bunch of other magazines, uh, has a, has announced that they're going to start
[02:37:13.920 --> 02:37:17.920]   charging just as Twitter does for their API access.
[02:37:17.920 --> 02:37:24.280]   And just as Twitter had, uh, Reddit has a number, quite a few, a third party
[02:37:24.280 --> 02:37:31.280]   clients that don't show the ads, uh, and allow people to change their
[02:37:31.280 --> 02:37:32.440]   experience on Reddit.
[02:37:32.440 --> 02:37:34.280]   This is one of them Apollo.
[02:37:34.280 --> 02:37:35.280]   This is for iOS.
[02:37:35.280 --> 02:37:36.680]   This is what I use on.
[02:37:36.680 --> 02:37:37.720]   Yep.
[02:37:37.840 --> 02:37:38.840]   Really, really nice.
[02:37:38.840 --> 02:37:41.280]   And the developer is very engaged.
[02:37:41.280 --> 02:37:46.840]   Uh, he encourages donations, but it's free, very powerful.
[02:37:46.840 --> 02:37:48.640]   There's a paid version too.
[02:37:48.640 --> 02:37:53.760]   So it, um, it, yeah, there, there's a subscription, but he is, but it has, like
[02:37:53.760 --> 02:37:56.840]   I think a million and a half users and many of them are free.
[02:37:56.840 --> 02:38:01.640]   And then the people who do pay what he's charging, um, the new changes that
[02:38:01.640 --> 02:38:05.840]   Reddit is going to introduce will basically make him make it not sustainable, even
[02:38:05.840 --> 02:38:06.400]   charging.
[02:38:06.400 --> 02:38:06.840]   It's.
[02:38:06.960 --> 02:38:11.760]   So his name Christian Selig, he, uh, was on a call now, unlike Elon, who just
[02:38:11.760 --> 02:38:17.320]   cut off third party access and at first denied it, then said they broke the rules
[02:38:17.320 --> 02:38:17.960]   then said, all right.
[02:38:17.960 --> 02:38:22.440]   Yeah, we're cutting it off, which is his, by the way, it's his privilege.
[02:38:22.440 --> 02:38:24.480]   He didn't do it very nicely, but it's his privilege.
[02:38:24.480 --> 02:38:29.400]   Cause it does undermine, you know, the, the, the financial structure of Twitter
[02:38:29.400 --> 02:38:31.680]   because ads are what pays for Twitter.
[02:38:32.080 --> 02:38:38.160]   So I, it's certainly within call, Condene S, uh, a power and, and it's appropriate
[02:38:38.160 --> 02:38:39.600]   for them to say this.
[02:38:39.600 --> 02:38:40.760]   He talked to them.
[02:38:40.760 --> 02:38:44.320]   They have been unlike Leon calling everybody talking to everybody.
[02:38:44.320 --> 02:38:49.680]   He talked to them and they explained, well, I think it's $12,000 for 500,000
[02:38:49.680 --> 02:38:51.000]   access as I can remember what it was.
[02:38:51.000 --> 02:38:55.600]   He did some math and based on the number of people that use Apollo, he said,
[02:38:55.600 --> 02:38:59.960]   it's going to cost me $20 million a year to continue my app.
[02:39:01.520 --> 02:39:02.800]   Uh, huge mistake.
[02:39:02.800 --> 02:39:04.240]   It's not sustainable.
[02:39:04.240 --> 02:39:10.080]   So if Twitter does this, it will put, uh, Apollo out of business, Joey
[02:39:10.080 --> 02:39:12.440]   out of business, a lot of third party apps.
[02:39:12.440 --> 02:39:16.400]   There's already a Mac app that I was using called a stellar that I paid for
[02:39:16.400 --> 02:39:21.200]   actually that has already announced that they are sunsetting it because they had
[02:39:21.200 --> 02:39:22.840]   kind of a free, free meal model.
[02:39:22.840 --> 02:39:26.400]   And they basically have said that they, they're not making enough money to be
[02:39:26.400 --> 02:39:28.520]   profitable to change the upcoming changes.
[02:39:29.000 --> 02:39:33.440]   Because like I personally don't have a problem with them charging for the API.
[02:39:33.440 --> 02:39:38.680]   I think the pricing is insane, um, especially given the fact that so many of
[02:39:38.680 --> 02:39:42.240]   their power users who do a lot of free labor for them, like this is unlike
[02:39:42.240 --> 02:39:46.600]   Twitter in the sense that most of the things that make Reddit work are the,
[02:39:46.600 --> 02:39:49.920]   the admin or the mods rather that are not paid.
[02:39:49.920 --> 02:39:54.920]   And many of them rely almost entirely on third party tools because the regular
[02:39:54.920 --> 02:39:57.640]   Reddit tools are not equipped for their needs.
[02:39:57.960 --> 02:40:05.480]   And, um, and so it raises an interesting question because yes, they own Reddit
[02:40:05.480 --> 02:40:05.880]   continent.
[02:40:05.880 --> 02:40:07.360]   As does they can monetize it anyway.
[02:40:07.360 --> 02:40:10.840]   They choose yet the content of Reddit, just like the content of Twitter is
[02:40:10.840 --> 02:40:12.360]   created by its users for free.
[02:40:12.360 --> 02:40:16.720]   We, we post on Reddit, uh, to, and that's what makes Reddit exist.
[02:40:16.720 --> 02:40:21.040]   And if users can't use third party apps, many users distraught, there is a boycott,
[02:40:21.040 --> 02:40:26.840]   um, or not a boycott exactly a protest that's going to happen, uh, I think next
[02:40:26.840 --> 02:40:34.280]   month, uh, where a lot of subreddits, the, the groups on Reddit will go dark.
[02:40:34.280 --> 02:40:35.600]   Next week, I think it's the next week.
[02:40:35.600 --> 02:40:35.960]   Okay.
[02:40:35.960 --> 02:40:36.320]   Sorry.
[02:40:36.320 --> 02:40:36.600]   Yeah.
[02:40:36.600 --> 02:40:39.920]   We'll go dark for several days and that will be interesting.
[02:40:39.920 --> 02:40:43.240]   That might, that might show how important this is to people.
[02:40:43.240 --> 02:40:50.520]   Well, I, I am the one benefit of all of these things is that we are getting
[02:40:51.240 --> 02:41:00.440]   schooled, uh, in the kind of nastiest, nastiest possible way about the dangers of centralized
[02:41:00.440 --> 02:41:11.640]   social media, corporate owned and in, you know, bad guy owned in one case, uh, social media,
[02:41:11.640 --> 02:41:18.000]   making capricious and, and, and wildly counterproductive moves.
[02:41:18.720 --> 02:41:26.520]   Uh, and I think maybe continent has this expecting that this was a ploy that they
[02:41:26.520 --> 02:41:30.640]   could then lower the price that and get people to do it.
[02:41:30.640 --> 02:41:36.440]   I, I think they've made a kind of drastic error in this case.
[02:41:36.440 --> 02:41:38.000]   Now, I agree.
[02:41:38.000 --> 02:41:44.880]   Again, like, in Christian has even said, like he, um, um, um, uh, sassy Q on YouTube did
[02:41:44.880 --> 02:41:46.720]   a great interview with him that went up on YouTube.
[02:41:46.720 --> 02:41:47.560]   I think it was yesterday.
[02:41:47.560 --> 02:41:48.760]   It's like 45 minutes long.
[02:41:48.760 --> 02:41:53.200]   And, you know, he even said that he does think that it's fair that he pays, you know,
[02:41:53.200 --> 02:41:55.600]   some of his profits essentially to Reddit.
[02:41:55.600 --> 02:42:00.440]   Um, but the, the split that they're wanting is, is just not sustainable.
[02:42:00.440 --> 02:42:05.260]   And, you know, to me, you would be much more honest if Reddit would just say we
[02:42:05.260 --> 02:42:07.200]   don't want any third party apps to exist.
[02:42:07.200 --> 02:42:09.280]   I mean, at least Elon did that.
[02:42:09.280 --> 02:42:12.960]   Like I'm not going to give the guy credit for anything, but at least he was honest at
[02:42:12.960 --> 02:42:17.160]   the end and Twitter as a company hadn't wanted third party clients for a long time,
[02:42:17.160 --> 02:42:19.040]   even before, you know, he took it over.
[02:42:19.040 --> 02:42:23.520]   But, but I think in Reddit's case that the problem is, is that, you know, this
[02:42:23.520 --> 02:42:27.920]   really is a big part of how mods use their tools and they'd gone from actively,
[02:42:27.920 --> 02:42:33.000]   you know, encouraging and using and helping out the devs to this change.
[02:42:33.000 --> 02:42:35.720]   And I think that there are some devs who might not want to pay anything and that's
[02:42:35.720 --> 02:42:36.280]   okay.
[02:42:36.280 --> 02:42:37.440]   Some would.
[02:42:37.680 --> 02:42:43.520]   But what's also was so odd to me is that they don't let the ad aspect into their
[02:42:43.520 --> 02:42:46.800]   API, like in Twitter, do the same thing.
[02:42:46.800 --> 02:42:51.080]   I get that you're saying, okay, we're losing out on impressions and this and that.
[02:42:51.080 --> 02:42:51.320]   Okay.
[02:42:51.320 --> 02:42:53.920]   So make that part of the API and make an requirement.
[02:42:53.920 --> 02:42:58.160]   I don't think advertisers would support that for the same reason.
[02:42:58.160 --> 02:43:03.560]   You don't see ads when you're watching YouTube or some other rebroadcast of local
[02:43:03.560 --> 02:43:04.520]   channels.
[02:43:04.520 --> 02:43:08.520]   They don't carry the ads because the advertisers say, no, no, no, that's not part
[02:43:08.520 --> 02:43:09.480]   of what we're paying for.
[02:43:09.480 --> 02:43:11.200]   So I think that's why they don't do that.
[02:43:11.200 --> 02:43:13.280]   I think that's an advertiser mandated.
[02:43:13.280 --> 02:43:14.240]   I agree.
[02:43:14.240 --> 02:43:16.720]   It would be nice to be able to do that, but that's not going to happen.
[02:43:16.720 --> 02:43:17.920]   Advertisers don't.
[02:43:17.920 --> 02:43:21.240]   They want to know exactly where their ads going to be and they want to know how to count.
[02:43:21.240 --> 02:43:29.080]   But the, again, the basic, the basic thing going on is that the central
[02:43:29.080 --> 02:43:36.200]   lies sites rely, they count on these third party developers to, to make help make them
[02:43:36.200 --> 02:43:36.800]   popular.
[02:43:36.800 --> 02:43:43.560]   And then the minute they have sufficient leverage, they cut them off.
[02:43:43.560 --> 02:43:43.840]   Yeah.
[02:43:43.840 --> 02:43:47.160]   And even before, even Twitter did this once before.
[02:43:47.160 --> 02:43:47.600]   Yes.
[02:43:47.600 --> 02:43:48.760]   A twice before.
[02:43:48.760 --> 02:43:49.440]   Twice before.
[02:43:49.440 --> 02:43:53.720]   As, as an investor in one of the companies, Twitter killed way back then.
[02:43:53.720 --> 02:44:00.560]   I was, I was, you know, I got, I've been schooled in this a number of times, but
[02:44:00.560 --> 02:44:05.680]   as a user, the hell with it, I'll just go over and do stuff somewhere else.
[02:44:05.680 --> 02:44:07.640]   I don't need the Twitter fame anymore.
[02:44:07.640 --> 02:44:11.000]   That's the beauty of this is there are many choices now.
[02:44:11.000 --> 02:44:17.120]   June 12th through 15th, here's the R slash R slash Samsung as an example.
[02:44:17.120 --> 02:44:21.520]   This subreddit will be going private June 12th through 15th and protest.
[02:44:22.520 --> 02:44:29.320]   Many, many, uh, subreddits will, um, and I think Conde all sit up and take notice.
[02:44:29.320 --> 02:44:31.760]   Maybe this is a negotiate employee and they'll come back.
[02:44:31.760 --> 02:44:35.000]   In fact, they've already said, Oh, no, no, no, we weren't going to charge them.
[02:44:35.000 --> 02:44:36.880]   We would never charge them 20 million.
[02:44:36.880 --> 02:44:38.840]   We there'd be a way to do this.
[02:44:38.840 --> 02:44:41.120]   So maybe they'll come back with something.
[02:44:41.120 --> 02:44:42.040]   I hope so.
[02:44:42.040 --> 02:44:43.280]   It was, it was interesting.
[02:44:43.280 --> 02:44:46.960]   Um, there was something else for me to look into this and I was just trying to
[02:44:46.960 --> 02:44:48.920]   think of like who has the most expensive APIs.
[02:44:49.320 --> 02:44:54.120]   And I was shocked to look and find discover that IMDB does now finally have an API.
[02:44:54.120 --> 02:45:03.280]   But their pricing is like the most obscene, uh, API pricing I can, I can recall, um,
[02:45:03.280 --> 02:45:03.680]   seeing.
[02:45:03.680 --> 02:45:08.040]   So this is like a relatively new thing I think that they've had, uh, and their
[02:45:08.040 --> 02:45:11.880]   pricing on their API starts at like $450,000 a month.
[02:45:11.880 --> 02:45:13.080]   And that's before usage.
[02:45:13.080 --> 02:45:14.640]   Yeah.
[02:45:14.640 --> 02:45:18.720]   Wow, which is insane because I think, I think the reason I've looked into this was
[02:45:18.720 --> 02:45:22.240]   that somebody, people were complaining about the new max app, which is awful.
[02:45:22.240 --> 02:45:25.240]   And it didn't have a lot of the credits listed and someone said, but why aren't
[02:45:25.240 --> 02:45:27.080]   they just using IMDB API?
[02:45:27.080 --> 02:45:30.960]   And then I, I was like, I had the same question and then I looked into it and I
[02:45:30.960 --> 02:45:33.280]   was like, Oh, because data is as well.
[02:45:33.280 --> 02:45:34.080]   It's very cheap.
[02:45:34.080 --> 02:45:36.080]   And that's why that there.
[02:45:36.080 --> 02:45:37.960]   That's, that's just saying don't use this.
[02:45:37.960 --> 02:45:39.080]   Absolutely.
[02:45:39.080 --> 02:45:40.080]   Oh, 100%.
[02:45:40.080 --> 02:45:40.800]   Yeah.
[02:45:40.800 --> 02:45:41.080]   Yeah.
[02:45:41.080 --> 02:45:46.160]   So, so the essential metadata, uh, and this is for, um, um, movies TV and over
[02:45:46.160 --> 02:45:49.560]   the top is $150,000 plus meter costs.
[02:45:49.560 --> 02:45:56.760]   Uh, and then the complete data set is, um, or excuse me, um, uh, the IMDB
[02:45:56.760 --> 02:46:01.240]   and box on a mojo for movies TV over the top is $400,000 a month plus
[02:46:01.240 --> 02:46:02.000]   meter cost.
[02:46:02.000 --> 02:46:02.360]   Wow.
[02:46:02.360 --> 02:46:02.880]   Pretty dumb.
[02:46:02.880 --> 02:46:05.320]   I am DB is owned by Amazon, right?
[02:46:05.320 --> 02:46:08.200]   Yeah, they've owned them for, I think 20 years now.
[02:46:08.200 --> 02:46:08.440]   Yeah.
[02:46:08.440 --> 02:46:08.960]   At this point.
[02:46:08.960 --> 02:46:09.760]   Yeah.
[02:46:09.760 --> 02:46:11.760]   Used to be, this is a perfect example.
[02:46:12.440 --> 02:46:17.160]   Uh, there were a number of sites like this CDDB was another one where
[02:46:17.160 --> 02:46:22.640]   users created it to put all they knew in there like a wiki or whatever.
[02:46:22.640 --> 02:46:26.440]   And as soon as it grained critical mass, a company came in and bought them.
[02:46:26.440 --> 02:46:29.680]   And all the users said, wait, what hat?
[02:46:29.680 --> 02:46:30.120]   What?
[02:46:30.120 --> 02:46:36.520]   So again, a lesson learned, um, these, you know, we're in late stage capitalism,
[02:46:36.520 --> 02:46:37.040]   folks.
[02:46:37.040 --> 02:46:40.440]   If somebody calls you screaming, they want your money.
[02:46:40.440 --> 02:46:40.720]   Yes.
[02:46:40.720 --> 02:46:41.240]   Okay.
[02:46:41.320 --> 02:46:42.320]   That's just the way it works.
[02:46:42.320 --> 02:46:43.640]   Let me take it.
[02:46:43.640 --> 02:46:47.720]   Fortunately, that I was going to, the TVDB exists as an alternative.
[02:46:47.720 --> 02:46:48.320]   Oh, good.
[02:46:48.320 --> 02:46:49.160]   Yeah.
[02:46:49.160 --> 02:46:50.480]   That's the way to solve this.
[02:46:50.480 --> 02:46:54.040]   Just create a, and in fact, there's already a number of people creating
[02:46:54.040 --> 02:46:57.760]   Reddit, Reddit clones hoping that they can take advantage of this.
[02:46:57.760 --> 02:46:58.160]   We'll see.
[02:46:58.160 --> 02:46:59.680]   Uh, quick break.
[02:46:59.680 --> 02:47:02.560]   And then we're going to wrap things up with a panel that's been very patient,
[02:47:02.560 --> 02:47:03.400]   but awesome.
[02:47:03.400 --> 02:47:05.200]   Awesome.
[02:47:05.200 --> 02:47:07.440]   Our show today brought to you by Cisco.
[02:47:07.920 --> 02:47:13.120]   Maraki, the experts in cloud-based networking for hybrid work, whether your
[02:47:13.120 --> 02:47:18.720]   employers are working at home, at a cabin in the mountains, or a lounge chair at the
[02:47:18.720 --> 02:47:23.400]   beach, I choose the beach, by the way, a cloud managed network provides the same
[02:47:23.400 --> 02:47:26.880]   exceptional work experience, no matter where they are.
[02:47:26.880 --> 02:47:27.680]   And that's what you want.
[02:47:27.680 --> 02:47:30.920]   You may as well roll out the welcome at hybrid work is here to stay.
[02:47:30.920 --> 02:47:33.600]   We, we tried to get people to come back.
[02:47:33.600 --> 02:47:36.880]   No, hybrid work works best in the cloud.
[02:47:37.280 --> 02:47:40.280]   It has its perks for both employees and leaders.
[02:47:40.280 --> 02:47:44.720]   Workers can move faster, deliver better results with a cloud managed network.
[02:47:44.720 --> 02:47:48.840]   Leaders can automate distributed operations, build more sustainable work
[02:47:48.840 --> 02:47:51.600]   spaces, proactively protect the network.
[02:47:51.600 --> 02:47:57.040]   IDG market pulse research, just put out a report that Maraki commissioned it,
[02:47:57.040 --> 02:48:00.360]   highlighting top tier opportunities and supporting hybrid work.
[02:48:00.360 --> 02:48:05.280]   Besides, I mean, the, we workers love it, right?
[02:48:05.280 --> 02:48:07.160]   But hybrid work is a priority also.
[02:48:07.160 --> 02:48:13.040]   This is, this kind of surprised me for 78%, 78% of C-suite executives, because
[02:48:13.040 --> 02:48:15.440]   leaders want to derive collaboration forward.
[02:48:15.440 --> 02:48:20.360]   They, they want to stay on top of or even boost productivity.
[02:48:20.360 --> 02:48:22.120]   Of course, security is always a concern.
[02:48:22.120 --> 02:48:24.560]   Hybrid work does have its challenges.
[02:48:24.560 --> 02:48:27.120]   The IDG report raised a red flag about security.
[02:48:27.120 --> 02:48:34.480]   48% of leaders report cybersecurity threats as a primary obstacle to improving
[02:48:34.480 --> 02:48:35.600]   workforce experiences.
[02:48:35.600 --> 02:48:36.560]   The workforce wants it.
[02:48:36.560 --> 02:48:37.480]   How do we secure it?
[02:48:37.480 --> 02:48:41.520]   Well, always on security monitoring is part of what makes the cloud managed
[02:48:41.520 --> 02:48:42.560]   network so awesome.
[02:48:42.560 --> 02:48:47.720]   IT can use apps from Maraki's vast ecosystem of partners, turn key solutions
[02:48:47.720 --> 02:48:51.760]   built to work seamlessly with the Maraki cloud platform for asset tracking
[02:48:51.760 --> 02:48:53.320]   and location analytics and more.
[02:48:53.320 --> 02:48:57.160]   You can actually use it if you're hybrid for work, the office too.
[02:48:57.160 --> 02:49:01.880]   You can gather insights on how people are using the work provided workspaces.
[02:49:02.400 --> 02:49:07.040]   In a smart space, environmental sensors can track activity and occupancy levels,
[02:49:07.040 --> 02:49:09.120]   stay on top of cleanliness.
[02:49:09.120 --> 02:49:13.600]   Workers can reserve workspace based on vacancy and employee profiles, you know,
[02:49:13.600 --> 02:49:14.440]   hot desking.
[02:49:14.440 --> 02:49:17.720]   So employees can, you know, quickly find a place to work.
[02:49:17.720 --> 02:49:22.720]   Locations in restricted environments can be both booked and advanced and
[02:49:22.720 --> 02:49:24.600]   include time-based door access.
[02:49:24.600 --> 02:49:27.040]   Again, security becomes very important.
[02:49:27.040 --> 02:49:29.960]   And then of course, you've always got mobile device management,
[02:49:29.960 --> 02:49:34.720]   integrating devices and systems so that IT can manage, update and troubleshoot
[02:49:34.720 --> 02:49:39.200]   company-owned devices, even when the device and employee are on the beach.
[02:49:39.200 --> 02:49:45.560]   Turn any, this is the whole point, turn any space into a place of productivity,
[02:49:45.560 --> 02:49:50.320]   empower your organization with the same experience no matter where they work.
[02:49:50.320 --> 02:49:54.200]   How do you do it with Maraki and the Cisco suite of technologies?
[02:49:54.200 --> 02:49:57.680]   Learn how your organization can make hybrid work work.
[02:49:57.920 --> 02:50:04.520]   Maraki, M-E-R-A-K-I, maraki.cisco.com/twit.
[02:50:04.520 --> 02:50:07.280]   Your workers want it, you want it.
[02:50:07.280 --> 02:50:09.240]   Here's how you can do it safely and effectively.
[02:50:09.240 --> 02:50:14.000]   Maraki.cisco.com/twit.
[02:50:14.000 --> 02:50:18.000]   You, did you, Dan, or do you need to go?
[02:50:18.000 --> 02:50:20.680]   I heard you squeal when I said we're almost done.
[02:50:24.960 --> 02:50:29.080]   I, I, I, I postponed another call.
[02:50:29.080 --> 02:50:30.640]   Okay, we'll do it real quickly.
[02:50:30.640 --> 02:50:33.080]   We'll wrap this up.
[02:50:33.080 --> 02:50:35.680]   Now you can, we have still tomorrow.
[02:50:35.680 --> 02:50:36.120]   That's fine.
[02:50:36.120 --> 02:50:38.160]   Oh, okay. Well, that case.
[02:50:38.160 --> 02:50:40.520]   Here's what you missed this week on Twitter.
[02:50:40.520 --> 02:50:44.640]   Oh, you know the problem is I changed where I part.
[02:50:44.640 --> 02:50:47.280]   Is that, that's, show me, show a single I can't tell.
[02:50:47.280 --> 02:50:49.200]   This, I think the new one doesn't work.
[02:50:49.200 --> 02:50:49.880]   Oh, really?
[02:50:49.880 --> 02:50:51.040]   Yeah, that, that's, that makes sense.
[02:50:51.040 --> 02:50:52.240]   I, yeah.
[02:50:52.280 --> 02:50:55.040]   It's really unflattering for that side of your head.
[02:50:55.040 --> 02:50:58.680]   Well, I wonder where he's going now.
[02:50:58.680 --> 02:50:59.920]   Previously on Twitter.
[02:50:59.920 --> 02:51:03.760]   iOS today, Rosemary Orchard and I, my
[02:51:03.760 --> 02:51:08.280]   mycocergent are going to be covering Final Cut Pro and Logic Pro,
[02:51:08.280 --> 02:51:10.840]   which are now on iPad.
[02:51:10.840 --> 02:51:11.880]   All about Android.
[02:51:11.880 --> 02:51:14.200]   Today I give my review of the OnePlus
[02:51:14.200 --> 02:51:18.880]   pad. I've got the Stylo that snaps to the top and right away.
[02:51:18.880 --> 02:51:21.480]   I get the little notification that tells me that it's connected.
[02:51:21.480 --> 02:51:22.480]   Just like an eye on that.
[02:51:22.480 --> 02:51:27.840]   Like the overall package, like I really was surprised at how much I like this keyboard case.
[02:51:27.840 --> 02:51:29.120]   Mac break weekly.
[02:51:29.120 --> 02:51:35.400]   By the way, Alex, I think your LSD adult nephew did, in fact, design the home page for
[02:51:35.400 --> 02:51:37.600]   WWDC.
[02:51:37.600 --> 02:51:40.320]   It's a trippy.
[02:51:40.320 --> 02:51:42.120]   To say the least.
[02:51:42.120 --> 02:51:43.880]   The melting apple.
[02:51:43.880 --> 02:51:48.200]   I think if you look at it on an iPhone, there's a VR.
[02:51:48.200 --> 02:51:49.200]   Oh, what's this?
[02:51:49.200 --> 02:51:51.080]   Oh, he's found the apple.
[02:51:51.360 --> 02:51:53.080]   It's floating over my head.
[02:51:53.080 --> 02:51:55.280]   Ah, ah,
[02:51:55.280 --> 02:51:56.840]   Twit does end digestion.
[02:51:56.840 --> 02:51:58.080]   Keep you up at night.
[02:51:58.080 --> 02:51:59.920]   This news is new.
[02:51:59.920 --> 02:52:04.400]   Soothing your tech indigestion one show at a time.
[02:52:04.400 --> 02:52:09.520]   By the way, I don't know when this happened, but we seem to have replaced our
[02:52:09.520 --> 02:52:15.800]   professionally trained human announcer, Jim Cutler, with an AI voice of mine.
[02:52:15.800 --> 02:52:18.720]   So I guess there's one more person out of work.
[02:52:18.720 --> 02:52:19.720]   Thanks to AI.
[02:52:19.720 --> 02:52:21.080]   Sorry about that, Jim.
[02:52:21.800 --> 02:52:22.400]   Holy cow.
[02:52:22.400 --> 02:52:23.480]   He was doing it for free.
[02:52:23.480 --> 02:52:23.720]   Maybe.
[02:52:23.720 --> 02:52:25.360]   I mean, we just didn't.
[02:52:25.360 --> 02:52:26.360]   We don't save money.
[02:52:26.360 --> 02:52:28.680]   We just, uh, let's see.
[02:52:28.680 --> 02:52:33.720]   What else Amazon's going to pay $30 million to settle FTC privacy complaints
[02:52:33.720 --> 02:52:37.000]   over their ring doorbell and Amazon Echo.
[02:52:37.000 --> 02:52:40.120]   There's been a number of judgments lately.
[02:52:40.120 --> 02:52:41.200]   Meta lost a big one.
[02:52:41.200 --> 02:52:47.280]   Was it 1.3 billion euros in the, in the Ireland, uh, because it was spying on
[02:52:47.280 --> 02:52:49.240]   people and selling that information on good.
[02:52:49.680 --> 02:52:51.400]   I hope regulators cracked down.
[02:52:51.400 --> 02:52:58.040]   Um, notice though, here in the United States, it's pretty, it's very quiet.
[02:52:58.040 --> 02:53:01.240]   No, don't, don't mess with that.
[02:53:01.240 --> 02:53:07.120]   Uh, Intel is going to put an AI engine in its new made-your-lake systems on a chip.
[02:53:07.120 --> 02:53:08.520]   I learned that actually at build.
[02:53:08.520 --> 02:53:13.000]   Everybody's processors now have a, uh, AI engine in there.
[02:53:13.000 --> 02:53:15.480]   And this made me very sad.
[02:53:15.480 --> 02:53:20.400]   The Amazon Echo is losing Samuel Jackson and Melissa McCarthy.
[02:53:20.400 --> 02:53:26.760]   I paid for those celebrity voices and I want them, but no, uh, Amazon will refund
[02:53:26.760 --> 02:53:29.520]   you, uh, but you have to contact customer service.
[02:53:29.520 --> 02:53:34.560]   It was really nice because Samuel Jackson, you'd, you'd say Samuel was fun.
[02:53:34.560 --> 02:53:34.880]   Yeah.
[02:53:34.880 --> 02:53:37.400]   You say, what's the weather and he would swear up a blue streak.
[02:53:37.400 --> 02:53:40.400]   If you asked him, why did he, have you done this?
[02:53:40.680 --> 02:53:45.360]   Christina asks Samuel, why does he swear so much? Oh, I haven't asked him that.
[02:53:45.360 --> 02:53:45.600]   Okay.
[02:53:45.600 --> 02:53:46.040]   I will.
[02:53:46.040 --> 02:53:47.480]   I don't have a swear.
[02:53:47.480 --> 02:53:49.200]   I'm a, it's very good.
[02:53:49.200 --> 02:53:50.120]   It's very good.
[02:53:50.120 --> 02:53:53.640]   Melissa Mc, I would always, I would ask Samuel what the weather is going to be.
[02:53:53.640 --> 02:53:55.040]   And then say Melissa, what's the weather going to be?
[02:53:55.040 --> 02:53:56.800]   And she would always say something cute.
[02:53:56.800 --> 02:53:57.520]   It was fun.
[02:53:57.520 --> 02:54:00.000]   99 cents at launch.
[02:54:00.000 --> 02:54:01.800]   They were then $4.99.
[02:54:01.800 --> 02:54:04.880]   Uh, I think we always knew it was time limited.
[02:54:04.880 --> 02:54:08.440]   They're not going to have an unlimited license to use a celebrity voices.
[02:54:08.880 --> 02:54:13.480]   But Amazon says after three years, we're going to wind down the celebrity voices.
[02:54:13.480 --> 02:54:15.680]   You'll be able to continue using them for a limited time.
[02:54:15.680 --> 02:54:19.800]   If you want your money back, you can just give the AI to create their own
[02:54:19.800 --> 02:54:20.640]   celebrity voices.
[02:54:20.640 --> 02:54:21.480]   Yeah.
[02:54:21.480 --> 02:54:22.720]   Oh, yeah.
[02:54:22.720 --> 02:54:23.160]   The law.
[02:54:23.160 --> 02:54:27.640]   Well, it was a, actually, because Samuel Jackson went in, he would record some
[02:54:27.640 --> 02:54:27.920]   stuff.
[02:54:27.920 --> 02:54:30.560]   Some of it would be act you could tell it would actually be him.
[02:54:30.560 --> 02:54:34.440]   And then when he was saying the weather, it would kind of be a little less lively.
[02:54:34.440 --> 02:54:36.840]   That was the only difference.
[02:54:37.520 --> 02:54:41.240]   Uh, through June 7th for, uh, well, that's Cully.
[02:54:41.240 --> 02:54:42.640]   That's three days from now.
[02:54:42.640 --> 02:54:48.480]   Uh, for a Samuel L Jackson, Melissa McCarthy and Shaquille Neil will continue
[02:54:48.480 --> 02:54:49.920]   through September 30th.
[02:54:49.920 --> 02:54:52.480]   I'm going to miss that.
[02:54:52.480 --> 02:54:59.040]   Um, part of this is Andy Jassy saying, uh, we were losing money like crazy.
[02:54:59.040 --> 02:55:00.000]   Yeah.
[02:55:00.000 --> 02:55:02.960]   And I can't imagine that the usage honestly was great because they were fun.
[02:55:02.960 --> 02:55:06.080]   It was a fun party trick, but I can't imagine that most people used it.
[02:55:06.080 --> 02:55:09.920]   So if you have to pay, like, and also I also imagine that the celebrities, when
[02:55:09.920 --> 02:55:14.200]   they're maybe being asked to renegotiate the contracts, given all the stuff that, um,
[02:55:14.200 --> 02:55:17.520]   is, uh, you know, happening with the writer's guild and I know that the
[02:55:17.520 --> 02:55:19.360]   screen actor's guild was looking at things too.
[02:55:19.360 --> 02:55:22.360]   They wouldn't be wrong if they were wanting to maybe ask for more money.
[02:55:22.360 --> 02:55:23.920]   This is just my interpretation.
[02:55:23.920 --> 02:55:29.120]   I have no idea, but, you know, I, I could imagine that they would possibly be
[02:55:29.120 --> 02:55:33.360]   wanting to negotiate for more or similar terms in Amazon might not want to give
[02:55:33.360 --> 02:55:34.920]   an usage being lower.
[02:55:34.920 --> 02:55:37.800]   So yeah, but Andy, it's cost cutting time.
[02:55:37.800 --> 02:55:40.120]   And I'm sure he's looking at this like fair.
[02:55:40.120 --> 02:55:43.120]   If you only Christina and Leo are using that basically is us.
[02:55:43.120 --> 02:55:43.800]   Basically.
[02:55:43.800 --> 02:55:46.880]   So we're not a personal note from Andy.
[02:55:46.880 --> 02:55:48.720]   Right.
[02:55:48.720 --> 02:55:50.040]   We know you do.
[02:55:50.040 --> 02:55:53.840]   Uh, honestly, I'm not in a hurry to go home.
[02:55:53.840 --> 02:55:57.320]   We, this show would have been over an hour ago if succession were on tonight, but.
[02:55:57.320 --> 02:55:59.200]   No, very sad.
[02:55:59.200 --> 02:56:01.760]   I love that.
[02:56:01.760 --> 02:56:07.680]   What a great show for seasons, uh, something like 40 episodes, which is, if you think
[02:56:07.680 --> 02:56:12.560]   about, I mean, a movie is what two hours is 40 hours or more of content.
[02:56:12.560 --> 02:56:15.680]   Uh, and yet this, the standards were very high.
[02:56:15.680 --> 02:56:17.040]   I'm sure many Emmy awards.
[02:56:17.040 --> 02:56:18.200]   They have already won 13.
[02:56:18.200 --> 02:56:19.760]   I'm sure they'll win many more this year.
[02:56:19.760 --> 02:56:20.280]   It was 10th.
[02:56:20.280 --> 02:56:21.080]   Last though, Mrs.
[02:56:21.080 --> 02:56:22.320]   Mabel and succession over.
[02:56:22.320 --> 02:56:23.800]   I don't have anything to watch anymore.
[02:56:23.800 --> 02:56:29.720]   I was going to say, I was going to say, Barry as well, like just, um, I watched
[02:56:29.720 --> 02:56:35.280]   succession, um, in a bar actually that is co-owned by cousin Greg.
[02:56:35.280 --> 02:56:36.920]   Oh, you're kidding.
[02:56:36.920 --> 02:56:38.680]   It was no, it was packed.
[02:56:38.680 --> 02:56:40.160]   It was, it was a glory side.
[02:56:40.160 --> 02:56:41.440]   It's called raise.
[02:56:41.440 --> 02:56:42.880]   It was amazing.
[02:56:42.880 --> 02:56:44.000]   Did Greg show up?
[02:56:44.000 --> 02:56:44.560]   See the screen.
[02:56:44.560 --> 02:56:45.440]   He did not.
[02:56:45.440 --> 02:56:46.520]   He had his own thing.
[02:56:46.520 --> 02:56:47.040]   Yeah.
[02:56:47.040 --> 02:56:47.200]   Yeah.
[02:56:47.200 --> 02:56:50.800]   And Nicholas want to know he had his own thing, but, but the place was packed.
[02:56:50.800 --> 02:56:54.680]   And what we, what we did, because they're planted over a sound system, um, we
[02:56:54.680 --> 02:56:55.520]   couldn't see the screen.
[02:56:55.520 --> 02:56:59.200]   So I pulled up my phone and we streamed it on my phone.
[02:56:59.200 --> 02:57:03.000]   And captions on and like a bunch of people were crowded around my phone,
[02:57:03.000 --> 02:57:03.520]   watching it.
[02:57:03.520 --> 02:57:08.240]   Um, but I have to say, like watching the finale with like, oh, I'm jealous.
[02:57:08.240 --> 02:57:10.440]   I have a hundred other people literally crammed into this bar.
[02:57:10.440 --> 02:57:11.000]   It was great.
[02:57:11.000 --> 02:57:15.400]   And then because to just make it literally my perfect night, the bar then
[02:57:15.400 --> 02:57:17.360]   turned into a Taylor Swift dance party.
[02:57:17.360 --> 02:57:23.240]   So it was genuinely like, it really was like the Venn diagram of people who
[02:57:23.240 --> 02:57:27.080]   care about those two things is larger than I thought it would be.
[02:57:27.400 --> 02:57:29.880]   Um, although a lot of the succession people left and a lot of the
[02:57:29.880 --> 02:57:35.920]   Taylor Swift people came in, but a little bit, but no, but it was, uh,
[02:57:35.920 --> 02:57:36.680]   I thought I saw it.
[02:57:36.680 --> 02:57:38.040]   Really would have stayed for both.
[02:57:38.040 --> 02:57:38.520]   I'm sure.
[02:57:38.520 --> 02:57:40.000]   Willa would have stayed for both.
[02:57:40.000 --> 02:57:42.280]   No, some of the people were wearing jerseys.
[02:57:42.280 --> 02:57:43.920]   I was like, where are my con heads at?
[02:57:43.920 --> 02:57:48.120]   I'm sure Greg would have kissed up to both sides as well.
[02:57:48.120 --> 02:57:50.360]   Oh, he would have absolutely kissed on both sides.
[02:57:50.360 --> 02:57:50.560]   Yeah.
[02:57:50.560 --> 02:57:52.480]   What's interesting is this.
[02:57:53.640 --> 02:57:58.200]   So this is not as big a TV show as network television was in its heyday.
[02:57:58.200 --> 02:57:59.080]   This, right.
[02:57:59.080 --> 02:58:04.760]   The show averaged 8.4 million viewers per episode, which is pretty much nothing
[02:58:04.760 --> 02:58:12.160]   compared to 13, 14, 15 million, uh, that would watch a prime time TV show.
[02:58:12.160 --> 02:58:14.920]   You know, every week, mad men, 30, 30 million.
[02:58:14.920 --> 02:58:16.760]   I mean, yeah, I used to get 40 million.
[02:58:16.760 --> 02:58:17.160]   Yeah.
[02:58:17.160 --> 02:58:22.360]   Um, yeah, even the, the, the sopranos, uh, you know, much higher game of thrones, much
[02:58:22.360 --> 02:58:29.400]   higher, but the, I think the cultural impact of succession was outsized compared to the
[02:58:29.400 --> 02:58:30.600]   people who actually watched it.
[02:58:30.600 --> 02:58:35.560]   In other words, you and I really influenced people.
[02:58:35.560 --> 02:58:36.680]   Many fear.
[02:58:36.680 --> 02:58:38.400]   Sorry.
[02:58:38.400 --> 02:58:40.040]   Good PR for our friend, Keras Wisher.
[02:58:40.040 --> 02:58:44.920]   I'd actually never listened to her podcast, but I, we saw that on every episode that she had a podcast.
[02:58:44.920 --> 02:58:45.520]   No, yeah.
[02:58:45.520 --> 02:58:45.600]   Yeah.
[02:58:45.600 --> 02:58:46.920]   She hosted the, which was great.
[02:58:46.920 --> 02:58:48.920]   And yeah, she got good interviews now.
[02:58:48.920 --> 02:58:49.360]   Oh, sure.
[02:58:49.360 --> 02:58:51.760]   Questions, you know, like, yeah.
[02:58:51.760 --> 02:58:54.280]   So it's very strange.
[02:58:54.280 --> 02:59:01.760]   I ended up watching the show and only because people keep reminding me it was on.
[02:59:01.760 --> 02:59:05.280]   Do I ever think about it in the weeks since it ended?
[02:59:05.280 --> 02:59:14.120]   Uh, I just, I've never seen anything where there was not a single.
[02:59:15.520 --> 02:59:21.920]   Character with redeem any redeeming features except for the brother of the.
[02:59:21.920 --> 02:59:26.560]   Uncle, uncle, uncle, uncle, yeah.
[02:59:26.560 --> 02:59:28.480]   I don't know if he ever had me features either.
[02:59:28.480 --> 02:59:30.240]   I was going to say, I don't think he was.
[02:59:30.240 --> 02:59:31.720]   They were all awful.
[02:59:31.720 --> 02:59:33.720]   That was, yeah, that was.
[02:59:33.720 --> 02:59:36.760]   But I just, there was no hero as all anti-hero, wasn't it?
[02:59:36.760 --> 02:59:40.920]   But, but, but they were, these are, they were all loathsome.
[02:59:40.920 --> 02:59:41.360]   Yes.
[02:59:41.360 --> 02:59:45.360]   Not just, not just, not just sort of bad.
[02:59:45.360 --> 02:59:53.280]   And I, yeah, I don't, that's not something I want to carry around in my warm memories.
[02:59:53.280 --> 02:59:54.000]   Sorry.
[02:59:54.000 --> 02:59:57.520]   I just want to, I just want to have it go into the past.
[02:59:57.520 --> 02:59:58.680]   So I'm not even thinking about it.
[02:59:58.680 --> 03:00:00.280]   You know with a documentary, right?
[03:00:00.280 --> 03:00:02.920]   It was about the Murdoch family.
[03:00:02.920 --> 03:00:03.200]   Yeah.
[03:00:03.200 --> 03:00:04.960]   The Murdochs and the Red Stones.
[03:00:04.960 --> 03:00:06.240]   Um, yeah.
[03:00:06.240 --> 03:00:08.400]   It was about a, it was about a few things.
[03:00:08.400 --> 03:00:10.440]   It was about late stage capitalism.
[03:00:10.440 --> 03:00:17.240]   It was about control of mainstream media by a handful of, as you say, evil people.
[03:00:17.240 --> 03:00:24.080]   It was also though about family and about how damaged these children were by a father.
[03:00:24.080 --> 03:00:24.800]   All of the success.
[03:00:24.800 --> 03:00:27.440]   Rule and by all the excess they had growing up.
[03:00:27.440 --> 03:00:29.560]   So that made it more personal.
[03:00:29.560 --> 03:00:31.040]   I, you're right though.
[03:00:31.040 --> 03:00:33.640]   I think in, in some ways, the fact that there was no hero.
[03:00:33.640 --> 03:00:36.720]   Made it exceptional.
[03:00:36.720 --> 03:00:40.160]   Uh, I can't imagine if there was a good guy in this show.
[03:00:40.160 --> 03:00:41.920]   I would have wanted to watch it to be honest.
[03:00:41.920 --> 03:00:42.720]   No, decent.
[03:00:42.720 --> 03:00:44.120]   I mean, Dan made a good put on.
[03:00:44.120 --> 03:00:45.920]   Were there even anybody who's halfway decent?
[03:00:45.920 --> 03:00:47.240]   Well, Jerry was all right.
[03:00:47.240 --> 03:00:48.000]   Jerry was pretty.
[03:00:48.000 --> 03:00:49.080]   Jerry, maybe.
[03:00:49.080 --> 03:00:52.520]   Well, I, I, I think, well, it was okay.
[03:00:52.520 --> 03:00:55.520]   Honestly, like the prostitute who ended up marrying the.
[03:00:55.520 --> 03:00:56.560]   Yeah, she would have had a court.
[03:00:56.560 --> 03:00:56.880]   Honestly.
[03:00:56.880 --> 03:01:03.520]   So that he is your presidential campaign, but do better because he was married.
[03:01:03.520 --> 03:01:08.680]   But also, you know, his dad dies and he, and she still goes through with the wedding.
[03:01:08.680 --> 03:01:11.720]   Like, yeah, could have called it off and he could have that option.
[03:01:11.720 --> 03:01:14.360]   You know, like the money, like everybody else.
[03:01:14.360 --> 03:01:15.920]   And it was all about the money.
[03:01:15.920 --> 03:01:16.360]   Absolutely.
[03:01:16.360 --> 03:01:17.760]   100%.
[03:01:17.760 --> 03:01:19.520]   I mean, you know, you're right.
[03:01:19.520 --> 03:01:22.120]   Like for a lot of people, this is not a show that they would enjoy.
[03:01:22.120 --> 03:01:23.280]   And I totally understand that.
[03:01:23.280 --> 03:01:25.800]   I personally loving the antihero.
[03:01:25.800 --> 03:01:27.080]   I really, really did.
[03:01:27.080 --> 03:01:28.200]   I think it was totally.
[03:01:28.200 --> 03:01:28.680]   I really.
[03:01:28.680 --> 03:01:29.440]   I watched it.
[03:01:29.440 --> 03:01:31.920]   I watched it in total fascination.
[03:01:31.920 --> 03:01:32.240]   Yeah.
[03:01:32.240 --> 03:01:32.880]   Not that I.
[03:01:32.880 --> 03:01:33.920]   Well, this is your business.
[03:01:33.920 --> 03:01:36.120]   So in that sense, I enjoyed it.
[03:01:36.120 --> 03:01:41.800]   But it I don't I miss them all.
[03:01:41.800 --> 03:01:43.560]   I don't know what I'm going to do tonight.
[03:01:43.560 --> 03:01:44.600]   I'm so sad.
[03:01:44.600 --> 03:01:45.040]   I know.
[03:01:45.040 --> 03:01:47.480]   Which is hysterical.
[03:01:47.480 --> 03:01:50.160]   There was a good article in Vanity Fair.
[03:01:50.160 --> 03:01:53.120]   I'm not sure I actually agree with the premise.
[03:01:53.120 --> 03:01:54.040]   Joy press writing.
[03:01:54.040 --> 03:01:57.520]   Will there be any successors to succession?
[03:01:57.520 --> 03:02:00.960]   Is it the end of an era for Hollywood or inspiration for a new beginning?
[03:02:00.960 --> 03:02:02.480]   It's a reasonable question to ask.
[03:02:02.480 --> 03:02:05.560]   We don't all watch the same things anymore, right?
[03:02:05.920 --> 03:02:06.440]   Right.
[03:02:06.440 --> 03:02:12.960]   And even, you know, with succession, 8.4 million viewers is a tiny fraction of the total audience.
[03:02:12.960 --> 03:02:18.320]   Well, I mean, but also to be clear, that's like the live audience that they kind of or week of audience, they capture.
[03:02:18.320 --> 03:02:21.920]   We don't know how many people watched, you know, and bingeing and other things are consumed.
[03:02:21.920 --> 03:02:23.440]   Well, it probably has a long tail.
[03:02:23.440 --> 03:02:31.800]   I don't know if it's one saying if David Zaz love will cancel it and move it to, you know, Roku channel or free or something.
[03:02:31.800 --> 03:02:34.600]   But honestly, that that's that's the big question, right?
[03:02:34.600 --> 03:02:43.560]   Which is why I buy every season of it when it comes out because I even before he was installed, I had my own concerns about that.
[03:02:43.560 --> 03:02:53.880]   I was like, Oh, I don't know if this whole like era of let's just keep all the content here in our libraries to, you know, boost the things for people like me is going to last.
[03:02:53.880 --> 03:02:56.480]   I bet they're going to take this back out again and try to sell it.
[03:02:56.480 --> 03:02:57.560]   They can make more money.
[03:02:57.560 --> 03:02:59.400]   And let's not forget the writer strike.
[03:02:59.400 --> 03:03:01.800]   I mean, this was very much a writer's.
[03:03:01.800 --> 03:03:03.440]   100%
[03:03:03.640 --> 03:03:09.800]   And without those writers, I mean, it's just going to be Beverly Hills housewives all the way down.
[03:03:09.800 --> 03:03:11.760]   OK, and fairness.
[03:03:11.760 --> 03:03:20.400]   And fairness this season, the season finale and then the three part we've only seen two of the parts of the Vanderpump rules reunion is incredible television.
[03:03:20.400 --> 03:03:23.120]   So I'm not that's what I was afraid of.
[03:03:23.120 --> 03:03:27.000]   That's exactly what I'm thinking.
[03:03:28.560 --> 03:03:35.640]   There apparently is a Venn diagram of people who love succession and Vanderpump rules, and there's more than one person in that intersection.
[03:03:35.640 --> 03:03:39.120]   In fact, I've seen a lot of people say, Oh, you should start watching Vanderpump rules.
[03:03:39.120 --> 03:03:39.680]   I can't.
[03:03:39.680 --> 03:03:40.160]   I can't.
[03:03:40.160 --> 03:03:40.560]   I don't.
[03:03:40.560 --> 03:03:41.480]   I can't.
[03:03:41.480 --> 03:03:42.080]   I can't.
[03:03:42.080 --> 03:03:43.960]   I am sorry.
[03:03:43.960 --> 03:03:45.600]   I apologize to Dan.
[03:03:45.600 --> 03:03:48.200]   I hope your phone call wasn't important.
[03:03:48.200 --> 03:03:51.120]   I am thrilled that you spent some time with us.
[03:03:51.120 --> 03:03:52.480]   Thank you, Dan.
[03:03:52.480 --> 03:03:53.000]   One more.
[03:03:54.880 --> 03:04:03.760]   Your students are very lucky co-founder of the Arizona State News CoLab professor at the Walter Cronkite School of Journalism at Arizona State.
[03:04:03.760 --> 03:04:07.800]   He's at Dan Gilmore on the mastodon.
[03:04:07.800 --> 03:04:12.400]   And I can't wait to hear what you're up to with this new project.
[03:04:12.400 --> 03:04:17.520]   I will let you know as soon as I.
[03:04:17.520 --> 03:04:18.200]   Great.
[03:04:18.200 --> 03:04:19.600]   Have something to.
[03:04:19.600 --> 03:04:20.000]   Right.
[03:04:20.000 --> 03:04:20.960]   Real.
[03:04:20.960 --> 03:04:21.720]   Nice.
[03:04:22.240 --> 03:04:26.120]   Actually, I realize we have two people on the show have written for the Mercury News, Chris.
[03:04:26.120 --> 03:04:28.400]   That's where I first met Dan.
[03:04:28.400 --> 03:04:34.760]   And I guess, Larry, you just published that article about the screaming woman on the Mercury News.
[03:04:34.760 --> 03:04:39.560]   So, yeah, in fact, Dan, I was danced with sandwich in between me writing for the Mercury.
[03:04:39.560 --> 03:04:45.480]   And then they hired Dan and I stopped writing for the Mercury because he took over the main column and then somehow I got it back.
[03:04:45.480 --> 03:04:46.040]   I'm not sure why.
[03:04:46.040 --> 03:04:48.600]   So you guys, oh, you even wrote the same column.
[03:04:48.600 --> 03:04:51.400]   Sort of the main, the main column, the business section.
[03:04:51.400 --> 03:04:52.960]   No, one day of the week.
[03:04:52.960 --> 03:04:53.200]   Yeah.
[03:04:53.200 --> 03:04:53.960]   I did not.
[03:04:53.960 --> 03:04:58.600]   I'd like to think of it as feeding LLMs.
[03:04:58.600 --> 03:05:02.760]   Yes, all your columns have now been to just.
[03:05:02.760 --> 03:05:03.320]   That's fine.
[03:05:03.320 --> 03:05:05.120]   If I'm being received, now I get it.
[03:05:05.120 --> 03:05:12.480]   I'm I'm I'm my my service as training data is over temporarily.
[03:05:12.480 --> 03:05:14.920]   No more training data.
[03:05:14.920 --> 03:05:18.000]   I guess this is training data, too, though, right?
[03:05:18.000 --> 03:05:20.800]   Some LLMs might use audio as well.
[03:05:20.800 --> 03:05:21.360]   I don't know.
[03:05:21.360 --> 03:05:22.400]   Oh, yeah.
[03:05:22.400 --> 03:05:25.680]   Anybody who creates content should be worried.
[03:05:25.680 --> 03:05:29.440]   All right, we're going to stop feeding the LLMs now.
[03:05:29.440 --> 03:05:30.760]   Thank you, Larry Maggett.
[03:05:30.760 --> 03:05:32.160]   Connect safely.org.
[03:05:32.160 --> 03:05:38.040]   That's where he is CEO and president and where you'll find that that victim.
[03:05:38.040 --> 03:05:39.520]   What is it?
[03:05:39.520 --> 03:05:40.400]   What do you call that?
[03:05:40.400 --> 03:05:42.560]   The virtual kidnapping.
[03:05:42.560 --> 03:05:44.360]   Virtual kidnapping.
[03:05:44.360 --> 03:05:45.200]   Holy cow.
[03:05:45.200 --> 03:05:46.160]   Yeah.
[03:05:46.160 --> 03:05:46.760]   Yeah.
[03:05:46.760 --> 03:05:50.680]   It's right there in the upper right at Larry Maggett on the Twitter.
[03:05:50.920 --> 03:05:52.120]   Thank you, Larry.
[03:05:52.120 --> 03:05:53.120]   My pleasure.
[03:05:53.120 --> 03:05:53.960]   Always always enjoyed.
[03:05:53.960 --> 03:05:55.520]   Always great to see you.
[03:05:55.520 --> 03:06:01.160]   When you've been you and Dan and I guess Christina, too, have all been with us for more than a decade.
[03:06:01.160 --> 03:06:02.600]   Yeah, amazing.
[03:06:02.600 --> 03:06:04.520]   I am so grateful.
[03:06:04.520 --> 03:06:06.280]   I'm not I'm not kidding here.
[03:06:06.280 --> 03:06:10.640]   I'm just so grateful to the people who participate in our shows with us.
[03:06:10.640 --> 03:06:13.840]   I've actually known you Leo for probably 25 years.
[03:06:13.840 --> 03:06:14.200]   Amazing.
[03:06:14.200 --> 03:06:16.680]   You were both young young upstart in the media.
[03:06:16.680 --> 03:06:17.160]   It's a pop.
[03:06:17.160 --> 03:06:18.200]   Young pop.
[03:06:19.280 --> 03:06:20.920]   And eventually we'll figure out this craft.
[03:06:20.920 --> 03:06:23.600]   Thank you, Larry.
[03:06:23.600 --> 03:06:25.920]   Christina, always great to see you.
[03:06:25.920 --> 03:06:28.800]   Christina Warren, senior dev advocate at GitHub.
[03:06:28.800 --> 03:06:32.040]   Film underscore girl, almost everywhere.
[03:06:32.040 --> 03:06:33.400]   Almost everywhere.
[03:06:33.400 --> 03:06:39.760]   If I'm not the one who's girl, someplace than his film girl, one word, because the site doesn't support underscores.
[03:06:39.760 --> 03:06:45.000]   Again, if I've been smarter about this, I would have been consistent and but I wasn't.
[03:06:45.000 --> 03:06:45.680]   So here we are.
[03:06:45.680 --> 03:06:52.040]   I want to learn how to turn my microblog into my my AT proto site.
[03:06:52.040 --> 03:06:53.640]   I'll have to see if I can figure that out.
[03:06:53.640 --> 03:06:55.080]   That's cool.
[03:06:55.080 --> 03:06:55.360]   Yeah.
[03:06:55.360 --> 03:06:56.280]   I've got it.
[03:06:56.280 --> 03:06:57.960]   I've got it with the mastodontin.
[03:06:57.960 --> 03:06:59.200]   All right.
[03:06:59.200 --> 03:07:00.560]   I'll have to do some searching.
[03:07:00.560 --> 03:07:04.800]   That's a that's a nice plug for micro dot blog in Mantone Reese's.
[03:07:04.800 --> 03:07:08.080]   He's been he's been open web since the web was open.
[03:07:08.080 --> 03:07:11.600]   Basically, I mean, I was going to say like one of the OG kind of things.
[03:07:11.600 --> 03:07:14.160]   I when I think of the Fediverse, I really honestly think of microblog.
[03:07:14.160 --> 03:07:14.680]   I agree.
[03:07:14.680 --> 03:07:15.280]   Yeah.
[03:07:15.280 --> 03:07:18.520]   Before anything else because it predated a lot of this other stuff.
[03:07:18.520 --> 03:07:23.200]   And I'm glad for him for for doing the work that he's done.
[03:07:23.200 --> 03:07:26.720]   And I'm glad to see so many other projects, you know, come around.
[03:07:26.720 --> 03:07:37.200]   I mean, actually, if we really wanted to be honest, we could also give a shout out to Dalton Caldwell for the original app.net, which, you know, had many of these same ideas, but obviously didn't work.
[03:07:37.200 --> 03:07:40.160]   So, you know, we ran.
[03:07:40.160 --> 03:07:44.640]   We ran an identity server back in the day that went on me.
[03:07:44.640 --> 03:07:45.400]   Identica.
[03:07:45.400 --> 03:07:46.000]   Yes.
[03:07:46.000 --> 03:07:49.320]   Based on app.net, the earliest version of activity pub.
[03:07:49.320 --> 03:07:53.840]   Then when there was status net and there was social.
[03:07:53.840 --> 03:07:56.040]   Yeah, there were a friend.
[03:07:56.040 --> 03:07:57.360]   Or something like that.
[03:07:57.360 --> 03:07:57.680]   Yeah.
[03:07:57.680 --> 03:07:58.200]   Yeah.
[03:07:58.200 --> 03:07:59.200]   Yeah.
[03:07:59.200 --> 03:08:06.400]   So we've been part of this, you know, kind of alternative social media for a long time.
[03:08:06.400 --> 03:08:08.240]   I'm glad to see it's getting the attention.
[03:08:08.240 --> 03:08:09.160]   It's long deserved.
[03:08:10.040 --> 03:08:10.880]   Thank you, Christina.
[03:08:10.880 --> 03:08:11.800]   Thank you, Dan.
[03:08:11.800 --> 03:08:12.880]   Thank you, Larry.
[03:08:12.880 --> 03:08:19.480]   Thank you all for being here, especially thanks to our club members who make this show and everything we do possible.
[03:08:19.480 --> 03:08:21.120]   I want to invite you to join.
[03:08:21.120 --> 03:08:24.360]   If you're not yet a member, seven dollars a month, you get ad free versions.
[03:08:24.360 --> 03:08:38.280]   That means tracker free versions as well of everything we do plus shows we don't put out anywhere else, including the brand new Scott Wilkinson's home theater, Geeks, hands on Mac with Micah, Sergeant, hands on windows with Paul, the Rott, the untitled Linux show.
[03:08:39.000 --> 03:08:41.520]   The Giz Fizz Stacey's book club.
[03:08:41.520 --> 03:08:42.800]   All of these are club only.
[03:08:42.800 --> 03:08:45.400]   We also have access to the discord.
[03:08:45.400 --> 03:08:50.440]   We have a club discord that is honestly, I think this is discord is in many ways, the future of social.
[03:08:50.440 --> 03:08:55.400]   And our discord is all made up of people who are in the club.
[03:08:55.400 --> 03:09:01.040]   And we talk about not just what's going on in the shows, but all sorts of topics that any geek would be interested in.
[03:09:01.040 --> 03:09:04.440]   Plus we have special events at Pruitt's, our community manager there.
[03:09:04.440 --> 03:09:08.960]   And you get the Twipless feed, which includes conversations like the weird one we had before.
[03:09:08.960 --> 03:09:13.280]   The show, the no one else hears all that for seven bucks a month.
[03:09:13.280 --> 03:09:14.720]   I think it's the best deal possible.
[03:09:14.720 --> 03:09:16.640]   Just go to twit.tv/club twit.
[03:09:16.640 --> 03:09:17.920]   There's annual memberships.
[03:09:17.920 --> 03:09:20.680]   There's family memberships, corporate memberships as well.
[03:09:20.680 --> 03:09:25.320]   But your membership, your members, you members really make a big difference.
[03:09:25.320 --> 03:09:26.240]   We thank you very much.
[03:09:26.240 --> 03:09:28.800]   Your membership dollars really help keep us on the air.
[03:09:28.800 --> 03:09:30.480]   It's a tough time for podcasting.
[03:09:30.480 --> 03:09:31.160]   I'll be honest.
[03:09:31.160 --> 03:09:34.440]   And if we're going to survive, it's going to have to be with your help.
[03:09:34.440 --> 03:09:37.440]   Please join twit.tv/club.
[03:09:38.120 --> 03:09:40.800]   Twit. We do this show every Sunday.
[03:09:40.800 --> 03:09:46.240]   It's 2 p.m. Pacific, 5 p.m. Eastern, 2,100 UTC right after ask the tech guys.
[03:09:46.240 --> 03:09:52.120]   You can watch us do it live as with all our shows, almost all our shows at live.twit.tv.
[03:09:52.120 --> 03:10:00.520]   If you're watching live chat live at IRC.twit.tv or in the club Twit Discord after the fact, you can get the show at Twit.tv, the website.
[03:10:00.520 --> 03:10:02.600]   While you're there, I think it's what is it?
[03:10:02.600 --> 03:10:04.280]   Twit.tv/towit.tv/towit.
[03:10:04.280 --> 03:10:06.680]   It's a long, just click the button that says this week in tech.
[03:10:07.120 --> 03:10:10.600]   You'll see a YouTube channel that has the video from every show.
[03:10:10.600 --> 03:10:18.120]   You'll also see links to various podcast players, honestly, subscribing for any of our shows is probably the best way to get them.
[03:10:18.120 --> 03:10:19.560]   That way you don't have to even think about it.
[03:10:19.560 --> 03:10:22.160]   There's audio or video feeds you choose.
[03:10:22.160 --> 03:10:27.160]   Subscribe, then you'll get it automatically and you'll have it for tomorrow.
[03:10:27.160 --> 03:10:29.120]   Don't forget, we will be here tomorrow.
[03:10:29.120 --> 03:10:35.280]   10 a.m. Pacific, 1 p.m. Eastern for WWDC, the keynote and all our club Twit members.
[03:10:35.400 --> 03:10:40.960]   Don't forget to go into the stage and you're going to be my co-hosts for that live stream.
[03:10:40.960 --> 03:10:42.040]   We'll see you in there.
[03:10:42.040 --> 03:10:44.480]   Thanks for joining us everybody. We'll see you next time.
[03:10:44.480 --> 03:10:46.520]   Another Twit is in the can.
[03:10:46.520 --> 03:10:47.040]   Bye bye.
[03:10:47.040 --> 03:10:48.120]   It's amazing.
[03:10:48.120 --> 03:10:58.120]   [Music]

