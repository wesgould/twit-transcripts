;FFMETADATA1
title=Artisanal Pickles from Williamsburg
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=727
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.840]   It's time for Twit this week in Tech Caroline Haskins is here from Motherboard.
[00:00:03.840 --> 00:00:08.800]   Mike Elgin has settled down from his trip around the world to this chair right here next to me.
[00:00:08.800 --> 00:00:14.480]   We're going to talk about the biggest fine in FTC history, Instagram versus the bullies,
[00:00:14.480 --> 00:00:19.920]   and Caroline will talk about what she's learned about a technology local cities are using to spy
[00:00:19.920 --> 00:00:22.960]   on you and me. It's all coming up next on Twit.
[00:00:25.520 --> 00:00:28.800]   Netcasts you love from people you trust.
[00:00:28.800 --> 00:00:34.160]   This is Twit.
[00:00:34.160 --> 00:00:49.360]   This is Twit this week in Tech. Episode 727 recorded Sunday July 14th 2019.
[00:00:49.360 --> 00:00:54.080]   Artisanal Pickles from Williamsburg. This week in Tech is brought to you by
[00:00:54.080 --> 00:00:59.520]   worldwide technology, worldwide technologies, advanced technology centers like no other testing
[00:00:59.520 --> 00:01:05.360]   and research lab with more than half a billion dollars of equipment including OEMs like NetApp
[00:01:05.360 --> 00:01:10.720]   and it's virtual so you can access it 24/7. To learn more and get insights into all it offers
[00:01:10.720 --> 00:01:18.720]   go to www.wt.com/twit. And by Mint Mobile. Mint Mobile provides the same premium network
[00:01:18.720 --> 00:01:24.000]   coverage you're used to but at a fraction of the cost because everything's online. To get your
[00:01:24.000 --> 00:01:32.000]   new wireless plan for $15 a month and get the plan shipped to your door free go to mintmobile.com/twit.
[00:01:32.000 --> 00:01:37.600]   And by Wasabi Hot Cloud Storage. Thinking about moving your data storage to the cloud,
[00:01:37.600 --> 00:01:44.480]   Wasabi is enterprise class cloud storage at 1/5 the price of Amazon S3 and up to six times faster
[00:01:44.480 --> 00:01:50.560]   with no hidden fees for egress or API requests. Calculate your savings and try Wasabi with free
[00:01:50.560 --> 00:01:58.560]   unlimited storage for a month at wasabi.com. Code Twit. And by ZipRecruiter. Hiring is challenging
[00:01:58.560 --> 00:02:04.000]   but there's one place you can go where hiring is simple and smart. That place is ZipRecruiter.
[00:02:04.000 --> 00:02:10.480]   We're growing businesses connect to qualified candidates. Try it free at ziprecruiter.com/twit.
[00:02:10.480 --> 00:02:19.120]   It's time for Twit this week at Tech. The show we cover the week's tech news.
[00:02:19.120 --> 00:02:26.000]   And look who's here in studio. Mike Elgin is joining us. He is off the road in town and whenever
[00:02:26.000 --> 00:02:31.120]   Mike's in town we'd like to get you in the studio. Thank you. It's great to be in town. He's a
[00:02:31.120 --> 00:02:38.320]   Gastronomad. In fact Gastronomad.net. What was your most was Barcelona your most recent event?
[00:02:38.320 --> 00:02:42.320]   Barcelona is the next one. Oh it's coming. So we still have some space open for this one.
[00:02:42.320 --> 00:02:48.560]   That's what this post is. Carva Barcelona. So what we do is everybody loves Barcelona but a lot of
[00:02:48.560 --> 00:02:54.240]   people don't know about the nearby Carva wine country. No. So we stay there for five nights, six days.
[00:02:54.240 --> 00:02:58.560]   And we explore a wine tasting, we make cheese, we do all this kind of stuff with
[00:02:58.560 --> 00:03:02.720]   baked bread but we take a couple of surgical strikes into the city of Barcelona.
[00:03:03.760 --> 00:03:10.560]   And it's just the most fantastic thing in the world. The food in Catalonia is incredible.
[00:03:10.560 --> 00:03:14.880]   We know exactly where the most authentic and best tapas places are. Where
[00:03:14.880 --> 00:03:21.360]   just like the best baker in Spain that woman there she gives us an exclusive.
[00:03:21.360 --> 00:03:27.040]   The best baker in Spain. Two years ago she was named the best baker in Spain.
[00:03:27.040 --> 00:03:32.800]   Wow. And she is amazing but she's a good friend. We go and we explore. If you go down,
[00:03:32.800 --> 00:03:35.440]   this is a good one. See this one right here? There's the cave.
[00:03:35.440 --> 00:03:39.440]   So this is Carva. They store it in in in a sparkling wine like sparkling wine.
[00:03:39.440 --> 00:03:41.840]   It's like you know it's like Prosecco champagne something like that.
[00:03:41.840 --> 00:03:46.240]   But this guy stores it in a bomb shelter from the Spanish Civil War. Wow.
[00:03:46.240 --> 00:03:50.400]   And it's in the middle of a vineyard. I can't tell you where it is or he'll...
[00:03:50.400 --> 00:03:52.880]   Generalists of the row of Francisco Franco's buried there.
[00:03:52.880 --> 00:03:57.440]   But it's a fascinating region and the culture of Catalonia and food which people don't quite get
[00:03:57.440 --> 00:04:00.480]   when you go to Barcelona on vacation. Oh quite get to it.
[00:04:00.480 --> 00:04:04.800]   Much fun. We go straight to it. I could spend years in Barcelona. You'd never plumb the depth.
[00:04:04.800 --> 00:04:08.480]   Yeah absolutely. Absolutely true. We also want to welcome a brand new
[00:04:08.480 --> 00:04:12.400]   member to our team. I'm really pleased to welcome from Motherboard the vice
[00:04:12.400 --> 00:04:19.360]   publication Caroline Haskins. Hi Caroline. Hi. Great to have you. Welcome. Don't be intimidated.
[00:04:19.360 --> 00:04:23.600]   This is going to be a lot of fun. We are going to start...
[00:04:24.320 --> 00:04:30.720]   Mike's shaking his head. He says it's not fun. We're going to start today with Facebook.
[00:04:30.720 --> 00:04:37.360]   The largest fine in the history of the Federal Trade Commission. The FTC had a consent decree with
[00:04:37.360 --> 00:04:41.920]   with Facebook in 2011 that they would not do whatever his Facebook does.
[00:04:41.920 --> 00:04:49.600]   They did it apparently. When some of our data went to Cambridge Analytica so the FTC got together
[00:04:49.600 --> 00:04:54.400]   voted three to two. Interestingly it was the Republicans have voted for the fine. The Democrats
[00:04:54.400 --> 00:05:00.640]   have voted against the fine. The largest fine in the history of the FTC five billion dollars.
[00:05:00.640 --> 00:05:05.840]   That sounds like a lot of money right? That's not a good Dr. Evil. Except Facebook's stock price
[00:05:05.840 --> 00:05:13.040]   went up after the fine six billion dollars. So Mark Zuckerberg is smiling because he made money.
[00:05:14.640 --> 00:05:21.520]   He made money. Facebook was smart. They had said in their annual and their quarterly report that
[00:05:21.520 --> 00:05:25.840]   they expected this five billion dollars. They budgeted it. They budgeted it. And so they prepared
[00:05:25.840 --> 00:05:31.200]   the market right? And then when the fine actually was five billion as usual you sell on the rumor
[00:05:31.200 --> 00:05:34.640]   you buy on the news or you buy on the news and you sell on the rumor I can't remember but whatever
[00:05:34.640 --> 00:05:41.120]   was good news. Everybody laments the political division of society but both Republicans and
[00:05:41.120 --> 00:05:46.080]   Democrats can get behind crushing Silicon Valley. That is an odd part of this issue.
[00:05:46.080 --> 00:05:52.320]   So Carolina is five billion dollars a slap on that sounds like a lot of money. Is it a slap on the
[00:05:52.320 --> 00:05:59.120]   wrist? It kind of seems like a drop in the bucket. I don't know. I remember a couple of months ago
[00:05:59.120 --> 00:06:02.880]   there were a couple of other figures that were being thrown around and it seems like they were
[00:06:02.880 --> 00:06:07.360]   potentially going to be hit with a lot more money but it really doesn't seem like this is going to
[00:06:07.360 --> 00:06:11.680]   have an impact on them at all. Kara Swisher said you really want to find them at a zero
[00:06:11.680 --> 00:06:18.560]   to the five. Make it 50 billion. Elizabeth Warren said Facebook made five billion in profits in the
[00:06:18.560 --> 00:06:23.600]   first three months of the year. Actually that's not true. I think it was revenue, not profits.
[00:06:23.600 --> 00:06:27.280]   The company is too big to oversee and this drop in the bucket penalty confirms that.
[00:06:27.280 --> 00:06:33.280]   It is true that I mean look at anything with a B is a lot of money for any company
[00:06:34.160 --> 00:06:41.520]   but it isn't, Facebook had 15 billion in revenue. Oh I'm sorry it is. 15 billion in revenue last
[00:06:41.520 --> 00:06:50.560]   quarter, 22 billion in profit. So it was one quarter profit. You could see the stock jump there.
[00:06:50.560 --> 00:06:58.320]   Mike Isaac said the real story is that Facebook shares surged with that fine. So clearly the
[00:06:58.320 --> 00:07:06.960]   market is saying slap on the wrist. You're not on Facebook. In fact I used you as an example, Mike
[00:07:06.960 --> 00:07:12.160]   Elgin because you have Mike's nice book. Yes. So what I did was last year I said I was getting
[00:07:12.160 --> 00:07:18.800]   off 4th of July and I spent the whole all these months this year harassing and criticizing Facebook.
[00:07:18.800 --> 00:07:21.760]   Of course they did no good at all because they were censoring my posts clearly.
[00:07:22.560 --> 00:07:29.120]   But I replaced it with a Google folder I call my nice book and it's a public folder. So anybody
[00:07:29.120 --> 00:07:32.960]   can follow it just as they would on Facebook. Yes. So if you search for Mike Elgin,
[00:07:32.960 --> 00:07:36.960]   nice book you'll find links to it and I invite you all. But this is my personal, this is my
[00:07:36.960 --> 00:07:41.120]   family. So it's really it's the kind of pictures you would have put exactly. Is there text though or
[00:07:41.120 --> 00:07:45.280]   is it just pictures? Text you can do posts. Where do you put your dank memes? Can you put them
[00:07:45.280 --> 00:07:49.680]   up there too? No you cannot. So each person's, this is the beauty of it. This is how social
[00:07:49.680 --> 00:07:54.400]   network should work. Only I can post in my nice book and I can follow your nice book and then I
[00:07:54.400 --> 00:07:58.880]   see it in the stream. So if you're on Google Photos and you go to sharing which nobody ever does,
[00:07:58.880 --> 00:08:05.040]   it's the it's the tab on the far right. You get all the stuff that's been shared with you all the,
[00:08:05.040 --> 00:08:09.280]   you know, and it's it's kind of like a social knock. So I think Google can turn it into a
[00:08:09.280 --> 00:08:16.080]   little bit. I follow your nice book. Yeah there it is. Yeah. So I could make one of these what
[00:08:16.080 --> 00:08:21.200]   if I wanted to make one I would just set up a sharing and say public. Yes. And it's kind of,
[00:08:21.200 --> 00:08:25.840]   I believe that's the default actually that is public and then you just share the link. Okay.
[00:08:25.840 --> 00:08:30.880]   With family and friends by email people can choose their own notifications. So some people get it on
[00:08:30.880 --> 00:08:34.880]   their phone. Some people get an email. Some people don't get the notifications. They just see it in
[00:08:34.880 --> 00:08:38.960]   Google Photos and they go. But it's beautiful. These photos are full quality. If you click on
[00:08:38.960 --> 00:08:42.880]   these photos, they're full quality much better than Facebook. Much better than Facebook. And you
[00:08:42.880 --> 00:08:48.640]   can you can add little, you know, comments you can like things and it's an it's a stream without ads.
[00:08:48.640 --> 00:08:53.520]   That's Mike's wife by the way, Amira who's so beautiful sitting in a restaurant that apparently
[00:08:53.520 --> 00:08:58.400]   has parking meters called the naughty pig. That's on Sunset Boulevard. Okay. Two nights ago.
[00:08:58.400 --> 00:09:04.400]   But it's it's a wonderful alternative. It's really what we want to do on Facebook is we want to
[00:09:04.400 --> 00:09:10.640]   see a stream of of our people we care about and the photos. Because I say get all the time. I say
[00:09:10.640 --> 00:09:15.120]   get off Facebook. I'm off Facebook. Caroline, you you have to. It's an occupational hazard. Be on
[00:09:15.120 --> 00:09:21.760]   all the networks probably. Yeah. Yeah. I feel like it would be sort of irresponsible for me to like
[00:09:21.760 --> 00:09:27.200]   not even know what's going on. I wish you couldn't you could like get rid of Instagram. I wish I could.
[00:09:27.200 --> 00:09:34.480]   I wish I could. It would be nice. The strange thing is that a lot of my peers still use Facebook
[00:09:34.480 --> 00:09:39.440]   for event invitations. And I've noticed people a little bit more recently using different types
[00:09:39.440 --> 00:09:45.680]   of forums. But it almost seems like for a lot of people at least in my immediate social circle that
[00:09:45.680 --> 00:09:50.000]   Facebook is still a way to guarantee that someone is going to be able to see something. But
[00:09:50.000 --> 00:09:54.240]   I don't know. I want to go back to the days of like personal websites.
[00:09:54.240 --> 00:09:59.360]   Well, you know, this is really nice. Yeah. I'm sorry to to to step on your
[00:09:59.360 --> 00:10:04.880]   your comment there, Caroline. But this is what I hear. I've talked in lots and lots of people all
[00:10:04.880 --> 00:10:09.600]   over the world, Leo, because in Europe, but you have a blog, I have a blog, but I know lots of people
[00:10:09.600 --> 00:10:14.800]   in Europe who use one, two or three social things. And those things are always Facebook properties.
[00:10:14.800 --> 00:10:20.240]   So I know people do Instagram and WhatsApp or Instagram and WhatsApp. And it's and that was one
[00:10:20.240 --> 00:10:24.240]   of the hardships for me is when I got rid of Facebook, I felt like, well, I should also
[00:10:24.240 --> 00:10:27.920]   give it a face Instagram. I didn't really use WhatsApp, but I should also I should get through
[00:10:27.920 --> 00:10:32.880]   all three. If I've any one of them on my phone, I've given Facebook an entree into my life. Absolutely.
[00:10:32.880 --> 00:10:38.000]   And this is the interesting thing about WhatsApp. Americans don't use WhatsApp much, but everybody
[00:10:38.000 --> 00:10:42.320]   else. It's well, and that's what I hear when I say get off Facebook, what I universally hear is
[00:10:42.320 --> 00:10:48.880]   that's a that's a privileged point of view. I can't get off this is the problem with Facebook.
[00:10:48.880 --> 00:10:54.640]   We're we're on the brink where we everybody is the internet. Well, exactly. And some people
[00:10:54.640 --> 00:10:59.120]   some lots of lots of people say I don't use the internet. I use Facebook. Right. And so it's
[00:10:59.120 --> 00:11:05.840]   AOL for the 21st century. And the scheme that Mark Zuckerberg is hatching as we sit here is he's
[00:11:05.840 --> 00:11:09.920]   trying to turn it into WeChat. So in China, WeChat is everything. It's an e-commerce. That's why
[00:11:09.920 --> 00:11:14.640]   Libra coin. That's why he got a currency because he wants this to be everything goes through Facebook.
[00:11:14.640 --> 00:11:18.720]   And so we're already like everybody I talked to says, Oh, I'm not going to get off Facebook
[00:11:18.720 --> 00:11:22.960]   because I need it for this reason or that reason. Caroline has her reasons. Everybody has a reason
[00:11:22.960 --> 00:11:28.800]   why they need Facebook or Instagram or WhatsApp. And this is a problem. We shouldn't. Caroline,
[00:11:28.800 --> 00:11:33.280]   I had heard that younger people thought maybe maybe you're not as young as you look. But younger
[00:11:33.280 --> 00:11:41.920]   people said Facebook is like your parents social network. And there and people under 30 say are
[00:11:41.920 --> 00:11:49.280]   Snapchat or TikTok. There are other places. Right. Right. Yeah. I think there's definitely still that
[00:11:49.280 --> 00:11:55.760]   reputation. You but in college, did you use Facebook? My daughter is 27 still is on Facebook because
[00:11:55.760 --> 00:12:03.280]   that's how she stays in touch with her college friends. I did. Yeah. I mean, Facebook was used
[00:12:03.280 --> 00:12:09.360]   in terms of like finding college roommates freshman year doing introductions. And that's sort of pivoted
[00:12:09.360 --> 00:12:14.400]   onto Instagram, which is, you know, again, still a Facebook property. So it seems like you're still
[00:12:14.400 --> 00:12:20.720]   sort of relying on this biggest common denominator. Like there's this assumption that you have an
[00:12:20.720 --> 00:12:27.040]   account on at least one Facebook. It's really the reason Facebook is an incumbent and it will be
[00:12:27.040 --> 00:12:33.600]   hard to unseat is so-called network effect. You have to be where everybody is. There's no point.
[00:12:33.600 --> 00:12:42.000]   I mean, I'm on Mastodon. I'm on L.O. But there's no point because it's me and a couple other people.
[00:12:42.000 --> 00:12:47.840]   There's I don't even know them and we're looking at each other gone. But this is true. But here's
[00:12:47.840 --> 00:12:53.120]   the problem. So the only social network that's bigger than Facebook is email, which is about
[00:12:53.120 --> 00:12:56.720]   three or four times the size of Facebook. And you're a big proponent of mail newsletters.
[00:12:56.720 --> 00:13:00.960]   But the problem is people have lost control of their inboxes. They're afraid of it and they don't
[00:13:00.960 --> 00:13:05.600]   want to go there. Yeah. And so but if people could just get a handle on their email inboxes,
[00:13:05.600 --> 00:13:10.560]   email would be the ultimate social network because you could be on Mastodon and use email-based
[00:13:10.560 --> 00:13:14.240]   notifications and it'd be like everybody's on the same social network. But people are
[00:13:14.240 --> 00:13:17.360]   reticent to use email. I often say that if you want to stay in touch with your college friends or
[00:13:17.360 --> 00:13:23.680]   your grandparents, email would work, right? But I guess it's not that there's not that.
[00:13:23.680 --> 00:13:28.320]   But it's it's email should be a delivery mechanism for the notifications. It feels like it's a one-on-one
[00:13:28.320 --> 00:13:32.960]   conversation email. It doesn't feel like that stream like you're tapping into what's happening. I want
[00:13:32.960 --> 00:13:37.760]   to be in wait into the pool of the zeitgeist. And this is how the social networks get us.
[00:13:37.760 --> 00:13:42.960]   They they've tapped into our psychology and makes us feel like we've got our finger on the pulse.
[00:13:42.960 --> 00:13:48.320]   Whereas email feels like your finger is not on the pulse. And so it's the it's all these feel
[00:13:48.320 --> 00:13:54.400]   you know feel-based emotion-based tricks that they use. The little red thing with a number in it.
[00:13:54.400 --> 00:14:00.240]   People have to click on that. So Caroline, should it have would it would it does the FTC have any
[00:14:00.240 --> 00:14:05.040]   solution? If it had been 500 billion, would have that have mattered? It doesn't it feels like the
[00:14:05.040 --> 00:14:12.560]   FTC is is helpless in the face of this. Right. It's hard to say. I mean without I mean a fine is
[00:14:12.560 --> 00:14:19.040]   one thing. But I mean regulations that are going to be preventing something like an
[00:14:19.040 --> 00:14:25.280]   a Cambridge analytical situation from happening again would be. I mean more ideal and I mean that
[00:14:25.280 --> 00:14:32.240]   doesn't look like I mean I know that legislation is on the way in California but it doesn't seem like
[00:14:32.240 --> 00:14:38.560]   the regulations we need are on the way any time any time soon. So it's hard to say whether a fine
[00:14:38.560 --> 00:14:43.520]   would be like a hundred percent the answer here. And a lot of times when we talk about regulation
[00:14:43.520 --> 00:14:48.000]   there's always this issue of unintended consequences. Casey Newton has a great article this week
[00:14:48.000 --> 00:14:55.280]   on the verge talking about the ugly side of Facebook's pivot to privacy. So Facebook's solution was this
[00:14:55.280 --> 00:14:59.680]   Mark Zuckerberg post saying oh okay no problem everything will be end-to-end encrypted private.
[00:14:59.680 --> 00:15:07.360]   But as Casey points out now what you have is private groups like that horrific border patrol
[00:15:07.360 --> 00:15:14.160]   group the pro public have found last week. It's private. It's you know they had to dig to find it.
[00:15:14.160 --> 00:15:24.000]   Le Monde found in France a group of 56,000 people devoted largely to making anti-female comments
[00:15:24.000 --> 00:15:31.440]   French group. A Twitter user stumbled across a Facebook TV ad investigated one of the featured
[00:15:31.440 --> 00:15:38.800]   groups and found a Facebook TV ad. Oh he found it. And he posted some of the ugly posts.
[00:15:38.800 --> 00:15:44.400]   Large private this is Elizabeth Dwaskin writing in the washer post. Large private groups remain
[00:15:44.400 --> 00:15:51.440]   unmoderated black boxes where users could freely threaten vulnerable populations. Actually she's
[00:15:51.440 --> 00:15:56.880]   quoting Jonathan Greenblatt who's with the Eddie Defamation League. Without any AI or human
[00:15:56.880 --> 00:16:02.320]   moderators it's easy to orchestrate harassment campaigns. It becomes 4chan or 8chan. It becomes
[00:16:02.320 --> 00:16:08.160]   it becomes a secret place for for evilness. So that's not a solution either. I'm less I mean
[00:16:08.160 --> 00:16:12.880]   this is gonna sound horrible but I'm less bothered by this sort of thing because in private you don't
[00:16:12.880 --> 00:16:20.160]   mind. Well it's not that I don't mind. I mind that people are evil bothers me but they're going
[00:16:20.160 --> 00:16:24.560]   they're going to be. But at least they're not in my face evil. Well I mean if it's to me this is
[00:16:24.560 --> 00:16:28.960]   more akin to just having a an evil conversation in your living room or whatever. Yeah that's gonna
[00:16:28.960 --> 00:16:35.200]   happen. What bothers me is the viral mechanisms of using algorithms to like bring in lots of people
[00:16:35.200 --> 00:16:39.840]   and sort of change their minds about. See I thought I always thought maybe this was naive of me but that
[00:16:39.840 --> 00:16:46.000]   light was a disinfectant that you lift up the rock and you let the world see this and the world will
[00:16:46.000 --> 00:16:54.320]   rear back in revulsion and say no this will not stand. We don't want nazis but what is in fact
[00:16:54.320 --> 00:17:01.440]   happen is that nazis have taken over Twitter. It's it's not there's no revulsion but but
[00:17:01.440 --> 00:17:06.400]   well the unfortunate thing is the unfortunate thing is that yahoo's
[00:17:06.400 --> 00:17:13.600]   alga or YouTube's algorithms sees revulsion as that's positive that's engagement I don't care if
[00:17:13.600 --> 00:17:19.040]   you're disgusted it's engagement. It's who who's it that said that it's like people slow down to look
[00:17:19.040 --> 00:17:23.360]   at a car accident and so the internet responds by producing more car accidents. Yeah
[00:17:24.320 --> 00:17:29.040]   this is the speech this is the slavish following of the algorithm is a bad thing I would I've often
[00:17:29.040 --> 00:17:33.280]   said that the real problem with YouTube is the recommendation engine the algorithm clearly the
[00:17:33.280 --> 00:17:38.560]   problem with Facebook is the algorithm that drives the newsfeed because absent any human
[00:17:38.560 --> 00:17:43.520]   oversight these are optimized for engagement because that's more revenue and that's where you get
[00:17:43.520 --> 00:17:51.040]   these horrible vicious circles that promote offensive behavior. And to me the elephant and
[00:17:51.040 --> 00:17:56.400]   the liver and that's not a GOP pun by the way the elephant in the living room is is is which I don't
[00:17:56.400 --> 00:18:00.080]   think he's exploring here enough is that you talked about the disinfectant quality of light.
[00:18:00.080 --> 00:18:07.040]   What what Zuckerberg has proposed for his big privacy pivot which is complete BS is to unite all
[00:18:07.040 --> 00:18:13.040]   of his net all of his messaging platforms and have end-to-end encryption so nobody can see inside
[00:18:13.040 --> 00:18:18.400]   that black box. Yeah and then and the other part of it is they all the solution to bullying and hate
[00:18:18.400 --> 00:18:24.000]   speech and all that stuff are algorithms that work behind the scenes that nobody can see how they
[00:18:24.000 --> 00:18:28.240]   work or what they're doing and all that kind of stuff so we're entering into an era where all
[00:18:28.240 --> 00:18:35.200]   this stuff where the conversations are happening beyond anyone's ability to see it and also where
[00:18:35.200 --> 00:18:38.960]   the solution the algorithms are happening behind the scenes with nobody's ability to see it. This is
[00:18:38.960 --> 00:18:44.000]   kind of like a golden age of journalists can go and expose these things and write articles like
[00:18:44.000 --> 00:18:47.920]   Casey Newton's article. That's not going to happen. We're on the brink of him not having any access
[00:18:47.920 --> 00:18:53.040]   to be able to find it. He's a secret. Poor Caroline you know you're going on the grumpy old men
[00:18:53.040 --> 00:18:59.840]   complaint about the you kids today show you. In my day we didn't do this because we didn't have any
[00:18:59.840 --> 00:19:06.320]   of it. I just think it's strange that this whole pivot to groups and like emphasizing groups on
[00:19:06.320 --> 00:19:12.000]   Facebook has been presented as this solution both to like hate speech and fake news and from
[00:19:12.000 --> 00:19:17.760]   making Facebook just a place to want that people want to go. I mean anecdotally it seems pretty
[00:19:17.760 --> 00:19:23.360]   clear that Facebook groups is one of the only reasons that people would go on Facebook for
[00:19:23.360 --> 00:19:31.520]   pleasure but in terms of that's interesting. Yeah yeah in terms of like I don't know mean groups
[00:19:31.520 --> 00:19:36.320]   or like the rise of certain Facebook groups where it's like Facebook group where we all pretend to
[00:19:36.320 --> 00:19:43.120]   be ex like for instance a really popular one is a group where we all pretend to be boomers and
[00:19:43.120 --> 00:19:47.440]   everybody makes one of the way that their parents tell me you're not in that group.
[00:19:47.440 --> 00:19:56.160]   I joined for journalistic purposes. Oh yeah sure. I think we boomers should invade Area 51. Oh no
[00:19:56.160 --> 00:20:04.000]   that's another one. Sorry. Yeah that's an event. That's a big event. Oh that's interesting. See
[00:20:04.000 --> 00:20:10.880]   this is where I am at a disadvantage so Facebook has kind of pivoted from what its original promise
[00:20:10.880 --> 00:20:16.880]   was you follow the news feed and see what your friends and family are up to to finding affinity
[00:20:16.880 --> 00:20:20.800]   groups and we know affinity groups are the worst thing in the world because really what happens is
[00:20:20.800 --> 00:20:27.200]   whatever you know it's a mob mentality whatever that prime factor for that affinity group is
[00:20:27.200 --> 00:20:35.840]   becomes amplified. It depends. I mean they say that certain types of flagged posts or fake
[00:20:35.840 --> 00:20:42.800]   news will be deprioritized even in private Facebook groups which I mean I think personally at least
[00:20:42.800 --> 00:20:48.880]   in some cases I mean not in the case of like outright hate speech but in terms of certain gray areas
[00:20:48.880 --> 00:20:54.000]   it might be a better solution than just pulling the stuff off the internet entirely and just having
[00:20:54.000 --> 00:21:00.560]   really no grasp on how different communities are spreading different kinds of information but
[00:21:01.680 --> 00:21:07.360]   you know at the same time pretending that Facebook groups is going to be like solving all of Facebook
[00:21:07.360 --> 00:21:14.000]   problems while I mean your feed is going to still be including a lot of the same type of content.
[00:21:14.000 --> 00:21:19.520]   I mean at the end of the day it's just going to be like a category specific news feed with a
[00:21:19.520 --> 00:21:25.440]   couple of specific people. I mean when you open Facebook right now I mean almost every time I open
[00:21:25.440 --> 00:21:31.040]   it it'll say right at the top. This many of your friends have joined Facebook groups and
[00:21:31.040 --> 00:21:38.800]   find new Facebook groups. It's so funny because Google+ in its last dying days did the same pivot.
[00:21:38.800 --> 00:21:44.320]   It didn't save it but they said join a community. So there must be some research that says oh yeah
[00:21:44.320 --> 00:21:49.120]   people are happiest when they're in a community. Yeah and people do terrible though because the
[00:21:49.120 --> 00:21:57.040]   communities are horrific. My biggest or awesome yeah thank you Carsten.
[00:21:57.040 --> 00:22:04.560]   My biggest what Carsten what groups are you in? I also am a Facebook refugee but oh okay he's one
[00:22:04.560 --> 00:22:12.480]   of those guys who tends to be a boomer. There's definitely something to be said for affinity groups
[00:22:12.480 --> 00:22:18.480]   where people can find people like them. Well yeah in a way that's what geeks did in the very
[00:22:18.480 --> 00:22:25.920]   beginning. That's what we did. We found each other. You know I keep thinking that the platforms can't
[00:22:25.920 --> 00:22:32.080]   solve this. It's not going to be and government can't solve it. I only humans can solve it but humans
[00:22:32.080 --> 00:22:38.640]   were so flawed. I don't know if there's a solution there either. I feel like it's I'm starting to
[00:22:38.640 --> 00:22:42.960]   despair. But we have to be clear about what the problem is and it's why it bothers me that
[00:22:42.960 --> 00:22:47.760]   you know that somebody is saying something mean on the internet somewhere is not the problem.
[00:22:47.760 --> 00:22:52.080]   Right if people should be able to have private groups no matter what they say to each other in
[00:22:52.080 --> 00:22:57.040]   those groups. Private conversations people should be able to say on the telephone for example.
[00:22:57.040 --> 00:23:00.400]   You shouldn't have an algorithm to listen to the conversation. Nobody should. You shouldn't talk
[00:23:00.400 --> 00:23:06.720]   about that. Yeah and that's historically the case. That's what section 230 of the internet whatever
[00:23:06.720 --> 00:23:12.720]   it's called. Decency collaboration. What is it called? I don't know what it's called.
[00:23:12.720 --> 00:23:20.880]   The communications decency act of 1996 I think. Section 230 said that just like the phone system
[00:23:20.880 --> 00:23:26.640]   you can't expect platforms online services to police content. They're just conveyors. Yes.
[00:23:26.640 --> 00:23:31.280]   Content. And so it does by the way people want to, President Trump wants to get rid of
[00:23:31.280 --> 00:23:38.000]   section 230. There's a move afoot to do that. So that may be actually under assault anyway. But
[00:23:38.000 --> 00:23:41.760]   I don't know if that protects it. I don't know what the solution is. I really don't. I really
[00:23:41.760 --> 00:23:47.520]   think we need to focus on algorithms and amplification. That's what Facebook says algorithms. And not
[00:23:47.520 --> 00:23:52.560]   no but focus on the problem with algorithms. Oh and not the focus on private little groups
[00:23:52.560 --> 00:23:56.480]   where our NASA little people talk to each other about it. Let them do their things in private.
[00:23:56.480 --> 00:24:00.000]   They try to make it seem like a bigger problem. I say oh they could be plotting and planning all
[00:24:00.000 --> 00:24:03.120]   the stuff. They could be. Yes you know that's going to that stuff is going to happen and when
[00:24:03.120 --> 00:24:06.720]   they break the law they need to go to prison. Right. When they break the law in real life but
[00:24:06.720 --> 00:24:11.040]   but whatever happened to free speech we should have a place for free speech. What we shouldn't have
[00:24:11.040 --> 00:24:14.960]   is a place where it amplifies by platforms. It amplifies by platforms. Yeah yeah yeah.
[00:24:14.960 --> 00:24:20.080]   And so Jason Calicanos of all people proposed a new social network which he described as a
[00:24:20.080 --> 00:24:25.920]   simple feed with no algorithmic filtering. One ad on the top of the page you can pay $50 a month
[00:24:25.920 --> 00:24:31.120]   to get rid of the ad and it's just you get everything that the people you follow post in reverse
[00:24:31.120 --> 00:24:34.720]   chronological order. End of story. It's kind of what Facebook was supposed to be in the beginning.
[00:24:34.720 --> 00:24:40.080]   Exactly but there's no money in that. Right. I mean you can't be wildly you can't end up
[00:24:40.080 --> 00:24:43.520]   being the most powerful person in the world and one of the richest. There has to be somewhere in
[00:24:43.520 --> 00:24:50.160]   between no money and $22 billion a year. There has to be some mid-ground where you make you make a
[00:24:50.160 --> 00:24:56.880]   decent living. It's a good service. People use it. It's called Macedon. Yeah. The pro I mean I don't
[00:24:56.880 --> 00:25:02.320]   give a crap if Mark Zuckerberg is rich. I really don't care if he makes a lot of money. What I care
[00:25:02.320 --> 00:25:06.400]   about is that people can have conversations. We have free speech but we also have and we don't
[00:25:06.400 --> 00:25:11.120]   amplify the negative speech because it's good for engagement and all this kind of stuff.
[00:25:11.120 --> 00:25:14.400]   So that's where the problem is. It's not these little private groups I don't think.
[00:25:14.400 --> 00:25:20.080]   And then there was this week's social media summit at the White House where the president said
[00:25:20.080 --> 00:25:28.480]   to a bunch of kind of conspiracy theory social media folks the crap you guys think of is unbelievable.
[00:25:29.520 --> 00:25:37.760]   It is. Even though it's often believed I guess they did not invite Facebook or Jack Dorsey from
[00:25:37.760 --> 00:25:42.880]   Twitter anybody who was actually from one of the platforms to this quote social media summit was
[00:25:42.880 --> 00:25:47.440]   really just a bunch of it was mostly right wing right wing pundits. They had to pull a few
[00:25:47.440 --> 00:25:52.240]   at the last night because they found all kinds of anti-Semitic and racist stuff. And of course
[00:25:52.240 --> 00:25:58.160]   Josh Hawley is proposed legislation that would eliminate section 230 treat every political
[00:25:58.160 --> 00:26:04.240]   opinion equally because of this. I don't think there's any evidence for but there's a belief that's in
[00:26:04.240 --> 00:26:10.000]   the conservative circles that somehow Twitter and Facebook can you know are biased against
[00:26:10.000 --> 00:26:16.720]   conservatives. I don't see any evidence of that but the evidence is I mean this is actually one
[00:26:16.720 --> 00:26:21.520]   of the problems you don't have insight into how they why they make the decisions they make. So
[00:26:21.520 --> 00:26:25.120]   for example let's say there's some well it is invisible isn't it the algorithm is an all right
[00:26:25.120 --> 00:26:29.360]   person who posts an alt right type post and then gets banned and then all of his followers
[00:26:29.360 --> 00:26:33.520]   members are like ah you see it's conservative bias but what they don't know is that Twitter
[00:26:33.520 --> 00:26:37.840]   might tell you yeah but they were also had this other account they had these five other accounts
[00:26:37.840 --> 00:26:41.760]   where they were orchestrating bots and doing all this other stuff and the combined effect of all
[00:26:41.760 --> 00:26:46.000]   those things were why we banned and and but again we have to take the word for it because
[00:26:46.000 --> 00:26:52.480]   algorithms are company secrets they don't have to reveal them and they won't. So they're you know
[00:26:52.480 --> 00:26:55.920]   the Facebooks and the Twitters are really on the spot they're essentially the police and the
[00:26:55.920 --> 00:27:01.040]   judge and the jury and the everything. So that's Trump's Trump said evidence that there's a bias
[00:27:01.040 --> 00:27:05.920]   against him is because in the old days he would tweet that the Obama campaign was spying on him
[00:27:05.920 --> 00:27:10.240]   and it would in his words take off like a rocket and now it doesn't so much anymore.
[00:27:10.240 --> 00:27:15.200]   And it's like that's your evidence. Yeah and I you know he's got how many tens of millions of
[00:27:15.200 --> 00:27:20.720]   followers I think it's pretty clear Twitter is not blocking followers. The president has
[00:27:20.720 --> 00:27:25.520]   narcissistic personality disorder if Georgia were here she could diagnose it for us but
[00:27:25.520 --> 00:27:29.520]   but the problem this whole summit seemed to be a place for him to vent and say about him the
[00:27:29.520 --> 00:27:32.560]   fact that I don't have more followers than Obama is should have more followers.
[00:27:32.560 --> 00:27:35.680]   I should have more followers. There's something has to change and we're going to do something and he
[00:27:35.680 --> 00:27:40.400]   was threatening. You know I should have more followers. So should I. Yeah okay I'm just saying.
[00:27:40.400 --> 00:27:46.000]   Caroline tweets were printed on poster boards. So so and aside that shows you right that's it right
[00:27:46.000 --> 00:27:51.360]   there. That's things boomers do print out tweets and put them on a poster board.
[00:27:51.360 --> 00:27:57.360]   That's what they do in the group that she's in. So so so Instagram used to be kind of the
[00:27:57.360 --> 00:28:02.800]   refuge you would go to Instagram all it's nice it's pictures it's nice and now Instagram is just
[00:28:02.800 --> 00:28:07.520]   as corrupt and nice and messed up. Caroline you've written about that it's just as bad
[00:28:07.520 --> 00:28:15.120]   on Instagram right. I haven't been in a while so is it. Yeah they've they've had a problem with
[00:28:15.120 --> 00:28:22.240]   a rise of like certain types of like extremist hate accounts with QAnon accounts with anti-vaxxer
[00:28:22.240 --> 00:28:28.080]   accounts and Instagram's been taking some steps to design infrastructure that blocks certain hashtags
[00:28:28.080 --> 00:28:35.600]   like in the same way that if you search say for things like pro anorexia those hashtags are blocked
[00:28:35.600 --> 00:28:41.840]   and so they're working on doing the same kind of thing for for anti-vaccinations and those types of
[00:28:41.840 --> 00:28:49.040]   ideologies but I mean the last time I checked that feature hadn't rolled out but I mean yeah it's
[00:28:49.040 --> 00:28:55.840]   it's just strange that you there was a period when I went on Facebook and you know you search for
[00:28:55.840 --> 00:29:00.880]   anti-vaxxer content and there were I mean there were no results like like the groups seemed to
[00:29:00.880 --> 00:29:07.840]   have been banned but then when you went on Instagram all those accounts were still up. Yeah yeah yeah
[00:29:07.840 --> 00:29:11.760]   I don't know in terms of the social media summit I think one important thing to keep in mind is
[00:29:11.760 --> 00:29:18.480]   that he also invited I think it was the CEO or whoever founded that that social network network
[00:29:18.480 --> 00:29:27.520]   mines M.I.N.D.S. which for at least a certain period of time was a like a favored group among I
[00:29:27.520 --> 00:29:36.000]   mean it was like it was like GAB yeah exactly um I don't know I think it speaks volumes it goes
[00:29:36.000 --> 00:29:41.360]   I think it goes beyond trumping angry about himself and trying to send a political message by I mean
[00:29:41.360 --> 00:29:50.640]   obviously the types of people that he invited to that. He has to see these social networks as
[00:29:50.640 --> 00:29:57.040]   very powerful for the 2020 election as places that they can use I mean he does he uses Twitter
[00:29:57.040 --> 00:30:03.360]   very effectively yeah and they have to see these as the place where the campaign is going to play
[00:30:03.360 --> 00:30:08.560]   out next year right that's that's that's where it's going to matter the truth is that scares me
[00:30:08.560 --> 00:30:13.200]   about everybody's talks about how Twitter is so influential in shaping public opinion all this
[00:30:13.200 --> 00:30:18.240]   kind of stuff and it just isn't true what happens is the media picks up on Twitter but that's the
[00:30:18.240 --> 00:30:22.560]   respect it's true is that yeah Twitter is only a couple hundred million people it's not a big deal
[00:30:22.560 --> 00:30:27.040]   but the media treats it as if it's the most important thing in the world and if anything happens on
[00:30:27.040 --> 00:30:32.720]   Twitter suddenly that's a story to be clear on the nightly now. The choices are what make
[00:30:32.720 --> 00:30:38.080]   everything that happens on Twitter such a big deal if if the media completely ignored what happened
[00:30:38.080 --> 00:30:43.600]   on Twitter they would have no effect and it's really not. Have you seen this yet on Carolina on
[00:30:43.600 --> 00:30:50.480]   Instagram where if you put comments in that are I guess bullying that it will they won't say wait
[00:30:50.480 --> 00:30:57.200]   a minute do you really want right do you want to say that let's keep Instagram a supportive place
[00:30:57.200 --> 00:31:02.400]   rethink that comment. It's really interesting because I went to
[00:31:02.720 --> 00:31:07.440]   I went to a meeting with Instagram a couple months ago when they were talking about the things
[00:31:07.440 --> 00:31:12.560]   that they were doing to try and combat bullying and I think a large part of the difficulty in
[00:31:12.560 --> 00:31:20.720]   moderating that type of thing is I mean a defining bullying and b figuring out a way to do that
[00:31:20.720 --> 00:31:26.160]   that isn't just that isn't preemptive. I mean they don't want to censor it they just want to
[00:31:27.360 --> 00:31:32.880]   right I mean even in the example that they used I think they said like oh if you're using the words
[00:31:32.880 --> 00:31:39.680]   I mean it's clear that like the words like ugly and stupid triggered that sort of pop up to say
[00:31:39.680 --> 00:31:45.680]   oh you want to say that but is that really the ways that are those really the mechanisms that
[00:31:45.680 --> 00:31:52.720]   people use to like make others feel excluded they were talking about also like induced FOMO
[00:31:52.720 --> 00:32:02.560]   which is it's difficult to define it's basically where the answer is Instagram is made of FOMO
[00:32:02.560 --> 00:32:10.400]   that's the problem like it's like the whole idea is that if you post something at a certain place
[00:32:10.400 --> 00:32:16.640]   and a person may not have been invited and you say you at the person and you say oh don't you
[00:32:16.640 --> 00:32:22.320]   wish you were here do people actually do that I think that was the quote that's that was one of
[00:32:22.320 --> 00:32:26.400]   the main questions that I had coming out of that but don't you think they do haven't you ever
[00:32:26.400 --> 00:32:31.280]   seen stuff like that I'll tell you where I see it every time I go to a concert every time I go
[00:32:31.280 --> 00:32:37.040]   to a concert now most of the selfie taking is the person turned away from the artist
[00:32:37.040 --> 00:32:44.720]   videoing the artist singing and them in the picture and what's the whole point of doing that to show
[00:32:44.720 --> 00:32:51.920]   all your friends what a great time how good your life is I'm at the concert if you if it used to be
[00:32:51.920 --> 00:32:56.480]   video the singer right because oh you love this singer you're gonna like the video now it's
[00:32:56.480 --> 00:33:06.000]   look at me at the concert that's that's FOMO that's induced FOMO right so they clearly were trying to
[00:33:06.000 --> 00:33:13.120]   make it seem as if that sort of thing is malicious oh it's not malicious it's just neurotic it's just
[00:33:13.120 --> 00:33:19.600]   it's just a narcissistic I'm hesitant almost to judge people that want to like document those
[00:33:19.600 --> 00:33:25.040]   types of experience yeah right yeah you know if you want to if you want to share that if that's what
[00:33:25.040 --> 00:33:29.040]   if that's what makes you feel good if you want to make sure you capture something that made you
[00:33:29.040 --> 00:33:34.240]   happy and I'm hesitant to be like oh that's bad but when you when you're trying to find what actually
[00:33:34.240 --> 00:33:42.080]   is bad and what is harmful I don't know I mean it seems like a lot of the I mean the social ways in
[00:33:42.080 --> 00:33:47.920]   which people try and exclude and alienate and make other people feel bad I mean that's not something
[00:33:47.920 --> 00:33:54.480]   that an algorithm can easily pick up right and this is only rolling out on the English-speaking
[00:33:54.480 --> 00:34:01.440]   part of Instagram so I mean this is only going to be affecting like a marginal part of their audience
[00:34:01.440 --> 00:34:07.440]   anyway surely we're talking about children though right I mean adults don't like so one of the
[00:34:07.440 --> 00:34:13.200]   problems they they they have is that they talked about how some bullying victims if they block
[00:34:13.200 --> 00:34:17.920]   somebody it's like a big problem because they they're with the bully every day in other words they go
[00:34:17.920 --> 00:34:23.120]   to school with the bully and so this is even blocking and the mechanisms that exist for people to
[00:34:23.120 --> 00:34:28.960]   to protect themselves don't work because of the of the fact that people are you know in real
[00:34:28.960 --> 00:34:33.920]   con communication that sounds to me like high school or junior high or something like that I mean it's
[00:34:33.920 --> 00:34:39.520]   right I have to say when I when I was posting stuff on Instagram I would I would actually
[00:34:39.520 --> 00:34:46.240]   consciously not try to show off on Instagram because it felt like it was about like look how
[00:34:46.240 --> 00:34:52.160]   great my life is and I was concerned it you know that when you're looking at these people's lives
[00:34:52.160 --> 00:34:57.040]   that's not that's just the slice that makes it look good oh yeah everybody's life has equal amounts
[00:34:57.040 --> 00:35:03.120]   of bad stuff and good stuff and to just put the pictures of the good stuff so I try to take pictures
[00:35:03.120 --> 00:35:08.800]   of all the bad stuff that happened to make people feel better it's very freeing to have low standard
[00:35:08.800 --> 00:35:13.680]   Lee is going to the bathroom again it's very freeing to have low standards that's kind of that's
[00:35:13.680 --> 00:35:20.480]   gonna go in my tombstone actually thank you Caroline well you're welcome we are we are very pleased
[00:35:20.480 --> 00:35:26.400]   to have Caroline with us Caroline haskins is a reporter for motherboard and vice a great site
[00:35:26.400 --> 00:35:30.320]   in fact I want to talk about Palantir when we come back because you did just did a great piece
[00:35:30.320 --> 00:35:35.760]   on this software this Palantir software it's really kind of scary also Mike Elgin off the road
[00:35:35.760 --> 00:35:42.080]   and in our studios great to have you Elgin calm and of course you could always subscribe to Mike's
[00:35:42.080 --> 00:35:48.960]   nice book yes although there's a lot of FOMO for me yes of you and Barcelona drinking whining if you
[00:35:48.960 --> 00:35:55.840]   want to see an example of this FOMO we've been talking about go to my nice book what a life
[00:35:55.840 --> 00:36:03.920]   he's he's is beautiful though isn't it there's nothing like cheese are you there's a that's definitely
[00:36:03.920 --> 00:36:08.720]   things baby boomers say right exactly it's in that it's in that there's nothing like cheese
[00:36:08.720 --> 00:36:17.920]   I'm not going to say you're wrong no I know I'm not wrong I know I know I'm the poster child
[00:36:17.920 --> 00:36:24.320]   our show today brought to you by worldwide technology WWT they began with their advanced
[00:36:24.320 --> 00:36:30.560]   technology center 10 years ago the lab contains more than half a billion dollars in equipment
[00:36:30.560 --> 00:36:37.760]   from hundreds of OEMs everything you might want to use in your business Cisco NetApp VMware emerging
[00:36:37.760 --> 00:36:42.400]   disruptors too maybe things you you just recently heard about and curious about like Tanium and
[00:36:42.400 --> 00:36:48.960]   Equinix and Expanse they're all in there and because when you're running an enterprise nothing exists
[00:36:48.960 --> 00:36:55.280]   in a vacuum they test it all you know in a lab that gets you an example how they're going to
[00:36:55.280 --> 00:37:01.040]   interact how they're going to work together WWT worldwide technology is your trusted partner
[00:37:01.040 --> 00:37:06.080]   been with you over the years many of their customers have been with them for more than a decade
[00:37:06.080 --> 00:37:10.320]   because they know WWT is where they can go to get the answers they need to make sure their business
[00:37:10.320 --> 00:37:15.360]   runs right and this advanced technology center is something else it's amazing an incubator for
[00:37:15.360 --> 00:37:21.200]   IT innovation and you can use it this is the best part they've got hundreds of on-demand
[00:37:21.200 --> 00:37:26.800]   integrated solution labs that you can use representing the newest advances in things like flash
[00:37:26.800 --> 00:37:32.400]   storage and software defined networking network automation end point security architectures
[00:37:32.400 --> 00:37:37.840]   before you try a product before you install a product you can learn about it of course WWT's
[00:37:37.840 --> 00:37:43.040]   own engineers are using it all the time to spin up proof of concepts pilots using the sandbox
[00:37:43.040 --> 00:37:50.000]   customers can confidently see and choose the right solutions but you can use it too lab is a service
[00:37:50.000 --> 00:37:54.640]   they call it it's a dedicated lab space within the advanced technology center that customers
[00:37:54.640 --> 00:38:00.240]   can use to perform programmatic testing using that incredible half billion dollar technology
[00:38:00.240 --> 00:38:06.240]   ecosystem WWT's already built and there's more coming in all the time and it's completely virtual
[00:38:06.240 --> 00:38:09.920]   you don't have to go to st. Louis you could take advantage of the atc's unique benefits
[00:38:09.920 --> 00:38:16.320]   anywhere in the world anytime of the day or night 24/7 later this summer WWT is going to launch a
[00:38:16.320 --> 00:38:22.640]   new digital platform that gives you hands-on access to more than 200 lab environments this is a big
[00:38:22.640 --> 00:38:27.600]   deal if you're looking at enterprise purchases if you're looking at setting up networks if you're
[00:38:27.600 --> 00:38:34.640]   in it if you just want to understand how stuff works learn more about WWT the atc sign up for
[00:38:34.640 --> 00:38:39.920]   pre-launch access to their new on-demand lab platform at wwt.com/twit
[00:38:40.720 --> 00:38:47.520]   wwt.com/twit. WWT simplifies the complex delivering business and technology outcomes around the
[00:38:47.520 --> 00:38:54.960]   worldwide technology we thank him so much for supporting twit wwt.com/twit we thank you for
[00:38:54.960 --> 00:39:02.160]   supporting twit by visiting that address wwt.com/twit
[00:39:03.920 --> 00:39:14.080]   well we kind of did the facebook walk the instagram walk prime day tomorrow this is the eve of prime
[00:39:14.080 --> 00:39:25.520]   day are you is everybody excited yeah woof. First of all not everything in prime day is a deal right
[00:39:25.520 --> 00:39:32.560]   right the wire cutter does a great job the new york times of a wire cutter they do a great job
[00:39:32.560 --> 00:39:36.720]   of looking at prime day deals and comparing them to other prices to see if they are deals
[00:39:36.720 --> 00:39:44.320]   and oh by the way here's here's the best prime day deals for 2019 notice they're not all on Amazon
[00:39:44.320 --> 00:39:51.120]   in fact walmart is having their own version of prime day and they're they're doing it I think
[00:39:51.120 --> 00:39:55.920]   for three or four days targets doing it even Nordstrom later this week will be having their
[00:39:55.920 --> 00:40:00.960]   version of prime day I think rapidly the middle of the year is going to become the week for for
[00:40:00.960 --> 00:40:04.640]   you know like christmas and they used to call it christmas in july yeah yeah ebay as well
[00:40:04.640 --> 00:40:11.360]   ebay too wired magazine called uh amazon prime day a back canal of modestly discounted if
[00:40:11.360 --> 00:40:18.640]   i'm modestly just crap that you wouldn't ever buy i like what ebay's doing because ebay is
[00:40:18.640 --> 00:40:28.080]   they're calling it crash deals in case amazon crashes because i guess what the amazon crash last
[00:40:28.080 --> 00:40:35.120]   year during prime day unbelievable um i i don't know what to say about this except i'm staying
[00:40:35.120 --> 00:40:40.720]   home tomorrow yeah no i'm not i don't i don't want us i i'm coming to work you guys have much
[00:40:40.720 --> 00:40:48.480]   better bandwidth there oh yeah we do we have a lot of bandwidth do you uh do you buy stuff caroline
[00:40:48.480 --> 00:40:55.760]   on prime day do you go crazy is that where you get your battleship uh deals yeah i've not participated
[00:40:55.760 --> 00:41:02.560]   in prime day i think mainly because i mean this year last year definitely uh i mean there have been
[00:41:02.560 --> 00:41:07.840]   different types of demonstrations at warehouses um and people yeah minnesota shut it down shut
[00:41:07.840 --> 00:41:13.360]   down the warehouse tomorrow yeah like that's good i mean you got to shut them all down if you
[00:41:13.360 --> 00:41:17.920]   should shut it down they just route a rad use like the internet oh minnesota is down let's just use
[00:41:17.920 --> 00:41:22.240]   it let's get it from nebraska i don't here's a really interesting thing that happened to me
[00:41:22.240 --> 00:41:27.600]   regarding amazon very bizarre last year november december something like that i went to logan and
[00:41:27.600 --> 00:41:33.760]   said your account has been terminated and i was like okay that's terrible so i i've been calling
[00:41:33.760 --> 00:41:38.800]   and calling couple three four times a month trying to get my amazon account back and the reason i
[00:41:38.800 --> 00:41:43.680]   want to back is because i'm an author you don't have an amazon account correct how do you live i
[00:41:43.680 --> 00:41:50.080]   do but i don't so so i have a uh i'm an author so i have a book going this being sold i get checks
[00:41:50.080 --> 00:41:54.320]   from them i i you know i've done some amazon associate stuff i get checks from that but
[00:41:54.320 --> 00:41:59.200]   they couldn't figure it out i kept calling amazon they're like we have no idea we can't figure it out
[00:41:59.200 --> 00:42:03.440]   you're at here and then we finally figured it out they had a high level person look into it
[00:42:03.440 --> 00:42:10.080]   i was one of the first like you were uh audible oh subscribers and you're audible i can't i subscribed
[00:42:10.080 --> 00:42:16.160]   in 2000 and i don't know what it was yeah i was describing 2000 something like that yeah and so
[00:42:16.800 --> 00:42:21.760]   i recently canceled it because my wife had won oh it was tied to your amazon account yes and they
[00:42:21.760 --> 00:42:26.480]   couldn't figure it out but it's basically they just turned into my account so they're still trying to
[00:42:26.480 --> 00:42:30.400]   activate can you get it back they don't know is there any reason why you care why don't you set
[00:42:30.400 --> 00:42:35.920]   up a new account because because i need access to my book oh oh well you're still using well i'll let
[00:42:35.920 --> 00:42:43.680]   you know there's all kinds of data in there that i need to this is the new uh digital dilemma yeah
[00:42:43.680 --> 00:42:47.360]   what do you do if you're i mean it's bad enough when if people lose their google accounts but if
[00:42:47.360 --> 00:42:55.760]   you lose your amazon account do you really exist not really lily i'm shut out of prime there i mean
[00:42:55.760 --> 00:43:02.080]   for starters i mean oh my god the horror i have to pay full price on everything so are you so
[00:43:02.080 --> 00:43:06.160]   that's interesting that you brought that up caroline because it is true we heard nothing but
[00:43:06.160 --> 00:43:11.120]   horror stories about people working amazon warehouses amazon announced and now they're already doing
[00:43:11.120 --> 00:43:17.440]   it in our neck of the woods next day delivery which is going to make you make gotta make it even
[00:43:17.440 --> 00:43:23.760]   worse in the warehouses i had i had an amazon driver deliver a book at nine o'clock last night
[00:43:23.760 --> 00:43:29.920]   i mean this poor person has probably been out in the truck all day was a little van yeah it's one of
[00:43:29.920 --> 00:43:37.120]   those prime those new amazon vans the headset caroline is wearing was placed in an amazon locker by an
[00:43:37.120 --> 00:43:43.680]   amazon worker at nine eight nine p.m. last night just yeah just so she could be on the show today
[00:43:43.680 --> 00:43:55.600]   wow see yeah it's see right yeah yeah i don't know it's i feel like uh in an ideal world it would be nice to
[00:43:55.600 --> 00:44:03.920]   they had times the convenience of it is really right i mean hard without this is the modern dilemma
[00:44:04.720 --> 00:44:10.640]   we want all our conveniences and and we kind of know in the back of our mind is it's due to the
[00:44:10.640 --> 00:44:17.280]   suffering of some other human right i mean in terms of the price i mean a lot of times especially
[00:44:17.280 --> 00:44:21.440]   if you're buying things in bulk like there have been times that i've bought like food or certain
[00:44:21.440 --> 00:44:27.520]   types of food or cleaning supplies in bulk because that's really the only like money efficient way
[00:44:27.520 --> 00:44:33.760]   to do it but i think the important thing to keep in mind is that with amazon i mean it's not just
[00:44:33.760 --> 00:44:38.080]   something that affects amazon warehouses i mean this is something that's creating oh yeah i mean
[00:44:38.080 --> 00:44:44.880]   going to walmart's no better walmart yeah yeah pretty much any place that's relying on on warehouses
[00:44:44.880 --> 00:44:51.920]   um i mean it's but what do you do you live on artisanal picketal pickles from Williamsburg for
[00:44:51.920 --> 00:44:56.560]   the rest of your life i mean you can you can try what would Brooklyn what would Brooklyn do
[00:44:56.560 --> 00:45:03.360]   you only shop local merchants that have handcrafted your goods the worst part is they're going to
[00:45:03.360 --> 00:45:09.680]   solve the the the uh the plight of their workers by replacing with robots and so is that better
[00:45:09.680 --> 00:45:15.120]   it's hard it's so i i thought oh this is a this is amazon announced that they're going to spend
[00:45:15.120 --> 00:45:19.520]   they're going to retrain yeah what is it maniacs which is a hundred million i can't remember what it
[00:45:19.520 --> 00:45:22.960]   it sounds like a good plan to me they're going oh it's like seven hundred thousand or something
[00:45:22.960 --> 00:45:28.000]   like that they're going to retrain people nationwide for seven hundred million dollars to retrain
[00:45:28.000 --> 00:45:33.200]   one third of its workforce but if you do the math it's like twelve hundred bucks per person
[00:45:33.200 --> 00:45:40.240]   it's barely enough to send somebody to school for a week uh they're going to retrain a hundred
[00:45:40.240 --> 00:45:48.160]   thousand workers by 2025 because they say well these people are going to be out of uh out of work
[00:45:48.160 --> 00:45:53.760]   and and amazon is very robots totalitarian in their pr approach if you're in the press you know
[00:45:53.760 --> 00:45:59.040]   this that they're very difficult to work with and they are masters at spinning stories in their favor
[00:45:59.040 --> 00:46:03.200]   and you know they talk about retraining and then they throw out some examples to get your mind
[00:46:03.200 --> 00:46:07.040]   thinking about advanced degrees and like doing all this kind of stuff but really they're going to be
[00:46:07.040 --> 00:46:11.680]   reput retraining people factory workers to be drivers and they're going to be you know because
[00:46:11.680 --> 00:46:15.440]   how many of those people i don't know but how many of those people can become coders and how
[00:46:15.440 --> 00:46:19.840]   many coders do exactly exactly it's a slight of hand they're going to be doing a lot of training
[00:46:19.840 --> 00:46:24.480]   like every other company every company okay uh the seven thousand dollars a worker which actually
[00:46:24.480 --> 00:46:30.240]   is decent it's a thousand twelve one thousand two hundred dollars a year for five years so that's
[00:46:30.240 --> 00:46:36.080]   what's bloomberg had an interesting analysis of it they said breaking down the numbers
[00:46:36.080 --> 00:46:40.640]   they'll spend seven hundred million a hundred thousand employees by 2025 one thousand seventy
[00:46:40.640 --> 00:46:46.000]   seven dollars a person annually according to an estimate from the association for talent development
[00:46:46.000 --> 00:46:51.520]   a trade group the average organization spends already twelve hundred ninety six dollars per
[00:46:51.520 --> 00:46:56.080]   employee annually on training so it affects amazon's committing to spending less and on its face i
[00:46:56.080 --> 00:47:00.320]   don't believe that that's an apples to apples comparison they say this is how much apple that
[00:47:00.320 --> 00:47:06.880]   amazon's going to spend how much of that is on leasing buildings and and and and and getting
[00:47:06.880 --> 00:47:12.400]   projectors you know so you think it's just a pr move it it's not a genuine effort to retrain
[00:47:12.960 --> 00:47:16.880]   well no i think i mean i think they're doing every company every enterprise has to train
[00:47:16.880 --> 00:47:20.640]   they have to do training all right and i think they're trying to get a lot of credit for something
[00:47:20.640 --> 00:47:27.760]   that is it possible for amazon to treat its workforce humanely absolutely they make enough
[00:47:27.760 --> 00:47:31.840]   money they could do that well the prices would go up i mean that this is why things are cheap on
[00:47:31.840 --> 00:47:37.120]   amazon you they're squeezing everywhere right they're squeezing everything yeah so it's possible
[00:47:37.120 --> 00:47:43.440]   right i mean a fifteen dollar minimum wage i mean for most people still isn't a livable wage and i
[00:47:43.440 --> 00:47:47.920]   think in terms of retraining the details are really important here i mean is this something that's
[00:47:47.920 --> 00:47:53.120]   going to be happening in addition to slash outside of work hours i mean for people who might have to
[00:47:53.120 --> 00:47:58.080]   work like more than one job or for people who have a family i mean that might not be feasible
[00:47:58.080 --> 00:48:02.640]   and for what to stay within the same company um are they actually going to be making more money
[00:48:02.640 --> 00:48:07.680]   and they're retrained positions or the conditions going to be comparable i don't know i mean it seems
[00:48:07.680 --> 00:48:13.280]   like at this stage like those kinds of details are unclear i feel bad because this show used to
[00:48:13.280 --> 00:48:17.920]   be all about how great technology is it's making our lives better and you're going to have robots
[00:48:17.920 --> 00:48:21.920]   and the cars are going to drive themselves and pretty soon you won't have to wear glasses anymore
[00:48:21.920 --> 00:48:29.840]   and now it's all about dystopia is it is that what happened or is it just me i don't think well i
[00:48:29.840 --> 00:48:34.720]   don't think it's just me i don't think the story about amazon's training workers is that much of a
[00:48:34.720 --> 00:48:40.080]   technology story it's it's a technology company and there's that's partly what's happened is basically
[00:48:40.080 --> 00:48:46.080]   this is a type of capitalism it's a business story yeah it's a business story and and um you know
[00:48:46.080 --> 00:48:52.880]   they well let's do a technology story of amazon is building a robot but not for the warehouse it's a
[00:48:52.880 --> 00:48:58.640]   home robot with amazon's echo bill to be coming something i thought it was gonna be this little
[00:48:58.640 --> 00:49:04.400]   tiny robot that goes but it's waist high it's like the size of a top that's so you could put your
[00:49:04.400 --> 00:49:10.560]   beer on it's yeah exactly that that's a bad joke i'm sorry i said i've been to place i've been to
[00:49:10.560 --> 00:49:15.360]   cocktail parties in in sonoma where they have robots they bring you your tray of hors d'oeuvres
[00:49:15.360 --> 00:49:19.360]   they they well actually one of the things that they had this was at the at the future of food
[00:49:19.360 --> 00:49:25.520]   conference in in sonoma they had robots with a tray on top and they were carrying little desserts
[00:49:25.520 --> 00:49:30.560]   that were pastry versions of robots with trays on top that's little and it has little desserts
[00:49:30.560 --> 00:49:36.400]   on top of those dessert inception amazing where the desserts are on top of the robot desserts it just
[00:49:36.400 --> 00:49:45.360]   it was desserts all the way up it was amazing the robot is called vesta and they say we can't yet
[00:49:45.360 --> 00:49:51.840]   mass produce vesta um they had hope to reveal it this year but it's just not ready
[00:49:52.960 --> 00:49:58.400]   now this isn't we had an advertiser uh Bosch who had a robot called uh
[00:49:58.400 --> 00:50:03.440]   kuro remember kuro was a little robot that would wander around it was kind of like an
[00:50:03.440 --> 00:50:09.920]   amazon alexa on wheels they canceled that project um i wonder i don't think it's hard to make
[00:50:09.920 --> 00:50:14.960]   i think it's hard to find a market yeah does anybody really want a little robot
[00:50:14.960 --> 00:50:21.920]   amazon echo that wanders around yes i mean it's it's like it sounds cute but i feel like i'm
[00:50:21.920 --> 00:50:26.000]   struggling i'm struggling to understand this here so let's say you have an apartment like mine
[00:50:26.000 --> 00:50:32.480]   that isn't that big i mean this is a matter of walking literally three steps to wherever like
[00:50:32.480 --> 00:50:38.400]   yeah it is or let's say you have an actually big house like is it gonna go up the stairs probably
[00:50:38.400 --> 00:50:46.800]   not right i'm just right right i don't know it just seems why i mean i think the idea of having an
[00:50:46.800 --> 00:50:52.880]   echo on a rumba i mean that's cute i guess i think jes pesos instagramed his kids had taped an echo
[00:50:52.880 --> 00:50:58.560]   to a rumba he instagram that a year ago i think he's blaming a kid yeah i won't think about jeff
[00:50:58.560 --> 00:51:03.360]   but i think that's i think his kids are genius for doing that everything should just every product
[00:51:03.360 --> 00:51:08.160]   should just be placed on top of a rumba and it would instantly make it better i think all animals
[00:51:08.160 --> 00:51:14.080]   especially house cats should be placed on top of a rumba i mean the reality is that dammit we
[00:51:14.080 --> 00:51:19.360]   boomers were promised robots in a home right and uh i never got them so i anybody who's working on
[00:51:19.360 --> 00:51:25.200]   that i think it's great the problem is that amazon is a is a privacy nightmare in your home but if
[00:51:25.200 --> 00:51:29.840]   you ignore that part of it it's like i'm glad that i come to because look what amazon did with the
[00:51:29.840 --> 00:51:33.840]   with the virtual assistant appliances or what we now call smart speakers right they nobody
[00:51:33.840 --> 00:51:37.280]   believed in that stuff when they came out with the echo that's true people said the same things
[00:51:37.280 --> 00:51:40.560]   about the echo that we're saying now but absolutely absolutely people like under you don't need
[00:51:40.560 --> 00:51:46.080]   to watch that that's silly it's really changing the tech the consumer technology world uh in a
[00:51:46.080 --> 00:51:52.400]   major way so i would love to see uh you know virtual assistant robot type things i just would
[00:51:52.400 --> 00:51:58.560]   prefer that income from amazon so i mentioned like no matter what i mean you're gonna be having like
[00:51:58.560 --> 00:52:03.920]   the types of security issues i mean the same sorts of issues with like people being creeped out
[00:52:03.920 --> 00:52:09.920]   about humans transcribing and having access to voice recordings i mean that's a shared problem
[00:52:09.920 --> 00:52:15.440]   with amazon i mean it seems like whoever is making these speakers i mean if you're having a device
[00:52:15.440 --> 00:52:20.320]   that's listening to you in your home and has access to literally everything that's going on i mean
[00:52:20.320 --> 00:52:24.720]   that's going to be there's going to be a security concern that's going to be a privacy concern no
[00:52:24.720 --> 00:52:29.200]   matter which company is behind it i mean with companies like google and amazon i mean it's even
[00:52:29.200 --> 00:52:33.920]   more of a concern considering like a large amount of data they already have about you and your customer
[00:52:33.920 --> 00:52:39.440]   profile but yeah i don't know if it's i don't know if it's strictly an amazon problem just to be
[00:52:39.440 --> 00:52:45.520]   clear this is a much bigger potential risk than it sounds it's not that this thing will be on
[00:52:45.520 --> 00:52:51.520]   wheels and roll around listening it will be 3d mapping your home it will be videotaping your
[00:52:51.520 --> 00:52:56.000]   furniture and it'll be finding out who else is in the house which will be recognized through
[00:52:56.000 --> 00:52:59.280]   face recognition and don't be surprised the next time you got an amazon if it says you know your
[00:52:59.280 --> 00:53:06.080]   couch is yeah i can do amazon's choice couch this couch will fit perfectly in space and this rug
[00:53:06.080 --> 00:53:10.560]   will really tie the room together really is it going to do that i mean is that how valuable is that to
[00:53:10.560 --> 00:53:15.200]   amazon oh infinitely valuable i think i think they sit there i think jeff bezos sits in his basement
[00:53:15.200 --> 00:53:19.360]   three o'clock in the morning seething because he has all this data from amazon echo products
[00:53:19.360 --> 00:53:23.600]   but he just doesn't know what's in the other room what's in there who's in there like i mean i
[00:53:23.600 --> 00:53:28.160]   really think that that's and is like i know we'll put wheels on this thing and it'll roll around have
[00:53:28.160 --> 00:53:31.840]   access to the lab where you're sleeping i'll go in the bathroom and go through your medicine cabinet
[00:53:31.840 --> 00:53:38.160]   i mean who knows but it's lots and lots of data that they i mean these amazon sells everything
[00:53:38.160 --> 00:53:42.000]   yeah they sell everything and they want to see what you got this is i think this is one of the
[00:53:42.000 --> 00:53:47.040]   reasons for this uh this version of the echo that takes pictures of your outfit the whole point is
[00:53:47.040 --> 00:53:50.880]   the see what's in your car yeah see what's in your closet like any of my outfits
[00:53:50.880 --> 00:54:00.000]   none of them you said echo don't look sorry but i anyway i said it to stacy stacy higgin by them
[00:54:00.000 --> 00:54:05.280]   has my echo look i'm sure it likes her outfits much better um well uh this weekend law i'm
[00:54:05.280 --> 00:54:10.560]   blanking out Denise howl is super into it like she she posts she has she has a great look and she
[00:54:10.560 --> 00:54:14.560]   she's very put together yeah and she's creating fomo on instagram every day with this thing
[00:54:14.560 --> 00:54:20.880]   so the reason i'm glad i'm not so i have a new i've been a new kick i've been thinking about because
[00:54:20.880 --> 00:54:27.760]   we we talk a lot there's a lot of energy spent uh about artificial intelligence you know i think
[00:54:27.760 --> 00:54:33.600]   elan musk and others who said oh no you know it's the terminators are coming and this whole notion
[00:54:33.600 --> 00:54:38.720]   that ai the fear of ai is that it's going to somehow at some point gain consciousness
[00:54:38.720 --> 00:54:44.480]   and it's going to think uh we don't need the humans it's going to wipe us out or it's going to
[00:54:44.480 --> 00:54:49.920]   turn us into batteries as it did in the matrix or something like that and i've i've read a couple
[00:54:49.920 --> 00:54:54.640]   people most recently you've all know our harari in his 21 lessons for the 21st century who said
[00:54:54.640 --> 00:55:00.080]   no one has ever demonstrated that if you get better at ai that at some point you'll leap this
[00:55:00.080 --> 00:55:04.960]   gap into consciousness yeah we don't know where consciousness comes from there's no evidence
[00:55:04.960 --> 00:55:10.400]   that you feed a machine a lot of informations you know and it becomes a great chess player and
[00:55:10.400 --> 00:55:15.040]   then suddenly it goes i i think i'm gonna eat gary kesparo for lunch it doesn't jump to that
[00:55:15.040 --> 00:55:18.880]   consciousness thing we don't know where consciousness comes from maybe someday we'll be able to
[00:55:19.440 --> 00:55:24.080]   so i'm gonna submit this is my new thing that consciousness is not the threat of ai
[00:55:24.080 --> 00:55:30.080]   the threat of ai is the humans who use ai now you've been talking about this a little bit uh on
[00:55:30.080 --> 00:55:36.880]   motherboard there was a piece in slate this morning that creeped me out it turns out license plate
[00:55:36.880 --> 00:55:42.560]   reading software has gotten so cheap and it will work with hundred-dollar cameras that it's being
[00:55:42.560 --> 00:55:51.680]   installed everywhere license plate readers they give an example of a housing complex where people
[00:55:51.680 --> 00:55:58.480]   are using it to make sure that you know you're not abusing your rental agreement that you you know
[00:55:58.480 --> 00:56:03.440]   if it's they are everywhere it's about a hundred dollars for the software on a hundred-dollar
[00:56:03.440 --> 00:56:11.280]   camera cities we the city i live in petaluma i noticed was putting up cameras on every traffic
[00:56:11.280 --> 00:56:16.800]   light every street light and i realized they're not valuable if you have to have somebody sitting
[00:56:16.800 --> 00:56:21.520]   and watching them if you've got 400 cameras in a town of 50,000 people what do you just want
[00:56:21.520 --> 00:56:27.280]   that got you know 800 officers looking at them no you put license plate reading software on it
[00:56:27.280 --> 00:56:34.880]   and then if you want to know where leo was last night you know yeah exactly and this is an example
[00:56:34.880 --> 00:56:39.760]   to me a palantir is another one and i'm really interested that you caroline have been able to
[00:56:39.760 --> 00:56:46.080]   break through because palantir has been almost a legendarily secret company in silicon valley
[00:56:46.080 --> 00:56:51.760]   yeah funded by peter teal uh... alex carp is its secretive ceo
[00:56:51.760 --> 00:56:58.240]   and we knew it was some sort of well what is it we don't i don't know what we knew
[00:56:58.240 --> 00:57:04.880]   yeah i guess the best way to describe it is that it's an aggregator it takes data from like all
[00:57:04.880 --> 00:57:10.080]   different types of sources and it organizes it and is able to find connections in ways that like
[00:57:10.080 --> 00:57:15.360]   if you're just a regular human analyst you might not be able to do it or if you're just a person
[00:57:15.360 --> 00:57:21.040]   with access to five different databases and you're looking to use something from there i mean
[00:57:21.040 --> 00:57:26.400]   that's not actionable just having all this unstructured data so that's palantir's role um it organizes
[00:57:26.400 --> 00:57:32.480]   it and it's able to make charts and sort through very sensitive data very easily as easy as making
[00:57:32.480 --> 00:57:37.280]   a search this is what's worrying me about ai it's not that it's going to become conscious and
[00:57:37.280 --> 00:57:41.920]   got want to get rid of the humans it's that there are humans who will be using this you wrote that
[00:57:41.920 --> 00:57:46.800]   three hundred california cities northern california cities i wouldn't be surprised if petaluma is one
[00:57:46.800 --> 00:57:52.880]   of them are using palantir why did petaluma put up all those cameras to gather data
[00:57:52.880 --> 00:57:55.680]   and what are they going to do with the data
[00:57:57.120 --> 00:58:03.920]   i think the noteworthy thing here oh sorry go ahead no no um the noteworthy thing here is that i
[00:58:03.920 --> 00:58:09.040]   started this whole thing by sending public information request to individual cities and almost all of
[00:58:09.040 --> 00:58:14.800]   them came back empty the only one that came back with information was through the the ncric the
[00:58:14.800 --> 00:58:19.440]   northern california regional intelligence center so even if you're a citizen living in one of these
[00:58:19.440 --> 00:58:25.680]   cities and you want to find out about it there's really no way for you to know i mean only by
[00:58:25.680 --> 00:58:31.680]   filing a public record request to this regional center um do you realize like because here's the
[00:58:31.680 --> 00:58:39.920]   thing like police officers can request this type of data from the ncric um but there's no paper
[00:58:39.920 --> 00:58:44.000]   trail for those individual cities and there's really no transparency for the citizens that are
[00:58:44.000 --> 00:58:48.800]   actually living there so this is run through the department of homeland security do they say it's
[00:58:48.800 --> 00:58:56.400]   to cut down terrorism right so on their website they claim that they well they most of their
[00:58:56.400 --> 00:59:03.040]   efforts are focused on major drug operations and terrorism operations but at the same time there's
[00:59:03.040 --> 00:59:09.120]   they will pretty much investigate anything that a local police department requests help with and
[00:59:09.120 --> 00:59:13.760]   this could be i mean literally any type of crime it could be it could be a robbery it could be
[00:59:13.760 --> 00:59:20.000]   a domestic assault it could be really any type of investigation that a local police department
[00:59:20.000 --> 00:59:27.200]   is operating so this isn't just a let's catch people trying to commit like a major major
[00:59:27.200 --> 00:59:33.920]   act of terrorism this is you know local operations um and this is you know they have a massive
[00:59:33.920 --> 00:59:38.240]   amount of data on the people that are living in those areas on one hand i would say oh that's good
[00:59:38.240 --> 00:59:43.920]   we're going to be a safer city in pedaling by the way our county is one of the participating
[00:59:43.920 --> 00:59:49.600]   14 participating northern california counties in uh and c yep ric there are no that's going to be
[00:59:49.600 --> 00:59:58.560]   safer it's going to be safer here i think i don't know i think that this is it sort of reminds me
[00:59:58.560 --> 01:00:04.240]   about the arguments that people make with respect to privacy like oh i have nothing to hide so i
[01:00:04.240 --> 01:00:09.440]   shouldn't have any reason to worry about this you know i don't buy that one yeah i mean if you have
[01:00:09.440 --> 01:00:15.440]   nothing to hide leave the bathroom door open uh you know i've tried that argument people don't like
[01:00:15.440 --> 01:00:20.960]   that um there's reason it's not that you have something to hide it's not that you're criminals
[01:00:20.960 --> 01:00:26.720]   just that you deserve to live privately the risk here is the slippery slope you mentioned that you
[01:00:26.720 --> 01:00:32.160]   know why are they installing these cameras in pedaluma and it doesn't matter why once they have the
[01:00:32.160 --> 01:00:37.440]   cameras they can later decide oh hey we have all these cameras why don't we do xyz so it's really
[01:00:37.440 --> 01:00:43.200]   the infrastructure is is just another step i feel like we're sliding down this slope well here's
[01:00:43.200 --> 01:00:47.600]   another example i just wrote a piece for fast company about emotion technology and cars where
[01:00:47.600 --> 01:00:51.280]   cars will be able to read your motions understand what your mindset is understand what's happening
[01:00:51.280 --> 01:00:57.360]   in the car this is part of the you know self-driving cars are like super computers on wheels with all
[01:00:57.360 --> 01:01:01.680]   these sensors pointing outward what we're not talking enough about is the sensors pointing inward
[01:01:01.680 --> 01:01:06.160]   at the drivers and everybody else but what's interesting is that faktiva is in the lead with this
[01:01:06.160 --> 01:01:10.640]   technology of being able to read emotions of drivers and they're not doing it by installing
[01:01:10.640 --> 01:01:16.640]   cameras in cars they're using the cameras already installed in cars and they're just tapping into
[01:01:16.640 --> 01:01:22.800]   that feed it's it's like an it's like a uh you know after the fact kind of uh an idea where it's
[01:01:22.800 --> 01:01:27.120]   like well you know these high-end cars have cameras why don't we just read people's emotions with them
[01:01:27.120 --> 01:01:32.560]   and so this is the this is why do they want the emotions i'm i can understand for self-driving
[01:01:32.560 --> 01:01:36.240]   vehicles it's important to know what that other driver is going to do that doesn't bother me well
[01:01:36.240 --> 01:01:40.880]   there's so the humans are the big problem with self-driving vehicles well the biggest the biggest
[01:01:40.880 --> 01:01:45.920]   thing is this is going to go online before they're fully self-driving but but most urgently that
[01:01:45.920 --> 01:01:52.800]   europe has as uh uh enacted legislation saying that by 2021 all cars sold in europe will have to have
[01:01:53.760 --> 01:01:58.400]   a basic reading functionality they'll be able to they have to tell whether the driver's falling
[01:01:58.400 --> 01:02:03.200]   asleep whether the driver's drunk this is you know this is a Cadillac just put that in their CTS for
[01:02:03.200 --> 01:02:08.480]   their yeah autopilot feature yes because as you know with same with tessel you got to keep your hand
[01:02:08.480 --> 01:02:13.280]   on the wheel uh and you have to keep your eyes on the road it used to be that it was enough to have
[01:02:13.280 --> 01:02:17.840]   your hand on the wheel but Cadillac actually has a camera that sees where you're looking yeah so
[01:02:17.840 --> 01:02:23.520]   that if you look down or away from the road it while the autopilot's on i can it actually vibrates
[01:02:23.520 --> 01:02:28.160]   your seat and then eventually disengages it i had to trick it i had to cover my eyes but i was too
[01:02:28.160 --> 01:02:32.080]   scared while driving well that was the thing i was too scared to actually do that so i did like
[01:02:32.080 --> 01:02:37.360]   this and and you know i had my peer through my fingers and it did it but it works on glasses
[01:02:37.360 --> 01:02:42.160]   yeah but it but well exactly i mean it does it's looking at you driving well the best thing and if
[01:02:42.160 --> 01:02:45.920]   they were able to get that and i guess with on-star they probably are getting that but i love that
[01:02:45.920 --> 01:02:49.680]   technology because you can be able to look at a store and say is that open and they'll they'll
[01:02:49.680 --> 01:02:53.760]   know what you're looking at and it'll say oh that so-and-so Starbucks is not open right now so caroline
[01:02:53.760 --> 01:03:00.400]   you filed a public records request and got the Palantier user manual for cops
[01:03:00.400 --> 01:03:08.880]   right so wow yeah this was a specific user manual that seemed to be designed for cops that were
[01:03:08.880 --> 01:03:17.840]   operating in like the west uh north california area um and full disclosure the manual was almost
[01:03:17.840 --> 01:03:22.160]   unreadable it took me several hours to be able to parse all of this through because of jargon um
[01:03:22.160 --> 01:03:27.120]   because of jargon because it was just poorly organized it was poorly worded
[01:03:27.120 --> 01:03:33.360]   they did not define any of the terms they just sort of said that the way it was organized was
[01:03:33.360 --> 01:03:38.320]   here's how you make a search um and then they sort of introduced different charting functions
[01:03:38.320 --> 01:03:44.720]   um by the way this is confidential and proprietary not for distribution so please close your eyes
[01:03:44.720 --> 01:03:48.560]   another thing boomers say in my day we used to go through dumpsters to find that kind of thing
[01:03:48.560 --> 01:03:54.400]   yeah i go ahead and well so searching for records in palantier start start in the graph
[01:03:54.400 --> 01:04:00.480]   application this is the worst this manual you're right caroline this is horrible it is like unreadable
[01:04:00.480 --> 01:04:04.400]   it is like the gear icon oh my god
[01:04:04.400 --> 01:04:12.480]   right i think it's the best way to describe it is in terms of what you can do and what information
[01:04:12.480 --> 01:04:18.560]   that you're starting with if you're starting with just a name you can get i mean if they have it in
[01:04:18.560 --> 01:04:24.080]   their data systems if they have it in police record management systems if they have it through
[01:04:24.080 --> 01:04:29.520]   other types of private data that they obtain things that could have access to people's uh email
[01:04:29.520 --> 01:04:35.600]   addresses or bank account numbers really sensitive information like that i mean this is all pulled
[01:04:35.600 --> 01:04:39.840]   instantly just from a simple name search and if you know somebody's license plate and you use
[01:04:39.840 --> 01:04:44.720]   this example in your article uh you can know where they because of license
[01:04:44.720 --> 01:04:49.360]   print readers and cameras everywhere you can know pretty much everywhere somebody's been
[01:04:49.360 --> 01:04:58.320]   and when right for any time period right so basically you set the radius and you say
[01:04:58.320 --> 01:05:03.600]   you're looking for a particular license plate number that's traveled in like i don't know let's say
[01:05:03.600 --> 01:05:08.880]   a hundred mile radius or more and then you can set a date range if you want and since they have
[01:05:08.880 --> 01:05:14.480]   timestamps and they have location stamps um you can get all of that information and then they also
[01:05:14.480 --> 01:05:19.040]   have a picture of the car and the license plate itself so all of this information is stored just
[01:05:19.040 --> 01:05:23.920]   by searching a license plate number with the name this is the i mean we've all seen this on tv on
[01:05:23.920 --> 01:05:29.920]   ncis and stuff but really this is even much better yeah than anything with a name you can get a
[01:05:29.920 --> 01:05:33.920]   person's email address phone number current previous addresses bank account social security
[01:05:33.920 --> 01:05:38.800]   numbers business relationships family relationships license information height weight i color
[01:05:38.800 --> 01:05:44.720]   if it's in an agency's database you can get it so that's what Palantir's doing do they talk about
[01:05:44.720 --> 01:05:48.720]   the sources of this information are they getting it some of it obviously from DMV
[01:05:48.720 --> 01:05:54.320]   department of motor vehicles is it all government agencies is it google and facebook
[01:05:54.320 --> 01:06:02.240]   see the ncric specifically it's a fusion center so they would use palantir to take the different
[01:06:02.240 --> 01:06:08.080]   police like local police records so this would be like crime arrest etc data and
[01:06:08.080 --> 01:06:13.840]   Palantir also says that they get data really vaguely from government and federal agencies
[01:06:13.840 --> 01:06:18.880]   so this could be where you're getting information about like license plate data for instance um
[01:06:18.880 --> 01:06:24.320]   where you could get information about like birthday whether you're married whether you have a
[01:06:24.320 --> 01:06:29.520]   business logged in your name that kind of thing so some of its public records yeah right some
[01:06:29.520 --> 01:06:35.200]   of the public records in terms of email addresses or bank account records i mean that would be
[01:06:35.200 --> 01:06:41.680]   something that would have to be acquired privately in most situations um and i mean do we have any
[01:06:41.680 --> 01:06:46.720]   evidence that they for instance in their manual they show you examples of different data bases
[01:06:46.720 --> 01:06:53.680]   you can search including alpr as licensed automatic license plate readers you can do l_a_p_d uh
[01:06:53.680 --> 01:07:00.880]   l_a_s_d border crossing vehicle query dmv vehicle query national license plate reader vehicle
[01:07:00.880 --> 01:07:07.040]   query and and that's just a portion of it i imagine this would scroll down quite away yeah it's an
[01:07:07.040 --> 01:07:12.320]   incomplete list and it's also with only a sample search um a lot of the examples that they had
[01:07:12.320 --> 01:07:17.040]   that they had um you could see that the search query was palantir and then they had like example
[01:07:17.040 --> 01:07:24.000]   profiles that were named things like john bad guy smith but still bad guys my
[01:07:26.720 --> 01:07:32.000]   you can see on the right um if you choose a particular profile like let's say you search
[01:07:32.000 --> 01:07:36.960]   john bad guy smith and there's ten people in the area with that same name you can click on one of
[01:07:36.960 --> 01:07:42.080]   them and you can see the specific details that distinguish him from the other john bad guy smith
[01:07:42.080 --> 01:07:46.720]   in the area and by the way for you view at home if you there's an area with ten bad guy smith
[01:07:46.720 --> 01:07:52.560]   there don't go to that area stay away it's a bad guy area yeah very dangerous i like the uh i like
[01:07:52.560 --> 01:08:00.320]   the the icons they use for john bad guy i think that's the hamburger i'm sorry but i think that's
[01:08:00.320 --> 01:08:05.120]   the hamburger so so leo you should subscribe with this because you always talk to the audience and
[01:08:05.120 --> 01:08:10.480]   find i would love to get this information here who's coming so and did you know the email address
[01:08:10.480 --> 01:08:20.640]   down with the us.org say again that's me down with the us.org that's the email domain oh oh yeah jbg
[01:08:20.640 --> 01:08:28.720]   oh one at down with the us.org because as you know anybody any malefactor you know is doesn't like
[01:08:28.720 --> 01:08:34.640]   the america that we know i'm scared this now let me ask you because this came out a couple of days
[01:08:34.640 --> 01:08:44.560]   ago was there any reaction to your article uh palance here emailed me once and said why did you say
[01:08:44.560 --> 01:08:50.720]   that we didn't respond to multiple requests for comment and i said because you didn't that's pretty
[01:08:50.720 --> 01:08:56.480]   much the only thing that i've heard from palantir um in terms of in terms of the response i mean i
[01:08:56.480 --> 01:09:02.960]   guess what sort of surprised me is that i generally had an understanding of i mean sort of how palantir
[01:09:02.960 --> 01:09:07.680]   worked but in terms of in terms of getting the visuals and in terms of seeing the actual manual
[01:09:07.680 --> 01:09:13.840]   it seems like that had um it seems like that had an effect on people because i mean it's one thing
[01:09:13.840 --> 01:09:19.520]   to hear that there's like this secret omnipotent data company that's operating in the shadows with
[01:09:19.520 --> 01:09:25.200]   with regional level governments and investigative agencies but it's another one to see that you can
[01:09:25.200 --> 01:09:31.280]   just search your name and find out virtually everything about yourself um so yeah and then this is
[01:09:31.280 --> 01:09:38.080]   accessible to any law enforcement agency in northern california pretty much it's only law
[01:09:38.080 --> 01:09:43.760]   enforcement though right right so it would be it would be law enforcement and any local police
[01:09:43.760 --> 01:09:48.560]   department within those counties would be able to tap into those resources you know who decides
[01:09:48.560 --> 01:09:54.640]   that that's who gets to see it or if they if anyone is in a in a position to make an exception to
[01:09:54.640 --> 01:10:01.680]   that like for example mall cops maybe or i hope mulk i have paul bark here can i get this
[01:10:01.680 --> 01:10:07.920]   mall cops are usually aren't they usually private security forces right no private security force
[01:10:07.920 --> 01:10:13.920]   my point is is this a matter of law or is this uh company policy that they decide who gets to see
[01:10:13.920 --> 01:10:19.600]   it and who doesn't right so i think that's one important thing to keep in mind is that you know
[01:10:19.600 --> 01:10:24.960]   palantir is the infrastructure but exactly how it gets used that's all in the hands of the regional
[01:10:24.960 --> 01:10:31.120]   intelligence agency what's what's important is how the data that would not have been actionable
[01:10:31.120 --> 01:10:37.520]   is made actionable and it's put into the hands of without a warrant right well in terms of i
[01:10:37.520 --> 01:10:43.440]   don't really get into the article in terms of like you do have to enter for instance like a case
[01:10:43.440 --> 01:10:50.000]   number in order to make a search in the um the alpr database and in terms of the people search
[01:10:50.000 --> 01:10:55.040]   at least in that version of it it's an open field so when you're saying uh the reason for your
[01:10:55.040 --> 01:11:01.520]   search it seems like you can just type in words and that'll be accepted type the reason for the
[01:11:01.520 --> 01:11:07.360]   search in the search purpose field this could be a case number or a suspect right or i'm just bored
[01:11:07.360 --> 01:11:12.880]   i'm sitting here i thought i'd find out what my answer is up to this is kind of timely because
[01:11:12.880 --> 01:11:20.560]   this is what ice is using this weekend uh to track down uh undocumented uh immigrants and
[01:11:20.560 --> 01:11:27.760]   arrest them and deport them the big issue for me and i and i have to say on the one hand you want
[01:11:27.760 --> 01:11:35.120]   law enforcement to have good tools protected by courts and and warrants but you also want to have
[01:11:35.120 --> 01:11:39.280]   some oversight in the data and the big concern for me is this is a private company
[01:11:39.280 --> 01:11:46.960]   data sources may be corrupt may be inaccurate uh secure face we know face recognition is often
[01:11:46.960 --> 01:11:54.400]   used in these cases we know that's often full of false positives uh like the end of the
[01:11:54.400 --> 01:12:01.040]   ric does have access to facial recognition this was in i believe it was in oakland the head of the
[01:12:01.040 --> 01:12:06.400]   oakland police department submitted uh a letter to to the town when they could because they're
[01:12:06.400 --> 01:12:12.240]   going to be voting on whether or not to ban facial recognition i think on the 16th um and they said
[01:12:12.240 --> 01:12:18.960]   based on this ban we wouldn't be able to access the facial recognition um through the ncric
[01:12:18.960 --> 01:12:25.040]   so interesting remember here is that like this data can be combined with all different
[01:12:25.040 --> 01:12:29.600]   other types of databases as well and for instance one of the documents that i got back was how to
[01:12:29.600 --> 01:12:35.920]   use palantir with toms and rueters clear which is similarly a giant database tool for police
[01:12:35.920 --> 01:12:40.400]   and i think that what you were getting up before is really important to keep in mind that
[01:12:40.400 --> 01:12:48.720]   you know the way that you act on data is only as good as the data itself and you know we know that
[01:12:48.720 --> 01:12:53.600]   the state of policing in this country isn't perfect so i mean i think the important thing to keep
[01:12:53.600 --> 01:12:58.800]   in mind is how this could be used against marginalized populations and what we're seeing with the ice
[01:12:58.800 --> 01:13:03.840]   raids i mean that's pretty much a worst case scenario i'm looking at your table of where this data
[01:13:03.840 --> 01:13:11.680]   comes from and it is of right so i think it's important to note that this is through the d oj
[01:13:11.680 --> 01:13:18.240]   and this is in general how fusion centers work um because palantir itself is incredibly vague about
[01:13:18.240 --> 01:13:22.880]   exactly they may be getting it from additional sources but this one includes amusement parks
[01:13:22.880 --> 01:13:29.600]   cruise lines hotels motels gaming industry gaming industry sports authority securities
[01:13:29.600 --> 01:13:42.720]   for i s p's email providers daycare centers mental health records physician patient records
[01:13:42.720 --> 01:13:48.160]   veterinary records it says veteran knows all about your dog
[01:13:49.360 --> 01:13:54.000]   so this is a very broad net and this is the d oj but this is their guidelines
[01:13:54.000 --> 01:14:01.520]   right and this is a right so the parents here i would bet because it's a private business
[01:14:01.520 --> 01:14:09.520]   it goes beyond this yeah yeah i think what's important to note you know the nc or i see this is a dhs
[01:14:09.520 --> 01:14:16.400]   entity and if you have some sort of like subpoena or you can request certain types of data from these
[01:14:16.400 --> 01:14:21.280]   private agencies then this automatically can get pulled into palantir which
[01:14:21.280 --> 01:14:27.360]   organized the data in a way that perhaps it would not have been able to use to have been used
[01:14:27.360 --> 01:14:33.120]   for see sometimes people say oh it's public data big deal but we've seen already in many ways how
[01:14:33.120 --> 01:14:39.840]   public data is changed by companies like palantir you know every time you buy or sell a house is
[01:14:39.840 --> 01:14:48.000]   recorded at the county seat and in a big book and basically you'd have to go to the county building
[01:14:48.000 --> 01:14:54.400]   to find out who lives at that address but now because so many data companies find this valuable
[01:14:54.400 --> 01:14:58.960]   they send people down to county seats they record all this they put it in a database you know who
[01:14:58.960 --> 01:15:03.200]   else finds this valuable russia when they're trying to assassinate a political dissident or
[01:15:03.200 --> 01:15:09.440]   china when they want to track i mean what are the chances that this is uh not a target for
[01:15:09.440 --> 01:15:15.200]   hacking by oh my god yeah the chinese government and the russian government the chances are zero
[01:15:15.200 --> 01:15:19.440]   it either they're trying and failing or they're trying and succeeding to get this access so they
[01:15:19.440 --> 01:15:25.520]   can track their dissidents it's a real problem yeah i think my major concern here is how this is going
[01:15:25.520 --> 01:15:33.440]   to be used by police departments against like bodies that are already at risk um you know i mean
[01:15:33.440 --> 01:15:41.840]   in particular like people of color living in these regions um i mean yeah it's only i think like
[01:15:41.840 --> 01:15:47.440]   you know like i said earlier it's these investigations this investigations are only as good as the data
[01:15:47.440 --> 01:15:53.040]   that's informing them and you know for instance with i wrote an article a couple months ago about
[01:15:53.040 --> 01:15:58.560]   um police departments with uh that are that were using predpole which is a predictive policing
[01:15:58.560 --> 01:16:03.280]   software and i mean dozens of police departments around the country were using this and there was
[01:16:03.280 --> 01:16:09.840]   no disclosure at all publicly about this and what it claims to do is take crime records crimes that
[01:16:09.840 --> 01:16:15.760]   were reported or crimes that were acted upon and put it into this database and basically tell police
[01:16:15.760 --> 01:16:22.000]   go back um and look at those places so obviously the question that you have to be asking there is
[01:16:22.000 --> 01:16:29.440]   um how good are those crime report records you know i mean obviously not every crime gets reported
[01:16:29.440 --> 01:16:35.280]   and the types of you know crimes that are prosecuted or like investigated by police i mean that's going
[01:16:35.280 --> 01:16:40.800]   to vary place by place and there's going to be you know individual bias that comes into play there
[01:16:40.800 --> 01:16:46.720]   um so there's a lot of like systemic factors that has to that plays into the way that this state
[01:16:46.720 --> 01:16:52.240]   is used um and i think you know predpole that's another good example and that's been combined with
[01:16:52.240 --> 01:16:58.000]   palantir in places like los angeles you know predpole and palantir have been used simultaneously
[01:16:58.000 --> 01:17:02.560]   at the same time i mean that's a huge amount of power put in the hands of police that
[01:17:02.560 --> 01:17:07.760]   at times have a contentious relationship with their citizens you know and you know those
[01:17:07.760 --> 01:17:13.360]   underlying problems aren't being addressed and at the same time we have these like amplifying
[01:17:13.360 --> 01:17:18.880]   forces that are making it easier and easier to do these super powered investigations in a way that
[01:17:18.880 --> 01:17:23.760]   we've never ever seen before we tell ais applied to me we're talking about the power of ai in the
[01:17:23.760 --> 01:17:28.880]   hands of various people but you could you could turn this data loose with the right kind of ai to
[01:17:28.880 --> 01:17:35.440]   essentially do ai enhanced fishing expeditions i guarantee you they're doing that right now inside
[01:17:35.440 --> 01:17:41.360]   palantir i guarantee you that's that's the most valuable thing i mean data we now know data is
[01:17:41.360 --> 01:17:47.120]   hugely valuable and that the difference is that by connecting data from a disparate variety of
[01:17:47.120 --> 01:17:53.040]   sources you can infer connections that you couldn't see before and that is to me that i don't know if
[01:17:53.040 --> 01:17:57.520]   this makes us safer i don't feel like it does make us safer i'm all for police having the tools they
[01:17:57.520 --> 01:18:03.440]   need to track down criminals but i don't think this makes us safer in the water every time there's
[01:18:03.440 --> 01:18:10.800]   advances in technology um political and police authorities always use those advances to change
[01:18:10.800 --> 01:18:16.720]   the balance of power between the citizens and themselves and so i've always argued for you know
[01:18:16.720 --> 01:18:20.240]   if they can track us we should be able to track them if they can record us we should be able to
[01:18:20.240 --> 01:18:24.320]   record them there should if there's a video camera interrogation room you should also have a video
[01:18:24.320 --> 01:18:29.440]   there is a disparity in power that is yes you can't i mean i don't care if you can record everything
[01:18:29.440 --> 01:18:33.760]   but it's growing because of technology and that's not right we should always push back and you know
[01:18:33.760 --> 01:18:40.480]   technology is good but not when the technology when when one side of the equation has a monopoly yes
[01:18:40.480 --> 01:18:44.960]   police want to do their jobs they want to fight crime we want them to fight crime we want less
[01:18:44.960 --> 01:18:51.520]   crime all of that stuff that's all true but everything like policing has to be balanced against other
[01:18:51.520 --> 01:18:55.520]   considerations we have a constitution exactly and that's why we have rights and that's why we have
[01:18:55.520 --> 01:18:59.040]   all that stuff but if if it's being circumvented through technology that nobody's really paying
[01:18:59.040 --> 01:19:05.040]   attention to that's a problem let's take a break uh mike algan is here at algan.com the nice book
[01:19:05.040 --> 01:19:13.360]   see nice pictures um of books of books no of people of mike is family he's at uh mike algan on twitter
[01:19:13.360 --> 01:19:19.200]   and uh his gastronomatic adventures continue in barce alone of this fall in september it's right
[01:19:19.200 --> 01:19:24.720]   gastronoment.net take a look it's going to be amazing yeah if you like delicious food in
[01:19:24.720 --> 01:19:28.880]   wonderful wine yeah if you're one of those people i guess you'd probably enjoy it
[01:19:28.880 --> 01:19:34.720]   yes uh caroline haskins also hear great work at mother board uh it's amazing what a public
[01:19:34.720 --> 01:19:39.520]   information request can do i think it's just a matter of time before we cut those off that
[01:19:39.520 --> 01:19:45.520]   freedom of information act that is anti-american that is a problem people like caroline go and root
[01:19:45.520 --> 01:19:50.080]   in a round from my favorite thing did you ever do dumpster diving for this kind of stuff
[01:19:50.080 --> 01:19:56.640]   huh no oh my computer you're the modern you're the modern yeah reporter now you know we don't
[01:19:56.640 --> 01:20:02.400]   have to go in the dumpsters anymore mike that's what things baby boomers do yes hey i'm gonna show
[01:20:02.400 --> 01:20:06.080]   you something i mean i'm gonna take off my shirt here i gotta show you something it's really a
[01:20:06.080 --> 01:20:14.000]   superhero i am this is a job for i am stealing the fox out i even have
[01:20:14.000 --> 01:20:23.040]   i even have fox mint mobile socks fox socks ladies and gentlemen i have been converted to
[01:20:23.040 --> 01:20:30.720]   mint mobile let me tell you this is the best the best network for people with smartphones ever
[01:20:30.720 --> 01:20:36.000]   mint mobile dot com slash twin if you're still using one of the big four wireless providers
[01:20:36.880 --> 01:20:44.560]   take a look at your bill it's cray cray my Verizon phone is 90 bucks a month and i was you know i thought
[01:20:44.560 --> 01:20:51.440]   oh it's a good deal unlimited data and texts and phone calls but they're inflating the price because
[01:20:51.440 --> 01:20:57.840]   they have those big retail stores they've got all the hidden fees mint mobile provides the same exact
[01:20:57.840 --> 01:21:02.560]   premium network coverage you're used to at a fraction of the cost because they've eliminated
[01:21:02.560 --> 01:21:09.520]   the middleman everything is online no retail locations and i got to say this is the modern way
[01:21:09.520 --> 01:21:15.120]   to do a cell phone a smartphone every plan on mint mobile mint mobile's routing on the t-mobile
[01:21:15.120 --> 01:21:19.680]   network so if you've got t-mobile in your neck of the woods this is a great solution it comes with
[01:21:19.680 --> 01:21:26.160]   unlimited nationwide text and talk and you stop paying for unlimited data you never use you choose
[01:21:26.160 --> 01:21:33.200]   between plans with three eight or twelve gigabytes of 4g LTE data i actually paid 300 dollars i bought
[01:21:33.200 --> 01:21:38.160]   a year ahead of time because i love that it's a great price 12 gigabytes a month i'm never going
[01:21:38.160 --> 01:21:43.520]   to use that up but if i ever do use more you can buy it at a very affordable price use your own
[01:21:43.520 --> 01:21:49.120]   phone with any mint mobile plan i'm using i love it too by the way the one plus seven pro it's my
[01:21:49.120 --> 01:21:56.080]   new favorite phone and i go around town i do harry potter wizards unite lots of data but i
[01:21:56.080 --> 01:22:02.800]   still well under 12 and it's cost me 25 bucks a month it's amazing use your own phone with any
[01:22:02.800 --> 01:22:08.240]   mobile plan you can keep your phone number along with all your existing contacts all your ditching
[01:22:08.240 --> 01:22:15.360]   that old wireless bill i am such a fan of the fox i can't believe in fact so much so that i'm now i
[01:22:15.360 --> 01:22:21.120]   have accounts with all the major carriers i'm going to dump them all and because why not mint mobile
[01:22:21.120 --> 01:22:26.960]   dot com slash twit get your new wireless plan 15 dollars a month it'll be shipped to your door for
[01:22:26.960 --> 01:22:32.880]   free you get the sim card great support by the way these are you know you're not you're not suddenly
[01:22:32.880 --> 01:22:38.400]   at a at a luck with support they're great mint mobile dot com slash twit
[01:22:38.400 --> 01:22:46.480]   as little as 15 dollars a month mint is the fox is smart he's also pretty cool mint mobile dot
[01:22:46.480 --> 01:22:52.640]   com slash twit love on the sim card they have the fox playing a guitar at a campsite
[01:22:52.640 --> 01:23:00.960]   this is awesome mint mobile thank you mint for saving me a lot of money i don't want to say i'm
[01:23:00.960 --> 01:23:11.280]   all in on the mint but mmm mmm mmm what does it say mint mobile shop clever join the move mint
[01:23:12.960 --> 01:23:16.720]   i am i have to say i was really pleased when they said hey we want to get as i said do you ever hear
[01:23:16.720 --> 01:23:23.760]   of us i said yes i've heard of you 25 bucks a month 12 gigs i've heard of you he's actually drinking
[01:23:23.760 --> 01:23:29.040]   mint tea are they all the way down oh yeah mint tea in my mint mumble mug are uh our
[01:23:29.040 --> 01:23:34.720]   redditors going to invade area 51 in september by the way is that going to happen i hope though
[01:23:34.720 --> 01:23:39.280]   you know the air force says we stand ready i'm not sure that's a good idea
[01:23:41.120 --> 01:23:47.920]   the last time i checked the facebook event for the area 51 raid i think three quarters of a million
[01:23:47.920 --> 01:23:55.600]   people had responded that they were going oh my god so the theory is that there's alien technology
[01:23:55.600 --> 01:24:01.440]   hidden away at area 51 that the government has never told us about it so let's just let's just
[01:24:01.440 --> 01:24:09.040]   go on mass and just march in and get it what could possibly go wrong this is the dumbest thing i've
[01:24:09.040 --> 01:24:17.440]   ever heard they can't stop us all okay it is so much doesn't understand machine
[01:24:17.440 --> 01:24:22.320]   yeah machine guns can stop you all and barbed wire and the air force has planes and and missiles
[01:24:22.320 --> 01:24:27.760]   and i don't know i don't know if this is a good idea didn't didn't they try to levitate the pentagon
[01:24:27.760 --> 01:24:33.280]   in the 60s or something yes this is kind of this is baby boomers do we tried to levinate the pentagon
[01:24:33.280 --> 01:24:41.520]   your kids are trying to air invade area 51 um there was this there was this meme that i saw
[01:24:41.520 --> 01:24:47.120]   and it was just uh it was almost like a battle plan and they had like various types of internet
[01:24:47.120 --> 01:24:53.520]   niche subcultures um but all ones that were like a little bit a little bit off center you know you
[01:24:53.520 --> 01:24:59.280]   had like you had furries i think that was one i feel i think now that i'm only remembering furries
[01:24:59.280 --> 01:25:03.680]   on that list no disrespect in ten different ways but they were like half a dozen different groups
[01:25:03.680 --> 01:25:09.520]   in different colors sort of combining on on area 51 i don't know if i were the government i'd be
[01:25:09.520 --> 01:25:14.480]   scared they're coming this has been the best source there's the attack plan this has been the best
[01:25:14.480 --> 01:25:19.200]   source of memeification ever yeah no ruto runners there's one
[01:25:20.400 --> 01:25:30.800]   all the people battle royale streamers oh my gosh yeah i play for it and i can be i can do this i
[01:25:30.800 --> 01:25:40.080]   can do this we can do this yeah yeah you know even so by now they've moved whatever was there out right
[01:25:40.080 --> 01:25:44.640]   so even if you get in there's not gonna be anything left right well they use the alien technology to
[01:25:44.640 --> 01:25:50.080]   sort of beam themselves to another part of the galaxy you know what are the chances that a
[01:25:50.080 --> 01:25:57.520]   that a place that that flies experimental aircraft that people in would see strange aircraft in the
[01:25:57.520 --> 01:26:09.520]   area how odd is that hmm hmm so uh apple updated the macbook era macbook pro and i think this is
[01:26:09.520 --> 01:26:17.280]   maybe sad killed the 12 inch macbook nothing it was sexy it was innovative was influential
[01:26:17.280 --> 01:26:22.960]   it was the smallest macbook kerry macracken writing a brief eulogy for the 12 inch macbook a
[01:26:22.960 --> 01:26:31.360]   machine of unfulfilled promise a tear a tear shed i thought it was you know it was a little
[01:26:31.360 --> 01:26:36.480]   underpowered and i guess the macbook era kind of probably you know scratches all the itches yep
[01:26:37.360 --> 01:26:42.320]   that the 12 inch did but i you know i like that old macbook i thought that was a pretty sweet
[01:26:42.320 --> 01:26:49.360]   well it reminds us back in the day when apple was so far ahead of everyone else in laptops so far
[01:26:49.360 --> 01:26:54.160]   now they just they just have lost the plot they keep coming out with things that people don't
[01:26:54.160 --> 01:27:00.320]   really want keyboards that don't really work and it's like back you know like five six seven eight
[01:27:00.320 --> 01:27:04.800]   years ago there was just nothing like macbooks nothing they were just perfect they had like the
[01:27:04.800 --> 01:27:12.000]   magsafe connectors they didn't have the dumb strip they you know etc etc but nowadays they
[01:27:12.000 --> 01:27:18.400]   and i know it's using a pixel yep yep do you use a macbook caroline i do yeah i have an air
[01:27:18.400 --> 01:27:24.160]   i have an i love my air yeah and the battery life is great and i carry it around i'm i i'm not a fan
[01:27:24.160 --> 01:27:30.000]   of the butterfly keyboard i have to say yeah my work laptop has the butterfly keyboard and it's
[01:27:30.000 --> 01:27:36.000]   awful it's been causing me problems um yeah there's this thing where if i press the shift key it acts
[01:27:36.000 --> 01:27:40.800]   as if i'm pressing the command key so then the next letter that i press it'll just open some random
[01:27:40.800 --> 01:27:47.040]   app which is great i love that you never know what's going to happen it's kind of like macbook
[01:27:47.040 --> 01:27:54.640]   roulette yeah there was an issue with zoom this is bad behavior zoom which makes the you know
[01:27:54.640 --> 01:27:59.680]   we've all used it who hasn't used it teleconferencing software it's kind of like skype
[01:27:59.680 --> 01:28:05.280]   yeah um and there's a lot of times you would use it uh maybe you'd uninstall it afterwards
[01:28:05.280 --> 01:28:12.800]   guess what even if you uninstalled zoom on your macintosh it put a hidden web server on your
[01:28:12.800 --> 01:28:19.600]   computer that was continuing to run even after uninstall and bad guys could use to turn on your
[01:28:19.600 --> 01:28:28.160]   camera to silently quietly put you in a zoom conference with them apple uh did the right thing
[01:28:28.160 --> 01:28:37.600]   though they just pushed a mac update without your knowledge disabling that server zoom zoom did not
[01:28:37.600 --> 01:28:45.120]   respond too well to the calls to disable it so apple had to zoom released a fixed app on Tuesday
[01:28:45.120 --> 01:28:50.880]   but apple said its actions will protect users past and present from the undocumented web server
[01:28:50.880 --> 01:28:55.680]   vulnerability without affecting or hindering the functionality of the zoom app itself which
[01:28:55.680 --> 01:29:02.000]   means zoom didn't have to do this yeah that's what's that's what's strange because that the whole
[01:29:02.000 --> 01:29:07.600]   the whole selling point of that was it would open up uh the meeting faster and that everything
[01:29:07.600 --> 01:29:13.440]   would be faster but i mean i i guess not it's hard to say well i did if it's always running in the
[01:29:13.440 --> 01:29:17.200]   background but i think that's a bad behavior to always have a web server running in the background
[01:29:17.200 --> 01:29:22.880]   even after you've uninstalled the software yeah that's not nice anyway it's fixed right to know
[01:29:22.880 --> 01:29:27.200]   before i downloaded zoom yeah i didn't have it on my desktop but i had it on my
[01:29:27.200 --> 01:29:31.680]   oh yeah i didn't on my phone i deleted it anyway but it doesn't i mean uh you know
[01:29:31.680 --> 01:29:40.000]   it's not my preferred teleconferencing software but everybody used it all the time so uh if you
[01:29:40.000 --> 01:29:45.840]   i was very excited when the apple watch uh series four with it the most recent one came out with
[01:29:45.840 --> 01:29:51.360]   walkie talkie i thought my wife and i oh we'll just press a button and say lisa are you there
[01:29:51.360 --> 01:29:59.040]   and she'd press a button say hi hi can can can you bring me can you bring me a cup of coffee
[01:29:59.040 --> 01:30:07.760]   what no it's it's not working so um apparently remember the FaceTime vulnerability led bad guys
[01:30:07.760 --> 01:30:13.760]   to get in a FaceTime conversation and watch you well apparently walkie talkie same thing same thing
[01:30:13.760 --> 01:30:20.960]   you could listen to another customer's iPhone without consent apple has disabled the walkie talkie
[01:30:20.960 --> 01:30:29.200]   app pending a fix apple pod size for the bug and the inconvenience of being unable to use walkie
[01:30:29.200 --> 01:30:34.880]   talkie while a fix is made all five walkie talkie users were yes mad as heck and nobody to complain
[01:30:34.880 --> 01:30:40.880]   to i use yeah talkies it's not working i stopped using walkie talkie like four days in it was so
[01:30:40.880 --> 01:30:46.880]   annoying yes i thought this is going to be great it'll be like a next-ale phone yeah i mean apps that
[01:30:46.880 --> 01:30:50.720]   do that have been around forever on phones and people use it for a couple days and they're like yeah
[01:30:50.720 --> 01:30:55.840]   you know what this is not that cool it's not that cool there is one apple feature that is being
[01:30:55.840 --> 01:31:04.400]   used to do god's work in hong kong where the protesters are you know un-happing protesting about
[01:31:04.400 --> 01:31:13.280]   the extradition treaty with china yep the chinese firewall of course is uh is keeping uh them from
[01:31:13.280 --> 01:31:21.360]   accessing content so they're using apple's airdrop allows devices send photos and video over
[01:31:21.360 --> 01:31:26.240]   bluetooth and wi-fi to breach china's great firewall to spread information to mainland chinese
[01:31:26.240 --> 01:31:32.560]   visitors in the city so confused yeah by the protest yeah i don't understand believe airdrop
[01:31:32.560 --> 01:31:38.320]   open and you'll say it'll say oh Fred's going to send you something and you go really and you get
[01:31:38.320 --> 01:31:44.160]   a you get some information uh this is this is uh this is to let people know about the
[01:31:44.160 --> 01:31:51.280]   extradition bill and it's one way around the the great firewall of china one of the things that
[01:31:51.280 --> 01:31:55.920]   the story highlights is something a lot of people don't realize is that hong kong is a huge
[01:31:55.920 --> 01:32:02.000]   tourist attraction for mainland chinese tourists it's fun it's fun it's like wow this is very strange
[01:32:02.000 --> 01:32:07.360]   and this is the whole point because all of that there's a total blackout within mainland china
[01:32:07.360 --> 01:32:12.320]   of the fact that hong kong that what is it one quarter of the population of hong kong at some
[01:32:12.320 --> 01:32:18.720]   point is protested yeah uh just a massive unprecedented uh ongoing sustained protest
[01:32:18.720 --> 01:32:22.960]   which the mainland chinese people have no idea they've heard it yeah it's blacked out completely
[01:32:22.960 --> 01:32:27.840]   blacked out so they so as they stand they're gawking at these protests they're sending details about
[01:32:27.840 --> 01:32:33.440]   what this means what it all is what's going on here's a tweet from allis sushi arrived at tsd
[01:32:33.440 --> 01:32:41.200]   station and immediately her phone is bombarded with simplified chinese flyers by a air drop
[01:32:41.200 --> 01:32:46.080]   explaining you know what's going on simplified chinese is the version that mainland chinese
[01:32:46.080 --> 01:32:50.160]   people use whereas in hong kong they use oh i didn't know that classical chinese that's why
[01:32:50.160 --> 01:32:53.280]   that's why it's newsworthy that they're using simplified it proves that they aimed at
[01:32:53.280 --> 01:32:59.520]   maining chinese visitors wow i didn't know that yeah very interesting so this is fascinating but
[01:32:59.520 --> 01:33:05.360]   you know they're always going to find a way around it i hope it's just the problem is
[01:33:05.360 --> 01:33:10.560]   trying to get mainland chinese people to care of because the big controversy in china is
[01:33:10.560 --> 01:33:15.600]   don't rock the boat we're we're have we're on the road to prosperity let's not go for freedom
[01:33:15.600 --> 01:33:20.720]   let's let's go for prosperity and etc i don't you know it was a feudal nation 50 years ago
[01:33:20.720 --> 01:33:26.720]   we're hundred years ago and and there's made amazing progress but at some cost yeah here's
[01:33:26.720 --> 01:33:31.040]   among other posters did you know over the past month hong kong has seen three massive rallies
[01:33:31.040 --> 01:33:35.120]   with as many as two million people taking to the streets don't wait until freedom is gone to
[01:33:35.120 --> 01:33:40.160]   regret its loss freedom isn't god given its fought for by the people i think we may be using that
[01:33:40.160 --> 01:33:47.200]   soon here yes just remember the news yeah exactly uh wow that's uh so that's an apple technology that
[01:33:47.200 --> 01:33:54.400]   is working quite well yeah really clever i thought isn't that great it's a feature but i think the
[01:33:54.400 --> 01:34:01.200]   chinese communist party would consider it a bug yeah we can't block it yeah apple is making tv
[01:34:01.200 --> 01:34:05.840]   they've already said we don't want to be netflix we're going to do handcrafted artisanal
[01:34:05.840 --> 01:34:10.720]   block brooklyn style shows throwing millions of dollars per episode at these hand made one of the
[01:34:10.720 --> 01:34:16.640]   shows which is called sea about uh i don't know it's it's about a future where everybody's blind yeah
[01:34:16.640 --> 01:34:23.200]   jason mimoa is in it alphra woodard um 15 million dollars an episode that doesn't you know in the
[01:34:23.200 --> 01:34:27.520]   game of thrones zero that doesn't seem like that much well the for the first many seasons of game
[01:34:27.520 --> 01:34:32.400]   of thrones they didn't reach that level so they eventually leave it to 50 million and above so
[01:34:32.400 --> 01:34:38.000]   they're going in just throwing in bigoted first but so far apple has demonstrated a complete lack of
[01:34:38.000 --> 01:34:44.320]   ability to do compelling programming go ahead karline yeah i just think that's i don't know that's
[01:34:44.320 --> 01:34:50.000]   a huge gamble to make up front i don't know i remember remember that i don't even remember the name of
[01:34:50.000 --> 01:34:57.280]   it that like reality type or game oh god it was awful the app yeah yeah yeah and i'm just thinking
[01:34:57.280 --> 01:35:03.760]   it's the app yeah yeah yes yes that's it they had people pitching the so the idea was a time your
[01:35:03.760 --> 01:35:08.640]   pitch to the investors was shark tank basically so they put them on an escalator and they pitch
[01:35:08.640 --> 01:35:12.560]   out to be done with it like they're ready for president we'll see who first put banner chuck in
[01:35:12.560 --> 01:35:18.480]   the chat room it's an escalator pitch it's the escalator pitch bainer chuck was one of the judges
[01:35:18.480 --> 01:35:26.640]   gary vamp chuck along with wenneth paltrow and will i am it was terrible quite a brain trust there it's
[01:35:26.640 --> 01:35:33.200]   terrible but that was made by other people they brought in sony executives and they're and they're
[01:35:33.200 --> 01:35:37.360]   bringing in major talent and all that stuff they're gonna it's a lot of money apples got money but
[01:35:37.360 --> 01:35:42.240]   even when apple took over carpool karaoke they're kind of smothered the charm out of it well they took
[01:35:42.240 --> 01:35:47.360]   james cordon out of it right what's carpool karaoke but they also have a cordon did super high-end
[01:35:47.360 --> 01:35:51.280]   cameras and it was too good better production values and the whole point of it was that it was
[01:35:51.280 --> 01:35:57.760]   cheesy and low budget right and they didn't get that so anyway we'll see if if a the planet of the
[01:35:57.760 --> 01:36:02.080]   blind people is going to be better did you see these stranger things ar ad i didn't see this in
[01:36:02.080 --> 01:36:10.320]   the new york times i i didn't see the actual ad but so if you had so if you had google lens and you saw
[01:36:10.320 --> 01:36:16.560]   the ads for the star court mall which is the huckens indianum all the stranger things that by the way
[01:36:16.560 --> 01:36:22.960]   they took an old 80s mall that was decrepit and empty and rebuilt it set they rebuilt it
[01:36:22.960 --> 01:36:30.400]   it's so cool i haven't seen all of it's no spoilers no but here's what would happen if you pointed
[01:36:30.400 --> 01:36:38.560]   your google lens at the print ad it would come alive that's pretty neat that is one of the stranger
[01:36:38.560 --> 01:36:49.600]   things i've ever seen done we saw we were in hollywood a couple nights ago and one and
[01:36:49.600 --> 01:36:53.520]   around our sunset boulevard they do all the movie the billboards the movie billboards they are
[01:36:53.520 --> 01:36:59.200]   amazing yeah and uh stranger things it's just like there there's a whole several blocks where every
[01:36:59.200 --> 01:37:02.720]   billboard is stranger things it's like you have to read it as you're driving down the street but
[01:37:02.720 --> 01:37:09.280]   these are gigantic it's like bermish avads yeah caroline don't listen although you could use this
[01:37:09.280 --> 01:37:14.000]   for your things boomers do group yes oh my god this is good research here
[01:37:14.000 --> 01:37:24.560]   nobody that's 20s but you would sometimes see even though today sometimes that maybe i don't know
[01:37:24.560 --> 01:37:28.400]   when i was a kid you drive around the country side these old ads for this shaving cream
[01:37:28.400 --> 01:37:33.840]   and the and the whole idea was they would be sequential right and you read them and they would
[01:37:33.840 --> 01:37:37.760]   always be a poem and they would be a joke and would end with bermish av
[01:37:37.760 --> 01:37:48.160]   okay i'm sorry i even brought it up do you want to see a bermish av sign where are they now i don't
[01:37:48.160 --> 01:37:54.160]   know here it is if you so you'd go by and the first sign says if you then don't know
[01:37:55.120 --> 01:38:01.920]   who signs these are you can't have driven very far bermish av
[01:38:01.920 --> 01:38:04.160]   genius marketing june
[01:38:04.160 --> 01:38:12.800]   this bermish av was out of business by 1966 coincidence i think my grandparents had a whole
[01:38:12.800 --> 01:38:19.840]   book of them one of my one of my new hobbies is uh taking pictures of ghost signs which i see
[01:38:19.840 --> 01:38:23.680]   everywhere what's a ghost sign it's when when they had a sign in a building and they just
[01:38:23.680 --> 01:38:27.920]   forgotten about it and still there we have some impediment we do there's a really cool one
[01:38:27.920 --> 01:38:32.400]   we saw driving in what was it it was like uh the chicken there's an egg hatchery or something
[01:38:32.400 --> 01:38:36.000]   yeah yeah yeah there's a lot of old hatcheries one here because there used to be a lot of chicken
[01:38:36.000 --> 01:38:40.480]   farms but there's there's one that uh in france where it's like they're actually advertising
[01:38:40.480 --> 01:38:46.240]   telegraph services wow it's so cool that's kind of a neat uh and in cartographic well here's another
[01:38:46.240 --> 01:38:50.720]   thing you realize we go to a lot of wine countries and everything used to be called champagne so you
[01:38:50.720 --> 01:38:54.960]   go to the calva country in varsalina and it's a champagne because they can't say it anymore because
[01:38:54.960 --> 01:38:59.200]   it's a registered trademark and prosecco and all this stuff everything used to be champagne you see
[01:38:59.200 --> 01:39:04.640]   champagne and all these places where they don't make champagne regions said you can't do that so
[01:39:04.640 --> 01:39:08.240]   amazade's making the remake in the lord of rings he spent a billion dollars for the rights
[01:39:08.240 --> 01:39:14.400]   for the rights and they are also making a game and they're gonna they're not related to which i'm
[01:39:14.400 --> 01:39:19.600]   confused please come on it's an mmo multiplayer massively multiplayer online game
[01:39:19.600 --> 01:39:26.480]   um so this is good they're gonna merchandize the hell out of this right there was a lard of the
[01:39:26.480 --> 01:39:32.320]   rings online i didn't know this uh in this mid-2000s does anybody care about lord of the rings anymore
[01:39:32.320 --> 01:39:38.160]   is that a thing anyone i'm wondering okay we got i mean i don't know i think people like to read
[01:39:38.160 --> 01:39:42.640]   the books i mean i've read that i was over at halfway through the second one harry potter's even
[01:39:42.640 --> 01:39:51.280]   over is it yeah yeah it's kind of sad i think after like like eventually at some point down the post
[01:39:51.280 --> 01:39:57.280]   american like after the books come out and you just merchandise everything and like add new stuff
[01:39:57.280 --> 01:40:01.280]   to the canon yeah people get exhausted it's too much i think that happened with harry potter i have to
[01:40:01.280 --> 01:40:07.440]   see that stupid kid with a lightning on his forehead one more time i was in the bookstore last night
[01:40:07.440 --> 01:40:13.600]   and there's a whole table of harry potter games and they all have harry potter yeah it's vexing
[01:40:13.600 --> 01:40:21.360]   it's vexing cuphead you ever play cuphead yep great game right cuphead no it's uh probably the
[01:40:21.360 --> 01:40:28.080]   hardest game ever uh it's kind of an old 20s cartoon style max flesha cartoon style um they're
[01:40:28.080 --> 01:40:33.920]   making a netflix tv show out of cuphead the cuphead show is in production it will channel the game's
[01:40:33.920 --> 01:40:42.320]   homage to flesha era animation it's very violent very violent yeah as cartoons used to be yeah but
[01:40:42.320 --> 01:40:49.120]   i think it'd be fun yeah yeah all right let's take a break let's more to talk about caroline hasskins
[01:40:49.120 --> 01:40:55.200]   is here for motherboard oh love the motherboard i just want to say love the motherboard could you do
[01:40:55.200 --> 01:41:02.080]   a freedom of information act request of area 51 wow good question wouldn't that be i mean let's
[01:41:02.080 --> 01:41:09.680]   try be simpler than invading it yeah and easier and if you get something that's redacted redacted
[01:41:09.680 --> 01:41:16.400]   that's gonna be that's something that's an article yeah yeah she's excited about it can't tell
[01:41:16.400 --> 01:41:22.880]   mike elgens also here elgens.com if you missed anything this week we had a fun week on twitter
[01:41:22.880 --> 01:41:28.320]   here's a little sample of some of the things on twitter unexpected announcement this week creative
[01:41:28.320 --> 01:41:34.080]   celebrates 30 years of sound blaster with new ae9 and ae7 sound cards like you know the nintendo
[01:41:34.080 --> 01:41:38.560]   consoles have enjoyed a renaissance with the the nes mini and the snes mini and they'll
[01:41:38.560 --> 01:41:45.440]   capitalize on this trend by serving a market of 10 to 15 people globally with reimagined
[01:41:45.440 --> 01:41:51.920]   game blaster cards in vintage packaging creative if you're watching uh-huh what by one
[01:41:52.960 --> 01:41:58.640]   ios today so the new ios 13 came out yesterday if you're in the public beta we all downloaded
[01:41:58.640 --> 01:42:03.840]   the public beta we should demonstrate this but i don't know if we can really do it properly now
[01:42:03.840 --> 01:42:10.000]   look at me look at the camera now look at me you know we got there i feel like i ran a marathon
[01:42:10.000 --> 01:42:15.840]   today i'm so proud all about android you remember a couple of weeks ago where i brought on my
[01:42:15.840 --> 01:42:20.880]   motoral azim and i couldn't power it on because of the power supply you found it i got the power
[01:42:20.880 --> 01:42:26.640]   supply how to shift to me play music remember look at that look at the upper left end menu
[01:42:26.640 --> 01:42:33.680]   triangulation bitcoin is based off this uh wonderful horrible idea
[01:42:33.680 --> 01:42:40.960]   i'm not that interested in what is the price of bitcoin right i'm interested in the technology
[01:42:40.960 --> 01:42:46.560]   bitcoin's whole point is that it's decentralized but it's not as decentralized as you'd really like
[01:42:46.560 --> 01:42:55.040]   it to be to it technology isn't always pretty but we are that was a triangulation with bram co and
[01:42:55.040 --> 01:43:02.080]   the guy who invented bit torrent yeah and maybe he's mad that bitcoin took his bit yeah i just just
[01:43:02.080 --> 01:43:09.600]   a bit here's a quote this was a tweet from theo fight the question was explain bitcoin to grandpa
[01:43:11.360 --> 01:43:19.920]   and the explanation was imagine if keeping your car idling 24/7 produced solved sotoku puzzles
[01:43:19.920 --> 01:43:27.360]   you could trade for heroin that's bitcoin i like that i finally understand it that makes sense
[01:43:27.360 --> 01:43:32.960]   now i understand it our show today brought to you by wasabi not that green hot green stuff next to
[01:43:32.960 --> 01:43:38.720]   your sushi but it is hot it's hot cloud storage from two of my favorite people in the world
[01:43:38.720 --> 01:43:43.760]   david friend the founder of carbonite his ct o jeff flowers it was actually i think jeff who
[01:43:43.760 --> 01:43:50.480]   created a patented technology for writing a hard drives sequentially not in blocks that's how
[01:43:50.480 --> 01:43:55.840]   every other hard drive works but by writing sequentially they were able to get improved speed
[01:43:55.840 --> 01:44:00.800]   improve reliability that's how carbonite was founded they've gone on to found this new wasabi
[01:44:00.800 --> 01:44:06.720]   how cloud storage and it's incredible they are able to using this revolutionary process give you
[01:44:06.720 --> 01:44:14.560]   enterprise grade cloud storage that's one fifth the cost of amazon s3 and six times faster
[01:44:14.560 --> 01:44:21.840]   one fifth the cost six times faster they never have hidden fees for egress or api
[01:44:21.840 --> 01:44:27.040]   fact they have a great api because it's amazon s3's api so they're completely compatible that you
[01:44:27.040 --> 01:44:32.720]   know already know how to use it eleven nines of durability they do integrity checking on your data
[01:44:32.720 --> 01:44:35.920]   and this is so great in this day and age of ransomware
[01:44:35.920 --> 01:44:43.200]   they do immutable data so you can say this data may not be changed not by ransomware not by a
[01:44:43.200 --> 01:44:49.680]   fumble fingered employee not by me not by anybody immutable data is the secret to keeping your data
[01:44:49.680 --> 01:44:56.720]   safe hip a compliant finry compliant cjis compliant where everybody's moving to the cloud
[01:44:56.720 --> 01:45:03.280]   gardener group says by 20 25 80 of businesses will have shut down their data center 10
[01:45:03.280 --> 01:45:09.120]   today are in the cloud it's going to be 80 in five years zettabytes well let me tell you i know
[01:45:09.120 --> 01:45:12.720]   if you're you know and i'm a lot of businesses thinking about this right now maybe you're charged
[01:45:12.720 --> 01:45:17.120]   with looking into this you're going to look at amazon google and microsoft i know you are
[01:45:17.120 --> 01:45:23.360]   but can i add a fourth name to the list take a look at wasabi one fifth the cost six times faster
[01:45:24.320 --> 01:45:29.760]   immutable storage this is an amazing company with a revolutionary solution
[01:45:29.760 --> 01:45:36.800]   and and and because they are eleven nines of durability you just don't have to worry your data is
[01:45:36.800 --> 01:45:42.960]   actually more secure safer than it probably beyond premises so and with the wasabi ball which i
[01:45:42.960 --> 01:45:48.080]   love that name the wasabi ball is not a green ball of hot stuff it is actually a giant drive
[01:45:48.080 --> 01:45:52.400]   array that they send to you you put your data on it you move it up to the cloud instantly like that
[01:45:52.400 --> 01:45:56.000]   look we've got unlimited storage for you for a month so you can really bang on this if you go
[01:45:56.000 --> 01:46:01.440]   to wasabi.com w-a-s-a-b-i click the free trial link enter the code twit
[01:46:01.440 --> 01:46:07.520]   join the movement migrate your data to the cloud with confidence wasabi.com don't forget that
[01:46:07.520 --> 01:46:12.880]   offer code twit thank you wasabi thank you david and jeff for supporting us they've been supporting
[01:46:12.880 --> 01:46:19.600]   us for a long time they're really twit fans and i thank you to it fans for supporting us by using
[01:46:19.600 --> 01:46:23.200]   the offer code twit at wasabi.com
[01:46:23.200 --> 01:46:29.200]   uh watch out by the way we were mentioning prime day i just want a little public service
[01:46:29.200 --> 01:46:36.000]   announcement there are a number of fishing scams around prime day so you know it happens at tax
[01:46:36.000 --> 01:46:46.160]   time the fishers are just terrible uh prime day uh fishing scams that look like emails from amazon
[01:46:46.960 --> 01:46:53.760]   just be careful what you click on is that part of your things boomers do group click on fishing scams
[01:46:53.760 --> 01:47:00.800]   i think that i think to be fair i think that i don't know reply all did a good thing about this
[01:47:00.800 --> 01:47:07.040]   also like there's like a lot of people fall victim to fishing scams like even if you're
[01:47:07.040 --> 01:47:13.040]   even if you consider yourself like a generally competent person or a skeptical person too easy
[01:47:13.040 --> 01:47:18.720]   yeah yeah it's very easy it's really i feel like i'm skeptical of everything in my inbox now yeah
[01:47:18.720 --> 01:47:24.720]   like i got a i got a email like asking to do like a survey for work and i was like emailing all my
[01:47:24.720 --> 01:47:29.280]   colleagues like is this real am i being picked right that's what you have to do now right yeah
[01:47:29.280 --> 01:47:36.160]   because if it's from work that's almost a certain this guaranteed to be a scam right yeah
[01:47:36.160 --> 01:47:41.520]   or if it's from yourself or you want you know a relative it's not from strangers were you a
[01:47:41.520 --> 01:47:46.160]   were you a friends fam caroline were you a fan of the friends tv show you're too young for that
[01:47:46.160 --> 01:47:52.880]   probably friends i've i've watched friends it's like it's fine i didn't realize it was the number
[01:47:52.880 --> 01:47:56.880]   one streaming show on netflix that in the office or hud i didn't know that it's leaving
[01:47:56.880 --> 01:48:04.320]   it's moving off of netflix it's going to something new called hbo max oh boy just another thing you're
[01:48:04.320 --> 01:48:14.160]   gonna want to pay for thank you at and t hbo max will be uh over the top streaming service you know
[01:48:14.160 --> 01:48:22.160]   hbo already has hbo go right hbo now that's going to be shutting down that is yes when when hbo max
[01:48:22.160 --> 01:48:28.240]   launches hbo now we'll shut 18 t is going to ruin hbo remember where the att executive came into
[01:48:28.240 --> 01:48:34.960]   hbo and said we want to be more like netflix i thought oh boy yeah um so you'll be able to get
[01:48:34.960 --> 01:48:40.800]   friends on hbo max as well as fresh prince of bell air whoo who this is hot programming pretty
[01:48:40.800 --> 01:48:45.280]   little liars whoo whoo and i guess all the hbo shows if they're gonna if they're not gonna have hbo
[01:48:45.280 --> 01:48:55.840]   go on hbo now cw shows including batman and katie keen will be sounds like a flop i'm sorry exclusive
[01:48:55.840 --> 01:49:04.160]   movies with reese witherspoon there's a name that's gonna two romantic comedies and four young
[01:49:04.160 --> 01:49:08.240]   adult titles with greg brilandy i don't even know who that is mayo i'm not a young adult
[01:49:08.240 --> 01:49:16.800]   what about us boomers and an anacendrick led comedy series called love life and animated
[01:49:16.800 --> 01:49:24.880]   prequel series for gremlins this sounds like compelling very sounds like the the worst
[01:49:25.760 --> 01:49:31.200]   uh it's gonna debut in spring 2020 with 10 000 hours of material
[01:49:31.200 --> 01:49:41.440]   like dune the sister whole friends is it right and yeah that's it okay netflix tweeted the one
[01:49:41.440 --> 01:49:46.080]   where we have to say goodbye we're sorry to see friends go to warner streaming service at the
[01:49:46.080 --> 01:49:49.920]   beginning of 2020 thanks for the memories gang and then there's a little coffee cup
[01:49:51.040 --> 01:49:53.840]   which i'm sure means something in friends friends language
[01:49:53.840 --> 01:50:00.400]   we don't know what the price will be uh but i could tell you right now uh if they got game of
[01:50:00.400 --> 01:50:05.840]   thrones on there i guess oh wait a minute it's over now that it's over they should have timed
[01:50:05.840 --> 01:50:08.640]   that better and so france
[01:50:08.640 --> 01:50:18.560]   that's all i'm gonna say france etienne france etienne's in our studio he lives in france has
[01:50:18.560 --> 01:50:27.760]   decided to tax tech companies three percent on digital services if you make more than 750
[01:50:27.760 --> 01:50:33.040]   million euros in global revenue and 25 million in french revenue in other words if you're google
[01:50:33.040 --> 01:50:40.080]   facebook or amazon you're gonna pay a three percent tax three percent of your total annual revenue
[01:50:40.080 --> 01:50:47.200]   that's a ton and and what's wrong with that i mean i think why
[01:50:47.920 --> 01:50:52.720]   the other question is why should tech companies be tax-free no they shouldn't be but they shouldn't
[01:50:52.720 --> 01:51:00.880]   be specially taxed either well um right should there be a like a surcharge for being a successful
[01:51:00.880 --> 01:51:07.520]   u.s tech company how much how much u.s income tax to amazon pay this year you know that's a that's
[01:51:07.520 --> 01:51:14.160]   kind of a major league that they yeah i think i got a nice check h&r block gave him an advance
[01:51:14.160 --> 01:51:20.320]   on it was nice it was like you know but but how much was it no no it's not zero it's not zero everybody
[01:51:20.320 --> 01:51:28.320]   says amazon paid zero and uh it's very low it's very low it's low okay because they spent money they
[01:51:28.320 --> 01:51:34.560]   made they made they put you know here's the thing the the tax situation in france is a really touchy
[01:51:34.560 --> 01:51:39.760]   subject you can verify this i've talked a lot i got a lot of friends in france we spent a lot of time
[01:51:39.760 --> 01:51:45.520]   in france um i've been told that the that the income tax rate can be like 70 percent is that
[01:51:45.520 --> 01:51:53.280]   roughly is that an exaggeration it's high ridiculously high right yes high services you get what you
[01:51:53.280 --> 01:52:01.280]   pay for and so we pay high taxes and get nothing but but but in that tax environment to tax these big
[01:52:01.280 --> 01:52:06.240]   companies three percent or something is really a tiny drop in the bucket and i think you know i
[01:52:06.240 --> 01:52:12.800]   think uh i personally am in favor of national governments doing more to police to to tax to do
[01:52:12.800 --> 01:52:17.040]   whatever they want with these international companies i was listening to this uh political
[01:52:17.040 --> 01:52:24.720]   podcast uh where they were taught saying how the british parliament called mark zuckerberg to
[01:52:24.720 --> 01:52:30.800]   testify and he's like no i'm not gonna he said no he said and so in fact he can't he can't go to
[01:52:30.800 --> 01:52:37.520]   canada he'll be arrested as he said no to canada fair enough but i i think but my point is the the
[01:52:37.520 --> 01:52:43.280]   the podcasters were saying dumb things namely that wow zuckerberg the reason is that that facebook
[01:52:43.280 --> 01:52:47.520]   is more powerful than the british government no the difference is the british government isn't
[01:52:47.520 --> 01:52:53.440]   using its power it what they should say is that we're shutting off facebook until mark zuckerberg
[01:52:53.440 --> 01:52:57.760]   appears before us this is a problem though i think it's fantastic we're gonna have a
[01:52:57.760 --> 01:53:01.840]   american internet a chinese internet a russian internet a european internet that's a different
[01:53:01.840 --> 01:53:08.160]   question things like the right to be forgotten that's a splinternet but taxing or or or banning a
[01:53:08.160 --> 01:53:14.480]   social network until their ceo comes before the government is i think perfectly reasonable i don't
[01:53:14.480 --> 01:53:19.760]   mind yeah attacks i just don't think there should be more tax because you're a successful tech company
[01:53:19.760 --> 01:53:25.360]   i mean lots lots of taxes are levied for lots of reasons and you know in europe like the cost of
[01:53:25.360 --> 01:53:29.920]   gasoline super high because they're taxing that for all kinds of reasons i think it's the united
[01:53:29.920 --> 01:53:35.200]   states trade representative said we're gonna investigate and there may even be a tariff war
[01:53:35.200 --> 01:53:41.200]   as a result go ahead at carolyn what do you think oh no i think it's just that i mean if citizens are
[01:53:41.200 --> 01:53:46.800]   treated like a well of data that the company is being is using profit it's only fair to expect the
[01:53:46.800 --> 01:53:52.000]   company to pay back into services that will help those people that's and i think like the sentiment
[01:53:52.000 --> 01:53:58.320]   that like the sentiment that it's like anti-american at any kind of way i mean that's that's ridiculous
[01:53:58.320 --> 01:54:03.840]   i mean they like the solutions that is just tax the companies in the u_s but obviously the people
[01:54:03.840 --> 01:54:08.880]   who are saying that it's anti-american would not be in favor of that kind of tax but i don't know
[01:54:08.880 --> 01:54:14.400]   that's just my opinion with it the big finds it like for example against google for uh for for
[01:54:14.400 --> 01:54:19.600]   favoring their own services and search results all those kinds of things are also said called
[01:54:19.600 --> 01:54:24.960]   anti-american i think i've said that myself but but it's really american companies are complaining
[01:54:24.960 --> 01:54:29.120]   to european authorities about other american companies so it's like also favoring american
[01:54:29.120 --> 01:54:35.840]   companies for you know namely microsoft etc uh to to to to sort of ding these giant companies that are
[01:54:35.840 --> 01:54:43.680]   you know appear to be favoring their own search results one more break let's uh i got a i've got a
[01:54:44.640 --> 01:54:52.400]   obituary for a computer science pioneer you never heard of but he's done two things that you will
[01:54:52.400 --> 01:54:59.360]   know one good one bad okay that's coming up our show today brought to you by zip recruiter if you
[01:54:59.360 --> 01:55:04.400]   need to do we just recently done a lot of hiring i gotta say zip recruiter is the bee's knees as my
[01:55:04.400 --> 01:55:11.440]   people say it is it is the easiest way to hire if you're the person in charge of hiring you know
[01:55:12.480 --> 01:55:17.840]   that's it it's a terrible job because first of all you're down a person or two or three or whatever so
[01:55:17.840 --> 01:55:22.480]   you're working harder anyway especially if you're small business like ours but also the person you're
[01:55:22.480 --> 01:55:27.680]   hiring that could make or break your company companies are just a bunch of people with a common
[01:55:27.680 --> 01:55:33.040]   purpose a great employee and i think we just hired some really great employees to the moon
[01:55:33.040 --> 01:55:39.840]   but somebody not so great could bring you down zip recruiter is going to improve your
[01:55:39.840 --> 01:55:43.360]   chance of getting that magic employee there's somebody out there that's just right for your
[01:55:43.360 --> 01:55:48.240]   opening the question is how do you reach that person zip recruiter zip recruiter the first thing
[01:55:48.240 --> 01:55:52.880]   that happens posts your job listing on zip recruiter to more than a hundred job sites with one click
[01:55:52.880 --> 01:55:58.320]   including social networks like twitter and facebook so you're going to reach the largest number of
[01:55:58.320 --> 01:56:03.440]   candidates that's great that means that person that perfect person no matter where they're hiding
[01:56:03.440 --> 01:56:07.680]   it's going to see your job posting but zip recruiter does it one more one step better
[01:56:08.720 --> 01:56:14.320]   they use powerful matching technology to scan all the resumes to find people who have the
[01:56:14.320 --> 01:56:19.600]   experience that be right for you and then invite them to apply which means you're gonna they're
[01:56:19.600 --> 01:56:23.280]   actually they're actually saying hey this is a great job come here you got to see this
[01:56:23.280 --> 01:56:27.680]   and as those applications come in and all the other applications zip recruiter will analyze
[01:56:27.680 --> 01:56:33.520]   each one spotlight the top candidates so you don't miss a great match all the applications
[01:56:33.520 --> 01:56:37.920]   they don't go into your mail your inbox or your phone they go right into the zip recruiter interface
[01:56:38.560 --> 01:56:42.960]   this matching technology is so powerful four out of five employers who post on zip recruiter get a
[01:56:42.960 --> 01:56:47.200]   quality candidate on the site within the first day that's been our experience by the way it's
[01:56:47.200 --> 01:56:53.200]   exactly within the first few hours right now you can try zip recruiter free we've got a special
[01:56:53.200 --> 01:56:58.320]   address for you zip recruiter dot com slash twit show your support for the show just that zip
[01:56:58.320 --> 01:57:03.360]   recruiter has shown its support by going to zip recruiter dot com slash twit we love zip recruiter
[01:57:03.360 --> 01:57:11.280]   with it's been such a great thing for us z i p r e c r u i t e r dot com slash twit
[01:57:11.280 --> 01:57:15.680]   zip recruiter since that french language and messes up our spelling
[01:57:15.680 --> 01:57:25.680]   ochre ochre ochre zipp recruiter um yeah we hired we've hired uh one two three
[01:57:25.680 --> 01:57:31.200]   four new people in the last month some of whom you don't even know about yet but you will hear
[01:57:32.160 --> 01:57:37.920]   soon niantic now i said i don't like harry potter but i can't stop playing harry potter
[01:57:37.920 --> 01:57:42.960]   wizards united's the pokemon go replacement niantic was a spinoff of google remember they
[01:57:42.960 --> 01:57:48.480]   were the google maps guys they first app you know what their first app was field trip remember that
[01:57:48.480 --> 01:57:55.040]   yes that's great turn on mike mike's mic oh yes yes that was great so field trip was the
[01:57:55.040 --> 01:57:57.920]   app where you'd go around and would tell you in fact it was fun and betaluma because there's
[01:57:57.920 --> 01:58:04.480]   only like four points of interest do you know the the movie remembering the abbots was shot in
[01:58:04.480 --> 01:58:10.000]   that house yes i know you told me every time i tried to find american graffiti was shot right
[01:58:10.000 --> 01:58:14.400]   here on this empty lot there's four points of interest anyway they're shutting it down it's
[01:58:14.400 --> 01:58:20.720]   over for field trip which is another just how google works isn't it yeah yeah they're the new yahoo
[01:58:20.720 --> 01:58:29.360]   yeah now there's chatterbox is this is this uh oh wait a minute that's uh i'm a safe chatterbox for
[01:58:29.360 --> 01:58:34.240]   the end that wasn't it there's a new google what is it called safety pin oh well it's shoelace
[01:58:34.240 --> 01:58:42.160]   shoelace shoelace their boots traveling i knew it was a common household object so what is what
[01:58:42.160 --> 01:58:49.920]   does shoelather do shoelace is a so i'm i'm especially bitter about this one leo so let me
[01:58:49.920 --> 01:58:54.320]   allow me to express my settings you you already you went all in on google plus yes and your heart
[01:58:54.320 --> 01:59:06.880]   was broken yes what is shoehorn going to do shoe leather is a shoelace it's supposedly a social
[01:59:06.880 --> 01:59:13.600]   network organized around events and other things that are geographically specific so this is from
[01:59:13.600 --> 01:59:20.720]   google's research army yes 120 which is not area 51 do not invade it this great uh although if you
[01:59:20.720 --> 01:59:24.960]   do invade it this would be a great way to organize that interview right now it's only in new york
[01:59:24.960 --> 01:59:29.360]   in fact you could use it carolina if you tried shoelather i haven't now
[01:59:29.360 --> 01:59:38.240]   i'm just playing into this whole baby boomer thing did you see have you seen what i'm wearing
[01:59:38.240 --> 01:59:43.680]   a mint socks have you seen that nevermind yeah lovely yeah lovely so so here's what here's my
[01:59:43.680 --> 01:59:48.880]   shoe what is shoelace it's it's this dumb thing that is basically supposedly a social network but
[01:59:48.880 --> 01:59:54.800]   it's focused on location location based so it's basically it's basically it's like five percent of
[01:59:54.800 --> 01:59:58.880]   google plus they decided to come out with the spectacularly insignificant thing i have to point
[01:59:58.880 --> 02:00:03.600]   out they did this already yes they did several times schema which i tried that was the one with
[02:00:03.600 --> 02:00:10.320]   a mustache this is basically schema that one was shut down in three years yeah they just need more
[02:00:10.320 --> 02:00:14.160]   things to shut down so they can't shut down things unless they launch things so it's an invite only
[02:00:14.160 --> 02:00:24.560]   testing phase on ios and android if you want though you can uh go to this url docs.co.com/form/de/1fai
[02:00:24.560 --> 02:00:33.200]   qls i'll forget it and you can say keep me in the loop about shoelace get it oh
[02:00:33.200 --> 02:00:45.120]   i want to put a bow on this story and move on shoelace why this with google they really suffer
[02:00:45.120 --> 02:00:50.720]   from the inability to understand how people communicate yes that's the humans we don't get them
[02:00:51.520 --> 02:00:56.000]   this was a great uh uh p i have to say the privacy project in the new york times have been very
[02:00:56.000 --> 02:01:00.240]   interesting charlie wartsell and ash and goo created i don't know if that's how you pronounce
[02:01:00.240 --> 02:01:08.560]   new maybe uh new yeah created uh a comparison between google's original privacy policy in the
[02:01:08.560 --> 02:01:14.080]   in the late 1990s was 600 words to their current privacy policy for 20 years later
[02:01:14.080 --> 02:01:23.200]   four thousand words it's called all the way down the page here it is really more it's less about
[02:01:23.200 --> 02:01:30.080]   google than about how the internet has changed and how technology has changed because in 1999
[02:01:30.080 --> 02:01:35.840]   there wasn't any you know no smartphones they weren't collecting mobile information at all
[02:01:35.840 --> 02:01:41.440]   in 1999 they say google may share information about users with advertisers business partners
[02:01:41.440 --> 02:01:47.200]   sponsors and other third parties but we only talk about our users in aggregate not as it not as
[02:01:47.200 --> 02:01:55.840]   individuals that that was cut out three months later uh 20 years later uh it's it's quite the
[02:01:55.840 --> 02:02:03.680]   opposite we will share information things we know about you everything and we will share it with
[02:02:03.680 --> 02:02:10.640]   everybody and uh it's a little longer location information they didn't have that before
[02:02:10.640 --> 02:02:14.720]   your android device type carrier name crash reports which apps are installed
[02:02:14.720 --> 02:02:19.920]   so i it isn't so much demonizing google i think as just saying
[02:02:19.920 --> 02:02:24.320]   she's pointing out how modern technology like smartphones has given them so much
[02:02:24.320 --> 02:02:29.760]   more information and it's really they're they're stapling on new things every time they get caught
[02:02:29.760 --> 02:02:34.160]   doing something or every time they start with gdpr to be fair yeah gm
[02:02:36.080 --> 02:02:41.200]   i think like the average person just figures like if they've been using google for a long time like
[02:02:41.200 --> 02:02:46.160]   i don't think it registers in the average person's head just how many changes have occurred i don't
[02:02:46.160 --> 02:02:51.920]   know just between like 2005 and now for example just like this scope of the information that they
[02:02:51.920 --> 02:02:56.160]   have on you that they didn't necessarily even have before yeah i don't know i don't even think
[02:02:56.160 --> 02:02:59.600]   that registers in the average person that's why i like this and i don't know why but the times
[02:02:59.600 --> 02:03:03.920]   marks at a pro an opinion piece i guess everything on the privacy project is an opinion piece but i
[02:03:03.920 --> 02:03:08.640]   think it's a really interesting article and it is it is an eye opener i don't think people are really
[02:03:08.640 --> 02:03:12.400]   aware of how this has changed because nobody reads it right they didn't read it when it was 600
[02:03:12.400 --> 02:03:18.400]   words it didn't read it minutes a thousand or four thousand i think that amazon's privacy policies
[02:03:18.400 --> 02:03:21.920]   are still pretty short because they don't want to talk about it and they don't want to guarantee
[02:03:21.920 --> 02:03:27.360]   anything short isn't better you know it's not and i've i've read their privacy policy around a
[02:03:27.360 --> 02:03:32.720]   and because because i was wondering you know do they reserve them for themselves the right to
[02:03:32.720 --> 02:03:38.400]   listen without the wake word do they xyz and they pretty much give themselves the ability to do
[02:03:38.400 --> 02:03:43.280]   anything and their privacy policy doesn't say anything about them we make no promises use it
[02:03:43.280 --> 02:03:49.360]   uh or don't use it but uh we make no promises yeah so and then as if you thought google assistant
[02:03:49.360 --> 02:03:54.400]   or google's home devices were any less intrusive well you probably be wrong you be wrong google
[02:03:54.400 --> 02:03:59.840]   workers listen to your queries but you know this is part i mean all these businesses do this again
[02:03:59.840 --> 02:04:05.040]   i think this is like you as you said caroline it's just a case of people don't really think about it
[02:04:05.040 --> 02:04:09.600]   and so they don't pay attention but if they thought about it yeah one of the things that happens
[02:04:09.600 --> 02:04:14.640]   with both amazon's echo google and i bet you cortana and serie too is they need humans to listen
[02:04:14.640 --> 02:04:22.320]   to it to see if the computer understood it and it how it's how they get better uh vrt news which is
[02:04:22.320 --> 02:04:29.760]   a flamish public broadcaster in belgian we all we all watch we all read that yeah well if you if
[02:04:29.760 --> 02:04:34.960]   you if you if you speak flamish is really the place to go right where else we can finish uh
[02:04:34.960 --> 02:04:42.160]   was able to so apparently somebody leaked the user voice recordings from within google to this
[02:04:42.160 --> 02:04:49.840]   belgian organization uh a google subcontractor passed on more than a thousand assistant recordings
[02:04:50.480 --> 02:04:54.560]   vrt said in these recordings we clearly hear addresses and other sensitive information
[02:04:54.560 --> 02:05:00.160]   this made it easy for us to find the people involved and confront them with the audio recordings
[02:05:00.160 --> 02:05:04.800]   and they they said yep that's my voice i said that wow here's my favorite factoid from the article
[02:05:04.800 --> 02:05:11.280]   153 of the one thousand recordings of the command ok google was clearly not given
[02:05:11.280 --> 02:05:17.360]   uh oh so misunderstood or for some reason without people doing the wake word they started
[02:05:17.360 --> 02:05:21.680]   recording and they maintained and they retain that recording remember they only got a thousand
[02:05:21.680 --> 02:05:28.480]   recordings more than 15 percent of them didn't have the wake word and included bedroom conversations
[02:05:28.480 --> 02:05:34.160]   conversations between parents and their children blazing rouse and professional phone calls
[02:05:34.160 --> 02:05:38.800]   containing lots of private information and in really blazing rouse or the worst kind that's
[02:05:38.800 --> 02:05:44.560]   gotta be a flamish phrase it's got it we uh google's response uh to ours technica
[02:05:44.560 --> 02:05:50.960]   actually to vrt we just learned that one of our language reviewers has violated our data
[02:05:50.960 --> 02:05:57.280]   security policies by leaking confidential dutch audio data our security and privacy response
[02:05:57.280 --> 02:06:02.560]   teams have been activated oh god i bet those are robots with laser eyes and are investigating
[02:06:02.560 --> 02:06:06.640]   we will take action we're conducting a full review of our safeguards in this space to
[02:06:06.640 --> 02:06:09.840]   prevent misconduct like this from happening again this is gonna happen right
[02:06:09.840 --> 02:06:14.320]   that may be the really thing people always say oh i don't want an amazon in my home because i
[02:06:14.320 --> 02:06:18.960]   don't want amazon listening to me bigger the problem may be that it's not just amazon
[02:06:18.960 --> 02:06:27.120]   uh it is not just amazon yeah just like remember a couple of months ago when i think someone used
[02:06:27.120 --> 02:06:33.280]   uh some gdpr clause and requested all of the data that amazon had about themselves through
[02:06:33.280 --> 02:06:37.680]   their echo device and they sent the wrong person's information the wrong person's
[02:06:37.680 --> 02:06:42.400]   recordings to them yeah that's what i that's what i thought about when i read this and it's
[02:06:42.400 --> 02:06:47.760]   there's gonna be individual instances like this and it's just i don't know i mean obviously it
[02:06:47.760 --> 02:06:52.400]   raises the question like wait a minute actually the ice cream truck's out front run out get yourself
[02:06:52.400 --> 02:06:56.480]   a frozen tree wow i guess the audio is very good
[02:06:58.320 --> 02:07:02.560]   why just they move into brooklyn is the place to be you got an ice cream truck
[02:07:02.560 --> 02:07:08.880]   is it just calm down my block about five times a day i've never got an ice cream though what
[02:07:08.880 --> 02:07:15.920]   you don't go chasing it no that's the fun of having an ice cream truck some people do i guess
[02:07:15.920 --> 02:07:21.680]   it's it's not all right i'm sorry i mean you're ripped you but the thought of a frozen treat got
[02:07:21.680 --> 02:07:27.920]   me excited it's a nice light something boomers do it's don't pay no attention
[02:07:27.920 --> 02:07:38.960]   we in my day we would chase that ice cream truck mr softie mr softie stop i i want a rocket pop
[02:07:38.960 --> 02:07:44.960]   what was your favorite i i don't like drumsticks yeah we didn't look like i didn't grow up in brooklyn
[02:07:44.960 --> 02:07:51.760]   okay didn't have the ice cream trucks i'm sorry maybe this has just gone on too long um
[02:07:51.760 --> 02:08:00.160]   did we cover every possible angle of the tech week everything going on twitter's gonna try
[02:08:00.160 --> 02:08:10.240]   its hide replies feature next week in canada not here it's so actually this is a i think this is
[02:08:10.240 --> 02:08:14.800]   kind of a significant story because we've had many conversations uh on this show and also this
[02:08:14.800 --> 02:08:20.480]   week in google about uh my prescription for the problems that plague twitter and my prescription
[02:08:20.480 --> 02:08:26.880]   has always been let users delete yes if they come up if i tweet do a tweet and somebody comments on
[02:08:26.880 --> 02:08:30.800]   the tweet i should be able to delete it for everyone so this is that's kind of google plus did and
[02:08:30.800 --> 02:08:35.440]   that's works plus did facebook does it too but this in in this case you kind of halfway there you
[02:08:35.440 --> 02:08:40.880]   can hide people's comments and they go into it sort of a hidden area it's not that great of a
[02:08:40.880 --> 02:08:45.280]   solution but it's a step in the right direction giving the original poster control over the
[02:08:45.280 --> 02:08:48.800]   comments i think that's always that how they're gonna do because i thought it was gonna be they
[02:08:48.800 --> 02:08:54.160]   would look at this is different from the one where they say if it's from a famous or political
[02:08:54.160 --> 02:08:59.760]   figure don't look at this right then put a thing up are you sure this is different yeah this is
[02:08:59.760 --> 02:09:05.200]   different this is different oh so you post a comment and i go in there with a nasty comment
[02:09:05.200 --> 02:09:09.040]   and you can just say you don't hide that it's if people want to go in and see the hidden comments
[02:09:09.040 --> 02:09:15.600]   they can but they know that people are lazy and probably won't so it devalues the the comments
[02:09:15.600 --> 02:09:20.080]   that the poster wants devalued and some of the criticism i think that i disagree with quite a bit
[02:09:20.080 --> 02:09:24.480]   which is that well with somebody's fact checking what if somebody's bothered about i think that on
[02:09:24.480 --> 02:09:29.360]   a social network you should be able to manage and and and moderate the conversation that follows
[02:09:29.360 --> 02:09:35.760]   of your own tweets if people want to disagree on their own stream they can but i i think people
[02:09:35.760 --> 02:09:42.080]   should have control over your comments uh works and gains traction i hope so yeah i think one of
[02:09:42.080 --> 02:09:47.680]   twitter's like better features is just the where it hides the some of the bad replies at the bottom
[02:09:47.680 --> 02:09:51.680]   i don't know it's vastly improved my twitter experience i got the new twitter and it looks very
[02:09:51.680 --> 02:09:56.480]   different it's like i it's no longer out it's no longer chronological or anything it's just like i
[02:09:56.480 --> 02:10:03.120]   don't they've really changed the feed well they have an option to flip over two recent tweets first
[02:10:03.120 --> 02:10:07.680]   and then but that it only lasts a few days and then they go back to algorithmically sorted which
[02:10:07.680 --> 02:10:14.000]   is kind of to me it's kind of a dark pattern yeah i don't know what it's doing but um i think
[02:10:14.000 --> 02:10:18.560]   clearly what people want is many people want is reverse chronological with no algorithm and
[02:10:18.560 --> 02:10:24.000]   that's why you use tweet deck because it still does that yes right yeah or if you do like if you
[02:10:24.000 --> 02:10:29.520]   pull up like the mobile twitter url like because it doesn't like i can do the whole like display
[02:10:29.520 --> 02:10:35.440]   latest tweets first on my on my mobile app but on desktop i like type in the the mobile url and
[02:10:35.440 --> 02:10:42.480]   do it that way hmm a tip a tip i'll give you another tip if you're using tweet deck i exclude
[02:10:42.480 --> 02:10:48.480]   retweets and the only reason i do that is i feel like that's where you get the viral inflammation
[02:10:48.480 --> 02:10:53.760]   happening like this sudden all these retweets i want to see the originals i have a
[02:10:53.760 --> 02:10:58.320]   different approach to the same problem if somebody retweets something objectionable i
[02:10:58.320 --> 02:11:03.840]   just stopped following the person who retweeted it yeah and so then you get quality retweets yeah
[02:11:03.840 --> 02:11:07.600]   because i i followed like like Matthew Ingram and there's a bunch of other people who are great
[02:11:07.600 --> 02:11:12.560]   retweeters it's like an art form i would want to see those i discover a lot of new twitter users by
[02:11:12.560 --> 02:11:17.920]   other people's retweets but i'm very real answers we need better moderation tools for ourselves yes
[02:11:17.920 --> 02:11:22.640]   yes so that we can make our twitter feeds better twitter wants to maintain the control over
[02:11:22.640 --> 02:11:25.920]   which all that stuff and i think you know you've controlled the people man
[02:11:25.920 --> 02:11:34.320]   that's what i that's what i say as a boomer i want to mention this because he was a very
[02:11:34.320 --> 02:11:38.800]   important person in computer science that i thought i knew everybody and all the names in history
[02:11:38.800 --> 02:11:43.040]   but i had never heard of Fernando Corbato i'd never heard of him you didn't know who your father
[02:11:43.040 --> 02:11:48.480]   was the father of your father of my computer he just passed away at the age of 93 great
[02:11:48.480 --> 02:11:54.160]   you would uh obituary in the New York Times by katie halfner and here's why you want to remember
[02:11:54.160 --> 02:12:00.640]   Fernando Corbato before he came along if you wanted to use a computer you had to print out a
[02:12:00.640 --> 02:12:07.920]   bunch of punch cards a stack carry them carefully without letting them fall spindle fold or mutilate
[02:12:07.920 --> 02:12:12.480]   to the high priest running the computer who would then dane to run your job maybe tomorrow maybe
[02:12:12.480 --> 02:12:17.280]   the next day maybe next week and if heaven for heaven the cards had gotten mixed up or there was
[02:12:17.280 --> 02:12:22.240]   a bug you'd come get them back and you have to start all over in this process again computing
[02:12:22.240 --> 02:12:29.920]   was not interactive in other words Corbato realized that these computers are fast enough
[02:12:29.920 --> 02:12:37.280]   that they could probably do something he called time sharing he invented in the early 60s something
[02:12:37.280 --> 02:12:43.040]   called ctss the compatible time sharing system which allowed multiple users in different locations
[02:12:43.040 --> 02:12:47.520]   to access one big expensive computer at the same time through telephone lines with
[02:12:47.520 --> 02:12:54.560]   teletypes remember that yeah and without that uh bill gates would you know very famously
[02:12:54.560 --> 02:13:01.040]   bill gates mom and other mothers at the lakeside school in seattle had a cake sale to raise the
[02:13:01.040 --> 02:13:06.480]   money to get a time share teletype in a closet at the school he and paul allen learned to use
[02:13:06.480 --> 02:13:11.680]   computers to program computers with that interactive time share device and that would then that all
[02:13:11.680 --> 02:13:21.440]   eventually led to clippy that's he created clippy uh the other thing though he had to invent in order
[02:13:21.440 --> 02:13:26.720]   to make a time sharing system work you have multiple people on the same computer at first
[02:13:26.720 --> 02:13:33.760]   everybody was able to see everybody else's stuff he had to create accounts and passwords hmm so
[02:13:33.760 --> 02:13:41.360]   corby also he gets credit for vetting time sharing he also gets credit and interact true interactive
[02:13:41.360 --> 02:13:47.040]   computing for creating the password and really isn't that what cloud computing is isn't cloud
[02:13:47.040 --> 02:13:52.720]   computing yeah basically time sharing right if you use google stadia you're playing a game on a
[02:13:52.720 --> 02:14:02.080]   computer in the cloud that's right so uh i think huge ctss gave rise to maltics which was
[02:14:02.080 --> 02:14:08.960]   a operating system which gave rise to unix get it unix is a singular version of maltics
[02:14:09.920 --> 02:14:15.760]   uh and in fact uh maltics was the inspiration for lennox because lennox torvalds didn't like
[02:14:15.760 --> 02:14:21.760]   maltics he wanted to make his own version of maltics and created lennox so lennox is what
[02:14:21.760 --> 02:14:28.240]   android etc is based on and still makos right back i'm s is based on unix unix not lennox but
[02:14:28.240 --> 02:14:33.680]   same thing can be traced back to the back to corbato so if next time you log in and give a
[02:14:33.680 --> 02:14:38.720]   password you can thank corby the father of the computer and the interactive computing and the
[02:14:38.720 --> 02:14:46.240]   password um hey everybody thank you so much for being here i really appreciate caroline you're great
[02:14:46.240 --> 02:14:51.040]   you showed amazing fortitude and not chasing that ice cream truck and i admired
[02:14:51.040 --> 02:14:58.400]   caroline haskins you can catch her work at mother board advice she does such good stuff i look
[02:14:58.400 --> 02:15:04.640]   forward to seeing your next public information request look at all the stuff she covers including
[02:15:04.640 --> 02:15:11.360]   rogue cyclists creating a bike lane with toilet plungers new york city baby boy a boy it's a good
[02:15:11.360 --> 02:15:20.400]   city to live in isn't it yeah it's nice yeah it's never boring no nope uh and neither is caroline
[02:15:20.400 --> 02:15:25.520]   thank you caroline for being here we appreciate it she's on twitter caroline hu other score
[02:15:26.960 --> 02:15:33.200]   c_a_r_o_l_i_n_h_a_ underscore that's what i think i think the underscore is the hot it just stops
[02:15:33.200 --> 02:15:39.440]   you're trying to stop it's a glottal stop huh yeah somebody already had haskins i wish i i wish i
[02:15:39.440 --> 02:15:45.040]   had caroline haskins but i like i i just found um yeah it's good i can't even change my twitter
[02:15:45.040 --> 02:15:49.680]   handle even if i wanted to apparently or you lose the verification badge right no you're stuck with
[02:15:49.680 --> 02:15:57.120]   now you can give yourself a clever nickname just change your real name yeah yeah true you have an
[02:15:57.120 --> 02:16:02.880]   underscore in your legal name hot first yeah the first uh the first uh line of hearts very nice
[02:16:02.880 --> 02:16:12.000]   also mike lgan who could be mike a a l g a n on the twitter elgan.com don't forget uh
[02:16:12.000 --> 02:16:17.040]   gastronomad.net and i want to give a plug to kevin your son who is here today yes he's an
[02:16:17.600 --> 02:16:22.240]   silicon valley entrepreneur in the education he's created something called shoe leather no no no no
[02:16:22.240 --> 02:16:28.800]   chatterbox which is we've talked about it before it is so cool to teach kids a little bit more about
[02:16:28.800 --> 02:16:32.640]   see i think one of the things parents are doing which i think is a huge mistake they're teaching
[02:16:32.640 --> 02:16:37.520]   their kids to say please and thank you to their amazon echo i think that's a mistake that's
[02:16:37.520 --> 02:16:42.880]   personifying something that is a machine i think more important the kid learns it's a machine and
[02:16:42.880 --> 02:16:48.880]   here's what the machine does here's how it works here's what an echo is and so you this is a smart
[02:16:48.880 --> 02:16:55.680]   speaker anyone can build and program with google blocks it's really really cool it's called chatter
[02:16:55.680 --> 02:17:00.960]   box now it was on kickstarter you raised a bunch of money on kickstarter in about one week you can't
[02:17:00.960 --> 02:17:07.200]   can't kids are pretty turned on by this that kid just dabbed because he built a chatterbox uh
[02:17:09.280 --> 02:17:15.200]   you can um you can get one but you have to wait a little bit it's going to indy gogo next they've
[02:17:15.200 --> 02:17:19.680]   they've used up kickstarter we were just talking about the privacy elements of these
[02:17:19.680 --> 02:17:26.240]   virtual assistants yeah home this one doesn't listen to you push the button and then no data is
[02:17:26.240 --> 02:17:32.480]   retained very nice just period and can be those skills is it as an echo is it google assistant
[02:17:32.480 --> 02:17:41.760]   or is it your own what is the skills oh it's my craft yeah yeah my craft is a very cool AI yeah
[02:17:41.760 --> 02:17:47.680]   so we essentially created a visual skill builder so kids all the skills are actually run
[02:17:47.680 --> 02:17:53.040]   locally on the device oh so you get to program the skill it's kind of like the apple model you
[02:17:53.040 --> 02:17:58.320]   keep it private and secure through just running it locally no ads no data collection it isn't always
[02:17:58.320 --> 02:18:03.680]   listening it's a chatterbox and you can make your own how many of you shipped have you shipped
[02:18:03.680 --> 02:18:08.160]   him yet i know they're gonna be shipping uh in time for christmas so what's the processors in
[02:18:08.160 --> 02:18:14.640]   our jouino what's in there uh it's actually based on raspberry pie it's a pie yeah nice nice
[02:18:14.640 --> 02:18:18.720]   last right now we're actually looking at uh using for upgrading everybody to the forties
[02:18:18.720 --> 02:18:23.120]   but oh yeah those new raspberry pie forties are awesome awesome
[02:18:24.160 --> 02:18:26.720]   awesome yeah you have to change your power supply
[02:18:26.720 --> 02:18:34.640]   chat hello chatterbox.com let's give you a nice big plug because i think that's
[02:18:34.640 --> 02:18:37.600]   if i'm gonna get one for the kids i think that's really really cool i love it yeah
[02:18:37.600 --> 02:18:42.720]   thank you mike thank you caroline thank you everybody for being here we had a great studio
[02:18:42.720 --> 02:18:47.920]   audience visiting from all over the world you guys were very patient that's all i can ever say
[02:18:47.920 --> 02:18:54.480]   about our studio audiences they have bums of steel your fortitude is impressive it is well
[02:18:54.480 --> 02:18:58.960]   what we do is we invite them here they sit down in the most uncomfortable chairs we can find
[02:18:58.960 --> 02:19:03.920]   and then we make them stay there for two solid hours tim and nina from orlando and rich from san
[02:19:03.920 --> 02:19:10.800]   francisco and uh mik and mik was from doublin etienne is from paris and uh rod and caroline
[02:19:10.800 --> 02:19:15.440]   they're from um you're gonna have to pronounce it for me styla cook cum
[02:19:16.160 --> 02:19:22.640]   stellicum in uh the great state of washington and uh erin and katelyn visiting of course
[02:19:22.640 --> 02:19:27.600]   roberto great to have you visiting from brooklyn no queens say again stan island
[02:19:27.600 --> 02:19:34.720]   he uh he got here on the ferry it goes to it's a long way if you want to be in our studio audience
[02:19:34.720 --> 02:19:38.560]   email tickets at twitt.tv we love having a studio audience it makes us so much more fun
[02:19:38.560 --> 02:19:42.960]   uh especially when they pretend to laugh at my jokes i really appreciate that it's always nice
[02:19:42.960 --> 02:19:46.800]   if you want to watch live you can do that too we have a live video and audio stream available
[02:19:46.800 --> 02:19:51.760]   twitt.tv/live but if you're doing that you know chat because the chat room is a big part of all
[02:19:51.760 --> 02:19:58.880]   of our shows they're always there at irc.twitt.tv it's family friendly and fun irc.twitt.tv
[02:19:58.880 --> 02:20:05.200]   after the fact everything we do is available on demand just go to our website twitt.tv
[02:20:05.200 --> 02:20:10.560]   you can even subscribe in fact that's the best way to get every show we do subscribe that way
[02:20:10.560 --> 02:20:14.320]   the minute the show is available you'll get it automatically on your smart device and don't be
[02:20:14.320 --> 02:20:20.320]   a chicken subscribe to the all twitch shows feed there's a feed for all of the shows i
[02:20:20.320 --> 02:20:26.880]   subscribe to it are you not okay thank you it's if you actually why not then why not you know you
[02:20:26.880 --> 02:20:31.600]   you got a ton of stuff you get it all there that's right that's it sounds like a good idea
[02:20:31.600 --> 02:20:36.400]   thanks everybody uh did i do anything did i do need to do anything else karsten bonnie our
[02:20:36.400 --> 02:20:42.080]   producer our executive producer the man in charge i'm all set i'm done i can go home i can have lunch
[02:20:42.080 --> 02:20:47.360]   all right thanks everybody i'm gonna go home and watch the last episode of game of throm... oh no
[02:20:47.360 --> 02:20:53.280]   that's over friends friends friends thanks everybody we'll see you next time another twitt
[02:20:53.760 --> 02:21:04.640]   music do the twitt all right do the twitt baby do the twitt all right do the twitt
[02:21:04.640 --> 02:21:06.640]   You

