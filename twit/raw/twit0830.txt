;FFMETADATA1
title=Meat Us in Singapore
artist=Leo Laporte, Brianna Wu, Amy Webb
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-07-04
track=830
language=English
genre=Podcast
encoded_by=Uniblab 5.3
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.060]   It's time for Twit this week in Tech.
[00:00:04.060 --> 00:00:09.240]   I really only have to say two words to make you watch a show.
[00:00:09.240 --> 00:00:10.520]   Amy and Brianna.
[00:00:10.520 --> 00:00:13.040]   Amy Webb and Brianna Wu, our guests.
[00:00:13.040 --> 00:00:15.760]   This is a early recording because of the 4th of July,
[00:00:15.760 --> 00:00:17.920]   but we have lots to talk about.
[00:00:17.920 --> 00:00:19.880]   Amazon's getting a new CEO.
[00:00:19.880 --> 00:00:23.320]   We'll take a look back at the career of Jeff Bezos,
[00:00:23.320 --> 00:00:25.960]   a ransomware attack hitting hundreds of companies,
[00:00:25.960 --> 00:00:28.320]   thanks to another supply chain failure,
[00:00:28.320 --> 00:00:31.920]   a Windows 0 day, and a whole lot more.
[00:00:31.920 --> 00:00:34.040]   It's all coming up next on Twit.
[00:00:34.040 --> 00:00:36.560]   [MUSIC PLAYING]
[00:00:36.560 --> 00:00:38.400]   Podcasts you love.
[00:00:38.400 --> 00:00:40.920]   From people you trust.
[00:00:40.920 --> 00:00:42.480]   This is Twit.
[00:00:42.480 --> 00:00:44.640]   [MUSIC PLAYING]
[00:00:44.640 --> 00:00:50.560]   This is Twit this week in Tech.
[00:00:50.560 --> 00:00:55.560]   Episode 830 recorded Saturday, July 3, 2021.
[00:00:55.560 --> 00:00:56.920]   Meet us in Singapore.
[00:00:56.920 --> 00:00:58.880]   [MUSIC PLAYING]
[00:00:58.880 --> 00:01:01.600]   It's time for Twit this week in Tech.
[00:01:01.600 --> 00:01:04.520]   We cover the week's tech news a little bit different today
[00:01:04.520 --> 00:01:07.360]   because Sunday is the 4th of July.
[00:01:07.360 --> 00:01:09.520]   I looked back.
[00:01:09.520 --> 00:01:12.480]   Lisa told me a couple of weeks ago, oh, no show on Sunday.
[00:01:12.480 --> 00:01:13.680]   I said, what do you mean?
[00:01:13.680 --> 00:01:14.640]   She says the 4th of July.
[00:01:14.640 --> 00:01:17.680]   I said, we don't miss a show.
[00:01:17.680 --> 00:01:19.000]   What are you talking about?
[00:01:19.000 --> 00:01:21.240]   And she said, oh, we always miss it on the 4th.
[00:01:21.240 --> 00:01:22.240]   I said, what do you mean?
[00:01:22.240 --> 00:01:23.480]   So I looked back.
[00:01:23.480 --> 00:01:25.360]   There has only, in the 15 years of this show,
[00:01:25.360 --> 00:01:29.440]   there's only been one other time the show was on the 4th of July.
[00:01:29.440 --> 00:01:31.280]   And we did the show on the 3rd.
[00:01:31.280 --> 00:01:33.120]   So I thought, OK, there's a precedent.
[00:01:33.120 --> 00:01:35.600]   So we're doing this today on Saturday the 3rd.
[00:01:35.600 --> 00:01:36.440]   Let's open another thing.
[00:01:36.440 --> 00:01:37.360]   No big news breaks.
[00:01:37.360 --> 00:01:39.840]   And then the next 24 hours.
[00:01:39.840 --> 00:01:41.600]   And of course, I'm thrilled because we
[00:01:41.600 --> 00:01:43.840]   were able to get two wonderful people to share
[00:01:43.840 --> 00:01:45.520]   their Saturday evenings with us.
[00:01:45.520 --> 00:01:47.120]   First Brianna Wu.
[00:01:47.120 --> 00:01:49.720]   Good to see you from the executive director
[00:01:49.720 --> 00:01:51.720]   of the Rebellion Pack.
[00:01:51.720 --> 00:01:52.840]   Brianna Wu on Twitter.
[00:01:52.840 --> 00:01:53.560]   Hi, Brianna.
[00:01:53.560 --> 00:01:54.760]   So happy to be here.
[00:01:54.760 --> 00:01:55.600]   Thanks for having me.
[00:01:55.600 --> 00:01:56.400]   You look so good.
[00:01:56.400 --> 00:01:57.040]   What did you do?
[00:01:57.040 --> 00:01:58.040]   Are you doing noon?
[00:01:58.040 --> 00:01:59.240]   Are you doing anything?
[00:01:59.240 --> 00:02:00.280]   I am.
[00:02:00.280 --> 00:02:03.880]   So last year, I had reconstructive knee surgery.
[00:02:03.880 --> 00:02:07.320]   They actually drilled six holes through the Nutella.
[00:02:07.320 --> 00:02:09.160]   I could not walk for three months.
[00:02:09.160 --> 00:02:12.920]   And I just put on a ton of weight after that.
[00:02:12.920 --> 00:02:16.120]   And I started-- I did noon to start losing it.
[00:02:16.120 --> 00:02:18.520]   And as long as I figured it out,
[00:02:18.520 --> 00:02:20.640]   it's like just write down what you eat.
[00:02:20.640 --> 00:02:22.440]   Keep the calories lower.
[00:02:22.440 --> 00:02:24.720]   And I just decided to keep going until the week.
[00:02:24.720 --> 00:02:26.480]   I was back to my high school weight.
[00:02:26.480 --> 00:02:27.200]   Yeah, thank you.
[00:02:27.200 --> 00:02:28.280]   High school weight.
[00:02:28.280 --> 00:02:29.080]   Wow.
[00:02:29.080 --> 00:02:30.520]   That's pretty impressive.
[00:02:30.520 --> 00:02:31.520]   Lisa's done the same thing.
[00:02:31.520 --> 00:02:32.280]   She's on noon.
[00:02:32.280 --> 00:02:33.000]   We're both on noon.
[00:02:33.000 --> 00:02:35.320]   She's done a little better than I have.
[00:02:35.320 --> 00:02:37.760]   Do you find it's a little like the lessons?
[00:02:37.760 --> 00:02:40.000]   I find the longer you do the lessons, they get a little--
[00:02:40.000 --> 00:02:41.040]   I stopped.
[00:02:41.040 --> 00:02:42.120]   I stopped.
[00:02:42.120 --> 00:02:42.840]   Yeah.
[00:02:42.840 --> 00:02:44.360]   Because they're cutesy.
[00:02:44.360 --> 00:02:47.480]   I feel like they're being written for young women
[00:02:47.480 --> 00:02:48.080]   or something.
[00:02:48.080 --> 00:02:48.640]   I agree.
[00:02:48.640 --> 00:02:50.560]   They're not aimed at--
[00:02:50.560 --> 00:02:52.800]   they should have a setting that says,
[00:02:52.800 --> 00:02:53.640]   leave out the cute.
[00:02:53.640 --> 00:02:54.440]   Just give me the facts.
[00:02:54.440 --> 00:02:56.080]   'Cause the information is great.
[00:02:56.080 --> 00:02:59.760]   It's a CBT, it's a cognitive behavioral therapy.
[00:02:59.760 --> 00:03:01.880]   They help you understand why you overeat
[00:03:01.880 --> 00:03:04.140]   and how you overeating, and become aware,
[00:03:04.140 --> 00:03:05.200]   mindful of the whole thing.
[00:03:05.200 --> 00:03:09.000]   And it works, but it's a little cutesy, I agree.
[00:03:09.000 --> 00:03:10.520]   - That was very much my experience.
[00:03:10.520 --> 00:03:12.840]   'Cause it's like even, like when you've been doing it
[00:03:12.840 --> 00:03:15.160]   three months, the lessons start being like,
[00:03:15.160 --> 00:03:16.500]   "And you can do it.
[00:03:16.500 --> 00:03:18.280]   We believe in you here last night."
[00:03:18.280 --> 00:03:19.440]   - Yeah, and I'm doing it, it's fine.
[00:03:19.440 --> 00:03:20.700]   - I'm here, I'm here, I'm here.
[00:03:20.700 --> 00:03:21.520]   - I'm here, I'm here.
[00:03:21.520 --> 00:03:22.360]   Tell me about nutrition.
[00:03:22.360 --> 00:03:23.200]   - No, I stopped doing the lesson
[00:03:23.200 --> 00:03:24.760]   and I just continue to record.
[00:03:24.760 --> 00:03:27.320]   And it works, it really is effective.
[00:03:27.320 --> 00:03:28.440]   - Yeah, that's amazing.
[00:03:28.440 --> 00:03:29.800]   - You look great, I'm so glad you just worked with it.
[00:03:29.800 --> 00:03:32.040]   - Thank you, you too, too, Leo.
[00:03:32.040 --> 00:03:34.000]   - You didn't have to say that, it's okay.
[00:03:34.000 --> 00:03:36.760]   (laughing)
[00:03:36.760 --> 00:03:38.080]   That was one of those.
[00:03:38.080 --> 00:03:40.680]   After the fact, you look great too.
[00:03:40.680 --> 00:03:42.360]   Yeah, no, that's a good thing.
[00:03:42.360 --> 00:03:44.040]   (laughing)
[00:03:44.040 --> 00:03:46.080]   Hey, it's great also to welcome Amy Webb.
[00:03:46.080 --> 00:03:50.320]   We love Amy Webb and she is celebrating tonight,
[00:03:50.320 --> 00:03:53.880]   CEO of Future Today Institute at Amy Webb.
[00:03:53.880 --> 00:03:55.200]   And she's going on her first vacation,
[00:03:55.200 --> 00:03:57.080]   what's the last vacation you went on?
[00:03:57.080 --> 00:03:59.440]   - First of all, I believe in vacation.
[00:03:59.440 --> 00:04:03.040]   So what I'm about to say does not mean that I don't think
[00:04:03.040 --> 00:04:04.040]   vacations are important.
[00:04:04.040 --> 00:04:05.680]   - This is not the future, this is not the future
[00:04:05.680 --> 00:04:06.840]   we'll be living in.
[00:04:06.840 --> 00:04:11.240]   - No, no, no, just between books and family and work.
[00:04:11.240 --> 00:04:12.640]   I think this is the first vacation
[00:04:12.640 --> 00:04:14.760]   where I don't have to give a speech.
[00:04:14.760 --> 00:04:17.040]   Arjansa, I don't have the privilege of giving a speech
[00:04:17.040 --> 00:04:18.800]   or meeting with a client.
[00:04:18.800 --> 00:04:20.200]   - Yes.
[00:04:20.200 --> 00:04:21.680]   - I think in about 10 years.
[00:04:21.680 --> 00:04:22.680]   (sighing)
[00:04:22.680 --> 00:04:23.880]   - Oh my god, wow.
[00:04:23.880 --> 00:04:27.080]   - See, I bring out, and because of COVID,
[00:04:27.080 --> 00:04:29.280]   we haven't gone on vacations.
[00:04:29.280 --> 00:04:32.600]   - I mean, we take little, like, you know,
[00:04:32.600 --> 00:04:35.520]   I get pre-COVID, I was traveling all over the world.
[00:04:35.520 --> 00:04:37.360]   So we would sort of, you know, we didn't really do
[00:04:37.360 --> 00:04:40.520]   a true vacation 'cause we were always in a new city
[00:04:40.520 --> 00:04:44.480]   or a new country and we just tack on a few days.
[00:04:44.480 --> 00:04:47.560]   So this is the first time that I'm like gonna be just
[00:04:47.560 --> 00:04:49.000]   no work for her.
[00:04:49.000 --> 00:04:52.600]   - You're not bringing any digital equipment with you.
[00:04:52.600 --> 00:04:55.440]   - You know, I was gonna bring a Kindle
[00:04:55.440 --> 00:04:57.880]   and I decided I really hate reading on a Kindle.
[00:04:57.880 --> 00:05:00.080]   (laughing)
[00:05:00.080 --> 00:05:05.080]   So no, I've got an iPad 'cause I'm a big rush
[00:05:05.080 --> 00:05:09.360]   and prog rock fan and there's a concert I've decided
[00:05:09.360 --> 00:05:11.520]   I wanted to watch at some point on the plane.
[00:05:11.520 --> 00:05:13.920]   I know, whatever, don't make fun.
[00:05:13.920 --> 00:05:15.520]   - Oh, I love rush.
[00:05:15.520 --> 00:05:19.600]   In fact, I use them as the poster child
[00:05:19.600 --> 00:05:22.920]   for why not all Apple's spatial audio mixes are good
[00:05:22.920 --> 00:05:25.800]   and why Apple's all the Bluetooth headphones are terrible.
[00:05:25.800 --> 00:05:27.920]   Tom Sawyer, which is an amazing song
[00:05:27.920 --> 00:05:29.480]   given there's only three people playing it.
[00:05:29.480 --> 00:05:31.120]   - You need the beginning, you need that beginning,
[00:05:31.120 --> 00:05:33.560]   you need the sound to be right, totally.
[00:05:33.560 --> 00:05:36.960]   - And they offer it as one of their, you know,
[00:05:36.960 --> 00:05:39.760]   poster children for their new spatial mix
[00:05:39.760 --> 00:05:42.400]   and it sounds horrible on the AirPod Pro
[00:05:42.400 --> 00:05:44.080]   and the AirPod Pro Max,
[00:05:44.080 --> 00:05:47.160]   but if you get good headphones, it sounds fine.
[00:05:47.160 --> 00:05:49.240]   - I've got pair of Bose headphones.
[00:05:49.240 --> 00:05:51.080]   - There you go, it'll sound better.
[00:05:51.080 --> 00:05:53.440]   - Bluetooth, I think Bluetooth is not good for music
[00:05:53.440 --> 00:05:54.280]   to be honest with you.
[00:05:54.280 --> 00:05:55.200]   - No, it's not.
[00:05:55.200 --> 00:05:58.720]   Did you know Sidebar, there's a new documentary
[00:05:58.720 --> 00:06:01.600]   that Dave Grohl has out on one of the networks,
[00:06:01.600 --> 00:06:03.360]   maybe Netflix or maybe Amazon,
[00:06:03.360 --> 00:06:04.840]   it's called Cradle the Stage.
[00:06:04.840 --> 00:06:06.360]   Have you seen this yet?
[00:06:06.360 --> 00:06:09.880]   So Getty, so basically it's like Dave Grohl's mom
[00:06:09.880 --> 00:06:11.440]   had a book come out, whatever.
[00:06:11.440 --> 00:06:15.600]   So it's not the greatest show, there's a lot of problems,
[00:06:15.600 --> 00:06:18.400]   but he does interview musicians and their moms.
[00:06:18.400 --> 00:06:21.120]   So there's an episode with Getty Lee.
[00:06:21.120 --> 00:06:24.880]   And he's, you hear, I mean, she tells this story
[00:06:24.880 --> 00:06:27.080]   about surviving the Holocaust and like,
[00:06:27.080 --> 00:06:28.240]   the whole family went through
[00:06:28.240 --> 00:06:30.160]   and then there's a really weird transition
[00:06:30.160 --> 00:06:32.840]   to Dave Grohl's daughter wanting to become a musician,
[00:06:32.840 --> 00:06:34.680]   but it's pretty amazing.
[00:06:34.680 --> 00:06:37.320]   And Getty is like super, super interesting.
[00:06:37.320 --> 00:06:39.600]   There's also an episode, highly recommended
[00:06:39.600 --> 00:06:42.040]   featuring Tom Morello and his mother.
[00:06:42.040 --> 00:06:45.080]   Tom Morello's mom is a badass.
[00:06:45.080 --> 00:06:47.600]   Oh my God, that woman is so cool.
[00:06:47.600 --> 00:06:51.000]   So totally off topic, but highly recommended
[00:06:51.000 --> 00:06:53.760]   if you're looking for something to do on July 3rd,
[00:06:53.760 --> 00:06:56.160]   if you're listening today, which is a holiday,
[00:06:56.160 --> 00:06:58.280]   it's National Fried Clam Day.
[00:06:58.280 --> 00:06:59.120]   So there you go.
[00:06:59.120 --> 00:07:00.360]   (laughing)
[00:07:00.360 --> 00:07:04.320]   - My favorite, there is a wonderful rush documentary
[00:07:04.320 --> 00:07:05.760]   on Netflix if you haven't seen it,
[00:07:05.760 --> 00:07:07.680]   Time Stand Still that I really enjoyed.
[00:07:07.680 --> 00:07:08.520]   - Oh really?
[00:07:08.520 --> 00:07:09.360]   I'll download it.
[00:07:09.360 --> 00:07:12.240]   - Yeah, that you should watch on the plane.
[00:07:12.240 --> 00:07:13.080]   - Okay.
[00:07:13.080 --> 00:07:14.720]   - Yeah, that's really, really good.
[00:07:14.720 --> 00:07:16.280]   Wow, we haven't done any tech news
[00:07:16.280 --> 00:07:17.880]   and then Zardy were six minutes in.
[00:07:17.880 --> 00:07:19.440]   (laughing)
[00:07:19.440 --> 00:07:21.600]   - This is my kind show.
[00:07:21.600 --> 00:07:22.440]   I love it.
[00:07:22.440 --> 00:07:25.320]   Well, we're gonna because one of the stories,
[00:07:25.320 --> 00:07:27.640]   probably our biggest story of the week is that
[00:07:27.640 --> 00:07:30.520]   this is the week that Jeff Bezos will step down
[00:07:30.520 --> 00:07:33.480]   as CEO of Amazon.
[00:07:33.480 --> 00:07:37.640]   And Andy Jassy will be taking over.
[00:07:37.640 --> 00:07:41.240]   Kind of, he's been a long time Amazon guy.
[00:07:41.240 --> 00:07:43.400]   But the question in a lot of people's minds is,
[00:07:43.400 --> 00:07:47.400]   can Andy gonna bring to the table what Jeff Bezos,
[00:07:47.400 --> 00:07:50.880]   unquestionably, which as Jeff Bezos brought,
[00:07:50.880 --> 00:07:52.880]   the guy who wrote the Everything Store
[00:07:52.880 --> 00:07:54.400]   and has a new book about Jeff Bezos,
[00:07:54.400 --> 00:07:57.360]   did an opinion piece in today's New York Times,
[00:07:57.360 --> 00:08:00.120]   he calls it the Jeff Bezos Paradox,
[00:08:00.120 --> 00:08:02.080]   we're talking about Brad Stone.
[00:08:02.080 --> 00:08:06.120]   And he says, let me see if I can find the,
[00:08:06.960 --> 00:08:09.440]   he says, I've written two books,
[00:08:09.440 --> 00:08:10.760]   including the one you just read,
[00:08:10.760 --> 00:08:11.720]   which I'm gonna ask you about,
[00:08:11.720 --> 00:08:12.960]   chronicling Amazon's history.
[00:08:12.960 --> 00:08:17.120]   I've come to view both the company's never ending expansion
[00:08:17.120 --> 00:08:19.520]   and declining reputation as byproducts
[00:08:19.520 --> 00:08:21.160]   of Mr. Bezos' personality.
[00:08:21.160 --> 00:08:25.200]   His towering intellect along with a notable deficit
[00:08:25.200 --> 00:08:28.800]   of empathy and fear of stasis.
[00:08:28.800 --> 00:08:31.280]   So those are the three kind of, he says,
[00:08:31.280 --> 00:08:34.440]   and this is a guy who studied Bezos more than almost anybody,
[00:08:34.440 --> 00:08:36.720]   his three kind of pillars of his personality
[00:08:36.720 --> 00:08:38.560]   and of the company as a result.
[00:08:38.560 --> 00:08:42.000]   Brilliant guy, nobody questions Amazon's strategy
[00:08:42.000 --> 00:08:44.960]   has been perfect, almost perfect,
[00:08:44.960 --> 00:08:46.760]   except for the fire phone.
[00:08:46.760 --> 00:08:51.600]   But also, it's the most, becoming the most successful company,
[00:08:51.600 --> 00:08:54.240]   one of the most successful companies in the history.
[00:08:54.240 --> 00:08:56.640]   It's also getting a lot of heat for its
[00:08:56.640 --> 00:08:58.480]   poor treatment of its employees.
[00:08:58.480 --> 00:09:02.720]   That's, he says, because of Bezos' own lack of empathy
[00:09:02.720 --> 00:09:05.080]   and of maybe perhaps moving too fast
[00:09:05.080 --> 00:09:06.440]   or trying too many things,
[00:09:06.440 --> 00:09:10.160]   and that's Jeff's fear of stasis.
[00:09:10.160 --> 00:09:13.600]   He wants it always day one as his kind of his saying.
[00:09:13.600 --> 00:09:15.560]   He never wants it to be the second day of a company.
[00:09:15.560 --> 00:09:17.800]   It's always the first day of the company.
[00:09:17.800 --> 00:09:20.840]   He says, day two is stasis followed by irrelevance,
[00:09:20.840 --> 00:09:25.480]   followed by excruciating painful decline followed by death,
[00:09:25.480 --> 00:09:28.560]   which seems a little, that's Jeff talking, not Brad.
[00:09:28.560 --> 00:09:29.880]   That's a little extreme.
[00:09:32.200 --> 00:09:33.600]   What do you think?
[00:09:33.600 --> 00:09:35.800]   Let me start with you, Brianna,
[00:09:35.800 --> 00:09:37.480]   and then because Amy's read the book
[00:09:37.480 --> 00:09:39.680]   that we have yet to read, which is The New Amazon.
[00:09:39.680 --> 00:09:43.280]   Do you think, what do you think of Jeff Bezos
[00:09:43.280 --> 00:09:45.640]   as the head of Amazon?
[00:09:45.640 --> 00:09:48.760]   - Yeah, it's so hard because the political part of me
[00:09:48.760 --> 00:09:53.640]   is kind of appalled by a lot of the way he treats workers, right?
[00:09:53.640 --> 00:09:55.720]   I saw this firsthand running for Congress.
[00:09:55.720 --> 00:10:00.680]   The tech part of me is really in admiration
[00:10:00.680 --> 00:10:03.560]   of the way he keeps pushing the boundaries
[00:10:03.560 --> 00:10:05.240]   on what his company can be.
[00:10:05.240 --> 00:10:07.280]   Look at Amazon Prime Video.
[00:10:07.280 --> 00:10:09.440]   When that came out, I thought that was stupid.
[00:10:09.440 --> 00:10:11.640]   I would bet you probably did too.
[00:10:11.640 --> 00:10:13.520]   And now all these years later,
[00:10:13.520 --> 00:10:16.840]   it's like Apple is doing this.
[00:10:16.840 --> 00:10:19.440]   All these different things are doing a video service.
[00:10:19.440 --> 00:10:22.480]   Look at his plays in the video game industry.
[00:10:22.480 --> 00:10:25.280]   Every single game Amazon has put out
[00:10:25.280 --> 00:10:28.040]   has been a financial disaster.
[00:10:28.040 --> 00:10:29.960]   They've dumped tons of money into it.
[00:10:29.960 --> 00:10:34.960]   It's gone nowhere, but that refusal to just sit still
[00:10:34.960 --> 00:10:38.400]   and stackmate, I really, really respect that.
[00:10:38.400 --> 00:10:41.920]   So I think he's very much a man of contradictions.
[00:10:41.920 --> 00:10:44.320]   And I think that empathy, quote,
[00:10:44.320 --> 00:10:46.800]   it really comes through in looking
[00:10:46.800 --> 00:10:47.960]   at the history of the company.
[00:10:47.960 --> 00:10:49.440]   - Yeah.
[00:10:49.440 --> 00:10:52.600]   It's gonna be, they'll be big shoes to fill though, right?
[00:10:52.600 --> 00:10:55.120]   One of the things that's kind of interesting
[00:10:55.120 --> 00:10:59.000]   as Jeff Bezos steps away from Amazon,
[00:10:59.000 --> 00:11:04.000]   Thursday, the company changed its list of corporate values
[00:11:04.000 --> 00:11:09.600]   and added some things that maybe Jeff Bezos
[00:11:09.600 --> 00:11:11.680]   would not have wanted to add.
[00:11:11.680 --> 00:11:13.040]   The new leadership principles,
[00:11:13.040 --> 00:11:16.360]   this is from an article in Bloomberg by Matt Day
[00:11:16.360 --> 00:11:18.960]   a couple of days ago, the new leadership principles,
[00:11:18.960 --> 00:11:20.720]   strive, these are two new ones,
[00:11:20.720 --> 00:11:23.400]   strive to be the earth's best employer
[00:11:23.400 --> 00:11:28.760]   and success and scale bring broad responsibility.
[00:11:28.760 --> 00:11:30.600]   These two new principles require employees
[00:11:30.600 --> 00:11:34.400]   to take into account the wellbeing of their coworkers
[00:11:34.400 --> 00:11:37.120]   and society beyond the company's walls
[00:11:37.120 --> 00:11:38.560]   when making decisions,
[00:11:38.560 --> 00:11:42.400]   which is I think kind of interesting
[00:11:42.400 --> 00:11:45.640]   as Jeff Bezos steps down, suddenly they get some empathy.
[00:11:45.640 --> 00:11:48.200]   So you've read Amazon and Bani,
[00:11:48.200 --> 00:11:50.640]   Amy, you were saying it's really good.
[00:11:50.640 --> 00:11:52.800]   - So I have many, many thoughts.
[00:11:52.800 --> 00:11:54.880]   First of all, I've read excerpts of it,
[00:11:54.880 --> 00:11:56.400]   and the excerpts, so Brad said--
[00:11:56.400 --> 00:11:58.520]   - Well, you're taking it on vacation with you three.
[00:11:58.520 --> 00:12:01.440]   - So Brad is such a good writer, he's phenomenal.
[00:12:01.440 --> 00:12:03.920]   - His first book, The Everything Store, was read,
[00:12:03.920 --> 00:12:07.840]   by the way, Bezos and his wife hated it,
[00:12:07.840 --> 00:12:09.400]   his ex-wife hated it.
[00:12:09.400 --> 00:12:12.640]   In fact, Mackenzie famously put a very nasty review
[00:12:12.640 --> 00:12:15.800]   on Amazon's site of the Everything Store,
[00:12:15.800 --> 00:12:20.520]   but really the nastiness came down to, it was too accurate.
[00:12:20.520 --> 00:12:23.200]   - Yeah, Brad, listen, Brad's a really good reporter.
[00:12:23.200 --> 00:12:25.360]   He's a very good character-driven,
[00:12:25.360 --> 00:12:27.400]   he's a very, very good writer.
[00:12:27.400 --> 00:12:31.400]   I read excerpts of the book, and I couldn't put it down,
[00:12:31.400 --> 00:12:34.960]   so I decided to, this is my treat for myself, for vacation.
[00:12:34.960 --> 00:12:35.800]   But-- - Good show.
[00:12:35.800 --> 00:12:37.160]   - Let me say just a couple of things.
[00:12:37.160 --> 00:12:42.160]   So going back to what you said initially
[00:12:42.160 --> 00:12:44.240]   and what Brad says in the op-ed,
[00:12:44.240 --> 00:12:48.600]   that Bezos is kind of the intersecting vector
[00:12:48.600 --> 00:12:52.640]   of intellect, a lack of empathy, and fear of stasis is Bezos.
[00:12:52.640 --> 00:12:57.640]   But I think you could say that of many, many, many CEOs.
[00:12:57.640 --> 00:13:00.440]   So what I do for a living, I'm a futurist,
[00:13:00.440 --> 00:13:01.760]   but I'm a management consultant,
[00:13:01.760 --> 00:13:05.400]   and my company advises the CEOs and the C-suites
[00:13:05.400 --> 00:13:07.760]   of enormous corporations, all over the world,
[00:13:07.760 --> 00:13:10.600]   and governments and our elected people.
[00:13:10.600 --> 00:13:14.040]   I would say that those three criteria
[00:13:14.040 --> 00:13:18.560]   also describe lots of CEOs that I work with.
[00:13:18.560 --> 00:13:20.920]   And a lot of them fear death.
[00:13:20.920 --> 00:13:23.400]   A lot of them feel like they're running out of time.
[00:13:23.400 --> 00:13:24.440]   - Oh, interesting.
[00:13:24.440 --> 00:13:27.280]   Do you think that's a trait that makes you a good CEO?
[00:13:27.280 --> 00:13:31.200]   - You know, I would, I don't know that it's mandatory,
[00:13:31.200 --> 00:13:33.800]   but I would say that I do see this
[00:13:33.800 --> 00:13:37.240]   among lots of the executives that we work with.
[00:13:37.240 --> 00:13:40.320]   There's also another, but what separates Jeff
[00:13:40.320 --> 00:13:43.400]   from everybody else, I think is the drive,
[00:13:43.400 --> 00:13:48.160]   and he has, he invests himself in thinking
[00:13:48.160 --> 00:13:50.280]   about the longer-term future.
[00:13:50.280 --> 00:13:55.280]   And that is difficult to do because it takes courage
[00:13:55.280 --> 00:13:58.000]   to confront the future, to imagine yourself
[00:13:58.000 --> 00:14:00.240]   and your organization in the biggest possible terms
[00:14:00.240 --> 00:14:01.080]   in the future.
[00:14:01.080 --> 00:14:03.400]   And you know, the new principles,
[00:14:03.400 --> 00:14:04.360]   I think are pretty interesting,
[00:14:04.360 --> 00:14:06.160]   but for totally different reasons.
[00:14:06.160 --> 00:14:08.160]   The word earth is in there.
[00:14:08.160 --> 00:14:09.920]   Did you notice that, Leo?
[00:14:09.920 --> 00:14:11.800]   It wasn't in previous ones,
[00:14:11.800 --> 00:14:13.440]   but they're now talking about the planet.
[00:14:13.440 --> 00:14:16.000]   - First, that's about climate change, you think?
[00:14:16.000 --> 00:14:18.680]   Or-- - No, I think that's about,
[00:14:18.680 --> 00:14:21.920]   I think that's an overt announcement
[00:14:21.920 --> 00:14:26.040]   of global expansion and pretty soon intergalactic,
[00:14:26.040 --> 00:14:27.160]   maybe not intergalactic,
[00:14:27.160 --> 00:14:30.720]   but an expansion off planet is what I think is happening.
[00:14:30.720 --> 00:14:32.520]   And one, just last quick thing.
[00:14:32.520 --> 00:14:34.720]   - Well, Jeff, because he runs Blue Origin,
[00:14:34.720 --> 00:14:37.400]   is very much about space exploration, right?
[00:14:37.400 --> 00:14:39.000]   - Right, and the space economy.
[00:14:39.000 --> 00:14:40.720]   There's, you know, so we,
[00:14:40.720 --> 00:14:43.680]   FTI produces an annual tech trends report,
[00:14:43.680 --> 00:14:46.920]   and we did in 2021, this year's had,
[00:14:46.920 --> 00:14:48.760]   it was 12 volumes.
[00:14:48.760 --> 00:14:51.120]   Anyways, and the, this is the volume
[00:14:51.120 --> 00:14:53.440]   that just deals with technology trends
[00:14:53.440 --> 00:14:57.000]   having to do with health, medical, and wearables.
[00:14:57.000 --> 00:15:00.040]   Amazon is doing so much in this space
[00:15:00.040 --> 00:15:02.320]   that we had to write an entire section
[00:15:02.320 --> 00:15:05.400]   about all of the different things that Amazon's doing.
[00:15:05.400 --> 00:15:08.280]   And my point is, I think what is,
[00:15:08.280 --> 00:15:12.720]   I think what makes the company so interesting
[00:15:12.720 --> 00:15:16.280]   and Jeff's such a profound leader,
[00:15:16.280 --> 00:15:19.560]   is that they are completely disrupting
[00:15:19.560 --> 00:15:20.920]   so many different industries,
[00:15:20.920 --> 00:15:25.920]   from pharmaceuticals to logistics to, you know, you name it.
[00:15:25.920 --> 00:15:29.960]   And I, and another way, listen,
[00:15:29.960 --> 00:15:32.680]   some of the human rights,
[00:15:32.680 --> 00:15:35.360]   the employee complaints, you know,
[00:15:35.360 --> 00:15:39.160]   are certainly important and should be looked at.
[00:15:39.160 --> 00:15:42.920]   I think we also need to acknowledge
[00:15:42.920 --> 00:15:46.000]   that humanity is in this period of transition.
[00:15:46.000 --> 00:15:48.840]   And also transmission, but transition,
[00:15:48.840 --> 00:15:52.960]   we're moving from people doing things to people
[00:15:52.960 --> 00:15:55.480]   and machines doing things together,
[00:15:55.480 --> 00:15:59.840]   to machines and cognitive systems making decisions
[00:15:59.840 --> 00:16:01.640]   and telling the humans what to do.
[00:16:01.640 --> 00:16:05.080]   And pretty soon, the humans won't be in the loop anymore.
[00:16:05.080 --> 00:16:09.480]   And collaborative robots will do a lot of that manual work for us.
[00:16:09.480 --> 00:16:10.280]   So.
[00:16:10.280 --> 00:16:14.320]   And we may look back and say Amazon started that all with Alexa.
[00:16:15.600 --> 00:16:17.360]   I mean, that's certainly part of it.
[00:16:17.360 --> 00:16:21.040]   But the other piece of this is all of the warehouse technology,
[00:16:21.040 --> 00:16:22.720]   you know, Amazon's AWS.
[00:16:22.720 --> 00:16:24.720]   - Is why they automated the warehouses?
[00:16:24.720 --> 00:16:30.200]   - Oh yeah, yes, but in, so yes, but what is automated?
[00:16:30.200 --> 00:16:34.280]   It's the decisions that otherwise would have been made by people.
[00:16:34.280 --> 00:16:36.440]   - The humans are just labor.
[00:16:36.440 --> 00:16:37.680]   - The humans are the robots.
[00:16:37.680 --> 00:16:38.520]   That's right. - Other robots.
[00:16:38.520 --> 00:16:39.560]   - That's right.
[00:16:39.560 --> 00:16:41.760]   - Wow, that's a weird flip-flop.
[00:16:41.760 --> 00:16:46.720]   By the way, I have all 300 volumes of this.
[00:16:46.720 --> 00:16:48.480]   I highly recommend the digital download,
[00:16:48.480 --> 00:16:51.520]   which is free to everybody at the Future Today Institute website
[00:16:51.520 --> 00:16:53.280]   because I can barely lift this.
[00:16:53.280 --> 00:16:54.480]   - Yeah, it's pretty big.
[00:16:54.480 --> 00:16:55.640]   - I just have to run and get it.
[00:16:55.640 --> 00:16:58.240]   And he kind of hands it to me with his pinky.
[00:16:58.240 --> 00:16:59.640]   - Yeah, look at that.
[00:16:59.640 --> 00:17:00.640]   - That went.
[00:17:00.640 --> 00:17:03.040]   But this is actually a great read.
[00:17:03.040 --> 00:17:04.160]   Really fantastic.
[00:17:04.160 --> 00:17:06.160]   So well done on that.
[00:17:06.160 --> 00:17:08.400]   And it was nice of you to divide it into chunks
[00:17:08.400 --> 00:17:11.240]   because I don't think I could lift the whole thing.
[00:17:11.240 --> 00:17:13.000]   So let me read you.
[00:17:13.000 --> 00:17:16.520]   This is one of the new principles
[00:17:16.520 --> 00:17:21.760]   that Amazon has now adopted as of Thursday.
[00:17:21.760 --> 00:17:23.800]   Quote, "We are big.
[00:17:23.800 --> 00:17:28.680]   "We impact the world and we are far from perfect.
[00:17:28.680 --> 00:17:30.360]   "We must be humble and thoughtful
[00:17:30.360 --> 00:17:33.680]   "about even the secondary effects of our actions.
[00:17:33.680 --> 00:17:37.520]   "Our local communities, planet, planet,
[00:17:37.520 --> 00:17:41.080]   "and future generations need us to be better every day."
[00:17:41.080 --> 00:17:42.120]   That's actually lofty.
[00:17:42.120 --> 00:17:43.240]   That's a great goal.
[00:17:43.240 --> 00:17:46.280]   And it's an acknowledgement of how impactful Amazon is,
[00:17:46.280 --> 00:17:49.680]   not just in the US economy, but on the global economy.
[00:17:49.680 --> 00:17:51.760]   - I guess I would find that more credible
[00:17:51.760 --> 00:17:54.720]   if they hadn't just invested a ton of resources
[00:17:54.720 --> 00:17:56.840]   to squash a unionizing effort
[00:17:56.840 --> 00:18:00.760]   in a way that I think a lot of other people
[00:18:00.760 --> 00:18:03.640]   found to be really, really concerning.
[00:18:03.640 --> 00:18:06.640]   You know, I was thinking about what you were saying, Amy.
[00:18:06.640 --> 00:18:08.480]   And I think this is so interesting.
[00:18:08.480 --> 00:18:10.760]   And I don't mean to get political here,
[00:18:10.760 --> 00:18:12.880]   but in the last few years,
[00:18:12.880 --> 00:18:16.440]   I think a lot of us have thought about the trait of narcissism
[00:18:16.440 --> 00:18:18.800]   and a lack of empathy in our leaders.
[00:18:18.800 --> 00:18:20.960]   And I think we've seen the effects of that
[00:18:20.960 --> 00:18:23.280]   in high government positions.
[00:18:23.280 --> 00:18:26.160]   If basis kind of has that as well,
[00:18:26.160 --> 00:18:29.880]   and you're telling me that's something that is a common occurrence,
[00:18:29.880 --> 00:18:31.240]   that kind of lack of empathy
[00:18:31.240 --> 00:18:33.840]   in the highest levels of leadership,
[00:18:33.840 --> 00:18:36.120]   that's, it makes sense to me.
[00:18:36.120 --> 00:18:38.520]   Like imagine everything you could get done
[00:18:38.520 --> 00:18:41.120]   if you didn't have to care about people
[00:18:41.120 --> 00:18:43.320]   or the effects in your wake.
[00:18:43.320 --> 00:18:45.960]   If that's something as kind of a congenital
[00:18:45.960 --> 00:18:47.720]   to these higher level positions,
[00:18:47.720 --> 00:18:52.160]   I mean, it explains a lot, but I think it also,
[00:18:52.160 --> 00:18:54.160]   it makes me pessimistic that, you know,
[00:18:54.160 --> 00:18:57.400]   these lofty goals that they're going to put in a marketing slope
[00:18:57.400 --> 00:19:00.360]   and might result in not squashing unions.
[00:19:00.360 --> 00:19:03.960]   - Is it, Amy, the case that companies founders
[00:19:03.960 --> 00:19:05.760]   tend to be more like that,
[00:19:05.760 --> 00:19:08.280]   and that at some point in the natural evolution
[00:19:08.280 --> 00:19:12.000]   of a business, the founders are replaced by people
[00:19:12.000 --> 00:19:15.080]   who are a little bit, I don't know, more sedate.
[00:19:15.080 --> 00:19:20.400]   - So I, just back on what Brianna said,
[00:19:20.400 --> 00:19:24.000]   so I would say narcissism and lack of empathy
[00:19:24.000 --> 00:19:28.280]   are adjacently or fagonally related maybe,
[00:19:28.280 --> 00:19:30.240]   but not necessarily the same thing.
[00:19:30.240 --> 00:19:35.200]   You know, if you stop and think about
[00:19:35.200 --> 00:19:37.080]   and sort of reverse engineer what it takes
[00:19:37.080 --> 00:19:39.760]   to build a company like Amazon or Tesla
[00:19:39.760 --> 00:19:43.040]   or, you know, Apple, pick any company you want,
[00:19:43.040 --> 00:19:44.560]   that's, you know, Facebook,
[00:19:44.560 --> 00:19:51.120]   you know, you have to be willing to make great sacrifices.
[00:19:51.120 --> 00:19:56.320]   Whether that's in your family or the relationships
[00:19:56.320 --> 00:19:58.800]   that you might have, and if you reverse engineer,
[00:19:58.800 --> 00:20:02.200]   what would it take to make that kind of a sacrifice, right?
[00:20:02.200 --> 00:20:06.760]   You would have to feel a little less than other people.
[00:20:07.760 --> 00:20:11.400]   And so I think probably some of this must be
[00:20:11.400 --> 00:20:14.040]   qualities that you're born with.
[00:20:14.040 --> 00:20:17.760]   I think a lack of empathy would be
[00:20:17.760 --> 00:20:20.120]   probably difficult to cultivate,
[00:20:20.120 --> 00:20:22.600]   especially if the main goal of that was to,
[00:20:22.600 --> 00:20:24.200]   you know, have an amazing company.
[00:20:24.200 --> 00:20:26.200]   - Isn't that sociopathy?
[00:20:26.200 --> 00:20:30.240]   Ultimately, at its most extreme,
[00:20:30.240 --> 00:20:31.960]   isn't that become, you know, this?
[00:20:31.960 --> 00:20:32.800]   - Yeah.
[00:20:32.800 --> 00:20:35.360]   - I mean, you know, I,
[00:20:35.360 --> 00:20:38.680]   I'm not qualified in any way to talk about
[00:20:38.680 --> 00:20:41.040]   people's personalities.
[00:20:41.040 --> 00:20:43.200]   I can just tell you what I've observed
[00:20:43.200 --> 00:20:45.440]   based on all of the work that I do with, you know,
[00:20:45.440 --> 00:20:46.440]   these executives.
[00:20:46.440 --> 00:20:49.960]   Most of the people that I work with
[00:20:49.960 --> 00:20:52.280]   at the highest levels of business and government,
[00:20:52.280 --> 00:20:56.960]   I would say have what you would expect,
[00:20:56.960 --> 00:21:01.480]   which is just unparalleled, just unbridled drive,
[00:21:01.480 --> 00:21:04.720]   which again, you could try to cultivate that,
[00:21:04.720 --> 00:21:07.360]   that you either, you have it or you don't.
[00:21:07.360 --> 00:21:11.520]   They have a fear of stasis and many of them seem, you know,
[00:21:11.520 --> 00:21:15.040]   when we've discussed it, it's a fear of not having enough time
[00:21:15.040 --> 00:21:17.400]   to get accomplished what they want to accomplish.
[00:21:17.400 --> 00:21:19.000]   Obviously, they're very, very smart.
[00:21:19.000 --> 00:21:21.280]   And then there's this other piece where they're willing
[00:21:21.280 --> 00:21:25.120]   to take risks or make decisions that people who have just
[00:21:25.120 --> 00:21:28.040]   lots of empathy may not do.
[00:21:28.040 --> 00:21:33.040]   - This is gonna verge into the realm of armchair psychology.
[00:21:33.320 --> 00:21:35.920]   But Forbes had a piece a few years ago called
[00:21:35.920 --> 00:21:40.920]   the psychopathic CEO and they quote a study that said,
[00:21:40.920 --> 00:21:43.640]   you know, in the average population,
[00:21:43.640 --> 00:21:47.680]   about 1% of people are clinically psychopathic.
[00:21:47.680 --> 00:21:52.680]   Roughly four to as high as 12% of CEOs are psychopathic.
[00:21:52.680 --> 00:21:55.520]   15% is the rate found in prisons.
[00:21:55.520 --> 00:22:00.520]   So there is some evidence that CEOs tend to have those traits,
[00:22:01.160 --> 00:22:04.840]   particularly lack of empathy and drive.
[00:22:04.840 --> 00:22:06.160]   - Mm-hmm, yep.
[00:22:06.160 --> 00:22:11.160]   - And it makes sense just in the generalized point of view.
[00:22:11.160 --> 00:22:12.480]   Go ahead, Brianna.
[00:22:12.480 --> 00:22:14.880]   - No, I was just gonna say, all three of us here
[00:22:14.880 --> 00:22:17.840]   have existed in the startup world, right?
[00:22:17.840 --> 00:22:22.160]   Like, and I think a commonality I see with people working
[00:22:22.160 --> 00:22:27.160]   in startups is a, you wanna put your values into action,
[00:22:27.160 --> 00:22:28.840]   right?
[00:22:28.840 --> 00:22:30.160]   Like you see something in the world
[00:22:30.160 --> 00:22:31.240]   and it doesn't exist,
[00:22:31.240 --> 00:22:33.440]   you wanna create a company your way.
[00:22:33.440 --> 00:22:36.600]   Leo here at Twet, it's hard for me not to notice.
[00:22:36.600 --> 00:22:38.120]   I've never seen a show you've done,
[00:22:38.120 --> 00:22:40.520]   you don't have an underrepresented group on.
[00:22:40.520 --> 00:22:43.200]   I imagine that's something you sit down and you're like,
[00:22:43.200 --> 00:22:44.040]   - We do that.
[00:22:44.040 --> 00:22:44.880]   - These are our values.
[00:22:44.880 --> 00:22:47.040]   That's your values, right?
[00:22:47.040 --> 00:22:48.640]   For me, the way I run rebellion,
[00:22:48.640 --> 00:22:51.160]   there are certain values that I stick to.
[00:22:51.160 --> 00:22:52.320]   Same thing with my games.
[00:22:52.320 --> 00:22:53.840]   - Credit to Lisa on that, by the way,
[00:22:53.840 --> 00:22:56.360]   'cause I am completely, I'm a psychopath, so.
[00:22:56.360 --> 00:22:57.600]   (laughing)
[00:22:57.600 --> 00:22:58.600]   - No, but my point is,
[00:22:58.600 --> 00:23:02.000]   I don't think you can necessarily link like the,
[00:23:02.000 --> 00:23:03.720]   I mean, maybe at the highest levels,
[00:23:03.720 --> 00:23:05.560]   it gives you a certain edge
[00:23:05.560 --> 00:23:08.080]   because there is a human cost,
[00:23:08.080 --> 00:23:09.960]   especially on the labor side.
[00:23:09.960 --> 00:23:13.960]   But I can just, I can see that being a superpower
[00:23:13.960 --> 00:23:17.200]   where if you just don't have to care about your workforce,
[00:23:17.200 --> 00:23:21.480]   if you just care about market share at no,
[00:23:21.480 --> 00:23:23.320]   at all cost, I can just,
[00:23:23.320 --> 00:23:24.840]   I can see that being the factor
[00:23:24.840 --> 00:23:27.000]   that puts you over the top and makes you win.
[00:23:27.000 --> 00:23:28.360]   - Yeah, absolutely.
[00:23:28.360 --> 00:23:31.880]   If anybody really wants to take a deep dive
[00:23:31.880 --> 00:23:33.280]   down this rabbit hole at some point,
[00:23:33.280 --> 00:23:35.400]   John Ronson, who's another really great writer.
[00:23:35.400 --> 00:23:36.720]   - Yes, the psychopath test.
[00:23:36.720 --> 00:23:40.040]   - Yeah, he wrote the psychopath test, it's terrific.
[00:23:40.040 --> 00:23:43.200]   - Yeah, yeah, it makes perfect sense.
[00:23:43.200 --> 00:23:44.560]   But again, my question is,
[00:23:44.560 --> 00:23:47.560]   isn't that something that you see in early stages
[00:23:47.560 --> 00:23:49.360]   of companies, especially startups,
[00:23:49.360 --> 00:23:51.560]   that doesn't wear well in the long term?
[00:23:51.560 --> 00:23:53.280]   And maybe Andy Jassy coming along,
[00:23:53.280 --> 00:23:57.000]   this is kind of a natural transition for Amazon,
[00:23:57.000 --> 00:24:01.400]   from the founder, the kind of driven founder
[00:24:01.400 --> 00:24:03.120]   to something more humane.
[00:24:03.120 --> 00:24:05.480]   - But Andy was employee, what, number six or something?
[00:24:05.480 --> 00:24:06.880]   - Yeah, he's early days guy.
[00:24:06.880 --> 00:24:09.440]   - Right, so I mean, in most cases,
[00:24:09.440 --> 00:24:12.120]   there's a number two who's been there the entire time,
[00:24:12.120 --> 00:24:14.240]   who is acting as a translator,
[00:24:14.240 --> 00:24:18.680]   or the connective tissue who understands the founder,
[00:24:18.680 --> 00:24:21.160]   but also understands humans.
[00:24:21.160 --> 00:24:24.520]   I think we see that at lots and lots of companies,
[00:24:24.520 --> 00:24:27.960]   again, from like Tesla to Sheryl Sandberg at Facebook,
[00:24:27.960 --> 00:24:32.360]   I mean, I think we see that behind every incredibly
[00:24:32.360 --> 00:24:36.200]   successful CEO is like an incredibly perceptive,
[00:24:36.200 --> 00:24:39.680]   you know, is a person who knows how to connect that CEO
[00:24:39.680 --> 00:24:41.600]   and their vision to the rest of the company.
[00:24:41.600 --> 00:24:43.840]   - Jassy, who will be the CEO on Monday,
[00:24:43.840 --> 00:24:47.720]   there's a piece about him in the Wall Street Journal,
[00:24:47.720 --> 00:24:51.040]   in which they say he is, people work closely with him
[00:24:51.040 --> 00:24:53.640]   to describe him as soft spoken approachable.
[00:24:53.640 --> 00:24:56.320]   Some say he's more even keeled with employees
[00:24:56.320 --> 00:24:58.040]   than Mr. Bezos.
[00:24:58.040 --> 00:25:01.760]   He's detailed focused, which psychopaths,
[00:25:01.760 --> 00:25:04.920]   or I'm sorry, CEO's, startup CEOs are often not
[00:25:04.920 --> 00:25:07.120]   so detailed focused.
[00:25:07.120 --> 00:25:08.680]   In fact, that's one of the complaints
[00:25:08.680 --> 00:25:12.880]   that Brad Stone mentions in his op-ed piece is that
[00:25:12.880 --> 00:25:15.080]   Bezos is always looking at the next thing,
[00:25:15.080 --> 00:25:18.000]   and so as a result, the last thing kind of loses focus.
[00:25:18.000 --> 00:25:19.000]   - It doesn't matter anymore.
[00:25:19.000 --> 00:25:20.600]   - Doesn't matter anymore, in which he says,
[00:25:20.600 --> 00:25:23.320]   this is why maybe the Amazon store has become riddled
[00:25:23.320 --> 00:25:26.720]   with fraud, fraudulent reviews, fraudulent sellers,
[00:25:26.720 --> 00:25:27.560]   because it's not--
[00:25:27.560 --> 00:25:29.840]   - But I would argue that that's, but that's not,
[00:25:29.840 --> 00:25:31.880]   you know, there's, this is the problem.
[00:25:31.880 --> 00:25:33.760]   There's the chief executive officer,
[00:25:33.760 --> 00:25:35.120]   which is what CEO stands for,
[00:25:35.120 --> 00:25:36.720]   and then there's the visionary person
[00:25:36.720 --> 00:25:39.480]   who's both the leader and the founder
[00:25:39.480 --> 00:25:42.480]   and the lead idea generator.
[00:25:42.480 --> 00:25:45.760]   You know, sometimes if you don't have both skills,
[00:25:45.760 --> 00:25:48.280]   and at some point when you go beyond that vision,
[00:25:48.280 --> 00:25:50.400]   then you need, you know, you need to have
[00:25:50.400 --> 00:25:51.800]   that chief executive in there,
[00:25:51.800 --> 00:25:54.040]   and the chief executive is a management,
[00:25:54.040 --> 00:25:55.720]   an executive manager.
[00:25:55.720 --> 00:25:58.560]   - Yeah, we look at Apple, who replaced Steve Jobs,
[00:25:58.560 --> 00:26:01.960]   who was kind of the quintessential psychopath
[00:26:01.960 --> 00:26:05.040]   with Tim Cook, who was the quintessential operations guy.
[00:26:05.040 --> 00:26:09.520]   - That's right, so I think maybe for anybody who's listening,
[00:26:09.520 --> 00:26:12.080]   the insight and the takeaway here is, you know,
[00:26:12.080 --> 00:26:13.680]   if you're an entrepreneur,
[00:26:13.680 --> 00:26:15.840]   or if you're somebody who's a founder,
[00:26:15.840 --> 00:26:17.240]   you know, it's probably lessons to learn
[00:26:17.240 --> 00:26:19.960]   from both groups of people, from the psychopaths
[00:26:19.960 --> 00:26:21.040]   and from the operators.
[00:26:21.040 --> 00:26:22.640]   - Yeah, you need both.
[00:26:22.640 --> 00:26:24.840]   And Jobs was smart enough, and it looks like
[00:26:24.840 --> 00:26:27.640]   Jeff was smart enough to have a number two,
[00:26:27.640 --> 00:26:28.480]   or somebody around.
[00:26:28.480 --> 00:26:29.320]   - To cultivate that person.
[00:26:29.320 --> 00:26:30.400]   - And cultivate that person,
[00:26:30.400 --> 00:26:33.280]   knowing that he's gonna need an operations guy now,
[00:26:33.280 --> 00:26:35.200]   and someday perhaps to run it.
[00:26:35.200 --> 00:26:38.000]   What is your experience, Amy, with the boards?
[00:26:38.000 --> 00:26:41.320]   Because Jeff Bezos will continue as chairman of the board.
[00:26:41.320 --> 00:26:44.560]   How much operational influence will he have?
[00:26:44.560 --> 00:26:48.320]   - So again, you know, a lot of these boards of directors,
[00:26:48.320 --> 00:26:49.520]   they start to get nervous.
[00:26:49.520 --> 00:26:51.960]   Well, they get nervous for all kinds of reasons.
[00:26:51.960 --> 00:26:53.000]   I mean, at some point, Steve,
[00:26:53.000 --> 00:26:54.440]   they didn't like Steve Jobs so much.
[00:26:54.440 --> 00:26:56.000]   They got nervous about what he was doing.
[00:26:56.000 --> 00:26:57.000]   - They fired him. - Right, they fired him.
[00:26:57.000 --> 00:26:59.200]   - Yeah. - Right, so, you know,
[00:26:59.200 --> 00:27:01.440]   ultimately it's the vision,
[00:27:01.440 --> 00:27:03.520]   I think there's some belief that the vision
[00:27:03.520 --> 00:27:04.640]   can't be replicated.
[00:27:04.640 --> 00:27:07.640]   I mean, you know, it comes from that person.
[00:27:07.640 --> 00:27:10.360]   And when that person leaves,
[00:27:10.360 --> 00:27:12.920]   some of that vision goes away.
[00:27:12.920 --> 00:27:16.040]   You know, you need the vision,
[00:27:16.040 --> 00:27:18.160]   and that's hard to replicate.
[00:27:18.160 --> 00:27:22.600]   Finding an extraordinary CEO who is an operator,
[00:27:22.600 --> 00:27:27.600]   who's a manager who can work with that visionary,
[00:27:27.600 --> 00:27:29.440]   and eventually take over for them,
[00:27:29.440 --> 00:27:31.640]   you know, there are many, many more talented CEOs
[00:27:31.640 --> 00:27:34.360]   than there are true visionaries.
[00:27:34.360 --> 00:27:37.920]   So I think boards, you know, my hunches,
[00:27:37.920 --> 00:27:39.600]   Bezos isn't going anywhere.
[00:27:39.600 --> 00:27:41.000]   He's just not doing the day.
[00:27:41.000 --> 00:27:42.800]   - Well, he's going in space.
[00:27:42.800 --> 00:27:43.880]   - He's going, yeah.
[00:27:43.880 --> 00:27:45.800]   - Which, by the way, if he were CEO
[00:27:45.800 --> 00:27:46.880]   and I were the board member,
[00:27:46.880 --> 00:27:48.920]   I would have said under no circumstances,
[00:27:48.920 --> 00:27:52.240]   are you allowed to strap a giant explosive to your ass
[00:27:52.240 --> 00:27:53.560]   and fire yourself into space?
[00:27:53.560 --> 00:27:55.480]   That just not gonna happen.
[00:27:55.480 --> 00:27:57.760]   - Is Branson still the CEO of Virgin?
[00:27:57.760 --> 00:27:58.600]   - Oh, that's interesting.
[00:27:58.600 --> 00:28:00.600]   I doubt he is hands-on the day.
[00:28:00.600 --> 00:28:03.080]   - Okay. - Yeah, so that's the funny thing.
[00:28:03.080 --> 00:28:05.760]   Bezos announced he's going into space.
[00:28:05.760 --> 00:28:06.840]   What is it, the 22nd?
[00:28:06.840 --> 00:28:08.360]   And so Branson says,
[00:28:08.360 --> 00:28:10.560]   I'm going into space the 11th.
[00:28:10.560 --> 00:28:12.640]   That is, by the way,
[00:28:12.640 --> 00:28:17.640]   another quintessential hallmark of these high-end CEOs
[00:28:17.640 --> 00:28:19.840]   is their competitiveness.
[00:28:19.840 --> 00:28:21.200]   - Yeah, yeah.
[00:28:21.200 --> 00:28:23.280]   - I mean, who the hell cares who's the first
[00:28:23.280 --> 00:28:24.400]   to go in his space?
[00:28:24.400 --> 00:28:27.480]   Or who has very famously the tallest yacht.
[00:28:27.480 --> 00:28:32.200]   Larry Ellison built a yacht with his mask,
[00:28:32.200 --> 00:28:35.160]   just a little bit taller than the number two guy.
[00:28:35.160 --> 00:28:37.640]   In fact, so much taller that he couldn't get it
[00:28:37.640 --> 00:28:39.760]   under the Bay Bridge anymore.
[00:28:40.760 --> 00:28:43.080]   But he wanted the tallest yacht.
[00:28:43.080 --> 00:28:44.520]   Yeah, that's just dumb.
[00:28:44.520 --> 00:28:46.560]   - Absolutely.
[00:28:46.560 --> 00:28:49.120]   - No, I'm sorry, go ahead.
[00:28:49.120 --> 00:28:50.680]   - No, thanks.
[00:28:50.680 --> 00:28:52.800]   I just wanted to say we had this discussion a lot
[00:28:52.800 --> 00:28:55.600]   when Steve Jobs sadly passed away.
[00:28:55.600 --> 00:28:59.600]   And obviously Tim Cook took over.
[00:28:59.600 --> 00:29:01.920]   And there were a lot of naysayers saying,
[00:29:01.920 --> 00:29:03.720]   this is the end of Apple.
[00:29:03.720 --> 00:29:06.720]   Tim Cook obviously had much more of a background
[00:29:06.720 --> 00:29:08.000]   in logistics.
[00:29:08.000 --> 00:29:10.400]   And there were people asking if he was gonna have
[00:29:10.400 --> 00:29:12.520]   the vision to keep the company going.
[00:29:12.520 --> 00:29:14.160]   Apple did several things.
[00:29:14.160 --> 00:29:17.640]   They instituted Apple University to kind of codify
[00:29:17.640 --> 00:29:19.080]   the lessons that they had learned
[00:29:19.080 --> 00:29:22.960]   under the Steve Jobs era to kind of facilitate
[00:29:22.960 --> 00:29:25.320]   that kind of visionary leadership.
[00:29:25.320 --> 00:29:28.080]   And what I hear from a lot of my engineer friends
[00:29:28.080 --> 00:29:30.840]   that work at Apple is it actually improved
[00:29:30.840 --> 00:29:32.960]   in the post job years.
[00:29:32.960 --> 00:29:36.400]   Because this kind of tolerance for a-holes,
[00:29:36.400 --> 00:29:39.080]   it was just a lot, I hope I could say that Leo,
[00:29:39.080 --> 00:29:40.360]   but it was just, it was not as-
[00:29:40.360 --> 00:29:41.200]   - Stars up concerning.
[00:29:41.200 --> 00:29:42.200]   - Always on these teams.
[00:29:42.200 --> 00:29:43.960]   - He's the perfect word.
[00:29:43.960 --> 00:29:46.760]   - You know, I think in a lot of ways,
[00:29:46.760 --> 00:29:49.400]   Apple ended up getting bigger and better.
[00:29:49.400 --> 00:29:54.200]   And I think Amazon more than Apple is less dependent
[00:29:54.200 --> 00:29:58.360]   on product hits and more on a supply chain
[00:29:58.360 --> 00:30:00.760]   like going into further and further markets.
[00:30:00.760 --> 00:30:04.080]   So I think that is a more,
[00:30:04.080 --> 00:30:05.560]   I don't wanna diminish this,
[00:30:05.560 --> 00:30:07.680]   but it's a more fungible skill.
[00:30:07.680 --> 00:30:11.080]   It's less dependent on like a Steve Jobs type, I think.
[00:30:11.080 --> 00:30:13.800]   - I would disagree on that because Amazon
[00:30:13.800 --> 00:30:15.160]   is in a product based company,
[00:30:15.160 --> 00:30:16.960]   whereas Apple is very much tied,
[00:30:16.960 --> 00:30:18.760]   Apple is its products.
[00:30:18.760 --> 00:30:23.080]   Amazon is very much not its traditional products.
[00:30:23.080 --> 00:30:26.200]   Amazon is a enormous, I mean, Amazon
[00:30:26.200 --> 00:30:27.960]   is the invisible infrastructure now
[00:30:27.960 --> 00:30:30.640]   powering enormous parts of everyday life
[00:30:30.640 --> 00:30:36.680]   that range from huge parts of our economy
[00:30:36.680 --> 00:30:41.240]   to logistics, to web hosting,
[00:30:41.240 --> 00:30:43.720]   to literally to pharmaceuticals and medicine
[00:30:43.720 --> 00:30:45.960]   and I could go on and on.
[00:30:45.960 --> 00:30:50.360]   So I'm, I don't know, I would be curious to know
[00:30:50.360 --> 00:30:53.360]   what's actually going on over there.
[00:30:53.360 --> 00:30:57.840]   I mean, it's hard not to also look at the changeover
[00:30:57.840 --> 00:31:02.600]   at Amazon in the same,
[00:31:02.600 --> 00:31:05.240]   while also looking at some personal decisions
[00:31:05.240 --> 00:31:09.320]   that Bezos made kind of sort around the same time.
[00:31:09.320 --> 00:31:14.720]   So, you know, we have these enormous tech companies.
[00:31:14.720 --> 00:31:16.480]   - You think he's having a midlife crisis?
[00:31:16.480 --> 00:31:18.520]   Is that what you're saying?
[00:31:18.520 --> 00:31:19.880]   - You know, I didn't say that Leo,
[00:31:19.880 --> 00:31:22.080]   you said that just now, but.
[00:31:22.080 --> 00:31:24.120]   - It might be, he started lifting weights.
[00:31:24.120 --> 00:31:26.280]   - You know, what happens when boys
[00:31:26.280 --> 00:31:27.640]   have their midlife crises,
[00:31:27.640 --> 00:31:30.680]   they buy the fast car and they try to drive it real fast
[00:31:30.680 --> 00:31:32.640]   and Jeff is driving real fast to space.
[00:31:32.640 --> 00:31:34.240]   - When you're the richest man in the world,
[00:31:34.240 --> 00:31:37.040]   your fast car could be pretty fast.
[00:31:37.040 --> 00:31:37.880]   - It could be really fast.
[00:31:37.880 --> 00:31:40.040]   It can be a suborbital.
[00:31:40.040 --> 00:31:43.960]   So, yeah, but there's another piece of this,
[00:31:43.960 --> 00:31:48.960]   which is the world is quickly being divvied up by China
[00:31:48.960 --> 00:31:53.160]   and you know, China is also making huge advancements
[00:31:53.160 --> 00:31:54.600]   in some of these same areas.
[00:31:54.600 --> 00:31:57.400]   And you know, China under Xi Jinping
[00:31:57.400 --> 00:32:02.320]   is in the process of a infrastructure land grab
[00:32:02.320 --> 00:32:04.480]   and literally just a couple of days ago
[00:32:04.480 --> 00:32:09.480]   when the CCP had its hundred year parade in Beijing,
[00:32:09.480 --> 00:32:13.960]   you know, she showed up in a Mao era style suit,
[00:32:13.960 --> 00:32:14.960]   like a great suit. - Oh, that's interesting.
[00:32:14.960 --> 00:32:16.280]   - So he's making it, yeah.
[00:32:16.280 --> 00:32:17.120]   - Oh, that's really interesting.
[00:32:17.120 --> 00:32:19.760]   - And he's very, very, he's the fifth leader, I think,
[00:32:19.760 --> 00:32:24.800]   since Mao and he is very clearly saying that, you know,
[00:32:24.800 --> 00:32:27.440]   they're gonna be more like wolves than sheep
[00:32:27.440 --> 00:32:32.080]   and aggressively pursuing CCP ideas,
[00:32:32.080 --> 00:32:32.920]   which makes sense.
[00:32:32.920 --> 00:32:35.640]   I mean, he sees like a very short window of time.
[00:32:35.640 --> 00:32:39.280]   So they're pushing pretty aggressively, you know,
[00:32:39.280 --> 00:32:41.840]   so I don't know, we'll have to see which shakes out
[00:32:41.840 --> 00:32:43.600]   with our companies. - I did not see that.
[00:32:43.600 --> 00:32:47.000]   Here's a picture of the hundredth anniversary
[00:32:47.000 --> 00:32:49.280]   of the president Xi in a Mao suit,
[00:32:49.280 --> 00:32:53.160]   which is not widely worn anymore in China,
[00:32:53.160 --> 00:32:57.680]   but it is very important iconography for the Chinese people.
[00:32:57.680 --> 00:33:00.080]   I, when I was in Beijing some years ago,
[00:33:00.080 --> 00:33:03.480]   I went to Mao's tomb and there's long hours
[00:33:03.480 --> 00:33:06.000]   and hours long lines of people from all over the country
[00:33:06.000 --> 00:33:10.400]   coming to see Mao's body in that suit.
[00:33:10.400 --> 00:33:13.320]   And so I think he, despite the fact that he did
[00:33:13.320 --> 00:33:18.320]   some horrible things in China, killed millions of his citizens,
[00:33:18.320 --> 00:33:22.200]   but he still revered, I think, by some.
[00:33:22.200 --> 00:33:25.080]   And it's certainly significant that Xi was wearing that suit.
[00:33:25.080 --> 00:33:26.240]   I had not-- - Oh yeah.
[00:33:26.240 --> 00:33:28.040]   - I was so worried. - Totally.
[00:33:28.040 --> 00:33:30.200]   - And there's a rise in nationalism.
[00:33:30.200 --> 00:33:33.200]   People are very excited about Chinese companies in China.
[00:33:33.200 --> 00:33:37.400]   And Alibaba, you know, is one of those companies
[00:33:37.400 --> 00:33:39.880]   and it doesn't, we do a ton of work overseas
[00:33:39.880 --> 00:33:44.240]   in Scandinavia and Western Europe and, you know, all over.
[00:33:44.240 --> 00:33:46.120]   You know, and Alibaba is pushing hard
[00:33:46.120 --> 00:33:47.080]   into some of these markets.
[00:33:47.080 --> 00:33:50.040]   So again, I look at the new values statement from Amazon
[00:33:50.040 --> 00:33:51.680]   and I'm seeing this a little bit less
[00:33:51.680 --> 00:33:54.960]   as a sustainability goal and more like,
[00:33:54.960 --> 00:33:57.480]   we're gonna be more globally dominant.
[00:33:57.480 --> 00:34:00.640]   You know, outside of the US, for example.
[00:34:00.640 --> 00:34:02.120]   So.
[00:34:02.120 --> 00:34:03.760]   - So do you think that's why he wore that suit
[00:34:03.760 --> 00:34:05.240]   to, is that of national pride?
[00:34:05.240 --> 00:34:09.000]   - I mean, not because of just Jeff Bezos and Amazon,
[00:34:09.000 --> 00:34:12.880]   but yes, he's very public.
[00:34:12.880 --> 00:34:15.720]   I mean, they are very, very intent.
[00:34:15.720 --> 00:34:17.560]   She was very intent on, yeah.
[00:34:17.560 --> 00:34:19.920]   - You normally see him in a Western suit and tie.
[00:34:19.920 --> 00:34:20.760]   - Yeah.
[00:34:20.760 --> 00:34:23.320]   - No, I mean, that's a huge red flag.
[00:34:23.320 --> 00:34:24.880]   - Literally interesting.
[00:34:24.880 --> 00:34:26.640]   Yeah, yeah, bright red.
[00:34:27.640 --> 00:34:32.560]   So I don't know if Jassy will be a leader in the mold
[00:34:32.560 --> 00:34:35.920]   of Jeff Bezos, if he will be a caretaker,
[00:34:35.920 --> 00:34:38.400]   more in the mold of a Tim Cook.
[00:34:38.400 --> 00:34:40.680]   Well, but it starts on Monday.
[00:34:40.680 --> 00:34:43.160]   One of the things Amazon has done,
[00:34:43.160 --> 00:34:46.640]   maybe it's Jeff Bezos' last act,
[00:34:46.640 --> 00:34:50.800]   has asked to have the brand new chair of the FTC,
[00:34:50.800 --> 00:34:53.040]   Lena Kahn, recuse herself,
[00:34:54.360 --> 00:34:57.040]   over any antitrust actions against Amazon,
[00:34:57.040 --> 00:35:00.720]   because she's clearly biased, says Amazon's attorney.
[00:35:00.720 --> 00:35:03.160]   She's written many papers saying the big tech companies
[00:35:03.160 --> 00:35:04.360]   ought to be broken up.
[00:35:04.360 --> 00:35:08.240]   Both as a professor and as an attorney.
[00:35:08.240 --> 00:35:11.920]   I think it's a reasonable thing for Amazon to try.
[00:35:11.920 --> 00:35:15.000]   I don't know if a judge will back it up.
[00:35:15.000 --> 00:35:16.920]   It's kind of an interesting way to fight
[00:35:16.920 --> 00:35:19.360]   this antitrust furore.
[00:35:19.360 --> 00:35:21.520]   - I saw this Leo and I thought of every single
[00:35:21.520 --> 00:35:25.040]   twin appearance I've had criticizing all these different
[00:35:25.040 --> 00:35:27.120]   big companies and realizing,
[00:35:27.120 --> 00:35:31.640]   well, there goes my dream of operating being at the FTC.
[00:35:31.640 --> 00:35:35.760]   It sounds like you said a ploy.
[00:35:35.760 --> 00:35:37.360]   - It's a Hail Mary's.
[00:35:37.360 --> 00:35:39.480]   - Yeah, I don't know how credible it is.
[00:35:39.480 --> 00:35:42.680]   It seems like working the refs, right?
[00:35:42.680 --> 00:35:44.640]   Like you got to yell at the referees
[00:35:44.640 --> 00:35:48.080]   and hope that it helps more of the calls go your way.
[00:35:48.080 --> 00:35:49.400]   And they've shown that it works.
[00:35:49.400 --> 00:35:53.600]   So it's probably-- - She was asked by Mike Lee
[00:35:53.600 --> 00:35:57.720]   during her confirmation if she should recuse herself.
[00:35:57.720 --> 00:35:59.080]   - Well.
[00:35:59.080 --> 00:36:00.520]   - So on the Republican side anyway,
[00:36:00.520 --> 00:36:04.560]   there might be some sentiment that she shouldn't.
[00:36:04.560 --> 00:36:08.320]   - I just, I feel like you can't know a lot about these things
[00:36:08.320 --> 00:36:10.400]   and not have opinions about things.
[00:36:10.400 --> 00:36:13.760]   And I think the better question is, can you be fair?
[00:36:13.760 --> 00:36:16.280]   And everything I've seen,
[00:36:16.280 --> 00:36:19.600]   this does seem that the Biden administration in general
[00:36:19.600 --> 00:36:21.600]   is gonna be a lot tougher on antitrust
[00:36:21.600 --> 00:36:24.840]   and anti-competitive action, which I fully support.
[00:36:24.840 --> 00:36:27.520]   And I think she's going to be in that vein,
[00:36:27.520 --> 00:36:30.720]   but I think she'll, I don't think she's gonna be underhanded
[00:36:30.720 --> 00:36:31.880]   if you know, I think she'll be fine.
[00:36:31.880 --> 00:36:34.280]   - I hate them and I'm going to get them.
[00:36:34.280 --> 00:36:39.760]   - Yeah, although you could look at the EU commissioner,
[00:36:39.760 --> 00:36:44.760]   Margarit Vesta here, and she clearly hates big American tech
[00:36:45.120 --> 00:36:49.320]   companies so much so what was the latest thing?
[00:36:49.320 --> 00:36:54.320]   She said, well, we don't, I'm not sure if what Amazon
[00:36:54.320 --> 00:36:59.680]   and Apple are up to in protecting privacy.
[00:36:59.680 --> 00:37:02.560]   She said Apple's privacy protections might just be a way
[00:37:02.560 --> 00:37:05.000]   of trying to become more dominant.
[00:37:05.000 --> 00:37:07.040]   So I'm gonna really have to look at this.
[00:37:07.040 --> 00:37:09.760]   Wow, it's like--
[00:37:09.760 --> 00:37:10.600]   - Yeah.
[00:37:10.600 --> 00:37:13.960]   - I have to say there you might have a case
[00:37:13.960 --> 00:37:15.520]   that she actually has an extagrion.
[00:37:15.520 --> 00:37:19.120]   She's really already decided how the case will be decided.
[00:37:19.120 --> 00:37:21.640]   - I kind of love the fact that Europe's main,
[00:37:21.640 --> 00:37:25.000]   the Europe's main contribution to the future of technology
[00:37:25.000 --> 00:37:26.760]   at this point is regulations.
[00:37:26.760 --> 00:37:27.600]   - Yeah.
[00:37:27.600 --> 00:37:31.600]   - I mean, can I offer a counterpoint to like another way
[00:37:31.600 --> 00:37:32.600]   to do this?
[00:37:32.600 --> 00:37:35.760]   So I was talking to some folks in the government of Dubai
[00:37:35.760 --> 00:37:38.560]   a couple of weeks ago and they were telling me that
[00:37:38.560 --> 00:37:40.520]   in Dubai's, I think everybody knows,
[00:37:40.520 --> 00:37:43.600]   they're starting to test all different types of drones
[00:37:43.600 --> 00:37:45.160]   that can carry different payloads.
[00:37:45.160 --> 00:37:50.160]   So drones to take pictures, drones to carry small packages
[00:37:50.160 --> 00:37:54.440]   around and of course, those ridiculous flying taxis,
[00:37:54.440 --> 00:37:56.240]   drones that would carry one or two people
[00:37:56.240 --> 00:37:58.040]   from place to place.
[00:37:58.040 --> 00:38:01.240]   And so as this technology's being developed and tested,
[00:38:01.240 --> 00:38:04.600]   they've actually launched a beta testing group
[00:38:04.600 --> 00:38:09.600]   within the government to like develop and pilot
[00:38:09.600 --> 00:38:13.720]   and beta test policy, which is really, really cool.
[00:38:13.720 --> 00:38:16.320]   So basically what they're doing is as the tech is being
[00:38:16.320 --> 00:38:19.360]   developed, there's a group of people who are thinking about
[00:38:19.360 --> 00:38:21.400]   and trying to like beta test policy
[00:38:21.400 --> 00:38:23.480]   and they're talking to each other rather than it being
[00:38:23.480 --> 00:38:24.800]   totally siloed.
[00:38:24.800 --> 00:38:27.440]   So by the time that the technology's actually ready,
[00:38:27.440 --> 00:38:29.840]   there's policy ready to go with it.
[00:38:29.840 --> 00:38:32.320]   And it just seems like such a smart,
[00:38:32.320 --> 00:38:36.240]   a really smart approach, hard one, really, really difficult,
[00:38:36.240 --> 00:38:40.280]   but also like a much easier path forward for everybody.
[00:38:40.280 --> 00:38:44.080]   - I'm just booking a drone taxi right now in Dubai.
[00:38:44.080 --> 00:38:47.880]   I just wanna see how much it's gonna cost.
[00:38:47.880 --> 00:38:50.960]   - Like Uber tried that for a while, right?
[00:38:50.960 --> 00:38:53.840]   - Yeah, so here's the Margaret Margarit.
[00:38:53.840 --> 00:38:56.600]   I gotta get work on her name.
[00:38:56.600 --> 00:39:01.600]   Margarit Vistiger, she warned Apple against using privacy
[00:39:03.760 --> 00:39:07.240]   and security to limit competition.
[00:39:07.240 --> 00:39:10.320]   Yeah, we wouldn't want that.
[00:39:10.320 --> 00:39:13.200]   But you know, I mean, she's right,
[00:39:13.200 --> 00:39:16.920]   Apple is focusing on privacy and security as a differentiator
[00:39:16.920 --> 00:39:18.600]   with other companies and tech.
[00:39:18.600 --> 00:39:21.040]   And yes, they would like to beat the competition
[00:39:21.040 --> 00:39:25.760]   by doing so, but I can't see any benefit to the EU's
[00:39:25.760 --> 00:39:27.720]   penalizing them for that.
[00:39:27.720 --> 00:39:30.440]   Maybe other companies should do their best
[00:39:30.440 --> 00:39:33.360]   to improve privacy and security too.
[00:39:33.360 --> 00:39:35.320]   - Or maybe incent them instead.
[00:39:35.320 --> 00:39:38.480]   If you approach all of this punitively,
[00:39:38.480 --> 00:39:40.520]   then these companies are gonna fight back.
[00:39:40.520 --> 00:39:42.040]   And ultimately, what is the end goal?
[00:39:42.040 --> 00:39:45.000]   You want better privacy for people, I would assume, right?
[00:39:45.000 --> 00:39:49.000]   And so is there another approach that doesn't result in
[00:39:49.000 --> 00:39:52.600]   years and years of protracted legal arguing?
[00:39:52.600 --> 00:39:54.320]   And I think there is.
[00:39:54.320 --> 00:39:58.400]   Is there a way to incent these companies
[00:39:58.400 --> 00:40:01.680]   to change their policies in a way that everybody benefits?
[00:40:01.680 --> 00:40:04.400]   That is a much more challenging approach.
[00:40:04.400 --> 00:40:07.000]   I think it's the smarter approach in the longterm.
[00:40:07.000 --> 00:40:10.680]   - Yeah, I think all of us here,
[00:40:10.680 --> 00:40:13.160]   we're hyper educated on these tech issues.
[00:40:13.160 --> 00:40:15.520]   I do want to say though,
[00:40:15.520 --> 00:40:18.400]   I can understand how someone from the outside
[00:40:18.400 --> 00:40:21.760]   would see some of Apple's stuff as anti-competitive.
[00:40:21.760 --> 00:40:23.080]   I do.
[00:40:23.080 --> 00:40:25.280]   I just bought a very, very top of the line
[00:40:25.280 --> 00:40:27.240]   Windows laptop this week.
[00:40:27.240 --> 00:40:30.680]   It kills me that I cannot get my eye messages on that.
[00:40:30.680 --> 00:40:33.600]   - Yeah, all of us here, all of us understand,
[00:40:33.600 --> 00:40:35.080]   yeah, the encryption.
[00:40:35.080 --> 00:40:37.920]   And if they open that up to things like Windows,
[00:40:37.920 --> 00:40:40.440]   it's just drastically less private.
[00:40:40.440 --> 00:40:43.080]   It makes the entire ecosystem weaker.
[00:40:43.080 --> 00:40:46.240]   We understand that, but I can understand
[00:40:46.240 --> 00:40:49.160]   how a regulator would look at that fact
[00:40:49.160 --> 00:40:50.760]   and the fact that Apple music,
[00:40:50.760 --> 00:40:53.640]   it's much more difficult to use it on Windows laptop
[00:40:53.640 --> 00:40:55.600]   and a lot of that lock in.
[00:40:55.600 --> 00:40:58.360]   I think there's a discussion you can have there.
[00:40:58.360 --> 00:41:00.560]   So I think again, it comes back to,
[00:41:00.560 --> 00:41:03.000]   can you trust these regulators to be fair?
[00:41:03.000 --> 00:41:05.600]   And then ultimately, what is the legal environment
[00:41:05.600 --> 00:41:07.200]   that they're working in?
[00:41:07.200 --> 00:41:09.480]   Are the laws going to be fair?
[00:41:09.480 --> 00:41:14.480]   And I personally fall on the side where I have faith in that.
[00:41:14.480 --> 00:41:16.720]   - All right, now you're forcing me to be fair
[00:41:16.720 --> 00:41:18.520]   to Margaritivis, thank you.
[00:41:18.520 --> 00:41:21.160]   And point out that this response was,
[00:41:21.160 --> 00:41:24.200]   and Apple said, look, we can't allow side loading
[00:41:24.200 --> 00:41:27.000]   in our store 'cause it would hurt privacy and security.
[00:41:27.000 --> 00:41:30.040]   And she says, no, I'm not against privacy and security,
[00:41:30.040 --> 00:41:32.080]   but I wanna make sure that that's the real reason
[00:41:32.080 --> 00:41:35.240]   they're doing it not as an any competitive move.
[00:41:35.240 --> 00:41:38.640]   They're hiding behind privacy and security
[00:41:38.640 --> 00:41:41.520]   as a shield to allow them to be monopolistic.
[00:41:41.520 --> 00:41:43.080]   And I guess that's not unreasonable.
[00:41:43.080 --> 00:41:44.640]   So I apologize. - Yeah, I agree.
[00:41:44.640 --> 00:41:46.880]   - Don't please don't investigate me.
[00:41:46.880 --> 00:41:48.640]   - I don't know if this means anything to anybody,
[00:41:48.640 --> 00:41:50.400]   but I had to get a new phone.
[00:41:50.400 --> 00:41:51.440]   - Yeah.
[00:41:51.440 --> 00:41:55.120]   - And so I've been using a Galaxy forever.
[00:41:55.120 --> 00:41:58.320]   And I was thinking, you know,
[00:41:58.320 --> 00:42:02.000]   these new privacy features actually like a lot.
[00:42:02.000 --> 00:42:05.720]   And I came this close to switching back to an iPhone.
[00:42:05.720 --> 00:42:07.160]   - But you didn't.
[00:42:07.160 --> 00:42:08.520]   - But I didn't, I didn't.
[00:42:08.520 --> 00:42:09.360]   - What'd you get?
[00:42:09.360 --> 00:42:12.240]   - Ultimately, I just got another, I got to ask,
[00:42:12.240 --> 00:42:13.640]   what is it? - S21.
[00:42:13.640 --> 00:42:14.640]   - S21. - S21.
[00:42:14.640 --> 00:42:15.480]   - Yeah.
[00:42:15.480 --> 00:42:20.320]   - I got a enormous, I can't hold it in one hand phone.
[00:42:20.320 --> 00:42:22.800]   - That's the way the Samsung, yes.
[00:42:22.800 --> 00:42:24.400]   - 'Cause it was the mistress,
[00:42:24.400 --> 00:42:26.440]   one that was smaller than I had before.
[00:42:26.440 --> 00:42:29.520]   But I don't know, I hate the way that Apple
[00:42:29.520 --> 00:42:32.960]   locks down the entire OS and you can't do any,
[00:42:32.960 --> 00:42:35.960]   you know, it's so locked down and dumbed down.
[00:42:35.960 --> 00:42:39.760]   And I ultimately just went back to Samsung.
[00:42:39.760 --> 00:42:43.840]   I might, whatever the new OS, the mobile OS is called.
[00:42:43.840 --> 00:42:45.680]   - iOS 15. - Yep.
[00:42:45.680 --> 00:42:47.160]   - Yeah. - So that comes out,
[00:42:47.160 --> 00:42:49.000]   what, November? - Yeah, the public beta
[00:42:49.000 --> 00:42:49.840]   came out this week.
[00:42:49.840 --> 00:42:51.440]   I've been playing with it a little bit.
[00:42:51.440 --> 00:42:55.520]   You know what, on the iPad, it really is nice.
[00:42:55.520 --> 00:42:58.200]   And they've fixed up the multitasking,
[00:42:58.200 --> 00:42:59.760]   they've added some nice features.
[00:42:59.760 --> 00:43:02.840]   Apple is really good and I hate them for this
[00:43:02.840 --> 00:43:06.560]   at making you wanna live entirely in the ecosystem.
[00:43:06.560 --> 00:43:07.880]   - Absolutely. - That's the whole point
[00:43:07.880 --> 00:43:11.120]   of not putting eye messages on Android or Windows.
[00:43:11.120 --> 00:43:14.720]   That's the whole point of adding features that work only,
[00:43:14.720 --> 00:43:17.720]   as long as you have a Mac and iPad and iPhone.
[00:43:17.720 --> 00:43:20.280]   So then for instance, notes, Apple's Notes app,
[00:43:20.280 --> 00:43:22.280]   which comes with all three devices.
[00:43:22.280 --> 00:43:24.880]   Now as a nice feature, you could swipe up from the corner
[00:43:24.880 --> 00:43:26.440]   and add a note to a webpage.
[00:43:26.440 --> 00:43:27.640]   The next time you go to the webpage,
[00:43:27.640 --> 00:43:29.240]   that note is available.
[00:43:29.240 --> 00:43:30.840]   That is a great feature.
[00:43:30.840 --> 00:43:34.600]   - Hmm, that's kinda cool. - Yeah, I'm gonna use that a lot.
[00:43:34.600 --> 00:43:37.280]   And I'm kind of a, the reason I'm very aware,
[00:43:37.280 --> 00:43:40.400]   hyper aware of it is I'm into note-taking apps.
[00:43:40.400 --> 00:43:42.160]   I have the weird fetish,
[00:43:42.160 --> 00:43:44.680]   looking for the best note-taking app.
[00:43:44.680 --> 00:43:47.240]   And it's gonna make me mad if there are features
[00:43:47.240 --> 00:43:50.200]   in Apple's Notes, which is not the best note-taking app.
[00:43:50.200 --> 00:43:52.960]   But that I want in all of my apps.
[00:43:52.960 --> 00:43:56.080]   And similarly, you're gonna be able to drag,
[00:43:56.080 --> 00:44:00.520]   this is wild, but if you have an iMac or a Mac desktop
[00:44:00.520 --> 00:44:05.160]   with an iPad, you can drag a folder from one to the other,
[00:44:05.160 --> 00:44:09.120]   just by dragging it with the mouse over to the next screen.
[00:44:09.120 --> 00:44:10.720]   These are things Apple does very well,
[00:44:10.720 --> 00:44:12.640]   an ecosystem place that Apple does very well,
[00:44:12.640 --> 00:44:13.840]   that make it very attractive.
[00:44:13.840 --> 00:44:16.400]   And to people like you, to both of you,
[00:44:16.400 --> 00:44:19.480]   who are in hybrid universes,
[00:44:19.480 --> 00:44:23.160]   I don't know if you're not getting the worst of both worlds.
[00:44:23.160 --> 00:44:25.440]   - I don't know.
[00:44:25.440 --> 00:44:29.600]   - Wouldn't you want all Galaxy or all Apple or?
[00:44:29.600 --> 00:44:30.440]   No?
[00:44:30.440 --> 00:44:32.720]   - Something I found lately is,
[00:44:32.720 --> 00:44:34.800]   Amy, I really agree with what you're saying,
[00:44:34.800 --> 00:44:38.400]   where I feel like a frog boiling in water,
[00:44:38.400 --> 00:44:41.880]   I had slowly gotten not used to how much I was giving up
[00:44:41.880 --> 00:44:45.320]   being on all 100% Apple.
[00:44:45.320 --> 00:44:48.680]   Yeah, I'm one of the people that around 2007,
[00:44:48.680 --> 00:44:52.480]   I've kind of discovered the Mac and just went all in.
[00:44:52.480 --> 00:44:55.840]   Now with Unreal Engine 5 coming out,
[00:44:55.840 --> 00:44:56.920]   I'm really wanting to learn in the master.
[00:44:56.920 --> 00:44:58.320]   - There's a good example.
[00:44:58.320 --> 00:44:59.160]   Yeah.
[00:44:59.160 --> 00:45:02.800]   - I've spent a lot of time working in Windows now,
[00:45:02.800 --> 00:45:06.360]   I just bought this, this is a Razer Blade.
[00:45:06.360 --> 00:45:08.200]   - Apple did this as an unforced error,
[00:45:08.200 --> 00:45:10.360]   this is the Apple Epic problem.
[00:45:10.360 --> 00:45:11.440]   - Absolutely.
[00:45:11.440 --> 00:45:15.640]   But my point is, I was talking to Christina about this,
[00:45:15.640 --> 00:45:17.000]   I feel like for people like us,
[00:45:17.000 --> 00:45:19.880]   we do like to play with things and tinker.
[00:45:19.880 --> 00:45:23.280]   And what I've found spending more time in Windows lately,
[00:45:23.280 --> 00:45:28.280]   is there's this joyous experience of tweaking things
[00:45:28.280 --> 00:45:30.760]   and downloading whatever I want.
[00:45:30.760 --> 00:45:32.640]   I've really just kind of,
[00:45:32.640 --> 00:45:34.400]   it's like I've shut that part of my brain off
[00:45:34.400 --> 00:45:37.160]   and then they introduce the M1 chip
[00:45:37.160 --> 00:45:39.560]   and you give it up a little bit more
[00:45:39.560 --> 00:45:42.200]   and then you give up unsigned apps.
[00:45:42.200 --> 00:45:44.720]   And I've just, I think I've come to the point
[00:45:44.720 --> 00:45:47.360]   where I really feel like I need both machines,
[00:45:47.360 --> 00:45:51.200]   like one to do my fun experimental play stuff with
[00:45:51.200 --> 00:45:55.960]   and then an Apple device for consuming stuff
[00:45:55.960 --> 00:45:59.200]   or for having a phone, doing that kind of stuff
[00:45:59.200 --> 00:46:02.680]   where it's very secure and predictable and easy to use,
[00:46:02.680 --> 00:46:05.240]   but you have very, very locked down.
[00:46:05.240 --> 00:46:07.640]   - Do you think that the ideal will ultimately
[00:46:07.640 --> 00:46:09.480]   being a little bit of everything?
[00:46:09.480 --> 00:46:13.200]   - Yeah, I'm sorry, I didn't mean to interrupt you.
[00:46:13.200 --> 00:46:14.200]   - No, please.
[00:46:14.200 --> 00:46:15.280]   - Oh, go ahead.
[00:46:15.280 --> 00:46:17.560]   - So, Brianna just made me think of something.
[00:46:17.560 --> 00:46:22.720]   So I think all of this antitrust and anti-competitiveness,
[00:46:22.720 --> 00:46:24.080]   stuff that's going on,
[00:46:24.080 --> 00:46:30.760]   is actually an argument about interoperability,
[00:46:30.760 --> 00:46:33.680]   but I don't think you can legislate, right?
[00:46:33.680 --> 00:46:37.040]   - I'm trying to, that's one of the six bills.
[00:46:37.040 --> 00:46:40.200]   - Yeah, I mean, because I'm not saying
[00:46:40.200 --> 00:46:41.680]   that they should totally work together,
[00:46:41.680 --> 00:46:44.160]   but haven't we been having this, you know,
[00:46:44.160 --> 00:46:47.080]   Mac versus PC problem now for,
[00:46:47.080 --> 00:46:49.320]   I don't know, since I was in high school?
[00:46:49.320 --> 00:46:51.680]   Could you say very long time ago?
[00:46:51.680 --> 00:46:54.920]   - Could you say that, as I think it's,
[00:46:54.920 --> 00:46:56.840]   is this David Sisolini's bill,
[00:46:56.840 --> 00:46:59.920]   the one that requires companies be interoperable?
[00:46:59.920 --> 00:47:02.120]   Could you order that?
[00:47:02.120 --> 00:47:04.880]   - You know, I don't know, I mean,
[00:47:04.880 --> 00:47:06.880]   I can tell you that I can, you know,
[00:47:06.880 --> 00:47:11.120]   we use Macs at work, I'm on a Mac right now
[00:47:11.120 --> 00:47:13.200]   and I can plug this Android in using something
[00:47:13.200 --> 00:47:15.960]   called the DEX app, Samsung,
[00:47:15.960 --> 00:47:16.920]   which is actually pretty cool,
[00:47:16.920 --> 00:47:20.480]   it emulates the, it's an emulator, so it'll emulate.
[00:47:20.480 --> 00:47:22.120]   - You're using DEX, really?
[00:47:22.120 --> 00:47:24.160]   - I am, yeah. - Wow.
[00:47:24.160 --> 00:47:25.720]   - It's not janky anymore.
[00:47:25.720 --> 00:47:28.680]   - So you have a DEX dock that you put your Samsung in
[00:47:28.680 --> 00:47:30.520]   and a big screen the dock is attached to
[00:47:30.520 --> 00:47:31.320]   with keyboard and mouse
[00:47:31.320 --> 00:47:33.560]   and now suddenly you have a desktop operating system
[00:47:33.560 --> 00:47:36.040]   powered by your phone, which admittedly,
[00:47:36.040 --> 00:47:38.760]   with an 888 in there is a pretty powerful device.
[00:47:38.760 --> 00:47:40.920]   - Yeah, I mean, it works and mostly I just thought
[00:47:40.920 --> 00:47:43.560]   in that one. - Like really fast file transfer, no,
[00:47:43.560 --> 00:47:47.040]   but you know, so I mean, it works enough,
[00:47:47.040 --> 00:47:52.960]   but some of these central challenges still persist
[00:47:52.960 --> 00:47:54.640]   and I think beyond just like being able
[00:47:54.640 --> 00:47:58.080]   to open up a document, which is harder than it should be,
[00:47:58.080 --> 00:48:01.480]   still, these are communication devices.
[00:48:01.480 --> 00:48:03.480]   We should be able to communicate the same way
[00:48:03.480 --> 00:48:04.600]   on all of them.
[00:48:04.600 --> 00:48:06.680]   You know, I have plenty of friends with iPhone,
[00:48:06.680 --> 00:48:10.680]   so no, I don't use Messenger on my, by Android.
[00:48:11.320 --> 00:48:13.360]   But I can run a system, you know,
[00:48:13.360 --> 00:48:16.680]   I can sort of couple together and make things work.
[00:48:16.680 --> 00:48:18.800]   - So are you saying the market should do this,
[00:48:18.800 --> 00:48:21.640]   not the house?
[00:48:21.640 --> 00:48:25.560]   - What I'm saying is I'm wondering if some of the concerns
[00:48:25.560 --> 00:48:28.200]   that regulators are talking about
[00:48:28.200 --> 00:48:31.800]   and people who are funneling up the pipeline,
[00:48:31.800 --> 00:48:33.080]   what they're really talking about
[00:48:33.080 --> 00:48:35.560]   is their frustration or an interoperability.
[00:48:35.560 --> 00:48:36.840]   Not always just anti, I mean,
[00:48:36.840 --> 00:48:39.960]   some of this is clearly anti-competitive-ness, but.
[00:48:39.960 --> 00:48:42.440]   - I think there's a lot of it that is definitely not
[00:48:42.440 --> 00:48:43.760]   about interoperability.
[00:48:43.760 --> 00:48:46.080]   I mean, look at, you know,
[00:48:46.080 --> 00:48:49.240]   one of the things like Amazon, Facebook have a real tendency
[00:48:49.240 --> 00:48:51.680]   to gobble up an incompatible that's out there.
[00:48:51.680 --> 00:48:54.240]   That's not like a data transfer issue.
[00:48:54.240 --> 00:48:57.520]   That's a monopoly issue.
[00:48:57.520 --> 00:49:01.760]   And I also had to say there's a, you know,
[00:49:01.760 --> 00:49:04.440]   every time you do that, you are,
[00:49:04.440 --> 00:49:07.360]   you're opening up privacy issues, right?
[00:49:07.360 --> 00:49:09.880]   Like, yeah, something that I think many people
[00:49:09.880 --> 00:49:12.400]   have thought about is opening up all the Facebook data
[00:49:12.400 --> 00:49:16.240]   to competitors so you could have more people using that data
[00:49:16.240 --> 00:49:18.640]   and allow Facebook competitors to come along.
[00:49:18.640 --> 00:49:20.720]   That also would be a privacy apocalypse.
[00:49:20.720 --> 00:49:23.440]   Just to give you a really specific example,
[00:49:23.440 --> 00:49:27.200]   you know, Rebellion Pack is making a really big data play
[00:49:27.200 --> 00:49:28.120]   right now.
[00:49:28.120 --> 00:49:32.960]   We are getting in and using hyper-micro-targeting
[00:49:32.960 --> 00:49:36.720]   in a way that just political people don't have a way to do it.
[00:49:36.720 --> 00:49:38.840]   - Hyper-micro-targeting.
[00:49:38.840 --> 00:49:41.640]   - I'm, so let me give you a very specific example.
[00:49:41.640 --> 00:49:42.480]   - I love it.
[00:49:42.480 --> 00:49:44.920]   - We have the voter file over here.
[00:49:44.920 --> 00:49:48.320]   So I can go and find people that just registered to vote
[00:49:48.320 --> 00:49:50.360]   in order to vote Trump out of office.
[00:49:50.360 --> 00:49:54.360]   And I can find people that based on their location data
[00:49:54.360 --> 00:49:58.560]   have spent a lot of time in hospitals or healthcare
[00:49:58.560 --> 00:50:01.520]   or red articles on how to file for bankruptcy.
[00:50:01.520 --> 00:50:04.880]   And then I can show them ads on universal healthcare.
[00:50:04.880 --> 00:50:06.920]   That's how targeted it is.
[00:50:06.920 --> 00:50:08.640]   Like that's so much information.
[00:50:08.640 --> 00:50:10.840]   - The voting information is public record.
[00:50:10.840 --> 00:50:11.680]   How do you get that?
[00:50:11.680 --> 00:50:13.000]   - You have to pay a ton for that.
[00:50:13.000 --> 00:50:14.200]   Oh my God, do you?
[00:50:14.200 --> 00:50:16.120]   - But it is public record, right?
[00:50:16.120 --> 00:50:16.960]   I mean--
[00:50:16.960 --> 00:50:18.800]   - It's a public record but getting access to the internet.
[00:50:18.800 --> 00:50:19.640]   - Somebody has--
[00:50:19.640 --> 00:50:22.200]   - Yeah, somebody has to go to the courthouse
[00:50:22.200 --> 00:50:25.040]   and get it and collate it and put it online.
[00:50:25.040 --> 00:50:25.880]   So that's what you're able to--
[00:50:25.880 --> 00:50:27.720]   - And the Democratic Party and Republican parties
[00:50:27.720 --> 00:50:29.320]   have virtual monopolies on that.
[00:50:29.320 --> 00:50:30.880]   - What about the medical information?
[00:50:30.880 --> 00:50:32.200]   Where are you getting that from?
[00:50:32.200 --> 00:50:33.040]   Is that Facebook?
[00:50:33.040 --> 00:50:34.800]   - You get it from third party data brokers.
[00:50:34.800 --> 00:50:37.600]   You know, they get all these different information--
[00:50:37.600 --> 00:50:38.440]   - Would you do this?
[00:50:38.440 --> 00:50:40.680]   Would you feel okay doing that?
[00:50:40.680 --> 00:50:41.520]   - Yeah, I do.
[00:50:41.520 --> 00:50:43.400]   - Spying those ads that way, targeting them.
[00:50:43.400 --> 00:50:45.560]   - Yeah, I mean, I'm trying to show people
[00:50:45.560 --> 00:50:48.800]   information as relevant to their voting interests.
[00:50:48.800 --> 00:50:49.640]   My point is--
[00:50:49.640 --> 00:50:52.080]   - So you understand that people are,
[00:50:52.080 --> 00:50:54.200]   this is really interesting because a lot of the people
[00:50:54.200 --> 00:50:55.400]   that you're trying to reach,
[00:50:55.400 --> 00:50:58.840]   a lot of these voters are also very concerned about privacy.
[00:50:58.840 --> 00:50:59.680]   - Absolutely.
[00:50:59.680 --> 00:51:01.480]   - And they're being hyper-micro-targeted.
[00:51:01.480 --> 00:51:04.720]   That's interesting.
[00:51:04.720 --> 00:51:06.520]   - I understand the value of it.
[00:51:06.520 --> 00:51:08.600]   - I understand the value of it.
[00:51:08.600 --> 00:51:11.960]   - It is a gut check situation, Leo,
[00:51:11.960 --> 00:51:14.240]   but at the same time, you know,
[00:51:14.240 --> 00:51:18.400]   I think the side I happen to be on is losing in a lot of ways.
[00:51:18.400 --> 00:51:20.480]   And I think we have to fight with the same tools
[00:51:20.480 --> 00:51:21.480]   the other people are.
[00:51:21.480 --> 00:51:23.080]   - Right.
[00:51:23.080 --> 00:51:25.240]   And of course, businesses too,
[00:51:25.240 --> 00:51:29.160]   using the same tools with the same kind of rationale behind it.
[00:51:29.160 --> 00:51:30.720]   - Absolutely.
[00:51:30.720 --> 00:51:34.800]   This is by such a difficult and nuanced conversation
[00:51:34.800 --> 00:51:35.640]   because we talk,
[00:51:35.640 --> 00:51:38.520]   if you talk about privacy abstractly,
[00:51:38.520 --> 00:51:39.520]   everybody's for it.
[00:51:39.520 --> 00:51:41.560]   You know, everybody loves the idea that Apple's giving you
[00:51:41.560 --> 00:51:43.680]   the choice whether you should be tracked or not
[00:51:43.680 --> 00:51:45.880]   in the, when you're installing app.
[00:51:45.880 --> 00:51:48.200]   But there is another side of that equation,
[00:51:48.200 --> 00:51:50.360]   which is, what do you think,
[00:51:50.360 --> 00:51:52.040]   would it be better, Brianna,
[00:51:52.040 --> 00:51:56.320]   if all hyper-micro-targeting were eliminated,
[00:51:56.320 --> 00:51:59.440]   then you're all on a level playing field.
[00:51:59.440 --> 00:52:01.040]   Would you prefer that?
[00:52:01.040 --> 00:52:02.520]   - I don't know if you could do it.
[00:52:02.520 --> 00:52:05.160]   I mean, can you legally stop someone
[00:52:05.160 --> 00:52:08.840]   from crossing one database with another database?
[00:52:08.840 --> 00:52:12.040]   - That's actually to me of all the big tech regulation.
[00:52:12.040 --> 00:52:14.640]   I think the best one to focus on
[00:52:14.640 --> 00:52:16.080]   is not security privacy,
[00:52:16.080 --> 00:52:19.880]   but it's to focus on data control of your data.
[00:52:19.880 --> 00:52:20.720]   - Yeah.
[00:52:20.720 --> 00:52:23.320]   - That to me seems like an area
[00:52:23.320 --> 00:52:26.200]   maybe we could do something about.
[00:52:26.200 --> 00:52:27.040]   - Yeah.
[00:52:27.040 --> 00:52:29.200]   I think there are a lot of critiques of Andrew Yang,
[00:52:29.200 --> 00:52:30.800]   but I've never seen a politician
[00:52:30.800 --> 00:52:32.280]   talking about owning your own data
[00:52:32.280 --> 00:52:33.800]   and being able to monetize it
[00:52:33.800 --> 00:52:35.880]   'cause I can promise you other people are.
[00:52:35.880 --> 00:52:37.720]   You know, you actually--
[00:52:37.720 --> 00:52:38.960]   - Yang is interested in that.
[00:52:38.960 --> 00:52:40.480]   - He's very much so.
[00:52:40.480 --> 00:52:41.320]   - Yeah.
[00:52:41.320 --> 00:52:47.320]   The FTC did vote this week to expand their enforcement powers
[00:52:47.320 --> 00:52:49.960]   three to two along party lines
[00:52:49.960 --> 00:52:53.400]   to repeal a statement policy statement from 2015
[00:52:53.400 --> 00:52:56.160]   that said they couldn't challenge unfair.
[00:52:56.160 --> 00:52:59.120]   Now, I don't know why in 2015 they agreed to this.
[00:52:59.120 --> 00:53:02.240]   That they couldn't challenge unfair methods of competition
[00:53:02.240 --> 00:53:06.160]   if they don't violate existing antitrust laws.
[00:53:06.160 --> 00:53:07.400]   So I guess the commission said,
[00:53:07.400 --> 00:53:09.240]   "Look, there's gotta be a law against it
[00:53:09.240 --> 00:53:11.520]   "before we can investigate it or challenge it."
[00:53:11.520 --> 00:53:12.400]   I guess.
[00:53:12.400 --> 00:53:16.520]   Lena Kahn, this is one of her first actions,
[00:53:16.520 --> 00:53:18.280]   is chair of the FTC,
[00:53:18.280 --> 00:53:21.800]   said the 2015 statement doubled down
[00:53:21.800 --> 00:53:24.320]   in the agency's longstanding failure
[00:53:24.320 --> 00:53:28.920]   to investigate and pursue unfair methods of competition.
[00:53:28.920 --> 00:53:32.280]   The guidance only hindered the agency's enforcement efforts.
[00:53:32.280 --> 00:53:38.640]   Certainly, definitely a statement by the FTC.
[00:53:38.640 --> 00:53:41.440]   No, no, we're gonna start getting much more active.
[00:53:41.440 --> 00:53:42.880]   This is the same FTC, of course,
[00:53:42.880 --> 00:53:44.520]   that allowed Facebook to buy Instagram
[00:53:44.520 --> 00:53:47.560]   and WhatsApp without any noise
[00:53:47.560 --> 00:53:49.880]   and is now saying, well, they shouldn't have been able to.
[00:53:49.880 --> 00:53:51.440]   And Facebook quite rightly is saying,
[00:53:51.440 --> 00:53:52.600]   "Well, you let us."
[00:53:52.600 --> 00:53:56.440]   You can't unwind that, you gave us permission.
[00:53:56.440 --> 00:53:57.440]   We did ask.
[00:53:58.400 --> 00:54:00.720]   Part of the problem is you've got different administration
[00:54:00.720 --> 00:54:01.880]   cycling people through these.
[00:54:01.880 --> 00:54:03.160]   It changes too fast.
[00:54:03.160 --> 00:54:04.000]   Yeah.
[00:54:04.000 --> 00:54:06.480]   You could say a lot of the same stuff about the FCC.
[00:54:06.480 --> 00:54:11.520]   In the United States, our elected officials
[00:54:11.520 --> 00:54:13.560]   have the ability to appoint people
[00:54:13.560 --> 00:54:14.880]   the heads of these agencies
[00:54:14.880 --> 00:54:17.280]   and these agencies have some on that pattern.
[00:54:17.280 --> 00:54:21.040]   I should point out this was an Obama era guidance, however.
[00:54:21.040 --> 00:54:23.400]   So I don't know.
[00:54:23.400 --> 00:54:24.880]   I don't know.
[00:54:24.880 --> 00:54:29.880]   The FTC did have a successful action this week against Broadcom.
[00:54:29.880 --> 00:54:31.160]   They had accused Broadcom
[00:54:31.160 --> 00:54:34.400]   of monopolizing the market for semiconductor components.
[00:54:34.400 --> 00:54:40.000]   There is a consent decree and the Qualcomm has,
[00:54:40.000 --> 00:54:42.840]   without admitting guilt, of course,
[00:54:42.840 --> 00:54:46.360]   agreed to a settlement.
[00:54:46.360 --> 00:54:49.480]   So they had been charges were filed,
[00:54:49.480 --> 00:54:52.360]   or at least the FTC had voted to file charges.
[00:54:52.360 --> 00:54:57.360]   And now Broadcom said, "Okay, let's work this out."
[00:54:57.360 --> 00:55:01.280]   They settled in October with the European Commission.
[00:55:01.280 --> 00:55:02.880]   What was Broadcom doing?
[00:55:02.880 --> 00:55:06.200]   Is this 5G?
[00:55:06.200 --> 00:55:08.480]   Is it wireless?
[00:55:08.480 --> 00:55:10.880]   Yeah, it sounded like they were buying up all the components
[00:55:10.880 --> 00:55:14.520]   to basically produce their modem technology.
[00:55:14.520 --> 00:55:16.760]   Yeah, FTC said and released Broadcom
[00:55:16.760 --> 00:55:19.040]   as one of the few significant suppliers
[00:55:19.040 --> 00:55:22.360]   of five related types of chips.
[00:55:22.360 --> 00:55:25.440]   They made long-term agreements with at least 10 OEMs
[00:55:25.440 --> 00:55:28.240]   that make set top boxes and broadband devices,
[00:55:28.240 --> 00:55:31.760]   preventing those OEMs from buying chips from anybody else.
[00:55:31.760 --> 00:55:32.960]   You can't do that.
[00:55:32.960 --> 00:55:36.360]   And they got caught.
[00:55:36.360 --> 00:55:39.920]   And so they're not gonna be allowed to do that any further.
[00:55:39.920 --> 00:55:41.480]   Broadcom used to be an American company,
[00:55:41.480 --> 00:55:45.160]   but I think it is no longer American.
[00:55:45.160 --> 00:55:47.720]   I feel like I remember that they were bought
[00:55:47.720 --> 00:55:49.160]   by a Chinese company.
[00:55:49.160 --> 00:55:54.000]   Well, I think this kind of access to chips,
[00:55:54.000 --> 00:55:57.600]   it's just gonna get to be more and more of a problem.
[00:55:57.600 --> 00:55:59.800]   And I think you can expect these monopsies
[00:55:59.800 --> 00:56:02.080]   that companies like Apple create.
[00:56:02.080 --> 00:56:03.440]   I think you can expect it to be
[00:56:03.440 --> 00:56:06.480]   under further and further scrutiny moving forward.
[00:56:06.480 --> 00:56:07.960]   I mean, the reason,
[00:56:07.960 --> 00:56:10.280]   Leah, I love to collect old cars
[00:56:10.280 --> 00:56:12.680]   and every single one of my collectable cars
[00:56:12.680 --> 00:56:14.560]   is worth a ton of money right now
[00:56:14.560 --> 00:56:17.040]   'cause the used market is so hot
[00:56:17.040 --> 00:56:19.920]   because you can't buy new cars with the chip shortage.
[00:56:19.920 --> 00:56:24.920]   So I think more equal access to the things you used
[00:56:24.920 --> 00:56:27.640]   to build a high technology,
[00:56:27.640 --> 00:56:28.760]   I think very correctly,
[00:56:28.760 --> 00:56:30.640]   that should be under more scrutiny.
[00:56:30.640 --> 00:56:31.480]   - I apologize.
[00:56:31.480 --> 00:56:32.520]   Broadcom, which was a US company,
[00:56:32.520 --> 00:56:35.960]   was bought by a Singaporean company called Avago.
[00:56:35.960 --> 00:56:40.520]   So they are owned by Singaporean company at this point.
[00:56:40.520 --> 00:56:46.400]   I would pause at this moment for a commercial,
[00:56:46.400 --> 00:56:48.600]   but we don't have any 'cause it's the 4th of July
[00:56:48.600 --> 00:56:51.360]   and nobody tried to sell this show.
[00:56:51.360 --> 00:56:52.240]   (laughs)
[00:56:52.240 --> 00:56:56.720]   So good news, you guys gotta sit here and take it.
[00:56:56.720 --> 00:56:59.800]   No break, but I guess I could say,
[00:56:59.800 --> 00:57:03.480]   this episode of Twit is brought to you by listeners like you,
[00:57:03.480 --> 00:57:04.680]   members of Club Twit,
[00:57:04.680 --> 00:57:08.880]   who are helping us survive a bit of a downturn
[00:57:08.880 --> 00:57:10.440]   in the economy over the last year.
[00:57:10.440 --> 00:57:12.960]   Club Twit is a way that you could show your support
[00:57:12.960 --> 00:57:16.320]   for everything we do, $7 a month.
[00:57:16.320 --> 00:57:19.200]   It gets you not only ad-free versions of all of our shows,
[00:57:19.200 --> 00:57:21.960]   in effect, this is the Club Twit version of the show.
[00:57:21.960 --> 00:57:23.640]   You're getting a listen to what it would be like
[00:57:23.640 --> 00:57:24.920]   without any ads.
[00:57:24.920 --> 00:57:26.640]   Audio and video of all of our shows,
[00:57:26.640 --> 00:57:28.600]   you also get a Twit Plus feed,
[00:57:28.600 --> 00:57:32.600]   which has some good stuff from before and after the show's
[00:57:32.600 --> 00:57:35.240]   special episodes of things.
[00:57:35.240 --> 00:57:38.280]   We have a Linux show we're doing on Club Twit Plus feed,
[00:57:38.280 --> 00:57:39.120]   so forth.
[00:57:39.120 --> 00:57:40.680]   And then finally, you get access to our members
[00:57:40.680 --> 00:57:45.280]   only Discord, which is a lovely place to hang out.
[00:57:45.280 --> 00:57:47.560]   If you haven't tried Discord,
[00:57:47.560 --> 00:57:48.920]   this is a great way to get to know it.
[00:57:48.920 --> 00:57:51.000]   If you're already a user of Discord,
[00:57:51.000 --> 00:57:53.720]   (laughs)
[00:57:53.720 --> 00:57:57.640]   you'll love adding Club Twit to your Discord channels.
[00:57:57.640 --> 00:58:02.640]   We have discussion sections for all of the shows,
[00:58:02.640 --> 00:58:07.480]   plus conversations about every topic under the sun,
[00:58:07.480 --> 00:58:10.960]   loved by Geeks, books, data science, gaming, hacking,
[00:58:12.000 --> 00:58:17.000]   sci-fi travel, and that's where I play my Valheim
[00:58:17.000 --> 00:58:19.960]   in the club.
[00:58:19.960 --> 00:58:21.200]   It's a lot of fun.
[00:58:21.200 --> 00:58:22.560]   If you wanna know more about Club Twit,
[00:58:22.560 --> 00:58:27.160]   twit.tv/clubtwit, we really thank you
[00:58:27.160 --> 00:58:29.880]   for all the Club Twit numbers for your support,
[00:58:29.880 --> 00:58:31.160]   for everything we're doing.
[00:58:31.160 --> 00:58:36.400]   - Hey, Leo, will there be on Discord for Club Twit?
[00:58:36.400 --> 00:58:39.160]   Will there be a foundation channel?
[00:58:39.160 --> 00:58:40.600]   - Oh, are you excited?
[00:58:40.600 --> 00:58:41.440]   - Ooh.
[00:58:41.440 --> 00:58:42.280]   - Am I excited?
[00:58:42.280 --> 00:58:43.880]   Have I been waiting my entire life
[00:58:43.880 --> 00:58:46.680]   to see my favorite book ever written,
[00:58:46.680 --> 00:58:48.560]   finally made into something watchable?
[00:58:48.560 --> 00:58:51.480]   - Apple TV's doing it, so they'll be putting
[00:58:51.480 --> 00:58:53.640]   a lot of money into it.
[00:58:53.640 --> 00:58:56.000]   - The trailers look pretty amazing.
[00:58:56.000 --> 00:58:59.000]   - It's coming, we now know September 24th
[00:58:59.000 --> 00:59:00.520]   will be the debut.
[00:59:00.520 --> 00:59:03.000]   - Yeah, now I'll say the trailer that I saw
[00:59:03.000 --> 00:59:06.000]   had a little bit less emphasis on Selden than--
[00:59:06.000 --> 00:59:09.240]   - I think, yeah, you're a Harry Selden.
[00:59:09.240 --> 00:59:11.920]   Is that how you got the inspiration to be a futurist?
[00:59:11.920 --> 00:59:14.200]   - Well, I'm alive as far as I know.
[00:59:14.200 --> 00:59:15.400]   (laughing)
[00:59:15.400 --> 00:59:16.400]   - Unlike Harry.
[00:59:16.400 --> 00:59:22.200]   But that's a big part of the whole novel is,
[00:59:22.200 --> 00:59:26.240]   Harry Selden was able to kind of predict
[00:59:26.240 --> 00:59:29.240]   through scientific means, not clear--
[00:59:29.240 --> 00:59:31.720]   - Science and psychological means.
[00:59:31.720 --> 00:59:33.840]   - What do they call them, psycho historians, I think?
[00:59:33.840 --> 00:59:35.320]   - Yeah. - Yeah.
[00:59:35.320 --> 00:59:37.120]   - It was the '60s, you know.
[00:59:37.120 --> 00:59:39.960]   - Well, okay, so this is my quam about the whole thing.
[00:59:39.960 --> 00:59:42.320]   First of all, have you reread the novels lately?
[00:59:42.320 --> 00:59:44.640]   - No. - I have not.
[00:59:44.640 --> 00:59:46.240]   - I haven't either, and I'm a little
[00:59:46.240 --> 00:59:48.000]   a feared of doing so.
[00:59:48.000 --> 00:59:51.480]   A lot of the science fiction that I loved as a kid
[00:59:51.480 --> 00:59:54.040]   when I reread it does not age well.
[00:59:54.040 --> 00:59:54.880]   (laughing)
[00:59:54.880 --> 00:59:56.160]   - Highlight stuff, for instance.
[00:59:56.160 --> 00:59:57.400]   - I love highlight.
[00:59:57.400 --> 01:00:00.480]   - I love it too, but it does an age well.
[01:00:00.480 --> 01:00:03.240]   And I'm afraid the foundation,
[01:00:03.240 --> 01:00:05.080]   I don't know if it's gonna age well,
[01:00:05.080 --> 01:00:07.720]   then the other quam I have is this is way glitzier
[01:00:07.720 --> 01:00:09.480]   than I think it should be.
[01:00:09.480 --> 01:00:11.280]   Where, I mean, it just--
[01:00:11.280 --> 01:00:14.840]   - Well, I mean, it spans a thousand years, so there's that.
[01:00:14.840 --> 01:00:18.240]   - I feel like it shouldn't look like Star Wars,
[01:00:18.240 --> 01:00:19.080]   if you know what I mean.
[01:00:19.080 --> 01:00:19.920]   - I don't think it looks like Star Wars.
[01:00:19.920 --> 01:00:21.760]   I think it looks more a little bit more like the,
[01:00:21.760 --> 01:00:23.440]   I mean, if anything, it looks a little bit more
[01:00:23.440 --> 01:00:25.120]   duney to me. - Yeah.
[01:00:25.120 --> 01:00:26.720]   - Yeah. - Well, that's also a concern.
[01:00:26.720 --> 01:00:28.320]   - Except for that part that you're showing right now,
[01:00:28.320 --> 01:00:29.880]   which does look a little more-- - It does look exactly
[01:00:29.880 --> 01:00:32.160]   like stormtroopers, but okay.
[01:00:32.160 --> 01:00:33.840]   - Lee Pace was a good casting choice.
[01:00:33.840 --> 01:00:35.280]   I don't know, whatever, let's not go.
[01:00:35.280 --> 01:00:36.640]   - Are you excited? - Are you excited?
[01:00:36.640 --> 01:00:38.120]   - Let's let my dream live for a while.
[01:00:38.120 --> 01:00:39.880]   - The dream will live, you know what?
[01:00:39.880 --> 01:00:43.320]   We'll have you all back on on the 25th and you can--
[01:00:43.320 --> 01:00:44.680]   - I love it. - Let us know.
[01:00:44.680 --> 01:00:47.000]   I do love him, he is wonderful.
[01:00:47.000 --> 01:00:49.320]   Is he Harry Sullen, who is he is he gonna be?
[01:00:49.320 --> 01:00:52.240]   - Who, Lee Pace? - Yeah.
[01:00:52.240 --> 01:00:53.960]   - No, he's not. - Jared Harris,
[01:00:53.960 --> 01:00:54.880]   that's who I was thinking of.
[01:00:54.880 --> 01:00:56.880]   I like Jared Harris. - Yeah.
[01:00:56.880 --> 01:00:59.760]   - I feel like Heinlein is my feminist guilty pleasure.
[01:00:59.760 --> 01:01:01.240]   - Yeah, it should be. - I love it so much.
[01:01:01.240 --> 01:01:03.120]   - There's most sexist books ever written.
[01:01:03.120 --> 01:01:05.120]   - It's so sexist. - All the women's are--
[01:01:05.120 --> 01:01:07.240]   - All the women are buxom and beautiful.
[01:01:07.240 --> 01:01:10.680]   - Right, it is so belittlic, but he also says so much
[01:01:10.680 --> 01:01:14.280]   about human nature that is profound and interesting
[01:01:14.280 --> 01:01:17.840]   and fascinating and he's a libertarian.
[01:01:17.840 --> 01:01:21.800]   Like what I love about Heinlein is he's putting ideas
[01:01:21.800 --> 01:01:23.920]   out there that I don't agree with,
[01:01:23.920 --> 01:01:26.920]   but he's truly challenging your mind and making you go,
[01:01:26.920 --> 01:01:28.280]   why don't I agree with it?
[01:01:28.280 --> 01:01:31.320]   'Cause he's putting such a good argument forward.
[01:01:31.320 --> 01:01:33.000]   - That's what I try to do with Anne Rand
[01:01:33.000 --> 01:01:34.040]   as well as well. - Yeah.
[01:01:34.040 --> 01:01:36.800]   - When I love her writing and I have to admit,
[01:01:36.800 --> 01:01:40.800]   I love her books completely disagree with the premise.
[01:01:40.800 --> 01:01:42.480]   - Yeah. - And you're right,
[01:01:42.480 --> 01:01:44.600]   it forces you to say, well, why do I disagree?
[01:01:44.600 --> 01:01:46.040]   Why do I, why do I--
[01:01:46.040 --> 01:01:47.840]   - I have to tell you, I was writing with Frank
[01:01:47.840 --> 01:01:51.080]   and we were listening to a book called "Sixth Column"
[01:01:51.080 --> 01:01:56.080]   which has more anti-Asian slurs than we could possibly imagine.
[01:01:56.080 --> 01:02:00.360]   It's almost like it was so painful
[01:02:00.360 --> 01:02:02.000]   and it's such a good moment at the same time.
[01:02:02.000 --> 01:02:04.200]   - Well, right, yeah. - Yeah, that's timeline.
[01:02:04.200 --> 01:02:07.920]   - Okay, foundation, no, we're excited.
[01:02:07.920 --> 01:02:09.160]   We're excited. - Good.
[01:02:09.160 --> 01:02:10.480]   - It's gonna be good. - I'm excited.
[01:02:10.480 --> 01:02:13.000]   We got Dune coming out, finally, maybe.
[01:02:13.000 --> 01:02:16.280]   - Dune should have been a mini-series, not a movie.
[01:02:16.280 --> 01:02:19.880]   - Dune is impossible to make a movie, so we will see.
[01:02:19.880 --> 01:02:21.480]   - Yeah, sorry. - Dune is 20.
[01:02:21.480 --> 01:02:23.000]   - They're like doing it in two parts.
[01:02:23.000 --> 01:02:25.800]   - Yeah, no, no, sorry.
[01:02:25.800 --> 01:02:28.680]   This is what the opportunity is now
[01:02:28.680 --> 01:02:31.160]   is you could take novels like Foundation
[01:02:31.160 --> 01:02:33.880]   and spread them out over many, many more hours.
[01:02:33.880 --> 01:02:37.480]   So you get more of the flavor of it.
[01:02:37.480 --> 01:02:39.800]   And I feel like any attempt, and there have been, what,
[01:02:39.800 --> 01:02:42.520]   two now to make Dune into a movie that would be the third.
[01:02:42.520 --> 01:02:43.840]   - Oh, no, I think it's been more than that.
[01:02:43.840 --> 01:02:44.680]   - More than that. - Yeah.
[01:02:44.680 --> 01:02:47.200]   - Dumed, just because it's too short.
[01:02:47.200 --> 01:02:49.640]   - It was the source materials we're to start with.
[01:02:49.640 --> 01:02:50.600]   I mean, I'll hold, you know.
[01:02:50.600 --> 01:02:54.600]   - Yeah, it always ends up, Dune ends up being campy.
[01:02:54.600 --> 01:02:56.160]   You can't help it. - Yeah.
[01:02:56.160 --> 01:02:58.400]   - Yeah, can we just like take a second
[01:02:58.400 --> 01:03:01.840]   and met Apple TV content has been way better
[01:03:01.840 --> 01:03:04.200]   than it has any business being.
[01:03:04.200 --> 01:03:06.520]   Like, Lacey's story is out right now.
[01:03:06.520 --> 01:03:09.440]   That's an adaptation of a Stephen King novel.
[01:03:09.440 --> 01:03:10.520]   That is excellent. - Is it?
[01:03:10.520 --> 01:03:11.880]   - The morning show is excellent.
[01:03:11.880 --> 01:03:14.160]   - I didn't think morning show was excellent.
[01:03:14.160 --> 01:03:15.480]   - No. - I have felt
[01:03:15.480 --> 01:03:18.040]   that it's less than it should have been.
[01:03:18.040 --> 01:03:20.680]   Ted Lasser was pretty good. - Yeah.
[01:03:20.680 --> 01:03:25.920]   - I haven't watched Lacey's story, so I'll have to watch that.
[01:03:27.360 --> 01:03:29.280]   Mosquito Coast, have you seen that?
[01:03:29.280 --> 01:03:32.120]   - None of these really draw me.
[01:03:32.120 --> 01:03:33.560]   That's the problem. - Oh, fair enough.
[01:03:33.560 --> 01:03:34.800]   - I'm not just a fan. - Fair enough.
[01:03:34.800 --> 01:03:38.120]   - C was terrible. - Oh, that's horrible.
[01:03:38.120 --> 01:03:40.080]   - I'm told physical is good.
[01:03:40.080 --> 01:03:41.440]   This is a brand new show.
[01:03:41.440 --> 01:03:42.880]   Have you watched that?
[01:03:42.880 --> 01:03:43.960]   - I haven't seen it yet. - I haven't seen it.
[01:03:43.960 --> 01:03:46.560]   - I'm saving it with Frank to watch it.
[01:03:46.560 --> 01:03:47.400]   - Okay.
[01:03:47.400 --> 01:03:52.000]   Although the elevator pitch is horrible,
[01:03:52.000 --> 01:03:55.920]   Sheila Rubin is a quietly-termated housewife
[01:03:55.920 --> 01:04:00.560]   in 80s San Diego who discovers aerobics,
[01:04:00.560 --> 01:04:02.000]   which changes her life.
[01:04:02.000 --> 01:04:05.360]   Okay, sorry. (laughs)
[01:04:05.360 --> 01:04:06.600]   I don't know.
[01:04:06.600 --> 01:04:08.400]   That is not a great pitch.
[01:04:08.400 --> 01:04:10.400]   Is aerobics changing her life?
[01:04:10.400 --> 01:04:11.240]   Is it kind of?
[01:04:11.240 --> 01:04:12.080]   I don't know.
[01:04:12.080 --> 01:04:13.400]   We'll find out.
[01:04:13.400 --> 01:04:15.040]   It is time for 80s shows though.
[01:04:15.040 --> 01:04:16.200]   See this?
[01:04:16.200 --> 01:04:18.600]   We had the 70s. - Yeah, glow was better
[01:04:18.600 --> 01:04:20.720]   than I think any of us thought it was gonna be.
[01:04:20.720 --> 01:04:22.120]   - Glow was great. - Glow was red.
[01:04:22.120 --> 01:04:23.840]   - But we'll give it the 80s chance.
[01:04:23.840 --> 01:04:25.960]   - Yeah, glow was 80s, wasn't it?
[01:04:25.960 --> 01:04:27.560]   Yeah. - Yeah.
[01:04:27.560 --> 01:04:29.520]   - Leo, I wonder what is,
[01:04:29.520 --> 01:04:31.720]   since we're talking about everything,
[01:04:31.720 --> 01:04:35.880]   what's the one untold story from the world of tech
[01:04:35.880 --> 01:04:38.800]   that you think deserves to be made into a movie or a show?
[01:04:38.800 --> 01:04:40.360]   - I was just thinking this the other day
[01:04:40.360 --> 01:04:42.480]   that it really should be.
[01:04:42.480 --> 01:04:45.120]   So there are a number of really good books,
[01:04:45.120 --> 01:04:47.960]   and actually, Brianna, you know these books, I'm sure, about--
[01:04:47.960 --> 01:04:49.000]   - Or it doesn't have to be a book.
[01:04:49.000 --> 01:04:50.240]   It could just be like something--
[01:04:50.240 --> 01:04:52.680]   - Well, it's gotta start with some source material.
[01:04:53.680 --> 01:04:55.440]   So there have been a number of good books
[01:04:55.440 --> 01:04:57.840]   about game development.
[01:04:57.840 --> 01:05:00.480]   What is the book about id?
[01:05:00.480 --> 01:05:01.520]   That would be a great movie.
[01:05:01.520 --> 01:05:03.640]   - Masters of Doom. - Masters of Doom.
[01:05:03.640 --> 01:05:08.400]   - Brila Carmen, she's a good untold story from tech.
[01:05:08.400 --> 01:05:10.760]   Like something crazy that happened at CES
[01:05:10.760 --> 01:05:12.160]   in the earlier days.
[01:05:12.160 --> 01:05:13.320]   (laughing)
[01:05:13.320 --> 01:05:14.160]   You know what I mean?
[01:05:14.160 --> 01:05:15.040]   Like what's like something that absolutely--
[01:05:15.040 --> 01:05:16.680]   - No, we can't tell those stories
[01:05:16.680 --> 01:05:19.720]   because people are still alive.
[01:05:19.720 --> 01:05:24.360]   We have to wait until some people are no longer with us
[01:05:24.360 --> 01:05:27.520]   before those stories can be told.
[01:05:27.520 --> 01:05:29.280]   I don't think--
[01:05:29.280 --> 01:05:31.960]   - Do you think G4, G4 could be a really interesting story.
[01:05:31.960 --> 01:05:33.880]   I've heard some of the drama there.
[01:05:33.880 --> 01:05:35.400]   You don't think?
[01:05:35.400 --> 01:05:36.240]   - Well, I don't know.
[01:05:36.240 --> 01:05:37.400]   I'm too close to it to really--
[01:05:37.400 --> 01:05:38.240]   - Yeah, yeah.
[01:05:38.240 --> 01:05:42.400]   - But I do think the hidden life of Bill Gates,
[01:05:42.400 --> 01:05:43.600]   I think we're starting to learn,
[01:05:43.600 --> 01:05:48.240]   might be kind of an intriguing mini series.
[01:05:48.240 --> 01:05:51.280]   He certainly had a lot more going on than we really realize.
[01:05:51.280 --> 01:05:54.120]   Don't you think?
[01:05:54.120 --> 01:05:57.280]   There's also a redemption arc,
[01:05:57.280 --> 01:05:58.640]   which you have to have, right?
[01:05:58.640 --> 01:06:03.280]   To make a good story where Evil Guy turns into a nice guy
[01:06:03.280 --> 01:06:05.760]   thanks to life experiences.
[01:06:05.760 --> 01:06:07.040]   And then turns back into the evil guy.
[01:06:07.040 --> 01:06:08.040]   - Have you read anything he's meant to do?
[01:06:08.040 --> 01:06:09.760]   - And then turns back into the evil guy
[01:06:09.760 --> 01:06:12.520]   and then maybe there'll be redemption down the road.
[01:06:12.520 --> 01:06:14.320]   - What was the name of that movie that was great?
[01:06:14.320 --> 01:06:15.520]   The Pirates--
[01:06:15.520 --> 01:06:17.720]   - Pirates of Silicon Valley was the only good--
[01:06:17.720 --> 01:06:20.880]   - The only good movie ever made about the--
[01:06:20.880 --> 01:06:22.560]   - Factual movie ever made.
[01:06:22.560 --> 01:06:26.720]   I think Silicon Valley, the TV show, was a good non.
[01:06:26.720 --> 01:06:28.920]   Somebody's talking to me, is it my phone?
[01:06:28.920 --> 01:06:30.520]   Whose phone is answering?
[01:06:30.520 --> 01:06:31.680]   That's in the call.
[01:06:31.680 --> 01:06:34.160]   How about Elon Musk?
[01:06:34.160 --> 01:06:35.880]   There's a story.
[01:06:35.880 --> 01:06:37.800]   His life story, is there?
[01:06:37.800 --> 01:06:41.480]   - Is he, is he really that compelling?
[01:06:41.480 --> 01:06:46.720]   - Well, he's doing a good impression of it if he's not.
[01:06:46.720 --> 01:06:47.920]   (laughing)
[01:06:47.920 --> 01:06:52.920]   - I mean, I would rather watch something on Bezos
[01:06:52.920 --> 01:06:54.360]   than on Elon Musk.
[01:06:54.360 --> 01:06:56.720]   - I'm fascinated by Jeff Bezos, I agree.
[01:06:56.720 --> 01:06:57.920]   And I think he's the real deal.
[01:06:57.920 --> 01:06:58.760]   I don't know.
[01:06:58.760 --> 01:07:00.120]   Elon, I think is the real deal
[01:07:00.120 --> 01:07:02.280]   'cause what he's done with both SpaceX and Tesla
[01:07:02.280 --> 01:07:03.560]   is truly remarkable.
[01:07:03.560 --> 01:07:07.360]   - Yeah, but he also acquired those companies.
[01:07:07.360 --> 01:07:08.200]   - Yeah, but--
[01:07:08.200 --> 01:07:09.040]   - He acquired and built them.
[01:07:09.040 --> 01:07:10.040]   Bezos did not do that.
[01:07:10.040 --> 01:07:10.880]   - From scratch.
[01:07:10.880 --> 01:07:13.400]   - He started from scratch and built this jogger knot.
[01:07:13.400 --> 01:07:14.400]   - Yeah.
[01:07:14.400 --> 01:07:15.960]   - That's him. I think--
[01:07:15.960 --> 01:07:19.440]   - I think there is a truly great story about women in tech
[01:07:19.440 --> 01:07:22.680]   to be told like in this kind of form.
[01:07:22.680 --> 01:07:24.960]   You can make an amazing TV show about it
[01:07:24.960 --> 01:07:27.280]   'cause we're not a monolith, right?
[01:07:27.280 --> 01:07:29.000]   You've got some startup people,
[01:07:29.000 --> 01:07:32.320]   you've got some people that are more on the activist side.
[01:07:32.320 --> 01:07:34.440]   You've got some people that leave their careers
[01:07:34.440 --> 01:07:35.880]   due to atrocities.
[01:07:35.880 --> 01:07:38.160]   You have some people that become the villain
[01:07:38.160 --> 01:07:41.800]   and enable the system that hurts a lot of people
[01:07:41.800 --> 01:07:43.840]   because it's convenient for their career.
[01:07:43.840 --> 01:07:47.800]   - I think there is an amazing TV series to be made about that.
[01:07:47.800 --> 01:07:52.200]   - If I were a producer, I'm gonna tie all of these threads together.
[01:07:52.200 --> 01:07:57.200]   I would run, not walk, to buy the rights to Wally Funk.
[01:07:57.200 --> 01:08:00.200]   Do you know who Wally Funk is?
[01:08:00.200 --> 01:08:01.120]   - No.
[01:08:01.120 --> 01:08:06.360]   - Okay, Wally Funk, her full name is Mary Wallace Funk,
[01:08:06.360 --> 01:08:09.160]   was chosen by NASA in the '60s
[01:08:09.160 --> 01:08:12.080]   be one of 13 American women to get astronaut training
[01:08:12.080 --> 01:08:13.920]   in the Mercury program.
[01:08:13.920 --> 01:08:16.400]   They were not allowed to fly
[01:08:16.400 --> 01:08:18.400]   because NASA had a requirement at the time
[01:08:18.400 --> 01:08:20.320]   you had to be a military fighter pilot.
[01:08:20.320 --> 01:08:21.440]   And of course, there were no at the time
[01:08:21.440 --> 01:08:24.760]   female military fighter pilots.
[01:08:24.760 --> 01:08:28.400]   She became the first female
[01:08:28.400 --> 01:08:31.120]   Federal Aviation Administration Inspector.
[01:08:31.120 --> 01:08:36.320]   She was a pilot, airline pilot and instructor.
[01:08:36.320 --> 01:08:39.880]   And she is now going to be the oldest person ever to go
[01:08:39.880 --> 01:08:40.720]   into space.
[01:08:40.720 --> 01:08:42.560]   She's 82 years old.
[01:08:42.560 --> 01:08:46.520]   Wally Funk will be flying sitting next to Jeff Bezos
[01:08:46.520 --> 01:08:49.520]   and his brother in the new Shepard rocket
[01:08:49.520 --> 01:08:52.000]   when they go up on the 24th.
[01:08:52.000 --> 01:08:54.440]   She's been selected to go along with somebody
[01:08:54.440 --> 01:08:56.360]   who's paying $28 million.
[01:08:56.360 --> 01:08:58.360]   Foolishly.
[01:08:58.360 --> 01:08:59.200]   - How high up are they going?
[01:08:59.200 --> 01:09:00.200]   - Not that high.
[01:09:00.200 --> 01:09:02.600]   The reason they're calling it the new Shepard
[01:09:02.600 --> 01:09:04.720]   is because it's essentially paralleling
[01:09:04.720 --> 01:09:06.360]   Alan Shepard's flight,
[01:09:06.360 --> 01:09:09.720]   which was the first manned space flight the US attempted.
[01:09:09.720 --> 01:09:12.720]   And all he did was go up and then come back down.
[01:09:12.720 --> 01:09:14.520]   It's not even an orbit.
[01:09:14.520 --> 01:09:17.520]   But so she's going to have four minutes in zero gravity.
[01:09:17.520 --> 01:09:22.080]   I should show this video on Instagram.
[01:09:22.080 --> 01:09:24.560]   This is what I would,
[01:09:24.560 --> 01:09:27.400]   this is why I want to buy the rights to this.
[01:09:27.400 --> 01:09:28.240]   Here she is.
[01:09:28.240 --> 01:09:30.040]   Wally Funk with Jeff Bezos.
[01:09:30.040 --> 01:09:32.280]   - I have it for four minutes.
[01:09:32.280 --> 01:09:33.920]   You come back down,
[01:09:33.920 --> 01:09:37.920]   we land gently on the desert surface.
[01:09:37.920 --> 01:09:40.800]   We open the hatch and you step outside.
[01:09:40.800 --> 01:09:42.680]   What's the first thing you say?
[01:09:42.680 --> 01:09:44.400]   - I was saying honey, that was the best thing
[01:09:44.400 --> 01:09:45.720]   that ever happened tonight.
[01:09:45.720 --> 01:09:46.840]   (laughing)
[01:09:46.840 --> 01:09:51.120]   - She's going into space 82 years old.
[01:09:51.120 --> 01:09:51.960]   Wally Funk.
[01:09:51.960 --> 01:09:52.800]   Now see?
[01:09:52.800 --> 01:09:55.680]   - 19,600 flying hours.
[01:09:55.680 --> 01:09:58.160]   - Wouldn't you run out and buy her life story?
[01:09:58.160 --> 01:09:59.680]   - I'm not making certain people to fly.
[01:09:59.680 --> 01:10:01.760]   Private commercial agent.
[01:10:01.760 --> 01:10:04.880]   - This is actually Jeff Bezos' Instagram.
[01:10:04.880 --> 01:10:06.400]   - Everything that the FAA has,
[01:10:06.400 --> 01:10:08.160]   I've got the license for.
[01:10:08.160 --> 01:10:09.640]   And I can outrun you.
[01:10:09.640 --> 01:10:11.880]   (laughing)
[01:10:11.880 --> 01:10:12.720]   - 82.
[01:10:12.720 --> 01:10:13.560]   - Back in the,
[01:10:13.560 --> 01:10:14.400]   - I like her.
[01:10:14.400 --> 01:10:15.240]   - Yeah.
[01:10:15.240 --> 01:10:16.240]   - 13,000.
[01:10:16.240 --> 01:10:17.080]   - Yeah.
[01:10:17.080 --> 01:10:18.200]   - Now what's happening with SpaceX,
[01:10:18.200 --> 01:10:22.400]   because there's that whole thing tied in with St. Jude.
[01:10:22.400 --> 01:10:25.120]   Seems like there's this race, micro race,
[01:10:25.120 --> 01:10:27.240]   nano race to space.
[01:10:27.240 --> 01:10:29.400]   - To be the winner of the charity.
[01:10:29.400 --> 01:10:31.880]   So this is, yeah, SpaceX is,
[01:10:31.880 --> 01:10:33.000]   when is John knows this?
[01:10:33.000 --> 01:10:34.240]   He follows space closely.
[01:10:34.240 --> 01:10:35.080]   When is the space?
[01:10:35.080 --> 01:10:37.000]   - The publicists must be very crazy,
[01:10:37.000 --> 01:10:38.240]   'cause they probably had their plans on.
[01:10:38.240 --> 01:10:40.080]   - They're all at the same time, I know.
[01:10:40.080 --> 01:10:41.560]   - Why is it going to be?
[01:10:41.560 --> 01:10:43.520]   - No, no, no, but they're gonna do the,
[01:10:43.520 --> 01:10:46.280]   the manned launch with the,
[01:10:46.280 --> 01:10:50.520]   the Falcon, not the Falcon, the.
[01:10:50.520 --> 01:10:52.480]   - SAS.
[01:10:52.480 --> 01:10:54.400]   - The new.
[01:10:54.400 --> 01:10:55.920]   - Can't remember the name of it.
[01:10:55.920 --> 01:10:57.920]   John, you're supposed to know all of this stuff.
[01:10:57.920 --> 01:10:59.440]   The Dragon.
[01:10:59.440 --> 01:11:01.960]   The Dragon's going up with four people in it, right?
[01:11:01.960 --> 01:11:04.920]   The Crew One Mission.
[01:11:04.920 --> 01:11:07.680]   Here's the Crew One mission of SpaceX.
[01:11:07.680 --> 01:11:09.280]   When is that coming up?
[01:11:09.280 --> 01:11:10.160]   Oh, that already happened.
[01:11:10.160 --> 01:11:11.680]   No, no, no, not this one.
[01:11:11.680 --> 01:11:14.120]   The one where you get to, get to pay to go,
[01:11:14.120 --> 01:11:16.680]   and the millionaire bought it all up.
[01:11:16.680 --> 01:11:17.520]   - Right.
[01:11:17.520 --> 01:11:19.760]   - And this is, this is gonna be,
[01:11:19.760 --> 01:11:21.920]   this is gonna be the,
[01:11:21.920 --> 01:11:25.040]   the one that's going to the moon.
[01:11:25.040 --> 01:11:28.800]   Let me see if I can find this here.
[01:11:28.800 --> 01:11:32.360]   You know what I'm talking about, John?
[01:11:32.360 --> 01:11:33.440]   - Well, while you guys are looking,
[01:11:33.440 --> 01:11:34.840]   let me plug the expanse,
[01:11:34.840 --> 01:11:35.680]   which is awesome.
[01:11:35.680 --> 01:11:36.760]   - That's a great show.
[01:11:36.760 --> 01:11:37.600]   - I love it.
[01:11:37.600 --> 01:11:38.440]   - There you go.
[01:11:38.440 --> 01:11:39.280]   - Yeah.
[01:11:39.280 --> 01:11:40.240]   - Although it's cheap, okay?
[01:11:40.240 --> 01:11:42.080]   I just, I really like it.
[01:11:42.080 --> 01:11:43.480]   - The first?
[01:11:43.480 --> 01:11:44.960]   - But it looks a little cheap.
[01:11:44.960 --> 01:11:46.520]   - It looks a little, yeah.
[01:11:46.520 --> 01:11:47.720]   And the first, I don't know,
[01:11:47.720 --> 01:11:49.680]   the first maybe four episodes were so bad.
[01:11:49.680 --> 01:11:50.640]   I, I.
[01:11:50.640 --> 01:11:51.920]   - Yeah, took me three-watching.
[01:11:51.920 --> 01:11:53.240]   - My husband's face trying made me powerful.
[01:11:53.240 --> 01:11:55.240]   - Yes, it took me three-watching's
[01:11:55.240 --> 01:11:57.000]   to get far enough in where I could figure out.
[01:11:57.000 --> 01:11:58.480]   - I watched it with subtitles.
[01:11:58.480 --> 01:12:00.280]   - It's in English.
[01:12:00.280 --> 01:12:02.080]   - I have strong feelings about this,
[01:12:02.080 --> 01:12:06.120]   'cause the author James S.E. Corey, it's two people.
[01:12:06.120 --> 01:12:07.440]   I consider him a friend.
[01:12:07.440 --> 01:12:09.960]   - There would be friends, right?
[01:12:09.960 --> 01:12:10.880]   What pronouns did two people?
[01:12:10.880 --> 01:12:11.720]   - Yeah, friends of mine.
[01:12:11.720 --> 01:12:13.640]   - There's two, there's two guys who are writers.
[01:12:13.640 --> 01:12:18.160]   - Yeah, it's, it's Ty and Daniel, just amazing people.
[01:12:18.160 --> 01:12:22.360]   I love that book so, so desperately.
[01:12:22.360 --> 01:12:25.480]   I've listened to it probably 20 or 30 times
[01:12:25.480 --> 01:12:26.880]   every single one of the books in this area.
[01:12:26.880 --> 01:12:28.360]   - There's quite a few of them too, right?
[01:12:28.360 --> 01:12:29.600]   - Yeah, it's long.
[01:12:29.600 --> 01:12:32.040]   They're eight of them and they're not short at all.
[01:12:32.040 --> 01:12:35.240]   But he writes in a way that you really,
[01:12:35.240 --> 01:12:37.400]   it's one of those books you have to read twice
[01:12:37.400 --> 01:12:38.880]   to understand it.
[01:12:38.880 --> 01:12:41.880]   And that first series, that first season,
[01:12:41.880 --> 01:12:44.560]   I'm telling you, I know that book backwards and forwards
[01:12:44.560 --> 01:12:47.120]   and I'm just like, people are gonna bounce off this.
[01:12:47.120 --> 01:12:48.320]   They're not gonna understand it.
[01:12:48.320 --> 01:12:51.120]   - There's so much politics and so many sub-plots.
[01:12:51.120 --> 01:12:52.800]   - There's so much world building.
[01:12:52.800 --> 01:12:57.800]   But I tell you, it is a brilliant commentary
[01:12:57.800 --> 01:13:02.080]   about people that are trying to lessen the violence
[01:13:02.080 --> 01:13:04.880]   in the world and people that are trying to exploit
[01:13:04.880 --> 01:13:06.560]   the amount of violence in the world.
[01:13:06.560 --> 01:13:07.560]   - The politics are great.
[01:13:07.560 --> 01:13:08.560]   I'm not surprised you love it.
[01:13:08.560 --> 01:13:09.400]   - It's 10 out of 10.
[01:13:09.400 --> 01:13:10.240]   - 'Cause you're a politician.
[01:13:10.240 --> 01:13:11.360]   You love this, right?
[01:13:11.360 --> 01:13:12.360]   - Yeah, it's great.
[01:13:12.360 --> 01:13:13.360]   - The human dynamics. - It's great.
[01:13:13.360 --> 01:13:14.200]   - Yeah, yeah.
[01:13:14.200 --> 01:13:19.960]   Amazon is planning in a related story, a Rebel Alliance.
[01:13:19.960 --> 01:13:23.600]   They want, what?
[01:13:23.600 --> 01:13:24.440]   - What?
[01:13:24.440 --> 01:13:26.360]   - You doing a Johnny Carson for us here?
[01:13:26.360 --> 01:13:29.040]   - Yeah, I am good at this kind of segue.
[01:13:29.040 --> 01:13:32.560]   They have decided to partner with Slack, Dropbox,
[01:13:32.560 --> 01:13:37.560]   and others to create a office to fight Microsoft.
[01:13:37.560 --> 01:13:42.640]   The Rebel Alliance, they're calling it.
[01:13:42.640 --> 01:13:46.720]   I don't know if there's more to say than that, but we'll see.
[01:13:46.720 --> 01:13:48.360]   - I think that's more like, Java the Hut
[01:13:48.360 --> 01:13:50.600]   is particularly a minimifier myself.
[01:13:50.600 --> 01:13:54.840]   - Isn't it interesting how these giant oligarchs
[01:13:54.840 --> 01:13:57.360]   like to think of themselves as Rebels?
[01:13:57.360 --> 01:13:58.200]   - Yeah.
[01:13:58.200 --> 01:14:02.880]   Yeah, I mean, I'd like to see competition, that space.
[01:14:02.880 --> 01:14:06.000]   You know, I think, I feel like we in the tech industry
[01:14:06.000 --> 01:14:09.320]   have never really re-evaluated our position on Microsoft,
[01:14:09.320 --> 01:14:13.160]   which is much better today than it was a couple of decades ago.
[01:14:13.160 --> 01:14:17.000]   But I think, I'd love to see more competition there.
[01:14:17.000 --> 01:14:17.840]   That'd be great.
[01:14:17.840 --> 01:14:20.800]   - Yeah, and actually, I have to say that,
[01:14:20.800 --> 01:14:25.240]   I've said this a few times, the DOJ investigation
[01:14:25.240 --> 01:14:27.840]   of Microsoft on the late '90s turned Microsoft
[01:14:27.840 --> 01:14:30.160]   into a better company as a result.
[01:14:30.160 --> 01:14:32.800]   And they have under Satya Nadella pivoted in such a way
[01:14:32.800 --> 01:14:35.440]   that I think they're a very strong company,
[01:14:35.440 --> 01:14:38.720]   much more than stronger than they had any right to believe.
[01:14:38.720 --> 01:14:42.160]   You know, they were turning into the next IBM,
[01:14:42.160 --> 01:14:45.480]   and instead they've actually shown that they've got a future.
[01:14:45.480 --> 01:14:47.320]   They might be the next Apple, go ahead.
[01:14:47.320 --> 01:14:50.280]   - Yeah, now it's just gonna ask what you thought the Windows--
[01:14:50.280 --> 01:14:52.120]   - What a fraud there. - What a fraud there.
[01:14:52.120 --> 01:14:53.120]   - Yeah, that's fine.
[01:14:53.120 --> 01:14:55.680]   - So I think there's a certain irony to this.
[01:14:55.680 --> 01:14:57.920]   Microsoft last week announced Windows 11,
[01:14:57.920 --> 01:15:01.160]   and I thought that all of the storm and drawing,
[01:15:01.160 --> 01:15:04.320]   all the fewer over this would be about A,
[01:15:04.320 --> 01:15:06.720]   it's Windows 10 with a new skin,
[01:15:06.720 --> 01:15:10.800]   and B, they centered the start button and the test menu.
[01:15:10.800 --> 01:15:14.840]   I thought, I'm gonna see acres of ink over this stupidity,
[01:15:14.840 --> 01:15:15.960]   and then it'll be done.
[01:15:15.960 --> 01:15:18.160]   And instead, Microsoft decided, you know,
[01:15:18.160 --> 01:15:21.120]   they said, "Hold my beer, we can really make a problem here."
[01:15:21.120 --> 01:15:23.400]   We're gonna say, you can't use it with a processor
[01:15:23.400 --> 01:15:24.720]   that is as little as two years old,
[01:15:24.720 --> 01:15:27.080]   and oh, by the way, you have to have TPM 2.0.
[01:15:27.080 --> 01:15:29.120]   And then all the people who are about ready to say,
[01:15:29.120 --> 01:15:32.240]   "I don't want Windows 11, I'm gonna stick with Windows 10,"
[01:15:32.240 --> 01:15:33.680]   'cause that's the usual refrain
[01:15:33.680 --> 01:15:35.640]   when a new version of Windows comes out.
[01:15:35.640 --> 01:15:39.440]   Instead, got to whine, "I can't get Windows 11,
[01:15:39.440 --> 01:15:41.440]   I want it." (laughing)
[01:15:41.440 --> 01:15:44.520]   Talk, I think this is brilliant marketing on Microsoft's part.
[01:15:44.520 --> 01:15:48.120]   They made people want something that they were prepared to hate.
[01:15:48.120 --> 01:15:48.960]   Yeah.
[01:15:48.960 --> 01:15:51.760]   I think a lot of the stuff in there, like auto HDR,
[01:15:51.760 --> 01:15:54.160]   I'm very, very excited for that.
[01:15:54.160 --> 01:15:58.240]   If you don't know, this is HDR is a photography technique,
[01:15:58.240 --> 01:16:01.120]   it over saturates it under saturates in image.
[01:16:01.120 --> 01:16:03.760]   And for post processing and game engines,
[01:16:03.760 --> 01:16:05.920]   you can do that same kind of process
[01:16:05.920 --> 01:16:07.960]   is not actually that expensive.
[01:16:07.960 --> 01:16:11.000]   And Microsoft is enabling auto HDR.
[01:16:11.000 --> 01:16:13.640]   Is that something that's gonna be built into the,
[01:16:13.640 --> 01:16:16.560]   is it from the graphics cards doing the work?
[01:16:16.560 --> 01:16:19.080]   Or, I mean, I know it's a feature of Windows.
[01:16:19.080 --> 01:16:21.040]   It came from Xbox, right?
[01:16:21.040 --> 01:16:22.200]   Yeah, that's exactly right.
[01:16:22.200 --> 01:16:25.720]   It's gonna be built in to basically the graphics API.
[01:16:25.720 --> 01:16:28.680]   I don't know much about, I haven't dug into the API myself
[01:16:28.680 --> 01:16:30.000]   to learn how it works.
[01:16:30.000 --> 01:16:33.520]   But I could say the results of it are stunning.
[01:16:33.520 --> 01:16:36.840]   The processor overhead from my own experimentation
[01:16:36.840 --> 01:16:41.080]   with these kinds of post HDR processes, it's not terrible.
[01:16:41.080 --> 01:16:45.560]   And it's like a lot of, I wouldn't say home runs,
[01:16:45.560 --> 01:16:48.240]   but a whole lot of doubles that they announced
[01:16:48.240 --> 01:16:50.760]   of Windows 11 that have me really excited for it.
[01:16:50.760 --> 01:16:52.480]   Let's see, there you go.
[01:16:52.480 --> 01:16:54.640]   And how do you feel about rounded corners
[01:16:54.640 --> 01:16:57.120]   and centered taskbars?
[01:16:57.120 --> 01:17:00.280]   I'm for it, I think Windows looks like a Mac.
[01:17:00.280 --> 01:17:01.480]   Yeah, it does.
[01:17:01.480 --> 01:17:03.880]   You know, John Gruber has said for a long time,
[01:17:03.880 --> 01:17:07.680]   Microsoft should go hire some really talented designers
[01:17:07.680 --> 01:17:11.920]   and force Windows to start over and make it look more modern.
[01:17:11.920 --> 01:17:12.760]   Yes, they did that.
[01:17:12.760 --> 01:17:14.160]   And from what I've seen,
[01:17:14.160 --> 01:17:17.640]   it looks cool, I'm into it.
[01:17:17.640 --> 01:17:18.480]   Yeah.
[01:17:18.480 --> 01:17:21.680]   That actually, I have to agree with you.
[01:17:21.680 --> 01:17:24.960]   The course of all the controversy is about Microsoft's
[01:17:24.960 --> 01:17:29.960]   seemingly arbitrary requirements for compatibility.
[01:17:29.960 --> 01:17:33.360]   You know, I think it's their right to do this.
[01:17:33.360 --> 01:17:34.920]   Apple does this every time, right?
[01:17:34.920 --> 01:17:37.000]   They just, so.
[01:17:37.000 --> 01:17:43.120]   And I think the unsaid supposition is that they're doing this
[01:17:43.680 --> 01:17:48.680]   to sell PCs, to get, to help the PC industry sell more PCs,
[01:17:48.680 --> 01:17:52.000]   to require you if you want to use it by a new PC.
[01:17:52.000 --> 01:17:53.200]   But I always.
[01:17:53.200 --> 01:17:55.240]   I know PC versus what, a phone?
[01:17:55.240 --> 01:17:56.080]   Oh great.
[01:17:56.080 --> 01:17:58.960]   No, but versus, so PC industry, by the way,
[01:17:58.960 --> 01:18:01.200]   had a great year last year in 2020.
[01:18:01.200 --> 01:18:02.040]   That's why I'm saying like,
[01:18:02.040 --> 01:18:04.560]   what's the problem they're trying to solve?
[01:18:04.560 --> 01:18:07.400]   There's a suspicion that that great year
[01:18:07.400 --> 01:18:09.200]   won't be repeated next year.
[01:18:09.200 --> 01:18:10.360]   Done.
[01:18:10.360 --> 01:18:13.080]   That everybody who wanted a PC will have bought a PC,
[01:18:13.080 --> 01:18:15.440]   that the reasons people ran out and buy PCs
[01:18:15.440 --> 01:18:18.840]   for Zoom classrooms and work at home,
[01:18:18.840 --> 01:18:20.720]   are gonna evaporate.
[01:18:20.720 --> 01:18:22.600]   Actually, that turns out so far this year,
[01:18:22.600 --> 01:18:23.440]   not to be true.
[01:18:23.440 --> 01:18:28.440]   The people are in fact buying PCs in droves this year as well,
[01:18:28.440 --> 01:18:31.240]   maybe because they're getting to work from home, I don't know.
[01:18:31.240 --> 01:18:32.760]   But I think there was some concern
[01:18:32.760 --> 01:18:35.240]   that PC industry would see a big dip.
[01:18:35.240 --> 01:18:37.480]   And that the supposition was,
[01:18:37.480 --> 01:18:39.000]   well, Microsoft's gonna juke it
[01:18:39.000 --> 01:18:40.440]   by offering a version of Windows.
[01:18:40.440 --> 01:18:41.920]   Traditionally, in the Windows world,
[01:18:41.920 --> 01:18:43.920]   that's how you get the next version of Windows.
[01:18:43.920 --> 01:18:45.200]   You don't upgrade.
[01:18:45.200 --> 01:18:46.880]   Windows 10 broke that mold.
[01:18:46.880 --> 01:18:48.560]   But prior to Windows 10,
[01:18:48.560 --> 01:18:50.440]   you'd just use Windows until you got the new computer
[01:18:50.440 --> 01:18:52.920]   and then you'd use the next, whatever it came with.
[01:18:52.920 --> 01:18:53.760]   People didn't--
[01:18:53.760 --> 01:18:55.280]   - I mean, a lot of these companies that are doing
[01:18:55.280 --> 01:18:58.240]   some form of intermittent or permanent work from home
[01:18:58.240 --> 01:18:59.560]   or flexible work schedules,
[01:18:59.560 --> 01:19:03.200]   which include enormous, enormous companies like Fujitsu,
[01:19:03.200 --> 01:19:06.560]   - Google's doing it now.
[01:19:06.560 --> 01:19:07.640]   - Yeah. - What's that?
[01:19:07.640 --> 01:19:09.040]   - Google. - Yeah, so I mean,
[01:19:09.040 --> 01:19:12.080]   so if that's the case, then everyone of your,
[01:19:12.080 --> 01:19:14.000]   we're moving into this sort of BYO,
[01:19:14.000 --> 01:19:15.840]   bring your own device. - That's right.
[01:19:15.840 --> 01:19:17.600]   - So that has to help. - So that has to help.
[01:19:17.600 --> 01:19:19.400]   - Unexpectedly, I think that's helped.
[01:19:19.400 --> 01:19:20.240]   - That's gonna, right.
[01:19:20.240 --> 01:19:22.840]   And that will probably continue as people sort of sort out
[01:19:22.840 --> 01:19:23.960]   and figure out where they're gonna be.
[01:19:23.960 --> 01:19:26.760]   But on the other side of that,
[01:19:26.760 --> 01:19:28.320]   if I was a CISO right now,
[01:19:28.320 --> 01:19:31.520]   I would be a hair on fire trying to figure out
[01:19:31.520 --> 01:19:33.080]   what I'm gonna do for security 'cause--
[01:19:33.080 --> 01:19:34.960]   - No kidding. - What's the overunder
[01:19:34.960 --> 01:19:37.920]   on your employee patching firmware or whatever's updating
[01:19:37.920 --> 01:19:39.840]   your router, you know what I mean?
[01:19:39.840 --> 01:19:41.600]   - Oh, I completely agree with you.
[01:19:41.600 --> 01:19:45.920]   - Yeah. - I think that in the long run,
[01:19:45.920 --> 01:19:47.600]   the only thing that went wrong
[01:19:47.600 --> 01:19:49.880]   with the Windows 11 launch was PR.
[01:19:49.880 --> 01:19:52.520]   And that Microsoft's not handled it well,
[01:19:52.520 --> 01:19:54.080]   the communication's been poor.
[01:19:54.080 --> 01:19:56.120]   I think they'll probably solve this
[01:19:56.120 --> 01:19:58.320]   by changing the requirements over time.
[01:19:58.320 --> 01:19:59.520]   And I don't think it's gonna end up being
[01:19:59.520 --> 01:20:00.520]   a long-term black mark,
[01:20:00.520 --> 01:20:02.960]   but I think the PR was terrible on this.
[01:20:02.960 --> 01:20:04.760]   Now, let's talk about security
[01:20:04.760 --> 01:20:06.680]   'cause there's a lot of stories there.
[01:20:06.680 --> 01:20:09.680]   - Go ahead, finish the-- - No, no, no, go ahead, Leo.
[01:20:09.680 --> 01:20:14.680]   - So, big supply chain attack just announced
[01:20:14.680 --> 01:20:21.240]   the R-evil ransomware gang has taken over the Casaya VSA.
[01:20:21.240 --> 01:20:26.840]   Casaya is a cloud-based MSP platform.
[01:20:26.840 --> 01:20:29.200]   A lot of managed service providers use it.
[01:20:29.200 --> 01:20:31.000]   MSPs, we use an MSP.
[01:20:31.000 --> 01:20:32.080]   If you're a smaller company,
[01:20:32.080 --> 01:20:33.920]   you don't have a full-time IT staff,
[01:20:33.920 --> 01:20:38.920]   you might use a managed service provider to do your IT.
[01:20:38.920 --> 01:20:41.760]   A lot of them use Casaya,
[01:20:41.760 --> 01:20:45.360]   which is a patch management client monitoring platform.
[01:20:45.360 --> 01:20:47.280]   Casaya has been compromised.
[01:20:47.280 --> 01:20:51.440]   And as a result, the fear is so have thousands of MSPs.
[01:20:51.440 --> 01:20:54.740]   All using Casaya.
[01:20:54.740 --> 01:21:01.560]   Huntress Labs, John Hammond told Lawrence Abrams
[01:21:01.560 --> 01:21:02.440]   at Bleakibing Computer,
[01:21:02.440 --> 01:21:05.480]   "We're tracking 20 MSPs where Casaya was used
[01:21:05.480 --> 01:21:10.480]   to encrypt over 1,000 businesses with ransomware."
[01:21:10.480 --> 01:21:13.720]   And I'm working in close collaboration with six of them.
[01:21:13.720 --> 01:21:16.720]   The biggest so far looks to be
[01:21:16.720 --> 01:21:21.720]   a Swedish supermarket chain called Koop.
[01:21:21.720 --> 01:21:24.800]   They've closed 800 stores in Sweden.
[01:21:24.800 --> 01:21:27.000]   Look at this, this is a great sign.
[01:21:27.000 --> 01:21:28.680]   IT problem.
[01:21:28.680 --> 01:21:31.520]   There is something that goes that's cross barriers,
[01:21:31.520 --> 01:21:32.480]   cross-language barriers.
[01:21:32.480 --> 01:21:35.960]   I can't read the rest of it, but IT problem, I understand.
[01:21:35.960 --> 01:21:37.360]   Ransomware has shut them down,
[01:21:37.360 --> 01:21:38.480]   they can't take payments,
[01:21:38.480 --> 01:21:41.360]   their cash register stopped working.
[01:21:41.360 --> 01:21:42.720]   So the stores had a close,
[01:21:42.720 --> 01:21:44.800]   they're hoping they can reopen tomorrow.
[01:21:44.800 --> 01:21:49.520]   But this is gonna be another nasty one.
[01:21:49.520 --> 01:21:51.760]   And supply chain is very tough.
[01:21:51.760 --> 01:21:54.600]   SolarWinds was also a supply chain attack.
[01:21:54.600 --> 01:21:55.680]   - Yeah, right.
[01:21:55.680 --> 01:21:58.000]   So let me tell you if I could buy a--
[01:21:58.000 --> 01:21:58.840]   - I'm telling you if I could buy a--
[01:21:58.840 --> 01:21:59.920]   - Go ahead, no.
[01:21:59.920 --> 01:22:00.880]   We've got a lag.
[01:22:00.880 --> 01:22:02.080]   - Yeah, I apologize.
[01:22:02.080 --> 01:22:02.920]   - Yeah.
[01:22:02.920 --> 01:22:03.760]   (laughs)
[01:22:03.760 --> 01:22:06.160]   - I was gonna say, no, I mean,
[01:22:06.160 --> 01:22:11.880]   especially as people are using more and more remote services,
[01:22:11.880 --> 01:22:13.280]   we're gonna see more of this.
[01:22:13.280 --> 01:22:17.560]   And it sounds like this was yet another nation state attack,
[01:22:17.560 --> 01:22:18.400]   possibly.
[01:22:18.400 --> 01:22:19.240]   - Yep.
[01:22:19.240 --> 01:22:22.600]   - And MSPs have been a popular target.
[01:22:22.600 --> 01:22:24.640]   - 'Cause it leverages.
[01:22:24.640 --> 01:22:25.680]   - By a nation state.
[01:22:25.680 --> 01:22:29.000]   - Oh, you hack, you hack Casea, and boom.
[01:22:29.000 --> 01:22:32.040]   - We've got thousands of other computers systems.
[01:22:32.040 --> 01:22:36.680]   - Right, so there's a guy, CTO of a breach quest
[01:22:36.680 --> 01:22:39.640]   called Casea, the Coca-Cola of remote management.
[01:22:39.640 --> 01:22:40.480]   (laughs)
[01:22:40.480 --> 01:22:42.040]   - Could you do a sense of how significant this is?
[01:22:42.040 --> 01:22:45.360]   - Does that mean it's sugary and bad for your teeth?
[01:22:45.360 --> 01:22:46.200]   - It's--
[01:22:46.200 --> 01:22:48.040]   (laughs)
[01:22:48.040 --> 01:22:48.880]   - It's big.
[01:22:48.880 --> 01:22:52.040]   - And you can buy it in a can with your friend's name on it.
[01:22:52.040 --> 01:22:54.440]   (laughs)
[01:22:54.440 --> 01:22:55.280]   - But did you--
[01:22:55.280 --> 01:22:57.400]   - No, I was just gonna say, you know,
[01:22:57.400 --> 01:23:00.360]   this is ransomware in general.
[01:23:00.360 --> 01:23:02.720]   It's been really, really, really bad lately.
[01:23:02.720 --> 01:23:05.720]   There was a story that came out this week about Russia
[01:23:05.720 --> 01:23:07.920]   actually going through and brute force it,
[01:23:07.920 --> 01:23:12.040]   trying old school brute force attacks to get passwords
[01:23:12.040 --> 01:23:17.040]   on various companies in the energy sector.
[01:23:17.040 --> 01:23:18.800]   This was Fancy Bear doing it,
[01:23:18.800 --> 01:23:21.160]   which was a part of the GRU.
[01:23:21.160 --> 01:23:26.480]   If I could buy stock in ransomware continuing
[01:23:26.480 --> 01:23:28.640]   to devastate companies, I would do that
[01:23:28.640 --> 01:23:31.040]   'cause it's almost certainly going to happen.
[01:23:31.040 --> 01:23:35.080]   It's such a difficult problem to solve too
[01:23:35.080 --> 01:23:37.600]   because here in Massachusetts,
[01:23:37.600 --> 01:23:41.240]   we had a company shut down because three foreign nationals
[01:23:41.240 --> 01:23:43.400]   came in and interned for the company
[01:23:43.400 --> 01:23:46.120]   and stole the entire code base and left
[01:23:46.120 --> 01:23:48.920]   and thousands of people lost their job because of that.
[01:23:48.920 --> 01:23:51.000]   You've got attacks from the inside,
[01:23:51.000 --> 01:23:54.160]   you've got attacks from the outside for nation states,
[01:23:54.160 --> 01:23:57.720]   you've got problems not updating software,
[01:23:57.720 --> 01:24:02.560]   like with Equifax where they didn't run an Apache update.
[01:24:02.560 --> 01:24:06.320]   I mean, this is, it's such a massive problem to solve.
[01:24:06.320 --> 01:24:09.640]   And the thing is, the pipeline to address this
[01:24:09.640 --> 01:24:13.160]   is so astonishingly inconvenient for everyone.
[01:24:13.160 --> 01:24:17.720]   And it's just gonna get worse from here.
[01:24:17.720 --> 01:24:20.800]   - It's so funny because we think of these nation state attacks
[01:24:20.800 --> 01:24:22.720]   as being highly sophisticated.
[01:24:22.720 --> 01:24:26.320]   But fancy bears just trying different passwords.
[01:24:26.320 --> 01:24:29.200]   - Yeah, I mean, what's really,
[01:24:29.200 --> 01:24:33.200]   I think the part here that's so challenging is that,
[01:24:33.200 --> 01:24:38.360]   so we've all, lots of people have been talking about
[01:24:38.360 --> 01:24:39.840]   the need for digital transformation,
[01:24:39.840 --> 01:24:41.560]   which is like a gopply, go-crediculous,
[01:24:41.560 --> 01:24:45.920]   Jarcony made up phrase, but essentially what that means is,
[01:24:45.920 --> 01:24:47.840]   like you gotta automate your stuff,
[01:24:47.840 --> 01:24:50.080]   like stop using paper records, you gotta digitize,
[01:24:50.080 --> 01:24:51.600]   you gotta put yourself in the cloud,
[01:24:51.600 --> 01:24:53.720]   use security, all that stuff.
[01:24:53.720 --> 01:24:56.760]   And Leo, as you mentioned at the top,
[01:24:56.760 --> 01:24:58.880]   small to mid-sized businesses,
[01:24:58.880 --> 01:25:02.360]   they can't hire a full-time IT person,
[01:25:02.360 --> 01:25:04.800]   - We depend on our MSPs to keep us secure.
[01:25:04.800 --> 01:25:08.200]   - So right, and so what's smart about,
[01:25:08.200 --> 01:25:10.000]   from the hackers point of view,
[01:25:10.000 --> 01:25:14.480]   attacking that MSP, and targeting all these tiny businesses,
[01:25:14.480 --> 01:25:17.280]   a lot of these places don't have cash in the bank,
[01:25:17.280 --> 01:25:19.040]   they don't have runway.
[01:25:19.040 --> 01:25:23.160]   So if you target thousands, tens of thousands,
[01:25:23.160 --> 01:25:25.640]   hundreds of thousands of tiny little businesses,
[01:25:25.640 --> 01:25:28.840]   a lot of them are gonna do whatever they have to do
[01:25:28.840 --> 01:25:32.160]   in order to get back online so they can keep working.
[01:25:32.160 --> 01:25:37.800]   So it's a really shitty thing that they've done,
[01:25:37.800 --> 01:25:42.080]   but also kind of a, unfortunately, it's a vulnerability.
[01:25:42.080 --> 01:25:43.720]   - We're gonna have to accept the fact
[01:25:43.720 --> 01:25:47.240]   that this is gonna be the future of warfare.
[01:25:47.240 --> 01:25:48.680]   This is cyber warfare.
[01:25:48.680 --> 01:25:49.680]   I'm sure you've written about this state.
[01:25:49.680 --> 01:25:51.000]   - In economic terms, right.
[01:25:51.000 --> 01:25:52.520]   So that's the issue.
[01:25:52.520 --> 01:25:54.400]   I wrote about this in my last book,
[01:25:54.400 --> 01:25:56.680]   so it was a big nine. - Sure, yes, right.
[01:25:56.680 --> 01:25:59.360]   - One of the big challenges we face going forward
[01:25:59.360 --> 01:26:03.080]   is that we don't have a definition for warfare
[01:26:03.080 --> 01:26:06.560]   outside the realms that we've currently defined it.
[01:26:06.560 --> 01:26:10.120]   So if you've got a nation-state attack,
[01:26:10.120 --> 01:26:12.400]   that is targeting bits and pieces
[01:26:12.400 --> 01:26:14.800]   of our technological and economic infrastructure,
[01:26:14.800 --> 01:26:18.160]   and you are in some way creating,
[01:26:18.160 --> 01:26:22.160]   wreaking economic havoc on parts of our society,
[01:26:22.160 --> 01:26:23.840]   that does cripple us in some way.
[01:26:23.840 --> 01:26:26.200]   It makes us weaker as a nation,
[01:26:26.200 --> 01:26:28.560]   but we don't currently define that type of attack
[01:26:28.560 --> 01:26:30.880]   as warfare.
[01:26:30.880 --> 01:26:33.240]   - Well, didn't Biden say to Putin,
[01:26:33.240 --> 01:26:37.880]   hands off these critical infrastructure?
[01:26:37.880 --> 01:26:40.480]   - He didn't, and then he gave him her two glasses.
[01:26:40.480 --> 01:26:41.840]   (both laughing)
[01:26:41.840 --> 01:26:44.360]   - No, I think that's a little pessimistic.
[01:26:44.360 --> 01:26:46.640]   I mean, he went through and he started to try
[01:26:46.640 --> 01:26:48.720]   to make a framework about, look,
[01:26:48.720 --> 01:26:51.760]   we are nations, we're not friends, right?
[01:26:51.760 --> 01:26:55.920]   But these are targets that we would consider legitimate.
[01:26:55.920 --> 01:26:58.000]   These are the targets we would not consider
[01:26:58.000 --> 01:26:58.840]   to legitimate these battles. - Do we need
[01:26:58.840 --> 01:26:59.840]   to explain these battles? - Do we need to do
[01:26:59.840 --> 01:27:02.600]   these battles? - For cyber warfare in effect,
[01:27:02.600 --> 01:27:06.240]   where, look, we all agree, we're not gonna attack hospitals.
[01:27:06.240 --> 01:27:07.480]   - I think we do.
[01:27:07.480 --> 01:27:11.800]   And another thing, this may be a controversial opinion,
[01:27:11.800 --> 01:27:16.800]   but you don't affect yourself to defend,
[01:27:16.800 --> 01:27:20.600]   I don't need to defend Massachusetts from external attack.
[01:27:20.600 --> 01:27:23.840]   I do think there's a role for the Pentagon to play here.
[01:27:23.840 --> 01:27:28.040]   And maybe we need to subsidize certain systems
[01:27:28.040 --> 01:27:32.080]   to give small business access to basic cybersecurity tools.
[01:27:32.080 --> 01:27:34.520]   That would be a very good investment, I think,
[01:27:34.520 --> 01:27:36.080]   as far as being a subsidy.
[01:27:36.080 --> 01:27:39.000]   I think it's time to really think about this holistically
[01:27:39.000 --> 01:27:41.440]   because it's about our economy,
[01:27:41.440 --> 01:27:43.120]   it's about our ability to function,
[01:27:43.120 --> 01:27:46.000]   it's about our schools, our hospitals, our privacy.
[01:27:46.000 --> 01:27:48.600]   There's clearly a national security interest here.
[01:27:48.600 --> 01:27:49.880]   - I think there's also an issue though,
[01:27:49.880 --> 01:27:54.120]   because I would say Russia is a failed state or close to.
[01:27:54.120 --> 01:27:57.520]   And as a result, they have less to lose.
[01:27:57.520 --> 01:28:00.360]   I think we might be able to go to China, Amy, and say,
[01:28:00.360 --> 01:28:02.440]   "Look, we won't attack your infrastructure
[01:28:02.440 --> 01:28:03.880]   "if you don't attack ours."
[01:28:03.880 --> 01:28:06.680]   But Russia at this point is punching up
[01:28:06.680 --> 01:28:10.760]   and they don't have much of those, right?
[01:28:10.760 --> 01:28:11.920]   - Yeah, they have a lot more to lose.
[01:28:11.920 --> 01:28:14.360]   Putin is also not quite a strong of a leader.
[01:28:14.360 --> 01:28:17.800]   And then the pantheon of autocrats,
[01:28:17.800 --> 01:28:20.240]   Putin is standing on slightly shakier ground
[01:28:20.240 --> 01:28:21.600]   than Xi Jinping, so.
[01:28:21.600 --> 01:28:27.800]   I would say trying to cut some kind of
[01:28:27.800 --> 01:28:31.880]   Cold War era deal,
[01:28:31.880 --> 01:28:36.880]   but so mutually assured destruction, but make it code.
[01:28:36.880 --> 01:28:39.040]   - Oh, that's interesting.
[01:28:39.040 --> 01:28:40.080]   - I think it's gonna be challenging.
[01:28:40.080 --> 01:28:43.000]   - Are hackers as good as their hackers?
[01:28:43.000 --> 01:28:46.600]   - Branim, I know this better.
[01:28:46.600 --> 01:28:48.920]   I mean, it's my understanding that the folks
[01:28:48.920 --> 01:28:51.840]   who are working in Russia and China are elite.
[01:28:51.840 --> 01:28:54.760]   - Well, they also have,
[01:28:54.760 --> 01:28:57.280]   they have huge alliances with the mob, right?
[01:28:57.280 --> 01:29:00.240]   So the mob is over there conducting a live.
[01:29:00.240 --> 01:29:02.960]   It's like one of the most profitable sectors in Russia
[01:29:02.960 --> 01:29:07.120]   is the Russian mob will go out and get a young coder
[01:29:07.120 --> 01:29:10.480]   and we'll do all these cryptocurrency schemes.
[01:29:10.480 --> 01:29:14.240]   So it's really one of these nations where,
[01:29:14.240 --> 01:29:15.800]   well, that's my point.
[01:29:15.800 --> 01:29:20.280]   Like they are, we are constrained by rules of warfare
[01:29:20.280 --> 01:29:22.400]   and largely they are not.
[01:29:22.400 --> 01:29:25.240]   And I think that's where something like
[01:29:25.240 --> 01:29:27.920]   a Geneva Convention for Code could be very helpful.
[01:29:27.920 --> 01:29:30.160]   - Yeah, unless they don't play the game,
[01:29:30.160 --> 01:29:31.400]   they don't play along.
[01:29:31.400 --> 01:29:36.360]   Microsoft has a zero day that's actively being exploited
[01:29:36.360 --> 01:29:38.440]   in its Windows Print Spooler.
[01:29:38.440 --> 01:29:42.440]   Somebody, some wag, some security firm
[01:29:42.440 --> 01:29:44.800]   is named this Print Nightmare,
[01:29:44.800 --> 01:29:48.640]   Sangfor published unfortunately,
[01:29:48.640 --> 01:29:51.560]   the proof of code accidentally.
[01:29:51.560 --> 01:29:53.280]   It's been, it was immediately forked
[01:29:53.280 --> 01:29:56.160]   and as a result, the code is out there
[01:29:56.160 --> 01:29:58.680]   resulting in multiple zero day vulnerabilities
[01:29:58.680 --> 01:30:01.360]   in the Windows Print Spooler service.
[01:30:01.360 --> 01:30:04.480]   Sangfor was gonna show it at Black Hat,
[01:30:04.480 --> 01:30:07.840]   but once the proof of code got out,
[01:30:07.840 --> 01:30:09.360]   they immediately ran over and said,
[01:30:09.360 --> 01:30:14.360]   "Uh, whoops, whoops, the vulnerability allows attackers
[01:30:14.360 --> 01:30:18.720]   to use remote code execution through the Print Spooler.
[01:30:18.720 --> 01:30:21.480]   You can remotely install programs, modify data,
[01:30:21.480 --> 01:30:24.200]   create new accounts with full admin rights.
[01:30:24.200 --> 01:30:29.200]   The vulnerability exists in all versions of Windows.
[01:30:29.200 --> 01:30:32.400]   Not clear though if it's exploitable
[01:30:32.400 --> 01:30:34.320]   beyond server versions of Windows.
[01:30:34.320 --> 01:30:36.760]   So this is another one where your MSP's gonna have
[01:30:36.760 --> 01:30:40.120]   to come on over and there are some mitigations,
[01:30:40.120 --> 01:30:43.400]   mostly which involve taking your Print Spooler offline.
[01:30:43.400 --> 01:30:46.560]   I don't know if that's a long term fix.
[01:30:46.560 --> 01:30:50.200]   That's, add that to the zero day
[01:30:50.200 --> 01:30:53.440]   with the Western Digital drives
[01:30:53.440 --> 01:30:56.560]   and now you've got a lot of fun going on, my book live.
[01:30:56.560 --> 01:30:58.960]   This is one that might make an interesting movie.
[01:30:58.960 --> 01:31:02.880]   At first we thought that the reason that
[01:31:02.880 --> 01:31:05.400]   owners of this already out of data,
[01:31:05.400 --> 01:31:07.480]   I mean they stopped selling it in 2015,
[01:31:07.480 --> 01:31:09.880]   stopped supporting it in 2015.
[01:31:09.880 --> 01:31:13.360]   Network Attached Storage Device had a flaw
[01:31:13.360 --> 01:31:15.520]   that was exposed in 2018.
[01:31:15.520 --> 01:31:19.040]   We thought this was the flaw being used to erase these drives.
[01:31:19.040 --> 01:31:21.240]   Many, many users said that their data
[01:31:21.240 --> 01:31:24.480]   had been completely erased after a reset of the drive.
[01:31:24.480 --> 01:31:27.000]   Then it turns out, no, maybe it wasn't that.
[01:31:27.000 --> 01:31:30.880]   There was a remote code execution flaw, zero day.
[01:31:30.880 --> 01:31:33.240]   Nevertheless, Western Digital is not patching these.
[01:31:33.240 --> 01:31:36.760]   Their fix is take your drive offline.
[01:31:36.760 --> 01:31:39.640]   We will not, we, now there's a problem
[01:31:39.640 --> 01:31:42.760]   in the MyCloud OS 3.0 firmware which may affect
[01:31:42.760 --> 01:31:44.920]   many, many more devices.
[01:31:44.920 --> 01:31:46.920]   Western Digital's a solution to that is
[01:31:46.920 --> 01:31:49.720]   you need to upgrade to OS5, some say,
[01:31:49.720 --> 01:31:52.040]   yeah, but that's a complete rewrite
[01:31:52.040 --> 01:31:54.680]   with lots of functionality missing.
[01:31:54.680 --> 01:31:56.640]   So there's kind of a mess.
[01:31:56.640 --> 01:32:00.720]   Maybe get a different manufacturer for your NAS.
[01:32:01.000 --> 01:32:02.600]   Very interesting story.
[01:32:02.600 --> 01:32:07.640]   Dan Gooden had a good breakdown of it on ours, Technica.
[01:32:07.640 --> 01:32:11.680]   But just an example of, you know, an IoT device
[01:32:11.680 --> 01:32:14.200]   that is not being patched is a recipe for disaster.
[01:32:14.200 --> 01:32:15.920]   It's just gonna happen.
[01:32:15.920 --> 01:32:17.840]   It's just gonna happen.
[01:32:17.840 --> 01:32:20.920]   Apps with 5.8 million downloads on Google Play
[01:32:20.920 --> 01:32:25.920]   stole users Facebook passwords, nine apps,
[01:32:25.920 --> 01:32:29.320]   that, and this is something you should just be aware of,
[01:32:29.320 --> 01:32:31.160]   the apps did actually work.
[01:32:31.160 --> 01:32:34.240]   They actually, unusually, they actually did things
[01:32:34.240 --> 01:32:36.880]   like photo editing and framing, exercise and training,
[01:32:36.880 --> 01:32:39.200]   horoscopes, removing junk files.
[01:32:39.200 --> 01:32:43.080]   But at some point the apps offered users an option
[01:32:43.080 --> 01:32:47.320]   to log into their Facebook account in order to disable ads.
[01:32:47.320 --> 01:32:51.560]   They saw a genuine Facebook login,
[01:32:51.560 --> 01:32:55.720]   but the Trojans used a mechanism to steal that login.
[01:32:57.440 --> 01:33:02.440]   Unfortunately, 5.8 million people downloaded those apps,
[01:33:02.440 --> 01:33:07.960]   like processing photo, app lock, keep rubbish cleaner,
[01:33:07.960 --> 01:33:11.600]   horoscope daily, horoscope pie, app lock manager,
[01:33:11.600 --> 01:33:15.400]   lock it master, in well fitness and pip photo.
[01:33:15.400 --> 01:33:17.440]   Do not give them your Facebook credentials,
[01:33:17.440 --> 01:33:19.120]   even if you don't have any ads.
[01:33:19.120 --> 01:33:23.880]   - Are we, I thought, I've been noticing companies
[01:33:23.880 --> 01:33:27.200]   are not using OAuth so much.
[01:33:27.200 --> 01:33:28.280]   - Yeah, that's disappointing.
[01:33:28.280 --> 01:33:31.240]   I think maybe because it doesn't work,
[01:33:31.240 --> 01:33:36.360]   doesn't play well with some apps, doesn't do well on mobile.
[01:33:36.360 --> 01:33:38.520]   Google still uses it, but I think a lot of them,
[01:33:38.520 --> 01:33:40.080]   you know Microsoft does, which I really like,
[01:33:40.080 --> 01:33:43.520]   is a single sign on with its authenticator.
[01:33:43.520 --> 01:33:46.600]   It'll say, okay, I'm firing up authenticator,
[01:33:46.600 --> 01:33:49.160]   give me, click the button that says this number,
[01:33:49.160 --> 01:33:51.160]   they give you four buttons, you click it
[01:33:51.160 --> 01:33:53.360]   and the login's done, this is how you log into Windows
[01:33:53.360 --> 01:33:54.720]   and a lot of Microsoft accounts.
[01:33:54.720 --> 01:33:57.680]   And I think it's a little bit better than OAuth.
[01:33:57.680 --> 01:34:01.080]   - Yeah, didn't Twitter move over to where you can use
[01:34:01.080 --> 01:34:03.360]   an external device like a UB key for your--
[01:34:03.360 --> 01:34:04.960]   - Yes, I've been doing that for a while.
[01:34:04.960 --> 01:34:05.800]   - Second-factor authentication.
[01:34:05.800 --> 01:34:07.120]   - Yeah, I've been doing that for a while on Twitter.
[01:34:07.120 --> 01:34:11.560]   And for a while, you had to have a fallback to SMS,
[01:34:11.560 --> 01:34:15.360]   which not good, after Jack Dorsey's SMS got it.
[01:34:15.360 --> 01:34:16.360]   (laughs)
[01:34:16.360 --> 01:34:20.160]   Remember that, he got some snatched,
[01:34:20.160 --> 01:34:23.680]   what do they call it, they got into his account
[01:34:23.680 --> 01:34:26.680]   and so they turned off, Sim, what?
[01:34:26.680 --> 01:34:27.520]   - Yeah.
[01:34:27.520 --> 01:34:30.600]   - Sim-yacked, Sim-jack, that's the word.
[01:34:30.600 --> 01:34:31.440]   - Sim-jack.
[01:34:31.440 --> 01:34:34.000]   - Although I'm gonna call it Sim-yacked from now on.
[01:34:34.000 --> 01:34:37.280]   You got Sim-yacked, and so after that,
[01:34:37.280 --> 01:34:40.880]   they allowed you to have UB key minus the SMS,
[01:34:40.880 --> 01:34:43.900]   which I immediately turned on, so that's good.
[01:34:43.900 --> 01:34:47.680]   Let's see, the states, let's talk about some states.
[01:34:47.680 --> 01:34:52.360]   Maine passes the strongest state facial recognition ban yet.
[01:34:53.640 --> 01:34:54.560]   - Stronger than Illinois?
[01:34:54.560 --> 01:34:57.920]   - Yeah, in fact, Illinois just had a successful prosecution
[01:34:57.920 --> 01:35:01.440]   and a case against somebody using face recognition.
[01:35:01.440 --> 01:35:03.240]   - So what's the deal now in Maine?
[01:35:03.240 --> 01:35:05.320]   - The new law in Maine prohibits government use
[01:35:05.320 --> 01:35:07.360]   of facial recognition except in specifically
[01:35:07.360 --> 01:35:12.120]   outlined situations, especially probable cause
[01:35:12.120 --> 01:35:14.160]   that an unidentified person in an image committed
[01:35:14.160 --> 01:35:17.040]   to serious crime or for proactive fraud prevention,
[01:35:17.040 --> 01:35:20.920]   they will have to do that through the FBI
[01:35:20.920 --> 01:35:23.000]   or the Maine Bureau of Motor Vehicles.
[01:35:23.000 --> 01:35:25.800]   Maine police won't have access to facial recognition.
[01:35:25.800 --> 01:35:28.080]   ACL uses.
[01:35:28.080 --> 01:35:31.160]   - What I really like about this is, yes, go ahead.
[01:35:31.160 --> 01:35:32.720]   - No, I was gonna say what I like about this
[01:35:32.720 --> 01:35:34.840]   is it has accountability, so that the police
[01:35:34.840 --> 01:35:38.080]   do end up looking for an active suspect, right?
[01:35:38.080 --> 01:35:39.400]   They've got probable cause,
[01:35:39.400 --> 01:35:42.240]   they've got a good reason to use facial recognition.
[01:35:42.240 --> 01:35:44.080]   There's actually oversight with it
[01:35:44.080 --> 01:35:47.000]   where that use of facial recognition has to go
[01:35:47.000 --> 01:35:49.640]   in a place where the public can review.
[01:35:49.640 --> 01:35:54.480]   Additionally, it shuts down a lot of the tricks
[01:35:54.480 --> 01:35:56.400]   that they had used to kind of skirt this,
[01:35:56.400 --> 01:35:58.440]   but maybe someone in Maine law enforcement
[01:35:58.440 --> 01:36:01.120]   would call up a friend in Massachusetts and say,
[01:36:01.120 --> 01:36:03.760]   "Hey, can you run this search for me?"
[01:36:03.760 --> 01:36:07.240]   And it basically shuts that exactly down.
[01:36:07.240 --> 01:36:11.320]   So I do worry about like a federated state by state
[01:36:11.320 --> 01:36:14.640]   approach for this, but it seems to have good stuff
[01:36:14.640 --> 01:36:16.360]   in it as best as I can tell.
[01:36:16.360 --> 01:36:19.440]   - Illinois law is actually a biometric,
[01:36:19.440 --> 01:36:21.880]   you know, anti-biometric law.
[01:36:21.880 --> 01:36:23.280]   - I thought that covered facial recognition.
[01:36:23.280 --> 01:36:25.000]   - It does, it does.
[01:36:25.000 --> 01:36:27.800]   In fact, Walmart had to pay a $10 million fine
[01:36:27.800 --> 01:36:29.820]   to resolve its lawsuit.
[01:36:29.820 --> 01:36:34.600]   They were asking workers to scan their handprints.
[01:36:34.600 --> 01:36:41.000]   Court has allowed a lawsuit against Apple to go ahead
[01:36:41.000 --> 01:36:45.840]   using Illinois's protections.
[01:36:45.840 --> 01:36:47.960]   So this has been very effective.
[01:36:47.960 --> 01:36:49.280]   In fact, we all kind of, I think,
[01:36:49.280 --> 01:36:51.640]   owe a little debt of gratitude to Illinois
[01:36:51.640 --> 01:36:55.080]   'cause that was a very forward thinking law.
[01:36:55.080 --> 01:36:59.040]   And we've all benefited because of course nobody wants
[01:36:59.040 --> 01:37:02.840]   to do business in everywhere except Illinois.
[01:37:02.840 --> 01:37:06.120]   Six flags, America, $36 million settlement
[01:37:06.120 --> 01:37:08.980]   over the park's use of finger scan entry gates.
[01:37:08.980 --> 01:37:13.120]   So it's been very effective, very effective.
[01:37:14.920 --> 01:37:19.560]   Also that Florida law that said,
[01:37:19.560 --> 01:37:21.760]   if you're a social network, you can't ban politicians
[01:37:21.760 --> 01:37:25.280]   unless you own a amusement park has been struck down.
[01:37:25.280 --> 01:37:27.480]   - Wait, what is that?
[01:37:27.480 --> 01:37:28.320]   Wait, is this law?
[01:37:28.320 --> 01:37:29.480]   - You haven't heard about this one?
[01:37:29.480 --> 01:37:31.040]   Oh, this is a good one.
[01:37:31.040 --> 01:37:33.320]   Once again, Rick DeSantis just showing he is
[01:37:33.320 --> 01:37:35.600]   in the forefront, Rhonda Santis.
[01:37:35.600 --> 01:37:37.600]   I was calling him Rick, Rhonda Santis.
[01:37:37.600 --> 01:37:41.400]   Past, this was a, by the way,
[01:37:41.400 --> 01:37:45.920]   the Republican, Florida legislature was, you know,
[01:37:45.920 --> 01:37:48.440]   overwhelmingly in favor of this.
[01:37:48.440 --> 01:37:50.960]   It's the state can find large social media companies
[01:37:50.960 --> 01:37:53.160]   a quarter million dollars a day
[01:37:53.160 --> 01:37:56.320]   if they remove an account of a statewide political candidate
[01:37:56.320 --> 01:37:57.920]   in 25,000 dollars a day
[01:37:57.920 --> 01:38:00.040]   if they remove an account of someone running for local office.
[01:38:00.040 --> 01:38:03.040]   Obviously, this is a response to Twitter and Facebook
[01:38:03.040 --> 01:38:05.260]   deplatforming the former president.
[01:38:05.260 --> 01:38:09.400]   But on the face of it, it violates a couple
[01:38:09.400 --> 01:38:11.440]   of important precepts in federal law,
[01:38:11.440 --> 01:38:13.440]   one being the first amendment,
[01:38:13.440 --> 01:38:17.840]   the other being a regulation
[01:38:17.840 --> 01:38:20.960]   that states cannot regulate interstate commerce.
[01:38:20.960 --> 01:38:25.040]   The US district judge granted a preliminary injunction
[01:38:25.040 --> 01:38:27.920]   stopping the new law from being enforced the day before
[01:38:27.920 --> 01:38:30.240]   it was to take effect on Thursday.
[01:38:30.240 --> 01:38:35.040]   Now it's a preliminary injunction, so the suit will go ahead.
[01:38:35.040 --> 01:38:38.800]   But it seems unlikely that this will,
[01:38:38.800 --> 01:38:42.480]   and I don't think the intent ever was to have it be a law.
[01:38:42.480 --> 01:38:45.840]   'Cause it did exempt anybody who owned an amusement park.
[01:38:45.840 --> 01:38:51.640]   Which I think Disney is a pretty big important company
[01:38:51.640 --> 01:38:53.880]   in Florida, I might be wrong.
[01:38:53.880 --> 01:39:01.880]   So the Loon Law was aimed at social media businesses
[01:39:01.880 --> 01:39:04.160]   and made exemptions for Disney and their apps
[01:39:04.160 --> 01:39:07.680]   by including the provision that theme park owners
[01:39:07.680 --> 01:39:09.040]   would not be subject to.
[01:39:09.040 --> 01:39:14.520]   - Disney is full of biometrics and full of--
[01:39:14.520 --> 01:39:15.640]   - This is not biometric.
[01:39:15.640 --> 01:39:18.200]   - Oh yeah, I know, but I meant just like using PIIs
[01:39:18.200 --> 01:39:20.640]   and keeping and stage getting the whole thing.
[01:39:20.640 --> 01:39:23.360]   - For a while, Disney wanted to use your thumbprint
[01:39:23.360 --> 01:39:27.000]   for their fast pass technology, right?
[01:39:27.000 --> 01:39:29.480]   And we, on security now, Steve gives them a say,
[01:39:29.480 --> 01:39:32.440]   "Don't give them your thumbprint, do an elbow instead."
[01:39:32.440 --> 01:39:33.600]   (laughs)
[01:39:33.600 --> 01:39:35.160]   I don't know if it worked.
[01:39:35.160 --> 01:39:37.600]   But the problem with that, of course,
[01:39:37.600 --> 01:39:40.720]   for those who just want a little explication is
[01:39:40.720 --> 01:39:43.200]   you can change a lot of things,
[01:39:43.200 --> 01:39:44.440]   but you can't change your face,
[01:39:44.440 --> 01:39:45.920]   and you can't change your thumb.
[01:39:45.920 --> 01:39:48.600]   And so you're giving them biometric information
[01:39:48.600 --> 01:39:50.280]   that is permanent.
[01:39:50.280 --> 01:39:52.520]   And if they don't keep tracking that database,
[01:39:52.520 --> 01:39:54.360]   that could be problematic.
[01:39:54.360 --> 01:39:56.400]   - You know, it's worse, it's your face.
[01:39:56.400 --> 01:39:59.160]   So even if you're wearing a mask at Disney,
[01:39:59.160 --> 01:40:02.680]   then it's gonna trap you by either the magic band
[01:40:02.680 --> 01:40:05.280]   or the app that you have to do everything
[01:40:05.280 --> 01:40:06.880]   to get food right now with COVID.
[01:40:06.880 --> 01:40:10.320]   So yeah, you're basically entering 1984
[01:40:10.320 --> 01:40:12.520]   when you go to Disney World, I'm sorry to say.
[01:40:12.520 --> 01:40:13.360]   - Yeah.
[01:40:13.360 --> 01:40:16.240]   When signing the Billings of Law in May,
[01:40:16.240 --> 01:40:17.520]   Governor Ron DeSantis said,
[01:40:17.520 --> 01:40:20.440]   "Silicon Valley companies were exerting unprecedented power
[01:40:20.440 --> 01:40:23.360]   over the American people when you deplatform, quote,
[01:40:23.360 --> 01:40:25.600]   "when you deplatform the president of the United States,
[01:40:25.600 --> 01:40:29.800]   "but you let Ayatollah Khomeini talk about killing Jews."
[01:40:29.800 --> 01:40:33.120]   That is wrong, and the governor said,
[01:40:33.120 --> 01:40:35.620]   "We are disappointed by Judge Hinkle's ruling
[01:40:35.620 --> 01:40:37.320]   "and disagree with his determination
[01:40:37.320 --> 01:40:41.080]   "that the US Constitution protects big techs censorship
[01:40:41.080 --> 01:40:43.640]   "of certain individuals and content of her others.
[01:40:43.640 --> 01:40:45.360]   "They're gonna appeal to the 11th circuit,
[01:40:45.360 --> 01:40:47.480]   "which will also throw it out instantly."
[01:40:47.480 --> 01:40:51.680]   Khome is working on an M1 competitor.
[01:40:51.680 --> 01:40:56.240]   So you talked about how, oh, that Apple M1 is very compelling.
[01:40:56.240 --> 01:40:58.240]   Intel doesn't have anything like it.
[01:40:58.240 --> 01:41:02.720]   Khome's CEO, or new CEO, Cristiano Amman,
[01:41:02.720 --> 01:41:06.440]   says, "No, we're gonna work on a chip
[01:41:06.440 --> 01:41:08.760]   "that would compete with the M1.
[01:41:08.760 --> 01:41:11.400]   "They acquired a startup called Nuvia
[01:41:11.400 --> 01:41:13.320]   "founded by former Apple employees
[01:41:13.320 --> 01:41:17.440]   "who had worked on the M1 for $1.4 billion."
[01:41:17.440 --> 01:41:19.200]   So there you go.
[01:41:19.200 --> 01:41:20.440]   - How did Adam work out?
[01:41:20.440 --> 01:41:21.800]   Can someone jog my memory up now?
[01:41:21.800 --> 01:41:24.000]   - Oh Lord, oh Lord.
[01:41:24.000 --> 01:41:25.800]   - Oh, remember? - Have they even towels like
[01:41:25.800 --> 01:41:26.800]   - Scale? - Scambersions
[01:41:26.800 --> 01:41:27.640]   into this space. - Yeah, remember
[01:41:27.640 --> 01:41:29.240]   the mobile scale processor?
[01:41:29.240 --> 01:41:30.560]   Yeah, no Intel has no. - Yeah.
[01:41:30.560 --> 01:41:33.200]   - But maybe Qualcomm, I mean, they make arm chips
[01:41:33.200 --> 01:41:34.440]   that are very low power.
[01:41:34.440 --> 01:41:38.400]   That's what's in your fine Samsung device.
[01:41:38.400 --> 01:41:40.920]   It's possible that they could come up with something.
[01:41:40.920 --> 01:41:44.480]   They're desktop, so far they're desktop chips
[01:41:44.480 --> 01:41:45.880]   who have been uninspiring with this.
[01:41:45.880 --> 01:41:48.160]   Was it the CX? - Yeah.
[01:41:48.160 --> 01:41:50.320]   - Not very powerful.
[01:41:50.320 --> 01:41:53.200]   I'm sure Microsoft would love to see something
[01:41:53.200 --> 01:41:55.520]   because Windows on ARM is at this point,
[01:41:55.520 --> 01:41:57.840]   hard to use is very slow.
[01:41:57.840 --> 01:41:58.840]   - Definitely.
[01:41:58.840 --> 01:42:00.920]   I mean, there's, you know,
[01:42:00.920 --> 01:42:03.920]   I just bought like a $3,400 laptop
[01:42:03.920 --> 01:42:06.040]   and my biggest complaint with it is--
[01:42:06.040 --> 01:42:06.960]   - How do you like that?
[01:42:06.960 --> 01:42:08.040]   - Oh, I'm sorry, keep it up.
[01:42:08.040 --> 01:42:09.920]   - It gets very hot very quickly.
[01:42:09.920 --> 01:42:11.280]   - Yes, that's the problem.
[01:42:11.280 --> 01:42:12.520]   - It's good. - It's an I7
[01:42:12.520 --> 01:42:13.520]   or is it an AMD?
[01:42:13.520 --> 01:42:16.120]   - It's an I9, so it's Intel,
[01:42:16.120 --> 01:42:19.160]   but it's got the RTX 3090, 3080 in it.
[01:42:19.160 --> 01:42:20.320]   The notebook version of that.
[01:42:20.320 --> 01:42:23.040]   - That's one thing Apple's gonna have a hard time
[01:42:23.040 --> 01:42:26.480]   because their GPUs will all be in the M1 chip
[01:42:26.480 --> 01:42:28.200]   or whatever its success is.
[01:42:28.200 --> 01:42:31.480]   - Well, the problem is not even the chip technology.
[01:42:31.480 --> 01:42:34.240]   It's the game developers are choosing to work with that.
[01:42:34.240 --> 01:42:35.600]   Like look at Apple Arcade.
[01:42:35.600 --> 01:42:37.080]   I like Apple Arcade.
[01:42:37.080 --> 01:42:40.600]   There's no game there that I would call visually impressive.
[01:42:40.600 --> 01:42:42.160]   - No, not at all.
[01:42:42.160 --> 01:42:43.280]   - There hasn't been anything.
[01:42:43.280 --> 01:42:44.840]   - Can you say so conveniently?
[01:42:44.840 --> 01:42:46.680]   - I don't think they're designing those games though
[01:42:46.680 --> 01:42:47.880]   for gamers, I think.
[01:42:47.880 --> 01:42:49.040]   - No, they're catching game.
[01:42:49.040 --> 01:42:49.960]   - That's my point.
[01:42:49.960 --> 01:42:51.960]   And this is why you've got to have a PC.
[01:42:51.960 --> 01:42:55.520]   I've just given up on Apple making the kind of games
[01:42:55.520 --> 01:42:56.480]   I want to play.
[01:42:56.480 --> 01:43:00.880]   They make some great pleasant distractions on Apple Arcade,
[01:43:00.880 --> 01:43:04.000]   but it's just not a core gaming experience.
[01:43:04.000 --> 01:43:07.440]   - So I was really excited about Stadia and XCloud
[01:43:07.440 --> 01:43:09.920]   and GeForce Now.
[01:43:09.920 --> 01:43:11.840]   These are the streaming services.
[01:43:11.840 --> 01:43:16.840]   I tried, X-Cloud just came out for the iPad for any browser,
[01:43:16.840 --> 01:43:20.240]   tried it on my iPad Pro with an M1 chip in it
[01:43:20.240 --> 01:43:22.800]   and it was stuttering like crazy.
[01:43:22.800 --> 01:43:23.920]   I was trying to play Sea of Thieves.
[01:43:23.920 --> 01:43:25.320]   It was just unacceptable.
[01:43:25.320 --> 01:43:27.720]   But I'm hoping XCloud will get better.
[01:43:27.720 --> 01:43:29.160]   Stadia was pretty decent.
[01:43:29.160 --> 01:43:31.920]   I played Cyberpunk on it
[01:43:31.920 --> 01:43:34.920]   and it was the only way I could play Cyberpunk.
[01:43:34.920 --> 01:43:36.800]   And it was pretty decent.
[01:43:36.800 --> 01:43:39.880]   It wasn't as good as a console or PC would be,
[01:43:39.880 --> 01:43:42.760]   but if you have a device like an iPad,
[01:43:42.760 --> 01:43:45.680]   at least you could play those games, right?
[01:43:45.680 --> 01:43:48.480]   - Yeah, I feel like for Apple
[01:43:48.480 --> 01:43:51.320]   and for Microsoft rather to really compete
[01:43:51.320 --> 01:43:55.240]   with things like the M1, the issue is the latency, right?
[01:43:55.240 --> 01:43:57.560]   Like you were talking earlier at Leo about people
[01:43:57.560 --> 01:43:59.880]   they're angry that they're only supporting devices
[01:43:59.880 --> 01:44:02.760]   that are roughly two years old.
[01:44:02.760 --> 01:44:04.800]   That's great to have a lot of that,
[01:44:04.800 --> 01:44:07.160]   but by chopping off like the end of it,
[01:44:07.160 --> 01:44:10.640]   you can move forward and you can make the UI more responsive
[01:44:10.640 --> 01:44:13.200]   'cause the layer is thinner.
[01:44:13.200 --> 01:44:17.280]   And I just, I think, I want to see Microsoft
[01:44:17.280 --> 01:44:20.360]   be more aggressive with some of their products
[01:44:20.360 --> 01:44:21.480]   in that way. - There's really,
[01:44:21.480 --> 01:44:22.320]   truly compete.
[01:44:22.320 --> 01:44:23.160]   - Here's some good news.
[01:44:23.160 --> 01:44:26.120]   The blue screen of death in Windows 11 will now be black.
[01:44:26.120 --> 01:44:29.000]   (both laughing)
[01:44:29.000 --> 01:44:30.000]   - Why?
[01:44:30.000 --> 01:44:31.800]   Who's UI?
[01:44:31.800 --> 01:44:33.760]   - Microsoft's.
[01:44:33.760 --> 01:44:37.720]   - Microsoft's the black screen of death.
[01:44:37.720 --> 01:44:39.640]   There it is. - That's awesome.
[01:44:39.640 --> 01:44:41.360]   - Thank you, our code's blue.
[01:44:41.360 --> 01:44:43.720]   - Yeah, maybe that's why they wanted a better contrast
[01:44:43.720 --> 01:44:45.880]   with the QR code, which will do apps, by the way,
[01:44:45.880 --> 01:44:48.200]   scan it, go ahead, absolutely useless.
[01:44:48.200 --> 01:44:50.040]   (both laughing)
[01:44:50.040 --> 01:44:51.400]   You know, I don't know.
[01:44:51.400 --> 01:44:54.440]   I guess it's to make you feel better about it.
[01:44:54.440 --> 01:44:55.600]   - Actually, it's kind of smart
[01:44:55.600 --> 01:44:59.080]   because the blue screen of death is associated with Microsoft,
[01:44:59.080 --> 01:44:59.920]   which is blue.
[01:44:59.920 --> 01:45:04.000]   And if you want to associate your brand a little bit less
[01:45:04.000 --> 01:45:06.560]   with catastrophic failure.
[01:45:06.560 --> 01:45:08.640]   - Well, and if you're going to have a screen of death,
[01:45:08.640 --> 01:45:10.400]   shouldn't it be black, really?
[01:45:10.400 --> 01:45:13.360]   I mean, shouldn't be pink.
[01:45:13.360 --> 01:45:15.760]   - I got Apple's screen of death.
[01:45:15.760 --> 01:45:16.760]   - Oh, gosh.
[01:45:16.760 --> 01:45:17.760]   - That's great. - Six months ago?
[01:45:17.760 --> 01:45:18.920]   - That one's great.
[01:45:18.920 --> 01:45:21.680]   - It was, yeah, that was horrible.
[01:45:21.680 --> 01:45:23.840]   I had spent three days rebuilding my machine.
[01:45:23.840 --> 01:45:24.680]   - Was it hardware?
[01:45:24.680 --> 01:45:26.160]   No, it was software.
[01:45:26.160 --> 01:45:27.880]   - I thought, I don't know if I told you this or not.
[01:45:27.880 --> 01:45:32.880]   It was a weird, possibly a NAS-related time machine
[01:45:32.880 --> 01:45:40.520]   backup issue, or possibly I bought like really bad RAM.
[01:45:40.520 --> 01:45:42.680]   - That's what it was, bad RAM.
[01:45:42.680 --> 01:45:44.760]   That's what it was, yeah.
[01:45:44.760 --> 01:45:48.840]   - Maybe like your Razer besides being a laptop heater.
[01:45:48.840 --> 01:45:49.680]   - Yeah.
[01:45:49.680 --> 01:45:50.520]   - Is it good?
[01:45:50.520 --> 01:45:53.520]   - It's it, it's it, it runs out in real five very well.
[01:45:53.520 --> 01:45:56.240]   Unreal Engine five, it's still in preview.
[01:45:56.240 --> 01:46:00.240]   So we can't really evaluate what's in there yet.
[01:46:00.240 --> 01:46:03.280]   But for everything I've seen, it runs out in real engine four,
[01:46:03.280 --> 01:46:04.880]   really, really well.
[01:46:04.880 --> 01:46:09.720]   For me, a lot of the way I learn new features is by sitting
[01:46:09.720 --> 01:46:12.360]   down and just experimenting while I'm watching Netflix.
[01:46:12.360 --> 01:46:16.120]   So I've wanted something I could mess around with blueprint
[01:46:16.120 --> 01:46:19.240]   or new APIs or things like that while I'm watching TV
[01:46:19.240 --> 01:46:20.080]   in the den.
[01:46:20.080 --> 01:46:22.440]   So that's why I got it.
[01:46:22.440 --> 01:46:24.600]   - When are we going to see Unreal five games?
[01:46:24.600 --> 01:46:26.400]   Unreal five is so beautiful.
[01:46:26.400 --> 01:46:27.880]   - Very soon, yeah.
[01:46:27.880 --> 01:46:29.480]   I think it's a new version.
[01:46:29.480 --> 01:46:31.760]   - Who will be the first I charted or what will.
[01:46:31.760 --> 01:46:34.280]   - Now they use their own engine.
[01:46:34.280 --> 01:46:37.800]   The what's the Uncharted team, I'm spacing it right now.
[01:46:37.800 --> 01:46:39.720]   They use their own proprietary engine.
[01:46:40.800 --> 01:46:43.040]   But I think this is going to be a big success,
[01:46:43.040 --> 01:46:46.400]   which is why Apple's lawsuit is kind of ill-advised.
[01:46:46.400 --> 01:46:47.600]   - The Apple Epic lawsuit.
[01:46:47.600 --> 01:46:52.600]   I just stunning what Unreal five just looks so real.
[01:46:52.600 --> 01:46:55.120]   It's pretty amazing.
[01:46:55.120 --> 01:46:57.480]   Yeah, and of course it won't appear on any Apple devices
[01:46:57.480 --> 01:46:59.360]   because the Apple Epic battle.
[01:46:59.360 --> 01:47:04.880]   - Well, the apps themselves could theoretically end up there.
[01:47:04.880 --> 01:47:08.480]   I think the question that I would have is investing
[01:47:08.480 --> 01:47:10.760]   in making Unreal Engine work on Apple
[01:47:10.760 --> 01:47:14.000]   is a very expensive proposition.
[01:47:14.000 --> 01:47:16.600]   So do you have faith in that relationship enough
[01:47:16.600 --> 01:47:20.720]   to invest hundreds of thousands of dollars literally?
[01:47:20.720 --> 01:47:24.880]   I could say for this studio we're thinking about, no,
[01:47:24.880 --> 01:47:27.480]   I personally would not do that.
[01:47:27.480 --> 01:47:29.320]   What's really interesting when trying to figure out
[01:47:29.320 --> 01:47:31.720]   is the actual game development pipeline.
[01:47:31.720 --> 01:47:33.880]   Like some of the things like the number of tries
[01:47:33.880 --> 01:47:37.480]   you see up close for some things in Unreal Engine.
[01:47:37.480 --> 01:47:40.440]   How do you take that and put that in a viable
[01:47:40.440 --> 01:47:42.520]   Unreal Engine pipeline?
[01:47:42.520 --> 01:47:44.200]   Is it just an LOD trick?
[01:47:44.200 --> 01:47:47.280]   That's all the things that we're trying to figure out right now.
[01:47:47.280 --> 01:47:50.140]   - Have you tried as a coder,
[01:47:50.140 --> 01:47:54.440]   Microsoft's new GitHub co-pilot?
[01:47:54.440 --> 01:47:56.880]   - I've not, I've heard such good things about it.
[01:47:56.880 --> 01:47:59.720]   - When I first saw this, your AI pair programmer,
[01:47:59.720 --> 01:48:01.440]   I thought it said your O pair programmer,
[01:48:01.440 --> 01:48:04.880]   I thought, wow, that would be an interesting O pair.
[01:48:04.880 --> 01:48:08.840]   - No, the idea is as you're writing something,
[01:48:08.840 --> 01:48:12.600]   your pair programming, normally pair programming
[01:48:12.600 --> 01:48:15.600]   is two programmers in the same editor working together.
[01:48:15.600 --> 01:48:18.760]   In this case, co-pilot is an artificial intelligence
[01:48:18.760 --> 01:48:23.280]   watching you work, making suggestions for code
[01:48:23.280 --> 01:48:25.120]   you might wanna use.
[01:48:25.120 --> 01:48:26.600]   - So it's clippy.
[01:48:26.600 --> 01:48:29.080]   - It's clippy without the annoying,
[01:48:29.080 --> 01:48:32.320]   I see you're writing in a login sequence, would you lie?
[01:48:32.320 --> 01:48:36.840]   - Apparently it will, but already people are saying,
[01:48:36.840 --> 01:48:41.640]   hold on there, this is one tweet I saw
[01:48:41.640 --> 01:48:45.000]   from Armin Roenacher, he says,
[01:48:45.000 --> 01:48:46.760]   "I don't wanna say anything,
[01:48:46.760 --> 01:48:49.880]   but that's not the right license, Mr. co-pilot."
[01:48:49.880 --> 01:48:54.880]   He's actually, co-pilot is typing in a hack
[01:48:54.880 --> 01:48:58.960]   for square roots from Quake.
[01:48:58.960 --> 01:49:00.880]   - Wow.
[01:49:00.880 --> 01:49:04.240]   - As it types it in, it says copyright, 2015,
[01:49:04.240 --> 01:49:07.440]   all rights reserved, redistribution and use and source
[01:49:07.440 --> 01:49:10.880]   with or without modification and permitted only
[01:49:10.880 --> 01:49:13.320]   if you do all of these things.
[01:49:13.320 --> 01:49:14.960]   - So wait, where's the corpus,
[01:49:14.960 --> 01:49:16.640]   where's it pulling that in from?
[01:49:16.640 --> 01:49:18.160]   - That's a good question.
[01:49:18.160 --> 01:49:20.680]   People are wondering, some people--
[01:49:20.680 --> 01:49:22.560]   - What's their repository?
[01:49:22.560 --> 01:49:26.240]   - Yeah, well, I don't know, they say they're going through
[01:49:26.240 --> 01:49:29.080]   a lot of different code bases.
[01:49:30.280 --> 01:49:32.800]   They famously open sourced Quake many years ago.
[01:49:32.800 --> 01:49:35.560]   John Carmack put a lot of political capital in the line
[01:49:35.560 --> 01:49:38.480]   to make that happen, so that actually doesn't surprise me,
[01:49:38.480 --> 01:49:39.880]   that's there, I think that's awesome.
[01:49:39.880 --> 01:49:41.960]   - It was from Quake 3 Arena.
[01:49:41.960 --> 01:49:42.800]   - Yeah.
[01:49:42.800 --> 01:49:45.720]   - By the way, if you actually,
[01:49:45.720 --> 01:49:47.520]   you might wanna watch more closely,
[01:49:47.520 --> 01:49:49.880]   I can't read the comment,
[01:49:49.880 --> 01:49:51.760]   but you, let me make this full screen
[01:49:51.760 --> 01:49:53.680]   so you can perhaps see it,
[01:49:53.680 --> 01:49:56.800]   there is a profane comment in this fast
[01:49:56.800 --> 01:50:01.000]   inverse square root routine where the guy offers a constant
[01:50:01.000 --> 01:50:05.040]   in hex and the comment is what, the F?
[01:50:05.040 --> 01:50:08.800]   And then I saw on Hacker News,
[01:50:08.800 --> 01:50:10.800]   actually that's not even the right constant,
[01:50:10.800 --> 01:50:13.200]   so you might not wanna use that code.
[01:50:13.200 --> 01:50:16.600]   People famously paste in code from Stack Overflow
[01:50:16.600 --> 01:50:23.200]   all the time, causing the lawyers to wonder.
[01:50:23.200 --> 01:50:26.160]   - This seems like a vulnerability or a way
[01:50:26.160 --> 01:50:28.360]   for somebody to insert Janky code.
[01:50:28.360 --> 01:50:29.520]   - Seems like a bad idea.
[01:50:29.520 --> 01:50:34.520]   - Back door, like all, like the seams concerning.
[01:50:34.520 --> 01:50:38.640]   - I don't know, I think pair programming can be
[01:50:38.640 --> 01:50:39.720]   a really viable pipeline.
[01:50:39.720 --> 01:50:41.920]   - With a human. - Especially with a human.
[01:50:41.920 --> 01:50:44.600]   - Yeah, but the person, this is with the domain.
[01:50:44.600 --> 01:50:46.440]   - Or they just need to, like,
[01:50:46.440 --> 01:50:49.280]   then before this thing gets much further,
[01:50:49.280 --> 01:50:53.280]   they ought to like show everybody where the code pieces are,
[01:50:53.280 --> 01:50:54.880]   like where are they pulling from?
[01:50:54.880 --> 01:50:56.800]   - They need to be having accountability chain.
[01:50:56.800 --> 01:50:59.520]   - Yeah, there's issues about,
[01:50:59.520 --> 01:51:01.160]   questions about, not necessarily issues,
[01:51:01.160 --> 01:51:02.920]   but questions about the license.
[01:51:02.920 --> 01:51:04.200]   One of the developers says,
[01:51:04.200 --> 01:51:06.600]   "GitHub Copilot is a code synthesizer,
[01:51:06.600 --> 01:51:08.360]   not a search engine.
[01:51:08.360 --> 01:51:10.480]   The vast majority of the code is suggests
[01:51:10.480 --> 01:51:14.800]   is uniquely generated and has never been seen before."
[01:51:14.800 --> 01:51:15.640]   But so.
[01:51:15.640 --> 01:51:17.280]   - Like GPT-3, but make it kind of.
[01:51:17.280 --> 01:51:19.160]   - Yeah, yeah.
[01:51:19.160 --> 01:51:20.080]   - I don't know, so you still,
[01:51:20.080 --> 01:51:22.480]   it's still gonna start with some kind of corpus for training.
[01:51:22.480 --> 01:51:24.800]   And I don't know, but they need to,
[01:51:24.800 --> 01:51:26.720]   maybe it's there, I don't know,
[01:51:26.720 --> 01:51:28.920]   but I would think that they should just--
[01:51:28.920 --> 01:51:31.680]   - Let me see if they say,
[01:51:31.680 --> 01:51:35.480]   "Powered by codecs, the new AI system created by OpenAI,
[01:51:35.480 --> 01:51:36.680]   which created GPT."
[01:51:36.680 --> 01:51:40.040]   So it may be a GPT-like,
[01:51:40.040 --> 01:51:42.040]   I don't know if it's using GAN or what.
[01:51:42.040 --> 01:51:47.600]   We've always thought the computers at some point
[01:51:47.600 --> 01:51:49.680]   would be writing their own code.
[01:51:49.680 --> 01:51:51.120]   - Well, DeepMind, yeah,
[01:51:51.120 --> 01:51:53.120]   I mean, that we saw beginnings of this, right?
[01:51:53.120 --> 01:51:55.320]   But it was a couple of years ago,
[01:51:55.320 --> 01:52:00.080]   and Alpha Zero was writing its own child programs
[01:52:00.080 --> 01:52:02.680]   and developing its own strategy.
[01:52:02.680 --> 01:52:04.920]   - One of the things they suggest you do
[01:52:04.920 --> 01:52:07.400]   is write a comment describing the logic you want
[01:52:07.400 --> 01:52:10.880]   and then let co-pilot write the code for you.
[01:52:10.880 --> 01:52:14.320]   Some of these things should be easy to do,
[01:52:14.320 --> 01:52:16.920]   things like repetitive code, it'll have auto fill,
[01:52:16.920 --> 01:52:20.200]   just like, you know, Excel does, basically.
[01:52:20.200 --> 01:52:21.400]   Tests without the toil,
[01:52:21.400 --> 01:52:23.800]   it'll automatically write test code for you,
[01:52:23.800 --> 01:52:25.480]   which is a good thing, right?
[01:52:25.480 --> 01:52:26.320]   That's good.
[01:52:26.320 --> 01:52:27.160]   - Yeah.
[01:52:27.160 --> 01:52:28.960]   - Wanna evaluate a few different approaches,
[01:52:28.960 --> 01:52:31.440]   co-pilot can show you a list of solutions.
[01:52:31.440 --> 01:52:37.200]   But where is the description of where the,
[01:52:37.200 --> 01:52:38.640]   what is the training set?
[01:52:38.640 --> 01:52:40.280]   Is that what you're asking, Amy?
[01:52:40.280 --> 01:52:41.120]   - Yeah.
[01:52:41.120 --> 01:52:43.600]   - Yeah, it says, it looks like here,
[01:52:43.600 --> 01:52:46.600]   the training set is public code and text on the internet.
[01:52:46.600 --> 01:52:48.760]   - Yeah, so like, let's click on that,
[01:52:48.760 --> 01:52:49.920]   somebody in MapleList.
[01:52:49.920 --> 01:52:51.480]   - That's what that is.
[01:52:51.480 --> 01:52:53.360]   - Yeah, training set.
[01:52:53.360 --> 01:52:55.600]   - I feel like I'm less pessimistic.
[01:52:55.600 --> 01:52:57.800]   I would like to try this and see
[01:52:57.800 --> 01:53:00.200]   if it's giving you useful suggestions.
[01:53:00.200 --> 01:53:02.920]   I mean, you know, the way pair programming works
[01:53:02.920 --> 01:53:06.080]   is you write something that maybe someone sitting next
[01:53:06.080 --> 01:53:09.000]   to you sees a way to do it in fewer cycles, right?
[01:53:09.000 --> 01:53:11.800]   Like, lightly refactoring it.
[01:53:11.800 --> 01:53:14.880]   If it's more of that, I could see it being useful.
[01:53:14.880 --> 01:53:17.840]   If it's just pasting in garbage,
[01:53:17.840 --> 01:53:19.400]   maybe a little bit less useful,
[01:53:19.400 --> 01:53:20.560]   but I think it's interesting.
[01:53:20.560 --> 01:53:23.440]   It's certainly a technology I would like to see us
[01:53:23.440 --> 01:53:24.600]   start developing.
[01:53:24.600 --> 01:53:26.680]   - Yeah, I mean, I love the idea.
[01:53:26.680 --> 01:53:28.520]   OpenAI is doing some interesting things.
[01:53:28.520 --> 01:53:31.040]   They said that they've trained this
[01:53:31.040 --> 01:53:32.880]   on publicly available data,
[01:53:32.880 --> 01:53:37.880]   and they can say that means it's considered fair use.
[01:53:37.880 --> 01:53:43.600]   So you can reuse this code because it's fair use.
[01:53:43.600 --> 01:53:47.560]   It's been trained in a selection of English language
[01:53:47.560 --> 01:53:50.000]   and source code publicly available sources,
[01:53:50.000 --> 01:53:53.320]   including code in public repositories on GitHub.
[01:53:53.320 --> 01:53:55.840]   Now, just by the way, if you have a free account on GitHub,
[01:53:55.840 --> 01:53:58.120]   your repositories are generally public.
[01:53:58.120 --> 01:54:00.040]   That doesn't make it open source.
[01:54:00.040 --> 01:54:02.480]   That doesn't mean it's licensed.
[01:54:02.480 --> 01:54:05.600]   So I don't know, it's interesting.
[01:54:05.600 --> 01:54:06.440]   It's interesting.
[01:54:06.440 --> 01:54:09.200]   You know, I would say don't use it whole cloth,
[01:54:09.200 --> 01:54:10.760]   but maybe if you get an insight from it,
[01:54:10.760 --> 01:54:12.920]   then you can write your own code around it.
[01:54:12.920 --> 01:54:14.240]   Who owns the code GitHub?
[01:54:14.240 --> 01:54:16.120]   Co-pilot helps me write.
[01:54:16.120 --> 01:54:18.840]   GitHub, this is from their frequently asked questions.
[01:54:18.840 --> 01:54:22.520]   Co-pilot is a tool like a compiler or a pin.
[01:54:22.520 --> 01:54:24.440]   The suggestions Co-pilot generates
[01:54:24.440 --> 01:54:27.440]   and the code you write with its help belong to you
[01:54:27.440 --> 01:54:29.040]   and you are responsible for it.
[01:54:29.040 --> 01:54:31.040]   We recommend that you carefully test it.
[01:54:31.040 --> 01:54:33.120]   (laughs)
[01:54:33.120 --> 01:54:35.560]   - Well, no, but that's an interesting question
[01:54:35.560 --> 01:54:38.160]   because then does the code that you write
[01:54:38.160 --> 01:54:40.120]   become part of that code base?
[01:54:40.120 --> 01:54:40.960]   - Yeah.
[01:54:40.960 --> 01:54:42.240]   - That is then used, you know what I mean?
[01:54:42.240 --> 01:54:43.680]   - It may be.
[01:54:43.680 --> 01:54:45.320]   - GitHub Co-pilot, because it was trained
[01:54:45.320 --> 01:54:46.440]   publicly available code.
[01:54:46.440 --> 01:54:49.720]   It's training set included public personal data.
[01:54:49.720 --> 01:54:51.840]   For our internal testing, we found it to be rare
[01:54:51.840 --> 01:54:54.240]   that suggestions include personal data verbatim
[01:54:54.240 --> 01:54:55.080]   from the training set.
[01:54:55.080 --> 01:54:58.320]   However, there is that risk.
[01:54:58.320 --> 01:55:00.480]   - This is getting worse the more you read me on.
[01:55:00.480 --> 01:55:01.920]   (laughs)
[01:55:01.920 --> 01:55:03.120]   - It's a technical preview.
[01:55:03.120 --> 01:55:04.040]   You can sign up for it.
[01:55:04.040 --> 01:55:05.840]   I want you to try it next time you're watching
[01:55:05.840 --> 01:55:07.240]   Netflix, Brianna, and--
[01:55:07.240 --> 01:55:09.000]   - I will give it a big hug.
[01:55:09.000 --> 01:55:10.600]   Yeah, I think that's a, I mean,
[01:55:10.600 --> 01:55:13.840]   I think if it's not looking at what you're writing
[01:55:13.840 --> 01:55:16.360]   and adding that, like using it to train itself
[01:55:16.360 --> 01:55:18.880]   to figure out what's useful or what's not useful,
[01:55:18.880 --> 01:55:20.800]   that would be giving up like the biggest tool
[01:55:20.800 --> 01:55:22.200]   it has to refine itself.
[01:55:22.200 --> 01:55:26.360]   So obviously, I would assume your code is going to go into it
[01:55:26.360 --> 01:55:27.520]   and to help it train.
[01:55:27.520 --> 01:55:30.920]   So, and that's just an assumption of my part.
[01:55:30.920 --> 01:55:34.840]   That's very, that would give me serious pause.
[01:55:34.840 --> 01:55:38.280]   - Here's a great story from Stars and Stripes.
[01:55:38.280 --> 01:55:39.800]   I wish I-- - Oh, I love this.
[01:55:39.800 --> 01:55:40.960]   - Isn't this great?
[01:55:40.960 --> 01:55:43.960]   So as you know, we're withdrawing from Afghanistan.
[01:55:43.960 --> 01:55:47.400]   Bagram Airfield has now been completely given back
[01:55:47.400 --> 01:55:52.440]   to the Afghanis, but apparently the airmen there
[01:55:52.440 --> 01:55:54.520]   played a lot of Pokemon Go.
[01:55:54.520 --> 01:55:59.160]   And you can by, you know, doing some, you know,
[01:55:59.160 --> 01:56:01.920]   location shifting.
[01:56:01.920 --> 01:56:04.120]   Go look at the various gyms.
[01:56:04.120 --> 01:56:07.000]   Here's the Air Force gym at Bagram Airfield.
[01:56:07.000 --> 01:56:09.520]   It was hit by mortar fire later.
[01:56:09.520 --> 01:56:14.520]   You can, there are still Pokemon gyms that are held
[01:56:14.520 --> 01:56:17.760]   by service men that are no longer there.
[01:56:17.760 --> 01:56:20.520]   Bagram once had a thriving Pokemon Go community
[01:56:20.520 --> 01:56:22.640]   of troops, contractors and civilians
[01:56:22.640 --> 01:56:25.480]   who played the game while exercising and after work.
[01:56:25.480 --> 01:56:29.320]   US Army specialist Corey Olson,
[01:56:29.320 --> 01:56:31.840]   an electrical technician for attack helicopters
[01:56:31.840 --> 01:56:35.600]   who played the game with his buds from his shop in 2019
[01:56:35.600 --> 01:56:38.080]   says we weren't expecting Pokemon Go to be thriving
[01:56:38.080 --> 01:56:39.720]   in Bagram and yet it was.
[01:56:39.720 --> 01:56:43.240]   And by the way, if you play Pokemon Go in Afghanistan,
[01:56:43.240 --> 01:56:44.560]   you're gonna catch some monsters
[01:56:44.560 --> 01:56:46.400]   you can't get in the United States.
[01:56:46.400 --> 01:56:51.760]   Here is a Pokemon guarding a location much longer
[01:56:51.760 --> 01:56:54.600]   than they normally would because nobody else
[01:56:54.600 --> 01:56:57.400]   is playing anymore so you can be down to two health points
[01:56:57.400 --> 01:57:00.840]   and still just be sitting there.
[01:57:00.840 --> 01:57:02.760]   Nobody's taking over the gyms.
[01:57:02.760 --> 01:57:04.040]   I gotta get my wife to go over there.
[01:57:04.040 --> 01:57:06.400]   She's always looking for gyms that are abandoned.
[01:57:07.920 --> 01:57:10.240]   Yeah, Frank is beyond a detector to this game too.
[01:57:10.240 --> 01:57:14.000]   I deeply, deeply regret introducing it to him.
[01:57:14.000 --> 01:57:17.040]   I feel like such a hypocrite Leo because it's like,
[01:57:17.040 --> 01:57:20.640]   yeah, I just sat down and spent like 60 hours
[01:57:20.640 --> 01:57:22.600]   beating Neo to write.
[01:57:22.600 --> 01:57:25.520]   So then complaining at Frank for playing like Pokemon Go
[01:57:25.520 --> 01:57:27.000]   is complete hypocrisy.
[01:57:27.000 --> 01:57:28.920]   But I'm glad Lisa plays Pokemon Go
[01:57:28.920 --> 01:57:30.680]   'cause she can't say anything about my Valheim.
[01:57:30.680 --> 01:57:34.960]   So you know what, we were at the bookstore yesterday
[01:57:34.960 --> 01:57:37.440]   and the clerk says, oh, I like your key chain.
[01:57:37.440 --> 01:57:38.920]   It's a Pokeball.
[01:57:38.920 --> 01:57:40.480]   And Lisa said, oh, that's no key chain.
[01:57:40.480 --> 01:57:41.680]   That's an actual Pokeball.
[01:57:41.680 --> 01:57:42.840]   I'm playing Pokemon Go.
[01:57:42.840 --> 01:57:43.680]   It's a walker.
[01:57:43.680 --> 01:57:45.480]   (laughing)
[01:57:45.480 --> 01:57:48.000]   The clerk thought that was super cool.
[01:57:48.000 --> 01:57:51.680]   One last story that we're gonna wrap it up
[01:57:51.680 --> 01:57:54.760]   for the 4th of July weekend, which is coming strong.
[01:57:54.760 --> 01:57:57.720]   I hope there's some hot dogs and hamburgers on the grill.
[01:57:57.720 --> 01:58:02.360]   Tim Berners-Lee, as you know, offered an NFT
[01:58:02.360 --> 01:58:06.600]   for his source code for the World Wide Web.
[01:58:07.360 --> 01:58:09.400]   It has been sold.
[01:58:09.400 --> 01:58:14.400]   The final bid, $5.4 million.
[01:58:14.400 --> 01:58:17.960]   Unidentified buyer, he sold it at Sotheby's.
[01:58:17.960 --> 01:58:21.520]   The opening bid was $1,000 back in June 23rd.
[01:58:21.520 --> 01:58:26.000]   There were a flurry of bids in the closing minutes.
[01:58:26.000 --> 01:58:30.560]   Profits go towards causes chosen by Sir Tim and his wife.
[01:58:30.560 --> 01:58:32.960]   You would get a timestamp file of the source code,
[01:58:32.960 --> 01:58:36.040]   an animated video of the code being written,
[01:58:36.040 --> 01:58:39.880]   a letter from Sir Tim and a digital poster of the cold code.
[01:58:39.880 --> 01:58:43.560]   I think whoever bought it probably didn't buy it to make
[01:58:43.560 --> 01:58:45.760]   money on it, but just to support his causes.
[01:58:45.760 --> 01:58:49.600]   But he should be aware there is an HTML error
[01:58:49.600 --> 01:58:51.600]   in the code as written.
[01:58:51.600 --> 01:58:56.320]   Apparently when they made the animation,
[01:58:56.320 --> 01:58:59.320]   it encoded some of the HTML-
[01:58:59.320 --> 01:59:00.160]   - Oh no.
[01:59:00.160 --> 01:59:01.880]   - Left bracket, right bracket,
[01:59:01.880 --> 01:59:05.680]   as Ampersand LT and Ampersand GT.
[01:59:06.680 --> 01:59:09.640]   The poster has been corrected,
[01:59:09.640 --> 01:59:11.920]   and apparently they've fixed the animation.
[01:59:11.920 --> 01:59:15.120]   So, I don't know.
[01:59:15.120 --> 01:59:17.160]   The question now is,
[01:59:17.160 --> 01:59:21.200]   did the NFT that you bought, does that have the error?
[01:59:21.200 --> 01:59:23.760]   'Cause they can't change it now.
[01:59:23.760 --> 01:59:26.720]   So, I don't know.
[01:59:26.720 --> 01:59:28.640]   If it was, actually they can't change it
[01:59:28.640 --> 01:59:31.840]   once the NFT is created, you know, then that freezes it.
[01:59:31.840 --> 01:59:32.840]   - It's immutable.
[01:59:32.840 --> 01:59:34.080]   - It's immutable.
[01:59:34.080 --> 01:59:38.520]   So, it's my guess that that error will be in there.
[01:59:38.520 --> 01:59:43.520]   - Have you seen the website where you can sell,
[01:59:43.520 --> 01:59:46.120]   you know, I don't remember the name of this,
[01:59:46.120 --> 01:59:47.760]   and I feel like it might have been Kevin Rose,
[01:59:47.760 --> 01:59:48.600]   who told me.
[01:59:48.600 --> 01:59:49.880]   - Oh, I know what you're gonna tell me.
[01:59:49.880 --> 01:59:51.480]   - You can like sell your,
[01:59:51.480 --> 01:59:55.200]   you can have people trade on yourself on you?
[01:59:55.200 --> 01:59:56.320]   - Yes.
[01:59:56.320 --> 01:59:57.160]   - What is that called?
[01:59:57.160 --> 01:59:58.560]   - On your Twitter account.
[01:59:58.560 --> 01:59:59.480]   - Oh.
[01:59:59.480 --> 02:00:01.240]   - Yeah, what is that called?
[02:00:01.240 --> 02:00:02.060]   - I'm gonna look it up.
[02:00:02.060 --> 02:00:02.900]   'Cause I went on--
[02:00:02.900 --> 02:00:05.720]   - 'Cause after he told me, I was like,
[02:00:05.720 --> 02:00:06.960]   "All right, I'll do it.
[02:00:06.960 --> 02:00:07.800]   "I'll see."
[02:00:07.800 --> 02:00:08.960]   - Did you do it?
[02:00:08.960 --> 02:00:10.080]   - Yeah, it was demoralizing.
[02:00:10.080 --> 02:00:11.800]   Like literally nobody cared.
[02:00:11.800 --> 02:00:14.280]   - Well, you didn't get on Twitter to publicize it.
[02:00:14.280 --> 02:00:17.880]   - I don't see how I benefit at all.
[02:00:17.880 --> 02:00:19.800]   - You don't benefit in any way.
[02:00:19.800 --> 02:00:20.640]   - Okay, good.
[02:00:20.640 --> 02:00:22.720]   - Nobody gives you money.
[02:00:22.720 --> 02:00:25.720]   - Good, then it was, you know, in the algorithm
[02:00:25.720 --> 02:00:28.760]   where I assess my time spent.
[02:00:28.760 --> 02:00:29.600]   - It was.
[02:00:29.600 --> 02:00:30.880]   - And revenue generated.
[02:00:30.880 --> 02:00:32.440]   That was really, really good.
[02:00:32.440 --> 02:00:33.340]   - Yeah.
[02:00:33.340 --> 02:00:34.780]   I can't remember the company's name
[02:00:34.780 --> 02:00:35.620]   and I'm looking for it.
[02:00:35.620 --> 02:00:37.020]   We did talk about it.
[02:00:37.020 --> 02:00:37.860]   - Yeah, yeah.
[02:00:37.860 --> 02:00:38.700]   - Yeah.
[02:00:38.700 --> 02:00:39.540]   - Trying to remember.
[02:00:39.540 --> 02:00:40.380]   Anyways.
[02:00:40.380 --> 02:00:42.300]   - Do you talk to Kevin on a regular basis?
[02:00:42.300 --> 02:00:43.140]   - Every now and then, yeah.
[02:00:43.140 --> 02:00:46.500]   You know, after that episode, that last episode,
[02:00:46.500 --> 02:00:48.500]   I'm now sleeping on an eight sleep bed.
[02:00:48.500 --> 02:00:49.740]   - Nice.
[02:00:49.740 --> 02:00:50.580]   - Yeah, but it's not.
[02:00:50.580 --> 02:00:52.940]   - Kevin is a wealth of information on stuff like that.
[02:00:52.940 --> 02:00:56.220]   - They just pushed a new update to their app,
[02:00:56.220 --> 02:00:57.220]   which is terrible.
[02:00:57.220 --> 02:00:59.380]   So if anybody at eight sleep is listening,
[02:00:59.380 --> 02:01:01.980]   please, please, please fix the jankiness.
[02:01:01.980 --> 02:01:04.860]   It was good before and now it is challenging.
[02:01:04.860 --> 02:01:08.540]   - This is the problem with IOT mattresses.
[02:01:08.540 --> 02:01:09.940]   - Yeah.
[02:01:09.940 --> 02:01:13.260]   - The mattress succeeds, lives on, but the software.
[02:01:13.260 --> 02:01:14.100]   - The app fails.
[02:01:14.100 --> 02:01:15.580]   Yeah, yeah.
[02:01:15.580 --> 02:01:19.080]   I will tell you though, I sleep great,
[02:01:19.080 --> 02:01:21.820]   I sleep great with a thermal regulation.
[02:01:21.820 --> 02:01:24.020]   - That's cooling is very important, isn't it?
[02:01:24.020 --> 02:01:27.100]   - And heating, I cook myself awake.
[02:01:27.100 --> 02:01:28.340]   - Oh, that's interesting.
[02:01:28.340 --> 02:01:29.940]   So you chill yourself to sleep
[02:01:29.940 --> 02:01:31.660]   and you cook yourself to awake.
[02:01:31.660 --> 02:01:36.660]   - I do and I haven't woken up to an actual alarm in months.
[02:01:36.660 --> 02:01:38.820]   So I'm up at five, four, and every morning.
[02:01:38.820 --> 02:01:41.820]   - So the software, so the whole idea of these mattresses,
[02:01:41.820 --> 02:01:43.900]   eight sleep.com spell that,
[02:01:43.900 --> 02:01:48.020]   is it lets you control the temperature.
[02:01:48.020 --> 02:01:48.860]   - Wow.
[02:01:48.860 --> 02:01:52.940]   - In the mattress and with, it has heating and cooling coils.
[02:01:52.940 --> 02:01:55.780]   So the software you're saying, the software Amy says,
[02:01:55.780 --> 02:01:58.260]   okay, when you're going to sleep and it cools the mattress
[02:01:58.260 --> 02:02:00.180]   and then when you want to wake up and it heats it up.
[02:02:00.180 --> 02:02:02.140]   But it gives you a bunch of other metrics,
[02:02:02.140 --> 02:02:04.260]   like what your REM cycles were,
[02:02:04.260 --> 02:02:08.060]   what your respiration is, what your heartbeat is.
[02:02:08.060 --> 02:02:08.900]   Makes it, oh.
[02:02:08.900 --> 02:02:09.740]   - Wow.
[02:02:09.740 --> 02:02:10.580]   - So how to get more restful sleep.
[02:02:10.580 --> 02:02:12.180]   It's actually up until this,
[02:02:12.180 --> 02:02:14.220]   I'm sure they're going to fix whatever the problem was.
[02:02:14.220 --> 02:02:16.660]   And actually they would be a good advertiser for this show
[02:02:16.660 --> 02:02:18.140]   'cause I'm sure everybody listening is,
[02:02:18.140 --> 02:02:18.980]   this is their kind of show.
[02:02:18.980 --> 02:02:20.140]   - And you don't have to buy the mattress,
[02:02:20.140 --> 02:02:21.420]   you can just get a part of our cover.
[02:02:21.420 --> 02:02:23.100]   - No, no, in fact, because we love our,
[02:02:23.100 --> 02:02:24.460]   we have a really nice mattress.
[02:02:24.460 --> 02:02:26.100]   - Oh, so you didn't get the mattress, you got the cover?
[02:02:26.100 --> 02:02:28.220]   - No, I didn't, no, we just have a cover.
[02:02:28.220 --> 02:02:30.140]   - That's a lot more affordable, I'm sure.
[02:02:30.380 --> 02:02:32.180]   - It is, and if you've got a mattress
[02:02:32.180 --> 02:02:34.180]   that you really love, you don't need the whole thing.
[02:02:34.180 --> 02:02:35.420]   - Holy cow.
[02:02:35.420 --> 02:02:40.180]   I love my Casper, I'm not sure I must spend $1,895
[02:02:40.180 --> 02:02:42.460]   for a mattress pet, but I guess you need it
[02:02:42.460 --> 02:02:43.620]   because it's got a big deal.
[02:02:43.620 --> 02:02:44.460]   - I'm telling you.
[02:02:44.460 --> 02:02:45.540]   - And all that stuff?
[02:02:45.540 --> 02:02:48.900]   - My husband and I both, and he has a weighted
[02:02:48.900 --> 02:02:50.340]   thermal blanket and I just have a,
[02:02:50.340 --> 02:02:52.100]   I sleep under a 20 pound blanket.
[02:02:52.100 --> 02:02:52.940]   - Yeah.
[02:02:52.940 --> 02:02:54.460]   - I'm sleeping better than I've ever slept
[02:02:54.460 --> 02:02:55.940]   and I had chronic insomnia.
[02:02:55.940 --> 02:02:56.780]   - Wow.
[02:02:56.780 --> 02:02:58.060]   - What is it worth to sleep well?
[02:02:58.060 --> 02:02:58.900]   I mean, absolutely.
[02:02:58.900 --> 02:02:59.940]   - Yeah, definitely.
[02:02:59.940 --> 02:03:00.780]   - Any price, right?
[02:03:00.780 --> 02:03:04.940]   - Sleep tech is under, if you're like in the no,
[02:03:04.940 --> 02:03:08.780]   then you be kind of super geek overnight, but you spend--
[02:03:08.780 --> 02:03:09.780]   - How hot?
[02:03:09.780 --> 02:03:13.060]   - How hot do you have to get it to be to wake up?
[02:03:13.060 --> 02:03:14.980]   - So I have mindset to plus eight,
[02:03:14.980 --> 02:03:16.580]   I don't know what the actual temperature is,
[02:03:16.580 --> 02:03:17.980]   but I think it's a little warm.
[02:03:17.980 --> 02:03:18.820]   - It's a little warm.
[02:03:18.820 --> 02:03:20.140]   - It's not like-- - 115 degrees.
[02:03:20.140 --> 02:03:22.380]   - Okay, so it's eight above your body temperature.
[02:03:22.380 --> 02:03:25.700]   It's not about making me so uncomfortable.
[02:03:25.700 --> 02:03:26.580]   Oh, you do get hot.
[02:03:26.580 --> 02:03:28.740]   - Yeah, no, no, I get like so uncomfortable
[02:03:28.740 --> 02:03:31.540]   but I don't know the act, it might be 120, I don't know.
[02:03:31.540 --> 02:03:35.340]   But no, no, it's like you wanna be up 'cause you're--
[02:03:35.340 --> 02:03:37.140]   - It's too hot.
[02:03:37.140 --> 02:03:39.180]   - Yeah, but you wake up and you're like,
[02:03:39.180 --> 02:03:42.940]   I'm like awake, I'm ready, I'm well-rested.
[02:03:42.940 --> 02:03:43.780]   - Very good.
[02:03:43.780 --> 02:03:45.780]   - Versus like jolting awake to an alarm clock
[02:03:45.780 --> 02:03:47.380]   when your body's not quite,
[02:03:47.380 --> 02:03:49.220]   you know, you haven't gradually come out of your sleep.
[02:03:49.220 --> 02:03:50.700]   - I hate alarm clocks, yeah.
[02:03:50.700 --> 02:03:54.260]   So this was the number one way you got rid of your insomnia
[02:03:54.260 --> 02:03:55.700]   or there were other.
[02:03:55.700 --> 02:03:56.860]   - No, no, no.
[02:03:56.860 --> 02:03:59.660]   - Years of cognitive behavioral therapy.
[02:03:59.660 --> 02:04:01.420]   Also-- - My son is going through this.
[02:04:01.420 --> 02:04:03.940]   He cannot sleep all night, he cannot.
[02:04:03.940 --> 02:04:05.860]   He lies awake a long, long time.
[02:04:05.860 --> 02:04:09.340]   And I feel like he's suffering because of it, so anything.
[02:04:09.340 --> 02:04:12.140]   - So, bows make some--
[02:04:12.140 --> 02:04:13.460]   - I have those.
[02:04:13.460 --> 02:04:15.740]   - Those ridiculous earbuds, it makes sounds.
[02:04:15.740 --> 02:04:16.940]   - Yeah, I also use those.
[02:04:16.940 --> 02:04:19.700]   I sleep, I work with brown noise during the day
[02:04:19.700 --> 02:04:21.980]   and I sleep with pink noise at night.
[02:04:21.980 --> 02:04:22.820]   - It's very--
[02:04:22.820 --> 02:04:24.100]   - See, I found the trick,
[02:04:24.100 --> 02:04:25.660]   the way I hacked my insomnia
[02:04:25.660 --> 02:04:27.460]   'cause Amy, I'm exactly like you.
[02:04:27.460 --> 02:04:30.100]   I just had fought it for years.
[02:04:30.100 --> 02:04:32.340]   Is I found out if I get an odd book
[02:04:32.340 --> 02:04:34.660]   that I really, really, really know well.
[02:04:34.660 --> 02:04:36.620]   Like something I've read so many times,
[02:04:36.620 --> 02:04:38.700]   I kind of can guess what the next sentence is going to be.
[02:04:38.700 --> 02:04:40.100]   - Like the expanse, yes.
[02:04:40.100 --> 02:04:43.300]   - Like the expanse, this is why I've listened to it so much.
[02:04:43.300 --> 02:04:46.260]   What I do is I crack an audible file.
[02:04:46.260 --> 02:04:47.860]   I hope I can say that on here.
[02:04:47.860 --> 02:04:49.580]   I own it legally.
[02:04:49.580 --> 02:04:53.460]   And then turn into an MP3 and I put it on an iPod touch.
[02:04:53.460 --> 02:04:55.780]   And then I get earbuds
[02:04:55.780 --> 02:04:59.660]   and I listen to this novel again and again all night long.
[02:04:59.660 --> 02:05:03.500]   And the way that reason the hack works
[02:05:03.500 --> 02:05:05.420]   is because when you get busy brain,
[02:05:05.420 --> 02:05:07.060]   like three in the morning,
[02:05:07.060 --> 02:05:08.340]   it's because you're thinking like,
[02:05:08.340 --> 02:05:09.500]   oh, I've gotta do this tomorrow.
[02:05:09.500 --> 02:05:10.500]   I've gotta do this tomorrow.
[02:05:10.500 --> 02:05:12.060]   I've gotta do this tomorrow.
[02:05:12.060 --> 02:05:16.060]   But if it's a story you've listened to a ton,
[02:05:16.060 --> 02:05:17.780]   then you can kind of follow it,
[02:05:17.780 --> 02:05:19.220]   but you can also drift off
[02:05:19.220 --> 02:05:21.420]   and just start thinking about random things.
[02:05:21.420 --> 02:05:24.540]   And this has been the magic bullet from the insomnia.
[02:05:24.540 --> 02:05:26.780]   - I think that's like a linguistic.
[02:05:26.780 --> 02:05:29.620]   - But I don't wanna miss anything.
[02:05:29.620 --> 02:05:30.780]   So I love that idea.
[02:05:30.780 --> 02:05:32.700]   Listen to something you already heard.
[02:05:32.700 --> 02:05:33.780]   - Yeah.
[02:05:33.780 --> 02:05:37.180]   It's linguistic counting, right?
[02:05:37.180 --> 02:05:39.300]   - It's counting sheep you mean?
[02:05:39.300 --> 02:05:41.300]   - Right, most people who count already know how to count.
[02:05:41.300 --> 02:05:43.580]   You already know sequentially what's coming next.
[02:05:43.580 --> 02:05:46.940]   So it's reducing the processing power.
[02:05:46.940 --> 02:05:50.380]   And when you're focused on a single thing,
[02:05:50.380 --> 02:05:53.820]   all of the chatters gets quiet.
[02:05:53.820 --> 02:05:55.620]   - Chatter is the problem, isn't it?
[02:05:55.620 --> 02:05:56.620]   - Chatter is the problem.
[02:05:56.620 --> 02:05:58.500]   - Yeah, I find as I get older,
[02:05:58.500 --> 02:06:00.500]   my mind is just an empty slate now.
[02:06:00.500 --> 02:06:03.140]   So I just, I go.
[02:06:03.140 --> 02:06:04.100]   - Hey Leo, before we go,
[02:06:04.100 --> 02:06:05.580]   can I ask you one last question?
[02:06:05.580 --> 02:06:07.140]   - Yes.
[02:06:07.140 --> 02:06:09.380]   - So have you NFTED?
[02:06:09.380 --> 02:06:10.220]   Is there a NFTED?
[02:06:10.220 --> 02:06:11.580]   - I have never NFTED.
[02:06:11.580 --> 02:06:12.900]   (laughing)
[02:06:12.900 --> 02:06:13.740]   I am proud to say.
[02:06:13.740 --> 02:06:14.580]   - Are you going to?
[02:06:14.580 --> 02:06:16.460]   - Never gonna, we,
[02:06:16.460 --> 02:06:19.460]   John dug up a bunch of old recordings.
[02:06:19.460 --> 02:06:21.700]   He said, you know, we could turn these into NFTs.
[02:06:21.700 --> 02:06:25.860]   You know, the first Twitter me exploding my ball or whatever.
[02:06:25.860 --> 02:06:29.300]   And I just, it feels a little scammy to me.
[02:06:29.300 --> 02:06:31.460]   So I didn't, why do you wanna know?
[02:06:31.460 --> 02:06:32.780]   Do you wanna make an NFT?
[02:06:32.780 --> 02:06:34.780]   - No, I was just curious.
[02:06:34.780 --> 02:06:37.340]   It feels a little, you know, of the moment.
[02:06:37.340 --> 02:06:38.940]   - It's a little of the moment.
[02:06:38.940 --> 02:06:39.780]   - Yeah.
[02:06:39.780 --> 02:06:41.340]   - Actually, funny you should say that
[02:06:41.340 --> 02:06:45.660]   because remember when Kevin was immersing himself
[02:06:45.660 --> 02:06:47.420]   in liquid nitrogen or something?
[02:06:47.420 --> 02:06:48.260]   - Yeah, yeah.
[02:06:48.260 --> 02:06:50.060]   - That was a big fad, right?
[02:06:50.060 --> 02:06:52.420]   A friend of mine just rented a space in Marin
[02:06:52.420 --> 02:06:56.100]   that was a former cryo facility.
[02:06:56.100 --> 02:06:57.060]   And they just left.
[02:06:57.060 --> 02:07:01.060]   They left 250 bathrobes, all the tanks, everything.
[02:07:01.060 --> 02:07:02.460]   Just they skipped town.
[02:07:02.460 --> 02:07:03.300]   It was over.
[02:07:03.300 --> 02:07:04.500]   The whole fad is gone.
[02:07:04.500 --> 02:07:05.340]   And I think Marin--
[02:07:05.340 --> 02:07:06.260]   - This is not a thing people aren't freezing--
[02:07:06.260 --> 02:07:07.700]   - I think Marin is it, yeah.
[02:07:07.700 --> 02:07:12.340]   I think Marin is a bellwether for the future of cryo tech,
[02:07:12.340 --> 02:07:13.420]   cryofitness.
[02:07:13.420 --> 02:07:15.180]   - Oh my goodness.
[02:07:15.180 --> 02:07:16.740]   - Actually, misfuturist.
[02:07:16.740 --> 02:07:17.620]   How about this one?
[02:07:17.620 --> 02:07:19.780]   I'm wondering, we've been talking about this a lot
[02:07:19.780 --> 02:07:21.780]   in the family.
[02:07:21.780 --> 02:07:25.740]   That grown meat, there is a Israeli company
[02:07:25.740 --> 02:07:29.260]   about that is looking for FDA approval
[02:07:29.260 --> 02:07:34.260]   to start growing meat in bio reactors.
[02:07:34.260 --> 02:07:37.100]   Future meat is talking with US regulators
[02:07:37.100 --> 02:07:39.060]   to start offering its products and restaurants
[02:07:39.060 --> 02:07:41.260]   by the end of next year.
[02:07:41.260 --> 02:07:43.420]   Cellular meat.
[02:07:43.420 --> 02:07:46.140]   - So this is actually kind of awesome.
[02:07:46.140 --> 02:07:48.860]   It is not the same thing as a plant-based burger
[02:07:48.860 --> 02:07:50.700]   that you would get from impossible.
[02:07:50.700 --> 02:07:54.340]   So this is meat cultured from the--
[02:07:54.340 --> 02:07:58.220]   So it shares genetic material with the original animal.
[02:07:58.220 --> 02:08:02.180]   So whether that's a chicken or a salmon or a beef.
[02:08:02.180 --> 02:08:05.860]   And rather than it growing in a energy-intensive,
[02:08:05.860 --> 02:08:10.460]   resource-intensive, oftentimes incredibly inhumane space,
[02:08:10.460 --> 02:08:11.980]   instead it's--
[02:08:11.980 --> 02:08:12.820]   - It's just a fat.
[02:08:12.820 --> 02:08:13.660]   - It's brewed.
[02:08:13.660 --> 02:08:15.660]   It's brewed in a bio reactor.
[02:08:15.660 --> 02:08:18.900]   And chicken nuggets have already gone on sale.
[02:08:18.900 --> 02:08:20.580]   - In Singapore, you can get a chicken--
[02:08:20.580 --> 02:08:22.300]   - That's right. - Eat just chicken nugget.
[02:08:22.300 --> 02:08:23.140]   - That's right.
[02:08:23.140 --> 02:08:25.860]   So it is chicken meat, but it never had a heartbeat.
[02:08:25.860 --> 02:08:27.740]   It never came from an actual animal.
[02:08:27.740 --> 02:08:30.660]   And I will probably get in a lot of trouble
[02:08:30.660 --> 02:08:34.100]   for saying this on your show, but I'm gonna do it anyways.
[02:08:34.100 --> 02:08:35.940]   In the books that I just wrote,
[02:08:35.940 --> 02:08:40.460]   there's a scenario chapters in the middle
[02:08:40.460 --> 02:08:43.420]   to describe what is the future of synthetic biology,
[02:08:43.420 --> 02:08:45.660]   which is what we're talking about, look like.
[02:08:45.660 --> 02:08:48.340]   And I wanted to write a chapter,
[02:08:48.340 --> 02:08:51.300]   a scenario set in the 2030s that had to do
[02:08:51.300 --> 02:08:56.380]   with how we will brew our food,
[02:08:56.380 --> 02:08:59.260]   rather than growing it on a traditional farm.
[02:08:59.260 --> 02:09:03.620]   And so that chapter is called Akira Gold's
[02:09:03.620 --> 02:09:05.940]   Where to Eat, 2032.
[02:09:05.940 --> 02:09:07.900]   And it's a restaurant guide.
[02:09:07.900 --> 02:09:08.900]   It's a restaurant guide.
[02:09:08.900 --> 02:09:11.820]   And it rates the best bio reactors
[02:09:11.820 --> 02:09:13.660]   and the best molecular whiskeys.
[02:09:13.660 --> 02:09:16.020]   - Do you think people will be making it in the back room?
[02:09:16.020 --> 02:09:17.300]   Like they'll be growing--
[02:09:17.300 --> 02:09:19.020]   - No, it's not that.
[02:09:19.020 --> 02:09:20.660]   It has to be highly sanitized.
[02:09:20.660 --> 02:09:23.700]   It requires robots, 5G, high-tech stuff.
[02:09:23.700 --> 02:09:26.460]   - Plus, if you just grew undifferentiated
[02:09:26.460 --> 02:09:28.460]   chicken muscle cells.
[02:09:28.460 --> 02:09:29.500]   - Right, it's not that.
[02:09:29.500 --> 02:09:30.660]   - That wouldn't be any good.
[02:09:30.660 --> 02:09:35.660]   I mean, meat has to have more than just the same cells.
[02:09:35.660 --> 02:09:37.420]   - Right, this is not gross.
[02:09:37.420 --> 02:09:38.620]   This is, I mean, I know it sounds--
[02:09:38.620 --> 02:09:39.860]   - Oh, I don't even mean it was gross.
[02:09:39.860 --> 02:09:42.420]   It just wouldn't, we need fat, we need skin.
[02:09:42.420 --> 02:09:44.460]   - Yeah, but you can program that, right?
[02:09:44.460 --> 02:09:47.220]   So you could genetically engineer marbling.
[02:09:47.220 --> 02:09:48.420]   - You can. - I mean, it's very--
[02:09:48.420 --> 02:09:50.100]   - That's the question. - You can't.
[02:09:50.100 --> 02:09:51.820]   You will be able to.
[02:09:51.820 --> 02:09:56.260]   And it's plausible, Leo, that 10 years from now,
[02:09:56.260 --> 02:09:57.540]   - Oh, I'm ready. - the freshest,
[02:09:57.540 --> 02:10:01.820]   the freshest sushi that you can get
[02:10:01.820 --> 02:10:04.780]   will not necessarily be flown in from Japan.
[02:10:04.780 --> 02:10:06.340]   - We're overfishing. - That will be thrown in a bio.
[02:10:06.340 --> 02:10:08.380]   - We're killing our oceans.
[02:10:08.380 --> 02:10:09.580]   - That's right. - That's right.
[02:10:09.580 --> 02:10:11.780]   - Cows are terrible for the environment.
[02:10:11.780 --> 02:10:13.860]   - That's right. - But we wanna eat meat.
[02:10:13.860 --> 02:10:15.540]   So this might be great.
[02:10:15.540 --> 02:10:18.700]   By the way, guess who the big investors are in future meat?
[02:10:18.700 --> 02:10:22.500]   Tyson Foods, Archer Daniels, Midland,
[02:10:22.500 --> 02:10:24.900]   big meat producers. - Yeah, yeah.
[02:10:24.900 --> 02:10:26.620]   - They see the future.
[02:10:26.620 --> 02:10:31.060]   - And again, this future can't come fast enough
[02:10:31.060 --> 02:10:31.940]   as far as I'm concerned.
[02:10:31.940 --> 02:10:33.820]   And I just wanna say one quick thing
[02:10:33.820 --> 02:10:36.420]   because people start to get squeamish
[02:10:36.420 --> 02:10:40.260]   and they conflate this science with GMO
[02:10:40.260 --> 02:10:43.340]   and genetically modified organisms,
[02:10:43.340 --> 02:10:45.220]   which we heard a lot about in the '90s
[02:10:45.220 --> 02:10:47.060]   became highly, highly politicized.
[02:10:47.060 --> 02:10:48.940]   - I hope you're debunking that in the book as well.
[02:10:48.940 --> 02:10:51.100]   - I absolutely am. - I absolutely am.
[02:10:51.100 --> 02:10:55.820]   And we have to keep our minds open here
[02:10:55.820 --> 02:10:58.060]   because we're not gonna make it alone
[02:10:58.060 --> 02:11:01.420]   on our own and the current conditions we have on this planet.
[02:11:01.420 --> 02:11:03.300]   And quite frankly, as exciting as it is
[02:11:03.300 --> 02:11:06.940]   that Bezos and Branson and Musk
[02:11:06.940 --> 02:11:11.500]   and everybody's lining up to go into space,
[02:11:11.500 --> 02:11:12.820]   I mean, how plausible is it
[02:11:12.820 --> 02:11:14.620]   that we're gonna lift everybody off this planet
[02:11:14.620 --> 02:11:16.820]   onto another one and somehow escape all of our problems?
[02:11:16.820 --> 02:11:17.660]   We're not going to. - We're not.
[02:11:17.660 --> 02:11:21.460]   - So we have to come up with much more sophisticated solutions
[02:11:21.460 --> 02:11:23.060]   and synthetic biology.
[02:11:23.060 --> 02:11:27.580]   And what you're talking about, which is meat,
[02:11:27.580 --> 02:11:30.020]   cellular meat grown in a bio-reactor,
[02:11:30.020 --> 02:11:34.100]   brewed in a bio-reactor is something we ought to celebrate
[02:11:34.100 --> 02:11:35.660]   and be excited about.
[02:11:35.660 --> 02:11:37.220]   - The wonderful Amy Webb.
[02:11:37.220 --> 02:11:39.340]   I love the signals are talking.
[02:11:39.340 --> 02:11:41.100]   Her latest book is The Big Nine.
[02:11:41.100 --> 02:11:43.100]   What's the name of this new one and when is it gonna be out?
[02:11:43.100 --> 02:11:45.180]   You showed us the manuscript before the '70s.
[02:11:45.180 --> 02:11:46.020]   - Yeah. - Yeah.
[02:11:46.020 --> 02:11:46.860]   - You're done. - It's.
[02:11:46.860 --> 02:11:47.700]   - Are you done?
[02:11:47.700 --> 02:11:48.620]   - I'm done, I'm done.
[02:11:48.620 --> 02:11:50.560]   It's called the Genesis Machine.
[02:11:50.560 --> 02:11:53.500]   It launches on February 15th
[02:11:53.500 --> 02:11:57.540]   and some of the, we've gotten some unbelievably amazing
[02:11:57.540 --> 02:11:59.140]   reviews so far. - Okay.
[02:11:59.140 --> 02:12:01.580]   - I can't, I really can't wait for everybody to read it.
[02:12:01.580 --> 02:12:02.420]   - Well, we'll have you out.
[02:12:02.420 --> 02:12:05.060]   Let's do a special triangulation when it comes out
[02:12:05.060 --> 02:12:06.860]   'cause I would really love to spend some time.
[02:12:06.860 --> 02:12:11.020]   It's all about things like mRNA and CRISPR
[02:12:11.020 --> 02:12:13.260]   and all of the things that we're doing.
[02:12:13.260 --> 02:12:17.860]   How are we about to experience an elbow,
[02:12:17.860 --> 02:12:20.780]   a hockey stick, a revolution in biotech?
[02:12:20.780 --> 02:12:22.860]   - Yes, yes we are.
[02:12:22.860 --> 02:12:27.660]   So there's trillions of dollars invested in the bio-economy.
[02:12:27.660 --> 02:12:30.380]   The best way to sort of understand where we are right now
[02:12:30.380 --> 02:12:35.380]   is if you remember the story of Watson being on stage
[02:12:35.380 --> 02:12:38.460]   and like making the very first,
[02:12:38.460 --> 02:12:40.260]   or Alexander Graham Bell being on the stage
[02:12:40.260 --> 02:12:42.580]   making the first telephone call.
[02:12:42.580 --> 02:12:45.980]   You know, and everybody in Chickering Hall in New York
[02:12:45.980 --> 02:12:48.700]   and people watching this exchange
[02:12:48.700 --> 02:12:50.820]   and thinking it was some kind of hoax
[02:12:50.820 --> 02:12:54.620]   and they demanded to go behind the stage
[02:12:54.620 --> 02:12:55.980]   to see what was happening,
[02:12:55.980 --> 02:12:58.660]   to see the magical genie or whatever.
[02:12:58.660 --> 02:13:00.420]   You know, and when they realized it was real,
[02:13:00.420 --> 02:13:03.900]   it completely blew up everybody's mental model
[02:13:03.900 --> 02:13:06.660]   for what it meant to talk to each other.
[02:13:06.660 --> 02:13:10.460]   So sick, you know, and today there is no way
[02:13:10.460 --> 02:13:13.780]   to create a valuation for telecommunications.
[02:13:13.780 --> 02:13:14.900]   It is, right?
[02:13:14.900 --> 02:13:16.180]   It's a basic technology.
[02:13:16.180 --> 02:13:18.660]   It's a fundamental part of our economy.
[02:13:18.660 --> 02:13:21.420]   We're at the Chickering Hall stage of synthetic biology.
[02:13:21.420 --> 02:13:22.260]   - Wow.
[02:13:22.260 --> 02:13:24.860]   - Which means that you're gonna start to see significant
[02:13:24.860 --> 02:13:26.900]   development over the next decade.
[02:13:26.900 --> 02:13:27.980]   Some of that is investment,
[02:13:27.980 --> 02:13:29.500]   but some of that is just compute.
[02:13:29.500 --> 02:13:32.500]   We finally have the machines that we need
[02:13:32.500 --> 02:13:34.620]   and now a proven business case and a use case
[02:13:34.620 --> 02:13:38.220]   because MNRA, the messenger RNA that's in Moderna
[02:13:38.220 --> 02:13:41.220]   and visor biointech was in development
[02:13:41.220 --> 02:13:43.140]   for 10 years for a different purpose.
[02:13:43.140 --> 02:13:45.260]   But that is synth bio.
[02:13:45.260 --> 02:13:46.100]   So very--
[02:13:46.100 --> 02:13:49.660]   - It's saved our planet or is it the process of it?
[02:13:49.660 --> 02:13:52.540]   - As far as I'm concerned, like AI is important,
[02:13:52.540 --> 02:13:53.900]   it's the third era of computing,
[02:13:53.900 --> 02:13:56.940]   but the most important technology of the 21st century
[02:13:56.940 --> 02:13:58.820]   is biology.
[02:13:58.820 --> 02:14:01.020]   - Can't wait to read the book.
[02:14:01.020 --> 02:14:02.780]   Always it's real to have you on.
[02:14:02.780 --> 02:14:03.860]   Go ahead, Brianna.
[02:14:03.860 --> 02:14:05.260]   - No, no, no, no.
[02:14:05.260 --> 02:14:08.500]   Yeah, the thing is this is Frank's Field
[02:14:08.500 --> 02:14:10.980]   and sometimes I can't talk about stuff,
[02:14:10.980 --> 02:14:13.700]   but I just agree with everything you just said.
[02:14:13.700 --> 02:14:15.900]   - He's working in this arena.
[02:14:15.900 --> 02:14:18.300]   - This is what has come to us from living.
[02:14:18.300 --> 02:14:19.140]   - Oh, yeah.
[02:14:19.140 --> 02:14:19.980]   - That's correct.
[02:14:19.980 --> 02:14:20.820]   - We should get together.
[02:14:20.820 --> 02:14:22.660]   - In a few separate chat.
[02:14:22.660 --> 02:14:24.900]   - We should, like when Frank is,
[02:14:24.900 --> 02:14:27.580]   when he was working on the delivery aspect
[02:14:27.580 --> 02:14:29.740]   of the Moderna vaccine, I couldn't talk about that.
[02:14:29.740 --> 02:14:32.220]   And there are other things I can't talk about today.
[02:14:32.220 --> 02:14:33.220]   But I agree with you,
[02:14:33.220 --> 02:14:35.300]   this is an interesting area of science.
[02:14:35.300 --> 02:14:37.420]   - Well, please thank Frank because thanks to him,
[02:14:37.420 --> 02:14:39.100]   I'm gonna be able to go to Hawaii in a week
[02:14:39.100 --> 02:14:40.980]   and I'm gonna be able to go to Mexico in the fall.
[02:14:40.980 --> 02:14:41.820]   - There you go.
[02:14:41.820 --> 02:14:42.660]   - And be able to go to Singapore
[02:14:42.660 --> 02:14:45.220]   and have a chicken McNugget in January.
[02:14:45.220 --> 02:14:46.860]   - I'll be there, I'll meet you there.
[02:14:46.860 --> 02:14:49.500]   - Oh yeah, when are you gonna be there?
[02:14:49.500 --> 02:14:50.820]   - I don't know, I don't know how many plants,
[02:14:50.820 --> 02:14:51.940]   but I would go.
[02:14:51.940 --> 02:14:53.660]   - Oh, okay.
[02:14:53.660 --> 02:14:56.420]   I have to find this store wherever it is
[02:14:56.420 --> 02:14:58.340]   and get, I wanna taste this meat
[02:14:58.340 --> 02:14:59.940]   because I'm fascinated by this.
[02:14:59.940 --> 02:15:02.180]   We're gonna be in Singapore in early February.
[02:15:02.180 --> 02:15:03.020]   - Oh, okay, bye.
[02:15:03.020 --> 02:15:03.860]   - How on times day?
[02:15:03.860 --> 02:15:06.820]   - So I will, I'll send you some stuff.
[02:15:06.820 --> 02:15:07.660]   - Good.
[02:15:07.660 --> 02:15:10.180]   Valentine's Day in Singapore, meet us.
[02:15:10.180 --> 02:15:11.660]   YouTube, Brianna.
[02:15:11.660 --> 02:15:12.660]   (laughing)
[02:15:12.660 --> 02:15:14.580]   - Hello, M-E-A-T-S.
[02:15:14.580 --> 02:15:16.220]   - Meet us.
[02:15:16.220 --> 02:15:19.700]   Brianna Wu is at Rebellion Pack.
[02:15:19.700 --> 02:15:20.520]   Give us a plug.
[02:15:20.520 --> 02:15:22.400]   Tell us about Rebellion Pack.
[02:15:22.400 --> 02:15:26.120]   - So, you know, just to really be honest with you, Leo,
[02:15:26.120 --> 02:15:28.240]   I would rather be developing games right now,
[02:15:28.240 --> 02:15:30.880]   but I'm looking at the midterm numbers
[02:15:30.880 --> 02:15:32.080]   and just hearing me.
[02:15:32.080 --> 02:15:32.920]   - Yeah, I agree with you.
[02:15:32.920 --> 02:15:36.080]   - And, you know, something I learned running for office
[02:15:36.080 --> 02:15:38.760]   is you really underestimate,
[02:15:38.760 --> 02:15:43.120]   you did no idea how bad a lot of political operatives are
[02:15:43.120 --> 02:15:43.960]   and their levels of technical--
[02:15:43.960 --> 02:15:45.320]   - I think we're starting to learn.
[02:15:45.320 --> 02:15:49.480]   - So, I don't know if I agree with that actually
[02:15:49.480 --> 02:15:50.320]   from what I see.
[02:15:50.320 --> 02:15:53.360]   So, for what rebellion is doing,
[02:15:53.360 --> 02:15:56.280]   we actually just, it's almost like a seed round.
[02:15:56.280 --> 02:15:57.640]   We're working on a really exciting,
[02:15:57.640 --> 02:16:00.280]   we have a major donor that got involved
[02:16:00.280 --> 02:16:02.640]   in letting us do a bunch of stuff
[02:16:02.640 --> 02:16:04.960]   with a really big data play
[02:16:04.960 --> 02:16:08.360]   to start using these really advanced analytics
[02:16:08.360 --> 02:16:09.680]   for political messaging
[02:16:09.680 --> 02:16:14.000]   and basically getting voters out to show up.
[02:16:14.000 --> 02:16:15.920]   That's the $100 million question.
[02:16:15.920 --> 02:16:20.360]   And, yeah, I feel like if we don't seriously figure this out,
[02:16:20.360 --> 02:16:22.880]   I have real fears about the country.
[02:16:22.880 --> 02:16:25.120]   So, if you want to support us,
[02:16:25.120 --> 02:16:28.000]   you can do that by going to helptherebellion.com.
[02:16:28.000 --> 02:16:29.520]   Yeah, I look forward to a future show
[02:16:29.520 --> 02:16:31.720]   where I could talk to you about the nitty gritty
[02:16:31.720 --> 02:16:32.560]   of what we're doing
[02:16:32.560 --> 02:16:34.560]   'cause it's really awesome stuff, Leo.
[02:16:34.560 --> 02:16:36.560]   - And you are working on games, right?
[02:16:36.560 --> 02:16:37.560]   I mean, last time we talked--
[02:16:37.560 --> 02:16:38.560]   - Yeah. - Yeah.
[02:16:38.560 --> 02:16:40.880]   - Yeah, you know, I got it in religion five
[02:16:40.880 --> 02:16:42.640]   and it's one of these things I'm working on
[02:16:42.640 --> 02:16:44.440]   in the background.
[02:16:44.440 --> 02:16:46.880]   - Just to be really frank with you,
[02:16:46.880 --> 02:16:49.160]   right after a race,
[02:16:49.160 --> 02:16:51.400]   it's a little slower for the first year.
[02:16:51.400 --> 02:16:53.520]   So I actually have spare time right now.
[02:16:53.520 --> 02:16:54.360]   - Nice. - So,
[02:16:54.360 --> 02:16:57.080]   I'm looking on real engine,
[02:16:57.080 --> 02:16:58.760]   seeing if I want to start a game studio.
[02:16:58.760 --> 02:17:01.680]   But, yeah, then we start working on this data play
[02:17:01.680 --> 02:17:02.520]   at rebellion.
[02:17:02.520 --> 02:17:04.120]   It is crazy stuff.
[02:17:04.120 --> 02:17:06.240]   - One of the things, unfortunately, that happens,
[02:17:06.240 --> 02:17:07.480]   especially as you get older in life,
[02:17:07.480 --> 02:17:09.120]   is you realize you're not gonna have enough time
[02:17:09.120 --> 02:17:10.680]   to do everything you want to do.
[02:17:10.680 --> 02:17:12.360]   - Yeah, yep, absolutely.
[02:17:12.360 --> 02:17:13.600]   - And I don't want to choose.
[02:17:13.600 --> 02:17:15.520]   (laughing)
[02:17:15.520 --> 02:17:19.080]   If you write a Valheim, the next Valheim,
[02:17:19.080 --> 02:17:20.520]   I'll be ready 'cause I am done.
[02:17:20.520 --> 02:17:21.720]   - There we go. - I finish the game
[02:17:21.720 --> 02:17:23.280]   and I'm very depressed.
[02:17:23.280 --> 02:17:25.320]   I have to find my next game.
[02:17:25.320 --> 02:17:28.840]   Brianna will always a pleasure, Amy Webb.
[02:17:28.840 --> 02:17:31.080]   When we were trying to figure out what we should do
[02:17:31.080 --> 02:17:32.840]   for the third of July,
[02:17:32.840 --> 02:17:34.680]   Special Saturday edition of Twit.
[02:17:34.680 --> 02:17:37.000]   I said, "Look, just get Amy and Brianna on
[02:17:37.000 --> 02:17:39.280]   and my job will be very easy.
[02:17:39.280 --> 02:17:43.640]   You two are amazing, so smart, so interesting.
[02:17:43.640 --> 02:17:46.160]   Thank you so much for spending this Saturday with me.
[02:17:46.160 --> 02:17:47.560]   I appreciate it. - Thank you.
[02:17:47.560 --> 02:17:48.400]   - Thanks for having us.
[02:17:48.400 --> 02:17:49.760]   - Yeah, great to have you.
[02:17:49.760 --> 02:17:53.760]   We do Twit normally on a Sunday afternoon.
[02:17:53.760 --> 02:17:55.920]   In fact, we'll be back on our Sunday afternoon schedule
[02:17:55.920 --> 02:18:00.920]   starting next week, 2.30 PM Pacific 530 Eastern, 2.130 UTC.
[02:18:00.920 --> 02:18:04.040]   You can watch us do it live.
[02:18:04.040 --> 02:18:05.840]   There's a live stream, audio or video
[02:18:05.840 --> 02:18:07.280]   at twit.tv/live.
[02:18:07.280 --> 02:18:12.280]   If you're watching live, chat live at irc.twit.tv.
[02:18:12.280 --> 02:18:15.800]   Club Twit members are also chatting live in our Discord.
[02:18:15.800 --> 02:18:18.000]   That's another place you can go.
[02:18:18.000 --> 02:18:19.920]   We sometimes take comments and questions
[02:18:19.920 --> 02:18:21.440]   from our Discord stage.
[02:18:21.440 --> 02:18:25.600]   We have lots of great conversations all day, all night.
[02:18:25.600 --> 02:18:27.520]   And of course, add free versions of the show,
[02:18:27.520 --> 02:18:29.600]   twit.tv/clubtwit.
[02:18:29.600 --> 02:18:32.200]   After the fact on demand versions of every show we do,
[02:18:32.200 --> 02:18:33.520]   including this one are available
[02:18:33.520 --> 02:18:36.520]   in a number of places on the web at our website,
[02:18:36.520 --> 02:18:38.320]   twit.tv.
[02:18:38.320 --> 02:18:40.320]   There's a YouTube channel for each of our shows,
[02:18:40.320 --> 02:18:41.160]   including this one.
[02:18:41.160 --> 02:18:44.680]   You can watch the shows there, not in 8K, I'm sorry to say,
[02:18:44.680 --> 02:18:48.080]   but not even in 4K, but 720p, okay?
[02:18:48.080 --> 02:18:50.320]   I mean, there's not that much to see anyway.
[02:18:50.320 --> 02:18:53.280]   You can also subscribe in your favorite podcast player,
[02:18:53.280 --> 02:18:54.800]   audio and video.
[02:18:54.800 --> 02:18:57.160]   Please do, that way you'll get it automatically
[02:18:57.160 --> 02:18:58.760]   the minute it's available.
[02:18:58.760 --> 02:19:00.640]   And if you would, leave us a review.
[02:19:00.640 --> 02:19:02.280]   Five-star review, be much appreciated.
[02:19:02.280 --> 02:19:04.640]   Spread the word that helps us quite a bit.
[02:19:04.640 --> 02:19:05.600]   Thank you all for being here.
[02:19:05.600 --> 02:19:08.280]   Have a great, for those of you living in the US,
[02:19:08.280 --> 02:19:10.400]   have a great Independence Day.
[02:19:10.400 --> 02:19:12.040]   Enjoy your 4th of July.
[02:19:12.040 --> 02:19:16.040]   I think this year there's some real cause for celebration
[02:19:16.040 --> 02:19:17.200]   and reflection.
[02:19:17.200 --> 02:19:18.760]   And I hope you do both.
[02:19:18.760 --> 02:19:20.920]   And I'll be back next week.
[02:19:20.920 --> 02:19:21.760]   Thanks everybody.
[02:19:21.760 --> 02:19:23.880]   Another twit is in the can.
[02:19:23.880 --> 02:19:25.040]   - This is amazing.
[02:19:25.040 --> 02:19:27.620]   (upbeat music)
[02:19:27.620 --> 02:19:28.560]   - Do it the twit.
[02:19:28.560 --> 02:19:29.400]   - Do it the twit.
[02:19:29.400 --> 02:19:30.400]   - All right.
[02:19:30.400 --> 02:19:32.280]   - Do it the twit, baby.
[02:19:32.280 --> 02:19:33.120]   - Do it the twit.
[02:19:33.120 --> 02:19:34.120]   - All right.
[02:19:34.120 --> 02:19:35.560]   - Do it the twit, baby.
[02:19:35.560 --> 02:19:37.560]   Do as a Twitter.

