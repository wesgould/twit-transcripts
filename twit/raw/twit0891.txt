;FFMETADATA1
title=Use a Rivian? That's Nuts
artist=Leo Laporte, Paris Martineau, Glenn Fleishman, Dan Moren
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-09-05
track=891
language=English
genre=Podcast
comment=Apple event preview, USB 4, Stable Diffusion AI, fit CEOs
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:06.000]   It's time for Twit this week in Tech. I know it's gonna be a long one because I've got one of the best panels ever.
[00:00:06.000 --> 00:00:10.000]   Paris Martin, No. Glenn Fleischman and Dan Morin.
[00:00:10.000 --> 00:00:14.000]   Of course we're gonna talk about the upcoming Apple event, what to expect.
[00:00:14.000 --> 00:00:18.000]   And then a very deep conversation about A.I.R.
[00:00:18.000 --> 00:00:25.000]   Is it legit? And a doctor who used his truck to perform a vasectomy.
[00:00:25.000 --> 00:00:28.000]   It's not what you think. It's all coming up next.
[00:00:28.000 --> 00:00:31.000]   This is Twit.
[00:00:31.000 --> 00:00:33.000]   Podcasts you love.
[00:00:33.000 --> 00:00:35.000]   From people you trust.
[00:00:35.000 --> 00:00:38.000]   This is Twit.
[00:00:38.000 --> 00:00:46.000]   This is Twit. This week in Tech.
[00:00:46.000 --> 00:00:48.000]   Episode 891.
[00:00:48.000 --> 00:00:52.000]   Recorded Sunday September 4th, 2022.
[00:00:52.000 --> 00:00:56.000]   Use the Rivian? That's nuts.
[00:00:56.000 --> 00:00:59.000]   This week in Tech is brought to you by podium.
[00:00:59.000 --> 00:01:05.000]   Join more than 100,000 businesses that already use podium to streamline their customer interactions.
[00:01:05.000 --> 00:01:07.000]   See how podium can grow your business.
[00:01:07.000 --> 00:01:12.000]   Watch a demo today at podium.com/twit.
[00:01:12.000 --> 00:01:19.000]   And by ClickUp, the productivity platform that will save you one day a week on work guaranteed.
[00:01:19.000 --> 00:01:25.000]   Use a code Twit to get 15% off ClickUp's massive unlimited plan for a year.
[00:01:25.000 --> 00:01:29.000]   Meaning you can start reclaiming your time for under $5 a month.
[00:01:29.000 --> 00:01:32.000]   Sign up today at ClickUp.com, but hurry.
[00:01:32.000 --> 00:01:34.000]   This offer ends soon.
[00:01:34.000 --> 00:01:36.000]   And by Zapier.
[00:01:36.000 --> 00:01:43.000]   Zapier makes it easy to connect all your apps, automate routine tasks, and streamline your processes.
[00:01:43.000 --> 00:01:48.000]   Try Zapier for free today at Zapier.com/twit.
[00:01:48.000 --> 00:01:52.000]   And by Stamps.com.
[00:01:52.000 --> 00:01:54.000]   Get ahead of the holiday chaos this year.
[00:01:54.000 --> 00:01:56.000]   Sign up at Stamps.com.
[00:01:56.000 --> 00:01:58.000]   Click the microphone at the top of the page.
[00:01:58.000 --> 00:01:59.000]   Enter the code Twit.
[00:01:59.000 --> 00:02:07.000]   And you'll get a special offer that includes a four-week trial, plus free postage, and a digital scale.
[00:02:07.000 --> 00:02:14.000]   It's time for Twit this week in Tech, the show where we talk about the week's tech news.
[00:02:14.000 --> 00:02:18.000]   And, you know, sometimes we put shows together with great thought.
[00:02:18.000 --> 00:02:20.000]   And we really, you know, kind of balance it.
[00:02:20.000 --> 00:02:23.000]   Sometimes we just throw the names in the air and they come down.
[00:02:23.000 --> 00:02:24.000]   And sometimes that's better.
[00:02:24.000 --> 00:02:26.000]   This is one of those...
[00:02:26.000 --> 00:02:31.000]   Maybe, I don't know, maybe Jason Howell had an idea, but I am thrilled by this panel.
[00:02:31.000 --> 00:02:33.000]   Let's say hi to Paris Martenau.
[00:02:33.000 --> 00:02:35.000]   She was a reporter for the information.
[00:02:35.000 --> 00:02:36.000]   Always great to see her.
[00:02:36.000 --> 00:02:37.000]   Very crafty.
[00:02:37.000 --> 00:02:42.000]   Today, no sequined, no bejeweled mannequins in the background.
[00:02:42.000 --> 00:02:45.000]   I'm gonna, I'll move it in frame at the right.
[00:02:45.000 --> 00:02:46.000]   That's okay.
[00:02:46.000 --> 00:02:48.000]   I'm gonna keep people on their toes, you know.
[00:02:48.000 --> 00:02:54.000]   I bet you though, the rest of the panel will recognize something right behind you.
[00:02:54.000 --> 00:02:58.000]   And let's find out when we ask, when we say hello to...
[00:02:58.000 --> 00:03:00.000]   Yeah, Glenn Fleischman.
[00:03:00.000 --> 00:03:03.000]   I think he's, I think I recognize the chuckle from Glenn Dodd Fund.
[00:03:03.000 --> 00:03:12.000]   Long time reporter for a variety of Macintosh magazines, former Jeopardy contestant type historian.
[00:03:12.000 --> 00:03:13.000]   Hello, Glenn.
[00:03:13.000 --> 00:03:14.000]   Hello.
[00:03:14.000 --> 00:03:20.000]   Do I recognize a, I see the transparent speaker, I've forgotten the exact name of it.
[00:03:20.000 --> 00:03:21.000]   Is that an original...
[00:03:21.000 --> 00:03:25.000]   Was he the Carmen Carden, now we stick?
[00:03:25.000 --> 00:03:28.000]   Yes, I picked them up from...
[00:03:28.000 --> 00:03:32.000]   They are, and yet they sound fantastic and they look very cool.
[00:03:32.000 --> 00:03:33.000]   Sure.
[00:03:33.000 --> 00:03:39.000]   I've got them connected to a record player in a fun little mishmash of technology there.
[00:03:39.000 --> 00:03:43.000]   Is there a person standing behind those Harmon Carden sticks?
[00:03:43.000 --> 00:03:51.000]   That is a lamp shaped like a man with a uniquely placed light switch that I will leave to your eye.
[00:03:51.000 --> 00:03:53.000]   Did you make that?
[00:03:53.000 --> 00:03:58.000]   I did not, but I bought it from a vintage dealer who I really enjoy.
[00:03:58.000 --> 00:04:00.000]   And every time you flip it on, you just go...
[00:04:00.000 --> 00:04:05.000]   Every time you flip it on, you know, I'll often have people come over and they're like, "So where's the light switch?"
[00:04:05.000 --> 00:04:08.000]   And I'm like, "Guys, there's only one thing on here."
[00:04:08.000 --> 00:04:10.000]   There's only one place to look.
[00:04:10.000 --> 00:04:11.000]   Wow, that's his starter.
[00:04:11.000 --> 00:04:13.000]   Do you lay?
[00:04:13.000 --> 00:04:21.000]   Also with us great to have Dan Moore, and this is the, I think, one of the first things you've done off your paternity leave.
[00:04:21.000 --> 00:04:22.000]   Is that right?
[00:04:22.000 --> 00:04:25.000]   You first, you are my first stop on the return from our paternity leave.
[00:04:25.000 --> 00:04:26.000]   Yes.
[00:04:26.000 --> 00:04:27.000]   There you go.
[00:04:27.000 --> 00:04:33.000]   Six colors co-editor with Jason Snell, he has been off for a couple of months with a brand new baby.
[00:04:33.000 --> 00:04:34.000]   How old is a baby?
[00:04:34.000 --> 00:04:36.000]   Baby's about six weeks old.
[00:04:36.000 --> 00:04:38.000]   Oh, so it's not even a couple of months.
[00:04:38.000 --> 00:04:40.000]   How's it been?
[00:04:40.000 --> 00:04:41.000]   It's been tiring.
[00:04:41.000 --> 00:04:47.000]   I can need a better camera that doesn't show you the bags under my eyes, but other than that, it's been wonderful.
[00:04:47.000 --> 00:04:48.000]   I love it.
[00:04:48.000 --> 00:04:56.000]   Well, I hope even if you're on paternity leave, you write because we are waiting for volume four of the Galactic Cold War saga.
[00:04:56.000 --> 00:05:04.000]   I have here, you thankfully sent me the latest volume, which is book three, the Nova incident.
[00:05:04.000 --> 00:05:06.000]   All three of them.
[00:05:06.000 --> 00:05:09.000]   By the way, really good.
[00:05:09.000 --> 00:05:14.000]   I'll be honest, it's terrible to say, but I wasn't expecting much.
[00:05:14.000 --> 00:05:15.000]   Okay.
[00:05:15.000 --> 00:05:17.000]   I like my bar.
[00:05:17.000 --> 00:05:19.000]   You're starting off.
[00:05:19.000 --> 00:05:20.000]   Absolutely brutal.
[00:05:20.000 --> 00:05:21.000]   Let me explain.
[00:05:21.000 --> 00:05:25.000]   We're just at a really low bar.
[00:05:25.000 --> 00:05:29.000]   Not in any way a reflection on you, Dan.
[00:05:29.000 --> 00:05:35.000]   It's just that I know quite a few people in our business, you know, that's the old canard.
[00:05:35.000 --> 00:05:39.000]   I have everybody's going to novel in their desk drawer.
[00:05:39.000 --> 00:05:44.000]   The few people I know who have released those novels probably should not have.
[00:05:44.000 --> 00:05:50.000]   Now, did you start as a writer and then as a sci-fi writer and then become a tech writer?
[00:05:50.000 --> 00:05:54.000]   It was always a passion of mine.
[00:05:54.000 --> 00:05:56.000]   I got out of college and didn't really know what I was doing.
[00:05:56.000 --> 00:05:57.000]   I worked IT for a while.
[00:05:57.000 --> 00:05:59.000]   Then I started writing professionally.
[00:05:59.000 --> 00:06:04.000]   Once I had done that, I think it really helped me hone a lot of stuff about my fiction.
[00:06:04.000 --> 00:06:09.000]   I went back to it after spending my first couple of years in tech journalism because it felt
[00:06:09.000 --> 00:06:14.000]   like, "Oh, now I understand I work with deadlines and I've had to do this regularly and put
[00:06:14.000 --> 00:06:16.000]   in all these hours and you can't help but get better."
[00:06:16.000 --> 00:06:20.000]   I think at those kinds of tasks, if you do them enough.
[00:06:20.000 --> 00:06:22.000]   I think it kind of works together nicely.
[00:06:22.000 --> 00:06:24.000]   That's what I look for in a good novel.
[00:06:24.000 --> 00:06:26.000]   Somebody who makes deadlines.
[00:06:26.000 --> 00:06:29.000]   It's harder than you think.
[00:06:29.000 --> 00:06:32.000]   Hey, just ask George RR Martin, right?
[00:06:32.000 --> 00:06:33.000]   Exactly.
[00:06:33.000 --> 00:06:35.000]   That guy is really suffering for not making you deadlines.
[00:06:35.000 --> 00:06:36.000]   Let me tell you.
[00:06:36.000 --> 00:06:40.000]   Patrick Roth, there are quite a few in our...
[00:06:40.000 --> 00:06:45.000]   Dan was the person I thought most likely to become a breakout science fiction author best-seller.
[00:06:45.000 --> 00:06:47.000]   I think you're well in the way to do that.
[00:06:47.000 --> 00:06:48.000]   I'm really happy.
[00:06:48.000 --> 00:06:51.000]   I was at the beta edition of the Caldonian Gambit.
[00:06:51.000 --> 00:06:54.000]   I think I read pre-release and I was like, "Yes, that was a long time ago."
[00:06:54.000 --> 00:06:55.000]   This is some fine work.
[00:06:55.000 --> 00:07:00.000]   Then I read the release version and I'm like, "Man, this guy also revises, which is incredible."
[00:07:00.000 --> 00:07:05.000]   I like the draft and then the release version was fantastic.
[00:07:05.000 --> 00:07:06.000]   There is a way.
[00:07:06.000 --> 00:07:07.000]   There is a way.
[00:07:07.000 --> 00:07:12.000]   A somewhat of a leap from nonfiction writing to fiction writing.
[00:07:12.000 --> 00:07:14.000]   Not everybody can make that leap.
[00:07:14.000 --> 00:07:18.000]   Some have fallen in the crevasse in between.
[00:07:18.000 --> 00:07:20.000]   I just wanted to say it's really good.
[00:07:20.000 --> 00:07:25.000]   Everybody should, if you like sci-fi, this is it.
[00:07:25.000 --> 00:07:27.000]   The Galactic Cold War saga.
[00:07:27.000 --> 00:07:31.000]   You've told me that there may be a fourth one in the works.
[00:07:31.000 --> 00:07:32.000]   Yeah.
[00:07:32.000 --> 00:07:33.000]   I'd like to.
[00:07:33.000 --> 00:07:37.000]   I've been working on this long overarching plot line and I am hoping I get a chance to bring
[00:07:37.000 --> 00:07:38.000]   that to fruition.
[00:07:38.000 --> 00:07:42.000]   I will say a lot of people don't like to pick up series unless it's done.
[00:07:42.000 --> 00:07:46.000]   I tried to also structure it so that each of the books tells a standalone story in its
[00:07:46.000 --> 00:07:48.000]   own regard as well.
[00:07:48.000 --> 00:07:52.000]   With a background, there's an overarching plot that's developing slowly.
[00:07:52.000 --> 00:07:56.000]   I think you can still get a lot of enjoyment out of the individual volumes as it goes along
[00:07:56.000 --> 00:08:00.000]   without worrying too much about like, "Oh no, will the series ever be finished?"
[00:08:00.000 --> 00:08:03.000]   You also have the best publisher ever, Angry Robot.
[00:08:03.000 --> 00:08:04.000]   I just love that name.
[00:08:04.000 --> 00:08:05.000]   That's true.
[00:08:05.000 --> 00:08:06.000]   Yeah.
[00:08:06.000 --> 00:08:12.000]   So when you do a thing with the series within 30 years, you'll have a leg up on R.R. Martin.
[00:08:12.000 --> 00:08:19.000]   If they start shooting a season one of your show, let's say you're five years to get the last book done.
[00:08:19.000 --> 00:08:20.000]   That's good as a comfortable right?
[00:08:20.000 --> 00:08:22.000]   We're at least the next book done.
[00:08:22.000 --> 00:08:27.000]   It doesn't even have to be the last book.
[00:08:27.000 --> 00:08:30.000]   It can just be an A edition.
[00:08:30.000 --> 00:08:33.000]   You could do the Wheel of Time and just die before you die.
[00:08:33.000 --> 00:08:34.000]   Yeah!
[00:08:34.000 --> 00:08:36.000]   You always have you considered that?
[00:08:36.000 --> 00:08:37.000]   Leo, how's your business?
[00:08:37.000 --> 00:08:40.000]   That's one way to avoid deadlines.
[00:08:40.000 --> 00:08:41.000]   I just had a kid.
[00:08:41.000 --> 00:08:42.000]   Come on.
[00:08:42.000 --> 00:08:43.000]   I'm already feeling more than that.
[00:08:43.000 --> 00:08:44.000]   I need to know more of that.
[00:08:44.000 --> 00:08:47.000]   It's a very spicy Leo today.
[00:08:47.000 --> 00:08:49.000]   You could die.
[00:08:49.000 --> 00:08:51.000]   I don't know about these photos.
[00:08:51.000 --> 00:08:56.000]   Get to know Brian St. Brandon Sanderson ahead of time.
[00:08:56.000 --> 00:09:03.000]   I will tell you, I actually have met Brandon because we have the same agent.
[00:09:03.000 --> 00:09:04.000]   Ah!
[00:09:04.000 --> 00:09:05.000]   Well, you know what?
[00:09:05.000 --> 00:09:06.000]   I know him.
[00:09:06.000 --> 00:09:10.000]   Robert Jordan was very lucky because he passed away before finishing this.
[00:09:10.000 --> 00:09:11.000]   How many?
[00:09:11.000 --> 00:09:12.000]   14 book series?
[00:09:12.000 --> 00:09:14.000]   17 as a reticulum.
[00:09:14.000 --> 00:09:15.000]   It's insane.
[00:09:15.000 --> 00:09:16.000]   Yeah.
[00:09:16.000 --> 00:09:19.000]   And of course he died before finishing it.
[00:09:19.000 --> 00:09:20.000]   It's pretty clear.
[00:09:20.000 --> 00:09:23.000]   He was going to just keep writing until he died.
[00:09:23.000 --> 00:09:29.000]   Thank God he had Brandon Sanderson to come along and finish it and finish it in high style.
[00:09:29.000 --> 00:09:36.000]   Of all of the TV productions, the Wheel of Time TV show is the worst.
[00:09:36.000 --> 00:09:37.000]   I don't know.
[00:09:37.000 --> 00:09:39.000]   There are some pretty bad TV productions.
[00:09:39.000 --> 00:09:42.000]   I mean out of classic science fiction, I guess.
[00:09:42.000 --> 00:09:43.000]   Yeah.
[00:09:43.000 --> 00:09:44.000]   I don't know anything.
[00:09:44.000 --> 00:09:46.000]   I have no loyalty to the book series.
[00:09:46.000 --> 00:09:47.000]   I hadn't read it.
[00:09:47.000 --> 00:09:48.000]   I kind of like Wheel of Time.
[00:09:48.000 --> 00:09:49.640]   That's pretty much how it works.
[00:09:49.640 --> 00:09:51.400]   If you've read it, you'll hate it.
[00:09:51.400 --> 00:09:54.480]   And if you've never read it, you'll like the TV show.
[00:09:54.480 --> 00:09:58.360]   And it is better now that I think of it than Foundation, which was truly awful.
[00:09:58.360 --> 00:10:00.040]   I have so much hope for it.
[00:10:00.040 --> 00:10:02.000]   And then by the end, it's like, what do you do?
[00:10:02.000 --> 00:10:03.000]   Oh horrible.
[00:10:03.000 --> 00:10:04.000]   Oh come on.
[00:10:04.000 --> 00:10:05.000]   Horrible.
[00:10:05.000 --> 00:10:06.000]   I don't know.
[00:10:06.000 --> 00:10:07.000]   I don't know what they're doing.
[00:10:07.000 --> 00:10:08.000]   How about Lord of the Rings?
[00:10:08.000 --> 00:10:09.000]   Do we like Lord of the Rings?
[00:10:09.000 --> 00:10:10.000]   I mean, that's the plan about Lord of the Rings.
[00:10:10.000 --> 00:10:11.000]   Glenn is a big Lord of the Rings fan.
[00:10:11.000 --> 00:10:13.440]   Yeah, I'm sitting here with my 15 year old who has not read Lord of the Rings.
[00:10:13.440 --> 00:10:17.440]   And I'm like, oh, well, they're not telling a story about Fainor, the creator of the
[00:10:17.440 --> 00:10:19.200]   Silamar Reels.
[00:10:19.200 --> 00:10:21.960]   And his original name was Mel, and I'm like, oh my God, I don't remember that I know
[00:10:21.960 --> 00:10:22.960]   all this stuff.
[00:10:22.960 --> 00:10:24.320]   Why do I know all this stuff?
[00:10:24.320 --> 00:10:25.960]   It's the second unusual information.
[00:10:25.960 --> 00:10:27.760]   All the useful information you've had.
[00:10:27.760 --> 00:10:29.520]   It's like every useful thing you've forgotten.
[00:10:29.520 --> 00:10:32.320]   It's just taken out by names of elves.
[00:10:32.320 --> 00:10:33.320]   It's still beautiful.
[00:10:33.320 --> 00:10:38.360]   It's not which being read wise, but Calimbroir was the friend of the dwarf.
[00:10:38.360 --> 00:10:40.960]   Ah, I'm a little, I'm a little confused.
[00:10:40.960 --> 00:10:42.840]   Have they, it's only been two episodes.
[00:10:42.840 --> 00:10:45.520]   Have they botched the series yet?
[00:10:45.520 --> 00:10:49.560]   Or is it pretty much faithful to what little we know?
[00:10:49.560 --> 00:10:56.120]   I think they're doing an incredibly good job of trying to bring in new people without
[00:10:56.120 --> 00:11:00.680]   100% offending people like I was 30 years ago.
[00:11:00.680 --> 00:11:09.520]   The Simmerilla, Simmerilla, did not specify a lot of this.
[00:11:09.520 --> 00:11:15.480]   For instance, and I saw somebody note this, and I don't think this is a spoiler.
[00:11:15.480 --> 00:11:20.600]   But they're sailing the ship into the Grey Havens.
[00:11:20.600 --> 00:11:23.360]   Our Prince, I won't say names.
[00:11:23.360 --> 00:11:27.560]   Our Prince has been awarded release into the Grey Haven.
[00:11:27.560 --> 00:11:29.000]   She's on the ship.
[00:11:29.000 --> 00:11:30.360]   They're standing there, the Warriors.
[00:11:30.360 --> 00:11:33.400]   Long voyage, I think, but they're standing there the whole time in their armor.
[00:11:33.400 --> 00:11:36.640]   Then these maidens come out and help undress them.
[00:11:36.640 --> 00:11:38.080]   Yes, yes.
[00:11:38.080 --> 00:11:42.120]   Are them, so here's my question.
[00:11:42.120 --> 00:11:44.120]   Are the maidens?
[00:11:44.120 --> 00:11:46.600]   Are they going to sail the ship back for the next trip?
[00:11:46.600 --> 00:11:48.320]   Or are they going into the Grey Havens?
[00:11:48.320 --> 00:11:49.920]   I think it's the one you ship.
[00:11:49.920 --> 00:11:50.920]   It seems like it's pretty...
[00:11:50.920 --> 00:11:51.920]   It feels like it's not going to survive the...
[00:11:51.920 --> 00:11:53.920]   It doesn't seem like it's turning around again.
[00:11:53.920 --> 00:11:54.920]   Yeah.
[00:11:54.920 --> 00:11:55.920]   Where are those maidens going?
[00:11:55.920 --> 00:11:57.960]   They can then go down the sister garb?
[00:11:57.960 --> 00:11:59.720]   There's some people that can go...
[00:11:59.720 --> 00:12:02.360]   So there's these different order of celestial beings.
[00:12:02.360 --> 00:12:06.400]   The elves have servants.
[00:12:06.400 --> 00:12:11.600]   And then the weird thing is, so the servants take the weapons and the armor from the elves
[00:12:11.600 --> 00:12:13.840]   and they throw them on the ground.
[00:12:13.840 --> 00:12:15.720]   They don't fold them up, put them away.
[00:12:15.720 --> 00:12:16.720]   They just go here.
[00:12:16.720 --> 00:12:18.720]   It's like, what were you doing?
[00:12:18.720 --> 00:12:20.640]   Couldn't they have just held the armor?
[00:12:20.640 --> 00:12:23.720]   Could they just held them for scrap later on?
[00:12:23.720 --> 00:12:25.720]   They can repurpose in a recycle, you know.
[00:12:25.720 --> 00:12:28.080]   Because they've got to get another boat to get back.
[00:12:28.080 --> 00:12:30.240]   That's a wings of desire.
[00:12:30.240 --> 00:12:31.880]   The armor things and wings of desire, right?
[00:12:31.880 --> 00:12:34.480]   So they'll all come back to the Peter Falk universe.
[00:12:34.480 --> 00:12:37.000]   It all comes back to Vim vendors in the end.
[00:12:37.000 --> 00:12:39.000]   Vim vendors.
[00:12:39.000 --> 00:12:40.640]   How about the House of the Dragon?
[00:12:40.640 --> 00:12:43.280]   Are we happy with the House of the Dragon?
[00:12:43.280 --> 00:12:47.960]   I watched the first one and thought it was better than I expected it was going to be.
[00:12:47.960 --> 00:12:50.800]   I was not super invested, but my wife was actually like, oh, I really like to watch this.
[00:12:50.800 --> 00:12:52.680]   I was like, all right, we'll sit down and watch it.
[00:12:52.680 --> 00:12:53.680]   And I don't know.
[00:12:53.680 --> 00:12:54.680]   I thought it was interesting.
[00:12:54.680 --> 00:13:00.480]   Having felt like the Game of Thrones show ended on pretty sour note for me, I was more
[00:13:00.480 --> 00:13:05.680]   than pleasantly surprised by this and having a little more, you know, I thought like having
[00:13:05.680 --> 00:13:09.400]   the positioning of female characters, a little more agency and making it a more involved
[00:13:09.400 --> 00:13:11.640]   story and like sort of focusing in a bit too, right?
[00:13:11.640 --> 00:13:15.560]   And having like, there's a six million characters and they're all over this continent.
[00:13:15.560 --> 00:13:18.680]   Instead, it's like a smaller group and yeah, we'll see where it goes.
[00:13:18.680 --> 00:13:19.680]   We'll watch the next few.
[00:13:19.680 --> 00:13:23.800]   I feel like Glenn, you're a little bruised still from season eight of the Game of Thrones.
[00:13:23.800 --> 00:13:28.160]   I've never seen a single episode or a portion of that.
[00:13:28.160 --> 00:13:29.160]   Oh, never mind.
[00:13:29.160 --> 00:13:30.160]   Throw my show.
[00:13:30.160 --> 00:13:31.160]   I just didn't.
[00:13:31.160 --> 00:13:35.080]   It didn't strike me as I'd read George RR Martin before he saw a rain that series and
[00:13:35.080 --> 00:13:37.920]   I like some of his I like his earlier funnier stuff better.
[00:13:37.920 --> 00:13:40.720]   Before he was cool.
[00:13:40.720 --> 00:13:45.480]   Yeah, I'm starting the show with some light material because we're going to get pretty
[00:13:45.480 --> 00:13:46.480]   heavy.
[00:13:46.480 --> 00:13:49.360]   Right off the bat.
[00:13:49.360 --> 00:13:54.720]   We talked about this on Wednesday with our wonderful Mike Masnick from Tech Dirt who
[00:13:54.720 --> 00:14:01.840]   has really kind of burst a socket on this children's online safety bill that was just
[00:14:01.840 --> 00:14:03.640]   passed in California.
[00:14:03.640 --> 00:14:08.360]   We're waiting for the governor to sign it into law.
[00:14:08.360 --> 00:14:15.240]   It seems like a good idea who wouldn't want to protect children online.
[00:14:15.240 --> 00:14:17.940]   And of course, if you're not in California, you're probably thinking you can take the
[00:14:17.940 --> 00:14:18.940]   rod pile lower.
[00:14:18.940 --> 00:14:22.400]   They're telling them why that's there.
[00:14:22.400 --> 00:14:24.760]   There is no rod pile in this story whatsoever.
[00:14:24.760 --> 00:14:25.760]   Oh my God.
[00:14:25.760 --> 00:14:28.200]   I mean, that's the New York Times baby.
[00:14:28.200 --> 00:14:29.200]   He's less control.
[00:14:29.200 --> 00:14:34.440]   I think he's less control of the lower third wheel that the center to the extension.
[00:14:34.440 --> 00:14:40.000]   I would say Leo, all of your smart assistants have taken over the show.
[00:14:40.000 --> 00:14:42.000]   Scrub the lawn, scrub the lawn.
[00:14:42.000 --> 00:14:43.200]   Yeah, that was the Artemis story.
[00:14:43.200 --> 00:14:45.920]   They'll be coming later.
[00:14:45.920 --> 00:14:47.200]   None of you are in California, right?
[00:14:47.200 --> 00:14:48.200]   Dan, are you in California?
[00:14:48.200 --> 00:14:49.360]   I can't remember where you are, Dan.
[00:14:49.360 --> 00:14:50.360]   Massachusetts.
[00:14:50.360 --> 00:14:51.360]   Massachusetts.
[00:14:51.360 --> 00:14:52.360]   That's why your window is open.
[00:14:52.360 --> 00:14:55.960]   If you were in California, you would be hunker down now.
[00:14:55.960 --> 00:14:57.960]   It's about a hundred and nine degrees outside.
[00:14:57.960 --> 00:14:58.960]   Oh yeah.
[00:14:58.960 --> 00:15:01.000]   And you guys don't believe in air conditioning, right?
[00:15:01.000 --> 00:15:02.000]   Yeah.
[00:15:02.000 --> 00:15:05.880]   That's the funny thing is people in California say, well, we have natural air conditioning
[00:15:05.880 --> 00:15:09.000]   until it gets hot and then we don't.
[00:15:09.000 --> 00:15:11.080]   I don't say much of anything.
[00:15:11.080 --> 00:15:15.280]   How did you change my change that for you?
[00:15:15.280 --> 00:15:16.280]   Yeah.
[00:15:16.280 --> 00:15:19.640]   So this is maybe not on your radar, but it sure is for us in California.
[00:15:19.640 --> 00:15:23.720]   And Mike Masnick was also a California, the New York Times story about this.
[00:15:23.720 --> 00:15:26.160]   You would never think there was a problem.
[00:15:26.160 --> 00:15:34.240]   The California Age Appropriate Design Code Act requires any website, unlike COPPA, where
[00:15:34.240 --> 00:15:37.520]   it only affects websites that are aimed at children.
[00:15:37.520 --> 00:15:43.120]   This is a website, any website that might have somebody under 18 visited, which I think
[00:15:43.120 --> 00:15:44.760]   is any website.
[00:15:44.760 --> 00:15:49.080]   Certainly ours, certainly Mike Masnick's.
[00:15:49.080 --> 00:15:53.000]   So if you have the potential that somebody under 18 might visit your website, then you
[00:15:53.000 --> 00:15:59.920]   are required to, first of all, know the age of everybody who visits your website.
[00:15:59.920 --> 00:16:03.720]   Problem number one, that's pretty intrusive.
[00:16:03.720 --> 00:16:06.880]   We don't know the age of anybody who listens to our shows or visits our website.
[00:16:06.880 --> 00:16:13.640]   I don't want to know their age, but in order to enforce this law, I need to.
[00:16:13.640 --> 00:16:20.440]   That Mike's concern was, oh, am I going to have to do age verification?
[00:16:20.440 --> 00:16:22.920]   And if I do, how am I going to do that?
[00:16:22.920 --> 00:16:26.560]   He says, it may be face ID.
[00:16:26.560 --> 00:16:33.360]   You may have to start because you've got to know, you've got to know how old these people
[00:16:33.360 --> 00:16:34.360]   are.
[00:16:34.360 --> 00:16:42.840]   Then you have to look at every single feature of your site and do what they call it a DPIA,
[00:16:42.840 --> 00:16:47.200]   kind of like an environmental impact report on each feature of your site and how it might
[00:16:47.200 --> 00:16:51.280]   infect people under 18.
[00:16:51.280 --> 00:16:55.000]   And that has to be, you have to do it because it has to be available.
[00:16:55.000 --> 00:17:01.440]   Should the Attorney General of California ask, you have 72 hours to produce that?
[00:17:01.440 --> 00:17:05.680]   This surely feels like the kinds of rules that go into effect in countries that are
[00:17:05.680 --> 00:17:10.760]   trying to prohibit speech or use workarounds.
[00:17:10.760 --> 00:17:14.560]   India has imposed laws and Russia has imposed more draconian ones.
[00:17:14.560 --> 00:17:20.240]   Even India, ostensibly a democracy, has imposed laws that restrict freedom in the name of safety
[00:17:20.240 --> 00:17:25.200]   or freedom from libel or other things, and they're meant to chill speech.
[00:17:25.200 --> 00:17:29.360]   And this isn't per se designed that way, but it has all the hallmarks of one.
[00:17:29.360 --> 00:17:32.680]   It's expensive or impossible to implement.
[00:17:32.680 --> 00:17:37.120]   It's probably, I'm not a lawyer, it's probably unconstitutional based on what I'm reading,
[00:17:37.120 --> 00:17:43.040]   does not seem to fit within the permissibility of how this could possibly be enforced.
[00:17:43.040 --> 00:17:47.800]   So the burden is undue and it's chilling of speech.
[00:17:47.800 --> 00:17:50.640]   And so you're kind of like, well, how did it get this far?
[00:17:50.640 --> 00:17:54.160]   I think Mike pointed out in his article, he was like, well, nobody wants to, you were
[00:17:54.160 --> 00:17:56.880]   saying, oh, no, I'm opposed to protecting children online.
[00:17:56.880 --> 00:18:01.840]   I just clipped that out when I just said, glad to listen, I'm not, I'm not, but no one's
[00:18:01.840 --> 00:18:02.840]   going to vote.
[00:18:02.840 --> 00:18:06.240]   No politician is going to say, I'm voting against the protecting children from terrible
[00:18:06.240 --> 00:18:07.600]   things online bill.
[00:18:07.600 --> 00:18:08.600]   Exactly.
[00:18:08.600 --> 00:18:12.840]   Which is why it passed in the Senate 30 to nothing.
[00:18:12.840 --> 00:18:14.800]   Don't we do this every few decades?
[00:18:14.800 --> 00:18:18.520]   I mean, I'm remembering when I was a teenager, the Communications Decency Act was the big
[00:18:18.520 --> 00:18:22.680]   sort of hot button issue at the time, which, you know, that was the, the heady days of
[00:18:22.680 --> 00:18:24.920]   1996 or so, right?
[00:18:24.920 --> 00:18:27.400]   Where the internet was not at all what it is today.
[00:18:27.400 --> 00:18:29.960]   But there was still the sort of the same idea like, oh, we've got to figure out a way
[00:18:29.960 --> 00:18:35.840]   to flag stuff that might be objectionable so that people aren't exposed to it.
[00:18:35.840 --> 00:18:40.440]   And ultimately, I think it ended up being sort of impossible to deal with because how
[00:18:40.440 --> 00:18:41.440]   do you do that?
[00:18:41.440 --> 00:18:42.440]   Right?
[00:18:42.440 --> 00:18:45.440]   And I think the thing that I think is really important is that the internet is not going
[00:18:45.440 --> 00:18:46.440]   to be a good thing.
[00:18:46.440 --> 00:18:49.440]   And I think the thing that I think is really important is that the internet is not going
[00:18:49.440 --> 00:18:50.440]   to be a good thing.
[00:18:50.440 --> 00:18:52.440]   And I think the internet is going to be a good thing.
[00:18:52.440 --> 00:18:54.440]   And I think the internet is going to be a good thing.
[00:18:54.440 --> 00:18:55.440]   And I think the internet is going to be a good thing.
[00:18:55.440 --> 00:18:56.440]   And I think the internet is going to be a good thing.
[00:18:56.440 --> 00:18:57.440]   And I think the internet is going to be a good thing.
[00:18:57.440 --> 00:18:58.440]   And I think the internet is going to be a good thing.
[00:18:58.440 --> 00:18:59.440]   And I think the internet is going to be a good thing.
[00:18:59.440 --> 00:19:00.440]   And I think the internet is going to be a good thing.
[00:19:00.440 --> 00:19:01.440]   And I think the internet is going to be a good thing.
[00:19:01.440 --> 00:19:02.440]   And I think the internet is going to be a good thing.
[00:19:02.440 --> 00:19:09.440]   And I think the internet is going to be a good thing.
[00:19:09.440 --> 00:19:12.440]   And I think the internet is going to be a good thing.
[00:19:12.440 --> 00:19:15.440]   And I think the internet is going to be a good thing.
[00:19:15.440 --> 00:19:17.440]   And I think the internet is going to be a good thing.
[00:19:17.440 --> 00:19:19.440]   And I think the internet is going to be a good thing.
[00:19:19.440 --> 00:19:20.440]   And I think the internet is going to be a good thing.
[00:19:20.440 --> 00:19:21.440]   And I think the internet is going to be a good thing.
[00:19:21.440 --> 00:19:22.440]   And I think the internet is going to be a good thing.
[00:19:22.440 --> 00:19:23.440]   And I think the internet is going to be a good thing.
[00:19:23.440 --> 00:19:24.440]   And I think the internet is going to be a good thing.
[00:19:24.440 --> 00:19:25.440]   And I think the internet is going to be a good thing.
[00:19:25.440 --> 00:19:26.440]   And I think the internet is going to be a good thing.
[00:19:26.440 --> 00:19:27.440]   And I think the internet is going to be a good thing.
[00:19:27.440 --> 00:19:28.440]   And I think the internet is going to be a good thing.
[00:19:28.440 --> 00:19:31.440]   And I think the internet is going to be a good thing.
[00:19:31.440 --> 00:19:32.440]   And I think the internet is going to be a good thing.
[00:19:32.440 --> 00:19:33.440]   And I think the internet is going to be a good thing.
[00:19:33.440 --> 00:19:34.440]   And I think the internet is going to be a good thing.
[00:19:34.440 --> 00:19:35.440]   And I think the internet is going to be a good thing.
[00:19:35.440 --> 00:19:36.440]   And I think the internet is going to be a good thing.
[00:19:36.440 --> 00:19:37.440]   And I think the internet is going to be a good thing.
[00:19:37.440 --> 00:19:38.440]   And I think the internet is going to be a good thing.
[00:19:38.440 --> 00:19:39.440]   And I think the internet is going to be a good thing.
[00:19:39.440 --> 00:19:40.440]   And I think the internet is going to be a good thing.
[00:19:40.440 --> 00:19:41.440]   And I think the internet is going to be a good thing.
[00:19:41.440 --> 00:19:42.440]   And I think the internet is going to be a good thing.
[00:19:42.440 --> 00:19:43.440]   And I think the internet is going to be a good thing.
[00:19:43.440 --> 00:19:44.440]   And I think the internet is going to be a good thing.
[00:19:44.440 --> 00:19:50.440]   And I think the internet is going to be a good thing.
[00:19:50.440 --> 00:19:57.440]   And I think the internet is going to be a good thing.
[00:19:57.440 --> 00:20:02.440]   And I think the internet is going to be a good thing.
[00:20:02.440 --> 00:20:09.440]   And I think the internet is going to be a good thing.
[00:20:09.440 --> 00:20:14.440]   And I think the internet is going to be a good thing.
[00:20:14.440 --> 00:20:19.440]   And I think the internet is going to be a good thing.
[00:20:19.440 --> 00:20:22.440]   And I think the internet is going to be a good thing.
[00:20:22.440 --> 00:20:24.440]   And I think the internet is going to be a good thing.
[00:20:24.440 --> 00:20:26.440]   And I think the internet is going to be a good thing.
[00:20:26.440 --> 00:20:27.440]   And I think the internet is going to be a good thing.
[00:20:27.440 --> 00:20:28.440]   And I think the internet is going to be a good thing.
[00:20:28.440 --> 00:20:29.440]   And I think the internet is going to be a good thing.
[00:20:29.440 --> 00:20:30.440]   And I think the internet is going to be a good thing.
[00:20:30.440 --> 00:20:31.440]   And I think the internet is going to be a good thing.
[00:20:31.440 --> 00:20:32.440]   And I think the internet is going to be a good thing.
[00:20:32.440 --> 00:20:33.440]   And I think the internet is going to be a good thing.
[00:20:33.440 --> 00:20:34.440]   And I think the internet is going to be a good thing.
[00:20:34.440 --> 00:20:35.440]   And I think the internet is going to be a good thing.
[00:20:35.440 --> 00:20:40.440]   And I think the internet is going to be a good thing.
[00:20:40.440 --> 00:20:43.440]   And I think the internet is going to be a good thing.
[00:20:43.440 --> 00:20:45.440]   And I think the internet is going to be a good thing.
[00:20:45.440 --> 00:20:46.440]   And I think the internet is going to be a good thing.
[00:20:46.440 --> 00:20:47.440]   And I think the internet is going to be a good thing.
[00:20:47.440 --> 00:20:48.440]   And I think the internet is going to be a good thing.
[00:20:48.440 --> 00:20:49.440]   And I think the internet is going to be a good thing.
[00:20:49.440 --> 00:20:50.440]   And I think the internet is going to be a good thing.
[00:20:50.440 --> 00:20:51.440]   And I think the internet is going to be a good thing.
[00:20:51.440 --> 00:20:52.440]   And I think the internet is going to be a good thing.
[00:20:52.440 --> 00:20:53.440]   And I think the internet is going to be a good thing.
[00:20:53.440 --> 00:20:54.440]   And I think the internet is going to be a good thing.
[00:20:54.440 --> 00:20:55.440]   And I think the internet is going to be a good thing.
[00:20:55.440 --> 00:20:56.440]   And I think the internet is going to be a good thing.
[00:20:56.440 --> 00:21:03.440]   And I think the internet is going to be a good thing.
[00:21:03.440 --> 00:21:10.440]   And I think the internet is going to be a good thing.
[00:21:10.440 --> 00:21:17.440]   And I think the internet is going to be a good thing.
[00:21:17.440 --> 00:21:24.440]   And I think the internet is going to be a good thing.
[00:21:24.440 --> 00:21:29.440]   And I think the internet is going to be a good thing.
[00:21:29.440 --> 00:21:34.440]   And I think the internet is going to be a good thing.
[00:21:34.440 --> 00:21:39.440]   And I think the internet is going to be a good thing.
[00:21:39.440 --> 00:21:44.440]   And I think the internet is going to be a good thing.
[00:21:44.440 --> 00:21:51.440]   And I think the internet is going to be a good thing.
[00:21:51.440 --> 00:21:56.440]   And I hear the words.
[00:21:56.440 --> 00:22:01.440]   And anytime I hear United Kingdom and protect children,
[00:22:01.440 --> 00:22:04.440]   I want to run the other direction.
[00:22:04.440 --> 00:22:07.440]   They're so bad at everything they do there.
[00:22:07.440 --> 00:22:10.440]   There's been so many scandals and conflicts.
[00:22:10.440 --> 00:22:13.440]   The whole Jimmy Savile thing with the BBC.
[00:22:13.440 --> 00:22:16.440]   I mean, like large and small, I think the United Kingdom has done a terrible job
[00:22:16.440 --> 00:22:19.440]   protecting children from predators on-site and off.
[00:22:19.440 --> 00:22:24.440]   And you're like, "Okay, we really want to follow their lead."
[00:22:24.440 --> 00:22:25.440]   Because they know what's right.
[00:22:25.440 --> 00:22:29.440]   The Corey Doctor has written so effectively about the failures of the UK.
[00:22:29.440 --> 00:22:30.440]   Yeah.
[00:22:30.440 --> 00:22:31.440]   He lived there for a long time.
[00:22:31.440 --> 00:22:34.440]   Is this ignorance on the part of lawmakers?
[00:22:34.440 --> 00:22:37.440]   Like they want to do the right thing.
[00:22:37.440 --> 00:22:40.440]   And they just don't understand the impact of what they're doing.
[00:22:40.440 --> 00:22:43.440]   Or is it some sort of malevolence?
[00:22:43.440 --> 00:22:48.440]   Certainly the Baroness seems to have some sort of malevolence against the open internet.
[00:22:48.440 --> 00:22:50.440]   Do you think there is a trend?
[00:22:50.440 --> 00:22:54.440]   It feels like politicians these days, it's not merely to protect the children.
[00:22:54.440 --> 00:22:56.440]   They want to take down big tech.
[00:22:56.440 --> 00:22:58.440]   They don't like it.
[00:22:58.440 --> 00:23:02.440]   I mean, something that I think about a lot is, as part of my job and all of our jobs,
[00:23:02.440 --> 00:23:07.440]   we end up watching these big tech hearings that we had for a couple of months,
[00:23:07.440 --> 00:23:08.440]   over the last couple of years.
[00:23:08.440 --> 00:23:12.440]   And I remember the first couple that I watched, I was like, "Oh, this is going to be interesting."
[00:23:12.440 --> 00:23:16.440]   You know, an actual political discussion about what's going on was not that.
[00:23:16.440 --> 00:23:22.440]   It is mostly just politicians giving a one minute stump speech totally unhinged
[00:23:22.440 --> 00:23:26.440]   or unrelated to the thing they're actually discussing.
[00:23:26.440 --> 00:23:28.440]   And I think that's what we're seeing here in some ways.
[00:23:28.440 --> 00:23:35.440]   Is it is partially ignorance, but partially a lot of modern-day politicians have realized
[00:23:35.440 --> 00:23:41.440]   that the way to get attention from your base, or the sort of people that you want to bring
[00:23:41.440 --> 00:23:46.440]   into your base is by making big bold statements, like bringing down tech,
[00:23:46.440 --> 00:23:53.440]   making all the children safe, regardless of whether or not that actually has any teeth in it.
[00:23:53.440 --> 00:23:58.440]   And the end result of that is sometimes you get policies like this that can have disastrous
[00:23:58.440 --> 00:24:00.440]   unintended consequences.
[00:24:00.440 --> 00:24:03.440]   And there's both a carrot and a stick.
[00:24:03.440 --> 00:24:07.440]   There's the threat, the fear that a soundbite will be used against you.
[00:24:07.440 --> 00:24:10.440]   Oh, he's against protecting children online.
[00:24:10.440 --> 00:24:15.440]   But there's also the carrot that it makes a great soundbite that might get you some votes,
[00:24:15.440 --> 00:24:20.440]   if you say, "Yeah, she's really -- she voted to protect the kids online."
[00:24:20.440 --> 00:24:23.440]   So it's the problem is soundbites.
[00:24:23.440 --> 00:24:25.440]   It's just posturing.
[00:24:25.440 --> 00:24:26.440]   I think you're exactly --
[00:24:26.440 --> 00:24:28.440]   I think a lot of the -- there is ignorance at the base of it, too,
[00:24:28.440 --> 00:24:33.440]   because I think it also presupposes this fallacy that there is somehow a switch in flip
[00:24:33.440 --> 00:24:39.440]   that will protect -- make it perfectly functional in a way that protects children.
[00:24:39.440 --> 00:24:44.440]   We entertain this idea that, "Oh, well, technology can solve this issue for us."
[00:24:44.440 --> 00:24:48.440]   And we just -- technology is very black and white, one and zero.
[00:24:48.440 --> 00:24:53.440]   And it's like, "Well, it can obviously tell you whether or not you're able to consume
[00:24:53.440 --> 00:24:54.440]   this information."
[00:24:54.440 --> 00:24:58.440]   But that's, I think, as Paris pointed out, it's very difficult to know who is consuming
[00:24:58.440 --> 00:24:59.440]   this information.
[00:24:59.440 --> 00:25:04.440]   I think also about things like libraries that have computers available for people to access
[00:25:04.440 --> 00:25:09.440]   the Internet, who don't -- might otherwise not have ways to do that.
[00:25:09.440 --> 00:25:11.440]   How do you tell who's using that, right?
[00:25:11.440 --> 00:25:13.440]   Like, how do you tell who's on the other end of that?
[00:25:13.440 --> 00:25:17.440]   So I think people think tech is very clear-cut when they don't know anything about it,
[00:25:17.440 --> 00:25:22.440]   but anybody who spends time with it realizes, "No, this is extremely nuanced,
[00:25:22.440 --> 00:25:24.440]   and there's a lot of gray area."
[00:25:24.440 --> 00:25:27.440]   The springs are back to the whole -- well, there's two things.
[00:25:27.440 --> 00:25:31.440]   One is the overarching issue is that I think we probably all agree -- I won't speak for other people,
[00:25:31.440 --> 00:25:36.440]   but I think it's likely -- that we know that the big social media companies and other companies,
[00:25:36.440 --> 00:25:39.440]   in most cases, are failing to protect children online.
[00:25:39.440 --> 00:25:42.440]   They're doing things that are actively bad for children.
[00:25:42.440 --> 00:25:47.440]   They don't seem to have any compunction about it, and no one has an idea of precisely how to stop it,
[00:25:47.440 --> 00:25:52.440]   except through public disclosure, whistle-blowing threats by legislators,
[00:25:52.440 --> 00:25:57.440]   but nothing -- there's no compulsion, particularly in countries with expansive free speech laws
[00:25:57.440 --> 00:26:02.440]   and commercial speech laws, people feel impotent to solve that problem.
[00:26:02.440 --> 00:26:05.440]   So we know there are problems that Instagram used to promote eating disorders,
[00:26:05.440 --> 00:26:12.440]   and they've made great strides in that, but there's just all of this negative content aimed at children
[00:26:12.440 --> 00:26:16.440]   algorithm fed often that has some kind of beneficial effect,
[00:26:16.440 --> 00:26:20.440]   but it also reminds me, damn, what you were just saying of when they kept talking about --
[00:26:20.440 --> 00:26:24.440]   I think it was the Clinton administration later trying to bring this up -- like,
[00:26:24.440 --> 00:26:29.440]   what if we had a special key that only law enforcement could access only in very particular cases,
[00:26:29.440 --> 00:26:34.440]   otherwise the encryption would be perfect and protected when they were talking about end-to-end encryption?
[00:26:34.440 --> 00:26:37.440]   It's like, well, there's no way to build an algorithm like that,
[00:26:37.440 --> 00:26:42.440]   and every encryption expert in the world agrees, but nobody on the political side wants to accept that.
[00:26:42.440 --> 00:26:46.440]   They don't want to accept the technological problem that you can have a system with a key
[00:26:46.440 --> 00:26:48.440]   that only legitimate parties can use.
[00:26:48.440 --> 00:26:55.440]   Same thing here is how do you prevent harm to children, which is active and underway without also chilling speech?
[00:26:55.440 --> 00:26:58.440]   I was terrified for my children that they grew up in this environment,
[00:26:58.440 --> 00:27:01.440]   and fortunately I think we managed to avoid any significant problems,
[00:27:01.440 --> 00:27:08.440]   but I'm sure we all have stories or know people of stories of children being harassed online or singled out or groomed or so forth.
[00:27:08.440 --> 00:27:12.440]   It's just a massive problem made worse by many of these companies.
[00:27:12.440 --> 00:27:17.440]   People don't want to hear that the hard solutions to this problem are awful.
[00:27:17.440 --> 00:27:21.440]   Often ones that require societal change in work rather than just technology,
[00:27:21.440 --> 00:27:24.440]   which can solve it at the snap of your fingers.
[00:27:24.440 --> 00:27:28.440]   People are like, "Oh, there's a shortcut. We just make the technology do that."
[00:27:28.440 --> 00:27:34.440]   Nobody wants to think about the fact that it's like, "No, that's a band-aid, and it's not a particularly good one.
[00:27:34.440 --> 00:27:38.440]   It's a band-aid on a gushing, amputated arm or something.
[00:27:38.440 --> 00:27:40.440]   It's not going to fix your problem."
[00:27:40.440 --> 00:27:45.440]   I also wonder if Baroness Kidrin, if she used to work in Hollywood,
[00:27:45.440 --> 00:27:53.440]   if Hollywood were told, "Oh, no, no. You have to make sure that every movie you make is appropriate for somebody under 18,
[00:27:53.440 --> 00:27:58.440]   or every novelist had to make sure every novel was appropriate for somebody under 18."
[00:27:58.440 --> 00:28:03.440]   I think they would bridle at those kinds of restrictions as they should.
[00:28:03.440 --> 00:28:07.440]   We don't want to design the world for people under 18.
[00:28:07.440 --> 00:28:09.440]   Don't forget Tipper Gore.
[00:28:09.440 --> 00:28:12.440]   What does appropriate mean in that case?
[00:28:12.440 --> 00:28:18.440]   First of all, kids at different ages have different levels of maturity and different levels of appropriateness.
[00:28:18.440 --> 00:28:22.440]   A 13-year-old and a 17-year-old are going to have very different levels of what they think is appropriate.
[00:28:22.440 --> 00:28:29.440]   I definitely went to rated R movies as a 17-year-old and probably with my parents went to rated R movies younger
[00:28:29.440 --> 00:28:32.440]   if it was a serious topic or something like that.
[00:28:32.440 --> 00:28:36.440]   How do you deem what is appropriate?
[00:28:36.440 --> 00:28:43.440]   The ratings actually is a good system because all the ratings were designed to do is to inform you or parents ahead of time.
[00:28:43.440 --> 00:28:46.440]   Here's the content, not to censor the content.
[00:28:46.440 --> 00:28:51.440]   I'm not sure the ratings, I think the ratings I think kind of worked.
[00:28:51.440 --> 00:28:59.440]   I don't know. Would you want to rate every website on the Internet and say, "This website rated R?"
[00:28:59.440 --> 00:29:02.440]   I don't know if that's appropriate either.
[00:29:02.440 --> 00:29:05.440]   Who would do it? I mean, the motion picture.
[00:29:05.440 --> 00:29:09.440]   The association of America had a reason to do it.
[00:29:09.440 --> 00:29:16.440]   A lot of organizations, a lot of trade groups grew up specifically to address preventing regulation from going into effect
[00:29:16.440 --> 00:29:19.440]   by voluntarily adhering to standards and enforcing them.
[00:29:19.440 --> 00:29:20.440]   That's right.
[00:29:20.440 --> 00:29:21.440]   Yeah.
[00:29:21.440 --> 00:29:22.440]   Right.
[00:29:22.440 --> 00:29:23.440]   So who wants a law?
[00:29:23.440 --> 00:29:30.440]   But in this case, I think how do you protect children, even from some very coarse things,
[00:29:30.440 --> 00:29:37.440]   like how do you protect predators? I have this discussion a lot. It's not that there are so many predators in the world.
[00:29:37.440 --> 00:29:44.440]   I want to believe that the number of people who are pedophiles and active predators against other people is a relatively small percentage of humanity.
[00:29:44.440 --> 00:29:45.440]   A very small percentage.
[00:29:45.440 --> 00:29:48.440]   I want to believe that too. I'm not sure that that's the case, but I want to believe.
[00:29:48.440 --> 00:29:53.440]   I don't know either. Let's pretend it's the case briefly, but say that the Internet is an amplification for it.
[00:29:53.440 --> 00:29:58.440]   So it doesn't matter if it's one in 100,000 people or one in a million is out after children.
[00:29:58.440 --> 00:30:03.440]   If they can access 10 million children and window it down and find one near them or they can reach out to,
[00:30:03.440 --> 00:30:06.440]   it's a yield issue.
[00:30:06.440 --> 00:30:12.440]   And so online services need to impose their own protections that reduce that yield potential.
[00:30:12.440 --> 00:30:16.440]   And that's where I think things are failing is I don't think they have a motivation to do it.
[00:30:16.440 --> 00:30:19.440]   There's no financial benefit. There's no regulatory framework.
[00:30:19.440 --> 00:30:23.440]   And they haven't yet suffered significantly from it.
[00:30:23.440 --> 00:30:28.440]   So when there's backlash, there was a lot of backlash against Facebook for all the revelations from,
[00:30:28.440 --> 00:30:31.440]   oh, I forgot the group in England.
[00:30:31.440 --> 00:30:32.440]   - Every channel.
[00:30:32.440 --> 00:30:36.440]   - Yes, thank you. So they had backlash. It seemed like it affected their market price.
[00:30:36.440 --> 00:30:39.440]   There was the threat of a lot of regulation. There were open hearings, right?
[00:30:39.440 --> 00:30:48.440]   But I don't think we've seen the same concerted effort that's gone into the sort of diffuse choices that have led to children being endangered.
[00:30:48.440 --> 00:30:57.440]   Occasionally something will leap up and Facebook will say, "Oh, okay, we're going to disable all contents on comments on videos uploaded by people under 18."
[00:30:57.440 --> 00:31:02.440]   Or, "Didn't they just say they're going to make all uploaded videos private by default on YouTube?"
[00:31:02.440 --> 00:31:04.440]   - Yeah, you too. Yeah, if you're under 17, yeah.
[00:31:04.440 --> 00:31:08.440]   - Right. So these band-aids, but they do it only in response to the most outrageous stuff.
[00:31:08.440 --> 00:31:13.440]   So it's possible this lobbying past, maybe it gets signed, it's held up instantly in the courts.
[00:31:13.440 --> 00:31:20.440]   Obviously, you can imagine every giant tech company files for injunction, and it takes years to resolve conceivably.
[00:31:20.440 --> 00:31:30.440]   - I also can imagine every state legislature, well, at least Texas, Florida, and others saying, "Oh, good idea," and passing a similar bill,
[00:31:30.440 --> 00:31:32.440]   which makes it harder and harder.
[00:31:32.440 --> 00:31:38.440]   - But with their own slant, like specifications for what is good or bad for children.
[00:31:38.440 --> 00:31:39.440]   - Right.
[00:31:39.440 --> 00:31:47.440]   - You know, obviously, given what we've seen in Texas, that would properly include a lot of anti-LGBTQ sort of points,
[00:31:47.440 --> 00:31:55.440]   as well as, you know, maybe content around women's rights to, you know, choose is also considered bad for children.
[00:31:55.440 --> 00:32:07.440]   I think it's really tricky when you are handing over, like, content controls and censorship opportunities to these different state legislatures
[00:32:07.440 --> 00:32:10.440]   in such a vague way.
[00:32:10.440 --> 00:32:15.440]   - This brings up something, and you added this link to the rundown, Glenn.
[00:32:15.440 --> 00:32:22.440]   I wasn't going to talk about it, but I think it's appropriate to talk about it, which is the battle between CloudFlare and Kiwi Farms,
[00:32:22.440 --> 00:32:25.440]   which is a horrendous, horrific website.
[00:32:25.440 --> 00:32:30.440]   I didn't even want to mention the name of, because it's really been used to docs people, to target people, to swat people.
[00:32:30.440 --> 00:32:36.440]   By the way, both political persuasions is just a nasty, nasty.
[00:32:36.440 --> 00:32:45.440]   CloudFlare was protecting it with their DDoS services, and as recently as a couple of days ago, Matthew Prince said,
[00:32:45.440 --> 00:32:51.440]   "We are not going to stop protecting him. That's what we do. We don't judge.
[00:32:51.440 --> 00:32:57.440]   We just protect these websites, and everybody has the right to speak.
[00:32:57.440 --> 00:33:00.440]   He changed his mind, I think, under incredible pressure."
[00:33:00.440 --> 00:33:07.440]   He says also because we see so many dangerous threats on Kiwi Farms.
[00:33:07.440 --> 00:33:16.440]   - I believe right before CloudFlare ended up making a decision to end its relationship with the website.
[00:33:16.440 --> 00:33:25.440]   There had been posts on the forum about people being like, "Oh, we heard in this podcast that this Twitch streamer was
[00:33:25.440 --> 00:33:31.440]   maybe going to go to, I believe it was a poutine place in Belfast."
[00:33:31.440 --> 00:33:37.440]   They looked up, put a list of every poutine place in Belfast, and somebody had said,
[00:33:37.440 --> 00:33:39.440]   "Oh, I've planted bombs at three of them."
[00:33:39.440 --> 00:33:43.440]   Another one was like, "I have men with guns waiting outside of one of them."
[00:33:43.440 --> 00:33:45.440]   It escalated dramatically.
[00:33:45.440 --> 00:33:53.440]   - This is a Canadian Twitch streamer, Clara Cerenity, who actually fled to Ireland to be safe from Kiwi Farms threats,
[00:33:53.440 --> 00:33:56.440]   and they just followed her right there.
[00:33:56.440 --> 00:34:00.440]   - She, I believe, had been swatted multiple times.
[00:34:00.440 --> 00:34:01.440]   - Yeah, she did.
[00:34:01.440 --> 00:34:05.440]   - Her whole family had been docked. It escalated beyond...
[00:34:05.440 --> 00:34:09.440]   It escalated, and then escalated again, and then escalated again.
[00:34:09.440 --> 00:34:19.440]   I also think it's interesting that the way that CloudFlare came to this decision with regards to saying,
[00:34:19.440 --> 00:34:22.440]   "No, no, no, we're not going to sever this business relationship."
[00:34:22.440 --> 00:34:25.440]   Again, and again, public statements, and then two days later,
[00:34:25.440 --> 00:34:30.440]   deciding to do it after increased pressure is kind of the exact same way that this broke down,
[00:34:30.440 --> 00:34:34.440]   I believe, with 8chan and Staley Stormra, whenever they kicked them off.
[00:34:34.440 --> 00:34:36.440]   - That's right. They resisted that too.
[00:34:36.440 --> 00:34:37.440]   - Yeah, they resisted that too.
[00:34:37.440 --> 00:34:40.440]   - But then a couple of days later, they did it.
[00:34:40.440 --> 00:34:42.440]   - I have sympathy for the...
[00:34:42.440 --> 00:34:50.440]   And the reason I bring this up is this is kind of another facet of the same argument, which is,
[00:34:50.440 --> 00:34:56.440]   "How do you do this? Do you allow all speech and let God sort it out?
[00:34:56.440 --> 00:35:02.440]   Do you prohibit... do you attempt to lock it down to protect everybody?
[00:35:02.440 --> 00:35:07.440]   Or is there some middle ground?" And I don't know if any of those solutions work.
[00:35:07.440 --> 00:35:13.440]   - This is one of those places where I feel like I've got a pet peeve about people who cry at censorship
[00:35:13.440 --> 00:35:17.440]   at a lot of these things, because people are like, "Oh, free speech everywhere,
[00:35:17.440 --> 00:35:20.440]   and you can have all the speech you want, and it doesn't matter."
[00:35:20.440 --> 00:35:22.440]   But these are businesses.
[00:35:22.440 --> 00:35:25.440]   So what we're talking about with this particular company...
[00:35:25.440 --> 00:35:28.440]   - Oh, it's not illegal for Cloudflare to block them or unblock them.
[00:35:28.440 --> 00:35:29.440]   - Yeah.
[00:35:29.440 --> 00:35:31.440]   - They can decide who they want to do business with.
[00:35:31.440 --> 00:35:34.440]   When I worked back in Macworld, people complain on our forums all the time.
[00:35:34.440 --> 00:35:37.440]   They're like, "The censorship and threads got shut down," and whatever.
[00:35:37.440 --> 00:35:40.440]   It's like, "Whatever, we're a business. We get to choose all these censorship."
[00:35:40.440 --> 00:35:41.440]   - This is not censorship if the government does it.
[00:35:41.440 --> 00:35:45.440]   - This is my exact response for everybody complaining about losing their Twitter account.
[00:35:45.440 --> 00:35:48.440]   So I'm like, "You're not being censored. It's a business."
[00:35:48.440 --> 00:35:50.440]   - That's not what this is.
[00:35:50.440 --> 00:35:53.440]   That's capitalism, people. That is what you sign up for.
[00:35:53.440 --> 00:35:55.440]   - This is the free market.
[00:35:55.440 --> 00:36:03.440]   - We never see this with Cloudflare standing up for this LGBTQ site
[00:36:03.440 --> 00:36:09.440]   that expresses extreme inclusiveness to an extent that bigots all over the world are so angry
[00:36:09.440 --> 00:36:14.440]   they're demanding Cloudflare drop it, even though the site is engaged in peaceful posting
[00:36:14.440 --> 00:36:19.440]   of articles about drag queens reading at libraries and people getting married around the world
[00:36:19.440 --> 00:36:23.440]   and the increase in inclusive, non-toxic environments.
[00:36:23.440 --> 00:36:27.440]   You don't see Cloudflare out there because it's always violence.
[00:36:27.440 --> 00:36:29.440]   It's always the right wing.
[00:36:29.440 --> 00:36:32.440]   It's always fascism or flirting or way over the line.
[00:36:32.440 --> 00:36:33.440]   - Although...
[00:36:33.440 --> 00:36:34.440]   - If the Samitism...
[00:36:34.440 --> 00:36:37.440]   - Kiwi Farms was used to swat Marjorie Taylor Greene.
[00:36:37.440 --> 00:36:39.440]   - Yeah, I feel like equal opportunity.
[00:36:39.440 --> 00:36:41.440]   - I mean, spotting anybody. Bad idea.
[00:36:41.440 --> 00:36:42.440]   - Bad.
[00:36:42.440 --> 00:36:45.440]   - Even this is my one chance. I'll come for Marjorie Taylor Greene.
[00:36:45.440 --> 00:36:46.440]   Shouldn't be swatted.
[00:36:46.440 --> 00:36:47.440]   - Bad.
[00:36:47.440 --> 00:36:48.440]   - No, but it's what...
[00:36:48.440 --> 00:36:50.440]   - No, but it's fascinating because it's all these people...
[00:36:50.440 --> 00:36:52.440]   This is where you get back into the whole...
[00:36:52.440 --> 00:36:57.440]   Most terrorism seems to arise out of domestic terrorism as in domestic in the house.
[00:36:57.440 --> 00:37:01.440]   You find most of the people get involved in these kinds of efforts have trouble already.
[00:37:01.440 --> 00:37:02.440]   That's well known.
[00:37:02.440 --> 00:37:08.440]   They already commit or are victims of domestic partner or familial violence.
[00:37:08.440 --> 00:37:10.440]   And so you keep seeing these things writ large.
[00:37:10.440 --> 00:37:15.440]   So it's like they find the weakest target, which right now is trans people remain a
[00:37:15.440 --> 00:37:17.440]   vulnerable target worldwide.
[00:37:17.440 --> 00:37:21.440]   And they attack them because they feel like they will get the most support from even
[00:37:21.440 --> 00:37:23.440]   people who otherwise would be negative.
[00:37:23.440 --> 00:37:25.440]   But then it's always about violence.
[00:37:25.440 --> 00:37:29.440]   So they go up to Marjorie Taylor Greene for who knows what reason, totally unacceptable,
[00:37:29.440 --> 00:37:34.440]   that they would do this, put her family at risk, whatever you think about her, obviously.
[00:37:34.440 --> 00:37:35.440]   But they don't have any discrimination.
[00:37:35.440 --> 00:37:36.440]   So you think...
[00:37:36.440 --> 00:37:37.440]   I always go back to that.
[00:37:37.440 --> 00:37:38.440]   Like they came for...
[00:37:38.440 --> 00:37:41.440]   They come for one group first, but they're always going to come for another, another,
[00:37:41.440 --> 00:37:44.440]   another, and you're always going to be in one of those groups.
[00:37:44.440 --> 00:37:46.440]   It means that's the issue more than anything, right?
[00:37:46.440 --> 00:37:47.440]   It's not...
[00:37:47.440 --> 00:37:49.440]   It's how they're going about it, right?
[00:37:49.440 --> 00:37:52.440]   I mean, I think about this too when I see stuff in the politics around where people are like,
[00:37:52.440 --> 00:37:54.440]   "Oh, if this happens, there'll be violence in the street."
[00:37:54.440 --> 00:37:58.440]   And again, to Glenn's point, you never hear the people who are about inclusiveness
[00:37:58.440 --> 00:38:02.440]   and diversity and like, "Oh, there'll be violence in the street if there's not inclusion
[00:38:02.440 --> 00:38:03.440]   and diversity."
[00:38:03.440 --> 00:38:04.440]   No, that's not how it works.
[00:38:04.440 --> 00:38:06.440]   So yeah, it's about the means of how you're going about...
[00:38:06.440 --> 00:38:10.440]   I have a friend who has actually been targeting by these kinds of people,
[00:38:10.440 --> 00:38:17.440]   and it is truly horrific, the kind of things that they have done in order to try and basically
[00:38:17.440 --> 00:38:20.440]   just shut these people up and harass them into silence.
[00:38:20.440 --> 00:38:23.440]   Or for often, for very little provocation at all,
[00:38:23.440 --> 00:38:25.440]   other than they just feel like doing it.
[00:38:25.440 --> 00:38:27.440]   So yeah, I don't know.
[00:38:27.440 --> 00:38:29.440]   I'm glad that Cloudflare make this decision.
[00:38:29.440 --> 00:38:32.440]   It's a shame it took them so long and it's not going to solve the problem,
[00:38:32.440 --> 00:38:34.440]   but it's a step in the right direction.
[00:38:34.440 --> 00:38:37.440]   The thing I've seen said, which I think is worth repeating,
[00:38:37.440 --> 00:38:41.440]   is that freedom of speech, freedom of expression,
[00:38:41.440 --> 00:38:46.440]   can be suppressed by people who are also engaging in freedom of expression.
[00:38:46.440 --> 00:38:50.440]   So when Cloudflare says, "We're about maximalist free speech,
[00:38:50.440 --> 00:38:53.440]   we don't want to suppress anybody," because then governments come to us
[00:38:53.440 --> 00:38:56.440]   and tell us to take human rights groups off the internet,
[00:38:56.440 --> 00:38:59.440]   which to me seems like a species argument to begin with.
[00:38:59.440 --> 00:39:01.440]   There's a bright, light difference between groups.
[00:39:01.440 --> 00:39:05.440]   If a group's advocating violence and doxing people
[00:39:05.440 --> 00:39:08.440]   and involved in the coordination of harassment and abuse,
[00:39:08.440 --> 00:39:12.440]   it's quite different than a group saying this government's committing human rights violations.
[00:39:12.440 --> 00:39:19.440]   But the fact is these groups that are harassment are decreasing the amount of freedom of speech of other people.
[00:39:19.440 --> 00:39:21.440]   And I think there has to be a balance.
[00:39:21.440 --> 00:39:24.440]   When you're dealing with extremists, you can say,
[00:39:24.440 --> 00:39:26.440]   some people are never going to like trans people,
[00:39:26.440 --> 00:39:30.440]   they're never like the concept that Jewish people exist or whatever.
[00:39:30.440 --> 00:39:34.440]   I mean, I can live with that. I can't, I don't try to live with that.
[00:39:34.440 --> 00:39:38.440]   But it's like I accept that people may have bigoted ideas
[00:39:38.440 --> 00:39:43.440]   or terrible ideas that affect me and my family and my community, my country, whatever.
[00:39:43.440 --> 00:39:48.440]   I can accept that, but facilitating them to coordinate activities
[00:39:48.440 --> 00:39:52.440]   that are intended to suppress speech and harm lives,
[00:39:52.440 --> 00:39:54.440]   that is a very bright line.
[00:39:54.440 --> 00:39:59.440]   And it suppresses my ability, it suppresses everyone around me's ability to be able to have their own version of free speech.
[00:39:59.440 --> 00:40:04.440]   To Glenn's point, when I was in college, one of the jobs I had was during,
[00:40:04.440 --> 00:40:09.440]   in subsequent years after I was a freshman, I was in an orientation group
[00:40:09.440 --> 00:40:13.440]   where we taught incoming students about using all the text systems at our school.
[00:40:13.440 --> 00:40:15.440]   We said, "Here's how you use your email," whatever.
[00:40:15.440 --> 00:40:19.440]   And we went over free speech, hate speech and harassment.
[00:40:19.440 --> 00:40:22.440]   And this was in like 1999 and 2000.
[00:40:22.440 --> 00:40:26.440]   And we could, if I could teach hundreds of incoming freshmen the difference,
[00:40:26.440 --> 00:40:29.440]   then, you know, we can figure this out.
[00:40:29.440 --> 00:40:37.440]   It seems so difficult, though, to know exactly where the line is drawn.
[00:40:37.440 --> 00:40:41.440]   Is that maybe is violence where the line is drawn?
[00:40:41.440 --> 00:40:44.440]   But I would balance this certainly one of the lines.
[00:40:44.440 --> 00:40:51.440]   I think that it also shouldn't be a terrible thing if you have a group of people
[00:40:51.440 --> 00:40:56.440]   where they are actively coordinating to harass others.
[00:40:56.440 --> 00:41:00.440]   And do you want to actively coordinate a group of people to say,
[00:41:00.440 --> 00:41:04.440]   "Hey, business that's making money from this group, protecting them,
[00:41:04.440 --> 00:41:06.440]   we think that's a bad business relationship.
[00:41:06.440 --> 00:41:08.440]   You should be allowed to do that."
[00:41:08.440 --> 00:41:09.440]   I think that's not a big deal.
[00:41:09.440 --> 00:41:15.440]   And if CloudFlare is deeply annoyed and frustrated by it, so be it.
[00:41:15.440 --> 00:41:18.440]   You're the one keeping this business relationship going.
[00:41:18.440 --> 00:41:23.440]   The CEO was calling that bullying, the fact that people who are under the gun,
[00:41:23.440 --> 00:41:27.440]   being attacked, stalked, forced from their homes, people committing self-harm,
[00:41:27.440 --> 00:41:33.440]   that people standing up for them against Kiwi farms and against CloudFlare's relationship was bullying.
[00:41:33.440 --> 00:41:35.440]   And that is bullying...
[00:41:35.440 --> 00:41:37.440]   [laughter]
[00:41:37.440 --> 00:41:39.440]   Should Libs of TikTok be banned?
[00:41:39.440 --> 00:41:43.440]   This is a big controversy on Twitter right now.
[00:41:43.440 --> 00:41:50.440]   I think that the $1.3 million followers is not actively inciting violence,
[00:41:50.440 --> 00:41:57.440]   although it paints LGBTQ teachers of being pedophiles and groomers.
[00:41:57.440 --> 00:42:02.440]   I mean, I do think it's worth noting that Libs of TikTok's content
[00:42:02.440 --> 00:42:11.440]   and specific hyper-focus on children's hospitals that also treat trans children
[00:42:11.440 --> 00:42:17.440]   and are also treated seemingly resulted in bomb threats being called to children's hospitals.
[00:42:17.440 --> 00:42:19.440]   I want to...
[00:42:19.440 --> 00:42:23.440]   That's Boston Children's Hospital right down the street for me, basically, where I am.
[00:42:23.440 --> 00:42:25.440]   Yeah, that's a big deal.
[00:42:25.440 --> 00:42:28.440]   That is a major hospital that does a lot of important work,
[00:42:28.440 --> 00:42:31.440]   including the kinds of things they're getting harassed for.
[00:42:31.440 --> 00:42:34.440]   And to call on a bomb threat on a hospital?
[00:42:34.440 --> 00:42:36.440]   But is Libs a TikTok?
[00:42:36.440 --> 00:42:38.440]   I mean, look at their indication.
[00:42:38.440 --> 00:42:42.440]   As we 100% condemn any acts or threats of violence,
[00:42:42.440 --> 00:42:45.440]   they are not saying go bomb the hospitals, but...
[00:42:45.440 --> 00:42:48.440]   It's plausible that, you know, will nobody rid me of the troublesome priest
[00:42:48.440 --> 00:42:50.440]   as still an incitement to violence?
[00:42:50.440 --> 00:42:51.440]   Yeah, yeah.
[00:42:51.440 --> 00:42:52.440]   There you go.
[00:42:52.440 --> 00:42:55.440]   I mean, that's a great reference there on that one.
[00:42:55.440 --> 00:42:56.440]   Yeah.
[00:42:56.440 --> 00:42:57.440]   Yeah.
[00:42:57.440 --> 00:43:00.440]   I haven't taken a close look at their tweets as a white.
[00:43:00.440 --> 00:43:03.440]   The only tweet I happened to see last night was, I believe,
[00:43:03.440 --> 00:43:05.440]   I'm forgetting the woman's name behind the account,
[00:43:05.440 --> 00:43:11.440]   but she had tweeted, "Oh, I'm prepared for Twitter to suspend or block my account."
[00:43:11.440 --> 00:43:14.440]   And once they do, I'm going to sue Twitter and everybody there,
[00:43:14.440 --> 00:43:18.440]   because they're censoring me and taking away my freedom to speech.
[00:43:18.440 --> 00:43:21.440]   And I'm like, "That's not how any of this works, sweetie.
[00:43:21.440 --> 00:43:26.440]   Like, you losing your Twitter account is not a First Amendment violation."
[00:43:26.440 --> 00:43:27.440]   No.
[00:43:27.440 --> 00:43:28.440]   No.
[00:43:28.440 --> 00:43:30.440]   Twitter has not banned them as of...
[00:43:30.440 --> 00:43:31.440]   No.
[00:43:31.440 --> 00:43:32.440]   They suspended them for weeks, but...
[00:43:32.440 --> 00:43:38.440]   I think they got one slap on the wrist for one tweet, which, like everybody else in the world who's gotten one tweet flagged,
[00:43:38.440 --> 00:43:40.440]   you delete it and you move on.
[00:43:40.440 --> 00:43:41.440]   Right.
[00:43:41.440 --> 00:43:42.440]   Like Jordan Peterson.
[00:43:42.440 --> 00:43:43.440]   It's a...
[00:43:43.440 --> 00:43:44.440]   I such a...
[00:43:44.440 --> 00:43:45.440]   Yeah.
[00:43:45.440 --> 00:43:49.440]   It's a difficult thing to know how to do it.
[00:43:49.440 --> 00:43:52.440]   It is not de-platforming somebody to kick him off of Twitter.
[00:43:52.440 --> 00:43:53.440]   I'm sorry.
[00:43:53.440 --> 00:43:54.440]   It's...
[00:43:54.440 --> 00:43:55.440]   Yeah.
[00:43:55.440 --> 00:43:58.440]   A brief aside since you mentioned Jordan Peterson, though, I think all the funniest things is if you look back
[00:43:58.440 --> 00:44:02.440]   over all the different times that he's been suspended or something and been like, "That's it.
[00:44:02.440 --> 00:44:03.440]   I'm done with Twitter.
[00:44:03.440 --> 00:44:07.440]   This platform is terrible and I refuse to, you know, acknowledge this.
[00:44:07.440 --> 00:44:10.440]   Next time they try to suspend one of my tweets, I'm never going to delete it from back."
[00:44:10.440 --> 00:44:12.440]   He says that and then like...
[00:44:12.440 --> 00:44:13.440]   Every time.
[00:44:13.440 --> 00:44:14.440]   22 hours later, he's back.
[00:44:14.440 --> 00:44:15.440]   That man is addicted.
[00:44:15.440 --> 00:44:16.440]   He could not stop.
[00:44:16.440 --> 00:44:23.440]   I gotta say, the point of joy I have right now is watching Alex Jones being raked over the coals over and over and over again
[00:44:23.440 --> 00:44:29.440]   and all of his garbage exposed and also the success and lack of success of so-called de-platforming, right?
[00:44:29.440 --> 00:44:35.440]   So Alex Jones lost his access to a number of platforms and yet he apparently continues to make tens of millions
[00:44:35.440 --> 00:44:38.440]   or more dollars a year from selling snake oil to people.
[00:44:38.440 --> 00:44:46.440]   But it's been great to see that grimy underbelly, like fully the rock picked up and exposed to understand exactly the ecosystem
[00:44:46.440 --> 00:44:50.440]   and what the value is to people spreading misinformation and hatred.
[00:44:50.440 --> 00:44:51.440]   This stuff is so hard, though.
[00:44:51.440 --> 00:44:56.440]   I mean, I can't really blame Twitter for trying to thread this needle.
[00:44:56.440 --> 00:45:04.440]   I may disagree with what they've done in some cases and not in others, but it's a hard thing.
[00:45:04.440 --> 00:45:09.440]   It is a very hard thing to do and it's hard to know what the line is, right?
[00:45:09.440 --> 00:45:12.440]   They also put themselves in that position, right?
[00:45:12.440 --> 00:45:16.440]   I mean, they open that can of worms by creating the product that they did.
[00:45:16.440 --> 00:45:23.440]   And to a certain degree, they are responsible for policing it in a responsible manner, right?
[00:45:23.440 --> 00:45:30.440]   So on the other one hand, yeah, there is some trickiness to this, but it's a problem that could be, at least if not solved,
[00:45:30.440 --> 00:45:33.440]   addressed by prioritizing it.
[00:45:33.440 --> 00:45:38.440]   And it always feels like they kind of want to throw up their hands and be like, "Oh, well, yeah, there's nothing we can do.
[00:45:38.440 --> 00:45:40.440]   We don't make the rules."
[00:45:40.440 --> 00:45:48.440]   The main character of the day, the problem at Twitter, which has, I think, gone up and down at times,
[00:45:48.440 --> 00:45:50.440]   is nobody wants to be the main character of the day.
[00:45:50.440 --> 00:45:51.440]   That's your job in life.
[00:45:51.440 --> 00:45:53.440]   If you participate in social media, don't do it.
[00:45:53.440 --> 00:45:56.440]   I know some people who have become the main character of the day.
[00:45:56.440 --> 00:45:58.440]   It's not very enjoyable to them ever.
[00:45:58.440 --> 00:45:59.440]   Occasionally it's positive.
[00:45:59.440 --> 00:46:00.440]   On either side.
[00:46:00.440 --> 00:46:01.440]   On any side, right?
[00:46:01.440 --> 00:46:02.440]   Any part of it, right?
[00:46:02.440 --> 00:46:07.440]   And so Twitter has built a machine that creates main characters of the day, sometimes multiple ones.
[00:46:07.440 --> 00:46:15.440]   And the issue is a right wing accounts typically, but accounts that want to encourage harassment
[00:46:15.440 --> 00:46:18.440]   up to the point of violence and real harm.
[00:46:18.440 --> 00:46:29.440]   They have figured out how to leverage the Twitter main figure of the day algorithmic feed so that they can cause harm by fluttering butterfly wings, right?
[00:46:29.440 --> 00:46:43.440]   So they can say something that isn't directly, doesn't meet any kind of First Amendment test, even of imminent harm, and doesn't meet Twitter's direct tests about incitement to violence or commitment of hatred speech, but they are hate speech.
[00:46:43.440 --> 00:46:55.440]   But they surface up these accounts like Libs of TikTok, surface up people who are absolutely innocuous and make them the five-minute hate figure, what's it for matching A4.
[00:46:55.440 --> 00:47:03.440]   And everybody in the world who is of that ilk gets that amplified out to them through the mechanism of Twitter and then other subsidiary mechanisms.
[00:47:03.440 --> 00:47:08.440]   And that person becomes a subject of this for no reason it should happen.
[00:47:08.440 --> 00:47:10.440]   It's a failure of the network that that happens.
[00:47:10.440 --> 00:47:11.440]   I want to take a break.
[00:47:11.440 --> 00:47:16.440]   When we come back, he used AI to win a fine arts competition.
[00:47:16.440 --> 00:47:18.440]   Was it cheating?
[00:47:18.440 --> 00:47:21.440]   This is going to be a fun conversation.
[00:47:21.440 --> 00:47:22.440]   A great panel.
[00:47:22.440 --> 00:47:24.440]   You guys, I love this.
[00:47:24.440 --> 00:47:27.440]   This is going to be a long one I can tell because I don't ever want to stop.
[00:47:27.440 --> 00:47:32.440]   We have people like Dan Morin here from sixcolors.com.
[00:47:32.440 --> 00:47:33.440]   Glenn Fleischman.
[00:47:33.440 --> 00:47:35.440]   Glenn F.
[00:47:35.440 --> 00:47:36.440]   Glenn.fun.
[00:47:36.440 --> 00:47:37.440]   I'm sorry.
[00:47:37.440 --> 00:47:41.440]   I don't, my F key is broken, so I can only go to Glenn.un.
[00:47:41.440 --> 00:47:42.440]   Could you?
[00:47:42.440 --> 00:47:44.440]   I'll like you register that to me.
[00:47:44.440 --> 00:47:48.440]   I'll join the United Nations.
[00:47:48.440 --> 00:47:51.440]   It's just, it's falling off.
[00:47:51.440 --> 00:47:56.440]   And somebody who lost her slash and question mark key, but survived.
[00:47:56.440 --> 00:47:57.440]   Parris Martin, no.
[00:47:57.440 --> 00:47:58.440]   Reporter for the Internet.
[00:47:58.440 --> 00:48:02.440]   Listen, you know, I learned to ask fewer questions and I think that's all right.
[00:48:02.440 --> 00:48:04.440]   And go to fewer websites.
[00:48:04.440 --> 00:48:05.440]   Yeah.
[00:48:05.440 --> 00:48:07.440]   All my websites are insecure now.
[00:48:07.440 --> 00:48:08.440]   Right.
[00:48:08.440 --> 00:48:09.440]   No slash slash.
[00:48:09.440 --> 00:48:12.440]   Our show today brought to you by podium.
[00:48:12.440 --> 00:48:15.440]   If you own a small business, it's, you know, congratulations.
[00:48:15.440 --> 00:48:17.440]   You're still, you're still running.
[00:48:17.440 --> 00:48:19.440]   It's been tough, tough few years.
[00:48:19.440 --> 00:48:25.040]   From supply chain issues, increased demand, business owners have to manage the businesses
[00:48:25.040 --> 00:48:29.440]   that are thriving right now are the ones who are forward thinking and that's why I love
[00:48:29.440 --> 00:48:30.440]   podium.
[00:48:30.440 --> 00:48:37.440]   I, you know, there are a number of businesses in our area that use podium and when I love
[00:48:37.440 --> 00:48:42.040]   it, I'll leave the, my dentist and they, as I'm leaving, I get a text message saying,
[00:48:42.040 --> 00:48:48.200]   "Don't forget your appointment is coming up in six weeks or whatever and, or maybe would
[00:48:48.200 --> 00:48:52.200]   you like to leave a review on Yelp or, you know, that kind of thing.
[00:48:52.200 --> 00:48:57.400]   Potium helps your small business stay ahead of the curve with modern messaging tools that
[00:48:57.400 --> 00:49:01.720]   make it easy for your customers to connect with your business.
[00:49:01.720 --> 00:49:04.160]   And we have this one thing we learned during the pandemic.
[00:49:04.160 --> 00:49:05.800]   No one wants to make a phone call.
[00:49:05.800 --> 00:49:06.800]   Texting is it.
[00:49:06.800 --> 00:49:08.680]   A lot of people hate calling a business.
[00:49:08.680 --> 00:49:10.800]   I don't care if it's a plumber, a landscaper.
[00:49:10.800 --> 00:49:13.480]   I hate playing phone tag.
[00:49:13.480 --> 00:49:16.560]   If I could just send a quick tech, text message, you get a text message back.
[00:49:16.560 --> 00:49:18.720]   I know that's, that's the way to do it.
[00:49:18.720 --> 00:49:20.700]   Well, that's what podium does.
[00:49:20.700 --> 00:49:23.520]   If you're running a business and the only way to get in touch with you is a phone number
[00:49:23.520 --> 00:49:28.480]   attached to an answering machine or a service, you, you're probably losing customers.
[00:49:28.480 --> 00:49:33.060]   Potium gives businesses the tools to compete with the convenience that, you know, big businesses
[00:49:33.060 --> 00:49:36.360]   have known about for a long time.
[00:49:36.360 --> 00:49:41.360]   From healthcare providers to plumbers, over a hundred thousand businesses are texting with
[00:49:41.360 --> 00:49:44.560]   customers through podium.
[00:49:44.560 --> 00:49:47.880]   And by the way, not only do customers love it, you will love the results.
[00:49:47.880 --> 00:49:50.880]   One car dealer sold a truck and just four text messages.
[00:49:50.880 --> 00:49:56.760]   A jeweler sold a ringing coordinated curbside pickup, did it all through texts and the customers
[00:49:56.760 --> 00:49:57.760]   love it.
[00:49:57.760 --> 00:50:00.960]   A dentist had a bunch of outstanding payments.
[00:50:00.960 --> 00:50:04.680]   He'd been sending a mail, you know, trying to get, he sent a payment request through
[00:50:04.680 --> 00:50:11.720]   text got 70% of the outstanding collections in just two weeks because it works.
[00:50:11.720 --> 00:50:14.960]   I don't remember, but the open rate I think on text is well over 90%.
[00:50:14.960 --> 00:50:19.360]   It's the number one way to reach people because it's effective.
[00:50:19.360 --> 00:50:23.280]   With podiums all in one inbox, your employees will love it too.
[00:50:23.280 --> 00:50:26.520]   It all goes into the inbox and you could do more than just chat.
[00:50:26.520 --> 00:50:29.200]   You can get online reviews by sending an easy to use link.
[00:50:29.200 --> 00:50:32.120]   You can collect payments, send marketing campaigns.
[00:50:32.120 --> 00:50:34.720]   I get, and I'm ashamed to say it really works.
[00:50:34.720 --> 00:50:37.920]   I get our local ice cream parlor every few weeks.
[00:50:37.920 --> 00:50:39.560]   I'll get something saying, Hey, we haven't seen you in a while.
[00:50:39.560 --> 00:50:41.040]   How about 30% off a pint?
[00:50:41.040 --> 00:50:42.040]   And it works.
[00:50:42.040 --> 00:50:43.840]   Gosh darn it every single time.
[00:50:43.840 --> 00:50:49.280]   All by a quick text, your employees can stay in touch with customers in one unified inbox.
[00:50:49.280 --> 00:50:51.320]   See how podium could grow your business.
[00:50:51.320 --> 00:50:52.320]   Watch a demo today.
[00:50:52.320 --> 00:50:54.560]   You just go to podium.com/twit.
[00:50:54.560 --> 00:50:57.800]   P-O-D-I-U-M.com/twit.
[00:50:57.800 --> 00:51:01.720]   If you want to know more, you can learn the facts are there.
[00:51:01.720 --> 00:51:05.920]   And I think you probably, if you think about it already know, this is how your customers
[00:51:05.920 --> 00:51:06.920]   want to do business.
[00:51:06.920 --> 00:51:08.320]   Why don't you do it that way too?
[00:51:08.320 --> 00:51:11.000]   podium.com/twit.
[00:51:11.000 --> 00:51:12.000]   podium.
[00:51:12.000 --> 00:51:14.600]   Let's grow the ultimate text messaging platform.
[00:51:14.600 --> 00:51:20.200]   We thank them so much for supporting this week in tech.
[00:51:20.200 --> 00:51:26.800]   He, I loved this story, but it's actually kind of a deeper question.
[00:51:26.800 --> 00:51:32.560]   He won a fine arts competition, but was it cheating?
[00:51:32.560 --> 00:51:40.840]   This is a Colorado State Fair digital, digital category, the digitally manipulated photography
[00:51:40.840 --> 00:51:41.840]   category.
[00:51:41.840 --> 00:51:49.000]   Jason Allen won beating 20 other artists, Blue Ribbon $300 prize with an artwork he created
[00:51:49.000 --> 00:51:54.800]   through mid journey, which is one of the new generative art tools that are just taking
[00:51:54.800 --> 00:51:55.800]   off.
[00:51:55.800 --> 00:51:58.720]   Somebody said it's a Cambrian explosion of AI art.
[00:51:58.720 --> 00:52:02.560]   And I think that may be, may be accurate.
[00:52:02.560 --> 00:52:07.840]   The portrait, beautiful portrait looked like Renaissance art.
[00:52:07.840 --> 00:52:12.000]   Although if I, when I look at it, I can tell that that's, that's, that's AI generated.
[00:52:12.000 --> 00:52:18.120]   I mean, that's not, that's clear to me, but maybe the judges didn't have as much experience
[00:52:18.120 --> 00:52:19.520]   with this stuff.
[00:52:19.520 --> 00:52:27.480]   Is it fair to, I mean, he says, well, I wrote the prompt and then he imported in Photoshop
[00:52:27.480 --> 00:52:28.960]   and fixed it up a little bit.
[00:52:28.960 --> 00:52:31.200]   So what category is it?
[00:52:31.200 --> 00:52:32.640]   Was it in like painting?
[00:52:32.640 --> 00:52:34.080]   No, no, no.
[00:52:34.080 --> 00:52:37.000]   Normally they have a digitally manipulated photography.
[00:52:37.000 --> 00:52:39.640]   Okay, then that's fair.
[00:52:39.640 --> 00:52:40.640]   I'm hard to argue about that.
[00:52:40.640 --> 00:52:42.240]   No, I mean, that is exactly what it is.
[00:52:42.240 --> 00:52:43.240]   Yeah.
[00:52:43.240 --> 00:52:49.040]   I mean, I'm gonna say, I think if you're talking about high art or experimental art, using
[00:52:49.040 --> 00:52:54.720]   an AI to make a weird painting that kind of looks like a computer made it, that is art
[00:52:54.720 --> 00:52:57.440]   in and of itself.
[00:52:57.440 --> 00:53:02.880]   And if the judges don't know and they still look at it as just a work itself and they're
[00:53:02.880 --> 00:53:04.080]   like, yeah, that's great.
[00:53:04.080 --> 00:53:05.080]   We love that.
[00:53:05.080 --> 00:53:07.040]   If they didn't know it was AI, I don't know.
[00:53:07.040 --> 00:53:11.360]   It's hard to, I find it hard to ding them on that even as someone who, you know, writes
[00:53:11.360 --> 00:53:15.240]   creatively and I feel like if an AI wrote a book and that book won a competition, I
[00:53:15.240 --> 00:53:19.240]   would be a little miffed, but I also don't know that that would just probably make me
[00:53:19.240 --> 00:53:20.240]   a hypocrite that.
[00:53:20.240 --> 00:53:21.400]   Like you got a hand to do it.
[00:53:21.400 --> 00:53:24.800]   Like you've some of the AI made a better book or art.
[00:53:24.800 --> 00:53:25.800]   Right?
[00:53:25.800 --> 00:53:26.800]   Like, yeah.
[00:53:26.800 --> 00:53:29.720]   I have a degree in art.
[00:53:29.720 --> 00:53:34.080]   It doesn't make me better informed on this, but I'd said a lot of art history as part
[00:53:34.080 --> 00:53:35.160]   of that degree.
[00:53:35.160 --> 00:53:42.200]   And one of the courses looked at the concept of connoisseurship, which is how art is evaluated
[00:53:42.200 --> 00:53:43.840]   for its quality, right?
[00:53:43.840 --> 00:53:48.200]   So both by like, you know, price, but also how do you evaluate pieces of art and say
[00:53:48.200 --> 00:53:51.400]   this is better or worse or this is a masterwork and so forth.
[00:53:51.400 --> 00:53:57.480]   And I think that this actually gets into that, you know, very obstruse little thing, like
[00:53:57.480 --> 00:54:01.880]   connoisseurship is that there are, you know, this whole thing about kits, right?
[00:54:01.880 --> 00:54:05.960]   Kitsch was a concept developed by a Clinton Greenberg, like, I don't know, I think 80
[00:54:05.960 --> 00:54:10.160]   years ago to describe art that was predigested.
[00:54:10.160 --> 00:54:12.040]   You looked at it and it required no interpretation.
[00:54:12.040 --> 00:54:16.920]   It was talking about a Soviet art and other art that was super pedagogical and designed
[00:54:16.920 --> 00:54:20.240]   to just be the kind of, you know, pabulum to the masses.
[00:54:20.240 --> 00:54:22.640]   You looked at it, it's like, this is the message, right?
[00:54:22.640 --> 00:54:26.520]   And that the opposite of kitsch is something that requires, you know, this interpretation,
[00:54:26.520 --> 00:54:30.120]   you look at it and there's a perception and it was, you know, Jackson Pollack and all
[00:54:30.120 --> 00:54:35.640]   the people doing abstract impressionism, dot ism before it, surrealism and then a pop
[00:54:35.640 --> 00:54:40.920]   art and later movements, they all rely on this impression that art is something beyond
[00:54:40.920 --> 00:54:43.960]   just looking at a thing and saying that's a picture of a cake, right?
[00:54:43.960 --> 00:54:47.080]   So I'm sorry to get so deep into art history stuff here.
[00:54:47.080 --> 00:54:51.840]   It applies because you're like, on what basis do the judges evaluate this?
[00:54:51.840 --> 00:54:56.280]   Are the components that they evaluated as being winning work?
[00:54:56.280 --> 00:54:59.600]   Ones where they were mistaken things that they should have actually been looking at
[00:54:59.600 --> 00:55:01.960]   it more carefully or is it justified?
[00:55:01.960 --> 00:55:06.080]   Does this work actually because of the sources in which it derives and how the algorithm
[00:55:06.080 --> 00:55:07.480]   has recombined it?
[00:55:07.480 --> 00:55:11.560]   Does it make it justifiably something that you can compare and use that kind of search
[00:55:11.560 --> 00:55:12.560]   up today?
[00:55:12.560 --> 00:55:15.680]   This is actually equivalent to other work of this caliber.
[00:55:15.680 --> 00:55:17.680]   Or is it just prettier?
[00:55:17.680 --> 00:55:18.680]   Is it prettier?
[00:55:18.680 --> 00:55:24.760]   I will say we do need to see what the other art works, competing works, maybe they all
[00:55:24.760 --> 00:55:25.960]   really sucked.
[00:55:25.960 --> 00:55:27.680]   We're not considering that.
[00:55:27.680 --> 00:55:33.720]   Two, if we're looking at this from an art history perspective, I think AI generated
[00:55:33.720 --> 00:55:37.680]   art would be a very futurist, like piece.
[00:55:37.680 --> 00:55:43.040]   Like, this is kind of what the early futurist movement, like around impressionism, was talking
[00:55:43.040 --> 00:55:49.800]   about, which is that art is not precious, art can be fast, art is like movement and technology.
[00:55:49.800 --> 00:55:56.440]   And if, in this case, art is typing in a couple words in a screen, that's art, baby.
[00:55:56.440 --> 00:55:58.120]   I actually go beyond that.
[00:55:58.120 --> 00:56:04.040]   I think that the skill involved in typing the prompt, because it isn't usually just a few
[00:56:04.040 --> 00:56:05.040]   words.
[00:56:05.040 --> 00:56:11.080]   It's usually elaborate and you often refine it, is a form of computer programming.
[00:56:11.080 --> 00:56:19.240]   And I think maybe the future of computer programming, as we interact with machines, with AI, computer
[00:56:19.240 --> 00:56:23.560]   programming up to this point, you're very specific, the computer's going to do exactly
[00:56:23.560 --> 00:56:25.840]   what you said no more, no less.
[00:56:25.840 --> 00:56:29.800]   But this is now the new way of communicating with a computer.
[00:56:29.800 --> 00:56:32.160]   It's more of a conversation.
[00:56:32.160 --> 00:56:35.800]   I think that that's actually the new form of programming.
[00:56:35.800 --> 00:56:38.640]   And I think we're going to see in a lot of areas.
[00:56:38.640 --> 00:56:45.960]   To me, I think the clear syndication that is a valid form of art is that every new medium
[00:56:45.960 --> 00:56:50.320]   or style is always greeted with that question, but is it art?
[00:56:50.320 --> 00:56:52.320]   Look at the impressionists.
[00:56:52.320 --> 00:56:53.320]   They were reviled.
[00:56:53.320 --> 00:56:57.960]   They had to have their own art show just to get show people their work.
[00:56:57.960 --> 00:56:59.200]   People are asking that question.
[00:56:59.200 --> 00:57:01.680]   I think the answer is usually yes.
[00:57:01.680 --> 00:57:03.920]   And so, yeah, I mean, it's not what you expect.
[00:57:03.920 --> 00:57:09.200]   It's not art, whatever it was just the same.
[00:57:09.200 --> 00:57:13.920]   The prompt for it is a still of Donald Trump and Alex Jones in jail photograph natural light
[00:57:13.920 --> 00:57:19.320]   sharp detailed face magazine press photo, Steve McCurry, David Lazar, Canon Nikon focus.
[00:57:19.320 --> 00:57:21.480]   These prompts, I mean, pick one you like.
[00:57:21.480 --> 00:57:25.240]   Can we put a Leo themed prompt in here?
[00:57:25.240 --> 00:57:26.640]   I did search for my name.
[00:57:26.640 --> 00:57:28.720]   This is the new Google search.
[00:57:28.720 --> 00:57:29.720]   I'm sorry to say.
[00:57:29.720 --> 00:57:30.720]   Was that your first search?
[00:57:30.720 --> 00:57:32.120]   Yeah, one of my first.
[00:57:32.120 --> 00:57:35.560]   So this is a Cohen in the upper left corner there.
[00:57:35.560 --> 00:57:36.560]   Yeah.
[00:57:36.560 --> 00:57:37.560]   Well, this is a search engine.
[00:57:37.560 --> 00:57:40.120]   The search is stable diffusion.
[00:57:40.120 --> 00:57:45.240]   Stable diffusion is responsible for a lot of this Cambrian explosion over the last two
[00:57:45.240 --> 00:57:46.240]   weeks.
[00:57:46.240 --> 00:57:50.240]   It's an open source generator that's been made available to anybody.
[00:57:50.240 --> 00:57:55.320]   You can install it if you have enough horsepower, big enough GPU and run it yourself.
[00:57:55.320 --> 00:57:57.360]   There are a lot of nuances to this story.
[00:57:57.360 --> 00:57:58.520]   A lot of facets to this story.
[00:57:58.520 --> 00:58:03.240]   One of the problems with stable diffusion, according to some, is it uses a lot of images.
[00:58:03.240 --> 00:58:09.360]   We'll talk about this later that are not in the public domain as it's training material.
[00:58:09.360 --> 00:58:14.160]   And yet because people can play with it, we've seen a lot of progress in the stuff we can
[00:58:14.160 --> 00:58:16.240]   generate.
[00:58:16.240 --> 00:58:18.400]   How about I'll just do Adam Driver.
[00:58:18.400 --> 00:58:19.400]   How about that?
[00:58:19.400 --> 00:58:20.400]   Because they're more of him.
[00:58:20.400 --> 00:58:23.240]   A clear one to one for you.
[00:58:23.240 --> 00:58:25.960]   Yeah, me and Adam Driver.
[00:58:25.960 --> 00:58:29.760]   So these are all images that include that.
[00:58:29.760 --> 00:58:32.040]   Now one thing stable diffusion does is it will show...
[00:58:32.040 --> 00:58:33.800]   Oh, there's some Nazi iconography.
[00:58:33.800 --> 00:58:35.600]   Yeah, it's going to say it for some reason.
[00:58:35.600 --> 00:58:41.160]   So this one is a portrait of John Oliver standing next to Adam Driver's stoic full body military
[00:58:41.160 --> 00:58:42.160]   uniform.
[00:58:42.160 --> 00:58:43.160]   Oh, that's weird.
[00:58:43.160 --> 00:58:44.160]   Fantasy, intricate, delicate.
[00:58:44.160 --> 00:58:48.160]   John Oliver did an excellent segment, by the way, recently on during a cabbage.
[00:58:48.160 --> 00:58:49.160]   He did.
[00:58:49.160 --> 00:58:50.160]   And he married a cabbage.
[00:58:50.160 --> 00:58:51.160]   It was beautiful.
[00:58:51.160 --> 00:58:52.160]   Yeah.
[00:58:52.160 --> 00:58:55.160]   In fact, if I search on Oliver, I'll find a lot more of his...
[00:58:55.160 --> 00:58:56.160]   And it looks like he's...
[00:58:56.160 --> 00:58:58.160]   He really just out of Driver.
[00:58:58.160 --> 00:58:59.160]   He's all out of Driver.
[00:58:59.160 --> 00:59:02.360]   I never thought about that, but yeah.
[00:59:02.360 --> 00:59:04.760]   Well, John and Adam have a history, I guess.
[00:59:04.760 --> 00:59:05.760]   But...
[00:59:05.760 --> 00:59:06.760]   Thank you.
[00:59:06.760 --> 00:59:07.760]   Yeah, there's a lot of John Adam.
[00:59:07.760 --> 00:59:08.760]   Oh, that's right.
[00:59:08.760 --> 00:59:09.760]   They have a lot of...
[00:59:09.760 --> 00:59:10.760]   Oh, there's a lot of him...
[00:59:10.760 --> 00:59:14.280]   John Oliver covered in blood in a way that I would not have expected.
[00:59:14.280 --> 00:59:15.920]   Well, that came from...
[00:59:15.920 --> 00:59:17.760]   He did a lot of searches, I think, in preparation.
[00:59:17.760 --> 00:59:23.680]   Well, he was searching on people doing searches of him or about him and then found the one
[00:59:23.680 --> 00:59:25.160]   in which he were ready to cabbage.
[00:59:25.160 --> 00:59:28.560]   Highly recommend watching that segment from the last Sunday.
[00:59:28.560 --> 00:59:29.560]   Oh, God.
[00:59:29.560 --> 00:59:30.560]   That one over there.
[00:59:30.560 --> 00:59:31.640]   That one over there is terrifying.
[00:59:31.640 --> 00:59:32.640]   This one?
[00:59:32.640 --> 00:59:33.640]   Yes.
[00:59:33.640 --> 00:59:35.960]   That is my actual nightmare.
[00:59:35.960 --> 00:59:39.960]   This is a facial portrait of John Oliver looking at the camera, laughing like a maniac, colorful
[00:59:39.960 --> 00:59:43.560]   background, lighting like in the Blair Witch Project.
[00:59:43.560 --> 00:59:46.560]   His teeth look like corn.
[00:59:46.560 --> 00:59:47.560]   They don't mention the corn teeth.
[00:59:47.560 --> 00:59:50.000]   They look like the yogurt in John Oliver had a baby.
[00:59:50.000 --> 00:59:51.000]   Yeah.
[00:59:51.000 --> 00:59:52.000]   So, I find this stuff...
[00:59:52.000 --> 00:59:53.600]   Oh, sorry.
[00:59:53.600 --> 00:59:54.600]   There's a classic sci-fi story.
[00:59:54.600 --> 00:59:55.880]   I want to say it's by C.M.
[00:59:55.880 --> 01:00:00.080]   Cornbluth, but I may be wrong, in which a machine falls through from the future until
[01:00:00.080 --> 01:00:05.240]   like 1940s or '50s and a guy finds it and he discovers it produces art.
[01:00:05.240 --> 01:00:08.560]   He gives it limited inputs and it produces beautiful work.
[01:00:08.560 --> 01:00:11.960]   And what he doesn't realize is he's written in a language as understanding.
[01:00:11.960 --> 01:00:12.960]   So, he sends off...
[01:00:12.960 --> 01:00:13.960]   It looks like Swedish.
[01:00:13.960 --> 01:00:17.280]   So, he sends off an instruction manual that came with it to get it translated.
[01:00:17.280 --> 01:00:19.320]   As he's using it, he starts to make these contracts.
[01:00:19.320 --> 01:00:20.720]   He's getting gallery shows.
[01:00:20.720 --> 01:00:22.600]   It's all being produced by the machine.
[01:00:22.600 --> 01:00:25.040]   And what he finds out, he gets the translation back and it's a form...
[01:00:25.040 --> 01:00:28.200]   The person's like this looks a little like Swedish, but he's been pressing the delete
[01:00:28.200 --> 01:00:29.280]   button all the time.
[01:00:29.280 --> 01:00:34.280]   So, as the last image comes out, it's basically empty and it draws a circle.
[01:00:34.280 --> 01:00:39.920]   And I was like, it was a perfect story from 80 years ago or 70 years ago about this idea
[01:00:39.920 --> 01:00:42.400]   of automated art from the future.
[01:00:42.400 --> 01:00:44.880]   But then you hit the wrong button and you're done.
[01:00:44.880 --> 01:00:47.640]   It's all over.
[01:00:47.640 --> 01:00:55.000]   And senior research scientist at Google actually is sounding an alarm.
[01:00:55.000 --> 01:01:00.160]   Negar Rustomset says, "Can't believe stable diffusion is out there for public use and
[01:01:00.160 --> 01:01:02.160]   that's considered okay."
[01:01:02.160 --> 01:01:03.160]   Wait, why?
[01:01:03.160 --> 01:01:04.320]   Google's been very careful.
[01:01:04.320 --> 01:01:10.800]   That says OpenAI to limit access to some of these engines.
[01:01:10.800 --> 01:01:17.160]   And apparently some scientists think you've released the Kraken.
[01:01:17.160 --> 01:01:28.280]   If you've released AI, I think what is required for the development of a lot of these things
[01:01:28.280 --> 01:01:31.120]   is kind of this tight loop of interaction.
[01:01:31.120 --> 01:01:34.600]   And I think stable diffusion, that's exactly what's happened.
[01:01:34.600 --> 01:01:38.240]   It's getting better and better and it's more and more intriguing.
[01:01:38.240 --> 01:01:42.320]   I just typed in bunny and I'm getting a lot of weird things.
[01:01:42.320 --> 01:01:45.480]   I would say that's a not safe for work search.
[01:01:45.480 --> 01:01:51.280]   Oh yeah, well apparently Portrait of Taylor Swift has Lola Bunny in Space Jam.
[01:01:51.280 --> 01:01:52.280]   Wow.
[01:01:52.280 --> 01:01:57.120]   Again, another sentence that I would not have expected to take back that alarm.
[01:01:57.120 --> 01:01:58.920]   There's a lot more for that.
[01:01:58.920 --> 01:02:00.400]   Oh yeah, there's a third arm.
[01:02:00.400 --> 01:02:01.400]   There's a third arm.
[01:02:01.400 --> 01:02:02.400]   Yeah.
[01:02:02.400 --> 01:02:03.400]   You need a third arm.
[01:02:03.400 --> 01:02:07.360]   I mean, we're talking about day one earlier, you know, but I've really called Bezos using
[01:02:07.360 --> 01:02:09.160]   that because they're not a startup anymore.
[01:02:09.160 --> 01:02:11.000]   They want to pretend to be one and they're multi-glue.
[01:02:11.000 --> 01:02:13.400]   It's day two or day three at Amazon.
[01:02:13.400 --> 01:02:16.120]   Yeah, Joe Biden, we are in bunny years.
[01:02:16.120 --> 01:02:18.000]   We are day one for a lot of AI stuff.
[01:02:18.000 --> 01:02:22.960]   It's amazing how much utility we can get an AI and it's still not very good by many
[01:02:22.960 --> 01:02:23.960]   measures.
[01:02:23.960 --> 01:02:27.280]   Like, you know, if you can, I mean, I think voice recognition has gotten pretty good,
[01:02:27.280 --> 01:02:29.040]   but it still has a long way to go.
[01:02:29.040 --> 01:02:34.400]   This is in the early days of being practical and it produces remarkable stuff.
[01:02:34.400 --> 01:02:36.440]   The bid journey stuff is can be.
[01:02:36.440 --> 01:02:37.440]   It's amazing.
[01:02:37.440 --> 01:02:41.600]   The sci-fi, sorry, horror author Dan, what do you call Chuck Wendy?
[01:02:41.600 --> 01:02:42.600]   Oh, he does.
[01:02:42.600 --> 01:02:43.600]   He's a weird, weird horror.
[01:02:43.600 --> 01:02:44.600]   Multi-genre.
[01:02:44.600 --> 01:02:46.600]   It's really interesting guy.
[01:02:46.600 --> 01:02:47.600]   Great.
[01:02:47.600 --> 01:02:48.600]   Yeah.
[01:02:48.600 --> 01:02:55.400]   And very funny guy and I think an interesting user of technology that he is constantly publishing
[01:02:55.400 --> 01:02:59.000]   sort of writing probably his mid-journey queries onto Instagram.
[01:02:59.000 --> 01:03:01.600]   You're like, it's so beautiful sometimes.
[01:03:01.600 --> 01:03:05.560]   It's hard for me to believe this could be any kind of amalgamation that it's not directly
[01:03:05.560 --> 01:03:06.560]   from a source.
[01:03:06.560 --> 01:03:07.560]   Here's some.
[01:03:07.560 --> 01:03:08.560]   Not that image.
[01:03:08.560 --> 01:03:10.720]   This is from the mid-journey community showcase.
[01:03:10.720 --> 01:03:15.840]   So these are, and this is another thing you, it's my mom always said, if all good bakers
[01:03:15.840 --> 01:03:21.600]   leave many cakes on the windowsill that, you know, if you're going to, you throw out
[01:03:21.600 --> 01:03:25.480]   the bad ones, but if you're going to show in a community showcase, it's a success.
[01:03:25.480 --> 01:03:29.560]   And I would say these are stunning these images of.
[01:03:29.560 --> 01:03:34.880]   The first like 15 these world through all looked like PlayStation five, like title characters
[01:03:34.880 --> 01:03:35.880]   basically.
[01:03:35.880 --> 01:03:36.880]   Yeah.
[01:03:36.880 --> 01:03:38.480]   Well, maybe those prompts, but I mean, look at this.
[01:03:38.480 --> 01:03:43.480]   This looks like breathtaking Baroque beauty, blonde beauty, full head oval baroque frame.
[01:03:43.480 --> 01:03:44.480]   Not baroque.
[01:03:44.480 --> 01:03:46.200]   Not baroque, but you know what?
[01:03:46.200 --> 01:03:48.040]   It's more than a white.
[01:03:48.040 --> 01:03:52.480]   It's something here's a crescent moon covered in vines and roses, art nouveau.
[01:03:52.480 --> 01:03:56.480]   I mean, if they can't get, if it ain't Baroque, don't fix it.
[01:03:56.480 --> 01:03:58.800]   I knew you were going to go there.
[01:03:58.800 --> 01:04:01.360]   I just had to.
[01:04:01.360 --> 01:04:05.320]   I think this, I mean, if you, if you follow communities of cartoonists and illustrators,
[01:04:05.320 --> 01:04:06.600]   they're more freaked out than authors are.
[01:04:06.600 --> 01:04:08.160]   I mean, we were past being freaked out by it.
[01:04:08.160 --> 01:04:10.080]   I don't know about animation.
[01:04:10.080 --> 01:04:11.720]   Yeah, I don't know.
[01:04:11.720 --> 01:04:14.920]   There's always something very tempting in this to me too, as somebody who has very little
[01:04:14.920 --> 01:04:20.200]   like visual arts skills whatsoever and has done some work with like, you know, stock photos
[01:04:20.200 --> 01:04:23.400]   making some book covers for stuff I'd self published.
[01:04:23.400 --> 01:04:27.320]   I mean, the idea that you could generate art that would be not just a stock photo that
[01:04:27.320 --> 01:04:32.600]   you've sort of manipulated or work with, like, I don't know, it's an attractive option because
[01:04:32.600 --> 01:04:37.600]   I there's no way in a million years that I would get the skills and develop them and
[01:04:37.600 --> 01:04:41.000]   have time to like sort of spend all the time it required to get to this point.
[01:04:41.000 --> 01:04:44.560]   Does it mean it's taking jobs away from like people I could be paying to do that?
[01:04:44.560 --> 01:04:45.560]   I don't know.
[01:04:45.560 --> 01:04:46.560]   That's an interesting question.
[01:04:46.560 --> 01:04:49.000]   It questions us from augmentation.
[01:04:49.000 --> 01:04:55.240]   I was saying I think where it gets complicated is I've seen a lot of major media publications
[01:04:55.240 --> 01:05:01.320]   as of late come under fire for using like lead images and stories that are generated
[01:05:01.320 --> 01:05:07.080]   by mid journey or Dolly or something like that when it's like, this is the sort of illustration
[01:05:07.080 --> 01:05:14.400]   that typically these publications are paying a couple dozen different freelance artists
[01:05:14.400 --> 01:05:17.440]   any given week or month to create.
[01:05:17.440 --> 01:05:21.800]   I think the Atlantic's Charlie Warzell had recently gotten some hot water because he had
[01:05:21.800 --> 01:05:23.840]   he has a small budget for his newsletter.
[01:05:23.840 --> 01:05:25.600]   So he's already just using stock images.
[01:05:25.600 --> 01:05:26.600]   That's his image.
[01:05:26.600 --> 01:05:27.600]   I'm sure.
[01:05:27.600 --> 01:05:32.080]   But he had used the most horrifying photo of Alex Jones in a newsstand generated I think
[01:05:32.080 --> 01:05:34.000]   by mid journey.
[01:05:34.000 --> 01:05:36.040]   And it was.
[01:05:36.040 --> 01:05:39.680]   I mean, people were quite upset about it.
[01:05:39.680 --> 01:05:43.880]   Were they upset because of the image or upset because it took the took bread out of the
[01:05:43.880 --> 01:05:45.440]   mouth of some illustrator somewhere?
[01:05:45.440 --> 01:05:51.080]   I mean, I think that people seeing it without context saw it and they were like, oh, the
[01:05:51.080 --> 01:05:55.800]   Atlantic a publication that has a lot of money and typically is going to be paying and working
[01:05:55.800 --> 01:06:01.840]   with illustrators is using an artificial intelligence powered system to create their
[01:06:01.840 --> 01:06:02.840]   lead artists.
[01:06:02.840 --> 01:06:07.040]   So if the Atlantic is doing it, what is stopping any other publication?
[01:06:07.040 --> 01:06:10.400]   I mean, I think this is a little bit of a different case because he had a follow up newsletter
[01:06:10.400 --> 01:06:11.400]   we explained.
[01:06:11.400 --> 01:06:15.560]   I just run my small newsletter and have a very limited budget.
[01:06:15.560 --> 01:06:20.880]   But I do think that it begs some question when you're talking about larger publications
[01:06:20.880 --> 01:06:21.880]   as it is value.
[01:06:21.880 --> 01:06:27.160]   Does it devalue the work to write because if you know, if you are a freelance illustrator
[01:06:27.160 --> 01:06:28.600]   and you're like, well, here's my rate.
[01:06:28.600 --> 01:06:32.200]   And they're like, well, we could just go to an AI and plug in a few words and get the
[01:06:32.200 --> 01:06:35.400]   by the way, this is generated by mid journey.
[01:06:35.400 --> 01:06:37.920]   And the caption says it's by mid journey.
[01:06:37.920 --> 01:06:42.000]   Alex Jones inside an American office under fluorescent lights.
[01:06:42.000 --> 01:06:46.000]   But also I should point out mid journey retains the right to these images.
[01:06:46.000 --> 01:06:50.000]   If they're turning NFT, it may get a cut.
[01:06:50.000 --> 01:06:51.000]   Yeah.
[01:06:51.000 --> 01:06:53.160]   NFT is the most popular thing.
[01:06:53.160 --> 01:06:57.840]   As a musician, I know told me the most horrifying thing, horrifying phrase I've ever heard about
[01:06:57.840 --> 01:06:59.720]   creativity a few years ago.
[01:06:59.720 --> 01:07:04.560]   She said, I'm competing against all music ever published now.
[01:07:04.560 --> 01:07:08.680]   And I think that might be what terrifies artists and rightly so as a writer, I'm slightly
[01:07:08.680 --> 01:07:12.200]   terrified because I don't.
[01:07:12.200 --> 01:07:13.200]   It's funny.
[01:07:13.200 --> 01:07:18.840]   It's not like it's easier to do art, but it's harder to get a corpus to produce a news article
[01:07:18.840 --> 01:07:24.600]   or analysis of a contemporary thing or even a description of something because you have
[01:07:24.600 --> 01:07:25.600]   to have a deep corpus.
[01:07:25.600 --> 01:07:28.960]   But with art, the corpus is all art ever created.
[01:07:28.960 --> 01:07:34.840]   And so every time a rabbit appears in anything, that could be fodder for an AI to use.
[01:07:34.840 --> 01:07:36.840]   So it's really the sheer amount, right?
[01:07:36.840 --> 01:07:37.840]   It's the training set.
[01:07:37.840 --> 01:07:41.560]   So I don't think someone's going to write an article about, I don't think an AI could
[01:07:41.560 --> 01:07:48.160]   write a feasibly credible article now or in the next few years about a person entering
[01:07:48.160 --> 01:07:52.600]   an AI generated piece of art in two contests of winning.
[01:07:52.600 --> 01:07:53.840]   I don't go ahead.
[01:07:53.840 --> 01:07:54.840]   I've seen some.
[01:07:54.840 --> 01:07:55.840]   There were those ones.
[01:07:55.840 --> 01:07:58.640]   I can't remember what the library was, but there was a thing recently where it would
[01:07:58.640 --> 01:08:01.240]   generate a story in the same sort of way.
[01:08:01.240 --> 01:08:03.320]   You'd be like, tell them write a story in this way.
[01:08:03.320 --> 01:08:07.760]   I know because of our pal Lex Friedman did it like, oh, write a description of this podcast.
[01:08:07.760 --> 01:08:11.320]   And it was surprisingly good again.
[01:08:11.320 --> 01:08:14.760]   I think the, but you get a full link for you made a feature that I don't know.
[01:08:14.760 --> 01:08:18.320]   I think not now maybe, but I wouldn't discount it for the future.
[01:08:18.320 --> 01:08:21.520]   And I think the biggest challenge for this is, is this argument?
[01:08:21.520 --> 01:08:26.320]   The only thing we have is this argument argue ultimately moot because it's like, well,
[01:08:26.320 --> 01:08:28.040]   the genie's out of the bottle.
[01:08:28.040 --> 01:08:30.960]   Like once it's out there, you can't stop it.
[01:08:30.960 --> 01:08:31.960]   It's done.
[01:08:31.960 --> 01:08:32.960]   Yeah, it comes down to right.
[01:08:32.960 --> 01:08:36.960]   It's like the source material, the training set to me becomes the issue.
[01:08:36.960 --> 01:08:41.320]   We were talking about that on this very podcast with Christina Warren, who couldn't talk about
[01:08:41.320 --> 01:08:47.560]   it because she works for Microsoft and the co-pilot product some weeks ago because that's
[01:08:47.560 --> 01:08:51.480]   the same thing is like who, if, if there are public, you know, this art is, you know,
[01:08:51.480 --> 01:08:52.480]   it's not even public, right?
[01:08:52.480 --> 01:08:55.800]   Some of it is art that's copyrighted, but it was.
[01:08:55.800 --> 01:08:59.360]   So Andy bio did a very interesting study.
[01:08:59.360 --> 01:09:00.360]   Yes.
[01:09:00.360 --> 01:09:02.480]   He, this is a waxy.org.
[01:09:02.480 --> 01:09:08.800]   He stable to, unlike Dolly too, open AI does not release the training set.
[01:09:08.800 --> 01:09:13.360]   So we can't see where that came from because stable diffusion is open source.
[01:09:13.360 --> 01:09:16.160]   You know where it came from, where the training set came from.
[01:09:16.160 --> 01:09:20.120]   He says we indexed the 12 million images in a sample, by the way, there are many, many
[01:09:20.120 --> 01:09:21.120]   more images.
[01:09:21.120 --> 01:09:24.400]   They, you know, they didn't want to go through 2.3 billion images.
[01:09:24.400 --> 01:09:29.760]   So they took a subset 12 million images and they indexed them by domain.
[01:09:29.760 --> 01:09:34.160]   Half the images were sourced from only a hundred domains.
[01:09:34.160 --> 01:09:37.040]   And the largest number of images came from Pinterest.
[01:09:37.040 --> 01:09:38.040]   Hmm.
[01:09:38.040 --> 01:09:44.080]   8.5% of the total data set scraped from Pinterest.
[01:09:44.080 --> 01:09:50.480]   So completely disregarding, you know, copyright or ownership.
[01:09:50.480 --> 01:09:52.320]   Fine art America.
[01:09:52.320 --> 01:09:55.560]   Second biggest domain, which sells art prints and posters.
[01:09:55.560 --> 01:10:01.520]   244,000 from Shopify, then Wix and Squarespace, Redbubble.
[01:10:01.520 --> 01:10:07.360]   So it scraped images all over the net.
[01:10:07.360 --> 01:10:12.640]   Number one artist of the top 25 artists on data set, only three are still living.
[01:10:12.640 --> 01:10:16.520]   Phil Koch, Erin Hanson and Steve Henderson, the most frequent artist.
[01:10:16.520 --> 01:10:17.680]   Who would you guess?
[01:10:17.680 --> 01:10:18.680]   Thomas Kincaid.
[01:10:18.680 --> 01:10:19.680]   Of course.
[01:10:19.680 --> 01:10:23.680]   That the, the, the, the blazer of light trademark.
[01:10:23.680 --> 01:10:26.000]   Well, and it depends to a certain degree.
[01:10:26.000 --> 01:10:29.760]   Like how does this, I, you know, again, I don't know enough about the technology of
[01:10:29.760 --> 01:10:32.760]   this one to know how that works in terms of ingesting that material.
[01:10:32.760 --> 01:10:38.080]   I mean, one argues that if you are an artist, you have gone and looked at a lot of art and
[01:10:38.080 --> 01:10:39.800]   you have done the same thing.
[01:10:39.800 --> 01:10:40.800]   Yeah.
[01:10:40.800 --> 01:10:41.800]   Good.
[01:10:41.800 --> 01:10:46.640]   So, but you know, as long as you're not storing those, if you're just sort of exposing the
[01:10:46.640 --> 01:10:50.640]   AI to that and it's deriving its own conclusions, I don't know.
[01:10:50.640 --> 01:10:54.440]   That seems legit to me, but maybe I'm not taking everything into consideration here.
[01:10:54.440 --> 01:10:55.440]   Yeah.
[01:10:55.440 --> 01:10:56.680]   That's the same thing with co-pilot.
[01:10:56.680 --> 01:10:57.920]   Same thing with JavaScript.
[01:10:57.920 --> 01:10:58.920]   Yeah.
[01:10:58.920 --> 01:11:03.680]   This is why we should legally ban every artist who has a photographic memory from looking
[01:11:03.680 --> 01:11:04.680]   at any art.
[01:11:04.680 --> 01:11:05.680]   Right.
[01:11:05.680 --> 01:11:06.680]   Right.
[01:11:06.680 --> 01:11:08.320]   You know, this is a story of Harrison Bergeron, right?
[01:11:08.320 --> 01:11:13.000]   The Kurt Vonnegut story about someone who is, you know, the society in which everyone has
[01:11:13.000 --> 01:11:17.720]   to be equal, but they define equal by disabling people who are too good instead of raising
[01:11:17.720 --> 01:11:19.960]   up everybody to the same level.
[01:11:19.960 --> 01:11:25.040]   And so Harrison Bergeron is the most beautiful, capable, intelligent, agile person.
[01:11:25.040 --> 01:11:29.360]   And so he has to walk around wearing like 300 pounds of weights and disfiguring masks
[01:11:29.360 --> 01:11:30.360]   and so forth.
[01:11:30.360 --> 01:11:34.520]   And that is, it does get you to those fields where like, well, the AI should be blind.
[01:11:34.520 --> 01:11:36.800]   It must be blinded because it can see too much beauty.
[01:11:36.800 --> 01:11:39.840]   For some reason, when you described that, I thought, you know, someone would make a good
[01:11:39.840 --> 01:11:44.400]   point and you just club them in the knee, you know, disable on the media.
[01:11:44.400 --> 01:11:46.360]   Like just trap off an arm.
[01:11:46.360 --> 01:11:48.040]   You were too smart in that meeting.
[01:11:48.040 --> 01:11:49.040]   Sorry.
[01:11:49.040 --> 01:11:50.840]   Got to take down a peg.
[01:11:50.840 --> 01:11:52.640]   Andy did an interesting thing.
[01:11:52.640 --> 01:11:59.040]   He gave a prompt, the same prompt to Dolly to and stable diffusion on the left.
[01:11:59.040 --> 01:12:02.880]   Realistic 3D rendering of Mickey Mouse working on a vintage computer doing his taxes.
[01:12:02.880 --> 01:12:07.800]   And you can see that the one on the left, the Dolly to image doesn't know who Mickey Mouse
[01:12:07.800 --> 01:12:08.800]   is.
[01:12:08.800 --> 01:12:09.800]   It's just a mouse.
[01:12:09.800 --> 01:12:10.800]   That's the original Mickey Mouse.
[01:12:10.800 --> 01:12:12.600]   Maybe that's what he used to like.
[01:12:12.600 --> 01:12:13.600]   White tank top.
[01:12:13.600 --> 01:12:14.600]   Yeah.
[01:12:14.600 --> 01:12:15.600]   Yeah.
[01:12:15.600 --> 01:12:16.600]   White Peter.
[01:12:16.600 --> 01:12:17.600]   That's what he had on before.
[01:12:17.600 --> 01:12:21.280]   You know, when he goes off for the day, you know, when he goes to relax, that's what
[01:12:21.280 --> 01:12:22.280]   he was.
[01:12:22.280 --> 01:12:23.600]   Mickey Mouse after I was.
[01:12:23.600 --> 01:12:25.640]   But yeah, but it's stable diffusion.
[01:12:25.640 --> 01:12:30.560]   It knows who Mickey Mouse is because it's trained on Mickey Mouse.
[01:12:30.560 --> 01:12:33.080]   And so that alone is going to get them into trouble, right?
[01:12:33.080 --> 01:12:34.080]   Yeah.
[01:12:34.080 --> 01:12:36.320]   I'm saying, hey, yeah, you got to pay up for that.
[01:12:36.320 --> 01:12:37.880]   That's our property.
[01:12:37.880 --> 01:12:38.880]   Yeah.
[01:12:38.880 --> 01:12:41.360]   So that does raise an issue.
[01:12:41.360 --> 01:12:47.120]   Then there's also the issue of AI generated pornography and a number of publications have
[01:12:47.120 --> 01:12:52.520]   started to point out there may be more of this appearing.
[01:12:52.520 --> 01:12:57.360]   Stable diffusion team built a predictor for adult material assigned every image.
[01:12:57.360 --> 01:13:00.000]   And this must have been a fun project.
[01:13:00.000 --> 01:13:06.960]   An NSFW probability score ranging from zero to one.
[01:13:06.960 --> 01:13:13.880]   So about 2.9% of the English language images were unsafe.
[01:13:13.880 --> 01:13:18.360]   But it's so so there's definitely a hardcore content.
[01:13:18.360 --> 01:13:21.520]   Yeah, we've wrapped around back to that first story.
[01:13:21.520 --> 01:13:24.440]   Here's the solution to all that age verification.
[01:13:24.440 --> 01:13:26.400]   Everything on the internet is produced by AI.
[01:13:26.400 --> 01:13:28.240]   It's tagged as to how unsafe it is.
[01:13:28.240 --> 01:13:29.840]   And you can pull the content.
[01:13:29.840 --> 01:13:30.840]   There you go.
[01:13:30.840 --> 01:13:31.840]   There you go.
[01:13:31.840 --> 01:13:32.840]   Perfect.
[01:13:32.840 --> 01:13:33.840]   Yeah.
[01:13:33.840 --> 01:13:35.720]   In the future, the internet doesn't have any idea about Mickey Mouse.
[01:13:35.720 --> 01:13:37.880]   And I think that's beautiful.
[01:13:37.880 --> 01:13:42.360]   I think the future is a fax machine which feeds into a shredder.
[01:13:42.360 --> 01:13:43.360]   That's my future.
[01:13:43.360 --> 01:13:44.360]   Oh, that's great.
[01:13:44.360 --> 01:13:47.800]   So it's AI's consuming content generated by AI's.
[01:13:47.800 --> 01:13:48.960]   We don't need to be involved.
[01:13:48.960 --> 01:13:50.840]   Yeah, we just take a break.
[01:13:50.840 --> 01:13:51.840]   Go to the beach.
[01:13:51.840 --> 01:13:52.840]   Relax.
[01:13:52.840 --> 01:13:53.840]   Right.
[01:13:53.840 --> 01:13:54.840]   Let them do their work.
[01:13:54.840 --> 01:13:57.560]   Good research from Andy bio.
[01:13:57.560 --> 01:14:02.720]   And I'm not sure what it concludes.
[01:14:02.720 --> 01:14:05.160]   There's a lot of copyright violation.
[01:14:05.160 --> 01:14:08.440]   And there's certainly a lot of cribbing from existing art.
[01:14:08.440 --> 01:14:13.960]   I still think though that this is interesting because we've seen it's been such a stop
[01:14:13.960 --> 01:14:16.080]   and start saying AI.
[01:14:16.080 --> 01:14:19.400]   There's been several AI winners already.
[01:14:19.400 --> 01:14:24.240]   We've seen how difficult it is to get cars to drive safely, to get voice assistance,
[01:14:24.240 --> 01:14:26.280]   to answer intelligently.
[01:14:26.280 --> 01:14:32.880]   And yet I think in this one area, AI has made a huge amount of progress very quickly.
[01:14:32.880 --> 01:14:35.040]   Or is that just my imagination?
[01:14:35.040 --> 01:14:36.680]   We fill in details visually better.
[01:14:36.680 --> 01:14:39.320]   We see things that aren't there.
[01:14:39.320 --> 01:14:43.160]   You know, if 10 words in row are misspelled, we're going to notice that or if the sentence
[01:14:43.160 --> 01:14:45.640]   doesn't make sense, it has to make logical sense.
[01:14:45.640 --> 01:14:48.600]   But we look at like the Mickey Mouse one on the right there.
[01:14:48.600 --> 01:14:52.480]   It's Mickey's hand has two fingers and it's distorted and the keyboard doesn't have all
[01:14:52.480 --> 01:14:53.480]   the keys.
[01:14:53.480 --> 01:14:54.480]   And that's OK.
[01:14:54.480 --> 01:14:55.480]   We don't notice that as much.
[01:14:55.480 --> 01:14:56.480]   Like he's doing his taxes.
[01:14:56.480 --> 01:14:57.480]   I get it.
[01:14:57.480 --> 01:15:00.960]   It's a Mickey on the left.
[01:15:00.960 --> 01:15:06.760]   The Mickey on the left is using a external clickety-clack keyboard on what looks like
[01:15:06.760 --> 01:15:07.760]   a typewriter.
[01:15:07.760 --> 01:15:08.760]   There's a pay.
[01:15:08.760 --> 01:15:09.760]   Make any sense.
[01:15:09.760 --> 01:15:10.760]   Yeah, it's great.
[01:15:10.760 --> 01:15:13.880]   Well, yeah, we're like the mouse is ripped and doing his taxes.
[01:15:13.880 --> 01:15:17.040]   And I think that's actually our Paris your keyboard because it looks like it's missing
[01:15:17.040 --> 01:15:19.040]   the question mark.
[01:15:19.040 --> 01:15:20.040]   That's true.
[01:15:20.040 --> 01:15:22.480]   You know, he's just a very serious mouse.
[01:15:22.480 --> 01:15:25.800]   I'd like to know if the rear end next to you is AI generator that appeared during the
[01:15:25.800 --> 01:15:26.800]   break.
[01:15:26.800 --> 01:15:27.800]   That's my question.
[01:15:27.800 --> 01:15:28.800]   Could be.
[01:15:28.800 --> 01:15:29.800]   Could be.
[01:15:29.800 --> 01:15:31.800]   I don't know.
[01:15:31.800 --> 01:15:33.560]   Suddenly I realized I noticed.
[01:15:33.560 --> 01:15:34.560]   Yeah.
[01:15:34.560 --> 01:15:35.560]   Oh, that's the sequined.
[01:15:35.560 --> 01:15:36.560]   I understand that's that piece of art.
[01:15:36.560 --> 01:15:38.560]   This is the sequined mannequin, but yes, yes, yes.
[01:15:38.560 --> 01:15:39.560]   I understand.
[01:15:39.560 --> 01:15:40.960]   I can't figure out which way to turn.
[01:15:40.960 --> 01:15:41.960]   Yes.
[01:15:41.960 --> 01:15:42.960]   I, you know, brought it.
[01:15:42.960 --> 01:15:43.960]   Wow.
[01:15:43.960 --> 01:15:46.160]   That's a straight out of stable diffusion.
[01:15:46.160 --> 01:15:47.160]   Wow.
[01:15:47.160 --> 01:15:49.160]   What's the two percent of the three percent?
[01:15:49.160 --> 01:15:52.600]   You're a little bit prompt to know or on that.
[01:15:52.600 --> 01:15:53.600]   Yeah.
[01:15:53.600 --> 01:15:55.800]   You made three-me-denting plus AI plus 3D printing.
[01:15:55.800 --> 01:15:59.320]   You'll type in sequined mannequin, but and then it'll come out of your device and you'll
[01:15:59.320 --> 01:16:00.320]   have, you know, there you go.
[01:16:00.320 --> 01:16:02.320]   And you'll be like, you know, you don't remember.
[01:16:02.320 --> 01:16:05.200]   But only has one butt cheek, but I get it.
[01:16:05.200 --> 01:16:10.600]   You remember with here over a nanotechnology of like Gregu, everything would turn into
[01:16:10.600 --> 01:16:11.600]   Gregu.
[01:16:11.600 --> 01:16:15.000]   Now I'm worried that we're going to be a watch in 3D printed stuff generated by AI.
[01:16:15.000 --> 01:16:19.480]   Just a little nasty, gregu-og everywhere that an AI has created.
[01:16:19.480 --> 01:16:20.480]   Oh my god.
[01:16:20.480 --> 01:16:21.480]   That's kind of true.
[01:16:21.480 --> 01:16:22.480]   It's kind of true around.
[01:16:22.480 --> 01:16:24.400]   It's like a mouse with six arms.
[01:16:24.400 --> 01:16:27.640]   But, you know, like, you know, like, I'm taking a lot of like robotics and AI stuff.
[01:16:27.640 --> 01:16:31.000]   And I don't know if this is, I sometimes feel like I'm way off base and sometimes I feel
[01:16:31.000 --> 01:16:33.720]   like I'm seeing it as a trend is augmentation, right?
[01:16:33.720 --> 01:16:37.520]   We have more people, despite the pandemic, more people are now employed in America than
[01:16:37.520 --> 01:16:38.880]   there were before the pandemic.
[01:16:38.880 --> 01:16:40.520]   We've recovered from that.
[01:16:40.520 --> 01:16:44.600]   Employment continues ever upward even as American productivity just to take, you know, us as
[01:16:44.600 --> 01:16:46.720]   an easy to find example.
[01:16:46.720 --> 01:16:50.640]   Productivity continues on an unprecedented pace continuously for all American workers.
[01:16:50.640 --> 01:16:51.960]   Blah, blah, blah, right?
[01:16:51.960 --> 01:16:53.840]   We have so much robotics now.
[01:16:53.840 --> 01:16:57.200]   We have AI's engaged in a lot of aspects of daily life.
[01:16:57.200 --> 01:17:00.640]   You know, I think Dan, I think you use this pair of sumay as well.
[01:17:00.640 --> 01:17:03.840]   AI based transcription for interviews and things like that.
[01:17:03.840 --> 01:17:06.040]   At least as a first pass, if not the verbatim one.
[01:17:06.040 --> 01:17:07.040]   I love Trent.
[01:17:07.040 --> 01:17:08.040]   You love who?
[01:17:08.040 --> 01:17:10.040]   Yeah, I'm using Trent.
[01:17:10.040 --> 01:17:11.040]   Trent.
[01:17:11.040 --> 01:17:15.640]   Trent, he's a great fellow that just listens to all of my interviews and kind of mashes
[01:17:15.640 --> 01:17:16.640]   some buttons.
[01:17:16.640 --> 01:17:18.880]   There's a trint with an eye.
[01:17:18.880 --> 01:17:20.320]   Trent, Trent with an eye.
[01:17:20.320 --> 01:17:21.320]   It's a really good transcript.
[01:17:21.320 --> 01:17:22.320]   Yes.
[01:17:22.320 --> 01:17:23.320]   I think I need Trent.
[01:17:23.320 --> 01:17:24.320]   I thought you loved Trent.
[01:17:24.320 --> 01:17:25.320]   I didn't know.
[01:17:25.320 --> 01:17:26.320]   Listen, Trent.
[01:17:26.320 --> 01:17:27.320]   I use Trent.
[01:17:27.320 --> 01:17:28.320]   He does.
[01:17:28.320 --> 01:17:29.320]   AI.
[01:17:29.320 --> 01:17:30.320]   But Otter is integrated with this.
[01:17:30.320 --> 01:17:31.320]   Otter is integrated with this.
[01:17:31.320 --> 01:17:33.920]   He's been integrated with Zoom and made a huge difference in my reporting.
[01:17:33.920 --> 01:17:38.560]   It would sometimes take hours out of the work I was doing to have a searchable thing.
[01:17:38.560 --> 01:17:41.000]   So this was substantial augmentation.
[01:17:41.000 --> 01:17:47.560]   My word rate or my hourly rate for articles I wrote essentially went out by paying a relatively
[01:17:47.560 --> 01:17:50.960]   low yearly fee to have this AI assistant.
[01:17:50.960 --> 01:17:54.760]   So augmentation, my question is, and I think a lot of artists are thinking about this now
[01:17:54.760 --> 01:17:58.600]   and people like web cartoonists and folks who do commercial art.
[01:17:58.600 --> 01:18:02.560]   The field has shrunk and it's a very complicated one already.
[01:18:02.560 --> 01:18:08.240]   So the question is, does this become an augmentation where they can use this as a tool, as a visualization
[01:18:08.240 --> 01:18:09.240]   tool?
[01:18:09.240 --> 01:18:12.840]   Is it something that helps them get to where they're going faster for commercial work?
[01:18:12.840 --> 01:18:19.160]   Or as a writer, do I wind up, there's another science fiction story which some writers like
[01:18:19.160 --> 01:18:23.840]   us are sitting there typing away and you have to control the AI for moving too fast ahead
[01:18:23.840 --> 01:18:28.040]   of you, but it's a partnership and it allows someone to produce more work that's more
[01:18:28.040 --> 01:18:32.520]   accurate and more quickly, maybe less creative or you're providing the creative component.
[01:18:32.520 --> 01:18:38.160]   So there is a future in which augmentation reduces less of the scutt work and produces
[01:18:38.160 --> 01:18:44.140]   a better impact, but again, we have so much, so many professions and fields have been
[01:18:44.140 --> 01:18:49.840]   robotized or have had even tiny robots or other kinds of automation added in the last
[01:18:49.840 --> 01:18:51.800]   50 years and yet employment, right?
[01:18:51.800 --> 01:18:54.360]   We're not employing 50% fewer people.
[01:18:54.360 --> 01:18:56.080]   We're at all time high employment.
[01:18:56.080 --> 01:19:00.840]   Yeah, in fact, there's too many jobs for too few people.
[01:19:00.840 --> 01:19:03.640]   They're robot hamburger flippers, right?
[01:19:03.640 --> 01:19:05.480]   I think you nailed it though.
[01:19:05.480 --> 01:19:12.240]   I mean, honestly, the best self-driving cars are not driving entirely on their own.
[01:19:12.240 --> 01:19:13.560]   They're not level four or five.
[01:19:13.560 --> 01:19:20.080]   It's level two where it's human augmentation and that's why it's a mistake to call it autopilot,
[01:19:20.080 --> 01:19:24.760]   but those kinds of self-driving vehicles are great.
[01:19:24.760 --> 01:19:26.400]   They're not self-driving.
[01:19:26.400 --> 01:19:27.400]   They're assist.
[01:19:27.400 --> 01:19:29.960]   They're computer assist and I think that works well.
[01:19:29.960 --> 01:19:34.720]   By the way, Trent, have you tried Otter, which is better, Paris?
[01:19:34.720 --> 01:19:35.720]   You like to?
[01:19:35.720 --> 01:19:38.680]   I have been a Trent fan for many years.
[01:19:38.680 --> 01:19:40.480]   I find that Otter for me, I don't know.
[01:19:40.480 --> 01:19:44.040]   I know a lot of people who swear by Otter.
[01:19:44.040 --> 01:19:50.240]   I found that it doesn't often transcribe with the same level of accuracy and most importantly
[01:19:50.240 --> 01:19:59.400]   for me, I think that Trent's, that's T-R-I-N-T, not my friend Trent, Trent's editing service
[01:19:59.400 --> 01:20:04.240]   is really perfect for the sort of things that I use transcription for.
[01:20:04.240 --> 01:20:11.120]   Once you load up your transcription, you kind of have a time-coded live transcription of
[01:20:11.120 --> 01:20:16.640]   it that scrolls with the audio and you can, whenever, let's say, if there's a word that
[01:20:16.640 --> 01:20:22.080]   it got wrong, you go into edit and it pauses and then once you're done editing, it restarts
[01:20:22.080 --> 01:20:24.120]   at the word right after.
[01:20:24.120 --> 01:20:30.040]   I think it's really good for the sort of workflow that I do, but I know a lot of people who
[01:20:30.040 --> 01:20:31.040]   are like Otter.
[01:20:31.040 --> 01:20:34.520]   I should point out there's another reason I discovered Otter was for live captioning
[01:20:34.520 --> 01:20:38.920]   and I should point out this is an incredible area in which it was almost impossible to
[01:20:38.920 --> 01:20:43.960]   get and now is widely available is you needed live people typing to do real-time transcription
[01:20:43.960 --> 01:20:45.360]   of conversations.
[01:20:45.360 --> 01:20:50.640]   Now, there are a billion conversations a day that can be transcribed or sort of can have
[01:20:50.640 --> 01:20:51.640]   live caption.
[01:20:51.640 --> 01:20:55.680]   It's an amazing job with Android 12 and 13.
[01:20:55.680 --> 01:20:56.680]   And Skype is incredible.
[01:20:56.680 --> 01:20:59.120]   Apple's adding it and iOS 6.
[01:20:59.120 --> 01:21:00.800]   Yeah, Apple's.
[01:21:00.800 --> 01:21:06.680]   So in that, that is a, I mean, so 99.999% of all phone conversations, video conferences
[01:21:06.680 --> 01:21:11.080]   and whatever, did not have live captions and now you have the potential and I suspect it
[01:21:11.080 --> 01:21:16.080]   is a more than 10% number where people enable that and that is, oh my God, the amount of
[01:21:16.080 --> 01:21:21.280]   cognitive energy that saves and the improvement in conversations.
[01:21:21.280 --> 01:21:27.000]   So not like I want to be like, AI only has positive impacts, but I'm like, my wife has
[01:21:27.000 --> 01:21:28.720]   hearing issues.
[01:21:28.720 --> 01:21:29.720]   She can't drive at night.
[01:21:29.720 --> 01:21:35.160]   Like I see always, I look at AI and improvements in all these kinds of automations and robotics
[01:21:35.160 --> 01:21:40.400]   as tools for people, you know, directly near me where it's when improved captions already
[01:21:40.400 --> 01:21:45.200]   improve her life and augmentation that would allow like night vision or other tools, it
[01:21:45.200 --> 01:21:48.640]   will have to be full automated driving could allow her to drive at night again.
[01:21:48.640 --> 01:21:49.640]   Definitely.
[01:21:49.640 --> 01:21:55.640]   So, uh, Dolly too, is announced, uh, a new feature called out painting, which is a kind
[01:21:55.640 --> 01:22:00.880]   of another way of taking a human generated product and then applying.
[01:22:00.880 --> 01:22:05.320]   So I'll show you the time lapse of Vermeer is the girl with the pearl earring.
[01:22:05.320 --> 01:22:12.400]   Uh, Dolly too is painting the room the girls in and it's, I mean, it's not obviously probably
[01:22:12.400 --> 01:22:14.760]   not the real room, but it's pretty credible.
[01:22:14.760 --> 01:22:20.680]   Oh, that's weird.
[01:22:20.680 --> 01:22:24.280]   Did you see the example of someone she was, uh, she's a fashion designer and technologist
[01:22:24.280 --> 01:22:29.800]   and she created a video by combining out painting, uh, and like two or three other tools to,
[01:22:29.800 --> 01:22:34.640]   to create seamless images and, um, it just shows her like walking down the street and
[01:22:34.640 --> 01:22:40.280]   her outfit is changing into AI generated alternatives as she walks and it's, I, I
[01:22:40.280 --> 01:22:41.280]   did see that video.
[01:22:41.280 --> 01:22:42.280]   It's phenomenal.
[01:22:42.280 --> 01:22:43.280]   Yeah.
[01:22:43.280 --> 01:22:45.800]   She's cobbled together a bunch of stuff.
[01:22:45.800 --> 01:22:48.880]   That Vermeer thing though, I mean, it really makes the girl with the pearl earring look
[01:22:48.880 --> 01:22:49.880]   like a bit of a slob.
[01:22:49.880 --> 01:22:51.880]   There's like, very tighty.
[01:22:51.880 --> 01:22:53.680]   What is going on there?
[01:22:53.680 --> 01:22:54.680]   Yeah.
[01:22:54.680 --> 01:22:56.880]   I was like, do you need that many lemons on your shelf?
[01:22:56.880 --> 01:22:58.880]   Like, no, many fridge.
[01:22:58.880 --> 01:22:59.880]   I don't know.
[01:22:59.880 --> 01:23:00.880]   Yeah.
[01:23:00.880 --> 01:23:01.880]   It's.
[01:23:01.880 --> 01:23:04.720]   The club with a clutter problem.
[01:23:04.720 --> 01:23:05.720]   Maybe baby.
[01:23:05.720 --> 01:23:07.000]   Maybe AI has a problem.
[01:23:07.000 --> 01:23:08.800]   Maybe AI is a border.
[01:23:08.800 --> 01:23:09.800]   Oh my God.
[01:23:09.800 --> 01:23:10.800]   Yeah.
[01:23:10.800 --> 01:23:11.800]   It's amazing.
[01:23:11.800 --> 01:23:12.800]   Yeah.
[01:23:12.800 --> 01:23:13.800]   It's a weird AI.
[01:23:13.800 --> 01:23:15.480]   T cups up on that top shelf there.
[01:23:15.480 --> 01:23:16.480]   What is, what is happening?
[01:23:16.480 --> 01:23:19.960]   We can perspective issues too with where those see your right.
[01:23:19.960 --> 01:23:24.480]   So now, so now I'm realizing, like man, if you, you got so many Amazon packages there
[01:23:24.480 --> 01:23:29.880]   in the bottom right, take their, like, like, no, I'm realizing that you were right.
[01:23:29.880 --> 01:23:35.240]   Glenn Fleischman that we are filling in the details that make it feel better than it really
[01:23:35.240 --> 01:23:36.240]   is.
[01:23:36.240 --> 01:23:38.640]   If you look closely, this is for me or by way of M.C.
[01:23:38.640 --> 01:23:39.640]   Escher.
[01:23:39.640 --> 01:23:40.640]   Yeah, it's kind of awful.
[01:23:40.640 --> 01:23:41.640]   Kind of cool though.
[01:23:41.640 --> 01:23:43.600]   It's kind of cool, kind of awful.
[01:23:43.600 --> 01:23:44.600]   That's, that's the, that's basically the.
[01:23:44.600 --> 01:23:46.080]   This is kind of an eye spy.
[01:23:46.080 --> 01:23:47.720]   And what's this thing hanging here?
[01:23:47.720 --> 01:23:48.720]   What is this?
[01:23:48.720 --> 01:23:49.720]   Is that a purchase?
[01:23:49.720 --> 01:23:50.720]   Like, it's a light.
[01:23:50.720 --> 01:23:52.720]   It's a like, it's a moon, like.
[01:23:52.720 --> 01:23:54.680]   It looks like a mouse hanging by its tail.
[01:23:54.680 --> 01:23:55.880]   And a mirror edited out.
[01:23:55.880 --> 01:23:56.880]   It was a goof.
[01:23:56.880 --> 01:23:57.880]   And that's why Permira.
[01:23:57.880 --> 01:23:58.880]   Yeah.
[01:23:58.880 --> 01:23:59.880]   Yeah.
[01:23:59.880 --> 01:24:01.880]   It's action to the goof section of IMDB.
[01:24:01.880 --> 01:24:04.360]   And I'll see you in the frame.
[01:24:04.360 --> 01:24:07.120]   What if AI is like, what if Permira painted this?
[01:24:07.120 --> 01:24:09.160]   Would we say, oh, what a beautiful painting.
[01:24:09.160 --> 01:24:10.160]   This is art.
[01:24:10.160 --> 01:24:11.160]   I don't know.
[01:24:11.160 --> 01:24:12.160]   I don't know.
[01:24:12.160 --> 01:24:14.280]   I feel like it was long enough ago.
[01:24:14.280 --> 01:24:16.320]   Like, this is kind of impressive.
[01:24:16.320 --> 01:24:18.760]   Cause it just like, man, you survived.
[01:24:18.760 --> 01:24:19.760]   You didn't get enough.
[01:24:19.760 --> 01:24:22.560]   You didn't die of illness in the time of 16.
[01:24:22.560 --> 01:24:24.760]   This took 16 years to paint.
[01:24:24.760 --> 01:24:26.160]   I hope you got a good price.
[01:24:26.160 --> 01:24:27.160]   Yeah.
[01:24:27.160 --> 01:24:28.160]   Yeah.
[01:24:28.160 --> 01:24:29.160]   It's, it's.
[01:24:29.160 --> 01:24:30.160]   Oh, man, that's great.
[01:24:30.160 --> 01:24:32.320]   Do you think AI might be slightly psychotic?
[01:24:32.320 --> 01:24:34.960]   Like, it's not conscious.
[01:24:34.960 --> 01:24:35.960]   So who knows?
[01:24:35.960 --> 01:24:37.720]   I hesitate to give a clinical diagnosis.
[01:24:37.720 --> 01:24:38.720]   Yeah.
[01:24:38.720 --> 01:24:39.720]   I don't know.
[01:24:39.720 --> 01:24:40.720]   We're not armchair.
[01:24:40.720 --> 01:24:43.320]   I don't want to armchair a psychologist AI here.
[01:24:43.320 --> 01:24:46.080]   All Georgia now and see if she'll.
[01:24:46.080 --> 01:24:47.640]   Here's an interesting application.
[01:24:47.640 --> 01:24:49.080]   Oh, AI therapy.
[01:24:49.080 --> 01:24:50.440]   That's going to be a growth industry.
[01:24:50.440 --> 01:24:51.440]   AI.
[01:24:51.440 --> 01:24:52.600]   I think we started with that, Glenn.
[01:24:52.600 --> 01:24:54.160]   That was where Eliza came from.
[01:24:54.160 --> 01:24:55.160]   Yeah.
[01:24:55.160 --> 01:24:56.160]   That was literally Eliza.
[01:24:56.160 --> 01:24:57.160]   We're full circle.
[01:24:57.160 --> 01:24:59.880]   Oh my, no, no, but therapists for the AI.
[01:24:59.880 --> 01:25:02.280]   Not therapists for all the AI needs.
[01:25:02.280 --> 01:25:03.280]   Yeah.
[01:25:03.280 --> 01:25:07.200]   The AI is going to be scrolling through TikTok and it's going to tell the AI you have ADHD
[01:25:07.200 --> 01:25:09.720]   and have got to get stimulants right now.
[01:25:09.720 --> 01:25:11.600]   Georgia Dow, therapist to AI.
[01:25:11.600 --> 01:25:12.600]   That's good.
[01:25:12.600 --> 01:25:16.520]   Here is, I think, a very good use of AI.
[01:25:16.520 --> 01:25:17.520]   Storybooks.ai.
[01:25:17.520 --> 01:25:25.360]   They want to take all of the Gutenberg project, public domain, text and illustrate it with
[01:25:25.360 --> 01:25:26.360]   AI.
[01:25:26.360 --> 01:25:27.680]   It's a fascinating idea.
[01:25:27.680 --> 01:25:29.760]   Is that an interesting idea?
[01:25:29.760 --> 01:25:32.080]   It's problematic and entirely different.
[01:25:32.080 --> 01:25:36.440]   Although it's weird is the original drawings of many of these works have exist.
[01:25:36.440 --> 01:25:39.600]   Like many of these works were illustrated and they're not in copyright either.
[01:25:39.600 --> 01:25:42.880]   So, but it's an interesting exploration.
[01:25:42.880 --> 01:25:44.480]   Is that what Sherlock Holmes looks like?
[01:25:44.480 --> 01:25:45.480]   No, I don't like it.
[01:25:45.480 --> 01:25:46.480]   I don't like it.
[01:25:46.480 --> 01:25:47.480]   I like it a long way.
[01:25:47.480 --> 01:25:48.480]   What is going on with his jaw?
[01:25:48.480 --> 01:25:50.280]   No, son.
[01:25:50.280 --> 01:25:52.960]   You've become AI critics, you guys.
[01:25:52.960 --> 01:25:53.960]   Let's see.
[01:25:53.960 --> 01:25:55.640]   Well, Cenia, no, they are going to be therapy after we produce it.
[01:25:55.640 --> 01:25:58.320]   If they're producing art, they better be okay with getting rid of the internet.
[01:25:58.320 --> 01:26:00.620]   Philip K. Dix, do Androids Dream of Electric Sheep.
[01:26:00.620 --> 01:26:02.960]   That would be a natural for AI, right?
[01:26:02.960 --> 01:26:04.880]   There's that's a look again.
[01:26:04.880 --> 01:26:05.880]   I like the book.
[01:26:05.880 --> 01:26:07.400]   A very thick man.
[01:26:07.400 --> 01:26:08.400]   Rick Teggart.
[01:26:08.400 --> 01:26:10.400]   What is it called?
[01:26:10.400 --> 01:26:11.400]   What is it called?
[01:26:11.400 --> 01:26:14.400]   Galactic Poshard Healer or something?
[01:26:14.400 --> 01:26:15.840]   It's one of his strangest works.
[01:26:15.840 --> 01:26:16.840]   Much better though.
[01:26:16.840 --> 01:26:17.840]   Philip K. Dix.
[01:26:17.840 --> 01:26:24.240]   I do think we have to try and analyze why AI is obsessed with making ripped arms.
[01:26:24.240 --> 01:26:28.080]   You know, like all of the every arm is Jack.
[01:26:28.080 --> 01:26:30.800]   Because it's been trained on Jeff Bezos images.
[01:26:30.800 --> 01:26:31.800]   That's why.
[01:26:31.800 --> 01:26:32.800]   That's true.
[01:26:32.800 --> 01:26:33.800]   Then the next one is very long.
[01:26:33.800 --> 01:26:34.800]   A little too much.
[01:26:34.800 --> 01:26:36.680]   It must, it must pump iron.
[01:26:36.680 --> 01:26:38.680]   A little too much.
[01:26:38.680 --> 01:26:39.680]   Oh, they're amazing.
[01:26:39.680 --> 01:26:40.680]   I want to know a fun fact.
[01:26:40.680 --> 01:26:41.960]   I've never been able to get in copy.
[01:26:41.960 --> 01:26:47.880]   But I've heard from sources since Jeff Bezos got on his fitness kick, you know, obviously
[01:26:47.880 --> 01:26:49.400]   to kind of go to space.
[01:26:49.400 --> 01:26:54.040]   He started taking, he never would use the am the elevators at Amazon's HQ.
[01:26:54.040 --> 01:26:58.080]   When he was CEO, he would always take the stairs up and down.
[01:26:58.080 --> 01:27:02.400]   And that meant, of course, due to the power like politics, like internal politics dynamics
[01:27:02.400 --> 01:27:07.080]   of working amongst like next to a CEO who's trying to get jacked all of his underlings,
[01:27:07.080 --> 01:27:09.040]   but also be like, Oh, I got to take the stairs.
[01:27:09.040 --> 01:27:10.040]   Jeff is up there.
[01:27:10.040 --> 01:27:11.600]   I can't be seen in the elevator.
[01:27:11.600 --> 01:27:13.240]   I think that's just very funny sometimes.
[01:27:13.240 --> 01:27:14.240]   You worked at Amazon.
[01:27:14.240 --> 01:27:15.240]   Did you go in there?
[01:27:15.240 --> 01:27:16.240]   Do you know that?
[01:27:16.240 --> 01:27:17.240]   Do you know the story?
[01:27:17.240 --> 01:27:19.840]   I think, I don't know if you're at the time.
[01:27:19.840 --> 01:27:23.360]   I worked, I worked at Amazon when we were on the second and fourth floor of the.
[01:27:23.360 --> 01:27:24.360]   Oh, so you don't know.
[01:27:24.360 --> 01:27:25.360]   Columbia.
[01:27:25.360 --> 01:27:27.200]   Yeah, I think he walked to the, I think he walked to the fourth floor.
[01:27:27.200 --> 01:27:28.680]   I think his office is on the second floor.
[01:27:28.680 --> 01:27:29.680]   Wall Street.
[01:27:29.680 --> 01:27:31.760]   He had a rack of identical shirts behind him though.
[01:27:31.760 --> 01:27:35.400]   That's a line of Wall Street Journal article this weekend.
[01:27:35.400 --> 01:27:36.520]   Yachts and watches.
[01:27:36.520 --> 01:27:40.840]   The real CEO flex is washboard abs.
[01:27:40.840 --> 01:27:42.680]   Amazing.
[01:27:42.680 --> 01:27:45.040]   And there's, there's the exit.
[01:27:45.040 --> 01:27:46.040]   Oh, no.
[01:27:46.040 --> 01:27:47.040]   Elon Musk.
[01:27:47.040 --> 01:27:48.040]   There's the counter example.
[01:27:48.040 --> 01:27:52.400]   Elon Musk getting, getting hosed down by Aria Manuel.
[01:27:52.400 --> 01:27:56.440]   I never felt better about Elon Musk and seeing that makes me feel very sympathetic.
[01:27:56.440 --> 01:28:01.040]   Listen, we all want to believe in our hearts that we would be the Jeff Bezos physique if
[01:28:01.040 --> 01:28:02.040]   we were billionaires.
[01:28:02.040 --> 01:28:04.560]   But really a lot of us would be the Elon Musk physique.
[01:28:04.560 --> 01:28:06.360]   If you had the money.
[01:28:06.360 --> 01:28:12.600]   Here's, here's from a selling sunset, the Jeff Jason Oppenheim, the, the buff realtor.
[01:28:12.600 --> 01:28:13.600]   Chef Bezos.
[01:28:13.600 --> 01:28:15.520]   Here's Jeff Bezos pulling a Putin.
[01:28:15.520 --> 01:28:16.520]   On a horse.
[01:28:16.520 --> 01:28:17.520]   Yes.
[01:28:17.520 --> 01:28:20.360]   He was very bad when you were Putin looking.
[01:28:20.360 --> 01:28:21.360]   Yeah.
[01:28:21.360 --> 01:28:22.360]   Here is the video game.
[01:28:22.360 --> 01:28:24.200]   Oh, no, that is too tight.
[01:28:24.200 --> 01:28:25.200]   Stress.
[01:28:25.200 --> 01:28:27.200]   Oh, I don't know.
[01:28:27.200 --> 01:28:28.200]   I can't see.
[01:28:28.200 --> 01:28:29.200]   You can see Nip.
[01:28:29.200 --> 01:28:30.200]   That's too tight.
[01:28:30.200 --> 01:28:33.000]   It's the Batman costume.
[01:28:33.000 --> 01:28:34.000]   It's the tiny outfit.
[01:28:34.000 --> 01:28:36.960]   I think the best part of that is his photo courtesy of stress.
[01:28:36.960 --> 01:28:37.960]   All these layers.
[01:28:37.960 --> 01:28:40.760]   Like, you give your, your, your, your back picture.
[01:28:40.760 --> 01:28:41.760]   Yeah.
[01:28:41.760 --> 01:28:42.760]   Re.
[01:28:42.760 --> 01:28:44.680]   The Wall Street Journal reached out and was like, Hey, can we get a photo?
[01:28:44.680 --> 01:28:45.680]   We're doing this.
[01:28:45.680 --> 01:28:46.680]   I walked forward out.
[01:28:46.680 --> 01:28:47.680]   Yeah.
[01:28:47.680 --> 01:28:48.680]   Where he shows Nip.
[01:28:48.680 --> 01:28:49.680]   Yeah.
[01:28:49.680 --> 01:28:51.560]   And you see your apex, sir.
[01:28:51.560 --> 01:28:57.220]   Mr. Zellnick, who favors twice daily workouts and says he exercises up to 12 times a week, is
[01:28:57.220 --> 01:29:02.260]   not dad bodying it and neither are friends like Mr. Emmanuel, chief executive media company
[01:29:02.260 --> 01:29:04.260]   endeavor, the journal says, Oh my God.
[01:29:04.260 --> 01:29:09.180]   On a recent visit to Mr. Emmanuel's office in Beverly Hills, Mr. Zellnick said the executive
[01:29:09.180 --> 01:29:12.940]   took phone calls and wrote emails while walking at a treadmill desk.
[01:29:12.940 --> 01:29:17.580]   Mr. Emmanuel, who did not walk during his actual meeting with Zellnick did not comment.
[01:29:17.580 --> 01:29:19.300]   Oh, treadmill desk.
[01:29:19.300 --> 01:29:20.300]   Nothing.
[01:29:20.300 --> 01:29:22.660]   You need to have like a rowing receipt desk.
[01:29:22.660 --> 01:29:23.660]   Yeah.
[01:29:23.660 --> 01:29:24.660]   All right.
[01:29:24.660 --> 01:29:25.660]   All right.
[01:29:25.660 --> 01:29:26.660]   I have to say, I'm a lot closer.
[01:29:26.660 --> 01:29:29.860]   I'm a little closer to desk on the Elon to Ari scale.
[01:29:29.860 --> 01:29:35.780]   I'm a little closer to the Elon side of that scale, but that's the new thing.
[01:29:35.780 --> 01:29:38.860]   If you got the money, washboard abs.
[01:29:38.860 --> 01:29:39.860]   It's a little cheaper.
[01:29:39.860 --> 01:29:40.860]   I'm a lot harder.
[01:29:40.860 --> 01:29:41.860]   I'm on my Amazon.
[01:29:41.860 --> 01:29:42.860]   I'm flat right on.
[01:29:42.860 --> 01:29:47.700]   It costs a lot of money to look as good as people who have to do manual labor.
[01:29:47.700 --> 01:29:52.340]   I think probably Paris, you're glad you didn't get assigned this story.
[01:29:52.340 --> 01:29:53.340]   Yeah.
[01:29:53.340 --> 01:29:58.060]   Yeah, as an Amazon story, you didn't really want to write.
[01:29:58.060 --> 01:29:59.060]   Not really.
[01:29:59.060 --> 01:30:04.100]   I don't need to spend any more time looking at the physiques of tech executives.
[01:30:04.100 --> 01:30:05.860]   I think it's a little weird.
[01:30:05.860 --> 01:30:06.860]   Oh, great.
[01:30:06.860 --> 01:30:09.340]   Let's take a little break.
[01:30:09.340 --> 01:30:10.340]   Fun panel today.
[01:30:10.340 --> 01:30:16.940]   Paris Martino from the information Glenn Fleischman from, well, Glenn F dot Glenn dot
[01:30:16.940 --> 01:30:17.940]   fun.
[01:30:17.940 --> 01:30:18.940]   I'm sorry.
[01:30:18.940 --> 01:30:19.940]   He's Glenn F on Twitter.
[01:30:19.940 --> 01:30:21.500]   You're going to get it by the fourth ad break.
[01:30:21.500 --> 01:30:27.100]   You know, if I could just get this F key work in here, I'd let the F.
[01:30:27.100 --> 01:30:28.100]   Why did it?
[01:30:28.100 --> 01:30:29.940]   Why the F key?
[01:30:29.940 --> 01:30:32.700]   Like, did I, was I pounding hard on?
[01:30:32.700 --> 01:30:34.700]   You've got hard on the literature.
[01:30:34.700 --> 01:30:35.700]   You've got hard on the literature.
[01:30:35.700 --> 01:30:36.700]   Stop there.
[01:30:36.700 --> 01:30:37.700]   What?
[01:30:37.700 --> 01:30:38.700]   You got 10 more F keys right at the top.
[01:30:38.700 --> 01:30:39.700]   Oh, fun thing.
[01:30:39.700 --> 01:30:40.700]   Yeah.
[01:30:40.700 --> 01:30:43.620]   Just apply a sign and F key to the F key.
[01:30:43.620 --> 01:30:44.620]   What happened here?
[01:30:44.620 --> 01:30:48.700]   Like a computer just to macro time just died.
[01:30:48.700 --> 01:30:55.640]   So that is, of course, Dan, more in the author of the Galactic Cold War saga.
[01:30:55.640 --> 01:31:01.620]   If we're all really nice to him, maybe he will write another one soon.
[01:31:01.620 --> 01:31:04.620]   The Cambrian explosion was not one of your books.
[01:31:04.620 --> 01:31:06.660]   No, no, no, no, no, no.
[01:31:06.660 --> 01:31:07.660]   Not yet.
[01:31:07.660 --> 01:31:11.540]   Oh, Tony and Gambit is not the Cambrian explosion.
[01:31:11.540 --> 01:31:14.760]   Our show today brought to you by ClickUp.
[01:31:14.760 --> 01:31:19.700]   Imagine in your five day work week having one of those days off, right?
[01:31:19.700 --> 01:31:21.600]   One extra day a week.
[01:31:21.600 --> 01:31:22.720]   What would you do with the time?
[01:31:22.720 --> 01:31:24.880]   I'd be watching more reality TV.
[01:31:24.880 --> 01:31:27.360]   Maybe you'd cook healthy meals.
[01:31:27.360 --> 01:31:28.520]   Maybe you'd work out more.
[01:31:28.520 --> 01:31:34.280]   Get that buff CEO bot or maybe like Dan, you'd write volume four of the Galactic Cold
[01:31:34.280 --> 01:31:35.960]   War, an extra day a week.
[01:31:35.960 --> 01:31:42.720]   Well, you can with ClickUp, the productivity platform that's so good, it'll save you a
[01:31:42.720 --> 01:31:46.040]   day a week on work guaranteed.
[01:31:46.040 --> 01:31:49.480]   ClickUp began with the premise that productivity was broken.
[01:31:49.480 --> 01:31:52.640]   There are too many tools to keep track of, too many things.
[01:31:52.640 --> 01:31:55.800]   And you siloed into completely separate ecosystems.
[01:31:55.800 --> 01:32:00.200]   There had to be a better way, a more productive way to get through the daily hustle.
[01:32:00.200 --> 01:32:01.800]   ClickUp does it.
[01:32:01.800 --> 01:32:07.560]   One tool to house all the tools you use, all your tasks, projects, all your docs and
[01:32:07.560 --> 01:32:13.880]   goals and spreadsheets and more, and it's a great idea for any size team.
[01:32:13.880 --> 01:32:18.440]   Even if it's a team of one, if it's just you or for a thousand plus people packed with
[01:32:18.440 --> 01:32:23.400]   features and customization options, no other productivity tool has out of the box, you'll
[01:32:23.400 --> 01:32:28.240]   be productive immediately, but you can also customize it to work the way you work best,
[01:32:28.240 --> 01:32:34.440]   whether you're in project management or engineering or sales or marketing or HR, everybody gets
[01:32:34.440 --> 01:32:36.760]   an easy to use solution.
[01:32:36.760 --> 01:32:40.000]   It creates a more efficient work environment.
[01:32:40.000 --> 01:32:41.000]   It's ClickUp.
[01:32:41.000 --> 01:32:47.040]   Join more than 800,000, 800,000 highly productive teams using ClickUp today.
[01:32:47.040 --> 01:32:54.920]   Our offer code, TWIT, will get you 15% off ClickUp's massive unlimited plan for a year.
[01:32:54.920 --> 01:32:58.960]   That means you can start reclaiming your time for under $5 a month.
[01:32:58.960 --> 01:33:01.160]   Sign up today at ClickUp.com.
[01:33:01.160 --> 01:33:04.720]   Use the code TWIT, but don't delay.
[01:33:04.720 --> 01:33:07.480]   This offer doesn't last forever.
[01:33:07.480 --> 01:33:09.440]   It's limited time.
[01:33:09.440 --> 01:33:11.600]   Thank you, ClickUp for support and TWIT.
[01:33:11.600 --> 01:33:17.120]   ClickUp.com and again, the offer code is TWIT.
[01:33:17.120 --> 01:33:19.720]   Thank you, ClickUp.
[01:33:19.720 --> 01:33:23.920]   Well, Wednesday is a big day.
[01:33:23.920 --> 01:33:26.080]   Apple is a good...
[01:33:26.080 --> 01:33:30.240]   I think there should be a new word for the Apple events.
[01:33:30.240 --> 01:33:35.880]   It's a product launch, but it's also an infomercial.
[01:33:35.880 --> 01:33:37.240]   Could we call it a launch-emerscial?
[01:33:37.240 --> 01:33:38.240]   I don't know.
[01:33:38.240 --> 01:33:40.720]   Did any product launch not an infomercial, Leo?
[01:33:40.720 --> 01:33:41.720]   No.
[01:33:41.720 --> 01:33:42.920]   Well, actually, all of these are, aren't they?
[01:33:42.920 --> 01:33:47.320]   In fact, I'm kind of thinking, should we really be giving these the coverage that we do,
[01:33:47.320 --> 01:33:48.320]   but people want to know.
[01:33:48.320 --> 01:33:49.320]   People want to see it.
[01:33:49.320 --> 01:33:51.400]   Could you call them eye candy?
[01:33:51.400 --> 01:33:52.400]   Yes.
[01:33:52.400 --> 01:33:53.400]   Oh, very good.
[01:33:53.400 --> 01:33:54.400]   I can't.
[01:33:54.400 --> 01:33:58.640]   I think we should call them should be an email.
[01:33:58.640 --> 01:34:01.320]   See, that's a reporter talking.
[01:34:01.320 --> 01:34:03.960]   Can you just send me the talking points, please?
[01:34:03.960 --> 01:34:08.760]   I will say, one of the happiest moments of my life is when I changed in the tech reporter
[01:34:08.760 --> 01:34:15.040]   world from having to sit at like two, three different computers paying attention during
[01:34:15.040 --> 01:34:19.600]   all the Apple releases to not caring about it and just witnessing it as a consumer.
[01:34:19.600 --> 01:34:20.600]   Did you ever have to do that?
[01:34:20.600 --> 01:34:21.600]   Do you ever have to do that?
[01:34:21.600 --> 01:34:23.360]   To sit at the computer and transcribe the...
[01:34:23.360 --> 01:34:24.360]   Oh, yeah.
[01:34:24.360 --> 01:34:29.360]   I've logged a bunch of events, financial calls, which are always the best because everybody
[01:34:29.360 --> 01:34:30.360]   loves my life.
[01:34:30.360 --> 01:34:31.360]   Oh, yeah.
[01:34:31.360 --> 01:34:32.360]   Now I'm here on quarterly earnings.
[01:34:32.360 --> 01:34:33.360]   Oh, my God.
[01:34:33.360 --> 01:34:34.360]   Yep.
[01:34:34.360 --> 01:34:35.360]   Done that a lot.
[01:34:35.360 --> 01:34:36.720]   This is from one fire to another.
[01:34:36.720 --> 01:34:37.720]   Him.
[01:34:37.720 --> 01:34:38.720]   This is Tim.
[01:34:38.720 --> 01:34:39.720]   This is Tim.
[01:34:39.720 --> 01:34:40.720]   Good morning.
[01:34:40.720 --> 01:34:41.720]   Good morning.
[01:34:41.720 --> 01:34:45.720]   Will you be going, Dan, to the Apple campus this week?
[01:34:45.720 --> 01:34:49.360]   No, with a six-week-old, I will not be going to the Apple campus.
[01:34:49.360 --> 01:34:50.360]   Jason, we'll watch.
[01:34:50.360 --> 01:34:51.360]   Jason, we'll watch.
[01:34:51.360 --> 01:34:52.360]   He'll be there.
[01:34:52.360 --> 01:34:53.360]   He'll be there.
[01:34:53.360 --> 01:34:57.400]   Last time they were just there via video, right?
[01:34:57.400 --> 01:35:01.160]   I was there in June, which was an interesting experience.
[01:35:01.160 --> 01:35:05.560]   I know Jason will be there and Leo, your pal and mine, my sergeant, I believe will be
[01:35:05.560 --> 01:35:06.560]   there as well.
[01:35:06.560 --> 01:35:07.560]   Yep.
[01:35:07.560 --> 01:35:08.560]   So that's very exciting.
[01:35:08.560 --> 01:35:14.000]   Leaving me in the lurch all alone, sitting here, snarking during the infomercial.
[01:35:14.000 --> 01:35:19.400]   But that's the job I chose.
[01:35:19.400 --> 01:35:20.400]   It's interesting.
[01:35:20.400 --> 01:35:22.120]   The invite said Steve Jobs Theatre.
[01:35:22.120 --> 01:35:26.320]   So the event you went to in June was outside with a big screen.
[01:35:26.320 --> 01:35:30.960]   Well, yeah, you could get it outside or it was partially in the, the, the, the, cafe
[01:35:30.960 --> 01:35:32.200]   called the ring, right?
[01:35:32.200 --> 01:35:33.680]   Yeah, the cafe max in the ring.
[01:35:33.680 --> 01:35:34.680]   Yeah.
[01:35:34.680 --> 01:35:37.680]   So we were in just indoors for that, but it was like, but air.
[01:35:37.680 --> 01:35:42.200]   But yeah, the Steve Jobs Theatre definitely suggests a more traditional presentation than
[01:35:42.200 --> 01:35:44.000]   what we saw in June.
[01:35:44.000 --> 01:35:49.080]   And yeah, calling it the rings makes it sound like Tim Cook is going to fight someone.
[01:35:49.080 --> 01:35:54.360]   It's like the octagon, Tim Cook versus Jeff Bezos.
[01:35:54.360 --> 01:35:55.360]   Tim Cook.
[01:35:55.360 --> 01:35:56.360]   You got some washboard abs.
[01:35:56.360 --> 01:35:57.360]   Okay.
[01:35:57.360 --> 01:36:04.880]   So we've never seen Tim's, but I have a feeling he probably does have a six pack under there.
[01:36:04.880 --> 01:36:05.880]   Every story about Tim.
[01:36:05.880 --> 01:36:10.320]   Well, this reminds me, this does remind me of the time I got a call from reporter asking
[01:36:10.320 --> 01:36:12.360]   about Jeff Bezos as private parts.
[01:36:12.360 --> 01:36:13.920]   And I'm glad that story never ran.
[01:36:13.920 --> 01:36:15.360]   Oh, I remember that.
[01:36:15.360 --> 01:36:16.360]   Was it National?
[01:36:16.360 --> 01:36:17.560]   Thank you, Mr. Packer.
[01:36:17.560 --> 01:36:18.560]   Yeah.
[01:36:18.560 --> 01:36:19.560]   Thank you, Mr. Packer.
[01:36:19.560 --> 01:36:22.280]   He's aptly named the head of the National Inquirer.
[01:36:22.280 --> 01:36:23.280]   I did do that interview.
[01:36:23.280 --> 01:36:27.120]   And then I was like, maybe I don't need to be talking about Jeff Bezos as private.
[01:36:27.120 --> 01:36:28.120]   Not from personal.
[01:36:28.120 --> 01:36:29.120]   You know what?
[01:36:29.120 --> 01:36:30.120]   Jeff called their bluff.
[01:36:30.120 --> 01:36:31.120]   He said, go ahead.
[01:36:31.120 --> 01:36:36.200]   I will say, listen, that was the one of the best PR moves I have seen in a long time,
[01:36:36.200 --> 01:36:41.600]   like is taking what should have been a moment of weakness and somehow spinning it into no,
[01:36:41.600 --> 01:36:46.320]   I'm going to drop all the receipts and then also make a couple of like.
[01:36:46.320 --> 01:36:54.080]   I think this was of course going back to when the National Inquirer came claimed.
[01:36:54.080 --> 01:36:58.480]   And I think they got it from his girlfriend's brother, which was really shameful.
[01:36:58.480 --> 01:36:59.480]   That's right.
[01:36:59.480 --> 01:37:00.480]   Was it?
[01:37:00.480 --> 01:37:01.480]   Yeah.
[01:37:01.480 --> 01:37:02.480]   Next message is.
[01:37:02.480 --> 01:37:03.480]   Huh?
[01:37:03.480 --> 01:37:04.480]   I wasn't sure if it was proven.
[01:37:04.480 --> 01:37:06.400]   I think it was pretty strongly.
[01:37:06.400 --> 01:37:07.400]   The correlation.
[01:37:07.400 --> 01:37:09.280]   I believe there was litigation.
[01:37:09.280 --> 01:37:12.680]   I'm not sure as to how that shook out though.
[01:37:12.680 --> 01:37:17.040]   It's very icky, very squirmy.
[01:37:17.040 --> 01:37:21.400]   But that's called, what do they call that in PR where you take the lead on a story?
[01:37:21.400 --> 01:37:23.000]   You get ahead of the story.
[01:37:23.000 --> 01:37:24.000]   He got ahead.
[01:37:24.000 --> 01:37:25.800]   He got way ahead of the story.
[01:37:25.800 --> 01:37:30.240]   He got so far ahead of the story that it was like, well, we can't publish these now.
[01:37:30.240 --> 01:37:32.240]   They'll just just ruin it.
[01:37:32.240 --> 01:37:36.200]   Instead of like leaning into a crash, he caused like a different crash.
[01:37:36.200 --> 01:37:38.320]   But it somehow ended up okay.
[01:37:38.320 --> 01:37:39.320]   For him.
[01:37:39.320 --> 01:37:40.520]   He's, I guess, doing all right.
[01:37:40.520 --> 01:37:41.520]   Anyway, back to Apple.
[01:37:41.520 --> 01:37:42.520]   Sorry.
[01:37:42.520 --> 01:37:43.520]   Doing all right.
[01:37:43.520 --> 01:37:44.520]   He's still all right.
[01:37:44.520 --> 01:37:45.520]   He's okay.
[01:37:45.520 --> 01:37:46.520]   He's okay.
[01:37:46.520 --> 01:37:47.520]   Whatever happened to him.
[01:37:47.520 --> 01:37:48.520]   Is he happy?
[01:37:48.520 --> 01:37:49.520]   He's coming back together.
[01:37:49.520 --> 01:37:51.160]   What a rough time he's had.
[01:37:51.160 --> 01:37:56.200]   He's just going to space whenever he feels like we're at a cowboy and take drive in some
[01:37:56.200 --> 01:37:57.200]   helicopters.
[01:37:57.200 --> 01:37:59.400]   You know, do okay.
[01:37:59.400 --> 01:38:01.800]   Billionaires in space.
[01:38:01.800 --> 01:38:03.400]   That's a story from a few months ago.
[01:38:03.400 --> 01:38:04.400]   Apple is pigs in space.
[01:38:04.400 --> 01:38:08.280]   I think you meant more like something that Jero says.
[01:38:08.280 --> 01:38:09.280]   I don't know.
[01:38:09.280 --> 01:38:10.280]   You just said the same thing.
[01:38:10.280 --> 01:38:11.280]   Yeah.
[01:38:11.280 --> 01:38:12.280]   That's a story.
[01:38:12.280 --> 01:38:13.280]   I think you're right.
[01:38:13.280 --> 01:38:14.920]   That could be the name.
[01:38:14.920 --> 01:38:19.760]   Apple's Pro product products will steal the show at the iPhone 14 launch event says Mark
[01:38:19.760 --> 01:38:22.120]   a german.
[01:38:22.120 --> 01:38:29.040]   One of the things I'm a little interested in the rumor that the new Apple Watch Pro,
[01:38:29.040 --> 01:38:35.800]   kind of bigger, heftier sport model, will cost as much as $900.
[01:38:35.800 --> 01:38:39.360]   It's still not the most expensive Apple watch ever made by a launch.
[01:38:39.360 --> 01:38:40.360]   Yes.
[01:38:40.360 --> 01:38:41.360]   Right?
[01:38:41.360 --> 01:38:43.960]   I mean, this is $1,000 short of.
[01:38:43.960 --> 01:38:45.640]   Yeah, they've come down over the years.
[01:38:45.640 --> 01:38:47.040]   So 900 bucks is a price point.
[01:38:47.040 --> 01:38:49.520]   I feel like is a pretty, pretty reasonable ask.
[01:38:49.520 --> 01:38:55.680]   Plus, I mean, you know, Apple always tried to position those watches as a fashion accessory.
[01:38:55.680 --> 01:39:00.720]   If you know anybody who's like really into watches, you spend way more than $900.
[01:39:00.720 --> 01:39:03.960]   Yeah, but that was Johnny's folly.
[01:39:03.960 --> 01:39:05.680]   The Apple Watch solopo.
[01:39:05.680 --> 01:39:09.840]   I feel like with Johnny gone.
[01:39:09.840 --> 01:39:15.880]   It's a little tone deaf to say we're going to make a $900 Apple watch.
[01:39:15.880 --> 01:39:17.560]   The question is what is this?
[01:39:17.560 --> 01:39:21.400]   What is the story Apple's going to tell about why is this watch better than the watches we
[01:39:21.400 --> 01:39:22.400]   already have?
[01:39:22.400 --> 01:39:23.960]   What makes this a pro watch?
[01:39:23.960 --> 01:39:25.440]   Is it just that it's bigger?
[01:39:25.440 --> 01:39:27.800]   Is there something else going on here?
[01:39:27.800 --> 01:39:28.800]   You know, I think.
[01:39:28.800 --> 01:39:31.520]   What could they do to make it worth that much?
[01:39:31.520 --> 01:39:32.520]   That's a great question.
[01:39:32.520 --> 01:39:35.320]   I'll tell you exactly the time that you'll tell you.
[01:39:35.320 --> 01:39:37.680]   How much time do you have left?
[01:39:37.680 --> 01:39:40.680]   It's just a countdown clock.
[01:39:40.680 --> 01:39:42.680]   Johnny, why is this watch going backwards?
[01:39:42.680 --> 01:39:43.680]   It's this thing.
[01:39:43.680 --> 01:39:44.680]   Dominus.
[01:39:44.680 --> 01:39:45.680]   Yeah.
[01:39:45.680 --> 01:39:51.600]   Grumman does remind us that, you know, you can get a solar powered Garmin watch for
[01:39:51.600 --> 01:39:52.600]   it.
[01:39:52.600 --> 01:39:53.680]   And that's $1,300.
[01:39:53.680 --> 01:39:58.720]   So it's not unheard of in these high end sport watches to spend a significant amount of money.
[01:39:58.720 --> 01:40:03.200]   I think Mark made the point as well that it's just some of this is Apple's way of saying
[01:40:03.200 --> 01:40:06.080]   we don't want to see that portion of the market.
[01:40:06.080 --> 01:40:08.160]   We are premium product brands.
[01:40:08.160 --> 01:40:12.000]   So therefore, if we can be charging people more and they will pay it, then we should be
[01:40:12.000 --> 01:40:13.000]   doing that.
[01:40:13.000 --> 01:40:14.000]   Yeah.
[01:40:14.000 --> 01:40:15.000]   Yeah.
[01:40:15.000 --> 01:40:18.280]   Apple holds 36% of the smart watch market.
[01:40:18.280 --> 01:40:20.000]   I would have thought it would be a bigger percentage.
[01:40:20.000 --> 01:40:21.840]   That's according to CounterPoint research.
[01:40:21.840 --> 01:40:22.840]   Yeah.
[01:40:22.840 --> 01:40:23.840]   That's actually surprising.
[01:40:23.840 --> 01:40:24.840]   Yeah.
[01:40:24.840 --> 01:40:28.440]   Garmin is the market share leader for watches over a hundred bucks.
[01:40:28.440 --> 01:40:33.240]   So Garmin is, I think you're right, got a target on its back.
[01:40:33.240 --> 01:40:34.240]   Yeah.
[01:40:34.240 --> 01:40:35.240]   Yeah.
[01:40:35.240 --> 01:40:41.000]   My thought is, I mean, obviously this is incredibly speculative, but I feel like there's always
[01:40:41.000 --> 01:40:45.320]   been people talking about how Apple could at one point buy Peloton.
[01:40:45.320 --> 01:40:50.920]   And I think that would be the thing that would really jumpstart their watch business if they
[01:40:50.920 --> 01:40:56.440]   kind of sold it as a tie in to some sort of exercise based platform in a way that kind
[01:40:56.440 --> 01:41:01.280]   of broaden all the aspects of their fitness ecosystem as well as Apple music.
[01:41:01.280 --> 01:41:06.400]   It could, I think, really kind of bring about an interesting flywheel effect.
[01:41:06.400 --> 01:41:09.760]   Probably they won't do it because Peloton's a big flaming trash fire, but...
[01:41:09.760 --> 01:41:11.400]   Well, it makes it cheaper, right?
[01:41:11.400 --> 01:41:12.400]   That's true.
[01:41:12.400 --> 01:41:15.760]   And fitness plus is already doing a lot of that for them.
[01:41:15.760 --> 01:41:18.640]   I mean, they already have watch integration with that.
[01:41:18.640 --> 01:41:19.640]   And it is pleasing.
[01:41:19.640 --> 01:41:20.640]   It was, I believe, life it.
[01:41:20.640 --> 01:41:22.200]   And all that stuff too.
[01:41:22.200 --> 01:41:23.200]   Yeah.
[01:41:23.200 --> 01:41:27.200]   So I don't know what Peloton would get them other than just scooping up a lot of time.
[01:41:27.200 --> 01:41:28.200]   Hardware, yeah.
[01:41:28.200 --> 01:41:29.200]   Yeah.
[01:41:29.200 --> 01:41:30.200]   And I don't think they want...
[01:41:30.200 --> 01:41:31.200]   Yeah.
[01:41:31.200 --> 01:41:32.200]   Yeah.
[01:41:32.200 --> 01:41:33.600]   Apple could just buy all the Peloton's used.
[01:41:33.600 --> 01:41:34.600]   They're so cheap now.
[01:41:34.600 --> 01:41:35.600]   Just buy a bunch of Peloton's.
[01:41:35.600 --> 01:41:40.080]   Peloton is announced that they're going to start selling their bikes in Amazon and they're
[01:41:40.080 --> 01:41:44.040]   going to stop doing the white glove install service.
[01:41:44.040 --> 01:41:48.400]   I think Peloton's angling to get bought by Amazon, to be honest with you.
[01:41:48.400 --> 01:41:49.400]   Okay.
[01:41:49.400 --> 01:41:54.720]   My thought though is once they start selling these bikes on Amazon, they're going to have
[01:41:54.720 --> 01:42:00.520]   a nightmare on their hands because the delivery of Peloton bikes is very complicated.
[01:42:00.520 --> 01:42:01.520]   Yes.
[01:42:01.520 --> 01:42:02.800]   The bikes are very fragile.
[01:42:02.800 --> 01:42:04.320]   I know this because I have one.
[01:42:04.320 --> 01:42:05.320]   I do too.
[01:42:05.320 --> 01:42:06.320]   And they're heavy as hell.
[01:42:06.320 --> 01:42:09.120]   And so now when it was delivered, it broke immediately.
[01:42:09.120 --> 01:42:13.600]   And it was just that one wire was slightly pulled the wrong way.
[01:42:13.600 --> 01:42:17.800]   And it literally took multiple different technicians coming out.
[01:42:17.800 --> 01:42:22.040]   They had to replace each piece of the bike fully from the ground up.
[01:42:22.040 --> 01:42:25.400]   And another person moved it once when they were replacing one other part of it and they
[01:42:25.400 --> 01:42:26.600]   had to do it all over again.
[01:42:26.600 --> 01:42:31.640]   I mean, if you have an average Amazon delivery person somehow now trucking in your Peloton
[01:42:31.640 --> 01:42:34.200]   bike, it's going to blow apart.
[01:42:34.200 --> 01:42:36.200]   It's going to turn to dust.
[01:42:36.200 --> 01:42:38.240]   How many hundreds of pounds does it weigh?
[01:42:38.240 --> 01:42:40.040]   Also, my exaggerating, it's heavy.
[01:42:40.040 --> 01:42:42.480]   It's not hundreds, but it might be a hundred.
[01:42:42.480 --> 01:42:46.680]   I mean, that's that flywheel has to have a lot of weight.
[01:42:46.680 --> 01:42:48.760]   That's what you're moving when you paddle.
[01:42:48.760 --> 01:42:50.240]   So it has to be heavy.
[01:42:50.240 --> 01:42:53.160]   That's a hard box to get into a lot of places without some.
[01:42:53.160 --> 01:42:54.160]   I agree.
[01:42:54.160 --> 01:42:55.160]   You couldn't get up three slides.
[01:42:55.160 --> 01:42:57.200]   Nobody really makes light exercise equipment.
[01:42:57.200 --> 01:43:00.160]   I want to say this is somebody who will move weights a lot.
[01:43:00.160 --> 01:43:02.040]   Like this design could be heavy.
[01:43:02.040 --> 01:43:03.040]   That is all they do.
[01:43:03.040 --> 01:43:04.640]   You know, wait a minute.
[01:43:04.640 --> 01:43:06.520]   That's a hot category.
[01:43:06.520 --> 01:43:08.480]   Light exercise equipment.
[01:43:08.480 --> 01:43:10.720]   I like it.
[01:43:10.720 --> 01:43:13.800]   If this was Shark Tank, I'd give you some money.
[01:43:13.800 --> 01:43:17.280]   There's some exercise stuff you can fill with water later.
[01:43:17.280 --> 01:43:18.280]   And then it gets heavy.
[01:43:18.280 --> 01:43:19.280]   Oh, that's not good.
[01:43:19.280 --> 01:43:20.280]   That's silly.
[01:43:20.280 --> 01:43:22.280]   That's how you get to be milk for us.
[01:43:22.280 --> 01:43:24.680]   Hey, water is water is weight.
[01:43:24.680 --> 01:43:26.400]   What's wrong with water?
[01:43:26.400 --> 01:43:27.400]   What do you want to do?
[01:43:27.400 --> 01:43:28.400]   Rocky, listen.
[01:43:28.400 --> 01:43:29.400]   I don't know.
[01:43:29.400 --> 01:43:33.800]   I've heard that every person who's drink water is died, so I'm going to stay away.
[01:43:33.800 --> 01:43:35.320]   This is a dark exercise.
[01:43:35.320 --> 01:43:37.280]   It strikes again.
[01:43:37.280 --> 01:43:39.280]   Oh my God.
[01:43:39.280 --> 01:43:43.440]   Listen, there's a common thread that no one's talking about since water.
[01:43:43.440 --> 01:43:44.760]   Very good point.
[01:43:44.760 --> 01:43:49.240]   New Wi-Fi data shows that they know.
[01:43:49.240 --> 01:43:57.560]   The data streamed to at home fitness bikes was down 23% in the first half of 2022.
[01:43:57.560 --> 01:43:58.560]   That's interesting.
[01:43:58.560 --> 01:44:00.080]   We're all even housing.
[01:44:00.080 --> 01:44:03.560]   We're going back to the gym maybe, huh?
[01:44:03.560 --> 01:44:09.560]   Fitness bikes, the single biggest contraction followed by Blu-ray players, iPods, iPods,
[01:44:09.560 --> 01:44:10.560]   and similar devices.
[01:44:10.560 --> 01:44:13.400]   I don't think they had to go out of the iPod.
[01:44:13.400 --> 01:44:15.560]   I put a small computer iPod.
[01:44:15.560 --> 01:44:18.240]   Just media players was the whole category.
[01:44:18.240 --> 01:44:20.080]   Declined 14%.
[01:44:20.080 --> 01:44:23.280]   This is the data streaming via Wi-Fi.
[01:44:23.280 --> 01:44:28.520]   This comes from Blu-ray players stream Wi-Fi.
[01:44:28.520 --> 01:44:29.520]   They don't have enough.
[01:44:29.520 --> 01:44:30.520]   That doesn't make any sense.
[01:44:30.520 --> 01:44:31.520]   I'm streaming.
[01:44:31.520 --> 01:44:32.520]   I'm streaming back to the day.
[01:44:32.520 --> 01:44:34.520]   Let's just say media players.
[01:44:34.520 --> 01:44:37.240]   We're picking the sentence apart, Leo.
[01:44:37.240 --> 01:44:38.240]   I'm sorry.
[01:44:38.240 --> 01:44:39.240]   I am right.
[01:44:39.240 --> 01:44:40.240]   We're coming for you.
[01:44:40.240 --> 01:44:41.240]   Let's go Rutgers.
[01:44:41.240 --> 01:44:42.240]   Who's watching you Blu-ray?
[01:44:42.240 --> 01:44:43.240]   Yes.
[01:44:43.240 --> 01:44:44.240]   I am.
[01:44:44.240 --> 01:44:47.960]   And then PCs were down 7% year over year.
[01:44:47.960 --> 01:44:52.040]   This is because Plume, which makes those little pods that Comcast uses.
[01:44:52.040 --> 01:44:55.280]   A number of companies also use for their mesh Wi-Fi.
[01:44:55.280 --> 01:44:58.320]   Of course, you can buy them directly.
[01:44:58.320 --> 01:45:04.320]   Smart TVs, data consumption increased by 34%.
[01:45:04.320 --> 01:45:08.760]   Smart speakers, and I think I'm mostly responsible for this, up 27%.
[01:45:08.760 --> 01:45:12.800]   I was going to say you have 18 in every room in your house.
[01:45:12.800 --> 01:45:15.560]   They're all sending every conversation, of course.
[01:45:15.560 --> 01:45:17.080]   So up 27%.
[01:45:17.080 --> 01:45:19.760]   Every cough, every shower you take.
[01:45:19.760 --> 01:45:22.440]   I'm not allowed to say I'm on it.
[01:45:22.440 --> 01:45:24.600]   This Plume group here, this day is...
[01:45:24.600 --> 01:45:28.040]   Yeah, I was going to say Plume is just selling data left and right.
[01:45:28.040 --> 01:45:29.960]   Well, I say it's anonymous.
[01:45:29.960 --> 01:45:33.200]   They're in the house of every person that owns an iPod.
[01:45:33.200 --> 01:45:34.200]   It's anonymous.
[01:45:34.200 --> 01:45:35.200]   Yeah, all the iPods.
[01:45:35.200 --> 01:45:40.680]   I don't think it's a non-representative sample, honestly, if they're only an iPod that Blu-ray
[01:45:40.680 --> 01:45:41.680]   players.
[01:45:41.680 --> 01:45:45.120]   I think it's media things, and they broke it down to Blu-ray players' iPods in similar
[01:45:45.120 --> 01:45:46.600]   devices, but I think media players.
[01:45:46.600 --> 01:45:48.640]   I think it's Blu-ray streamers.
[01:45:48.640 --> 01:45:49.640]   I think it's Apple TV.
[01:45:49.640 --> 01:45:50.640]   Creamy box.
[01:45:50.640 --> 01:45:51.640]   Roku.
[01:45:51.640 --> 01:45:52.640]   It's Apple TV.
[01:45:52.640 --> 01:45:53.640]   I hope it is.
[01:45:53.640 --> 01:45:54.640]   I hope it is.
[01:45:54.640 --> 01:45:57.640]   I will say this is an unpopular thing, but I want my TVs to be dumb.
[01:45:57.640 --> 01:46:00.320]   I still have TVs that are all like 5.
[01:46:00.320 --> 01:46:04.840]   I bought my TV like 5 years ago, and I'm like, "I don't want you to connect to the Internet."
[01:46:04.840 --> 01:46:05.840]   No.
[01:46:05.840 --> 01:46:08.920]   I'll plug my Roku in and out as a reset and nothing more.
[01:46:08.920 --> 01:46:09.920]   Exactly.
[01:46:09.920 --> 01:46:13.040]   We got a smart TV, because we're on a bigger TV during pandemic.
[01:46:13.040 --> 01:46:14.360]   We were like, "Oh, it's got a bigger TV.
[01:46:14.360 --> 01:46:15.360]   We're watching it more."
[01:46:15.360 --> 01:46:17.320]   It needs something to watch.
[01:46:17.320 --> 01:46:20.760]   We did, and I got a Vizio.
[01:46:20.760 --> 01:46:24.880]   The great part is not only that it's smart, and I have to deal with its stuff.
[01:46:24.880 --> 01:46:29.200]   At some point in the last few months, it developed this new firmware update that I can't figure
[01:46:29.200 --> 01:46:35.440]   out how to control, where when the video source gets turned off like the Apple TV, if the HDMI,
[01:46:35.440 --> 01:46:40.480]   what does it call, HDMI CEC, thing doesn't work, and your TV gets turned off also, then
[01:46:40.480 --> 01:46:42.560]   the TV just starts advertising itself.
[01:46:42.560 --> 01:46:45.960]   It starts putting up movies and stuff you can buy through the smart TV part.
[01:46:45.960 --> 01:46:50.080]   I did not buy you, so you could advertise to me in your downtime.
[01:46:50.080 --> 01:46:51.800]   That is not the reason.
[01:46:51.800 --> 01:46:54.400]   This is the kindlefication of everything.
[01:46:54.400 --> 01:46:56.240]   It's just like, "Oh, you have a blank screen?
[01:46:56.240 --> 01:46:57.640]   Let me throw some ads up there."
[01:46:57.640 --> 01:46:59.480]   Nothing you can do to stop it.
[01:46:59.480 --> 01:47:00.480]   Try.
[01:47:00.480 --> 01:47:01.480]   Yeah, I paid full price.
[01:47:01.480 --> 01:47:02.480]   There's no special offer there.
[01:47:02.480 --> 01:47:03.480]   That's right.
[01:47:03.480 --> 01:47:05.720]   I paid full price for that puppy, but it's pretty cheap.
[01:47:05.720 --> 01:47:06.720]   I don't know.
[01:47:06.720 --> 01:47:10.480]   The video of monitor companies, TV companies, I think a few years ago, it turned out that
[01:47:10.480 --> 01:47:13.160]   most of them weren't making any money, an exam song.
[01:47:13.160 --> 01:47:17.600]   Only the ones that made their own displays, their own panels were, so they have to make
[01:47:17.600 --> 01:47:18.600]   money somehow.
[01:47:18.600 --> 01:47:19.600]   They're skimming it off the top.
[01:47:19.600 --> 01:47:20.600]   Yeah.
[01:47:20.600 --> 01:47:23.520]   I'm writing down the kindlefication of everything.
[01:47:23.520 --> 01:47:24.520]   I will use that somewhere.
[01:47:24.520 --> 01:47:25.520]   That's good.
[01:47:25.520 --> 01:47:26.520]   I like it.
[01:47:26.520 --> 01:47:27.520]   Thank you for letting the hat.
[01:47:27.520 --> 01:47:31.160]   Paris, when you disconnect, you'll be the ad replacing you on the screen.
[01:47:31.160 --> 01:47:32.160]   Yeah, yeah.
[01:47:32.160 --> 01:47:38.920]   It'll just be a little emoji, but it'll just say, "Buy Apple products."
[01:47:38.920 --> 01:47:45.160]   One of the things apparently, and this is contrary to Johnny Ives' long instructions,
[01:47:45.160 --> 01:47:49.640]   German says the iPhone 14 Pro models will have bigger batteries.
[01:47:49.640 --> 01:47:54.160]   Okay, why is that what Johnny Ives was against?
[01:47:54.160 --> 01:47:55.160]   He likes thinner lighter.
[01:47:55.160 --> 01:47:56.160]   Yeah, yeah.
[01:47:56.160 --> 01:47:59.800]   Okay, I know, but a worse performing phone?
[01:47:59.800 --> 01:48:02.080]   Well, yes, and this is why I'm glad Sir Johnny.
[01:48:02.080 --> 01:48:03.400]   Battery for Inno against.
[01:48:03.400 --> 01:48:04.400]   That was the problem.
[01:48:04.400 --> 01:48:05.400]   Yes.
[01:48:05.400 --> 01:48:10.120]   Being able to use your phone for more than eight hours a day is really an elegant.
[01:48:10.120 --> 01:48:11.120]   No, no, no.
[01:48:11.120 --> 01:48:12.120]   No, no.
[01:48:12.120 --> 01:48:13.760]   This is going to end up being controversial.
[01:48:13.760 --> 01:48:21.040]   I predict the new iPhone cutouts will be a lozenge and a pill, but, and this is the
[01:48:21.040 --> 01:48:24.880]   thing I think is going to be controversial, Apple is going to darken the screen between
[01:48:24.880 --> 01:48:26.720]   the lozenge and the pill.
[01:48:26.720 --> 01:48:28.240]   Yeah, I saw this.
[01:48:28.240 --> 01:48:32.360]   And that's where it will do simulated lights for if your camera is on or your microphone
[01:48:32.360 --> 01:48:33.360]   is on.
[01:48:33.360 --> 01:48:36.840]   I think that it will look like a big lozenge, except it won't be.
[01:48:36.840 --> 01:48:40.720]   The cutouts aren't, but the screen is going to just extend that.
[01:48:40.720 --> 01:48:42.880]   And then what happens to this wedge above?
[01:48:42.880 --> 01:48:48.240]   Yeah, no, what happens is border up here is just thrown away, just useless.
[01:48:48.240 --> 01:48:50.160]   You can't put anything up there.
[01:48:50.160 --> 01:48:55.080]   Up there is just going to be where it keeps saying like 5G plus plus plus.
[01:48:55.080 --> 01:48:56.080]   No, no.
[01:48:56.080 --> 01:48:58.880]   I think you're going to go all the way up like one bar.
[01:48:58.880 --> 01:48:59.880]   It's going to be here.
[01:48:59.880 --> 01:49:00.880]   Yeah.
[01:49:00.880 --> 01:49:01.880]   We've changed the bar system.
[01:49:01.880 --> 01:49:02.880]   It's no longer five bars.
[01:49:02.880 --> 01:49:04.880]   Twenty five small bars.
[01:49:04.880 --> 01:49:08.360]   Twenty five small bars and like seven star symbols.
[01:49:08.360 --> 01:49:09.360]   It's a bar crawl now.
[01:49:09.360 --> 01:49:11.360]   You're going to go to twenty five bars.
[01:49:11.360 --> 01:49:16.600]   You're still not going to be able to make a video call if you're not connected to Wi-Fi.
[01:49:16.600 --> 01:49:17.600]   Okay.
[01:49:17.600 --> 01:49:19.440]   Let's see what else.
[01:49:19.440 --> 01:49:21.080]   Oh, the AirPod 2.
[01:49:21.080 --> 01:49:22.400]   Now AirPods are a product.
[01:49:22.400 --> 01:49:25.680]   I am not, I have to be really covering.
[01:49:25.680 --> 01:49:26.920]   I'm rocking some AirPods.
[01:49:26.920 --> 01:49:27.920]   Are you?
[01:49:27.920 --> 01:49:28.920]   Are you like AirPods?
[01:49:28.920 --> 01:49:29.920]   Oh, yeah.
[01:49:29.920 --> 01:49:32.480]   I love my AirPods just because I enjoy being in.
[01:49:32.480 --> 01:49:34.640]   As close to silence as possible.
[01:49:34.640 --> 01:49:36.480]   I really love to walk around with.
[01:49:36.480 --> 01:49:40.680]   I have a choice of contributors on CNN, I believe.
[01:49:40.680 --> 01:49:42.880]   A very hot market.
[01:49:42.880 --> 01:49:45.360]   I see them all the time.
[01:49:45.360 --> 01:49:48.880]   And you can't miss it because it's a big white thing in your ear.
[01:49:48.880 --> 01:49:51.040]   And all of them have like the same buzz cut, right?
[01:49:51.040 --> 01:49:52.040]   Yeah.
[01:49:52.040 --> 01:49:53.040]   Well, that too.
[01:49:53.040 --> 01:49:54.040]   Yeah.
[01:49:54.040 --> 01:49:57.440]   They will, the AirPod 2 will have, you'll appreciate this.
[01:49:57.440 --> 01:49:59.400]   Well, the pros have Norris cancellation.
[01:49:59.400 --> 01:50:00.400]   The AirPod 2 is going to have.
[01:50:00.400 --> 01:50:01.400]   I've got pros.
[01:50:01.400 --> 01:50:06.840]   They're going to sound better because the rumor is, and I hope this is true, because Apple
[01:50:06.840 --> 01:50:09.840]   has been usually kind of laggard in Bluetooth.
[01:50:09.840 --> 01:50:12.480]   Oh, the sound quality is awful.
[01:50:12.480 --> 01:50:13.480]   It's terrible.
[01:50:13.480 --> 01:50:16.920]   I'm using these just for audio in, but I have my big mic for audio.
[01:50:16.920 --> 01:50:19.200]   Don't use them for a mic, absolutely.
[01:50:19.200 --> 01:50:21.640]   The sound quality in your ear is pretty soft.
[01:50:21.640 --> 01:50:25.800]   I use them for a podcast, and that's about it listening to the music.
[01:50:25.800 --> 01:50:26.800]   This is that thing.
[01:50:26.800 --> 01:50:32.040]   I used to get so much email about Bluetooth stuff at Macworld, and a few years ago when
[01:50:32.040 --> 01:50:36.000]   I was doing an editing stint at Wirecutter, we wrote kind of a fact about people always
[01:50:36.000 --> 01:50:38.600]   like, "Why is Bluetooth audio so bad?"
[01:50:38.600 --> 01:50:39.600]   So bad.
[01:50:39.600 --> 01:50:42.640]   So we were like, "Well, and I actually contacted the Bluetooth SIG, and they were kind of like,
[01:50:42.640 --> 01:50:46.480]   "Well, it is bad because I was like, 'Wait, where are you thinking?
[01:50:46.480 --> 01:50:52.800]   There's a very particular scenario that Bluetooth weirdly wasn't prepared for, which was streaming
[01:50:52.800 --> 01:50:58.640]   high quality audio in both directions, certain circumstances, or I think over Bluetooth
[01:50:58.640 --> 01:51:05.040]   LE, Bluetooth 4 LE, if you have a low power version, low energy, you can be using the wrong
[01:51:05.040 --> 01:51:06.040]   profile.
[01:51:06.040 --> 01:51:10.280]   So if you have the right device and the right profiles, it'll sound great, then you will
[01:51:10.280 --> 01:51:15.320]   walk to a different device or even a different model of the same device, like an older MacBook,
[01:51:15.320 --> 01:51:21.120]   and you sound like you're coming in an old radio show from the 1950s or something.
[01:51:21.120 --> 01:51:24.600]   It's strange also, Bluetooth, I don't know if you guys have noticed this when walking
[01:51:24.600 --> 01:51:29.560]   around in large cities, but here in New York, Bluetooth has always really craps out when
[01:51:29.560 --> 01:51:34.960]   you're walking across like an avenue or something, even if your phone is in your pocket.
[01:51:34.960 --> 01:51:38.440]   I will say like, one out of five times will cut in and out.
[01:51:38.440 --> 01:51:41.560]   And I had a colleague who looked into this and wrote an article that I'm now forgetting
[01:51:41.560 --> 01:51:45.160]   the details of, of course, but I think it's because there are so many different signals
[01:51:45.160 --> 01:51:48.640]   crossing off the same path that it ends up having to do.
[01:51:48.640 --> 01:51:51.600]   So you're crossing, you're with like an across walk or you're a bunch of people with you
[01:51:51.600 --> 01:51:56.800]   crossing or just, yeah, even if you're by yourself, like, just because there is a lot
[01:51:56.800 --> 01:51:58.320]   happening in this space.
[01:51:58.320 --> 01:52:03.840]   Well, but Bluetooth uses, let's thank Hedy Lamarr, actress Hedy Lamarr for Bluetooth frequency
[01:52:03.840 --> 01:52:05.320]   hopping spread spectrum.
[01:52:05.320 --> 01:52:07.360]   Yes, it still uses frequency hopping.
[01:52:07.360 --> 01:52:13.240]   So it should actually be more resilient for, but then anything else because it swaps among
[01:52:13.240 --> 01:52:16.880]   like 80 something frequencies on a pattern basis.
[01:52:16.880 --> 01:52:19.760]   But that's a, I think in like, I just, it's a lot of equations.
[01:52:19.760 --> 01:52:21.120]   It gets a bit confused.
[01:52:21.120 --> 01:52:22.120]   What's the flexion?
[01:52:22.120 --> 01:52:23.800]   Yeah, a lot of reflection going on.
[01:52:23.800 --> 01:52:30.160]   I think there's a great gulf between the spec and the results on all of this stuff.
[01:52:30.160 --> 01:52:35.840]   And Apple will have a new, the new H, what is it, the H chip, the H3, I guess it will
[01:52:35.840 --> 01:52:37.360]   be in these new air pods.
[01:52:37.360 --> 01:52:43.320]   And it is hoped by some that it will in fact upgrade the Bluetooth LE spec to use LC3,
[01:52:43.320 --> 01:52:45.920]   which is a much higher quality codec.
[01:52:45.920 --> 01:52:52.040]   Definitely a better, I don't know if the beats studio buds and they are absolutely rock solid
[01:52:52.040 --> 01:52:55.280]   for audio playback and the audio input is terrible.
[01:52:55.280 --> 01:52:57.200]   I mean, it's, and I don't think it's the mic quality.
[01:52:57.200 --> 01:52:58.640]   I think it's simply the standard.
[01:52:58.640 --> 01:52:59.640]   Yeah.
[01:52:59.640 --> 01:53:00.640]   Yeah.
[01:53:00.640 --> 01:53:01.640]   I mean, that's one of the first.
[01:53:01.640 --> 01:53:04.520]   I will say the one thing about these air pods pro, which I guess is like a recent software
[01:53:04.520 --> 01:53:07.720]   update that I absolutely love and wish would come to other devices that I have.
[01:53:07.720 --> 01:53:13.040]   Like I have over the ear, both headphones is with apples, find my app.
[01:53:13.040 --> 01:53:14.640]   I lose everything always.
[01:53:14.640 --> 01:53:18.360]   So literally before we were recording this, I was like, where did I put my air pods?
[01:53:18.360 --> 01:53:21.920]   And you can just open it up and it will tell you like you're not close.
[01:53:21.920 --> 01:53:25.160]   And then you walk over to the other side of your house and you're like, you're closer.
[01:53:25.160 --> 01:53:28.240]   And then it's like, that's five feet to your right and one foot down.
[01:53:28.240 --> 01:53:29.560]   And you can find them like that.
[01:53:29.560 --> 01:53:32.160]   And I never more than 10 feet from a pair of air pods.
[01:53:32.160 --> 01:53:33.160]   Yeah.
[01:53:33.160 --> 01:53:36.560]   I mean, you know, every person in Jess about 3.1.
[01:53:36.560 --> 01:53:39.200]   Yeah, if you got them in your tummy, you're right next to them.
[01:53:39.200 --> 01:53:44.120]   I have air tags on my keys, but it does, the problem is you have to be near them or it
[01:53:44.120 --> 01:53:46.040]   says, I can't, I don't know where they are.
[01:53:46.040 --> 01:53:50.840]   And I'm a big and able to, uh, no, no, no, wait, other devices.
[01:53:50.840 --> 01:53:53.480]   Oh, oh, you want the very short range.
[01:53:53.480 --> 01:53:55.480]   I want to find my keys.
[01:53:55.480 --> 01:53:57.600]   I'm very simply the sound on them.
[01:53:57.600 --> 01:54:02.440]   Why don't you play unless they're unless they're, I tried.
[01:54:02.440 --> 01:54:05.200]   It says something's something's terribly wrong, then.
[01:54:05.200 --> 01:54:07.960]   I lost my keys this morning.
[01:54:07.960 --> 01:54:12.440]   And it said we can't, you have to get closer to your keys for us to play a sound on them.
[01:54:12.440 --> 01:54:13.440]   Oh, that's right.
[01:54:13.440 --> 01:54:14.920]   It's Bluetooth range for keys.
[01:54:14.920 --> 01:54:15.920]   That's right.
[01:54:15.920 --> 01:54:19.040]   But it should be able to find them when you're has no idea.
[01:54:19.040 --> 01:54:20.040]   It says it's in the house.
[01:54:20.040 --> 01:54:22.800]   It'll show you like near or far in the house.
[01:54:22.800 --> 01:54:23.800]   They're in the house.
[01:54:23.800 --> 01:54:24.800]   They're in the house.
[01:54:24.800 --> 01:54:25.800]   You can't, it doesn't look too simple.
[01:54:25.800 --> 01:54:26.800]   But I know they're in the house.
[01:54:26.800 --> 01:54:28.080]   That's not the problem.
[01:54:28.080 --> 01:54:29.800]   It's where in the house.
[01:54:29.800 --> 01:54:33.920]   When you're looking at your house, you shouldn't have built your house as a Faraday cage that
[01:54:33.920 --> 01:54:34.920]   blocks all signal signals.
[01:54:34.920 --> 01:54:35.920]   My hand.
[01:54:35.920 --> 01:54:36.920]   Single story.
[01:54:36.920 --> 01:54:37.920]   So it's spread out.
[01:54:37.920 --> 01:54:38.920]   So funny.
[01:54:38.920 --> 01:54:42.360]   And I think honestly, so what I have to do and what I did this morning is, I'm going to
[01:54:42.360 --> 01:54:47.520]   bring his wanderer all around the house until it says, I see him now.
[01:54:47.520 --> 01:54:48.960]   And then it'll play a sound.
[01:54:48.960 --> 01:54:49.960]   I see.
[01:54:49.960 --> 01:54:55.400]   Leo, you know what you need to do with you and your 87 phones, the ones that are iPhones,
[01:54:55.400 --> 01:55:00.480]   you should hide one in each room and then connect it in the cloud.
[01:55:00.480 --> 01:55:06.520]   So that you're never, you have one iPhone within every room to find your keys.
[01:55:06.520 --> 01:55:08.800]   No, there's always an iPhone within 10 feet.
[01:55:08.800 --> 01:55:12.880]   I wrote a book about air tags and fine my, of course, because I've written a book about
[01:55:12.880 --> 01:55:14.080]   everything now.
[01:55:14.080 --> 01:55:18.640]   And is it a take control book or it's a take control of air tags and fine my or fine
[01:55:18.640 --> 01:55:20.400]   my, I might have to buy this.
[01:55:20.400 --> 01:55:21.880]   And it's, but it's fun.
[01:55:21.880 --> 01:55:24.400]   I mean, how do you write a book, 120 page book about it?
[01:55:24.400 --> 01:55:27.560]   It turns out to be easy because there's so much complexity.
[01:55:27.560 --> 01:55:32.640]   But recently my wife, we call, we say, we use that we call my wife the early rejector
[01:55:32.640 --> 01:55:34.200]   in the house and I use that with affection.
[01:55:34.200 --> 01:55:38.720]   She doesn't want to adopt new technology before it's matured enough.
[01:55:38.720 --> 01:55:40.760]   And you know, I'm the person who is testing everything out.
[01:55:40.760 --> 01:55:43.760]   And so the other day she said she got lost in a parking garage.
[01:55:43.760 --> 01:55:47.840]   I've been lost in as well because there's multiple floors you can't get to from each
[01:55:47.840 --> 01:55:48.840]   other.
[01:55:48.840 --> 01:55:52.200]   So she'd parked in one of them, go out up an elevator and then take the elevator down
[01:55:52.200 --> 01:55:54.760]   to a different part that's non-contiguous.
[01:55:54.760 --> 01:55:57.600]   And she's like, all right, can I get an air tag for the car?
[01:55:57.600 --> 01:56:00.760]   And she's a car she drives more often than I do.
[01:56:00.760 --> 01:56:03.880]   But then when my older son and I drive that car, we're like, there's an air tag moving
[01:56:03.880 --> 01:56:04.880]   with you.
[01:56:04.880 --> 01:56:05.880]   Like, what?
[01:56:05.880 --> 01:56:06.880]   Oh, it's so I hate that.
[01:56:06.880 --> 01:56:07.880]   Yeah.
[01:56:07.880 --> 01:56:08.880]   Yeah.
[01:56:08.880 --> 01:56:10.080]   Yeah, you know what?
[01:56:10.080 --> 01:56:13.680]   This book, 130 pages, it should be longer.
[01:56:13.680 --> 01:56:14.680]   Should be longer.
[01:56:14.680 --> 01:56:15.680]   Make it longer.
[01:56:15.680 --> 01:56:17.520]   Yeah, there's a lot of privacy stuff.
[01:56:17.520 --> 01:56:18.520]   And all right.
[01:56:18.520 --> 01:56:19.520]   I need more.
[01:56:19.520 --> 01:56:23.160]   Honestly, the thing I wish air tags could do, which I don't know why they have an Institute
[01:56:23.160 --> 01:56:28.520]   this feature is if you, let's say, like, I was on a trip the other week and I had my
[01:56:28.520 --> 01:56:33.240]   air tag in my keys and like carry on that I left at the hotel while I was out and about
[01:56:33.240 --> 01:56:34.240]   like, you know, storage thing.
[01:56:34.240 --> 01:56:36.200]   And I was like, oh, I'd love it.
[01:56:36.200 --> 01:56:40.080]   Shouldn't I be able to turn the setting that if my bag is moved, if my air tag is moved
[01:56:40.080 --> 01:56:42.480]   from this location, it gives me an alert.
[01:56:42.480 --> 01:56:44.320]   You can't do that for some reason.
[01:56:44.320 --> 01:56:45.880]   And you should be able to do that.
[01:56:45.880 --> 01:56:47.080]   Oh, but it's an anti-stocking.
[01:56:47.080 --> 01:56:49.960]   It's an anti-stocking problem.
[01:56:49.960 --> 01:56:50.960]   It's a two edge.
[01:56:50.960 --> 01:56:51.960]   I know it's funny.
[01:56:51.960 --> 01:56:52.960]   Somebody was just asking me the day.
[01:56:52.960 --> 01:56:53.960]   They're like, I exactly that.
[01:56:53.960 --> 01:56:55.000]   I left my luggage in my hotel room.
[01:56:55.000 --> 01:56:56.000]   Why can I mark it?
[01:56:56.000 --> 01:57:00.000]   So when it moves, like because the small percentage of people ruining it for all the rest of
[01:57:00.000 --> 01:57:05.280]   us are going to use that as a tool to track people, but there should be a consent thing.
[01:57:05.280 --> 01:57:09.320]   There should be a way to say, I'm going to do this, but it's also going to every time
[01:57:09.320 --> 01:57:13.280]   it does it, it's going to put out an alert immediately to everybody around you as opposed
[01:57:13.280 --> 01:57:14.920]   to moving with you thing.
[01:57:14.920 --> 01:57:19.640]   I mean, there should be ways to make it more.
[01:57:19.640 --> 01:57:23.240]   It should be able to announce itself more than it's doing it while still keeping you,
[01:57:23.240 --> 01:57:25.520]   your device or your stuff safe.
[01:57:25.520 --> 01:57:26.520]   Absolutely.
[01:57:26.520 --> 01:57:31.520]   Apple will not be announcing an AR/VR headset on Wednesday.
[01:57:31.520 --> 01:57:33.000]   We're pretty sure of that.
[01:57:33.000 --> 01:57:37.880]   Although there were rumors they would do a VR headset this year.
[01:57:37.880 --> 01:57:46.240]   Mark Herman is now saying next year, but they did file a trademark for, this is filed by
[01:57:46.240 --> 01:57:51.040]   Immersive Health Solutions LLC in Wilmington, Delaware, which it turns out is an Apple Shell
[01:57:51.040 --> 01:57:56.320]   Corporation for a trademark for Reality 1.
[01:57:56.320 --> 01:58:05.600]   So reality, and maybe they're going to call AR Apple reality, right?
[01:58:05.600 --> 01:58:07.760]   Reality 1 and Reality Pro have been trademarked.
[01:58:07.760 --> 01:58:12.040]   Apple and Facebook meta, I guess, are just really going as broad as possible with these
[01:58:12.040 --> 01:58:13.040]   names.
[01:58:13.040 --> 01:58:14.040]   Yeah.
[01:58:14.040 --> 01:58:15.040]   All part.
[01:58:15.040 --> 01:58:16.040]   What are you going to call it?
[01:58:16.040 --> 01:58:17.040]   I mean, Google Glass Apple Reality.
[01:58:17.040 --> 01:58:18.040]   I don't know.
[01:58:18.040 --> 01:58:20.960]   I mean, just how do you trademark the word reality?
[01:58:20.960 --> 01:58:21.960]   That's my question.
[01:58:21.960 --> 01:58:22.960]   Well, apparently you do.
[01:58:22.960 --> 01:58:23.960]   Apparently you can.
[01:58:23.960 --> 01:58:24.960]   Apparently you can.
[01:58:24.960 --> 01:58:27.480]   Well, the world was living in it.
[01:58:27.480 --> 01:58:35.000]   In several countries over the past few weeks, including the US, they also trademarked Apple,
[01:58:35.000 --> 01:58:38.580]   let's see, Reality Processor.
[01:58:38.580 --> 01:58:40.560]   So that would be the on board.
[01:58:40.560 --> 01:58:42.260]   It's called our brains.
[01:58:42.260 --> 01:58:44.260]   The reality processor.
[01:58:44.260 --> 01:58:47.120]   We'll handle the Apple Brain if you have to be traded.
[01:58:47.120 --> 01:58:48.120]   So what's your thoughts?
[01:58:48.120 --> 01:58:53.520]   I mean, are you waiting with Bated Breath for an Apple VR AR headset?
[01:58:53.520 --> 01:58:56.920]   I want to be convinced that I want it.
[01:58:56.920 --> 01:58:58.920]   That's the thing for me.
[01:58:58.920 --> 01:59:01.200]   I've used a little bit of VR stuff in the past.
[01:59:01.200 --> 01:59:05.240]   Apple's been talking a good AR game for a long time, but I've always thought that the
[01:59:05.240 --> 01:59:10.680]   demos they've done in their product demonstrations before are, they don't really represent what's
[01:59:10.680 --> 01:59:11.680]   so great about it.
[01:59:11.680 --> 01:59:13.840]   Because they'll be up on the stage with the phone or the iPad and they'll be pointing
[01:59:13.840 --> 01:59:16.080]   at a table and like, this is amazing.
[01:59:16.080 --> 01:59:17.640]   The reality's been all augmented.
[01:59:17.640 --> 01:59:21.840]   But fundamentally, a phone and a tablet are not a good way to experience this because
[01:59:21.840 --> 01:59:25.320]   it's like peeping through a peephole, like look into the immersive world, but you have
[01:59:25.320 --> 01:59:29.120]   to hold this thing of glass in front of you in order to peek through.
[01:59:29.120 --> 01:59:32.640]   Whereas, you know, obviously a headset seems like a much better proposition for that.
[01:59:32.640 --> 01:59:36.800]   So they've laid a lot of groundwork for it, which is encouraging.
[01:59:36.800 --> 01:59:38.280]   But it's a hard sell.
[01:59:38.280 --> 01:59:40.040]   I mean, I think Google learned that.
[01:59:40.040 --> 01:59:41.040]   Yeah.
[01:59:41.040 --> 01:59:44.200]   So, you know, glasses are the ideal format.
[01:59:44.200 --> 01:59:47.360]   And I don't think we're ready for glasses because you want something that's inobtrusive.
[01:59:47.360 --> 01:59:49.400]   It doesn't look like Google Glass.
[01:59:49.400 --> 01:59:53.600]   I know somebody on Twitter was pointing out, it was only a few years ago that people were
[01:59:53.600 --> 01:59:56.120]   literally beating people in the street for wearing Google Glasses.
[01:59:56.120 --> 01:59:58.320]   I'm like, yeah, it was kind of crazy.
[01:59:58.320 --> 02:00:03.640]   People got so mad about the feeling of invasiveness by them and the way that people who had early
[02:00:03.640 --> 02:00:06.680]   Google Glasses were using them without kind of exercise.
[02:00:06.680 --> 02:00:09.680]   Have we changed enough as a society in the intervening time?
[02:00:09.680 --> 02:00:13.320]   I mean, everybody's got a camera on their phone or four different cameras on their phone.
[02:00:13.320 --> 02:00:14.320]   I mean, we used to the...
[02:00:14.320 --> 02:00:18.400]   I mean, no one's beating people up for wearing those Facebook Raybans.
[02:00:18.400 --> 02:00:19.400]   [laughter]
[02:00:19.400 --> 02:00:21.400]   Now, I'm good with this.
[02:00:21.400 --> 02:00:22.400]   Snap.
[02:00:22.400 --> 02:00:23.400]   That's the same thing.
[02:00:23.400 --> 02:00:24.400]   Or the snapchat glasses.
[02:00:24.400 --> 02:00:28.400]   But once they're inobtrusive enough, if they're just a layer that you barely visit.
[02:00:28.400 --> 02:00:32.160]   What if you were wearing them right now, Glenn, and on your existing spectacles?
[02:00:32.160 --> 02:00:37.400]   Just joining us with sunglasses on for the entire hour.
[02:00:37.400 --> 02:00:38.400]   Or on our hair.
[02:00:38.400 --> 02:00:39.400]   I don't speak English.
[02:00:39.400 --> 02:00:40.400]   I don't speak English.
[02:00:40.400 --> 02:00:42.400]   This is being translated live on my gosh.
[02:00:42.400 --> 02:00:44.400]   It could be a heads up display on those glasses.
[02:00:44.400 --> 02:00:47.400]   I guess they'd have to have a bigger battery.
[02:00:47.400 --> 02:00:50.400]   They'd have to have more than a million years to get to the point.
[02:00:50.400 --> 02:00:56.200]   I will say, I mean, the thing is, over the past month, I've had my just randomly thinking
[02:00:56.200 --> 02:00:58.840]   while I was doing something, I was like, man, it would be really nice to have a heads up
[02:00:58.840 --> 02:01:04.480]   display that somehow did work with either eye movements or maybe a voice component.
[02:01:04.480 --> 02:01:07.400]   You're like washing the dishes and you're like, oh man, I want it.
[02:01:07.400 --> 02:01:09.920]   I can't remember the name of that one thing I'm thinking of.
[02:01:09.920 --> 02:01:11.920]   I don't want to have to dry my hands to look it up.
[02:01:11.920 --> 02:01:12.920]   That was great.
[02:01:12.920 --> 02:01:13.920]   But I think that-
[02:01:13.920 --> 02:01:14.920]   But I think that-
[02:01:14.920 --> 02:01:15.920]   That's between his legs, Paris, between his legs.
[02:01:15.920 --> 02:01:16.920]   I know.
[02:01:16.920 --> 02:01:17.920]   I always forget that.
[02:01:17.920 --> 02:01:20.920]   But the thing is, I think that while-
[02:01:20.920 --> 02:01:21.920]   Sorry, that's weird.
[02:01:21.920 --> 02:01:22.920]   I apologize.
[02:01:22.920 --> 02:01:24.920]   Listen, sometimes you got to look it up.
[02:01:24.920 --> 02:01:25.920]   It's a callback.
[02:01:25.920 --> 02:01:26.920]   Yeah.
[02:01:26.920 --> 02:01:32.120]   The thing is, I think that we're a long ways away from making that work, like technology.
[02:01:32.120 --> 02:01:34.360]   I don't feel like anybody wants this.
[02:01:34.360 --> 02:01:38.360]   That this is the tech industry saying, we need the next iPhone.
[02:01:38.360 --> 02:01:39.800]   What is it going to be?
[02:01:39.800 --> 02:01:40.800]   And that's the best they could do.
[02:01:40.800 --> 02:01:42.360]   That's why I feel about some of the VR stuff right now.
[02:01:42.360 --> 02:01:48.320]   I think that AR could have very interesting real world applications that would be very
[02:01:48.320 --> 02:01:49.320]   useful.
[02:01:49.320 --> 02:01:53.480]   But I think that full VR right now is just too on Cany Valley, and it doesn't seem like
[02:01:53.480 --> 02:01:55.400]   there are that many use cases for it.
[02:01:55.400 --> 02:01:56.400]   Yeah.
[02:01:56.400 --> 02:02:00.000]   The AR side of things, I think, you got to remember, it's not like someone's going to
[02:02:00.000 --> 02:02:04.640]   come out and hand you that pair of spectacles that looks totally unobtrusive.
[02:02:04.640 --> 02:02:05.960]   That's not the first gen product.
[02:02:05.960 --> 02:02:06.960]   No one's going to be-
[02:02:06.960 --> 02:02:07.960]   We've perfected it already.
[02:02:07.960 --> 02:02:08.960]   We've done it on the front.
[02:02:08.960 --> 02:02:09.960]   Right?
[02:02:09.960 --> 02:02:14.680]   If there's something there, it's going to take a whole bunch of iteration to get to that
[02:02:14.680 --> 02:02:15.680]   point.
[02:02:15.680 --> 02:02:20.560]   I think that's the ultimate goal, but it's going to be messy and ugly for a while.
[02:02:20.560 --> 02:02:22.360]   Not everything is the original smartphone.
[02:02:22.360 --> 02:02:24.360]   That might be enough to kill it, right?
[02:02:24.360 --> 02:02:25.360]   It could be.
[02:02:25.360 --> 02:02:28.480]   It looked so dorky on a segue that it was over.
[02:02:28.480 --> 02:02:30.560]   It was like, yeah, that's not going anywhere.
[02:02:30.560 --> 02:02:31.560]   Yeah.
[02:02:31.560 --> 02:02:34.680]   I mean, here's the thing that I think is a good example of how it would work, though,
[02:02:34.680 --> 02:02:39.000]   is, you know, an iPhone as awkward as it is to hold an iPhone up for augmented reality,
[02:02:39.000 --> 02:02:43.040]   they can work incredibly well and for things like translation, right?
[02:02:43.040 --> 02:02:44.480]   That's the perfect use case.
[02:02:44.480 --> 02:02:49.240]   You hold it up and Google Translate and other apps will do that live translation thing on
[02:02:49.240 --> 02:02:52.400]   signage or subtitles or whatever.
[02:02:52.400 --> 02:02:53.400]   And you-
[02:02:53.400 --> 02:02:54.400]   Yeah.
[02:02:54.400 --> 02:02:55.400]   That's great.
[02:02:55.400 --> 02:02:58.000]   I mean, we're not always traveling or offering live captioning.
[02:02:58.000 --> 02:03:03.640]   If you had AI, you know, if you had Siri, live caption, Skype, whatever system Google-
[02:03:03.640 --> 02:03:06.840]   But nobody wants to hold up a phone all the time.
[02:03:06.840 --> 02:03:07.840]   Right.
[02:03:07.840 --> 02:03:11.080]   And then you see personalizes you from like, okay, I'm staring Glenn, I'm talking to you,
[02:03:11.080 --> 02:03:12.080]   but I'm looking at my phone.
[02:03:12.080 --> 02:03:13.080]   Well, I'm talking to you.
[02:03:13.080 --> 02:03:17.760]   It's a worthwhile use case, but the form factor's wrong in most cases.
[02:03:17.760 --> 02:03:18.760]   Right.
[02:03:18.760 --> 02:03:19.760]   Right.
[02:03:19.760 --> 02:03:20.760]   Right.
[02:03:20.760 --> 02:03:23.080]   Let's take a break.
[02:03:23.080 --> 02:03:24.160]   More to come.
[02:03:24.160 --> 02:03:26.400]   Our fabulous panel.
[02:03:26.400 --> 02:03:28.680]   Our show today brought to you by Zapier.
[02:03:28.680 --> 02:03:30.000]   I use Zapier all the time.
[02:03:30.000 --> 02:03:32.840]   In fact, it's how we produce the shows.
[02:03:32.840 --> 02:03:35.560]   Zapier is automation done, right?
[02:03:35.560 --> 02:03:39.120]   If you're trying to grow a business, you know, your time is precious.
[02:03:39.120 --> 02:03:44.760]   What if you could streamline the boring stuff, the routine operations that eat up your time?
[02:03:44.760 --> 02:03:48.760]   Things like lead management, employee onboarding, customer support.
[02:03:48.760 --> 02:03:49.760]   That's what Zapier does.
[02:03:49.760 --> 02:03:51.160]   This was awesome about Zapier.
[02:03:51.160 --> 02:03:57.560]   It makes it easy to connect all the apps you use to automate routine tasks, to streamline
[02:03:57.560 --> 02:03:59.040]   your processes.
[02:03:59.040 --> 02:04:04.000]   So you got more time to do the stuff you're good at, customer, client needs, that kind
[02:04:04.000 --> 02:04:05.000]   of thing.
[02:04:05.000 --> 02:04:08.080]   The power of automation made possible for everyone.
[02:04:08.080 --> 02:04:14.560]   For instance, when I am going through my news feeds, my RSS feeds, I have a Zapier script.
[02:04:14.560 --> 02:04:21.200]   If I click a link or a star of news feed, it automatically zaps it.
[02:04:21.200 --> 02:04:23.400]   It puts it up on my Twitch social feed.
[02:04:23.400 --> 02:04:24.920]   I have a news feed there.
[02:04:24.920 --> 02:04:31.360]   It puts it onto pinboard and it even adds it to a spreadsheet called Leo's links that
[02:04:31.360 --> 02:04:34.080]   can easily be moved into a rundown spreadsheet.
[02:04:34.080 --> 02:04:35.080]   It all does that.
[02:04:35.080 --> 02:04:36.080]   I don't even have to think about it.
[02:04:36.080 --> 02:04:41.280]   I set this Zap up years ago and we've been using it ever since as a big part of our production.
[02:04:41.280 --> 02:04:44.600]   It saves me so much time.
[02:04:44.600 --> 02:04:49.280]   If I thought about it, I figured it's probably saving me hours every week, hundreds of hours
[02:04:49.280 --> 02:04:51.120]   every year.
[02:04:51.120 --> 02:04:52.120]   That's kind of mind-boggling.
[02:04:52.120 --> 02:04:53.760]   It was so easy to set up.
[02:04:53.760 --> 02:04:58.320]   I use Zapier for all kinds of automation.
[02:04:58.320 --> 02:05:02.440]   If you're in business, what a great way to get started with business automation.
[02:05:02.440 --> 02:05:03.920]   This is not low code.
[02:05:03.920 --> 02:05:06.520]   This is no code, zero coding.
[02:05:06.520 --> 02:05:11.360]   There are 4,000 apps that Zapier connects with.
[02:05:11.360 --> 02:05:15.880]   The most popular business apps out there, Google Sheets.
[02:05:15.880 --> 02:05:18.320]   I mentioned that we use that QuickBooks Facebook.
[02:05:18.320 --> 02:05:22.800]   You have Google Ads, any RSS feed.
[02:05:22.800 --> 02:05:27.360]   You can automate almost any workflow imaginable with easy to use templates.
[02:05:27.360 --> 02:05:28.640]   Thousands of them right on the site.
[02:05:28.640 --> 02:05:31.720]   You don't even have to write your own a lot of the times.
[02:05:31.720 --> 02:05:34.320]   Once you get good at it, the sky's the limit.
[02:05:34.320 --> 02:05:39.640]   The average user saves over $10,000 in recovered time every year.
[02:05:39.640 --> 02:05:41.080]   I guess that's accurate.
[02:05:41.080 --> 02:05:49.520]   If it's 200 hours a week a month for me, that's 1,000 hours a year or more.
[02:05:49.520 --> 02:05:52.520]   Yeah, I get paid more than $10 an hour.
[02:05:52.520 --> 02:05:59.760]   No wonder over 1.8 million people and businesses use Zapier to streamline their work and for
[02:05:59.760 --> 02:06:01.320]   more time for what matters most.
[02:06:01.320 --> 02:06:06.240]   I've been a Zapier subscriber for, I think, as long as they've been around for years and
[02:06:06.240 --> 02:06:07.240]   years.
[02:06:07.240 --> 02:06:11.160]   See for yourself why teams at Airtable and Dropbox and HubSpot and Zendesk.
[02:06:11.160 --> 02:06:16.600]   Thousands of other companies, including Twit, use Zapier every day to automate their businesses.
[02:06:16.600 --> 02:06:18.160]   Try Zapier free today.
[02:06:18.160 --> 02:06:20.200]   Z-A-P-I-E-R Zapier.
[02:06:20.200 --> 02:06:22.560]   I love the name.
[02:06:22.560 --> 02:06:24.440]   They call the program Zaps.
[02:06:24.440 --> 02:06:27.920]   Zapier.com/Twit.
[02:06:27.920 --> 02:06:30.120]   Thank you, Zapier, for supporting the show.
[02:06:30.120 --> 02:06:34.560]   I mean, literally, streamlining my workflow.
[02:06:34.560 --> 02:06:35.920]   Let them streamline yours.
[02:06:35.920 --> 02:06:37.560]   Try it free.
[02:06:37.560 --> 02:06:38.560]   Zapier.com/Twit.
[02:06:38.560 --> 02:06:41.040]   Don't forget the /Twit.
[02:06:41.040 --> 02:06:42.040]   That's very important.
[02:06:42.040 --> 02:06:46.960]   They know you sign here.
[02:06:46.960 --> 02:06:54.800]   We were talking before the show about Adam Newman, the guy who started WeWork and then
[02:06:54.800 --> 02:06:55.800]   drove it to the ground.
[02:06:55.800 --> 02:06:57.440]   But walked away with a lot of money.
[02:06:57.440 --> 02:07:01.160]   He got bought out by SoftBank.
[02:07:01.160 --> 02:07:02.160]   Now he's buying...
[02:07:02.160 --> 02:07:04.280]   The definition of failing up.
[02:07:04.280 --> 02:07:05.440]   Failing up is right.
[02:07:05.440 --> 02:07:06.440]   He's now buying...
[02:07:06.440 --> 02:07:08.200]   Now getting more money.
[02:07:08.200 --> 02:07:09.200]   Apartments.
[02:07:09.200 --> 02:07:10.200]   Yeah.
[02:07:10.200 --> 02:07:11.200]   And there's...
[02:07:11.200 --> 02:07:15.840]   And Driessen Horowitz just gave him $350 million.
[02:07:15.840 --> 02:07:17.040]   But that's not the news story.
[02:07:17.040 --> 02:07:25.480]   The news story is my friend Kevin Rose getting $50 million for his moonbirds.
[02:07:25.480 --> 02:07:26.480]   And FTS.
[02:07:26.480 --> 02:07:30.400]   A $50 million funding round for the proof collective.
[02:07:30.400 --> 02:07:31.960]   Now I have to...
[02:07:31.960 --> 02:07:32.960]   Wait out.
[02:07:32.960 --> 02:07:38.840]   Kevin actually already made $50 million selling these moonbirds as NFTs.
[02:07:38.840 --> 02:07:43.040]   They made so much money that he had to make a YouTube video saying we're going to do
[02:07:43.040 --> 02:07:45.160]   something really good with this money.
[02:07:45.160 --> 02:07:48.440]   That's always a sign.
[02:07:48.440 --> 02:07:49.960]   Yeah.
[02:07:49.960 --> 02:07:51.440]   Yeah.
[02:07:51.440 --> 02:07:55.040]   So I guess the rich get richer.
[02:07:55.040 --> 02:07:59.920]   And I love Kevin so I'm glad he's getting the money.
[02:07:59.920 --> 02:08:08.000]   The proof collective will get $50 million not only from A16Z but also from Alexa.
[02:08:08.000 --> 02:08:11.760]   So Haines VC from 776.
[02:08:11.760 --> 02:08:13.800]   What's with the numbers on these things?
[02:08:13.800 --> 02:08:19.680]   I had a $10 million funding round in April.
[02:08:19.680 --> 02:08:21.680]   And great.
[02:08:21.680 --> 02:08:23.920]   Here's my pitch.
[02:08:23.920 --> 02:08:29.680]   I think everybody instead of investing all this money at NFTs should give me their $50
[02:08:29.680 --> 02:08:30.680]   million.
[02:08:30.680 --> 02:08:31.680]   Yes, sure.
[02:08:31.680 --> 02:08:33.680]   And I in return will give everybody...
[02:08:33.680 --> 02:08:34.680]   Everybody...
[02:08:34.680 --> 02:08:35.680]   Okay.
[02:08:35.680 --> 02:08:44.120]   Let's say instead of your NFTs you can get a very bespoke, collectable once in a lifetime
[02:08:44.120 --> 02:08:48.680]   opportunity to own a piece of a mannequin but there's only one of in the world.
[02:08:48.680 --> 02:08:53.440]   And it's encoded in this blockchain called reality which Albeit has been copyrighted
[02:08:53.440 --> 02:08:55.720]   by Apple so we may have to work around that.
[02:08:55.720 --> 02:08:56.720]   Good point.
[02:08:56.720 --> 02:09:01.120]   But there's only one of them so you know it's going to be a real collectors object.
[02:09:01.120 --> 02:09:03.720]   I'd just want to say if you want to buy one art...
[02:09:03.720 --> 02:09:04.720]   Go ahead.
[02:09:04.720 --> 02:09:05.720]   Go ahead Glenn.
[02:09:05.720 --> 02:09:07.200]   What do you want to buy?
[02:09:07.200 --> 02:09:09.040]   I would like to buy one art please.
[02:09:09.040 --> 02:09:13.320]   Please give me our huge future armor reference for those who have.
[02:09:13.320 --> 02:09:14.320]   How about you?
[02:09:14.320 --> 02:09:15.320]   What do you think Dan?
[02:09:15.320 --> 02:09:18.520]   I think $50 million buys a lot of copies of my book.
[02:09:18.520 --> 02:09:20.120]   So we'll just want to do that.
[02:09:20.120 --> 02:09:21.120]   That's good.
[02:09:21.120 --> 02:09:22.120]   Best, very best sold.
[02:09:22.120 --> 02:09:24.560]   How about the four of us split 50 mail?
[02:09:24.560 --> 02:09:25.560]   We call it even.
[02:09:25.560 --> 02:09:26.560]   Yeah, there we go.
[02:09:26.560 --> 02:09:27.560]   I think that's good.
[02:09:27.560 --> 02:09:28.560]   I think you know.
[02:09:28.560 --> 02:09:29.560]   I agree.
[02:09:29.560 --> 02:09:30.560]   Yeah.
[02:09:30.560 --> 02:09:31.560]   I think...
[02:09:31.560 --> 02:09:34.560]   NFTs are the biggest hammer that's ever been developed in technology with no nails.
[02:09:34.560 --> 02:09:36.720]   There are no nails.
[02:09:36.720 --> 02:09:38.360]   I think almost blockchain is that.
[02:09:38.360 --> 02:09:43.960]   I know everybody says oh the technology is really interesting but it's just a distributed
[02:09:43.960 --> 02:09:44.960]   database.
[02:09:44.960 --> 02:09:45.960]   That's really what it is.
[02:09:45.960 --> 02:09:46.960]   It's a lot.
[02:09:46.960 --> 02:09:48.960]   So many people are screaming at their phones.
[02:09:48.960 --> 02:09:50.960]   They're throwing them across the wall.
[02:09:50.960 --> 02:09:51.960]   You've got to stop this.
[02:09:51.960 --> 02:09:56.240]   I can see a purpose for a blockchain.
[02:09:56.240 --> 02:09:57.200]   And I think we will have them.
[02:09:57.200 --> 02:10:00.120]   I think we will have a purely digital...
[02:10:00.120 --> 02:10:06.960]   Sorry, I think we have a version of fiat currency, a government-backed currency.
[02:10:06.960 --> 02:10:10.200]   China is already working on the digital renminbi and we're going to have a version of it in
[02:10:10.200 --> 02:10:15.480]   America and there'll be a version of the EU and the euro and so forth that uses blockchain
[02:10:15.480 --> 02:10:16.480]   technology.
[02:10:16.480 --> 02:10:19.560]   And it may be an incredibly bad idea but I think it will happen and I think it will
[02:10:19.560 --> 02:10:20.560]   be...
[02:10:20.560 --> 02:10:21.760]   Why does blockchain make it better?
[02:10:21.760 --> 02:10:23.360]   What makes that better?
[02:10:23.360 --> 02:10:32.240]   There is a utility of having an irritable, irreversible, permanent, cryptographically
[02:10:32.240 --> 02:10:38.520]   verifiable record of things but the number of cases in which that's useful compared to
[02:10:38.520 --> 02:10:41.320]   a database are very small.
[02:10:41.320 --> 02:10:42.320]   And there's a high price.
[02:10:42.320 --> 02:10:43.320]   Yes.
[02:10:43.320 --> 02:10:44.320]   I mean let's face it.
[02:10:44.320 --> 02:10:50.360]   It's a very significant price in terms of transaction costs, transaction time and energy
[02:10:50.360 --> 02:10:52.920]   usage to doing it this way.
[02:10:52.920 --> 02:10:58.080]   So you better damn well have a good reason for it other than blockchain.
[02:10:58.080 --> 02:11:00.840]   But those monkeys are real cool looking.
[02:11:00.840 --> 02:11:01.840]   I think that's a great thing.
[02:11:01.840 --> 02:11:08.600]   Proof is also creating a Moonbirds Dow which will oversee licensing of the Moonbirds name
[02:11:08.600 --> 02:11:13.960]   by granting trademark rights and deploying capital projects that "further the Moonbirds
[02:11:13.960 --> 02:11:16.240]   mission" I'm glad they have a mission.
[02:11:16.240 --> 02:11:21.680]   The Dow will control a soon to be formed Dow Treasury.
[02:11:21.680 --> 02:11:22.680]   Quick question.
[02:11:22.680 --> 02:11:24.520]   Are we sending these birds to the Moon?
[02:11:24.520 --> 02:11:25.920]   Did the birds come from the Moon?
[02:11:25.920 --> 02:11:26.920]   Are they already there?
[02:11:26.920 --> 02:11:27.920]   The relationship with the Moonbirds.
[02:11:27.920 --> 02:11:28.920]   I'm not clear on this.
[02:11:28.920 --> 02:11:29.920]   I don't know what their mission is.
[02:11:29.920 --> 02:11:32.680]   Why is the Moon like an aspirational goal?
[02:11:32.680 --> 02:11:34.720]   Like the birds to the Moon spiritually?
[02:11:34.720 --> 02:11:35.720]   They're one.
[02:11:35.720 --> 02:11:36.720]   I love that they're owls.
[02:11:36.720 --> 02:11:37.720]   They're owls.
[02:11:37.720 --> 02:11:38.720]   Birds to the Moon.
[02:11:38.720 --> 02:11:39.720]   They're owls.
[02:11:39.720 --> 02:11:40.720]   That's all they do is owls.
[02:11:40.720 --> 02:11:41.720]   Monkey.
[02:11:41.720 --> 02:11:42.720]   Wait, they're owls?
[02:11:42.720 --> 02:11:43.720]   They're owls.
[02:11:43.720 --> 02:11:45.800]   Monkeys were taken.
[02:11:45.800 --> 02:11:47.960]   So they're owls.
[02:11:47.960 --> 02:11:50.040]   The only Dow I invest in is Georgia Dow.
[02:11:50.040 --> 02:11:51.040]   Yeah, I like Georgia.
[02:11:51.040 --> 02:11:53.040]   The Dow stands for a decentralized...
[02:11:53.040 --> 02:11:57.440]   The only Dow I invest in is Ann Dowd, you know, great character actors.
[02:11:57.440 --> 02:12:01.240]   The Dow stands for decentralized autonomous organ...
[02:12:01.240 --> 02:12:02.240]   No, what is it?
[02:12:02.240 --> 02:12:03.240]   Now I've forgotten.
[02:12:03.240 --> 02:12:04.240]   Women.
[02:12:04.240 --> 02:12:05.240]   Decentralized autonomous organization.
[02:12:05.240 --> 02:12:06.240]   Yeah.
[02:12:06.240 --> 02:12:09.000]   It's also kind of a cool idea.
[02:12:09.000 --> 02:12:11.520]   A lot of these things are cool ideas.
[02:12:11.520 --> 02:12:14.320]   And the people that implement them make me want to run away into the hills.
[02:12:14.320 --> 02:12:15.920]   They're cool ideas for you forever.
[02:12:15.920 --> 02:12:20.920]   Taking your money out of your wallet and putting it in mine.
[02:12:20.920 --> 02:12:21.920]   Proof is also...
[02:12:21.920 --> 02:12:24.560]   At least for a little bit until it gets hacked and then put in songs.
[02:12:24.560 --> 02:12:25.560]   That's true.
[02:12:25.560 --> 02:12:26.560]   That's a good point.
[02:12:26.560 --> 02:12:27.560]   I feel like it's all implementation though.
[02:12:27.560 --> 02:12:28.560]   I mean, we've...
[02:12:28.560 --> 02:12:29.560]   Go ahead.
[02:12:29.560 --> 02:12:31.960]   Oh, I was going to say something completely irrelevant.
[02:12:31.960 --> 02:12:32.960]   Please continue.
[02:12:32.960 --> 02:12:35.400]   We've all talked about...
[02:12:35.400 --> 02:12:36.640]   Web 3 is going great.
[02:12:36.640 --> 02:12:37.640]   You know, my favorite site.
[02:12:37.640 --> 02:12:38.640]   Wonderful site.
[02:12:38.640 --> 02:12:39.640]   A wonderful site.
[02:12:39.640 --> 02:12:40.640]   A wonderful site.
[02:12:40.640 --> 02:12:46.440]   It's the kazoo playing at the ongoing funeral that people refuse to accept is a funeral for
[02:12:46.440 --> 02:12:49.400]   cryptocurrency and dows and NFTs.
[02:12:49.400 --> 02:12:53.680]   But I think there's a fundamental problem.
[02:12:53.680 --> 02:12:55.400]   The technology is bad.
[02:12:55.400 --> 02:13:01.440]   Everyone involved in it immediately went into the sort of grifting and churn and hype
[02:13:01.440 --> 02:13:02.440]   mode.
[02:13:02.440 --> 02:13:04.440]   So there's no...
[02:13:04.440 --> 02:13:05.440]   The entire...
[02:13:05.440 --> 02:13:06.920]   You can't take anything that's going on right now.
[02:13:06.920 --> 02:13:11.640]   I think it builds something meaningful because there is so much bad that's happening.
[02:13:11.640 --> 02:13:13.280]   Here's an example.
[02:13:13.280 --> 02:13:19.040]   Bill Murray auctioned off an NFT representing the right to drink a beer with him during
[02:13:19.040 --> 02:13:22.680]   which a painter will paint a picture of the scene that the buyer can keep.
[02:13:22.680 --> 02:13:28.440]   The auction it benefits charity, chive charities, veterans, and first responder focused on profit.
[02:13:28.440 --> 02:13:31.680]   The NFT sold for 119.2 ETH.
[02:13:31.680 --> 02:13:34.160]   That's $185,000.
[02:13:34.160 --> 02:13:39.440]   However, take hours after the auction a hacker gained access to Murray's crypto wallet and
[02:13:39.440 --> 02:13:41.880]   snagged the ETH for themselves.
[02:13:41.880 --> 02:13:46.640]   They also attempted to steal 800 NFTs from the remaining collection by Bill Murray, although
[02:13:46.640 --> 02:13:52.160]   a wallet security team was able to safeguard those NFTs in time.
[02:13:52.160 --> 02:13:55.440]   What I love about this is everybody compares these things to gold rushes.
[02:13:55.440 --> 02:13:57.600]   But what I love about this is the grift rush, right?
[02:13:57.600 --> 02:14:00.560]   It's just everybody constantly stealing from each other.
[02:14:00.560 --> 02:14:02.640]   It's kind of hilarious from the outside.
[02:14:02.640 --> 02:14:04.160]   That's the key.
[02:14:04.160 --> 02:14:05.760]   Stay on the outside, baby.
[02:14:05.760 --> 02:14:09.800]   We're speaking of which the one person who's doing quite well is, I don't know if you guys
[02:14:09.800 --> 02:14:17.320]   saw the story that Crypto.com accidentally sent a woman $10 million in fiat instead
[02:14:17.320 --> 02:14:18.320]   of $100,000.
[02:14:18.320 --> 02:14:21.240]   They didn't know this for months.
[02:14:21.240 --> 02:14:27.440]   She bought a mansion because if I was to accidentally get $10,000,000 and not have anybody
[02:14:27.440 --> 02:14:30.480]   follow up for months, I would have.
[02:14:30.480 --> 02:14:33.160]   They can't get her in your favor as the old card says.
[02:14:33.160 --> 02:14:37.640]   They sent a woman asked for $100 Australian refund.
[02:14:37.640 --> 02:14:43.760]   They sent her $10.5 million instead because instead of entering the amount of the refund,
[02:14:43.760 --> 02:14:49.000]   somebody working at Crypto.com accidentally entered an account number into the refund amount
[02:14:49.000 --> 02:14:55.880]   section, which turned out that account number was effectively $10.5 million.
[02:14:55.880 --> 02:14:59.320]   Now I have to say she's a little bit of fault here.
[02:14:59.320 --> 02:15:02.080]   She didn't say, "Hey, you gave me $10 million.
[02:15:02.080 --> 02:15:03.360]   I only wanted $100."
[02:15:03.360 --> 02:15:07.200]   She put the money into a joint account with her sister, bought her sister a five-bedroom
[02:15:07.200 --> 02:15:08.200]   house.
[02:15:08.200 --> 02:15:09.200]   That's sweet.
[02:15:09.200 --> 02:15:10.200]   Yeah, for sure.
[02:15:10.200 --> 02:15:11.200]   No, I don't know.
[02:15:11.200 --> 02:15:16.680]   Well, now it's a little litigation and Crypto.com has to prove that she did something wrong, which
[02:15:16.680 --> 02:15:18.840]   is kind of difficult to do.
[02:15:18.840 --> 02:15:22.080]   Yeah, actually it's an interesting point.
[02:15:22.080 --> 02:15:25.520]   This happened recently though in the mainstream financial market was last year.
[02:15:25.520 --> 02:15:28.680]   I was about to say with Citigroup, the Revlon thing.
[02:15:28.680 --> 02:15:33.880]   Yeah, somebody didn't check some compliance box and they sent, what was it, $400 million
[02:15:33.880 --> 02:15:43.720]   when Citigroup accidentally paid $900 million in debt to Revlon creditors, like their lenders.
[02:15:43.720 --> 02:15:46.320]   It was supposed to be just like, "Oh, a small thing."
[02:15:46.320 --> 02:15:50.200]   They accidentally paid the whole debt and they were like, "Oopsie, can we have that money
[02:15:50.200 --> 02:15:51.200]   back?"
[02:15:51.200 --> 02:15:52.200]   And they're like, "No."
[02:15:52.200 --> 02:15:53.200]   "You owed it to us."
[02:15:53.200 --> 02:15:59.200]   You got like $400 million back and the remaining companies that didn't want to return it went
[02:15:59.200 --> 02:16:03.200]   to court and the judge said, "Well, based on the doctrine of finders, keepers and no
[02:16:03.200 --> 02:16:04.200]   backsees."
[02:16:04.200 --> 02:16:05.200]   Really?
[02:16:05.200 --> 02:16:06.200]   I don't know.
[02:16:06.200 --> 02:16:07.200]   Really?
[02:16:07.200 --> 02:16:08.200]   That's a legal doctrine.
[02:16:08.200 --> 02:16:13.200]   It was a legal doctrine that, I mean, the judge analyzed the terms of the transfer.
[02:16:13.200 --> 02:16:17.800]   It didn't matter that it was an error because the creditors had every reason to believe
[02:16:17.800 --> 02:16:19.720]   it was a legitimate transaction.
[02:16:19.720 --> 02:16:20.720]   They were all transaction.
[02:16:20.720 --> 02:16:21.720]   Yeah.
[02:16:21.720 --> 02:16:23.200]   That's the money it was spent to them.
[02:16:23.200 --> 02:16:27.360]   So some, I think half the creditors did not agree to return it and it's a very funny
[02:16:27.360 --> 02:16:28.360]   long-term thing.
[02:16:28.360 --> 02:16:32.240]   It's always hilarious when you see just how fragile the underpinnings of our society
[02:16:32.240 --> 02:16:33.680]   actually is, right?
[02:16:33.680 --> 02:16:36.200]   Just one wrong checkbox and the whole thing comes crashing.
[02:16:36.200 --> 02:16:42.040]   Like this was two random dudes, I believe, like overseas whose job was to put some numbers
[02:16:42.040 --> 02:16:46.440]   in a box and like one of them put the wrong numbers and the other one didn't catch it.
[02:16:46.440 --> 02:16:47.440]   That's it.
[02:16:47.440 --> 02:16:48.440]   Almost a million dollars.
[02:16:48.440 --> 02:16:54.680]   Missed out on the original Moonbirds, there's an early 2023 Moonbird mythics.
[02:16:54.680 --> 02:16:55.880]   Oh, thank God.
[02:16:55.880 --> 02:17:03.840]   A profile picture collection of 20,000 NFTs with quote, "An eye toward giving back to the
[02:17:03.840 --> 02:17:07.720]   original Moonbirds and oddities collectors."
[02:17:07.720 --> 02:17:10.160]   I still don't know what any of those words mean.
[02:17:10.160 --> 02:17:11.160]   What is, yeah.
[02:17:11.160 --> 02:17:13.000]   Okay, what is the hearth of an odd guy?
[02:17:13.000 --> 02:17:16.440]   Okay, I'm just going to read this sentence, scroll up a little bit for a second because
[02:17:16.440 --> 02:17:17.440]   there was a really good one.
[02:17:17.440 --> 02:17:18.440]   Okay, here.
[02:17:18.440 --> 02:17:24.600]   "The hearth of the odd God, when an egg enters the hearth, it will hatch 24 hours later."
[02:17:24.600 --> 02:17:25.600]   What does that mean?
[02:17:25.600 --> 02:17:26.600]   You can hatch.
[02:17:26.600 --> 02:17:27.600]   That's the answer to the question.
[02:17:27.600 --> 02:17:28.600]   You can hatch.
[02:17:28.600 --> 02:17:29.600]   How do you get a mythic with an oddity?
[02:17:29.600 --> 02:17:31.120]   You can hatch it.
[02:17:31.120 --> 02:17:32.120]   You can hatch it.
[02:17:32.120 --> 02:17:35.320]   Only 25 oddities will be burned each day.
[02:17:35.320 --> 02:17:36.320]   Oh, sure.
[02:17:36.320 --> 02:17:37.320]   What?
[02:17:37.320 --> 02:17:38.320]   Sure.
[02:17:38.320 --> 02:17:39.320]   What?
[02:17:39.320 --> 02:17:40.320]   Someone started saying those words to me.
[02:17:40.320 --> 02:17:41.320]   I'd assume I was having a stroke.
[02:17:41.320 --> 02:17:46.840]   So, you know what breaks my heart was like the art insurance, the artist who paints these.
[02:17:46.840 --> 02:17:47.840]   Yeah.
[02:17:47.840 --> 02:17:51.480]   Same thing with the Yuga Labs board apes.
[02:17:51.480 --> 02:17:53.440]   They get paid like a flat fee.
[02:17:53.440 --> 02:17:58.240]   Yeah, they don't get any of this money from the actual trades.
[02:17:58.240 --> 02:18:02.160]   Yeah, but they also don't like to do exposure to so it feels like you walk away with at
[02:18:02.160 --> 02:18:04.720]   least a yacht paid and probably real money.
[02:18:04.720 --> 02:18:06.680]   Yeah, a couple hundred bucks.
[02:18:06.680 --> 02:18:07.680]   Yeah.
[02:18:07.680 --> 02:18:10.960]   Yeah, well, this is how I feel like my grandparents felt when I tried to explain the internet
[02:18:10.960 --> 02:18:11.960]   to them.
[02:18:11.960 --> 02:18:14.800]   You know, they're just like, there's no context for it.
[02:18:14.800 --> 02:18:15.800]   It doesn't make any sense.
[02:18:15.800 --> 02:18:21.320]   There's no, it's all abstract, but it's like, I mean, the part is of the internet, print
[02:18:21.320 --> 02:18:22.320]   out to be useful.
[02:18:22.320 --> 02:18:24.000]   No, that might be a mistake.
[02:18:24.000 --> 02:18:27.760]   Maybe I was wrong on that part where NFTs are definitely not, but maybe maybe the jury
[02:18:27.760 --> 02:18:28.760]   still hasn't even.
[02:18:28.760 --> 02:18:36.600]   I'm sure if I got Kevin on, he would have a explanation for why this is good and valuable
[02:18:36.600 --> 02:18:40.480]   and true and sure and all that.
[02:18:40.480 --> 02:18:43.160]   It's unlicensed securities trading.
[02:18:43.160 --> 02:18:44.160]   Yes.
[02:18:44.160 --> 02:18:46.040]   You're afraid what the SEC is going to decide.
[02:18:46.040 --> 02:18:48.440]   Without any of the rules or regulations.
[02:18:48.440 --> 02:18:49.440]   What are you telling me?
[02:18:49.440 --> 02:18:54.280]   And I guess if you're really into that and making money, that's good for you.
[02:18:54.280 --> 02:18:55.280]   Good for making money.
[02:18:55.280 --> 02:18:58.640]   But I don't think that it's an unnet good to the world.
[02:18:58.640 --> 02:19:02.640]   It's the multi-billion dollar version of the two kids I saw today on the bike path near
[02:19:02.640 --> 02:19:05.840]   my house selling Pokemon cards at their table.
[02:19:05.840 --> 02:19:10.280]   I think that's like basically future NFT crypto traders.
[02:19:10.280 --> 02:19:12.880]   Yeah, you're just going to walk up to those because you're like, so if you heard about
[02:19:12.880 --> 02:19:17.160]   the blockchain, it's going to get out now.
[02:19:17.160 --> 02:19:18.160]   Get out now.
[02:19:18.160 --> 02:19:19.160]   It's too.
[02:19:19.160 --> 02:19:21.160]   They'll call their parents immediately.
[02:19:21.160 --> 02:19:22.960]   Somebody in the chat, I'm saying, "Well, bring Kevin on."
[02:19:22.960 --> 02:19:23.960]   Nope.
[02:19:23.960 --> 02:19:26.920]   I'm not going to do it.
[02:19:26.920 --> 02:19:32.320]   I've already told staff and producers, and I'm sorry if you're all into this stuff.
[02:19:32.320 --> 02:19:37.840]   Nobody who's going to come on, any of our shows who's going to flog NFTs, flog current,
[02:19:37.840 --> 02:19:43.280]   cryptocurrency, I'm washing my hands of it because I don't want to be responsible if
[02:19:43.280 --> 02:19:46.920]   you go out and bet your rent money on an NFT thinking you're going to make all this
[02:19:46.920 --> 02:19:50.560]   money and it just lines the pocket of somebody who's already got it.
[02:19:50.560 --> 02:19:52.600]   But, Leo, what about an algorithm stablecoin?
[02:19:52.600 --> 02:19:53.600]   It's got the word stable in it.
[02:19:53.600 --> 02:19:54.600]   It's stable.
[02:19:54.600 --> 02:19:55.600]   It's got to be good unless it's real absolute.
[02:19:55.600 --> 02:19:58.600]   I mean, there's nothing that's ever gone wrong with those before.
[02:19:58.600 --> 02:19:59.840]   Oh, Lord.
[02:19:59.840 --> 02:20:01.560]   I like Kevin a lot.
[02:20:01.560 --> 02:20:06.360]   I can't say I endorse what he's doing with this.
[02:20:06.360 --> 02:20:07.360]   You know what?
[02:20:07.360 --> 02:20:08.840]   They're going to say, "Well, that's just because you're old and you don't understand
[02:20:08.840 --> 02:20:09.840]   it.
[02:20:09.840 --> 02:20:11.840]   You're like Larry David.
[02:20:11.840 --> 02:20:12.840]   You're a boom.
[02:20:12.840 --> 02:20:13.840]   You can't even find your keys.
[02:20:13.840 --> 02:20:14.840]   Okay.
[02:20:14.840 --> 02:20:16.840]   I can't even find my keys.
[02:20:16.840 --> 02:20:18.840]   So what do I know?
[02:20:18.840 --> 02:20:20.840]   I'm hitting the bloop bloop and the button doesn't go.
[02:20:20.840 --> 02:20:21.840]   Does it play a sound?
[02:20:21.840 --> 02:20:22.840]   What's going on with that?
[02:20:22.840 --> 02:20:26.040]   His garage door is opening clothes about six, seven times.
[02:20:26.040 --> 02:20:27.680]   That's working great.
[02:20:27.680 --> 02:20:35.120]   I'm logged out of Facebook suddenly.
[02:20:35.120 --> 02:20:42.440]   I'm laughing because it hurts because actually I am that old, but I do a radio show where
[02:20:42.440 --> 02:20:48.800]   people are actually much older than me call in and exactly those questions and I have
[02:20:48.800 --> 02:20:49.800]   to go, "Hey."
[02:20:49.800 --> 02:20:53.440]   Oh, I have a great account to follow.
[02:20:53.440 --> 02:20:59.880]   A Jessamine West who is at Jessamine, that's M-Y-N, who's a librarian and a futurist and
[02:20:59.880 --> 02:21:03.320]   a great technology thinker, but she's a librarian in Vermont.
[02:21:03.320 --> 02:21:07.760]   And one of the greatest people who I think writes and thinks about the future of what
[02:21:07.760 --> 02:21:11.360]   information online is, but also is just like a librarian.
[02:21:11.360 --> 02:21:14.920]   And she has this regular thing where she, seniors come in and people of any age, but
[02:21:14.920 --> 02:21:18.880]   often seniors come in and she helps them with technology and she tweets it out and with
[02:21:18.880 --> 02:21:20.240]   absolute love, right?
[02:21:20.240 --> 02:21:22.040]   And it's fascinating to see.
[02:21:22.040 --> 02:21:23.200]   I love this insight.
[02:21:23.200 --> 02:21:27.000]   This is why I love running the Mac 911 column at Macworld also.
[02:21:27.000 --> 02:21:28.000]   Same thing.
[02:21:28.000 --> 02:21:29.000]   Yeah.
[02:21:29.000 --> 02:21:30.000]   You're doing the same thing.
[02:21:30.000 --> 02:21:31.000]   You're doing God's work.
[02:21:31.000 --> 02:21:32.000]   Yeah.
[02:21:32.000 --> 02:21:33.000]   You're doing people.
[02:21:33.000 --> 02:21:34.000]   What is the friction?
[02:21:34.000 --> 02:21:35.400]   What doesn't make sense to them?
[02:21:35.400 --> 02:21:39.800]   And if I can solve that, I feel great, but I also, I want to understand that people conceptualize
[02:21:39.800 --> 02:21:40.800]   technology.
[02:21:40.800 --> 02:21:46.360]   And so she's seeing, you know, people who have not encountered or are using it or functionally
[02:21:46.360 --> 02:21:49.760]   almost illiterate in technology, but they have to use it every day and they come to
[02:21:49.760 --> 02:21:51.760]   her a librarian for help.
[02:21:51.760 --> 02:21:52.760]   I love it.
[02:21:52.760 --> 02:21:53.760]   Just live in Seattle.
[02:21:53.760 --> 02:22:01.120]   She calls herself the rural tech geek, which is easier than saying the rural juror.
[02:22:01.120 --> 02:22:03.120]   Jessamine J E S S A.
[02:22:03.120 --> 02:22:04.640]   I technically met a filter.
[02:22:04.640 --> 02:22:05.640]   That's right.
[02:22:05.640 --> 02:22:09.240]   She also was a lot for many years was one of the main people below Matt Howie.
[02:22:09.240 --> 02:22:10.240]   Oh, I love that.
[02:22:10.240 --> 02:22:11.240]   And I love meta filters.
[02:22:11.240 --> 02:22:12.240]   All right.
[02:22:12.240 --> 02:22:13.240]   Well, that's great.
[02:22:13.240 --> 02:22:15.720]   She was a moderator or whatever they call, I don't think they call moderators, whatever
[02:22:15.720 --> 02:22:17.080]   the name was there.
[02:22:17.080 --> 02:22:19.680]   That was I still subscribe to meta filter.
[02:22:19.680 --> 02:22:20.680]   It's still active.
[02:22:20.680 --> 02:22:21.680]   Very lovely.
[02:22:21.680 --> 02:22:22.680]   Yes.
[02:22:22.680 --> 02:22:23.680]   They did a great job building a community.
[02:22:23.680 --> 02:22:27.240]   Boy, we've now mentioned waxy links and meta filter.
[02:22:27.240 --> 02:22:28.240]   We're in.
[02:22:28.240 --> 02:22:29.240]   Tell us.
[02:22:29.240 --> 02:22:30.240]   Wait, we are old.
[02:22:30.240 --> 02:22:32.240]   I've been right about this in my live journal.
[02:22:32.240 --> 02:22:33.240]   So everything sucks.
[02:22:33.240 --> 02:22:34.240]   Come.
[02:22:34.240 --> 02:22:35.240]   I don't know.
[02:22:35.240 --> 02:22:37.600]   Oh, I miss such a time.
[02:22:37.600 --> 02:22:38.600]   I'm dig.
[02:22:38.600 --> 02:22:39.600]   It's never going to change.
[02:22:39.600 --> 02:22:40.600]   Yeah, that's right.
[02:22:40.600 --> 02:22:41.600]   That's right.
[02:22:41.600 --> 02:22:42.600]   That's right.
[02:22:42.600 --> 02:22:43.600]   Speaking of Kevin Rose.
[02:22:43.600 --> 02:22:44.600]   Yeah, we kind of got dig in there as well.
[02:22:44.600 --> 02:22:46.360]   Yeah, it's all comes around.
[02:22:46.360 --> 02:22:47.360]   That's funny.
[02:22:47.360 --> 02:22:50.760]   And then meanwhile, these youngsters, these are your person average have announced USB
[02:22:50.760 --> 02:22:51.760]   four version 2.0.
[02:22:51.760 --> 02:22:54.560]   Oh, I got things to say about that.
[02:22:54.560 --> 02:22:55.560]   I knew.
[02:22:55.560 --> 02:22:56.560]   I knew Glenn.
[02:22:56.560 --> 02:22:57.560]   I knew it.
[02:22:57.560 --> 02:23:00.400]   It was a boom all too many types.
[02:23:00.400 --> 02:23:02.040]   We just got a quick one.
[02:23:02.040 --> 02:23:06.840]   I got a back channel to somebody who's involved, these discussions who is very unhappy because
[02:23:06.840 --> 02:23:08.480]   of people making fun of this name.
[02:23:08.480 --> 02:23:14.400]   But simultaneously also told me that this is an internal product spec version release
[02:23:14.400 --> 02:23:15.400]   number.
[02:23:15.400 --> 02:23:20.800]   And at some level, the people, the USB implementers for on the USB IF that manages the spec.
[02:23:20.800 --> 02:23:22.040]   They have these two parts.
[02:23:22.040 --> 02:23:26.440]   One is kind of a public casing part and one is a developer and like manufacturer facing
[02:23:26.440 --> 02:23:27.440]   part.
[02:23:27.440 --> 02:23:28.440]   And the customer's like, oh, God, they're giving us new numbers.
[02:23:28.440 --> 02:23:32.440]   Is the U is going to be a trapezoidal connection again or a dodecuhedron?
[02:23:32.440 --> 02:23:33.440]   What am I going to do?
[02:23:33.440 --> 02:23:34.440]   I need to get new cords.
[02:23:34.440 --> 02:23:39.440]   So he's like, he's like, look, when this actually shipped, it's going to be called USB
[02:23:39.440 --> 02:23:40.440]   four.
[02:23:40.440 --> 02:23:56.960]   And what it's going to say is 80 gigabits per second and it'll be labeled and you'll know
[02:23:56.960 --> 02:23:59.160]   if your devices are and it'll all be.
[02:23:59.160 --> 02:24:00.720]   Oh, that's such a lie.
[02:24:00.720 --> 02:24:01.720]   You are going to be.
[02:24:01.720 --> 02:24:03.160]   No, that's such a lie.
[02:24:03.160 --> 02:24:04.320]   That's not true.
[02:24:04.320 --> 02:24:05.520]   That's so untrue.
[02:24:05.520 --> 02:24:06.520]   They always say this.
[02:24:06.520 --> 02:24:07.640]   Oh, don't worry.
[02:24:07.640 --> 02:24:09.360]   You get that type C cable.
[02:24:09.360 --> 02:24:13.680]   There'll be a s and a sigil on it that will tell you what it can do.
[02:24:13.680 --> 02:24:17.480]   And then on the port of your computer, there will be a little sigil that will say what
[02:24:17.480 --> 02:24:18.480]   it can do.
[02:24:18.480 --> 02:24:23.040]   And then you match the two to nobody puts any of those symbols on anything.
[02:24:23.040 --> 02:24:24.840]   You don't know what it does.
[02:24:24.840 --> 02:24:29.280]   The product will say when you buy it, we'll say every word that has ever been made.
[02:24:29.280 --> 02:24:30.840]   It's associated with technology.
[02:24:30.840 --> 02:24:33.840]   It'll be like cord monitor power speed.
[02:24:33.840 --> 02:24:37.960]   80 bits 200 fast extra high ultra speed.
[02:24:37.960 --> 02:24:39.960]   You got to do what version four point two.
[02:24:39.960 --> 02:24:43.160]   You match your schedules than you summon a day and take it.
[02:24:43.160 --> 02:24:44.160]   Exactly.
[02:24:44.160 --> 02:24:45.600]   Where the sigil there is a good.
[02:24:45.600 --> 02:24:47.600]   That's a good $64 word there.
[02:24:47.600 --> 02:24:48.920]   I think that's what they are.
[02:24:48.920 --> 02:24:50.640]   They seem to be burning with fire.
[02:24:50.640 --> 02:24:54.800]   As I said, I've written a book on every topic and I have a book called take control of untangle
[02:24:54.800 --> 02:24:55.800]   and connections.
[02:24:55.800 --> 02:24:56.800]   It isn't part about understanding.
[02:24:56.800 --> 02:25:01.040]   It's either about cords or about your loved ones.
[02:25:01.040 --> 02:25:02.040]   Or both.
[02:25:02.040 --> 02:25:06.360]   It's a narrative that runs through it.
[02:25:06.360 --> 02:25:07.800]   She was USB.
[02:25:07.800 --> 02:25:08.800]   He was thunderbolt.
[02:25:08.800 --> 02:25:09.800]   Never.
[02:25:09.800 --> 02:25:10.800]   Never.
[02:25:10.800 --> 02:25:11.800]   Yes.
[02:25:11.800 --> 02:25:15.840]   Then he got a sigil and plays it on his forehead and some of the end.
[02:25:15.840 --> 02:25:22.360]   No child of mine will ever marry a trapezoidal type B connector.
[02:25:22.360 --> 02:25:25.640]   This is the book you have to read if you want to get all the answers on how to connect USB
[02:25:25.640 --> 02:25:28.640]   Thunderbolt, ethernet, DisplayPort, HDMI and audio.
[02:25:28.640 --> 02:25:29.640]   Oh Lord.
[02:25:29.640 --> 02:25:32.120]   It was an incredible journey running this book.
[02:25:32.120 --> 02:25:36.760]   But I think the nice part and still the announcement of USB four version 2.0 was that
[02:25:36.760 --> 02:25:41.760]   it seemed like we have, if you have relatively recent devices and they have a USB C port on
[02:25:41.760 --> 02:25:47.160]   it, generally you plug in a thunderbolt four slash USB four cable.
[02:25:47.160 --> 02:25:49.960]   That's kind of the new universal cable and everything looks.
[02:25:49.960 --> 02:25:50.960]   It looks lost me.
[02:25:50.960 --> 02:25:51.960]   All right.
[02:25:51.960 --> 02:25:52.960]   Let me go back to the slasher.
[02:25:52.960 --> 02:25:55.200]   There's a slash on parathons keyboard.
[02:25:55.200 --> 02:25:56.200]   Does it work?
[02:25:56.200 --> 02:25:59.480]   I'm not going to say give a fucking tell me about thunderbolt.
[02:25:59.480 --> 02:26:01.360]   And I'm like, this is too much.
[02:26:01.360 --> 02:26:04.680]   Listen, I just want one.
[02:26:04.680 --> 02:26:05.680]   Thunder slash.
[02:26:05.680 --> 02:26:07.120]   You're digging a deeper hole.
[02:26:07.120 --> 02:26:09.760]   I don't think it's getting better.
[02:26:09.760 --> 02:26:14.960]   Whatever it is, we'll have 80 gigabits per second of bandwidth, which is, you know, about
[02:26:14.960 --> 02:26:16.840]   100 gigabits more than you'll ever need.
[02:26:16.840 --> 02:26:19.160]   But okay, good, frankly, fine.
[02:26:19.160 --> 02:26:23.960]   Whatever it is, it's going in the shoe box in the bottom of my drawer with all the other
[02:26:23.960 --> 02:26:24.960]   cables.
[02:26:24.960 --> 02:26:27.960]   You can use your old cables, but they won't be as fast.
[02:26:27.960 --> 02:26:31.000]   I did discover the reason you want 40 gigabit per second.
[02:26:31.000 --> 02:26:33.120]   And this is, it took because most things don't need it.
[02:26:33.120 --> 02:26:34.120]   You don't have a raid driver.
[02:26:34.120 --> 02:26:35.120]   Hard drive can't do it.
[02:26:35.120 --> 02:26:36.120]   Yeah.
[02:26:36.120 --> 02:26:37.120]   It's like a modern.
[02:26:37.120 --> 02:26:38.120]   Yeah.
[02:26:38.120 --> 02:26:42.040]   SSD is the most, the fastest modern SSDs can actually perform at rates where you're starting
[02:26:42.040 --> 02:26:46.520]   to hit above a limit of 20 gigabits per second, which was kind of the earlier high point for
[02:26:46.520 --> 02:26:50.600]   USB and Thunderbolt 2 in some forms, Thunderbolt 3.
[02:26:50.600 --> 02:26:53.280]   So all the current specs are like, okay, we do 40.
[02:26:53.280 --> 02:26:55.040]   If you're all up to date, everything does 40.
[02:26:55.040 --> 02:27:00.600]   And then the fastest SSD you can buy can perform at its highest internal bus rate.
[02:27:00.600 --> 02:27:02.400]   And that's kind of the, that's the bottom line.
[02:27:02.400 --> 02:27:06.680]   So they're doing 80 because there's a new generation of SSDs that will be out at some
[02:27:06.680 --> 02:27:10.680]   point and you'll want to be able to use those for video and whatever.
[02:27:10.680 --> 02:27:11.680]   People will use docs.
[02:27:11.680 --> 02:27:16.240]   I mean, I think this is the real reason with multiple connections sharing that 80 gig
[02:27:16.240 --> 02:27:17.240]   of bits that 80 gigabit.
[02:27:17.240 --> 02:27:22.720]   I've got a stupidly expensive dock sitting under my monitor and it pained me to buy it,
[02:27:22.720 --> 02:27:23.720]   but man, it's lovely.
[02:27:23.720 --> 02:27:24.720]   Yeah.
[02:27:24.720 --> 02:27:27.360]   Got all my cards on it or a review.
[02:27:27.360 --> 02:27:31.200]   I mean, because it's like a Mac one, it's not as many as I like.
[02:27:31.200 --> 02:27:32.200]   It's obsolete, by the way.
[02:27:32.200 --> 02:27:35.640]   Does it do Thunderbolt 4 3.2?
[02:27:35.640 --> 02:27:37.640]   It does do all the thunderbolts.
[02:27:37.640 --> 02:27:38.640]   I don't know.
[02:27:38.640 --> 02:27:39.640]   I've hated it.
[02:27:39.640 --> 02:27:42.600]   I was like, I just want one that has like six USBs.
[02:27:42.600 --> 02:27:44.280]   I have the Cal digit.
[02:27:44.280 --> 02:27:46.280]   I bet she has the Cal digit.
[02:27:46.280 --> 02:27:47.280]   The Cal digit.
[02:27:47.280 --> 02:27:48.280]   And the Thunderbolt.
[02:27:48.280 --> 02:27:49.280]   And the Internet.
[02:27:49.280 --> 02:27:50.280]   Yeah.
[02:27:50.280 --> 02:27:52.360]   It's like, the Cal digit one is like $300.
[02:27:52.360 --> 02:27:55.240]   But it has, I think literally 18 ports coming out of it.
[02:27:55.240 --> 02:27:57.280]   So you plug in the Cal digit.
[02:27:57.280 --> 02:27:58.560]   It's a great unit.
[02:27:58.560 --> 02:28:03.800]   It's kind of expensive, but it's like, it has every kind of USB Thunderbolt display
[02:28:03.800 --> 02:28:05.960]   port HDMI, multi-bolts.
[02:28:05.960 --> 02:28:09.880]   It's like, if you just want to buy a thing and it answers the need, it's the throw that
[02:28:09.880 --> 02:28:10.880]   thing.
[02:28:10.880 --> 02:28:11.880]   That's not it.
[02:28:11.880 --> 02:28:12.880]   That's a tiny one.
[02:28:12.880 --> 02:28:15.440]   It was, you couldn't get it for a long time.
[02:28:15.440 --> 02:28:16.440]   I waited.
[02:28:16.440 --> 02:28:17.440]   Yeah, it was out of forever to get.
[02:28:17.440 --> 02:28:21.040]   The thing is, I took a long time looking in this and I was like, I want one that has
[02:28:21.040 --> 02:28:25.920]   at least a cord that like plugs it into my computer that is at least more than three inches
[02:28:25.920 --> 02:28:27.920]   because I have my computer up on a monitor.
[02:28:27.920 --> 02:28:29.760]   Yeah, that is, I do have that.
[02:28:29.760 --> 02:28:30.760]   T.S.
[02:28:30.760 --> 02:28:33.760]   Plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus
[02:28:33.760 --> 02:28:36.760]   plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus
[02:28:36.760 --> 02:28:37.760]   plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus plus.
[02:28:37.760 --> 02:28:38.760]   It looks like it could be like a small like four.
[02:28:38.760 --> 02:28:39.760]   It kind of seems obscene.
[02:28:39.760 --> 02:28:43.720]   If you have that, what's the thing if you have the fear of holes?
[02:28:43.720 --> 02:28:44.720]   Not tricky.
[02:28:44.720 --> 02:28:45.720]   Yeah, yeah.
[02:28:45.720 --> 02:28:46.720]   I think it's a computer.
[02:28:46.720 --> 02:28:48.720]   Is it a computer of holes?
[02:28:48.720 --> 02:28:49.720]   Yeah.
[02:28:49.720 --> 02:28:50.720]   Yeah.
[02:28:50.720 --> 02:28:51.720]   People are like on the camera.
[02:28:51.720 --> 02:28:53.720]   Holes are scary and people left and right.
[02:28:53.720 --> 02:28:55.640]   You can't live in Switzerland if you have this fear.
[02:28:55.640 --> 02:28:56.640]   It's very, very hard.
[02:28:56.640 --> 02:28:57.640]   Yeah.
[02:28:57.640 --> 02:28:58.640]   I like that.
[02:28:58.640 --> 02:28:59.640]   I like that.
[02:28:59.640 --> 02:29:00.640]   I like that.
[02:29:00.640 --> 02:29:01.640]   I like that.
[02:29:01.640 --> 02:29:02.800]   Paul, those ports because it looks like the old onion article about we're going to put
[02:29:02.800 --> 02:29:04.800]   five blades on the right there.
[02:29:04.800 --> 02:29:06.800]   It's like, what the hell is that?
[02:29:06.800 --> 02:29:07.800]   It is.
[02:29:07.800 --> 02:29:08.800]   It is.
[02:29:08.800 --> 02:29:11.680]   But it's really, it's a really good unit.
[02:29:11.680 --> 02:29:12.680]   It's really good.
[02:29:12.680 --> 02:29:15.440]   It draws like 140 watts.
[02:29:15.440 --> 02:29:18.000]   It's this is kind of, this is the ultimate thing.
[02:29:18.000 --> 02:29:20.200]   This is the like the killer doc.
[02:29:20.200 --> 02:29:23.480]   If you really just want to go inside, I've taken a wrong turn somewhere.
[02:29:23.480 --> 02:29:24.680]   I've done something wrong in life.
[02:29:24.680 --> 02:29:27.120]   I need that many ports and I've taken a lot of wrong turns.
[02:29:27.120 --> 02:29:28.120]   Here's the thing.
[02:29:28.120 --> 02:29:29.120]   I mean, I waited.
[02:29:29.120 --> 02:29:30.120]   I finally got it a couple of months ago.
[02:29:30.120 --> 02:29:34.400]   I waited six months to get it because they could keep, could get the parts or whatever.
[02:29:34.400 --> 02:29:38.880]   And now thanks to USB, what is it?
[02:29:38.880 --> 02:29:40.840]   USB four version 2.0.
[02:29:40.840 --> 02:29:43.560]   It's absolutely got a buy new one.
[02:29:43.560 --> 02:29:44.560]   Just throw it in the garbage.
[02:29:44.560 --> 02:29:45.560]   Throw it in the recycling.
[02:29:45.560 --> 02:29:47.000]   The electronics was like me.
[02:29:47.000 --> 02:29:49.120]   Thanks a bunch USB.
[02:29:49.120 --> 02:29:51.120]   They should do what the Wi-Fi Alliance does.
[02:29:51.120 --> 02:29:53.560]   They should just have names instead of all these.
[02:29:53.560 --> 02:29:55.560]   Just call it USB four grease lightning.
[02:29:55.560 --> 02:29:57.360]   They should name it like the name hurricanes.
[02:29:57.360 --> 02:29:58.360]   Yeah.
[02:29:58.360 --> 02:30:00.600]   It should be like, this is the season of female names.
[02:30:00.600 --> 02:30:01.600]   Yeah.
[02:30:01.600 --> 02:30:02.600]   It's the season of male names.
[02:30:02.600 --> 02:30:04.600]   Screw you, Mike.
[02:30:04.600 --> 02:30:06.040]   I want Nina.
[02:30:06.040 --> 02:30:10.840]   You mean the Wi-Fi Alliance, the one that introduced the numbering system and then decided to
[02:30:10.840 --> 02:30:13.000]   introduce Wi-Fi 6E?
[02:30:13.000 --> 02:30:14.000]   Yeah.
[02:30:14.000 --> 02:30:15.000]   That Wi-Fi Alliance.
[02:30:15.000 --> 02:30:16.000]   That Wi-Fi Alliance.
[02:30:16.000 --> 02:30:20.280]   This feels like it's the same people who are naming cell phone like signals.
[02:30:20.280 --> 02:30:21.840]   It's actually, yeah, it's one group.
[02:30:21.840 --> 02:30:24.080]   It's actually one group of people who just need everything.
[02:30:24.080 --> 02:30:25.080]   The name of my 5th.
[02:30:25.080 --> 02:30:28.440]   They just have a big bag full of numbers and letters.
[02:30:28.440 --> 02:30:29.440]   Yeah.
[02:30:29.440 --> 02:30:30.440]   Let's pull it out.
[02:30:30.440 --> 02:30:31.440]   What is this?
[02:30:31.440 --> 02:30:34.920]   I was riding with my 15-year-old yesterday and they're like, I'm not getting good 5G service.
[02:30:34.920 --> 02:30:36.400]   What an incredible complaint.
[02:30:36.400 --> 02:30:41.240]   And I said, oh, does it say 5G or does it say 5UG or does it say 5UW?
[02:30:41.240 --> 02:30:43.400]   And they're like, what are you talking about?
[02:30:43.400 --> 02:30:44.400]   Yeah, that's right.
[02:30:44.400 --> 02:30:45.400]   Well, let me explain.
[02:30:45.400 --> 02:30:46.400]   5GE.
[02:30:46.400 --> 02:30:47.400]   5GE.
[02:30:47.400 --> 02:30:48.400]   Which wasn't 5GE.
[02:30:48.400 --> 02:30:49.400]   It was LTE.
[02:30:49.400 --> 02:30:50.400]   I asked you, Mike.
[02:30:50.400 --> 02:30:54.600]   Did your kid open the door to the car and just roll slowly out as you do?
[02:30:54.600 --> 02:30:55.600]   [LAUGHTER]
[02:30:55.600 --> 02:30:56.600]   Is it USB?
[02:30:56.600 --> 02:30:57.600]   Yeah.
[02:30:57.600 --> 02:30:58.600]   Is it USB?
[02:30:58.600 --> 02:30:59.600]   You're looking at USB.
[02:30:59.600 --> 02:31:00.600]   Yeah.
[02:31:00.600 --> 02:31:02.880]   Everything is like now that all the networks have upgraded.
[02:31:02.880 --> 02:31:07.400]   Like, if your cell phone service even goes to like 4G LTE, it's like you're living in
[02:31:07.400 --> 02:31:08.400]   the Stone Age.
[02:31:08.400 --> 02:31:10.120]   It's like, I cannot download a tweet.
[02:31:10.120 --> 02:31:11.120]   It's already absolutely.
[02:31:11.120 --> 02:31:12.560]   It's like, somehow we lived like this?
[02:31:12.560 --> 02:31:13.560]   Yeah.
[02:31:13.560 --> 02:31:15.880]   How do we survive?
[02:31:15.880 --> 02:31:19.760]   Actually, interestingly, I think this is a-- we'll do one more story that we'll take
[02:31:19.760 --> 02:31:21.760]   a break, but I think this is really-- I'm making.
[02:31:21.760 --> 02:31:22.760]   I'm happy.
[02:31:22.760 --> 02:31:29.200]   Comcast and Charter says Fast Company face a grim new reality actual competition.
[02:31:29.200 --> 02:31:30.200]   And where's it coming from?
[02:31:30.200 --> 02:31:36.560]   T-Mobile and Verizon's residential internet service using their 5G network.
[02:31:36.560 --> 02:31:37.560]   Oh, man.
[02:31:37.560 --> 02:31:38.560]   Verizon files.
[02:31:38.560 --> 02:31:39.560]   That's what I've got right now.
[02:31:39.560 --> 02:31:40.560]   Yeah.
[02:31:40.560 --> 02:31:41.560]   I have--
[02:31:41.560 --> 02:31:42.560]   Not even files.
[02:31:42.560 --> 02:31:43.560]   --severed internet speeds.
[02:31:43.560 --> 02:31:44.560]   Yeah.
[02:31:44.560 --> 02:31:45.560]   But that's-- you got a landline good thing.
[02:31:45.560 --> 02:31:47.040]   I got this for my daughter.
[02:31:47.040 --> 02:31:49.400]   I'm already a Verizon customer, so it's $25 a month.
[02:31:49.400 --> 02:31:55.520]   It's just a little Verizon 5G receiver that turns it into Wi-Fi.
[02:31:55.520 --> 02:32:00.320]   It's for residential service, 138 gigs, megabits down.
[02:32:00.320 --> 02:32:02.680]   It's like 20 or 30 megabits up.
[02:32:02.680 --> 02:32:05.480]   It's very good service for $25 a month.
[02:32:05.480 --> 02:32:09.840]   And this has stalled Comcast and Charter, zero growth last quarter.
[02:32:09.840 --> 02:32:14.400]   I love that this is the thing because it's the inverse of like 10 years ago, AT&T had
[02:32:14.400 --> 02:32:16.400]   those micro cells, right?
[02:32:16.400 --> 02:32:17.400]   Yeah.
[02:32:17.400 --> 02:32:18.400]   And you go, Wi-Fi and give you phone signal.
[02:32:18.400 --> 02:32:21.040]   And now it's like, ha ha, shoes on the other side.
[02:32:21.040 --> 02:32:26.920]   Yeah, my dad and stepmom live in a relatively small town in Washington that has OK cable
[02:32:26.920 --> 02:32:27.920]   service.
[02:32:27.920 --> 02:32:31.560]   And my dad calls me a piece reasonably technically savvy in his 80s.
[02:32:31.560 --> 02:32:33.680]   For his 80s, he's very technically savvy.
[02:32:33.680 --> 02:32:36.400]   And he said, I think the service is going in and out.
[02:32:36.400 --> 02:32:37.400]   We're talking about stuff.
[02:32:37.400 --> 02:32:38.640]   And he's like, it just keeps going down and up.
[02:32:38.640 --> 02:32:40.400]   And I'm like, have you called the cable company?
[02:32:40.400 --> 02:32:42.480]   They get like 100 megabits per second.
[02:32:42.480 --> 02:32:45.520]   Like, yeah, I'm like-- and he said-- then he emails me.
[02:32:45.520 --> 02:32:47.600]   He said, I've heard about this new T-Mobile service.
[02:32:47.600 --> 02:32:48.600]   Should I get it?
[02:32:48.600 --> 02:32:50.360]   I'm like, you know, I've heard good things about it.
[02:32:50.360 --> 02:32:53.040]   It's $50, including tax a month.
[02:32:53.040 --> 02:32:54.040]   Plugs it in.
[02:32:54.040 --> 02:32:57.600]   And like I say, they're in a relatively small-- you know, not super remote.
[02:32:57.600 --> 02:33:00.440]   It's like two hours from Seattle with a ferry ride.
[02:33:00.440 --> 02:33:01.920]   And he's getting 100 megabits per second.
[02:33:01.920 --> 02:33:05.240]   It's been up like 99.9% of the time.
[02:33:05.240 --> 02:33:09.400]   They canceled their 80-something dollar a month cable service because they don't like
[02:33:09.400 --> 02:33:10.400]   it.
[02:33:10.400 --> 02:33:11.400]   And now they got a thing that works.
[02:33:11.400 --> 02:33:13.920]   So T-Mobile there has more than a million subscribers.
[02:33:13.920 --> 02:33:14.920]   Wow.
[02:33:14.920 --> 02:33:17.120]   Half of whom they added in the last quarter.
[02:33:17.120 --> 02:33:20.240]   This is Brian Roberts at Comcast said this is killing us.
[02:33:20.240 --> 02:33:27.200]   Verizon has 384,000 home internet subscribers, two-thirds of which from the last quarter.
[02:33:27.200 --> 02:33:29.760]   It's gone from zero to 60 very, very quickly.
[02:33:29.760 --> 02:33:31.640]   Now I should caution.
[02:33:31.640 --> 02:33:33.000]   It will not work everywhere.
[02:33:33.000 --> 02:33:37.360]   You have to be close to a 5G tower that it can't be congestion.
[02:33:37.360 --> 02:33:40.880]   If you're lucky enough to have that service nearby as my daughter is, it works great.
[02:33:40.880 --> 02:33:41.880]   It's flawless.
[02:33:41.880 --> 02:33:42.880]   I checked.
[02:33:42.880 --> 02:33:44.680]   And the other thing is because I'm the Verizon customer.
[02:33:44.680 --> 02:33:46.840]   I can check her bandwidth at any time.
[02:33:46.840 --> 02:33:48.800]   So I've been checking it day and night just to see.
[02:33:48.800 --> 02:33:49.800]   And it's fine.
[02:33:49.800 --> 02:33:50.800]   It never goes down.
[02:33:50.800 --> 02:33:51.800]   What are you doing?
[02:33:51.800 --> 02:33:52.800]   What are you doing?
[02:33:52.800 --> 02:33:53.800]   I'm going blind blind.
[02:33:53.800 --> 02:33:54.800]   Day and night.
[02:33:54.800 --> 02:33:55.800]   I'm checking.
[02:33:55.800 --> 02:33:56.800]   I'm checking.
[02:33:56.800 --> 02:33:59.800]   And the Blu-ray player usage is really down.
[02:33:59.800 --> 02:34:00.800]   Where's your iPod?
[02:34:00.800 --> 02:34:01.800]   Three iPods.
[02:34:01.800 --> 02:34:08.440]   We have one gigabit internet service to our house and it's because our local telephone
[02:34:08.440 --> 02:34:09.560]   company was failing.
[02:34:09.560 --> 02:34:14.320]   And so they rolled out, did a big bet the company thing and they rolled out fiber all
[02:34:14.320 --> 02:34:15.320]   over the place.
[02:34:15.320 --> 02:34:20.760]   The flip side is I pay 80 something dollars a month for gigabit internet and Comcast
[02:34:20.760 --> 02:34:24.360]   still has terrible, more expensive service in my neighborhood.
[02:34:24.360 --> 02:34:27.680]   And so I think they're being killed on that lower end and the higher end.
[02:34:27.680 --> 02:34:28.680]   Yeah.
[02:34:28.680 --> 02:34:29.680]   Yeah, exactly.
[02:34:29.680 --> 02:34:30.680]   The competition is a good thing here.
[02:34:30.680 --> 02:34:32.680]   Yeah, because all the cable monopoly.
[02:34:32.680 --> 02:34:36.120]   20 bucks a month for 400 up down and it's fantastic.
[02:34:36.120 --> 02:34:37.120]   What?
[02:34:37.120 --> 02:34:38.120]   How is that?
[02:34:38.120 --> 02:34:43.120]   Somehow with Verizon, I went over my phone bill.
[02:34:43.120 --> 02:34:45.040]   It like bundled with my home.
[02:34:45.040 --> 02:34:47.240]   The home is only $20.
[02:34:47.240 --> 02:34:52.200]   Every apartment I move to in New York, I make a Verizon man come up, scale the wall and
[02:34:52.200 --> 02:34:55.960]   drill a hole into my wall to put the fiber thing in.
[02:34:55.960 --> 02:34:58.360]   And then I run extension cords throughout my entire home.
[02:34:58.360 --> 02:35:02.000]   Don't show him the lamp though because he made every contact.
[02:35:02.000 --> 02:35:03.000]   I won't.
[02:35:03.000 --> 02:35:08.360]   Last time I think he might have seen one because they accidentally left a fancy router just
[02:35:08.360 --> 02:35:10.080]   like sitting on my fire escapes.
[02:35:10.080 --> 02:35:11.080]   Nice as they ran.
[02:35:11.080 --> 02:35:12.080]   That's great.
[02:35:12.080 --> 02:35:13.080]   They ran.
[02:35:13.080 --> 02:35:15.720]   All right, let's take a break and I have the final stories.
[02:35:15.720 --> 02:35:18.320]   This is going on way too long, but I'm having way too much fun.
[02:35:18.320 --> 02:35:20.800]   Paris, Martin Oakland, Fleischman.
[02:35:20.800 --> 02:35:22.600]   So great to have you, Dan Morin.
[02:35:22.600 --> 02:35:23.680]   We got to do this again soon.
[02:35:23.680 --> 02:35:25.360]   You guys are fantastic.
[02:35:25.360 --> 02:35:28.160]   Our show today brought to you by Stamps.com.
[02:35:28.160 --> 02:35:32.320]   Somebody called the radio show today said I want to sell online.
[02:35:32.320 --> 02:35:34.400]   How should I do the mailing?
[02:35:34.400 --> 02:35:36.000]   What kind of printer do I need?
[02:35:36.000 --> 02:35:37.000]   Whatever.
[02:35:37.000 --> 02:35:38.960]   I said, no, no, no, it's very simple.
[02:35:38.960 --> 02:35:43.160]   You check with the vendor, see what they tell you to use.
[02:35:43.160 --> 02:35:45.520]   If it's Stamps.com, you're golden.
[02:35:45.520 --> 02:35:46.960]   You don't need a special printer.
[02:35:46.960 --> 02:35:49.000]   You don't need anything.
[02:35:49.000 --> 02:35:53.120]   Stamps.com is the biggest boon for the small business owner.
[02:35:53.120 --> 02:35:54.880]   We're getting ready for the holiday season.
[02:35:54.880 --> 02:35:56.520]   It's coming up.
[02:35:56.520 --> 02:36:00.800]   If you, I hope you still don't, and I remember this a couple of years ago going to the post
[02:36:00.800 --> 02:36:05.880]   office, standing behind somebody who had an armful of packages.
[02:36:05.880 --> 02:36:07.240]   She was mailing.
[02:36:07.240 --> 02:36:08.920]   She was waiting in line to mail them.
[02:36:08.920 --> 02:36:11.000]   That's crazy.
[02:36:11.000 --> 02:36:14.040]   Stamps.com can do it all without a trip to the post office.
[02:36:14.040 --> 02:36:15.200]   You never have to leave your desk.
[02:36:15.200 --> 02:36:17.520]   It's a 24/7 post office.
[02:36:17.520 --> 02:36:18.760]   You can access anywhere.
[02:36:18.760 --> 02:36:21.400]   No lines, no traffic, no hassle.
[02:36:21.400 --> 02:36:24.360]   We've been using them since, I don't know when.
[02:36:24.360 --> 02:36:25.760]   We've been doing their ads since 2012.
[02:36:25.760 --> 02:36:28.520]   I think we've been using them for longer than that.
[02:36:28.520 --> 02:36:29.760]   Haven't you tried them yet?
[02:36:29.760 --> 02:36:30.840]   You've heard me talk about it.
[02:36:30.840 --> 02:36:32.640]   What are you waiting for?
[02:36:32.640 --> 02:36:35.120]   By the way, it's getting better.
[02:36:35.120 --> 02:36:39.720]   Next.com now is your one stop shop for all your shipping and mailing needs.
[02:36:39.720 --> 02:36:44.680]   For more than 20 years, it's been indispensable for more than a million businesses because
[02:36:44.680 --> 02:36:49.360]   they can print real US posts, which are right in the envelope or on a package.
[02:36:49.360 --> 02:36:57.400]   But now it's USPS, the United States Postal Service, and UPS together, which means you
[02:36:57.400 --> 02:37:03.040]   can price shop right there from your desk, get the best deal, get the shipping you need.
[02:37:03.040 --> 02:37:06.360]   And nowadays, with inflation on the rise, every dollar counts, every business needs
[02:37:06.360 --> 02:37:07.360]   this.
[02:37:07.360 --> 02:37:08.360]   Protect your margins.
[02:37:08.360 --> 02:37:14.520]   You'll get major discounts on both Postal Service and UPS rates, up to 86% off.
[02:37:14.520 --> 02:37:16.680]   Deals you can't get at the post office.
[02:37:16.680 --> 02:37:20.440]   In fact, stamps.com negotiated a great deal with UPS.
[02:37:20.440 --> 02:37:23.040]   There's no residential surcharge.
[02:37:23.040 --> 02:37:27.320]   That saves you a huge amount for every package you ship to a home.
[02:37:27.320 --> 02:37:29.560]   It's a stress-free solution for every small business.
[02:37:29.560 --> 02:37:31.280]   You could print postage.
[02:37:31.280 --> 02:37:34.440]   Every you do business, all you need is a computer and a printer.
[02:37:34.440 --> 02:37:35.440]   You don't have to get up.
[02:37:35.440 --> 02:37:36.440]   You don't have to go to the post office.
[02:37:36.440 --> 02:37:40.920]   In fact, if you're using the post office, a uniformed employee of the federal government
[02:37:40.920 --> 02:37:45.840]   will come and pick up that mail and take it to the post office, your mail carrier.
[02:37:45.840 --> 02:37:49.660]   If you need a package pickup, you could schedule it right through the dashboard.
[02:37:49.660 --> 02:37:51.160]   Same with UPS.
[02:37:51.160 --> 02:37:56.440]   Rates are constantly changing, but with stamps.com's switch and save feature, you can easily
[02:37:56.440 --> 02:37:58.920]   compare carriers and rates.
[02:37:58.920 --> 02:38:00.280]   You know you're getting the best deal.
[02:38:00.280 --> 02:38:01.840]   You're saving money.
[02:38:01.840 --> 02:38:03.840]   It's fantastic for an online store.
[02:38:03.840 --> 02:38:07.080]   Works seamlessly with all the major shopping carts and marketplaces.
[02:38:07.080 --> 02:38:09.880]   They'll actually fill in all the forms automatically.
[02:38:09.880 --> 02:38:11.280]   You don't have to do any typing.
[02:38:11.280 --> 02:38:14.560]   Right on the envelope if you're sending an envelope print a package label.
[02:38:14.560 --> 02:38:17.120]   They'll even suggest, "Hey, this might be a better media mail.
[02:38:17.120 --> 02:38:18.520]   Save some money this way."
[02:38:18.520 --> 02:38:20.520]   Look, get ahead of the holiday chaos this year.
[02:38:20.520 --> 02:38:22.280]   Get started with stamps.com.
[02:38:22.280 --> 02:38:25.080]   We love it.
[02:38:25.080 --> 02:38:26.520]   I can't recommend more highly.
[02:38:26.520 --> 02:38:28.760]   Sign up with the promo code TWIT right now.
[02:38:28.760 --> 02:38:32.000]   You get a very generous special offer.
[02:38:32.000 --> 02:38:38.320]   Four-week trial, free postage, free digital scale, no long-term commitments, no contracts.
[02:38:38.320 --> 02:38:39.320]   It's easy.
[02:38:39.320 --> 02:38:40.320]   Here's how you do it.
[02:38:40.320 --> 02:38:41.320]   Go to stamps.com.
[02:38:41.320 --> 02:38:44.560]   Up in the right-hand corner of the webpage, there's a microphone and it says something
[02:38:44.560 --> 02:38:47.000]   like, "Do you hear this on a podcast or on the radio?"
[02:38:47.000 --> 02:38:48.000]   That's the one.
[02:38:48.000 --> 02:38:49.000]   Click that.
[02:38:49.000 --> 02:38:50.640]   See there, up there on the right and enter the code TWIT.
[02:38:50.640 --> 02:38:51.920]   That's it.
[02:38:51.920 --> 02:38:52.920]   Then you get the deal.
[02:38:52.920 --> 02:38:53.920]   It's the best deal.
[02:38:53.920 --> 02:38:54.920]   Don't do the front page deal.
[02:38:54.920 --> 02:38:56.360]   Use this offer code.
[02:38:56.360 --> 02:38:57.360]   Trust me.
[02:38:57.360 --> 02:38:59.560]   It's worth $110.
[02:38:59.560 --> 02:39:01.200]   Stamps.com.
[02:39:01.200 --> 02:39:02.200]   We love you.
[02:39:02.200 --> 02:39:05.240]   Thank you stamps.com for your long support of our network.
[02:39:05.240 --> 02:39:07.400]   You support us by the way when you use that offer code TWIT.
[02:39:07.400 --> 02:39:10.000]   That's how you let them know that you saw it here.
[02:39:10.000 --> 02:39:11.840]   Stamps.com.
[02:39:11.840 --> 02:39:13.760]   All right.
[02:39:13.760 --> 02:39:16.480]   We got a few quick stories to...
[02:39:16.480 --> 02:39:18.560]   Oh, before we do that, I want to wrap it up.
[02:39:18.560 --> 02:39:21.080]   Before we do that, we made this great promo.
[02:39:21.080 --> 02:39:23.040]   Thank you, Benita, for reminding me.
[02:39:23.040 --> 02:39:25.040]   This is what happened this week on TWIT.
[02:39:25.040 --> 02:39:26.040]   Hey, Heather.
[02:39:26.040 --> 02:39:27.040]   Heather, are you trying to connect?
[02:39:27.040 --> 02:39:29.240]   Are you trying to connect the sideburns now?
[02:39:29.240 --> 02:39:30.240]   Are they going to actually...
[02:39:30.240 --> 02:39:31.480]   Or is that their idea?
[02:39:31.480 --> 02:39:33.280]   I am fully aware of it.
[02:39:33.280 --> 02:39:34.280]   Thank you very much.
[02:39:34.280 --> 02:39:35.280]   You're looking at me.
[02:39:35.280 --> 02:39:36.280]   At least you've got my profile.
[02:39:36.280 --> 02:39:40.120]   I thought that maybe they'd come to life and they were deciding that they should join.
[02:39:40.120 --> 02:39:41.120]   Join up in the middle.
[02:39:41.120 --> 02:39:42.120]   No, no.
[02:39:42.120 --> 02:39:43.120]   They're a team player.
[02:39:43.120 --> 02:39:48.000]   My brand is not always on my side, but by sideburns, they're always pulling with the team.
[02:39:48.000 --> 02:39:50.320]   Previously on TWIT.
[02:39:50.320 --> 02:39:51.320]   Tech News Weekly.
[02:39:51.320 --> 02:39:56.640]   Jason Allen ended up with the first place win in the Colorado State competitions.
[02:39:56.640 --> 02:40:01.000]   And as you can probably guess, Jason did not paint this picture.
[02:40:01.000 --> 02:40:06.920]   The picture were generated by an artificial intelligence engine called Midjourney.
[02:40:06.920 --> 02:40:08.320]   This week in Google.
[02:40:08.320 --> 02:40:12.440]   Start with AB 2273, which passed 33 to nothing.
[02:40:12.440 --> 02:40:15.120]   Age-appropriate design code.
[02:40:15.120 --> 02:40:20.200]   What California is doing is really sort of trying to completely change the way the internet
[02:40:20.200 --> 02:40:21.200]   works.
[02:40:21.200 --> 02:40:27.560]   I think effectively they're sort of trying to turn into Disneyland where everything has
[02:40:27.560 --> 02:40:31.480]   to be appropriate for children at all times.
[02:40:31.480 --> 02:40:32.480]   Windows Weekly.
[02:40:32.480 --> 02:40:34.240]   And I got on Twitter yesterday.
[02:40:34.240 --> 02:40:36.800]   I'm at Jensen Harris.
[02:40:36.800 --> 02:40:39.720]   Has come out of a hole in some woods somewhere.
[02:40:39.720 --> 02:40:42.760]   He's criticizing the Windows 11 start menu.
[02:40:42.760 --> 02:40:43.760]   Oh!
[02:40:43.760 --> 02:40:46.560]   There is such a lack of self-awareness here.
[02:40:46.560 --> 02:40:49.920]   This is like a drunk driver pointing to someone not using their blinkers and saying, "That
[02:40:49.920 --> 02:40:50.920]   guy's being bad."
[02:40:50.920 --> 02:40:51.920]   Twit.
[02:40:51.920 --> 02:40:53.920]   You destroyed Windows.
[02:40:53.920 --> 02:40:57.840]   You destroyed it.
[02:40:57.840 --> 02:40:59.000]   I have some very good news.
[02:40:59.000 --> 02:41:05.040]   Paul Therat will not be joining the Apple Live Event Stream on Wednesday.
[02:41:05.040 --> 02:41:10.520]   It scared the hell out of so many people.
[02:41:10.520 --> 02:41:12.360]   You understand why that would be humorous.
[02:41:12.360 --> 02:41:16.040]   I don't know if you've ever followed Paul during an Apple event and his tweets.
[02:41:16.040 --> 02:41:19.200]   He is not the biggest fan.
[02:41:19.200 --> 02:41:22.200]   But I thought, "Well, that would be a good antidote to the Apple event, but no, it will
[02:41:22.200 --> 02:41:24.240]   be me and Andy and Aco this Wednesday."
[02:41:24.240 --> 02:41:26.440]   It would have been fun though.
[02:41:26.440 --> 02:41:27.880]   I wish we could have gotten Paul to do it.
[02:41:27.880 --> 02:41:31.920]   He said, "I don't need the, I don't need the appropriate."
[02:41:31.920 --> 02:41:40.600]   But 10 AM Pacific, yep, 1 PM Eastern time, 1700 UTC, Wednesday the 7th for the big Apple
[02:41:40.600 --> 02:41:42.720]   infomercial.
[02:41:42.720 --> 02:41:46.120]   And Andy and I will give you context.
[02:41:46.120 --> 02:41:47.120]   Story of the week.
[02:41:47.120 --> 02:41:50.160]   Actually the headline scared the hell out of me when I saw the picture.
[02:41:50.160 --> 02:41:57.000]   "Doctor uses his Rivian R1T to perform vasectomy during power outage."
[02:41:57.000 --> 02:41:59.880]   Was a one in a million chance, Doctor.
[02:41:59.880 --> 02:42:01.520]   One in a million chance.
[02:42:01.520 --> 02:42:05.320]   I probably like you might have misunderstood this story especially because it looks like
[02:42:05.320 --> 02:42:08.280]   that Rivian is crashing into the doctor's office.
[02:42:08.280 --> 02:42:09.280]   But no.
[02:42:09.280 --> 02:42:16.440]   In fact, a doctor based in Austin, Texas, Dr. Christopher Yang, was set to perform a
[02:42:16.440 --> 02:42:23.080]   vasectomy in his clinic when the power went out as it is want to do in Austin, Texas.
[02:42:23.080 --> 02:42:27.160]   His patient said, "Doc, I can't reschedule.
[02:42:27.160 --> 02:42:29.560]   Can we just go ahead and do this?"
[02:42:29.560 --> 02:42:30.560]   The doctor.
[02:42:30.560 --> 02:42:31.560]   Now you could probably--
[02:42:31.560 --> 02:42:37.560]   I just want to be inside the mind of a patient who's like, "Listen, I know the power's
[02:42:37.560 --> 02:42:39.320]   out, but I gotta get this cut.
[02:42:39.320 --> 02:42:40.320]   I gotta get this cut.
[02:42:40.320 --> 02:42:41.320]   I gotta get this cut.
[02:42:41.320 --> 02:42:42.320]   I can't wait."
[02:42:42.320 --> 02:42:44.360]   What is the rationale?
[02:42:44.360 --> 02:42:45.960]   What is driving this person?
[02:42:45.960 --> 02:42:46.960]   Why do they move?
[02:42:46.960 --> 02:42:50.640]   I need the back story.
[02:42:50.640 --> 02:42:52.360]   It's got 314 miles of range.
[02:42:52.360 --> 02:42:54.800]   It was play close enough too.
[02:42:54.800 --> 02:42:56.920]   So I think if you give me a ride, you say.
[02:42:56.920 --> 02:43:00.960]   Like a lot of electric vehicles has 120 volt AC power outlets.
[02:43:00.960 --> 02:43:01.960]   Four of them, oh my gosh.
[02:43:01.960 --> 02:43:02.960]   On the Rivian.
[02:43:02.960 --> 02:43:06.660]   Now, the Ford Lightning, you can actually plug your house into if there's a power outage
[02:43:06.660 --> 02:43:08.080]   and run your house off of it.
[02:43:08.080 --> 02:43:09.080]   This is not that.
[02:43:09.080 --> 02:43:12.200]   This is just regular 120 volt AC outlets.
[02:43:12.200 --> 02:43:15.560]   The Doc, I guess, I don't know, what do you have to plug in?
[02:43:15.560 --> 02:43:18.560]   I would think it says his electric quadratic.
[02:43:18.560 --> 02:43:19.560]   Quadratic.
[02:43:19.560 --> 02:43:20.560]   Quadratic.
[02:43:20.560 --> 02:43:21.560]   Which I would think would be more like a 220.
[02:43:21.560 --> 02:43:24.960]   I mean, that's my opinion as somebody who uses electricity.
[02:43:24.960 --> 02:43:27.360]   I mean, are you under or during this procedure?
[02:43:27.360 --> 02:43:32.760]   No, well, I can fill you in because I've had a vasectomy and no, you're not.
[02:43:32.760 --> 02:43:38.360]   You're wide awake, but you're anesthetized.
[02:43:38.360 --> 02:43:39.360]   It's a local.
[02:43:39.360 --> 02:43:40.360]   It's a local.
[02:43:40.360 --> 02:43:43.960]   So what sort of a car powered year of a sex to me?
[02:43:43.960 --> 02:43:47.960]   Fortunately, I'll never know the power didn't go out and we didn't have it.
[02:43:47.960 --> 02:43:48.960]   We're going to fill you up.
[02:43:48.960 --> 02:43:49.960]   We're going to fill you up.
[02:43:49.960 --> 02:43:50.960]   We're going to fill you up.
[02:43:50.960 --> 02:43:51.960]   We're going to fill you up.
[02:43:51.960 --> 02:43:52.960]   We're going to fill you up.
[02:43:52.960 --> 02:43:53.960]   We're going to fill you up.
[02:43:53.960 --> 02:43:54.960]   We're going to fill you up.
[02:43:54.960 --> 02:43:55.960]   We're going to fill you up.
[02:43:55.960 --> 02:43:56.960]   We're going to fill you up.
[02:43:56.960 --> 02:43:57.960]   We're going to fill you up.
[02:43:57.960 --> 02:43:58.960]   We're going to fill you up.
[02:43:58.960 --> 02:43:59.960]   We're going to fill you up.
[02:43:59.960 --> 02:44:00.960]   We're going to fill you up.
[02:44:00.960 --> 02:44:01.960]   We're going to fill you up.
[02:44:01.960 --> 02:44:02.960]   We're going to fill you up.
[02:44:02.960 --> 02:44:03.960]   We're going to fill you up.
[02:44:03.960 --> 02:44:04.960]   We're going to fill you up.
[02:44:04.960 --> 02:44:05.960]   We're going to fill you up.
[02:44:05.960 --> 02:44:06.960]   We're going to fill you up.
[02:44:06.960 --> 02:44:13.960]   We're going to fill you up.
[02:44:13.960 --> 02:44:21.960]   We're going to fill you up.
[02:44:21.960 --> 02:44:22.960]   We're going to fill you up.
[02:44:22.960 --> 02:44:23.960]   We're going to fill you up.
[02:44:23.960 --> 02:44:24.960]   We're going to fill you up.
[02:44:24.960 --> 02:44:25.960]   We're going to fill you up.
[02:44:25.960 --> 02:44:26.960]   We're going to fill you up.
[02:44:26.960 --> 02:44:27.960]   We're going to fill you up.
[02:44:27.960 --> 02:44:28.960]   We're going to fill you up.
[02:44:28.960 --> 02:44:29.960]   We're going to fill you up.
[02:44:29.960 --> 02:44:30.960]   We're going to fill you up.
[02:44:30.960 --> 02:44:31.960]   We're going to fill you up.
[02:44:31.960 --> 02:44:32.960]   We're going to fill you up.
[02:44:32.960 --> 02:44:33.960]   We're going to fill you up.
[02:44:33.960 --> 02:44:34.960]   We're going to fill you up.
[02:44:34.960 --> 02:44:35.960]   We're going to fill you up.
[02:44:35.960 --> 02:44:36.960]   We're going to fill you up.
[02:44:36.960 --> 02:44:37.960]   We're going to fill you up.
[02:44:37.960 --> 02:44:38.960]   We're going to fill you up.
[02:44:38.960 --> 02:44:39.960]   We're going to fill you up.
[02:44:39.960 --> 02:44:40.960]   We're going to fill you up.
[02:44:40.960 --> 02:44:41.960]   We're going to fill you up.
[02:44:41.960 --> 02:44:42.960]   We're going to fill you up.
[02:44:42.960 --> 02:44:43.960]   We're going to fill you up.
[02:44:43.960 --> 02:44:44.960]   We're going to fill you up.
[02:44:44.960 --> 02:44:45.960]   We're going to fill you up.
[02:44:45.960 --> 02:44:46.960]   We're going to fill you up.
[02:44:46.960 --> 02:44:47.960]   We're going to fill you up.
[02:44:47.960 --> 02:44:48.960]   We're going to fill you up.
[02:44:48.960 --> 02:44:49.960]   We're going to fill you up.
[02:44:49.960 --> 02:44:50.960]   We're going to fill you up.
[02:44:50.960 --> 02:44:51.960]   We're going to fill you up.
[02:44:51.960 --> 02:44:52.960]   We're going to fill you up.
[02:44:52.960 --> 02:44:53.960]   We're going to fill you up.
[02:44:53.960 --> 02:44:54.960]   We're going to fill you up.
[02:44:54.960 --> 02:44:55.960]   We're going to fill you up.
[02:44:55.960 --> 02:44:56.960]   We're going to fill you up.
[02:44:56.960 --> 02:44:57.960]   We're going to fill you up.
[02:44:57.960 --> 02:44:58.960]   We're going to fill you up.
[02:44:58.960 --> 02:44:59.960]   We're going to fill you up.
[02:44:59.960 --> 02:45:00.960]   We're going to fill you up.
[02:45:00.960 --> 02:45:01.960]   We're going to fill you up.
[02:45:01.960 --> 02:45:02.960]   We're going to fill you up.
[02:45:02.960 --> 02:45:03.960]   We're going to fill you up.
[02:45:03.960 --> 02:45:04.960]   We're going to fill you up.
[02:45:04.960 --> 02:45:05.960]   We're going to fill you up.
[02:45:05.960 --> 02:45:06.960]   We're going to fill you up.
[02:45:06.960 --> 02:45:07.960]   We're going to fill you up.
[02:45:07.960 --> 02:45:08.960]   We're going to fill you up.
[02:45:08.960 --> 02:45:09.960]   We're going to fill you up.
[02:45:09.960 --> 02:45:10.960]   We're going to fill you up.
[02:45:10.960 --> 02:45:11.960]   We're going to fill you up.
[02:45:11.960 --> 02:45:12.960]   We're going to fill you up.
[02:45:12.960 --> 02:45:13.960]   We're going to fill you up.
[02:45:13.960 --> 02:45:14.960]   We're going to fill you up.
[02:45:14.960 --> 02:45:15.960]   We're going to fill you up.
[02:45:15.960 --> 02:45:16.960]   We're going to fill you up.
[02:45:16.960 --> 02:45:17.960]   We're going to fill you up.
[02:45:17.960 --> 02:45:18.960]   We're going to fill you up.
[02:45:18.960 --> 02:45:19.960]   We're going to fill you up.
[02:45:19.960 --> 02:45:20.960]   We're going to fill you up.
[02:45:20.960 --> 02:45:21.960]   We're going to fill you up.
[02:45:21.960 --> 02:45:22.960]   We're going to fill you up.
[02:45:22.960 --> 02:45:23.960]   We're going to fill you up.
[02:45:23.960 --> 02:45:24.960]   We're going to fill you up.
[02:45:24.960 --> 02:45:25.960]   We're going to fill you up.
[02:45:25.960 --> 02:45:26.960]   We're going to fill you up.
[02:45:26.960 --> 02:45:27.960]   We're going to fill you up.
[02:45:27.960 --> 02:45:28.960]   We're going to fill you up.
[02:45:28.960 --> 02:45:29.960]   We're going to fill you up.
[02:45:29.960 --> 02:45:30.960]   We're going to fill you up.
[02:45:30.960 --> 02:45:31.960]   We're going to fill you up.
[02:45:31.960 --> 02:45:32.960]   We're going to fill you up.
[02:45:32.960 --> 02:45:33.960]   We're going to fill you up.
[02:45:33.960 --> 02:45:34.960]   We're going to fill you up.
[02:45:34.960 --> 02:45:35.960]   We're going to fill you up.
[02:45:35.960 --> 02:45:36.960]   We're going to fill you up.
[02:45:36.960 --> 02:45:37.960]   We're going to fill you up.
[02:45:37.960 --> 02:45:38.960]   We're going to fill you up.
[02:45:38.960 --> 02:45:39.960]   We're going to fill you up.
[02:45:39.960 --> 02:45:40.960]   We're going to fill you up.
[02:45:41.960 --> 02:45:45.960]   Let's check back in with this guy in a year or two to see if it actually was successful.
[02:45:45.960 --> 02:45:46.960]   Wow.
[02:45:46.960 --> 02:45:48.960]   This way they're going to do Brissa's next.
[02:45:48.960 --> 02:45:49.960]   That's going to be cool.
[02:45:49.960 --> 02:45:50.960]   Oh Lord.
[02:45:50.960 --> 02:45:51.960]   That's going to be cool.
[02:45:51.960 --> 02:45:55.960]   Inside EVs says the R1T continues to prove its versatility.
[02:45:55.960 --> 02:45:56.960]   Oh my God.
[02:45:56.960 --> 02:45:57.960]   Come on.
[02:45:57.960 --> 02:45:58.960]   I would agree.
[02:45:58.960 --> 02:46:00.960]   The other power plant.
[02:46:00.960 --> 02:46:01.960]   Sorry.
[02:46:01.960 --> 02:46:04.960]   The other story I would like to mention.
[02:46:04.960 --> 02:46:05.960]   I don't know.
[02:46:05.960 --> 02:46:09.960]   Maybe this isn't the most exciting story when I brought this up earlier with another panel.
[02:46:09.960 --> 02:46:11.960]   They didn't like it, but I like it.
[02:46:11.960 --> 02:46:17.960]   The New York Times says we can now talk to naked mole rats.
[02:46:17.960 --> 02:46:22.960]   It turns out the naked mole rat has a fairly elaborate vocabulary.
[02:46:22.960 --> 02:46:27.960]   When the two rats meet in a dark tunnel, they exchange a standard salutation.
[02:46:27.960 --> 02:46:32.960]   They make a soft chirp and then repeating soft chirps as Alison Barker, a neuroscientist
[02:46:32.960 --> 02:46:35.960]   at the Max Planck Institute for Brain Research in Germany.
[02:46:35.960 --> 02:46:38.960]   They have a little conversation.
[02:46:38.960 --> 02:46:39.960]   Would you like to hear it?
[02:46:39.960 --> 02:46:41.960]   Here you go.
[02:46:41.960 --> 02:46:45.960]   From the courtesy of the New York Times, is my volume turned down?
[02:46:45.960 --> 02:46:47.960]   Let me turn it up here.
[02:46:47.960 --> 02:46:49.960]   Oh, it's up to you, Benito.
[02:46:49.960 --> 02:46:52.960]   We're never going to know what the naked mole rat said.
[02:46:52.960 --> 02:46:53.960]   It's a doctor.
[02:46:53.960 --> 02:46:54.960]   Wow.
[02:46:54.960 --> 02:46:56.960]   Barker is a nominative determinism right there, I think.
[02:46:56.960 --> 02:46:58.960]   She's studying animal talking.
[02:46:58.960 --> 02:46:59.960]   There it is.
[02:46:59.960 --> 02:47:01.960]   That's the two rats talking.
[02:47:01.960 --> 02:47:04.960]   The greeting call, which I thought was going to be pretty basic,
[02:47:04.960 --> 02:47:08.960]   turns out to be incredibly complicated, it's a doctor Barker.
[02:47:08.960 --> 02:47:11.960]   Machine learning, turns for my research.
[02:47:11.960 --> 02:47:21.960]   They're feeding animal sounds to machine learning systems to help them understand what the animals
[02:47:21.960 --> 02:47:23.960]   are doing and saying.
[02:47:23.960 --> 02:47:28.960]   In fact, they've been able to see that there are multiple patterns.
[02:47:28.960 --> 02:47:32.960]   For instance, not only does each mole rat have its own vocal signature.
[02:47:32.960 --> 02:47:35.960]   They don't have any clothes, but they have their own vocal signature.
[02:47:35.960 --> 02:47:43.960]   Each colony has its own distinct dialect, which is passed down culturally over generations.
[02:47:43.960 --> 02:47:44.960]   This is the most interesting.
[02:47:44.960 --> 02:47:50.960]   During times of social instability, such as in the weeks after a colony's queen is violently
[02:47:50.960 --> 02:47:52.960]   deposed, the dialect...
[02:47:52.960 --> 02:47:53.960]   Wow.
[02:47:53.960 --> 02:47:55.960]   It's an elaborate...
[02:47:55.960 --> 02:47:58.960]   Where is going full circle of active Game of Thrones?
[02:47:58.960 --> 02:48:01.960]   The cohesive dialects fell apart.
[02:48:01.960 --> 02:48:07.960]   Then when the new queen begins her reign, a new dialect appears to take hold.
[02:48:07.960 --> 02:48:08.960]   I think that's fascinating.
[02:48:08.960 --> 02:48:10.960]   It was all this blockchain.
[02:48:10.960 --> 02:48:13.960]   Let's throw some blockchain at it.
[02:48:13.960 --> 02:48:16.960]   Can these rats go to the moon?
[02:48:16.960 --> 02:48:18.960]   That's the question.
[02:48:18.960 --> 02:48:20.960]   I love this area.
[02:48:20.960 --> 02:48:26.960]   There was an article recently also about the systems for dog and cat communication.
[02:48:26.960 --> 02:48:32.960]   Mary Robinette Cowell, the science fiction writer on her Instagram, she's been posting updates
[02:48:32.960 --> 02:48:37.960]   of talking with her cat, Elsie, with one of those button-based systems.
[02:48:37.960 --> 02:48:39.960]   I haven't seen anybody else use it as...
[02:48:39.960 --> 02:48:43.960]   Or document it as publicly or elaborately.
[02:48:43.960 --> 02:48:47.960]   It is extraordinary to see what Elsie will say.
[02:48:47.960 --> 02:48:53.960]   Elsie will just walk across this thing that's full of buttons and not touch one, but literally
[02:48:53.960 --> 02:48:58.960]   walk in casually across it and go to the right one and hit the thing or several to express
[02:48:58.960 --> 02:48:59.960]   her thoughts.
[02:48:59.960 --> 02:49:00.960]   You're like, "There is intent.
[02:49:00.960 --> 02:49:01.960]   This is not trained.
[02:49:01.960 --> 02:49:03.960]   It's more sophisticated than that.
[02:49:03.960 --> 02:49:04.960]   How much is it?
[02:49:04.960 --> 02:49:05.960]   Real communication.
[02:49:05.960 --> 02:49:10.960]   It seems like it's fairly substantial actually, but we always lay things on top of it.
[02:49:10.960 --> 02:49:12.960]   This is a little cleaner because you're not training the rats.
[02:49:12.960 --> 02:49:13.960]   You're just observing that.
[02:49:13.960 --> 02:49:14.960]   Yeah, exactly.
[02:49:14.960 --> 02:49:17.960]   It's clear that there are a variety of vocalizations.
[02:49:17.960 --> 02:49:23.960]   There is a University of Washington, up your way, has a software called Deep Squeak.
[02:49:23.960 --> 02:49:27.960]   Well played University of Washington.
[02:49:27.960 --> 02:49:33.960]   It can automatically detect, analyze, and categorize the ultrasonic vocalizations of
[02:49:33.960 --> 02:49:34.960]   rodents.
[02:49:34.960 --> 02:49:39.960]   It can also distinguish between the complex song-like calls the animals make when they're feeling
[02:49:39.960 --> 02:49:42.960]   good and the long, flat ones they make when they're not.
[02:49:42.960 --> 02:49:45.960]   You could tell if a rat's depressed.
[02:49:45.960 --> 02:49:47.960]   Deep Squeak is...
[02:49:47.960 --> 02:49:48.960]   I had a description.
[02:49:48.960 --> 02:49:49.960]   Couldn't anybody?
[02:49:49.960 --> 02:49:50.960]   Yeah.
[02:49:50.960 --> 02:49:54.960]   Well you really looked down in the mouth, Mr. Rat.
[02:49:54.960 --> 02:49:59.960]   Deep Squeak has been repurposed for other species, including lemurs and whales.
[02:49:59.960 --> 02:50:02.960]   That's a very deep Squeak.
[02:50:02.960 --> 02:50:07.960]   Other teams have developed their own systems for automatically detecting when clucking chickens
[02:50:07.960 --> 02:50:10.960]   or squealing pigs are in distress.
[02:50:10.960 --> 02:50:13.960]   Often it turns out when people are about to kill them.
[02:50:13.960 --> 02:50:15.960]   As I say, how are they making two cats?
[02:50:15.960 --> 02:50:17.960]   Well I wonder if we could start to understand the animals.
[02:50:17.960 --> 02:50:20.960]   We may not be so likely to kill them.
[02:50:20.960 --> 02:50:25.960]   Well they might have a lot of things to say that we're not very interested in.
[02:50:25.960 --> 02:50:27.960]   Oh they may be so annoying that we want to kill them.
[02:50:27.960 --> 02:50:28.960]   Is that what you're thinking?
[02:50:28.960 --> 02:50:29.960]   You learn a dialect.
[02:50:29.960 --> 02:50:34.960]   You're like, "Oh my God, they won't shut up about that new tree that grows."
[02:50:34.960 --> 02:50:35.960]   Although I like to talk to Croes.
[02:50:35.960 --> 02:50:38.960]   I think Croes have a lot of interesting things to say.
[02:50:38.960 --> 02:50:43.960]   Here's a recording of fruit bats engaged in perch aggression.
[02:50:43.960 --> 02:50:47.960]   Just in case you're interested.
[02:50:47.960 --> 02:50:49.960]   Are they wearing hats?
[02:50:49.960 --> 02:50:51.960]   They call it probably.
[02:50:51.960 --> 02:50:58.960]   Also like it looks like one of those classic 1912 black and white movies.
[02:50:58.960 --> 02:51:03.960]   Yeah, but this looks like Nosferatu is going to pop out of state, right?
[02:51:03.960 --> 02:51:06.960]   So apparently, why are you stealing the middle seat?
[02:51:06.960 --> 02:51:07.960]   Those are rest of mine.
[02:51:07.960 --> 02:51:08.960]   No that's exactly.
[02:51:08.960 --> 02:51:10.960]   The performance of aectomy.
[02:51:10.960 --> 02:51:12.960]   No the power is up.
[02:51:12.960 --> 02:51:18.960]   Apparently fruit bats fight for a good position in the colony.
[02:51:18.960 --> 02:51:21.960]   And that's what they figured out they were doing.
[02:51:21.960 --> 02:51:22.960]   It looked like it.
[02:51:22.960 --> 02:51:24.960]   In fact that's what they were doing.
[02:51:24.960 --> 02:51:29.960]   Project SETI, the Sotation Translation Initiative, C-E-T-I,
[02:51:29.960 --> 02:51:33.960]   bringing together machine learning experts, machine biology.
[02:51:33.960 --> 02:51:38.960]   Actually, we need a machine to understand me.
[02:51:38.960 --> 02:51:43.960]   Learning experts, machine biologists, roboticists, linguists and cryptographers
[02:51:43.960 --> 02:51:47.960]   to detect what whales are saying to one another.
[02:51:47.960 --> 02:51:50.960]   I think this is a very, you want to hear some whales?
[02:51:50.960 --> 02:51:51.960]   Sure.
[02:51:51.960 --> 02:51:52.960]   Why not?
[02:51:52.960 --> 02:51:56.960]   I think this is a very, it's clicking.
[02:51:56.960 --> 02:52:01.960]   This is really, to me, a very interesting application of machine learning.
[02:52:01.960 --> 02:52:04.960]   Figure out what they're saying to each other.
[02:52:04.960 --> 02:52:08.960]   I love science because there's just people sitting around being like,
[02:52:08.960 --> 02:52:11.960]   "What are whales saying?" and they get to figure it out.
[02:52:11.960 --> 02:52:12.960]   Yeah.
[02:52:12.960 --> 02:52:15.960]   And then in theory, talk to the animals.
[02:52:15.960 --> 02:52:20.960]   That's the slight, that's where there's a thin line between science and drug use.
[02:52:20.960 --> 02:52:22.960]   Right, yeah, yeah.
[02:52:22.960 --> 02:52:27.960]   There's like, "Man, what do you think whales are talking about, man?"
[02:52:27.960 --> 02:52:30.960]   According to the New York Times, the prospect of ongoing two-way dialogue
[02:52:30.960 --> 02:52:36.960]   with other species remains unknown, but true conversations will require a number of prerequisites,
[02:52:36.960 --> 02:52:42.960]   including matching intelligence types, compatible sensory systems,
[02:52:42.960 --> 02:52:45.960]   and crucially a shared desire to chat.
[02:52:45.960 --> 02:52:50.960]   Maybe the whales don't want to talk to us, and I cannot blame them for that.
[02:52:50.960 --> 02:52:54.960]   There has to be motivation on both sides to want to communicate.
[02:52:54.960 --> 02:52:55.960]   In your TikToks.
[02:52:55.960 --> 02:52:59.960]   This is Natalie Womini, an expert on cognitive evolution.
[02:52:59.960 --> 02:53:04.960]   Again, at the Max Planck Institute, but this time for evolutionary anthropology,
[02:53:04.960 --> 02:53:06.960]   Max Planck got around.
[02:53:06.960 --> 02:53:09.960]   And also there's Epp...
[02:53:09.960 --> 02:53:11.960]   Not like Elon Musk.
[02:53:11.960 --> 02:53:13.960]   That's a different kind.
[02:53:13.960 --> 02:53:15.960]   He was busy in a different fashion.
[02:53:15.960 --> 02:53:20.960]   There are two genres Max Planck and Elon Musk.
[02:53:20.960 --> 02:53:22.960]   Inside all of us are two wolves.
[02:53:22.960 --> 02:53:23.960]   Two wolves.
[02:53:23.960 --> 02:53:24.960]   Max Planck's Planck.
[02:53:24.960 --> 02:53:25.960]   And Elon Musk.
[02:53:25.960 --> 02:53:26.960]   It is.
[02:53:26.960 --> 02:53:28.960]   They are archetypes for us all.
[02:53:28.960 --> 02:53:39.960]   Mr. Glenn Fleischman, Auto-Didact, Jeopardy contestant, creator of Amazing Letter Press.
[02:53:39.960 --> 02:53:40.960]   What is this here?
[02:53:40.960 --> 02:53:41.960]   What is this?
[02:53:41.960 --> 02:53:42.960]   One fancier.
[02:53:42.960 --> 02:53:43.960]   A flog.
[02:53:43.960 --> 02:53:44.960]   He's a flong fanfare.
[02:53:44.960 --> 02:53:45.960]   Yes.
[02:53:45.960 --> 02:53:46.960]   Some Seattle Star.
[02:53:46.960 --> 02:53:47.960]   I need to speak for this.
[02:53:47.960 --> 02:53:49.960]   I've been out of business for 70 years.
[02:53:49.960 --> 02:53:50.960]   Flong.
[02:53:50.960 --> 02:53:51.960]   And a mystery I have.
[02:53:51.960 --> 02:53:52.960]   That's from World War II.
[02:53:52.960 --> 02:53:53.960]   MacArthur faces all out.
[02:53:53.960 --> 02:54:02.960]   The confusing part is this flong is a two-page tabloid-style flong.
[02:54:02.960 --> 02:54:11.960]   But on the same purchase I was able to obtain a full-page broad sheet flong for that style.
[02:54:11.960 --> 02:54:15.960]   Where does one go to acquire flongs?
[02:54:15.960 --> 02:54:16.960]   Well, eBay.
[02:54:16.960 --> 02:54:21.960]   But then sometimes things happen like a guy from Sweden says, "Hey, I've got a couple hundred pieces of peanuts flong.
[02:54:21.960 --> 02:54:22.960]   You want it?"
[02:54:22.960 --> 02:54:25.960]   They open up their coat and they're like, "You want some flong?"
[02:54:25.960 --> 02:54:26.960]   That's a good question.
[02:54:26.960 --> 02:54:28.960]   You use it as flong.
[02:54:28.960 --> 02:54:30.960]   Flong floggers or?
[02:54:30.960 --> 02:54:31.960]   Flong flong.
[02:54:31.960 --> 02:54:33.960]   One time no see.
[02:54:33.960 --> 02:54:40.960]   You better explain for those who are not completely up on antiquated newspaper technology what a flong is.
[02:54:40.960 --> 02:54:42.960]   Just going to always show you the flong.
[02:54:42.960 --> 02:54:44.960]   Not far by my Caribbean.
[02:54:44.960 --> 02:54:45.960]   It's a flong.
[02:54:45.960 --> 02:54:47.960]   I have a light switch on my lamp.
[02:54:47.960 --> 02:54:50.960]   Oh, that's a good name for it.
[02:54:50.960 --> 02:54:51.960]   It's a flong.
[02:54:51.960 --> 02:54:55.960]   A mold used in metal type days when printing was all.
[02:54:55.960 --> 02:54:57.960]   Say porn led into that mold?
[02:54:57.960 --> 02:54:58.960]   Exactly.
[02:54:58.960 --> 02:55:04.960]   They'd make the flong under high pressure from a blade-out page of disparate elements like pieces of type in illustrations.
[02:55:04.960 --> 02:55:18.960]   Then they'd put that in a press to create a single sheet that they could then cast into a hemispherical or half a circular metal plate that could go on a high-speed rotary press and spin really fast and print vast numbers of newspapers every hour.
[02:55:18.960 --> 02:55:27.960]   In every newspaper in the country they had dozens of people laying out pages, making flongs, making these plates, putting on presses.
[02:55:27.960 --> 02:55:33.960]   It was an incredibly wild amount of lead pouring.
[02:55:33.960 --> 02:55:38.960]   It's a crazy industrial operation that every newspaper had to do until about the 70s or 80s.
[02:55:38.960 --> 02:55:43.960]   Then it was utterly thrown out in favor of a simpler photographic process that everyone uses now.
[02:55:43.960 --> 02:55:44.960]   It's sad though.
[02:55:44.960 --> 02:55:47.960]   Thank goodness you're keeping the flong alive.
[02:55:47.960 --> 02:55:50.960]   It's a little aspect of it in history and documentary.
[02:55:50.960 --> 02:55:52.960]   Glenn always a pleasure.
[02:55:52.960 --> 02:55:53.960]   Glenn F Dot...
[02:55:53.960 --> 02:55:55.960]   I'm sorry, Glenn Dot Fund two ends.
[02:55:55.960 --> 02:55:57.960]   God, you had this is your last opportunity.
[02:55:57.960 --> 02:55:59.960]   I screwed it up every single time.
[02:55:59.960 --> 02:56:00.960]   Glenn Dot Fund.
[02:56:00.960 --> 02:56:01.960]   Easy to find me.
[02:56:01.960 --> 02:56:02.960]   Glenn Dot Fund.
[02:56:02.960 --> 02:56:03.960]   Glenn Dot Fund.
[02:56:03.960 --> 02:56:05.960]   And there's a lot of good stuff there.
[02:56:05.960 --> 02:56:06.960]   It's so great to see you.
[02:56:06.960 --> 02:56:07.960]   Thank you for being here.
[02:56:07.960 --> 02:56:08.960]   Thanks, Tommy.
[02:56:08.960 --> 02:56:09.960]   Dan Morin.
[02:56:09.960 --> 02:56:12.960]   Bayer in agenda.
[02:56:12.960 --> 02:56:18.960]   Alif extraction and the newest the Nova incident all part of the Galactic Cold War saga.
[02:56:18.960 --> 02:56:20.960]   Great reading.
[02:56:20.960 --> 02:56:22.960]   I could vouch for it really good.
[02:56:22.960 --> 02:56:24.960]   Surprisingly good, really.
[02:56:24.960 --> 02:56:28.960]   Again, thank you.
[02:56:28.960 --> 02:56:34.960]   Chocks that a man like you could shrink a sentence together.
[02:56:34.960 --> 02:56:35.960]   Really, really.
[02:56:35.960 --> 02:56:36.960]   Look at you.
[02:56:36.960 --> 02:56:38.960]   I see nothing behind those eyes.
[02:56:38.960 --> 02:56:39.960]   It's remarkable.
[02:56:39.960 --> 02:56:41.960]   I appreciate it.
[02:56:41.960 --> 02:56:43.960]   Sixcolors.com, of course.
[02:56:43.960 --> 02:56:44.960]   Anything.
[02:56:44.960 --> 02:56:46.960]   What are you going to be doing on Tuesday?
[02:56:46.960 --> 02:56:48.960]   You're going to be doing a live thing or?
[02:56:48.960 --> 02:56:50.960]   Jason will be there at the actual event.
[02:56:50.960 --> 02:56:52.960]   So I think he'll be doing some of the heavy lifting,
[02:56:52.960 --> 02:56:54.960]   but we'll have some stuff afterwards on the site.
[02:56:54.960 --> 02:56:58.960]   And I think we may actually, we started doing some video like wrap ups afterwards.
[02:56:58.960 --> 02:57:00.960]   Just sort of quick hit things to sort of discuss.
[02:57:00.960 --> 02:57:02.960]   So we might have one of those going up.
[02:57:02.960 --> 02:57:06.960]   Then there'll be plenty more coverage to come on six colors.
[02:57:06.960 --> 02:57:07.960]   Excellent.
[02:57:07.960 --> 02:57:08.960]   Excellent.
[02:57:08.960 --> 02:57:14.960]   And of course, the wonderful Paris Martenau, the crafty Paris Martenau.
[02:57:14.960 --> 02:57:16.960]   More ways than one in the information.com.
[02:57:16.960 --> 02:57:18.960]   There's your signal number.
[02:57:18.960 --> 02:57:20.960]   She's at Paris Martenau on the Twitter.
[02:57:20.960 --> 02:57:22.960]   What are you working right now?
[02:57:22.960 --> 02:57:23.960]   Anything exciting?
[02:57:23.960 --> 02:57:27.960]   I cover Amazon right about different parts of the business.
[02:57:27.960 --> 02:57:31.960]   If you work for Amazon or you used to, come chat with me on the signal.
[02:57:31.960 --> 02:57:32.960]   Talk off the record.
[02:57:32.960 --> 02:57:33.960]   Be really fun.
[02:57:33.960 --> 02:57:35.960]   Kind of like this chat, but also about your work.
[02:57:35.960 --> 02:57:36.960]   Nice.
[02:57:36.960 --> 02:57:40.960]   Are you covering Amazon's, all the unionization stuff, I assume?
[02:57:40.960 --> 02:57:41.960]   Yeah.
[02:57:41.960 --> 02:57:45.960]   I mean, that as well as I'm really interested in kind of the movements and health care right
[02:57:45.960 --> 02:57:50.960]   now as well as, I mean, I think everything going on with Project Santos.
[02:57:50.960 --> 02:57:51.960]   It's a massive company.
[02:57:51.960 --> 02:57:55.960]   And I feel like every time I look in a different part of it, I find something more interesting.
[02:57:55.960 --> 02:57:56.960]   So.
[02:57:56.960 --> 02:57:59.960]   Always a pleasure.
[02:57:59.960 --> 02:58:01.960]   Thanks to all three of you.
[02:58:01.960 --> 02:58:02.960]   This was so much fun.
[02:58:02.960 --> 02:58:03.960]   I hate to stop.
[02:58:03.960 --> 02:58:04.960]   I really do.
[02:58:04.960 --> 02:58:11.960]   We do Twitch every Sunday about 2 p.m. Pacific, 5 p.m. Eastern 2100 UTC.
[02:58:11.960 --> 02:58:15.960]   So you can watch us do it live at live.tuit.tv or chat with us live while you're watching
[02:58:15.960 --> 02:58:20.960]   an IRC.tuit.tv club members get to chat in our club Discord.
[02:58:20.960 --> 02:58:22.960]   If you're not a club member, 7 bucks a month.
[02:58:22.960 --> 02:58:23.960]   Let me do a little pitch for this.
[02:58:23.960 --> 02:58:27.960]   7 bucks a month gets you ad free versions of all the shows.
[02:58:27.960 --> 02:58:31.960]   You get access to the Discord, which is always full of fascinating stuff.
[02:58:31.960 --> 02:58:34.960]   We do events all the time.
[02:58:34.960 --> 02:58:37.960]   We have a fireside chat coming up September 22 featuring club Twit members.
[02:58:37.960 --> 02:58:41.960]   Stacey Higginbotham's book club, the untitled Linux show.
[02:58:41.960 --> 02:58:43.960]   Hands on Mac with Micah Sargent.
[02:58:43.960 --> 02:58:45.960]   Hands on Windows with Paul Therat.
[02:58:45.960 --> 02:58:47.960]   There's just a ton of good stuff in there.
[02:58:47.960 --> 02:58:50.960]   And all those shows also appear on the Twit Plus feed.
[02:58:50.960 --> 02:58:54.960]   So I invite you to join us in the club Twit.
[02:58:54.960 --> 02:58:58.960]   Just go to twit.tv/clubduit for information.
[02:58:58.960 --> 02:59:00.960]   7 bucks a month. I think it's a good deal.
[02:59:00.960 --> 02:59:01.960]   And it helps us out.
[02:59:01.960 --> 02:59:06.960]   Helps us smooth out the rough edges in the ad world.
[02:59:06.960 --> 02:59:12.960]   After the fact, of course, that we offer on-demand versions ad supported of all of our shows at Twit.tv.
[02:59:12.960 --> 02:59:14.960]   You can also go to YouTube.
[02:59:14.960 --> 02:59:18.960]   Each show has its own YouTube channel, youtube.com/thisweekintech.
[02:59:18.960 --> 02:59:22.960]   There's also, of course, the best way to get it.
[02:59:22.960 --> 02:59:25.960]   There's a chance to subscribe in your favorite podcast player.
[02:59:25.960 --> 02:59:27.960]   That's free despite the name.
[02:59:27.960 --> 02:59:29.960]   It's not on Confuses people, but it's free.
[02:59:29.960 --> 02:59:33.960]   Just follow us or subscribe to us and you'll get it the minute it's available.
[02:59:33.960 --> 02:59:35.960]   So you haven't time for your Monday morning commute.
[02:59:35.960 --> 02:59:37.960]   I hope you don't commute tomorrow because it's Labor Day.
[02:59:37.960 --> 02:59:40.960]   Have a great Labor Day weekend.
[02:59:40.960 --> 02:59:42.960]   Go out and barbecue.
[02:59:42.960 --> 02:59:45.960]   Don't forget no more white pants or white shoes after tomorrow.
[02:59:45.960 --> 02:59:47.960]   This is it. Last chance.
[02:59:47.960 --> 02:59:49.960]   Get those shoes on.
[02:59:49.960 --> 02:59:50.960]   And we'll see you next time.
[02:59:50.960 --> 02:59:52.960]   Another Twit is in the can.
[02:59:52.960 --> 02:59:53.960]   Bye-bye.
[02:59:53.960 --> 03:00:03.960]   [music]
[03:00:03.960 --> 03:00:04.800]   Baby.

