;FFMETADATA1
title=Autonowashing
artist=Leo Laporte, Brian McCullough, Sam Abuelsamid, Shelly Brisbin
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-02-12
track=914
language=English
genre=Podcast
comment=Microsoft AI Search, CPU Decline, Google Bard
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.340]   It's time for Twit this weekend Tech.
[00:00:02.340 --> 00:00:04.640]   Sam and Bull Sam and my car guy is here.
[00:00:04.640 --> 00:00:07.780]   So is Brian McCullough from the Tech meme ride home.
[00:00:07.780 --> 00:00:09.320]   But it's not just cars, we're also gonna talk
[00:00:09.320 --> 00:00:10.380]   about floating objects.
[00:00:10.380 --> 00:00:14.480]   So Shelley Brisbane is here from sixcolors.com.
[00:00:14.480 --> 00:00:15.960]   We'll talk about the Chinese balloons,
[00:00:15.960 --> 00:00:17.720]   that tech payload they might have had,
[00:00:17.720 --> 00:00:19.860]   and why six Chinese companies now ban
[00:00:19.860 --> 00:00:21.660]   from doing business in the US.
[00:00:21.660 --> 00:00:24.240]   We'll talk about Sergey Brin's blimp,
[00:00:24.240 --> 00:00:25.560]   or is it a Zeppelin?
[00:00:25.560 --> 00:00:27.080]   I know it's a derigible.
[00:00:27.080 --> 00:00:31.520]   And then chat GPT, Microsoft's all in on Bing,
[00:00:31.520 --> 00:00:34.720]   and Google, well they might have fumbled their launch.
[00:00:34.720 --> 00:00:37.600]   That and a lot more coming up next on Twit.
[00:00:37.600 --> 00:00:43.880]   - Podcasts you love from people you trust.
[00:00:43.880 --> 00:00:46.640]   - This is Twit.
[00:00:46.640 --> 00:00:55.200]   - This is Twit, this week at Tech,
[00:00:55.200 --> 00:01:00.200]   episode 914, recorded Saturday, February 11th, 2023.
[00:01:00.200 --> 00:01:02.840]   Autano Washington.
[00:01:02.840 --> 00:01:05.440]   This week at Tech is brought to you by Bitwarden.
[00:01:05.440 --> 00:01:07.800]   Get the password manager that offers a robust
[00:01:07.800 --> 00:01:09.640]   and cost effective solution,
[00:01:09.640 --> 00:01:12.120]   that can drastically increase your chances
[00:01:12.120 --> 00:01:14.120]   of staying safe online.
[00:01:14.120 --> 00:01:15.960]   Get started with a free trial of a team's
[00:01:15.960 --> 00:01:18.440]   or enterprise plan, or get started for free
[00:01:18.440 --> 00:01:20.960]   across all devices as an individual user
[00:01:20.960 --> 00:01:24.120]   at bitwarden.com/twit.
[00:01:24.120 --> 00:01:26.280]   And by ZipRecruiter.
[00:01:26.280 --> 00:01:28.240]   Are you hiring for your team?
[00:01:28.240 --> 00:01:30.640]   Despite current headlines, several industries
[00:01:30.640 --> 00:01:32.600]   like hospitality and healthcare,
[00:01:32.600 --> 00:01:35.280]   are heading for a hiring boom.
[00:01:35.280 --> 00:01:36.560]   No matter what industry you're in,
[00:01:36.560 --> 00:01:40.720]   if you need to hire, go to ziprecruiter.com/twit
[00:01:40.720 --> 00:01:41.920]   and try it for free.
[00:01:41.920 --> 00:01:45.840]   Thanks for listening to this show.
[00:01:45.840 --> 00:01:47.960]   As an ad supported network,
[00:01:47.960 --> 00:01:50.080]   we are always looking for new partners
[00:01:50.080 --> 00:01:52.480]   with products and services that will benefit
[00:01:52.480 --> 00:01:54.800]   our qualified audience.
[00:01:54.800 --> 00:01:56.400]   Are you ready to grow your business?
[00:01:56.400 --> 00:01:59.360]   Reach out to advertise at twit.tv
[00:01:59.360 --> 00:02:00.800]   and launch your campaign now.
[00:02:00.800 --> 00:02:03.380]   (upbeat music)
[00:02:03.380 --> 00:02:08.920]   It's time for Twit this week at Tech.
[00:02:08.920 --> 00:02:12.000]   The show where we talk about the weeks, Tech News.
[00:02:12.000 --> 00:02:15.160]   I have assembled as always a panel of excellent people
[00:02:15.160 --> 00:02:18.680]   who are willing to get together on a Saturday
[00:02:18.680 --> 00:02:21.680]   because apparently there's something going on tomorrow,
[00:02:21.680 --> 00:02:23.720]   some big football game.
[00:02:23.720 --> 00:02:25.120]   Samable, Samit is here.
[00:02:25.120 --> 00:02:30.120]   He's rooting for the Michigan steam boilers or something.
[00:02:30.120 --> 00:02:30.960]   I don't know.
[00:02:30.960 --> 00:02:34.600]   There's a trade lions, are you a lions fan?
[00:02:34.600 --> 00:02:35.920]   I am not a lions fan.
[00:02:35.920 --> 00:02:38.680]   No, I'm not into pain.
[00:02:38.680 --> 00:02:39.560]   I don't like to suffer.
[00:02:39.560 --> 00:02:40.720]   That would be suffering, wouldn't it?
[00:02:40.720 --> 00:02:41.560]   Yes.
[00:02:41.560 --> 00:02:43.000]   Sam is our card guy.
[00:02:43.000 --> 00:02:45.720]   I think they last won a championship about 60 years ago.
[00:02:45.720 --> 00:02:47.640]   I was rooting for him.
[00:02:47.640 --> 00:02:48.720]   I like them.
[00:02:48.720 --> 00:02:50.080]   I like an underdog.
[00:02:50.080 --> 00:02:52.640]   Wheel bearings podcast at Wheel bearings.media.
[00:02:52.640 --> 00:02:54.560]   Regular on our tech guy show.
[00:02:54.560 --> 00:02:56.440]   Now on the new Ask the Tech guys.
[00:02:56.440 --> 00:02:59.120]   And on Twit Social as Samable, Samit.
[00:02:59.120 --> 00:03:01.040]   Hello, Sam.
[00:03:01.040 --> 00:03:02.440]   Hello, everybody.
[00:03:02.440 --> 00:03:03.800]   Got lots of card questions for you.
[00:03:03.800 --> 00:03:05.480]   But we'll get that in a moment.
[00:03:05.480 --> 00:03:07.520]   First I want to say hello to Brian McCullough,
[00:03:07.520 --> 00:03:09.920]   host of the Tech meme ride home podcast.
[00:03:09.920 --> 00:03:12.640]   You probably listened to him in your car.
[00:03:12.640 --> 00:03:13.480]   Hello, Brian.
[00:03:13.480 --> 00:03:14.640]   Good to see you.
[00:03:14.640 --> 00:03:15.480]   Hello, Leo.
[00:03:15.480 --> 00:03:16.960]   As always, good to see you.
[00:03:16.960 --> 00:03:22.400]   And I guess I'm going to ask you if anything happened this week
[00:03:22.400 --> 00:03:24.080]   because I don't have much.
[00:03:24.080 --> 00:03:24.760]   That's OK.
[00:03:24.760 --> 00:03:25.760]   That's OK.
[00:03:25.760 --> 00:03:28.920]   I've been filling the spreadsheet with some things.
[00:03:28.920 --> 00:03:29.440]   Fill it up.
[00:03:29.440 --> 00:03:30.400]   Fill it up.
[00:03:30.400 --> 00:03:32.680]   Hey, I'm thrilled to see Shelly Brisbane here.
[00:03:32.680 --> 00:03:35.040]   She is a producer and reporter at Texas Standard.
[00:03:35.040 --> 00:03:39.120]   You see her at sixcolors.com and on many incomparable podcasts.
[00:03:39.120 --> 00:03:40.880]   Hello, Shelly.
[00:03:40.880 --> 00:03:41.560]   Hi, Leo.
[00:03:41.560 --> 00:03:43.520]   Thanks for having me back.
[00:03:43.520 --> 00:03:45.800]   I've been meaning to get you back for a long time.
[00:03:45.800 --> 00:03:47.080]   I'm glad you could be on the show.
[00:03:47.080 --> 00:03:47.720]   A lot of you.
[00:03:47.720 --> 00:03:48.640]   Today.
[00:03:48.640 --> 00:03:52.160]   And all three of you will be watching the Super Bowl tomorrow.
[00:03:52.160 --> 00:03:54.720]   Along with me, I've been informed
[00:03:54.720 --> 00:03:57.880]   that we have 10 people coming over.
[00:03:57.880 --> 00:03:59.320]   And that I will be barbecuing.
[00:03:59.320 --> 00:04:01.760]   So I'm glad I'm doing the show on Saturday.
[00:04:01.760 --> 00:04:04.200]   For those of you watching, yes, a rare Saturday edition
[00:04:04.200 --> 00:04:04.960]   of these week in tech.
[00:04:04.960 --> 00:04:07.320]   If something happens Saturday night,
[00:04:07.320 --> 00:04:09.920]   if there's another Chinese balloon discovered,
[00:04:09.920 --> 00:04:13.920]   we don't know about it yet because we're off by a day.
[00:04:13.920 --> 00:04:17.280]   Actually, there wasn't a Chinese balloon discovered.
[00:04:17.280 --> 00:04:18.200]   Right.
[00:04:18.200 --> 00:04:19.560]   They're everywhere.
[00:04:19.560 --> 00:04:21.240]   How many?
[00:04:21.240 --> 00:04:24.960]   I don't really understand what's going on.
[00:04:24.960 --> 00:04:28.160]   And of course, we haven't heard from the Department of Defense
[00:04:28.160 --> 00:04:28.960]   what they found.
[00:04:28.960 --> 00:04:31.360]   They did recover the balloon they shot down over North Carolina.
[00:04:31.360 --> 00:04:32.600]   I don't think we've heard anything
[00:04:32.600 --> 00:04:34.160]   about what was on that balloon.
[00:04:34.160 --> 00:04:37.200]   Another one was shot down yesterday in Alaska.
[00:04:37.200 --> 00:04:38.480]   And I guess there's another one.
[00:04:41.520 --> 00:04:45.040]   They're attacking us with balloons.
[00:04:45.040 --> 00:04:47.040]   What kind of technology could you put on the balloon
[00:04:47.040 --> 00:04:53.920]   that your 26 satellites wouldn't tell you what's going on?
[00:04:53.920 --> 00:04:59.480]   Is it harder to do listening of some kind from outer space?
[00:04:59.480 --> 00:05:01.600]   It would seem like it would be easier
[00:05:01.600 --> 00:05:07.200]   because a lot of the signals intelligence, that stuff's
[00:05:07.200 --> 00:05:10.520]   going through satellites anyway.
[00:05:10.520 --> 00:05:12.640]   And it's all encrypted.
[00:05:12.640 --> 00:05:13.640]   Yeah.
[00:05:13.640 --> 00:05:15.600]   So it would seem like the satellites
[00:05:15.600 --> 00:05:18.120]   should be able to pick that up without too much difficulty
[00:05:18.120 --> 00:05:23.160]   and probably more consistently than a balloon.
[00:05:23.160 --> 00:05:25.680]   The balloon, you don't really know precisely
[00:05:25.680 --> 00:05:27.720]   what its trajectory is going to be.
[00:05:27.720 --> 00:05:29.720]   You know where the satellite's going to be.
[00:05:29.720 --> 00:05:32.960]   And if it's geostationary, it's not going anywhere.
[00:05:32.960 --> 00:05:35.560]   But that's also like the poker tell in this.
[00:05:35.560 --> 00:05:37.760]   There must be something incredibly valuable
[00:05:37.760 --> 00:05:42.160]   because it's so easy to have been discovered.
[00:05:42.160 --> 00:05:44.040]   You know what I mean?
[00:05:44.040 --> 00:05:47.200]   And if it loses control and because the wind blows it
[00:05:47.200 --> 00:05:49.000]   where it can be seen and stuff like that,
[00:05:49.000 --> 00:05:51.720]   there must be something really, really useful
[00:05:51.720 --> 00:05:56.120]   that they're willing to take this risk of discovery.
[00:05:56.120 --> 00:05:57.360]   Hmm.
[00:05:57.360 --> 00:05:59.840]   Or are we learning about how they discover it?
[00:05:59.840 --> 00:06:02.800]   You know, poke the bear and just try to get us riled up.
[00:06:02.800 --> 00:06:04.440]   There's also a possibility, isn't it?
[00:06:04.440 --> 00:06:04.800]   Oh, isn't that?
[00:06:04.800 --> 00:06:07.080]   Although why would you want to do saber rattling?
[00:06:07.080 --> 00:06:10.520]   I mean, it's terrifying to me to think of us going to war
[00:06:10.520 --> 00:06:11.960]   with a nuclear power.
[00:06:11.960 --> 00:06:14.640]   I mean, why would you want to stir up trouble?
[00:06:14.640 --> 00:06:19.840]   And this is, by the way, it's inevitable
[00:06:19.840 --> 00:06:20.960]   the balloons would be seen.
[00:06:20.960 --> 00:06:22.720]   I mean, it wasn't, yeah.
[00:06:22.720 --> 00:06:23.880]   As you said, Brian, it's interesting.
[00:06:23.880 --> 00:06:24.840]   Well, and the reason that we know about it
[00:06:24.840 --> 00:06:26.600]   is because civilians saw them.
[00:06:26.600 --> 00:06:28.040]   And I don't think the Defense Department
[00:06:28.040 --> 00:06:29.240]   has known about them for a long time,
[00:06:29.240 --> 00:06:31.400]   but they haven't been sharing that with us.
[00:06:31.400 --> 00:06:34.440]   And that benefits them from a propaganda point of view
[00:06:34.440 --> 00:06:36.520]   as well as just from not letting China know
[00:06:36.520 --> 00:06:38.840]   what we know, but people were seeing them,
[00:06:38.840 --> 00:06:40.760]   especially when it was up in Montana.
[00:06:40.760 --> 00:06:42.200]   And so there came a point where they were like,
[00:06:42.200 --> 00:06:43.520]   okay, we have to talk about it.
[00:06:43.520 --> 00:06:45.280]   And then we have to shoot it down.
[00:06:45.280 --> 00:06:47.000]   I'm bringing up, 'cause I feel like it is,
[00:06:47.000 --> 00:06:48.040]   there is a tech story here.
[00:06:48.040 --> 00:06:52.560]   Obviously, whatever that three bus long electronics panel
[00:06:52.560 --> 00:06:55.800]   that was suspended from the North Carolina balloon was,
[00:06:55.800 --> 00:06:57.880]   you know, it was high tech.
[00:06:57.880 --> 00:07:00.440]   And I mean, I don't know if it was for surveillance
[00:07:00.440 --> 00:07:01.520]   or weather measurements.
[00:07:01.520 --> 00:07:03.400]   We don't, we don't really know,
[00:07:03.400 --> 00:07:05.040]   'cause nobody's said yet.
[00:07:05.040 --> 00:07:08.200]   I wish that, I guess it's unlikely that
[00:07:08.200 --> 00:07:09.440]   the Department of Defense will say,
[00:07:09.440 --> 00:07:11.880]   well, here's, you know, the bill of goods that was on there.
[00:07:11.880 --> 00:07:15.120]   They have banned some Chinese companies
[00:07:15.120 --> 00:07:19.040]   who they say provided technology used on the balloon.
[00:07:19.040 --> 00:07:24.520]   So maybe we kind of do know something, I don't know.
[00:07:24.520 --> 00:07:27.280]   - Doesn't, the Biden administration kind of has priors
[00:07:27.280 --> 00:07:30.880]   for trying to out what's happening ahead of time.
[00:07:30.880 --> 00:07:33.680]   Like remember before Russia invaded Ukraine,
[00:07:33.680 --> 00:07:36.240]   they were saying Russia was gonna invade Ukraine
[00:07:36.240 --> 00:07:38.280]   so that there couldn't be a false flag thing.
[00:07:38.280 --> 00:07:39.120]   - Ah.
[00:07:39.120 --> 00:07:42.600]   - So if your theory is that this has always been going on,
[00:07:42.600 --> 00:07:44.280]   but maybe for whatever reason,
[00:07:44.280 --> 00:07:46.040]   the US government is letting everyone know
[00:07:46.040 --> 00:07:47.840]   what's going on now.
[00:07:47.840 --> 00:07:49.320]   - Maybe that's it.
[00:07:49.320 --> 00:07:53.760]   So six Chinese companies were blacklisted yesterday
[00:07:53.760 --> 00:07:55.800]   for supporting the balloon program.
[00:07:55.800 --> 00:07:57.880]   (laughs)
[00:07:57.880 --> 00:08:01.440]   The companies, this is from, I think Reuters,
[00:08:01.440 --> 00:08:03.120]   the companies and organizations which have allegedly
[00:08:03.120 --> 00:08:06.240]   supported China's aerospace programs to develop airships
[00:08:06.240 --> 00:08:08.920]   and balloons for intelligence and reconnaissance efforts
[00:08:08.920 --> 00:08:12.280]   are banned from obtaining US items and technology.
[00:08:12.280 --> 00:08:13.840]   They've been added to that entity list
[00:08:13.840 --> 00:08:15.880]   while Wei is on and other companies
[00:08:15.880 --> 00:08:18.280]   so that American companies can't buy from them
[00:08:18.280 --> 00:08:19.920]   and they more importantly, probably to them,
[00:08:19.920 --> 00:08:22.720]   they can't buy from American companies.
[00:08:22.720 --> 00:08:27.160]   I don't see a list of the names of the companies.
[00:08:27.160 --> 00:08:29.240]   'Cause I thought it was--
[00:08:29.240 --> 00:08:31.000]   - I was wondering, yeah, I would,
[00:08:31.000 --> 00:08:32.720]   but I always wonder when that happens,
[00:08:32.720 --> 00:08:37.120]   what tech initiative where is inadvertently affected?
[00:08:37.120 --> 00:08:39.400]   I'm not saying it's good or bad that they were banned.
[00:08:39.400 --> 00:08:41.920]   I'm just saying some company somewhere was doing business
[00:08:41.920 --> 00:08:43.920]   with a banned company and is now going,
[00:08:43.920 --> 00:08:47.320]   oh, well, we're screwed now, at least temporarily.
[00:08:47.320 --> 00:08:49.280]   And in a way that you wouldn't expect
[00:08:49.280 --> 00:08:50.800]   or that probably wouldn't make the news.
[00:08:50.800 --> 00:08:51.640]   - Right.
[00:08:51.640 --> 00:08:56.400]   Yeah, at least in the industry that I primarily cover,
[00:08:56.400 --> 00:08:59.480]   which is automotive, increasingly we're seeing
[00:08:59.480 --> 00:09:04.480]   the auto industry trying to migrate away from
[00:09:04.480 --> 00:09:08.720]   too much dependence on China.
[00:09:08.720 --> 00:09:10.120]   'Cause they're seeing what's been happening
[00:09:10.120 --> 00:09:14.520]   in these other sectors, in chips and various other things
[00:09:14.520 --> 00:09:19.520]   and telecoms over the last five years or so, five, six years.
[00:09:19.520 --> 00:09:24.640]   And they increasingly seem to want to diversify
[00:09:24.640 --> 00:09:27.000]   their supply chain away from,
[00:09:27.000 --> 00:09:29.680]   and their customers away from China
[00:09:29.680 --> 00:09:32.200]   and not be overly dependent on the China market.
[00:09:32.200 --> 00:09:33.360]   - Reasonably. - Quite reasonably.
[00:09:33.360 --> 00:09:35.720]   I mean, it's probably a matter of time
[00:09:35.720 --> 00:09:37.760]   before we get iPhones that are not made in China,
[00:09:37.760 --> 00:09:40.840]   maybe made in India or Brazil or Vietnam.
[00:09:40.840 --> 00:09:44.120]   But I think Apple would like to move away as promptly as possible.
[00:09:44.120 --> 00:09:48.440]   I did get the list of six companies from MarketWatch,
[00:09:48.440 --> 00:09:51.840]   the Beijing Nanjung Aerospace Technology Company,
[00:09:51.840 --> 00:09:54.280]   China Electronics Technology Group Corporation,
[00:09:54.280 --> 00:09:56.600]   48th Research Institute.
[00:09:56.600 --> 00:10:00.360]   The Dongguan, Lingkong, remote sensing technology company.
[00:10:00.360 --> 00:10:03.400]   They didn't really try to hide what they do, do they?
[00:10:03.400 --> 00:10:07.160]   The Eagles, Men Aviation Science and Technology Group,
[00:10:07.160 --> 00:10:11.320]   the Guangzhou, Tianhai, Xiang Aviation Technology Company
[00:10:11.320 --> 00:10:14.960]   and the Shang-Chi Eagles, Men Aviation Science
[00:10:14.960 --> 00:10:15.800]   and Technology Group.
[00:10:15.800 --> 00:10:18.440]   Wait a minute, that's the same name, different company.
[00:10:18.440 --> 00:10:19.720]   Okay.
[00:10:19.720 --> 00:10:24.720]   So aerospace companies, aerospace companies.
[00:10:24.720 --> 00:10:26.240]   - And sensors. - And sensors.
[00:10:26.240 --> 00:10:28.160]   - Yeah, sensors is a big one.
[00:10:28.160 --> 00:10:31.440]   - Which could affect a lot of different industries or not.
[00:10:31.440 --> 00:10:33.040]   I mean, I don't know what sort of sensors they are,
[00:10:33.040 --> 00:10:35.600]   but sensors aren't everything of different.
[00:10:35.600 --> 00:10:39.600]   - There's probably a variety of imaging sensors,
[00:10:39.600 --> 00:10:43.600]   both infrared and visible light imaging
[00:10:43.600 --> 00:10:46.680]   and probably maybe things like some LiDAR.
[00:10:46.680 --> 00:10:50.280]   Some very high resolution LiDAR,
[00:10:50.280 --> 00:10:55.280]   which is often used for aerial mapping purposes.
[00:10:55.840 --> 00:10:58.640]   So that may be one of the things that they're looking at
[00:10:58.640 --> 00:11:02.240]   is doing some aerial mapping with LiDAR.
[00:11:02.240 --> 00:11:08.080]   - NBC says the Chinese balloon had multiple antennas
[00:11:08.080 --> 00:11:10.720]   capable of collecting signals intelligence.
[00:11:10.720 --> 00:11:15.040]   This is a State Department statement on Thursday.
[00:11:15.040 --> 00:11:17.680]   The balloon maker has proven ties to the Chinese military.
[00:11:17.680 --> 00:11:21.720]   So we do know now, I think, it wasn't a weather balloon
[00:11:21.720 --> 00:11:24.560]   that it was some sort of surveillance balloon,
[00:11:24.560 --> 00:11:26.680]   particularly communications surveillance,
[00:11:26.680 --> 00:11:30.160]   which explain why you don't do a satellite.
[00:11:30.160 --> 00:11:32.240]   Now, just as you said, it's, you know,
[00:11:32.240 --> 00:11:33.960]   that's close to the ground.
[00:11:33.960 --> 00:11:38.440]   Photos taken by high altitude YouTube planes.
[00:11:38.440 --> 00:11:39.600]   See, we still got those.
[00:11:39.600 --> 00:11:41.440]   I'm sure the Chinese have those too.
[00:11:41.440 --> 00:11:42.880]   Confirm the presence of the equipment,
[00:11:42.880 --> 00:11:45.440]   including multiple antennas,
[00:11:45.440 --> 00:11:46.600]   according to the State Department,
[00:11:46.600 --> 00:11:50.080]   likely capable of collecting and geolocating communications.
[00:11:50.080 --> 00:11:52.760]   Solar panels, large enough to produce the requisite power
[00:11:52.760 --> 00:11:57.040]   to operate multiple active intelligence collection sensors.
[00:11:57.040 --> 00:12:00.680]   The equipment was inconsistent with a weather balloon.
[00:12:00.680 --> 00:12:03.600]   - Here's a funny thing to think about
[00:12:03.600 --> 00:12:05.760]   because obviously the joke is we're using,
[00:12:05.760 --> 00:12:08.040]   we're there using such old technology,
[00:12:08.040 --> 00:12:09.400]   oh, literally floating balloons.
[00:12:09.400 --> 00:12:12.760]   But think about this also goes back to like,
[00:12:12.760 --> 00:12:15.720]   like a naval warfare, you had to have the weather gauge.
[00:12:15.720 --> 00:12:17.720]   Like if we wanted to do something similar,
[00:12:17.720 --> 00:12:18.920]   if we're like, we're gonna retaliate
[00:12:18.920 --> 00:12:20.640]   and send balloons over China,
[00:12:20.640 --> 00:12:22.320]   where would we couldn't float them
[00:12:22.320 --> 00:12:24.640]   from the west coast over there, right?
[00:12:24.640 --> 00:12:27.720]   We'd have to float them from Russia.
[00:12:27.720 --> 00:12:29.080]   Wouldn't let us do that.
[00:12:29.080 --> 00:12:30.760]   - Alas. - Would we,
[00:12:30.760 --> 00:12:32.440]   you would probably launch and bomb--
[00:12:32.440 --> 00:12:33.280]   - Oh, now we have the basis
[00:12:33.280 --> 00:12:34.560]   in the middle of the east again.
[00:12:34.560 --> 00:12:37.720]   - Yeah, 'cause the jet stream is going west to east.
[00:12:37.720 --> 00:12:38.960]   - Oh yeah, yeah, yeah. - So you have to be--
[00:12:38.960 --> 00:12:39.800]   - That's my point. - High speed, yeah.
[00:12:39.800 --> 00:12:40.640]   - That's my point. - High speed.
[00:12:40.640 --> 00:12:41.480]   - That's my point. - That's my point.
[00:12:41.480 --> 00:12:43.280]   - Somewhere west or east of China. - Right.
[00:12:43.280 --> 00:12:45.280]   - No west of China. - So this is so old school
[00:12:45.280 --> 00:12:46.640]   that we can't do it in reverse
[00:12:46.640 --> 00:12:49.120]   because the weather, the wind doesn't blow that way.
[00:12:50.120 --> 00:12:52.680]   - Yeah. - Which is why it feels to me,
[00:12:52.680 --> 00:12:55.640]   like it may not even be that they are
[00:12:55.640 --> 00:12:57.960]   after precise pieces of information.
[00:12:57.960 --> 00:13:00.920]   They're sort of, it's beyond proof of concept,
[00:13:00.920 --> 00:13:02.240]   obviously they can do it,
[00:13:02.240 --> 00:13:04.880]   but you're going to see what you can get
[00:13:04.880 --> 00:13:06.600]   because you don't care specifically
[00:13:06.600 --> 00:13:09.000]   about where you're targeting.
[00:13:09.000 --> 00:13:10.440]   You're floating over Montana
[00:13:10.440 --> 00:13:12.480]   then eventually you're over North Carolina
[00:13:12.480 --> 00:13:15.840]   and in between you're gathering a lot of stuff
[00:13:15.840 --> 00:13:18.840]   that you can then later assimilate
[00:13:18.840 --> 00:13:21.640]   but it's more important that you be able to get here
[00:13:21.640 --> 00:13:24.640]   undetected or unshot down than it is
[00:13:24.640 --> 00:13:26.480]   that you hover over a particular place
[00:13:26.480 --> 00:13:28.960]   to get a particular piece of information.
[00:13:28.960 --> 00:13:30.960]   - Yeah, it seems like a not the ideal way
[00:13:30.960 --> 00:13:32.680]   to do these kinds of spy efforts,
[00:13:32.680 --> 00:13:35.800]   but I guess we, it can't steer itself.
[00:13:35.800 --> 00:13:37.920]   It didn't have propellers, it didn't have rudders,
[00:13:37.920 --> 00:13:39.920]   it just could go all the way down.
[00:13:39.920 --> 00:13:42.960]   - You know, is it possible that they have something
[00:13:42.960 --> 00:13:45.600]   similar to what Google was doing with their lunes?
[00:13:45.600 --> 00:13:48.640]   - The lunes, yeah. - Where they could adjust the--
[00:13:48.640 --> 00:13:50.040]   - That's where Google's doing. - Sure, the balloon.
[00:13:50.040 --> 00:13:51.920]   - Yeah. - You know, and adapt
[00:13:51.920 --> 00:13:54.880]   to the wind patterns, you know, so based on
[00:13:54.880 --> 00:13:57.920]   where the wind was, you know, they could go up or down
[00:13:57.920 --> 00:14:00.560]   and altitude to maybe catch some wind
[00:14:00.560 --> 00:14:04.000]   and have at least some modicum of control
[00:14:04.000 --> 00:14:05.560]   over the trajectory of the balloon.
[00:14:05.560 --> 00:14:06.400]   - Yeah.
[00:14:06.400 --> 00:14:08.680]   I don't know.
[00:14:08.680 --> 00:14:10.120]   Anyway, I guess there's a lot more questions
[00:14:10.120 --> 00:14:11.160]   than there are answers.
[00:14:11.160 --> 00:14:16.160]   I just, it scares me that I don't wanna,
[00:14:16.160 --> 00:14:18.320]   we don't wanna get in the hot war.
[00:14:18.320 --> 00:14:21.840]   I don't even think we should get in a cold war with China.
[00:14:21.840 --> 00:14:23.680]   I guess we should say knock it off.
[00:14:23.680 --> 00:14:28.000]   And I don't know, what's the answer to this?
[00:14:28.000 --> 00:14:31.520]   It's not good, it's not good.
[00:14:31.520 --> 00:14:33.000]   Do we send balloons over China?
[00:14:33.000 --> 00:14:37.000]   From Turkey or where, I don't know where we'd sign from.
[00:14:37.000 --> 00:14:40.360]   - Yeah, I mean, I have a hard time seeing
[00:14:40.360 --> 00:14:42.440]   why the US would need to.
[00:14:42.440 --> 00:14:45.480]   I mean, we've got enough satellites covering
[00:14:45.480 --> 00:14:47.200]   pretty much everywhere.
[00:14:47.200 --> 00:14:48.920]   - Well, China has TikTok.
[00:14:48.920 --> 00:14:50.960]   Don't they know?
[00:14:50.960 --> 00:14:53.760]   Don't they know everything anyway?
[00:14:53.760 --> 00:14:57.840]   It has, by the way, it has distracted certain,
[00:14:57.840 --> 00:15:02.080]   in certain quarters, distracted people from TikTok, right?
[00:15:02.080 --> 00:15:03.320]   More worried about the balloons now
[00:15:03.320 --> 00:15:04.240]   than they were about TikTok.
[00:15:04.240 --> 00:15:05.800]   Maybe that was the point.
[00:15:05.800 --> 00:15:06.640]   - I don't know.
[00:15:06.640 --> 00:15:09.040]   - Well, some of the same people are very exercised
[00:15:09.040 --> 00:15:11.280]   about the balloons as our exercise about TikTok.
[00:15:11.280 --> 00:15:12.560]   And that's what's unfortunate to me
[00:15:12.560 --> 00:15:15.440]   that it immediately became this political thing
[00:15:15.440 --> 00:15:16.960]   where if you're on one side,
[00:15:16.960 --> 00:15:18.080]   oh, balloons are fine.
[00:15:18.080 --> 00:15:20.200]   If you're on the other side, balloons are gonna destroy us
[00:15:20.200 --> 00:15:23.640]   and they had AK-47s that they were gonna shoot him down with.
[00:15:23.640 --> 00:15:26.200]   And it's just like, hello, this is national security.
[00:15:26.200 --> 00:15:29.840]   Can we address it on a more substantive level face?
[00:15:29.840 --> 00:15:30.680]   - Yeah, yeah.
[00:15:30.680 --> 00:15:34.520]   - On the other side, after 20 years in service,
[00:15:34.520 --> 00:15:36.920]   the F-22 Raptor finally got its first kill.
[00:15:36.920 --> 00:15:38.280]   - Isn't that amazing?
[00:15:38.280 --> 00:15:39.640]   That is an amazing plane,
[00:15:39.640 --> 00:15:41.480]   but yeah, we don't have anybody to shoot at.
[00:15:41.480 --> 00:15:43.840]   A multi-hundred million dollar stealth fighter.
[00:15:43.840 --> 00:15:44.680]   And we--
[00:15:44.680 --> 00:15:45.520]   - Shut down, balloon.
[00:15:46.800 --> 00:15:48.520]   - Imagine the pilot that he's sitting there
[00:15:48.520 --> 00:15:50.160]   and they're like, hey, you're going up today
[00:15:50.160 --> 00:15:51.440]   and you're gonna arm your weapons
[00:15:51.440 --> 00:15:53.200]   and you're gonna shoot out a balloon.
[00:15:53.200 --> 00:15:54.080]   - Yeah, yeah.
[00:15:54.080 --> 00:15:55.760]   Granddad, what did you do before?
[00:15:55.760 --> 00:15:59.440]   Oh, I was the first F-22 Raptor pilot
[00:15:59.440 --> 00:16:00.920]   to ever get an air-to-wear kill.
[00:16:00.920 --> 00:16:02.880]   - Does he get a little outline of a balloon
[00:16:02.880 --> 00:16:04.240]   then on the plane?
[00:16:04.240 --> 00:16:05.280]   (laughing)
[00:16:05.280 --> 00:16:06.120]   - Below his--
[00:16:06.120 --> 00:16:07.440]   - Maybe a patch that indicates that,
[00:16:07.440 --> 00:16:09.440]   yeah, I'll just let his coat.
[00:16:09.440 --> 00:16:11.520]   - You know, you can probably,
[00:16:11.520 --> 00:16:14.160]   I would bet that whoever it was is trying to,
[00:16:14.160 --> 00:16:17.320]   trying to deny it, I had nothing to do with that.
[00:16:17.320 --> 00:16:18.400]   - Was me.
[00:16:18.400 --> 00:16:19.840]   - Was probably not allowed to say it.
[00:16:19.840 --> 00:16:20.680]   - Yeah, maybe.
[00:16:20.680 --> 00:16:21.520]   - Maybe.
[00:16:21.520 --> 00:16:22.520]   - But it was a nice shot.
[00:16:22.520 --> 00:16:24.920]   I mean, one missile boom, the thing came down,
[00:16:24.920 --> 00:16:26.400]   nice shot, well done.
[00:16:26.400 --> 00:16:29.560]   We don't know how they shot down the balloon yesterday,
[00:16:29.560 --> 00:16:30.400]   do we?
[00:16:30.400 --> 00:16:33.720]   - I would assume probably the same method.
[00:16:33.720 --> 00:16:35.320]   - Now that we know that that works.
[00:16:35.320 --> 00:16:36.880]   I think that was actually,
[00:16:36.880 --> 00:16:38.840]   I'm gonna guess one of the concerns of the military
[00:16:38.840 --> 00:16:40.200]   is we don't wanna miss.
[00:16:40.200 --> 00:16:41.160]   We don't wanna--
[00:16:41.160 --> 00:16:43.040]   - I would look bad.
[00:16:43.040 --> 00:16:45.400]   - Yeah, like we can't get it down.
[00:16:45.400 --> 00:16:47.080]   That would not be good.
[00:16:47.080 --> 00:16:51.280]   So I imagine some of the delay was them researching it.
[00:16:51.280 --> 00:16:52.600]   And now that we know,
[00:16:52.600 --> 00:16:54.720]   we'll take them in Alaska from now on.
[00:16:54.720 --> 00:16:59.280]   Interesting, interesting, I don't know.
[00:16:59.280 --> 00:17:01.920]   Yeah, and as so many of you are chair, I'm saying,
[00:17:01.920 --> 00:17:05.120]   there are theories that China did it just to see
[00:17:05.120 --> 00:17:07.840]   if we could respond, would respond,
[00:17:07.840 --> 00:17:09.840]   and had the capability to shoot it down.
[00:17:09.840 --> 00:17:11.760]   Yeah, it was another Sidewinder X
[00:17:11.760 --> 00:17:14.760]   that was used on the balloon in Alaska.
[00:17:14.760 --> 00:17:16.600]   Very interesting, I don't know.
[00:17:16.600 --> 00:17:17.960]   I feel like we don't know anything,
[00:17:17.960 --> 00:17:21.200]   and it's anything we say is speculation at this point.
[00:17:21.200 --> 00:17:23.720]   Here's some things we do know.
[00:17:23.720 --> 00:17:26.180]   The robots are about to take over.
[00:17:26.180 --> 00:17:32.400]   This week, both Microsoft and Google announced AI
[00:17:32.400 --> 00:17:34.940]   in search, Microsoft started it all.
[00:17:34.940 --> 00:17:38.760]   On, was that Tuesday?
[00:17:38.760 --> 00:17:41.400]   Yeah, I think Tuesday they announced,
[00:17:41.400 --> 00:17:46.360]   Microsoft is going to use OpenAI's chat GPT in Bing Search.
[00:17:46.360 --> 00:17:50.360]   I attempted to use it, in order to do it,
[00:17:50.360 --> 00:17:52.000]   you have to use Microsoft Edge,
[00:17:52.000 --> 00:17:54.440]   sign up for Bing to be your default search engine.
[00:17:54.440 --> 00:17:56.880]   You pretty much have to give your entire Windows machine
[00:17:56.880 --> 00:17:58.320]   over to Microsoft.
[00:17:58.320 --> 00:18:03.480]   And I still didn't get access to it.
[00:18:03.480 --> 00:18:06.520]   But some have, and we've seen some demos of it.
[00:18:06.520 --> 00:18:08.400]   What do you think, Shelley?
[00:18:08.400 --> 00:18:11.640]   You've probably, as most of us have, played with chat GPT.
[00:18:11.640 --> 00:18:14.720]   Sure, and I haven't done the Bing thing
[00:18:14.720 --> 00:18:17.040]   because I am a minus one Windows machine,
[00:18:17.040 --> 00:18:20.760]   so we Mac users are going to be in the dark for a little while.
[00:18:20.760 --> 00:18:22.680]   But you know what, I think they want you to,
[00:18:22.680 --> 00:18:24.000]   I think they want you to--
[00:18:24.000 --> 00:18:24.840]   Oh, I'm sure they do.
[00:18:24.840 --> 00:18:26.560]   Yeah, they'd like you to change your search engine
[00:18:26.560 --> 00:18:28.360]   at Bing.com.
[00:18:28.360 --> 00:18:30.760]   Sure, I mean, goodbye.duck.go off to Bing.
[00:18:30.760 --> 00:18:33.280]   I am going, no, I don't know.
[00:18:33.280 --> 00:18:37.000]   I think we've sort of gone from,
[00:18:37.000 --> 00:18:41.160]   isn't this chat GPT stuff amazing to, very quickly,
[00:18:41.160 --> 00:18:42.840]   which is what I would have expected to happen,
[00:18:42.840 --> 00:18:46.440]   is this is going to be, what does it mean to say
[00:18:46.440 --> 00:18:47.840]   this is going to be the next big thing?
[00:18:47.840 --> 00:18:50.280]   What business application is it going to have?
[00:18:50.280 --> 00:18:53.920]   And the first one is search, which is why so much
[00:18:53.920 --> 00:18:56.520]   of the story this week has been the competition
[00:18:56.520 --> 00:18:58.080]   between Microsoft and Google.
[00:18:58.080 --> 00:19:00.400]   And the stories that say Google is wrong-footed,
[00:19:00.400 --> 00:19:04.160]   even before it had the problem with their demo.
[00:19:04.160 --> 00:19:06.000]   Google is wrong-footed relative to Microsoft
[00:19:06.000 --> 00:19:09.080]   because Microsoft has access to chat GPT
[00:19:09.080 --> 00:19:11.000]   and Google's trying to counter them.
[00:19:11.000 --> 00:19:14.680]   And so it's sort of a weird mishmash of stories
[00:19:14.680 --> 00:19:17.840]   where some of us are amazed by this technology.
[00:19:17.840 --> 00:19:20.680]   Some of us, and we run the risk of being called Luddites.
[00:19:20.680 --> 00:19:23.040]   If we say it out loud, some of us are concerned about it.
[00:19:23.040 --> 00:19:24.880]   And some of us are just writing cool business stories
[00:19:24.880 --> 00:19:27.920]   about chat wars or AI wars.
[00:19:27.920 --> 00:19:29.760]   And it's an interesting time.
[00:19:29.760 --> 00:19:33.080]   Brian, didn't Google call a red alert
[00:19:33.080 --> 00:19:35.360]   when they found out that when Microsoft admitted, yeah,
[00:19:35.360 --> 00:19:37.960]   we've put another $10 billion into chat GPT
[00:19:37.960 --> 00:19:39.000]   and we're going to use it in search.
[00:19:39.000 --> 00:19:40.880]   Didn't this scare Google?
[00:19:40.880 --> 00:19:42.120]   Yes, absolutely.
[00:19:42.120 --> 00:19:46.000]   And of course, the ironic thing is that this is all technology
[00:19:46.000 --> 00:19:51.320]   that Google created years ago and open sourced.
[00:19:51.320 --> 00:19:55.400]   The T in chat GPT, I believe, is the part of the technology
[00:19:55.400 --> 00:19:56.240]   that Google--
[00:19:56.240 --> 00:19:57.240]   Chance Armor.
[00:19:57.240 --> 00:19:57.840]   Yeah.
[00:19:57.840 --> 00:20:03.600]   So what I have heard from listeners that work at Google
[00:20:03.600 --> 00:20:09.720]   that can't be named, of course, but also this was echoed in--
[00:20:09.720 --> 00:20:12.680]   I think it was TechCrunch had an article a couple days ago
[00:20:12.680 --> 00:20:13.840]   about this too--
[00:20:13.840 --> 00:20:17.520]   that what happened was, if you remember,
[00:20:17.520 --> 00:20:21.240]   there was the time when there was a lot of controversy
[00:20:21.240 --> 00:20:27.400]   where certain AI people in Google were writing papers saying,
[00:20:27.400 --> 00:20:29.760]   we shouldn't do certain things, certain people were fired.
[00:20:29.760 --> 00:20:33.000]   There was a lot of controversy around this stuff.
[00:20:33.000 --> 00:20:36.400]   And also, so number one, you have--
[00:20:36.400 --> 00:20:39.040]   this was culturally divisive within Google,
[00:20:39.040 --> 00:20:41.600]   so that's why maybe it didn't get productized.
[00:20:41.600 --> 00:20:45.040]   Number two, you have the classic innovators, dilemma of, well,
[00:20:45.040 --> 00:20:47.480]   would this kill the golden goose?
[00:20:47.480 --> 00:20:51.280]   If you don't follow a link to get the answer you need,
[00:20:51.280 --> 00:20:54.160]   then we're not making as much money.
[00:20:54.160 --> 00:20:56.600]   But the thing that I heard that was echoed in the TechCrunch
[00:20:56.600 --> 00:21:02.480]   piece was that the way that you get money for projects
[00:21:02.480 --> 00:21:08.000]   inside Google is to affix it to another big team.
[00:21:08.000 --> 00:21:12.540]   So for example, the chatbot team, or the search team,
[00:21:12.540 --> 00:21:13.720]   or the maps team, or whatever.
[00:21:13.720 --> 00:21:15.800]   And if you think about it, we've seen that the last three
[00:21:15.800 --> 00:21:21.920]   or four Google I/Os have been little, little bits of AI
[00:21:21.920 --> 00:21:22.920]   added to maps.
[00:21:22.920 --> 00:21:24.520]   AI added to this.
[00:21:24.520 --> 00:21:26.360]   Essentially, what I've heard is no one
[00:21:26.360 --> 00:21:30.040]   had the constituency or the power to say, well, let's go and make
[00:21:30.040 --> 00:21:32.400]   this a full product or a full division.
[00:21:32.400 --> 00:21:35.440]   And so because of that, they were sort of dripping it
[00:21:35.440 --> 00:21:36.600]   into other little things.
[00:21:36.600 --> 00:21:39.840]   And then OpenAI comes in and just releases the whole product.
[00:21:39.840 --> 00:21:41.920]   And that's what they weren't prepared for.
[00:21:41.920 --> 00:21:43.680]   Google went through--
[00:21:43.680 --> 00:21:47.560]   you're talking about firing Timnett Gabru, who
[00:21:47.560 --> 00:21:51.520]   wrote a paper called Stochastic Parrots with other Google
[00:21:51.520 --> 00:21:52.080]   researchers.
[00:21:52.080 --> 00:21:56.040]   She was in the ethics division of Google AI.
[00:21:56.040 --> 00:21:59.560]   And when she said, this is potentially problematic,
[00:21:59.560 --> 00:22:04.000]   not only because of limitations and training data and so forth,
[00:22:04.000 --> 00:22:05.680]   but also people trust computers.
[00:22:05.680 --> 00:22:07.840]   And when a computer comes up with a wrong answer,
[00:22:07.840 --> 00:22:09.680]   it carries more weight than when a human comes up
[00:22:09.680 --> 00:22:10.720]   with a wrong answer.
[00:22:10.720 --> 00:22:12.200]   She got fired.
[00:22:12.200 --> 00:22:18.800]   Then there was Brent Lemoine, the priest, I guess you call him,
[00:22:18.800 --> 00:22:21.640]   who was working with Lambda, who said, oh no, it's
[00:22:21.640 --> 00:22:23.400]   gone sentient.
[00:22:23.400 --> 00:22:26.120]   And then Google immediately fired him because it hadn't.
[00:22:26.120 --> 00:22:28.320]   And it was ridiculous to say so.
[00:22:28.320 --> 00:22:30.720]   Maybe Lemoine had the right idea, though.
[00:22:30.720 --> 00:22:34.440]   Do you think Google was too judicious, Sam?
[00:22:34.440 --> 00:22:38.640]   Do you think they should have productized sooner?
[00:22:38.640 --> 00:22:42.040]   No, I don't think they should have productized sooner.
[00:22:42.040 --> 00:22:46.760]   And I'm not convinced that OpenAI and Microsoft
[00:22:46.760 --> 00:22:48.640]   should be doing it now.
[00:22:48.640 --> 00:22:52.720]   As you mentioned, the paper that Timnett Gabru and her colleagues
[00:22:52.720 --> 00:22:55.360]   did on the dangers of Stochastic Parrots,
[00:22:55.360 --> 00:22:59.640]   to find a system of haphazardly stitching together sequences
[00:22:59.640 --> 00:23:02.720]   of linguistic forms that have been observed in training data,
[00:23:02.720 --> 00:23:04.640]   according to probabilistic information about how they
[00:23:04.640 --> 00:23:05.160]   combine.
[00:23:05.160 --> 00:23:07.600]   That's how it works.
[00:23:07.600 --> 00:23:09.680]   But without any reference to meaning,
[00:23:09.680 --> 00:23:15.000]   that is the key, is that these models don't really understand.
[00:23:15.000 --> 00:23:16.840]   They are not intelligent.
[00:23:16.840 --> 00:23:20.960]   They're not even trying to be factual, in many cases.
[00:23:20.960 --> 00:23:24.920]   They're sophisticated pattern matching mechanisms,
[00:23:24.920 --> 00:23:27.600]   but they don't really understand what it--
[00:23:27.600 --> 00:23:30.080]   and this applies really across the board
[00:23:30.080 --> 00:23:35.040]   to most AI applications, whether it's large language models
[00:23:35.040 --> 00:23:40.600]   or machine vision systems or anything else.
[00:23:40.600 --> 00:23:44.200]   We throw around the terms AI, but none of these systems
[00:23:44.200 --> 00:23:48.760]   are actually anything close to human intelligence, the way
[00:23:48.760 --> 00:23:50.080]   the human brain works.
[00:23:50.080 --> 00:23:52.760]   They don't function the way a brain works.
[00:23:52.760 --> 00:23:55.880]   It's a very, very rough approximation.
[00:23:55.880 --> 00:24:01.080]   And from what I've seen, I think the best description I've
[00:24:01.080 --> 00:24:08.600]   seen of chat GPT is that what was the phrase--
[00:24:08.600 --> 00:24:12.960]   it's very confidently wrong.
[00:24:12.960 --> 00:24:14.960]   It's a man's pointer.
[00:24:14.960 --> 00:24:15.840]   Right.
[00:24:15.840 --> 00:24:20.080]   It puts together strings of words in a sequence
[00:24:20.080 --> 00:24:22.480]   that looks really plausible.
[00:24:22.480 --> 00:24:23.480]   Right.
[00:24:23.480 --> 00:24:25.920]   It's combined with very well-done writing,
[00:24:25.920 --> 00:24:29.240]   just in terms of not only just grammar and structure,
[00:24:29.240 --> 00:24:31.800]   it can make a sentence, but it can make sentences that
[00:24:31.800 --> 00:24:35.320]   read well, which is why so many people have applied it
[00:24:35.320 --> 00:24:37.760]   to making a document.
[00:24:37.760 --> 00:24:38.680]   Write a news story.
[00:24:38.680 --> 00:24:39.920]   Write an academic paper.
[00:24:39.920 --> 00:24:41.160]   Write this thing.
[00:24:41.160 --> 00:24:42.040]   And it reads--
[00:24:42.040 --> 00:24:42.840]   Think it write code.
[00:24:42.840 --> 00:24:44.000]   It can write runnable code.
[00:24:44.000 --> 00:24:45.000]   Sure.
[00:24:45.000 --> 00:24:45.520]   Right.
[00:24:45.520 --> 00:24:47.040]   But that's kind of a different issue,
[00:24:47.040 --> 00:24:50.800]   because the syntax of the writing
[00:24:50.800 --> 00:24:53.560]   is one of the things that always sort of alerts you.
[00:24:53.560 --> 00:24:56.560]   You've seen plenty of spam emails that come to you
[00:24:56.560 --> 00:24:58.360]   from a non-English speaker, or you've
[00:24:58.360 --> 00:25:02.600]   seen stuff that was generated by some sort of AI,
[00:25:02.600 --> 00:25:03.560]   some sort of computer.
[00:25:03.560 --> 00:25:05.600]   And you think you can always tell.
[00:25:05.600 --> 00:25:08.160]   I know that's not generated by a human,
[00:25:08.160 --> 00:25:11.600]   or by a human that had a certain amount of education.
[00:25:11.600 --> 00:25:13.840]   But you read this chat GPT stuff, and it
[00:25:13.840 --> 00:25:15.520]   reads as if it's well-constructed,
[00:25:15.520 --> 00:25:18.480]   and that gives the authenticity and authority of facts
[00:25:18.480 --> 00:25:19.440]   that aren't there.
[00:25:19.440 --> 00:25:23.080]   So a month ago, I did an experiment,
[00:25:23.080 --> 00:25:25.000]   and I shared it on the show, where
[00:25:25.000 --> 00:25:29.120]   I tried to create two entire YouTube videos from scratch,
[00:25:29.120 --> 00:25:32.440]   using not only the video selected by AI,
[00:25:32.440 --> 00:25:35.960]   but the writing made by chat GPT,
[00:25:35.960 --> 00:25:40.400]   and the actual-- the voice itself, again, not my voice.
[00:25:40.400 --> 00:25:43.240]   It was a complete robot voice, or whatever.
[00:25:43.240 --> 00:25:45.120]   Nearing in on the chat GPT part of it,
[00:25:45.120 --> 00:25:48.360]   so I picked the real story of Robinson Caruso,
[00:25:48.360 --> 00:25:51.560]   because that was based on a guy that really did get marooned.
[00:25:51.560 --> 00:25:56.640]   And then the Crystal Palace in the 1851 exhibition
[00:25:56.640 --> 00:25:57.400]   in London, blah, blah, blah.
[00:25:57.400 --> 00:25:59.800]   Just two things that I had recently read about,
[00:25:59.800 --> 00:26:01.960]   and so I knew enough about.
[00:26:01.960 --> 00:26:03.920]   The interesting thing was is that--
[00:26:03.920 --> 00:26:06.920]   so let's say you would say, give me 1,000 words
[00:26:06.920 --> 00:26:11.440]   on whether or not Robinson Caruso was based on a true story.
[00:26:11.440 --> 00:26:13.920]   And it would give you a perfect third grade level
[00:26:13.920 --> 00:26:17.160]   essay about it, but it would miss all the details that, again,
[00:26:17.160 --> 00:26:19.480]   since I had just seen a history channel thing on it,
[00:26:19.480 --> 00:26:20.520]   I knew there were more details.
[00:26:20.520 --> 00:26:22.400]   So I would have to go in and say, OK,
[00:26:22.400 --> 00:26:25.400]   but tell me about how he survived on the island.
[00:26:25.400 --> 00:26:28.160]   OK, now tell me about what happened
[00:26:28.160 --> 00:26:30.120]   to him after he was rescued.
[00:26:30.120 --> 00:26:34.080]   Now tell me about how Robert Lewis Stevenson found out
[00:26:34.080 --> 00:26:34.680]   about this story.
[00:26:34.680 --> 00:26:37.480]   So it was never wrong.
[00:26:37.480 --> 00:26:39.760]   And again, think about the fact that I knew enough
[00:26:39.760 --> 00:26:41.680]   to know if it was doing the wrong thing,
[00:26:41.680 --> 00:26:44.520]   but that wasn't even the thing that was interesting to me.
[00:26:44.520 --> 00:26:48.040]   Because obviously, if you don't know the facts,
[00:26:48.040 --> 00:26:49.800]   you wouldn't know if it was wrong.
[00:26:49.800 --> 00:26:54.040]   But to get it to be a good enough story,
[00:26:54.040 --> 00:26:55.880]   I also had to know the story.
[00:26:55.880 --> 00:26:59.840]   It wasn't good enough yet to be as sophisticated as I wanted it
[00:26:59.840 --> 00:27:02.480]   to be, just to make a five-minute long YouTube video.
[00:27:02.480 --> 00:27:05.360]   And so again, we're talking about people
[00:27:05.360 --> 00:27:09.360]   trying to train it to do things that are factual.
[00:27:09.360 --> 00:27:11.040]   It was also like--
[00:27:11.040 --> 00:27:13.320]   I felt like a conductor more than--
[00:27:13.320 --> 00:27:15.280]   it was all prompt engineering.
[00:27:15.280 --> 00:27:18.320]   And if you didn't know what you were talking about to begin with,
[00:27:18.320 --> 00:27:20.760]   it sure helped save you time to write all the words,
[00:27:20.760 --> 00:27:23.800]   but it's not like it did everything without me knowing
[00:27:23.800 --> 00:27:24.880]   word one.
[00:27:24.880 --> 00:27:26.840]   But why are we expecting so much of it?
[00:27:26.840 --> 00:27:31.640]   It's so funny, because on the one hand,
[00:27:31.640 --> 00:27:33.640]   we're kind of like children blown away
[00:27:33.640 --> 00:27:36.080]   by the capabilities of this thing.
[00:27:36.080 --> 00:27:37.520]   On the other hand, we're going, what?
[00:27:37.520 --> 00:27:38.320]   It's so stupid.
[00:27:38.320 --> 00:27:43.040]   And why are we expecting so much of it?
[00:27:43.040 --> 00:27:44.920]   It's kind of what I was getting to before.
[00:27:44.920 --> 00:27:47.320]   It's because those of us who are interested in tech,
[00:27:47.320 --> 00:27:49.800]   and now even those of us who might not be,
[00:27:49.800 --> 00:27:53.800]   but who have seen this thing do something that's--
[00:27:53.800 --> 00:27:55.240]   it's amazing.
[00:27:55.240 --> 00:27:56.680]   It seems amazing.
[00:27:56.680 --> 00:27:57.840]   We're excited about it.
[00:27:57.840 --> 00:27:59.360]   But then you have Microsoft and Google
[00:27:59.360 --> 00:28:02.040]   and all these companies that have to productize it,
[00:28:02.040 --> 00:28:03.760]   otherwise why do it?
[00:28:03.760 --> 00:28:07.680]   And even if Google didn't have the history with AI that it did,
[00:28:07.680 --> 00:28:10.360]   they have to productize it because Microsoft is doing it,
[00:28:10.360 --> 00:28:14.160]   which is why we're forcing it into search right now.
[00:28:14.160 --> 00:28:16.920]   And when whether search is the best or worst application,
[00:28:16.920 --> 00:28:19.960]   they've got to find some way to put it into products, which
[00:28:19.960 --> 00:28:22.360]   I don't think is good for us as humans.
[00:28:22.360 --> 00:28:25.120]   I don't-- it may be successful for those companies.
[00:28:25.120 --> 00:28:27.440]   It may, in fact, lead to better AI.
[00:28:27.440 --> 00:28:28.760]   However, we define it.
[00:28:28.760 --> 00:28:32.000]   I don't know whether that means accurate or what it means.
[00:28:32.000 --> 00:28:35.240]   But it doesn't necessarily mean it's a good thing for us
[00:28:35.240 --> 00:28:38.280]   that we have more AI, but it keeps Microsoft and Google
[00:28:38.280 --> 00:28:39.120]   afloat, I suppose.
[00:28:39.120 --> 00:28:41.600]   Even in December, this is a headline from the New York
[00:28:41.600 --> 00:28:43.120]   Times from December.
[00:28:43.120 --> 00:28:47.640]   A new chatbot is a code read for Google's search business.
[00:28:47.640 --> 00:28:49.880]   A new wave of chatbots like chat GPT
[00:28:49.880 --> 00:28:52.840]   use artificial intelligence that could reinvent or even
[00:28:52.840 --> 00:28:55.760]   replace the traditional internet search engines.
[00:28:55.760 --> 00:29:02.640]   So even back in December, this terrified Google.
[00:29:02.640 --> 00:29:05.440]   And of course, Microsoft had the announcement on Tuesday.
[00:29:05.440 --> 00:29:10.400]   On Wednesday, Google threw together hastily threw together
[00:29:10.400 --> 00:29:14.120]   its own event where they announced
[00:29:14.120 --> 00:29:19.680]   they were going to have their own chat GPT-like AI called
[00:29:19.680 --> 00:29:24.440]   BARD, originally a Prentice BARD, in Google's search.
[00:29:24.440 --> 00:29:26.800]   They didn't say when, nor did Microsoft, by the way.
[00:29:26.800 --> 00:29:28.720]   It's not available generally yet.
[00:29:28.720 --> 00:29:33.200]   But they did show an animated GIF of BARD
[00:29:33.200 --> 00:29:36.240]   answering a question, what new discoveries from the James
[00:29:36.240 --> 00:29:39.960]   web space telescope can I tell my nine year old about?
[00:29:39.960 --> 00:29:42.600]   And it got it come wrong.
[00:29:42.600 --> 00:29:45.760]   Two of the three answers it provided were correct.
[00:29:45.760 --> 00:29:48.720]   But then the third bullet point was, and you could tell
[00:29:48.720 --> 00:29:51.920]   you're nine year old, we got the first pictures of an exoplanet,
[00:29:51.920 --> 00:29:54.600]   the Earth-like planet and another solar system,
[00:29:54.600 --> 00:29:56.640]   to which many astronomers, including one astronomer
[00:29:56.640 --> 00:29:58.840]   from the University of California, Santa Cruz,
[00:29:58.840 --> 00:30:02.520]   said, I took a picture of an exoplanet 14 years ago.
[00:30:02.520 --> 00:30:04.960]   What are you talking about?
[00:30:04.960 --> 00:30:08.760]   So you got to wonder, was there nobody at Google
[00:30:08.760 --> 00:30:10.640]   looking at these answers or checking?
[00:30:10.640 --> 00:30:13.480]   And that's certainly not a good way to announce
[00:30:13.480 --> 00:30:15.040]   your new chat for search.
[00:30:15.040 --> 00:30:17.160]   They were using the editors from CNET.
[00:30:17.160 --> 00:30:18.600]   Yeah, right.
[00:30:18.600 --> 00:30:19.240]   Yeah.
[00:30:19.240 --> 00:30:24.120]   Now, I think another big issue with using search
[00:30:24.120 --> 00:30:27.440]   as the first application for this,
[00:30:27.440 --> 00:30:30.320]   as the first mainstream application for this,
[00:30:30.320 --> 00:30:35.560]   is the fact that, you know, AIs don't really learn on the--
[00:30:35.560 --> 00:30:37.600]   these models don't learn on the fly.
[00:30:37.600 --> 00:30:40.640]   You have to run them through your training data.
[00:30:40.640 --> 00:30:42.960]   And every time that data set changes,
[00:30:42.960 --> 00:30:45.120]   you have to completely rerun that training.
[00:30:45.120 --> 00:30:49.000]   That's why when you play with chat GPT, the chat GPT--
[00:30:49.000 --> 00:30:54.640]   the GPT 3.5 model was trained on data back in 2021.
[00:30:54.640 --> 00:30:57.080]   So it was current up through the moment
[00:30:57.080 --> 00:31:01.480]   that it ran the training in 2021.
[00:31:01.480 --> 00:31:06.000]   But if you ask it about anything that is from the last two
[00:31:06.000 --> 00:31:09.280]   years, it will almost certainly get it wrong.
[00:31:09.280 --> 00:31:14.000]   And for a search engine, it has to be kept current
[00:31:14.000 --> 00:31:16.240]   in real time, all the time.
[00:31:16.240 --> 00:31:21.120]   And this is, again, also one of the challenges
[00:31:21.120 --> 00:31:23.040]   with automated driving, where you're
[00:31:23.040 --> 00:31:27.600]   trying to use AI, that the models have to be continuously
[00:31:27.600 --> 00:31:28.720]   updated.
[00:31:28.720 --> 00:31:32.320]   And for something like a search engine that is so vast,
[00:31:32.320 --> 00:31:35.960]   there's so much data out there, rerunning that training
[00:31:35.960 --> 00:31:41.800]   continuously to keep it fresh within minutes or at least
[00:31:41.800 --> 00:31:45.520]   hours, which is what we expect today from a Google or Bing
[00:31:45.520 --> 00:31:47.280]   or DuckDuckGo.
[00:31:47.280 --> 00:31:48.720]   When something pops up on the web,
[00:31:48.720 --> 00:31:51.720]   we expect it to be found almost immediately.
[00:31:51.720 --> 00:31:56.720]   That doing that with a large language model
[00:31:56.720 --> 00:32:01.520]   seems like it would be both impractical and use
[00:32:01.520 --> 00:32:06.200]   enormous amounts of energy and time to do that.
[00:32:06.200 --> 00:32:08.560]   Yeah, people are already talking about the environmental
[00:32:08.560 --> 00:32:10.520]   impact of this.
[00:32:10.520 --> 00:32:13.600]   Chat GPT costs millions of dollars a day.
[00:32:13.600 --> 00:32:16.240]   We know that because Sam Altman has said--
[00:32:16.240 --> 00:32:19.840]   so he called it an eye-warheringly large number.
[00:32:19.840 --> 00:32:21.920]   And Chat GPT, when it was first released,
[00:32:21.920 --> 00:32:26.160]   only was up to date through spring of last year of 2021,
[00:32:26.160 --> 00:32:27.560]   even older than that.
[00:32:27.560 --> 00:32:29.720]   So to keep it running constantly,
[00:32:29.720 --> 00:32:34.000]   spidering the web, as Google does with its spiders,
[00:32:34.000 --> 00:32:36.920]   would cost kind of an unheard of amount of money.
[00:32:36.920 --> 00:32:38.200]   Chat GPT plus--
[00:32:38.200 --> 00:32:40.800]   And Google has gone to enormous efforts
[00:32:40.800 --> 00:32:46.200]   to make their spidering mechanisms as efficient as possible
[00:32:46.200 --> 00:32:49.920]   to minimize the amount of energy that's consumed for that.
[00:32:49.920 --> 00:32:51.960]   And even that consumes a lot.
[00:32:51.960 --> 00:32:53.720]   Doing this with a large language model
[00:32:53.720 --> 00:32:55.520]   would be far, far worse.
[00:32:55.520 --> 00:33:00.320]   Now, there are ways to make AI models more efficient
[00:33:00.320 --> 00:33:03.640]   in terms of at least--
[00:33:03.640 --> 00:33:07.400]   once they've been trained, you can optimize them
[00:33:07.400 --> 00:33:08.760]   to make them more efficient.
[00:33:08.760 --> 00:33:10.240]   But then what ends up happening is
[00:33:10.240 --> 00:33:13.680]   you tend to lose some resolution, some granularity
[00:33:13.680 --> 00:33:17.040]   from that, which then has its own issues.
[00:33:17.040 --> 00:33:19.440]   And there's a lot of work being done
[00:33:19.440 --> 00:33:23.440]   on making processors more efficient for processing
[00:33:23.440 --> 00:33:25.080]   this kind of data.
[00:33:25.080 --> 00:33:29.240]   And I saw some interesting stuff at CES this year
[00:33:29.240 --> 00:33:35.240]   on AI accelerator chips that they're trying to really optimize
[00:33:35.240 --> 00:33:37.560]   the performance per watt on those.
[00:33:37.560 --> 00:33:40.880]   But to do that at the kind of scale
[00:33:40.880 --> 00:33:43.400]   that you would need for a search engine--
[00:33:43.400 --> 00:33:46.000]   I mean, they're nowhere near that kind of capability.
[00:33:46.000 --> 00:33:48.240]   There's really two questions, or maybe more than two.
[00:33:48.240 --> 00:33:50.120]   But there's at least one is, should we even
[00:33:50.120 --> 00:33:51.560]   be trying to do this?
[00:33:51.560 --> 00:33:52.480]   Is it useful?
[00:33:52.480 --> 00:33:55.400]   And third, is it cost effective?
[00:33:55.400 --> 00:33:58.760]   A couple of researchers from a company called Semi Analysis,
[00:33:58.760 --> 00:34:02.080]   Dylan Patel and the Fazamad are quoted in Forbes
[00:34:02.080 --> 00:34:07.560]   saying how expensive it would be to give Google-like search
[00:34:07.560 --> 00:34:11.000]   capabilities to chat GPT.
[00:34:11.000 --> 00:34:13.120]   They said it would represent a direct transfer
[00:34:13.120 --> 00:34:16.880]   of $30 billion of Google's profit into the hands
[00:34:16.880 --> 00:34:19.200]   of the picks and shovels of the computing industry,
[00:34:19.200 --> 00:34:20.280]   probably Microsoft.
[00:34:20.280 --> 00:34:22.920]   Because one of the reasons Microsoft's invested in chat GPT
[00:34:22.920 --> 00:34:24.840]   is it's running on Azure.
[00:34:24.840 --> 00:34:26.840]   They say deploying current chat GPT
[00:34:26.840 --> 00:34:29.520]   into every search done by Google would
[00:34:29.520 --> 00:34:32.920]   require half a million A100HGX servers
[00:34:32.920 --> 00:34:36.600]   with a total of $4 million A100 GPUs.
[00:34:36.600 --> 00:34:39.000]   The total cost of these servers and networking
[00:34:39.000 --> 00:34:42.200]   exceeds $100 billion of capital expenditure
[00:34:42.200 --> 00:34:47.080]   alone just to build them, of which Nvidia would get a large portion.
[00:34:47.080 --> 00:34:49.160]   I guess they make the A100s.
[00:34:49.160 --> 00:34:51.440]   Yeah.
[00:34:51.440 --> 00:34:54.040]   So Nvidia's going, they're rubbing their hands with Glee.
[00:34:54.040 --> 00:34:55.800]   Microsoft's rubbing their hands with Glee.
[00:34:55.800 --> 00:34:58.520]   Well, and so get ready for this.
[00:34:58.520 --> 00:35:04.600]   Imagine the regulatory issues because if the only way,
[00:35:04.600 --> 00:35:06.480]   right now, financially, you can make this happen
[00:35:06.480 --> 00:35:10.200]   is to sign a deal with one of the big cloud computing
[00:35:10.200 --> 00:35:10.880]   platforms.
[00:35:10.880 --> 00:35:12.400]   And those are also the platforms that
[00:35:12.400 --> 00:35:14.760]   are investing in these companies.
[00:35:14.760 --> 00:35:18.000]   There was an article I saw this week where people were complaining.
[00:35:18.000 --> 00:35:21.160]   I know of academics that want to start companies,
[00:35:21.160 --> 00:35:24.960]   but the upfront capital cost is so huge that the only--
[00:35:24.960 --> 00:35:26.480]   the first conversation they have to have
[00:35:26.480 --> 00:35:30.240]   is they have to go to one of the big cloud providers
[00:35:30.240 --> 00:35:31.560]   and sign a deal with them.
[00:35:31.560 --> 00:35:34.520]   So don't you think that regulators, the Justice Department,
[00:35:34.520 --> 00:35:38.760]   would look at that as a certain pretty clear cut case
[00:35:38.760 --> 00:35:41.560]   of anti-competitive practice.
[00:35:41.560 --> 00:35:45.440]   If this new industry, everybody has to go to the incumbents
[00:35:45.440 --> 00:35:46.760]   to sign a deal.
[00:35:46.760 --> 00:35:48.720]   But on the other hand, I'm sure regulators
[00:35:48.720 --> 00:35:51.760]   want this to come along because this is the kind of disruption
[00:35:51.760 --> 00:35:54.640]   that can create new startups.
[00:35:54.640 --> 00:35:56.640]   But it couldn't if the only way you can do it--
[00:35:56.640 --> 00:35:58.400]   If you have to go to Microsoft and Amazon,
[00:35:58.400 --> 00:36:00.600]   it's cool to do it.
[00:36:00.600 --> 00:36:04.440]   By the way, it could very well be.
[00:36:04.440 --> 00:36:07.560]   But Microsoft thought, for a mere $10 billion,
[00:36:07.560 --> 00:36:13.160]   we can get Google to spend $30 billion on infrastructure
[00:36:13.160 --> 00:36:13.840]   so that--
[00:36:13.840 --> 00:36:15.000]   I mean--
[00:36:15.000 --> 00:36:15.560]   Who knows?
[00:36:15.560 --> 00:36:18.400]   And in fact, Sacha Najala said something essentially
[00:36:18.400 --> 00:36:20.920]   to that effect in an interview earlier this week
[00:36:20.920 --> 00:36:23.960]   with Neil Ipatel after their announcement.
[00:36:23.960 --> 00:36:27.000]   He talked about what they're doing
[00:36:27.000 --> 00:36:30.880]   is forcing Google to come to the dance.
[00:36:30.880 --> 00:36:31.720]   Oh, yeah.
[00:36:31.720 --> 00:36:33.440]   And who's the piper?
[00:36:33.440 --> 00:36:35.160]   Who's the fiddler?
[00:36:35.160 --> 00:36:35.760]   Yeah.
[00:36:35.760 --> 00:36:37.080]   It's the old Bill Gates thing.
[00:36:37.080 --> 00:36:41.080]   We don't have to make a dime on the web browser.
[00:36:41.080 --> 00:36:43.080]   Microsoft doesn't have to make a dime on search,
[00:36:43.080 --> 00:36:45.200]   but Google sure does.
[00:36:45.200 --> 00:36:51.120]   Google employees not thrilled about this rushed launch of a bard.
[00:36:51.120 --> 00:36:52.640]   In fact, they called it--
[00:36:52.640 --> 00:36:57.360]   there's an internal message board in Google called MemeGen.
[00:36:57.360 --> 00:37:00.240]   And according to CNBC, which saw some of the messages,
[00:37:00.240 --> 00:37:03.520]   the board filled with criticisms of company leadership,
[00:37:03.520 --> 00:37:09.240]   calling it rushed and botched, comically short-sighted.
[00:37:09.240 --> 00:37:10.480]   Stock market didn't like it either.
[00:37:10.480 --> 00:37:12.800]   Alphabet shares dropped 9%.
[00:37:12.800 --> 00:37:14.840]   How did NVIDIA do this week?
[00:37:14.840 --> 00:37:15.880]   Should have checked.
[00:37:15.880 --> 00:37:16.560]   Interos--
[00:37:16.560 --> 00:37:17.080]   No, they've been up.
[00:37:17.080 --> 00:37:18.200]   They've been up a lot.
[00:37:18.200 --> 00:37:18.480]   Yeah.
[00:37:18.480 --> 00:37:22.200]   Ungoogly, according to some of the messages,
[00:37:22.200 --> 00:37:26.840]   Microsoft might have pulled a little--
[00:37:26.840 --> 00:37:29.720]   in football, since the big game is tomorrow,
[00:37:29.720 --> 00:37:31.640]   sometimes you'll line up pretending
[00:37:31.640 --> 00:37:34.400]   that you're going to run in a fourth-down situation,
[00:37:34.400 --> 00:37:38.240]   trying to get the opponents to fall start to jump ahead
[00:37:38.240 --> 00:37:40.160]   so you get a little five-yards extra,
[00:37:40.160 --> 00:37:41.480]   but with four of the punt.
[00:37:41.480 --> 00:37:46.400]   Maybe Microsoft did a little faint to get Google
[00:37:46.400 --> 00:37:49.000]   to leap off the line prematurely.
[00:37:49.000 --> 00:37:53.160]   I think it's strategically brilliant because, if again,
[00:37:53.160 --> 00:37:55.720]   let's assume that Google could have done this five years ago,
[00:37:55.720 --> 00:37:57.600]   could have done a chat GPT and released it
[00:37:57.600 --> 00:38:00.160]   as a standalone product, but didn't, again,
[00:38:00.160 --> 00:38:03.760]   for the most obvious reason, which would be they have their golden goose,
[00:38:03.760 --> 00:38:08.080]   which is search advertising, and this obviates that.
[00:38:08.080 --> 00:38:10.800]   So number one, Microsoft has gotten more people
[00:38:10.800 --> 00:38:12.960]   to talk about being in the last week than they have
[00:38:12.960 --> 00:38:14.840]   in the last five years.
[00:38:14.840 --> 00:38:18.640]   And they forced Google to rush products out
[00:38:18.640 --> 00:38:20.520]   that they had previously decided-- we know,
[00:38:20.520 --> 00:38:23.800]   they previously decided this isn't probably good enough.
[00:38:23.800 --> 00:38:28.840]   And then number three, they've moved--
[00:38:28.840 --> 00:38:32.120]   If Google was hoping-- if we have to kill the golden goose
[00:38:32.120 --> 00:38:36.080]   someday, hopefully we can mitigate that over a period
[00:38:36.080 --> 00:38:37.640]   of a decade or something like that.
[00:38:37.640 --> 00:38:40.400]   So they're forcing Google to make decisions
[00:38:40.400 --> 00:38:43.320]   about whether or not to burn the boats sooner than they
[00:38:43.320 --> 00:38:44.760]   maybe were ready for.
[00:38:44.760 --> 00:38:46.000]   Interesting, yeah.
[00:38:46.000 --> 00:38:48.720]   And again, Microsoft gets nothing--
[00:38:48.720 --> 00:38:52.480]   there's no negative to Microsoft out of this other than a lot
[00:38:52.480 --> 00:38:55.560]   of money spent on their cloud platform.
[00:38:55.560 --> 00:38:55.840]   Yeah.
[00:38:55.840 --> 00:38:56.840]   Right?
[00:38:56.840 --> 00:38:58.640]   Yeah.
[00:38:58.640 --> 00:39:01.560]   And I think what we saw from Google this week
[00:39:01.560 --> 00:39:05.880]   was another proof point in Mike Elgin's assessment
[00:39:05.880 --> 00:39:11.160]   of Satya Ndar Pachai as CEO of Alphabet and Google.
[00:39:11.160 --> 00:39:14.120]   Yeah, Mike has said that he's not actually very good at it.
[00:39:14.120 --> 00:39:14.560]   Yeah.
[00:39:14.560 --> 00:39:17.200]   Michael Beyand Tweig on Wednesday.
[00:39:17.200 --> 00:39:18.200]   You probably take a look at it.
[00:39:18.200 --> 00:39:19.840]   Now, I would be interested to know
[00:39:19.840 --> 00:39:22.360]   how the perception would have been different.
[00:39:22.360 --> 00:39:25.000]   Had Google not had this giant fail,
[00:39:25.000 --> 00:39:26.920]   because the first headlines I read about this--
[00:39:26.920 --> 00:39:28.520]   and this is not to defend Google by--
[00:39:28.520 --> 00:39:31.480]   or defend or attack them, for me, honestly--
[00:39:31.480 --> 00:39:33.240]   but had they not--
[00:39:33.240 --> 00:39:36.280]   the first headlines said Google loses $100 million
[00:39:36.280 --> 00:39:37.240]   when the first--
[00:39:37.240 --> 00:39:37.880]   it stopped.
[00:39:37.880 --> 00:39:39.920]   The first-- that was the first headline.
[00:39:39.920 --> 00:39:44.440]   And so it was obviously on the level of great demo fails
[00:39:44.440 --> 00:39:46.000]   that we've seen from companies in the past,
[00:39:46.000 --> 00:39:49.360]   including Google a couple of years ago when they made people
[00:39:49.360 --> 00:39:53.680]   crazy with the AI-based voice assistant that they were demoing
[00:39:53.680 --> 00:39:55.080]   and didn't work out so well.
[00:39:55.080 --> 00:39:58.000]   And so I'm sure that there is a lot of internal concern
[00:39:58.000 --> 00:40:02.400]   at Google of the kind that kept them out of chat, out of AI,
[00:40:02.400 --> 00:40:04.800]   previously because of ethical issues.
[00:40:04.800 --> 00:40:08.280]   But I also feel like there's nothing like a pile on.
[00:40:08.280 --> 00:40:11.520]   And I think the assessment that you guys have made about Microsoft
[00:40:11.520 --> 00:40:13.400]   position and all this makes total sense.
[00:40:13.400 --> 00:40:16.680]   But I do think it's sort of easy to pile on Google
[00:40:16.680 --> 00:40:19.280]   at this point and say, well, they had this terrible demo,
[00:40:19.280 --> 00:40:21.080]   which proves they're doing a bad job.
[00:40:21.080 --> 00:40:22.640]   And there may be plenty of other reasons
[00:40:22.640 --> 00:40:23.960]   to indicate they're doing a bad job.
[00:40:23.960 --> 00:40:26.720]   Again, I've heard from everyone that Google actually
[00:40:26.720 --> 00:40:29.240]   has the better technology.
[00:40:29.240 --> 00:40:33.840]   People are like, it makes chat GPT look amateurish.
[00:40:33.840 --> 00:40:35.720]   It's just that they weren't ready to release it.
[00:40:35.720 --> 00:40:38.840]   So until proven otherwise, I'm still
[00:40:38.840 --> 00:40:41.120]   willing to believe that Google has the goods.
[00:40:41.120 --> 00:40:45.000]   It's just that they weren't ready to put the shine on the goods
[00:40:45.000 --> 00:40:47.040]   and present it to the public, I think.
[00:40:47.040 --> 00:40:49.640]   Well, there was a good wired article that sort of gave
[00:40:49.640 --> 00:40:52.720]   some of the history of how we got chat GPT.
[00:40:52.720 --> 00:40:54.840]   And we've talked about some of it, where you have this model
[00:40:54.840 --> 00:40:59.560]   from 2021 that the CEO of chat GPT says, of OpenAI says,
[00:40:59.560 --> 00:41:00.880]   let's release it to the public.
[00:41:00.880 --> 00:41:03.080]   And an unsuspecting public is all of a sudden
[00:41:03.080 --> 00:41:05.080]   able to use this technology.
[00:41:05.080 --> 00:41:07.480]   And that's all anybody's been talking about for a couple of months.
[00:41:07.480 --> 00:41:09.240]   But it's old technology.
[00:41:09.240 --> 00:41:13.360]   But it's a great big PR balloon to go back to balloons.
[00:41:13.360 --> 00:41:16.520]   It's an opportunity to get into the public consciousness
[00:41:16.520 --> 00:41:20.400]   in a way that that company and that technology had been before.
[00:41:20.400 --> 00:41:23.160]   Certainly there was Dali and there are all the other things
[00:41:23.160 --> 00:41:26.360]   that have been out in 2022 that have to do with images.
[00:41:26.360 --> 00:41:28.480]   But nothing like this, nothing that
[00:41:28.480 --> 00:41:30.560]   captured the general public's imagination.
[00:41:30.560 --> 00:41:32.040]   And so Google's hand was forced.
[00:41:32.040 --> 00:41:35.000]   And I don't think it's good for the future of AI.
[00:41:35.000 --> 00:41:37.920]   And it's probably not good for all the incumbents involved
[00:41:37.920 --> 00:41:39.920]   with the possible exception of Microsoft.
[00:41:39.920 --> 00:41:41.480]   Let me ask you this.
[00:41:41.480 --> 00:41:47.520]   If Bing does come out with a chat enhanced search,
[00:41:47.520 --> 00:41:51.880]   would you consider changing from Google to Bing, Shelley?
[00:41:51.880 --> 00:41:52.840]   Why use DuckDuckGo?
[00:41:52.840 --> 00:41:53.720]   So what do I know?
[00:41:53.720 --> 00:41:56.200]   OK, of course switching from DuckDuckGo.
[00:41:56.200 --> 00:41:59.160]   Well, I guess for me, I have to figure out
[00:41:59.160 --> 00:42:03.280]   what it is that I want from search that AI would give me.
[00:42:03.280 --> 00:42:06.120]   A lot of the searching I do has to do with my day job.
[00:42:06.120 --> 00:42:09.920]   It has to do with finding sources for stories I want to write.
[00:42:09.920 --> 00:42:11.880]   It has to do with finding new stories initially
[00:42:11.880 --> 00:42:14.800]   that I can turn into segments for the radio show
[00:42:14.800 --> 00:42:16.280]   or for my podcasts.
[00:42:16.280 --> 00:42:20.720]   And I have to ask myself, other than the sheer entertainment
[00:42:20.720 --> 00:42:24.640]   and technology enthusiast perspective, what does AI
[00:42:24.640 --> 00:42:26.280]   provide me in terms of search?
[00:42:26.280 --> 00:42:29.240]   And that case has it been made to me as an individual.
[00:42:29.240 --> 00:42:29.800]   Would I try?
[00:42:29.800 --> 00:42:31.040]   Of course I would.
[00:42:31.040 --> 00:42:33.120]   Would I give it all sorts of fun experiments to do?
[00:42:33.120 --> 00:42:33.760]   Yeah.
[00:42:33.760 --> 00:42:37.640]   But I don't have a use case for it in my own life.
[00:42:37.640 --> 00:42:40.520]   A lot of people are probably not in the same boat I'm in.
[00:42:40.520 --> 00:42:44.480]   And I am a careful adopter when things like that come along.
[00:42:44.480 --> 00:42:46.240]   But I'm not ready to share.
[00:42:46.240 --> 00:42:46.960]   How about you, Brian?
[00:42:46.960 --> 00:42:49.440]   Would you consider changing to a Bing.com?
[00:42:49.440 --> 00:42:51.040]   Well, I was going to say, I don't know if anyone
[00:42:51.040 --> 00:42:55.760]   read Joanna Stern's hands on with the Bing.
[00:42:55.760 --> 00:42:59.480]   Or maybe it was more with the Edge browser version of it.
[00:42:59.480 --> 00:43:02.640]   But what she was describing was more the--
[00:43:02.640 --> 00:43:05.720]   she asked it for questions.
[00:43:05.720 --> 00:43:07.480]   She was interviewing Sacha Nadella,
[00:43:07.480 --> 00:43:11.480]   and she asked it for questions that a Joanna Stern might
[00:43:11.480 --> 00:43:12.840]   ask Sacha Nadella.
[00:43:12.840 --> 00:43:15.120]   And she's like, I didn't-- not all of them were good,
[00:43:15.120 --> 00:43:17.120]   but there were some good ideas in there that I used.
[00:43:17.120 --> 00:43:22.880]   And she said, it's maybe we have to think of search
[00:43:22.880 --> 00:43:23.600]   in a different way.
[00:43:23.600 --> 00:43:26.040]   Maybe it's not search in the way that we're used to it.
[00:43:26.040 --> 00:43:30.040]   She said that I'm already using this tool to come up
[00:43:30.040 --> 00:43:31.360]   with story ideas.
[00:43:31.360 --> 00:43:34.400]   It's not writing the story for me.
[00:43:34.400 --> 00:43:37.400]   But it's prompt-- again, it's prompt engineering.
[00:43:37.400 --> 00:43:40.960]   It's giving me ideas that I'm free to take or not take.
[00:43:40.960 --> 00:43:42.440]   It's me as the conductor.
[00:43:42.440 --> 00:43:46.720]   I'm not the one performing--
[00:43:46.720 --> 00:43:50.080]   the conductor analogy is sort of like you use this tool
[00:43:50.080 --> 00:43:52.280]   to do the things and sing in the right direction,
[00:43:52.280 --> 00:43:54.520]   but you're still in charge.
[00:43:54.520 --> 00:43:57.600]   But as opposed to just put a keyword in,
[00:43:57.600 --> 00:44:03.000]   get a link to a page, you can use this in sort of the Star
[00:44:03.000 --> 00:44:04.560]   Trek the Next Generation computer thing
[00:44:04.560 --> 00:44:07.640]   to give ideas to steer in a different direction.
[00:44:12.040 --> 00:44:14.800]   The example Microsoft used for the browser
[00:44:14.800 --> 00:44:18.360]   was if you wanted to craft a response to your boss
[00:44:18.360 --> 00:44:22.400]   in an email, and it had access to your email.
[00:44:22.400 --> 00:44:26.280]   So it says, all right, give me the status of the TPS reports,
[00:44:26.280 --> 00:44:30.320]   and it would know the whole history of the TPS reports
[00:44:30.320 --> 00:44:32.120]   because it would have access to your email.
[00:44:32.120 --> 00:44:36.120]   And it would say to you, OK, do you want a hopeful response?
[00:44:36.120 --> 00:44:38.840]   Do you want a skeptical response?
[00:44:38.840 --> 00:44:40.520]   Do you want a humorous response?
[00:44:40.520 --> 00:44:44.120]   And then it's up to you to either cut and paste that
[00:44:44.120 --> 00:44:46.920]   or whatever, but it gives you thought starters
[00:44:46.920 --> 00:44:47.920]   and things like that.
[00:44:47.920 --> 00:44:50.320]   That to me is something--
[00:44:50.320 --> 00:44:54.080]   that's a type of computing that we really haven't done
[00:44:54.080 --> 00:44:55.480]   a lot of till now.
[00:44:55.480 --> 00:44:56.280]   How about you, Sam?
[00:44:56.280 --> 00:44:58.520]   I think that's a really interesting idea.
[00:44:58.520 --> 00:45:01.600]   I think what Brian-- I think Brian's onto something there,
[00:45:01.600 --> 00:45:04.640]   but I think that there's-- that brings up
[00:45:04.640 --> 00:45:09.000]   another problem, though, and that one thing we've seen
[00:45:09.000 --> 00:45:14.360]   over the last decade, especially, is we've become really
[00:45:14.360 --> 00:45:19.080]   bad at media literacy at filtering through and picking
[00:45:19.080 --> 00:45:21.480]   out what are the--
[00:45:21.480 --> 00:45:25.320]   what's real, what's not, picking through the misinformation
[00:45:25.320 --> 00:45:29.440]   and picking out the genuine information
[00:45:29.440 --> 00:45:31.480]   that we should be paying attention to.
[00:45:31.480 --> 00:45:38.640]   And I think what Brian's onto is a really good idea of--
[00:45:38.640 --> 00:45:44.440]   if we can use tools like this to generate those prompts
[00:45:44.440 --> 00:45:47.320]   in our minds to give us things to think about,
[00:45:47.320 --> 00:45:50.560]   the problem is I think it's going to require a whole lot
[00:45:50.560 --> 00:45:54.960]   of education for everybody in terms of how to use it
[00:45:54.960 --> 00:46:01.400]   in a way that is going to be positive and actually
[00:46:01.400 --> 00:46:04.280]   useful to us and not create new problems.
[00:46:04.280 --> 00:46:05.120]   Yes.
[00:46:05.120 --> 00:46:08.360]   It can definitely really go off the rails very quickly.
[00:46:08.360 --> 00:46:10.200]   I agree with you, but sorry, I lost my point,
[00:46:10.200 --> 00:46:14.080]   and I wanted to point it to Shelley
[00:46:14.080 --> 00:46:17.600]   and give her ideas of where I think it might be useful for her
[00:46:17.600 --> 00:46:19.080]   to create podcasts and things.
[00:46:19.080 --> 00:46:21.160]   So another analogy I've heard people use is,
[00:46:21.160 --> 00:46:23.400]   why do TV shows have a writer's room?
[00:46:23.400 --> 00:46:26.040]   Because it's like, all right, for this next episode,
[00:46:26.040 --> 00:46:28.640]   what are the eight things that could happen to our characters?
[00:46:28.640 --> 00:46:31.880]   Or why does a magazine--
[00:46:31.880 --> 00:46:34.560]   we're in the old days used to have an editorial meeting--
[00:46:34.560 --> 00:46:37.560]   what are the 10 stories that we could put in the next issue?
[00:46:37.560 --> 00:46:40.560]   And so in essence, I think even Joanna might have said this,
[00:46:40.560 --> 00:46:46.320]   that what it becomes is your own personal writer's room,
[00:46:46.320 --> 00:46:49.080]   where you can come up with jokes.
[00:46:49.080 --> 00:46:52.080]   It'll help you come up with story ideas and things
[00:46:52.080 --> 00:46:53.320]   like that.
[00:46:53.320 --> 00:46:56.600]   And again, I hear what you're saying
[00:46:56.600 --> 00:46:59.560]   in terms of getting the answers right.
[00:46:59.560 --> 00:47:02.280]   You wouldn't want your doctor using this stuff yet.
[00:47:02.280 --> 00:47:07.520]   But for now, I am intrigued by that sort of level of usage
[00:47:07.520 --> 00:47:10.640]   where it is prompt-based, it is idea-based,
[00:47:10.640 --> 00:47:13.960]   and then it's up to you to sift the wheat from the chaff.
[00:47:13.960 --> 00:47:15.720]   I take that point, and I really do.
[00:47:15.720 --> 00:47:17.040]   And I was thinking about--
[00:47:17.040 --> 00:47:18.440]   well, an example that came to mind
[00:47:18.440 --> 00:47:20.680]   is what if I got a call tomorrow?
[00:47:20.680 --> 00:47:23.680]   And the caller said, you get to interview
[00:47:23.680 --> 00:47:25.560]   Sundar Pachai or Sacha Nadella.
[00:47:25.560 --> 00:47:27.240]   You get to interview one of those guys.
[00:47:27.240 --> 00:47:30.920]   And you have an hour before the interview starts.
[00:47:30.920 --> 00:47:32.800]   And I am in my own little bubble.
[00:47:32.800 --> 00:47:35.040]   I know how to research.
[00:47:35.040 --> 00:47:36.960]   I would be panicking, because I want
[00:47:36.960 --> 00:47:38.040]   to ask good questions.
[00:47:38.040 --> 00:47:41.440]   I don't just want to ask, so how do you think Microsoft is
[00:47:41.440 --> 00:47:42.480]   going to do this quarter?
[00:47:42.480 --> 00:47:45.680]   Or, hey, what's up with Chach EPT?
[00:47:45.680 --> 00:47:47.560]   I want to ask good, intelligent questions.
[00:47:47.560 --> 00:47:51.000]   And I can see a tool like this giving me prompt.
[00:47:51.000 --> 00:47:52.920]   I start by giving it prompts.
[00:47:52.920 --> 00:47:57.720]   And it comes back with substance based on not only
[00:47:57.720 --> 00:48:00.800]   whatever publication or podcast I might be working for
[00:48:00.800 --> 00:48:05.040]   and the approach that we typically take.
[00:48:05.040 --> 00:48:07.880]   What my own interests are that it would presumably know.
[00:48:07.880 --> 00:48:10.760]   And what my readers/listeners are interested in.
[00:48:10.760 --> 00:48:13.840]   And I can see that if there's a way in which it's actually
[00:48:13.840 --> 00:48:16.360]   really intellectually stimulating to think about how
[00:48:16.360 --> 00:48:21.240]   you could ask more deeper questions than you could ask
[00:48:21.240 --> 00:48:23.640]   just of a search engine where you're looking for a link
[00:48:23.640 --> 00:48:24.640]   in response.
[00:48:24.640 --> 00:48:27.120]   And I think that's another aspect of media literacy,
[00:48:27.120 --> 00:48:30.480]   where it's not simply that you have to not trust things that
[00:48:30.480 --> 00:48:34.040]   are potentially lying to you or giving you bad facts,
[00:48:34.040 --> 00:48:37.280]   but that you actually have to change the way you think
[00:48:37.280 --> 00:48:40.400]   in order to get the most out of this resource.
[00:48:40.400 --> 00:48:43.120]   And that's actually a really interesting educational
[00:48:43.120 --> 00:48:46.600]   discipline that we could see down the road that I'd be
[00:48:46.600 --> 00:48:48.560]   sort of fascinated to be a part of.
[00:48:48.560 --> 00:48:52.160]   Google's always said that this has been somewhat controversial.
[00:48:52.160 --> 00:48:55.800]   People search for, looking, often search for an answer.
[00:48:55.800 --> 00:48:57.720]   Sometimes you search for the nearest pizza place.
[00:48:57.720 --> 00:49:01.560]   Sometimes you search to find out who is somebody.
[00:49:01.560 --> 00:49:03.600]   But a lot of times they just want an answer.
[00:49:03.600 --> 00:49:08.080]   And Google has increasingly given its search results
[00:49:08.080 --> 00:49:11.120]   as answers, which has bothered a lot of publications
[00:49:11.120 --> 00:49:12.600]   because they're not driving traffic.
[00:49:12.600 --> 00:49:17.120]   I did a search for who won the Grammys this year on Google.
[00:49:17.120 --> 00:49:19.920]   And it gives you a knowledge graph.
[00:49:19.920 --> 00:49:22.200]   There's the winners right there, obviously extracted
[00:49:22.200 --> 00:49:22.840]   from somewhere.
[00:49:22.840 --> 00:49:25.760]   I don't have to go any further than that.
[00:49:25.760 --> 00:49:26.760]   It's got an overview.
[00:49:26.760 --> 00:49:27.680]   It's got nominations.
[00:49:27.680 --> 00:49:30.080]   It also has a knowledge graph on the right
[00:49:30.080 --> 00:49:32.880]   that it pulls solely from Wikipedia.
[00:49:32.880 --> 00:49:35.680]   And one of the things that's gotten Google in trouble
[00:49:35.680 --> 00:49:40.000]   with original sources is, well, done, right?
[00:49:40.000 --> 00:49:42.800]   I don't need to click any of the subsequent links.
[00:49:42.800 --> 00:49:44.520]   In fact, good luck getting to the subsequent links.
[00:49:44.520 --> 00:49:48.040]   That's a long scroll because Google's already extracted
[00:49:48.040 --> 00:49:49.320]   the answer that I want.
[00:49:49.320 --> 00:49:51.760]   And Google's answer is, well, that's why people search.
[00:49:51.760 --> 00:49:54.000]   I'm going to show you, I use another search engine
[00:49:54.000 --> 00:49:54.720]   called NIVA.
[00:49:54.720 --> 00:49:58.400]   Let me search for who won the Grammys 2023 on NIVA.
[00:49:58.400 --> 00:50:00.720]   NIVA uses an AI.
[00:50:00.720 --> 00:50:05.120]   It's generated this knowledge graph from four sources,
[00:50:05.120 --> 00:50:09.080]   BBC, New York Post, HuffPost, CNN about who won.
[00:50:09.080 --> 00:50:11.480]   Not as complete as the Google answer,
[00:50:11.480 --> 00:50:15.000]   but maybe for many queries, this would be sufficient.
[00:50:15.000 --> 00:50:18.880]   Again, stopping me from going further, right?
[00:50:18.880 --> 00:50:21.160]   Answering the simple question.
[00:50:21.160 --> 00:50:23.520]   I think one thing that we're going to see
[00:50:23.520 --> 00:50:26.040]   that Google has not addressed, Microsoft has not addressed
[00:50:26.040 --> 00:50:29.560]   is, and this is the problem you've seen with chat, GPT,
[00:50:29.560 --> 00:50:31.760]   and stable diffusion and other AI,
[00:50:31.760 --> 00:50:35.160]   is by scraping original content,
[00:50:35.160 --> 00:50:39.120]   it's giving people no need to go any farther.
[00:50:39.120 --> 00:50:44.120]   And it's in effect disintermediating sites like yours,
[00:50:44.120 --> 00:50:47.680]   Brian's, your podcast, like Tech meme,
[00:50:47.680 --> 00:50:51.400]   like Sam's podcast, like Shelley's podcast,
[00:50:51.400 --> 00:50:55.080]   because they've extracted the information people want
[00:50:55.080 --> 00:50:57.400]   and they're just going to provide it right on the search page.
[00:50:57.400 --> 00:51:01.040]   The worrying thing is that it obviates the need for search.
[00:51:01.040 --> 00:51:05.040]   The Galaxy brain thing is it obviates the need for media.
[00:51:05.040 --> 00:51:06.600]   Yeah.
[00:51:06.600 --> 00:51:09.240]   And you could see why Microsoft, Google,
[00:51:09.240 --> 00:51:11.400]   and others might want that.
[00:51:11.400 --> 00:51:13.320]   I think they're going to get a world of hurt
[00:51:13.320 --> 00:51:14.480]   from the original sources.
[00:51:14.480 --> 00:51:16.560]   And by the way, if you kill the original sources,
[00:51:16.560 --> 00:51:19.800]   then you have no fuel for your AI.
[00:51:19.800 --> 00:51:22.280]   Well, okay, another Galaxy brain thing, Leo.
[00:51:22.280 --> 00:51:24.600]   Everyone is talking about how potentially
[00:51:24.600 --> 00:51:28.160]   the internet could be flooded with a ton of spam from this,
[00:51:28.160 --> 00:51:30.960]   like me creating some dumb YouTube video.
[00:51:30.960 --> 00:51:31.800]   You could game it.
[00:51:31.800 --> 00:51:32.640]   Yeah.
[00:51:32.640 --> 00:51:33.480]   Right.
[00:51:33.480 --> 00:51:35.440]   So what if, so right now,
[00:51:35.440 --> 00:51:37.320]   all of these tools have been based,
[00:51:37.320 --> 00:51:39.600]   have been fed on the internet?
[00:51:39.600 --> 00:51:42.600]   What if two years from now, half of the internet
[00:51:42.600 --> 00:51:45.280]   is spam generated by the bots?
[00:51:45.280 --> 00:51:46.480]   Right.
[00:51:46.480 --> 00:51:48.520]   Then what happens to the quality of the bots?
[00:51:48.520 --> 00:51:50.960]   Their feet, then they're going to a vicious cycle.
[00:51:50.960 --> 00:51:53.400]   Their sources, it's a recursive thing that could happen
[00:51:53.400 --> 00:51:55.200]   where the quality would go to.
[00:51:55.200 --> 00:51:59.200]   Well, then you either keep going and you end up in a
[00:51:59.200 --> 00:52:03.400]   mediocrisy or you say, okay, we're going to,
[00:52:03.400 --> 00:52:05.800]   at the point when we turned on chat GPT,
[00:52:05.800 --> 00:52:08.000]   we're not going to trend on any more new content after that.
[00:52:08.000 --> 00:52:08.840]   Right.
[00:52:08.840 --> 00:52:09.680]   Exactly.
[00:52:09.680 --> 00:52:12.720]   So my theory is, what if it's like varietals,
[00:52:12.720 --> 00:52:15.760]   like the type of grape, the flavor of grape?
[00:52:15.760 --> 00:52:19.520]   What if the business model and/or the skill set is,
[00:52:19.520 --> 00:52:23.400]   well, you can trust our varietal of this AI
[00:52:23.400 --> 00:52:27.600]   because we've only trained it on the best medicine stuff.
[00:52:27.600 --> 00:52:28.440]   Right.
[00:52:28.440 --> 00:52:30.520]   Well, that's what Niva, by the way, Niva does.
[00:52:30.520 --> 00:52:31.360]   Which is interesting.
[00:52:31.360 --> 00:52:34.560]   If you ask it about, if you say, what do COVID vaccines work,
[00:52:34.560 --> 00:52:36.840]   it will, is already pre-identified.
[00:52:36.840 --> 00:52:38.880]   Certain sources is verified and trusted.
[00:52:38.880 --> 00:52:41.520]   And will only give you results from those verified
[00:52:41.520 --> 00:52:42.640]   and trusted sources.
[00:52:42.640 --> 00:52:43.480]   Not another.
[00:52:43.480 --> 00:52:45.800]   So then, so then do you end up having,
[00:52:45.800 --> 00:52:48.520]   do you end up creating a market for custom,
[00:52:48.520 --> 00:52:50.160]   high quality AI-based?
[00:52:50.160 --> 00:52:51.280]   That's what I'm suggesting.
[00:52:51.280 --> 00:52:53.520]   Well, interestingly, Niva is not free.
[00:52:53.520 --> 00:52:56.880]   But actually, this Niva is a $5 a month paid alternative to Google.
[00:52:56.880 --> 00:53:01.680]   And, and something like this actually just happened this past week.
[00:53:01.680 --> 00:53:06.800]   There was a Twitch channel that was running a completely,
[00:53:06.800 --> 00:53:08.800]   procedurally generated AI generated
[00:53:08.800 --> 00:53:14.960]   Seinfeld-like animated show, running continuously.
[00:53:14.960 --> 00:53:15.800]   Not funny.
[00:53:15.800 --> 00:53:19.040]   Yeah, that was generating the dialogue and generating the animation.
[00:53:19.040 --> 00:53:25.560]   But what happened was they, for some reason that I can't recall exactly right now,
[00:53:25.560 --> 00:53:28.280]   they had to switch to a different version of GPT
[00:53:28.280 --> 00:53:30.720]   from the one they were originally using.
[00:53:30.720 --> 00:53:37.720]   And very quickly, within a day or so, it might have even been hours,
[00:53:37.720 --> 00:53:44.560]   it started making transphobic and homophobic statements in that dialogue.
[00:53:45.360 --> 00:53:50.880]   And Twitch pulled it down because you had this varietal,
[00:53:50.880 --> 00:53:54.760]   you know, it switched to a different varietal of the AI model
[00:53:54.760 --> 00:53:58.760]   that just went completely off the rails within hours.
[00:53:58.760 --> 00:53:59.760]   Yeah.
[00:53:59.760 --> 00:54:01.400]   Yeah.
[00:54:01.400 --> 00:54:04.240]   Not that its loss is mourned, but yeah.
[00:54:04.240 --> 00:54:05.240]   No.
[00:54:05.240 --> 00:54:11.680]   So really the question, I mean, so, so funny,
[00:54:11.680 --> 00:54:17.160]   because what's happened is these companies have almost been panicked
[00:54:17.160 --> 00:54:19.560]   into rushing this stuff out.
[00:54:19.560 --> 00:54:21.520]   They've been holding on to it for a long time.
[00:54:21.520 --> 00:54:29.040]   Even open AI was really slow and judicious until recently at releasing its chat bots
[00:54:29.040 --> 00:54:32.040]   or its art bots.
[00:54:32.040 --> 00:54:35.320]   But all of a sudden there's this land rush and they're panicking
[00:54:35.320 --> 00:54:37.400]   and they're all releasing it at once.
[00:54:37.400 --> 00:54:39.920]   What do you think the outcome of this is going to be?
[00:54:39.920 --> 00:54:42.200]   Are we going to finally all just say, you know what?
[00:54:42.200 --> 00:54:43.320]   It's a parlor trick.
[00:54:43.320 --> 00:54:44.320]   It's stupid.
[00:54:44.320 --> 00:54:46.440]   Forget about it.
[00:54:46.440 --> 00:54:51.040]   Or do we buy into it as much as the big companies are buying into it?
[00:54:51.040 --> 00:54:55.000]   And suddenly, idiocracy, what is the outcome of all this?
[00:54:55.000 --> 00:54:57.320]   Shelly, what do you think?
[00:54:57.320 --> 00:55:02.680]   Well, first of all, I think my biggest concern about all of this is not
[00:55:02.680 --> 00:55:07.040]   whatever the initial thing is, whether we get some sort of AI generated
[00:55:07.040 --> 00:55:11.080]   search out of it, but how because it's generative AI, how we move forward,
[00:55:11.080 --> 00:55:15.080]   and whether that's the spam we were talking about, or whether it's just a new
[00:55:15.080 --> 00:55:18.080]   new normal on the internet, I don't know what that is.
[00:55:18.080 --> 00:55:23.280]   And I don't know that we have the ability to sort of control it or process it at
[00:55:23.280 --> 00:55:23.800]   this point.
[00:55:23.800 --> 00:55:25.640]   Have we already lost control, really?
[00:55:25.640 --> 00:55:27.120]   I don't know.
[00:55:27.120 --> 00:55:29.320]   But he said somebody back in time, quick.
[00:55:29.320 --> 00:55:31.280]   Sounds like a plan.
[00:55:31.280 --> 00:55:32.520]   Let's write that up.
[00:55:32.520 --> 00:55:34.160]   We'll have the AI write that up.
[00:55:34.160 --> 00:55:37.200]   But when Dali came out, it was easy for me to ignore the art stuff.
[00:55:37.200 --> 00:55:41.720]   I mean, not that I had great sympathy for artists who were concerned about it,
[00:55:41.720 --> 00:55:44.960]   but it didn't seem practical in the same way.
[00:55:44.960 --> 00:55:47.520]   Once they started talking about writing, then I cared all of a sudden.
[00:55:47.520 --> 00:55:54.120]   But it feels like it's gone too far in terms of holding it back in the bottle.
[00:55:54.120 --> 00:55:58.600]   And it obviously Microsoft and Google are so completely engaged in it.
[00:55:58.600 --> 00:56:03.080]   I don't think it's going to come out the way they necessarily want it to.
[00:56:03.080 --> 00:56:05.840]   I don't think it's going to be like a traditional, well, we're going to Microsoft
[00:56:05.840 --> 00:56:07.960]   and Google or Google are going to battle for a while.
[00:56:07.960 --> 00:56:10.960]   And then one of them is going to have supremacy because of their superior
[00:56:10.960 --> 00:56:13.120]   technology or their superior marketing or whatever.
[00:56:13.120 --> 00:56:16.200]   I think it's I think it's totally predicted to be honest.
[00:56:16.200 --> 00:56:17.880]   I think we're in this sort of Wild West.
[00:56:17.880 --> 00:56:20.480]   How many metaphors can I jump into West?
[00:56:20.480 --> 00:56:22.760]   But I think we're kind of in the world.
[00:56:22.760 --> 00:56:24.320]   Are we in Westworld?
[00:56:24.320 --> 00:56:26.080]   Is you a printer coming for us?
[00:56:26.080 --> 00:56:27.080]   I think that's it.
[00:56:27.080 --> 00:56:29.440]   Yes, the original Westworld is back.
[00:56:29.440 --> 00:56:30.800]   You'll be a winner is after us.
[00:56:31.800 --> 00:56:32.880]   It's always been our fear.
[00:56:32.880 --> 00:56:36.480]   It's interesting because it's always been our fear about AI that we would lose control,
[00:56:36.480 --> 00:56:42.200]   that Whopper or Buehl Brenner would take over or, you know, skyded.
[00:56:42.200 --> 00:56:46.040]   And I don't and when I say lose control, I'm not even talking about some sort of
[00:56:46.040 --> 00:56:48.120]   all-encompassing big bad.
[00:56:48.120 --> 00:56:51.960]   I think it's easy to sort of knock those worries down because I will will
[00:56:51.960 --> 00:56:54.440]   let eyes for people who don't understand or care about technology.
[00:56:54.440 --> 00:56:56.200]   You're going to always say something like that.
[00:56:56.200 --> 00:57:00.560]   But I do think there are ways in which and I think we always see it with technology
[00:57:00.560 --> 00:57:04.960]   and whether it's social media or whatever it is, there are ways in which things
[00:57:04.960 --> 00:57:08.320]   grow and mutate in an unanticipated fashion.
[00:57:08.320 --> 00:57:12.800]   And I think we're at the stage right now where there's no sort of governors on how
[00:57:12.800 --> 00:57:15.440]   this stuff is going to change and grow and evolve.
[00:57:15.440 --> 00:57:18.400]   And I don't mean regulation specifically, although that's an issue.
[00:57:18.400 --> 00:57:23.400]   But just in terms of like how much is out there and how much technology
[00:57:23.400 --> 00:57:26.240]   that is controlled by different people who want to fight each other for market
[00:57:26.240 --> 00:57:27.240]   supremacy.
[00:57:27.240 --> 00:57:29.280]   Can I give you a best case scenario?
[00:57:29.480 --> 00:57:32.240]   Because we all know what the worst case scenario is.
[00:57:32.240 --> 00:57:36.360]   There's a reason why in the Dune universe, computers were outlawed and you had to
[00:57:36.360 --> 00:57:39.400]   have those guys in the tanks piloting the ships.
[00:57:39.400 --> 00:57:40.600]   Right.
[00:57:40.600 --> 00:57:43.000]   Here's a best case scenario for the near term.
[00:57:43.000 --> 00:57:44.680]   And I have to credit Mark Andreessen.
[00:57:44.680 --> 00:57:51.080]   I heard him say this on a podcast months ago, but like, do you prefer an older
[00:57:51.080 --> 00:57:52.560]   doctor or a younger doctor?
[00:57:52.560 --> 00:57:55.160]   An older doctor has more experience.
[00:57:55.160 --> 00:57:58.920]   A younger doctor maybe is more up on the latest thing.
[00:57:59.880 --> 00:58:04.680]   Do you trust when you go to the doctor that they're having a good day or a bad
[00:58:04.680 --> 00:58:08.560]   day? Maybe she is going through a divorce and is distracted or whatever.
[00:58:08.560 --> 00:58:12.440]   Here's the best case scenario that 10 years from now, you go to the doctor.
[00:58:12.440 --> 00:58:16.200]   There's an AI ambiently in the background listening to the consultation.
[00:58:16.200 --> 00:58:21.200]   You describe your symptoms, you know, it reads the X rays, etc.
[00:58:21.200 --> 00:58:24.400]   And it's not going to have a good day or a bag.
[00:58:24.400 --> 00:58:25.520]   Because it's an AI.
[00:58:26.360 --> 00:58:29.720]   It's late. It's up on all of the latest stuff, even the stuff that was just
[00:58:29.720 --> 00:58:31.040]   published 30 minutes ago.
[00:58:31.040 --> 00:58:37.440]   And it can give you a 99.999% accurate diagnosis, etc, etc, etc.
[00:58:37.440 --> 00:58:41.120]   In the same way that and Sam, you can speak to this, that in theory,
[00:58:41.120 --> 00:58:43.360]   self-driving cars are safer than humans.
[00:58:43.360 --> 00:58:46.880]   And so the role of the doctor becomes the counselor.
[00:58:46.880 --> 00:58:47.480]   Okay.
[00:58:47.480 --> 00:58:49.760]   This is what the AI says is going on.
[00:58:49.760 --> 00:58:51.080]   Here are your options.
[00:58:51.080 --> 00:58:53.840]   Here's the option recommended by the AI.
[00:58:54.120 --> 00:58:57.600]   But let's think of these other things and it becomes more of a counselor thing
[00:58:57.600 --> 00:58:59.040]   than a diagnostic thing.
[00:58:59.040 --> 00:59:02.640]   Yeah, we put together a terrible position because they're supposed to have
[00:59:02.640 --> 00:59:06.440]   perfect memories and be good at bedside manner and hand holding.
[00:59:06.440 --> 00:59:11.000]   And so again, it comes back to that sort of man computer symbiosis where
[00:59:11.000 --> 00:59:16.440]   you're the conductor, you're the prompt engineer and you you don't expect the
[00:59:16.440 --> 00:59:18.600]   computer to be perfect every time.
[00:59:18.600 --> 00:59:23.440]   But you're as a professional, your job is to sort of get the best out of that
[00:59:23.440 --> 00:59:23.880]   instrument.
[00:59:23.920 --> 00:59:27.840]   This is very Mark Zuckerberg because of course that's the best case scenario.
[00:59:27.840 --> 00:59:28.920]   Sure.
[00:59:28.920 --> 00:59:29.360]   Doesn't.
[00:59:29.360 --> 00:59:33.560]   And we're already learning that this doctor could also say, you know, that
[00:59:33.560 --> 00:59:38.000]   gallbladder, take his foot off and then be completely wrong, in other words.
[00:59:38.000 --> 00:59:43.000]   And that's why the mutation scares me more than the concept itself.
[00:59:43.000 --> 00:59:47.760]   Like I'm not going to argue AI good or AI bad, but the mutations are what
[00:59:47.760 --> 00:59:49.040]   we're seeing.
[00:59:49.040 --> 00:59:52.360]   They could be wrong, more to the point.
[00:59:52.360 --> 00:59:54.960]   And you don't want a doctor to be wrong.
[00:59:54.960 --> 00:59:56.560]   Sam, what do you think?
[00:59:56.560 --> 01:00:02.040]   And I, you know, I like where Brian was going with that, you know, as a best
[01:00:02.040 --> 01:00:02.680]   case scenario.
[01:00:02.680 --> 01:00:09.280]   I think ideally, you know, having, you know, something like an AI as a
[01:00:09.280 --> 01:00:15.360]   co-pilot, you know, to augment human capability, I think is fantastic.
[01:00:15.360 --> 01:00:20.560]   I think I think it's a that that would be ideal, you know, to, because there are
[01:00:20.560 --> 01:00:23.280]   all, you know, human memory is always flawed.
[01:00:23.280 --> 01:00:23.600]   Yeah.
[01:00:23.600 --> 01:00:29.440]   There are there are advantages to, you know, things that we gain from human
[01:00:29.440 --> 01:00:35.520]   experience and the nuance of the way humans think and process information that
[01:00:35.520 --> 01:00:40.400]   I think, you know, in that, that role as the counselor can be extremely valuable.
[01:00:40.400 --> 01:00:49.360]   Um, and if you can get the AI to the point where it can filter through the
[01:00:49.400 --> 01:00:53.600]   erroneous information and keep that out of it, I think that would be great.
[01:00:53.600 --> 01:01:00.200]   Um, but, you know, I, I'm, I remain unconvinced that we can get there.
[01:01:00.200 --> 01:01:01.320]   Um,
[01:01:01.320 --> 01:01:05.680]   but self-driving cars are good template for this because we do like driver
[01:01:05.680 --> 01:01:07.840]   assist AI technologies, right?
[01:01:07.840 --> 01:01:10.760]   Uh, to some degree, yes.
[01:01:10.760 --> 01:01:15.360]   I mean, even even those, you know, have their, have very strong limitations.
[01:01:15.680 --> 01:01:18.280]   Oh, but the humans never gives up complete control.
[01:01:18.280 --> 01:01:23.480]   You always kind of in case the AI decides to drive into a wall, you always keep some control.
[01:01:23.480 --> 01:01:26.480]   Well, that's the, that's the theory.
[01:01:26.480 --> 01:01:32.720]   The, the reality is though that when you have a system, you know, whether, whether
[01:01:32.720 --> 01:01:38.120]   it's just a mechanical machine doing something or a piece of software that's
[01:01:38.120 --> 01:01:44.480]   doing something that works really well most of the time, the, the closer it, the
[01:01:44.480 --> 01:01:49.920]   closer that machine or software, uh, or met the closer that mechanism, whatever it
[01:01:49.920 --> 01:01:53.760]   is, whether it's hardware or software, the closer that mechanism gets to a hundred
[01:01:53.760 --> 01:01:59.600]   percent, you start to get into that uncanny valley where we trust it so much that we
[01:01:59.600 --> 01:02:07.920]   cannot be, we as humans cannot necessarily be trusted as the supervisors to say that,
[01:02:07.920 --> 01:02:11.920]   yes, this, you know, this is okay, or maybe this is a better solution.
[01:02:12.120 --> 01:02:16.880]   We tend to then give up control to that machine.
[01:02:16.880 --> 01:02:21.400]   We, we are not good at retaining that level of control that we need.
[01:02:21.400 --> 01:02:28.440]   And what we see with, um, with some of the, um, assisted driving systems out there is
[01:02:28.440 --> 01:02:35.680]   that the better it works most of the time, then, um, you know, the more likely we are
[01:02:35.680 --> 01:02:37.120]   to become complacent about it.
[01:02:37.120 --> 01:02:37.520]   Right.
[01:02:37.560 --> 01:02:43.560]   And then what we actually end up needing is more mechanisms to keep us engaged in the
[01:02:43.560 --> 01:02:46.600]   process of supervising the original mechanism.
[01:02:46.600 --> 01:02:52.000]   So you end up with something like GM Super Cruise, where you have a lane centering
[01:02:52.000 --> 01:02:56.160]   and speed control system that works really well most of the time, but it requires
[01:02:56.160 --> 01:02:57.080]   human supervision.
[01:02:57.080 --> 01:03:01.360]   So you add a driver monitor system to monitor the driver to make sure the driver
[01:03:01.360 --> 01:03:03.080]   is still monitoring the other system.
[01:03:03.440 --> 01:03:08.960]   And, you know, I'm not sure that it's necessarily improving things, right?
[01:03:08.960 --> 01:03:14.600]   Um, because it, it creates new, new cognitive workloads for, for humans.
[01:03:14.600 --> 01:03:18.880]   And, you know, it gets back to what I said earlier, but it requires a whole new kind
[01:03:18.880 --> 01:03:24.400]   of education for the people using these tools, you know, whatever, whatever, whatever
[01:03:24.400 --> 01:03:32.160]   context to understand and monitor what they're doing and to make the best decisions about.
[01:03:32.560 --> 01:03:33.000]   Okay.
[01:03:33.000 --> 01:03:38.000]   Is what this is telling me good or is this nonsense?
[01:03:38.000 --> 01:03:43.520]   You know, and do I need to ignore the mechanism and go with my own, my own
[01:03:43.520 --> 01:03:44.800]   knowledge, my own intuition?
[01:03:44.800 --> 01:03:48.600]   Or, you know, do I just follow what it's saying blindly?
[01:03:48.600 --> 01:03:50.680]   It really is that dichotomy.
[01:03:50.680 --> 01:03:55.480]   And on the one hand, we want to, we want these things to be as smart as possible.
[01:03:55.480 --> 01:03:59.320]   Trust them as much as possible, because it would really be a great thing to lift
[01:03:59.640 --> 01:04:04.720]   the burden off of us and let us do things we're good at and not have to do things
[01:04:04.720 --> 01:04:05.440]   we're not so good at.
[01:04:05.440 --> 01:04:09.800]   And on the other hand, uh, it looks like these things have the judgment and
[01:04:09.800 --> 01:04:14.040]   intelligence of a four year old and nobody in their right mind would trust them to
[01:04:14.040 --> 01:04:15.600]   take over anything.
[01:04:15.600 --> 01:04:17.920]   And we're stuck in the middle right now.
[01:04:17.920 --> 01:04:20.960]   And I don't know if it's because of timing or because this is actually the
[01:04:20.960 --> 01:04:23.000]   problem inherent in this.
[01:04:23.000 --> 01:04:23.760]   Let's take a little break.
[01:04:23.760 --> 01:04:24.520]   I want to take a little break.
[01:04:24.520 --> 01:04:28.440]   We've got a great panel and we're not going to end this discussion because there's
[01:04:28.440 --> 01:04:31.440]   a super bowl ad coming up about this very top example.
[01:04:31.440 --> 01:04:32.280]   Sam is here.
[01:04:32.280 --> 01:04:35.840]   He is a car guy, a wheel bearings podcast.
[01:04:35.840 --> 01:04:37.480]   He's also regular on our shows.
[01:04:37.480 --> 01:04:38.240]   We love having him.
[01:04:38.240 --> 01:04:40.200]   Uh, thank you for being here, Sam.
[01:04:40.200 --> 01:04:45.160]   I appreciate it from the tech meme ride home, Brian McCullough, internet historian
[01:04:45.160 --> 01:04:46.320]   and podcaster.
[01:04:46.320 --> 01:04:50.080]   Always a pleasure to have you, Brian and Shelly Brisbane.
[01:04:50.080 --> 01:04:52.960]   You know her from the incomparable in six colors.com.
[01:04:52.960 --> 01:04:56.880]   And if you live in Texas or even if you don't, you probably know the Texas standard
[01:04:57.240 --> 01:05:00.120]   and Shelly is a producer and reporter there.
[01:05:00.120 --> 01:05:02.000]   What is the website for Texas standard?
[01:05:02.000 --> 01:05:04.640]   I'll be Texas standard.org, Leo.
[01:05:04.640 --> 01:05:05.520]   That's pretty easy.
[01:05:05.520 --> 01:05:06.880]   I should have known that it is.
[01:05:06.880 --> 01:05:07.320]   Yeah.
[01:05:07.320 --> 01:05:09.560]   And yes, our show is available as a podcast.
[01:05:09.560 --> 01:05:12.840]   So if you're a Texas ex-app, we have a quite a substantial Texas
[01:05:12.840 --> 01:05:15.320]   expat community that listens to our show that way.
[01:05:15.320 --> 01:05:19.320]   So if you're not from Texas, there's not a lot here for you.
[01:05:19.320 --> 01:05:20.240]   Is that what you're saying?
[01:05:20.240 --> 01:05:20.560]   Come on.
[01:05:20.560 --> 01:05:21.480]   I wouldn't say that.
[01:05:21.480 --> 01:05:24.200]   I would say that if you're a Texas expat, there's more for you.
[01:05:24.200 --> 01:05:29.240]   However, if you would like to know why Dairy Queens in Texas are very different
[01:05:29.240 --> 01:05:31.680]   than Dairy Queens in other states or if you would like to know what our
[01:05:31.680 --> 01:05:35.600]   governor has been up to this week, we have got you covered on both of those.
[01:05:35.600 --> 01:05:37.080]   Dairy Queen is just.
[01:05:37.080 --> 01:05:38.600]   I saw that article.
[01:05:38.600 --> 01:05:43.000]   Those who was not in Texas need to be keeping an eye on Texas, just in case.
[01:05:43.000 --> 01:05:43.400]   Yes.
[01:05:43.400 --> 01:05:44.520]   I agree.
[01:05:44.520 --> 01:05:48.280]   If you think a steak finger country basket sounds good,
[01:05:48.280 --> 01:05:53.840]   you need to you need to go to DQ in Dallas or a hunger buster.
[01:05:53.920 --> 01:05:55.120]   Mm-hmm.
[01:05:55.120 --> 01:05:57.160]   Our show.
[01:05:57.160 --> 01:05:57.840]   Thank you, Sean.
[01:05:57.840 --> 01:05:58.680]   You're a show day.
[01:05:58.680 --> 01:06:01.200]   For the Queen sponsoring the show today.
[01:06:01.200 --> 01:06:01.720]   DQ.
[01:06:01.720 --> 01:06:04.960]   I wish they were, you know, because I could use a hunger buster right now or maybe
[01:06:04.960 --> 01:06:05.800]   a, what was it?
[01:06:05.800 --> 01:06:06.840]   A finger basket?
[01:06:06.840 --> 01:06:07.920]   I know it sounds so good.
[01:06:07.920 --> 01:06:08.840]   Take finger basket.
[01:06:08.840 --> 01:06:09.400]   They are.
[01:06:09.400 --> 01:06:11.800]   We all there's a country basket and a stick finger.
[01:06:11.800 --> 01:06:12.800]   I don't.
[01:06:12.800 --> 01:06:13.360]   Yeah.
[01:06:13.360 --> 01:06:17.640]   Are no, our show today is not brought you by DQ.
[01:06:17.640 --> 01:06:20.120]   This week, Attack is brought to you by Bitwarden.
[01:06:20.120 --> 01:06:21.240]   Bitwarden.
[01:06:21.240 --> 01:06:22.000]   Bitwarden.
[01:06:22.000 --> 01:06:25.960]   We talked to so many people these days who are ready to leave those other guys
[01:06:25.960 --> 01:06:31.360]   after their horrific breach, their lack of concern and lack of communication
[01:06:31.360 --> 01:06:32.160]   for their customers.
[01:06:32.160 --> 01:06:36.120]   Can I recommend the password manager I've been using for the last two years,
[01:06:36.120 --> 01:06:38.720]   password manager, I trust and love Bitwarden.
[01:06:38.720 --> 01:06:42.920]   It's the only open source cross platform password manager.
[01:06:42.920 --> 01:06:48.600]   You can use it home at work on the go and trusted by millions.
[01:06:49.040 --> 01:06:52.120]   I think a great many of us have switched over to Bitwarden.
[01:06:52.120 --> 01:06:54.880]   I'm happy to say I did that a few years ago.
[01:06:54.880 --> 01:06:56.680]   Bitwarden is easy to use.
[01:06:56.680 --> 01:06:59.800]   It's cross platform, but more importantly, it's open source.
[01:06:59.800 --> 01:07:02.080]   So you know exactly what's going on.
[01:07:02.080 --> 01:07:07.480]   And when there are is a better way to do things, people contribute and Bitwarden
[01:07:07.480 --> 01:07:08.320]   gets better.
[01:07:08.320 --> 01:07:11.840]   Bitwarden will let you securely store your credentials across your business
[01:07:11.840 --> 01:07:12.960]   and your personal worlds.
[01:07:12.960 --> 01:07:18.200]   If you're using a personal account free forever, I might add across multiple
[01:07:18.200 --> 01:07:20.200]   platforms with unlimited passwords.
[01:07:20.200 --> 01:07:24.320]   You can even store your vault on your own.
[01:07:24.320 --> 01:07:27.400]   You can keep it out of any centralized storage.
[01:07:27.400 --> 01:07:28.640]   And that's pretty nice.
[01:07:28.640 --> 01:07:32.520]   Of course, Bitwarden doesn't track data in the mobile apps.
[01:07:32.520 --> 01:07:36.840]   In fact, only crash reporting in even that is removed in the after installation.
[01:07:36.840 --> 01:07:40.800]   If you're on Android, Bitwarden's open source invites anybody to review the
[01:07:40.800 --> 01:07:44.840]   library implementations at any time on GitHub to review the Bitwarden privacy
[01:07:44.840 --> 01:07:47.680]   policies at bitwarden.com/privacy.
[01:07:48.560 --> 01:07:49.720]   It's the one I trust.
[01:07:49.720 --> 01:07:51.560]   I love these guys at Bitwarden.
[01:07:51.560 --> 01:07:55.400]   Protect your personal data and privacy with Bitwarden by adding security
[01:07:55.400 --> 01:08:00.040]   your passwords with strong, randomly generated, unique passwords for every
[01:08:00.040 --> 01:08:04.600]   account, but you can go a step further because they also have a username generator.
[01:08:04.600 --> 01:08:07.880]   So you can create unique usernames for each account.
[01:08:07.880 --> 01:08:12.120]   In fact, if you use one of the five integrated email alias services, you can
[01:08:12.120 --> 01:08:16.880]   have unique emails for every account that kind of doubles your security.
[01:08:16.880 --> 01:08:20.120]   Not only does the hacker have to guess your password, they got to guess your
[01:08:20.120 --> 01:08:25.080]   email works with simple login and non-addie Firefox relay, our sponsor
[01:08:25.080 --> 01:08:28.160]   fast mail and now Shelley DuckDuck go.
[01:08:28.160 --> 01:08:33.480]   They will allow you to create a masked email address when you can use only
[01:08:33.480 --> 01:08:37.800]   ones on one website, for example, forwards any email to that primary email
[01:08:37.800 --> 01:08:39.360]   account when you get email at that address.
[01:08:39.360 --> 01:08:41.480]   It's a really great way to keep an eye on people.
[01:08:41.480 --> 01:08:43.480]   See if they're selling your email address off.
[01:08:43.480 --> 01:08:44.880]   Use a unique email address with them.
[01:08:44.880 --> 01:08:49.640]   You'll know if anybody else sent you email, they sold your address and it
[01:08:49.640 --> 01:08:52.880]   keeps your main email address out of the databases of services and sites
[01:08:52.880 --> 01:08:53.680]   you sign up for.
[01:08:53.680 --> 01:08:56.760]   It's just an extra security layer and it's great.
[01:08:56.760 --> 01:08:58.440]   It works so well.
[01:08:58.440 --> 01:09:02.560]   Bitwarden, great for business too, fully customizable, adapts to your business
[01:09:02.560 --> 01:09:08.760]   needs, the Teams organization, the $3 a month per user for a smaller
[01:09:08.760 --> 01:09:09.320]   organization.
[01:09:09.320 --> 01:09:14.840]   If you're a big company, your enterprise plan, $5 a month per user, it adds new
[01:09:14.840 --> 01:09:19.360]   features too like sharing data securely with coworkers across departments,
[01:09:19.360 --> 01:09:20.360]   the entire company.
[01:09:20.360 --> 01:09:25.320]   The basic free account though, for individuals free forever for an unlimited
[01:09:25.320 --> 01:09:28.400]   number of passwords, you can always upgrade to the premium account.
[01:09:28.400 --> 01:09:31.000]   I did that just to support Bitwarden 10 bucks a year.
[01:09:31.000 --> 01:09:31.760]   Come on, man.
[01:09:31.760 --> 01:09:36.000]   There's a family plan as well, gives up to six users, all the premium features
[01:09:36.000 --> 01:09:38.720]   for just $3.33 a month.
[01:09:38.720 --> 01:09:42.200]   Very easy to get data into Bitwarden.
[01:09:42.200 --> 01:09:45.880]   If you're on another password manager, it's so simple to export that data,
[01:09:45.880 --> 01:09:48.120]   important and Bitwarden and you're good to go.
[01:09:48.120 --> 01:09:51.920]   Look, you know, you got to use a password manager.
[01:09:51.920 --> 01:09:56.360]   If you're not, you're really at risk, but it's important you choose the right
[01:09:56.360 --> 01:10:01.800]   password manager, choose Bitwarden, the only open source cross platform
[01:10:01.800 --> 01:10:02.600]   password manager.
[01:10:02.600 --> 01:10:06.760]   You can use it home, you can use on the go, you can use it work trusted by
[01:10:06.760 --> 01:10:09.480]   millions of individuals, teams and organizations worldwide.
[01:10:10.320 --> 01:10:14.400]   Get started with a free trial right now of teams or enterprise or free for all
[01:10:14.400 --> 01:10:20.080]   devices forever as an individual user at bitwarden.com/twit.
[01:10:20.080 --> 01:10:24.320]   Really, there shouldn't be any reason not to use a password manager.
[01:10:24.320 --> 01:10:26.440]   Make it easy, make it free.
[01:10:26.440 --> 01:10:28.760]   Just go to bitwarden.com/twit.
[01:10:28.760 --> 01:10:31.120]   I tell your friends, tell your family.
[01:10:31.120 --> 01:10:33.360]   Uh, it's just the right thing to do.
[01:10:33.360 --> 01:10:35.760]   Bitwarden.com/twit.
[01:10:35.760 --> 01:10:38.800]   We thank you for their support of this weekend tech.
[01:10:38.800 --> 01:10:40.680]   We thank you for supporting us by using that address.
[01:10:40.680 --> 01:10:45.680]   So they know you saw it here, bitwarden.com/twit.
[01:10:45.680 --> 01:10:50.840]   I don't, you don't want to put all your passwords in one stake finger country
[01:10:50.840 --> 01:10:55.440]   basket, my friend, you want to spread it across or something.
[01:10:55.440 --> 01:10:55.840]   I don't know.
[01:10:55.840 --> 01:10:57.400]   That didn't really work.
[01:10:57.400 --> 01:10:59.480]   I've got my mind still on DQ.
[01:10:59.480 --> 01:11:00.200]   All right.
[01:11:00.200 --> 01:11:01.720]   You know the name.
[01:11:01.720 --> 01:11:03.680]   I'm sure Sam Dan O'Doud.
[01:11:03.680 --> 01:11:08.760]   He is a California entrepreneur, a billionaire who has been buying ads.
[01:11:09.160 --> 01:11:11.960]   Slamming Tesla's full self driving.
[01:11:11.960 --> 01:11:15.320]   He's now going to buy a Super Bowl ad.
[01:11:15.320 --> 01:11:20.720]   Now he's not buying the full ticket item, the one you see coast to coast nationwide.
[01:11:20.720 --> 01:11:25.360]   That's seven million dollars for 30 seconds, but he is going to buy Super Bowl
[01:11:25.360 --> 01:11:31.400]   ads in all the major states in the state capitals in Washington, DC.
[01:11:31.400 --> 01:11:33.000]   He wants to speak to legislators.
[01:11:33.000 --> 01:11:37.200]   Austin, Tallahassee, Albany, Atlanta and Sacramento.
[01:11:37.480 --> 01:11:39.920]   Now, Sam, tell us who Dan O'Doud is.
[01:11:39.920 --> 01:11:41.600]   So Dan.
[01:11:41.600 --> 01:11:49.960]   Oh, Dan O'Doud is the CEO and founder of Green Hill Software, which is a company
[01:11:49.960 --> 01:11:55.600]   that has been making, among other things, real time operating systems since their early
[01:11:55.600 --> 01:12:00.920]   1980s, very secure, very safe system, software systems.
[01:12:01.840 --> 01:12:09.440]   They run on everything from satellites and military hardware to vehicles.
[01:12:09.440 --> 01:12:09.720]   A lot.
[01:12:09.720 --> 01:12:14.720]   You know, there's a lot of Green Hill software in most vehicles around the world.
[01:12:14.720 --> 01:12:15.840]   Oh, that's interesting.
[01:12:15.840 --> 01:12:18.360]   So maybe he has a little competitive.
[01:12:18.360 --> 01:12:21.320]   No, not really.
[01:12:21.320 --> 01:12:26.480]   You know, what the software that Green Hill makes is lower level operating
[01:12:26.480 --> 01:12:29.040]   software, operating system software.
[01:12:29.320 --> 01:12:36.520]   So, you know, he, they're not making the software that runs, that runs.
[01:12:36.520 --> 01:12:37.800]   So it's not competitive.
[01:12:37.800 --> 01:12:44.920]   But he does, he does want legislatures to ban Tesla-style full self-driving, right?
[01:12:44.920 --> 01:12:47.800]   Here's, here's his tweet with video.
[01:12:47.800 --> 01:12:49.040]   I guess this is 30 second ad.
[01:12:49.040 --> 01:12:54.280]   This is the one that's going to run on Super Bowl and some local markets,
[01:12:54.280 --> 01:12:55.360]   Tesla full self-driving.
[01:12:55.360 --> 01:12:56.640]   There's a kid on a skateboard.
[01:12:56.640 --> 01:12:57.400]   Boom.
[01:12:57.400 --> 01:12:58.440]   Got him.
[01:12:58.680 --> 01:13:03.600]   There's a, there's a Tesla hitting a stroller and a crosswalk, just driving right through it,
[01:13:03.600 --> 01:13:06.240]   not stopping for a school bus.
[01:13:06.240 --> 01:13:10.320]   Um, Elon says the guy's nuts.
[01:13:10.320 --> 01:13:12.720]   What do you think is wrong?
[01:13:12.720 --> 01:13:14.400]   Elon, you think this is credible?
[01:13:14.400 --> 01:13:16.480]   Yes, it's absolutely credible.
[01:13:16.480 --> 01:13:20.600]   You know, please, the head off that man.
[01:13:20.600 --> 01:13:28.320]   Yeah, the Don project, which is O'Dowd's current effort that that is doing this.
[01:13:28.320 --> 01:13:34.520]   Um, you know, they're not the first, they're not, and not the only ones to be creating,
[01:13:34.520 --> 01:13:40.240]   uh, or to be doing this kind of testing and evaluation of different driving assist systems.
[01:13:40.240 --> 01:13:51.400]   Um, Luminar, for example, a LiDAR manufacturer, uh, they've done similar demos, uh, to this, uh, showing that, um, Tesla's camera only system.
[01:13:51.400 --> 01:13:58.280]   The, the, the thing that O'Dowd is against is a system like what Tesla is using that is a camera.
[01:13:58.280 --> 01:14:03.000]   It's a software only system, uh, that, you know, doesn't have some deterministic components to it.
[01:14:03.000 --> 01:14:06.200]   It's not, it's not safe the way Tesla has done it.
[01:14:06.200 --> 01:14:24.120]   And, and particularly, not only is there approach to the problem of automated driving and driver assist, not sufficiently safe, um, but also, um, the way that they marketed it and sold this capability that they call full self driving to consumers.
[01:14:25.080 --> 01:14:30.600]   It's a $15,000 per vehicle and it is nowhere close to being full self driving.
[01:14:30.600 --> 01:14:32.520]   I've tried the FSD beta.
[01:14:32.520 --> 01:14:38.320]   It is not capable of safely driving, safely operating a vehicle without human supervision.
[01:14:38.320 --> 01:14:41.560]   So what should you call, what should he, what should Elon have called it?
[01:14:41.560 --> 01:14:49.600]   I mean, if you want to call it something, you know, like driver assist, driver, yeah, driver assist system.
[01:14:49.800 --> 01:14:54.800]   I mean, there's, there's lots of different brand names out there from various companies.
[01:14:54.800 --> 01:14:58.080]   You know, GM has super crews for it has blue crews.
[01:14:58.080 --> 01:15:06.720]   You know, they're, they're, they're driving, you know, some, something that doesn't imply that the system is more capable than it is.
[01:15:06.720 --> 01:15:18.360]   Because what, you know, what, what's what he's talking about here is a phenomenon known as a ton of washing, which is a term that was coined by a woman named Liza Dixon.
[01:15:19.080 --> 01:15:23.640]   She's, she's a human machine factors researcher.
[01:15:23.640 --> 01:15:30.640]   And she wrote a great paper on this cuts on this topic a few years ago and coined that term, a ton of washing.
[01:15:30.640 --> 01:15:41.600]   And if you think back, you know, to the late 20, late 2000s, early 2010s, you hear about companies greenwashing, where, you know, they were doing things that, you know, to try to
[01:15:42.600 --> 01:15:47.880]   conjure up this image of being more green, more environmentally friendly than they were.
[01:15:47.880 --> 01:15:56.720]   A ton of washing, the idea is that you're marketing or, or talking about a system that is more capable.
[01:15:56.720 --> 01:16:04.040]   You're making it seem as the system as though the system is more capable of driving the vehicle safely than it is.
[01:16:04.040 --> 01:16:12.360]   And, and that's problematic because it gets back to what I talked about earlier, where as humans, you know, see the system,
[01:16:12.560 --> 01:16:19.360]   functioning a lot of the time, the more it functions correctly, the more they are inclined to trust it and then to not supervise it correctly.
[01:16:19.360 --> 01:16:39.040]   And then when it does hit those inevitable limits of its capabilities, then you get into problems where, you know, it can't properly detect a pedestrian or it can't properly detect how far away you are from a fire truck that's sitting, or a police vehicle, it's sitting on the edge of the road and runs right into it.
[01:16:39.240 --> 01:16:53.240]   Or can't tell the difference between a traffic signal that is sitting on the back of a truck being transported by a crew to somewhere for installation from one that is actually at an intersection.
[01:16:53.240 --> 01:17:02.840]   So, so, I mean, they even warn here's the disclaimer in the full self driving in, in, in this is, by the way, from the Dawn Projects website.
[01:17:02.840 --> 01:17:07.040]   It says it may do the wrong thing at the worst time.
[01:17:07.240 --> 01:17:16.240]   Yeah, this, this comes up, you know, in a Tesla with FSD, the, you know, when you launch FSD for the first time, when you enable it, that is the actual message.
[01:17:16.240 --> 01:17:17.240]   I've seen that come up.
[01:17:17.240 --> 01:17:18.240]   Yeah.
[01:17:18.240 --> 01:17:20.240]   That, that is the message that comes up on the screen.
[01:17:20.240 --> 01:17:21.240]   Full self driving is enabled.
[01:17:21.240 --> 01:17:29.240]   Your vehicle make lane changes off highway, select forks, all the good things it does, but you, but you have to pay attention and keep your hands on the wheel, right?
[01:17:29.240 --> 01:17:31.240]   Because it might do the wrong thing.
[01:17:31.440 --> 01:17:40.440]   If you watch some of these videos that were uploaded to YouTube of mistakes made by full self driving, you'd know, keep your hands on the wheel.
[01:17:40.440 --> 01:17:45.440]   And who's viable, by the way, if you, if you do go past it, school bus.
[01:17:45.440 --> 01:17:57.440]   The driver, the driver, the driver is Tesla does not accept liability for the performance of their autopilot or FSD, which is different from other manufacturers.
[01:17:57.640 --> 01:18:05.640]   Mercedes Benz, for example, last year launched a system in Germany that should be coming to the US later this year called Drive Pilot.
[01:18:05.640 --> 01:18:10.640]   It's a so called level three system where the driver is not required to watch the road.
[01:18:10.640 --> 01:18:19.840]   So it's a hands off system and an eyes off system, but you still have to be behind the wheel and ready to take over within about 10 seconds if it's reaching the limits of its capability.
[01:18:19.840 --> 01:18:20.640]   What is O'Dowd?
[01:18:20.640 --> 01:18:24.640]   What is his goal here besides carrying the hell out of me?
[01:18:24.840 --> 01:18:35.840]   Yeah, he wants, well, most specifically, he wants regulators to prevent Tesla from selling something that is claimed to be self driving that is not.
[01:18:35.840 --> 01:18:38.840]   Tesla is banned in some states.
[01:18:38.840 --> 01:18:42.840]   I think California from calling it full self driving, right?
[01:18:42.840 --> 01:18:47.840]   California passed a law, but they haven't, don't seem to have actually done anything to enforce it yet.
[01:18:47.840 --> 01:18:48.840]   Okay.
[01:18:48.840 --> 01:18:50.840]   So we'll see.
[01:18:51.040 --> 01:18:57.040]   It's the National Highway Transportation Safety Administration to ban FSD, fantastic FSD.
[01:18:57.040 --> 01:19:03.040]   Yes, and many of us have been saying that for many years that they should not be allowed.
[01:19:03.040 --> 01:19:15.040]   And both NHTSA, which is the regulator that governs automotive transportation safety and the Federal Trade Commission should also be stepping in and saying, look, this is false advertising.
[01:19:15.240 --> 01:19:24.840]   This is not self driving capability. You cannot sell this as self driving if it is not actually capable of driving the vehicle safely by itself.
[01:19:24.840 --> 01:19:27.040]   Yeah.
[01:19:27.040 --> 01:19:30.040]   And, Jelly, I presume you don't have a Tesla.
[01:19:30.040 --> 01:19:31.640]   Do not.
[01:19:31.640 --> 01:19:40.640]   I was interested in this though because our legislature here in Austin, Texas, which meets every two years for about six months has just gone into session.
[01:19:40.840 --> 01:19:43.840]   And so they might see the Super Bowl ad tomorrow.
[01:19:43.840 --> 01:19:45.240]   I suspect they will.
[01:19:45.240 --> 01:20:00.840]   And also keep in mind that Tesla has a Gigafactory here in Austin and Elon Musk has been embraced by a lot of the Texas political leaders because is bringing so much of his empire, not only Tesla, but SpaceX, his here, the boring company.
[01:20:00.840 --> 01:20:05.840]   Most of most of what Elon is doing has some Texas and Austin specific footprint.
[01:20:06.040 --> 01:20:12.040]   We also have the GM crews, uh, raw taxis here in Austin.
[01:20:12.040 --> 01:20:14.240]   And those have had some problems too.
[01:20:14.240 --> 01:20:19.440]   I understand that there are investigations into how those cars operate.
[01:20:19.440 --> 01:20:22.440]   I understand when they initially started here, they always had a driver.
[01:20:22.440 --> 01:20:24.640]   I believe there might be some.
[01:20:24.640 --> 01:20:27.240]   I don't know much about the how autonomous they are.
[01:20:27.240 --> 01:20:35.840]   Sam probably knows a lot more about it than I do, but there have been some specific issues here in Austin with it backing, not staying in lanes properly.
[01:20:35.840 --> 01:20:37.040]   And not parking properly.
[01:20:37.040 --> 01:20:43.440]   And there haven't been injuries that I'm aware of, but there have been a lot of concerns here in Austin on the local level.
[01:20:43.440 --> 01:20:51.240]   And here in the here in the capital city of Texas, our local and our state leaders are often on opposite sides of things.
[01:20:51.240 --> 01:20:54.240]   We have a fairly progressive local government.
[01:20:54.240 --> 01:20:56.440]   The people's Republic of Austin.
[01:20:56.440 --> 01:20:57.040]   Sure.
[01:20:57.040 --> 01:21:00.640]   The blueberry in the sea of red as they often say.
[01:21:00.840 --> 01:21:09.840]   And what that means is that sometimes what we're seeing in Austin, whether it be through what's going on with Tesla, with with full cell driving, or whether it be what's going on with crews.
[01:21:09.840 --> 01:21:16.640]   Uh, it doesn't necessarily mean those concerns are going to percolate to the legislature in the same way that they might.
[01:21:16.640 --> 01:21:23.540]   So, but there's not as far as I'm understanding and we've done a lot of coverage of what's coming up in the legislature.
[01:21:23.740 --> 01:21:34.540]   So far, there isn't really anything filed or expected to be taken up that has to do with regulation of solve autonomous driving or specifically with Tesla and self driving.
[01:21:34.540 --> 01:21:41.140]   And given Musk's courting a successful courting of the statewide officials, my guess is a lot won't come of it.
[01:21:41.140 --> 01:21:44.940]   I imagine they also are a fan of what Elon's doing on Twitter.
[01:21:44.940 --> 01:21:46.740]   He's made even more friends.
[01:21:46.740 --> 01:21:52.140]   Uh, probably I think I mean, that's sort of that, but they don't have a door.
[01:21:52.340 --> 01:21:54.740]   It's more on the sort of economic political side.
[01:21:54.740 --> 01:21:56.540]   Yeah, as I understand, you know, jobs, baby.
[01:21:56.540 --> 01:21:56.940]   Yeah.
[01:21:56.940 --> 01:22:01.740]   So that's the purely on the political side in terms of Twitter in terms of the job situation.
[01:22:01.740 --> 01:22:10.140]   Uh, SpaceX is very popular among elected officials, but it's not very popular in Boca Chico where there are a lot of environmental impacts.
[01:22:10.140 --> 01:22:15.140]   So there's a, there's a lot of back and forth in terms of how people regard Elon down here.
[01:22:15.140 --> 01:22:15.940]   Yeah.
[01:22:16.140 --> 01:22:27.940]   Uh, I had a Tesla model X didn't have full self driving, but I knew enough not to trust it even to do a lane change and frequently had to take control of the wheel.
[01:22:27.940 --> 01:22:35.940]   I have a Ford Mustang with blue crews right now and I theoretically could take my hands off the wheel, but I'm still keeping an eye on things.
[01:22:35.940 --> 01:22:40.940]   I'm still holding onto the wheel just in case no issues yet.
[01:22:40.940 --> 01:22:42.740]   How about you, Brian?
[01:22:42.740 --> 01:22:45.940]   Do you drive a, do you have a Tesla you have experience with full self driving?
[01:22:45.940 --> 01:22:58.940]   I actually, um, rented a model Y over the holidays and, you know, because I live in New York and, you know, haven't owned a car for almost 15 years.
[01:22:58.940 --> 01:23:08.940]   Um, all of the things when I get in a car these days that I'm sure are common, like, oh my God, look at that huge touch screen are still a wow factors to me.
[01:23:08.940 --> 01:23:14.940]   But just, just the, the adaptive cruise control was like, oh, yeah, that's all I need.
[01:23:15.140 --> 01:23:19.140]   You know, um, so like really, I know I love adaptive cruise control.
[01:23:19.140 --> 01:23:19.940]   I'm a fan.
[01:23:19.940 --> 01:23:21.540]   Yeah, it's all most people need.
[01:23:21.540 --> 01:23:21.940]   Yeah.
[01:23:21.940 --> 01:23:22.340]   Yeah.
[01:23:22.340 --> 01:23:26.340]   And, and, you know, frankly, Sam, tell me how far away this is.
[01:23:26.340 --> 01:23:32.140]   All I really want, I don't need the self driving in towns, cities or whatever.
[01:23:32.140 --> 01:23:36.940]   But if I could read a book, I'll still stay behind the driver's wheel or whatever.
[01:23:36.940 --> 01:23:42.140]   If I could watch a movie on that giant touchscreen while on the highway, that's all I need.
[01:23:42.340 --> 01:23:46.140]   Yeah. You know, and that's, that's exactly what the Mercedes Drive Pilot system does.
[01:23:46.140 --> 01:23:54.540]   And there are other systems coming over the next couple of years that will have that capability for automated highway driving.
[01:23:54.540 --> 01:24:04.940]   Because that's, you know, in the world of automated driving, they have used a term called operational design domain, which is what are the limits of where the system can operate.
[01:24:04.940 --> 01:24:12.140]   And, you know, unfortunately, you know, one of the things that Tesla has done wrong, I think, is say,
[01:24:12.140 --> 01:24:14.340]   you know, the ODD is everywhere.
[01:24:14.340 --> 01:24:18.140]   You know, you can, they're not putting any restrictions on where you can use it.
[01:24:18.140 --> 01:24:23.740]   Things like Mercedes Drive Pilot, GM Blue Crew or GM Super Cruise Ford Blue Crews,
[01:24:23.740 --> 01:24:26.940]   they're limited to operating on divided highways.
[01:24:26.940 --> 01:24:34.340]   And so, you know, we're going to see systems like this that allow you to take your hands off.
[01:24:34.340 --> 01:24:36.140]   You still, you can't take a nap.
[01:24:36.140 --> 01:24:40.940]   You know, you've got to be still awake and ready to take over within a reasonable period of time.
[01:24:41.140 --> 01:24:46.940]   But you'll be able to read or watch a video while you're cruising down the highway.
[01:24:46.940 --> 01:24:55.540]   And then, you know, when it comes, you know, when you set the navigation, when it comes to the off ramp, you know, then it'll give you an alert, you know, 10, 15 seconds before and say,
[01:24:55.540 --> 01:24:57.740]   "Hey, get ready to take over control."
[01:24:57.740 --> 01:24:58.940]   And then you'll do that.
[01:24:58.940 --> 01:25:04.140]   So that is going to be in cars over the next two, three, four years.
[01:25:05.940 --> 01:25:11.140]   Oh, and one other thing too, what Shelley was talking about, the cruise, rubber taxis in Austin.
[01:25:11.140 --> 01:25:15.340]   Yeah, they are, those are operating without safety drivers, safety drivers in them.
[01:25:15.340 --> 01:25:20.340]   They started doing that in San Francisco, June of last year.
[01:25:20.340 --> 01:25:28.140]   And then in December, it was past December, they expanded that operation to both Chandler, Arizona and Austin.
[01:25:28.340 --> 01:25:35.740]   But currently, they only operate like that at night between about 10 p.m. and 5 a.m.
[01:25:35.740 --> 01:25:47.340]   And only in a relatively small portion of the city as they gather more data and learn and get to a point where they have enough confidence to start expanding that operational area.
[01:25:47.340 --> 01:25:48.340]   They're in my neighborhood.
[01:25:48.340 --> 01:25:54.140]   I've, in fact, we were out pretty late last week and we saw one and it was a weird sight.
[01:25:54.140 --> 01:25:56.340]   I was like, "Wait, is there somebody in that car? I don't know."
[01:25:56.540 --> 01:26:07.540]   I've written in them. I mean, I've seen the reports of some of the issues and yeah, they're not perfect, but in my experience with them, they worked very well.
[01:26:07.540 --> 01:26:10.140]   They worked better than most Lyft or Uber drivers.
[01:26:10.140 --> 01:26:17.340]   Shelley was in San Francisco for the first time in about four years, two weekends ago, and Market Street after 10 p.m.
[01:26:17.340 --> 01:26:17.940]   It's a little weird.
[01:26:17.940 --> 01:26:20.940]   And there was one every other, every five minutes.
[01:26:20.940 --> 01:26:22.140]   They were all over the place.
[01:26:22.340 --> 01:26:27.340]   Shelley, I know you have low vision. Do you use a cane, a white stick?
[01:26:27.340 --> 01:26:31.340]   I have one. I only use it when I'm in unfamiliar areas.
[01:26:31.340 --> 01:26:39.340]   And yeah, as somebody who does not drive, cannot drive, the idea of self-driving on autonomous cars has always been exciting to me.
[01:26:39.340 --> 01:26:46.340]   And to be honest, I would get in one of those cruise robot taxis because I'm just like, I would like to be driven around by a robot.
[01:26:46.340 --> 01:26:47.140]   It's freedom.
[01:26:47.140 --> 01:26:47.740]   And imagine.
[01:26:47.740 --> 01:26:48.340]   Yeah.
[01:26:48.340 --> 01:26:48.940]   Absolutely.
[01:26:49.140 --> 01:27:06.740]   And the problem with whatever Tesla is doing and whatever is that there are humans interacting with, we talked before about the systems of, when we were talking about AI, we talked about how humans are involved.
[01:27:06.740 --> 01:27:18.940]   Well, in theory, if you had autonomous vehicles, if there were only autonomous vehicles on the streets and we didn't have those pesky humans, pedestrians, transit users and all that stuff, in theory, the system would be better for all.
[01:27:18.940 --> 01:27:21.140]   In theory, I'm not saying that it's true.
[01:27:21.140 --> 01:27:25.740]   Do you ever worry through as a pedestrian about getting covered by the city?
[01:27:25.740 --> 01:27:28.940]   I also worry about electric cars that don't have motor noise.
[01:27:28.940 --> 01:27:31.940]   So there's plenty of stuff to worry about as a pedestrian.
[01:27:31.940 --> 01:27:35.340]   I worry about human drivers just as much or more.
[01:27:35.340 --> 01:27:35.740]   More maybe.
[01:27:35.740 --> 01:27:41.140]   The only time I ever made physical contact with a car while I was walking, it was driven by a human.
[01:27:41.140 --> 01:27:44.540]   So yeah, I wasn't knocked down, but it was very scary.
[01:27:44.540 --> 01:27:45.340]   Yeah.
[01:27:45.540 --> 01:28:09.540]   And to your point, Shelley, in cases like yours are one of the driving forces for automated driving to enable mobility, freedom of mobility for people that either can't or don't want to drive, whether it's because of a physical limitation or just getting older or the young or people who just don't want to drive.
[01:28:09.740 --> 01:28:17.740]   You know, having that ability to call up a vehicle to take you where you need to go, not have to worry about parking and things like that.
[01:28:17.740 --> 01:28:22.740]   I think, you know, there's a lot of potential societal benefit out of that.
[01:28:22.740 --> 01:28:38.740]   And if we can get it right, you know, the potential improvements in safety, you know, these things should be able to see pedestrians way better than humans can because, you know, if they're done right, you know, which means not like Tesla, which relies only on.
[01:28:38.940 --> 01:28:58.940]   Simple RGB cameras, visible light cameras, you know, those cruise vehicles, WAMO vehicles, others, they use a combination of cameras and radar imaging radar sensors, high resolution radar sensors, the LiDAR sensors, thermal imaging sensors that should be able to help, especially help detect pedestrians and animals.
[01:28:59.140 --> 01:29:19.140]   You know, all of this combination of these sensors is what's key to try to generate a sufficient level of safety that is hopefully at least as good or better than human safety, which we haven't yet fundamentally proven that, that they are even as good as humans yet.
[01:29:19.340 --> 01:29:32.340]   Right, and it's fair to point out the pedestrians and animals are as unpredictable, or worse so than other vehicles on the road. So you can count on certain things with software about how other vehicles behave because there are rules governing them.
[01:29:32.340 --> 01:29:37.340]   That doesn't always work, but it's less effective with a pedestrian who might dart out in front of you.
[01:29:37.540 --> 01:29:56.540]   I think that there have been incidents where when they traced it down the car, the automated car was at fault in terms of hitting the pedestrian that they should have seen, but the pedestrian or the bicyclist or whoever is also not necessarily guaranteed to behave in a way that the car expects, that the software that's running the car expects.
[01:29:56.740 --> 01:30:14.740]   Yeah, that's actually turns out to be one of the hardest parts of developing automated driving. You can kind of break down the problem into four parts, sensing, detecting what's around the vehicle, prediction, then planning a path through that environment, and then actually controlling the hardware to do it.
[01:30:14.940 --> 01:30:29.940]   Addiction turns out to be really one of the toughest parts, especially with pedestrians and cyclists, vulnerable road users, because the physics of a vehicle means that it's going to put some limitations on how fast it can change direction.
[01:30:29.940 --> 01:30:37.940]   A human or an animal or a cyclist can change direction on a dime, and that is really hard to figure out for software.
[01:30:38.140 --> 01:30:44.140]   That's disappointing, because the main reason I want self-driving cars is so I can ride my bicycle more.
[01:30:44.140 --> 01:30:57.140]   Well, that dream up in smoke as well. Although, as I get older, I would love it that I guess my wife will drive me around, but until then, or maybe after that, self-driving vehicles would be great.
[01:30:57.140 --> 01:31:03.140]   I guess the problem really is that we have self-driving vehicles and human driven vehicles on the same road.
[01:31:03.340 --> 01:31:12.340]   Right, that's fundamentally what I see as the issue, because the rules about how you arrange streets and how traffic worked.
[01:31:12.340 --> 01:31:15.340]   So the ramp cars, yeah, it's all about, yeah, yeah.
[01:31:15.340 --> 01:31:31.340]   We're automated, and if I get on my sort of red hat here, but if we didn't have to have private car ownership, if I could just rent access to a car when I needed it and send it away, I wouldn't have to have a driveway, I wouldn't have to have a parking space or garage.
[01:31:31.540 --> 01:31:37.540]   Right. If I wanted to pick up truck to take a load of stuff somewhere, but never wanted to see that truck again, I could do that.
[01:31:37.540 --> 01:31:40.140]   It is a person with low vision. That's pretty attractive.
[01:31:40.140 --> 01:31:48.740]   Yeah, yeah. Well, and for society as a whole, there are seven and a half parking spaces for every vehicle in the United States.
[01:31:48.740 --> 01:31:53.940]   There's 290 million registered vehicles, and there's seven and a half parking spaces for every one of those.
[01:31:54.140 --> 01:32:03.340]   And in urban areas of the city are devoted to market centers, about 25 to 30 percent of an average urban center is devoted to parking.
[01:32:03.340 --> 01:32:23.340]   If you could eliminate all or most of that, imagine what you could do with that landmass in cities to provide more housing, more, more commercial real estate and more of everything that would be of benefit to society in addition to the potential for saving lives and saving energy from these vehicles.
[01:32:23.540 --> 01:32:34.740]   The way I always say it is if I have in my past, I've been a cube dweller in office environments, the car that I don't have, but if I had a car, it would have more space than I would have had in some of the cubes I've been in.
[01:32:34.740 --> 01:32:35.740]   Yeah.
[01:32:35.740 --> 01:32:40.740]   PC shipments. Oh, this is bad.
[01:32:40.740 --> 01:32:50.340]   An analyst working for a company called Mercury Research, Dean McCarran has put out a report that says
[01:32:50.540 --> 01:33:06.540]   the X86 processor market just endured, quote, "the largest on quarter and on year declines in our 30 year history, the worst quarter ever for X86 processors."
[01:33:06.540 --> 01:33:17.540]   And look up a recent article about the memory chip industry, which we know is famously boom and bust, but they've apparently had the worst bust that they've ever seen
[01:33:17.740 --> 01:33:21.740]   in terms of inventory, price declines, things like that.
[01:33:21.740 --> 01:33:26.740]   It's funny because wasn't it, wasn't the prices going up for a while because they couldn't make enough?
[01:33:26.740 --> 01:33:29.740]   And now they've made too many. Isn't that what always happens?
[01:33:29.740 --> 01:33:32.940]   Pretty much, yeah.
[01:33:32.940 --> 01:33:33.740]   Yeah.
[01:33:33.740 --> 01:33:37.740]   It was interesting that apparently server class chips are an exception to that.
[01:33:37.740 --> 01:33:47.540]   And I wonder in my non macroeconomic brain, whether that has to do with people replacing servers after the pandemic that they didn't during that time.
[01:33:47.540 --> 01:33:49.740]   Yeah, this was good for AMD apparently.
[01:33:49.740 --> 01:33:57.940]   It had a gain in server CPU share, one of the only segments that saw growth in the fourth quarter of 2022.
[01:33:57.940 --> 01:34:03.740]   Why is this, Brian? You think people, why aren't people buying PCs?
[01:34:03.740 --> 01:34:05.940]   Well, everybody did.
[01:34:05.940 --> 01:34:08.140]   And it's the already half.
[01:34:08.140 --> 01:34:09.140]   Right.
[01:34:09.140 --> 01:34:09.340]   Right.
[01:34:09.340 --> 01:34:10.340]   Everybody.
[01:34:10.340 --> 01:34:10.940]   Everybody needed it.
[01:34:10.940 --> 01:34:12.140]   Three, four, five years.
[01:34:12.140 --> 01:34:16.340]   Yeah, because, yeah, we're, it's the pool for it.
[01:34:16.340 --> 01:34:17.540]   It's the same thing as e-commerce.
[01:34:17.540 --> 01:34:23.340]   It was the pull forward of demand and then people were like, oh, new reality.
[01:34:23.340 --> 01:34:29.740]   And then the problem that the PC industry and the chip industry has is that then it,
[01:34:29.740 --> 01:34:33.740]   it's the rubber band sort of pulled back sooner than you could have a replacement
[01:34:33.740 --> 01:34:37.940]   cycle, a natural replacement cycle happen.
[01:34:37.940 --> 01:34:41.940]   So are we going to see a recovery at some point in the future?
[01:34:41.940 --> 01:34:46.140]   Like will people go back and buy PCs again or?
[01:34:47.140 --> 01:34:54.140]   I mean, the, so again, pandemic times you've got to have a tablet or a laptop for your kids.
[01:34:54.140 --> 01:34:57.940]   You've got to have an extra one, you know, your work furnished you with one and this,
[01:34:57.940 --> 01:35:00.140]   that and the other thing.
[01:35:00.140 --> 01:35:11.740]   And then think of, think of the, the snap back for if remote work doesn't happen as much as people thought, you know.
[01:35:11.940 --> 01:35:20.940]   I don't, there's, there's so many factors in terms of the, you know, you had the supply chain things as well.
[01:35:20.940 --> 01:35:24.540]   And now you've got fears of recession, dampening consumer demand.
[01:35:24.540 --> 01:35:35.540]   It's like it is one of those perfect storm things where you could point, we could sit here and talk for a half an hour, about four different points that are hitting the PC industry all at once.
[01:35:35.540 --> 01:35:39.940]   And, and certain corners of the hardware industry all at once.
[01:35:40.140 --> 01:35:57.140]   But all it really is is that this was a sort of black swan event that no one could have planned for people over planned or got giddy about, oh, this could be a phase change in terms of how consumers or even enterprises behave and what their demand could be.
[01:35:57.140 --> 01:36:09.940]   And so it's just going to take time to unwind that and, you know, in theory, like inflation is coming down and supply chains are getting back to normal, but there, maybe it's just going to take longer for, for.
[01:36:09.940 --> 01:36:11.940]   And tech hardware.
[01:36:11.940 --> 01:36:13.340]   Very interesting.
[01:36:13.340 --> 01:36:14.940]   All right, let's take a little break.
[01:36:14.940 --> 01:36:21.740]   Shelley Brisbane is here from six colors.com and the Texas standard at Texas standard org.
[01:36:21.740 --> 01:36:23.340]   Great to have you here.
[01:36:23.340 --> 01:36:24.340]   She's also on.
[01:36:24.340 --> 01:36:26.340]   I didn't even know he had one.
[01:36:26.340 --> 01:36:32.740]   Jason Snell's Mastodon instance, which has the marvelous name of Zeppelin dot flights.
[01:36:32.740 --> 01:36:34.340]   And she's Shelly on there.
[01:36:34.340 --> 01:36:35.540]   Zeppelin flights.
[01:36:35.740 --> 01:36:39.740]   Ask Jason about that on Tuesday and break weekly.
[01:36:39.740 --> 01:36:40.740]   Also here, Brian.
[01:36:40.740 --> 01:36:42.540]   I can tell you why it's called that if you care.
[01:36:42.540 --> 01:36:44.540]   Yeah, why is it called that?
[01:36:44.540 --> 01:36:53.740]   Well, so Zeppelin is the sort of mascot of the incomparable network, which is Jason's network of podcasts about pop culture and movies and TV superheroes.
[01:36:53.740 --> 01:36:56.540]   The incomparable Zeppelin for a logo.
[01:36:56.540 --> 01:36:57.340]   Yeah.
[01:36:57.340 --> 01:36:57.940]   Right.
[01:36:57.940 --> 01:37:00.540]   And so I can just decide to call it a simple and flex.
[01:37:00.540 --> 01:37:01.740]   That makes perfect sense.
[01:37:01.740 --> 01:37:04.540]   Besides Zeppelins are super cool.
[01:37:04.940 --> 01:37:05.940]   They are.
[01:37:05.940 --> 01:37:06.740]   Yeah.
[01:37:06.740 --> 01:37:12.540]   Some day, Jason's going to let me do a show on this is this is just I'm never casting to Jason.
[01:37:12.540 --> 01:37:18.540]   If he's out there somewhere, I want to do a show for the incomparable about movies featuring Zeppelins.
[01:37:18.540 --> 01:37:19.940]   Oh, I would listen to that.
[01:37:19.940 --> 01:37:20.540]   All right.
[01:37:20.540 --> 01:37:23.540]   There's a there's an audience yesterday.
[01:37:23.540 --> 01:37:30.740]   We had science fiction, Arthur Daniel Suarez on triangulation and there's a scene in his new book, Critical Mass.
[01:37:30.940 --> 01:37:38.540]   Where these billionaires are need to fly around and they don't want to pollute the atmosphere with their private jets.
[01:37:38.540 --> 01:37:43.340]   They have a private Zeppelin and it's so cool.
[01:37:43.340 --> 01:37:44.940]   Is it a self driving private?
[01:37:44.940 --> 01:37:46.940]   No, I think it's I think there's a pie.
[01:37:46.940 --> 01:37:52.540]   There's a captain and there's white glove stewards and there's silverware and it's very luxurious.
[01:37:52.540 --> 01:38:00.740]   They fly to Nigeria over a period of a day and a half from I think from Sweden and it sounds very very sophisticated.
[01:38:00.740 --> 01:38:02.940]   Very beautiful and very leisurely.
[01:38:02.940 --> 01:38:04.340]   They tried that once.
[01:38:04.340 --> 01:38:06.140]   No, that was the engine.
[01:38:06.140 --> 01:38:07.940]   But don't use hydrogen.
[01:38:07.940 --> 01:38:09.140]   That's all I'm right.
[01:38:09.140 --> 01:38:09.340]   Right.
[01:38:09.340 --> 01:38:10.940]   I Leo, I'm with you.
[01:38:10.940 --> 01:38:15.140]   I've never understood why that hasn't come back aside from the blowing up part.
[01:38:15.140 --> 01:38:25.940]   But how many other technologies have had lots of crashes and blowups and sinkings and yet we still went further with the technology.
[01:38:25.940 --> 01:38:29.740]   Why this one technology did we have a couple of bad.
[01:38:29.940 --> 01:38:34.540]   Events and we just abandoned it because airplanes were faster.
[01:38:34.540 --> 01:38:41.140]   Unfortunately, yeah, Sergey Brin, one of the founders of Google who has infinite money,
[01:38:41.140 --> 01:38:45.540]   had a Zeppelin company, an airship company.
[01:38:45.540 --> 01:38:46.340]   Really?
[01:38:46.340 --> 01:38:46.940]   Yes.
[01:38:46.940 --> 01:38:52.540]   In fact, you know those big if you've ever been down by Google down in Silicon Valley.
[01:38:52.540 --> 01:38:55.140]   There are leftover blimp hangers.
[01:38:55.140 --> 01:38:57.540]   I think from must be from World War II.
[01:38:57.540 --> 01:38:59.340]   He at least those.
[01:38:59.540 --> 01:39:04.540]   And was and was building blimps in there.
[01:39:04.540 --> 01:39:11.740]   But unfortunately, I think that was another of the of the victims of Google's.
[01:39:11.740 --> 01:39:16.540]   Fireings and shutdowns because I think that they they're not doing it anymore.
[01:39:16.540 --> 01:39:24.140]   You know, the the current the current generation of the Goodyear blimps that they introduced five or six years ago, I think.
[01:39:24.140 --> 01:39:25.940]   Those are actually not blimps anymore.
[01:39:26.140 --> 01:39:30.540]   They are in fact, Zeppelins and the Zeppelin company is still in business.
[01:39:30.540 --> 01:39:31.940]   They never went away.
[01:39:31.940 --> 01:39:38.540]   And so if you see a Goodyear blimp around these days, it is in fact a Zeppelin
[01:39:38.540 --> 01:39:46.140]   because the difference between a Zeppelin and a blimp is a blimp is basically just a balloon with the the pod underneath.
[01:39:46.140 --> 01:39:46.940]   I forget what it's called.
[01:39:46.940 --> 01:39:47.940]   Gondola.
[01:39:47.940 --> 01:39:48.940]   Yeah, Gondola.
[01:39:48.940 --> 01:39:49.140]   Yes.
[01:39:49.140 --> 01:39:49.740]   Thank you.
[01:39:49.740 --> 01:39:54.340]   And Zeppelin actually has a rigid frame and you're inside the balloon.
[01:39:54.340 --> 01:39:55.140]   Yeah.
[01:39:55.340 --> 01:40:01.140]   Well, I mean, they still have a Gondola, but the balloon itself has a rigid frame.
[01:40:01.140 --> 01:40:05.540]   Is this a Zeppelin or a derigable or a blimp?
[01:40:05.540 --> 01:40:06.740]   What are we looking at?
[01:40:06.740 --> 01:40:09.740]   A derigable is the generic term that encompasses both.
[01:40:09.740 --> 01:40:11.940]   So that's that's that looks like it's actually a Zeppelin.
[01:40:11.940 --> 01:40:13.740]   This is a very like a frame.
[01:40:13.740 --> 01:40:14.740]   To learn things.
[01:40:14.740 --> 01:40:16.140]   This is this is Larry.
[01:40:16.140 --> 01:40:17.540]   I mean, the Sergey's company.
[01:40:17.540 --> 01:40:20.140]   This is LTA research.com.
[01:40:20.140 --> 01:40:24.340]   And that is one of the big blimp hangers in Mountain View.
[01:40:24.540 --> 01:40:25.940]   And there is the blimp.
[01:40:25.940 --> 01:40:28.340]   I guess they're not out of business.
[01:40:28.340 --> 01:40:33.060]   They says we're we're united in the belief that next generation airships can complement
[01:40:33.060 --> 01:40:37.940]   humanitarian aid and reduce the carbon footprint of aviation.
[01:40:37.940 --> 01:40:45.140]   So, you know, if you got unlimited funds, do something crazy and imaginative.
[01:40:45.140 --> 01:40:53.740]   Here is a 3D model of the Pathfinder one, which is their their airship.
[01:40:53.940 --> 01:40:55.540]   Wow.
[01:40:55.540 --> 01:41:01.940]   So they they've avoided the whole blimp derigable thing by calling it an airship, right?
[01:41:01.940 --> 01:41:02.940]   But the Gondola.
[01:41:02.940 --> 01:41:06.340]   Yeah, it's got a fly by wire system.
[01:41:06.340 --> 01:41:11.060]   I feel like we we saw a bunch of stories five or six years ago about also this has
[01:41:11.060 --> 01:41:16.860]   industrial uses for like flying things into remote areas that, you know, a helicopter,
[01:41:16.860 --> 01:41:20.700]   like would be too heavy for a helicopter or even like a seat, whatever, you know,
[01:41:20.700 --> 01:41:23.540]   like so like you got light are, baby.
[01:41:23.740 --> 01:41:27.540]   Yeah, the payload capability is more than you could get with a helicopter.
[01:41:27.540 --> 01:41:28.540]   They're huge.
[01:41:28.540 --> 01:41:31.140]   Yeah, but they're slow, right?
[01:41:31.140 --> 01:41:32.340]   That's the back.
[01:41:32.340 --> 01:41:34.140]   So it depends on.
[01:41:34.140 --> 01:41:37.940]   I mean, there's a lot of applications where speed is not necessarily of the essence.
[01:41:37.940 --> 01:41:39.940]   And, you know, these would be a great.
[01:41:39.940 --> 01:41:42.540]   These are a great solution for those applications.
[01:41:42.540 --> 01:41:46.540]   I think Jason Snell should sell his,
[01:41:46.540 --> 01:41:51.940]   his Zeppelin site to Larry or a Sergey.
[01:41:51.940 --> 01:41:53.140]   Wow.
[01:41:53.340 --> 01:41:57.140]   Also here, Brian McCullough, Tech meme ride home podcast.
[01:41:57.140 --> 01:41:58.140]   Have you been doing it?
[01:41:58.140 --> 01:41:58.740]   How many years now?
[01:41:58.740 --> 01:42:00.740]   Five years, six years.
[01:42:00.740 --> 01:42:02.740]   Yeah, actually March 5th.
[01:42:02.740 --> 01:42:04.340]   So I got to think about that.
[01:42:04.340 --> 01:42:09.540]   It will be five years has has the news changed over five years.
[01:42:09.540 --> 01:42:10.740]   That's so funny.
[01:42:10.740 --> 01:42:16.540]   I have said that on recent shows where, you know, for how long the first few years
[01:42:16.540 --> 01:42:21.540]   it was tech is a descendant and tech is conquering all before it and tech is going
[01:42:21.740 --> 01:42:22.740]   to take over the world.
[01:42:22.740 --> 01:42:26.340]   And the last six months have been so.
[01:42:26.340 --> 01:42:30.420]   So many episodes where I'm like, hey, here's another narrative breaking.
[01:42:30.420 --> 01:42:32.140]   Here's a layoff story.
[01:42:32.140 --> 01:42:34.540]   Here's, you know, like it.
[01:42:34.540 --> 01:42:41.340]   The the triumphalism of the first three to four years has really been, you know,
[01:42:41.340 --> 01:42:43.540]   record scratched in the last year.
[01:42:43.540 --> 01:42:46.940]   I, you know, I mean, I've been doing this long enough that I've been through
[01:42:46.940 --> 01:42:51.540]   the bust in 2000, the bust in 2008.
[01:42:51.540 --> 01:42:55.140]   Now the bust in 2023.
[01:42:55.140 --> 01:43:00.940]   And it always seems to come back, but it is definitely it's it's interesting
[01:43:00.940 --> 01:43:05.540]   to watch the come the ebb and flow of our fortunes, isn't it?
[01:43:05.540 --> 01:43:06.140]   Yeah.
[01:43:06.140 --> 01:43:06.540]   Yeah.
[01:43:06.540 --> 01:43:11.340]   And like I said, I mean, that maybe that makes it better for me because it's not
[01:43:11.340 --> 01:43:12.740]   the same narrative.
[01:43:12.740 --> 01:43:14.540]   Oh, that's why I like it.
[01:43:14.540 --> 01:43:15.340]   It's not boring.
[01:43:15.340 --> 01:43:19.060]   I mean, who knew that we would spend so much time talking about balloons and
[01:43:19.260 --> 01:43:23.780]   attempts on this episode, right? Well, or, or like the, the, the, you know, now
[01:43:23.780 --> 01:43:27.060]   we have a new horse race, like going back to the, is it going to be Google?
[01:43:27.060 --> 01:43:27.860]   I bang.
[01:43:27.860 --> 01:43:28.260]   Yeah.
[01:43:28.260 --> 01:43:29.460]   You know, like, yeah, yeah.
[01:43:29.460 --> 01:43:33.980]   So there, I feel like the first three to four years of the show, the pandemic
[01:43:33.980 --> 01:43:38.260]   notwithstanding the narratives that I was covering it in the tech industry were
[01:43:38.260 --> 01:43:43.820]   sort of stuck in amber and the chessboard is sort of been thrown up in the air this
[01:43:43.820 --> 01:43:44.060]   year.
[01:43:44.060 --> 01:43:48.060]   Good thing to remember, if at any point we get a little bored, sometimes I do.
[01:43:48.260 --> 01:43:49.860]   Got sick of talking about Elon.
[01:43:49.860 --> 01:43:54.860]   There'll always be something new, different and interesting to talk about.
[01:43:54.860 --> 01:43:56.260]   Tech meme ride home.
[01:43:56.260 --> 01:44:01.060]   You get it right at tech meme.com or subscribe in your favorite podcast player.
[01:44:01.060 --> 01:44:04.260]   And hear the news change right before your very eyes.
[01:44:04.260 --> 01:44:06.060]   Sam, a bull Sam, it is also here.
[01:44:06.060 --> 01:44:11.860]   Our car, my car, my personal car guy, wheel bearings podcast is at wheel bearings.media.
[01:44:11.860 --> 01:44:13.260]   Robbie's on that too.
[01:44:13.260 --> 01:44:13.860]   I love him.
[01:44:13.860 --> 01:44:15.060]   We've been trying to get him on Twitter.
[01:44:15.060 --> 01:44:15.860]   And the cold.
[01:44:15.860 --> 01:44:17.260]   Don't forget the cold too.
[01:44:17.860 --> 01:44:23.060]   Great show for people who love vehicles, motor vehicles.
[01:44:23.060 --> 01:44:27.460]   Our show today brought to you by Zip Recruiter
[01:44:27.460 --> 01:44:32.860]   as the world ebbs and flows jobs come and go.
[01:44:32.860 --> 01:44:37.060]   You may want to know about Zip Recruiter because there's going to come a time when
[01:44:37.060 --> 01:44:37.860]   you're hiring.
[01:44:37.860 --> 01:44:41.860]   And of course, those vacancies always come along at the worst time.
[01:44:41.860 --> 01:44:43.580]   You know, for us, we're a small company.
[01:44:43.580 --> 01:44:44.660]   Somebody leaves.
[01:44:44.860 --> 01:44:50.460]   That means we all have to work a little bit harder and we've got to scramble to fill
[01:44:50.460 --> 01:44:51.260]   that position.
[01:44:51.260 --> 01:44:52.660]   Thank goodness.
[01:44:52.660 --> 01:44:53.860]   Zip Recruiter is here.
[01:44:53.860 --> 01:44:57.460]   We use Zip Recruiter all the time to do our hiring.
[01:44:57.460 --> 01:45:02.260]   And believe me, there are plenty of industries that are hiring these days.
[01:45:02.260 --> 01:45:07.060]   There's in many industries a shortage of talent, but don't worry because no matter
[01:45:07.060 --> 01:45:12.060]   one industry you're in, when you need to hire, you go to ziprecruiter.com/twit.
[01:45:12.060 --> 01:45:13.860]   You could try it for free.
[01:45:14.060 --> 01:45:15.660]   It works so well.
[01:45:15.660 --> 01:45:20.860]   We've hired some of our best people there when when our wonderful Ashley got a better
[01:45:20.860 --> 01:45:21.060]   job.
[01:45:21.060 --> 01:45:24.460]   She was in our continuity department and Lisa and I are going, Oh, what are we going to do?
[01:45:24.460 --> 01:45:25.460]   What are we going to do?
[01:45:25.460 --> 01:45:29.660]   Lisa went to Zip Recruiter, posted the job opening right there.
[01:45:29.660 --> 01:45:34.060]   And it's fast because you're posting to more than 100 job sites with one click of the mouse.
[01:45:34.060 --> 01:45:39.060]   But it's also fast because Zip Recruiter looks at your requirements, then looks at all the resumes.
[01:45:39.060 --> 01:45:42.060]   They have more than a million current resumes on hand.
[01:45:42.260 --> 01:45:47.060]   And we'll say they'll pick out people who match those needs and they'll say to you,
[01:45:47.060 --> 01:45:50.460]   Hey, look, here's 10 people who really fit the bill.
[01:45:50.460 --> 01:45:53.260]   You decide whether you want to invite them to apply or not.
[01:45:53.260 --> 01:45:54.460]   But I'll tell you this too.
[01:45:54.460 --> 01:45:57.260]   When you invite somebody to apply, they're so flattered.
[01:45:57.260 --> 01:45:58.260]   They're so excited.
[01:45:58.260 --> 01:46:03.060]   It's a great beginning to a potentially fabulous experience.
[01:46:03.060 --> 01:46:08.540]   When we post on Zip Recruiter, we usually get somebody great within an hour or two at least
[01:46:08.740 --> 01:46:10.140]   post at breakfast by lunch.
[01:46:10.140 --> 01:46:11.940]   She's going, Oh, here's another one.
[01:46:11.940 --> 01:46:13.140]   Here's another one.
[01:46:13.140 --> 01:46:15.340]   And that is such a relief when you're a person down.
[01:46:15.340 --> 01:46:16.220]   It really is great.
[01:46:16.220 --> 01:46:17.340]   That's how we found Viva.
[01:46:17.340 --> 01:46:24.340]   Zip Recruiter uses its powerful matching technology to find candidates for practically any role
[01:46:24.340 --> 01:46:31.340]   that makes it very easy for you to process those applications because they don't go to your voice mail or email inbox.
[01:46:31.340 --> 01:46:33.140]   They go into the Zip Recruiter interface.
[01:46:33.140 --> 01:46:35.940]   They reformat all the resumes so it's easy to scan them.
[01:46:36.140 --> 01:46:44.140]   You can create screening questions, true, false, multiple choice, even essay to eliminate people who just don't fit the fit the bill.
[01:46:44.140 --> 01:46:50.940]   You can narrow it down, rank them, screen them, rank them and hire the right one fast in one place.
[01:46:50.940 --> 01:46:52.340]   It's so great.
[01:46:52.340 --> 01:46:57.940]   In fact, four out of five employers who post on Zip Recruiter get a quality candidate in the first day.
[01:46:57.940 --> 01:46:59.740]   That's always been our experience.
[01:46:59.740 --> 01:47:01.140]   It really is a relief.
[01:47:01.140 --> 01:47:05.740]   You go, Oh, we're not going to have to operate at half speed.
[01:47:05.940 --> 01:47:07.340]   Find quality candidates fast.
[01:47:07.340 --> 01:47:13.740]   Let Zip Recruiter keep your team growing strong no matter what industry, no matter what field.
[01:47:13.740 --> 01:47:15.140]   Zip Recruiter is great.
[01:47:15.140 --> 01:47:17.340]   Go to ziprecruiter.com/twit.
[01:47:17.340 --> 01:47:18.740]   You could try it there for free.
[01:47:18.740 --> 01:47:21.740]   Zip Recruiter.com/twit.
[01:47:21.740 --> 01:47:24.540]   They have been so great for us all these years.
[01:47:24.540 --> 01:47:25.940]   We've used them many, many times.
[01:47:25.940 --> 01:47:27.540]   They've been a sponsor here for a long time.
[01:47:27.540 --> 01:47:29.140]   Thank you, Zip Recruiter.
[01:47:29.140 --> 01:47:30.140]   We really appreciate it.
[01:47:30.140 --> 01:47:35.340]   And thank you to all of you who listen and use those addresses
[01:47:35.540 --> 01:47:36.540]   because that makes a big difference.
[01:47:36.540 --> 01:47:37.340]   And they know you saw it here.
[01:47:37.340 --> 01:47:42.340]   ZipRecruiter.com/twit.
[01:47:42.340 --> 01:47:46.340]   We had a really great week this week on all the shows.
[01:47:46.340 --> 01:47:47.740]   I mean, not just Daniel Swarth.
[01:47:47.740 --> 01:47:48.540]   Well, you know what?
[01:47:48.540 --> 01:47:52.740]   I think we have a little video you can watch for yourself only on Club Twit.
[01:47:52.740 --> 01:47:57.940]   I'm going to sit down today with another fine edition here at Twit.
[01:47:57.940 --> 01:48:01.340]   The wonderful co-host of all about Android.
[01:48:01.540 --> 01:48:05.740]   She's also quite busy doing things in the world of Devlife
[01:48:05.740 --> 01:48:08.740]   and the fitness world and a whole bunch of other things.
[01:48:08.740 --> 01:48:13.140]   Allow me to welcome today's guest, Miss Nguyen, Twit Dao.
[01:48:13.140 --> 01:48:14.340]   How you be, lady?
[01:48:14.340 --> 01:48:15.740]   Oh, my gosh. So good.
[01:48:15.740 --> 01:48:17.340]   Especially after that kind of intro.
[01:48:17.340 --> 01:48:18.140]   Oh, my God.
[01:48:18.140 --> 01:48:19.340]   That's so nice.
[01:48:19.340 --> 01:48:23.540]   Oh, I don't really feel that fascinating, but I'm really happy to be here.
[01:48:23.540 --> 01:48:27.340]   You are previously on Twit.
[01:48:27.540 --> 01:48:32.340]   Triangulation on his fifth visit to our studios.
[01:48:32.340 --> 01:48:35.740]   Daniel Swarth has a new book and the world rejoices.
[01:48:35.740 --> 01:48:38.340]   It feels like we're in a renaissance of space exploration.
[01:48:38.340 --> 01:48:39.140]   I truly believe that.
[01:48:39.140 --> 01:48:41.940]   And we just need more and more people,
[01:48:41.940 --> 01:48:45.340]   put billionaires and government officials who vote on these things.
[01:48:45.340 --> 01:48:50.140]   As well as the general public to realize, we really are on sort of the brink of a golden age.
[01:48:50.140 --> 01:48:51.740]   We make the right decisions.
[01:48:51.740 --> 01:48:54.740]   So we're at, I'd say, a fork in the road.
[01:48:54.940 --> 01:48:57.540]   One way is a crisis and one way is a golden age.
[01:48:57.540 --> 01:48:58.740]   All about Android.
[01:48:58.740 --> 01:49:06.340]   I have in my hands the one plus 11, five G and I give you my full review in tonight's episode.
[01:49:06.340 --> 01:49:11.140]   Tech news weekly, Mike Asargent and I, Jason Howell, talk to Joseph Cox from Vice
[01:49:11.140 --> 01:49:15.940]   about how AI systems are creating convincing replicas of celebrity voices.
[01:49:15.940 --> 01:49:19.340]   Like mine right now, often without permission to do so.
[01:49:19.340 --> 01:49:21.540]   AI sure is a big deal, it seems.
[01:49:21.740 --> 01:49:25.940]   Hey, there's only room for one AI host on the Twit Network.
[01:49:25.940 --> 01:49:30.940]   That my voice in that whole promo was artificially generated.
[01:49:30.940 --> 01:49:36.740]   That's the 11 labs version of Jason Howell and my voice.
[01:49:36.740 --> 01:49:37.540]   I've done that.
[01:49:37.540 --> 01:49:38.140]   I did that.
[01:49:38.140 --> 01:49:38.740]   I have to.
[01:49:38.740 --> 01:49:41.340]   Well, yeah, it's kind of tempting, isn't it?
[01:49:41.340 --> 01:49:44.740]   Well, I have to take a trip to Ireland.
[01:49:44.740 --> 01:49:46.340]   Well, have to.
[01:49:46.340 --> 01:49:51.340]   What if I did instead of hiring somebody to fill in for me for a week?
[01:49:51.540 --> 01:49:52.140]   Exactly.
[01:49:52.140 --> 01:49:57.740]   What if I just wrote out the show and didn't have to edit it and process it just.
[01:49:57.740 --> 01:49:58.940]   Yeah.
[01:49:58.940 --> 01:50:01.940]   So I it's not there yet, though, is it?
[01:50:01.940 --> 01:50:08.940]   No, but it's actually the 11 labs is the closest one because I did the descriptive one a couple years ago and it was nowhere near.
[01:50:08.940 --> 01:50:09.540]   Yeah, the script.
[01:50:09.540 --> 01:50:13.740]   The nice thing about the script was it was clearly generated.
[01:50:13.740 --> 01:50:15.140]   That one was close enough.
[01:50:15.140 --> 01:50:16.740]   It just sounded like it was Leo sick.
[01:50:16.940 --> 01:50:20.340]   This this adds like the breaths.
[01:50:20.340 --> 01:50:21.740]   Yeah, that's pretty good.
[01:50:21.740 --> 01:50:23.140]   Sort of the emotion.
[01:50:23.140 --> 01:50:30.140]   So if they could just add the dials where I could add emphasis and, you know, with italics or something like that, we're almost there.
[01:50:30.140 --> 01:50:33.740]   But I don't know if you've played with it, but the other weird thing is.
[01:50:33.740 --> 01:50:44.340]   If you if you say if you give it a paragraph and you say generate, it'll perform it in a certain way, generate it again with changing nothing.
[01:50:44.340 --> 01:50:46.340]   It'll perform it in a completely different way.
[01:50:46.340 --> 01:50:47.340]   Like do it again completely.
[01:50:47.340 --> 01:50:48.140]   Yeah, yeah.
[01:50:48.140 --> 01:50:49.340]   Yeah, it's good at that.
[01:50:49.340 --> 01:50:50.940]   Shelly, you said you played with it too?
[01:50:50.940 --> 01:51:06.740]   Yeah, well, I decided I don't know why I decided made this terrible decision, but it had been going around in a podcast or slack and a bunch of us have been posting samples and I was due to do a Q and A on the radio show, which as a producer means that I wrote the script and that I was going to talk to the host.
[01:51:06.740 --> 01:51:09.340]   I wrote him some questions that he asked me and then I answered them.
[01:51:09.340 --> 01:51:16.140]   And so what I did was I took that script and I trained it on both of our voices and I was like, why am I putting myself out?
[01:51:16.140 --> 01:51:20.140]   I don't know, but you can train it with multiple samples.
[01:51:20.140 --> 01:51:27.140]   And so the first thing I did was I trained it on myself doing something like I would do for the radio show, which is essentially reading about a serious topic.
[01:51:27.140 --> 01:51:30.340]   We were talking about disability legislation and the legislature.
[01:51:30.340 --> 01:51:38.540]   And then I decided to give it a couple of samples, one from my podcast about tech and one from my podcast about movies.
[01:51:38.540 --> 01:51:40.940]   So sort of a different tone, sort of a different focus.
[01:51:41.140 --> 01:51:52.340]   And I noticed as I added samples, and this was my interpretation and maybe what Brian says is right that you can read the same script and have it come back with different sort of tone or whatever.
[01:51:52.340 --> 01:51:58.540]   But my feeling was that once I had given it a couple of more of my voice samples, it changed and it got better.
[01:51:58.540 --> 01:51:59.540]   It still wasn't me.
[01:51:59.540 --> 01:52:08.740]   I could absolutely tell the breathing was right, the sort of emphasis, especially when I because I made a couple punctuation mistakes at first and I changed them up and it was a little bit better.
[01:52:08.940 --> 01:52:20.340]   But it was sort of flatter and without personality and I chose the clips from the movie podcast specifically where I'm sort of, you know, excited about something or I'm trying to engage a guest and I've got energy.
[01:52:20.340 --> 01:52:23.940]   So I'm trying to figure out whether it's going to transfer that to the clip and it didn't.
[01:52:23.940 --> 01:52:31.540]   And it really did not do particularly well on my on my radio show host where I only put one clip of him in.
[01:52:31.540 --> 01:52:34.340]   But yeah, I was able to fake an entire radio segment.
[01:52:34.340 --> 01:52:35.940]   I didn't put it on the air, but I had it.
[01:52:35.940 --> 01:52:37.540]   Can I find it? Is it on the net?
[01:52:37.740 --> 01:52:41.740]   No, no, no, it's not. I can send it to you, but I want to hear it.
[01:52:41.740 --> 01:52:42.940]   It was entertaining.
[01:52:42.940 --> 01:52:45.340]   Yeah, I don't think we have to worry yet.
[01:52:45.340 --> 01:52:50.740]   But in fact, Brian, you put this story in my inbox.
[01:52:50.740 --> 01:53:00.340]   Voice artists are worried about chat GPT and other AI genera.
[01:53:00.340 --> 01:53:03.540]   I guess not chat GPT, so much as 11 labs.
[01:53:03.740 --> 01:53:06.540]   Every generative stuff generative is scaring him.
[01:53:06.540 --> 01:53:13.740]   And in fact, more than that, they're now being asked to sign away their rights,
[01:53:13.740 --> 01:53:15.140]   which is interesting.
[01:53:15.140 --> 01:53:18.740]   This is the story from Vice to disrespectful to the craft.
[01:53:18.740 --> 01:53:22.540]   Actors say they're being asked to sign away their voice to AI.
[01:53:22.540 --> 01:53:29.540]   You know, when we do voice work, there's always a contract usually says something like we own this recording
[01:53:29.740 --> 01:53:36.740]   for use in every medium conceived of to this point and anything that has ever conceived of in the future.
[01:53:36.740 --> 01:53:47.340]   We use the same release, I believe, but in any event, it's now becoming a little bit concerning.
[01:53:47.340 --> 01:53:54.340]   We had the story a couple of weeks ago that folks at 4chan had used 11 labs to create celebrity voices,
[01:53:54.340 --> 01:53:57.740]   doing appalling things.
[01:53:57.940 --> 01:54:10.340]   Well, the converse would be, I said this recently, Morgan Friedman should sell the rights and perpetuity to his voice.
[01:54:10.340 --> 01:54:13.140]   He'd probably be able to get $50 million right now.
[01:54:13.140 --> 01:54:16.540]   So like, maybe he wouldn't have to do a thing.
[01:54:16.540 --> 01:54:19.540]   Oh, well, maybe he'd have to go to a studio and record for a few days.
[01:54:19.540 --> 01:54:24.540]   No, they have so much of it and this stuff is getting so fast, good so fast.
[01:54:24.740 --> 01:54:31.740]   Sign away your rights and perpetuity and then you'll never lose the March of the Penguins voice over.
[01:54:31.740 --> 01:54:33.740]   Well, wait a minute now, here is an AI.
[01:54:33.740 --> 01:54:35.140]   Well, James Jones already did that.
[01:54:35.140 --> 01:54:36.540]   Oh, that's right.
[01:54:36.540 --> 01:54:37.940]   James Jones did sell his voice.
[01:54:37.940 --> 01:54:44.140]   Well, and that's great for somebody whose career is close to an end or who may be looking for a legacy for their family.
[01:54:44.140 --> 01:54:49.340]   But if I'm a worse voice actor and I'm 30 years old and maybe I've even just had a hit,
[01:54:49.540 --> 01:54:58.340]   maybe I had a big role in a video game or a movie or something like that and you're asking me to sign away my rights to a specific piece of work that I am about to create for you.
[01:54:58.340 --> 01:54:59.740]   That's a really different situation.
[01:54:59.740 --> 01:55:01.340]   Yeah.
[01:55:01.340 --> 01:55:10.140]   Well, all of us are, I guess, affected now by AI, no matter what your skill.
[01:55:10.140 --> 01:55:14.940]   I mean, I can have chat, GPT, write the script and then I can have 11 laps of reading.
[01:55:15.140 --> 01:55:23.740]   Yeah, and then it's going to come for images too, you know, all the new Indiana Jones thing with Harrison Ford being the younger thing like you won't need Harrison Ford.
[01:55:23.740 --> 01:55:31.540]   You could just do Indiana Jones and perpetuity if he signed over his rights to his visage and now his voice and.
[01:55:31.540 --> 01:55:32.740]   Yeah.
[01:55:32.740 --> 01:55:39.740]   Here is another story from Motherboard about chat GPT and for some reason.
[01:55:40.740 --> 01:55:45.940]   You can put weird words in there and it responds in an even weirder way.
[01:55:45.940 --> 01:55:47.940]   We think we know what's going on though.
[01:55:47.940 --> 01:55:53.940]   There is a subreddit called count where people are just counting them and doing it for five years.
[01:55:53.940 --> 01:56:03.140]   They're trying to get to what five million one at a time and they have a list of the top.
[01:56:03.340 --> 01:56:09.740]   Chat chat, a chat, a Reddit handles in count and those are the words.
[01:56:09.740 --> 01:56:32.940]   So something about the way chat GPT absorbs the count subreddit has given these people some sort of weird celebrity in GPT's token set is what vice calls it, including solid gold, magic, carp, streamer, bot and the nitroam fan with the leading space.
[01:56:33.140 --> 01:56:36.940]   Now you probably can't chat type those into chat GPT anymore.
[01:56:36.940 --> 01:56:38.340]   I'm sure people have.
[01:56:38.340 --> 01:57:00.140]   You know, but the funny thing was like even even even if they fix this, what I thought was interesting was like the way that you can sort of glitch out the bots sort of like Blade Runner style because one of the quotes in first five paragraphs Leo is like they they put the name in and the bot responded by saying to hell with you.
[01:57:00.340 --> 01:57:03.340]   It's like it's hysterical.
[01:57:03.340 --> 01:57:06.340]   It's like it breaks the AI.
[01:57:06.340 --> 01:57:06.740]   Yeah.
[01:57:06.740 --> 01:57:07.540]   Yeah.
[01:57:07.540 --> 01:57:10.740]   And for some reason that's just wonderful.
[01:57:10.740 --> 01:57:12.740]   We just love that, don't we?
[01:57:12.740 --> 01:57:13.940]   Oh, yeah.
[01:57:13.940 --> 01:57:19.940]   How nine thousand not with stand gives us a sense of superiority that we're still we're still be relevant.
[01:57:19.940 --> 01:57:21.540]   We're still on top at least.
[01:57:21.540 --> 01:57:26.540]   Well, you could just still do things that are whimsical and weird and funny and hell with you.
[01:57:26.540 --> 01:57:27.940]   Yeah, I live for that sort of.
[01:57:30.140 --> 01:57:30.940]   Wow.
[01:57:30.940 --> 01:57:37.540]   Hey, everybody, it's Leo, the port the founder and host of many of the Twitter podcasts.
[01:57:37.540 --> 01:57:43.340]   I don't normally talk to you about advertising, but I want to take a moment to do that right now.
[01:57:43.340 --> 01:57:50.340]   Our mission statement of Twitter, we're dedicated to building a highly engaged community of tech enthusiasts.
[01:57:50.340 --> 01:57:59.540]   That's our audience and you, I guess, since you're listening by offering them the knowledge they need to understand and use technology in today's world.
[01:57:59.940 --> 01:58:06.940]   To do that, we also create partnerships with trusted brands and make important introductions between them and our audience.
[01:58:06.940 --> 01:58:12.340]   That's how we finance our podcasts, but it's also and our audience tells us this all the time.
[01:58:12.340 --> 01:58:18.340]   A part of the service we offer, it's a valued bit of information for our audience members.
[01:58:18.340 --> 01:58:21.940]   They want to know about great brands like yours.
[01:58:21.940 --> 01:58:28.340]   So can we help you by introducing you to our highly qualified audience?
[01:58:28.340 --> 01:58:32.740]   And why do you get a lot with advertising on the Twitter podcasts?
[01:58:32.740 --> 01:58:38.540]   Partnering with Twitter means you're going to get, if I may say so, humbly, the gold standard in podcast advertising.
[01:58:38.540 --> 01:58:41.240]   And we throw in a lot of valuable services.
[01:58:41.240 --> 01:58:46.640]   You get a full service continuity team supporting everything from copywriting to graphic design.
[01:58:46.640 --> 01:58:50.540]   I don't think anybody else does this or does this as well as we do.
[01:58:50.540 --> 01:58:54.740]   You get ads that are embedded in our content that are unique every time.
[01:58:54.740 --> 01:58:56.140]   I read them, our hosts read them.
[01:58:56.640 --> 01:58:58.940]   We always over deliver on impressions.
[01:58:58.940 --> 01:59:03.140]   And frankly, we're here to talk about your product.
[01:59:03.140 --> 01:59:08.040]   So we really give our listeners a great introduction to what you offer.
[01:59:08.040 --> 01:59:13.440]   We've got onboarding services, ad tech with pod sites that's free for direct clients.
[01:59:13.440 --> 01:59:17.640]   We give you a lot of reporting so you know who saw your advertisement.
[01:59:17.640 --> 01:59:21.040]   You'll even know how many responded by going to your website.
[01:59:21.040 --> 01:59:26.440]   We'll also give you courtesy commercials that you can share across social media and landing pages.
[01:59:26.740 --> 01:59:28.340]   We think these are really valuable.
[01:59:28.340 --> 01:59:34.140]   People like me and our other hosts talking about your products sincerely and information.
[01:59:34.140 --> 01:59:35.740]   Those are incredibly valuable.
[01:59:35.740 --> 01:59:40.640]   You also get other free goodies, mentions in our weekly newsletter that's sent out to thousands of fans.
[01:59:40.640 --> 01:59:45.740]   We give bonus ads to people who buy a significant amount of advertising.
[01:59:45.740 --> 01:59:47.740]   You'll get social media promotion too.
[01:59:47.740 --> 01:59:53.040]   But let me tell you, we are looking for an advertising partner that's going to be with us long term.
[01:59:53.440 --> 01:59:57.140]   Visit twit.tv/advertise check out our partner testimonials.
[01:59:57.140 --> 01:59:59.740]   Tim Broome, founder of ITProTV.
[01:59:59.740 --> 02:00:10.940]   They started ITProTV in 2013, immediately started advertising with us and grew that company to a really amazing success.
[02:00:10.940 --> 02:00:14.040]   Hundreds of thousands of ongoing customers.
[02:00:14.040 --> 02:00:21.140]   They've been on our network for more than 10 years and they say and I'll quote Tim, we would not be where we are today without the Twit Network.
[02:00:21.140 --> 02:00:22.240]   That's just one example.
[02:00:22.640 --> 02:00:24.740]   Mark McCurry, who's the CEO of Authentic.
[02:00:24.740 --> 02:00:29.440]   He was actually one of the first people to buy ads on our network.
[02:00:29.440 --> 02:00:31.240]   He's been with us for 16 years.
[02:00:31.240 --> 02:00:44.440]   He said, and I'm quoting the feedback from many advertisers over those 16 years across a range of product categories is that if ads and podcasts are going to work for a brand, they're going to work on Twit shows.
[02:00:44.440 --> 02:00:51.340]   I'm proud to say that the ads we do over deliver, they work really well because they're honest.
[02:00:51.340 --> 02:00:52.340]   They have integrity.
[02:00:52.340 --> 02:00:55.740]   Our audience trusts us and we say, this is a great product.
[02:00:55.740 --> 02:00:56.940]   They believe it.
[02:00:56.940 --> 02:00:57.740]   They listen.
[02:00:57.740 --> 02:01:00.040]   Our listeners are highly intelligent.
[02:01:00.040 --> 02:01:01.340]   They're heavily engaged.
[02:01:01.340 --> 02:01:02.440]   They're tech savvy.
[02:01:02.440 --> 02:01:04.240]   They're dedicated to our network.
[02:01:04.240 --> 02:01:11.840]   And that's partly because we only work with high integrity partners that we have thoroughly and personally vetted.
[02:01:11.840 --> 02:01:15.640]   I approve every single advertiser on the network.
[02:01:15.640 --> 02:01:20.540]   If you're ready to elevate your brand and you've got a great product, I want you to reach out to us.
[02:01:20.740 --> 02:01:23.440]   Advertise at Twit.tv.
[02:01:23.440 --> 02:01:26.840]   So I want you to break out of the advertising norm.
[02:01:26.840 --> 02:01:32.840]   Grow your brand with host red authentic ads on Twit.tv.
[02:01:32.840 --> 02:01:36.740]   Visit Twit.tv/Advertise for more details or email us.
[02:01:36.740 --> 02:01:40.840]   Advertise at Twit.tv if you're ready to launch your campaign now.
[02:01:40.840 --> 02:01:44.140]   AI for video is coming.
[02:01:44.140 --> 02:01:48.340]   The folks who did stable diffusion runway have a new model.
[02:01:48.340 --> 02:01:49.940]   They call it Gen 1.
[02:01:50.540 --> 02:01:56.340]   They can create videos based on an input, a sample.
[02:01:56.340 --> 02:01:58.540]   And then they can they can.
[02:01:58.540 --> 02:02:00.740]   Oh, let's see if I can get it to playback.
[02:02:00.740 --> 02:02:06.740]   It looks like it won't play back in my on Linux here, but the top animated gift gives you an example,
[02:02:06.740 --> 02:02:13.540]   but you could give them a video of a real person doing something and then change the background entirely.
[02:02:13.540 --> 02:02:14.040]   So get rid.
[02:02:14.040 --> 02:02:17.540]   I mean, this is deep fakes paradise, isn't it?
[02:02:18.640 --> 02:02:21.740]   Transforming existing videos into into new ones.
[02:02:21.740 --> 02:02:22.140]   Let's see.
[02:02:22.140 --> 02:02:26.140]   Maybe here I can get it to play from the from the runway site.
[02:02:26.140 --> 02:02:30.040]   So this is the this is the actual video on the left.
[02:02:30.040 --> 02:02:36.440]   It's a subway real subway in New York City, but I can make it a cartoon using the AI.
[02:02:36.440 --> 02:02:39.540]   It's a pretty good cartoon, right?
[02:02:39.540 --> 02:02:48.440]   You can have a human being acting kind of silly in a park, but then apply it to a magma.
[02:02:49.040 --> 02:02:52.940]   Monster and suddenly you've got a pretty good.
[02:02:52.940 --> 02:02:55.540]   I mean, this is a lot easier than doing.
[02:02:55.540 --> 02:02:57.540]   Roto scoping.
[02:02:57.540 --> 02:02:59.240]   Here's another one.
[02:02:59.240 --> 02:03:10.640]   They use blocks to or actually these are looks like these are runway manuals or notebooks set up to kind of simulate skyscrapers in a road and turn it into skyscrapers in a road.
[02:03:10.640 --> 02:03:12.840]   This is really impressive.
[02:03:12.840 --> 02:03:14.840]   Give a dog some dots.
[02:03:16.940 --> 02:03:22.140]   Research that runway ML.com if you want to see this is Gen one.
[02:03:22.140 --> 02:03:25.940]   It's going to allow you to do some very interesting things.
[02:03:25.940 --> 02:03:38.840]   Again, varietals though, like what if you prefer the style of one bot versus the style of another bot or you were in you were an architect versus an artist.
[02:03:38.840 --> 02:03:45.840]   Well, I wonder, you know, for instance, this image that it's creating is this is some artist draw that originally and this is just kind of.
[02:03:46.840 --> 02:03:50.840]   Stealing it in effect or is it completely generated from scratch?
[02:03:50.840 --> 02:03:54.840]   No, it's been it's been trained on various real real images.
[02:03:54.840 --> 02:03:54.840]   Yeah.
[02:03:54.840 --> 02:03:55.340]   Yeah.
[02:03:55.340 --> 02:03:55.840]   Yeah.
[02:03:55.840 --> 02:03:56.340]   Yeah.
[02:03:56.340 --> 02:04:05.840]   And I assume that would happen with video where you could have trained it on the existing TV and movie universe or any video and all of you to forget this.
[02:04:05.840 --> 02:04:06.340]   Right.
[02:04:06.340 --> 02:04:08.840]   And has already been done.
[02:04:08.840 --> 02:04:09.340]   Right.
[02:04:09.340 --> 02:04:12.840]   Uh, very interesting.
[02:04:14.340 --> 02:04:17.340]   I think we can probably call it a day.
[02:04:17.340 --> 02:04:22.840]   You guys have been working hard and I think it's time to say goodbye.
[02:04:22.840 --> 02:04:25.340]   But thank you so much for being here.
[02:04:25.340 --> 02:04:26.840]   We had a wonderful time.
[02:04:26.840 --> 02:04:27.840]   Shelly Brisbane.
[02:04:27.840 --> 02:04:29.340]   You are fantastic.
[02:04:29.340 --> 02:04:34.340]   I'll look for you on six colors and at Texas standard.org and on Zeppelin flights.
[02:04:34.340 --> 02:04:37.340]   I've already followed you on Mastodon and I will follow you.
[02:04:37.340 --> 02:04:38.340]   At Shelley at.
[02:04:38.340 --> 02:04:39.340]   I've already followed some of.
[02:04:39.340 --> 02:04:40.340]   Yeah.
[02:04:40.340 --> 02:04:42.340]   That's nice.
[02:04:42.340 --> 02:04:43.340]   Really nice.
[02:04:43.340 --> 02:04:46.340]   Anything else you want to mention that you're working on?
[02:04:46.340 --> 02:04:57.340]   Uh, well, I have a book that I produce for each edition of iOS, iOS access for all, which is a book about everything to do with accessibility for the iOS platform.
[02:04:57.340 --> 02:04:59.340]   And you can find that at iOS access book.com.
[02:04:59.340 --> 02:05:01.340]   I forgot all about that.
[02:05:01.340 --> 02:05:03.340]   You were on mentioned that last time and good.
[02:05:03.340 --> 02:05:04.340]   I'm glad we could give it a plug.
[02:05:04.340 --> 02:05:05.340]   Got a plug to book.
[02:05:05.340 --> 02:05:06.340]   Got a plug to plug.
[02:05:06.340 --> 02:05:07.340]   You got a new one.
[02:05:07.340 --> 02:05:08.340]   Mike about it for iOS today.
[02:05:08.340 --> 02:05:09.340]   Yes.
[02:05:09.340 --> 02:05:12.340]   You know, the new one for iOS 16 is out.
[02:05:12.340 --> 02:05:19.340]   And, uh, and yeah, if you have accessibility needs, uh, iPhone does a very good job.
[02:05:19.340 --> 02:05:24.340]   iOS does a very good job for accessibility, but it kind of helps to know what it's capable of.
[02:05:24.340 --> 02:05:28.340]   Uh, Shelley, this is a much needed, very valuable book for everybody.
[02:05:28.340 --> 02:05:30.340]   So thank you for doing that.
[02:05:30.340 --> 02:05:35.340]   Um, iOS access book.com.
[02:05:35.340 --> 02:05:36.340]   Brian McCullough.
[02:05:36.340 --> 02:05:38.340]   When are you going to Ireland?
[02:05:38.340 --> 02:05:41.340]   Oh, you're muted.
[02:05:41.340 --> 02:05:42.340]   All right.
[02:05:42.340 --> 02:05:43.340]   Sorry.
[02:05:43.340 --> 02:05:44.340]   Second, second week of April.
[02:05:44.340 --> 02:05:46.340]   Are you excited?
[02:05:46.340 --> 02:05:58.340]   Uh, definitely seeing as how that's the first time that I've personally gone and, um, you know, like I said, um, maybe that week, the five episodes of the tech meme ride home, uh,
[02:05:58.340 --> 02:05:59.340]   may I generated?
[02:05:59.340 --> 02:06:04.340]   Maybe I won't have to take a microphone in my, in my carry on and things like that.
[02:06:04.340 --> 02:06:05.340]   I will literally just do it.
[02:06:05.340 --> 02:06:07.340]   I, Leo, I really might.
[02:06:07.340 --> 02:06:09.340]   I really like that's an interesting idea.
[02:06:09.340 --> 02:06:12.340]   You could still write it and just have the AI generate it.
[02:06:12.340 --> 02:06:13.340]   Would you just prepare?
[02:06:13.340 --> 02:06:14.340]   What would you use?
[02:06:14.340 --> 02:06:18.340]   No, I'm going to use the 11 labs if, because I keep feeding it stuff from my show.
[02:06:18.340 --> 02:06:19.340]   It's getting better and better.
[02:06:19.340 --> 02:06:20.340]   Yeah.
[02:06:20.340 --> 02:06:22.340]   I've been preparing the audience for this.
[02:06:22.340 --> 02:06:25.340]   Like we've done segments and I've said, okay, here we go.
[02:06:25.340 --> 02:06:29.340]   Cause every, you know, when all this week, it was nothing but AI news.
[02:06:29.340 --> 02:06:30.340]   And so I couldn't help myself.
[02:06:30.340 --> 02:06:31.340]   I'm like, okay.
[02:06:31.340 --> 02:06:32.340]   Yeah.
[02:06:32.340 --> 02:06:34.340]   We, we got to do this next segment with my, uh,
[02:06:34.340 --> 02:06:35.340]   We're all doing it.
[02:06:35.340 --> 02:06:36.340]   We're all doing it.
[02:06:36.340 --> 02:06:41.980]   So if, if I prepare the audience and I say, I'm not going to do this forever, but for this
[02:06:41.980 --> 02:06:44.300]   one week, will you accept it?
[02:06:44.300 --> 02:06:48.460]   And the interesting thing was I did one experiment with different voice.
[02:06:48.460 --> 02:06:52.180]   I did an Irish accent, you know, not my voice.
[02:06:52.180 --> 02:06:57.380]   Um, and people hated it, but when I did it with my voice this week, people were like,
[02:06:57.380 --> 02:06:59.340]   Oh, that's not bad.
[02:06:59.340 --> 02:07:02.700]   So again, now, but again, what are the implications?
[02:07:02.700 --> 02:07:09.180]   The implications are, um, I'm not paying a substitute host for a week to take over my
[02:07:09.180 --> 02:07:10.180]   show.
[02:07:10.180 --> 02:07:11.180]   Right.
[02:07:11.180 --> 02:07:12.740]   So ride home.info.
[02:07:12.740 --> 02:07:14.820]   What wheel Brian McCullough do?
[02:07:14.820 --> 02:07:19.540]   Stay tuned and find out ride home.info.
[02:07:19.540 --> 02:07:20.900]   Thank you, Brian for being here.
[02:07:20.900 --> 02:07:23.620]   Really appreciate it.
[02:07:23.620 --> 02:07:24.620]   And thank you.
[02:07:24.620 --> 02:07:27.620]   Samable Sam and my car guy.
[02:07:27.620 --> 02:07:32.060]   Wheel bearings podcast is a wheel bearings.media between you, Nicole and Robbie.
[02:07:32.060 --> 02:07:34.260]   I think you're probably covered for vacations.
[02:07:34.260 --> 02:07:35.260]   Uh, yeah.
[02:07:35.260 --> 02:07:38.020]   Yeah, we try.
[02:07:38.020 --> 02:07:40.740]   Also, uh, I, we didn't even say it today.
[02:07:40.740 --> 02:07:41.740]   I don't know.
[02:07:41.740 --> 02:07:44.140]   You want me to principal researcher, a guide has insights.
[02:07:44.140 --> 02:07:45.140]   Absolutely.
[02:07:45.140 --> 02:07:46.140]   Please.
[02:07:46.140 --> 02:07:47.140]   Yeah.
[02:07:47.140 --> 02:07:48.140]   That's his day job folks.
[02:07:48.140 --> 02:07:49.140]   They're, they're the ones that pay the bills.
[02:07:49.140 --> 02:07:50.140]   Yeah.
[02:07:50.140 --> 02:07:53.340]   So all the other stuff is, uh, just a hobby.
[02:07:53.340 --> 02:07:55.620]   Uh, it's good to have hobbies.
[02:07:55.620 --> 02:07:56.620]   Yeah.
[02:07:56.620 --> 02:07:57.620]   Yeah.
[02:07:57.620 --> 02:07:59.300]   You never know what's going to happen.
[02:07:59.300 --> 02:08:05.740]   Wheel bearings dot media for more information or to hear the podcast you've launched a patreon.
[02:08:05.740 --> 02:08:06.740]   That's great.
[02:08:06.740 --> 02:08:07.740]   Yeah.
[02:08:07.740 --> 02:08:08.740]   So people can become fans.
[02:08:08.740 --> 02:08:13.380]   Uh, last, last, last week we had, uh, Kelly Funkhouser from Consumer Reports on to talk
[02:08:13.380 --> 02:08:19.400]   about, uh, they, they recently did, uh, a bunch of testing on driver assist systems and
[02:08:19.400 --> 02:08:22.220]   had some interesting results.
[02:08:22.220 --> 02:08:27.460]   And so we invited Kelly to come on and talk us through, uh, what their methodology was,
[02:08:27.460 --> 02:08:31.020]   what they were actually looking for to get a better understanding of how they came to
[02:08:31.020 --> 02:08:32.900]   the conclusions that they did.
[02:08:32.900 --> 02:08:36.300]   Um, and, uh, so that was a really interesting conversation.
[02:08:36.300 --> 02:08:40.740]   And tomorrow morning will be, uh, Nicole and Robbie and I will be recording another one.
[02:08:40.740 --> 02:08:44.180]   Um, and, uh, we'll have some interesting content there as well.
[02:08:44.180 --> 02:08:47.220]   Join Sam Nicole Robbie at wheel bearings.media.
[02:08:47.220 --> 02:08:48.700]   Thank you, Sam.
[02:08:48.700 --> 02:08:49.940]   And of course, join us.
[02:08:49.940 --> 02:08:51.420]   Normally we are not on a Saturday.
[02:08:51.420 --> 02:08:55.460]   We just thought we'd give everybody a break because tomorrow is a big game.
[02:08:55.460 --> 02:08:57.020]   Super Bowl Sunday.
[02:08:57.020 --> 02:08:59.660]   So we invite you to tune in next Sunday.
[02:08:59.660 --> 02:09:01.740]   We'll be back on in our usual time, 2 p.m.
[02:09:01.740 --> 02:09:02.740]   Pacific five p.m.
[02:09:02.740 --> 02:09:03.740]   Eastern.
[02:09:03.740 --> 02:09:10.420]   That's 2200 UTC right after asked the tech guys, you can watch live at live.twit.tv chat
[02:09:10.420 --> 02:09:16.020]   with us live at IRC.twit.tv or if you're a member of club twit, join us in the fabulous
[02:09:16.020 --> 02:09:19.140]   club twit discord.
[02:09:19.140 --> 02:09:24.220]   You can also, uh, get after the fact, get versions of the show, audio or video at our
[02:09:24.220 --> 02:09:26.300]   website, twit.tv.
[02:09:26.300 --> 02:09:29.460]   There's a YouTube, a YouTube channel dedicated to this weekend tech.
[02:09:29.460 --> 02:09:35.460]   And of course you can subscribe in your favorite podcast client and get it automatically.
[02:09:35.460 --> 02:09:39.420]   The minute it's done, which we are now.
[02:09:39.420 --> 02:09:40.660]   Thank you for joining us.
[02:09:40.660 --> 02:09:42.580]   I'm glad you tuned in on a Saturday.
[02:09:42.580 --> 02:09:43.820]   We'll see you next week.
[02:09:43.820 --> 02:09:45.180]   Another twit is in the can.
[02:09:45.180 --> 02:09:46.180]   Bye bye.
[02:09:46.180 --> 02:09:47.180]   Do the twit.
[02:09:47.180 --> 02:09:48.180]   All right.
[02:09:48.180 --> 02:09:49.180]   Do the twit, baby.
[02:09:49.180 --> 02:09:50.180]   Do the twit.
[02:09:50.180 --> 02:09:51.180]   All right.
[02:09:51.180 --> 02:09:52.180]   Do the twit.
[02:09:52.180 --> 02:09:53.180]   All right.
[02:09:53.180 --> 02:09:54.180]   Do the twit.
[02:09:54.180 --> 02:09:55.180]   All right.

