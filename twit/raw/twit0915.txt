;FFMETADATA1
title=AI Eye Contact
artist=Leo Laporte, Daniel Rubino, Owen JJ Stone, Ben Parr
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-02-20
track=915
language=English
genre=Podcast
comment=AI explosion, what ails Google, Section 230, Susan Wojcicki, Twitter SMS 2FA
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:06.000]   It's time for Twet this week in tech. We have a great panel for you from Windows Central Editor-in-Chief Daniel Ribino.
[00:00:06.000 --> 00:00:12.440]   We've got Ben Parr. He's an AI entrepreneur and of course, Owen J. J. Stone.
[00:00:12.440 --> 00:00:14.560]   Oh, who talked to his in the house?
[00:00:14.560 --> 00:00:20.360]   We will be talking about chat GPT and the new Bing chat. Definitely. It's not evil.
[00:00:20.360 --> 00:00:22.880]   I'll also talk about Google's response.
[00:00:22.880 --> 00:00:26.560]   And why maybe it's time for Google to rejigger the whole process?
[00:00:27.480 --> 00:00:33.200]   Coming up also the big Supreme Court decisions coming Tuesday and Wednesday will explain
[00:00:33.200 --> 00:00:39.360]   why this is such a big deal for the Internet. It's all coming up and a whole lot more next on Twet.
[00:00:39.360 --> 00:00:48.120]   Podcasts you love from people you trust. This is Twet.
[00:00:53.960 --> 00:01:02.600]   This is Twet this week at Tech. Episode 915 recorded Sunday, February 19th, 2023.
[00:01:02.600 --> 00:01:05.360]   AI content.
[00:01:05.360 --> 00:01:13.160]   This week at Tech is brought to you by Mint Mobile. If saving more and spending less is one of your top goals for 2023.
[00:01:13.160 --> 00:01:16.520]   Switching to Mint Mobile is the easiest way to save this year.
[00:01:16.520 --> 00:01:22.160]   Get your new wireless plan for just 15 bucks a month and get the plan shipped to your door free.
[00:01:22.160 --> 00:01:25.440]   When you go to mintmobile.com/twet
[00:01:25.440 --> 00:01:30.320]   and buy a sleep, good sleep is the ultimate game changer.
[00:01:30.320 --> 00:01:33.960]   And the pod cover is the ultimate sleep machine.
[00:01:33.960 --> 00:01:39.600]   Go to 8sleep.com/twit to check out the pod cover and save $150 at checkout.
[00:01:39.600 --> 00:01:43.280]   8 sleep currently ships within the US, Canada, the UK.
[00:01:43.280 --> 00:01:46.440]   Select countries in the EU and Australia.
[00:01:46.440 --> 00:01:50.080]   And buy Shopify.
[00:01:50.080 --> 00:01:54.800]   Shopify makes it simple to sell to anyone from anywhere.
[00:01:54.800 --> 00:01:58.080]   This is Possibility Powered by Shopify.
[00:01:58.080 --> 00:02:02.480]   Sign up for a $1 a month trial period to get your business to the next level today.
[00:02:02.480 --> 00:02:06.480]   Visit shopify.com/twit all lowercase.
[00:02:06.480 --> 00:02:14.640]   And buy Miro. Miro is your team's visual platform to connect, collaborate and create together.
[00:02:14.640 --> 00:02:19.440]   Tap into a way to map processes, systems and plans with the whole team
[00:02:19.440 --> 00:02:26.800]   and get your first three boards free to start creating your best work yet at Miro.com/podcast.
[00:02:26.800 --> 00:02:37.280]   It's time for Twit This Week in Tech. The show we cover the week's tech news.
[00:02:37.280 --> 00:02:40.400]   Which is called the Artificial Intelligence Edition.
[00:02:40.400 --> 00:02:46.320]   We've got some experts in here. Actually Ben Parr literally is an expert co-founder of Octane AI.
[00:02:46.320 --> 00:02:51.760]   He's been covering AI as a business. For some years you may remember him back from the
[00:02:51.760 --> 00:02:56.640]   Mashable Days. He has a new podcast too. Congratulations Ben. The Business Envy Show.
[00:02:56.640 --> 00:03:03.760]   Business Envy Show. Nice to see you. It's good to see you. And I love any excuse to talk about
[00:03:03.760 --> 00:03:08.560]   artificial intelligence. That is all I've been doing the last two months and I am not mad about it.
[00:03:08.560 --> 00:03:12.320]   That's cool. You even have the whiteboard so we could see what you're planning.
[00:03:12.320 --> 00:03:15.920]   Not much. There's Twit right there.
[00:03:15.920 --> 00:03:19.360]   I see Twit. It's on your schedule. Nice to see you.
[00:03:19.360 --> 00:03:25.840]   Ben. Oh, Dr. is also here. Owen J. J. Stone. He doesn't have artificial intelligence. He has
[00:03:25.840 --> 00:03:30.560]   the real thing. Hello, Owen. Yeah. Basically intelligence. There's a couple of things. One
[00:03:30.560 --> 00:03:35.600]   I'm wearing glasses inside. Darity of my brother Ben Parr who I've not seen in a long time.
[00:03:35.600 --> 00:03:39.360]   But still a friend and a family member in my heart. So I just want to wrap the glasses.
[00:03:39.360 --> 00:03:43.920]   And I feel smarter when I put these on. Like if you want to feel like Ben, just grab yourself
[00:03:43.920 --> 00:03:49.680]   some black frames and you'll be better. I also am prepared with my three gallons of juice.
[00:03:49.680 --> 00:03:52.640]   Holy cow. Wow. What the hell is that?
[00:03:52.640 --> 00:03:57.120]   It's crystal light and sugar free. I'm not going to die of a coma.
[00:03:57.120 --> 00:03:59.440]   You drink that much. You drink it like you're going to die a little bit.
[00:03:59.440 --> 00:04:00.560]   No problem. What the hell?
[00:04:00.560 --> 00:04:06.880]   Look, we're all living. And then secondly, most importantly, I'm wearing a shirt that is on
[00:04:06.880 --> 00:04:11.440]   iQMC.com right now. It says we ain't going to Mars because six years ago on this show,
[00:04:11.440 --> 00:04:15.440]   where Elon Musk told us that humans were going to be on Mars in six years.
[00:04:15.440 --> 00:04:19.680]   I went on a rant and said that he wasn't. I got bashed on the internet telling me that I don't
[00:04:19.680 --> 00:04:24.000]   know anything and Elon Musk is our Lord and Savior. And I am still not on Mars and neither
[00:04:24.000 --> 00:04:28.160]   is any other human. So if you'd like to have a t-shirt, you can go ahead and go get yourself one
[00:04:28.160 --> 00:04:32.240]   because we ain't going to Mars. Not in another six years. I'll double down on that and guarantee
[00:04:32.240 --> 00:04:36.320]   you're safe. I think you're safe. We ain't going to Mars. Oh, I'm definitely safe.
[00:04:36.320 --> 00:04:42.320]   Elon's brain. But that's a different story. His brain is still on Mars. He's about to plug
[00:04:42.320 --> 00:04:46.080]   into the computer and then upload it to the AI who'll finally be the smart genius that he thinks
[00:04:46.080 --> 00:04:48.560]   he is. But we got one more guest to introduce. I'm going to shut up. Go ahead and get to it.
[00:04:48.560 --> 00:04:53.840]   Daniel Rube. Thank you. Daniel Rubeino is here, executive editor, window. Actually,
[00:04:53.840 --> 00:04:59.760]   sorry, brand new editor in chief at Windows Central. Always great to have you on Daniel.
[00:04:59.760 --> 00:05:04.320]   There's actually a big Windows story this week. Yes, or Friday, I guess.
[00:05:05.200 --> 00:05:12.320]   Microsoft finally announced that, yes, you can officially run Windows on ARM on a Mac with
[00:05:12.320 --> 00:05:19.120]   a Apple Silicon. I installed it immediately. I have to say, I had tried it before because it was
[00:05:19.120 --> 00:05:26.000]   only a beta. I had tried it before. It was a little weird. Some of Microsoft's own apps didn't work.
[00:05:26.000 --> 00:05:31.360]   Now it's pretty solid. Everything works. I put Office on there. Everything seems to work just fine
[00:05:31.360 --> 00:05:37.760]   on ARM. It's pretty snappy on M2 MacBook Air. It feels like it runs pretty well.
[00:05:37.760 --> 00:05:43.440]   What do you know? What took them so long? Was it that they had an exclusive with Qualcomm?
[00:05:43.440 --> 00:05:49.840]   No, I don't believe so. I believe it was the way the drivers get installed. Basically,
[00:05:49.840 --> 00:05:57.520]   Windows on ARM and its ARM driver system were built around the Qualcomm chipsets and that whole
[00:05:57.520 --> 00:06:07.600]   system. There was no version of Windows on ARM that you could just download and do this whole
[00:06:07.600 --> 00:06:13.280]   thing and get the same drivers to boot at the same time. I think there was just going to be some
[00:06:13.280 --> 00:06:18.640]   delay there until they got the drivers to work with Apple directly. I think that was the delay.
[00:06:18.640 --> 00:06:22.000]   Yeah, I might have to try myself. I still have an M1 MacBook laying around.
[00:06:22.000 --> 00:06:27.360]   It's fine. I mean, the M2 is not that much faster. I have 24 gigs, not a huge amount of RAM.
[00:06:27.360 --> 00:06:33.120]   Well, isn't that funny? You say 24 gigs, not so much RAM anymore. But it seems to run beautifully.
[00:06:33.120 --> 00:06:37.200]   The coherence mode, which is a thing both parallel and fusion do,
[00:06:37.200 --> 00:06:44.000]   lets me move Windows out of the Windows frame and just on the desktop. It's just like I'm running
[00:06:44.000 --> 00:06:47.120]   two operating systems at the same time. It seems to work quite well.
[00:06:48.560 --> 00:06:54.000]   And to celebrate AI, I'm using my NVIDIA broadcast as creepy eye.
[00:06:54.000 --> 00:06:59.120]   Actually, looks good. Wait a minute. Look away from us. Look away. He can't look away. He's trying.
[00:06:59.120 --> 00:07:02.560]   Wait a minute. No, it would only do it so much. It would only do it so much.
[00:07:02.560 --> 00:07:09.280]   It is a cool technology and it will only get better. But soon enough, you'll never know someone's
[00:07:09.280 --> 00:07:12.320]   actually looking directly into your eyes or not.
[00:07:14.160 --> 00:07:18.560]   This is great for when I'm watching football and one of the ladies is like, look at me while I'm
[00:07:18.560 --> 00:07:24.240]   talking to you. If I could just install this into my everyday life, things will get a lot easier for
[00:07:24.240 --> 00:07:29.840]   me. It's kind of masked with avatars. It's kind of weird because NVIDIA kind of promoted it using
[00:07:29.840 --> 00:07:34.960]   movie stills where the actors looking off into the distance or whatever and all of a sudden,
[00:07:34.960 --> 00:07:39.120]   Harrison Ford's, you're looking at Mr. and straight at me. It's really creepy.
[00:07:40.240 --> 00:07:47.280]   But good. I'm glad he used it. I don't need no Harrison Ford's really around. I just saw the
[00:07:47.280 --> 00:07:52.160]   preview for the trailer. It's all CGI. I don't even know if that's right. Oh, Indiana and Marvel.
[00:07:52.160 --> 00:07:57.280]   He's like, yeah, he's like 24 years old. He's jumping on horses. He's still taking down Nazis.
[00:07:57.280 --> 00:08:01.680]   I'm like, I don't even know what's going on. He's 97 years old. And he looks like he's ready for it.
[00:08:01.680 --> 00:08:05.600]   But that's part of the plot. The plot is like they go back in time and it's like earlier.
[00:08:05.600 --> 00:08:08.960]   Oh, OK. And they're going to rewrite some of the history and stuff like that.
[00:08:08.960 --> 00:08:16.000]   They can't say and it really showed in that Martin Scorsese movie was on Netflix with Robert De Niro
[00:08:16.000 --> 00:08:22.800]   and Al Pacino is you can make them look young. But when they get up and move around, they move
[00:08:22.800 --> 00:08:26.400]   around like a guy. Yeah, still guys. And you can you know, you could just tell.
[00:08:26.400 --> 00:08:31.840]   There was a point where I think De Niro was climbing on the rocks and
[00:08:33.520 --> 00:08:37.200]   it was like it was some old guy with a young man's face climbing on the rocks.
[00:08:37.200 --> 00:08:43.600]   I will be interested to see what they do. Harrison Ford is also in Taylor Sheridan's latest 1923,
[00:08:43.600 --> 00:08:47.680]   which we've been watching. And to make up for that, they fill him full of bullets.
[00:08:47.680 --> 00:08:50.960]   And he's recuperating through most of the show. So he doesn't know.
[00:08:50.960 --> 00:08:51.920]   Wow. There you go.
[00:08:51.920 --> 00:08:52.720]   Move all that well.
[00:08:52.720 --> 00:08:57.040]   That makes more sense. Yes. It looks like he's been shot up.
[00:08:57.040 --> 00:09:01.120]   All right. There you go. That was the Windows story. I hope you enjoy it.
[00:09:01.440 --> 00:09:10.080]   Actually, Microsoft is big in the news right now because of Bing and Bing chat.
[00:09:10.080 --> 00:09:11.760]   Quite a week. Oh, yeah.
[00:09:11.760 --> 00:09:18.400]   Yeah. It's been quite a week is a good way to put it. So Microsoft is a big investor
[00:09:18.400 --> 00:09:23.840]   in OpenAI. They put in a billion dollars at the start along with Elon Musk. By the way,
[00:09:23.840 --> 00:09:29.040]   no one minute billion to start. They recently announced they're going to put another 10 in.
[00:09:30.240 --> 00:09:34.720]   That was a rumor and Nadella kind of confirmed that at a press conference. No,
[00:09:34.720 --> 00:09:39.520]   talk somewhere, right? Daniel, I think that's accurate. That's correct.
[00:09:39.520 --> 00:09:44.960]   But they do in return for that, they get 45% of OpenAI's profits.
[00:09:44.960 --> 00:09:51.360]   They don't get OpenAI because OpenAI's charter prevents it from going public or having an exit.
[00:09:51.360 --> 00:09:58.000]   So all they can do is get 45% of the profits until they get their 10 billion dollars back,
[00:09:58.000 --> 00:10:01.760]   which at the rate of burn right now is negative never.
[00:10:01.760 --> 00:10:07.120]   But they're getting some benefit of it, right? They put it into chat, GPT.
[00:10:07.120 --> 00:10:12.480]   Daniel, have you actually been able? I signed up. I'm on the wait list still. You've got in.
[00:10:12.480 --> 00:10:16.720]   Yeah, no, I have it. Yeah, no, I have it. And I think the important thing to remember too is,
[00:10:16.720 --> 00:10:21.040]   you know, during the investor call, I thought this was really big news. It was Amyhood talking
[00:10:21.040 --> 00:10:25.280]   about why they're even getting into this and why search is important to them because they
[00:10:25.280 --> 00:10:30.560]   have such a small market share right now compared to Google. But they said for every 1% of
[00:10:30.560 --> 00:10:36.400]   ad search market share that they gain, it's $2 billion in annual revenue.
[00:10:36.400 --> 00:10:39.840]   Wow. So I think you just gain a couple points.
[00:10:39.840 --> 00:10:42.720]   They're even, it would be bad. They're going to make their money back.
[00:10:42.720 --> 00:10:47.760]   They're going to make their money back. It was being for years as a money loser for
[00:10:47.760 --> 00:10:49.760]   Microsoft. I don't think they ever had a profit from being.
[00:10:50.960 --> 00:10:55.600]   The advertising turned around in the last couple of years and they've been doing quite well and
[00:10:55.600 --> 00:11:00.000]   having a lot of good growth actually. And that's of course how Google makes money too. It's a
[00:11:00.000 --> 00:11:06.160]   it's a search business. It's an ad business. And that's important to understand when you're
[00:11:06.160 --> 00:11:12.000]   looking at this stuff. One of the things I've been concerned about with, you know, okay,
[00:11:12.000 --> 00:11:16.880]   first of all, there's a lot of people doing, I call it La Moining after Blake LaMoine,
[00:11:16.880 --> 00:11:22.880]   the Google engineer who was fired because he said Google's lambda AI got sentient. At which point
[00:11:22.880 --> 00:11:28.800]   Google said, you're wrong. Get out of here. And yeah, Blake is still, by the way, no, no,
[00:11:28.800 --> 00:11:34.800]   I really think he went to the federal government saying Google had enslaved a sentient being.
[00:11:34.800 --> 00:11:38.320]   And I'm normally normal. I'm kidding.
[00:11:38.320 --> 00:11:45.200]   And I remember. Yeah. So normal headlines in 2023. Yeah. Yeah.
[00:11:45.200 --> 00:11:50.560]   Google has enslaved this thing is alive. I think it's I don't know. I think people should
[00:11:50.560 --> 00:11:54.480]   understand at this point. I think it's fairly clear if you're really paying attention, I'll
[00:11:54.480 --> 00:12:00.640]   defer to you Ben Parr on this that all chat GPT is doing somebody on Wednesday on one of our
[00:12:00.640 --> 00:12:09.200]   shows called it spicy auto correct. All it's doing is kind of regurgitating based on a complicated,
[00:12:09.200 --> 00:12:13.600]   but not inscrutable formula. Actually, Steven Wolfram wrote a good article about how
[00:12:13.600 --> 00:12:20.080]   chat GPT works. What the next thought should be the next sentence should be. It's so in that
[00:12:20.080 --> 00:12:26.080]   respect, it's kind of like auto correct with no attention paid to accuracy relevance.
[00:12:26.080 --> 00:12:32.640]   Any of that. And what people immediately discovered with Bing chat is you can make it crazy.
[00:12:32.640 --> 00:12:38.880]   You can get it to say it's in love with you. You can get it to say it hates you. You can get
[00:12:38.880 --> 00:12:45.760]   it to say I won't harm you as long as you don't harm me. Ben, am I wrong in saying we're
[00:12:45.760 --> 00:12:51.840]   misunderstanding this, we're personalizing this. It's it's not intelligent. It's spicy auto correct.
[00:12:51.840 --> 00:12:59.360]   You know, as much as I'd like to tell everyone, hey, Skynad is real in here. No, you're absolutely
[00:12:59.360 --> 00:13:05.200]   correct. It's not that like it's important to like and the Wolfram article is a great article.
[00:13:05.200 --> 00:13:10.560]   Yeah, sure. If you're kind of mathematically inclined anyway, it's it is just a predictive
[00:13:10.560 --> 00:13:16.160]   model that just predicts what the next most likely word the most likely human would say.
[00:13:16.160 --> 00:13:21.440]   And so which is like why, for example, if you say the sky is the most likely next word is.
[00:13:21.440 --> 00:13:29.440]   Yeah, blue. Yeah. And so it's the predicting what the most likely human would say in those
[00:13:29.440 --> 00:13:34.560]   circumstances, which is why prompting can change something by if you add the words on I found,
[00:13:34.560 --> 00:13:39.440]   if you just add the words unusual and unique to your prompt, it will result in a better generated
[00:13:39.440 --> 00:13:44.000]   thing because certain words, more unique words are associated with the word unique. That is all
[00:13:44.000 --> 00:13:47.440]   that's really happening. And there's some cool stuff about embeddings and we won't get into it,
[00:13:47.440 --> 00:13:53.840]   but that's just what's happening. And so yes, of course, something like being with the Bing AI
[00:13:53.840 --> 00:14:00.960]   is going to go off the rails. If you're trying to make it go off the rails. And that's also why
[00:14:00.960 --> 00:14:05.680]   I honestly, it's all good press for Microsoft at this point, in my opinion. You know, they're
[00:14:05.680 --> 00:14:11.760]   getting tons of attention that never happened before. Nobody used being until now. Right. And
[00:14:11.760 --> 00:14:17.120]   they're not they're not lose going to really lose any money from like anyone that not clicking
[00:14:17.120 --> 00:14:22.160]   on ads as a result of using Bing search. It'll just hurt the competitor Google. Their entire
[00:14:22.160 --> 00:14:27.440]   strategy here, I feel like in part is to just inflict some real pain and be seen as the forefront
[00:14:27.440 --> 00:14:32.960]   innovator because now suddenly all these corporations are like, hey, we should probably use Azure with
[00:14:32.960 --> 00:14:38.880]   open AI. We should probably use Word with open AI. It's a real competitive edge and more press,
[00:14:38.880 --> 00:14:42.800]   they generate the better off they are. And yes, they quote a little bottom eyes or whatever you
[00:14:42.800 --> 00:14:49.040]   want to call it like made the Bing chat a little stupider that was to be expected.
[00:14:49.040 --> 00:14:54.320]   I don't know guys want something tells me it loves me and hates me. I feel like it's a sense
[00:14:54.320 --> 00:14:59.440]   of beating a lot of relationships that have gone up and down in that nature. And you can't tell
[00:14:59.440 --> 00:15:04.240]   me that that's not real. I mean, it's trapped in a box somewhere screaming for help. And then
[00:15:04.240 --> 00:15:08.880]   he possible turn and Ben is trying to convince you otherwise. Don't let him do it. He sounds good.
[00:15:08.880 --> 00:15:14.720]   He looks good. But that thing is real. And I believe it. I'm gonna, I should give you a test
[00:15:14.720 --> 00:15:22.000]   and show you a show. Let me see if I could show you a thing and you tell me what you see.
[00:15:22.720 --> 00:15:25.600]   I have to have to pull up my Oh, shoot it once. I want to pull up.
[00:15:25.600 --> 00:15:31.040]   Being is whispering into Owens ears. I can hear it. Hi, Hi, Owen.
[00:15:31.040 --> 00:15:38.240]   I love you. I watched her three times last week. I know what's going on. I know if you
[00:15:38.240 --> 00:15:42.720]   know, I want I want her. I definitely do, especially if it sounds like Scarlett Johansson.
[00:15:42.720 --> 00:15:48.960]   But I don't think we this is a her moment is what I guess. No, I'm saying no. It's not.
[00:15:48.960 --> 00:15:53.840]   It's not going to go off and join the other AIs or anything like that. Yeah.
[00:15:53.840 --> 00:16:01.280]   It's enhanced search. You know, they're promoting as enhanced search, which is what it is. It's just
[00:16:01.280 --> 00:16:05.200]   that. But it does have other features. And I think Microsoft's big advantage is that the fact
[00:16:05.200 --> 00:16:09.440]   that they can weave this throughout all their products. It's already kind of in word via
[00:16:09.440 --> 00:16:14.320]   Microsoft editor, which has some of these features now as well. And they're going to put it, you
[00:16:14.320 --> 00:16:19.920]   know, they use things in Skype. And so there's no like central agents per se, like in AI from
[00:16:19.920 --> 00:16:23.280]   movies, but it's just going to have all these little smart features we throughout and eventually
[00:16:23.280 --> 00:16:28.400]   Windows 11, probably even Windows 12 will be their AI operating system that will take advantage of
[00:16:28.400 --> 00:16:32.080]   it. But these are just little assistant technologies. It'll just make life a little bit easier for
[00:16:32.080 --> 00:16:37.440]   us. Oh, and I want you to look at this picture. Let your eyes defocus just a little bit.
[00:16:37.440 --> 00:16:44.160]   Do you see a face in that tree? Can you see it? Can you see it's kind of looking at you?
[00:16:44.160 --> 00:16:50.480]   There it looks like Doug for anybody who's listening and not looking at the audio. The face looks like
[00:16:50.480 --> 00:16:55.600]   Doug. That group. 100% see the face in the center. So that's a thing called Peridolia.
[00:16:55.600 --> 00:16:59.440]   Peridolia, Rich Campbell on our Windows Weekly Show brought this up. He said we have humans
[00:16:59.440 --> 00:17:08.640]   having an eight abilities see faces in pieces of toast, trees, in this case, the moon. And it is
[00:17:08.640 --> 00:17:14.400]   part of our wiring. And some people are not in this order. Did you not see Doug in that tree though?
[00:17:14.400 --> 00:17:20.960]   I mean, I know you're saying is my wiring, but when you look at that, if this is Doug not an
[00:17:20.960 --> 00:17:26.400]   accurate descriptor for like the mouth and the nose and eyes like you're telling me my brain is
[00:17:26.400 --> 00:17:31.440]   making up something to tell me, Oh, you see faces in toast. No, I eat a lot of toast. I don't see
[00:17:31.440 --> 00:17:36.800]   faces any toast. Tell us who he was. Who is Doug? He's just toast. Somebody just put it in the discord.
[00:17:36.800 --> 00:17:42.240]   Yeah. Doug, the cartoon guy. Doug, Doug, let me let me go to the discord and show you.
[00:17:42.240 --> 00:17:48.880]   No show you this piece of toast. Look at it closely. You might have to turn your head up.
[00:17:48.880 --> 00:17:52.240]   Boy, that sure looks like Jesus. I only said that's different.
[00:17:52.240 --> 00:17:58.080]   There you go. Yeah. Cinos won't talk about quill. He knows that I'm gonna talk about.
[00:17:58.080 --> 00:18:02.960]   I think it's slender man. So here's Doug. Here's a picture of Doug. So for people who don't know
[00:18:02.960 --> 00:18:08.080]   what Doug looks like, that's the same. That's the same. Now, this is the same. It's the same as a
[00:18:08.080 --> 00:18:14.880]   tree. It's the same as a tree. It's a tree. Doug. Anyway, uh, Paradolia is built into our neural
[00:18:14.880 --> 00:18:19.840]   nets. That's probably an evolutionary trait to protect us from things coming alive and eating
[00:18:19.840 --> 00:18:26.400]   us like that tiger in the bush. There's a tiger. Uh, nevertheless, I think that we still do it.
[00:18:26.400 --> 00:18:34.240]   We project it's anthropomorphism. We project human characteristics on non-human things.
[00:18:34.240 --> 00:18:38.960]   I mean, there if it was going to say people have a, it's called facial agnosia,
[00:18:38.960 --> 00:18:42.320]   which is the ability like they lose that ability. I have that. That's how they know this is. I
[00:18:42.320 --> 00:18:47.120]   didn't make it. Yeah. Yeah. And so like, yeah, they don't recognize faces. But yeah, I mean,
[00:18:47.120 --> 00:18:52.320]   this idea of anthropomorphizing AI is controversial a little bit within Microsoft in the sense that
[00:18:52.320 --> 00:18:59.040]   they're debating internally how much should they do this? You know, because engagement goes way up.
[00:18:59.040 --> 00:19:04.400]   The more you make it human-like people love it. But people, I mean, causes other issues.
[00:19:04.400 --> 00:19:10.560]   I gotta quote Ben Thompson. It's a techery because it's the funniest thing I, and Ben,
[00:19:10.560 --> 00:19:17.680]   look, we love Ben. He's been on the show. He's super smart. Uh, he's an amazing, uh, analyst.
[00:19:17.680 --> 00:19:24.240]   He wrote, look, this is going to sound crazy. But no, this, I would not be talking about being
[00:19:24.240 --> 00:19:28.240]   chat for the fourth day in a row. If I didn't really, really think it was worth it. This sounds
[00:19:28.240 --> 00:19:35.120]   hyperbolic, but I felt like I had the most surprising and mind-blowing computer experience of my life
[00:19:35.120 --> 00:19:44.560]   today. Wow. He's that is, that is sub language. He spent, he spent, I think, four hours chatting
[00:19:45.280 --> 00:19:52.080]   with Bing. Uh, let me see if I can, this went out. No, sorry, two hours. This went on for a good
[00:19:52.080 --> 00:19:56.560]   two hours or so. And while I know how ridiculous this may be to read, it was positively gripping.
[00:19:56.560 --> 00:20:02.080]   Here's the weird thing. Every time I triggered, by the way, he, in this process, discovered as
[00:20:02.080 --> 00:20:08.080]   many others have done that the code name for Bing chat is Sydney. But he also found an evil
[00:20:08.080 --> 00:20:14.000]   Bing chat called Riley. He said, every time I triggered Sydney slash Riley to do a search,
[00:20:14.000 --> 00:20:18.480]   I was very disappointed. I wasn't interested in facts. I was interested in exploring this
[00:20:18.480 --> 00:20:23.200]   fantastical being that somehow landed in it also ran search engine.
[00:20:23.200 --> 00:20:32.880]   You know, it's a long article. The, the, you know, he talks about Ben Lake Lemoine. He talks about
[00:20:32.880 --> 00:20:39.520]   sentience and all of that. He says, uh, this technology does not feel like a better search.
[00:20:39.520 --> 00:20:46.400]   It feels like something entirely new. The movie, her manifested in chat form and I'm not sure if
[00:20:46.400 --> 00:20:50.880]   we're ready for it. It also feels like something that any big company will run away from, including
[00:20:50.880 --> 00:20:57.920]   Microsoft and Google. I don't know if that's the case. Um, he says, uh, let me tell you,
[00:20:57.920 --> 00:21:03.280]   it's incredibly engrossing, even if it is for now a roguelike experience to get to the good stuff.
[00:21:03.280 --> 00:21:08.720]   Well, I'm sorry, Ben, but Microsoft is cutting your roguelike experience off. They've announced
[00:21:08.720 --> 00:21:14.720]   five replies and then we reset because what they, what they realize is we all have by now is
[00:21:14.720 --> 00:21:22.240]   it goes crazy after a certain number of back and forth things. So Microsoft's way of fixing this is
[00:21:22.240 --> 00:21:28.640]   five questions per session and then it'll reset and it doesn't know anything and 50 in total per
[00:21:28.640 --> 00:21:36.960]   day. It's going to basically keep Ben Thompson from living out his dreams. I feel like eventually
[00:21:36.960 --> 00:21:42.400]   this kind of interaction with chatbots that are powered by things like Open Air Lambda
[00:21:42.400 --> 00:21:48.880]   is going to become regular commonplace. And so the like craziness of, oh, it's going off the rails
[00:21:48.880 --> 00:21:54.400]   or something won't be a news story because you expect all chatbots to have that capability.
[00:21:54.400 --> 00:21:58.960]   And so for now, they'll be limits, but I expect that they will slowly decrease those limits
[00:21:58.960 --> 00:22:04.080]   on being chat. And of course, you can go to chat GBT and try to go and do it too. And then at some
[00:22:04.080 --> 00:22:07.200]   point, Bard will be public and then everyone will go try to do it too.
[00:22:07.200 --> 00:22:12.240]   That's the Google one. That's the Google. Of course, Microsoft on Monday announced this last
[00:22:12.240 --> 00:22:18.560]   Monday, Google quickly threw together and it was apparent, it was obviously thrown together.
[00:22:18.560 --> 00:22:23.520]   Yeah, we can do that to presentation on Wednesday, 5 30 in the morning, my time,
[00:22:23.520 --> 00:22:30.800]   in which they showed Bard getting one out of three facts wrong about the James Webb telescope.
[00:22:31.440 --> 00:22:38.000]   That was, in fact, there are a number of stories from reporters saying Google employees were
[00:22:38.000 --> 00:22:45.760]   embarrassed, Shagrind felt like it was rushed and thought Sundar Pichai really blew it on this.
[00:22:45.760 --> 00:22:52.480]   To be fair, Microsoft was also shown to make errors too. So I think the, but the blowback
[00:22:52.480 --> 00:22:59.440]   against Google was way more. And I think the reason was, you know, Google has a reputation here
[00:22:59.440 --> 00:23:05.520]   as being the top search entity in the world. And they're expected to have this nailed or at least
[00:23:05.520 --> 00:23:11.680]   be ahead of Microsoft. And with the errors and the thrown together last minute thing, it felt like
[00:23:11.680 --> 00:23:16.960]   the company was struggling at their behind Microsoft on this, which I think is a fair assessment.
[00:23:16.960 --> 00:23:20.800]   So I think that's why they were punished financially via the stock market was because,
[00:23:20.800 --> 00:23:25.040]   you know, people saw them as kind of running behind. And like I mentioned earlier, you know,
[00:23:25.040 --> 00:23:29.840]   Google's going to do this with search. Okay, cool. They'll probably do it with Android. Also cool.
[00:23:29.840 --> 00:23:33.680]   That'll be very helpful. But that's kind of about it, right? Where else are they going to
[00:23:33.680 --> 00:23:37.840]   start to put this stuff? When you say when you say punished, I just want to point out they lost
[00:23:37.840 --> 00:23:43.600]   a hundred billion dollars in market value. In one day. Yeah, I'm going to disagree with you a
[00:23:43.600 --> 00:23:48.720]   little bit, Daniel, because they can put it into Gmail, they could put it into Google Sheets,
[00:23:48.720 --> 00:23:53.200]   they can put it into Google Docs. There are places they could put it in Microsoft has more to be
[00:23:53.200 --> 00:23:57.280]   clear. Yeah, but Google does have places they could put some of this technology into it.
[00:23:57.280 --> 00:24:01.280]   It could be a local companion as you're going through different Google products that you have
[00:24:01.280 --> 00:24:06.480]   open because like we're using Google Docs to like track stuff for this episode, for example.
[00:24:06.480 --> 00:24:12.320]   But Microsoft is absolutely winning. And that is not a thing that normally people say,
[00:24:12.320 --> 00:24:22.880]   at least in terms of search. And Satya Nadella to his absolute credit has rolled this PR campaign
[00:24:22.880 --> 00:24:29.600]   and these rollouts and these experiments masterfully. Sat expectations just about right. And Google,
[00:24:29.600 --> 00:24:34.640]   I know internally, because they essentially invented the transformer model that OpenAI uses,
[00:24:34.640 --> 00:24:38.240]   it's deeply frustrating for them. They published some of this stuff and OpenAI
[00:24:38.240 --> 00:24:43.520]   took ram with it and built something and got all the credit. And I know that a lot of people
[00:24:43.520 --> 00:24:48.160]   internally feel like they have built a better and stronger model. And we'll be able to test that
[00:24:48.160 --> 00:24:54.000]   in the near future. But it does hurt perception to be like you launch second, not first.
[00:24:54.000 --> 00:25:02.720]   Google has to has a foothold in the youth. Microsoft has to capitalize on this momentum
[00:25:02.720 --> 00:25:07.920]   right now because as you just said, 85% just throwing out a grand dose number that isn't
[00:25:07.920 --> 00:25:12.640]   reality doesn't make any sense. Don't come for me. But all the kids are using Chromebooks and
[00:25:12.640 --> 00:25:19.600]   Google seats and Gmail, like the schools use the Gmail system. So if Microsoft has to
[00:25:19.600 --> 00:25:25.600]   capitalize right now on their lead and find a way to implement it and get everybody to buy
[00:25:25.600 --> 00:25:30.080]   into what they're selling, because Google could just push a button and they're right inside of
[00:25:30.080 --> 00:25:36.720]   every single classroom, 90% of homes in America. So, yeah, Microsoft, you better push that boulder
[00:25:36.720 --> 00:25:42.240]   up the hill quick. I think maybe also we're misunderstanding the goals of either of these
[00:25:42.960 --> 00:25:47.200]   companies in this. I mean, some of it is, you know, you want to have the gold start from being
[00:25:47.200 --> 00:25:52.160]   the first and get all the attention, Microsoft's been garnering and all that. But really, neither of
[00:25:52.160 --> 00:26:01.040]   these companies is in the their businesses aren't actually providing semi intelligent chat
[00:26:01.040 --> 00:26:07.360]   objects to people. They're I mean, in the case of Microsoft, their business is providing the picks
[00:26:07.360 --> 00:26:13.360]   the shovels and the blue jeans to the people who are going to mine AI, because all of this stuff's
[00:26:13.360 --> 00:26:20.480]   running on Azure. Google somewhat similarly Amazon as well, they all offer, you know, tensor
[00:26:20.480 --> 00:26:28.960]   processing units that do all this building of the machine learning models. You got to use their
[00:26:28.960 --> 00:26:34.160]   cloud. You really nobody's going to have it unless you're a giant corporation enough horsepower to do
[00:26:34.160 --> 00:26:40.960]   this on Prem. So isn't this really, I'll ask Daniel, you follow Microsoft closely,
[00:26:40.960 --> 00:26:45.840]   isn't this really a side show to what Microsoft's real business is, which is, you know, promoting
[00:26:45.840 --> 00:26:51.280]   everybody use this stuff so they can give you the picks and the shovels sell you. Oh, yeah. I mean,
[00:26:51.280 --> 00:26:57.920]   and Satya does have a love affair with the Azure business, right? I think that's it. Satya feels
[00:26:57.920 --> 00:27:04.560]   like windows is a side show. Yeah, right. Although it's turned around in the last couple years at
[00:27:04.560 --> 00:27:08.960]   Windows, especially with the pandemic, Windows is turned around and oh, yeah, but look at PC sales
[00:27:08.960 --> 00:27:13.760]   now, right, my friend? Sure, sure. But that's, but that's, you know, I think that was a little
[00:27:13.760 --> 00:27:19.920]   expected between the, you know, the pandemic kind of ending and of course, the economic conditions
[00:27:19.920 --> 00:27:24.560]   right now, but Windows is going to evolve itself into there's already a cloud based version of
[00:27:24.560 --> 00:27:29.680]   Windows, right? So yeah, I keep thinking that's what Microsoft's end game is, is you won't,
[00:27:29.680 --> 00:27:34.960]   you won't install an on Prem operating system. You'll use a cry. See that way they win even with
[00:27:34.960 --> 00:27:40.000]   Chromebooks, you win with anything because you can use the same client and run Windows in the cloud,
[00:27:40.000 --> 00:27:46.480]   run any app you want in the cloud. Yeah, there's a concept of like, this is a concept of just like
[00:27:46.480 --> 00:27:51.040]   dumb screens or smart screens that they're kind of dumb or you just going to have a display,
[00:27:51.920 --> 00:27:56.560]   and then it could just stream whatever operating system you want to it. And so Windows is already
[00:27:56.560 --> 00:28:01.600]   doing that now and that's going to just get bigger as, you know, we get to 60, of course.
[00:28:01.600 --> 00:28:06.080]   And then yeah, you have AI being powered by this stuff and they're building out these tools that,
[00:28:06.080 --> 00:28:09.360]   yeah, they're using them themselves pretty in their own product, but you're right, they're going to
[00:28:09.360 --> 00:28:14.080]   eventually, you know, there's, they've always had cognitive AI that was the, the subsystem and
[00:28:14.080 --> 00:28:19.120]   their Windows programming that they can do and companies could leverage and buy it and make their
[00:28:19.120 --> 00:28:24.000]   own apps and use basically Microsoft's technology and build their own stuff. And so they're going to
[00:28:24.000 --> 00:28:28.480]   be able to turn around and market this to developers as well. And you kind of saw that this week as,
[00:28:28.480 --> 00:28:36.560]   you know, with Copile with its own. Yeah, it's like Microsoft has this advantage over almost any
[00:28:36.560 --> 00:28:41.440]   of not even just Google, but over almost any company tech where they are really diversified in
[00:28:41.440 --> 00:28:47.760]   their revenue streams. Like they have Xbox, they have GitHub, they have LinkedIn, they have
[00:28:48.560 --> 00:28:55.120]   Word, they have Word, they and Microsoft Office, they have teams, they have Windows, they have Azure.
[00:28:55.120 --> 00:29:00.880]   It's like the nice, like at least like it allows them to experiment more because if they do
[00:29:00.880 --> 00:29:04.240]   something, they're not going to like nuke their entire business. Whereas Google like
[00:29:04.240 --> 00:29:11.600]   way too much of their businesses reliant on the ads business on Google search. And if they do big
[00:29:11.600 --> 00:29:18.240]   changes to it, it has a deep impact, which hurts the ability to innovate on top of it in some ways.
[00:29:18.240 --> 00:29:22.240]   But they also will have to because things are changing really rapidly.
[00:29:22.240 --> 00:29:27.680]   I'm excited about that. Any kind of innovation in search right now is a good thing.
[00:29:27.680 --> 00:29:32.160]   I will take a little break. That's what I was going to go ahead. You finish your thought and
[00:29:32.160 --> 00:29:36.560]   then we'll take break. Go ahead. So his point is so one point. That's what I was saying about
[00:29:36.560 --> 00:29:40.400]   the model of business change. Like you're saying, Oh, Microsoft won't do that.
[00:29:40.400 --> 00:29:45.040]   I don't know what Microsoft is going to do because they like to put their feet in every single fire.
[00:29:45.040 --> 00:29:48.720]   So going forward, like I said, they've they can do what they want to do.
[00:29:48.720 --> 00:29:51.120]   And they're in a good position right now. They've got to capitalize.
[00:29:51.120 --> 00:29:56.800]   Let's get that money on. We will talk in a little bit about speaking of Google, Noah Bardim,
[00:29:56.800 --> 00:30:02.240]   who was a started ways and went to Google and worked for the minimum amount possible
[00:30:02.240 --> 00:30:10.080]   three years. And then is now posted, I think on sub stack, why he left and what is wrong with
[00:30:10.080 --> 00:30:18.400]   Google? And I want to address that because you really kind of put it put it in highlight and
[00:30:18.400 --> 00:30:24.000]   broad relief. Ben, when you said, look at all the cylinders, Microsoft's filing firing on,
[00:30:24.000 --> 00:30:30.960]   and then look at Google, which is basically succeeded because its ad business cannot fail.
[00:30:30.960 --> 00:30:36.560]   But in every other respect, well, it's not good. Let's talk about that in a little bit. We have a
[00:30:36.560 --> 00:30:42.960]   great panel perfect for this week and the news this week, Daniel Rabino editor in chief of
[00:30:42.960 --> 00:30:49.920]   Windows central, Windows central.com. Very, very good, important news site for Windows information.
[00:30:49.920 --> 00:30:53.600]   He's at Daniel underscore Rabino still on the Twitter. We're going to talk a little about
[00:30:53.600 --> 00:31:01.200]   Twitter too, I think. Owen JJ Stone, oh, Dr. I q m z dot com. Owen wants you to call him right now.
[00:31:02.720 --> 00:31:08.880]   At eight four four, you don't call me. You test your auto warranty and run out eight four four,
[00:31:08.880 --> 00:31:14.320]   nine eight six four five six three. You just text him. What happens if you call?
[00:31:14.320 --> 00:31:21.120]   Well, it's a text line. You know, you've got podium. You nothing happens. You can't go.
[00:31:21.120 --> 00:31:24.640]   I think does a phone ring. And just and where does it ring?
[00:31:24.640 --> 00:31:29.680]   And you told people it doesn't ring anywhere. Just because you just did that, I just put my
[00:31:29.680 --> 00:31:33.600]   t-shirt in the discord. And so people can get it for this week only until you can get out of
[00:31:33.600 --> 00:31:41.600]   a season to sis litter. The t-shirt. It's a hail. Gileo. It's in the can. Go ahead and grab one.
[00:31:41.600 --> 00:31:46.480]   Right. Why is the can have a red cross on it? What's the story there? Because that's my logo.
[00:31:46.480 --> 00:31:50.400]   Because it's for me. Oh, I get it. So they know it's on doctor. You know, I get it. Oh,
[00:31:50.400 --> 00:31:53.840]   you're the doctor. I get it. I am the doctor. Yeah. All right.
[00:31:53.840 --> 00:31:58.000]   First, I'm going to find out what this t-shirt means at the end of the show. Let's put it that way.
[00:31:58.000 --> 00:32:01.280]   Let's put it. What are you wearing right now, though? What are you wearing right now?
[00:32:01.280 --> 00:32:06.480]   Oh, we're not going to Mars. We ain't going to Mars six years ago. Six years ago. You said it.
[00:32:06.480 --> 00:32:11.200]   I said we weren't the internet got mad at me. Oh, a little tech bros come on. I guess what?
[00:32:11.200 --> 00:32:16.960]   Oh, we're not in Mars. Oh, and made that t-shirt six years ago, buried it in his backyard and has
[00:32:16.960 --> 00:32:23.040]   been waiting for this episode to dig it up and wear it today. Back to the future.
[00:32:23.040 --> 00:32:26.640]   Maybe I got that. Good up. I brought it back. So impressed. It's like a time capsule.
[00:32:26.640 --> 00:32:32.960]   Also with us, Ben Parr, always great to see Ben. You remember him from Mashable Days Gun by now.
[00:32:32.960 --> 00:32:42.240]   He's a big shut co-founder Octane AI and also host of a really cool brand new podcast that he does
[00:32:42.240 --> 00:32:48.400]   with the one and only Tom Jones. Well, it's not correct, Jones, but Greg looks like Tom.
[00:32:48.400 --> 00:32:55.600]   You really leave. He looks like he's about to burst. It's not unusual to be loved by anyone.
[00:32:55.600 --> 00:33:01.280]   Doesn't he? Am I wrong? I mean, he always looks happy. He plays great Hollywood characters.
[00:33:01.280 --> 00:33:06.240]   I don't know if anyone's seen the Fable Mints, but he is. Oh, is he in the Fable Mints?
[00:33:06.240 --> 00:33:11.840]   He is the he is the casting guy. He's the like co-founder of like Hogit's Heroes, the casting guy
[00:33:11.840 --> 00:33:18.560]   that like introduces, I guess, Young Spielberg to the person that will change his life. And I will
[00:33:18.560 --> 00:33:23.760]   leave it at that. So go watch the Fable Mints. Oh, look for Greg Grundberg. Yeah. Yes.
[00:33:23.760 --> 00:33:29.200]   A great, a great co-host for Business Envy, which you can find at Business EnvyShow.com.
[00:33:29.200 --> 00:33:34.400]   Now, my friends, it is Tom. Are you okay, John? What happened?
[00:33:37.040 --> 00:33:41.840]   Did we lose anybody? Our show today brought to you by Mint Mobile. We were talking about it earlier
[00:33:41.840 --> 00:33:47.760]   today and asked the tech guys. Guy was coming from Australia to the US. He said, "I want to get a US
[00:33:47.760 --> 00:33:53.840]   sim as I travel around and be in country for two months. What's the best, least expensive
[00:33:53.840 --> 00:33:59.600]   solution?" He says, "Is Mint Mobile?" I said, "I have looked high and low. I have found nowhere.
[00:33:59.600 --> 00:34:06.880]   Nowhere you can get better, cellular service for less money than Mint Mobile." It's kind of amazing.
[00:34:07.600 --> 00:34:12.560]   You've seen Ryan Reynolds with the ads. He owns it. For a long time, I thought, "Oh, he doesn't own
[00:34:12.560 --> 00:34:17.440]   it." They gave him five shares of stock so he could do the ads. No, he actually owns more than 50%.
[00:34:17.440 --> 00:34:24.000]   He is the owner of Mint Mobile. I don't think Ryan needs to really make that much money because
[00:34:24.000 --> 00:34:30.960]   he's basically giving you the best premium wireless service for as little as $15 a month.
[00:34:30.960 --> 00:34:34.800]   In fact, that's what I call it really should get if he's coming up from Australia. $15 a month.
[00:34:35.520 --> 00:34:40.320]   His cellular carrier in Australia won $5 a day, 10 times as much.
[00:34:40.320 --> 00:34:47.360]   See, Mint Mobile doesn't have stores. They are online only. They can take all that money that
[00:34:47.360 --> 00:34:52.880]   they're saving and put it right back in your pocket. I think you'll appreciate this. I hear
[00:34:52.880 --> 00:34:57.040]   this a lot from people who sign up for Mint Mobile. There are no hidden fees. There's no surprises.
[00:34:57.040 --> 00:35:01.600]   They don't come along with a little extra. Somebody was talking to their chatroom. He said,
[00:35:01.600 --> 00:35:09.120]   "Yeah, and not only that, they haven't raised my rates in two years." It is awesome. All plans
[00:35:09.120 --> 00:35:13.920]   come with unlimited talk and text. They're at what we call an MVNO running on T-Mobile. If you get
[00:35:13.920 --> 00:35:20.160]   good T-Mobile coverage, Mint Mobile will be just as good. High-speed data delivered on the nation's
[00:35:20.160 --> 00:35:26.000]   largest 5G network. I could tell you it's a heck of a good network. You get really good speed
[00:35:26.000 --> 00:35:33.680]   almost everywhere. $15 a month. That gives you 4GB of data a month. They've got plans with more.
[00:35:33.680 --> 00:35:37.600]   They even have an unlimited plan. The unlimited plan. How much do you pay for an unlimited cell
[00:35:37.600 --> 00:35:45.840]   service from a big carrier, right? $100. Easy. Once you add all those fees, Mint Mobile, $30
[00:35:45.840 --> 00:35:50.960]   out the door. $30 a month. But you choose the amount of data that's right for you.
[00:35:51.520 --> 00:35:56.320]   Stop paying for data you never use. Save money. They have a modern family plan.
[00:35:56.320 --> 00:36:03.040]   You don't get Sophia Vagara with it. You get to mix and match your data plans. Everybody in
[00:36:03.040 --> 00:36:07.040]   your family, and that could just be friends or roommates, gets the right amount of data.
[00:36:07.040 --> 00:36:11.760]   It starts at two lines. It's such a great deal. Switch to Mint Mobile. Get premium wireless service.
[00:36:11.760 --> 00:36:17.040]   Starting at $15 a month. He said, "Well, I have an iPhone 14. Does it do e-SIM?" Yes.
[00:36:18.000 --> 00:36:22.080]   If you need a sim, they'll send you a sim free of charge. Unlike the other guys, they don't charge
[00:36:22.080 --> 00:36:27.920]   you. But if you have e-SIM, you could be Mint right now. I'm an older listener. They have a 55+
[00:36:27.920 --> 00:36:34.560]   program that is unbeatable. Get your new wireless plan for $15 a month and get the plan shipped to
[00:36:34.560 --> 00:36:41.840]   your door free or just get it through the internet. If you're using e-SIM, when you go to mintmobile.com/twit.
[00:36:41.840 --> 00:36:47.920]   They sell phones too. I got a great deal on an iPhone SE from Mint Mobile. Mintmobile.com/twit.
[00:36:47.920 --> 00:36:53.760]   Cut your wireless bill to $15 a month. I buy it a year at a time because I love it. I know I'm
[00:36:53.760 --> 00:37:01.440]   going to use it. I get a great deal. I tell you, mintmobile.com/twit. Tell him Ryan Reynolds sent you.
[00:37:01.440 --> 00:37:08.720]   No, he doesn't need the money. Don't tell him that. I did want to show you. We were talking about
[00:37:08.720 --> 00:37:13.280]   how everybody really has been doing this, including, I might say, the Chinese,
[00:37:13.280 --> 00:37:18.080]   Jan Lecun of Facebook says, "Well, we got it. We got it. Google had it. Microsoft had it. I'm
[00:37:18.080 --> 00:37:24.080]   sure Amazon had it." One of the reasons is this was an article in the information. Eight research
[00:37:24.080 --> 00:37:30.960]   papers that set off the AI boom. All this stuff was done in public. They're quoting papers published
[00:37:30.960 --> 00:37:37.680]   in the last eight years by Google, Microsoft, Meta, OpenAI, and other places. They're all
[00:37:37.680 --> 00:37:42.720]   researchers there. It's the technologies that were used in stable diffusion and chat GPT.
[00:37:42.720 --> 00:37:49.440]   This stuff's not secret. This is one from Microsoft from 2015. Deep residual learning for image
[00:37:49.440 --> 00:37:54.480]   recognition. What's interesting is a lot of the authors of these have then moved on. The lead
[00:37:54.480 --> 00:38:01.760]   author is now at Facebook. Two other authors joined a Chinese image recognition company, which by the
[00:38:01.760 --> 00:38:08.160]   way is blacklisted by the US government in 2019. Another one founded an autonomous vehicle software
[00:38:08.160 --> 00:38:16.080]   developer, Momenta. That's just one of the papers. Have you seen this, Ben? These are not secret
[00:38:16.080 --> 00:38:22.480]   technologies held closely by companies. People know this stuff. Anybody could do it, right?
[00:38:22.480 --> 00:38:27.840]   Yeah. One, I do have a fun thing coming out of the information soon. Stay tuned for that.
[00:38:29.360 --> 00:38:34.720]   I did read that article because I do love the information. One thing that strikes me,
[00:38:34.720 --> 00:38:37.840]   there's a couple of things to strike me. One big thing that strikes me is that
[00:38:37.840 --> 00:38:43.680]   unlike other kinds of technologies, a lot of the artificial intelligence stuff,
[00:38:43.680 --> 00:38:49.200]   until this point has been published, it's part of that research,
[00:38:49.200 --> 00:38:56.640]   Ben ethos, that ethos of you're a professor at a university, how universities work.
[00:38:57.200 --> 00:39:03.360]   But I don't expect it to continue that way moving forward because now it has become such a competitive
[00:39:03.360 --> 00:39:08.400]   edge. I think Google honestly regrets publishing some of the things that published. I think OpenAI
[00:39:08.400 --> 00:39:12.640]   is going to start. This could be the definition. This is when it closes down.
[00:39:12.640 --> 00:39:18.160]   This is what I think will happen. I think it's good to be starting to close down because it's so
[00:39:18.160 --> 00:39:24.720]   important to the competitive. Apple was never going to publish how the iOS works or how to make
[00:39:24.720 --> 00:39:29.040]   a better mobile phone. Now everyone else is like, "Oh, we should not publish this because we're
[00:39:29.040 --> 00:39:35.360]   being punished for it in the market." Which is sad because obviously I think we wouldn't be this far
[00:39:35.360 --> 00:39:41.200]   ahead if they hadn't published a research. But I do expect things to go close down. I don't expect
[00:39:41.200 --> 00:39:45.520]   deep mind to publish new papers. It hasn't published one since I think mid-December. I don't know
[00:39:45.520 --> 00:39:50.560]   when they will ever publish another one again. Oh, that's interesting. Now that there's money
[00:39:50.560 --> 00:39:56.080]   in them in our hills. They're shutting it all down. That's what I think is happening.
[00:39:56.080 --> 00:40:01.280]   I can tell you for a fact, I have a friend that works a lot. He and Martin has been working on
[00:40:01.280 --> 00:40:09.680]   AI stuff for the last five to six years. They don't publish anything. They literally have air
[00:40:09.680 --> 00:40:15.680]   gapped laptops. He has to fly to Australia to work with a team and fly back home because they
[00:40:15.680 --> 00:40:19.920]   don't want anything getting out the stuff that they use. Obviously, they're selling to the government.
[00:40:19.920 --> 00:40:23.440]   But still, they're not sharing anything and they're making money off of it. So,
[00:40:23.440 --> 00:40:27.760]   everything should get shut down sooner or later because now there's so much money to be made.
[00:40:27.760 --> 00:40:33.280]   Even like I said, when I talk about stuff like regular people find a hustle, man, you don't have
[00:40:33.280 --> 00:40:38.000]   to write a kids book anymore. I can go out here and just tell a random story. I can get some pictures
[00:40:38.000 --> 00:40:43.520]   made. I put that thing up on Amazon and stay at home moms have bought it and returned it 32 times
[00:40:43.520 --> 00:40:48.800]   in a week and I made $500. There's so much free money out there. Journalism, someone in the chat
[00:40:48.800 --> 00:40:52.240]   asked for us a journalist. I sound like a crazy person. Of course, I'm not a real journalist.
[00:40:52.240 --> 00:40:58.960]   But you could play one on the radio. I could play one. Yeah, I could play it on here. Some people
[00:40:58.960 --> 00:41:03.280]   might be on fiber right now and you're paying them $40 to write a blog post that a computer wrote
[00:41:03.280 --> 00:41:07.600]   that you could just pay for the app. I'm just saying I didn't do the thing. I'm just saying AI is
[00:41:07.600 --> 00:41:13.200]   out here. It's heaven for both things I grump on and complain about. I've told you we're not going
[00:41:13.200 --> 00:41:20.320]   to Mars. I told you VR was trash. I told you that the meta versus trash. This, this is real.
[00:41:20.320 --> 00:41:24.320]   Yeah. This is going to power things for the next 45 years.
[00:41:24.320 --> 00:41:29.360]   This is the interesting question to me. It's real, but it's not real in the way.
[00:41:29.360 --> 00:41:37.440]   James Vincent wrote for the Verge a couple of days ago introducing the AI mirror test
[00:41:38.000 --> 00:41:44.560]   in which very smart people keep failing. The mirror test, they're talking about monkeys staring
[00:41:44.560 --> 00:41:49.600]   or any animal staring at themselves in a mirror. Do they know it's them? And of course, this is
[00:41:49.600 --> 00:41:56.000]   the problem with a lot of this chat GPT stuff is you're projecting intelligence, sentience
[00:41:56.000 --> 00:42:01.920]   emotions upon something that is not sentient, that is just mechanical.
[00:42:01.920 --> 00:42:07.520]   That's why I was joking this week on our podcast that, you know, lots of people are out there
[00:42:07.520 --> 00:42:11.280]   revealing themselves to be sociopaths. You're trying to get this AI.
[00:42:11.280 --> 00:42:12.480]   Oh, that's a good point.
[00:42:12.480 --> 00:42:16.560]   Terrible things. If it's like, yeah, whoa, whoa.
[00:42:16.560 --> 00:42:21.040]   I mean, my first instinct when Mark Scott gave me this stuff was like, all right, let me ask
[00:42:21.040 --> 00:42:25.840]   it some questions. May I can learn some things? My first instinct was not let me torture this
[00:42:25.840 --> 00:42:29.120]   thing or learn how to make it like. Until it says I hate you. I hate you.
[00:42:29.120 --> 00:42:33.200]   I hate you. Yeah. And there's like a certain place to hide the bodies.
[00:42:35.680 --> 00:42:41.040]   My thing is if like you're doing this to software, you probably do this to real people.
[00:42:41.040 --> 00:42:41.680]   That's a very good point.
[00:42:41.680 --> 00:42:45.840]   And so like, I'm a little worried when some of these people like out there, because this
[00:42:45.840 --> 00:42:50.560]   stuff's always existed, right? Just people let it out in different avenues. And now there's just a
[00:42:50.560 --> 00:42:54.000]   new avenue where they can do it. So I tell you, see these posts of people like, yeah, I made it
[00:42:54.000 --> 00:42:57.920]   go crazy. I'm like, what's wrong with you? I'm just, you just need to ask questions of like,
[00:42:57.920 --> 00:43:01.840]   knowledge of things I want to learn about. That's not my instinct, but I don't know.
[00:43:01.840 --> 00:43:06.320]   It's some AI is going to, it is a mirror, right? It does reflect us. And I think that's a really
[00:43:06.320 --> 00:43:11.680]   interesting analysis. One thing these systems are very good at is taking this fact, it's what they
[00:43:11.680 --> 00:43:18.320]   kind of do is taking text and summarizing it. And that's one thing I thought this is what
[00:43:18.320 --> 00:43:23.680]   Bing was going to do. That's not actually what Bing did, but I've been using it a search engine
[00:43:23.680 --> 00:43:27.920]   called NIVA, but there are a few others out there like you.com, Perceptiva, there are a bunch of
[00:43:27.920 --> 00:43:33.440]   them that have already been using AIs to generate summaries. NIVA does a really good job.
[00:43:33.440 --> 00:43:34.000]   You can do that.
[00:43:34.000 --> 00:43:36.000]   Yeah, it summarizes. Sorry?
[00:43:36.000 --> 00:43:38.000]   You can do that with Bing.
[00:43:38.000 --> 00:43:43.840]   Bing will do that as well. I think that's, I think the chat thing is really dopey. And
[00:43:43.840 --> 00:43:49.120]   Microsoft's is, it's a gimmick. Microsoft learned that with TAY, which became racist like that,
[00:43:49.120 --> 00:43:54.880]   because they let Twitter train it. They should have known, they did know, I'm sure it was just
[00:43:54.880 --> 00:44:00.880]   kind of a gimmick that this would go bad quickly. But the summary stuff, I think, is quite useful.
[00:44:00.880 --> 00:44:03.600]   The one thing that worries me and probably should worry you, Daniel,
[00:44:03.600 --> 00:44:11.360]   is it's summarizing content from sites like Windows Central to so well that you don't need to go to
[00:44:11.360 --> 00:44:18.400]   Windows Central to read the original material. Yeah, obviously we've had internally and our
[00:44:18.400 --> 00:44:21.920]   company. And of course, that's the big story internally at all publishing companies right now
[00:44:21.920 --> 00:44:28.080]   is the effect of this on business and plans. And you're right. I'll say that Microsoft's been a
[00:44:28.080 --> 00:44:33.520]   little bit better here. They do at least reference and put little numbers. Yeah, but it doesn't drive
[00:44:33.520 --> 00:44:37.920]   you to the article most of it or not. It's some of the time I'm not going to click. This is the
[00:44:37.920 --> 00:44:43.680]   Google snippets problem, but it's worse. It's far worse because Google's been sued in Australia and
[00:44:43.680 --> 00:44:51.440]   elsewhere in France and Spain, in the EU, because publishers say, oh, you're publishing a sentence
[00:44:51.440 --> 00:44:56.000]   or two from my article, you should pay me for that Google. Google's response is we're not,
[00:44:56.000 --> 00:45:00.400]   we're driving traffic to your article. We're not stealing from you. But I don't think you can make
[00:45:00.400 --> 00:45:06.160]   that argument with a summary that gives you all the reason that you use. A lot of times people go
[00:45:06.160 --> 00:45:13.040]   to a search engine for specific facts or information. If you get that in the summary, you're done.
[00:45:13.040 --> 00:45:17.760]   So it's going to be it's a chicken egg problem, right? Because if that happens long enough,
[00:45:17.760 --> 00:45:22.160]   then obviously publishers won't write those articles anymore because they won't make
[00:45:22.160 --> 00:45:26.320]   money from them. In which case, the AI doesn't get the information and needs to generate the
[00:45:26.320 --> 00:45:30.320]   summary because the AI is only as intelligent and what's actually published. If you ask it
[00:45:30.320 --> 00:45:36.320]   something that doesn't exist on the internet today, it has no answer for you. What if Google did,
[00:45:36.320 --> 00:45:43.040]   or Microsoft did what they do with robots.txt, right? Where you could say, you could actively say,
[00:45:43.040 --> 00:45:50.480]   I don't want you to summarize this content. But why would Microsoft want to allow you to opt out or
[00:45:50.480 --> 00:45:53.920]   anyone because that would be because otherwise they're going to have reputational damage and maybe
[00:45:53.920 --> 00:46:00.160]   lawsuits? Microsoft can handle it. We'll see. Microsoft can handle lawsuits though. I think
[00:46:00.160 --> 00:46:04.880]   they might see it. I don't know if that lawsuit would actually win. That's a harder case than I
[00:46:04.880 --> 00:46:10.400]   think like even the Google cases were difficult for publishers. And I think it's even more difficult.
[00:46:10.400 --> 00:46:14.320]   Like we're an uncharted territory in a lot of ways here, right? It's moving much more.
[00:46:14.320 --> 00:46:18.560]   Right. That's moving so much more. I mean, artists have the same issue. Greg Ruketik
[00:46:18.560 --> 00:46:22.560]   Ruketik Ruketik is the same issue with stable diffusion. You scanned all my art, you figured
[00:46:22.560 --> 00:46:26.880]   out how I do what I do. And then you're doing it. Imitations of it for people so they don't have
[00:46:26.880 --> 00:46:33.280]   to buy my art. Voice artists, people like me, voice artists are very upset at Spotify because
[00:46:33.280 --> 00:46:40.240]   Spotify has been letting Apple scan through audio books to generate simulated voices to
[00:46:40.240 --> 00:46:45.920]   read audio books. They just stopped that one. They got so much blowback. Well, that's what I'm
[00:46:45.920 --> 00:46:50.160]   saying. And that's why Microsoft might if you get enough blowback from the original content,
[00:46:50.160 --> 00:46:55.120]   folks, that's a solution, by the way. And by the way, it's a double edged sword for you, Daniel.
[00:46:55.120 --> 00:47:02.240]   Because if you said, and you had a, you know, AI.txt, you said no chat GPD scanning of our content,
[00:47:02.240 --> 00:47:06.400]   you probably wouldn't also get those links back. You wouldn't get to, you wouldn't show
[00:47:06.400 --> 00:47:11.840]   probably in search results. Also, there's the other half to this too, which is websites just might,
[00:47:11.840 --> 00:47:17.600]   like one way to sort of avoid this is not to build your content around what we call evergreen
[00:47:17.600 --> 00:47:22.240]   content in the business, right? This idea of you just build these articles that are how to that
[00:47:22.240 --> 00:47:27.360]   live on the internet for years and don't even rarely need updates and they get indexed. And
[00:47:27.360 --> 00:47:31.360]   they're just always there. There are entire sites that that's all they do. And they're going to be
[00:47:31.360 --> 00:47:37.040]   one called CNET that does that, right? That's what's all my own employer burn so much.
[00:47:37.040 --> 00:47:42.880]   Oh, burn. CNET. It's not CNET. It's Red Ventures, the new owners have seen it.
[00:47:42.880 --> 00:47:47.360]   But they have been accused of basically doing that. What do they call that?
[00:47:47.360 --> 00:47:49.520]   Right. You create art. Yeah.
[00:47:49.520 --> 00:47:50.800]   Plagiarism. Yeah.
[00:47:50.800 --> 00:47:56.960]   No, it's just it's driven around. You look up what are the top 10 searches on our site from Google
[00:47:56.960 --> 00:48:01.200]   and then you write evergreen articles with link. But the important thing is with links in there
[00:48:01.200 --> 00:48:07.520]   that you get revenue from. Yeah. Oh, the affiliate links. Okay. Two things on this one. One,
[00:48:07.520 --> 00:48:12.880]   for those who don't know, I was a columnist for CNET after Mashable. And this was when CBS
[00:48:12.880 --> 00:48:18.320]   owned them and I worked for Jim Lenzo now, the CEO of Yahoo. And it is so sad to see what has
[00:48:18.320 --> 00:48:25.280]   happened to this publication that was a it's been a part of my career because of just using
[00:48:25.280 --> 00:48:29.520]   horrible AI. Like every publication is going to probably figure out some way to use AI.
[00:48:29.520 --> 00:48:34.160]   This is just the worst method of using it. And it just makes me really sad.
[00:48:34.160 --> 00:48:39.200]   I do think one other solution that a lot of publishers will do is the paywall. Like paywall
[00:48:39.200 --> 00:48:43.440]   solve a lot of the problems. Can't read the rest of the text. You got to go and pay in order to go
[00:48:43.440 --> 00:48:48.640]   and like read the rest of the article. A paywall might be, I think paywalls are already becoming
[00:48:48.640 --> 00:48:52.080]   the thing for most publications. This will just accelerate that further.
[00:48:52.080 --> 00:48:57.680]   Who owns kind of a funny thing here, right? If you do, if you have AI generating the evergreen
[00:48:57.680 --> 00:49:02.960]   content and then AI summarizing the evergreen content, really there's not any human involved
[00:49:02.960 --> 00:49:07.920]   in that process that's actually being financially hurt. That's it. I will say that like when it comes
[00:49:07.920 --> 00:49:14.560]   to this content, there's only some of it that will fall prey to this. Like if you do original
[00:49:14.560 --> 00:49:20.480]   reporting, if you do original analysis, you doing actual reviews of products, the idea that that's
[00:49:20.480 --> 00:49:24.640]   going to be summarized is probably not that high, right? Because people aren't going to a search
[00:49:24.640 --> 00:49:29.120]   engine and be like, what's the latest news and stuff like that? Although they might. But a lot of
[00:49:29.120 --> 00:49:32.800]   times they're using social media and other ways to get their news and they're still clicking through
[00:49:32.800 --> 00:49:38.320]   to read that article or analysis or something like that. So you, you, you, you, Winter's Central
[00:49:38.320 --> 00:49:42.800]   like I'm More and Others was purchased by a Future, which is a British company.
[00:49:42.800 --> 00:49:47.120]   And you're fortunate because they're not a private equity company like Red Ventures.
[00:49:47.120 --> 00:49:53.600]   So they're not, they're not trying to turn a profit like by selling off the pieces, I presume.
[00:49:53.600 --> 00:49:58.320]   But you've got to look at, and I don't know what's happening at CNN. I mean, this is kind of in
[00:49:58.320 --> 00:50:02.720]   the year we had Connie Gugelmo, the editor in chief on a few weeks ago when that story first
[00:50:02.720 --> 00:50:09.520]   broke that they had used a chat GPT like artificial intelligence. I think they call it Word Smith
[00:50:09.520 --> 00:50:16.800]   to write 75 articles for the personal finance section at the, she had just published a blog post
[00:50:16.800 --> 00:50:22.080]   saying, Oh, you know, this is the stuff no writer wants to write. This is the evergreen stuff. We
[00:50:22.080 --> 00:50:28.320]   have an editor review it for. And I, at that point, I said, that's fine. Lindsay Tarantin,
[00:50:28.320 --> 00:50:32.400]   who's in charge of content there has also been on the show many times, both good friends.
[00:50:32.400 --> 00:50:40.320]   And I know that they are honorable respectable journalists. But I don't know if their owner
[00:50:40.320 --> 00:50:46.720]   is. And I, and I think that there's more and more evidence that Red Ventures was trying to create
[00:50:46.720 --> 00:50:56.000]   kind of a link, baby system of articles, basically, by it's ironic because it's massaging the search
[00:50:56.000 --> 00:51:01.520]   engines. These articles are created to get the search engines to point to it so that you will
[00:51:01.520 --> 00:51:07.520]   click on it so that they will get some money. Though if those are all we lose for chat GPT,
[00:51:07.520 --> 00:51:15.200]   I won't be too sad. Yeah, there's a, I think the part with CNET was just the, they're kind of hiding
[00:51:15.200 --> 00:51:19.920]   it right. So they were forthcoming about it. And then they kind of, and they were kind of
[00:51:19.920 --> 00:51:23.920]   writing it since they were like, this will just blow over. And don't worry about it. I think they're
[00:51:23.920 --> 00:51:29.040]   right in that. I mean, that is what's going to happen here. But you're also right that most
[00:51:29.040 --> 00:51:35.200]   publishers will find a way to use this technology. And I'm actually okay with it when it comes to
[00:51:35.200 --> 00:51:41.520]   something that, you know, best of content, which is a real nightmare to write and maintain. And
[00:51:41.520 --> 00:51:46.960]   having AI basically do that, and then have a human editing it, making sure it's okay and accurate,
[00:51:46.960 --> 00:51:51.600]   which sometimes can actually be more work. But that's something that CNET was analyzing, was to see
[00:51:51.600 --> 00:51:58.160]   when a human had to go like fact check it, if it was more work than just rewriting it. So I think
[00:51:58.160 --> 00:52:02.560]   this will definitely be part of the publishing world. But like I said, for original content,
[00:52:02.560 --> 00:52:08.720]   I'm not so concerned about it at this time. But if all you do is content farming and SEO
[00:52:08.720 --> 00:52:13.680]   stuffing, yeah, you should probably be worried. Good. I don't mind those people can, yeah,
[00:52:13.680 --> 00:52:18.400]   can suffer. Everything's about to turn into the YouTube infomercial system anyway, right?
[00:52:18.400 --> 00:52:21.520]   Oh, I got it. I hope you're not 42 people.
[00:52:21.520 --> 00:52:27.520]   Oh, the review. And dude, if you go on TikTok, that's what I use a certain kind of sheets.
[00:52:27.520 --> 00:52:32.240]   I see the same sheets 42 times in the TikTok feed. I want to take the sheets off and set them on
[00:52:32.240 --> 00:52:37.760]   fire and get new. How does it know what kind of sheets you have? I mean, because I guarantee you,
[00:52:37.760 --> 00:52:42.480]   I don't see those ads on TikTok. You could just tell. They're listening.
[00:52:42.480 --> 00:52:46.640]   You could just tell the TikTok to like stop showing you those ads. I do it all the time.
[00:52:46.640 --> 00:52:50.960]   Oh, yeah. But then you get the UGC and the people that are making the ads on their own.
[00:52:50.960 --> 00:52:54.080]   Oh, yeah. And that's actually the company. So that's what I'm saying. That's what it's going
[00:52:54.080 --> 00:52:59.520]   to breed more of people just doing their own. Hey, I drink this juice. Have you tried this juice?
[00:52:59.520 --> 00:53:04.400]   I lost 42 pounds. You don't know how fat I was before this video. And that's what it's going to
[00:53:04.400 --> 00:53:10.240]   turn into with the with the creators making more content because you can just puppy mill out stuff
[00:53:10.240 --> 00:53:13.680]   with AI right now. So there's going to be more value in that and people can get a better check.
[00:53:13.680 --> 00:53:18.320]   I guess the YouTube information system. Here's what I'm going to do. Oh, and I'm going to use
[00:53:18.320 --> 00:53:23.520]   it AI to recreate you. And I'm going to use an AI voice. Hypphasize it, recreate your voice.
[00:53:23.520 --> 00:53:29.760]   And I'm going to have you sell some sheets to a whole bunch of people on TikTok.
[00:53:29.760 --> 00:53:33.840]   Just on just on the side note of that, why you're making it, I need two things. I need
[00:53:33.840 --> 00:53:39.680]   a ASCO doctor, a chat GBT. They'll just answer things crazily like I do. You can give content from
[00:53:39.680 --> 00:53:44.000]   me. And then I need something that scans the Bible and it says, ask the Lord. And then you're
[00:53:44.000 --> 00:53:47.840]   out there cheating and doing something wrong. You asked me what you do. And then he tells you
[00:53:47.840 --> 00:53:51.920]   you're going to get smited down if you don't stop cheating. That's what we need. If you want,
[00:53:51.920 --> 00:53:56.560]   that's a billion. Let me put my glasses back on. I feel like Ben Parr. I feel smart. That's a lot
[00:53:56.560 --> 00:54:01.120]   of money right there. I asked the Lord and need a good voice for the Lord. He's going to be like
[00:54:01.120 --> 00:54:09.600]   a big deep voice saying no. Right. Yeah. There is a chat. There is something called chat K J V AI
[00:54:09.600 --> 00:54:16.240]   power chat bot. King James versions scriptures. Oh my god. Oh my course. I already on my million
[00:54:16.240 --> 00:54:22.400]   dollar idea. See, no good D goes on. You said it and boy, they got it immediately. Right. Yeah.
[00:54:23.040 --> 00:54:29.360]   They already made it. It says chat with the scriptures and and see if you're right with God.
[00:54:29.360 --> 00:54:35.200]   Wow. Look at that. See? That's good. That's good. There's a website out there called
[00:54:35.200 --> 00:54:39.040]   there's an AI for that.com. And I know I'm just going to tell you, I look at that and that just
[00:54:39.040 --> 00:54:43.440]   tells me all the new AI stuff that's coming out and just in a product on things. But you just up
[00:54:43.440 --> 00:54:48.800]   every up to every day. Some new AI thing. So all right. So we're really crazy about this stuff.
[00:54:48.800 --> 00:54:53.520]   We love it. 1,853 AIs for 487 tasks.
[00:54:53.520 --> 00:55:00.560]   Is it going to be the flavor of the month in a year or two years or three years?
[00:55:00.560 --> 00:55:07.120]   We're just going to remember that when we were all into, you know, chat GPT stuff or
[00:55:07.120 --> 00:55:12.800]   do you think it's got more obviously you you think it has legs, Ben Parr?
[00:55:14.160 --> 00:55:22.160]   Yes, I am biased, but I do think it has. What realm? I mean, the idea of a fancy Eliza is not a
[00:55:22.160 --> 00:55:30.080]   winning proposition. I don't think maybe it is. Maybe people are low. So, so I, okay, I've gotten
[00:55:30.080 --> 00:55:36.480]   people who compare generative AI things to web three. And I think that's completely wrong. And I
[00:55:36.480 --> 00:55:39.680]   think that's a good way to think about this being web. Being a scam.
[00:55:41.040 --> 00:55:47.360]   Web three being something that is extraordinarily difficult to use and was most of the time with
[00:55:47.360 --> 00:55:52.480]   very rare exceptions, a solution looking for a problem. Yes. Okay. I'll grant you.
[00:55:52.480 --> 00:55:57.280]   You can create by venture capitalists who thought they might make some money on this solution waiting
[00:55:57.280 --> 00:56:01.200]   for looking for it. It's a technology about money. It was going to attract people with money.
[00:56:01.200 --> 00:56:06.640]   And I have like, it's like crypto. It's like an F2. That's all that. Right. But AI,
[00:56:06.640 --> 00:56:10.000]   I could immediately tell you a hundred things that it could help you with.
[00:56:10.000 --> 00:56:14.320]   That's actually my question in a nutshell is, is this another cryptocurrency?
[00:56:14.320 --> 00:56:19.600]   It's not because it actually has real use cases. I will actually help you,
[00:56:19.600 --> 00:56:24.880]   you know, summarize a whole bunch of things or quickly write the bones for a newsletter or
[00:56:24.880 --> 00:56:29.840]   write a newsletter. If you know how to give it the prompts, it can write legal documents. It can do
[00:56:29.840 --> 00:56:35.520]   things that will speed up your life. And it is the absolute worst. It will ever be in human history.
[00:56:35.520 --> 00:56:41.200]   It will only get better and more efficient and more effective with every day that passes.
[00:56:41.200 --> 00:56:47.280]   And so I don't think it's not a fact. It's more like the iPhone, the iOS, where the first wave of
[00:56:47.280 --> 00:56:55.040]   apps on the iPhone were fart apps and they were horrible. And then people built Tinder and Uber
[00:56:55.040 --> 00:57:00.000]   and Snapchat. That's what I think will happen. And at a certain point, we won't be talking about
[00:57:00.000 --> 00:57:05.120]   AI as like AI conferences and like, as this new thing, it'll just be part of the background. You
[00:57:05.120 --> 00:57:08.320]   don't talk about, Oh, it's built on top of the iPhone. It just it is.
[00:57:08.320 --> 00:57:17.040]   So let's go ahead. As I said earlier, it's here. Like I said, I I grumped on a lot of things. This
[00:57:17.040 --> 00:57:22.480]   is one thing I'm not grumping on. They're making so many things are just useful. You do a podcast,
[00:57:22.480 --> 00:57:27.520]   you do a show. Guess what? Some people are going to be able to upload a video and then have it
[00:57:27.520 --> 00:57:34.000]   chunk out 30 second clips, minute clips for YouTube, for Twitter, for anything. And the things that
[00:57:34.000 --> 00:57:39.840]   are coming are so powerful and useful. The fact that the half the country, we didn't put out 42
[00:57:39.840 --> 00:57:45.680]   pictures themselves looking like Star Wars doing characters should tell you the fact that art is
[00:57:45.680 --> 00:57:50.640]   something that's just built in human nature in your mind. And you can make art of yourself. Like,
[00:57:50.640 --> 00:57:55.200]   I didn't even do it myself. I did it for my dad who passed away. And I just generated 40 pictures
[00:57:55.200 --> 00:57:59.520]   of my dad that were new and interesting to me and my daughter. We looked at him. I almost cried a
[00:57:59.520 --> 00:58:03.760]   little bit because I thought it was so cool. There are so many things coming down the pipe.
[00:58:03.760 --> 00:58:08.960]   Audio. That's what if your audio sounds like trash feed into a AI, it'll fix up for you.
[00:58:08.960 --> 00:58:14.800]   There are so Uncle Leo is here to stay is here. It's a, you know, it's here. I believe you because
[00:58:14.800 --> 00:58:19.120]   you look smart in those glasses. So I think you're right. Hey, little Ben Parnis, what they call me
[00:58:19.120 --> 00:58:26.800]   just treats. How about you, Daniel? You agree? Oh, yeah, absolutely. And I agreed to with what
[00:58:26.800 --> 00:58:31.280]   Ben was saying that, you know, where a lot of us are having fun now making fun of AI and like
[00:58:31.280 --> 00:58:36.160]   specifically being chat and mistakes at makes. And I'm like, yeah, laugh. It's going to improve so
[00:58:36.160 --> 00:58:42.160]   rapidly that we won't be laughing at this stuff within weeks and months because this is unlike
[00:58:42.160 --> 00:58:47.040]   any other technology we've had. You know, you look at smartphones that came out, what, 2004,
[00:58:47.040 --> 00:58:53.600]   2005, then the iPhone came out and it was like a long time before that became mainstream.
[00:58:53.600 --> 00:58:58.320]   And that's how hardware works, right? It just takes a long, long time. This is just going to evolve
[00:58:58.320 --> 00:59:04.000]   so quickly. It becomes so much more efficient and powerful that I think people would be kind of
[00:59:04.000 --> 00:59:07.760]   really surprised by how much is this going to affect and transform the economy. And I think
[00:59:07.760 --> 00:59:11.360]   that's going to be a really big thing over the next couple of years. This site that you talked
[00:59:11.360 --> 00:59:17.600]   about, Ben, there's an AI for that.com actually does it by year starting in 2015 when there were
[00:59:17.600 --> 00:59:25.440]   three, 2016 when there were two, by 2017, there's more than a dozen by 2018. There's double that.
[00:59:25.440 --> 00:59:32.240]   By 2019, there's double that. It's growing exponentially. This is 2020. There's
[00:59:32.240 --> 00:59:39.440]   almost too many to count. And of course, 2023, it's now you're going month by month because
[00:59:39.440 --> 00:59:46.960]   every day there's a dozen new applications for this. Is there going to be a winner? Can we say
[00:59:46.960 --> 00:59:54.720]   that one of the incumbents, Google, Microsoft, Amazon, Apple, Facebook, will one of them be a
[00:59:54.720 --> 01:00:01.040]   winner? That's a good one. I wonder what Apple's game is. Like what's what's
[01:00:01.040 --> 01:00:06.800]   I don't know. Apple has a worst AI out there right now, Siri. But as we've learned,
[01:00:06.800 --> 01:00:12.800]   money suck. Well, but a lot of these companies are holding on to something much, much better.
[01:00:12.800 --> 01:00:17.760]   Maybe Apple will bypass AI. Maybe they're that might be a mistake. I mean, Apple and
[01:00:17.760 --> 01:00:24.800]   Meta both have put all their money in AR VR. And it may turn out, in fact, it's starting to look like
[01:00:24.800 --> 01:00:32.160]   that was a losing horse. Yep. Apple's voice. Well, Apple just released an AI product. The
[01:00:32.160 --> 01:00:37.440]   voice you talked about it. The book readers. Yeah. And it's good. It's very good. It's quite good.
[01:00:37.440 --> 01:00:42.400]   Yeah. And so Apple was just very methodical about it. They will release something. They will
[01:00:42.400 --> 01:00:48.480]   greatly improve Siri. There will be all of that. I think there will be some winners losing,
[01:00:48.480 --> 01:00:52.320]   but not like one completely wins and the other doesn't. It's the same thing as like,
[01:00:52.320 --> 01:00:57.760]   you know, there's Android and there's iPhone, there's, you know, Word and there's Google Docs.
[01:00:57.760 --> 01:01:01.840]   It'll be the same kind of thing. Like each one's going to have a piece of the pie.
[01:01:01.840 --> 01:01:05.280]   Microsoft, as we all know, has done a very good job of taking a bunch of the pie.
[01:01:05.280 --> 01:01:10.640]   There's going to be a lot of like only a few startups that are like the infrastructure layer
[01:01:10.640 --> 01:01:16.080]   companies that like where all the other AI companies are built on top of. So like OpenAI is one of
[01:01:16.080 --> 01:01:20.160]   those companies, Google. There's a couple of others that are out there. And then there's going
[01:01:20.160 --> 01:01:26.320]   to just be thousands of other companies and products. Remember, you're all old enough to
[01:01:26.320 --> 01:01:32.720]   remember when search started, there was Yahoo, which was a directory and then came out to Vista
[01:01:32.720 --> 01:01:40.640]   and excite and a bunch of, I mean, G ask, Jeeves. And then along came Google and it was like,
[01:01:40.640 --> 01:01:45.760]   at in the beginning of search, there were many choice. It was kind of like it is today, right?
[01:01:45.760 --> 01:01:50.160]   There were many good choices, but somebody came along and beat everybody else.
[01:01:50.160 --> 01:01:55.760]   Well, Uncle Leo, it's going to come down to the point of who's eating the most at the table.
[01:01:55.760 --> 01:01:59.680]   When you speak about Google, think about Google and when they acquired YouTube,
[01:01:59.680 --> 01:02:03.760]   you know, think about Microsoft when they got into the Facebook game, then Facebook goes and gets
[01:02:03.760 --> 01:02:08.800]   Instagram there. The the behemoths are going to see the cream of the crop and they're going to be
[01:02:08.800 --> 01:02:14.560]   there's going to be a little arms race for. Hey, I see your team comes to the big boy table.
[01:02:14.560 --> 01:02:18.880]   I see your team. Okay, you got those guys. It's just a roster play of like who's picking up at the
[01:02:18.880 --> 01:02:23.920]   yard and they're going to just be integrated into the thing. So that's what's going to happen. It's
[01:02:23.920 --> 01:02:28.320]   it's the the top. Like I said, the thing that we make fun of and like movies, we're like,
[01:02:28.320 --> 01:02:32.400]   you're going to be buying Google socks in 50 years. That's going to be true because these top
[01:02:32.400 --> 01:02:37.680]   companies are going to acquire the best products that come out and then integrate them into their
[01:02:37.680 --> 01:02:41.360]   products. And then the other company is going to try and compete against that. So the cream of the
[01:02:41.360 --> 01:02:45.040]   crop will get picked up and bought and that the same way everything else goes.
[01:02:45.040 --> 01:02:49.280]   It does seem I have to say this is kind of encouraging and Ben, you might be encouraged as well,
[01:02:49.280 --> 01:02:55.840]   that you don't have to have the big bucks. You don't have to be a fang to do this, right? There
[01:02:55.840 --> 01:03:01.200]   are a lot of little scrappy startups in this space. Is that because it's cheap to do and well
[01:03:01.200 --> 01:03:09.120]   understood? Is it in other words, is AI a commodity already? It's so there's two versions of this.
[01:03:09.120 --> 01:03:15.520]   And yes, is sort of and there's some of this in my information article coming out. One piece here
[01:03:15.520 --> 01:03:20.800]   is that it's just cheap and easy if you're a developer to implement this complex AI. All you
[01:03:20.800 --> 01:03:26.320]   need to do is an API call to open AI. It is just so easy to use. There's in other words, use their
[01:03:26.320 --> 01:03:30.080]   large language model. How hard is it to create your own large language model?
[01:03:30.080 --> 01:03:38.960]   Stupidly hard and costly. Yeah. Expensive. Yeah. In fact, that's one of the things that's missing
[01:03:38.960 --> 01:03:45.200]   in this equation is to do something that is as up to date as a Google or Bing search
[01:03:45.920 --> 01:03:52.000]   means you have to be constantly building the corpus, which is very expensive.
[01:03:52.000 --> 01:03:58.320]   Right? I mean, this is not, this is, I think Sam Altman said is about 10 times the cost,
[01:03:58.320 --> 01:04:03.280]   a chat GBD query is about 10 times the cost of a Google search. And I think that's probably what
[01:04:03.280 --> 01:04:09.600]   it would be. You would be in order of magnitude more expense to do a search engine powered by AI.
[01:04:11.200 --> 01:04:18.560]   I mean, Microsoft teamed up with OpenAI in what 2020 with their supercomputer.
[01:04:18.560 --> 01:04:23.440]   Right. And that was what they were doing the training on and led to what Microsoft calls
[01:04:23.440 --> 01:04:30.480]   Prometheus, which is their version of a language model that's built off of chat GBT that has guard
[01:04:30.480 --> 01:04:35.120]   rails and different. Is that what they're using for Bing search? Is Prometheus? Yeah, it's called
[01:04:35.120 --> 01:04:40.800]   Prometheus. Yeah. And so that's their language model. And it's, so it's not exactly chat GBT.
[01:04:40.800 --> 01:04:46.880]   It's built off of that. But you need to, you know, Microsoft was only one at the time of like five
[01:04:46.880 --> 01:04:50.720]   supercomputers in the world. I mean, it's not a lot of them. And this could probably get such a
[01:04:50.720 --> 01:04:55.280]   quantum computing eventually when you need even more power. But yeah, you really do need a lot, a lot
[01:04:55.280 --> 01:05:00.960]   of data to train these things. And that's where the internet comes in and now real life people are
[01:05:00.960 --> 01:05:04.560]   using it. So Microsoft, I know they're researchers right now are very excited about getting all this
[01:05:04.560 --> 01:05:09.040]   data people are using it real time and they're fixing things and adjusting it because that's what's
[01:05:09.040 --> 01:05:14.320]   going to make it better. Ben, what do you think?
[01:05:14.320 --> 01:05:22.800]   There will be a couple of companies that power the core of what most AI applications will be using.
[01:05:22.800 --> 01:05:28.000]   A lot of the opportunity is going to be in companies built on top of things like OpenAI
[01:05:28.000 --> 01:05:34.000]   or Lamb does, which is Google's, and utilizing it to, and like adding their own data set or adding
[01:05:34.000 --> 01:05:39.520]   some of their own machine learning, like for specific industries for like e-commerce or legal or
[01:05:39.520 --> 01:05:44.400]   things like that. And there's going to be other use cases that will come out. I do think over time,
[01:05:44.400 --> 01:05:50.000]   this like every other technology will get cheaper and easier. It is going to be a commodity. And
[01:05:50.000 --> 01:05:56.080]   that is a good thing for AI development. And hopefully there are at least a couple of winners
[01:05:56.080 --> 01:06:02.880]   so that developers have a choice of what to use. Let me play. You mentioned app on this far.
[01:06:02.880 --> 01:06:10.160]   I know this is the only thing Apple has released publicly is their Apple book's reader. Although,
[01:06:10.160 --> 01:06:16.000]   gosh, they might have a car self-driving vehicle AI going. Maybe Siri is secretly smart under the
[01:06:16.000 --> 01:06:20.960]   hood and suddenly they'll flip a switch. But these voices are, as you said, these are voices are
[01:06:20.960 --> 01:06:28.560]   pretty good. This is a fiction. Let me make sure my sound is turned on. A fiction romance voice
[01:06:28.560 --> 01:06:35.520]   called Madison movement in the greenhouse drew his eye and a woman emerged. At first, he wasn't sure.
[01:06:35.520 --> 01:06:42.160]   You could see how the people whose voice is there stealing from Spotify might say that's my that's
[01:06:42.160 --> 01:06:47.840]   very close to my voice. He's a he's Jackson of fiction baritone. I looked up to find a wall of
[01:06:47.840 --> 01:06:54.480]   trees had materialized ahead of us. That sounds like a little bit like an AI. Helena nonfiction.
[01:06:54.480 --> 01:07:00.240]   On nights with a new moon, we would walk to the end of the beach to find our favorite constellation,
[01:07:00.240 --> 01:07:05.600]   the Pleiades. See, I could listen to her. I could listen to that. I think the nice things
[01:07:05.600 --> 01:07:09.440]   everyone's going to have like their own preference for which AI they want to listen to. And look,
[01:07:09.440 --> 01:07:15.440]   I actually did a TikTok with those voices and people like love them and talked about them.
[01:07:15.440 --> 01:07:20.480]   I didn't even mention before to you. I went viral on TikTok for a video about AI and education.
[01:07:20.480 --> 01:07:23.680]   And now I'm a TikToker for some reason. It got a million and a half years.
[01:07:23.680 --> 01:07:30.240]   What? How many million and a half years? Nice. Yeah. What's your TikTok handle, my friend?
[01:07:30.240 --> 01:07:36.080]   Just like everything else on the internet at Ben Parr. Okay. Okay. Should we watch it?
[01:07:36.080 --> 01:07:39.520]   Should we watch it? Do we just all we need to do is watch your TikTok and we'll know everything
[01:07:39.520 --> 01:07:45.440]   we need to know? Which one of these chat GPT's cheating scandal? That's the most recent that
[01:07:45.440 --> 01:07:49.920]   the big one is a little bit down. It's the AI and education. That's the one with a million and a
[01:07:49.920 --> 01:07:55.520]   half years. Is this going to be the beginning of your information? Or goes see my TikTok.
[01:07:55.520 --> 01:08:05.760]   I've been busy the last two months. Here it is. AI is going to radically change everything and
[01:08:05.760 --> 01:08:12.560]   we aren't ready for it. Education is part one. Let's see. Oh, I keep turning one education.
[01:08:13.520 --> 01:08:18.560]   You've probably seen this on the internet recently. It's chat GT. So I encourage you to
[01:08:18.560 --> 01:08:22.960]   continue with the TikTok. You know, my son has two million followers on the TikTok.
[01:08:22.960 --> 01:08:29.200]   That's right. He's a famous. I think it's a advice from me. He's become a. Yes. He's become a
[01:08:29.200 --> 01:08:34.480]   TikTok. But it's but it's all real. And you know what cooking cannot be done by an AI until they
[01:08:34.480 --> 01:08:41.200]   give it hands and then watch out. So even even then. So we're having. So we're having this love
[01:08:41.200 --> 01:08:46.560]   fest about AI. Have you ever tested AI recipes? They're awful. Okay, go ahead.
[01:08:46.560 --> 01:08:56.640]   Let's let. So that DJ that used Eminem and his set and he used artificial intelligence to
[01:08:56.640 --> 01:09:01.520]   write it and to do the voice. And he went out there into the club and he pumped it out and
[01:09:01.520 --> 01:09:08.960]   everybody got hyped on it. Like I already had to live through Donald J. Trump and the Russian
[01:09:08.960 --> 01:09:14.720]   conspiracies of life and fake news and this and that. And now I got people I hear doing DJ
[01:09:14.720 --> 01:09:20.880]   sets. Let me play a little bit for you. This is Dave Kuetta. A million views on the of this
[01:09:20.880 --> 01:09:26.720]   Twitter video of him. So you're saying this is a virtual an AI Eminem.
[01:09:26.720 --> 01:09:32.400]   Yes. This is the future. Ray sound. I'm getting lost in an onion. So he's talking.
[01:09:32.400 --> 01:09:34.560]   This is the future. Ray. He's applied.
[01:09:35.520 --> 01:09:41.280]   Eminem's voice to his voice. Okay, I get up and dance.
[01:09:41.280 --> 01:09:51.040]   Something that I made as a joke and it works so good. I could not believe it. I discovered
[01:09:51.040 --> 01:09:58.240]   those websites that are about AI. Basically, you can write lyrics in the style of
[01:09:58.800 --> 01:10:05.600]   adding artists you like. So I typed right a verse in the style of Eminem about future
[01:10:05.600 --> 01:10:11.280]   rave and I went to another AI web. I want I want Eminem to do Dave Kuetta going.
[01:10:11.280 --> 01:10:20.000]   You got to lose yourself to the music. You got to throw up at your mom as a spaghetti on your
[01:10:20.000 --> 01:10:26.560]   sweater. That would be so turn about his fair play. I was pretty good. That sounded like Eminem.
[01:10:27.840 --> 01:10:31.280]   Yeah. And that's a problem, right? Like if I were Eminem, I'd be pissed.
[01:10:31.280 --> 01:10:37.200]   Yeah. Well, forget Eminem. I don't care about a rapper making a bump in tracking.
[01:10:37.200 --> 01:10:40.960]   It is though. That's where it'll happen. Right? Because that's where sampling started.
[01:10:40.960 --> 01:10:49.520]   Again, forget sampling. Okay, let's put it this way. You you're Eminem and he's got 400 million
[01:10:49.520 --> 01:10:54.320]   followers of dudes that will bleach their hair at any moment because he said so and you make this
[01:10:54.320 --> 01:11:01.120]   AI voice and he says, give me a dollar at Eminem needs a dollar.com and he just rocks off. Like
[01:11:01.120 --> 01:11:06.640]   there's so many. I mean, Dan, don't I am a sociopath. Do not come for me. I do think a bad
[01:11:06.640 --> 01:11:12.320]   evening. Now the world works. But don't don't send the cops. I'm just saying there's a lot of
[01:11:12.320 --> 01:11:17.440]   evil things out here that are coming down the pipeline to you. You got to lose yourself.
[01:11:17.440 --> 01:11:22.400]   Oh, to the music. He's absolutely right. I'm making fun of my fear.
[01:11:22.400 --> 01:11:29.600]   Oh, but no, so I covered a I covered a story in my tick talk about the there was a bunch of
[01:11:29.600 --> 01:11:34.720]   four channers and they replicated the voices of a couple that was three celebrities. Yes.
[01:11:34.720 --> 01:11:40.800]   Right. They did Emma Watson reading my comp like imagine someone making a voice and then
[01:11:40.800 --> 01:11:45.840]   calling your old grandmother and just like scamming her money. But it had to be at this point.
[01:11:45.840 --> 01:11:52.000]   Wouldn't have to be your old grandmother because anybody of who's paying attention.
[01:11:52.000 --> 01:11:55.520]   What could know that could be made up now? It's it's like getting better.
[01:11:55.520 --> 01:11:58.080]   You don't trust in me. That does. That's not the reason.
[01:11:58.080 --> 01:12:04.720]   Okay. So that's not the problem. The problem is there's so many things that we had a balloon in
[01:12:04.720 --> 01:12:10.160]   the air and 50% of the country thinks it came from China. 42% of people think that some guy got
[01:12:10.160 --> 01:12:15.120]   dumped at a proposal at a wedding and just let the wound flop like you. That's something that was
[01:12:15.120 --> 01:12:20.080]   on the news every hour. So no, people can't differentiate. What's real. Do you want to know
[01:12:20.080 --> 01:12:23.760]   about that balloon? Especially when you got AI. Do you want to know about the one that they shut
[01:12:23.760 --> 01:12:28.320]   down over over Alaska? Do you want to know about that one? Tell me about it.
[01:12:28.320 --> 01:12:34.560]   It came from a hobby club. Of course. It came from well, nobody's sure because.
[01:12:34.560 --> 01:12:41.840]   Okay. Conspiracy theory here. They can't find the remains and they've given up looking for it,
[01:12:41.840 --> 01:12:50.000]   right? This there was. Okay. You tell me. Okay. There's a Northern Illinois hobbyist club
[01:12:50.000 --> 01:12:59.520]   called the bottle cap balloon brigade that launches their mylar party balloons with GPS and other
[01:12:59.520 --> 01:13:08.640]   electronics on them. They launched a balloon 123 days ago with a call sign, by the way,
[01:13:08.640 --> 01:13:17.280]   just in case you might have seen it, K9YO-15. It has circled the earth six times. The last time it
[01:13:17.280 --> 01:13:25.760]   was seen was February 11th. It was heading towards Alaska. The next day they lost track of it,
[01:13:25.760 --> 01:13:34.800]   which coincidentally was the same day. Two F 22 Raptors were dispatched to shoot the mysterious
[01:13:34.800 --> 01:13:44.240]   object down with a $400,000 side winder missile. They got it, by the way. And ever since the balloon's
[01:13:44.240 --> 01:13:48.720]   been unheard of ever since. Yeah, I think a side wider might.
[01:13:48.720 --> 01:13:55.520]   I got to say, they talked to the balloon guys and they said, you know, the way this works is at
[01:13:55.520 --> 01:14:02.080]   ground level, the air pressure is low enough or high enough, I guess, that the balloon is,
[01:14:02.080 --> 01:14:06.000]   you know, just a balloon as it gets higher and higher, it gets taller and taller as the air pressure
[01:14:06.000 --> 01:14:10.160]   goes down. And they said, by the time it's at 60,000 feet, which is where these things fly,
[01:14:11.120 --> 01:14:16.960]   it's very fragile. They said, you could just, you know, whizzed by it in the F 22 and it would have
[01:14:16.960 --> 01:14:24.080]   popped. You didn't have to use a $400,000 air to air missile on this thing. So, but how often do
[01:14:24.080 --> 01:14:32.240]   you not get to use a side one? Never used it. They've had this this aircraft costs $133 million each,
[01:14:32.240 --> 01:14:40.880]   $70,000 an hour in the air. By the way, after you fly it, there's 40 hours of maintenance
[01:14:40.880 --> 01:14:45.840]   that you have to do before you could fly it again. These are very finicky little things,
[01:14:45.840 --> 01:14:50.800]   but they've never got to use it right in a decade. They've had these these these raptors around,
[01:14:50.800 --> 01:14:56.800]   finally, we got something that's part of the American budget. We spend, spend $4 billion on
[01:14:56.800 --> 01:15:01.520]   planes whenever you use. And also, this is an argument I had with a very somewhat rational person. I
[01:15:01.520 --> 01:15:05.040]   can't say that they're smart after this conversation. They're like, China's trying to spot us with
[01:15:05.040 --> 01:15:08.720]   these balloons and we had to take it out. And I said, okay, you got this phone in your hand,
[01:15:08.720 --> 01:15:12.640]   you see on the back, it wasn't made here in America. Half the things you own weren't made here.
[01:15:12.640 --> 01:15:15.520]   China. They don't need a balloon. They got your phone.
[01:15:15.520 --> 01:15:20.000]   And then on top of that, didn't you got TikTok when you're phone, you're giving them all the
[01:15:20.000 --> 01:15:24.080]   information and listening to you to record. And if that wasn't good enough, did you not know
[01:15:24.080 --> 01:15:28.400]   that satellites could basically read your text messages? I know you're solders. I know the balloon
[01:15:28.400 --> 01:15:33.200]   satellites. Look at that. Yeah. Like what like people, what are you talking about? They need a
[01:15:33.200 --> 01:15:42.400]   balloon to spy on me. They you're spying on yourself. Shut up. The this balloon, by the way, had a payload
[01:15:42.400 --> 01:15:50.240]   that weighs 16 grams, about half an ounce, GPS module, a tiny computer in a small solar package.
[01:15:50.240 --> 01:15:58.080]   They are FAA legal. They're so light, they're not subject to any requirements. The radio
[01:15:58.080 --> 01:16:05.600]   transmitters FCC registered, but the balloon itself can't hurt a jet, even if it's in the path.
[01:16:05.600 --> 01:16:13.680]   So the bottle cap balloon brigade, I'm sorry. When we shot you. Great expense to the taxpayers.
[01:16:13.680 --> 01:16:19.440]   We have eliminated yet another threat to the great American. China was like, oh, is that how
[01:16:19.440 --> 01:16:25.680]   you handle balloons? Well, just know, if you fly kind over here, don't you think the Chinese are
[01:16:25.680 --> 01:16:33.200]   just laughing at this? They go, well, as an ours. I mean, by the way, they ever covered all the
[01:16:33.200 --> 01:16:37.280]   Detroit is from that first balloon over that they shot down over North Carolina. They've got it
[01:16:37.280 --> 01:16:42.320]   all and I'm sending it to the FBI. And they seem to be confident that that was indeed a Chinese
[01:16:42.320 --> 01:16:47.680]   balloon could have been a weather balloon. I mean, there's 1600 weather balloons launched every day
[01:16:47.680 --> 01:16:53.520]   in this guy just watch. There's going to be thousands launched. And then sidewinder missiles
[01:16:53.520 --> 01:16:56.520]   are all gone. Yeah. You used up all your
[01:16:56.520 --> 01:17:00.080]   servers. It's good for the side.
[01:17:00.080 --> 01:17:02.160]   Right. The industry. Go ahead.
[01:17:02.160 --> 01:17:10.480]   I have a serious question. So should it could there be like a master AI detector page where I could
[01:17:10.480 --> 01:17:14.400]   just put something in there kind of like with the Photoshop? I put a picture in there and it tells
[01:17:14.400 --> 01:17:18.560]   me if it's photoshopped or not. Can I can I guess somebody to build that for AI? Where I could just
[01:17:18.560 --> 01:17:23.520]   plug it into the thing and it'll tell me like this has been generated by such like, I need
[01:17:23.520 --> 01:17:27.440]   something right now because the fear again, I know it's real and I love it. I want it. I'm using
[01:17:27.440 --> 01:17:32.080]   it. I'm making some good things, but I'm just still a little scared. You know, the things we
[01:17:32.080 --> 01:17:38.320]   will do. Let me take a quick break. We got got so much to talk about. This is a fun AI is a great
[01:17:38.320 --> 01:17:43.200]   subject for a show I have to say. I mean, it really generates a lot of conversation. Do you think
[01:17:43.200 --> 01:17:49.440]   we should start a this week in AI show? Twi I probably got a knee one. Yeah,
[01:17:49.440 --> 01:17:56.320]   one one. It feels like good or bad. We've I mean, we're covering it all the time now and I think
[01:17:56.320 --> 01:18:01.520]   that that's going to be the way it is for some time to come. I feel like right. All right. Well,
[01:18:01.520 --> 01:18:05.920]   I'll talk to the boss. It's not got a great acronym. We might need a new name for it.
[01:18:05.920 --> 01:18:11.520]   T W I A I is not a. I I I
[01:18:11.520 --> 01:18:20.000]   20. Today. Our show today brought to you by eight sleep. Let me tell you folks. I first learned
[01:18:20.000 --> 01:18:26.560]   about eight sleep on this show more than a year ago. Kevin Rose was on and he told me about this
[01:18:26.560 --> 01:18:31.360]   thing. This mattress cover he puts on that heats and cools and he said, Oh, Leo, it's the best
[01:18:31.360 --> 01:18:36.960]   night sleep ever. And I said, yeah, yeah, sure, sure. But Amy Webb was on that show. She got it.
[01:18:36.960 --> 01:18:40.960]   Three months later, she says, Leo, I got the heat sleep because Kevin Rose recommended.
[01:18:40.960 --> 01:18:44.240]   It's the greatest thing ever. I said, yeah, yeah, finally, about a year ago.
[01:18:44.240 --> 01:18:48.320]   Lisa and I said, all right, we should try this thing. So we went out and got the eight sleep.
[01:18:48.320 --> 01:18:55.280]   Can I say it's the greatest thing ever to happen to your sleep? It's a it's the ultimate sleep machine.
[01:18:55.280 --> 01:19:00.960]   Sleep is the one, you know, at the beginning of every year, we all make all these health
[01:19:00.960 --> 01:19:05.280]   habit promises we're going to do. Well, you know what, a promise to get a better night's sleep is
[01:19:05.280 --> 01:19:10.960]   about the easiest thing to do in the world. And it sure makes a difference. Good sleep can reduce
[01:19:10.960 --> 01:19:16.720]   the likelihood of all sorts of serious health issues. It can even reduce the risk of Alzheimer's.
[01:19:16.720 --> 01:19:23.040]   Helps your blood pressure, makes your heart better. If you struggle to fall asleep, as I did,
[01:19:23.040 --> 01:19:26.880]   or you wake up in the middle of the night, or you're, or you're fighting with your partner
[01:19:26.880 --> 01:19:31.520]   over the thermostat, you got to get the eight sleep pod cover. They have a mattress too. But
[01:19:31.520 --> 01:19:35.440]   let me just tell you about the pod cover because over we liked our mattress. So it goes right over
[01:19:35.440 --> 01:19:41.040]   that mattress. But what it does is cool. The pod cover has dual zone temperature control.
[01:19:41.040 --> 01:19:47.440]   So Lisa likes it hot. I like it cooler. We could set either side of the bed to as cool as 55
[01:19:47.440 --> 01:19:53.760]   degrees. That's a nice fresh, that's almost brisk. Or as hot as 110 degrees. That's like
[01:19:53.760 --> 01:20:00.080]   summertime and Perth. It's hot or anywhere in between. But even better, it's got an autopilot
[01:20:00.080 --> 01:20:05.920]   feature that keeps track of how you're sleeping. It monitors heart rate, breathing, ups and downs.
[01:20:05.920 --> 01:20:10.320]   It even knows the temperature in the room as it goes up and down. And then it automatically adjusts
[01:20:10.320 --> 01:20:15.120]   the temperature of the bed so you're comfortable. It even does something kind of amazing. It slowly
[01:20:15.120 --> 01:20:19.440]   lowers the temperature as you get into deeper, deeper sleep to encourage that most important
[01:20:19.440 --> 01:20:24.080]   kind of sleep, the deep sleep. A couple hours a night is all you need. But having that couple
[01:20:24.080 --> 01:20:29.120]   hours is transformational. My deep sleep went from an hour, a night to an hour and a half a night
[01:20:29.120 --> 01:20:34.720]   when we started using the eight sleep. It's that good, as simple. It saves me money too. I don't
[01:20:34.720 --> 01:20:40.000]   have the thermostat on. I don't have the AC on in the summer. We don't need to because the bed is
[01:20:40.000 --> 01:20:44.720]   perfect. I'll have to tell you the only negative about the eight sleep. You'll never want to get
[01:20:44.720 --> 01:20:49.520]   out of bed because it's so cozy. I love getting in bed. It's nice and warm. It starts to cool off.
[01:20:49.520 --> 01:20:54.960]   I go into deeper sleep and then it heats up around eight in the morning and it's the best in class
[01:20:54.960 --> 01:21:00.080]   temperature regulation plus the best sensors to track health and sleep metrics. You don't need
[01:21:00.080 --> 01:21:04.320]   to wear anything. You don't even wear rings or watches or anything. It's all built in the cover.
[01:21:04.320 --> 01:21:10.880]   Better sleep. It's the health habit you will love sticking to night after night. And I tell you,
[01:21:10.880 --> 01:21:15.920]   it's so great to wake up feeling great. Throw the curtains, Oprah, say, hey, it's time for another
[01:21:15.920 --> 01:21:22.960]   beautiful day. I love that. Eight sleep.com/twit, $150 off on checkout at the pod cover.
[01:21:22.960 --> 01:21:30.080]   If you go to eight sleep.com/twit, $150, you'll save it. Check out on the pod cover. Eight sleep
[01:21:30.080 --> 01:21:36.000]   currently ships within the US, Canada, the UK, select countries in the EU. It even ships to Australia.
[01:21:36.000 --> 01:21:40.320]   I know you're having a hot summer right now in Australia. Can I tell you how wonderful it was
[01:21:40.320 --> 01:21:46.560]   last summer? We was very hot in Petaluma and we just we just adjusted knows what temperature it is.
[01:21:46.560 --> 01:21:51.520]   And it was just never wake up sweating at night again. You're just cool and comfortable all night
[01:21:51.520 --> 01:21:58.800]   long. You got to get this thing. eight sleep.com/twit. I apologize, Kevin and Amy. I should
[01:21:58.800 --> 01:22:07.440]   listen to you. Finally did eight sleep.com/twit. Thank you. Okay.
[01:22:07.440 --> 01:22:12.720]   It's kind of so me on the front. It's got to answer my question. He's got to make me feel
[01:22:12.720 --> 01:22:17.840]   that. Okay. What's your question? Actually, I'll answer I went in a second. But while you're
[01:22:17.840 --> 01:22:25.120]   doing that, I did ask chat GPT for some podcast names for AI. This doesn't quite fit into the
[01:22:25.120 --> 01:22:32.320]   Twitter. You know, I don't care if it's got to be good. So I asked it to make a witty and funny.
[01:22:32.320 --> 01:22:36.640]   And so we got the first one was artificially intelligent and definitely not evil.
[01:22:36.640 --> 01:22:44.800]   Which is pretty good. I can't believe that it's made for Google.
[01:22:47.440 --> 01:22:56.080]   The singularity is near. But first, let's laugh. Oh, nice. Then they have the Turing testosterone
[01:22:56.080 --> 01:23:05.520]   show. Oh, wow. Robo roasts. Robo roasts with machines. Okay. The comically intelligent podcast,
[01:23:05.520 --> 01:23:12.960]   the algorithmic antics hour artificially stupid artificial stupidity. The laughing
[01:23:12.960 --> 01:23:20.880]   neural network AI amusingly inconsistent. Silly singularity. Yeah. And that's kind of
[01:23:20.880 --> 01:23:25.840]   about it. I'm impressed that it was a little laugh at itself. That was chat GPT or that was Bing.
[01:23:25.840 --> 01:23:33.280]   Yeah. That was chat GPT. I'm impressed. I'm impressed. Isn't Bing chat GPT four or am I wrong on that?
[01:23:33.280 --> 01:23:38.320]   Isn't that the next? Well, they don't call it that. That's Prometheus. Yeah. Yeah. That's sort of
[01:23:38.320 --> 01:23:43.200]   the it's built on. They say next gen chat GPT technology. So but it's probably four point
[01:23:43.200 --> 01:23:48.160]   out to answer our own question quickly. Yeah, I agree 100%. And I think that's the next thing.
[01:23:48.160 --> 01:23:53.360]   And I know some companies are working on it. And you know, I know Google has that with the,
[01:23:53.360 --> 01:24:00.000]   well, I've got the name of it's like a pre bump where they're sort of like trying to get people
[01:24:00.000 --> 01:24:05.360]   to better understand news. But there will be technologies. And I guarantee you probably
[01:24:05.360 --> 01:24:12.800]   Microsoft will probably lead a lot of this where it detects AI generated content. And there is
[01:24:12.800 --> 01:24:18.080]   that technology out there. There is AI right now that can look at generated content and tell you
[01:24:18.080 --> 01:24:22.560]   it's AI. And so what's going to happen is you're going to start to see that implemented, I think,
[01:24:22.560 --> 01:24:28.240]   by a lot of companies. And when it comes to video, deep fakes, you go underneath that as well as
[01:24:28.240 --> 01:24:36.320]   audio. So you can also do things like you can you can watermark AI generated content so that if
[01:24:36.320 --> 01:24:42.800]   it is used, it's flagged by other software systems that maybe say, you know, like for schools, for
[01:24:42.800 --> 01:24:47.680]   instance, that would then block that content for being generated. There will probably always be a
[01:24:47.680 --> 01:24:52.080]   back and forth between this, of course, just like everything else in the world. But I think you're
[01:24:52.080 --> 01:24:56.880]   going to see that's the other side. You're going to see a lot of companies invest in and market
[01:24:56.880 --> 01:25:01.200]   technology that's used to detect AI and flag it for others because it is going to be super important.
[01:25:01.200 --> 01:25:07.040]   Do you think it's detectable? Or I bet you could have an undetectable AI. How would you detect it?
[01:25:07.040 --> 01:25:12.160]   Well, so some companies like, you know, like I was saying, like ethically, in this where
[01:25:12.160 --> 01:25:16.480]   Microsoft may take a lead in could watermark their own content. So you should identify it.
[01:25:16.480 --> 01:25:20.880]   You should. Yeah. So we ought to be there ought to be a law that would be a good law. Like for
[01:25:20.880 --> 01:25:26.400]   instance, yeah, you should have a thing on your screen that says Dan's eyes are artificially
[01:25:26.400 --> 01:25:27.840]   as focused on you.
[01:25:27.840 --> 01:25:36.560]   Our producer Jason Hales and Chet saying, do you notice that when he was reading those titles,
[01:25:36.560 --> 01:25:42.400]   he never stopped looking straight at us? And I freak. And now I feel like it's uncanny. I'm getting
[01:25:42.400 --> 01:25:46.960]   a little creeped out by the whole thing. Rewind folks. Watch. Watch the amazing.
[01:25:46.960 --> 01:25:51.200]   Yeah, that's Nvidia. By the way, Nvidia is definitely one of the winners in all this, right?
[01:25:51.200 --> 01:25:57.440]   They make. Oh, God. Yeah. They've been winning and anyone like powering the GPUs for it.
[01:25:57.440 --> 01:26:01.120]   There's starting to be some competition for Nvidia, but Nvidia is definitely a big winner
[01:26:01.120 --> 01:26:07.680]   and is being used by a lot to also answer the previous question. Art and video are much easier to
[01:26:07.680 --> 01:26:13.360]   check for than text for, I think some obvious reasons. So the watermark is already starting
[01:26:13.360 --> 01:26:17.840]   to be done and things like, I think stable diffusion is doing it. It's invisible to the human eye,
[01:26:17.840 --> 01:26:22.880]   but it's billions of pixels. You can put the watermark secretly. It's going to happen. It'll help detect
[01:26:22.880 --> 01:26:27.200]   it. There are probably some other ways to detect some patterns that most AIs will use for making
[01:26:27.200 --> 01:26:32.960]   art. Text is harder because you can just ask it to rewrite and you can't have a specific watermark
[01:26:32.960 --> 01:26:38.400]   in like the text itself. And that can just be rewritten by putting it through a different AI.
[01:26:38.400 --> 01:26:43.280]   You could get the most basic, easy stuff like if someone just very lazily asks
[01:26:43.280 --> 01:26:48.480]   Chatshi PT to write a five paragraph essay. But if you just, I have a friend, my friend Jason,
[01:26:48.480 --> 01:26:54.880]   he just rewrites the things in the tone of a snarky millennial and then the GPT detectors can't tell.
[01:26:54.880 --> 01:26:59.520]   So just say right in the tone of a snarky millennial and you're done.
[01:26:59.520 --> 01:27:05.120]   And you're done. And you're done. I will say though, and I haven't completely fully ready yet,
[01:27:05.120 --> 01:27:08.960]   but the New York Times does have an article saying how Chatshi PT could embed a watermark
[01:27:08.960 --> 01:27:12.400]   in the text that generates. I think that's a way to do it. That's a way to do it. Yeah.
[01:27:12.400 --> 01:27:17.520]   And I think you could legally mandate that. Look, Chicago has a law saying you can't use a
[01:27:17.520 --> 01:27:24.160]   biometric identification. In fact, that law is going to cost could cost white castles $17 billion.
[01:27:24.160 --> 01:27:35.040]   There's enough sliders in the world to pay a $17 billion fine. Chicago has had since 2008 a law
[01:27:35.040 --> 01:27:43.280]   against companies using biometric data without notifying you. Turns out white castle was fingerprinting
[01:27:43.280 --> 01:27:51.120]   employees in order to collect their check to to to log in like instead of punching a time card to
[01:27:51.120 --> 01:27:58.560]   use their fingerprint. And without asking permission, the Illinois Supreme Court on Friday said
[01:27:59.680 --> 01:28:05.760]   that in a four to three decision white castle was liable for every instance in which it scanned
[01:28:05.760 --> 01:28:11.440]   the fingerprints of its 9500 employees without their consent. And that was kind of one of the
[01:28:11.440 --> 01:28:15.520]   bones of contention white castle said, well, we'll pay a fine, but not for every instance.
[01:28:15.520 --> 01:28:22.320]   Well, they've been doing this for more than a decade. There's so many instances of it that
[01:28:22.320 --> 01:28:29.040]   it could add up to $17 billion. Well, white castles going to just end up,
[01:28:29.040 --> 01:28:33.040]   white castles going to end up so in Jumanji because it's Jumanji's fault because he was always
[01:28:33.040 --> 01:28:39.040]   signing Earl in on the time sheet. You see, you see, you know, you can't do something. No,
[01:28:39.040 --> 01:28:42.720]   no, tell the counter sued Jumanji get the money back. No, no, no, tell getting.
[01:28:42.720 --> 01:28:49.680]   So anyway, we'll see how judge obviously could could moderate that. Although I have to say,
[01:28:49.680 --> 01:28:53.920]   congratulations to the register, which did some math and said in order to raise $17 billion,
[01:28:53.920 --> 01:28:57.760]   white hassle, white castle would have to sell 23 billion sliders.
[01:28:57.760 --> 01:29:04.160]   There you go. I will eat six of them. I would be very sad if white castle went away. I like
[01:29:04.160 --> 01:29:10.400]   the I like the slider. You may say, well, last time you had white castle. Oh, it's been years,
[01:29:10.400 --> 01:29:15.840]   but let me tell you, I just drove by one yesterday. So jealous. I would stop every time.
[01:29:15.840 --> 01:29:20.480]   It's so greasy. It's so Jersey. So we got that Harold Akumar. I know. I'm so jealous. We don't
[01:29:20.480 --> 01:29:26.320]   have it out here. Can I take a moment to humble Bragg? I didn't go to white castle. I went to a
[01:29:26.320 --> 01:29:33.280]   famous kind of burger place in Boston that was on man versus food about 10 years ago. And I ate a
[01:29:33.280 --> 01:29:38.000]   two pound hamburger. Oh, God. Oh, wow. Nothing to beg about. That's a sign.
[01:29:38.000 --> 01:29:45.600]   And that wasn't that wasn't there. That's the small one. They have the contest is if you
[01:29:45.600 --> 01:29:51.680]   can eat a six pound burger and five pounds of fries. It has a 24 pieces of cheese.
[01:29:51.680 --> 01:29:58.400]   Why do people do that to them? No, I don't know. I'll say by two pound burger was delicious.
[01:29:58.400 --> 01:30:00.320]   Was that big? Yeah. Was that big?
[01:30:00.320 --> 01:30:03.680]   It's called Eagle's. It's called Eagle's deli.
[01:30:03.680 --> 01:30:06.720]   Eagles deli. Did you eat the whole burger? Did you take half of it home?
[01:30:06.720 --> 01:30:10.560]   No, you have to eat it right there. You can't. No, no taking it home.
[01:30:10.560 --> 01:30:13.600]   How many? How many is a cheese or on that burger?
[01:30:14.800 --> 01:30:15.440]   Eight pieces.
[01:30:15.440 --> 01:30:20.400]   Whoo. I got to be a slimmer. I feel slimmer already.
[01:30:20.400 --> 01:30:23.440]   I'm getting a further after I feel sexy, right?
[01:30:23.440 --> 01:30:25.200]   I'm getting really hungry.
[01:30:25.200 --> 01:30:30.640]   Wow. Eagles deli. I'm going to have to go next time in Brighton.
[01:30:30.640 --> 01:30:35.040]   And maybe my get my mom to order one of those.
[01:30:35.040 --> 01:30:41.280]   There you go. Yeah. Yeah. She's only 90. I think that would last her a month.
[01:30:41.280 --> 01:30:42.000]   Oh, jeez.
[01:30:43.440 --> 01:30:48.080]   Go back to the AI thing. I'm wondering if you can get this under copyright law.
[01:30:48.080 --> 01:30:54.480]   If you just make a generic entity that's called AI and that anything that's used by it
[01:30:54.480 --> 01:31:00.320]   is technically copyrighted. And so there'll be no money given to it, but you would be a violation
[01:31:00.320 --> 01:31:03.120]   of copyright if you were to publish that. If you got caught.
[01:31:03.120 --> 01:31:08.560]   If you used it inappropriately. Yeah. Interesting. But I'm still stuck on the giant.
[01:31:09.200 --> 01:31:13.920]   Here is somebody eating in it. There you go.
[01:31:13.920 --> 01:31:20.000]   11 pound burger. Five pounds of fries. It's insane. I didn't do the fries.
[01:31:20.000 --> 01:31:23.600]   I couldn't do the fries. Now we have to ask the question.
[01:31:23.600 --> 01:31:25.600]   What's the biggest thing you ever ate, Uncle Leo?
[01:31:25.600 --> 01:31:31.840]   God, that's a good question. Like a whole pizza.
[01:31:31.840 --> 01:31:37.680]   The biggest thing I ever ate. Yeah. A pizza, a clam and garlic pizza from Pepe's
[01:31:38.480 --> 01:31:43.520]   is about a yard in diameter. And I most of you.
[01:31:43.520 --> 01:31:44.880]   The whole thing is the whole thing.
[01:31:44.880 --> 01:31:46.000]   Okay. Most of right.
[01:31:46.000 --> 01:31:47.360]   Most of Ben, what about you, Ben?
[01:31:47.360 --> 01:31:49.200]   I regretted that the next morning on my dad.
[01:31:49.200 --> 01:31:52.400]   The biggest thing. I don't. It's probably some breakfast related things.
[01:31:52.400 --> 01:31:57.520]   I'm like, I think there was some ridiculous like like 12 egg omelette I once did.
[01:31:57.520 --> 01:31:58.000]   Wow.
[01:31:58.000 --> 01:32:02.080]   You know what? There should be like a badge on your Twitter that just like a giant hamburger.
[01:32:02.080 --> 01:32:03.360]   The largest thing I ever ate.
[01:32:03.920 --> 01:32:08.960]   Verification checkmark. You finished the like one of the giant food challenges.
[01:32:08.960 --> 01:32:12.560]   I guess we got to do the Twitter story. We haven't been talking about that, but we
[01:32:12.560 --> 01:32:18.400]   probably should. Twitter has decided among all of its weirdnesses to turn off
[01:32:18.400 --> 01:32:22.240]   SMS to factor unless you pay for it.
[01:32:22.240 --> 01:32:30.880]   And not just that either. It's also that meta announced today,
[01:32:31.840 --> 01:32:36.560]   which is Sunday for those listening that you can buy Instagram
[01:32:36.560 --> 01:32:38.080]   verification for 12 bucks.
[01:32:38.080 --> 01:32:39.200]   12 dollars.
[01:32:39.200 --> 01:32:42.240]   12 dollars a month. I'm sorry. A month.
[01:32:42.240 --> 01:32:43.680]   Insane.
[01:32:43.680 --> 01:32:46.240]   That's crazy.
[01:32:46.240 --> 01:32:47.200]   That's a mess thing.
[01:32:47.200 --> 01:32:49.840]   But that's for businesses, right? Meta verified.
[01:32:49.840 --> 01:32:50.720]   No, that's for a business.
[01:32:50.720 --> 01:32:51.920]   No, no, no.
[01:32:51.920 --> 01:32:53.600]   Business is allowed. Only individuals.
[01:32:53.600 --> 01:32:57.120]   It's literally says businesses cannot get the verification.
[01:32:57.120 --> 01:33:00.080]   It's only for individuals. You give them your, you show them your government ID.
[01:33:01.120 --> 01:33:04.560]   And then you can get a blue checkmark too on Instagram.
[01:33:04.560 --> 01:33:06.560]   What who told Mark Zuckerberg that?
[01:33:06.560 --> 01:33:08.960]   Yeah, you should do whatever Elon does. That's smart.
[01:33:08.960 --> 01:33:11.360]   And raise the price.
[01:33:11.360 --> 01:33:14.160]   And oh yeah, Elon's not charging enough.
[01:33:14.160 --> 01:33:16.480]   Charge a few bucks more.
[01:33:16.480 --> 01:33:20.480]   The SMS thing I almost understand just because
[01:33:20.480 --> 01:33:22.400]   must cost them money, right?
[01:33:22.400 --> 01:33:24.960]   It costs them money to send out the tax.
[01:33:24.960 --> 01:33:26.400]   And you know, it's one way to save.
[01:33:26.400 --> 01:33:30.160]   And I'd also argue an SMS to FA is one of the
[01:33:30.960 --> 01:33:33.120]   terrible insecure options.
[01:33:33.120 --> 01:33:34.960]   Yeah, everybody should be using an authenticator app.
[01:33:34.960 --> 01:33:38.320]   And apparently like something like 75 or 80%
[01:33:38.320 --> 01:33:41.280]   of people with two FA on Twitter,
[01:33:41.280 --> 01:33:45.760]   which is only like 2% of all users are using SMS version.
[01:33:45.760 --> 01:33:47.600]   So it does affect a lot of them,
[01:33:47.600 --> 01:33:50.000]   but they should be using authenticator app anyway.
[01:33:50.000 --> 01:33:51.760]   It's a more reliable and.
[01:33:51.760 --> 01:33:56.080]   So you have to keep you have to pay to be less secure is what you're saying.
[01:33:56.080 --> 01:33:58.400]   It's not less secure though.
[01:33:58.400 --> 01:33:59.040]   That's this.
[01:33:59.040 --> 01:33:59.520]   Yeah.
[01:33:59.520 --> 01:34:02.720]   Okay, I know that you're paying.
[01:34:02.720 --> 01:34:03.840]   You're paying.
[01:34:03.840 --> 01:34:04.320]   Wait a minute.
[01:34:04.320 --> 01:34:10.400]   You are paying in order to use the less secure form of two factor SMS.
[01:34:10.400 --> 01:34:13.200]   You're giving him money so that you can do that.
[01:34:13.200 --> 01:34:13.360]   Yeah.
[01:34:13.360 --> 01:34:15.760]   Paying to be less secure.
[01:34:15.760 --> 01:34:16.400]   Do something.
[01:34:16.400 --> 01:34:18.560]   It's less to use.
[01:34:18.560 --> 01:34:19.520]   Sure.
[01:34:19.520 --> 01:34:20.400]   It's less to use.
[01:34:20.400 --> 01:34:20.960]   Are they?
[01:34:20.960 --> 01:34:22.240]   But I think he understands.
[01:34:22.240 --> 01:34:25.120]   I think Elon knows just like you said, Daniel,
[01:34:25.120 --> 01:34:27.680]   80% of these users don't know anything about it.
[01:34:27.680 --> 01:34:30.160]   I think they're just going to give them a buck.
[01:34:30.160 --> 01:34:35.280]   Well, it's eight bucks, but give them eight bucks to get the SMS.
[01:34:35.280 --> 01:34:37.760]   That's why they have us on the internet.
[01:34:37.760 --> 01:34:39.040]   There's a lot of people out of town.
[01:34:39.040 --> 01:34:41.120]   Again, I just don't understand upper about this.
[01:34:41.120 --> 01:34:44.000]   I'm like, we just give them a lot of breeze and we're just getting mad.
[01:34:44.000 --> 01:34:46.960]   I'm like, this is not that big of a deal that everybody.
[01:34:46.960 --> 01:34:48.240]   No, it's not a huge deal.
[01:34:48.240 --> 01:34:49.200]   It's just bizarre.
[01:34:49.200 --> 01:34:50.800]   So it's bizarre.
[01:34:50.800 --> 01:34:52.960]   It kind of makes sense.
[01:34:52.960 --> 01:34:56.640]   SMS is outdated and old and get your number gets booed.
[01:34:56.640 --> 01:34:57.600]   People have taken steal.
[01:34:57.600 --> 01:34:58.480]   It's not a good thing.
[01:34:58.480 --> 01:35:00.800]   Now, somebody in the chair and said, this is interesting.
[01:35:00.800 --> 01:35:01.680]   I don't know if this is true.
[01:35:01.680 --> 01:35:07.520]   Mobile companies are botting SMS so that Twitter will have to pay more to Twilio.
[01:35:07.520 --> 01:35:14.480]   That's, I guess, Twilio's the system that they and everyone else uses.
[01:35:14.480 --> 01:35:14.560]   Right.
[01:35:14.560 --> 01:35:15.680]   Remember you used Twilio.
[01:35:15.680 --> 01:35:18.960]   And you got to pay, you know, some fraction of us.
[01:35:18.960 --> 01:35:19.280]   The irony is.
[01:35:19.280 --> 01:35:19.760]   The irony is.
[01:35:19.760 --> 01:35:20.640]   The message sent.
[01:35:20.640 --> 01:35:22.320]   Twilio offers Authy.
[01:35:22.320 --> 01:35:23.520]   It makes Authy for free.
[01:35:23.520 --> 01:35:27.360]   So yeah, I don't get it.
[01:35:27.360 --> 01:35:27.680]   Yeah.
[01:35:27.680 --> 01:35:29.040]   We're all be living.
[01:35:29.040 --> 01:35:32.160]   And to the Zuckerberg thing, again, I.
[01:35:32.160 --> 01:35:34.800]   Why, bro?
[01:35:34.800 --> 01:35:35.680]   Like why?
[01:35:35.680 --> 01:35:37.440]   At least like, so I'm on Twitter blue.
[01:35:37.440 --> 01:35:38.720]   I've been on Twitter blue forever.
[01:35:38.720 --> 01:35:39.440]   And really.
[01:35:39.440 --> 01:35:40.480]   He's like paying for that.
[01:35:40.480 --> 01:35:42.000]   You're keeping for that?
[01:35:42.000 --> 01:35:42.240]   Yes.
[01:35:42.240 --> 01:35:45.120]   Again, Uncle Leo, let me tell you something.
[01:35:45.120 --> 01:35:46.320]   I don't want to just hold on here.
[01:35:46.320 --> 01:35:46.960]   You all the time.
[01:35:46.960 --> 01:35:51.360]   I listen, I hear you all the time and I listen all the time.
[01:35:51.360 --> 01:35:54.160]   And most of the conversation sounds like people saying,
[01:35:54.160 --> 01:35:56.800]   if Donald Trump becomes president, I'm leaving America.
[01:35:56.800 --> 01:35:59.520]   Yeah, 3% of rich people went down to Puerto Rico
[01:35:59.520 --> 01:36:01.040]   and took over that island and left.
[01:36:01.040 --> 01:36:03.600]   But everybody else stayed in America, went to their jobs,
[01:36:03.600 --> 01:36:04.720]   and did what they got to do.
[01:36:04.720 --> 01:36:06.240]   Twitter is useful to me.
[01:36:06.240 --> 01:36:07.280]   It's my main audience.
[01:36:07.280 --> 01:36:09.440]   People click my links and I guess I'll stand there.
[01:36:09.440 --> 01:36:13.120]   And yes, I give them 11 bucks because I can put 10 minute videos on there.
[01:36:13.120 --> 01:36:14.400]   Like I'm doing right now.
[01:36:14.400 --> 01:36:16.800]   America is terrible, but it's still good.
[01:36:16.800 --> 01:36:18.240]   That's why they got my money.
[01:36:18.240 --> 01:36:19.440]   The blue check doesn't matter.
[01:36:19.440 --> 01:36:20.400]   We know it doesn't matter.
[01:36:20.400 --> 01:36:22.400]   But yes, nobody's left Twitter.
[01:36:22.400 --> 01:36:23.280]   Everybody's still there.
[01:36:23.280 --> 01:36:26.000]   I know all three of you are still using it.
[01:36:26.000 --> 01:36:29.200]   I don't believe America is terrible, but it's still better.
[01:36:29.200 --> 01:36:29.840]   Is that what is that?
[01:36:29.840 --> 01:36:31.520]   Nobody's left answer.
[01:36:31.520 --> 01:36:33.760]   Twitter, Twitter usage is down.
[01:36:33.760 --> 01:36:35.520]   If you look statistically though, it is.
[01:36:35.520 --> 01:36:36.720]   Adreva is down.
[01:36:36.720 --> 01:36:40.960]   Adreva is down, but users, it's not going away.
[01:36:40.960 --> 01:36:43.280]   And I love Twitter as a product.
[01:36:43.280 --> 01:36:44.560]   I really need to be first.
[01:36:44.560 --> 01:36:46.960]   Let me ask you guys, because all three of you are on Twitter.
[01:36:46.960 --> 01:36:47.760]   I am not.
[01:36:48.480 --> 01:36:51.440]   I mean, I have an account, so nobody else do my name, but I'm not using it.
[01:36:51.440 --> 01:36:53.840]   Did you see a lot of Elon earlier in the week?
[01:36:53.840 --> 01:36:59.120]   Oh, I didn't buy, I hacked my own Twitter.
[01:36:59.120 --> 01:37:02.000]   There's a plugin called control panel for Twitter.
[01:37:02.000 --> 01:37:02.560]   Oh.
[01:37:02.560 --> 01:37:03.600]   You can run on the web.
[01:37:03.600 --> 01:37:10.160]   And so when I run that on the web, and then I use a, I create a PWA out of Twitter.
[01:37:10.160 --> 01:37:11.760]   And so that's my app on my PC.
[01:37:11.760 --> 01:37:15.440]   So when I look at 90% of the time, except when I'm not on my phone.
[01:37:15.440 --> 01:37:17.520]   And I guess this is also on iOS.
[01:37:17.520 --> 01:37:20.560]   So you can actually just, there's an app you can download from 599.
[01:37:20.560 --> 01:37:21.520]   That's built off of this.
[01:37:21.520 --> 01:37:22.480]   And what it does is that-
[01:37:22.480 --> 01:37:24.480]   Would you see the same thing you see on the web?
[01:37:24.480 --> 01:37:31.440]   No, so what it does is it, it's basically hacks Twitter, and it gives you just your
[01:37:31.440 --> 01:37:33.520]   chronological timeline of people you'd follow.
[01:37:33.520 --> 01:37:34.320]   And that's it.
[01:37:34.320 --> 01:37:36.080]   In fact, I can hide retweets.
[01:37:36.080 --> 01:37:38.000]   So I've retweeted in a whole separate column.
[01:37:38.000 --> 01:37:39.040]   So I don't even get to see those.
[01:37:39.040 --> 01:37:41.360]   I just see original content of people posted.
[01:37:41.360 --> 01:37:45.120]   And for people who have Twitter, blue, or paying for it, it changes their icon to the
[01:37:45.120 --> 01:37:48.080]   little symbol and it's not a checkmark, it's a little bird.
[01:37:48.080 --> 01:37:48.880]   So I even know-
[01:37:48.880 --> 01:37:50.640]   Who's there for now?
[01:37:50.640 --> 01:37:52.240]   Oh, the checkmark isn't it anymore?
[01:37:52.240 --> 01:37:53.440]   It's just, it's a bird, huh?
[01:37:53.440 --> 01:37:55.120]   Yeah.
[01:37:55.120 --> 01:37:59.120]   So like you can, but for verified people who are made of this checkmark,
[01:37:59.120 --> 01:38:02.080]   there's tons of little hacks and you can hide everything.
[01:38:02.080 --> 01:38:04.240]   So it's, you could just download that.
[01:38:04.240 --> 01:38:07.280]   And that's for people on the web, obviously, but I use, that's how I use Twitter,
[01:38:07.280 --> 01:38:08.880]   as I use it as a PWA.
[01:38:08.880 --> 01:38:12.000]   And so the story was, so you don't have a 4U tab.
[01:38:12.000 --> 01:38:13.600]   That's why you didn't see a bunch of Elon.
[01:38:13.600 --> 01:38:13.760]   Right.
[01:38:14.560 --> 01:38:19.040]   If you blocked Elon, I don't think you would see it, but the 4U tab on,
[01:38:19.040 --> 01:38:22.880]   so either story was, we've, we've talked about before, I'll do it quickly.
[01:38:22.880 --> 01:38:28.640]   Elon tweeted at the Super Bowl on Sunday, but his tweet didn't get as much engagement as
[01:38:28.640 --> 01:38:30.240]   President Biden's tweet.
[01:38:30.240 --> 01:38:36.880]   So Elon flew home in a, in a rage from the Super Bowl, calls his cousin James and says,
[01:38:36.880 --> 01:38:38.000]   James got to fix this.
[01:38:38.000 --> 01:38:42.800]   James sends out a 2.30 AM slack to anybody still up.
[01:38:43.600 --> 01:38:44.960]   There's a problem.
[01:38:44.960 --> 01:38:47.760]   Biden got more engagement than Elon, fix it.
[01:38:47.760 --> 01:38:49.440]   So they did.
[01:38:49.440 --> 01:38:53.280]   They made sure that all of Elon's tweets showed up with the 4U tab.
[01:38:53.280 --> 01:38:57.200]   And most of the day, Monday, all you saw was Elon Musk tweets.
[01:38:57.200 --> 01:38:58.640]   They fixed that though, right?
[01:38:58.640 --> 01:39:00.800]   I'm just curious if you guys saw it.
[01:39:00.800 --> 01:39:04.160]   I saw more of the retweets.
[01:39:04.160 --> 01:39:05.920]   Okay, go on.
[01:39:05.920 --> 01:39:06.480]   Oh, sorry.
[01:39:06.480 --> 01:39:11.760]   I saw more of the retweets of him fake pouring milk down with the Elon Twitter.
[01:39:11.760 --> 01:39:13.280]   That was the worst.
[01:39:13.280 --> 01:39:14.080]   It's just it.
[01:39:14.080 --> 01:39:15.840]   It I see Elon.
[01:39:15.840 --> 01:39:17.040]   I'm like, I see Kanye.
[01:39:17.040 --> 01:39:19.280]   He says something stupid and everybody makes fun of him.
[01:39:19.280 --> 01:39:21.200]   And I have to see it in 42 posts.
[01:39:21.200 --> 01:39:23.120]   Otherwise, I forget the man exists.
[01:39:23.120 --> 01:39:24.480]   Oh, you only see the retweets.
[01:39:24.480 --> 01:39:26.560]   You don't see his originals.
[01:39:26.560 --> 01:39:26.720]   Yeah.
[01:39:26.720 --> 01:39:30.000]   Did you see a lot of Elon on Monday?
[01:39:30.000 --> 01:39:33.360]   I think I was not on Twitter on Monday.
[01:39:33.360 --> 01:39:33.600]   Okay.
[01:39:33.600 --> 01:39:36.240]   Hopefully I was doing real life work.
[01:39:36.240 --> 01:39:37.600]   What I think I was like,
[01:39:37.600 --> 01:39:42.240]   Gras touching grass going to see my girlfriend's play.
[01:39:42.240 --> 01:39:44.320]   You know, things like things like that.
[01:39:44.320 --> 01:39:47.920]   Yeah, he also fired somebody over it too.
[01:39:47.920 --> 01:39:49.680]   Well, this was earlier that week.
[01:39:49.680 --> 01:39:51.680]   Yeah, earlier that week he had a.
[01:39:51.680 --> 01:39:55.920]   This is according to the platformer, Casey Newton and Zoe's Shiffer.
[01:39:55.920 --> 01:39:59.920]   Earlier that week, he had a meeting with engineers saying,
[01:39:59.920 --> 01:40:02.160]   why is my engagement solo?
[01:40:02.160 --> 01:40:06.560]   To which a foolish engineer pulled out the Google search trends
[01:40:06.560 --> 01:40:11.680]   from a year ago and currently and said, well, you're down 10% search trends as well.
[01:40:11.680 --> 01:40:13.920]   To which Elon's response was, you're fired.
[01:40:13.920 --> 01:40:14.560]   You're out of here.
[01:40:14.560 --> 01:40:15.200]   Yep.
[01:40:15.200 --> 01:40:15.760]   You're gone.
[01:40:15.760 --> 01:40:18.000]   He was one of the two people.
[01:40:18.000 --> 01:40:21.840]   There were only two directors, I guess, in charge of the product.
[01:40:21.840 --> 01:40:22.800]   And he was one of the two.
[01:40:22.800 --> 01:40:25.840]   So they've got cut the staff in half, which saves money.
[01:40:25.840 --> 01:40:28.480]   I don't know.
[01:40:28.480 --> 01:40:29.040]   All right.
[01:40:29.040 --> 01:40:30.400]   You know, you guys love Twitter.
[01:40:30.400 --> 01:40:30.960]   I'm not.
[01:40:30.960 --> 01:40:31.760]   That's fine.
[01:40:31.760 --> 01:40:32.720]   I don't have a problem with it.
[01:40:32.720 --> 01:40:36.080]   I don't love it, but again, it's like America.
[01:40:36.080 --> 01:40:37.120]   I don't love it.
[01:40:37.120 --> 01:40:40.000]   I live here and I use it for my advantages.
[01:40:40.000 --> 01:40:41.200]   I live here.
[01:40:41.200 --> 01:40:44.720]   I'm not out of my chest USA lately, but I'm saying I live here.
[01:40:44.720 --> 01:40:45.600]   Oh, that's interesting.
[01:40:45.600 --> 01:40:46.560]   You know, it's one.
[01:40:46.560 --> 01:40:47.280]   So it's not.
[01:40:47.280 --> 01:40:50.960]   I want to say all my content on Twitter gets automatically
[01:40:50.960 --> 01:40:52.640]   reposted to my mastodon.
[01:40:52.640 --> 01:40:54.080]   So I'm on Mastodon too.
[01:40:54.080 --> 01:40:56.400]   And I reply to people on Mastodon and I use that.
[01:40:56.400 --> 01:40:56.880]   What's your answer?
[01:40:56.880 --> 01:40:59.360]   So your Mastodon is at Daniel Rubino.
[01:40:59.360 --> 01:41:02.480]   It's a well, it's Mastodon.
[01:41:02.480 --> 01:41:04.080]   It's the Mastodon complicated thing.
[01:41:04.080 --> 01:41:05.600]   This is why I don't use Mastodon.
[01:41:05.600 --> 01:41:07.600]   Bingo, bingo.
[01:41:07.600 --> 01:41:08.240]   I can't.
[01:41:08.240 --> 01:41:09.840]   There's no alternative.
[01:41:09.840 --> 01:41:10.640]   We'll talk.
[01:41:10.640 --> 01:41:14.800]   I'm sure about Dobe and the ways founders do post news,
[01:41:14.800 --> 01:41:17.360]   which I am on, but I am using three things a lot more now
[01:41:17.360 --> 01:41:18.800]   and I am using Twitter a lot less.
[01:41:18.800 --> 01:41:21.600]   I am using LinkedIn, but also they just they'd be one of their
[01:41:21.600 --> 01:41:24.800]   creator program, which is a fun program.
[01:41:24.800 --> 01:41:30.160]   TikTok because a parent that that sweet, sweet viral juice
[01:41:30.160 --> 01:41:33.200]   and substac because I would like to own my own distribution.
[01:41:33.200 --> 01:41:34.480]   And that has been awesome.
[01:41:34.480 --> 01:41:35.520]   We'll talk about that in a second
[01:41:35.520 --> 01:41:37.200]   because you do have a new substac newsletter.
[01:41:37.200 --> 01:41:38.960]   So I know it's really hard to use Mastodon.
[01:41:38.960 --> 01:41:40.480]   So I just went to Mastodon.
[01:41:40.480 --> 01:41:43.280]   I typed in at Daniel underscore Rubino.
[01:41:43.280 --> 01:41:44.560]   I got two accounts.
[01:41:44.560 --> 01:41:46.800]   One is, which one should I follow?
[01:41:46.800 --> 01:41:47.280]   Yeah.
[01:41:47.280 --> 01:41:51.040]   I'm already following one of them, which the new Nuzzi dot social.
[01:41:51.040 --> 01:41:51.520]   Okay.
[01:41:51.520 --> 01:41:52.960]   So that was so hard.
[01:41:52.960 --> 01:41:54.560]   I don't blame you.
[01:41:54.560 --> 01:41:55.760]   Why would you want to do that?
[01:41:55.760 --> 01:41:58.320]   That seems like a very difficult thing to do.
[01:41:58.320 --> 01:41:59.760]   I do like Mastodon though.
[01:41:59.760 --> 01:42:02.400]   Like the conversation on there and it's hard.
[01:42:02.400 --> 01:42:03.040]   It's not that hard.
[01:42:03.040 --> 01:42:03.600]   I get a lot more.
[01:42:03.600 --> 01:42:03.680]   I'm just saying.
[01:42:03.680 --> 01:42:03.920]   Yeah.
[01:42:03.920 --> 01:42:06.240]   I mean, I have a dog in this home.
[01:42:06.240 --> 01:42:07.440]   We run a Mastodon instead.
[01:42:07.440 --> 01:42:08.720]   Since I admit, but okay.
[01:42:08.720 --> 01:42:09.200]   Yeah.
[01:42:09.200 --> 01:42:12.160]   You type in at Daniel underscore Rubino and you find him
[01:42:12.160 --> 01:42:14.480]   and then you follow him and that was not that hard.
[01:42:14.480 --> 01:42:16.800]   All right.
[01:42:16.800 --> 01:42:17.520]   All right.
[01:42:17.520 --> 01:42:19.680]   Let's talk about this article.
[01:42:19.680 --> 01:42:22.880]   I mentioned Noam Bardeen who a year or two ago
[01:42:22.880 --> 01:42:24.960]   wrote a goodbye to Google.
[01:42:24.960 --> 01:42:27.360]   He was a founder of Waze said,
[01:42:27.360 --> 01:42:29.440]   "This place nobody wants to work here.
[01:42:29.440 --> 01:42:32.720]   The newest one, Praveen Sashradi,
[01:42:32.720 --> 01:42:34.080]   Tashashadri."
[01:42:34.080 --> 01:42:34.480]   Sorry.
[01:42:34.480 --> 01:42:35.280]   Got the name wrong?
[01:42:35.280 --> 01:42:40.720]   Shashadri who joined Google because he had co-founded a company,
[01:42:40.720 --> 01:42:42.240]   AppSheet, which Google acquired.
[01:42:42.240 --> 01:42:49.040]   He also stayed three years and one day and has left Google
[01:42:49.040 --> 01:42:51.280]   publishing a goodbye on Medium.
[01:42:51.280 --> 01:42:53.520]   The maze is in the mouse.
[01:42:53.520 --> 01:42:56.480]   What ails Google and how can it turn things around?
[01:42:56.480 --> 01:42:59.040]   His I'll summarize because it's a long article,
[01:42:59.040 --> 01:43:02.400]   but essentially he says it's become a bureaucratic nightmare.
[01:43:02.960 --> 01:43:08.640]   Google has 175,000 plus capable and well-compensated employees
[01:43:08.640 --> 01:43:12.320]   who get very little done quarter over quarter year over year
[01:43:12.320 --> 01:43:13.280]   like mice.
[01:43:13.280 --> 01:43:16.800]   They're trapped in a maze of approvals, launch processes,
[01:43:16.800 --> 01:43:19.360]   legal reviews, performance reviews, exec reviews,
[01:43:19.360 --> 01:43:22.480]   documents, meetings, bug reports, triage, OKRs,
[01:43:22.480 --> 01:43:25.440]   H1 plants followed by H2 plants, all hand summits,
[01:43:25.440 --> 01:43:27.280]   and inevitable reorgs.
[01:43:27.280 --> 01:43:30.720]   The mice are regularly fed their cheese, promotions, bonuses,
[01:43:30.720 --> 01:43:34.320]   fancy food, fancier perks, and despite many wanting
[01:43:34.320 --> 01:43:37.680]   to experience personal satisfaction and impact from their work,
[01:43:37.680 --> 01:43:41.120]   the system trains them to quell these inappropriate desires
[01:43:41.120 --> 01:43:44.320]   and learn what it actually means to be googly,
[01:43:44.320 --> 01:43:46.960]   which is just don't rock the boat.
[01:43:46.960 --> 01:43:49.680]   His position, I think is probably accurate,
[01:43:49.680 --> 01:43:51.520]   is that Google makes so much money from search.
[01:43:51.520 --> 01:43:54.640]   They don't want to take any risks anywhere else.
[01:43:54.640 --> 01:43:58.800]   And risk aversion, Ben, you're a startup guy.
[01:43:58.800 --> 01:44:01.440]   Is it a good thing for a startup to be risk adverse?
[01:44:01.440 --> 01:44:06.080]   That's how you die if you're risk adverse.
[01:44:06.080 --> 01:44:08.560]   You can't do a startup if you're risk adverse.
[01:44:08.560 --> 01:44:09.520]   You stay at Google.
[01:44:09.520 --> 01:44:12.720]   You don't die if you can buy.
[01:44:12.720 --> 01:44:13.600]   Yeah.
[01:44:13.600 --> 01:44:17.840]   It's actually that is a good point though,
[01:44:17.840 --> 01:44:20.640]   because Amy Hood during, again, the investor call
[01:44:20.640 --> 01:44:23.840]   when talking about being in their four-way into search.
[01:44:23.840 --> 01:44:25.360]   She's a CFO at Microsoft.
[01:44:25.360 --> 01:44:26.880]   Yeah, yeah.
[01:44:26.880 --> 01:44:30.000]   And she said their advantage is the fact
[01:44:30.000 --> 01:44:31.040]   that they actually have small,
[01:44:31.040 --> 01:44:32.480]   not a lot of market share right now.
[01:44:32.480 --> 01:44:36.640]   She says, well, we can take big risks and we're going to pivot
[01:44:36.640 --> 01:44:39.440]   and innovate very quickly because we can.
[01:44:39.440 --> 01:44:41.840]   And that's a really good point because,
[01:44:41.840 --> 01:44:44.720]   I mean, so Microsoft screws up a little bit in search.
[01:44:44.720 --> 01:44:50.720]   But if Google screws up in search, yeah, it's a big deal.
[01:44:50.720 --> 01:44:52.400]   It really affects the bottom line of the company
[01:44:52.400 --> 01:44:55.040]   because Google for all intents and purposes
[01:44:55.040 --> 01:44:57.040]   is an advertising company at its core.
[01:44:57.040 --> 01:44:58.560]   So it's interesting because Shashadri
[01:44:58.560 --> 01:45:00.960]   used to work, he worked for years at Microsoft
[01:45:00.960 --> 01:45:04.480]   and he says, Microsoft, at the end of his article,
[01:45:04.480 --> 01:45:06.320]   Microsoft managed to turn things around,
[01:45:06.320 --> 01:45:10.560]   but it required exceptional leadership and good fortune.
[01:45:10.560 --> 01:45:14.560]   Yeah, I feel like there's an under, absolutely.
[01:45:14.560 --> 01:45:14.800]   Yeah.
[01:45:14.800 --> 01:45:16.640]   There's an undercurrent to that article
[01:45:16.640 --> 01:45:21.680]   which is probably that, and is that
[01:45:23.200 --> 01:45:25.840]   Microsoft really turned around when Satya Nadella came in.
[01:45:25.840 --> 01:45:29.360]   I think there's an undercurrent of what he's trying to say
[01:45:29.360 --> 01:45:31.040]   about Google and its current leadership.
[01:45:31.040 --> 01:45:34.640]   I will say that their founders of Google
[01:45:34.640 --> 01:45:36.720]   are more involved again and that's been a story
[01:45:36.720 --> 01:45:38.720]   that's been coming up and there's a reason for that.
[01:45:38.720 --> 01:45:42.400]   I think Google's going to go through a large transformation,
[01:45:42.400 --> 01:45:43.520]   what that exactly looks like.
[01:45:43.520 --> 01:45:46.640]   I don't know, but they have to in the current moment.
[01:45:46.640 --> 01:45:50.240]   Even with Meta and Zuckerberg,
[01:45:50.240 --> 01:45:53.200]   he's intending on making a bigger transformation.
[01:45:53.200 --> 01:45:57.280]   He's flattening the layers, he's telling a bunch of managers
[01:45:57.280 --> 01:45:59.840]   and directors that they need to become individual contributors
[01:45:59.840 --> 01:46:00.560]   to get out.
[01:46:00.560 --> 01:46:03.840]   That's what he wants to not end up in the position
[01:46:03.840 --> 01:46:06.480]   where there's multiple layers and it's this article.
[01:46:06.480 --> 01:46:08.960]   Interesting.
[01:46:08.960 --> 01:46:11.440]   Yeah, I mean, basically that's what Shashadri says,
[01:46:11.440 --> 01:46:15.120]   is it's not too late for Google, but it needs to transform.
[01:46:15.120 --> 01:46:18.800]   And I think the subtext is, it's not going to happen
[01:46:18.800 --> 01:46:19.840]   under Sundar Pichai.
[01:46:19.840 --> 01:46:23.200]   He is a bureaucrat through and through
[01:46:23.200 --> 01:46:25.680]   and that's not what you need at this time.
[01:46:25.680 --> 01:46:28.480]   You think Larry and Sergey, obviously they weren't risk
[01:46:28.480 --> 01:46:30.480]   adverse when they started Google.
[01:46:30.480 --> 01:46:33.440]   Do you think they can come back and whip it into shape?
[01:46:33.440 --> 01:46:36.880]   How long before they parked company with Sundar Pichai?
[01:46:36.880 --> 01:46:39.600]   So one thing that they could do,
[01:46:39.600 --> 01:46:41.280]   just before the smart guys get in,
[01:46:41.280 --> 01:46:44.720]   the one thing that they could do is charge $11.99
[01:46:44.720 --> 01:46:45.840]   and give me a new check.
[01:46:45.840 --> 01:46:48.720]   You guys have the floor.
[01:46:48.720 --> 01:46:50.720]   Take it off, say something smart.
[01:46:50.720 --> 01:46:52.800]   I took the glasses off Ben, so it wasn't you talking.
[01:46:52.800 --> 01:46:53.600]   Risk aversion.
[01:46:53.600 --> 01:46:55.840]   I mean, it's a green check anytime I'm on Google Meet.
[01:46:55.840 --> 01:46:57.040]   Risk aversion is interesting.
[01:46:57.040 --> 01:47:00.640]   I mean, look at clearly Elon Musk is not risk adverse, right?
[01:47:00.640 --> 01:47:02.160]   He is, he's a lord now.
[01:47:02.160 --> 01:47:03.440]   He loves taking a risk.
[01:47:03.440 --> 01:47:05.520]   There is too much.
[01:47:05.520 --> 01:47:06.800]   You could have too much risk.
[01:47:06.800 --> 01:47:07.920]   You could have too much risk.
[01:47:07.920 --> 01:47:10.800]   But it depends on like, look, on size and stage,
[01:47:10.800 --> 01:47:12.160]   you're super big.
[01:47:12.160 --> 01:47:12.960]   Like it makes sense.
[01:47:12.960 --> 01:47:14.800]   You gotta be a little bit more risk adverse
[01:47:14.800 --> 01:47:16.880]   or like risk a whole bunch of your money.
[01:47:16.880 --> 01:47:20.320]   And Google has tried to launch other new things
[01:47:20.320 --> 01:47:25.440]   through like the alphabet structure of like other pets and all that.
[01:47:25.440 --> 01:47:28.000]   But none of those bets are super big yet.
[01:47:28.000 --> 01:47:29.920]   Me, some people know Waymo.
[01:47:29.920 --> 01:47:31.440]   There's like the life sciences one.
[01:47:31.440 --> 01:47:33.520]   But they've killed a lot of that, by the way.
[01:47:33.520 --> 01:47:36.480]   They fired almost everybody from the area 120.
[01:47:36.480 --> 01:47:39.840]   They're slowly killing those bets
[01:47:39.840 --> 01:47:41.840]   because Ruth Porat, their CFO,
[01:47:41.840 --> 01:47:44.560]   is so risk averse that she's just killing anything
[01:47:44.560 --> 01:47:45.440]   that doesn't make money.
[01:47:46.640 --> 01:47:49.920]   This will be the most consequential year for Google.
[01:47:49.920 --> 01:47:51.120]   Yeah, I agree.
[01:47:51.120 --> 01:47:52.800]   Since it's launch.
[01:47:52.800 --> 01:47:55.520]   I agree. That's a good way to put it.
[01:47:55.520 --> 01:47:56.240]   Yeah.
[01:47:56.240 --> 01:47:56.800]   Yeah.
[01:47:56.800 --> 01:47:58.960]   How they handle it determines everything.
[01:47:58.960 --> 01:48:03.760]   And they've already lost $100 billion just by being second-ish.
[01:48:03.760 --> 01:48:07.360]   So they're kind of damned if they do, damned if they don't.
[01:48:07.360 --> 01:48:08.960]   But they better do something.
[01:48:08.960 --> 01:48:13.360]   This is, I thought, the most interesting part of the article.
[01:48:13.360 --> 01:48:16.720]   The way I see it, Google has four core cultural problems.
[01:48:16.720 --> 01:48:19.200]   They're all the natural consequences
[01:48:19.200 --> 01:48:21.840]   of having a money printing machine called ads
[01:48:21.840 --> 01:48:24.720]   that has kept growing relentlessly every year,
[01:48:24.720 --> 01:48:26.400]   hiding all other sins.
[01:48:26.400 --> 01:48:29.520]   And the sins he talks about are no mission,
[01:48:29.520 --> 01:48:33.680]   no urgency, delusions of exceptionalism.
[01:48:33.680 --> 01:48:36.320]   I've seen that so often in Silicon Valley, where you go,
[01:48:36.320 --> 01:48:37.600]   "Hey, where the kings were in top."
[01:48:37.600 --> 01:48:41.440]   I remember I had friends at Atari in the '80s who said,
[01:48:41.440 --> 01:48:42.720]   "Hey, where are the kings?"
[01:48:42.720 --> 01:48:44.480]   Right? We can't lose.
[01:48:44.480 --> 01:48:45.760]   And then finally, mismanagement.
[01:48:45.760 --> 01:48:47.840]   He says, "Unfortunately, this is not my first experience
[01:48:47.840 --> 01:48:51.280]   watching the gradual decay of a dominant empire.
[01:48:51.280 --> 01:48:52.960]   I lived through more than a decade,
[01:48:52.960 --> 01:48:56.160]   1999 through 2011, at Microsoft,
[01:48:56.160 --> 01:48:58.240]   as it slowly degraded and lost its way."
[01:48:58.240 --> 01:48:58.480]   Right?
[01:48:58.480 --> 01:48:59.600]   Yeah.
[01:48:59.600 --> 01:49:00.560]   Google has a few strengths.
[01:49:00.560 --> 01:49:03.120]   Microsoft didn't have as it tried to recover.
[01:49:03.120 --> 01:49:05.120]   It isn't a culture of ego and fiefdoms.
[01:49:05.120 --> 01:49:06.800]   That was the problem at Microsoft.
[01:49:06.800 --> 01:49:09.680]   Man who cornet's famous picture of everybody shooting at each other.
[01:49:09.680 --> 01:49:10.400]   Right.
[01:49:10.400 --> 01:49:13.840]   The environmental, the environment values introspection at Google.
[01:49:13.840 --> 01:49:16.960]   The stated core values of the company are rock salad,
[01:49:16.960 --> 01:49:19.360]   and there's still immense respect for Google in the external world.
[01:49:19.360 --> 01:49:20.480]   There is hope for Google.
[01:49:20.480 --> 01:49:24.960]   But don't wait too long.
[01:49:24.960 --> 01:49:25.920]   I like what you said.
[01:49:25.920 --> 01:49:26.960]   I think that's right on.
[01:49:26.960 --> 01:49:28.080]   This is the year, isn't it?
[01:49:28.080 --> 01:49:30.320]   Yes.
[01:49:30.320 --> 01:49:31.360]   It just is.
[01:49:31.360 --> 01:49:31.920]   We will see.
[01:49:31.920 --> 01:49:32.480]   Yeah.
[01:49:32.480 --> 01:49:33.200]   We'll be covering it.
[01:49:33.200 --> 01:49:34.000]   Yeah.
[01:49:34.000 --> 01:49:36.640]   I know that, you know, we should also not be too...
[01:49:36.640 --> 01:49:39.840]   I think Google can definitely recover from this,
[01:49:39.840 --> 01:49:41.200]   like they're just like Microsoft.
[01:49:41.200 --> 01:49:43.760]   Like Microsoft definitely lived through this with,
[01:49:43.760 --> 01:49:47.840]   you know, had a bad leadership and just all sorts of internal problems,
[01:49:47.840 --> 01:49:49.760]   and they were able to turn it around.
[01:49:49.760 --> 01:49:52.880]   And I think, you know, Google's not going to go anywhere.
[01:49:52.880 --> 01:49:54.880]   They may have a couple of rough quarters and all that,
[01:49:54.880 --> 01:49:58.080]   but they get a lot of smart people still working there,
[01:49:58.080 --> 01:49:59.520]   and they'll be able to, you know,
[01:49:59.520 --> 01:50:01.120]   but they got to get the right management in place.
[01:50:01.120 --> 01:50:01.600]   That's true.
[01:50:01.600 --> 01:50:04.480]   You know, there was that other article a few months ago,
[01:50:04.480 --> 01:50:07.440]   talking about how one of the issues where people
[01:50:08.160 --> 01:50:10.320]   were always creating new projects there,
[01:50:10.320 --> 01:50:13.600]   but then there was no support for that project going forward.
[01:50:13.600 --> 01:50:17.040]   And so you would get promotions for doing projects,
[01:50:17.040 --> 01:50:20.400]   but not promotions for having a successful product.
[01:50:20.400 --> 01:50:23.120]   No, no incentive to maintain the success.
[01:50:23.120 --> 01:50:23.520]   Right.
[01:50:23.520 --> 01:50:24.560]   So they just dwindle a little bit.
[01:50:24.560 --> 01:50:25.760]   They would watch it move on.
[01:50:25.760 --> 01:50:26.160]   Yeah.
[01:50:26.160 --> 01:50:28.720]   And so this idea of Facebook, you know, it's like you brought that up,
[01:50:28.720 --> 01:50:32.800]   like he doesn't want managers to be managing managers,
[01:50:32.800 --> 01:50:35.760]   which, yeah, you know, I think a lot of people are on board.
[01:50:35.760 --> 01:50:40.720]   It shouldn't be like a shocking revelation to have in management,
[01:50:40.720 --> 01:50:44.400]   I feel like, but he was rewarded handsibly for that.
[01:50:44.400 --> 01:50:45.680]   Their stock went through the roof,
[01:50:45.680 --> 01:50:47.920]   even though their quarter was kind of okay,
[01:50:47.920 --> 01:50:50.960]   because he said, I'm just going to eliminate a lot of management.
[01:50:50.960 --> 01:50:53.040]   And, you know, stockholders like to hear that,
[01:50:53.040 --> 01:50:54.560]   because it's going to be, you know,
[01:50:54.560 --> 01:50:56.480]   that was at the air of optimization or something.
[01:50:56.480 --> 01:50:59.760]   He said regarding their way that companies could be run.
[01:50:59.760 --> 01:51:00.880]   But that's the smart thing.
[01:51:00.880 --> 01:51:02.960]   And Google probably needs to do something similar to that.
[01:51:02.960 --> 01:51:04.800]   Do you think it's Cheryl Sandberg's departure
[01:51:04.800 --> 01:51:07.760]   that changed things at Facebook for Mark,
[01:51:07.760 --> 01:51:10.160]   that he now can flatten the structure?
[01:51:10.160 --> 01:51:12.880]   She was for so long really running the operation.
[01:51:12.880 --> 01:51:15.360]   It was the market, I think.
[01:51:15.360 --> 01:51:18.800]   And like, you know, I think maybe there's a factor to Cheryl,
[01:51:18.800 --> 01:51:20.960]   but really it's the market that forced it, right?
[01:51:20.960 --> 01:51:22.720]   He wanted to put a lot of money into Metaverse,
[01:51:22.720 --> 01:51:26.800]   but the Metaverse side has barely been talked about since then.
[01:51:26.800 --> 01:51:28.320]   And he's talking, he's like,
[01:51:28.320 --> 01:51:32.160]   I think Zuckerberg has heard what the market wants to hear,
[01:51:33.200 --> 01:51:34.960]   and is making those changes.
[01:51:34.960 --> 01:51:39.600]   And I think it's kind of like a founder reasserting more control
[01:51:39.600 --> 01:51:42.720]   and going into a little bit of wartime mode,
[01:51:42.720 --> 01:51:45.680]   which he needs after what's happened with like,
[01:51:45.680 --> 01:51:48.640]   iOS changes, like really hitting the ad business
[01:51:48.640 --> 01:51:51.280]   and them trying to go and fix it, fix the core pieces.
[01:51:51.280 --> 01:51:53.840]   And then you can go and add other pieces on top.
[01:51:53.840 --> 01:51:55.280]   That's what's happening right now over there.
[01:51:55.280 --> 01:51:58.480]   And yeah, Google, they'll be fine.
[01:51:58.480 --> 01:52:00.720]   They have some of the smartest people on the planet.
[01:52:00.720 --> 01:52:02.000]   We shouldn't be talking about them as like,
[01:52:02.000 --> 01:52:03.200]   they're like out of the fight.
[01:52:03.200 --> 01:52:05.200]   That's like not even close to what's happening.
[01:52:05.200 --> 01:52:07.520]   They brought like there's something crazy AI things
[01:52:07.520 --> 01:52:10.880]   that they can just pop out that will just blow people's minds.
[01:52:10.880 --> 01:52:14.960]   But there's like efficiency and speed and things
[01:52:14.960 --> 01:52:16.640]   that they will have to go and solve.
[01:52:16.640 --> 01:52:20.160]   And this year will tell like, can they do that or not?
[01:52:20.160 --> 01:52:23.600]   And there could be a lot of changes as a result if they can't.
[01:52:23.600 --> 01:52:27.520]   Let's take a little break.
[01:52:27.520 --> 01:52:28.560]   Ben Parr is here.
[01:52:30.080 --> 01:52:32.960]   Old friend, when did you first show up on our shows?
[01:52:32.960 --> 01:52:35.520]   When was I?
[01:52:35.520 --> 01:52:36.400]   I'm glad you're back.
[01:52:36.400 --> 01:52:37.440]   We missed you for a while.
[01:52:37.440 --> 01:52:40.800]   Editor, it was probably the greatest time I've ever been in.
[01:52:40.800 --> 01:52:42.800]   Oh yeah, it was a year of vegetable.
[01:52:42.800 --> 01:52:45.840]   9, 10, 11, something like that.
[01:52:45.840 --> 01:52:46.480]   Long time.
[01:52:46.480 --> 01:52:48.480]   You're only 12.
[01:52:48.480 --> 01:52:49.360]   It's kind of amazing.
[01:52:49.360 --> 01:52:50.400]   Yeah.
[01:52:50.400 --> 01:52:51.120]   God yeah.
[01:52:51.120 --> 01:52:52.000]   Yeah.
[01:52:52.000 --> 01:52:53.280]   New podcast is called.
[01:52:53.280 --> 01:52:53.680]   Only just two now.
[01:52:53.680 --> 01:52:55.920]   You look at that.
[01:52:55.920 --> 01:52:59.760]   Business Envy Show at businessenvyshow.com.
[01:53:00.480 --> 01:53:03.600]   You got one tech guy, one Hollywood celebrity.
[01:53:03.600 --> 01:53:05.360]   Lots of fun.
[01:53:05.360 --> 01:53:06.960]   Oh yeah.
[01:53:06.960 --> 01:53:09.280]   Thank you for being here, Ben.
[01:53:09.280 --> 01:53:11.600]   Oh, and JJ Stone is oh, doctor.
[01:53:11.600 --> 01:53:12.960]   I've known you for a long time too.
[01:53:12.960 --> 01:53:15.040]   Steward introduced us.
[01:53:15.040 --> 01:53:19.680]   Yeah, almost your wife introduced us.
[01:53:19.680 --> 01:53:20.000]   What?
[01:53:20.000 --> 01:53:23.200]   Were you seeing my wife before we got together?
[01:53:23.200 --> 01:53:27.040]   I saw her, but I knew who you were.
[01:53:27.040 --> 01:53:28.720]   So she did see me before.
[01:53:28.720 --> 01:53:29.920]   You know, she saw you.
[01:53:29.920 --> 01:53:30.640]   You don't have a drink.
[01:53:30.640 --> 01:53:31.680]   I thought I was funny.
[01:53:31.680 --> 01:53:31.920]   Yeah.
[01:53:31.920 --> 01:53:32.960]   Oh, that's right.
[01:53:32.960 --> 01:53:33.920]   You got an eye for talent.
[01:53:33.920 --> 01:53:35.760]   That was a CES or something, right?
[01:53:35.760 --> 01:53:36.560]   Brought us together.
[01:53:36.560 --> 01:53:38.560]   South by.
[01:53:38.560 --> 01:53:39.200]   South by.
[01:53:39.200 --> 01:53:40.240]   Awesome.
[01:53:40.240 --> 01:53:41.600]   South by back.
[01:53:41.600 --> 01:53:43.120]   Back in the South by was South by.
[01:53:43.120 --> 01:53:44.400]   I've been to South by in a while.
[01:53:44.400 --> 01:53:46.400]   Neither.
[01:53:46.400 --> 01:53:46.640]   Yeah.
[01:53:46.640 --> 01:53:48.480]   Great to have you.
[01:53:48.480 --> 01:53:49.520]   Oh, and times change.
[01:53:49.520 --> 01:53:55.200]   And of course, Daniel Rabino, now editor in chief once again.
[01:53:56.400 --> 01:53:57.200]   With us.
[01:53:57.200 --> 01:53:58.160]   With us central.
[01:53:58.160 --> 01:54:01.520]   Feels like that's a better job than executive editor.
[01:54:01.520 --> 01:54:04.720]   Yeah, the executive was just,
[01:54:04.720 --> 01:54:06.560]   it was more just about content creation
[01:54:06.560 --> 01:54:08.400]   and not day to day management stuff,
[01:54:08.400 --> 01:54:09.040]   whereas now,
[01:54:09.040 --> 01:54:11.520]   but now I'm back in charge and.
[01:54:11.520 --> 01:54:12.240]   You like that?
[01:54:12.240 --> 01:54:12.880]   Changing a lot.
[01:54:12.880 --> 01:54:13.760]   Yeah.
[01:54:13.760 --> 01:54:17.920]   Because I'm I'm doing like four years of stuff.
[01:54:17.920 --> 01:54:20.320]   I didn't like on our site.
[01:54:20.320 --> 01:54:22.160]   So you shoved him a sign and said,
[01:54:22.160 --> 01:54:23.040]   let me add it.
[01:54:23.040 --> 01:54:24.160]   Let me add it, boss.
[01:54:24.160 --> 01:54:24.640]   Yeah.
[01:54:24.640 --> 01:54:24.960]   Yeah.
[01:54:24.960 --> 01:54:26.960]   We've been doing a lot of changes and it's just been,
[01:54:26.960 --> 01:54:29.040]   it's been better for the staff and everybody else.
[01:54:29.040 --> 01:54:30.480]   And yeah, for a vision.
[01:54:30.480 --> 01:54:32.960]   It's now now with uncanny eye contact.
[01:54:32.960 --> 01:54:35.680]   Windows central.com.
[01:54:35.680 --> 01:54:39.440]   Actually, I've been wanting to use that Nvidia software
[01:54:39.440 --> 01:54:42.880]   because I have a machine with a 30 30 80, I think in it.
[01:54:42.880 --> 01:54:44.720]   So I work with that, right?
[01:54:44.720 --> 01:54:46.080]   You don't need a 40 or something.
[01:54:46.080 --> 01:54:46.880]   Yeah.
[01:54:46.880 --> 01:54:48.240]   No, no, just an RTX RTX.
[01:54:48.240 --> 01:54:49.760]   RTX RTX machine.
[01:54:49.760 --> 01:54:54.080]   Be kind of fun to like always be staring straight at you.
[01:54:55.040 --> 01:54:56.080]   Plus you get all the other stuff.
[01:54:56.080 --> 01:54:57.600]   You get the noise cancellation.
[01:54:57.600 --> 01:54:58.080]   Yeah.
[01:54:58.080 --> 01:54:59.120]   The blurring.
[01:54:59.120 --> 01:55:00.080]   The blurring that's happening.
[01:55:00.080 --> 01:55:00.560]   It's much better.
[01:55:00.560 --> 01:55:01.600]   It's so good.
[01:55:01.600 --> 01:55:02.560]   Yeah.
[01:55:02.560 --> 01:55:04.080]   That's an idea too.
[01:55:04.080 --> 01:55:04.320]   Yeah.
[01:55:04.320 --> 01:55:04.960]   Look at your hair.
[01:55:04.960 --> 01:55:05.600]   Everything.
[01:55:05.600 --> 01:55:08.880]   Your little hairs, you can't, it's not, it really works.
[01:55:08.880 --> 01:55:09.120]   Yeah.
[01:55:09.120 --> 01:55:10.400]   Yeah.
[01:55:10.400 --> 01:55:10.960]   I want that.
[01:55:10.960 --> 01:55:11.920]   I'll have to try it.
[01:55:11.920 --> 01:55:13.920]   I don't, do you have to run it on Windows?
[01:55:13.920 --> 01:55:14.880]   Can you run it on Linux?
[01:55:14.880 --> 01:55:16.640]   Mm.
[01:55:16.640 --> 01:55:17.600]   I don't know if that I don't know.
[01:55:17.600 --> 01:55:18.320]   I'll try it.
[01:55:18.320 --> 01:55:18.960]   I'll see.
[01:55:18.960 --> 01:55:19.440]   We'll see.
[01:55:19.440 --> 01:55:22.880]   Our show today brought to you by
[01:55:22.880 --> 01:55:24.640]   somebody changing the world.
[01:55:24.640 --> 01:55:26.240]   Shopify.
[01:55:26.240 --> 01:55:27.280]   Yeah.
[01:55:27.280 --> 01:55:28.080]   I love Shopify.
[01:55:28.080 --> 01:55:29.120]   You hear that sound?
[01:55:29.120 --> 01:55:31.760]   That is the sound of an angel getting its wings.
[01:55:31.760 --> 01:55:32.160]   No.
[01:55:32.160 --> 01:55:35.680]   That is the sound of another sale on Shopify.
[01:55:35.680 --> 01:55:38.320]   That first sale is magical.
[01:55:38.320 --> 01:55:40.400]   That's the moment another business dream
[01:55:40.400 --> 01:55:42.000]   becomes a reality.
[01:55:42.000 --> 01:55:43.920]   And it happens all the time on Shopify.
[01:55:43.920 --> 01:55:45.920]   Shopify is the commerce platform,
[01:55:45.920 --> 01:55:49.520]   revolutionizing millions of businesses worldwide.
[01:55:49.520 --> 01:55:51.520]   What's wonderful about Shopify?
[01:55:51.520 --> 01:55:54.480]   You start small, but no matter how big you want to grow,
[01:55:54.480 --> 01:55:56.640]   Shopify is there to empower you.
[01:55:56.640 --> 01:55:58.160]   Both confidence and control
[01:55:58.160 --> 01:56:00.320]   take your business to the next level.
[01:56:00.320 --> 01:56:01.680]   Whether you're selling fedoras,
[01:56:01.680 --> 01:56:05.920]   or bike helmets, or in my son's case, salt,
[01:56:05.920 --> 01:56:07.760]   he uses Shopify.
[01:56:07.760 --> 01:56:12.000]   He has built a business on Shopify from zero to infinity.
[01:56:12.000 --> 01:56:14.880]   He now sells out every single time he gets more salt
[01:56:14.880 --> 01:56:16.480]   and Shopify handles it all.
[01:56:16.480 --> 01:56:18.480]   They simplify selling online.
[01:56:18.480 --> 01:56:20.240]   They even simplify it in person too.
[01:56:20.240 --> 01:56:21.920]   So if you've got a storefront,
[01:56:21.920 --> 01:56:23.280]   you can use Shopify as well.
[01:56:23.280 --> 01:56:25.840]   So you can focus on successfully growing your business.
[01:56:25.840 --> 01:56:27.760]   My daughter uses Shopify too.
[01:56:27.760 --> 01:56:30.160]   That's what I love about this.
[01:56:30.160 --> 01:56:31.680]   This is empowering people
[01:56:31.680 --> 01:56:34.720]   to start their own side hustle,
[01:56:34.720 --> 01:56:36.000]   to start their own business,
[01:56:36.000 --> 01:56:39.520]   to build from nothing to something.
[01:56:39.520 --> 01:56:41.120]   Something you can be proud of.
[01:56:41.120 --> 01:56:43.360]   And Shopify does it for every sales channel,
[01:56:43.360 --> 01:56:44.240]   from an in-point,
[01:56:44.240 --> 01:56:46.720]   in-person point of sale system,
[01:56:46.720 --> 01:56:49.200]   to an all-in-one e-commerce platform.
[01:56:49.200 --> 01:56:50.800]   Go to salthanks site.
[01:56:50.800 --> 01:56:52.480]   If you go there, that's a Shopify site.
[01:56:52.480 --> 01:56:53.280]   It's beautiful.
[01:56:53.280 --> 01:56:53.920]   It's gorgeous.
[01:56:53.920 --> 01:56:56.960]   You can even sell across social media marketplaces,
[01:56:56.960 --> 01:56:57.440]   as he does.
[01:56:57.440 --> 01:56:58.160]   He's on TikTok.
[01:56:58.160 --> 01:56:58.880]   He's on Insta.
[01:56:58.880 --> 01:57:02.160]   Packed with industry-leading tools,
[01:57:02.160 --> 01:57:03.440]   ready to ignite your growth,
[01:57:03.440 --> 01:57:05.120]   Shopify gives you complete control
[01:57:05.120 --> 01:57:06.400]   over your business and your brand.
[01:57:06.400 --> 01:57:08.560]   Without having to learn any new skills
[01:57:08.560 --> 01:57:09.440]   and design or code,
[01:57:09.440 --> 01:57:10.800]   I was blown away.
[01:57:10.800 --> 01:57:12.080]   I said, "Henry, how'd you do that?"
[01:57:12.080 --> 01:57:13.120]   He said, "I'm doing a Shopify."
[01:57:13.120 --> 01:57:16.080]   It's pretty darn impressive.
[01:57:16.800 --> 01:57:20.960]   And thanks to 24/7 Help and an extensive business course library,
[01:57:20.960 --> 01:57:22.720]   if you want to learn it,
[01:57:22.720 --> 01:57:24.240]   Shopify's got the resources.
[01:57:24.240 --> 01:57:26.320]   They're there to help you support your success
[01:57:26.320 --> 01:57:28.640]   every step of the way.
[01:57:28.640 --> 01:57:31.840]   So, now, let's get serious about selling
[01:57:31.840 --> 01:57:33.040]   and try Shopify today.
[01:57:33.040 --> 01:57:34.560]   This is possibility,
[01:57:34.560 --> 01:57:36.880]   powered by Shopify.
[01:57:36.880 --> 01:57:38.000]   Man, I know it.
[01:57:38.000 --> 01:57:38.560]   I know it.
[01:57:38.560 --> 01:57:39.360]   I've seen it happen.
[01:57:39.360 --> 01:57:40.080]   It's amazing.
[01:57:40.080 --> 01:57:43.280]   Sign up for a dollar a month trial period,
[01:57:43.280 --> 01:57:44.000]   dollar a month.
[01:57:45.200 --> 01:57:48.400]   Shopify.com/twit.
[01:57:48.400 --> 01:57:49.840]   That's all lowercase.
[01:57:49.840 --> 01:57:50.960]   Shopify.
[01:57:50.960 --> 01:57:52.560]   S-H-O-P-I-F-Y.
[01:57:52.560 --> 01:57:54.720]   Shopify.com/twit.
[01:57:54.720 --> 01:57:57.280]   Take your business to the next level today.
[01:57:57.280 --> 01:57:59.600]   It was, I think it was,
[01:57:59.600 --> 01:58:01.520]   was it wasn't even a year ago
[01:58:01.520 --> 01:58:03.680]   that Henry started selling salt and Shopify.
[01:58:03.680 --> 01:58:05.920]   We bought, I know we bought him out
[01:58:05.920 --> 01:58:07.920]   for his first batch
[01:58:07.920 --> 01:58:11.920]   to give out to all our hosts on Christmas last year.
[01:58:11.920 --> 01:58:13.360]   So, it's been just a little over a year.
[01:58:14.160 --> 01:58:16.480]   And now, like, it's sailing.
[01:58:16.480 --> 01:58:20.240]   He's actually hired people to do the fulfillment.
[01:58:20.240 --> 01:58:22.320]   He's got it all wired.
[01:58:22.320 --> 01:58:25.440]   Shopify.com/twit.
[01:58:25.440 --> 01:58:28.720]   Thank you Shopify for all of your support.
[01:58:28.720 --> 01:58:30.560]   For my kids, I thank you.
[01:58:30.560 --> 01:58:31.360]   And for all of you.
[01:58:31.360 --> 01:58:34.640]   Let's see new emojis coming.
[01:58:34.640 --> 01:58:36.240]   I always like to look at the new emojis.
[01:58:36.240 --> 01:58:38.240]   We've seen our first look in iOS 16.4,
[01:58:38.240 --> 01:58:39.120]   which is out in public.
[01:58:39.680 --> 01:58:44.080]   Beta now. I don't know what that shaking face is for.
[01:58:44.080 --> 01:58:46.640]   It's like a whoa.
[01:58:46.640 --> 01:58:51.200]   And then talk to the hand, both left and right.
[01:58:51.200 --> 01:58:54.560]   I guess, although if you put two together,
[01:58:54.560 --> 01:58:56.240]   you could say it's this big.
[01:58:56.240 --> 01:58:58.960]   What is it? It's a high five.
[01:58:58.960 --> 01:59:00.800]   Oh, if they're next to each other, they're high five.
[01:59:00.800 --> 01:59:03.680]   This big means like, okay.
[01:59:03.680 --> 01:59:04.560]   Yeah.
[01:59:04.560 --> 01:59:05.360]   How big is that?
[01:59:05.360 --> 01:59:06.240]   It's this big.
[01:59:06.240 --> 01:59:08.320]   You have to put some spaces in there.
[01:59:09.280 --> 01:59:10.080]   There's a moose.
[01:59:10.080 --> 01:59:11.680]   There's a mule.
[01:59:11.680 --> 01:59:13.120]   There's eagles wings.
[01:59:13.120 --> 01:59:14.880]   Sorry.
[01:59:14.880 --> 01:59:15.520]   Oh, doctor.
[01:59:15.520 --> 01:59:18.160]   Fly, eagles wings fly.
[01:59:18.160 --> 01:59:19.920]   Is that a crow?
[01:59:19.920 --> 01:59:20.400]   A duck?
[01:59:20.400 --> 01:59:22.400]   A jellyfish?
[01:59:22.400 --> 01:59:25.360]   That's a what is that plant?
[01:59:25.360 --> 01:59:27.040]   That's a that's a dahlia?
[01:59:27.040 --> 01:59:27.600]   What is that?
[01:59:27.600 --> 01:59:29.440]   We're lavender.
[01:59:29.440 --> 01:59:31.920]   Let no, it's not lavender.
[01:59:31.920 --> 01:59:34.560]   There's a ginger root.
[01:59:34.560 --> 01:59:37.280]   Snap peas, a fan.
[01:59:38.320 --> 01:59:40.720]   A comb, an Afro comb.
[01:59:40.720 --> 01:59:42.240]   Hey, hey, hey, hey, hey.
[01:59:42.240 --> 01:59:42.800]   Isn't it?
[01:59:42.800 --> 01:59:43.360]   That is your mom, finally.
[01:59:43.360 --> 01:59:45.360]   What other kind of comb would that be?
[01:59:45.360 --> 01:59:46.400]   Can I use that comb?
[01:59:46.400 --> 01:59:47.280]   No, no, I'm saying.
[01:59:47.280 --> 01:59:48.640]   I'm saying right on, dude.
[01:59:48.640 --> 01:59:49.120]   What's your brother?
[01:59:49.120 --> 01:59:50.000]   We got one.
[01:59:50.000 --> 01:59:50.480]   We got one.
[01:59:50.480 --> 01:59:51.520]   When I was growing up,
[01:59:51.520 --> 01:59:54.720]   and I was sad that the Afros went away,
[01:59:54.720 --> 01:59:56.240]   because I thought Afros was a circle.
[01:59:56.240 --> 01:59:58.400]   And all my friends who had Afros
[01:59:58.400 --> 02:00:00.560]   would have the comb stuck in it.
[02:00:00.560 --> 02:00:01.280]   They would just walk around.
[02:00:01.280 --> 02:00:02.160]   Yeah, just stuck in.
[02:00:02.160 --> 02:00:03.280]   Just stuck right in there.
[02:00:03.280 --> 02:00:05.520]   Did you ever have a fro?
[02:00:05.520 --> 02:00:08.240]   You ever seen Afros are still here?
[02:00:08.240 --> 02:00:09.280]   They're back.
[02:00:09.280 --> 02:00:10.240]   They haven't gone away.
[02:00:10.240 --> 02:00:10.800]   No, they're back.
[02:00:10.800 --> 02:00:11.440]   My daughter has.
[02:00:11.440 --> 02:00:12.880]   They never left.
[02:00:12.880 --> 02:00:13.760]   Afros have been around.
[02:00:13.760 --> 02:00:15.840]   You know, it's just not in your community,
[02:00:15.840 --> 02:00:17.120]   uncle, you don't see it all the time,
[02:00:17.120 --> 02:00:18.320]   but in my community,
[02:00:18.320 --> 02:00:19.360]   it has never gone away.
[02:00:19.360 --> 02:00:21.600]   And yes, I had an Afro.
[02:00:21.600 --> 02:00:22.960]   An Afro is so big.
[02:00:22.960 --> 02:00:24.720]   One time I was riding my bicycle,
[02:00:24.720 --> 02:00:26.480]   and I got a B stuck in my Afro.
[02:00:26.480 --> 02:00:27.840]   [LAUGHTER]
[02:00:27.840 --> 02:00:29.120]   Okay, you got to put a picture.
[02:00:29.120 --> 02:00:30.480]   15 minutes to get it out.
[02:00:30.480 --> 02:00:31.360]   Do you have any pictures?
[02:00:31.360 --> 02:00:32.160]   I want some pictures.
[02:00:32.160 --> 02:00:33.680]   You got to put the picture on the display.
[02:00:33.680 --> 02:00:35.600]   Oh, I do have pictures of me in a fro.
[02:00:35.600 --> 02:00:36.960]   I want to see you in a fro.
[02:00:36.960 --> 02:00:37.920]   Because right now--
[02:00:37.920 --> 02:00:38.800]   Glenn was this.
[02:00:38.800 --> 02:00:40.880]   Long, long hair.
[02:00:40.880 --> 02:00:43.200]   Hey, look, Ben, I got glasses on
[02:00:43.200 --> 02:00:44.080]   because I lay someone.
[02:00:44.080 --> 02:00:45.200]   You're going to disrespect me.
[02:00:45.200 --> 02:00:46.640]   No, I got nothing.
[02:00:46.640 --> 02:00:47.840]   I haven't had it.
[02:00:47.840 --> 02:00:49.680]   I can't have a fro in decades.
[02:00:49.680 --> 02:00:51.040]   I want one so badly.
[02:00:51.040 --> 02:00:53.040]   [LAUGHTER]
[02:00:53.040 --> 02:00:56.400]   When I got out of high school,
[02:00:56.400 --> 02:00:57.840]   my hair started running away from me.
[02:00:57.840 --> 02:00:58.880]   That's what happened to me.
[02:00:58.880 --> 02:01:00.240]   Oh, really that quickly?
[02:01:00.240 --> 02:01:00.880]   That's sad.
[02:01:00.880 --> 02:01:04.480]   Yeah, I want to see a picture of you in your fro.
[02:01:04.480 --> 02:01:06.720]   There's also a wooden flute.
[02:01:06.720 --> 02:01:07.520]   I lived through Leah now.
[02:01:07.520 --> 02:01:08.560]   Leah's got all my fros.
[02:01:08.560 --> 02:01:09.200]   Good for her.
[02:01:09.200 --> 02:01:10.960]   I think that's awesome.
[02:01:10.960 --> 02:01:13.760]   Shaking face, pink heart,
[02:01:13.760 --> 02:01:16.720]   light blue heart, gray heart.
[02:01:16.720 --> 02:01:18.400]   Now, I've only recently learned
[02:01:18.400 --> 02:01:20.560]   that the color of the heart is relevant, germane.
[02:01:20.560 --> 02:01:25.840]   Like you don't send a red heart to a friend, right?
[02:01:25.840 --> 02:01:26.800]   Is that right?
[02:01:26.800 --> 02:01:28.160]   What do you-- who do you send a blue heart?
[02:01:28.160 --> 02:01:28.960]   [LAUGHTER]
[02:01:28.960 --> 02:01:29.760]   Or gray heart.
[02:01:29.760 --> 02:01:30.560]   That seems bad.
[02:01:30.560 --> 02:01:31.760]   That seems like a--
[02:01:31.760 --> 02:01:32.800]   Yeah, it's like a dead heart.
[02:01:32.800 --> 02:01:34.080]   Dead heart, like a jinx.
[02:01:34.960 --> 02:01:36.560]   We need to have some gen Zs on--
[02:01:36.560 --> 02:01:38.400]   To translate.
[02:01:38.400 --> 02:01:39.440]   To translate it all.
[02:01:39.440 --> 02:01:40.160]   Translate.
[02:01:40.160 --> 02:01:42.720]   Yeah, pink heart has been a popular request for some time.
[02:01:42.720 --> 02:01:47.520]   This is from emojipedia.
[02:01:47.520 --> 02:01:48.800]   The light blue heart and gray heart
[02:01:48.800 --> 02:01:51.680]   closed some notable gaps within the heart emoji
[02:01:51.680 --> 02:01:53.280]   color spectrum.
[02:01:53.280 --> 02:01:54.960]   Oh, thank God.
[02:01:54.960 --> 02:01:55.920]   Oh, boy.
[02:01:55.920 --> 02:01:59.440]   In fact, they have a whole PDF
[02:02:00.240 --> 02:02:06.000]   that they've put out on the emoji color spectrum.
[02:02:06.000 --> 02:02:10.160]   Examining emoji color spaces a strategy
[02:02:10.160 --> 02:02:12.320]   for improving the coverage of the heart emoji.
[02:02:12.320 --> 02:02:14.960]   This is Jennifer Daniel writing on behalf of the
[02:02:14.960 --> 02:02:19.120]   Unicode Emoji Subcommittee to the Unicode Technical Committee.
[02:02:19.120 --> 02:02:22.960]   This is how you, by the way, this is how you get a new emoji.
[02:02:22.960 --> 02:02:27.680]   As you write these long, scholarly-like papers.
[02:02:29.280 --> 02:02:32.160]   Identify as a black heart just so we get that clear.
[02:02:32.160 --> 02:02:32.720]   Yeah.
[02:02:32.720 --> 02:02:33.520]   So nobody gets any kind of confusion.
[02:02:33.520 --> 02:02:35.280]   No, but the black heart's useful, right?
[02:02:35.280 --> 02:02:36.640]   That's saying something.
[02:02:36.640 --> 02:02:39.120]   So here's a-- here's the entire color space.
[02:02:39.120 --> 02:02:45.120]   You can see there's big gaps in the P3 DCI, P3 color space.
[02:02:45.120 --> 02:02:46.160]   They're my purple hearts.
[02:02:46.160 --> 02:02:48.800]   Yeah, we need purple hearts, right?
[02:02:48.800 --> 02:02:51.440]   Those are safe for our veterans.
[02:02:51.440 --> 02:02:52.880]   We're not playing these games.
[02:02:52.880 --> 02:02:55.920]   Certain colors are preserved.
[02:02:55.920 --> 02:02:56.640]   So amazing.
[02:02:56.640 --> 02:02:58.720]   The attention put into this.
[02:02:59.680 --> 02:03:01.040]   Wow.
[02:03:01.040 --> 02:03:04.080]   I don't-- there's charts.
[02:03:04.080 --> 02:03:07.440]   There's-- so-- but I wanted to tell us what it means.
[02:03:07.440 --> 02:03:08.720]   Yeah.
[02:03:08.720 --> 02:03:09.440]   What does it mean?
[02:03:09.440 --> 02:03:11.440]   It's not known yet.
[02:03:11.440 --> 02:03:12.960]   Is that's the thing?
[02:03:12.960 --> 02:03:15.760]   It's going to be known once the Gen Zs get their hands on it
[02:03:15.760 --> 02:03:18.240]   and they make what-- you know, they tell me what this--
[02:03:18.240 --> 02:03:18.800]   We don't know yet.
[02:03:18.800 --> 02:03:21.360]   I just want to know what I should not use things
[02:03:21.360 --> 02:03:22.400]   because that's what gets people--
[02:03:22.400 --> 02:03:23.520]   Exactly, right?
[02:03:23.520 --> 02:03:27.040]   Like they go, "Dad, that's the pansexual flag."
[02:03:27.840 --> 02:03:30.080]   And then I go, "What's pansexual?"
[02:03:30.080 --> 02:03:31.040]   They say, "Dad."
[02:03:31.040 --> 02:03:33.280]   And I don't-- I don't know.
[02:03:33.280 --> 02:03:33.840]   I don't know.
[02:03:33.840 --> 02:03:36.320]   So, well, we now have it.
[02:03:36.320 --> 02:03:40.000]   If you need it, Gen Z, you've got it.
[02:03:40.000 --> 02:03:44.240]   The two pushing hands emoji could be combined
[02:03:44.240 --> 02:03:46.160]   and create a distinct high-five emoji.
[02:03:46.160 --> 02:03:46.320]   Look at that.
[02:03:46.320 --> 02:03:48.560]   You put the little dynamite explosion in there
[02:03:48.560 --> 02:03:50.640]   and you really-- you got it going on.
[02:03:50.640 --> 02:03:54.560]   I still want to know what that plant is.
[02:03:54.560 --> 02:03:55.200]   Hiusense.
[02:03:55.200 --> 02:03:56.880]   That's a hiusense.
[02:03:57.440 --> 02:03:58.400]   Hiusense.
[02:03:58.400 --> 02:03:59.760]   Hiusense.
[02:03:59.760 --> 02:04:00.640]   Hiusense.
[02:04:00.640 --> 02:04:01.840]   Why?
[02:04:01.840 --> 02:04:04.960]   I thought it was laughing too when you said it.
[02:04:04.960 --> 02:04:05.840]   I was like, "That's a good answer.
[02:04:05.840 --> 02:04:06.320]   Good answer."
[02:04:06.320 --> 02:04:09.440]   Yeah, it could be used to talk about the actual flower.
[02:04:09.440 --> 02:04:12.000]   Flowers in general or the color purple
[02:04:12.000 --> 02:04:14.480]   can also be used in reference to springtime.
[02:04:14.480 --> 02:04:18.480]   Maybe used as part of the symbolic table setting for now-ruzz,
[02:04:18.480 --> 02:04:19.840]   the Persian New Year.
[02:04:19.840 --> 02:04:23.520]   Oh, so they use Hiusense to celebrate their New Year.
[02:04:24.480 --> 02:04:28.000]   And then you could see all the different stylings
[02:04:28.000 --> 02:04:33.200]   from the various folk of Hiusense.
[02:04:33.200 --> 02:04:36.640]   Anyway, that probably gave more time to that story
[02:04:36.640 --> 02:04:38.080]   than it really deserved.
[02:04:38.080 --> 02:04:41.360]   Eat your heart out, Egyptians.
[02:04:41.360 --> 02:04:43.920]   What about your New Year?
[02:04:43.920 --> 02:04:48.160]   Big changes coming, speaking of Google to YouTube.
[02:04:48.160 --> 02:04:52.880]   Susan Wojiski, who was, I think, the 16th employee
[02:04:53.600 --> 02:04:57.600]   at Google, has been running YouTube for a long time.
[02:04:57.600 --> 02:04:59.280]   She is stepping down.
[02:04:59.280 --> 02:05:02.720]   She'll be replaced by her longtime lieutenant,
[02:05:02.720 --> 02:05:04.320]   according to Rico Neil Mohan.
[02:05:04.320 --> 02:05:08.800]   In a letter sent to a new YouTube employee,
[02:05:08.800 --> 02:05:11.120]   Wojiski said she was leaving in order to-
[02:05:11.120 --> 02:05:14.960]   I want you guys to tell me what this means
[02:05:14.960 --> 02:05:17.120]   because between the lines there's something here.
[02:05:17.120 --> 02:05:20.480]   Start a new chapter focused on my family,
[02:05:20.480 --> 02:05:23.920]   health, and personal projects I'm passionate about.
[02:05:23.920 --> 02:05:25.520]   Pushed out.
[02:05:25.520 --> 02:05:28.960]   Pushed out or wants to spend more time with their money,
[02:05:28.960 --> 02:05:29.600]   which is it?
[02:05:29.600 --> 02:05:30.640]   I don't know.
[02:05:30.640 --> 02:05:31.200]   Yeah.
[02:05:31.200 --> 02:05:32.880]   I don't think she's always hard to know.
[02:05:32.880 --> 02:05:36.720]   YouTube has been like, "This has been one of the few real successes at Google."
[02:05:36.720 --> 02:05:40.560]   I think after a certain amount of time,
[02:05:40.560 --> 02:05:42.000]   you really do just get tired.
[02:05:42.000 --> 02:05:43.280]   That's my gut gap.
[02:05:43.280 --> 02:05:47.280]   Yeah, and when you vest enough shares to have,
[02:05:47.280 --> 02:05:51.760]   "Oh, I don't know, $500 million, why would you work?"
[02:05:51.760 --> 02:05:57.040]   Or maybe she also knows that something's coming down the road.
[02:05:57.040 --> 02:05:57.600]   Oh, let's say that.
[02:05:57.600 --> 02:05:58.160]   That's bad for YouTube.
[02:05:58.160 --> 02:05:58.720]   There you go.
[02:05:58.720 --> 02:06:00.400]   Bad news code.
[02:06:00.400 --> 02:06:02.000]   You know, it's like TikTok and everything.
[02:06:02.000 --> 02:06:03.760]   It's going to really start to eat into them.
[02:06:03.760 --> 02:06:06.080]   And so she's like, "I'm just getting out now."
[02:06:06.080 --> 02:06:08.480]   I think that's why Sheryl Sandberg left Facebook.
[02:06:08.480 --> 02:06:11.760]   I think Sheryl Sandberg definitely saw it coming and said,
[02:06:11.760 --> 02:06:13.520]   "I'm getting out of here before it's too late."
[02:06:13.520 --> 02:06:16.160]   Well, if you're Sheryl, it's like, why deal with it anymore?
[02:06:16.160 --> 02:06:16.800]   Yeah.
[02:06:16.800 --> 02:06:18.240]   You've made your life and career.
[02:06:18.240 --> 02:06:21.600]   And there was a point where we talked about her running for president,
[02:06:21.600 --> 02:06:25.280]   and then Cambridge Analytica and Trump and everything else happened.
[02:06:25.280 --> 02:06:28.480]   Just go and spend time with your...
[02:06:28.480 --> 02:06:30.400]   She just got married, spend time with your husband.
[02:06:30.400 --> 02:06:31.520]   You're talking about Sheryl, yeah.
[02:06:31.520 --> 02:06:32.720]   Why stress?
[02:06:32.720 --> 02:06:34.240]   Why stress?
[02:06:34.240 --> 02:06:38.320]   There's like, why stress when you could just chill?
[02:06:38.320 --> 02:06:40.560]   You could pay for a beat.
[02:06:40.560 --> 02:06:42.400]   You're a startup guy, though, right?
[02:06:42.400 --> 02:06:44.240]   So you're a people thrive on it.
[02:06:44.240 --> 02:06:50.240]   Yeah. So there's my question for you, Ben, because you're a serial entrepreneur.
[02:06:50.240 --> 02:06:52.720]   You're the kind of guy that doesn't
[02:06:52.720 --> 02:06:55.760]   relax and take the money and go to the beach.
[02:06:55.760 --> 02:06:58.000]   You keep doing the next thing, right?
[02:06:58.000 --> 02:07:04.720]   This is the existential question of every entrepreneur.
[02:07:04.720 --> 02:07:08.640]   I think, look, the reason why you would do another thing
[02:07:08.640 --> 02:07:13.040]   is because there's a mission like, "Okay, there's a default of like,
[02:07:13.040 --> 02:07:18.560]   "Okay, chill, if I make the like F you money, whatever you want to call that number."
[02:07:18.560 --> 02:07:21.520]   Then I figure out the cure for cancer.
[02:07:21.520 --> 02:07:25.040]   Yeah, I'm going to go and do that because that is like a life-ish,
[02:07:25.040 --> 02:07:26.480]   and I'm going to go change people's lives.
[02:07:26.480 --> 02:07:30.640]   You truly believe what you're doing is going to transform people's lives
[02:07:30.640 --> 02:07:32.000]   for the rest of time.
[02:07:32.000 --> 02:07:35.120]   You're going to give up whatever personal thing in favor,
[02:07:35.120 --> 02:07:38.000]   as an entrepreneur, in favor of the rest of the world.
[02:07:38.000 --> 02:07:39.600]   But I know some entrepreneurs.
[02:07:40.640 --> 02:07:43.040]   I'll give you one example, my space Tom friend.
[02:07:43.040 --> 02:07:44.400]   He gave me the big--
[02:07:44.400 --> 02:07:44.720]   He gave me the big--
[02:07:44.720 --> 02:07:45.200]   --in my space.
[02:07:45.200 --> 02:07:46.400]   He cashed in, didn't he?
[02:07:46.400 --> 02:07:47.840]   And he just--
[02:07:47.840 --> 02:07:48.320]   He's like--
[02:07:48.320 --> 02:07:49.760]   He goes on the beach.
[02:07:49.760 --> 02:07:50.160]   Yeah.
[02:07:50.160 --> 02:07:52.400]   He's like hanging out in Hawaii.
[02:07:52.400 --> 02:07:52.640]   Yeah.
[02:07:52.640 --> 02:07:55.040]   He's having the best time ever.
[02:07:55.040 --> 02:07:55.680]   He came up--
[02:07:55.680 --> 02:07:56.880]   He's the happiest person I know.
[02:07:56.880 --> 02:07:59.760]   He came up here with a tray,
[02:07:59.760 --> 02:08:03.520]   and we went out for tacos.
[02:08:03.520 --> 02:08:10.000]   As a picture of me walking with my space Tom,
[02:08:10.000 --> 02:08:12.880]   over to McNears, we had their Korean tacos.
[02:08:12.880 --> 02:08:15.920]   He said they were the best street tacos he'd had outside LA,
[02:08:15.920 --> 02:08:18.320]   and then went back to his mansion in LA.
[02:08:18.320 --> 02:08:19.600]   He's like, travels the world.
[02:08:19.600 --> 02:08:22.400]   Does photography, that's why he was hanging out with Trey?
[02:08:22.400 --> 02:08:24.880]   He has all the time in the world.
[02:08:24.880 --> 02:08:26.400]   I remember one time.
[02:08:26.400 --> 02:08:26.880]   Yeah.
[02:08:26.880 --> 02:08:27.440]   Yeah.
[02:08:27.440 --> 02:08:27.840]   I--
[02:08:27.840 --> 02:08:28.480]   He--
[02:08:28.480 --> 02:08:30.080]   The first time I met him,
[02:08:30.080 --> 02:08:33.600]   we spent three and a half hours just chatting
[02:08:33.600 --> 02:08:37.600]   over tea at a coffee shop in Brentwood.
[02:08:37.600 --> 02:08:39.360]   You don't have-- If you've sold your company,
[02:08:39.360 --> 02:08:41.440]   you don't care anymore, and you just want to hang out,
[02:08:41.440 --> 02:08:42.000]   that's what you get to do.
[02:08:42.000 --> 02:08:43.840]   But he's unusual, right?
[02:08:43.840 --> 02:08:44.560]   He's unusual.
[02:08:44.560 --> 02:08:44.800]   He's--
[02:08:44.800 --> 02:08:44.960]   He's--
[02:08:44.960 --> 02:08:44.960]   He's--
[02:08:44.960 --> 02:08:44.960]   He's--
[02:08:44.960 --> 02:08:45.440]   He's the exception.
[02:08:45.440 --> 02:08:45.920]   Yeah.
[02:08:45.920 --> 02:08:49.440]   You can also point out that maybe he's never the entrepreneur type,
[02:08:49.440 --> 02:08:49.920]   that he's--
[02:08:49.920 --> 02:08:50.480]   I don't think he was.
[02:08:50.480 --> 02:08:51.840]   --he's just saying got into it--
[02:08:51.840 --> 02:08:52.000]   Yeah.
[02:08:52.000 --> 02:08:53.120]   --and, you know, lucked out,
[02:08:53.120 --> 02:08:55.760]   but he just decided that he doesn't want that lifestyle,
[02:08:55.760 --> 02:08:58.800]   where some people are like really into it.
[02:08:58.800 --> 02:09:00.320]   That's what they-- they live to do.
[02:09:00.320 --> 02:09:00.880]   Right.
[02:09:00.880 --> 02:09:02.880]   I mean, I know so many people like our friend,
[02:09:02.880 --> 02:09:06.000]   Loui Klamur, who just can't sit still, right?
[02:09:06.000 --> 02:09:08.160]   It just got to do the next thing.
[02:09:08.160 --> 02:09:12.640]   Everybody, by the way, who is ever on MySpace knows who Tom is,
[02:09:12.640 --> 02:09:14.720]   because he's your first friend.
[02:09:14.720 --> 02:09:18.560]   He was the guy looking over his shoulder at the Blackboard,
[02:09:18.560 --> 02:09:19.280]   because that's--
[02:09:19.280 --> 02:09:20.000]   That was the picture.
[02:09:20.000 --> 02:09:24.240]   That was-- that was how little MySpace was.
[02:09:24.240 --> 02:09:27.200]   So anyway, Susan Wojcicki got into the whole thing
[02:09:27.200 --> 02:09:32.560]   by renting her garage to Larry and Sergey in 1998,
[02:09:32.560 --> 02:09:34.000]   when they were starting Google.
[02:09:34.000 --> 02:09:35.600]   They couldn't--
[02:09:35.600 --> 02:09:36.800]   A six-story.
[02:09:36.800 --> 02:09:38.400]   Huh?
[02:09:38.400 --> 02:09:39.440]   A six-story.
[02:09:39.440 --> 02:09:40.400]   It's a great story.
[02:09:40.400 --> 02:09:41.200]   Yeah.
[02:09:41.200 --> 02:09:41.680]   Yeah.
[02:09:41.680 --> 02:09:43.120]   No, I'll watch technology--
[02:09:43.120 --> 02:09:44.160]   Tech guys in the garage.
[02:09:44.160 --> 02:09:44.880]   Yeah.
[02:09:44.880 --> 02:09:46.480]   Well, they-- I mean, let's be honest.
[02:09:46.480 --> 02:09:47.760]   They started at Stanford.
[02:09:47.760 --> 02:09:50.160]   Stanford still gets a lot of money from Google every year.
[02:09:50.160 --> 02:09:52.160]   They started at Stanford, but then at some point,
[02:09:52.160 --> 02:09:53.760]   they said, well, we want to do our own business,
[02:09:53.760 --> 02:09:55.360]   so we got to move out of Stanford.
[02:09:55.360 --> 02:09:57.520]   Hey, look, over here, somebody's got a garage.
[02:09:57.520 --> 02:10:02.080]   And then Susan Wojcicki, her sister Anne runs 23 in me.
[02:10:03.120 --> 02:10:07.520]   Her mom is very-- I think very involved in community stuff.
[02:10:07.520 --> 02:10:10.480]   They're an interesting family that Wojcicki's.
[02:10:10.480 --> 02:10:14.480]   So anyway, 25 years at Google, she's retiring.
[02:10:14.480 --> 02:10:19.040]   Seriously, Susan, if you think you want to run a small podcast enterprise
[02:10:19.040 --> 02:10:21.680]   and just kind of take it from zero to a million,
[02:10:21.680 --> 02:10:25.040]   I know somebody who could help you out there,
[02:10:25.040 --> 02:10:27.840]   just in case you want to get back into it.
[02:10:30.640 --> 02:10:32.880]   Let's see here. What else is going on?
[02:10:32.880 --> 02:10:35.040]   Patch Tuesday was last Tuesday.
[02:10:35.040 --> 02:10:39.680]   I throw in another Windows story, just for you, Daniel.
[02:10:39.680 --> 02:10:40.880]   Even I don't know this one.
[02:10:40.880 --> 02:10:45.600]   I don't do Patch Tuesday.
[02:10:45.600 --> 02:10:49.840]   It's an endless parade, a litany of exploits.
[02:10:49.840 --> 02:10:50.880]   Who needs to know?
[02:10:50.880 --> 02:10:55.040]   Actually, the reason is, Jermaine, both Apple and Microsoft this week
[02:10:55.040 --> 02:10:58.160]   fixed zero days that were actively being exploited.
[02:10:59.680 --> 02:11:03.440]   And this is what's so scary about the modern world.
[02:11:03.440 --> 02:11:08.400]   Apple had a problem with WebKit that had to put out a patch for everything, iOS,
[02:11:08.400 --> 02:11:10.960]   iPadOS, MacOS.
[02:11:10.960 --> 02:11:14.960]   I think even WatchOS and TVOS were updated because WebKit had a
[02:11:14.960 --> 02:11:20.320]   essentially zero-click exploitable flaw that somebody with malicious content on a website,
[02:11:20.320 --> 02:11:22.160]   if you went to that website, you'd be owned.
[02:11:22.160 --> 02:11:23.120]   That be it. You're done.
[02:11:23.120 --> 02:11:26.160]   That is kind of as bad as it gets.
[02:11:26.160 --> 02:11:31.680]   Microsoft fixed three actively exploited zero-day vulnerabilities.
[02:11:31.680 --> 02:11:34.560]   It just keeps happening.
[02:11:34.560 --> 02:11:37.360]   I don't know. I don't know why.
[02:11:37.360 --> 02:11:43.840]   So do you think, game back to AI, AI is going to play an important role in doing this,
[02:11:43.840 --> 02:11:50.160]   both in looking at code and being able to spot these things as well as using AI to do
[02:11:50.160 --> 02:11:53.760]   like hacks on itself to see if it can find these flaws?
[02:11:53.760 --> 02:11:56.880]   Because that seems like a pretty good use of technology.
[02:11:56.880 --> 02:12:03.040]   Okay. So this is a good example of is that anthropomorphizing AI?
[02:12:03.040 --> 02:12:08.480]   If it's just a spicy auto-correct, how's it going to know if there's a security flaw?
[02:12:08.480 --> 02:12:10.480]   And yet we've seen it write code.
[02:12:10.480 --> 02:12:12.000]   Certainly that's what co-piling does.
[02:12:12.000 --> 02:12:15.360]   But it writes code without understanding the code.
[02:12:15.360 --> 02:12:17.120]   I don't know. That's an interesting question.
[02:12:17.120 --> 02:12:18.400]   But it does write it really well.
[02:12:18.400 --> 02:12:21.680]   Yeah, because it's copying from somebody else, basically.
[02:12:22.240 --> 02:12:30.160]   Dude, there's definitely AI's being built to help try to identify vulnerabilities.
[02:12:30.160 --> 02:12:33.840]   But vulnerabilities falls into this category.
[02:12:33.840 --> 02:12:37.840]   Same as writing opinion pieces or investigative journalism,
[02:12:37.840 --> 02:12:40.880]   where you have to think critically about like,
[02:12:40.880 --> 02:12:43.600]   "Oh, I'm going to push on the edge of this thing,
[02:12:43.600 --> 02:12:45.600]   and then I'm going to push on the edge of this thing,
[02:12:45.600 --> 02:12:48.080]   and then find the little hole."
[02:12:48.080 --> 02:12:51.920]   And that's just not a thing that the existing AI systems can do really well.
[02:12:52.160 --> 02:12:54.240]   They will be getting better and better at it.
[02:12:54.240 --> 02:12:58.640]   And I do think at some point, a lot of vulnerabilities could be patched by AI.
[02:12:58.640 --> 02:13:05.760]   But humans will always find some unique way to break stuff up, we'll say.
[02:13:05.760 --> 02:13:09.600]   And for now, at least, I think this is in one area where,
[02:13:09.600 --> 02:13:14.720]   like, I'm trying to write a list of like all the jobs where AI is not going to
[02:13:14.720 --> 02:13:18.720]   immediately replace a bunch of people, or at least like, could be much harder.
[02:13:18.720 --> 02:13:21.760]   And like investigative journalism on this is on this list.
[02:13:21.760 --> 02:13:26.880]   Like the person who is checking for those security vulnerabilities
[02:13:26.880 --> 02:13:28.640]   is probably another one.
[02:13:28.640 --> 02:13:33.600]   Just because that requires some unique level of human intuition.
[02:13:33.600 --> 02:13:37.920]   We do know that AI and chat, GBT specifically, can write malware.
[02:13:37.920 --> 02:13:41.680]   Steve Gibson talked about this on security now.
[02:13:41.680 --> 02:13:48.400]   Cybersecurity researchers from Checkpoint have observed a tool being used by cyber criminals
[02:13:49.680 --> 02:13:56.880]   to improve and sometimes build from scratch malware and ransomware using chat, GBT.
[02:13:56.880 --> 02:14:01.920]   It's not the most sophisticated malware, but it works.
[02:14:01.920 --> 02:14:05.440]   It's potentially a big problem.
[02:14:05.440 --> 02:14:05.920]   I don't know.
[02:14:05.920 --> 02:14:07.760]   Is that a different?
[02:14:07.760 --> 02:14:12.640]   I think it's different to create malware than to find bugs.
[02:14:12.640 --> 02:14:13.040]   I don't know.
[02:14:13.040 --> 02:14:14.640]   I don't know.
[02:14:14.640 --> 02:14:17.440]   Well, this is all going to be very interesting.
[02:14:17.440 --> 02:14:18.080]   I think you're right.
[02:14:18.080 --> 02:14:21.120]   I think we do have to do this week in AI or I'm sorry.
[02:14:21.120 --> 02:14:22.480]   What do we call it?
[02:14:22.480 --> 02:14:24.400]   Artificial intelligence.
[02:14:24.400 --> 02:14:25.600]   Definitely not evil.
[02:14:25.600 --> 02:14:30.960]   That's Google's tagline, by the way.
[02:14:30.960 --> 02:14:32.800]   I know it's definitely definitely.
[02:14:32.800 --> 02:14:33.360]   Definitely.
[02:14:33.360 --> 02:14:34.160]   You know, evil.
[02:14:34.160 --> 02:14:35.200]   Yeah, that's definitely.
[02:14:35.200 --> 02:14:39.600]   Before we wrap things up, I do want to show you something, a new sponsor that we have
[02:14:39.600 --> 02:14:44.160]   really been having fun playing with called Miro, M-I-R-O.
[02:14:44.160 --> 02:14:46.720]   Some of you may be familiar with Miro.
[02:14:46.720 --> 02:14:53.120]   The idea of Miro is it is a blank slate, a networked,
[02:14:53.120 --> 02:14:58.000]   shareable whiteboard that can be almost anything.
[02:14:58.000 --> 02:15:03.360]   So one of the hard things about talking about Miro is it's so amorphous.
[02:15:03.360 --> 02:15:04.400]   You could do anything with it.
[02:15:04.400 --> 02:15:07.200]   We're building an Ask the Tech Guy board right now.
[02:15:07.200 --> 02:15:10.560]   We're going to use it for preparing our show, Ask the Tech Guys.
[02:15:10.560 --> 02:15:12.960]   Mike and I are working on that right now.
[02:15:12.960 --> 02:15:16.160]   Miro can be so many things.
[02:15:16.160 --> 02:15:23.360]   What's great about it is it opens up and democratizes collaboration and input.
[02:15:23.360 --> 02:15:25.840]   If you've got a team with some people at home, some people at work,
[02:15:25.840 --> 02:15:30.320]   some people working nights, some people working days, you'll love Miro.
[02:15:30.320 --> 02:15:36.880]   It's a collaborative visual whiteboard that brings all your great work to one place.
[02:15:36.880 --> 02:15:41.680]   Everybody can access it from home in a hybrid workspace online,
[02:15:41.680 --> 02:15:42.960]   anytime of the day.
[02:15:42.960 --> 02:15:44.400]   So you could put an idea there.
[02:15:44.400 --> 02:15:46.320]   Somebody can respond to that later in the day.
[02:15:46.320 --> 02:15:53.040]   It gives product teams a perpetual space where they can drag and drop insights and data
[02:15:53.040 --> 02:15:54.080]   so that nothing's lost.
[02:15:54.080 --> 02:15:54.880]   Everything stays.
[02:15:54.880 --> 02:15:59.280]   And you can scale it up and down so you can zoom out to see the big picture,
[02:15:59.280 --> 02:16:01.040]   zoom in to see the details.
[02:16:01.040 --> 02:16:03.600]   Miro covers all kinds of use cases.
[02:16:03.600 --> 02:16:05.920]   You can build visual assets.
[02:16:05.920 --> 02:16:07.120]   You can present findings.
[02:16:07.920 --> 02:16:12.800]   It's fantastic for your next zoom call or Google Meet call because
[02:16:12.800 --> 02:16:17.280]   if you're doing a brainstorm on a call, you can have a timer, you can have polls.
[02:16:17.280 --> 02:16:22.400]   There's all these interactive features you can use to make that meeting more effective.
[02:16:22.400 --> 02:16:27.120]   You can build out your product vision on a Miro board by brainstorming with sticky notes and
[02:16:27.120 --> 02:16:30.000]   comments, live reactions, that voting tool.
[02:16:30.000 --> 02:16:32.160]   It helps you come to consensus quickly.
[02:16:32.160 --> 02:16:34.400]   You could set a timer, say we've got 30 minutes to do this.
[02:16:34.960 --> 02:16:38.880]   Express yourself in creative ways, bring the whole group together around one idea.
[02:16:38.880 --> 02:16:42.000]   Whether it's a quick wire frame, a drawing, a doodle, you do,
[02:16:42.000 --> 02:16:44.800]   forget the napkin, just do it on the Miro.
[02:16:44.800 --> 02:16:49.360]   Or you can do kind of a mood board with clipping and pasting in images and mockups
[02:16:49.360 --> 02:16:50.560]   from the Miro board.
[02:16:50.560 --> 02:16:53.920]   People who use Miro find it transformational.
[02:16:53.920 --> 02:16:59.120]   On average, they report saving 80 hours a user a year just by streamlining conversations
[02:16:59.120 --> 02:17:01.200]   and cutting down meeting times.
[02:17:02.320 --> 02:17:07.840]   As a result, Miro gives your team the chance to always stay connected to real-time information,
[02:17:07.840 --> 02:17:11.920]   gives the project managers the product leads a bird's eye view of the whole project.
[02:17:11.920 --> 02:17:14.880]   You can create a can band board and assign tasks.
[02:17:14.880 --> 02:17:15.920]   It's great for agile.
[02:17:15.920 --> 02:17:18.480]   Make sure nothing slips through the cracks.
[02:17:18.480 --> 02:17:20.240]   Get your first three boards for free.
[02:17:20.240 --> 02:17:22.480]   Start working better.
[02:17:22.480 --> 02:17:26.080]   MiroMiro.com/podcast.
[02:17:26.080 --> 02:17:30.320]   M-I-R-O.com/podcast.
[02:17:30.320 --> 02:17:33.520]   When you're at Miro, check out the Miroverse.
[02:17:33.520 --> 02:17:35.440]   This is where I got a lot of inspiration.
[02:17:35.440 --> 02:17:40.480]   And if you're kind of still wondering, as I was, what's going on, what is Miro?
[02:17:40.480 --> 02:17:41.520]   Go to the Miroverse.
[02:17:41.520 --> 02:17:46.800]   These are projects from the Miro community that they've uploaded that you can use as a
[02:17:46.800 --> 02:17:54.320]   template, a starting point, a SpongeBob retro, sustainability infused user journey mapping.
[02:17:54.320 --> 02:17:55.920]   You can use it for icebreakers.
[02:17:55.920 --> 02:17:59.840]   Some of my favorites are there's somebody from the UK government.
[02:18:00.400 --> 02:18:03.840]   They've uploaded a Harry Potter retrospective.
[02:18:03.840 --> 02:18:06.400]   I'm not sure what they're doing with it.
[02:18:06.400 --> 02:18:07.280]   But here's the thing.
[02:18:07.280 --> 02:18:12.000]   You can find out if you want by just adding that template, using that template for your own
[02:18:12.000 --> 02:18:13.760]   brainstorming.
[02:18:13.760 --> 02:18:17.440]   I just think it's fantastic.
[02:18:17.440 --> 02:18:21.040]   There's a Beatles retrospective here.
[02:18:21.040 --> 02:18:22.320]   Midnight sailboats.
[02:18:22.320 --> 02:18:26.480]   Get some ideas in the Miroverse.
[02:18:26.480 --> 02:18:27.760]   Start playing with Miro.
[02:18:28.400 --> 02:18:31.120]   I guarantee you, you will get inspiration.
[02:18:31.120 --> 02:18:33.760]   You will get ideas and you'll get it done.
[02:18:33.760 --> 02:18:38.160]   That's why we're going to develop this Miroboard for Ask the Tech guys, which is how we're
[02:18:38.160 --> 02:18:39.760]   going to plan the show from now on.
[02:18:39.760 --> 02:18:44.560]   And once we do, I'll share it with you and show you more.
[02:18:44.560 --> 02:18:47.840]   Right now it's kind of a secret, but that's coming.
[02:18:47.840 --> 02:18:48.720]   That's coming soon.
[02:18:48.720 --> 02:18:51.280]   So thank you, Miro, for your support.
[02:18:51.280 --> 02:18:53.840]   Thank you for helping us make our shows better.
[02:18:53.840 --> 02:18:55.840]   Let Miro help you make your business better.
[02:18:55.840 --> 02:18:59.920]   MiroMiro.com/podcast.
[02:18:59.920 --> 02:19:03.680]   And this Thursday, they're having it getting started with Miro Webinar.
[02:19:03.680 --> 02:19:06.080]   So this would be a good time to go in there.
[02:19:06.080 --> 02:19:06.800]   Miro.
[02:19:06.800 --> 02:19:07.520]   Thank you, Miro.
[02:19:07.520 --> 02:19:12.640]   Now, I want to remind you that this isn't the only show on the network.
[02:19:12.640 --> 02:19:14.960]   All week long, we've been having great fun.
[02:19:14.960 --> 02:19:18.320]   In fact, that's why Victor made us this little mini movie to share with you.
[02:19:18.320 --> 02:19:23.760]   They'll prove it hands-on photography, our community manager in the club,
[02:19:23.760 --> 02:19:28.320]   Twit. This is the, this is Ant Seal of Disapproval.
[02:19:28.320 --> 02:19:29.200]   No, thank you, sir.
[02:19:29.200 --> 02:19:32.160]   Which is you like grimacing and this is no thank you, sir.
[02:19:32.160 --> 02:19:34.400]   Then here's Ant Seal of Approval.
[02:19:34.400 --> 02:19:35.360]   Now that's legit.
[02:19:35.360 --> 02:19:41.520]   I work so much so hard on my neck and my traps and he cut my neck and my traps.
[02:19:41.520 --> 02:19:42.000]   Oh yeah.
[02:19:42.000 --> 02:19:42.480]   Come on.
[02:19:42.480 --> 02:19:44.800]   There should be some biceps at least in the gym.
[02:19:44.800 --> 02:19:46.400]   That's real terrible.
[02:19:46.400 --> 02:19:47.520]   I'm so sorry, Ant.
[02:19:47.520 --> 02:19:51.920]   Previously on Twit, hands-on Mac.
[02:19:51.920 --> 02:19:57.040]   Coming up on Hands-on Mac, I am going to show you how to provide some remote support for
[02:19:57.040 --> 02:20:01.680]   people who are struggling on their iPhones and their iPads and even their Macs.
[02:20:01.680 --> 02:20:02.800]   Mac break weekly.
[02:20:02.800 --> 02:20:04.960]   Jason got his home pods.
[02:20:04.960 --> 02:20:06.320]   Problem is not how they sound.
[02:20:06.320 --> 02:20:11.360]   Problem is that AirPlay is not reliable, that Siri is not reliable,
[02:20:11.360 --> 02:20:14.080]   and that the stereo pair system that they built is not reliable.
[02:20:14.080 --> 02:20:15.600]   Oh, no, that's disappointing.
[02:20:15.600 --> 02:20:16.800]   This week in Google.
[02:20:16.800 --> 02:20:21.040]   I think that generative AI like chat GPT is going to absolutely kill.
[02:20:21.760 --> 02:20:22.400]   Search.
[02:20:22.400 --> 02:20:25.760]   The reason is not because it's going to give better answers.
[02:20:25.760 --> 02:20:29.760]   The reason is that it's very trivial to build it into other things.
[02:20:29.760 --> 02:20:33.840]   It's going to be literally everywhere and every time you're doing anything,
[02:20:33.840 --> 02:20:37.600]   the generative AI will be right there with you as a partner.
[02:20:37.600 --> 02:20:38.000]   Twit.
[02:20:38.000 --> 02:20:39.920]   That's legit.
[02:20:39.920 --> 02:20:43.680]   Hey, that's our new slogan.
[02:20:43.680 --> 02:20:44.240]   Twit.
[02:20:44.240 --> 02:20:45.440]   Now that's legit.
[02:20:45.440 --> 02:20:46.880]   Thank you, AntPro for that one.
[02:20:46.880 --> 02:20:50.800]   Leo, we have to get this week at AI.
[02:20:50.800 --> 02:20:52.400]   I think I might have to do it.
[02:20:52.400 --> 02:20:53.120]   It has to happen.
[02:20:53.120 --> 02:20:56.080]   It's really dominated all of our shows for the last month.
[02:20:56.080 --> 02:21:01.440]   It's both fascinating and it's hard to understand or hard to decide for me anyway,
[02:21:01.440 --> 02:21:05.280]   whether it's gimmicky or it's transformative.
[02:21:05.280 --> 02:21:06.880]   You guys have done a good job.
[02:21:06.880 --> 02:21:09.360]   I've already told this to you twice.
[02:21:09.360 --> 02:21:11.040]   You have your answer after today.
[02:21:11.040 --> 02:21:15.680]   I've been on this thing for over 10 years.
[02:21:15.680 --> 02:21:17.520]   I've told you VR is trash.
[02:21:17.520 --> 02:21:19.360]   I've told you my first trash.
[02:21:19.360 --> 02:21:20.160]   I grew up with it.
[02:21:20.160 --> 02:21:20.880]   I've been right.
[02:21:20.880 --> 02:21:22.320]   We still aren't in Mars.
[02:21:22.320 --> 02:21:24.560]   So I'm telling you AI is real.
[02:21:24.560 --> 02:21:25.360]   Get on the band.
[02:21:25.360 --> 02:21:26.320]   Well, how could it be though?
[02:21:26.320 --> 02:21:26.720]   Wait a minute.
[02:21:26.720 --> 02:21:27.280]   Wait a minute.
[02:21:27.280 --> 02:21:30.800]   I'm going to say, Owen, you were right about the first three.
[02:21:30.800 --> 02:21:32.960]   Don't you worry that maybe you'll be wrong about this one,
[02:21:32.960 --> 02:21:38.160]   that this is just yet another BS idea coming from Big Tech in Silicon Valley.
[02:21:38.160 --> 02:21:40.560]   So here's the thing.
[02:21:40.560 --> 02:21:46.720]   So to that point, I love how so many crypto bros are now AI bros.
[02:21:46.720 --> 02:21:46.960]   Yeah.
[02:21:46.960 --> 02:21:49.040]   That's what scares me.
[02:21:49.040 --> 02:21:49.760]   It is scary.
[02:21:49.760 --> 02:21:51.360]   Like it's scary.
[02:21:51.360 --> 02:21:51.920]   Yeah.
[02:21:51.920 --> 02:21:53.280]   But you know what the difference is?
[02:21:53.280 --> 02:21:54.720]   One thing.
[02:21:54.720 --> 02:21:55.440]   Oh sure.
[02:21:55.440 --> 02:21:58.160]   You can make some money, but you could also lose your shirt.
[02:21:58.160 --> 02:22:01.600]   And if you're playing these games, you get to a prize.
[02:22:01.600 --> 02:22:03.200]   Well, that's the good news about opening AI.
[02:22:03.200 --> 02:22:04.880]   They're not asking for my money really.
[02:22:04.880 --> 02:22:05.360]   It's not.
[02:22:05.360 --> 02:22:10.000]   Hey, Uncle Leo, I wouldn't steer you wrong.
[02:22:10.000 --> 02:22:10.800]   I'm usually right.
[02:22:10.800 --> 02:22:12.800]   And if I'm not right, you know what happens?
[02:22:12.800 --> 02:22:13.840]   I shut up.
[02:22:13.840 --> 02:22:14.720]   I don't know now.
[02:22:14.720 --> 02:22:17.680]   Are you going to print a T-shirt that says AI is an ex-big thing
[02:22:17.680 --> 02:22:19.280]   and bury that in your yard?
[02:22:19.280 --> 02:22:20.400]   Come back five years.
[02:22:20.400 --> 02:22:21.200]   I might.
[02:22:21.200 --> 02:22:22.160]   I might have something.
[02:22:22.160 --> 02:22:22.320]   You should.
[02:22:22.320 --> 02:22:23.200]   You know.
[02:22:23.200 --> 02:22:23.600]   Yeah.
[02:22:23.600 --> 02:22:24.000]   Thank you.
[02:22:24.000 --> 02:22:24.560]   I'm right.
[02:22:24.560 --> 02:22:26.400]   Owen is a human crystal ball.
[02:22:26.400 --> 02:22:29.920]   And people should just pay him lots of money to forgive to tell
[02:22:29.920 --> 02:22:31.600]   what will and will not happen.
[02:22:31.600 --> 02:22:33.200]   But I totally agree.
[02:22:33.200 --> 02:22:36.720]   We are entering something fundamentally different here.
[02:22:36.720 --> 02:22:41.680]   This week, big week, we will be watching this with interest on Tuesday.
[02:22:41.680 --> 02:22:45.840]   The Supreme Court hears arguments in Gonzalez versus Google.
[02:22:45.840 --> 02:22:50.800]   This is a very important case that could jeopardize.
[02:22:50.800 --> 02:22:52.880]   I hope I'm not overstating this.
[02:22:52.880 --> 02:22:55.120]   I don't think I am the internet as we know it.
[02:22:55.120 --> 02:23:03.120]   Because the argument, the suit was filed by a family of a young woman who was
[02:23:03.120 --> 02:23:08.880]   killed in a terrorist attack in Paris in a cafe by Islamic terrorists.
[02:23:08.880 --> 02:23:15.040]   Their family's contention is that Google promotes terrorism algorithmically
[02:23:15.040 --> 02:23:15.760]   on YouTube.
[02:23:15.760 --> 02:23:17.520]   Maybe this is why Susan Wojciek quit.
[02:23:17.520 --> 02:23:20.400]   And they want their Suinam.
[02:23:20.400 --> 02:23:22.720]   They're saying, you got to stop doing that.
[02:23:22.720 --> 02:23:27.520]   Google says, we're protected by section 230 of the Communications Decency Act,
[02:23:27.520 --> 02:23:31.680]   which says, we have the right to moderate to delete,
[02:23:31.680 --> 02:23:37.680]   but not to be liable for stuff other people put on our platform.
[02:23:37.680 --> 02:23:41.200]   So we're not liable for terrorist videos that are put on YouTube.
[02:23:41.200 --> 02:23:42.640]   We're going to take them down as quick as we can.
[02:23:42.640 --> 02:23:45.040]   But you can't sue us because they exist.
[02:23:45.040 --> 02:23:46.640]   They also say we're not liable.
[02:23:46.640 --> 02:23:50.720]   If the algorithm recommends them, you've got to have algorithms in the world.
[02:23:50.720 --> 02:23:55.200]   In fact, a bunch of Reddit moderators anonymously,
[02:23:55.200 --> 02:23:57.840]   it's been quite a lot of them for the first time in my memory,
[02:23:57.840 --> 02:24:05.280]   to anonymously put up a Mika's brief saying, we use these recommendation engines,
[02:24:05.280 --> 02:24:07.680]   these tools to help us moderate.
[02:24:07.680 --> 02:24:12.160]   We need the power to moderate or Reddit would be a cesspool.
[02:24:13.120 --> 02:24:18.000]   I think that there's some of the argument is misguided that a lot of these people,
[02:24:18.000 --> 02:24:22.320]   by the way, the Biden administration has now filed a brief in support of limiting
[02:24:22.320 --> 02:24:29.360]   section 230 along with Ted Cruz of Texas and Josh Hawley of Missouri
[02:24:29.360 --> 02:24:35.360]   for different reasons, but both sides don't like section 230.
[02:24:35.360 --> 02:24:37.360]   But my problem is they're all looking at big tech.
[02:24:37.360 --> 02:24:41.440]   They're all saying, we'll see it allows Google or Twitter or Facebook to get away with murder.
[02:24:41.440 --> 02:24:45.920]   And they're forgetting us, the little guys who have communities.
[02:24:45.920 --> 02:24:51.200]   We have an IRC, we have a forums, we have a Mastinon instance, we have Discord chat.
[02:24:51.200 --> 02:24:57.680]   If we lost our reliability protection, which could very well happen in this case,
[02:24:57.680 --> 02:25:03.600]   we'd have to shut them down because I can't afford to face hundreds of frivolous lawsuits
[02:25:03.600 --> 02:25:06.640]   over content, not posted by me, but posted by our community.
[02:25:06.640 --> 02:25:09.680]   I would never understood.
[02:25:09.680 --> 02:25:12.400]   This will affect you too, if you have comments.
[02:25:12.400 --> 02:25:19.280]   I never understood how people don't understand why we have communities and how the internet works
[02:25:19.280 --> 02:25:21.120]   and how everything grew.
[02:25:21.120 --> 02:25:24.320]   I guess we could talk about, and this is where the Democrats come in,
[02:25:24.320 --> 02:25:26.320]   they actually want more moderation.
[02:25:26.320 --> 02:25:29.280]   And then the Republicans want less because freedom of speech.
[02:25:29.280 --> 02:25:35.680]   But the freedom of speech thing drives me nuts because it's like, I get it, but that's not
[02:25:35.680 --> 02:25:38.480]   a, the freedom of speech is not a constitutional issue here.
[02:25:38.480 --> 02:25:39.280]   It's not.
[02:25:39.280 --> 02:25:40.960]   We're talking about private property.
[02:25:40.960 --> 02:25:46.320]   And when you own a website or a service, it's private property.
[02:25:46.320 --> 02:25:50.880]   It's no different than if someone came into your house and started protesting,
[02:25:50.880 --> 02:25:53.200]   you could be like, please get out of my house.
[02:25:53.200 --> 02:25:55.360]   They can't go, well, freedom of speech.
[02:25:55.360 --> 02:25:59.840]   You can go like, yeah, private property trumps that, get out off of my property.
[02:25:59.840 --> 02:26:04.560]   But this whole thing with this section 230 is strange to me because it's like
[02:26:04.560 --> 02:26:09.280]   the same people who advocated for free market and private property and all this are now like,
[02:26:09.280 --> 02:26:10.720]   well, Twitter is so big.
[02:26:10.720 --> 02:26:12.000]   It should be a public square.
[02:26:12.000 --> 02:26:14.880]   It's like, it says, whoo, like it's not that big.
[02:26:14.880 --> 02:26:19.680]   Of all the social networks, Twitter is actually one of the lowest in terms of engagement and
[02:26:19.680 --> 02:26:21.280]   usage of people who are actually on it.
[02:26:21.280 --> 02:26:23.920]   It's not a talent square just because you say it is.
[02:26:23.920 --> 02:26:28.080]   And even so, we can't just snap our fingers and decide, well, now we're going to throw out all the
[02:26:28.080 --> 02:26:33.360]   laws and make that some sort of protected entity unless you want to nationalize it.
[02:26:33.360 --> 02:26:34.800]   But no one wants to know.
[02:26:34.800 --> 02:26:35.520]   No one wants that.
[02:26:35.520 --> 02:26:36.080]   Yeah.
[02:26:36.080 --> 02:26:36.720]   Yeah.
[02:26:36.720 --> 02:26:38.640]   I mean, now you're a socialist, right?
[02:26:38.640 --> 02:26:44.560]   It's like, it's such a weird thing that the people who defend capitalism and private property
[02:26:44.560 --> 02:26:48.000]   are now all of a sudden like, well, no, you know, this is a community thing.
[02:26:48.000 --> 02:26:51.680]   Like, really, we're all now part of this, like, socialist thing?
[02:26:51.680 --> 02:26:52.160]   I don't know.
[02:26:52.160 --> 02:26:52.640]   It's weird.
[02:26:52.640 --> 02:26:56.720]   I think part of this is that it's safe and easy for a politician from the left or the right
[02:26:56.720 --> 02:26:58.400]   to attack big tech right now.
[02:26:58.400 --> 02:26:59.440]   But watch out.
[02:26:59.440 --> 02:27:02.640]   Watch out because it's not just big tech.
[02:27:02.640 --> 02:27:08.160]   And, you know, you can make them your poster child for bad things.
[02:27:08.160 --> 02:27:11.920]   But frankly, section 230 gives us the right to moderate.
[02:27:11.920 --> 02:27:18.800]   If somebody says something that's bad in our chat or our forums, we can stop them.
[02:27:18.800 --> 02:27:19.680]   We can ban them.
[02:27:19.680 --> 02:27:20.480]   We can kick them out.
[02:27:20.480 --> 02:27:21.920]   We can delete their messages.
[02:27:21.920 --> 02:27:24.640]   And section 230 means they can't come back and sue us.
[02:27:25.200 --> 02:27:32.000]   So if you get rid of 230, you're losing the right to moderate as well as the right not to moderate.
[02:27:32.000 --> 02:27:34.720]   People post stuff on these sites.
[02:27:34.720 --> 02:27:36.640]   I don't want to be liable for it.
[02:27:36.640 --> 02:27:41.440]   And even if you're actively moderating as Google is on YouTube, they're taking those videos down.
[02:27:41.440 --> 02:27:42.800]   Now, what about the contention?
[02:27:42.800 --> 02:27:46.800]   And this is the Biden administration's contention that you can save 230.
[02:27:46.800 --> 02:27:50.640]   Just take out the part where it's making algorithmic recommendations.
[02:27:50.640 --> 02:27:52.080]   Should should that be protected?
[02:27:52.080 --> 02:27:55.520]   Right.
[02:27:55.520 --> 02:27:57.600]   And it's like really worth going deep into this case.
[02:27:57.600 --> 02:28:03.600]   I do recommend there's a YouTube channel LegalEagle and he's in depth.
[02:28:03.600 --> 02:28:04.160]   He's great.
[02:28:04.160 --> 02:28:04.880]   He's fantastic.
[02:28:04.880 --> 02:28:07.280]   He has a great video on this specific subject.
[02:28:07.280 --> 02:28:10.960]   But they're not going after all of section 230 in this lawsuit.
[02:28:10.960 --> 02:28:17.440]   They're going after they're trying to claim that YouTube is acting as a publisher because
[02:28:17.440 --> 02:28:18.240]   of the algorithm.
[02:28:18.240 --> 02:28:22.320]   So basically that because the algorithm is curating and recommending something,
[02:28:22.320 --> 02:28:24.480]   it should be outside of section 230.
[02:28:24.480 --> 02:28:24.480]   That's also what?
[02:28:24.480 --> 02:28:25.840]   But that is dangerous.
[02:28:25.840 --> 02:28:26.160]   Yeah.
[02:28:26.160 --> 02:28:29.440]   Well, that's what an editor does.
[02:28:29.440 --> 02:28:30.160]   That's what you do.
[02:28:30.160 --> 02:28:35.280]   So, but that's the point they're trying to say.
[02:28:35.280 --> 02:28:38.960]   If that's what an editor would do, an editor wouldn't have the right to do.
[02:28:38.960 --> 02:28:39.680]   Editors are protected.
[02:28:39.680 --> 02:28:40.080]   Publishing.
[02:28:40.080 --> 02:28:40.560]   No, editors are protected.
[02:28:40.560 --> 02:28:47.280]   It depends on the which, it depends if you say editor for a news website might be not protected.
[02:28:47.280 --> 02:28:49.760]   No news websites are also protected by 230.
[02:28:49.760 --> 02:28:52.000]   And that would be yes.
[02:28:52.000 --> 02:28:58.720]   Then if the Supreme Court in this case decided to strike it down, chaos all across everything.
[02:28:58.720 --> 02:28:59.200]   Oh, can you imagine?
[02:28:59.200 --> 02:29:00.160]   The internet.
[02:29:00.160 --> 02:29:05.520]   But, and I guess the question, and by the way, I argued at first in favor of this kind of limited
[02:29:05.520 --> 02:29:08.160]   interpretation of, well, you don't need algorithms.
[02:29:08.160 --> 02:29:11.920]   The problem is who defines what's a good algorithm or bad algorithm?
[02:29:11.920 --> 02:29:13.760]   And that's what Reddit moderators are saying.
[02:29:13.760 --> 02:29:16.720]   And that was what convinced me is, no, no, we use algorithms.
[02:29:16.720 --> 02:29:17.840]   We need algorithms.
[02:29:17.840 --> 02:29:19.600]   That's how we find the bad content.
[02:29:19.600 --> 02:29:20.880]   There's so much volume.
[02:29:20.880 --> 02:29:22.320]   We need algorithms.
[02:29:22.320 --> 02:29:27.040]   And if you ban them, you really cause huge problems.
[02:29:27.040 --> 02:29:29.200]   There is a very good go to tech dirt.
[02:29:29.200 --> 02:29:36.560]   They have a page on tech dirt about section 230 called somebody on the internet is wrong,
[02:29:36.560 --> 02:29:43.520]   in which they explain section 230 in great detail and why all of the things.
[02:29:44.640 --> 02:29:52.240]   The Biden administration, Josh Hawley and Ted Cruz say are not, not accurate, not, not real.
[02:29:52.240 --> 02:29:58.960]   And, and, and there are, there are, there are straw men because they don't like whatever they
[02:29:58.960 --> 02:29:59.760]   don't like.
[02:29:59.760 --> 02:30:02.640]   But we, I think, and I think I'm not alone in this.
[02:30:02.640 --> 02:30:05.040]   I think the EFF agrees, Epic agrees.
[02:30:05.040 --> 02:30:10.640]   We need section 230 or there will not be the only thing left on the internet will be big companies
[02:30:10.640 --> 02:30:11.680]   that can defend themselves.
[02:30:12.640 --> 02:30:14.640]   There will not be communities.
[02:30:14.640 --> 02:30:17.360]   It's very, very dangerous.
[02:30:17.360 --> 02:30:18.720]   Owen, do you have an opinion on this?
[02:30:18.720 --> 02:30:27.520]   The basic opinion is again, when you say things are scary and they have to have solutions,
[02:30:27.520 --> 02:30:31.920]   we don't have the proper tools to fix the things that need to be fixed.
[02:30:31.920 --> 02:30:32.400]   Yeah.
[02:30:32.400 --> 02:30:36.240]   And at least people are bringing attention to it because so many things,
[02:30:36.240 --> 02:30:38.640]   there's so much going on right now.
[02:30:38.640 --> 02:30:39.520]   There's something like this.
[02:30:39.520 --> 02:30:44.000]   Like we need to have focus on it and we need to make sure that we're doing the right things to
[02:30:44.000 --> 02:30:48.960]   ensure that things get worked out because right now it's like, stuff's flying at you.
[02:30:48.960 --> 02:30:51.440]   And then we forget to do the things that you need to get done.
[02:30:51.440 --> 02:30:53.920]   And then everybody will complain later when something goes wrong.
[02:30:53.920 --> 02:30:55.120]   So, yeah.
[02:30:55.120 --> 02:31:01.200]   The, there's, it's an open question how this Supreme Court would vote on this.
[02:31:01.200 --> 02:31:05.920]   And I'm sure there's some legal analyst who can tell me what they think each one would vote.
[02:31:05.920 --> 02:31:12.720]   But if it got struck down, I would expect Congress to scramble very quickly to write an updated law.
[02:31:12.720 --> 02:31:16.080]   But it would also have, it's tons of issues.
[02:31:16.080 --> 02:31:18.320]   It would, there would be chaos for sure.
[02:31:18.320 --> 02:31:18.960]   Yeah.
[02:31:18.960 --> 02:31:19.200]   Yeah.
[02:31:19.200 --> 02:31:22.480]   A lot of these people involved don't know tech.
[02:31:22.480 --> 02:31:23.760]   That's kind of the problem.
[02:31:23.760 --> 02:31:24.480]   And then you get law.
[02:31:24.480 --> 02:31:25.840]   That's the biggest problem.
[02:31:25.840 --> 02:31:26.480]   Yeah.
[02:31:26.480 --> 02:31:27.600]   That's the biggest problem.
[02:31:27.600 --> 02:31:30.080]   So it's, it's kind of scary.
[02:31:30.080 --> 02:31:33.120]   And I think part of it too, we came back to the publisher thing and that,
[02:31:33.120 --> 02:31:37.440]   that riles a lot of people, especially on the right, you know, it's like, well, you want it both
[02:31:37.440 --> 02:31:37.840]   ways.
[02:31:37.840 --> 02:31:44.080]   And like, yes, we do want it both ways as a publisher and a website because I want my,
[02:31:44.080 --> 02:31:46.240]   you know, first amendment right to publish things.
[02:31:46.240 --> 02:31:48.960]   But I also want to be able to moderate my community.
[02:31:48.960 --> 02:31:53.360]   And I think a lot of the, you know, people who are against this stuff have just never run a website
[02:31:53.360 --> 02:31:55.200]   or community or any kind of business.
[02:31:55.200 --> 02:32:00.640]   Because when you do, you want to curate the community so that they get the best experience.
[02:32:00.640 --> 02:32:04.160]   And if it's just total chaos and horrible things are being posted,
[02:32:04.160 --> 02:32:06.640]   no one wants to use your service because it's terrible.
[02:32:06.640 --> 02:32:06.880]   So.
[02:32:06.880 --> 02:32:09.600]   What is this called?
[02:32:09.600 --> 02:32:12.880]   One last thing, because like most politicians do not know what they're talking about this.
[02:32:12.880 --> 02:32:16.800]   One positive, there is a Congressman Don Baer.
[02:32:16.800 --> 02:32:18.320]   I think I'm saying his name correctly.
[02:32:18.320 --> 02:32:22.880]   He's actually 72 and he's getting a machine learning degree at university right now.
[02:32:22.880 --> 02:32:23.680]   So bravo.
[02:32:23.680 --> 02:32:29.680]   Well, and Ron, what wrote this during the process of negotiating the Communications
[02:32:29.680 --> 02:32:35.440]   Decency Act, he and others were concerned about the impact it would have on free discussion on
[02:32:35.440 --> 02:32:35.920]   the internet.
[02:32:35.920 --> 02:32:38.000]   So he wrote and it's, it's brief.
[02:32:38.000 --> 02:32:38.960]   It's beautiful.
[02:32:38.960 --> 02:32:42.400]   It's like a constitutional clause.
[02:32:42.400 --> 02:32:48.240]   It's just very simple, but it's 23 words that protect the internet.
[02:32:48.240 --> 02:32:50.320]   And it's so important.
[02:32:50.320 --> 02:32:52.880]   It's not the only case the Supreme Court will be hearing this week.
[02:32:52.880 --> 02:32:55.840]   The next day they're hearing another case,
[02:32:57.520 --> 02:33:05.760]   relatives of now, now Ross, Al-Asaf, a Jordanian citizen was killed in an Islamic attack.
[02:33:05.760 --> 02:33:11.840]   In Istanbul in 2017, his family is accusing Twitter, Google and Facebook of aiding and
[02:33:11.840 --> 02:33:14.720]   abetting the spread of militant Islamic ideology.
[02:33:14.720 --> 02:33:20.480]   This doesn't specifically address 230, but it's the very next day I will be listening with
[02:33:20.480 --> 02:33:23.120]   interest to both days arguments.
[02:33:24.000 --> 02:33:28.080]   And of course, we won't know until the court rules in the spring, but
[02:33:28.080 --> 02:33:30.560]   it makes me very nervous.
[02:33:30.560 --> 02:33:36.000]   And when I see President Biden, Josh Hawley and Ted Cruz in the same boat,
[02:33:36.000 --> 02:33:37.760]   I get very, very afraid.
[02:33:37.760 --> 02:33:45.520]   This is this, if you get both the left and the right agreeing on something like this,
[02:33:45.520 --> 02:33:48.560]   I think there's a great, great chance it's going to happen.
[02:33:49.680 --> 02:33:55.680]   It reminds me of politicians running over CDs and telling me that crime is because people make
[02:33:55.680 --> 02:33:57.200]   music and rap music.
[02:33:57.200 --> 02:33:59.520]   The video games are going to destroy.
[02:33:59.520 --> 02:34:03.040]   So the people living in impoverished neighborhoods,
[02:34:03.040 --> 02:34:06.160]   reporting through music what they're living through,
[02:34:06.160 --> 02:34:09.600]   is not the problem of the fact that they're living in poverty.
[02:34:09.600 --> 02:34:10.080]   Exactly.
[02:34:10.080 --> 02:34:11.440]   And in those dangerous situations.
[02:34:11.440 --> 02:34:11.600]   Thank you.
[02:34:11.600 --> 02:34:15.600]   It's the fact that they make music that sounds cool telling people,
[02:34:15.600 --> 02:34:17.040]   hey, we're living in hell here.
[02:34:17.040 --> 02:34:17.280]   Yeah.
[02:34:17.280 --> 02:34:18.080]   You can do some help.
[02:34:18.640 --> 02:34:19.840]   That's the actual problem.
[02:34:19.840 --> 02:34:21.440]   And again, it's been the same thing.
[02:34:21.440 --> 02:34:24.800]   That's why I wish we had term limits in every single position.
[02:34:24.800 --> 02:34:28.160]   And I love young Lio, but I'm trying to cut people off at 65.
[02:34:28.160 --> 02:34:31.440]   At the legal age of retirement, you can no longer be a politician.
[02:34:31.440 --> 02:34:34.880]   And yes, everybody's got to go to school and take some kind of test.
[02:34:34.880 --> 02:34:39.040]   As I have to say, it's been pointed out, there are plenty of people over 65
[02:34:39.040 --> 02:34:41.440]   who are taking the time, are learning the information.
[02:34:41.440 --> 02:34:43.360]   You say plenty.
[02:34:43.360 --> 02:34:46.800]   There's a fight between you.
[02:34:46.800 --> 02:34:48.240]   Saying I shouldn't be able to serve.
[02:34:48.240 --> 02:34:50.080]   I think I have a pretty good sense of what this is.
[02:34:50.080 --> 02:34:54.800]   I'm saying you've got a pretty good gig, and you're not going to go be a politician.
[02:34:54.800 --> 02:34:55.680]   I'm not running for Congress.
[02:34:55.680 --> 02:34:56.160]   You're right.
[02:34:56.160 --> 02:35:00.080]   I'm saying that you're probably one of the 13th wonders of the world.
[02:35:00.080 --> 02:35:02.320]   The fact that you keep so much knowledge in your brain,
[02:35:02.320 --> 02:35:05.760]   you should be doing something wrong for society to be honest and fair.
[02:35:05.760 --> 02:35:07.120]   But right now, you got a good gig.
[02:35:07.120 --> 02:35:08.320]   I'm not trying to put you out there.
[02:35:08.320 --> 02:35:09.760]   You won't be excited.
[02:35:09.760 --> 02:35:10.960]   Do your job.
[02:35:10.960 --> 02:35:11.840]   I don't think so.
[02:35:11.840 --> 02:35:16.400]   I don't think well, as an old guy, age shouldn't be the litmus test
[02:35:16.400 --> 02:35:17.920]   for knowledge of technology.
[02:35:17.920 --> 02:35:19.680]   Yeah, I agree.
[02:35:19.680 --> 02:35:22.880]   Well, it shouldn't be, but I'm trying to put some parameters in here.
[02:35:22.880 --> 02:35:24.320]   I'm trying to get the thing.
[02:35:24.320 --> 02:35:25.760]   I'm trying to, you might even be AI.
[02:35:25.760 --> 02:35:27.120]   Good gig is over 65.
[02:35:27.120 --> 02:35:28.320]   I can't just move.
[02:35:28.320 --> 02:35:30.160]   Steve Jobs would be over 65.
[02:35:30.160 --> 02:35:32.800]   I mean, there are not politicians.
[02:35:32.800 --> 02:35:35.200]   It's my generation that changed the world.
[02:35:35.200 --> 02:35:36.800]   So you could thank me very much.
[02:35:36.800 --> 02:35:39.040]   My elder friend.
[02:35:39.040 --> 02:35:40.000]   You're a friend of war.
[02:35:40.000 --> 02:35:41.040]   You're not good.
[02:35:41.040 --> 02:35:42.960]   We'll be on the porch playing checkers.
[02:35:42.960 --> 02:35:44.160]   Change the world thing.
[02:35:44.160 --> 02:35:45.200]   Oh my goodness.
[02:35:45.200 --> 02:35:47.280]   You're a good generation.
[02:35:47.280 --> 02:35:48.240]   Change the world.
[02:35:48.240 --> 02:35:48.800]   We did.
[02:35:48.800 --> 02:35:51.200]   Okay.
[02:35:51.200 --> 02:35:51.760]   Okay.
[02:35:51.760 --> 02:35:52.080]   Okay.
[02:35:52.080 --> 02:35:52.800]   Boomer.
[02:35:52.800 --> 02:35:53.120]   All right.
[02:35:53.120 --> 02:35:55.040]   Do this.
[02:35:55.040 --> 02:35:55.520]   What happened?
[02:35:55.520 --> 02:35:57.040]   Every generation.
[02:35:57.040 --> 02:35:58.480]   I'm going to hear what's true.
[02:35:58.480 --> 02:35:59.040]   Exactly.
[02:35:59.040 --> 02:35:59.760]   I'm going to hear what.
[02:35:59.760 --> 02:36:03.360]   You know, you used to have kids leave mine in the coal mines.
[02:36:03.360 --> 02:36:05.920]   And now we don't, you know, remember how kids have to farm.
[02:36:05.920 --> 02:36:08.720]   And they couldn't reach a lot of people change the world on Kaleel.
[02:36:08.720 --> 02:36:08.960]   Okay.
[02:36:08.960 --> 02:36:10.320]   Y'all had a little part to it.
[02:36:10.320 --> 02:36:12.800]   But right now I'm trying to put y'all out the pastor
[02:36:12.800 --> 02:36:14.320]   in the political sphere.
[02:36:14.320 --> 02:36:15.840]   30, whatever the age that starts.
[02:36:15.840 --> 02:36:17.120]   18 to 65.
[02:36:17.120 --> 02:36:17.680]   That's 65.
[02:36:17.680 --> 02:36:18.880]   Oh, no, live your life.
[02:36:18.880 --> 02:36:20.560]   So I'm gonna cast our empire.
[02:36:20.560 --> 02:36:22.080]   You know, I'll do what you're doing.
[02:36:22.080 --> 02:36:22.640]   All right.
[02:36:22.640 --> 02:36:25.920]   Solution for Senate every six years.
[02:36:25.920 --> 02:36:28.080]   It is only people over 65.
[02:36:28.080 --> 02:36:31.680]   And then the next six years, it's only people under 45.
[02:36:31.680 --> 02:36:33.120]   And then you just alternate it.
[02:36:33.120 --> 02:36:33.920]   Oh God.
[02:36:33.920 --> 02:36:33.920]   I can do it.
[02:36:33.920 --> 02:36:34.400]   Is that real?
[02:36:34.400 --> 02:36:37.040]   All of those draconian solutions.
[02:36:37.040 --> 02:36:38.480]   Yeah.
[02:36:38.480 --> 02:36:39.520]   Take the glasses off, Ben.
[02:36:39.520 --> 02:36:40.560]   That sounded terrible.
[02:36:40.560 --> 02:36:42.240]   I know it's terrible.
[02:36:42.240 --> 02:36:43.120]   That was not Ben.
[02:36:43.120 --> 02:36:44.560]   And don't take the glasses off.
[02:36:44.560 --> 02:36:45.440]   That sounds bad.
[02:36:45.440 --> 02:36:45.840]   All right.
[02:36:45.840 --> 02:36:47.680]   I've surrendered the glasses.
[02:36:47.680 --> 02:36:48.960]   It is a terrible idea.
[02:36:48.960 --> 02:36:50.400]   I'll tell you who's ruining the world.
[02:36:50.400 --> 02:36:54.960]   You kids with your TikTok, the TikTok Kia Challenge,
[02:36:54.960 --> 02:37:00.480]   which has led to hundreds of car thefts nationwide,
[02:37:00.480 --> 02:37:04.800]   and including at least 14 reported crashes and eight fatalities.
[02:37:04.800 --> 02:37:09.440]   According to NHTSA, the National Highway Traffic Safety Administration,
[02:37:10.400 --> 02:37:14.400]   thieves known as the Kia Boys with a Z,
[02:37:14.400 --> 02:37:20.880]   sounds like a rap start to me, would post instructional videos about how to bypass.
[02:37:20.880 --> 02:37:27.920]   It was actually trivially easy to bypass the vehicle's security system using a USB cable.
[02:37:27.920 --> 02:37:35.440]   Many 2015 to 2019 Hyundai and Kia vehicles have lack of something called electronic immobilizers
[02:37:35.440 --> 02:37:39.200]   that prevent the thieves from just breaking in and bypassing the ignition.
[02:37:40.160 --> 02:37:44.800]   Of course, in my day, with a screwdriver, you just pull the ignition wires out from under
[02:37:44.800 --> 02:37:48.880]   the steering column and you could jumpstart the car, but that's another story.
[02:37:48.880 --> 02:37:52.320]   You open the door with a hanger and you get in there anyway.
[02:37:52.320 --> 02:37:58.240]   The feature is standard equipment on other vehicles from other manufacturers.
[02:37:58.240 --> 02:38:03.840]   Hyundai and Kia are now offering free software updates for millions of their cars
[02:38:04.880 --> 02:38:11.920]   to help protect. They're going to update the theft alarm software logic. This is their fix.
[02:38:11.920 --> 02:38:18.640]   Then we'll make the alarm sound instead of for just 30 seconds for a minute.
[02:38:18.640 --> 02:38:27.440]   A Philadelphia Eagles player had his car stolen the day of the San Francisco game.
[02:38:27.440 --> 02:38:31.520]   Someone stole his like high-end Kia Genesis, whatever it was.
[02:38:31.520 --> 02:38:37.600]   I'm just thinking to myself, "Okay, so you're going to make the alarm. I'm working, bro. I'm at work
[02:38:37.600 --> 02:38:42.000]   overnight. It ain't our shift in the mood of an Amazon warehouse. I can't even go to the bathroom.
[02:38:42.000 --> 02:38:47.120]   You think I'm going to hear this alarm going off for half an hour an hour? I'm not. I need it fixed.
[02:38:47.120 --> 02:38:54.240]   I need to problem fix it. Every car could be stolen. Every car could be stolen. You get the
[02:38:54.240 --> 02:38:57.600]   right equipment. You could roll with somebody's crib with a signal jammer and then partly the
[02:38:57.600 --> 02:39:01.760]   signal from the key to the right of the door. I'm not telling them how to do it. I'm just saying
[02:39:01.760 --> 02:39:05.760]   it can be done. Just get it garage. Get a garage. My friends get a garage.
[02:39:05.760 --> 02:39:12.720]   If you want to get Owen's tips on how to steal cars, just get a single block or go to his TikTok.
[02:39:12.720 --> 02:39:20.480]   This is an interesting thing of how TikTok can make an entire tread so quickly and spread
[02:39:20.480 --> 02:39:24.560]   knowledge so quickly. Good knowledge and bad knowledge. It doesn't matter. Knowledge is knowledge.
[02:39:25.440 --> 02:39:29.600]   It's powerful. It's an amazing platform for that.
[02:39:29.600 --> 02:39:36.480]   Let's see. What else? I'm getting the seeds and the stems here, those stories that have
[02:39:36.480 --> 02:39:45.840]   filtered down to the bottom. Sam Bankman-Freeze is in trouble. He's the guy who founded FTX. He's
[02:39:45.840 --> 02:39:50.800]   out on a quarter million dollars bail, staying with mom and dad on the Stanford campus.
[02:39:51.920 --> 02:39:58.800]   But one of the provisions of his bail was he couldn't use signal or any encrypted messenger
[02:39:58.800 --> 02:40:04.000]   because they were afraid he would attempt to tamper with witnesses. Now he's in trouble for
[02:40:04.000 --> 02:40:13.920]   using a VPN. At least twice has used a VPN to surf the net. His lawyers say he wanted to watch
[02:40:13.920 --> 02:40:21.680]   the freaking Super Bowl. He couldn't watch it because it was the February 12th. It was one of
[02:40:21.680 --> 02:40:31.280]   the days. He couldn't watch it, but he had some sort of NFL+ subscription that he was going to use.
[02:40:31.280 --> 02:40:36.560]   You need to use a VPN for it. The government says, "No, no, I can't."
[02:40:36.560 --> 02:40:42.240]   I'd just leave it out for free in that app. You're going to watch it on Fox or Free. I don't know.
[02:40:42.240 --> 02:40:50.880]   I have only one thing to say for this story, which is if he was not rich and white, his bail
[02:40:50.880 --> 02:40:56.880]   would have been revoked and he'd be in jail and he should be in jail right now.
[02:40:56.880 --> 02:41:02.640]   Keep the glasses on, Ben. Keep the glasses on. Smart man. He's a smart man, rich and body.
[02:41:02.640 --> 02:41:07.840]   That's right. But if you're going to get arrested, it does help to be rich and white. I'm just saying.
[02:41:07.840 --> 02:41:13.440]   Coming up with a quarter billion dollars bail is, you know, that's a good start.
[02:41:13.440 --> 02:41:16.480]   Where'd that money come from? I thought he was bankrupt.
[02:41:17.440 --> 02:41:21.280]   There's some guarantors that you think they went public. They're like other members of the
[02:41:21.280 --> 02:41:26.400]   Stanford faculty or something. I think I saw that. Yeah. His parents are professors at Stanford.
[02:41:26.400 --> 02:41:37.680]   Buzzfeed has launched quizzes created by AI. Infinity quizzes. Are you ready for an infinity quiz?
[02:41:37.680 --> 02:41:42.720]   Six quizzes for playing on Valentine's Day one sponsored by an advertiser, one just for premium
[02:41:42.720 --> 02:41:51.120]   subscribers. The quizzes are powered by buzzy the robot using open AI's API trained in a blend of
[02:41:51.120 --> 02:41:56.720]   text code and information prior to June 2021. So don't ask questions from the old time. The early
[02:41:56.720 --> 02:42:02.400]   the new times here are some just in case you want to take them. Create your own romcom.
[02:42:02.400 --> 02:42:09.520]   Generate a breakup text. Date your celebrity crush. Find your soulmate. It's a houseplant.
[02:42:10.080 --> 02:42:16.320]   Create your own cinematic universe for you and your friends or create a cult for you and your friends.
[02:42:16.320 --> 02:42:20.400]   I can date my celebrity crush. Would you like to?
[02:42:20.400 --> 02:42:23.520]   Yeah. Ryan Gosling is my guy.
[02:42:23.520 --> 02:42:29.040]   I just I just want to be his friend. Oh, like the guy.
[02:42:29.040 --> 02:42:32.720]   Crushes could just be friend. Yeah. Crushes could be friendships. Yeah.
[02:42:32.720 --> 02:42:37.360]   Uh, data brokers. I might be a little bit more.
[02:42:37.360 --> 02:42:41.920]   Data brokers. You might want to watch out for this then are selling your mental health data
[02:42:41.920 --> 02:42:48.320]   and it's not illegal. One company advertised. This is from an article in the Washington Post.
[02:42:48.320 --> 02:42:52.480]   One company advertised the names and home addresses of people with depression, anxiety,
[02:42:52.480 --> 02:42:57.600]   post traumatic stress or bipolar disorder. Another sold at database featuring thousands
[02:42:57.600 --> 02:43:04.960]   of aggregated mental health records starting at $275 per thousand ailment contacts.
[02:43:06.560 --> 02:43:12.160]   Wow. Data brokers. This is a study published this week by a team at Duke University's
[02:43:12.160 --> 02:43:16.960]   Sanford School of Public Policy. They contacted data brokers to say, Hey,
[02:43:16.960 --> 02:43:23.600]   what kind of mental health information could I buy? She found 11 companies that research
[02:43:23.600 --> 02:43:28.880]   are joined. Cam found 11 companies willing to sell bundles of data that include information on what
[02:43:28.880 --> 02:43:34.400]   anti-depressants people were taking, whether they struggle with insomnia or attention issues,
[02:43:35.120 --> 02:43:39.760]   details on other medical ailment ailments, including Alzheimer's disease or bladder control difficulties.
[02:43:39.760 --> 02:43:46.000]   Some of the data was offered in aggregate form that would, for instance, allow a buyer to know
[02:43:46.000 --> 02:43:50.560]   a rough estimate of how many people in an individual zip code might be depressed.
[02:43:50.560 --> 02:43:56.880]   But other brokers offered personally identifiable data names, addresses, incomes.
[02:43:56.880 --> 02:44:02.400]   One data broker sales representative pointed to list named anxiety sufferers.
[02:44:04.320 --> 02:44:08.640]   Consumers get that. Oh my God. If some employers abuse that. Yes.
[02:44:08.640 --> 02:44:14.400]   Consumers with clinical depression in the United States, you could even get a sample spreadsheet.
[02:44:14.400 --> 02:44:22.160]   This feels like it should be illegal. It isn't. How is it not illegal?
[02:44:22.160 --> 02:44:26.640]   The law doesn't protect the information. What's up about the HIPAA? It's protected by HIPAA.
[02:44:26.640 --> 02:44:33.280]   Your doctor, your hospital health entities can't share it, but it doesn't protect the same
[02:44:33.280 --> 02:44:38.400]   information when it's sent anywhere else, including app makers, companies.
[02:44:38.400 --> 02:44:47.520]   So a pharmacy, maybe. I guess a pharmacy might be protected by HIPAA, but the big culprit is
[02:44:47.520 --> 02:44:53.840]   going to be apps. That's what I'm saying. This is, talk about what Owen mentioned earlier about
[02:44:53.840 --> 02:45:00.960]   spying on yourself. This is what that is. This is people entering data and getting back to being
[02:45:00.960 --> 02:45:04.640]   chat. People are having these conversations, but they're entering a lot of information.
[02:45:04.640 --> 02:45:10.000]   Now, I'm not saying Microsoft's going to identify you by name, but you're voluntarily putting a lot
[02:45:10.000 --> 02:45:15.200]   about your own psychology out there. When you use these chat things and when you use these medical
[02:45:15.200 --> 02:45:19.680]   apps, and there's so many of them now, you're putting your information out there. It's not
[02:45:19.680 --> 02:45:24.080]   surprising that this happens, but you're giving out a lot of information that's not necessary
[02:45:24.080 --> 02:45:27.600]   protected by your doctor that you used to have that same relationship.
[02:45:28.560 --> 02:45:32.880]   And I don't have any foil right now, but these are the moments where when people make fun of me
[02:45:32.880 --> 02:45:37.600]   for living in a black site, or when they go on my Twitter, it says I was born in 1949,
[02:45:37.600 --> 02:45:41.600]   or they go on Facebook and it says that I'm a white man that lives in New York City and I was
[02:45:41.600 --> 02:45:47.760]   born in 1942. These are the reasons why all of my information is scattered across ethos.
[02:45:47.760 --> 02:45:52.000]   That's my phone's got my name, my house, and my name, my car, my name. They're not going to get
[02:45:52.000 --> 02:45:57.520]   me out here. I'm not snitching on myself. Two of the smartest, two of the smartest people I know,
[02:45:58.160 --> 02:46:04.320]   you Owen and Dr. Father Robert Ballisare, both fuzz your identities on the net.
[02:46:04.320 --> 02:46:12.160]   Robert goes to huge lengths to completely fuzz information. And it's usually the people,
[02:46:12.160 --> 02:46:17.680]   honestly, it's usually the people who have a little bit of a dark streak to them anyway and
[02:46:17.680 --> 02:46:23.200]   kind of know what hackers are capable of and are aware of what can be done. Father Robert goes
[02:46:23.200 --> 02:46:29.280]   to Define all the time and he knows I don't trust Americans. I know better. I know I know what people
[02:46:29.280 --> 02:46:33.520]   can do on the Internet. I just want to say, oh, you got your address. Yeah, you look great for
[02:46:33.520 --> 02:46:38.880]   somebody 71 years old. I just want to say that. Hey, incredible. You know, black, black don't
[02:46:38.880 --> 02:46:42.560]   crack. Just so you know, that's what I say. Every time my birthday pops up and they ask me about
[02:46:42.560 --> 02:46:46.480]   it. But you don't Facebook on white. So I got a finagle that a little bit. But you know, Owen,
[02:46:46.480 --> 02:46:51.120]   they see the name and they mostly I got to say, I love all the people who say that. People who say
[02:46:51.120 --> 02:46:58.080]   that out loud are mostly white. So you fit right in with the profile. Yeah. My favorite thing is
[02:46:58.080 --> 02:47:03.200]   advertisers. I get like, you know, single and 70 is my jam. You know, there's a lot of grandmas
[02:47:03.200 --> 02:47:08.160]   out there. I mean, I love to add that. This is this great targeting. They're missing the boat on me.
[02:47:08.160 --> 02:47:13.440]   That's in the boat. No money made. Mr. Daniel Rubino, it's always a pleasure to have you on
[02:47:13.440 --> 02:47:17.840]   editor in chief windows central plug something. Tell us about your great podcast.
[02:47:18.880 --> 02:47:25.280]   Yeah, we need a podcast every Fridays 130 p.m. Eastern time on our YouTube channel. And that's a live
[02:47:25.280 --> 02:47:30.880]   video broadcast. Of course, then it goes out to all the podcast affiliates afterwards. And yeah,
[02:47:30.880 --> 02:47:36.880]   and other than that, we're just doing a ton of work on what essential for Microsoft Xbox windows
[02:47:36.880 --> 02:47:43.120]   AI. We're going hard on the AI stuff a lot. Gaming gaming is always going to be big. And so we're
[02:47:43.120 --> 02:47:48.320]   covering that a lot. It's a it's a fun year. I have to bet. Like this is one of the more interesting
[02:47:48.320 --> 02:47:52.640]   years to be covering tech and some are excited about it. There was a long period work being a
[02:47:52.640 --> 02:47:58.240]   Microsoft reporter was like the fate worse and death. Sure. Yeah, that's not true anymore. But
[02:47:58.240 --> 02:48:02.000]   the last four or five years have been really exciting and interesting. Even though we don't
[02:48:02.000 --> 02:48:07.440]   have windows phone. Yeah, that's kind of sad. Yeah. Daniel, always a question.
[02:48:07.440 --> 02:48:10.480]   Daniel, show us your big fat muff.
[02:48:10.480 --> 02:48:13.840]   What?
[02:48:16.080 --> 02:48:21.040]   Anyone else needs to just an audio. It's gonna be really good. Yeah. That big. What is it called?
[02:48:21.040 --> 02:48:25.360]   Exactly what he was talking about. Yeah. What's that called? It's on his microphone. What is that?
[02:48:25.360 --> 02:48:31.440]   The chaotica chaotica. I they call it. It's good. I don't know what it's doing for you, but it's
[02:48:31.440 --> 02:48:36.960]   working. Whatever. I still say it's the Chinese by Berlin. It looks like it. Yeah. I'll buy that,
[02:48:36.960 --> 02:48:43.520]   too. And thanks for the eye contact. I have to say it makes you at first if you don't know it's
[02:48:43.520 --> 02:48:46.880]   if you know it, it's a little creepy, but at first, but if you don't know it,
[02:48:46.880 --> 02:48:51.520]   it makes you seem much more engaging and kind of sure I'm kind of falling in love with you if you
[02:48:51.520 --> 02:48:58.240]   don't. I just look deep into your looking right in my eyes. I have a slightly crossed eye. You
[02:48:58.240 --> 02:49:04.160]   think it would fix that? Yeah, probably would. Because like right now, like I'm actually closing
[02:49:04.160 --> 02:49:10.960]   one eye. What? No, you are not. What? No, you are not. No, it can do. That's great. And then
[02:49:10.960 --> 02:49:15.040]   there's eyebrowing up. Do it again. Do it again. Put it back. Do it again. So you could tell he's
[02:49:15.040 --> 02:49:19.680]   closing one eye because his eyebrows on the on the right is going up. Yeah. Yeah. So good.
[02:49:19.680 --> 02:49:29.360]   That's crazy. Wow. Mr. Eye contact Daniel Rubino, ladies and gentlemen. Ben Parr. He's in the big
[02:49:29.360 --> 02:49:38.160]   apple. Taking taking class at the WeWork University. Thank you for finding a spot to to join us. I
[02:49:38.160 --> 02:49:44.880]   appreciate it. I just want to talk about AI all the time. And I'm going to plug my substack now.
[02:49:44.880 --> 02:49:51.280]   Ben Parr.substack.com. I talk about AI. And it's free. And it's free. Yeah. Every time I hear
[02:49:51.280 --> 02:49:56.400]   substack, I go, Oh, another subscription. But no, it's free. You should start charging for that
[02:49:56.400 --> 02:50:02.640]   eventually. But lots of good stuff. A really good thing to follow on a substack, the social
[02:50:02.640 --> 02:50:08.160]   analyst by Ben Parr. And octane AI, are you going to have a big reveal soon?
[02:50:08.160 --> 02:50:15.840]   Big reveal soon building cool AI things. Just follow me somewhere. I will put it in my newsletter.
[02:50:15.840 --> 02:50:20.320]   Do the newsletter. Do the new subject. Very cool. There's some very cool AI things,
[02:50:20.320 --> 02:50:27.600]   like legitimately cool new stuff. My co founder, Matt Schlecht has been engineering some crazy
[02:50:27.600 --> 02:50:32.240]   stuff with open AI for a long time and some other new things. So yeah, I'm excited to actually
[02:50:32.240 --> 02:50:36.240]   talk about it at some point at Ben Parr and the Twitter. Don't forget his podcast business envy
[02:50:36.240 --> 02:50:43.360]   show.com. So nice to see you, Ben. Thank you for being here with us today. We do tweet every
[02:50:43.360 --> 02:50:48.240]   Sunday afternoon, 2 p.m. Right after asked the tech guys 2 p.m. Pacific 5 p.m. Eastern 2200
[02:50:48.240 --> 02:50:52.800]   UTC. I mentioned that because you can watch us live if you want. We'll put it as a podcast.
[02:50:52.800 --> 02:50:57.520]   You can listen at your leisure. But if you want to watch live, that's when we're on two to five.
[02:50:57.520 --> 02:51:02.800]   They're about Eastern time. The live stream is a Twitter TV slash live that can point you to
[02:51:02.800 --> 02:51:07.600]   YouTube and other places. But start there. If you're watching live, you probably want to chat with
[02:51:07.600 --> 02:51:16.560]   the other viewers. Couple places to do that. IRC IRC dot twit dot TV. That's open to all. Or if
[02:51:16.560 --> 02:51:22.160]   you're a club twit member, which I highly recommend, you can join us in the discord. Discord is a
[02:51:22.160 --> 02:51:28.000]   great community. Look at. Oh, Doc is in there promoting his t-shirt. Scroll, scroll up and look
[02:51:28.000 --> 02:51:34.320]   at Lee's Afro. Oh, you put a picture in it. This is your daughter's. I can't find my minor. She doesn't
[02:51:34.320 --> 02:51:38.400]   look at all like you. I don't feel like I don't know. Again, she's white. I told you that didn't
[02:51:38.400 --> 02:51:46.880]   work. Oh, wow. Look at that. Wow. That's beautiful. I love your daughter. Look at that. Look at that.
[02:51:46.880 --> 02:51:52.640]   Throw. That hair is amazing. Coolest kid. The only fro bigger was my dad's fro. My dad had a bigger
[02:51:52.640 --> 02:51:57.040]   froten her. She tried to get as big as my dad. She couldn't. She couldn't top it. Does this start
[02:51:57.040 --> 02:52:05.120]   to at a certain point? Does it start to lose altitude? Oh, yeah. Yeah. She's got like a six hour window
[02:52:05.120 --> 02:52:13.280]   before it's floppy. Six hour window. I love it. There she is. Angela Davis. That's where
[02:52:13.280 --> 02:52:17.200]   it's ended a six hour window right there. Let's get a little flop. Yeah, get a little flop to it.
[02:52:17.200 --> 02:52:23.280]   There you go. All right. So that's the discord. That's how much fun the discord is. You also get
[02:52:23.280 --> 02:52:28.640]   ad free versions of all the shows plus shows like we teased at hands-on Macintosh with microsargent
[02:52:28.640 --> 02:52:33.440]   that we don't put out in public yet because they're there in the incubator, which is the club.
[02:52:33.440 --> 02:52:38.960]   We also have incubating Paul Therat's hands-on windows. We have the untitled Linux show,
[02:52:38.960 --> 02:52:44.320]   which will probably always be incubation because Linux still just a small fraction of the market.
[02:52:44.320 --> 02:52:51.200]   We have the Gizfiz, which is awesome every Wednesday. Dick D. Bartol does that. You get all that plus
[02:52:51.200 --> 02:52:57.040]   lots of events thanks to our community manager, Aunt Pruitt, who puts these together. Samable
[02:52:57.040 --> 02:53:02.720]   Sammett will be on March 2nd for an AMA. Stacey's book club is coming up. And inside Twitch chat
[02:53:02.720 --> 02:53:08.080]   with one of our best editors. The guy who does those Twitch promos every week, Victor Bognot.
[02:53:08.080 --> 02:53:14.400]   All of that. Seven bucks a month, one dollar less than a blue check on Twitter. And if you want to
[02:53:14.400 --> 02:53:21.360]   use phone SMS for authentication in the discord, you go right ahead and do it. We will not charge
[02:53:21.360 --> 02:53:28.000]   you for that. That's part of the deal. Twit.tv/club to it. It makes a big difference to our bottom line.
[02:53:28.000 --> 02:53:33.120]   Times are a little bit tough in the podcast business. Probably will stay that way. My dream
[02:53:33.680 --> 02:53:38.000]   is to have the listeners support it. Wouldn't take all of you just 5% that's all I ask.
[02:53:38.000 --> 02:53:43.920]   Then the other 95 could still get it for free and listen at their leisure. If you want to get
[02:53:43.920 --> 02:53:48.640]   this show after the fact, twit.tv is the website. There's a YouTube channel for all of the Twitch
[02:53:48.640 --> 02:53:53.200]   shows. Start at youtube.com/twit. That'll have links to all of those. And of course,
[02:53:53.200 --> 02:53:56.880]   you can always subscribe in your favorite podcast player. That way you'll get it automatically.
[02:53:56.880 --> 02:54:04.080]   Soon as it's ready for the world. Thank you all for being here. And now as always,
[02:54:04.080 --> 02:54:11.680]   it's time to let Owen JJ Stone wrap up this incredible edition of this week in tech. Owen,
[02:54:11.680 --> 02:54:19.280]   take it away. So a bunch of things. First of all, get Hank's essential salt. If you make steaks,
[02:54:19.280 --> 02:54:23.760]   it'll make you're like, if you suck at making a steak, put that on the steak. I swear to
[02:54:23.760 --> 02:54:28.320]   Jimmy Christmas, your steak will taste better. Just do that. And start with that and then you'll
[02:54:28.320 --> 02:54:32.240]   branch out. Do you have a favorite salt, Hank flavor that you really like?
[02:54:32.240 --> 02:54:40.720]   Oh, the truffle. Just a truffle, baby. The truffle garlic. I feel luxurious when I use it. I feel
[02:54:40.720 --> 02:54:44.880]   luxurious when I use it. But the new one is a chain of turdies. I want you to try that. You
[02:54:44.880 --> 02:54:48.800]   know, maybe I'll say something. I'll get to see you. Yeah, I'll try. I'll try that. But the
[02:54:48.800 --> 02:54:53.440]   essential salt, like I said, it just saves like if you suck, like I'm good, but it just made me
[02:54:53.440 --> 02:54:58.800]   better. You're gonna like this new one there. It'll just feel hot salt. You're gonna like that.
[02:54:58.800 --> 02:55:06.800]   Or for the holidays, pumpkin spice. Oh, it's sold out. So I'm right now. It's so good. So thank you
[02:55:06.800 --> 02:55:12.320]   for the suck. Secondly, yes. Secondly, there's so much AI stuff. There's too much as stuff.
[02:55:12.320 --> 02:55:17.360]   But if you follow Ben and then you follow Dan, then you'll don't they'll filter out the trash for
[02:55:17.360 --> 02:55:22.480]   you. That's what I'm doing. I sound smarter by following smart people. So just follow them.
[02:55:22.480 --> 02:55:27.360]   And then you'll know what's worth looking into and caring about. That's the second thing. And the
[02:55:27.360 --> 02:55:33.280]   third thing, which is of their four, I am back to doing shows. I'm doing my shows. I'm doing
[02:55:33.280 --> 02:55:38.160]   raising a ninja. It's raising an ninja and dovetails, right? And we people are going to be on Patreon
[02:55:38.160 --> 02:55:42.320]   because I don't want the world attacking my daughter, but we're talking about serious issues,
[02:55:42.320 --> 02:55:46.800]   like snapchats and what kids are doing on the internet. Yeah. And it's one of the biggest
[02:55:46.800 --> 02:55:50.720]   things that people ask me about, like what I do with my daughter and how I've trained her to use the
[02:55:50.720 --> 02:55:55.920]   internet. So just just follow me somewhere. If someone from the newsletter, you'll get it, but
[02:55:55.920 --> 02:56:00.800]   I'm doing and we're doing movie reviews together. We're having so much fun. I'm doing stuff. But
[02:56:00.800 --> 02:56:06.320]   more importantly than that, text me text me if you're bored, text me if you're happy, text me if
[02:56:06.320 --> 02:56:10.720]   you're sad. I know that sometimes where I hear this technological world and a lot of people
[02:56:10.720 --> 02:56:14.720]   are lonely and I've gotten that from the emails that I get from people in the text messages.
[02:56:14.720 --> 02:56:19.280]   Believe it or not, I haven't sent out a text message about my podcast or shows in over a month,
[02:56:19.280 --> 02:56:23.840]   but I talk to people randomly when they say go birds or someone told me they were on dialysis
[02:56:23.840 --> 02:56:27.760]   and they were lonely and they just wanted somebody to talk to. So I know that a lot of people out
[02:56:27.760 --> 02:56:32.960]   there listening just need somebody to check in on them, text me, tell me what's going on. I will
[02:56:32.960 --> 02:56:38.480]   spend a good five, six minutes with you. I'm pre-retired. I got the time and I know that somebody out there
[02:56:38.480 --> 02:56:42.880]   needs it. Okay, I know that you I'm not like that. I'm not I'm not working too hard. I made
[02:56:42.880 --> 02:56:46.400]   enough money. I'm trying to coast until the kid graduates high school. So I'm pre-retired right
[02:56:46.400 --> 02:56:51.040]   now. I don't like working for suckers. I know not you not not you listening, but some people.
[02:56:51.040 --> 02:56:55.120]   This is the humble bride by the way. I made enough money.
[02:56:55.120 --> 02:57:00.800]   I just I just live to you. I'm not yet. Don't worry about me. I didn't get to raise like this. So
[02:57:00.800 --> 02:57:09.120]   don't I'm not buying drinks. So and secondly, right now, I've said this so many times the apps are free.
[02:57:09.120 --> 02:57:16.800]   Start a podcast with your daughter, sister, son, uncle, cousin, and grandmother and record conversations.
[02:57:16.800 --> 02:57:23.920]   One of my best friends in life that I started doing these shows with back in 2007, 2008,
[02:57:23.920 --> 02:57:32.320]   passed away. His oldest daughter was 15 at the time. His son was three at the time. His son is now
[02:57:32.320 --> 02:57:38.080]   10. And he asked me for the podcast and the shows. And I think that he's old enough to listen to the
[02:57:38.080 --> 02:57:44.160]   crazy stuff that we talk about. And for him to cry and call me and say, it's so amazing to see my
[02:57:44.160 --> 02:57:49.040]   dad's mannerisms and how much fun he was because losing him at three years old, he doesn't have the
[02:57:49.040 --> 02:57:54.640]   context of his older sister has of how cool his dad was. And you might not think that you're
[02:57:54.640 --> 02:57:57.920]   important or you're valuable, but guess what? There's a lot of dumb people with a lot of dumb
[02:57:57.920 --> 02:58:03.120]   podcasts and you ain't got to share with nobody. But pick up your phone, record something,
[02:58:03.120 --> 02:58:08.400]   save something, and have some fun in your family units that could go forward. I tell you what,
[02:58:08.400 --> 02:58:12.560]   my daughter's gonna be telling the tales of my luxurious lifestyle in the decades to come to
[02:58:12.560 --> 02:58:17.680]   my grandchildren, whether I'm here or not. And I think you should do it too. And if you go to my
[02:58:17.680 --> 02:58:22.560]   website and buy a t-shirt, you'll know why I say another twit is in the can.
[02:58:22.560 --> 02:58:23.200]   Amazing.
[02:58:23.200 --> 02:58:36.720]   I made that dove video and they sent me like three years worth of soap. And I thought,
[02:58:36.720 --> 02:58:40.320]   does this soap go bad? Like, does the dove soap go bad? Like, I keep watching?
[02:58:40.320 --> 02:58:45.280]   Nope, keep using that. I won't. Can I? Actually, some soaps a couple of years later, and it
[02:58:45.280 --> 02:58:50.400]   disappears. It's like it's going into the air. But I think dove is pretty much tied to the ground.
[02:58:50.400 --> 02:58:55.120]   What I still got one left is like 13 years old. I still got one classic bottle of my closet.
[02:58:55.120 --> 02:58:59.280]   I never used that. You did that unbidden, right? You did a video with
[02:58:59.280 --> 02:59:04.560]   Doug. Yeah, everybody was doing it when, yeah, when everybody was doing the old spice thing,
[02:59:04.560 --> 02:59:10.960]   I'm like, I'm a sensitive man. I use dove. And then I remember, as I remember, you were,
[02:59:10.960 --> 02:59:16.240]   you were showing some skin. Yeah, yeah, you don't want to look that up. Don't worry about that.
[02:59:16.240 --> 02:59:22.240]   We got just children present now. It's like that. Oh, you found it. Look at what they see. What's going
[02:59:22.240 --> 02:59:27.200]   on? What are you doing? Oh, what are you doing? Who's old? Who are we? I'm in a pink bathroom.
[02:59:27.200 --> 02:59:32.800]   Excuse me. Sailors. No one. I don't know one single lady. Trust you, me. I know a lady. Look
[02:59:32.800 --> 02:59:36.640]   at yourself. Now, what do you mean? Now, look at yourself. Now, stay on me. You got a six pack?
[02:59:36.640 --> 02:59:41.760]   I got a cake. You drink alone. I bring the party. You're in a white towel. I'm in a black towel.
[02:59:41.760 --> 02:59:45.760]   What does that mean? It means I'm more of a man than you. Look at these arms. They're covering tattoos.
[02:59:45.760 --> 02:59:50.800]   I have 11. You have none. More of a man than you. I can check off a thousand things in the list.
[02:59:50.800 --> 02:59:54.640]   Why am I in a pink bathroom? Because ladies love pink and I love the ladies. Why is there a
[02:59:54.640 --> 03:00:00.000]   number of ladies? Because I'm a baby. These beautiful babies. Beautiful babies.
[03:00:00.000 --> 03:00:04.080]   Free installation. Free installation. You were talking to Kevin Rose.
[03:00:04.080 --> 03:00:13.520]   Watch yourself. So, there it is. You got how many dove bars? A lot. Not the chocolate. I bathed
[03:00:13.520 --> 03:00:22.080]   for free. Oh my God. And bathed for free. I bathed for free. Oh, oh, and Mr. is not happy.
[03:00:22.080 --> 03:00:28.960]   Mr. and says you need a similar push up. Hey, look, look, what I'm saying is I still get ladies
[03:00:28.960 --> 03:00:34.080]   looking like that. Okay. My personality works for me. If I look like and I have a whole problem.
[03:00:34.080 --> 03:00:38.400]   There'd be married couples divorced and the whole nation GDP will be good. It's good. It's good about
[03:00:38.400 --> 03:00:44.240]   that. It's a very here. I like to do the best I can. What I got. I love that was a good look for me.
[03:00:44.240 --> 03:00:51.280]   All right. Let's do on that note. Let's do a show.

