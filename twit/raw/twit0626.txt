;FFMETADATA1
title=CEO Barbie
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=626
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:01.560]   It's time for Twit This Week at Tech.
[00:00:01.560 --> 00:00:03.920]   Amy Webb joins us futurist Amy Webb.
[00:00:03.920 --> 00:00:05.600]   Rob Reed's also here science fiction author.
[00:00:05.600 --> 00:00:07.040]   His new book just came out.
[00:00:07.040 --> 00:00:09.560]   And of course Ian Thompson, our old friend from the register,
[00:00:09.560 --> 00:00:12.160]   will talk about that embarrassing Google memo.
[00:00:12.160 --> 00:00:15.320]   The eye generation is something wrong with kids today.
[00:00:15.320 --> 00:00:18.560]   We'll actually have an actual kid visiting the show.
[00:00:18.560 --> 00:00:22.920]   And lots more to talk about, including DJI and the United
[00:00:22.920 --> 00:00:24.560]   States Army.
[00:00:24.560 --> 00:00:26.080]   It's all coming up next on Twit.
[00:00:29.920 --> 00:00:32.000]   And that casts you love.
[00:00:32.000 --> 00:00:33.360]   From people you trust.
[00:00:33.360 --> 00:00:39.520]   This is Twit.
[00:00:39.520 --> 00:00:42.960]   Bandwidth for this week in Tech is provided by CashFly at
[00:00:42.960 --> 00:00:45.560]   cachefly.com.
[00:00:45.560 --> 00:00:56.320]   This is Twit this week in Tech, episode 626, recorded Sunday,
[00:00:56.320 --> 00:00:58.640]   August 6, 2017.
[00:00:58.640 --> 00:01:01.560]   CEO Barbie.
[00:01:01.560 --> 00:01:04.600]   This week in Tech is brought to you by Stamps.com.
[00:01:04.600 --> 00:01:07.960]   Start using your time more effectively with Stamps.com.
[00:01:07.960 --> 00:01:10.840]   Use Stamps.com to buy and print real US postage, the
[00:01:10.840 --> 00:01:13.040]   instant you need it right from your desk.
[00:01:13.040 --> 00:01:15.400]   To get my special offer, go to Stamps.com, click the
[00:01:15.400 --> 00:01:17.840]   microphone, and enter Twit.
[00:01:17.840 --> 00:01:20.560]   And buy rocket mortgage from Quick and Loans.
[00:01:20.560 --> 00:01:22.240]   Home plays a big role in your life.
[00:01:22.240 --> 00:01:25.200]   That's why Quick and Loans created rocket mortgage.
[00:01:25.200 --> 00:01:27.560]   It lets you apply simply and understand the entire
[00:01:27.560 --> 00:01:30.280]   mortgage process fully so you could be confident you're
[00:01:30.280 --> 00:01:31.640]   getting the right mortgage for you.
[00:01:31.640 --> 00:01:36.040]   Get started at rocketmortgage.com/twit2.
[00:01:36.040 --> 00:01:39.640]   And buy Casper, an online retailer premium mattresses for
[00:01:39.640 --> 00:01:42.360]   a fraction of the price because everyone deserves a
[00:01:42.360 --> 00:01:43.560]   great night's sleep.
[00:01:43.560 --> 00:01:46.120]   Get $50 off any mattress purchase by visiting
[00:01:46.120 --> 00:01:50.640]   casper.com/twit and entering the promo code TWIT.
[00:01:50.640 --> 00:01:53.680]   And this week in Tech is literally brought to you by
[00:01:53.680 --> 00:01:58.320]   Sonic, Twit's 10 gigabit fiber internet service provider.
[00:01:58.320 --> 00:02:01.120]   Join Sonic's internet revolution as they bring fast,
[00:02:01.120 --> 00:02:04.440]   affordable internet, phone and TV to homes and
[00:02:04.440 --> 00:02:06.400]   businesses all over California.
[00:02:06.400 --> 00:02:10.120]   Visit sonic.com/twit to sign up for service and
[00:02:10.120 --> 00:02:11.760]   receive your first month, free.
[00:02:11.760 --> 00:02:17.880]   It's time for Twit this week in Tech, the show we cover
[00:02:17.880 --> 00:02:20.520]   the week's tech news.
[00:02:20.520 --> 00:02:23.640]   And one of the fun things about doing this show for me is
[00:02:23.640 --> 00:02:26.040]   I get to bring in whoever I am.
[00:02:26.040 --> 00:02:29.440]   Well, please, fun people that I, it's always somebody I say,
[00:02:29.440 --> 00:02:31.760]   do we, you know, right, Carson?
[00:02:31.760 --> 00:02:33.520]   Oh, I want to talk to Rob Reed.
[00:02:33.520 --> 00:02:37.160]   Can we get Rob Reed on and lo and behold, he appears.
[00:02:37.160 --> 00:02:39.760]   I said your name three times.
[00:02:39.760 --> 00:02:41.160]   I actually went with going on.
[00:02:41.160 --> 00:02:42.840]   I was sitting in my living room in New York and
[00:02:42.840 --> 00:02:45.560]   saying it in the puff of sulfur is smoke.
[00:02:45.560 --> 00:02:46.120]   Here I am.
[00:02:46.120 --> 00:02:49.280]   Rob, as you probably all know, but if you don't,
[00:02:49.280 --> 00:02:51.280]   Yotto is a good friend of the network.
[00:02:51.280 --> 00:02:54.240]   He created, he was a multiple entrepreneur, created
[00:02:54.240 --> 00:02:57.600]   listen.com, Rhapsody, then took his experience in the
[00:02:57.600 --> 00:03:00.360]   music industry to write a book that was a really good book
[00:03:00.360 --> 00:03:03.200]   called Year Zero about aliens and pop music.
[00:03:03.200 --> 00:03:05.800]   It's a complicated story.
[00:03:05.800 --> 00:03:07.360]   And they love our pop music.
[00:03:07.360 --> 00:03:10.120]   They love our past alien civilization that is way too
[00:03:10.120 --> 00:03:12.960]   into American pop music and it causes problems.
[00:03:12.960 --> 00:03:14.800]   You said though, you told me in an email, you were worried
[00:03:14.800 --> 00:03:17.280]   about the sophomore jinx, your second novel, your
[00:03:17.280 --> 00:03:17.880]   second novel.
[00:03:17.880 --> 00:03:18.520]   Fourth book, actually.
[00:03:18.520 --> 00:03:19.720]   Fourth, but second novel.
[00:03:19.720 --> 00:03:22.720]   So year zero was a play on year one, your book about
[00:03:22.720 --> 00:03:23.920]   your first year in law school.
[00:03:23.920 --> 00:03:25.920]   Yeah, it was a joke with myself in business school.
[00:03:25.920 --> 00:03:27.160]   It was kind of a joke with myself.
[00:03:27.160 --> 00:03:28.720]   It was like a working title.
[00:03:28.720 --> 00:03:30.880]   So my first book was called Year One.
[00:03:30.880 --> 00:03:32.600]   It was about being a first year student at Harvard
[00:03:32.600 --> 00:03:35.200]   Business School, wrote it long ago and I was a first year
[00:03:35.200 --> 00:03:36.720]   student at Harvard Business School.
[00:03:36.720 --> 00:03:39.040]   And then when I started, made the pivot to writing
[00:03:39.040 --> 00:03:41.240]   fiction, which is a giant step in literature.
[00:03:41.240 --> 00:03:43.720]   It's basically a reset button.
[00:03:43.720 --> 00:03:47.280]   Just as a playful joke for myself, I had my working
[00:03:47.280 --> 00:03:49.880]   file, I just called it year zero.
[00:03:49.880 --> 00:03:50.880]   That's cool.
[00:03:50.880 --> 00:03:52.320]   It was a phrase that I used in the book.
[00:03:52.320 --> 00:03:54.880]   It was a phrase I genuinely used in the book, but then
[00:03:54.880 --> 00:03:57.440]   the folks at Random House really liked that as a title.
[00:03:57.440 --> 00:03:59.120]   And it's like, it's going to cause a little confusion
[00:03:59.120 --> 00:03:59.680]   with the old book.
[00:03:59.680 --> 00:04:01.400]   They're like, don't worry Rob, nobody read that book.
[00:04:01.400 --> 00:04:02.400]   Yeah.
[00:04:02.400 --> 00:04:03.400]   Wow.
[00:04:03.400 --> 00:04:03.920]   Wow.
[00:04:03.920 --> 00:04:04.560]   That was true.
[00:04:04.560 --> 00:04:05.120]   It was true.
[00:04:05.120 --> 00:04:05.960]   Wow.
[00:04:05.960 --> 00:04:07.880]   Well, his newest just came out August 1.
[00:04:07.880 --> 00:04:10.920]   I was at the reading, the booksmiths that you did on day
[00:04:10.920 --> 00:04:12.960]   one, on day one, it hits a year one.
[00:04:12.960 --> 00:04:16.040]   And the new book is called After On.
[00:04:16.040 --> 00:04:18.120]   And it is truly a masterpiece.
[00:04:18.120 --> 00:04:21.360]   John Slanina has been saying, finish it Leo, so we could
[00:04:21.360 --> 00:04:22.040]   talk about it.
[00:04:22.040 --> 00:04:23.680]   He says, I don't want to give you any spoilers.
[00:04:23.680 --> 00:04:24.920]   There's some spoilers.
[00:04:24.920 --> 00:04:26.440]   No spoilers here today.
[00:04:26.440 --> 00:04:31.600]   But it is really a great book about near future Silicon
[00:04:31.600 --> 00:04:31.960]   Valley.
[00:04:31.960 --> 00:04:33.840]   Nine seconds in the future, Leo.
[00:04:33.840 --> 00:04:34.840]   It's very near.
[00:04:34.840 --> 00:04:35.360]   Yeah.
[00:04:35.360 --> 00:04:38.160]   It's incumbent on the reader to read it very quickly.
[00:04:38.160 --> 00:04:40.840]   It takes place nine seconds from when you start reading it.
[00:04:40.840 --> 00:04:42.600]   So it is a long book.
[00:04:42.600 --> 00:04:44.840]   So you have to read it rapidly, less at fall into the past.
[00:04:44.840 --> 00:04:47.320]   But it's present-- it's set in present day San Francisco
[00:04:47.320 --> 00:04:52.440]   in an imaginary startup, a rather diabolical social media
[00:04:52.440 --> 00:04:53.920]   company.
[00:04:53.920 --> 00:04:57.600]   And everything that's in it, I research pretty rigorously
[00:04:57.600 --> 00:05:00.800]   with long conversations with scientists and technologists.
[00:05:00.800 --> 00:05:05.000]   So the tech is very much grounded in what could just
[00:05:05.000 --> 00:05:07.080]   happen tomorrow or nine seconds from now.
[00:05:07.080 --> 00:05:09.440]   And I know you did a podcast with Tom Merritt.
[00:05:09.440 --> 00:05:11.200]   There's what eight or nine episodes coming?
[00:05:11.200 --> 00:05:12.200]   Yeah.
[00:05:12.200 --> 00:05:14.820]   So what we're doing is we're doing an eight episode
[00:05:14.820 --> 00:05:15.620]   podcast.
[00:05:15.620 --> 00:05:17.960]   And it may persist, actually, because I've
[00:05:17.960 --> 00:05:20.160]   found that I'm having so much fun with it.
[00:05:20.160 --> 00:05:21.400]   I may just keep going with it.
[00:05:21.400 --> 00:05:25.240]   But basically, I interviewed so many fascinating people
[00:05:25.240 --> 00:05:27.080]   in preparation to write the book.
[00:05:27.080 --> 00:05:29.560]   And I ended up learning so much about the main topics
[00:05:29.560 --> 00:05:31.960]   of the book, which includes synthetic biology and quantum
[00:05:31.960 --> 00:05:33.280]   computing.
[00:05:33.280 --> 00:05:36.200]   There's a great deal about superintelligence risk
[00:05:36.200 --> 00:05:37.200]   and a lot of other stuff.
[00:05:37.200 --> 00:05:41.040]   And I found myself wanting to inject these 10-page
[00:05:41.040 --> 00:05:44.860]   aggressions about how cool synthetic biology is at this moment.
[00:05:44.860 --> 00:05:46.300]   And that's bad storytelling.
[00:05:46.300 --> 00:05:48.860]   So my reward to myself from my compromise
[00:05:48.860 --> 00:05:52.140]   was we're going to do a bunch of podcasts, which are basically
[00:05:52.140 --> 00:05:56.140]   very long-form interviews with scientists, technologists,
[00:05:56.140 --> 00:05:59.780]   thinkers, founders that go deep into the technology
[00:05:59.780 --> 00:06:01.940]   and the science that's connected to the book.
[00:06:01.940 --> 00:06:03.580]   And then also the geopolitical issues.
[00:06:03.580 --> 00:06:04.940]   There's a lot about terrorism.
[00:06:04.940 --> 00:06:07.180]   And so I interviewed Sam Harris at great length
[00:06:07.180 --> 00:06:09.140]   for one of the podcasts.
[00:06:09.140 --> 00:06:12.080]   And so those premiered on Thursday, actually, two days
[00:06:12.080 --> 00:06:13.480]   after the book.
[00:06:13.480 --> 00:06:15.840]   And Tom Merritt is my host, which is fantastic,
[00:06:15.840 --> 00:06:18.440]   because he is extremely seasoned and has all the experience
[00:06:18.440 --> 00:06:20.480]   and podcasting that I personally lack.
[00:06:20.480 --> 00:06:21.640]   And it's been a blast.
[00:06:21.640 --> 00:06:24.280]   Well, we're thrilled to have you so shortly after the release
[00:06:24.280 --> 00:06:25.600]   of the book, unaudible.
[00:06:25.600 --> 00:06:27.760]   But you can also get it a Kindle,
[00:06:27.760 --> 00:06:31.080]   and you can get it at Amazon if you want the 10-tree edition.
[00:06:31.080 --> 00:06:33.920]   And I have three copies, and I didn't have any of them here.
[00:06:33.920 --> 00:06:34.760]   They're all at home.
[00:06:34.760 --> 00:06:36.400]   But thank you for bringing in your copy.
[00:06:36.400 --> 00:06:38.480]   After on is the name of the book.
[00:06:38.480 --> 00:06:39.980]   Also joining us, Amy Webb.
[00:06:39.980 --> 00:06:42.600]   You may remember Amy, her appearance a few months ago
[00:06:42.600 --> 00:06:44.560]   in "Tryingulation," talking about her book,
[00:06:44.560 --> 00:06:47.040]   "The Signals Are Talking," in which she gives away
[00:06:47.040 --> 00:06:48.280]   the secrets of her business.
[00:06:48.280 --> 00:06:51.240]   She's a futurist and tells you how you can look at what's
[00:06:51.240 --> 00:06:54.240]   going on today, the signals, and determine
[00:06:54.240 --> 00:06:57.460]   what might be happening tomorrow, why today's "Fringes,
[00:06:57.460 --> 00:06:58.220]   Tomorrow's Mainstream."
[00:06:58.220 --> 00:07:00.520]   Amy, great to have you back on the show.
[00:07:00.520 --> 00:07:00.880]   Thanks.
[00:07:00.880 --> 00:07:01.720]   Thanks for having me.
[00:07:01.720 --> 00:07:02.320]   Books doing great.
[00:07:02.320 --> 00:07:04.600]   I see you just got a business-- what is it?
[00:07:04.600 --> 00:07:07.760]   2017 Business Award?
[00:07:07.760 --> 00:07:09.160]   Yeah, it's won some awards.
[00:07:09.160 --> 00:07:11.320]   It's hit some bestseller lists.
[00:07:11.320 --> 00:07:11.920]   It's doing well.
[00:07:11.920 --> 00:07:13.280]   It just went into a six printing.
[00:07:13.280 --> 00:07:14.520]   So people are reading it.
[00:07:14.520 --> 00:07:15.280]   That's awesome.
[00:07:15.280 --> 00:07:15.800]   That's great.
[00:07:15.800 --> 00:07:16.320]   Yeah.
[00:07:16.320 --> 00:07:17.000]   That's one.
[00:07:17.000 --> 00:07:20.400]   And Ian Thompson, who is not plugging a book--
[00:07:20.400 --> 00:07:21.640]   I have no book to plug.
[00:07:21.640 --> 00:07:22.760]   I'm terribly sorry.
[00:07:22.760 --> 00:07:27.520]   But I do write a considerable amount during each week.
[00:07:27.520 --> 00:07:30.160]   Well, he writes a lot at the register.co.uk.
[00:07:30.160 --> 00:07:33.200]   And I saw your--
[00:07:33.200 --> 00:07:35.560]   well, I saw your upset.
[00:07:35.560 --> 00:07:38.600]   Look, see all of that about Marcus Hutchins?
[00:07:38.600 --> 00:07:39.600]   Yes, it's been--
[00:07:39.600 --> 00:07:41.320]   So this is a really interesting story.
[00:07:41.320 --> 00:07:42.520]   He came out for Defcon.
[00:07:42.520 --> 00:07:43.880]   He's a hacker.
[00:07:43.880 --> 00:07:45.440]   He was the guy-- you may know the name,
[00:07:45.440 --> 00:07:48.080]   because he was the guy when WannaCry, the ransomware,
[00:07:48.080 --> 00:07:51.800]   was ravaging Europe a few months ago.
[00:07:51.800 --> 00:07:53.720]   He was a guy who noted in the source code
[00:07:53.720 --> 00:07:56.960]   that there was a domain name, a really long, crazy domain
[00:07:56.960 --> 00:07:59.120]   name, and it wasn't registered.
[00:07:59.120 --> 00:08:02.160]   And so he registered it, and turned out to be the kill switch.
[00:08:02.160 --> 00:08:02.600]   Pretty much.
[00:08:02.600 --> 00:08:06.440]   Written in, probably by the North Korean authors of WannaCry.
[00:08:06.440 --> 00:08:07.880]   We think it was North Korea.
[00:08:07.880 --> 00:08:09.360]   What does Donald Trump think it was?
[00:08:09.360 --> 00:08:11.520]   Opinion is divided by Donald Trump.
[00:08:11.520 --> 00:08:14.000]   It's probably a state.
[00:08:14.000 --> 00:08:17.160]   There are elements of state-sponsored coding in there.
[00:08:17.160 --> 00:08:20.160]   Certainly, if you look at the way that it was put together,
[00:08:20.160 --> 00:08:23.960]   it's use of ex-NSA exploits, but also a whole bunch
[00:08:23.960 --> 00:08:24.960]   of other stuff as well.
[00:08:24.960 --> 00:08:26.960]   It was not an amateur.
[00:08:26.960 --> 00:08:27.960]   Yeah.
[00:08:27.960 --> 00:08:30.600]   And the kill switch makes sense if it's state-sponsored,
[00:08:30.600 --> 00:08:32.200]   because you want to wait and stop.
[00:08:32.200 --> 00:08:35.160]   It's spread if it should get out somewhere you didn't want it to,
[00:08:35.160 --> 00:08:36.960]   like in North Korea.
[00:08:36.960 --> 00:08:38.840]   So there was a kill switch.
[00:08:38.840 --> 00:08:39.920]   Marcus found it.
[00:08:39.920 --> 00:08:40.480]   Yep.
[00:08:40.480 --> 00:08:40.960]   Set it off.
[00:08:40.960 --> 00:08:42.760]   Save the world.
[00:08:42.760 --> 00:08:47.960]   Except it brought him to the attention of US federal authorities.
[00:08:47.960 --> 00:08:50.680]   Tell us what happened after Defcon.
[00:08:50.680 --> 00:08:51.800]   Well, he came out to the--
[00:08:51.800 --> 00:08:53.360]   I don't think he actually went to the Defcon conferences.
[00:08:53.360 --> 00:08:54.360]   Was he for Blackout?
[00:08:54.360 --> 00:08:54.680]   Or--
[00:08:54.680 --> 00:08:55.000]   No.
[00:08:55.000 --> 00:08:57.680]   I mean, he came out with a whole bunch of friends.
[00:08:57.680 --> 00:09:01.960]   There's been an awful lot of misdirection going on here.
[00:09:01.960 --> 00:09:04.480]   He didn't come out on a thousands of dollars' expenses paid
[00:09:04.480 --> 00:09:05.440]   vacation.
[00:09:05.440 --> 00:09:07.600]   He stayed in an Airbnb with eight friends.
[00:09:07.600 --> 00:09:09.200]   And yes, it was a very expensive Airbnb.
[00:09:09.200 --> 00:09:09.960]   In Vegas?
[00:09:09.960 --> 00:09:10.460]   Yeah.
[00:09:10.460 --> 00:09:12.440]   But at the same time, it came down to less than the cost
[00:09:12.440 --> 00:09:15.120]   of my hotel room in the Flamingo for the Saturday night.
[00:09:15.120 --> 00:09:15.640]   OK.
[00:09:15.640 --> 00:09:17.920]   And if I could tell you some stories about that hotel.
[00:09:17.920 --> 00:09:18.520]   That's--
[00:09:18.520 --> 00:09:20.080]   Please, no.
[00:09:20.080 --> 00:09:21.480]   But no, I mean, he came out.
[00:09:21.480 --> 00:09:22.720]   He did the party circuit.
[00:09:22.720 --> 00:09:23.760]   He met some people.
[00:09:23.760 --> 00:09:26.880]   He did what British tourists in Las Vegas always do,
[00:09:26.880 --> 00:09:28.640]   which would you go to a gun range?
[00:09:28.640 --> 00:09:30.800]   You go to the Grand Canyon.
[00:09:30.800 --> 00:09:32.280]   You go to a gambling hall and see--
[00:09:32.280 --> 00:09:36.080]   It's not coincidental that his decision of vacation in Vegas
[00:09:36.080 --> 00:09:37.680]   was during Blackout and Defcon, though.
[00:09:37.680 --> 00:09:38.240]   Oh, no, no.
[00:09:38.240 --> 00:09:40.280]   I mean, but his friends were here for--
[00:09:40.280 --> 00:09:40.680]   Well, yeah.
[00:09:40.680 --> 00:09:42.560]   I mean, he didn't go to the conference.
[00:09:42.560 --> 00:09:44.320]   For that week, you have B-Sides.
[00:09:44.320 --> 00:09:45.120]   You've got Blackout.
[00:09:45.120 --> 00:09:45.920]   You've got Defcon.
[00:09:45.920 --> 00:09:48.920]   It's the biggest collection of hackers in the US.
[00:09:48.920 --> 00:09:50.360]   He came out here for this.
[00:09:50.360 --> 00:09:51.240]   He went to some parties.
[00:09:51.240 --> 00:09:52.280]   He had some fun.
[00:09:52.280 --> 00:09:52.760]   All right.
[00:09:52.760 --> 00:09:54.160]   And then as he was about to board the play--
[00:09:54.160 --> 00:09:56.040]   He was actually at the airport on the way home.
[00:09:56.040 --> 00:09:57.080]   Where's he from, Britain?
[00:09:57.080 --> 00:09:58.600]   Oh, yeah.
[00:09:58.600 --> 00:09:59.800]   That they arrested him.
[00:09:59.800 --> 00:10:02.280]   Now, that, by the way, is SOP, right?
[00:10:02.280 --> 00:10:04.160]   Because if you're at the airport,
[00:10:04.160 --> 00:10:05.800]   it's a lot easier to arrest somebody.
[00:10:05.800 --> 00:10:06.160]   Yeah.
[00:10:06.160 --> 00:10:08.880]   You've got a bunch of TSA people around who are so bored
[00:10:08.880 --> 00:10:09.840]   that they've got nothing else to do.
[00:10:09.840 --> 00:10:10.440]   But to take it down--
[00:10:10.440 --> 00:10:11.880]   Or to control the environment.
[00:10:11.880 --> 00:10:13.600]   You've got a lot of armed police around.
[00:10:13.600 --> 00:10:16.720]   It's a good way to catch someone if you're actually doing it.
[00:10:16.720 --> 00:10:18.240]   But it does send a chilling effect
[00:10:18.240 --> 00:10:20.600]   to other security researchers--
[00:10:20.600 --> 00:10:21.320]   Oh, it already has.
[00:10:21.320 --> 00:10:24.760]   --coming to the United States that they might be arrested.
[00:10:24.760 --> 00:10:25.600]   Now, if--
[00:10:25.600 --> 00:10:27.960]   But they didn't bust him for helping us with WannaCry, right?
[00:10:27.960 --> 00:10:28.320]   No.
[00:10:28.320 --> 00:10:29.840]   They busted him because--
[00:10:29.840 --> 00:10:31.040]   --the different accusations.
[00:10:31.040 --> 00:10:33.640]   Well, he admitted to writing "Chronos," right?
[00:10:33.640 --> 00:10:34.320]   "The Trojan?"
[00:10:34.320 --> 00:10:35.040]   No.
[00:10:35.040 --> 00:10:36.680]   No, this is another thing.
[00:10:36.680 --> 00:10:41.880]   He has-- his attorney has, as of Friday afternoon,
[00:10:41.880 --> 00:10:43.480]   had barely spoken to the guy.
[00:10:43.480 --> 00:10:46.040]   So we don't know exactly what he's told the authorities.
[00:10:46.040 --> 00:10:49.680]   Now, he did write a blog post in 2014
[00:10:49.680 --> 00:10:52.280]   putting together a piece of potential malware,
[00:10:52.280 --> 00:10:54.440]   but crippling it so it couldn't possibly be used.
[00:10:54.440 --> 00:10:55.640]   Yeah, like his instruction, right?
[00:10:55.640 --> 00:10:57.480]   I mean, it was anti-malware instruction.
[00:10:57.480 --> 00:10:58.480]   Yeah, exactly.
[00:10:58.480 --> 00:10:58.960]   Yeah, it was like--
[00:10:58.960 --> 00:11:01.240]   I mean, you can't research malware
[00:11:01.240 --> 00:11:03.280]   unless you know how malware works.
[00:11:03.280 --> 00:11:04.920]   Yeah, and he really did it to train--
[00:11:04.920 --> 00:11:08.240]   Oh, but-- well, OK, this sounds like, though,
[00:11:08.240 --> 00:11:10.680]   making a lot of excuses for the guy.
[00:11:10.680 --> 00:11:12.920]   The malware he--
[00:11:12.920 --> 00:11:16.600]   the thing he created ended up being sold as malware, am I wrong?
[00:11:16.600 --> 00:11:17.920]   Actually, no, the thing he created
[00:11:17.920 --> 00:11:20.800]   was a Windows XP bootloader.
[00:11:20.800 --> 00:11:21.320]   So--
[00:11:21.320 --> 00:11:22.440]   He had nothing to do.
[00:11:22.440 --> 00:11:23.160]   It had virtually--
[00:11:23.160 --> 00:11:23.600]   --it had virtually.
[00:11:23.600 --> 00:11:26.200]   As far as we can tell, it had nothing at all to do with "Chronos."
[00:11:26.200 --> 00:11:29.200]   He has been arrested, however, for writing "Chronos."
[00:11:29.200 --> 00:11:33.440]   Not for selling it, but his buddy ended up putting it on the market.
[00:11:33.440 --> 00:11:35.080]   Buddy is possibly a bit strong.
[00:11:35.080 --> 00:11:36.120]   The guy's name has been--
[00:11:36.120 --> 00:11:36.640]   acquaintance?
[00:11:36.640 --> 00:11:40.040]   --is co-conspirator to use the FBI's term--
[00:11:40.040 --> 00:11:41.920]   --who has not been charged, we should point out.
[00:11:41.920 --> 00:11:43.320]   And has not even been named.
[00:11:43.320 --> 00:11:45.800]   The supposition is he probably turned state's evidence.
[00:11:45.800 --> 00:11:47.840]   Well, it's either that he turned state's evidence
[00:11:47.840 --> 00:11:50.920]   or that they don't know exactly where to get their hands on him
[00:11:50.920 --> 00:11:52.480]   and they don't want to warn him of what he's coming on.
[00:11:52.480 --> 00:11:53.280]   Oh, we haven't arrested him.
[00:11:53.280 --> 00:11:54.280]   Well, he's kind of warned.
[00:11:54.280 --> 00:11:55.520]   It all seems to be--
[00:11:55.520 --> 00:11:57.040]   A few ask me.
[00:11:57.040 --> 00:11:58.000]   He knows his name.
[00:11:58.000 --> 00:11:59.360]   It doesn't matter if we know his name.
[00:11:59.360 --> 00:12:00.840]   Yeah, but it all comes down to--
[00:12:00.840 --> 00:12:02.920]   I mean, I've read through all the court documents
[00:12:02.920 --> 00:12:04.440]   that's in "Dietman" the rest of it.
[00:12:04.440 --> 00:12:08.800]   It basically seems to come down to his word against Marxes.
[00:12:08.800 --> 00:12:10.320]   Interesting.
[00:12:10.320 --> 00:12:12.360]   So you've read the Fed's indictment?
[00:12:12.360 --> 00:12:12.840]   Yep.
[00:12:12.840 --> 00:12:15.640]   The heavily redacted Fed's indictment, yes.
[00:12:15.640 --> 00:12:19.280]   And so he's-- by the way, being--
[00:12:19.280 --> 00:12:21.280]   Wasn't there a question about what--
[00:12:21.280 --> 00:12:25.320]   For a while, nobody knows where he was because the US
[00:12:25.320 --> 00:12:26.840]   marshals had taken him, right?
[00:12:26.840 --> 00:12:30.240]   Yeah, he had a big question as to nobody could find him
[00:12:30.240 --> 00:12:30.960]   for a while.
[00:12:30.960 --> 00:12:33.360]   Yeah, there were 18 hours when nobody, not his family,
[00:12:33.360 --> 00:12:36.200]   not his friends knew where he was.
[00:12:36.200 --> 00:12:38.440]   I was calling down jails in Nevada area,
[00:12:38.440 --> 00:12:39.960]   trying to find out if he was there.
[00:12:39.960 --> 00:12:41.960]   Other journalists were doing the same.
[00:12:41.960 --> 00:12:44.480]   The FBI finally came through and said, yeah, OK,
[00:12:44.480 --> 00:12:47.920]   we're holding him at our field office.
[00:12:47.920 --> 00:12:50.200]   They've been interrogating him there.
[00:12:50.200 --> 00:12:53.760]   I've got to say, as a Brit who, if I was coming to the US
[00:12:53.760 --> 00:12:57.520]   an awful lot and the FBI came round and said, you know what,
[00:12:57.520 --> 00:13:00.040]   we really need to talk and sat you down in an office for 18
[00:13:00.040 --> 00:13:02.720]   hours, I'm not quite sure what I'd say.
[00:13:02.720 --> 00:13:03.240]   But--
[00:13:03.240 --> 00:13:06.400]   Do you think he might have confessed to something?
[00:13:06.400 --> 00:13:06.880]   I don't think he's--
[00:13:06.880 --> 00:13:09.600]   They had an indictment written before they arrested.
[00:13:09.600 --> 00:13:11.360]   The indictment was filed on July 12th.
[00:13:11.360 --> 00:13:13.160]   They went back in the back of the back of the week.
[00:13:13.160 --> 00:13:16.040]   The Fed's-- whether it's true or not,
[00:13:16.040 --> 00:13:17.520]   we don't know your guilty proof of it,
[00:13:17.520 --> 00:13:19.920]   and he has pled not guilty.
[00:13:19.920 --> 00:13:21.720]   And by the way, the bail is not very high.
[00:13:21.720 --> 00:13:23.240]   It's $30,000 bail.
[00:13:23.240 --> 00:13:24.000]   Has he made bail?
[00:13:24.000 --> 00:13:24.600]   Has he still improved?
[00:13:24.600 --> 00:13:25.600]   It was a timing thing, right?
[00:13:25.600 --> 00:13:25.600]   Yeah.
[00:13:25.600 --> 00:13:26.120]   He basically--
[00:13:26.120 --> 00:13:26.520]   He basically--
[00:13:26.520 --> 00:13:27.000]   He could get it done in--
[00:13:27.000 --> 00:13:31.400]   He went up into court at 3 o'clock.
[00:13:31.400 --> 00:13:35.240]   He was given a 30 grand bail at 3.30.
[00:13:35.240 --> 00:13:37.360]   They then had to rush across town to get to the bail office.
[00:13:37.360 --> 00:13:38.880]   They didn't make it in time.
[00:13:38.880 --> 00:13:42.400]   So he spent the weekend in a jail.
[00:13:42.400 --> 00:13:44.640]   And then on Tuesday, he has to fly up to Wisconsin
[00:13:44.640 --> 00:13:45.800]   to go through the preliminary--
[00:13:45.800 --> 00:13:46.680]   Are you upset?
[00:13:46.680 --> 00:13:49.280]   So I don't pretend to know the facts.
[00:13:49.280 --> 00:13:50.600]   I just-- well, he was arrested.
[00:13:50.600 --> 00:13:51.800]   There's an indictment.
[00:13:51.800 --> 00:13:54.240]   Presumably there's evidence that led to his arrest,
[00:13:54.240 --> 00:13:56.160]   which none of us know.
[00:13:56.160 --> 00:13:58.040]   Why are you upset about this?
[00:13:58.040 --> 00:14:01.880]   I'm not upset so much as I think it's been massively
[00:14:01.880 --> 00:14:04.720]   overblown, but also it's going to have a really, really
[00:14:04.720 --> 00:14:07.360]   serious chilling effect on security researchers
[00:14:07.360 --> 00:14:08.240]   coming to this country.
[00:14:08.240 --> 00:14:08.640]   Well--
[00:14:08.640 --> 00:14:10.320]   Because after he got arrested, there
[00:14:10.320 --> 00:14:13.200]   are a whole bunch of people who I know in the field.
[00:14:13.200 --> 00:14:15.200]   Marcus has been a member of the Infosec community
[00:14:15.200 --> 00:14:16.320]   for at least five years.
[00:14:16.320 --> 00:14:18.360]   And as he respected, is he respected--
[00:14:18.360 --> 00:14:20.240]   Is he a gray hat or white hat?
[00:14:20.240 --> 00:14:20.880]   No, he's white.
[00:14:20.880 --> 00:14:26.120]   He does pro bono work for other malware researchers.
[00:14:26.120 --> 00:14:29.080]   He was given $10,000 for stopping Wanna Cry, which he
[00:14:29.080 --> 00:14:30.200]   donated to charity.
[00:14:30.200 --> 00:14:31.920]   He has a good mark in my book.
[00:14:31.920 --> 00:14:34.120]   And it was also given free pizza from his local pizza
[00:14:34.120 --> 00:14:36.040]   rare, which is just for real life.
[00:14:36.040 --> 00:14:37.720]   Which he did not donate to charity.
[00:14:37.720 --> 00:14:38.720]   Well, no.
[00:14:38.720 --> 00:14:40.040]   He didn't know where he ate that.
[00:14:40.040 --> 00:14:42.480]   I mean, seriously, he's considered an ethical hacker.
[00:14:42.480 --> 00:14:45.280]   He's considered an ethical hacker, certainly.
[00:14:45.280 --> 00:14:46.840]   Well, here's what I--
[00:14:46.840 --> 00:14:47.160]   Sure.
[00:14:47.160 --> 00:14:48.520]   I'm going to say--
[00:14:48.520 --> 00:14:50.520]   Well, here's what I think some of the problem
[00:14:50.520 --> 00:14:56.000]   is that we don't have clear case law on what constitutes
[00:14:56.000 --> 00:14:58.080]   a violation, right?
[00:14:58.080 --> 00:15:02.200]   And in the past, the feds have been very overly aggressive.
[00:15:02.200 --> 00:15:04.360]   Aaron Schwartz would be the kind of--
[00:15:04.360 --> 00:15:05.720]   Yeah, that's holding you politely.
[00:15:05.720 --> 00:15:07.040]   --a big example.
[00:15:07.040 --> 00:15:09.240]   Somebody who really was founded--
[00:15:09.240 --> 00:15:10.800]   He was founded into suicide.
[00:15:10.800 --> 00:15:12.000]   --and end up killing himself.
[00:15:12.000 --> 00:15:14.360]   And so, yeah, I think there is this distrust
[00:15:14.360 --> 00:15:16.640]   of federal prosecution of hacking crimes.
[00:15:16.640 --> 00:15:19.600]   But also, I mean, we're facing a difficult situation
[00:15:19.600 --> 00:15:21.560]   because we have a thing called the Watson our Agreement
[00:15:21.560 --> 00:15:25.520]   amongst 30 or 40 nations about what constitutes arms dealing.
[00:15:25.520 --> 00:15:28.480]   And under the current US State Department rules,
[00:15:28.480 --> 00:15:32.000]   simply having a fuzzer could be considered software arms
[00:15:32.000 --> 00:15:34.560]   dealing if you exported outside of the US.
[00:15:34.560 --> 00:15:36.240]   So there's a tremendous disconnect
[00:15:36.240 --> 00:15:39.200]   between what the feds see as offensive hacking tools
[00:15:39.200 --> 00:15:42.480]   and what a lot of other people see as basic research tools
[00:15:42.480 --> 00:15:43.480]   that you need to do your job.
[00:15:43.480 --> 00:15:46.720]   The feds don't understand what he wrote,
[00:15:46.720 --> 00:15:48.480]   that they didn't understand the purpose of it
[00:15:48.480 --> 00:15:50.560]   and misinterpreted his malware.
[00:15:50.560 --> 00:15:55.400]   Fab it for me to sort of disparage the good FBI people
[00:15:55.400 --> 00:15:58.880]   in Wisconsin that I've got a reasonable expectation
[00:15:58.880 --> 00:16:01.480]   that they might have over-reg the pudding on this one.
[00:16:01.480 --> 00:16:04.080]   It comes just after Alpha Bay.
[00:16:04.080 --> 00:16:09.400]   And my first thought was they grabbed someone at Alpha Bay,
[00:16:09.400 --> 00:16:12.360]   said, spill your guts or you're going to prison for 30 years.
[00:16:12.360 --> 00:16:14.520]   He picked the first time out for Hatley who'd come up with
[00:16:14.520 --> 00:16:17.160]   and that's where the end when the weather in Darwin came from.
[00:16:17.160 --> 00:16:18.280]   - Well, if that's the case,
[00:16:18.280 --> 00:16:22.920]   then I'm sure that Marcus will be exonerated.
[00:16:22.920 --> 00:16:25.680]   - I'm really not because considering the technical sophistication
[00:16:25.680 --> 00:16:27.520]   of most juries in this country,
[00:16:27.520 --> 00:16:29.120]   I mean, considering that it's a widely thing.
[00:16:29.120 --> 00:16:33.240]   - Well, I think it's clear if he is being prosecuted
[00:16:33.240 --> 00:16:37.560]   inappropriately and ends up getting punished inappropriately,
[00:16:37.560 --> 00:16:39.920]   that that's exactly what the feds don't want
[00:16:39.920 --> 00:16:41.480]   because nobody in their right mind
[00:16:41.480 --> 00:16:44.600]   will at that point help the US government
[00:16:44.600 --> 00:16:45.880]   in trying to stop malware.
[00:16:45.880 --> 00:16:46.960]   I mean, that'll be that.
[00:16:46.960 --> 00:16:48.560]   - Well, that would be a good one.
[00:16:48.560 --> 00:16:52.960]   - I mean, he stopped wanting to cry
[00:16:52.960 --> 00:16:55.320]   and as a result, he's facing 40 years in prison.
[00:16:55.320 --> 00:16:57.760]   That is not the right outcome unless,
[00:16:57.760 --> 00:16:59.240]   and I think it's perfectly possible.
[00:16:59.240 --> 00:17:02.160]   You know hackers, it's perfectly possible
[00:17:02.160 --> 00:17:04.120]   that in addition to doing all this good stuff,
[00:17:04.120 --> 00:17:06.320]   he happened to write a banking Trojan
[00:17:06.320 --> 00:17:08.520]   that happened to bring down some banks.
[00:17:08.520 --> 00:17:11.520]   - Yeah, it is possible, however,
[00:17:11.520 --> 00:17:15.400]   I think you're assuming long-term rational thinking
[00:17:15.400 --> 00:17:17.000]   on behalf of the FBI.
[00:17:17.000 --> 00:17:17.840]   Yes.
[00:17:17.840 --> 00:17:19.080]   - You're assuming the opposite, however.
[00:17:19.080 --> 00:17:21.200]   - Well, no, what I'm saying is
[00:17:21.200 --> 00:17:24.200]   the FBI know that they need good security researchers
[00:17:24.200 --> 00:17:25.440]   to lock down, to make sure--
[00:17:25.440 --> 00:17:26.560]   - People like Marcus Hochins.
[00:17:26.560 --> 00:17:28.680]   - Yeah, I can be a system to secure.
[00:17:28.680 --> 00:17:30.240]   But if you're a regional FBI,
[00:17:30.240 --> 00:17:32.800]   so he's looking to make a name for himself in the field,
[00:17:32.800 --> 00:17:35.400]   you go for what you think you can take down.
[00:17:35.400 --> 00:17:37.440]   Now, looking through the indictment,
[00:17:37.440 --> 00:17:41.320]   it is very thin, and they state that he definitely wrote
[00:17:41.320 --> 00:17:43.080]   the chronos malware.
[00:17:43.080 --> 00:17:45.720]   Now, they better have really good evidence for that
[00:17:45.720 --> 00:17:47.160]   because people are gonna take it apart.
[00:17:47.160 --> 00:17:48.520]   - People defending him said,
[00:17:48.520 --> 00:17:50.720]   "Well, he never made any money from it."
[00:17:50.720 --> 00:17:53.840]   You know, he didn't sell it.
[00:17:53.840 --> 00:17:56.680]   But I think, and so I've heard some people say,
[00:17:56.680 --> 00:17:58.400]   "Well, it's like prosecuting the person
[00:17:58.400 --> 00:18:00.840]   "who made the gun for the crime committed by the gun."
[00:18:00.840 --> 00:18:03.280]   But if you're writing malware,
[00:18:03.280 --> 00:18:05.920]   I think that's a little different than making a gun.
[00:18:05.920 --> 00:18:08.480]   - Well, look at the guy, one of the people
[00:18:08.480 --> 00:18:10.880]   who went down for the TJ Maxx hacking incident.
[00:18:10.880 --> 00:18:13.600]   You had a programmer there who happened to write a fuzzer
[00:18:13.600 --> 00:18:16.320]   for somebody not knowing that it was gonna be used in malware.
[00:18:16.320 --> 00:18:17.720]   He got two years in prison for that.
[00:18:17.720 --> 00:18:18.560]   - Yeah.
[00:18:18.560 --> 00:18:19.640]   - This guy-- - That's wrong.
[00:18:19.640 --> 00:18:20.480]   - Yeah.
[00:18:20.480 --> 00:18:21.320]   - But I'll grant you that.
[00:18:21.320 --> 00:18:23.400]   - But all you've gotta do with a jury like this
[00:18:23.400 --> 00:18:27.200]   is convince two thirds or maybe three quarters of people
[00:18:27.200 --> 00:18:28.920]   of people on the jury.
[00:18:28.920 --> 00:18:30.240]   And I've had a prosecutor say to me,
[00:18:30.240 --> 00:18:32.560]   he worries about juries because it's basically made up
[00:18:32.560 --> 00:18:34.360]   of 12 people who couldn't work out
[00:18:34.360 --> 00:18:35.960]   to get out of jury.
[00:18:35.960 --> 00:18:38.040]   - Everybody who's ever been sued or prosecuted
[00:18:38.040 --> 00:18:38.920]   worries about juries.
[00:18:38.920 --> 00:18:39.760]   - Yeah.
[00:18:39.760 --> 00:18:40.600]   (laughs)
[00:18:40.600 --> 00:18:44.120]   - And I tell people, we have friends, family members
[00:18:44.120 --> 00:18:46.520]   who have been in panel for juries, they say serve.
[00:18:46.520 --> 00:18:48.760]   Because if you ever are on trial,
[00:18:48.760 --> 00:18:50.080]   you don't want it to be a jury of people
[00:18:50.080 --> 00:18:51.720]   who aren't smart enough to get off jury duty.
[00:18:51.720 --> 00:18:52.560]   - Exactly.
[00:18:52.560 --> 00:18:53.400]   No, right?
[00:18:53.400 --> 00:18:54.240]   - Right?
[00:18:54.240 --> 00:18:55.080]   - Yeah.
[00:18:55.080 --> 00:18:57.040]   - It's your duty if you're asked to serve on a jury.
[00:18:57.040 --> 00:18:58.320]   - And if you're technically literate,
[00:18:58.320 --> 00:18:59.800]   maybe you wanna get on this jury.
[00:18:59.800 --> 00:19:01.320]   - Well, if you're technically literate chances
[00:19:01.320 --> 00:19:02.160]   are you'll be kicked off.
[00:19:02.160 --> 00:19:03.000]   - You get bounced off.
[00:19:03.000 --> 00:19:03.840]   - Yeah, you get bounced off the lawyers.
[00:19:03.840 --> 00:19:05.440]   They don't want people who are technically literate.
[00:19:05.440 --> 00:19:06.600]   - One side does, one side does.
[00:19:06.600 --> 00:19:07.440]   - Rastucution doesn't.
[00:19:07.440 --> 00:19:08.280]   - Yeah.
[00:19:08.280 --> 00:19:09.720]   - All it takes is one side to boot.
[00:19:09.720 --> 00:19:11.280]   - At this point, I don't know what there is to say
[00:19:11.280 --> 00:19:12.120]   about this.
[00:19:12.120 --> 00:19:13.360]   I think the facts aren't really well known.
[00:19:13.360 --> 00:19:15.040]   It doesn't seem like it's a strong indictment,
[00:19:15.040 --> 00:19:17.440]   but it was highly redacted.
[00:19:17.440 --> 00:19:19.080]   The other shoe hasn't dropped yet.
[00:19:19.080 --> 00:19:19.920]   - Yeah.
[00:19:19.920 --> 00:19:20.760]   - Yeah, I mean.
[00:19:20.760 --> 00:19:21.920]   - But I think we will watch this with interest.
[00:19:21.920 --> 00:19:22.760]   Of course, as it develops.
[00:19:22.760 --> 00:19:23.600]   - Oh, absolutely.
[00:19:23.600 --> 00:19:26.120]   And I'm just gonna say I feel for the guy
[00:19:26.120 --> 00:19:27.720]   'cause he's stuck in a country
[00:19:27.720 --> 00:19:30.800]   that he doesn't know very well, he can't leave it.
[00:19:30.800 --> 00:19:32.960]   His job's up for balance.
[00:19:32.960 --> 00:19:36.960]   - And once he makes bail, I guess he's struck in Vegas, right?
[00:19:36.960 --> 00:19:37.800]   I mean, so he's in--
[00:19:37.800 --> 00:19:39.000]   - When he's stuck in Wisconsin?
[00:19:39.000 --> 00:19:39.840]   - In Wisconsin?
[00:19:39.840 --> 00:19:41.480]   - Yeah, well, honestly.
[00:19:41.480 --> 00:19:42.320]   - I'd rather be Wisconsin.
[00:19:42.320 --> 00:19:43.160]   - I would be stuck in Wisconsin.
[00:19:43.160 --> 00:19:45.080]   - It's not that likely the time of year, actually,
[00:19:45.080 --> 00:19:45.920]   in Wisconsin.
[00:19:45.920 --> 00:19:47.680]   - And it's a lousy time of year.
[00:19:47.680 --> 00:19:49.000]   - It's a terrible time in Vegas.
[00:19:49.000 --> 00:19:50.720]   - I was there for all three conferences
[00:19:50.720 --> 00:19:52.360]   and it was 105 degrees.
[00:19:52.360 --> 00:19:53.760]   - It would be much better to be in Wisconsin.
[00:19:53.760 --> 00:19:55.720]   - Yeah, humanity is not meant to live in them
[00:19:55.720 --> 00:19:57.040]   to those kind of temperatures.
[00:19:57.040 --> 00:19:58.960]   - But at least he's only being imprisoned
[00:19:58.960 --> 00:20:02.680]   in a FBI field office, which I imagine is better than likely.
[00:20:02.680 --> 00:20:04.560]   - He's not even imprisoned in a FBI field office.
[00:20:04.560 --> 00:20:06.720]   - No, he's now in a local jail.
[00:20:06.720 --> 00:20:08.040]   - Oh, he is, okay, got it, got it.
[00:20:08.040 --> 00:20:11.200]   - I was thinking like if an, I would take an FBI,
[00:20:11.200 --> 00:20:12.320]   the worst thing that could happen to you
[00:20:12.320 --> 00:20:14.120]   is probably awful coffee there, but no, he's--
[00:20:14.120 --> 00:20:17.160]   - I think the safe thing to say is there is a chilling effect
[00:20:17.160 --> 00:20:18.360]   on security research.
[00:20:18.360 --> 00:20:20.680]   We've seen this before, and this may well be
[00:20:20.680 --> 00:20:22.240]   another case of it where--
[00:20:22.240 --> 00:20:24.760]   - It also sets a, oh, sorry, go ahead.
[00:20:24.760 --> 00:20:27.200]   - No, no, no, no, please, Amy, I wanna hear from you.
[00:20:27.200 --> 00:20:31.840]   - I'm livid about this entire case,
[00:20:31.840 --> 00:20:35.400]   because the real challenge is that it sets a precedent
[00:20:35.400 --> 00:20:40.400]   for law enforcement going after cases involving technology
[00:20:40.400 --> 00:20:44.840]   going forward, and the reality is that most law enforcement
[00:20:44.840 --> 00:20:49.320]   agencies and offices don't have enough technical competency.
[00:20:49.320 --> 00:20:50.160]   - Exactly.
[00:20:50.160 --> 00:20:54.560]   - And our laws are nowhere near close being able to keep up
[00:20:54.560 --> 00:20:58.960]   with the changes in technology, whether that's in hacking
[00:20:58.960 --> 00:21:03.960]   or in AI or in bioethics or regardless of what it might be.
[00:21:03.960 --> 00:21:09.040]   So, you know, this to me should be a national conversation
[00:21:09.040 --> 00:21:10.920]   and it's not.
[00:21:10.920 --> 00:21:14.720]   And regardless of what happens to this guy
[00:21:14.720 --> 00:21:16.920]   and whether or not he serves time,
[00:21:16.920 --> 00:21:18.800]   there's gonna be another case and another case
[00:21:18.800 --> 00:21:20.960]   and another case and another case.
[00:21:20.960 --> 00:21:23.640]   And that should have us all very, very concerned.
[00:21:23.640 --> 00:21:24.480]   - All of us.
[00:21:24.480 --> 00:21:27.200]   - I was talking to researchers who were due to come out
[00:21:27.200 --> 00:21:28.640]   for DEF CON.
[00:21:28.640 --> 00:21:32.000]   One guy who's got dual French Moroccan citizenship,
[00:21:32.000 --> 00:21:35.440]   good respected bloke within the field, turned away,
[00:21:35.440 --> 00:21:38.040]   just told, he applied with his Moroccan passport,
[00:21:38.040 --> 00:21:40.160]   was told no, he applied with his French passport,
[00:21:40.160 --> 00:21:43.600]   was told no, this is coming up again and again and again.
[00:21:43.600 --> 00:21:46.280]   And after this arrest, sorry?
[00:21:46.280 --> 00:21:47.800]   - Is he on some kind of like a list?
[00:21:47.800 --> 00:21:48.640]   Is there like a--
[00:21:48.640 --> 00:21:50.600]   - Well, no, he wasn't given a reason,
[00:21:50.600 --> 00:21:51.800]   that's part of the problem.
[00:21:51.800 --> 00:21:53.560]   - Wonder if there is a question.
[00:21:53.560 --> 00:21:56.520]   - And now there's a, after the Marcus arrest,
[00:21:56.520 --> 00:21:59.280]   there was serious talk among the community,
[00:21:59.280 --> 00:22:01.800]   it was like, well, are we gonna come to DEF CON again?
[00:22:01.800 --> 00:22:04.480]   Are we gonna do SchmooCon, are we gonna do, you know,
[00:22:04.480 --> 00:22:05.320]   Derby CON?
[00:22:05.320 --> 00:22:07.640]   Are we actually gonna go and share this information?
[00:22:07.640 --> 00:22:10.200]   - I have to say at this point, you'd be nuts
[00:22:10.200 --> 00:22:12.040]   to come to the United States.
[00:22:12.040 --> 00:22:15.120]   If there is any reason at all to think there,
[00:22:15.120 --> 00:22:16.480]   you have a shadow on your record.
[00:22:16.480 --> 00:22:19.240]   If you're from Morocco, not said.
[00:22:19.240 --> 00:22:22.280]   If you're from a primarily Muslim nation, enough said.
[00:22:22.280 --> 00:22:25.920]   If you've ever touched a computer, enough said.
[00:22:25.920 --> 00:22:27.360]   - I think that that makes a lot of sense.
[00:22:27.360 --> 00:22:28.920]   And it's a shame for the United States.
[00:22:28.920 --> 00:22:31.440]   It's not that there are plenty of places people can go.
[00:22:31.440 --> 00:22:34.240]   There are plenty of places people can go for DEF CON.
[00:22:34.240 --> 00:22:37.200]   There's the Chaos Computer Club has a great conference.
[00:22:37.200 --> 00:22:38.560]   - Fantastic conference in Germany.
[00:22:38.560 --> 00:22:41.000]   - There's plenty of places you can go.
[00:22:41.000 --> 00:22:42.200]   They won't be coming here.
[00:22:42.200 --> 00:22:43.560]   - No, I don't blame them.
[00:22:43.560 --> 00:22:45.400]   - As a result, at this point, I don't blame them.
[00:22:45.400 --> 00:22:46.240]   - At this point, I'll blame them.
[00:22:46.240 --> 00:22:48.640]   - I mean, not that that's a good outcome.
[00:22:48.640 --> 00:22:49.600]   - Well, it's important to note,
[00:22:49.600 --> 00:22:51.720]   we're not just missing out on tourism dollars.
[00:22:51.720 --> 00:22:54.000]   I mean, we're missing out on intellectual capital
[00:22:54.000 --> 00:22:56.520]   that we are going to need going forward.
[00:22:56.520 --> 00:22:58.120]   Need now more than ever.
[00:22:58.120 --> 00:22:59.760]   - Yeah, absolutely.
[00:22:59.760 --> 00:23:00.640]   - And it's, you know, so fine.
[00:23:00.640 --> 00:23:02.760]   People jet in, they go to the conference and they leave,
[00:23:02.760 --> 00:23:04.840]   but while they're here, there's, you know,
[00:23:04.840 --> 00:23:07.280]   physical face-to-face networking that happens.
[00:23:07.280 --> 00:23:10.000]   There's, you know, transfer of knowledge.
[00:23:10.000 --> 00:23:13.560]   Maybe they stay and they do some work on behalf of us.
[00:23:13.560 --> 00:23:16.080]   I mean, this is a terrible precedent that's being said.
[00:23:16.080 --> 00:23:17.160]   - I mean, you know, it's interesting
[00:23:17.160 --> 00:23:21.720]   because in effect, really, this is the terrorist's winning.
[00:23:21.720 --> 00:23:23.400]   This is what happens when a nation
[00:23:23.400 --> 00:23:27.240]   gets very afraid and this started in 2001.
[00:23:27.240 --> 00:23:28.080]   - I know.
[00:23:28.080 --> 00:23:30.080]   - And it's what happens when a nation gets very afraid.
[00:23:30.080 --> 00:23:33.800]   They start doing, we start doing kind of things like this,
[00:23:33.800 --> 00:23:37.160]   but it has the negative, it has the exact reverse outcome.
[00:23:37.160 --> 00:23:38.000]   - Oh, that's right.
[00:23:38.000 --> 00:23:39.120]   - It doesn't make you safer.
[00:23:39.120 --> 00:23:41.040]   - It's playing entirely into the terrorists
[00:23:41.040 --> 00:23:43.560]   in the criminals game and I just wish Americans
[00:23:43.560 --> 00:23:45.360]   one such pussy's about this sort of thing.
[00:23:45.360 --> 00:23:46.520]   - Marsworms for mine.
[00:23:46.520 --> 00:23:47.360]   - Yeah, yeah.
[00:23:47.360 --> 00:23:49.240]   - Sorry, I shouldn't.
[00:23:49.240 --> 00:23:51.480]   - As a pussy, I'm glad to say thank you.
[00:23:51.480 --> 00:23:53.280]   - No, no, you're one of the good guys, Leo.
[00:23:53.280 --> 00:23:55.480]   But I'm sorry, you've really got to get over this
[00:23:55.480 --> 00:23:58.040]   because terrorism happens.
[00:23:58.040 --> 00:24:00.480]   - Playing their game means we lose.
[00:24:00.480 --> 00:24:02.160]   So that's-- - It happens a lot more often
[00:24:02.160 --> 00:24:04.320]   in Europe even than it does here.
[00:24:04.320 --> 00:24:07.200]   - Well, look at the rise of nationalism everywhere.
[00:24:07.200 --> 00:24:09.800]   Isn't that one of the side effects of this, right?
[00:24:09.800 --> 00:24:11.840]   This thing, well, this nativism, well,
[00:24:11.840 --> 00:24:14.200]   we can't let the other come into the country.
[00:24:14.200 --> 00:24:15.520]   Those are the people of the problem.
[00:24:15.520 --> 00:24:17.720]   - America was a country built on other people.
[00:24:17.720 --> 00:24:18.560]   - We're built on other people,
[00:24:18.560 --> 00:24:20.200]   and we are the others. - Taking the skills.
[00:24:20.200 --> 00:24:21.960]   - This reminds me of a story we talked about this
[00:24:21.960 --> 00:24:24.120]   on the security now on Wednesday.
[00:24:24.120 --> 00:24:30.360]   Nice story about a Hungarian kid, teenager, 18 years old.
[00:24:30.360 --> 00:24:35.320]   There's a brand new subway system in Budapest,
[00:24:35.320 --> 00:24:38.000]   the BKK, the Budapest Public Transportation Authority.
[00:24:38.000 --> 00:24:42.080]   He found that on the website, you could buy a ticket,
[00:24:42.080 --> 00:24:44.160]   any ticket at all, and you set your own price
[00:24:44.160 --> 00:24:46.960]   because the form on the website was so poorly coded,
[00:24:46.960 --> 00:24:50.360]   you could just go behind the scenes in the HTML
[00:24:50.360 --> 00:24:53.080]   and change the price, and it would accept it
[00:24:53.080 --> 00:24:55.640]   and you'd buy a $400 ticket for a penny.
[00:24:55.640 --> 00:24:57.560]   - Maybe they got old price line codes
[00:24:57.560 --> 00:24:59.520]   that they built the website from.
[00:24:59.520 --> 00:25:01.160]   - Name your price. - Name your price.
[00:25:01.160 --> 00:25:02.600]   I'll go for a warrant.
[00:25:02.600 --> 00:25:05.240]   He goes to the BKK and says,
[00:25:05.240 --> 00:25:07.560]   "You've got a problem on this website."
[00:25:07.560 --> 00:25:08.400]   He showed them the code.
[00:25:08.400 --> 00:25:10.120]   He didn't buy any tickets.
[00:25:10.120 --> 00:25:11.480]   I mean, he bought a ticket so he could show them the code,
[00:25:11.480 --> 00:25:12.640]   but he didn't use it.
[00:25:12.640 --> 00:25:13.600]   They arrested him.
[00:25:13.600 --> 00:25:14.440]   He's in jail.
[00:25:14.440 --> 00:25:16.800]   He ordered a $35 ticket for 20 cents.
[00:25:16.800 --> 00:25:19.000]   - Tickulous. - He is now in jail.
[00:25:19.000 --> 00:25:20.600]   This wasn't even hacking.
[00:25:20.600 --> 00:25:24.040]   This was looking at the underlying HTML.
[00:25:24.040 --> 00:25:25.440]   He went straight to them.
[00:25:25.440 --> 00:25:27.920]   He didn't buy tickets for his friends.
[00:25:27.920 --> 00:25:30.720]   Instead of thanking him, they called the police,
[00:25:30.720 --> 00:25:34.400]   had him arrested for hacking.
[00:25:34.400 --> 00:25:36.800]   This is actually, this isn't a way of completely
[00:25:36.800 --> 00:25:38.280]   analogous story. - So basically,
[00:25:38.280 --> 00:25:40.520]   you can still get really cheap subway tickets
[00:25:40.520 --> 00:25:41.360]   and Budapest, right?
[00:25:41.360 --> 00:25:42.600]   - They did not fix it.
[00:25:42.600 --> 00:25:44.760]   - Excellent. - They did not fix it.
[00:25:44.760 --> 00:25:47.000]   - He's a free subway all weekend long.
[00:25:47.000 --> 00:25:50.080]   This is exactly what we saw 20 years ago in the US.
[00:25:50.080 --> 00:25:51.920]   When people would call up companies and say,
[00:25:51.920 --> 00:25:53.960]   "Look, you've got this vulnerability."
[00:25:53.960 --> 00:25:57.320]   Far too many cases, they then got either legal hassles,
[00:25:57.320 --> 00:25:58.720]   they got the police knocking on the door.
[00:25:58.720 --> 00:26:00.280]   - Well, the Budapest we worked out always.
[00:26:00.280 --> 00:26:01.680]   - Transport. - They said,
[00:26:01.680 --> 00:26:03.480]   "Well, we've secured our system now."
[00:26:03.480 --> 00:26:04.320]   That's how they did.
[00:26:04.320 --> 00:26:06.200]   - We locked the kid up. - We locked the kid up.
[00:26:06.200 --> 00:26:08.200]   - We locked him up. - Other white hats
[00:26:08.200 --> 00:26:09.160]   have scrutinized the site.
[00:26:09.160 --> 00:26:12.040]   One Twitter user called it a goddamn train wreck.
[00:26:12.040 --> 00:26:14.680]   - It's a little close to home.
[00:26:14.680 --> 00:26:16.480]   - Yeah. - There are so many things wrong,
[00:26:16.480 --> 00:26:18.640]   but what it goes down to is a million dollar contract
[00:26:18.640 --> 00:26:22.920]   BKK with a Hungarian, had with a Hungarian IT company
[00:26:22.920 --> 00:26:24.480]   that created this site.
[00:26:24.480 --> 00:26:25.960]   And basically at this point,
[00:26:25.960 --> 00:26:28.080]   everybody's just covering their, you know what's.
[00:26:28.080 --> 00:26:29.480]   - Yeah. - But unfortunately,
[00:26:29.480 --> 00:26:31.480]   there's an 18 year old in Hungarian jail,
[00:26:31.480 --> 00:26:33.280]   which is somewhere he probably don't wanna be.
[00:26:33.280 --> 00:26:35.960]   - Not quite Turkish bad, but still pretty bad.
[00:26:35.960 --> 00:26:38.880]   - Every time you say BKK, I feel like it's a Burger King
[00:26:38.880 --> 00:26:41.120]   cover and a sandwich, you know, subway system.
[00:26:41.120 --> 00:26:42.760]   - Free, free, - He had it in place.
[00:26:42.760 --> 00:26:45.240]   - Why don't you want more BKK?
[00:26:45.240 --> 00:26:46.920]   - Amy Webb is here, she's the author.
[00:26:46.920 --> 00:26:49.120]   The signals are talking of futurist.
[00:26:49.120 --> 00:26:50.880]   And I have a good one for you, Amy.
[00:26:50.880 --> 00:26:53.720]   We're gonna talk about the Google memo when we come back.
[00:26:53.720 --> 00:26:59.800]   And you're gonna explain to me as an old white male,
[00:26:59.800 --> 00:27:01.680]   what's wrong with that?
[00:27:01.680 --> 00:27:04.320]   And we actually have a female Google engineer
[00:27:04.320 --> 00:27:07.560]   just happens to be here from Germany she's visiting.
[00:27:07.560 --> 00:27:09.280]   - Don't let her out. - She's not happy
[00:27:09.280 --> 00:27:10.120]   about it either.
[00:27:10.120 --> 00:27:12.320]   We'll find out what the women think.
[00:27:12.320 --> 00:27:14.320]   What does the ladies think of this memo?
[00:27:14.320 --> 00:27:15.880]   - I'll represent. - Will you represent?
[00:27:15.880 --> 00:27:17.200]   - Thank you, I'm glad you're here, Amy.
[00:27:17.200 --> 00:27:18.040]   It's really good to have you.
[00:27:18.040 --> 00:27:22.120]   Rob Reed also here on the heels of his spectacular new book,
[00:27:22.120 --> 00:27:26.680]   which everyone has to read after on at after-on.com.
[00:27:26.680 --> 00:27:29.520]   - We have more to listen to Leo because there is a--
[00:27:29.520 --> 00:27:31.040]   - I'm very briefly on it.
[00:27:31.040 --> 00:27:32.480]   There's a well-known yes.
[00:27:32.480 --> 00:27:35.440]   Tom Merritt's on it, better John Hodgman's on it,
[00:27:35.440 --> 00:27:38.120]   reading the phony Amazon reviews that it turns out
[00:27:38.120 --> 00:27:41.040]   you used to write as a hobby.
[00:27:41.040 --> 00:27:42.760]   And you have now incorporated in the noneh.
[00:27:42.760 --> 00:27:44.320]   Is it the same reviews that you wrote?
[00:27:44.320 --> 00:27:46.640]   - Yeah, yeah, so to get a little background.
[00:27:46.640 --> 00:27:47.480]   - So funny.
[00:27:47.480 --> 00:27:49.080]   - Back when I was running Rhapsody,
[00:27:49.080 --> 00:27:50.840]   which I found very stressful,
[00:27:50.840 --> 00:27:53.040]   part of me desperately wanted to be a fiction writer.
[00:27:53.040 --> 00:27:56.280]   And my personal therapy for late at night,
[00:27:56.280 --> 00:27:57.720]   after I was done with all my emails
[00:27:57.720 --> 00:27:59.800]   and all my managerial chores and all these things
[00:27:59.800 --> 00:28:02.200]   that were just so hard, I got online.
[00:28:02.200 --> 00:28:05.040]   This is before the, I think the playful Amazon review
[00:28:05.040 --> 00:28:06.200]   became a thing.
[00:28:06.200 --> 00:28:09.400]   I started writing these kind of deranged Amazon reviews
[00:28:09.400 --> 00:28:11.680]   and I called myself Charles Henry Higginsworth III
[00:28:11.680 --> 00:28:13.080]   of Boston, Massachusetts.
[00:28:13.080 --> 00:28:15.520]   This sort of character that I invented for myself.
[00:28:15.520 --> 00:28:18.160]   And he was like, you know, 20 years older than me
[00:28:18.160 --> 00:28:20.160]   and living in Boston and this crumbling mansion
[00:28:20.160 --> 00:28:22.600]   that it was an ancestral thing and they ran out of money
[00:28:22.600 --> 00:28:24.800]   and so he doesn't have money for heating oil.
[00:28:24.800 --> 00:28:26.840]   He'd start these reviews and he'd get about a third
[00:28:26.840 --> 00:28:29.240]   of the way in and do a 180 and start complaining about his life.
[00:28:29.240 --> 00:28:31.920]   I almost became a top thousand reviewer,
[00:28:31.920 --> 00:28:32.760]   which would have been really cool.
[00:28:32.760 --> 00:28:34.640]   - But none of these reviews were legit, right?
[00:28:34.640 --> 00:28:37.280]   - Well, no, they were all by this Mr. Higginsworth character.
[00:28:37.280 --> 00:28:39.880]   So all these years later, I realized that these reviews
[00:28:39.880 --> 00:28:42.760]   that I wrote in 2002 and are still on the web
[00:28:42.760 --> 00:28:45.840]   actually could plug very nicely into the storytelling
[00:28:45.840 --> 00:28:46.920]   of this novel.
[00:28:46.920 --> 00:28:48.520]   - You were writing the novel even back then.
[00:28:48.520 --> 00:28:50.000]   - Even back, it's like Boyhood, right?
[00:28:50.000 --> 00:28:52.160]   'Cause they are still up there and John Hodgman
[00:28:52.160 --> 00:28:53.000]   does read the money on it.
[00:28:53.000 --> 00:28:54.040]   - He'd be perfect for this.
[00:28:54.040 --> 00:28:54.880]   - He is fantastic.
[00:28:54.880 --> 00:28:55.920]   - I'll just read the first sentence of,
[00:28:55.920 --> 00:28:57.440]   this is the one you read and you're reading.
[00:28:57.440 --> 00:28:59.560]   This is a review of an actual book
[00:28:59.560 --> 00:29:04.240]   by the founder of Dunkin' Donuts called Time to Make the Donuts,
[00:29:04.240 --> 00:29:07.840]   the founder of Dunkin' Donuts shares an American story
[00:29:07.840 --> 00:29:10.840]   and your reviewer, Higginsworth writes,
[00:29:10.840 --> 00:29:12.840]   "Like a charmed wind, you're gonna love this.
[00:29:12.840 --> 00:29:14.560]   "As writers, you're gonna appreciate this.
[00:29:14.560 --> 00:29:15.760]   "This is a great lead.
[00:29:15.760 --> 00:29:19.120]   "Like a charmed wind, hurling vital presvisions
[00:29:19.120 --> 00:29:20.600]   "on to a castaway's beach.
[00:29:20.600 --> 00:29:23.480]   "Fate landed a copy of this in a conference room
[00:29:23.480 --> 00:29:26.840]   "in which I served a recent sentence to traffic school.
[00:29:26.840 --> 00:29:29.760]   "As reading was a scorned pastime among my fellow inmates,
[00:29:29.760 --> 00:29:31.640]   "I laid easy claim to the volume,
[00:29:31.640 --> 00:29:35.480]   "a mental sob for a mind numbed by the day's prattle."
[00:29:35.480 --> 00:29:36.400]   That'll give you some idea.
[00:29:36.400 --> 00:29:37.240]   - Hodgman does it better.
[00:29:37.240 --> 00:29:39.000]   - Hodgman does it beautifully.
[00:29:39.000 --> 00:29:40.680]   - I'm on it, Tom Merritt's on it.
[00:29:40.680 --> 00:29:43.840]   You actually have some great people on the audio version
[00:29:43.840 --> 00:29:44.920]   of this one. - Felicia Day.
[00:29:44.920 --> 00:29:46.480]   - Felicia Day, it's on all of the books.
[00:29:46.480 --> 00:29:48.320]   - Patrick Rothfuss.
[00:29:48.320 --> 00:29:49.160]   - Yes.
[00:29:49.160 --> 00:29:50.320]   - Name of the wind.
[00:29:50.320 --> 00:29:51.160]   - Name of the wind.
[00:29:51.160 --> 00:29:52.000]   - Yeah.
[00:29:52.000 --> 00:29:52.840]   - Wow.
[00:29:52.840 --> 00:29:53.680]   - Magnificent book.
[00:29:53.680 --> 00:29:57.480]   - He reads the faux spy novel that's embedded in the broader novel.
[00:29:57.480 --> 00:29:58.320]   - I'm telling you.
[00:29:58.320 --> 00:30:04.640]   - You look up Pick-A-R-esque, this novel has replaced Tom Jones
[00:30:04.640 --> 00:30:05.880]   in the dictionary.
[00:30:05.880 --> 00:30:06.720]   I don't know what I'm saying.
[00:30:06.720 --> 00:30:08.400]   - I'm looking for ways to get on your log rolling.
[00:30:08.400 --> 00:30:09.720]   - Thank you.
[00:30:09.720 --> 00:30:10.840]   - That wasn't it.
[00:30:10.840 --> 00:30:12.240]   - One of you said it's a fun reviews.
[00:30:12.240 --> 00:30:15.800]   That was my one for opening line, I thought.
[00:30:15.800 --> 00:30:16.880]   - Oh, cheese grater.
[00:30:16.880 --> 00:30:18.400]   - Yeah, that is great.
[00:30:18.400 --> 00:30:19.240]   - That one is.
[00:30:19.240 --> 00:30:20.400]   - I've never written an Amazon review.
[00:30:20.400 --> 00:30:22.360]   What am I missing out on?
[00:30:22.360 --> 00:30:24.680]   Amy, have you written an Amazon review?
[00:30:24.680 --> 00:30:26.360]   - I have written an Amazon review,
[00:30:26.360 --> 00:30:29.400]   but I'm one of those people that is on the opposite ends
[00:30:29.400 --> 00:30:30.560]   of the bell curve, right?
[00:30:30.560 --> 00:30:33.120]   So like most people, I write when I'm extremely disappointed
[00:30:33.120 --> 00:30:34.400]   or extremely happy.
[00:30:34.400 --> 00:30:36.120]   - Yeah, it's usually an extreme state.
[00:30:36.120 --> 00:30:38.800]   - That's what drives me to Yelp and TripAdvisor
[00:30:38.800 --> 00:30:39.640]   and things like that.
[00:30:39.640 --> 00:30:42.280]   - Yeah, I'm just got one of my last Yelp reviews up here
[00:30:42.280 --> 00:30:44.440]   and it begins, "I would soon reply cheese grater
[00:30:44.440 --> 00:30:46.440]   "to my scrotum than use EZ Plus again."
[00:30:46.440 --> 00:30:48.600]   (laughing)
[00:30:48.600 --> 00:30:50.440]   - That should be in your novel.
[00:30:50.440 --> 00:30:51.280]   - One day.
[00:30:51.280 --> 00:30:53.040]   - That's the end of the register.co.uk.
[00:30:53.040 --> 00:30:53.880]   We'll have more on Twitter.
[00:30:53.880 --> 00:30:58.360]   In just a second, I show today brought to you by Stamps.com.
[00:30:58.360 --> 00:31:00.440]   The post office in your office.
[00:31:00.440 --> 00:31:02.040]   You don't need to go to the post office anymore
[00:31:02.040 --> 00:31:03.360]   because you've got Stamps.com
[00:31:03.360 --> 00:31:05.800]   and everything you could do at the US Postal Service,
[00:31:05.800 --> 00:31:07.520]   you could, well, not if you're in Albania,
[00:31:07.520 --> 00:31:08.960]   but if you're in the United States,
[00:31:08.960 --> 00:31:11.920]   you could do from your desk with Stamps.com.
[00:31:11.920 --> 00:31:14.520]   You could buy and print real US postage.
[00:31:14.520 --> 00:31:16.000]   You do not need special ink.
[00:31:16.000 --> 00:31:17.440]   You do not need a special postage meter.
[00:31:17.440 --> 00:31:21.880]   You just need your computer, your printer and stamps.com.
[00:31:21.880 --> 00:31:24.560]   And I have to say, if you do mailing of any kind
[00:31:24.560 --> 00:31:27.240]   in your business, this is, it's just a no brainer.
[00:31:27.240 --> 00:31:28.400]   You gotta do this.
[00:31:28.400 --> 00:31:30.720]   But especially if you sell things on life,
[00:31:30.720 --> 00:31:34.000]   you sell it Etsy or eBay or Amazon,
[00:31:34.000 --> 00:31:36.880]   I get so many packages that just look,
[00:31:36.880 --> 00:31:38.560]   well, like they were made on Etsy.
[00:31:38.560 --> 00:31:40.840]   Like they were handmade, right, with twine
[00:31:40.840 --> 00:31:43.240]   and licked stamps going around the side
[00:31:43.240 --> 00:31:45.640]   and they put too many stamps on.
[00:31:45.640 --> 00:31:47.880]   You wanna give a little bit more professional appearance
[00:31:47.880 --> 00:31:50.400]   to your mailing, you gotta try Stamps.com.
[00:31:50.400 --> 00:31:52.000]   First of all, we're gonna give you an offer
[00:31:52.000 --> 00:31:53.360]   that comes with a USB scale.
[00:31:53.360 --> 00:31:54.880]   So you always have exactly the right postage.
[00:31:54.880 --> 00:31:57.760]   You know, putting on extra stamps just in case.
[00:31:57.760 --> 00:32:00.200]   You can actually print stamps from Stamps.com,
[00:32:00.200 --> 00:32:02.640]   but most people will put the package on the scale
[00:32:02.640 --> 00:32:05.040]   and print a very nice label for the package.
[00:32:05.040 --> 00:32:06.840]   You can also print right on the envelopes.
[00:32:06.840 --> 00:32:08.800]   Your label will include your return address,
[00:32:08.800 --> 00:32:10.300]   your business logo.
[00:32:10.300 --> 00:32:13.920]   It will even populate the address from the website
[00:32:13.920 --> 00:32:15.920]   so you don't have to type anything in.
[00:32:15.920 --> 00:32:17.560]   If you are selling overseas,
[00:32:17.560 --> 00:32:20.360]   it does all the customs forms automatically for you.
[00:32:20.360 --> 00:32:22.360]   If you're doing a certified mail
[00:32:22.360 --> 00:32:25.800]   or some other fancy mail where you have forms to fill out,
[00:32:25.800 --> 00:32:27.800]   it will fill out the forms for you.
[00:32:27.800 --> 00:32:30.600]   You even get discounts, you can't get it the post office
[00:32:30.600 --> 00:32:32.480]   on Stamps.com.
[00:32:32.480 --> 00:32:35.400]   Print official US postage for any letter, any package,
[00:32:35.400 --> 00:32:37.840]   any class of mail, it even suggests the right class.
[00:32:37.840 --> 00:32:39.280]   So Rob, if you're mailing out your book,
[00:32:39.280 --> 00:32:40.320]   it'll say, you know, these are books
[00:32:40.320 --> 00:32:43.160]   or you wanna do media mail and save you big money.
[00:32:43.160 --> 00:32:45.640]   Stamps.com, it's easy to create your account,
[00:32:45.640 --> 00:32:48.600]   no special equipment to lease, no long-term commitments,
[00:32:48.600 --> 00:32:50.000]   and we've got a special offer
[00:32:50.000 --> 00:32:52.160]   that gives you a chance to try Stamps.com
[00:32:52.160 --> 00:32:53.800]   and get some really nice stuff.
[00:32:53.800 --> 00:32:56.440]   Go to Stamps.com, click the microphone.
[00:32:56.440 --> 00:32:57.680]   In the upper right-hand corner,
[00:32:57.680 --> 00:33:00.600]   use our offer code TWIT, T-W-I-T,
[00:33:00.600 --> 00:33:04.200]   and we've got a $110 bonus offer that includes $55
[00:33:04.200 --> 00:33:07.480]   in free postage that you can use over the first few months.
[00:33:07.480 --> 00:33:08.960]   You can't use it all once, but a few months.
[00:33:08.960 --> 00:33:11.200]   It'd be good for your Christmas mailing list, how about that?
[00:33:11.200 --> 00:33:14.000]   You also get that great USB scale.
[00:33:14.000 --> 00:33:16.560]   You get a supply kit, you also get a free
[00:33:16.560 --> 00:33:19.560]   30daytrial of Stamps.com.
[00:33:19.560 --> 00:33:22.080]   Actually, I can't say free 'cause you have to pay postage
[00:33:22.080 --> 00:33:23.880]   on the scale, it's about five bucks,
[00:33:23.880 --> 00:33:26.240]   that's why they give you the $5 activity kit.
[00:33:26.240 --> 00:33:29.680]   It's a no risk trial offer, how about that?
[00:33:29.680 --> 00:33:34.120]   Stamps.com, we've used it for years,
[00:33:34.120 --> 00:33:37.000]   and if you're doing mail-in, you should do it too.
[00:33:37.000 --> 00:33:39.560]   You can thank me later, Stamps.com,
[00:33:39.560 --> 00:33:40.560]   and don't forget the offer code TWIT.
[00:33:40.560 --> 00:33:42.640]   Actually, that's how you thank me.
[00:33:42.640 --> 00:33:44.040]   We're talking about the week's tech news,
[00:33:44.040 --> 00:33:45.440]   it's so funny, Rob brought his,
[00:33:45.440 --> 00:33:47.200]   'cause last week we were talking about Apple
[00:33:47.200 --> 00:33:50.520]   phasing out the shuffle and the nano, right?
[00:33:50.520 --> 00:33:52.120]   And you brought your original shuffle,
[00:33:52.120 --> 00:33:53.280]   that's the one you hang around your neck.
[00:33:53.280 --> 00:33:55.200]   - Yeah, it was in the show notes,
[00:33:55.200 --> 00:33:57.360]   I hadn't realized you covered it last week,
[00:33:57.360 --> 00:34:01.360]   but yeah, I got this, I think it was in January of 2005.
[00:34:01.360 --> 00:34:03.360]   - Was the dopiest thing, remember Steve Jobs said,
[00:34:03.360 --> 00:34:05.240]   what we found is nobody really wants to know
[00:34:05.240 --> 00:34:07.840]   what they're playing, they just wanna play it.
[00:34:07.840 --> 00:34:10.080]   - Yeah, and you just, for his sake,
[00:34:10.080 --> 00:34:11.080]   - Steve was such a good salesman.
[00:34:11.080 --> 00:34:13.560]   - I also bought the diamond multimedia Rio,
[00:34:13.560 --> 00:34:17.480]   my baby's first MP3 player could carry all of 30 minutes
[00:34:17.480 --> 00:34:19.280]   of very low resolution.
[00:34:19.280 --> 00:34:21.760]   - That was, how much could you get on the shuffle?
[00:34:21.760 --> 00:34:24.280]   - The shuffle, this one was an eight hour one, I think.
[00:34:24.280 --> 00:34:26.000]   - Yeah, I think this was one of the fancy ones.
[00:34:26.000 --> 00:34:27.600]   - Just in the control of what you're listening to,
[00:34:27.600 --> 00:34:28.440]   just. - Absolutely not.
[00:34:28.440 --> 00:34:29.560]   - All you could do is shuffle.
[00:34:29.560 --> 00:34:30.480]   - All you could do is shuffle.
[00:34:30.480 --> 00:34:31.800]   - And then they made one that was,
[00:34:31.800 --> 00:34:33.080]   so that was the size of a Riggly's,
[00:34:33.080 --> 00:34:34.800]   Dick is Riggly's, and then they made one the size
[00:34:34.800 --> 00:34:36.480]   as dentine, right? - Oh, this is a mainframe.
[00:34:36.480 --> 00:34:37.840]   - And then there's one. - This is a mainframe
[00:34:37.840 --> 00:34:40.480]   compared to what they ended up being like tie clips.
[00:34:40.480 --> 00:34:41.800]   - Yeah, tie clips.
[00:34:41.800 --> 00:34:45.160]   All right, Amy, we're gonna put you on the spot.
[00:34:45.160 --> 00:34:48.000]   You have to represent all of woman kind.
[00:34:48.000 --> 00:34:52.680]   Although a number of men jumped in on this,
[00:34:52.680 --> 00:34:57.680]   so apparently there was a memo placed by a Googler,
[00:34:57.680 --> 00:35:00.440]   as yet unnamed Googler, although if his name
[00:35:00.440 --> 00:35:04.160]   is ever revealed, I think we'll find out about it.
[00:35:04.160 --> 00:35:07.680]   He posted it on apparently a meme site,
[00:35:07.680 --> 00:35:12.680]   as well as the internal equivalent of Google Plus.
[00:35:12.680 --> 00:35:17.680]   And the full content of the memo has been finally revealed
[00:35:17.680 --> 00:35:22.040]   by Gizmodo, they got it, wasn't probably very hard.
[00:35:22.040 --> 00:35:25.800]   And I'm gonna try to characterize this
[00:35:25.800 --> 00:35:27.400]   so that people can react to it.
[00:35:27.400 --> 00:35:29.280]   The reaction's been very, very strong.
[00:35:29.280 --> 00:35:31.640]   You know, Google has had a diversity problem.
[00:35:31.640 --> 00:35:33.640]   The FTC's been trying to investigate them.
[00:35:33.640 --> 00:35:36.600]   Google said, no, you can't have any documents.
[00:35:36.600 --> 00:35:39.400]   Google says we're satisfied that we're doing enough
[00:35:39.400 --> 00:35:43.360]   to hire more women and more minorities.
[00:35:43.360 --> 00:35:44.960]   It's apparent though, if you look at the numbers,
[00:35:44.960 --> 00:35:46.400]   they're not.
[00:35:46.400 --> 00:35:50.000]   Women engineers represent one in five Googlers,
[00:35:50.000 --> 00:35:52.440]   African-Americans and Hispanics even worse.
[00:35:52.440 --> 00:35:54.680]   I think it's one or two percent
[00:35:54.680 --> 00:35:57.520]   African-American engineers at Google's
[00:35:57.520 --> 00:36:01.720]   engineering division in the United States.
[00:36:01.720 --> 00:36:06.720]   But this memo says, well, this is just
[00:36:06.720 --> 00:36:08.960]   another form of discrimination.
[00:36:08.960 --> 00:36:13.960]   This all this diversity, all this attempt to,
[00:36:13.960 --> 00:36:16.120]   let me see if I can find the text of it.
[00:36:16.120 --> 00:36:18.480]   - It's pretty long.
[00:36:18.480 --> 00:36:20.400]   - Yeah, I don't wanna read it.
[00:36:20.400 --> 00:36:23.960]   - I can summarize.
[00:36:23.960 --> 00:36:26.960]   - Yeah, can you Amy, go ahead.
[00:36:26.960 --> 00:36:30.520]   - So there are different sections of it,
[00:36:30.520 --> 00:36:32.760]   but the sort of gist of it is,
[00:36:32.760 --> 00:36:35.280]   women don't make good coders.
[00:36:35.280 --> 00:36:37.880]   According to the person who wrote this, because.
[00:36:37.880 --> 00:36:40.960]   - They're genetically just not suited.
[00:36:40.960 --> 00:36:44.600]   - Right, genetically and socially not suited.
[00:36:44.600 --> 00:36:46.440]   Now-- - They have other skills,
[00:36:46.440 --> 00:36:48.080]   he's saying. - Right.
[00:36:48.080 --> 00:36:50.040]   - Women are extroverts, men are introverts.
[00:36:50.040 --> 00:36:52.280]   Right, so there's like a whole litany of--
[00:36:52.280 --> 00:36:56.280]   - Women like people, women like machines or things.
[00:36:56.280 --> 00:36:59.160]   This is like Larry Summers, then president of Harvard,
[00:36:59.160 --> 00:37:01.320]   saying, well, women aren't good mathematicians,
[00:37:01.320 --> 00:37:03.240]   it's just genetically the case.
[00:37:03.240 --> 00:37:05.320]   - So, thank you. - So, I love life.
[00:37:05.320 --> 00:37:06.960]   - Yeah, here's what I would say.
[00:37:06.960 --> 00:37:12.040]   My viewpoint on this is possibly a little controversial,
[00:37:12.040 --> 00:37:16.880]   I don't think so, but I would say that we are not all the same.
[00:37:17.800 --> 00:37:20.800]   That women and men are not the same,
[00:37:20.800 --> 00:37:25.040]   but I would also say that within women,
[00:37:25.040 --> 00:37:26.640]   there's quite a bit of diversity,
[00:37:26.640 --> 00:37:29.360]   just like there's quite a bit of diversity within men.
[00:37:29.360 --> 00:37:30.960]   The three of you sitting at that table
[00:37:30.960 --> 00:37:35.440]   are not archetypes or carbon copies of each other.
[00:37:35.440 --> 00:37:37.040]   At least I don't think so.
[00:37:37.040 --> 00:37:39.400]   And by this guy's definition,
[00:37:39.400 --> 00:37:43.360]   I would read like a 16 year old teenage boy,
[00:37:43.360 --> 00:37:45.840]   if we agree with what he,
[00:37:45.840 --> 00:37:49.440]   how he delineates the genders in the memo.
[00:37:49.440 --> 00:37:51.520]   So, there's a couple of things going on.
[00:37:51.520 --> 00:37:57.480]   One, he's making broad generalizations about women
[00:37:57.480 --> 00:38:01.120]   and about men, which any sociologist would tell you,
[00:38:01.120 --> 00:38:02.880]   don't track.
[00:38:02.880 --> 00:38:05.160]   So, his generalizations,
[00:38:05.160 --> 00:38:08.840]   and anybody who's a coder has taken math,
[00:38:08.840 --> 00:38:11.280]   which means that he should have known better
[00:38:11.280 --> 00:38:12.800]   than to generalize to begin with.
[00:38:12.800 --> 00:38:15.040]   - He says, so science will back me up.
[00:38:15.040 --> 00:38:17.840]   The sociology proves that.
[00:38:17.840 --> 00:38:18.680]   - It doesn't.
[00:38:18.680 --> 00:38:19.520]   - But it doesn't.
[00:38:19.520 --> 00:38:20.560]   - So, that's point one.
[00:38:20.560 --> 00:38:22.280]   Is just wrong.
[00:38:22.280 --> 00:38:26.800]   - Point two, this had to have been a pretty low level engineer
[00:38:26.800 --> 00:38:30.320]   because the way that he's describing some of the work
[00:38:30.320 --> 00:38:33.520]   would tell me that he hasn't yet worked on any more
[00:38:33.520 --> 00:38:35.480]   advanced projects at Google.
[00:38:35.480 --> 00:38:38.560]   He's probably like an entry level 23 year old kid.
[00:38:38.560 --> 00:38:40.680]   Nothing, you know,
[00:38:40.680 --> 00:38:42.520]   that's if there's anything wrong with 23 year old kids,
[00:38:42.520 --> 00:38:45.120]   but what it does tell me is that, you know,
[00:38:45.120 --> 00:38:46.600]   his work has been binary.
[00:38:46.600 --> 00:38:49.040]   So, he hasn't had to solve any real problems.
[00:38:49.040 --> 00:38:51.680]   And anybody who's ever had to think about the future
[00:38:51.680 --> 00:38:54.160]   or about the past or solve any real problems
[00:38:54.160 --> 00:38:56.680]   understands that biodiversity is a good thing.
[00:38:56.680 --> 00:39:00.760]   You know, and that teams are requisite.
[00:39:00.760 --> 00:39:05.040]   So, to me, you know, this was probably written by somebody
[00:39:05.040 --> 00:39:07.960]   in reaction to either a bad performance review
[00:39:07.960 --> 00:39:09.440]   or a bad day at work,
[00:39:09.440 --> 00:39:11.440]   or, you know, feeling left out of the club,
[00:39:11.440 --> 00:39:14.680]   or maybe he got rejected by a fellow coder,
[00:39:14.680 --> 00:39:17.040]   a developer who happened to be a woman,
[00:39:17.040 --> 00:39:18.400]   you know, could have been any of those things.
[00:39:18.400 --> 00:39:21.600]   But there's so much inexperience
[00:39:21.600 --> 00:39:26.600]   throughout the entire memo to me that, you know,
[00:39:26.600 --> 00:39:30.720]   it sounds like a petulant 20 year old who's, you know,
[00:39:30.720 --> 00:39:32.320]   who doesn't have a lot of experience
[00:39:32.320 --> 00:39:33.760]   and doesn't know what he's talking about anyway.
[00:39:33.760 --> 00:39:36.920]   So, I wasn't super angry or upset by the memo.
[00:39:36.920 --> 00:39:41.280]   Now, what's been interesting is the,
[00:39:41.280 --> 00:39:43.920]   the reaction to that memo,
[00:39:43.920 --> 00:39:48.280]   which to me almost feels a little inauthentic.
[00:39:48.280 --> 00:39:50.480]   It feels as though in the year 20,
[00:39:50.480 --> 00:39:52.760]   what was the year that movie PCU came out?
[00:39:52.760 --> 00:39:53.800]   Do you guys remember that movie?
[00:39:53.800 --> 00:39:55.320]   It was like early years.
[00:39:55.320 --> 00:39:56.240]   - It was quite some time ago.
[00:39:56.240 --> 00:39:58.280]   It was like 94, 95.
[00:39:58.280 --> 00:39:59.840]   - I feel like we've circled, you know,
[00:39:59.840 --> 00:40:01.440]   all of history cycle, right?
[00:40:01.440 --> 00:40:02.280]   - Yeah.
[00:40:02.280 --> 00:40:04.480]   - Yeah, I feel like we've hit that cycle again,
[00:40:04.480 --> 00:40:09.480]   where our responses are expected,
[00:40:10.720 --> 00:40:12.120]   and I feel like some of the responses
[00:40:12.120 --> 00:40:14.880]   that I've seen to this memo have almost been formulaic.
[00:40:14.880 --> 00:40:16.960]   - What about Google's response though?
[00:40:16.960 --> 00:40:19.000]   That's the one that really matters.
[00:40:19.000 --> 00:40:21.160]   And some people have been critical
[00:40:21.160 --> 00:40:23.600]   about the Google management's response to this,
[00:40:23.600 --> 00:40:27.800]   which was we wanna make a safe place at Google
[00:40:27.800 --> 00:40:31.560]   for all beliefs, including these beliefs.
[00:40:31.560 --> 00:40:32.560]   - Which is why it would maybe think--
[00:40:32.560 --> 00:40:33.400]   - Was that an official,
[00:40:33.400 --> 00:40:34.640]   what was that a press release that they,
[00:40:34.640 --> 00:40:37.000]   yeah, that was their official response to it.
[00:40:37.000 --> 00:40:37.840]   - Right, right.
[00:40:37.840 --> 00:40:40.120]   They just hired a diversity head.
[00:40:40.120 --> 00:40:41.920]   - Yeah, she's literally been in the job two weeks,
[00:40:41.920 --> 00:40:43.680]   and then this blips up.
[00:40:43.680 --> 00:40:46.360]   - So it's interesting you say what you said, Amy,
[00:40:46.360 --> 00:40:49.120]   because before the show, you mentioned you hadn't read
[00:40:49.120 --> 00:40:52.560]   Janatan Zungar's medium piece about this.
[00:40:52.560 --> 00:40:53.640]   - No, no, no, I had.
[00:40:53.640 --> 00:40:54.720]   - You had, okay.
[00:40:54.720 --> 00:40:55.560]   - Yeah, yeah, yeah.
[00:40:55.560 --> 00:40:56.560]   - So he was actually--
[00:40:56.560 --> 00:40:57.400]   - He said the same thing.
[00:40:57.400 --> 00:40:58.920]   - He said exactly the same thing.
[00:40:58.920 --> 00:41:03.720]   He's a very well-known Googler, very politically active.
[00:41:03.720 --> 00:41:06.000]   He has recently left Google, so he said,
[00:41:06.000 --> 00:41:07.720]   well, I could speak a little more freely about this
[00:41:07.720 --> 00:41:09.680]   than I would have if I were still a manager at Google.
[00:41:09.680 --> 00:41:11.280]   And he said exactly the same thing.
[00:41:11.280 --> 00:41:14.280]   He said, when you first start coding,
[00:41:14.280 --> 00:41:16.760]   you are just dealing with a machine,
[00:41:16.760 --> 00:41:19.120]   but as you get more senior, you realize,
[00:41:19.120 --> 00:41:22.040]   really the job is not coding.
[00:41:22.040 --> 00:41:23.960]   The job is solving problems,
[00:41:23.960 --> 00:41:27.720]   and in particular, interacting with other people
[00:41:27.720 --> 00:41:29.160]   to solve those problems.
[00:41:29.160 --> 00:41:32.040]   It isn't, it isn't, and I think you're right,
[00:41:32.040 --> 00:41:34.800]   I think a lot of young coders starting off
[00:41:34.800 --> 00:41:36.360]   think what coding is all about
[00:41:36.360 --> 00:41:38.600]   is learning how to instruct the machine.
[00:41:38.600 --> 00:41:42.480]   And he says, essentially, this is younger,
[00:41:42.480 --> 00:41:44.960]   engineering is all about cooperation, collaboration,
[00:41:44.960 --> 00:41:47.760]   and empathy for both your colleagues and customers,
[00:41:47.760 --> 00:41:51.320]   something that the memo writer said,
[00:41:51.320 --> 00:41:53.520]   oh, those are female traits.
[00:41:53.520 --> 00:41:55.040]   If someone told you engineering was a field
[00:41:55.040 --> 00:41:56.560]   where you could get away with not dealing
[00:41:56.560 --> 00:41:58.720]   with people or feelings, then I'm very sorry
[00:41:58.720 --> 00:42:00.840]   to tell you, you've been lied to.
[00:42:00.840 --> 00:42:02.760]   Solitary work is something that only happens
[00:42:02.760 --> 00:42:06.040]   at the most junior levels.
[00:42:06.040 --> 00:42:09.040]   And in fact, the traits which the manifesto described
[00:42:09.040 --> 00:42:10.320]   as female are the core traits,
[00:42:10.320 --> 00:42:12.480]   which makes someone successful at engineering.
[00:42:12.480 --> 00:42:14.880]   But I think the bigger issue, Amy,
[00:42:14.880 --> 00:42:20.080]   is that the lack of diversity at Google
[00:42:20.080 --> 00:42:25.080]   and in Silicon Valley, does it hurt Google?
[00:42:25.080 --> 00:42:26.920]   Does it hurt the product?
[00:42:26.920 --> 00:42:30.840]   - Well, I would say yes.
[00:42:30.840 --> 00:42:34.560]   And I would say, one of the things that I'm researching
[00:42:34.560 --> 00:42:36.680]   most intently is artificial intelligence.
[00:42:36.680 --> 00:42:39.160]   So that's been a big part of my life
[00:42:39.160 --> 00:42:40.840]   for the past couple of years.
[00:42:40.840 --> 00:42:44.920]   And you can see the strange and weird ways
[00:42:44.920 --> 00:42:48.720]   that AI is starting to break or break down.
[00:42:48.720 --> 00:42:53.720]   And that's a result of having too few people
[00:42:53.720 --> 00:42:56.880]   with too narrow a worldview,
[00:42:56.880 --> 00:42:59.280]   trying to solve problems together.
[00:42:59.280 --> 00:43:03.880]   So everybody that I know that works in tech
[00:43:03.880 --> 00:43:06.400]   likes to talk incessantly about nature.
[00:43:06.400 --> 00:43:09.840]   And yet they seem to not take some very basic cues
[00:43:09.840 --> 00:43:11.560]   from nature.
[00:43:11.560 --> 00:43:15.320]   Nature tells us that biodiversity is good
[00:43:15.320 --> 00:43:17.200]   for the ecosystem.
[00:43:17.200 --> 00:43:22.600]   And it's always good to introduce complimentary
[00:43:22.600 --> 00:43:27.080]   species and life forms to be together.
[00:43:27.080 --> 00:43:32.000]   What I find so interesting is that we see study
[00:43:32.000 --> 00:43:34.960]   after study and anecdotally and evidence
[00:43:34.960 --> 00:43:36.680]   all around the biodiversity is great.
[00:43:36.680 --> 00:43:39.680]   And yet we don't seem to take that advice
[00:43:39.680 --> 00:43:44.440]   when it comes to staffing the offices.
[00:43:44.440 --> 00:43:45.280]   - It's a monocleure.
[00:43:45.280 --> 00:43:47.120]   - Where the stuff is being built, right.
[00:43:47.120 --> 00:43:51.680]   Now, it's difficult.
[00:43:51.680 --> 00:43:53.600]   And it's hard to have this conversation
[00:43:53.600 --> 00:43:56.760]   without using those broad generalizations.
[00:43:56.760 --> 00:43:57.600]   - Right.
[00:43:57.600 --> 00:43:59.280]   Well, that's so I want you to school me
[00:43:59.280 --> 00:44:02.280]   because then I'm gonna walk right into a minefield here.
[00:44:02.280 --> 00:44:04.880]   But that's okay.
[00:44:04.880 --> 00:44:06.320]   I wanna be educated here.
[00:44:06.320 --> 00:44:09.920]   It is true that, I mean, and of course,
[00:44:09.920 --> 00:44:12.880]   every individual is a unique individual.
[00:44:12.880 --> 00:44:15.280]   And many guys have more female traits,
[00:44:15.280 --> 00:44:18.760]   many women have more male traits, whatever.
[00:44:18.760 --> 00:44:20.840]   My wife is much butcher than I am.
[00:44:20.840 --> 00:44:25.000]   But it seems to me there are gender,
[00:44:25.000 --> 00:44:26.520]   are there gender differences?
[00:44:28.440 --> 00:44:30.760]   - Well, I think it depends on who you ask, right?
[00:44:30.760 --> 00:44:35.320]   I think Dinesh D'Souza, who's a right wing conservative
[00:44:35.320 --> 00:44:38.800]   writer would tell you that stereotypes are born out of fact
[00:44:38.800 --> 00:44:40.760]   and that they exist for a reason.
[00:44:40.760 --> 00:44:43.760]   I think that any sociologist who's had any real training
[00:44:43.760 --> 00:44:45.920]   would tell you that it depends on the data set.
[00:44:45.920 --> 00:44:49.520]   And gender is not just influenced biologically,
[00:44:49.520 --> 00:44:52.480]   but also by the cultural norms and standards in each country
[00:44:52.480 --> 00:44:53.960]   and region and everything else.
[00:44:53.960 --> 00:44:56.520]   - And yet, you have kids, right?
[00:44:56.520 --> 00:44:57.440]   - Yeah, I have a kid, yeah.
[00:44:57.440 --> 00:44:59.800]   Okay, well, we have a boy and a girl.
[00:44:59.800 --> 00:45:04.360]   And I'm pretty convinced that we did not in any way
[00:45:04.360 --> 00:45:05.920]   try to push them in any direction,
[00:45:05.920 --> 00:45:10.920]   but the boy liked playing with guns and balls
[00:45:10.920 --> 00:45:14.600]   and the girl did not.
[00:45:14.600 --> 00:45:16.720]   And this was out of the womb.
[00:45:16.720 --> 00:45:18.360]   This was very early on.
[00:45:18.360 --> 00:45:20.600]   I don't think this was socialization.
[00:45:20.600 --> 00:45:25.880]   So there's a massive debate over nature versus nurture
[00:45:25.880 --> 00:45:28.880]   and it's very hard to know what's nature and what's nurture.
[00:45:28.880 --> 00:45:33.640]   I guess, look, I think we do need diversity.
[00:45:33.640 --> 00:45:36.680]   Clearly we need diversity in Silicon Valley and everywhere.
[00:45:36.680 --> 00:45:39.680]   Everywhere is, every workplace is better
[00:45:39.680 --> 00:45:41.400]   if it's a diverse workplace.
[00:45:41.400 --> 00:45:42.880]   It's just better for everything,
[00:45:42.880 --> 00:45:44.840]   for everybody in the workplace.
[00:45:44.840 --> 00:45:46.560]   I know that, that's true.
[00:45:46.560 --> 00:45:49.520]   But part of the reason that's true
[00:45:49.520 --> 00:45:52.240]   is because people bring different flavors to it.
[00:45:52.240 --> 00:45:56.080]   And I think you, I don't think it's unreasonable to observe
[00:45:56.080 --> 00:45:58.280]   that women bring different flavors than men.
[00:45:58.280 --> 00:46:00.800]   Or is that really a bad thing to look at?
[00:46:00.800 --> 00:46:03.680]   - Here's what I would say.
[00:46:03.680 --> 00:46:07.600]   Humans are not wired for change.
[00:46:07.600 --> 00:46:11.320]   We are, our limbic systems react very poorly
[00:46:11.320 --> 00:46:12.920]   when the status quo is challenged.
[00:46:12.920 --> 00:46:15.160]   That puts us into fight or flight mode,
[00:46:15.160 --> 00:46:18.200]   and it causes us to make bad decisions.
[00:46:18.200 --> 00:46:20.240]   And more importantly, it causes us to pine
[00:46:20.240 --> 00:46:22.080]   for the way that things used to be.
[00:46:22.080 --> 00:46:23.640]   And it's because of our limbic systems
[00:46:23.640 --> 00:46:26.720]   that a lot of people are not good at thinking about the future.
[00:46:26.720 --> 00:46:29.360]   However, if we look from a statistical standpoint,
[00:46:29.360 --> 00:46:34.800]   that change is often better, right?
[00:46:34.800 --> 00:46:37.760]   And that it helps us approach and understand the future
[00:46:37.760 --> 00:46:39.040]   in a better way.
[00:46:39.040 --> 00:46:42.800]   What's happening right now is that technology has,
[00:46:42.800 --> 00:46:44.120]   reach a certain inflection point.
[00:46:44.120 --> 00:46:46.400]   We have enough people who are trained
[00:46:46.400 --> 00:46:47.880]   that can go into the field.
[00:46:47.880 --> 00:46:50.480]   We have enough compute power to be able
[00:46:50.480 --> 00:46:52.320]   to work on meaningful projects.
[00:46:52.320 --> 00:46:57.320]   And because of how things have always been,
[00:46:57.320 --> 00:47:01.960]   those roles are predominantly dominated by white males,
[00:47:01.960 --> 00:47:06.400]   and males from other, a few other countries.
[00:47:06.400 --> 00:47:08.840]   So this is really about change.
[00:47:08.840 --> 00:47:10.920]   And it happens through every field.
[00:47:10.920 --> 00:47:13.800]   If you look throughout time, it's happened in every field
[00:47:13.800 --> 00:47:15.400]   and in every case.
[00:47:15.400 --> 00:47:17.400]   And once that change happens,
[00:47:17.400 --> 00:47:21.520]   and the workforce has become more diversified,
[00:47:21.520 --> 00:47:23.320]   they tend to be more productive, they do better.
[00:47:23.320 --> 00:47:24.840]   There's all this data showing that.
[00:47:24.840 --> 00:47:27.320]   So I think when it comes down to it,
[00:47:27.320 --> 00:47:31.400]   we're just dealing with the latest round of this,
[00:47:31.400 --> 00:47:33.680]   and we feel it and sense it acutely
[00:47:33.680 --> 00:47:35.400]   because it's the field that we all pay attention,
[00:47:35.400 --> 00:47:37.560]   that us, the people who are on the show
[00:47:37.560 --> 00:47:40.720]   and who are listening to this show and watching it,
[00:47:40.720 --> 00:47:43.320]   it feels acute because we pay attention to it.
[00:47:43.320 --> 00:47:45.400]   But it's the same story that's been written
[00:47:45.400 --> 00:47:49.960]   over and over again in law and in academia,
[00:47:49.960 --> 00:47:53.160]   and in all of these other different fields and industries.
[00:47:53.160 --> 00:47:58.840]   - I remember talking to a man named Jonathan Hite.
[00:47:58.840 --> 00:48:00.320]   You probably know his stuff.
[00:48:00.320 --> 00:48:01.880]   He wrote a book called The Righteous Mind,
[00:48:01.880 --> 00:48:04.960]   why gold people are divided by politics and religion.
[00:48:04.960 --> 00:48:07.320]   He's an interesting fellow with some very interesting
[00:48:07.320 --> 00:48:09.520]   points of view, somewhat echoed by the way,
[00:48:09.520 --> 00:48:11.520]   in this Google memo.
[00:48:13.920 --> 00:48:15.440]   This is such a complicated subject,
[00:48:15.440 --> 00:48:17.600]   I want us to do this in a succinct way so we can move on.
[00:48:17.600 --> 00:48:21.600]   But I also, the memo says that there is unconscious bias
[00:48:21.600 --> 00:48:25.280]   at Google, but it's unconscious bias towards
[00:48:25.280 --> 00:48:27.120]   what we traditionally call the left,
[00:48:27.120 --> 00:48:28.200]   as opposed to the right.
[00:48:28.200 --> 00:48:29.640]   He says the bias in the left,
[00:48:29.640 --> 00:48:32.280]   and this is what Jonathan Hite's research also shows.
[00:48:32.280 --> 00:48:34.640]   Things like mercy, compassion for the weak,
[00:48:34.640 --> 00:48:38.720]   disparities, a real sense of disparities due to injustice,
[00:48:38.720 --> 00:48:41.360]   open, idealistic.
[00:48:41.360 --> 00:48:42.920]   And by the way, one of the things he says,
[00:48:42.920 --> 00:48:44.680]   echoes what you just said, Amy,
[00:48:44.680 --> 00:48:47.760]   the left biases towards change is good.
[00:48:47.760 --> 00:48:50.520]   The right or conservative, let's maybe say conservative
[00:48:50.520 --> 00:48:52.240]   versus liberal, but conservative bias
[00:48:52.240 --> 00:48:54.440]   would be respect for authority.
[00:48:54.440 --> 00:48:57.400]   Humans are competitive, change is dangerous.
[00:48:57.400 --> 00:48:58.840]   We're more pragmatic.
[00:48:58.840 --> 00:49:02.600]   He says Google airs so far on the liberal side
[00:49:02.600 --> 00:49:07.200]   that it misses some values that the conservative side
[00:49:07.200 --> 00:49:08.040]   could offer.
[00:49:08.040 --> 00:49:11.240]   For instance, he says, it's a good thing to have a programmer
[00:49:11.240 --> 00:49:15.840]   who doesn't like change, who wants to maintain a product.
[00:49:15.840 --> 00:49:17.640]   And the problem with Google does have this problem
[00:49:17.640 --> 00:49:19.960]   where nobody wants to maintain an existing product.
[00:49:19.960 --> 00:49:21.800]   They all want to go to the next thing.
[00:49:21.800 --> 00:49:24.880]   And he says, a company too far to the left
[00:49:24.880 --> 00:49:26.760]   will constantly be changing,
[00:49:26.760 --> 00:49:31.120]   deprecating much-loved services over diversify its interest,
[00:49:31.120 --> 00:49:32.800]   ignoring or being ashamed of its core business
[00:49:32.800 --> 00:49:35.120]   and overly trust its employees and competitors.
[00:49:35.120 --> 00:49:37.840]   He says it could do with a little bit of this,
[00:49:37.840 --> 00:49:39.960]   more conservative thinking.
[00:49:39.960 --> 00:49:42.800]   And Jonathan Haidt says, neither side is right.
[00:49:42.800 --> 00:49:44.600]   Haidt, he always say, Haidt,
[00:49:44.600 --> 00:49:46.840]   says neither side is right in this,
[00:49:46.840 --> 00:49:48.320]   but there are these very different ways
[00:49:48.320 --> 00:49:50.440]   of looking at the world.
[00:49:50.440 --> 00:49:53.680]   And I don't think this memo is wrong in one sense.
[00:49:53.680 --> 00:49:57.760]   Google and most of Silicon Valley
[00:49:57.760 --> 00:50:02.200]   is so progressive focused that there isn't a lot of room
[00:50:02.200 --> 00:50:03.920]   for somebody who's more conservative.
[00:50:03.920 --> 00:50:07.760]   - So how many, do any of the three of you
[00:50:07.760 --> 00:50:09.920]   happen to know how many people work at Google?
[00:50:09.920 --> 00:50:13.760]   - Well, do you mean like engineers
[00:50:13.760 --> 00:50:15.360]   or do you mean in general?
[00:50:15.360 --> 00:50:16.840]   - Just in general.
[00:50:16.840 --> 00:50:19.840]   - You have the number of three, it's 121,000
[00:50:19.840 --> 00:50:21.280]   but I'm not entirely sure.
[00:50:21.280 --> 00:50:22.120]   - It's a lot.
[00:50:22.120 --> 00:50:22.960]   - It's six feet.
[00:50:22.960 --> 00:50:23.800]   - It's a lot.
[00:50:23.800 --> 00:50:24.640]   It's a lot.
[00:50:24.640 --> 00:50:28.120]   So statistically speaking, isn't it possible that,
[00:50:28.120 --> 00:50:33.600]   just like any other group of people in any other bell curve,
[00:50:33.600 --> 00:50:36.040]   you've got a bunch of people who are in the middle.
[00:50:36.040 --> 00:50:38.080]   You have a bunch of people who are out,
[00:50:38.080 --> 00:50:40.800]   and then you have your outliers, right?
[00:50:40.800 --> 00:50:43.080]   Again, I think here's,
[00:50:43.080 --> 00:50:47.200]   I worry about hive mind mentality circulating
[00:50:47.200 --> 00:50:48.800]   as a result. - In any direction, right?
[00:50:48.800 --> 00:50:51.280]   - In any case, but also having specifically
[00:50:51.280 --> 00:50:52.320]   to do with this memo.
[00:50:52.320 --> 00:50:57.160]   This memo goes wrong as far as I'm concerned
[00:50:57.160 --> 00:51:00.000]   because of the broad generalizations that it makes.
[00:51:00.000 --> 00:51:01.680]   - That's always in danger, yeah.
[00:51:01.680 --> 00:51:04.400]   - That's right, but, and this is my,
[00:51:04.400 --> 00:51:06.480]   I was a debate nerd all the way through high school
[00:51:06.480 --> 00:51:07.920]   and college.
[00:51:07.920 --> 00:51:10.200]   If I put my debating hat back on,
[00:51:10.200 --> 00:51:12.560]   like there's no way that I could argue,
[00:51:12.560 --> 00:51:16.480]   any argument that comes out of the assertions
[00:51:16.480 --> 00:51:19.120]   that are being made due to these broad strokes
[00:51:19.120 --> 00:51:22.080]   is a completely worthless argument, right?
[00:51:22.080 --> 00:51:24.680]   - It's paid for us not logos or something like that.
[00:51:24.680 --> 00:51:26.400]   - Well, but for us to now,
[00:51:26.400 --> 00:51:27.480]   - I'm not kidding in terms.
[00:51:27.480 --> 00:51:29.720]   - To Google is not,
[00:51:29.720 --> 00:51:31.560]   Google is not specifically hiring people
[00:51:31.560 --> 00:51:34.120]   who are left of the political spectrum.
[00:51:34.120 --> 00:51:36.840]   There are a lot of people who work at that company
[00:51:36.840 --> 00:51:38.360]   and this is not in defense of Google.
[00:51:38.360 --> 00:51:42.040]   Google has some serious problems when it comes to diversity.
[00:51:42.040 --> 00:51:46.280]   But I just think we ought to be careful and cautious
[00:51:46.280 --> 00:51:51.280]   in making these assertions that everybody at Google
[00:51:51.280 --> 00:51:56.280]   is left leaning, everybody in Silicon Valley is left leaning.
[00:51:56.280 --> 00:51:57.800]   - Well, we know it's not true.
[00:51:57.800 --> 00:51:59.560]   I know if a Trump letter works at Google.
[00:51:59.560 --> 00:52:01.080]   - Yeah. - No, we know that it's not true.
[00:52:01.080 --> 00:52:03.400]   - It happens, but I mean, but isn't there a bias?
[00:52:03.400 --> 00:52:05.800]   But I guess what he's saying, this memorizer saying,
[00:52:05.800 --> 00:52:07.520]   - There is a whole liquid. - There's a bias.
[00:52:07.520 --> 00:52:11.160]   - There's a bunch of, oh, there's a bunch of unconscious bias.
[00:52:11.160 --> 00:52:14.120]   But that unconscious bias is in a different place.
[00:52:14.120 --> 00:52:17.160]   And you can see that in,
[00:52:17.160 --> 00:52:22.000]   here, let's do a little thought experiment.
[00:52:22.000 --> 00:52:25.080]   Go to Google image search and type in CEO.
[00:52:25.080 --> 00:52:27.160]   - Oh yeah, oh yeah. - And yeah.
[00:52:27.160 --> 00:52:28.720]   - You're screaming. - All right.
[00:52:28.720 --> 00:52:31.200]   Who is the first woman CEO that shows up?
[00:52:31.200 --> 00:52:32.200]   - Oh, this is a good test.
[00:52:32.200 --> 00:52:33.280]   Let's see.
[00:52:33.280 --> 00:52:34.840]   - A little bit of that images.
[00:52:34.840 --> 00:52:36.200]   - I haven't done this in a couple of days.
[00:52:36.200 --> 00:52:38.440]   - Man, man, man, man, lot of clip art, man, man, man, man.
[00:52:38.440 --> 00:52:39.280]   Well, let's see.
[00:52:39.280 --> 00:52:40.280]   Here's the first-- - We'll have to see the end.
[00:52:40.280 --> 00:52:41.120]   - I should have see, right?
[00:52:41.120 --> 00:52:42.200]   - Is this it?
[00:52:42.200 --> 00:52:43.200]   Is this it?
[00:52:43.200 --> 00:52:44.480]   Oh, here's another one.
[00:52:44.480 --> 00:52:46.360]   Okay, I had to go to two rows.
[00:52:46.360 --> 00:52:50.200]   Charity Navigator CEO compensation study.
[00:52:50.200 --> 00:52:51.680]   I think that's close. - Oh, that's a double close.
[00:52:51.680 --> 00:52:52.880]   - I think that's clip art.
[00:52:52.880 --> 00:52:54.440]   Here's an actual CEO.
[00:52:54.440 --> 00:52:55.760]   - Thanks, Sworny. - Right.
[00:52:55.760 --> 00:52:56.920]   Can you scroll down a little bit?
[00:52:56.920 --> 00:52:58.720]   - Yeah. - 'Cause I'm actually,
[00:52:58.720 --> 00:52:59.560]   there she is.
[00:52:59.560 --> 00:53:01.760]   No, okay, it's not on your computer.
[00:53:01.760 --> 00:53:02.600]   Oh, there it is.
[00:53:02.600 --> 00:53:03.520]   Do you see CEO Barbie?
[00:53:03.520 --> 00:53:05.080]   - CEO Barbie, right in there.
[00:53:05.080 --> 00:53:07.520]   - But for a long time, that was the first woman
[00:53:07.520 --> 00:53:09.280]   that showed, the first female that showed up.
[00:53:09.280 --> 00:53:11.320]   - So, who has fixed that?
[00:53:11.320 --> 00:53:12.480]   - She's wearing a micromanic shirt.
[00:53:12.480 --> 00:53:14.080]   - I think Google fixed it this morning.
[00:53:14.080 --> 00:53:16.160]   But where, so the interesting question is,
[00:53:16.160 --> 00:53:17.040]   why is that?
[00:53:17.040 --> 00:53:18.040]   Is this algorithmic?
[00:53:18.040 --> 00:53:19.040]   This is not a human.
[00:53:19.040 --> 00:53:20.800]   - This is unconscious bias
[00:53:20.800 --> 00:53:22.840]   because in our culture in America,
[00:53:22.840 --> 00:53:25.560]   when we say CEO,
[00:53:25.560 --> 00:53:29.000]   women and men left leaning and right leaning,
[00:53:29.000 --> 00:53:32.280]   are trained to think of old white dude.
[00:53:32.280 --> 00:53:33.120]   - CEO Barbie.
[00:53:33.120 --> 00:53:36.080]   - So, yeah.
[00:53:36.080 --> 00:53:38.080]   - And by the way, that's the response
[00:53:38.080 --> 00:53:38.920]   that a lot of people would say,
[00:53:38.920 --> 00:53:40.120]   "Well, no, no, this is what we'll do.
[00:53:40.120 --> 00:53:42.320]   "We'll make a CEO Barbie in a mini skirt."
[00:53:42.320 --> 00:53:43.160]   That'll fix it.
[00:53:43.160 --> 00:53:45.200]   - That'll fix everything. (laughs)
[00:53:45.200 --> 00:53:47.480]   - My wife is the CEO of this company
[00:53:47.480 --> 00:53:49.760]   and is a CEO in writ large.
[00:53:49.760 --> 00:53:52.320]   And I know the only CEO I really know is a woman.
[00:53:52.320 --> 00:53:53.160]   So.
[00:53:54.440 --> 00:53:57.320]   Okay, so again, like, and over, and to be fair,
[00:53:57.320 --> 00:54:00.560]   overwhelmingly, women are not CEOs of companies.
[00:54:00.560 --> 00:54:01.400]   - Yeah.
[00:54:01.400 --> 00:54:05.120]   - You know, again, like as a statistical sample size,
[00:54:05.120 --> 00:54:06.480]   we are smaller.
[00:54:06.480 --> 00:54:10.200]   So, my point is there is unconscious bias.
[00:54:10.200 --> 00:54:12.920]   There is a diversity problem at Google.
[00:54:12.920 --> 00:54:14.640]   And the person who wrote this memo
[00:54:14.640 --> 00:54:16.560]   was obviously very, very young
[00:54:16.560 --> 00:54:20.040]   and feeling disparaged at work.
[00:54:20.040 --> 00:54:20.880]   - Right.
[00:54:20.880 --> 00:54:22.200]   - And, you know,
[00:54:22.200 --> 00:54:25.520]   but again, the conversation that we ought to be having,
[00:54:25.520 --> 00:54:28.040]   I think is a different one.
[00:54:28.040 --> 00:54:31.280]   You know, and we should acknowledge
[00:54:31.280 --> 00:54:33.440]   that we're all pretty different.
[00:54:33.440 --> 00:54:36.520]   But even within our groups were really different.
[00:54:36.520 --> 00:54:38.640]   And if you want to spin this out, you know,
[00:54:38.640 --> 00:54:40.560]   I mentioned AI earlier,
[00:54:40.560 --> 00:54:43.040]   the thing that keeps me up that I worry about
[00:54:43.040 --> 00:54:46.440]   is that humans, you know, this,
[00:54:46.440 --> 00:54:48.400]   pretty soon we will not have the luxury
[00:54:48.400 --> 00:54:50.560]   of discriminating against each other
[00:54:50.560 --> 00:54:53.480]   in these little taxonomies
[00:54:53.480 --> 00:54:56.280]   that we understand like gender and race.
[00:54:56.280 --> 00:54:57.840]   I think within the next 30 years,
[00:54:57.840 --> 00:55:00.800]   we're gonna have algorithms that are sectioning us off
[00:55:00.800 --> 00:55:02.600]   in a more Huxley in way.
[00:55:02.600 --> 00:55:05.080]   And, you know, we're gonna find ourselves in tribes
[00:55:05.080 --> 00:55:07.320]   that we don't even understand.
[00:55:07.320 --> 00:55:10.720]   And being lumped together by machines on the back end
[00:55:10.720 --> 00:55:12.640]   and that will have real world consequences.
[00:55:12.640 --> 00:55:15.160]   But that's all being built by people like the dude
[00:55:15.160 --> 00:55:16.200]   who wrote that memo.
[00:55:16.200 --> 00:55:17.200]   - Yeah.
[00:55:17.200 --> 00:55:19.360]   - The foundations of it, so.
[00:55:19.360 --> 00:55:21.000]   That's what Kathy O'Neill would also say,
[00:55:21.000 --> 00:55:23.440]   weapons of math destruction we talked about here last week,
[00:55:23.440 --> 00:55:24.640]   which has been on triangulation,
[00:55:24.640 --> 00:55:26.680]   is that there's this unconscious bias
[00:55:26.680 --> 00:55:28.840]   being written into these algorithms now.
[00:55:28.840 --> 00:55:33.680]   A lot of it from in numeracy, from math illiteracy.
[00:55:33.680 --> 00:55:38.480]   And a lot of it from a lack of diversity and ignorance.
[00:55:38.480 --> 00:55:39.320]   So.
[00:55:39.320 --> 00:55:41.840]   - We'll say the one thing that,
[00:55:41.840 --> 00:55:43.560]   where I read this memo and I thought,
[00:55:43.560 --> 00:55:47.400]   actually, it's kind of like a stopped clock is twice.
[00:55:47.400 --> 00:55:49.760]   - Yeah, it's right twice. - It's right twice a day.
[00:55:49.760 --> 00:55:53.160]   He was just saying that the fact that I'm writing this
[00:55:53.160 --> 00:55:55.080]   is going to get decried by some people.
[00:55:55.080 --> 00:55:57.520]   And no, he's got a perfect right to write that
[00:55:57.520 --> 00:56:00.120]   and to get all these things out in print.
[00:56:00.120 --> 00:56:04.160]   What he doesn't have is a perfect right to be taken seriously
[00:56:04.160 --> 00:56:08.080]   and not criticized for inaccuracies
[00:56:08.080 --> 00:56:10.400]   or for failings that one has written about.
[00:56:10.400 --> 00:56:12.960]   There is a massive diversity problem in Silicon Valley.
[00:56:12.960 --> 00:56:14.880]   There is a much bigger racial diversity problem
[00:56:14.880 --> 00:56:17.600]   instead of the Silicon Valley than there is a sexism problem.
[00:56:17.600 --> 00:56:19.920]   And that's not to minimize the sexism side of it,
[00:56:19.920 --> 00:56:21.800]   but we've got to get this sorted out.
[00:56:21.800 --> 00:56:25.000]   Both as an industry and also as a society.
[00:56:25.000 --> 00:56:25.840]   Because you're right, Amy,
[00:56:25.840 --> 00:56:28.440]   the most diverse ecosystems are the ones
[00:56:28.440 --> 00:56:29.560]   which flourish the most.
[00:56:29.560 --> 00:56:32.520]   And we've really got to work toward attending those.
[00:56:32.520 --> 00:56:35.560]   - Well, and your points well taken is that
[00:56:35.560 --> 00:56:39.120]   we're about to enter an era where the code written
[00:56:39.120 --> 00:56:41.760]   by these people is going to impact our lives
[00:56:41.760 --> 00:56:43.680]   in every possible way.
[00:56:43.680 --> 00:56:44.520]   - And you don't--
[00:56:44.520 --> 00:56:46.480]   - Right, and to segment us into,
[00:56:46.480 --> 00:56:49.000]   I mean, that's, I think honestly,
[00:56:49.000 --> 00:56:54.000]   I think my child who's young will look back 30 years from now
[00:56:54.000 --> 00:56:57.200]   on this, the Halcyon days when,
[00:56:57.200 --> 00:57:02.520]   how quaint it was that we were writing memos
[00:57:02.520 --> 00:57:05.800]   about gender discrimination in our workplaces.
[00:57:05.800 --> 00:57:10.800]   We're gonna wind up truly in a sort of brave new world sense
[00:57:12.400 --> 00:57:15.000]   being segregated into different type of tribes
[00:57:15.000 --> 00:57:18.240]   for financial reasons and mental abilities
[00:57:18.240 --> 00:57:21.120]   and all different kinds of ways
[00:57:21.120 --> 00:57:23.360]   that we've never really thought through before.
[00:57:23.360 --> 00:57:26.680]   And we're assigning that decision making ability
[00:57:26.680 --> 00:57:29.920]   to algorithms that are being written
[00:57:29.920 --> 00:57:32.800]   and tested by people who don't share
[00:57:32.800 --> 00:57:34.360]   the same worldviews as we do.
[00:57:34.360 --> 00:57:39.360]   And that to me is a bigger problem.
[00:57:39.360 --> 00:57:41.680]   But I do think it's interesting that
[00:57:41.680 --> 00:57:44.080]   if we cycle back, we really have started
[00:57:44.080 --> 00:57:45.960]   to hit a fever pitch, I think,
[00:57:45.960 --> 00:57:50.320]   with regards to gender and race conversations
[00:57:50.320 --> 00:57:53.120]   and then people feeling very much like
[00:57:53.120 --> 00:57:54.920]   the conversation has gotten too far,
[00:57:54.920 --> 00:57:57.760]   which sort of reminds me of where we were in the late 90s.
[00:57:57.760 --> 00:58:00.720]   And I don't know why at this particular moment in time,
[00:58:00.720 --> 00:58:01.560]   it's happened again,
[00:58:01.560 --> 00:58:02.400]   that it's cycled.
[00:58:02.400 --> 00:58:04.240]   - I know exactly why.
[00:58:04.240 --> 00:58:05.240]   (laughs)
[00:58:05.240 --> 00:58:06.480]   - The economy?
[00:58:06.480 --> 00:58:08.240]   - Yeah, it's the economy, of course, that's what it is.
[00:58:08.240 --> 00:58:09.320]   - Yeah.
[00:58:09.320 --> 00:58:10.840]   - All right, we'll talk about your daughter.
[00:58:10.840 --> 00:58:12.360]   She's in the eye gen, isn't she?
[00:58:12.360 --> 00:58:14.320]   I don't want to make this a sociology show.
[00:58:14.320 --> 00:58:15.960]   We'll find some other stuff to talk about too,
[00:58:15.960 --> 00:58:18.360]   but at some point we have to talk about
[00:58:18.360 --> 00:58:21.000]   this amazing article by Jean M. Twengie.
[00:58:21.000 --> 00:58:22.920]   It's this excerpt from her upcoming book
[00:58:22.920 --> 00:58:27.320]   about the eye gen, the generation that have grown up
[00:58:27.320 --> 00:58:28.400]   with iPhones.
[00:58:28.400 --> 00:58:30.200]   - I wonder if she trademarked that before.
[00:58:30.200 --> 00:58:33.640]   - Not only did she, she has the website EyeGen Consulting.
[00:58:33.640 --> 00:58:37.640]   - It's fingernails on the blackboard to me EyeGen.
[00:58:37.640 --> 00:58:39.920]   Like EyeGen brought to you by Apple.
[00:58:39.920 --> 00:58:42.280]   - Well, I tweeted this article in a number of people said,
[00:58:42.280 --> 00:58:45.480]   oh come on, do we have to label every freakin' generation?
[00:58:45.480 --> 00:58:48.280]   Baby Boomer, Gen X, Gen 1.
[00:58:48.280 --> 00:58:50.280]   - We're running out of, we're running out of letters
[00:58:50.280 --> 00:58:51.120]   to be fair.
[00:58:51.120 --> 00:58:54.280]   Like after Gen Z, you either go to an entirely new--
[00:58:54.280 --> 00:58:56.200]   - EyeGen is a fair coin.
[00:58:56.200 --> 00:58:58.440]   - EyeGen isn't bad, but it's so deliberate.
[00:58:58.440 --> 00:58:59.960]   - It started earlier in the alphabet.
[00:58:59.960 --> 00:59:02.160]   - The winner is, should have been Generation L.
[00:59:02.160 --> 00:59:03.000]   - We're going back to this.
[00:59:03.000 --> 00:59:04.120]   - We're gonna have to think for really long time.
[00:59:04.120 --> 00:59:06.880]   - Her book, which this is Atlantic articles
[00:59:06.880 --> 00:59:10.880]   that Exer From is coming out later this month is called.
[00:59:10.880 --> 00:59:12.160]   - And this will tell you everything.
[00:59:12.160 --> 00:59:14.680]   EyeGen, why today's super connected kids
[00:59:14.680 --> 00:59:17.040]   are growing up less rebellious.
[00:59:17.040 --> 00:59:21.880]   By the way, she's a PhD, she studies generational differences.
[00:59:21.880 --> 00:59:25.800]   This is her bailiwick, less rebellious, more tolerant,
[00:59:25.800 --> 00:59:29.600]   less happy, and completely unprepared for adulthood.
[00:59:29.600 --> 00:59:30.760]   And what this means for the rest.
[00:59:30.760 --> 00:59:32.320]   - Yeah, because there's never been anybody who's got poor
[00:59:32.320 --> 00:59:33.920]   by saying, God, the next generation,
[00:59:33.920 --> 00:59:34.760]   don't know what they're on.
[00:59:34.760 --> 00:59:35.840]   (laughing)
[00:59:35.840 --> 00:59:38.920]   - You know, it turned out that rock and roll, you kids.
[00:59:38.920 --> 00:59:40.280]   And cut your hair, Leo.
[00:59:40.280 --> 00:59:41.120]   - Oh wait, that's right.
[00:59:41.120 --> 00:59:42.280]   - It was the same when computers came in
[00:59:42.280 --> 00:59:43.840]   when the personal computer came in
[00:59:43.840 --> 00:59:45.920]   in the sort of the late '70s, early '80s,
[00:59:45.920 --> 00:59:48.080]   it's like, oh, these kids are just hammering away
[00:59:48.080 --> 00:59:50.480]   at a computer, and it's like, it'll be the death of them.
[00:59:50.480 --> 00:59:51.960]   And now we've got an entire industry built
[00:59:51.960 --> 00:59:53.160]   in Silicon Valley.
[00:59:53.160 --> 00:59:55.120]   - I do have to say, I didn't want to get
[00:59:55.120 --> 00:59:56.480]   in this conversation, we're gonna take a break,
[00:59:56.480 --> 00:59:59.920]   but I do have to say, there is something going on
[00:59:59.920 --> 01:00:03.480]   because we have gotten very good in designing software
[01:00:03.480 --> 01:00:05.760]   that addicts you, right?
[01:00:05.760 --> 01:00:07.280]   - Oh yeah, very good.
[01:00:07.280 --> 01:00:08.120]   - Very deliberately.
[01:00:08.120 --> 01:00:09.840]   - Deliberately, gamifying.
[01:00:09.840 --> 01:00:11.480]   - Yeah, and it's dangerous.
[01:00:11.480 --> 01:00:13.320]   - Understands the way we respond to it in the brain.
[01:00:13.320 --> 01:00:14.320]   - If you're not.
[01:00:14.320 --> 01:00:15.360]   - That's what they're doing at Blizzard
[01:00:15.360 --> 01:00:17.440]   with World of Warcraft, they have all the data.
[01:00:17.440 --> 01:00:18.280]   This is what's changed.
[01:00:18.280 --> 01:00:20.400]   We have all the data, we know exactly how much time
[01:00:20.400 --> 01:00:22.680]   people spend in each raid, what raids they like,
[01:00:22.680 --> 01:00:25.480]   what raids they don't like, and we can maximize the loot
[01:00:25.480 --> 01:00:27.480]   to make sure they spend more time online.
[01:00:27.480 --> 01:00:30.160]   And as a result, there are people who have died
[01:00:30.160 --> 01:00:31.840]   from playing World of Warcraft.
[01:00:31.840 --> 01:00:33.760]   There are people who have lost their families
[01:00:33.760 --> 01:00:34.760]   from playing World of Warcraft.
[01:00:34.760 --> 01:00:35.960]   - They die happily.
[01:00:35.960 --> 01:00:37.840]   - They died with a full bladder.
[01:00:37.840 --> 01:00:38.680]   That's all I can say.
[01:00:38.680 --> 01:00:39.520]   - It's true.
[01:00:39.520 --> 01:00:40.360]   - They died in battle.
[01:00:40.360 --> 01:00:41.200]   - That's what they died of.
[01:00:41.200 --> 01:00:42.040]   - And glory.
[01:00:42.040 --> 01:00:42.880]   (laughing)
[01:00:42.880 --> 01:00:46.720]   - But I mean, I know, I'm playing these stupid games.
[01:00:46.720 --> 01:00:49.240]   Yes, it's time for me to quickly play a little round
[01:00:49.240 --> 01:00:50.880]   of field runners.
[01:00:50.880 --> 01:00:53.240]   I play these games, they're very addictive.
[01:00:53.240 --> 01:00:56.440]   And I think that there is something to be said
[01:00:56.440 --> 01:00:58.400]   for the fact that we have a generation
[01:00:58.400 --> 01:01:01.040]   who are at the mercy of this stuff,
[01:01:01.040 --> 01:01:02.640]   who sleep with their phones.
[01:01:02.640 --> 01:01:04.200]   You just walk downtown,
[01:01:04.200 --> 01:01:05.320]   nobody's looking at anybody.
[01:01:05.320 --> 01:01:07.160]   They're all looking at their phones.
[01:01:07.160 --> 01:01:08.560]   And I'm just as guilty as anybody.
[01:01:08.560 --> 01:01:09.880]   But the difference is,
[01:01:09.880 --> 01:01:12.400]   my brain was fully developed before this happened.
[01:01:12.400 --> 01:01:14.040]   What about the generation that's grown up
[01:01:14.040 --> 01:01:17.200]   in the last 10 years with the iPhone and Android phones?
[01:01:17.200 --> 01:01:19.960]   That those young malleable brains,
[01:01:19.960 --> 01:01:20.960]   what about them?
[01:01:20.960 --> 01:01:23.000]   So I think that that's a legitimate cause for concern.
[01:01:23.000 --> 01:01:26.480]   I'm not sure we need to name them the eye gen or fear
[01:01:26.480 --> 01:01:27.760]   what's gonna happen.
[01:01:27.760 --> 01:01:28.880]   - But when you start an article,
[01:01:28.880 --> 01:01:30.680]   I mean, I know that the journalists do the thing.
[01:01:30.680 --> 01:01:31.520]   - It's the link bait.
[01:01:31.520 --> 01:01:33.000]   - They're not the ones who write the titles,
[01:01:33.000 --> 01:01:36.760]   but nonetheless have smartphones destroyed a generation.
[01:01:36.760 --> 01:01:37.920]   - Well,
[01:01:37.920 --> 01:01:38.760]   - Say do click.
[01:01:38.760 --> 01:01:39.840]   - No, yeah.
[01:01:39.840 --> 01:01:41.680]   - Rock and roll destroyed my generation.
[01:01:41.680 --> 01:01:42.520]   - Rected.
[01:01:42.520 --> 01:01:43.360]   - Well, I mean,
[01:01:43.360 --> 01:01:44.200]   - Rected.
[01:01:44.200 --> 01:01:45.360]   - I mean, rock and roll was supposed to bring down
[01:01:45.360 --> 01:01:47.680]   the end of Sato with my generation of his punk.
[01:01:47.680 --> 01:01:48.520]   You know, it's like,
[01:01:48.520 --> 01:01:50.000]   - This is different than music.
[01:01:50.000 --> 01:01:53.440]   - This is very addictive stuff.
[01:01:53.440 --> 01:01:55.640]   - This is different even than the games we played
[01:01:55.640 --> 01:01:56.640]   when 20 years ago.
[01:01:56.640 --> 01:01:59.000]   - True, but it is so much less addictive
[01:01:59.000 --> 01:02:00.560]   than the pretzels filled with peanut butter.
[01:02:00.560 --> 01:02:01.400]   - Those are good.
[01:02:01.400 --> 01:02:02.240]   - The pantry.
[01:02:02.240 --> 01:02:04.040]   (laughing)
[01:02:04.040 --> 01:02:04.920]   - I mean,
[01:02:04.920 --> 01:02:05.760]   - You should try them,
[01:02:05.760 --> 01:02:06.600]   maybe they're fine.
[01:02:06.600 --> 01:02:07.440]   - But everyone says,
[01:02:07.440 --> 01:02:08.280]   oh, no one's looking up and around.
[01:02:08.280 --> 01:02:09.120]   If you look at a photo,
[01:02:09.120 --> 01:02:12.480]   there was a marvelous photo from the British commuting train
[01:02:12.480 --> 01:02:15.400]   and going into London in the 1930s and everybody.
[01:02:15.400 --> 01:02:16.240]   - Reading a paper.
[01:02:16.240 --> 01:02:17.080]   - Has a newspaper.
[01:02:17.080 --> 01:02:17.920]   - Yeah, sure.
[01:02:17.920 --> 01:02:18.760]   - No one's convinced nobody was.
[01:02:18.760 --> 01:02:19.920]   - And it equally walled off.
[01:02:19.920 --> 01:02:20.760]   - Yeah.
[01:02:20.760 --> 01:02:21.600]   - Yeah.
[01:02:21.600 --> 01:02:22.440]   - There are actually some physiological changes
[01:02:22.440 --> 01:02:24.440]   that are happening that will affect us in the future.
[01:02:24.440 --> 01:02:26.480]   My husband's an eye doctor.
[01:02:26.480 --> 01:02:29.360]   And so he's noticed over the past couple of years,
[01:02:29.360 --> 01:02:31.640]   more and more people having all kinds
[01:02:31.640 --> 01:02:32.800]   of eye strain and issues.
[01:02:32.800 --> 01:02:34.960]   And that's because humans were really not designed
[01:02:34.960 --> 01:02:38.400]   to see near for longer periods of time,
[01:02:38.400 --> 01:02:40.920]   we were designed to see far.
[01:02:40.920 --> 01:02:43.960]   And our bodies are not evolving as quickly
[01:02:43.960 --> 01:02:47.920]   as the technology that we're using with our bodies.
[01:02:47.920 --> 01:02:49.880]   And so there are people having all kinds
[01:02:49.880 --> 01:02:52.720]   of eye related problems and strain and,
[01:02:52.720 --> 01:02:54.920]   you know,
[01:02:54.920 --> 01:02:55.840]   - Right, this is also a big issue.
[01:02:55.840 --> 01:02:56.840]   I mean, I was talking to a doctor.
[01:02:56.840 --> 01:02:57.840]   - That's problems.
[01:02:57.840 --> 01:03:00.560]   - And they were saying that he's seeing people
[01:03:00.560 --> 01:03:03.800]   with hand arthritis problems in their 30s
[01:03:03.800 --> 01:03:06.640]   that he would expect to see in their 50s and 60s
[01:03:06.640 --> 01:03:08.600]   because they've spent their entire life
[01:03:08.600 --> 01:03:10.160]   go texting on phones,
[01:03:10.160 --> 01:03:12.560]   thumb problems in particular are becoming
[01:03:12.560 --> 01:03:14.840]   a major arthritic issue at the moment.
[01:03:14.840 --> 01:03:15.680]   Because we've just got--
[01:03:15.680 --> 01:03:18.320]   - Right, and it all seems silly and funny
[01:03:18.320 --> 01:03:19.880]   and we all sort of snicker about it,
[01:03:19.880 --> 01:03:23.600]   except that there's data to prove out
[01:03:23.600 --> 01:03:25.000]   everything that we're talking about.
[01:03:25.000 --> 01:03:26.920]   And at some point, you know,
[01:03:26.920 --> 01:03:29.120]   when you have spent all this time staring down
[01:03:29.120 --> 01:03:30.320]   and you've now got these headaches
[01:03:30.320 --> 01:03:31.760]   and you don't go away and you've got, you know,
[01:03:31.760 --> 01:03:33.680]   your shoulders hurt,
[01:03:33.680 --> 01:03:36.360]   which is not, you know, it's not life threatening,
[01:03:36.360 --> 01:03:38.320]   but it becomes, you know,
[01:03:38.320 --> 01:03:41.760]   you realize that you are causing your body to change.
[01:03:41.760 --> 01:03:44.280]   - Yeah, by the line we were designed to procreate
[01:03:44.280 --> 01:03:45.920]   and die at 30 though.
[01:03:45.920 --> 01:03:48.320]   So we're gonna run out the hardware.
[01:03:48.320 --> 01:03:51.760]   One way or another, like everything is, you know,
[01:03:51.760 --> 01:03:52.600]   man, I don't wanna,
[01:03:52.600 --> 01:03:55.440]   don't get me started about my ankles.
[01:03:55.440 --> 01:03:56.320]   My ankles were like--
[01:03:56.320 --> 01:03:58.280]   - I thought I was supposed to be done in the third.
[01:03:58.280 --> 01:03:59.920]   - What are you doing? - What's walking around
[01:03:59.920 --> 01:04:02.120]   at this advanced stage?
[01:04:02.120 --> 01:04:03.840]   Yeah, crazy.
[01:04:03.840 --> 01:04:04.680]   All right, we're gonna take a break.
[01:04:04.680 --> 01:04:05.560]   We'll come back with more.
[01:04:05.560 --> 01:04:07.120]   And yes, we'll have some more tech,
[01:04:07.120 --> 01:04:10.160]   including Vic Condotra throwing some shade
[01:04:10.160 --> 01:04:12.560]   at his former employer.
[01:04:12.560 --> 01:04:13.880]   - A little bit of shade.
[01:04:13.880 --> 01:04:15.520]   - A little bit of shade.
[01:04:15.520 --> 01:04:18.240]   You know, I can now use the language of the kids today.
[01:04:18.240 --> 01:04:21.520]   There's Facebook story.
[01:04:21.520 --> 01:04:22.440]   I think Mark Zuckerberg,
[01:04:22.440 --> 01:04:24.800]   I think we have conclusive evidence
[01:04:24.800 --> 01:04:26.440]   that Mark Zuckerberg is running for president.
[01:04:26.440 --> 01:04:27.280]   We'll tell you what that is.
[01:04:27.280 --> 01:04:29.040]   I think he's gonna get beaten like a Reddit
[01:04:29.040 --> 01:04:30.840]   to step child if he does, but yeah.
[01:04:30.840 --> 01:04:31.680]   - I don't care.
[01:04:31.680 --> 01:04:34.080]   I think it's gonna be fascinating.
[01:04:34.080 --> 01:04:35.760]   And I don't know.
[01:04:35.760 --> 01:04:37.240]   We could talk about the note eight.
[01:04:37.240 --> 01:04:40.000]   We could talk about Travis Kalanek attempting
[01:04:40.000 --> 01:04:42.920]   to get back in the driver's seat at Uber.
[01:04:42.920 --> 01:04:43.760]   - Come back kid.
[01:04:43.760 --> 01:04:44.720]   - The comeback kid.
[01:04:44.720 --> 01:04:45.720]   - Come back kid.
[01:04:45.720 --> 01:04:47.040]   - The Game of Thrones hack?
[01:04:47.040 --> 01:04:48.160]   They called it the Game of Thrones hack.
[01:04:48.160 --> 01:04:49.560]   Really it was a hack of HBO.
[01:04:49.560 --> 01:04:50.400]   - Yeah.
[01:04:50.400 --> 01:04:53.040]   - And a lot worse stuff than Game of Thrones.
[01:04:53.040 --> 01:04:55.160]   - Although apparently not their email system, but yeah.
[01:04:55.160 --> 01:04:56.000]   - Yeah.
[01:04:56.000 --> 01:04:59.560]   - It was about 1% of the data that North Korea got out of Sony.
[01:04:59.560 --> 01:05:01.800]   So you're putting in things in perspective.
[01:05:01.800 --> 01:05:02.640]   - Yeah.
[01:05:02.640 --> 01:05:04.640]   - Serious stuff.
[01:05:04.640 --> 01:05:05.480]   So I'll come in.
[01:05:05.480 --> 01:05:06.640]   - Clearly none of you are watching Game of Thrones.
[01:05:06.640 --> 01:05:07.480]   - Yes.
[01:05:07.480 --> 01:05:08.320]   - I am.
[01:05:08.320 --> 01:05:10.600]   I'm watching it religiously.
[01:05:10.600 --> 01:05:12.040]   - I'm a recent converse.
[01:05:12.040 --> 01:05:13.560]   - And no spoilers.
[01:05:13.560 --> 01:05:16.720]   I'm not gonna watch the hacked pirate version of it.
[01:05:16.720 --> 01:05:19.120]   I wanna watch it every Sunday night.
[01:05:19.120 --> 01:05:21.440]   Like you're supposed to as you're supposed to.
[01:05:21.440 --> 01:05:22.680]   - I was a very late.
[01:05:22.680 --> 01:05:23.680]   - God intended.
[01:05:23.680 --> 01:05:26.560]   I was a late convert. I was considered Game of Thrones
[01:05:26.560 --> 01:05:28.200]   to be basically dragons with nudity.
[01:05:28.200 --> 01:05:29.040]   But it's.
[01:05:29.040 --> 01:05:29.880]   - It's exactly right.
[01:05:29.880 --> 01:05:31.960]   And what did you find out it actually is?
[01:05:31.960 --> 01:05:36.000]   - Well actually Peter Dinkley's really, really world
[01:05:36.000 --> 01:05:36.840]   - He's good at this show.
[01:05:36.840 --> 01:05:38.200]   - And I think he's the hero.
[01:05:38.200 --> 01:05:39.040]   I forget. - Really.
[01:05:39.040 --> 01:05:39.880]   - Of all of it.
[01:05:39.880 --> 01:05:42.280]   - And Charles Dancers as his father was just.
[01:05:42.280 --> 01:05:45.040]   - Well, super not to spoil anything,
[01:05:45.040 --> 01:05:47.080]   but Charles Dances came to a bad end.
[01:05:47.080 --> 01:05:48.120]   - I'm saying nothing.
[01:05:48.120 --> 01:05:48.960]   No spoilers.
[01:05:48.960 --> 01:05:51.120]   - I'm not spoiling anything at all.
[01:05:51.120 --> 01:05:53.360]   (laughing)
[01:05:53.360 --> 01:05:55.240]   - Um, okay.
[01:05:55.240 --> 01:05:58.040]   I know my wife used to, I told this story before,
[01:05:58.040 --> 01:06:00.320]   but my wife never got into Game of Thrones
[01:06:00.320 --> 01:06:01.880]   and I would sit and watch it religiously.
[01:06:01.880 --> 01:06:04.520]   And she'd always come in at the worst possible time.
[01:06:04.520 --> 01:06:06.120]   (laughing)
[01:06:06.120 --> 01:06:08.080]   - I was trying to get my wife into Breaking Bad
[01:06:08.080 --> 01:06:09.120]   and she, the same thing.
[01:06:09.120 --> 01:06:10.520]   She would come in and he sat in the moment.
[01:06:10.520 --> 01:06:12.240]   - And she'd say this is soft core porn you're watching.
[01:06:12.240 --> 01:06:14.360]   This is not, I'm not gonna watch that.
[01:06:14.360 --> 01:06:15.800]   - I could never get Breaking Bad
[01:06:15.800 --> 01:06:17.800]   'cause it was the whole premise of it.
[01:06:17.800 --> 01:06:18.760]   When you come from somewhere
[01:06:18.760 --> 01:06:20.440]   with a national healthcare system,
[01:06:20.440 --> 01:06:22.040]   you're just like, well why doesn't he just,
[01:06:22.040 --> 01:06:23.920]   "Oh, oh, this is America."
[01:06:23.920 --> 01:06:25.320]   - Right, that's a very good point.
[01:06:25.320 --> 01:06:26.680]   It's actually a very good point, right?
[01:06:26.680 --> 01:06:27.520]   - Yeah.
[01:06:27.520 --> 01:06:28.680]   - It's only America.
[01:06:28.680 --> 01:06:29.720]   - For those who haven't watched it,
[01:06:29.720 --> 01:06:32.000]   the premise is that this guy's a high school science teacher
[01:06:32.000 --> 01:06:34.600]   gets cancer and in order to pay for his cancer treatments,
[01:06:34.600 --> 01:06:37.280]   turns his skills to the dark side.
[01:06:37.280 --> 01:06:38.720]   - And makes crystal meth.
[01:06:38.720 --> 01:06:40.040]   - No spoiler, that's a good one.
[01:06:40.040 --> 01:06:40.880]   - But really good, Christopher.
[01:06:40.880 --> 01:06:42.280]   - Really good, Chris Blue.
[01:06:42.280 --> 01:06:43.120]   - Well branded.
[01:06:43.120 --> 01:06:44.200]   - Really good, Chris.
[01:06:44.200 --> 01:06:45.040]   - Right.
[01:06:45.040 --> 01:06:46.400]   - Really good and crystal meth.
[01:06:46.400 --> 01:06:48.280]   Those phrases don't go together, but no.
[01:06:48.280 --> 01:06:50.800]   - So we're told, I know nothing.
[01:06:50.800 --> 01:06:51.640]   - Yes.
[01:06:51.640 --> 01:06:53.880]   (laughs)
[01:06:53.880 --> 01:06:56.680]   - Or should they brought to you by rocket mortgage, FV?
[01:06:56.680 --> 01:06:58.560]   You know, I have to say,
[01:06:58.560 --> 01:07:00.440]   if you're getting a home loan these days,
[01:07:00.440 --> 01:07:03.600]   you might as well do it the 21st century way.
[01:07:03.600 --> 01:07:04.720]   Quick and loan is one of the biggest
[01:07:04.720 --> 01:07:06.240]   and best lenders in the country.
[01:07:06.240 --> 01:07:10.040]   Notice that it was frankly an industry stuck
[01:07:10.040 --> 01:07:11.640]   in the 19th century.
[01:07:11.640 --> 01:07:13.280]   You know, you practically had quill pens
[01:07:13.280 --> 01:07:15.200]   and little sleeve garters and, you know,
[01:07:15.200 --> 01:07:17.560]   those green eye shades when you go in to get the loan.
[01:07:17.560 --> 01:07:20.200]   Well, all right, let's bring your paperwork with you
[01:07:20.200 --> 01:07:23.240]   and nowadays we got computers.
[01:07:23.240 --> 01:07:24.080]   We've got a better way.
[01:07:24.080 --> 01:07:26.000]   And that's what rocket mortgage is designed
[01:07:26.000 --> 01:07:28.520]   by quick and loans for us, the Geeks.
[01:07:28.520 --> 01:07:31.680]   It's a totally online loan approval process.
[01:07:31.680 --> 01:07:33.560]   And what do you get because it's totally online?
[01:07:33.560 --> 01:07:35.000]   You get transparency.
[01:07:35.000 --> 01:07:36.640]   So you know exactly what's happening,
[01:07:36.640 --> 01:07:38.080]   the power is in your hands.
[01:07:38.080 --> 01:07:40.680]   You get the confidence that what you're doing
[01:07:40.680 --> 01:07:43.320]   is exactly what you need from a great lender.
[01:07:43.320 --> 01:07:46.480]   And most importantly, it's fast.
[01:07:46.480 --> 01:07:48.200]   It's all online.
[01:07:48.200 --> 01:07:51.040]   I want you to go to rocketmortgage.com/twit2
[01:07:51.040 --> 01:07:52.160]   when you need a home loan.
[01:07:52.160 --> 01:07:54.760]   You could even be at an open house.
[01:07:54.760 --> 01:07:55.920]   Now I have to say the last home loan
[01:07:55.920 --> 01:07:59.160]   we got three years ago when Lisa and I bought our house
[01:07:59.160 --> 01:08:00.320]   took us four months.
[01:08:00.320 --> 01:08:01.920]   We were faxing.
[01:08:01.920 --> 01:08:03.160]   First I had to find a fax machine.
[01:08:03.160 --> 01:08:05.120]   I would go down the mail depot down here.
[01:08:05.120 --> 01:08:06.520]   They have a fax machine.
[01:08:06.520 --> 01:08:08.360]   It's like this dusty old fax machine
[01:08:08.360 --> 01:08:09.440]   and a 10 cents a page.
[01:08:09.440 --> 01:08:11.440]   I was feeding the pages of the document
[01:08:11.440 --> 01:08:13.800]   'cause they wanted me to fax it to the bank.
[01:08:13.800 --> 01:08:16.760]   And that went on for weeks, literally more than a month.
[01:08:16.760 --> 01:08:19.160]   I think Lisa said it was a couple of months.
[01:08:19.160 --> 01:08:21.040]   So much so that we almost lost the house.
[01:08:21.040 --> 01:08:22.760]   The seller was saying, come on guys,
[01:08:22.760 --> 01:08:25.480]   you gotta get, tell me what's going on.
[01:08:25.480 --> 01:08:26.760]   If only they had rocket mortgage.
[01:08:26.760 --> 01:08:28.360]   You could do this at an open house.
[01:08:28.360 --> 01:08:29.720]   You say, I like this house.
[01:08:29.720 --> 01:08:30.520]   Let's buy it.
[01:08:30.520 --> 01:08:31.360]   You pull out your phone.
[01:08:31.360 --> 01:08:34.040]   You go to rocketmortgage.com/twit2.
[01:08:34.040 --> 01:08:35.160]   Everything you need to do,
[01:08:35.160 --> 01:08:36.560]   you can do with the touch of a finger
[01:08:36.560 --> 01:08:38.920]   including submit pay stubs and bank statements
[01:08:38.920 --> 01:08:40.760]   'cause it got trusted partnerships
[01:08:40.760 --> 01:08:42.920]   with all the financial institutions.
[01:08:42.920 --> 01:08:44.360]   It's all online.
[01:08:44.360 --> 01:08:45.640]   They can crunch the numbers.
[01:08:45.640 --> 01:08:46.640]   I mean, well it's computers.
[01:08:46.640 --> 01:08:47.880]   It takes some seconds to crunch.
[01:08:47.880 --> 01:08:49.240]   It takes no time to crunch the numbers.
[01:08:49.240 --> 01:08:50.840]   There's no faxing involved.
[01:08:50.840 --> 01:08:53.480]   And they will present you with all the loans
[01:08:53.480 --> 01:08:55.200]   that you qualify for based on your income,
[01:08:55.200 --> 01:08:56.640]   your assets and your credit.
[01:08:56.640 --> 01:09:00.160]   You pick the term, you pick the interest rate.
[01:09:00.160 --> 01:09:02.680]   You know exactly what's going on.
[01:09:02.680 --> 01:09:04.800]   It's just awesome.
[01:09:04.800 --> 01:09:07.680]   You're applying fast and easy.
[01:09:07.680 --> 01:09:09.120]   You're doing it confidently.
[01:09:09.120 --> 01:09:10.640]   You understand what's going on.
[01:09:10.640 --> 01:09:11.640]   This is quick and loans.
[01:09:11.640 --> 01:09:13.400]   I want you to try it.
[01:09:13.400 --> 01:09:16.840]   Apply simply, understand fully, mortgage confidently
[01:09:16.840 --> 01:09:19.640]   at rocketmortgage.com/twit2.
[01:09:19.640 --> 01:09:21.160]   That's twit and the number two,
[01:09:21.160 --> 01:09:22.840]   rocketmortgage.com/twit2,
[01:09:22.840 --> 01:09:25.360]   equal housing, lender, licensed in all 50 states
[01:09:25.360 --> 01:09:27.880]   and MLS consumer access.org number 30, 30.
[01:09:27.880 --> 01:09:29.320]   You can refi too.
[01:09:29.320 --> 01:09:30.600]   Whether you're buying or refying,
[01:09:30.600 --> 01:09:33.080]   rocketmortgage.com/twit.
[01:09:33.080 --> 01:09:33.920]   And the number two,
[01:09:33.920 --> 01:09:37.080]   and we thank you so much for making this week in tech,
[01:09:37.080 --> 01:09:37.920]   possible.
[01:09:37.920 --> 01:09:42.640]   Ah, there's so many things we could talk about here.
[01:09:43.520 --> 01:09:44.880]   What do you want to talk about?
[01:09:44.880 --> 01:09:47.800]   Amy, is there something that you're looking at going,
[01:09:47.800 --> 01:09:50.360]   this is something important that happened this week?
[01:09:50.360 --> 01:09:53.160]   I don't know where to go.
[01:09:53.160 --> 01:09:54.680]   Yeah, there was a one.
[01:09:54.680 --> 01:09:56.160]   We did the two big ones.
[01:09:56.160 --> 01:09:59.320]   You want to talk about Vic?
[01:09:59.320 --> 01:10:01.320]   Vic, Vic Condotra.
[01:10:01.320 --> 01:10:03.560]   So Vic used to work at Microsoft, right?
[01:10:03.560 --> 01:10:05.960]   He was an executive there, got a job at Google.
[01:10:05.960 --> 01:10:10.160]   And his first job there was interfacing with Apple,
[01:10:10.160 --> 01:10:14.200]   with Steve Jobs on the Google Apps for the iPhone.
[01:10:14.200 --> 01:10:15.160]   In the early days of the iPhone,
[01:10:15.160 --> 01:10:16.880]   he tells great stories, Vic told great stories
[01:10:16.880 --> 01:10:18.880]   about how Steve would call him one Sunday.
[01:10:18.880 --> 01:10:21.400]   He'd call him and Vic didn't answer
[01:10:21.400 --> 01:10:23.520]   'cause he was at church, he was at services.
[01:10:23.520 --> 01:10:25.960]   And later called Steve back and Steve said,
[01:10:25.960 --> 01:10:26.800]   "Why didn't you call me back?"
[01:10:26.800 --> 01:10:27.800]   He said, "Well, I was at services."
[01:10:27.800 --> 01:10:29.400]   He says, "Oh, that's different."
[01:10:29.400 --> 01:10:34.400]   He says, "The color of the O on your Google app is wrong.
[01:10:34.400 --> 01:10:37.080]   You gotta change it."
[01:10:37.080 --> 01:10:38.200]   And things like that.
[01:10:38.200 --> 01:10:39.840]   Anyway, great stories.
[01:10:39.840 --> 01:10:44.200]   But then Vic became completely enamored of social
[01:10:44.200 --> 01:10:45.880]   and said, "Google is missing out.
[01:10:45.880 --> 01:10:48.120]   We're not getting so much data
[01:10:48.120 --> 01:10:49.960]   that Facebook and Twitter get from social.
[01:10:49.960 --> 01:10:52.280]   We've got, social's gotta be our bottom line."
[01:10:52.280 --> 01:10:55.680]   He lobbied, Larry Page mercilessly, incessantly
[01:10:55.680 --> 01:10:57.920]   for more than a year, I'm told.
[01:10:57.920 --> 01:10:59.680]   And finally got Google+ off the ground.
[01:10:59.680 --> 01:11:01.680]   That was Vic Condotra's baby.
[01:11:01.680 --> 01:11:02.920]   Wasn't that a success?
[01:11:02.920 --> 01:11:04.200]   Yeah, well, it wasn't a great baby,
[01:11:04.200 --> 01:11:06.080]   but it was a funny looking kid.
[01:11:06.080 --> 01:11:09.760]   But anyway, anyway, he then left
[01:11:09.760 --> 01:11:12.440]   after the kind of the failure of Google+
[01:11:12.440 --> 01:11:13.960]   I think he left, not in disgrace,
[01:11:13.960 --> 01:11:16.120]   but he said, "Well, all right, fine, it didn't work out."
[01:11:16.120 --> 01:11:18.200]   He says, "Actually, started an interesting startup
[01:11:18.200 --> 01:11:21.280]   called Cardia, which I have.
[01:11:21.280 --> 01:11:24.240]   It's a EKG in a little device that goes to your phone
[01:11:24.240 --> 01:11:26.400]   and you get a doctor to read it if you've got AFib,
[01:11:26.400 --> 01:11:28.000]   which I don't have.
[01:11:28.000 --> 01:11:30.440]   But Jeff Jarvis who's on Twig does,
[01:11:30.440 --> 01:11:32.200]   and he uses it and sends it to his doctor.
[01:11:32.200 --> 01:11:34.080]   And it's a great way to diagnostic.
[01:11:34.080 --> 01:11:38.840]   It's one of the few FDA approved iPhone devices out there.
[01:11:38.840 --> 01:11:40.960]   So he's doing great, but for some reason he's decided
[01:11:40.960 --> 01:11:44.920]   he just wanted one last shot to Google,
[01:11:44.920 --> 01:11:47.240]   and he did it, and I think this is not an accident
[01:11:47.240 --> 01:11:48.320]   on Facebook.
[01:11:48.320 --> 01:11:52.080]   He posted pictures of his daughters,
[01:11:52.080 --> 01:11:54.400]   beautiful pictures, taken with an iPhone,
[01:11:54.400 --> 01:11:57.880]   and announced, "The iPhone is so good.
[01:11:57.880 --> 01:12:00.880]   Throw away your DSLRs."
[01:12:00.880 --> 01:12:04.560]   And especially in the comment he says later,
[01:12:04.560 --> 01:12:06.040]   and you don't need an Android phone either.
[01:12:06.040 --> 01:12:07.480]   This is it right now.
[01:12:07.480 --> 01:12:09.680]   Throw away your Android phone.
[01:12:09.680 --> 01:12:11.680]   No, you deal with it, throw away that, really.
[01:12:11.680 --> 01:12:12.960]   In favor of an iPhone.
[01:12:12.960 --> 01:12:14.120]   Yeah, good luck with that.
[01:12:14.120 --> 01:12:14.840]   That's a little bold.
[01:12:14.840 --> 01:12:16.240]   Until the iPhone can shoot.
[01:12:16.240 --> 01:12:17.040]   A little bold?
[01:12:17.040 --> 01:12:18.040]   I'm thankful of the...
[01:12:18.040 --> 01:12:19.280]   That's my British understatement.
[01:12:19.280 --> 01:12:20.120]   I'm trying to...
[01:12:20.120 --> 01:12:21.120]   True.
[01:12:21.120 --> 01:12:22.920]   Well, the comments are great.
[01:12:22.920 --> 01:12:25.200]   I mean, Scoble says, "Yeah, I gave away all my Canon cameras."
[01:12:25.200 --> 01:12:26.920]   Of course, Scoble would give that to me.
[01:12:26.920 --> 01:12:27.760]   I know.
[01:12:27.760 --> 01:12:28.600]   I'm a Canon guy.
[01:12:28.600 --> 01:12:30.360]   Look, until the iPhone can shoot in RAW,
[01:12:30.360 --> 01:12:34.360]   I mean, and has significantly better lenses, et cetera.
[01:12:34.360 --> 01:12:35.760]   It's a little extreme.
[01:12:35.760 --> 01:12:37.320]   But they are very infectious.
[01:12:37.320 --> 01:12:39.800]   Somebody says, "Indeed, the era has arrived,
[01:12:39.800 --> 01:12:43.280]   and Samsung S8 does an even better job than the iPhone."
[01:12:43.280 --> 01:12:44.880]   And Vic responds, "Well, here's the problem."
[01:12:44.880 --> 01:12:46.200]   It's Android.
[01:12:46.200 --> 01:12:47.400]   Yeah, but I'm sorry.
[01:12:47.400 --> 01:12:51.000]   The Lumia range did better photos than any of these,
[01:12:51.000 --> 01:12:54.200]   than either Apple or Android, but it was Windows phones,
[01:12:54.200 --> 01:12:55.640]   so nobody used it.
[01:12:55.640 --> 01:12:58.360]   So, I mean, the Lumia phones were fantastic for photos.
[01:12:58.360 --> 01:12:59.200]   Yeah, they were.
[01:12:59.200 --> 01:13:00.040]   But remember those?
[01:13:00.040 --> 01:13:00.880]   40 megapixel...
[01:13:00.880 --> 01:13:02.680]   40 megapixel...
[01:13:02.680 --> 01:13:04.440]   Which felt like just showing off at that point.
[01:13:04.440 --> 01:13:06.120]   Oh, yeah, I mean, it was a huge thing.
[01:13:06.120 --> 01:13:09.120]   But at Samsung, if you want good photos, don't use your phone.
[01:13:09.120 --> 01:13:09.960]   Well, it is the phone.
[01:13:09.960 --> 01:13:13.560]   I gotta say, we went, we, I've got an S8,
[01:13:13.560 --> 01:13:17.320]   and we took, that phone takes pretty stunning pictures.
[01:13:17.320 --> 01:13:21.680]   I agree. We also, it takes pretty stunning pictures.
[01:13:21.680 --> 01:13:22.640]   Yeah, full stop.
[01:13:22.640 --> 01:13:23.480]   It really does.
[01:13:23.480 --> 01:13:26.280]   It's of all the phones that I've ever had.
[01:13:26.280 --> 01:13:29.720]   And I haven't used a camera camera in a while, but the...
[01:13:29.720 --> 01:13:31.240]   Well, there you are.
[01:13:31.240 --> 01:13:32.160]   You're the example.
[01:13:32.160 --> 01:13:33.160]   You don't need a camera.
[01:13:33.160 --> 01:13:34.720]   You've got a phone.
[01:13:34.720 --> 01:13:35.560]   I don't.
[01:13:35.560 --> 01:13:36.560]   I don't.
[01:13:36.560 --> 01:13:37.560]   The best camera is the one you have with you.
[01:13:37.560 --> 01:13:39.560]   And we always have our phones.
[01:13:39.560 --> 01:13:40.560]   And so there's...
[01:13:40.560 --> 01:13:45.920]   It's an unambiguous great thing as these phones get better and better and better.
[01:13:45.920 --> 01:13:46.920]   The camera phones.
[01:13:46.920 --> 01:13:49.280]   And we always have that.
[01:13:49.280 --> 01:13:50.280]   But there's...
[01:13:50.280 --> 01:13:51.280]   Until you can...
[01:13:51.280 --> 01:13:52.640]   You just can't swap lenses on the phone.
[01:13:52.640 --> 01:13:54.840]   There's things you simply can't do with light.
[01:13:54.840 --> 01:13:58.400]   And so people who want to do a wide range of things in photography are simply always
[01:13:58.400 --> 01:14:00.040]   going to be on cameras.
[01:14:00.040 --> 01:14:03.560]   But I think it is magnificent how great these camera phones are getting.
[01:14:03.560 --> 01:14:07.760]   And their ubiquity does mean that all kinds of things are being captured that never would
[01:14:07.760 --> 01:14:08.760]   have been in the past.
[01:14:08.760 --> 01:14:10.600]   I think this is a really big societal change.
[01:14:10.600 --> 01:14:14.680]   I mean, we first saw with Rodney King back in the '90s and now we're seeing...
[01:14:14.680 --> 01:14:15.680]   Right.
[01:14:15.680 --> 01:14:16.680]   Yeah.
[01:14:16.680 --> 01:14:18.680]   Well, the whole Rodney King beating was caught.
[01:14:18.680 --> 01:14:19.680]   Oh, on a camera phone.
[01:14:19.680 --> 01:14:20.680]   You're right.
[01:14:20.680 --> 01:14:24.000]   Because somebody had a low-cost video camera.
[01:14:24.000 --> 01:14:25.000]   Yeah.
[01:14:25.000 --> 01:14:26.720]   And they were just like, "Okay, well, I've got the slide around the house.
[01:14:26.720 --> 01:14:27.720]   Oh, well, video, that.
[01:14:27.720 --> 01:14:28.720]   That's something else."
[01:14:28.720 --> 01:14:31.640]   And now everybody's got a video camera on them all the time.
[01:14:31.640 --> 01:14:33.120]   You can Facebook, live this stuff.
[01:14:33.120 --> 01:14:37.240]   We know as much as we do about police shootings of black people, if it weren't for camera
[01:14:37.240 --> 01:14:38.240]   phones.
[01:14:38.240 --> 01:14:39.240]   Presumably now.
[01:14:39.240 --> 01:14:40.240]   Yeah.
[01:14:40.240 --> 01:14:41.240]   Yeah.
[01:14:41.240 --> 01:14:42.240]   I mean, all of...
[01:14:42.240 --> 01:14:43.240]   I mean, Amy, you must have...
[01:14:43.240 --> 01:14:46.360]   Oh, I would say I have a funny story.
[01:14:46.360 --> 01:14:48.640]   So I've had two careers.
[01:14:48.640 --> 01:14:51.000]   The first career was as a journalist.
[01:14:51.000 --> 01:14:54.600]   And I lived in Japan in the mid '90s.
[01:14:54.600 --> 01:14:59.120]   And when I moved back to the United States to go to journalism school at Columbia, my
[01:14:59.120 --> 01:15:00.600]   phone had a camera in it.
[01:15:00.600 --> 01:15:03.680]   And it was one of the first of its kind ever.
[01:15:03.680 --> 01:15:05.600]   And the very first class...
[01:15:05.600 --> 01:15:07.640]   And I paid my way through Columbia.
[01:15:07.640 --> 01:15:08.640]   So I'm sitting in my...
[01:15:08.640 --> 01:15:10.000]   I'd moved back to the United States.
[01:15:10.000 --> 01:15:11.000]   I'm sitting in class.
[01:15:11.000 --> 01:15:12.000]   First day.
[01:15:12.000 --> 01:15:13.760]   It was an ethics class.
[01:15:13.760 --> 01:15:17.840]   And the question was, should journalists be allowed to use digital cameras?
[01:15:17.840 --> 01:15:18.840]   Oh, the allowers?
[01:15:18.840 --> 01:15:19.840]   And...
[01:15:19.840 --> 01:15:20.840]   Right.
[01:15:20.840 --> 01:15:21.840]   That was the question.
[01:15:21.840 --> 01:15:22.840]   Now to be fair, this was 2000.
[01:15:22.840 --> 01:15:23.840]   Right?
[01:15:23.840 --> 01:15:24.840]   So it was a while ago.
[01:15:24.840 --> 01:15:25.840]   It wasn't a stunt question.
[01:15:25.840 --> 01:15:26.840]   It was an earnest question.
[01:15:26.840 --> 01:15:27.840]   No, it was a premise.
[01:15:27.840 --> 01:15:28.840]   Why not?
[01:15:28.840 --> 01:15:34.200]   This was not too long after the O.J. Simpson dual cover had come out.
[01:15:34.200 --> 01:15:37.280]   In time and news, we could run different covers.
[01:15:37.280 --> 01:15:40.400]   And one was clearly digitally altered to make him look more menacing.
[01:15:40.400 --> 01:15:41.400]   Oh, yes.
[01:15:41.400 --> 01:15:42.400]   Oh, remember that.
[01:15:42.400 --> 01:15:43.400]   Yeah.
[01:15:43.400 --> 01:15:46.320]   So the question was, should journalists be allowed to alter their photos?
[01:15:46.320 --> 01:15:48.080]   Should they be allowed to use digital cameras?
[01:15:48.080 --> 01:15:50.200]   Because you can alter the photos in that way.
[01:15:50.200 --> 01:15:53.160]   And I raised my hand and I said, so I...
[01:15:53.160 --> 01:15:56.160]   I get what kind of conversation we're having here.
[01:15:56.160 --> 01:16:01.600]   However, I've just come from a country where people have cameras in their phones, right?
[01:16:01.600 --> 01:16:04.880]   And I pull out my phone, which doesn't work in the U.S. but I pulled out and I was like,
[01:16:04.880 --> 01:16:07.120]   look, there's a camera in this phone.
[01:16:07.120 --> 01:16:09.120]   And this phone is also connected to the internet.
[01:16:09.120 --> 01:16:17.480]   So theoretically, like I could publish this photo from my phone to your newspaper's website.
[01:16:17.480 --> 01:16:21.440]   And everybody looked at me like it was an alien and I got dressed down by the professor
[01:16:21.440 --> 01:16:29.240]   who said, mark my word, no news organization would ever, ever publish a photo, taken a
[01:16:29.240 --> 01:16:32.120]   grainy photo taken by a phone.
[01:16:32.120 --> 01:16:33.600]   And I said, but you're missing the point.
[01:16:33.600 --> 01:16:35.040]   It's not the quality of the photo.
[01:16:35.040 --> 01:16:36.040]   It's the immediacy.
[01:16:36.040 --> 01:16:43.040]   It's the fact that I can publish visual content anywhere I'm at at any time.
[01:16:43.040 --> 01:16:47.600]   And then I thought, well, I've wasted 30 grand going to grad school.
[01:16:47.600 --> 01:16:52.360]   In addition, the professor, 13 years later when the Chicago Sun-Times fired every one
[01:16:52.360 --> 01:16:58.440]   of its photojournalists and told its reporters, now it's your job to take pictures with your
[01:16:58.440 --> 01:16:59.440]   phones.
[01:16:59.440 --> 01:17:00.440]   13 years.
[01:17:00.440 --> 01:17:02.000]   That's all it took.
[01:17:02.000 --> 01:17:04.200]   It comes back to change.
[01:17:04.200 --> 01:17:07.360]   So we're used to thinking of quality photos.
[01:17:07.360 --> 01:17:11.160]   And so to be, now I don't think the iPhone 7 takes the world's greatest pictures.
[01:17:11.160 --> 01:17:16.800]   However, part of our sort of reaction, I think, has to do with the idea that we're in our
[01:17:16.800 --> 01:17:22.440]   heads, great quality photos only come from these kinds of cameras.
[01:17:22.440 --> 01:17:27.200]   And part of our reaction, I think, is this a version of change, right?
[01:17:27.200 --> 01:17:28.200]   Isn't that--
[01:17:28.200 --> 01:17:33.800]   You know, we have taken more pictures last year than in the entire history of humankind.
[01:17:33.800 --> 01:17:38.920]   Yeah, not all of them are good, but not all of them survive.
[01:17:38.920 --> 01:17:44.520]   But I think it's a good thing.
[01:17:44.520 --> 01:17:49.720]   I think Vic's point was more-- I'm going to give him credit, maybe that he doesn't
[01:17:49.720 --> 01:17:50.720]   serve.
[01:17:50.720 --> 01:17:55.440]   But I think his point was more that computational photography is changing our notion of what
[01:17:55.440 --> 01:18:00.200]   a camera is, that the thing that makes an iPhone great is not its lens.
[01:18:00.200 --> 01:18:02.080]   It's not its capturing equipment.
[01:18:02.080 --> 01:18:06.560]   It's the massive computer in the iPhone that takes those images and makes something out
[01:18:06.560 --> 01:18:07.560]   of them.
[01:18:07.560 --> 01:18:09.880]   It's better than it should be than it has any right to be.
[01:18:09.880 --> 01:18:12.480]   And the pixel does exactly the same thing.
[01:18:12.480 --> 01:18:18.280]   Google's got a whole lot of artificial intelligence built into the software for the camera.
[01:18:18.280 --> 01:18:21.600]   I do think Vic was slightly disingenuous, though, in that there was a--
[01:18:21.600 --> 01:18:22.600]   It's a little shot.
[01:18:22.600 --> 01:18:25.080]   There was a definite two fingers to you, Google, and this.
[01:18:25.080 --> 01:18:26.080]   Yeah, it was a little shot.
[01:18:26.080 --> 01:18:28.080]   And he's just like, you passed me over for senior management.
[01:18:28.080 --> 01:18:29.080]   Screw you guys.
[01:18:29.080 --> 01:18:30.080]   I'm out of here.
[01:18:30.080 --> 01:18:31.080]   Screw you guys.
[01:18:31.080 --> 01:18:32.080]   And he's going out.
[01:18:32.080 --> 01:18:34.160]   The good news is he's doing just fine on his own.
[01:18:34.160 --> 01:18:37.480]   He's got a great company.
[01:18:37.480 --> 01:18:43.000]   I thought Facebook pulling the plug on SkyNet was a really fun rumor.
[01:18:43.000 --> 01:18:44.000]   Oh, that lasted.
[01:18:44.000 --> 01:18:45.000]   Tell me about that.
[01:18:45.000 --> 01:18:46.000]   It was such yellow journalism.
[01:18:46.000 --> 01:18:47.000]   Oh, it was serious.
[01:18:47.000 --> 01:18:48.000]   The plug on SkyNet.
[01:18:48.000 --> 01:18:51.000]   It was in the show notes.
[01:18:51.000 --> 01:18:56.080]   So basically-- but I watched the story as it unfolded.
[01:18:56.080 --> 01:19:01.920]   I guess that there was some kind of a chatbot thing going on inside of Facebook.
[01:19:01.920 --> 01:19:08.000]   And the sensationalistic and not at all accurate but really cool headline was that these two
[01:19:08.000 --> 01:19:13.640]   chatbots had established their own language that only they could understand.
[01:19:13.640 --> 01:19:15.160]   And it was creepy stuff.
[01:19:15.160 --> 01:19:18.920]   Actually, if you looked at the sentences that were going back and forth, and Facebook,
[01:19:18.920 --> 01:19:22.720]   somebody had this diving save where they grabbed the power cord and pulled it out.
[01:19:22.720 --> 01:19:23.720]   No, I think that part--
[01:19:23.720 --> 01:19:29.320]   Wait, before SkyNet woke up, no, this was kind of like when the tabloids got pulled of it.
[01:19:29.320 --> 01:19:34.480]   But the first Boy Genius report article that I saw-- the first article I saw was on Boy Genius
[01:19:34.480 --> 01:19:35.480]   report.
[01:19:35.480 --> 01:19:37.880]   It was actually the day before my book came out, which has something to do with it.
[01:19:37.880 --> 01:19:38.880]   You have?
[01:19:38.880 --> 01:19:39.880]   You have an artist.
[01:19:39.880 --> 01:19:40.880]   You have a social network.
[01:19:40.880 --> 01:19:41.880]   Can I say this?
[01:19:41.880 --> 01:19:42.880]   Yeah, sure.
[01:19:42.880 --> 01:19:43.880]   Absolutely.
[01:19:43.880 --> 01:19:45.360]   It's called Flutter because it's very early on in the book.
[01:19:45.360 --> 01:19:46.360]   Yeah, yeah.
[01:19:46.360 --> 01:19:47.360]   That does achieve sentience.
[01:19:47.360 --> 01:19:48.360]   Yes, exactly.
[01:19:48.360 --> 01:19:51.160]   But it turns out not to be exactly malicious.
[01:19:51.160 --> 01:19:53.960]   Well, not deliberately so.
[01:19:53.960 --> 01:19:55.680]   So anyway, just think about it.
[01:19:55.680 --> 01:19:56.680]   It's fired funnily so.
[01:19:56.680 --> 01:19:57.680]   Yeah.
[01:19:57.680 --> 01:20:01.580]   So Facebook, so there was a story was that, there was this freak out that Facebook, these
[01:20:01.580 --> 01:20:02.580]   Yeah.
[01:20:02.580 --> 01:20:04.040]   these just awesome, let me read you actually.
[01:20:04.040 --> 01:20:07.220]   why don't you and I going to say we let's do a dramatic read.
[01:20:07.220 --> 01:20:08.220]   Okay, would be.
[01:20:08.220 --> 01:20:09.220]   What are the gentline?
[01:20:09.220 --> 01:20:10.340]   That you want to be Alice or Bob?
[01:20:10.340 --> 01:20:11.400]   Oh, you be Bob.
[01:20:11.400 --> 01:20:12.400]   I'll be Bob.
[01:20:12.400 --> 01:20:13.400]   This is all the best line.
[01:20:13.400 --> 01:20:17.760]   Okay, this is Bob and Alice are two say ais on the Facebook network.
[01:20:17.760 --> 01:20:18.760]   I can.
[01:20:18.760 --> 01:20:21.700]   I Everything else.
[01:20:21.700 --> 01:20:27.560]   I have zero to me, to me, to me, to me, to me, to me, to me, to me, to me, to you.
[01:20:27.560 --> 01:20:30.800]   I Everything else.
[01:20:30.800 --> 01:20:35.040]   Balls have a ball to me, to me, to me, to me, to me, to me.
[01:20:35.040 --> 01:20:36.040]   And they walked off.
[01:20:36.040 --> 01:20:37.040]   I'm an I am into the sunset.
[01:20:37.040 --> 01:20:39.080]   It's me to me, to me, to me.
[01:20:39.080 --> 01:20:41.720]   It's a guy that this ain't.
[01:20:41.720 --> 01:20:44.280]   That would be a frightening thing to see spontaneous.
[01:20:44.280 --> 01:20:45.960]   Coming up on the monitor.
[01:20:45.960 --> 01:20:48.520]   You've never had a drunk text from my wife, have you?
[01:20:48.520 --> 01:20:54.220]   I remember when somebody had a Google home talking to an Amazon Echo and it was very similar.
[01:20:54.220 --> 01:20:55.220]   It was very charming.
[01:20:55.220 --> 01:20:58.220]   I think that that was actually pretty well scripted as well.
[01:20:58.220 --> 01:20:59.220]   I think you're.
[01:20:59.220 --> 01:21:01.460]   They understood each other a little too well.
[01:21:01.460 --> 01:21:02.460]   Yeah, yeah.
[01:21:02.460 --> 01:21:03.460]   But it was fun.
[01:21:03.460 --> 01:21:07.220]   Yeah, well, it was clearly scripted because there's no way that that actually could have
[01:21:07.220 --> 01:21:10.700]   happened unless there was some skill taught to both of them.
[01:21:10.700 --> 01:21:15.900]   Well, you guys just read is the kind of conversation that's going to happen for at least the foreseeable
[01:21:15.900 --> 01:21:16.900]   future.
[01:21:16.900 --> 01:21:21.040]   I talked about this last week because Elon Musk is panicking about artificial intelligence
[01:21:21.040 --> 01:21:26.140]   and artificial intelligence researcher at MIT.
[01:21:26.140 --> 01:21:30.380]   One of the most famous roboticists said, that's only because he knows nothing about AI.
[01:21:30.380 --> 01:21:35.400]   And anybody who's worked in the field knows this is we're on this stuff is not even close
[01:21:35.400 --> 01:21:36.400]   to coming.
[01:21:36.400 --> 01:21:40.680]   Now, maybe that's disinformation being passed along by the AI community.
[01:21:40.680 --> 01:21:50.800]   We're pretty far off from making autonomous big autonomous decisions.
[01:21:50.800 --> 01:21:53.720]   However, the foundation is being laid right now.
[01:21:53.720 --> 01:21:58.400]   And so if you think about a house being built, right, if you if the first layer of that foundation
[01:21:58.400 --> 01:22:01.880]   is a little off and then you build the next one and the next one, you wind up with the
[01:22:01.880 --> 01:22:02.880]   building.
[01:22:02.880 --> 01:22:07.720]   It's a building, but it's a cock-eyed sort of weird tilting, you know, building that
[01:22:07.720 --> 01:22:10.560]   probably wouldn't pass code.
[01:22:10.560 --> 01:22:11.720]   Some day in the future.
[01:22:11.720 --> 01:22:16.920]   So, you know, I actually think there's plenty to be worried about, but it has to do with
[01:22:16.920 --> 01:22:24.280]   how the corpus is being built, what's happening with the data, and then what decisions we're
[01:22:24.280 --> 01:22:30.440]   asking the machines to make because at some point in order for the systems to advance,
[01:22:30.440 --> 01:22:32.560]   they have to write their own software, right?
[01:22:32.560 --> 01:22:38.040]   And again, this is a little farther off, but it's within our lifetimes.
[01:22:38.040 --> 01:22:45.800]   And then at some point, you know, we lose some of that control and understanding.
[01:22:45.800 --> 01:22:50.840]   So I do think that there's a lot for us to be worried about, but it's a lot more subtle,
[01:22:50.840 --> 01:22:53.840]   you know, but it's concerning for sure.
[01:22:53.840 --> 01:22:57.800]   Yeah, but and this was the point the MIT scientist made.
[01:22:57.800 --> 01:23:00.360]   If you're going to make a law now, what are you going to regulate?
[01:23:00.360 --> 01:23:02.240]   Yeah, well, that's the problem, right?
[01:23:02.240 --> 01:23:04.880]   This is so much of it comes back to this.
[01:23:04.880 --> 01:23:11.680]   And that is in order for our laws to be enforced and to be applicable, they have to be descriptive,
[01:23:11.680 --> 01:23:17.080]   but it's also hard to, because you can't have a law that's so over encompassing and broad
[01:23:17.080 --> 01:23:20.040]   because it'll get struck down and it'll get challenged.
[01:23:20.040 --> 01:23:21.560]   People won't know how to enforce it.
[01:23:21.560 --> 01:23:25.200]   On the other hand, if it's too specific, it becomes outdated right away.
[01:23:25.200 --> 01:23:30.040]   So one of the challenges every democracy on planet Earth is going to have going forward
[01:23:30.040 --> 01:23:36.360]   is how do you maintain some kind of rule in society and some kind of laws that make sense
[01:23:36.360 --> 01:23:44.200]   that are neither too restrictive nor too broad so that they can govern us going forward
[01:23:44.200 --> 01:23:46.920]   and that they can be, you know, it's hard.
[01:23:46.920 --> 01:23:48.840]   But even then, that's only in one country.
[01:23:48.840 --> 01:23:52.760]   You know, I mean, there are a lot of AI researchers in a lot of countries around the world who
[01:23:52.760 --> 01:23:57.400]   are developing this stuff and there's no way one law is going to control them all.
[01:23:57.400 --> 01:24:00.960]   And yeah, we're so far away from a singularity at the moment.
[01:24:00.960 --> 01:24:05.680]   It's a joke, but even so, we've got to start thinking about AI on a serious way.
[01:24:05.680 --> 01:24:10.040]   And as you say, if the foundations are faulty, then the whole thing comes tumbling down.
[01:24:10.040 --> 01:24:15.040]   Again, to Elon's credit, I think a lot of what he's talking about is thinking now about
[01:24:15.040 --> 01:24:16.040]   AI safety.
[01:24:16.040 --> 01:24:17.040]   And he's doing it.
[01:24:17.040 --> 01:24:21.000]   And he's giving $10 million to the future of life instituting.
[01:24:21.000 --> 01:24:24.600]   And there's an AI institute that he did.
[01:24:24.600 --> 01:24:30.080]   Yeah, and so even if it's wildly improbable that something terrible will happen, and unfortunately,
[01:24:30.080 --> 01:24:32.440]   it's probably not wildly improbable.
[01:24:32.440 --> 01:24:34.880]   Hopefully it is somewhat improbable.
[01:24:34.880 --> 01:24:39.720]   But the consequences are so devastating that it makes enormous sense to be thinking about
[01:24:39.720 --> 01:24:43.520]   it right now and to be working about it and laying the right foundations.
[01:24:43.520 --> 01:24:47.240]   And I think that's generally Elon's point.
[01:24:47.240 --> 01:24:51.240]   Although he did say something kind of chilling that meeting with the governor's council,
[01:24:51.240 --> 01:24:55.200]   which I think is what you're referring to, Leo, when he was talking to the governors,
[01:24:55.200 --> 01:24:59.560]   he basically indicated, and it was sort of impassing, that he had seen something that
[01:24:59.560 --> 01:25:00.720]   we haven't.
[01:25:00.720 --> 01:25:04.960]   And it implied that he'd seen something behind the curtains, whether it was a Google or somewhere
[01:25:04.960 --> 01:25:10.960]   else, that had really kind of chilled him in recent days or weeks at that point.
[01:25:10.960 --> 01:25:12.440]   And I was like, "Who bots going you mean?"
[01:25:12.440 --> 01:25:13.440]   You mean me?
[01:25:13.440 --> 01:25:15.320]   Well, no, I mean, you say that.
[01:25:15.320 --> 01:25:21.120]   But at DEF CON this year, there was a presentation from researchers who'd taken Musk's open
[01:25:21.120 --> 01:25:27.040]   AI framework and had used it to develop malware, which could get past security antivirus engines
[01:25:27.040 --> 01:25:32.160]   by doing slower iterations and evolution onto the software until finally, after 100,000
[01:25:32.160 --> 01:25:38.240]   repetitions, you got up with malware, which could easily slip by a security engine.
[01:25:38.240 --> 01:25:40.680]   This stuff is coming, but it's just why--
[01:25:40.680 --> 01:25:41.680]   Well, part of the problem is--
[01:25:41.680 --> 01:25:44.600]   --whether or not we need to be hardwiring, you know, electromagnetic shotguns to these
[01:25:44.600 --> 01:25:48.120]   computers, foreheads is another matter entirely.
[01:25:48.120 --> 01:25:54.440]   In order to test-- so you have to have adversarial versions of the tools, right, in order to test
[01:25:54.440 --> 01:25:55.440]   and train them.
[01:25:55.440 --> 01:25:56.440]   Yeah, you need to black balls.
[01:25:56.440 --> 01:25:57.440]   I mean, that's right.
[01:25:57.440 --> 01:26:04.840]   So one of the problems is in order to prevent problems and protect us in the future, you
[01:26:04.840 --> 01:26:10.720]   have to train machines to recognize problems, but in order to train them to recognize problems,
[01:26:10.720 --> 01:26:14.120]   you have to teach them the problems, right?
[01:26:14.120 --> 01:26:19.640]   So again, this is why I don't think we need rules and regulations, but we do need some
[01:26:19.640 --> 01:26:24.720]   kind of meaningful way to have conversations that translate to more than just news headlines
[01:26:24.720 --> 01:26:26.200]   and white papers.
[01:26:26.200 --> 01:26:31.320]   We have to have good conversations, and right now there are seven companies that really do
[01:26:31.320 --> 01:26:34.040]   control the fate of AI in our future.
[01:26:34.040 --> 01:26:41.320]   One of those companies is Tencent, which is a big Chinese company, which, you know,
[01:26:41.320 --> 01:26:45.640]   next week had an AI chatbot start talking smack about the Communist Party.
[01:26:45.640 --> 01:26:46.640]   Yeah.
[01:26:46.640 --> 01:26:47.640]   Where is that?
[01:26:47.640 --> 01:26:48.640]   Yeah.
[01:26:48.640 --> 01:26:50.840]   Oh, you mentioned Tay earlier.
[01:26:50.840 --> 01:26:53.040]   Yeah, Microsoft Tay and that story very well.
[01:26:53.040 --> 01:26:54.040]   No, no, no.
[01:26:54.040 --> 01:26:59.280]   Tencent did almost exactly the same thing they released to bot, and it was taught to hate
[01:26:59.280 --> 01:27:02.520]   communism, and then somebody asked you, "Well, how do you feel about the Communist Party?"
[01:27:02.520 --> 01:27:05.480]   It was kind of exactly the same thing.
[01:27:05.480 --> 01:27:06.480]   Wow.
[01:27:06.480 --> 01:27:07.480]   Yeah.
[01:27:07.480 --> 01:27:08.480]   Yeah.
[01:27:08.480 --> 01:27:09.480]   Yeah.
[01:27:09.480 --> 01:27:14.680]   It's a big deal, but, you know, every one of these companies, several of these companies
[01:27:14.680 --> 01:27:18.760]   say that they have open source, you know, they've open source their tools and their
[01:27:18.760 --> 01:27:20.880]   software, but these are still commercial.
[01:27:20.880 --> 01:27:24.440]   They are commercially focused and in the United States, so this is where things get
[01:27:24.440 --> 01:27:26.600]   really weird and interesting and different.
[01:27:26.600 --> 01:27:32.280]   So China is pretty far ahead in a lot of ways and it's AI research, and its government
[01:27:32.280 --> 01:27:33.720]   funded a lot of it.
[01:27:33.720 --> 01:27:41.200]   And the work is being done in academia, you know, and a lot of it is, so there's big government
[01:27:41.200 --> 01:27:44.480]   component and it's a national effort to push ahead.
[01:27:44.480 --> 01:27:46.960]   In the United States, it's commercial.
[01:27:46.960 --> 01:27:52.160]   All of, you know, our government is very, very far behind state, military, DOD, very
[01:27:52.160 --> 01:27:54.840]   far behind when it comes to AI.
[01:27:54.840 --> 01:28:01.680]   The majority of the funding that's going into it is commercially focused, you know,
[01:28:01.680 --> 01:28:08.880]   we don't have the academic, we have academic, you know, research centers that are working
[01:28:08.880 --> 01:28:14.880]   on this, but again, there's a commercial, there's a commercial focus rather than it
[01:28:14.880 --> 01:28:21.440]   being for the, you know, taking this much longer view and being done sort of as big national
[01:28:21.440 --> 01:28:22.440]   effort.
[01:28:22.440 --> 01:28:25.440]   So that's a problem going forward because we have to-
[01:28:25.440 --> 01:28:26.440]   Really?
[01:28:26.440 --> 01:28:27.440]   Which would you-
[01:28:27.440 --> 01:28:30.080]   You don't think the commercial effort could succeed over the government effort?
[01:28:30.080 --> 01:28:32.080]   Well, the problem is-
[01:28:32.080 --> 01:28:37.640]   I think of the genome project where there was a government attempt to decode the genome
[01:28:37.640 --> 01:28:39.880]   and it was a private effort that ended up-
[01:28:39.880 --> 01:28:42.880]   Yeah, Frank Venture ended up like coming from behind and-
[01:28:42.880 --> 01:28:47.440]   Well, hang on, hang on, he came to me coming from behind because he took all the government
[01:28:47.440 --> 01:28:49.440]   research and said, "Right, that's done.
[01:28:49.440 --> 01:28:50.440]   I'm just going to do the-
[01:28:50.440 --> 01:28:51.440]   It's a nice part of the project.
[01:28:51.440 --> 01:28:53.440]   Yeah, this is a public-private policy shit.
[01:28:53.440 --> 01:28:57.280]   But Amy, do you think that China has a lead just because it's a government-funded effort?
[01:28:57.280 --> 01:29:00.880]   Yeah, it did make- it did give a little- make things go a little bit faster.
[01:29:00.880 --> 01:29:05.120]   But for him to say, "Oh, I decoded the home and genome," a lot of other work went into
[01:29:05.120 --> 01:29:07.120]   "Oh, you have an opinion on this matter.
[01:29:07.120 --> 01:29:08.120]   I could tell."
[01:29:08.120 --> 01:29:09.120]   I'm sorry.
[01:29:09.120 --> 01:29:12.320]   I got so- Craig was- he was so self-congratulatory about it.
[01:29:12.320 --> 01:29:13.320]   Just like-
[01:29:13.320 --> 01:29:16.880]   Well, and then he tried to patent it, which was perhaps the hundreds of millions of governments
[01:29:16.880 --> 01:29:17.880]   have put into this-
[01:29:17.880 --> 01:29:18.880]   So it didn't go too far.
[01:29:18.880 --> 01:29:19.880]   Actually, I got their first one.
[01:29:19.880 --> 01:29:20.880]   I'm going to patent it and take all that money and-
[01:29:20.880 --> 01:29:21.880]   Yeah.
[01:29:21.880 --> 01:29:22.880]   I just-
[01:29:22.880 --> 01:29:23.880]   I'm on the genome.
[01:29:23.880 --> 01:29:25.880]   So Amy, do you think China has an advantage though because it's government-funded?
[01:29:25.880 --> 01:29:28.120]   Well, it depends on your perspective.
[01:29:28.120 --> 01:29:35.560]   So I think that when it comes to the war of the future is going to be fought by software,
[01:29:35.560 --> 01:29:36.560]   not by hardware.
[01:29:36.560 --> 01:29:37.560]   Right?
[01:29:37.560 --> 01:29:40.520]   The hardware will carry out the software, but it's the software and the data that's going
[01:29:40.520 --> 01:29:42.280]   to be making-
[01:29:42.280 --> 01:29:46.440]   So to be fair, this is top of mind because I was in Japan for the past couple of weeks
[01:29:46.440 --> 01:29:48.000]   and I was there when Korea sent its latest-
[01:29:48.000 --> 01:29:50.040]   North Korea sent its latest ICBM.
[01:29:50.040 --> 01:29:51.040]   Yeah.
[01:29:51.040 --> 01:29:55.520]   Not that we were worried about it, but I was thinking a lot about it.
[01:29:55.520 --> 01:29:58.800]   So the United States just doesn't have-
[01:29:58.800 --> 01:30:01.400]   We don't have that same coordinated effort.
[01:30:01.400 --> 01:30:03.160]   And so I'm a capitalist.
[01:30:03.160 --> 01:30:09.360]   So I've got nothing against big companies pushing ahead and for commercial purposes,
[01:30:09.360 --> 01:30:10.840]   pushing into AI.
[01:30:10.840 --> 01:30:13.720]   The problem that I've got is that I just-
[01:30:13.720 --> 01:30:22.920]   As a nation, I just don't see us pushing in a coordinated way ahead, taking a much longer
[01:30:22.920 --> 01:30:23.920]   view.
[01:30:23.920 --> 01:30:28.960]   So the AI that's going to get developed, and again, the AI that-
[01:30:28.960 --> 01:30:30.480]   This is about decision making.
[01:30:30.480 --> 01:30:33.920]   We are asking the premise of AI.
[01:30:33.920 --> 01:30:41.640]   The premise of AI, if you really go back, all of the early systems were built on games.
[01:30:41.640 --> 01:30:45.800]   So checkers, chess, go, right?
[01:30:45.800 --> 01:30:49.040]   And now there's another game that's that we're training.
[01:30:49.040 --> 01:30:56.480]   From the beginning, we have trained these systems to beat us, right?
[01:30:56.480 --> 01:30:57.480]   And-
[01:30:57.480 --> 01:30:58.480]   What do you want them to do?
[01:30:58.480 --> 01:31:00.080]   Train them to lose?
[01:31:00.080 --> 01:31:05.640]   My point is, at some point, we're asking the system to make decisions.
[01:31:05.640 --> 01:31:08.440]   We're asking them to make decisions.
[01:31:08.440 --> 01:31:13.320]   And not that I have any more trust in our government than I do in Google.
[01:31:13.320 --> 01:31:22.360]   However, I like the idea of public money and public research being used on behalf of all
[01:31:22.360 --> 01:31:24.160]   of humanity going forward.
[01:31:24.160 --> 01:31:29.640]   I think, culturally, China has an advantage because it thinks in a much longer time from
[01:31:29.640 --> 01:31:30.640]   100 years.
[01:31:30.640 --> 01:31:31.640]   Oh, no, no.
[01:31:31.640 --> 01:31:32.640]   That's right.
[01:31:32.640 --> 01:31:37.000]   One of the things that worries me about commercial AI projects as well is that there's such a
[01:31:37.000 --> 01:31:38.240]   little hype talked about it.
[01:31:38.240 --> 01:31:42.680]   I mean, Facebook has been baying on about its AI chatbots for over a year now.
[01:31:42.680 --> 01:31:44.880]   And then when you actually talk to them about it, it's not very interesting.
[01:31:44.880 --> 01:31:47.640]   Well, it's not really AI because of the human behind it.
[01:31:47.640 --> 01:31:48.640]   This is like a lie.
[01:31:48.640 --> 01:31:49.640]   Yeah.
[01:31:49.640 --> 01:31:54.280]   We have not made any progress since this is a lie's a written basic for an Apple II.
[01:31:54.280 --> 01:31:55.920]   That's exactly what these guys seem like.
[01:31:55.920 --> 01:32:01.240]   Well, until the Tencent AI started denouncing the communist party, and that's kind of cool.
[01:32:01.240 --> 01:32:02.240]   That's very plausible.
[01:32:02.240 --> 01:32:03.240]   That's a lie's a couldn't do that.
[01:32:03.240 --> 01:32:04.880]   Oh, I think she did.
[01:32:04.880 --> 01:32:06.960]   The communist party to save her life.
[01:32:06.960 --> 01:32:10.400]   But when we think about AI, we anthropomorphize it, right?
[01:32:10.400 --> 01:32:13.840]   So the real promise of AI isn't the human form that it takes.
[01:32:13.840 --> 01:32:18.480]   It's the automation of all the crap that we don't want to deal with or do, right?
[01:32:18.480 --> 01:32:21.560]   And that has tentacles into all different areas of our life.
[01:32:21.560 --> 01:32:27.040]   Goldman's figured out a way to automate three quarters of its IPO process, right?
[01:32:27.040 --> 01:32:29.200]   They're all these different steps involved.
[01:32:29.200 --> 01:32:31.720]   And they figured that out through machine learning.
[01:32:31.720 --> 01:32:39.080]   So I think this is one of those things where, again, we don't realize the great opportunities
[01:32:39.080 --> 01:32:44.120]   that we created and the way that we maybe seeded our future with some things we don't
[01:32:44.120 --> 01:32:46.160]   like until it gets here.
[01:32:46.160 --> 01:32:48.520]   And we realize that we're sorted into different tastes.
[01:32:48.520 --> 01:32:53.480]   And suddenly we don't have opportunities that we think we should have or life.
[01:32:53.480 --> 01:32:57.080]   Life, real life is very much an uncanny valley, right?
[01:32:57.080 --> 01:33:01.520]   Do you see 21st century Luddites taking over on this front?
[01:33:01.520 --> 01:33:02.520]   I don't know.
[01:33:02.520 --> 01:33:03.520]   I don't think so.
[01:33:03.520 --> 01:33:10.480]   And that's because humans want to use as little energy as possible, right?
[01:33:10.480 --> 01:33:11.920]   I know I do.
[01:33:11.920 --> 01:33:12.920]   I know I do too.
[01:33:12.920 --> 01:33:13.920]   Most people do.
[01:33:13.920 --> 01:33:16.680]   We're programmed that way, right?
[01:33:16.680 --> 01:33:22.920]   So how realistic is it that you're going to go back to writing letters and sending letters
[01:33:22.920 --> 01:33:26.640]   and just, it's just not realistic.
[01:33:26.640 --> 01:33:31.840]   So I think that there's a lot more to be concerned with.
[01:33:31.840 --> 01:33:33.600]   It's just not very interesting stuff.
[01:33:33.600 --> 01:33:38.240]   It's not as interesting or as exciting as autonomous weapons.
[01:33:38.240 --> 01:33:41.120]   I also feel like it's very difficult to...
[01:33:41.120 --> 01:33:45.680]   I would agree with you, but I'm not sure it's easy to figure out what to think about or
[01:33:45.680 --> 01:33:48.320]   how to think about it.
[01:33:48.320 --> 01:33:50.320]   I don't know if...
[01:33:50.320 --> 01:33:51.960]   I guess we should spend time doing it.
[01:33:51.960 --> 01:33:54.760]   And Elon's funded many efforts to do that.
[01:33:54.760 --> 01:33:56.520]   And I don't fault him for that.
[01:33:56.520 --> 01:34:00.000]   And there's some very rigorous thinking that's going on in that domain.
[01:34:00.000 --> 01:34:03.200]   Yeah, you really wondered, though, how useful it's going to be.
[01:34:03.200 --> 01:34:06.000]   Well, we hope it's very useful, right?
[01:34:06.000 --> 01:34:10.200]   Because the number of scenarios in which things could go spinning out of control or...
[01:34:10.200 --> 01:34:11.400]   The list is long.
[01:34:11.400 --> 01:34:17.200]   And a lot of these scenarios as they kind of get war-gamed are about a lot of people...
[01:34:17.200 --> 01:34:20.400]   About smart people doing the wrong thing for the right reasons.
[01:34:20.400 --> 01:34:25.240]   And unless you think through second and third and fourth order effects, terrible unintended
[01:34:25.240 --> 01:34:30.400]   consequences could ensue as you start designing something that can improve itself and improve
[01:34:30.400 --> 01:34:36.040]   itself at a compounding exponential rate that's much more like the pace of Moore's law than
[01:34:36.040 --> 01:34:37.560]   the pace of evolution.
[01:34:37.560 --> 01:34:42.240]   And so I think that we're gaming these things now and having very, very careful thought
[01:34:42.240 --> 01:34:43.760]   applied to it.
[01:34:43.760 --> 01:34:47.600]   It may not be the perfect machine or it may not be the perfect webman.
[01:34:47.600 --> 01:34:50.760]   I can't think what else we can do right now.
[01:34:50.760 --> 01:34:53.840]   And so I do think it's an important exercise.
[01:34:53.840 --> 01:34:55.880]   Nicholas Bostrom's book is now a little bit old.
[01:34:55.880 --> 01:34:58.000]   It's been about three years since it came out.
[01:34:58.000 --> 01:35:04.280]   But it probably paints the most detailed sets of scenarios of what could go wrong.
[01:35:04.280 --> 01:35:08.280]   And I do think that that has kind of been like the intellectual framework for a lot
[01:35:08.280 --> 01:35:10.560]   of the AI safety work that's going on right now.
[01:35:10.560 --> 01:35:11.560]   It's called Superintelligence.
[01:35:11.560 --> 01:35:12.560]   Yeah, we interviewed Nick.
[01:35:12.560 --> 01:35:13.560]   But you did.
[01:35:13.560 --> 01:35:14.560]   Right.
[01:35:14.560 --> 01:35:15.560]   Yeah, very sharp.
[01:35:15.560 --> 01:35:16.560]   I've...
[01:35:16.560 --> 01:35:20.080]   We've spent a lot of time on triangulation talking to various factions in this.
[01:35:20.080 --> 01:35:21.680]   And there's a lot of debate about it.
[01:35:21.680 --> 01:35:22.680]   Yeah.
[01:35:22.680 --> 01:35:24.560]   I don't think there's anything wrong with thinking about it.
[01:35:24.560 --> 01:35:26.040]   I think it's everything right with thinking about it.
[01:35:26.040 --> 01:35:29.680]   No, I think we've got to play the grounds for this because we're going to look at just
[01:35:29.680 --> 01:35:34.440]   from a societal perspective, just with the technology that we have at the moment, you're
[01:35:34.440 --> 01:35:39.960]   looking at massive employment problems coming down the line in five, 10, 15 years when we
[01:35:39.960 --> 01:35:43.480]   get computers to do things like driving or to a degree.
[01:35:43.480 --> 01:35:45.120]   Head straight for that future.
[01:35:45.120 --> 01:35:46.120]   I understand that.
[01:35:46.120 --> 01:35:47.960]   I just don't know if there's much we could do about it.
[01:35:47.960 --> 01:35:51.440]   Well, we work out the social systems now that we can deal with it.
[01:35:51.440 --> 01:35:53.320]   Oh, you know what's going to happen.
[01:35:53.320 --> 01:35:54.320]   Well, there are two...
[01:35:54.320 --> 01:35:55.320]   There are two ways.
[01:35:55.320 --> 01:35:56.320]   There are two ways.
[01:35:56.320 --> 01:35:57.320]   We're not going to do anything.
[01:35:57.320 --> 01:35:58.320]   It's like the climate crisis.
[01:35:58.320 --> 01:35:59.320]   Yeah.
[01:35:59.320 --> 01:36:02.880]   We're just going to mess around, debate over it, and then not going to do anything until
[01:36:02.880 --> 01:36:04.880]   it hits us overhead, and then it'll be too late.
[01:36:04.880 --> 01:36:07.840]   Yeah, but we've gone through this before with industrialization.
[01:36:07.840 --> 01:36:08.840]   We've gone through it.
[01:36:08.840 --> 01:36:09.840]   Yeah, we've planned for that very well.
[01:36:09.840 --> 01:36:11.160]   To an extent with literacy.
[01:36:11.160 --> 01:36:14.440]   Yeah, and we didn't plan for either of them, but we've got seven billion people on the
[01:36:14.440 --> 01:36:16.040]   planet right now.
[01:36:16.040 --> 01:36:19.560]   You know, and everything is interconnected in such an extent that if society does break
[01:36:19.560 --> 01:36:21.160]   down, you're looking at megadats.
[01:36:21.160 --> 01:36:22.160]   I know.
[01:36:22.160 --> 01:36:24.280]   So we need to sort this out now.
[01:36:24.280 --> 01:36:28.840]   I just don't feel too sanguine about our ability to plan for this future.
[01:36:28.840 --> 01:36:30.680]   You're such a fatalist.
[01:36:30.680 --> 01:36:36.120]   I am a fatalist, but look, I mean, sometimes I feel like certainly our government is in
[01:36:36.120 --> 01:36:42.960]   a different dimension, a different world, and from the world that we're living in.
[01:36:42.960 --> 01:36:48.760]   And anything that they plan to do about anything is irrelevant because they're almost, it's
[01:36:48.760 --> 01:36:49.760]   an impedence mismatch.
[01:36:49.760 --> 01:36:53.160]   They're operating at a different timescale than we are.
[01:36:53.160 --> 01:36:54.160]   I know.
[01:36:54.160 --> 01:36:55.160]   Just go to the DMV.
[01:36:55.160 --> 01:36:56.160]   They're just going to the DMV.
[01:36:56.160 --> 01:37:01.360]   It's like a terrible branding event for government every time a citizen walks into the DMV.
[01:37:01.360 --> 01:37:02.960]   It is just a catastrophic branding event.
[01:37:02.960 --> 01:37:03.960]   That right.
[01:37:03.960 --> 01:37:04.960]   Yes.
[01:37:04.960 --> 01:37:05.960]   And we have it.
[01:37:05.960 --> 01:37:06.960]   How are we going to solve AI?
[01:37:06.960 --> 01:37:07.960]   Yeah.
[01:37:07.960 --> 01:37:11.000]   I've got enough people who have any kind of science or technology background that go into
[01:37:11.000 --> 01:37:12.000]   politics.
[01:37:12.000 --> 01:37:13.000]   Yeah.
[01:37:13.000 --> 01:37:17.600]   We at some point years ago had an office called the Office of Technology Assessment, and it
[01:37:17.600 --> 01:37:18.600]   was staffed with.
[01:37:18.600 --> 01:37:19.600]   Yeah, it was staffed.
[01:37:19.600 --> 01:37:20.600]   It was staffed with the exam.
[01:37:20.600 --> 01:37:26.480]   It was right, who were not political, and it was a teaching exercise.
[01:37:26.480 --> 01:37:30.600]   Their job was to go and meet with members of Congress and help them get educated.
[01:37:30.600 --> 01:37:32.520]   Gingrich defended that in the '90s.
[01:37:32.520 --> 01:37:36.120]   However, that office became the gold standard around the world.
[01:37:36.120 --> 01:37:39.680]   So a bunch of other companies, countries, have that office.
[01:37:39.680 --> 01:37:40.680]   We don't.
[01:37:40.680 --> 01:37:46.720]   And so now, in the absence of the OTA, and with so much change coming so quickly, all
[01:37:46.720 --> 01:37:50.040]   technology gets politicized.
[01:37:50.040 --> 01:37:53.160]   And very few people really understand what it is or how it works.
[01:37:53.160 --> 01:37:57.760]   So the US does not have a national biology policy, and yet we're standing on the precipice
[01:37:57.760 --> 01:38:00.800]   of CRISPR and editing embryos.
[01:38:00.800 --> 01:38:03.600]   And we don't have a national AI policy.
[01:38:03.600 --> 01:38:07.600]   We don't have people who are elected who even understand what AI is, even though they're
[01:38:07.600 --> 01:38:10.960]   talking about technological unemployment and displacement.
[01:38:10.960 --> 01:38:13.040]   I think that's all true.
[01:38:13.040 --> 01:38:15.600]   And I'm not sure that that's the worst possible outcome.
[01:38:15.600 --> 01:38:20.240]   I think a worse outcome would be a government that thinks it knows what's going to happen
[01:38:20.240 --> 01:38:21.240]   in future.
[01:38:21.240 --> 01:38:22.240]   Look at the Stalinist model.
[01:38:22.240 --> 01:38:24.680]   And has a plan for the future.
[01:38:24.680 --> 01:38:26.320]   I think that could be worse.
[01:38:26.320 --> 01:38:28.440]   In some ways, I think it's better.
[01:38:28.440 --> 01:38:31.160]   Maybe I'm a little more laissez-faire.
[01:38:31.160 --> 01:38:37.880]   They just go ahead and worry about transgender bathrooms and let technology happen.
[01:38:37.880 --> 01:38:38.880]   And we'll see what happens.
[01:38:38.880 --> 01:38:43.120]   And there are, I'll say, there are some really talented technologists, thank God, who are
[01:38:43.120 --> 01:38:45.680]   giving back and getting involved in public life.
[01:38:45.680 --> 01:38:48.440]   So there is something called, "We have Brianna Wu on the show.
[01:38:48.440 --> 01:38:50.160]   She's running for Congress in Massachusetts."
[01:38:50.160 --> 01:38:52.800]   I mean, she's certainly technologically illiterate.
[01:38:52.800 --> 01:38:55.360]   There's something called the United States Digital Service who--
[01:38:55.360 --> 01:38:57.120]   We had Matt Cutts on on Wednesday.
[01:38:57.120 --> 01:38:58.120]   We had Matt Cutts on on Wednesday.
[01:38:58.120 --> 01:38:59.120]   Yeah.
[01:38:59.120 --> 01:39:00.120]   Oh, you had just had him on Wednesday.
[01:39:00.120 --> 01:39:01.120]   Yeah.
[01:39:01.120 --> 01:39:02.120]   I was just about to mention Matt.
[01:39:02.120 --> 01:39:03.120]   He's the acting director?
[01:39:03.120 --> 01:39:04.120]   Yes.
[01:39:04.120 --> 01:39:05.120]   Former Googler.
[01:39:05.120 --> 01:39:06.120]   So by the way, I asked him.
[01:39:06.120 --> 01:39:07.120]   You're still doing stuff?
[01:39:07.120 --> 01:39:08.600]   He said, yeah, even in the Trump administration.
[01:39:08.600 --> 01:39:09.600]   Yeah, you're not been defunded?
[01:39:09.600 --> 01:39:10.600]   No.
[01:39:10.600 --> 01:39:11.840]   Yeah, no, he re-opt.
[01:39:11.840 --> 01:39:14.880]   He was wondering whether or not there was going to be a place for him after the administration
[01:39:14.880 --> 01:39:17.440]   changed, but he is still there, and he's doing incredible work.
[01:39:17.440 --> 01:39:19.840]   I think he's one of the first 50 people at Google.
[01:39:19.840 --> 01:39:23.880]   I know he was Matt at Google.com, which I thought was a really cool email address.
[01:39:23.880 --> 01:39:24.880]   So that you've got to be--
[01:39:24.880 --> 01:39:27.760]   Your first name at Google, you were there mighty early.
[01:39:27.760 --> 01:39:30.400]   So there are some folks that are out there and--
[01:39:30.400 --> 01:39:31.880]   But there's a bunch who aren't.
[01:39:31.880 --> 01:39:35.520]   The OSTP is basically blood dry of people too.
[01:39:35.520 --> 01:39:36.520]   Because that's sad.
[01:39:36.520 --> 01:39:37.520]   Yeah, that's so sad.
[01:39:37.520 --> 01:39:38.520]   Yeah.
[01:39:38.520 --> 01:39:43.480]   I don't-- and again, I don't want anybody to walk away with the idea that I think that
[01:39:43.480 --> 01:39:45.240]   the government should be in charge of everything.
[01:39:45.240 --> 01:39:47.600]   I don't think that at all.
[01:39:47.600 --> 01:39:49.400]   And I am not myself political.
[01:39:49.400 --> 01:39:52.240]   I'm not Democrat or Republican.
[01:39:52.240 --> 01:39:53.440]   I'm independent.
[01:39:53.440 --> 01:39:58.600]   However, I want people who are making decisions on behalf of our collective future to be
[01:39:58.600 --> 01:39:59.600]   informed.
[01:39:59.600 --> 01:40:04.760]   I don't care what party they belong to as long as they believe in science and data,
[01:40:04.760 --> 01:40:08.360]   and they're not politicizing technology to the point where our laws don't make any
[01:40:08.360 --> 01:40:09.360]   sense anymore.
[01:40:09.360 --> 01:40:10.360]   It would be nice to--
[01:40:10.360 --> 01:40:11.360]   And to that end, yeah.
[01:40:11.360 --> 01:40:13.080]   I mean, and that's-- it's our country.
[01:40:13.080 --> 01:40:14.080]   It's England.
[01:40:14.080 --> 01:40:17.600]   It's-- you know, it's-- this is true of democracies all over the world.
[01:40:17.600 --> 01:40:19.440]   We're all facing the same problem right now.
[01:40:19.440 --> 01:40:20.440]   Oh, no.
[01:40:20.440 --> 01:40:23.880]   I mean, in the case of my-- the country of my birth, we've had Amber Rudd over here
[01:40:23.880 --> 01:40:27.240]   this week chatting to Silicon Valley people, and she's got this--
[01:40:27.240 --> 01:40:28.240]   Fantastic idea.
[01:40:28.240 --> 01:40:31.120]   Well, we're just going to have to mandate back doors, and it's up to you guys to--
[01:40:31.120 --> 01:40:32.120]   Figure it out.
[01:40:32.120 --> 01:40:33.120]   --in encryption.
[01:40:33.120 --> 01:40:34.120]   You figure it out.
[01:40:34.120 --> 01:40:35.120]   You figure it out.
[01:40:35.120 --> 01:40:36.120]   You're the Wiz kids.
[01:40:36.120 --> 01:40:37.120]   And you're like, OK.
[01:40:37.120 --> 01:40:38.120]   It's math.
[01:40:38.120 --> 01:40:40.840]   It doesn't really-- you know, there's not a lot of wiggle root.
[01:40:40.840 --> 01:40:44.360]   I love Malcolm-- Malcolm Turnbull, the prime minister of Australia said--
[01:40:44.360 --> 01:40:45.360]   Oh, good law.
[01:40:45.360 --> 01:40:46.360]   --the laws of America.
[01:40:46.360 --> 01:40:47.360]   Yeah.
[01:40:47.360 --> 01:40:48.360]   Supercede the laws of mathematics.
[01:40:48.360 --> 01:40:49.360]   [LAUGHTER]
[01:40:49.360 --> 01:40:50.360]   This is like--
[01:40:50.360 --> 01:40:52.360]   I thought we said that as a joke.
[01:40:52.360 --> 01:40:53.360]   No.
[01:40:53.360 --> 01:40:54.360]   He was actually serious.
[01:40:54.360 --> 01:40:55.360]   The laws of Austria--
[01:40:55.360 --> 01:40:56.360]   But that's a weed.
[01:40:56.360 --> 01:40:57.360]   --and supercede the laws of nature.
[01:40:57.360 --> 01:40:58.360]   The laws of--
[01:40:58.360 --> 01:40:59.360]   --of my master's.
[01:40:59.360 --> 01:41:00.360]   --of my master's.
[01:41:00.360 --> 01:41:03.240]   Now, if that's the case, well, why not just take a walk out that window and see how
[01:41:03.240 --> 01:41:04.800]   you do against the laws of gravity?
[01:41:04.800 --> 01:41:05.800]   Yeah.
[01:41:05.800 --> 01:41:06.800]   Because, you know, these things are immutable.
[01:41:06.800 --> 01:41:09.480]   And it's always one new ways of doing it.
[01:41:09.480 --> 01:41:11.440]   And there's the story from the Telegraph.
[01:41:11.440 --> 01:41:15.080]   Malcolm Turnbull says, well, I'll get the quote here.
[01:41:15.080 --> 01:41:17.920]   And this is back down to encrypted messages once again.
[01:41:17.920 --> 01:41:18.920]   Which was a--
[01:41:18.920 --> 01:41:19.920]   --with a reds issue, right?
[01:41:19.920 --> 01:41:20.920]   Yeah.
[01:41:20.920 --> 01:41:21.920]   That's what she was talking about.
[01:41:21.920 --> 01:41:25.640]   On challenge one, whether it's possible to fully crack down on encryption, saying the
[01:41:25.640 --> 01:41:30.040]   laws of Australia-- well, quote, the laws of Australia prevail in Australia.
[01:41:30.040 --> 01:41:31.240]   I can assure you of that.
[01:41:31.240 --> 01:41:34.000]   The laws of mathematics are very commendable.
[01:41:34.000 --> 01:41:37.320]   But the only law that applies in Australia is the law of Australia.
[01:41:37.320 --> 01:41:38.320]   Well, you know, they have--
[01:41:38.320 --> 01:41:39.320]   --they have--
[01:41:39.320 --> 01:41:40.320]   --is not strictly true.
[01:41:40.320 --> 01:41:42.760]   They have done a very good job of evading the laws of gravity.
[01:41:42.760 --> 01:41:43.760]   Of physics, yeah.
[01:41:43.760 --> 01:41:46.760]   --on the bottom side of the planet there.
[01:41:46.760 --> 01:41:47.760]   So maybe--
[01:41:47.760 --> 01:41:48.760]   Let's take a break.
[01:41:48.760 --> 01:41:49.760]   --when you look at the globe.
[01:41:49.760 --> 01:41:50.760]   Oh, god, this is--
[01:41:50.760 --> 01:41:51.760]   --Gee, please.
[01:41:51.760 --> 01:41:56.480]   --I actually have-- I have, at great expense, imported a member of the eye gen.
[01:41:56.480 --> 01:41:57.480]   She is here.
[01:41:57.480 --> 01:41:59.240]   We're going to talk in just a second to Piper Reese.
[01:41:59.240 --> 01:42:00.240]   She is very famous.
[01:42:00.240 --> 01:42:01.680]   I'm going to put you on, Piper.
[01:42:01.680 --> 01:42:03.480]   I know this is a surprise.
[01:42:03.480 --> 01:42:07.760]   But we want to talk about your generation and how screwed up it is, if you don't mind.
[01:42:07.760 --> 01:42:10.560]   Blame your phone.
[01:42:10.560 --> 01:42:13.160]   She started on YouTube as a YouTuber when you were seven, right?
[01:42:13.160 --> 01:42:14.480]   10 years ago, she's 17.
[01:42:14.480 --> 01:42:18.040]   Now she's up here interviewing to go to Stanford.
[01:42:18.040 --> 01:42:22.600]   And I thought, well, here's a perfect example of somebody who grew up in the iPhone era.
[01:42:22.600 --> 01:42:25.920]   And let's find out if the eye gen really is all that screwed up.
[01:42:25.920 --> 01:42:26.920]   OK, not yet.
[01:42:26.920 --> 01:42:28.560]   We're going to go to a commercial break.
[01:42:28.560 --> 01:42:30.560]   We'll figure out how to-- Mike Amber.
[01:42:30.560 --> 01:42:33.400]   And she can find a place at the table or something like that.
[01:42:33.400 --> 01:42:35.880]   Just a brief-- just a brief one, Piper.
[01:42:35.880 --> 01:42:36.880]   Not a long one.
[01:42:36.880 --> 01:42:37.880]   No, Amber Red is not here.
[01:42:37.880 --> 01:42:40.120]   I got confused for a second.
[01:42:40.120 --> 01:42:41.120]   It's good to say.
[01:42:41.120 --> 01:42:42.120]   I'll show you.
[01:42:42.120 --> 01:42:44.040]   In some way, she probably couldn't even open the door.
[01:42:44.040 --> 01:42:45.800]   She's not clued to the plot.
[01:42:45.800 --> 01:42:50.760]   The laws of Australia supersede the laws of mathematics.
[01:42:50.760 --> 01:42:51.760]   And gravity.
[01:42:51.760 --> 01:42:52.760]   And gravity.
[01:42:52.760 --> 01:42:55.400]   So I mean, just push them out of an airplane.
[01:42:55.400 --> 01:42:56.400]   See what happens.
[01:42:56.400 --> 01:42:58.160]   No, I shouldn't say that because that's probably--
[01:42:58.160 --> 01:42:59.160]   That's going to be a lot of--
[01:42:59.160 --> 01:43:00.160]   [INTERPOSING VOICES]
[01:43:00.160 --> 01:43:01.160]   Yeah, yeah.
[01:43:01.160 --> 01:43:03.360]   I hope you've seen the Sydney Opera House already.
[01:43:03.360 --> 01:43:05.360]   They'll lead-- I wouldn't go there now.
[01:43:05.360 --> 01:43:07.360]   No, no, I love you, Malcolm.
[01:43:07.360 --> 01:43:08.360]   You're great.
[01:43:08.360 --> 01:43:10.480]   They'll peg you out in the outback and drop bears on you
[01:43:10.480 --> 01:43:11.600]   until you die, yes.
[01:43:11.600 --> 01:43:12.600]   I love Australia.
[01:43:12.600 --> 01:43:13.360]   I do.
[01:43:13.360 --> 01:43:15.040]   And I love the laws of mathematics.
[01:43:15.040 --> 01:43:18.040]   So somehow, I have to make those work together.
[01:43:18.040 --> 01:43:19.800]   Our show today brought to you by Casper,
[01:43:19.800 --> 01:43:22.600]   the online retailer of fabulous mattresses
[01:43:22.600 --> 01:43:24.200]   at a fraction of the price.
[01:43:24.200 --> 01:43:27.560]   Because everyone deserves a good night's sleep.
[01:43:27.560 --> 01:43:31.720]   The Casper mattress was designed by 20 engineers,
[01:43:31.720 --> 01:43:34.720]   perfected by a community of half a million sleepers.
[01:43:34.720 --> 01:43:39.040]   It is now the mattress for you.
[01:43:39.040 --> 01:43:41.760]   But you may say, but Leo, how do I know that?
[01:43:41.760 --> 01:43:43.600]   I want to go lie on it before I buy it.
[01:43:43.600 --> 01:43:44.480]   You can't.
[01:43:44.480 --> 01:43:45.640]   You have to buy it first.
[01:43:45.640 --> 01:43:46.840]   But here's the beauty part.
[01:43:46.840 --> 01:43:48.480]   You can lie on it for 100 nights.
[01:43:48.480 --> 01:43:51.400]   And at any time, if at any time you do not agree with me,
[01:43:51.400 --> 01:43:53.520]   this is the best mattress you have ever had.
[01:43:53.520 --> 01:43:54.640]   You call Casper.
[01:43:54.640 --> 01:43:55.680]   They'll give you a full refund.
[01:43:55.680 --> 01:43:56.200]   They'll come.
[01:43:56.200 --> 01:43:57.400]   They'll take the mattress.
[01:43:57.400 --> 01:43:58.600]   That's that.
[01:43:58.600 --> 01:44:01.960]   So in effect, you get 100 nights to try before you buy.
[01:44:01.960 --> 01:44:03.600]   Casper is amazing.
[01:44:03.600 --> 01:44:07.320]   They've combined pressure relieving, supportive memory
[01:44:07.320 --> 01:44:10.520]   foam with a breathable open cell layer.
[01:44:10.520 --> 01:44:12.720]   For all night, Comfort Time Magazine
[01:44:12.720 --> 01:44:17.040]   said one of the best 25 inventions of 2015.
[01:44:17.040 --> 01:44:19.440]   Fast Company said one of the most innovative companies
[01:44:19.440 --> 01:44:24.120]   of 2017, big innovation award 2016.
[01:44:24.120 --> 01:44:25.400]   Because it's a great mattress.
[01:44:25.400 --> 01:44:26.320]   I sleep on a Casper.
[01:44:26.320 --> 01:44:27.000]   I love it.
[01:44:27.000 --> 01:44:29.880]   Fast Company probably sleeps on it too.
[01:44:29.880 --> 01:44:33.360]   That's why they call it the most innovative brand of 2017.
[01:44:33.360 --> 01:44:35.120]   Casper mattresses are delivered to your door
[01:44:35.120 --> 01:44:37.040]   in a surprisingly compact box.
[01:44:37.040 --> 01:44:38.360]   It's very easy.
[01:44:38.360 --> 01:44:42.040]   I sent one to my son in college and they just bring it upstairs.
[01:44:42.040 --> 01:44:45.040]   Free shipping and returns to the US, Canada, and the UK now,
[01:44:45.040 --> 01:44:47.000]   by the way, Ian.
[01:44:47.000 --> 01:44:50.360]   Considering you spent about a third of your life on a mattress,
[01:44:50.360 --> 01:44:54.240]   it's important you truly sleep on it before committing.
[01:44:54.240 --> 01:44:56.560]   That's why Casper gives you 100 nights to try out.
[01:44:56.560 --> 01:44:58.240]   The best mattress ever.
[01:44:58.240 --> 01:45:00.240]   And by the way, on these hot summer nights,
[01:45:00.240 --> 01:45:02.080]   you'll be glad the Casper breathes.
[01:45:02.080 --> 01:45:03.640]   It's very-- it sleeps cool.
[01:45:03.640 --> 01:45:04.960]   That's really important.
[01:45:04.960 --> 01:45:06.840]   And I don't know how they do it.
[01:45:06.840 --> 01:45:09.280]   I've had other mattresses where you have to let them air out
[01:45:09.280 --> 01:45:11.040]   for a month.
[01:45:11.040 --> 01:45:12.160]   Not the Casper.
[01:45:12.160 --> 01:45:14.480]   Out of the box, fresh is a daisy.
[01:45:14.480 --> 01:45:15.120]   Wonderful.
[01:45:15.120 --> 01:45:18.120]   It is a fabulous night's sleep.
[01:45:18.120 --> 01:45:21.600]   Get $50 towards your mattress purchase at Casper.com/twit.
[01:45:21.600 --> 01:45:24.680]   Promocode to it, Casper.com/twit.
[01:45:24.680 --> 01:45:30.200]   Use the promo code, "twit," terms and conditions, apply $50 off
[01:45:30.200 --> 01:45:34.600]   at Casper.com/twit.
[01:45:34.600 --> 01:45:37.600]   How can we get Piper on--
[01:45:37.600 --> 01:45:39.360]   Would you mind letting her--
[01:45:39.360 --> 01:45:41.800]   You can lean over my shoulder, share my mic.
[01:45:41.800 --> 01:45:43.520]   Do you mind, Piper?
[01:45:43.520 --> 01:45:44.960]   So did you see this article?
[01:45:44.960 --> 01:45:46.760]   Come on and have a see here.
[01:45:46.760 --> 01:45:47.640]   Piper Reese is here.
[01:45:47.640 --> 01:45:49.280]   Piper's picks on YouTube.
[01:45:49.280 --> 01:45:51.240]   Congratulations on your success on YouTube.
[01:45:51.240 --> 01:45:51.800]   Thank you so much.
[01:45:51.800 --> 01:45:53.680]   You've been doing this for 10 years?
[01:45:53.680 --> 01:45:55.640]   Yeah, since I was seven.
[01:45:55.640 --> 01:45:56.800]   That's incredible.
[01:45:56.800 --> 01:45:59.000]   There she is as a seven-year-old.
[01:45:59.000 --> 01:46:01.280]   You've interviewed all the biggest names in Hollywood.
[01:46:01.280 --> 01:46:02.280]   Yep.
[01:46:02.280 --> 01:46:05.880]   You're the iPhone generation, the YouTube generation.
[01:46:05.880 --> 01:46:07.000]   Did you read this article?
[01:46:07.000 --> 01:46:09.000]   Probably not, because you guys don't read.
[01:46:09.000 --> 01:46:09.720]   No, we don't.
[01:46:09.720 --> 01:46:12.400]   We only read BuzzFeed, so--
[01:46:12.400 --> 01:46:13.560]   Well, this is not BuzzFeed.
[01:46:13.560 --> 01:46:14.320]   This is the Atlantic.
[01:46:14.320 --> 01:46:18.240]   Have smartphones destroyed a generation.
[01:46:18.240 --> 01:46:19.400]   And you tell me what you think.
[01:46:19.400 --> 01:46:21.920]   Obviously, you're not one of these people.
[01:46:21.920 --> 01:46:24.640]   And no one's saying that everybody that grew up
[01:46:24.640 --> 01:46:28.480]   with the iPhone came out in 2007 when you were seven.
[01:46:28.480 --> 01:46:29.400]   Right?
[01:46:29.400 --> 01:46:29.920]   Right?
[01:46:29.920 --> 01:46:31.040]   This is your whole life.
[01:46:31.040 --> 01:46:32.480]   Have you had an iPhone your whole life?
[01:46:32.480 --> 01:46:33.240]   No, I haven't.
[01:46:33.240 --> 01:46:36.200]   I actually was not allowed to have a phone till eighth grade.
[01:46:36.200 --> 01:46:37.800]   Oh my god, how did you live?
[01:46:37.800 --> 01:46:39.000]   Yeah, I mean, I don't know.
[01:46:39.000 --> 01:46:39.840]   Was my name--
[01:46:39.840 --> 01:46:41.160]   Do you Snapchat now?
[01:46:41.160 --> 01:46:41.960]   Yes, I do.
[01:46:41.960 --> 01:46:45.000]   I mean, I have the Piper's pick Snapchat that I do.
[01:46:45.000 --> 01:46:46.280]   Yeah, it seems a little different.
[01:46:46.280 --> 01:46:49.160]   You're creating content for your peers.
[01:46:49.160 --> 01:46:50.120]   Is this your dad?
[01:46:50.120 --> 01:46:50.480]   Yes.
[01:46:50.480 --> 01:46:51.280]   Smart man.
[01:46:51.280 --> 01:46:53.000]   You didn't let her use an iPhone that kept--
[01:46:53.000 --> 01:46:55.400]   So here's the stats that are in this article.
[01:46:55.400 --> 01:46:57.800]   I just want to know what you think of it.
[01:46:57.800 --> 01:46:59.640]   A smartphone generation.
[01:46:59.640 --> 01:47:03.920]   So after the iPhone came out, that's that bar right here.
[01:47:03.920 --> 01:47:06.640]   The amount of time teenagers spent out
[01:47:06.640 --> 01:47:10.080]   without their parents, just with their friends, plummeted.
[01:47:10.080 --> 01:47:11.920]   Eighth graders are pink.
[01:47:11.920 --> 01:47:13.040]   Tenth graders are green.
[01:47:13.040 --> 01:47:14.680]   Twelfth graders are a little higher.
[01:47:14.680 --> 01:47:16.800]   But you see, it's falling dramatically,
[01:47:16.800 --> 01:47:18.760]   especially with the younger generation.
[01:47:18.760 --> 01:47:20.160]   Do you hang out with your friends?
[01:47:20.160 --> 01:47:20.760]   I do.
[01:47:20.760 --> 01:47:21.960]   Go to the mall with your friends.
[01:47:21.960 --> 01:47:24.520]   Yeah, I do go to the mall with my friends a lot.
[01:47:24.520 --> 01:47:27.320]   I think that specifically is really interesting to me,
[01:47:27.320 --> 01:47:29.960]   because in my case, it's just because I can't get out a lot.
[01:47:29.960 --> 01:47:31.360]   I don't have a car yet.
[01:47:31.360 --> 01:47:31.880]   So--
[01:47:31.880 --> 01:47:33.800]   Ah, this is the other one.
[01:47:33.800 --> 01:47:35.560]   Percentage of 12th graders who drive.
[01:47:35.560 --> 01:47:36.360]   That makes sense.
[01:47:36.360 --> 01:47:37.200]   Plumbited.
[01:47:37.200 --> 01:47:37.480]   Yeah.
[01:47:37.480 --> 01:47:38.680]   Do you have a driver's license?
[01:47:38.680 --> 01:47:40.000]   I mean, no, not yet.
[01:47:40.000 --> 01:47:42.120]   I am about to try for my permit.
[01:47:42.120 --> 01:47:44.600]   So when you said the DMV thing, that was interesting to me.
[01:47:44.600 --> 01:47:45.560]   My daughter's here.
[01:47:45.560 --> 01:47:46.960]   She's 25.
[01:47:46.960 --> 01:47:49.960]   When she was 15 and 1/2, she said,
[01:47:49.960 --> 01:47:50.720]   I'm going to DMV.
[01:47:50.720 --> 01:47:52.560]   I'm getting the driver's license.
[01:47:52.560 --> 01:47:54.520]   She cried when she failed the test first time.
[01:47:54.520 --> 01:47:54.920]   I'm sorry.
[01:47:54.920 --> 01:47:57.520]   But you were very-- you really were excited about driving.
[01:47:57.520 --> 01:47:59.560]   My son was just a couple of years older.
[01:47:59.560 --> 01:48:02.480]   He didn't get his license until he was 18.
[01:48:02.480 --> 01:48:03.120]   He's 22.
[01:48:03.120 --> 01:48:05.120]   So I think that this is true, right?
[01:48:05.120 --> 01:48:06.480]   This falling?
[01:48:06.480 --> 01:48:06.840]   Yeah.
[01:48:06.840 --> 01:48:08.520]   I mean, that makes a lot of sense to me,
[01:48:08.520 --> 01:48:11.000]   because I feel like we can talk to our friends over the phone
[01:48:11.000 --> 01:48:12.320]   or we can text our friends.
[01:48:12.320 --> 01:48:14.000]   So the rush to drive isn't as much.
[01:48:14.000 --> 01:48:16.880]   And then also with phones, that is when Uber started
[01:48:16.880 --> 01:48:17.920]   and things like that.
[01:48:17.920 --> 01:48:19.840]   So the question is, do we really need
[01:48:19.840 --> 01:48:21.840]   to be able to drive when I can just be like, oh, I'll
[01:48:21.840 --> 01:48:23.760]   get an Uber here in five minutes.
[01:48:23.760 --> 01:48:25.920]   But also, do you need to be physically present in order
[01:48:25.920 --> 01:48:26.920]   to get on the mic here?
[01:48:26.920 --> 01:48:28.040]   Oh, that one's not on.
[01:48:28.040 --> 01:48:29.400]   Can you turn that on?
[01:48:29.400 --> 01:48:30.120]   We gave him a mic.
[01:48:30.120 --> 01:48:31.840]   Look at that, isn't that nice?
[01:48:31.840 --> 01:48:32.240]   There it is.
[01:48:32.240 --> 01:48:33.000]   Go ahead.
[01:48:33.000 --> 01:48:35.200]   Well, so I mean, you don't need to be physically present
[01:48:35.200 --> 01:48:37.000]   with your friends anymore, because you can chat with them
[01:48:37.000 --> 01:48:39.920]   online or do you get the same thing out of chatting online
[01:48:39.920 --> 01:48:41.760]   with friends you do with meeting physically?
[01:48:41.760 --> 01:48:44.880]   I mean, in a way, yes, like my friends and I text a lot.
[01:48:44.880 --> 01:48:47.280]   I mean, we use FaceTime sometimes, stuff like that.
[01:48:47.280 --> 01:48:50.760]   But for me personally, I like being with people.
[01:48:50.760 --> 01:48:51.920]   Oh, you're so weird.
[01:48:51.920 --> 01:48:52.360]   I'm sorry.
[01:48:52.360 --> 01:48:54.160]   [LAUGHTER]
[01:48:54.160 --> 01:48:56.200]   I'm really busy now.
[01:48:56.200 --> 01:48:58.640]   You're probably not a good dating.
[01:48:58.640 --> 01:49:00.120]   Again, the line goes down.
[01:49:00.120 --> 01:49:00.960]   I'm dating one down.
[01:49:00.960 --> 01:49:01.840]   That's interesting.
[01:49:01.840 --> 01:49:02.160]   Dating.
[01:49:02.160 --> 01:49:05.840]   Well, but Abby, Abby, you dated as a teenager.
[01:49:05.840 --> 01:49:08.400]   But Henry was always going out with a group of friends,
[01:49:08.400 --> 01:49:09.200]   right?
[01:49:09.200 --> 01:49:11.080]   And it was much more of a kind of a hookup scene
[01:49:11.080 --> 01:49:13.120]   than it was like a dating scene.
[01:49:13.120 --> 01:49:14.280]   Like that now, too.
[01:49:14.280 --> 01:49:15.080]   It is, isn't it?
[01:49:15.080 --> 01:49:16.000]   I'll skip the sex part.
[01:49:16.000 --> 01:49:17.000]   I don't want to embarrass you.
[01:49:17.000 --> 01:49:18.600]   More likely to feel lonely.
[01:49:18.600 --> 01:49:19.800]   I'm just sitting just there.
[01:49:19.800 --> 01:49:20.320]   Yes.
[01:49:20.320 --> 01:49:21.520]   I don't want to embarrass you.
[01:49:21.520 --> 01:49:24.240]   Percentage of 8th, 10th, and 12th graders who agree with
[01:49:24.240 --> 01:49:25.440]   or mostly agree with the statement,
[01:49:25.440 --> 01:49:27.280]   I often feel left out of things.
[01:49:27.280 --> 01:49:28.440]   And a lot of time, I feel lonely.
[01:49:28.440 --> 01:49:29.720]   Look how that's gone straight up.
[01:49:29.720 --> 01:49:33.200]   OK, the Snapchat map totally made that so much worse,
[01:49:33.200 --> 01:49:35.040]   because I know a lot of my friends are talking about,
[01:49:35.040 --> 01:49:36.920]   like, oh, I can see when my friends are hanging out
[01:49:36.920 --> 01:49:37.560]   without me now.
[01:49:37.560 --> 01:49:39.320]   So people are going to feel more lonely.
[01:49:39.320 --> 01:49:41.840]   Or when people are posting with one friend on Instagram,
[01:49:41.840 --> 01:49:44.400]   then you see, oh, my friend hung out with that person
[01:49:44.400 --> 01:49:45.320]   and didn't invite me.
[01:49:45.320 --> 01:49:46.560]   Oh, so you--
[01:49:46.560 --> 01:49:47.760]   A lot of that.
[01:49:47.760 --> 01:49:50.600]   I have to say, this is true for adults, too.
[01:49:50.600 --> 01:49:52.640]   A lot of social media is depressing.
[01:49:52.640 --> 01:49:53.160]   Yeah.
[01:49:53.160 --> 01:49:56.080]   Like there's the FOMO, the fear of missing out.
[01:49:56.080 --> 01:49:58.160]   Well, there have been psychological studies about this,
[01:49:58.160 --> 01:50:00.200]   because when people post on social media,
[01:50:00.200 --> 01:50:01.800]   they only post the good stuff.
[01:50:01.800 --> 01:50:02.280]   Exactly.
[01:50:02.280 --> 01:50:04.280]   They don't say, work up with a hangover,
[01:50:04.280 --> 01:50:05.480]   I'm feeling like death.
[01:50:05.480 --> 01:50:07.760]   My girlfriend's just had a massive argument with me.
[01:50:07.760 --> 01:50:09.480]   You know, you don't want to put that stuff online,
[01:50:09.480 --> 01:50:11.560]   but it's just like, how do great day at work?
[01:50:11.560 --> 01:50:12.080]   You know?
[01:50:12.080 --> 01:50:12.800]   OK, yes.
[01:50:12.800 --> 01:50:15.040]   But a lot of teenagers have accounts
[01:50:15.040 --> 01:50:17.840]   that adults don't necessarily see so much that are more
[01:50:17.840 --> 01:50:21.520]   of the private accounts that have stuff that is more depressing,
[01:50:21.520 --> 01:50:24.680]   so they can vent and get it out that people haven't seen as much.
[01:50:24.680 --> 01:50:26.320]   So that does exist.
[01:50:26.320 --> 01:50:27.120]   And that is--
[01:50:27.120 --> 01:50:27.560]   Did you watch that?
[01:50:27.560 --> 01:50:27.800]   What is it?
[01:50:27.800 --> 01:50:28.440]   12 reasons?
[01:50:28.440 --> 01:50:28.920]   Why?
[01:50:28.920 --> 01:50:29.440]   Very--
[01:50:29.440 --> 01:50:30.600]   13 reasons why.
[01:50:30.600 --> 01:50:32.320]   I started it, then I wasn't allowed to watch it,
[01:50:32.320 --> 01:50:34.960]   because of all the reviews on it, which I understand.
[01:50:34.960 --> 01:50:35.480]   I get out--
[01:50:35.480 --> 01:50:37.720]   It's about a young girl who ends up committing suicide.
[01:50:37.720 --> 01:50:39.040]   It's about some teenage teenage suicide.
[01:50:39.040 --> 01:50:41.720]   And I think that triggered a lot for a lot of people.
[01:50:41.720 --> 01:50:44.440]   And it obviously has caused some really bad things.
[01:50:44.440 --> 01:50:48.040]   And I think with that, I don't think it was as much as causing it.
[01:50:48.040 --> 01:50:51.040]   I think it was people relating to it and saying, oh, if she tried it,
[01:50:51.040 --> 01:50:52.480]   maybe I should try it too.
[01:50:52.480 --> 01:50:54.160]   Do you get enough sleep?
[01:50:54.160 --> 01:50:56.080]   No, not at all, because I mean--
[01:50:56.080 --> 01:50:58.120]   I don't know any teenager who gets enough sleep, though.
[01:50:58.120 --> 01:50:58.760]   That's be fair.
[01:50:58.760 --> 01:50:59.320]   I never--
[01:50:59.320 --> 01:51:00.960]   You're supposed to get nine to 10 hours.
[01:51:00.960 --> 01:51:02.200]   Like, that's not going to happen.
[01:51:02.200 --> 01:51:03.160]   So here's the graph, though.
[01:51:03.160 --> 01:51:03.560]   Look at this.
[01:51:03.560 --> 01:51:07.480]   And one of the theories is that teenagers take their phone--
[01:51:07.480 --> 01:51:08.800]   do you take your phone to bed with you?
[01:51:08.800 --> 01:51:09.640]   Always.
[01:51:09.640 --> 01:51:12.080]   I am on my phone till 4 AM this summer.
[01:51:12.080 --> 01:51:14.040]   We have to take our 14-year-old's phone away,
[01:51:14.040 --> 01:51:16.560]   because he will stay up almost all night,
[01:51:16.560 --> 01:51:18.360]   talking to friends, chatting, playing games.
[01:51:18.360 --> 01:51:20.200]   You're talking to my dad that idea.
[01:51:20.200 --> 01:51:20.800]   You actually--
[01:51:20.800 --> 01:51:22.640]   You actually just did.
[01:51:22.640 --> 01:51:23.320]   What do you do, Piper?
[01:51:23.320 --> 01:51:24.240]   Do you talk?
[01:51:24.240 --> 01:51:28.080]   OK, it depends on who I'm talking to, or I mean, even texting,
[01:51:28.080 --> 01:51:29.920]   because it really depends on the person.
[01:51:29.920 --> 01:51:32.800]   I mean, I prefer to be talking to someone on the phone
[01:51:32.800 --> 01:51:33.920]   if I have enough to talk about.
[01:51:33.920 --> 01:51:35.200]   But if it's been a really boring day
[01:51:35.200 --> 01:51:36.680]   and I have nothing to discuss, I'll text
[01:51:36.680 --> 01:51:38.280]   so I have more time to think of stuff.
[01:51:38.280 --> 01:51:40.360]   That's also probably part of it with dating,
[01:51:40.360 --> 01:51:43.120]   is it's easier to text a guy than it is to call a guy.
[01:51:43.120 --> 01:51:45.440]   So if you have that option, you have more time to think,
[01:51:45.440 --> 01:51:47.520]   and you don't have to be nervous and on your feet,
[01:51:47.520 --> 01:51:49.840]   which is probably why dating's gone down.
[01:51:49.840 --> 01:51:52.000]   Wow.
[01:51:52.000 --> 01:51:55.800]   You know, if you don't date in person,
[01:51:55.800 --> 01:51:57.640]   it's not going to go well in the future.
[01:51:57.640 --> 01:51:59.120]   You've got to, at some point--
[01:51:59.120 --> 01:52:01.960]   It might take a while for people to realize.
[01:52:01.960 --> 01:52:03.360]   It's just easier.
[01:52:03.360 --> 01:52:05.440]   I think you probably are not a great example.
[01:52:05.440 --> 01:52:07.560]   You're obviously very well-spoken, outgoing,
[01:52:07.560 --> 01:52:11.680]   and you have a YouTube channel, and you obviously--
[01:52:11.680 --> 01:52:13.720]   Do you--
[01:52:13.720 --> 01:52:15.480]   There's a thing in Snapchat that I didn't know about
[01:52:15.480 --> 01:52:17.960]   where you go for a streak, like how many days
[01:52:17.960 --> 01:52:18.600]   you've posted--
[01:52:18.600 --> 01:52:18.960]   Snap streaks.
[01:52:18.960 --> 01:52:19.960]   Snap streaks.
[01:52:19.960 --> 01:52:20.680]   Do you pay attention to that?
[01:52:20.680 --> 01:52:21.680]   Your friends pay attention to that?
[01:52:21.680 --> 01:52:23.520]   I mean, I think my friends pay attention to it.
[01:52:23.520 --> 01:52:25.240]   I don't really do it because I'm not really
[01:52:25.240 --> 01:52:27.280]   going to do that on my Piper's Fix stuff.
[01:52:27.280 --> 01:52:29.680]   Just be like, oh, yeah, I'm going to streak Piper's Fix stuff.
[01:52:29.680 --> 01:52:31.480]   Because you post every day, I bet.
[01:52:31.480 --> 01:52:33.360]   I mean, I think we post most days.
[01:52:33.360 --> 01:52:35.680]   We kind of switched over to Instagram Stories for the most.
[01:52:35.680 --> 01:52:37.120]   Oh, there's another problem.
[01:52:37.120 --> 01:52:38.240]   Yeah.
[01:52:38.240 --> 01:52:40.280]   Oh, we'll talk about that.
[01:52:40.280 --> 01:52:41.120]   Yeah.
[01:52:41.120 --> 01:52:43.800]   All right, well, I thought if we're going to--
[01:52:43.800 --> 01:52:44.480]   how old are you?
[01:52:44.480 --> 01:52:45.200]   That's so cute.
[01:52:45.200 --> 01:52:46.200]   Is that you?
[01:52:46.200 --> 01:52:48.040]   Yeah, I mean, that's actually not that old.
[01:52:48.040 --> 01:52:48.640]   I just--
[01:52:48.640 --> 01:52:50.040]   Looks like it's last year.
[01:52:50.040 --> 01:52:52.280]   I changed my hair a lot this year.
[01:52:52.280 --> 01:52:53.760]   So that's really old.
[01:52:53.760 --> 01:52:55.760]   You're growing up, Piper.
[01:52:55.760 --> 01:52:56.800]   Aw.
[01:52:56.800 --> 01:52:58.520]   Piper, thank you for joining us.
[01:52:58.520 --> 01:53:00.440]   Piper's Fix.tv.
[01:53:00.440 --> 01:53:01.920]   And we're going to talk after the show, right?
[01:53:01.920 --> 01:53:02.760]   Yeah, thank you.
[01:53:02.760 --> 01:53:03.800]   All right, and you're--
[01:53:03.800 --> 01:53:04.600]   How did it go at Stanford?
[01:53:04.600 --> 01:53:05.240]   Did you have fun?
[01:53:05.240 --> 01:53:06.920]   Yeah, it was a really awesome campus.
[01:53:06.920 --> 01:53:08.360]   Yeah, it's very nice.
[01:53:08.360 --> 01:53:09.600]   Yeah, it's the bomb.
[01:53:09.600 --> 01:53:10.560]   It's the bomb.
[01:53:10.560 --> 01:53:11.400]   Thank you.
[01:53:11.400 --> 01:53:12.160]   Piper Reese.
[01:53:12.160 --> 01:53:12.600]   Thank you.
[01:53:12.600 --> 01:53:14.920]   Amy, did you want to ask Piper any questions?
[01:53:14.920 --> 01:53:16.440]   I got her here.
[01:53:16.440 --> 01:53:17.360]   This is your future.
[01:53:17.360 --> 01:53:18.120]   You know, you're--
[01:53:18.120 --> 01:53:19.440]   This is--
[01:53:19.440 --> 01:53:19.960]   Yeah.
[01:53:19.960 --> 01:53:22.720]   This happens in just a few years.
[01:53:22.720 --> 01:53:25.320]   I know, I have a seven-year-old.
[01:53:25.320 --> 01:53:26.160]   This is far away.
[01:53:26.160 --> 01:53:28.080]   So I'm mostly just shock.
[01:53:28.080 --> 01:53:28.920]   It's all shock.
[01:53:28.920 --> 01:53:30.480]   But when you be proud to be--
[01:53:30.480 --> 01:53:31.760]   to have Piper as a dog--
[01:53:31.760 --> 01:53:33.760]   I mean, Piper's very impressive, right?
[01:53:33.760 --> 01:53:35.920]   Smart, outgoing.
[01:53:35.920 --> 01:53:37.680]   There's nothing wrong with this generation,
[01:53:37.680 --> 01:53:38.840]   eye generation at all.
[01:53:38.840 --> 01:53:40.640]   And that's, I think, maybe a larger point.
[01:53:40.640 --> 01:53:41.440]   Thank you, Piper.
[01:53:41.440 --> 01:53:41.440]   Thank you.
[01:53:41.440 --> 01:53:43.080]   I appreciate it.
[01:53:43.080 --> 01:53:45.120]   You may sit back down, Mr. Thompson.
[01:53:45.120 --> 01:53:48.040]   Thank you for letting us talk to Piper Reese.
[01:53:48.040 --> 01:53:49.520]   I thought it was Jen Alpha.
[01:53:49.520 --> 01:53:50.680]   Didn't we circle back?
[01:53:50.680 --> 01:53:52.120]   Nope, too late.
[01:53:52.120 --> 01:53:53.120]   I like it.
[01:53:53.120 --> 01:53:54.120]   I think that's a very good name.
[01:53:54.120 --> 01:53:54.920]   I think that's a very good name.
[01:53:54.920 --> 01:53:56.120]   It's been used to fair amount.
[01:53:56.120 --> 01:53:58.400]   There's going to be a brawl between I, Jen, and Gen Z,
[01:53:58.400 --> 01:53:59.200]   I have a feeling.
[01:53:59.200 --> 01:54:00.040]   And just because--
[01:54:00.040 --> 01:54:03.800]   So is it "I, Jen" as a name that was created by Baby Boomers?
[01:54:03.800 --> 01:54:05.880]   Yeah, it was created and also was created--
[01:54:05.880 --> 01:54:06.880]   There's been a lot of things.
[01:54:06.880 --> 01:54:09.200]   It's been tripped, so I don't know if people are going to be down with it.
[01:54:09.200 --> 01:54:11.200]   But it's named after the iPhone, and the premise is--
[01:54:11.200 --> 01:54:13.280]   I feel like the generation should be able to allow--
[01:54:13.280 --> 01:54:15.880]   We should have the luxury of naming ourselves.
[01:54:15.880 --> 01:54:17.920]   Well, you're not in it, so you don't get to.
[01:54:17.920 --> 01:54:19.680]   No, I'm Jen, why?
[01:54:19.680 --> 01:54:21.520]   I'm part of that in between--
[01:54:21.520 --> 01:54:22.520]   Nobody cared about a generation.
[01:54:22.520 --> 01:54:23.520]   You have a sucky generation.
[01:54:23.520 --> 01:54:25.440]   I just want to say, your generation sucks.
[01:54:25.440 --> 01:54:27.200]   I was a baby boomer, my friend.
[01:54:27.200 --> 01:54:27.920]   Jen, why am I in the line?
[01:54:27.920 --> 01:54:30.080]   That was the greatest generation after the last--
[01:54:30.080 --> 01:54:31.640]   No, we're not millennials.
[01:54:31.640 --> 01:54:32.560]   Is there a difference between--
[01:54:32.560 --> 01:54:33.560]   Sorry to be so eager.
[01:54:33.560 --> 01:54:37.000]   The millennials came of age at the turn of the century.
[01:54:37.000 --> 01:54:38.760]   Gen-wise a little older than that.
[01:54:38.760 --> 01:54:41.600]   And then there was Gen-X, because you succeeded Gen-X--
[01:54:41.600 --> 01:54:45.640]   Gen-X is basically '64 to '84 to '80, if you were born.
[01:54:45.640 --> 01:54:49.440]   I was born in '56, which is the very tail end of the Baby Boomers.
[01:54:49.440 --> 01:54:51.160]   I'm the me generation, baby.
[01:54:51.160 --> 01:54:53.120]   I was right up the start of the Gen-X's.
[01:54:53.120 --> 01:54:53.920]   That's genuine.
[01:54:53.920 --> 01:54:55.560]   That's all about me.
[01:54:55.560 --> 01:54:57.000]   And it was a great book, don't get me wrong,
[01:54:57.000 --> 01:54:59.360]   but I'm not building my entire persona around it.
[01:54:59.360 --> 01:55:02.480]   Well, what I few realized is before it was a book, it was a band.
[01:55:02.480 --> 01:55:03.480]   It was Billy Idles--
[01:55:03.480 --> 01:55:04.920]   It was Billy Idles' band before--
[01:55:04.920 --> 01:55:06.160]   It was a band called Gen-X.
[01:55:06.160 --> 01:55:07.400]   Yeah, it was called Generation-X.
[01:55:07.400 --> 01:55:09.240]   It was Billy Idles' first band.
[01:55:09.240 --> 01:55:11.760]   Before he became-- well, I guess he went by Billy Idle
[01:55:11.760 --> 01:55:14.000]   in those days as well, but he made much bigger deal
[01:55:14.000 --> 01:55:15.680]   in his solo career.
[01:55:15.680 --> 01:55:17.480]   Yeah, their first band was called Generation-X.
[01:55:17.480 --> 01:55:19.680]   Speaking of Billy Idle, did you hear about its performance
[01:55:19.680 --> 01:55:20.880]   at the Salesforce Conference?
[01:55:20.880 --> 01:55:22.040]   I did not.
[01:55:22.040 --> 01:55:23.080]   That is just the kind of--
[01:55:23.080 --> 01:55:23.560]   Is this here?
[01:55:23.560 --> 01:55:24.560]   --is the kind of--
[01:55:24.560 --> 01:55:24.760]   Oh, no.
[01:55:24.760 --> 01:55:26.760]   He performed at the Salesforce Conference,
[01:55:26.760 --> 01:55:29.840]   and no word of a lie, he changed the white wedding lyrics
[01:55:29.840 --> 01:55:32.400]   to, "It's a nice day for CRM."
[01:55:32.400 --> 01:55:34.200]   Oh, that is like, that's it.
[01:55:34.200 --> 01:55:35.400]   Get out.
[01:55:35.400 --> 01:55:38.680]   You know, it's like, how good your music was.
[01:55:38.680 --> 01:55:39.200]   That's just--
[01:55:39.200 --> 01:55:42.360]   Well, Billy was never really edgy.
[01:55:42.360 --> 01:55:43.320]   It was all--
[01:55:43.320 --> 01:55:44.120]   It was all an act.
[01:55:44.120 --> 01:55:46.240]   He was the Gary Glitter of his--
[01:55:46.240 --> 01:55:47.400]   Well, please don't--
[01:55:47.400 --> 01:55:48.320]   No, don't go there.
[01:55:48.320 --> 01:55:48.960]   Don't go there.
[01:55:48.960 --> 01:55:51.400]   Gary Glitter jokes are really--
[01:55:51.400 --> 01:55:53.240]   Great Britain, they're just-- it's too soon.
[01:55:53.240 --> 01:55:53.240]   Yeah.
[01:55:53.240 --> 01:55:54.240]   It's too soon.
[01:55:54.240 --> 01:55:54.960]   Yeah, it's too soon.
[01:55:54.960 --> 01:55:56.320]   For Gary Glitter jokes.
[01:55:56.320 --> 01:55:57.960]   As long as we've dissed one generation,
[01:55:57.960 --> 01:56:00.880]   let's diss another generation, my daughter's generation,
[01:56:00.880 --> 01:56:03.600]   this is not the onion, the Wall Street Journal,
[01:56:03.600 --> 01:56:08.120]   millennials on Earth, an amazing hack to get free TV.
[01:56:08.120 --> 01:56:10.040]   The antenna.
[01:56:10.040 --> 01:56:12.160]   Is this legal?
[01:56:12.160 --> 01:56:14.200]   This is the Wall Street Journal.
[01:56:14.200 --> 01:56:17.400]   They interviewed Dan Sisko, who's 28 years old,
[01:56:17.400 --> 01:56:19.120]   and a major dumb-ass little--
[01:56:19.120 --> 01:56:23.240]   He said, I was just kind of surprised this technology exists.
[01:56:23.240 --> 01:56:25.160]   He's talking about an antenna.
[01:56:25.160 --> 01:56:26.360]   It's been awesome.
[01:56:26.360 --> 01:56:30.840]   It doesn't log out, and it doesn't skip.
[01:56:30.840 --> 01:56:32.640]   A weep for a few generations.
[01:56:32.640 --> 01:56:34.280]   I weep for the Wall Street Journal.
[01:56:34.280 --> 01:56:35.240]   This is just--
[01:56:35.240 --> 01:56:35.760]   Oh, come on.
[01:56:35.760 --> 01:56:36.600]   It was a great--
[01:56:36.600 --> 01:56:37.680]   It was a great story.
[01:56:37.680 --> 01:56:38.840]   It was a great story.
[01:56:38.840 --> 01:56:39.200]   I'm sorry.
[01:56:39.200 --> 01:56:40.800]   It was a journalist you couldn't look at.
[01:56:40.800 --> 01:56:42.160]   You couldn't interview somebody like that.
[01:56:42.160 --> 01:56:43.160]   You're mad at me.
[01:56:43.160 --> 01:56:44.360]   I'm smiling on millennials.
[01:56:44.360 --> 01:56:48.720]   We picked some doofus, the worst example of his generation,
[01:56:48.720 --> 01:56:52.800]   and say, look, they're discovering antennas.
[01:56:52.800 --> 01:56:53.160]   Yeah.
[01:56:53.160 --> 01:56:55.520]   I'll tell you where antennas are big, Japan.
[01:56:55.520 --> 01:56:59.160]   They've got cars all have television monitored--
[01:56:59.160 --> 01:56:59.640]   Wait a minute.
[01:56:59.640 --> 01:57:01.000]   They have video on the car.
[01:57:01.000 --> 01:57:02.600]   They have antenna TV.
[01:57:02.600 --> 01:57:04.720]   They have rabbit ears on their cars?
[01:57:04.720 --> 01:57:06.720]   No, they've got digital antennas on their cars,
[01:57:06.720 --> 01:57:09.480]   and anybody who wants to can get live television
[01:57:09.480 --> 01:57:11.840]   while you're driving in the car.
[01:57:11.840 --> 01:57:13.920]   Which is great for driver's safety.
[01:57:13.920 --> 01:57:14.440]   Yes.
[01:57:14.440 --> 01:57:16.920]   You can always pull up those kind of top tips on--
[01:57:16.920 --> 01:57:17.280]   They're great for things.
[01:57:17.280 --> 01:57:18.920]   --out of order an accident before you're
[01:57:18.920 --> 01:57:19.840]   about to get into one.
[01:57:19.840 --> 01:57:21.080]   Yeah, on the other hand, though, which
[01:57:21.080 --> 01:57:23.560]   is worse for driver's safety, having two kids going out
[01:57:23.560 --> 01:57:26.240]   at hammer and tongs with fists out in the back seat,
[01:57:26.240 --> 01:57:27.960]   or just sticking them in front of the Google box
[01:57:27.960 --> 01:57:31.320]   and letting them just chill out in front of a--
[01:57:31.320 --> 01:57:32.480]   Oh, definitely the latter.
[01:57:32.480 --> 01:57:35.960]   I'm worried about driver distraction.
[01:57:35.960 --> 01:57:40.120]   They quote Scott Wills, who is a person of my era,
[01:57:40.120 --> 01:57:42.160]   and a wireless industry executive.
[01:57:42.160 --> 01:57:45.040]   He worked for years that set the transition
[01:57:45.040 --> 01:57:48.560]   from analog to digital in motion over TV--
[01:57:48.560 --> 01:57:49.720]   Which took forever.
[01:57:49.720 --> 01:57:50.280]   --took forever.
[01:57:50.280 --> 01:57:53.160]   And finally, about what eight years ago,
[01:57:53.160 --> 01:57:55.160]   all the televisions-- most of the televisions
[01:57:55.160 --> 01:57:58.480]   went digital, which was a good thing all around, I think.
[01:57:58.480 --> 01:57:59.200]   He wondered, though.
[01:57:59.200 --> 01:58:02.000]   He thought that the millennials might be a little confused
[01:58:02.000 --> 01:58:02.480]   by this.
[01:58:02.480 --> 01:58:03.840]   So he asked his son--
[01:58:03.840 --> 01:58:09.560]   do you think there's broadcast anymore?
[01:58:09.560 --> 01:58:11.720]   His son said, Dad, you should know better than anyone.
[01:58:11.720 --> 01:58:13.880]   There's no broadcast TV.
[01:58:13.880 --> 01:58:17.560]   Broadcast TV is gone.
[01:58:17.560 --> 01:58:19.400]   I had no idea.
[01:58:19.400 --> 01:58:21.480]   He said of broadcast continued existence.
[01:58:21.480 --> 01:58:25.120]   I'm still not even familiar with the concept.
[01:58:25.120 --> 01:58:26.960]   [LAUGHTER]
[01:58:26.960 --> 01:58:29.320]   I'm telling you, I think this is just bad journalism,
[01:58:29.320 --> 01:58:31.520]   where you find a couple of really stupid people
[01:58:31.520 --> 01:58:32.640]   in the generation.
[01:58:32.640 --> 01:58:33.800]   See what I did the opposite.
[01:58:33.800 --> 01:58:37.320]   I found a smart representative of the eye gen.
[01:58:37.320 --> 01:58:38.440]   Showed you.
[01:58:38.440 --> 01:58:39.840]   There's nothing to fear.
[01:58:39.840 --> 01:58:41.160]   Hey, we had a great week on Twitch.
[01:58:41.160 --> 01:58:44.080]   Let me show you a little example of some of the fun we had.
[01:58:44.080 --> 01:58:45.480]   Roll the film.
[01:58:45.480 --> 01:58:47.720]   Previously on Twitch.
[01:58:47.720 --> 01:58:49.040]   So that's it for iPad today.
[01:58:49.040 --> 01:58:51.480]   We didn't say why we were in hats, but I don't think.
[01:58:51.480 --> 01:58:51.840]   What?
[01:58:51.840 --> 01:58:52.840]   It's iOS today now.
[01:58:52.840 --> 01:58:54.360]   That's it for iOS today.
[01:58:54.360 --> 01:58:55.080]   It is too early.
[01:58:55.080 --> 01:58:55.840]   I'm Leo LaPorte.
[01:58:55.840 --> 01:58:58.640]   That's Sarah Lane, and we thank you all for--
[01:58:58.640 --> 01:59:00.520]   The new screen savers.
[01:59:00.520 --> 01:59:02.760]   Horizons, you are a go for a departure.
[01:59:02.760 --> 01:59:04.800]   Mr. Della Hante, take us out.
[01:59:04.800 --> 01:59:06.400]   What speed?
[01:59:06.400 --> 01:59:09.480]   This is the prettiest thing ever all cleared licenses.
[01:59:09.480 --> 01:59:11.600]   What's the internet, that's LaPorte?
[01:59:11.600 --> 01:59:13.240]   NSS, LaPorte on screen.
[01:59:13.240 --> 01:59:14.240]   Ray Shields.
[01:59:14.240 --> 01:59:15.880]   Should I demand them to surrender?
[01:59:15.880 --> 01:59:16.560]   Yes.
[01:59:16.560 --> 01:59:18.520]   Well, I divert power to the weapons captain.
[01:59:18.520 --> 01:59:20.320]   Setting red alert.
[01:59:20.320 --> 01:59:21.000]   Fire.
[01:59:21.000 --> 01:59:22.920]   Everything we've got at them.
[01:59:22.920 --> 01:59:24.920]   Looks like we have impact.
[01:59:24.920 --> 01:59:25.920]   Take our probe.
[01:59:25.920 --> 01:59:29.800]   It just doesn't happen when you take down NSS LaPorte.
[01:59:29.800 --> 01:59:31.000]   This week in Google.
[01:59:31.000 --> 01:59:33.880]   I thought I'm reading the onion, but it wasn't the onion.
[01:59:33.880 --> 01:59:36.440]   It was the Wall Street Journal of Millennials on Earth.
[01:59:36.440 --> 01:59:40.320]   An amazing hack to get free TV.
[01:59:40.320 --> 01:59:41.840]   The antenna.
[01:59:41.840 --> 01:59:44.600]   I was just kind of surprised that this technology exists,
[01:59:44.600 --> 01:59:47.240]   says Mr. Sisko, 28 years old.
[01:59:47.240 --> 01:59:50.320]   It doesn't log out, and it doesn't skip.
[01:59:50.320 --> 01:59:51.840]   I'm doomed.
[01:59:51.840 --> 01:59:53.440]   I just want to give them a dial telephone
[01:59:53.440 --> 01:59:54.720]   and see what they do with it.
[01:59:54.720 --> 01:59:55.480]   To it.
[01:59:55.480 --> 01:59:58.040]   Now in color.
[01:59:58.040 --> 02:00:00.040]   Life support is now 41%.
[02:00:00.040 --> 02:00:02.160]   Should keep firing tactical.
[02:00:02.160 --> 02:00:03.720]   I'm doing the best I can.
[02:00:03.720 --> 02:00:04.960]   You can do a little bit better.
[02:00:04.960 --> 02:00:06.720]   We're going to roast these pirates.
[02:00:06.720 --> 02:00:08.720]   Yeah.
[02:00:08.720 --> 02:00:10.480]   That's the nerdiest thing we have ever.
[02:00:10.480 --> 02:00:12.040]   What is the name of that game so we can--
[02:00:12.040 --> 02:00:14.040]   It's--
[02:00:14.040 --> 02:00:15.080]   Star Horizons.
[02:00:15.080 --> 02:00:16.520]   Horizons?
[02:00:16.520 --> 02:00:18.800]   Starship Horizons.com?
[02:00:18.800 --> 02:00:20.320]   Something like that.
[02:00:20.320 --> 02:00:21.920]   Is it like a Star Trek ripple for something?
[02:00:21.920 --> 02:00:22.920]   No.
[02:00:22.920 --> 02:00:27.280]   Well, you're-- I mean, not every starship comes from Star Trek.
[02:00:27.280 --> 02:00:27.800]   No.
[02:00:27.800 --> 02:00:28.720]   Hitchcock is good to the galaxy.
[02:00:28.720 --> 02:00:29.240]   OK.
[02:00:29.240 --> 02:00:30.120]   It's a well before.
[02:00:30.120 --> 02:00:32.200]   So you're on the cap.
[02:00:32.200 --> 02:00:34.040]   You're the bridge of a starship.
[02:00:34.040 --> 02:00:35.560]   And each of you has to take a turn.
[02:00:35.560 --> 02:00:38.040]   You have one server, a local server running the game,
[02:00:38.040 --> 02:00:39.560]   and then everybody logs in via a browser,
[02:00:39.560 --> 02:00:41.760]   so you can use a phone or an iPad.
[02:00:41.760 --> 02:00:44.200]   It's actually a pretty cool game.
[02:00:44.200 --> 02:00:46.720]   The whole show is the new screensabers from yesterday.
[02:00:46.720 --> 02:00:48.800]   You can watch the whole thing.
[02:00:48.800 --> 02:00:50.240]   Our show today-- in fact, everything
[02:00:50.240 --> 02:00:52.920]   you see today brought to you by our internet service provider
[02:00:52.920 --> 02:00:59.080]   want to give Sonic a huge plug, our gigabit, 10 gigabit
[02:00:59.080 --> 02:01:03.480]   fiber line up and down comes from the best internet service
[02:01:03.480 --> 02:01:04.920]   provider in the country.
[02:01:04.920 --> 02:01:06.600]   Look at the deal you get.
[02:01:06.600 --> 02:01:08.360]   You can go to sonic.com/twit.
[02:01:08.360 --> 02:01:10.480]   There's the address form.
[02:01:10.480 --> 02:01:11.080]   You can fill it out.
[02:01:11.080 --> 02:01:12.080]   Look at the deal you get.
[02:01:12.080 --> 02:01:15.080]   One of my friends, actually, was Bemone.
[02:01:15.080 --> 02:01:18.680]   We went over the Comcast Terabyte Band.
[02:01:18.680 --> 02:01:22.200]   And they asked, how do I get out of the Terabyte Bandwidth
[02:01:22.200 --> 02:01:22.600]   cap?
[02:01:22.600 --> 02:01:23.480]   How do I get out of this?
[02:01:23.480 --> 02:01:24.880]   And they said, well, you have to get business class.
[02:01:24.880 --> 02:01:25.920]   And he called business.
[02:01:25.920 --> 02:01:26.960]   And they said, well, we can't have--
[02:01:26.960 --> 02:01:28.680]   it's too different to be.
[02:01:28.680 --> 02:01:30.480]   The guy was-- he said, should I go Sonic?
[02:01:30.480 --> 02:01:31.720]   I said, yes.
[02:01:31.720 --> 02:01:32.760]   Look what you get.
[02:01:32.760 --> 02:01:33.760]   Look what you get.
[02:01:33.760 --> 02:01:37.200]   You get fiber for home.
[02:01:37.200 --> 02:01:38.800]   I won't tell you the price to the end,
[02:01:38.800 --> 02:01:40.120]   although you can see it on the screen.
[02:01:40.120 --> 02:01:41.760]   You get 15 email accounts.
[02:01:41.760 --> 02:01:43.400]   You get a gigabyte of storage.
[02:01:43.400 --> 02:01:44.720]   You get personal web hosting.
[02:01:44.720 --> 02:01:47.040]   You get a domain name, custom domain name.
[02:01:47.040 --> 02:01:49.800]   You get fax line service, phone service,
[02:01:49.800 --> 02:01:52.360]   with unlimited local and long distance.
[02:01:52.360 --> 02:01:58.720]   You get a gigabit down, and all of this $40 a month.
[02:01:58.720 --> 02:02:01.240]   And you can switch from your current carrier,
[02:02:01.240 --> 02:02:02.760]   and you could port your phone number over.
[02:02:02.760 --> 02:02:06.160]   So the phone service, it's just like your old phone service.
[02:02:06.160 --> 02:02:10.560]   But it's all total, flat fee $40 a month.
[02:02:10.560 --> 02:02:11.520]   And here's the other thing.
[02:02:11.520 --> 02:02:15.520]   By standing up for privacy, Sonic will fight.
[02:02:15.520 --> 02:02:17.440]   Look at their EFF scorecard.
[02:02:17.440 --> 02:02:19.840]   Green checks across the board.
[02:02:19.840 --> 02:02:21.200]   They stand up for their customers.
[02:02:21.200 --> 02:02:22.760]   They fight government subpoenas.
[02:02:22.760 --> 02:02:25.840]   They have no bandwidth caps, zero bandwidth caps.
[02:02:25.840 --> 02:02:27.240]   Use all you want.
[02:02:27.240 --> 02:02:29.040]   They'll make more.
[02:02:29.040 --> 02:02:31.960]   And of course, great local customer support.
[02:02:31.960 --> 02:02:34.440]   sonic.com/twit.
[02:02:34.440 --> 02:02:35.520]   Join the internet revolution.
[02:02:35.520 --> 02:02:37.400]   Get your first month of Sonic.
[02:02:37.400 --> 02:02:40.640]   Internet and phone service free when you go to sonic.com/twit.
[02:02:40.640 --> 02:02:44.160]   Bundle it with dish, and you'll save $120 on your Sonic bill.
[02:02:44.160 --> 02:02:46.360]   And if you can't get it, I'm sorry.
[02:02:46.360 --> 02:02:47.560]   I feel bad for you.
[02:02:47.560 --> 02:02:49.440]   But if you can, there's no reason.
[02:02:49.440 --> 02:02:50.720]   No reason to stay with those other guys.
[02:02:50.720 --> 02:02:53.200]   sonic.com/twit.
[02:02:53.200 --> 02:02:54.120]   You know what I forgot to do?
[02:02:54.120 --> 02:02:56.080]   And I know Carsten knows I forgot to do this.
[02:02:56.080 --> 02:02:58.520]   I forgot to ask Jason Howell.
[02:02:58.520 --> 02:03:00.360]   What's coming up this week, Jason?
[02:03:00.360 --> 02:03:04.720]   This week on Tuesday, August 8th,
[02:03:04.720 --> 02:03:07.720]   Sharp is set to unveil the AquaS2.
[02:03:07.720 --> 02:03:10.240]   It's a smartphone with a 5.5 inch display
[02:03:10.240 --> 02:03:15.240]   and a tri-bezzaless design with an 85% screen to body ratio.
[02:03:15.240 --> 02:03:17.560]   They're all doing that right now.
[02:03:17.560 --> 02:03:21.280]   It's also rumored to have an on-screen fingerprint sensor.
[02:03:21.280 --> 02:03:22.400]   Should be interesting.
[02:03:22.400 --> 02:03:25.680]   On Thursday, August 10th, Motorola will release
[02:03:25.680 --> 02:03:28.880]   its latest flagship phone, the Moto Z2 Force
[02:03:28.880 --> 02:03:32.240]   with a shatterproof screen, dual rear-facing cameras,
[02:03:32.240 --> 02:03:35.440]   and a design that facilitates Motorola's modular extensions
[02:03:35.440 --> 02:03:37.880]   that they call Moto Mods, including by the way,
[02:03:37.880 --> 02:03:40.880]   a new 360 degree camera Moto Mod
[02:03:40.880 --> 02:03:42.840]   that will be released concurrently.
[02:03:42.840 --> 02:03:46.040]   Also on the 10th, Snap will release its second earnings
[02:03:46.040 --> 02:03:49.800]   report to date, and it's not expected to be all that great.
[02:03:49.800 --> 02:03:52.640]   Snapchat has seen an escalation of feature parity
[02:03:52.640 --> 02:03:54.280]   among its biggest competitors,
[02:03:54.280 --> 02:03:56.280]   and that has been part of what has resulted
[02:03:56.280 --> 02:03:59.040]   in the value of Snapstock hitting its lowest points
[02:03:59.040 --> 02:04:01.600]   since its IPO at the beginning of the year.
[02:04:01.600 --> 02:04:03.280]   That's a look at a few of the things we'll be tracking
[02:04:03.280 --> 02:04:04.120]   in the coming week.
[02:04:04.120 --> 02:04:06.520]   Join Mega Moroni and me on Tech News today,
[02:04:06.520 --> 02:04:09.440]   every weekday at 4PM Pacific, 7PM Eastern,
[02:04:09.440 --> 02:04:11.200]   here on Twitter.tv.
[02:04:11.200 --> 02:04:12.440]   - Thank you, Jason Howell.
[02:04:12.440 --> 02:04:16.040]   You heard Piper say it, I think.
[02:04:16.040 --> 02:04:17.160]   Everybody's going to Instagram.
[02:04:17.160 --> 02:04:18.160]   They're leaving Snapchat behind.
[02:04:18.160 --> 02:04:19.000]   - Stampede. - Stampede.
[02:04:19.000 --> 02:04:20.120]   - Stampede. - Yeah.
[02:04:20.120 --> 02:04:20.960]   - I don't get it.
[02:04:20.960 --> 02:04:23.160]   I try to do those Instagrams, or look at those Instagrams.
[02:04:23.160 --> 02:04:25.920]   I don't understand who's doing that, and why?
[02:04:25.920 --> 02:04:28.960]   Who's, is it just, I'm just old.
[02:04:28.960 --> 02:04:30.680]   - I spend all my time on Vine.
[02:04:30.680 --> 02:04:34.600]   - Vine at least it had this thing, it was six seconds.
[02:04:34.600 --> 02:04:36.320]   - Six seconds, it was kind of fun.
[02:04:36.320 --> 02:04:39.360]   But Instagram stories, I see people, people I know,
[02:04:39.360 --> 02:04:41.460]   spending lots of time.
[02:04:41.460 --> 02:04:44.560]   And like I, Justine, I'm watching I, Justine,
[02:04:44.560 --> 02:04:46.720]   she's like doing something, but obviously not paying
[02:04:46.720 --> 02:04:48.840]   any attention to any of her friends or anything else,
[02:04:48.840 --> 02:04:50.120]   'cause she's constantly making her,
[02:04:50.120 --> 02:04:50.960]   do you do that?
[02:04:50.960 --> 02:04:51.800]   I bet you do that, Piper.
[02:04:51.800 --> 02:04:54.000]   You're constantly making your story.
[02:04:54.000 --> 02:04:56.240]   And then, I'd love to see the stats.
[02:04:56.240 --> 02:04:58.720]   Who's looking at those stories, really?
[02:04:58.720 --> 02:05:00.400]   Do you look at Instagram stories?
[02:05:00.400 --> 02:05:01.240]   No, I don't know.
[02:05:01.240 --> 02:05:02.680]   - I don't even have an Instagram account,
[02:05:02.680 --> 02:05:04.200]   I feel quite out of her.
[02:05:04.200 --> 02:05:05.040]   - How about you, Amy?
[02:05:05.040 --> 02:05:06.040]   Am I? - No, I don't.
[02:05:06.040 --> 02:05:07.280]   I don't, I don't.
[02:05:07.280 --> 02:05:08.480]   But I use Snap.
[02:05:08.480 --> 02:05:09.840]   - You do?
[02:05:09.840 --> 02:05:10.680]   - I do.
[02:05:10.680 --> 02:05:12.680]   - Is that how you communicate with your seven year old?
[02:05:12.680 --> 02:05:14.320]   - Yeah, that's exactly how.
[02:05:14.320 --> 02:05:17.920]   - Reparents, we need something.
[02:05:17.920 --> 02:05:19.280]   - It's hard to believe that it was reported
[02:05:19.280 --> 02:05:22.560]   that last year Google nearly paid 30 billion for Snap.
[02:05:22.560 --> 02:05:24.000]   - That I thought was interesting.
[02:05:24.000 --> 02:05:25.360]   - Yeah, you like what?
[02:05:25.360 --> 02:05:27.000]   - Snap had some really good patents
[02:05:27.000 --> 02:05:29.880]   that you have to do with the usual object recognition.
[02:05:29.880 --> 02:05:33.200]   And so, Snap, as it exists today,
[02:05:33.200 --> 02:05:35.720]   I don't see lasting in perpetuity,
[02:05:35.720 --> 02:05:37.600]   but some of the research that they've done
[02:05:37.600 --> 02:05:41.320]   in augmented reality and visual object recognition,
[02:05:41.320 --> 02:05:43.720]   and there's some interesting things happening
[02:05:43.720 --> 02:05:44.560]   behind the scenes.
[02:05:44.560 --> 02:05:45.840]   - Wow, there's an opportunity.
[02:05:45.840 --> 02:05:47.680]   - It's a pity that so much of their stuff
[02:05:47.680 --> 02:05:49.600]   is being hijacked by Facebook and others,
[02:05:49.600 --> 02:05:52.000]   but that said, if I never see another one of those
[02:05:52.000 --> 02:05:53.520]   dog filters over somebody's faces,
[02:05:53.520 --> 02:05:56.240]   I will be a happy man, 'cause those are freakies all hell.
[02:05:56.240 --> 02:05:59.480]   - It does show you, though, that any business
[02:05:59.480 --> 02:06:04.480]   that's relying on millennials and younger
[02:06:04.760 --> 02:06:09.560]   for their business model not being fickle is doomed.
[02:06:09.560 --> 02:06:11.800]   Why would you invest in any company that,
[02:06:11.800 --> 02:06:13.920]   well, 'cause they're hot with the kids right now,
[02:06:13.920 --> 02:06:17.600]   yeah, but that's like the lifespan of a Mayfly.
[02:06:17.600 --> 02:06:19.800]   I mean, that could change it any second.
[02:06:19.800 --> 02:06:20.880]   - We should be-- - And it's easy
[02:06:20.880 --> 02:06:22.080]   to steal their features.
[02:06:22.080 --> 02:06:24.000]   - We used to see that with social networks.
[02:06:24.000 --> 02:06:26.520]   I mean, remember when "Frenster" came out?
[02:06:26.520 --> 02:06:27.360]   - Yeah. - And sort of
[02:06:27.360 --> 02:06:28.200]   the United States-- - Yeah.
[02:06:28.200 --> 02:06:29.520]   - There are some Abrams as a friend.
[02:06:29.520 --> 02:06:30.360]   He's been our showman. - Yeah.
[02:06:30.360 --> 02:06:31.800]   - And then it was MySpace,
[02:06:31.800 --> 02:06:34.560]   and then Facebook got critical, you know,
[02:06:34.560 --> 02:06:36.720]   got critical mass, and it's difficult to see
[02:06:36.720 --> 02:06:39.240]   another social network actually taking over from that.
[02:06:39.240 --> 02:06:40.080]   - That's a good point.
[02:06:40.080 --> 02:06:41.920]   You could duplicate Facebook's functionality,
[02:06:41.920 --> 02:06:43.720]   but there's no way you could steal it.
[02:06:43.720 --> 02:06:45.080]   - And what it is, it's not a good effect.
[02:06:45.080 --> 02:06:46.400]   - It's got to have the social graph.
[02:06:46.400 --> 02:06:47.400]   - It's the Metcalf effect.
[02:06:47.400 --> 02:06:49.600]   You've got to have that number of interconnected people.
[02:06:49.600 --> 02:06:51.720]   - And when you were talking about Snapchat failing
[02:06:51.720 --> 02:06:53.720]   to take $30 billion from Google,
[02:06:53.720 --> 02:06:56.520]   I was thinking there was one even worse decision
[02:06:56.520 --> 02:06:58.680]   not to take Google's money, which was "Frensters."
[02:06:58.680 --> 02:07:01.120]   Because Jonathan or "Frenster" is born--
[02:07:01.120 --> 02:07:04.920]   - You were offered, I think it was 50 or $60 million.
[02:07:04.920 --> 02:07:06.360]   - So it's a rumor. - A lot of money
[02:07:06.360 --> 02:07:08.960]   in those guys. - Three IPO Google stock.
[02:07:08.960 --> 02:07:09.960]   That's the thing.
[02:07:09.960 --> 02:07:13.680]   - So it's going up, and what "Frenster" ultimately was worth,
[02:07:13.680 --> 02:07:14.520]   I believe was zero.
[02:07:14.520 --> 02:07:16.000]   I mean, they might have been sold to parts or something
[02:07:16.000 --> 02:07:17.160]   like that.
[02:07:17.160 --> 02:07:19.080]   So yes, it was pre-IPO Google stock,
[02:07:19.080 --> 02:07:21.600]   which went up many dozens of fold.
[02:07:21.600 --> 02:07:22.480]   That was a bummer.
[02:07:22.480 --> 02:07:25.600]   - I know of a PR agency that when Microsoft came
[02:07:25.600 --> 02:07:28.000]   to their country, Microsoft offered them stock
[02:07:28.000 --> 02:07:30.160]   in exchange, rather than fees, and they said,
[02:07:30.160 --> 02:07:31.760]   "No, no, we'll take the money, thanks."
[02:07:31.760 --> 02:07:32.680]   - That was pre-IPO.
[02:07:32.680 --> 02:07:35.200]   - Yeah, that was pre-IPO, and you were just like,
[02:07:35.200 --> 02:07:37.520]   "Wow, you could have retired at the end
[02:07:37.520 --> 02:07:40.280]   "and here's down the line, but we all make these decisions."
[02:07:40.280 --> 02:07:41.120]   - We all do.
[02:07:41.120 --> 02:07:43.360]   - I advised a friend not to bother investing in Google
[02:07:43.360 --> 02:07:45.120]   when it went on IPO, because there'd be
[02:07:45.120 --> 02:07:47.160]   another better search for a engine along in a minute.
[02:07:47.160 --> 02:07:49.480]   - Yeah, Google was being on TV for advantage
[02:07:49.480 --> 02:07:52.520]   being the charm, and that actually worked in search engines.
[02:07:52.520 --> 02:07:54.800]   - Yeah, unfortunately we didn't talk to my friend,
[02:07:54.800 --> 02:07:56.080]   and I don't talk that much now, but--
[02:07:56.080 --> 02:07:58.960]   (laughing)
[02:07:58.960 --> 02:08:02.680]   DJI, I didn't know that DJI, the makers of DJI fan,
[02:08:02.680 --> 02:08:07.520]   I'm probably the number one sub $20,000 drone.
[02:08:07.520 --> 02:08:11.680]   I didn't know that DJI stands for
[02:08:11.680 --> 02:08:14.240]   Da Jaiang Innovation Corporation.
[02:08:14.240 --> 02:08:16.200]   It's a Chinese company.
[02:08:16.200 --> 02:08:18.080]   Maybe the Department of Justice didn't know,
[02:08:18.080 --> 02:08:19.680]   because they've just issued a memo,
[02:08:19.680 --> 02:08:21.680]   the Department of the Army saying,
[02:08:21.680 --> 02:08:26.220]   "Guys, don't use DJI drones anymore.
[02:08:27.320 --> 02:08:31.480]   They are the most widely used non-program of record
[02:08:31.480 --> 02:08:35.400]   commercial off-the-shelf UAS employed by the Army.
[02:08:35.400 --> 02:08:39.640]   But, as you see in this memo, in item three,
[02:08:39.640 --> 02:08:43.920]   cease all use, uninstall all DJI applications,
[02:08:43.920 --> 02:08:46.520]   remove all batteries and storage media from device,
[02:08:46.520 --> 02:08:49.760]   and secure equipment for follow-on direction.
[02:08:49.760 --> 02:08:52.560]   We'll be sending Robocop to destroy it.
[02:08:52.560 --> 02:08:54.160]   - It's a full-on direction.
[02:08:54.160 --> 02:08:55.960]   It's a sledgehammer.
[02:08:55.960 --> 02:09:00.320]   - So, DJI, by the way, says,
[02:09:00.320 --> 02:09:02.000]   "We're surprised to disappoint it.
[02:09:02.000 --> 02:09:02.920]   They didn't contact us.
[02:09:02.920 --> 02:09:05.200]   We could have probably reassured them,
[02:09:05.200 --> 02:09:06.840]   because we're made in China.
[02:09:06.840 --> 02:09:10.260]   Everything on this desk is made in China.
[02:09:10.260 --> 02:09:14.880]   Should, I understand, remember when the Department
[02:09:14.880 --> 02:09:16.040]   of Commerce a couple of years ago said,
[02:09:16.040 --> 02:09:18.120]   "Don't buy Huawei and ZTE phones
[02:09:18.120 --> 02:09:20.200]   'cause the Chinese military is a part owner."
[02:09:20.200 --> 02:09:22.280]   Amy, you're an expert on China.
[02:09:22.280 --> 02:09:25.800]   Is this paranoia or is this sensible?
[02:09:26.800 --> 02:09:30.240]   - I wouldn't call myself a China expert.
[02:09:30.240 --> 02:09:31.080]   - I just told you.
[02:09:31.080 --> 02:09:32.840]   - I did, however-- - I did, however,
[02:09:32.840 --> 02:09:35.600]   fly my DJI maverick this morning.
[02:09:35.600 --> 02:09:36.440]   - Oh.
[02:09:36.440 --> 02:09:40.840]   - So, I clearly wasn't obeying the rules.
[02:09:40.840 --> 02:09:41.680]   - Well, it's just the Army.
[02:09:41.680 --> 02:09:43.520]   You're not in the Army yet, so it's okay.
[02:09:43.520 --> 02:09:45.520]   - I mean, I think it depends on where you are.
[02:09:45.520 --> 02:09:48.920]   Listen, I had the connected headset.
[02:09:48.920 --> 02:09:50.720]   So, you have bird's eye view,
[02:09:50.720 --> 02:09:53.520]   and I had a lovely time flying around the bay this morning.
[02:09:53.520 --> 02:09:54.360]   - Oh, fun.
[02:09:54.360 --> 02:09:55.880]   - I like the virtual reality.
[02:09:55.880 --> 02:09:57.360]   - Yeah, it's pretty cool.
[02:09:57.360 --> 02:09:58.720]   - It's like-- - It's cool.
[02:09:58.720 --> 02:10:00.720]   - Maverick has a gimbal camera,
[02:10:00.720 --> 02:10:03.360]   and you can control the gimbal by moving your head around.
[02:10:03.360 --> 02:10:04.920]   So, it's pretty slick. - Oh, fantastic.
[02:10:04.920 --> 02:10:07.480]   So, you feel like you're there, like you're a bird?
[02:10:07.480 --> 02:10:08.600]   - Yeah, it's bird's eye view.
[02:10:08.600 --> 02:10:10.200]   Yeah, it's pretty amazing.
[02:10:10.200 --> 02:10:13.480]   My husband, and you technically, you can fly it
[02:10:13.480 --> 02:10:15.800]   and do that at the same time, but I'm not that dexterous.
[02:10:15.800 --> 02:10:17.320]   So, I usually have somebody else fly
[02:10:17.320 --> 02:10:19.080]   if I want to look in vice versa.
[02:10:19.080 --> 02:10:21.800]   Listen, I don't know, you know,
[02:10:21.800 --> 02:10:24.320]   if you think about the information that could be scraped,
[02:10:24.320 --> 02:10:29.320]   I don't know how much necessarily, yeah, that's it.
[02:10:29.320 --> 02:10:32.320]   It's affordable, you can fly it into a tree.
[02:10:32.320 --> 02:10:33.400]   - I need that, by the way,
[02:10:33.400 --> 02:10:36.360]   'cause I have not yet successfully flying a tree.
[02:10:36.360 --> 02:10:38.400]   - I have obliterated many drones.
[02:10:38.400 --> 02:10:39.480]   - Me too.
[02:10:39.480 --> 02:10:40.320]   - To many things.
[02:10:40.320 --> 02:10:43.360]   - And I keep thinking, oh, maybe it's 'cause I have a cheap one,
[02:10:43.360 --> 02:10:45.480]   so I buy a more expensive one, fly it into a tree.
[02:10:45.480 --> 02:10:46.400]   - Yeah, that'll help.
[02:10:46.400 --> 02:10:47.480]   - Maybe that one was too cheap,
[02:10:47.480 --> 02:10:49.440]   so I'll buy a more expensive tree easily.
[02:10:49.440 --> 02:10:50.360]   - Yeah, yeah, yeah.
[02:10:50.360 --> 02:10:51.520]   - You need cheap, but they'll just go right now.
[02:10:51.520 --> 02:10:53.040]   - Oh, I just give it up on drugs.
[02:10:53.040 --> 02:10:56.160]   - I think the theory is that it could be mapping,
[02:10:56.160 --> 02:10:58.120]   like they've got GPS, they've got cameras,
[02:10:58.120 --> 02:10:58.960]   they've got--
[02:10:58.960 --> 02:11:00.400]   - They could be sending data back to China,
[02:11:00.400 --> 02:11:04.560]   but this is, this strikes me as cuspersecule over again.
[02:11:04.560 --> 02:11:06.360]   It's just like, oh, they're not proper American,
[02:11:06.360 --> 02:11:08.160]   home grown, let's ban 'em, you know.
[02:11:08.160 --> 02:11:09.800]   - Good luck finding a drone that's not me,
[02:11:09.800 --> 02:11:10.800]   at least made in China.
[02:11:10.800 --> 02:11:11.640]   - Exactly.
[02:11:11.640 --> 02:11:12.600]   - Yeah, even 3D robotics.
[02:11:12.600 --> 02:11:13.440]   - Half of our technology.
[02:11:13.440 --> 02:11:15.480]   - When they were making, they were being made largely in China.
[02:11:15.480 --> 02:11:17.000]   I had some tea one or two.
[02:11:17.000 --> 02:11:17.840]   Sorry, go ahead.
[02:11:17.840 --> 02:11:20.160]   - Yeah, it's just, there is no technology
[02:11:20.160 --> 02:11:21.400]   that any of us owns that has,
[02:11:21.400 --> 02:11:25.800]   at some point, hasn't had some interaction with China.
[02:11:25.800 --> 02:11:30.800]   So, you know, in terms of mapping,
[02:11:30.800 --> 02:11:32.600]   anywhere that you would want to,
[02:11:32.600 --> 02:11:36.520]   that the Chinese government may get excited about,
[02:11:36.520 --> 02:11:41.400]   you're gonna have a hard time flying over anyways.
[02:11:41.400 --> 02:11:43.120]   So because there's just their space,
[02:11:43.120 --> 02:11:44.760]   and it's hard to get to.
[02:11:44.760 --> 02:11:48.400]   And to be honest, there's, you know, within two years,
[02:11:48.400 --> 02:11:50.800]   there are fleets of CubeSats and microsats
[02:11:50.800 --> 02:11:54.680]   that are being launched by companies
[02:11:54.680 --> 02:11:58.080]   that will be collecting real-time hourly maps
[02:11:58.080 --> 02:12:00.400]   and data all around planet Earth.
[02:12:00.400 --> 02:12:05.400]   So to harness me to get a snapshot
[02:12:05.400 --> 02:12:08.280]   on this particular day of the place where I happen to be flying,
[02:12:08.280 --> 02:12:12.840]   just seems like something that wouldn't be worth their while.
[02:12:12.840 --> 02:12:14.760]   - Also, if there was something to be worried about,
[02:12:14.760 --> 02:12:17.600]   shouldn't, instead of just banning it from US military use,
[02:12:17.600 --> 02:12:19.440]   and if there's something that really,
[02:12:19.440 --> 02:12:20.280]   - Tell us.
[02:12:20.280 --> 02:12:22.040]   - Tell us about the things.
[02:12:22.040 --> 02:12:25.040]   You know, it's like, why shouldn't you be buying this?
[02:12:25.040 --> 02:12:26.720]   - That's, hmm.
[02:12:26.720 --> 02:12:29.120]   - Cyber criminals have decided that Hollywood
[02:12:29.120 --> 02:12:34.120]   is ripe for the plucking and the latest victim, HBO.
[02:12:34.120 --> 02:12:37.600]   HBO recently experienced a cyber incident.
[02:12:37.600 --> 02:12:38.560]   - We got pwned.
[02:12:38.560 --> 02:12:41.440]   - Which resulted in the compromise of proprietary information,
[02:12:41.440 --> 02:12:45.760]   1.5 terabytes of data, according to the hackers,
[02:12:45.760 --> 02:12:50.120]   were including, oh dear, upcoming episodes of Ballers.
[02:12:50.120 --> 02:12:50.960]   - Mm.
[02:12:50.960 --> 02:12:52.080]   - And Room 104.
[02:12:52.080 --> 02:12:53.160]   - I don't even know.
[02:12:53.160 --> 02:12:54.080]   I know what Ballers is,
[02:12:54.080 --> 02:12:55.280]   'cause it's on after Game of Thrones.
[02:12:55.280 --> 02:12:56.840]   What the hell's Room 104?
[02:12:56.840 --> 02:12:59.640]   It's on after Room 103, that's all I know.
[02:12:59.640 --> 02:13:02.800]   - And some written material from next week's episode
[02:13:02.800 --> 02:13:05.440]   of Game of Thrones, and more is promised soon.
[02:13:05.440 --> 02:13:08.200]   - It's a pretty small league.
[02:13:08.200 --> 02:13:09.040]   - Okay.
[02:13:09.040 --> 02:13:11.680]   - The Sony hack was 100 terabytes, this is 1.5.
[02:13:11.680 --> 02:13:12.520]   - Yeah, yeah.
[02:13:12.520 --> 02:13:15.520]   - And I think this after the Netflix hack
[02:13:15.520 --> 02:13:17.400]   that this, I think criminals feel like,
[02:13:17.400 --> 02:13:19.040]   oh, we could probably get some money,
[02:13:19.040 --> 02:13:21.640]   the blackmailing these companies,
[02:13:21.640 --> 02:13:23.080]   the value of Game of Thrones.
[02:13:23.080 --> 02:13:25.880]   I don't, you know, is it gonna reduce the number of people
[02:13:25.880 --> 02:13:27.280]   watching if they knew what was gonna happen?
[02:13:27.280 --> 02:13:28.120]   It's just gonna kiss people off.
[02:13:28.120 --> 02:13:29.800]   - It's gonna kiss people actively encourage pirates.
[02:13:29.800 --> 02:13:31.200]   - Put it back in the news.
[02:13:31.200 --> 02:13:32.040]   I mean, they encourage it.
[02:13:32.040 --> 02:13:33.560]   It is the most pirated TV show.
[02:13:33.560 --> 02:13:36.200]   - I was gonna say, the first couple of series,
[02:13:36.200 --> 02:13:38.680]   they were like, yeah, we know it's one of the most pirated TV
[02:13:38.680 --> 02:13:40.320]   shows, and we're fine with that,
[02:13:40.320 --> 02:13:42.120]   because it gets more people actually going out
[02:13:42.120 --> 02:13:42.960]   and watching it.
[02:13:42.960 --> 02:13:43.800]   - Wow, that's in the way.
[02:13:43.800 --> 02:13:46.880]   - You know, it was like, okay, well, no fair enough,
[02:13:46.880 --> 02:13:48.560]   and it's worked.
[02:13:48.560 --> 02:13:50.920]   Everyone watched, okay, not everyone watches it,
[02:13:50.920 --> 02:13:53.400]   but I had a lot of people watch it.
[02:13:53.400 --> 02:13:56.480]   And piracy has helped build up that base of people
[02:13:56.480 --> 02:13:59.200]   who maybe they pirated the first couple of episodes,
[02:13:59.200 --> 02:14:02.040]   and then they signed on and watched the next four or five.
[02:14:02.040 --> 02:14:05.120]   That said, we're still waiting for the final book.
[02:14:05.120 --> 02:14:08.680]   So speaking of the Iron Throne Travis Kalanek,
[02:14:08.680 --> 02:14:11.760]   has hired three dragons and is about to attack
[02:14:11.760 --> 02:14:13.960]   who breed headquarters. - Three headquarters.
[02:14:13.960 --> 02:14:15.440]   (laughs)
[02:14:15.440 --> 02:14:17.160]   - Apparently this is from the information,
[02:14:17.160 --> 02:14:18.400]   big scoop from the information.
[02:14:18.400 --> 02:14:22.000]   He was prospecting among former employees and others
[02:14:22.000 --> 02:14:24.600]   to see if he should stage a proxy fight
[02:14:24.600 --> 02:14:28.240]   to try to get back in as CEO of Uber.
[02:14:28.240 --> 02:14:31.080]   Information says, well, he's not currently pursuing
[02:14:31.080 --> 02:14:34.360]   a shareholder battle, 'cause he hasn't asked anybody
[02:14:34.360 --> 02:14:37.160]   to sign any documents, but I wouldn't be surprised.
[02:14:37.160 --> 02:14:40.800]   They have yet to find a replacement for this guy.
[02:14:40.800 --> 02:14:43.720]   And all the women they've asked have said, hell no.
[02:14:43.720 --> 02:14:45.080]   - Yeah.
[02:14:45.080 --> 02:14:46.560]   I mean, their approach to Charles Ann Burger
[02:14:46.560 --> 02:14:48.200]   is like, you can't even-- - Well why would she--
[02:14:48.200 --> 02:14:50.920]   - Yeah, exactly. - Yeah, she's even
[02:14:50.920 --> 02:14:51.960]   Marissa Meyer.
[02:14:51.960 --> 02:14:54.120]   I mean, nobody wants that job.
[02:14:54.120 --> 02:14:56.440]   - No. - Meg Whitman.
[02:14:56.440 --> 02:14:57.680]   - Now Meg Whitman could do it.
[02:14:57.680 --> 02:15:00.880]   She's got the, I mean, she's used to dealing with people
[02:15:00.880 --> 02:15:02.280]   and pushing through an agenda.
[02:15:02.280 --> 02:15:04.640]   She could do it, but why would she want to?
[02:15:04.640 --> 02:15:05.480]   - Yeah, she wouldn't.
[02:15:05.480 --> 02:15:07.920]   And she took herself, didn't she take herself out of the right,
[02:15:07.920 --> 02:15:10.520]   running during a board meeting when they were talking about her?
[02:15:10.520 --> 02:15:12.280]   I mean, she liked, we know. - Sorry guys.
[02:15:12.280 --> 02:15:15.160]   - I don't usually comment on rumor, but--
[02:15:15.160 --> 02:15:17.000]   - Forget it. - I want to kill this one.
[02:15:17.000 --> 02:15:18.800]   - Dad, right now. - I mean, gender aside,
[02:15:18.800 --> 02:15:20.360]   I don't know who would, at this point,
[02:15:20.360 --> 02:15:23.440]   I don't even know who would want to step in to take that job.
[02:15:23.440 --> 02:15:24.280]   Right? - But there's--
[02:15:24.280 --> 02:15:25.120]   - I mean, there's-- - There's inheriting
[02:15:25.120 --> 02:15:26.840]   such a mess. - Isn't there a big upside though,
[02:15:26.840 --> 02:15:27.880]   if you can turn it around?
[02:15:27.880 --> 02:15:29.080]   I mean, that's a company.
[02:15:29.080 --> 02:15:31.120]   It's already valued at $60 billion.
[02:15:31.120 --> 02:15:33.720]   You know, I guess there's a run up, whatever it was.
[02:15:33.720 --> 02:15:36.320]   I mean, there's rumors that SoftBank was buying out
[02:15:36.320 --> 02:15:39.080]   benchmark for like a 30% discount off of that.
[02:15:39.080 --> 02:15:41.560]   I mean, there could be a fairly long period of time
[02:15:41.560 --> 02:15:45.880]   where it's value contracts before it rescales
[02:15:45.880 --> 02:15:48.480]   the great heights in, you know, $70 billion, whatever.
[02:15:48.480 --> 02:15:52.200]   That was a advanced public market price back in the day,
[02:15:52.200 --> 02:15:54.120]   but it's already been valued very richly.
[02:15:54.120 --> 02:15:55.680]   Oh, and if I'm Jeffrey, I'm out, right?
[02:15:55.680 --> 02:15:58.560]   I mean, you've done this whole, you've run, you know, GE.
[02:15:58.560 --> 02:16:01.200]   I don't, that would be a crazy job to take on next.
[02:16:01.200 --> 02:16:02.560]   - It's also Uber isn't profitable.
[02:16:02.560 --> 02:16:04.800]   They're burning through a staggering amount of VC.
[02:16:04.800 --> 02:16:06.160]   I mean, I don't think they have a lot of,
[02:16:06.160 --> 02:16:07.680]   even at that valuation, I don't think they have a lot
[02:16:07.680 --> 02:16:09.600]   of runway, I think it's-- - It's a year.
[02:16:09.600 --> 02:16:12.320]   - It's more than a year, but it's not a lot of years.
[02:16:12.320 --> 02:16:14.640]   It's like two years or something like that.
[02:16:14.640 --> 02:16:16.800]   I did the math a few months ago, and it was--
[02:16:16.800 --> 02:16:19.800]   - Can they go back again and say, give us more money?
[02:16:19.800 --> 02:16:21.120]   - Sure, but it would be at a lower--
[02:16:21.120 --> 02:16:22.120]   - If they did it tomorrow-- - Yeah, yeah.
[02:16:22.120 --> 02:16:24.360]   - The significantly lower, it'd be very deluded transaction
[02:16:24.360 --> 02:16:26.240]   to be significantly lower valuation,
[02:16:26.240 --> 02:16:29.120]   but they could get money, but they probably
[02:16:29.120 --> 02:16:31.320]   throttle back the marketing as a first step.
[02:16:31.320 --> 02:16:34.400]   I think that's how a lot of the money goes.
[02:16:34.400 --> 02:16:38.840]   - Yeah, it's, as I say, well, if they really want to get
[02:16:38.840 --> 02:16:41.280]   a female CEO, they should get some news,
[02:16:41.280 --> 02:16:44.320]   young, just below secret, looking to make their mark,
[02:16:44.320 --> 02:16:45.840]   and then let them go for it.
[02:16:45.840 --> 02:16:48.280]   Go into an established name. - Piper?
[02:16:48.280 --> 02:16:51.320]   - I think Piper Reese would be perfect.
[02:16:51.320 --> 02:16:53.640]   - She takes you, though, you already know that.
[02:16:53.640 --> 02:16:56.640]   - A female CEO doesn't solve the problems that are endemic to.
[02:16:56.640 --> 02:16:57.480]   - That's right, no, it doesn't.
[02:16:57.480 --> 02:16:59.320]   - It's window dressing, yeah. - Yeah.
[02:16:59.320 --> 02:17:02.880]   - And honestly, he's gonna try to stage a comeback.
[02:17:02.880 --> 02:17:06.680]   So anybody in their right, I would be surprised
[02:17:06.680 --> 02:17:11.520]   that they're gonna get anybody who's gonna be great,
[02:17:11.520 --> 02:17:14.120]   who'd be willing to take that job right now.
[02:17:14.120 --> 02:17:17.520]   - Yeah, he has a staggering amount of stock ownership,
[02:17:17.520 --> 02:17:20.160]   so his ability to wreak havoc in the boardroom.
[02:17:20.160 --> 02:17:23.840]   And there is definitely a history of CEOs
[02:17:23.840 --> 02:17:26.480]   that have been pushed out who re-engineered
[02:17:26.480 --> 02:17:29.160]   or engineered the return to the great detriment
[02:17:29.160 --> 02:17:30.680]   of the people who succeeded him.
[02:17:30.680 --> 02:17:34.360]   And you know that's gonna, that will at least be attempted here.
[02:17:34.360 --> 02:17:37.920]   - Yeah, so you're really signing up for a doozy of a brawl.
[02:17:37.920 --> 02:17:39.320]   - It's like sitting on the iron throne
[02:17:39.320 --> 02:17:41.200]   when you got three dragons coming your way.
[02:17:41.200 --> 02:17:42.120]   It's done. - Yeah.
[02:17:42.120 --> 02:17:44.120]   - I'm sorry, I got going through the mic.
[02:17:44.120 --> 02:17:46.360]   We gotta let everybody go, 'cause back east,
[02:17:46.360 --> 02:17:48.360]   it's gonna happen in about half an hour.
[02:17:48.360 --> 02:17:50.760]   We wanna give Amy web-- - I gotta go pop my popcorn.
[02:17:50.760 --> 02:17:53.280]   - A chance to get some popcorn, get some butter.
[02:17:53.280 --> 02:17:55.160]   Amy, it's been great having you again.
[02:17:55.160 --> 02:17:57.080]   Come back anytime, you're so smart.
[02:17:57.080 --> 02:17:59.240]   The book is The Signals are Talking Why.
[02:17:59.240 --> 02:18:02.200]   Today's fringe is tomorrow's mainstream.
[02:18:02.200 --> 02:18:03.320]   She's not so smart in the sense
[02:18:03.320 --> 02:18:05.360]   that she's actually telling people how she does her job,
[02:18:05.360 --> 02:18:07.200]   which is probably not a good idea.
[02:18:07.200 --> 02:18:10.360]   She's the formula for being a futurist to your very own self.
[02:18:10.360 --> 02:18:12.840]   Actually, every science fiction author should read this.
[02:18:12.840 --> 02:18:15.440]   This would be a valuable, valuable tool.
[02:18:15.440 --> 02:18:17.120]   And it really is an engaging book.
[02:18:17.120 --> 02:18:18.600]   Thank you so much for being here, Amy.
[02:18:18.600 --> 02:18:19.600]   I appreciate it. - Thank you.
[02:18:19.600 --> 02:18:20.640]   I love being here, thank you.
[02:18:20.640 --> 02:18:21.920]   - Good, always welcome.
[02:18:21.920 --> 02:18:25.920]   And as are you, Mr. Ian Thompson, my good friend,
[02:18:25.920 --> 02:18:29.480]   nothing for me to plug except go read the register.
[02:18:29.480 --> 02:18:31.760]   And I'm sure you'll be doing coverage,
[02:18:31.760 --> 02:18:34.120]   more coverage of Marcus's situation.
[02:18:34.120 --> 02:18:36.240]   - Yeah, yeah. - And we'll see
[02:18:36.240 --> 02:18:39.000]   what happens with Marcus Hutchins because...
[02:18:39.000 --> 02:18:40.640]   - I like, wanna cry killer.
[02:18:40.640 --> 02:18:43.600]   Is that what you decided to call him, the wanna cry killer?
[02:18:43.600 --> 02:18:45.160]   - Yeah. (laughs)
[02:18:45.160 --> 02:18:47.920]   - You don't understand, that's the wrong connotation.
[02:18:47.920 --> 02:18:52.160]   - Well, it did also make a really unfortunate URL
[02:18:52.160 --> 02:18:53.760]   with just wanna cry killer.
[02:18:53.760 --> 02:18:56.240]   And it was like, oh, no, hang on, no, we need to edit that.
[02:18:56.240 --> 02:18:57.600]   (laughs)
[02:18:57.600 --> 02:18:59.040]   - I wanna cry.
[02:18:59.040 --> 02:18:59.840]   - It's always a pleasure.
[02:18:59.840 --> 02:19:00.840]   Thank you for making your way out.
[02:19:00.840 --> 02:19:02.160]   - That was great for me. - Come back soon.
[02:19:02.160 --> 02:19:03.600]   - And some great, yes, fellow guests.
[02:19:03.600 --> 02:19:06.320]   - And also, yeah, congratulations to Rob Reed.
[02:19:06.320 --> 02:19:09.760]   The book just came out after on, I'm not done with it,
[02:19:09.760 --> 02:19:11.240]   but it is, you can't put it down,
[02:19:11.240 --> 02:19:14.360]   is 547 pages of really, really good stuff.
[02:19:14.360 --> 02:19:15.400]   And very challenging.
[02:19:15.400 --> 02:19:17.720]   - It was exciting. - Very exciting, yeah.
[02:19:17.720 --> 02:19:20.680]   Besides being great fiction, it's smart fiction.
[02:19:20.680 --> 02:19:23.120]   And that's what Rob's gonna be known for, I think,
[02:19:23.120 --> 02:19:23.960]   going forward.
[02:19:23.960 --> 02:19:26.800]   The Thomas Pinchon of Silicon Valley, ladies and gentlemen.
[02:19:26.800 --> 02:19:28.080]   Thank you, Rob, for being here.
[02:19:28.080 --> 02:19:29.960]   Give my best to Morgan.
[02:19:29.960 --> 02:19:31.960]   - So you're living in New York for a while, is that the plan?
[02:19:31.960 --> 02:19:34.000]   - Yeah, we've been in New York for over a year,
[02:19:34.000 --> 02:19:35.040]   and we're enjoying life.
[02:19:35.040 --> 02:19:36.760]   We kind of jokingly call it our junior year
[02:19:36.760 --> 02:19:39.960]   abroad in Manhattan, because it probably is temporary.
[02:19:39.960 --> 02:19:42.280]   - You're gonna come back to California,
[02:19:42.280 --> 02:19:43.520]   or where will you end up?
[02:19:43.520 --> 02:19:45.600]   - It's, you know, it's entirely possible.
[02:19:45.600 --> 02:19:46.880]   - If I were you. - It's entirely possible.
[02:19:46.880 --> 02:19:47.720]   - Yeah.
[02:19:47.720 --> 02:19:50.040]   - I'm just saying, Santa Rini, beautiful place.
[02:19:50.040 --> 02:19:50.880]   - Santa Rini?
[02:19:50.880 --> 02:19:52.520]   - Yeah. - Yeah.
[02:19:52.520 --> 02:19:54.680]   Lovely town, you could have a house with a blue roof.
[02:19:54.680 --> 02:19:56.760]   - I was lost there in 1988, before they had the idea.
[02:19:56.760 --> 02:19:57.800]   - It's exactly the same.
[02:19:57.800 --> 02:19:58.960]   - Somehow I doubt that.
[02:19:58.960 --> 02:20:00.920]   (both laughing)
[02:20:00.920 --> 02:20:03.560]   - Santa Rini, you could live anywhere.
[02:20:03.560 --> 02:20:05.160]   You could be anywhere, right?
[02:20:05.160 --> 02:20:06.960]   Writers, that's the beauty of being a novelist.
[02:20:06.960 --> 02:20:07.800]   - You can be anywhere, yeah.
[02:20:07.800 --> 02:20:09.920]   - You can live anywhere, anywhere in the world.
[02:20:09.920 --> 02:20:11.080]   Thank you all for being here.
[02:20:11.080 --> 02:20:12.760]   We do Twitter every Saturday, I'm sorry,
[02:20:12.760 --> 02:20:15.920]   Sunday afternoon, 3 p.m. Pacific, 6 p.m. Eastern time.
[02:20:15.920 --> 02:20:19.120]   2,200 UTC, if you wanna join us in the studio.
[02:20:19.120 --> 02:20:21.360]   Some great studio audience from all over the world,
[02:20:21.360 --> 02:20:25.080]   from Munich, and Calgary, Canada, India, and Aplus.
[02:20:25.080 --> 02:20:26.440]   And you guys just snuck in here.
[02:20:26.440 --> 02:20:27.360]   I don't know where you're from,
[02:20:27.360 --> 02:20:28.960]   but thank you all for being here.
[02:20:28.960 --> 02:20:31.080]   Email tickets@twit.tv, and we'll be glad
[02:20:31.080 --> 02:20:33.320]   to put a chair out for you.
[02:20:33.320 --> 02:20:34.760]   You can also watch the live stream
[02:20:34.760 --> 02:20:38.320]   from anywhere in the world, twit.tv/live.
[02:20:38.320 --> 02:20:40.320]   If you do that though, I invite you to be in our chatroom
[02:20:40.320 --> 02:20:43.800]   and IRC.twit.tv, join the kids, the wise-ass kids
[02:20:43.800 --> 02:20:46.640]   in the back of the class, throw in spitballs and stuff.
[02:20:46.640 --> 02:20:48.800]   That's what we do without them.
[02:20:48.800 --> 02:20:51.040]   - Oh, I always log on to the Twit thing.
[02:20:51.040 --> 02:20:52.680]   - I know, you're one of the few guests
[02:20:52.680 --> 02:20:55.000]   who actually joins the chat.
[02:20:55.000 --> 02:20:56.720]   - Well, I've always watched it on the monitor,
[02:20:56.720 --> 02:20:59.280]   and I saw you even watching it on the screen,
[02:20:59.280 --> 02:21:01.160]   so I put it up as well. - Yeah, so it's kind of fun.
[02:21:01.160 --> 02:21:03.040]   - And I even threw in a couple of...
[02:21:03.040 --> 02:21:06.320]   - We sometimes warn guests not to look.
[02:21:06.320 --> 02:21:09.240]   - I've always looked here, and it's never been that bad.
[02:21:09.240 --> 02:21:10.640]   Of course, now they're gonna get me.
[02:21:10.640 --> 02:21:12.080]   (laughing)
[02:21:12.080 --> 02:21:13.280]   - CEO Barbie.
[02:21:13.280 --> 02:21:15.020]   - That's gonna be the title of the show,
[02:21:15.020 --> 02:21:16.320]   Gary and Jia. - Oh, is it CEO Barbie?
[02:21:16.320 --> 02:21:17.160]   - Yep, yeah, yeah.
[02:21:17.160 --> 02:21:18.000]   - Yeah, yeah, yeah.
[02:21:18.000 --> 02:21:19.600]   - Glad that's my contribution.
[02:21:19.600 --> 02:21:20.760]   (laughing)
[02:21:20.760 --> 02:21:21.760]   - Good job, Amy.
[02:21:21.760 --> 02:21:25.080]   If you can't be here during the show's broadcast time,
[02:21:25.080 --> 02:21:26.080]   of course you can watch on demand.
[02:21:26.080 --> 02:21:28.400]   Everything we do really is about on demand.
[02:21:28.400 --> 02:21:31.240]   So just go to twit.tv and download a copy,
[02:21:31.240 --> 02:21:33.760]   or my favorite thing, if you,
[02:21:33.760 --> 02:21:36.280]   however you listen to podcasts, find Twit,
[02:21:36.280 --> 02:21:38.760]   and subscribe that way every Monday.
[02:21:38.760 --> 02:21:40.120]   You'll be ready as you go into work,
[02:21:40.120 --> 02:21:42.120]   you'll have a nice Twit to listen to.
[02:21:42.120 --> 02:21:43.280]   And given the length of the show,
[02:21:43.280 --> 02:21:44.720]   you can listen to it all week long.
[02:21:44.720 --> 02:21:47.200]   (laughing)
[02:21:47.200 --> 02:21:48.920]   Anything else I need to say?
[02:21:48.920 --> 02:21:51.000]   Thank you, Carson Bondi and Tanya Hall
[02:21:51.000 --> 02:21:53.840]   for booking the show and putting the run down together.
[02:21:53.840 --> 02:21:54.840]   Thank you, John Slanina,
[02:21:54.840 --> 02:21:57.600]   for making sure the microphones work.
[02:21:57.600 --> 02:21:58.840]   Thank you to be for being here.
[02:21:58.840 --> 02:21:59.680]   We'll see you next time.
[02:21:59.680 --> 02:22:01.640]   Another Twit is in the can.
[02:22:01.640 --> 02:22:02.480]   Bye-bye.
[02:22:02.480 --> 02:22:03.400]   (clapping)
[02:22:03.400 --> 02:22:04.240]   - Hey!
[02:22:04.240 --> 02:22:07.720]   - Good job!
[02:22:07.720 --> 02:22:10.800]   (upbeat music)
[02:22:10.800 --> 02:22:11.640]   ♪ Do it the Twit ♪
[02:22:11.640 --> 02:22:13.800]   ♪ Do it the Twit, baby ♪
[02:22:13.800 --> 02:22:14.640]   ♪ Do it the Twit ♪
[02:22:14.640 --> 02:22:15.640]   ♪ All right ♪

