;FFMETADATA1
title=Paste & Match Style
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=724
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:01.760]   It's time for Twit this week in Tech.
[00:00:01.760 --> 00:00:02.760]   Great panel for you.
[00:00:02.760 --> 00:00:04.540]   Today Nate Langson from Bloomberg,
[00:00:04.540 --> 00:00:06.960]   Devendra Hardewar from Engadget,
[00:00:06.960 --> 00:00:09.920]   and Anubi, you're gonna be nice to Kate O'Neill,
[00:00:09.920 --> 00:00:12.600]   the Tech Humanist, we'll talk about it.
[00:00:12.600 --> 00:00:15.400]   The wacky GE light bulb reset,
[00:00:15.400 --> 00:00:18.920]   the ambitious plans behind Facebook's new cryptocurrency
[00:00:18.920 --> 00:00:21.560]   and the SimSwap horror story.
[00:00:21.560 --> 00:00:23.520]   It's all coming up next on Twit.
[00:00:23.520 --> 00:00:28.120]   - Netcast you love.
[00:00:28.120 --> 00:00:29.640]   From people you trust.
[00:00:29.640 --> 00:00:34.960]   - This is Twit.
[00:00:34.960 --> 00:00:45.320]   - This is Twit this week at Tech.
[00:00:45.320 --> 00:00:50.320]   Episode 724 recorded Sunday, June 23rd, 2019.
[00:00:50.320 --> 00:00:52.560]   Paste and match style.
[00:00:52.560 --> 00:00:55.960]   This week at Tech is brought to you by Kep Terra.
[00:00:55.960 --> 00:00:58.560]   Find the right tools to make an informed software decision
[00:00:58.560 --> 00:00:59.720]   for your business.
[00:00:59.720 --> 00:01:04.640]   Visit Kep Terra's free website at kepterra.com/twit.
[00:01:04.640 --> 00:01:06.480]   And by Atlassian.
[00:01:06.480 --> 00:01:09.720]   Atlassian Software powers the full spectrum of collaboration
[00:01:09.720 --> 00:01:12.880]   between IT teams and the rest of your organization.
[00:01:12.880 --> 00:01:16.520]   Visit Atlassian.com/teams/IT.
[00:01:16.520 --> 00:01:20.320]   To see what IT can be by giving their products a try for free.
[00:01:20.320 --> 00:01:22.080]   And by HelloFresh.
[00:01:22.080 --> 00:01:25.920]   HelloFresh shops, plans and delivers step-by-step recipes
[00:01:25.920 --> 00:01:29.880]   and pre-measured ingredients so you can just cook, eat and enjoy.
[00:01:29.880 --> 00:01:34.360]   For $80 off your first month, go to hellofresh.com/twit80
[00:01:34.360 --> 00:01:36.080]   and use the code Twit80.
[00:01:36.080 --> 00:01:39.120]   And by Wasabi Hot Cloud Storage.
[00:01:39.120 --> 00:01:41.400]   Thinking about moving your data storage to the cloud?
[00:01:41.400 --> 00:01:44.040]   Wasabi is enterprise-class cloud storage
[00:01:44.040 --> 00:01:46.480]   at 1/5 the price of Amazon S3
[00:01:46.480 --> 00:01:48.600]   and up to six times faster
[00:01:48.600 --> 00:01:51.840]   with no hidden fees for egress or API requests.
[00:01:51.840 --> 00:01:54.200]   Calculate your settings and try Wasabi
[00:01:54.200 --> 00:01:57.800]   with free unlimited storage for a month at wasabi.com.
[00:01:57.800 --> 00:01:58.720]   Code Twit.
[00:01:58.720 --> 00:02:04.600]   It's time for Twit this week in tech to show we cover the weeks.
[00:02:04.600 --> 00:02:09.200]   Tech News, a great panel this week back
[00:02:09.200 --> 00:02:14.520]   for a reprise of his starring role in 2018.
[00:02:14.520 --> 00:02:15.480]   I don't know what I'm saying.
[00:02:15.480 --> 00:02:18.960]   Devindra Hardowar is your senior editor at Engadget.
[00:02:18.960 --> 00:02:20.880]   Hello, Devindra.
[00:02:20.880 --> 00:02:22.200]   Hello, happy to be here.
[00:02:22.200 --> 00:02:25.560]   There was some new borns, how they doing?
[00:02:25.560 --> 00:02:26.560]   She's good.
[00:02:26.560 --> 00:02:29.120]   Sophia just hit what, eight months?
[00:02:29.120 --> 00:02:33.200]   So big milestone, she's happy, she's laughing.
[00:02:33.200 --> 00:02:35.720]   Not the sleep we're still working on, but she's great.
[00:02:35.720 --> 00:02:37.680]   And we've already done some Engadget stuff with her.
[00:02:37.680 --> 00:02:40.160]   She's a budding gadget reviewer at this point.
[00:02:40.160 --> 00:02:41.960]   Budding gadget reviewer.
[00:02:41.960 --> 00:02:42.800]   Yeah.
[00:02:42.800 --> 00:02:43.960]   Is she reviewing baby gadgets?
[00:02:43.960 --> 00:02:47.160]   Yeah. Yeah, there's so many and so many are terrible.
[00:02:47.160 --> 00:02:49.240]   And she has a good bad gadget face.
[00:02:49.240 --> 00:02:51.360]   I saw your mama, your momma, Rue.
[00:02:51.360 --> 00:02:54.080]   Oh, the mama, Rue.
[00:02:54.080 --> 00:02:55.240]   She doesn't like it at all.
[00:02:55.240 --> 00:02:57.160]   She doesn't like the mama, Rue?
[00:02:57.160 --> 00:02:57.960]   She hates it.
[00:02:57.960 --> 00:03:00.840]   And I know some parents who've had kids who love it.
[00:03:00.840 --> 00:03:02.440]   It's just every kid is different.
[00:03:02.440 --> 00:03:03.320]   That's the main advice.
[00:03:03.320 --> 00:03:05.840]   So reviewing baby gadgets is even
[00:03:05.840 --> 00:03:07.320]   weirder than normal gadgets.
[00:03:07.320 --> 00:03:08.760]   It's all about personal preference.
[00:03:08.760 --> 00:03:11.560]   Very, very, yeah, babies, you know how they are.
[00:03:11.560 --> 00:03:13.280]   Yeah.
[00:03:13.280 --> 00:03:16.520]   Also back for more Nate Langston, the tech editor
[00:03:16.520 --> 00:03:18.880]   at Bloomberg, joining us from the UK.
[00:03:18.880 --> 00:03:20.160]   Hi, Nate.
[00:03:20.160 --> 00:03:20.880]   Hey, Leo.
[00:03:20.880 --> 00:03:22.520]   Hey, guys, I'm very sleepy.
[00:03:22.520 --> 00:03:24.800]   So I'm going to do my best to promise, I promise.
[00:03:24.800 --> 00:03:28.960]   If you start to drift off, just get back on those drums.
[00:03:28.960 --> 00:03:30.600]   Pound the skins.
[00:03:30.600 --> 00:03:32.640]   Wake us up, too.
[00:03:32.640 --> 00:03:34.520]   And I know how much you desperately want.
[00:03:34.520 --> 00:03:36.760]   Every time I'm on the show, I do a drum solo.
[00:03:36.760 --> 00:03:37.480]   I can't help it.
[00:03:37.480 --> 00:03:38.840]   You got a drum kit behind you.
[00:03:38.840 --> 00:03:41.120]   It's going to come up.
[00:03:41.120 --> 00:03:41.800]   It's true.
[00:03:41.800 --> 00:03:43.880]   There it is.
[00:03:43.880 --> 00:03:44.800]   Trump's not the truth.
[00:03:44.800 --> 00:03:47.240]   You haven't played those in 12 years.
[00:03:47.240 --> 00:03:49.480]   No, I haven't played those in about 12 hours, actually.
[00:03:49.480 --> 00:03:50.360]   Oh, good.
[00:03:50.360 --> 00:03:50.880]   Good.
[00:03:50.880 --> 00:03:51.240]   Yeah.
[00:03:51.240 --> 00:03:51.560]   Nice.
[00:03:51.560 --> 00:03:52.080]   Frequent.
[00:03:52.080 --> 00:03:53.200]   Yes.
[00:03:53.200 --> 00:03:55.080]   And we have a brand new panelist.
[00:03:55.080 --> 00:03:57.640]   I'm very excited to have joined us.
[00:03:57.640 --> 00:04:00.160]   Kato Neil, she is a humanist.
[00:04:00.160 --> 00:04:00.840]   What the hell?
[00:04:00.840 --> 00:04:01.720]   And tech.
[00:04:01.720 --> 00:04:02.240]   Hi, Kate.
[00:04:02.240 --> 00:04:04.320]   Great to have you.
[00:04:04.320 --> 00:04:05.040]   Thank you.
[00:04:05.040 --> 00:04:09.280]   Kioinsights.com, her book, Tech Humanist, is great.
[00:04:09.280 --> 00:04:12.080]   And I think we had you on triangulation talking about it.
[00:04:12.080 --> 00:04:13.160]   So we thought this--
[00:04:13.160 --> 00:04:13.680]   That's right.
[00:04:13.680 --> 00:04:14.040]   Yeah.
[00:04:14.040 --> 00:04:17.000]   This would be great to have Kate on Twit, too.
[00:04:17.000 --> 00:04:20.000]   Because we need some humanity and technology these days.
[00:04:20.000 --> 00:04:21.000]   OK.
[00:04:21.000 --> 00:04:23.240]   Indeed, technology needs humanity, for sure.
[00:04:23.240 --> 00:04:25.480]   And the first thing, of course, you and the vendor
[00:04:25.480 --> 00:04:29.200]   started talking about is the GE light bulb.
[00:04:29.200 --> 00:04:31.680]   What's that all about?
[00:04:31.680 --> 00:04:35.400]   That was a big flurry on Twitter over the last few days
[00:04:35.400 --> 00:04:39.560]   about this nonsense video that GE released for people
[00:04:39.560 --> 00:04:42.520]   who owned a certain light bulb that had firmware that needed
[00:04:42.520 --> 00:04:43.680]   to be reset.
[00:04:43.680 --> 00:04:47.720]   And they needed to reset it by starting and restarting.
[00:04:47.720 --> 00:04:50.240]   This is the C by GE Smart Tip.
[00:04:50.240 --> 00:04:53.320]   How to factory reset your GE light bulbs.
[00:04:53.320 --> 00:04:53.600]   Go ahead.
[00:04:53.600 --> 00:04:55.080]   You could turn the sound on.
[00:04:55.080 --> 00:04:56.640]   And apps that it's connected to.
[00:04:56.640 --> 00:04:57.240]   Yes.
[00:04:57.240 --> 00:04:58.400]   He's a happy person.
[00:04:58.400 --> 00:04:59.560]   Factory reset process.
[00:04:59.560 --> 00:05:00.280]   OK.
[00:05:00.280 --> 00:05:03.400]   Which depend on the generation of bulbs and the firmware
[00:05:03.400 --> 00:05:04.040]   you're running on.
[00:05:04.040 --> 00:05:04.600]   OK.
[00:05:04.600 --> 00:05:05.760]   Here's the first process.
[00:05:05.760 --> 00:05:06.240]   Yes.
[00:05:06.240 --> 00:05:08.360]   Designed for bulbs with this package.
[00:05:08.360 --> 00:05:08.840]   OK.
[00:05:08.840 --> 00:05:10.480]   Or for firmware version 2.8.
[00:05:10.480 --> 00:05:12.320]   I didn't save the package.
[00:05:12.320 --> 00:05:14.840]   Start with your bulb off for at least five seconds.
[00:05:14.840 --> 00:05:15.920]   Five seconds off.
[00:05:15.920 --> 00:05:17.280]   OK.
[00:05:17.280 --> 00:05:20.000]   Honey, can you find the packaging that those bulbs came in?
[00:05:20.000 --> 00:05:20.960]   Well, for eight seconds.
[00:05:20.960 --> 00:05:23.240]   Turn on for eight seconds, OK.
[00:05:23.240 --> 00:05:28.080]   Honey, the packaging, the bulb packaging.
[00:05:28.080 --> 00:05:29.600]   Turn off for two seconds.
[00:05:29.600 --> 00:05:30.880]   No, I know we threw it out.
[00:05:30.880 --> 00:05:31.800]   But I need it because--
[00:05:31.800 --> 00:05:33.240]   Turn on for eight seconds.
[00:05:33.240 --> 00:05:34.240]   OK.
[00:05:34.240 --> 00:05:37.520]   How many seconds?
[00:05:37.520 --> 00:05:38.280]   Oh, no.
[00:05:38.280 --> 00:05:39.200]   I've lost track.
[00:05:39.200 --> 00:05:40.240]   Off for two seconds.
[00:05:40.240 --> 00:05:41.240]   OK.
[00:05:41.240 --> 00:05:41.840]   OK.
[00:05:41.840 --> 00:05:43.600]   Turn on for eight seconds.
[00:05:43.600 --> 00:05:45.160]   This is serious.
[00:05:46.040 --> 00:05:47.960]   [LAUGHTER]
[00:05:47.960 --> 00:05:50.960]   Honey, turn off for two seconds.
[00:05:50.960 --> 00:05:52.400]   Bring me my stopwatch, honey.
[00:05:52.400 --> 00:05:54.720]   Turn on for eight seconds.
[00:05:54.720 --> 00:05:57.280]   By the way, they're not stopping for eight seconds.
[00:05:57.280 --> 00:05:57.960]   Either, are they?
[00:05:57.960 --> 00:05:58.960]   Are they?
[00:05:58.960 --> 00:05:59.640]   It's just--
[00:05:59.640 --> 00:06:01.400]   You can't follow the video and do this.
[00:06:01.400 --> 00:06:03.280]   Two seconds.
[00:06:03.280 --> 00:06:03.960]   That wasn't--
[00:06:03.960 --> 00:06:04.440]   On.
[00:06:04.440 --> 00:06:05.440]   For eight seconds.
[00:06:05.440 --> 00:06:06.440]   [LAUGHTER]
[00:06:06.440 --> 00:06:07.440]   [LAUGHTER]
[00:06:07.440 --> 00:06:08.440]   [LAUGHTER]
[00:06:08.440 --> 00:06:10.960]   I could believe it.
[00:06:10.960 --> 00:06:12.120]   Who coded this?
[00:06:12.120 --> 00:06:13.120]   For two seconds.
[00:06:13.120 --> 00:06:13.880]   We're still going.
[00:06:13.880 --> 00:06:16.000]   And then turn it on one last time.
[00:06:16.000 --> 00:06:16.920]   Oh, I'm last.
[00:06:16.920 --> 00:06:19.480]   All will flash on and off three times
[00:06:19.480 --> 00:06:21.760]   to show that the reset was successful.
[00:06:21.760 --> 00:06:24.120]   If it doesn't, your ball may be running
[00:06:24.120 --> 00:06:25.800]   on an older version of firmware.
[00:06:25.800 --> 00:06:26.320]   Oh, no.
[00:06:26.320 --> 00:06:27.600]   I need to try the second factor.
[00:06:27.600 --> 00:06:28.160]   Wait, there's more.
[00:06:28.160 --> 00:06:29.640]   [LAUGHTER]
[00:06:29.640 --> 00:06:32.760]   Which is designed for C by GE balls with this packet.
[00:06:32.760 --> 00:06:34.920]   Honey, can you find that packaging?
[00:06:34.920 --> 00:06:36.360]   Two point seven or earlier.
[00:06:36.360 --> 00:06:37.240]   Oh, my god.
[00:06:37.240 --> 00:06:38.080]   Ready?
[00:06:38.080 --> 00:06:38.760]   OK.
[00:06:38.760 --> 00:06:39.280]   Starbuckers--
[00:06:39.280 --> 00:06:40.440]   No wonder--
[00:06:40.440 --> 00:06:41.920]   Off for at least five seconds.
[00:06:41.920 --> 00:06:44.320]   People think we're nuts.
[00:06:44.320 --> 00:06:45.320]   This is why the home--
[00:06:45.320 --> 00:06:46.560]   this is why home automation--
[00:06:46.560 --> 00:06:47.480]   For eight seconds.
[00:06:47.480 --> 00:06:49.480]   [LAUGHTER]
[00:06:49.480 --> 00:06:51.360]   Do you think they wrote code?
[00:06:51.360 --> 00:06:53.160]   How did this happen?
[00:06:53.160 --> 00:06:55.040]   Turn off for two seconds.
[00:06:55.040 --> 00:06:56.080]   They're just very bad at that.
[00:06:56.080 --> 00:06:57.480]   Turn on for two seconds.
[00:06:57.480 --> 00:06:58.440]   [LAUGHTER]
[00:06:58.440 --> 00:07:01.120]   Turn off for two seconds.
[00:07:01.120 --> 00:07:01.640]   By the way.
[00:07:01.640 --> 00:07:02.360]   Turn on for two seconds.
[00:07:02.360 --> 00:07:04.560]   Completely coincidentally, this is how you reboot
[00:07:04.560 --> 00:07:06.200]   Twitter as well.
[00:07:06.200 --> 00:07:06.720]   [LAUGHTER]
[00:07:06.720 --> 00:07:08.400]   Turn on for two seconds.
[00:07:08.400 --> 00:07:09.520]   Oh, my god.
[00:07:09.520 --> 00:07:10.880]   Turn off for two seconds.
[00:07:10.880 --> 00:07:11.880]   [LAUGHTER]
[00:07:11.880 --> 00:07:13.680]   I thought I'd write reboot baby gadget for eight seconds.
[00:07:13.680 --> 00:07:14.680]   Oh, yeah.
[00:07:14.680 --> 00:07:15.680]   [LAUGHTER]
[00:07:15.680 --> 00:07:16.680]   Boy.
[00:07:16.680 --> 00:07:17.680]   OK, enough.
[00:07:17.680 --> 00:07:20.000]   [LAUGHTER]
[00:07:20.000 --> 00:07:21.200]   Turn off for eight seconds.
[00:07:21.200 --> 00:07:22.760]   I didn't believe it when you told me.
[00:07:22.760 --> 00:07:24.760]   I had to watch that video.
[00:07:24.760 --> 00:07:27.800]   It's a piece of work.
[00:07:27.800 --> 00:07:28.960]   Wow.
[00:07:28.960 --> 00:07:29.960]   Yeah.
[00:07:29.960 --> 00:07:31.600]   OK.
[00:07:31.600 --> 00:07:33.920]   So tech humanist, is that tech or human?
[00:07:33.920 --> 00:07:35.160]   I don't know.
[00:07:35.160 --> 00:07:36.160]   It is--
[00:07:36.160 --> 00:07:37.160]   It is inhumane.
[00:07:37.160 --> 00:07:38.680]   It's inhumane.
[00:07:38.680 --> 00:07:40.040]   Maybe that's your next book, Kate.
[00:07:40.040 --> 00:07:41.560]   Tech inhumanist.
[00:07:41.560 --> 00:07:43.720]   Inhumane tech experiences.
[00:07:43.720 --> 00:07:45.720]   I mean, that'll be the featured experience.
[00:07:45.720 --> 00:07:46.320]   Oh, my goodness.
[00:07:46.320 --> 00:07:47.720]   Crazy.
[00:07:47.720 --> 00:07:50.160]   So I missed that whole thing because apparently I don't pay
[00:07:50.160 --> 00:07:52.040]   close enough attention to Twitter.
[00:07:52.040 --> 00:07:53.080]   So I apologize.
[00:07:53.080 --> 00:07:56.880]   But now we know the whole story.
[00:07:56.880 --> 00:07:58.640]   The story I was going to start with
[00:07:58.640 --> 00:08:01.200]   is I thought kind of interesting, which
[00:08:01.200 --> 00:08:06.200]   is Facebook's decision to make their own cryptocurrency.
[00:08:06.200 --> 00:08:10.320]   Libra coin, it's well, Facebook says not ours.
[00:08:10.320 --> 00:08:11.800]   But it is kind of theirs.
[00:08:11.800 --> 00:08:14.040]   I don't think if it were just another cryptocurrency
[00:08:14.040 --> 00:08:17.760]   without Facebook's a premature, it wouldn't be of any interest.
[00:08:17.760 --> 00:08:19.200]   How many cryptocurrencies are there?
[00:08:19.200 --> 00:08:21.400]   There are many dozens.
[00:08:21.400 --> 00:08:25.520]   This one has a large group of people behind it
[00:08:25.520 --> 00:08:31.360]   besides Facebook, including PayPal and MasterCard and Visa.
[00:08:31.360 --> 00:08:33.600]   No banks, oddly enough.
[00:08:33.600 --> 00:08:35.920]   They've Facebook open sourced the technology,
[00:08:35.920 --> 00:08:37.800]   even though they developed it in-house.
[00:08:37.800 --> 00:08:41.560]   And according to Wired Seed Control of the Blockchain
[00:08:41.560 --> 00:08:45.840]   to the Neutral Libra Association,
[00:08:45.840 --> 00:08:49.040]   Wired called it the Switzerland of Digital Coinage
[00:08:49.040 --> 00:08:51.560]   because it's in Switzerland.
[00:08:51.560 --> 00:08:54.600]   Each of the initial hundred founding members,
[00:08:54.600 --> 00:08:57.600]   including Facebook, will invest at least $10 million
[00:08:57.600 --> 00:09:03.240]   to fund operations and receive interest earned off the reserve.
[00:09:03.240 --> 00:09:05.720]   There are some NGO members, Kiva, for instance.
[00:09:05.720 --> 00:09:07.680]   The micro lending company is part of this.
[00:09:07.680 --> 00:09:10.120]   And that's what Facebook's pitching this as,
[00:09:10.120 --> 00:09:14.360]   is a way to bring banking to the next billion people.
[00:09:14.360 --> 00:09:17.720]   I can wait for them not to pay any fees.
[00:09:17.720 --> 00:09:18.360]   It's pretty great.
[00:09:18.360 --> 00:09:19.120]   Yeah.
[00:09:19.120 --> 00:09:21.960]   So in that regard, it's kind of like a Facebook's internet.org.
[00:09:21.960 --> 00:09:26.680]   It seems like a beneficial to humanity thing to do.
[00:09:26.680 --> 00:09:28.800]   There was some question about privacy.
[00:09:28.800 --> 00:09:30.400]   In other words, would Facebook get caught,
[00:09:30.400 --> 00:09:32.080]   get all the transactions?
[00:09:32.080 --> 00:09:34.320]   They say no.
[00:09:34.320 --> 00:09:35.120]   It'll be private.
[00:09:35.120 --> 00:09:39.320]   So if Defender, why does Facebook want to start a cryptocurrency?
[00:09:39.320 --> 00:09:40.400]   I mean, it's a big question.
[00:09:40.400 --> 00:09:41.840]   I will warn you guys up front.
[00:09:41.840 --> 00:09:44.840]   I was off this entire week, so I was trying to stay away.
[00:09:44.840 --> 00:09:46.480]   I don't even know.
[00:09:46.480 --> 00:09:47.640]   I don't even know any.
[00:09:47.640 --> 00:09:49.240]   But you knew about the light bulb.
[00:09:49.240 --> 00:09:49.760]   Yeah.
[00:09:49.760 --> 00:09:50.680]   I knew about the light bulb.
[00:09:50.680 --> 00:09:51.200]   What was that?
[00:09:51.200 --> 00:09:51.640]   It's just Twitter.
[00:09:51.640 --> 00:09:52.400]   That's just fun.
[00:09:52.400 --> 00:09:53.680]   That's just culture.
[00:09:53.680 --> 00:09:55.840]   But the Facebook cryptocurrency, you know...
[00:09:55.840 --> 00:09:57.560]   Well, Doc, you could say this off the top of your head
[00:09:57.560 --> 00:09:58.920]   because we don't really know anything.
[00:09:58.920 --> 00:10:00.840]   It's not going to happen until next year.
[00:10:00.840 --> 00:10:03.520]   But I think you can speculate that, you know,
[00:10:03.520 --> 00:10:05.240]   having your cryptocurrency smart for them
[00:10:05.240 --> 00:10:07.560]   because they won't have to pay any transaction fees
[00:10:07.560 --> 00:10:10.640]   to credit card companies or to banks or anything,
[00:10:10.640 --> 00:10:13.120]   which I think most companies want.
[00:10:13.120 --> 00:10:16.440]   They want kind of the freedom to handle payments like that.
[00:10:16.440 --> 00:10:18.960]   And also Facebook is so big, you know,
[00:10:18.960 --> 00:10:21.120]   we've said this many times on the show.
[00:10:21.120 --> 00:10:23.920]   It's bigger than most countries, you know,
[00:10:23.920 --> 00:10:25.120]   in terms of the amount of users.
[00:10:25.120 --> 00:10:28.720]   So it kind of makes sense for them to have their own specific currency.
[00:10:28.720 --> 00:10:30.400]   Actually, it's surprising it took this long.
[00:10:30.400 --> 00:10:32.360]   Facebook's bigger than any country.
[00:10:32.360 --> 00:10:32.720]   Yeah.
[00:10:32.720 --> 00:10:34.240]   If you take the two and a half,
[00:10:34.240 --> 00:10:37.080]   what is it, two and a half billion people use Facebook?
[00:10:37.080 --> 00:10:38.680]   That's huge.
[00:10:38.680 --> 00:10:39.400]   Huge.
[00:10:39.400 --> 00:10:43.320]   And of course, Facebook Messenger and WhatsApp would clearly...
[00:10:43.320 --> 00:10:46.000]   It strikes me in Facebook wants to be WeChat
[00:10:46.000 --> 00:10:47.320]   for the rest of the world, right?
[00:10:47.320 --> 00:10:50.920]   Because WeChat runs commerce in China.
[00:10:50.920 --> 00:10:53.040]   But on the other hand,
[00:10:53.040 --> 00:10:57.120]   that Facebook association sticks in some people's...
[00:10:57.120 --> 00:10:58.000]   Crawl.
[00:10:58.000 --> 00:10:58.800]   Kate, what...
[00:10:58.800 --> 00:11:00.520]   It's like we shouldn't trust anything they say.
[00:11:00.520 --> 00:11:01.720]   Yeah, I don't.
[00:11:01.720 --> 00:11:02.320]   Yeah.
[00:11:02.320 --> 00:11:03.320]   Clearly.
[00:11:03.320 --> 00:11:06.160]   Kate, which side do you come...
[00:11:06.160 --> 00:11:08.120]   This is a good thing, yeah?
[00:11:08.120 --> 00:11:10.320]   Or is it an evil plan?
[00:11:10.320 --> 00:11:13.320]   I mean, I think there's certainly good that can come of it.
[00:11:13.320 --> 00:11:16.040]   And I think for whatever good that can come of it,
[00:11:16.040 --> 00:11:17.720]   like the banking, the unbanked,
[00:11:17.720 --> 00:11:20.400]   and you know, sort of getting rid of these
[00:11:20.400 --> 00:11:24.120]   international transaction fees and the kinds of things
[00:11:24.120 --> 00:11:29.120]   that prohibit people from being able to support each other financially
[00:11:29.120 --> 00:11:31.160]   across global currencies,
[00:11:31.160 --> 00:11:33.160]   yeah, that it makes a lot of sense.
[00:11:33.160 --> 00:11:37.440]   But I do think that it's wise of anyone to be suspicious
[00:11:37.440 --> 00:11:40.280]   of how it's going to play out with Facebook's reputation
[00:11:40.280 --> 00:11:43.480]   for data breaches and overreaches.
[00:11:43.480 --> 00:11:45.480]   Not to mention if you're a bank.
[00:11:45.480 --> 00:11:47.200]   (laughs)
[00:11:47.200 --> 00:11:48.960]   This could be terrifying.
[00:11:48.960 --> 00:11:53.160]   A threat to national sovereignty too,
[00:11:53.160 --> 00:11:56.040]   because in fact, India has already banned...
[00:11:56.040 --> 00:11:57.680]   You could go to jail for 10 years in India
[00:11:57.680 --> 00:12:00.160]   for using any cryptocurrency.
[00:12:00.160 --> 00:12:02.160]   Because it threatens the government, right?
[00:12:02.160 --> 00:12:06.160]   One of the government's main functions is to create and support a currency.
[00:12:06.160 --> 00:12:17.160]   I mean, one of the important things here is that these coins are going to be backed
[00:12:17.160 --> 00:12:19.160]   by real assets.
[00:12:19.160 --> 00:12:27.160]   So it's not just Bitcoin that has a cryptographic value
[00:12:27.160 --> 00:12:30.160]   that has ascribed to it, and that's it.
[00:12:30.160 --> 00:12:34.160]   This will have money apparently all over the world to back it.
[00:12:34.160 --> 00:12:35.160]   We said $4.
[00:12:35.160 --> 00:12:36.760]   They said, what,
[00:12:36.760 --> 00:12:39.360]   "Dollars, euros, yen, and marks."
[00:12:39.360 --> 00:12:41.160]   I can't remember what the fourth one was.
[00:12:41.160 --> 00:12:42.160]   Yeah.
[00:12:42.160 --> 00:12:43.160]   There are no marks.
[00:12:43.160 --> 00:12:45.160]   That's the last one.
[00:12:45.160 --> 00:12:46.160]   Pounds.
[00:12:46.160 --> 00:12:49.160]   Yeah, we're a bit volatile right now.
[00:12:49.160 --> 00:12:51.160]   Thank you, Brexit.
[00:12:51.160 --> 00:12:54.160]   But I think that's one of the flaws with Bitcoin for sure is you don't know.
[00:12:54.160 --> 00:12:55.160]   It's now $10,000 again.
[00:12:55.160 --> 00:12:57.160]   You don't know what it's going to be worth.
[00:12:57.160 --> 00:13:01.160]   And somebody could spend millions of dollars for a pizza,
[00:13:01.160 --> 00:13:03.160]   not knowing what that Bitcoin is worth.
[00:13:03.160 --> 00:13:05.160]   So that's one thing that I think Facebook wants.
[00:13:05.160 --> 00:13:08.160]   They're not pegging it to the dollar, but they want it to be stable.
[00:13:08.160 --> 00:13:09.160]   Yeah, exactly.
[00:13:09.160 --> 00:13:11.160]   They want it to be stable.
[00:13:11.160 --> 00:13:14.160]   And I think the other thing to be, you know,
[00:13:14.160 --> 00:13:17.160]   I think we have to be very mindful of is the fact that,
[00:13:17.160 --> 00:13:21.160]   number one, Facebook has a massive trust issue in the first instance anyway.
[00:13:21.160 --> 00:13:24.160]   A lot of people are very skeptical of Facebook because of its reach.
[00:13:24.160 --> 00:13:35.160]   Also, we haven't really had a massively successful use of crypto in the kind of mainstream world.
[00:13:35.160 --> 00:13:40.160]   You know, we've seen companies like Dell accept it for payments for laptops and things.
[00:13:40.160 --> 00:13:46.160]   We've seen stores accept it, but we've also seen plenty of stores drop support of it because the volatility is too great.
[00:13:46.160 --> 00:13:51.160]   You know, there's, I don't think we have a massively successful example to say,
[00:13:51.160 --> 00:13:54.160]   "This is the reason why this can be a success."
[00:13:54.160 --> 00:13:58.160]   And I think that paired with the lack of trust that Facebook inherently has right now
[00:13:58.160 --> 00:14:01.160]   makes me very skeptical whether this can take off,
[00:14:01.160 --> 00:14:04.160]   and that's only magnified by the fact that no banks are involved in this.
[00:14:04.160 --> 00:14:06.160]   So I think they're going to have a really tough time.
[00:14:06.160 --> 00:14:09.160]   One of the other problems, there's really two problems with Bitcoin.
[00:14:09.160 --> 00:14:10.160]   One is the volatility.
[00:14:10.160 --> 00:14:15.160]   The other is just due to the algorithm for the blockchain,
[00:14:15.160 --> 00:14:18.160]   transactions could take a really long time.
[00:14:18.160 --> 00:14:24.160]   If you're in a store to buy a pair of pants, you can't wait half an hour, an hour, two hours for that transaction to go through.
[00:14:24.160 --> 00:14:26.160]   It has to go through much faster.
[00:14:26.160 --> 00:14:31.160]   Facebook says that they're going to be able to process tens of thousands of transactions a second,
[00:14:31.160 --> 00:14:38.160]   but that's not a huge number compared to the potential if this becomes used by billions of customers worldwide.
[00:14:38.160 --> 00:14:42.160]   It is interesting that they're trying to keep it open, at least in some respect.
[00:14:42.160 --> 00:14:45.160]   Knowing Facebook, like, it seems like their first approach to everything.
[00:14:45.160 --> 00:14:48.160]   When they got into VR, Oculus was this closed off thing.
[00:14:48.160 --> 00:14:52.160]   It took a while for them to even cooperate with SteamVR and everything.
[00:14:52.160 --> 00:14:56.160]   So the openness from the get-go is interesting to me.
[00:14:56.160 --> 00:15:04.160]   I just wonder, is it open in name only because Facebook is probably the most powerful force among that open source organization there?
[00:15:04.160 --> 00:15:08.160]   Kate, I read too much science fiction, so maybe you could get down to Earth.
[00:15:08.160 --> 00:15:14.160]   But if you read a lot of science fiction, and a lot of it talks about corporate governance in the future,
[00:15:14.160 --> 00:15:17.160]   there won't be countries that will be corporations.
[00:15:17.160 --> 00:15:22.160]   And you're looking in our current affairs for the seeds of that future.
[00:15:22.160 --> 00:15:29.160]   This, to me, could be in 20, 30, 40 years we might look back and say this was a watershed moment.
[00:15:29.160 --> 00:15:35.160]   When Silicon Valley decided, screw dollars, we're going to use our own currency.
[00:15:35.160 --> 00:15:38.160]   Yeah, I think it's true.
[00:15:38.160 --> 00:15:42.160]   I think there are shades of that going on.
[00:15:42.160 --> 00:15:52.160]   But I think there's enough going on that Facebook and its partners tried to put into place to alleviate some of those doubts and concerns.
[00:15:52.160 --> 00:16:01.160]   Certainly this consortium of 28 or whatever partners do look like the kind of partners that you would want to have involved in this process.
[00:16:01.160 --> 00:16:10.160]   As you mentioned, Kiva and other NGOs that sort of provide this kind of stabilizing humanistic force to the whole thing.
[00:16:10.160 --> 00:16:16.160]   But I do think that the association with Facebook is just going to be really hard to overcome.
[00:16:16.160 --> 00:16:26.160]   That there's no sense that anybody's going to have that this is not a play to monetize the back end of messaging.
[00:16:26.160 --> 00:16:32.160]   And the content that Facebook is increasingly pushing to the dark corners of its platform,
[00:16:32.160 --> 00:16:39.160]   where people are going to be clustered in groups and that sort of thing is going to need some sort of reach,
[00:16:39.160 --> 00:16:41.160]   some sort of monetization.
[00:16:41.160 --> 00:16:49.160]   This seems like a way to get around having monetization through the news feed advertising and the news feed.
[00:16:49.160 --> 00:17:01.160]   And it does seem like they're sort of thinking two steps ahead to get to where they're diversified and well set up for continued leadership and sprawl.
[00:17:01.160 --> 00:17:11.160]   This is completely weirdly paranoid of me, but I can just imagine, I thought two years ago Mark Zuckerberg, especially with his 50th state tour, he was going to read for president.
[00:17:11.160 --> 00:17:23.160]   I thought he thought, and I think this is kind of the, I could see the Silicon Valley billionaires thinking, and not just Silicon Valley, Jeff Bezos thinking, you know, this democracy thing, this is a mess.
[00:17:23.160 --> 00:17:28.160]   We just let out, we know we're smart, we can figure this out, let's just let us do it.
[00:17:28.160 --> 00:17:32.160]   I think more efficient. Yeah, it'd be more efficient. Let us handle it.
[00:17:32.160 --> 00:17:40.160]   And then I obviously, Zuck realized after he went to Congress and people thought he was a robot, okay, maybe I'm not going to make it.
[00:17:40.160 --> 00:17:45.160]   I couldn't become president, but who, you know, why would I want to take a step down?
[00:17:45.160 --> 00:17:51.160]   I don't need to be president. I'm CEO. All I need is a currency.
[00:17:51.160 --> 00:18:05.160]   I mean, seriously, this is what if global coin, what if Libra became the, I could see how what there's no, if there's no VIG going to the banks, it became a dominant form of payment.
[00:18:05.160 --> 00:18:09.160]   It's how you do internet. We use PayPal. We have international hosts.
[00:18:09.160 --> 00:18:12.160]   We have to use what's easier for us to use PayPal to pay them.
[00:18:12.160 --> 00:18:15.160]   Yeah, the PayPal support is interesting on this. Yeah, for sure.
[00:18:15.160 --> 00:18:20.160]   I'm sure there's enough revenue to go around. I'm sure they'll figure out a way to get something out of it.
[00:18:20.160 --> 00:18:33.160]   But to me, this doesn't strike me from, at least certainly from Facebook's point of view, as either economic play, like we're going to make a lot of money on this, although they might have a hand in every transaction going forward.
[00:18:33.160 --> 00:18:38.160]   That's Amazon's plan, right? And I don't think this is a privacy thing. I think that's missing the poll point of this.
[00:18:38.160 --> 00:18:44.160]   If you say, "Oh, Facebook's just doing this so they can get more information about all transactions," I think this is a power play.
[00:18:44.160 --> 00:18:49.160]   I think this is Facebook saying, "Who needs a government? Who needs banks?"
[00:18:49.160 --> 00:18:53.160]   Imagine just being paranoid, Devindra.
[00:18:53.160 --> 00:18:56.160]   No, I think that's probably a good way to look at it.
[00:18:56.160 --> 00:19:02.160]   To me, it's a little less scary than something like the portal that they announced their video chats.
[00:19:02.160 --> 00:19:04.160]   And I should point out, I have two of them.
[00:19:04.160 --> 00:19:09.160]   Oh, come on, Leo. Well, I thought I need two. One to talk to the other one.
[00:19:09.160 --> 00:19:14.160]   Yeah, it makes sense. I mean, which is good tech, but probably came at the wrong time for them.
[00:19:14.160 --> 00:19:19.160]   Whereas this, yeah, it does seem like a power play. It does seem like, how can we just be a global superpower?
[00:19:19.160 --> 00:19:23.160]   What if we didn't need banks? What if we could just manage our own ecosystem?
[00:19:23.160 --> 00:19:28.160]   It's preparing us for the world of the future where we're all living in VR and just buying stuff digitally.
[00:19:28.160 --> 00:19:29.160]   It all makes sense.
[00:19:29.160 --> 00:19:35.160]   Yeah. And I guess I'm not the only one who's paranoid because the French government has already said we're investigating
[00:19:35.160 --> 00:19:42.160]   and Facebook is going to have to go to Congress next month to explain yourself Facebook.
[00:19:42.160 --> 00:19:49.160]   Because there's some, I think, reasonable concern in government about, well, what is Facebook up to?
[00:19:49.160 --> 00:19:51.160]   So maybe I'm not completely paranoid.
[00:19:51.160 --> 00:19:55.160]   Although, lately getting grilled by Congress is just kind of...
[00:19:55.160 --> 00:20:00.160]   It's table stakes for anybody who wants to play this game.
[00:20:00.160 --> 00:20:09.160]   Yeah, I thought it was interesting on that the Congress investigation articles that I've been reading that it's both...
[00:20:09.160 --> 00:20:13.160]   It's a sort of a bipartisan suspicion of this plan.
[00:20:13.160 --> 00:20:14.160]   Right.
[00:20:14.160 --> 00:20:21.160]   And you can easily see what sort of stakes that each side is thinking about.
[00:20:21.160 --> 00:20:26.160]   One side's thinking about the sort of social and data privacy implications.
[00:20:26.160 --> 00:20:28.160]   And one side is thinking about, "But banks!"
[00:20:28.160 --> 00:20:29.160]   Right.
[00:20:29.160 --> 00:20:32.160]   That goes stepping in and taking over banks.
[00:20:32.160 --> 00:20:38.160]   So it is interesting that it is a bipartisan concern that's being expressed here.
[00:20:38.160 --> 00:20:46.160]   If you think about banks, not just central banks, national banks, but just banks, commercial banks,
[00:20:46.160 --> 00:20:52.160]   they really are an important constituency to Congress, I would imagine.
[00:20:52.160 --> 00:20:54.160]   They spend a lot of money on Congress.
[00:20:54.160 --> 00:20:59.160]   Congress has a certain sense of responsibility to banks.
[00:20:59.160 --> 00:21:01.160]   And I'm sure that's what some of this is.
[00:21:01.160 --> 00:21:06.160]   And honestly, I...
[00:21:06.160 --> 00:21:10.160]   I'm so torn about this because as you probably gathered, I'm not a fan of Facebook.
[00:21:10.160 --> 00:21:19.160]   But at the same time, anything that disintermediates, especially banks, I think that's a good thing.
[00:21:19.160 --> 00:21:22.160]   So I don't know where to come down on this one.
[00:21:22.160 --> 00:21:38.160]   Well, it's so easy too to sort of fast forward a few years and imagine that, you know, the kind of investigation that's been going on over the last couple of years about the Russian involvement in the 2016 Presidential election and the campaigns.
[00:21:38.160 --> 00:21:47.160]   It's so easy to imagine a couple of years down the road, this whole obfuscation that happens as a result of this all happening through your Swiss bank.
[00:21:47.160 --> 00:21:49.160]   And with, you know, there's some data trail, of course.
[00:21:49.160 --> 00:21:50.160]   Imagine that.
[00:21:50.160 --> 00:21:51.160]   Yeah.
[00:21:51.160 --> 00:21:57.160]   Instead of subpoenaing Deutsche Bank, who are you going to subpoena if it's a Libra coin?
[00:21:57.160 --> 00:21:58.160]   Yeah.
[00:21:58.160 --> 00:21:59.160]   That is a real concern.
[00:21:59.160 --> 00:22:01.160]   This would be a money launderers dream.
[00:22:01.160 --> 00:22:05.160]   Can I just throw one other massive bit of skepticism in here?
[00:22:05.160 --> 00:22:06.160]   Yeah.
[00:22:06.160 --> 00:22:09.160]   I feel like a massive naysayer on all this.
[00:22:09.160 --> 00:22:10.160]   No, that's good.
[00:22:10.160 --> 00:22:15.160]   Have we really examined why Facebook is doing this, right?
[00:22:15.160 --> 00:22:20.160]   Because Facebook gets about 99% of its all its revenue from advertising.
[00:22:20.160 --> 00:22:23.160]   And we know how that side of its business model works.
[00:22:23.160 --> 00:22:26.160]   But what is its real motivation here?
[00:22:26.160 --> 00:22:35.160]   Like I've heard talk about that the more people who have better access to money and banking and, you know, revenue in general mean they may be more likely to want to spend on advertising.
[00:22:35.160 --> 00:22:37.160]   And therefore that's good for Facebook.
[00:22:37.160 --> 00:22:39.160]   And I get that if that's the reason.
[00:22:39.160 --> 00:22:49.160]   But I, maybe it's just my inherent distrust for most of what Facebook does that makes me just want to question why exactly is it doing this?
[00:22:49.160 --> 00:22:53.160]   And why is Facebook the right company to do this and to pull this off?
[00:22:53.160 --> 00:22:57.160]   What has it been telling other partners about why it's the right one to spearhead all this?
[00:22:57.160 --> 00:23:01.160]   You know, amidst all this stuff that's going on with Facebook, why now and why Facebook?
[00:23:01.160 --> 00:23:03.160]   I don't have a good answer for that.
[00:23:03.160 --> 00:23:06.160]   It's a question I suppose to to everyone else.
[00:23:06.160 --> 00:23:12.160]   I just I find myself to be maybe more skeptical than some other people I've heard talking about this.
[00:23:12.160 --> 00:23:14.160]   I'm not sure why that is.
[00:23:14.160 --> 00:23:16.160]   It's the same thing as what Facebook said.
[00:23:16.160 --> 00:23:18.160]   Let's do internet.org.
[00:23:18.160 --> 00:23:21.160]   We want to, you know, oh, this is a wonderful thing.
[00:23:21.160 --> 00:23:25.160]   We're going to give the internet to the next billion people who don't have access to the internet.
[00:23:25.160 --> 00:23:28.160]   And then it turns out for them the internet means Facebook.
[00:23:28.160 --> 00:23:35.160]   And India very interestingly said this is this is just another form of colonialism.
[00:23:35.160 --> 00:23:37.160]   We have plenty of experience with that.
[00:23:37.160 --> 00:23:38.160]   No, thank you.
[00:23:38.160 --> 00:23:39.160]   Yeah.
[00:23:39.160 --> 00:23:50.160]   And if you look at if you look at something that's happening with WhatsApp, which Facebook owns, you know, WhatsApp wants to be more of a business to consumer operation.
[00:23:50.160 --> 00:23:53.160]   You know, it wants to have a business model around.
[00:23:53.160 --> 00:24:00.160]   Well, this is the place you can go for real time chat between between a subscriber and the company being subscribed to.
[00:24:00.160 --> 00:24:11.160]   It wouldn't take a giant leap of imagination to think that this could be built into something like that, where actually you can do your transactions on this platform.
[00:24:11.160 --> 00:24:13.160]   You can send and receive money on this platform.
[00:24:13.160 --> 00:24:17.160]   And maybe there would be an incentive to Facebook for you to do that.
[00:24:17.160 --> 00:24:22.160]   Plus it wouldn't necessarily cost Facebook anything to do that, you know, on an individual transaction basis.
[00:24:22.160 --> 00:24:26.160]   And I wonder if that's part of the long term ambition here.
[00:24:26.160 --> 00:24:34.160]   Yeah, it's a lot of speculation, but I feel what you're saying, Leo, that it seems it's hard not to feel like there's a conspiracy here.
[00:24:34.160 --> 00:24:36.160]   And I think it's not just Facebook, right?
[00:24:36.160 --> 00:24:44.160]   They have a ton of partners and it feels like them working together with all these other technology companies and a lot of VC firms, you know, and Drizyn Horowitz.
[00:24:44.160 --> 00:24:45.160]   Union Square Ventures.
[00:24:45.160 --> 00:24:47.160]   I'm just looking at the list here.
[00:24:47.160 --> 00:24:51.160]   Okay, but VC firms, they're just covering their bets.
[00:24:51.160 --> 00:24:55.160]   If anybody, you know, a VC firm says, yeah, we'll give you 10 million.
[00:24:55.160 --> 00:24:56.160]   It's pocket change.
[00:24:56.160 --> 00:24:58.160]   We just want to make sure if that's the next big thing that we're here.
[00:24:58.160 --> 00:25:00.160]   I don't think that that's a vote of confidence.
[00:25:00.160 --> 00:25:03.160]   It's just, you know, hedging our bets.
[00:25:03.160 --> 00:25:10.160]   No, I, yeah, I haven't really seen them jump into a lot of digital currencies, even though it's like could be the big hot thing now.
[00:25:10.160 --> 00:25:14.160]   It just seems like this is a really interesting group of people, right?
[00:25:14.160 --> 00:25:16.160]   Well, yeah, Facebook weren't involved.
[00:25:16.160 --> 00:25:17.160]   Yeah.
[00:25:17.160 --> 00:25:18.160]   Yep.
[00:25:18.160 --> 00:25:19.160]   We wouldn't be talking about it.
[00:25:19.160 --> 00:25:21.160]   It would just be another coin.
[00:25:21.160 --> 00:25:22.160]   Yeah.
[00:25:22.160 --> 00:25:26.160]   This seems like the beginning of like the ultimate digital currency, basically.
[00:25:26.160 --> 00:25:28.160]   And they have these VC firms.
[00:25:28.160 --> 00:25:36.160]   They have Stripe, which is, you know, a company that's handling credit card transactions for a lot of, you know, online companies and things like that.
[00:25:36.160 --> 00:25:42.160]   And PayPal, which we mentioned before, which is still big, even though it seems like an old school company, they're still huge.
[00:25:42.160 --> 00:25:43.160]   They have Ben Mow.
[00:25:43.160 --> 00:25:53.160]   Like there's a lot of, there's a lot of really interesting partnership here that I think could, it's more interesting than just Facebook doing this on their own, basically.
[00:25:53.160 --> 00:26:07.160]   I think one of the things that makes it interesting is not only that the Facebook's name is part of this collective, but that Facebook has been kind of the name that we've associated with this idea for the last few weeks as the rumors have been flying around.
[00:26:07.160 --> 00:26:17.160]   And that also they're building the Calibra wallet that's going to be at least initially the way that you sort of manage the money and push things around.
[00:26:17.160 --> 00:26:26.160]   So it will supposedly that there will be third parties that will build wallets other than Calibra, but I think that's an interesting function at the beginning too.
[00:26:26.160 --> 00:26:32.160]   And I still come back to, for me, I just see Facebook looking into their own future and seeing that they're going to need to fundamentally turn their business model kind of inside out in order to get into their own future.
[00:26:32.160 --> 00:26:44.160]   And seeing that they're going to need to fundamentally turn their business model kind of inside out in order to deal with all of the complaints about content moderation and all the complaints that they're getting about regulation and data privacy.
[00:26:44.160 --> 00:26:52.160]   Their content that's been pushed to the news feed is going to need to be pulled back into the groups and private messaging.
[00:26:52.160 --> 00:27:00.160]   And so they need to think in a completely different way about how they monetize that and how they make that a successful model going forward.
[00:27:00.160 --> 00:27:03.160]   And to me, this just feels like one piece of that equation.
[00:27:03.160 --> 00:27:10.160]   I think it might boil down to any Nate, you ask an interesting question, but I think a fairly impenetrable question.
[00:27:10.160 --> 00:27:17.160]   What is their intent? No, I mean, I don't know if you can look in the Facebook soul and see what it's intended for anything.
[00:27:17.160 --> 00:27:19.160]   Well, they've already held us for a half or so.
[00:27:19.160 --> 00:27:25.160]   Facebook has no soul. That's the point. It's got the dark soulless eyes of a man eating shark.
[00:27:25.160 --> 00:27:32.160]   But no, it all seriousness. It kind of comes down to how you feel about Facebook.
[00:27:32.160 --> 00:27:43.160]   If you think Facebook is true to what Mark says all the time, which is it's all about connection man, this would be just one another way to do that, right?
[00:27:43.160 --> 00:27:47.160]   And disintermeeting banks and governments. That couldn't be bad.
[00:27:47.160 --> 00:27:53.160]   But maybe it's also about getting a piece of every financial transaction in the future.
[00:27:53.160 --> 00:28:00.160]   And the other question is, if not Facebook, who, if Google did this, would we be having the same conversation? You bad?
[00:28:00.160 --> 00:28:09.160]   And if not Facebook, China, I mean, I'm sure that there are other people trying to do this.
[00:28:09.160 --> 00:28:14.160]   So maybe Facebook is the best possible choice to do this. I don't know.
[00:28:14.160 --> 00:28:21.160]   Fundamentally, I don't know how different it is for Facebook to be doing this from China doing this.
[00:28:21.160 --> 00:28:26.160]   In some ways, I mean, there's still a lot of surveillance that kind of goes into the equation.
[00:28:26.160 --> 00:28:31.160]   And, you know, the amount of trust on the table is still just as much in question.
[00:28:31.160 --> 00:28:39.160]   I don't know if it were a choice between China and Facebook. Now you got me.
[00:28:39.160 --> 00:28:45.160]   I think I choose Facebook. Yeah.
[00:28:45.160 --> 00:28:50.160]   Maybe this is the future of Facebook, right? I mean, it's still a really young company.
[00:28:50.160 --> 00:28:55.160]   And in 50 years time, if Facebook's still around and somebody came back from the future and said,
[00:28:55.160 --> 00:29:01.160]   Facebook is the global standard for international borderless payments.
[00:29:01.160 --> 00:29:06.160]   And that's it. Like a massive one world bank. I could believe it.
[00:29:06.160 --> 00:29:11.160]   I mean, you look at a company like Nokia, which is my favorite example that used to make toilet paper.
[00:29:11.160 --> 00:29:14.160]   And then it's a decade later.
[00:29:14.160 --> 00:29:22.160]   It's a bit of rubber and stuff. Like if someone came back in from 2003 and said to this person buying Nokia toilet paper
[00:29:22.160 --> 00:29:26.160]   that in the future was going to make most of the world smartphones, you'd be like,
[00:29:26.160 --> 00:29:32.160]   "Hmm, how?" Not sure. Maybe Facebook can do that. I don't know. But I'm skeptical for now.
[00:29:32.160 --> 00:29:36.160]   I just think it's a fascinating subject. And it feels to me like a watershed moment.
[00:29:36.160 --> 00:29:43.160]   And it might end up not being. But it feels like this could be a moment you'd look back in time and say,
[00:29:43.160 --> 00:29:48.160]   "Yeah, that's when Facebook started to take over." Oh, God.
[00:29:48.160 --> 00:29:55.160]   That's when? Oh, God. Fascinating subject. Yeah. Facebook is in a lot of heat, of course.
[00:29:55.160 --> 00:30:03.160]   The latest from Casey Newton, who does such a good job, he's once again blowing the lid off moderation on Facebook,
[00:30:03.160 --> 00:30:11.160]   his article on the Verge, talking about the Tampa content moderators. And what a nightmare.
[00:30:11.160 --> 00:30:16.160]   First of all, I think an important thing to point out is that they don't work for Facebook.
[00:30:16.160 --> 00:30:24.160]   They work for a contractor called Cognizant, who has a $200 million deal with Facebook for two years to hire
[00:30:24.160 --> 00:30:32.160]   and deploy these content moderators. It sounds like a nightmare job. Casey got, once again,
[00:30:32.160 --> 00:30:38.160]   got a number of current and former moderators to talk to him about what's going on there.
[00:30:38.160 --> 00:30:47.160]   And it's just depressing as hell. Conditions, he reported first on the Phoenix site,
[00:30:47.160 --> 00:30:53.160]   which is how he got on-train to the Tampa site. He said, "Conditions of the Phoenix site have not improved since I visited
[00:30:53.160 --> 00:31:00.160]   last week some employees were sent home after an infestation of bed bugs was discovered in the office.
[00:31:00.160 --> 00:31:07.160]   The second time that's happened, Cognizant's statement, they also run the Phoenix office,
[00:31:07.160 --> 00:31:12.160]   was, "Well, bed bugs can be found in virtually every place people tend together, including the workplace.
[00:31:12.160 --> 00:31:19.160]   I don't think we've ever had bed bugs here. No, no bed bugs here."
[00:31:19.160 --> 00:31:23.160]   They're naturally recurring pests. You can get anywhere.
[00:31:23.160 --> 00:31:28.160]   Oh my God. The stories that he described, this is Tampa.
[00:31:28.160 --> 00:31:33.160]   Contractors told me that Cognizant had lured them away. This is the thing that's really depressing from less demanding jobs
[00:31:33.160 --> 00:31:38.160]   by promising regular schedules, bonuses, and career development. None of that.
[00:31:38.160 --> 00:31:45.160]   None of that. They reneged on. Contractors described a filthy workplace in which they regularly find pubic hair
[00:31:45.160 --> 00:31:51.160]   and other bodily waste at their workstations. Employees said managers laugh off her
[00:31:51.160 --> 00:31:56.160]   ignore sexual harassment and threats of violence. The use of marijuana is so prevalent,
[00:31:56.160 --> 00:32:02.160]   a manager joked it in all hands, meaning he'd gotten a contact high walking in the door.
[00:32:02.160 --> 00:32:08.160]   You know what? I don't blame him for getting high because what a crappy job.
[00:32:08.160 --> 00:32:17.160]   He talked to one guy who saw a video, he was asked to review a video of a horrific animal abuse.
[00:32:17.160 --> 00:32:24.160]   The guy who was a dog lover couldn't take it. He flagged it. Facebook said, "No, no. That's okay. We're going to let it through."
[00:32:24.160 --> 00:32:32.160]   At which point that video got reshared and the poor moderator had to see it again and again and again and again.
[00:32:32.160 --> 00:32:40.160]   For $15 an hour, which I should point out is almost twice the minimum wage in Florida.
[00:32:40.160 --> 00:32:45.160]   They kept reposting it again and again, he said, pounding the table as he spoke.
[00:32:45.160 --> 00:32:49.160]   It made me so angry. I had to listen to it screams all day.
[00:32:49.160 --> 00:32:53.160]   I don't know what there is to say about this. I mean, this is part of the problem.
[00:32:53.160 --> 00:33:00.160]   Facebook and an all of Silicon Valley, Google too, tends to say, "Oh, no, there's technological solutions to obviously,
[00:33:00.160 --> 00:33:05.160]   when you have a giant platform as we do, people are people. Some people are going to misbehave.
[00:33:05.160 --> 00:33:08.160]   There's technological solutions. There's AI solutions."
[00:33:08.160 --> 00:33:13.160]   It seems like that they just hire more people. 30,000 people now work for Facebook and content moderation,
[00:33:13.160 --> 00:33:17.160]   which sounds like one of the worst jobs I can imagine.
[00:33:17.160 --> 00:33:25.160]   There's an episode of HBO's Chernobyl of the people whose job it is to go and just kill all the irradiated pets.
[00:33:25.160 --> 00:33:29.160]   Spoilers, I guess. But one of the worst jobs I can imagine.
[00:33:29.160 --> 00:33:35.160]   This seems like the modern equivalent of that, basically, having to live through this sort of content.
[00:33:35.160 --> 00:33:40.160]   It's not like they can just skip it. They're required to watch 15 to 20 seconds of these videos,
[00:33:40.160 --> 00:33:44.160]   just to make sure that they're flagging it appropriately.
[00:33:44.160 --> 00:33:48.160]   This is a thankless job, but it also goes to show all these tech companies.
[00:33:48.160 --> 00:33:52.160]   They talk about the holy algorithm that could fix this eventually.
[00:33:52.160 --> 00:33:55.160]   They're not really going to have any time soon. Right now, it's all human help.
[00:33:55.160 --> 00:34:00.160]   They farm out this stuff to these contract companies, which just don't care.
[00:34:00.160 --> 00:34:05.160]   They have a really low level of employee care.
[00:34:05.160 --> 00:34:08.160]   They're separated by Facebook and Google.
[00:34:08.160 --> 00:34:10.160]   You don't even really work for Facebook.
[00:34:10.160 --> 00:34:11.160]   It's the shame.
[00:34:11.160 --> 00:34:12.160]   Exactly.
[00:34:12.160 --> 00:34:16.160]   Just an awful, sad thing.
[00:34:16.160 --> 00:34:19.160]   I don't know if there's anything to say about this.
[00:34:19.160 --> 00:34:25.160]   I don't know if you can even really blame Facebook, although what do they make at $15 billion a quarter or something like that?
[00:34:25.160 --> 00:34:35.160]   Yeah. I think you can blame them a little for not putting the conditions in place that even through a contractor that are livable, manageable conditions.
[00:34:35.160 --> 00:34:43.160]   These are horrible jobs. They're doing horrible work, but at least the workplace that they're in could be something that is not disgusting.
[00:34:43.160 --> 00:34:49.160]   They could be being provided with competent psychiatric help, which it sounds like they're not.
[00:34:49.160 --> 00:34:50.160]   Nope.
[00:34:50.160 --> 00:34:58.160]   There's just any number of opportunities that Facebook has to make this situation better through its contractor.
[00:34:58.160 --> 00:35:01.160]   It's not Facebook's fault per se.
[00:35:01.160 --> 00:35:03.160]   It's not cognizance fault per se.
[00:35:03.160 --> 00:35:12.160]   It's that the scale of the content problem is so vast that they really need to double down and triple down on the resources they're throwing at this.
[00:35:12.160 --> 00:35:21.160]   I hope that they are. It sounds like the person they have now in charge of this at Facebook is looking to do some earnest improvements over the situation.
[00:35:21.160 --> 00:35:26.160]   I really hope he's effective, but it's complicated. It's very complicated.
[00:35:26.160 --> 00:35:33.160]   You see in this one line in here that one of the people that Casey Newton interviews says, "I asked him what he thought needed to change.
[00:35:33.160 --> 00:35:36.160]   He says, "I think Facebook needs to shut down."
[00:35:36.160 --> 00:35:37.160]   Wow.
[00:35:37.160 --> 00:35:38.160]   Yeah.
[00:35:38.160 --> 00:35:50.160]   Well, I guess it does raise the question. I'm sure you talk about this in your work, Kate, that is it possible to have a social network that doesn't descend into nightmarish hell?
[00:35:50.160 --> 00:35:53.160]   I don't know if it's possible.
[00:35:53.160 --> 00:36:07.160]   I do know that a lot of people have already been down this road in the academic landscape and have evaluated what happens when we're given the benefit of quasi anonymity and the reach that we're given.
[00:36:07.160 --> 00:36:15.160]   When these platforms and what happens when you try to moderate that? It's all levels of complexity happening at the same time.
[00:36:15.160 --> 00:36:21.160]   It feeds into the best and the worst of human nature. It's hard.
[00:36:21.160 --> 00:36:31.160]   It brings up this quote that Tim Cook said this week or last week about if you build a chaos factory, you can't not be responsible for the chaos or however he said it.
[00:36:31.160 --> 00:36:33.160]   That's a good point.
[00:36:33.160 --> 00:36:39.160]   Yeah, you have to be responsible for this. Facebook has to be responsible for the chaos that's been unleashed.
[00:36:39.160 --> 00:36:43.160]   There's certainly benefiting from it. They have enough resources.
[00:36:43.160 --> 00:36:49.160]   This is their opportunity to step up and really make something good happen here or better than as bad as it is.
[00:36:49.160 --> 00:36:54.160]   It reminds me a lot of the stories we heard about Amazon, warehouse workers.
[00:36:54.160 --> 00:37:04.160]   It just feels like this is the modern world we live in. I feel bad for our kids. I really do.
[00:37:04.160 --> 00:37:09.160]   So much good comes of these platforms. That's true too. We can't ignore that.
[00:37:09.160 --> 00:37:15.160]   So much good. There's a lot of complications that arise out of it.
[00:37:15.160 --> 00:37:19.160]   People are the collateral damage. We need to do something about that.
[00:37:19.160 --> 00:37:24.160]   Facebook needs to do something about that. Regulations perhaps need to do something about that.
[00:37:24.160 --> 00:37:31.160]   It's something very complicated that needs to be unwoven and figured out at a level that has not been solved before.
[00:37:31.160 --> 00:37:37.160]   As you guys have talked about, it's the biggest company. It represents a kind of a country.
[00:37:37.160 --> 00:37:40.160]   So we need to be thinking on that scale with the solutions.
[00:37:40.160 --> 00:37:47.160]   And with all due respect to Tim Cook, but Apple's had its own problems with contract workers in China.
[00:37:47.160 --> 00:37:52.160]   It feels like Apple's tried to do something about it. Whether they've done so successfully, I can't be sure.
[00:37:52.160 --> 00:37:57.160]   But you've got to take responsibility for it.
[00:37:57.160 --> 00:38:01.160]   It's something I think Twitter is learning too.
[00:38:01.160 --> 00:38:12.160]   It's certainly not as if Twitter doesn't have its share of trolls and bots and all kinds of disinformation and nuttiness that happens on that platform.
[00:38:12.160 --> 00:38:16.160]   It's just the nature of having that kind of scale and reach.
[00:38:16.160 --> 00:38:22.160]   And people using it to good advantages and bad advantages.
[00:38:22.160 --> 00:38:26.160]   So they need to be doing something to moderate that for sure.
[00:38:26.160 --> 00:38:29.160]   Taking responsibility definitely seems like the good first step.
[00:38:29.160 --> 00:38:34.160]   And I think for the past few years, even that much, even getting that far seemed impossible.
[00:38:34.160 --> 00:38:38.160]   Because these companies just wouldn't listen to a lot of the complaints or would say,
[00:38:38.160 --> 00:38:42.160]   "Oh yeah, we have algorithms dealing with this or something like that."
[00:38:42.160 --> 00:38:48.160]   So at least now it seems more like Facebook, Twitter, Facebook at least is looking a little more at what it's doing.
[00:38:48.160 --> 00:38:57.160]   I'm sure I'm thinking that whole Zuckerberg 50 State tour, maybe it was about a potential presidential run.
[00:38:57.160 --> 00:39:00.160]   But it also seemed like, what did we do?
[00:39:00.160 --> 00:39:03.160]   How did Facebook affect the 2016 election?
[00:39:03.160 --> 00:39:05.160]   How did we break this democracy?
[00:39:05.160 --> 00:39:07.160]   Or how did we basically help to break it?
[00:39:07.160 --> 00:39:10.160]   Did they ever admit it that they broke democracy though?
[00:39:10.160 --> 00:39:15.160]   I don't think so, even though we know about the rest of the world's control farms and everything.
[00:39:15.160 --> 00:39:17.160]   Yeah, certainly not like that.
[00:39:17.160 --> 00:39:20.160]   But I think that tour alone showed a certain amount of responsibilities.
[00:39:20.160 --> 00:39:25.160]   Like maybe I have this thing that has so greatly influenced this country.
[00:39:25.160 --> 00:39:28.160]   I should go see how that's happened.
[00:39:28.160 --> 00:39:36.160]   With these sorts of things, these contract farms basically seem like these are labor practices that we've been talking about.
[00:39:36.160 --> 00:39:37.160]   Oh yeah.
[00:39:37.160 --> 00:39:38.160]   Oh yeah.
[00:39:38.160 --> 00:39:39.160]   Oh yeah.
[00:39:39.160 --> 00:39:40.160]   Oh yeah.
[00:39:40.160 --> 00:39:41.160]   Even before tech.
[00:39:41.160 --> 00:39:44.160]   And it's just the basics that we can't get down and it's a problem inherent in America too.
[00:39:44.160 --> 00:39:46.160]   Like we just don't treat our workers that well.
[00:39:46.160 --> 00:39:50.160]   And certainly everybody's looking at their bottom line and they wanted the cheapest, easiest solution.
[00:39:50.160 --> 00:39:56.160]   That's kind of a bigger thing we need to work through as a society to really fix it for all these tech companies too.
[00:39:56.160 --> 00:39:57.160]   I don't want to.
[00:39:57.160 --> 00:40:05.160]   I feel like last couple of years, we've all focused at least a tweet on the dystopian,
[00:40:05.160 --> 00:40:06.160]   that we're creating.
[00:40:06.160 --> 00:40:13.160]   And I don't want to imply that there isn't so many wonderful and good things coming of it.
[00:40:13.160 --> 00:40:18.160]   It's just that it's hard to miss the scary things that are also happening.
[00:40:18.160 --> 00:40:19.160]   Yeah, because they're getting bigger.
[00:40:19.160 --> 00:40:21.160]   They're bigger and more impactful than ever.
[00:40:21.160 --> 00:40:22.160]   So it's hard to avoid that.
[00:40:22.160 --> 00:40:28.160]   Well, it's clear that because technology, you know, when I started doing this, technology was a little back alley.
[00:40:28.160 --> 00:40:35.160]   It wasn't like life, it was just the nerds gathering to build their little z80 machines.
[00:40:35.160 --> 00:40:38.160]   It wasn't important.
[00:40:38.160 --> 00:40:39.160]   It was fun.
[00:40:39.160 --> 00:40:40.160]   It was cool.
[00:40:40.160 --> 00:40:41.160]   It was a hobby.
[00:40:41.160 --> 00:40:42.160]   It was the toy store.
[00:40:42.160 --> 00:40:49.160]   But now as technology is directly woven into the fabric of everything we do, these bigger issues have emerged.
[00:40:49.160 --> 00:40:56.160]   You know, when you're typing in a checkbook program in BASIC, you don't...
[00:40:56.160 --> 00:40:57.160]   None of this comes up.
[00:40:57.160 --> 00:40:58.160]   It's not an issue.
[00:40:58.160 --> 00:41:01.160]   We have deeper, like, non-technical problems to solve now.
[00:41:01.160 --> 00:41:02.160]   Right.
[00:41:02.160 --> 00:41:04.160]   So, yeah, that philosophy degree may come in handy.
[00:41:04.160 --> 00:41:06.160]   It certainly has for, like, dealing with writing about this stuff.
[00:41:06.160 --> 00:41:07.160]   Yeah.
[00:41:07.160 --> 00:41:08.160]   So that's interesting, because, Devindra, you too.
[00:41:08.160 --> 00:41:13.160]   I mean, when you start... when you started doing this, you were covering fun stuff.
[00:41:13.160 --> 00:41:14.160]   Yeah.
[00:41:14.160 --> 00:41:16.160]   I mean, all fun stuff, certainly.
[00:41:16.160 --> 00:41:21.160]   Like, when I got into tech, honestly, Leo, by watching you guys in tech TV, all that stuff back in the days...
[00:41:21.160 --> 00:41:22.160]   It was still fun back then.
[00:41:22.160 --> 00:41:23.160]   It was so much fun.
[00:41:23.160 --> 00:41:24.160]   It was just exciting.
[00:41:24.160 --> 00:41:36.160]   But tech, you know, I think over the past decade, tech has won, certainly in certain respects, like, in terms of being the dominant culture, being the dominant business culture, pervading consumer lives.
[00:41:36.160 --> 00:41:41.160]   Like, tech is... we all revolve our lives around tech now in so many ways, even if people don't think about that.
[00:41:41.160 --> 00:41:46.160]   And, yeah, now is the time to stop and think about, like, the human cost to all that.
[00:41:46.160 --> 00:41:48.160]   So, that's why I like the tech humanist angle.
[00:41:48.160 --> 00:41:49.160]   It's certainly great.
[00:41:49.160 --> 00:41:52.160]   But, yeah, these companies need to think a lot harder in general.
[00:41:52.160 --> 00:42:01.160]   Yeah, I kind of want to go back to the days of talking about how to install, you know, Red Hat Linux on a P90 processor.
[00:42:01.160 --> 00:42:03.160]   It was just... it was a simpler time.
[00:42:03.160 --> 00:42:04.160]   Simpler times.
[00:42:04.160 --> 00:42:05.160]   It was a simpler time.
[00:42:05.160 --> 00:42:07.160]   I think it was still a lot of fun to be had around that stuff.
[00:42:07.160 --> 00:42:19.160]   I do think that, you know, people like Davinder who have the philosophy background and a lot of other folks who are bringing, you know, the liberal arts discussions into tech are bringing a lot of value.
[00:42:19.160 --> 00:42:26.160]   Because a lot of what happened with the playing around with the toys of tech, I think sort of disambiguated it.
[00:42:26.160 --> 00:42:31.160]   You know, it sort of divorced it from the reality of what happens at scale with this stuff.
[00:42:31.160 --> 00:42:38.160]   And I think that's part of why we're facing these consequences now is that we've looked at technology as, "Eh, it's just fun."
[00:42:38.160 --> 00:42:42.160]   You know, and then it's taken on scale that has impacted our lives in very real ways.
[00:42:42.160 --> 00:43:09.160]   So it takes people coming in with that philosophy background, with the ethics backgrounds, with the, you know, the rigor of looking at the human condition across society and culture and saying, you know, how do we make this where it actually is, does have the potential to use the powers of emerging technologies of automation and the scale of AI and all of that to actually make human life better, to solve things like climate catastrophe through the powers of AI.
[00:43:09.160 --> 00:43:12.160]   And I think that all is still possible, and we can harness that.
[00:43:12.160 --> 00:43:19.160]   That's why you're on the show, Kate. Seriously. Because I need somebody, I need a coach to say, "It's going to be okay."
[00:43:19.160 --> 00:43:26.160]   Kate didn't read science fiction as a kid. She studied German in college, and she still believes in humanity.
[00:43:26.160 --> 00:43:32.160]   I think it's, I don't want to say it's going to be okay. I think it's only going to be okay if we do the work.
[00:43:32.160 --> 00:43:33.160]   Okay.
[00:43:33.160 --> 00:43:35.160]   You know, so that's what we got to do.
[00:43:35.160 --> 00:43:37.160]   Yeah. Yeah. We got to do that.
[00:43:37.160 --> 00:43:40.160]   I don't want to say a lot of science fiction does teach the humanity thing.
[00:43:40.160 --> 00:43:42.160]   Oh, it does. I was teasing.
[00:43:42.160 --> 00:43:51.160]   Like reading Asimov or something. Like it's, it is all about the potential for this stuff to be good, but deep down, it's, it's the human story coming out, basically, that's important.
[00:43:51.160 --> 00:43:57.160]   Yeah. The best science fiction is, is not, it's not about machines and robots. It's about people.
[00:43:57.160 --> 00:44:03.160]   Yeah. I agree with you. And I also agree with Kate that we need more human in our tech.
[00:44:03.160 --> 00:44:07.160]   And so it, please inject that into the show at any point.
[00:44:07.160 --> 00:44:08.160]   Thanks.
[00:44:08.160 --> 00:44:09.160]   At any point you can, you can think of.
[00:44:09.160 --> 00:44:20.160]   Uh, Devindra Hardewar is here. He's senior editor at Engadget Kato Neil, her book Tech Humanist, is great and was the subject of a recent triangulation with Denise Howell.
[00:44:20.160 --> 00:44:22.160]   If you get a chance to listen to it, please do.
[00:44:22.160 --> 00:44:29.160]   And of course, Nate Langston, who is here from Bloomberg and is trying to stay awake.
[00:44:29.160 --> 00:44:34.160]   Because it's the middle of the night in the UK.
[00:44:34.160 --> 00:44:37.160]   Our show today brought to you by Kaptera, my friends.
[00:44:37.160 --> 00:44:38.160]   I had a caller.
[00:44:38.160 --> 00:44:44.160]   What was the, called the radio show yesterday saying, uh, I'm working for an association.
[00:44:44.160 --> 00:44:47.160]   We need software to keep track of our associates and the accounts.
[00:44:47.160 --> 00:44:50.160]   And I said, I don't know, but you know where I know you can go.
[00:44:50.160 --> 00:44:53.160]   And I was able to give a nice little plug for our sponsor, Kaptera.
[00:44:53.160 --> 00:44:56.160]   Don't call me if you need business software.
[00:44:56.160 --> 00:45:04.160]   Don't Google if you need business software. There's a directory online that has every business program out there organized into 700 specific categories.
[00:45:04.160 --> 00:45:15.160]   Obvious stuff like email marketing, CMS, IT service, SEO, workflow management, direct line of business stuff like veterinary office, management, or yoga studio management.
[00:45:15.160 --> 00:45:17.160]   Every program, modern software.
[00:45:17.160 --> 00:45:24.160]   So if you're suffering with something written for Windows 95, you can move to something that really suits that works well.
[00:45:24.160 --> 00:45:29.160]   It doesn't require use Internet Explorer eight or Vista.
[00:45:29.160 --> 00:45:32.160]   It's a Kaptera, Kaptera.com/twit.
[00:45:32.160 --> 00:45:34.160]   And what I love is Kaptera.
[00:45:34.160 --> 00:45:37.160]   We've been kind of counting down over the last few months.
[00:45:37.160 --> 00:45:40.160]   They're getting very close now to 1 million reviews.
[00:45:40.160 --> 00:45:43.160]   They get about 1000 new reviews every single day.
[00:45:43.160 --> 00:45:45.160]   Actual software users.
[00:45:45.160 --> 00:45:46.160]   They're very careful.
[00:45:46.160 --> 00:45:47.160]   They vet the reviews.
[00:45:47.160 --> 00:45:52.160]   So when you look for software in Kaptera, there's dry cleaning software to run a dry cleaning establishment.
[00:45:52.160 --> 00:45:55.160]   First, you're going to find every program that does that.
[00:45:55.160 --> 00:46:03.160]   You can then narrow it down by ratings, by how it works, whether it's on premises or on your hard drive or in the cloud and how many different does it handle.
[00:46:03.160 --> 00:46:05.160]   But keeping what does it do?
[00:46:05.160 --> 00:46:13.160]   Narrow it down, set up a comparison chart side by side, and then read the reviews because the reviews are so valuable.
[00:46:13.160 --> 00:46:19.160]   With 30,000 fresh reviews every month, you know that there's going to be somebody whose reviews start job.
[00:46:19.160 --> 00:46:22.160]   Start job. What a good name for dry cleaning software.
[00:46:22.160 --> 00:46:23.160]   Start job.
[00:46:23.160 --> 00:46:26.160]   And it is going to say, "That's actually...look at that five star reviews.
[00:46:26.160 --> 00:46:30.160]   This is the...man, if I were going to start a dry cleaner, I would use start job."
[00:46:30.160 --> 00:46:31.160]   See?
[00:46:31.160 --> 00:46:33.160]   Now, how much would you pay for this service?
[00:46:33.160 --> 00:46:35.160]   It's free.
[00:46:35.160 --> 00:46:36.160]   That's the thing that blows me away.
[00:46:36.160 --> 00:46:38.160]   I always bury the lead by accident.
[00:46:38.160 --> 00:46:40.160]   This is a free service.
[00:46:40.160 --> 00:46:48.160]   Kaptera believes software can make the world a better place because it can help every organization become a more efficient, effective version of itself.
[00:46:48.160 --> 00:46:51.160]   Kaptera is software selection simplified.
[00:46:51.160 --> 00:46:56.160]   And if you need software for your business, kaptera.com/twit.
[00:46:56.160 --> 00:46:59.160]   C-A-P-T-E-D-L-R-A.
[00:46:59.160 --> 00:47:01.160]   Kaptera.com/twit.
[00:47:01.160 --> 00:47:06.160]   Thank you, Kaptera, for making a great site with really useful stuff.
[00:47:06.160 --> 00:47:09.160]   And thank you for using that URL and letting them know you heard it here.
[00:47:09.160 --> 00:47:14.160]   Kaptera.com/twit software selection.
[00:47:14.160 --> 00:47:15.160]   Simplify.
[00:47:15.160 --> 00:47:18.160]   Josh Hawley.
[00:47:18.160 --> 00:47:23.160]   Senator from...where is...is he Idaho?
[00:47:23.160 --> 00:47:25.160]   Where is Josh from?
[00:47:25.160 --> 00:47:26.160]   Missouri.
[00:47:26.160 --> 00:47:42.160]   Has finally presented a legislation that will read Facebook, Google, and Twitter of suspected political bias by stripping away the protections.
[00:47:42.160 --> 00:47:55.160]   All internet service providers have from the Section 230 of the Communications Decency Act.
[00:47:55.160 --> 00:48:06.160]   Section 230 says that internet publishers... anything that distributes content supplied by users, Twitter, YouTube, Facebook is not liable for that content.
[00:48:06.160 --> 00:48:14.160]   Hawley...Hawley wants them to be liable because he thinks they're biased against conservatives.
[00:48:14.160 --> 00:48:17.160]   That, by the way, is obviously a red herring.
[00:48:17.160 --> 00:48:23.160]   But it's terrifying to think that 230 could be eliminated.
[00:48:23.160 --> 00:48:27.160]   There are those, including the EFF who say you eliminate Section 230.
[00:48:27.160 --> 00:48:34.160]   If you make platforms responsible for the content on those platforms, you eliminate the internet.
[00:48:34.160 --> 00:48:41.160]   Nobody's ever said Ma Bell should be responsible for the content of phone calls that would be impossible.
[00:48:41.160 --> 00:48:49.160]   Section 230 says that internet providers, online services, are like the phone company.
[00:48:49.160 --> 00:48:52.160]   Hawley wants to change all that.
[00:48:52.160 --> 00:49:01.160]   I don't think it has a chance, but it is being introduced in the Senate, which means it has a decent chance of at least getting through the Senate.
[00:49:01.160 --> 00:49:07.160]   Not a good idea.
[00:49:07.160 --> 00:49:20.160]   And I think as much as we talk about how scary Facebook is and what a dumpster fire Twitter has become, nobody wants 230 to be overturned.
[00:49:20.160 --> 00:49:26.160]   Mike Masnick has a really good piece, as usual, on tech dirt.
[00:49:26.160 --> 00:49:30.160]   Let me see if I can find that. I've bookmarked it.
[00:49:30.160 --> 00:49:35.160]   Once more with feeling, there's no legal distinction between a platform and a publisher.
[00:49:35.160 --> 00:49:43.160]   I want to mention this because I have fallen prey to this notion that Facebook, for instance, can't decide if it's a platform or a publisher.
[00:49:43.160 --> 00:49:48.160]   A platform would not be responsible. A publisher would, like a magazine would.
[00:49:48.160 --> 00:49:55.160]   Section 230 says that any online platform, any internet platform is protected.
[00:49:55.160 --> 00:49:59.160]   There's no need to distinguish between platform or publisher.
[00:49:59.160 --> 00:50:04.160]   In fact, there is no special distinction for platforms. It makes no difference, Masnick writes.
[00:50:04.160 --> 00:50:07.160]   How an internet company refers to itself.
[00:50:07.160 --> 00:50:17.160]   All it matters is they meet the legal definition of an interactive computer service, which, if they're online, the answer is generally yes to be protected.
[00:50:17.160 --> 00:50:29.160]   I want to apologize for causing more confusion about that than necessary. There is no distinction. They are protected by 230, and the assault has begun.
[00:50:29.160 --> 00:50:34.160]   It's actually in Europe, I think. Tell me, Nate. I can't.
[00:50:34.160 --> 00:50:39.160]   The new European intellectual property act did pass, right?
[00:50:39.160 --> 00:50:41.160]   But each nation has to...
[00:50:41.160 --> 00:50:45.160]   I'll be honest, I have been keeping my finger on the European side.
[00:50:45.160 --> 00:50:48.160]   You're Brexit-ed already. You're out of there, huh?
[00:50:48.160 --> 00:50:49.160]   You're going...
[00:50:49.160 --> 00:50:51.160]   Well, if only.
[00:50:51.160 --> 00:50:54.160]   Well, no. Not if only. This wouldn't have happened in the first place.
[00:50:54.160 --> 00:50:58.160]   But no, we're still delayed on that front.
[00:50:58.160 --> 00:51:02.160]   Whatever Europe has passed, I'm sure we are still subject to.
[00:51:02.160 --> 00:51:08.160]   And in general, with most of the European laws, when we do finally leave, we're just going to invoke them as our own anyway.
[00:51:08.160 --> 00:51:12.160]   Right. So there's two articles, Article 11 and Article 13.
[00:51:12.160 --> 00:51:17.160]   Actually, they just got renumbered just to make it hard to figure out what's going on to 15 and 17.
[00:51:17.160 --> 00:51:26.160]   Yeah, these ones did pass. These are the ones that people are calling the war on memes or things like that.
[00:51:26.160 --> 00:51:28.160]   And the war on snippets, too.
[00:51:28.160 --> 00:51:30.160]   Yeah, exactly.
[00:51:30.160 --> 00:51:31.160]   Yeah.
[00:51:31.160 --> 00:51:40.160]   And really kind of a war on... The same kind of war is happening in the US with this attack on Article on 230, Section 230.
[00:51:40.160 --> 00:51:44.160]   So, okay. I'm just bringing it up. Obviously, none of you care.
[00:51:44.160 --> 00:51:45.160]   You're just... Just...
[00:51:45.160 --> 00:51:47.160]   ...blithely going along with your day.
[00:51:47.160 --> 00:51:49.160]   How about this?
[00:51:49.160 --> 00:51:52.160]   This is just going to add fuel to the fire.
[00:51:52.160 --> 00:52:03.160]   The knitting site Ravelry announced just earlier today that they are going to ban... Ravelry is a site my mom uses Ravelry, where you can get knitting designs.
[00:52:03.160 --> 00:52:09.160]   She, you know, you could share... You could say a forum where you could share, you know, knitting stories.
[00:52:09.160 --> 00:52:15.160]   They're going to ban users that show Trump support because they say that support for white supremacy.
[00:52:15.160 --> 00:52:20.160]   We cannot... Let's go to the site statement just to show it.
[00:52:20.160 --> 00:52:21.160]   New policy.
[00:52:21.160 --> 00:52:28.160]   Do not post in support of Trump or his administration. We're banning support of Donald Trump and his administration on Ravelry.
[00:52:28.160 --> 00:52:35.160]   Posts, projects, patterns, profiles. No more make America great knitted hats.
[00:52:35.160 --> 00:52:38.160]   We're not going to delete your project.
[00:52:38.160 --> 00:52:43.160]   But we could ban you. We don't want to provide a space that's inclusive of all.
[00:52:43.160 --> 00:52:47.160]   And we feel that that's not doing it. What do you say?
[00:52:47.160 --> 00:52:52.160]   This just... This just is fuel to the Josh Hawley fire.
[00:52:52.160 --> 00:52:59.160]   Because this is such a Ravelry is a huge publisher of content, I guess. I don't know.
[00:52:59.160 --> 00:53:07.160]   It is funny how... I feel like the conservative argument always is like, you know, free speech. Everyone's free to do what they want.
[00:53:07.160 --> 00:53:16.160]   But once a conservative voice is kind of squashed in a way or limited, it's instantly censorship.
[00:53:16.160 --> 00:53:20.160]   I don't know. It just seems confusing. This seems like a big step though.
[00:53:20.160 --> 00:53:28.160]   I'm surprised a knitting site of all places is the first to kind of make this leap because it's something people have been calling for on Twitter for a while.
[00:53:28.160 --> 00:53:39.160]   I think I wonder if this is a response to all the more recent reports of the immigrant detention centers in Texas and the horrid condensations there.
[00:53:39.160 --> 00:53:47.160]   And the things kids have to live through under this administration. It's horrific. It's not great.
[00:53:47.160 --> 00:53:53.160]   I think it's not that controversial to describe it as a racist policy against a certain group of people.
[00:53:53.160 --> 00:54:01.160]   There was a whole hubbub last week around calling them concentration camps. What else would you call them? I don't know.
[00:54:01.160 --> 00:54:08.160]   But yeah, this seems like a controversial move, but I want to see what happens after this if other people follow suit.
[00:54:08.160 --> 00:54:17.160]   Yeah, they say in their statement, we cannot provide a space that's inclusive of all and also allow support for open white supremacy.
[00:54:17.160 --> 00:54:22.160]   And then they go on to note, we're not endorsing Democrats and our banning Republicans.
[00:54:22.160 --> 00:54:30.160]   We're definitely not banning conservative politics, et cetera, et cetera. But it seems to me that's not going to be understood that way.
[00:54:30.160 --> 00:54:38.160]   I think this is, I'm personally on a very personal politics level in support of this idea and I'm all for it.
[00:54:38.160 --> 00:54:45.160]   And of course, it's a private corporation, a private forum so they can do this.
[00:54:45.160 --> 00:54:54.160]   And I think that there's this whole intolerance idea that goes beyond this discussion.
[00:54:54.160 --> 00:55:03.160]   But yeah, we don't have to tolerate intolerance. That's not part of allowing for open exchange of ideas.
[00:55:03.160 --> 00:55:11.160]   But even having said that, I guess what I'm getting back to is they're not going to convince people who feel attacked by this decision
[00:55:11.160 --> 00:55:16.160]   that they're not banning conservative politics and they're not endorsing Democrats or banning Republicans.
[00:55:16.160 --> 00:55:28.160]   So this is, it's going to be tricky, I think, to get the nuance across here that it's really just about keeping that support of open white supremacy out of their platform.
[00:55:28.160 --> 00:55:31.160]   They have every right to do.
[00:55:31.160 --> 00:55:36.160]   And because they're a knitting site, I think probably they'll get away with it.
[00:55:36.160 --> 00:55:40.160]   But Twitter has every right to do it as well.
[00:55:40.160 --> 00:55:46.160]   In fact, they've been accused of doing it. I don't think they are, but they've been accused of doing it by conservatives.
[00:55:46.160 --> 00:55:51.160]   They've only recently started to really clamp down on some Nazi accounts and things.
[00:55:51.160 --> 00:55:54.160]   Yeah, you're a Nazi, you've got a nice home there at Twitter.
[00:55:54.160 --> 00:56:00.160]   But now it's all semantics because then what do you call an administration that is putting people in cages?
[00:56:00.160 --> 00:56:06.160]   And concentrating certain types of people into a camp-like environment.
[00:56:06.160 --> 00:56:07.160]   A camp-like environment.
[00:56:07.160 --> 00:56:10.160]   Yeah, yeah. I don't know what you call that.
[00:56:10.160 --> 00:56:13.160]   So, yeah, a lot of this is semantics, I think.
[00:56:13.160 --> 00:56:24.160]   I feel bad because in one respect, I don't think we'll ever come to, can never again come to a national consensus about anything if everything becomes so polarized.
[00:56:24.160 --> 00:56:36.160]   If nobody's allowed to say anything, I only want to hear speech that I agree with that eliminates the chance for a consensus of any kind or any reprochement at all.
[00:56:36.160 --> 00:56:39.160]   I totally hear you, Leo. I totally hear you.
[00:56:39.160 --> 00:56:43.160]   But it's like, I do feel like who casts the first stone here, right?
[00:56:43.160 --> 00:56:51.160]   Who is the one who took things so far that you can't even negotiate or have a conversation about what their beliefs are?
[00:56:51.160 --> 00:56:57.160]   And I think for me personally, it is certainly like the Trump administration and kind of what they're doing to this country.
[00:56:57.160 --> 00:57:09.160]   So, yeah, it's hard to have a conversation with somebody who supports Trump and then, yeah, I couldn't really consider them friends, you know, because of what they support, basically.
[00:57:09.160 --> 00:57:15.160]   So, Kate, if Twitter does the same thing, what would people, what would you say?
[00:57:15.160 --> 00:57:23.160]   I think it would be just as complicated in terms of how it's received by people broadly.
[00:57:23.160 --> 00:57:32.160]   Obviously, any decision is going to be received well by those who align with it and it's an attack by those who don't.
[00:57:32.160 --> 00:57:42.160]   But I think that the idea that they're allowing basically Nazi participation and the amplification.
[00:57:42.160 --> 00:57:48.160]   Knitters, what are they? Are they saying no swastikas? I mean, that would be reasonable.
[00:57:48.160 --> 00:58:01.160]   Yeah, I suspect that there is an awful lot of stuff going on in their forums and that there are patterns going around that are all about supporting Trump and the Nazi flag or symbols and things like that.
[00:58:01.160 --> 00:58:12.160]   But we don't know and it may well be they've been trolled by non knitters. I mean, this happens all the time, right, who have decided let's attack Ravelry.
[00:58:12.160 --> 00:58:16.160]   For all I know that's what this is in response to.
[00:58:16.160 --> 00:58:25.160]   Yeah, I think for what I see happening or what I would see happening, you asked if it happened on Twitter, if this kind of decision happened on Twitter.
[00:58:25.160 --> 00:58:34.160]   I think you would see an awful lot of people who would conflate that decision with an attack on conservative politics.
[00:58:34.160 --> 00:58:54.160]   And I think that's really unfortunate. I think to your point that there's a lack of nuance and there's a lack of willingness to see the differences of opinion and just assume that everything that falls into one camp, so to speak, is all bad or all good.
[00:58:54.160 --> 00:59:00.160]   It's way too lacking and nuance in the discussions that we're having.
[00:59:00.160 --> 00:59:16.160]   So it's a shame, but I think it's a really, really respectable effort on Ravelry's part to say this is specifically that we don't want to allow support of an administration that is support for white supremacy.
[00:59:16.160 --> 00:59:23.160]   And we want to call out that that's not about blanket banning of conservative politics and so on.
[00:59:23.160 --> 00:59:28.160]   It's just I don't really buy that that's how that's going to be received when it comes down to it.
[00:59:28.160 --> 00:59:29.160]   Yeah.
[00:59:29.160 --> 00:59:39.160]   Wow. Of course, it's a little more complicated on Twitter because if you did that, you'd also have to ban Twitter's number one user who happens to be the President of the United States.
[00:59:39.160 --> 00:59:56.160]   What's amazing to me, and I guess it you can't say it started with President Trump, but it's certainly a big part of it is due to President Trump, who has essentially used Twitter as a way to make announcements, is that more and more companies, politicians,
[00:59:56.160 --> 01:00:11.160]   people in general are using Twitter to make announcements has become the place you announce this stuff, which is kind of shocking to me because I always think of Twitter as a place you do off-handed stuff instead of making major, you know, presidential proclamations.
[01:00:11.160 --> 01:00:13.160]   What is going on?
[01:00:13.160 --> 01:00:24.160]   That changed through even the protests in Egypt. And I think we saw the role of Twitter change in society over the last decade.
[01:00:24.160 --> 01:00:32.160]   It became a very important medium for very prominent social discussions and cultural conversations.
[01:00:32.160 --> 01:00:45.160]   Do you think the next President, if it's not, let's say, whenever President Trump's at office, if the next President will continue to use Twitter as a platform to make public announcements?
[01:00:45.160 --> 01:00:47.160]   I think so. I think that's done.
[01:00:47.160 --> 01:00:48.160]   That's done.
[01:00:48.160 --> 01:00:49.160]   That's done.
[01:00:49.160 --> 01:00:50.160]   Yeah.
[01:00:50.160 --> 01:00:53.160]   It's going to be an important platform going forward for sure.
[01:00:53.160 --> 01:01:07.160]   Or at least have somebody on staff who kind of knows the platform because we're looking at this after AOC started doing those great Instagram lives that are so relaxed and also breaking down complex policy.
[01:01:07.160 --> 01:01:10.160]   Everybody started copying her and trying to do it too.
[01:01:10.160 --> 01:01:16.160]   So in a way, it's the fireside chat, which Roosevelt introduced.
[01:01:16.160 --> 01:01:24.160]   It's the fireside chat of this generation is social media, not just Twitter, but Instagram to the next president might make her announcements on Instagram.
[01:01:24.160 --> 01:01:35.160]   Maybe she'll put the other angle to this is that, you know, Trump, as much as he likes to hate on a lot of media, he knows the power of the media.
[01:01:35.160 --> 01:01:48.160]   I think that the use of Twitter and the immediacy of Twitter gives him, you know, he knows that it's going to get straight into the papers, straight onto the websites and the blogs, you know, unfiltered.
[01:01:48.160 --> 01:01:51.160]   And that's kind of what he wants, I think.
[01:01:51.160 --> 01:01:53.160]   Wow. Interesting.
[01:01:53.160 --> 01:01:55.160]   It's a savvy strategy.
[01:01:55.160 --> 01:01:57.160]   And it is direct.
[01:01:57.160 --> 01:02:04.160]   I mean, this is the, I mean, didn't we want to have a disinterme, this isn't internet about this intermediate, but it's not a bad thing.
[01:02:04.160 --> 01:02:05.160]   It's about this intermediation.
[01:02:05.160 --> 01:02:09.160]   Didn't we want to have this intermediated communication with our leaders?
[01:02:09.160 --> 01:02:12.160]   And isn't that what Twitter is giving us directly?
[01:02:12.160 --> 01:02:14.160]   One sided is a bit too one sided.
[01:02:14.160 --> 01:02:16.160]   You know, I don't seem replying very often.
[01:02:16.160 --> 01:02:23.160]   No, but the courts, by the way, said that you can't block you because if he's going to use it as a platform, he can't block anybody.
[01:02:23.160 --> 01:02:25.160]   Everybody has to have a right to respond.
[01:02:25.160 --> 01:02:31.160]   I doubt very much President Trump is reading his at replies, but they're there and he can't block them.
[01:02:31.160 --> 01:02:37.160]   You're not from his official, I think, POTUS account, but I think from his Donald J. Trump or whatever account he can't.
[01:02:37.160 --> 01:02:38.160]   He can do it every once.
[01:02:38.160 --> 01:02:39.160]   I believe that that's how that plays out.
[01:02:39.160 --> 01:02:41.160]   I'm not totally clear on that.
[01:02:41.160 --> 01:02:46.160]   But I think it's interesting to vendor, you mentioned AOC and her savvy use of social media.
[01:02:46.160 --> 01:02:54.160]   And I think it's so fascinating that it ended up where she even led that Twitter workshop for members of Congress.
[01:02:54.160 --> 01:02:56.160]   Yeah, teaching the old, the old's how to use Twitter.
[01:02:56.160 --> 01:02:57.160]   Yeah.
[01:02:57.160 --> 01:02:58.160]   Exactly.
[01:02:58.160 --> 01:03:01.160]   I thought Regis Philbin had to use Twitter. It did not end well.
[01:03:01.160 --> 01:03:04.160]   That's all I'm going to say.
[01:03:04.160 --> 01:03:07.160]   Within two weeks, it was like, I don't like this.
[01:03:07.160 --> 01:03:09.160]   People are talking back.
[01:03:09.160 --> 01:03:12.160]   People are talking back.
[01:03:12.160 --> 01:03:14.160]   That was one of the things that was charming about Twitter.
[01:03:14.160 --> 01:03:15.160]   Exactly.
[01:03:15.160 --> 01:03:16.160]   That was the point.
[01:03:16.160 --> 01:03:17.160]   I'm trying to say this is it.
[01:03:17.160 --> 01:03:18.160]   At first he loved it.
[01:03:18.160 --> 01:03:21.160]   So he was like, oh, Leo, look, because he announced his retirement.
[01:03:21.160 --> 01:03:23.160]   That was the first thing he tweeted was, I'm retiring.
[01:03:23.160 --> 01:03:26.160]   And he got a lot of response and love and stuff.
[01:03:26.160 --> 01:03:27.160]   He called his wife.
[01:03:27.160 --> 01:03:28.160]   He said, Joy, Joy, you got to look at this.
[01:03:28.160 --> 01:03:30.160]   Look at the world talking to me on Twitter.
[01:03:30.160 --> 01:03:32.160]   He was so excited.
[01:03:32.160 --> 01:03:36.160]   But within two weeks, the bloom was off the rose.
[01:03:36.160 --> 01:03:37.160]   I don't like this Twitter thing.
[01:03:37.160 --> 01:03:38.160]   This is, I'm done.
[01:03:38.160 --> 01:03:39.160]   It's over.
[01:03:39.160 --> 01:03:41.160]   We have people to do this for me.
[01:03:41.160 --> 01:03:42.160]   Okay.
[01:03:42.160 --> 01:03:44.160]   There's just no question.
[01:03:44.160 --> 01:03:46.160]   We live in interesting times.
[01:03:46.160 --> 01:03:47.160]   That's all I'm going to say.
[01:03:47.160 --> 01:03:48.160]   Yeah.
[01:03:48.160 --> 01:03:50.160]   It's, it's, to me, it's fascinating.
[01:03:50.160 --> 01:03:53.160]   And I just, I'm, I just fascinated by it.
[01:03:53.160 --> 01:03:55.160]   Endlessly fascinated by it.
[01:03:55.160 --> 01:04:00.160]   I think it's still, it's still cool if you have that moment where you can interact with it.
[01:04:00.160 --> 01:04:03.160]   We talked about Twitter in the early days as, as the great leveler, right?
[01:04:03.160 --> 01:04:07.880]   Like you could, you could talk with somebody who is at a different level from you in terms
[01:04:07.880 --> 01:04:11.160]   of your profession or in terms of their celebrity or whatever.
[01:04:11.160 --> 01:04:13.560]   I think you still have those moments from time to time.
[01:04:13.560 --> 01:04:16.600]   And it is a magical thing how, how that happens.
[01:04:16.600 --> 01:04:23.240]   It really is something that doesn't exist in pretty much any other format or platform.
[01:04:23.240 --> 01:04:25.640]   So it's a, it's a wonderful aspect of Twitter.
[01:04:25.640 --> 01:04:29.400]   I, I think it just, you know, it comes with some other stuff in the territory that is
[01:04:29.400 --> 01:04:30.840]   not as exciting.
[01:04:30.840 --> 01:04:34.560]   I think when I first jumped on Twitter, he's described it to people like that scene in
[01:04:34.560 --> 01:04:38.280]   the matrix where they're like, Oh, the code is just flowing down and you know, they can
[01:04:38.280 --> 01:04:39.280]   see it.
[01:04:39.280 --> 01:04:40.280]   You know, I could see who's talking.
[01:04:40.280 --> 01:04:41.640]   They can see the conversation.
[01:04:41.640 --> 01:04:42.640]   That's it.
[01:04:42.640 --> 01:04:44.240]   That is a global pulse of conversation.
[01:04:44.240 --> 01:04:45.240]   It's just a shame.
[01:04:45.240 --> 01:04:47.720]   Like, Oh, you take that to its logical conclusion.
[01:04:47.720 --> 01:04:53.160]   It's yeah, a world leader using it to espouse whatever he wants and really toxic dialogue
[01:04:53.160 --> 01:04:58.320]   and yeah, it is sort of everything we wanted and everything, I guess it comes with that
[01:04:58.320 --> 01:04:59.320]   too.
[01:04:59.320 --> 01:05:01.320]   All the bad that comes with it.
[01:05:01.320 --> 01:05:02.320]   Yeah.
[01:05:02.320 --> 01:05:07.160]   I'm trying to, trying to take the bright side to take the optimistic approach to all of
[01:05:07.160 --> 01:05:08.160]   this stuff.
[01:05:08.160 --> 01:05:11.160]   Twitter will save us all Twitter will save us.
[01:05:11.160 --> 01:05:12.160]   Yeah.
[01:05:12.160 --> 01:05:16.720]   Or people will save us, which I think it all comes down to people saving us.
[01:05:16.720 --> 01:05:17.720]   Yeah.
[01:05:17.720 --> 01:05:18.720]   Yeah.
[01:05:18.720 --> 01:05:22.600]   People being motivated enough to use the tools at their disposal will save us.
[01:05:22.600 --> 01:05:23.600]   What do you do?
[01:05:23.600 --> 01:05:25.000]   What do you say, Kate, to people?
[01:05:25.000 --> 01:05:29.480]   What is it that we need to do to make this work?
[01:05:29.480 --> 01:05:33.280]   So right now, I think our biggest threat is climate change.
[01:05:33.280 --> 01:05:37.920]   And we need to be thinking about a bunch of different dimensions of threat at the same
[01:05:37.920 --> 01:05:38.920]   time.
[01:05:38.920 --> 01:05:45.120]   We need to be thinking about the threat of disintermediation of jobs from intelligent
[01:05:45.120 --> 01:05:47.400]   automation over the next few decades.
[01:05:47.400 --> 01:05:48.720]   And what does that look like?
[01:05:48.720 --> 01:05:50.520]   It's not the jobs that we have to worry about.
[01:05:50.520 --> 01:05:52.320]   It's the economic consequences of that.
[01:05:52.320 --> 01:05:54.280]   And the socioeconomic fallout.
[01:05:54.280 --> 01:05:56.360]   And then what it means to our identities.
[01:05:56.360 --> 01:06:00.640]   But even while that's happening, we've got climate catastrophe happening in the background.
[01:06:00.640 --> 01:06:08.080]   So there's, and then there's this whole national global kind of identity politics happening.
[01:06:08.080 --> 01:06:12.680]   When I say identity politics, I mean on a national level, like nations trying to figure
[01:06:12.680 --> 01:06:15.960]   out where they fit on this global ecosystem scale.
[01:06:15.960 --> 01:06:18.840]   So I think there's just a lot that's happening all at once.
[01:06:18.840 --> 01:06:26.080]   When we have the opportunity to use the exponential power of technology to help us solve an awful
[01:06:26.080 --> 01:06:32.440]   lot of those problems, we could be using AI and other forms of emerging technology to
[01:06:32.440 --> 01:06:39.000]   be addressing climate change issues in immediate ways if we were to put our resources that
[01:06:39.000 --> 01:06:42.600]   way, if we were to put our priorities in line that way.
[01:06:42.600 --> 01:06:46.920]   I think it just comes down to that, to kind of getting some consensus in place and getting
[01:06:46.920 --> 01:06:50.400]   some resources aligned that we're going to solve these problems.
[01:06:50.400 --> 01:06:54.320]   I'm going to use what tools we have and solve the problems.
[01:06:54.320 --> 01:06:59.600]   That's to me, the problem with cutting off one side of the conversation is nothing can
[01:06:59.600 --> 01:07:00.600]   be done unilaterally.
[01:07:00.600 --> 01:07:02.600]   It can't be done by one party.
[01:07:02.600 --> 01:07:05.040]   It can't be done by one nation.
[01:07:05.040 --> 01:07:07.840]   We're going to have to work together globally to solve these problems.
[01:07:07.840 --> 01:07:17.160]   And if it's nothing but inner, nice-seeing war, Hatfields and McCoy's, we're going to
[01:07:17.160 --> 01:07:18.160]   be busy shooting each other.
[01:07:18.160 --> 01:07:22.480]   Never going to buckle down and solving the big problems.
[01:07:22.480 --> 01:07:24.440]   We have to, first of all, agree that there is a problem.
[01:07:24.440 --> 01:07:25.920]   That isn't even there.
[01:07:25.920 --> 01:07:26.920]   Yeah.
[01:07:26.920 --> 01:07:30.720]   That's the bigger thing.
[01:07:30.720 --> 01:07:35.600]   And coming down to, we'll come back to Devindra and ask him to teach us all about philosophy
[01:07:35.600 --> 01:07:38.600]   so that we can understand human nature.
[01:07:38.600 --> 01:07:39.600]   I was a terrible philosopher.
[01:07:39.600 --> 01:07:41.600]   Kant had nothing to say in this regard.
[01:07:41.600 --> 01:07:45.120]   I'm just going to say, why am that's maybe?
[01:07:45.120 --> 01:07:46.320]   I don't know.
[01:07:46.320 --> 01:07:48.320]   All right.
[01:07:48.320 --> 01:07:50.080]   I'm trying to find something positive.
[01:07:50.080 --> 01:07:51.080]   Let's see here.
[01:07:51.080 --> 01:07:52.080]   Something positive.
[01:07:52.080 --> 01:07:53.080]   Something positive.
[01:07:53.080 --> 01:07:55.600]   Did we cover, I can't remember if we covered this last week.
[01:07:55.600 --> 01:07:59.880]   I thought this was one of the best stories of the week.
[01:07:59.880 --> 01:08:00.880]   Let's see.
[01:08:00.880 --> 01:08:01.880]   This is the 23rd.
[01:08:01.880 --> 01:08:05.320]   No, I think this happened on Monday.
[01:08:05.320 --> 01:08:06.920]   The website genius.
[01:08:06.920 --> 01:08:10.080]   I'm using some real genius.
[01:08:10.080 --> 01:08:11.080]   So this is the lyrics site.
[01:08:11.080 --> 01:08:12.080]   Used to be rap genius.
[01:08:12.080 --> 01:08:13.080]   I love this site.
[01:08:13.080 --> 01:08:14.080]   It's a great site.
[01:08:14.080 --> 01:08:20.000]   They're not thrilled because Google now, if you search for lyrics, will deliver the entire
[01:08:20.000 --> 01:08:24.320]   lyric in its search results, meaning you no longer have to go to rap genius.
[01:08:24.320 --> 01:08:29.560]   Okay, that's fine if maybe Google's getting it from the artist or they hired 30,000 people
[01:08:29.560 --> 01:08:34.040]   to write down lyrics or maybe they even licensed it from the publishing company.
[01:08:34.040 --> 01:08:36.760]   But rap genius, I don't know where they're getting these lyrics.
[01:08:36.760 --> 01:08:38.600]   So what if we did something kind of funny?
[01:08:38.600 --> 01:08:47.080]   We used straight quotes and smart quotes in combination in a unique combination.
[01:08:47.080 --> 01:08:48.760]   In fact, I'll show you the video.
[01:08:48.760 --> 01:08:54.520]   This is an Alicia Cara song.
[01:08:54.520 --> 01:08:59.720]   And in the lyrics of Alicia Cara's song, not today, they put a combination of straight
[01:08:59.720 --> 01:09:06.320]   quotes and smart quotes that if you just take the smart quotes, okay, now that was
[01:09:06.320 --> 01:09:07.320]   rap genius.
[01:09:07.320 --> 01:09:08.480]   Now they're showing the Google results.
[01:09:08.480 --> 01:09:09.480]   Well, interesting.
[01:09:09.480 --> 01:09:12.160]   Exactly the same pattern.
[01:09:12.160 --> 01:09:15.080]   If you take the straight quotes and smart quotes and say the straight quotes are dots
[01:09:15.080 --> 01:09:21.000]   and the smart quotes are dashes, it spells out red handed in Morse code.
[01:09:21.000 --> 01:09:22.800]   Busted.
[01:09:22.800 --> 01:09:25.120]   Busted Google.
[01:09:25.120 --> 01:09:31.040]   It's very clever and this is the exact reason why the EU has been very heavy handed on the
[01:09:31.040 --> 01:09:35.240]   likes of Google using snippets, using bits of code.
[01:09:35.240 --> 01:09:41.800]   It's basically for this and similar has been used as a reason why Google News is harming
[01:09:41.800 --> 01:09:44.760]   publishers which I personally do not support at all.
[01:09:44.760 --> 01:09:45.760]   I don't think it does.
[01:09:45.760 --> 01:09:49.640]   That's what a little bit bothers me because this conflates the two.
[01:09:49.640 --> 01:09:56.000]   It's two or three lines from an article with a link back to the article that snippets
[01:09:56.000 --> 01:10:00.200]   drives traffic more than it would kill traffic.
[01:10:00.200 --> 01:10:02.400]   It's not stealing the content of the article.
[01:10:02.400 --> 01:10:09.360]   On the other hand, if in the search results, if you search for Alicia Cara lyrics and let's
[01:10:09.360 --> 01:10:10.600]   what was the name of the song again?
[01:10:10.600 --> 01:10:14.440]   I have to search for the song so that I can get the actual result.
[01:10:14.440 --> 01:10:20.160]   But if you search for Alicia Cara not today lyrics, you'll get the whole lyric.
[01:10:20.160 --> 01:10:25.480]   Let me do it here just to prove this.
[01:10:25.480 --> 01:10:26.580]   Not today lyric.
[01:10:26.580 --> 01:10:29.400]   You won't get a link to rap genius.
[01:10:29.400 --> 01:10:35.880]   You'll in the search results not get a snippet, but you'll get the whole thing thereby bypassing
[01:10:35.880 --> 01:10:37.440]   entirely traffic.
[01:10:37.440 --> 01:10:40.680]   Now the second link by the way is to genius.
[01:10:40.680 --> 01:10:45.600]   Google has incidentally run a grip script now to remove all smart quotes from all search
[01:10:45.600 --> 01:10:46.600]   results.
[01:10:46.600 --> 01:10:50.040]   I bet you Google is not the only one.
[01:10:50.040 --> 01:10:53.040]   This is a technique the map makers of old used to do.
[01:10:53.040 --> 01:10:57.640]   They put fake streets in and stuff because Rand McNally was sure that Thomas guides was
[01:10:57.640 --> 01:10:58.920]   stealing their maps.
[01:10:58.920 --> 01:11:01.640]   So they put in little Easter eggs.
[01:11:01.640 --> 01:11:04.880]   If the Easter eggs showed up in the other guy's map, they'd say, "Gotcha."
[01:11:04.880 --> 01:11:05.880]   Red-handed.
[01:11:05.880 --> 01:11:11.160]   You know, somebody once did this with a Yula many, many years ago as a way of showing how
[01:11:11.160 --> 01:11:13.600]   funny it actually reads a Yula.
[01:11:13.600 --> 01:11:20.680]   And it was some PC repair site in the USA thing and said, "Halfway down the Yula."
[01:11:20.680 --> 01:11:25.280]   If you're reading this and you've actually got this far down in the Yula, then call this
[01:11:25.280 --> 01:11:29.040]   number or something and you'll get $1,000 or something like that.
[01:11:29.040 --> 01:11:30.280]   And someone did it.
[01:11:30.280 --> 01:11:31.280]   Someone did it.
[01:11:31.280 --> 01:11:32.280]   Nice.
[01:11:32.280 --> 01:11:37.280]   You didn't pay to that Yula.
[01:11:37.280 --> 01:11:37.720]   You were deeply regretful later on.
[01:11:37.720 --> 01:11:44.120]   So Google in their defense says, "Well, we didn't do it.
[01:11:44.120 --> 01:11:45.800]   We hired something called Lyric Find.
[01:11:45.800 --> 01:11:48.400]   They did it."
[01:11:48.400 --> 01:11:53.800]   Which is very much like Facebook saying, "We're not responsible for those contractors.
[01:11:53.800 --> 01:11:56.200]   They did it."
[01:11:56.200 --> 01:11:59.040]   Lyric Find is also disputing the allegations.
[01:11:59.040 --> 01:12:04.680]   They say, "We have a whole content team devoted to compiling song lyrics from numerous sources,"
[01:12:04.680 --> 01:12:07.800]   which can include them getting directly from the artists as songwriter as well as other
[01:12:07.800 --> 01:12:08.800]   websites.
[01:12:08.800 --> 01:12:11.080]   Well, maybe they aren't denying it.
[01:12:11.080 --> 01:12:15.800]   Yeah, they said they offered to remove Lyric's that Genius said we're stolen, but Genius
[01:12:15.800 --> 01:12:16.800]   didn't respond.
[01:12:16.800 --> 01:12:17.800]   Oh.
[01:12:17.800 --> 01:12:20.680]   And then they said, "Despite that, our team is currently investigating the content in
[01:12:20.680 --> 01:12:23.760]   our database and removing any lyrics that seem to have originated from me."
[01:12:23.760 --> 01:12:28.200]   In other words, we're running a grep search for smart quotes right now.
[01:12:28.200 --> 01:12:31.760]   And all of them are finding other sites, other different types of steel lyrics from
[01:12:31.760 --> 01:12:32.760]   the artists.
[01:12:32.760 --> 01:12:33.760]   That's right.
[01:12:33.760 --> 01:12:36.400]   So somebody who copied from Genius that they took, and it's like it's going all the way
[01:12:36.400 --> 01:12:37.400]   around.
[01:12:37.400 --> 01:12:42.240]   Honestly, Google could afford to license every one of these lyrics from the actual publishers,
[01:12:42.240 --> 01:12:44.880]   the people who publish the lyrics, right?
[01:12:44.880 --> 01:12:46.720]   Give some of the money back to the artist.
[01:12:46.720 --> 01:12:49.920]   But it's so complicated, right?
[01:12:49.920 --> 01:12:54.760]   Like this whole structure is so complicated.
[01:12:54.760 --> 01:12:59.840]   We would have to have a third party that's managing all the rights and access and so
[01:12:59.840 --> 01:13:00.840]   on.
[01:13:00.840 --> 01:13:01.840]   It is complicated.
[01:13:01.840 --> 01:13:02.840]   Yeah.
[01:13:02.840 --> 01:13:07.440]   So maybe just publish the first verse and link to rap Genius.
[01:13:07.440 --> 01:13:08.440]   How about that?
[01:13:08.440 --> 01:13:10.800]   We're just paste and match style.
[01:13:10.800 --> 01:13:13.800]   There's a solution.
[01:13:13.800 --> 01:13:19.240]   Yeah, that little paintbrush and word just used that from now on.
[01:13:19.240 --> 01:13:21.320]   Oh, that's hysterical.
[01:13:21.320 --> 01:13:23.320]   Paste and match style.
[01:13:23.320 --> 01:13:24.720]   Who's the fix?
[01:13:24.720 --> 01:13:28.240]   All this time we thought we had to pay somebody.
[01:13:28.240 --> 01:13:33.200]   It's a silly story because, I mean, it's just a he said, she said, by the way, Tom Petty
[01:13:33.200 --> 01:13:36.360]   or the publisher of Tom Petty's music.
[01:13:36.360 --> 01:13:39.480]   They also wicks and they also publish Rage Against the Machine in Weezer's music is
[01:13:39.480 --> 01:13:43.360]   suing Pandora for using lyrics without any license.
[01:13:43.360 --> 01:13:48.480]   Pandora shows lyrics behind beneath the songs on some mobile and desktop apps.
[01:13:48.480 --> 01:13:56.200]   It's done that for 10 years using a lyric finder by the way for the rights.
[01:13:56.200 --> 01:13:59.000]   It's all going to come down to lyric find anyway.
[01:13:59.000 --> 01:14:01.840]   Wicks and heads at suit Spotify over the same thing.
[01:14:01.840 --> 01:14:05.320]   This comes at a time that Apple has announced that they're going to start showing lyrics
[01:14:05.320 --> 01:14:08.880]   on Apple TV when you listen to music on Apple TV and Apple music.
[01:14:08.880 --> 01:14:13.200]   But I bet you, Apple is probably licensing it.
[01:14:13.200 --> 01:14:14.840]   I would be my guess.
[01:14:14.840 --> 01:14:19.480]   Well, Apple already had such longstanding relationships with publishers and with the
[01:14:19.480 --> 01:14:20.480]   industry music industry.
[01:14:20.480 --> 01:14:21.480]   Sure.
[01:14:21.480 --> 01:14:22.640]   It's part of their mechanicals for the whole thing.
[01:14:22.640 --> 01:14:25.400]   They just say, oh, yeah, we would like to show lyrics.
[01:14:25.400 --> 01:14:26.880]   Is that okay with you?
[01:14:26.880 --> 01:14:30.280]   Actually, lyrics fall outside of mechanicals as I understand.
[01:14:30.280 --> 01:14:31.280]   Oh, do they?
[01:14:31.280 --> 01:14:32.280]   Oh, interesting.
[01:14:32.280 --> 01:14:34.280]   Twelve years in Nashville and a little bit of fun.
[01:14:34.280 --> 01:14:35.560]   Oh, you know about this.
[01:14:35.560 --> 01:14:37.800]   You know what mechanicals are.
[01:14:37.800 --> 01:14:40.360]   What have you not done, Kate O'Neill?
[01:14:40.360 --> 01:14:41.360]   You've been around.
[01:14:41.360 --> 01:14:43.840]   It's really like I have the craziest life.
[01:14:43.840 --> 01:14:46.320]   What did you do in Nashville?
[01:14:46.320 --> 01:14:51.520]   I moved there for songwriting, but I was also working in technology alongside that.
[01:14:51.520 --> 01:14:56.000]   So for about the first five years, I was out every night playing at the writer's nights
[01:14:56.000 --> 01:14:57.000]   playing my song.
[01:14:57.000 --> 01:14:58.000]   That's awesome.
[01:14:58.000 --> 01:15:03.680]   I was drunk the day my mama got out of prison.
[01:15:03.680 --> 01:15:05.000]   Is that that kind of song?
[01:15:05.000 --> 01:15:08.360]   Yeah, that's where the song, you're singing my song.
[01:15:08.360 --> 01:15:09.760]   Now you owe me.
[01:15:09.760 --> 01:15:14.040]   If you show the lyrics, you have to pay me for those.
[01:15:14.040 --> 01:15:19.920]   You got run over by that goddamn train.
[01:15:19.920 --> 01:15:20.920]   That's cool.
[01:15:20.920 --> 01:15:24.280]   What kind of do you write weepy country songs or?
[01:15:24.280 --> 01:15:28.840]   Like pop country with a little Calypso reggae thing.
[01:15:28.840 --> 01:15:29.840]   Nice.
[01:15:29.840 --> 01:15:31.720]   Did it kind of a thing?
[01:15:31.720 --> 01:15:32.720]   That's awesome.
[01:15:32.720 --> 01:15:33.720]   Did it work out?
[01:15:33.720 --> 01:15:37.080]   Oh, yeah, like no.
[01:15:37.080 --> 01:15:38.640]   But you lived the dream.
[01:15:38.640 --> 01:15:44.480]   It worked out in the sense that I got to be in Nashville during a really big development
[01:15:44.480 --> 01:15:48.480]   time for their ecosystem, their tech ecosystem and their entrepreneurial scene.
[01:15:48.480 --> 01:15:49.480]   So I got to be part of helping.
[01:15:49.480 --> 01:15:52.520]   Was this after the Toshiba internet or before the Toshiba?
[01:15:52.520 --> 01:15:54.080]   Yes, it was after.
[01:15:54.080 --> 01:15:55.640]   What a checkered career.
[01:15:55.640 --> 01:15:56.640]   Wow.
[01:15:56.640 --> 01:16:01.520]   And I should mention that Kate was also one of the first hundred employees at Netflix.
[01:16:01.520 --> 01:16:02.520]   That's right.
[01:16:02.520 --> 01:16:03.520]   Yeah.
[01:16:03.520 --> 01:16:06.720]   You could have did a lot of copying and pasting of other people.
[01:16:06.720 --> 01:16:09.200]   I'm sorry.
[01:16:09.200 --> 01:16:17.080]   Although I love it that there was a petition to Netflix saying you must cancel Good Omens,
[01:16:17.080 --> 01:16:20.120]   which is an Amazon show by the way.
[01:16:20.120 --> 01:16:26.320]   So Netflix tweets in response to this petition, which had 20,000 signatures.
[01:16:26.320 --> 01:16:31.040]   Okay, we promised not to do any more episodes of Good Omens.
[01:16:31.040 --> 01:16:36.200]   Thousands petition Netflix to cancel Amazon Prime's Good Omens to which Amazon Prime responds.
[01:16:36.200 --> 01:16:43.360]   Okay, Netflix, we'll cancel Stranger Things if you cancel Good Omens.
[01:16:43.360 --> 01:16:46.400]   I love it when brands tweeted each other every now and again.
[01:16:46.400 --> 01:16:48.480]   It happens and it's a great.
[01:16:48.480 --> 01:16:51.160]   It's a lot of fun.
[01:16:51.160 --> 01:16:56.640]   Clearly the Christian group behind the petition didn't watch the show and didn't really care.
[01:16:56.640 --> 01:16:58.400]   And the people signing it didn't really care.
[01:16:58.400 --> 01:16:59.880]   That you see, this seems like a good idea.
[01:16:59.880 --> 01:17:00.880]   We'll sign that.
[01:17:00.880 --> 01:17:01.880]   It's mad.
[01:17:01.880 --> 01:17:02.880]   It's mad.
[01:17:02.880 --> 01:17:03.880]   We don't.
[01:17:03.880 --> 01:17:10.400]   We don't have the devil and it portrays the antichrist as just an average kid.
[01:17:10.400 --> 01:17:11.400]   So that's not good.
[01:17:11.400 --> 01:17:14.040]   Plus God is voiced by Francis McDormand.
[01:17:14.040 --> 01:17:15.200]   Oh my God.
[01:17:15.200 --> 01:17:16.200]   Sacrilege.
[01:17:16.200 --> 01:17:22.360]   Everyone knows God's a man with a beard.
[01:17:22.360 --> 01:17:24.240]   Francis McDormand.
[01:17:24.240 --> 01:17:25.240]   Come on.
[01:17:25.240 --> 01:17:27.280]   I think that's the best going to be the best role ever.
[01:17:27.280 --> 01:17:28.600]   I actually like that show.
[01:17:28.600 --> 01:17:30.440]   But I love the novel.
[01:17:30.440 --> 01:17:35.600]   Great fantasy novel from Neil Gaiman and Terry Pratchett.
[01:17:35.600 --> 01:17:41.080]   And who doesn't love David, David Tennant as a demon with demonic eyeballs?
[01:17:41.080 --> 01:17:42.640]   He was good in Jessica Jones.
[01:17:42.640 --> 01:17:47.840]   It's one of my favorite, favorite actors in that.
[01:17:47.840 --> 01:17:50.000]   You should say he's Dr. Who?
[01:17:50.000 --> 01:17:53.080]   He is Dr. Who but he's also Scrooge McDuck.
[01:17:53.080 --> 01:17:58.560]   And he's what's his name?
[01:17:58.560 --> 01:17:59.560]   Will Killjoy?
[01:17:59.560 --> 01:18:00.560]   No.
[01:18:00.560 --> 01:18:01.560]   Hang on.
[01:18:01.560 --> 01:18:02.560]   What was his name in Jessica Jones?
[01:18:02.560 --> 01:18:05.240]   I don't remember.
[01:18:05.240 --> 01:18:10.240]   I did find out though that I thought that demon eyes were like special effects.
[01:18:10.240 --> 01:18:14.640]   They're just contact lenses and you can buy them online for $18 or $19.
[01:18:14.640 --> 01:18:16.120]   Any color, any kind.
[01:18:16.120 --> 01:18:17.120]   So.
[01:18:17.120 --> 01:18:18.880]   Much cheaper than special effects.
[01:18:18.880 --> 01:18:19.880]   Yeah.
[01:18:19.880 --> 01:18:21.440]   They're contact lenses.
[01:18:21.440 --> 01:18:23.160]   These are the ones that are really scary.
[01:18:23.160 --> 01:18:26.160]   They just make your whole eyeball black.
[01:18:26.160 --> 01:18:27.520]   You know, look at that.
[01:18:27.520 --> 01:18:28.520]   You know there's something wrong.
[01:18:28.520 --> 01:18:31.920]   You just can't quite figure out what it is.
[01:18:31.920 --> 01:18:34.400]   There's something wrong with that person.
[01:18:34.400 --> 01:18:44.960]   Leo, how much would it take for you to wear a pair of these live on Twitch one week?
[01:18:44.960 --> 01:18:46.960]   They may be on order.
[01:18:46.960 --> 01:18:49.360]   Oh, they really.
[01:18:49.360 --> 01:18:52.200]   Can we start a GoFundMe?
[01:18:52.200 --> 01:18:53.200]   That's me.
[01:18:53.200 --> 01:18:55.200]   Sounds like a game.
[01:18:55.200 --> 01:19:01.360]   So I'm a bad, I bought on Kickstarter the light up eyelashes.
[01:19:01.360 --> 01:19:06.800]   So the F flashes, but maybe I'll combine the two and then put in those fake teeth and
[01:19:06.800 --> 01:19:08.520]   you won't you won't even know it's me.
[01:19:08.520 --> 01:19:10.320]   It'll be amazing.
[01:19:10.320 --> 01:19:11.320]   Our show today.
[01:19:11.320 --> 01:19:12.320]   I don't know what I'm talking about.
[01:19:12.320 --> 01:19:14.080]   The Halloween episode is going to rock.
[01:19:14.080 --> 01:19:15.080]   Oh Halloween.
[01:19:15.080 --> 01:19:17.320]   And then it would be that have a good excuse.
[01:19:17.320 --> 01:19:18.320]   Yes.
[01:19:18.320 --> 01:19:19.320]   Mm.
[01:19:19.320 --> 01:19:21.280]   Kate, you want to come on the Halloween episode too?
[01:19:21.280 --> 01:19:22.280]   Yes.
[01:19:22.280 --> 01:19:23.280]   Yes.
[01:19:23.280 --> 01:19:24.280]   I'll wear beady eyes.
[01:19:24.280 --> 01:19:26.280]   And a national songwriter with BDA.
[01:19:26.280 --> 01:19:28.280]   That'd be cool.
[01:19:28.280 --> 01:19:29.280]   Close too soon.
[01:19:29.280 --> 01:19:31.320]   Too soon.
[01:19:31.320 --> 01:19:34.400]   How many years did you do to pursue that?
[01:19:34.400 --> 01:19:37.720]   I lived in Nashville for 12 years, but I was really only pursuing the songwriting for
[01:19:37.720 --> 01:19:39.720]   about five years.
[01:19:39.720 --> 01:19:41.040]   We got a drummer here.
[01:19:41.040 --> 01:19:43.080]   If I got, could you get your guitar?
[01:19:43.080 --> 01:19:46.080]   Devendra, you want to play tambourine?
[01:19:46.080 --> 01:19:47.080]   I can do that.
[01:19:47.080 --> 01:19:48.080]   Yeah.
[01:19:48.080 --> 01:19:49.920]   Devenders got baby gadgets that probably make noise.
[01:19:49.920 --> 01:19:51.640]   They probably got little rattles.
[01:19:51.640 --> 01:19:52.640]   Yeah.
[01:19:52.640 --> 01:19:53.640]   Oh, but it's so good.
[01:19:53.640 --> 01:19:57.080]   I was shown to brought to you by Atlassian.
[01:19:57.080 --> 01:19:59.720]   Actually, I showed today brought to you by our IT team.
[01:19:59.720 --> 01:20:05.480]   If you are in business these days, you know you owe it all to the IT team.
[01:20:05.480 --> 01:20:10.320]   Atlassian is a collaboration software company that powers IT teams all over the world.
[01:20:10.320 --> 01:20:16.120]   IT in a cloud environment, in a cloud world we live in, is at the center of everything.
[01:20:16.120 --> 01:20:19.720]   They have to plan and execute better and faster than ever.
[01:20:19.720 --> 01:20:22.280]   Today's apps are more complex than ever.
[01:20:22.280 --> 01:20:29.320]   Incidents require open, agile and smart coordination between operations and your software developers.
[01:20:29.320 --> 01:20:31.600]   Expectations are high, the stakes are high.
[01:20:31.600 --> 01:20:32.600]   You got to perform.
[01:20:32.600 --> 01:20:35.400]   You cannot fall short inside of business critical workflow.
[01:20:35.400 --> 01:20:37.080]   That's why we use Atlassian tools.
[01:20:37.080 --> 01:20:38.160]   JIRA, for instance.
[01:20:38.160 --> 01:20:42.000]   That's how we keep track of who's doing what projects are working on, the status of the
[01:20:42.000 --> 01:20:43.000]   projects.
[01:20:43.000 --> 01:20:46.520]   I think a lot of people know JIRA, but think of it as a tool for agile development teams.
[01:20:46.520 --> 01:20:48.800]   Yeah, it's that, but it's so much more.
[01:20:48.800 --> 01:20:51.400]   We also use Confluence to document everything that happens.
[01:20:51.400 --> 01:20:54.560]   To me, that's almost the most important part.
[01:20:54.560 --> 01:20:58.960]   If you're doing something, you got to document so that people who follow after know what
[01:20:58.960 --> 01:21:03.520]   you did, know how to replicate the solution, that kind of thing.
[01:21:03.520 --> 01:21:08.520]   Atlassian is great because you don't have to ever leave the tools to get the job done.
[01:21:08.520 --> 01:21:10.600]   That's why I think our team likes it.
[01:21:10.600 --> 01:21:12.360]   It's not just for developers.
[01:21:12.360 --> 01:21:16.040]   Atlassian offers an affordable, reliable suite of tools for teams of all kinds and all sizes
[01:21:16.040 --> 01:21:22.800]   from DevOps to Agile, from IT apps to ops to ITSM to whatever is next for your team.
[01:21:22.800 --> 01:21:26.400]   If you've got code, you need Bitbucket.
[01:21:26.400 --> 01:21:30.800]   Atlassian forms a backbone of effective cross-team project planning, organization and communication.
[01:21:30.800 --> 01:21:36.200]   If you've got ops, this is something, you know, back in the day, we could have used this
[01:21:36.200 --> 01:21:41.600]   ops, genie and status page, not only so you can detect incidents, but so you can coordinate
[01:21:41.600 --> 01:21:42.600]   your response efforts.
[01:21:42.600 --> 01:21:48.480]   Most importantly, again, communicate with customers, stakeholders, the boss about what's being
[01:21:48.480 --> 01:21:50.360]   done and how it's happening.
[01:21:50.360 --> 01:21:54.240]   And it all integrates seamlessly so you never have to leave the platform.
[01:21:54.240 --> 01:21:56.680]   Our team loves Atlassian U-will too.
[01:21:56.680 --> 01:22:05.520]   The tools for your IT team are easy and free to try ahead to Atlassian.com/teams/IT and
[01:22:05.520 --> 01:22:07.760]   find out which Atlassian offering is right for your team.
[01:22:07.760 --> 01:22:10.680]   They've got a ton of them and more all the time.
[01:22:10.680 --> 01:22:14.040]   Try Atlassian today to see what IT can be.
[01:22:14.040 --> 01:22:16.960]   Atlassian.com/teams/IT.
[01:22:16.960 --> 01:22:28.280]   Ah, Devindra Hardewars here and Gadget Senior Editor from Bloomberg, their technology editor,
[01:22:28.280 --> 01:22:29.880]   the great Nate Langston.
[01:22:29.880 --> 01:22:31.880]   I always want to say I call you Claxon.
[01:22:31.880 --> 01:22:32.880]   I've had these in the drums.
[01:22:32.880 --> 01:22:33.880]   I don't know.
[01:22:33.880 --> 01:22:34.880]   Kate Langston.
[01:22:34.880 --> 01:22:45.160]   My Claxon Langston and Kato Neil, the tech humanist speaker, writer, K-O Insights.com.
[01:22:45.160 --> 01:22:50.960]   You said that Kato is your Twitter handle is the place to keep up with you.
[01:22:50.960 --> 01:22:51.960]   That's right.
[01:22:51.960 --> 01:22:54.480]   I'm going to be too chatty on Twitter.
[01:22:54.480 --> 01:22:55.480]   Oh, really?
[01:22:55.480 --> 01:22:56.480]   Oh, I'm going to have to follow Kato.
[01:22:56.480 --> 01:22:58.880]   I think I already am.
[01:22:58.880 --> 01:22:59.880]   I think I must be.
[01:22:59.880 --> 01:23:00.880]   Boy, if I'm not.
[01:23:00.880 --> 01:23:01.880]   That's my...
[01:23:01.880 --> 01:23:02.880]   I think I'm going to get on that.
[01:23:02.880 --> 01:23:07.880]   My problem with Twitter is I don't respect it.
[01:23:07.880 --> 01:23:11.720]   I need to respect it more.
[01:23:11.720 --> 01:23:15.120]   I need to treat it more like a partner.
[01:23:15.120 --> 01:23:20.000]   You'd find out about stories like the light bulb, if you did.
[01:23:20.000 --> 01:23:21.000]   I know.
[01:23:21.000 --> 01:23:22.760]   I'm missing out on all the fun.
[01:23:22.760 --> 01:23:25.960]   You just got to dip in and dip out with a drink in your hand.
[01:23:25.960 --> 01:23:27.840]   That's the best way to do it.
[01:23:27.840 --> 01:23:29.760]   Or having several drinks in your...
[01:23:29.760 --> 01:23:30.760]   Yeah.
[01:23:30.760 --> 01:23:32.280]   It's a cocktail party basically.
[01:23:32.280 --> 01:23:37.840]   I look at Twitter and even though you can now write 280 characters, everybody's really
[01:23:37.840 --> 01:23:40.320]   opting for brevity.
[01:23:40.320 --> 01:23:41.640]   I feel like I had a stroke.
[01:23:41.640 --> 01:23:45.320]   I try to read it and it doesn't make any sense.
[01:23:45.320 --> 01:23:46.320]   You know what I mean?
[01:23:46.320 --> 01:23:50.080]   If you read a page of text and nothing makes sense, you think I must be dying.
[01:23:50.080 --> 01:23:52.080]   My brain is malfunctioned.
[01:23:52.080 --> 01:23:54.280]   No, it's just Twitter, Leo.
[01:23:54.280 --> 01:23:55.280]   Oh, good.
[01:23:55.280 --> 01:23:59.960]   I'm entirely new lexicon that people have.
[01:23:59.960 --> 01:24:00.960]   It is.
[01:24:00.960 --> 01:24:07.560]   No, just the other day, we had Sherlin Lowe on from CNET and she said, "You subtweeted
[01:24:07.560 --> 01:24:08.560]   me.
[01:24:08.560 --> 01:24:09.560]   I don't even know what that was.
[01:24:09.560 --> 01:24:10.560]   I had to look it up."
[01:24:10.560 --> 01:24:11.560]   Sherlin's saying gadget.
[01:24:11.560 --> 01:24:12.760]   I'm forced to work with her.
[01:24:12.760 --> 01:24:13.760]   Oh, you know her.
[01:24:13.760 --> 01:24:14.760]   Not CNET.
[01:24:14.760 --> 01:24:16.520]   And gadget, thank you.
[01:24:16.520 --> 01:24:18.520]   I just subtweeted her, apparently.
[01:24:18.520 --> 01:24:21.160]   No, because I...
[01:24:21.160 --> 01:24:23.400]   No, so you can't subtweet him if you mentioned her.
[01:24:23.400 --> 01:24:24.400]   You mentioned her.
[01:24:24.400 --> 01:24:25.400]   Right.
[01:24:25.400 --> 01:24:27.920]   So some person who doesn't work for CNET.
[01:24:27.920 --> 01:24:33.440]   Anyway, I am now following K.O. on the Twitter.
[01:24:33.440 --> 01:24:34.440]   Yay.
[01:24:34.440 --> 01:24:35.440]   Apple.
[01:24:35.440 --> 01:24:36.440]   We haven't talked about Apple yet.
[01:24:36.440 --> 01:24:37.840]   There's a couple of things on Apple.
[01:24:37.840 --> 01:24:38.840]   Not a whole lot.
[01:24:38.840 --> 01:24:46.520]   Apple's recalling a number of 15-inch MacBook Pros from the year 2015 over a fire risk.
[01:24:46.520 --> 01:24:49.000]   These are the MacBook Pros everybody still uses.
[01:24:49.000 --> 01:24:51.440]   The ones before the bad keyboard and the touch bar.
[01:24:51.440 --> 01:24:52.440]   Sure.
[01:24:52.440 --> 01:24:53.440]   Unquote fire risk.
[01:24:53.440 --> 01:24:54.440]   Yeah.
[01:24:54.440 --> 01:24:55.440]   Fire risk.
[01:24:55.440 --> 01:24:56.440]   Yeah.
[01:24:56.440 --> 01:24:57.440]   Yeah, we don't want it.
[01:24:57.440 --> 01:24:59.640]   We have a ton of these.
[01:24:59.640 --> 01:25:02.840]   That's all I would use for a long time.
[01:25:02.840 --> 01:25:05.000]   Apparently the batteries...
[01:25:05.000 --> 01:25:06.720]   Actually, this is kudos to Apple.
[01:25:06.720 --> 01:25:12.480]   You can get a brand new battery for your five-year-old laptop, four-year-old laptop.
[01:25:12.480 --> 01:25:21.000]   This laptop sold between 2015 and February 2015 and February 2017.
[01:25:21.000 --> 01:25:25.160]   There is a little tool on the Apple recall site you can enter your serial number and
[01:25:25.160 --> 01:25:26.160]   find out.
[01:25:26.160 --> 01:25:29.480]   I was kind of hoping initially they'd just give me a new one, but apparently there are
[01:25:29.480 --> 01:25:30.480]   no new ones.
[01:25:30.480 --> 01:25:34.680]   So, you just get a new battery.
[01:25:34.680 --> 01:25:36.160]   That's not much of a story.
[01:25:36.160 --> 01:25:37.640]   Let's see.
[01:25:37.640 --> 01:25:45.960]   Apple says if we had to, if we had to, we could make our phones outside of China.
[01:25:45.960 --> 01:25:52.960]   If we had to, they're right now exploring a shift of anywhere from 15 to 30 percent of
[01:25:52.960 --> 01:25:56.200]   its capacity to somewhere in Elson, Southeast Asia.
[01:25:56.200 --> 01:26:02.280]   This is according to a report from Nikkei Asia Review, a fundamental restructuring of
[01:26:02.280 --> 01:26:03.560]   Apple's supply chain.
[01:26:03.560 --> 01:26:06.520]   This has to do, of course, with tariffs.
[01:26:06.520 --> 01:26:10.360]   But also, I think some concern over products made in China, right?
[01:26:10.360 --> 01:26:11.560]   After the Huawei...
[01:26:11.560 --> 01:26:12.560]   What is this?
[01:26:12.560 --> 01:26:15.560]   What's the status with the Huawei ban?
[01:26:15.560 --> 01:26:16.560]   Are we still...?
[01:26:16.560 --> 01:26:21.280]   I haven't fully checked in on it, but it does seem like it's an ongoing thing.
[01:26:21.280 --> 01:26:25.480]   Huawei is pretending they haven't done anything wrong, and they're kind of standing for them
[01:26:25.480 --> 01:26:27.960]   there, and things are getting worse.
[01:26:27.960 --> 01:26:29.960]   And Washington is pretending they have.
[01:26:29.960 --> 01:26:30.960]   Yeah.
[01:26:30.960 --> 01:26:33.480]   We don't have evidence on either side, to be honest with you.
[01:26:33.480 --> 01:26:35.200]   The truth probably lies somewhere in the middle.
[01:26:35.200 --> 01:26:39.520]   I think there is some bell suspicion around any of the companies are directly tied to
[01:26:39.520 --> 01:26:40.920]   the Chinese government.
[01:26:40.920 --> 01:26:46.880]   I'm surprised Apple is making this move now, I guess, just when tariffs are a potential
[01:26:46.880 --> 01:26:52.120]   thing or around the trade war, and not the behavior of the Chinese government, what they're
[01:26:52.120 --> 01:26:57.560]   doing with some of their populations well.
[01:26:57.560 --> 01:27:03.160]   There's a lot of things to be concerned about with what China is doing right now, too.
[01:27:03.160 --> 01:27:04.160]   Yeah.
[01:27:04.160 --> 01:27:05.480]   This is another one of those where there...
[01:27:05.480 --> 01:27:10.000]   It doesn't seem to be a clear answer one way or the other.
[01:27:10.000 --> 01:27:11.800]   I did look when this story surfaced.
[01:27:11.800 --> 01:27:16.960]   I was looking up what the latest information is about what it would cost Apple if they
[01:27:16.960 --> 01:27:22.520]   were to make the iPhone, the iPhones, for example, in the US, which isn't on the table,
[01:27:22.520 --> 01:27:26.480]   but just out of sheer speculation.
[01:27:26.480 --> 01:27:33.640]   The latest that I saw was if they got the raw materials for about 600 and could manufacture,
[01:27:33.640 --> 01:27:39.880]   the cost of the manufacture wouldn't significantly go up, but the overall price they speculate
[01:27:39.880 --> 01:27:43.480]   would come to around $2,000 in retail.
[01:27:43.480 --> 01:27:44.480]   Wow.
[01:27:44.480 --> 01:27:45.480]   Yeah.
[01:27:45.480 --> 01:27:46.480]   Okay.
[01:27:46.480 --> 01:27:47.480]   That's a non-starter.
[01:27:47.480 --> 01:27:51.880]   But the key issue they're saying is not so much the cost of the materials.
[01:27:51.880 --> 01:27:58.880]   It's that if you've ever seen the way that Chinese factories have really...
[01:27:58.880 --> 01:28:04.360]   Or the process has come to sophistication in China, we're not set up for that.
[01:28:04.360 --> 01:28:10.400]   We would be looking at an entirely new set of skills and entirely new set of competencies
[01:28:10.400 --> 01:28:14.280]   for manufacturing facilities here that we don't have.
[01:28:14.280 --> 01:28:15.600]   It's going to happen.
[01:28:15.600 --> 01:28:19.400]   Foxconn is setting up a facility in Wisconsin.
[01:28:19.400 --> 01:28:20.400]   Well, maybe.
[01:28:20.400 --> 01:28:22.400]   We don't know.
[01:28:22.400 --> 01:28:23.400]   Possibly.
[01:28:23.400 --> 01:28:26.800]   They're going to take the money and then think about it.
[01:28:26.800 --> 01:28:27.800]   Okay.
[01:28:27.800 --> 01:28:33.440]   But it's going to take some time to figure out what it even looks like to have a facility
[01:28:33.440 --> 01:28:35.120]   like that in the US.
[01:28:35.120 --> 01:28:36.680]   It's crazy.
[01:28:36.680 --> 01:28:37.680]   It's crazy.
[01:28:37.680 --> 01:28:40.840]   I think everybody's just hoping the trade war will blow over and we can go back to business
[01:28:40.840 --> 01:28:41.840]   as usual.
[01:28:41.840 --> 01:28:42.840]   It is.
[01:28:42.840 --> 01:28:47.440]   We are so interdependent on one another, not just the US and China, but everybody in the
[01:28:47.440 --> 01:28:52.080]   world to say, "Oh, this part of the world, you can't make stuff for us."
[01:28:52.080 --> 01:28:53.080]   It's just impossible.
[01:28:53.080 --> 01:28:54.560]   I think it's just impossible.
[01:28:54.560 --> 01:28:57.400]   But a similar thing happened with ZTE, right, in 2018.
[01:28:57.400 --> 01:28:59.840]   Put them out of business, right?
[01:28:59.840 --> 01:29:00.840]   Almost.
[01:29:00.840 --> 01:29:01.840]   Almost.
[01:29:01.840 --> 01:29:02.840]   But then it was pulled back.
[01:29:02.840 --> 01:29:06.080]   I believe a similar thing could happen here.
[01:29:06.080 --> 01:29:09.720]   It's just having a pretty devastating effect on Huawei.
[01:29:09.720 --> 01:29:14.160]   I seem to be pretending that it's not, but I get the feeling it is, right?
[01:29:14.160 --> 01:29:15.160]   They've stopped.
[01:29:15.160 --> 01:29:19.640]   You just have to look at the plummeting sales of their smartphones worldwide.
[01:29:19.640 --> 01:29:23.560]   I mean, they are just tanking.
[01:29:23.560 --> 01:29:26.000]   Nobody wants to buy them.
[01:29:26.000 --> 01:29:31.600]   And it's, yeah, we've still got this issue in the UK over whether to rip out all the 5G
[01:29:31.600 --> 01:29:33.560]   hardware that's been installed.
[01:29:33.560 --> 01:29:39.600]   We recently had 5G rollout nationwide by one of our networks.
[01:29:39.600 --> 01:29:43.680]   And a lot of the technology that they're using is Huawei, but they don't know whether
[01:29:43.680 --> 01:29:47.400]   they're going to have to just ripple that out and replace it with something else.
[01:29:47.400 --> 01:29:52.520]   And we've got no clarity even on when a decision about it is going to be made, thanks to our
[01:29:52.520 --> 01:29:54.760]   super organized government right now.
[01:29:54.760 --> 01:29:57.680]   What is going on over there?
[01:29:57.680 --> 01:29:59.280]   By the way.
[01:29:59.280 --> 01:30:02.240]   Dude, I mean, at this point.
[01:30:02.240 --> 01:30:04.120]   Who knows?
[01:30:04.120 --> 01:30:05.120]   Who knows?
[01:30:05.120 --> 01:30:08.200]   I love the speaker of Parliament.
[01:30:08.200 --> 01:30:10.800]   That's the hero to us here.
[01:30:10.800 --> 01:30:11.800]   Yeah.
[01:30:11.800 --> 01:30:12.800]   I love him.
[01:30:12.800 --> 01:30:13.800]   Yeah.
[01:30:13.800 --> 01:30:15.760]   John, John, Burckow.
[01:30:15.760 --> 01:30:19.360]   Yeah, he's an interesting guy.
[01:30:19.360 --> 01:30:24.280]   If you watch a lot of the, I mean, don't ever watch Parliament live like I have to sometimes.
[01:30:24.280 --> 01:30:26.240]   It's enjoyable for us.
[01:30:26.240 --> 01:30:28.240]   Well, it is sometimes.
[01:30:28.240 --> 01:30:29.240]   It is sometimes.
[01:30:29.240 --> 01:30:37.120]   But I was doing a thing on my podcast this week about the pornography agegate law that's
[01:30:37.120 --> 01:30:41.400]   got pushed back because government forgot to tell the EU about part of it.
[01:30:41.400 --> 01:30:47.320]   So they had to delay it by six months because they literally, they just administrative error.
[01:30:47.320 --> 01:30:51.600]   Well, but also that may not have been the best plan because the, at least one of the
[01:30:51.600 --> 01:30:57.200]   age verification companies was the biggest pornographer in the world, the people behind
[01:30:57.200 --> 01:30:59.200]   you two, are you porn?
[01:30:59.200 --> 01:31:02.760]   A parent company of, yeah.
[01:31:02.760 --> 01:31:08.520]   So really the age verification stuff comes from the people who produce the pornography.
[01:31:08.520 --> 01:31:09.520]   Yes.
[01:31:09.520 --> 01:31:11.320]   Who you would have to send a scan of your passport to.
[01:31:11.320 --> 01:31:14.320]   Oh, I can't wait to do that.
[01:31:14.320 --> 01:31:15.920]   And then where you go to a pub, right?
[01:31:15.920 --> 01:31:17.640]   You could go to a pub and buy a license.
[01:31:17.640 --> 01:31:18.640]   What was it?
[01:31:18.640 --> 01:31:19.640]   It was crazy.
[01:31:19.640 --> 01:31:20.640]   Yeah.
[01:31:20.640 --> 01:31:24.160]   I think, I think if it got enforced, people would be going to pubs and buying more than
[01:31:24.160 --> 01:31:28.200]   just a license in order to not get around for law.
[01:31:28.200 --> 01:31:30.480]   I can't speak from personal experience.
[01:31:30.480 --> 01:31:39.120]   There is the best part of this is John Burckow, who is the, could you have my sound?
[01:31:39.120 --> 01:31:43.760]   Speaker of the house, it does not tolerate any mischief from MPs.
[01:31:43.760 --> 01:31:48.120]   Mr. Lewis, get a grip of yourself, man.
[01:31:48.120 --> 01:31:51.120]   We need this in the Congress.
[01:31:51.120 --> 01:31:52.120]   Take up yoga.
[01:31:52.120 --> 01:31:53.680]   You'll find it beneficial, man.
[01:31:53.680 --> 01:31:57.080]   I say to the member order, resume your seat, Mr Harper.
[01:31:57.080 --> 01:31:59.480]   You don't stand when I'm standing and that's the end of it.
[01:31:59.480 --> 01:32:01.400]   Mr. Angus Brendan McNeil.
[01:32:01.400 --> 01:32:03.040]   Calm yourself.
[01:32:03.040 --> 01:32:08.200]   You may be a cheeky chappy, but you're also an exceptionally noisy one.
[01:32:08.200 --> 01:32:12.560]   The Prime Minister will please withdraw the word "idiot."
[01:32:12.560 --> 01:32:14.560]   It's unpalimentary, a simple word.
[01:32:14.560 --> 01:32:17.120]   Why don't we have somebody like this in Congress?
[01:32:17.120 --> 01:32:19.120]   That's what I want.
[01:32:19.120 --> 01:32:21.840]   Oh, this is famous.
[01:32:21.840 --> 01:32:23.840]   This guy stole the mace.
[01:32:23.840 --> 01:32:28.000]   Order the honorable gentleman to withdraw immediately from the house for the remainder
[01:32:28.000 --> 01:32:29.680]   of this day's sitting.
[01:32:29.680 --> 01:32:33.320]   This is intolerable behavior as far as the public.
[01:32:33.320 --> 01:32:34.680]   No, it's not funny.
[01:32:34.680 --> 01:32:36.560]   Nor does he like his authority.
[01:32:36.560 --> 01:32:40.800]   He's like the principle of parliament.
[01:32:40.800 --> 01:32:41.800]   You think you're amusing?
[01:32:41.800 --> 01:32:42.800]   You are not amusing.
[01:32:42.800 --> 01:32:45.560]   Oh, my God.
[01:32:45.560 --> 01:32:46.640]   John Burckow.
[01:32:46.640 --> 01:32:48.080]   Is he a big hero in Britain?
[01:32:48.080 --> 01:32:49.840]   Because he ought to be.
[01:32:49.840 --> 01:32:51.400]   It depends who you ask.
[01:32:51.400 --> 01:32:53.360]   He has a talk show here in the United States.
[01:32:53.360 --> 01:32:55.360]   He'd be on against Ellen.
[01:32:55.360 --> 01:33:00.640]   In fact, he's going to take Ellen's job, I'm sure.
[01:33:00.640 --> 01:33:06.960]   I'm sure Ellen would enjoy being interviewed by him in such a circumstance.
[01:33:06.960 --> 01:33:09.800]   I think they're quite different people, but I'd tune in.
[01:33:09.800 --> 01:33:11.600]   Have a nice feeling.
[01:33:11.600 --> 01:33:13.360]   Is Ellen in his role, honestly?
[01:33:13.360 --> 01:33:15.240]   Yeah, maybe Ellen could do a good job.
[01:33:15.240 --> 01:33:19.800]   She'd have a button she could push that the member of parliament would just disappear.
[01:33:19.800 --> 01:33:25.640]   Poor, I feel so bad for Matthew Miller of ZD and that fame.
[01:33:25.640 --> 01:33:26.640]   Great guy.
[01:33:26.640 --> 01:33:30.880]   We had him on the Technos Weekly on Thursday talking about this.
[01:33:30.880 --> 01:33:36.400]   He woke up a week ago, 1130 at night.
[01:33:36.400 --> 01:33:39.000]   His oldest daughter shook his shoulder to wake him up from a deep sleep.
[01:33:39.000 --> 01:33:43.880]   She says, "Daddy, Daddy, your Twitter account has been hacked."
[01:33:43.880 --> 01:33:47.840]   He writes, "It turns out it was much worse than that."
[01:33:47.840 --> 01:33:52.880]   He had an alert on his phone saying, "The SIM card for your number has been changed.
[01:33:52.880 --> 01:33:55.360]   If this change is not authorized, call 6-1-1."
[01:33:55.360 --> 01:33:59.680]   Well, he writes, "Seeing as how T-Mobile took away my cell service, I couldn't call
[01:33:59.680 --> 01:34:02.640]   6-1-1, so that was worthless."
[01:34:02.640 --> 01:34:07.040]   He had a Google Fi SIM and he called T-Mobile and they said, "Yeah.
[01:34:07.040 --> 01:34:10.120]   Yeah, somebody called you called.
[01:34:10.120 --> 01:34:14.280]   You called and said, "Change my SIM."
[01:34:14.280 --> 01:34:16.800]   The you that called was not him.
[01:34:16.800 --> 01:34:17.800]   It was, of course, a hacker.
[01:34:17.800 --> 01:34:22.320]   The representative said, "Well, we can't really tell who over the phone."
[01:34:22.320 --> 01:34:27.040]   As long as some key information is provided, we can authorize a swap.
[01:34:27.040 --> 01:34:29.320]   They authorize the swap.
[01:34:29.320 --> 01:34:34.480]   Now, without his password, that Matthew's Google password, the bad guys were able to
[01:34:34.480 --> 01:34:41.000]   take over Google, his Gmail, Twitter, and then once having taken over Gmail, they were
[01:34:41.000 --> 01:34:42.960]   able to reset his accounts.
[01:34:42.960 --> 01:34:48.560]   He got into his bank account, bought $25,000 worth of Bitcoin.
[01:34:48.560 --> 01:34:49.760]   It was a nightmare.
[01:34:49.760 --> 01:34:53.280]   I think he's pretty much dug out now.
[01:34:53.280 --> 01:34:56.240]   But imagine.
[01:34:56.240 --> 01:35:05.640]   And the most notable point on this is do not rely on SMS for authentication.
[01:35:05.640 --> 01:35:11.200]   If you're forced to, as many of us are, my bank will only take SMS.
[01:35:11.200 --> 01:35:17.520]   Make sure that you call your carrier and some will allow a pin number, T-Mobile will, and
[01:35:17.520 --> 01:35:21.520]   set that up so that somebody without the pin number cannot do this.
[01:35:21.520 --> 01:35:22.520]   Really?
[01:35:22.520 --> 01:35:24.440]   Such an amazing story.
[01:35:24.440 --> 01:35:28.440]   Is this, did it originate with his T-Mobile account being hacked?
[01:35:28.440 --> 01:35:29.440]   Yeah.
[01:35:29.440 --> 01:35:30.440]   Is that okay?
[01:35:30.440 --> 01:35:31.440]   Not hacked.
[01:35:31.440 --> 01:35:33.520]   They called customer service and said, "Hello, I'm Matthew Miller.
[01:35:33.520 --> 01:35:35.360]   I want to swap SIM cards."
[01:35:35.360 --> 01:35:36.360]   Still, I'm sorry.
[01:35:36.360 --> 01:35:39.880]   Matthew, what's your mother's maiden name or whatever?
[01:35:39.880 --> 01:35:43.560]   I don't know what the security question was, but clearly it wasn't very secure.
[01:35:43.560 --> 01:35:45.040]   Wow.
[01:35:45.040 --> 01:35:57.160]   Good advice to everybody, actually, in his ZDNet column on how to lock down your accounts,
[01:35:57.160 --> 01:35:58.040]   certainly put a pin number.
[01:35:58.040 --> 01:36:01.680]   I use Google Fi and Google Fi automatically does.
[01:36:01.680 --> 01:36:04.280]   Actually, you'd expect Google get this right and they did.
[01:36:04.280 --> 01:36:09.520]   The very secure pin number that's changed every time you log into your Google account.
[01:36:09.520 --> 01:36:10.520]   Terrifying.
[01:36:10.520 --> 01:36:17.040]   By the way, at the end, he says, "If anyone has tips on how I can get my Google and Twitter
[01:36:17.040 --> 01:36:20.000]   accounts back, I'd appreciate it."
[01:36:20.000 --> 01:36:23.840]   I guess he's still out of luck.
[01:36:23.840 --> 01:36:27.040]   Another reminder, don't use text based two-factor.
[01:36:27.040 --> 01:36:28.040]   Terrible.
[01:36:28.040 --> 01:36:30.680]   But most banks do, my bank does.
[01:36:30.680 --> 01:36:35.040]   Yeah, but more and more are supporting other things too, like authenticators.
[01:36:35.040 --> 01:36:38.160]   I haven't seen too many banks do authenticators, so you're right there.
[01:36:38.160 --> 01:36:44.080]   I have a UBICY on my keychain, carry with me at all times.
[01:36:44.080 --> 01:36:48.560]   Everything that I can, I have on that UBICY.
[01:36:48.560 --> 01:36:52.200]   If you can, and Google will allow you to turn this off, Twitter will not.
[01:36:52.200 --> 01:36:53.200]   Turn off SMS.
[01:36:53.200 --> 01:36:56.880]   Twitter still has SMS authentication as a fallback and always has it.
[01:36:56.880 --> 01:36:57.880]   There's nothing you can do about it.
[01:36:57.880 --> 01:37:02.440]   You can say, "I want to use a UBICY," but they'll still use SMS if you say, "Oh, I can't
[01:37:02.440 --> 01:37:05.120]   find my UBICY."
[01:37:05.120 --> 01:37:10.640]   I hope this is an example that maybe some of these companies pay attention to.
[01:37:10.640 --> 01:37:13.160]   Oh, I was doing Apple stuff.
[01:37:13.160 --> 01:37:14.160]   I'm sorry.
[01:37:14.160 --> 01:37:15.160]   Let's see.
[01:37:15.160 --> 01:37:18.400]   It's not that much to say.
[01:37:18.400 --> 01:37:23.440]   Best Buy is now going to be an authorized Apple repair.
[01:37:23.440 --> 01:37:28.760]   1000+ Best Buy stores across the nation.
[01:37:28.760 --> 01:37:30.280]   Apple certified repairs.
[01:37:30.280 --> 01:37:31.920]   This is something Apple started doing.
[01:37:31.920 --> 01:37:38.640]   It's funny, for a long time there were Apple Vars and resellers and Apple authorized repair
[01:37:38.640 --> 01:37:39.640]   centers.
[01:37:39.640 --> 01:37:44.320]   Then eventually, once Apple opened the retail stores, they closed all these down.
[01:37:44.320 --> 01:37:45.960]   They disappeared.
[01:37:45.960 --> 01:37:51.000]   It was a real loss, I thought, because Apple, well, for a lot of reasons, it's nice to
[01:37:51.000 --> 01:37:53.360]   have a shade tree mechanic.
[01:37:53.360 --> 01:37:54.360]   Go ahead.
[01:37:54.360 --> 01:37:59.960]   I like the story, though, because I like the idea of an ecosystem play.
[01:37:59.960 --> 01:38:07.120]   I liked when I was at Adobe Summit in March and heard Hubert Jolly, who was then the
[01:38:07.120 --> 01:38:12.240]   CEO at Best Buy, talking about this total tech support package they had rolled out for
[01:38:12.240 --> 01:38:18.560]   a $200 program a year that they will support all the tech, all the gadgets in your home,
[01:38:18.560 --> 01:38:21.440]   even if they didn't sell it to you.
[01:38:21.440 --> 01:38:26.520]   That's a pretty interesting play, I thought, to offer into a subscription household tech
[01:38:26.520 --> 01:38:28.680]   support service.
[01:38:28.680 --> 01:38:34.360]   It's neat to think of them figuring out a way to partner with Apple on this and entrench
[01:38:34.360 --> 01:38:40.280]   that further, offer value back to Apple Vars, offer value back to Best Buy.
[01:38:40.280 --> 01:38:44.760]   It seems like a pretty smart way to add value across that whole ecosystem.
[01:38:44.760 --> 01:38:46.400]   Yeah.
[01:38:46.400 --> 01:38:53.800]   I think the only problem is that people historically have not really trusted Best Buy's technicians.
[01:38:53.800 --> 01:38:55.360]   That's the Geek Squad, right?
[01:38:55.360 --> 01:38:57.160]   That's the Geek Squad.
[01:38:57.160 --> 01:39:01.040]   It's good and bad, and I've heard a lot of bad stories and a lot of new things about
[01:39:01.040 --> 01:39:02.680]   them upselling on the Sisserie Geek.
[01:39:02.680 --> 01:39:05.160]   But there are very good Geek Squad people, too.
[01:39:05.160 --> 01:39:06.160]   That's the problem.
[01:39:06.160 --> 01:39:07.160]   It's hit or miss.
[01:39:07.160 --> 01:39:09.000]   You might get somebody great.
[01:39:09.000 --> 01:39:10.000]   You might not.
[01:39:10.000 --> 01:39:11.200]   I agree with you, though.
[01:39:11.200 --> 01:39:16.920]   We desperately need something like this, Kate, because I was half an hour late to the show,
[01:39:16.920 --> 01:39:21.480]   because I spent an extra half hour after the radio show, helping an 80-year-old woman
[01:39:21.480 --> 01:39:27.160]   who had a Macintosh that was loaded with a malware adware, all sorts of stuff.
[01:39:27.160 --> 01:39:29.400]   There was nowhere she could go.
[01:39:29.400 --> 01:39:33.000]   Where are you going to go for something like that?
[01:39:33.000 --> 01:39:34.000]   Yeah.
[01:39:34.000 --> 01:39:36.600]   It seems really encouraging.
[01:39:36.600 --> 01:39:41.840]   The example that Huber Jolly gave at Adobe Summit was you deal with something like trying
[01:39:41.840 --> 01:39:47.800]   to watch a show on Netflix, and maybe you're dealing with the Netflix service itself on
[01:39:47.800 --> 01:39:52.680]   top of a Roku on top of your internet service provider on top of whatever.
[01:39:52.680 --> 01:39:57.520]   There's so many layers of complexity that the average household is set up with.
[01:39:57.520 --> 01:40:02.080]   That's not even taking into account all of the Internet of Things, smart home automation
[01:40:02.080 --> 01:40:03.080]   stuff.
[01:40:03.080 --> 01:40:07.000]   There's just a ton of complexity that people are adding into their households.
[01:40:07.000 --> 01:40:11.640]   Without anyone who necessarily has the expertise to troubleshoot all this stuff.
[01:40:11.640 --> 01:40:13.640]   That's a holistic approach.
[01:40:13.640 --> 01:40:17.000]   Because Apple's response to this is, "Well, it would all be fine if you just only buy Apple
[01:40:17.000 --> 01:40:18.000]   stuff."
[01:40:18.000 --> 01:40:19.000]   Right?
[01:40:19.000 --> 01:40:21.000]   Are you needing Apple TV?
[01:40:21.000 --> 01:40:22.000]   What's this Roku?
[01:40:22.000 --> 01:40:23.840]   You need an Apple TV.
[01:40:23.840 --> 01:40:27.960]   You need an Apple, but you don't make a router.
[01:40:27.960 --> 01:40:34.560]   If you just bought all Apple stuff, then you'd know who to call, but that's not practical.
[01:40:34.560 --> 01:40:39.840]   The best thing I'm surprised to that it's taken so long for even them to do this because
[01:40:39.840 --> 01:40:42.240]   they've had Geek Squad for so long.
[01:40:42.240 --> 01:40:46.040]   And all-inclusive annual pack would just make sense.
[01:40:46.040 --> 01:40:49.760]   There was, I think there was a tech support startup that somebody, I forget who was.
[01:40:49.760 --> 01:40:51.440]   Oh, there are quite a few.
[01:40:51.440 --> 01:40:54.400]   But somebody who would just send a person to their house.
[01:40:54.400 --> 01:40:57.840]   There was, in fact, we had them as an advertiser early on.
[01:40:57.840 --> 01:41:00.840]   I want to say Geek's On Call or something like that.
[01:41:00.840 --> 01:41:02.240]   I can't remember the name.
[01:41:02.240 --> 01:41:05.360]   But yeah, there've been a few attempts at this.
[01:41:05.360 --> 01:41:08.760]   It seems to be a low margin business.
[01:41:08.760 --> 01:41:11.400]   And the real problem is training.
[01:41:11.400 --> 01:41:15.160]   And this is where I would hope Apple would come to the best by Geek Squad folks or bring
[01:41:15.160 --> 01:41:18.680]   them to Cupertino or something and really give them some training.
[01:41:18.680 --> 01:41:21.720]   Apple used to do really good training with its geniuses.
[01:41:21.720 --> 01:41:23.640]   I think they've slowed down on that.
[01:41:23.640 --> 01:41:30.240]   My experience has been that the Apple geniuses are not as genius as they used to be.
[01:41:30.240 --> 01:41:33.120]   They're Apple subgeniuses.
[01:41:33.120 --> 01:41:36.800]   But I think that's an opportunity.
[01:41:36.800 --> 01:41:41.400]   The problem is that somebody who's that smart and that good can make a lot more money than
[01:41:41.400 --> 01:41:43.400]   a can at the Geek Squad.
[01:41:43.400 --> 01:41:44.400]   Yeah.
[01:41:44.400 --> 01:41:48.960]   Well, then they can at the genius bar too, probably.
[01:41:48.960 --> 01:41:49.960]   Yeah.
[01:41:49.960 --> 01:41:55.440]   It just says in this story, I write about this, that there's now, because of this, there's
[01:41:55.440 --> 01:42:01.360]   Apple-certified repairs courtesy of 7,600 newly Apple-certified technicians.
[01:42:01.360 --> 01:42:06.760]   So that's a wonderful volume to add into the landscape.
[01:42:06.760 --> 01:42:14.560]   So it has been frustrating to be in this gap between where genius bars existed and where
[01:42:14.560 --> 01:42:18.840]   you can now get some kind of support at maybe Best Buy.
[01:42:18.840 --> 01:42:25.400]   But yeah, there does seem to need to be some kind of service offering out there.
[01:42:25.400 --> 01:42:28.520]   And I think this idea from Best Buy is a solid one.
[01:42:28.520 --> 01:42:30.720]   I look forward to seeing how it plays out for them.
[01:42:30.720 --> 01:42:34.920]   Also, something you could probably gift for your family members, just like if you were
[01:42:34.920 --> 01:42:38.960]   the family tech support person, maybe get your parents this or something.
[01:42:38.960 --> 01:42:41.160]   It's a little passive aggressive.
[01:42:41.160 --> 01:42:42.160]   Yeah, yeah.
[01:42:42.160 --> 01:42:44.160]   Don't call me Call the Geek Squad.
[01:42:44.160 --> 01:42:45.800]   It was a nerds on site.
[01:42:45.800 --> 01:42:46.800]   They used to advertise with this.
[01:42:46.800 --> 01:42:50.040]   In fact, they're founders still in the chat room from time to time.
[01:42:50.040 --> 01:42:53.240]   David Redicop, I see him from time to time.
[01:42:53.240 --> 01:43:00.120]   I'm curious if David, if you're listening, I'd love to know how you get good quality
[01:43:00.120 --> 01:43:01.120]   people.
[01:43:01.120 --> 01:43:06.840]   I mean, they've been doing this since '95 and a very good reputation.
[01:43:06.840 --> 01:43:09.640]   But it must be difficult to get and retain good technicians.
[01:43:09.640 --> 01:43:10.640]   I don't know.
[01:43:10.640 --> 01:43:11.640]   I don't know.
[01:43:11.640 --> 01:43:12.640]   It's an expensive thing.
[01:43:12.640 --> 01:43:15.400]   That's why I remember Dell used to have this great tech support.
[01:43:15.400 --> 01:43:16.400]   Really great.
[01:43:16.400 --> 01:43:18.680]   I remember going down to Austin to visit the call center.
[01:43:18.680 --> 01:43:23.120]   And these guys were super smart, but it was not cost effective.
[01:43:23.120 --> 01:43:29.560]   And I don't know what the answer is to that, but clearly we need it.
[01:43:29.560 --> 01:43:33.440]   All people cannot be expected to use technology safely and effectively.
[01:43:33.440 --> 01:43:34.440]   Yeah.
[01:43:34.440 --> 01:43:40.920]   Not when you have to restart your light bulb 12 times.
[01:43:40.920 --> 01:43:43.040]   Exactly.
[01:43:43.040 --> 01:43:44.840]   We had a fun week this week on Twit.
[01:43:44.840 --> 01:43:49.200]   Do we have a little Kevin, a little video, a little something prepared for the people
[01:43:49.200 --> 01:43:50.200]   that they can watch?
[01:43:50.200 --> 01:43:53.120]   This is what you might have missed this week on Twit.
[01:43:53.120 --> 01:43:54.680]   Previously on Twit.
[01:43:54.680 --> 01:43:57.880]   Mary Jo, I know you know, "Handhouse Brewing Company."
[01:43:57.880 --> 01:43:59.080]   I've heard of them.
[01:43:59.080 --> 01:44:03.480]   I don't know how to cross the street, but I thought you'd enjoy this one.
[01:44:03.480 --> 01:44:05.320]   Keanu is immortal.
[01:44:05.320 --> 01:44:07.720]   Oh, that's perfect.
[01:44:07.720 --> 01:44:08.720]   Windows Weekly.
[01:44:08.720 --> 01:44:12.120]   You want to run the world, but you realize you can't get elected.
[01:44:12.120 --> 01:44:15.200]   Well, how much better than a global currency?
[01:44:15.200 --> 01:44:17.680]   So you say a little higher, you say it's a little higher.
[01:44:17.680 --> 01:44:19.200]   Maybe like Bond villain level.
[01:44:19.200 --> 01:44:20.200]   Yeah.
[01:44:20.200 --> 01:44:21.200]   Yeah.
[01:44:21.200 --> 01:44:22.480]   This is Emperor of the World.
[01:44:22.480 --> 01:44:24.600]   This is not president.
[01:44:24.600 --> 01:44:26.360]   All about Android.
[01:44:26.360 --> 01:44:32.440]   Can we nominate the Nvidia Shield entry into the Android Hall of Fame as one of the most
[01:44:32.440 --> 01:44:34.680]   durable and used and loved products?
[01:44:34.680 --> 01:44:37.080]   The sucker is still updated and it came out.
[01:44:37.080 --> 01:44:40.680]   Carrying the flag for Android TV.
[01:44:40.680 --> 01:44:42.400]   This weekend, Enterprise Tech.
[01:44:42.400 --> 01:44:48.480]   There's no question that this has been an expensive year for municipalities having to
[01:44:48.480 --> 01:44:50.320]   deal with ransomware.
[01:44:50.320 --> 01:44:55.400]   Are there any sort of hard and fast rules that say, "Okay, it's okay to pay whatever
[01:44:55.400 --> 01:44:59.080]   the attackers want just to get the data back?"
[01:44:59.080 --> 01:45:00.080]   Hands on tech.
[01:45:00.080 --> 01:45:05.080]   I have right here the 10.1 inch on tablet and I've been living with it for a little over
[01:45:05.080 --> 01:45:08.360]   a week and let me just say, I have thoughts to it.
[01:45:08.360 --> 01:45:13.880]   We dumb the kick and tap the "twit."
[01:45:13.880 --> 01:45:18.720]   You see, if you just, you know, watch, just always ask five, ten hours a day.
[01:45:18.720 --> 01:45:23.240]   Just watch and you'll be really up to date on what's going on.
[01:45:23.240 --> 01:45:24.240]   Get.tv.
[01:45:24.240 --> 01:45:25.880]   Our show today brought to you by HelloFresh.
[01:45:25.880 --> 01:45:29.880]   I am looking forward to going home and making a fabulous dinner.
[01:45:29.880 --> 01:45:32.560]   This is a good example of why you want HelloFresh.
[01:45:32.560 --> 01:45:33.560]   I'm working all day.
[01:45:33.560 --> 01:45:34.560]   I get home.
[01:45:34.560 --> 01:45:35.560]   I'll get home at dinner time.
[01:45:35.560 --> 01:45:40.280]   I don't have time to plan a meal to shop for the ingredients and to make it, but I
[01:45:40.280 --> 01:45:42.000]   don't want to go out to McDonald's either.
[01:45:42.000 --> 01:45:44.560]   I want something fresh and healthy and delicious.
[01:45:44.560 --> 01:45:45.960]   HelloFresh.
[01:45:45.960 --> 01:45:47.760]   I get to home and there it is in the fridge.
[01:45:47.760 --> 01:45:50.200]   I go, "HelloFresh."
[01:45:50.200 --> 01:45:55.840]   HelloFresh offers home cooked meals made simple because they provide you with all the ingredients
[01:45:55.840 --> 01:45:58.920]   to really neat recipes.
[01:45:58.920 --> 01:46:03.080]   They'll put you a little bit, I like it because I like to cook, but I'm getting a rut, don't
[01:46:03.080 --> 01:46:04.080]   we all?
[01:46:04.080 --> 01:46:05.360]   Cooking the same thing day and day out.
[01:46:05.360 --> 01:46:08.840]   Taco Tuesday, lasagna Wednesday.
[01:46:08.840 --> 01:46:10.120]   Get outside your comfort zone.
[01:46:10.120 --> 01:46:12.040]   Discover new delicious recipes.
[01:46:12.040 --> 01:46:15.600]   You'll be able to put together in 30 minutes or less.
[01:46:15.600 --> 01:46:18.400]   Let them do the meal planning, the shopping, the prep.
[01:46:18.400 --> 01:46:22.840]   You conquered the kitchen and it smells great in the house and people come with excitement
[01:46:22.840 --> 01:46:24.360]   to sit down at the table.
[01:46:24.360 --> 01:46:29.080]   Each box comes with fresh pre-measured ingredients and easy to follow recipe cards, no more than
[01:46:29.080 --> 01:46:31.760]   six steps, even pictures.
[01:46:31.760 --> 01:46:34.760]   You know what this is supposed to look like.
[01:46:34.760 --> 01:46:37.120]   It comes right to your door in a special insulated box.
[01:46:37.120 --> 01:46:40.640]   They even have instructions on how to recycle everything in the box.
[01:46:40.640 --> 01:46:41.960]   You don't even have to leave your house.
[01:46:41.960 --> 01:46:44.240]   They have three meal plans to choose from.
[01:46:44.240 --> 01:46:45.960]   Classic, veggie and family.
[01:46:45.960 --> 01:46:48.560]   But you're not locked in once you choose your plan.
[01:46:48.560 --> 01:46:51.840]   You could change it at any point to match your taste buds or your mood.
[01:46:51.840 --> 01:46:54.800]   Make family dinners fuss free with Hello Frishes.
[01:46:54.800 --> 01:46:58.320]   Picky Eater Kid tested and approved family plan recipes.
[01:46:58.320 --> 01:47:01.440]   It's a great thing to do with the kids too, by the way.
[01:47:01.440 --> 01:47:03.180]   They're going to have to learn how to cook.
[01:47:03.180 --> 01:47:04.180]   Make it fun.
[01:47:04.180 --> 01:47:05.180]   Make it exciting.
[01:47:05.180 --> 01:47:06.680]   They have fun menu features too.
[01:47:06.680 --> 01:47:07.800]   Things like dinner to lunch.
[01:47:07.800 --> 01:47:12.340]   So you get dinner, but you also get a nice little lunch all from the dinner and from left
[01:47:12.340 --> 01:47:13.340]  overs.
[01:47:13.340 --> 01:47:16.340]   I'm going to take a meal.
[01:47:16.340 --> 01:47:18.340]   I'm going to take a meal.
[01:47:18.340 --> 01:47:19.340]   I'm going to take a meal.
[01:47:19.340 --> 01:47:20.340]   I'm going to take a meal.
[01:47:20.340 --> 01:47:21.340]   I'm going to take a meal.
[01:47:21.340 --> 01:47:22.340]   I'm going to take a meal.
[01:47:22.340 --> 01:47:23.340]   I'm going to take a meal.
[01:47:23.340 --> 01:47:24.340]   I'm going to take a meal.
[01:47:24.340 --> 01:47:25.340]   I'm going to take a meal.
[01:47:25.340 --> 01:47:26.340]   I'm going to take a meal.
[01:47:26.340 --> 01:47:27.340]   I'm going to take a meal.
[01:47:27.340 --> 01:47:28.340]   I'm going to take a meal.
[01:47:28.340 --> 01:47:29.340]   I'm going to take a meal.
[01:47:29.340 --> 01:47:30.340]   I'm going to take a meal.
[01:47:30.340 --> 01:47:31.340]   I'm going to take a meal.
[01:47:31.340 --> 01:47:32.340]   I'm going to take a meal.
[01:47:32.340 --> 01:47:33.340]   I'm going to take a meal.
[01:47:33.340 --> 01:47:34.340]   I'm going to take a meal.
[01:47:34.340 --> 01:47:35.340]   I'm going to take a meal.
[01:47:35.340 --> 01:47:36.340]   I'm going to take a meal.
[01:47:36.340 --> 01:47:37.340]   I'm going to take a meal.
[01:47:37.340 --> 01:47:38.340]   I'm going to take a meal.
[01:47:38.340 --> 01:47:39.340]   I'm going to take a meal.
[01:47:39.340 --> 01:47:40.340]   I'm going to take a meal.
[01:47:40.340 --> 01:47:41.340]   I'm going to take a meal.
[01:47:41.340 --> 01:47:42.340]   I'm going to take a meal.
[01:47:42.340 --> 01:47:43.340]   I'm going to take a meal.
[01:47:43.340 --> 01:47:44.340]   I'm going to take a meal.
[01:47:44.340 --> 01:47:45.340]   I'm going to take a meal.
[01:47:45.340 --> 01:47:46.340]   I'm going to take a meal.
[01:47:46.340 --> 01:47:47.340]   I'm going to take a meal.
[01:47:47.340 --> 01:47:48.340]   I'm going to take a meal.
[01:47:48.340 --> 01:47:49.340]   I'm going to take a meal.
[01:47:49.340 --> 01:47:50.340]   I'm going to take a meal.
[01:47:50.340 --> 01:47:51.340]   I'm going to take a meal.
[01:47:51.340 --> 01:47:52.340]   I'm going to take a meal.
[01:47:52.340 --> 01:47:53.340]   I'm going to take a meal.
[01:47:53.340 --> 01:47:54.340]   I'm going to take a meal.
[01:47:54.340 --> 01:47:55.340]   I'm going to take a meal.
[01:47:55.340 --> 01:47:56.340]   I'm going to take a meal.
[01:47:56.340 --> 01:47:57.340]   I'm going to take a meal.
[01:47:57.340 --> 01:47:58.340]   I'm going to take a meal.
[01:47:58.340 --> 01:47:59.340]   I'm going to take a meal.
[01:47:59.340 --> 01:48:00.340]   I'm going to take a meal.
[01:48:00.340 --> 01:48:02.340]   I'm going to take a meal.
[01:48:02.340 --> 01:48:03.340]   I'm going to take a meal.
[01:48:03.340 --> 01:48:04.340]   I'm going to take a meal.
[01:48:04.340 --> 01:48:05.340]   I'm going to take a meal.
[01:48:05.340 --> 01:48:06.340]   I'm going to take a meal.
[01:48:06.340 --> 01:48:07.340]   I'm going to take a meal.
[01:48:07.340 --> 01:48:08.340]   I'm going to take a meal.
[01:48:08.340 --> 01:48:09.340]   I'm going to take a meal.
[01:48:09.340 --> 01:48:10.340]   I'm going to take a meal.
[01:48:10.340 --> 01:48:11.340]   I'm going to take a meal.
[01:48:11.340 --> 01:48:12.340]   I'm going to take a meal.
[01:48:12.340 --> 01:48:13.340]   I'm going to take a meal.
[01:48:13.340 --> 01:48:14.340]   I'm going to take a meal.
[01:48:14.340 --> 01:48:15.340]   I'm going to take a meal.
[01:48:15.340 --> 01:48:16.340]   I'm going to take a meal.
[01:48:16.340 --> 01:48:17.340]   I'm going to take a meal.
[01:48:17.340 --> 01:48:18.340]   I'm going to take a meal.
[01:48:18.340 --> 01:48:19.340]   I'm going to take a meal.
[01:48:19.340 --> 01:48:20.340]   I'm going to take a meal.
[01:48:20.340 --> 01:48:21.340]   I'm going to take a meal.
[01:48:21.340 --> 01:48:22.340]   I'm going to take a meal.
[01:48:22.340 --> 01:48:23.340]   I'm going to take a meal.
[01:48:23.340 --> 01:48:24.340]   I'm going to take a meal.
[01:48:24.340 --> 01:48:25.340]   I'm going to take a meal.
[01:48:25.340 --> 01:48:26.340]   I'm going to take a meal.
[01:48:26.340 --> 01:48:27.340]   I'm going to take a meal.
[01:48:27.340 --> 01:48:28.340]   I'm going to take a meal.
[01:48:28.340 --> 01:48:29.340]   I'm going to take a meal.
[01:48:29.340 --> 01:48:30.340]   I'm going to take a meal.
[01:48:30.340 --> 01:48:31.340]   I'm going to take a meal.
[01:48:31.340 --> 01:48:32.340]   I'm going to take a meal.
[01:48:32.340 --> 01:48:33.340]   I'm going to take a meal.
[01:48:33.340 --> 01:48:34.340]   I'm going to take a meal.
[01:48:34.340 --> 01:48:35.340]   I'm going to take a meal.
[01:48:35.340 --> 01:48:36.340]   I'm going to take a meal.
[01:48:36.340 --> 01:48:37.340]   I'm going to take a meal.
[01:48:37.340 --> 01:48:38.340]   I'm going to take a meal.
[01:48:38.340 --> 01:48:39.340]   I'm going to take a meal.
[01:48:39.340 --> 01:48:40.340]   I'm going to take a meal.
[01:48:40.340 --> 01:48:41.340]   I'm going to take a meal.
[01:48:41.340 --> 01:48:42.340]   I'm going to take a meal.
[01:48:42.340 --> 01:48:43.340]   I'm going to take a meal.
[01:48:43.340 --> 01:48:44.340]   I'm going to take a meal.
[01:48:44.340 --> 01:48:45.340]   I'm going to take a meal.
[01:48:45.340 --> 01:48:46.340]   What do you think, Nate?
[01:48:46.340 --> 01:48:49.340]   Is this a little slanted against Chrome?
[01:48:49.340 --> 01:48:54.580]   I mean, cookies are not the end of the world, despite what you might think if you go wandering
[01:48:54.580 --> 01:48:57.340]   around the web these days.
[01:48:57.340 --> 01:49:02.540]   I don't know what his methodology was, but it wouldn't surprise me to be perfectly honest.
[01:49:02.540 --> 01:49:09.340]   A lot of the problem is these background processes that we generally always turn them
[01:49:09.340 --> 01:49:10.340]   on and leave them on.
[01:49:10.340 --> 01:49:14.340]   But what we don't do is monitor what they're doing in the background overnight.
[01:49:14.340 --> 01:49:16.940]   I think that was part of his point.
[01:49:16.940 --> 01:49:23.580]   On Apple, I was a little more surprised at the numbers because Apple tends to be a little
[01:49:23.580 --> 01:49:29.260]   more conservative over what apps are allowed to do what in the background.
[01:49:29.260 --> 01:49:33.220]   With Google, though, I always think why is anybody surprised?
[01:49:33.220 --> 01:49:37.300]   Most of Google's products don't cost you anything.
[01:49:37.300 --> 01:49:40.340]   I'm excluding, obviously, phones in the Google Home.
[01:49:40.340 --> 01:49:45.340]   In terms of his web products, you're not paying for them, really.
[01:49:45.340 --> 01:49:47.140]   So, 11,000, I don't know.
[01:49:47.140 --> 01:49:49.340]   It doesn't sound like a massive number to me.
[01:49:49.340 --> 01:49:52.340]   I mean, rather, it doesn't sound like a surprising number to me.
[01:49:52.340 --> 01:49:53.340]   Yeah, it's credible.
[01:49:53.340 --> 01:49:54.340]   Yeah.
[01:49:54.340 --> 01:49:55.340]   Yeah.
[01:49:55.340 --> 01:49:56.340]   So he calls them tracker cookies.
[01:49:56.340 --> 01:50:00.220]   He said they would be automatically blocked by Firefox.
[01:50:00.220 --> 01:50:02.580]   But cookies by themselves are not a bad thing.
[01:50:02.580 --> 01:50:05.620]   It's how a website knows you're you when you go back to the website.
[01:50:05.620 --> 01:50:08.460]   There's a lot of cookies that you want, I think.
[01:50:08.460 --> 01:50:13.140]   And if you don't believe me, go into Google, go into Chrome and turn off cookies.
[01:50:13.140 --> 01:50:14.140]   You can.
[01:50:14.140 --> 01:50:16.820]   You could do that in any browser, say, "I don't want you to save any cookies."
[01:50:16.820 --> 01:50:18.380]   And see what your web experience is like.
[01:50:18.380 --> 01:50:19.380]   It's not fun.
[01:50:19.380 --> 01:50:20.380]   Yeah.
[01:50:20.380 --> 01:50:24.620]   But you don't have, let's sign in data for the handful of sites you actually use, but
[01:50:24.620 --> 01:50:26.780]   that's not going to be 11,000, right?
[01:50:26.780 --> 01:50:32.780]   And I think what Jeff is doing is kind of interesting because it kind of goes to like,
[01:50:32.780 --> 01:50:36.900]   we're talking, we complain now about Chrome being a memory hog, about being all these
[01:50:36.900 --> 01:50:37.900]   sorts of things.
[01:50:37.900 --> 01:50:42.460]   When it came out, it was the fast competitor to even Firefox and to even explore.
[01:50:42.460 --> 01:50:43.460]   Not anymore.
[01:50:43.460 --> 01:50:45.300]   Now it's become the dominant browser.
[01:50:45.300 --> 01:50:47.900]   What is weighing Chrome down?
[01:50:47.900 --> 01:50:49.740]   Cookies don't take up much disk space.
[01:50:49.740 --> 01:50:52.220]   They don't really hinder your system that much.
[01:50:52.220 --> 01:50:56.460]   But this is another sign of bloat that kind of worries me.
[01:50:56.460 --> 01:51:00.940]   And another reason why we should probably take a closer look at Firefox.
[01:51:00.940 --> 01:51:05.500]   But even like Microsoft with their next edge, they're also going to be blocking certain
[01:51:05.500 --> 01:51:06.500]   types of cookies.
[01:51:06.500 --> 01:51:10.700]   They have a really understandable security system that they're going to be producing
[01:51:10.700 --> 01:51:11.700]   with that browser.
[01:51:11.700 --> 01:51:15.140]   And I think that's going to be an important thing moving forward because people are caring
[01:51:15.140 --> 01:51:16.420]   more about privacy now.
[01:51:16.420 --> 01:51:18.060]   So this seems useful.
[01:51:18.060 --> 01:51:19.500]   This is not just like anti-Google.
[01:51:19.500 --> 01:51:23.660]   I think this is kind of a wake up call to like, you know, all of us who have been using
[01:51:23.660 --> 01:51:29.620]   Chrome for so long to remind us of the cost of using Chrome basically.
[01:51:29.620 --> 01:51:33.500]   And maybe there's a reason to use another browser, really is consider one.
[01:51:33.500 --> 01:51:41.820]   He said, I watched Etna and weirdly the federal student aid website set cookies for Facebook
[01:51:41.820 --> 01:51:42.820]   and Google.
[01:51:42.820 --> 01:51:48.820]   In other words, they told Facebook and Google that he was on the insurance site or the loan
[01:51:48.820 --> 01:51:51.340]   services login page.
[01:51:51.340 --> 01:51:58.660]   So I don't know if that's an oversight from Etna and the FSA site or if it's a convenience
[01:51:58.660 --> 01:52:02.420]   because they use Google sign on and Facebook sign on.
[01:52:02.420 --> 01:52:05.100]   It's not completely clear.
[01:52:05.100 --> 01:52:06.900]   He sounds like the whole tracking.
[01:52:06.900 --> 01:52:08.340]   Maybe it is pixel tracking.
[01:52:08.340 --> 01:52:13.500]   And he points out that Google did change their policy that you're as soon as you use
[01:52:13.500 --> 01:52:17.620]   Gmail at any time, you're automatically now logged into Google constantly whenever you
[01:52:17.620 --> 01:52:20.580]   use Chrome, you're in Google.
[01:52:20.580 --> 01:52:26.580]   So Google that does know where you're going at this point.
[01:52:26.580 --> 01:52:28.860]   Yeah, I want to know more about the methodology.
[01:52:28.860 --> 01:52:33.420]   These are part of the internet, the modern internet and not a bad thing.
[01:52:33.420 --> 01:52:34.740]   Tracking cookies maybe.
[01:52:34.740 --> 01:52:36.820]   I've been using Brave of late.
[01:52:36.820 --> 01:52:37.820]   I don't know.
[01:52:37.820 --> 01:52:38.820]   Do you guys familiar with Brave?
[01:52:38.820 --> 01:52:39.820]   Do you know it?
[01:52:39.820 --> 01:52:40.820]   Do you like it?
[01:52:40.820 --> 01:52:41.820]   I've never really looked at that.
[01:52:41.820 --> 01:52:42.820]   Yeah.
[01:52:42.820 --> 01:52:45.940]   Brave is a privacy based version of Chromium.
[01:52:45.940 --> 01:52:48.500]   So it's Chrome.
[01:52:48.500 --> 01:52:51.900]   But for instance, they have settings in the Brave browser to turn off all that single
[01:52:51.900 --> 01:52:56.180]   sign on stuff from LinkedIn, Facebook, Google and Twitter.
[01:52:56.180 --> 01:52:59.540]   And that means no tracking information will be sent back to those sites because you don't
[01:52:59.540 --> 01:53:06.740]   even have to be using it because that's going to ping those sites every time you go there.
[01:53:06.740 --> 01:53:08.180]   Brave seems to do a pretty good job.
[01:53:08.180 --> 01:53:12.660]   There are other privacy focused browsers like Vivaldi, Opera, both of which are based on
[01:53:12.660 --> 01:53:14.260]   Chromium.
[01:53:14.260 --> 01:53:17.380]   And Microsoft's doing its version of Edge based on Chromium.
[01:53:17.380 --> 01:53:21.780]   And I think Microsoft's also saying, yeah, we're not going to do all that stuff that Google's
[01:53:21.780 --> 01:53:23.860]   doing right now.
[01:53:23.860 --> 01:53:25.780]   I am not a fan of Firefox.
[01:53:25.780 --> 01:53:26.780]   I just can't.
[01:53:26.780 --> 01:53:29.780]   It's just an aesthetic thing to me.
[01:53:29.780 --> 01:53:32.380]   But maybe what do you use, Kate?
[01:53:32.380 --> 01:53:34.220]   I switched to Firefox.
[01:53:34.220 --> 01:53:37.980]   You know, I've been back and forth over the years to different browsers.
[01:53:37.980 --> 01:53:44.060]   And in this last year, I saw a woman who was one of the leaders at Firefox speaking about
[01:53:44.060 --> 01:53:46.180]   security and data privacy.
[01:53:46.180 --> 01:53:47.180]   And she sold me.
[01:53:47.180 --> 01:53:49.140]   So I switched.
[01:53:49.140 --> 01:53:51.180]   And it is an aesthetic difference.
[01:53:51.180 --> 01:53:52.180]   You're right.
[01:53:52.180 --> 01:53:56.500]   And it does kind of change the way things are rounded and the default fonts.
[01:53:56.500 --> 01:53:59.180]   And everything is just a little different.
[01:53:59.180 --> 01:54:04.620]   But I think it is important to your point that we understand that cookies themselves
[01:54:04.620 --> 01:54:10.060]   are not the bad-- they're not the evil that sounds like we're talking about here.
[01:54:10.060 --> 01:54:14.260]   Cookies are an important part of personalizing content and making it convenient and easy
[01:54:14.260 --> 01:54:19.420]   for all of us to do what we do in the contemporary usage of the web.
[01:54:19.420 --> 01:54:25.340]   But I remember about a dozen years ago, whenever you were doing something that required a
[01:54:25.340 --> 01:54:31.460]   new tool or a new-- like you were adding some functionality to a website, it was always
[01:54:31.460 --> 01:54:35.660]   like, oh, it's just one line of JavaScript that you add to the checkout page or something
[01:54:35.660 --> 01:54:36.660]   like that.
[01:54:36.660 --> 01:54:37.660]   Like one line of JavaScript.
[01:54:37.660 --> 01:54:38.980]   It's like one cookie.
[01:54:38.980 --> 01:54:44.060]   And after you do this for a number of years, you've got another line of JavaScript and another
[01:54:44.060 --> 01:54:45.060]   cookie.
[01:54:45.060 --> 01:54:46.940]   It's just we've accumulated a lot of this craft.
[01:54:46.940 --> 01:54:51.900]   So I think it's a good awareness campaign to bring our attention to it.
[01:54:51.900 --> 01:54:53.700]   We're all being tracked all the time.
[01:54:53.700 --> 01:54:57.820]   There's so much of a data trail that we're all generating and everything that we do.
[01:54:57.820 --> 01:54:59.980]   It's good to be aware of that.
[01:54:59.980 --> 01:55:02.940]   And at the same time, I don't think it's good to panic about that.
[01:55:02.940 --> 01:55:08.100]   I also think all you've got to do is go to Europe and use a browser.
[01:55:08.100 --> 01:55:10.460]   Nate knows.
[01:55:10.460 --> 01:55:15.620]   Like the moment you try to hit any website, you're being prompted to accept these cookies.
[01:55:15.620 --> 01:55:21.500]   The whole GDPR implementation is a real user experience, like nuisance.
[01:55:21.500 --> 01:55:22.500]   And that's a failure.
[01:55:22.500 --> 01:55:23.500]   It's not ideal.
[01:55:23.500 --> 01:55:27.340]   That's clearly a failure because it becomes then it becomes background noise.
[01:55:27.340 --> 01:55:28.500]   Every site uses cookies.
[01:55:28.500 --> 01:55:30.020]   Every site warns you they use cookies.
[01:55:30.020 --> 01:55:34.260]   The first thing you do when you go to a new site is you click that banner, which is an
[01:55:34.260 --> 01:55:37.700]   annoying extra click and it solves nothing.
[01:55:37.700 --> 01:55:38.700]   No, it's not.
[01:55:38.700 --> 01:55:43.580]   I mean, we've got to the point now, honestly, where ads are the least annoying things on
[01:55:43.580 --> 01:55:46.180]   the web in Europe.
[01:55:46.180 --> 01:55:53.180]   I would take pop-ups any day over what we have to put up with in Europe.
[01:55:53.180 --> 01:55:55.340]   I don't trust anywhere browser either.
[01:55:55.340 --> 01:56:01.060]   I mean, I use Safari because I trust Apple slightly more than I trust any of the others.
[01:56:01.060 --> 01:56:04.740]   I don't use any Google products whatsoever.
[01:56:04.740 --> 01:56:05.740]   Really?
[01:56:05.740 --> 01:56:06.740]   Yeah.
[01:56:06.740 --> 01:56:08.700]   The only one is Google search.
[01:56:08.700 --> 01:56:12.140]   I use Google search in a private window, but that's the only one.
[01:56:12.140 --> 01:56:16.340]   I don't use anything else because I don't trust them.
[01:56:16.340 --> 01:56:20.780]   I think more people should be massively distrustful of a lot of these companies, to be honest.
[01:56:20.780 --> 01:56:23.700]   Anything that raises awareness is good.
[01:56:23.700 --> 01:56:27.620]   One thing I will agree, I think the problem with Jeffery's article is written for a general
[01:56:27.620 --> 01:56:28.620]   audience.
[01:56:28.620 --> 01:56:30.940]   And so it doesn't tell me the specifics that I'd like to know.
[01:56:30.940 --> 01:56:34.940]   It sounds like I don't know if all 11,000 of those tracking cookies, so-called tracking
[01:56:34.940 --> 01:56:37.260]   cookies were third-party cookies.
[01:56:37.260 --> 01:56:39.580]   The third-party cookies are a problem.
[01:56:39.580 --> 01:56:43.660]   And that's the case when Enna is setting a cookie for Facebook, a third-party.
[01:56:43.660 --> 01:56:46.660]   And every browser except Chrome lets you turn that off.
[01:56:46.660 --> 01:56:50.500]   No browser, in fact, most browsers default as block third-party cookies.
[01:56:50.500 --> 01:56:55.780]   Don't allow a site that you're visiting to set a cookie for another site like Facebook
[01:56:55.780 --> 01:56:57.380]   when you're visiting it.
[01:56:57.380 --> 01:56:59.460]   And that's really most of the tracking.
[01:56:59.460 --> 01:57:05.180]   So I don't know if Jeffery's saying, "Yeah, I had 11,000 third-party cookies set."
[01:57:05.180 --> 01:57:08.620]   If that's the case, that's appalling.
[01:57:08.620 --> 01:57:11.380]   And I guess I should do my own research.
[01:57:11.380 --> 01:57:17.620]   But in every browser, including Safari, Firefox, Edge, everything but Chrome, as far as I could
[01:57:17.620 --> 01:57:19.740]   tell, you can block third-party cookies.
[01:57:19.740 --> 01:57:22.700]   And in most cases, that's default and should be default.
[01:57:22.700 --> 01:57:25.420]   Because that's the one that's the tracking cookie.
[01:57:25.420 --> 01:57:31.580]   There's even a tweet, and I retweeted it most recently on my account.
[01:57:31.580 --> 01:57:35.580]   But there's a tweet from the Pinboard account, which points out that-
[01:57:35.580 --> 01:57:36.580]   Oh, but Chase-
[01:57:36.580 --> 01:57:38.380]   Oh, this is such a good article.
[01:57:38.380 --> 01:57:39.380]   Go ahead.
[01:57:39.380 --> 01:57:41.700]   Yeah, so he points out that the Washington-
[01:57:41.700 --> 01:57:47.260]   Well, it's not the article that you're about to reference, but he points out that this Washington
[01:57:47.260 --> 01:57:51.820]   Post article about the Google's web browser contains itself-
[01:57:51.820 --> 01:57:52.820]   Oh, yeah.
[01:57:52.820 --> 01:58:00.580]   A bunch of adcraft, including from the Washington Post, and it doesn't contain disclosure, which
[01:58:00.580 --> 01:58:06.620]   he goes on through a nice tweet thread to unpack that and what that really means.
[01:58:06.620 --> 01:58:14.300]   But I think that the overall gist of this from the big picture perspective is you can't
[01:58:14.300 --> 01:58:16.540]   cause panic about this.
[01:58:16.540 --> 01:58:19.580]   It's not a crisis when cookies are set.
[01:58:19.580 --> 01:58:24.820]   But if we're going to talk about cruft and a bunch of junk being set in browsers, then
[01:58:24.820 --> 01:58:29.260]   we have to be really candid about where these are being set, what they're being set for.
[01:58:29.260 --> 01:58:32.380]   And I think you're right, Leo, that we've got to understand the difference between first
[01:58:32.380 --> 01:58:33.540]   and third-party cookies.
[01:58:33.540 --> 01:58:34.900]   We've got to understand why.
[01:58:34.900 --> 01:58:37.940]   We've got to understand what they're accomplishing for us.
[01:58:37.940 --> 01:58:43.220]   And none of this is happening in this article or in the GDPR or in anything that's happening.
[01:58:43.220 --> 01:58:44.220]   Yeah.
[01:58:44.220 --> 01:58:45.220]   So it's still a long way to go.
[01:58:45.220 --> 01:58:49.460]   When I see at the beginning of the article a video of some guy walking down the street
[01:58:49.460 --> 01:58:55.300]   being followed by cookies, I guess, I feel like that's a little sensationalistic.
[01:58:55.300 --> 01:58:57.700]   Yes, the Washington Post is setting cookies too.
[01:58:57.700 --> 01:59:02.900]   And in fact, in his previous article about his iPhone phoning home, he does mention it
[01:59:02.900 --> 01:59:03.900]   in the article.
[01:59:03.900 --> 01:59:07.660]   And I think those apps was Washington Post's own app that did, in fact, do exactly the
[01:59:07.660 --> 01:59:08.860]   same thing.
[01:59:08.860 --> 01:59:11.660]   So yeah, I don't know.
[01:59:11.660 --> 01:59:15.980]   I don't use Firefox if you feel better about Firefox.
[01:59:15.980 --> 01:59:17.060]   I like Brave.
[01:59:17.060 --> 01:59:19.660]   I use Brave for the most part on the Mac.
[01:59:19.660 --> 01:59:25.260]   I use Safari because I feel like, do you guys think that Apple is now becoming the de facto
[01:59:25.260 --> 01:59:28.500]   protector of privacy for some people?
[01:59:28.500 --> 01:59:29.500]   Like do people trust?
[01:59:29.500 --> 01:59:31.500]   What do you think?
[01:59:31.500 --> 01:59:35.740]   Do you think Apple is the trustworthy company in this?
[01:59:35.740 --> 01:59:37.260]   I look at it the other way around.
[01:59:37.260 --> 01:59:40.140]   I look at who is the least untrustworthy.
[01:59:40.140 --> 01:59:42.100]   Oh, good.
[01:59:42.100 --> 01:59:43.100]   And I have.
[01:59:43.100 --> 01:59:44.100]   That's very British of you.
[01:59:44.100 --> 01:59:45.100]   Yeah.
[01:59:45.100 --> 01:59:51.780]   Well, let me walk back slightly on this because maybe two years ago I did an audit of all
[01:59:51.780 --> 01:59:52.780]   my data.
[01:59:52.780 --> 01:59:53.780]   Oh, sorry.
[01:59:53.780 --> 01:59:54.780]   Privacy concerned.
[01:59:54.780 --> 01:59:58.620]   Part concerns partly out of pure economics.
[01:59:58.620 --> 02:00:03.780]   And I had subscriptions I was paying for for Google Drive, for Dropbox, for iCloud, for
[02:00:03.780 --> 02:00:08.460]   Evernote, for pockets, for like loads and loads of things.
[02:00:08.460 --> 02:00:14.860]   And I thought, well, a lot of this, probably like 90% of this I'm getting from Apple's
[02:00:14.860 --> 02:00:16.500]   own apps and iCloud.
[02:00:16.500 --> 02:00:19.140]   And I can unify there.
[02:00:19.140 --> 02:00:20.460]   And that includes email.
[02:00:20.460 --> 02:00:25.100]   That includes maps and a bunch of other stuff, storage.
[02:00:25.100 --> 02:00:29.980]   And I sort of thought, well, if I'm going to give, I don't want any large company having
[02:00:29.980 --> 02:00:31.540]   all of my data.
[02:00:31.540 --> 02:00:36.500]   But if I'm going to let any one company have all my data, I'd rather it be a company that
[02:00:36.500 --> 02:00:39.620]   doesn't technically need it.
[02:00:39.620 --> 02:00:44.460]   Apple's Apple wants me locked into its ecosystem for another whole load of reasons, which is
[02:00:44.460 --> 02:00:49.180]   to make sure I keep buying iMacs and iPhones and iPads and stuff.
[02:00:49.180 --> 02:00:54.500]   But it doesn't need it to sell that data or rather to use that data to sell me other
[02:00:54.500 --> 02:00:56.460]   things or to market to me or things like that.
[02:00:56.460 --> 02:00:59.260]   So look at the business model then.
[02:00:59.260 --> 02:01:00.780]   I always look at the business model.
[02:01:00.780 --> 02:01:05.980]   It's why I was, you know, in the very beginning of the show today, I was talking about Facebook.
[02:01:05.980 --> 02:01:10.700]   Like the fundamental of that, of using cryptocurrency isn't what bothers me.
[02:01:10.700 --> 02:01:16.460]   It's taking a step back and asking, well, why did this idea come up in the first place?
[02:01:16.460 --> 02:01:20.060]   What isn't it doing that it wants to do that this helps it do?
[02:01:20.060 --> 02:01:24.260]   And I think the same is true when it comes to who has your data and who you trust.
[02:01:24.260 --> 02:01:28.020]   And I think that I'd rather it be a company that just wants to sell me products and rather
[02:01:28.020 --> 02:01:30.580]   than use my data for something else.
[02:01:30.580 --> 02:01:33.740]   I think that's a good litmus test.
[02:01:33.740 --> 02:01:34.860]   What do they make their money on?
[02:01:34.860 --> 02:01:40.380]   And if they make their money on advertising, and in particular, if their advertising is
[02:01:40.380 --> 02:01:43.820]   based on what they know about you, then just consider that.
[02:01:43.820 --> 02:01:45.180]   They're going to collect that information.
[02:01:45.180 --> 02:01:47.820]   That's their business model.
[02:01:47.820 --> 02:01:51.980]   Apple has another business model, which may also involve exploiting you and extracting
[02:01:51.980 --> 02:01:54.460]   money from you.
[02:01:54.460 --> 02:01:55.460]   But who doesn't?
[02:01:55.460 --> 02:01:57.020]   I mean, that's my job too, right?
[02:01:57.020 --> 02:02:00.580]   How can I get more money out of you, the listener?
[02:02:00.580 --> 02:02:03.380]   I think about that at night, all night long.
[02:02:03.380 --> 02:02:05.300]   How can I extract more money?
[02:02:05.300 --> 02:02:09.340]   But first, a word with wasabi hot cloud storage.
[02:02:09.340 --> 02:02:12.780]   What a great name wasabi is that hot stuff you have with your sushi.
[02:02:12.780 --> 02:02:17.580]   This is the hot cloud storage, which is about the hottest topic in the world today is moving
[02:02:17.580 --> 02:02:23.900]   to the cloud by 2025 and six years, 80% of enterprises will have literally shut down
[02:02:23.900 --> 02:02:25.500]   their traditional data centers.
[02:02:25.500 --> 02:02:27.780]   It's 10% right now.
[02:02:27.780 --> 02:02:32.940]   Moving to the cloud for cost savings, operational efficiencies, a transformative way of scaling
[02:02:32.940 --> 02:02:34.580]   your business.
[02:02:34.580 --> 02:02:41.060]   By then it's estimated there will be 163 zettabytes in the cloud.
[02:02:41.060 --> 02:02:46.020]   That's 21 zeros, 163 zettabytes in the cloud.
[02:02:46.020 --> 02:02:48.980]   But it's important when you put stuff in the cloud, I know this is a concern for anybody
[02:02:48.980 --> 02:02:55.020]   looking at cloud, that your data be safe, that it be secure, and I've got something
[02:02:55.020 --> 02:02:58.900]   that is actually arguably more secure than your own on-premises.
[02:02:58.900 --> 02:03:00.900]   I know it is.
[02:03:00.900 --> 02:03:05.340]   Wasabi was started by these two guys, Jeff Flowers and David Friend, good friends of mine.
[02:03:05.340 --> 02:03:07.380]   I've known David for years.
[02:03:07.380 --> 02:03:11.860]   They figured out and patented a revolutionary way of writing data on disks sequentially,
[02:03:11.860 --> 02:03:15.740]   not in blocks, which made it more efficient and faster.
[02:03:15.740 --> 02:03:17.620]   That's what wasabi is based on.
[02:03:17.620 --> 02:03:23.580]   This enterprise-grade cloud storage that with this patented technology is 80% cheaper than
[02:03:23.580 --> 02:03:30.500]   Amazon S3, 80% less expensive and up to six times faster and as secure as you can get,
[02:03:30.500 --> 02:03:33.460]   11 nines of durability.
[02:03:33.460 --> 02:03:34.940]   It's really remarkable.
[02:03:34.940 --> 02:03:38.580]   I'll tell you, there's some of the things that they do that mean you will not be like
[02:03:38.580 --> 02:03:44.260]   that Florida city that just paid $600,000 in ransomware fees because you can designate
[02:03:44.260 --> 02:03:47.060]   some data as immutable.
[02:03:47.060 --> 02:03:51.780]   Can't be changed by ransomware or fungible finger employees or anything immutable.
[02:03:51.780 --> 02:03:52.980]   That's brilliant.
[02:03:52.980 --> 02:03:56.780]   There's no charge for egress, no charge for API access.
[02:03:56.780 --> 02:04:00.820]   It uses the S3 API, so you already know how to use it.
[02:04:00.820 --> 02:04:02.900]   HIPAA FINRA CJIS compliant.
[02:04:02.900 --> 02:04:08.060]   Look, I know the boss has set you off to research cloud storage solutions and you're
[02:04:08.060 --> 02:04:11.060]   going to, of course, you're going to come back with the big three names.
[02:04:11.060 --> 02:04:13.260]   You're going to have Amazon and Microsoft and Google on there.
[02:04:13.260 --> 02:04:16.980]   I just want you to add a fourth name wasabi.
[02:04:16.980 --> 02:04:17.980]   Try it yourself.
[02:04:17.980 --> 02:04:19.780]   W-A-S-A-B-I.com.
[02:04:19.780 --> 02:04:25.540]   Click the free trial link off or code TWIT, unlimited storage for a month so you can slam
[02:04:25.540 --> 02:04:26.540]   stuff up there.
[02:04:26.540 --> 02:04:30.180]   They also have, if you want to migrate a ton of data, you can migrate up to a petabyte
[02:04:30.180 --> 02:04:33.660]   of time with a wasabi ball, which is so cool.
[02:04:33.660 --> 02:04:36.140]   This is the solution you've been looking for.
[02:04:36.140 --> 02:04:40.580]   80% more affordable, six times faster.
[02:04:40.580 --> 02:04:44.820]   It combines a durability, no charge for API access, no charge for egress.
[02:04:44.820 --> 02:04:48.340]   I just got to try it.
[02:04:48.340 --> 02:04:49.340]   Wasabi.
[02:04:49.340 --> 02:04:50.340]   W-A-S-A-B-I.com.
[02:04:50.340 --> 02:04:53.380]   Oh, don't forget the offer code TWIT for unlimited storage.
[02:04:53.380 --> 02:04:54.380]   Wasabi.
[02:04:54.380 --> 02:04:55.860]   W-A-S-A-B-I.com.
[02:04:55.860 --> 02:05:05.340]   Mache, I hope I know I'm pronouncing his name completely wrong.
[02:05:05.340 --> 02:05:09.220]   Even though he has an entire website devoted to how to pronounce his name.
[02:05:09.220 --> 02:05:13.540]   Chek Chekolowski, he created Pinboard, which I use religiously.
[02:05:13.540 --> 02:05:15.540]   He's become actually a great blogger.
[02:05:15.540 --> 02:05:22.300]   His idle words blog talks this week about what he calls the new wilderness.
[02:05:22.300 --> 02:05:27.460]   He's talking about a kind of privacy we don't really think about.
[02:05:27.460 --> 02:05:34.180]   He calls it ambient privacy, that there is a value in having our everyday interactions
[02:05:34.180 --> 02:05:37.740]   with one another remain outside the reach of monitoring it.
[02:05:37.740 --> 02:05:42.860]   That the small details of our daily lives should pass by unremembered.
[02:05:42.860 --> 02:05:47.260]   Not every conversation needs to be a deposition.
[02:05:47.260 --> 02:05:52.020]   He's pointing out that in this modern world, it's not the case that every single thing
[02:05:52.020 --> 02:05:57.540]   you do is being tracked and recorded.
[02:05:57.540 --> 02:06:00.820]   I think, Kate, you must have done your homework for this show and read that article.
[02:06:00.820 --> 02:06:03.460]   It sounds like I'm impressed.
[02:06:03.460 --> 02:06:04.460]   These guys have been around a while.
[02:06:04.460 --> 02:06:05.460]   They don't read anything.
[02:06:05.460 --> 02:06:13.060]   I liked the line, "To what extent is living in a surveillance saturated world compatible
[02:06:13.060 --> 02:06:14.820]   with pluralism and democracy?"
[02:06:14.820 --> 02:06:18.740]   See, that's the real question, right?
[02:06:18.740 --> 02:06:20.180]   Yeah.
[02:06:20.180 --> 02:06:22.100]   Does this...
[02:06:22.100 --> 02:06:23.460]   It's one thing to give this up.
[02:06:23.460 --> 02:06:24.460]   That's a little thing.
[02:06:24.460 --> 02:06:25.700]   Who cares?
[02:06:25.700 --> 02:06:33.140]   If the consequence of giving it up is that we can't function as a democracy, that's a
[02:06:33.140 --> 02:06:34.140]   serious consequence.
[02:06:34.140 --> 02:06:36.060]   You'll have to read the piece.
[02:06:36.060 --> 02:06:42.220]   It's a very good piece to understand why he links the two.
[02:06:42.220 --> 02:06:43.980]   But I think that it is a pre...
[02:06:43.980 --> 02:06:44.980]   He says...
[02:06:44.980 --> 02:06:47.180]   I agree with this, "A shared sense of reality."
[02:06:47.180 --> 02:06:51.780]   An agreement on common facts is a prerequisite for self-government.
[02:06:51.780 --> 02:06:54.660]   That is something for whatever reason.
[02:06:54.660 --> 02:06:56.980]   This goes back to Ravelry and Twitter.
[02:06:56.980 --> 02:07:01.220]   For whatever reason, that seems to be the case that we no longer have a shared sense
[02:07:01.220 --> 02:07:07.100]   of reality, that we're living in our own fractured worlds.
[02:07:07.100 --> 02:07:08.100]   Good...
[02:07:08.100 --> 02:07:09.100]   Really good piece from a check.
[02:07:09.100 --> 02:07:10.100]   It was a really good piece.
[02:07:10.100 --> 02:07:11.100]   There's also...
[02:07:11.100 --> 02:07:15.740]   I think there's a really important piece of this where the idea of moving through the
[02:07:15.740 --> 02:07:16.900]   world, he does this...
[02:07:16.900 --> 02:07:23.180]   draws this parallel with nature and regulation and regulating natural spaces.
[02:07:23.180 --> 02:07:28.060]   I think that parallel is more than just an analogy that he uses here.
[02:07:28.060 --> 02:07:32.020]   I think there's a really important, meaningful dimension that can seep through that.
[02:07:32.020 --> 02:07:39.060]   There's something about moving through the world and experiencing the world as a series
[02:07:39.060 --> 02:07:40.540]   of places.
[02:07:40.540 --> 02:07:46.580]   What that means is a really important part of understanding fundamental human nature.
[02:07:46.580 --> 02:07:48.580]   Our embodied experience of the world.
[02:07:48.580 --> 02:07:51.100]   I wrote about it in my last book, "Pixels in Place."
[02:07:51.100 --> 02:08:00.460]   There's so much that we experience about our embodied surroundings in physical space.
[02:08:00.460 --> 02:08:04.060]   It's a really important parallel that he draws there.
[02:08:04.060 --> 02:08:10.420]   I would just add to the encouragement that people read this piece because it's very well
[02:08:10.420 --> 02:08:11.420]   thought out.
[02:08:11.420 --> 02:08:13.580]   Idlewords.com.
[02:08:13.580 --> 02:08:19.700]   There's another hopeful thing about that analogy because he says we were able to preserve our
[02:08:19.700 --> 02:08:24.780]   public spaces and our environment with regulation, that it is possible to regulate, and that
[02:08:24.780 --> 02:08:32.020]   regulation can make a big difference as it has in the air in London, for instance, and
[02:08:32.020 --> 02:08:36.700]   as it has with the ozone layer.
[02:08:36.700 --> 02:08:38.420]   I think that's important.
[02:08:38.420 --> 02:08:42.700]   It's conceivable that there can be regulations that can make this a better place, that we
[02:08:42.700 --> 02:08:47.020]   shouldn't turn our back on the notion that maybe that it is possible to fix this with
[02:08:47.020 --> 02:08:48.020]   regulation.
[02:08:48.020 --> 02:08:52.700]   As much as we rag on GDPR, it's not the best solution.
[02:08:52.700 --> 02:08:56.740]   It's certainly a start towards regulating privacy in a way.
[02:08:56.740 --> 02:09:04.260]   It's also this article, the idea of ambient privacy, it reminds me of just hanging out
[02:09:04.260 --> 02:09:10.220]   with friends and family now and how a lot of people just can't even take in their experiences
[02:09:10.220 --> 02:09:12.740]   without recording them or documenting them in some way.
[02:09:12.740 --> 02:09:16.980]   I think that for me is the thing that I lose.
[02:09:16.980 --> 02:09:21.780]   I am sad that we've come to this point as a society where we definitely want to Instagram
[02:09:21.780 --> 02:09:24.580]   or Snapchat our meals.
[02:09:24.580 --> 02:09:28.900]   Doing it quickly is one thing, but I know a lot of people who will spend several minutes
[02:09:28.900 --> 02:09:32.660]   before they even start eating to make sure they have the right lighting, the right perfect
[02:09:32.660 --> 02:09:34.220]   pose for it.
[02:09:34.220 --> 02:09:36.820]   That to me seems deeply sad.
[02:09:36.820 --> 02:09:40.660]   We have forgotten just how to eat because we have to document it before we eat, but that's
[02:09:40.660 --> 02:09:42.580]   true of any vacation spot or anything too.
[02:09:42.580 --> 02:09:47.540]   I see so many people rather than absorb their surroundings or what they're looking at working
[02:09:47.540 --> 02:09:53.700]   hard just to take photos, take videos and maybe missing out on the experience.
[02:09:53.700 --> 02:09:55.620]   It's really funny about that too.
[02:09:55.620 --> 02:10:00.300]   It's a really good point, the distinction you make between capturing it quickly as a photo
[02:10:00.300 --> 02:10:05.220]   of your food versus staging the lighting and making sure it's all perfectly plated and
[02:10:05.220 --> 02:10:06.980]   all that.
[02:10:06.980 --> 02:10:12.900]   I came across research, I was actually four pixels in place that talked about that the
[02:10:12.900 --> 02:10:18.340]   difference between the way we experience things like our meals when we take photos of them
[02:10:18.340 --> 02:10:22.740]   or say like a concert that we may capture with photos or video.
[02:10:22.740 --> 02:10:28.420]   I think people tend to rail against that and think that we are missing out on being present
[02:10:28.420 --> 02:10:32.500]   in the experience, but it turns out there is some studies that suggest that we actually
[02:10:32.500 --> 02:10:40.820]   remember those experiences more by choosing to frame them in some sort of digital capture
[02:10:40.820 --> 02:10:41.820]   like that.
[02:10:41.820 --> 02:10:45.180]   There's a really interesting nuance of human experience that comes across that it's like
[02:10:45.180 --> 02:10:50.500]   a new modality of how we experience the world, but I think to your point, you can certainly
[02:10:50.500 --> 02:10:51.500]   go to your class.
[02:10:51.500 --> 02:10:54.500]   Just eat, folks, please.
[02:10:54.500 --> 02:10:59.060]   God, now I thought this was just a temporary aberration.
[02:10:59.060 --> 02:11:01.980]   Chris, by the way, I might have missed what you were talking about because I was busy
[02:11:01.980 --> 02:11:04.140]   playing Harry Potter Wizard's Unite.
[02:11:04.140 --> 02:11:07.780]   But I thought that this might have been a temporary aberration that in years to come
[02:11:07.780 --> 02:11:13.860]   we'll look back and say, remember the 2010s and how everybody was on their phone all
[02:11:13.860 --> 02:11:18.500]   the time and people would get run over as they walked across the street and nobody really
[02:11:18.500 --> 02:11:20.780]   knew what they were eating if they didn't Instagram it first.
[02:11:20.780 --> 02:11:21.780]   Remember that?
[02:11:21.780 --> 02:11:22.780]   Wasn't that weird?
[02:11:22.780 --> 02:11:27.460]   By the way, in New York, I have stopped several baby carriages from rolling onto a street
[02:11:27.460 --> 02:11:30.900]   as their parents are on the phone or something.
[02:11:30.900 --> 02:11:32.900]   They don't think we grow out of this.
[02:11:32.900 --> 02:11:33.900]   No, this is it.
[02:11:33.900 --> 02:11:34.900]   This is the future.
[02:11:34.900 --> 02:11:37.460]   And it'll become more ambient.
[02:11:37.460 --> 02:11:39.660]   And then I don't know if that's better or worse, right?
[02:11:39.660 --> 02:11:45.620]   If it's in our glasses or our eyes, at least now you can tell that somebody may be distracted
[02:11:45.620 --> 02:11:47.180]   when it's ambient, who knows?
[02:11:47.180 --> 02:11:48.180]   It will be in our glasses.
[02:11:48.180 --> 02:11:52.140]   In fact, we just went down to San Francisco to get fitted for some new spectacles that
[02:11:52.140 --> 02:11:53.380]   show all your notifications.
[02:11:53.380 --> 02:11:55.380]   No, that's a nightmare.
[02:11:55.380 --> 02:11:56.380]   Oh, man.
[02:11:56.380 --> 02:12:00.460]   Well, I was going to go down and do it because we wanted to review it for Hands on Tech,
[02:12:00.460 --> 02:12:01.460]   our review show.
[02:12:01.460 --> 02:12:03.340]   And then I said, no, that sounds really annoying.
[02:12:03.340 --> 02:12:04.340]   I am not.
[02:12:04.340 --> 02:12:05.500]   I am not.
[02:12:05.500 --> 02:12:08.220]   So our producer, I made the producer Anthony do it.
[02:12:08.220 --> 02:12:09.220]   This is hell.
[02:12:09.220 --> 02:12:11.020]   Like, yeah, this is my worst nightmare.
[02:12:11.020 --> 02:12:12.140]   It's bad enough.
[02:12:12.140 --> 02:12:13.820]   My watch tells me I got a text.
[02:12:13.820 --> 02:12:16.780]   I don't want to pop up in front of him.
[02:12:16.780 --> 02:12:18.420]   That's like horrible.
[02:12:18.420 --> 02:12:23.060]   And yet it's such a difference between what you're describing in this whole kind of like
[02:12:23.060 --> 02:12:25.180]   involuntary heads up display almost.
[02:12:25.180 --> 02:12:28.140]   It sounds like it's a terrible use of technology.
[02:12:28.140 --> 02:12:31.780]   It's like at a clockwork orange, like they might as well just prop your eyes open and
[02:12:31.780 --> 02:12:35.260]   you must watch all of your alerts.
[02:12:35.260 --> 02:12:41.700]   I think we still have a long way to go to come up with meaningful ways to use augmented
[02:12:41.700 --> 02:12:47.180]   reality, for example, to create new layers of understanding of our context and our surroundings.
[02:12:47.180 --> 02:12:48.380]   That'll be really cool.
[02:12:48.380 --> 02:12:49.580]   So that's still coming.
[02:12:49.580 --> 02:12:52.380]   I think that we'll see some benefit to that.
[02:12:52.380 --> 02:12:53.380]   Always upbeat.
[02:12:53.380 --> 02:12:55.380]   It's always me to see my notifications.
[02:12:55.380 --> 02:12:56.380]   Always upbeat.
[02:12:56.380 --> 02:12:57.380]   Always looking at the bright side.
[02:12:57.380 --> 02:12:59.420]   See, that's good.
[02:12:59.420 --> 02:13:00.660]   Just remind me of people's names.
[02:13:00.660 --> 02:13:01.660]   That's what I need.
[02:13:01.660 --> 02:13:02.660]   Faces and names.
[02:13:02.660 --> 02:13:03.660]   No.
[02:13:03.660 --> 02:13:04.660]   Really bad ads.
[02:13:04.660 --> 02:13:05.660]   Me too.
[02:13:05.660 --> 02:13:06.660]   That's a Google class demo.
[02:13:06.660 --> 02:13:07.660]   I think initially.
[02:13:07.660 --> 02:13:08.660]   Me too.
[02:13:08.660 --> 02:13:09.660]   His name is Javindra.
[02:13:09.660 --> 02:13:10.660]   He just had a baby.
[02:13:10.660 --> 02:13:11.660]   Yes.
[02:13:11.660 --> 02:13:12.660]   Yeah.
[02:13:12.660 --> 02:13:13.660]   I want to know that.
[02:13:13.660 --> 02:13:14.660]   I don't know who you are.
[02:13:14.660 --> 02:13:15.660]   Are you?
[02:13:15.660 --> 02:13:18.060]   If you see me on the street, Javindra, and I walk right by, that's because I don't have
[02:13:18.060 --> 02:13:19.060]   my glasses on.
[02:13:19.060 --> 02:13:20.060]   Yes.
[02:13:20.060 --> 02:13:22.260]   My special glasses.
[02:13:22.260 --> 02:13:30.420]   Phillips lighting group is now called signify, by the way, because Phillips is too easy to
[02:13:30.420 --> 02:13:32.780]   remember.
[02:13:32.780 --> 02:13:36.700]   Signify makes Hugh Brand lights and they've announced a new kind of lights called True
[02:13:36.700 --> 02:13:44.100]   Li-Fi transmit data using light waves at the speeds of up to 150 megabits.
[02:13:44.100 --> 02:13:48.620]   Now, of course, you don't have Li-Fi on your laptop, so you'd have to get a special Li-Fi
[02:13:48.620 --> 02:13:51.380]   plug-in.
[02:13:51.380 --> 02:13:59.980]   And if you're transmitting from place to place, from Li-Fi to Li-Fi, 250 megabits, this is
[02:13:59.980 --> 02:14:02.180]   not the first time we've heard of it.
[02:14:02.180 --> 02:14:04.580]   Javindra, you probably saw the demos at CES a couple of years ago.
[02:14:04.580 --> 02:14:09.580]   I've seen some demos, but this is the sort of thing where like, man, it seems rough like
[02:14:09.580 --> 02:14:11.980]   what happens if something gets in the way of that light.
[02:14:11.980 --> 02:14:15.500]   What if you stand in front of your laptop?
[02:14:15.500 --> 02:14:18.620]   I'm presenting here.
[02:14:18.620 --> 02:14:21.140]   You can retrofit it into existing Philips Hugh lights.
[02:14:21.140 --> 02:14:23.980]   They have a little retrofit or you can buy new light bulbs.
[02:14:23.980 --> 02:14:25.620]   I don't know what the cost is.
[02:14:25.620 --> 02:14:28.100]   It's got to be expensive.
[02:14:28.100 --> 02:14:32.900]   But one of the markets for this is places where you can't have radio frequency.
[02:14:32.900 --> 02:14:35.300]   There's no Wi-Fi in a lot of hospitals, right?
[02:14:35.300 --> 02:14:40.660]   Because it can interfere with pacemakers and stuff, so Li-Fi might be the solution there.
[02:14:40.660 --> 02:14:45.260]   So I thought we started with a weird light bulb story and how is it maybe an alternative?
[02:14:45.260 --> 02:14:51.900]   I do have to mention that town in Florida that has decided the city council voted.
[02:14:51.900 --> 02:14:57.500]   This is a little town called Riviera Beach.
[02:14:57.500 --> 02:15:01.220]   They've been shut down just like Baltimore was a couple of weeks ago by ransomware.
[02:15:01.220 --> 02:15:04.700]   They've decided to pay the ransom $600,000.
[02:15:04.700 --> 02:15:09.220]   This is a town of 35,000 people.
[02:15:09.220 --> 02:15:10.980]   They said, we asked security experts.
[02:15:10.980 --> 02:15:14.340]   They said, we should just pay.
[02:15:14.340 --> 02:15:18.300]   That doesn't seem like a good idea.
[02:15:18.300 --> 02:15:22.420]   Especially since you're paying a criminal, there's no guarantee he's going to honor the
[02:15:22.420 --> 02:15:25.940]   promise to unlock your data.
[02:15:25.940 --> 02:15:28.820]   You could just say thank you very much and move on.
[02:15:28.820 --> 02:15:31.020]   This is the cost of bad IT.
[02:15:31.020 --> 02:15:33.580]   The cost of bad IT.
[02:15:33.580 --> 02:15:35.140]   This is what it is.
[02:15:35.140 --> 02:15:37.940]   If you're not updated, you're not fully patched.
[02:15:37.940 --> 02:15:38.940]   It's tough.
[02:15:38.940 --> 02:15:43.660]   I think around when the one in Christ stuff was happening, it kind of revealed, like, oh,
[02:15:43.660 --> 02:15:50.100]   all these organizations, hospitals in the UK, were not fully updated and then had to
[02:15:50.100 --> 02:15:53.300]   deal with all these issues.
[02:15:53.300 --> 02:15:54.300]   It stinks.
[02:15:54.300 --> 02:16:00.660]   I'm not a fan of paying the people keeping your data hostage, but I don't know what
[02:16:00.660 --> 02:16:02.060]   else people need to do.
[02:16:02.060 --> 02:16:03.540]   Again, we could say the R word.
[02:16:03.540 --> 02:16:04.540]   It's regulation.
[02:16:04.540 --> 02:16:07.100]   You're going to regulate better updates and better IT support.
[02:16:07.100 --> 02:16:08.100]   I don't know.
[02:16:08.100 --> 02:16:09.100]   Certainly for cities.
[02:16:09.100 --> 02:16:17.020]   Baltimore, ironically, the Baltimore Ransom Guy only wanted $76,000.
[02:16:17.020 --> 02:16:19.780]   To their credit, Baltimore did not pay.
[02:16:19.780 --> 02:16:25.580]   The FBI always says do not pay because of no guarantee you're going to get your money
[02:16:25.580 --> 02:16:26.580]   back.
[02:16:26.580 --> 02:16:36.100]   We were talking in security now about a ransomware as a service company.
[02:16:36.100 --> 02:16:40.540]   By the way, we should point out that a lot of these ransomware is not coming from accomplished
[02:16:40.540 --> 02:16:41.540]   hackers.
[02:16:41.540 --> 02:16:45.460]   They're just using existing ransomware platforms.
[02:16:45.460 --> 02:16:47.220]   This company is retiring.
[02:16:47.220 --> 02:16:53.820]   They're retiring because what was the name of the company I'm going to try to remember?
[02:16:53.820 --> 02:16:58.420]   They made $2 billion in a couple of years and so they said, "Well, I think we're going
[02:16:58.420 --> 02:16:59.420]   to stop."
[02:16:59.420 --> 02:17:01.700]   They put well your head.
[02:17:01.700 --> 02:17:03.340]   We never got caught.
[02:17:03.340 --> 02:17:05.340]   Does anybody ever get caught for ransomware?
[02:17:05.340 --> 02:17:11.300]   There's occasional arrests, but it doesn't seem like this is a low risk crime.
[02:17:11.300 --> 02:17:12.860]   Back up your data, folks.
[02:17:12.860 --> 02:17:13.860]   Get better IT.
[02:17:13.860 --> 02:17:14.860]   Nate Langson.
[02:17:14.860 --> 02:17:15.860]   It is interesting.
[02:17:15.860 --> 02:17:16.860]   Go ahead.
[02:17:16.860 --> 02:17:21.900]   Sorry, I was just going to say it is interesting about the discrepancy between the ransom amounts
[02:17:21.900 --> 02:17:23.060]   for the city in Florida.
[02:17:23.060 --> 02:17:24.060]   It's Randall.
[02:17:24.060 --> 02:17:25.060]   It's Randall.
[02:17:25.060 --> 02:17:26.060]   It's Randall.
[02:17:26.060 --> 02:17:31.220]   I also assume you guys saw too, there's an update over the last week that the governor
[02:17:31.220 --> 02:17:39.420]   of Maryland signed an executive order to boost their cybersecurity policies after the Baltimore
[02:17:39.420 --> 02:17:40.420]   attacks.
[02:17:40.420 --> 02:17:41.420]   Yeah.
[02:17:41.420 --> 02:17:42.420]   Good thinking.
[02:17:42.420 --> 02:17:43.420]   Yeah.
[02:17:43.420 --> 02:17:47.020]   There's an idea a few months ago.
[02:17:47.020 --> 02:17:48.620]   Good thinking.
[02:17:48.620 --> 02:17:49.620]   Yeah.
[02:17:49.620 --> 02:17:51.260]   Kate, thank you so much for being here.
[02:17:51.260 --> 02:17:52.780]   I really appreciate it.
[02:17:52.780 --> 02:17:53.780]   Hey, thank you.
[02:17:53.780 --> 02:17:55.420]   Everybody should read Tech Humanist.
[02:17:55.420 --> 02:18:03.740]   Kato Neil is the Tech Humanist.
[02:18:03.740 --> 02:18:05.740]   K-O Insights.com.
[02:18:05.740 --> 02:18:06.740]   Great public speaker.
[02:18:06.740 --> 02:18:09.740]   Watch the triangulation she was on last week.
[02:18:09.740 --> 02:18:10.740]   It was really good.
[02:18:10.740 --> 02:18:13.820]   K-O on the Twitter and I am now following you.
[02:18:13.820 --> 02:18:14.820]   Yeah.
[02:18:14.820 --> 02:18:15.820]   All right.
[02:18:15.820 --> 02:18:19.580]   So say interesting things and don't make me feel like I had a stroke, okay?
[02:18:19.580 --> 02:18:24.020]   None of those abbreviations the kids use.
[02:18:24.020 --> 02:18:25.020]   I won't sub tweet you.
[02:18:25.020 --> 02:18:26.500]   Thank you for staying up so late.
[02:18:26.500 --> 02:18:30.140]   I know it's now, well, middle of the night.
[02:18:30.140 --> 02:18:31.140]   Yeah.
[02:18:31.140 --> 02:18:32.540]   It's nearly 1.30 in the morning here.
[02:18:32.540 --> 02:18:34.460]   My cat is outside somewhere.
[02:18:34.460 --> 02:18:36.540]   He's not usually out when it's this late in the dark.
[02:18:36.540 --> 02:18:38.060]   So I'm going to go outside and find him.
[02:18:38.060 --> 02:18:39.060]   Go get him.
[02:18:39.060 --> 02:18:40.420]   Tech editor in Bloomberg doing great work.
[02:18:40.420 --> 02:18:42.220]   I really appreciate it, Nate.
[02:18:42.220 --> 02:18:48.020]   In fact, just because of you, just because of you, I'm going to pay the 35 bucks a month.
[02:18:48.020 --> 02:18:52.700]   I've been pushing it off and Bloomberg finally said, "You've had enough Laport.
[02:18:52.700 --> 02:18:54.180]   We're cutting you off."
[02:18:54.180 --> 02:18:55.180]   So now I'm going to pay.
[02:18:55.180 --> 02:18:56.180]   So thank you.
[02:18:56.180 --> 02:18:57.180]   All right.
[02:18:57.180 --> 02:18:58.180]   Yeah.
[02:18:58.180 --> 02:18:59.180]   That's good.
[02:18:59.180 --> 02:19:00.180]   Yeah.
[02:19:00.180 --> 02:19:01.180]   You should script my podcast as well because that's free.
[02:19:01.180 --> 02:19:02.580]   What's the name of your podcast?
[02:19:02.580 --> 02:19:03.580]   Text message.
[02:19:03.580 --> 02:19:04.580]   He had to think about it.
[02:19:04.580 --> 02:19:06.580]   So it's uktechshow.com.
[02:19:06.580 --> 02:19:10.580]   Did you say the wrong name at first?
[02:19:10.580 --> 02:19:16.220]   No, it's just the URL is different to the website because the when you hear the name
[02:19:16.220 --> 02:19:18.580]   text message, you think it's text.
[02:19:18.580 --> 02:19:20.020]   It works when it's written down.
[02:19:20.020 --> 02:19:22.660]   It's just not so good when it's spoken.
[02:19:22.660 --> 02:19:24.860]   So I got a very literal domain.
[02:19:24.860 --> 02:19:30.660]   uktechshow.com for tech message.
[02:19:30.660 --> 02:19:32.180]   There you go.
[02:19:32.180 --> 02:19:33.180]   With an apostrophe.
[02:19:33.180 --> 02:19:34.180]   Yes.
[02:19:34.180 --> 02:19:35.180]   I love that.
[02:19:35.180 --> 02:19:36.180]   What a great idea.
[02:19:36.180 --> 02:19:37.180]   Thank you.
[02:19:37.180 --> 02:19:38.380]   Great to have you, Nate.
[02:19:38.380 --> 02:19:39.380]   I appreciate it.
[02:19:39.380 --> 02:19:40.380]   Thanks, Leo.
[02:19:40.380 --> 02:19:41.820]   Thanks also, of course, to Davenger hardware.
[02:19:41.820 --> 02:19:42.820]   We always love having you on.
[02:19:42.820 --> 02:19:46.220]   Every time I see your buy line, I always go, "I love Davenger."
[02:19:46.220 --> 02:19:48.420]   I think you're a great smart guy.
[02:19:48.420 --> 02:19:50.220]   Oh, it's looking on.
[02:19:50.220 --> 02:19:52.620]   Exactly the right sensibilities about all of this stuff.
[02:19:52.620 --> 02:19:54.140]   It's always a pleasure.
[02:19:54.140 --> 02:19:56.020]   Senior editor and a gadget.
[02:19:56.020 --> 02:19:57.020]   Great panel.
[02:19:57.020 --> 02:19:58.660]   And movie podcasts too.
[02:19:58.660 --> 02:19:59.660]   Slash film.
[02:19:59.660 --> 02:20:00.660]   Slash film.
[02:20:00.660 --> 02:20:01.660]   Slash film.
[02:20:01.660 --> 02:20:02.660]   Slash film cast.
[02:20:02.660 --> 02:20:03.660]   Let's not forget.
[02:20:03.660 --> 02:20:04.660]   What do you see?
[02:20:04.660 --> 02:20:05.660]   What do you like lately?
[02:20:05.660 --> 02:20:07.380]   It's a summertime film fest riot.
[02:20:07.380 --> 02:20:09.180]   Oh, it's been a rough summer.
[02:20:09.180 --> 02:20:10.180]   Yeah.
[02:20:10.180 --> 02:20:11.180]   It's so...
[02:20:11.180 --> 02:20:12.180]   Where are the great movies?
[02:20:12.180 --> 02:20:13.180]   Toy Story 4.
[02:20:13.180 --> 02:20:14.180]   No.
[02:20:14.180 --> 02:20:15.180]   That's the first green one.
[02:20:15.180 --> 02:20:16.180]   No.
[02:20:16.180 --> 02:20:17.180]   Very, very long.
[02:20:17.180 --> 02:20:19.340]   I will not watch a movie with a number in the title Space Jam 2.
[02:20:19.340 --> 02:20:20.340]   No.
[02:20:20.340 --> 02:20:21.340]   They're really good.
[02:20:21.340 --> 02:20:22.500]   Space Toy Story for some reason.
[02:20:22.500 --> 02:20:24.500]   They've done these sequels really well.
[02:20:24.500 --> 02:20:28.900]   And yeah, I don't know how they could surpass the third one, but I haven't seen four yet,
[02:20:28.900 --> 02:20:31.220]   but everyone is telling me it's amazing and gonna see it tomorrow.
[02:20:31.220 --> 02:20:33.980]   Pixar really is a genius company.
[02:20:33.980 --> 02:20:34.980]   Usually.
[02:20:34.980 --> 02:20:35.980]   Yeah.
[02:20:35.980 --> 02:20:36.980]   They really are good.
[02:20:36.980 --> 02:20:37.980]   I still cry when I see up.
[02:20:37.980 --> 02:20:39.980]   I still cry every time.
[02:20:39.980 --> 02:20:41.980]   All of their movies, yeah.
[02:20:41.980 --> 02:20:42.980]   Yeah.
[02:20:42.980 --> 02:20:43.980]   Slash film.
[02:20:43.980 --> 02:20:47.020]   Is Slash film.com the best place to go to get the questions?
[02:20:47.020 --> 02:20:49.020]   Slash film.com or the Slash film cast.
[02:20:49.020 --> 02:20:50.820]   There's a tab right there.
[02:20:50.820 --> 02:20:51.820]   Yeah.
[02:20:51.820 --> 02:20:52.820]   Gets to the iTunes.
[02:20:52.820 --> 02:20:53.820]   Yeah.
[02:20:53.820 --> 02:20:54.820]   Subscribe.
[02:20:54.820 --> 02:20:55.820]   Check us out.
[02:20:55.820 --> 02:20:56.820]   Was Matt was men in black good?
[02:20:56.820 --> 02:20:57.820]   No.
[02:20:57.820 --> 02:20:58.820]   Oh.
[02:20:58.820 --> 02:20:59.820]   Oh.
[02:20:59.820 --> 02:21:00.820]   I fell asleep in that movie.
[02:21:00.820 --> 02:21:01.820]   You fell asleep.
[02:21:01.820 --> 02:21:02.820]   Yeah.
[02:21:02.820 --> 02:21:06.580]   I never fall asleep during movies and somehow that one just didn't work.
[02:21:06.580 --> 02:21:07.580]   Yeah.
[02:21:07.580 --> 02:21:08.580]   Avoid it.
[02:21:08.580 --> 02:21:11.300]   There's a lot of great stuff on Netflix now.
[02:21:11.300 --> 02:21:16.140]   You know, there's a, I saw a great little sci-fi movie called See You Yesterday, which
[02:21:16.140 --> 02:21:21.140]   is the time travel fun teenage romp set here in Brooklyn.
[02:21:21.140 --> 02:21:22.940]   I think it's worth watching.
[02:21:22.940 --> 02:21:26.380]   I just last night we watched the one about mother.
[02:21:26.380 --> 02:21:27.580]   I am mother.
[02:21:27.580 --> 02:21:28.580]   I am mother.
[02:21:28.580 --> 02:21:29.580]   Yeah.
[02:21:29.580 --> 02:21:30.580]   That's creepy as hell.
[02:21:30.580 --> 02:21:31.580]   Yeah.
[02:21:31.580 --> 02:21:35.180]   I, you know, maybe once we get off the earth you explain what really happened because I
[02:21:35.180 --> 02:21:36.180]   don't understand.
[02:21:36.180 --> 02:21:37.180]   I don't care.
[02:21:37.180 --> 02:21:38.180]   It's one of those movies.
[02:21:38.180 --> 02:21:39.180]   Yeah.
[02:21:39.180 --> 02:21:40.180]   One of those movies.
[02:21:40.180 --> 02:21:41.980]   Wait a minute.
[02:21:41.980 --> 02:21:43.180]   Did she, was that?
[02:21:43.180 --> 02:21:44.180]   Could they?
[02:21:44.180 --> 02:21:45.180]   Yeah.
[02:21:45.180 --> 02:21:46.180]   No spoilers.
[02:21:46.180 --> 02:21:49.020]   Don't in fact, Steve Gibson warned me about this and I, and I blew it.
[02:21:49.020 --> 02:21:52.100]   He said, don't watch the trailer because the trailer tells you the whole story.
[02:21:52.100 --> 02:21:53.100]   Netflix trailers are bad.
[02:21:53.100 --> 02:21:55.820]   I don't know what's wrong with them, but they spoil all their reviews.
[02:21:55.820 --> 02:21:56.820]   Kate, can you fix this?
[02:21:56.820 --> 02:21:58.220]   Call Reed and tell him it's not working.
[02:21:58.220 --> 02:21:59.220]   They auto play.
[02:21:59.220 --> 02:22:00.220]   They play the trailer.
[02:22:00.220 --> 02:22:05.180]   If you, if you stay on the icon too long and then you see the spoiler.
[02:22:05.180 --> 02:22:08.380]   I really want to know the data for the usage on that.
[02:22:08.380 --> 02:22:09.380]   Oh, that's why.
[02:22:09.380 --> 02:22:11.740]   It should be a button.
[02:22:11.740 --> 02:22:12.740]   It should be button.
[02:22:12.740 --> 02:22:13.740]   It's not that hard.
[02:22:13.740 --> 02:22:14.740]   Please let us turn that off.
[02:22:14.740 --> 02:22:15.740]   I ask.
[02:22:15.740 --> 02:22:18.140]   You heard him, Reed.
[02:22:18.140 --> 02:22:21.540]   Turn off the auto play.
[02:22:21.540 --> 02:22:24.500]   Because I don't want, I saw the trailer and I'm setting, my wife and I said, I said,
[02:22:24.500 --> 02:22:25.500]   we should watch this.
[02:22:25.500 --> 02:22:27.220]   And then this starts playing.
[02:22:27.220 --> 02:22:28.900]   And I said, no, no, no, no.
[02:22:28.900 --> 02:22:30.380]   I literally, I screamed.
[02:22:30.380 --> 02:22:33.500]   I think I felt bad because I think she was kind of like, what's a matter?
[02:22:33.500 --> 02:22:36.300]   I said, Steve Gibson said, don't watch the trailer and I've just seen it.
[02:22:36.300 --> 02:22:41.220]   Now I know what's going to happen.
[02:22:41.220 --> 02:22:43.900]   Everybody tells me I could turn it off, but I have yet to find a place to turn that off.
[02:22:43.900 --> 02:22:48.340]   They say on the website, no, this you can't turn it off.
[02:22:48.340 --> 02:22:50.260]   You can't.
[02:22:50.260 --> 02:22:52.020]   We ended with on a negative note.
[02:22:52.020 --> 02:22:56.140]   I want to end on a happy note.
[02:22:56.140 --> 02:22:57.460]   It's international cake day.
[02:22:57.460 --> 02:22:58.460]   Go have some cake.
[02:22:58.460 --> 02:22:59.460]   I made that up, but it's true.
[02:22:59.460 --> 02:23:00.460]   All right.
[02:23:00.460 --> 02:23:01.460]   All right.
[02:23:01.460 --> 02:23:02.460]   Thank you guys.
[02:23:02.460 --> 02:23:03.620]   Thank you all for joining us.
[02:23:03.620 --> 02:23:08.260]   We do this week in tech Sunday afternoons, 230 Pacific.
[02:23:08.260 --> 02:23:10.220]   That'll be about 530 Eastern time.
[02:23:10.220 --> 02:23:11.700]   That'd be 2130 UTC.
[02:23:11.700 --> 02:23:14.740]   If you want to watch or listen live, you can't at Twit.tv/live.
[02:23:14.740 --> 02:23:18.460]   That's a variety of streams there for your delegation.
[02:23:18.460 --> 02:23:23.660]   If you're in the stream live, you should tweet, tweet, chat live.
[02:23:23.660 --> 02:23:26.660]   IRC.tv, I'm losing it.
[02:23:26.660 --> 02:23:27.660]   IRC.Twit.tv.
[02:23:27.660 --> 02:23:31.420]   I'm going through Wizards Unite Withdrawal.
[02:23:31.420 --> 02:23:34.220]   I know there's magical creatures.
[02:23:34.220 --> 02:23:36.940]   I got to go catch them.
[02:23:36.940 --> 02:23:40.860]   The chat room is a fun place to hang, but if you're not here live, you don't need to
[02:23:40.860 --> 02:23:41.860]   be there.
[02:23:41.860 --> 02:23:46.540]   You can just download copies of the show anytime you want at Twit.tv or your favorite
[02:23:46.540 --> 02:23:47.540]   podcaster.
[02:23:47.540 --> 02:23:53.500]   You can even go in studio on a weird coincidence happen today.
[02:23:53.500 --> 02:23:57.700]   If you email tickets at Twit.tv, you can join us in the studio.
[02:23:57.700 --> 02:24:04.500]   We have two visitors, Chris, and Henrik, completely independently here, both from Sweden.
[02:24:04.500 --> 02:24:06.980]   One's from North Stockholm, one's from South Stockholm.
[02:24:06.980 --> 02:24:07.980]   Wow.
[02:24:07.980 --> 02:24:08.980]   Weird, right?
[02:24:08.980 --> 02:24:09.980]   Yeah.
[02:24:09.980 --> 02:24:11.700]   Anyway, it's great to have you.
[02:24:11.700 --> 02:24:12.700]   Thank you for coming.
[02:24:12.700 --> 02:24:13.700]   I appreciate it.
[02:24:13.700 --> 02:24:15.220]   Just email tickets at Twit.tv.
[02:24:15.220 --> 02:24:16.220]   We'll put a cheer up for you.
[02:24:16.220 --> 02:24:17.220]   Thanks for joining us.
[02:24:17.220 --> 02:24:18.220]   We'll see you next time.
[02:24:18.220 --> 02:24:19.220]   Another Twit.
[02:24:19.220 --> 02:24:20.220]   See you in the can.
[02:24:20.220 --> 02:24:20.220]   Bye-bye.
[02:24:20.340 --> 02:24:21.220]   Bye-bye.
[02:24:21.220 --> 02:24:28.220]   Bye-bye.
[02:24:28.220 --> 02:24:30.220]   Do it the Twitter all right.

