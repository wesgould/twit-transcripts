;FFMETADATA1
title=It's Coming From Leo2
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=800
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.360]   It's time for Twit this week in Tech Seth Rosenblatt's here from the Parallax.
[00:00:03.360 --> 00:00:06.560]   We'll talk Max with Jason Snell from Six Colors.
[00:00:06.560 --> 00:00:10.880]   Georgia Dow, the VR Queen is here.
[00:00:10.880 --> 00:00:15.340]   Slack selling the Salesforce for $27.5 billion.
[00:00:15.340 --> 00:00:17.760]   Is it pocket? It possibly be worth that much.
[00:00:17.760 --> 00:00:22.640]   And it's the beginning of the end for the "Bring Your Own Productivity" tool to work.
[00:00:22.640 --> 00:00:29.920]   The future of motion picture theaters, HBO Max makes a play to take over the end
[00:00:29.920 --> 00:00:32.480]   of the computer fraud act.
[00:00:32.480 --> 00:00:35.920]   The Supreme Court seems to be moving to overturn it.
[00:00:35.920 --> 00:00:38.480]   And then it's bye-bye, Adjut Pie.
[00:00:38.480 --> 00:00:40.800]   It's all coming up next on Twit.
[00:00:40.800 --> 00:00:44.240]   This week at Tech comes to you from Twit's LastPass Studios.
[00:00:44.240 --> 00:00:47.600]   Securing every access point in your company does not have to be a challenge.
[00:00:47.600 --> 00:00:50.480]   LastPass unifies access and authentication
[00:00:50.480 --> 00:00:55.200]   to make securing your employees simple and secure even when they're working remotely.
[00:00:55.200 --> 00:00:58.400]   Check out lastpass.com/twit to learn more.
[00:00:58.960 --> 00:01:00.880]   [Music]
[00:01:00.880 --> 00:01:02.720]   Podcasts you love.
[00:01:02.720 --> 00:01:03.920]   From people you trust.
[00:01:03.920 --> 00:01:06.720]   This is Twit.
[00:01:06.720 --> 00:01:16.080]   This is Twit this week in Tech.
[00:01:16.080 --> 00:01:21.200]   Episode 800 recorded Sunday, December 6, 2020.
[00:01:21.200 --> 00:01:23.520]   It's coming from Leo2.
[00:01:25.120 --> 00:01:28.480]   This episode of this week in Tech is brought to you by
[00:01:28.480 --> 00:01:30.480]   Worldwide Technology.
[00:01:30.480 --> 00:01:35.440]   Worldwide Technology's Advanced Technology Center is like no other testing and research lab
[00:01:35.440 --> 00:01:38.080]   with more than a half billion dollars of equipment,
[00:01:38.080 --> 00:01:44.080]   including OEMs like Dell Technologies and its virtual so you can access it 24/7.
[00:01:44.080 --> 00:01:49.840]   To learn more and get insights into all it offers, go to www.wt.com/twit.
[00:01:49.840 --> 00:01:53.680]   And by Barracuda.
[00:01:53.680 --> 00:01:58.000]   Did you know that 91% of all cyber attacks start with an email?
[00:01:58.000 --> 00:02:01.280]   To uncover the threats hiding in your Office 365 account,
[00:02:01.280 --> 00:02:06.480]   get a secure and free email threat scan at barracuda.com/twit.
[00:02:06.480 --> 00:02:08.960]   And by Casper.
[00:02:08.960 --> 00:02:12.960]   From award-winning mattresses to pillows, sheets and duvets,
[00:02:12.960 --> 00:02:16.960]   Casper transforms the way we sleep once news at a time.
[00:02:16.960 --> 00:02:22.560]   Go to www.casper.com/twit1 and use a code to it one for $100 off select mattresses.
[00:02:23.520 --> 00:02:25.840]   And by Manscaped.
[00:02:25.840 --> 00:02:28.640]   The best in men's below the belt grooming.
[00:02:28.640 --> 00:02:33.360]   Manscaped offers precision engineering tools for your family jewels.
[00:02:33.360 --> 00:02:38.960]   Get 20% off plus free shipping at manscaped.com/twit.
[00:02:38.960 --> 00:02:46.960]   It's time for Twit this week in Tech.
[00:02:46.960 --> 00:02:49.760]   The show where we cover the week's tech news.
[00:02:49.760 --> 00:02:57.360]   We've decked the halls with bows of plastic holly and fake, you'll tie cheer.
[00:02:57.360 --> 00:03:00.400]   Because there's nothing that cheery.
[00:03:00.400 --> 00:03:04.480]   Although I'm really thrilled to get Seth Rosenblatt on because he's the editor in chief of the
[00:03:04.480 --> 00:03:10.160]   parallax and the last time you were on was before your wedding and you had a internet wedding.
[00:03:10.160 --> 00:03:10.800]   Congratulations.
[00:03:10.800 --> 00:03:11.600]   I did.
[00:03:11.600 --> 00:03:14.800]   I did and we are still married, which is great.
[00:03:14.800 --> 00:03:15.600]   It's a good sign.
[00:03:16.400 --> 00:03:18.320]   It was because of Covid, right?
[00:03:18.320 --> 00:03:19.760]   I mean, you didn't feel it.
[00:03:19.760 --> 00:03:19.760]   Yeah.
[00:03:19.760 --> 00:03:27.040]   I mean, she moved in just before Covid shutdowns happened for all the right reasons.
[00:03:27.040 --> 00:03:28.400]   And timing was good.
[00:03:28.400 --> 00:03:29.840]   Timing was good.
[00:03:29.840 --> 00:03:35.200]   And we realized that we probably should make this serious given that the world was
[00:03:35.200 --> 00:03:35.920]   falling apart.
[00:03:35.920 --> 00:03:43.280]   And we put together a small outdoors socially distant in person gathering for our families.
[00:03:43.280 --> 00:03:44.400]   And I watched on the screen.
[00:03:44.400 --> 00:03:46.320]   We had about 300.
[00:03:46.320 --> 00:03:50.000]   People on the live stream, which was really special.
[00:03:50.000 --> 00:03:50.560]   That was cool.
[00:03:50.560 --> 00:03:55.120]   Also, it's a hell of a lot cheaper than inviting 300 people to your wedding.
[00:03:55.120 --> 00:03:56.720]   Honestly, that's how I'm doing it.
[00:03:56.720 --> 00:03:59.600]   From now on, every time I get married, it's going to be strange.
[00:03:59.600 --> 00:03:59.840]   Yeah.
[00:03:59.840 --> 00:04:01.040]   Good.
[00:04:01.040 --> 00:04:03.040]   Once a year, you know, why not?
[00:04:03.040 --> 00:04:03.680]   Yeah.
[00:04:03.680 --> 00:04:04.240]   Yeah.
[00:04:04.240 --> 00:04:07.120]   Actually, Lisa and I, when we got married, didn't invite anybody.
[00:04:07.120 --> 00:04:09.040]   It was just the two of us, which is a little...
[00:04:09.040 --> 00:04:09.840]   That's the way to do it.
[00:04:09.840 --> 00:04:14.640]   It was a little embarrassing because we got the package, which included a bartender
[00:04:14.640 --> 00:04:16.080]   and a waiter with canopays.
[00:04:16.080 --> 00:04:18.480]   It was just the two of us.
[00:04:18.480 --> 00:04:19.840]   So, would you like a canopaid?
[00:04:19.840 --> 00:04:22.240]   Would you like a canopaid?
[00:04:22.240 --> 00:04:23.840]   How about you?
[00:04:23.840 --> 00:04:24.320]   Would you like...
[00:04:24.320 --> 00:04:25.520]   It was honest to God.
[00:04:25.520 --> 00:04:27.680]   I eventually said, "Okay, you can leave.
[00:04:27.680 --> 00:04:28.960]   The bartender can leave.
[00:04:28.960 --> 00:04:30.320]   The DJ."
[00:04:30.320 --> 00:04:31.040]   Just leave the canopaids.
[00:04:31.040 --> 00:04:32.000]   DJ can leave.
[00:04:32.000 --> 00:04:33.200]   Yeah, we don't need you.
[00:04:33.200 --> 00:04:34.080]   It's just the two of us.
[00:04:34.080 --> 00:04:35.920]   Frankly, I'm actually...
[00:04:35.920 --> 00:04:36.640]   I actually...
[00:04:36.640 --> 00:04:37.360]   Go ahead.
[00:04:37.360 --> 00:04:42.480]   I was going to say, I think that one of the interesting things that's come out of
[00:04:42.480 --> 00:04:48.000]   COVID that will have been very positive is that we'll see a lot more live-streamed weddings,
[00:04:48.000 --> 00:04:54.960]   especially for guests who are too far or infirm or otherwise unable to attend.
[00:04:54.960 --> 00:04:55.920]   And it's really neat.
[00:04:55.920 --> 00:05:02.880]   Like Julianna's grandmother who's in her mid-90s and couldn't make it under any circumstances,
[00:05:02.880 --> 00:05:04.560]   was able to watch.
[00:05:04.560 --> 00:05:09.840]   And that's really cool that people who are in those situations are able to participate in these
[00:05:09.840 --> 00:05:11.920]   life events in a more personal way.
[00:05:11.920 --> 00:05:17.360]   And it's sort of stunning to me that it took something as terrible as a global pandemic
[00:05:17.360 --> 00:05:19.280]   to get us to that point.
[00:05:19.280 --> 00:05:22.480]   But I think for now, it's going to be here to stay for a long time.
[00:05:22.480 --> 00:05:26.080]   I think actually, if you think about it, it's unusual that for crises to
[00:05:26.080 --> 00:05:32.800]   be an inflection point for a new way of being in many respects, whether it's
[00:05:32.800 --> 00:05:36.080]   in a war or famine or COVID.
[00:05:36.080 --> 00:05:39.360]   I mean, obviously, you don't want to encourage those things, but it isn't unusual at all.
[00:05:39.360 --> 00:05:40.960]   Georgia Dow, you're a psychotherapist.
[00:05:40.960 --> 00:05:41.920]   Welcome, Georgia Dow.
[00:05:41.920 --> 00:05:42.800]   It's great to have you.
[00:05:42.800 --> 00:05:44.480]   Nice to be here.
[00:05:44.480 --> 00:05:45.520]   You were ill last week.
[00:05:45.520 --> 00:05:46.560]   I hope you're feeling better.
[00:05:46.560 --> 00:05:48.000]   Oh, yes, I am.
[00:05:48.000 --> 00:05:48.320]   Thank you.
[00:05:48.320 --> 00:05:48.880]   Good.
[00:05:48.880 --> 00:05:51.120]   So we reschedule, but you're doing your therapy.
[00:05:51.120 --> 00:05:54.960]   We were just talking before the show over Zoom and stuff like that.
[00:05:54.960 --> 00:05:56.560]   Yeah, it's nice.
[00:05:56.560 --> 00:06:00.720]   I think that that's the wonderful thing is that we've opened up to being able to do
[00:06:00.720 --> 00:06:03.600]   many things remotely that we had the ability to do.
[00:06:03.600 --> 00:06:06.560]   But because there wasn't a need, we weren't doing it.
[00:06:06.560 --> 00:06:12.880]   And so I do, like I would say, 95% of my sessions are through video.
[00:06:12.880 --> 00:06:17.040]   And some of them are still through phone call because people might feel shy or not
[00:06:17.040 --> 00:06:21.360]   uncomfortable setting up their camera or some people have a tech phobia.
[00:06:21.360 --> 00:06:23.360]   And so they're just using a device.
[00:06:23.360 --> 00:06:27.680]   They're worried about being spied upon or being seen.
[00:06:27.680 --> 00:06:29.280]   Is it better with video?
[00:06:29.280 --> 00:06:29.760]   For most of them.
[00:06:29.760 --> 00:06:31.920]   It is better with video.
[00:06:31.920 --> 00:06:32.800]   Because you get visual issues.
[00:06:32.800 --> 00:06:33.280]   Yeah.
[00:06:33.280 --> 00:06:33.760]   Yeah.
[00:06:33.760 --> 00:06:36.800]   Yeah. Like I don't get all of the visual cues.
[00:06:36.800 --> 00:06:40.160]   Like I'm missing feet and hands most of the time.
[00:06:40.160 --> 00:06:41.440]   But it's much better.
[00:06:41.440 --> 00:06:43.600]   And I think that we can form a bond sooner.
[00:06:43.600 --> 00:06:46.560]   I'm more emotive than lots of people.
[00:06:46.560 --> 00:06:50.320]   So if you're a very quiet therapist or you don't emote a lot,
[00:06:50.320 --> 00:06:53.440]   I think that it becomes more difficult, especially if it's just on the phone.
[00:06:53.440 --> 00:06:55.600]   Because you're wanting, like if you don't hear anything,
[00:06:55.600 --> 00:06:56.960]   you're wondering, are they listening?
[00:06:56.960 --> 00:06:57.520]   They like that.
[00:06:57.520 --> 00:06:58.240]   Did they hate that?
[00:06:58.240 --> 00:06:58.960]   Did they care?
[00:06:58.960 --> 00:07:00.720]   Are they staring off into the space?
[00:07:01.440 --> 00:07:05.440]   And that can cause a lot of more anxiety, especially if they're dealing with that.
[00:07:05.440 --> 00:07:09.040]   Also joining us, Jason Snell from 6Colors.com.
[00:07:09.040 --> 00:07:13.760]   And I think for our business, Jason, as podcasters, nothing's changed at all.
[00:07:13.760 --> 00:07:18.000]   No, I feel really guilty that I have with my friends whose lives have changed.
[00:07:18.000 --> 00:07:21.760]   And I still work in my garage and talk to everybody on the internet.
[00:07:21.760 --> 00:07:22.320]   Exactly.
[00:07:22.320 --> 00:07:22.800]   Yeah.
[00:07:22.800 --> 00:07:23.280]   Yeah.
[00:07:23.280 --> 00:07:25.840]   At least I used to be able to come and visit you in person.
[00:07:25.840 --> 00:07:27.280]   That was my one time out of the house.
[00:07:27.280 --> 00:07:27.600]   I know.
[00:07:27.600 --> 00:07:28.640]   It's not even possible now.
[00:07:28.640 --> 00:07:29.280]   I kind of missed that.
[00:07:29.280 --> 00:07:30.960]   I liked having you come up and sing.
[00:07:30.960 --> 00:07:31.600]   It was nice.
[00:07:31.600 --> 00:07:33.840]   And congratulations on episode 800.
[00:07:33.840 --> 00:07:34.560]   Oh my god.
[00:07:34.560 --> 00:07:35.680]   800, I know.
[00:07:35.680 --> 00:07:37.520]   And look, it is 800.
[00:07:37.520 --> 00:07:39.040]   800.
[00:07:39.040 --> 00:07:41.280]   That means this is the toll-free episode.
[00:07:41.280 --> 00:07:42.480]   Is that 10 foil?
[00:07:42.480 --> 00:07:43.760]   Is that 1-800?
[00:07:43.760 --> 00:07:44.480]   Dial now.
[00:07:44.480 --> 00:07:47.120]   It's the Tin Foil episode.
[00:07:47.120 --> 00:07:48.240]   Thank you, Seth.
[00:07:48.240 --> 00:07:49.120]   So did I sue you.
[00:07:49.120 --> 00:07:53.040]   Actually, you know, I was thinking, Seth, nobody ever mentions this.
[00:07:53.040 --> 00:07:53.520]   Uh-oh.
[00:07:53.520 --> 00:07:55.280]   Nobody ever mentions this.
[00:07:55.280 --> 00:08:00.400]   But if you're a single person and you go into quarantine,
[00:08:01.360 --> 00:08:05.600]   yes, it's been now nine, 10 months you haven't had sex.
[00:08:05.600 --> 00:08:08.640]   That would be horrible.
[00:08:08.640 --> 00:08:10.640]   We'll all partner up.
[00:08:10.640 --> 00:08:12.320]   But I would, I just thinking,
[00:08:12.320 --> 00:08:14.800]   there's nobody to hug, nobody to kiss.
[00:08:14.800 --> 00:08:18.320]   For nine or 10 months, that's tough.
[00:08:18.320 --> 00:08:19.680]   Yeah.
[00:08:19.680 --> 00:08:21.440]   It would be tough for me anyway.
[00:08:21.440 --> 00:08:22.000]   I don't know.
[00:08:22.000 --> 00:08:24.000]   I just, I never even think about that.
[00:08:24.000 --> 00:08:25.280]   Does that come up for your patience?
[00:08:25.280 --> 00:08:28.880]   Oh, I have a lot of people that are alone and in a lot of distress
[00:08:28.880 --> 00:08:31.680]   because they can't meet anyone in Quebec.
[00:08:31.680 --> 00:08:32.320]   You can't date?
[00:08:32.320 --> 00:08:33.120]   We are after lockdown.
[00:08:33.120 --> 00:08:33.520]   Yeah.
[00:08:33.520 --> 00:08:36.320]   They can't date, but we, if you are single and alone,
[00:08:36.320 --> 00:08:39.440]   you are allowed to have one other person come to visit you.
[00:08:39.440 --> 00:08:41.440]   That's being allowed.
[00:08:41.440 --> 00:08:42.640]   That's how they do it in Montreal.
[00:08:42.640 --> 00:08:44.000]   Yeah.
[00:08:44.000 --> 00:08:44.400]   Yeah.
[00:08:44.400 --> 00:08:46.160]   Because of how difficult it is,
[00:08:46.160 --> 00:08:49.120]   like this has now been a long, long time.
[00:08:49.120 --> 00:08:52.320]   And just to be hugged, to be touched.
[00:08:52.320 --> 00:08:53.600]   And even when you meet them,
[00:08:53.600 --> 00:08:55.920]   you're not really even supposed to be hugged and touched.
[00:08:55.920 --> 00:08:58.720]   So that, we need to be touched.
[00:08:58.720 --> 00:09:02.000]   They, we actually release neurotransmitters and chemicals
[00:09:02.000 --> 00:09:05.520]   when we are feeling touched and the hugged and cared about.
[00:09:05.520 --> 00:09:08.000]   And you don't get that through a screen,
[00:09:08.000 --> 00:09:10.800]   even if you're feeling that affection, it's not the same.
[00:09:10.800 --> 00:09:11.680]   I hugged the ground.
[00:09:11.680 --> 00:09:13.120]   The levels of depression are growing.
[00:09:13.120 --> 00:09:14.800]   I hugged the Grubhub guy the other night.
[00:09:14.800 --> 00:09:16.960]   It was a little weird, but I needed it.
[00:09:16.960 --> 00:09:17.440]   I needed it.
[00:09:17.440 --> 00:09:18.960]   How's he doing?
[00:09:18.960 --> 00:09:20.000]   [Laughter]
[00:09:20.000 --> 00:09:20.800]   He's in shock.
[00:09:20.800 --> 00:09:22.160]   Not well.
[00:09:23.440 --> 00:09:28.160]   Anyway, thank you all for being here and staying healthy and being well.
[00:09:28.160 --> 00:09:33.760]   And we, I think more and more what we're going to talk about
[00:09:33.760 --> 00:09:36.640]   over the next six months is how the world has changed.
[00:09:36.640 --> 00:09:41.200]   For instance, I don't think before COVID-19,
[00:09:41.200 --> 00:09:44.800]   Slack was worth $27.7 billion.
[00:09:44.800 --> 00:09:48.080]   I think you could even say they may not be worth that now.
[00:09:48.080 --> 00:09:51.600]   They lose money every quarter.
[00:09:51.600 --> 00:09:54.560]   They have revenue of $234 million, which is up,
[00:09:54.560 --> 00:09:59.360]   but not up nearly as much as Teams and Zoom.
[00:09:59.360 --> 00:10:01.600]   They're kind of the laggard in the COVID.
[00:10:01.600 --> 00:10:04.800]   Think, were you surprised, Jason, when you saw that sales force buy them?
[00:10:04.800 --> 00:10:06.320]   I wasn't.
[00:10:06.320 --> 00:10:10.080]   And I think the value, I mean, I don't know about about billions.
[00:10:10.080 --> 00:10:10.720]   What do I know?
[00:10:10.720 --> 00:10:12.640]   $20 billion, $28 billion, whatever.
[00:10:12.640 --> 00:10:13.200]   Yeah.
[00:10:13.200 --> 00:10:17.120]   But, you know, I think this is, I was excited about it because I do use Slack.
[00:10:17.120 --> 00:10:18.000]   I use it a lot.
[00:10:18.000 --> 00:10:20.320]   I've been out on my own, you know, even pre-COVID.
[00:10:20.320 --> 00:10:22.160]   Like, I've been working in my garage for six years.
[00:10:22.160 --> 00:10:25.040]   The way I collaborate with people is mostly through Slack.
[00:10:25.040 --> 00:10:26.400]   And it has a lot of value.
[00:10:26.400 --> 00:10:31.120]   I know that they've been attacked by Microsoft because Microsoft has such leverage.
[00:10:31.120 --> 00:10:34.880]   If you're an Office 365 user, you can just, or 360 user,
[00:10:34.880 --> 00:10:35.920]   I always get those mixed up.
[00:10:35.920 --> 00:10:36.320]   I don't know.
[00:10:36.320 --> 00:10:37.680]   Is it 365?
[00:10:37.680 --> 00:10:38.400]   It's not too crazy.
[00:10:38.400 --> 00:10:39.360]   It stays in the year.
[00:10:39.360 --> 00:10:40.240]   It stays in the year.
[00:10:40.240 --> 00:10:40.480]   Okay.
[00:10:40.480 --> 00:10:42.400]   So, 365 and a quarter technically.
[00:10:42.400 --> 00:10:47.280]   But anyway, Microsoft, you know, why spend money on Slack?
[00:10:47.280 --> 00:10:51.520]   If you already have that, even if Microsoft Teams isn't as good, it's good enough, right?
[00:10:51.520 --> 00:10:56.000]   Oh, but it's beating Slack handily in terms of signups because it's part of the deal, right?
[00:10:56.000 --> 00:10:58.880]   A lot of people aren't Microsoft 365 users.
[00:10:58.880 --> 00:11:01.280]   Why would you pay for Slack under those circumstances?
[00:11:01.280 --> 00:11:07.520]   So, I think what's good about this is that maybe it frees Slack up to focus more on their product
[00:11:07.520 --> 00:11:08.000]   because you're right.
[00:11:08.000 --> 00:11:12.320]   They have gotten lapped by not only is Microsoft caught up with them or at least gotten close enough
[00:11:12.320 --> 00:11:15.120]   with them, but you look at some of the other stuff that they do.
[00:11:15.920 --> 00:11:20.000]   I talk to people who have tried to do audio conferences in Slack.
[00:11:20.000 --> 00:11:21.040]   It's not very good.
[00:11:21.040 --> 00:11:26.960]   Like, they kind of missed the Zoom opportunity to be the place where everybody gathered for video
[00:11:26.960 --> 00:11:28.320]   in their workplaces.
[00:11:28.320 --> 00:11:33.840]   So, you know, for Salesforce, it's an interesting purchase, but Slack is important.
[00:11:33.840 --> 00:11:39.280]   And I think it adds another chapter to Salesforce's story talking to their clients.
[00:11:39.280 --> 00:11:44.480]   And hopefully it frees the Slack people up to improve their product because that product
[00:11:44.480 --> 00:11:47.600]   does feel like it's sort of stalled and hasn't really done a whole lot in the last three or four
[00:11:47.600 --> 00:11:47.840]   years.
[00:11:47.840 --> 00:11:53.680]   In a way, it's better to be a smaller money loser because, no, I mean, if Microsoft or somebody
[00:11:53.680 --> 00:11:57.280]   wanted Zoom, its market cap is 115 billion.
[00:11:57.280 --> 00:12:04.240]   It's a little bit bigger of a chunk to bite off as much as 27.7 billion sounds.
[00:12:04.240 --> 00:12:11.680]   Interesting article by Casey Newton in his new website, Platformer, remember, he left the
[00:12:11.680 --> 00:12:17.520]   verge to start this, the kind of like you did, Jason, to start his own solo blogging venture.
[00:12:17.520 --> 00:12:27.280]   He points out that Slack was the poster child for worker-centered work tools, even more tools
[00:12:27.280 --> 00:12:31.280]   that the workers brought into the company, used it in small groups.
[00:12:31.280 --> 00:12:34.160]   And then eventually it spread to the corporate culture.
[00:12:34.160 --> 00:12:37.440]   And that's how Slack got into the door because people loved it.
[00:12:38.320 --> 00:12:43.920]   But now we're kind of in a world where the company decides and the company's a Microsoft
[00:12:43.920 --> 00:12:46.080]   company or a Google company.
[00:12:46.080 --> 00:12:49.280]   So they're just going to use the tool that they're already buying in effect.
[00:12:49.280 --> 00:12:51.600]   And that's going to be hard for Slack.
[00:12:51.600 --> 00:12:56.400]   That's one of the reasons Slack had to sell to somebody with a sales force, if you will.
[00:12:56.400 --> 00:13:01.760]   I do think, though, that one thing about this acquisition, I've seen a lot of people say,
[00:13:01.760 --> 00:13:06.320]   "Well, the free tier of Slack," and there's so many of us who are on free Slack instances,
[00:13:06.320 --> 00:13:09.120]   is going to go away because Salesforce is going to want to make money.
[00:13:09.120 --> 00:13:13.920]   And a lot of buyers of companies do stupid things with the business model of the company they buy.
[00:13:13.920 --> 00:13:15.200]   And so they might do it.
[00:13:15.200 --> 00:13:19.520]   But I feel like that free tier is Slack's best sales pitch, right?
[00:13:19.520 --> 00:13:24.800]   That Slack is getting people to like Slack and then go to their bosses and say, "You need to buy Slack."
[00:13:24.800 --> 00:13:26.160]   So I hope they keep that around.
[00:13:26.160 --> 00:13:32.400]   But yeah, I also imagine this is an opportunity for Salesforce to start selling Slack into all
[00:13:32.400 --> 00:13:35.200]   of the organizations where Salesforce is already playing.
[00:13:35.200 --> 00:13:38.480]   And if anybody can take it to Microsoft, it's probably Salesforce.
[00:13:38.480 --> 00:13:41.120]   They must feel that way, otherwise they wouldn't have spent
[00:13:41.120 --> 00:13:46.320]   what is a considerable premium on what Slack's value was.
[00:13:46.320 --> 00:13:48.480]   Do you don't use Slack's, Seth, or do you?
[00:13:48.480 --> 00:13:48.960]   I don't know.
[00:13:48.960 --> 00:13:55.920]   I mean, I've been forced to for projects that I've worked on and whatever.
[00:13:55.920 --> 00:14:03.120]   I mean, I think that if Salesforce winds up fixing Slack so that it works fine
[00:14:03.120 --> 00:14:07.280]   when you have more than four Chrome tabs open, it would be in a normal--
[00:14:07.280 --> 00:14:08.400]   It's an Electron app.
[00:14:08.400 --> 00:14:09.440]   That's part of the problem.
[00:14:09.440 --> 00:14:10.480]   It's just a hook.
[00:14:10.480 --> 00:14:11.440]   Yeah, it's just a hook.
[00:14:11.440 --> 00:14:12.240]   It really is.
[00:14:12.240 --> 00:14:20.800]   I mean, I feel very weirdly about Slack because I have enough distractions throughout my day.
[00:14:20.800 --> 00:14:24.560]   I don't really need to use it to communicate with a lot of people.
[00:14:24.560 --> 00:14:26.800]   I'm not in an office environment.
[00:14:27.920 --> 00:14:34.880]   But I also feel like people communicate about things on Slack
[00:14:34.880 --> 00:14:37.840]   that they feel very comfortable talking about in person,
[00:14:37.840 --> 00:14:40.320]   which is great for Slack.
[00:14:40.320 --> 00:14:43.360]   It's amazing that they created a product that people are doing that with.
[00:14:43.360 --> 00:14:46.960]   It's just really unfortunate that a lot of those things are going to be
[00:14:46.960 --> 00:14:51.920]   either work-sensitive or personally sensitive, and that Slack is probably
[00:14:51.920 --> 00:14:53.600]   not a really good platform for that.
[00:14:53.600 --> 00:14:57.760]   You don't want to be talking about super sensitive information.
[00:14:57.760 --> 00:14:59.360]   Is it insecure?
[00:14:59.360 --> 00:15:00.800]   Is it insecure?
[00:15:00.800 --> 00:15:04.000]   I think it depends on which instance you're using.
[00:15:04.000 --> 00:15:08.480]   Last time I looked, I could be wrong about this,
[00:15:08.480 --> 00:15:12.000]   but the last I looked, Slack was not intended encrypted,
[00:15:12.000 --> 00:15:17.040]   which means that those messages are simply available either to your employer
[00:15:17.040 --> 00:15:23.600]   or possibly available to be subpoenaed, if needed, if you're not in a non-converment.
[00:15:23.600 --> 00:15:25.280]   So there's all kinds of things.
[00:15:25.280 --> 00:15:27.280]   You didn't even think of that.
[00:15:27.280 --> 00:15:31.040]   Yeah, there's all kinds of things that I personally am a little twitchy about.
[00:15:31.040 --> 00:15:33.120]   If you'll forgive the reference.
[00:15:33.120 --> 00:15:36.560]   Yeah, you want to not commit to email or Slack
[00:15:36.560 --> 00:15:41.600]   anything in writing that you don't want to be subpoenaed because it lives forever.
[00:15:41.600 --> 00:15:44.800]   And I didn't even consider that as an example.
[00:15:44.800 --> 00:15:47.120]   I mean, if you have a PGP key in email, that's fine.
[00:15:47.120 --> 00:15:47.680]   Right?
[00:15:47.680 --> 00:15:53.360]   But certainly I advise people who want to talk with me in a Slack instance about
[00:15:53.360 --> 00:15:57.920]   something that's sensitive, that we go take the conversation to any other platform.
[00:15:57.920 --> 00:16:02.720]   Even Facebook Messenger has its secret conversations, which are
[00:16:02.720 --> 00:16:03.680]   intended encrypted.
[00:16:03.680 --> 00:16:07.440]   There's just so many more options out there right now.
[00:16:07.440 --> 00:16:11.760]   And probably for Slack's developers, they feel that there are so many other options.
[00:16:11.760 --> 00:16:14.000]   It's not really something that's worth worrying about.
[00:16:14.000 --> 00:16:20.240]   I'm just thinking about the most famous Slack security flaw,
[00:16:20.240 --> 00:16:22.000]   and it really wasn't Slack's fault.
[00:16:22.000 --> 00:16:28.000]   It was social engineering of Twitter engineers that that's how all of those Twitter accounts
[00:16:28.000 --> 00:16:35.760]   got co-opted, including Joe Biden's because apparently Slack had rather Twitter had posted
[00:16:35.760 --> 00:16:38.960]   credentials for its God mode in Slack, which was a big mistake.
[00:16:38.960 --> 00:16:45.200]   But as I remember, and I haven't followed it since, but I think it was social engineering
[00:16:45.200 --> 00:16:48.960]   right that got them into this Slack community and they found the keys there.
[00:16:48.960 --> 00:16:52.000]   But I think that's a really important point.
[00:16:52.000 --> 00:16:57.360]   If you, of all the things that you must be doing in Slack, perhaps pinning your super
[00:16:57.360 --> 00:17:01.920]   credentials, is less brilliant.
[00:17:01.920 --> 00:17:03.840]   Yeah, but not good.
[00:17:03.840 --> 00:17:04.320]   What can you do?
[00:17:04.320 --> 00:17:10.000]   It kind of shows how people don't really know what is safe and what isn't safe,
[00:17:10.000 --> 00:17:15.600]   and what they should put into water that people, if they're, you know, your admin can read your
[00:17:15.600 --> 00:17:21.200]   emails. I don't think that people, by and large, are taught what, you know, this should be,
[00:17:21.200 --> 00:17:25.440]   this media savvy should be taught in schools so that people are thinking, you know, most people
[00:17:25.440 --> 00:17:29.600]   don't know what end-to-end encryption is. People don't know that, you know, when they send an email
[00:17:29.600 --> 00:17:34.160]   to someone that someone else might be able to read that and have full access to whatever's
[00:17:34.160 --> 00:17:38.800]   happening. And so I think that it's not just people at large's fault.
[00:17:38.800 --> 00:17:44.720]   Boy, you would sure hope that Twitter engineers would have some sense of OXX,
[00:17:44.720 --> 00:17:51.280]   but you're exactly right, Georgia. Even they probably don't know how to assess the risks.
[00:17:51.280 --> 00:17:59.440]   I mean, clearly pinning credentials in a Slack channel is not safe. But maybe they thought it was.
[00:17:59.440 --> 00:18:03.200]   Is that your experience, Seth, when you're talking to enterprises, that people who should
[00:18:03.200 --> 00:18:06.880]   know better just are not as educated about operational security?
[00:18:06.880 --> 00:18:12.560]   It's challenging. I mean, I think, you know, in Twitter's instance, they just did really well by
[00:18:12.560 --> 00:18:20.320]   hiring Mudge, Peter Zatko, you know, as their new... Mudge is great.
[00:18:20.320 --> 00:18:26.400]   Mudge is a great security guy, right? He really, you know, of all the people they could have gotten,
[00:18:26.400 --> 00:18:32.000]   he really knows his stuff, whether he's able to influence the culture there. So this kind of
[00:18:32.000 --> 00:18:37.600]   thing that happened doesn't happen again in the future is, I think, an open question. But I think
[00:18:37.600 --> 00:18:44.560]   it's a question that a lot of organizations are facing, especially from the pandemic work from home
[00:18:44.560 --> 00:18:50.960]   era, where, you know, you don't know what your employees are using for their home routers. You
[00:18:50.960 --> 00:18:58.800]   don't know who else is on their network. You don't know, you know, what else is going on. My wife,
[00:18:58.800 --> 00:19:04.720]   for example, wasn't able to use our printer here because she has a work laptop and she doesn't have
[00:19:04.720 --> 00:19:09.760]   permissions to connect it, which is really good. I think, I mean, I told her it's unfortunate that
[00:19:09.760 --> 00:19:13.040]   you have to connect to my computer to print, but whatever, there's worse things in life.
[00:19:13.040 --> 00:19:17.760]   But I suspect that there's a lot of organizations that don't have that kind of,
[00:19:17.760 --> 00:19:25.760]   you know, a forethought in their processes because they're just trying to get stuff done,
[00:19:25.760 --> 00:19:30.640]   because everybody's under the gun these days, and it's so hard just to ensure that you're,
[00:19:30.640 --> 00:19:36.560]   you're, you know, able to come close to making your numbers. So, and when it's a real challenge,
[00:19:36.560 --> 00:19:41.680]   when companies do enforce that, I mean, obviously her company is aware of security flaws in HP
[00:19:41.680 --> 00:19:47.360]   printers and others. And so it was reluctant to allow her to connect to an unknown device.
[00:19:47.360 --> 00:19:54.960]   But I had a caller on the radio show today who was pissed off because his, he had video from his
[00:19:54.960 --> 00:20:01.280]   Okey security camera, and he couldn't share it. It was downloaded to his Apple photos,
[00:20:01.280 --> 00:20:06.640]   and he couldn't share it without giving Okey permission to access his Apple photos. And he was
[00:20:06.640 --> 00:20:13.360]   all upset about that. And I said, no, that's a good thing. You want a company to have to ask for
[00:20:13.360 --> 00:20:19.360]   permission to access your stuff. And if you'd, you know, go ahead. I don't think that he knew that
[00:20:19.360 --> 00:20:23.200]   they weren't acts like that if you they were just granted permission, they're going to be
[00:20:23.200 --> 00:20:27.600]   accessed. And it was, I don't think that people really grasp that that means that they have access
[00:20:27.600 --> 00:20:32.160]   to everything. No, he called the company. He called Okey and said, how would you, and they said,
[00:20:32.160 --> 00:20:37.200]   no, we don't want your photos, but you can't access it unless Apple says we can't access your
[00:20:37.200 --> 00:20:42.720]   photos unless we can access your photos. Yeah. Yeah. Right. Right. Exactly.
[00:20:42.720 --> 00:20:50.880]   Maybe the signal biggest hurdle in security is just people don't have that mindset. I don't blame
[00:20:50.880 --> 00:20:54.560]   them. Nobody wants to have that mindset, but you need to kind of be paranoid.
[00:20:54.560 --> 00:21:02.640]   There. Yeah, you do. And it's challenging when the technology is developing at such a rapid clip
[00:21:02.640 --> 00:21:07.760]   that you're not even able to stay on top of how things are changing. So things that were true,
[00:21:07.760 --> 00:21:15.440]   maybe 10 years ago, you know, in, in some cases aren't true. In other cases, I think are even worse.
[00:21:17.200 --> 00:21:24.160]   You know, and we see this in, in so many otherwise benign instances. So, you know, I'm, what I'm
[00:21:24.160 --> 00:21:30.640]   hoping of all things that happens with the, with the Slack acquisition is that Salesforce is able to
[00:21:30.640 --> 00:21:39.520]   really force, you know, its Slack users to improve the levels of security that they're, that they're
[00:21:39.520 --> 00:21:44.240]   using and to really consider what they're posting, how they're posting it, which is a challenge because
[00:21:44.240 --> 00:21:49.920]   it's in their interest, you know, as the owners of Slack to have people post as much as possible.
[00:21:49.920 --> 00:21:56.560]   But I think there's, you know, there's, there really has to be some, some thought given from
[00:21:56.560 --> 00:22:04.240]   software developers and manufacturers into what kinds of things you want people posted. It's not
[00:22:04.240 --> 00:22:09.600]   good that you, that people put everything into Gmail. We've seen, you know, time and again that
[00:22:09.600 --> 00:22:17.120]   come back to bite politicians and military generals and all sorts of people, you know, in the behind.
[00:22:17.120 --> 00:22:24.640]   So it's, it's not technology isn't for everything. It's not your, it's not your side brain. It's
[00:22:24.640 --> 00:22:28.560]   not where you should be dumping everything that you can't keep track of at the moment.
[00:22:28.560 --> 00:22:34.480]   But let's be honest, like our brains are not that good at keeping the information in the first place.
[00:22:34.960 --> 00:22:40.160]   So we want to be able to have a place where it can exist and we don't have to worry about it
[00:22:40.160 --> 00:22:45.440]   being forgotten or lost or have an easy way to remember it. And so dealing with passwords,
[00:22:45.440 --> 00:22:49.360]   like, yeah, you could have one password or something else that would then be a safer way to do it.
[00:22:49.360 --> 00:22:54.640]   But a lot of people would feel stressed out just putting on to one password. And then you might feel
[00:22:54.640 --> 00:22:58.560]   comfortable with say your ring doorbell and then they get bought out with another company and
[00:22:58.560 --> 00:23:04.560]   everything changes. And are you going to change your doorbell once it's hooked up now? So it's,
[00:23:04.560 --> 00:23:09.440]   it's really hard because reading through these privacy policies and figuring everything out
[00:23:09.440 --> 00:23:12.000]   isn't an easy thing if you're not already tech minded.
[00:23:12.000 --> 00:23:18.400]   I think even if you're tech minded reading through privacy policies is a real.
[00:23:18.400 --> 00:23:23.760]   Who does it? Nobody does it. I do do it. You do that, Georgia. You read those.
[00:23:23.760 --> 00:23:29.120]   I read them. I will read them. I don't even understand them all, but especially like, like stuff that
[00:23:29.120 --> 00:23:34.880]   I'm like, it seems sketchy. I will read through it and I'll be then and almost 100% of the time
[00:23:34.880 --> 00:23:38.800]   it'll come to a point where I'm like, oh, and they're going to snoop through every and I'm like,
[00:23:38.800 --> 00:23:42.160]   okay, no, I won't. And sometimes I'll read through it and I'll be like, you know what,
[00:23:42.160 --> 00:23:48.160]   I really want to play plants versus zombies too anyways. And so fun. Take it all. And I've sold
[00:23:48.160 --> 00:23:53.280]   my soul to, you know, what complicates it, of course, is that we're not lawyers. And a lot of
[00:23:53.280 --> 00:23:59.920]   the time stuff that's put in there is put in there as legal boilerplate. And we read it as a normal
[00:23:59.920 --> 00:24:06.080]   human and say, Oh my God, they say they reserve rights in all forms of media and invented or not
[00:24:06.080 --> 00:24:11.440]   invented for the rest of the eternity. I can't, I can't agree to that. And it's just, it's just
[00:24:11.440 --> 00:24:17.680]   typical legalese. And it's not, it's not something as bad or as onerous as it sounds. But we don't,
[00:24:17.680 --> 00:24:20.400]   we're not good. But it could be. We don't know. I guess it could be.
[00:24:20.400 --> 00:24:24.480]   Right. It could be. It could choose to use it. Right. Facebook could say, you know what,
[00:24:24.480 --> 00:24:28.880]   we're going to be using your picture as an ad campaign. Yeah. You've already given us permission
[00:24:28.880 --> 00:24:35.920]   by doing it. And are you going to be able to even fight them? No. No. No. And I think one of the,
[00:24:35.920 --> 00:24:40.800]   one of the solutions, one of the partial solutions to the problem is really going to wind up being a
[00:24:40.800 --> 00:24:49.040]   national privacy policy law. Yeah. And California has taken some very interesting steps and,
[00:24:49.040 --> 00:24:54.800]   you know, that push it much more in line with Canada and the EU. And I think it's really going to
[00:24:54.800 --> 00:25:00.720]   happen at some point in the, in the Biden administration. I don't know if he's going to support it. But I
[00:25:00.720 --> 00:25:07.520]   think that, you know, tech companies are going to be so frustrated in dealing with this, you know,
[00:25:07.520 --> 00:25:14.880]   multi-prong approach to privacy that they just want to have one standard. And, you know, if
[00:25:14.880 --> 00:25:18.720]   California's law looks a lot like Europe's law, there's a really good chance that the rest of the
[00:25:18.720 --> 00:25:22.800]   US is going to go in that direction as well. There's kind of a sneaky thing that happens to
[00:25:22.800 --> 00:25:29.840]   Jason and I are California voters. The ballot proposition that we voted on in November, which
[00:25:29.840 --> 00:25:37.280]   passed to even strengthen the California Privacy Act, was actually kind of written by Facebook.
[00:25:37.280 --> 00:25:46.000]   Yeah. Well, so it looked good. And it seemed like a strengthen, but there were these weird loopholes.
[00:25:47.040 --> 00:25:53.440]   So I wrote about this. And I'm actually doing a piece, a follow-up piece for a non-tech
[00:25:53.440 --> 00:25:59.600]   publisher, which is kind of exciting because it's so rare to get an opportunity to talk about
[00:25:59.600 --> 00:26:09.440]   these very deep in the woods tech issues for non-tech audiences. But Prop 24 does some really
[00:26:09.440 --> 00:26:17.600]   interesting things. It creates a California government agency similar to the FTC to regulate this stuff.
[00:26:17.600 --> 00:26:23.120]   Right. And it gave them a budget that is large, that is almost the same size as the budget that
[00:26:23.120 --> 00:26:28.960]   the FTC has for regulating this stuff. Wow. So California alone will have a 10 million
[00:26:28.960 --> 00:26:36.480]   dollar budget to be looking at these issues. It did change the minimum threshold for
[00:26:38.560 --> 00:26:45.360]   ultimately what is a violation of the law in a data breach, which is 100,000 users, which is up
[00:26:45.360 --> 00:26:55.600]   from 50,000. But it also means that these companies could potentially be exposed to legal action,
[00:26:55.600 --> 00:27:06.800]   possibly class action suits, possibly individual suits, in situations where they were breached
[00:27:06.800 --> 00:27:15.680]   and they didn't protect their consumers' data. It's revolutionary in California and in the US.
[00:27:15.680 --> 00:27:21.840]   The thing that was controversial was besides the giveaways to the big companies, the feeling was,
[00:27:21.840 --> 00:27:29.600]   well, Facebook can afford to comply. But what it does is kind of pulls up the ladder for smaller
[00:27:29.600 --> 00:27:35.440]   companies. It makes it very hard for them to compete with Facebook. So it had this kind of inverse.
[00:27:36.320 --> 00:27:41.680]   It was a very confusing law. The way that the bill was written, it was not particularly clear.
[00:27:41.680 --> 00:27:46.880]   I was even a little weirded out and I asked one of my sources for the story about this,
[00:27:46.880 --> 00:27:51.920]   about the fact that it physically looked or visually looked, the PDF looked sort of
[00:27:51.920 --> 00:27:57.440]   off. And they said, yeah, it looks like it's been photocopied five times and then scanned in as a
[00:27:57.440 --> 00:28:02.400]   PDF. And I said, oh, good. This isn't just me that is sort of unnerved by that.
[00:28:03.760 --> 00:28:09.600]   The other thing to keep in mind is that while the California privacy law that was passed,
[00:28:09.600 --> 00:28:18.160]   that created this tougher regime, in the first place in 2018 didn't take effect until January 1st
[00:28:18.160 --> 00:28:27.600]   of this year, they didn't even finish delineating a lot of the nuances of it until September,
[00:28:28.880 --> 00:28:33.760]   which means that the law had been in effect for nine months. And then they sort of kind of
[00:28:33.760 --> 00:28:38.640]   finished it, but still haven't really. And there's a very good chance that that's going to happen
[00:28:38.640 --> 00:28:49.840]   with the CPRA, which is this law that Prop 24 codified. So it's going to be a long,
[00:28:49.840 --> 00:28:55.600]   it's going to be a while before things start to get settled in California privacy.
[00:28:55.600 --> 00:28:59.520]   But this is a step in the right direction. It's probably better to do something rather than
[00:28:59.520 --> 00:29:04.880]   nothing. The other thing that was a little controversial was companies could pay or
[00:29:04.880 --> 00:29:08.560]   lab businesses to create pay for privacy schemes so that if you could afford it,
[00:29:08.560 --> 00:29:14.240]   it could be private. You'd pay Facebook about 50 a month or whatever, and then they wouldn't use
[00:29:14.240 --> 00:29:19.040]   your information. Actually, I'm not against that. Great for them. Yeah, I'm not against it.
[00:29:19.040 --> 00:29:23.840]   Even more money off of you. But it certainly doesn't seem like privacy should be a luxury item.
[00:29:23.840 --> 00:29:30.480]   Like that should be the fault. But at the same time, I understand that Facebook has costs.
[00:29:30.480 --> 00:29:35.600]   This is the conundrum. Oh, come on, Leo. But they're providing a free service.
[00:29:35.600 --> 00:29:40.960]   But they can find another way to monetize it. That's not my fault. One way would be to charge.
[00:29:40.960 --> 00:29:46.160]   Right? They can charge and then they lose their, they don't want to charge. Right? That's the thing
[00:29:46.160 --> 00:29:50.400]   is that they don't want to charge because they're making more money off of selling off our privacy.
[00:29:50.400 --> 00:29:54.480]   So they end up getting their cake and eating it too. That it's like the phone book. You have to pay
[00:29:54.480 --> 00:29:59.840]   not to be in a part of a phone. I know that's dopey. It's the same thing, right? We're going to sell
[00:29:59.840 --> 00:30:04.720]   off everything that you do and be able to make money off of that. And we're going to be able to
[00:30:04.720 --> 00:30:08.480]   go against laws that we've said that we're going to say, "Oh, we're not going to, you know,
[00:30:08.480 --> 00:30:13.600]   to click and this is not all going to be linked to where you are and then do it anyways." And then
[00:30:13.600 --> 00:30:18.400]   get a small slap on the wrist that isn't going to bother us. Or you can pay us money
[00:30:18.400 --> 00:30:23.840]   not to abuse your privacy. That seems... Did you deal with this, Jason, when you were voting? I
[00:30:23.840 --> 00:30:32.160]   ended up voting for it, Prop 24. But I was very mixed. I voted against it and I voted against
[00:30:32.160 --> 00:30:37.280]   it mostly because our privacy restrictions that went through California's legislature
[00:30:37.280 --> 00:30:42.960]   are very recent. And I kind of want to see... I'm really suspicious when we have a brand new law
[00:30:42.960 --> 00:30:47.120]   that's taking effect and we don't really know how it's going to go. And then all of a sudden,
[00:30:47.120 --> 00:30:50.800]   there's a ballot measure that wants to tweak it in some way because it feels to me like,
[00:30:50.800 --> 00:30:56.480]   maybe we should watch this work and then everybody should learn. And I get real suspicious at that
[00:30:56.480 --> 00:31:02.160]   point that maybe this is industry trying to quickly knock out the things that they don't want to
[00:31:02.160 --> 00:31:07.520]   have in there. So I voted against it. I'm for more protection, but it did seem a little bit iffy
[00:31:07.520 --> 00:31:11.520]   and I always figured, you know, you can come back next year and fix it if we don't like it this year.
[00:31:11.520 --> 00:31:17.920]   You all said, we're groups opposing it like the ACLU opposed it. The Republicans opposed it,
[00:31:17.920 --> 00:31:23.040]   but so did the Green Party and the Libertarian Party. It's like kind of... I think that this was
[00:31:23.040 --> 00:31:30.000]   one of those cases. It was created by Alistair McTaggett, who is a real estate guy in San Francisco who
[00:31:30.000 --> 00:31:35.840]   proposed the original ballot proposition that the legislature jumped in and said, "No, no, no, no,
[00:31:35.840 --> 00:31:40.960]   if we pass a bill, will you yank this two years ago?" He said, "All right,
[00:31:40.960 --> 00:31:44.560]   but pass that bill." And they did. I don't think he talks like that, but he should with an
[00:31:44.560 --> 00:31:50.160]   alien like Alistair McTaggett. All right. And they passed it, but then he came back and said,
[00:31:50.160 --> 00:31:55.280]   "But I'm going to do it anyway." And that's by the way why that so quickly after the initial
[00:31:55.280 --> 00:31:59.440]   California law went in effect, we had to vote again on a new proposition.
[00:31:59.440 --> 00:32:06.480]   I mean, I voted against it in part because I couldn't easily parse what was happening with it.
[00:32:06.480 --> 00:32:12.880]   Yeah. Yeah. And I really feel that California is already besotted with so many horribly written
[00:32:12.880 --> 00:32:18.960]   laws that are really hard to follow and have unclear impacts on voters,
[00:32:18.960 --> 00:32:24.960]   and have really harmful effects down the line like look at Prop 13.
[00:32:25.520 --> 00:32:30.400]   Oh, God, yes. It is a real estate law which you can have an entire different podcast on.
[00:32:30.400 --> 00:32:39.600]   Ultimately, I think that Prop 24 is good in the sense that even though I voted against it,
[00:32:39.600 --> 00:32:42.720]   I think it's good in the sense that it gets people thinking and discussing this.
[00:32:42.720 --> 00:32:55.440]   But it's very early in the process as Jason noted. And I really feel like it seems we all do.
[00:32:55.680 --> 00:33:03.520]   That pay for privacy is a bad idea. I just think that we're already halfway there in a lot of cases.
[00:33:03.520 --> 00:33:10.480]   If you have the resources to understand that there are more private email options available
[00:33:10.480 --> 00:33:15.120]   to you, for example, for texting options. Yeah, I don't use Gmail anymore. I decided not to.
[00:33:15.120 --> 00:33:19.920]   I use a paid service called Fastmail because I don't trust Google with my mail.
[00:33:19.920 --> 00:33:23.120]   Right. Right. So it is a luxury. I understand.
[00:33:24.000 --> 00:33:31.600]   But the issue is you could not use Facebook. That's free. You could use Facebook completely for free,
[00:33:31.600 --> 00:33:35.440]   in which case they need to monetize you some way, which they do with advertising.
[00:33:35.440 --> 00:33:39.040]   Or if you don't like that, you could say here, I'll give you a few.
[00:33:39.040 --> 00:33:43.280]   The estimate is not as pretty low as like four bucks a year or something that they
[00:33:43.280 --> 00:33:47.600]   but let's say they sounds off. It seems like it'd be worth more. But anyway,
[00:33:47.600 --> 00:33:51.040]   say it's five bucks a month. If you really wanted to use Facebook in a private way,
[00:33:51.040 --> 00:33:55.920]   you could pay five bucks. I like that having that option that gives you three options.
[00:33:55.920 --> 00:34:00.480]   But you think it should be by default private Georgia? Yeah.
[00:34:00.480 --> 00:34:04.320]   But then how do they pay for themselves? I think that if they get caught
[00:34:04.320 --> 00:34:10.080]   misusing going like every time that they get caught misusing our privacy by their own privacy
[00:34:10.080 --> 00:34:14.640]   policy that I think that there should be like we should have a right to our privacy.
[00:34:14.640 --> 00:34:17.840]   Well, if they don't hear it here to their privacy policy, they should absolutely
[00:34:17.840 --> 00:34:21.920]   punish that. They should have, but I don't think it should be a little tiny fine. I'm sorry.
[00:34:21.920 --> 00:34:25.280]   These fines are nothing to them. They're nothing.
[00:34:25.280 --> 00:34:31.360]   And it takes so very long to be able to go through it. They're absolutely ineffective.
[00:34:31.360 --> 00:34:37.920]   I think that if I get caught going into your house, Leo, and stealing your documents
[00:34:37.920 --> 00:34:43.040]   and then selling them, I'm going to jail. And I'm just going to say it. Yeah.
[00:34:43.040 --> 00:34:48.160]   You run a company, you're the head of that organization. You should be legally liable for
[00:34:48.160 --> 00:34:51.600]   everything that happens within your company. And then I bet everyone's only going to be watching
[00:34:51.600 --> 00:34:56.880]   their piece and cues and being much more careful because that restriction to their own freedom
[00:34:56.880 --> 00:35:01.920]   would be something that they would care about. But if I'm making, say I'm making $100 and you're
[00:35:01.920 --> 00:35:07.200]   going to find me $1, that's just a cost of operation. Great. I'm going to use this without any
[00:35:07.760 --> 00:35:14.400]   worry and fear. So I think that one is there should be laws about our privacy and what information
[00:35:14.400 --> 00:35:18.320]   should be used and what information shouldn't be used. And they should be able, they should have
[00:35:18.320 --> 00:35:24.080]   to disclose what is specifically going to happen in layman's terms that everyone will understand.
[00:35:24.080 --> 00:35:28.560]   And that should be across the board. And then they get caught with face-
[00:35:28.560 --> 00:35:29.120]   I think Georgia's a issue.
[00:35:29.120 --> 00:35:30.560]   --face-
[00:35:30.560 --> 00:35:30.560]   I think Georgia's a issue.
[00:35:30.560 --> 00:35:30.560]   --how many times, right?
[00:35:30.560 --> 00:35:36.960]   Yeah. Georgia, I think you're absolutely right. And something like fines being equal to a
[00:35:36.960 --> 00:35:38.160]   percentage of rev.
[00:35:38.160 --> 00:35:40.320]   --has in GDPR, yeah.
[00:35:40.320 --> 00:35:47.520]   Yeah. Would be great. And the challenge is, is that even in GDPR, which for the most part is a
[00:35:47.520 --> 00:35:55.520]   significant improvement over anything we have in the US, there is still a loophole in it that
[00:35:55.520 --> 00:36:02.480]   allows companies to remedy the problem and then come back to the regulatory agency and say,
[00:36:02.480 --> 00:36:12.160]   look, we fixed it. It's amazing. So even when there are reasonable safeguards ensuring that
[00:36:12.160 --> 00:36:19.680]   companies are fined for bad behavior at a rate that is fair, I think it's still really challenging
[00:36:19.680 --> 00:36:25.840]   to deal with situations that allow them to do things like fix it when there may not really be
[00:36:25.840 --> 00:36:32.880]   a fix. If your data has been stolen and leaked, then that horse has long since left the barn.
[00:36:32.880 --> 00:36:34.560]   There's no way you're going to get that one back.
[00:36:34.560 --> 00:36:36.640]   --and often it's not--
[00:36:36.640 --> 00:36:40.400]   --and I think that laws are just not up to snuff at this point.
[00:36:40.400 --> 00:36:44.320]   --also, I hear a lot of people talking about, oh, well, what is Facebook going to do? How are they
[00:36:44.320 --> 00:36:47.520]   going to survive if they can't do this? And I just want to point out of somebody who worked for a
[00:36:47.520 --> 00:36:53.760]   media company for years-- yeah, poor Facebook-- is a lot of what they do in terms of personalization,
[00:36:53.760 --> 00:36:59.360]   in terms of targeting. Some of that is necessary. A lot of it is unnecessary. A lot of personal
[00:36:59.360 --> 00:37:07.120]   information gets collected by all of us, and we get tracked. And it's all in the purposes to target
[00:37:07.120 --> 00:37:13.360]   us and know everything about us. Although what you'll find is a lot of times, untargeted advertising
[00:37:13.360 --> 00:37:20.640]   works pretty well. Facebook, I get like Facebook micro-targets, and it's great for small businesses
[00:37:20.640 --> 00:37:25.040]   that need to find a very particular kind of person. But I am-- I'm going to say I am,
[00:37:25.040 --> 00:37:30.320]   based on my history in the media, extremely skeptical that you cannot put in a privacy regime
[00:37:30.320 --> 00:37:35.840]   and limit the information that gets shared and limit tracking that happens, especially across
[00:37:35.840 --> 00:37:40.640]   sites, and that tech companies that rely on advertising are not going to be able to find a way
[00:37:40.640 --> 00:37:46.320]   to make a living at it. I just don't believe it. I think that they are very smart and that there
[00:37:46.320 --> 00:37:50.560]   are lots of ways to sell advertising, and there are lots of ways to identify target audiences,
[00:37:50.880 --> 00:37:55.360]   and that you don't need to do the level of collection. I think a lot of the data collection that goes on
[00:37:55.360 --> 00:38:01.920]   is completely unnecessary and is just used as a sales tool. So I think that the stricter the laws--
[00:38:01.920 --> 00:38:02.720]   That's interesting.
[00:38:02.720 --> 00:38:06.720]   The more evidence you're going to get that a lot of this stuff, not all of it, but a lot of it,
[00:38:06.720 --> 00:38:12.080]   and some of the most invasive stuff, I don't think it's really necessary. I think it's just a
[00:38:12.080 --> 00:38:16.080]   technique they use to convince people that their ads on the internet are going to be effective,
[00:38:16.080 --> 00:38:21.840]   and then they aren't, and we move on with our lives. So I think always make them prove it,
[00:38:21.840 --> 00:38:24.800]   make them prove that they actually need that information, because I don't think they do.
[00:38:24.800 --> 00:38:29.360]   You actually, Jason, that's a-- I think you're right.
[00:38:29.360 --> 00:38:32.000]   I can't tell you how much is going on about that, but I think you're right.
[00:38:32.000 --> 00:38:36.320]   In my career in the publishing industry, in print, but also on the web,
[00:38:36.320 --> 00:38:42.560]   I cannot tell you how many times we got asked for more data, more data about our customers,
[00:38:42.560 --> 00:38:45.600]   more agencies and advertisers die for that stuff.
[00:38:45.600 --> 00:38:46.800]   They demand it.
[00:38:46.800 --> 00:38:51.520]   They don't do anything with it, and it's very rarely used, and it's very rare that it has value.
[00:38:51.520 --> 00:38:56.400]   Sometimes it does, but more often it doesn't. It's just a sales technique.
[00:38:56.400 --> 00:38:58.240]   Podcasting is going through the same thing now.
[00:38:58.240 --> 00:38:58.880]   We are.
[00:38:58.880 --> 00:39:03.840]   Oh, now we need to have data about the number of downloads and the time of day we need to target,
[00:39:03.840 --> 00:39:09.440]   and a lot of it is just a psychological tool that sales people use to get money from their
[00:39:09.440 --> 00:39:14.320]   clients. It's not something that anyone at any point in the chain actually uses.
[00:39:14.320 --> 00:39:18.160]   And Facebook has done this before. They did this in 2015 with Video,
[00:39:18.160 --> 00:39:24.960]   and how many news organizations and publishers died because they listened to Facebook, which told
[00:39:24.960 --> 00:39:30.080]   them, "You need to move everything to video. Video is the next big thing. Here's how it's going
[00:39:30.080 --> 00:39:35.920]   to help your business. All these organizations invested in Video and Blammo, they're gone."
[00:39:35.920 --> 00:39:40.800]   The dirty little secret of advertising, nobody knows what the hell works or why.
[00:39:40.800 --> 00:39:41.520]   Is that right?
[00:39:41.520 --> 00:39:42.000]   Right.
[00:39:42.000 --> 00:39:43.040]   Is that fair?
[00:39:43.040 --> 00:39:47.360]   Except that we do know, right? We do know that there are certain things that you work well.
[00:39:47.360 --> 00:39:48.080]   You got to do it.
[00:39:48.080 --> 00:39:56.240]   But we don't need to do the kinds of data mining that Facebook says we do.
[00:39:56.240 --> 00:40:00.160]   And advertisers want to rationalize it.
[00:40:00.160 --> 00:40:04.960]   But you're saying that they're effective in advertising.
[00:40:04.960 --> 00:40:07.680]   Yeah, they want to say we have a scientific.
[00:40:07.680 --> 00:40:11.200]   Our spend is, we're going to get this value.
[00:40:11.760 --> 00:40:14.560]   It's not like there isn't data that is useful. There absolutely is.
[00:40:14.560 --> 00:40:20.000]   And Facebook's really good at that. It's that of the ocean of data that gets collected,
[00:40:20.000 --> 00:40:25.520]   what of it actually does get used and is effective? And I would guess it's very small,
[00:40:25.520 --> 00:40:30.880]   and that they could be limited dramatically by laws in terms of what information gets shared,
[00:40:30.880 --> 00:40:36.960]   and what cross-site tracking is allowed. And they would still be able to do a pretty good job
[00:40:36.960 --> 00:40:43.520]   of targeting, especially advertising into people's eyeballs. I think that they overstate how much
[00:40:43.520 --> 00:40:48.400]   they need. And again, I think it's up for them to prove it. I think that as members of the public,
[00:40:48.400 --> 00:40:53.280]   we should be demanding as much privacy as we can, and hear from those tech giants about
[00:40:53.280 --> 00:40:57.760]   what they actually need. Because we can't take them at their word.
[00:40:57.760 --> 00:41:03.200]   There is the chat rooms are running me. There's a two-part free economics that came out.
[00:41:03.200 --> 00:41:11.440]   Episode 440 and 441 does advertising actually work? First one's about TV, which is a half-trillion
[00:41:11.440 --> 00:41:19.600]   dollar a year business worldwide. And the second is digital. Google and Facebook are worth two
[00:41:19.600 --> 00:41:26.000]   trillion themselves. And I don't know what Jason, do you sell your own podcast, or do you have
[00:41:26.000 --> 00:41:29.360]   somebody do that for you? I have my friend Mike Hurley sell on my podcast.
[00:41:29.360 --> 00:41:35.440]   Oh, yeah, Mike's good. And we did a little bit with Mike as well. And of course, Lisa,
[00:41:35.440 --> 00:41:40.000]   my wife and CEO of Twit does a lot of the selling. We have a sales team, but
[00:41:40.000 --> 00:41:47.120]   there's a huge demand in podcasting now. And it's almost universal from agencies, especially.
[00:41:47.120 --> 00:41:52.800]   We've got to know more than just, you know, you're a geek podcast. That's not enough.
[00:41:52.800 --> 00:41:57.360]   We've got to know a lot more. And I don't want to give them any information. So this is this kind of
[00:41:57.360 --> 00:42:04.160]   tug of war between. And Lisa says, if we don't do it, we won't get the buy. They'll go to somebody
[00:42:04.160 --> 00:42:09.680]   who will. And the problem is Spotify and iHeart, the two biggest podcast companies in the world,
[00:42:09.680 --> 00:42:14.160]   are because they use apps, do know that information and are willing to give it to advertisers. And
[00:42:14.160 --> 00:42:18.400]   so advertisers say, well, why would I buy your podcast? I get all that information from Spotify,
[00:42:18.400 --> 00:42:21.520]   if you won't give it to me, see you later. It's very challenging.
[00:42:21.520 --> 00:42:25.680]   Yeah, one of my professors in college actually wrote a book that was essentially
[00:42:25.680 --> 00:42:30.080]   does advertising work. And the answer is yes, sometimes, but not in the ways that you think and
[00:42:30.080 --> 00:42:34.800]   not from necessarily the reasons that the advertisers wanted to work. It's super complicated. But
[00:42:34.800 --> 00:42:39.920]   one thing that you learn spending a couple of decades around salespeople, right, is there's
[00:42:39.920 --> 00:42:45.600]   sales technique and what works to make the sale. And then there's what is actually required. And
[00:42:45.600 --> 00:42:50.240]   I would I would wage you not only that we are living in an era where everybody wants more data,
[00:42:50.240 --> 00:42:54.880]   but I would I would very strongly suggest that the people who want more data won't do anything with it.
[00:42:54.880 --> 00:42:59.040]   Yeah, well, that's been kind of our experience because sometimes the data is
[00:42:59.040 --> 00:43:03.520]   for a testically wrong and it doesn't matter. It doesn't. It's not for it's irrelevant.
[00:43:03.520 --> 00:43:09.440]   For folks that are that that haven't really seen this and want to take a good look at it,
[00:43:09.440 --> 00:43:17.600]   I did a piece back in I think it was October, maybe November, on the Markups new tool called Blacklight,
[00:43:17.600 --> 00:43:25.680]   which shines a light on how you are how you are being tracked across the web, even when you're not
[00:43:25.680 --> 00:43:31.440]   logged in to sites that are tracking you. And if you get a chance to take a look at that,
[00:43:31.440 --> 00:43:38.240]   it's it's really interesting. There it is. Perfect. Yeah, it's a it's a great way to see how you know,
[00:43:38.240 --> 00:43:44.000]   how these organizations are following you and to what end. It's important these days.
[00:43:45.600 --> 00:43:50.080]   You can light went on for me with you, Jason, because we all know this is happening.
[00:43:50.080 --> 00:43:56.720]   And in my mind, I had justified it as I just did with you, Georgia, but well, this is how they make
[00:43:56.720 --> 00:44:01.920]   they pay for these free services, Google and Facebook pay for free services. But but if all of
[00:44:01.920 --> 00:44:09.840]   this collection of data doesn't actually help them or make it more efficient, then there there is a
[00:44:09.840 --> 00:44:16.160]   way out. I mean, we don't have to be doing this. But even if it even if it didn't, why should we
[00:44:16.160 --> 00:44:22.160]   be giving up our privacy and them allowing them to go giving it to policy place out, right?
[00:44:22.160 --> 00:44:27.920]   It's a trade. You're using Facebook for free. Fair trade. Like we they tell us it's a fair trade,
[00:44:27.920 --> 00:44:32.720]   but it isn't really a fair trade because they have the lawyers, they have the legalese, they
[00:44:32.720 --> 00:44:38.320]   understand they've written this privacy policy. We don't we no one reads it. No one I read it every
[00:44:38.320 --> 00:44:42.960]   now and then I'm fine. But I don't understand it when I read it. It's 60 pages long. They try to
[00:44:42.960 --> 00:44:49.040]   make this as difficult as possible. Like almost 0% of people will read this. They click, they go.
[00:44:49.040 --> 00:44:55.840]   And they just but you use safety. Don't use face. I don't. I don't use face. But let's be honest,
[00:44:55.840 --> 00:45:01.040]   is that a company that is going against their own policies should be. Yes. I don't feel any
[00:45:01.040 --> 00:45:04.240]   sympathy for them. But what if they weren't? What if they were doing what they said they were
[00:45:04.240 --> 00:45:09.840]   going to do, which is still invasive? Yeah. I think that there should be laws for what companies
[00:45:09.840 --> 00:45:14.160]   should be able to track and how it goes and who they should sell it to and that we should
[00:45:14.160 --> 00:45:18.720]   like, I think that the entire infrastructure needs to change, whether they're using it for good or
[00:45:18.720 --> 00:45:23.360]   ill. I think the entire infrastructure needs to be an overhaul and that it should be, you know,
[00:45:23.360 --> 00:45:28.240]   one page and it should be comfortable. If we were going through and going to a contract and we were
[00:45:28.240 --> 00:45:33.280]   equal parties had, you know, the same, you know, lawyers that were going through it, we would make
[00:45:33.280 --> 00:45:37.120]   sure that we both fully understand, you go to a court of law, they ask you, do you understand
[00:45:37.120 --> 00:45:41.440]   all of the charges that you're going to be way late to? I don't think that it's fair that people
[00:45:41.440 --> 00:45:45.200]   are going to be held to a contract that they don't understand. I understand. But I also don't
[00:45:45.200 --> 00:45:49.200]   think it's fair that people say, well, I want my Gmail for free and I want my Facebook for free
[00:45:49.200 --> 00:45:54.720]   and I'm not willing to do anything to change it. Well, at least, at least with Gmail, dude.
[00:45:54.720 --> 00:45:59.600]   Fine. Yeah, at least with Gmail, you have options, right? There's there's multiple options out there
[00:45:59.600 --> 00:46:03.920]   that are, you know, some of them are paid, some of them are free. You could even go if you
[00:46:03.920 --> 00:46:08.800]   have a service on the web is free. But there's no other, there's no other option to Facebook.
[00:46:08.800 --> 00:46:13.360]   Yeah, I mean, this podcast is free. There's no way I could do what I do if we didn't have
[00:46:13.360 --> 00:46:20.080]   advertising. It's just wouldn't be viable. And I don't think Facebook and Google would be viable
[00:46:20.080 --> 00:46:24.240]   if they didn't, if they weren't massive ad companies. I mean, really, that's their real business.
[00:46:24.240 --> 00:46:29.360]   But, but Leo, I know this is, this is disheartening, but there are other podcasts on the internet
[00:46:29.360 --> 00:46:34.560]   that talk about tech besides this freaking deck that are free. That are free.
[00:46:34.560 --> 00:46:37.520]   Don't have ads. Yeah. They don't have ads. Well, and
[00:46:37.520 --> 00:46:45.120]   people, it's a hobby, not a living for them. Well, sure. But I feel like I
[00:46:45.120 --> 00:46:49.040]   want to make a living doing this. And I do it by advertising. I don't think there's anything
[00:46:49.040 --> 00:46:54.800]   wrong with that. You're saying, no, people should only listen to free. No, I'm saying that Facebook
[00:46:54.800 --> 00:47:00.320]   is a monopoly. There's no other choice. Oh, I see. There's no legitimate social
[00:47:00.320 --> 00:47:02.480]   Yeah, if you don't like ads, you could listen to something else.
[00:47:02.480 --> 00:47:07.040]   Yeah. You know what I mean? Right. Or there's other ways of doing things.
[00:47:07.040 --> 00:47:12.160]   But there really is no other option at this point. And this sort of ties into a story from
[00:47:12.160 --> 00:47:19.280]   last week that Reuters ran that Facebook and Google are about this close to facing
[00:47:19.280 --> 00:47:21.520]   legal action from the US government. It's just about to happen.
[00:47:22.720 --> 00:47:28.320]   But wait, now if there were a wonderful place, it made the best soup in the world down the street.
[00:47:28.320 --> 00:47:32.000]   And it really, there was nowhere else that you could get soup with that was that good.
[00:47:32.000 --> 00:47:35.920]   That wouldn't justify you going in and saying, I'm not going to pay you for the soup. Give me the
[00:47:35.920 --> 00:47:42.640]   soup. It doesn't matter that that's the only place you can get that soup. You got to pay for it.
[00:47:42.640 --> 00:47:46.640]   That's the deal. I don't think monopoly or not monopoly makes any difference.
[00:47:47.520 --> 00:47:54.720]   If you want that service, you should pay for it. Yeah, but they don't want us to pay through it
[00:47:54.720 --> 00:47:59.440]   for money because they won't make as much. Well, maybe they do. I mean, they were instrumental
[00:47:59.440 --> 00:48:03.680]   behind this Prop 24 when that was one of the big things that people didn't like about Prop 24
[00:48:03.680 --> 00:48:09.840]   is the pay for privacy clause. So say it, they were kind of squeezed into that. They would rather it be
[00:48:09.840 --> 00:48:14.400]   the way that they're doing it now. I don't want to guess what Facebook would prefer. But
[00:48:15.120 --> 00:48:18.800]   I don't think it's reasonable to say, well, we shouldn't give them anything for the service we
[00:48:18.800 --> 00:48:22.960]   get from them. I think it's fine to give them. They can also, they can also give them anything
[00:48:22.960 --> 00:48:28.160]   because I don't use it. But it's not. I mean, just because they're not. If it's the only place you
[00:48:28.160 --> 00:48:32.400]   can get that soup doesn't mean I can steal it because I can't get anywhere else. Yeah, I don't
[00:48:32.400 --> 00:48:38.240]   think anyone's saying that steal it. So we're going back to the whole, you know, to the 90s internet and
[00:48:38.240 --> 00:48:44.480]   does information want to be free? This was like the big debate. And by the way, that was the biggest
[00:48:44.480 --> 00:48:50.720]   myth of the hacker manifesto was, it's for me. Maybe you wanted that way. But
[00:48:50.720 --> 00:48:55.840]   apparently they were wrong on that one. Yeah. So, so, so, you know, and I don't think that that
[00:48:55.840 --> 00:49:02.160]   the commercial internet has ever really come up with a good way to move beyond that. Some things
[00:49:02.160 --> 00:49:07.200]   on the internet are free. Some you have to sign up and you give them your email, you know, Facebook
[00:49:07.200 --> 00:49:13.200]   or whatever. Some things you do have to pay for. There are certain email services you can pay for.
[00:49:13.200 --> 00:49:19.760]   And I think that that if Facebook weren't a monopoly, if there were other options, if Instagram was
[00:49:19.760 --> 00:49:28.400]   its own entity, if WhatsApp was its own entity, you know, there would be much more diversity in what
[00:49:28.400 --> 00:49:33.520]   you can do. Oh, that's that's cool. And there would be, you know, I'm like, if you're a parent and you
[00:49:33.520 --> 00:49:38.480]   don't want your kids photos plastered all over Facebook, but you do want to share them with a group,
[00:49:38.480 --> 00:49:44.080]   is that is there a social network that you can pay so, you know, so that you are, you know,
[00:49:44.080 --> 00:49:50.000]   for a nominal fee, you're able to share photos and details and whatever and not worry about getting,
[00:49:50.000 --> 00:49:55.840]   you know, having your toddler data mind. You know, I think that that's a really, those kinds of
[00:49:55.840 --> 00:50:01.040]   questions are really interesting. We haven't seen effective answers to them. We've seen a lot of
[00:50:01.040 --> 00:50:06.880]   sort of plays and stabs, but nothing is really stuck. The hacker manifesto information
[00:50:07.600 --> 00:50:12.400]   wants to be free. I think what we've learned is that the gatekeepers want to keep the gate.
[00:50:12.400 --> 00:50:17.360]   Because we thought in those days, we thought, Oh, no more gatekeepers. You're going to anybody
[00:50:17.360 --> 00:50:22.640]   wants to put up a blog or a video or what anybody can do anything. There's no more networks.
[00:50:22.640 --> 00:50:27.600]   There's no more newspapers. There's no more gatekeepers. And I think what we've learned over time
[00:50:27.600 --> 00:50:35.360]   is for whatever reason, the gatekeepers end up keeping the gate. YouTube now is run by Google.
[00:50:35.360 --> 00:50:41.040]   And yeah, you get to put videos up for free, but Google still gets to pick who is the winner and
[00:50:41.040 --> 00:50:46.720]   who's the loser. I feel like the gatekeepers, how about Jason, what do you think? This is my,
[00:50:46.720 --> 00:50:50.880]   I was thinking about this this morning. This is my new thesis that for some reason,
[00:50:50.880 --> 00:50:55.520]   we never, we didn't get rid of gatekeepers. We thought we would with the internet, but we didn't
[00:50:55.520 --> 00:51:01.040]   get rid of them. Well, you know, Ben Thompson, who does trajectory talks a lot about aggregation
[00:51:01.040 --> 00:51:05.120]   theory, right? And like you either want to be on one end of the funnel or the other end of the funnel.
[00:51:05.120 --> 00:51:12.480]   And you can see it like having the enormity of YouTube or Facebook, like that is so powerful.
[00:51:12.480 --> 00:51:19.040]   And it is very hard to not end up in situations like that unless you have regulation. This is the
[00:51:19.040 --> 00:51:23.440]   argument for regulation. And I'm not a big, like let's regulate everything kind of person,
[00:51:23.440 --> 00:51:27.440]   but there is a way to which you have to regulate what happens.
[00:51:27.440 --> 00:51:28.240]   Just replacing.
[00:51:28.240 --> 00:51:29.920]   You will just go to a monopoly.
[00:51:29.920 --> 00:51:35.520]   And that's just replacing one gatekeeper with another, the government. It's what's, it's depressing
[00:51:35.520 --> 00:51:38.960]   to me because what I think we all thought with the internet, there's this was this great hope
[00:51:38.960 --> 00:51:42.640]   that would be the ultimate democratic institution that everybody would get a voice.
[00:51:42.640 --> 00:51:47.440]   And what it ended up happening. And yeah, Ben tall calls them aggregators,
[00:51:47.440 --> 00:51:51.120]   but really the aggregators are no different really than the old gatekeepers. I mean, the New York
[00:51:51.120 --> 00:51:56.800]   Times was, I guess, an aggregator of stories written by its journalists. You know, but it was
[00:51:56.800 --> 00:52:01.920]   a gatekeeper. If you couldn't get into that, you know, if when I started in radio, if you couldn't
[00:52:01.920 --> 00:52:06.800]   convince one of a handful of radio stations to, to give you a job, you didn't get on the air. Now
[00:52:06.800 --> 00:52:10.720]   anybody can have a podcast. It's not quite a gatekeeper in the sense that I can post things on YouTube
[00:52:10.720 --> 00:52:16.240]   and, and YouTube is happy to let me do that. We'll see it. If I mail, well, yeah, but if I
[00:52:16.240 --> 00:52:19.360]   mailed it to the New York Times, they're gonna be like, no, no, no, no, I'm not a crackpot. I'm
[00:52:19.360 --> 00:52:24.560]   not gonna put this on. But, but they still contain this enormous amount of power, especially in terms
[00:52:24.560 --> 00:52:28.480]   of reach, right? Because they, you know, YouTube or Facebook reaches this huge audience and they're
[00:52:28.480 --> 00:52:34.400]   sort of the only game in town or one of the only games in town. Right. So my, my, this wonderful idea
[00:52:34.400 --> 00:52:40.560]   that anybody can get into radio and do a podcast is true, but no one, but without the gatekeepers,
[00:52:40.560 --> 00:52:46.960]   without Apple's iTunes and iHeart and the various directories, no one's gonna know you're doing the
[00:52:46.960 --> 00:52:51.760]   podcast. You're talking to thin air. It's a little disappointing to me, but I think it's the way
[00:52:51.760 --> 00:52:57.120]   the world is. It's interesting how we trend in that direction, no matter what. I don't,
[00:52:57.120 --> 00:53:02.480]   anyway, I have to take a break because I got to pay for this. I can't do this for free. This,
[00:53:02.480 --> 00:53:07.920]   all this, all these bows of Holly, they don't, they don't come cheap. They don't come cheap.
[00:53:07.920 --> 00:53:13.760]   I can't go and say, Hey, Holly store, give me Holly, give it to me for free. I want it for free.
[00:53:13.760 --> 00:53:18.160]   No, they make me pay for it. So I'm gonna make you pay right now. Listen to this ad.
[00:53:20.560 --> 00:53:24.320]   And if you don't like it, go listen to some podcast without ads. Good luck to you.
[00:53:24.320 --> 00:53:29.600]   Arsh. You really depressed me with that, Seth. You can always listen to some of the
[00:53:29.600 --> 00:53:34.480]   ads. I do that. I do that. Just depressed the hell. Every, every time you have me on,
[00:53:34.480 --> 00:53:38.160]   I just, you get really, you know, there are podcasts without ads. You know that, right?
[00:53:38.160 --> 00:53:45.280]   That's not what I said. I don't have a monopoly. They don't have us on it. So there's no one.
[00:53:45.280 --> 00:53:48.000]   Yeah, they might. There's no reason why you couldn't be on it.
[00:53:48.720 --> 00:53:54.960]   Our show today brought to you by WWT. This is actually a great company. About 10 years ago,
[00:53:54.960 --> 00:53:58.320]   they started building what they called the Advanced Technology Center. Lisa and I went out to visit
[00:53:58.320 --> 00:54:04.960]   it earlier this year when you could still travel. We went to St. Louis. The ATC is amazing. Started
[00:54:04.960 --> 00:54:10.320]   in one little building, one little rack of servers. And now it's mile after mile of racks in many
[00:54:10.320 --> 00:54:16.080]   buildings, half a billion dollars in equipment. What they've done, because their consultants,
[00:54:16.080 --> 00:54:22.640]   their experts, they help big companies buy and use enterprise technology. What they've done is they've
[00:54:22.640 --> 00:54:29.200]   just aggregated all of the biggest and best technologies from the hundreds of OEMs and key
[00:54:29.200 --> 00:54:35.760]   partners like Heavyweights like Dell Technologies and VMware and Intel. But and also the little guys,
[00:54:35.760 --> 00:54:40.240]   the emerging disruptors, companies like Equinix, they put it all in this ATC.
[00:54:41.280 --> 00:54:46.400]   Their engineers, it's first started up, I think mostly because their engineers wanted to work
[00:54:46.400 --> 00:54:52.560]   in these labs to beta test new equipment, to build proofs of concept and reference architectures
[00:54:52.560 --> 00:54:58.480]   to set up integrations for their customers to make decisions and see results faster.
[00:54:58.480 --> 00:55:04.000]   But they realized there's so much value in having this ATC that they should open it up to customers
[00:55:04.000 --> 00:55:10.160]   as well. And this is what this is what makes WWT such an interesting company. They really are
[00:55:10.160 --> 00:55:15.600]   a partner. The people stay with over over decades. Many of their customers have been with them since
[00:55:15.600 --> 00:55:19.360]   the beginning because they know they can go to WWT to get the answers. They need to make sure
[00:55:19.360 --> 00:55:25.280]   their business runs right in every possible arena. So as an example, they have at the Advanced
[00:55:25.280 --> 00:55:31.200]   Technology Center, on demand, schedulable labs, you as a customer can use for things like
[00:55:31.200 --> 00:55:38.000]   Dell's VxRail or PowerStore, Unity or PowerMax, Data Protection Central and IDPA.
[00:55:38.560 --> 00:55:45.120]   These are, if you're looking at storage, these labs have the latest, greatest and primary storage.
[00:55:45.120 --> 00:55:50.240]   And there's hundreds of labs there covering everything from networking, secondary storage,
[00:55:50.240 --> 00:55:58.880]   data analysis, AI. You can learn so much just as the engineers do about the products before you
[00:55:58.880 --> 00:56:07.680]   launch them. You can use the ATC lab environment to make sure you're getting the right solution that
[00:56:07.680 --> 00:56:12.400]   solves the problem you're trying to solve. And frankly, the ATC reduces concept time for months
[00:56:12.400 --> 00:56:18.320]   to weeks. It really improves speed to market. This lab as a service is exactly what makes WWT
[00:56:18.320 --> 00:56:22.080]   unique. They could say, "Oh, no, no. We built this. We'll keep it to ourselves." No, they let their
[00:56:22.080 --> 00:56:27.280]   customers do their own programmatic testing using this vast half a billion dollar technology
[00:56:27.280 --> 00:56:31.360]   ecosystem that they've built. And they're always building. They're always adding more. Something new
[00:56:31.360 --> 00:56:35.760]   comes along they added. And it's completely virtual. You don't have to go to a St. Louis like we did.
[00:56:35.760 --> 00:56:43.760]   You can use the ATC anywhere in the world 24/7. They launched this last year, their digital platform
[00:56:43.760 --> 00:56:52.400]   encompassing the entire ATC ecosystem. It's a multiplier of knowledge and speed and agility
[00:56:52.400 --> 00:56:57.360]   for their customers and their partners anytime, anywhere in the world. Not just the labs,
[00:56:57.360 --> 00:57:01.440]   you get access to articles, case studies, all the tools you need to make a difference in today's
[00:57:01.440 --> 00:57:05.760]   fast-paced world. You want to learn more? Here's what you do. You can actually sign up and start
[00:57:05.760 --> 00:57:12.320]   using the labs right now. www.t.com/twit. Learn more about worldwide technology. Their advanced
[00:57:12.320 --> 00:57:16.960]   technology center become a member of their growing community. It's a place where people go to get
[00:57:16.960 --> 00:57:23.840]   answers to learn www.t.com/twit. You can create and account and gain access to their on-demand labs
[00:57:23.840 --> 00:57:31.920]   right now. Worldwide technology simplifies the complex. www.com/twit. Worldwide technology
[00:57:31.920 --> 00:57:36.400]   delivering business and technology outcomes around the world with Thank You, WWT for
[00:57:36.400 --> 00:57:43.600]   your supportive twit. All year long. And you can support Twit as a listener by going to that site
[00:57:43.600 --> 00:57:50.560]   www.t.com/twit. That was a great conversation. Thank you.
[00:57:51.760 --> 00:57:55.200]   Kind of went places I didn't expect it to go. Very interesting.
[00:57:55.200 --> 00:58:03.200]   I just started talking about Slack. See? You never know. We got a great panel here. Seth Rosenblatt
[00:58:03.200 --> 00:58:12.240]   is a security guru from the dash parallax.com. Don't forget the dash at Seth R on the story.
[00:58:12.240 --> 00:58:15.200]   Still waiting for someone to buy me that full URL.
[00:58:17.360 --> 00:58:23.040]   Somebody's got it. I know. I want Leo.com. I can't get it. It's gone. Some squatters stole it.
[00:58:23.040 --> 00:58:28.640]   I can never get it. It was the Bank of Canada for a long time. I guess that was their mascot.
[00:58:28.640 --> 00:58:33.760]   And then they let it go. It's so weird. I didn't even have lions up there.
[00:58:33.760 --> 00:58:40.080]   They did. RBC's mascot was a lion. Leo the lion. I think that was the whole idea. I don't know.
[00:58:40.080 --> 00:58:43.440]   Do you remember that, Georgia? Georgia, Dows and Marterial? Yeah.
[00:58:43.440 --> 00:58:49.200]   You still have the little emblem. Yeah, it's on the logo. Yeah. Yeah. Anxiety dash videos
[00:58:49.200 --> 00:58:53.440]   where she sells with her partner videos. Not just about anxiety, but being a better parent.
[00:58:53.440 --> 00:59:00.720]   Sleeping better. Georgia has also been very generous. Last few times you've been on during
[00:59:00.720 --> 00:59:08.080]   COVID-19. She's a licensed psychotherapist and she gives out her email. So even if you're not in
[00:59:08.080 --> 00:59:14.080]   Montreal, you can email Georgia@westmounttherapy.com. People doing that?
[00:59:14.080 --> 00:59:19.760]   They do. They do. I can't give therapy online unless you're actually a client, but I can help out or
[00:59:19.760 --> 00:59:25.440]   help out how to find therapy or what makes for good therapy. Some people are like, no, we'll
[00:59:25.440 --> 00:59:31.120]   do different things. There's lots of options out there. Jason Snell is also here from The
[00:59:31.120 --> 00:59:37.600]   Uncomparable. Yes, indeed. I'm drinking out of my uncomfortable. Uncomfortable. The greatest
[00:59:37.600 --> 00:59:42.880]   podcast network ever. Yeah. And six colors.com where I had to pay like four or five grand to get
[00:59:42.880 --> 00:59:47.440]   it without the dash. You're kidding. You had a buy there for somebody? I did. I did. It was
[00:59:47.440 --> 00:59:52.080]   worth every penny. You didn't want six colors. Pizza? Because I bet you could have gotten that.
[00:59:52.080 --> 00:59:56.240]   I probably could have. I learned a long time ago when I had an un-unspellable
[00:59:56.240 --> 01:00:01.680]   domain that was not a dot com that everybody wants something you can spell that's a dot com. So I
[01:00:01.680 --> 01:00:07.360]   just paid the money and I'm happy for it. Yeah. I also do have the colors with you just for Georgia.
[01:00:07.440 --> 01:00:08.880]   Oh, do you? Thanks.
[01:00:08.880 --> 01:00:10.240]   Colors with you. Sure.
[01:00:10.240 --> 01:00:10.880]   Fancy.
[01:00:10.880 --> 01:00:15.600]   Canada needs to be represented. Aren't you fancy? That was a lot cheaper.
[01:00:15.600 --> 01:00:20.160]   We go there and you didn't know this, but it was aptly named because you become famous for the
[01:00:20.160 --> 01:00:25.840]   quarterly charts you do on the Apple results. They're so useful. They're very colorful because
[01:00:25.840 --> 01:00:29.600]   I use those colors. You got it. It's in the name. All six of them. Every one of them.
[01:00:29.600 --> 01:00:37.040]   Every tie check. I count. Also, great podcast there. I imagine people around you and Dan
[01:00:37.040 --> 01:00:42.480]   Morin and people around you are very excited about the M1. I've come and gone on this. When Apple
[01:00:42.480 --> 01:00:47.760]   announced it, I was over the moon as people will know and everybody said, "Come down. Calm down."
[01:00:47.760 --> 01:00:54.000]   And I said, "No, you don't understand. This is the greatest thing." And then I got mine and I
[01:00:54.000 --> 01:00:59.360]   calmed down because as fast as it is, it wouldn't run some of the things that I really wanted.
[01:00:59.360 --> 01:01:02.320]   Mostly geeky edge stuff like e-max stuff.
[01:01:02.320 --> 01:01:05.520]   Yeah. That's it. Yeah. And did at Homebrew where you're building a lot of stuff.
[01:01:06.160 --> 01:01:09.840]   It's not there yet for Apple Silicon. I think that's part of the charm of these
[01:01:09.840 --> 01:01:14.880]   max though is that they're still max and recognizable. And if most of the stuff you do,
[01:01:14.880 --> 01:01:20.000]   they just work. And so even though it's a lot faster, the speed you can get for that $9.99
[01:01:20.000 --> 01:01:26.480]   MacBook Air is mind-blowing. It is still just a Mac doing Mac stuff. But it is the most exciting
[01:01:26.480 --> 01:01:31.280]   thing to happen, I think, to the Mac in years. So we are loving it.
[01:01:31.280 --> 01:01:35.760]   And I think you could argue it might be the most exciting thing to happen to desktop computing in
[01:01:35.760 --> 01:01:42.320]   years. And maybe not so much the M1, although I have to say, I never used Safari for a long time.
[01:01:42.320 --> 01:01:47.280]   I was just Firefox. I don't like Chrome. It's a pig. And I use Firefox everywhere, Windows,
[01:01:47.280 --> 01:01:53.280]   Mac and Linux. So it was convenient to sync it up. But I'm only using Safari on the M1 MacBook Pro
[01:01:53.280 --> 01:01:59.520]   because, oh my God, I didn't know the web could be that fast. I thought it was about bandwidth.
[01:02:00.160 --> 01:02:03.920]   But apparently after you get the website, there's a certain amount of churning that is to go to
[01:02:03.920 --> 01:02:09.440]   generate it. I open Twit.tv because Lisa's been using my old MacBook Air and I said,
[01:02:09.440 --> 01:02:13.040]   oh, Lisa, you're going to want this. She said, no, I'm not. I said, Lisa, look, come here,
[01:02:13.040 --> 01:02:18.080]   I'm going to load for the first time ever Twit.tv on this site. It came up literally
[01:02:18.080 --> 01:02:25.680]   the whole thing like that. And it was like mind-boggling. They're definitely doing something
[01:02:27.040 --> 01:02:31.120]   pretty amazing. There's a lot of stuff going on. There was a back and forth on Twitter the other
[01:02:31.120 --> 01:02:37.360]   week that was a cranky reviewer basically saying Apple has cheated by making JavaScript so fast,
[01:02:37.360 --> 01:02:42.640]   which I mean, it's faster. That's not cheap. It's actually faster. But it's interesting to see
[01:02:42.640 --> 01:02:48.160]   when you control your own operating system and you built your own chip, you can prioritize certain
[01:02:48.160 --> 01:02:54.880]   kinds of tasks. So like I can edit 4K video on that MacBook Air, the 999 MacBook Air, and it is
[01:02:54.880 --> 01:03:00.000]   way better at it, way more capable than on a $5,000 iMac Pro from three years ago.
[01:03:00.000 --> 01:03:05.520]   And why is that? It's because Apple's got a lot of video and code decode stuff that it built in
[01:03:05.520 --> 01:03:11.680]   because it knows people edit video on their platforms. But that's why it's great. That's why
[01:03:11.680 --> 01:03:16.880]   having the hardware and the software together works because you can make a purpose built chip.
[01:03:16.880 --> 01:03:21.520]   Now I'm going to say though, so one of the tools that I really love and use, and it's very much
[01:03:21.520 --> 01:03:26.640]   an edge case is it's a programming environment and language called Racket. And they have a IDE
[01:03:26.640 --> 01:03:32.640]   called Dr. Racket and it didn't work. One of the guys, Matthew Flat, spent some energy, didn't take
[01:03:32.640 --> 01:03:37.120]   him that long. Three days after he got his M1 Mac, he said, okay, I've made it work. And it
[01:03:37.120 --> 01:03:43.200]   compiles and works. And now I guarantee you that Apple did not optimize for Dr. Racket. It's a
[01:03:43.200 --> 01:03:47.840]   scheme like language. It's like, you know, I mean, I know they did not have that thing. I won't,
[01:03:47.840 --> 01:03:54.080]   I can't use my iMac Pro with 64 gigs of RAM and 10 cores anymore because it feels like a pig.
[01:03:54.080 --> 01:04:01.200]   So it is faster across the board, even in things Apple didn't contemplate. There's a
[01:04:01.200 --> 01:04:07.360]   there's a Lisp that I use SBCL steel bank common Lisp that is somebody asked one of the maintainers,
[01:04:07.360 --> 01:04:11.200]   are you going to do it? Are you going to update it? Because it won't run at all. There's stuff
[01:04:11.200 --> 01:04:15.520]   that literally won't even run on Rosetta just won't run. And he said, no, I don't have an M1 Mac.
[01:04:15.520 --> 01:04:20.640]   So I said, look, let me buy you one because I want this. And they put up the fundraiser. He's
[01:04:20.640 --> 01:04:24.960]   actually raising, he's raised, I think, enough money now to buy at least a Mac mini. Eventually,
[01:04:24.960 --> 01:04:30.720]   he's going to it's on it's on Reddit. So he put up a fundraiser for it. Thank goodness, because if
[01:04:30.720 --> 01:04:35.760]   people spend a little time, it's not it's it's little tweaks on some of its big security tweaks.
[01:04:35.760 --> 01:04:41.360]   It's not even M1. So if you rely on that app, that's $700 well spent. Absolutely.
[01:04:41.360 --> 01:04:47.040]   Right? Absolutely. So my question for you, Leo, is what does Microsoft, what does Qualcomm,
[01:04:47.040 --> 01:04:52.960]   you know, what is the response? Is this enough of a game changer in terms of price, you know,
[01:04:52.960 --> 01:04:59.840]   and power and and energy usage for the PC side to look at this and say, we have to find a way
[01:04:59.840 --> 01:05:03.920]   to be more integrated than we are right now. Or is it just not a big deal? Right now, what I
[01:05:03.920 --> 01:05:10.160]   see from PC people for the most part is yeah, tell me when you can run a radion GTX
[01:05:11.040 --> 01:05:16.640]   630 or well, sure. Yeah, tell me when Far Cry looks good on it, but they're missing the point.
[01:05:16.640 --> 01:05:20.640]   And I don't think it's the pressures on quite yet. Here's what I see, Jason. I think it's going to
[01:05:20.640 --> 01:05:26.000]   be very interesting. This M1 is really, really, really fast, but this is the first one. Yeah,
[01:05:26.000 --> 01:05:31.360]   for low end chip, it's low end chip on the low end next year and the year after. What about a
[01:05:31.360 --> 01:05:38.560]   Mac pro with that has 64 cores, you know, 32 high efficiency 32 lower, lower efficient, you know,
[01:05:38.560 --> 01:05:47.360]   high blow power. And I don't know, 64 core GPU built into the system on a chip with 64 gigs of RAM
[01:05:47.360 --> 01:05:51.040]   of unified memory architecture. But when did that when that comes out in a year or two,
[01:05:51.040 --> 01:05:58.880]   then what is the PC industry going to say? The pressure will mount. And I suspect I actually told
[01:05:58.880 --> 01:06:03.760]   people on the radio show, this might be jumping the gun, I said, buy a Mac mini and start learning
[01:06:03.760 --> 01:06:08.480]   Mac OS, I know you hate Mac OS, I know you hate Apple, I know you hate it. But trust me,
[01:06:08.480 --> 01:06:14.560]   in a year, you'll wish you were using it. Because I don't think Microsoft and Qualcomm can compete.
[01:06:14.560 --> 01:06:18.480]   Yeah, if I'm Microsoft, I do have to start thinking about like, is there a way for us to
[01:06:18.480 --> 01:06:22.960]   how do we be more Apple like? Because I think they've been playing that game for a while now,
[01:06:22.960 --> 01:06:27.440]   and it's really served them pretty well to be like, what are the secrets to Apple having its own
[01:06:27.440 --> 01:06:31.760]   hardware with its own software? And we've seen that with the surface line. But the challenge now is
[01:06:31.760 --> 01:06:39.200]   that it's Intel or Qualcomm essentially making these chips that they can shop for,
[01:06:39.200 --> 01:06:43.040]   and it's not integrated, and they're not able to take advantage like Apple has.
[01:06:43.040 --> 01:06:44.720]   Exactly. I don't know where it goes from there.
[01:06:44.720 --> 01:06:51.120]   It has to be Microsoft, but Microsoft's never going to do it because they have all these other
[01:06:51.120 --> 01:06:57.440]   companies that make Windows computers, and they can't screw them. So the problem, it's the
[01:06:57.440 --> 01:07:01.280]   innovator's dilemma kind of the problem is you can't cannibalize your existing business.
[01:07:01.280 --> 01:07:04.000]   Only Apple's business sucked so bad that they could do that.
[01:07:04.000 --> 01:07:10.880]   That's not actually fair, but they were small enough, and they made enough money on the iPhone
[01:07:10.880 --> 01:07:15.680]   that they didn't mind disrupting the Macintosh ecosystem.
[01:07:15.680 --> 01:07:18.640]   And they figured that their users will come with them. They always figured that,
[01:07:18.640 --> 01:07:22.800]   which is why they've done like three or four chip transitions, and they did the OS 10 transition,
[01:07:22.800 --> 01:07:25.840]   and they just figured that they're going to come along for the ride, and they generally do.
[01:07:26.400 --> 01:07:31.920]   They also didn't need Mac to be. It was a small part of their business.
[01:07:31.920 --> 01:07:38.000]   It's a little wedge on your graph. So they can afford to cannibalize Mac a little bit,
[01:07:38.000 --> 01:07:43.360]   and I'd be honest, I think anybody, look, I have a, that's a $6,000 iMac Pro
[01:07:43.360 --> 01:07:49.840]   that I'm not going to use much anymore. I don't think people understand how much
[01:07:49.840 --> 01:07:55.360]   this is going to disrupt the Intel Mac world. It's a different game.
[01:07:55.360 --> 01:07:59.440]   I mean, it's almost already over for people who are looking at this from the other side and
[01:07:59.440 --> 01:08:04.240]   saying, you know, how fast are these low end chips? The only Macs that are running Intel that
[01:08:04.240 --> 01:08:11.040]   are faster than it are a couple of i9 configured to order iMacs at the very, very high end,
[01:08:11.040 --> 01:08:16.240]   the most expensive iMacs you can buy, and the iMac Pro and Mac Pro with the Xeon processor.
[01:08:16.240 --> 01:08:22.080]   That's it. So like the every other Mac, every other iMac, every other MacBook Pro, they're all
[01:08:22.080 --> 01:08:29.440]   slower than a 999 MacBook or a 699 Mac mini, which, you know, if you extrapolate,
[01:08:29.440 --> 01:08:33.280]   what's the next chip going to be like and what else is Apple going to do? But it's starting from
[01:08:33.280 --> 01:08:38.720]   this position. These are the weakest Apple Silicon chips that will ever be in Macs, and they're already
[01:08:38.720 --> 01:08:45.440]   like just dramatically more powerful. Yeah, and a PC people hate it when we say this,
[01:08:45.440 --> 01:08:50.560]   because it's just, they go, oh, no, don't worry, AMD and Intel can do this. I feel like there may be,
[01:08:50.560 --> 01:08:55.760]   maybe they'll try, but I don't know. I don't know. This is some pretty secret magic secret sauce
[01:08:55.760 --> 01:09:01.040]   they got. Apple's got going on, and it's not like this is the first time they've made an arm chip.
[01:09:01.040 --> 01:09:07.920]   They've been doing this as 2007. They kind of know what they're doing. So I don't think there's
[01:09:07.920 --> 01:09:15.120]   pressure yet. The pressure will mount. And over the next year or two, people have no, I mean,
[01:09:15.120 --> 01:09:20.320]   the potential for this chip is mind boggling. And as soon as I can get, you know, the things that
[01:09:20.320 --> 01:09:25.760]   I need, I found an e-max, a development version of e-max that runs native. Once you get things
[01:09:25.760 --> 01:09:29.920]   running native, the problem is a lot of stuff doesn't even run on a Rosetta, like some of the
[01:09:29.920 --> 01:09:34.000]   weird stuff doesn't even run. We're just starting to see it all. There are a couple of guys who took
[01:09:34.000 --> 01:09:38.880]   QEMU, the open source emulator, and they got Windows on ARM running on it, and it runs like
[01:09:38.880 --> 01:09:47.520]   WysusFast is on a certain X. Which isn't saying much. The surface X is awful. Yeah, it's not
[01:09:47.520 --> 01:09:51.600]   great. But the idea here that this hardware that is running in a virtual machine on Apple
[01:09:51.600 --> 01:09:56.080]   silicon is still able to put it out like that. I think that's a good example where I think it's
[01:09:56.080 --> 01:10:02.480]   really Microsoft. Windows ARM is not a product, right? It's only bundled with hardware, and now
[01:10:02.480 --> 01:10:06.800]   there's another place it could potentially run, and I imagine it will. I mean, I wrote a thing
[01:10:06.800 --> 01:10:10.480]   I think this week basically said it's going to happen. It's just a matter of when they're ready,
[01:10:10.480 --> 01:10:14.720]   but it's clearly possible for that stuff to happen. So that'll all get there. But yeah,
[01:10:14.720 --> 01:10:19.600]   it is a chip transition. As boring as it is, there's still going to be some things that don't work,
[01:10:19.600 --> 01:10:24.400]   and there are still some reasons to use an Intel Mac if you're on one, because it all works,
[01:10:24.400 --> 01:10:30.320]   and that platform has been there for 15 years. Anyway, I know Windows people hate me when I
[01:10:30.320 --> 01:10:41.840]   say this. And you know what? I'll make up for it. And meanwhile, in Italy, Apple's in big trouble.
[01:10:41.840 --> 01:10:48.480]   This will make you feel better. $12 million fine, which is basically the cigarette money
[01:10:48.480 --> 01:10:56.560]   in Tim Cook's couch. But it's there. I think they're absolutely right. It's for misleading
[01:10:56.560 --> 01:11:03.360]   iPhone water resistance claims in the Italian watchdog cited this famous, I won't play the
[01:11:03.360 --> 01:11:08.480]   audio, but the famous ad where they have an iPhone 12 in a test chamber, and they do all sorts of
[01:11:08.480 --> 01:11:15.600]   miserable things to including dumping a ton of water on it. And it turns out that the iPhone,
[01:11:15.600 --> 01:11:21.760]   you can only call the iPhone 11 and 12 waterproof. If in specific lab conditions with the use of
[01:11:21.760 --> 01:11:31.120]   static, pure distilled water, and you can't dump a wedding cake on it and make it rain.
[01:11:31.120 --> 01:11:35.360]   But furthermore, and I think this was really the big issue, was Apple,
[01:11:35.360 --> 01:11:40.560]   there weren't, and I know this because I had it happen to me, they will not warranty water damage.
[01:11:40.560 --> 01:11:45.200]   Yeah. So they advertise water resistant, but then they don't get it wet because if it breaks,
[01:11:45.200 --> 01:11:49.280]   you're right. No, it's outrageous. It's outrageous that they advertise water and dust resistance.
[01:11:49.280 --> 01:11:56.960]   But if you drop your phone in the toilet or in the pool and it dies because the water entered
[01:11:56.960 --> 01:12:02.240]   and broke something, they won't cover it. So what they basically say, I've asked them about this
[01:12:02.240 --> 01:12:07.200]   time and again, and what they say is, we're making it less likely that when you drop it in the water
[01:12:07.200 --> 01:12:11.840]   that you'll need to replace it. But if you drop it and you need to replace it, you got to pay.
[01:12:11.840 --> 01:12:18.240]   And I think it's, I don't like that, but I really hate it when they then roll out an ad campaign
[01:12:18.240 --> 01:12:24.000]   like they did last year for the iPhone 11 series where they're showing it be impervious to all
[01:12:24.000 --> 01:12:29.360]   these things because the implication there I would say is that it's safe for you to drop it in the
[01:12:29.360 --> 01:12:35.920]   water and that if it fails, that's Apple's failure. And Apple saying, no, if it fails, it's your problem.
[01:12:35.920 --> 01:12:42.000]   And it's ridiculous. Yeah. And everyone's probably dropped their phone in the toilet, in the sink,
[01:12:42.640 --> 01:12:49.040]   in the pool, like it just happens, right? So we can't see that it's, you know, Georgia,
[01:12:49.040 --> 01:12:54.880]   you're your former colleague and mine, Serenity Caldwell, I mean, she lost a phone in a swimming
[01:12:54.880 --> 01:13:00.400]   pool taking some video underwater. And it's not unreasonable at all to think if this is a waterproof
[01:13:00.400 --> 01:13:06.400]   phone and I want to take video underwater, I can do that. Yeah. And I don't have the same thing.
[01:13:06.400 --> 01:13:10.800]   I don't do it. I've done the same thing. I've done the same thing. Like we used to do it just for
[01:13:10.800 --> 01:13:17.280]   tests for fun. But even Renee, he has a whole bunch of like, underwater shots that he took with the
[01:13:17.280 --> 01:13:23.200]   phone. And I, you know, after that, I'm like blowing all of like trying to get water out. I'm
[01:13:23.200 --> 01:13:28.080]   like hoping that this is not going to be the phone that that, you know, this one's the time that it
[01:13:28.080 --> 01:13:34.080]   dies, but it happens so often. So yeah. Good sound in our chat room says you missed the disclaimer at
[01:13:34.080 --> 01:13:37.440]   the bottom of the video that says professional iPhone user on a closed pool do not attempt.
[01:13:40.240 --> 01:13:43.200]   Yeah. Professionals only. Yeah. Distilled water only to still water.
[01:13:43.200 --> 01:13:51.280]   Anyway, so 12 million no big deal. And I'm sure Apple just is fine, fine, whatever. But I hope that
[01:13:51.280 --> 01:13:57.520]   that raises some awareness in the fruit company a little bit. I mean, I will say having tested
[01:13:57.520 --> 01:14:02.640]   these things, they have done a lot lately in the last like five years of making the iPhone more
[01:14:02.640 --> 01:14:10.400]   water resistant and dust resistant and shatterproof, it still will break. But I think as an iPhone
[01:14:10.400 --> 01:14:14.720]   user, it's really nice to know that if I drop it because I've done that, yeah, drop it in water.
[01:14:14.720 --> 01:14:21.040]   There's a good chance it'll be fine. And five years ago, that was not true. So that part of it is
[01:14:21.040 --> 01:14:26.720]   good. It's just making this a claim that I think is misleading that if it breaks, it's Apple's fault
[01:14:26.720 --> 01:14:35.280]   because they'll still blame you. Yeah. Yeah. Yeah. Let's see. So there, see, I gave you a bad Apple
[01:14:35.280 --> 01:14:40.640]   story. Make you feel better about the M one. Apple has announced their best of 2020. We're
[01:14:40.640 --> 01:14:48.720]   going to do a whole show on it on iOS today on Tuesday. The best products in the app store.
[01:14:51.920 --> 01:14:58.400]   I don't know what they are. Fantastic. Cal. Yes, I agree. Endel, which is a really cool relaxation
[01:14:58.400 --> 01:15:04.160]   and sleep app. I have not used that. It's really cool. Is it just like 4D sound that you're listening
[01:15:04.160 --> 01:15:08.960]   to? Yeah. But what's cool, it was the Apple. It was the Apple Watch app of the year because they
[01:15:08.960 --> 01:15:17.920]   have an Apple Watch version for it, which is kind of neat. Let's see. Gen Shen Impact. Don't know it.
[01:15:17.920 --> 01:15:23.120]   Also, Play Store Best Game Winner for iPhone Game of the Year. Legends of Runeterra,
[01:15:23.120 --> 01:15:30.000]   iPad Game of the Year, Disco Elysium. Back Game of the Year. Dondara, Trials of Fear,
[01:15:30.000 --> 01:15:36.160]   Apple TV Game of the Year. And for Apple Arcade, Sneaky Sasquatch, which is kind of fun.
[01:15:36.160 --> 01:15:43.360]   Okay. I guess it's cute. Yeah. You're a Sasquatch and you have to sneak around.
[01:15:46.000 --> 01:15:50.720]   All right. Just thought I mentioned it. No big deal. I don't understand it. Since we're in the
[01:15:50.720 --> 01:15:55.600]   Apple section, I don't understand this acquisition and why they're making a big deal about it. Former
[01:15:55.600 --> 01:16:01.520]   venture capitalistic gray lock partners. He led early investments in Discord and Musically
[01:16:01.520 --> 01:16:10.080]   hired away from gray lock, Josh Elman, to run the app store. Jason, you probably know why this is
[01:16:10.080 --> 01:16:15.920]   a big deal. Why would Apple hire this VC to run the app store? I don't understand why it's
[01:16:15.920 --> 01:16:20.400]   to work on the app store, right? Whatever that means and help customers, he says,
[01:16:20.400 --> 01:16:23.760]   discovered the best apps for them. Boy, there must be given a ton of money. I mean,
[01:16:23.760 --> 01:16:30.400]   if the guy's a VC at gray lock, what? Yeah. And what is his skill set? I mean,
[01:16:30.400 --> 01:16:35.920]   he was vice president product at Robinhood. So that was a wildly, there still is a wildly
[01:16:35.920 --> 01:16:43.440]   successful stock trading app. I mean, the best pitch I could do because I really am a little bit
[01:16:43.440 --> 01:16:50.000]   boggled by this is probably that Apple views app store revenue as important because it's part
[01:16:50.000 --> 01:16:56.160]   of their services revenue and that they view enhancing app store revenue by selling more apps
[01:16:56.160 --> 01:17:03.360]   as being super valuable. And this guy is going to help figure out how to get more apps in front
[01:17:03.360 --> 01:17:07.920]   of people and sell more apps and generate more app store revenue. And that's sort of his thing.
[01:17:07.920 --> 01:17:13.120]   But it's a weird one. Unless there's some behind the scenes like secret power of his that we're
[01:17:13.120 --> 01:17:18.240]   not aware of, but also to leave VC and go work for Apple is an interesting choice for him too.
[01:17:18.240 --> 01:17:22.000]   That's a that's got to be a real change and in pace and attitude.
[01:17:22.000 --> 01:17:26.480]   People must really want to work for Apple. I can't believe Lori Gill left Mac break weekly to work
[01:17:26.480 --> 01:17:32.000]   at Apple. I mean, that's just nuts. Serendi Caldwell left iMOR to work for Apple. What is wrong
[01:17:32.000 --> 01:17:38.800]   with people? Thank God, you're not working for Apple, Jason. Yeah, it's that's it. I'll tell you
[01:17:38.800 --> 01:17:43.520]   this. Apple is in a more stable industry than the media. Probably right. Yeah, but
[01:17:43.520 --> 01:17:48.960]   it's nice to work for yourself, Jason. It is. It is very much nice to work for yourself.
[01:17:48.960 --> 01:17:53.680]   They're waving the temptation wand over Jason as we speak. We asked Renee Richie, so when are they?
[01:17:53.680 --> 01:18:00.160]   When are you going to Apple? He wouldn't talk. He wouldn't say they've stolen so many of our
[01:18:00.160 --> 01:18:06.400]   hosts at this point. Stolen's not the right word enticed. Well, you work in the word way.
[01:18:06.400 --> 01:18:13.440]   It's a long way. It's nice to work for a company that is hiring and not doing rounds and rounds
[01:18:13.440 --> 01:18:19.680]   of layoffs. So I get it. But again, venture capital guy playing with money all day, rolling around
[01:18:19.680 --> 01:18:23.520]   and that's what they do. That's what PCs do is roll around the money all day. And now he's going
[01:18:23.520 --> 01:18:29.120]   to have to work for a company. So that'll be different. We feel long about last time.
[01:18:29.120 --> 01:18:34.800]   Apple did do and I want to give them kudos. They reduced the commission for anybody except Epic
[01:18:34.800 --> 01:18:41.600]   and Spotify to practically that's practically that's pretty much what they did. They're like,
[01:18:41.600 --> 01:18:48.240]   you know what? You're going to take us to court. I guess we're from 30% to 15%. If you make more
[01:18:48.240 --> 01:18:54.560]   than a million, you got to pay the 30%. Now the international book publishing association or
[01:18:54.560 --> 01:19:00.240]   apps or independent book publishing association says, Hey, what about books? You still charge 30%
[01:19:00.240 --> 01:19:08.000]   for Apple books. Let's let's lower the price there too. Is Apple doing this because
[01:19:08.000 --> 01:19:13.760]   they're really nice and they feel like they've overcharged people all this time? Or are they doing
[01:19:13.760 --> 01:19:20.320]   it more because they're looking at the antitrust action going on all around them? And the amount
[01:19:20.320 --> 01:19:24.240]   of time Tim Cook spent it on the hill and going, maybe better do something.
[01:19:25.440 --> 01:19:31.120]   It's such a small amount. It's not going to really affect them. These are for people that made the
[01:19:31.120 --> 01:19:37.360]   last year a million or less. So it's not going to be a huge dent for Apple to be able to. That
[01:19:37.360 --> 01:19:42.560]   causes a whole bunch of goodwill, a whole bunch of, we dealt with it and it does put a little bit
[01:19:42.560 --> 01:19:48.000]   of a pin into Epic. I'm sure that that made them. I love they should just say, unless you spotify
[01:19:48.000 --> 01:19:50.800]   our Epic, they should just say that. That would have been really funny.
[01:19:51.600 --> 01:19:56.480]   Yeah, I mean, it's a vast majority of their developers and a teeny tiny fraction of the money that
[01:19:56.480 --> 01:20:02.560]   passes through the app store. So it's a great PR move. And you can see in the anger of those guys,
[01:20:02.560 --> 01:20:09.680]   like David Hanamayar Hanson and Sweeney, right, you could see they were kind of rageful when this
[01:20:09.680 --> 01:20:14.000]   announcement happened. It was a good chess move. It was a good chess move.
[01:20:14.000 --> 01:20:16.560]   It's so right, Jason. You're so right. Good.
[01:20:16.560 --> 01:20:22.480]   Right? They were, especially when Tim was coming out, I'm doing this for the little guy. I'm fighting
[01:20:22.480 --> 01:20:27.040]   for those companies. And that's what we're going into it. And then they said, Oh, guess what?
[01:20:27.040 --> 01:20:30.240]   Good idea. We'll take care of the little guys. We just won't take care of you.
[01:20:30.240 --> 01:20:37.040]   So that's a very good point too. It didn't cost them that much because the bulk of the revenue
[01:20:37.040 --> 01:20:41.440]   from the app store, you think comes from people over a million bucks? That's really where the big
[01:20:41.440 --> 01:20:47.200]   bucks is for them. Yeah, it's the big whales who were doing lots of in-app purchase and stuff like
[01:20:47.200 --> 01:20:52.160]   that. That's where the bulk of the money comes from. So it is one of these things that has a big
[01:20:52.160 --> 01:20:56.960]   impact in terms of a happy story about a small business without it really cutting into Apple's
[01:20:56.960 --> 01:20:58.000]   bottom line very much. Yeah.
[01:20:58.000 --> 01:21:03.600]   Let me think.
[01:21:06.960 --> 01:21:14.480]   I think that we've done Apple. We're done on Apple. And there is rejoicing herd around the world.
[01:21:14.480 --> 01:21:21.120]   We will move on. That was our Apple segment. Can I pause it a question? I'm listening to this,
[01:21:21.120 --> 01:21:29.440]   and I'm not an Apple device user these days. No, actually, I care greatly. I'm not sure
[01:21:30.880 --> 01:21:39.520]   where Apple came up with its 30% number in the first place. And even 15% sounds to me to be
[01:21:39.520 --> 01:21:45.440]   very, very high for what they are doing. Actually, in their defense, 30 is not bad at all. I mean,
[01:21:45.440 --> 01:21:49.680]   if you were selling in the old days, if you're selling your software through Ingram or any of the
[01:21:49.680 --> 01:21:56.640]   big distributors, they take 50%. Sure. Microsoft and Sony both take 30% for game titles,
[01:21:56.640 --> 01:22:02.640]   sold in their platforms. 30 is kind of the going rate, I think. Are they doing that? Are they doing
[01:22:02.640 --> 01:22:09.360]   that because they because Apple started taking a 30% take from the iTunes start first? It's just,
[01:22:09.360 --> 01:22:16.160]   I think it's the standard. I think in 2008 as well, which was a very different period than it is now.
[01:22:16.160 --> 01:22:21.200]   Like Leo said, 30% was a number that maybe they pulled out of a hat, but you look around at other
[01:22:21.200 --> 01:22:27.040]   closed platforms and it didn't seem unreasonable. And at the in the moment, it seemed great. It
[01:22:27.040 --> 01:22:32.000]   actually seemed generous because in fact, Steve Jobs at one point said, "Or you can sell it for free
[01:22:32.000 --> 01:22:36.320]   and we won't charge you anything and we'll host the download and you don't have to pay anything.
[01:22:36.320 --> 01:22:41.840]   It's completely free if you want your app to be free." And at the time, that was like, wow,
[01:22:41.840 --> 01:22:47.520]   that's really generous of them. So it's just only over time. And they make some disingenuous
[01:22:47.520 --> 01:22:52.640]   arguments, right? Because it's not like you couldn't buy software online in 2008. You could. And
[01:22:52.640 --> 01:22:56.960]   for a lot, you're using Coggy or something like that. And you could, you could, you would share a
[01:22:56.960 --> 01:23:04.080]   lot less of the money with your processor than 30%. But it was still not unreasonable for the time
[01:23:04.080 --> 01:23:10.080]   looking at it as a closed platform. It's just that in 2020, it looks a little bit less generous.
[01:23:10.080 --> 01:23:15.440]   Right. Yeah. Yeah. And unlike Coggy, they did all the hosting, they did the promotion.
[01:23:16.240 --> 01:23:20.960]   You're getting a lot more than just, we'll help you with a financial transaction.
[01:23:20.960 --> 01:23:26.480]   I mean, being on the iPhone has been, I think, very lucrative for a lot of people.
[01:23:26.480 --> 01:23:32.480]   Yeah. So small developers will tell you that the 15% they're now going to take is certainly
[01:23:32.480 --> 01:23:36.080]   worth giving to Apple given what they get in return. Yeah, I think that's fair at this point.
[01:23:36.080 --> 01:23:39.200]   I think that's very fair. And if you're making a million dollars a year,
[01:23:39.200 --> 01:23:43.280]   the funny thing is, if you're making a million dollars a year, it's a lot of money you're giving
[01:23:43.280 --> 01:23:48.800]   an Apple that 15% is that might make people say, you know what, we're going to pull the app now.
[01:23:48.800 --> 01:23:53.920]   Yeah. Right. We're not, we're at 990. We're going to be like, let's pull the app for a little
[01:23:53.920 --> 01:23:59.040]   while. Let's get any more so that we don't get bumped up the next year to 30. Yeah. No kidding.
[01:23:59.040 --> 01:24:04.160]   All right. Let's take another break and then we will come back. And I've been putting this off
[01:24:04.160 --> 01:24:10.000]   because it, it's going to be a little bit painful. We could talk about the labor issues at Google.
[01:24:10.880 --> 01:24:12.880]   Yeah.
[01:24:12.880 --> 01:24:19.120]   For years working in broadcast media, I was told there's, there's two subjects. Nobody cares about
[01:24:19.120 --> 01:24:25.760]   court and labor. We're going to do both. We'll save the stuff. I saved the stuff. Nobody cares
[01:24:25.760 --> 01:24:30.000]   about that. That's coming up. It's great to have you guys though. Georgia Dow, wonderful to see you.
[01:24:30.000 --> 01:24:36.480]   Jason Stell, it's been way too long. Seth Rosenblatt, always a pleasure. I, for some reason, I assumed
[01:24:36.480 --> 01:24:39.840]   that you lived in Brooklyn, that you weren't a California voter. But now I remember you're in
[01:24:39.840 --> 01:24:44.480]   Silicon Valley, aren't you? I'm in San Francisco. Yeah. You deserve being in Brooklyn. You should
[01:24:44.480 --> 01:24:49.920]   have been in Brooklyn. Is that right? Hello. It's too, too damn cold. Yeah. What's the,
[01:24:49.920 --> 01:24:55.520]   is the, who's the beeping coming from? It's not for me, is it? That's, it's Seth, you're something
[01:24:55.520 --> 01:24:59.680]   of yours is beeping. Beeping. I'll let you check it while I'm not hearing it, but I'll,
[01:24:59.680 --> 01:25:03.040]   yeah, you do your thing and I'll take a look. Yeah. We hear a beeping coming from you.
[01:25:03.040 --> 01:25:07.200]   This episode of this week in Tech brought to you, there it is. You hear that? Yeah.
[01:25:07.200 --> 01:25:16.240]   Yeah. What's that? What is that? It's me? They're saying it's me. Oh yeah. Every time,
[01:25:16.240 --> 01:25:22.080]   every time they say Leo too, for some reasons, is it me beeping?
[01:25:22.080 --> 01:25:30.960]   It's not Seth. Seth, it's not you. It's coming from inside the house.
[01:25:32.480 --> 01:25:39.520]   Every time they say Leo too, it beeps. I don't know why. Why? Because I'm, I'm not Leo too. Am I?
[01:25:39.520 --> 01:25:45.040]   Yeah. I'm inside the Leo. Now the chat room has just gone crazy saying Leo too, Leo too, Leo too.
[01:25:45.040 --> 01:25:54.160]   Here, let me turn my sound up again. We'll see. Let's see. That's me. Say Leo too. Oh, see? Oh my
[01:25:54.160 --> 01:26:02.960]   goodness. Who's Leo too? When he's at home. I showed it. They brought to you by Barracuda.
[01:26:02.960 --> 01:26:12.400]   Barracuda, bing. Barracuda is the provider of client. I try to resist doing that every time,
[01:26:12.400 --> 01:26:17.840]   but I can't. It's, as, of course, everybody knows the name Barracuda. Cloud-enabled enterprise
[01:26:17.840 --> 01:26:23.120]   grade security solutions that protect everything you've got from email, the networks, the data,
[01:26:23.120 --> 01:26:29.360]   to applications. Let's talk about your email today. Barracuda's total email protection.
[01:26:29.360 --> 01:26:38.880]   91% of all cyber attacks start in email. That is the number one vector for getting spearfished
[01:26:38.880 --> 01:26:44.720]   and getting a count takeover and conversation hijacking and ransomware. We see the stories
[01:26:44.720 --> 01:26:50.880]   again and again. You know, Steve Gibson talks about it all the time. You got to know what's in
[01:26:50.880 --> 01:26:55.840]   your email. And that's what Barracuda is so good at. And remember, you've got now employees working
[01:26:55.840 --> 01:26:59.680]   at home where they don't, you know, they can't say, Hey, does this look right? This email?
[01:26:59.680 --> 01:27:05.600]   Did you just send me an email boss? They don't have the software protection? Well, they don't
[01:27:05.600 --> 01:27:12.240]   have Barracuda total email protection. The Barracuda researchers since January have noticed a spike
[01:27:12.240 --> 01:27:20.240]   of 667% in coronavirus related spearfishing. The spearfishers pose is the World Health
[01:27:20.240 --> 01:27:26.800]   Organization. They promise information. They give you a download or a PDF. And of course,
[01:27:26.800 --> 01:27:32.720]   it's not that it's it's ransomware. Get the protection you need for your company with Barracuda's
[01:27:32.720 --> 01:27:38.320]   total email protection. It's an all-in-one email security backup and archiving tool.
[01:27:38.320 --> 01:27:43.440]   It gives you AI based protection from spearfishing account takeover and business email
[01:27:43.440 --> 01:27:48.560]   compromise. You need that because, you know, the bad guys are not sitting still. They're constantly
[01:27:48.560 --> 01:27:53.680]   refining their attacks. You need protection that is constantly evolving to take care of that.
[01:27:53.680 --> 01:27:58.320]   You also get an automated incident response that helps you quickly and efficiently address
[01:27:58.320 --> 01:28:05.360]   attacks. We know the faster you respond to a malware attack, the less damage it does.
[01:28:05.360 --> 01:28:10.800]   They'll even give you security awareness training for your employees because they are, after all,
[01:28:10.800 --> 01:28:17.520]   the first line of defense against attack. Now, here's the key. You can get a free email threat
[01:28:17.520 --> 01:28:22.880]   scan right now of your Office 365 account and no risk, no pressure. All you have to do is go to
[01:28:22.880 --> 01:28:28.000]   barracuda.com/twit. I know there's some bosses who say, "I don't want to know. I just don't want to
[01:28:28.000 --> 01:28:33.840]   know." But really, you got to know. I hate, I know it's frustrating. It's scary. It's bad out there,
[01:28:33.840 --> 01:28:39.440]   but you got to know. Barracuda.com/twit. Uncover the threats hiding in your inbox.
[01:28:39.440 --> 01:28:45.120]   And then after you get that secure, free scan, I think you really ought to consider Barracuda
[01:28:45.120 --> 01:28:52.880]   total email protection. Barracuda is your journey secured. Protect yourself with Barracuda.
[01:28:52.880 --> 01:28:59.280]   So I apologize for the beeping. I furthermore apologize, Seth, for accusing you of the beeping.
[01:28:59.280 --> 01:29:05.120]   I'd never heard that sound before. I'm just offended that you accused me of living in Brooklyn.
[01:29:05.120 --> 01:29:11.600]   All right. Tell me I'm wrong. Doesn't he look like he belongs in Brooklyn?
[01:29:13.600 --> 01:29:17.840]   You look like Seth. They were crickets there, man. They were crickets.
[01:29:17.840 --> 01:29:22.480]   Well, they're all too polite. They're too polite. People at home were thinking, "Yeah, Brooklyn,
[01:29:22.480 --> 01:29:32.240]   for sure. Yeah, definitely." So there is labor strife, a plenty at Google. The NLRB
[01:29:32.240 --> 01:29:42.000]   is now accusing Google of surveilling its employees and other labor violations.
[01:29:42.720 --> 01:29:49.040]   They investigated the firing of several employees in November a year ago, not this past November,
[01:29:49.040 --> 01:29:53.280]   and they say the Google violated parts of the National Labor Relations Act by surveilling
[01:29:53.280 --> 01:29:59.520]   employees and generally interfering with restraining and coercing employees in the exercise of
[01:29:59.520 --> 01:30:05.280]   their rights guaranteed by the National Labor Relations Act. You have the right, by the way.
[01:30:05.280 --> 01:30:11.280]   If you're my employees, stop listening for a second. You have the right to form, join,
[01:30:11.280 --> 01:30:18.160]   or assist a union or engage in other protected, concerted activities. Don't you dare.
[01:30:18.160 --> 01:30:23.200]   No, I don't care. The complaint makes it clear workers have the right to speak
[01:30:23.200 --> 01:30:30.880]   two issues of not merely unionizing, but ethical business. And that's a big deal.
[01:30:30.880 --> 01:30:40.480]   On the heels of that, Google fired perhaps the best known black expert on ethics and AI
[01:30:41.200 --> 01:30:50.000]   Timit Gebru. She was fired after she complained that her research had been suppressed by Google.
[01:30:50.000 --> 01:30:56.960]   Now more than 1,500 researchers and 1,200 Google employees have signed a petition
[01:30:56.960 --> 01:31:04.720]   protesting her firing. This is always a challenge for me to weigh into these.
[01:31:06.240 --> 01:31:18.080]   The story from Google is she said, "I will resign if you continue to suppress this or not at least
[01:31:18.080 --> 01:31:23.360]   not tell me why you're suppressing this research." And then Google said, "And so she resigned."
[01:31:23.360 --> 01:31:29.200]   She says, "No, I didn't resign. I was fired." The dispute arose last month when a senior
[01:31:29.200 --> 01:31:33.760]   manager at Google told her that she would have either have to retract or remove her name from a
[01:31:33.760 --> 01:31:40.560]   paper she had co-authored by researchers inside and outside Google saying, and this is the reason,
[01:31:40.560 --> 01:31:45.600]   by the way, Google hired her in the first place saying that technology companies could do more to
[01:31:45.600 --> 01:31:54.960]   ensure AI systems do not exacerbate historic gender biases. She says, "I feel like we were
[01:31:54.960 --> 01:32:02.000]   censored." And I thought this had implications for all ethical AI research. You're not going to
[01:32:02.000 --> 01:32:05.120]   have this issue. She was talking to Wired. You're not going to have papers that make the company
[01:32:05.120 --> 01:32:09.760]   happy all the time. And don't point out problems. That's antithetical to what it means to be that
[01:32:09.760 --> 01:32:15.360]   kind of researcher. That's why she was brought in to help Google with ethics. She's the technical
[01:32:15.360 --> 01:32:24.560]   co-lead of Google's ethical AI team. So Google and a lot of labor issues right now.
[01:32:24.560 --> 01:32:30.400]   So just to be clear, legally, if you say you're going to resign at some point in the future,
[01:32:30.400 --> 01:32:33.760]   and then you're thinking about quitting or something like that, and they terminate you
[01:32:33.760 --> 01:32:40.000]   immediately, you got fired. Legally, you got, she fired her. And although her boss's boss's boss
[01:32:40.000 --> 01:32:47.600]   can say, "Oh no, we accept her resignation." It's not true. It's not a resignation. She was saying,
[01:32:47.600 --> 01:32:52.480]   it was also like, if this is true that you're going to do this, we're going to need to plan for
[01:32:52.480 --> 01:32:56.960]   me to leave because this is an unacceptable choice that you made. And they're like, "Well,
[01:32:56.960 --> 01:33:02.000]   you're fired." And then they can claim that it's a resignation, but it's not. Also,
[01:33:02.000 --> 01:33:06.800]   their defense of what they didn't like about the paper is amazing because it's basically like,
[01:33:06.800 --> 01:33:11.440]   it didn't have enough sources. It didn't, which it had like 100 sources. But it seems very clear
[01:33:11.440 --> 01:33:17.920]   that what Google didn't like about it is they wanted to insert some things that maybe softened
[01:33:17.920 --> 01:33:25.120]   it or made it seem a little bit easier. They basically wanted to use some PR spin on an academic
[01:33:25.120 --> 01:33:31.040]   paper, which is not, I think it really makes everybody should stop and say, "Why are big companies
[01:33:31.040 --> 01:33:38.880]   like this?" Hiring people to do research like this because clearly they don't want independent
[01:33:38.880 --> 01:33:44.080]   research and independent thought. They want something that'll back up whatever their corporate
[01:33:44.080 --> 01:33:47.120]   desire is. That's the message that this firing sends, I think.
[01:33:47.120 --> 01:33:54.800]   Yeah. And that's the more serious issue because it means she was a figurehead. They hired her
[01:33:55.360 --> 01:34:00.720]   to make it look like they cared. But the minute she did anything that challenged the status quo,
[01:34:00.720 --> 01:34:06.800]   it was like, "See ya. We don't really care." And they call out a bunch of the processes that
[01:34:06.800 --> 01:34:11.280]   Google has for submitting papers and said, "Well, you know, by the letter of the law." And then what
[01:34:11.280 --> 01:34:15.600]   we've heard in the last few days is they never followed that. There were all sorts of things
[01:34:15.600 --> 01:34:20.080]   that were submitted at the last minute. People didn't get fired over it. People weren't told to
[01:34:20.080 --> 01:34:25.280]   stand down. So in the cases where they can say that she violated some of their rules,
[01:34:25.280 --> 01:34:30.800]   they were rules that were never enforced before. One of the things Google didn't like was in the
[01:34:30.800 --> 01:34:36.560]   paper they talked about, these large AI models consume a huge amount of processing power and hence
[01:34:36.560 --> 01:34:44.800]   a lot of electricity. Google is very sensitive about this energy usage. They've gone 100%
[01:34:45.680 --> 01:34:51.680]   carbon neutral, etc. They said, "Well, that's old information. The carbon emissions
[01:34:51.680 --> 01:34:59.760]   hasn't been a problem." You know what? That's not how you don't suppress the research
[01:34:59.760 --> 01:35:09.360]   to prove that point. So I think this is... I always hate to weigh in on employment issues because
[01:35:09.360 --> 01:35:14.800]   there's usually a number of things we don't know, information we don't know about the whole process.
[01:35:15.360 --> 01:35:19.120]   But this does not look good for Google at all. The so-called Do Not Be Evil
[01:35:19.120 --> 01:35:23.520]   company is looking more and more evil all time. They got rid of that for a reason, right?
[01:35:23.520 --> 01:35:30.080]   They got rid of that. You went, "Oh, no, wait." Why isn't that your tagline anymore?
[01:35:30.080 --> 01:35:37.120]   But it also shows that it's such a hard thing to actually stand up to a certain amount of values
[01:35:37.120 --> 01:35:41.200]   when you work for these companies. Because I think she's one person that stood up and said,
[01:35:41.200 --> 01:35:47.760]   "You know what? I'm going to stand by my research." How difficult it is to be able to do that against
[01:35:47.760 --> 01:35:52.640]   these large... Like any company to be able to stand up against the people that hired you,
[01:35:52.640 --> 01:35:57.360]   that hired you for a reason. And yes, you're absolutely right. It was so that they could look
[01:35:57.360 --> 01:36:01.680]   like they were doing the right thing and then they could hopefully squeeze it out to make it look like
[01:36:01.680 --> 01:36:07.840]   they're not doing as bad of a job as this paper showed that they were. And they didn't like that.
[01:36:07.840 --> 01:36:12.320]   And I think that out of 100 people, I think that there would only be a handful of researchers
[01:36:12.320 --> 01:36:19.440]   that would be able to stand up. It takes so much fortitude and it goes to stand where these companies
[01:36:19.440 --> 01:36:24.720]   cannot police themselves. For this reason, this is one of the things that shows that these companies
[01:36:24.720 --> 01:36:30.960]   do a very poor job. In the end, the company's job is to take care of themselves, not take care of you.
[01:36:30.960 --> 01:36:36.960]   HR is for the company. HR is not for the workers. They just look like they're for the workers.
[01:36:37.920 --> 01:36:44.880]   Yeah. And I think it's important to also mention that it's not just that other employees were
[01:36:44.880 --> 01:36:51.680]   allowed to get away with rule bending and so on, according to Google's own explanation of what
[01:36:51.680 --> 01:36:57.760]   happened, but that the one time that they come down on somebody happens to be an African-American
[01:36:57.760 --> 01:37:07.200]   woman who is already in an extreme minority at the company. This is such a horrific bad look for
[01:37:07.200 --> 01:37:13.600]   Google at the very least. I can't imagine that there's a single labor attorney in California that
[01:37:13.600 --> 01:37:21.280]   is not rubbing their hands hoping that they get this case. She's got a really good case.
[01:37:21.280 --> 01:37:27.920]   And Google is on the ropes with it. They're not going to come out of this smelling well no
[01:37:27.920 --> 01:37:33.920]   matter what happens. And this is a PR own goal, right? Honestly, you've got an ethics researcher,
[01:37:33.920 --> 01:37:40.160]   you fire her in this way. Her message was public. People know she had outside collaborators who
[01:37:40.160 --> 01:37:43.840]   were going to have the paper. We're going to know everything about it. That's what I don't get
[01:37:43.840 --> 01:37:49.520]   about it. Leo's right. We never know about behind the scenes things. Maybe there's something where
[01:37:49.520 --> 01:37:53.680]   people at Google weren't happy with her work and they wanted her out anyway. Why would you do it
[01:37:53.680 --> 01:38:01.280]   this way in this public way that makes you seem like a villain? I don't understand why they did it
[01:38:01.280 --> 01:38:06.320]   the way they did it. Even if they decided that they hated the paper and that they really wish it had
[01:38:06.320 --> 01:38:11.600]   been better, I think the right thing to do as a crisis advisor would be you just got a letter
[01:38:11.600 --> 01:38:17.360]   published because you hired independent researchers. It's the Streisand effect, right? Jason,
[01:38:17.360 --> 01:38:22.720]   like now everyone wants to read this paper and what's happening. They wanted to signal boost
[01:38:22.720 --> 01:38:26.320]   what she was coming out to talk about. This was the way to do it.
[01:38:26.320 --> 01:38:33.040]   Yeah. And you could argue that however damaging this paper was, if Google was seen having had their
[01:38:33.040 --> 01:38:38.160]   people involved in it, it would almost be like a credit to Google. Like look at what Google is
[01:38:38.160 --> 01:38:43.440]   allowing. They're allowing criticism of aspects of Google because they've got this group that does
[01:38:43.440 --> 01:38:47.840]   independent ethics research. Instead, it sends the opposite message, which is don't believe
[01:38:47.840 --> 01:38:52.080]   anything that comes from people who work at Google because the only stuff that they let out is good PR.
[01:38:52.080 --> 01:39:01.200]   And one of the worst things is that of all the various fields that are connected to tech that do
[01:39:01.200 --> 01:39:08.400]   need independent review that do need a real close encounter with an ethics committee,
[01:39:09.920 --> 01:39:15.360]   machine learning and algorithms, I think, have to be up at the top.
[01:39:15.360 --> 01:39:24.000]   There are other places that do this, but if it doesn't happen in the companies and the companies
[01:39:24.000 --> 01:39:29.280]   are not subject to outside scrutiny, it almost isn't going to have much of an effect, right?
[01:39:29.280 --> 01:39:36.160]   We need to know as users what's going into these algorithms that are deciding what we see on Facebook,
[01:39:36.160 --> 01:39:44.160]   what shows up in Google search, how Amazon is determining what products to show us.
[01:39:44.160 --> 01:39:50.640]   I think these are really crucial questions. And for Google to do this at this time is
[01:39:50.640 --> 01:39:56.320]   just so boneheaded. It's a bad look at the least.
[01:39:56.320 --> 01:40:02.400]   The MIT Technology Review, Karen Howe, has a story. They got the paper. They summarized it.
[01:40:02.400 --> 01:40:05.920]   But I think the re if you ask, why should I care? It's an employment issue.
[01:40:05.920 --> 01:40:11.840]   The reason you should care comes in the last paragraph of the story. It's a quote from Emily
[01:40:11.840 --> 01:40:19.680]   Bender who was one of the co authors of Gebruz paper. Bender worries they say that Google's actions
[01:40:19.680 --> 01:40:26.080]   could create a chilling effect on future AI ethics research. Many of the top experts in AI
[01:40:26.080 --> 01:40:31.120]   ethics work at large tech companies because that's where the money is. Bender says,
[01:40:31.120 --> 01:40:36.640]   this has been beneficial in many ways, but we end up with an ecosystem that maybe has incentives
[01:40:36.640 --> 01:40:39.920]   that are not the very best ones for the progress of science for the world.
[01:40:39.920 --> 01:40:47.760]   You know, I think everybody understands that AI is a cash cow for a lot of these companies.
[01:40:47.760 --> 01:40:54.800]   It's important for research. It's important for us as a country, but that there are potential
[01:40:54.800 --> 01:41:03.200]   ethical landmines here. It's really important that ethics researchers are able to weigh in on this
[01:41:03.200 --> 01:41:09.680]   and say things like, for instance, one of the Gebruz previous studies was the one that found that
[01:41:09.680 --> 01:41:18.960]   women of color were something like 40% misrecognized by face recognition technologies because they've
[01:41:18.960 --> 01:41:24.240]   been trained on white men. She was one of first rang alarm on this.
[01:41:24.240 --> 01:41:30.240]   That stuff is important. Otherwise, these these AI tools get used that they have built in biases
[01:41:30.240 --> 01:41:35.440]   and they can really screw things up. And so it's it's absolutely paramount if Google's going to be
[01:41:35.440 --> 01:41:44.320]   doing AI research or any company that they have independent ethics people there who have the power
[01:41:44.320 --> 01:41:51.920]   to say, no, this is bad. This is wrong without fear of censorship. It really is risky. It's really
[01:41:51.920 --> 01:41:58.880]   I wonder if and this is a very dark term. So I apologize. But I wonder if this is kind of like
[01:41:58.880 --> 01:42:04.320]   the palladium tea like that they're fine with it's they are there to send a chilling effect that,
[01:42:04.320 --> 01:42:08.720]   you know, what? Be careful what you choose to stand up against us because we'll fire you and we really
[01:42:08.720 --> 01:42:14.720]   don't care. That's probably the case back to us that there's definitely intentional or not.
[01:42:14.720 --> 01:42:17.280]   That's the signal they're sending, isn't it? Yeah.
[01:42:18.560 --> 01:42:23.200]   Yeah, but there's a lot of outside independent research that's funded, you know, that comes from
[01:42:23.200 --> 01:42:27.120]   places that are funded by corporate money and they're not necessarily independent either.
[01:42:27.120 --> 01:42:32.800]   The tragedy here is the advantage of having somebody a group like this inside Google is you get
[01:42:32.800 --> 01:42:40.480]   they get access to Google and Google's tech. And it is important for us all to have somebody who's
[01:42:40.480 --> 01:42:47.840]   looking at what Google is doing and not from the outside. But at this point, it's hard to see without
[01:42:47.840 --> 01:42:53.920]   a pretty dramatic turn on Google's part that anybody is going to really look at the output of that
[01:42:53.920 --> 01:42:58.480]   group without a huge amount of skepticism. And I feel bad for the other people who work in that
[01:42:58.480 --> 01:43:02.720]   group because I would imagine they're all going to feel like they have to go somewhere else in order
[01:43:02.720 --> 01:43:08.320]   to reclaim their credibility. So it's a it's a real like I said, it's an own goal by by Google. Like
[01:43:08.320 --> 01:43:13.120]   they even if they were unhappy with her or some of the work she was doing, I you would think that
[01:43:13.120 --> 01:43:18.000]   they would move sort of silently in the background to sort of just wind it up and and finish the
[01:43:18.000 --> 01:43:23.680]   program or have her go to the door. And instead they just did this. I'm glad they did. I'm glad
[01:43:23.680 --> 01:43:28.800]   I'm glad we all benefit from knowing the truth. Yeah, it focuses our minds. You know, companies
[01:43:28.800 --> 01:43:33.680]   are doing AI research have a high I think they have a high ethical requirement to pay attention to
[01:43:33.680 --> 01:43:39.040]   what the impacts of this research are. And if they don't, they need to be regulated. They need
[01:43:39.040 --> 01:43:42.400]   something, you know, this is where government needs to step in and say, no, no, no, no.
[01:43:42.400 --> 01:43:50.640]   So there is a solution here, I think, and I cannot foresee any future on any, you know,
[01:43:50.640 --> 01:43:55.360]   version of Earth in the multiverse that would that it would fly. But the real solution I think
[01:43:55.360 --> 01:44:01.520]   is is that these companies, especially when they're publicly traded, need to have a corporate ethics
[01:44:01.520 --> 01:44:07.760]   on budsman. That's what that person cannot be a figurehead. And they have to have some kind of
[01:44:07.760 --> 01:44:15.040]   power within the company. That's really the only way I could see, you know, Google coming out of this
[01:44:15.040 --> 01:44:21.760]   looking better. And I can't see that happening. Or we would love to see it.
[01:44:21.760 --> 01:44:31.920]   Union's union's would help. I absolutely think that that tech is tech employees
[01:44:33.200 --> 01:44:39.200]   who, you know, are developers and whatnot are long overdue to unionize.
[01:44:39.200 --> 01:44:46.320]   But again, like, I think people look at their salaries, I think people look at their own salaries
[01:44:46.320 --> 01:44:49.360]   and say, what do we need a union for that would that would hamper our
[01:44:49.360 --> 01:44:57.680]   earning ability if it's tied to metrics. And they look at the way that unions have gone in the US
[01:44:57.680 --> 01:45:02.800]   over the past 30 years and say, what do we need any of that drama for either? But
[01:45:02.800 --> 01:45:07.040]   I agree. I think unions would be a big, a big, a good place to start.
[01:45:07.040 --> 01:45:12.640]   There's a long history of these companies kind of playing lip service to corporate ethics and
[01:45:12.640 --> 01:45:18.960]   doing the right thing. And it's all just a veneer Facebook or oversight board is a perfect example
[01:45:18.960 --> 01:45:25.040]   of this. They are now starting to hear cases. But we're learning that it, the cases they hear,
[01:45:25.040 --> 01:45:32.560]   they won't be ruled on for 90 days like, Oh, yeah. 90 days later, Oh, we were wrong
[01:45:32.560 --> 01:45:37.680]   to take that content down. I'm sorry. Do you want us to put it back up? No, no one cares about it.
[01:45:37.680 --> 01:45:43.680]   It's over. Thank you. So there's all sorts of, all sorts of issues. The idea of the oversight
[01:45:43.680 --> 01:45:48.880]   board, very, I mean, the people they picked were very impressive. And the idea was that
[01:45:48.880 --> 01:45:55.120]   you could appeal to them about Facebook takedowns or maybe that something should be
[01:45:55.120 --> 01:46:00.080]   taken down is not taken down. But the problem is the board can only take up appeals against
[01:46:00.080 --> 01:46:05.680]   removal of content. I'm sorry. So I was wrong. It's only about removal. I can't look at where
[01:46:05.680 --> 01:46:10.080]   cases were disputed materials left up. It can't review cases that aren't appealed.
[01:46:10.080 --> 01:46:16.560]   And it's got operate completely in secrecy. Members of the board can't discuss their work,
[01:46:16.560 --> 01:46:24.720]   except through Facebook PR. It, you know, just overall, and they have 90 days to reach conclusions.
[01:46:24.720 --> 01:46:29.680]   They've selected out of 20,000 appeals. They've selected their first six.
[01:46:29.680 --> 01:46:36.640]   So this is a perfect example of Facebook doing something that looks good on paper is great PR
[01:46:36.640 --> 01:46:44.400]   and is completely pointless. Yeah. It reminds me of when you used to be able to vote on
[01:46:44.400 --> 01:46:52.320]   changes to Facebook policy. And I remember, I remember there was some big one that they did.
[01:46:52.320 --> 01:47:00.720]   It was like 2010 or maybe 2011. And almost 300,000 people voted for it in the US.
[01:47:00.720 --> 01:47:06.800]   But because 300,000 US voters is equal, even back then was equal to, you know,
[01:47:06.800 --> 01:47:13.520]   some, you know, tiny percentage of Facebook users. And they decided that it was for global,
[01:47:13.520 --> 01:47:20.160]   it had to be a global, global users to account. Yeah. And so it didn't go anywhere.
[01:47:20.160 --> 01:47:25.120]   Of course. So here's the, here's the six questions that they're going to look at.
[01:47:25.120 --> 01:47:32.640]   Should a set of Instagram posts that showed nipples be allowed considering the photos were
[01:47:32.640 --> 01:47:38.720]   part of a breast cancer prevention program, they really picked like tough, tough things to decide.
[01:47:38.720 --> 01:47:45.680]   Should historical quotes from Nazi propagatus Joseph Gebels be allowed to circulate,
[01:47:45.680 --> 01:47:52.640]   even if they're in violation of Facebook's dangerous individuals policy? Should screenshots of tweets,
[01:47:52.640 --> 01:47:58.160]   not even Facebook posts, but tweets by Malaysian Prime Minister stay up in order to raise awareness
[01:47:58.160 --> 01:48:04.000]   of the hateful nature of his bigotry or be removed because they violate the speech policy.
[01:48:04.000 --> 01:48:10.480]   Should posts criticizing the French government be reinstated despite their allusions to alternative
[01:48:10.480 --> 01:48:17.280]   treatments for COVID? There nothing about conservative politicians being censored, nothing about QAnon,
[01:48:17.280 --> 01:48:24.880]   nothing about anti-vaxxers, some of the most thorny problems facing Facebook completely ignored. So,
[01:48:24.880 --> 01:48:31.840]   you know, it's just PR. We knew it, but now we know it. You know what I mean? We know it, man.
[01:48:31.840 --> 01:48:37.680]   Facebook says we will remove misinformation about COVID vaccines. You know, as we get closer
[01:48:38.640 --> 01:48:45.120]   to actual working vaccines, they're going to start to be distributed at the end of this month in the
[01:48:45.120 --> 01:48:51.440]   United States to healthcare workers. And it is expected that most of the nation will have access
[01:48:51.440 --> 01:48:57.840]   to either the Moderna or what is it, the Pfizer is the second one vaccine by April.
[01:48:57.840 --> 01:49:04.400]   What's going to be really important is the public relations and marketing campaign to get
[01:49:04.400 --> 01:49:10.080]   people to take these vaccines. What's going to be really important is to get stuff saying,
[01:49:10.080 --> 01:49:17.680]   don't get vaccinated off Facebook, right? Or maybe I'm biased in this matter.
[01:49:17.680 --> 01:49:23.760]   No, that's the next frontier of this. That's where it's going to really be. The rubber is going
[01:49:23.760 --> 01:49:31.600]   to hit the road, as they say. Yeah. Yeah. Already, if not everyone gets it, then it doesn't really,
[01:49:31.600 --> 01:49:34.880]   it's still going to, we're still going to have an influx of people in our hospitals.
[01:49:34.880 --> 01:49:36.800]   Public health. It's going to fog up the whole system.
[01:49:36.800 --> 01:49:43.520]   Officials estimate that to get herd immunity 70 to 80% of the population will have to take the
[01:49:43.520 --> 01:49:49.920]   vaccine far, a far lower percentage of people say they will. I can't remember what the number was,
[01:49:49.920 --> 01:49:56.400]   but it was a very low number, like 50 or 60%. I mean, this is one of the big concerning things,
[01:49:57.520 --> 01:50:05.680]   you know, about the firing of Chris Krebs, who is the leader of CISA, the Cybersecurity Division
[01:50:05.680 --> 01:50:14.960]   at DHS, is that not only was he working on securing the election and providing
[01:50:14.960 --> 01:50:26.000]   factual information in a digestible way to counter election disinformation, he was also working on
[01:50:26.000 --> 01:50:35.920]   a very similar project for COVID vaccines. And not having a figurehead running that as we,
[01:50:35.920 --> 01:50:45.360]   you know, start getting into a place where we have vaccines that can fight COVID, I think is really
[01:50:45.360 --> 01:50:51.040]   just utterly messed up. I mean, Biden, of course, will put somebody in there.
[01:50:52.240 --> 01:50:58.880]   But, you know, even though, you know, Krebs is a lifelong Republican, he was doing the right thing
[01:50:58.880 --> 01:51:04.480]   regardless of politics. And it's certainly going to set the agency back in their efforts to fight,
[01:51:04.480 --> 01:51:08.080]   you know, vaccine misinformation and disinformation.
[01:51:08.080 --> 01:51:14.400]   He was on face the nation this morning. And with some very scary information, he says
[01:51:14.400 --> 01:51:23.120]   Russia, China, Iran, and North Korea are trying to steal coronavirus vaccine IP. The big four Russia,
[01:51:23.120 --> 01:51:26.480]   China, Iran, and North Korea, we've seen to some extent all four of these countries doing some kind
[01:51:26.480 --> 01:51:33.680]   of espionage or spying, trying to get intellectual property related to the vaccine. And what we had
[01:51:33.680 --> 01:51:40.240]   been thinking through at CISA was not just the vaccine developers, but the entire supply chain
[01:51:40.800 --> 01:51:47.360]   trying to undermine, not merely steal the idea or the recipe, but actually trying to undermine
[01:51:47.360 --> 01:51:53.520]   the program. And I expect there will be big disinformation campaigns as well from these countries.
[01:51:53.520 --> 01:52:00.160]   This is going to be a big issue. And man, it's really sad that Krebs is no longer
[01:52:00.160 --> 01:52:05.440]   running CISA because this is just this is as important as anything he was doing.
[01:52:08.000 --> 01:52:11.680]   You know, it's it's going to be a big problem. Yeah. So,
[01:52:11.680 --> 01:52:16.960]   but of course, you know, that's the world we live in right now.
[01:52:16.960 --> 01:52:25.040]   The good, you know, what's interesting is despite all of the pressures against him, he was able to
[01:52:25.040 --> 01:52:31.600]   make the election run well. Is it? I should I should this is your your your bailiwick, Seth.
[01:52:31.600 --> 01:52:35.120]   Is it considered he said it was the most secure election we've ever had.
[01:52:36.320 --> 01:52:42.560]   Even in 2016, there wasn't really any assertion that the Russians tried to change vote counts.
[01:52:42.560 --> 01:52:47.920]   It was mostly disinformation campaigns and things like that, right? I mean, there were there were
[01:52:47.920 --> 01:52:55.040]   assertions that the Russians had changed vote counts that those proved. I'm trying to remember,
[01:52:55.040 --> 01:53:00.640]   I think there was maybe one instance in a Florida county where they were able to get access to
[01:53:01.520 --> 01:53:09.040]   I think it was a the county registrar. That's right. I don't that's right.
[01:53:09.040 --> 01:53:13.360]   I don't try to get the voter rolls, but they weren't able to change the actual numbers.
[01:53:13.360 --> 01:53:18.080]   Right. Yeah. That's hard to change. It's a very heterogeneous system.
[01:53:18.080 --> 01:53:21.600]   Well, and we talked to election experts before 20x16,
[01:53:21.600 --> 01:53:29.920]   including Ron Rivest, and everybody has there's been a strong consensus since that time that a paper
[01:53:29.920 --> 01:53:38.000]   trail fixes the problem. And I think in the four years intervening between 2016 and 2020,
[01:53:38.000 --> 01:53:46.640]   that was of and thanks to Chris Krebs, among others, very successful notion that we put in much
[01:53:46.640 --> 01:53:55.280]   more secure systems. Yes. Well, we did. We sort of did it in a haphazard
[01:53:55.280 --> 01:54:01.520]   election system in the US. No, no, no, it had nothing really it had nothing to do with the with the
[01:54:01.520 --> 01:54:06.560]   election system. But if there's any, if you're a Mandalorian fan and you remember the episode where
[01:54:06.560 --> 01:54:15.200]   the Mon Calamari guy fixed the Mandalorians ship, Razor Krebs, with a whole bunch of string and
[01:54:15.200 --> 01:54:21.440]   twine, that's sort of what happened. There was a whole bunch of string and twine that led to
[01:54:22.000 --> 01:54:27.920]   an improvement in the number of states that created a paper trail, but it wasn't
[01:54:27.920 --> 01:54:35.280]   from a federal government mandate, which is really unfortunate. And it means that there are still
[01:54:35.280 --> 01:54:42.480]   states and large jurisdictions in some states that do not produce paper trails. It's better than it
[01:54:42.480 --> 01:54:53.040]   was in 2016, but it's better than it was by luck, by just people fighting and getting small victories
[01:54:53.040 --> 01:55:02.160]   in their own jurisdictions. The, ironically, the defeat of Stacey Abrams in Georgia in 2018 led to
[01:55:02.160 --> 01:55:09.200]   Georgia and led to a lawsuit. And the consequences of that lawsuit were to mandate that Georgia
[01:55:09.200 --> 01:55:17.200]   has to have a risk limiting audit. So in addition to an improved paper trail in Georgia,
[01:55:17.200 --> 01:55:24.720]   they also had to do the recounts. And that's what led to, that's what's led to this runoff.
[01:55:24.720 --> 01:55:33.440]   There's been a lot of improvements made, but they're not systemic and there's still plenty of
[01:55:33.440 --> 01:55:40.240]   opportunity for backsliding. These are things that I think are tenuous right now. But yes,
[01:55:40.240 --> 01:55:47.120]   2020 was no doubt a much safer election than 2016 in terms of cybersecurity, in terms of
[01:55:47.120 --> 01:55:53.520]   reaction, reacting to widespread disinformation campaigns. And it's not that the campaigns didn't
[01:55:53.520 --> 01:55:59.920]   happen. The campaigns were absolutely out there, but there was just more of an effort to get
[01:55:59.920 --> 01:56:04.320]   factual information out there. A lot more jurisdictions were on board with it,
[01:56:04.320 --> 01:56:13.280]   as well as national agencies like DHS, CSSA. And it did feel like people, companies like Twitter
[01:56:13.280 --> 01:56:19.920]   and Facebook were kind of throwing off the bots and getting rid of a lot of the disinformation
[01:56:19.920 --> 01:56:26.640]   factories and things like that, right? I think, I mean, I have no evidence of this,
[01:56:26.640 --> 01:56:33.040]   but I just might my reaction to things and seeing how things have gone on Twitter is that Twitter's
[01:56:33.040 --> 01:56:42.880]   decision earlier this year to tamp down on the most egregious misleading tweets from politicians
[01:56:42.880 --> 01:56:50.960]   actually had a pretty substantial effect on people who are connected to Twitter, which is,
[01:56:51.920 --> 01:56:58.080]   not every match. It's not everybody. But those people, I think, are influencers in their own
[01:56:58.080 --> 01:57:04.160]   circles. And so when they're not on Twitter, they're probably talking things up. And I think
[01:57:04.160 --> 01:57:12.240]   just news reports about somebody taking some kind of action against leadership-driven disinformation
[01:57:12.240 --> 01:57:17.040]   is probably helped out a lot more than we think.
[01:57:19.600 --> 01:57:23.760]   Let's take a little break, Seth Rosenblatt is here. It's great to have you from the dash,
[01:57:23.760 --> 01:57:31.360]   parallax.com. He's the editor-in-chief at SR. Jason Snell, six colors.com.
[01:57:31.360 --> 01:57:40.960]   Georgia Dow, anxiety-videos.com. Is this your VR room or is this just an office? What is this room?
[01:57:40.960 --> 01:57:44.960]   This is my office. So this is where I podcast and do sessions.
[01:57:44.960 --> 01:57:49.120]   It's great. I love it. It's kind of very lavender kind of calming,
[01:57:49.120 --> 01:57:54.160]   calming color. We had a great, we had a fun week this week on Twitter. I thought we'd
[01:57:54.160 --> 01:57:59.280]   prepare, do we have a promo? Yeah, we'd prepare a little video for you. So in case you missed
[01:57:59.280 --> 01:58:04.960]   anything, this is what you missed. Ben Rhodes today, major points. He had Christmas decorations
[01:58:04.960 --> 01:58:10.480]   on the bookshop behind. Oh boy. He's got a new competition. Oh, look at this.
[01:58:12.880 --> 01:58:23.040]   Happy decorations. You just decked the holes. Previously on Twitter. Tech news weekly.
[01:58:23.040 --> 01:58:30.240]   The late breaking news that's really interesting. HBO Max is going to get all of Warner Media's 2021
[01:58:30.240 --> 01:58:37.280]   movie catalog the day that they hit the theater. That's a pretty big deal. This week in Enterprise
[01:58:37.280 --> 01:58:42.240]   Tech, we're going to talk about the fascinating research around developing eye tracking and just
[01:58:42.240 --> 01:58:48.400]   how it can help your organization make some critical business decisions. This is a set of eye tracking
[01:58:48.400 --> 01:58:56.080]   glasses. This is Toby Pro glasses too and the glasses record their visual attention during that
[01:58:56.080 --> 01:59:02.800]   process. Wow. Hands on Android. Google announced that it's free Google Photos. Cloud storage is going
[01:59:02.800 --> 01:59:07.280]   away, but I'm going to show you how to create your own cloud storage for your photos next.
[01:59:08.160 --> 01:59:15.200]   Hands on tech. Got something brand new, an Apple MacBook Pro 13 inch, but it's not how it looks
[01:59:15.200 --> 01:59:21.120]   outside that matters. It's what's inside the counts. Twit working at home with you.
[01:59:21.120 --> 01:59:29.600]   If you're working most of the time, you're just eating marshmallows and pretending to work. That's
[01:59:29.600 --> 01:59:34.320]   that's at least me anyway. Our show today brought to you by Casper. I want to talk about that HBO
[01:59:34.320 --> 01:59:39.840]   thing in just a second, the Warner media thing. Very interesting. But first a word by from Casper,
[01:59:39.840 --> 01:59:47.040]   my mattress. Man, did I sleep good last night? I have to say, a lot of it is having the perfect
[01:59:47.040 --> 01:59:54.640]   sleep environment. It's the mattress. It's the pillows. It's the sheets. It's the cool glow light.
[01:59:54.640 --> 02:00:01.520]   It transforms the way you sleep. That's Casper's mission to transform the way you sleep. One snooze
[02:00:02.480 --> 02:00:08.080]   at a time, outrageously comfortable products at not so outrageous prices. Here I am opening my new
[02:00:08.080 --> 02:00:14.400]   Casper mattress. We've actually had several of them from the original Casper. This is the latest
[02:00:14.400 --> 02:00:20.240]   Casper that adds actually some springs in it. It's amazing what Casper has done. They've created a
[02:00:20.240 --> 02:00:28.880]   system that reacts and adapts to your body all night long. Inside, 1,500 won perforations.
[02:00:28.880 --> 02:00:33.520]   To create a cooling system so effective, you won't even notice it because you'll be fast asleep.
[02:00:33.520 --> 02:00:39.040]   But sleeping cool really is important to a good night's sleep. Only Casper mattresses are made with
[02:00:39.040 --> 02:00:46.080]   86 supportive gel pods. That's what aligns your spine, eliminates aches and pains.
[02:00:46.080 --> 02:00:52.720]   It's both firm and giving like a good parent. Altogether, it's the cooling,
[02:00:52.720 --> 02:00:59.600]   supportive comfort you need. Even Samantha, our kitty loves the Casper with over 20,000 reviews
[02:00:59.600 --> 02:01:06.240]   in an average of 4.8 stars across Casper. Amazon and Google Casper is rapidly becoming the internet's
[02:01:06.240 --> 02:01:11.040]   favorite mattress. I'll say it's mine. I'll say that. There is no risk, by the way.
[02:01:11.040 --> 02:01:17.200]   You could be sure of your purchase with Casper's 100 night risk free sleep on a trial,
[02:01:17.920 --> 02:01:24.160]   free shipping and painless returns in the US and Canada. Get the mattress at a great price that
[02:01:24.160 --> 02:01:28.960]   you want. Then, if for any reason it's not exactly right, they'll come and get it. You don't have to
[02:01:28.960 --> 02:01:33.120]   put it back in that surprisingly compact box. Thank goodness. I don't know how they do that.
[02:01:33.120 --> 02:01:38.240]   They'll come and get it refund you every penny. Now with 0% APR financing options,
[02:01:38.240 --> 02:01:43.280]   it's even easier to try a Casper mattress. Wouldn't it be nice to get a good night's sleep
[02:01:44.320 --> 02:01:50.400]   on Christmas Eve and wake up and Santa's delivered maybe a Casper pillow or the glow lamp too?
[02:01:50.400 --> 02:01:58.880]   Go to Casper.com/twit1. Use our offer code TWIT and the number one, TWIT1. You'll get $100
[02:01:58.880 --> 02:02:07.040]   off select mattresses. That's $100 off select mattresses. This is a great time to transform the
[02:02:07.040 --> 02:02:11.200]   way you sleep. We all deserve a good night's sleep. Terms and conditions apply.
[02:02:11.760 --> 02:02:23.040]   See Casper.com/terms. Casper.com/twit1 promo code is TWIT1. You would agree when you
[02:02:23.040 --> 02:02:31.360]   Georgia, you've done videos on sleep that it's got to be cool. Don't you sleep better when it's
[02:02:31.360 --> 02:02:36.320]   cool, right? You sleep better. You'll fall asleep fine if you're warm, but you won't stay asleep
[02:02:36.320 --> 02:02:40.720]   really well. That's it. Because your body temperature drops, it's good to have it like between 17 and
[02:02:40.720 --> 02:02:45.040]   19 and a lot of people, if they find that it's really hot, they're not going to be able to sleep
[02:02:45.040 --> 02:02:49.360]   or they sweat and then they're waking up and your blankets are off and on. They're really
[02:02:49.360 --> 02:02:58.720]   comfortable mattresses. We have one for my dog. It came with a waffle. Nice. Came with a little
[02:02:58.720 --> 02:03:06.400]   chew toy. The anxiety dash videos. There are videos on everything including getting a better
[02:03:06.400 --> 02:03:13.840]   night's sleep. Is this sleep in here? There's conflict. Sleep in there. It's like the little baby one.
[02:03:13.840 --> 02:03:19.440]   Oh, sleep like a little sleep. There we go. That's the baby one. Get the sleep you've always dreamt of.
[02:03:19.440 --> 02:03:26.320]   Yeah. The nice thing is that it's like whenever we deal with people that have insomnia or they come
[02:03:26.320 --> 02:03:31.680]   in and they can't sleep, these are the things that I would say in my session to help them get
[02:03:31.680 --> 02:03:38.880]   their back on track. It's the stuff that's out of the day. Sleep is important and we spend 30%
[02:03:38.880 --> 02:03:43.360]   of our life on a mattress or depending on your sleep habits, maybe more, maybe less.
[02:03:43.360 --> 02:03:45.360]   Well, it's not all sleeping. You've been fasted in any mattress.
[02:03:45.360 --> 02:03:51.200]   Yeah, I agree. Fair. Well, actually, that's the big problem for me is a lot of time spent on the
[02:03:51.200 --> 02:03:57.840]   mattress that I'm not sleeping and I wish I were sleeping. Why do we look at the ceiling? No fun.
[02:03:57.840 --> 02:04:02.800]   You and Renee Ritchie doing a great new podcast called Apple Talk. I want to get everybody
[02:04:02.800 --> 02:04:07.040]   subscribed to that because Renee's got the brains, but you got the heart.
[02:04:07.040 --> 02:04:15.040]   It's a lot of fun. We haven't done a podcast in a really long time together. We're really excited.
[02:04:15.040 --> 02:04:20.240]   Can I give a little plug to my friend who just started a new service for people that want to kind
[02:04:20.240 --> 02:04:27.280]   of live with technology but not be engulfed with technology? It's called Navigate.
[02:04:27.840 --> 02:04:34.480]   It's Christina Crook with Navigate and it's this wonderful site. I think that it's Navigate
[02:04:34.480 --> 02:04:42.240]   Christina Crook. It's just this wonderful site that you can subscribe and she sends things to help
[02:04:42.240 --> 02:04:50.880]   you live a more peaceful, happy life and not be kind of sucked into technology but enjoy it
[02:04:50.880 --> 02:04:55.600]   along with still connecting. I think that we really do need people to be able to connect with
[02:04:55.600 --> 02:05:00.480]   each other. I'm really excited for her. Check that out. You don't have to ditch your phone.
[02:05:00.480 --> 02:05:08.080]   No. It's how you can live with both. You can still watch this podcast but also have a more full
[02:05:08.080 --> 02:05:13.520]   rich life. It's so pathetic. I get up in the morning. The first thing I have to do is go to my
[02:05:13.520 --> 02:05:20.960]   Animal Crossing Island and make sure there's no weeds and find all the fossils and hit all the
[02:05:20.960 --> 02:05:26.560]   rocks and shake all the trees. Then an hour later I get out of bed and I bring my laptop
[02:05:26.560 --> 02:05:31.120]   to the breakfast table and I sit down and I look for news for a tour. I'm in front of a screen
[02:05:31.120 --> 02:05:37.840]   all the freaking time. So maybe someone should get that for you. Subscribe for a few months to
[02:05:37.840 --> 02:05:43.200]   be. Maybe. Oh my god. The Animal Crossing is cute. I get it. Did you play it ever?
[02:05:43.200 --> 02:05:48.560]   I did play Animal Crossing. Yeah. My favorite thing was just getting the different animals.
[02:05:48.560 --> 02:05:53.040]   Yeah. It was everything from. But I could get sucked into that kind of thing. I don't know what
[02:05:53.040 --> 02:05:57.600]   it is. The Simpsons tapped out. Before that it was we rule before that it was farm hill.
[02:05:57.600 --> 02:06:05.600]   There's something about farming. It's so peaceful to me to us to want to hunt, gather, accumulate.
[02:06:05.600 --> 02:06:11.120]   Yeah. Yeah. The people that did that better survived. Right? Yeah. And some of the grasshoppers.
[02:06:11.120 --> 02:06:16.560]   I discovered that you get make good money if you raise pumpkins in Animal Crossing.
[02:06:16.560 --> 02:06:21.520]   So I basically took over the whole island with pumpkins. It's not good for the residents.
[02:06:21.520 --> 02:06:25.360]   It actually teaches a little bit about capitalism.
[02:06:25.360 --> 02:06:33.200]   It makes you understand the machine. Yeah. Yeah. Screw the residents. I'm growing pumpkins.
[02:06:33.200 --> 02:06:37.200]   Right. Pretty soon nobody will buy the pumpkins and then you'll be sorry.
[02:06:37.200 --> 02:06:44.000]   All right. Let's talk. I want to actually this is a big story and I'm sure that Jason,
[02:06:44.000 --> 02:06:50.800]   because you cover media with a lot of your shows, is something you've been kind of aware of.
[02:06:50.800 --> 02:06:59.440]   Let's go back in time. The Trump administration fought it for a couple of years, but AT&T wanted
[02:06:59.440 --> 02:07:05.600]   to buy Warner media. And boy, this was a big acquisition, more than $100 billion.
[02:07:05.600 --> 02:07:13.280]   And they finally got through the Trump administration, tried to slow it down,
[02:07:13.280 --> 02:07:18.160]   eventually a judge was convinced by AT&T that nothing bad will happen.
[02:07:18.160 --> 02:07:25.760]   So this is following Comcast buying NBC Universal.
[02:07:25.760 --> 02:07:33.040]   I mean, this has been what's happened is that via combine CBS, as these channels,
[02:07:33.040 --> 02:07:39.040]   the distributors of delivery companies, the streamers buy content up and they're just all
[02:07:39.040 --> 02:07:46.480]   vying. Eventually, everything will be owned by Verizon, AT&T, you know, Comcast, the big ISPs.
[02:07:46.480 --> 02:07:51.760]   So they bought Warner Brothers. They got HBO. They got Turner. They got all sorts of things like DC
[02:07:51.760 --> 02:07:56.640]   Comics, which by the way, the first thing they did was kill Mad Magazine.
[02:07:56.640 --> 02:08:05.440]   It's such a bummer. God, it just makes me mad. But now then the next thing that happens
[02:08:06.320 --> 02:08:12.480]   is that Richard Pepler, who's in charge of HBO and actually considered widely considered to be a
[02:08:12.480 --> 02:08:21.840]   brilliant movie executive gets forced out by AT&T's eventually became CEO, John Stanky.
[02:08:21.840 --> 02:08:27.040]   And I remember a couple of years ago when Stanky said, and I was, I felt a little chill
[02:08:27.040 --> 02:08:33.840]   got my spine HBO needs to be more like Netflix. Yeah, that's in effect. What's happened.
[02:08:33.840 --> 02:08:36.080]   Yeah, I loved him on the Little Rascals, by the way, Stanky.
[02:08:36.080 --> 02:08:40.000]   Stanky. Here's my favorite. He was the one with the hair that stuck up, right? Yeah.
[02:08:40.000 --> 02:08:43.120]   Everybody is afraid of Netflix's power, right? Everybody in the entertainment industry,
[02:08:43.120 --> 02:08:46.080]   and they all want their own Netflix. They want to be their own Netflix. And everybody took a
[02:08:46.080 --> 02:08:50.960]   different path to it. So like Disney took their path to it with Disney Plus. And what they wanted
[02:08:50.960 --> 02:08:55.600]   to do is Warner Media driven by AT&T. They wanted to create something where they could
[02:08:55.600 --> 02:09:01.440]   pour all of their resources into their own premium streaming service. And in doing so,
[02:09:02.160 --> 02:09:07.360]   they took HBO, which was probably their best property, honestly, their best brand.
[02:09:07.360 --> 02:09:12.880]   But they said, you can't be HBO anymore. You need to be Netflix. And that led to an
[02:09:12.880 --> 02:09:16.800]   exodus of HBO executives. Kepler is now an Apple, I think, right?
[02:09:16.800 --> 02:09:22.560]   Pep, the plep is doing his stuff for Apple TV Plus. Yeah, which is actually, I would say,
[02:09:22.560 --> 02:09:27.200]   maybe the streaming service most like classic HBO and that they're focused on originals. And
[02:09:28.400 --> 02:09:35.360]   meanwhile, HBO Max now, like they are trying to be Netflix. And Jason Killar, who's a smart guy,
[02:09:35.360 --> 02:09:41.840]   who was the founder of Hulu, basically is running it. And they're playing a different game. And now
[02:09:41.840 --> 02:09:47.040]   with the story, I assume we're about to talk about, they've got some content that's really
[02:09:47.040 --> 02:09:51.920]   interesting and may actually motivate people to sign up for this service, which apparently so far,
[02:09:51.920 --> 02:09:58.160]   most of the people who actually are eligible to get HBO Max have just not bothered to sign up for it.
[02:09:58.800 --> 02:10:05.920]   So in classic antitrust theory, the way it works is a company becomes bigger and bigger,
[02:10:05.920 --> 02:10:10.880]   pushes out all the competition. And one way they do it is by undercutting the competition,
[02:10:10.880 --> 02:10:15.120]   selling below cost and so forth. Until the competition is gone, they control everything,
[02:10:15.120 --> 02:10:21.840]   and then they raise prices through the roof. I think this is exactly what's happening. Warner
[02:10:21.840 --> 02:10:30.240]   Brothers has announced that starting in 2021, I don't think it's going to end, but they're going
[02:10:30.240 --> 02:10:36.640]   to release all their new movies, both on HBO Max and in theaters, they'll be on HBO Max for a
[02:10:36.640 --> 02:10:44.640]   period of a month. They gave the theater companies one hour notice of this, and distributors like AMC
[02:10:44.640 --> 02:10:51.760]   and Regal, who are this close to going bankrupt anyway, thanks to COVID-19, are freaked out.
[02:10:51.760 --> 02:10:58.560]   It is going to be a huge threat to the future of motion picture theaters.
[02:10:58.560 --> 02:11:03.520]   Yeah, for sure. For sure. It's actually starting Christmas Day, right? Because it's with Wonder Woman
[02:11:03.520 --> 02:11:10.080]   in 24. And then going on, there'll be a one month window where those will be playing not just in
[02:11:10.080 --> 02:11:14.800]   theaters, but on HBO Max. And then after a month, it will go to online sales and rental and
[02:11:14.800 --> 02:11:19.600]   blue rates, whatever else. And by the way, not for 20 bucks free on HBO Max.
[02:11:19.600 --> 02:11:25.280]   Yes. And if you, by the way, if you get HBO with your cable company, you probably can get HBO Max
[02:11:25.280 --> 02:11:29.440]   for free. You just need to sign up. Yeah. So, but a lot of people happen.
[02:11:29.440 --> 02:11:35.200]   If you're a movie theater, Jane, and you know that Wonder Woman 84 is going to be free for one
[02:11:35.200 --> 02:11:39.600]   month to HBO Max subscribers, what do you think that's going to do your ticket sales?
[02:11:39.600 --> 02:11:42.960]   Well, their ticket sales are already in the toilet.
[02:11:42.960 --> 02:11:45.200]   Right. But this is all of 2021.
[02:11:45.200 --> 02:11:50.720]   Look, look, anybody who thinks that we're going to be seeing, you know, people
[02:11:50.720 --> 02:11:57.680]   broadly going back into movie theaters before a year from now, I think is nuts.
[02:11:57.680 --> 02:12:01.920]   There's some hope that with a vaccine, people could be going back this summer. I think saying
[02:12:01.920 --> 02:12:07.760]   all 2021 is a big deal. I think I think this summer is ambitious at best.
[02:12:07.760 --> 02:12:14.640]   I think we're going to see a very slow and measured rollout of a vaccine. And I think that this is
[02:12:14.640 --> 02:12:22.240]   actually probably the only way that HBO could make any of its money back. I understand that
[02:12:22.240 --> 02:12:27.360]   theaters are freaked out. You know, there's a whole sidebar we could do on what AT&T has done
[02:12:27.360 --> 02:12:34.000]   to DC Comics. But, you know, ultimately, at least for the movies, I don't see any other option.
[02:12:34.000 --> 02:12:38.160]   You're sitting on these things which are done. They're in the can. They're waiting to go.
[02:12:38.160 --> 02:12:43.520]   And there's not going to be butts in seats at theaters. It's insane. I wouldn't...
[02:12:43.520 --> 02:12:48.240]   I mean, I don't know that I'd even go to a theater that a friend of mine rented out and there were,
[02:12:48.240 --> 02:12:54.560]   you know, 15 seats per person. Like, I just don't know that that kind of risk is worth it these days.
[02:12:54.560 --> 02:12:59.760]   And I think a lot of other people are making a similar calculation, especially with the massive
[02:12:59.760 --> 02:13:05.280]   spikes in COVID going on around the world right now. It's not just the US. It's everywhere is seeing
[02:13:05.280 --> 02:13:11.760]   a resurgence. And I'm just... I would not go into a theater, bottom line.
[02:13:11.760 --> 02:13:17.440]   So that's the excuse. I feel like really this is Warner Media saying, jumping on that and saying,
[02:13:17.440 --> 02:13:25.840]   "Good. We could put these theaters out of business." AMC's CEO slammed their plan saying they're going
[02:13:25.840 --> 02:13:33.200]   to lose... Not AMC. Warner is going to lose a ton of money. This is the statement published by the
[02:13:33.200 --> 02:13:38.320]   Hollywood Reporter. Clearly, Warner Media intends to sacrifice a considerable portion of the
[02:13:38.320 --> 02:13:43.680]   profitability of its movie studio division and that of its production partners and filmmakers
[02:13:43.680 --> 02:13:48.000]   to subsidize its HBO Max startup. I would submit that's exactly what you do if you're
[02:13:48.000 --> 02:13:55.520]   done monopoly is you give up profit for a short term and long term hoping and what better to be
[02:13:55.520 --> 02:14:01.040]   the... This was by the way, one of the very first antitrust cases was in the movie companies, which
[02:14:01.040 --> 02:14:06.480]   owned all the theaters were forced to divest the theaters because when you own the production
[02:14:07.040 --> 02:14:14.960]   and the distribution, there's no competition. And I think this is what Stanky is going for.
[02:14:14.960 --> 02:14:20.800]   I love saying that name. I think this is what Stanky is going for. AMC says,
[02:14:20.800 --> 02:14:24.800]   "We will do all in our power to ensure that Warner does not do this at our expense. We will
[02:14:24.800 --> 02:14:30.000]   aggressively pursue economic terms that preserve our business." They said, "We made a deal for
[02:14:30.000 --> 02:14:34.800]   Wonder Woman 1984 because..." Well, we knew the theaters wouldn't be reopening by Christmas,
[02:14:34.800 --> 02:14:39.120]   but this is all of 2021. And this is not the deal we made.
[02:14:39.120 --> 02:14:45.360]   Well, listen, I'm not a big fan of the movie theater chains either. I think that they have
[02:14:45.360 --> 02:14:52.400]   had their own little... They're not motivated and have not been motivated for a long time to actually
[02:14:52.400 --> 02:14:56.880]   have a good product because they had exclusivity in terms of these windows of time when they're
[02:14:56.880 --> 02:15:01.680]   only in theaters. And that may be coming to an end. I think people will still want to go to
[02:15:01.680 --> 02:15:06.160]   movie theaters in the long run, but I think that they're going to go for reasons that are not
[02:15:06.160 --> 02:15:09.360]   time exclusivity. They're going to be going because they want to be with an audience,
[02:15:09.360 --> 02:15:14.320]   because they want to see a big screen, because they want to get food with their movie and have
[02:15:14.320 --> 02:15:19.600]   an experience. It's going to be different. And a lot of these movie theater chains that have
[02:15:19.600 --> 02:15:25.200]   theaters that are getting by on a terrible experience, and we've all been in those theaters
[02:15:25.200 --> 02:15:28.640]   because it was the only place that you could see the Marvel movie of the week.
[02:15:29.680 --> 02:15:33.280]   That's going to... They're not going to be able to get away with that necessarily.
[02:15:33.280 --> 02:15:40.640]   That all said, I do believe, and Seth said it, the whole idea that even if people come back to
[02:15:40.640 --> 02:15:45.120]   the theaters over the summer, it's going to be at 40% or 20% or 50%. We don't know. It's not 100%.
[02:15:45.120 --> 02:15:52.480]   The big challenge is, if you're Warner or Disney, you can't make a movie for two or 300 million
[02:15:52.480 --> 02:15:59.200]   dollars for streaming. No, Mulan is the proof that was a terrible failure,
[02:15:59.200 --> 02:16:04.560]   was 30 bucks to watch it, and it just tants. The money's not there. People aren't going to pay
[02:16:04.560 --> 02:16:08.800]   like they do in a theater. So they're going to be motivated in the long run to have those big
[02:16:08.800 --> 02:16:13.280]   10 polls that cost hundreds and millions of dollars, and there's some exclusivity, and they get them
[02:16:13.280 --> 02:16:19.360]   into theaters because otherwise the alternative is those 300 million dollar movies are going to
[02:16:19.360 --> 02:16:24.640]   not be made anymore. And instead, everything's going to be the Mandalorian. Everything's going to be a
[02:16:24.640 --> 02:16:30.160]   100 million dollar Marvel movie that goes directly to Disney Plus. And I think that while we will
[02:16:30.160 --> 02:16:34.480]   probably see more of that, I think there will still be a place for the big blockbuster that
[02:16:34.480 --> 02:16:38.880]   costs a fortune in the movie theater. But how many of those theaters exist and whether to your
[02:16:38.880 --> 02:16:45.280]   point Leo, they're all studio owned and operated, or they're things that are more like some of these
[02:16:45.280 --> 02:16:50.960]   special restaurant movie theater chains that we see. Like Alamo or maybe Almodraffos.
[02:16:50.960 --> 02:16:56.800]   IMAX. Something that I want more of an experience. I want something different.
[02:16:56.800 --> 02:17:00.640]   You know, I want something different. I have a nice enough, it's a tiny little
[02:17:00.640 --> 02:17:04.640]   projector screen, but I have a nice enough kind of theater in my home. We bought the little
[02:17:04.640 --> 02:17:10.720]   home theater movie seats. That's great for me. Yeah, you can order them. They just,
[02:17:10.720 --> 02:17:15.440]   they set them up. Why wouldn't you want a nice couch? Why do you want to sit in a movie theater?
[02:17:15.440 --> 02:17:19.680]   Because they have a little drink holder. We have a little candy room.
[02:17:19.680 --> 02:17:23.680]   And so we're doing it if you want the couch. You're recreating the desk.
[02:17:23.680 --> 02:17:28.480]   You do it upstairs. If you want the movie theater, you do the movie theater, but we deck it out.
[02:17:28.480 --> 02:17:30.640]   We have like a little area for a few years ago. I think that's part of the risk.
[02:17:30.640 --> 02:17:33.200]   Is that people are going to do what you're doing, which is they're going to build home theaters
[02:17:33.200 --> 02:17:41.760]   that are great. Exactly. I want 4D cinema. IMAX. I want to have live actors that we interact with
[02:17:41.760 --> 02:17:44.800]   or have a full-scale meal like upper class. You want to smell a ramen?
[02:17:45.600 --> 02:17:54.960]   I want something more than that. Alamo Droughts House, where they bring you a beer and your
[02:17:54.960 --> 02:17:59.360]   food selection while you're sitting with your friends in a packed theater, all reacting to an
[02:17:59.360 --> 02:18:03.920]   exciting movie is a great, that's not going to go away. But the lousy shoebox, you and... The
[02:18:03.920 --> 02:18:10.720]   little ones are going to have to douse or die, right? Which I don't want any company to go under.
[02:18:10.720 --> 02:18:16.240]   But if it hasn't changed with the time, the dinosaurs slowly become extinct and we move for it.
[02:18:16.240 --> 02:18:20.800]   But the sad part is I'm not getting this. I'm not going to get to watch this. This is not coming
[02:18:20.800 --> 02:18:27.840]   out in Canada. So, yeah, I hope you guys enjoy Wonder Woman. Yeah, actually, you're not going to
[02:18:27.840 --> 02:18:31.920]   be able to download it. There's no streaming for it. No, it's only going to be streaming.
[02:18:31.920 --> 02:18:36.800]   It's me. Oh, you mean if you don't have HBO Max. In countries without HBO Max, that's
[02:18:36.800 --> 02:18:42.080]   right now. That's the question. It's pretty much right now. We're... So, you can't buy it on iTunes.
[02:18:42.080 --> 02:18:46.000]   You have to... So, that's the plan, right? Let's get everybody to buy HBO Max.
[02:18:46.000 --> 02:18:51.200]   In the US. And then they might have some streaming partners overseas or they may just wait. But
[02:18:51.200 --> 02:18:56.160]   I think the window for the rest of it's going to be really like a month later. So, it may be on
[02:18:56.160 --> 02:19:01.600]   video in those markets on like January 25th, for example. But that's making the best of a bad
[02:19:01.600 --> 02:19:07.680]   situation because they know that either they hold on to it till next year as happened with the James
[02:19:07.680 --> 02:19:11.600]   Bond movie or if you're going to release it, you have to release it streaming because nobody's
[02:19:11.600 --> 02:19:16.480]   going to go to movie theater and work at Christmas. Some of this may be contractually driven. We don't
[02:19:16.480 --> 02:19:24.160]   know what's in the contracts on Wonder Woman 84 and whether they had to get it out before the
[02:19:24.160 --> 02:19:29.200]   end of the year. Yeah. They... They... They... Something related to that related to the movies that are
[02:19:29.200 --> 02:19:34.720]   being released that they're releasing on streaming in 2021. But the... And they have to pay off.
[02:19:34.720 --> 02:19:41.520]   And they have to pay off the debts and all the actors and stuff. They actually have to do it
[02:19:41.520 --> 02:19:50.880]   structurally where HBO Max is writing a check to Warner the studio because they have... Basically,
[02:19:50.880 --> 02:19:54.320]   there are a bunch of actors and directors and stuff who worked on the movie who have a piece of the
[02:19:54.320 --> 02:20:00.640]   box office or related to the box office gross, which no longer exists. So they had to... And now
[02:20:00.640 --> 02:20:04.640]   they have presumably to do that with their whole 2021 slate is like figure out a way to
[02:20:04.640 --> 02:20:10.160]   negotiate a rate that is sort of like virtual box office because that is all they have.
[02:20:10.160 --> 02:20:19.840]   Sure. But we're also forgetting that in... Especially in terms of video and audio media that a lot of
[02:20:19.840 --> 02:20:25.200]   some of the most popular shows over the past, you know, 10 years, especially Game of Thrones,
[02:20:25.200 --> 02:20:33.280]   really hit their mark and blew up their audiences because people were torrenting them in countries
[02:20:33.280 --> 02:20:38.720]   where they couldn't stream it. And, you know, I would never encourage anyone to do anything illegal,
[02:20:38.720 --> 02:20:44.640]   but torrenting has been around for 20 years. People know how it works. And we've seen that when shows
[02:20:44.640 --> 02:20:49.600]   like Game of Thrones get torrented, things that are incredibly popular, they get even more popular.
[02:20:50.080 --> 02:20:56.320]   And I would be very surprised if that doesn't wind up having an impact, a positive impact on
[02:20:56.320 --> 02:21:01.600]   people wanting to go see these movies whenever they do get released into theaters because we
[02:21:01.600 --> 02:21:05.760]   know that's going to happen. They're not going to keep a tent pole like Wonder Woman 84 out of
[02:21:05.760 --> 02:21:11.600]   the theater once theaters reopen. They're going to have some big, you know, rewatching experience for
[02:21:11.600 --> 02:21:17.040]   it. So in some ways, this might be a positive thing. Theaters stick around for blockbusters.
[02:21:17.040 --> 02:21:22.880]   People do want the social experience. But there are great movies like Alfonso Cuarón's Roma,
[02:21:22.880 --> 02:21:29.680]   which came out on streaming only on Netflix that will find an audience. These aren't 100 million
[02:21:29.680 --> 02:21:34.320]   dollar movies. These are small, you know, artistic movies that might find an audience
[02:21:34.320 --> 02:21:39.680]   in streaming that they wouldn't have found otherwise. It could be more diversity in films
[02:21:39.680 --> 02:21:44.800]   just because there's a more diverse platform for films. That wouldn't be a bad outcome.
[02:21:45.520 --> 02:21:51.120]   Yeah. I just, I wouldn't want to be, you know, Alamo draft has closed 41 theaters this year.
[02:21:51.120 --> 02:21:54.240]   Yeah. I wouldn't want to be in the theater business this year anyway.
[02:21:54.240 --> 02:21:58.960]   I wouldn't want to be in any business this year. But, you know, here in San Francisco,
[02:21:58.960 --> 02:22:06.000]   we're very blessed. We still have a pretty impressive number of independent theaters.
[02:22:06.000 --> 02:22:12.800]   We have small chains like Alamo. We do have a couple of big multiplexes as well. But, you know,
[02:22:12.800 --> 02:22:19.360]   there, I live in the Richmond district and there are two independent movie theaters here,
[02:22:19.360 --> 02:22:27.680]   you know, a short walk away from where I live for both of them. And it's really remarkable that
[02:22:27.680 --> 02:22:33.760]   they've been able to eke out a living, you know, up until 2020. It would be horrible if they closed
[02:22:33.760 --> 02:22:38.800]   down. But I can't imagine, you know, just seeing where things are going that if they can make it
[02:22:38.800 --> 02:22:47.440]   into, you know, post COVID times, then they will see a resurgence because people be so excited to
[02:22:47.440 --> 02:22:52.480]   go do things. And people, at least here, you know, in this city and other large cities,
[02:22:52.480 --> 02:22:58.720]   love independent movies, they love smaller movies, they love second-run movies. They love it when,
[02:22:58.720 --> 02:23:06.480]   you know, Francis Coppola puts out a new cut of Godfather III, you know, that just happened.
[02:23:06.480 --> 02:23:10.880]   I mean, there's all sorts of things that I think are going to be driving people back into theaters.
[02:23:10.880 --> 02:23:15.520]   But that's unique. San Francisco, that's New York, that's LA, that's a few metros.
[02:23:15.520 --> 02:23:21.680]   I'm sure that's not important box office wise to the big blockbuster movies. They've got to play.
[02:23:21.680 --> 02:23:26.400]   But smaller ones like the Quarron movie that you're talking about. Maybe I'm sure that's a huge
[02:23:26.400 --> 02:23:32.320]   deal to them. Yeah, maybe. Yeah. I would, I would do drive-in. I would go back to a drive-in cinema.
[02:23:32.320 --> 02:23:36.640]   I think that this is the time I did that. Someone opened it. I would love it.
[02:23:36.640 --> 02:23:39.600]   We have drive-ins down here. I don't know about Montreal because of the way.
[02:23:39.600 --> 02:23:47.120]   So we don't have any left here. Yeah. In SF, we did do that. They did a series of
[02:23:47.120 --> 02:23:54.880]   drive-in showings at Fort Mason. I saw Fury Road there, which was great. And, you know,
[02:23:54.880 --> 02:23:58.960]   there are a whole bunch of people that I knew who we all bought tickets and drove our cars in.
[02:23:58.960 --> 02:24:04.960]   And it was nice to see people socially, even if distanced for the first time in months. But
[02:24:04.960 --> 02:24:12.560]   they got this giant screen LED projection just brighter than, you know, brighter than almost the
[02:24:12.560 --> 02:24:17.600]   sun. I mean, it was just incredible. You know, and people would either, you know, pull in with
[02:24:17.600 --> 02:24:22.800]   their hatchbacks or their trucks or they would, you know, whatever, they'd tune the radios to it.
[02:24:22.800 --> 02:24:28.560]   We had our dogs there. I mean, it was great. It was so much fun. But, you know, so if there's that
[02:24:28.560 --> 02:24:33.840]   kind of thing, that would be great. But I think, you know, we're going to see more options coming
[02:24:33.840 --> 02:24:37.840]   out in the post-COVID world and not fewer. Actually, if there's a bigger
[02:24:37.840 --> 02:24:43.520]   group of people that are disadvantaged by COVID-19 as live musicians. Oh, gosh. Yeah.
[02:24:43.520 --> 02:24:47.200]   And there have been, as there's some artists who are performing parking lots,
[02:24:47.200 --> 02:24:56.160]   yeah, kind of drive-in concerts. But I feel really bad for theater performance musicians.
[02:24:56.160 --> 02:25:00.960]   They've been doing live. That's tough. That's really good. I want to make a plug for a really good book.
[02:25:00.960 --> 02:25:05.040]   It depends on how you're feeling and what your mood is. But there's a book called The Song for
[02:25:05.040 --> 02:25:11.840]   A New Day by Sarah Pinsker that was one of the nebula award nominees, I want to say, from 2019.
[02:25:11.840 --> 02:25:18.480]   And it is literally about a pandemic that changes the face of live music performance. Oh, my God.
[02:25:18.480 --> 02:25:25.040]   Technology, how technology is a replacement, but not a good replacement for live music and how people
[02:25:25.040 --> 02:25:31.120]   still after a pandemic want to be in spaces together. And yeah, her timing wasn't the best
[02:25:31.120 --> 02:25:34.240]   because it came out and then the pandemic hit. But it's really a good book.
[02:25:34.240 --> 02:25:34.880]   It was press-
[02:25:34.880 --> 02:25:40.480]   If you're into sci-fi and sort of like near future social aspects of things like what we're living
[02:25:40.480 --> 02:25:45.600]   through, Sarah Pinsker, a song for A New Day, it's really good. Good book. Just, you know,
[02:25:45.600 --> 02:25:51.040]   just a little bit of a downer given what we're going through right now. But yeah, it'll all come
[02:25:51.040 --> 02:25:55.280]   back, but it will be different. I think that's the lesson, right? Is that what comes back won't be
[02:25:55.280 --> 02:26:03.280]   what was there in in 19. It'll be a new thing. Yeah. Alexis O'Haney, who started Reddit and is
[02:26:03.280 --> 02:26:09.200]   married to Serena Williams and is now VC is an investor in a company. I got talked into buying
[02:26:09.200 --> 02:26:16.480]   in on this called Oda, O-D-A dot CO. They send you speakers and on a Saturday night, you just,
[02:26:16.480 --> 02:26:20.720]   you turn on the speakers because they're going to have a live concert brought to your home
[02:26:21.040 --> 02:26:26.400]   over these speakers. And there's, I think, a subscription fee. They do quarterly seasons
[02:26:26.400 --> 02:26:33.680]   and they do series. And I guess for a lot of live musicians, this is their best hope.
[02:26:33.680 --> 02:26:38.160]   But here we are sitting in our house listening to live performance.
[02:26:38.160 --> 02:26:47.280]   My local theater did a streaming Zoom-based theater experience that was adapted from a regular
[02:26:47.280 --> 02:26:54.080]   play, but turned into a streaming thing. And it was really good. It was a lot of fun. And like,
[02:26:54.080 --> 02:26:57.920]   there was audience participation. We had to vote on some stuff that got picked up by the actors.
[02:26:57.920 --> 02:27:03.600]   And I think that it's no substitute for the real thing. But if you can't have the real thing,
[02:27:03.600 --> 02:27:06.400]   I'd like to see the creativity. Got to do something. Yeah.
[02:27:06.400 --> 02:27:12.720]   Yeah, it did a VR theater experience. It was actually quite nice. Really? Quite a lot of fun.
[02:27:12.720 --> 02:27:15.760]   Yeah. But you have to two hours. You have to wear that visor.
[02:27:15.760 --> 02:27:20.080]   You know what? If you've been doing it for a while, if you've built up the...
[02:27:20.080 --> 02:27:21.120]   You've built up the...
[02:27:21.120 --> 02:27:23.600]   Not really yet. You have a callus on your forehead.
[02:27:23.600 --> 02:27:25.680]   Oh, my neck will just become really thick.
[02:27:25.680 --> 02:27:31.680]   Actually, you couldn't lie in your back, right? You don't have to sit up.
[02:27:31.680 --> 02:27:38.160]   Well, yes, because when you lie back, it's depending on which version you look at the scene ceiling.
[02:27:38.160 --> 02:27:39.440]   Oh, she's suddenly looking at the scene ceiling, right? So,
[02:27:39.440 --> 02:27:44.320]   it's not really going to work. But it was kind of cool. You see people in their next to you.
[02:27:44.320 --> 02:27:49.440]   And it's this weird feeling because now being next to you, even when I watch things on shows,
[02:27:49.440 --> 02:27:53.200]   and when I discuss it, is when I watch people in shows and they're doing this dance scene,
[02:27:53.200 --> 02:27:56.480]   and it's this sweaty, hot dance area that I used to be like, "Oh, I can't...
[02:27:56.480 --> 02:28:01.040]   We'd love to go there and just dance until I drop." And now I'm like, "Oh, all the sweating
[02:28:01.040 --> 02:28:06.720]   and heavy breathing." And you can see everyone's spinning. You're like, "No, this is horrible."
[02:28:06.720 --> 02:28:11.040]   So even the way that I look at other people interacting, it's different.
[02:28:11.840 --> 02:28:15.760]   All right, we're going to have to run through about 100 stories because we've been enjoying
[02:28:15.760 --> 02:28:21.280]   the conversation so much I've got a backlog. We're going to do kind of a speed round when we come back
[02:28:21.280 --> 02:28:27.680]   and then wrap this puppy up. It's... When I have a great panel, sometimes the show goes long,
[02:28:27.680 --> 02:28:32.320]   and that's what happened today. Really good conversation. I do want to show you, though,
[02:28:32.320 --> 02:28:37.840]   and you can avert your eyes, Georgia, because this is not for you. Actually, it might be,
[02:28:37.840 --> 02:28:44.960]   it's for the man in your life. I want to show you my Manned Scaped Performance Package.
[02:28:44.960 --> 02:28:51.360]   I think just because we are quarantined doesn't mean we have to look like a Sasquatch.
[02:28:51.360 --> 02:28:53.760]   I guess is my point of view. This is the...
[02:28:53.760 --> 02:29:02.480]   This is the Manned Scaped Performance Package. Let me tell you, this is going to be the holiday
[02:29:02.480 --> 02:29:07.760]   gift for 2020 because so many people have been stuck inside and, guys, let's face it,
[02:29:07.760 --> 02:29:15.360]   our grooming kind of goes downhill, right? You don't need to fix yourself up so much.
[02:29:15.360 --> 02:29:22.080]   This is the men's hygiene bundle that makes for a perfect gift. Included, you get the weed whacker,
[02:29:22.080 --> 02:29:28.880]   which is their ear and nose hair trimmer. There is nothing worse than if I had a friend
[02:29:28.880 --> 02:29:33.280]   who had a forest growing out of his nose, young guy too, his 40s, young for me.
[02:29:33.280 --> 02:29:38.800]   Every time I talk to the guy, I'm looking at him and I can't stop staring at his nostrils.
[02:29:38.800 --> 02:29:46.480]   This don't let that happen to you. Don't have hair. I see you nodding, Georgia.
[02:29:46.480 --> 02:29:53.360]   You don't want hair coming out of your ears. The weed whacker is waterproof. It has a 9,000
[02:29:53.360 --> 02:29:59.920]   RPM motor and a 360 degree rotary dual blade system. High speed is good because it means no
[02:29:59.920 --> 02:30:06.640]   nicking, no pulling. 79% of partners polled admitted long nose hair is a major turn off
[02:30:06.640 --> 02:30:12.800]   and this is the best tool for the job. And by the way, when you get the performance package,
[02:30:12.800 --> 02:30:16.320]   you'll also get a subscription to replacement blades every three months. So you always have
[02:30:16.320 --> 02:30:22.640]   a nice sharp blade on your weed whacker. This is the lawn mower. This is awesome too. Look
[02:30:22.640 --> 02:30:28.640]   how quiet this is. This is a high speed ceramic blade. Look, there's a little LED light so you
[02:30:28.640 --> 02:30:36.080]   never end mystery about what you're where you're navigating. And you know, my friend, Micah Sargent
[02:30:36.080 --> 02:30:43.680]   says that the best example is you often will you'll trim your hedges to make the house look
[02:30:43.680 --> 02:30:52.720]   bigger. You know what I'm saying? This is all about keeping the lawn mower 3.0, my friends.
[02:30:52.720 --> 02:30:58.960]   The dads can't stop talking about it. The teens secretly buy it and and the people in our lives
[02:30:58.960 --> 02:31:05.200]   will love you for it is a season to get manscaped. Get the manscaped performance package. It also
[02:31:05.200 --> 02:31:11.280]   includes their famous liquid formulations, the crop preserver to keep your nether regions smelling
[02:31:11.280 --> 02:31:19.520]   fresh and the crop reviver. I don't know if you have a nether region hygiene routine, but if you did,
[02:31:19.520 --> 02:31:27.040]   it would be definitely, it would definitely involve the crop reviver. And you get two free
[02:31:27.040 --> 02:31:32.640]   gifts, you get your manscaped boxers. And it all comes with this wonderful shed travel bag.
[02:31:32.640 --> 02:31:39.360]   The performance package is the best value manscaped has ever offered. It's hot off the shelves,
[02:31:39.360 --> 02:31:43.760]   plus you're going to get 20% off because you're listening to the show and free shipping. This
[02:31:43.760 --> 02:31:52.480]   is the holiday gift that will make your loved ones smile and look and smell better free shipping
[02:31:52.480 --> 02:32:03.360]   20% off manscaped.com/twit. Make Santa proud this year. Get 20% off plus free shipping manscaped.com/twit.
[02:32:03.360 --> 02:32:07.920]   Maybe your wife should just take a nap. The LED light on the razor is cool.
[02:32:07.920 --> 02:32:13.520]   It's very bright. It's important to be able to see what you're shaving.
[02:32:13.520 --> 02:32:17.280]   You might get some good ideas. It also comes with what I thought was a newspaper,
[02:32:17.280 --> 02:32:23.760]   the manscaped daily news, but actually what it is, spread it out to catch the remnants. But
[02:32:23.760 --> 02:32:30.960]   they do have some good suggestions for how you might want to buy grams. Diagrams.
[02:32:32.960 --> 02:32:38.000]   Is there a flowchart? There's a flowchart serving suggestions. There's also a crossword puzzle.
[02:32:38.000 --> 02:32:42.960]   I do not recommend doing the crossword puzzle while you manscaped. That's just not a good idea.
[02:32:42.960 --> 02:32:52.880]   I don't know Leo. This is kind of nuts. Oh, I got that. Very nice. Well done. Sorry.
[02:32:52.880 --> 02:32:56.800]   See what he did there. That's all I can handle. See what he did there, ladies and gentlemen.
[02:32:56.800 --> 02:33:01.520]   Okay, speed round because we got to go through. There's a ton of these and there were so many
[02:33:01.520 --> 02:33:07.760]   big stories this week. I don't even know how to get through them all. Arguments in front of the
[02:33:07.760 --> 02:33:17.760]   US Supreme Court on the CFAA, the 1986 law that has been used to prosecute everybody from Aaron
[02:33:17.760 --> 02:33:24.000]   Schwartz on down. I mean, it's just a nasty the Computer Fraud and Abuse Act. The case
[02:33:25.280 --> 02:33:31.680]   is involving a Georgia police officer convicted of violating the Computer Fraud and Abuse Act by
[02:33:31.680 --> 02:33:41.920]   accessing a license plate database. The good news is the justices seem to come down on the side of
[02:33:41.920 --> 02:33:50.560]   privacy. Neil Gorsuch said that this was an example of the government trying to broaden the scope of
[02:33:50.560 --> 02:33:57.040]   criminal laws in contestable ways. The DOJ's argument risked, quote, making a federal criminal of us
[02:33:57.040 --> 02:34:04.080]   all. And I think if you remember, I mean, poor Aaron Schwartz, who was prosecuted under the CFAA,
[02:34:04.080 --> 02:34:09.360]   it eventually committed suicide because of it. It's a perfect example of overreach.
[02:34:09.360 --> 02:34:14.560]   And we've seen this time and time again, the feds using the CFAA in ways it clearly isn't intended.
[02:34:16.320 --> 02:34:20.720]   I presume you agree, Seth, on this, that this is a problematic law.
[02:34:20.720 --> 02:34:33.040]   The CFAA is everybody I speak with says it's overly broad. There's almost unanimous agreement,
[02:34:33.040 --> 02:34:39.040]   no matter what perspective somebody is coming to it from on that. This is actually the first time
[02:34:39.040 --> 02:34:45.280]   in its entire existence that it's gone before the Supreme Court. So this is a really sort of
[02:34:45.280 --> 02:34:54.160]   unique moment for the preeminent anti-hacking law in the US. I would hesitate despite what
[02:34:54.160 --> 02:35:01.040]   the questions were and what the justices said. I would hesitate to read too much into them.
[02:35:01.040 --> 02:35:12.240]   We've seen time and again. But it's interesting. And I think it's going to be very important to
[02:35:12.240 --> 02:35:22.880]   see what happens to this case. The president has attempted to tie a repeal of section 230
[02:35:22.880 --> 02:35:29.440]   to the Defense Spending Act. Actually, this is the time of year where Congress does this,
[02:35:29.440 --> 02:35:34.560]   where they've got big spending bills that need to be passed. And it's a great way to sneak something
[02:35:34.560 --> 02:35:43.360]   by. We'll see what happens. But he said, I'll veto it unless you have a repeal of section
[02:35:43.360 --> 02:35:48.160]   230. And of course, the National Defense Authorization Act should not be vetoed.
[02:35:48.160 --> 02:35:58.560]   It's pretty important. I think it would be very interesting for the anti-war folks to find
[02:35:59.920 --> 02:36:08.160]   that Trump is the one who defunded the Pentagon. But we're doing things that happen with this
[02:36:08.160 --> 02:36:16.960]   guy. I don't think that's going to happen. Well, and remember Friday was the day TikTok was
[02:36:16.960 --> 02:36:25.200]   supposed to go out of business. A couple of weeks ago said, guys, what's up? Because we just need
[02:36:25.200 --> 02:36:30.640]   to know that apparently the administration said, look, we're not going to enforce it.
[02:36:30.640 --> 02:36:38.640]   Keep negotiating. And then I think 50 days from now, it'll just magically disappear. I don't know.
[02:36:38.640 --> 02:36:43.840]   Very strange. Ajit Pai, 50 days from now, January 20th will be
[02:36:43.840 --> 02:36:49.760]   also mysteriously disappeared. Now, he's a chairman of the FCC. I don't remember what it does.
[02:36:49.760 --> 02:36:59.200]   It typically a president appoints a new chairman anyway. But I guess Ajit Pai wanted to keep the
[02:36:59.200 --> 02:37:02.960]   news cycle and say, well, I'm leaving. You wanted to leave. You wanted to leave, not get fired.
[02:37:02.960 --> 02:37:06.400]   No, you can't fire me. I quit. You didn't dump me. I dump you.
[02:37:06.400 --> 02:37:11.840]   We will watch with interest because my suspicion is he's already got a job at Verizon.
[02:37:11.840 --> 02:37:17.120]   And he's just ready to be. He's back in his briefcase, ready to move on.
[02:37:18.720 --> 02:37:24.880]   Let's see. What else? Big pressure on a BuzzFeed. And this is actually a BuzzFeed article,
[02:37:24.880 --> 02:37:33.600]   which is also in violation of ICE's request. ICE is demanding that BuzzFeed divulge its sources
[02:37:33.600 --> 02:37:42.720]   on a story about immigration. And of course, BuzzFeed protected sources as all good journalist
[02:37:42.720 --> 02:37:48.240]   enterprises do. ICE even said, not only do we want you to divulge your sources, but you can't tell
[02:37:48.240 --> 02:37:53.120]   anybody that we're asking for this. So of course, BuzzFeed immediately published the story.
[02:37:53.120 --> 02:38:04.240]   Needless to say, it's important that journalists can do their job without repercussions.
[02:38:04.240 --> 02:38:10.240]   David Snyder is a lawyer and executive director of the nonprofit First Amendment Coalition
[02:38:10.240 --> 02:38:15.520]   said, I think this is the best description. If journalists had to divulge sources of information,
[02:38:16.240 --> 02:38:20.320]   sources or information they obtained in the course of reporting, they'd be no better than the
[02:38:20.320 --> 02:38:26.560]   investigative arm of the state. The power of journalism to advance the cause of our democracy
[02:38:26.560 --> 02:38:35.280]   would be severely diminished. So this is just more of the same. I presume BuzzFeed will continue to say,
[02:38:35.280 --> 02:38:41.520]   yeah, no. And at some point this may end up in court, but I would hope a judge would be smart enough
[02:38:41.520 --> 02:38:46.560]   to protect the First Amendment. That is a pretty important part of the Constitution.
[02:38:46.560 --> 02:38:59.520]   Let's see. It looks like the Chinese have had a breakthrough in quantum computing. They are now,
[02:38:59.520 --> 02:39:05.200]   remember it was Google this year who said, we have, what do they call that, the quantum
[02:39:06.400 --> 02:39:12.000]   singularity where we finally have a quantum computer that can do a calculation. The main,
[02:39:12.000 --> 02:39:19.280]   you know, a supercomputer would take hundreds of years to do in mere seconds. This is a big breakthrough,
[02:39:19.280 --> 02:39:24.640]   a second breakthrough in quantum computing. I don't feel like we're that close to quantum
[02:39:24.640 --> 02:39:30.880]   computing suddenly taking over the world, but they are making some progress. And now it's the Chinese
[02:39:32.480 --> 02:39:38.960]   turn to quantum supremacy. Yeah, that's it. Right. And the idea of quantum supremacy is literally
[02:39:38.960 --> 02:39:44.080]   theoretically is great. But like, can you actually build something that can do something that a
[02:39:44.080 --> 02:39:49.040]   regular computer cannot do? Also, you should sign up for Bloomberg's newsletter. Yes, you saw that,
[02:39:49.040 --> 02:39:54.320]   Papa. Even though I already paid Bloomberg a ton of money for access to their ever stopped.
[02:39:54.320 --> 02:39:58.960]   They just repurposed there. You should pay us two way you should subscribe to our newsletter
[02:39:58.960 --> 02:40:03.040]   or give you a gift or telephone. Anyway, quantum supremacy. Yeah, the idea that you could really
[02:40:03.040 --> 02:40:10.560]   build something that a normal computer couldn't do. And China says this research group says they
[02:40:10.560 --> 02:40:13.600]   did something in minutes that would take a normal computer two billion years to do,
[02:40:13.600 --> 02:40:17.840]   which is a long time long computers. Not bad. So yeah, if it's true, that's awesome.
[02:40:17.840 --> 02:40:22.720]   The problem is what was that thing that we're able to do was at a very value at all.
[02:40:22.720 --> 02:40:28.240]   Was decrypting all of our emails. Oh, okay. Nice. 100 trillion times faster
[02:40:28.240 --> 02:40:36.240]   than the world's most advanced supercomputer. It's it's a very difficult thing to do. And it's not
[02:40:36.240 --> 02:40:42.560]   very reliable yet. But hey, it might happen. Look, I mean, they came up with a COVID vaccine in under
[02:40:42.560 --> 02:40:50.080]   a year. That's unheard of. Thanks to modern mRNA technologies. Oh, this is a sad one. I'm going to
[02:40:50.080 --> 02:40:59.680]   end with a sound bite from Puerto Rico. This is the Erisi Bo Observatory. Remember a couple of
[02:40:59.680 --> 02:41:03.120]   weeks ago, we talked about the fact that it had been damaged in the hurricanes of Puerto Rico and
[02:41:03.120 --> 02:41:07.520]   the storms. And the towers, they were concerned that we're going to fall so they were going to
[02:41:07.520 --> 02:41:15.040]   demolish them before they could fall in and damage the dish beneath it. They were unable to do so.
[02:41:15.040 --> 02:41:22.560]   And a few days ago, here's video from Erisi Bo. It collapsed. Let me turn up the sound because I
[02:41:22.560 --> 02:41:33.520]   know you're not getting it. Into the dish. Fortunately, they're no humans around. They were
[02:41:33.520 --> 02:41:39.360]   really concerned. The visitor center is below it and so forth. But that's the end of a telescope
[02:41:39.360 --> 02:41:44.000]   that is operated for, I think close to 60 years, was part of the SETI at home.
[02:41:44.000 --> 02:41:52.080]   Once the largest radio telescope in the world, it was the second, I think, largest radio telescope
[02:41:52.080 --> 02:41:59.600]   and has been used for all sorts of things, including SETI. Contact the movie with Jody Foster. There's
[02:41:59.600 --> 02:42:03.920]   a bunch of early scenes that are set there because that was definitely a place that SETI used.
[02:42:03.920 --> 02:42:11.360]   And yeah, it's a real tragedy not just for radio astronomy, but the whole astronomical community.
[02:42:11.360 --> 02:42:17.280]   So many of them astronomers have filmed memories of going to Puerto Rico and going to Erisi Bo,
[02:42:17.280 --> 02:42:24.160]   which is sort of like way out far away from the main parts of Puerto Rico. And it's like
[02:42:24.160 --> 02:42:29.280]   there's a little village that's grown up around there where all the astronomers are. And it's a
[02:42:29.280 --> 02:42:34.720]   point of pride in Puerto Rico. And it was already kind of
[02:42:34.720 --> 02:42:40.560]   broken and not going to come back, but now it's been completely destroyed. It's really tragic.
[02:42:40.560 --> 02:42:48.960]   The second video is from a drone, which I guess with some foresight had above the observation
[02:42:48.960 --> 02:42:55.600]   platform. And boy, it's dramatic to watch those cables snap and the whole thing tumble down.
[02:42:57.760 --> 02:43:01.840]   They knew it was happening, right? They were trying to do a control demolition so that they would
[02:43:01.840 --> 02:43:06.080]   be able to figure out exactly where to drop it. And it beat them to it.
[02:43:06.080 --> 02:43:18.480]   Okay, sad. Let's see. If you want to go to Singapore, you can have chicken nuggets grown in a vat.
[02:43:18.480 --> 02:43:26.640]   This is the first time ever lab-grown meat has gone on sale. It's a singer-restaurant
[02:43:26.640 --> 02:43:32.640]   Singapore that's selling it. The chicken bites are produced by a US company called Eat,
[02:43:32.640 --> 02:43:36.880]   just you may be aware of them. They have vegan mayonnaise and stuff in the stores.
[02:43:36.880 --> 02:43:42.080]   But there are dozens of firms trying to figure out how to grow chicken, beef, and pork in a vat
[02:43:42.080 --> 02:43:46.880]   to avoid killing animals. Vegetarians will be thrilled. And one of the things, you know,
[02:43:46.880 --> 02:43:50.240]   it's good for the environment. It's good for your health. But one of the things that stops
[02:43:50.240 --> 02:43:56.080]   people like me from being a vegetarian is I like the taste of meat. So now they can
[02:43:56.080 --> 02:44:03.600]   use a few cells of a chicken, feed it plant proteins, grow it in a 1,200 liter bio-reactor
[02:44:03.600 --> 02:44:08.880]   and serve it in a restaurant in Singapore. I would like to taste these, but I'm not going to
[02:44:08.880 --> 02:44:12.800]   fly to Singapore to do it. They say, I would try them. I would try them. Maybe not one of the
[02:44:12.800 --> 02:44:17.280]   first people to try it. No, it's like the vaccine. I watched all those out. Yeah.
[02:44:17.280 --> 02:44:21.120]   Yeah, wait. A few people try it out for me and I'll see how it goes. But I try it out.
[02:44:21.120 --> 02:44:25.600]   It can't be more artificial than a real chicken nugget, right?
[02:44:25.600 --> 02:44:29.280]   Yeah, I don't think there's any chicken in a chicken nugget. So yeah,
[02:44:29.280 --> 02:44:31.920]   pretty artificial already. It can't be any worse.
[02:44:31.920 --> 02:44:40.880]   All right. So that's, I think it's going to be, they think maybe by 20, 40 in the next 20 years,
[02:44:40.880 --> 02:44:46.160]   most meat will come from a vat because as they grow it and they scale up, it can get cheaper and
[02:44:46.160 --> 02:44:49.920]   cheaper. Right now it's fairly expensive. But so was the Impossible Burger. And now they're selling
[02:44:49.920 --> 02:44:54.960]   it at White Castle. So the difference is that a possible burger is plant protein. It's not an
[02:44:54.960 --> 02:45:01.280]   animal protein. It's just a burger like experience. This would be actual meat, but just not made by
[02:45:01.280 --> 02:45:09.120]   an animal. So I guess you could argue that it's not really meat. I don't know. I don't know.
[02:45:09.120 --> 02:45:15.680]   All right. I think we should wrap this up. There are still many, many stories I didn't get to.
[02:45:16.400 --> 02:45:21.680]   These things happen. I thank you for being here. Great panel.
[02:45:21.680 --> 02:45:27.520]   Jason Snell is six colors.com with Dan Morin, but also we really should plug the
[02:45:27.520 --> 02:45:30.800]   incomparable, all the podcasts you do on the,
[02:45:30.800 --> 02:45:36.160]   all the great podcast. I've been doing a series that's at six colors and there's a podcast on
[02:45:36.160 --> 02:45:43.600]   really FM called 20 Max for 2020, which are little audio documentaries and essays and little videos
[02:45:43.600 --> 02:45:50.320]   about what I think are the 12, the 20 most notable Macs in history counting down from 20 to number
[02:45:50.320 --> 02:45:55.680]   one. We are tomorrow will be at number four. So people should check those out. And the podcast,
[02:45:55.680 --> 02:46:00.080]   I'm really proud of the podcast, especially, which is a relay dot FM.
[02:46:00.080 --> 02:46:05.520]   I've had every one of these, I think. So have you. Yeah. Well, they're good. That's, that's
[02:46:05.520 --> 02:46:08.960]   what they're notable. Many of them are actually bad, but they're notable for how bad they were
[02:46:08.960 --> 02:46:13.920]   with the Macintosh portable or the 20th anniversary Macintosh. It's not the best Macs. It's the most
[02:46:13.920 --> 02:46:18.400]   interesting or notable Macs. I got the Macintosh portables over there. I think we gave away.
[02:46:18.400 --> 02:46:22.880]   Didn't we, John the 20th anniversary Macintosh? We gave that away. That's how bad it was.
[02:46:22.880 --> 02:46:28.000]   Yeah. It's, it's a prototype that escaped the lab, but I actually bought a Mac portable for this
[02:46:28.000 --> 02:46:32.640]   project and it's ridiculously heavy, but the keyboard's great. Yeah. Ridiculous. You should
[02:46:32.640 --> 02:46:35.920]   have just come to visit me. I got one right over there. It's the one with a track ball in the
[02:46:35.920 --> 02:46:40.720]   thing, right? Yeah. Yeah. On the side. On the side. Oh, that one. That one's like 30 pounds. That's
[02:46:40.720 --> 02:46:46.240]   huge. That one. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. It was the sewing machine Mac. Yeah.
[02:46:46.240 --> 02:46:52.160]   Well, that's fun. 20 Macs for 2020 with Jason Snell, yet another podcast. The guy,
[02:46:52.160 --> 02:46:55.920]   it's like an NPR style podcast. It's like super edited and I scripted and I've got a bunch of
[02:46:55.920 --> 02:47:00.320]   interviews with people who know and care about old Macs and stuff and it's a lot of fun and they're
[02:47:00.320 --> 02:47:05.120]   all like 15 minutes along. They're all super short. I'm really curious because I feel like,
[02:47:05.120 --> 02:47:11.440]   you know, obviously this is the opposite of an NPR podcast. It's not edited. It's sprawling.
[02:47:11.440 --> 02:47:17.120]   It goes on and on. It's a mess, but that's what I've always done for 15 years and I look at the
[02:47:17.120 --> 02:47:21.280]   success of the NPR style and I wonder, well, maybe we should do it that way. Is it a lot of work?
[02:47:21.280 --> 02:47:25.680]   Oh, yeah. It's a lot of work. It's the most work of the entire project has been doing that.
[02:47:25.680 --> 02:47:31.120]   Yeah. I guess. But it was a great creative challenge and I'm using a D script, which,
[02:47:31.120 --> 02:47:36.160]   if you haven't heard about it, is basically you feed audio files into this thing and it
[02:47:36.160 --> 02:47:42.080]   transcribes them and then you edit it like you're editing a Word document and audio comes out.
[02:47:42.080 --> 02:47:46.320]   Oh, are you using that? Yeah, that's a really interesting product. I've been very interested in
[02:47:46.320 --> 02:47:51.040]   that. Yeah. So that made it possible. Yeah. Otherwise, it would have killed me, but that made it
[02:47:51.040 --> 02:47:55.520]   possible. And then you can export the beauty of D script is that you can export it out into
[02:47:55.520 --> 02:48:00.480]   any popular editing software in Project form. So you can clean it up. So I take it out to logic
[02:48:00.480 --> 02:48:05.680]   and then I clean it up and add music and stuff like that. But that way, I can literally use my
[02:48:05.680 --> 02:48:10.480]   editorial skills as a writer and an editor to look at my interviews and move quotes around and
[02:48:10.480 --> 02:48:15.040]   stuff and build a script out of that. And that's what's enabled me to do it. It's pretty amazing
[02:48:15.040 --> 02:48:20.160]   technology. And that's interesting. So it helps you get it to kind of the first pass. What do you
[02:48:20.160 --> 02:48:26.480]   would you say? 80, 90% there? Yeah, I'd say 90%. It's pretty close. And now they've got a video
[02:48:27.280 --> 02:48:32.080]   version or video features too. So anybody who's doing like a documentary where you've got hours
[02:48:32.080 --> 02:48:36.640]   and hours of interviews to go through, can you imagine all you have to do is edit the text of
[02:48:36.640 --> 02:48:41.600]   your interview and you've got video at the end of it. Brilliant. It's brilliant. I like what you're
[02:48:41.600 --> 02:48:44.640]   saying because I never trusted to do it by itself. What I like what you're saying is you get the
[02:48:44.640 --> 02:48:49.360]   logic project and then you tweak it a little bit, make sure that's just a moment for me was when I
[02:48:49.360 --> 02:48:53.040]   knew I could take it out. I mean, they have a rudimentary editor in the app, but you can just
[02:48:53.040 --> 02:48:58.240]   take it out and then use your video editor or your audio editor to do the fine tuning. But it
[02:48:58.240 --> 02:49:03.040]   does a it's pretty amazing. You remove one word and then you play it and that word has just gone.
[02:49:03.040 --> 02:49:08.240]   Wow. It's pretty cool. Wow. Wow. Pretty soon we don't even have to do these shows, which is
[02:49:08.240 --> 02:49:12.320]   just do a Twitter. Computers will talk to each other. Computer, just do a Twitter.
[02:49:12.320 --> 02:49:20.560]   Thank you to the wonderful Georgia Dow. You are a sunshine in on a cloudy day.
[02:49:21.440 --> 02:49:25.280]   Thank you for being here. Everybody should go to anxiety-videos.com.
[02:49:25.280 --> 02:49:31.120]   If you are struggling a little bit and you want some advice, Georgia at Westmounttherapy.com,
[02:49:31.120 --> 02:49:35.120]   she maybe can steer you in the right direction. Thank you for being here, Georgia. We really
[02:49:35.120 --> 02:49:42.000]   appreciate it. Thank you for having me. Yes. And to Seth Rosenblatt, Wolverine, basically,
[02:49:42.000 --> 02:49:49.440]   minus the Adam and Emclaws. He's just, yeah. We don't know. It's going to happen.
[02:49:50.880 --> 02:49:56.240]   He is, of course, the editor-in-chief at v-parallax.com covering security primarily, right?
[02:49:56.240 --> 02:50:02.240]   Yep. It's getting privacy. Privacy, of course, to go hand in hand at Seth R on the Twitter.
[02:50:02.240 --> 02:50:05.360]   Anything you want to plug? You got to podcast. You're doing anything like that.
[02:50:05.360 --> 02:50:11.120]   I don't. I do have some news that I'm going to hold off on because I'm just not quite ready for
[02:50:11.120 --> 02:50:16.320]   it. But we are going to be making some interesting announcements very shortly. And I'm sure I'll
[02:50:16.320 --> 02:50:22.080]   see you again soon thereafter. With the best thing to do, be to go to the parallax and just follow
[02:50:22.080 --> 02:50:28.560]   you there and see what's going on. No, the best thing would be to send me a check. But the second
[02:50:28.560 --> 02:50:34.320]   best thing would be to go to the parallax and follow me there. I'll send you an LED lit
[02:50:34.320 --> 02:50:42.240]   clipper. Yeah. Sure. But also sign up for the newsletter.
[02:50:42.240 --> 02:50:48.560]   Newsletter. That's the place. Yeah. Yep. Th e dash parallax parallax.
[02:50:48.560 --> 02:50:54.480]   Dot com. We do Twitter every Sunday afternoon. It's right after the tech guys. It's about 230
[02:50:54.480 --> 02:51:00.560]   Pacific 530 Eastern time. 2230 UTC. If you want to watch us do it live or listen live. The streams
[02:51:00.560 --> 02:51:05.840]   are a twit.tv slash live. If you're if you're consuming it live, you should probably join us
[02:51:05.840 --> 02:51:10.480]   in the chat room. That's where a bunch of other people watching live are chatting. I R C dot
[02:51:10.480 --> 02:51:16.560]   twit.tv. Some of the best people there. They're really, really fun bunch on demand versions of
[02:51:16.560 --> 02:51:22.000]   the show. All of our shows available at our website, twit.tv. There's a YouTube channel for
[02:51:22.000 --> 02:51:26.880]   this week in tech. You can also subscribe in your favorite podcast application. That
[02:51:26.880 --> 02:51:30.080]   probably the best way because then you're going to get it the minute it's available
[02:51:30.080 --> 02:51:35.840]   of a Sunday evening or Monday morning just in time for your commute.
[02:51:36.560 --> 02:51:41.760]   Thank you everybody. We really appreciate your being here. Thanks Leo. Dress up your place next
[02:51:41.760 --> 02:51:47.520]   time I want to be seeing you all with your holly and your and your bows of evergreen.
[02:51:47.520 --> 02:51:49.280]   Another twit.
[02:51:49.440 --> 02:51:50.480]   It's amazing.
[02:51:50.480 --> 02:52:00.080]   Do the twit. All right. Do the twit, baby. Do the twit. All right. Do the twit.
[02:52:00.080 --> 02:52:10.120]   [BLANK_AUDIO]

