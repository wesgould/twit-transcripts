;FFMETADATA1
title=Written to Binge
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=688
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.000]   It's time for Twit this week in Tech.
[00:00:03.000 --> 00:00:04.200]   We've got a great panel for you.
[00:00:04.200 --> 00:00:06.640]   Ron Richards from All About AndrÃ© Cristina Warr
[00:00:06.640 --> 00:00:08.840]   and film girl is back from Microsoft.
[00:00:08.840 --> 00:00:11.160]   And Mark Millie in the Bloomberg Business Week.
[00:00:11.160 --> 00:00:14.200]   I'm going to ask Mark about that big Super Micro story.
[00:00:14.200 --> 00:00:16.080]   See what he has to say about that.
[00:00:16.080 --> 00:00:19.000]   Google plus bye bye, but why?
[00:00:19.000 --> 00:00:21.320]   And of course, all the details from Google's
[00:00:21.320 --> 00:00:22.560]   made with Google event.
[00:00:22.560 --> 00:00:25.000]   It's all coming up next on Twit.
[00:00:28.000 --> 00:00:30.080]   Netcast you love.
[00:00:30.080 --> 00:00:31.480]   From people you trust.
[00:00:31.480 --> 00:00:36.480]   This is Twit.
[00:00:36.480 --> 00:00:45.640]   This is Twit this week in Tech.
[00:00:45.640 --> 00:00:50.840]   Episode 688 recorded Sunday, October 14, 2018.
[00:00:50.840 --> 00:00:54.040]   Written to binge.
[00:00:54.040 --> 00:00:56.800]   This week in Tech is brought to you by Molecule.
[00:00:56.800 --> 00:01:00.280]   Molecule is the world's first molecular air purifier
[00:01:00.280 --> 00:01:03.360]   that reduces symptoms for allergy and asthma sufferers.
[00:01:03.360 --> 00:01:07.080]   For $75 off your first order, visit Molecule.com.
[00:01:07.080 --> 00:01:10.240]   And enter the promo code Twit75.
[00:01:10.240 --> 00:01:13.560]   And by FreshBooks, the ridiculously easy to use cloud
[00:01:13.560 --> 00:01:16.480]   accounting software helping small business owners thrive.
[00:01:16.480 --> 00:01:21.080]   Try it free for 30 days at freshbooks.com/twit.
[00:01:21.080 --> 00:01:22.720]   And by CashFly.
[00:01:22.720 --> 00:01:26.200]   Make this the last month your CDN bill gives you a headache.
[00:01:26.200 --> 00:01:28.040]   Join thousands of others like me who
[00:01:28.040 --> 00:01:31.800]   trust CashFly's reliable network for a complimentary detail
[00:01:31.800 --> 00:01:34.880]   analysis of your current CDN bill and usage trends.
[00:01:34.880 --> 00:01:38.200]   Is it twit.cashfly.com?
[00:01:38.200 --> 00:01:39.440]   And by Ring.
[00:01:39.440 --> 00:01:42.200]   Ring's alarm security kit is a smarter way
[00:01:42.200 --> 00:01:44.120]   to protect your entire home.
[00:01:44.120 --> 00:01:47.240]   Go to Ring.com/twit to learn how you can get whole home
[00:01:47.240 --> 00:01:49.240]   security for only $10 a month.
[00:01:49.240 --> 00:01:55.360]   It's time for Twit this week in Tech to show we cover the latest.
[00:01:55.360 --> 00:01:59.320]   Tech News, some of the best people in technology.
[00:01:59.320 --> 00:02:01.920]   Mark Billions here, Bloomberg Business Week.
[00:02:01.920 --> 00:02:03.600]   I haven't, we haven't, had you in ages.
[00:02:03.600 --> 00:02:05.400]   Thank you for coming up and--
[00:02:05.400 --> 00:02:06.560]   Yeah, thanks for having me.
[00:02:06.560 --> 00:02:07.600]   It's always great.
[00:02:07.600 --> 00:02:10.680]   I have some questions for you though about Bloomberg Business
[00:02:10.680 --> 00:02:11.640]   Week.
[00:02:11.640 --> 00:02:12.760]   And I just want you to know--
[00:02:12.760 --> 00:02:13.360]   Anytime.
[00:02:13.360 --> 00:02:17.120]   --I defended Bloomberg Business Week all week long.
[00:02:17.120 --> 00:02:17.760]   And I had to.
[00:02:17.760 --> 00:02:18.240]   Appreciate it.
[00:02:18.240 --> 00:02:19.280]   We'll talk about that in a second.
[00:02:19.280 --> 00:02:20.560]   Also hear from all about Android.
[00:02:20.560 --> 00:02:22.440]   Ron Richards, hi Ron.
[00:02:22.440 --> 00:02:23.600]   Hey Leo, good to be back.
[00:02:23.600 --> 00:02:24.320]   Nice to have you.
[00:02:24.320 --> 00:02:25.920]   You might have a little something to talk about.
[00:02:25.920 --> 00:02:28.160]   I guess Google released some stuff.
[00:02:28.160 --> 00:02:28.720]   Big week.
[00:02:28.720 --> 00:02:29.560]   Big week.
[00:02:29.560 --> 00:02:31.400]   Big week for the Goog.
[00:02:31.400 --> 00:02:35.240]   And from Microsoft, she's a senior cloud developer advocate.
[00:02:35.240 --> 00:02:38.200]   But of course, we love her as film girl from formerly
[00:02:38.200 --> 00:02:39.440]   Mashable Christina Warren.
[00:02:39.440 --> 00:02:40.600]   Hey Christina.
[00:02:40.600 --> 00:02:42.000]   Hey Leo, good to see you.
[00:02:42.000 --> 00:02:43.880]   Wonderful to see you.
[00:02:43.880 --> 00:02:46.960]   I love the bright pink purse, I guess, behind you.
[00:02:46.960 --> 00:02:48.040]   Yeah, the purses.
[00:02:48.040 --> 00:02:50.160]   So my office is now basically clean.
[00:02:50.160 --> 00:02:51.520]   So the last couple of times I've done
[00:02:51.520 --> 00:02:53.320]   to what I've had to do it from my office at Microsoft,
[00:02:53.320 --> 00:02:55.120]   because my actual office is so messy.
[00:02:55.120 --> 00:02:56.120]   Yeah.
[00:02:56.120 --> 00:02:57.920]   People are like, oh, there are boxes in the background.
[00:02:57.920 --> 00:02:58.640]   No, this is great.
[00:02:58.640 --> 00:02:59.880]   My husband did this.
[00:02:59.880 --> 00:03:01.920]   But I have this purse wall behind me.
[00:03:01.920 --> 00:03:04.960]   So you can see my full collection of--
[00:03:04.960 --> 00:03:08.440]   I didn't realize I had so many Kate Spade bags until they
[00:03:08.440 --> 00:03:09.120]   were all in one place.
[00:03:09.120 --> 00:03:13.680]   And I was like, oh, I do have a lot.
[00:03:13.680 --> 00:03:17.480]   We have much to talk about.
[00:03:17.480 --> 00:03:19.480]   Actually, it was really last week's story.
[00:03:19.480 --> 00:03:21.680]   But as long as we got somebody from Bloomberg here,
[00:03:21.680 --> 00:03:26.280]   I thought I'd ask about the Super Micro story, which, of course,
[00:03:26.280 --> 00:03:29.880]   really galvanized people late in the week last week.
[00:03:29.880 --> 00:03:33.040]   And we talked about it on all of our shows.
[00:03:33.040 --> 00:03:37.160]   The story from Bloomberg that Super Micro, which
[00:03:37.160 --> 00:03:40.720]   is an American company, but manufactures its motherboards.
[00:03:40.720 --> 00:03:44.680]   And they're very widely used in servers--
[00:03:44.680 --> 00:03:47.320]   not only all over the world, but in the Department of Defense,
[00:03:47.320 --> 00:03:48.680]   Apple, Amazon.
[00:03:48.680 --> 00:03:51.760]   The Super Micro have been caught, or one of its suppliers
[00:03:51.760 --> 00:03:56.040]   have been caught, putting a surveillance chip on the motherboard.
[00:03:56.040 --> 00:03:58.920]   This story by people you know, right?
[00:03:58.920 --> 00:03:59.680]   Jordan Robertson.
[00:03:59.680 --> 00:04:02.160]   Jordan Robertson and Mike Riley.
[00:04:02.160 --> 00:04:04.880]   Had 17 anonymous sources.
[00:04:04.880 --> 00:04:07.200]   Seemed very well sourced.
[00:04:07.200 --> 00:04:09.080]   We were talking before the show.
[00:04:09.080 --> 00:04:13.360]   You have a brand new EIC there, came from the economist,
[00:04:13.360 --> 00:04:14.480]   who you said oversaw.
[00:04:14.480 --> 00:04:16.320]   It's been a couple years, but yeah, yeah.
[00:04:16.320 --> 00:04:19.640]   But he's very highly respected in the industry.
[00:04:19.640 --> 00:04:25.080]   And I've been saying all week long, Bloomberg is so prestigious,
[00:04:25.080 --> 00:04:27.080]   there's no way this is--
[00:04:27.080 --> 00:04:29.720]   the only way you'd have to say it was fabricated.
[00:04:29.720 --> 00:04:32.000]   And there's no way that's possible.
[00:04:32.000 --> 00:04:34.120]   We have nothing to gain by doing that, and everything
[00:04:34.120 --> 00:04:38.360]   to lose by either fabricating something or even
[00:04:38.360 --> 00:04:39.720]   handling this irresponsibly.
[00:04:39.720 --> 00:04:44.440]   We knew what a huge story this was throughout the reporting,
[00:04:44.440 --> 00:04:48.760]   which had been going for well over a year.
[00:04:48.760 --> 00:04:52.760]   And spent months and months talking with the companies
[00:04:52.760 --> 00:04:56.160]   and getting their denials and repeated denials
[00:04:56.160 --> 00:04:59.640]   and doing more reporting.
[00:04:59.640 --> 00:05:03.000]   And everything I know about the story
[00:05:03.000 --> 00:05:06.400]   is that it's 100% solid, but it's frustrating
[00:05:06.400 --> 00:05:09.000]   that we couldn't get anybody to go on the record.
[00:05:09.000 --> 00:05:11.320]   That's kind of like the nature of reporting
[00:05:11.320 --> 00:05:13.520]   on topics of national security.
[00:05:13.520 --> 00:05:15.480]   We're never going to get somebody from these agencies.
[00:05:15.480 --> 00:05:18.760]   It's the subject of a top secret government investigation.
[00:05:18.760 --> 00:05:19.800]   But they have--
[00:05:19.800 --> 00:05:22.720]   People risk jail time if they talk to you on the record.
[00:05:22.720 --> 00:05:25.240]   Yeah, I mean, look at what the Trump administration is.
[00:05:25.240 --> 00:05:27.240]   They're cracked down on leakers.
[00:05:27.240 --> 00:05:30.160]   These people have nothing to gain by putting their names
[00:05:30.160 --> 00:05:31.400]   on the record.
[00:05:31.400 --> 00:05:35.280]   But our reporters had four very well-placed,
[00:05:35.280 --> 00:05:40.280]   independent US officials who all said Apple was specifically
[00:05:40.280 --> 00:05:43.440]   a target of this despite Apple saying it wasn't us,
[00:05:43.440 --> 00:05:45.680]   Samsung saying it wasn't us.
[00:05:45.680 --> 00:05:47.520]   The reporting was solid.
[00:05:47.520 --> 00:05:52.080]   And at some point, we just had to go with what we knew to be true.
[00:05:52.080 --> 00:05:56.120]   It almost becomes for the rest of us, the unwashed masses,
[00:05:56.120 --> 00:05:58.800]   of they said, they said story, because there's
[00:05:58.800 --> 00:06:02.320]   nothing to support it in either direction.
[00:06:02.320 --> 00:06:03.760]   I completely agree.
[00:06:03.760 --> 00:06:05.560]   And I've said this.
[00:06:05.560 --> 00:06:07.760]   I'm not saying it to you to kiss your butt.
[00:06:07.760 --> 00:06:09.560]   But Bloomberg Business--
[00:06:09.560 --> 00:06:11.760]   but you don't put this on the cover,
[00:06:11.760 --> 00:06:14.200]   unless you're absolutely sure.
[00:06:14.200 --> 00:06:17.080]   But that raises the question, how can Apple say,
[00:06:17.080 --> 00:06:19.600]   on this we can be very clear.
[00:06:19.600 --> 00:06:21.360]   We've never found malicious chips.
[00:06:21.360 --> 00:06:25.160]   We're unaware of any such investigation.
[00:06:25.160 --> 00:06:28.080]   And many people on our shows said,
[00:06:28.080 --> 00:06:30.880]   Apple is not the kind of company that would lie about this.
[00:06:30.880 --> 00:06:33.960]   And now we've really got a conundrum.
[00:06:33.960 --> 00:06:34.400]   Yeah.
[00:06:34.400 --> 00:06:39.560]   I mean, our reporting, not to my knowledge,
[00:06:39.560 --> 00:06:43.720]   shows that Tim Cook was directly involved in reporting this
[00:06:43.720 --> 00:06:46.720]   to government officials, or that even their chief security
[00:06:46.720 --> 00:06:48.520]   officer was involved in doing that.
[00:06:48.520 --> 00:06:52.840]   But our reporters talked to company insiders
[00:06:52.840 --> 00:06:59.000]   who were knowledgeable about the specifics of this attack.
[00:06:59.000 --> 00:07:02.640]   And to government officials who were knowledgeable of Apple
[00:07:02.640 --> 00:07:05.720]   being a victim of it.
[00:07:05.720 --> 00:07:08.200]   To their credits, Jordan and Michael
[00:07:08.200 --> 00:07:11.280]   published the statements from Apple Amazon,
[00:07:11.280 --> 00:07:13.640]   Super Micro China denied it.
[00:07:13.640 --> 00:07:15.560]   The Department of Homeland Security kind of gave it
[00:07:15.560 --> 00:07:17.440]   a lukewarm denial saying, well, we currently
[00:07:17.440 --> 00:07:21.040]   don't think anything like this is going on.
[00:07:21.040 --> 00:07:23.600]   Honestly, the real bottom line and the important thing
[00:07:23.600 --> 00:07:25.920]   that this story brought up is that this stuff does happen.
[00:07:25.920 --> 00:07:28.200]   We know it happens.
[00:07:28.200 --> 00:07:31.480]   We don't know the specifics about Super Micro,
[00:07:31.480 --> 00:07:34.600]   but you don't need to believe that it happened at Super Micro
[00:07:34.600 --> 00:07:36.520]   to know that this happens all the time,
[00:07:36.520 --> 00:07:39.520]   that the US does it as well as other governments,
[00:07:39.520 --> 00:07:41.600]   that often it's done in transit, not necessarily
[00:07:41.600 --> 00:07:45.560]   at the factory, but that firmware hacks, hardware hacks
[00:07:45.560 --> 00:07:48.680]   happen because you want to target an attack
[00:07:48.680 --> 00:07:53.800]   as server hardware is on its way to a company in transit.
[00:07:53.800 --> 00:07:54.800]   We know that happens.
[00:07:54.800 --> 00:07:55.960]   We know the NSA has done it.
[00:07:55.960 --> 00:07:57.800]   It was part of the Snowden released materials.
[00:07:57.800 --> 00:07:59.200]   The NSA has done that.
[00:07:59.200 --> 00:08:02.920]   So for the purpose of this reporting,
[00:08:02.920 --> 00:08:07.560]   we had a mountain of evidence of the Super Micro attack
[00:08:07.560 --> 00:08:10.160]   and how prevalent it was and how vulnerable they were
[00:08:10.160 --> 00:08:10.960]   to something like this.
[00:08:10.960 --> 00:08:14.240]   But yeah, I think the point that the security community who
[00:08:14.240 --> 00:08:17.560]   have been very skeptical about the story, as many people have,
[00:08:17.560 --> 00:08:20.560]   but the point that they've been trying to make is, regardless
[00:08:20.560 --> 00:08:24.280]   of whether or not this is true, it's a problem
[00:08:24.280 --> 00:08:25.240]   across the supply chain.
[00:08:25.240 --> 00:08:28.640]   The supply chain is a black box to everybody.
[00:08:28.640 --> 00:08:31.760]   Even the companies that put their logos on the products
[00:08:31.760 --> 00:08:35.320]   have no idea many of the things that happen in the supply chain.
[00:08:35.320 --> 00:08:38.560]   It's part of the reason why we'll often
[00:08:38.560 --> 00:08:43.480]   find out about what's in the next iPhone or because they--
[00:08:43.480 --> 00:08:46.880]   we've set up a system where everything is farmed out
[00:08:46.880 --> 00:08:50.160]   to hundreds of different companies.
[00:08:50.160 --> 00:08:53.040]   And anything can happen in between the time
[00:08:53.040 --> 00:08:56.200]   that the components are manufactured,
[00:08:56.200 --> 00:08:57.440]   to the time that they're assembled,
[00:08:57.440 --> 00:08:59.600]   to the time that they make it to the states
[00:08:59.600 --> 00:09:02.840]   or wherever you live and to your doorstep.
[00:09:02.840 --> 00:09:06.080]   Brian Krebs, you saw we flashed it up briefly,
[00:09:06.080 --> 00:09:09.040]   said that supply chain security is the whole enchilada,
[00:09:09.040 --> 00:09:13.400]   but the problem is nobody's willing to pay for it.
[00:09:13.400 --> 00:09:17.640]   We already save so much money by offshoring this stuff
[00:09:17.640 --> 00:09:19.840]   that nobody's even thinking about bringing it back
[00:09:19.840 --> 00:09:21.960]   to the United States.
[00:09:21.960 --> 00:09:26.040]   Bruce Schneider's got a new book, "Push Button to Kill People"
[00:09:26.040 --> 00:09:27.320]   or something like that.
[00:09:27.320 --> 00:09:31.880]   In which he talks about the fact that security, at this point,
[00:09:31.880 --> 00:09:35.560]   has to be-- in fact, he wrote an op-ed in the New York Times
[00:09:35.560 --> 00:09:37.960]   that he said this has to be government regulation
[00:09:37.960 --> 00:09:40.560]   because no company is going to spend the money
[00:09:40.560 --> 00:09:42.960]   to properly secure its hardware.
[00:09:42.960 --> 00:09:46.840]   And with IoT on the horizon, with self-driving cars
[00:09:46.840 --> 00:09:49.880]   on the horizon, it becomes more than your computer's
[00:09:49.880 --> 00:09:52.120]   crashing or they're stealing your credit card.
[00:09:52.120 --> 00:09:54.400]   It becomes a serious problem.
[00:09:54.400 --> 00:09:57.640]   And I don't know if we have either the will
[00:09:57.640 --> 00:09:59.200]   or the ability to fix it.
[00:09:59.200 --> 00:10:01.400]   Another important point to make on that
[00:10:01.400 --> 00:10:05.720]   is another one of the conspiracy theories around the story
[00:10:05.720 --> 00:10:08.960]   because it does rely, to a large extent,
[00:10:08.960 --> 00:10:13.040]   on government officials is that, oh, this is part of the Trump
[00:10:13.040 --> 00:10:13.480]   populist--
[00:10:13.480 --> 00:10:16.320]   Disinformation story, yes.
[00:10:16.320 --> 00:10:19.440]   It's important to note that the reporting on the story
[00:10:19.440 --> 00:10:23.440]   and a lot of the evidence came during the Obama administration
[00:10:23.440 --> 00:10:26.320]   and those conversations and the reporting continued
[00:10:26.320 --> 00:10:27.440]   into the Trump administration.
[00:10:27.440 --> 00:10:28.160]   That's what's fascinating.
[00:10:28.160 --> 00:10:29.280]   This is a three-year story.
[00:10:29.280 --> 00:10:31.200]   These guys have been working on this for some time.
[00:10:31.200 --> 00:10:31.480]   Yeah.
[00:10:31.480 --> 00:10:33.160]   Started in 2015.
[00:10:33.160 --> 00:10:35.080]   So yeah.
[00:10:35.080 --> 00:10:37.520]   This is a push by the government to bring
[00:10:37.520 --> 00:10:39.120]   manufacture back to the United States.
[00:10:39.120 --> 00:10:40.320]   That's not going to happen anyway.
[00:10:40.320 --> 00:10:42.240]   And it doesn't even necessarily need
[00:10:42.240 --> 00:10:46.200]   to happen for at least part of this problem to be resolved.
[00:10:46.200 --> 00:10:47.960]   I mean, a lot of companies could just
[00:10:47.960 --> 00:10:52.040]   take more responsibility, more ownership, over the process
[00:10:52.040 --> 00:10:56.080]   by owning the factories in China that
[00:10:56.080 --> 00:11:00.040]   manufactures these components and owning the assembly plants
[00:11:00.040 --> 00:11:03.480]   or at least being more directly involved in the process
[00:11:03.480 --> 00:11:07.040]   where as now it just goes to dozens or hundreds
[00:11:07.040 --> 00:11:09.800]   of different vendors who all have a piece in things
[00:11:09.800 --> 00:11:12.200]   and then it's assembled somewhere else.
[00:11:12.200 --> 00:11:15.800]   In 2007, the Department of Defense
[00:11:15.800 --> 00:11:18.840]   initiated a program with IBM called the Trusted Foundry
[00:11:18.840 --> 00:11:23.760]   program specifically to address this supply chain issue
[00:11:23.760 --> 00:11:26.280]   to create trusted suppliers, whether they're
[00:11:26.280 --> 00:11:29.280]   in the US or elsewhere, to secure manufacturing
[00:11:29.280 --> 00:11:31.520]   infrastructure.
[00:11:31.520 --> 00:11:33.440]   That was 11 years ago.
[00:11:33.440 --> 00:11:36.600]   I don't know where the Trusted Foundry program is today.
[00:11:36.600 --> 00:11:39.400]   But we've seen this coming as we've
[00:11:39.400 --> 00:11:40.840]   internationalized manufacture.
[00:11:40.840 --> 00:11:41.640]   This has been coming.
[00:11:41.640 --> 00:11:43.080]   So there's no question.
[00:11:43.080 --> 00:11:45.120]   We've got a problem.
[00:11:45.120 --> 00:11:45.620]   Yeah.
[00:11:45.620 --> 00:11:47.400]   The only thing I would say, just I guess
[00:11:47.400 --> 00:11:49.000]   somebody who's a complete outsider,
[00:11:49.000 --> 00:11:52.040]   I'm kind of with Uleo in so far as I'm not ready to say
[00:11:52.040 --> 00:11:54.600]   that this is something that didn't happen because knowing
[00:11:54.600 --> 00:11:57.400]   how journalism works and having been a reporter for many years,
[00:11:57.400 --> 00:11:59.280]   you don't put a story like this on your cover
[00:11:59.280 --> 00:12:02.520]   unless you feel like all your boxes are checked.
[00:12:02.520 --> 00:12:04.640]   But as a reader, there wasn't going
[00:12:04.640 --> 00:12:07.240]   to be-- there were lacking technical details.
[00:12:07.240 --> 00:12:09.400]   And I think that is--
[00:12:09.400 --> 00:12:11.080]   maybe that's the wrong focus, but I
[00:12:11.080 --> 00:12:13.280]   think that that's where a lot of the focus has been on this
[00:12:13.280 --> 00:12:15.720]   because we don't know what the technical details are
[00:12:15.720 --> 00:12:18.520]   and what things might be right or what things might be wrong
[00:12:18.520 --> 00:12:21.080]   because, again, it seems like most of these sources
[00:12:21.080 --> 00:12:24.120]   came from intelligence and not necessarily from cybersecurity.
[00:12:24.120 --> 00:12:26.920]   And so that's where I think this gets muddled.
[00:12:26.920 --> 00:12:29.600]   But that might be focusing on the wrong things
[00:12:29.600 --> 00:12:32.480]   because as you're saying, whether this story is
[00:12:32.480 --> 00:12:39.320]   100% accurate or has been told the full way or not,
[00:12:39.320 --> 00:12:42.760]   these supply chain issues, these black box issues,
[00:12:42.760 --> 00:12:47.880]   these kind of accounted for threats,
[00:12:47.880 --> 00:12:49.800]   our realities and our things that we
[00:12:49.800 --> 00:12:53.640]   should be realizing are our reality and not something
[00:12:53.640 --> 00:12:55.360]   we should just continue to ignore.
[00:12:55.360 --> 00:12:56.200]   Yeah.
[00:12:56.200 --> 00:13:02.680]   I mean, we wrote this story for a general kind of business
[00:13:02.680 --> 00:13:05.160]   reader and not so much for the security community.
[00:13:05.160 --> 00:13:09.120]   That Jordan has been covering cybersecurity for many, many years.
[00:13:09.120 --> 00:13:12.240]   He knows the topic better than anyone I know.
[00:13:12.240 --> 00:13:14.080]   I mean, he nerds out super hard on this stuff,
[00:13:14.080 --> 00:13:17.640]   which is probably part of the reason why some of the sources,
[00:13:17.640 --> 00:13:19.400]   the insiders at the companies in particular
[00:13:19.400 --> 00:13:21.400]   trusted him with this type of information
[00:13:21.400 --> 00:13:24.000]   because they knew he understood the particulars.
[00:13:24.000 --> 00:13:26.400]   So he will-- if you get him on the show,
[00:13:26.400 --> 00:13:29.920]   he would probably nerd out super hard on all the specifics
[00:13:29.920 --> 00:13:31.120]   of it.
[00:13:31.120 --> 00:13:34.280]   But for the purposes of the story that we were writing,
[00:13:34.280 --> 00:13:37.040]   it's for a general alliance.
[00:13:37.040 --> 00:13:38.400]   That was one of the criticisms, though.
[00:13:38.400 --> 00:13:41.280]   You're right, Christina, was, well, where's the detail?
[00:13:41.280 --> 00:13:42.040]   Where's the detail?
[00:13:42.040 --> 00:13:42.920]   That's not their job.
[00:13:42.920 --> 00:13:44.200]   That's not Bloomberg's job.
[00:13:44.200 --> 00:13:44.760]   No, it's not.
[00:13:44.760 --> 00:13:46.120]   It's encouraging to hear from you, Mark,
[00:13:46.120 --> 00:13:48.480]   that Jordan knew the detail.
[00:13:48.480 --> 00:13:49.840]   It's just not going to make it in the story
[00:13:49.840 --> 00:13:51.800]   because that's not the audience.
[00:13:51.800 --> 00:13:54.280]   Which is understandable, but unfortunately with a story
[00:13:54.280 --> 00:13:58.720]   with this level of kind of space, it makes it hard for people,
[00:13:58.720 --> 00:14:00.240]   especially when you have denials.
[00:14:00.240 --> 00:14:02.400]   And they're very well-couched, very well-written,
[00:14:02.400 --> 00:14:03.560]   legal denials.
[00:14:03.560 --> 00:14:07.560]   It's hard for people to do that.
[00:14:07.560 --> 00:14:10.760]   And I guess maybe understand the extent of things.
[00:14:10.760 --> 00:14:14.640]   And I've even said the same thing that you've said, Mark.
[00:14:14.640 --> 00:14:16.560]   It's written for a general audience.
[00:14:16.560 --> 00:14:17.640]   I understand that.
[00:14:17.640 --> 00:14:20.080]   But I think that's why some of the criticism
[00:14:20.080 --> 00:14:22.760]   has been so focused on the technical details
[00:14:22.760 --> 00:14:25.720]   because regardless of what audience has been written for,
[00:14:25.720 --> 00:14:28.160]   not having those things there leaves a lot of things
[00:14:28.160 --> 00:14:29.680]   open to interpretation.
[00:14:29.680 --> 00:14:32.160]   And that lets people either off the hook
[00:14:32.160 --> 00:14:34.920]   or start extrapolating things that may or may not be true.
[00:14:34.920 --> 00:14:36.600]   And that's just, unfortunately, the nature
[00:14:36.600 --> 00:14:38.600]   of these types of things when you're
[00:14:38.600 --> 00:14:39.800]   writing for a specific audience.
[00:14:39.800 --> 00:14:42.800]   And you might not have those, especially when you don't
[00:14:42.800 --> 00:14:46.360]   have people on the record, how you get that across.
[00:14:46.360 --> 00:14:48.720]   There is-- and the other thing that I think is true,
[00:14:48.720 --> 00:14:51.320]   that if this really happened, you're
[00:14:51.320 --> 00:14:54.360]   going to see over time people step forward, say, yeah,
[00:14:54.360 --> 00:14:55.720]   I have one of those motherboards.
[00:14:55.720 --> 00:14:56.960]   Here's the chip.
[00:14:56.960 --> 00:15:01.480]   Unless Spooks have gotten to them all, which also is possible.
[00:15:01.480 --> 00:15:02.480]   There is some evidence.
[00:15:02.480 --> 00:15:05.320]   There was a follow-up story this week,
[00:15:05.320 --> 00:15:08.680]   both Jordan and Michael on this one as well,
[00:15:08.680 --> 00:15:14.360]   of hacked Super Micro hardware found in US Telecom.
[00:15:14.360 --> 00:15:17.920]   And this came from a security expert named Yossi Appleboom,
[00:15:17.920 --> 00:15:20.240]   who was brought in by this telecom.
[00:15:20.240 --> 00:15:23.000]   He could not reveal the name of the telecom.
[00:15:23.000 --> 00:15:26.280]   He's formerly with the Israeli Army Intelligence Corps.
[00:15:26.280 --> 00:15:27.680]   And he's a hardware security guy.
[00:15:27.680 --> 00:15:30.480]   He was brought into scan data centers
[00:15:30.480 --> 00:15:33.040]   belonging to this telecommunications company.
[00:15:33.040 --> 00:15:36.200]   And Appleboom said, yeah, I've seen this before,
[00:15:36.200 --> 00:15:40.040]   and I did see this with Super Micro.
[00:15:40.040 --> 00:15:42.560]   He says, though, the Super Micro is a victim,
[00:15:42.560 --> 00:15:43.800]   but so is everybody else.
[00:15:43.800 --> 00:15:44.720]   It's not Super Micro.
[00:15:44.720 --> 00:15:45.920]   Didn't do this on purpose.
[00:15:45.920 --> 00:15:48.120]   And that doesn't surprise me that maybe one of the suppliers
[00:15:48.120 --> 00:15:53.520]   was sub-born, bribed, or forced into doing it.
[00:15:53.520 --> 00:15:56.280]   So all the AT&T said, no, not us.
[00:15:56.280 --> 00:15:57.920]   Verizon said, not us.
[00:15:57.920 --> 00:15:59.520]   Sprint says, not us.
[00:15:59.520 --> 00:16:02.000]   T-Mobile didn't respond.
[00:16:02.000 --> 00:16:03.600]   Doesn't mean it's them.
[00:16:03.600 --> 00:16:06.040]   But this is the kind of corroborating story
[00:16:06.040 --> 00:16:10.720]   to expect to see more of over time.
[00:16:10.720 --> 00:16:13.520]   As people step forward and say, yeah, we found this.
[00:16:13.520 --> 00:16:16.640]   That's going to settle it as far as I'm concerned.
[00:16:16.640 --> 00:16:18.600]   Well, that was my main takeaway from the article,
[00:16:18.600 --> 00:16:21.960]   just in both the specifics of that story, as well as stories
[00:16:21.960 --> 00:16:23.800]   to come, is that is this the tip of the iceberg?
[00:16:23.800 --> 00:16:27.160]   And it's now more proof comes out once the article's in place.
[00:16:27.160 --> 00:16:28.400]   And then you let the people who are
[00:16:28.400 --> 00:16:30.720]   doing the technical analysis, the technical analysis.
[00:16:30.720 --> 00:16:32.840]   And I've got to imagine that this is not going
[00:16:32.840 --> 00:16:36.440]   to be the only type of story that comes out of the supply chain
[00:16:36.440 --> 00:16:37.600]   that's going on in China.
[00:16:37.600 --> 00:16:40.680]   And there's just no question that the moral of the story
[00:16:40.680 --> 00:16:43.880]   is absolutely intact, which is we have to secure the supply
[00:16:43.880 --> 00:16:48.760]   chain, because there's huge risks that a state actor could
[00:16:48.760 --> 00:16:52.640]   use it for cyber warfare, could use it for industrial espionage
[00:16:52.640 --> 00:16:53.200]   or both.
[00:16:53.200 --> 00:16:56.640]   And there's huge risks as we get more and more automated.
[00:16:56.640 --> 00:16:57.960]   That was Bruce Schneider's point.
[00:16:57.960 --> 00:16:59.880]   And we're moving into a world where
[00:16:59.880 --> 00:17:01.120]   we're not going to be driving the cars.
[00:17:01.120 --> 00:17:03.120]   The computers are going to be driving the cars.
[00:17:03.120 --> 00:17:04.480]   And they're hackable.
[00:17:04.480 --> 00:17:05.840]   That's a problem.
[00:17:05.840 --> 00:17:08.520]   I worked at an IoT company a couple of years ago
[00:17:08.520 --> 00:17:12.360]   that has a device that they literally designed themselves
[00:17:12.360 --> 00:17:14.120]   and then brought the prototype to China
[00:17:14.120 --> 00:17:16.120]   and showed it to a supply chain manager.
[00:17:16.120 --> 00:17:18.560]   And they came back and said, yeah, we can make this for you.
[00:17:18.560 --> 00:17:20.080]   They don't know what's in their device.
[00:17:20.080 --> 00:17:22.360]   They don't know every chip that's in their device at all.
[00:17:22.360 --> 00:17:23.800]   And they claim they designed it.
[00:17:23.800 --> 00:17:28.120]   They didn't-- there's so much gray area in especially IoT.
[00:17:28.120 --> 00:17:30.320]   But in all the work that's being done now in hardware
[00:17:30.320 --> 00:17:32.800]   and with China, it's cheap for a reason, everyone.
[00:17:32.800 --> 00:17:34.720]   I mean, like that's--
[00:17:34.720 --> 00:17:36.200]   Right.
[00:17:36.200 --> 00:17:37.000]   We welcome that.
[00:17:37.000 --> 00:17:37.800]   We embraced that.
[00:17:37.800 --> 00:17:42.480]   China became the world's manufacturing arm, right?
[00:17:42.480 --> 00:17:45.640]   And it is actually powered huge innovation.
[00:17:45.640 --> 00:17:47.560]   I mean, you can do exactly that, Ron.
[00:17:47.560 --> 00:17:49.960]   You can come up with a great idea,
[00:17:49.960 --> 00:17:51.360]   not know how to implement it.
[00:17:51.360 --> 00:17:54.360]   It can be made and manufactured at scale,
[00:17:54.360 --> 00:17:55.160]   affordably.
[00:17:55.160 --> 00:17:58.000]   By the way, whether Apple admits to this or not,
[00:17:58.000 --> 00:18:00.080]   a lot of this can be attributed to Apple
[00:18:00.080 --> 00:18:03.120]   because when they brought the iPhone to China,
[00:18:03.120 --> 00:18:06.880]   and this is Tim Cook's expertise in supply chain,
[00:18:06.880 --> 00:18:13.280]   they really created an amazing industrial capabilities
[00:18:13.280 --> 00:18:16.680]   in Shenzhen, China, and machine specialized machines
[00:18:16.680 --> 00:18:18.040]   to do all sorts of things.
[00:18:18.040 --> 00:18:22.360]   Those chips, the accelerometers, the barometers,
[00:18:22.360 --> 00:18:25.160]   all the different chips in a smartphone today,
[00:18:25.160 --> 00:18:27.040]   you can really trace it back to 2007
[00:18:27.040 --> 00:18:28.920]   when Apple created the iPhone and started
[00:18:28.920 --> 00:18:31.400]   making this stuff at scale and taught China
[00:18:31.400 --> 00:18:32.520]   how to make it at scale.
[00:18:32.520 --> 00:18:35.720]   And as a result, we're getting all sorts of great stuff.
[00:18:35.720 --> 00:18:38.240]   But I'm better secure it.
[00:18:38.240 --> 00:18:39.880]   We're getting great stuff in the short term.
[00:18:39.880 --> 00:18:41.240]   I think for now, we're finding out
[00:18:41.240 --> 00:18:43.120]   what the long term ramifications this is going to be.
[00:18:43.120 --> 00:18:44.160]   Now, here's the question.
[00:18:44.160 --> 00:18:48.920]   Is it China risking its integrity and its future
[00:18:48.920 --> 00:18:50.040]   as an industrial--
[00:18:50.040 --> 00:18:52.040]   as the manufacturer of the world by doing?
[00:18:52.040 --> 00:18:52.920]   If they were--
[00:18:52.920 --> 00:18:54.360]   Well, where are people going to go?
[00:18:54.360 --> 00:18:55.600]   I mean, what's the alternative?
[00:18:55.600 --> 00:18:56.760]   Like, we literally--
[00:18:56.760 --> 00:18:57.480]   Well, no, because--
[00:18:57.480 --> 00:19:00.120]   Sun makes its phone in Thailand in South Korea.
[00:19:00.120 --> 00:19:01.440]   And right.
[00:19:01.440 --> 00:19:03.000]   But it's still Foxconn.
[00:19:03.000 --> 00:19:06.000]   And it's still a lot of these same companies.
[00:19:06.000 --> 00:19:11.120]   So OK, so maybe China loses its foothold.
[00:19:11.120 --> 00:19:13.240]   But that doesn't mean that these problems are
[00:19:13.240 --> 00:19:15.520]   going to go away because the profit motive, which
[00:19:15.520 --> 00:19:17.760]   is what's behind a lot of this and maybe
[00:19:17.760 --> 00:19:19.720]   the other types of motive still exists.
[00:19:19.720 --> 00:19:23.080]   A lot of the times when you hear this all the time
[00:19:23.080 --> 00:19:25.920]   from companies who they will put together a prototype
[00:19:25.920 --> 00:19:28.960]   and they'll design a board and they'll have somebody in Asia
[00:19:28.960 --> 00:19:31.800]   make it, and then it comes back and it's not what they ordered.
[00:19:31.800 --> 00:19:34.360]   And they've had lesser parts done and they have to have--
[00:19:34.360 --> 00:19:36.680]   they have to pay somebody to actually be on the line
[00:19:36.680 --> 00:19:38.720]   and watching things and doing spot checks.
[00:19:38.720 --> 00:19:39.840]   And even then, you have to make sure
[00:19:39.840 --> 00:19:42.200]   that the person who you're paying isn't being paid off
[00:19:42.200 --> 00:19:42.920]   by someone else.
[00:19:42.920 --> 00:19:44.400]   And it can be really complicated.
[00:19:44.400 --> 00:19:46.200]   I don't necessarily know if those issues go away
[00:19:46.200 --> 00:19:47.560]   if you just go to Thailand.
[00:19:47.560 --> 00:19:49.840]   So I don't really know what the alternative is
[00:19:49.840 --> 00:19:53.320]   because, as you said earlier, the manufacturing
[00:19:53.320 --> 00:19:55.400]   is not coming back to the United States.
[00:19:55.400 --> 00:19:56.280]   That's over.
[00:19:56.280 --> 00:19:57.720]   A, we don't have the capacity.
[00:19:57.720 --> 00:19:58.520]   And so I have to do it.
[00:19:58.520 --> 00:19:59.720]   B, we don't have the skilled workers.
[00:19:59.720 --> 00:20:02.400]   C, the amount of money that it would cost.
[00:20:02.400 --> 00:20:05.880]   Unless you can prove that there are very, very, very real,
[00:20:05.880 --> 00:20:08.240]   tangible security implications.
[00:20:08.240 --> 00:20:11.040]   I don't think that anybody is going to be convinced to do that.
[00:20:11.040 --> 00:20:15.960]   Well, and the kind of stuff that the administration is talking
[00:20:15.960 --> 00:20:18.000]   about is really just bringing assembly back.
[00:20:18.000 --> 00:20:19.600]   All the parts that they're assembling
[00:20:19.600 --> 00:20:21.640]   are still manufactured overseas.
[00:20:21.640 --> 00:20:24.400]   And also, if you just-- because you bring manufacturing back
[00:20:24.400 --> 00:20:27.400]   to America, I mean, I'm an Italian American from New York.
[00:20:27.400 --> 00:20:29.680]   Correction looms in America.
[00:20:29.680 --> 00:20:30.680]   This is much of China.
[00:20:30.680 --> 00:20:32.000]   It's a little kid.
[00:20:32.000 --> 00:20:32.960]   Oh, completely.
[00:20:32.960 --> 00:20:34.200]   And then you just have corruption,
[00:20:34.200 --> 00:20:35.320]   but maybe at higher prices.
[00:20:35.320 --> 00:20:36.680]   So yeah, I mean--
[00:20:36.680 --> 00:20:39.160]   Everything gets more expensive, including the bribes.
[00:20:39.160 --> 00:20:43.720]   So I guess my point is, but you were saying,
[00:20:43.720 --> 00:20:44.720]   what is this you for China?
[00:20:44.720 --> 00:20:45.400]   I mean, I don't know.
[00:20:45.400 --> 00:20:48.120]   I mean, I think that this is an interesting opportunity,
[00:20:48.120 --> 00:20:50.280]   maybe, for some other nations to kind of try
[00:20:50.280 --> 00:20:51.080]   to be a hub for that.
[00:20:51.080 --> 00:20:54.000]   But it took China a long time to become
[00:20:54.000 --> 00:20:55.680]   what it's become manufacturing to.
[00:20:55.680 --> 00:20:57.200]   And so it's not one of those things
[00:20:57.200 --> 00:20:59.560]   where you can just shift to another place overnight.
[00:20:59.560 --> 00:21:03.120]   But also, you've got to wonder what are China's primary motivations
[00:21:03.120 --> 00:21:05.240]   here, where their primary motivation to be the lowest
[00:21:05.240 --> 00:21:06.960]   cost manufacturer for the world?
[00:21:06.960 --> 00:21:09.200]   Or was it to be the lowest cost manufacturer in the world
[00:21:09.200 --> 00:21:13.200]   in order to do X, Y, and Z to get political agenda across?
[00:21:13.200 --> 00:21:14.520]   They're a socialist nation.
[00:21:14.520 --> 00:21:19.080]   I mean, aren't they not looking for profit?
[00:21:19.080 --> 00:21:20.840]   Isn't that the whole point of the country?
[00:21:20.840 --> 00:21:26.800]   So you've got to wonder, we can't look at their politics
[00:21:26.800 --> 00:21:28.680]   and their country and what they're doing
[00:21:28.680 --> 00:21:30.440]   as their motivations the same way we do,
[00:21:30.440 --> 00:21:32.840]   because it's a completely different starting point.
[00:21:32.840 --> 00:21:37.280]   That's a really interesting question.
[00:21:37.280 --> 00:21:39.480]   And we know that the Chinese government
[00:21:39.480 --> 00:21:42.240]   is able to think much more long term than any of us.
[00:21:42.240 --> 00:21:45.400]   Maybe 20 years ago, they said, you know, be great.
[00:21:45.400 --> 00:21:47.440]   You know, be great guys.
[00:21:47.440 --> 00:21:48.240]   Join lives.
[00:21:48.240 --> 00:21:49.760]   It would be an epic long game.
[00:21:49.760 --> 00:21:50.640]   I mean, it really would be.
[00:21:50.640 --> 00:21:51.640]   Play the long game.
[00:21:51.640 --> 00:21:53.240]   We know they played the long, look at their--
[00:21:53.240 --> 00:21:55.120]   It's going to make a great movie someday.
[00:21:55.120 --> 00:21:56.120]   Yeah.
[00:21:56.120 --> 00:21:57.800]   With lots of great movies, great, great TV show.
[00:21:57.800 --> 00:21:59.840]   The Americans, the equivalent of the Americans
[00:21:59.840 --> 00:22:03.160]   in 30 years is going to be this.
[00:22:03.160 --> 00:22:04.160]   Yeah.
[00:22:04.160 --> 00:22:05.040]   It'll be a lot of fun.
[00:22:05.040 --> 00:22:07.920]   I wish I were around in about 40 years when all the truth comes
[00:22:07.920 --> 00:22:09.360]   out about what happened today.
[00:22:09.360 --> 00:22:10.800]   Well, and the movie will be in Mandarin.
[00:22:10.800 --> 00:22:11.960]   So that's a lot.
[00:22:11.960 --> 00:22:14.760]   I'm like, yeah, I've been watching the man in the high castle.
[00:22:14.760 --> 00:22:15.840]   I know how this all ends.
[00:22:15.840 --> 00:22:17.240]   Yeah.
[00:22:17.240 --> 00:22:18.840]   Let's take a break when we come back.
[00:22:18.840 --> 00:22:19.800]   There is a lot to talk about.
[00:22:19.800 --> 00:22:23.080]   This was kind of a product release week in a big way
[00:22:23.080 --> 00:22:26.000]   from not just Google and Facebook
[00:22:26.000 --> 00:22:28.160]   released a new camera phone.
[00:22:28.160 --> 00:22:31.520]   [LAUGHTER]
[00:22:31.520 --> 00:22:32.800]   Complimentic in a few ways.
[00:22:32.800 --> 00:22:34.000]   We'll talk about that in a second.
[00:22:34.000 --> 00:22:35.240]   First though, a word.
[00:22:35.240 --> 00:22:35.960]   We got a great panel.
[00:22:35.960 --> 00:22:38.280]   Mark Millian from Bloomberg Business Week is here.
[00:22:38.280 --> 00:22:39.240]   What's your beat these days?
[00:22:39.240 --> 00:22:40.840]   Technology?
[00:22:40.840 --> 00:22:41.360]   Technology?
[00:22:41.360 --> 00:22:41.860]   Yeah.
[00:22:41.860 --> 00:22:42.360]   That's--
[00:22:42.360 --> 00:22:45.240]   They took you off the global beat and brought you back home?
[00:22:45.240 --> 00:22:47.400]   I added our coverage of startups and venture capital.
[00:22:47.400 --> 00:22:47.900]   Nice.
[00:22:47.900 --> 00:22:48.400]   So--
[00:22:48.400 --> 00:22:49.160]   Focus on the best, Mark.
[00:22:49.160 --> 00:22:49.880]   That's exciting.
[00:22:49.880 --> 00:22:50.960]   Yeah, that's fun.
[00:22:50.960 --> 00:22:52.360]   Fun beat.
[00:22:52.360 --> 00:22:53.480]   Film Girl is here.
[00:22:53.480 --> 00:22:56.840]   Christina Warren from Microsoft Senior Cloud Dev Advocate.
[00:22:56.840 --> 00:23:01.200]   But of course, a long time commentator and observer
[00:23:01.200 --> 00:23:03.360]   of the world of tech.
[00:23:03.360 --> 00:23:04.560]   Always a pleasure.
[00:23:04.560 --> 00:23:05.080]   And--
[00:23:05.080 --> 00:23:05.560]   Great to be here.
[00:23:05.560 --> 00:23:06.240]   I don't think-- I've run.
[00:23:06.240 --> 00:23:07.440]   Have you done this show before?
[00:23:07.440 --> 00:23:08.640]   This can't be your first time.
[00:23:08.640 --> 00:23:11.360]   I think this is like my second or third time.
[00:23:11.360 --> 00:23:12.680]   I mean, it's been four years.
[00:23:12.680 --> 00:23:13.200]   But--
[00:23:13.200 --> 00:23:13.880]   Oh my god.
[00:23:13.880 --> 00:23:15.240]   I'm sorry.
[00:23:15.240 --> 00:23:16.720]   I forgot why the last time.
[00:23:16.720 --> 00:23:20.080]   But I think my first time was after the Google Glass--
[00:23:20.080 --> 00:23:21.680]   my overreaction at Google Glass.
[00:23:21.680 --> 00:23:23.200]   Any time anything comes--
[00:23:23.200 --> 00:23:24.440]   did you say it was good or bad?
[00:23:24.440 --> 00:23:25.880]   Here's the test.
[00:23:25.880 --> 00:23:27.880]   Well, I said it was the greatest thing ever.
[00:23:27.880 --> 00:23:28.880]   [LAUGHTER]
[00:23:28.880 --> 00:23:29.880]   I was far more--
[00:23:29.880 --> 00:23:30.380]   Yeah.
[00:23:30.380 --> 00:23:31.880]   That's your fault, my friend.
[00:23:31.880 --> 00:23:32.600]   I should have been.
[00:23:32.600 --> 00:23:33.320]   I bought one.
[00:23:33.320 --> 00:23:34.400]   A lot of people were sucked into that.
[00:23:34.400 --> 00:23:35.320]   A lot of people were.
[00:23:35.320 --> 00:23:35.840]   I bought one.
[00:23:35.840 --> 00:23:36.800]   I never wore it.
[00:23:36.800 --> 00:23:37.840]   I gave it to Jason.
[00:23:37.840 --> 00:23:39.440]   But I did buy one.
[00:23:39.440 --> 00:23:39.960]   [LAUGHTER]
[00:23:39.960 --> 00:23:42.160]   And I believe Jason's is the one that I tried.
[00:23:42.160 --> 00:23:44.600]   And I believe that was my holy cow moment.
[00:23:44.600 --> 00:23:47.880]   Actually, on Friday, I got to try the Magic Leap.
[00:23:47.880 --> 00:23:49.480]   And we can talk a little bit about that.
[00:23:49.480 --> 00:23:50.680]   I'm only here about that.
[00:23:50.680 --> 00:23:51.760]   This was very interesting.
[00:23:51.760 --> 00:23:54.320]   We did an extensive demo on the new screensavers.
[00:23:54.320 --> 00:23:59.320]   I spent about an hour and a half talking to Nicole Lazaro,
[00:23:59.320 --> 00:24:03.240]   who is a game consultant to game companies.
[00:24:03.240 --> 00:24:07.720]   She's consulted with Myst, with Maxis on the Sims.
[00:24:07.720 --> 00:24:11.080]   She helps game companies make their games more fun.
[00:24:11.080 --> 00:24:13.400]   And she's had a lot of interesting things
[00:24:13.400 --> 00:24:14.600]   to say about the Magic Leap.
[00:24:14.600 --> 00:24:16.760]   So we will talk about that too.
[00:24:16.760 --> 00:24:18.760]   Our show today brought to you by--
[00:24:18.760 --> 00:24:20.720]   by the way, Ron is the host of all about Android.
[00:24:20.720 --> 00:24:21.220]   Yes.
[00:24:21.220 --> 00:24:22.040]   I forgot to mention that.
[00:24:22.040 --> 00:24:22.800]   Brought to you by--
[00:24:22.800 --> 00:24:24.680]   and every four years, we bring him in whenever
[00:24:24.680 --> 00:24:26.320]   Google does something.
[00:24:26.320 --> 00:24:27.960]   Actually, Google did a lot of things this week.
[00:24:27.960 --> 00:24:29.480]   You're going to be busy today, Ron.
[00:24:29.480 --> 00:24:30.960]   I was the one that went to the event.
[00:24:30.960 --> 00:24:31.400]   That's all I had to do.
[00:24:31.400 --> 00:24:34.480]   You're going to have to defend Google a little bit,
[00:24:34.480 --> 00:24:35.640]   I think, on this show.
[00:24:35.640 --> 00:24:37.320]   Our show today brought to you by Molecule.
[00:24:37.320 --> 00:24:40.080]   This is kind of like the apple of air purifiers.
[00:24:40.080 --> 00:24:41.040]   We love our Molecule.
[00:24:41.040 --> 00:24:43.920]   I got one-- Lisa used to wake up in the morning with headaches.
[00:24:43.920 --> 00:24:46.560]   We had moved to the country, and she'd never
[00:24:46.560 --> 00:24:47.360]   had allergies before.
[00:24:47.360 --> 00:24:48.720]   But she'd wake up every morning with headaches,
[00:24:48.720 --> 00:24:52.160]   and we figured it's kind of-- it's the pollen, it's the dust.
[00:24:52.160 --> 00:24:53.960]   I tried different air purifiers.
[00:24:53.960 --> 00:24:58.000]   We finally got a Molecule because it's very different
[00:24:58.000 --> 00:25:01.760]   than the HEPA filter air purifiers we had been trying.
[00:25:01.760 --> 00:25:07.000]   Turns out, HEPA technology was invented during World War II
[00:25:07.000 --> 00:25:09.120]   to get big particles out of the air.
[00:25:09.120 --> 00:25:12.120]   And all it does is it traps the particles in the purifier.
[00:25:12.120 --> 00:25:16.320]   And you know, OK, it's a start, but Molecule
[00:25:16.320 --> 00:25:17.440]   goes to the next level.
[00:25:17.440 --> 00:25:19.240]   So they also trap the big particles.
[00:25:19.240 --> 00:25:22.880]   But they also use something called PICO-- photoelectrochemical
[00:25:22.880 --> 00:25:24.640]   oxidation.
[00:25:24.640 --> 00:25:28.480]   This was technology funded by the EPA.
[00:25:28.480 --> 00:25:31.560]   They've verified the technology in university laboratories
[00:25:31.560 --> 00:25:33.240]   like the University of South Florida's Center
[00:25:33.240 --> 00:25:36.240]   for Biological Defense, University of Minnesota's
[00:25:36.240 --> 00:25:38.640]   Particle Calibration Laboratory.
[00:25:38.640 --> 00:25:44.920]   The PICO technology actually traps airborne pollutants.
[00:25:44.920 --> 00:25:47.960]   And it can trap stuff 1,000 times smaller
[00:25:47.960 --> 00:25:48.760]   than the HEPA filter.
[00:25:48.760 --> 00:25:54.560]   Things like mold, bacteria, viruses, even airborne chemicals
[00:25:54.560 --> 00:25:58.400]   like VOCs, you know, the volatile organic compounds emitted
[00:25:58.400 --> 00:26:01.680]   like formaldehyde, emitted from carpeting and paint.
[00:26:01.680 --> 00:26:04.160]   It can actually-- it not only traps it,
[00:26:04.160 --> 00:26:05.640]   but then it burns it up.
[00:26:05.640 --> 00:26:07.520]   It actually destroys it.
[00:26:07.520 --> 00:26:11.440]   So instead of like making a collection of all the bad stuff
[00:26:11.440 --> 00:26:15.520]   in the air, which a HEPA filter does, this gets rid of it.
[00:26:15.520 --> 00:26:17.160]   It is really amazing.
[00:26:17.160 --> 00:26:18.360]   Lisa's headaches went away.
[00:26:18.360 --> 00:26:20.880]   In fact, we know it works because when we travel
[00:26:20.880 --> 00:26:23.560]   or when the molecule's been turned off,
[00:26:23.560 --> 00:26:26.120]   sometimes like our sun comes in and turns it off.
[00:26:26.120 --> 00:26:28.000]   We know because the next morning she wakes up with a headache
[00:26:28.000 --> 00:26:29.520]   and we go check the molecule.
[00:26:29.520 --> 00:26:31.360]   Yeah, but it was off.
[00:26:31.360 --> 00:26:32.720]   This we cannot live with that.
[00:26:32.720 --> 00:26:33.400]   We like it so much.
[00:26:33.400 --> 00:26:34.960]   We got one for the studio.
[00:26:34.960 --> 00:26:37.840]   If you're in studio, you're breathing the better air here.
[00:26:37.840 --> 00:26:39.240]   Because this studio is completely sealed.
[00:26:39.240 --> 00:26:40.600]   There's no windows.
[00:26:40.600 --> 00:26:42.360]   And you know, sometimes they'll tar the roof.
[00:26:42.360 --> 00:26:43.320]   They're painting and stuff.
[00:26:43.320 --> 00:26:45.160]   It's really nice to get rid of all that stuff,
[00:26:45.160 --> 00:26:46.840]   get it out of the air so I'm not breathing it.
[00:26:46.840 --> 00:26:47.840]   You're not breathing it.
[00:26:47.840 --> 00:26:49.000]   We got one for our sun too.
[00:26:49.000 --> 00:26:50.960]   We love the molecule.
[00:26:50.960 --> 00:26:52.520]   And Lisa's headaches are gone.
[00:26:52.520 --> 00:26:53.720]   It really works.
[00:26:53.720 --> 00:26:54.520]   Easy to use.
[00:26:54.520 --> 00:26:55.920]   It's got a clean sleek design.
[00:26:55.920 --> 00:26:57.120]   We pair it to our phones.
[00:26:57.120 --> 00:26:58.800]   You don't have to because it's got controls on the top.
[00:26:58.800 --> 00:27:00.480]   But I like to pair it to the phone
[00:27:00.480 --> 00:27:04.320]   because then the phone will order new filters when necessary,
[00:27:04.320 --> 00:27:07.560]   which is really cool.
[00:27:07.560 --> 00:27:11.840]   You could do it on the Wi-Fi or using Bluetooth if you want.
[00:27:11.840 --> 00:27:13.080]   That is a really great deal.
[00:27:13.080 --> 00:27:14.800]   But I have to tell you, you've got to try this.
[00:27:14.800 --> 00:27:21.480]   You can try it at no risk to you if you go to moleculemolik.com
[00:27:21.480 --> 00:27:24.120]   and use a promo code TWIT 75.
[00:27:24.120 --> 00:27:25.800]   Molecule.com, if it doesn't work,
[00:27:25.800 --> 00:27:27.080]   you can return it and get your money back.
[00:27:27.080 --> 00:27:27.880]   Absolutely.
[00:27:27.880 --> 00:27:31.800]   Molecule.com offer code is TWIT 75.
[00:27:31.800 --> 00:27:34.960]   That's going to save you 75 bucks off your first order.
[00:27:34.960 --> 00:27:36.640]   They really reinvented the air purifier.
[00:27:36.640 --> 00:27:40.000]   This is a modern way to get great air.
[00:27:40.000 --> 00:27:44.800]   We have well water, so I have lots of processing on our water
[00:27:44.800 --> 00:27:46.360]   to make the water great.
[00:27:46.360 --> 00:27:49.120]   We process the air now too to make the air great.
[00:27:49.120 --> 00:27:50.760]   And it just makes a huge difference.
[00:27:50.760 --> 00:27:55.120]   Molecule.molik.com.
[00:27:55.120 --> 00:27:56.480]   Let's see.
[00:27:56.480 --> 00:27:58.240]   Google had an event.
[00:27:58.240 --> 00:27:59.320]   I almost want to start.
[00:27:59.320 --> 00:28:00.840]   Let me start with the Wall Street Journal.
[00:28:00.840 --> 00:28:03.760]   And then we'll get to the Google event.
[00:28:03.760 --> 00:28:08.160]   Wall Street Journal, I don't think they like Google that much.
[00:28:08.160 --> 00:28:10.360]   I think there's kind of this animosity towards Google
[00:28:10.360 --> 00:28:13.200]   that might have something to do with the fact that all newspapers
[00:28:13.200 --> 00:28:16.720]   are suffering because Google and Facebook are eating them
[00:28:16.720 --> 00:28:19.200]   alive and advertising.
[00:28:19.200 --> 00:28:25.560]   Wall Street Journal originally wrote that Google had a data breach.
[00:28:25.560 --> 00:28:29.520]   And I think correctly revised the headline.
[00:28:29.520 --> 00:28:32.760]   Still, it's a little bit of a nasty headline.
[00:28:32.760 --> 00:28:36.240]   Google exposed user data, feared repercussions
[00:28:36.240 --> 00:28:38.520]   of disclosing to public.
[00:28:38.520 --> 00:28:41.120]   What happened back in the spring, according to Wall Street
[00:28:41.120 --> 00:28:42.400]   Journal, and Google admits this, Google
[00:28:42.400 --> 00:28:45.680]   has been actually going through a-- they call it
[00:28:45.680 --> 00:28:49.280]   a root and branch assessment of their security
[00:28:49.280 --> 00:28:51.560]   with regard to third party applications.
[00:28:51.560 --> 00:28:54.720]   As you know, you can use Google to enable a third party
[00:28:54.720 --> 00:28:57.040]   application to go through your Gmail, for instance.
[00:28:57.040 --> 00:28:58.560]   Just recently, Google said, we're
[00:28:58.560 --> 00:29:03.080]   going to stop letting companies do anything but mail specific
[00:29:03.080 --> 00:29:05.320]   applications in that regard.
[00:29:05.320 --> 00:29:06.960]   So they've been going through all of this stuff.
[00:29:06.960 --> 00:29:09.800]   They discovered in the spring that there
[00:29:09.800 --> 00:29:12.960]   was a flaw in the Google+, that's their social network,
[00:29:12.960 --> 00:29:17.120]   API, that potentially would allow a third party developer
[00:29:17.120 --> 00:29:21.160]   to use the API to get at your private profile information.
[00:29:21.160 --> 00:29:23.880]   Now, if you use Google+, you know you create a profile.
[00:29:23.880 --> 00:29:26.040]   They really pushed this for a while a few years ago.
[00:29:26.040 --> 00:29:28.680]   Use this as your home on the web, everything you want to--
[00:29:28.680 --> 00:29:30.920]   and I was looking at my profile after the story came out
[00:29:30.920 --> 00:29:32.800]   to see, well, what private information is there?
[00:29:32.800 --> 00:29:36.160]   And it's not your social, it's not your credit cards.
[00:29:36.160 --> 00:29:39.080]   In my case, it was my home address and my personal phone
[00:29:39.080 --> 00:29:39.960]   number.
[00:29:39.960 --> 00:29:43.280]   And the reason I put it in there is because you could set up
[00:29:43.280 --> 00:29:46.280]   a profile to share different levels of access,
[00:29:46.280 --> 00:29:47.680]   and close family and friends.
[00:29:47.680 --> 00:29:50.560]   I wanted to always have my correct address and phone number.
[00:29:50.560 --> 00:29:54.520]   So Google said, for privacy reasons,
[00:29:54.520 --> 00:29:59.320]   we've destroyed the logs of how this API is used every two weeks.
[00:29:59.320 --> 00:30:03.200]   But-- so we can't be sure nobody's used it.
[00:30:03.200 --> 00:30:05.800]   But to the best of our knowledge, no one has used this,
[00:30:05.800 --> 00:30:07.200]   no one even knew about it.
[00:30:07.200 --> 00:30:09.920]   So we fixed the bug.
[00:30:09.920 --> 00:30:13.920]   There was no data breach, we fixed the bug.
[00:30:13.920 --> 00:30:17.560]   The journal basically found emails saying, you know,
[00:30:17.560 --> 00:30:19.720]   let's not say anything about this,
[00:30:19.720 --> 00:30:23.080]   and used this to kind of really hit Google.
[00:30:23.080 --> 00:30:25.440]   But my question for you guys, is it below the belt?
[00:30:25.440 --> 00:30:27.920]   And here's why I will say it's not.
[00:30:27.920 --> 00:30:29.400]   Companies do this all the time.
[00:30:29.400 --> 00:30:33.880]   When Microsoft finds a flaw in Windows that potentially,
[00:30:33.880 --> 00:30:36.240]   you know, allows a hacker to get into your system,
[00:30:36.240 --> 00:30:38.880]   it doesn't announce it, it fixes it.
[00:30:38.880 --> 00:30:41.480]   Otherwise, every minute Microsoft is saying, oh, yeah,
[00:30:41.480 --> 00:30:44.040]   we found another one, let's reveal this, let's reveal this,
[00:30:44.040 --> 00:30:45.880]   let's reveal this, they fix it.
[00:30:45.880 --> 00:30:48.480]   That's the normal process.
[00:30:48.480 --> 00:30:51.600]   Now admittedly, Google has a little bit of a fiduciary
[00:30:51.600 --> 00:30:55.040]   responsibility because they're holding your personal information.
[00:30:55.040 --> 00:30:59.160]   So in fact, GDPR said, had this data been breached,
[00:30:59.160 --> 00:31:01.720]   had they found evidence that somebody was using this,
[00:31:01.720 --> 00:31:03.880]   they would have had legally a requirement in Europe
[00:31:03.880 --> 00:31:05.640]   to reveal it.
[00:31:05.640 --> 00:31:06.560]   This was cool.
[00:31:06.560 --> 00:31:07.840]   That's the distinction.
[00:31:07.840 --> 00:31:12.520]   This wasn't a breach, this is a open hole in an API
[00:31:12.520 --> 00:31:14.880]   that there's no proof or no knowledge
[00:31:14.880 --> 00:31:16.760]   that anybody used to exploit.
[00:31:16.760 --> 00:31:17.520]   So I mean--
[00:31:17.520 --> 00:31:18.600]   Well, frankly, no one was using anyway.
[00:31:18.600 --> 00:31:20.200]   I mean, if we were going to be really real,
[00:31:20.200 --> 00:31:22.120]   the number of Google+ apps--
[00:31:22.120 --> 00:31:25.360]   No, I mean, genuinely, like were A, not significant and B,
[00:31:25.360 --> 00:31:26.080]   the amount of--
[00:31:26.080 --> 00:31:29.560]   I mean, the vector here is significantly smaller
[00:31:29.560 --> 00:31:31.960]   than it would be for say, this were like a Facebook app,
[00:31:31.960 --> 00:31:33.560]   for instance.
[00:31:33.560 --> 00:31:35.680]   I should say it has opened the window of opportunity
[00:31:35.680 --> 00:31:37.480]   for Google to shut down Google+,
[00:31:37.480 --> 00:31:41.200]   well, by the way, the weird upshot of this is Google says,
[00:31:41.200 --> 00:31:43.640]   well, fine, then we're taking our ball and going home.
[00:31:43.640 --> 00:31:47.640]   They project strobe, which is this security assay
[00:31:47.640 --> 00:31:49.680]   that they're in the midst of.
[00:31:49.680 --> 00:31:52.960]   They said, finding number one, we found this API flaw.
[00:31:52.960 --> 00:31:56.480]   So result number two, we're going to close Google+,
[00:31:56.480 --> 00:31:57.760]   and August.
[00:31:57.760 --> 00:32:01.600]   Which I got to say, as a moderator of a vibrant Google+
[00:32:01.600 --> 00:32:03.720]   community, I feel really bad about it.
[00:32:03.720 --> 00:32:06.760]   You still use it on all about Android for your Android
[00:32:06.760 --> 00:32:08.000]   App Arena?
[00:32:08.000 --> 00:32:11.760]   Yeah, all about Android, Google+ community is amazing.
[00:32:11.760 --> 00:32:13.040]   They're on there every day.
[00:32:13.040 --> 00:32:14.040]   They're posting stories.
[00:32:14.040 --> 00:32:14.720]   They're talking.
[00:32:14.720 --> 00:32:17.280]   They vote in our app polls and stuff like that.
[00:32:17.280 --> 00:32:19.440]   And so now we've got to find a new playground
[00:32:19.440 --> 00:32:22.440]   to go play in because it was one social network that actually
[00:32:22.440 --> 00:32:23.880]   was for the right people.
[00:32:23.880 --> 00:32:27.200]   And they used it, admittedly, that's just a small number
[00:32:27.200 --> 00:32:29.360]   compared to the massive billions that are on Facebook
[00:32:29.360 --> 00:32:30.120]   and things like that.
[00:32:30.120 --> 00:32:32.160]   But still, it's a bummer that we have
[00:32:32.160 --> 00:32:35.760]   to go find a new place for our voting mechanism.
[00:32:35.760 --> 00:32:38.280]   I think even if--
[00:32:38.280 --> 00:32:43.360]   I mean, granted, virtually nobody was using Google+.
[00:32:43.360 --> 00:32:45.440]   But they did push hard for a long time
[00:32:45.440 --> 00:32:48.480]   to get people to create profiles and plug information
[00:32:48.480 --> 00:32:49.480]   into it.
[00:32:49.480 --> 00:32:50.480]   Absolutely.
[00:32:50.480 --> 00:32:51.800]   I have no idea what was in my Google+ account.
[00:32:51.800 --> 00:32:53.760]   It could have had my address and phone number.
[00:32:53.760 --> 00:32:54.200]   Yeah.
[00:32:54.200 --> 00:32:55.200]   I don't even know.
[00:32:55.200 --> 00:32:55.680]   Oh, no.
[00:32:55.680 --> 00:32:58.480]   Because I think that the potential here was real.
[00:32:58.480 --> 00:33:02.320]   I also think that to push back a little bit on what you said,
[00:33:02.320 --> 00:33:05.360]   Leo, I mean, your Microsoft analogy isn't completely wrong
[00:33:05.360 --> 00:33:07.120]   because obviously-- and yes, disclosure,
[00:33:07.120 --> 00:33:09.480]   I work at Microsoft, but I'm not speaking for them.
[00:33:09.480 --> 00:33:11.520]   And it's very common when companies have--
[00:33:11.520 --> 00:33:12.800]   Well, use Apple.
[00:33:12.800 --> 00:33:13.800]   It's the same.
[00:33:13.800 --> 00:33:15.200]   Well, yeah, we're one of our--
[00:33:15.200 --> 00:33:16.840]   but it's very common when you have vulnerabilities
[00:33:16.840 --> 00:33:18.560]   that you don't immediately disclose.
[00:33:18.560 --> 00:33:20.400]   You fix it and then you talk about it.
[00:33:20.400 --> 00:33:21.400]   The slight difference here--
[00:33:21.400 --> 00:33:22.240]   You might not even talk about it.
[00:33:22.240 --> 00:33:24.200]   Apple doesn't tell us all about vulnerabilities.
[00:33:24.200 --> 00:33:27.200]   Yeah, I've handled a lot of PR kind of situations.
[00:33:27.200 --> 00:33:29.200]   Sometimes you go, hey, let's not talk about this
[00:33:29.200 --> 00:33:31.000]   in the public for a good reason.
[00:33:31.000 --> 00:33:32.640]   Well, well, just some-- well, OK.
[00:33:32.640 --> 00:33:34.400]   But if you have this amount of thing--
[00:33:34.400 --> 00:33:36.400]   I mean, as you were pointing out, the GDPR thing,
[00:33:36.400 --> 00:33:38.000]   in this case, Google probably would
[00:33:38.000 --> 00:33:40.360]   have to disclose the way that they did with or without
[00:33:40.360 --> 00:33:43.360]   the journal article just because of A, the Cambridge
[00:33:43.360 --> 00:33:44.320]   Analytica scandal.
[00:33:44.320 --> 00:33:46.640]   So that's in the crosshairs.
[00:33:46.640 --> 00:33:50.120]   And B, Google is on probation and had
[00:33:50.120 --> 00:33:52.960]   to pay that huge FCC fine because of Google bot.
[00:33:52.960 --> 00:33:53.400]   It's true.
[00:33:53.400 --> 00:33:54.760]   They have a consent decree.
[00:33:54.760 --> 00:33:56.120]   So it's exactly.
[00:33:56.120 --> 00:33:58.360]   So to me, that makes this slightly different.
[00:33:58.360 --> 00:34:01.000]   And that's why I think if you're a publication, regardless
[00:34:01.000 --> 00:34:04.280]   of your rationale for why you may or may not be mad at Google,
[00:34:04.280 --> 00:34:06.880]   if you see documents and emails and look,
[00:34:06.880 --> 00:34:08.360]   somebody internally is clearly leaking them
[00:34:08.360 --> 00:34:10.600]   because they're uncomfortable that are making it look like we
[00:34:10.600 --> 00:34:15.560]   might not be letting this be public in the light of the
[00:34:15.560 --> 00:34:19.560]   Cambridge Analytica stuff, the ongoing kind of push for more
[00:34:19.560 --> 00:34:22.400]   information about how this information is used, and the
[00:34:22.400 --> 00:34:25.600]   fact that there is a consent decree because of the
[00:34:25.600 --> 00:34:30.240]   massive fine that you were levied over Google Buzz.
[00:34:30.240 --> 00:34:34.120]   It's definitely in Google's best interest, regardless of
[00:34:34.120 --> 00:34:37.360]   when they do it to let people know exactly to say, hey, look,
[00:34:37.360 --> 00:34:39.480]   whether you're going to use it as an excuse to shut down a
[00:34:39.480 --> 00:34:43.040]   dying social network or not, you should say, look, this was
[00:34:43.040 --> 00:34:43.560]   what happened.
[00:34:43.560 --> 00:34:44.160]   We fixed it.
[00:34:44.160 --> 00:34:46.080]   We don't have any evidence of any of this happening.
[00:34:46.080 --> 00:34:48.200]   But this was a thing that was out there.
[00:34:48.200 --> 00:34:52.400]   I mean, this vulnerability was open for three years.
[00:34:52.400 --> 00:34:53.320]   And they said nothing.
[00:34:53.320 --> 00:34:55.920]   And I had missed the part of the story where you had said that
[00:34:55.920 --> 00:34:58.480]   they were deleting their logs every two weeks.
[00:34:58.480 --> 00:35:01.160]   I mean, how could they possibly know over those three years
[00:35:01.160 --> 00:35:04.040]   whether anybody stole data and is now selling it on the
[00:35:04.040 --> 00:35:04.480]   blind one?
[00:35:04.480 --> 00:35:06.320]   But Journal says Google was unable to determine which
[00:35:06.320 --> 00:35:07.400]   users were affected.
[00:35:07.400 --> 00:35:09.360]   What types of data may potentially have been
[00:35:09.360 --> 00:35:12.520]   properly collected to Briepe, or briefed on the matter, said
[00:35:12.520 --> 00:35:17.040]   the bug existence 2015, Google says, actually, the Journal
[00:35:17.040 --> 00:35:19.600]   says Google believes 438 applications.
[00:35:19.600 --> 00:35:22.800]   So it's not an insignificant number, had access to the
[00:35:22.800 --> 00:35:25.920]   unauthorized Google+ data.
[00:35:25.920 --> 00:35:28.440]   They did, in fact, as part of the project's
[00:35:28.440 --> 00:35:32.000]   strobe investigation, check those apps to see if any of the
[00:35:32.000 --> 00:35:34.920]   developers had previous complaints against them.
[00:35:34.920 --> 00:35:36.640]   Determine that none of them look suspicious.
[00:35:36.640 --> 00:35:40.920]   But the Leaker said the company did not call or visit with any
[00:35:40.920 --> 00:35:41.840]   of those developers.
[00:35:41.840 --> 00:35:44.440]   They didn't follow up on that.
[00:35:44.440 --> 00:35:48.280]   Internal lawyers advised Google that it wasn't legally
[00:35:48.280 --> 00:35:50.000]   required to disclose the incident.
[00:35:50.000 --> 00:35:52.480]   The public, this happened before GDPR was in effect, by
[00:35:52.480 --> 00:35:54.080]   the way.
[00:35:54.080 --> 00:35:56.400]   And because the companies didn't know what developers may
[00:35:56.400 --> 00:35:59.600]   have had, what data the group also didn't believe in
[00:35:59.600 --> 00:36:02.400]   notifying users would give any actionable benefit to the
[00:36:02.400 --> 00:36:04.720]   end users.
[00:36:04.720 --> 00:36:08.440]   And the legal policy staff said, let's not publicize this, it
[00:36:08.440 --> 00:36:12.520]   might attract regulatory attention.
[00:36:12.520 --> 00:36:13.840]   Which is within the right.
[00:36:13.840 --> 00:36:15.120]   Like, that's why you have lawyers.
[00:36:15.120 --> 00:36:16.120]   Right.
[00:36:16.120 --> 00:36:18.560]   Like, I don't think that there's--
[00:36:18.560 --> 00:36:22.880]   I mean, aside from the GDPR and the EU stuff, which gets
[00:36:22.880 --> 00:36:28.720]   bananas in general, the company has no obligation to reveal
[00:36:28.720 --> 00:36:31.400]   inner workings unless there's a--
[00:36:31.400 --> 00:36:31.920]   I feel--
[00:36:31.920 --> 00:36:32.880]   It wasn't a breach.
[00:36:32.880 --> 00:36:33.720]   I guess that's the point.
[00:36:33.720 --> 00:36:34.480]   Exactly.
[00:36:34.480 --> 00:36:38.240]   But to Mark's point, as far as we know--
[00:36:38.240 --> 00:36:38.600]   Right.
[00:36:38.600 --> 00:36:39.160]   --wasn't a breach.
[00:36:39.160 --> 00:36:39.680]   Yeah, I mean--
[00:36:39.680 --> 00:36:40.160]   As far as Google knows.
[00:36:40.160 --> 00:36:43.400]   The journal was transparent to say that we--
[00:36:43.400 --> 00:36:47.200]   they had only used the word breach in a print headline, and
[00:36:47.200 --> 00:36:49.160]   they retracted that and corrected the argument.
[00:36:49.160 --> 00:36:50.320]   Which was the wrong word.
[00:36:50.320 --> 00:36:52.000]   Which was the wrong word, because they don't have evidence
[00:36:52.000 --> 00:36:52.200]   of it.
[00:36:52.200 --> 00:36:55.280]   But we don't have evidence that there wasn't a breach either.
[00:36:55.280 --> 00:36:58.160]   It's not as strong a story, though, if you said Google had a
[00:36:58.160 --> 00:37:01.120]   bug that it fixed.
[00:37:01.120 --> 00:37:01.880]   Sure, yeah.
[00:37:01.880 --> 00:37:06.120]   Breeze would be a stronger story, but this still frightens me
[00:37:06.120 --> 00:37:08.920]   that the second most--
[00:37:08.920 --> 00:37:12.640]   third most valuable company on Earth had a tremendous
[00:37:12.640 --> 00:37:15.880]   vulnerability in its system for three years that it
[00:37:15.880 --> 00:37:16.880]   chose not to disclose.
[00:37:16.880 --> 00:37:20.680]   I don't know how tremendous, because Google+, it wasn't
[00:37:20.680 --> 00:37:22.160]   everything Google knows about you.
[00:37:22.160 --> 00:37:26.200]   It was just what you actually put in your Google+ profile.
[00:37:26.200 --> 00:37:27.080]   Well, and it also--
[00:37:27.080 --> 00:37:28.680]   You should look at your profile to see what's in there.
[00:37:28.680 --> 00:37:29.800]   Isn't it shut down now?
[00:37:29.800 --> 00:37:30.800]   No, no, no, not till now.
[00:37:30.800 --> 00:37:31.320]   No, no, no.
[00:37:31.320 --> 00:37:32.840]   It's still one month to do it.
[00:37:32.840 --> 00:37:35.080]   I immediately went to say, well, what God, what do they
[00:37:35.080 --> 00:37:35.840]   could have they have learned?
[00:37:35.840 --> 00:37:38.720]   And the worst case with my home address and phone number,
[00:37:38.720 --> 00:37:40.120]   which I know is not--
[00:37:40.120 --> 00:37:43.120]   I mean, I agree with Mark.
[00:37:43.120 --> 00:37:44.560]   It's bad.
[00:37:44.560 --> 00:37:47.120]   And it's even worse optics for Google to say--
[00:37:47.120 --> 00:37:48.640]   I think that's the worst thing.
[00:37:48.640 --> 00:37:49.320]   Yeah.
[00:37:49.320 --> 00:37:52.080]   I think the optics here are worse than anything else.
[00:37:52.080 --> 00:37:55.240]   And I think that the optics in context of all the other
[00:37:55.240 --> 00:37:57.840]   things that are happening, regardless of whether it was
[00:37:57.840 --> 00:38:00.040]   legal or not, I mean, fine.
[00:38:00.040 --> 00:38:03.040]   That's not necessarily always the best decision to do something
[00:38:03.040 --> 00:38:05.120]   like this, because then you have to play cleanup when you're
[00:38:05.120 --> 00:38:08.320]   already battling regulators.
[00:38:08.320 --> 00:38:09.400]   It's maybe not ideal.
[00:38:09.400 --> 00:38:10.560]   I don't know.
[00:38:10.560 --> 00:38:12.760]   I would say, I think even in addition to maybe the
[00:38:12.760 --> 00:38:13.400]   information--
[00:38:13.400 --> 00:38:16.160]   No, Mark has nothing in his profile.
[00:38:16.160 --> 00:38:16.960]   No, I didn't mean it.
[00:38:16.960 --> 00:38:20.120]   No, I click on profile to see what's in there, and I get an
[00:38:20.120 --> 00:38:22.840]   error message that won't even tell me.
[00:38:22.840 --> 00:38:25.000]   Oh, well, you might have deleted yours or might not have
[00:38:25.000 --> 00:38:25.200]   been active.
[00:38:25.200 --> 00:38:26.640]   He's not right, try again.
[00:38:26.640 --> 00:38:28.160]   I love Google's error messages.
[00:38:28.160 --> 00:38:29.600]   I hate an error page, by the way.
[00:38:29.600 --> 00:38:31.840]   It's not a Chromebook.
[00:38:31.840 --> 00:38:32.840]   Oh, no, you're on Apple.
[00:38:32.840 --> 00:38:34.040]   You're on Apple, OK.
[00:38:34.040 --> 00:38:36.160]   But all I was going to say, though, is that the number of
[00:38:36.160 --> 00:38:36.640]   apps--
[00:38:36.640 --> 00:38:38.040]   I mean, OK, 438.
[00:38:38.040 --> 00:38:40.320]   But genuinely, if you looked at app usage, I mean, I think
[00:38:40.320 --> 00:38:42.280]   they'd shut down that API a long time ago.
[00:38:42.280 --> 00:38:44.520]   And so it's possible that somebody could have given access
[00:38:44.520 --> 00:38:46.280]   and somebody might have allowed them to do that.
[00:38:46.280 --> 00:38:49.600]   But again, this isn't like--
[00:38:49.600 --> 00:38:51.800]   apps were never a big part of that platform, even when they
[00:38:51.800 --> 00:38:53.840]   were forcing everyone to create an account.
[00:38:53.840 --> 00:38:56.400]   It wasn't a big part of the experience.
[00:38:56.400 --> 00:38:59.840]   So that's, I think, the good thing for end users who might
[00:38:59.840 --> 00:39:04.240]   be worried about their data, the likelihood of your stuff
[00:39:04.240 --> 00:39:07.720]   being breached, to me, seems low just because the number of
[00:39:07.720 --> 00:39:11.200]   people actually using applications was low.
[00:39:11.200 --> 00:39:13.280]   Yeah, you might remember that one of the things Google
[00:39:13.280 --> 00:39:17.120]   Plus never allowed was an app to post on your behalf.
[00:39:17.120 --> 00:39:19.920]   Unlike Twitter and Facebook, you can't--
[00:39:19.920 --> 00:39:20.440]   Exactly.
[00:39:20.440 --> 00:39:21.880]   -- can't auto post to Google Plus.
[00:39:21.880 --> 00:39:24.880]   You had to actually go to Google Plus and post.
[00:39:24.880 --> 00:39:26.360]   And the thing about this is, this
[00:39:26.360 --> 00:39:31.760]   is a drop in the bucket compared to what Facebook has done.
[00:39:31.760 --> 00:39:35.120]   This is potentially 500,000 users where Facebook has been
[00:39:35.120 --> 00:39:38.520]   in the millions of users with an actual breach.
[00:39:38.520 --> 00:39:41.120]   And the other thing is that this is something that came out
[00:39:41.120 --> 00:39:44.120]   from one person who leaks stuff out there, and then Google
[00:39:44.120 --> 00:39:47.960]   responded, I guarantee there's got to be tons of stuff like
[00:39:47.960 --> 00:39:50.280]   this that we just don't know about.
[00:39:50.280 --> 00:39:54.480]   So it's flawed.
[00:39:54.480 --> 00:39:55.120]   It's flawed.
[00:39:55.120 --> 00:39:55.800]   They made a mistake.
[00:39:55.800 --> 00:39:57.040]   They did what they kind of fix it.
[00:39:57.040 --> 00:40:01.080]   I think the Project Stroke blog post was really well written,
[00:40:01.080 --> 00:40:03.720]   and the whole approach they're taking to deal with it.
[00:40:03.720 --> 00:40:05.160]   And the ramifications of shutting Google
[00:40:05.160 --> 00:40:07.240]   Plus down, that shutting down a major product,
[00:40:07.240 --> 00:40:12.680]   whether anybody was using it or not, but still had usage.
[00:40:12.680 --> 00:40:15.720]   If you want to know if you were impacted by the Facebook
[00:40:15.720 --> 00:40:19.760]   breach, Facebook's now saying approximately 30 million
[00:40:19.760 --> 00:40:24.240]   accounts actually had information.
[00:40:24.240 --> 00:40:27.880]   Or I guess the breach was that somebody unauthorized could
[00:40:27.880 --> 00:40:30.280]   get the access token to your account.
[00:40:30.280 --> 00:40:33.080]   We have now determined, says Facebook, that this is as
[00:40:33.080 --> 00:40:36.440]   bad as you get, that attackers used access tokens to gain
[00:40:36.440 --> 00:40:39.200]   unauthorized access to account information from
[00:40:39.200 --> 00:40:44.000]   approximately 30 million Facebook accounts.
[00:40:44.000 --> 00:40:48.200]   That means somebody with your token can be you, can not only
[00:40:48.200 --> 00:40:51.440]   read everything on your Facebook page, including whatever
[00:40:51.440 --> 00:40:54.880]   information you post there, but post is you.
[00:40:54.880 --> 00:40:57.280]   And so that is pretty bad.
[00:40:57.280 --> 00:41:01.120]   What you can do is go to the Facebook Security Center, and
[00:41:01.120 --> 00:41:04.200]   they have a notice there, oh, I'm not logged into Facebook
[00:41:04.200 --> 00:41:06.680]   because I deleted my account.
[00:41:06.680 --> 00:41:10.680]   But if you have a Facebook account, it would say whether
[00:41:10.680 --> 00:41:11.320]   you were impacted.
[00:41:11.320 --> 00:41:13.760]   I did actually log into my account, reactivate my account,
[00:41:13.760 --> 00:41:17.320]   and found out that I was not affected by it.
[00:41:17.320 --> 00:41:22.280]   But it's another good reason.
[00:41:22.280 --> 00:41:24.760]   You know why I had to activate my account because I bought a
[00:41:24.760 --> 00:41:25.640]   Facebook portal.
[00:41:25.640 --> 00:41:27.880]   I bought two of them.
[00:41:27.880 --> 00:41:28.800]   Yeah, I'm an idiot.
[00:41:28.800 --> 00:41:29.960]   To talk to someone else, right?
[00:41:29.960 --> 00:41:30.960]   Right.
[00:41:30.960 --> 00:41:33.560]   Well, I found out later that you both can't talk to anybody
[00:41:33.560 --> 00:41:37.520]   on Facebook or to anybody with Facebook Messenger.
[00:41:37.520 --> 00:41:41.800]   But in order to pre-order this thing, I had to activate my
[00:41:41.800 --> 00:41:44.360]   Facebook account.
[00:41:44.360 --> 00:41:47.840]   And spend a not insignificant amount of money because I'm not
[00:41:47.840 --> 00:41:50.040]   going to buy the little one.
[00:41:50.040 --> 00:41:51.040]   I want the big one.
[00:41:51.040 --> 00:41:53.520]   If I look, if Facebook's going to follow me around the room, I
[00:41:53.520 --> 00:41:56.400]   want it to have the best possible camera.
[00:41:56.400 --> 00:41:58.000]   With the 15-inch screen, geez.
[00:41:58.000 --> 00:41:59.600]   Yeah, it's a giant screen.
[00:41:59.600 --> 00:42:00.520]   It rotates.
[00:42:00.520 --> 00:42:01.840]   It's $349.
[00:42:01.840 --> 00:42:05.680]   But I did get $100 off because I bought two.
[00:42:05.680 --> 00:42:07.800]   It is essentially a FaceTime camera, right?
[00:42:07.800 --> 00:42:09.880]   It allows you to make video phone calls.
[00:42:09.880 --> 00:42:12.600]   It has Amazon's Echo built in.
[00:42:12.600 --> 00:42:15.480]   But here's the thing that maybe Facebook might want to think
[00:42:15.480 --> 00:42:16.800]   twice about this.
[00:42:16.800 --> 00:42:21.000]   The camera can automatically follow you around.
[00:42:21.000 --> 00:42:21.760]   So--
[00:42:21.760 --> 00:42:27.000]   Yeah, which happens with a lot of meeting systems and other
[00:42:27.000 --> 00:42:27.720]   types of cameras.
[00:42:27.720 --> 00:42:28.760]   It's not a unique thing.
[00:42:28.760 --> 00:42:30.960]   It's just creepy in the context of Facebook
[00:42:30.960 --> 00:42:31.840]   creeping us all out.
[00:42:31.840 --> 00:42:34.520]   But it's actually a useful feature.
[00:42:34.520 --> 00:42:36.440]   Anybody who's ever had those in conference rooms like
[00:42:36.440 --> 00:42:38.360]   lots of tech makes them other companies do too.
[00:42:38.360 --> 00:42:39.840]   They're actually really great.
[00:42:39.840 --> 00:42:40.840]   They follow you around too.
[00:42:40.840 --> 00:42:43.640]   Because what they're doing is a 4K image and then they can
[00:42:43.640 --> 00:42:45.720]   zoom in on any part of that 4K image.
[00:42:45.720 --> 00:42:48.600]   So the camera basically is getting the whole room.
[00:42:48.600 --> 00:42:49.960]   Facebook does say it's--
[00:42:49.960 --> 00:42:52.040]   I might point out private by design.
[00:42:52.040 --> 00:42:55.040]   They offer a plastic clip you could put over the camera.
[00:42:55.040 --> 00:42:57.520]   So you don't have to use a piece of gaffer tape?
[00:42:57.520 --> 00:42:58.200]   Yeah.
[00:42:58.200 --> 00:42:59.760]   That's by design.
[00:42:59.760 --> 00:43:02.200]   And the microphone apparently does not have a piece of
[00:43:02.200 --> 00:43:03.280]   plastic to put over it.
[00:43:03.280 --> 00:43:09.000]   I have to review it, so I bought two of them.
[00:43:09.000 --> 00:43:10.440]   Anybody want to buy it off of me?
[00:43:10.440 --> 00:43:14.440]   What I understand is that I understand Facebook has
[00:43:14.440 --> 00:43:17.960]   millions of users, but people actually wanting to
[00:43:17.960 --> 00:43:20.680]   communicate via messenger in this way.
[00:43:20.680 --> 00:43:22.800]   I mean, I keep messenger turned off.
[00:43:22.800 --> 00:43:24.640]   Like, I don't want to be bothered on Facebook.
[00:43:24.640 --> 00:43:25.640]   I never use it.
[00:43:25.640 --> 00:43:28.360]   But is there enough of a customer base to justify
[00:43:28.360 --> 00:43:29.560]   making a piece of hardware?
[00:43:29.560 --> 00:43:31.280]   I think there's a few people use Facebook.
[00:43:31.280 --> 00:43:31.560]   What is it?
[00:43:31.560 --> 00:43:32.920]   One and a half billion?
[00:43:32.920 --> 00:43:34.160]   But for this?
[00:43:34.160 --> 00:43:35.240]   Well, look at this.
[00:43:35.240 --> 00:43:36.560]   Here's grandma.
[00:43:36.560 --> 00:43:39.240]   She's reading me three little pigs.
[00:43:39.240 --> 00:43:42.400]   And with this fine Facebook portal, she can actually
[00:43:42.400 --> 00:43:46.120]   superimpose a pig face on herself while she's reading.
[00:43:46.120 --> 00:43:46.960]   Thanks, Snapchat.
[00:43:46.960 --> 00:43:54.240]   I understand my mediocracy earlier.
[00:43:54.240 --> 00:43:57.760]   I wonder, is this just, is this nobody's going to buy this?
[00:43:57.760 --> 00:43:58.440]   Or?
[00:43:58.440 --> 00:44:00.400]   Yeah, I think this is the this is the way.
[00:44:00.400 --> 00:44:00.920]   This is the way.
[00:44:00.920 --> 00:44:01.400]   Yeah.
[00:44:01.400 --> 00:44:01.920]   OK.
[00:44:01.920 --> 00:44:04.720]   And I think this would likely be DOA even without the
[00:44:04.720 --> 00:44:08.080]   other issues for the reasons that Ron mentioned.
[00:44:08.080 --> 00:44:11.320]   Like, I think that even though you potentially have a lot of
[00:44:11.320 --> 00:44:13.800]   people who use Facebook to communicate, they're not going
[00:44:13.800 --> 00:44:19.480]   to be aware or care about the Facebook portal.
[00:44:19.480 --> 00:44:21.800]   They're not going to spend $350.
[00:44:21.800 --> 00:44:22.240]   No.
[00:44:22.240 --> 00:44:25.280]   And also, just the way Facebook is branded themselves, you
[00:44:25.280 --> 00:44:27.920]   know, you don't think about them for hardware the same way
[00:44:27.920 --> 00:44:29.960]   that Amazon's been able to do that and the Google's been
[00:44:29.960 --> 00:44:31.280]   able to do that.
[00:44:31.280 --> 00:44:33.560]   But I think when you have all the other things going with it,
[00:44:33.560 --> 00:44:35.680]   the creep factor is just too high.
[00:44:35.680 --> 00:44:37.720]   Like, I mean, this is the sort of thing where if you've had
[00:44:37.720 --> 00:44:40.400]   this feature and it was another company who was well known
[00:44:40.400 --> 00:44:42.600]   for home products who released it under their name and their
[00:44:42.600 --> 00:44:45.640]   brand, and you could use, you know, you could do Facebook
[00:44:45.640 --> 00:44:47.200]   chat with people.
[00:44:47.200 --> 00:44:48.000]   That's one thing.
[00:44:48.000 --> 00:44:51.640]   But I think being under the Facebook branding and, you know,
[00:44:51.640 --> 00:44:55.360]   like, I feel like that's more as much as anything else, the
[00:44:55.360 --> 00:44:56.840]   DOA part.
[00:44:56.840 --> 00:44:59.520]   As far as I can tell, there's no way for me to cancel this.
[00:44:59.520 --> 00:45:03.520]   You can always return it.
[00:45:03.520 --> 00:45:04.120]   I guess.
[00:45:04.120 --> 00:45:05.520]   Use your credit card.
[00:45:05.520 --> 00:45:06.040]   Yeah.
[00:45:06.040 --> 00:45:06.520]   I did.
[00:45:06.520 --> 00:45:07.520]   Yeah.
[00:45:07.520 --> 00:45:12.120]   Um, I don't.
[00:45:12.120 --> 00:45:13.120]   This is expensive.
[00:45:13.120 --> 00:45:14.200]   You tax right off Leo.
[00:45:14.200 --> 00:45:14.960]   This is totally.
[00:45:14.960 --> 00:45:17.720]   I mean, it's for review purposes and I felt like maybe I
[00:45:17.720 --> 00:45:18.240]   should review it.
[00:45:18.240 --> 00:45:19.920]   But now I'm wondering if you like a loss.
[00:45:19.920 --> 00:45:21.720]   Does anybody care if I review it?
[00:45:21.720 --> 00:45:22.800]   It sounds like nobody.
[00:45:22.800 --> 00:45:24.080]   Nobody does.
[00:45:24.080 --> 00:45:26.120]   Well, let's let me take a break and then when we get back,
[00:45:26.120 --> 00:45:28.960]   we'll talk about it's interesting because Google's
[00:45:28.960 --> 00:45:31.840]   offering something similar without a camera.
[00:45:31.840 --> 00:45:34.400]   Google Google's not no fool, right?
[00:45:34.400 --> 00:45:36.960]   And they may even made a point at the event, didn't they?
[00:45:36.960 --> 00:45:39.000]   Run there and there's no camera.
[00:45:39.000 --> 00:45:40.080]   Yep.
[00:45:40.080 --> 00:45:42.320]   This is the day after Facebook announced this portal.
[00:45:42.320 --> 00:45:44.560]   We'll talk about the Google event when we come back with
[00:45:44.560 --> 00:45:44.840]   Robert.
[00:45:44.840 --> 00:45:48.360]   They didn't mention the Google+ thing at all.
[00:45:48.360 --> 00:45:49.480]   Oh, of course not.
[00:45:49.480 --> 00:45:50.960]   I thought I was wondering if they would.
[00:45:50.960 --> 00:45:51.280]   Yeah.
[00:45:51.280 --> 00:45:52.720]   It didn't bring it up.
[00:45:52.720 --> 00:45:54.760]   Why should they?
[00:45:54.760 --> 00:45:56.120]   It's old news.
[00:45:56.120 --> 00:45:58.160]   They haven't even mentioned it on Google+.
[00:45:58.160 --> 00:46:00.280]   That's the part that kind of incenses me.
[00:46:00.280 --> 00:46:03.040]   Is it for the people who still love it and are very upset?
[00:46:03.040 --> 00:46:05.000]   And I look, we've all had social networks.
[00:46:05.000 --> 00:46:06.520]   We we love go away.
[00:46:06.760 --> 00:46:09.720]   I'm not going to make fun of anybody who uses that community
[00:46:09.720 --> 00:46:11.800]   and as part of that, like I genuinely feel anything.
[00:46:11.800 --> 00:46:12.360]   I still post here.
[00:46:12.360 --> 00:46:13.360]   I love Google+.
[00:46:13.360 --> 00:46:15.920]   What bothers me though is that there's been literally
[00:46:15.920 --> 00:46:20.280]   no response or outreach to that community from Google, which,
[00:46:20.280 --> 00:46:21.920]   I mean, I feel like if you're going to do this,
[00:46:21.920 --> 00:46:24.840]   regardless of whether you're using as an excuse or not,
[00:46:24.840 --> 00:46:27.640]   like you owe it to the people that actually do use it,
[00:46:27.640 --> 00:46:30.880]   however infinitesimal they might be to, you know,
[00:46:30.880 --> 00:46:32.720]   Google killed Google+.
[00:46:32.720 --> 00:46:33.240]   Say something.
[00:46:33.240 --> 00:46:36.120]   A year ago when they, when they, I mean,
[00:46:36.560 --> 00:46:38.560]   three years ago when they did the Pinterest design,
[00:46:38.560 --> 00:46:40.000]   when they made it Pinterest to me.
[00:46:40.000 --> 00:46:40.520]   I like it.
[00:46:40.520 --> 00:46:43.000]   And you know, photographers like Trey Reklev
[00:46:43.000 --> 00:46:45.320]   made his, made his name on Google+.
[00:46:45.320 --> 00:46:47.400]   Because it was such a good place for photographers.
[00:46:47.400 --> 00:46:48.280]   Yeah.
[00:46:48.280 --> 00:46:51.080]   People like Mike Elgin,
[00:46:51.080 --> 00:46:53.480]   it's really saying it's praises.
[00:46:53.480 --> 00:46:54.120]   I loved it.
[00:46:54.120 --> 00:46:58.120]   But I mean, I'm looking at my Google+ page is filled with spam
[00:46:58.120 --> 00:47:00.840]   of a sexual nature in many cases.
[00:47:00.840 --> 00:47:03.880]   And I'm a, I'm a member of a lot of,
[00:47:03.880 --> 00:47:05.720]   remember they pushed communities?
[00:47:05.720 --> 00:47:07.920]   So I joined a lot of geek communities.
[00:47:07.920 --> 00:47:10.160]   And now I'm getting porn spam.
[00:47:10.160 --> 00:47:14.000]   That's not a good look for Google+.
[00:47:14.000 --> 00:47:17.120]   The one, the one metric from the product strobe,
[00:47:17.120 --> 00:47:20.240]   the one was that I think that the average time spent on the age.
[00:47:20.240 --> 00:47:20.800]   Plus fee.
[00:47:20.800 --> 00:47:22.040]   Don't show it on TV.
[00:47:22.040 --> 00:47:23.040]   Well, you see,
[00:47:23.040 --> 00:47:25.160]   Martin to join the geek porn community.
[00:47:25.160 --> 00:47:26.560]   It's not a geek porn community.
[00:47:26.560 --> 00:47:27.560]   It's geeks.
[00:47:27.560 --> 00:47:29.880]   I'm going to, I guess I should unjoin it.
[00:47:29.880 --> 00:47:31.720]   Nobody's, that's part of the problem.
[00:47:31.720 --> 00:47:33.760]   Right, Ron, if you don't moderate your community,
[00:47:33.760 --> 00:47:35.400]   right, the bad guys.
[00:47:35.800 --> 00:47:36.960]   Just totally get to it.
[00:47:36.960 --> 00:47:37.760]   They get there.
[00:47:37.760 --> 00:47:40.640]   So I'm going to leave geeks, the Google+ geek community,
[00:47:40.640 --> 00:47:42.000]   apparently is not.
[00:47:42.000 --> 00:47:43.880]   Oh man, what's this one?
[00:47:43.880 --> 00:47:44.640]   Photography.
[00:47:44.640 --> 00:47:46.760]   Oh, now I have to leave photography too.
[00:47:46.760 --> 00:47:48.840]   They're all full of it.
[00:47:48.840 --> 00:47:51.880]   The data point is that it was the average time spent on page by Google+
[00:47:51.880 --> 00:47:53.520]   users was like six seconds.
[00:47:53.520 --> 00:47:54.520]   They said, right.
[00:47:54.520 --> 00:47:54.720]   Yeah.
[00:47:54.720 --> 00:47:56.920]   This is in the project's show announcement.
[00:47:56.920 --> 00:48:01.320]   We found that 90% of Google+ users left the page after five seconds.
[00:48:01.320 --> 00:48:02.160]   Yeah.
[00:48:02.160 --> 00:48:04.960]   Because they went, whoa, why my eyes?
[00:48:04.960 --> 00:48:08.160]   Or they went to the page and posted that link and then went to the next one.
[00:48:08.160 --> 00:48:09.800]   Because they're just spamming it.
[00:48:09.800 --> 00:48:13.880]   It's too bad because honestly, it's a beautiful, am I wrong?
[00:48:13.880 --> 00:48:15.240]   I think it's a beautiful social network.
[00:48:15.240 --> 00:48:20.040]   It has moderation tools better than Twitter, better than Facebook.
[00:48:20.040 --> 00:48:24.080]   I thought Google, why Google+ should have won.
[00:48:24.080 --> 00:48:25.280]   Why did it not win?
[00:48:25.280 --> 00:48:30.280]   Google fundamentally doesn't understand social as a company, as a culture.
[00:48:30.280 --> 00:48:31.680]   They fundamentally don't understand the whole.
[00:48:31.680 --> 00:48:32.320]   Great tool.
[00:48:32.320 --> 00:48:33.240]   Why don't they use it?
[00:48:34.240 --> 00:48:39.320]   I don't think anybody building the products or steering it, understand social.
[00:48:39.320 --> 00:48:39.920]   I just don't.
[00:48:39.920 --> 00:48:42.360]   And I think that you can have all the best things.
[00:48:42.360 --> 00:48:46.640]   But if you're not coming at it from that perspective, I mean, there was an interesting
[00:48:46.640 --> 00:48:48.120]   medium post or it wasn't a medium.
[00:48:48.120 --> 00:48:51.320]   Sorry, it was somebody's blog who, someone who's on the design team for the first eight
[00:48:51.320 --> 00:48:53.720]   months who kind of talked about his experiences.
[00:48:53.720 --> 00:48:58.040]   And they're obviously colored in some ways that maybe aren't fair to the final product.
[00:48:58.040 --> 00:49:03.080]   But it was always from the very beginning, it was a defensive move, always.
[00:49:03.480 --> 00:49:04.680]   And that comes through.
[00:49:04.680 --> 00:49:10.040]   And I think that the fact that it was, it was designed because Google realized that they'd
[00:49:10.040 --> 00:49:10.760]   miss social.
[00:49:10.760 --> 00:49:16.120]   And then the fact that it was pushed down everyone's throats so much to the point where
[00:49:16.120 --> 00:49:18.360]   there wasn't even a reason for people to want to use it.
[00:49:18.360 --> 00:49:19.320]   You were almost turned off.
[00:49:19.320 --> 00:49:22.200]   I mean, there was a point where it was linked to your, to all of your Google stuff.
[00:49:22.200 --> 00:49:24.760]   Like I'm verified on YouTube because of it.
[00:49:24.760 --> 00:49:25.720]   So I'm thankful for that.
[00:49:25.720 --> 00:49:30.840]   I had a ridiculous number of Google+ followers.
[00:49:30.840 --> 00:49:34.600]   It was, I would joke about it because like it was hilarious.
[00:49:34.600 --> 00:49:35.560]   It still is.
[00:49:35.560 --> 00:49:37.560]   But I didn't really ever see that sort of engagement.
[00:49:37.560 --> 00:49:41.320]   It was, it was just kind of this weird thing foisted upon people.
[00:49:41.320 --> 00:49:46.120]   And I don't think it ever had direction from people who saw it as anything other than we've
[00:49:46.120 --> 00:49:51.640]   got to try to beat Facebook because that's an important part of the future going forward.
[00:49:51.640 --> 00:49:54.120]   And I mean, it is what it is.
[00:49:54.120 --> 00:49:55.320]   It's a shame, but it is what it is.
[00:49:55.320 --> 00:49:56.120]   It is a shame.
[00:49:57.960 --> 00:50:03.240]   By the way, if you want to learn Google+, the Brooklyn Public Library on Tuesday is having a
[00:50:03.240 --> 00:50:04.360]   class.
[00:50:04.360 --> 00:50:06.840]   It's never too late to learn Google+.
[00:50:06.840 --> 00:50:08.120]   Oh, God.
[00:50:08.120 --> 00:50:09.560]   It will be in 10 months.
[00:50:09.560 --> 00:50:10.280]   That's so good.
[00:50:10.280 --> 00:50:12.040]   I'm so, that's my old library.
[00:50:12.040 --> 00:50:13.160]   That's my old library.
[00:50:13.160 --> 00:50:14.360]   I'm so, oh, God.
[00:50:14.360 --> 00:50:16.360]   I want to go to that Tuesday.
[00:50:16.360 --> 00:50:16.840]   Me too.
[00:50:16.840 --> 00:50:17.640]   We want to go to that.
[00:50:17.640 --> 00:50:18.440]   Just see what we can see.
[00:50:18.440 --> 00:50:18.920]   I'll go.
[00:50:18.920 --> 00:50:21.560]   You guys want to, I'm just, I'm five minutes from Brooklyn.
[00:50:21.560 --> 00:50:22.600]   Check it out.
[00:50:22.600 --> 00:50:25.240]   I think that was literally like my, like,
[00:50:25.240 --> 00:50:29.080]   was literally like a five minute walk from my apartment for many years.
[00:50:29.080 --> 00:50:31.000]   I'm very sad that I can't go to that.
[00:50:31.000 --> 00:50:31.720]   That's so good.
[00:50:31.720 --> 00:50:37.160]   Everybody in Brooklyn who loves Google+, should go to this event.
[00:50:37.160 --> 00:50:39.480]   And it's a very Brooklyn thing too.
[00:50:39.480 --> 00:50:41.480]   It's like, you go to this ironically.
[00:50:41.480 --> 00:50:43.480]   What day in time is it?
[00:50:43.480 --> 00:50:46.760]   It's the Tuesday at 11 a.m. at the Pacific Library.
[00:50:46.760 --> 00:50:49.240]   I was going to say, I was going to say Tuesday is when we do all about Androids.
[00:50:49.240 --> 00:50:51.480]   Oh, but you can do it before all about Androids.
[00:50:52.680 --> 00:50:55.160]   25 fourth street at Pacific Avenue, Brooklyn.
[00:50:55.160 --> 00:50:59.240]   Everybody show up just to show the flag.
[00:50:59.240 --> 00:51:02.600]   It's sad to me because it really has great tools.
[00:51:02.600 --> 00:51:07.320]   It could have been, you know, I see Mike still posting.
[00:51:07.320 --> 00:51:09.400]   That's where I got that link, by the way, from Mike Elgin.
[00:51:09.400 --> 00:51:15.000]   Trey Radcliffe is actually putting together a documentary kind of an online thing about
[00:51:15.000 --> 00:51:18.840]   saying goodbye where people are going to, he wanted me to record something and I'll do it.
[00:51:19.560 --> 00:51:23.560]   Record their thoughts and memories of Google+, it's sad.
[00:51:23.560 --> 00:51:31.240]   Although, I noticed Carson will not show my Google+, he's not going to show my Google,
[00:51:31.240 --> 00:51:32.520]   he saw what happened, right?
[00:51:32.520 --> 00:51:35.640]   Poor Mark Millian's going, why?
[00:51:35.640 --> 00:51:36.760]   I don't want to see that.
[00:51:36.760 --> 00:51:40.600]   The spammers just got a hold of it.
[00:51:40.600 --> 00:51:41.960]   Well, how do you survive that, Ron?
[00:51:41.960 --> 00:51:43.560]   You have to go in every day, don't you?
[00:51:43.560 --> 00:51:45.960]   His name is Jason Howell.
[00:51:45.960 --> 00:51:49.400]   And he's a blessing for all of us.
[00:51:49.400 --> 00:51:52.600]   No, we flag, but also the community does.
[00:51:52.600 --> 00:51:56.360]   I mean, but we flag and mark things as spam and stuff like that, but stuff gets through.
[00:51:56.360 --> 00:52:00.360]   But if you flag it, you can, that's the thing, it has moderation tools.
[00:52:00.360 --> 00:52:01.560]   You can immediately get rid of it.
[00:52:01.560 --> 00:52:08.920]   But it's just a testament to how few groups are being moderated that these spammers are flourishing
[00:52:08.920 --> 00:52:11.560]   because it works, you know?
[00:52:14.200 --> 00:52:17.080]   Our show today brought to you by FreshBooks.
[00:52:17.080 --> 00:52:20.760]   If you are a freelancer, I know a lot of you are a small business, you know that the,
[00:52:20.760 --> 00:52:26.440]   you love, I'm sure you love what you do, but there's a part of your business you may not love.
[00:52:26.440 --> 00:52:31.480]   That is doing those invoices at the end of the month, right?
[00:52:31.480 --> 00:52:32.600]   Sending out the invoices.
[00:52:32.600 --> 00:52:35.160]   It's like paying bills.
[00:52:35.160 --> 00:52:40.120]   You just, you don't want to do it, but if you don't send out the invoices, the bills will not get paid.
[00:52:40.120 --> 00:52:42.120]   I learned that when I was a freelancer.
[00:52:42.120 --> 00:52:45.400]   This is back when I was going to Canada every month to do a call for help.
[00:52:45.400 --> 00:52:48.680]   Amber MacArthur, my co-host at the time, because I was moaning about it,
[00:52:48.680 --> 00:52:49.880]   said, "You got to try FreshBooks."
[00:52:49.880 --> 00:52:51.320]   They had just started up in Toronto.
[00:52:51.320 --> 00:52:53.400]   It was, I'll tell you how long ago that was.
[00:52:53.400 --> 00:52:55.640]   It was a Web 2.0 startup.
[00:52:55.640 --> 00:52:57.480]   And it changed my life.
[00:52:57.480 --> 00:52:59.000]   I was able to do these invoices.
[00:52:59.000 --> 00:53:00.280]   It was in Canadian dollars.
[00:53:00.280 --> 00:53:01.640]   It did the currency automatically.
[00:53:01.640 --> 00:53:03.400]   There's every currency automatically.
[00:53:03.400 --> 00:53:07.640]   And the neat thing with FreshBooks is just merely in the process of sending invoices,
[00:53:07.640 --> 00:53:11.400]   you're actually doing your books because you make the invoices.
[00:53:11.400 --> 00:53:13.480]   That's going to be your accounts receivable.
[00:53:13.480 --> 00:53:15.320]   You put expenses into the invoices.
[00:53:15.320 --> 00:53:17.720]   You can even use the FreshBooks app to take pictures of receipts,
[00:53:17.720 --> 00:53:20.280]   gets right into the invoice.
[00:53:20.280 --> 00:53:20.840]   So there you go.
[00:53:20.840 --> 00:53:21.720]   There's your expenses.
[00:53:21.720 --> 00:53:25.800]   It'll even download your expenses automatically from your bank account.
[00:53:25.800 --> 00:53:31.240]   And then when you get invoices gets paid, you record it in there and you'll have everything.
[00:53:31.240 --> 00:53:36.280]   You have sent viewed, viewed, yes, you know whether they've read your invoice or not.
[00:53:36.280 --> 00:53:37.320]   That's nice.
[00:53:37.320 --> 00:53:38.680]   People say, "No, I'm a good, good, good voice.
[00:53:38.680 --> 00:53:39.640]   Yes, you did."
[00:53:39.640 --> 00:53:41.000]   Paid, you know what's overdue.
[00:53:41.000 --> 00:53:44.600]   They'll automatically send out reminders to pay.
[00:53:44.600 --> 00:53:48.360]   And actually, the truth is you get paid faster on average about twice as fast
[00:53:48.360 --> 00:53:52.920]   because every FreshBooks invoice also has an automatic pay me button.
[00:53:52.920 --> 00:53:55.080]   People, your clients can pay online.
[00:53:55.080 --> 00:54:00.760]   Turns out clients are no more anxious to pay bills than you are to send bills.
[00:54:00.760 --> 00:54:03.400]   So if you make it easy for them, they go, "Yeah, yeah, let's just pay it.
[00:54:03.400 --> 00:54:04.040]   Get it over with."
[00:54:04.040 --> 00:54:06.280]   You can create proposals now.
[00:54:06.280 --> 00:54:09.480]   Because it's a web app, they constantly added features.
[00:54:09.480 --> 00:54:11.880]   Create proposals with rich text content and images.
[00:54:11.880 --> 00:54:13.400]   Request your clients e-signature.
[00:54:13.400 --> 00:54:15.080]   Yes, you can accept e-signatures.
[00:54:15.080 --> 00:54:15.960]   All this is automatic.
[00:54:15.960 --> 00:54:16.760]   You accept credit cards.
[00:54:16.760 --> 00:54:17.960]   You don't have to set up a merchant account.
[00:54:17.960 --> 00:54:18.840]   They do it all for you.
[00:54:18.840 --> 00:54:24.840]   Let's see, the mobile app will let you keep track of your business,
[00:54:24.840 --> 00:54:25.720]   no matter where you are.
[00:54:25.720 --> 00:54:27.080]   And you can also give employees.
[00:54:27.080 --> 00:54:29.320]   If you have them access to that information,
[00:54:29.320 --> 00:54:31.480]   you can control what their access looks like.
[00:54:31.480 --> 00:54:35.400]   So you can have them put in invoices or check in voices.
[00:54:35.400 --> 00:54:37.240]   There's a, I mean, I can go on and on.
[00:54:37.240 --> 00:54:38.520]   They're always adding new features.
[00:54:38.520 --> 00:54:41.400]   That's why it's a really, and what a great support team.
[00:54:41.400 --> 00:54:42.440]   The best, because they're nice.
[00:54:42.440 --> 00:54:43.480]   They're Canadian.
[00:54:43.480 --> 00:54:44.920]   Nice support team.
[00:54:44.920 --> 00:54:48.600]   Check out their learning hub videos now, which make it easier.
[00:54:48.600 --> 00:54:49.960]   Although I don't think you're going to need them.
[00:54:49.960 --> 00:54:51.800]   You just sit down and it's just kind of automatic.
[00:54:51.800 --> 00:54:54.440]   You're doing bookkeeping without doing bookkeeping.
[00:54:54.440 --> 00:54:57.960]   It is the ridiculously easy to use cloud accounting software,
[00:54:57.960 --> 00:54:59.880]   helping small business owners thrive.
[00:54:59.880 --> 00:55:03.160]   24 million people have used FreshBooks, including me,
[00:55:03.160 --> 00:55:06.520]   to painlessly send invoices, track time to capture.
[00:55:06.520 --> 00:55:08.600]   Yes, they've, you could build by time in hours.
[00:55:08.600 --> 00:55:11.000]   They have a little timer built into the website or the app.
[00:55:11.000 --> 00:55:13.960]   And to capture expenses, try it free right now.
[00:55:13.960 --> 00:55:14.840]   That's the best advice.
[00:55:14.840 --> 00:55:18.600]   That way you'll know, you know, if you don't like it, no harm.
[00:55:18.600 --> 00:55:21.480]   30 days free at freshbooks.com/twit.
[00:55:21.480 --> 00:55:24.200]   And this week in tech, and they, how did you hear best of sections?
[00:55:24.200 --> 00:55:24.920]   So we get credit.
[00:55:24.920 --> 00:55:27.720]   That's freshbooks.com/twit.
[00:55:27.720 --> 00:55:30.600]   It was really a great thing for me.
[00:55:30.600 --> 00:55:32.280]   So Ron, you went to the Google event.
[00:55:32.280 --> 00:55:33.000]   That looked pretty cool.
[00:55:33.000 --> 00:55:35.080]   And I have to give myself credit because we streamed it.
[00:55:35.080 --> 00:55:37.080]   They streamed it and we've re-streamed it.
[00:55:37.080 --> 00:55:38.600]   Florence, I, on, and I commented.
[00:55:38.600 --> 00:55:42.600]   And I looked at those pedestals they had in the middle of the audience and said,
[00:55:42.600 --> 00:55:44.680]   I bet you products are going to emerge from there.
[00:55:44.680 --> 00:55:47.960]   I was being facetious, but they did.
[00:55:47.960 --> 00:55:48.840]   Oh, Ron's muted.
[00:55:48.840 --> 00:55:50.920]   I was muted during the ad, sorry.
[00:55:50.920 --> 00:55:54.520]   I thought they were like big Google Home design props.
[00:55:54.520 --> 00:55:56.520]   I thought they're going to roll out like a new version of Google Home.
[00:55:56.520 --> 00:55:57.480]   That's what it looked like.
[00:55:57.480 --> 00:56:01.000]   But I was actually sitting right in front of one of those.
[00:56:01.000 --> 00:56:03.480]   And when the products came up out of it, I think,
[00:56:03.480 --> 00:56:05.880]   I'm so, I don't know if on the feed, on the live stream,
[00:56:05.880 --> 00:56:07.480]   you could hear me actually laugh that loud.
[00:56:07.480 --> 00:56:08.680]   >> I love it.
[00:56:08.680 --> 00:56:11.240]   >> Because it was just such a bananas decision.
[00:56:11.240 --> 00:56:15.160]   Because I mean, the whole story of this event was this was the work,
[00:56:15.160 --> 00:56:21.000]   this is the most poorly kept secret in all of tech for the last month and a half.
[00:56:21.000 --> 00:56:26.280]   We knew every product, we knew specs, we'd seen it.
[00:56:26.280 --> 00:56:30.840]   And they had, they kind of poked fun at it with that little video where they're showing clips
[00:56:30.840 --> 00:56:32.680]   of people talking about it and stuff like that.
[00:56:32.680 --> 00:56:38.200]   >> They actually asked YouTubers to give them clips of them saying we know everything Google's
[00:56:38.200 --> 00:56:38.840]   going to announce.
[00:56:38.840 --> 00:56:40.200]   And they did a montage.
[00:56:40.200 --> 00:56:43.960]   I as actor was in the opening place of the montage.
[00:56:43.960 --> 00:56:48.040]   >> And all the montages did was just show an emoji like a shrug and then they just moved on.
[00:56:48.040 --> 00:56:51.800]   >> I thought when they tweeted that the day before, they were saying something like,
[00:56:51.800 --> 00:56:53.720]   oh, you'll be surprised.
[00:56:53.720 --> 00:56:54.280]   >> You think you know.
[00:56:54.280 --> 00:56:55.960]   >> You think you know, but you don't know, we knew.
[00:56:55.960 --> 00:56:57.240]   >> That's what we held for.
[00:56:57.240 --> 00:56:59.640]   But it turns out we knew there was nothing, there was no surprise.
[00:56:59.640 --> 00:57:04.680]   But so they got up there and Rick Osterlo, the head of hardware for Google,
[00:57:04.680 --> 00:57:11.000]   gets out there and basically just says this is what we're announcing, the phone, the home hub,
[00:57:11.000 --> 00:57:13.000]   and the slate.
[00:57:13.000 --> 00:57:15.960]   And now you can see it and it comes up out of these pedestals.
[00:57:15.960 --> 00:57:18.840]   And he's like, you only got 30 seconds and then 30 seconds.
[00:57:18.840 --> 00:57:21.480]   >> They went back to the pedestal and they never came back out.
[00:57:21.480 --> 00:57:23.720]   >> That's a lot of work for that.
[00:57:23.720 --> 00:57:24.520]   >> Yeah.
[00:57:24.520 --> 00:57:26.040]   >> Well, I mean, what are they going to do?
[00:57:26.040 --> 00:57:30.600]   I mean, I feel for them because this was the most leaked phone ever because,
[00:57:30.600 --> 00:57:33.400]   okay, first the renders come out, that's common.
[00:57:33.400 --> 00:57:38.040]   Then so many renders come out that it's left in a lift and the lift driver recognizes it
[00:57:38.040 --> 00:57:39.640]   and sends it to one of the Android products.
[00:57:39.640 --> 00:57:41.240]   >> He actually recognized it.
[00:57:41.240 --> 00:57:42.040]   >> He actually recognized it.
[00:57:42.040 --> 00:57:46.040]   He returned it to the owner but took a photo first.
[00:57:46.040 --> 00:57:47.400]   You know, then more stuff gets leaked.
[00:57:47.400 --> 00:57:53.400]   It's available for sale in Hong Kong before the event is out.
[00:57:53.400 --> 00:57:56.280]   So Richard Lai at Engadget buys it and reviews it.
[00:57:56.280 --> 00:58:03.480]   Then, then, then, at the event, they literally make the event wrap up video
[00:58:03.480 --> 00:58:05.320]   public on YouTube before the event starts.
[00:58:05.320 --> 00:58:06.360]   >> You're kidding.
[00:58:06.360 --> 00:58:07.400]   >> No, I'm not.
[00:58:07.400 --> 00:58:15.080]   Like one of the guy from Android police noted that literally the wrap up video was live.
[00:58:15.080 --> 00:58:18.680]   >> Well, they did that to capture the search traffic.
[00:58:18.680 --> 00:58:21.160]   They wanted to have the first wrap up video out.
[00:58:21.160 --> 00:58:22.200]   That's why that's cool.
[00:58:22.200 --> 00:58:23.560]   What do they have to capture any?
[00:58:23.560 --> 00:58:26.040]   >> You think they could manipulate that.
[00:58:26.040 --> 00:58:31.640]   >> Yeah, no, I mean, literally, that's the, I mean, it's like, I gave Apple, I think,
[00:58:31.640 --> 00:58:35.400]   deservedly a lot of made fun of them or not.
[00:58:35.400 --> 00:58:36.440]   Yeah, yeah, did.
[00:58:36.440 --> 00:58:42.360]   When the iPhone 10s and the watch leaked because they were available on their website
[00:58:42.360 --> 00:58:44.920]   weeks before the event because that was just sloppiness.
[00:58:44.920 --> 00:58:47.720]   But this is just like, this is literally another level.
[00:58:47.720 --> 00:58:49.880]   I mean, and look, Microsoft, we've definitely had leaks too.
[00:58:49.880 --> 00:58:52.360]   And I feel for the teams that go through this.
[00:58:52.360 --> 00:58:53.400]   But what are you going to do?
[00:58:53.400 --> 00:58:54.760]   You still got to have the events.
[00:58:54.760 --> 00:59:00.440]   And in all honesty, there are lots and lots and lots and lots and lots of people out there
[00:59:00.440 --> 00:59:01.640]   who didn't pay attention to that.
[00:59:01.640 --> 00:59:03.960]   The people who were in that room all know what they're getting.
[00:59:03.960 --> 00:59:06.200]   And if anything, I mean, Google, congratulations.
[00:59:06.200 --> 00:59:09.320]   You made it way easier for all the journalists in the room to already have their stories pre-wrote
[00:59:09.320 --> 00:59:11.400]   and so they could just focus on other things.
[00:59:11.400 --> 00:59:14.440]   >> But there's also another interesting narrative that I think they came out of it.
[00:59:14.440 --> 00:59:21.080]   They're not all people talking about is that a lot of the opinions from the leaks were negative.
[00:59:21.080 --> 00:59:22.440]   >> Yes.
[00:59:22.440 --> 00:59:24.760]   >> And it was like the Pixel 3 is not going to be a good phone.
[00:59:24.760 --> 00:59:26.840]   It's not as good as the Pixel 2 and all this.
[00:59:26.840 --> 00:59:27.480]   What is this?
[00:59:27.480 --> 00:59:32.360]   >> People still, I mean, very few people have seen the Pixel 3 because it's only coming out on
[00:59:32.360 --> 00:59:34.360]   Thursday, although I guess it's some phone stores.
[00:59:34.360 --> 00:59:37.880]   People are still based on pictures alone saying that Nash is hideous.
[00:59:37.880 --> 00:59:38.600]   >> Yeah.
[00:59:38.600 --> 00:59:39.960]   >> And now you saw it.
[00:59:39.960 --> 00:59:40.840]   Is the not hideous?
[00:59:40.840 --> 00:59:42.200]   >> No, it's not hideous.
[00:59:42.200 --> 00:59:43.480]   And they have a whole bunch of people.
[00:59:43.480 --> 00:59:45.640]   >> That's a joke, by the way.
[00:59:45.640 --> 00:59:46.200]   >> I love the color.
[00:59:46.200 --> 00:59:47.160]   I love the not pink.
[00:59:47.160 --> 00:59:48.040]   That's my favorite part.
[00:59:48.040 --> 00:59:48.760]   I love the name.
[00:59:48.760 --> 00:59:49.960]   I love the whole thing.
[00:59:49.960 --> 00:59:51.160]   >> So well, they have a job with their colors.
[00:59:51.160 --> 00:59:53.880]   >> Somebody said that it isn't actually that pink.
[00:59:53.880 --> 00:59:55.400]   It's more opal lesson.
[00:59:55.400 --> 01:00:00.440]   And now I'm regretting because I ordered black, but maybe I should have ordered not pink.
[01:00:00.440 --> 01:00:00.760]   >> Yes.
[01:00:00.760 --> 01:00:02.600]   You always get the color.
[01:00:02.600 --> 01:00:04.520]   Always get the variation.
[01:00:04.520 --> 01:00:07.080]   >> I'm not secure enough in my masculinity.
[01:00:07.080 --> 01:00:08.600]   >> But the pink phones are so good.
[01:00:08.600 --> 01:00:12.520]   Even it's so funny because I have the gold tennis max.
[01:00:12.520 --> 01:00:16.360]   And it's so funny because Apple, they call it their pink phone Rose gold.
[01:00:16.360 --> 01:00:18.040]   And now they have gold.
[01:00:18.040 --> 01:00:20.760]   And this is actually in my opinion what Rose gold actually looks like.
[01:00:20.760 --> 01:00:27.640]   >> I actually do have the gold iPhone, but I feel like a pink phone might be a bit far.
[01:00:27.640 --> 01:00:28.040]   >> Come on, you can pull it off.
[01:00:28.040 --> 01:00:28.920]   >> No, you can pull it off.
[01:00:28.920 --> 01:00:31.480]   I don't think so.
[01:00:31.480 --> 01:00:32.680]   >> Is this the pink?
[01:00:32.680 --> 01:00:34.600]   Is this the, Ron, you were there.
[01:00:34.600 --> 01:00:36.280]   This is a picture of that pedestal.
[01:00:36.280 --> 01:00:37.000]   Is that the pink?
[01:00:37.000 --> 01:00:37.160]   >> Yeah.
[01:00:38.040 --> 01:00:39.080]   That's pretty pink.
[01:00:39.080 --> 01:00:40.280]   That ain't no opal lesson.
[01:00:40.280 --> 01:00:42.200]   >> Well, that's got stage lighting on it too.
[01:00:42.200 --> 01:00:45.560]   So that might be also, you know, because it wasn't a theater spot, you know.
[01:00:45.560 --> 01:00:47.400]   >> So you really didn't get much of a hands-on.
[01:00:47.400 --> 01:00:48.360]   Was this a 20?
[01:00:48.360 --> 01:00:48.360]   >> I did.
[01:00:48.360 --> 01:00:48.920]   No, we did.
[01:00:48.920 --> 01:00:51.000]   So it was a great, I got to give Google credit.
[01:00:51.000 --> 01:00:51.960]   It was a great event.
[01:00:51.960 --> 01:00:53.480]   It was a very well orchestrated event.
[01:00:53.480 --> 01:00:54.360]   >> It went very well.
[01:00:54.360 --> 01:00:59.400]   And by the way, I want to say it was great that Rick ran it because he's their hard work guy.
[01:00:59.400 --> 01:01:00.920]   And everybody loves Rick Osterlo.
[01:01:00.920 --> 01:01:03.240]   He came to Google when they acquired Motorola.
[01:01:03.240 --> 01:01:05.240]   And he stayed and he's great.
[01:01:05.240 --> 01:01:06.440]   Everybody loves him.
[01:01:06.440 --> 01:01:08.600]   >> Yeah, he was a great keynote speaker.
[01:01:08.600 --> 01:01:13.400]   Just as a attendee of the event, they did at Spring Studios downtown in Manhattan,
[01:01:13.400 --> 01:01:15.080]   which Microsoft used a lot.
[01:01:15.080 --> 01:01:15.480]   Yeah.
[01:01:15.480 --> 01:01:18.200]   But they completely rebranded everything on the inside.
[01:01:18.200 --> 01:01:20.600]   From the moment you walked in, there was stuff on the wall.
[01:01:20.600 --> 01:01:23.800]   It looked as if Google had, you know, took over the building for the event.
[01:01:23.800 --> 01:01:28.520]   And the room that keynote was in was actually smaller than I thought it would be.
[01:01:28.520 --> 01:01:32.600]   It was a performance space with great windows and the skyline and everything.
[01:01:32.600 --> 01:01:33.480]   But yeah.
[01:01:33.480 --> 01:01:35.960]   >> Here's the opening of the event.
[01:01:35.960 --> 01:01:39.880]   Where the screens kind of merged together to form a pixel phone.
[01:01:39.880 --> 01:01:41.240]   I think that's pretty sweet.
[01:01:41.240 --> 01:01:42.840]   And yeah, I like all the light in there.
[01:01:42.840 --> 01:01:43.480]   >> Yeah.
[01:01:43.480 --> 01:01:45.400]   It was great event.
[01:01:45.400 --> 01:01:47.800]   And so the event itself was an hour, right?
[01:01:47.800 --> 01:01:48.200]   So like--
[01:01:48.200 --> 01:01:49.480]   >> When I thought--
[01:01:49.480 --> 01:01:52.920]   I thought this was going to go on for hours and hours with all the stuff they were going to
[01:01:52.920 --> 01:01:54.920]   announce, but they really did only announce three things.
[01:01:54.920 --> 01:01:59.800]   >> Well, it was funny because when I checked in, when I got my badge and did everything,
[01:01:59.800 --> 01:02:02.840]   they said, oh, so the event was at 11 a.m. New York time.
[01:02:02.840 --> 01:02:07.240]   And on check in, they said, come back after one o'clock to receive your gift.
[01:02:07.240 --> 01:02:07.880]   >> Oh, God.
[01:02:07.880 --> 01:02:11.000]   >> And I was like, oh, it's going to be a two hour event.
[01:02:11.000 --> 01:02:11.640]   I got a meeting.
[01:02:11.640 --> 01:02:13.000]   I can't do this whole thing.
[01:02:13.000 --> 01:02:15.720]   And we were done by noon.
[01:02:15.720 --> 01:02:16.520]   I'm pretty sure that's off.
[01:02:16.520 --> 01:02:18.920]   >> It's an hour and 15 minutes because we covered it.
[01:02:18.920 --> 01:02:19.720]   Yeah.
[01:02:19.720 --> 01:02:26.440]   >> And then once the event finished, they ushered us all into this very well-designed demo pit room
[01:02:26.440 --> 01:02:29.560]   that basically had design.
[01:02:29.560 --> 01:02:30.440]   And I took pictures of it.
[01:02:30.440 --> 01:02:31.640]   I get a card that I say in links of it.
[01:02:31.640 --> 01:02:33.640]   You can see all the pictures I got were from this demo pit.
[01:02:33.640 --> 01:02:36.760]   There was a section dedicated to the Pixel 3.
[01:02:36.760 --> 01:02:39.240]   There was a section dedicated to the slate section.
[01:02:39.240 --> 01:02:43.880]   They had three or four mock-ups of the home where you can see the home hub working in a
[01:02:43.880 --> 01:02:46.040]   bedroom or a living room or a kitchen.
[01:02:46.040 --> 01:02:47.640]   And they were running demos of those.
[01:02:47.640 --> 01:02:50.280]   And so it was just like this playground.
[01:02:50.280 --> 01:02:56.440]   I knew after seeing it, after seeing once I walked into the event space,
[01:02:56.440 --> 01:02:59.240]   I ran to that demo area because I knew that within 15 minutes,
[01:02:59.240 --> 01:03:02.680]   it was going to be overrun by YouTubers and content creators.
[01:03:02.680 --> 01:03:06.040]   And sure enough, it was like dodging cameras left and right because everyone was filming
[01:03:06.040 --> 01:03:06.920]   in front of stuff.
[01:03:06.920 --> 01:03:09.960]   But they had all the products there for us to play with and touch.
[01:03:09.960 --> 01:03:11.320]   >> What was the gift?
[01:03:11.320 --> 01:03:14.200]   >> So the gift, I saw, I'm like, "Ooh, a gift.
[01:03:14.200 --> 01:03:15.160]   Gonna get a phone.
[01:03:15.160 --> 01:03:15.480]   Nice."
[01:03:15.480 --> 01:03:22.280]   The gift was actually the latest Chromecast, which they didn't include in the keynote
[01:03:22.280 --> 01:03:23.400]   that they didn't even talk about.
[01:03:23.400 --> 01:03:23.880]   >> Right.
[01:03:23.880 --> 01:03:31.480]   >> And the Google Home Mini Mickey Mouse stand that Otter put out.
[01:03:31.480 --> 01:03:34.760]   >> I'm glad you have that.
[01:03:34.760 --> 01:03:35.560]   >> Yeah.
[01:03:35.560 --> 01:03:38.920]   >> It was the new Chromecast in any way different except it's in white.
[01:03:38.920 --> 01:03:40.440]   >> It's 1080p now.
[01:03:40.440 --> 01:03:41.800]   >> And multi-room support.
[01:03:41.800 --> 01:03:43.240]   >> It wasn't 1080p before?
[01:03:43.240 --> 01:03:43.640]   >> Nope.
[01:03:43.640 --> 01:03:44.760]   >> That wasn't there.
[01:03:44.760 --> 01:03:45.080]   >> What?
[01:03:45.080 --> 01:03:48.440]   >> Yeah, the Chromecast Ultra was not, no, no.
[01:03:48.440 --> 01:03:49.400]   >> Yes, that was 4K.
[01:03:49.400 --> 01:03:53.240]   But if you wanted to get the actual dongle, that has been 720p since,
[01:03:53.240 --> 01:03:55.000]   I guess, 2013 when it came out.
[01:03:55.000 --> 01:03:55.640]   >> Okay.
[01:03:55.640 --> 01:04:00.920]   >> So, which honestly, at this point, it was pretty pretty pretty with that.
[01:04:00.920 --> 01:04:03.080]   >> Ron hasn't even taken the cellophane off the box.
[01:04:03.080 --> 01:04:03.960]   >> Well, okay.
[01:04:03.960 --> 01:04:07.720]   >> I have a TV with Chromecast in the old like, already in.
[01:04:07.720 --> 01:04:09.480]   >> I have Chromecast in my drawer.
[01:04:09.480 --> 01:04:10.120]   I can't figure it.
[01:04:10.120 --> 01:04:10.760]   I have so many--
[01:04:10.760 --> 01:04:12.120]   >> Oh, thanks.
[01:04:12.120 --> 01:04:13.560]   >> About so many Chromecasts.
[01:04:13.560 --> 01:04:18.840]   >> You say that, but I got this Vizio TV because it had the casting technology in it.
[01:04:18.840 --> 01:04:19.240]   >> Oh, he didn't.
[01:04:19.240 --> 01:04:19.240]   >> Yeah.
[01:04:19.240 --> 01:04:21.880]   >> Except that they haven't updated it for two years.
[01:04:21.880 --> 01:04:22.680]   >> Yeah.
[01:04:22.680 --> 01:04:26.520]   >> And so now I'm actually thinking about using an HDMI port with this sucker just to get the
[01:04:26.520 --> 01:04:27.800]   latest version of Chromecast.
[01:04:27.800 --> 01:04:27.800]   >> Yeah.
[01:04:27.800 --> 01:04:33.080]   >> I have every set top box dongle, whatever known to man.
[01:04:33.080 --> 01:04:37.000]   And yeah, I mean, I'm not going to get the newest Chromecast.
[01:04:37.000 --> 01:04:39.160]   I think I have an Ultra someplace, so that's fine.
[01:04:39.160 --> 01:04:40.760]   >> Yeah, I use an Ultra on my 4K TV.
[01:04:40.760 --> 01:04:46.120]   >> Yeah, no, but I do like to travel with them, especially if I'm going to--
[01:04:46.120 --> 01:04:49.800]   whether the hotel allows me access or not, I usually find a way to do it.
[01:04:49.800 --> 01:04:52.200]   Because some of the TVs, some of the hotels are getting better
[01:04:52.200 --> 01:04:53.240]   about letting you have Netflix.
[01:04:53.240 --> 01:04:53.800]   >> Oh.
[01:04:53.800 --> 01:04:56.920]   >> So you can use a Chromecast in a hotel?
[01:04:56.920 --> 01:04:57.800]   >> Oh, yeah.
[01:04:57.800 --> 01:05:00.600]   >> How do you do-- you have to put it on-- how do you do that?
[01:05:00.600 --> 01:05:03.720]   >> Sometimes you have to play around a little bit to get it on the network,
[01:05:03.720 --> 01:05:05.960]   but it's almost always doable.
[01:05:05.960 --> 01:05:08.280]   And if I don't use a Chromecast, I use like a Fire TV stick.
[01:05:08.280 --> 01:05:11.400]   >> So you have to put it on the hotel Wi-Fi, the work, though.
[01:05:11.400 --> 01:05:16.680]   >> Yeah, or I mean, depending on your stream plan, I mean, you could always just tether it
[01:05:16.680 --> 01:05:18.680]   to depending on your-- got a plan.
[01:05:18.680 --> 01:05:18.680]   >> Oh, yeah.
[01:05:18.680 --> 01:05:19.240]   >> Okay.
[01:05:19.240 --> 01:05:25.400]   >> But a lot of times you can get-- there are ways that you can get it on those networks.
[01:05:25.400 --> 01:05:25.880]   >> So--
[01:05:25.880 --> 01:05:28.440]   >> It's really changed air travel that people have iPads.
[01:05:28.440 --> 01:05:31.640]   In fact, some airlines are actually taking the screens out of the seats now.
[01:05:31.640 --> 01:05:32.200]   >> They sure are.
[01:05:32.200 --> 01:05:33.800]   >> Yeah, everybody's got an iPad.
[01:05:33.800 --> 01:05:34.920]   Nobody's watching the videos.
[01:05:34.920 --> 01:05:37.160]   I imagine the same thing will happen in hotels, right?
[01:05:37.160 --> 01:05:37.960]   >> Why?
[01:05:37.960 --> 01:05:41.640]   >> That or they'll start just putting those things there, because that's what I've noticed.
[01:05:41.640 --> 01:05:46.440]   I've been traveling a lot this year, and I've seen a huge uptick in the number of hotels
[01:05:46.440 --> 01:05:51.400]   that either are putting smart TVs in the rooms and actually enabling the smart TV features,
[01:05:51.400 --> 01:05:55.480]   or that are kind of encouraging the use of Netflix or other sorts of things.
[01:05:55.480 --> 01:05:57.560]   Because it's good for them and so far.
[01:05:57.560 --> 01:06:02.040]   They can still obviously sell their-- they want to upsell the in-room options.
[01:06:02.040 --> 01:06:05.720]   But a lot of times it means that they might not have to care as much about the full cable package
[01:06:05.720 --> 01:06:10.120]   if they know that people are just going to be watching Netflix or Hulu or whatever.
[01:06:10.120 --> 01:06:10.440]   >> Right.
[01:06:10.440 --> 01:06:15.560]   >> All right. So they announced Pixel 3 and Pixel 3 XL.
[01:06:15.560 --> 01:06:20.760]   It looks a lot like the Pixel 2, but the camera is much improved.
[01:06:20.760 --> 01:06:27.400]   In fact, we had Rishi Sanyal on the science editor for DP Review on Saturday yesterday
[01:06:27.400 --> 01:06:28.440]   on the new screensavers.
[01:06:28.440 --> 01:06:31.400]   Not ours.
[01:06:31.400 --> 01:06:37.880]   DP Review had a great couple of interviews with the product lead and the imaging guys from
[01:06:37.880 --> 01:06:42.280]   Google that were really fascinating. It sounds like what they're doing with computational photography
[01:06:42.280 --> 01:06:47.720]   is really leading edge. And ahead of Apple, we thought it would be because Apple's playing
[01:06:47.720 --> 01:06:52.760]   catch-up with the iPhone XS Max and XS with Google's last year phone.
[01:06:52.760 --> 01:06:59.400]   Now they're doing nine images in some cases. In some cases, as slow as a quarter second in low
[01:06:59.400 --> 01:07:03.880]   light, but because they're taking multiple images, they're able to eliminate handshake
[01:07:04.840 --> 01:07:10.600]   and motion blur. They're doing things like film lighting computationally. It looks great.
[01:07:10.600 --> 01:07:17.320]   >> Yeah. A lot of the gray area, not the gray area, but the crossover between hardware and
[01:07:17.320 --> 01:07:22.680]   software really came out in this keynote that, yes, here's the phone and it's got these sensors,
[01:07:22.680 --> 01:07:26.840]   it's got these lenses. But we took the computational power that we've learned from Google, put it on
[01:07:26.840 --> 01:07:33.320]   the chip in the phone to allow you to do these things to apply computational approach to it.
[01:07:33.320 --> 01:07:36.920]   And for me coming out of it, it was like, yeah, the phone's really cool. But wow,
[01:07:36.920 --> 01:07:42.440]   that mode, I forgot the name of the mode, but it takes a burst of pictures and recommends the
[01:07:42.440 --> 01:07:44.840]   best shot. >> The top shot feature, yeah.
[01:07:44.840 --> 01:07:50.680]   >> Yeah, top shot. That actually, I said, I am prone to say, holy cow, but I said, wow,
[01:07:50.680 --> 01:07:54.680]   out loud in the audience because it was just like seeing it in the demo and the funny video they
[01:07:54.680 --> 01:07:58.520]   had to go with it of all the missed shots. But that's something I would actually use.
[01:07:58.520 --> 01:08:02.520]   That's actually something that could improve my photo taking skills.
[01:08:02.520 --> 01:08:06.600]   >> Another thing camera phones are terrible at a zoom because they usually are optical zoom.
[01:08:06.600 --> 01:08:10.200]   Apple solved that a little bit with dual lenses, a number of companies have done that.
[01:08:10.200 --> 01:08:14.760]   Google still only has one lens, but they're using this multiple image technology to do a zoom.
[01:08:14.760 --> 01:08:22.920]   Here's an unzoomed picture, CNET took these with a Pixel 3 in New York, and watch this building here
[01:08:22.920 --> 01:08:29.640]   because this is a significant amount of zoom on that building. And that's from the same,
[01:08:29.640 --> 01:08:35.000]   that's the same kind of image, but zoomed in. >> They're just doing the cropping.
[01:08:35.000 --> 01:08:38.680]   >> Yeah, that's pretty good. >> You know what's really good?
[01:08:38.680 --> 01:08:46.360]   So I just got a, finally, so I had a, this is the Sony Mark- >> RX.
[01:08:46.360 --> 01:08:53.320]   >> And I had the Mark 4, and I left it during build, even though it was literally down the
[01:08:53.320 --> 01:08:58.040]   street from where I live, I was staying in a hotel because I was doing so much stuff.
[01:08:58.040 --> 01:09:03.560]   And I left my camera in the hotel. And I wasn't able to get it back in, and I was very mad at
[01:09:03.560 --> 01:09:09.720]   myself for leaving my expensive $900 camera. And so I was going to get the Mark 6, which has the
[01:09:09.720 --> 01:09:15.640]   giant zoom, but I actually wound up getting the 5A, which had the faster processor, but still has
[01:09:15.640 --> 01:09:21.000]   the lower, the better aperture and the better low light stuff. And this is, you know, I spent
[01:09:21.000 --> 01:09:25.240]   all told, you know, about $1,000 to get the camera and the kit. And it's a great camera,
[01:09:25.240 --> 01:09:27.720]   and it has, you know, zoom in, it's pocketable. It's really good.
[01:09:27.720 --> 01:09:30.520]   >> That's a one-inch sensor, so it's a really big sensor.
[01:09:30.520 --> 01:09:34.680]   >> So it's a really big sensor for a camera the small, and I love it. But I'm going to be honest,
[01:09:34.680 --> 01:09:40.760]   the way that, you know, sensor size notwithstanding that Google and Apple and others, but especially
[01:09:40.760 --> 01:09:47.720]   Google are changing their software to really get the most out of photos. I mean, even something
[01:09:47.720 --> 01:09:52.200]   like this, which is granted will do great 4K video and is a fantastic camera and is better in a lot
[01:09:52.200 --> 01:09:56.840]   of ways than what you would get with your phone. What we're seeing now with phones,
[01:09:56.840 --> 01:10:01.960]   I mean, I think unless you are an enthusiast or you're somebody like me who has to buy things,
[01:10:01.960 --> 01:10:09.560]   there's not so much reason to, you know, already the pocket camera market or whatever, you know,
[01:10:09.560 --> 01:10:12.200]   the people who weren't, we're not talking about the expensive one.
[01:10:12.200 --> 01:10:12.920]   >> It's dead.
[01:10:12.920 --> 01:10:16.600]   >> It was dead. It was already dead. But now, really, but now even the higher end,
[01:10:16.600 --> 01:10:20.680]   I mean, if I'm Sony and Sony does, I think the best job with a lot of the stuff, I would be
[01:10:21.320 --> 01:10:26.680]   really worried because you're starting to say, it's the AI tools and these autocropanes and it's
[01:10:26.680 --> 01:10:32.040]   making these decisions and it's doing all the stuff on the software side that I think you can
[01:10:32.040 --> 01:10:36.200]   make the argument that a tech company can do better than a camera company, especially a company
[01:10:36.200 --> 01:10:40.520]   like Google who has so much data and gets so much information and is so good at AI,
[01:10:40.520 --> 01:10:46.360]   it's really phenomenal. And so looking at the, I'm not an Android person, but looking at the camera
[01:10:46.360 --> 01:10:50.440]   on the Pixel XL3 and also seeing what they're saying about, you know, bringing some of those
[01:10:50.440 --> 01:10:54.600]   features to other models because of this software is so impressive.
[01:10:54.600 --> 01:11:00.440]   >> Look at this night shot from Manhattan at night. I mean, very low light. It's really
[01:11:00.440 --> 01:11:05.320]   remarkable. It's not going to be perfect. If you pixel peep and really zoom in, you're going to
[01:11:05.320 --> 01:11:11.720]   see some, you know, artifacts and softness. So it's not going to replace high-end professional
[01:11:11.720 --> 01:11:16.200]   cameras, but for Instagram and Facebook, which is all you need.
[01:11:16.200 --> 01:11:19.000]   And I'm starting to think I might not have to bring a camera.
[01:11:19.000 --> 01:11:24.520]   >> Well, as they showed, as they showed in the keynote, Conda asked shot like 12 magazine covers
[01:11:24.520 --> 01:11:30.520]   with the phone, which then, which then I said, oh, great. So old media is officially dead.
[01:11:30.520 --> 01:11:34.680]   >> That's not a great test because magazine covers, let's be honest, are not high quality
[01:11:34.680 --> 01:11:37.960]   images. I mean, first of all, it's in a studio. So it's well lit.
[01:11:37.960 --> 01:11:42.680]   Second, they screen the hell out of those things. And so I don't know if that's really a good test.
[01:11:42.680 --> 01:11:47.160]   >> Conda, Conda did that with the iPhone too. I remember having to, I remember getting a call
[01:11:47.160 --> 01:11:52.520]   for one of their food magazines that they shot the whole issue with an iPhone. And this was,
[01:11:52.520 --> 01:11:56.360]   I think, the success. So that's not even, you know, we can think that picture.
[01:11:56.360 --> 01:11:58.120]   >> Tell me that this picture does not make you hungry.
[01:11:58.120 --> 01:11:59.080]   >> Totally.
[01:11:59.080 --> 01:12:01.000]   >> That this picture makes--
[01:12:01.000 --> 01:12:01.560]   >> Oh, good.
[01:12:01.560 --> 01:12:02.840]   >> I want this pizza.
[01:12:02.840 --> 01:12:03.560]   >> I want it all.
[01:12:03.560 --> 01:12:09.720]   >> That's clearly a New York pizza. Very small, but good. So I'm excited. I'm going to take,
[01:12:09.720 --> 01:12:13.640]   I get the phone on Thursday. I'm going to take it to Chinatown. I want some challenging environments
[01:12:13.640 --> 01:12:19.400]   of lighting and so forth. But I'm pretty impressed. The top shot, to me, the best part of the keynote
[01:12:19.400 --> 01:12:22.120]   was the top shot video. >> That was great.
[01:12:22.120 --> 01:12:22.920]   >> Wasn't that fun?
[01:12:22.920 --> 01:12:28.600]   >> That was very good. It's a great feature. I mean, I was really impressed by a lot of the
[01:12:28.600 --> 01:12:33.240]   software stuff that they're doing. And I don't love the stuff that's only available on Pixel,
[01:12:33.240 --> 01:12:39.000]   like so Smart Compose, which we've seen in Gmail on the web is coming to the Gmail mail app on the
[01:12:39.000 --> 01:12:43.080]   Pixel, which is like, I don't like when they do that, but I understand they need something to
[01:12:43.080 --> 01:12:49.880]   something to motivate people to go buy the phones. But all the application features software stuff
[01:12:49.880 --> 01:12:52.840]   really impressed me this kind of thing. >> What's interesting, actually, though,
[01:12:52.840 --> 01:12:56.760]   is that Google's going to push a lot of this down to the other phones.
[01:12:56.760 --> 01:12:59.960]   >> Sure. >> That not all of it requires the Pixel 3 hardware.
[01:12:59.960 --> 01:13:00.120]   >> Yeah.
[01:13:00.120 --> 01:13:00.760]   >> Right.
[01:13:00.760 --> 01:13:04.840]   >> That is on her Apple would never do that. That's unheard of.
[01:13:04.840 --> 01:13:05.160]   >> Yeah.
[01:13:05.160 --> 01:13:10.600]   >> And I think that's, here's the top shot video. So this is something we're all used to,
[01:13:10.600 --> 01:13:14.680]   which is you get everybody ready to take a picture. And then because a camera light,
[01:13:14.680 --> 01:13:20.280]   you get people's eyes closed, their faces blurred, scarf in the woman's face.
[01:13:20.280 --> 01:13:25.160]   We all get these pictures of finger over the lens. Guys jumping off the cliff, and you get the shot
[01:13:25.160 --> 01:13:30.440]   after they've already gone off the cliff. Who doesn't have these pictures all over the place in
[01:13:30.440 --> 01:13:35.960]   their camera? And so I really thought this was a very identifiable video. What top shot does,
[01:13:35.960 --> 01:13:42.760]   of course, is it starts recording before you snap the shutter. So as a result, if you do get a
[01:13:42.760 --> 01:13:49.480]   shot and everybody's got a goofy look, you can actually scroll back and I guess the AI will attempt
[01:13:49.480 --> 01:13:56.440]   to offer the best shot, which my experiences may not be the best. But the fact that you can
[01:13:56.440 --> 01:14:01.400]   still go back, are these lower resolution though than the one you shot?
[01:14:01.400 --> 01:14:07.560]   >> No, so the ones in the carousel that you see there are lower resolution, but once you say,
[01:14:07.560 --> 01:14:12.440]   I want to choose that one, it bumps the resolution up on it. And I believe it's doing that
[01:14:12.440 --> 01:14:17.560]   computationally, not actually like it didn't capture everyone, but it is bridging the gap.
[01:14:17.560 --> 01:14:21.560]   >> Apple does this with live photos, I'm pretty sure. Although I had one of the same thing about
[01:14:21.560 --> 01:14:25.080]   whether the resolution changes. >> What Apple really taking so many.
[01:14:25.080 --> 01:14:29.640]   >> Apple does is they have one good quality still and then it's actually a video, the rest of it
[01:14:29.640 --> 01:14:31.800]   in effect. >> But you can go ahead and edit and change.
[01:14:31.800 --> 01:14:35.160]   >> Can you make it better? >> Yeah, you can pick different skills.
[01:14:35.160 --> 01:14:37.880]   >> They don't do it automatically, but you can't do it on live.
[01:14:37.880 --> 01:14:39.480]   >> They don't do it on live. >> Okay, well, let's get this.
[01:14:39.480 --> 01:14:43.640]   >> The next step is going to be they will, and this is probably a couple of years down the line,
[01:14:43.640 --> 01:14:48.600]   but probably not that far, where they will stitch parts of the photo together. So you'll have like,
[01:14:48.600 --> 01:14:50.600]   you know, okay. >> They're already doing that.
[01:14:50.600 --> 01:14:54.840]   >> Yeah, yeah, yeah. But you know, they'll make it automatically so that if my eyes are close,
[01:14:54.840 --> 01:15:00.040]   but yours are open, then they'll create a composite of the two together.
[01:15:00.040 --> 01:15:04.520]   >> So what Rishi showed us is an image that Google had taken of a guy sitting in a basketball,
[01:15:04.520 --> 01:15:10.680]   who tossing the basketball. And what the phone did was, for stuff that didn't move,
[01:15:10.680 --> 01:15:15.320]   it got very nice crisp stuff, because it's taking nine shots.
[01:15:15.320 --> 01:15:18.120]   >> Right. >> But the stuff that's moving, it picked one of the nine.
[01:15:18.120 --> 01:15:22.520]   So the basketball, as you can see, it's a little lower res, if you really look down at it.
[01:15:22.520 --> 01:15:27.720]   His hands are a little lower res. So the stuff that's static is perfect, and the stuff that's not,
[01:15:27.720 --> 01:15:31.320]   it's as good as it could get, given to it was taking one out of those nine shots.
[01:15:31.320 --> 01:15:34.200]   So it's already doing that. It's really interesting.
[01:15:34.200 --> 01:15:37.560]   >> It doesn't HDR mode do something like it. It's like a longer exposure.
[01:15:37.560 --> 01:15:38.600]   >> Yes. >> You can.
[01:15:38.600 --> 01:15:43.480]   >> Yes, precisely. There's basically stacking. So I think in some cases, it stacks five,
[01:15:43.480 --> 01:15:47.080]   in some cases it stacks nine. And basically,
[01:15:47.080 --> 01:15:52.520]   the other thing that Rishi talked about is, there's normally micro movements in the camera
[01:15:52.520 --> 01:15:57.320]   between the shots. They're very quick. They're 60 frames a second. But even so, your hand is
[01:15:57.320 --> 01:16:02.520]   moving in tiny little ways. And it's using that to get parallax and get different
[01:16:02.520 --> 01:16:07.400]   views of the same image from different locations. And then picking the, so you get,
[01:16:07.400 --> 01:16:12.680]   in effect, more pixel depth. It's just, it's pixel shifting, but because your hands moving,
[01:16:12.680 --> 01:16:17.880]   in supposed to having the actual pixel shifting on the hardware, it is mind blowing.
[01:16:17.880 --> 01:16:22.520]   Now I will have to all try it and see if it's going to be able to replace your RX100.
[01:16:22.520 --> 01:16:28.200]   >> Yeah. I mean, and look, I think that there are probably, for people who actually go out and buy
[01:16:28.200 --> 01:16:33.000]   that camera, no, but I think that there are times where if you don't have it with you.
[01:16:33.000 --> 01:16:38.760]   I mean, so I originally bought my first RX100 to see Beyonce in New York. And then I didn't get
[01:16:38.760 --> 01:16:44.680]   the second one in time to see her and Jay Z in Seattle. And so I had to rely on my iPhone 10S Max,
[01:16:44.680 --> 01:16:50.760]   and I still had really great photos. So it was kind of part of me was like, well,
[01:16:50.760 --> 01:16:55.800]   it's getting better in the Google software. I mean, I think this is Apple's doing really good
[01:16:55.800 --> 01:17:01.080]   stuff. And I think that historically they've had the best camera stack in the phone industry.
[01:17:01.080 --> 01:17:05.640]   But at this point, I do feel like Google software is better, which is something I didn't think I
[01:17:05.640 --> 01:17:09.800]   ever say, but it's really, really good. >> I think it makes the case it's been better
[01:17:09.800 --> 01:17:12.840]   for a couple of generations that Apple's cat playing catch up. This is-
[01:17:12.840 --> 01:17:17.080]   >> I wouldn't disagree it, but I think now is where you really see the difference.
[01:17:17.080 --> 01:17:21.560]   >> One thing Apple's getting some heat from is people is over sharpening, particularly in the
[01:17:21.560 --> 01:17:26.680]   selfie cam that there's a lot of beauty filtering going on. And you can't really turn it off.
[01:17:26.680 --> 01:17:32.280]   Marques Brownlee tweeted this picture with a Pixel 3. He says, "There's for sure no beauty
[01:17:32.280 --> 01:17:37.960]   smoothing happening with this Pixel 3 camera." And I agree this sharpness is pretty darn good in
[01:17:37.960 --> 01:17:48.120]   that selfie shot. All right, so here's the question. Is Google even close to competitive with Apple
[01:17:48.120 --> 01:17:52.920]   and Samsung in the smartphone market? I mean, they sold 10 million total last year.
[01:17:52.920 --> 01:17:59.240]   >> Right. This is the problem, right? It's almost like you wonder, okay, would they not be better off
[01:17:59.240 --> 01:18:04.760]   trying to charge and license this to people who are actually selling phones or cameras?
[01:18:04.760 --> 01:18:07.560]   >> It's a shame. >> It really is.
[01:18:07.560 --> 01:18:13.000]   >> I like- I mean, I also feel like I'm safer using a Google Android device than other Android
[01:18:13.000 --> 01:18:19.000]   devices. Is that crazy, talk Ron? >> No, it's not crazy, but I do think slightly tied to this,
[01:18:19.000 --> 01:18:24.120]   they didn't say the word Android in the keynote. >> That was wild. They did not say it once.
[01:18:24.120 --> 01:18:27.720]   >> At all. >> In an hour and seven minutes, they did not say it once.
[01:18:27.720 --> 01:18:35.480]   >> So I wonder if they're trying to combat whatever kind of in the ether negative opinion about
[01:18:35.480 --> 01:18:39.800]   Android that's out there to bridge the gap to compete with Apple and Samsung. Because Samsung
[01:18:39.800 --> 01:18:44.040]   users, the millions and millions of Samsung users do not say I have an Android phone, they say I have
[01:18:44.040 --> 01:18:46.840]   a Galaxy. >> No, they have a Samsung, they have a Galaxy. Well, and in fact, it's the point where
[01:18:46.840 --> 01:18:53.720]   people, I mean, look, granted, the die hearts get upset that the updates are delayed. And people,
[01:18:53.720 --> 01:18:58.840]   like us, Ron probably preferred that the stock Android experience. But most people who are buying
[01:18:58.840 --> 01:19:03.160]   Samsung devices, no matter what your opinion touch was, is they're buying it because that's
[01:19:03.160 --> 01:19:05.960]   how they use their phone. So if they were to go to the stock experience, they'd be like,
[01:19:05.960 --> 01:19:09.560]   what is this? >> Right. >> Yeah, because they're nine generations in now. They bought a Samsung
[01:19:09.560 --> 01:19:14.120]   10 years, six years ago, and they've just gotten every phone every time they upgrade. So it makes
[01:19:14.120 --> 01:19:17.480]   sense. >> Exactly. >> But I thought it was really enlightening that Google didn't acknowledge
[01:19:17.480 --> 01:19:24.680]   Android in the entire presentation. >> I was, chat rooms corrected me. Google shipped 3.9 million
[01:19:24.680 --> 01:19:31.720]   pixel devices in 2017. >> Oh, that's awesome. >> Compare that to 1.5 billion units sold worldwide
[01:19:31.720 --> 01:19:36.920]   of all other phones. >> We use a library number. >> It's less than one week's worth of iPhone sales,
[01:19:36.920 --> 01:19:40.840]   3. >> Genuinely, guys, those are like blackberry numbers. Those are like palm numbers back when
[01:19:40.840 --> 01:19:45.640]   like palm was selling the centro and stuff like that. I'm not even joking. >> So why is Google in
[01:19:45.640 --> 01:19:49.080]   this business? >> Because they cannot be. >> They have to be.
[01:19:49.080 --> 01:19:55.320]   >> Yeah. I mean, they get to show off some of their top tier things. I mean, I think it's the same way,
[01:19:55.320 --> 01:19:58.600]   you know, the surface is now selling really well, but it's the same way why it makes sense for
[01:19:58.600 --> 01:20:03.720]   Microsoft to be in the surface business. It's a good way, when so many people use your operating
[01:20:03.720 --> 01:20:07.880]   system, and that's both the plus and the minus of Android is that anybody can use it, as we just
[01:20:07.880 --> 01:20:12.680]   said. Samsung makes their own other companies do their own things with it. Google has this
[01:20:12.680 --> 01:20:17.880]   opportunity, regardless of how many units they sell, to show off the best aspects of
[01:20:17.880 --> 01:20:23.400]   Android. And they're the ones who can control the entire end-to-end experience. And, you know,
[01:20:23.400 --> 01:20:29.320]   they make enough money that it's worth it, I have to assume. >> And as you point out,
[01:20:29.320 --> 01:20:33.480]   Microsoft did the same thing with Surface, and it has turned the corner. It's now the top
[01:20:33.480 --> 01:20:41.560]   number five PC maker in the US, which Microsoft had really just started making PC hardware in the
[01:20:41.560 --> 01:20:43.400]   last few years. So that's- >> Yeah.
[01:20:43.400 --> 01:20:45.880]   >> So we did a quick catch up, maybe. I don't know.
[01:20:45.880 --> 01:20:51.160]   >> Yeah. And Christine, I agree with you there, but I also think that the story about Android
[01:20:51.160 --> 01:20:57.480]   and hardware and specifically with phones is all about accessibility. And the fact that these are
[01:20:57.480 --> 01:21:05.400]   799 and up phones, $799 and higher phones, so their expensive price point, they're available on
[01:21:05.400 --> 01:21:13.000]   the Google Store, which nobody knows exists unless you're a fan, and Verizon. So then it becomes
[01:21:13.000 --> 01:21:16.760]   an accessibility issue. I'm on T-Mobile, I got to wait until T-Mobile gets it and all that sort of
[01:21:16.760 --> 01:21:22.040]   stuff. They do these exclusivities. I mean, this is why OnePlus isn't known, I mean, by and large,
[01:21:22.040 --> 01:21:26.120]   I'm using the OnePlus 6 now as my daily driver, and people at work are like, what is that? Because
[01:21:26.120 --> 01:21:28.120]   they never hurt me. >> They've never hurt me because-
[01:21:28.120 --> 01:21:30.920]   >> Because you have to be a certain type of- No, I agree with you. I was actually going to say
[01:21:30.920 --> 01:21:34.840]   that I think that if they were really serious about taking this on, they would have to be
[01:21:34.840 --> 01:21:38.040]   serious about getting agreements with the other carriers. But the problem with that,
[01:21:38.040 --> 01:21:42.200]   right, is that the carriers want to make concessions and want to make changes and
[01:21:42.200 --> 01:21:45.000]   want to do things. And that goes against kind of the core model.
[01:21:45.000 --> 01:21:47.960]   >> I think a lot of- >> Go ahead, Mark.
[01:21:47.960 --> 01:21:52.200]   >> I was just going to say, I think a lot of what Google does is the illusion of competition.
[01:21:52.200 --> 01:21:57.480]   Like, we're going to make a phone. We know it's never going to be the best-selling phone, but we
[01:21:57.480 --> 01:22:04.600]   also don't want to be overly reliant on Samsung or overly- It's the same reason that
[01:22:04.600 --> 01:22:09.240]   they do Project 5. They're never going to be the biggest sell carrier in the US.
[01:22:09.240 --> 01:22:13.560]   That's why they do fiber. They're never going to be Comcast, but they're showing Comcast.
[01:22:13.560 --> 01:22:16.680]   We can do really fast internet at low prices, so why can't you?
[01:22:16.680 --> 01:22:20.920]   >> That sounds like they're being altruistic. Is that what you're saying?
[01:22:20.920 --> 01:22:23.400]   >> No, I think they're- >> [LAUGH]
[01:22:23.400 --> 01:22:28.040]   >> I think it's for competitive business reasons. They know their main business is not going to
[01:22:28.040 --> 01:22:34.200]   be cell phones or cell carriers, but they want to show we're not going to be overly
[01:22:34.200 --> 01:22:37.160]   reliant on anyone. >> And of course, we've always said that
[01:22:37.160 --> 01:22:40.680]   Google makes money when you use the internet, just in general.
[01:22:40.680 --> 01:22:46.040]   >> But it's also the implication of a suggestion of competing.
[01:22:46.040 --> 01:22:51.000]   You got to know that once they open fiber, Comcast was like, crap, Google's getting into this.
[01:22:51.000 --> 01:22:53.720]   >> Right. >> So it's the suggestion of-
[01:22:53.720 --> 01:22:58.440]   >> It's the illusion of- >> At any moment, we can shift this large
[01:22:58.440 --> 01:23:01.320]   cruise ship that we're steering in your direction and wipe you out.
[01:23:01.320 --> 01:23:05.800]   >> Odd though, is it a marketing failure? I mean, if the Pixel 3 XL is a great phone,
[01:23:05.800 --> 01:23:09.160]   it sounds like it is. Why isn't it selling better?
[01:23:09.160 --> 01:23:11.880]   >> Well, they- >> Because it's only
[01:23:11.880 --> 01:23:13.720]   available on Verizon and if you get it-
[01:23:13.720 --> 01:23:15.480]   >> No, I could get a Pixel 3 on every phone.
[01:23:15.480 --> 01:23:17.480]   Oh, you mean it's only in the Verizon stores?
[01:23:17.480 --> 01:23:18.520]   >> Yeah, yeah.
[01:23:18.520 --> 01:23:19.000]   >> Yeah.
[01:23:19.000 --> 01:23:22.920]   >> So getting in every- But remember, the iPhone was only in AT&T for years.
[01:23:22.920 --> 01:23:29.880]   >> Yeah, but it's limited in- >> And it did, and I remember I was at CES the year that they
[01:23:29.880 --> 01:23:34.120]   announced that it was going to be on Verizon and I was actually in a Motorola briefing.
[01:23:34.120 --> 01:23:41.000]   And we got the invite for a meeting with Verizon and it was clear that it was going to be the
[01:23:41.000 --> 01:23:43.480]   iPhone launch event they were going to do something in New York.
[01:23:43.480 --> 01:23:48.040]   And I remember, because it was Campfire at the time, kind of getting with my editor and
[01:23:48.040 --> 01:23:53.320]   being like, do I need to leave this CES, like Motorola keynote to write the iPhone news?
[01:23:53.320 --> 01:23:54.760]   And they're like, yeah. >> You do?
[01:23:54.760 --> 01:23:58.120]   >> Yeah. >> Because that was such a big deal.
[01:23:58.120 --> 01:24:02.520]   I mean, that held the iPhone back, but that was also the very beginning of all of this.
[01:24:02.520 --> 01:24:07.160]   And the iPhone was enough of a Halo product that many people did switch, but as soon as they
[01:24:07.160 --> 01:24:12.200]   came to Verizon, I mean, that was massive. The number of people who became iPhone users was-
[01:24:12.200 --> 01:24:13.480]   >> It did. It jumped, yeah.
[01:24:13.480 --> 01:24:17.000]   >> Is Google charging too much? That's what the chatroom says.
[01:24:17.000 --> 01:24:21.000]   >> Yes. Yeah, that's my other point is that going back to the price point,
[01:24:21.000 --> 01:24:25.160]   I've been saying since they came out with the Pixel, hell, I've been saying it since the Nexus.
[01:24:25.160 --> 01:24:29.160]   Give me a mid-range to low end phone from Google.
[01:24:29.160 --> 01:24:33.080]   Because the thing is, is that not everyone can afford an iPhone, not everyone can afford a Pixel,
[01:24:33.080 --> 01:24:37.000]   and you know what they're getting? They're getting those mid-range phones by ZTE and by
[01:24:37.000 --> 01:24:38.200]   other companies- >> Which is dangerous.
[01:24:38.200 --> 01:24:40.040]   >> Which is actually dangerous. >> Yep.
[01:24:40.040 --> 01:24:43.560]   >> Right. >> They're getting the cheapo phone from their carrier, basically.
[01:24:43.560 --> 01:24:44.600]   >> Yeah. >> That runs out.
[01:24:44.600 --> 01:24:48.520]   >> And what's interesting is that Google really could have an opportunity to really
[01:24:48.520 --> 01:24:54.200]   outsell those. If they used older SOCs, if they used maybe less memory, but they could still
[01:24:54.200 --> 01:24:58.760]   bring some of those, maybe you don't have a 12-megapixel camera, maybe not as good of a sensor,
[01:24:58.760 --> 01:25:03.000]   but you still bring some of those AI features, you could still get a really tremendous experience.
[01:25:03.000 --> 01:25:05.560]   >> That's the point of getting lock in. >> You could get lock in.
[01:25:05.560 --> 01:25:08.520]   >> Yeah. >> Android One, though, is to make a secure
[01:25:08.520 --> 01:25:12.600]   Google experience on other people's hardware. Is that a low cost? That's kind of what it is.
[01:25:12.600 --> 01:25:14.280]   >> Yeah, that hasn't worked, though. >> Oh.
[01:25:14.280 --> 01:25:19.800]   >> We'll see the latest flavor of Android One works.
[01:25:21.160 --> 01:25:23.720]   >> It's worth a 10, and I don't think it's going to. >> Yeah.
[01:25:23.720 --> 01:25:28.440]   >> Right. But, but, Kristian, to your point, you know, why not make an affordable phone that
[01:25:28.440 --> 01:25:32.600]   college kids can buy this, right? That's the, you know, the pixel, I don't say pixel light,
[01:25:32.600 --> 01:25:39.320]   I'm not a marketing guy. But to make it a flagship, to make it so expensive, immediately limits your
[01:25:39.320 --> 01:25:43.320]   audience. You make it accessible, you lock them into the brand, and then you get a generation like
[01:25:43.320 --> 01:25:49.240]   Samsung has done that only you think that's my phone. >> It's not a particularly good business,
[01:25:49.240 --> 01:25:54.120]   which is probably a big reason that they don't go to the low end, is because Google can't do it
[01:25:54.120 --> 01:25:59.480]   cheaply in the way that a lot of the Chinese and South Korean, Samsung owns a lot of the
[01:25:59.480 --> 01:26:05.160]   manufacturing or has, like, adjacent companies that do the manufacturing where they can get.
[01:26:05.160 --> 01:26:08.520]   >> That's true, and they own their chips. I mean, that's a good point. You could almost make the
[01:26:08.520 --> 01:26:12.120]   argument, though, because since really they want the data, and that's where they're making their
[01:26:12.120 --> 01:26:15.240]   money, and they get more of the information that they could- >> They get that anyway,
[01:26:15.240 --> 01:26:20.120]   because everyone uses Android. >> But that's a great question, though, is,
[01:26:20.120 --> 01:26:25.640]   is Google, after this event, with three, you know, with, they've got phones, they've got
[01:26:25.640 --> 01:26:31.720]   tablets and laptops, and they've got now home devices, is Google now a softer and a hardware
[01:26:31.720 --> 01:26:35.800]   company like Apple, or are they still just a softer company that has some hardware?
[01:26:35.800 --> 01:26:40.520]   Because that's a huge distinction. >> Yeah. >> Let's take a little break. There's still more
[01:26:40.520 --> 01:26:47.960]   to talk about. There's a new Chrome book from Google. It's the Pixel Slate and the new Google Home Hub.
[01:26:47.960 --> 01:26:52.280]   We had a great week this week. A lot of fun. In fact, some of the stories we're talking about
[01:26:52.280 --> 01:26:57.000]   here we talked about earlier. Watch this little video we've made, and I think previously- >> Previously
[01:26:57.000 --> 01:27:00.040]   on Twitch. >> Is there something inside of here we should- >> I don't know.
[01:27:00.040 --> 01:27:03.880]   >> Is there a rabbit in this hat? >> The new screensabers.
[01:27:03.880 --> 01:27:10.520]   Nicole Lazaro is the founder of Zio Design. >> This is the magically one. >> So this is
[01:27:10.520 --> 01:27:15.240]   augmenting reality in the sense that I'm like wearing sunglasses with some stuff superimposed
[01:27:15.240 --> 01:27:20.520]   on top of it. >> Really, the value of AR is going to be when I drop the walls out and then
[01:27:20.520 --> 01:27:25.800]   dynamically place the furniture and the puzzles and all that, you know, in the room that you're
[01:27:25.800 --> 01:27:31.000]   actually in. >> All about Android. >> Is it a tablet? Is the computer? What is it?
[01:27:31.000 --> 01:27:36.200]   >> Yes, what is it? I'll tell you what it is. It is the Pixel Slate. They kept on saying it's a
[01:27:36.200 --> 01:27:42.920]   portable device with a desktop experience. So this is a desktop computer with a huge 4K Acer
[01:27:42.920 --> 01:27:50.040]   monitor plugged into the Pixel Slate. >> So at the low end level, it's a iPad Pro competitor that
[01:27:50.040 --> 01:27:53.960]   the high end level, it's a Surface competitor. That's what it sounds like to me. >> Yes.
[01:27:53.960 --> 01:27:59.240]   Yeah, 100%. >> Mac Break Weekly. >> Apparently there's a daylight savings bug.
[01:27:59.240 --> 01:28:03.480]   >> The latest watch is flashing and rebooting. >> Yeah, time is tough.
[01:28:03.480 --> 01:28:06.840]   >> What if they pull the same sort of strategy that they use for the headphone
[01:28:06.840 --> 01:28:10.440]   jack? If they just simply say, we're Apple, we're just going to drop support for this and
[01:28:10.440 --> 01:28:14.840]   everybody else is just going to have to deal with it. And so if they say our phones, none of our
[01:28:14.840 --> 01:28:19.000]   devices can handle daylight savings time. You don't like it, call your congress phones. >> Oh my gosh,
[01:28:19.000 --> 01:28:23.640]   that would be great. >> To it, we read the tech news so you don't have to.
[01:28:23.640 --> 01:28:27.560]   >> What about the people who have to wind my clocks every twice a year?
[01:28:27.560 --> 01:28:31.400]   >> Well, they're jobs. >> How do we know when to change the batteries in our smarttakers?
[01:28:31.400 --> 01:28:33.000]   >> Lovey. >> I just love it.
[01:28:33.000 --> 01:28:38.360]   >> I think we were talking about something that's on the ballot in California that would keep
[01:28:38.360 --> 01:28:45.720]   California on daylight saving time all year round. >> You can put anything on the ballot in California.
[01:28:45.720 --> 01:28:51.480]   >> You can because as the ballot measure points out, it will take two-thirds of the state legislature
[01:28:51.480 --> 01:28:55.240]   to approve it and federal government approval. And we know the federal government's not going to
[01:28:55.240 --> 01:29:01.720]   say California can change its time zone. So it's like a non-starter, but I'm still voting yes.
[01:29:01.720 --> 01:29:08.280]   We've got to stop this insanity, the clock changing insanity. What if we just stayed daylight
[01:29:08.280 --> 01:29:11.880]   not as they stayed standard time? You could do that without federal approval. What's wrong with
[01:29:11.880 --> 01:29:16.600]   that, John? He doesn't like it. >> And it's dark all the time. It's dark to early.
[01:29:16.600 --> 01:29:21.720]   >> It's not dark all the time. Just here's a little tip. It actually doesn't change the amount of
[01:29:21.720 --> 01:29:25.160]   sunlight hours in any way that we change our clocks. It's the same.
[01:29:25.160 --> 01:29:29.640]   Right? When are we going to harvest our farms? >> It's an important question.
[01:29:29.640 --> 01:29:34.600]   >> Farmers can work in the dark. They are used to it. >> Can we get rid of time zones?
[01:29:34.600 --> 01:29:41.720]   >> Anyway, we all have machines to do all the taking a crash. >> Farmers don't care?
[01:29:41.720 --> 01:29:49.320]   >> No, farmers even say, we don't like it either. >> I know. It's always fun to blame the farmers.
[01:29:49.320 --> 01:29:52.920]   Trust No-Ann in our chat room says the kids need the extra daylight to work in the field.
[01:29:52.920 --> 01:29:56.920]   Since for the children, it's for the children. >> You did so much.
[01:29:56.920 --> 01:30:01.720]   >> We're about to do it again. >> I know. >> We have to do everything.
[01:30:01.720 --> 01:30:04.200]   >> You just set all your clocks and everything. I'm just going to be happy.
[01:30:04.200 --> 01:30:08.600]   That's the only reason I would want one of those Alexa microwaves would be that it could
[01:30:08.600 --> 01:30:13.080]   auto-set itself. >> Yeah. It's sad because in my house half the stuff does and half the stuff
[01:30:13.080 --> 01:30:15.560]   doesn't. >> Same. >> And I'm not sure which is right.
[01:30:17.560 --> 01:30:21.960]   I'm both dreading and can't wait for this year's fallback. Fallback is my favorite one because
[01:30:21.960 --> 01:30:23.800]   you get the year- >> You get another hour of sleep.
[01:30:23.800 --> 01:30:28.680]   >> Except that I'm going to be a Disney World running a half marathon that starts at 5.30 in the
[01:30:28.680 --> 01:30:34.200]   morning. >> I'm fine. >> And I miss out on sleeping in and be really wonder how many people are
[01:30:34.200 --> 01:30:37.560]   going to miss the start of the race because they didn't fall back or didn't know about it.
[01:30:37.560 --> 01:30:41.640]   >> That's an advantage for you. >> Yeah, possibly. >> Yeah. Yeah, because you're on,
[01:30:41.640 --> 01:30:44.840]   is it Disney World or Disneyland? >> Disney World.
[01:30:44.840 --> 01:30:48.760]   >> Oh, right. >> It's the Food Wine one. >> Sounds like fun. You run through Epcot and
[01:30:48.760 --> 01:30:55.720]   stop at each country and have some food and wine. >> And then by the end, you're drunk and your
[01:30:55.720 --> 01:31:00.040]   stomach is like, why not? >> Well, that's the thing for the Food and Wine. They used to do it
[01:31:00.040 --> 01:31:04.360]   after the park closed and then at the finish line, they had glasses of wine waiting for you.
[01:31:04.360 --> 01:31:07.160]   But now they've moved it to before the park opens and that's not appropriate.
[01:31:07.160 --> 01:31:10.280]   >> No. >> Yeah. >> That's a glass of wine at 8 in the morning.
[01:31:10.280 --> 01:31:12.360]   >> Who doesn't? >> Oh, I'm sorry.
[01:31:12.360 --> 01:31:19.000]   >> My mother would probably disagree with that. >> Oh, Lord. Our show is brought to you and I mean
[01:31:19.000 --> 01:31:23.160]   literally by Cashfly, if you ever download any of our Twitch shows for the last 10 years,
[01:31:23.160 --> 01:31:28.280]   you're downloading it from the Cashfly network. I think you know that. Cashfly is a content
[01:31:28.280 --> 01:31:34.920]   distribution network or CDN. And that's the reason you're getting our shows fast because you're
[01:31:34.920 --> 01:31:38.440]   getting it from a server that's near you. Cashfly has servers all over the world.
[01:31:40.120 --> 01:31:45.080]   By the way, they have guaranteed 100% availability because of this with a
[01:31:45.080 --> 01:31:52.280]   bulletproof 100% service level agreement in SLA. I don't know about you. I don't want to get a call
[01:31:52.280 --> 01:31:58.440]   three in the morning saying, "Twit is down. You can't download any of the shows. What are we going to do?"
[01:31:58.440 --> 01:32:04.040]   I have never received that call because we've always been on Cashfly. The other thing I love
[01:32:04.040 --> 01:32:10.680]   about Cashfly, it saves me a lot of money. More than 20% on average than other CDNs.
[01:32:10.680 --> 01:32:16.360]   Cashfly is awesome. And if there ever is an issue, they've got an expert team of smart,
[01:32:16.360 --> 01:32:21.160]   easy to talk to, agile techs that can answer all your questions the first time without having
[01:32:21.160 --> 01:32:26.760]   to pass you around to five different departments. If you deliver content to people, software,
[01:32:26.760 --> 01:32:31.960]   or if you're a podcast or any other business that has to have content readily available to your
[01:32:31.960 --> 01:32:36.920]   customers, you need Cashfly. No need to predict bandwidth consumption for the month.
[01:32:36.920 --> 01:32:45.320]   No contracts you're locked into inflexibly. It's, it, Cashfly builds its service around your unique
[01:32:45.320 --> 01:32:50.920]   demand and traffic patterns. Join the thousands of others. Like Twit, the Trust Cashfly is reliable
[01:32:50.920 --> 01:32:58.200]   network. Like Cashfly, I give you some sanity back. We love, and I mean this wholeheartedly,
[01:32:58.200 --> 01:33:05.320]   we love Cashfly. Twit literally would not exist without Cashfly. Petabytes of data every month
[01:33:05.320 --> 01:33:11.320]   from the Cashfly servers. I have an exclusive offer just for you. If you're interested and you
[01:33:11.320 --> 01:33:16.760]   want to move to Cashfly or if you've not used a CDN, believe me, trust me, it's the only way to
[01:33:16.760 --> 01:33:22.680]   travel. Cashfly is giving away a free detailed analysis of your current CDN bill and usage trends.
[01:33:22.680 --> 01:33:27.640]   You may believe in 20% or more on the table. I bet you you are and Cashfly is just better.
[01:33:28.520 --> 01:33:37.080]   Go to twit.cashfly.com to find out more. Twit.cashfly.com. I have to hide behind my surface because it's,
[01:33:37.080 --> 01:33:41.000]   I have to do Windows Hello. I don't know, it locked me out. Oh, says it's looking for me.
[01:33:41.000 --> 01:33:47.800]   Do you ever have to do that? Yes. It's looking for me. Yes, but I love it. It's so good. It's one of
[01:33:47.800 --> 01:33:53.640]   those things I have a, I have it on my service book and I just got a MateBook X Pro. I know I
[01:33:53.640 --> 01:33:58.360]   couldn't believe I bought a Windows laptop either, but it has the fingerprint sensor for Windows Hello,
[01:33:58.360 --> 01:34:03.640]   which is a little bit easier, but I do like, I love it. I have a Surface Go and it just, it's
[01:34:03.640 --> 01:34:10.280]   like it says hi. Hi Leo. I love it. This studio is great. Now I surface, of course, got updated a
[01:34:10.280 --> 01:34:13.480]   couple of weeks ago and they did update. I was hoping I was crossing my fingers. They wouldn't
[01:34:13.480 --> 01:34:18.920]   put out a new Surface Studio, but they did. They did. I got to buy it. It did. It's 3500 bucks,
[01:34:18.920 --> 01:34:25.480]   but it is. It's beautiful. I mean, it's nice. The things that I can annotate,
[01:34:25.480 --> 01:34:32.600]   I got to tell a strobes. It's set to be white right now. So let me set it to be a better color.
[01:34:32.600 --> 01:34:36.600]   I could tell a straight. I just love it. I just think it's so great. Pretty cool.
[01:34:36.600 --> 01:34:40.760]   My office mate has one and I'm always jealous. Then I realized that I would never use it. I
[01:34:40.760 --> 01:34:45.000]   have no reason to have it. It's really for illustrative. It's perfect for podcast hosts.
[01:34:47.960 --> 01:34:52.440]   Just perfect. All right. Quickly because we've spent enough time on Google today,
[01:34:52.440 --> 01:34:58.360]   but just quickly, let's talk about the slate, which is the, and again, none of this was a surprise.
[01:34:58.360 --> 01:35:01.800]   Although there were rumors, there were two rumors that didn't come true. One, that might be a
[01:35:01.800 --> 01:35:08.600]   Pixelbook 2, an update of last year's Chrome OS device. No. Nor was this a dual boot device that
[01:35:08.600 --> 01:35:14.200]   could run Windows. That was another rumor. Didn't come true. But the slate does look exactly like
[01:35:14.200 --> 01:35:20.680]   people like Kevin Tofel at all about Chromebooks.com predicted because of leaks from Bridge and others.
[01:35:20.680 --> 01:35:27.960]   It is a tablet 3000 by 2000, very nice high res, almost 300 DPI tablet to which you can add a
[01:35:27.960 --> 01:35:37.560]   keyboard and a pencil, very much like a Surface book, very much like an iPad. It could go either way.
[01:35:37.560 --> 01:35:46.200]   It's priced like a low end. Well, no, it gets super expensive. If you get the high end one and
[01:35:46.200 --> 01:35:51.240]   all the accessories and stuff like that, it becomes a $1,500 purchase, like a laptop.
[01:35:51.240 --> 01:35:56.040]   That's my one real critique on it, is that if I were going to be spending that much money,
[01:35:56.040 --> 01:36:01.000]   I think I would just get a Surface Pro, to be honest with you because I mean,
[01:36:01.000 --> 01:36:06.760]   you're a little biased. Well, okay, great. But even if we're like, okay, let's say that this
[01:36:06.760 --> 01:36:10.840]   was 14 months ago, I think I would probably still say the same thing just because you put crouton
[01:36:10.840 --> 01:36:14.520]   on it, but you can put other kind of, you know, you can dual boot and get kind of a Chrome OS
[01:36:14.520 --> 01:36:19.160]   experience just because I think it's really expensive. Whereas at least the Pixel book was
[01:36:19.160 --> 01:36:23.160]   expensive but had the really good hardware. This, it's, I don't know, it's like, I feel like you
[01:36:23.160 --> 01:36:29.000]   have to have that $200 keyboard. It's kind of like the Surface Go. It's a silly, silly to entertain
[01:36:29.000 --> 01:36:36.200]   buy-in one without it. But the keyboard is expensive. The keyboard's nice though. I mean, that's good,
[01:36:36.200 --> 01:36:40.680]   but it's still expensive. Yeah, they talked about how they brought in stuff from the Pixel book
[01:36:40.680 --> 01:36:45.240]   and the technology. They've, you know, and applied it to the mobile keyboard and I did a little
[01:36:45.240 --> 01:36:49.640]   typing with it. It definitely felt good. You know, given that I'm not a big fan of the foldable
[01:36:49.640 --> 01:36:54.920]   laptop and the portable keyboard aspect of it, just in general, but they did do a good job.
[01:36:54.920 --> 01:36:57.640]   And what blew my mind and what we saw in that clip earlier from all about Android,
[01:36:57.640 --> 01:37:02.520]   and what Flo and I were talking about on the show was that, you know, the fact that it can plug into
[01:37:02.520 --> 01:37:07.720]   a monitor and it could potentially be if you do buy the high-end version with all the memory and
[01:37:07.720 --> 01:37:14.600]   all the RAM or all the hard drive storage space, it could replace your desktop computer and your
[01:37:14.600 --> 01:37:18.840]   tablet, which is really kind of amazing. If I could carry my Mac mini around with me and have all
[01:37:18.840 --> 01:37:25.160]   my files on it, you could do that with this device. It was really impressive because it's both an
[01:37:25.160 --> 01:37:29.960]   iPad Pro competitor and a Surface Book competitor. Like, they're there.
[01:37:29.960 --> 01:37:37.720]   I gotta say though, the same question I asked about the Pixel phone. What is Google's plan here?
[01:37:37.720 --> 01:37:43.880]   I doubt they sell very many of these people buy Chromebooks typically want to spend $200, not
[01:37:43.880 --> 01:37:49.880]   $1,500. I don't honestly, I don't know, but that's the question we ask every time. And sometimes you go
[01:37:49.880 --> 01:37:54.440]   you go education. But no, it's too much for way too much for education.
[01:37:54.440 --> 01:37:57.400]   Yeah, but they've got all those deals. They've got all those development programs and stuff like
[01:37:57.400 --> 01:38:01.640]   that. Yeah, you're right. It's expensive. But I share your point. I don't know who's getting this.
[01:38:01.640 --> 01:38:09.320]   They had a rig set up in the demo pit area of one of these hooked up to a USB turntable with the
[01:38:09.320 --> 01:38:14.920]   DJ software up on there. And I would never even thought of that that. Like, I don't know who they're
[01:38:14.920 --> 01:38:19.240]   targeting for this stuff, but it seems pretty flexible. It seems like you could hit a bunch of
[01:38:19.240 --> 01:38:21.800]   markets now. How does Google market it? That's the question.
[01:38:21.800 --> 01:38:26.040]   Yeah. I think it would be for a lot of people would be a lot more attractive if the Linux app
[01:38:26.040 --> 01:38:29.480]   support was already there. If you could run like a electronic. They said it does. It runs
[01:38:29.480 --> 01:38:37.400]   not crouton. Christina. Christina. Okay. Yeah. Because this tablet is it's important to know
[01:38:37.400 --> 01:38:42.040]   this is not running Android because they never said the word Android. It's running Chrome OS,
[01:38:42.040 --> 01:38:47.560]   but they did say the Google Play Store is available on it. Yeah, which is which is still a terrible
[01:38:47.560 --> 01:38:51.960]   experience every time I mean, I know it's getting better, but it's still, you know, there are some
[01:38:51.960 --> 01:38:55.880]   some apps work better than others. But they flocked last couple of years ago with the Pixel
[01:38:55.880 --> 01:39:01.960]   C tablet, which was an Android tablet, which I thought hardware wise looked great. But and I bought
[01:39:01.960 --> 01:39:06.280]   one price point. The price point was awful. Yeah. Yeah. My phone was awful. And and and
[01:39:06.280 --> 01:39:11.720]   Android was not ready for tablets. Well, what happened is that the the one really well-selling
[01:39:11.720 --> 01:39:17.160]   Android tablet is from Amazon. And that you know, is largely consumption. And so, you know, most of
[01:39:17.160 --> 01:39:21.160]   your Android apps are designed for phones and haven't really been optimized. The Samsung
[01:39:21.160 --> 01:39:26.360]   tablet experience Android tablets not so well. Are they not? They all don't. And everybody,
[01:39:26.360 --> 01:39:33.320]   this is this is the question I get asked the most because when my what was the the Nexus 9 that
[01:39:33.320 --> 01:39:40.760]   that that tablet 10 Nexus 10. Yeah. So when my tablet when that when it bricked and I had to get
[01:39:40.760 --> 01:39:46.920]   a new tablet and the Pixel C was too much, I settled on the Asus Zen pad 3S10, which has been phenomenal.
[01:39:46.920 --> 01:39:52.360]   It's just been a great it's been a good price point. It was, you know, it running
[01:39:52.360 --> 01:39:56.920]   Asus's flavor of Android, which is fine because they don't really get in the way too much of it.
[01:39:56.920 --> 01:40:03.080]   But for $2.99 at Best Buy, you can't beat that. You know, like it's it's it solves it solves what
[01:40:03.080 --> 01:40:08.520]   you need for a tablet at a good price point. I'm a fan of Chrome OS. I kind of drank the Kool-Aid.
[01:40:08.520 --> 01:40:13.080]   At first, I was very skeptical, but I see it's value both in education and business where people
[01:40:13.080 --> 01:40:20.120]   want a secure platform that isn't as complicated as it uses Windows or Macintosh. But it also needs
[01:40:20.120 --> 01:40:26.440]   to be affordable. And it's a little pricey when you add it all up. Let's talk about the Home Hub.
[01:40:26.440 --> 01:40:32.440]   This is an interesting product because there are already a couple of Google Assistant-based
[01:40:32.440 --> 01:40:40.280]   devices with screens from Lenovo from JBL. I think somebody else is coming out with one at some point,
[01:40:40.280 --> 01:40:46.280]   LG maybe. But this one isn't like those. This one is running Chrome is more like a Chromecast.
[01:40:46.280 --> 01:40:50.120]   It's not running that specialized version of Android that Google put out for these other companies.
[01:40:50.120 --> 01:40:55.400]   Why is that? To make it better. But that's mean.
[01:40:55.400 --> 01:41:01.480]   No, but give it an edge against those other folks like when when Lenovo and those other folks
[01:41:01.480 --> 01:41:05.480]   started putting out, I think it was at CES. We started seeing these smart displays,
[01:41:05.480 --> 01:41:11.400]   Google Home with a display running Android. We're like, why isn't Google doing this? Are they
[01:41:11.400 --> 01:41:15.320]   just letting the OEMs figure it out? And then they're going to come do it better? And it looks like
[01:41:15.320 --> 01:41:20.200]   that's what happened. And again, this is the intersection of hardware and software because
[01:41:20.200 --> 01:41:22.920]   to me, the Google Home Hub was the star of the show.
[01:41:22.920 --> 01:41:24.920]   Yeah, I agree. That was what I said.
[01:41:24.920 --> 01:41:25.400]   Really?
[01:41:25.400 --> 01:41:26.840]   That's like the one I might actually buy.
[01:41:26.840 --> 01:41:27.640]   And I'm not even-
[01:41:27.640 --> 01:41:31.240]   Well, one thing is Google kept the price way down. They didn't put a camera on it.
[01:41:31.240 --> 01:41:35.960]   And it's tiny. Look at this image from ours, Technica, of a Pixel 2 XL next to it.
[01:41:35.960 --> 01:41:39.960]   It's basically not much bigger than a six inch screen, even though it's in a seven inch.
[01:41:39.960 --> 01:41:42.760]   How much is this digital picture frame? $149.
[01:41:42.760 --> 01:41:44.760]   That's a good price, right?
[01:41:44.760 --> 01:41:45.560]   Nice.
[01:41:45.560 --> 01:41:46.280]   Honestly, it is.
[01:41:46.280 --> 01:41:48.440]   I think I got one for parents for Christmas.
[01:41:48.440 --> 01:41:49.800]   Is it a digital plastic one?
[01:41:49.800 --> 01:41:50.840]   Is it more than that?
[01:41:50.840 --> 01:41:56.680]   They even said the keynote, we made the best digital picture frame capability
[01:41:56.680 --> 01:41:59.480]   because they knew that's what it would be used most of the time. But that said,
[01:41:59.480 --> 01:42:04.200]   it plugs into Google Photos, it curates your photos, it gives you a great experience for that.
[01:42:04.200 --> 01:42:09.240]   If you have Nest cameras, doesn't it automatically display or a Nest thermostat?
[01:42:09.240 --> 01:42:16.440]   So if you're in the Google ecosystem, it's kind of like a desk clock or a bedside clock
[01:42:16.440 --> 01:42:20.520]   sort of thing and you can talk to it. It's got assistance.
[01:42:20.520 --> 01:42:29.240]   And the whole overhaul of the Hall map and the dashboard for all your devices
[01:42:29.640 --> 01:42:33.480]   and the fact that this works with pretty much every IoT device in market,
[01:42:33.480 --> 01:42:35.560]   because right now IoT is a nightmare.
[01:42:35.560 --> 01:42:41.480]   I've got two lights in my house and I've got Samsung SmartThings in Google Home,
[01:42:41.480 --> 01:42:45.000]   and sometimes one light decides to turn on or off and I have no control over whether it
[01:42:45.000 --> 01:42:50.520]   wants to turn on or off. And if they're making steps to make IoT more easy to set up,
[01:42:50.520 --> 01:42:53.800]   easy to maintain, then that will help IoT as an industry.
[01:42:53.800 --> 01:42:56.680]   And I think this could be a game changer, especially at that price point.
[01:42:56.680 --> 01:43:01.240]   Admittedly, it's an IoT device that works with everything, but it works best
[01:43:01.240 --> 01:43:07.000]   if everything comes with Google. For instance, if you have the Nest doorbell,
[01:43:07.000 --> 01:43:11.000]   when the doorbell rings, the screen pops up the picture, which is awesome.
[01:43:11.000 --> 01:43:14.920]   But I don't want to replace my ring doorbell with a Nest doorbell.
[01:43:14.920 --> 01:43:20.120]   I ordered one, we'll see. I think the price is smart. At least Google there.
[01:43:20.120 --> 01:43:25.480]   We were just talking about how the Pixel 3 is too expensive, how the slate is.
[01:43:25.480 --> 01:43:29.400]   The price is smart. Well, there's a couple of differences. One, the smartphone market is
[01:43:29.400 --> 01:43:35.560]   totally saturated. So they don't need to be as competitive price-wise, whereas here,
[01:43:35.560 --> 01:43:41.960]   there's a ton of competition and a ton of open opportunity. Amazon is really, really aggressive.
[01:43:41.960 --> 01:43:47.240]   So I think that's part of it. I'd say my favorite thing. In addition to keeping the price down,
[01:43:47.240 --> 01:43:51.240]   they don't have the camera, which to me is really interesting.
[01:43:51.240 --> 01:43:55.960]   A, I think it's really smart when you juxtapose this product with the Facebook portal.
[01:43:55.960 --> 01:44:03.080]   Or the Echo show or the Echo spot. Amazon devices, which do have a camera on.
[01:44:03.080 --> 01:44:07.080]   Exactly, which I think a lot of people are comfortable having these assistants in their homes.
[01:44:07.080 --> 01:44:12.600]   My mom loves her Echo so much. I got it for her a couple years ago, and she loves it.
[01:44:12.600 --> 01:44:18.360]   I don't think people are okay with having the recording for the audio. They're not as
[01:44:18.360 --> 01:44:22.440]   comfortable with having the cameras necessarily. Lisa, let me put the spot in the bedroom because
[01:44:22.440 --> 01:44:28.520]   it's got a camera. Absolutely. I think this, eventually, people will be more comfortable,
[01:44:28.520 --> 01:44:35.000]   but this gets you over that hump. I think it cuts down the price. Google has to deal with,
[01:44:35.000 --> 01:44:39.960]   in my opinion, very valid reasons, people feeling creeped out by them knowing too much.
[01:44:39.960 --> 01:44:45.320]   By making that decision to not have the camera there, I think makes it a better product.
[01:44:47.400 --> 01:44:51.960]   Gets them in front of that critique, especially when they happen to launch the same week
[01:44:51.960 --> 01:44:58.520]   as the creepy Facebook assistant. The day after. Exactly.
[01:44:58.520 --> 01:45:04.040]   This product, people were saying that, it was genius that they didn't have a camera.
[01:45:04.040 --> 01:45:10.120]   It was complete luck. I'm speaking a mistake. I'm just saying that this is one of those
[01:45:10.120 --> 01:45:14.200]   instances where you can say, "Okay, they were really unlucky that all the leaks happened with
[01:45:14.200 --> 01:45:19.560]   the Pixel." They were lucky and so far as the timing on this, even though they didn't plan for it,
[01:45:19.560 --> 01:45:25.080]   it worked really well on their favor. It's hard to tell during the stream how big this device is.
[01:45:25.080 --> 01:45:31.960]   It's pretty tiny, right, Ron? The pictures I took are next to, there's a Nest thermostat.
[01:45:31.960 --> 01:45:36.200]   You can get a sense of it. It's basically the size of two Nest thermostats.
[01:45:36.200 --> 01:45:42.040]   It's a small device. It's smaller than a 7-inch tablet. It says a 6-inch screen.
[01:45:42.040 --> 01:45:47.960]   It doesn't. I think that's smart because my whole thing is I was talking to my wife about it,
[01:45:47.960 --> 01:45:54.200]   who is not one of us. She's technically capable, but she's not a tech person. She's like, "Well,
[01:45:54.200 --> 01:45:58.680]   I don't want a TV in the kitchen." I'm like, "Right, no, it's not a TV. It's a little screen
[01:45:58.680 --> 01:46:05.400]   that can help you do recipes and stuff like that." I think making it less cumbersome and less
[01:46:05.400 --> 01:46:11.480]   noticeable plays to its advantage because then it becomes a thing that you, it's not going to steal
[01:46:11.480 --> 01:46:14.920]   attention from the rest of the house, but rather fit into the house seamlessly so you help you
[01:46:14.920 --> 01:46:19.880]   better control things. Because it's so inexpensive, a lot of people, one, I just have an iPad. Well,
[01:46:19.880 --> 01:46:24.120]   iPads cost a lot of money and you might not want to have one in your kitchen at all times,
[01:46:24.120 --> 01:46:28.040]   whereas now you get the best of both worlds. You can ask it things and show things but still
[01:46:28.040 --> 01:46:32.760]   play YouTube videos if you want to see how something works. You can still have a visualization.
[01:46:32.760 --> 01:46:37.720]   And have parental restrictions on the YouTube, which is something that the other displays
[01:46:37.720 --> 01:46:42.120]   don't have, much to Jason, Hell's, or Shigarin, because this kids can pull up anything on YouTube
[01:46:42.120 --> 01:46:46.760]   and it's not filtering out the stuff they shouldn't be seeing. And Google smartly pointed that out
[01:46:46.760 --> 01:46:50.920]   in the presentation, which is nice to see. They say this will be the best YouTube device.
[01:46:50.920 --> 01:46:57.080]   They also, but they also in the announcement of I say it has a downtime mode where from six
[01:46:57.080 --> 01:47:01.560]   to seven it can't be used. It's only a clock or whatever it is. And again, going back to these,
[01:47:01.560 --> 01:47:06.120]   which I just find baffling and so funny, but this idea of wellness in technology and
[01:47:06.120 --> 01:47:09.560]   everyone's putting in functionality to make you not use their products.
[01:47:09.560 --> 01:47:18.200]   All right. Well, I should have bought two. Apparently for a brief period of time was available for
[01:47:18.200 --> 01:47:22.920]   a hundred bucks on Home Depot. I think they did an eight hour sale, which I thought was very
[01:47:22.920 --> 01:47:28.040]   interesting, but they sold quite a few. I saw that like spread virally very quickly.
[01:47:28.040 --> 01:47:31.080]   That would have, I wish I'd seen that because that would have been an absolute no-brainer.
[01:47:31.080 --> 01:47:34.120]   And I don't, I don't love Google Assistant for $100. I would have been like fine.
[01:47:34.120 --> 01:47:38.040]   Well, I wonder, I mean, I bet you'll there'll be more sales. Home Depot sells way more
[01:47:38.040 --> 01:47:44.120]   IoT stuff than I would have thought. Yes. They have, from the beginning, they've been really
[01:47:44.120 --> 01:47:47.240]   smarter than they targeted that area. I remember speaking with people at Home Depot a number of
[01:47:47.240 --> 01:47:50.840]   years ago when they started to kind of get into that space. I think they saw that they were,
[01:47:50.840 --> 01:47:55.800]   you know, it was an opportunity for them and the companies that are smart enough to kind of get
[01:47:55.800 --> 01:47:59.960]   the shelf space there are good because you have to think that people who go to Home Depot and
[01:47:59.960 --> 01:48:04.440]   spend a lot of time there aren't just, you know, individuals, but people who do contract work and
[01:48:04.440 --> 01:48:09.320]   who people hire to come do things for their homes. And, and, you know, if they can just do it,
[01:48:09.320 --> 01:48:13.240]   you know, go to Home Depot and get everything there. That's an awesome, awesome place for all
[01:48:13.240 --> 01:48:17.480]   those contractors. Another thing Google did not announce is an update to the Pixel Buds. I guess
[01:48:17.480 --> 01:48:23.640]   those are the OA now, right? Although one of the things they did just today, they put Pixel Buds
[01:48:23.640 --> 01:48:30.760]   real-time translation on all Google Assistant headphones. So I love my Pixel Buds.
[01:48:30.760 --> 01:48:34.760]   Do you use them? I use them every day. Yeah. I got it. Yeah. This is the one.
[01:48:34.760 --> 01:48:39.880]   Yeah. I'm like, I'm like, I'm seeing more and more people on the subway in New York wearing them.
[01:48:39.880 --> 01:48:43.960]   I'm, I'm, I'm, I think it's a, an emerging trend. I don't know. They're good. They were.
[01:48:43.960 --> 01:48:47.560]   No, come on. Everybody on the, on the subway has AirPods. Like that's been, that's been the
[01:48:47.560 --> 01:48:54.040]   thing. It's like the age of 2017. Like, AirPods are everywhere. Well, you know that. As of
[01:48:54.040 --> 01:49:00.760]   like last week, I saw, I've seen more Pixel Buds. What? No, come on. I see it. Like, I look,
[01:49:00.760 --> 01:49:05.080]   who's, we look at what's out in the wild and, no, I'm just going to sit on an airboat. Yeah,
[01:49:05.080 --> 01:49:08.840]   I agree. I'm not seeing you. You don't see them. I'm just saying like I, I, as a, and granted,
[01:49:08.840 --> 01:49:14.280]   I live in a place where it's not as, you know, I knew that Seattle even had a subway.
[01:49:14.920 --> 01:49:19.000]   It doesn't. But, but, but, but, I'm like on campus, but I have a monorail that goes to two stops.
[01:49:19.000 --> 01:49:21.160]   Yeah. Going back and forth. I've been on the monorail.
[01:49:21.160 --> 01:49:25.160]   And you are actually working on, on, on, on the subway system, but it won't be up for a
[01:49:25.160 --> 01:49:28.600]   couple of more years to connect the east and west sides. But I always joke like Seattle's like
[01:49:28.600 --> 01:49:34.760]   six months behind New York and it's true. Um, but, but yeah, uh, I, I, but you see AirPods
[01:49:34.760 --> 01:49:39.160]   everywhere, but I always, I'm, I'm like you, like, we're on like, I, I just notice like what
[01:49:39.160 --> 01:49:43.320]   every device everybody is using. Um, I haven't seen any Pixel Buds. I don't doubt that you see them
[01:49:43.320 --> 01:49:48.120]   in the wild. I'm just saying like AirPods are the ones that have like completely taken over.
[01:49:48.120 --> 01:49:50.520]   And it somehow pixel, pixel buds don't look as douchey.
[01:49:50.520 --> 01:49:54.600]   Yeah. That's part of the appeal. I don't know.
[01:49:54.600 --> 01:50:01.080]   It's funny how quickly we got used to those AirPods. I mean, I, I knew the second I, I got my first
[01:50:01.080 --> 01:50:06.920]   pre-production model from Apple in like September of 2016. I was like, yeah, I don't care how done
[01:50:06.920 --> 01:50:12.600]   these look. This is the best. Yeah. I'm on my third set. I keep losing them. Well, I keep buying
[01:50:12.600 --> 01:50:17.240]   the ones to replace the ones I lost. I, I was going to wait until Apple announced the new ones,
[01:50:17.240 --> 01:50:20.920]   but I apparently, I don't know what's, who knows. Yeah, we don't know if they're going to or what
[01:50:20.920 --> 01:50:27.080]   the changes. Yeah. We do. I'm sure we will get in the next two weeks an iPad announcement. Those,
[01:50:27.080 --> 01:50:32.280]   there's almost no doubt they'll be new iPad pros this year. The question is whether we'll see new
[01:50:32.280 --> 01:50:39.560]   MacBooks, um, or even a MacBook Air refresh or a MacBook mini or Mac mini refresh or an AirPod
[01:50:39.560 --> 01:50:44.840]   refresh. I doubt we're going to see the wireless charger. No, I think that things go on.
[01:50:44.840 --> 01:50:50.120]   Yes. Do a let's take a break in some final thoughts. A great panel, Mark Millian from Bloomberg
[01:50:50.120 --> 01:50:54.840]   Business Week. He's here. He's in the studio. It's really great to see you again, Mark.
[01:50:54.840 --> 01:51:00.440]   He's apparently gotten over me doing the droopy dog. So thank you. Yes. Thank you for not. Yes. No,
[01:51:00.440 --> 01:51:06.600]   I didn't do it. I've gotten over it too. I received it. I received the cease and assist.
[01:51:06.600 --> 01:51:11.240]   Ron Richards here from all about Android and Ron XO on the Twitter.
[01:51:11.240 --> 01:51:17.800]   Hence all the Google passion. Sorry. And hence all the Google passion. And for just passion in
[01:51:17.800 --> 01:51:22.440]   general, there's nobody better than Christina Warren from Microsoft. Great to have all three of you.
[01:51:22.440 --> 01:51:29.400]   Our show today brought to you interestingly enough by ring. We all love the ring video doorbell.
[01:51:29.400 --> 01:51:32.840]   Everybody in my neighborhood has it now, which is nice because of the neighborhood system on
[01:51:32.840 --> 01:51:36.680]   the ring. I can actually see what's going on in my neighborhood and neighbors will say,
[01:51:36.680 --> 01:51:41.320]   watch out for this guy or there's somebody stealing packages. Really pretty amazing.
[01:51:41.320 --> 01:51:47.320]   Ring has expanded. Of course, we've talked before about the doorbell, the spotlight cam and all of
[01:51:47.320 --> 01:51:55.800]   that. Now ring has the amazing ring alarm system, a wireless home security system that completely
[01:51:55.800 --> 01:52:03.080]   re events the home alarm system. You can do it yourself. You can do it for less. We know all know
[01:52:03.080 --> 01:52:08.280]   that those alarm companies make their money on the monthly, you know, high monthly fees for
[01:52:08.280 --> 01:52:14.680]   monitoring and of course tie you into contracts of I saw one that required two years and charged
[01:52:14.680 --> 01:52:21.160]   more than three times what ring charges. So ring is created and easy to install affordable home
[01:52:21.160 --> 01:52:25.400]   security system exactly what you'd expect from the ring folks. And there are no long-term
[01:52:25.400 --> 01:52:30.040]   contracts. You put the pieces together that are right for you. I have this is the kit,
[01:52:30.040 --> 01:52:37.160]   which comes with the basic set of sensors. It's got a motion sensor. It's got the keypad. Of
[01:52:37.160 --> 01:52:44.120]   course, there's an app that you can use a big loud alarm system and the sensors for door or window
[01:52:44.120 --> 01:52:49.480]   opening a range extended will extend the system, the signal from your base station. So if you've
[01:52:49.480 --> 01:52:55.720]   got outbuildings or a big house, it'll work there. The keypad's great. It's fantastic. Everything
[01:52:55.720 --> 01:53:03.160]   you need to protect your home and includes 24/7 professional monitoring for just $10 a month.
[01:53:03.160 --> 01:53:09.640]   This is such a great idea. I'm so glad ring did it. The ring alarm security kit,
[01:53:09.640 --> 01:53:15.240]   everything need to protect your home 24/7 professional monitoring $10 a month.
[01:53:16.040 --> 01:53:20.600]   It's available at ring.com and retail stores across the United States go to ring.com/twit
[01:53:20.600 --> 01:53:25.080]   to learn about how you can get a whole home security system for only $10 a month. Ring
[01:53:25.080 --> 01:53:32.440]   ring.com/twit. And if you don't have the ring doorbell or the ring floodlight cam,
[01:53:32.440 --> 01:53:37.640]   get it because we're surrounded. We have it everywhere and I just love it.
[01:53:37.640 --> 01:53:45.960]   I've had ring now for a couple of years. Ring.com/twit. Apple has sent a letter as you know
[01:53:45.960 --> 01:53:53.240]   the Australian government wants Apple to put a back door in its encryption. Apple has sent
[01:53:53.240 --> 01:53:59.000]   a strongly worded letter responding to the Australian Parliament's assistant and access bill,
[01:53:59.000 --> 01:54:03.400]   which was introduced late last month, designed to help the government more easily access devices.
[01:54:03.400 --> 01:54:08.760]   Of course, this is going to happen in the United States too. Apple writes in the letters,
[01:54:08.760 --> 01:54:13.640]   by the way, available online on a website hosted by the Australian Parliament, "The devices you carry
[01:54:14.440 --> 01:54:19.960]   not only contain personal emails, health information and photos, but are also conduits to corporations,
[01:54:19.960 --> 01:54:24.680]   infrastructure and other critical services. Vital infrastructure like power grids and
[01:54:24.680 --> 01:54:29.480]   transportation hubs will become more vulnerable when individual devices get hacked.
[01:54:29.480 --> 01:54:35.080]   Criminals and terrorists who want to infiltrate systems and disrupt sensitive networks may start
[01:54:35.080 --> 01:54:39.960]   their attacks by accessing just one person's smartphone. In the face of these threats, this is
[01:54:39.960 --> 01:54:46.360]   no time to weaken encryption. There is profound risk of making criminals jobs easier, not harder.
[01:54:46.360 --> 01:54:54.040]   Increasingly stronger, not weaker encryption is the best way to protect against those threats."
[01:54:54.040 --> 01:55:02.200]   Agreed? Definitely. Sure. But you can see law enforcement here in the States pushing for the
[01:55:02.200 --> 01:55:06.760]   same thing they want. Absolutely. Good luck, Australia. If the US are China, they can get Apple to give
[01:55:06.760 --> 01:55:10.360]   a increase in science or put in back doors. I hope this Australia has...
[01:55:10.360 --> 01:55:14.520]   You have to respect the bravado of asking for it.
[01:55:14.520 --> 01:55:16.840]   That's Australia for you right there in a nutshell.
[01:55:16.840 --> 01:55:22.840]   Oh, I'm going to get it, mate. It's like, no, we won't be doing that. Thank you. Thank you for
[01:55:22.840 --> 01:55:31.000]   your interest. Scary story in Wired this week starts in 2013, a young computational biologist
[01:55:31.000 --> 01:55:36.200]   named Janif Erlich shocked the research role by showing it was possible to unmask the
[01:55:36.200 --> 01:55:42.440]   identities of people listed in anonymous genetic databases using only an internet connection.
[01:55:42.440 --> 01:55:47.800]   Fast forward five years, the amount of DNA information housed in digital data stores has
[01:55:47.800 --> 01:55:53.480]   exploded thanks to consumer companies like our former sponsor, 23andMe and Ancestry.com.
[01:55:53.480 --> 01:55:57.400]   We actually have a new genetic analysis sponsor, Helix.
[01:55:57.400 --> 01:56:03.240]   Genetic profiles of 12 million people, according to recent industry estimates, have been created
[01:56:03.240 --> 01:56:12.360]   by these companies. And now there is some significant issue that no one's DNA is anonymous anymore,
[01:56:12.360 --> 01:56:19.400]   even though these databases are anonymous. Something to seriously worry about here.
[01:56:19.400 --> 01:56:24.440]   Researchers chose an anonymous female subject from the 1000 Genomes Project,
[01:56:24.440 --> 01:56:31.160]   which is an open access sequencing project. They reformatored DNA to resemble a typical
[01:56:31.160 --> 01:56:37.960]   consumer genetic profile, uploaded it to GED match. You may remember the name GED match that was used
[01:56:37.960 --> 01:56:46.680]   to arrest a guy on a cold rape case. I prosecute him and I think put him in jail because of a DNA
[01:56:46.680 --> 01:56:50.600]   match. Two relatives popped up one in North Dakota, one in Wyoming. The match suggested
[01:56:50.600 --> 01:56:55.480]   they were decently related four to six generations back. An hour of public record coming later,
[01:56:55.480 --> 01:57:04.120]   the team had found the husband and wife. So, yikes. I don't know what to say. I remember when
[01:57:04.120 --> 01:57:11.160]   Esther Dyson put her genome, there was something called the Personal Genome Project. I tried to
[01:57:11.160 --> 01:57:16.360]   get into it. And one of the things the Personal Genome Project, I think it's out of Harvard,
[01:57:16.360 --> 01:57:24.600]   does not attempt to anonymize the data because they say specifically, it's going to be impossible,
[01:57:24.600 --> 01:57:30.680]   but this genomic data is so valuable to medicine and to researchers, they asked volunteers to step
[01:57:30.680 --> 01:57:36.680]   forward and say, "Look, here's my genome, here's all my allotype information, all my phenotype
[01:57:36.680 --> 01:57:43.400]   information, all my illnesses, all my medical history, and it's public under my name because
[01:57:43.400 --> 01:57:50.360]   this information is so valuable to science." And at this point, I guess we're maybe all
[01:57:50.360 --> 01:57:57.720]   participating in that. I tried to get in, they wouldn't let me. But I thought this was worthwhile.
[01:57:57.720 --> 01:58:03.560]   The fact that my information would be public to me did not seem to be sufficient harm.
[01:58:03.560 --> 01:58:09.400]   Why wouldn't they leave? I don't know. I mean, I applied, didn't say no, not you. They just never
[01:58:09.400 --> 01:58:12.600]   let me do it. It's an exception. The application of Harvard has been declined.
[01:58:12.600 --> 01:58:19.240]   Yeah. Well, that wouldn't be the first time. It's the second time. And Harvard, you broke my heart
[01:58:19.240 --> 01:58:26.200]   twice. Hey, let's wrap this thing up because I know everybody wants to go to dinner and there's
[01:58:26.200 --> 01:58:32.040]   probably a great TV show on. Is there anything, Christina, that I'm missing Game of Thrones isn't
[01:58:32.040 --> 01:58:36.920]   back yet, right? I know. I know. I don't know. I've been watching a lot of stuff on Netflix,
[01:58:36.920 --> 01:58:45.640]   but I haven't really found anything new since what was the the Gillian Flynn show that was on HBO
[01:58:45.640 --> 01:58:50.680]   or the summer. I like that. Yeah, sure. Yeah. I really like that. What do you think of the
[01:58:50.680 --> 01:58:56.040]   ending of that though, kind of ended abruptly, didn't it? Yeah, it did. It did. But I, it was good.
[01:58:56.040 --> 01:59:00.600]   If you read the novel, it was the same ending, I guess. Oh, Rolando in an IRC just pointed out,
[01:59:00.600 --> 01:59:04.200]   and this is absolutely correct. The good place. The good place season three is. Everybody's
[01:59:04.200 --> 01:59:09.480]   loved that show is my, it is, you know what? It is. It's like the spiritual successor to like
[01:59:09.480 --> 01:59:14.120]   the office community, you know, Parks and Rec. Yeah. It's that show. And it's really good. My
[01:59:14.120 --> 01:59:19.480]   husband is really, really into it. And I, I really love it too. So yeah, I highly recommend the
[01:59:19.480 --> 01:59:24.920]   good place. Amazon Prime has been going crazy with new stuff. There's a new comedy that's kind
[01:59:24.920 --> 01:59:31.960]   of reminds me of the good place called Forever that I highly recommend. Fred Armisen and my
[01:59:31.960 --> 01:59:36.600]   Rudolph. Yes. Have you seen it? I watched the first couple episodes and we got to catch up my,
[01:59:36.600 --> 01:59:43.400]   my DVD player broke that had the Amazon thing through it. So I got to dig out my fire stick to
[01:59:43.400 --> 01:59:48.520]   finish watching it. Anthony Wiener has a new episodic show called the Romanovs, which I
[01:59:48.520 --> 01:59:53.960]   mean, Matt Wiener, not a Matt Wiener. Sorry. I did bad men. Anthony. Yeah. You don't want to say
[01:59:53.960 --> 01:59:59.800]   anything Anthony. We just got to show. No, no, documentary on him was really good. Oh, really?
[01:59:59.800 --> 02:00:05.720]   Yeah. They did during the campaign for, you know, I saw that that was wild. He was like,
[02:00:05.720 --> 02:00:11.800]   he was talking about it. Yeah. Crazy. Speaking of documentaries, and I mentioned this before,
[02:00:11.800 --> 02:00:15.560]   but you've got to, if you haven't yet watched the Fred Rogers, the Mr. Rogers.
[02:00:15.560 --> 02:00:19.320]   Yes. Oh, I saw the theater. That was, that was crazy.
[02:00:19.320 --> 02:00:22.040]   Ball in my eyes out. It was great. All right.
[02:00:22.040 --> 02:00:26.600]   And the Romanovs, our name is Amazon. I haven't started yet, but I'm very excited.
[02:00:26.600 --> 02:00:30.120]   That just came on a Friday. It's really interesting. We watched the first episode. I guess each
[02:00:30.120 --> 02:00:37.560]   episode will be a different story. All of them of people who think their descendants of the Tsar
[02:00:37.560 --> 02:00:43.160]   of Russia, Anastasia, of course, none of them are. The cast is crazy. The cast is great because
[02:00:43.160 --> 02:00:46.600]   it's Matthew Wiener. So there's a lot of men. A lot of men. A lot of men. Just laterally.
[02:00:46.600 --> 02:00:52.120]   Yeah. I know a Wiley and a really good cat. So far, I've only seen the first one, which I loved,
[02:00:52.120 --> 02:00:56.600]   and we'll be watching more of those. So yeah, there's some good stuff. Amazon, I thought it
[02:00:56.600 --> 02:01:02.680]   was interesting. Amazon Prime is taking advantage of this, you know, kind of laps from HBO and
[02:01:02.680 --> 02:01:08.040]   nothing going on. Well, this, I mean, this is the first year. I'm a huge TV watcher.
[02:01:08.040 --> 02:01:12.600]   And every year I sit down with the Entertainment Weekly, fall TV preview, and I make my list that
[02:01:12.600 --> 02:01:19.960]   I used to do a spreadsheet with grids and this year was the first year where I literally had no
[02:01:19.960 --> 02:01:23.080]   no new TV shows to add. There was nothing. There was nothing. We're in.
[02:01:23.080 --> 02:01:27.800]   Because all the talent is on streaming. Yeah, exactly. Like I just found streaming.
[02:01:28.600 --> 02:01:33.160]   Because literally, like, I mean, you know, between Netflix and Amazon and Apple even is getting into
[02:01:33.160 --> 02:01:38.360]   it. How successful Apple will be. Ultimately, I don't know. But there's certainly are hiring
[02:01:38.360 --> 02:01:42.680]   the right people. Everybody is signing these massive content deals with the streaming services.
[02:01:42.680 --> 02:01:47.560]   It's I mean, HBO is one of the only ones that can still retain people. Everybody else, like,
[02:01:47.560 --> 02:01:52.440]   why would you sign with with with NBC when you could sign with Amazon?
[02:01:52.440 --> 02:01:57.640]   Well, the big checks are coming these days from Apple. And CNBC had quite a scoop saying that
[02:01:57.640 --> 02:02:03.480]   Apple is planning to give away all this original content it's been buying as part of a new digital
[02:02:03.480 --> 02:02:08.040]   TV strategy. They want you to use the TV application on Apple TV and your iPhone.
[02:02:08.040 --> 02:02:15.320]   And I would be thrilled. I mean, they have signed huge, huge names to create content
[02:02:15.320 --> 02:02:20.440]   for this service. If they gave it away to Apple TV and Apple iPhone and Apple iPad owners,
[02:02:20.440 --> 02:02:24.680]   that would change the equation. I think quite a bit. It's an interesting
[02:02:24.680 --> 02:02:29.400]   argument away. The carpool karaoke. They can give that away to the cows come home.
[02:02:29.400 --> 02:02:32.840]   They can start up to the app.
[02:02:32.840 --> 02:02:39.000]   They're going for a family friendly. That's part of the problem. Apple has the HBO.
[02:02:39.000 --> 02:02:43.480]   Maybe they have to give it away because they don't want sex or violence. They killed the Dr.
[02:02:43.480 --> 02:02:48.920]   Dre. Which seemed amazing. Well, what's interesting because they did sign they signed a deal with
[02:02:48.920 --> 02:02:54.120]   the always sunny and Philadelphia guys. And I mean, they're fantastic. That's going to be wild, right?
[02:02:54.120 --> 02:02:59.240]   Yeah. But how, but how, like even the stuff that they've done for for networks that weren't FX or
[02:02:59.240 --> 02:03:05.960]   FXX is usually, you know, I mean, it's fine for network TV, but it's pushing boundaries a little bit.
[02:03:05.960 --> 02:03:11.320]   It's hard to family completely. So I wonder, I mean, and this is what a lot of people have kind of
[02:03:11.320 --> 02:03:15.480]   insane is they don't know how this is going to work with creators or signing these deals and
[02:03:15.480 --> 02:03:19.000]   don't necessarily know what freedom they have. Like the advantage you have with HBO is you know,
[02:03:19.000 --> 02:03:23.640]   basically nothing's off limits and Amazon and Netflix have kind of followed suit with that.
[02:03:23.640 --> 02:03:27.640]   And so it'll be interesting to see, you know, how you can get people with a lot of
[02:03:27.640 --> 02:03:34.200]   talents, how much money can kind of persuade them to maybe not have as much creative freedom.
[02:03:34.200 --> 02:03:41.160]   Apple has 19 shows in the works. A Spielberg reboot of amazing stories is coming. They signed
[02:03:41.160 --> 02:03:45.880]   Reese Witherspoon and Jennifer Aniston for a morning show drama series,
[02:03:45.880 --> 02:03:52.040]   which is a great idea. Something from the Battlestar Galactica guy. I'd love to see him back on TV.
[02:03:52.040 --> 02:03:57.960]   Yeah. Same. Or Ronald D Moore. I don't know if it'll be Battlestar Galactica like, but maybe
[02:03:57.960 --> 02:04:06.360]   Octavia Spencer and all are you sleeping? That's from Reese Witherspoon's Hello Sunshine Company,
[02:04:07.240 --> 02:04:14.200]   a thing called Home from, I think, who are these guys met? Turnour and Corey Reeser. Those names
[02:04:14.200 --> 02:04:22.600]   are so familiar. Well, this is all. Oh, great. Damien Chazelle got a show with him. I was talking
[02:04:22.600 --> 02:04:28.280]   about how I didn't like the Neil Armstrong movie first man. From the director of The Hunger Games,
[02:04:28.280 --> 02:04:35.800]   C, Kirsten Wiig, Kristen Wiig comedy series. Also Hello Sunshine. We love Little America.
[02:04:35.800 --> 02:04:43.880]   From a Camille Nijani and Emily V. Gordon, the big sick creators. Swagger from Kevin Duran
[02:04:43.880 --> 02:04:49.480]   of the Golden State Warriors. Another drama series for M Night Shyamalan, which would be amazing.
[02:04:49.480 --> 02:04:56.440]   You love Bob's Burgers, right? Central Park, written by Lauren Buschard.
[02:04:56.440 --> 02:04:58.840]   Lauren Bricketer, yeah, Lauren Buschard. Yeah, yeah, yeah.
[02:04:58.840 --> 02:05:05.480]   Creator of Bob's Burgers. They've got a lineup. And Oprah's in there too, right?
[02:05:05.480 --> 02:05:10.360]   Yeah, with the O Network, they're doing so much stuff with Apple and Oprah. But this is just
[02:05:10.360 --> 02:05:15.080]   another page in the big problem with the emergence of the streaming services, which is,
[02:05:15.080 --> 02:05:19.960]   I hate to make the reference, but it's the Springsteen 57 channels and nothing's on.
[02:05:19.960 --> 02:05:25.080]   There's so much, it's a flood of content to the point where there's no marketing behind any of
[02:05:25.080 --> 02:05:30.920]   these. And I mean, like, I have all the streaming services. I'm a huge TV and movie watcher. I'm a
[02:05:30.920 --> 02:05:35.960]   huge fan of Christopher Guest. Did you know that his latest movie was on Netflix? No. Nope.
[02:05:35.960 --> 02:05:43.000]   I know. How would you know? Exactly. Well, part of that's Netflix is completely crap app.
[02:05:43.000 --> 02:05:47.400]   But they all do. I mean, at least Amazon's buying some ads here in New York for like Ms.
[02:05:47.400 --> 02:05:52.520]   Maisel and stuff like that. But they, the problem is that they release shows on such a
[02:05:52.520 --> 02:05:57.960]   frenetic taste. And there's a burst of, there's a couple of days and then it goes away. Yeah.
[02:05:57.960 --> 02:06:02.040]   I mean, I honestly think that's why I like what Hulu does, where they often will, you know,
[02:06:02.040 --> 02:06:06.440]   do the weekly release schedule. They don't just dump it all once, let you binge. I think that
[02:06:06.440 --> 02:06:10.600]   that's a good model in a lot of cases. That's what your Amazon's doing with the Romanov.
[02:06:10.600 --> 02:06:15.960]   Exactly. Right. Which is the right way to do it. I think like, I think that you build people
[02:06:15.960 --> 02:06:19.720]   coming back and so much. I'm surprised to hear you say that because I love binge watching.
[02:06:20.920 --> 02:06:24.040]   I think binge watching is bad. It's not a good experience.
[02:06:24.040 --> 02:06:30.120]   I think the twashing is good if the show was written to be binged. I think that it's not as good
[02:06:30.120 --> 02:06:34.520]   if it was, which is what happened early on, which is if it's created episodically and then,
[02:06:34.520 --> 02:06:37.320]   you know, people wind up in jail. But I don't know. I just feel like there's, you know,
[02:06:37.320 --> 02:06:41.640]   it's the wrong point. There's so much content out there that you don't have the time to binge it
[02:06:41.640 --> 02:06:46.040]   all. I like at least having to, you know, if I can do it on my own time, that's great. But what
[02:06:46.040 --> 02:06:50.920]   makes television, and I think when HBO in particular, is still really relevant is that you know that
[02:06:50.920 --> 02:06:55.240]   you've got to tune in at a certain time or that this week, they're going to come out with the new
[02:06:55.240 --> 02:06:59.480]   episode and you'll find time to watch it. It's a problem to do it. Exactly. Because if it's always
[02:06:59.480 --> 02:07:03.240]   there, if I know I can just watch the whole thing, I'm much more likely to not bother.
[02:07:03.240 --> 02:07:07.560]   Well, I'm just going to say that every show we do on Twitter is written to be binged.
[02:07:07.560 --> 02:07:13.720]   I hope you'll binge us today. Thank you so much, Christina. Great to have you, Christina Warren,
[02:07:13.720 --> 02:07:19.240]   senior cloud dev advocate. I love saying that for a film girl. You can catch her on Channel
[02:07:19.240 --> 02:07:23.320]   Line at Microsoft, but also whenever we can right here. Thank you, Christina. Absolutely.
[02:07:23.320 --> 02:07:27.960]   Thank you so much. Mark Millian from Bloomberg Business Week. He's technology editor. He's at
[02:07:27.960 --> 02:07:32.840]   Mark Millian on Twitter. And it's always a pleasure. What are you working on? Anything exciting coming
[02:07:32.840 --> 02:07:39.320]   up? Just the usual. You file every day. I edit every day. You edit every day. Yeah.
[02:07:39.320 --> 02:07:43.000]   You're an editor now. You don't have to get my name out there.
[02:07:43.000 --> 02:07:46.120]   You get that blue pencil and you just go on like that all the time.
[02:07:46.120 --> 02:07:48.680]   Exactly. Nice. It's great to see you. Yeah, you too.
[02:07:48.680 --> 02:07:53.480]   And of course, Ron Richards, every four years, whether we need to or not, we're going to get
[02:07:53.480 --> 02:07:57.880]   you on the no, I'm sorry that we haven't. Listen, I was, I know flow wasn't available. It's fine.
[02:07:57.880 --> 02:08:05.320]   No, we love you, Ron. You'll catch him on all about Android at Ron X. Oh, we do Twitter every
[02:08:05.320 --> 02:08:09.080]   Sunday afternoon. I just kind of look forward to it all week. Like I can't wait to get together
[02:08:09.080 --> 02:08:13.160]   with my buddies. And God, I wonder what they think about this. I hope you feel the same.
[02:08:13.160 --> 02:08:18.280]   It's 3 p.m. Pacific, 6 p.m. Eastern time. 2200 UTC. If you want to tune in, you can go to
[02:08:18.280 --> 02:08:23.800]   twit.tv/live. You can also join our studio audience at a bunch of great people. Robert from
[02:08:23.800 --> 02:08:31.080]   Maryland. We had Ron and Anne visiting from Colorado and Tracy. Nice to see you. Ivan was here. He's
[02:08:31.080 --> 02:08:38.120]   the voice of IBM's Watson, I believe. No, no, but he is a big data researcher and an artificial
[02:08:38.120 --> 02:08:43.000]   intelligence expert. We had a great conversation before the show. Mike in Italian visiting from
[02:08:43.000 --> 02:08:48.840]   Bristol, England. Was it okay in Italian? Yeah, she's, I'm her nightmare. She hears from my head.
[02:08:48.840 --> 02:08:53.880]   Wonder why. And Frannick from Syracuse. Thank you all from Come and Buy. We really appreciate it.
[02:08:53.880 --> 02:08:58.280]   If you want to be in studio, we'll put a chair out for you. Just email tickets at twit.tv. Of
[02:08:58.280 --> 02:09:01.640]   course, you have to be in the Petaluma area up here in Northern California. We're about an hour
[02:09:01.640 --> 02:09:06.120]   north of San Francisco. So if you're coming to San Francisco or the wine country, we're perfectly
[02:09:06.120 --> 02:09:10.680]   situated and we'd love to have you here. You can also get on demand copies of everything we do at
[02:09:10.680 --> 02:09:19.800]   Twit. Written for binge. Written to binge, baby, at twit.tv. I probably wouldn't want to binge our
[02:09:19.800 --> 02:09:26.840]   shows. Even one episode basically is a binge. I mean, they're long enough. Just to go to twit.tv,
[02:09:26.840 --> 02:09:32.360]   you can either download them there or, thanks to Cashline, or subscribe in your favorite podcast
[02:09:32.360 --> 02:09:36.600]   application. We have instructions at twit.tv/subscribe, but I bet you can figure it out.
[02:09:36.600 --> 02:09:39.800]   Thanks for being here. We'll see you next time. Another Twitch.
[02:09:39.800 --> 02:09:52.200]   It's amazing.

