;FFMETADATA1
title=I Am Gozor, the Lunchmaster!
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=719
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.600]   It's time for Twit this week in Tech. Great panel coming up for you.
[00:00:03.600 --> 00:00:10.100]   We've got Georgia Dow from iMour.com. Her colleague, Mr. Mobile, from Mobile Nations, Michael Fisher,
[00:00:10.100 --> 00:00:15.200]   and Brian McCulloch, the tech meme ride home coming up the US versus Huawei.
[00:00:15.200 --> 00:00:19.700]   How do you know if your phones been hacked and the fake Joe Rogan?
[00:00:19.700 --> 00:00:21.300]   It's all next on Twit.
[00:00:21.300 --> 00:00:27.500]   Netcast, you love from people you trust.
[00:00:27.500 --> 00:00:32.500]   This is Twit.
[00:00:32.500 --> 00:00:47.500]   This is Twit. This week in Tech, episode 719, recorded Sunday, May 19th, 2019.
[00:00:47.500 --> 00:00:50.500]   I am Gozer, the lunchmaster.
[00:00:50.500 --> 00:00:54.500]   This week in Tech is brought to you by Thousand Eyes.
[00:00:54.500 --> 00:01:04.500]   Companies that run in the cloud rely on Thousand Eyes. It's the place they go first to see, understand, and improve the digital experience of their cloud-based applications and services.
[00:01:04.500 --> 00:01:11.500]   Do the cloud write and improve services for your customers and employees today. Visit ThousandEyes.com/Twit.
[00:01:11.500 --> 00:01:19.500]   And by ZipRecruiter. Hiring is challenging, but there's one place you can go where hiring is simple and smart.
[00:01:19.500 --> 00:01:27.500]   That place is ZipRecruiter, where growing businesses connect to qualified candidates. Try it free at zipprecruiter.com/Twit.
[00:01:27.500 --> 00:01:36.500]   And by Atlassian. Atlassian Software powers the full spectrum of collaboration between IT teams and the rest of your organization.
[00:01:36.500 --> 00:01:43.500]   Visit Atlassian.com/IT to see what IT can be by giving their products a try for free.
[00:01:43.500 --> 00:01:49.500]   And by ExpressVPN. Protect your online privacy with one click. It's that easy.
[00:01:49.500 --> 00:01:55.500]   For three extra months free with a one-year package, go to expressvpn.com/Twit.
[00:01:55.500 --> 00:02:00.500]   It's time for Twit this week in Tech to show where we cover the week's tech news.
[00:02:00.500 --> 00:02:11.500]   We're going to make this quick because the end of the Game of Thrones is a mere few hours away joining us from all around the world.
[00:02:11.500 --> 00:02:20.500]   Let's start in beautiful Brooklyn, New York, where Brian McCullough, the tech meme, "Righ Home" is with us. Internet historian. Hi Brian.
[00:02:20.500 --> 00:02:22.500]   How are you?
[00:02:22.500 --> 00:02:27.500]   You've got a lanyard tree on your wall, but it's so tiny and cute.
[00:02:27.500 --> 00:02:35.500]   Yeah, I'm lost. I had a lanyard box that I lost. So this is only in the last, I don't know, however many months.
[00:02:35.500 --> 00:02:41.500]   You're too over. I have mine hanging just inside my door. I could barely get my door open.
[00:02:41.500 --> 00:02:50.500]   And I haven't been to a trade show in years. It's all yellowed and old. I figure, I don't know what, my kids will want it when I die.
[00:02:50.500 --> 00:02:55.500]   You could get in this shows that happened 40 years ago, kids.
[00:02:55.500 --> 00:03:03.500]   Also with us from imore.com, the one and only senior editor, Georgia Dow. Hello, Georgia.
[00:03:03.500 --> 00:03:10.500]   Hey. Good to see you from Morel. Good to see you. Yes. Maybe I'm too cold here.
[00:03:10.500 --> 00:03:19.500]   Is it? Winter's still there? We had thunder lightning, very, very frightening. Hail stones coming down, rain pouring down.
[00:03:19.500 --> 00:03:25.500]   It looked like it was in the shower. It was the weirdest weather in May for us as any unheard of.
[00:03:25.500 --> 00:03:32.500]   Well, we didn't have the coming apocalypse, but it was cold. Winter is coming and ending. So.
[00:03:32.500 --> 00:03:37.500]   I feel like the apocalypse is not far off. You could be right.
[00:03:37.500 --> 00:03:44.500]   You've been preparing for the zombie apocalypse, I know. I have. I have. You have to always be ready. You never know.
[00:03:44.500 --> 00:03:52.500]   Did I read somewhere that it was 84 degrees in Antarctica this week? I think I did read that.
[00:03:52.500 --> 00:03:57.500]   Not a normal temperature this time of year. I don't think.
[00:03:57.500 --> 00:04:05.500]   Also with us, first timer, let's be kind. He's Georgia's colleague at Mobile Nations, Mr. Mobile himself.
[00:04:05.500 --> 00:04:10.500]   Michael Fisher. Hi, Michael. Hi, Leo. Thank you for having me. A long time listener, actually.
[00:04:10.500 --> 00:04:15.500]   It's a true thing. Not just something you say. I think I've been listening to you since 2013.
[00:04:15.500 --> 00:04:18.500]   So it's not easy to share their waves. Are you a first time caller?
[00:04:18.500 --> 00:04:22.500]   Yeah. A precise. I was a first time guest.
[00:04:22.500 --> 00:04:28.500]   So who here? I'm not. No spoilers. We're not going to talk about the Game of Thrones as what's happened,
[00:04:28.500 --> 00:04:39.500]   but just who here will be glued to the TV tonight? Yeah. It's going to be 9 p.m. Pacific, 6 p.m. for us so we can watch it, you know, in your time.
[00:04:39.500 --> 00:04:43.500]   I'm sure I'm not the first person to ask you this, but is that why we're starting earlier?
[00:04:43.500 --> 00:04:49.500]   What? You think I care that much about a TV show? No one would be watching.
[00:04:49.500 --> 00:04:55.500]   It's just a coincidence that we started doing the show earlier about six weeks ago. Just a coincidence.
[00:04:55.500 --> 00:04:57.500]   That's all I can say.
[00:04:57.500 --> 00:05:02.500]   Correct me if I'm wrong, though. Didn't you say about the two or three weeks ago that you stopped after the Red Wedding?
[00:05:02.500 --> 00:05:07.500]   You were hopeless behind. No. Was it, I think it was Paris Martynot that said that?
[00:05:07.500 --> 00:05:14.500]   No. She was hopeless behind. By the way, this is the maybe the most annoying humble brag anybody can make.
[00:05:14.500 --> 00:05:21.500]   As soon as they deviated from the books, I just had to stop watching. There you go. That's the way to be.
[00:05:21.500 --> 00:05:26.500]   I just had to stop watching. People are really upset with this final session.
[00:05:26.500 --> 00:05:30.500]   I'm really upset too. I'm really upset. I'm not going to lie. I'm very upset.
[00:05:30.500 --> 00:05:34.500]   Without saying why, don't say what happened to me. I won't say why. I'm not going to ruin it.
[00:05:34.500 --> 00:05:40.500]   This is my feeling towards it is that once you make a mythos or characters development,
[00:05:40.500 --> 00:05:45.500]   you need to follow through with that. If some character has a certain set of personality traits,
[00:05:45.500 --> 00:05:52.500]   a certain lineage, a certain way about them, you can't then make a 180 switch or change that or just throw out
[00:05:52.500 --> 00:06:00.500]   the threads of story that you've started. That bothers me. You have to follow your own rules of the game that you play.
[00:06:00.500 --> 00:06:05.500]   Once they deviated from the books, they didn't really have this great template.
[00:06:05.500 --> 00:06:09.500]   They just did things willy-nilly for absolutely no reason.
[00:06:09.500 --> 00:06:15.500]   Characters that are brilliant are suddenly idiotic. Characters that are conniving are suddenly naive.
[00:06:15.500 --> 00:06:22.500]   That intensely bothers me. They say that George RR Martin sat down with Benny Off and the other guy
[00:06:22.500 --> 00:06:27.500]   and told him what was going to happen. They had the outline.
[00:06:27.500 --> 00:06:34.500]   They did not place in a certain character from the books and changed things.
[00:06:34.500 --> 00:06:37.500]   Then George RR Martin was really angry with them.
[00:06:37.500 --> 00:06:39.500]   You've been keeping up with the gossip.
[00:06:39.500 --> 00:06:45.500]   I'm way too far into the gossip. They were left on their own.
[00:06:45.500 --> 00:06:49.500]   Then they do things just because everyone knows about it. They're like,
[00:06:49.500 --> 00:06:52.500]   "Oh, everyone thinks this is going to be this thing. We have to shock just for shock value."
[00:06:52.500 --> 00:06:53.500]   Just to the opposite.
[00:06:53.500 --> 00:06:54.500]   That's bad storytelling.
[00:06:54.500 --> 00:06:59.500]   Really good article. I won't go too deep into it because it is a spoiler-laden article
[00:06:59.500 --> 00:07:05.500]   in Scientific American by the great Zanep Tufeki who is a brilliant sociologist.
[00:07:05.500 --> 00:07:12.500]   She came up with a sociological explanation. She says the real reason fans hate the last season of Game of Thrones,
[00:07:12.500 --> 00:07:14.500]   somewhat what you were talking about, Georgia.
[00:07:14.500 --> 00:07:22.500]   She says most television is psychological. It's based on characters and their development and their arc
[00:07:22.500 --> 00:07:26.500]   and their psychology. One of the reasons we loved Game of Thrones is it wasn't.
[00:07:26.500 --> 00:07:33.500]   It was sociological so that the characters in Game of Thrones was why in Game of Thrones
[00:07:33.500 --> 00:07:39.500]   you could spend a whole season with Ned Stark and kill him because it wasn't about the individuals.
[00:07:39.500 --> 00:07:44.500]   It was how individuals are affected by their circumstances.
[00:07:44.500 --> 00:07:47.500]   That's the sociological part, is what's going on.
[00:07:47.500 --> 00:07:49.500]   They followed those rules.
[00:07:49.500 --> 00:07:51.500]   They did follow those rules.
[00:07:51.500 --> 00:07:57.500]   I used to be angry at the characters because they did something stupid and then they paid the price because of that.
[00:07:57.500 --> 00:08:01.500]   Now I'm angry at the writers because people have plot armor.
[00:08:01.500 --> 00:08:06.500]   They can go through the zombie apocalypse. They can go through 100,000 people.
[00:08:06.500 --> 00:08:10.500]   You know their plot armor will keep them safe and nothing is going to happen to them.
[00:08:10.500 --> 00:08:16.500]   You either shouldn't put your characters in that place or you shouldn't kill them off.
[00:08:16.500 --> 00:08:22.500]   They should be killed off. It shouldn't be that you don't have any worry or fear and things don't make any more logical sense.
[00:08:22.500 --> 00:08:27.500]   I'm going to tie this to technology in a second. Don't worry. Go ahead.
[00:08:27.500 --> 00:08:31.500]   It's just why are we only getting six episodes, right?
[00:08:31.500 --> 00:08:34.500]   Something happened in the last episode that I'm going to try hard not to spoil.
[00:08:34.500 --> 00:08:36.500]   Wait a minute, you've seen it already?
[00:08:36.500 --> 00:08:39.500]   No, the previous episode.
[00:08:39.500 --> 00:08:41.500]   Don't tell me.
[00:08:41.500 --> 00:08:44.500]   I know this character was going to do a thing throughout the whole season.
[00:08:44.500 --> 00:08:47.500]   All the books, this character was always going to do this thing.
[00:08:47.500 --> 00:08:53.500]   And then a character in the last episode turns to this other character and says you don't want to do that.
[00:08:53.500 --> 00:08:56.500]   And the character says, oh no, you're right. I'm going to bounce.
[00:08:56.500 --> 00:09:02.500]   And the problem is that we used to get two or three episodes of that building out.
[00:09:02.500 --> 00:09:06.500]   We spent 69 episodes getting them ready to do this thing.
[00:09:06.500 --> 00:09:11.500]   So if you just had two additional episodes this season, I don't think anyone would be angry.
[00:09:11.500 --> 00:09:19.500]   So Tufekki is saying in sociological storytelling, the characters have personal stories and agency, of course.
[00:09:19.500 --> 00:09:24.500]   We don't get rid of that, but they're also greatly shaped by institutions and events around them.
[00:09:24.500 --> 00:09:30.500]   The incentives for character behavior come from external forces and even influence their inner life.
[00:09:30.500 --> 00:09:33.500]   You're a psychologist, George, and I think you'll understand this.
[00:09:33.500 --> 00:09:35.500]   This is actually how life is.
[00:09:35.500 --> 00:09:43.500]   And in plot-driven fiction, especially television, we throw out the circumstances and it's all about the internal life.
[00:09:43.500 --> 00:09:49.500]   People then fit, she says, their internal narrative to align with their incentives, justifying and rationalizing their behavior along the way.
[00:09:49.500 --> 00:09:52.500]   I mean, you could, that's Game of Thrones, right?
[00:09:52.500 --> 00:10:00.500]   She says, understanding, here's another example, understanding Hitler's personality alone will not tell us much about the rise of fascism.
[00:10:00.500 --> 00:10:10.500]   So that question, the time machine, if you could go back in time, would you kill baby Hitler is irrelevant because it isn't the fact that Hitler existed that created Nazism.
[00:10:10.500 --> 00:10:17.500]   It was the times and it didn't matter if Hitler was born because somebody else would have done it anyway.
[00:10:17.500 --> 00:10:23.500]   That's the sociological narrative as opposed to the great man theory or the psychological narrative.
[00:10:23.500 --> 00:10:32.500]   It says this, the problem we have now understanding what's happening in technology comes down to this exactly.
[00:10:32.500 --> 00:10:42.500]   That we're in a sociological environment trying to understand it by saying, what's going on in Mark Zuckerberg's brain?
[00:10:42.500 --> 00:10:44.500]   Does that make sense?
[00:10:44.500 --> 00:10:53.500]   She says, whether done well or badly, the psychological internal genre leaves us unable to understand and react to social change.
[00:10:53.500 --> 00:11:04.500]   Arguably, the dominance of the hero, anti-hero narrative is also the reason we're having such a difficult time dealing with the current historic technology transition.
[00:11:04.500 --> 00:11:07.500]   And she sees this in her research and writing.
[00:11:07.500 --> 00:11:19.500]   She says, I encounter this obstacle all the time. There are significant number of stories and books that talk about Zuckerberg or Sandburger, Dorsey or Bezos and going back to Steve Jobs.
[00:11:19.500 --> 00:11:32.500]   And of course their personalities matter, but only in the context of business models, technological advances, the political environment, the lack of meaningful regulation, the existing economic and political forces, it's much more complex.
[00:11:32.500 --> 00:11:40.500]   So the focus on the hero or anti-hero narrative is to simplify it so much you can't understand what's going on.
[00:11:40.500 --> 00:11:48.500]   All right, I've said enough. You notice there were no spoilers there. I'm still trying to figure out what the hell Brian was talking about.
[00:11:48.500 --> 00:11:54.500]   So it was not a, I can't even know who you meant, but we'll talk next week.
[00:11:54.500 --> 00:11:55.500]   I was with you, Brian.
[00:11:55.500 --> 00:11:56.500]   She got it.
[00:11:56.500 --> 00:12:03.500]   So was that makes sense? Is that one of the reasons we're having difficulty understanding this very dramatic transition we're going through?
[00:12:03.500 --> 00:12:09.500]   I didn't mean to get so deep and philosophical right away, but I thought it was really interesting is that we were still thinking in terms of people.
[00:12:09.500 --> 00:12:21.500]   And is it also a question that the so-called characters in this story of tech maybe aren't even in control of their own narratives?
[00:12:21.500 --> 00:12:22.500]   Exactly.
[00:12:22.500 --> 00:12:33.500]   It's, you know, to say, let's punish Mark Zuckerberg or even to say, let's break up Facebook is to miss what caused all that.
[00:12:33.500 --> 00:12:38.500]   It's like saying, well, if we killed Hitler, it was a baby, none of that World War II would never have happened.
[00:12:38.500 --> 00:12:40.500]   Of course it would have.
[00:12:40.500 --> 00:12:45.500]   If we didn't, if we didn't have Facebook, we would have been compelled to invent them.
[00:12:45.500 --> 00:12:48.500]   Yeah, you think?
[00:12:48.500 --> 00:12:55.500]   Well, certainly someone, I mean, you know, Google laid the groundwork for the whole surveillance capitalism to quote the.
[00:12:55.500 --> 00:12:58.500]   Yeah, that's what your son, that's what your son, Zoboff says.
[00:12:58.500 --> 00:12:59.500]   Yeah.
[00:12:59.500 --> 00:13:08.500]   All that, if, if, you know, you want to assign any sort of individual narrative to it, the only thing that happened was Zuckerberg was like, all right, that's a good business model for us.
[00:13:08.500 --> 00:13:15.500]   People forget that everyone was like, well, they've got these trillions of page views, but what does that mean? They're just a better Yahoo.
[00:13:15.500 --> 00:13:27.500]   And so he was smart enough to latch on to this larger business model and trend in capitalism that Google had pioneered and then did it just as well or maybe better than they did.
[00:13:27.500 --> 00:13:42.500]   So do you think really we can say, oh, yeah, if it weren't, if Larry Page and Sergey Brin hadn't come along, if Cheryl Sandberg hadn't then gone from Google to Facebook and handed the DNA over to Mark Zuckerberg, none of this would have happened?
[00:13:42.500 --> 00:13:47.500]   No, but that's what I'm saying is that this is those, those were business models.
[00:13:47.500 --> 00:14:01.500]   The idea that the most valuable thing in the world is information and data and the thousands of data points that we emit like the, the breaths we exhale, like someone was going to figure that out.
[00:14:01.500 --> 00:14:05.500]   And it just so happened that Facebook needed a business model at the time.
[00:14:05.500 --> 00:14:07.500]   So right, someone else would have come along.
[00:14:07.500 --> 00:14:14.500]   There would have been another, you know, maybe it would have been my space. Maybe my space. Tom would be sitting in front of congressional here.
[00:14:14.500 --> 00:14:24.500]   If it was Jack Dorsey who seemingly can't really go for the jugular as naturally as Zuckerberg can, like maybe someone else would have not, you know, right.
[00:14:24.500 --> 00:14:36.500]   As humans, we love the individual story because we kind of understand people because we are one, but it's a lot harder to kind of take in the whole scope of everything and say, well, these are the forces.
[00:14:36.500 --> 00:14:42.500]   And I would say that if you're going to regulate or if you're going to try to solve this problem, you have to understand at that level.
[00:14:42.500 --> 00:14:50.500]   You can't understand it at a level of, well, these are interactions between people and we can fix this if we just get rid of the people or fix the people or throw them in jail.
[00:14:50.500 --> 00:15:00.500]   It's the same thing for liberals who look at the 2016 election and say, oh, that, you know, what a miscarriage of justice, even if Donald Trump hadn't come along.
[00:15:00.500 --> 00:15:06.500]   This is a trend worldwide. This trend towards authoritarianism and nationalism is a trend worldwide.
[00:15:06.500 --> 00:15:11.500]   And you're not going to understand it if you say, well, it's that guy's fault.
[00:15:11.500 --> 00:15:20.500]   I just, by the way, I learned, I hope this is true. The ASL symbol for Donald Trump, by the way, if you'd like to know, I think this is president and this is Trump.
[00:15:20.500 --> 00:15:30.500]   I was taught this by a speaker of ASL. Don't know what that means, but I'm just saying.
[00:15:30.500 --> 00:15:35.500]   Now, don't write me because that's could be just completely wrong.
[00:15:35.500 --> 00:15:43.500]   I just think it's if we're going to understand it, we need to understand it in a broader sociological context than we do.
[00:15:43.500 --> 00:15:45.500]   Because we like to talk about people. Yeah.
[00:15:45.500 --> 00:15:48.500]   Yeah. And I mean, it isn't about the people.
[00:15:48.500 --> 00:15:53.500]   I like to blame the social minister, you know, it's all her fault.
[00:15:53.500 --> 00:15:58.500]   Nothing. Don't say anything. Don't say it. We're not going to say anything.
[00:15:58.500 --> 00:16:05.500]   Michael, you're all so into this. Mr. Mobile Nation. Are you Mr. Mobile? Are you still into this? Are you all over the game?
[00:16:05.500 --> 00:16:09.500]   We talking Game of Thrones. No, I have to say I gave it the college try.
[00:16:09.500 --> 00:16:14.500]   And after about the fifth or six week of being depressed before I went to bed every Sunday, I was like, no, I can't.
[00:16:14.500 --> 00:16:19.500]   I'm not going to show up for this. I've never seen anything saying I'm never watching this again.
[00:16:19.500 --> 00:16:23.500]   Yeah. Oh, that's what I remember. I think that that's the way that many people are going to feel after tonight.
[00:16:23.500 --> 00:16:27.500]   And I, by the way, the minute the next step is season came along.
[00:16:27.500 --> 00:16:29.500]   I said, I still kind of watch that.
[00:16:29.500 --> 00:16:33.500]   I'm not saying I was Star Trek discovery, but not. Yeah. I understand it.
[00:16:33.500 --> 00:16:37.500]   I understand it. I guarantee you that all the people are bitching about the last season are still watching it.
[00:16:37.500 --> 00:16:41.500]   Highest reading ever in HBO history. Yeah. They'll still watch it.
[00:16:41.500 --> 00:16:46.500]   Just like lost. It's like lost. Right. So it's exactly like lost. Everybody hated lost.
[00:16:46.500 --> 00:16:50.500]   But you couldn't. Well, they hated the ending. They liked the show. The ending. They did.
[00:16:50.500 --> 00:16:55.500]   I haven't seen it. Oh, you haven't? Yeah. Well, don't let it ever go to the room for you.
[00:16:55.500 --> 00:17:03.500]   The ending is great. Continue on. No, it's not. You mark the whole idea of spoilers when it's not you, but then, oh, no.
[00:17:03.500 --> 00:17:06.500]   Yeah. I know. I know it's bad.
[00:17:06.500 --> 00:17:14.500]   That's bad. I live in this world where, you know, I mean, geeks are the worst about, you know, spoiler alert and all that stuff.
[00:17:14.500 --> 00:17:16.500]   I live in that world. So I have to.
[00:17:16.500 --> 00:17:19.500]   I'm pretty bad about that too, though. I understand that.
[00:17:19.500 --> 00:17:26.500]   You know, I actually avoid the internet until I've watched the most recent episode of whatever show I'm really involved in.
[00:17:26.500 --> 00:17:33.500]   Because I can't even surf the net without having some sort of a spoiler or an image or a picture or CNN, which is horrible for that.
[00:17:33.500 --> 00:17:37.500]   I suddenly just stick whatever happened to ever died or was made.
[00:17:37.500 --> 00:17:40.500]   Last week on Game of Thrones. Yes.
[00:17:40.500 --> 00:17:47.500]   So I avoid the internet completely until I've watched it. So I feel a little bit of relief, either way, at least I'll be able to watch it.
[00:17:47.500 --> 00:17:52.500]   I did the same thing with Endgame. It was a really long week before we got to see it.
[00:17:52.500 --> 00:17:56.500]   Oh, yeah. I kind of feel like, yeah, I still haven't seen it, so don't tell me.
[00:17:56.500 --> 00:18:06.500]   I feel like we are particularly ill-equipped to understand what's happening to us as a society, what technology is doing to us and what we should do to fix it.
[00:18:06.500 --> 00:18:08.500]   Like we talk about it endlessly.
[00:18:08.500 --> 00:18:10.500]   Certainly on our show.
[00:18:10.500 --> 00:18:13.500]   What show you should fix about what though? Because there's so many.
[00:18:13.500 --> 00:18:17.500]   Well, maybe we can't even agree on what's broken. Yeah. All of it.
[00:18:17.500 --> 00:18:18.500]   Right.
[00:18:18.500 --> 00:18:19.500]   All of it.
[00:18:19.500 --> 00:18:25.500]   So the latest thing, and you're seeing it everywhere, is I was my daughter's college graduation was yesterday and Joanna Hoffman,
[00:18:25.500 --> 00:18:35.500]   was the honorary doctorate and speaker. She was, if you saw that terrible Steve Jobs movie, she was the one with the Russian accent who kept yelling at Steve.
[00:18:35.500 --> 00:18:37.500]   No, you can't do that. You can't do that.
[00:18:37.500 --> 00:18:53.500]   And she was the first business plan for Macintosh. She did the marketing for Macintosh. She later went on to next and joined General Magic and really was instrumental with General Magic and creating the template that became the smartphone later with the iPhone.
[00:18:53.500 --> 00:19:00.500]   So she was very much involved in technology and she was warning the graduates, don't let social media tell you what you're supposed to be.
[00:19:00.500 --> 00:19:10.500]   You know, she, and this is Chris Hughes, the Mark Zuckerberg's roommate at Harvard, one of the founders of Facebook published that op ed last week in the New York Times saying Facebook should be broken up.
[00:19:10.500 --> 00:19:12.500]   Mark Zuckerberg has too much power.
[00:19:12.500 --> 00:19:21.500]   This is the new thing in Silicon Valley is to make your millions or billions and then say it was a terrible idea. You shouldn't do it.
[00:19:21.500 --> 00:19:26.500]   But after, you know, buy it now, but it seems to be the thing, right?
[00:19:26.500 --> 00:19:27.500]   Yeah.
[00:19:27.500 --> 00:19:29.500]   Technology's bad for you. I don't want to.
[00:19:29.500 --> 00:19:33.500]   I mean, mainly from the Facebook people though, you know, Chamath and even.
[00:19:33.500 --> 00:19:36.500]   Oh, yeah. Actin, Brian Actin of WhatsApp.
[00:19:36.500 --> 00:19:45.500]   Yeah. Well, all of the people that were bought, but I'm talking about like even Sean Parker, not Sean Parker, right?
[00:19:45.500 --> 00:19:45.500]   Yeah, yeah, yeah.
[00:19:45.500 --> 00:19:48.500]   He said, yeah, I shouldn't. It was terrible. Don't do it.
[00:19:48.500 --> 00:19:53.500]   So, I mean, I don't know. I don't know what the virtue.
[00:19:53.500 --> 00:20:02.500]   I'm not sure what the virtue signaling like that seems so obvious to the Facebook people that never seems to occur to the Google people or to anyone else.
[00:20:02.500 --> 00:20:04.500]   That's a good point. It is Facebook, isn't it? Yeah.
[00:20:04.500 --> 00:20:05.500]   Yeah.
[00:20:05.500 --> 00:20:12.500]   Well, but it's reflected in the products from companies like Google though to an Apple, right? You see these. I mean, what's what was the new hot thing all of last year?
[00:20:12.500 --> 00:20:19.500]   It was like, oh, well, new iPhone feature, new Android, you know, Android 9 feature, digital wellbeing or whatever Apple calls it.
[00:20:19.500 --> 00:20:25.500]   It's like, this will tell you not only how often you use your phone, but it will encourage you to not use your phone.
[00:20:25.500 --> 00:20:32.500]   You've companies like OnePlus putting out Zen mode. We have all these companies building in features to stop you from overusing their own products.
[00:20:32.500 --> 00:20:41.500]   And I think there is an argument to be made that, well, how much of that is Leo as you said, like virtue signaling and how much is we genuinely are worried about what's happening.
[00:20:41.500 --> 00:20:46.500]   What these products are doing to you? Actually credit to Brian for using that phrase because it's. Oh, excuse me. I'm sorry.
[00:20:46.500 --> 00:20:55.500]   Very apt phrase for it's just signaling. I mean, I don't I bring it up because, you know, you can you can always get hits talking about the game of thrones.
[00:20:55.500 --> 00:21:06.500]   That's the only reason I care. Right. Actually, I read a great piece. Do I have a link to it in the rundown today?
[00:21:06.500 --> 00:21:12.500]   I'm Carson. I can't remember about why I still love technology and it made me really happy.
[00:21:12.500 --> 00:21:18.500]   Yeah, Paul Ford. Anytime I say on my show, if Paul Ford writes a thing, you read it.
[00:21:18.500 --> 00:21:35.500]   He he's a, you know, serial founder, smart guy. And he kind of made me feel better because he's not quite as old as I am, but he he goes back to the, you know, the old days, you know, typing in, you know,
[00:21:35.500 --> 00:21:49.500]   typing in listings to your fingers, bleed from compute magazine and stuff like that. And I thought his piece was refreshing because it's with all this, as you say, virtue signaling going on.
[00:21:49.500 --> 00:22:01.500]   It is good to remind ourselves that the technology is amazing. It's the cover story of this month's wire. The technology is incredible. What a world we live in.
[00:22:01.500 --> 00:22:10.500]   And how lucky we are while why I still love technology and defensive, a difficult industry. And he's talking about all the unicorns.
[00:22:10.500 --> 00:22:22.500]   You know, can I ask you something, Leo, because especially, you know, I wrote the book on the history of the internet last year and stuff like that. So I constantly have myself. I'm trying to check myself.
[00:22:22.500 --> 00:22:38.500]   Like, is it me getting old? Everyone is the same thing. And you're so young, compared to me. I know, but I legitimately am getting old. But Paul's maybe five years older than me. So like, it's, we're all sort of on the other end of this.
[00:22:38.500 --> 00:22:49.500]   Is this my get off get off my lawn? You kids moment. Yeah. Now, on the one hand, is it when I think what I always say is I don't, I don't, I could have written that essay.
[00:22:49.500 --> 00:23:08.500]   Like, I still love tech. Yes. I just want it to be better. Like, I thought it was. Now, did I think tech was amazing because I was 21 and I thought everything was possible. And now I'm jaded and 41 and, and the vistas are not as vast as they once were like, everyone gets old. Thanks.
[00:23:08.500 --> 00:23:19.500]   The music today isn't as good as the music. Yeah. Yeah. But what if also legitimately it was better. I tell. I've always tried to check myself on that.
[00:23:19.500 --> 00:23:37.500]   Yeah. Yeah. Or like, as a therapist, I do see how many kids are, you know, have like the amount of social anxiety that is kind of skyrocketing because kids aren't learning how to interrelate with other human beings and facial features and missing out on that and how strong of an addiction technology is, which is a very important thing.
[00:23:37.500 --> 00:23:49.500]   And technology is, which was it as strong as that for music or for television? I don't think so. But again, is that just my own bias because of where I am now compared to before?
[00:23:49.500 --> 00:23:57.500]   How young do you have to be before you say, Oh, I'm not, I'm not old and ossified. That really is a problem. Come on. You're pretty young, Georgia.
[00:23:57.500 --> 00:24:09.500]   And I would say that every single teenage person that I speak to and I ask them, are you addicted to your tech? And do you use it too much? Now, this is not when their parents are in the room because they're there and so and take away their technology.
[00:24:09.500 --> 00:24:19.500]   But every single one says, yes, like I've never come up to anyone. Like, I think that with music and with television, people say, no, it's not an addiction. I still go outside. I still do things.
[00:24:19.500 --> 00:24:29.500]   I have people that don't do anything else. This is their avoidance technique. They lose themselves in technology. And I love tech. I love tech like very strongly.
[00:24:29.500 --> 00:24:40.500]   But I don't have my kids freely just on technology because I know how addictive it can be. And I see the aftermath of what happens as people get lost in technology.
[00:24:40.500 --> 00:24:54.500]   How many women and men, young men, do I see that? See all of these wonderful, beautiful people on Instagram, which is totally a lie. And they feel less about themselves that they're, you know, foam on not doing enough because they see all of this.
[00:24:54.500 --> 00:25:06.500]   And it's all this fake facade that they buy into because maybe they're just mirroring their elders discomfort. Maybe they don't. Do they? Do they genuinely feel that?
[00:25:06.500 --> 00:25:13.500]   You know what? I think the opposite. I think what George is just saying is that a 21 year old wouldn't have written Paul's essay.
[00:25:13.500 --> 00:25:21.500]   Like, I've got to say he and we can look back and see some amazing things, but they don't have that.
[00:25:21.500 --> 00:25:32.500]   Well, I would say that they probably love it for certain pieces. The fact that they can talk to their, you know, favorite stars and information is for anyone.
[00:25:32.500 --> 00:25:40.500]   And you can freely express your opinion and you can find out things. I mean, there's a lot of good to it. But I think that we're not taught media literacy.
[00:25:40.500 --> 00:25:50.500]   They don't know about these things. And if you listen to anything long enough, anything, whatever it is, advertising works because we become complicit to it.
[00:25:50.500 --> 00:25:59.500]   If you hear that Nike is better than say a Walmart brand, we will believe that as true, even if it has no factual basis to that.
[00:25:59.500 --> 00:26:07.500]   And I think that that's the problem with Instagram and a lot of social media that's out there is that there's this false sense of, you know,
[00:26:07.500 --> 00:26:14.500]   expectation debt, we are not enough. We need to do this. If you're not doing this, you're not good enough.
[00:26:14.500 --> 00:26:23.500]   He talks about Paul talks about the Raspberry Pi. He talks about going to the library next to his kid's school in New York.
[00:26:23.500 --> 00:26:30.500]   And there's a guy in there and he's setting up and he says, what you doing? He says, I'm setting up. It's a birthday party. For what? For the Raspberry Pi.
[00:26:30.500 --> 00:26:40.500]   And he's thinking, this is great. See, there's still lots of, this is amazing, a $35 supercomputer. I mean, that's something to celebrate.
[00:26:40.500 --> 00:26:48.500]   And then I read Taylor Lawrence's article in the Atlantic about something I don't even know about. Do you know about T spill accounts?
[00:26:48.500 --> 00:26:57.500]   Oh, yeah. Well, I know that yes, I followed this. I should I should not be admitting to this explain what is T and what is T spilling.
[00:26:57.500 --> 00:27:12.500]   So just spilling the tea is like giving the dirt of what has happened. And now people are talking about the dirt that's happened and kind of following the aftermath of this spilling of all of the drama of what we did not know was happening
[00:27:12.500 --> 00:27:15.500]   behind on these two very popular.
[00:27:15.500 --> 00:27:29.500]   Well, but in fact, the T spill accounts are among the most popular Instagram and YouTube accounts of all like you. It's like TMZ for the millennial generation and and it's so popular.
[00:27:29.500 --> 00:27:42.500]   But really, I feel like it's kind of negative too because it's all about, you know, it's what like Logan Paul and his crew did with a faked feuds and all this stuff and they gin up views.
[00:27:42.500 --> 00:27:54.500]   Right. And because people will show up for conflict, right? People will show up for controversy. Like this is the one of its caters to our bassist instincts and YouTube has been making as a platform has been making money off of it for a long time.
[00:27:54.500 --> 00:28:06.500]   And so a bunch of creators. And I mean, I think it's legitimate to do that in this obviously in the context of a television show or a film or even a legitimate enterprise on, you know, on a streaming media platform.
[00:28:06.500 --> 00:28:12.500]   But when you're ginning up fake conflict, just so that you can get. I mean, it's just not good.
[00:28:12.500 --> 00:28:16.500]   With that then be okay. No, it's just because it's trivial.
[00:28:16.500 --> 00:28:18.500]   I think it's gross.
[00:28:18.500 --> 00:28:22.500]   That's a good one with freak in Iran and you're worried about.
[00:28:22.500 --> 00:28:27.500]   100%. But these are the stars of today. Right.
[00:28:27.500 --> 00:28:30.500]   It's these are like the letters of today.
[00:28:30.500 --> 00:28:33.500]   This is my get off your lawn moment. Right. I mean, this is.
[00:28:33.500 --> 00:28:40.500]   That's a good one for it. No, I'm just saying this is. That's what this is because I look at this and I know you kids are messed up.
[00:28:40.500 --> 00:28:52.500]   But they care. These are the people that they've allowed in their lives. So it's like the old newspaper articles about, you know, at least when I watch TMZ, I want to take a shower afterwards.
[00:28:52.500 --> 00:28:54.500]   But do you watch TMZ?
[00:28:54.500 --> 00:29:05.500]   No, because that's so gross. Sometimes you fall up. If you tune across it by accident, it's like a car wreck. You can't. You have to stop.
[00:29:05.500 --> 00:29:08.500]   You're going to be cast up. Unconvincing argument.
[00:29:08.500 --> 00:29:13.500]   Forensic file says on 24 hours a day on some channel, you can watch something.
[00:29:13.500 --> 00:29:22.500]   I don't want to watch that. I don't want to watch Harvey Levin and the TMZ crew spill some tea.
[00:29:22.500 --> 00:29:32.500]   It seems to me though, there's a little bit of cynicism among all of these people like Logan Paul and PewDiePie who are now engaging in this as well.
[00:29:32.500 --> 00:29:39.500]   They see it generates views and it's a culture that actually what am I saying?
[00:29:39.500 --> 00:29:50.500]   Because I just read an interview with Howard Stern in which he disses podcasts because he says these podcasters ought to be in an environment where they have to get audience.
[00:29:50.500 --> 00:29:55.500]   Then they, when they get an audience like I have, then they know.
[00:29:55.500 --> 00:30:02.500]   And I'm looking at you, Howard, and I'm saying the things you did, Howard, of course, generated an audience, but there's nothing to be proud of.
[00:30:02.500 --> 00:30:18.500]   I'm proud of the fact that we do podcasts for a small audience because we assume our audience is intelligent, always a guarantee for pad ratings and try to feed their brains, not their libidos.
[00:30:18.500 --> 00:30:22.500]   Yeah, and the nicer part about that is that sometimes it works.
[00:30:22.500 --> 00:30:26.500]   I recently had an experience where I thought I'm too late to cover this product.
[00:30:26.500 --> 00:30:30.500]   It was a Galaxy Fold video. I ran it with a stickier headline than I ordinarily would.
[00:30:30.500 --> 00:30:33.500]   It wasn't clickbait because I delivered on the question I asked.
[00:30:33.500 --> 00:30:34.500]   But it was a...
[00:30:34.500 --> 00:30:35.500]   Okay, but what was the headline?
[00:30:35.500 --> 00:30:37.500]   That's pretty buzzfeed-y headline.
[00:30:37.500 --> 00:30:40.500]   I think it was something like, did the Galaxy Fold kill foldables?
[00:30:40.500 --> 00:30:41.500]   That's good.
[00:30:41.500 --> 00:30:42.500]   I like that.
[00:30:42.500 --> 00:30:43.500]   That's good headline.
[00:30:43.500 --> 00:30:44.500]   Well, thank you.
[00:30:44.500 --> 00:30:49.500]   But it's so far afield from a normal headline that actually there was quite a bit of blowback.
[00:30:49.500 --> 00:30:53.500]   Because YouTube shows you all the analytics, you can see what the sentiment is.
[00:30:53.500 --> 00:30:59.500]   On Reddit, people were like, "You're not going to last you much longer if you keep putting headlines like that."
[00:30:59.500 --> 00:31:07.500]   So I'm like, "Okay, well, thank you. I appreciate that some people still appreciate what would today be characterized as a dull headline."
[00:31:07.500 --> 00:31:11.500]   So I think, Leo, you're right. There's intelligent people out there.
[00:31:11.500 --> 00:31:13.500]   And I know you're young because you got a faux-hawk.
[00:31:13.500 --> 00:31:14.500]   So for sure...
[00:31:14.500 --> 00:31:16.500]   Oh man, you fell for it.
[00:31:16.500 --> 00:31:18.500]   I'm actually glad it's still working out.
[00:31:18.500 --> 00:31:21.500]   No, I'm just teasing you, Michael.
[00:31:21.500 --> 00:31:22.500]   No, I just...
[00:31:22.500 --> 00:31:32.500]   I love it that Brian, you had the same thought in your early 40s that I am my early 60s of having, which is, I must just be too old.
[00:31:32.500 --> 00:31:34.500]   I'm just getting cranky.
[00:31:34.500 --> 00:31:39.500]   But it's because it is the check of...
[00:31:39.500 --> 00:31:47.500]   At some point, you have to come to terms with the fact that certain things are happening and you don't understand them, but that's because they're not for you.
[00:31:47.500 --> 00:31:49.500]   I know they're not for me.
[00:31:49.500 --> 00:31:50.500]   Right.
[00:31:50.500 --> 00:31:58.500]   But that's the point, is that sort of what we've just been talking about, like what George has been saying, is like these are not the stars that we need to care about.
[00:31:58.500 --> 00:31:59.500]   They're not for us.
[00:31:59.500 --> 00:32:00.500]   They're not for us.
[00:32:00.500 --> 00:32:01.500]   Yeah.
[00:32:01.500 --> 00:32:08.500]   And so part of it is, again, like, my dad checked out of music in 1976, right?
[00:32:08.500 --> 00:32:12.500]   And so, you know, he can appreciate rap and things like that, but it's not for him.
[00:32:12.500 --> 00:32:19.500]   It's like, so, but there's a difference between what you have to watch yourself is, well, this is no good.
[00:32:19.500 --> 00:32:26.500]   Well, it's good for somebody and maybe it's not good for you, but that's fine.
[00:32:26.500 --> 00:32:27.500]   It doesn't have to be good for you.
[00:32:27.500 --> 00:32:28.500]   Yeah.
[00:32:28.500 --> 00:32:30.500]   As long as you can understand why it is good.
[00:32:30.500 --> 00:32:39.500]   I'd like a rule of thumb so that something I can measure my cranky old manness against so that I could say, no, that's a legitimate complaint.
[00:32:39.500 --> 00:32:42.500]   No, that's just you and your fossilized.
[00:32:42.500 --> 00:32:45.500]   Georgia, you're the psychologist.
[00:32:45.500 --> 00:32:47.500]   Well, you know what?
[00:32:47.500 --> 00:32:54.500]   I think that that's why this is so important because you can eat like the wonderful thing about Twitter and many other podcasts.
[00:32:54.500 --> 00:33:02.500]   There are others. We can still have a civilized debate where we disagree with things and we are not going to attack each other personally.
[00:33:02.500 --> 00:33:04.500]   And then, you know, the pitchforks are going to come at us.
[00:33:04.500 --> 00:33:18.500]   I think that that's the one problem with social media is that we have this hair trigger effect of people become frightened of just saying one thing and then suddenly everyone's going to hate on them and they're going to like lose all of their followers and it can be really horrible.
[00:33:18.500 --> 00:33:24.500]   But I think that we need to always check ourselves, discuss things and be open-minded to the fact that maybe I'm wrong.
[00:33:24.500 --> 00:33:28.500]   And I don't think that that's often what happens.
[00:33:28.500 --> 00:33:29.500]   And so we have this debate.
[00:33:29.500 --> 00:33:30.500]   Maybe I'm wrong.
[00:33:30.500 --> 00:33:31.500]   Maybe you're right.
[00:33:31.500 --> 00:33:35.500]   We kind of discuss it and then we can come to agreement, think about it and deal with it.
[00:33:35.500 --> 00:33:43.500]   If you are able to do that, usually we'll come back to a semblance of, you know, closer to the truth.
[00:33:43.500 --> 00:33:51.500]   But Leo, I also want to come back and say, having said all that and try to be like, well, you know, I'm trying to have a governor on myself.
[00:33:51.500 --> 00:33:58.500]   What I said, I want tech to be better. There is bad stuff happening in this world that tech is the root cause of.
[00:33:58.500 --> 00:34:10.500]   There are bad products out there. And so at the same time, even, I'm not saying don't call a bad actor or a bad product or a bad thing.
[00:34:10.500 --> 00:34:18.500]   You have to call it if it's bad. So it's, what I'm saying is, is you don't have to be old man waves cane at everything just because it's new.
[00:34:18.500 --> 00:34:25.500]   But there has to be some legitimacy to being 41, 51, 61, 70 and be like, it was better.
[00:34:25.500 --> 00:34:30.500]   There were things that were supposed to change the world for the better that it turned out didn't.
[00:34:30.500 --> 00:34:31.500]   Well, that went out.
[00:34:31.500 --> 00:34:34.500]   What do you do, Brian? Like, what happens after that?
[00:34:34.500 --> 00:34:41.500]   He say, you know what, Facebook has been abusing people's privacy. Then what?
[00:34:41.500 --> 00:34:47.500]   That's all I'm arguing for is arguing for doing things better. So identifying, you know, hey,
[00:34:47.500 --> 00:35:01.500]   something that's so powerful is the thing is that when companies who are now people, at least in some parts of the world, are now larger than many governments and countries make more income than many countries.
[00:35:01.500 --> 00:35:09.500]   How do you rein that in once they have that much power and control and money and influence?
[00:35:09.500 --> 00:35:19.500]   So the argument that I would make is that I think, and this, I think this is coming true in this very moment, is that there are market incentives for doing things better as well.
[00:35:19.500 --> 00:35:27.500]   You know, the reason that Instagram is so popular, you know, it's inside Facebook. So it's a good argument.
[00:35:27.500 --> 00:35:31.500]   But it's because it's not as painful to people as Facebook became.
[00:35:31.500 --> 00:35:43.500]   And so I've made the argument before that the next generation of startups is going, there's going to be an incentive to create a product that doesn't make people feel bad.
[00:35:43.500 --> 00:35:54.500]   One of the problems that we have is for the last decade or 15 years, you had, you never, you could never create a product where your addressable market was every human being on the planet before.
[00:35:54.500 --> 00:36:03.500]   And so that led to essentially the whole move faster, break things, get big fast, but all of that low hanging fruit has been picked.
[00:36:03.500 --> 00:36:06.500]   So I think that the only way the next generation...
[00:36:06.500 --> 00:36:07.500]   Oh, my next generation.
[00:36:07.500 --> 00:36:10.500]   ...like the guy in the patent office who said everything that's going to be in FEDD has been in FEDD in the 1940s.
[00:36:10.500 --> 00:36:16.500]   Except for the fact that it's not just, oh, this is a virgin territory, plant my flag and get it.
[00:36:16.500 --> 00:36:23.500]   You're going, the only way you're going to be able to get the next billion users, the five billion users or whatever, is to not just be the first with the product that scales.
[00:36:23.500 --> 00:36:29.500]   But the product that scales because it offers a qualitatively better experience.
[00:36:29.500 --> 00:36:30.500]   And you don't think this possible?
[00:36:30.500 --> 00:36:31.500]   No, I do.
[00:36:31.500 --> 00:36:32.500]   Oh.
[00:36:32.500 --> 00:36:33.500]   Okay.
[00:36:33.500 --> 00:36:34.500]   So that's what I'm asking.
[00:36:34.500 --> 00:36:35.500]   It's a good question.
[00:36:35.500 --> 00:36:40.500]   Can somebody come along and best Google or Facebook after the network effect is kicked in?
[00:36:40.500 --> 00:36:43.500]   Is it possible to come along and make a better search engine or a better social network?
[00:36:43.500 --> 00:36:49.500]   I'm saying that it's going to be harder for the next generation of startups to achieve the same level of scale.
[00:36:49.500 --> 00:36:56.500]   But guess what? Whoever does is probably going to do it by making a better product, not just by being the first, not just by being the one to pick the low hanging food.
[00:36:56.500 --> 00:37:00.500]   I would submit it's even more difficult, but more, it's going to be orthogonal.
[00:37:00.500 --> 00:37:05.500]   It's going to be, it's not going to be the, it's the wrong question to say who's going to make a better search engine than Google.
[00:37:05.500 --> 00:37:07.500]   It's going to be what's the next thing?
[00:37:07.500 --> 00:37:11.500]   It's not going to, you know, who's going to be a better social network than Facebook?
[00:37:11.500 --> 00:37:13.500]   It's what's the thing after social networks?
[00:37:13.500 --> 00:37:16.500]   Because that happens all the time in technology.
[00:37:16.500 --> 00:37:21.500]   Those kind of, they call, I hate to use the phrase, paradigm shifts happen all the time.
[00:37:21.500 --> 00:37:27.500]   This continuity's, there's a, I would never have expected this in a million years in New Yorker.
[00:37:27.500 --> 00:37:31.500]   This week in annals of technology talks about the indie web.
[00:37:31.500 --> 00:37:36.500]   Cal Newport writes about Mastodon and microblog.
[00:37:36.500 --> 00:37:44.500]   And these are the, these are the top secret, not intentionally, but they were the top secret replacements for Twitter.
[00:37:44.500 --> 00:37:48.500]   And, you know, Facebook.
[00:37:48.500 --> 00:37:57.500]   And, and there's actually an article about it in a massive mainstream, well respected article in the New Yorker.
[00:37:57.500 --> 00:38:05.500]   I don't think it's going to change anything, but there are people trying to, I've been on both microblog and Mastodon.
[00:38:05.500 --> 00:38:09.500]   And the sad thing is, there's nobody else there.
[00:38:09.500 --> 00:38:10.500]   Right.
[00:38:10.500 --> 00:38:13.500]   I'm worried that if they get, oh, sorry.
[00:38:13.500 --> 00:38:15.500]   No, no, Michael, yes, you go ahead.
[00:38:15.500 --> 00:38:17.500]   I was just going to say my worry is that they would then become Facebook.
[00:38:17.500 --> 00:38:20.500]   If they become really powerful, do they then do the same thing?
[00:38:20.500 --> 00:38:22.500]   Well, that's why I wanted to be something that's not the same.
[00:38:22.500 --> 00:38:23.500]   Like it's got to be your thought.
[00:38:23.500 --> 00:38:25.500]   I'll, I'll, like a quick, a left turn.
[00:38:25.500 --> 00:38:26.500]   I'm sorry, go ahead, Michael.
[00:38:26.500 --> 00:38:27.500]   Right.
[00:38:27.500 --> 00:38:30.500]   And I think the potential exists for people to get behind that, for ordinary people.
[00:38:30.500 --> 00:38:35.500]   I mean, obviously seeing it in the New Yorker is evidence enough that people are, that this Facebook fatigue is, is real.
[00:38:35.500 --> 00:38:37.500]   And it's the thing that is happening just outside tech circles.
[00:38:37.500 --> 00:38:42.500]   I had a lovely dinner about a month ago with a three of my friends who I made before I got into the tech world.
[00:38:42.500 --> 00:38:43.500]   So they're theater friends.
[00:38:43.500 --> 00:38:47.500]   They are the, they're the opposite in my circles of tech people.
[00:38:47.500 --> 00:38:48.500]   Good.
[00:38:48.500 --> 00:38:49.500]   Right.
[00:38:49.500 --> 00:38:50.500]   Yeah, most of the time.
[00:38:50.500 --> 00:38:56.500]   And we, we sat down in all three of them head within the past six months left Facebook of, of their own accord.
[00:38:56.500 --> 00:39:08.500]   And that was a, that was a moment for me of, of realization that, oh, wow, this is actually hitting normal people in a way that's, you know, that's eventually will affect Facebook's numbers.
[00:39:08.500 --> 00:39:17.500]   And I do wonder what the replacement for that is because I'm not leaving without a, without a significant alternative, you know, that does things better.
[00:39:17.500 --> 00:39:20.500]   Actually, that's exactly what Cal Newport writes in this New York article.
[00:39:20.500 --> 00:39:26.500]   He says, according to Edison, 15 million people have left Facebook.
[00:39:26.500 --> 00:39:31.500]   Increasing numbers of teenagers are rejecting the ceaseless pressure for digital performance in March.
[00:39:31.500 --> 00:39:37.500]   Edison research released a report claiming that young people made up the largest share of the 15 million users Facebook.
[00:39:37.500 --> 00:39:39.500]   Has lost since 2017.
[00:39:39.500 --> 00:39:48.500]   I should point out that when you have two billion users losing 15 million in two years, you probably lost more people than that because they died.
[00:39:48.500 --> 00:39:50.500]   Right.
[00:39:50.500 --> 00:39:52.500]   So that's going to skew the numbers a little bit.
[00:39:52.500 --> 00:39:54.500]   This does have an older, yeah.
[00:39:54.500 --> 00:39:57.500]   To be 16 and offline.
[00:39:57.500 --> 00:40:03.500]   And this kind of supports what you said, Georgia has become counter cultural, like a cool thing.
[00:40:03.500 --> 00:40:06.500]   I'm offline, man.
[00:40:06.500 --> 00:40:10.500]   So the theater people are offline, but you know what's different about theater folk.
[00:40:10.500 --> 00:40:14.500]   Which many of you got.
[00:40:14.500 --> 00:40:19.500]   But they have a very strong social network, real life social network, right?
[00:40:19.500 --> 00:40:23.500]   That's, you know, they don't need it.
[00:40:23.500 --> 00:40:25.500]   That's a really good point.
[00:40:25.500 --> 00:40:32.500]   I think that what Georgia said is going to be my beacon, which is, and we try to do this with Twit.
[00:40:32.500 --> 00:40:41.500]   We try to bring as many diverse voices as possible because that's what's going to keep me from falling into that trap is people saying you're full of it.
[00:40:41.500 --> 00:40:43.500]   No, that's not bad.
[00:40:43.500 --> 00:40:45.500]   I mean, we try to get young people on.
[00:40:45.500 --> 00:40:46.500]   We try to get people of color.
[00:40:46.500 --> 00:40:51.500]   We try to get women or the ladies, as I like to call them on.
[00:40:51.500 --> 00:40:52.500]   Sorry, George.
[00:40:52.500 --> 00:40:53.500]   Just kidding.
[00:40:53.500 --> 00:40:54.500]   You're no lady.
[00:40:54.500 --> 00:41:01.500]   No, because I think that that's really important that you have to get a lot of different voices.
[00:41:01.500 --> 00:41:05.500]   And in the early days, I think Twit was a lot of people like me.
[00:41:05.500 --> 00:41:07.500]   And that wasn't probably a good thing.
[00:41:07.500 --> 00:41:11.500]   It becomes an echo chamber, right?
[00:41:11.500 --> 00:41:12.500]   Yeah.
[00:41:12.500 --> 00:41:24.500]   One of the things that replaced my theater friends when I wasn't able to do theater and more because of Twit is doing YouTube stuff was, I have this like 10 or 12 strong group of really fairly close friends who also do YouTube videos of this type.
[00:41:24.500 --> 00:41:26.500]   And that's something.
[00:41:26.500 --> 00:41:28.500]   That's a new group, right?
[00:41:28.500 --> 00:41:33.500]   That's wonderful.
[00:41:33.500 --> 00:41:39.500]   What is the percentage of people in this group who are just like me from either an economic or a social background?
[00:41:39.500 --> 00:41:41.500]   It's high.
[00:41:41.500 --> 00:41:54.500]   There's not a lot of ladies in the group, not because they're excluded, but because all of these, you have this constantly self-reinforcing like sameness to this human tribalism that it's like to sort of do work to get out of it.
[00:41:54.500 --> 00:41:59.500]   That filter bubble, and it takes a lot of work to get out of the filter bubble.
[00:41:59.500 --> 00:42:05.500]   It takes some consciousness that you're in it and an attempt to really break out of the bounds of groupthink.
[00:42:05.500 --> 00:42:06.500]   You're in Boston.
[00:42:06.500 --> 00:42:07.500]   Is that right, Michael?
[00:42:07.500 --> 00:42:08.500]   For the moment.
[00:42:08.500 --> 00:42:09.500]   Yeah.
[00:42:09.500 --> 00:42:09.500]   Yeah.
[00:42:09.500 --> 00:42:11.500]   I've been here for about 10 years.
[00:42:11.500 --> 00:42:12.500]   So I'm thinking about making a move.
[00:42:12.500 --> 00:42:23.500]   Those of us in the excerpts are not, you know, if you're a geek in a small town, you have to seek your group online as much as in person.
[00:42:23.500 --> 00:42:29.500]   I couldn't pull 12 geeks together here, except that all the people work here at geeks.
[00:42:29.500 --> 00:42:32.500]   But so maybe that's-
[00:42:32.500 --> 00:42:34.500]   But you can have different people than geeks.
[00:42:34.500 --> 00:42:35.500]   You can have-
[00:42:35.500 --> 00:42:36.500]   Oh, no, no.
[00:42:36.500 --> 00:42:37.500]   No.
[00:42:37.500 --> 00:42:38.500]   No.
[00:42:38.500 --> 00:42:39.500]   No.
[00:42:39.500 --> 00:42:40.500]   That too.
[00:42:40.500 --> 00:42:44.500]   Can I explain that I have three VR chambers in my home?
[00:42:44.500 --> 00:42:47.500]   It says, "Georgia Dow, do you get accepted?"
[00:42:47.500 --> 00:42:49.500]   And the- Do people understand?
[00:42:49.500 --> 00:42:53.500]   When you go to the book group and say, "You should join us as we fight the Fazamby apocalypse."
[00:42:53.500 --> 00:42:55.500]   You can have VR room four.
[00:42:55.500 --> 00:43:02.500]   You know, you know, I always, when there's someone that's never played, I always say you have to come by and try it and-
[00:43:02.500 --> 00:43:03.500]   They do.
[00:43:03.500 --> 00:43:04.500]   It sells really, really quick.
[00:43:04.500 --> 00:43:11.500]   So I have to say VR is pretty cool to try out at least once, even if it's not something that you're going to want to do.
[00:43:11.500 --> 00:43:13.500]   It's a neat experience.
[00:43:13.500 --> 00:43:14.500]   Oh, okay.
[00:43:14.500 --> 00:43:16.500]   I don't talk about technology.
[00:43:16.500 --> 00:43:19.500]   Like, I don't think that a lot of- Like, I don't- It's not everything.
[00:43:19.500 --> 00:43:22.500]   Like, we talk about other things, human kind of things, social stories, politics.
[00:43:22.500 --> 00:43:24.500]   There's other stuff out there.
[00:43:24.500 --> 00:43:29.500]   Everybody in technology has the experience of being at a party and somebody's saying, "What do you do?"
[00:43:29.500 --> 00:43:31.500]   And you say, "I'm in computers."
[00:43:31.500 --> 00:43:36.500]   Because that's- That's- That's all you can- If you get any deeper than that, it's over.
[00:43:36.500 --> 00:43:38.500]   Yeah, I'm in computers.
[00:43:38.500 --> 00:43:39.500]   And they go, "Oh, you're in computers.
[00:43:39.500 --> 00:43:41.500]   Hey, my Windows machine is not working."
[00:43:41.500 --> 00:43:42.500]   Yes, that's true.
[00:43:42.500 --> 00:43:43.500]   Yeah.
[00:43:43.500 --> 00:43:44.500]   Absolutely.
[00:43:44.500 --> 00:43:46.500]   Can you show me how to work this wireless printer real quick?
[00:43:46.500 --> 00:43:48.500]   Yeah, suddenly you are tech support.
[00:43:48.500 --> 00:43:49.500]   Yeah.
[00:43:49.500 --> 00:43:55.500]   And it's funny too, because it's been fun to watch it evolve over time as the platform you're on makes a big difference in today's world.
[00:43:55.500 --> 00:43:57.500]   We're talking about young people and whatever.
[00:43:57.500 --> 00:44:00.500]   I've done some A/B testing at parties where some people- You know, I was like, "Well, what do you do?"
[00:44:00.500 --> 00:44:02.500]   Well, I review consumer tech.
[00:44:02.500 --> 00:44:03.500]   And then people are like, "Oh, okay."
[00:44:03.500 --> 00:44:06.500]   But if I lead instead with, "Oh, well, I'm a YouTuber."
[00:44:06.500 --> 00:44:07.500]   Oh.
[00:44:07.500 --> 00:44:09.500]   Well, then they drop their drink and you know, that's- Oh.
[00:44:09.500 --> 00:44:11.500]   Well, they know about YouTubers.
[00:44:11.500 --> 00:44:12.500]   You're just at the evil ones.
[00:44:12.500 --> 00:44:13.500]   Right.
[00:44:13.500 --> 00:44:14.500]   Yeah.
[00:44:14.500 --> 00:44:15.500]   Right.
[00:44:15.500 --> 00:44:16.500]   Do you know PewDiePie?
[00:44:16.500 --> 00:44:17.500]   No.
[00:44:17.500 --> 00:44:20.500]   I was at a party yesterday.
[00:44:20.500 --> 00:44:25.500]   My daughter graduated from college and I was at the graduation party and met an old friend.
[00:44:25.500 --> 00:44:26.500]   I hadn't seen a long time.
[00:44:26.500 --> 00:44:29.500]   And she said, "I have to get a new iPad for my daughter.
[00:44:29.500 --> 00:44:32.500]   So I'd like to ask you some questions."
[00:44:32.500 --> 00:44:33.500]   Oh, man.
[00:44:33.500 --> 00:44:34.500]   And it turns out it wasn't an iPad.
[00:44:34.500 --> 00:44:37.500]   It was a hundred dollars Samsung and Red Taflin.
[00:44:37.500 --> 00:44:41.500]   But it took a little- It took about 15 minutes of conversation.
[00:44:41.500 --> 00:44:44.500]   Oh, that's sweet that you actually helped them.
[00:44:44.500 --> 00:44:47.500]   Well, what am I going to do?
[00:44:47.500 --> 00:44:49.500]   Call me during office hours.
[00:44:49.500 --> 00:44:50.500]   I don't really know that much.
[00:44:50.500 --> 00:44:52.500]   You can call live to the show.
[00:44:52.500 --> 00:44:53.500]   I know nothing.
[00:44:53.500 --> 00:44:54.500]   All right.
[00:44:54.500 --> 00:44:55.500]   Show today.
[00:44:55.500 --> 00:44:56.500]   This is fun.
[00:44:56.500 --> 00:44:57.500]   You guys are great.
[00:44:57.500 --> 00:44:58.500]   We're going to talk about Huawei next.
[00:44:58.500 --> 00:45:05.500]   Actually, somebody- I won't name names or show a picture, but there is somebody in the studio who works for our nation.
[00:45:05.500 --> 00:45:06.500]   What service are you in?
[00:45:06.500 --> 00:45:07.500]   I didn't even ask.
[00:45:07.500 --> 00:45:08.500]   Air Force.
[00:45:08.500 --> 00:45:11.500]   I was in cybersecurity.
[00:45:11.500 --> 00:45:15.500]   And I asked him, "Well, what about this stuff with Huawei?"
[00:45:15.500 --> 00:45:17.500]   He says, "I don't know."
[00:45:17.500 --> 00:45:18.500]   And if I did.
[00:45:18.500 --> 00:45:19.500]   He just can't tell you.
[00:45:19.500 --> 00:45:22.500]   I bet he knows.
[00:45:22.500 --> 00:45:23.500]   You think?
[00:45:23.500 --> 00:45:26.500]   Well, let's talk about it because there's big news.
[00:45:26.500 --> 00:45:29.500]   As of Friday and today, there was even breaking news about Huawei.
[00:45:29.500 --> 00:45:33.500]   Before we get into that, though, Georgia Dazz here from Imor, always a thrill.
[00:45:33.500 --> 00:45:34.500]   Thank you, Georgia.
[00:45:34.500 --> 00:45:35.500]   Yes.
[00:45:35.500 --> 00:45:37.500]   How many VR rooms do you have?
[00:45:37.500 --> 00:45:38.500]   A lot.
[00:45:38.500 --> 00:45:41.500]   You've got- I don't even want to say-
[00:45:41.500 --> 00:45:42.500]   Many.
[00:45:42.500 --> 00:45:43.500]   Just say many.
[00:45:43.500 --> 00:45:44.500]   Many.
[00:45:44.500 --> 00:45:45.500]   Oh my God.
[00:45:45.500 --> 00:45:47.500]   More than two?
[00:45:47.500 --> 00:45:49.500]   Oh my God.
[00:45:49.500 --> 00:45:50.500]   You're such a geek.
[00:45:50.500 --> 00:45:51.500]   I love it.
[00:45:51.500 --> 00:45:52.500]   There's four of us.
[00:45:52.500 --> 00:45:53.500]   There's four of us in the house.
[00:45:53.500 --> 00:45:54.500]   Oh, everybody has to have their own.
[00:45:54.500 --> 00:45:56.500]   They all- you can't fight for us.
[00:45:56.500 --> 00:45:58.500]   And are you Vive or Rift?
[00:45:58.500 --> 00:45:59.500]   I'm a Vive.
[00:45:59.500 --> 00:46:00.500]   Team Vive.
[00:46:00.500 --> 00:46:01.500]   Yeah, me too.
[00:46:01.500 --> 00:46:02.500]   Team Vive.
[00:46:02.500 --> 00:46:04.500]   I got the wireless Vive.
[00:46:04.500 --> 00:46:05.500]   So they're-
[00:46:05.500 --> 00:46:06.500]   Very nice.
[00:46:06.500 --> 00:46:07.500]   You have to go wireless.
[00:46:07.500 --> 00:46:08.500]   You can't be tethered.
[00:46:08.500 --> 00:46:09.500]   Yeah.
[00:46:09.500 --> 00:46:11.500]   Then you don't need it where you can run all over the house.
[00:46:11.500 --> 00:46:13.500]   Yeah, exactly.
[00:46:13.500 --> 00:46:15.500]   Take that, you're Zummeh!
[00:46:15.500 --> 00:46:18.500]   Brian McCull is also here at the Tech Meme Ride Home podcast.
[00:46:18.500 --> 00:46:23.500]   I must listen every single day you do a show every day you're insane.
[00:46:23.500 --> 00:46:28.500]   Uh, well, yeah, I'm going to take a Saturday off next week.
[00:46:28.500 --> 00:46:33.500]   Yeah, well, no, we do the daily 15-minute news roundup on the weekdays.
[00:46:33.500 --> 00:46:38.500]   And then now we've added the weekend bonus episodes, which are just interview episodes,
[00:46:38.500 --> 00:46:43.500]   which, you know, I did yesterday was Jay and Farhad, Farhad from New York Times and JRO
[00:46:43.500 --> 00:46:44.500]   from CNBC.
[00:46:44.500 --> 00:46:46.500]   They stopped doing that show, didn't they?
[00:46:46.500 --> 00:46:48.500]   They did, but I keep- this is the second time I've done it.
[00:46:48.500 --> 00:46:50.500]   I call it a Jay and Farhad Show reunion.
[00:46:50.500 --> 00:46:55.500]   And the reason I'm bringing this up is because the tail end of it, we just devolved into
[00:46:55.500 --> 00:46:57.500]   doing Thrones talk as well.
[00:46:57.500 --> 00:47:00.500]   It all comes down to the throne, baby.
[00:47:00.500 --> 00:47:06.500]   Also, with this Mr. Mobile 780,000 subscribers on his YouTube channel.
[00:47:06.500 --> 00:47:07.500]   Sir.
[00:47:07.500 --> 00:47:09.500]   That ain't bad, dude.
[00:47:09.500 --> 00:47:10.500]   Thank you.
[00:47:10.500 --> 00:47:11.500]   Thank you, sir.
[00:47:11.500 --> 00:47:12.500]   Can you make a living doing that?
[00:47:12.500 --> 00:47:14.500]   Is that enough to make a living?
[00:47:14.500 --> 00:47:17.500]   If you have a bunch of smart people who've helped you build the thing, or you're just a super
[00:47:17.500 --> 00:47:19.500]   genius, uh, smarter than I am, then yes.
[00:47:19.500 --> 00:47:21.500]   Unfortunately, I have one of those things.
[00:47:21.500 --> 00:47:23.500]   [laughter]
[00:47:23.500 --> 00:47:26.500]   Uh, and a recovering actor.
[00:47:26.500 --> 00:47:27.500]   Tell me about that.
[00:47:27.500 --> 00:47:28.500]   I didn't even know that.
[00:47:28.500 --> 00:47:29.500]   Yeah, I went to school for acting.
[00:47:29.500 --> 00:47:31.500]   I, uh, I got my degree in theater.
[00:47:31.500 --> 00:47:34.500]   I graduated in 2008, and I, in a case of wonderful luck.
[00:47:34.500 --> 00:47:38.500]   I had a company I was doing some voiceover work for who said, "Hey, we're moving to Boston
[00:47:38.500 --> 00:47:40.500]   from Virginia," which is where I went to school.
[00:47:40.500 --> 00:47:43.500]   Would you like to record books on tape for law students for a few years?
[00:47:43.500 --> 00:47:44.500]   Oh, no.
[00:47:44.500 --> 00:47:45.500]   And I said, "Yes, I would."
[00:47:45.500 --> 00:47:47.500]   Do you add accents and stuff?
[00:47:47.500 --> 00:47:48.500]   Uh, my favorite.
[00:47:48.500 --> 00:47:50.500]   It's a party of the first part.
[00:47:50.500 --> 00:47:52.500]   Until they told me to stop, yeah.
[00:47:52.500 --> 00:47:58.500]   But my favorite one was, um, when Mattel sued the people who did the Barbie girl so
[00:47:58.500 --> 00:48:00.500]   I was like, "You're gonna let her do that."
[00:48:00.500 --> 00:48:04.500]   The appendix to that case was the lyrics in full of, you know,
[00:48:04.500 --> 00:48:05.500]   "Come on, Barbie."
[00:48:05.500 --> 00:48:06.500]   Did you sing it?
[00:48:06.500 --> 00:48:07.500]   You called it "Dara."
[00:48:07.500 --> 00:48:09.500]   No, I just did pen it the whole way through.
[00:48:09.500 --> 00:48:10.500]   I'm a Barbie girl.
[00:48:10.500 --> 00:48:11.500]   I'm a Barbie girl.
[00:48:11.500 --> 00:48:12.500]   I'm a Barbie girl.
[00:48:12.500 --> 00:48:13.500]   It's fantastic.
[00:48:13.500 --> 00:48:14.500]   Yeah.
[00:48:14.500 --> 00:48:15.500]   Exactly.
[00:48:15.500 --> 00:48:17.500]   So that was a really fun ride.
[00:48:17.500 --> 00:48:18.500]   That's great.
[00:48:18.500 --> 00:48:19.500]   That's awesome.
[00:48:19.500 --> 00:48:20.500]   That's fun.
[00:48:20.500 --> 00:48:21.500]   You do have the voice.
[00:48:21.500 --> 00:48:22.500]   You have a great voice.
[00:48:22.500 --> 00:48:23.500]   You really, I could totally hear you doing that.
[00:48:23.500 --> 00:48:25.500]   You know, I don't have to tell you the same, Leah.
[00:48:25.500 --> 00:48:26.500]   Thank you.
[00:48:26.500 --> 00:48:27.500]   Thank you.
[00:48:27.500 --> 00:48:30.500]   I can look forward to a career reading law books.
[00:48:30.500 --> 00:48:36.500]   Now you're doing the gardens of the galaxy deeper and deeper voices with, uh, I once,
[00:48:36.500 --> 00:48:41.500]   I once went to a radio conference at the Fairmont San Francisco and I was just stunned.
[00:48:41.500 --> 00:48:42.500]   You'd go in the lobby.
[00:48:42.500 --> 00:48:43.500]   Hello.
[00:48:43.500 --> 00:48:44.500]   Where are you working these days?
[00:48:44.500 --> 00:48:45.500]   I'm working at W.P.
[00:48:45.500 --> 00:48:46.500]   Are you working?
[00:48:46.500 --> 00:48:49.500]   I'm working at W.P.
[00:48:49.500 --> 00:48:51.500]   Deeper and deeper and deeper.
[00:48:51.500 --> 00:48:54.500]   They go way down, way down.
[00:48:54.500 --> 00:48:55.500]   I can, uh, yeah.
[00:48:55.500 --> 00:48:57.500]   I know what you're talking about.
[00:48:57.500 --> 00:48:58.500]   There we go.
[00:48:58.500 --> 00:48:59.500]   Duncan radio.
[00:48:59.500 --> 00:49:00.500]   Yeah.
[00:49:00.500 --> 00:49:02.500]   Did you ever do radio?
[00:49:02.500 --> 00:49:05.500]   No, no, but as a matter of fact, I don't, I don't know if we're allowed to ask these things,
[00:49:05.500 --> 00:49:08.500]   but are you, do you do freelance voice over Leah?
[00:49:08.500 --> 00:49:13.500]   I don't because I don't like money.
[00:49:13.500 --> 00:49:17.500]   I just, more about the more problems you have here.
[00:49:17.500 --> 00:49:21.500]   Yeah, more money, more, no, I foolishly decided to be a journalist.
[00:49:21.500 --> 00:49:23.500]   This was my mistake.
[00:49:23.500 --> 00:49:29.100]   And so, um, while I do ads in the context of the shows, because I feel like, well, then
[00:49:29.100 --> 00:49:34.860]   it's in context, I don't want my voice to be hawking razor blades, you know, in, in
[00:49:34.860 --> 00:49:37.500]   Peoria and people that's like Neil Laport.
[00:49:37.500 --> 00:49:40.260]   So I've, this, that's me every day.
[00:49:40.260 --> 00:49:41.820]   Look, you, you said I'm from Boston.
[00:49:41.820 --> 00:49:42.820]   We love our Dunkin' Donuts.
[00:49:42.820 --> 00:49:44.900]   You know that they have an in-store radio loop.
[00:49:44.900 --> 00:49:45.900]   Oh my God.
[00:49:45.900 --> 00:49:49.900]   If you turn Dunkin' down, oh, I didn't turn it down.
[00:49:49.900 --> 00:49:52.500]   I go in every day and I'm like, well, that's Leah Laport right there.
[00:49:52.500 --> 00:49:54.500]   I swear to God, somebody's, somebody has a voice.
[00:49:54.500 --> 00:49:55.500]   You think I'm in Dunkin'?
[00:49:55.500 --> 00:49:56.500]   That's Dunkin' Radio.
[00:49:56.500 --> 00:49:57.500]   This Dunkin' Radio?
[00:49:57.500 --> 00:50:00.500]   Yeah, it's really, it's epic.
[00:50:00.500 --> 00:50:04.500]   It wasn't me, although now with modern, uh, real fake technology.
[00:50:04.500 --> 00:50:09.500]   In fact, I'm gonna show you a Joe Ro, I'm gonna show you a page when we come back.
[00:50:09.500 --> 00:50:11.500]   And we're gonna play a little game.
[00:50:11.500 --> 00:50:14.500]   Foujo or real Joe?
[00:50:14.500 --> 00:50:15.500]   Ooh.
[00:50:15.500 --> 00:50:16.500]   I like that.
[00:50:16.500 --> 00:50:17.500]   This is really amazing.
[00:50:17.500 --> 00:50:18.500]   I fooled me completely.
[00:50:18.500 --> 00:50:19.500]   But that's in a moment.
[00:50:19.500 --> 00:50:22.500]   Let's take a break for a commercial for more fine sponsor.
[00:50:22.500 --> 00:50:24.500]   Thousand Eyes.
[00:50:24.500 --> 00:50:25.500]   [laughs]
[00:50:25.500 --> 00:50:26.500]   [laughs]
[00:50:26.500 --> 00:50:29.500]   Someday, someday, I will decide to cash in.
[00:50:29.500 --> 00:50:32.500]   And you'll hear me in a Dunkin' Donuts near you.
[00:50:32.500 --> 00:50:37.500]   But until then, uh, we try to do ads for stuff we know about, we use.
[00:50:37.500 --> 00:50:38.500]   I know a lot about Thousand Eyes.
[00:50:38.500 --> 00:50:39.500]   I've done events for them.
[00:50:39.500 --> 00:50:40.500]   I really am impressed.
[00:50:40.500 --> 00:50:45.500]   It was created at UCLA some years ago as a graduate project to figure out
[00:50:45.500 --> 00:50:48.500]   when you go into that puffy little thing called the cloud.
[00:50:48.500 --> 00:50:50.500]   What's going on in there?
[00:50:50.500 --> 00:50:56.500]   Turns out if you have a service or a product or a app and you deliver through the internet,
[00:50:56.500 --> 00:51:02.500]   very often you know exactly what's happening from your desk into that puffy little cloud.
[00:51:02.500 --> 00:51:06.500]   You know what's happening after it gets out of the cloud, but you can't tell what's going on in the middle.
[00:51:06.500 --> 00:51:10.500]   And that means essentially you're at risk.
[00:51:10.500 --> 00:51:15.500]   With Thousand Eyes now for the first time ever, it's like somebody took a squeegee to that dark window
[00:51:15.500 --> 00:51:20.500]   and you can get an immediate, unmatched view of all the networks, all the dependencies that impact
[00:51:20.500 --> 00:51:26.500]   your users' digital experience right down to the coffee shop Wi-Fi that they're on.
[00:51:26.500 --> 00:51:28.500]   Cloud, I mean, everybody's moving to the cloud.
[00:51:28.500 --> 00:51:33.500]   And I think people get nervous because it feels like, yeah, you're gaining agility.
[00:51:33.500 --> 00:51:36.500]   There's a lot of power, but maybe you're also, it's risky.
[00:51:36.500 --> 00:51:42.500]   And most importantly, you feel like you're losing control because you're riding on networks you don't control.
[00:51:42.500 --> 00:51:47.500]   And when your cloud app or service goes down, it's hard to know when we're wrong.
[00:51:47.500 --> 00:51:54.500]   Instant visibility into the entire service delivery path from the cloud to your end user,
[00:51:54.500 --> 00:51:59.500]   including the portions you don't own or control that's Thousand Eyes.
[00:51:59.500 --> 00:52:06.500]   It's incredible, a cloud-based platform for organizations like you to help you do the cloud right.
[00:52:06.500 --> 00:52:12.500]   A massive array of vantage points spanning the entire global internet, both public and private.
[00:52:12.500 --> 00:52:19.500]   Cloud providers, the Wi-Fi in your Starbucks, old-school IT monitoring is passive.
[00:52:19.500 --> 00:52:23.500]   It's siloed. You can see what's going on in your data center.
[00:52:23.500 --> 00:52:26.500]   But again, once it's in that puffy little cloud, who knows what's going on?
[00:52:26.500 --> 00:52:30.500]   Thousand Eyes, unique path visualization technology extends beyond those boundaries.
[00:52:30.500 --> 00:52:35.500]   So you can see, understand, and improve the experience for all your apps, services, and website.
[00:52:35.500 --> 00:52:40.500]   It is an eye-opener. If you've not used it, you've got to check it out.
[00:52:40.500 --> 00:52:46.500]   Top banks use it, enterprises, SaaS companies, the world's largest and fastest growing brands,
[00:52:46.500 --> 00:52:50.500]   rely on Thousand Eyes software to do the cloud and do it right, and you should too.
[00:52:50.500 --> 00:52:52.500]   Visit ThousandEyes.com/twit.
[00:52:52.500 --> 00:52:58.500]   Now, I know you're listening to this show, so you're sophisticated, but you probably have a boss you have to impress.
[00:52:58.500 --> 00:53:02.500]   So you can get this e-book on five cloud migration challenges you shouldn't ignore.
[00:53:02.500 --> 00:53:04.500]   It's actually really good. You'll agree with everything.
[00:53:04.500 --> 00:53:10.500]   But this you can give to the boss while you say, "And boss, we need Thousand Eyes."
[00:53:10.500 --> 00:53:15.500]   If the cloud is important to you today or tomorrow, ThousandEyes.com/twit.
[00:53:15.500 --> 00:53:18.500]   And actually, they have some very good white papers on the site.
[00:53:18.500 --> 00:53:23.500]   Dig around a little bit about different cloud providers where they excel, where they don't.
[00:53:23.500 --> 00:53:29.500]   Some really surprising conclusions. They did an event that was at that was just an eye-opener.
[00:53:29.500 --> 00:53:31.500]   ThousandEyes.com/twit.
[00:53:31.500 --> 00:53:35.500]   ThousandEyes thrive in a connected world.
[00:53:35.500 --> 00:53:39.500]   We thank them so much for supporting this week in tech.
[00:53:39.500 --> 00:53:42.500]   Let me see if I can find this faux-rogan.
[00:53:42.500 --> 00:53:47.500]   This is a technique. Actually, I want to do this.
[00:53:47.500 --> 00:53:52.500]   They got Joe Rogan, legendary podcaster, MMA.
[00:53:52.500 --> 00:53:56.500]   Is he a fighter or just, I don't know, is he trained?
[00:53:56.500 --> 00:54:01.500]   Yeah, he does it as a hobby, but he's not in the ring.
[00:54:01.500 --> 00:54:03.500]   He's a commentator for the...
[00:54:03.500 --> 00:54:06.500]   And he's a hell of a funny guy.
[00:54:06.500 --> 00:54:09.500]   FakeJoeRogan.com, the place to go.
[00:54:09.500 --> 00:54:12.500]   And it's from a company called Dessa.
[00:54:12.500 --> 00:54:14.500]   Okay, we're going to play the game. We're going to see.
[00:54:14.500 --> 00:54:21.500]   Our deep learning engineers at Dessa build a model to replicate Joe Rogan's voice to showcase current AI techniques.
[00:54:21.500 --> 00:54:29.500]   All right, so we have, as you can see, eight Joe Rogan quotes, which one, pick a number from one to eight, Georgia, and I'll play that one.
[00:54:29.500 --> 00:54:30.500]   Six.
[00:54:30.500 --> 00:54:33.500]   One, two, three, four, five, six. Listen to this.
[00:54:33.500 --> 00:54:36.500]   I should have tested my sound. Let's see if it's working.
[00:54:36.500 --> 00:54:38.500]   Nope.
[00:54:38.500 --> 00:54:41.500]   Let me make sure I've got my buttons pushed here.
[00:54:41.500 --> 00:54:43.500]   Okay. Carson, you got me up?
[00:54:43.500 --> 00:54:44.500]   Discovered cows.
[00:54:44.500 --> 00:54:47.500]   All right, let's start at the beginning.
[00:54:47.500 --> 00:54:51.500]   Milk was fine for human consumption. And why did they do it in the first place?
[00:54:51.500 --> 00:54:53.500]   Let me play that again.
[00:54:53.500 --> 00:54:57.500]   You have to decide, is this real Joe or the fake AI generated Joe?
[00:54:57.500 --> 00:55:01.500]   What was the person thinking when they discovered cows? Milk was fine for human consumption.
[00:55:01.500 --> 00:55:03.500]   And why did they do it in the first place?
[00:55:03.500 --> 00:55:05.500]   Fake or real, Georgia?
[00:55:05.500 --> 00:55:06.500]   Fake.
[00:55:06.500 --> 00:55:07.500]   Fake.
[00:55:07.500 --> 00:55:08.500]   Fake.
[00:55:08.500 --> 00:55:11.500]   It sounds fake to me too, but I've heard him say something like that about it.
[00:55:11.500 --> 00:55:12.500]   Who is the first person to try a food?
[00:55:12.500 --> 00:55:13.500]   Yeah, yeah.
[00:55:13.500 --> 00:55:17.500]   Faux Rogan, I'm going to click that. Let's see if you're right. You're correct.
[00:55:17.500 --> 00:55:19.500]   That was faux. Pick another number, Joe.
[00:55:19.500 --> 00:55:21.500]   I mean, Joe, who's Joe, Georgia?
[00:55:21.500 --> 00:55:24.500]   Is that your real name, Georgia? Georgia Joe?
[00:55:24.500 --> 00:55:26.500]   I knew it's something, right?
[00:55:26.500 --> 00:55:27.500]   Georgia Joe, pick another one.
[00:55:27.500 --> 00:55:28.500]   Let's do number one.
[00:55:28.500 --> 00:55:30.500]   Number one, fake or real?
[00:55:30.500 --> 00:55:32.500]   Yeah, it's goofy. The world's goofy.
[00:55:32.500 --> 00:55:35.500]   There's a lot of goofy shit out there, but just go with it.
[00:55:35.500 --> 00:55:41.500]   It keeps sounding like he's reading off of something.
[00:55:41.500 --> 00:55:42.500]   I'm going to say fake.
[00:55:42.500 --> 00:55:43.500]   Exactly.
[00:55:43.500 --> 00:55:44.500]   Yeah, I'm going to go with fake too.
[00:55:44.500 --> 00:55:48.500]   Fake, so two fakes in a reel. Let's click the button where I'm going to say fake.
[00:55:48.500 --> 00:55:49.500]   It is fake.
[00:55:49.500 --> 00:55:50.500]   Yeah.
[00:55:50.500 --> 00:55:51.500]   Faux Rogan.
[00:55:51.500 --> 00:55:55.500]   Okay, let's try number two. I don't know the answers to these.
[00:55:55.500 --> 00:55:58.500]   Some of you just need to improve the quality of your existence on earth.
[00:55:58.500 --> 00:56:00.500]   You got to do the right things.
[00:56:00.500 --> 00:56:01.500]   Fake or real?
[00:56:01.500 --> 00:56:03.500]   Chatroom, we're going to let you chatroom vote.
[00:56:03.500 --> 00:56:08.500]   Chatroom, number two, fake or real. I'll play it for you one more time.
[00:56:08.500 --> 00:56:11.500]   Some of you just need to improve the quality of your existence on earth.
[00:56:11.500 --> 00:56:13.500]   You got to do the right things.
[00:56:13.500 --> 00:56:15.500]   Faux Row, they say. They're all fake.
[00:56:15.500 --> 00:56:17.500]   One person says real.
[00:56:17.500 --> 00:56:21.500]   Burntex says real. Let's say it's real.
[00:56:21.500 --> 00:56:23.500]   Nope, that's Faux. That was a jet.
[00:56:23.500 --> 00:56:26.500]   Okay, now that's now you're going to be a little puzzled because listen to that.
[00:56:26.500 --> 00:56:29.500]   Some of you just need to improve the quality of your existence on earth.
[00:56:29.500 --> 00:56:30.500]   You got to do the right things.
[00:56:30.500 --> 00:56:31.500]   That sounds modulated though.
[00:56:31.500 --> 00:56:32.500]   You think there's a give?
[00:56:32.500 --> 00:56:33.500]   There's a tell?
[00:56:33.500 --> 00:56:34.500]   There's a tell.
[00:56:34.500 --> 00:56:35.500]   All right.
[00:56:35.500 --> 00:56:36.500]   Yeah.
[00:56:36.500 --> 00:56:38.500]   And also all the words are being treated as separate things that are stronger.
[00:56:38.500 --> 00:56:40.500]   You can almost hear whether they're you know tied together.
[00:56:40.500 --> 00:56:41.500]   All right.
[00:56:41.500 --> 00:56:43.500]   I'm going to give you a little bridging sounds.
[00:56:43.500 --> 00:56:46.500]   If you've got a theory now, you got a theory.
[00:56:46.500 --> 00:56:48.500]   Georgia from the remaining samples.
[00:56:48.500 --> 00:56:49.500]   One, two, let's see.
[00:56:49.500 --> 00:56:53.500]   We've got three, four, five, seven and eight. Which one?
[00:56:53.500 --> 00:56:54.500]   Let's do seven.
[00:56:54.500 --> 00:57:01.500]   You are much less likely to injure yourself if you do it correctly.
[00:57:01.500 --> 00:57:04.500]   I'm going to say real on that one.
[00:57:04.500 --> 00:57:06.500]   That sounds real right because he's pausing.
[00:57:06.500 --> 00:57:07.500]   There's inflection.
[00:57:07.500 --> 00:57:08.500]   All right.
[00:57:08.500 --> 00:57:09.500]   We all agree.
[00:57:09.500 --> 00:57:10.500]   We're all broken.
[00:57:10.500 --> 00:57:11.500]   Yeah.
[00:57:11.500 --> 00:57:12.500]   Yeah.
[00:57:12.500 --> 00:57:13.500]   So I think maybe we figured it out.
[00:57:13.500 --> 00:57:15.500]   But it's still really good.
[00:57:15.500 --> 00:57:16.500]   It's good.
[00:57:16.500 --> 00:57:20.500]   These are these things are when they stick the face over someone else.
[00:57:20.500 --> 00:57:21.500]   They're really scared.
[00:57:21.500 --> 00:57:22.500]   It's really scary.
[00:57:22.500 --> 00:57:23.500]   Yeah.
[00:57:23.500 --> 00:57:27.500]   Because I would believe that like if someone showed a video of me saying some outlandish
[00:57:27.500 --> 00:57:28.500]   thing.
[00:57:28.500 --> 00:57:31.500]   I kind of gave it away by saying some are real some or not.
[00:57:31.500 --> 00:57:35.500]   I don't think if you were listening and you didn't have that ahead of time information
[00:57:35.500 --> 00:57:38.500]   ahead of time, you might just you probably would just.
[00:57:38.500 --> 00:57:39.500]   So there is.
[00:57:39.500 --> 00:57:44.500]   And wouldn't cognitive dissonance just kind of take over if you want to believe that
[00:57:44.500 --> 00:57:45.500]   this person said this?
[00:57:45.500 --> 00:57:46.500]   Look to me.
[00:57:46.500 --> 00:57:48.500]   You would just go with whatever you wanted.
[00:57:48.500 --> 00:57:50.500]   So there's a cadessa has a.
[00:57:50.500 --> 00:57:56.500]   A couple of medium articles which I can't read because I stopped paying medium money.
[00:57:56.500 --> 00:57:58.060]   Actually, here's a weird thing.
[00:57:58.060 --> 00:58:02.500]   I am paying medium money, but I can't figure out which account.
[00:58:02.500 --> 00:58:04.500]   So I can't read this.
[00:58:04.500 --> 00:58:07.260]   I think I give too many computers.
[00:58:07.260 --> 00:58:10.660]   I tried every possible account.
[00:58:10.660 --> 00:58:15.220]   I see the charge, but I can't read any more medium.
[00:58:15.220 --> 00:58:16.940]   I've used that.
[00:58:16.940 --> 00:58:19.060]   There's a great article by.
[00:58:19.060 --> 00:58:20.060]   There's a great article.
[00:58:20.060 --> 00:58:24.500]   In fact, I think it's an hour and down and why nobody should ever read anything on medium
[00:58:24.500 --> 00:58:25.500]   again.
[00:58:25.500 --> 00:58:29.860]   Anyway, there will be a dessa article.
[00:58:29.860 --> 00:58:35.140]   If you haven't used up your free ones or you do pay medium five bucks a month, can we
[00:58:35.140 --> 00:58:36.700]   all this is web distortion?
[00:58:36.700 --> 00:58:40.260]   Can we all please stop using medium now?
[00:58:40.260 --> 00:58:47.580]   My friend Jeff Jarvis who has a blog, he's on this twig every week, keeps posting articles
[00:58:47.580 --> 00:58:48.580]   on medium.
[00:58:48.580 --> 00:58:52.060]   I say, why Jeff?
[00:58:52.060 --> 00:58:53.300]   And sometimes it's so silly.
[00:58:53.300 --> 00:58:56.740]   It's like, well, I really like the content management system.
[00:58:56.740 --> 00:58:58.580]   It's really pretty.
[00:58:58.580 --> 00:59:05.580]   But medium is really F Williams attempts to take the free and open internet blogs and
[00:59:05.580 --> 00:59:07.180]   silo them.
[00:59:07.180 --> 00:59:09.540]   So I kind of agree with this posting.
[00:59:09.540 --> 00:59:10.540]   That's true.
[00:59:10.540 --> 00:59:13.580]   But it means that that gave us live journal back in the day, right?
[00:59:13.580 --> 00:59:14.740]   And whatever it's become now.
[00:59:14.740 --> 00:59:19.980]   I mean, I know I I made some of my first internet exclusively internet friends in that live
[00:59:19.980 --> 00:59:23.980]   journal community, you know, and I wouldn't have done that if I'd had to navigate all
[00:59:23.980 --> 00:59:26.540]   these disparate blogs spread across different properties.
[00:59:26.540 --> 00:59:28.020]   Yeah, but live journal was a blogging.
[00:59:28.020 --> 00:59:32.620]   I mean, you had a public blog, but live journal didn't charge people to read your blog, right?
[00:59:32.620 --> 00:59:37.940]   No, but they started eventually charging the people to write them or to have as many features
[00:59:37.940 --> 00:59:40.140]   and stuff, you know, they had to monetize somehow, but you're right.
[00:59:40.140 --> 00:59:41.140]   Yeah.
[00:59:41.140 --> 00:59:43.340]   Normal people could read that.
[00:59:43.340 --> 00:59:52.260]   He also says in web distortion, there are dark patterns, which is the new phrase on the
[00:59:52.260 --> 00:59:54.620]   internet to talk about something awful.
[00:59:54.620 --> 00:59:57.580]   It's dark patterns medium.
[00:59:57.580 --> 01:00:02.220]   You've read four stories this month, sign in to enjoy the full experience.
[01:00:02.220 --> 01:00:03.220]   Yeah.
[01:00:03.220 --> 01:00:05.780]   Anyway, that's that was a site.
[01:00:05.780 --> 01:00:10.580]   I think some people would rather would rather pay and not have a whole bunch of ads all
[01:00:10.580 --> 01:00:12.460]   over that are blaring at them.
[01:00:12.460 --> 01:00:13.460]   That's fair.
[01:00:13.460 --> 01:00:14.700]   Either way, you're paying something.
[01:00:14.700 --> 01:00:16.140]   Yeah, no, I can't that's true.
[01:00:16.140 --> 01:00:17.380]   I can't dispute that.
[01:00:17.380 --> 01:00:18.380]   I just like blogs.
[01:00:18.380 --> 01:00:21.980]   I like the open free internet where you can just browse around and read.
[01:00:21.980 --> 01:00:24.860]   Paywalls are starting to really impinge on my ability to read.
[01:00:24.860 --> 01:00:25.940]   And I do pay like wired.
[01:00:25.940 --> 01:00:26.940]   I pay.
[01:00:26.940 --> 01:00:31.540]   But I'm not going to give Bloomberg with their vast resources.
[01:00:31.540 --> 01:00:32.540]   30, what is it?
[01:00:32.540 --> 01:00:34.540]   35 bucks a month to read Bloomberg?
[01:00:34.540 --> 01:00:36.500]   Is it that high?
[01:00:36.500 --> 01:00:38.740]   It's crazy.
[01:00:38.740 --> 01:00:41.740]   Does tech meme, tech mean does not charge?
[01:00:41.740 --> 01:00:42.740]   They have ads.
[01:00:42.740 --> 01:00:46.140]   Very judicious, simple, not intrusive ads.
[01:00:46.140 --> 01:00:48.500]   There's an ad on there right now that I reckon that's.
[01:00:48.500 --> 01:00:50.620]   I really like the ads on.
[01:00:50.620 --> 01:00:54.380]   I really like the ads on tech meme.
[01:00:54.380 --> 01:00:56.180]   I don't know what it is about those ads.
[01:00:56.180 --> 01:01:01.140]   They're just very unobtrusive and pretty.
[01:01:01.140 --> 01:01:05.740]   But often the tech meme, see one of the things I like about tech meme, sometimes the links
[01:01:05.740 --> 01:01:08.540]   will be to a paywall.
[01:01:08.540 --> 01:01:12.980]   But even if they're not, there's always a more where you can click some other source
[01:01:12.980 --> 01:01:13.980]   for it.
[01:01:13.980 --> 01:01:17.260]   Every time I see financial times, I go, oh crap.
[01:01:17.260 --> 01:01:22.460]   And then the editors, because I can see the Slack channel and the backend and stuff.
[01:01:22.460 --> 01:01:27.740]   So the editors, if the big story is a paywall, they will try to find somebody, the link
[01:01:27.740 --> 01:01:29.060]   to the aggregator.
[01:01:29.060 --> 01:01:33.780]   And so then the original reporter, which is the journalist, of course, I understand this.
[01:01:33.780 --> 01:01:36.020]   You're not reporting to my thing.
[01:01:36.020 --> 01:01:39.420]   And then the argument would be, well, because no one can read your thing.
[01:01:39.420 --> 01:01:42.180]   And our job is to just help you what the news is.
[01:01:42.180 --> 01:01:45.860]   Thank you, Gabe Rivera and company.
[01:01:45.860 --> 01:01:47.460]   He's still at the helm though, right?
[01:01:47.460 --> 01:01:48.620]   Yeah, indeed.
[01:01:48.620 --> 01:01:49.620]   Just moved to Brooklyn actually.
[01:01:49.620 --> 01:01:50.620]   Love, Gabe.
[01:01:50.620 --> 01:01:51.620]   Oh, really?
[01:01:51.620 --> 01:01:55.860]   The two can hang out at the food truck thing.
[01:01:55.860 --> 01:01:56.860]   Indeed.
[01:01:56.860 --> 01:01:57.860]   The bird.
[01:01:57.860 --> 01:01:58.860]   What is it?
[01:01:58.860 --> 01:02:02.740]   Sporgus Berg.
[01:02:02.740 --> 01:02:04.540]   How many trucks?
[01:02:04.540 --> 01:02:08.060]   I swear there was at least 30 probably closer to 40.
[01:02:08.060 --> 01:02:09.060]   That's the nice thing.
[01:02:09.060 --> 01:02:10.060]   We even have it here in Petaluma.
[01:02:10.060 --> 01:02:11.060]   We're like, it's food truck night.
[01:02:11.060 --> 01:02:13.580]   And there's like all these food trucks.
[01:02:13.580 --> 01:02:15.620]   You have really an unlimited places.
[01:02:15.620 --> 01:02:18.100]   You can get a food poisoning from.
[01:02:18.100 --> 01:02:19.100]   So it's great.
[01:02:19.100 --> 01:02:21.100]   It's really good.
[01:02:21.100 --> 01:02:22.100]   So trucks aren't dangerous.
[01:02:22.100 --> 01:02:23.100]   That went dark.
[01:02:23.100 --> 01:02:24.100]   Got dark real quick.
[01:02:24.100 --> 01:02:25.900]   Well, I always feel like I'm going to buy food made in a truck.
[01:02:25.900 --> 01:02:27.740]   Is that a good idea?
[01:02:27.740 --> 01:02:29.860]   You haven't seen the back of some restaurants.
[01:02:29.860 --> 01:02:32.900]   Well, yeah, you're right.
[01:02:32.900 --> 01:02:34.580]   This restaurant gives it a chair.
[01:02:34.580 --> 01:02:35.580]   That's my thing.
[01:02:35.580 --> 01:02:36.580]   I like to be comfortable.
[01:02:36.580 --> 01:02:39.580]   I mean, you know, I'm sitting on sidewalk.
[01:02:39.580 --> 01:02:40.580]   I'm with you on that.
[01:02:40.580 --> 01:02:41.580]   I'm with you.
[01:02:41.580 --> 01:02:43.980]   I'm like, oh, I just like to sit down while I eat.
[01:02:43.980 --> 01:02:46.100]   I like to tell you.
[01:02:46.100 --> 01:02:49.740]   So Huawei.
[01:02:49.740 --> 01:02:53.380]   Whereas many people caught hoo-hoo-hoo-hoo-hoo-hoo-hoo.
[01:02:53.380 --> 01:02:57.420]   I always love it when we get when I hear on, you know, people.
[01:02:57.420 --> 01:03:00.220]   This Chinese company, hoo-hoo-hoo-hoo-hoo.
[01:03:00.220 --> 01:03:08.740]   Huawei, the president has signed an executive order declaring a national emergency.
[01:03:08.740 --> 01:03:11.140]   He likes to do that.
[01:03:11.140 --> 01:03:15.380]   And I don't know why it's a national emergency, but suddenly it is prohibiting US companies
[01:03:15.380 --> 01:03:24.140]   from using telecom services that are solely owned, controlled, directed by a foreign adversary,
[01:03:24.140 --> 01:03:28.300]   which includes Huawei.
[01:03:28.300 --> 01:03:32.340]   Huawei's been in the spotlight since 2012, I think, when the Department of Commerce said,
[01:03:32.340 --> 01:03:35.540]   be careful with Huawei's ETE and Xiaomi.
[01:03:35.540 --> 01:03:36.860]   They're all Chinese companies.
[01:03:36.860 --> 01:03:38.580]   People have pointed to the ownership of Huawei.
[01:03:38.580 --> 01:03:41.340]   It seems no one can really be sure who owns it.
[01:03:41.340 --> 01:03:44.860]   Huawei says we're owned by our employees.
[01:03:44.860 --> 01:03:47.460]   But without much proof and certainly not.
[01:03:47.460 --> 01:03:54.100]   Well, also, in Huawei's case, the founder was in the People's Liberation Army.
[01:03:54.100 --> 01:04:00.940]   So the implication is that he still has ties to the PLA.
[01:04:00.940 --> 01:04:03.260]   And I think also that's part of the story too.
[01:04:03.260 --> 01:04:08.340]   So, okay, go ahead and finish the intro and then I'll jump in.
[01:04:08.340 --> 01:04:09.340]   No, no, no.
[01:04:09.340 --> 01:04:10.340]   I mean, I think that's enough.
[01:04:10.340 --> 01:04:12.780]   I think people, we've been talking about this for a while.
[01:04:12.780 --> 01:04:17.100]   My position on this is, I'm a little on the fence because it does seem like it's part
[01:04:17.100 --> 01:04:23.260]   of a political, you know, it's part of the Chinese tariffs, kind of the political tenor.
[01:04:23.260 --> 01:04:24.260]   So...
[01:04:24.260 --> 01:04:26.140]   And we've never seen evidence that Huawei is doing anything.
[01:04:26.140 --> 01:04:27.940]   Huawei says we've never done anything.
[01:04:27.940 --> 01:04:28.940]   Show us those.
[01:04:28.940 --> 01:04:29.940]   That's exactly what I was going to say.
[01:04:29.940 --> 01:04:34.260]   So, you know, I mentioned the, or you mentioned that we're doing the weekend episodes now.
[01:04:34.260 --> 01:04:40.380]   And so last weekend I had Dexter Tillion on who's a telecoms analyst, I think, in London.
[01:04:40.380 --> 01:04:41.780]   Because I've been reading these headlines.
[01:04:41.780 --> 01:04:43.220]   I'm like, so what is this Huawei?
[01:04:43.220 --> 01:04:44.300]   I'd never heard of them.
[01:04:44.300 --> 01:04:47.620]   And now all of a sudden, you know, there's that huge tray war that they're at the center
[01:04:47.620 --> 01:04:48.620]   of.
[01:04:48.620 --> 01:04:54.140]   And the three things that I learned were, A, like you just said, one of the reasons that
[01:04:54.140 --> 01:05:01.220]   other countries are not willing to ban Huawei is because, and even the British and the Germans
[01:05:01.220 --> 01:05:03.460]   have said this apparently straight up.
[01:05:03.460 --> 01:05:10.340]   Like, if there is intelligence that Huawei is spying and is too close to the Chinese government,
[01:05:10.340 --> 01:05:13.620]   the United States government has not shown us that.
[01:05:13.620 --> 01:05:18.500]   Like maybe we'd be willing to get on board with this banning, but we haven't seen the
[01:05:18.500 --> 01:05:19.500]   proof.
[01:05:19.500 --> 01:05:24.660]   And again, I'm not an expert in this and Dexter is more than I am.
[01:05:24.660 --> 01:05:28.060]   And he said, yes, as far as I know, there is no, there is no smoking gun.
[01:05:28.060 --> 01:05:34.180]   Now that doesn't mean if there was some sort of a conflict or a war, maybe China wouldn't
[01:05:34.180 --> 01:05:37.780]   lean on Huawei and be like, you know, maybe shut this or that down.
[01:05:37.780 --> 01:05:39.940]   But then the US has been able to do that for years.
[01:05:39.940 --> 01:05:46.820]   So is part of this just somebody else can do the thing that we implicit that we could
[01:05:46.820 --> 01:05:48.060]   always do.
[01:05:48.060 --> 01:05:53.780]   But the two things aside from that, that there's no actual proof that anyone seems to know
[01:05:53.780 --> 01:05:57.140]   of that there are these close connections.
[01:05:57.140 --> 01:06:03.620]   The second thing was, is how come they're the only ones basically that can do 5G?
[01:06:03.620 --> 01:06:04.620]   And they're not.
[01:06:04.620 --> 01:06:05.620]   There's also Ericsson.
[01:06:05.620 --> 01:06:06.620]   Ericsson doesn't know if he does it.
[01:06:06.620 --> 01:06:07.620]   Samsung does it.
[01:06:07.620 --> 01:06:09.180]   But the problem is.
[01:06:09.180 --> 01:06:12.220]   Number one is Huawei by far.
[01:06:12.220 --> 01:06:15.220]   They're number one, they have the most 5G patents.
[01:06:15.220 --> 01:06:21.860]   And then what happened is over the last 10 years or so, like they've, it's classic.
[01:06:21.860 --> 01:06:24.660]   They come in and they do it cheaper and better.
[01:06:24.660 --> 01:06:29.380]   So if you're Vietnam, if you're India, if you're Argentina and you got to dial up a
[01:06:29.380 --> 01:06:34.980]   national 5G network, not only is the company that will do it for you the best.
[01:06:34.980 --> 01:06:36.580]   Huawei, it's also the cheapest.
[01:06:36.580 --> 01:06:40.460]   So if the US leans on all these other countries, like they're trying to do to say, no, no,
[01:06:40.460 --> 01:06:42.260]   no, don't use them.
[01:06:42.260 --> 01:06:44.420]   It doesn't make economic sense.
[01:06:44.420 --> 01:06:50.100]   And then the third thing was that, you know, I said, well, how come, how come they're the
[01:06:50.100 --> 01:06:51.460]   only ones left standing?
[01:06:51.460 --> 01:06:58.020]   Like, and Dexter said, well, it's like whatever became of Motorola, whatever became of Lucin
[01:06:58.020 --> 01:06:59.020]   Alcatel.
[01:06:59.020 --> 01:07:01.260]   He's like, yeah, it's just over the last 20 years.
[01:07:01.260 --> 01:07:06.100]   It's everything has has shifted to where, while no one was paying attention.
[01:07:06.100 --> 01:07:12.740]   Um, this Chinese company, uh, came, came up from below and, and can now do is the best
[01:07:12.740 --> 01:07:17.900]   at doing the technology that is as everyone tells us the key to the next generation of
[01:07:17.900 --> 01:07:19.900]   tech.
[01:07:19.900 --> 01:07:28.580]   I should also point out this is going to be a big on the other hand segment that much
[01:07:28.580 --> 01:07:32.820]   of Huawei's 5G switching gear is software bound.
[01:07:32.820 --> 01:07:37.060]   And so why it was just hard where you could look at it and say, well, this is okay.
[01:07:37.060 --> 01:07:38.060]   It's not phoning home.
[01:07:38.060 --> 01:07:43.020]   But once it's software, there's, and it's getting regular updates, anything could happen.
[01:07:43.020 --> 01:07:46.260]   And if you're a Chinese company, I wonder how much freedom, even if you're not owned
[01:07:46.260 --> 01:07:48.900]   by the Chinese government, I wonder how much freedom you have.
[01:07:48.900 --> 01:07:54.700]   If the Chinese government came to Huawei and said, okay, we'd like you to put this in your
[01:07:54.700 --> 01:07:56.420]   firmware.
[01:07:56.420 --> 01:07:57.420]   Could they say no?
[01:07:57.420 --> 01:07:58.420]   Hmm.
[01:07:58.420 --> 01:08:01.100]   But I noticed we don't say it's just Huawei.
[01:08:01.100 --> 01:08:02.180]   We don't say ZTE.
[01:08:02.180 --> 01:08:04.260]   We don't say Xiaomi.
[01:08:04.260 --> 01:08:09.060]   We don't, I mean, one plus one plus.
[01:08:09.060 --> 01:08:11.340]   And most of our gear is made in China.
[01:08:11.340 --> 01:08:16.740]   So I'm sure Foxcom would have the same problem if the Chinese government said, hey, according
[01:08:16.740 --> 01:08:23.900]   to Dexter, it is the, so the, the way that Huawei in the late 80s, early 90s, I think
[01:08:23.900 --> 01:08:31.060]   that's the timeline, became got off the ground is their first contract was with the PLA.
[01:08:31.060 --> 01:08:32.820]   So in the late 80s, early 90s.
[01:08:32.820 --> 01:08:33.820]   People's liberation army.
[01:08:33.820 --> 01:08:34.820]   Yes.
[01:08:34.820 --> 01:08:39.780]   So when the Chinese army says we need our own national network, telecoms network, for
[01:08:39.780 --> 01:08:44.740]   security purposes, it's Huawei that gets that contract and delivers.
[01:08:44.740 --> 01:08:50.180]   And then there have been these whispers behind the scenes that why does Huawei have apparently
[01:08:50.180 --> 01:08:55.020]   this access to unlimited capital in certain instances to win certain contracts?
[01:08:55.020 --> 01:08:59.460]   And so people are assuming that then that's because why do you think it's so cheap?
[01:08:59.460 --> 01:09:00.460]   Right.
[01:09:00.460 --> 01:09:08.780]   Well, the other shoe drops today because Google, according to Reuters, this has not yet confirmed
[01:09:08.780 --> 01:09:14.460]   at least not as of press time as if we had presses, as of press time.
[01:09:14.460 --> 01:09:20.780]   But apparently Reuters says Google has suspended business with Huawei will no longer update
[01:09:20.780 --> 01:09:23.580]   Android phones made by Huawei.
[01:09:23.580 --> 01:09:27.100]   That's not a concern inside China because of course Google doesn't do business in China.
[01:09:27.100 --> 01:09:31.060]   So those phones are not, you know, they're open source Android, but they're not using
[01:09:31.060 --> 01:09:32.060]   Google apps.
[01:09:32.060 --> 01:09:37.140]   And furthermore, Huawei phones will lose Google apps, Google services, including the
[01:09:37.140 --> 01:09:40.860]   Play Store and YouTube apps.
[01:09:40.860 --> 01:09:42.220]   That is a big penalty.
[01:09:42.220 --> 01:09:46.860]   And by the way, the Huawei phones are beloved in the United States.
[01:09:46.860 --> 01:09:51.860]   You can't buy them, you know, from a phone company, but people like Paul Thorett of Windows
[01:09:51.860 --> 01:09:54.380]   Weekly, he loves his Huawei.
[01:09:54.380 --> 01:09:55.700]   I was just going to make that point.
[01:09:55.700 --> 01:10:02.980]   I mean, when about a year and a half ago when Huawei took the stage at CES and was supposed
[01:10:02.980 --> 01:10:05.980]   to have this great AT&T deal to announce, they didn't.
[01:10:05.980 --> 01:10:09.700]   And then Richard Yu, their consumer business group executive sort of went off script and
[01:10:09.700 --> 01:10:13.260]   said, we think this is really bad because US consumers don't have access to the best
[01:10:13.260 --> 01:10:16.460]   choice, which at the time was their Mate 10 pro they were announcing.
[01:10:16.460 --> 01:10:20.980]   They were pissed because basically it was the United States government that went to AT&T
[01:10:20.980 --> 01:10:24.980]   and said, it would be foolish of you to sell this phone.
[01:10:24.980 --> 01:10:25.980]   And so they pulled out.
[01:10:25.980 --> 01:10:32.940]   AT&T exhibiting its usual, I forgot what the NSA memo said, it's unusual cooperativeness
[01:10:32.940 --> 01:10:33.940]   or something.
[01:10:33.940 --> 01:10:36.820]   It was like, oh, yeah, okay, fine.
[01:10:36.820 --> 01:10:37.820]   But my thing is-
[01:10:37.820 --> 01:10:40.140]   You can pretty much count on them to be cooperative usually.
[01:10:40.140 --> 01:10:41.140]   Yeah.
[01:10:41.140 --> 01:10:44.940]   Well, at the time I was like, well, the Mate 10 pro is fine, but I don't think it's the
[01:10:44.940 --> 01:10:47.580]   big loss to US consumers that Huawei said it was.
[01:10:47.580 --> 01:10:52.620]   And then what they did over the next year was build two of the most impressive and innovative
[01:10:52.620 --> 01:10:56.300]   from a hardware standpoint, smartphones I had ever used.
[01:10:56.300 --> 01:11:00.900]   And now, I mean, it's the cameras from the cameras to the reverse wireless charging that
[01:11:00.900 --> 01:11:03.380]   was Samsung due to the weird material finishes.
[01:11:03.380 --> 01:11:04.700]   They're wonderful phones.
[01:11:04.700 --> 01:11:09.340]   And now to see to see this develop is really a shame because if it goes through, yeah,
[01:11:09.340 --> 01:11:11.980]   they won't even work if you import them or they won't work.
[01:11:11.980 --> 01:11:16.300]   Well, you won't know what you want an Android phone that has no Google services on it.
[01:11:16.300 --> 01:11:17.300]   Exactly.
[01:11:17.300 --> 01:11:18.300]   That's a non-starter.
[01:11:18.300 --> 01:11:19.300]   Yeah.
[01:11:19.300 --> 01:11:21.860]   These are these phones have Leica cameras in them.
[01:11:21.860 --> 01:11:23.980]   They have 4000 milliamp hour batteries.
[01:11:23.980 --> 01:11:27.140]   I have a P20, which I guess I won't use anymore.
[01:11:27.140 --> 01:11:29.740]   Even more to the point, no more security updates.
[01:11:29.740 --> 01:11:33.100]   It means you're really, you know, this is not a phone to buy.
[01:11:33.100 --> 01:11:34.420]   We can't recommend this phone.
[01:11:34.420 --> 01:11:37.420]   But that's the other point.
[01:11:37.420 --> 01:11:41.780]   They actually haven't as far as I can tell brought the full hammer down yet where a US
[01:11:41.780 --> 01:11:43.180]   company cannot do business.
[01:11:43.180 --> 01:11:48.060]   Remember, they did that with ZT for a while and then dialed it back with a CT anyway.
[01:11:48.060 --> 01:11:49.060]   Yeah.
[01:11:49.060 --> 01:11:50.940]   Forget about the software.
[01:11:50.940 --> 01:11:55.700]   Leo, like they won't be able to manufacture their phones and there's articles that have
[01:11:55.700 --> 01:11:59.780]   come out about, you know, Huawei's been stockpiling components and stuff like that.
[01:11:59.780 --> 01:12:08.020]   But if they can't do business with US component providers, like they can't manufacture a modern
[01:12:08.020 --> 01:12:09.020]   smartphone.
[01:12:09.020 --> 01:12:10.500]   It's almost it's almost the reverse of what we're saying.
[01:12:10.500 --> 01:12:13.620]   Like you can't do a 5G network without Huawei.
[01:12:13.620 --> 01:12:21.220]   Huawei says in anticipation of this, they've been stockpiling American parts.
[01:12:21.220 --> 01:12:26.020]   But others are unconvinced.
[01:12:26.020 --> 01:12:32.320]   The head of Huawei's high silicon chip division Friday shrugged off concerns about disruptions
[01:12:32.320 --> 01:12:36.620]   to supply, saying in his long before preparing for this kind of extreme scenario, they will
[01:12:36.620 --> 01:12:48.140]   aim to be technologically self reliant going on going forward.
[01:12:48.140 --> 01:12:49.860]   Isn't that funny?
[01:12:49.860 --> 01:12:55.180]   If you ask somebody, what do you think the key components for the Huawei phone are?
[01:12:55.180 --> 01:12:56.180]   Where do they come from?
[01:12:56.180 --> 01:13:00.460]   You wouldn't say US, but that's what's different about the world today.
[01:13:00.460 --> 01:13:03.780]   It isn't just a Chinese phone.
[01:13:03.780 --> 01:13:14.220]   Saying your car is not just a German car or a Japanese car, it's a global economy.
[01:13:14.220 --> 01:13:21.100]   So you know, if there's a security issue, I mean, and you could certainly see that it
[01:13:21.100 --> 01:13:25.700]   might be prudent to say, well, let's not build our national infrastructure, particularly
[01:13:25.700 --> 01:13:30.820]   critical infrastructure on products from a Chinese company that could at any point pull
[01:13:30.820 --> 01:13:34.220]   the plug.
[01:13:34.220 --> 01:13:38.900]   But then they should state that, that it might not be smart, not that they believe that there's
[01:13:38.900 --> 01:13:43.300]   something nefarious that's happening and the timing of taking such a big competitor out
[01:13:43.300 --> 01:13:44.300]   of the market.
[01:13:44.300 --> 01:13:48.780]   Isn't though that the government's job, if they see a cyber, not that they do that job
[01:13:48.780 --> 01:13:52.980]   in other cases, but if they see a cyber threat, what is that Russia?
[01:13:52.980 --> 01:13:57.780]   If they see a cyber threat, they should state that this is what the cyber threat is.
[01:13:57.780 --> 01:14:01.260]   And if it's that they think that this would just be safer, they should be honest and transparent
[01:14:01.260 --> 01:14:04.860]   and say, we just think this would be a better way to make sure that it's all American made
[01:14:04.860 --> 01:14:06.780]   so that we have control over that.
[01:14:06.780 --> 01:14:12.700]   The thing is saying there's a threat then giving no proof and really creating, again,
[01:14:12.700 --> 01:14:16.900]   you hear something enough, you believe that it's true without any proof that's kind of
[01:14:16.900 --> 01:14:18.700]   behind it.
[01:14:18.700 --> 01:14:22.700]   That makes us think twice even about what the government then tells us.
[01:14:22.700 --> 01:14:27.100]   This is why it's so difficult when you have a government that is not only non-communicative,
[01:14:27.100 --> 01:14:33.300]   it doesn't seem to have a straight-arrow strategy as we zig and zag.
[01:14:33.300 --> 01:14:35.620]   It's hard to know, is this political?
[01:14:35.620 --> 01:14:37.380]   Is there a security threat?
[01:14:37.380 --> 01:14:38.820]   What is going on?
[01:14:38.820 --> 01:14:42.300]   And then also, is it too late anyway?
[01:14:42.300 --> 01:14:49.620]   Are they just doing this for purely jingoistic or like, there could be legitimate threats,
[01:14:49.620 --> 01:14:53.820]   but do they know what they need to know?
[01:14:53.820 --> 01:14:56.980]   The other articles that I've been reading is like, well, this won't matter.
[01:14:56.980 --> 01:15:02.020]   If you're afraid that working with Huawei will allow the Chinese government to listen
[01:15:02.020 --> 01:15:07.180]   in on everything you say, well, Huawei is already building or improving more than 100
[01:15:07.180 --> 01:15:08.900]   undersea internet cables.
[01:15:08.900 --> 01:15:09.900]   That's right.
[01:15:09.900 --> 01:15:14.700]   And there's only 380 in undersea that carry most of the internet anyway.
[01:15:14.700 --> 01:15:20.700]   And rural US broadband carriers, 25% already use Huawei equipment.
[01:15:20.700 --> 01:15:22.500]   So what's the inverse of?
[01:15:22.500 --> 01:15:23.500]   We're kind of a screwdriver.
[01:15:23.500 --> 01:15:25.540]   Because it's not just China.
[01:15:25.540 --> 01:15:32.420]   There are a lot of nation states who are actively hacking infrastructure in the US all the time.
[01:15:32.420 --> 01:15:33.820]   Am I wrong?
[01:15:33.820 --> 01:15:34.820]   No.
[01:15:34.820 --> 01:15:37.580]   So we're kind of...
[01:15:37.580 --> 01:15:41.100]   The smartest thing to do would not be to pick and choose, well, this guy's bad.
[01:15:41.100 --> 01:15:42.100]   That one's okay.
[01:15:42.100 --> 01:15:43.100]   This one's.
[01:15:43.100 --> 01:15:47.820]   But to really create a national cyber defense that's effective and to be working hard on
[01:15:47.820 --> 01:15:48.820]   that.
[01:15:48.820 --> 01:15:49.820]   Yeah.
[01:15:49.820 --> 01:15:54.180]   Fortunately, our visitor from the Air Force is going to do that.
[01:15:54.180 --> 01:15:55.180]   Right?
[01:15:55.180 --> 01:15:57.740]   I won't name names.
[01:15:57.740 --> 01:15:59.220]   He's a secret agent.
[01:15:59.220 --> 01:16:03.940]   I want to talk about face recognition just a little bit.
[01:16:03.940 --> 01:16:04.940]   This is a good...
[01:16:04.940 --> 01:16:06.940]   This is a juicy week.
[01:16:06.940 --> 01:16:08.940]   And we have a juicy panel.
[01:16:08.940 --> 01:16:12.860]   I don't know if that's the best way to describe you, but that's how I want to do it.
[01:16:12.860 --> 01:16:16.460]   Michael Fisher, he is a YouTube phenom.
[01:16:16.460 --> 01:16:18.380]   Oh, thank you.
[01:16:18.380 --> 01:16:21.220]   Were you in the rewind of this year?
[01:16:21.220 --> 01:16:22.220]   I was not.
[01:16:22.220 --> 01:16:23.220]   No, no, no, no, no, no.
[01:16:23.220 --> 01:16:27.540]   I focus to narrow a slice, I think, mobile tech is.
[01:16:27.540 --> 01:16:28.540]   But I like that.
[01:16:28.540 --> 01:16:29.540]   I like living where I'm comfortable.
[01:16:29.540 --> 01:16:30.540]   But you have...
[01:16:30.540 --> 01:16:32.740]   I mean, you're going to have a million subscribers this year.
[01:16:32.740 --> 01:16:35.540]   They've got to put you in the rewind then.
[01:16:35.540 --> 01:16:36.540]   Fingers crossed for next year.
[01:16:36.540 --> 01:16:37.540]   It's all right.
[01:16:37.540 --> 01:16:38.540]   Hope you know.
[01:16:38.540 --> 01:16:39.540]   We'll broaden things up a little bit.
[01:16:39.540 --> 01:16:41.460]   I want to see you next to them.
[01:16:41.460 --> 01:16:42.660]   What's the makeup lady?
[01:16:42.660 --> 01:16:47.180]   And I want to see you in that Fortnite bus.
[01:16:47.180 --> 01:16:49.700]   I want to see you up there with ninjas.
[01:16:49.700 --> 01:16:52.540]   No, no, this was the year not to begin rewind.
[01:16:52.540 --> 01:16:53.540]   It was kind of embarrassing.
[01:16:53.540 --> 01:16:54.540]   I was just going to say, yeah.
[01:16:54.540 --> 01:16:55.540]   You know what?
[01:16:55.540 --> 01:16:58.780]   That would have been a blessing.
[01:16:58.780 --> 01:17:00.620]   Even Youti Pie wasn't in the rewind.
[01:17:00.620 --> 01:17:02.060]   So you're in good company.
[01:17:02.060 --> 01:17:03.060]   Right.
[01:17:03.060 --> 01:17:05.060]   Mr. Armstrong's my story.
[01:17:05.060 --> 01:17:09.060]   Mr. Mobile.Tech.
[01:17:09.060 --> 01:17:10.580]   Also, the wonderful...
[01:17:10.580 --> 01:17:12.380]   That's a great picture.
[01:17:12.380 --> 01:17:13.380]   Thanks.
[01:17:13.380 --> 01:17:14.380]   That's a headshot, man.
[01:17:14.380 --> 01:17:17.100]   You could get a job reading law books with that.
[01:17:17.100 --> 01:17:19.020]   That is sweet.
[01:17:19.020 --> 01:17:22.460]   Wait, face for law books.
[01:17:22.460 --> 01:17:23.580]   I'm just teasing you.
[01:17:23.580 --> 01:17:25.820]   I would kill to read law books.
[01:17:25.820 --> 01:17:27.580]   Georgia Dow is also here.
[01:17:27.580 --> 01:17:33.620]   She is the woman at I'more, the senior at I'more, the woman who gives it its heart and
[01:17:33.620 --> 01:17:35.340]   its soul.
[01:17:35.340 --> 01:17:36.860]   And we adore you, Georgia.
[01:17:36.860 --> 01:17:38.860]   Love having you on.
[01:17:38.860 --> 01:17:43.380]   And Ryan McCullough, he is the host of the Tech Meade Ride Home.
[01:17:43.380 --> 01:17:46.100]   You've got to listen every single day.
[01:17:46.100 --> 01:17:47.460]   You could be riding to work.
[01:17:47.460 --> 01:17:49.620]   You don't have to be riding home.
[01:17:49.620 --> 01:17:50.780]   But it does come out later in the day.
[01:17:50.780 --> 01:17:51.980]   So I guess it's just...
[01:17:51.980 --> 01:17:55.540]   Some people say it's the Tech Meade while I'm in the shower show.
[01:17:55.540 --> 01:17:56.540]   Not a bad...
[01:17:56.540 --> 01:18:02.340]   People, as we've learned as podcasters, people listen to the strangest places.
[01:18:02.340 --> 01:18:03.940]   I had a guy call me...
[01:18:03.940 --> 01:18:05.980]   As long as they listen, there's no judgment.
[01:18:05.980 --> 01:18:06.980]   That's right.
[01:18:06.980 --> 01:18:07.980]   No judgment.
[01:18:07.980 --> 01:18:08.980]   That's what I said.
[01:18:08.980 --> 01:18:11.100]   The guy call me and he said, "I put you on when I want to go to sleep."
[01:18:11.100 --> 01:18:12.100]   And I said, "Yeah."
[01:18:12.100 --> 01:18:13.100]   It's good.
[01:18:13.100 --> 01:18:15.100]   As long as you listen.
[01:18:15.100 --> 01:18:19.340]   It's actually better the completion rate is higher on YouTube than later on.
[01:18:19.340 --> 01:18:20.340]   I think so.
[01:18:20.340 --> 01:18:21.340]   It goes right on the top.
[01:18:21.340 --> 01:18:22.340]   It's true.
[01:18:22.340 --> 01:18:23.340]   That is a good point.
[01:18:23.340 --> 01:18:24.340]   You know, every ad in sleep.
[01:18:24.340 --> 01:18:25.340]   But you know...
[01:18:25.340 --> 01:18:29.340]   Well, then if it's autoplay, then he's listened to 10 shows by the time he wakes up.
[01:18:29.340 --> 01:18:30.340]   Perfect.
[01:18:30.340 --> 01:18:32.020]   That's what he used to ask to do.
[01:18:32.020 --> 01:18:34.820]   I think half the things I bought on Instagram I bought in my sleep.
[01:18:34.820 --> 01:18:38.180]   So I don't think that's a problem.
[01:18:38.180 --> 01:18:39.380]   Think that's good.
[01:18:39.380 --> 01:18:40.620]   As long as you listen.
[01:18:40.620 --> 01:18:43.660]   Our show today brought to you by Zip Recruiter.
[01:18:43.660 --> 01:18:45.820]   If you're hiring, we use Zip Recruiter.
[01:18:45.820 --> 01:18:46.820]   I love it.
[01:18:46.820 --> 01:18:49.860]   If you're hiring, you know the right person's out there.
[01:18:49.860 --> 01:18:51.540]   You know, good shot the right person's out there.
[01:18:51.540 --> 01:18:52.540]   But where?
[01:18:52.540 --> 01:18:53.980]   What job board are they hunting?
[01:18:53.980 --> 01:18:55.700]   What social network are they?
[01:18:55.700 --> 01:18:57.740]   How do you reach them with your job?
[01:18:57.740 --> 01:19:01.500]   And then how do you kind of say, "Hey, you should apply to this job.
[01:19:01.500 --> 01:19:03.180]   Zip Recruiter makes it easy."
[01:19:03.180 --> 01:19:05.140]   It starts by posting on Zip Recruiter.
[01:19:05.140 --> 01:19:08.700]   By doing that, you're posting to more than 100 job sites, including Twitter and Facebook,
[01:19:08.700 --> 01:19:10.140]   all the social networks.
[01:19:10.140 --> 01:19:11.700]   So your reach is phenomenal.
[01:19:11.700 --> 01:19:14.580]   I mean, you're basically getting anywhere job hunters are looking.
[01:19:14.580 --> 01:19:15.900]   But then they go the next step.
[01:19:15.900 --> 01:19:22.220]   They use smart technology to look at the millions of resumes they have on file to find people
[01:19:22.220 --> 01:19:27.140]   who have the experience to fit the job you're posting and they invite them to apply.
[01:19:27.140 --> 01:19:32.540]   No, I love it because those applications don't go into your inbox or they don't ring
[01:19:32.540 --> 01:19:33.540]   your phone.
[01:19:33.540 --> 01:19:35.180]   They all go into the Zip Recruiter interface.
[01:19:35.180 --> 01:19:37.380]   They reformat all the resumes so they're quick to read.
[01:19:37.380 --> 01:19:40.140]   You can have screening questions so you can narrow it down.
[01:19:40.140 --> 01:19:46.500]   And Zip Recruiter will even analyze each applicant and move the best candidates to the top so
[01:19:46.500 --> 01:19:48.500]   you don't ever miss a great match.
[01:19:48.500 --> 01:19:53.500]   This works so well that on average, four out of five employers, 80% of all the people
[01:19:53.500 --> 01:19:58.340]   who post on Zip Recruiter get a quality candidate through the site within a day.
[01:19:58.340 --> 01:19:59.500]   And that's been our experience.
[01:19:59.500 --> 01:20:01.860]   At least we'll post a job at breakfast by lunch.
[01:20:01.860 --> 01:20:03.420]   In fact, I remember this.
[01:20:03.420 --> 01:20:04.940]   We were looking for a new bookkeeper.
[01:20:04.940 --> 01:20:08.500]   She's kind of depressed because she's going to have to do this work.
[01:20:08.500 --> 01:20:12.100]   If we can't hire somebody fast, we got two weeks, two weeks notice.
[01:20:12.100 --> 01:20:13.800]   She posts, I said post on Zip Recruiter.
[01:20:13.800 --> 01:20:16.660]   She does it at breakfast and it was so fun to watch.
[01:20:16.660 --> 01:20:17.660]   Like within an hour.
[01:20:17.660 --> 01:20:19.420]   Oh, I have an applicant.
[01:20:19.420 --> 01:20:20.420]   Oh, they're really good.
[01:20:20.420 --> 01:20:21.420]   Oh, another one.
[01:20:21.420 --> 01:20:23.020]   Oh, look, she lives right here.
[01:20:23.020 --> 01:20:24.020]   Perfect.
[01:20:24.020 --> 01:20:25.100]   By lunch, we had three.
[01:20:25.100 --> 01:20:27.440]   So it really, really works.
[01:20:27.440 --> 01:20:32.900]   You could try it free right now because you're listening to Twitter at ziprecruiter.com/twit.
[01:20:32.900 --> 01:20:33.900]   Please use that address.
[01:20:33.900 --> 01:20:37.100]   Let them know you heard it here at ziprecruiter.com/twit.
[01:20:38.100 --> 01:20:40.540]   It's the smartest way to hire.
[01:20:40.540 --> 01:20:42.780]   ziprecruiter.com/twit.
[01:20:42.780 --> 01:20:50.700]   We thank them so much for making Twit possible.
[01:20:50.700 --> 01:20:57.900]   Face recognition under the gun San Francisco has now passed a city ordinance banning face
[01:20:57.900 --> 01:20:59.140]   recognition tech.
[01:20:59.140 --> 01:21:04.780]   On Tuesday, the board of supervisors passed the stop secret surveillance ordinance.
[01:21:04.780 --> 01:21:09.100]   Oh, I love living in the Bay Area.
[01:21:09.100 --> 01:21:15.540]   But I actually am thrilled about this because as we know, well, here's what supervisor Aaron
[01:21:15.540 --> 01:21:18.460]   Pescan said, the sponsor.
[01:21:18.460 --> 01:21:19.460]   I want to be clear.
[01:21:19.460 --> 01:21:23.420]   This is not an anti technology policy.
[01:21:23.420 --> 01:21:28.780]   It's an accountability measure to ensure safe and responsible use of surveillance tech
[01:21:28.780 --> 01:21:33.180]   and pass by eight to one.
[01:21:33.180 --> 01:21:38.740]   And clearly we're starting to see the flaws in face recognition.
[01:21:38.740 --> 01:21:43.900]   And I'll give you a really great example from Vice this week.
[01:21:43.900 --> 01:21:49.220]   So when the police department interviews you and says, well, the guy who did it, what
[01:21:49.220 --> 01:21:50.500]   did he look like?
[01:21:50.500 --> 01:21:55.740]   If you say, well, kind of look like Woody Harrelson, they'll take a picture of Woody
[01:21:55.740 --> 01:22:01.820]   Harrelson and feed it to the face recognition technology law enforcement.
[01:22:01.820 --> 01:22:08.060]   This is a great piece in vise is identifying suspects based on all manner of probe photos,
[01:22:08.060 --> 01:22:11.540]   photos of unknown individuals submitted for search against the police or drivers license
[01:22:11.540 --> 01:22:19.340]   data base, license database, including celebrities.
[01:22:19.340 --> 01:22:22.660]   Harrelson's photo in this case returns.
[01:22:22.660 --> 01:22:28.980]   So a guy was caught was caught on camera stealing beer from a CVS in New York City.
[01:22:28.980 --> 01:22:32.980]   The surveillance footage was so low quality, they didn't get any suspects, no hits on the
[01:22:32.980 --> 01:22:35.300]   NYPD face recognition system.
[01:22:35.300 --> 01:22:42.220]   So the detective swapped in Woody Harrelson saying, well, he looks kind of like Woody.
[01:22:42.220 --> 01:22:47.300]   And yes, they got a lot of hits, including the guy who did it.
[01:22:47.300 --> 01:22:56.100]   Now you tell me, if I'm Woody Harrelson, I'm going to be a little insulted.
[01:22:56.100 --> 01:23:00.540]   He looks more like Colonel Sanders than Woody Harrelson.
[01:23:00.540 --> 01:23:06.020]   It looks like a Klingon without the forehead appliance.
[01:23:06.020 --> 01:23:12.500]   The NYPD also fed an image of an unnamed New York Knicks player to find a Brooklyn man
[01:23:12.500 --> 01:23:19.140]   wanted for assault.
[01:23:19.140 --> 01:23:22.900]   I just I think this is not ideal.
[01:23:22.900 --> 01:23:27.620]   Can we say, and so I kind of think that maybe San Francisco did the right thing.
[01:23:27.620 --> 01:23:28.620]   What do you think?
[01:23:28.620 --> 01:23:31.100]   And also, I think it's creepy.
[01:23:31.100 --> 01:23:36.100]   I'm ready to invest in fake noses and sunglasses.
[01:23:36.100 --> 01:23:44.340]   So our I mentioned this before, our founding engineer, Colleen, who now works for Facebook,
[01:23:44.340 --> 01:23:45.340]   right?
[01:23:45.340 --> 01:23:52.260]   She went to the dark side, had a plan to build an LED collar within for red LEDs that were
[01:23:52.260 --> 01:23:53.940]   so bright, they would blind cameras.
[01:23:53.940 --> 01:23:57.180]   So you just look like a glowing ball on camera.
[01:23:57.180 --> 01:24:02.060]   See if Carson can look this up real quick, but there's a shirt that had a T-shirt that's
[01:24:02.060 --> 01:24:03.060]   being sold.
[01:24:03.060 --> 01:24:04.060]   Oh, yeah.
[01:24:04.060 --> 01:24:05.060]   That'll trick him.
[01:24:05.060 --> 01:24:06.060]   Yeah, you maybe talked about it.
[01:24:06.060 --> 01:24:08.380]   No, no, but I remember reading it.
[01:24:08.380 --> 01:24:09.380]   Yeah.
[01:24:09.380 --> 01:24:14.300]   So it's got some sort of a graphic on it that will fool the cameras to be like, well, this
[01:24:14.300 --> 01:24:16.140]   is not a person.
[01:24:16.140 --> 01:24:17.140]   Forget about that.
[01:24:17.140 --> 01:24:20.460]   This is not going to solve the facial recognition thing, but yeah, there's a shirt out there
[01:24:20.460 --> 01:24:23.700]   to if we're in a crowd and a protest in theory.
[01:24:23.700 --> 01:24:27.020]   It's the hyperface anti paparazzi collection.
[01:24:27.020 --> 01:24:28.620]   Actually, that's different.
[01:24:28.620 --> 01:24:29.620]   This is glowing.
[01:24:29.620 --> 01:24:31.780]   Yeah, because that's only if it flashes.
[01:24:31.780 --> 01:24:32.780]   Yeah.
[01:24:32.780 --> 01:24:33.860]   That's for the flash of the cameras.
[01:24:33.860 --> 01:24:35.660]   And then it's reflective.
[01:24:35.660 --> 01:24:37.740]   So it reflects back.
[01:24:37.740 --> 01:24:38.940]   Sounds like a brilliant idea.
[01:24:38.940 --> 01:24:39.940]   That's a great idea.
[01:24:39.940 --> 01:24:50.140]   This is hyperface, which apparently if you wear this on a T-shirt, this image, you don't
[01:24:50.140 --> 01:24:51.140]   have to wear it on your face.
[01:24:51.140 --> 01:24:55.780]   You wear it on a T-shirt and the cameras go, "Hey, baby!"
[01:24:55.780 --> 01:24:59.340]   But on the other hand, this is being designed by an artist, so I'm nervous about the whole
[01:24:59.340 --> 01:25:00.340]   thing.
[01:25:00.340 --> 01:25:03.380]   Yeah, and how many software updates until the cameras just get smart enough to get around
[01:25:03.380 --> 01:25:04.380]   that?
[01:25:04.380 --> 01:25:05.380]   Right, yeah.
[01:25:05.380 --> 01:25:08.220]   I mean, it's immediately been borked.
[01:25:08.220 --> 01:25:09.500]   Still I like the idea.
[01:25:09.500 --> 01:25:12.580]   The other problem is if you're a person of color, if you're African American, there's
[01:25:12.580 --> 01:25:15.860]   a much higher incidence of false positives, mostly because the stuff's trained on white
[01:25:15.860 --> 01:25:17.860]   folk.
[01:25:17.860 --> 01:25:19.660]   So right.
[01:25:19.660 --> 01:25:21.100]   That's not a good thing.
[01:25:21.100 --> 01:25:27.420]   And also, if correct me if I'm wrong, but all this law said was that the city cannot
[01:25:27.420 --> 01:25:28.940]   use this technology.
[01:25:28.940 --> 01:25:36.180]   There's no, this doesn't say that any grocery store or private home or whatever cannot use
[01:25:36.180 --> 01:25:37.180]   this technology.
[01:25:37.180 --> 01:25:39.860]   Yeah, I don't think they can even stop you, right?
[01:25:39.860 --> 01:25:40.860]   I guess they could.
[01:25:40.860 --> 01:25:47.260]   It's not like, well, in theory, you could say something stops at the borders of our municipality.
[01:25:47.260 --> 01:25:48.260]   That's hard to do.
[01:25:48.260 --> 01:25:55.340]   Yeah, no, the ordinance says that city departments have to seek specific approval before acquiring
[01:25:55.340 --> 01:25:57.940]   any new surveillance equipment.
[01:25:57.940 --> 01:26:00.140]   And private companies are not affected.
[01:26:00.140 --> 01:26:04.940]   Although if you wanted to sell tech to the city government of this kind, you would be,
[01:26:04.940 --> 01:26:08.220]   but not if you're a CVS store that wants to have it in there.
[01:26:08.220 --> 01:26:13.540]   Actually, you know, there's some very beneficial uses of face recognition.
[01:26:13.540 --> 01:26:21.700]   Who was the artist who had a problem with stalkers and she used face recognition to,
[01:26:21.700 --> 01:26:23.100]   who was it?
[01:26:23.100 --> 01:26:24.100]   Richard, huh?
[01:26:24.100 --> 01:26:26.100]   It was Tay-Tay.
[01:26:26.100 --> 01:26:27.580]   Tay-Tay Taylor Swift.
[01:26:27.580 --> 01:26:29.220]   That's right.
[01:26:29.220 --> 01:26:34.380]   And so they used face recognition at the concert entrances to look for bad guys.
[01:26:34.380 --> 01:26:39.860]   We know other stadium have used it to ban people who are known disruptors or problems.
[01:26:39.860 --> 01:26:43.700]   On the other hand, if you get a false positive and you can't go to the Tay-Tay show, that's
[01:26:43.700 --> 01:26:44.700]   soft.
[01:26:44.700 --> 01:26:49.540]   Well, the classic positive use of it is that they always trot out is if you've lost the
[01:26:49.540 --> 01:26:50.700]   kid in Disney World.
[01:26:50.700 --> 01:26:51.700]   There you go.
[01:26:51.700 --> 01:26:52.700]   Right.
[01:26:52.700 --> 01:26:53.700]   Yeah.
[01:26:53.700 --> 01:26:57.740]   Yeah, I've saw a demo at Qualcomm's AI Day a few weeks back where they were, you know,
[01:26:57.740 --> 01:27:02.140]   showing you an interpreted form of what the AI sees when the camera looks at you in, say,
[01:27:02.140 --> 01:27:04.460]   a retail storefront situation.
[01:27:04.460 --> 01:27:08.340]   And you know, it's meant to do the catch shop lifters thing.
[01:27:08.340 --> 01:27:15.940]   But the demo was calibrated to detect whether, you know, to try and guess your apparent gender,
[01:27:15.940 --> 01:27:18.180]   you know, and also your age.
[01:27:18.180 --> 01:27:22.740]   So I spent, I wasted probably 35 minutes trying to, trying various pouts and smiles to get
[01:27:22.740 --> 01:27:25.300]   it to go from adult person to young person.
[01:27:25.300 --> 01:27:26.300]   And did it?
[01:27:26.300 --> 01:27:27.300]   And I was able to fool.
[01:27:27.300 --> 01:27:28.300]   Yeah, I've fooled about three or four times.
[01:27:28.300 --> 01:27:29.300]   I'm trying to get it.
[01:27:29.300 --> 01:27:30.300]   Right now I'm trying to get it.
[01:27:30.300 --> 01:27:31.300]   I can't figure it out.
[01:27:31.300 --> 01:27:32.300]   Yeah.
[01:27:32.300 --> 01:27:33.300]   It's too fuzzy.
[01:27:33.300 --> 01:27:34.300]   I got to be casually on the lens.
[01:27:34.300 --> 01:27:35.300]   Yeah.
[01:27:35.300 --> 01:27:38.300]   I mean, we've seen so many examples of these things making horrible mistakes.
[01:27:38.300 --> 01:27:42.500]   Like somebody in the chat room saying if it wasn't so long ago, a Google AI thought
[01:27:42.500 --> 01:27:44.620]   a turtle was a gun.
[01:27:44.620 --> 01:27:47.420]   Right now.
[01:27:47.420 --> 01:27:52.420]   So and honestly, let's think about this use case in Disneyland.
[01:27:52.420 --> 01:27:53.420]   So what is it doing?
[01:27:53.420 --> 01:27:55.820]   Looking at everybody's land to see if they're the lost kid?
[01:27:55.820 --> 01:27:56.820]   Yeah.
[01:27:56.820 --> 01:28:00.940]   I mean, again, they would find the person that took the child, but your child's still
[01:28:00.940 --> 01:28:01.940]   gone.
[01:28:01.940 --> 01:28:05.260]   This is better for finding someone that you know that you know that you're looking for.
[01:28:05.260 --> 01:28:08.740]   That's a stranger danger of duxions just the kid got lost.
[01:28:08.740 --> 01:28:10.300]   It could be either or though.
[01:28:10.300 --> 01:28:11.300]   Oh, wow.
[01:28:11.300 --> 01:28:12.300]   Yeah.
[01:28:12.300 --> 01:28:14.260]   What I'm saying is like, again, I don't know.
[01:28:14.260 --> 01:28:16.140]   That's just the thing that they trot out all the time.
[01:28:16.140 --> 01:28:21.300]   I feel like any human could walk into a crowd and see a kid wandering going, where's my
[01:28:21.300 --> 01:28:22.300]   mommy?
[01:28:22.300 --> 01:28:23.580]   I mean, that how hard is that?
[01:28:23.580 --> 01:28:24.580]   Yeah.
[01:28:24.580 --> 01:28:25.740]   And you do a better job probably.
[01:28:25.740 --> 01:28:26.740]   I'm sorry.
[01:28:26.740 --> 01:28:30.980]   That would exactly happen faster than it would to get the cameras and then, you know, deal
[01:28:30.980 --> 01:28:33.620]   with all the data and then find them that way.
[01:28:33.620 --> 01:28:36.420]   I, I'm ashamed to admit I'm a terrible parent.
[01:28:36.420 --> 01:28:41.780]   I lost my daughter at Lego land, but I got somebody who's better and she graduated from
[01:28:41.780 --> 01:28:42.780]   college.
[01:28:42.780 --> 01:28:46.660]   So I feel like it was truly terrifying.
[01:28:46.660 --> 01:28:47.980]   I was there with my son and daughter.
[01:28:47.980 --> 01:28:48.980]   Thank God.
[01:28:48.980 --> 01:28:49.980]   My wife wasn't there at the time.
[01:28:49.980 --> 01:28:51.780]   She would have killed me.
[01:28:51.780 --> 01:28:56.500]   She never she's hearing about it for the first time now.
[01:28:56.500 --> 01:28:59.780]   And I'm with Henry and where's Abby?
[01:28:59.780 --> 01:29:00.780]   Where's Abby?
[01:29:00.780 --> 01:29:05.620]   You know, I went immediately like to the entrance to block whoever stole her from getting out,
[01:29:05.620 --> 01:29:09.060]   which is stupid, but she found us and she was not happy.
[01:29:09.060 --> 01:29:10.900]   So I was shopping.
[01:29:10.900 --> 01:29:17.220]   She was like, I was in the store looking at Lego.
[01:29:17.220 --> 01:29:20.340]   My child did the same thing, but he was purposely hiding.
[01:29:20.340 --> 01:29:24.100]   So he was calling his name and he thought it was brilliantly funny that we would never
[01:29:24.100 --> 01:29:25.100]   find him.
[01:29:25.100 --> 01:29:30.740]   Oh, and I thought the flash of like strangling him or hugging him came to me and I thought
[01:29:30.740 --> 01:29:32.220]   in that moment, we finally got him.
[01:29:32.220 --> 01:29:33.860]   It was like 30 minutes.
[01:29:33.860 --> 01:29:36.660]   You're so torn because you're so grateful at the same time.
[01:29:36.660 --> 01:29:37.660]   So angry.
[01:29:37.660 --> 01:29:42.500]   When he said I was hiding, I was grateful until he said, no, no, I was hiding.
[01:29:42.500 --> 01:29:47.140]   I heard you guys and I chose to hide better.
[01:29:47.140 --> 01:29:49.140]   Yeah.
[01:29:49.140 --> 01:29:51.140]   You know what?
[01:29:51.140 --> 01:29:53.020]   I'm 27 and 24.
[01:29:53.020 --> 01:29:54.340]   I'm done.
[01:29:54.340 --> 01:29:56.700]   This day will come Georgia when you can look back.
[01:29:56.700 --> 01:29:57.700]   It said they didn't.
[01:29:57.700 --> 01:29:58.700]   Never really done.
[01:29:58.700 --> 01:29:59.700]   No, no.
[01:29:59.700 --> 01:30:01.460]   That's pointed there on their own recognizance.
[01:30:01.460 --> 01:30:02.460]   It's not my.
[01:30:02.460 --> 01:30:05.540]   And either way, the AI is not going to help us with the parenting part.
[01:30:05.540 --> 01:30:11.420]   So I basically, I got, you know, if you get them to 21, I mean, you can't, I mean, you've
[01:30:11.420 --> 01:30:14.660]   done your job.
[01:30:14.660 --> 01:30:18.580]   Grinder, did you know I did, but I bet a lot of people didn't know Grinder is owned by
[01:30:18.580 --> 01:30:20.940]   a Chinese company.
[01:30:20.940 --> 01:30:27.220]   The popular gay dating app is owned by Beijing Kunlun Tech Company.
[01:30:27.220 --> 01:30:33.660]   Did you also know that there is a committee on foreign investment in the US, CFIUS, which
[01:30:33.660 --> 01:30:40.940]   has ordered the Chinese owners of Grinder to sell Grinder by June of next year, June
[01:30:40.940 --> 01:30:41.940]   2020.
[01:30:41.940 --> 01:30:46.140]   And in the meantime, not transmit any information to entities based in China.
[01:30:46.140 --> 01:30:47.140]   You do.
[01:30:47.140 --> 01:30:49.380]   I kind of think that's a security problem.
[01:30:49.380 --> 01:30:50.380]   Right.
[01:30:50.380 --> 01:30:55.280]   I mean, I'm maybe it was just a simple, you know, acquisition.
[01:30:55.280 --> 01:30:59.240]   They also own TikTok Chinese company owns TikTok.
[01:30:59.240 --> 01:31:01.880]   I don't know.
[01:31:01.880 --> 01:31:06.420]   I think I'm happier that a Chinese company owns them than Facebook, but that might just
[01:31:06.420 --> 01:31:07.660]   be my own bias.
[01:31:07.660 --> 01:31:11.440]   You have to think though that maybe some of the motivation for buying this was the Chinese
[01:31:11.440 --> 01:31:13.600]   government hoping to be able to blackmail people.
[01:31:13.600 --> 01:31:19.400]   If you found a, you know, a closeted government employee that you could say, Hey, we know,
[01:31:19.400 --> 01:31:21.760]   we know what you've been doing.
[01:31:21.760 --> 01:31:22.760]   I don't know.
[01:31:22.760 --> 01:31:28.840]   Probably best, but they bought Grinder and I remember when they bought Grinder in 2018,
[01:31:28.840 --> 01:31:31.500]   I thought, Oh boy, same thing with TikTok.
[01:31:31.500 --> 01:31:32.600]   I worry.
[01:31:32.600 --> 01:31:33.600]   I worry.
[01:31:33.600 --> 01:31:34.600]   That's my job.
[01:31:34.600 --> 01:31:36.600]   I'm a warrior.
[01:31:36.600 --> 01:31:41.240]   Um, let's see.
[01:31:41.240 --> 01:31:42.520]   Ajit Pai, chairman.
[01:31:42.520 --> 01:31:44.160]   This is the government section.
[01:31:44.160 --> 01:31:51.400]   Chairman of the FCC has proposed finally new rules about robo calls, but it's not going
[01:31:51.400 --> 01:31:52.400]   to be government.
[01:31:52.400 --> 01:31:53.400]   Of course not.
[01:31:53.400 --> 01:31:54.400]   It's got to be private enterprise.
[01:31:54.400 --> 01:31:59.960]   He's saying a new rule that would allow carriers to block robo calls.
[01:31:59.960 --> 01:32:02.040]   Is there in that article?
[01:32:02.040 --> 01:32:05.720]   Because I only saw this on some subreddit.
[01:32:05.720 --> 01:32:09.880]   Is there also a clause in there that will allow them to charge for the privilege?
[01:32:09.880 --> 01:32:15.840]   Ooh, you just nailed the problem here.
[01:32:15.840 --> 01:32:18.320]   Wow.
[01:32:18.320 --> 01:32:21.120]   We know that, uh, who was it?
[01:32:21.120 --> 01:32:28.480]   Was it AT&T was blocking text messages from an anti-abortion organization?
[01:32:28.480 --> 01:32:33.080]   Uh, they said, well, they're a terrorist group.
[01:32:33.080 --> 01:32:34.160]   I don't know what they were saying.
[01:32:34.160 --> 01:32:38.280]   They were saying, but it was clearly political.
[01:32:38.280 --> 01:32:43.160]   So we know carriers will use these tools to do whatever is good for them.
[01:32:43.160 --> 01:32:47.720]   Uh, however, anything to fight robo calls them all offer it.
[01:32:47.720 --> 01:32:50.360]   Are you willing to pay $5 more a month?
[01:32:50.360 --> 01:32:51.360]   Not to have robo calls.
[01:32:51.360 --> 01:32:54.360]   Just like I pay $5 a month, not to have a listed phone number.
[01:32:54.360 --> 01:32:55.360]   Yeah.
[01:32:55.360 --> 01:32:57.120]   Don't you hate these guys?
[01:32:57.120 --> 01:33:01.400]   They are just, uh, it's hard not to get fatalistic about this.
[01:33:01.400 --> 01:33:05.920]   I mean, I have a very clear memory of, uh, my roommate in college had a, he worked a
[01:33:05.920 --> 01:33:09.640]   telemarketing job and he had a newsletter that he brought home from work.
[01:33:09.640 --> 01:33:13.480]   And the cover story was, you know, do not call got you down.
[01:33:13.480 --> 01:33:15.040]   Here's a bunch of ways to get around.
[01:33:15.040 --> 01:33:18.040]   But do not call registry.
[01:33:18.040 --> 01:33:24.040]   And you know that was, I was 17 years ago and like every five years, it seems like we
[01:33:24.040 --> 01:33:28.000]   have a new government mandate or a private sector mandate to do something about this.
[01:33:28.000 --> 01:33:31.960]   And now we're in a world where half of all calls made in the US or robo calls or something.
[01:33:31.960 --> 01:33:34.840]   I mean, oh, it's, it's madness.
[01:33:34.840 --> 01:33:39.780]   So in States, is there a do not call registry and then you, you would not be part of the
[01:33:39.780 --> 01:33:40.880]   robo call list?
[01:33:40.880 --> 01:33:41.880]   Yes.
[01:33:41.880 --> 01:33:44.760]   But didn't it exclude mobile numbers for a while?
[01:33:44.760 --> 01:33:49.920]   There's like, there's a ton of ways to get out of the way, which I sign up for religiously
[01:33:49.920 --> 01:33:56.680]   because absolutely nothing as everybody knows, because most robo calls are illegal anyway.
[01:33:56.680 --> 01:33:59.640]   For instance, it's illegal to do neighbor spoofing.
[01:33:59.640 --> 01:34:04.040]   You know, the robo call it comes from not only your area code, but your exchange.
[01:34:04.040 --> 01:34:05.040]   That's it.
[01:34:05.040 --> 01:34:06.040]   We got a phone cup.
[01:34:06.040 --> 01:34:07.040]   Yes.
[01:34:07.040 --> 01:34:08.040]   Yeah.
[01:34:08.040 --> 01:34:09.040]   Is that weird?
[01:34:09.040 --> 01:34:10.440]   But why would you answer that?
[01:34:10.440 --> 01:34:11.440]   You know, that's phony.
[01:34:11.440 --> 01:34:13.480]   Well, but you don't write it.
[01:34:13.480 --> 01:34:17.600]   It's like it's, it's, I mean, if I didn't know about it, I would be curious.
[01:34:17.600 --> 01:34:19.400]   It's like, how about me?
[01:34:19.400 --> 01:34:23.000]   Is it, you turn me calling to war house?
[01:34:23.000 --> 01:34:25.000]   Yeah, exactly.
[01:34:25.000 --> 01:34:26.240]   It's a mirror universe man.
[01:34:26.240 --> 01:34:28.200]   Brian, you'll get on the bridge tonight.
[01:34:28.200 --> 01:34:30.000]   Yeah, don't get on the train.
[01:34:30.000 --> 01:34:31.800]   Not answer it.
[01:34:31.800 --> 01:34:32.800]   Don't answer.
[01:34:32.800 --> 01:34:34.480]   Got to answer it.
[01:34:34.480 --> 01:34:38.120]   You can't not answer it if it's coming from you.
[01:34:38.120 --> 01:34:39.640]   That's actually a brilliant strategy.
[01:34:39.640 --> 01:34:42.360]   Do you think that that's what they're doing?
[01:34:42.360 --> 01:34:44.120]   They probably just try everything.
[01:34:44.120 --> 01:34:45.120]   Exactly.
[01:34:45.120 --> 01:34:46.120]   Yeah.
[01:34:46.120 --> 01:34:47.560]   And then they compare the, the, the, the accept rates.
[01:34:47.560 --> 01:34:52.760]   We notice that whenever we call from the caller's number, it really works.
[01:34:52.760 --> 01:34:58.760]   40, 48 billion, 48 billion robo calls placed in the US alone.
[01:34:58.760 --> 01:35:03.200]   Last year, most estimates are that more than 50% of the calls you get, in my case, more
[01:35:03.200 --> 01:35:06.360]   than 90% of the calls you get are robo calls.
[01:35:06.360 --> 01:35:08.040]   Because it's cheap.
[01:35:08.040 --> 01:35:09.040]   There's no risk.
[01:35:09.040 --> 01:35:10.960]   You're not going to get caught.
[01:35:10.960 --> 01:35:12.560]   Because there's no enforcement.
[01:35:12.560 --> 01:35:17.000]   Notice that even now the FCC is not saying, oh, we're going to enforce it.
[01:35:17.000 --> 01:35:19.720]   We're going to let the carriers do it.
[01:35:19.720 --> 01:35:21.200]   It's a profit thing.
[01:35:21.200 --> 01:35:22.200]   Mm.
[01:35:22.200 --> 01:35:26.000]   So, you know, this sounds like he's doing something really great.
[01:35:26.000 --> 01:35:31.680]   Well, and I'll, I will point out, and I'm not saying that I know that this is a conspiracy
[01:35:31.680 --> 01:35:35.280]   theory thing, but there have been a lot of articles over the last several months about
[01:35:35.280 --> 01:35:37.440]   that new technology that's supposed to be.
[01:35:37.440 --> 01:35:38.440]   Shaking in the carrier.
[01:35:38.440 --> 01:35:39.440]   Yeah.
[01:35:39.440 --> 01:35:40.440]   Exactly.
[01:35:40.440 --> 01:35:41.440]   All right.
[01:35:41.440 --> 01:35:45.600]   If you were conspiracy minded, isn't it odd that all of a sudden you see all these articles
[01:35:45.600 --> 01:35:48.920]   popping up about this great new technology that the carriers have?
[01:35:48.920 --> 01:35:52.280]   So, that's a James Bond's technology.
[01:35:52.280 --> 01:35:57.520]   Like a product rollout that then you can suddenly start charging people $5 a month for.
[01:35:57.520 --> 01:35:59.560]   They're one of the things that will happen.
[01:35:59.560 --> 01:36:00.560]   You'll have to opt in.
[01:36:00.560 --> 01:36:02.680]   It's not opt in by default.
[01:36:02.680 --> 01:36:05.280]   But one of the things that they will allow you to do, and I think this might be a good
[01:36:05.280 --> 01:36:08.040]   solution is to, you could say as a customer, and I bet you're right.
[01:36:08.040 --> 01:36:09.240]   I bet you have to pay for it.
[01:36:09.240 --> 01:36:14.800]   I only want to get calls from phone numbers in my contact list.
[01:36:14.800 --> 01:36:16.720]   Then you got to share your contact list with us.
[01:36:16.720 --> 01:36:17.720]   Oh crap.
[01:36:17.720 --> 01:36:19.280]   It's a profit deal.
[01:36:19.280 --> 01:36:25.280]   And if you're a business owner, you can't really do that.
[01:36:25.280 --> 01:36:26.800]   All right.
[01:36:26.800 --> 01:36:32.000]   My doctor uses a blocked, I wish he wouldn't, but it's probably some HIPAA requirement.
[01:36:32.000 --> 01:36:34.520]   Never calls me from a identified number.
[01:36:34.520 --> 01:36:37.040]   It's always a blocked number.
[01:36:37.040 --> 01:36:39.960]   So I have to answer blocked calls, except I don't care.
[01:36:39.960 --> 01:36:40.960]   I don't want to talk to him.
[01:36:40.960 --> 01:36:41.960]   He's just going to talk to you.
[01:36:41.960 --> 01:36:43.960]   I figure if it's important, they'll come to voicemail.
[01:36:43.960 --> 01:36:44.960]   Yeah, that's right.
[01:36:44.960 --> 01:36:45.960]   Leave a voicemail.
[01:36:45.960 --> 01:36:48.840]   And so many of the tools that are supposed to help you out on the handset side or on
[01:36:48.840 --> 01:36:50.800]   the carrier side are so imperfect at this point.
[01:36:50.800 --> 01:36:54.720]   I get dinner delivered maybe twice a week, right?
[01:36:54.720 --> 01:36:59.120]   And almost every time the delivery driver calls me to tell me is that the apartment building,
[01:36:59.120 --> 01:37:02.560]   it comes up as scam likely, regardless of who it is.
[01:37:02.560 --> 01:37:03.560]   That's not good.
[01:37:03.560 --> 01:37:04.560]   Yeah.
[01:37:04.560 --> 01:37:06.800]   Presumably because this is a phone number that makes the what?
[01:37:06.800 --> 01:37:12.120]   Like 800 outbound calls of less than 15 seconds every week, right?
[01:37:12.120 --> 01:37:13.120]   Yeah.
[01:37:13.120 --> 01:37:16.280]   And then you don't get your Moo Goo guy pan because you didn't answer.
[01:37:16.280 --> 01:37:17.640]   That doesn't make me happy.
[01:37:17.640 --> 01:37:18.640]   Nope.
[01:37:18.640 --> 01:37:19.640]   Yeah.
[01:37:19.640 --> 01:37:22.760]   Our show today brought to you by Atlassian.
[01:37:22.760 --> 01:37:24.160]   We are in Atlassian house.
[01:37:24.160 --> 01:37:26.600]   That means we use, well, it all started with Jira for us.
[01:37:26.600 --> 01:37:28.120]   I bet a lot of people know Jira.
[01:37:28.120 --> 01:37:33.560]   And if you're a developer, it's the keystone, the cornerstone to agile development.
[01:37:33.560 --> 01:37:37.840]   But really it's a project manager and that's why our IT team loves Jira because we can
[01:37:37.840 --> 01:37:43.760]   keep track of who's doing what, what projects are in process, where they are in the process.
[01:37:43.760 --> 01:37:47.880]   It's a great tool that works with, we use a confluence also so we can document what's
[01:37:47.880 --> 01:37:50.120]   going on.
[01:37:50.120 --> 01:37:56.960]   Things I like about Atlassian software, especially for IT, is it's a great way to organize your
[01:37:56.960 --> 01:37:57.960]   IT department.
[01:37:57.960 --> 01:38:03.280]   But it's also a great way to ensure collaboration so that people work together.
[01:38:03.280 --> 01:38:07.040]   And an important part of collaboration is communication, not just with each other but
[01:38:07.040 --> 01:38:11.440]   with stakeholders, with the boss, with your customers, with your colleagues.
[01:38:11.440 --> 01:38:17.160]   Atlassian empowers IT teams around the world to just do it right.
[01:38:17.160 --> 01:38:20.360]   And then this cloud-based world, man, it's the IT team that's on the front line.
[01:38:20.360 --> 01:38:25.840]   They really are the people who keep the roads rolling, right?
[01:38:25.840 --> 01:38:29.360]   Stakes are high, expectations are high, IT teams are at the center.
[01:38:29.360 --> 01:38:34.320]   But if it's a business-critical workflow, failure is not an option.
[01:38:34.320 --> 01:38:35.320]   Enter Atlassian.
[01:38:35.320 --> 01:38:39.720]   The company behind Jira, their software tools are designed to manage complex collaboration,
[01:38:39.720 --> 01:38:41.520]   make your life better.
[01:38:41.520 --> 01:38:44.480]   It's not just for developers anymore.
[01:38:44.480 --> 01:38:49.720]   Atlassian offers a reliable suite of tools for teams of all kinds, from DevOps to Agile
[01:38:49.720 --> 01:38:55.280]   to IT apps to ops to ITSM and to whatever's next.
[01:38:55.280 --> 01:39:00.080]   It provides a technology backbone to help modern IT organizations plan and service and support
[01:39:00.080 --> 01:39:04.320]   the kind of change that propels business, your business.
[01:39:04.320 --> 01:39:09.000]   We use Jira in Confluence if you have a code base that's got to be Bitbucket in there.
[01:39:09.000 --> 01:39:14.280]   If you have a monitor ops who's got ops, genie and status page, it's really the heart
[01:39:14.280 --> 01:39:18.560]   of DevOps is making life better for yourself, making it easier.
[01:39:18.560 --> 01:39:20.040]   Atlassian's got the tools.
[01:39:20.040 --> 01:39:24.480]   In fact, your team can find exactly the tool for your current framework but trust as you
[01:39:24.480 --> 01:39:29.440]   grow, as your needs grow, Atlassian's tools and abilities will grow with you.
[01:39:29.440 --> 01:39:33.200]   It all integrates seamlessly with Jira in Confluence so you don't have to jump around
[01:39:33.200 --> 01:39:34.640]   from platform to platform.
[01:39:34.640 --> 01:39:40.120]   Atlassian makes tools for the future of IT and like all of their products, the tools you
[01:39:40.120 --> 01:39:41.840]   need are easy and free to try.
[01:39:41.840 --> 01:39:48.760]   Just go to Atlassian.com/IT to find out which Atlassian offering is right for your team.
[01:39:48.760 --> 01:39:51.000]   Try Atlassian today to see what IT can be.
[01:39:51.000 --> 01:39:55.400]   It's Atlassian.com/IT.
[01:39:55.400 --> 01:40:01.600]   We thank them so much for their wholehearted support of the TWIT podcast.
[01:40:01.600 --> 01:40:08.640]   I thought this was a very provocative story from Vice and it ties into this story from
[01:40:08.640 --> 01:40:13.640]   last week about WhatsApp.
[01:40:13.640 --> 01:40:20.080]   The NSO group which is an Israeli firm that provides hacking tools, they say only to
[01:40:20.080 --> 01:40:24.720]   governments as if that makes it okay.
[01:40:24.720 --> 01:40:34.840]   NSO tools were found, well, in its interesting story how it happened, a human rights activist
[01:40:34.840 --> 01:40:41.440]   Claudio Guanyari found that a colleague of his, he was at Amnesty International, was targeted
[01:40:41.440 --> 01:40:48.880]   by NSO spyware.
[01:40:48.880 --> 01:40:50.280]   Actually, is that the story of how it was discovered?
[01:40:50.280 --> 01:40:53.280]   It was discovered by a human rights activist.
[01:40:53.280 --> 01:41:01.200]   It was a lawyer that was suing the NSO group.
[01:41:01.200 --> 01:41:03.960]   He noticed some odd behavior, right?
[01:41:03.960 --> 01:41:08.920]   I don't remember the details on how he saw it on his phone but the point was that he's
[01:41:08.920 --> 01:41:15.920]   in the business on behalf of human rights people, on behalf of Amnesty International,
[01:41:15.920 --> 01:41:24.360]   on behalf of dissidents that are overseas, trying to flee their government's wrath,
[01:41:24.360 --> 01:41:28.560]   suing them for essentially selling these tools, allowing those governments to track these
[01:41:28.560 --> 01:41:31.120]   people down and to monitor their stuff.
[01:41:31.120 --> 01:41:34.040]   Some of the things that he was working on on behalf of his clients, some of the things
[01:41:34.040 --> 01:41:38.000]   his clients were reporting, he started to see happening to his own device.
[01:41:38.000 --> 01:41:39.000]   I got it now.
[01:41:39.000 --> 01:41:42.760]   He was representing five Mexican journalists who were suing the NSO.
[01:41:42.760 --> 01:41:46.440]   They say their funds were hijacked with a company's Pegasus spyware but the lawyers
[01:41:46.440 --> 01:41:54.440]   started receiving weird calls on WhatsApp over three weeks ago from Sweden.
[01:41:54.440 --> 01:41:55.760]   He's like, "What's going on?
[01:41:55.760 --> 01:41:58.440]   He contacted Citizen Lab."
[01:41:58.440 --> 01:42:05.160]   Here's the key that the way that this thing could infect you is that it would call using
[01:42:05.160 --> 01:42:08.640]   the call feature.
[01:42:08.640 --> 01:42:10.960]   You didn't even have to pick up and receive the call.
[01:42:10.960 --> 01:42:13.560]   All you had to do was be called.
[01:42:13.560 --> 01:42:14.800]   Your device would be infected.
[01:42:14.800 --> 01:42:19.320]   By the way, if you didn't notice that the phone came in, it would often be deleted from
[01:42:19.320 --> 01:42:20.800]   your call records later.
[01:42:20.800 --> 01:42:25.280]   He would see these calls come in from Sweden and then notice that an hour or two later,
[01:42:25.280 --> 01:42:26.800]   it wasn't in his call-up.
[01:42:26.800 --> 01:42:28.280]   It was very suspicious.
[01:42:28.280 --> 01:42:33.640]   He was already aware of all this because of his work with these journalists.
[01:42:33.640 --> 01:42:36.600]   Fortunately, Citizen Lab was able to find it.
[01:42:36.600 --> 01:42:40.920]   They were able to tell WhatsApp mitigated it by changing their server code and then
[01:42:40.920 --> 01:42:43.080]   updated the software.
[01:42:43.080 --> 01:42:45.280]   The good news is it's been fixed.
[01:42:45.280 --> 01:42:47.400]   Here's the story that worries me.
[01:42:47.400 --> 01:42:53.480]   A motherboard, Lorenzo Franceschi Bicciarei, Bicciarei, writing, "It's almost impossible
[01:42:53.480 --> 01:42:57.480]   to tell if your iPhone has been hacked."
[01:42:57.480 --> 01:43:03.000]   This is a weird consequence of Apple's security on the iPhone.
[01:43:03.000 --> 01:43:08.680]   There is no way, really, to look at an iPhone without jailbreaking it to see if it's been
[01:43:08.680 --> 01:43:11.880]   compromised.
[01:43:11.880 --> 01:43:20.680]   It's so locked down, you can't get a tool on it to look for strange anomalies.
[01:43:20.680 --> 01:43:25.000]   You have to jailbreak it to do so.
[01:43:25.000 --> 01:43:32.080]   They quote a researcher, this guy, Claudio Guarnieri at Amnesty International.
[01:43:32.080 --> 01:43:37.400]   He said, "Security controls have made mobile devices extremely difficult to inspect, especially
[01:43:37.400 --> 01:43:42.240]   remotely, particularly for those of us working in human rights organizations lacking access
[01:43:42.240 --> 01:43:45.120]   to adequate forensic technology because of this.
[01:43:45.120 --> 01:43:51.280]   We're rarely able to confirm infections of those who we even already suspect being targeted."
[01:43:51.280 --> 01:43:57.960]   Frankly, we are on the losing side of a disheartening asymmetry of capabilities that favors attackers
[01:43:57.960 --> 01:44:00.760]   over defenders.
[01:44:00.760 --> 01:44:06.200]   Is it possible to have something so secure that you can't tell it's been hacked?
[01:44:06.200 --> 01:44:10.720]   If it was more open, would there be more attacks in the first place that would make
[01:44:10.720 --> 01:44:12.440]   it more vulnerable in the end?
[01:44:12.440 --> 01:44:15.000]   Well, they'd be more of an out there.
[01:44:15.000 --> 01:44:22.480]   The iPhone's vaunted security was no defense against WhatsApp being hacked.
[01:44:22.480 --> 01:44:26.920]   WhatsApp hack allowed bad guys to get right in iPhones.
[01:44:26.920 --> 01:44:31.080]   It didn't work to do what it was supposed to do, and in fact, it's working to do the
[01:44:31.080 --> 01:44:32.080]   contrary.
[01:44:32.080 --> 01:44:36.280]   iOS security researchers who spoke with motherboard, I'm quoting again, agree that the iPhone
[01:44:36.280 --> 01:44:40.760]   is too locked down for its own good.
[01:44:40.760 --> 01:44:42.680]   The bad guys will find a way one way or another.
[01:44:42.680 --> 01:44:44.840]   Shouldn't we enable the good guys to do their job?
[01:44:44.840 --> 01:44:46.400]   This may be Georgia to answer your question.
[01:44:46.400 --> 01:44:50.280]   It comes back to that old argument about which is more secure, open source or close
[01:44:50.280 --> 01:44:51.280]   source.
[01:44:51.280 --> 01:44:54.080]   Similar to that, right?
[01:44:54.080 --> 01:44:59.760]   I've always said open source is more secure, even though security people and bad guys have
[01:44:59.760 --> 01:45:07.200]   equal access to the code, at least you can find issues and fix them with Apple.
[01:45:07.200 --> 01:45:10.080]   You have no choice but to rely on Apple to find it and fix it.
[01:45:10.080 --> 01:45:18.280]   But do we have actual numbers for how many phones are hacked versus an iOS versus on Android?
[01:45:18.280 --> 01:45:25.240]   You see, that's what's interesting about this NSO stuff is that the point of this, a lot
[01:45:25.240 --> 01:45:27.160]   of articles about this made this point.
[01:45:27.160 --> 01:45:33.360]   It's unlikely that you are compromised on this because what this is, what they are in
[01:45:33.360 --> 01:45:38.240]   the business of doing is going to governments and being like, are there pesky journalists
[01:45:38.240 --> 01:45:40.520]   or human rights people that are bothering you?
[01:45:40.520 --> 01:45:44.680]   If you pay us a certain amount of money, we will get into their devices and tell you who
[01:45:44.680 --> 01:45:47.640]   they're organizing with, where they're going to be.
[01:45:47.640 --> 01:45:53.840]   In general, at least at this point, it's unlikely that anyone cares about any of our phones.
[01:45:53.840 --> 01:46:00.560]   It's like super targeted drone strike level stuff that this software can do.
[01:46:00.560 --> 01:46:06.080]   I don't have actual numbers, but they talked to Zuck Abraham, who is the founder of Zimpyrium
[01:46:06.080 --> 01:46:09.960]   and a well-known security researcher who studies iOS attacks.
[01:46:09.960 --> 01:46:14.720]   He said in the last few months he's seen a lot of targeted attacks against iPhone users.
[01:46:14.720 --> 01:46:19.560]   So many that it's mind-blowing, but he wouldn't provide numbers.
[01:46:19.560 --> 01:46:24.000]   The point here is, I think a lot of people, I hear it all the time, "Oh, I'd never go
[01:46:24.000 --> 01:46:25.520]   to Defcom with an Android phone.
[01:46:25.520 --> 01:46:27.200]   I only carry iOS."
[01:46:27.200 --> 01:46:31.000]   Even Phil Zimmerman, the creator PGP, told me that.
[01:46:31.000 --> 01:46:33.280]   Maybe that's a fool's paradise.
[01:46:33.280 --> 01:46:38.280]   You know what I wonder, I thought of the Bezos phone situation.
[01:46:38.280 --> 01:46:43.280]   I know that that was just straight SMS, but I'm wondering, like this idea, I don't know
[01:46:43.280 --> 01:46:45.200]   if any of you are in the security.
[01:46:45.200 --> 01:46:47.320]   Now that's the question with the Bezos phone.
[01:46:47.320 --> 01:46:50.920]   Was it really the Saudis and not his girlfriend's brother?
[01:46:50.920 --> 01:46:51.920]   Right.
[01:46:51.920 --> 01:46:56.920]   Well, this is what I'm saying, because that apparently is one of the big clients of NSO.
[01:46:56.920 --> 01:46:57.920]   Yeah.
[01:46:57.920 --> 01:47:02.000]   But it's okay, because we only provide our software to governments.
[01:47:02.000 --> 01:47:06.040]   Well, but okay, so what I'm going to ask is, does anyone, did any of your security shows
[01:47:06.040 --> 01:47:07.040]   get into?
[01:47:07.040 --> 01:47:12.160]   The thing that was weird to me about this is like, even as I've learned, all my life
[01:47:12.160 --> 01:47:16.440]   dealing with spam and things like that, you only really get infected if you click a link.
[01:47:16.440 --> 01:47:20.880]   You only get infected if you download the file, execute the file.
[01:47:20.880 --> 01:47:23.880]   Like this idea that you get into those days are over.
[01:47:23.880 --> 01:47:26.320]   That is so old fashioned, Brian.
[01:47:26.320 --> 01:47:30.480]   So now all I have to do is turn my phone on and there's a conductor.
[01:47:30.480 --> 01:47:31.480]   Yeah.
[01:47:31.480 --> 01:47:32.480]   That's a good zero day, right?
[01:47:32.480 --> 01:47:35.520]   All I have to do is call you and you're done.
[01:47:35.520 --> 01:47:37.240]   I own your phone.
[01:47:37.240 --> 01:47:40.640]   So that's what I'm saying is that apparently we maybe need to be thinking that that's out
[01:47:40.640 --> 01:47:45.520]   there and that's just the state of the world right now is that you connect to the network
[01:47:45.520 --> 01:47:48.200]   and you're compromised.
[01:47:48.200 --> 01:47:50.800]   Jonathan Levin, this is again from the motherboard article.
[01:47:50.800 --> 01:47:54.720]   A researcher has written books about iOS and macOS internals and security and provides
[01:47:54.720 --> 01:47:56.880]   training on iPhone security.
[01:47:56.880 --> 01:48:04.080]   Said the reason we know of so few iOS zero days is because they're worth so much money
[01:48:04.080 --> 01:48:05.480]   that only a government can afford them.
[01:48:05.480 --> 01:48:07.480]   They're for targeted attacks.
[01:48:07.480 --> 01:48:11.480]   Everybody's going to use a zero day to try to get into everybody's phone and get, you
[01:48:11.480 --> 01:48:16.120]   know, it's, I mean, in some ways I should reassure you because, but I think it's, you
[01:48:16.120 --> 01:48:18.520]   know, there's not mass attacks.
[01:48:18.520 --> 01:48:26.280]   You know, you have to be some target, a person of interest to the Saudis or somebody to exacerbate
[01:48:26.280 --> 01:48:27.280]   the situation.
[01:48:27.280 --> 01:48:32.580]   He says payloads are often tested and perfected for weeks or more before deployment thus ensuring
[01:48:32.580 --> 01:48:38.380]   a high chance of exploitation and inversely a low chance of detection, especially in the
[01:48:38.380 --> 01:48:42.380]   case of attacks requiring no user interaction.
[01:48:42.380 --> 01:48:47.660]   But again, other researchers say the problem is there's no practical way to tell an iPhone
[01:48:47.660 --> 01:48:48.660]   got infected.
[01:48:48.660 --> 01:48:54.260]   And of course, as we know, it's illegal to do research to reverse engineer or get into
[01:48:54.260 --> 01:48:56.900]   anything that's protected.
[01:48:56.900 --> 01:48:59.940]   And so that discourages researchers as well.
[01:48:59.940 --> 01:49:03.220]   Well, that should be changed.
[01:49:03.220 --> 01:49:06.820]   But also in this case, like you had to download the app.
[01:49:06.820 --> 01:49:10.340]   You had to have what's app on you, but yeah.
[01:49:10.340 --> 01:49:11.340]   Yeah.
[01:49:11.340 --> 01:49:12.340]   Yeah.
[01:49:12.340 --> 01:49:13.340]   So you're right.
[01:49:13.340 --> 01:49:16.340]   And I feel it would be a lot safer if you didn't use third party apps.
[01:49:16.340 --> 01:49:17.340]   Yeah.
[01:49:17.340 --> 01:49:19.540]   I'm, you know, funny thing.
[01:49:19.540 --> 01:49:20.540]   Okay.
[01:49:20.540 --> 01:49:22.180]   Here's the irony of it.
[01:49:22.180 --> 01:49:25.860]   Here's the irony of it.
[01:49:25.860 --> 01:49:31.980]   People like, you know, these, these NGOs and people are going to not use iMessage because
[01:49:31.980 --> 01:49:36.820]   they know that Apple knows what's going on in iMessage and if subpoenaed might give that
[01:49:36.820 --> 01:49:40.580]   information to the US government or whatever government that you're in, whether country
[01:49:40.580 --> 01:49:41.820]   you're in.
[01:49:41.820 --> 01:49:46.580]   So they want to use a third party that is encrypted and everybody thought, Oh, what's
[01:49:46.580 --> 01:49:47.580]   app?
[01:49:47.580 --> 01:49:48.580]   It's encrypted end to end.
[01:49:48.580 --> 01:49:50.380]   It's safe.
[01:49:50.380 --> 01:49:55.740]   Not not thinking about the fact that nothing is safe if your phone is owned and is a turned
[01:49:55.740 --> 01:49:59.700]   out, what's app was the vector to get your phone owned.
[01:49:59.700 --> 01:50:06.220]   And also if it's a call, isn't it using some lower or higher level basic call functionality
[01:50:06.220 --> 01:50:07.380]   in iOS?
[01:50:07.380 --> 01:50:12.060]   So in theory, anything that could receive a call could be vulnerable.
[01:50:12.060 --> 01:50:13.060]   Probably.
[01:50:13.060 --> 01:50:14.060]   Yeah.
[01:50:14.060 --> 01:50:15.060]   I hope app.
[01:50:15.060 --> 01:50:17.580]   Well, I mean, obviously Apple's going to fix this, but another security researcher who
[01:50:17.580 --> 01:50:23.460]   declined to be quoted said the fundamental problem is that I, or to be named rather is
[01:50:23.460 --> 01:50:28.580]   that iOS is a bug rich environment and Apple's strategy only works against hobbyist attackers,
[01:50:28.580 --> 01:50:32.500]   but it's quite counterproductive against professional attackers.
[01:50:32.500 --> 01:50:35.260]   So now, I mean, take this with a grain of salt.
[01:50:35.260 --> 01:50:36.300]   Everybody making their own decisions.
[01:50:36.300 --> 01:50:43.820]   I would say that anytime you're using a computer of any kind, you're always at risk.
[01:50:43.820 --> 01:50:46.700]   And you're never 100% safe, right?
[01:50:46.700 --> 01:50:50.940]   Is it really salty of me to say that anything that any company that is owned by Facebook,
[01:50:50.940 --> 01:50:53.260]   is probably going to be some sort of security risk.
[01:50:53.260 --> 01:50:55.060]   No, I don't think that's the case.
[01:50:55.060 --> 01:50:58.940]   Although I would, I mean, if you're really an activist, don't use WhatsApp use signal.
[01:50:58.940 --> 01:51:03.740]   If again, if we're going to look at all of the stats, Facebook has not been great for
[01:51:03.740 --> 01:51:04.740]   security.
[01:51:04.740 --> 01:51:05.740]   Absolutely.
[01:51:05.740 --> 01:51:06.740]   No.
[01:51:06.740 --> 01:51:08.180]   You know, you swim with the sharks.
[01:51:08.180 --> 01:51:09.180]   Yep.
[01:51:09.180 --> 01:51:10.180]   Absolutely.
[01:51:10.180 --> 01:51:12.980]   I love this.
[01:51:12.980 --> 01:51:20.220]   Just a word of warning, ProPublica, which is a great citizens news organization, published
[01:51:20.220 --> 01:51:21.220]   an article.
[01:51:21.220 --> 01:51:25.260]   Actually, it came, it was also published in The Guardian.
[01:51:25.260 --> 01:51:31.140]   Firms have promised high tech ransomware solutions almost always just pay the hackers.
[01:51:31.140 --> 01:51:36.300]   So if you're in business and you've been hit by ransomware and you go out and you hire
[01:51:36.300 --> 01:51:43.500]   a data recovery firm, almost always, they just send the hacker the money and hope that
[01:51:43.500 --> 01:51:45.420]   that'll all work out.
[01:51:45.420 --> 01:51:47.420]   But are they telling the client?
[01:51:47.420 --> 01:51:48.420]   No.
[01:51:48.420 --> 01:51:49.420]   Right.
[01:51:49.420 --> 01:51:55.220]   So you know that, you know that, that retainer that you have been paying for all these years
[01:51:55.220 --> 01:51:56.220]   to charge out?
[01:51:56.220 --> 01:51:58.460]   It was enough to pay the rent.
[01:51:58.460 --> 01:51:59.460]   Exactly.
[01:51:59.460 --> 01:52:03.900]   The way ProPublica found out, they, you know, people think Bitcoin transactions are anonymous.
[01:52:03.900 --> 01:52:04.900]   Actually, not.
[01:52:04.900 --> 01:52:07.340]   You can trace them back to the wallet if you get the number.
[01:52:07.340 --> 01:52:12.540]   And so ProPublica was able to trace four of the payments set in 2017 and 2018 from an
[01:52:12.540 --> 01:52:18.980]   online wallet controlled by proven data to ones specified by the hackers.
[01:52:18.980 --> 01:52:22.700]   The money was then laundered through as many as 12 Bitcoin addresses before reaching a
[01:52:22.700 --> 01:52:27.620]   wallet maintained by the Iranians.
[01:52:27.620 --> 01:52:28.620]   You know what Leo, hold on.
[01:52:28.620 --> 01:52:32.380]   I just, I think we should send a carrier and hit those Iranians.
[01:52:32.380 --> 01:52:33.380]   I'm sorry.
[01:52:33.380 --> 01:52:34.740]   But some just occurred to me.
[01:52:34.740 --> 01:52:37.020]   Isn't that the definition of insurance though?
[01:52:37.020 --> 01:52:43.140]   If you charge enough people, enough money when the house burns down, you're able to have
[01:52:43.140 --> 01:52:44.820]   enough money to reach the house, right?
[01:52:44.820 --> 01:52:45.820]   True.
[01:52:45.820 --> 01:52:47.900]   It's just that they're not telling people that that's what they're doing.
[01:52:47.900 --> 01:52:51.460]   Now, they're actually saying, well, this is more ethical than paying that they're actually
[01:52:51.460 --> 01:52:54.820]   claiming to offer an ethical way, not paying the hackers.
[01:52:54.820 --> 01:52:58.940]   Most law enforcement says, whatever you do, don't send money to the hackers, partly because
[01:52:58.940 --> 01:53:01.900]   it just encourages them, partly because chances are good.
[01:53:01.900 --> 01:53:05.780]   You're not going to actually get your, an encryption key.
[01:53:05.780 --> 01:53:10.100]   Often they don't, you know, that's just, that's just taking a chance.
[01:53:10.100 --> 01:53:11.100]   What are you going to do?
[01:53:11.100 --> 01:53:19.020]   Hey, I paid you, where's my key?
[01:53:19.020 --> 01:53:21.340]   Sam Sam was the ransomware.
[01:53:21.340 --> 01:53:27.940]   And in fact, last November, the DOJ indicted two Iranians on fraud charges for developing
[01:53:27.940 --> 01:53:34.700]   Sam Sam and orchestrating the extortion.
[01:53:34.700 --> 01:53:40.700]   So anyway, just, you know, passing that along, if you, I guess you could think of it as insurance.
[01:53:40.700 --> 01:53:46.780]   But it seems like you might have expected more than, you know, I just get a good backup
[01:53:46.780 --> 01:53:50.020]   program, please.
[01:53:50.020 --> 01:53:51.980]   Zombie load.
[01:53:51.980 --> 01:53:56.060]   There's, you know, you thought specter and meltdown were over and we've got the fixes.
[01:53:56.060 --> 01:53:57.540]   Now there's a new one.
[01:53:57.540 --> 01:54:07.020]   Same thing, speculative execution flaw hits Intel chips, all but the most recent two generations.
[01:54:07.020 --> 01:54:09.140]   It's being patched, of course.
[01:54:09.140 --> 01:54:10.140]   Here's the bad news.
[01:54:10.140 --> 01:54:14.940]   Like some other speculative execution patches, it could hit your performance by up to what?
[01:54:14.940 --> 01:54:15.940]   40%.
[01:54:15.940 --> 01:54:23.460]   No, I saw earlier this week, I thought it was only three to nine percent, but Apple says
[01:54:23.460 --> 01:54:24.460]   three percent.
[01:54:24.460 --> 01:54:25.460]   Yeah.
[01:54:25.460 --> 01:54:26.980]   Well, actually no, big difference.
[01:54:26.980 --> 01:54:27.980]   Yeah.
[01:54:27.980 --> 01:54:32.700]   During testing this month, Apple says it found as much as a 40% reduction in performance depends
[01:54:32.700 --> 01:54:38.740]   on what you're doing.
[01:54:38.740 --> 01:54:44.420]   So you'll slowly get, really, I think as an end user, none of these speculative execution
[01:54:44.420 --> 01:54:47.620]   texts are too much to worry about.
[01:54:47.620 --> 01:54:51.500]   Unless again, you're a human rights worker or something like that.
[01:54:51.500 --> 01:54:54.860]   More often it's a problem in the data center where you're on a shared processor than many
[01:54:54.860 --> 01:54:56.900]   people using the same processor.
[01:54:56.900 --> 01:55:00.900]   If one of them is malicious, he could then get information from the others.
[01:55:00.900 --> 01:55:06.660]   Well, and also even a three to nine percent hit in a data center would kind of be a problem.
[01:55:06.660 --> 01:55:07.660]   Well, that's the problem.
[01:55:07.660 --> 01:55:10.340]   That's the problem, isn't it?
[01:55:10.340 --> 01:55:11.340]   Yeah.
[01:55:11.340 --> 01:55:15.820]   So as long as we're talking the annals of hacking, the big hack that went down in San
[01:55:15.820 --> 01:55:24.340]   Francisco this week, a top California executive has been arrested by the FBI.
[01:55:24.340 --> 01:55:33.100]   Chief financial officer of a school lunch program called Choice Lunch was arrested.
[01:55:33.100 --> 01:55:38.860]   Finally he grabbed details from the lunch master by hacking them on finding out what
[01:55:38.860 --> 01:55:45.620]   youngsters across the San Francisco Bay Area like to eat and are allergic to.
[01:55:45.620 --> 01:55:50.460]   It is apparently a war out there between the lunch master and Choice Lunch.
[01:55:50.460 --> 01:55:52.460]   Wait, hold on.
[01:55:52.460 --> 01:55:55.260]   What are we talking about?
[01:55:55.260 --> 01:55:56.660]   Are these startups?
[01:55:56.660 --> 01:55:57.660]   No.
[01:55:57.660 --> 01:55:58.660]   Yeah.
[01:55:58.660 --> 01:55:59.660]   Yeah.
[01:55:59.660 --> 01:56:00.660]   It's not.
[01:56:00.660 --> 01:56:01.660]   There's nothing that's said.
[01:56:01.660 --> 01:56:05.140]   I'm a lunch master and I'm thinking, is that a title in the school system?
[01:56:05.140 --> 01:56:07.140]   I'm the one that's on the menu.
[01:56:07.140 --> 01:56:08.660]   Who's the key master?
[01:56:08.660 --> 01:56:11.300]   I am Gossa, the lunch master.
[01:56:11.300 --> 01:56:12.300]   Exactly.
[01:56:12.300 --> 01:56:22.700]   I was upset when I found out that all of my buying history was out and Google knows it.
[01:56:22.700 --> 01:56:24.460]   Oh, well, you knew that.
[01:56:24.460 --> 01:56:28.500]   You thought that was really, I was really upset.
[01:56:28.500 --> 01:56:29.660]   I was really upset.
[01:56:29.660 --> 01:56:30.660]   I was like, no.
[01:56:30.660 --> 01:56:32.540]   Because you're running it all through Gmail.
[01:56:32.540 --> 01:56:33.540]   I know.
[01:56:33.540 --> 01:56:34.540]   I know.
[01:56:34.540 --> 01:56:35.540]   I should have known better.
[01:56:35.540 --> 01:56:36.540]   I was like, you know what?
[01:56:36.540 --> 01:56:37.540]   Here I am.
[01:56:37.540 --> 01:56:38.540]   Duck, duck, go in and.
[01:56:38.540 --> 01:56:40.540]   Yeah, but do you still get stuff sent to Gmail?
[01:56:40.540 --> 01:56:41.540]   I stopped using Gmail.
[01:56:41.540 --> 01:56:42.540]   Google.
[01:56:42.540 --> 01:56:43.540]   Google stopped using.
[01:56:43.540 --> 01:56:44.540]   Google has been for us.
[01:56:44.540 --> 01:56:45.540]   Yeah.
[01:56:45.540 --> 01:56:47.180]   But it's also convenient.
[01:56:47.180 --> 01:56:52.260]   Now, wait a minute, before you get too upset, I stopped using Gmail for a variety of reasons.
[01:56:52.260 --> 01:56:56.940]   They did say a few years ago we're no longer going to scan emails for advertising keywords,
[01:56:56.940 --> 01:56:57.940]   right?
[01:56:57.940 --> 01:57:02.340]   But, and see, this I think is a misleading headline.
[01:57:02.340 --> 01:57:09.420]   Google has been tracking nearly everything you buy online because, well, for instance,
[01:57:09.420 --> 01:57:13.220]   Google will notify you when a package is on its way, right?
[01:57:13.220 --> 01:57:15.460]   Google will wait for the airport.
[01:57:15.460 --> 01:57:16.460]   Yeah.
[01:57:16.460 --> 01:57:19.100]   Google uses this information for you, Georgia.
[01:57:19.100 --> 01:57:22.100]   No, no, it's not giving me anything.
[01:57:22.100 --> 01:57:23.700]   No, I lies.
[01:57:23.700 --> 01:57:24.700]   These are lies.
[01:57:24.700 --> 01:57:28.140]   I like getting anything.
[01:57:28.140 --> 01:57:34.140]   I'll open Google and it'll say, yeah, your package is on its way.
[01:57:34.140 --> 01:57:35.140]   That's great.
[01:57:35.140 --> 01:57:36.300]   No, I don't want to know.
[01:57:36.300 --> 01:57:37.300]   I don't want to know.
[01:57:37.300 --> 01:57:38.300]   I don't want them tracking why.
[01:57:38.300 --> 01:57:39.540]   I don't want them to know.
[01:57:39.540 --> 01:57:41.020]   I don't want them to know.
[01:57:41.020 --> 01:57:42.780]   I was really kind of creeped out.
[01:57:42.780 --> 01:57:49.460]   I was like, really, I need to now add another layer to keep what I buy, you know, if I buy
[01:57:49.460 --> 01:57:51.420]   some, you know, a board.
[01:57:51.420 --> 01:57:52.860]   I don't want them to know.
[01:57:52.860 --> 01:57:55.020]   Well, in some cases, it's useful.
[01:57:55.020 --> 01:58:07.380]   For instance, apparently somebody last week bought two Blackberry Basil lemonades at DoorDash.
[01:58:07.380 --> 01:58:12.820]   And the only reason I know this and a cheesecake waffle and a strawberry Nutella waffle.
[01:58:12.820 --> 01:58:15.260]   Now, I don't know who did it.
[01:58:15.260 --> 01:58:16.860]   It wasn't me.
[01:58:16.860 --> 01:58:20.540]   But the only reason I know this is because my DoorDash is apparently connected to my
[01:58:20.540 --> 01:58:21.700]   Gmail account.
[01:58:21.700 --> 01:58:25.860]   Meanwhile, there are all these things that I purchased that are not in my Google account
[01:58:25.860 --> 01:58:26.860]   details.
[01:58:26.860 --> 01:58:32.420]   But at least I know somebody who's sneaking a midnight snack.
[01:58:32.420 --> 01:58:40.500]   Yes, somebody apparently named Snowleader bought some Stoney HS pants, Men Marine in Switzerland.
[01:58:40.500 --> 01:58:41.940]   This is an error, by the way.
[01:58:41.940 --> 01:58:42.940]   This is not.
[01:58:42.940 --> 01:58:44.340]   This is in my Google.
[01:58:44.340 --> 01:58:51.420]   But because my last name is LaPorte, I get a lot of mail to people, you know, like Joey
[01:58:51.420 --> 01:58:53.660]   LaPorte at Gmail.com.
[01:58:53.660 --> 01:58:57.580]   They put a space, you know, Snowleader put a space between Joey and LaPorte and I get
[01:58:57.580 --> 01:58:58.900]   the email.
[01:58:58.900 --> 01:59:03.780]   So that's the other thing is a lot of this stuff is not right.
[01:59:03.780 --> 01:59:04.780]   Mine was right.
[01:59:04.780 --> 01:59:06.140]   I was not happy.
[01:59:06.140 --> 01:59:12.700]   I am worried about who bought a couple of pairs of Nike Air Force ones.
[01:59:12.700 --> 01:59:14.460]   But you think that's useful to know?
[01:59:14.460 --> 01:59:15.460]   No.
[01:59:15.460 --> 01:59:20.140]   And by the way, I do want to point out that the whole this whole story is that people discovered
[01:59:20.140 --> 01:59:23.220]   there's a tool on Google to tell you this.
[01:59:23.220 --> 01:59:24.740]   I'll tell you this.
[01:59:24.740 --> 01:59:28.980]   So it's not like they hacked in and you're like, Oh my God, look at their.
[01:59:28.980 --> 01:59:31.980]   No, Google has a tool that says, by the way, here's all of your order history.
[01:59:31.980 --> 01:59:33.980]   So they weren't hiding it.
[01:59:33.980 --> 01:59:36.180]   They just didn't publicize it, I guess.
[01:59:36.180 --> 01:59:41.820]   I just want to say to Alex Tran of the Chub Corporation in Houston, Texas, I see your Air
[01:59:41.820 --> 01:59:47.940]   Force ones, they'll be there May 29th.
[01:59:47.940 --> 01:59:50.420]   I don't know how this even got in my Gmail.
[01:59:50.420 --> 01:59:51.420]   It's not me.
[01:59:51.420 --> 01:59:52.420]   And I don't think it's my credit card.
[01:59:52.420 --> 01:59:56.620]   It's just, you know, it's junk mail.
[01:59:56.620 --> 01:59:58.420]   Because nobody I know wears Air Force ones.
[01:59:58.420 --> 01:59:59.820]   That's so 1990s.
[01:59:59.820 --> 02:00:09.860]   By the way, as long as we're talking about Google and security.
[02:00:09.860 --> 02:00:12.180]   I've had my microphone muted and I'm sorry.
[02:00:12.180 --> 02:00:14.180]   Oh, we're you.
[02:00:14.180 --> 02:00:18.420]   Hello, hello, sweet voice stranger.
[02:00:18.420 --> 02:00:20.900]   What would you like to say?
[02:00:20.900 --> 02:00:22.500]   I did not know.
[02:00:22.500 --> 02:00:25.420]   Am I the only one who's just who's a weirdo?
[02:00:25.420 --> 02:00:27.580]   Because I'm not I'm not terribly creeped out by this.
[02:00:27.580 --> 02:00:30.020]   I mean, I'm not creeped out either.
[02:00:30.020 --> 02:00:31.220]   You should be.
[02:00:31.220 --> 02:00:32.340]   I probably should be.
[02:00:32.340 --> 02:00:33.580]   I mean, I'll take that argument.
[02:00:33.580 --> 02:00:37.580]   But I feel like everyone is the previous version of this argument was like, Oh, it's
[02:00:37.580 --> 02:00:41.180]   creepy that ads are getting so specific and so tailored to me.
[02:00:41.180 --> 02:00:44.980]   And I've always been sitting there like, I mean, I spend a little bit more because the
[02:00:44.980 --> 02:00:50.620]   Instagram ads give me give me vests that I like now instead of things that do not appeal
[02:00:50.620 --> 02:00:54.460]   to me, like boat covers, you know, like, why is it good?
[02:00:54.460 --> 02:00:55.460]   That's a good thing.
[02:00:55.460 --> 02:00:57.980]   Yeah, I'm not interested in boat covers.
[02:00:57.980 --> 02:00:58.980]   Right.
[02:00:58.980 --> 02:01:00.740]   I understand that it's a simplistic thing.
[02:01:00.740 --> 02:01:04.340]   But for me, if as a user, I am helped by more relevant ads.
[02:01:04.340 --> 02:01:05.700]   What is the harm, Georgia?
[02:01:05.700 --> 02:01:07.540]   Why do you not want?
[02:01:07.540 --> 02:01:09.780]   It's reflecting your actual tastes.
[02:01:09.780 --> 02:01:10.780]   It's creep.
[02:01:10.780 --> 02:01:14.580]   Why is if you're going to be selling my information, I want a piece.
[02:01:14.580 --> 02:01:17.500]   First of all, I just I just want a piece of the pie.
[02:01:17.500 --> 02:01:18.500]   You're making millions.
[02:01:18.500 --> 02:01:22.780]   I take a piece when you go to Google, you get war, you know, you get alerts that your
[02:01:22.780 --> 02:01:23.780]   package is on the way.
[02:01:23.780 --> 02:01:25.740]   I'm not finding anything.
[02:01:25.740 --> 02:01:27.580]   I am using a subpar search.
[02:01:27.580 --> 02:01:29.380]   Well, you're using GMA.
[02:01:29.380 --> 02:01:31.700]   And like I said, I don't get much of this.
[02:01:31.700 --> 02:01:33.500]   Yeah, fair enough.
[02:01:33.500 --> 02:01:34.500]   Fair enough.
[02:01:34.500 --> 02:01:35.500]   Fair enough.
[02:01:35.500 --> 02:01:37.740]   What did you pay for Gmail last year?
[02:01:37.740 --> 02:01:39.260]   True, true.
[02:01:39.260 --> 02:01:42.940]   Well, after this, I'm like, I'm going to switch off.
[02:01:42.940 --> 02:01:47.380]   Like my Gmail one is actually the one for work, sadly.
[02:01:47.380 --> 02:01:49.380]   But the other thing is, is it makes these people...
[02:01:49.380 --> 02:01:51.260]   So you're saying it's your husband that's buying all this stuff?
[02:01:51.260 --> 02:01:52.260]   No, no, it's me.
[02:01:52.260 --> 02:01:53.260]   It's me.
[02:01:53.260 --> 02:01:55.540]   It's just it goes to on his Amazon account.
[02:01:55.540 --> 02:01:56.940]   So yeah.
[02:01:56.940 --> 02:02:02.740]   But if someone was following you around and taking notes about, oh, eight, three eggs today.
[02:02:02.740 --> 02:02:04.700]   Oh, look at what they did this.
[02:02:04.700 --> 02:02:05.700]   That's creepy.
[02:02:05.700 --> 02:02:06.700]   That's creepy.
[02:02:06.700 --> 02:02:07.700]   That is creepy.
[02:02:07.700 --> 02:02:09.620]   But that's a human doing that.
[02:02:09.620 --> 02:02:10.700]   This is just a machine.
[02:02:10.700 --> 02:02:15.700]   That would be, but just because it's a machine, it is still your private information.
[02:02:15.700 --> 02:02:16.700]   And that's...
[02:02:16.700 --> 02:02:19.420]   If that person...
[02:02:19.420 --> 02:02:23.460]   I mean, if they at the end of the day then came over and said, "Hey, I noticed that you
[02:02:23.460 --> 02:02:26.860]   were having trouble last week because you couldn't decide what to buy."
[02:02:26.860 --> 02:02:28.340]   Like here, I found these things for you.
[02:02:28.340 --> 02:02:29.340]   You didn't know existed.
[02:02:29.340 --> 02:02:30.340]   Here they are.
[02:02:30.340 --> 02:02:32.460]   I've set up a storefront for you right here in front of your house.
[02:02:32.460 --> 02:02:34.660]   I see you're at the airport.
[02:02:34.660 --> 02:02:37.220]   The flight leaves from gate four.
[02:02:37.220 --> 02:02:38.980]   That's great.
[02:02:38.980 --> 02:02:45.380]   I send all every time I book a flight or a hotel or anything, I email it to trip it.
[02:02:45.380 --> 02:02:48.580]   I used to have trip it, scan my Gmail, but I don't use Gmail anymore, but that's fine.
[02:02:48.580 --> 02:02:50.340]   I just email it to trip it.
[02:02:50.340 --> 02:02:52.140]   They know all my travel plans.
[02:02:52.140 --> 02:02:53.140]   What's wrong with that?
[02:02:53.140 --> 02:02:55.860]   I like it because then I have a trip it.
[02:02:55.860 --> 02:02:56.860]   It tells me...
[02:02:56.860 --> 02:02:57.860]   No, I don't like any of it.
[02:02:57.860 --> 02:02:58.860]   And maybe it's a female thing.
[02:02:58.860 --> 02:03:00.180]   Maybe I just like my private...
[02:03:00.180 --> 02:03:03.340]   Like the thought of someone suddenly saying, "Oh, look at all..."
[02:03:03.340 --> 02:03:07.540]   And suddenly when I see I surf read it and I see all these therapy ads, I'm like, "Wow,
[02:03:07.540 --> 02:03:08.540]   therapy's really popular."
[02:03:08.540 --> 02:03:09.820]   And then I'm like, "Oh, no, wait.
[02:03:09.820 --> 02:03:11.540]   That's probably because of me."
[02:03:11.540 --> 02:03:12.740]   Well, it's reasonable.
[02:03:12.740 --> 02:03:13.740]   Women, women...
[02:03:13.740 --> 02:03:14.740]   I don't...
[02:03:14.740 --> 02:03:17.580]   I have a quite reasonable fear of being stalked.
[02:03:17.580 --> 02:03:19.420]   So I think that could be.
[02:03:19.420 --> 02:03:20.420]   Yeah.
[02:03:20.420 --> 02:03:21.420]   It could be.
[02:03:21.420 --> 02:03:22.420]   And it could just be personal.
[02:03:22.420 --> 02:03:23.420]   Maybe I'm just...
[02:03:23.420 --> 02:03:24.780]   I like to be private.
[02:03:24.780 --> 02:03:30.180]   I like to not have everyone know what I'm doing, even if it's a large conglomerate company that's
[02:03:30.180 --> 02:03:32.380]   using this to sell me better things.
[02:03:32.380 --> 02:03:39.860]   And I think that that's maybe one of the key reasonable arguments here too is essentially
[02:03:39.860 --> 02:03:44.340]   what you're saying is that you can't opt out at this point.
[02:03:44.340 --> 02:03:45.340]   You know what I mean?
[02:03:45.340 --> 02:03:51.340]   Like, so Georgia, if you're like, "I don't like this," but how can you live a modern life
[02:03:51.340 --> 02:03:53.260]   and completely wall it off?
[02:03:53.260 --> 02:03:54.260]   You know what I mean?
[02:03:54.260 --> 02:04:00.500]   So, it's almost like you have no agency anymore or you have to go through so many insane hoops
[02:04:00.500 --> 02:04:02.460]   if that's your choice.
[02:04:02.460 --> 02:04:05.300]   So like, even if it's just, "Oh, it's just...
[02:04:05.300 --> 02:04:08.660]   It's a personal preference of mine," but you just don't really have the ability to do
[02:04:08.660 --> 02:04:13.940]   that anymore because, you know, your trip emails have to go somewhere and that's owned
[02:04:13.940 --> 02:04:15.860]   by one of five companies.
[02:04:15.860 --> 02:04:25.700]   Georgia, so then to me, the notion is I like the idea that my world will be customized
[02:04:25.700 --> 02:04:29.460]   for me based on what I do and my preferences.
[02:04:29.460 --> 02:04:31.260]   I think that's kind of cool.
[02:04:31.260 --> 02:04:35.260]   For instance, Uber has just launched a quiet mode setting.
[02:04:35.260 --> 02:04:39.860]   When you book a car, you could say, "And by the way, I want them to shut up."
[02:04:39.860 --> 02:04:45.300]   I like that and I would like Uber to notice that I use it all the time and then from now
[02:04:45.300 --> 02:04:48.020]   on, tell the driver.
[02:04:48.020 --> 02:04:49.620]   What's wrong with that?
[02:04:49.620 --> 02:04:56.500]   If I was asked, I think that consent is a big issue in this for me in that if I was asked,
[02:04:56.500 --> 02:04:59.340]   "Would it be okay if we followed this?"
[02:04:59.340 --> 02:05:02.940]   If it was more transparent, I think I would be more comfortable with it as well.
[02:05:02.940 --> 02:05:07.060]   So I think transparency, I think consent, I think that...
[02:05:07.060 --> 02:05:08.420]   I also think, "You know what?
[02:05:08.420 --> 02:05:11.580]   Then what else are they tracking and following that I don't know of?"
[02:05:11.580 --> 02:05:17.100]   And I know that that would be even more terrifying because so many articles come out that we find
[02:05:17.100 --> 02:05:21.180]   out that they know even when your phone is off, where you are going or they know where
[02:05:21.180 --> 02:05:25.860]   you've stopped off for this long or what you've been doing in certain places.
[02:05:25.860 --> 02:05:30.820]   And so I think that those two pieces are quite weak in the companies and there are no laws
[02:05:30.820 --> 02:05:35.620]   that are in place to make sure that they're more transparent and that they ask for specific
[02:05:35.620 --> 02:05:41.500]   consent before they track things, what they track and what they do with them.
[02:05:41.500 --> 02:05:44.660]   And so I don't think that there's going to be a real easy solution for that and I think
[02:05:44.660 --> 02:05:47.860]   that it's still going to be creepy and every time I find out.
[02:05:47.860 --> 02:05:48.860]   And I read the privacy.
[02:05:48.860 --> 02:05:54.060]   I read these long, difficult read before and that's why I don't use a whole bunch of products
[02:05:54.060 --> 02:05:56.380]   that would make my life more easy.
[02:05:56.380 --> 02:05:58.940]   But I just don't think it can be creeped out.
[02:05:58.940 --> 02:06:03.060]   Totally support your right to opt out for that and I think everyone should have the right
[02:06:03.060 --> 02:06:04.420]   to opt out.
[02:06:04.420 --> 02:06:05.420]   What I don't...
[02:06:05.420 --> 02:06:06.420]   Easily.
[02:06:06.420 --> 02:06:07.420]   Yeah.
[02:06:07.420 --> 02:06:08.420]   Easily opt out.
[02:06:08.420 --> 02:06:09.420]   Disclosure.
[02:06:09.420 --> 02:06:10.780]   I agree with all that stuff.
[02:06:10.780 --> 02:06:17.180]   What I don't want is for government to come along or the consumer outrage to be so loud
[02:06:17.180 --> 02:06:21.100]   that companies stop doing this because I personally like it.
[02:06:21.100 --> 02:06:23.580]   And apparently, so does Michael.
[02:06:23.580 --> 02:06:28.540]   So I don't want to be prevented from using this because some people don't like it.
[02:06:28.540 --> 02:06:30.100]   So let's agree.
[02:06:30.100 --> 02:06:32.220]   It should be a personal choice.
[02:06:32.220 --> 02:06:36.380]   You should be disclosed and you should have explicit opt out choices.
[02:06:36.380 --> 02:06:37.380]   I think that's...
[02:06:37.380 --> 02:06:38.700]   I would like the idea to be forgotten.
[02:06:38.700 --> 02:06:39.780]   I like that idea.
[02:06:39.780 --> 02:06:45.060]   I think the EU has a really nice way of if you want to, you should be able to be forgotten
[02:06:45.060 --> 02:06:47.740]   and the things that you do should not be tracked.
[02:06:47.740 --> 02:06:49.420]   Because that breaks the internet.
[02:06:49.420 --> 02:06:54.420]   It's like saying, "Well, you did something.
[02:06:54.420 --> 02:06:57.420]   It's history, but you should have the right to erase history.
[02:06:57.420 --> 02:06:58.420]   I don't think so."
[02:06:58.420 --> 02:07:03.860]   Well, let's say the huge, maybe not what's on webpages, but all of the companies of what
[02:07:03.860 --> 02:07:09.220]   they track for you, you should have specifics that if I want them to forget everything.
[02:07:09.220 --> 02:07:10.780]   I would like to be able to do that.
[02:07:10.780 --> 02:07:12.980]   And whenever you sign up to fake...
[02:07:12.980 --> 02:07:17.660]   Google gives you that, by the way, you can go right to that dashboard and delete it.
[02:07:17.660 --> 02:07:19.300]   Yeah.
[02:07:19.300 --> 02:07:24.820]   Only in certain narrow windows, you'll notice that they can...
[02:07:24.820 --> 02:07:27.740]   After three months, you can opt out, remember.
[02:07:27.740 --> 02:07:32.060]   So they're only giving you the stuff that is not super, super valuable to them.
[02:07:32.060 --> 02:07:33.060]   Yeah.
[02:07:33.060 --> 02:07:36.660]   And isn't Facebook like you opt out, but it's always there forever, really?
[02:07:36.660 --> 02:07:38.420]   Well, you know what?
[02:07:38.420 --> 02:07:42.020]   Let me come back to your original point about you want to taste.
[02:07:42.020 --> 02:07:43.020]   If they...
[02:07:43.020 --> 02:07:44.300]   This is the problem.
[02:07:44.300 --> 02:07:45.620]   Let me rephrase it a different way.
[02:07:45.620 --> 02:07:51.060]   If they discover tomorrow that there's some enzyme in human breath that is more powerful
[02:07:51.060 --> 02:07:55.300]   than gold, and we just need to mine it and collect it.
[02:07:55.300 --> 02:07:59.220]   And so here, Brian, put this mask on and we're going to...
[02:07:59.220 --> 02:08:00.980]   Because I can't stop breathing.
[02:08:00.980 --> 02:08:02.300]   And that's sort of like what the data is now.
[02:08:02.300 --> 02:08:07.700]   I can't stop emitting data that all of these companies are monetizing without my consent
[02:08:07.700 --> 02:08:09.340]   or without giving me a taste.
[02:08:09.340 --> 02:08:13.980]   If they ask me to put on a mask to mine the gold from my breath, I might do that, but I'm
[02:08:13.980 --> 02:08:18.020]   going to ask before I do it, like, well, what's my share, right?
[02:08:18.020 --> 02:08:21.860]   Yeah, that's fair.
[02:08:21.860 --> 02:08:26.020]   But right, it's just that the flip side is that you are getting these services that
[02:08:26.020 --> 02:08:27.740]   are enabled by all the state of collection.
[02:08:27.740 --> 02:08:31.580]   Like you said, Leo, like you said, with the maps, with real-time traffic information,
[02:08:31.580 --> 02:08:35.020]   with the fact that I can swipe over to my Google feed right now, and it's telling me
[02:08:35.020 --> 02:08:39.900]   stories about Star Trek, Verizon, SpaceX, the Pixel, and Delta Airlines.
[02:08:39.900 --> 02:08:41.420]   I mean, that's pretty good.
[02:08:41.420 --> 02:08:43.060]   That's a good array of Sunday reading for me.
[02:08:43.060 --> 02:08:45.700]   I like that, and I like that it knows that already.
[02:08:45.700 --> 02:08:50.860]   And when it services a story about, you know, whatever, PewDiePie or something else, I'm
[02:08:50.860 --> 02:08:51.860]   not interested in it.
[02:08:51.860 --> 02:08:53.020]   I say, show me less.
[02:08:53.020 --> 02:08:54.020]   And then it learns over time.
[02:08:54.020 --> 02:08:55.820]   But I don't really have to do that all that.
[02:08:55.820 --> 02:08:59.860]   I'm going to feed your paranoia, Georgia.
[02:08:59.860 --> 02:09:04.940]   Somebody on Reddit discovered that Google Pay had some hidden privacy settings.
[02:09:04.940 --> 02:09:07.220]   You couldn't really access.
[02:09:07.220 --> 02:09:14.580]   Fortunately, when told about this by bleeping computer, Google said, "Oops, that was an
[02:09:14.580 --> 02:09:17.700]   update that was a problem, and we fixed the settings."
[02:09:17.700 --> 02:09:18.700]   And so it's okay now.
[02:09:18.700 --> 02:09:22.580]   But if you read the settings, it's pretty creepy.
[02:09:22.580 --> 02:09:24.340]   Allow Google payment questions.
[02:09:24.340 --> 02:09:28.900]   So these were settings that were turned on, but in theory, you had no access to.
[02:09:28.900 --> 02:09:34.220]   Allow Google payment to share third-party creditworthiness information about you with
[02:09:34.220 --> 02:09:38.980]   other companies owned and controlled by Google for their everyday business purposes.
[02:09:38.980 --> 02:09:41.700]   Allow your personal information to be used by other companies owned and controlled by
[02:09:41.700 --> 02:09:43.540]   Google to market to you.
[02:09:43.540 --> 02:09:46.500]   Opting out here does not impact whether there are other companies owned and controlled by
[02:09:46.500 --> 02:09:51.460]   Google can market to you based on information you provide to them outside of Google Pay.
[02:09:51.460 --> 02:09:55.460]   Allow Google or its affiliates to form a third-party merchant whose site or app you visit,
[02:09:55.460 --> 02:10:00.980]   whether you have a Google Pay account that could be used for payment at that merchant.
[02:10:00.980 --> 02:10:05.780]   Are those settings, would you like access to the, especially that first one?
[02:10:05.780 --> 02:10:11.340]   I would wonder if anyone would have said yes when Google started out all the information
[02:10:11.340 --> 02:10:13.220]   that they were gathered if they knew.
[02:10:13.220 --> 02:10:16.220]   These were all checked by default, and then they hit the setting.
[02:10:16.220 --> 02:10:21.860]   Yeah, but I don't think that anyone, any of us, when Google first started, when we first
[02:10:21.860 --> 02:10:26.180]   started using search engines, would we have said yes to all of the information that they
[02:10:26.180 --> 02:10:27.900]   would get easily?
[02:10:27.900 --> 02:10:34.220]   I think that we'd become acclimatized to us giving up our time to companies where we
[02:10:34.220 --> 02:10:35.460]   lose our privacy.
[02:10:35.460 --> 02:10:38.220]   Once we give it up, it's really hard to reclaim it.
[02:10:38.220 --> 02:10:40.980]   I think that also bothers me deeply.
[02:10:40.980 --> 02:10:41.980]   Yeah.
[02:10:41.980 --> 02:10:42.980]   As it should.
[02:10:42.980 --> 02:10:47.700]   I agree with that you should be bothered by it, but while Leo was running down those
[02:10:47.700 --> 02:10:52.260]   check marks, it just occurred to me, "Well, why are they asking for these permissions?"
[02:10:52.260 --> 02:10:55.500]   Probably so that they can send a notification to your phone when they detect that you pass
[02:10:55.500 --> 02:10:58.180]   the geofence of a Dunkin Donuts door and be like, "Hey, did you know you could use
[02:10:58.180 --> 02:10:59.180]   Google Pay here?"
[02:10:59.180 --> 02:11:00.180]   Okay.
[02:11:00.180 --> 02:11:01.180]   I mean, it's not bad.
[02:11:01.180 --> 02:11:02.180]   I like that.
[02:11:02.180 --> 02:11:03.180]   I like that.
[02:11:03.180 --> 02:11:04.180]   I don't mind knowing that.
[02:11:04.180 --> 02:11:05.180]   I tap here for donuts.
[02:11:05.180 --> 02:11:06.180]   What's wrong with that?
[02:11:06.180 --> 02:11:07.180]   I don't know.
[02:11:07.180 --> 02:11:13.820]   I mean, if somebody uses Google Pay at Dunkin Donuts every day, I sort of get what they're
[02:11:13.820 --> 02:11:14.820]   doing that.
[02:11:14.820 --> 02:11:17.580]   That first one is creepy as all get out, and they shouldn't be checked by the phone.
[02:11:17.580 --> 02:11:20.500]   I don't want my credit worth it as to just be shared to anybody any time.
[02:11:20.500 --> 02:11:24.180]   But it's the same thing, isn't it, that people say, "Well, I'm not doing anything wrong.
[02:11:24.180 --> 02:11:25.900]   What does it matter if they know?"
[02:11:25.900 --> 02:11:26.900]   I think that in...
[02:11:26.900 --> 02:11:27.900]   Oh, yeah, no.
[02:11:27.900 --> 02:11:29.940]   We all say, "Well, they're going to go after the other guy.
[02:11:29.940 --> 02:11:31.700]   It's always the other guy until they come for us."
[02:11:31.700 --> 02:11:34.740]   By the time they come for us, there's no one left to fight for us.
[02:11:34.740 --> 02:11:35.740]   100%.
[02:11:35.740 --> 02:11:36.740]   No, you're absolutely right.
[02:11:36.740 --> 02:11:37.740]   That's not a...
[02:11:37.740 --> 02:11:38.740]   I have nothing to hide.
[02:11:38.740 --> 02:11:39.740]   It's not something I have ever said.
[02:11:39.740 --> 02:11:40.740]   No.
[02:11:40.740 --> 02:11:41.740]   Plenty of...
[02:11:41.740 --> 02:11:42.740]   It is clearly not the defense.
[02:11:42.740 --> 02:11:47.380]   Hey, let's take one more break, and then it's a sad story to end.
[02:11:47.380 --> 02:11:49.460]   Has nothing to do with Game of Thrones, but stay tuned.
[02:11:49.460 --> 02:11:56.660]   Our show today brought to you by ExpressVPN when you are online, you are in danger.
[02:11:56.660 --> 02:11:57.660]   Stranger danger.
[02:11:57.660 --> 02:12:00.740]   Well, maybe not, but you should certainly be aware when you're on an open Wi-Fi access
[02:12:00.740 --> 02:12:04.940]   point that there are people out there who have things like Wi-Fi pineapples that can
[02:12:04.940 --> 02:12:06.780]   try to mess with you.
[02:12:06.780 --> 02:12:10.140]   You know what other things Wi-Fi pineapples do?
[02:12:10.140 --> 02:12:17.700]   Even if you are on a secure connection, a Wi-Fi pineapple can often see what common
[02:12:17.700 --> 02:12:20.260]   access points you join.
[02:12:20.260 --> 02:12:24.500]   That information can be fed to you, and it gives the hacker enough information to pose
[02:12:24.500 --> 02:12:30.620]   as your home access point and be closer than the coffee shop's access point.
[02:12:30.620 --> 02:12:32.940]   And then your computer goes, "Oh, we're home.
[02:12:32.940 --> 02:12:33.940]   Log in."
[02:12:33.940 --> 02:12:37.140]   And you're now in a man in the middle attack.
[02:12:37.140 --> 02:12:38.140]   Little things like that.
[02:12:38.140 --> 02:12:42.140]   When you use a VPN, you're safe because your data is encrypted from your computer to the
[02:12:42.140 --> 02:12:44.420]   VPN server.
[02:12:44.420 --> 02:12:48.740]   Another thing that's great about ExpressVPN, my favorite VPN is they have servers all over
[02:12:48.740 --> 02:12:52.060]   the world, more than 100 servers, countries all over the world.
[02:12:52.060 --> 02:12:57.860]   So when you emerge into the public internet, you can emerge at the location of your choice
[02:12:57.860 --> 02:13:00.660]   internet without borders, as they say.
[02:13:00.660 --> 02:13:07.420]   So security, privacy, even your ISP at home is probably selling information about where
[02:13:07.420 --> 02:13:09.540]   you go.
[02:13:09.540 --> 02:13:14.220]   ExpressVPN anonymizes your internet browsing by encrypting your data, hiding your public
[02:13:14.220 --> 02:13:15.220]   IP address.
[02:13:15.220 --> 02:13:20.780]   And at less than seven bucks a month, you can get the number one VPN service from tech
[02:13:20.780 --> 02:13:24.620]   radar talks about coming and it comes with a 30 day money back guarantee.
[02:13:24.620 --> 02:13:26.140]   So there's no risk.
[02:13:26.140 --> 02:13:30.300]   ExpressVPN does no logging, does not keep track of you.
[02:13:30.300 --> 02:13:31.980]   You're totally anonymous.
[02:13:31.980 --> 02:13:32.980]   ExpressVPN.
[02:13:32.980 --> 02:13:34.820]   Protect your online activity today.
[02:13:34.820 --> 02:13:38.580]   Find out how you can get three extra months free with a one year package.
[02:13:38.580 --> 02:13:42.820]   ExpressVPN.com/twit.
[02:13:42.820 --> 02:13:46.540]   And express VPN.com/twit.
[02:13:46.540 --> 02:13:52.100]   Number one, and absolutely secure express VPN.com/twit.
[02:13:52.100 --> 02:13:54.060]   Um, sad.
[02:13:54.060 --> 02:13:55.060]   Okay.
[02:13:55.060 --> 02:13:58.300]   So are you all sitting down?
[02:13:58.300 --> 02:14:02.220]   It's not tear in, is it?
[02:14:02.220 --> 02:14:03.620]   Almost as bad.
[02:14:03.620 --> 02:14:04.620]   Grumpy cat.
[02:14:04.620 --> 02:14:10.940]   Oh, grumpy cat has passed away.
[02:14:10.940 --> 02:14:14.220]   The internet celebrity with a piercing look of contempt.
[02:14:14.220 --> 02:14:21.660]   You got to love the New York Times and that headline.
[02:14:21.660 --> 02:14:29.260]   Grumpy cat, uh, who's real name by the way is tar dar sauce, T-A-R-D-A-R sauce.
[02:14:29.260 --> 02:14:31.660]   That would make me grumpy.
[02:14:31.660 --> 02:14:38.940]   Um, uh, died at the age of seven from complications from a urinary tract infection.
[02:14:38.940 --> 02:14:39.940]   I'm sorry.
[02:14:39.940 --> 02:14:40.940]   Nothing.
[02:14:40.940 --> 02:14:43.620]   There's nothing funny about this story.
[02:14:43.620 --> 02:14:46.900]   Uh, boy grumpy cat had a run though.
[02:14:46.900 --> 02:14:47.900]   Didn't she?
[02:14:47.900 --> 02:14:53.460]   Uh, her owner posted a picture of her to Reddit saying, and you want, you could still see
[02:14:53.460 --> 02:14:57.260]   the original photo saying, uh, meet grumpy cat.
[02:14:57.260 --> 02:14:59.820]   This was her internet debut.
[02:14:59.820 --> 02:15:04.700]   Apparently, why was she so grumpy?
[02:15:04.700 --> 02:15:06.820]   Was there something wrong with grumpy cat?
[02:15:06.820 --> 02:15:11.820]   Yeah, she has, she has an underbite and she has feline dwarfism.
[02:15:11.820 --> 02:15:12.820]   That's what it was.
[02:15:12.820 --> 02:15:13.820]   Yeah.
[02:15:13.820 --> 02:15:14.820]   She's teensy weeny.
[02:15:14.820 --> 02:15:17.700]   Um, and the underbite makes the grump, right?
[02:15:17.700 --> 02:15:19.460]   You're like, yeah, right?
[02:15:19.460 --> 02:15:23.100]   So many people with underbites do look grumpy.
[02:15:23.100 --> 02:15:28.860]   Uh, she quickly became a meme, the most famous picture of grumpy cat with the words, I had
[02:15:28.860 --> 02:15:30.220]   fun once.
[02:15:30.220 --> 02:15:31.220]   It was awful.
[02:15:31.220 --> 02:15:38.380]   Meme of the year 2013.
[02:15:38.380 --> 02:15:39.460]   People don't confuse it.
[02:15:39.460 --> 02:15:41.580]   Somebody's saying, Oh, is this like the third grumpy cat?
[02:15:41.580 --> 02:15:44.540]   No, you're thinking a keyboard cat, keyboard cat.
[02:15:44.540 --> 02:15:48.340]   There were many, but there was only one grumpy cat.
[02:15:48.340 --> 02:15:50.340]   Mm hmm.
[02:15:50.340 --> 02:15:56.780]   Meme of the year 2013 beating out some big, though gongnam style and Harlem shuffle.
[02:15:56.780 --> 02:15:57.780]   Come on.
[02:15:57.780 --> 02:16:04.600]   Listen, uh, whoever the owner is, take a lesson from Hollywood, wait a couple of years, just
[02:16:04.600 --> 02:16:07.940]   reboot it with a younger, yeah, actor cat.
[02:16:07.940 --> 02:16:10.340]   That's what they did with a, with keyboard cat.
[02:16:10.340 --> 02:16:11.900]   That's what they've done with Spiderman.
[02:16:11.900 --> 02:16:12.900]   Yeah.
[02:16:12.900 --> 02:16:16.100]   And, and Toby's not even dead.
[02:16:16.100 --> 02:16:21.060]   Uh, 900 grumpy cat items in the official grumpy cat shop.
[02:16:21.060 --> 02:16:23.180]   Should we go shopping?
[02:16:23.180 --> 02:16:26.180]   Find something terrible to buy.
[02:16:26.180 --> 02:16:28.180]   I've heard once.
[02:16:28.180 --> 02:16:29.180]   Monday.
[02:16:29.180 --> 02:16:30.180]   Yeah.
[02:16:30.180 --> 02:16:35.540]   Uh, they, uh, there were grumpy cat movies on lifetime.
[02:16:35.540 --> 02:16:37.020]   Of course there were.
[02:16:37.020 --> 02:16:42.460]   She was the official spokescat of Friskies grumpy cats worst Christmas ever.
[02:16:42.460 --> 02:16:43.460]   That was the lifetime movie.
[02:16:43.460 --> 02:16:47.460]   She was in the cover of New York magazine number seven on the New York Times advice,
[02:16:47.460 --> 02:16:52.100]   how to and miscellaneous bestseller list.
[02:16:52.100 --> 02:16:53.100]   Grumpy cat.
[02:16:53.100 --> 02:16:54.100]   Do you want to get?
[02:16:54.100 --> 02:16:55.100]   Well, it's too late.
[02:16:55.100 --> 02:16:56.100]   You already can see it on the screen.
[02:16:56.100 --> 02:17:04.740]   Guess those of you at home listening, how much grumpy cat made her owners in six years?
[02:17:04.740 --> 02:17:07.220]   Estimated $100 million.
[02:17:07.220 --> 02:17:08.220]   Wow.
[02:17:08.220 --> 02:17:11.060]   I don't believe that.
[02:17:11.060 --> 02:17:13.460]   I really, I really.
[02:17:13.460 --> 02:17:14.460]   Okay.
[02:17:14.460 --> 02:17:15.460]   Grumpy.
[02:17:15.460 --> 02:17:16.460]   Pontaster.
[02:17:16.460 --> 02:17:17.460]   I really don't.
[02:17:17.460 --> 02:17:18.460]   Well, they would.
[02:17:18.460 --> 02:17:20.460]   Do you not want to leave that or are you just jealous?
[02:17:20.460 --> 02:17:21.460]   They disputed it.
[02:17:21.460 --> 02:17:25.020]   They said no, that's not true, but they never provided an alternative number.
[02:17:25.020 --> 02:17:26.020]   That's rough.
[02:17:26.020 --> 02:17:30.180]   All that movie licensing, all the commercial licensing, all the tie ins, I can buy that
[02:17:30.180 --> 02:17:31.180]   number.
[02:17:31.180 --> 02:17:34.180]   I do think it seems I would have never guessed that much.
[02:17:34.180 --> 02:17:37.660]   I would have guessed like five million.
[02:17:37.660 --> 02:17:41.220]   If I were them, I would love to have that $100 million number floating around out there.
[02:17:41.220 --> 02:17:42.220]   So that's fine.
[02:17:42.220 --> 02:17:43.820]   I don't blame for saying that.
[02:17:43.820 --> 02:17:46.580]   So I was the rest doesn't hear.
[02:17:46.580 --> 02:17:49.140]   So once in a lifetime thing, when are you going to get another grumpy cat?
[02:17:49.140 --> 02:17:50.540]   I don't know.
[02:17:50.540 --> 02:17:51.540]   Maybe you could.
[02:17:51.540 --> 02:17:52.540]   I don't know.
[02:17:52.540 --> 02:17:53.540]   There's only one grumpy cat.
[02:17:53.540 --> 02:17:54.540]   That's only one.
[02:17:54.540 --> 02:17:55.540]   You seem the farewell meme.
[02:17:55.540 --> 02:17:57.180]   No, no, no.
[02:17:57.180 --> 02:18:01.100]   Grumpy cat is in a is in a beautiful place with rainbows in the fields on the center of
[02:18:01.100 --> 02:18:03.100]   the top is so this is heaven.
[02:18:03.100 --> 02:18:04.100]   Bottom.
[02:18:04.100 --> 02:18:06.300]   I hate it.
[02:18:06.300 --> 02:18:08.060]   Yeah.
[02:18:08.060 --> 02:18:10.140]   I can.
[02:18:10.140 --> 02:18:12.900]   Now I'm sad.
[02:18:12.900 --> 02:18:14.180]   You made me sad.
[02:18:14.180 --> 02:18:16.420]   She's really cute.
[02:18:16.420 --> 02:18:21.420]   She was a really cute cat that that really embodied our inner grump.
[02:18:21.420 --> 02:18:24.820]   I remember she was that South by Southwest and I wanted to go see her.
[02:18:24.820 --> 02:18:26.820]   It was just too crowded.
[02:18:26.820 --> 02:18:33.020]   Oh, you didn't say she did personal appearances too.
[02:18:33.020 --> 02:18:34.020]   Okay.
[02:18:34.020 --> 02:18:35.020]   Yeah.
[02:18:35.020 --> 02:18:36.020]   That's big.
[02:18:36.020 --> 02:18:37.020]   Yeah.
[02:18:37.020 --> 02:18:38.020]   Conference money.
[02:18:38.020 --> 02:18:39.020]   Oh, conference.
[02:18:39.020 --> 02:18:40.020]   Yeah.
[02:18:40.020 --> 02:18:42.060]   You probably paid a whole bunch of money just to get to pet grumpy cat.
[02:18:42.060 --> 02:18:48.260]   I mean, honestly, if you had a booth at a trade show and you you would pay grumpy cat.
[02:18:48.260 --> 02:18:49.260]   What?
[02:18:49.260 --> 02:18:50.260]   A hundred thousand?
[02:18:50.260 --> 02:18:51.860]   200,000 to just be in the booth.
[02:18:51.860 --> 02:18:53.540]   It doesn't take long.
[02:18:53.540 --> 02:18:54.540]   Yeah.
[02:18:54.540 --> 02:18:57.660]   Before you get a hundred million, that's not hard.
[02:18:57.660 --> 02:18:59.220]   Folks, I don't want to end on that set.
[02:18:59.220 --> 02:19:00.980]   This is bad to go from a UTI.
[02:19:00.980 --> 02:19:01.980]   I know.
[02:19:01.980 --> 02:19:05.180]   It's like a really rough way to go.
[02:19:05.180 --> 02:19:06.180]   Yeah.
[02:19:06.180 --> 02:19:07.180]   Kitties.
[02:19:07.180 --> 02:19:09.500]   Poor little kitties.
[02:19:09.500 --> 02:19:12.420]   Georgia Dow, you are the heart and soul.
[02:19:12.420 --> 02:19:19.340]   Not only of this show, but of the great imore.com senior editor there.
[02:19:19.340 --> 02:19:20.580]   We love having you on.
[02:19:20.580 --> 02:19:25.860]   Thank you for being here and spreading the good word.
[02:19:25.860 --> 02:19:26.860]   Thank you so much.
[02:19:26.860 --> 02:19:27.860]   What would you like me to plug?
[02:19:27.860 --> 02:19:29.420]   How about anxiety videos?
[02:19:29.420 --> 02:19:31.340]   Sure thing.
[02:19:31.340 --> 02:19:34.900]   See, I know something about you and I'm going to tell the world about it and it's going
[02:19:34.900 --> 02:19:37.300]   to be good.
[02:19:37.300 --> 02:19:41.980]   Anxiety-videos.com.
[02:19:41.980 --> 02:19:43.500]   Anxiety-videos.com.
[02:19:43.500 --> 02:19:45.020]   Tell me the URL.
[02:19:45.020 --> 02:19:47.020]   Anxiety-videos.com.
[02:19:47.020 --> 02:19:48.020]   Plural.
[02:19:48.020 --> 02:19:50.260]   Anxiety-videos.com.
[02:19:50.260 --> 02:19:53.460]   Really good videos that aren't just about anxiety.
[02:19:53.460 --> 02:19:55.380]   Who knew?
[02:19:55.380 --> 02:19:59.060]   Sleeping better, being a better parent.
[02:19:59.060 --> 02:20:05.420]   And they're all hosted by the wonderful Georgia and her partner in crime.
[02:20:05.420 --> 02:20:08.100]   They're both psychotherapists, right?
[02:20:08.100 --> 02:20:09.100]   Yeah.
[02:20:09.100 --> 02:20:10.100]   Yep.
[02:20:10.100 --> 02:20:11.100]   Very, very good.
[02:20:11.100 --> 02:20:12.100]   Anxiety-video.com.
[02:20:12.100 --> 02:20:15.540]   Any podcasts you'd like to plug or anything?
[02:20:15.540 --> 02:20:18.940]   Well, you can always check out Renee on Vector.
[02:20:18.940 --> 02:20:20.460]   We do the iMore show.
[02:20:20.460 --> 02:20:22.340]   Do you shop on Vector a lot?
[02:20:22.340 --> 02:20:24.340]   Every once in a while.
[02:20:24.340 --> 02:20:25.340]   Every once in a while.
[02:20:25.340 --> 02:20:28.740]   You're like his guinea pig, aren't you?
[02:20:28.740 --> 02:20:29.740]   Right.
[02:20:29.740 --> 02:20:30.980]   Try stuff on.
[02:20:30.980 --> 02:20:32.540]   Let's see what Georgia thinks.
[02:20:32.540 --> 02:20:33.540]   Yeah.
[02:20:33.540 --> 02:20:35.340]   Thank you, Georgia.
[02:20:35.340 --> 02:20:42.020]   We thank you so much, Mr. Brian McCullough, the host of the Tech Meme Ride home.
[02:20:42.020 --> 02:20:45.460]   Every day this man is in front of the mike, slaving in way in front of a hot mic to bring
[02:20:45.460 --> 02:20:48.820]   you the latest news and information.
[02:20:48.820 --> 02:20:49.820]   Thank you.
[02:20:49.820 --> 02:20:50.980]   Thank you, sir.
[02:20:50.980 --> 02:20:57.580]   If there's a, we were talking earlier about the customizing your entry into a Dunkin Donuts,
[02:20:57.580 --> 02:21:02.980]   can I customize not to hear the fake Leo on the radio?
[02:21:02.980 --> 02:21:09.740]   I hope, I hope someday to be, you know, like that, that would be good.
[02:21:09.740 --> 02:21:14.660]   And I could just retire and let some AI take over.
[02:21:14.660 --> 02:21:20.020]   Brian is also the author of how the internet happened from Netscape to the iPhone.
[02:21:20.020 --> 02:21:23.980]   And you can get it by Tuesday if you order it within two hours and 27 minutes, according
[02:21:23.980 --> 02:21:25.180]   to Amazon.
[02:21:25.180 --> 02:21:26.180]   Only 14 left.
[02:21:26.180 --> 02:21:28.740]   Don't you love seeing that?
[02:21:28.740 --> 02:21:29.740]   More on the way.
[02:21:29.740 --> 02:21:30.740]   Oh, second print.
[02:21:30.740 --> 02:21:32.620]   At the end of the print run.
[02:21:32.620 --> 02:21:33.620]   Okay.
[02:21:33.620 --> 02:21:35.260]   Great.
[02:21:35.260 --> 02:21:41.140]   Man, I am so pleased to meet you and, uh, and your hairdo, Mr. Mobile for mobile nations,
[02:21:41.140 --> 02:21:43.500]   the Mr. Mobile Michael Fisher.
[02:21:43.500 --> 02:21:45.940]   He's at MrMobile.tech, a 700.
[02:21:45.940 --> 02:21:46.940]   You are more.
[02:21:46.940 --> 02:21:49.140]   Do you get a, I know you get us.
[02:21:49.140 --> 02:21:52.260]   We got a, we got one, a silver button for a hundred thousand subscribers.
[02:21:52.260 --> 02:21:53.260]   Yeah.
[02:21:53.260 --> 02:21:54.260]   Yeah.
[02:21:54.260 --> 02:21:55.260]   Got that hanging out.
[02:21:55.260 --> 02:21:57.460]   Do you get like something special at a million, like a gold or platinum?
[02:21:57.460 --> 02:21:58.860]   You do get a gold one.
[02:21:58.860 --> 02:22:02.980]   Then if you become a real superstar, you get like a platinum one, then I got diamond one
[02:22:02.980 --> 02:22:04.740]   at some ridiculous level.
[02:22:04.740 --> 02:22:05.740]   Yeah.
[02:22:05.740 --> 02:22:06.740]   Yeah.
[02:22:06.740 --> 02:22:07.740]   I just, I'll be happy with the gold one.
[02:22:07.740 --> 02:22:08.740]   It's fine.
[02:22:08.740 --> 02:22:09.740]   I just want the seven digits.
[02:22:09.740 --> 02:22:10.740]   Everybody should.
[02:22:10.740 --> 02:22:11.740]   Everyone subscribes.
[02:22:11.740 --> 02:22:12.740]   Subscribe.
[02:22:12.740 --> 02:22:13.740]   These two.
[02:22:13.740 --> 02:22:21.740]   You know, if everybody subscribes right now, you'll have by, you know, 750,000.
[02:22:21.740 --> 02:22:22.740]   What is funny?
[02:22:22.740 --> 02:22:24.740]   You can see the unsubscribe rate alongside with the subscribe.
[02:22:24.740 --> 02:22:28.620]   And if you're, if you're not hip to how a YouTube works, that can be very, that can
[02:22:28.620 --> 02:22:29.820]   be a sobering day.
[02:22:29.820 --> 02:22:30.820]   So is it home early?
[02:22:30.820 --> 02:22:31.820]   Yeah.
[02:22:31.820 --> 02:22:32.820]   I don't ever want to see that.
[02:22:32.820 --> 02:22:33.820]   No, don't.
[02:22:33.820 --> 02:22:35.420]   I'm slash Mr. Mobile.
[02:22:35.420 --> 02:22:37.300]   Is that slash the Mr. Mobile?
[02:22:37.300 --> 02:22:40.460]   The T H E M R M O B I L E. Yep.
[02:22:40.460 --> 02:22:41.460]   And lots of reviews.
[02:22:41.460 --> 02:22:43.500]   What do you think of the one plus seven?
[02:22:43.500 --> 02:22:49.020]   I think it's a much more difficult phone to, to characterize in a, in a really short
[02:22:49.020 --> 02:22:52.340]   amount of time, because up until now, it's been like, yeah, it's the best deal you can
[02:22:52.340 --> 02:22:53.500]   get in mobile.
[02:22:53.500 --> 02:22:54.740]   Now it's, it's the best deal.
[02:22:54.740 --> 02:22:57.100]   You can get in a certain segment of mobile, the high end.
[02:22:57.100 --> 02:23:00.100]   So, you know, I'm just a new Asus SIN phone.
[02:23:00.100 --> 02:23:03.420]   And I thought, Oh, that's one plus.
[02:23:03.420 --> 02:23:06.620]   It's a potential disruptor there for sure.
[02:23:06.620 --> 02:23:07.620]   Yeah.
[02:23:07.620 --> 02:23:08.620]   Five thousand milliamp hours.
[02:23:08.620 --> 02:23:10.420]   It looks like a really nice phone.
[02:23:10.420 --> 02:23:11.940]   That rotating camera on top.
[02:23:11.940 --> 02:23:15.460]   So you can, if you're, if you shoot a lot of yourself, it's very useful.
[02:23:15.460 --> 02:23:16.460]   Nice.
[02:23:16.460 --> 02:23:17.460]   Nice.
[02:23:17.460 --> 02:23:21.500]   And you also have a three A review there and the folding PC and lots of stuff.
[02:23:21.500 --> 02:23:23.020]   So yeah, Mr.
[02:23:23.020 --> 02:23:24.020]   Mobile.
[02:23:24.020 --> 02:23:25.420]   Oh, well, tech you can think of.
[02:23:25.420 --> 02:23:26.420]   D.
[02:23:26.420 --> 02:23:27.420]   So thank you for having me.
[02:23:27.420 --> 02:23:28.420]   The Mr.
[02:23:28.420 --> 02:23:29.420]   Mobile.
[02:23:29.420 --> 02:23:30.420]   Thank you, Michael.
[02:23:30.420 --> 02:23:31.420]   Great to have you.
[02:23:31.420 --> 02:23:36.340]   You do this week in tech right after the radio show that about 215 to 230 Pacific.
[02:23:36.340 --> 02:23:40.820]   It'd be 515 to 530 Eastern time.
[02:23:40.820 --> 02:23:44.300]   That would be 21 15 to 21 30.
[02:23:44.300 --> 02:23:45.780]   Never going to do that again.
[02:23:45.780 --> 02:23:48.140]   UTC just, you know, 230.
[02:23:48.140 --> 02:23:49.860]   Let's say 230.
[02:23:49.860 --> 02:23:51.020]   That's easier.
[02:23:51.020 --> 02:23:53.140]   You can watch live at twit.tv/live.
[02:23:53.140 --> 02:23:55.260]   There's audio and video streams there.
[02:23:55.260 --> 02:23:58.820]   If you are watching live, join us in the chatroom, IRC.twit.tv.
[02:23:58.820 --> 02:24:02.220]   That's where everybody else watching live hangs out and talks.
[02:24:02.220 --> 02:24:03.980]   Oh, look at you, Michael.
[02:24:03.980 --> 02:24:04.980]   Look at you.
[02:24:04.980 --> 02:24:08.340]   You're going to make Georgia happy in there with your.
[02:24:08.340 --> 02:24:09.540]   Where was the VR thing?
[02:24:09.540 --> 02:24:12.340]   You were you were playing your VR.
[02:24:12.340 --> 02:24:16.340]   Oh, there he is.
[02:24:16.340 --> 02:24:20.300]   That's exactly what Georgia and her husband look like.
[02:24:20.300 --> 02:24:25.140]   That's exactly what they look like.
[02:24:25.140 --> 02:24:26.140]   Oh, you're tethered though.
[02:24:26.140 --> 02:24:27.140]   Yeah, you got to go wireless.
[02:24:27.140 --> 02:24:28.140]   Yeah, no.
[02:24:28.140 --> 02:24:29.140]   There we go.
[02:24:29.140 --> 02:24:33.020]   When you go wireless, you got little devil horns in your head for that wireless receiver.
[02:24:33.020 --> 02:24:34.020]   So look at me.
[02:24:34.020 --> 02:24:35.660]   I got them from my head with the head strap there.
[02:24:35.660 --> 02:24:36.660]   I got them anyway.
[02:24:36.660 --> 02:24:37.660]   It doesn't matter.
[02:24:37.660 --> 02:24:41.460]   You would at least need the dog chain so that it deals with like it hangs it on the
[02:24:41.460 --> 02:24:42.460]   ceiling.
[02:24:42.460 --> 02:24:44.460]   So you're kind of stepping on it.
[02:24:44.460 --> 02:24:45.460]   Yeah, that's right.
[02:24:45.460 --> 02:24:46.460]   Right.
[02:24:46.460 --> 02:24:47.460]   Oh my God.
[02:24:47.460 --> 02:24:51.620]   I hope you stayed through the whole show everybody because that's an amazing tip.
[02:24:51.620 --> 02:24:54.060]   Hang your tether from a dog chain.
[02:24:54.060 --> 02:24:55.660]   Yeah, they buy.
[02:24:55.660 --> 02:24:56.900]   You can buy them on Amazon.
[02:24:56.900 --> 02:25:02.620]   And they just kind of stick it to the ceiling and you're good to go.
[02:25:02.620 --> 02:25:03.980]   You do it like a choke chain.
[02:25:03.980 --> 02:25:07.420]   So it just tightens and loosens and he just kind of slides through.
[02:25:07.420 --> 02:25:08.420]   Oh my God.
[02:25:08.420 --> 02:25:09.420]   You're brilliant.
[02:25:09.420 --> 02:25:10.420]   Yeah, yeah, yeah.
[02:25:10.420 --> 02:25:11.420]   Not me.
[02:25:11.420 --> 02:25:13.220]   It was my husband that was Frant, and he did that.
[02:25:13.220 --> 02:25:16.780]   And I think that he wrote an article maybe on I'm on how to do it.
[02:25:16.780 --> 02:25:17.780]   Oh, that's off.
[02:25:17.780 --> 02:25:18.780]   I'm going to flip that.
[02:25:18.780 --> 02:25:19.780]   Brilliant.
[02:25:19.780 --> 02:25:20.780]   Yeah.
[02:25:20.780 --> 02:25:25.100]   If you can't be here for the live show and I understand, you know, you got your busy
[02:25:25.100 --> 02:25:27.060]   person, you got things to do places to go.
[02:25:27.060 --> 02:25:29.020]   You can always bring it with you.
[02:25:29.020 --> 02:25:34.540]   We have on demand versions of everything we do at our website, twit.tv.
[02:25:34.540 --> 02:25:39.540]   And of course, the best thing to do is subscribe in one of your favorite podcast applications.
[02:25:39.540 --> 02:25:40.860]   And then you don't even have to think about it.
[02:25:40.860 --> 02:25:43.780]   You'll just your phone will automatically download at the minute the show is available
[02:25:43.780 --> 02:25:46.020]   later Sunday evening.
[02:25:46.020 --> 02:25:49.420]   You'll be watching whatever the next thing is.
[02:25:49.420 --> 02:25:50.500]   Somebody said Game of Thrones.
[02:25:50.500 --> 02:25:56.580]   This is the last time we as a country will be all watching the same show at the same
[02:25:56.580 --> 02:25:57.580]   time.
[02:25:57.580 --> 02:25:59.260]   I think that's for sure.
[02:25:59.260 --> 02:26:01.140]   Isn't that weird to think of?
[02:26:01.140 --> 02:26:02.300]   I think that's for sure.
[02:26:02.300 --> 02:26:05.500]   Because most shows are going to be released in a batch.
[02:26:05.500 --> 02:26:06.700]   So you can binge, right?
[02:26:06.700 --> 02:26:10.620]   Well, no, not only that, but they'll be on different platforms like Disney will have
[02:26:10.620 --> 02:26:11.620]   a show.
[02:26:11.620 --> 02:26:12.620]   Really?
[02:26:12.620 --> 02:26:13.620]   Yeah.
[02:26:13.620 --> 02:26:14.620]   Netflix will have the hot show.
[02:26:14.620 --> 02:26:16.420]   Like how can you guarantee that you'll have that?
[02:26:16.420 --> 02:26:17.420]   Yeah, no.
[02:26:17.420 --> 02:26:18.420]   It's not that everyone's.
[02:26:18.420 --> 02:26:22.820]   Not that everyone's subscribed to HBO by any stretch of the imagination, but well, everybody
[02:26:22.820 --> 02:26:23.820]   has a task.
[02:26:23.820 --> 02:26:30.540]   But I'm saying what I'm saying, Michael is that like HBO was the only if you were going
[02:26:30.540 --> 02:26:36.060]   to subscribe to something 10 years ago, HBO was the only thing, right?
[02:26:36.060 --> 02:26:40.140]   So like that's part of it is it's completely fractured and five years from now.
[02:26:40.140 --> 02:26:41.140]   And this year, yeah.
[02:26:41.140 --> 02:26:42.140]   Yeah.
[02:26:42.140 --> 02:26:46.500]   I honestly think there'll be other great shows, but because there'll be so many other great
[02:26:46.500 --> 02:26:49.380]   shows and because many of them at different times.
[02:26:49.380 --> 02:26:50.380]   Yeah.
[02:26:50.380 --> 02:26:53.420]   And because many of them will be bingeable because honestly, I think that's the future too.
[02:26:53.420 --> 02:26:54.420]   Who's good?
[02:26:54.420 --> 02:26:55.620]   That's crazy to release it once a week.
[02:26:55.620 --> 02:26:56.620]   Nobody wants that.
[02:26:56.620 --> 02:27:00.140]   No, but that gives you the opportunity to have friends or regative of viewing party.
[02:27:00.140 --> 02:27:02.140]   We did a fair bit of stuff.
[02:27:02.140 --> 02:27:03.140]   That is true.
[02:27:03.140 --> 02:27:07.500]   It's nice also to read the hype of like I'm waiting for Altshift X to come out with what
[02:27:07.500 --> 02:27:08.500]   are they going to?
[02:27:08.500 --> 02:27:10.780]   How is he going to break down this episode and what's happening?
[02:27:10.780 --> 02:27:15.180]   It's a whole industry of recapables and hot takes that people need to get fed, man.
[02:27:15.180 --> 02:27:17.540]   What are these friends you're talking about, Michael?
[02:27:17.540 --> 02:27:19.220]   I'd like to know more about it.
[02:27:19.220 --> 02:27:22.420]   It's just something I made up on the play.
[02:27:22.420 --> 02:27:28.100]   I want to remind everybody we got a little survey for you focusing on how you use if
[02:27:28.100 --> 02:27:29.940]   you use collaborative software at work.
[02:27:29.940 --> 02:27:30.940]   It's a very brief survey.
[02:27:30.940 --> 02:27:37.100]   Take just a few minutes and you'd be doing me a favor if you go to twit.to/survey14 to
[02:27:37.100 --> 02:27:38.100]   take it.
[02:27:38.100 --> 02:27:39.100]   Thank you.
[02:27:39.100 --> 02:27:40.580]   Yes, Carsten.
[02:27:40.580 --> 02:27:42.140]   Was that what you wanted me to do?
[02:27:42.140 --> 02:27:43.140]   Just saying say goodbye.
[02:27:43.140 --> 02:27:47.100]   Oh, this has been the longest end of a show ever.
[02:27:47.100 --> 02:27:53.460]   Carsten's looking at his watch saying, "I gotta go to the bathroom before Game of Thrones.
[02:27:53.460 --> 02:27:54.460]   You gotta hurry up."
[02:27:54.460 --> 02:27:56.180]   We don't have much time.
[02:27:56.180 --> 02:27:57.580]   I'm going to stretch this out.
[02:27:57.580 --> 02:27:59.500]   I'm going to make it clean.
[02:27:59.500 --> 02:28:00.980]   Thanks everybody.
[02:28:00.980 --> 02:28:01.980]   We'll see you next time.
[02:28:01.980 --> 02:28:03.980]   Another twit is in the can.
[02:28:03.980 --> 02:28:04.980]   Bye bye.
[02:28:04.980 --> 02:28:13.340]   [music]

