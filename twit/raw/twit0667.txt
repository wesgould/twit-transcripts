;FFMETADATA1
title=Give Me your History Hat
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=667
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.000]   It's time for "Twit This Week" in tech.
[00:00:03.000 --> 00:00:04.000]   What a great panel we have.
[00:00:04.000 --> 00:00:06.000]   Christina Warren joins us from Microsoft.
[00:00:06.000 --> 00:00:09.520]   Brian McCullough, his first time on the show, give him a great big welcome.
[00:00:09.520 --> 00:00:13.600]   He's from Tech Memes New, Ryan Home Podcast, and of course, even in a history podcast.
[00:00:13.600 --> 00:00:18.120]   Plus Patrick Bejaw, our favorite Frenchman, we're going to talk, oh, so much to talk about
[00:00:18.120 --> 00:00:19.840]   Google duplex.
[00:00:19.840 --> 00:00:21.920]   Was it unethical?
[00:00:21.920 --> 00:00:25.200]   The advancing GDPR, it starts Friday.
[00:00:25.200 --> 00:00:31.600]   The impact of ad tech, China's renew rules and Ireland's taxes and the Seattle business,
[00:00:31.600 --> 00:00:35.600]   I can go on and on, but I think what you should probably do is just sit back and relax and
[00:00:35.600 --> 00:00:37.320]   enjoy "Twit It's Next."
[00:00:37.320 --> 00:00:42.960]   "Net Casts You Love"
[00:00:42.960 --> 00:00:48.080]   "From People You Trust"
[00:00:48.080 --> 00:00:50.320]   This is "Twit"
[00:00:50.320 --> 00:00:57.320]   The bandwidth for this week in tech is provided by cashfly at cachefly.com
[00:00:57.320 --> 00:01:10.320]   This is "Twit" this week in tech, episode 667, recorded Sunday, May 20, 2018.
[00:01:10.320 --> 00:01:13.840]   Give me your history hat.
[00:01:13.840 --> 00:01:17.440]   This week in tech is brought to you by Zipper Cruder Hiring.
[00:01:17.440 --> 00:01:19.880]   Zipper Cruder has revolutionized how you do it.
[00:01:19.880 --> 00:01:24.040]   Their technology identifies people with the right experience and then invites them to apply
[00:01:24.040 --> 00:01:25.320]   to your job.
[00:01:25.320 --> 00:01:27.480]   They find great candidates for you.
[00:01:27.480 --> 00:01:31.360]   Try it free today at zippercruder.com/twit.
[00:01:31.360 --> 00:01:37.080]   And by LastPass, join over 13 million LastPass users and start managing and securing your
[00:01:37.080 --> 00:01:38.480]   passwords today.
[00:01:38.480 --> 00:01:42.080]   Learn more at lastpass.com/twit.
[00:01:42.080 --> 00:01:46.760]   And by Wink, the best way to discover new wines you'll love.
[00:01:46.760 --> 00:01:52.280]   Go to trywink.com/twit and get $20 off your first shipment.
[00:01:52.280 --> 00:01:55.160]   And by Rocket Mortgage from Quick and Loans.
[00:01:55.160 --> 00:01:56.600]   Home plays a big role in your life.
[00:01:56.600 --> 00:01:59.320]   That's why Quick and Loans created Rocket Mortgage.
[00:01:59.320 --> 00:02:03.520]   It lets you apply simply and understand the entire mortgage process fully so you can be
[00:02:03.520 --> 00:02:04.520]   confident.
[00:02:04.520 --> 00:02:05.680]   You're getting the right mortgage for you.
[00:02:05.680 --> 00:02:12.680]   Get started at rocketmortgage.com/twit2.
[00:02:12.680 --> 00:02:13.920]   This is "Twit".
[00:02:13.920 --> 00:02:19.760]   Welcome everybody to the weekly round table show where we discuss the tech news of the
[00:02:19.760 --> 00:02:20.760]   week.
[00:02:20.760 --> 00:02:21.760]   And we're going to have some fun today.
[00:02:21.760 --> 00:02:23.600]   Patrick Bejaz joining us.
[00:02:23.600 --> 00:02:26.480]   He is a longtime podcaster, FrenchSpin.com.
[00:02:26.480 --> 00:02:29.440]   And you've heard him on many shows including DTNS and so forth.
[00:02:29.440 --> 00:02:31.440]   Hey, Patrick, it's good to see you back again.
[00:02:31.440 --> 00:02:32.880]   Hey, thanks for having me.
[00:02:32.880 --> 00:02:33.880]   It's nice to be back.
[00:02:33.880 --> 00:02:34.880]   It's been a while.
[00:02:34.880 --> 00:02:37.680]   He's at Finland, so we're going to talk about Eurovision.
[00:02:37.680 --> 00:02:42.240]   No, we're going to talk about GDPR.
[00:02:42.240 --> 00:02:44.760]   But first, let's introduce the rest of the panel.
[00:02:44.760 --> 00:02:49.480]   Christina Warren is also here, senior cloud dev advocate at Microsoft Film Girl.
[00:02:49.480 --> 00:02:50.480]   Hello, Christina.
[00:02:50.480 --> 00:02:51.480]   Yes.
[00:02:51.480 --> 00:02:52.480]   Channel 9 Live.
[00:02:52.480 --> 00:02:53.480]   Channel 9 Live.
[00:02:53.480 --> 00:03:01.680]   Yeah, I'm doing this from my actual Microsoft Office this time because I was trying to find
[00:03:01.680 --> 00:03:04.480]   my USB-C cable on Friday.
[00:03:04.480 --> 00:03:08.480]   I was working from home and tore my actual office at home apart.
[00:03:08.480 --> 00:03:10.320]   You didn't have a USB-C cable?
[00:03:10.320 --> 00:03:11.320]   No, I did.
[00:03:11.320 --> 00:03:12.320]   And I couldn't find one.
[00:03:12.320 --> 00:03:14.240]   So because I was trying to find my switch.
[00:03:14.240 --> 00:03:16.880]   And anyway, I tore things apart, couldn't find it, ended up ordering.
[00:03:16.880 --> 00:03:21.000]   It was faster to just order one from Amazon from Prime now and get it within an hour.
[00:03:21.000 --> 00:03:23.000]   Honestly, that's what I wound up doing because that's...
[00:03:23.000 --> 00:03:24.000]   Wow.
[00:03:24.000 --> 00:03:25.000]   Wow.
[00:03:25.000 --> 00:03:26.000]   Well, thank you.
[00:03:26.000 --> 00:03:29.160]   So I didn't want it to be messy as it always is when I'm on your show.
[00:03:29.160 --> 00:03:32.520]   I always feel terrible because everyone sees what wreckage I live in.
[00:03:32.520 --> 00:03:36.320]   So now you can see my office.
[00:03:36.320 --> 00:03:40.400]   How far away from your house is your office?
[00:03:40.400 --> 00:03:42.280]   It's like 20, 25 minutes.
[00:03:42.280 --> 00:03:44.280]   Oh, well, thank you for driving all the way in.
[00:03:44.280 --> 00:03:45.480]   Oh, I took an Uber.
[00:03:45.480 --> 00:03:46.480]   It's fine.
[00:03:46.480 --> 00:03:47.480]   And that's good.
[00:03:47.480 --> 00:03:48.480]   Not a problem.
[00:03:48.480 --> 00:03:49.480]   Oh, I took an Uber.
[00:03:49.480 --> 00:03:50.480]   It's okay.
[00:03:50.480 --> 00:03:51.480]   It's okay.
[00:03:51.480 --> 00:03:52.480]   It's very modern.
[00:03:52.480 --> 00:03:55.640]   Also with his first time and I'm thrilled to have him, the host of the new Tech Meme
[00:03:55.640 --> 00:03:57.760]   Daily podcast Ride Home, Brian McCulloch.
[00:03:57.760 --> 00:04:00.760]   Of course, you know him from the Internet History podcast.
[00:04:00.760 --> 00:04:03.440]   You've been doing that since when?
[00:04:03.440 --> 00:04:04.720]   It's not quite five years.
[00:04:04.720 --> 00:04:08.400]   It's four because it was right before my daughter was born and she just had her fourth birthday.
[00:04:08.400 --> 00:04:11.000]   So let's say four years and five months or something.
[00:04:11.000 --> 00:04:12.800]   That's an easy way to keep track of it.
[00:04:12.800 --> 00:04:13.800]   Yeah.
[00:04:13.800 --> 00:04:16.000]   Again, Leo, like I said, everything happening at once.
[00:04:16.000 --> 00:04:17.000]   Yeah.
[00:04:17.000 --> 00:04:18.800]   Launching podcasts, having kids.
[00:04:18.800 --> 00:04:19.800]   Yeah.
[00:04:19.800 --> 00:04:22.800]   And Brian is running a book right now too about the history of the Internet, which should
[00:04:22.800 --> 00:04:23.800]   be very interesting.
[00:04:23.800 --> 00:04:28.800]   Book is written coming out in October called How the Internet Happened.
[00:04:28.800 --> 00:04:29.800]   Available for pre-order.
[00:04:29.800 --> 00:04:30.800]   Oh.
[00:04:30.800 --> 00:04:33.840]   So it's written and they're holding it till October?
[00:04:33.840 --> 00:04:38.320]   Well, so you have, they give you the day.
[00:04:38.320 --> 00:04:39.320]   What are these, the galleys?
[00:04:39.320 --> 00:04:40.320]   Galleys?
[00:04:40.320 --> 00:04:41.320]   Yep.
[00:04:41.320 --> 00:04:42.320]   So I still have to mark up the galleys.
[00:04:42.320 --> 00:04:43.320]   Oh, I hated that.
[00:04:43.320 --> 00:04:44.920]   I used to hate that.
[00:04:44.920 --> 00:04:45.920]   But right.
[00:04:45.920 --> 00:04:49.160]   I don't know what they do for the next six months once we lock down the actual, you
[00:04:49.160 --> 00:04:56.160]   know, they set it in cold type, one letter at a time, inch by inch.
[00:04:56.160 --> 00:04:59.800]   So we got a boy, we have a few things to talk about.
[00:04:59.800 --> 00:05:02.920]   I want to ask you, Christina, I haven't seen you since build.
[00:05:02.920 --> 00:05:03.920]   It was built fun for you.
[00:05:03.920 --> 00:05:05.080]   Do you have a good time?
[00:05:05.080 --> 00:05:06.080]   I did.
[00:05:06.080 --> 00:05:10.880]   I was extremely busy because I was executive producing the build live stream.
[00:05:10.880 --> 00:05:15.440]   So we had content that was airing in between the sessions we were broadcasting live and
[00:05:15.440 --> 00:05:19.440]   ahead of the keynotes and then, you know, end of the day, wrap ups.
[00:05:19.440 --> 00:05:25.360]   And then we were also recording things on that stage when we were not live on the air.
[00:05:25.360 --> 00:05:29.000]   And so I was hosting stuff, but I was also coordinating all the content and guests and
[00:05:29.000 --> 00:05:30.000]   stuff.
[00:05:30.000 --> 00:05:33.360]   So I was very busy, but I had a great time and it was a good show and it was, it was
[00:05:33.360 --> 00:05:37.760]   really interesting to be at build, not as a reporter, but as a Microsoft employee, it
[00:05:37.760 --> 00:05:39.720]   was a little weird, but it was, it was cool.
[00:05:39.720 --> 00:05:40.720]   Yeah.
[00:05:40.720 --> 00:05:42.680]   It's a very different way to see build.
[00:05:42.680 --> 00:05:44.720]   What's your, what was your takeaway?
[00:05:44.720 --> 00:05:46.920]   What do you think was the most exciting stuff?
[00:05:46.920 --> 00:05:53.400]   Obviously, I think a lot of the focus on, on AI and kind of the, the future potential
[00:05:53.400 --> 00:05:58.120]   around that, especially some of the, the IoT edge stuff was really interesting.
[00:05:58.120 --> 00:06:01.800]   I really liked that DJI demo from a consumer perspective.
[00:06:01.800 --> 00:06:07.640]   I really want to see whatever the, you know, the phone, your phone app on, on, on Windows
[00:06:07.640 --> 00:06:12.480]   10 to see how well that will be able to integrate to kind of mirror things from, you know, your
[00:06:12.480 --> 00:06:13.760]   phone onto your computer.
[00:06:13.760 --> 00:06:16.600]   I think that's very cool.
[00:06:16.600 --> 00:06:22.320]   And we had, we had many horses, Leo, like that was, there were many, a lot of wood behind
[00:06:22.320 --> 00:06:23.320]   those zeros.
[00:06:23.320 --> 00:06:24.320]   Yeah.
[00:06:24.320 --> 00:06:25.800]   No, there were, there were many horses.
[00:06:25.800 --> 00:06:28.800]   Like we had like little like bunnies and, and, and puppies.
[00:06:28.800 --> 00:06:29.800]   Actual horses.
[00:06:29.800 --> 00:06:30.800]   Actual horses.
[00:06:30.800 --> 00:06:31.800]   I'm sorry.
[00:06:31.800 --> 00:06:32.800]   I misunderstood.
[00:06:32.800 --> 00:06:35.560]   There, there were actual equine animals.
[00:06:35.560 --> 00:06:36.560]   Yes.
[00:06:36.560 --> 00:06:39.040]   There were therapy animals and they were amazing.
[00:06:39.040 --> 00:06:40.320]   Are you teasing me?
[00:06:40.320 --> 00:06:41.320]   No, I'm not.
[00:06:41.320 --> 00:06:42.760]   I swear to God.
[00:06:42.760 --> 00:06:43.760]   Why would there?
[00:06:43.760 --> 00:06:44.760]   Okay.
[00:06:44.760 --> 00:06:47.960]   Therapy animals because it was amazing and people very much enjoyed them.
[00:06:47.960 --> 00:06:51.400]   And that was, I know it wasn't like a tech, you know, announcement or whatever, but I
[00:06:51.400 --> 00:06:52.400]   was very excited about it.
[00:06:52.400 --> 00:06:54.240]   Full size horses or ponies?
[00:06:54.240 --> 00:06:55.240]   Many horses.
[00:06:55.240 --> 00:06:56.240]   Like little Sebastian.
[00:06:56.240 --> 00:06:57.240]   Oh, many horses.
[00:06:57.240 --> 00:06:58.240]   Yeah.
[00:06:58.240 --> 00:07:00.280]   Oh, that's kind of cute.
[00:07:00.280 --> 00:07:01.280]   Yeah.
[00:07:01.280 --> 00:07:02.280]   Can I?
[00:07:02.280 --> 00:07:04.280]   Can you bring them on a plane?
[00:07:04.280 --> 00:07:05.280]   Great.
[00:07:05.280 --> 00:07:07.320]   No, it's from a local therapy animal.
[00:07:07.320 --> 00:07:08.320]   Oh my God.
[00:07:08.320 --> 00:07:09.320]   A place.
[00:07:09.320 --> 00:07:10.320]   So wait a minute.
[00:07:10.320 --> 00:07:13.560]   Not only was the keynote so long, three hours or 45 minutes that they had you do exercises
[00:07:13.560 --> 00:07:17.640]   in the middle, but then they trot out many ponies for you to feel better.
[00:07:17.640 --> 00:07:18.640]   Yes.
[00:07:18.640 --> 00:07:20.400]   But that's why they had to have it.
[00:07:20.400 --> 00:07:22.640]   The keynote was so long by the end.
[00:07:22.640 --> 00:07:23.640]   They were shaking.
[00:07:23.640 --> 00:07:25.200]   The meanie horses.
[00:07:25.200 --> 00:07:26.200]   You know, it's funny.
[00:07:26.200 --> 00:07:29.240]   And I probably had something to do with how many things Microsoft announced it built, but
[00:07:29.240 --> 00:07:32.720]   they held, I thought, one of the most interesting announcements for after build.
[00:07:32.720 --> 00:07:34.680]   And I'm talking about the Surface Hub 2.
[00:07:34.680 --> 00:07:36.280]   Yeah, the Surface Hub 2.
[00:07:36.280 --> 00:07:37.280]   Yeah.
[00:07:37.280 --> 00:07:39.280]   Now, are you using this in your conference room yet?
[00:07:39.280 --> 00:07:40.280]   Because this is the smallest thing.
[00:07:40.280 --> 00:07:45.160]   No, no, I have not seen one in person yet, but I have used the first Surface Hub.
[00:07:45.160 --> 00:07:46.160]   And that's amazing.
[00:07:46.160 --> 00:07:50.320]   And just this demo video, I have to say, like, and I, this is not a reflection of where
[00:07:50.320 --> 00:07:51.320]   I work at all.
[00:07:51.320 --> 00:07:52.320]   This is seriously cool.
[00:07:52.320 --> 00:07:54.240]   Like, this is the sort of thing.
[00:07:54.240 --> 00:07:59.040]   And it wasn't until I worked in a big corporate environment where you see a lot of these things
[00:07:59.040 --> 00:08:02.240]   that I actually understood the potential of this sort of stuff.
[00:08:02.240 --> 00:08:03.680]   But it's very cool.
[00:08:03.680 --> 00:08:06.880]   So Microsoft bought a company called PixelPerfect, which made giant displays.
[00:08:06.880 --> 00:08:09.120]   They released the Hub a year ago, right?
[00:08:09.120 --> 00:08:10.680]   They're an initial hub.
[00:08:10.680 --> 00:08:12.640]   But this is the follow-up Surface Hub 2.
[00:08:12.640 --> 00:08:13.640]   They haven't announced pricing.
[00:08:13.640 --> 00:08:16.040]   They say it'll be next year, I think, that it'll be available.
[00:08:16.040 --> 00:08:23.440]   But it's a giant Windows-based display that, well, has a couple of interesting features.
[00:08:23.440 --> 00:08:27.400]   There's one right now within the video we're watching, which is you can rotate from landscape
[00:08:27.400 --> 00:08:33.120]   to portrait mode, and you can then do Skype calls with a human, a full-size human standing
[00:08:33.120 --> 00:08:38.240]   across from you on this giant display.
[00:08:38.240 --> 00:08:40.680]   That's wild.
[00:08:40.680 --> 00:08:47.680]   And then, of course, you could put two side-by-side in landscape mode, or even cooler if you put
[00:08:47.680 --> 00:08:49.040]   them in the portrait mode.
[00:08:49.040 --> 00:08:50.680]   I think you can have, was it eight?
[00:08:50.680 --> 00:08:52.400]   You can have a bunch of them.
[00:08:52.400 --> 00:08:53.400]   It's all four.
[00:08:53.400 --> 00:08:54.400]   It's maybe it's four.
[00:08:54.400 --> 00:08:56.840]   There it is, four, like that.
[00:08:56.840 --> 00:08:59.160]   All communicating with one another.
[00:08:59.160 --> 00:09:03.000]   Microsoft said they'll be less expensive than the existing Surface Hub, but I don't know
[00:09:03.000 --> 00:09:05.000]   what that means.
[00:09:05.000 --> 00:09:06.000]   The existing hub is pretty expensive.
[00:09:06.000 --> 00:09:08.480]   It's like $8,000 for the 55-inch.
[00:09:08.480 --> 00:09:10.280]   Well, no, Christina though.
[00:09:10.280 --> 00:09:11.280]   It's definitely impressive.
[00:09:11.280 --> 00:09:17.760]   But I wonder how does it actually work that well that you can send data from your computer
[00:09:17.760 --> 00:09:18.760]   to the thing?
[00:09:18.760 --> 00:09:23.840]   Like, how integrated does your system at work have to be for that to actually work like
[00:09:23.840 --> 00:09:24.840]   that?
[00:09:24.840 --> 00:09:25.840]   Because if it does, it's magic.
[00:09:25.840 --> 00:09:26.840]   So cool.
[00:09:26.840 --> 00:09:27.840]   Look at that.
[00:09:27.840 --> 00:09:28.840]   That's the four of these four.
[00:09:28.840 --> 00:09:32.080]   So it's basically, I can't speak for whatever the new one is, but the existing one which
[00:09:32.080 --> 00:09:36.480]   has some of the similar features where you can just wirelessly transmit things, it uses
[00:09:36.480 --> 00:09:39.440]   kind of like a custom version of Windows 10.
[00:09:39.440 --> 00:09:43.520]   And so if you have a Windows 10 device and if you're on the same Active Directory thing,
[00:09:43.520 --> 00:09:47.480]   and I'm not really sure how it all needs to be configured, then you can wirelessly send
[00:09:47.480 --> 00:09:49.080]   files very easily.
[00:09:49.080 --> 00:09:51.320]   And it actually does work really well.
[00:09:51.320 --> 00:09:56.400]   And then if you don't have, if you're using an iOS or an Android or Mac device, you can
[00:09:56.400 --> 00:09:58.240]   either use, it has Miracast built in.
[00:09:58.240 --> 00:10:04.840]   So if you were on Android or Chrome or a non-surface Windows machine that didn't have
[00:10:04.840 --> 00:10:08.280]   something, you could use Miracast to send stuff over.
[00:10:08.280 --> 00:10:14.800]   But there are apps like AirCast or AirServer, sorry, that'll kind of add an AirPlay type
[00:10:14.800 --> 00:10:19.520]   of feature so you can still use that as a way to wirelessly send information.
[00:10:19.520 --> 00:10:20.520]   It works really well.
[00:10:20.520 --> 00:10:25.320]   Obviously, it works far better of everything you're using as Windows 10.
[00:10:25.320 --> 00:10:28.800]   But that's easily doable because this is so expensive a company is going to buy this,
[00:10:28.800 --> 00:10:30.640]   they're going to buy it, it's an integrated solution.
[00:10:30.640 --> 00:10:34.840]   I mean, you're going to put it, you're going to wire a whole conference room to use this.
[00:10:34.840 --> 00:10:35.840]   Exactly.
[00:10:35.840 --> 00:10:36.840]   And then it works with your phone system.
[00:10:36.840 --> 00:10:41.440]   So the really brilliant part, assuming your phone system works and let's be honest, everybody's
[00:10:41.440 --> 00:10:46.120]   offices, that's always the part that's the big thing is you can walk in and you can
[00:10:46.120 --> 00:10:52.120]   touch a button and start the Skype call that's already been programmed into the room and
[00:10:52.120 --> 00:10:59.480]   you can touch the screen or start it from your computer and then fling it to the hub
[00:10:59.480 --> 00:11:06.360]   that's then recording stuff but also projecting whoever is presenting information.
[00:11:06.360 --> 00:11:15.720]   I'm hoping the price will be sub $4,000 because I would, we, when we first set up this, so
[00:11:15.720 --> 00:11:20.360]   right now we're doing the show there, I'm in the studio, but the rest of you are all
[00:11:20.360 --> 00:11:24.520]   on Skype and what we do is we put you in TV screens around the round table so it's like
[00:11:24.520 --> 00:11:25.520]   you're there.
[00:11:25.520 --> 00:11:30.320]   But I always wanted to have it be portrait mode, more full size and I could tell I would
[00:11:30.320 --> 00:11:35.120]   dig it if you guys were, you know, like right here at the table, it would be, that was always
[00:11:35.120 --> 00:11:38.360]   the plan, you know, we just couldn't, it was too hard to make it work, too expensive to
[00:11:38.360 --> 00:11:39.360]   make it work.
[00:11:39.360 --> 00:11:43.400]   But maybe this will, maybe this time next year, Christina'll be right here just like that.
[00:11:43.400 --> 00:11:44.400]   That would be awesome.
[00:11:44.400 --> 00:11:52.280]   So you do at these events, you see demos and as many people are quick to point out, there's
[00:11:52.280 --> 00:11:56.160]   a big gulf between a demo and reality.
[00:11:56.160 --> 00:12:02.320]   And now two weeks after Google I/O, we're hearing more and more people wondering about
[00:12:02.320 --> 00:12:05.920]   the Google duplex that they demoed.
[00:12:05.920 --> 00:12:12.400]   That's the very accurate voice assistant that called and made reservations for dinner that
[00:12:12.400 --> 00:12:14.520]   called and made haircut appointments.
[00:12:14.520 --> 00:12:17.720]   And maybe we were a little too blindly accepting of that.
[00:12:17.720 --> 00:12:21.240]   Google's now admitted, yeah, we edited it a little bit so you wouldn't hear the restaurant
[00:12:21.240 --> 00:12:22.240]   location.
[00:12:22.240 --> 00:12:25.840]   And that you could tell they didn't put last names in and things like that.
[00:12:25.840 --> 00:12:32.440]   John Gruber said, hey, let's figure out what restaurant they're in because they, they,
[00:12:32.440 --> 00:12:36.600]   on the, on the blog posts for duplex, they posted a picture.
[00:12:36.600 --> 00:12:43.520]   They said, we're eating the lunch that we booked with Google duplex and people responded
[00:12:43.520 --> 00:12:46.960]   on Twitter and found it.
[00:12:46.960 --> 00:12:56.320]   And then Mashable, yes, Mashable called the restaurant following up on this.
[00:12:56.320 --> 00:13:01.920]   And it was called Hong's gourmet in Mountain View, of course, near the Googleplex.
[00:13:01.920 --> 00:13:08.720]   And here's, here's Jack Morse at Mashable writing when I called a woman answer the phone
[00:13:08.720 --> 00:13:12.680]   after explaining I was reportable with Mashable and I was curious about Google employees eating
[00:13:12.680 --> 00:13:15.480]   there after using an AI to make reservations.
[00:13:15.480 --> 00:13:17.440]   She said, well, let me put you on the phone with Victor.
[00:13:17.440 --> 00:13:19.360]   Victor got on the phone.
[00:13:19.360 --> 00:13:21.560]   I asked him if the AI had made a reservation.
[00:13:21.560 --> 00:13:26.520]   He said, yeah, I asked him, and this is important, if Google had let him know about the plan
[00:13:26.520 --> 00:13:29.120]   duplex test in advance, he said, no, no, of course not.
[00:13:29.120 --> 00:13:33.000]   And when I asked him to confirm one more time that duplex had called Hong's gourmet, he appeared
[00:13:33.000 --> 00:13:34.000]   to get nervous.
[00:13:34.000 --> 00:13:37.000]   And said he had to go and hung up.
[00:13:37.000 --> 00:13:43.160]   So Leo, I mean, obviously we're going to get into the idea of the consenting to talking
[00:13:43.160 --> 00:13:44.160]   to a robot thing.
[00:13:44.160 --> 00:13:49.480]   But is the other half of this are people starting to say that this was all staged and
[00:13:49.480 --> 00:13:51.040]   maybe this technology doesn't work?
[00:13:51.040 --> 00:13:52.040]   Okay.
[00:13:52.040 --> 00:13:56.200]   One of the reasons they say that is because California is a strict two party state, as
[00:13:56.200 --> 00:14:03.120]   you know, Brian, which means you cannot record a call unless both parties give consent,
[00:14:03.120 --> 00:14:04.120]   explicit consent.
[00:14:04.120 --> 00:14:05.120]   I'm recording.
[00:14:05.120 --> 00:14:06.680]   I would not even I'm recording.
[00:14:06.680 --> 00:14:08.760]   I know this because I work in radio in California.
[00:14:08.760 --> 00:14:11.560]   You can't say you can't have somebody on the phone be recording and say, I'm recording.
[00:14:11.560 --> 00:14:12.560]   Is that okay?
[00:14:12.560 --> 00:14:15.120]   You have to say before you record, I would like to record this.
[00:14:15.120 --> 00:14:17.400]   Do I have your permission?
[00:14:17.400 --> 00:14:22.120]   When they say, yes, you press record, then you say, I got your permission to record this.
[00:14:22.120 --> 00:14:25.440]   I would like you to say again, I have your permission, right?
[00:14:25.440 --> 00:14:26.440]   Yes.
[00:14:26.440 --> 00:14:28.760]   That's the only way you get to stay in trouble.
[00:14:28.760 --> 00:14:31.960]   None of that seemed to have happened with Google.
[00:14:31.960 --> 00:14:37.120]   But the other thing is, is because as people then did follow up right away and on the blog
[00:14:37.120 --> 00:14:40.480]   coast, Google admitted this, that it's already a very constrained system.
[00:14:40.480 --> 00:14:45.160]   You can't, you couldn't talk to the computer right now and have it pass a Turing test or
[00:14:45.160 --> 00:14:46.160]   whatever.
[00:14:46.160 --> 00:14:47.160]   You wouldn't be fooled.
[00:14:47.160 --> 00:14:51.840]   Well, I was, you know, this is my fault because I thought it passed that I thought this is
[00:14:51.840 --> 00:14:53.080]   the Turing test.
[00:14:53.080 --> 00:14:57.960]   But only for ordering or checking hours for making a reservation.
[00:14:57.960 --> 00:15:00.560]   It's still a very constrained use.
[00:15:00.560 --> 00:15:01.560]   Right.
[00:15:01.560 --> 00:15:03.880]   And the human wasn't asked to, it was half of it.
[00:15:03.880 --> 00:15:07.280]   The human wasn't asked to distinguish between a computer and a human.
[00:15:07.280 --> 00:15:09.680]   And had they been asked, maybe they would have been able to tell that voice.
[00:15:09.680 --> 00:15:11.080]   Well, that was a little odd.
[00:15:11.080 --> 00:15:13.240]   So maybe it wasn't a human.
[00:15:13.240 --> 00:15:14.240]   But it was pretty impressive.
[00:15:14.240 --> 00:15:15.240]   Right.
[00:15:15.240 --> 00:15:16.240]   Which is.
[00:15:16.240 --> 00:15:17.240]   Which is.
[00:15:17.240 --> 00:15:19.400]   Sorry, go on.
[00:15:19.400 --> 00:15:28.280]   Which is my question, are people now starting to suggest that, oh, this technology, it's
[00:15:28.280 --> 00:15:31.200]   kind of vapor-warish where like, oh, yeah, that'd be great if it works.
[00:15:31.200 --> 00:15:34.360]   But it's still five years away and they tricked us off.
[00:15:34.360 --> 00:15:36.760]   Well, we don't know.
[00:15:36.760 --> 00:15:40.760]   But it comes down to whether or not you trust Google with these kinds of things.
[00:15:40.760 --> 00:15:45.600]   And I can imagine that it's not completely ready yet for prime time.
[00:15:45.600 --> 00:15:51.120]   But if Google is making that kind of demo, I can't imagine that it wouldn't be close
[00:15:51.120 --> 00:15:55.920]   enough that in a few months or maybe a couple of years, it will work reliably well.
[00:15:55.920 --> 00:15:58.760]   It will not be as perfect as this.
[00:15:58.760 --> 00:16:04.640]   But I would be very surprised if, you know, it's completely doctored and fabricated.
[00:16:04.640 --> 00:16:08.840]   And this is what some people are kind of implying, like, oh, it was edited.
[00:16:08.840 --> 00:16:12.080]   So it holds no value, basically.
[00:16:12.080 --> 00:16:13.280]   They lied to us.
[00:16:13.280 --> 00:16:15.760]   I think that's a bit over the top.
[00:16:15.760 --> 00:16:19.320]   I agree with that, although I do have to say, like, I definitely think that the most people
[00:16:19.320 --> 00:16:21.960]   who watch it when I first saw it, and I think it's a very impressive demo.
[00:16:21.960 --> 00:16:26.080]   And I think we should acknowledge the fact that nearly two weeks later, the fact that
[00:16:26.080 --> 00:16:29.680]   we're still talking about this, regardless of it's because people have questions about
[00:16:29.680 --> 00:16:32.160]   it or not, it goes to the strength of the demo.
[00:16:32.160 --> 00:16:33.720]   We are still talking about this.
[00:16:33.720 --> 00:16:38.440]   But for me, you know, when I saw it as impressive as it was, my Spidey sense was kind of like,
[00:16:38.440 --> 00:16:40.840]   okay, well, this is not ready.
[00:16:40.840 --> 00:16:43.680]   The way that they are showing this was a very specific scenario.
[00:16:43.680 --> 00:16:47.000]   This is a best-case scenario demo, as most demonstrations are.
[00:16:47.000 --> 00:16:50.280]   And this has probably been edited even more so.
[00:16:50.280 --> 00:16:56.000]   That said, I think that because there is so much ambiguity around, you know, how do they
[00:16:56.000 --> 00:17:02.000]   call, what were the parameters, what types of information is required on the other end,
[00:17:02.000 --> 00:17:07.600]   what signals does it pick up on, what are some of the broader privacy implications, what
[00:17:07.600 --> 00:17:12.800]   are some of the broader ethical implications, people are starting to ask those other questions.
[00:17:12.800 --> 00:17:14.560]   But I kind of feel like they're two different things.
[00:17:14.560 --> 00:17:19.840]   And while I'm perfectly happy to have people kind of talk about like how real of a demo
[00:17:19.840 --> 00:17:23.600]   is this, sometimes I feel like some of the criticism about this, which is getting into
[00:17:23.600 --> 00:17:28.720]   the two-party consent laws and all that stuff, is kind of going into a rabbit hole, which
[00:17:28.720 --> 00:17:29.880]   shouldn't really be the focus.
[00:17:29.880 --> 00:17:32.880]   I feel like if we should be focusing on the ethical implications of this, we should be
[00:17:32.880 --> 00:17:38.120]   talking about the broader AI implications of talking to AIs and things calling and how
[00:17:38.120 --> 00:17:43.280]   should they be identifying themselves and not trying to nitpick a doctor demo and say,
[00:17:43.280 --> 00:17:48.120]   well, did they announce that they were making the recording and did the restaurant know?
[00:17:48.120 --> 00:17:52.560]   Because I don't feel like Google would have put recordings on the internet if they didn't
[00:17:52.560 --> 00:17:54.240]   have them cleared in some way.
[00:17:54.240 --> 00:17:55.240]   I don't know.
[00:17:55.240 --> 00:17:57.040]   Maybe they would, but I feel like Google's lawyers wouldn't do that.
[00:17:57.040 --> 00:18:01.480]   They need to step forward and say that because I would disagree with you to this extent.
[00:18:01.480 --> 00:18:10.720]   If they play fast and lose with this now, that is a problem down the road because companies
[00:18:10.720 --> 00:18:15.320]   that create are creating AIs are going to, I think, have a very high ethical bar that
[00:18:15.320 --> 00:18:19.160]   they have to cross to.
[00:18:19.160 --> 00:18:26.160]   And so if there's questions at this early stage, that's a little troubling.
[00:18:26.160 --> 00:18:29.360]   It's mostly tech crunch, by the way, and we should say this is tech crunch has decided
[00:18:29.360 --> 00:18:34.880]   to make this be their, you know, they're going to really go after Google on this one.
[00:18:34.880 --> 00:18:35.880]   But there are others.
[00:18:35.880 --> 00:18:37.800]   Here's Zaneb Tufekki, who I really love.
[00:18:37.800 --> 00:18:40.160]   I think she's brilliant.
[00:18:40.160 --> 00:18:44.840]   She tweeted, Google assistant making calls pretending to be human, not only without disclosing
[00:18:44.840 --> 00:18:49.520]   that it's a bot, but adding Om and Ah to deceive the human and the other end with the room
[00:18:49.520 --> 00:18:52.440]   cheering it, horrifying.
[00:18:52.440 --> 00:18:54.360]   Silicon Valley is ethically lost.
[00:18:54.360 --> 00:18:56.720]   Rutterless has not learned a thing.
[00:18:56.720 --> 00:18:57.720]   No?
[00:18:57.720 --> 00:18:59.720]   You disagree, Andrew?
[00:18:59.720 --> 00:19:00.720]   Completely.
[00:19:00.720 --> 00:19:04.560]   I mean, I understand, you know, there's such a thing as hyperbole, and I think this is
[00:19:04.560 --> 00:19:07.080]   a fine example of it.
[00:19:07.080 --> 00:19:12.160]   I understand why there would be concern about the way they went about doing it.
[00:19:12.160 --> 00:19:18.360]   But the phrasing she uses and the phrasing that some others have been using, I think,
[00:19:18.360 --> 00:19:21.320]   is uncalled for.
[00:19:21.320 --> 00:19:24.000]   And that is the problem with the internet, right?
[00:19:24.000 --> 00:19:27.640]   Everything, every little issue becomes a world and a problem.
[00:19:27.640 --> 00:19:28.640]   Yes.
[00:19:28.640 --> 00:19:29.640]   Yes.
[00:19:29.640 --> 00:19:33.560]   And I completely understand that we should ask the question and go to Google and say,
[00:19:33.560 --> 00:19:35.040]   "Hey, wait a second now."
[00:19:35.040 --> 00:19:36.840]   So this is what you did.
[00:19:36.840 --> 00:19:40.480]   Did you start thinking about the ethical implications of this?
[00:19:40.480 --> 00:19:42.080]   Should we know when a bot is calling?
[00:19:42.080 --> 00:19:46.400]   And in this case, the point is that we shouldn't know so that it can use systems that haven't
[00:19:46.400 --> 00:19:51.440]   been prepared for automated booking.
[00:19:51.440 --> 00:19:55.720]   But going that far, I think, is, I'm not going to say disingenuous.
[00:19:55.720 --> 00:20:02.960]   I don't know who she is, but at the very least, it seems like hyperbole that I'm wary of these
[00:20:02.960 --> 00:20:03.960]   days.
[00:20:03.960 --> 00:20:07.680]   So, St. Ephes is a very well-known academic who writes about these subjects and has been
[00:20:07.680 --> 00:20:17.880]   very outspoken about companies being surveillance capitalism and companies being really intrusive.
[00:20:17.880 --> 00:20:18.880]   So this is--
[00:20:18.880 --> 00:20:20.040]   Why I think that's fair.
[00:20:20.040 --> 00:20:21.520]   I mean, look, I think that there are--
[00:20:21.520 --> 00:20:22.520]   It is a concern, of course.
[00:20:22.520 --> 00:20:23.520]   --series ethical questions.
[00:20:23.520 --> 00:20:25.120]   I think there are huge ethical questions around this.
[00:20:25.120 --> 00:20:28.560]   My bigger question, honestly, though, other than this specific demo-- and this is where
[00:20:28.560 --> 00:20:29.800]   I kind of get hung up on this.
[00:20:29.800 --> 00:20:34.160]   Like, I feel like there are very big 10,000-foot questions we should be asking about this whole
[00:20:34.160 --> 00:20:35.160]   project.
[00:20:35.160 --> 00:20:38.840]   I just don't know if it behooves us to focus on the intricacies of the demos they showed
[00:20:38.840 --> 00:20:39.840]   off.
[00:20:39.840 --> 00:20:41.720]   But we should be asking questions of the program.
[00:20:41.720 --> 00:20:46.960]   When we talk about two-party consent, I don't really care if they asked on these calls
[00:20:46.960 --> 00:20:51.160]   for two-party consent if Hong's restaurant gave permission to be recorded or not.
[00:20:51.160 --> 00:20:53.640]   That doesn't really-- like, that's going down a rabbit hole.
[00:20:53.640 --> 00:20:57.640]   But concerns me more is, are they going to be capturing these calls?
[00:20:57.640 --> 00:20:59.520]   Because presumably they are.
[00:20:59.520 --> 00:21:03.120]   And because they're going to be using it for their machine learning models.
[00:21:03.120 --> 00:21:05.520]   And in that case, how is it going to be identified?
[00:21:05.520 --> 00:21:07.720]   And how is that going to work with the law?
[00:21:07.720 --> 00:21:11.640]   And how are people going to be notified that they're talking to machines?
[00:21:11.640 --> 00:21:15.480]   Like that, to me, is a much bigger question.
[00:21:15.480 --> 00:21:19.000]   Back to Leo's point, though, I think the fact, what he said right at the beginning, that
[00:21:19.000 --> 00:21:23.560]   this is so early in this sort of AI stuff that we're going to have to encounter over
[00:21:23.560 --> 00:21:24.560]   and over again.
[00:21:24.560 --> 00:21:28.120]   There was a great-- I don't remember if it was a Twitter thread or a blog post or whatever,
[00:21:28.120 --> 00:21:32.680]   but it was some computer scientists that made the point that every other discipline has had
[00:21:32.680 --> 00:21:39.520]   its moment of being chastened by their technology being more than they thought it would be.
[00:21:39.520 --> 00:21:43.240]   So the reason we have a Nobel Prize is because chemists thought that they were discovering
[00:21:43.240 --> 00:21:44.240]   the liberal--
[00:21:44.240 --> 00:21:45.240]   >> Yeah, for Nobel events.
[00:21:45.240 --> 00:21:46.240]   >> --the liberal developers.
[00:21:46.240 --> 00:21:47.240]   >> --of the university.
[00:21:47.240 --> 00:21:48.240]   >> And they invented dynamite.
[00:21:48.240 --> 00:21:50.800]   And then obviously physicists thought they were unlocking the secrets of the atom and the big
[00:21:50.800 --> 00:21:51.800]   bang.
[00:21:51.800 --> 00:21:53.280]   And then they created nuclear weapons.
[00:21:53.280 --> 00:21:59.680]   And so his point was computer scientists and computer science as a discipline hasn't had
[00:21:59.680 --> 00:22:01.400]   that moment yet.
[00:22:01.400 --> 00:22:06.040]   And he was like, I would hope that as computer scientists, we would get ahead of that.
[00:22:06.040 --> 00:22:09.400]   It wouldn't take, oh, we've created a nuclear bomb to--
[00:22:09.400 --> 00:22:10.880]   >> Now let's figure out.
[00:22:10.880 --> 00:22:12.200]   Now let's solve the problem.
[00:22:12.200 --> 00:22:17.840]   >> If you're a biologist, you know-- I think two Leo's point and two Zanep's point is that
[00:22:17.840 --> 00:22:20.480]   computer science tends to-- oh, this is a cool thing.
[00:22:20.480 --> 00:22:21.480]   Let's just do it.
[00:22:21.480 --> 00:22:27.880]   Because if you're a biologist, you don't just create a new virus just because you can.
[00:22:27.880 --> 00:22:31.320]   You think about it and you put safeguards in place.
[00:22:31.320 --> 00:22:33.360]   >> This is what Satya Nandela was talking about at Build.
[00:22:33.360 --> 00:22:34.680]   He was talking about ethics.
[00:22:34.680 --> 00:22:39.200]   >> I was going to say, I mean-- and I'm trying not to come across like a shill here, but
[00:22:39.200 --> 00:22:42.720]   one of the things that I think about a lot and the people that I work with think about
[00:22:42.720 --> 00:22:45.080]   a ton of people who are actively involved in the AI stuff.
[00:22:45.080 --> 00:22:46.080]   And I'm certainly not.
[00:22:46.080 --> 00:22:49.760]   I just kind of talk to developers about things around it, our ethics.
[00:22:49.760 --> 00:22:53.400]   And I think that it's something that every major company who's involved with AI, including
[00:22:53.400 --> 00:22:55.120]   Google, is very involved with.
[00:22:55.120 --> 00:22:59.680]   Now, how much of an emphasis you put on that in your consumer-facing demos is a different
[00:22:59.680 --> 00:23:00.680]   question.
[00:23:00.680 --> 00:23:05.480]   But I am heartened and at least I'm hopeful-- I mean, maybe I'm a little worried as well,
[00:23:05.480 --> 00:23:09.760]   but I am hopeful that the experts who are working on this have to be aware of these
[00:23:09.760 --> 00:23:14.600]   things because if they're not, the governments who don't understand the tech are going to
[00:23:14.600 --> 00:23:16.560]   start putting limitations on what can be done.
[00:23:16.560 --> 00:23:20.440]   And that has, in my opinion, the potential to be even more damaging.
[00:23:20.440 --> 00:23:25.240]   So there are companies who are actively thinking about ethical concerns with this stuff.
[00:23:25.240 --> 00:23:28.760]   And I think that it's a conversation that everyone needs to have and that all of the
[00:23:28.760 --> 00:23:32.720]   scientists doing this really serious-- the engineers doing this really serious work
[00:23:32.720 --> 00:23:36.840]   need to be thinking about because you're right, we don't want to be in a situation where
[00:23:36.840 --> 00:23:40.680]   we've created a nuclear weapon before we know it without even realizing.
[00:23:40.680 --> 00:23:41.680]   >> So, Derp, try it.
[00:23:41.680 --> 00:23:42.680]   >> We can predict that.
[00:23:42.680 --> 00:23:49.680]   >> I echo Nadella's briefly for one sentence, Nadella's ethics thing.
[00:23:49.680 --> 00:23:58.280]   But this is timely because tonight on 60 Minutes, there's going to be a piece that already
[00:23:58.280 --> 00:23:59.880]   has hurt Google's stock price.
[00:23:59.880 --> 00:24:05.360]   The price went down on Friday called the Power of Google that's, I think, going to be a
[00:24:05.360 --> 00:24:09.320]   little bit critical of Google's power.
[00:24:09.320 --> 00:24:13.720]   There's GDPR, which is becoming law on Friday, we're going to talk about that.
[00:24:13.720 --> 00:24:17.160]   I think this is what Scott Galloway was talking about in the four.
[00:24:17.160 --> 00:24:19.440]   There is a reckoning coming to Silicon Valley.
[00:24:19.440 --> 00:24:21.120]   It hit Facebook.
[00:24:21.120 --> 00:24:26.480]   Maybe it's Google's turn where people are saying to them, "We expect you not just to
[00:24:26.480 --> 00:24:27.480]   do it because it's cool."
[00:24:27.480 --> 00:24:31.400]   And I think one of the things, Patrick, your response, I think, was my response too, which
[00:24:31.400 --> 00:24:32.400]   is, "Oh, this is cool.
[00:24:32.400 --> 00:24:33.480]   This is great.
[00:24:33.480 --> 00:24:36.640]   Let's not shoot the baby before it's born."
[00:24:36.640 --> 00:24:40.560]   But I think some of that comes from, "Tell me if I'm right or wrong, Patrick."
[00:24:40.560 --> 00:24:46.080]   The fact that we think it's cool and I can't wait, I'm excited about AI.
[00:24:46.080 --> 00:24:51.040]   But that's the problem is that engineers might not think about the consequences because we're
[00:24:51.040 --> 00:24:52.800]   just so excited.
[00:24:52.800 --> 00:25:01.320]   I think we are currently in a context where we are overly sensitive to ethics in IT issues.
[00:25:01.320 --> 00:25:05.960]   And certainly there is a good reason for it and it is needed.
[00:25:05.960 --> 00:25:11.800]   I do also think, however, that in this specific case, the concern is putting a little bit
[00:25:11.800 --> 00:25:14.480]   of the cart before the horse.
[00:25:14.480 --> 00:25:19.320]   This is a very specific use case and it couldn't have been done.
[00:25:19.320 --> 00:25:26.600]   You don't lay down ethics rules before you even know what the system is going to entail.
[00:25:26.600 --> 00:25:29.200]   This is basically a test.
[00:25:29.200 --> 00:25:34.280]   And you do the test and you see what happens and then you start, you might be thinking
[00:25:34.280 --> 00:25:39.240]   about the ethical implications beforehand, but you don't come out with this demo and
[00:25:39.240 --> 00:25:42.240]   say, "Okay, let's try this."
[00:25:42.240 --> 00:25:45.840]   Well, then should you come out with the demo and you don't have those things there.
[00:25:45.840 --> 00:25:51.360]   Apparently, according to some articles, there were people that were mad that they pushed
[00:25:51.360 --> 00:25:54.560]   this out as a, that's a very big stage.
[00:25:54.560 --> 00:25:58.880]   This is Google's biggest stage of the year that they pushed Google to do Plex out.
[00:25:58.880 --> 00:26:04.200]   And people at Google apparently felt like this was not the time to do that.
[00:26:04.200 --> 00:26:06.440]   But I understand ethics first, right?
[00:26:06.440 --> 00:26:09.920]   But in this case, this is like research.
[00:26:09.920 --> 00:26:11.560]   It's not application.
[00:26:11.560 --> 00:26:12.560]   It's not really application.
[00:26:12.560 --> 00:26:15.560]   Didn't they say it's going to be this year that they're going to do it?
[00:26:15.560 --> 00:26:17.560]   I mean, they said they were going to do it.
[00:26:17.560 --> 00:26:18.560]   They seem like it's a product.
[00:26:18.560 --> 00:26:19.560]   Yeah.
[00:26:19.560 --> 00:26:22.320]   They were going to try to start adding it to the Google Home.
[00:26:22.320 --> 00:26:25.840]   Now I don't think that we could use it in our Google Home by the end of the summer, but
[00:26:25.840 --> 00:26:30.560]   they were going to start testing it in Google Home use cases by the end of the summer.
[00:26:30.560 --> 00:26:32.960]   We are all computer literate people.
[00:26:32.960 --> 00:26:38.000]   What are the dangerous things that could come out of this specific use case?
[00:26:38.000 --> 00:26:39.000]   Are you kidding?
[00:26:39.000 --> 00:26:40.000]   What is the ethics?
[00:26:40.000 --> 00:26:41.000]   Are you kidding?
[00:26:41.000 --> 00:26:42.000]   I'm kidding.
[00:26:42.000 --> 00:26:43.000]   I'm kidding.
[00:26:43.000 --> 00:26:44.400]   You can make anybody say anything.
[00:26:44.400 --> 00:26:50.000]   If you can have John Legend read Sundar Pichai his daily schedule, you can have anybody say
[00:26:50.000 --> 00:26:51.640]   anything at any time.
[00:26:51.640 --> 00:26:54.400]   I mean, this is not what we're talking about.
[00:26:54.400 --> 00:26:56.240]   We're talking about the calling.
[00:26:56.240 --> 00:26:57.800]   This is what people are upset about.
[00:26:57.800 --> 00:27:02.200]   People are upset that they didn't know it was a bot calling the hair salon.
[00:27:02.200 --> 00:27:07.280]   Yes, because Google made every effort to make it sound as if it's a human.
[00:27:07.280 --> 00:27:08.280]   Of course.
[00:27:08.280 --> 00:27:09.280]   Yeah.
[00:27:09.280 --> 00:27:14.200]   Because this is a system that is supposed to overcome the issue of the businesses that
[00:27:14.200 --> 00:27:18.160]   haven't implemented an online booking system.
[00:27:18.160 --> 00:27:19.160]   So what's the thing?
[00:27:19.160 --> 00:27:22.880]   Well, it's a people are saying, well, should deception be your primary goal here deceiving
[00:27:22.880 --> 00:27:23.880]   the human?
[00:27:23.880 --> 00:27:24.880]   Yes.
[00:27:24.880 --> 00:27:25.880]   Why?
[00:27:25.880 --> 00:27:26.880]   Shouldn't it?
[00:27:26.880 --> 00:27:30.440]   Wouldn't it be wrong if it's sad, you know, hi, this is the Google Assistant.
[00:27:30.440 --> 00:27:32.760]   I'm calling to help make an appointment.
[00:27:32.760 --> 00:27:34.040]   Would you like to help me?
[00:27:34.040 --> 00:27:35.200]   It doesn't have to say this.
[00:27:35.200 --> 00:27:36.200]   Hi, this is.
[00:27:36.200 --> 00:27:38.480]   Hi, I'm calling for Sarah.
[00:27:38.480 --> 00:27:41.480]   I'd like to make an appointment for ladies haircut.
[00:27:41.480 --> 00:27:42.480]   Okay.
[00:27:42.480 --> 00:27:49.080]   What people are asking for is for this a polar robot, Kohler to say, hi, this is the Google
[00:27:49.080 --> 00:27:51.000]   Assistant and I'm calling for a client.
[00:27:51.000 --> 00:27:54.720]   He would like to make an appointment on Wednesday for a haircut.
[00:27:54.720 --> 00:27:55.720]   Is that okay?
[00:27:55.720 --> 00:28:01.880]   This is not to see people as your very first goal in artificial intelligence.
[00:28:01.880 --> 00:28:04.960]   I think that's the kind of thing that we get as geeks go.
[00:28:04.960 --> 00:28:05.960]   Isn't that cool?
[00:28:05.960 --> 00:28:12.080]   It sounded just like a human, but legitimately is concerning for most other people, right?
[00:28:12.080 --> 00:28:13.480]   This is proprietary.
[00:28:13.480 --> 00:28:15.320]   I might not want a bot calling me.
[00:28:15.320 --> 00:28:17.120]   I mean, that's my choice as a business owner.
[00:28:17.120 --> 00:28:21.440]   If I choose not to have an automated booking system, I might not be comfortable knowing
[00:28:21.440 --> 00:28:23.000]   that someone's going to be calling me.
[00:28:23.000 --> 00:28:24.000]   It's petty fraud.
[00:28:24.000 --> 00:28:25.000]   Or status.
[00:28:25.000 --> 00:28:26.000]   It's petty fraud.
[00:28:26.000 --> 00:28:27.240]   I mean, I acknowledge that.
[00:28:27.240 --> 00:28:30.760]   It's not, you know, I mean, it's not like, but it could be, I think it could be expanded
[00:28:30.760 --> 00:28:32.800]   to something less petty.
[00:28:32.800 --> 00:28:34.880]   The technology is concerning.
[00:28:34.880 --> 00:28:35.880]   Is it not?
[00:28:35.880 --> 00:28:41.320]   I mean, Patrick, I actually, I feel like I could argue both sides of this because for
[00:28:41.320 --> 00:28:46.480]   two days, all I was doing on my pod was talking about how great this was and describing the
[00:28:46.480 --> 00:28:47.480]   demo and reason.
[00:28:47.480 --> 00:28:48.480]   Me too.
[00:28:48.480 --> 00:28:49.480]   And that's why I feel guilty, Brian.
[00:28:49.480 --> 00:28:53.640]   And then, and then I start hearing these other things and it makes a lot of sense to me.
[00:28:53.640 --> 00:28:58.360]   And your answer, your response where you said, well, let's just do it because it's cool.
[00:28:58.360 --> 00:29:01.480]   Like, let's not, let's not cut it off before we find out what it is.
[00:29:01.480 --> 00:29:05.840]   But I think what Zanep and people like her specifically are saying is that we can't,
[00:29:05.840 --> 00:29:08.600]   you can't run things that way anymore.
[00:29:08.600 --> 00:29:12.360]   Like you do need, you can't just do a cool thing and release it.
[00:29:12.360 --> 00:29:17.200]   You should at least give some thought to the implications and the fact that it seems like
[00:29:17.200 --> 00:29:20.160]   Google thought this was just cool and it worked.
[00:29:20.160 --> 00:29:21.160]   It worked on me.
[00:29:21.160 --> 00:29:22.160]   It worked on Leo.
[00:29:22.160 --> 00:29:23.160]   Yeah.
[00:29:23.160 --> 00:29:25.840]   And then didn't do any further thinking beyond that.
[00:29:25.840 --> 00:29:29.520]   I think that that's what, when we talk about these ethics and things like that's what all
[00:29:29.520 --> 00:29:34.160]   people are asking for is do the next step thought, maybe one or two steps down the line.
[00:29:34.160 --> 00:29:38.520]   So just to clarify, I'm not saying there aren't ethics implications.
[00:29:38.520 --> 00:29:40.360]   That's not what I'm saying.
[00:29:40.360 --> 00:29:42.000]   But it's about the hyperbole.
[00:29:42.000 --> 00:29:46.160]   I think what people are asking for is for the Google Assistant to disclose the fact that
[00:29:46.160 --> 00:29:48.440]   it's the Google Assistant at the beginning of the call.
[00:29:48.440 --> 00:29:51.880]   That's my impression of what most people were asking for.
[00:29:51.880 --> 00:29:54.480]   And that doesn't appear to me like it's a huge deal.
[00:29:54.480 --> 00:29:58.760]   Like if the bot is calling you and it's saying, "Hi, this is the Google Assistant that I'm
[00:29:58.760 --> 00:30:03.400]   calling on behalf of my client," blah, blah, blah, and then you're fine with it, then it
[00:30:03.400 --> 00:30:05.120]   wasn't a huge deal to begin with.
[00:30:05.120 --> 00:30:09.560]   You know, it wasn't the world ending scandal that many people are making it into.
[00:30:09.560 --> 00:30:10.560]   Right?
[00:30:10.560 --> 00:30:13.640]   If you don't want these things to exist, it's a different deal.
[00:30:13.640 --> 00:30:17.360]   But if the disclosure in the beginning of the sentence is enough to put your mind at
[00:30:17.360 --> 00:30:21.840]   ease, then it's not worth flipping the whole table over, I think.
[00:30:21.840 --> 00:30:23.200]   It's worth talking about.
[00:30:23.200 --> 00:30:24.200]   Fair enough.
[00:30:24.200 --> 00:30:25.200]   But...
[00:30:25.200 --> 00:30:28.080]   And by doing it, of course, Google started the conversation, whether intentionally or
[00:30:28.080 --> 00:30:29.080]   not.
[00:30:29.080 --> 00:30:34.360]   And I think that it's an important conversation to have about the ethics and impact of this
[00:30:34.360 --> 00:30:38.600]   kind of thing.
[00:30:38.600 --> 00:30:39.600]   It's...
[00:30:39.600 --> 00:30:40.600]   Hey, it was impressive.
[00:30:40.600 --> 00:30:44.640]   And not like you, Brian, I feel a little guilty of being so impressed.
[00:30:44.640 --> 00:30:45.800]   I'm still impressed.
[00:30:45.800 --> 00:30:46.800]   Don't get me wrong.
[00:30:46.800 --> 00:30:48.040]   I am very impressed.
[00:30:48.040 --> 00:30:52.080]   I felt like, hey, we just saw something that you're going to look back and say, I saw
[00:30:52.080 --> 00:30:54.080]   the world change on that day.
[00:30:54.080 --> 00:30:56.520]   I think it's that important.
[00:30:56.520 --> 00:30:57.600]   And it's going to keep...
[00:30:57.600 --> 00:31:00.080]   I mean, it's going to happen as well.
[00:31:00.080 --> 00:31:01.080]   Yeah.
[00:31:01.080 --> 00:31:02.080]   Well, of course it is.
[00:31:02.080 --> 00:31:03.080]   I think we just...
[00:31:03.080 --> 00:31:04.080]   A little bit...
[00:31:04.080 --> 00:31:11.920]   There's a little bit of fear of something new that I think is not going to be as fearful,
[00:31:11.920 --> 00:31:15.240]   afraid, scary, is the word I'm looking for.
[00:31:15.240 --> 00:31:17.400]   Once it's a little bit more commonplace.
[00:31:17.400 --> 00:31:21.080]   I think though that there are valid reasons because as you said, this is going to happen
[00:31:21.080 --> 00:31:22.080]   regardless.
[00:31:22.080 --> 00:31:23.080]   We don't know when.
[00:31:23.080 --> 00:31:25.720]   I think that there are valid reasons to question how far along this really is.
[00:31:25.720 --> 00:31:29.760]   I'm a little bothered maybe with the rollout and the way this was kind of productized
[00:31:29.760 --> 00:31:34.080]   if these issues haven't been worked out or if Google is not willing to share their thoughts
[00:31:34.080 --> 00:31:35.080]   on them.
[00:31:35.080 --> 00:31:38.040]   I think that's a little problematic if you're going to sell this as a productized thing.
[00:31:38.040 --> 00:31:42.920]   And yet you don't have some of these very, I think, reasonable questions to have answered.
[00:31:42.920 --> 00:31:44.560]   But this is going to be your reality.
[00:31:44.560 --> 00:31:47.280]   And then eventually it's going to become more mainstreamed.
[00:31:47.280 --> 00:31:52.360]   You look at deep fakes and how real and how easy those tools are to use.
[00:31:52.360 --> 00:31:55.760]   You have to start thinking about, well, what about if this sort of thing starts being used
[00:31:55.760 --> 00:31:59.680]   in other contexts and people just put it up on GitHub or wherever and start sharing models
[00:31:59.680 --> 00:32:01.880]   and using these in more nefarious ways.
[00:32:01.880 --> 00:32:05.760]   And that's, I think, I'm not trying to be hyperbolic and I understand your point and
[00:32:05.760 --> 00:32:07.600]   not saying this is the end of the world.
[00:32:07.600 --> 00:32:11.240]   But I think the reason people become concerned about this is because we do know how fast
[00:32:11.240 --> 00:32:12.240]   this is moving.
[00:32:12.240 --> 00:32:14.440]   We do know that this is an eventuality.
[00:32:14.440 --> 00:32:18.920]   And if we don't start at least having the conversations now and putting basic practices
[00:32:18.920 --> 00:32:23.600]   into place, then it will be much easier for a deep fake type of situation to come up where
[00:32:23.600 --> 00:32:28.800]   people can have really not great scenarios where you feel like someone's calling you
[00:32:28.800 --> 00:32:34.280]   to do phishing scams or to get personal information from you that you're not aware of.
[00:32:34.280 --> 00:32:40.160]   Like I think that's also like a hypothetical of all of you and everybody listening.
[00:32:40.160 --> 00:32:41.520]   What would it, what would it be like?
[00:32:41.520 --> 00:32:48.720]   How will it feel in five years when you interact with things and you're just not sure if that's
[00:32:48.720 --> 00:32:53.200]   a human or a computer?
[00:32:53.200 --> 00:32:57.160]   Are we just going to be okay with that?
[00:32:57.160 --> 00:32:58.360]   We don't have a choice.
[00:32:58.360 --> 00:33:01.360]   Google is probably going to say, "Hi, I'm the Google Assistant."
[00:33:01.360 --> 00:33:04.360]   But there are many other actors that might not.
[00:33:04.360 --> 00:33:05.360]   And I think-
[00:33:05.360 --> 00:33:06.360]   Is that okay with you?
[00:33:06.360 --> 00:33:07.360]   The solution.
[00:33:07.360 --> 00:33:08.360]   No, of course it isn't.
[00:33:08.360 --> 00:33:09.360]   But it's the reality.
[00:33:09.360 --> 00:33:14.960]   The fake and the fact that we got used to the idea that Photoshop exists and any image
[00:33:14.960 --> 00:33:15.960]   could not be revealed.
[00:33:15.960 --> 00:33:21.040]   It's more pervasive than that though, that you could go through your day.
[00:33:21.040 --> 00:33:25.120]   I guess we already do that to some degree with email and stuff, but that you could go
[00:33:25.120 --> 00:33:29.320]   through your day, talk to people, interact with people that aren't people.
[00:33:29.320 --> 00:33:31.680]   And you may not know.
[00:33:31.680 --> 00:33:34.520]   There's something a little disconcerting about that, but maybe that's just something you
[00:33:34.520 --> 00:33:37.440]   will all get used to.
[00:33:37.440 --> 00:33:39.440]   You could say that-
[00:33:39.440 --> 00:33:43.880]   Right, but the point is that you could think through those things now.
[00:33:43.880 --> 00:33:49.320]   So if Leo's right that he doesn't want to live in that sort of a world, if you're Google
[00:33:49.320 --> 00:33:54.040]   and you're the closest to this technology, you could think one or two steps ahead and
[00:33:54.040 --> 00:33:59.080]   start to think of ways that would maybe ease Leo's mind a bit.
[00:33:59.080 --> 00:34:00.640]   So it's the not thinking about it.
[00:34:00.640 --> 00:34:05.440]   Like one of the reasons that this got picked up is because every single talk radio show
[00:34:05.440 --> 00:34:06.440]   in the country-
[00:34:06.440 --> 00:34:07.440]   Yeah, everybody played it.
[00:34:07.440 --> 00:34:08.440]   Talked about the next day.
[00:34:08.440 --> 00:34:09.440]   Everybody played it.
[00:34:09.440 --> 00:34:10.440]   Everybody played it.
[00:34:10.440 --> 00:34:11.640]   The next caller, will the next caller be real?
[00:34:11.640 --> 00:34:12.640]   How will we know?
[00:34:12.640 --> 00:34:17.280]   So if you think even for a second, well, you would know that that would be the thing
[00:34:17.280 --> 00:34:19.000]   that would be picked up on.
[00:34:19.000 --> 00:34:23.840]   So again, in not even just the ethical sense, but clearly they didn't think it through in
[00:34:23.840 --> 00:34:25.320]   the PR sense.
[00:34:25.320 --> 00:34:30.840]   And so all we're asking for is maybe think a little bit and then maybe put it in place
[00:34:30.840 --> 00:34:31.840]   to say-
[00:34:31.840 --> 00:34:32.840]   And do what?
[00:34:32.840 --> 00:34:33.840]   And do what?
[00:34:33.840 --> 00:34:37.440]   What regards that would ease the minds of people like Leo?
[00:34:37.440 --> 00:34:38.440]   Like make the voice-
[00:34:38.440 --> 00:34:39.440]   But don't use me as the example.
[00:34:39.440 --> 00:34:41.860]   How do you feel about it, Brian?
[00:34:41.860 --> 00:34:48.480]   I wouldn't mind that, your scenario of interacting on a daily basis.
[00:34:48.480 --> 00:34:49.480]   I wouldn't mind that.
[00:34:49.480 --> 00:34:56.960]   What would slowly disturb me is I wouldn't know when I was being manipulated.
[00:34:56.960 --> 00:34:57.960]   Yes.
[00:34:57.960 --> 00:35:03.260]   Because it's the same thing as using the Photoshop example using the Deep Fake example.
[00:35:03.260 --> 00:35:06.120]   There would be some invisible threshold that would be cried.
[00:35:06.120 --> 00:35:07.120]   I think it's-
[00:35:07.120 --> 00:35:10.120]   I think everybody will be-
[00:35:10.120 --> 00:35:11.360]   It's disorienting.
[00:35:11.360 --> 00:35:16.080]   It's the same problem we have now where we don't know what's true and what's not true,
[00:35:16.080 --> 00:35:17.880]   what's a fact, what's not a fact.
[00:35:17.880 --> 00:35:21.880]   It would be disorienting if you're interacting in the world.
[00:35:21.880 --> 00:35:28.320]   Our biological- Our biology is that we're interacting with other humans, but we can't
[00:35:28.320 --> 00:35:31.880]   tell it would be highly disorienting.
[00:35:31.880 --> 00:35:33.980]   Christina, do you think so?
[00:35:33.980 --> 00:35:34.980]   Yeah.
[00:35:34.980 --> 00:35:35.980]   No, I think it would be disorienting.
[00:35:35.980 --> 00:35:44.400]   I also, I'm not opposed to the idea of interacting with AIs, especially if they can do their,
[00:35:44.400 --> 00:35:46.140]   respond appropriately.
[00:35:46.140 --> 00:35:49.060]   The fear that I have, frankly, in some of this stuff, and we've already seen this with
[00:35:49.060 --> 00:35:52.260]   the first generation of assistants, is a lot of times they don't work the way that the
[00:35:52.260 --> 00:35:53.260]   demos show that they will.
[00:35:53.260 --> 00:35:58.140]   So instead, you have to have very specific inputs and outputs to get specific outputs.
[00:35:58.140 --> 00:36:02.740]   And so if these things can trick people maybe to hand over a credit card number or a social
[00:36:02.740 --> 00:36:04.780]   security number, that's really scary.
[00:36:04.780 --> 00:36:08.940]   But then if I'm actually just trying to have something book an appointment for me and it
[00:36:08.940 --> 00:36:14.700]   has to, the person on the other end has to say a certain set of keywords for it to trigger,
[00:36:14.700 --> 00:36:16.380]   then it's not even accomplishing anything.
[00:36:16.380 --> 00:36:20.420]   So my fear in some cases is that it might not work that well and then might work well
[00:36:20.420 --> 00:36:21.980]   enough to trick people.
[00:36:21.980 --> 00:36:28.100]   But for me, and maybe this is just unrealistic, I almost feel like if I knew I'm talking
[00:36:28.100 --> 00:36:30.500]   to an AI, I would feel better about it.
[00:36:30.500 --> 00:36:32.860]   It's not like I would necessarily have a problem interacting with it.
[00:36:32.860 --> 00:36:36.700]   I would just know kind of what to expect and maybe know how to augment my responses better
[00:36:36.700 --> 00:36:41.500]   if I don't get the expected response back.
[00:36:41.500 --> 00:36:44.700]   I don't know.
[00:36:44.700 --> 00:36:46.700]   It's a really good conversation.
[00:36:46.700 --> 00:36:50.380]   It's funny because for the longest time, you know, I saw her and I thought, oh yeah,
[00:36:50.380 --> 00:36:56.580]   I can't wait to have a assistant in my ear that sounds like Scarlett Johansson or Christina
[00:36:56.580 --> 00:36:57.580]   Warren.
[00:36:57.580 --> 00:36:59.340]   I'm talking and we're having a conversation.
[00:36:59.340 --> 00:37:00.740]   It's like a real person.
[00:37:00.740 --> 00:37:01.980]   I think that sounds really cool.
[00:37:01.980 --> 00:37:06.460]   But now as we get closer, I'm starting to think maybe it's not that cool.
[00:37:06.460 --> 00:37:07.460]   Maybe it's a little creepy.
[00:37:07.460 --> 00:37:08.460]   I don't know.
[00:37:08.460 --> 00:37:14.860]   I do think we don't know what's going to be cool about it and what's going to be creepy.
[00:37:14.860 --> 00:37:19.820]   You know, we were talking the very interesting point about chemists and physicists having
[00:37:19.820 --> 00:37:21.580]   their reckoning moment.
[00:37:21.580 --> 00:37:28.820]   They had the moment after they realized what the wrong use was.
[00:37:28.820 --> 00:37:36.420]   And I think to an extent with the fake news issues and with the very targeted campaigning
[00:37:36.420 --> 00:37:45.140]   that we've seen on social networks, now we understand how these things can be used badly.
[00:37:45.140 --> 00:37:46.460]   I'm not sure.
[00:37:46.460 --> 00:37:51.260]   You know, we look at this AI technology and conversational AI and we're thinking, oh,
[00:37:51.260 --> 00:37:53.380]   it could be used for phishing scams and stuff like that.
[00:37:53.380 --> 00:37:57.580]   But we're just applying the issues that we already know of and other content.
[00:37:57.580 --> 00:37:58.580]   We all know stuff.
[00:37:58.580 --> 00:38:02.540]   So yeah, I think it's going to be completely different.
[00:38:02.540 --> 00:38:05.980]   Let me play to regulate this yet.
[00:38:05.980 --> 00:38:12.300]   Either that or I'm really just trying to, you know, look really good to our AI overlords.
[00:38:12.300 --> 00:38:14.780]   They can look at this.
[00:38:14.780 --> 00:38:16.220]   Well, and that's just one of many things.
[00:38:16.220 --> 00:38:17.780]   Somebody in the chat was mentioning Chris Burry.
[00:38:17.780 --> 00:38:20.220]   We're going to have genetic modification in the very near future.
[00:38:20.220 --> 00:38:23.700]   And we're going to have all sorts of issues that we are very ill prepared for.
[00:38:23.700 --> 00:38:31.060]   And I like what you said about the reckoning didn't happen till it was too late.
[00:38:31.060 --> 00:38:34.620]   Here's Robert Oppenheimer, the creator of the atomic bomb.
[00:38:34.620 --> 00:38:40.860]   These are the words he spoke after he saw the first test at Alamogordo.
[00:38:40.860 --> 00:38:46.860]   In the world would not be the same.
[00:38:46.860 --> 00:38:51.060]   Few people laughed.
[00:38:51.060 --> 00:38:53.380]   Few people cried.
[00:38:53.380 --> 00:38:58.180]   Most people were silent.
[00:38:58.180 --> 00:39:06.780]   I remembered the line from the Hindu scripture, the Bhagavad Gita.
[00:39:06.780 --> 00:39:16.700]   Vishnu is trying to persuade the prince that he should do his duty.
[00:39:16.700 --> 00:39:27.540]   And to impress him takes on his multi armed form and says, now I am become deaf and destroyer
[00:39:27.540 --> 00:39:28.540]   of worlds.
[00:39:28.540 --> 00:39:35.260]   I suppose we'll all put that one way or another.
[00:39:35.260 --> 00:39:36.260]   That's the reckoning.
[00:39:36.260 --> 00:39:39.020]   And of course then it's too late, you know.
[00:39:39.020 --> 00:39:47.460]   Well, not to make light of any of this, but I'm going to get so much hate for this.
[00:39:47.460 --> 00:39:55.060]   Yes, there were a couple of really bad things that happened and it was unspeakably terrible.
[00:39:55.060 --> 00:40:01.700]   But overall, I think we would agree that the research on the atom was more beneficial than
[00:40:01.700 --> 00:40:08.660]   it was on beneficial and detrimental.
[00:40:08.660 --> 00:40:15.700]   And once the bad thing happened and we knew what it was, we took the steps to until now,
[00:40:15.700 --> 00:40:18.340]   at least maybe it's going to change in the next few years.
[00:40:18.340 --> 00:40:23.300]   But until now, we've managed it barely, but we've managed it.
[00:40:23.300 --> 00:40:28.660]   So you said earlier, it's inevitable and you're right.
[00:40:28.660 --> 00:40:34.540]   The idea of the technology fission to split the atom, once people figured out it was
[00:40:34.540 --> 00:40:37.220]   possible, it was inevitable that it would be done at some point, right?
[00:40:37.220 --> 00:40:41.100]   So I agree with you in the sense that there was probably no way to put a lot in place
[00:40:41.100 --> 00:40:44.780]   that say we will never create fission technology.
[00:40:44.780 --> 00:40:50.140]   But at the same time, that happened at an accident at a moment in history when we were
[00:40:50.140 --> 00:40:51.140]   at war.
[00:40:51.140 --> 00:40:55.980]   So immediately the government took control of atomic energy and things like that, where
[00:40:55.980 --> 00:41:00.180]   it remains today, locked down within government control and things like that.
[00:41:00.180 --> 00:41:02.300]   Which is probably a good thing.
[00:41:02.300 --> 00:41:03.300]   Right.
[00:41:03.300 --> 00:41:04.420]   So just things like that.
[00:41:04.420 --> 00:41:09.580]   Again, if we have another thing like that, will we have the same ability because we won't
[00:41:09.580 --> 00:41:12.580]   be a war or we might not want the government to do it or whatever?
[00:41:12.580 --> 00:41:19.500]   But not everybody can create a reactor and a shoebox in their bedroom for various reasons,
[00:41:19.500 --> 00:41:24.860]   but also because all the governments in the world immediately found ways to control it
[00:41:24.860 --> 00:41:26.620]   and make it manageable.
[00:41:26.620 --> 00:41:27.620]   Yeah.
[00:41:27.620 --> 00:41:29.140]   I mean, it's a very complex discussion.
[00:41:29.140 --> 00:41:33.780]   It's in my mind because I just came back from Hiroshima and Nagasaki.
[00:41:33.780 --> 00:41:38.940]   But it's extremely complex and I don't know what the right answer is and there's a lot
[00:41:38.940 --> 00:41:40.940]   of reasons why we had to do it first.
[00:41:40.940 --> 00:41:44.460]   If the Germans had done it first, that might not have been a very good thing.
[00:41:44.460 --> 00:41:47.460]   You could make the argument that the war would have gone on a lot longer had they not
[00:41:47.460 --> 00:42:01.940]   used it in the same time.
[00:42:01.940 --> 00:42:15.640]   I think I'll end it with this, that the reason this comes up is because we have seen with
[00:42:15.640 --> 00:42:20.860]   social media a technology gone wild that we really didn't anticipate the consequences
[00:42:20.860 --> 00:42:26.100]   of and maybe would have worked better had we thought it out a little bit more clearly
[00:42:26.100 --> 00:42:27.260]   ahead of time.
[00:42:27.260 --> 00:42:28.620]   I don't know.
[00:42:28.620 --> 00:42:30.020]   I don't know.
[00:42:30.020 --> 00:42:33.700]   I don't think we could have anticipated this.
[00:42:33.700 --> 00:42:37.380]   You know, the I'm not going to go into the political aspects.
[00:42:37.380 --> 00:42:38.380]   It's fascinating.
[00:42:38.380 --> 00:42:39.380]   Isn't it though?
[00:42:39.380 --> 00:42:40.380]   This is the human condition.
[00:42:40.380 --> 00:42:41.380]   It is.
[00:42:41.380 --> 00:42:42.380]   We come up with this stuff.
[00:42:42.380 --> 00:42:43.380]   It happens.
[00:42:43.380 --> 00:42:44.380]   And then we have to deal with it.
[00:42:44.380 --> 00:42:47.060]   I don't think there's ever been a case where we said, oh, let's not do let's not create
[00:42:47.060 --> 00:42:48.060]   that bomb.
[00:42:48.060 --> 00:42:49.060]   Let's not create that weapon.
[00:42:49.060 --> 00:42:50.060]   Let's not that never happened.
[00:42:50.060 --> 00:42:51.980]   And that's not going to happen here either.
[00:42:51.980 --> 00:42:52.980]   It's going to happen.
[00:42:52.980 --> 00:42:53.980]   It's just nice.
[00:42:53.980 --> 00:42:54.980]   And it's not just bombs and weapons.
[00:42:54.980 --> 00:42:57.540]   I think it's important to technology.
[00:42:57.540 --> 00:43:00.780]   Humans are technology creation machines.
[00:43:00.780 --> 00:43:05.260]   It's what we do.
[00:43:05.260 --> 00:43:07.060]   I think it's fascinating myself.
[00:43:07.060 --> 00:43:08.060]   Let's take a break.
[00:43:08.060 --> 00:43:09.060]   We got a lot more to talk about.
[00:43:09.060 --> 00:43:12.260]   I want to talk since you're here, Patrick and Patrick's in Finland, but obviously he's
[00:43:12.260 --> 00:43:20.700]   French and I would love to talk to a European about GDPR and its impact because Friday it
[00:43:20.700 --> 00:43:21.860]   all happens.
[00:43:21.860 --> 00:43:25.100]   Our show today though, brought to you by Zip Recruiter.
[00:43:25.100 --> 00:43:32.780]   If your hiring Zip Recruiter has completely changed the most important job in any company.
[00:43:32.780 --> 00:43:38.220]   If you were, let's say you're America's Cup skipper and you're trying to hire a crew,
[00:43:38.220 --> 00:43:40.540]   that crew is going to make or break you, right?
[00:43:40.540 --> 00:43:41.540]   You can have the technology.
[00:43:41.540 --> 00:43:42.540]   You can have the boat.
[00:43:42.540 --> 00:43:46.900]   You can have it all, but you want to get the best people.
[00:43:46.900 --> 00:43:52.540]   Any companies like that, you're creating an entity where everybody, if everybody works
[00:43:52.540 --> 00:43:57.420]   in the same direction and works, well, you're going to go to the moon.
[00:43:57.420 --> 00:44:00.060]   It's easy enough to get somebody who's going to bring you down.
[00:44:00.060 --> 00:44:04.780]   When you've got an opening, it's important to remember you're actually creating something
[00:44:04.780 --> 00:44:06.260]   really important.
[00:44:06.260 --> 00:44:09.260]   It's nice to have tools to make that possible.
[00:44:09.260 --> 00:44:14.300]   Zip Recruiter takes technology to hiring to really improve the process.
[00:44:14.300 --> 00:44:15.300]   In a couple of ways.
[00:44:15.300 --> 00:44:19.700]   First of all, it helps you reach that perfect person.
[00:44:19.700 --> 00:44:22.940]   Let's assume that a great employee is out there, but where are they?
[00:44:22.940 --> 00:44:26.140]   With Zip Recruiter, you post 100 sites with one click.
[00:44:26.140 --> 00:44:30.140]   You're reaching the maximum number of people, but it does even more because they already
[00:44:30.140 --> 00:44:34.380]   have tens of millions of resumes, current resumes on file.
[00:44:34.380 --> 00:44:38.860]   Zip Recruiter builds a platform that goes through them and finds the right candidates
[00:44:38.860 --> 00:44:43.780]   for you, identifies people with the right experience, and invites them to apply to your
[00:44:43.780 --> 00:44:44.780]   job.
[00:44:44.780 --> 00:44:49.620]   They become your ally in finding the perfect person.
[00:44:49.620 --> 00:44:51.980]   It's really revolutionized how people find their next hire.
[00:44:51.980 --> 00:44:57.780]   80% of employees who post a job on Zip Recruiter get a quality candidate through the site in
[00:44:57.780 --> 00:44:59.140]   one day.
[00:44:59.140 --> 00:45:06.340]   You know, when you're down a man or woman, filling that job becomes pretty important.
[00:45:06.340 --> 00:45:09.860]   Zip Recruiter, by the way, doesn't stop there because whenever you use Zip Recruiter, all
[00:45:09.860 --> 00:45:12.540]   of the applicants, they don't come to come to your phone or your inbox.
[00:45:12.540 --> 00:45:14.900]   They all flow into the Zip Recruiter interface.
[00:45:14.900 --> 00:45:18.380]   In that interface, they spotlight the strongest applicants.
[00:45:18.380 --> 00:45:20.260]   You're never going to miss a great match.
[00:45:20.260 --> 00:45:23.020]   Look, the right candidate is out there for that job.
[00:45:23.020 --> 00:45:25.060]   Zip Recruiter is how you find that person.
[00:45:25.060 --> 00:45:29.700]   Businesses of all sizes, including to it, use Zip Recruiter for their hiring needs.
[00:45:29.700 --> 00:45:33.260]   Right now, you can try Zip Recruiter free.
[00:45:33.260 --> 00:45:42.580]   Right now, free if you go to ziprecruiter.com/twit, ziprecruiter.com/twit, the smartest way to hire
[00:45:42.580 --> 00:45:49.780]   ziprecruiter.com/twit, we thank you for their support of the Twit podcast.
[00:45:49.780 --> 00:45:55.700]   So I read a great article which made me feel a lot better about GDPR.
[00:45:55.700 --> 00:45:57.700]   I have such mixed feelings about GDPR.
[00:45:57.700 --> 00:46:04.740]   On the one hand, this general data protection regulation that the EU put into law two years
[00:46:04.740 --> 00:46:09.820]   ago but now is actually going to go into effect on Friday does things that everybody
[00:46:09.820 --> 00:46:12.500]   would agree are good.
[00:46:12.500 --> 00:46:18.260]   For instance, requires companies that have a data breach to disclose within 72 hours.
[00:46:18.260 --> 00:46:19.340]   I mean, really fast.
[00:46:19.340 --> 00:46:22.500]   And also, put some real teeth into those laws.
[00:46:22.500 --> 00:46:26.860]   These regulations have existed but there was no enforcement in the past.
[00:46:26.860 --> 00:46:27.860]   Now, there will be.
[00:46:27.860 --> 00:46:33.980]   In fact, this is a really good example of what happens if you make a law or you make
[00:46:33.980 --> 00:46:34.980]   a regulation.
[00:46:34.980 --> 00:46:38.660]   What was the, there was a privacy, what was the name of the privacy regulation, Patrick,
[00:46:38.660 --> 00:46:42.540]   it's been an effect for a year or two.
[00:46:42.540 --> 00:46:45.260]   I'm not sure we can't remember the name of it.
[00:46:45.260 --> 00:46:47.740]   I mean, there are things like the right to be forgotten.
[00:46:47.740 --> 00:46:48.740]   No, not that.
[00:46:48.740 --> 00:46:55.980]   But there's a general privacy regulation that effectively was GDPR but had no teeth and
[00:46:55.980 --> 00:46:57.740]   has been in place for a while.
[00:46:57.740 --> 00:46:59.300]   Oh, look it up.
[00:46:59.300 --> 00:47:03.460]   In any event that EU decided, well, we've got to have some, you know, if this is going
[00:47:03.460 --> 00:47:06.820]   to work, we've got to have a significant penalty.
[00:47:06.820 --> 00:47:15.340]   And the penalty is fairly significant as much as 20 million euros or 40% of your global
[00:47:15.340 --> 00:47:19.500]   revenue, whichever is higher, that's enough even to scare a big company like Google or
[00:47:19.500 --> 00:47:26.740]   Facebook, clearly it's intended for them.
[00:47:26.740 --> 00:47:30.940]   But so some things I think I'm in favor of, then there are other things that I think a
[00:47:30.940 --> 00:47:35.980]   lot of people are very worried about that compliance will be difficult.
[00:47:35.980 --> 00:47:37.180]   I read a good article.
[00:47:37.180 --> 00:47:40.980]   This is Jacques Matei, I think you pronounce it.
[00:47:40.980 --> 00:47:48.380]   He wrote a piece on his blog called GDPR hysteria, which calmed me down a lot.
[00:47:48.380 --> 00:47:51.460]   He said, you know, it's in here, it's in here, Leo.
[00:47:51.460 --> 00:47:53.780]   It's the European data privacy directive.
[00:47:53.780 --> 00:47:54.780]   That's it.
[00:47:54.780 --> 00:47:55.780]   That's it.
[00:47:55.780 --> 00:48:02.900]   Which, which was kind of a first attempt but didn't do anything because it had no teeth.
[00:48:02.900 --> 00:48:06.500]   So he's pointing out, look, this isn't going to happen overnight.
[00:48:06.500 --> 00:48:11.060]   You're going to get, if you're out of compliance, you're going to get a warning.
[00:48:11.060 --> 00:48:12.860]   That's the maximum fine.
[00:48:12.860 --> 00:48:16.260]   There may not be a fine at all if you fix it.
[00:48:16.260 --> 00:48:19.940]   I am in favor of privacy.
[00:48:19.940 --> 00:48:23.860]   But I think also here in the States, a lot of people say, how dare the EU tell us how
[00:48:23.860 --> 00:48:25.700]   to run our business?
[00:48:25.700 --> 00:48:28.420]   Well, they're not.
[00:48:28.420 --> 00:48:33.100]   They're telling you that that's what you have to do if you have customers in the EU.
[00:48:33.100 --> 00:48:38.740]   They could perfectly make two different sets of rules and settings.
[00:48:38.740 --> 00:48:41.980]   One for the EU customers and one for the US customers.
[00:48:41.980 --> 00:48:48.180]   Now, of course, it would get interesting reactions from the US customers and the press, I'm
[00:48:48.180 --> 00:48:53.580]   guessing, because the EU customers would have settings and rights to delete their data,
[00:48:53.580 --> 00:48:59.420]   portability of their data, downloading what you have used on their network, and magically
[00:48:59.420 --> 00:49:01.140]   the US customers wouldn't.
[00:49:01.140 --> 00:49:03.980]   So actually for a while we thought Facebook might do that.
[00:49:03.980 --> 00:49:06.780]   They might implement it just for EU customers.
[00:49:06.780 --> 00:49:10.020]   They said then later, no, we're going to implement it for everybody.
[00:49:10.020 --> 00:49:10.860]   In fact, it's great.
[00:49:10.860 --> 00:49:12.380]   You can download your Facebook data now.
[00:49:12.380 --> 00:49:14.820]   You can delete your Facebook data now if you want.
[00:49:14.820 --> 00:49:17.020]   That's a good thing.
[00:49:17.020 --> 00:49:19.100]   It's the power of government.
[00:49:19.100 --> 00:49:25.100]   Yes, government is sometimes overbearing and cumbersome, but sometimes you need it because
[00:49:25.100 --> 00:49:32.140]   it's the only entity that can have some kind of influence on other more very potent entities
[00:49:32.140 --> 00:49:34.660]   like some huge companies.
[00:49:34.660 --> 00:49:41.260]   I do think that Europeans have always been more concerned with all of these issues than
[00:49:41.260 --> 00:49:43.100]   the Americans for a couple of reasons.
[00:49:43.100 --> 00:49:48.420]   Historically, we have more genuine concern about all of this and the way our data is
[00:49:48.420 --> 00:49:49.420]   used.
[00:49:49.420 --> 00:49:56.300]   Of course, those big companies are usually not in the EU, so we have an easier time looking
[00:49:56.300 --> 00:50:03.660]   at them and saying, "Oh, that's one of the subtexts of all of this is, "Oh, here comes
[00:50:03.660 --> 00:50:07.980]   the EU after successful American companies again."
[00:50:07.980 --> 00:50:13.060]   No one is saying that what is being required of those companies is completely unreasonable,
[00:50:13.060 --> 00:50:16.180]   especially for the big ones.
[00:50:16.180 --> 00:50:21.020]   These things that really they should have implemented themselves a long time ago.
[00:50:21.020 --> 00:50:27.980]   I went off on a tiny, tiny rant on Twitter because after the 25th email I received from
[00:50:27.980 --> 00:50:31.580]   one of those companies saying, "That's company X.
[00:50:31.580 --> 00:50:36.980]   We value your privacy and this is why we're going to depend on it."
[00:50:36.980 --> 00:50:41.860]   If you valued our privacy, you wouldn't have waited until the law obliged you to do these
[00:50:41.860 --> 00:50:44.940]   things and put them into place.
[00:50:44.940 --> 00:50:51.260]   I would have preferred that some of them would say, "Hey, so the GDPR is coming into law and
[00:50:51.260 --> 00:50:55.060]   it requires this and this or so we're making these modifications."
[00:50:55.060 --> 00:51:01.620]   You didn't have to put that hypocritical PR spin to it saying, "Well, of course they
[00:51:01.620 --> 00:51:07.140]   did have to do it because that's how they work, but it's a little bit frustrating."
[00:51:07.140 --> 00:51:13.060]   I understand the concerns and again, I think it's good to ask and discuss them.
[00:51:13.060 --> 00:51:25.260]   But overall, I have a hard time justifying a full criticism of this GDPR ensemble of requirements.
[00:51:25.260 --> 00:51:32.580]   It seems like things that should have been put into place maybe earlier.
[00:51:32.580 --> 00:51:36.940]   I have to admit that myself, I might have been a little bit skeptical about all of it
[00:51:36.940 --> 00:51:41.700]   until a couple of years ago, but I think now it's clear to everyone in Europe that it's
[00:51:41.700 --> 00:51:45.300]   needed and I'm guessing in the US, most people agree.
[00:51:45.300 --> 00:51:49.340]   I agree that it's needed and I think that you make, I basically agree with everything
[00:51:49.340 --> 00:51:50.340]   you said.
[00:51:50.340 --> 00:51:54.060]   I do have some questions still about how this is actually going to be enforced.
[00:51:54.060 --> 00:51:59.700]   I think if you're a larger company, a Facebook or Google, a Microsoft and Apple, a Samsung
[00:51:59.700 --> 00:52:04.220]   and Amazon, you obviously are going to be in compliance because you're going to be watched
[00:52:04.220 --> 00:52:05.220]   very carefully.
[00:52:05.220 --> 00:52:12.140]   If you are a Chinese based company or if you are Eastern European based country who's
[00:52:12.140 --> 00:52:19.500]   not part of the EU or if you are just smaller and don't care, I am curious about how these
[00:52:19.500 --> 00:52:21.380]   things are actually going to be enforced.
[00:52:21.380 --> 00:52:25.820]   I think actually that's why Leo mentioned this piece specifically because I read this
[00:52:25.820 --> 00:52:27.300]   earlier in the week also, Leo.
[00:52:27.300 --> 00:52:31.620]   The point that he's making is that he's specifically saying, "Are you just a person that has a
[00:52:31.620 --> 00:52:35.540]   blog? Are you going to be on the hook for $20 million or whatever it finds?"
[00:52:35.540 --> 00:52:38.660]   And he's saying, "No, that's not the way that the EU works."
[00:52:38.660 --> 00:52:45.140]   Things like the DPD, the European Data Protection Directive, he said, "That's been in effect
[00:52:45.140 --> 00:52:46.140]   for two decades."
[00:52:46.140 --> 00:52:47.140]   Two decades.
[00:52:47.140 --> 00:52:52.260]   Have you never even heard of it because the point is not that they wanted to suddenly
[00:52:52.260 --> 00:52:53.260]   find everyone.
[00:52:53.260 --> 00:52:54.260]   But I'm not talking about it.
[00:52:54.260 --> 00:52:56.260]   They wanted to have the laws in the place.
[00:52:56.260 --> 00:52:57.260]   Right.
[00:52:57.260 --> 00:52:58.260]   Right.
[00:52:58.260 --> 00:53:00.620]   And I understand that what I'm saying is I'm not necessarily talking about your website
[00:53:00.620 --> 00:53:07.100]   or a small company who I'm talking about, potentially big conglomerates, big industries,
[00:53:07.100 --> 00:53:10.140]   big players who are based in countries that just aren't going to care.
[00:53:10.140 --> 00:53:14.900]   How are they going to enforce this against someone like Tencent, for instance, or someone
[00:53:14.900 --> 00:53:15.900]   else?
[00:53:15.900 --> 00:53:18.900]   Like, you know, where someone's really going to come back, that's becomes the interesting
[00:53:18.900 --> 00:53:19.900]   question.
[00:53:19.900 --> 00:53:23.500]   Are you going to start blocking access to their traffic?
[00:53:23.500 --> 00:53:25.540]   What, how much teeth is this really going to have?
[00:53:25.540 --> 00:53:29.780]   And I guess we will have to see on that because what we've seen before with Right to be forgotten
[00:53:29.780 --> 00:53:35.580]   and other sanctions that have gone against Google and other companies, those companies
[00:53:35.580 --> 00:53:37.540]   tend to fight back even when it is the law.
[00:53:37.540 --> 00:53:39.300]   So I don't know.
[00:53:39.300 --> 00:53:42.980]   Yes, but the right to be forgotten has been implemented.
[00:53:42.980 --> 00:53:47.700]   Initially I thought it was a horrible idea, but it has been implemented by Google and
[00:53:47.700 --> 00:53:50.020]   it seems to work relatively well.
[00:53:50.020 --> 00:53:52.220]   Of course, there are always issues in every.
[00:53:52.220 --> 00:53:59.580]   Oh, I still think that one's a horrible law because it's said of saying pull down the
[00:53:59.580 --> 00:54:01.260]   original content.
[00:54:01.260 --> 00:54:05.660]   They're putting the burden on the search engine to pull down the search results.
[00:54:05.660 --> 00:54:09.100]   They're also putting Google, Google in the position of judge and jury.
[00:54:09.100 --> 00:54:13.300]   Google has to go through each of these requests and judge whether it has merit or not.
[00:54:13.300 --> 00:54:15.740]   That's not something Google should be doing.
[00:54:15.740 --> 00:54:20.900]   Yes, but Leo, this is the practical approach to problems.
[00:54:20.900 --> 00:54:24.260]   Sometimes the solution is not perfect, but it's better than doing nothing.
[00:54:24.260 --> 00:54:29.340]   This is what we're asking of YouTube, of Facebook, of Twitter.
[00:54:29.340 --> 00:54:33.660]   We're telling them, take away the objectionable content within 24 hours.
[00:54:33.660 --> 00:54:34.660]   This is horrible.
[00:54:34.660 --> 00:54:36.100]   It shouldn't be on the internet.
[00:54:36.100 --> 00:54:42.660]   My initial concern was we're asking private enterprises to be the judge of what is acceptable
[00:54:42.660 --> 00:54:43.940]   or not acceptable to say.
[00:54:43.940 --> 00:54:44.940]   I agree.
[00:54:44.940 --> 00:54:46.180]   This is a valid concern.
[00:54:46.180 --> 00:54:52.620]   However, on the other side of it, and I'm sure some people would have more to say about
[00:54:52.620 --> 00:54:57.700]   it than me, but you see some horrible things on Twitter.
[00:54:57.700 --> 00:55:02.380]   Sometimes you are very frustrated that it's still there and that the accounts aren't blocked
[00:55:02.380 --> 00:55:06.940]   and the leads are the right thing to do to go to Google and say, "Hi, that Twitter search
[00:55:06.940 --> 00:55:08.500]   result or to go to Twitter?"
[00:55:08.500 --> 00:55:10.380]   They pull the tweet down.
[00:55:10.380 --> 00:55:12.140]   That's the problem I have with Right to Be Forgotten.
[00:55:12.140 --> 00:55:17.060]   It makes Google responsible for this, not the site that hosts the bad content.
[00:55:17.060 --> 00:55:20.860]   I think in the case of Twitter, obviously you would go to Twitter.
[00:55:20.860 --> 00:55:21.860]   Yes.
[00:55:21.860 --> 00:55:24.700]   The Right to Be Forgotten is about Google.
[00:55:24.700 --> 00:55:28.540]   Yes, absolutely.
[00:55:28.540 --> 00:55:31.100]   It's not about the link to Twitter.
[00:55:31.100 --> 00:55:36.580]   If it's a link to Twitter, you're going to go see Twitter.
[00:55:36.580 --> 00:55:41.180]   Because in practice, Google is our doorway to the internet, especially in Europe.
[00:55:41.180 --> 00:55:43.020]   Remember, they have 80%...
[00:55:43.020 --> 00:55:44.020]   Yes, dominant.
[00:55:44.020 --> 00:55:45.820]   Yeah, market share in Europe.
[00:55:45.820 --> 00:55:49.140]   Andy in Germany is in the chat room and he says, "You still have to delete the content
[00:55:49.140 --> 00:55:50.740]   from the original sites by yourself.
[00:55:50.740 --> 00:55:55.500]   You go to those sites, but the reason Google is in the equation is because they cache all
[00:55:55.500 --> 00:55:58.940]   this stuff, so you have to get them to kill the cache as well, I guess."
[00:55:58.940 --> 00:55:59.940]   I don't know.
[00:55:59.940 --> 00:56:01.500]   Well, no, no, no, not just the cache.
[00:56:01.500 --> 00:56:02.740]   They also have to kill the link.
[00:56:02.740 --> 00:56:03.740]   They have to buy the link.
[00:56:03.740 --> 00:56:04.740]   Right.
[00:56:04.740 --> 00:56:05.740]   Yeah.
[00:56:05.740 --> 00:56:08.020]   But if the content's gone, what does it link matter?
[00:56:08.020 --> 00:56:09.020]   Of course, yes.
[00:56:09.020 --> 00:56:12.300]   But usually the issue is that the content is more difficult to get to the internet.
[00:56:12.300 --> 00:56:15.260]   What about the internet archive?
[00:56:15.260 --> 00:56:21.380]   Which I think is a really valuable, important thing, the way back machine is saving the
[00:56:21.380 --> 00:56:22.380]   internet.
[00:56:22.380 --> 00:56:23.380]   Nobody's saving the internet.
[00:56:23.380 --> 00:56:24.380]   It's disappearing.
[00:56:24.380 --> 00:56:25.380]   Absolutely.
[00:56:25.380 --> 00:56:26.660]   Leo, I completely agree.
[00:56:26.660 --> 00:56:31.980]   But I will answer to you what I basically would like to formulate to Christina about the concerns
[00:56:31.980 --> 00:56:40.700]   that he was voicing, which is you're always going to have issues in any system, in any
[00:56:40.700 --> 00:56:42.300]   rule, in any law.
[00:56:42.300 --> 00:56:48.220]   You can point out instances where it doesn't work or where it breaks something.
[00:56:48.220 --> 00:56:54.860]   But having a few exceptions where it doesn't work out well doesn't mean that you shouldn't
[00:56:54.860 --> 00:56:56.220]   do something about the problem.
[00:56:56.220 --> 00:56:57.220]   I agree.
[00:56:57.220 --> 00:56:58.220]   Without a doubt, I agree.
[00:56:58.220 --> 00:57:01.980]   And to be clear, although I actually have a lot of issues, especially from a first minute
[00:57:01.980 --> 00:57:06.340]   of perspective with the right to be forgotten, I have serious, serious issues about that,
[00:57:06.340 --> 00:57:07.980]   I don't have a problem with GDPR.
[00:57:07.980 --> 00:57:10.620]   And I think that these are things that I'm with.
[00:57:10.620 --> 00:57:11.620]   They're very different.
[00:57:11.620 --> 00:57:12.620]   Yeah.
[00:57:12.620 --> 00:57:13.620]   Absolutely.
[00:57:13.620 --> 00:57:15.460]   And I think these are things that, frankly, most of these big companies should have been
[00:57:15.460 --> 00:57:16.460]   doing already.
[00:57:16.460 --> 00:57:19.660]   And that act that they were actively avoiding.
[00:57:19.660 --> 00:57:22.860]   And I think that the fact that we've had two years and there was all this pushback is
[00:57:22.860 --> 00:57:23.860]   proof.
[00:57:23.860 --> 00:57:30.020]   Facebook being publicly forced through the terrible press of is getting to not have
[00:57:30.020 --> 00:57:33.380]   two option screens, but one is kind of proof of that.
[00:57:33.380 --> 00:57:34.380]   I agree with you.
[00:57:34.380 --> 00:57:36.900]   I'm just saying, not to say not to do it.
[00:57:36.900 --> 00:57:38.140]   Obviously, you still do it.
[00:57:38.140 --> 00:57:44.620]   But I do have questions that other than some of the biggest American run conglomerates
[00:57:44.620 --> 00:57:48.860]   that you might still have some very big services that will just not respond to this.
[00:57:48.860 --> 00:57:51.660]   And I think that that's just something that people should reconcile.
[00:57:51.660 --> 00:57:56.780]   Sure, so the thing is, if one of those big companies, let's say a Chinese company for
[00:57:56.780 --> 00:58:01.420]   the sake of argument, if they do business in Europe, if they are big enough or serious
[00:58:01.420 --> 00:58:06.980]   enough, at this point, they still do have to have some kind of presence in the country.
[00:58:06.980 --> 00:58:11.940]   Basically, you need to have a company that is created in Europe to do business with European
[00:58:11.940 --> 00:58:13.940]   customers.
[00:58:13.940 --> 00:58:16.380]   You might have employees.
[00:58:16.380 --> 00:58:17.900]   You have someone who's responsible.
[00:58:17.900 --> 00:58:23.220]   So if they're big enough, the likeliness is that if there's a big problem, there is
[00:58:23.220 --> 00:58:27.220]   someone you can point to go after a company, you can find stuff like that.
[00:58:27.220 --> 00:58:30.380]   Yes, there might be a few instances where this doesn't happen.
[00:58:30.380 --> 00:58:31.380]   But it's...
[00:58:31.380 --> 00:58:33.260]   Actually, that's an interesting question.
[00:58:33.260 --> 00:58:38.740]   The interesting question though, what if you say, "No, I'm not going to pay the fine?"
[00:58:38.740 --> 00:58:39.740]   What do they do?
[00:58:39.740 --> 00:58:41.820]   Are they going to block you on the Internet?
[00:58:41.820 --> 00:58:44.940]   No, if you have a French...
[00:58:44.940 --> 00:58:45.940]   This is the reason why...
[00:58:45.940 --> 00:58:46.940]   What are they going to do though?
[00:58:46.940 --> 00:58:47.940]   Close your office?
[00:58:47.940 --> 00:58:48.940]   What are they going to do?
[00:58:48.940 --> 00:58:50.380]   Yes, absolutely.
[00:58:50.380 --> 00:58:53.260]   If you don't pay the fine, then you're liable to criminal...
[00:58:53.260 --> 00:58:54.260]   I'm guessing.
[00:58:54.260 --> 00:58:55.260]   I'm not a lawyer.
[00:58:55.260 --> 00:58:57.260]   So let's say it's not going to happen.
[00:58:57.260 --> 00:58:59.500]   But let's say they said, "When after me?"
[00:58:59.500 --> 00:59:04.940]   And said, "You're not a racing IP addresses of people who download your content."
[00:59:04.940 --> 00:59:07.700]   Well, again, I said, "Well, screw you.
[00:59:07.700 --> 00:59:09.500]   What are they going to do?"
[00:59:09.500 --> 00:59:10.980]   You don't have a company in France.
[00:59:10.980 --> 00:59:11.980]   Google does.
[00:59:11.980 --> 00:59:12.980]   Facebook does.
[00:59:12.980 --> 00:59:19.700]   But everybody who has viewers or listeners or readers or customers in France are still
[00:59:19.700 --> 00:59:22.820]   liable to the EU for those customers, right?
[00:59:22.820 --> 00:59:23.820]   Yes.
[00:59:23.820 --> 00:59:24.820]   Yes.
[00:59:24.820 --> 00:59:27.180]   And in theory, they could go after you.
[00:59:27.180 --> 00:59:28.180]   In theory, yes, Leo.
[00:59:28.180 --> 00:59:29.180]   Yes.
[00:59:29.180 --> 00:59:30.180]   They're not going to, obviously.
[00:59:30.180 --> 00:59:31.180]   Yes, exactly.
[00:59:31.180 --> 00:59:36.380]   But, yes, I mean, Google is going to pay or at least they've appealed it, but Google's
[00:59:36.380 --> 00:59:38.060]   being fined what was it?
[00:59:38.060 --> 00:59:39.580]   $1.7 billion or something.
[00:59:39.580 --> 00:59:41.300]   They'll eventually pay that.
[00:59:41.300 --> 00:59:42.300]   Maybe they will.
[00:59:42.300 --> 00:59:43.300]   Maybe they will get a knockdown.
[00:59:43.300 --> 00:59:44.300]   Right.
[00:59:44.300 --> 00:59:45.300]   Because this is my question.
[00:59:45.300 --> 00:59:46.300]   This is what I'm saying.
[00:59:46.300 --> 00:59:48.860]   Speaking of Google specifically, they've been fined multiple times by the EU.
[00:59:48.860 --> 00:59:54.620]   To my knowledge, they've paid much smaller fines than what's been issued to the new
[00:59:54.620 --> 00:59:55.620]   poor.
[00:59:55.620 --> 00:59:57.020]   $2.7 billion is the current one.
[00:59:57.020 --> 01:00:01.380]   Yeah, but there was a Google shopping thing that I think this is the shopping one.
[01:00:01.380 --> 01:00:02.380]   That's the shopping one.
[01:00:02.380 --> 01:00:04.060]   Okay, there was another one too that they put off.
[01:00:04.060 --> 01:00:06.180]   I mean, they've been putting this off for years.
[01:00:06.180 --> 01:00:09.380]   So part of me does kind of wonder, like, when are they actually going?
[01:00:09.380 --> 01:00:11.660]   I mean, I'm not opposed to this law at all.
[01:00:11.660 --> 01:00:13.460]   As I said, I agree with it.
[01:00:13.460 --> 01:00:16.740]   Part of me though does wonder like how much of this is bluster and how much of this is
[01:00:16.740 --> 01:00:19.340]   just going to carry on in the courts ad nauseam.
[01:00:19.340 --> 01:00:23.980]   Listen, how many emails have you gotten of becoming right implemented?
[01:00:23.980 --> 01:00:24.980]   They're taking it seriously.
[01:00:24.980 --> 01:00:27.660]   As they should, because it's the right thing to do.
[01:00:27.660 --> 01:00:29.940]   Well, I agree with my first join Microsoft.
[01:00:29.940 --> 01:00:31.420]   I joined Microsoft a year ago.
[01:00:31.420 --> 01:00:35.980]   I was they were already deep in the GDPR rollout plan.
[01:00:35.980 --> 01:00:37.900]   It was already deeply underway.
[01:00:37.900 --> 01:00:40.620]   And I'm sure that that's the same way that it's been at every other major company.
[01:00:40.620 --> 01:00:43.860]   This is not something that two months ago people were like, oh, we've got to get on this.
[01:00:43.860 --> 01:00:47.220]   This has been probably in the works for two years at most of these places.
[01:00:47.220 --> 01:00:48.220]   I agree.
[01:00:48.220 --> 01:00:52.380]   I'm just saying, you know, if people find that there isn't enough being done or Google
[01:00:52.380 --> 01:00:57.660]   were to or another company were to do something that the people claimed violated this.
[01:00:57.660 --> 01:01:01.580]   I have no doubt in my mind that there would be an appeal as, you know, for Bernie is sort
[01:01:01.580 --> 01:01:02.980]   of fine.
[01:01:02.980 --> 01:01:05.740]   And actually, I mean, I think this is a good thing.
[01:01:05.740 --> 01:01:10.140]   I'm just pointing out, I think that sometimes we say, oh, it's fixed now.
[01:01:10.140 --> 01:01:16.380]   And that's not necessarily going to be the case because companies will look for ways
[01:01:16.380 --> 01:01:18.020]   to still capture what they can.
[01:01:18.020 --> 01:01:20.780]   I do have a question because I'm not clear on this.
[01:01:20.780 --> 01:01:28.020]   If you ask users to opt into your agreement that says, I will willingly give you this
[01:01:28.020 --> 01:01:31.780]   information, can you bypass some of those restrictions?
[01:01:31.780 --> 01:01:36.620]   If you are able to get them to agree to a different user agreement that says, it's fine,
[01:01:36.620 --> 01:01:38.300]   you can capture my information.
[01:01:38.300 --> 01:01:40.420]   Well, I think that's the point.
[01:01:40.420 --> 01:01:45.520]   I think you have to explicitly have them agree to giving you this information instead of
[01:01:45.520 --> 01:01:46.520]   implicitly doing that.
[01:01:46.520 --> 01:01:49.780]   And I guess the right to delete it.
[01:01:49.780 --> 01:01:52.980]   They can come to you and say, I'll delete my information.
[01:01:52.980 --> 01:01:54.980]   And that is the other point I wanted to make.
[01:01:54.980 --> 01:01:57.540]   We can argue about whether or not they would comply or not.
[01:01:57.540 --> 01:01:58.980]   The fact is they are complying.
[01:01:58.980 --> 01:02:03.020]   You now have the ability to delete all of your information from all of these networks.
[01:02:03.020 --> 01:02:06.780]   Of course, most people are choosing not to do that and are probably going to keep choosing
[01:02:06.780 --> 01:02:08.080]   not to do that.
[01:02:08.080 --> 01:02:14.380]   But if you want to talk about how much teeth that law has and how much weight, you know,
[01:02:14.380 --> 01:02:16.340]   ultimately, it's all about money.
[01:02:16.340 --> 01:02:21.580]   All these companies are making a lot of money in EU, in the EU, and they're willing to invest
[01:02:21.580 --> 01:02:26.740]   a little bit of money in modifying their systems, first of all, because it's ultimately
[01:02:26.740 --> 01:02:27.740]   the right thing to do.
[01:02:27.740 --> 01:02:31.060]   I think a lot of them are saying now, well, maybe we should be a little bit regulated
[01:02:31.060 --> 01:02:33.300]   if it's the right way.
[01:02:33.300 --> 01:02:37.380]   And second of all, it's a small investment compared to the money they're making in the
[01:02:37.380 --> 01:02:38.620]   territory.
[01:02:38.620 --> 01:02:44.140]   And yes, they could close down their offices in the EU and have everyone go through Amazon
[01:02:44.140 --> 01:02:47.500]   dot com or whatever, you know, US side it is.
[01:02:47.500 --> 01:02:50.500]   But in practice, that's just not the way it works.
[01:02:50.500 --> 01:02:54.500]   If you want to do business, even in the age of the internet, you need to have a presence
[01:02:54.500 --> 01:02:57.700]   in the country or at least in the EU.
[01:02:57.700 --> 01:03:00.260]   So it's currently working.
[01:03:00.260 --> 01:03:05.980]   Now of course, if they end up having a fine of 20 million euros or however much it might
[01:03:05.980 --> 01:03:09.180]   be 4%, they will fight it.
[01:03:09.180 --> 01:03:10.500]   It will take years.
[01:03:10.500 --> 01:03:11.820]   But that's not even the point.
[01:03:11.820 --> 01:03:18.180]   The point is the deterring effect is working because they're already doing it.
[01:03:18.180 --> 01:03:22.860]   So Doc Serals, who I love and has been on the show, wrote an article.
[01:03:22.860 --> 01:03:24.940]   I thought was very interesting.
[01:03:24.940 --> 01:03:29.980]   He says one of the side effects of GDPR is it's going to pop the ad tech bubble.
[01:03:29.980 --> 01:03:35.500]   Remember that there are a lot of companies, Facebook and Google certainly, that make money
[01:03:35.500 --> 01:03:40.060]   by basically collecting information about you and selling it to advertisers.
[01:03:40.060 --> 01:03:44.820]   He says the main problem is tracking people without their knowledge approval or a court
[01:03:44.820 --> 01:03:47.380]   order is just flat out wrong.
[01:03:47.380 --> 01:03:52.140]   The fact that it can be done is no excuse, nor is the monstrous sum of money made by
[01:03:52.140 --> 01:03:53.220]   it.
[01:03:53.220 --> 01:03:58.620]   He says without ad tech, that's those, you know, that's basically what this is, is ad
[01:03:58.620 --> 01:04:02.420]   tech, the technology's ability to track you and then sell ads against you.
[01:04:02.420 --> 01:04:06.020]   The EU's GDPR would never have happened, but the GDPR did happen.
[01:04:06.020 --> 01:04:10.460]   And as a result, websites all over the world are suddenly posting notices about their
[01:04:10.460 --> 01:04:17.140]   changed privacy policies, use of cookies and opt-in choices and email lists are doing
[01:04:17.140 --> 01:04:18.140]   it.
[01:04:18.140 --> 01:04:20.140]   And he says, it's a good thing.
[01:04:20.140 --> 01:04:23.340]   Well, Leo, you know, this has already happened with cloud, right?
[01:04:23.340 --> 01:04:25.340]   Yeah, cloud went out of business.
[01:04:25.340 --> 01:04:26.340]   Yeah.
[01:04:26.340 --> 01:04:27.340]   Wait a minute.
[01:04:27.340 --> 01:04:31.020]   It's going to be how difficult for us to monetize this thing.
[01:04:31.020 --> 01:04:32.420]   Did they admit that?
[01:04:32.420 --> 01:04:36.660]   Because their last day is May 25th, which is completely coincidentally that they, the
[01:04:36.660 --> 01:04:37.660]   GDPR goes into favor.
[01:04:37.660 --> 01:04:38.660]   I mean, right.
[01:04:38.660 --> 01:04:42.900]   No, see, that's the point is that so the whoever bought cloud and whoever owns cloud
[01:04:42.900 --> 01:04:46.300]   now said that it just wasn't a no, it wasn't a strategic fit anymore.
[01:04:46.300 --> 01:04:50.220]   But everyone's even doing the math and be like, Oh, it's funny that that's happening
[01:04:50.220 --> 01:04:52.380]   on the day that this new law is coming into effect.
[01:04:52.380 --> 01:04:57.020]   Well, but the other, the other example is unroll me, which instead of going out of business,
[01:04:57.020 --> 01:05:00.060]   just said, well, you know, if you're in the EU, you can't use our service.
[01:05:00.060 --> 01:05:02.660]   We need to be collecting all that data.
[01:05:02.660 --> 01:05:04.660]   Well, I was going to say this.
[01:05:04.660 --> 01:05:05.860]   I think, are you?
[01:05:05.860 --> 01:05:06.860]   Well, stop.
[01:05:06.860 --> 01:05:08.980]   I, yeah, well, I, I'm going to have to.
[01:05:08.980 --> 01:05:11.980]   You have to stop because you recommended it a few years ago.
[01:05:11.980 --> 01:05:12.980]   I know.
[01:05:12.980 --> 01:05:15.940]   And I regret it because of course they got caught because really it turned out that unroll
[01:05:15.940 --> 01:05:20.100]   me, which the idea was you unsubscribe in newsletters for you and they'll digest them
[01:05:20.100 --> 01:05:25.500]   was selling information among others to Uber about you after going through all your email.
[01:05:25.500 --> 01:05:26.860]   Go ahead, Christina.
[01:05:26.860 --> 01:05:27.860]   I'm sorry.
[01:05:27.860 --> 01:05:28.860]   No, no, I agree.
[01:05:28.860 --> 01:05:31.740]   I was saying those, this is what's kind of interesting is the only thing I would push
[01:05:31.740 --> 01:05:34.740]   back on on Docs here is a little bit because I do agree that I think this will definitely
[01:05:34.740 --> 01:05:40.340]   impact especially ad tech firms that have like a European targeting base, but I would
[01:05:40.340 --> 01:05:43.980]   not be surprised if some of them or plenty of them are like, okay, well, we're just not
[01:05:43.980 --> 01:05:50.780]   going to target these markets because we don't have to and we can still capture a lot
[01:05:50.780 --> 01:06:01.180]   of information from, you know, Asian and, and US and, you know, South American web visitors.
[01:06:01.180 --> 01:06:02.180]   Yeah.
[01:06:02.180 --> 01:06:05.900]   Conversely, I've been using this month as an opportunity.
[01:06:05.900 --> 01:06:09.900]   All these emails when they come in and I think to myself, I haven't used that service
[01:06:09.900 --> 01:06:15.100]   in 10 years and they say, if you don't log in and, and opt in, we're going to have to
[01:06:15.100 --> 01:06:16.700]   delete all your, and I'm like, well, great.
[01:06:16.700 --> 01:06:17.700]   Yeah.
[01:06:17.700 --> 01:06:19.780]   This is a great opportunity to prune things back.
[01:06:19.780 --> 01:06:26.020]   Doc has a really great doc, by the way, the author of the Clute Train Manifesto, one of
[01:06:26.020 --> 01:06:29.380]   the authors of the Clute Train Manifesto is not against advertising.
[01:06:29.380 --> 01:06:31.060]   He, he actually likes advertising.
[01:06:31.060 --> 01:06:35.260]   What he doesn't like is ad tech and he has all the reasons why ad tech should go away.
[01:06:35.260 --> 01:06:39.420]   It's built to underman that mind the brand value of all the media it uses.
[01:06:39.420 --> 01:06:40.420]   This is interesting.
[01:06:40.420 --> 01:06:45.620]   In other words, it's, it undermines the content because it cares more about eyeballs than
[01:06:45.620 --> 01:06:50.300]   the content itself and it causes negative associations with brands.
[01:06:50.300 --> 01:06:54.380]   He says, not one brand known to the world has been made by ad tech.
[01:06:54.380 --> 01:06:55.580]   Ad tech wants to be personal.
[01:06:55.580 --> 01:07:00.900]   That's why it's tracking based ad tech spies on people and violates their privacy is full
[01:07:00.900 --> 01:07:01.900]   of fraud.
[01:07:01.900 --> 01:07:03.540]   It's a vector for malware.
[01:07:03.540 --> 01:07:09.340]   It incentivizes publications to prioritize content generation over journalism.
[01:07:09.340 --> 01:07:10.340]   Link Bay.
[01:07:10.340 --> 01:07:13.820]   Intermediators take most of what's spent anyway of ad tech.
[01:07:13.820 --> 01:07:17.940]   Ad tech gives fake news, a business model, right?
[01:07:17.940 --> 01:07:20.060]   There would be no fake news if a word for ad tech.
[01:07:20.060 --> 01:07:23.340]   But wasn't, wasn't, wasn't Google basically made by double click?
[01:07:23.340 --> 01:07:24.340]   Yeah.
[01:07:24.340 --> 01:07:27.260]   I mean, if we're honest, Google's revenue comes from ad tech.
[01:07:27.260 --> 01:07:28.260]   Right.
[01:07:28.260 --> 01:07:29.260]   I was mostly people.
[01:07:29.260 --> 01:07:30.740]   These techs are going to come from ad tech.
[01:07:30.740 --> 01:07:35.100]   And, and, and, and, right, and Google's known to us, I mean, they started this assertion
[01:07:35.100 --> 01:07:39.260]   and it was good, but it really didn't start to become everywhere until you had ads and
[01:07:39.260 --> 01:07:40.740]   that and all that stuff, which.
[01:07:40.740 --> 01:07:42.460]   Yeah, but Christina, sorry, sorry.
[01:07:42.460 --> 01:07:44.340]   I got to do the history thing.
[01:07:44.340 --> 01:07:45.340]   Good.
[01:07:45.340 --> 01:07:46.340]   Internet history is here.
[01:07:46.340 --> 01:07:47.340]   Yeah.
[01:07:47.340 --> 01:07:51.260]   Ad scents and ad words were 2001, 2002.
[01:07:51.260 --> 01:07:54.940]   So they don't buy double click till 2006, I think.
[01:07:54.940 --> 01:08:00.740]   So actually in, in a way, they don't get the sort of ad tech that Leo is talking about
[01:08:00.740 --> 01:08:04.260]   until they've already IPO'd until they're already this huge company.
[01:08:04.260 --> 01:08:05.260]   Okay.
[01:08:05.260 --> 01:08:09.860]   And that was sort of, that was sort of the thing that one of the reasons a couple years
[01:08:09.860 --> 01:08:14.260]   after the IPO, that then the stock starts to rise again, is because Wall Street was
[01:08:14.260 --> 01:08:15.260]   waiting for them.
[01:08:15.260 --> 01:08:21.580]   If you think about it as, as, as genius as the auction model of, of AdWords is, it's
[01:08:21.580 --> 01:08:26.540]   not very technical on the side of buying ads and placing ads.
[01:08:26.540 --> 01:08:28.820]   Because you're just picking keywords and things like that.
[01:08:28.820 --> 01:08:33.380]   So Wall Street wanted them desperately to do this personalized targeting and things
[01:08:33.380 --> 01:08:34.380]   like that.
[01:08:34.380 --> 01:08:38.620]   And, and it's the acquisition of AdWords, or not AdWords of double click that, that brought
[01:08:38.620 --> 01:08:39.780]   that into their stable.
[01:08:39.780 --> 01:08:40.780]   So also Doc points out that-
[01:08:40.780 --> 01:08:42.820]   Thank you for correcting me because you're right.
[01:08:42.820 --> 01:08:44.660]   I, I, I didn't put the timeline together.
[01:08:44.660 --> 01:08:46.020]   I thought double hook was earlier, but you're right.
[01:08:46.020 --> 01:08:47.020]   So thank you.
[01:08:47.020 --> 01:08:51.700]   Also, Doc points out that thanks to ad tech, it's the largest boycott in human history.
[01:08:51.700 --> 01:08:57.260]   1.7 billion people use ad blockers, just to block ad tech, right?
[01:08:57.260 --> 01:09:03.540]   I think there's a little bit of wishful thinking in all of this, you know, the, the cookies,
[01:09:03.540 --> 01:09:08.020]   not agreement, but you know, you have to be informed of the fact that people just, I
[01:09:08.020 --> 01:09:09.580]   just go, yeah, yeah, yeah.
[01:09:09.580 --> 01:09:16.300]   It's the, it's, it's the, the saddest thing that these companies didn't do it before,
[01:09:16.300 --> 01:09:18.780]   because I don't think it's going to change much.
[01:09:18.780 --> 01:09:23.180]   Most people, even, you know, myself, I looked at it and I figured, you know, they're going
[01:09:23.180 --> 01:09:25.940]   to be displaying ads anyway.
[01:09:25.940 --> 01:09:31.700]   Do I want them to be relevant to me or do I want them to be irrelevant to me?
[01:09:31.700 --> 01:09:32.700]   Well, there's one thing-
[01:09:32.700 --> 01:09:33.700]   There's one thing-
[01:09:33.700 --> 01:09:34.700]   I live in public.
[01:09:34.700 --> 01:09:39.540]   So one point that Leo just made from reading off the, the Searles thing is, you know, what
[01:09:39.540 --> 01:09:43.500]   when we're talking, if people aren't familiar with this, the simplest example is what are
[01:09:43.500 --> 01:09:49.060]   we talking about when you're talking about ad tech is you go, you, you research hotels
[01:09:49.060 --> 01:09:50.940]   in Macon, Georgia.
[01:09:50.940 --> 01:09:55.300]   And then the next thing you know for the next month, everywhere you go, you're seeing ads
[01:09:55.300 --> 01:10:00.500]   for hotels in Macon, Georgia or the thing you looked up on Amazon is suddenly you're
[01:10:00.500 --> 01:10:01.820]   seeing it on Facebook.
[01:10:01.820 --> 01:10:08.380]   It's when, when Searles says that it devalues the content, that's what we mean because what
[01:10:08.380 --> 01:10:11.940]   the advertiser cares about is they were able to track that you came and did a search once
[01:10:11.940 --> 01:10:13.620]   or you came to their website once.
[01:10:13.620 --> 01:10:16.780]   And so they don't actually care as opposed to in the old days where if I want to sell
[01:10:16.780 --> 01:10:22.140]   a car, then I would go to a website that has car content and buy ads.
[01:10:22.140 --> 01:10:23.140]   They don't care about that.
[01:10:23.140 --> 01:10:24.140]   But now it's in the eyeball.
[01:10:24.140 --> 01:10:25.140]   Right.
[01:10:25.140 --> 01:10:28.060]   Wherever you go, they follow you.
[01:10:28.060 --> 01:10:32.660]   So he, he points out, I think this is very interesting that last week Google alerted advertisers
[01:10:32.660 --> 01:10:38.260]   it would sharply limit the use of the double click advertising ID that Google is already
[01:10:38.260 --> 01:10:43.380]   changing how it works because of GDPR.
[01:10:43.380 --> 01:10:48.060]   So will it pop the bubble or will it make each ad placement cost a little bit more as
[01:10:48.060 --> 01:10:53.940]   it should and reward the content and what he's right.
[01:10:53.940 --> 01:10:57.220]   What he's right about is a lot of, especially the last decade.
[01:10:57.220 --> 01:11:01.380]   And I can even say this is someone that's bought a lot of ads as an advertiser is a lot
[01:11:01.380 --> 01:11:06.820]   of the bubble has been specifically that it's called retargeting where we can follow you
[01:11:06.820 --> 01:11:08.260]   around.
[01:11:08.260 --> 01:11:14.420]   But it is that and then Facebook came around at the right time when the ability to slice
[01:11:14.420 --> 01:11:19.140]   and dice like we were talking about off air before we went on air.
[01:11:19.140 --> 01:11:23.580]   Like that has been what has caused the explosion.
[01:11:23.580 --> 01:11:28.020]   So if there's a bubble, like obviously we all know that this is it's a it's a once in
[01:11:28.020 --> 01:11:31.580]   the generation thing they're standing in the tidal wave of history of all advertising
[01:11:31.580 --> 01:11:32.580]   is moving online.
[01:11:32.580 --> 01:11:33.580]   We know that.
[01:11:33.580 --> 01:11:39.740]   However, it got like steroids injected into it in the last decade by this sort of tech.
[01:11:39.740 --> 01:11:46.020]   He does say Google will be all right because their chief revenue comes from AdWords, the
[01:11:46.020 --> 01:11:49.820]   search advertising, which does doesn't need to collect information about you.
[01:11:49.820 --> 01:11:51.540]   It's based on what you search for.
[01:11:51.540 --> 01:11:53.420]   You're giving it a signal of the exchanges.
[01:11:53.420 --> 01:11:59.060]   I mean, even if they didn't Google owns all the sub exchanges and all the various, you
[01:11:59.060 --> 01:12:02.340]   know, networks on networks on networks are all based on Google and Facebook.
[01:12:02.340 --> 01:12:05.580]   So even if they're not collecting, even if they're not doing this retargeting, they're
[01:12:05.580 --> 01:12:09.700]   still the place, they're still the main network.
[01:12:09.700 --> 01:12:10.940]   Like there's not a competitor.
[01:12:10.940 --> 01:12:11.940]   Yeah.
[01:12:11.940 --> 01:12:14.820]   By the way, he's not against advertising.
[01:12:14.820 --> 01:12:15.820]   In fact, he says that.
[01:12:15.820 --> 01:12:16.820]   Oh, no, I know.
[01:12:16.820 --> 01:12:18.420]   I had text what's bad about advertising.
[01:12:18.420 --> 01:12:20.660]   You know, advertising itself is good.
[01:12:20.660 --> 01:12:24.220]   He says, compared to advertising, ad tech is ugly.
[01:12:24.220 --> 01:12:26.580]   Ad tech relies on misdirection.
[01:12:26.580 --> 01:12:33.420]   It's I mean, and I agree with him because we do ads, but we don't do ads based on what
[01:12:33.420 --> 01:12:35.300]   information we collect about you.
[01:12:35.300 --> 01:12:38.900]   We do ads based on the content and we know, you know, that our advertisers want to reach
[01:12:38.900 --> 01:12:43.460]   tech enthusiasts and that works really well for them and everybody's happy, right?
[01:12:43.460 --> 01:12:50.620]   It's not a, you know, it's there's a big difference in surveilling you and targeting you via the
[01:12:50.620 --> 01:12:51.620]   content.
[01:12:51.620 --> 01:12:53.100]   All right, I'm going to take a break and do an ad.
[01:12:53.100 --> 01:12:54.100]   How about that?
[01:12:54.100 --> 01:12:55.100]   Good timing.
[01:12:55.100 --> 01:12:56.100]   How about that?
[01:12:56.100 --> 01:13:01.820]   Hey, before I, before I do, I want to just ask Brian a little bit about the, the new show
[01:13:01.820 --> 01:13:04.460]   because this is great.
[01:13:04.460 --> 01:13:08.660]   Tech meme launched a daily news podcast on the tech meme front page.
[01:13:08.660 --> 01:13:12.380]   Everybody here reads tech meme religiously daily and you're right there.
[01:13:12.380 --> 01:13:14.020]   Tell me how that happened.
[01:13:14.020 --> 01:13:17.820]   It gave, it had been thinking about doing it for a while.
[01:13:17.820 --> 01:13:20.300]   The neighbor Vera, the creator of tech memes.
[01:13:20.300 --> 01:13:25.420]   And I've been doing podcasting for a while and we've been friends for many years.
[01:13:25.420 --> 01:13:28.900]   And the thing is, is everybody has a tech podcast.
[01:13:28.900 --> 01:13:33.580]   So how are we going to, if we did do a tech meme podcast, how can we possibly compete
[01:13:33.580 --> 01:13:37.540]   with the likes of Leo and a bunch of other great people?
[01:13:37.540 --> 01:13:41.700]   And so we thought that what tech meme does well, what's the use case for it that everyone
[01:13:41.700 --> 01:13:42.700]   likes?
[01:13:42.700 --> 01:13:43.700]   What did I miss?
[01:13:43.700 --> 01:13:44.700]   I'll go check tech meme.
[01:13:44.700 --> 01:13:46.540]   I was in a meeting for an hour.
[01:13:46.540 --> 01:13:47.540]   What did I miss?
[01:13:47.540 --> 01:13:52.780]   So the tech meme right home posts every day, every weekday at 5pm.
[01:13:52.780 --> 01:13:54.140]   It's about 15 minutes long.
[01:13:54.140 --> 01:13:57.340]   And that's the, that's the idea like on your way home.
[01:13:57.340 --> 01:13:58.340]   Guess what?
[01:13:58.340 --> 01:14:00.060]   Here's what you missed today in the world of tech.
[01:14:00.060 --> 01:14:05.780]   And so we, we think that that's a, it's just extending what tech meme does well into
[01:14:05.780 --> 01:14:06.780]   podcast form.
[01:14:06.780 --> 01:14:07.780]   Nice.
[01:14:07.780 --> 01:14:08.780]   Congratulations.
[01:14:08.780 --> 01:14:09.780]   They picked a great host.
[01:14:09.780 --> 01:14:10.780]   Thank you so much.
[01:14:10.780 --> 01:14:11.780]   Yeah.
[01:14:11.780 --> 01:14:16.180]   And I was just saying during the last break to Carson, I like this Brian guy.
[01:14:16.180 --> 01:14:17.820]   You should have a back.
[01:14:17.820 --> 01:14:20.500]   I hope you'll, I hope you'll come by on a regular basis.
[01:14:20.500 --> 01:14:22.300]   Well, first time, long time as they say.
[01:14:22.300 --> 01:14:24.580]   First time, long time listening to the show for years.
[01:14:24.580 --> 01:14:25.580]   Yeah.
[01:14:25.580 --> 01:14:30.220]   And of course, Patrick Bezia, we always love having you on from techspin.com.
[01:14:30.220 --> 01:14:33.300]   Tell me the shows you're doing these days.
[01:14:33.300 --> 01:14:36.420]   Frenchspin.com always have to have the French.
[01:14:36.420 --> 01:14:37.580]   Oh, I'm sorry.
[01:14:37.580 --> 01:14:38.580]   French spin.
[01:14:38.580 --> 01:14:41.060]   But yeah, do you have an English language still?
[01:14:41.060 --> 01:14:42.980]   Do the Phileas club is that still around?
[01:14:42.980 --> 01:14:43.980]   Yeah.
[01:14:43.980 --> 01:14:44.980]   Yeah.
[01:14:44.980 --> 01:14:45.980]   Yeah.
[01:14:45.980 --> 01:14:48.740]   In English, I do Le Honde voutet in French.
[01:14:48.740 --> 01:14:52.220]   I do Le Honde vousge in French as well.
[01:14:52.220 --> 01:14:54.060]   You're French is excellent, by the way.
[01:14:54.060 --> 01:14:55.860]   I just want to congratulate you.
[01:14:55.860 --> 01:14:56.860]   Thank you.
[01:14:56.860 --> 01:14:58.860]   I really sounds good.
[01:14:58.860 --> 01:14:59.860]   For many years.
[01:14:59.860 --> 01:15:00.860]   Yeah.
[01:15:00.860 --> 01:15:05.620]   I also do a pixels in English about gaming as well.
[01:15:05.620 --> 01:15:06.620]   Oh, you said it.
[01:15:06.620 --> 01:15:07.620]   He's coming up.
[01:15:07.620 --> 01:15:08.620]   That's going to be fun.
[01:15:08.620 --> 01:15:09.620]   Oh, that's great.
[01:15:09.620 --> 01:15:14.500]   Basically, when I left Blizzard a few years ago, I wanted to talk about gaming in English
[01:15:14.500 --> 01:15:18.740]   and French and I launched those two gaming shows.
[01:15:18.740 --> 01:15:23.300]   And yeah, and the Phileas club, which I invite a few people from different countries, different
[01:15:23.300 --> 01:15:24.660]   places in the world.
[01:15:24.660 --> 01:15:30.460]   And we talk about what's been happening in the world with different angles and interpretations
[01:15:30.460 --> 01:15:31.460]   from different cultures.
[01:15:31.460 --> 01:15:37.380]   We did a few months ago, a couple of months ago, an episode about gun control.
[01:15:37.380 --> 01:15:38.380]   That was specifically interesting.
[01:15:38.380 --> 01:15:39.380]   Oh, interesting.
[01:15:39.380 --> 01:15:42.780]   With American guests.
[01:15:42.780 --> 01:15:49.540]   And it was really hard because we did it from the point of view of gun owners and gun friendly
[01:15:49.540 --> 01:15:50.540]   people.
[01:15:50.540 --> 01:15:57.940]   And as we always do, we try to do it in a friendly and not yelly.
[01:15:57.940 --> 01:16:00.980]   I don't know how to describe this.
[01:16:00.980 --> 01:16:02.500]   Civilized debate.
[01:16:02.500 --> 01:16:09.380]   And I think we were all surprised at how rational it all ended being.
[01:16:09.380 --> 01:16:10.380]   But anyway.
[01:16:10.380 --> 01:16:16.020]   The Phileas club is one of the interesting ones about difficult topics, but trying to
[01:16:16.020 --> 01:16:19.140]   have a rational conversation about them.
[01:16:19.140 --> 01:16:21.980]   What game are you playing these days?
[01:16:21.980 --> 01:16:23.420]   I'm not playing Fortnite.
[01:16:23.420 --> 01:16:24.420]   I'm apparently trying to.
[01:16:24.420 --> 01:16:25.660]   Why not?
[01:16:25.660 --> 01:16:27.740]   That's the greatest game ever written.
[01:16:27.740 --> 01:16:29.500]   Yeah, it's for the kids.
[01:16:29.500 --> 01:16:30.500]   The kids are playing.
[01:16:30.500 --> 01:16:32.900]   Oh, I just finished out of war.
[01:16:32.900 --> 01:16:34.540]   God of war looks really good.
[01:16:34.540 --> 01:16:35.540]   Yeah.
[01:16:35.540 --> 01:16:36.540]   It's pretty good.
[01:16:36.540 --> 01:16:38.580]   I got a sea of thieves and I thought, this is terrible.
[01:16:38.580 --> 01:16:42.020]   This is somebody said, no man see.
[01:16:42.020 --> 01:16:47.220]   Basically, it's the criticism being leveled at it by many, many people.
[01:16:47.220 --> 01:16:51.780]   But then I hear that no man's sky is going to start doing online multiplayer.
[01:16:51.780 --> 01:16:57.100]   Yeah, they just announced they're coming to Xbox after the two years of it's young
[01:16:57.100 --> 01:16:58.340]   PS4.
[01:16:58.340 --> 01:17:01.540]   And they finally are bringing multiplayer.
[01:17:01.540 --> 01:17:04.100]   You might actually run into somebody in the universe.
[01:17:04.100 --> 01:17:05.100]   Incredible.
[01:17:05.100 --> 01:17:06.900]   I wandered for days.
[01:17:06.900 --> 01:17:10.900]   I couldn't figure out what is the point of this game.
[01:17:10.900 --> 01:17:12.660]   I'm not meeting anybody.
[01:17:12.660 --> 01:17:14.740]   I'm not doing anything.
[01:17:14.740 --> 01:17:15.740]   Okay.
[01:17:15.740 --> 01:17:22.740]   That's why I like Fortnite because at least I can kill somebody and build and build it
[01:17:22.740 --> 01:17:26.940]   before I like the forts.
[01:17:26.940 --> 01:17:28.660]   Do you play it on the switch Nintendo switch?
[01:17:28.660 --> 01:17:29.660]   You play any games there?
[01:17:29.660 --> 01:17:31.020]   Oh, I love my switch.
[01:17:31.020 --> 01:17:32.020]   I love my switch.
[01:17:32.020 --> 01:17:33.460]   I have every game that comes out.
[01:17:33.460 --> 01:17:36.260]   I wanted to come out on the switch because then you can play it.
[01:17:36.260 --> 01:17:38.180]   It's horrible on the go at home.
[01:17:38.180 --> 01:17:40.620]   I noticed our 15 year old plays on the big screen too though.
[01:17:40.620 --> 01:17:41.620]   He docks it a lot.
[01:17:41.620 --> 01:17:42.620]   Yeah.
[01:17:42.620 --> 01:17:43.620]   Yeah.
[01:17:43.620 --> 01:17:47.380]   And as the friends over, he's bought more controllers now.
[01:17:47.380 --> 01:17:49.900]   And you're a switch user right Christina?
[01:17:49.900 --> 01:17:50.900]   I am.
[01:17:50.900 --> 01:17:53.260]   I'm a switch user and I love it.
[01:17:53.260 --> 01:17:54.260]   I'm like your son.
[01:17:54.260 --> 01:17:55.740]   I was a 15 year old.
[01:17:55.740 --> 01:17:57.100]   I dock it and I play it.
[01:17:57.100 --> 01:17:58.260]   You are a 15 year old boy.
[01:17:58.260 --> 01:17:59.740]   I know that actually about you.
[01:17:59.740 --> 01:18:00.740]   This is true.
[01:18:00.740 --> 01:18:01.740]   This is true.
[01:18:01.740 --> 01:18:02.740]   All right.
[01:18:02.740 --> 01:18:04.900]   You're not still playing Zelda though.
[01:18:04.900 --> 01:18:06.220]   No, no, no, no.
[01:18:06.220 --> 01:18:08.300]   Although there's the new one which I haven't.
[01:18:08.300 --> 01:18:10.260]   I don't know if I'm going to get that one or not.
[01:18:10.260 --> 01:18:12.140]   But I like a lot of the indie games.
[01:18:12.140 --> 01:18:14.940]   And I'm actually I'm playing Fortnite a little bit.
[01:18:14.940 --> 01:18:16.820]   Not as much as everyone else.
[01:18:16.820 --> 01:18:17.820]   It's hard.
[01:18:17.820 --> 01:18:19.460]   It is, but I like it.
[01:18:19.460 --> 01:18:20.460]   It's fun.
[01:18:20.460 --> 01:18:21.460]   You know why I like it?
[01:18:21.460 --> 01:18:22.460]   Because I get killed right away.
[01:18:22.460 --> 01:18:24.860]   But then I can watch the guy who killed me get killed.
[01:18:24.860 --> 01:18:26.980]   And then I can watch the guy who killed him get killed.
[01:18:26.980 --> 01:18:27.980]   And it's fun.
[01:18:27.980 --> 01:18:29.740]   And you get to the really, you get to the end and you see the best player.
[01:18:29.740 --> 01:18:33.420]   And it's kind of actually it's actually for me it's a spectator game.
[01:18:33.420 --> 01:18:34.420]   I like to watch.
[01:18:34.420 --> 01:18:35.420]   Oh no.
[01:18:35.420 --> 01:18:36.420]   Oh no, totally.
[01:18:36.420 --> 01:18:39.540]   Like I think the biggest appeal of Fortnite for a lot of people.
[01:18:39.540 --> 01:18:42.020]   And I'm definitely counting myself on this is the spectator aspect.
[01:18:42.020 --> 01:18:45.060]   Whether it's in game or if it's on Twitch or on YouTube or whatever.
[01:18:45.060 --> 01:18:48.620]   I mean, there's just so many amazing players and you see so many cool things.
[01:18:48.620 --> 01:18:51.660]   And then you know, the mod community is so insane too.
[01:18:51.660 --> 01:18:55.540]   So yeah, it'll be interesting to see, you know, obviously there's there's the iOS version
[01:18:55.540 --> 01:18:56.540]   which is pretty good.
[01:18:56.540 --> 01:18:59.300]   But it'll be interesting to see what happens when the Android version launches.
[01:18:59.300 --> 01:19:01.420]   Yes, they said they're coming to Android.
[01:19:01.420 --> 01:19:02.420]   Yeah.
[01:19:02.420 --> 01:19:04.980]   This game has been so insane.
[01:19:04.980 --> 01:19:09.700]   I mean, last year, all we could talk about was a player known as Battleground.
[01:19:09.700 --> 01:19:16.100]   And when when Fortnite pivoted from their co-op campaign to Battle Royale, everyone was thinking,
[01:19:16.100 --> 01:19:17.940]   Oh, you know, they will.
[01:19:17.940 --> 01:19:18.940]   They're cute.
[01:19:18.940 --> 01:19:21.220]   But PUBG is the big dog here.
[01:19:21.220 --> 01:19:23.980]   They were free to play on every platform.
[01:19:23.980 --> 01:19:29.740]   And I'm telling you, every person I know that interacts with middle schoolers is telling
[01:19:29.740 --> 01:19:32.540]   me this is all they can talk about.
[01:19:32.540 --> 01:19:34.420]   Fortnite is their life.
[01:19:34.420 --> 01:19:38.420]   But 10 to 13 year old, they live for Fortnite.
[01:19:38.420 --> 01:19:42.820]   Not just PUBG, but Overwatch probably also hurt pretty badly by Fortnite, right?
[01:19:42.820 --> 01:19:47.540]   Well, it's not as so disclaimer, as I said, I used to work for Blizzard, but it's not
[01:19:47.540 --> 01:19:51.820]   as much in direct competition with Fortnite.
[01:19:51.820 --> 01:19:55.740]   But certainly now the Battle Royale genre is king.
[01:19:55.740 --> 01:20:02.140]   But I mean, to be fair, PUBG is not a free game and Fortnite is, but they're still making
[01:20:02.140 --> 01:20:03.140]   money handed with this.
[01:20:03.140 --> 01:20:04.140]   Both.
[01:20:04.140 --> 01:20:07.140]   And I find that I'm much more interested in Fortnite partly because it's cartoony.
[01:20:07.140 --> 01:20:08.140]   I kind of like it.
[01:20:08.140 --> 01:20:09.140]   It's more fun.
[01:20:09.140 --> 01:20:10.140]   It's more fun.
[01:20:10.140 --> 01:20:13.060]   Honestly, I think that's kind of the same thing as why the Switch has been so popular.
[01:20:13.060 --> 01:20:18.260]   There's a certain, I think we almost reached this place with the PS4 Pro and the Xbox One
[01:20:18.260 --> 01:20:24.820]   X and where the realism is great, but it's kind of fun to have a game look like a game.
[01:20:24.820 --> 01:20:25.820]   It's so funny.
[01:20:25.820 --> 01:20:29.940]   So Michael, our 15 year old is playing something, I don't know what it is on the Switch that
[01:20:29.940 --> 01:20:32.940]   looks as bad as my Atari 2600 practically.
[01:20:32.940 --> 01:20:36.660]   It's a bit, and I said, Michael, you like these games?
[01:20:36.660 --> 01:20:37.900]   Yeah, I love these games.
[01:20:37.900 --> 01:20:42.260]   He loves the eight bit, you know, it's like black and red.
[01:20:42.260 --> 01:20:44.300]   It's not even a colorful.
[01:20:44.300 --> 01:20:45.300]   And he loves it.
[01:20:45.300 --> 01:20:48.100]   And I think is it the gameplay that makes it so good?
[01:20:48.100 --> 01:20:49.940]   Same with your kid, Carsten.
[01:20:49.940 --> 01:20:53.380]   Is it the gameplay that they like or do they like the retro?
[01:20:53.380 --> 01:20:55.220]   The retro look can't mean anything to them.
[01:20:55.220 --> 01:20:56.780]   They don't remember that.
[01:20:56.780 --> 01:20:57.780]   Good games are good games.
[01:20:57.780 --> 01:20:59.060]   It's just a good game.
[01:20:59.060 --> 01:21:00.060]   Yeah.
[01:21:00.060 --> 01:21:06.380]   It shows you after you spend all this energy, making great graphics and high end hardware.
[01:21:06.380 --> 01:21:10.180]   And, you know, he has a fancy Windows 10 machine.
[01:21:10.180 --> 01:21:12.260]   He's playing on his Switch on the big straight TV.
[01:21:12.260 --> 01:21:14.380]   He's playing some eight bit game.
[01:21:14.380 --> 01:21:16.380]   It's kind of funny.
[01:21:16.380 --> 01:21:17.980]   Our show today brought you by LastPass.
[01:21:17.980 --> 01:21:21.340]   This I almost feel like I should do these ads as a public service announcement.
[01:21:21.340 --> 01:21:23.940]   That's how important I think LastPass is.
[01:21:23.940 --> 01:21:25.340]   13 million users.
[01:21:25.340 --> 01:21:26.340]   I love it.
[01:21:26.340 --> 01:21:28.140]   I've been using LastPass since they started.
[01:21:28.140 --> 01:21:34.020]   I sold it to, I was, you know, selling that's singing this praises as a password vault to
[01:21:34.020 --> 01:21:35.180]   Steve Gibson.
[01:21:35.180 --> 01:21:37.900]   He said, well, I'm going to look into this because that's how Steve is.
[01:21:37.900 --> 01:21:40.380]   He interviewed Joe Seagris, the creator of LastPass.
[01:21:40.380 --> 01:21:44.340]   Joe very generously showed him the source code, showed him how it works.
[01:21:44.340 --> 01:21:45.620]   And Steve said, this is great.
[01:21:45.620 --> 01:21:46.620]   He started using it.
[01:21:46.620 --> 01:21:48.220]   That was several years ago.
[01:21:48.220 --> 01:21:50.220]   We now use LastPass in the enterprise.
[01:21:50.220 --> 01:21:56.980]   So one of the things that happened to us a few years ago, one of our guys put all the
[01:21:56.980 --> 01:22:00.540]   company passwords on a public website.
[01:22:00.540 --> 01:22:03.460]   So he could remember them.
[01:22:03.460 --> 01:22:05.380]   And he doesn't work for us anymore.
[01:22:05.380 --> 01:22:08.900]   And we immediately adopted LastPass Enterprise.
[01:22:08.900 --> 01:22:11.580]   Look, there's too many passwords to remember.
[01:22:11.580 --> 01:22:13.380]   I actually am kind of sympathetic.
[01:22:13.380 --> 01:22:15.660]   He couldn't remember all the passwords.
[01:22:15.660 --> 01:22:21.780]   But really if you're, what you want is a way to keep all those passwords in a super secure
[01:22:21.780 --> 01:22:23.580]   encrypted vault.
[01:22:23.580 --> 01:22:27.860]   You remember one password that lets you unlock the vault and then it fills in your credentials
[01:22:27.860 --> 01:22:30.980]   no matter where you are.
[01:22:30.980 --> 01:22:33.100]   It generates strong passwords for you.
[01:22:33.100 --> 01:22:34.740]   Makes it easy to change passwords.
[01:22:34.740 --> 01:22:38.780]   One of the things I love about LastPass, when there is a breach, I can change the password
[01:22:38.780 --> 01:22:40.500]   in an automated fashion quickly.
[01:22:40.500 --> 01:22:43.860]   In fact, there's a security check you can run LastPass and you can go through and fix
[01:22:43.860 --> 01:22:44.860]   all of your passwords.
[01:22:44.860 --> 01:22:48.180]   So, you know, if you've not been using a password vault, you probably have duplicated
[01:22:48.180 --> 01:22:49.420]   passwords, things like that.
[01:22:49.420 --> 01:22:51.100]   It'll make strong passwords.
[01:22:51.100 --> 01:22:52.220]   It'll remember them.
[01:22:52.220 --> 01:22:58.740]   It'll fill them in and not just on your computer, but on your Android device, your iPhone.
[01:22:58.740 --> 01:23:05.740]   Everywhere you are, a plug-in for Chrome, Internet Explorer, Firefox, even Microsoft
[01:23:05.740 --> 01:23:06.740]   Edge.
[01:23:06.740 --> 01:23:11.700]   It's, I trust LastPass so much I put all my important documents in there, my passport
[01:23:11.700 --> 01:23:12.700]   images.
[01:23:12.700 --> 01:23:17.180]   That way I have them as I travel, my driver's license, my social security numbers, everything's
[01:23:17.180 --> 01:23:19.620]   in LastPass.
[01:23:19.620 --> 01:23:22.500]   And with LastPass Enterprise, our entire business is protected.
[01:23:22.500 --> 01:23:26.380]   In fact, I believe in LastPass so much, not only do we use LastPass Enterprise, but we
[01:23:26.380 --> 01:23:31.020]   offer it as a benefit to all our employees, a free LastPass account because I want them
[01:23:31.020 --> 01:23:35.820]   to use it for their personal stuff, not just my stuff, but their personal stuff too.
[01:23:35.820 --> 01:23:40.500]   Sensitive data encrypted at the device level with AES-256-bit encryption protects you from
[01:23:40.500 --> 01:23:42.140]   man in the middle attacks.
[01:23:42.140 --> 01:23:43.140]   Your stuff is safe.
[01:23:43.140 --> 01:23:44.500]   Your stuff is secure.
[01:23:44.500 --> 01:23:46.100]   It doesn't slow you down.
[01:23:46.100 --> 01:23:47.340]   It makes it easy.
[01:23:47.340 --> 01:23:50.620]   It is once you start using LastPass, you'll never go back.
[01:23:50.620 --> 01:23:51.780]   Now they have a number of products.
[01:23:51.780 --> 01:23:53.860]   We've got the Enterprise for business.
[01:23:53.860 --> 01:23:57.380]   LastPass premium for your personal use, LastPass families for the entire family.
[01:23:57.380 --> 01:24:02.980]   By the way, that's great too because Lisa and I have to share passwords for common stuff
[01:24:02.980 --> 01:24:06.500]   like our Comcast account, stuff like that.
[01:24:06.500 --> 01:24:07.500]   It's the easiest way.
[01:24:07.500 --> 01:24:08.500]   I just share it with her.
[01:24:08.500 --> 01:24:09.900]   I actually have a family folder.
[01:24:09.900 --> 01:24:11.060]   Everything goes in there.
[01:24:11.060 --> 01:24:15.060]   There's LastPass teams for smaller businesses, 50 or less.
[01:24:15.060 --> 01:24:19.220]   Get working at home, fix your password woes with LastPass.
[01:24:19.220 --> 01:24:22.380]   The number one most preferred password manager.
[01:24:22.380 --> 01:24:24.420]   It's got the Steve Gibson seal of approval.
[01:24:24.420 --> 01:24:25.420]   Mine too.
[01:24:25.420 --> 01:24:28.020]   LastPass.com/twit.
[01:24:28.020 --> 01:24:29.220]   Try it today.
[01:24:29.220 --> 01:24:30.780]   LastPass.com/twit.
[01:24:30.780 --> 01:24:32.140]   See which product is over it.
[01:24:32.140 --> 01:24:34.620]   Oh, there's a big picture of me on the page.
[01:24:34.620 --> 01:24:35.620]   Wow.
[01:24:35.620 --> 01:24:36.620]   LastPass.
[01:24:36.620 --> 01:24:42.740]   It is, this is actually, that quote is, I actually wrote it, is literally the first program I
[01:24:42.740 --> 01:24:44.740]   install on any new device or computer.
[01:24:44.740 --> 01:24:48.500]   The first thing I do is put the LastPass plug in on the desktop or the LastPass app because
[01:24:48.500 --> 01:24:52.660]   then all the other apps which I need to log into, I have LastPass.
[01:24:52.660 --> 01:24:54.340]   Makes it much easier.
[01:24:54.340 --> 01:24:58.660]   LastPass.com/twit.
[01:24:58.660 --> 01:25:00.660]   We're talking about countries operating.
[01:25:00.660 --> 01:25:03.780]   Oh, actually before we go on, I want to go talk a little bit about Apple, but before
[01:25:03.780 --> 01:25:08.460]   we go on, I think we should probably show you our little highlight reel from this week
[01:25:08.460 --> 01:25:09.460]   on Twit.
[01:25:09.460 --> 01:25:11.220]   Previously on Twit.
[01:25:11.220 --> 01:25:15.100]   What's up?
[01:25:15.100 --> 01:25:21.460]   I'm Jay Bonn, also known as Jason Howell, and I am here with the Art of Fawn, the new screen
[01:25:21.460 --> 01:25:22.460]   sabers.
[01:25:22.460 --> 01:25:26.420]   And this is kind of a new approach to MIDI controllers.
[01:25:26.420 --> 01:25:29.900]   This as you can see is kind of in the shape of a guitar.
[01:25:29.900 --> 01:25:31.140]   Mac Break Weekly.
[01:25:31.140 --> 01:25:35.300]   Do you like the new Macintosh laptop keyboards?
[01:25:35.300 --> 01:25:37.140]   Are you a butterfly girl?
[01:25:37.140 --> 01:25:39.820]   Yes, I am.
[01:25:39.820 --> 01:25:44.540]   But I know, Beyanna Shadow, I've doubt there is something wrong with that keyboard when
[01:25:44.540 --> 01:25:46.540]   thousands of people have had issues with it.
[01:25:46.540 --> 01:25:49.940]   I really think Apple needs to address this.
[01:25:49.940 --> 01:25:50.940]   Triangulation.
[01:25:50.940 --> 01:25:51.940]   It was not very long ago.
[01:25:51.940 --> 01:25:57.140]   I think it was earlier this month that kind of the greater public learned about the MISTI
[01:25:57.140 --> 01:25:58.300]   2.
[01:25:58.300 --> 01:26:04.100]   And one of our goals with MISTI was to really bring robots to people who aren't roboticists.
[01:26:04.100 --> 01:26:05.100]   Windows Weekly.
[01:26:05.100 --> 01:26:08.140]   Everybody's so excited about the new Surface 2 hub.
[01:26:08.140 --> 01:26:10.140]   Actually, it looks really cool.
[01:26:10.140 --> 01:26:13.700]   In a way, you can mix four together that they're showing right now with...
[01:26:13.700 --> 01:26:14.700]   Yeah.
[01:26:14.700 --> 01:26:15.700]   That's very interesting.
[01:26:15.700 --> 01:26:21.580]   If you think about a Surface hub as it exists, it's a very expensive enterprise product.
[01:26:21.580 --> 01:26:25.180]   I guess I'm being a skeptic on this because I'm like, is it really going to be for a more
[01:26:25.180 --> 01:26:27.140]   of a mainstream and more business?
[01:26:27.140 --> 01:26:28.140]   I'm very joined.
[01:26:28.140 --> 01:26:31.620]   Can I speak on behalf of everyone when I say I'm tired of your negativity?
[01:26:31.620 --> 01:26:32.620]   Tweet.
[01:26:32.620 --> 01:26:37.580]   It's like tech TV without the overpriced cable bill.
[01:26:37.580 --> 01:26:39.500]   That's true.
[01:26:39.500 --> 01:26:42.380]   We had a tech TV reunion this week.
[01:26:42.380 --> 01:26:47.660]   We all met in a bar about 50 of us because it was the 20th anniversary of the launch
[01:26:47.660 --> 01:26:49.740]   of tech TV on May 11th.
[01:26:49.740 --> 01:26:50.740]   It was really nice.
[01:26:50.740 --> 01:26:51.740]   So everybody was there.
[01:26:51.740 --> 01:26:52.740]   It was really fun.
[01:26:52.740 --> 01:26:53.740]   In fact, so much fun.
[01:26:53.740 --> 01:26:54.740]   I think we're going to do it again in August.
[01:26:54.740 --> 01:26:56.740]   We're going to have a part of our TV.
[01:26:56.740 --> 01:26:57.740]   TV TV.
[01:26:57.740 --> 01:26:58.740]   So it was TV TV first.
[01:26:58.740 --> 01:26:59.740]   Amazing yourself.
[01:26:59.740 --> 01:27:00.740]   Yup.
[01:27:00.740 --> 01:27:02.540]   It was my favorite thing to watch.
[01:27:02.540 --> 01:27:04.180]   I used to watch it from...
[01:27:04.180 --> 01:27:05.820]   I remember when it launched.
[01:27:05.820 --> 01:27:07.820]   And we got it on Comcast or whatever.
[01:27:07.820 --> 01:27:08.820]   And yeah.
[01:27:08.820 --> 01:27:11.140]   You must have been eight years old.
[01:27:11.140 --> 01:27:12.540]   I was a little older than that.
[01:27:12.540 --> 01:27:13.540]   Not much.
[01:27:13.540 --> 01:27:14.540]   I was a little older than that.
[01:27:14.540 --> 01:27:16.860]   20 years ago, you were probably 10, right?
[01:27:16.860 --> 01:27:17.860]   No.
[01:27:17.860 --> 01:27:18.860]   No.
[01:27:18.860 --> 01:27:19.860]   You were young.
[01:27:19.860 --> 01:27:20.860]   Hey, Leo.
[01:27:20.860 --> 01:27:25.580]   I know a tech history podcast that you might need to come back and tell stories on.
[01:27:25.580 --> 01:27:27.300]   Oh, I could tell so many stories.
[01:27:27.300 --> 01:27:29.300]   Most of them probably shouldn't.
[01:27:29.300 --> 01:27:31.020]   That's why you just need to go on.
[01:27:31.020 --> 01:27:32.020]   You need to go on.
[01:27:32.020 --> 01:27:33.020]   That's why you just got to go on.
[01:27:33.020 --> 01:27:35.780]   I actually went around to several of them.
[01:27:35.780 --> 01:27:38.820]   I was one of the young women, no longer young than women that were there.
[01:27:38.820 --> 01:27:40.980]   I said, I didn't harass you, did I?
[01:27:40.980 --> 01:27:42.420]   And they all said, I'm very proud.
[01:27:42.420 --> 01:27:43.420]   Oh, no.
[01:27:43.420 --> 01:27:44.420]   Of course not.
[01:27:44.420 --> 01:27:46.420]   I said, I'm trying to remember.
[01:27:46.420 --> 01:27:47.420]   Did I harass you?
[01:27:47.420 --> 01:27:48.860]   No, no, no, no.
[01:27:48.860 --> 01:27:50.580]   So you got to check in, right?
[01:27:50.580 --> 01:27:51.580]   Every once in a while.
[01:27:51.580 --> 01:27:55.220]   I said, every guy in the Me Too movement, every guy who worked in broadcasting is trying
[01:27:55.220 --> 01:27:56.220]   to...
[01:27:56.220 --> 01:27:58.420]   Oh, was I creepy?
[01:27:58.420 --> 01:27:59.420]   No.
[01:27:59.420 --> 01:28:02.780]   I'm glad to say, unless they're lying, but they got nothing to...
[01:28:02.780 --> 01:28:03.780]   They got nothing to...
[01:28:03.780 --> 01:28:05.300]   No reason to lie now.
[01:28:05.300 --> 01:28:07.060]   Was I creepy to you, Christina?
[01:28:07.060 --> 01:28:08.060]   No.
[01:28:08.060 --> 01:28:09.060]   No.
[01:28:09.060 --> 01:28:10.060]   No.
[01:28:10.060 --> 01:28:12.420]   That's why you're over there in Seattle.
[01:28:12.420 --> 01:28:14.020]   Far, far away.
[01:28:14.020 --> 01:28:15.380]   Apple is...
[01:28:15.380 --> 01:28:16.380]   This is interesting.
[01:28:16.380 --> 01:28:20.620]   This comes back to what we were talking about, getting, doing business in a country.
[01:28:20.620 --> 01:28:23.300]   You have to obey the laws of the country, right?
[01:28:23.300 --> 01:28:24.300]   Yeah.
[01:28:24.300 --> 01:28:28.620]   Apple has continuously kind of agreed to do what China's government wants it to do.
[01:28:28.620 --> 01:28:32.940]   They took VPNs out of the store last year.
[01:28:32.940 --> 01:28:37.940]   Now they're cracking down on apps that use the call kit API.
[01:28:37.940 --> 01:28:38.940]   This is...
[01:28:38.940 --> 01:28:41.420]   Christina, what do you know what call kit does?
[01:28:41.420 --> 01:28:42.420]   Yeah, I do.
[01:28:42.420 --> 01:28:43.420]   I do.
[01:28:43.420 --> 01:28:47.100]   So basically, you know how if you look up someone in your contacts, there's the option
[01:28:47.100 --> 01:28:49.220]   for you to call or send them an iMessage.
[01:28:49.220 --> 01:28:53.500]   And if you have certain apps installed like Skype or I guess for a while, WeChat or WhatsApp
[01:28:53.500 --> 01:28:57.540]   and there are a few others that support it, you can actually make like a WhatsApp call
[01:28:57.540 --> 01:28:59.100]   from your contact list.
[01:28:59.100 --> 01:29:03.140]   So you don't have to go into WhatsApp, you can just go into your contacts.
[01:29:03.140 --> 01:29:11.220]   And so that support comes via a call kit and it uses the VoIP protocol to plug into the
[01:29:11.220 --> 01:29:12.220]   contacts map.
[01:29:12.220 --> 01:29:18.660]   But I guess in this case, China is saying that we don't want any calls going through
[01:29:18.660 --> 01:29:19.660]   VoIP because...
[01:29:19.660 --> 01:29:20.660]   They're encrypted.
[01:29:20.660 --> 01:29:21.660]   Exactly.
[01:29:21.660 --> 01:29:28.540]   So Apple's way around that is just to say, "Okay, you're using call kit, remove it from
[01:29:28.540 --> 01:29:32.780]   your apps or we'll remove apps that are using that."
[01:29:32.780 --> 01:29:36.140]   I mean, I understand if you want to do business in China, you have to adhere to the laws of
[01:29:36.140 --> 01:29:37.140]   China.
[01:29:37.140 --> 01:29:39.700]   But I kind of admire Google's principles stand here.
[01:29:39.700 --> 01:29:41.500]   They still don't do business in China, right?
[01:29:41.500 --> 01:29:42.500]   No, they don't.
[01:29:42.500 --> 01:29:46.940]   I mean, but it is becoming very difficult for...
[01:29:46.940 --> 01:29:49.460]   I mean, it's created lots of issues with the Google Play.
[01:29:49.460 --> 01:29:52.740]   I mean, they found some ways around it, but it's become very difficult for Google Play
[01:29:52.740 --> 01:29:56.460]   and for Google to even distribute safe apps because people in China aren't using the
[01:29:56.460 --> 01:29:57.620]   official store.
[01:29:57.620 --> 01:30:02.580]   So instead, they're using these third-party stores that are scraping things and that's
[01:30:02.580 --> 01:30:04.180]   how malware gets spread.
[01:30:04.180 --> 01:30:06.100]   So there are trade-offs.
[01:30:06.100 --> 01:30:08.340]   And obviously, Apple doesn't even have the option.
[01:30:08.340 --> 01:30:14.020]   There are third-party app source for people who have jailbroken iPhones, but there isn't
[01:30:14.020 --> 01:30:16.060]   an alternative way for people to get apps.
[01:30:16.060 --> 01:30:21.220]   So yeah, I'm kind of with you.
[01:30:21.220 --> 01:30:24.060]   Part of me wishes that they could take a more principled stance, but the other part
[01:30:24.060 --> 01:30:30.260]   of me kind of understands that as a business, that's not really something where they can
[01:30:30.260 --> 01:30:36.580]   say no to one of their biggest revenue-generating countries.
[01:30:36.580 --> 01:30:40.580]   You know, Apple's...
[01:30:40.580 --> 01:30:44.180]   It's easy for Apple to be principled in the United States and say to the FBI, "We're
[01:30:44.180 --> 01:30:46.940]   not going to unlock that phone."
[01:30:46.940 --> 01:30:51.380]   Although I got a big fight with a waiter at a restaurant the other night who said, "Apple
[01:30:51.380 --> 01:30:53.580]   should unlock that phone."
[01:30:53.580 --> 01:30:57.660]   And I said, "Well, but yeah, but if they do, if they have a backdoor and then you're
[01:30:57.660 --> 01:30:58.660]   unsafe, no, no.
[01:30:58.660 --> 01:31:01.380]   Look, you're supporting terrorists."
[01:31:01.380 --> 01:31:04.780]   So Apple has this great principled stand in the United States and then when it's China,
[01:31:04.780 --> 01:31:05.940]   it's like, yeah, whatever.
[01:31:05.940 --> 01:31:09.100]   Well, but I do think that there's a difference there, right?
[01:31:09.100 --> 01:31:14.060]   Like I think that if China were demanding a backdoor, that's the difference.
[01:31:14.060 --> 01:31:15.060]   But they're gonna.
[01:31:15.060 --> 01:31:16.060]   And in that...
[01:31:16.060 --> 01:31:17.460]   I mean, they did that with Yahoo.
[01:31:17.460 --> 01:31:20.020]   Remember, they said, "Give us the information about these dissidents."
[01:31:20.020 --> 01:31:21.180]   They arrested the dissidents.
[01:31:21.180 --> 01:31:22.180]   It was a...
[01:31:22.180 --> 01:31:23.180]   I think a black mark right now.
[01:31:23.180 --> 01:31:24.860]   They don't need to anymore.
[01:31:24.860 --> 01:31:26.860]   They don't need to anymore.
[01:31:26.860 --> 01:31:30.460]   They have systems in all the intermediate steps.
[01:31:30.460 --> 01:31:36.220]   So they won't have to get handled to hand over the case.
[01:31:36.220 --> 01:31:37.220]   I don't think so.
[01:31:37.220 --> 01:31:38.220]   Potentially.
[01:31:38.220 --> 01:31:39.220]   But I would just...
[01:31:39.220 --> 01:31:41.820]   I mean, I would hope, and I don't know, obviously, I don't work there and I'm not trying to speak
[01:31:41.820 --> 01:31:45.740]   for them, but I would hope that if that were something that were coming down, that it wasn't
[01:31:45.740 --> 01:31:52.500]   about VPNs, if it wasn't about way protocols, if it wasn't about the concessions they'd
[01:31:52.500 --> 01:31:55.820]   always made about where they stored Chinese users' data.
[01:31:55.820 --> 01:32:01.460]   The different were about something as potentially overreaching as you have to give us a backdoor
[01:32:01.460 --> 01:32:08.380]   to the OS, that there would be much more pushback up to and including pulling out of the market
[01:32:08.380 --> 01:32:09.980]   or at least a threat of that.
[01:32:09.980 --> 01:32:13.380]   I don't know for sure, but I feel like there are variations.
[01:32:13.380 --> 01:32:18.180]   You can work with countries to abide by certain things, and then there's certain steps where
[01:32:18.180 --> 01:32:20.860]   you can just say, "No, I'm not going to do this."
[01:32:20.860 --> 01:32:28.900]   And for Google, part of it is that if they were to abide by China's laws, they would
[01:32:28.900 --> 01:32:33.100]   have to change fundamental parts of their business, and they're not willing to do that.
[01:32:33.100 --> 01:32:37.980]   In Google's case, it's easy enough to move Google Search to Hong Kong.
[01:32:37.980 --> 01:32:39.780]   Well, that's the thing.
[01:32:39.780 --> 01:32:40.780]   That's the thing.
[01:32:40.780 --> 01:32:44.260]   There are still having Android licensed phones sold in China.
[01:32:44.260 --> 01:32:47.820]   They just don't have the Google Play, Google Services running.
[01:32:47.820 --> 01:32:54.140]   But you're right, they still have things in Hong Kong, and the savvy users are still
[01:32:54.140 --> 01:32:57.940]   potentially able to touch into their services.
[01:32:57.940 --> 01:33:01.500]   It has created problems for them, and it is going to be an issue they kind of grapple
[01:33:01.500 --> 01:33:05.820]   with going forward, which is when you look at Tenson and Baidu and some of these other
[01:33:05.820 --> 01:33:11.820]   really, really big Chinese service companies that Google doesn't have a play in that market.
[01:33:11.820 --> 01:33:14.020]   But Google's business and Apple's businesses are very different.
[01:33:14.020 --> 01:33:18.060]   I mean, Google's selling data, Apple's primarily selling hardware.
[01:33:18.060 --> 01:33:24.540]   Yeah, with a VPN thing, I think they could kind of skate it because you could still use
[01:33:24.540 --> 01:33:25.540]   a VPN.
[01:33:25.540 --> 01:33:27.300]   You just can't get it in the App Store.
[01:33:27.300 --> 01:33:29.820]   And so that's not the end of the world.
[01:33:29.820 --> 01:33:35.860]   If you're a Chinese dissident or whatever, it's unsettling.
[01:33:35.860 --> 01:33:36.860]   I agree.
[01:33:36.860 --> 01:33:37.860]   I agree.
[01:33:37.860 --> 01:33:45.140]   I would be curious to know how many Chinese iOS users have iTunes accounts they created
[01:33:45.140 --> 01:33:49.220]   in other countries in order to get access to different apps.
[01:33:49.220 --> 01:33:54.220]   Because you can log in with the US account, and then you can log in with the Chinese account
[01:33:54.220 --> 01:33:55.580]   and download different apps.
[01:33:55.580 --> 01:33:58.700]   Now you've got a login each time that you want to do an update, but it's relatively
[01:33:58.700 --> 01:34:03.420]   easy to just log out of and log in to another country's account if you need to download
[01:34:03.420 --> 01:34:09.020]   a country-specific app or if you need to get an app that's not available in another country.
[01:34:09.020 --> 01:34:15.460]   So I would be interested to know how many savvy Chinese iPhone users also have either
[01:34:15.460 --> 01:34:18.540]   American or European iTunes accounts.
[01:34:18.540 --> 01:34:23.300]   Wouldn't the Chinese ISPs block the IPs of the stores?
[01:34:23.300 --> 01:34:25.820]   Well, that's kind of an interesting thing.
[01:34:25.820 --> 01:34:28.540]   They could, but some of them, I mean, it's intense.
[01:34:28.540 --> 01:34:29.540]   It's more selective.
[01:34:29.540 --> 01:34:35.380]   I was talking to somebody in Expat who lives in China, married a Chinese wife, and he travels
[01:34:35.380 --> 01:34:36.940]   a lot.
[01:34:36.940 --> 01:34:39.940]   And I was asking about the VPN thing.
[01:34:39.940 --> 01:34:46.860]   He said, "Well, you don't want to get caught using a VPN, but they also turn a blind eye
[01:34:46.860 --> 01:34:47.860]   to it."
[01:34:47.860 --> 01:34:52.300]   A lot of the intelligentsia uses it, and they know that they expect to be able to use that.
[01:34:52.300 --> 01:34:54.980]   They want access to the internet.
[01:34:54.980 --> 01:34:56.500]   And so they just turn a blind eye to it.
[01:34:56.500 --> 01:35:01.140]   On the other hand, he got in trouble.
[01:35:01.140 --> 01:35:03.940]   It was a minor thing, but he got in trouble.
[01:35:03.940 --> 01:35:09.860]   And now every time he enters the country, he gets questioned for two or three hours.
[01:35:09.860 --> 01:35:14.900]   And he has to present himself when he goes anywhere in China to the local authorities
[01:35:14.900 --> 01:35:17.020]   and say, "I'm here.
[01:35:17.020 --> 01:35:18.780]   Just so you know, I might be spying on you.
[01:35:18.780 --> 01:35:19.780]   I'm here."
[01:35:19.780 --> 01:35:23.140]   And so it's an interesting country in that regard.
[01:35:23.140 --> 01:35:27.500]   I mean, it is interesting, though, because like for instance, if you have like an American
[01:35:27.500 --> 01:35:31.820]   iPhone or whatever, I mean, I've had friends who've gone to China, yes, well, I was going
[01:35:31.820 --> 01:35:36.260]   to say, and I have friends who some of them are like actually Chinese nationals who will
[01:35:36.260 --> 01:35:41.420]   go there to get their visa renewed or whatever, and will then be communicating with me using
[01:35:41.420 --> 01:35:46.300]   Expat Messenger, which shouldn't be blocked, but it's not blocked yet.
[01:35:46.300 --> 01:35:47.300]   That's how I'm communicating with them.
[01:35:47.300 --> 01:35:49.500]   And I would even say, "Do we need to use WeChat?
[01:35:49.500 --> 01:35:50.980]   Do we need to use whatever?"
[01:35:50.980 --> 01:35:52.940]   No, no, no, Messenger's fine.
[01:35:52.940 --> 01:35:58.820]   So I'm not going to pretend that the intricacies, but I feel like the savvier people which probably
[01:35:58.820 --> 01:36:02.140]   tend to include people who have more monies, which tends to align with people who are iPhone
[01:36:02.140 --> 01:36:08.300]   users have methods of getting around some of these things I would assume.
[01:36:08.300 --> 01:36:12.700]   Apple is, interestingly, finally giving Ireland its due.
[01:36:12.700 --> 01:36:17.060]   Remember, one of the reasons, one of the ways Apple and many companies were able to defer
[01:36:17.060 --> 01:36:24.340]   US taxes is by opening essentially shell companies in Ireland and then transferring IP rights
[01:36:24.340 --> 01:36:31.540]   to the companies and then paying the companies fees for the use of their technology, thereby
[01:36:31.540 --> 01:36:32.980]   getting the money overseas.
[01:36:32.980 --> 01:36:38.740]   Now, of course, with the new tax law, Apple's repatriated a couple of hundred billion dollars,
[01:36:38.740 --> 01:36:43.620]   and then the EU goes to Ireland and said, "You should have been charging Apple tax on this."
[01:36:43.620 --> 01:36:51.100]   Apple has been told that Ireland has to hold 13 billion euros in disputed taxes.
[01:36:51.100 --> 01:36:52.180]   Apple has started to pay that.
[01:36:52.180 --> 01:36:54.740]   They paid 1.76 billion dollars.
[01:36:54.740 --> 01:36:58.100]   I will point out that they have not actually paid it.
[01:36:58.100 --> 01:36:59.100]   Oh, is this it?
[01:36:59.100 --> 01:37:00.100]   It's going into an escrow.
[01:37:00.100 --> 01:37:01.100]   It's going into an escrow.
[01:37:01.100 --> 01:37:02.100]   It's escrow.
[01:37:02.100 --> 01:37:03.100]   It's escrow account, yeah.
[01:37:03.100 --> 01:37:05.860]   Because it's not technically, there's still appeals and things like that.
[01:37:05.860 --> 01:37:06.860]   Yeah.
[01:37:06.860 --> 01:37:07.860]   Yeah.
[01:37:07.860 --> 01:37:10.180]   Now, is it earning interest while it's an escrow?
[01:37:10.180 --> 01:37:11.180]   Yeah.
[01:37:11.180 --> 01:37:12.620]   Isn't that what escrow is for, I think, right?
[01:37:12.620 --> 01:37:14.620]   No, escrow means third part of the whole.
[01:37:14.620 --> 01:37:16.340]   I think it was like third party is just holding it.
[01:37:16.340 --> 01:37:17.340]   So that's what I'm saying.
[01:37:17.340 --> 01:37:18.860]   I bet you don't get interest on it.
[01:37:18.860 --> 01:37:19.860]   Yeah.
[01:37:19.860 --> 01:37:20.860]   Yeah.
[01:37:20.860 --> 01:37:21.860]   No, I don't.
[01:37:21.860 --> 01:37:22.860]   I think that, well, I don't know.
[01:37:22.860 --> 01:37:25.700]   Well, the other thing that I'll point out is that that could pay for it, you know, during
[01:37:25.700 --> 01:37:26.700]   the first process of the day.
[01:37:26.700 --> 01:37:27.700]   You know what?
[01:37:27.700 --> 01:37:28.700]   They are earning interest because you know who manages it?
[01:37:28.700 --> 01:37:32.540]   Goldman Sachs Asset Management.
[01:37:32.540 --> 01:37:37.220]   They're holding the money and they make low risk investment decisions to protect the
[01:37:37.220 --> 01:37:39.380]   Irish taxpayer.
[01:37:39.380 --> 01:37:41.940]   This is from Reuters.
[01:37:41.940 --> 01:37:48.540]   Did you notice Bloomberg and a bunch of other places had this that in the latest, you know,
[01:37:48.540 --> 01:37:52.260]   everybody in Tech had their quarterly earnings last month or a couple of weeks ago.
[01:37:52.260 --> 01:37:55.860]   And for years, they all reported the overseas money.
[01:37:55.860 --> 01:37:56.860]   Right.
[01:37:56.860 --> 01:37:58.260]   And it's gone from Apple.
[01:37:58.260 --> 01:38:04.780]   It's gone from Microsoft, Netflix, Alphabet, everybody is no longer because it's not that
[01:38:04.780 --> 01:38:06.780]   important to point that out, I guess, anymore.
[01:38:06.780 --> 01:38:08.420]   Well, isn't it much of it repatriated?
[01:38:08.420 --> 01:38:12.380]   Or are you saying it's still there, but they just don't want to talk about it?
[01:38:12.380 --> 01:38:18.660]   My understanding is that like there's no rule saying that they ever had to say what it was.
[01:38:18.660 --> 01:38:23.580]   So the implication is, is that by them always reporting that there's all this money they
[01:38:23.580 --> 01:38:24.580]   can't touch.
[01:38:24.580 --> 01:38:25.580]   Yes.
[01:38:25.580 --> 01:38:29.220]   That it was like getting shareholders to apply pressure that we need to bring.
[01:38:29.220 --> 01:38:30.740]   That's exactly right.
[01:38:30.740 --> 01:38:35.540]   Because that was always the implication, which was we, this is money that could be in the
[01:38:35.540 --> 01:38:36.540]   United States.
[01:38:36.540 --> 01:38:38.220]   If only you would relax the tax laws.
[01:38:38.220 --> 01:38:40.380]   And now that that's gone, they don't have to mention that.
[01:38:40.380 --> 01:38:45.700]   And in fact, I mean, in some cases it works out well because if there were currency, if
[01:38:45.700 --> 01:38:50.540]   the dollar was stronger, for instance, then all that cash would be worthless.
[01:38:50.540 --> 01:38:56.020]   So in some cases, it would be negative even sometimes applying those things.
[01:38:56.020 --> 01:39:01.140]   But yeah, I think you're exactly right is that that's now it's like, well, who cares?
[01:39:01.140 --> 01:39:05.740]   We got what we wanted and now we can not talk about that.
[01:39:05.740 --> 01:39:10.940]   Which, by the way, Europeans aren't super happy that all of that money is gone because
[01:39:10.940 --> 01:39:15.140]   it was made on European citizens, right?
[01:39:15.140 --> 01:39:20.900]   And so you were you, you know, the American people or the American stockholders.
[01:39:20.900 --> 01:39:21.900]   Oh, it's me.
[01:39:21.900 --> 01:39:22.900]   You could blame me.
[01:39:22.900 --> 01:39:25.380]   It was Leo was basically saying, "Bring the money back!"
[01:39:25.380 --> 01:39:29.620]   Oh, and the shareholders are very happy.
[01:39:29.620 --> 01:39:34.460]   Because most of the time, in most cases, this money is being used to buy back stock,
[01:39:34.460 --> 01:39:39.460]   which makes their stock worth more.
[01:39:39.460 --> 01:39:42.380]   It's I guess some of it might be used.
[01:39:42.380 --> 01:39:44.020]   Actually, isn't Apple, aren't they?
[01:39:44.020 --> 01:39:46.500]   Didn't they announce a new campus?
[01:39:46.500 --> 01:39:47.500]   They're shopping.
[01:39:47.500 --> 01:39:48.500]   They're shopping around.
[01:39:48.500 --> 01:39:51.580]   So maybe they're spending, I think, 30 billion, something like that.
[01:39:51.580 --> 01:39:55.020]   Well, they claimed, well, remember, it was a couple months ago that they said that they
[01:39:55.020 --> 01:39:59.020]   were going to spend, I think, the number was $200 billion.
[01:39:59.020 --> 01:40:03.140]   And bring, I think, the number was 20,000 jobs to the US.
[01:40:03.140 --> 01:40:07.900]   And so the rumors this week, again, I think both of the things I mentioned recently are
[01:40:07.900 --> 01:40:08.900]   Mark Gurman scoops.
[01:40:08.900 --> 01:40:10.580]   Of course they are.
[01:40:10.580 --> 01:40:14.180]   Is that they've been poking around DC, but then also...
[01:40:14.180 --> 01:40:15.180]   Or North Carolina.
[01:40:15.180 --> 01:40:16.180]   Right.
[01:40:16.180 --> 01:40:17.180]   What's that?
[01:40:17.180 --> 01:40:18.180]   Or no, Virginia, sorry.
[01:40:18.180 --> 01:40:20.180]   No, no, it's North Carolina.
[01:40:20.180 --> 01:40:21.180]   The research triangle.
[01:40:21.180 --> 01:40:22.180]   Yeah.
[01:40:22.180 --> 01:40:23.180]   Yeah.
[01:40:23.180 --> 01:40:24.180]   The research triangle.
[01:40:24.180 --> 01:40:25.180]   Yeah.
[01:40:25.180 --> 01:40:26.180]   The general health.
[01:40:26.180 --> 01:40:27.180]   Yeah, exactly.
[01:40:27.180 --> 01:40:28.580]   That has MBA from Duke.
[01:40:28.580 --> 01:40:29.580]   So...
[01:40:29.580 --> 01:40:32.620]   Well, and they already have the data centers.
[01:40:32.620 --> 01:40:38.860]   Well, and apparently though, if I, again, I don't have the article in front of me, the...
[01:40:38.860 --> 01:40:44.100]   I think that the amount of office space they were looking for, at least in Virginia, was
[01:40:44.100 --> 01:40:48.820]   half of what Amazon was looking for for its entire second headquarters.
[01:40:48.820 --> 01:40:54.540]   So this would be a significant location if they do do it.
[01:40:54.540 --> 01:40:55.740]   No, it definitely would.
[01:40:55.740 --> 01:40:56.740]   Because Amazon has...
[01:40:56.740 --> 01:41:01.020]   I mean, granted, a lot of Amazon's employees are in their warehouses and things like that.
[01:41:01.020 --> 01:41:05.740]   But I believe Amazon has more engineers, corporate employees than Apple does.
[01:41:05.740 --> 01:41:12.780]   So if they're looking at half the number of HQ2, which is supposed to be a true second
[01:41:12.780 --> 01:41:18.380]   headquarters, then that's very significant considering the number of employees that are
[01:41:18.380 --> 01:41:23.300]   typically at non-copertino Apple campuses.
[01:41:23.300 --> 01:41:27.580]   I was... my understanding, maybe I'm wrong, that really it's not a new second headquarters
[01:41:27.580 --> 01:41:28.580]   for Apple.
[01:41:28.580 --> 01:41:30.380]   It's probably where their call center...
[01:41:30.380 --> 01:41:31.380]   No, they...
[01:41:31.380 --> 01:41:34.380]   Right, no, but they said it was going to be technical support and something else.
[01:41:34.380 --> 01:41:35.660]   So yeah, yeah, it would...
[01:41:35.660 --> 01:41:39.020]   But also, this is not the first one because they...
[01:41:39.020 --> 01:41:44.340]   Again, the size of this would not at all rival what they have in California.
[01:41:44.340 --> 01:41:50.580]   But the point of the articles I read was it was so significant that the states are bending
[01:41:50.580 --> 01:41:51.580]   over backwards.
[01:41:51.580 --> 01:41:55.860]   Apparently, Cook has met with the governor of Virginia, the governor of North Carolina.
[01:41:55.860 --> 01:41:57.780]   So they're almost going...
[01:41:57.780 --> 01:42:01.860]   And all of these cities have already sort of put on their best...
[01:42:01.860 --> 01:42:03.260]   Their Sunday best for Amazon.
[01:42:03.260 --> 01:42:05.940]   So it was very easy for them to do the same thing for Apple.
[01:42:05.940 --> 01:42:08.180]   They're just gonna flip the application.
[01:42:08.180 --> 01:42:12.980]   What's interesting though is obviously that these cities are bending over backwards for
[01:42:12.980 --> 01:42:14.900]   Amazon and now Apple.
[01:42:14.900 --> 01:42:18.580]   I live in Seattle, which has...
[01:42:18.580 --> 01:42:22.220]   Dealing firsthand with some of the good and the bad things that happen when you have
[01:42:22.220 --> 01:42:24.300]   Amazon in your backyard.
[01:42:24.300 --> 01:42:28.220]   And it will be interesting to see, sometimes these things, these deals that cities make
[01:42:28.220 --> 01:42:32.020]   with these companies aren't always as beneficial as you would think.
[01:42:32.020 --> 01:42:33.020]   And they're not...
[01:42:33.020 --> 01:42:34.460]   How do people feel about Amazon in Seattle?
[01:42:34.460 --> 01:42:36.100]   Because I've heard mixed reports.
[01:42:36.100 --> 01:42:39.020]   I mean, they pretty much are kind of in the downtown.
[01:42:39.020 --> 01:42:40.020]   They're not like a...
[01:42:40.020 --> 01:42:41.020]   Oh, no, no.
[01:42:41.020 --> 01:42:42.580]   No, they own the downtown.
[01:42:42.580 --> 01:42:44.220]   Like downtown is Amazon.
[01:42:44.220 --> 01:42:47.980]   Like the South of Union, West like Union area is Amazon, period.
[01:42:47.980 --> 01:42:48.980]   Is that okay?
[01:42:48.980 --> 01:42:49.980]   Or is it...
[01:42:49.980 --> 01:42:55.820]   It's interesting because I'm sort of biased in that I have a tremendous amount of friends
[01:42:55.820 --> 01:42:56.980]   that work there.
[01:42:56.980 --> 01:43:01.260]   And like my building, my apartment building, I would say they're 88 units.
[01:43:01.260 --> 01:43:08.340]   I would say 80 of them, the people in them work at Amazon or Microsoft or Google or Facebook.
[01:43:08.340 --> 01:43:09.820]   You know, like it's...
[01:43:09.820 --> 01:43:11.980]   I would say genuinely like that's probably the...
[01:43:11.980 --> 01:43:15.500]   And the breakdown is you might have a couple of people who don't do those things.
[01:43:15.500 --> 01:43:21.100]   But Amazon has a huge workforce in that building and Microsoft does too.
[01:43:21.100 --> 01:43:24.780]   But when I talk to locals like like Uber drivers or Lyft drivers or people who've been around
[01:43:24.780 --> 01:43:28.260]   for a while, their opinion is a lot more negative.
[01:43:28.260 --> 01:43:32.220]   And so I think for people who are in the tech industry, there's one perspective for people
[01:43:32.220 --> 01:43:35.900]   who aren't who especially have been in the city for longer than in the area for longer.
[01:43:35.900 --> 01:43:40.540]   I think they kind of blame Amazon on some of the housing issues and some of the housing
[01:43:40.540 --> 01:43:46.620]   crisis issues and home prices in Seattle area have just exploded.
[01:43:46.620 --> 01:43:53.020]   And it's what's happened in New York and in San Francisco is happening in Seattle and
[01:43:53.020 --> 01:43:58.940]   the growth has just made it really hard for people to buy homes let alone rents go up
[01:43:58.940 --> 01:43:59.940]   too.
[01:43:59.940 --> 01:44:03.820]   So there's a lot of I think locals who don't like the company.
[01:44:03.820 --> 01:44:07.460]   And then you kind of have this flip side where a lot of the new people who are coming into
[01:44:07.460 --> 01:44:12.020]   the city, which obviously are what's making the city kind of boom, it's because they're
[01:44:12.020 --> 01:44:15.700]   working in tech and Amazon is obviously one of the biggest employers.
[01:44:15.700 --> 01:44:17.420]   So it's kind of mixed.
[01:44:17.420 --> 01:44:22.500]   Well, as Roland and our chairam said, that's how you got a one hour delivery of a USB
[01:44:22.500 --> 01:44:23.500]   seat charger.
[01:44:23.500 --> 01:44:25.460]   That is exactly what I mean.
[01:44:25.460 --> 01:44:29.420]   I'm definitely like, I'm not going to like speak against it, you know, because I take
[01:44:29.420 --> 01:44:30.420]   advantage of it.
[01:44:30.420 --> 01:44:35.820]   But I also can understand, you know, that there was, you know, the city council is imposing
[01:44:35.820 --> 01:44:39.820]   this tax for instance, on companies saying that if you generate more than a certain amount
[01:44:39.820 --> 01:44:43.020]   of revenue in the city proper, then you have to pay a tax.
[01:44:43.020 --> 01:44:46.340]   And it was originally supposed to be, I think, $500 per employee.
[01:44:46.340 --> 01:44:49.260]   And they've since settled to $250 per employee.
[01:44:49.260 --> 01:44:54.700]   And Amazon's response was a play pretty significant hardball and to say, okay, well, you know,
[01:44:54.700 --> 01:44:58.340]   these two skyscrapers we're working on, we're just going to abandon them.
[01:44:58.340 --> 01:45:00.500]   And they've since risen construction on one.
[01:45:00.500 --> 01:45:02.820]   And I think they're still in talks to the other.
[01:45:02.820 --> 01:45:05.660]   But I mean, it was significant enough that I think the city thought that they kind of
[01:45:05.660 --> 01:45:11.780]   had Amazon by the proverbial, you know, balls, so to speak, I was trying to think of it.
[01:45:11.780 --> 01:45:13.620]   Short hairs is what we say.
[01:45:13.620 --> 01:45:14.620]   The short hairs, exactly.
[01:45:14.620 --> 01:45:16.620]   That was the better euphemism.
[01:45:16.620 --> 01:45:21.860]   But they didn't because what ended up happening was Amazon said, okay, fine, we'll stop.
[01:45:21.860 --> 01:45:27.660]   And then all of a sudden, you know, Amazon said, nice city you have here to be a shame
[01:45:27.660 --> 01:45:29.820]   if anything would have happened to it.
[01:45:29.820 --> 01:45:30.820]   No, totally.
[01:45:30.820 --> 01:45:34.380]   And it does create a really conflicting thing for the city.
[01:45:34.380 --> 01:45:38.900]   Because again, on the one hand, it's not as if Amazon is the only company that wants
[01:45:38.900 --> 01:45:39.900]   to still.
[01:45:39.900 --> 01:45:42.220]   Yeah, it's a Microsoft is Boeing.
[01:45:42.220 --> 01:45:43.780]   I mean, Seattle is a big city.
[01:45:43.780 --> 01:45:48.340]   Well, not only that, but you know, Facebook, you know, has a small campus.
[01:45:48.340 --> 01:45:51.900]   Yeah, Google has two different offices and they're expanding.
[01:45:51.900 --> 01:45:56.220]   Obviously, there's a taking over the world.
[01:45:56.220 --> 01:45:57.220]   Apple has.
[01:45:57.220 --> 01:46:00.220]   I mean, it's all, but Apple has has a presence.
[01:46:00.220 --> 01:46:04.900]   So it's not as if other companies wouldn't want to own this office space.
[01:46:04.900 --> 01:46:10.940]   But you know, at the same time, you don't want one of your biggest, you know, landowners
[01:46:10.940 --> 01:46:14.020]   to say we're done either.
[01:46:14.020 --> 01:46:16.780]   So it's they ended up compromising.
[01:46:16.780 --> 01:46:20.820]   But I would say I feel like the city blinked pretty hard.
[01:46:20.820 --> 01:46:24.220]   Like, you know, that's what's happening with Apple that Tim Cook, when he meets with these
[01:46:24.220 --> 01:46:27.620]   mayors, he doesn't walk in and says, say, hey, we can't wait to be here.
[01:46:27.620 --> 01:46:29.900]   He says, what are you going to do for us?
[01:46:29.900 --> 01:46:30.900]   Absolutely.
[01:46:30.900 --> 01:46:31.900]   Absolutely.
[01:46:31.900 --> 01:46:34.980]   But what's interesting is I think what happens though is, you know, the cities at first,
[01:46:34.980 --> 01:46:36.380]   they look at how great it's going to be.
[01:46:36.380 --> 01:46:41.380]   And then you start to realize some of the side effects if you're not prepared for that.
[01:46:41.380 --> 01:46:46.460]   And then if you want to start taxing and start respecting more, that's when it becomes difficult.
[01:46:46.460 --> 01:46:52.660]   And that's why I do think that for whatever, you know, either any city looking at taking
[01:46:52.660 --> 01:46:57.340]   on, you know, Apple or HQ2 or any other tech companies, like you need to look at places
[01:46:57.340 --> 01:47:04.380]   like Seattle to kind of see like, this is what happens when you kind of give them the world.
[01:47:04.380 --> 01:47:09.660]   Now, conversely, there was an article last week I did a segment on, I think it was Wall
[01:47:09.660 --> 01:47:15.060]   Street Journal that all of these cities specifically they mentioned Cincinnati, all these cities,
[01:47:15.060 --> 01:47:16.860]   they dress up for Amazon.
[01:47:16.860 --> 01:47:17.980]   Amazon comes to town.
[01:47:17.980 --> 01:47:23.740]   Amazon says no, but then they give you the reasons why you say no.
[01:47:23.740 --> 01:47:26.140]   And all of these cities are implementing the changes.
[01:47:26.140 --> 01:47:27.140]   Fixing it.
[01:47:27.140 --> 01:47:28.140]   Yeah.
[01:47:28.140 --> 01:47:29.140]   Yeah.
[01:47:29.140 --> 01:47:30.140]   Thanks.
[01:47:30.140 --> 01:47:32.340]   Amazon gave us some notes and we're going to, we're going to, we're going to fix this city
[01:47:32.340 --> 01:47:33.340]   up.
[01:47:33.340 --> 01:47:36.940]   Well, I guess, you know, tax base, right?
[01:47:36.940 --> 01:47:37.940]   I give it.
[01:47:37.940 --> 01:47:38.940]   Yeah.
[01:47:38.940 --> 01:47:43.980]   I want to take a break because I have to have some wine, but I, but I want, I'm wondering
[01:47:43.980 --> 01:47:47.940]   now somebody who said something in the chat room that kind of got me thinking.
[01:47:47.940 --> 01:47:50.340]   He said, this isn't tech.
[01:47:50.340 --> 01:47:51.580]   These are big companies.
[01:47:51.580 --> 01:47:54.300]   They just use tech to make money.
[01:47:54.300 --> 01:48:01.700]   Are big companies bad for tech have, have, have the big companies taken over tech?
[01:48:01.700 --> 01:48:02.860]   And is that hurting us?
[01:48:02.860 --> 01:48:03.860]   Is that hurting innovation?
[01:48:03.860 --> 01:48:05.140]   Is it?
[01:48:05.140 --> 01:48:06.140]   It's an interesting question.
[01:48:06.140 --> 01:48:10.460]   Let's take a break and talk about it because I have a little delivery.
[01:48:10.460 --> 01:48:12.140]   I've got to open up here.
[01:48:12.140 --> 01:48:15.340]   My monthly, my monthly, oh, it even made a sound.
[01:48:15.340 --> 01:48:19.180]   My monthly, my monthly wink is here.
[01:48:19.180 --> 01:48:22.060]   I love the wink wine.
[01:48:22.060 --> 01:48:24.060]   It is a really great idea.
[01:48:24.060 --> 01:48:31.100]   If you go to W.I. and see, actually it's try, t-r-y-w-i-n-c-i-n-c-i-m slash to it, you can
[01:48:31.100 --> 01:48:37.060]   go through their taste questionnaire to figure out what kind of wine you would like.
[01:48:37.060 --> 01:48:39.700]   They ask you about things like, do you like, how do you like your coffee?
[01:48:39.700 --> 01:48:41.700]   Do you like salt?
[01:48:41.700 --> 01:48:44.100]   Do you like mushrooms?
[01:48:44.100 --> 01:48:45.940]   Do you like citrus?
[01:48:45.940 --> 01:48:51.140]   And then the taste profile, they'll build you your first wink delivery.
[01:48:51.140 --> 01:48:54.420]   You get to choose all white, all red, a split.
[01:48:54.420 --> 01:48:58.140]   You get to, actually, if you don't like a wine, you can replace it as well.
[01:48:58.140 --> 01:48:59.140]   But wink is neat.
[01:48:59.140 --> 01:49:03.380]   Wink is not, it's a wine club, but they're the wine maker.
[01:49:03.380 --> 01:49:04.380]   That's really important.
[01:49:04.380 --> 01:49:08.140]   They're not reselling old wine from some company that couldn't sell it.
[01:49:08.140 --> 01:49:10.820]   Oh, it makes such a nice sound when I open it up.
[01:49:10.820 --> 01:49:14.380]   They are making the wine and they make some amazing wine.
[01:49:14.380 --> 01:49:16.220]   Here's something from New Zealand.
[01:49:16.220 --> 01:49:20.500]   Outer sounds, a beautiful Sauvignon Blanc.
[01:49:20.500 --> 01:49:22.860]   I love it when my delivery comes.
[01:49:22.860 --> 01:49:25.780]   This is, look at the labels too, they're great.
[01:49:25.780 --> 01:49:29.580]   V-Bass Merlot from Paydock, a French Merlot.
[01:49:29.580 --> 01:49:32.660]   Actually, I'm going to see, you know, what I usually do.
[01:49:32.660 --> 01:49:34.580]   We go through about a bottle a week.
[01:49:34.580 --> 01:49:40.380]   You know, we have it with a nice dinner and maybe, maybe, maybe, maybe three bottles a
[01:49:40.380 --> 01:49:41.540]   month.
[01:49:41.540 --> 01:49:44.900]   And then we always have one and I always decide when I open the box, oh, this is the one
[01:49:44.900 --> 01:49:49.140]   folly of the beast, a Pinot, probably from the central coast.
[01:49:49.140 --> 01:49:50.140]   Yeah.
[01:49:50.140 --> 01:49:54.700]   So I always look, oh, this is the one that I'm going to bring when we go to visit people.
[01:49:54.700 --> 01:49:57.060]   You know, it's nice to bring a little bottle of wine.
[01:49:57.060 --> 01:49:59.900]   And we actually did that last night and it's nice.
[01:49:59.900 --> 01:50:02.100]   You have a little something to give.
[01:50:02.100 --> 01:50:03.820]   So do the quiz.
[01:50:03.820 --> 01:50:10.020]   They work with the best winemakers and growers from around the world to make this wine.
[01:50:10.020 --> 01:50:13.500]   Each month there are new delicious wines.
[01:50:13.500 --> 01:50:14.500]   Summertime's coming.
[01:50:14.500 --> 01:50:16.420]   You've got to try the summer water, Rose.
[01:50:16.420 --> 01:50:18.180]   It's amazingly refreshing.
[01:50:18.180 --> 01:50:19.740]   And frankly, this is my favorite.
[01:50:19.740 --> 01:50:22.060]   We always get at least one bottle of this.
[01:50:22.060 --> 01:50:27.460]   It's the Field Theory Blau Frankish, a Passo Robles.
[01:50:27.460 --> 01:50:29.580]   So it's also central coast.
[01:50:29.580 --> 01:50:31.540]   This is the most delicious wine I have ever had.
[01:50:31.540 --> 01:50:32.540]   And I love the bottle.
[01:50:32.540 --> 01:50:36.060]   It's kind of a cool, almost looks like a whiskey bottle instead of a regular wine bottle and
[01:50:36.060 --> 01:50:38.340]   the labels painted on.
[01:50:38.340 --> 01:50:42.180]   This is, I think this is the wine that they pick the grapes at midnight.
[01:50:42.180 --> 01:50:45.340]   They pick them in the dark because they don't want the hot sun.
[01:50:45.340 --> 01:50:47.940]   They don't want the grapes to warm up when they're picking them.
[01:50:47.940 --> 01:50:52.340]   They also partner with local artists to make these beautiful labels.
[01:50:52.340 --> 01:50:56.780]   Each bottle, a work of art, each bottle, great wine at a great price.
[01:50:56.780 --> 01:50:57.780]   Shipping is covered.
[01:50:57.780 --> 01:51:01.380]   If you don't like a bottle, if you say, "Ah, yeah, that wasn't so good."
[01:51:01.380 --> 01:51:02.380]   No questions asked.
[01:51:02.380 --> 01:51:03.980]   They'll just replace it with a bottle you love.
[01:51:03.980 --> 01:51:06.060]   You don't just send the other one back.
[01:51:06.060 --> 01:51:10.100]   Sit back, relax, and celebrate with Wink.
[01:51:10.100 --> 01:51:12.860]   Don't forget, you know, I think a lot of us get in wine ruts.
[01:51:12.860 --> 01:51:18.060]   This is a great way to try new wines, find new flavors, and let you're guaranteed the
[01:51:18.060 --> 01:51:19.060]   like.
[01:51:19.060 --> 01:51:21.980]   Why settle for the same bottle of wine you always get?
[01:51:21.980 --> 01:51:28.140]   Discover great wine today at trywink.com/twit and get $20 off your first shipment.
[01:51:28.140 --> 01:51:29.300]   T-R-Y-W-I-N-C.
[01:51:29.300 --> 01:51:35.140]   Get it wine club, w-w-I-N-C.com/twit.
[01:51:35.140 --> 01:51:39.540]   $20 off trywink.com/twit.
[01:51:39.540 --> 01:51:47.140]   Now we need his glasses and a corkscrew, and we can continue on with the show.
[01:51:47.140 --> 01:51:48.140]   What do you think?
[01:51:48.140 --> 01:51:52.260]   It's an interesting point of view.
[01:51:52.260 --> 01:51:56.380]   Do we need the big companies or do they really not help?
[01:51:56.380 --> 01:52:00.780]   Do they get in the way of innovation and tech?
[01:52:00.780 --> 01:52:05.020]   I think that for some of the bigger problems that we solve, like artificial intelligence,
[01:52:05.020 --> 01:52:13.420]   like machine learning, like robotics, I do think that you probably do need the bigger
[01:52:13.420 --> 01:52:18.820]   companies only because you need the money to be able to fund research and development.
[01:52:18.820 --> 01:52:20.260]   You agree, guys?
[01:52:20.260 --> 01:52:27.660]   Well, that's one of the problems for the two big areas of growth that we have for the future,
[01:52:27.660 --> 01:52:31.100]   AI, machine learning, all of this, and VR.
[01:52:31.100 --> 01:52:37.620]   You need to have the data on one hand and the R&D on the other hand, and it's really
[01:52:37.620 --> 01:52:43.900]   difficult for a small company to get in that game, and especially when the big companies
[01:52:43.900 --> 01:52:48.940]   are buying all of the up-and-comers and sort of taking them for themselves.
[01:52:48.940 --> 01:52:55.140]   I'm wondering if, like many others are wondering if we haven't reached a point where they are
[01:52:55.140 --> 01:53:00.380]   a little bit too big.
[01:53:00.380 --> 01:53:05.340]   The thing in tech is that there can always be a newcomer that changes everything, but
[01:53:05.340 --> 01:53:14.500]   the new things are now those areas where the old comers are already snagging everything
[01:53:14.500 --> 01:53:18.180]   there is to snag.
[01:53:18.180 --> 01:53:23.540]   It's the first time in the 20 or 30 years I've liked tech where I'm wondering if there
[01:53:23.540 --> 01:53:29.660]   can be a change or if they're really just those behemoths.
[01:53:29.660 --> 01:53:31.420]   Have they ever been this large?
[01:53:31.420 --> 01:53:34.020]   Has any company ever been this large?
[01:53:34.020 --> 01:53:38.340]   My problem is that I think that they stifle innovation ultimately.
[01:53:38.340 --> 01:53:42.820]   You see things like Google, and I know Microsoft does this too, really trying to preserve that
[01:53:42.820 --> 01:53:46.700]   small company point of view within the big company because they know that's where innovation
[01:53:46.700 --> 01:53:47.700]   happens.
[01:53:47.700 --> 01:53:52.380]   Yes, you need resources, but it's the guy in the garage or gal in the garage, even in
[01:53:52.380 --> 01:53:57.780]   things like robotics and AI that often makes the big breakthrough that then can be leveraged
[01:53:57.780 --> 01:53:58.780]   by a big company.
[01:53:58.780 --> 01:54:02.380]   Again, forgive me for putting on history hat here.
[01:54:02.380 --> 01:54:04.820]   Please do know that's why you're here.
[01:54:04.820 --> 01:54:11.100]   We've never had, and it depends on your definition of what technology industry we're talking
[01:54:11.100 --> 01:54:13.300]   about.
[01:54:13.300 --> 01:54:17.660]   We've never had in previous eras there was always one big company that everyone was afraid
[01:54:17.660 --> 01:54:18.660]   of.
[01:54:18.660 --> 01:54:21.580]   It was IBM for a while, then it was Microsoft for a while.
[01:54:21.580 --> 01:54:26.500]   We've never had in at least computing, and again, this is debatable, going back to the
[01:54:26.500 --> 01:54:30.140]   traders, eight and all the companies that came out of that and stuff like that.
[01:54:30.140 --> 01:54:34.100]   So now you have essentially these big four dominance.
[01:54:34.100 --> 01:54:40.660]   You have an oligopoly as opposed to, if there's only one target, if everyone, I'm sorry,
[01:54:40.660 --> 01:54:45.620]   Kristina, going back to a different era, if everyone hates Microsoft and is gunning for
[01:54:45.620 --> 01:54:52.580]   Microsoft, that's a different ecosystem than there's a club at the top.
[01:54:52.580 --> 01:54:59.220]   It would seem to me that it would be much, much harder to break into a club than to kill
[01:54:59.220 --> 01:55:02.540]   off the giant if there's only one giant.
[01:55:02.540 --> 01:55:08.420]   >> I agree with that 100%, but I would say I think that it is, and you're the history
[01:55:08.420 --> 01:55:10.660]   expert, so I would like your perspective on this.
[01:55:10.660 --> 01:55:14.060]   I don't think it's necessarily fair to say we've never had anything as big as this, because
[01:55:14.060 --> 01:55:16.700]   I do think we've had companies that have been equally as powerful.
[01:55:16.700 --> 01:55:19.580]   I think the difference as you say is now there are multiple, right?
[01:55:19.580 --> 01:55:21.420]   >> Right, that's the point I'm making.
[01:55:21.420 --> 01:55:26.980]   >> That's the point I'm making is that no one will ever be as dominant as IBM was through
[01:55:26.980 --> 01:55:28.580]   the early 80s, right?
[01:55:28.580 --> 01:55:32.460]   In computing, not in consumer electronics or anything like that.
[01:55:32.460 --> 01:55:36.180]   And then a similar argument can be made for Microsoft in software.
[01:55:36.180 --> 01:55:43.740]   But this is my point is that those were always one giant, and so all of the little, David's
[01:55:43.740 --> 01:55:47.220]   were running around, nipping at their heels.
[01:55:47.220 --> 01:55:52.500]   But what happens when there's a club, when there is an oligopoly, and so that even though
[01:55:52.500 --> 01:55:57.860]   Microsoft is competing with Google, with Apple is competing with Google, but at the
[01:55:57.860 --> 01:56:03.820]   same time, it would seem to me that it would be easier to close the gate behind you if
[01:56:03.820 --> 01:56:05.220]   you're in that club.
[01:56:05.220 --> 01:56:09.020]   >> Yeah, who would want to start a search engine these days?
[01:56:09.020 --> 01:56:12.140]   Who would want to write another operating system these days?
[01:56:12.140 --> 01:56:15.340]   Who would want to create a social network these days?
[01:56:15.340 --> 01:56:17.940]   Those guys are dominant, they have a lock.
[01:56:17.940 --> 01:56:22.860]   >> Well, you're right, but at the same time, I mean, who would have predicted that WhatsApp
[01:56:22.860 --> 01:56:26.660]   would become, would sell for 19.2 billion dollars.
[01:56:26.660 --> 01:56:27.660]   >> But this is what we're saying.
[01:56:27.660 --> 01:56:28.660]   >> Well, that's exactly what we're saying.
[01:56:28.660 --> 01:56:31.700]   >> Because as soon as WhatsApp comes around, it's gone.
[01:56:31.700 --> 01:56:33.220]   >> No, you're not wrong.
[01:56:33.220 --> 01:56:35.220]   >> That's the point I don't.
[01:56:35.220 --> 01:56:39.060]   >> Of course, but I do think that's, I mean, but while I'm saying it was I do think that's
[01:56:39.060 --> 01:56:42.340]   proof that you can disrupt whether they get acquired by someone else or not.
[01:56:42.340 --> 01:56:45.260]   And if we want to talk about should companies be split up or not?
[01:56:45.260 --> 01:56:46.260]   I mean, I don't know.
[01:56:46.260 --> 01:56:49.820]   I mean, I think you can make the argument that, you know, AT&T was split up and ultimately
[01:56:49.820 --> 01:56:51.540]   nothing changed and then it just reformed.
[01:56:51.540 --> 01:56:54.060]   >> Yeah, I don't think splitting it up is the right answer.
[01:56:54.060 --> 01:56:58.700]   And I think true technologists believe that they'll be split up by innovation.
[01:56:58.700 --> 01:57:01.940]   >> Yeah, and I think that's probably true.
[01:57:01.940 --> 01:57:07.260]   >> But even the WhatsApp thing, let's say WhatsApp doesn't get acquired.
[01:57:07.260 --> 01:57:13.460]   And I mean, WhatsApp is cute, but it's not disrupting Apple, Amazon and Facebook.
[01:57:13.460 --> 01:57:16.020]   It's somewhat- >> No, in fact, you could say the reason
[01:57:16.020 --> 01:57:18.900]   Facebook acquired them is precisely so it wouldn't.
[01:57:18.900 --> 01:57:20.900]   Same thing with Instagram, right?
[01:57:20.900 --> 01:57:21.900]   >> No, right.
[01:57:21.900 --> 01:57:22.900]   >> But imagine it didn't.
[01:57:22.900 --> 01:57:26.620]   >> And look, look, the company they couldn't get, which was Snap.
[01:57:26.620 --> 01:57:27.620]   >> Right.
[01:57:27.620 --> 01:57:28.620]   >> Yeah.
[01:57:28.620 --> 01:57:29.620]   >> Yeah.
[01:57:29.620 --> 01:57:30.780]   >> So is Snap a threat anymore?
[01:57:30.780 --> 01:57:31.780]   >> No.
[01:57:31.780 --> 01:57:32.780]   >> But I don't think it's ever a threat.
[01:57:32.780 --> 01:57:37.220]   >> The other thing is, the other thing is unlike what's been happening in the past,
[01:57:37.220 --> 01:57:43.260]   30 or 50 years, tech doesn't mean the same thing anymore, right?
[01:57:43.260 --> 01:57:47.020]   As you were saying, Brian, IBM was hardware.
[01:57:47.020 --> 01:57:48.500]   Microsoft was software.
[01:57:48.500 --> 01:57:49.860]   Today, tech is everything.
[01:57:49.860 --> 01:57:50.860]   >> Yeah.
[01:57:50.860 --> 01:57:51.860]   >> It's cars, it's social networks, right?
[01:57:51.860 --> 01:57:52.860]   >> Yeah, it does.
[01:57:52.860 --> 01:57:55.540]   Look at what Apple does, look at even what Facebook does.
[01:57:55.540 --> 01:57:59.660]   And they're, you know, it's in every single thing.
[01:57:59.660 --> 01:58:04.340]   Why were publishers pissed off in Europe and actually in the US, I'm sure as well?
[01:58:04.340 --> 01:58:09.700]   Why are taxi drivers pissed off because of, et cetera, et cetera?
[01:58:09.700 --> 01:58:12.380]   I mean, tech is not- >> Well, well, well, well, well, who wears a great example, though?
[01:58:12.380 --> 01:58:14.980]   Like, because if you think about it, I mean, granted, we don't know what the final story
[01:58:14.980 --> 01:58:20.960]   on Uber's going to be, but Uber has disrupted significant industries.
[01:58:20.960 --> 01:58:26.060]   And it has done it, you know, and like, I mean, it was able to get significant funding.
[01:58:26.060 --> 01:58:27.200]   Now, I think that's the key, right?
[01:58:27.200 --> 01:58:30.180]   Is that it got significant institutional funding.
[01:58:30.180 --> 01:58:31.900]   I agree with what you're all saying.
[01:58:31.900 --> 01:58:35.820]   But I feel like I'm not going to say that I don't think there's any way that others can
[01:58:35.820 --> 01:58:36.820]   innovate.
[01:58:36.820 --> 01:58:38.540]   Do I think it's extremely difficult without a doubt?
[01:58:38.540 --> 01:58:41.220]   The bar is so much higher than it's ever been.
[01:58:41.220 --> 01:58:46.620]   But if you have a, but the right, you know, stars aligning and with the right institutional
[01:58:46.620 --> 01:58:51.980]   funding, I do feel like there would be the potential for someone to be what Google was,
[01:58:51.980 --> 01:58:52.980]   you know.
[01:58:52.980 --> 01:58:53.980]   >> There's 100%.
[01:58:53.980 --> 01:58:54.980]   >> Thanks for your second.
[01:58:54.980 --> 01:58:57.620]   >> There always will be the potential to that.
[01:58:57.620 --> 01:59:02.860]   I guess the argument that I'm making by this idea of a club in an oligopoly is like it's
[01:59:02.860 --> 01:59:04.660]   in the rundown.
[01:59:04.660 --> 01:59:07.260]   Apple and Samsung are having their trial, right?
[01:59:07.260 --> 01:59:08.260]   >> Right.
[01:59:08.260 --> 01:59:10.260]   Monday, we're going to hear the results I think.
[01:59:10.260 --> 01:59:11.260]   >> Exactly.
[01:59:11.260 --> 01:59:13.300]   But they don't stop doing business with each other.
[01:59:13.300 --> 01:59:19.260]   Meanwhile, in the old olden timey days, you had a company like Netscape come around and
[01:59:19.260 --> 01:59:24.180]   they wanted to give Microsoft the middle finger very clearly because they, you know what
[01:59:24.180 --> 01:59:25.180]   I mean?
[01:59:25.180 --> 01:59:26.180]   So if you're in a club.
[01:59:26.180 --> 01:59:27.180]   What happened in Netscape?
[01:59:27.180 --> 01:59:28.180]   >> Well, that's true.
[01:59:28.180 --> 01:59:34.220]   >> I have a vivid memory of this because when Internet Explorer 3 came out, this was in
[01:59:34.220 --> 01:59:42.420]   1990, I want to say 4 or 5, I went on TV on NBC and said, you might as well close the
[01:59:42.420 --> 01:59:47.220]   doors at Netscape because Microsoft's giving away a good browser.
[01:59:47.220 --> 01:59:48.220]   It's all over for Netscape.
[01:59:48.220 --> 01:59:49.220]   >> Well, it was better.
[01:59:49.220 --> 01:59:50.220]   I mean, it's better.
[01:59:50.220 --> 01:59:51.220]   >> It's better.
[01:59:51.220 --> 01:59:52.220]   >> It's not just good, better.
[01:59:52.220 --> 01:59:56.500]   Somebody told me later that Mark and Driesin had the TV on and he heard me say this and
[01:59:56.500 --> 01:59:59.380]   he went screaming down the, who is this ass?
[01:59:59.380 --> 02:00:03.380]   You know, but I was, I think I was right.
[02:00:03.380 --> 02:00:04.380]   >> You're right.
[02:00:04.380 --> 02:00:06.300]   David doesn't always kill Goliath.
[02:00:06.300 --> 02:00:10.780]   But what I'm saying is, is it much harder if there's six Goliath's and they're in a club
[02:00:10.780 --> 02:00:13.660]   and they'll have their disagreements and they'll sue each other.
[02:00:13.660 --> 02:00:14.860]   But then they'll also do business.
[02:00:14.860 --> 02:00:16.660]   >> But there's a reason why David making the screen.
[02:00:16.660 --> 02:00:20.820]   >> David and Goliath is a story we remember because it's the exception, right?
[02:00:20.820 --> 02:00:21.820]   >> Well, I understand.
[02:00:21.820 --> 02:00:24.860]   >> I'm going to borrow your, Brian, let me borrow your history hat for a moment.
[02:00:24.860 --> 02:00:25.860]   >> Sure, please.
[02:00:25.860 --> 02:00:29.180]   >> Because here's an article from Dig that I love.
[02:00:29.180 --> 02:00:32.300]   This is a great article, a little history.
[02:00:32.300 --> 02:00:36.140]   The largest company of all time is not any of the companies you might think of it was
[02:00:36.140 --> 02:00:43.380]   the 1637, 1637, the Dutch East India Company.
[02:00:43.380 --> 02:00:47.380]   If you adjust for inflation was worth $7.9 trillion.
[02:00:47.380 --> 02:00:53.380]   We're talking about how Apple might hit a trillion later this year.
[02:00:53.380 --> 02:01:00.420]   These are the 20 tech companies that combine to make the one Dutch East India Company.
[02:01:00.420 --> 02:01:02.060]   So now that's an exception.
[02:01:02.060 --> 02:01:08.180]   Obviously that was a blip and it wasn't worth $7.9 million, a bit trillion a few years later.
[02:01:08.180 --> 02:01:12.740]   >> But I think that does kind of point out though that historically we have had these
[02:01:12.740 --> 02:01:14.020]   huge conglomerates.
[02:01:14.020 --> 02:01:18.660]   We even had, if you look at, obviously we view things, I think most of us throw it through
[02:01:18.660 --> 02:01:19.660]   a Western lens.
[02:01:19.660 --> 02:01:24.660]   I mean, three of us on this panel are from the United States and we also have our French
[02:01:24.660 --> 02:01:25.660]   compatriot.
[02:01:25.660 --> 02:01:30.660]   But if you look at Korea for instance, there's Shaibal system where Samsung, I think it's
[02:01:30.660 --> 02:01:31.660]   an important to know.
[02:01:31.660 --> 02:01:33.260]   >> 30% of the GNP.
[02:01:33.260 --> 02:01:39.260]   >> Yes, but Samsung is not just the electronics company there.
[02:01:39.260 --> 02:01:43.260]   They run the hospitals, they run the insurance companies, they run the manufacturing, they
[02:01:43.260 --> 02:01:45.580]   make, they're involved in clothing.
[02:01:45.580 --> 02:01:48.340]   I mean, they're literally involved in every single industry.
[02:01:48.340 --> 02:01:52.820]   It is not just Samsung electronics is the most well known and most profitable, but it
[02:01:52.820 --> 02:01:55.380]   is far from the only part of that business.
[02:01:55.380 --> 02:02:01.860]   And that was the same thing with the West Indies trade company.
[02:02:01.860 --> 02:02:04.620]   It was this huge touch into everything.
[02:02:04.620 --> 02:02:08.180]   And I think that's kind of what was going to be in said earlier is that tech is no longer
[02:02:08.180 --> 02:02:09.180]   just one thing.
[02:02:09.180 --> 02:02:15.700]   It kind of goes into everything and by extension, it means that if you do have a club, it is
[02:02:15.700 --> 02:02:18.020]   going to be harder for anybody else to get in.
[02:02:18.020 --> 02:02:23.380]   >> My guess my question was really about what we should cover is, are we now just covering
[02:02:23.380 --> 02:02:25.500]   business because we're covering big companies?
[02:02:25.500 --> 02:02:27.980]   Just they happen to be in kind of in the business of tech.
[02:02:27.980 --> 02:02:28.980]   >> Have a little one in the case though?
[02:02:28.980 --> 02:02:34.300]   >> No, no, no, because look at the top by market cap companies in the world right now.
[02:02:34.300 --> 02:02:36.660]   And what is it like seven of the 10 are all tech?
[02:02:36.660 --> 02:02:38.820]   And look at even five years ago, that wasn't the case.
[02:02:38.820 --> 02:02:43.180]   But I could tell you when tech TV started, there was a lot of little stuff to cover, a
[02:02:43.180 --> 02:02:45.700]   lot of interesting innovations to cover.
[02:02:45.700 --> 02:02:49.940]   It's only kind of lately that it's become these dominant giant companies.
[02:02:49.940 --> 02:02:52.980]   I find that less interesting for, I mean, it's interesting in other ways.
[02:02:52.980 --> 02:02:56.020]   There's the philosophy that we were talking about at the beginning of the show.
[02:02:56.020 --> 02:03:01.060]   >> Well, I mean, I agree with you, but I think the main part of it is that, and so
[02:03:01.060 --> 02:03:04.980]   we kind of maybe reach this next big technological breakthrough, we're trying to solve really
[02:03:04.980 --> 02:03:05.980]   big problems.
[02:03:05.980 --> 02:03:11.940]   So whether it's one big company or six, you have to be big to be able to solve those problems.
[02:03:11.940 --> 02:03:17.620]   And potentially once we reach that next breakthrough, maybe then it'll be the next time for the
[02:03:17.620 --> 02:03:20.580]   upstarts, the startups, the smaller places to come in and innovate.
[02:03:20.580 --> 02:03:25.340]   But it's almost like the big challenges that we have to achieve now require, for better
[02:03:25.340 --> 02:03:29.500]   or worse, a tremendous amount of money, a tremendous amount of talents.
[02:03:29.500 --> 02:03:35.700]   And you just don't get that unless you're gigantic, unfortunately.
[02:03:35.700 --> 02:03:38.020]   >> You guys know what a stingray is.
[02:03:38.020 --> 02:03:39.260]   I'm not talking about a fish.
[02:03:39.260 --> 02:03:45.820]   I'm talking about a device that impersonates a cell phone tower, captures your cell phone
[02:03:45.820 --> 02:03:46.820]   traffic.
[02:03:46.820 --> 02:03:49.620]   And as a result can intercept your calls.
[02:03:49.620 --> 02:03:58.260]   The Washington DC, NBC affiliate channel four drove around the DC area with a mobile
[02:03:58.260 --> 02:04:02.140]   security expert and some hardware and software.
[02:04:02.140 --> 02:04:11.420]   They were able to detect in the DC area 40 stingray devices that were being used presumably
[02:04:11.420 --> 02:04:17.460]   to intercept calls by members of Congress, Department of Homeland Security, President
[02:04:17.460 --> 02:04:18.460]   Trump.
[02:04:18.460 --> 02:04:25.500]   He said, we got picked up twice while driving along K Street, the lobbyist's corridor.
[02:04:25.500 --> 02:04:26.500]   And then they drop them.
[02:04:26.500 --> 02:04:31.820]   So you could actually see the stingray pick up the call, examine the traffic and then
[02:04:31.820 --> 02:04:34.460]   pass it off and say, you're not interesting.
[02:04:34.460 --> 02:04:40.300]   There's nothing to say about this except, wow, 40.
[02:04:40.300 --> 02:04:43.260]   >> The dark night was right, right?
[02:04:43.260 --> 02:04:44.340]   >> Yeah.
[02:04:44.340 --> 02:04:47.660]   Don't drive around Washington DC if you want to be private.
[02:04:47.660 --> 02:04:50.260]   Well, but are those legal?
[02:04:50.260 --> 02:04:53.700]   Are those like, well, they're legal if law enforcement uses them.
[02:04:53.700 --> 02:04:55.500]   I don't imagine it's law enforcement.
[02:04:55.500 --> 02:04:57.220]   >> We don't know.
[02:04:57.220 --> 02:05:00.940]   I mean, maybe they're all being used by law enforcement.
[02:05:00.940 --> 02:05:03.500]   >> But maybe they're being used by the embassies.
[02:05:03.500 --> 02:05:07.060]   >> You go down to embassy row, there's a ton of them.
[02:05:07.060 --> 02:05:18.060]   Just think it's, by the way, the longest connection that the channel four team had was next to
[02:05:18.060 --> 02:05:23.700]   the Russian embassy, the phones appeared remain connected to a fake tower, the longest.
[02:05:23.700 --> 02:05:30.500]   Oh, and the Chinese and Israeli embassies and then the Romanian and Turkish embassies.
[02:05:30.500 --> 02:05:34.300]   Apparently, they're all running stingrays.
[02:05:34.300 --> 02:05:38.140]   And by the way, here's the quote from the security expert.
[02:05:38.140 --> 02:05:44.380]   You know, governments do this to each other all the time and laws schmoz.
[02:05:44.380 --> 02:05:45.860]   Laws schmoz.
[02:05:45.860 --> 02:05:46.860]   Take it.
[02:05:46.860 --> 02:05:48.740]   We'll take a quick break.
[02:05:48.740 --> 02:05:50.140]   We got to wrap it up.
[02:05:50.140 --> 02:05:51.420]   This has been going on for a long time.
[02:05:51.420 --> 02:05:54.220]   We got to go, what is it, the Billboard Music Awards or something?
[02:05:54.220 --> 02:05:56.300]   Lisa said, come home early.
[02:05:56.300 --> 02:05:59.060]   We got to show to watch.
[02:05:59.060 --> 02:06:04.300]   If you get a chance to go see Pink, I don't know if she's coming up your way, Christina,
[02:06:04.300 --> 02:06:05.300]   do.
[02:06:05.300 --> 02:06:06.300]   >> Okay.
[02:06:06.300 --> 02:06:07.300]   >> I saw it.
[02:06:07.300 --> 02:06:08.300]   >> Is she good?
[02:06:08.300 --> 02:06:09.300]   It was a good Friday night.
[02:06:09.300 --> 02:06:10.300]   We saw her.
[02:06:10.300 --> 02:06:11.300]   It was amazing.
[02:06:11.300 --> 02:06:12.300]   Great show.
[02:06:12.300 --> 02:06:13.300]   She flies through.
[02:06:13.300 --> 02:06:15.500]   At the end, the best grand finale I've ever seen in a concert.
[02:06:15.500 --> 02:06:19.060]   They put her in a harness and she flies all over the stadium, sings to every part of it.
[02:06:19.060 --> 02:06:20.060]   She goes up to the balcony.
[02:06:20.060 --> 02:06:22.060]   She's just flying around, singing.
[02:06:22.060 --> 02:06:24.100]   >> Well, she's awesome.
[02:06:24.100 --> 02:06:25.420]   >> No, she's great.
[02:06:25.420 --> 02:06:28.420]   No, I always love her aerobatic stuff.
[02:06:28.420 --> 02:06:29.420]   >> Oh, man.
[02:06:29.420 --> 02:06:30.420]   I'm teen Taylor.
[02:06:30.420 --> 02:06:31.420]   I've never seen.
[02:06:31.420 --> 02:06:32.420]   >> You're seeing Tay Tay?
[02:06:32.420 --> 02:06:33.420]   >> You're going to the foundation tour?
[02:06:33.420 --> 02:06:34.420]   >> I'm seeing her on Tuesday.
[02:06:34.420 --> 02:06:35.420]   I'm going on Tuesday.
[02:06:35.420 --> 02:06:36.420]   We got floor seats.
[02:06:36.420 --> 02:06:38.740]   >> Whoa, you got to tell us all about it.
[02:06:38.740 --> 02:06:39.740]   >> I'm very excited.
[02:06:39.740 --> 02:06:40.740]   >> Oh, I'm sorry.
[02:06:40.740 --> 02:06:43.100]   I know what I have to go home for.
[02:06:43.100 --> 02:06:44.100]   Basketball.
[02:06:44.100 --> 02:06:46.860]   Warriors are playing the night.
[02:06:46.860 --> 02:06:50.300]   Our show today brought to you by Rocket Mortgage, actually.
[02:06:50.300 --> 02:06:54.620]   Quick and loans, the creators of Rocket Mortgage.
[02:06:54.620 --> 02:06:57.340]   Isn't it the Quick and Loans Arena in Cleveland?
[02:06:57.340 --> 02:07:00.620]   And Dan Harmon, of course, the founder of Quick and Loans has done amazing things to
[02:07:00.620 --> 02:07:02.140]   revitalize downtown Detroit.
[02:07:02.140 --> 02:07:03.700]   It's a really cool company.
[02:07:03.700 --> 02:07:06.900]   The number one lender in the country, number one in customer satisfaction, eight years
[02:07:06.900 --> 02:07:09.900]   in a row according to JD Power.
[02:07:09.900 --> 02:07:13.420]   And they realized that geeks weren't being well served by the mortgage system.
[02:07:13.420 --> 02:07:14.860]   You have to go to a bank.
[02:07:14.860 --> 02:07:15.860]   You go, please.
[02:07:15.860 --> 02:07:18.180]   You have some money, put on a tie or a dress.
[02:07:18.180 --> 02:07:19.660]   It's just no fun.
[02:07:19.660 --> 02:07:22.660]   Then you have to go to the attic and find pay stubs and bank statements.
[02:07:22.660 --> 02:07:24.260]   And it takes a long time.
[02:07:24.260 --> 02:07:25.260]   Weeks.
[02:07:25.260 --> 02:07:29.820]   The last house Lisa and I bought when we bought our house three or four years ago took two
[02:07:29.820 --> 02:07:31.860]   months to get a loan.
[02:07:31.860 --> 02:07:33.700]   We almost lost the house.
[02:07:33.700 --> 02:07:39.620]   Fast forward to the 21st century and Rocket Mortgage from Quick and Loans, an entirely online
[02:07:39.620 --> 02:07:41.580]   loan approval process.
[02:07:41.580 --> 02:07:42.940]   You could do the whole thing on your phone.
[02:07:42.940 --> 02:07:43.940]   You could do it so fast.
[02:07:43.940 --> 02:07:47.500]   Do it in an open house and say we're approved before you leave.
[02:07:47.500 --> 02:07:48.860]   That's how easy it is.
[02:07:48.860 --> 02:07:50.300]   Go right now actually.
[02:07:50.300 --> 02:07:56.100]   Set up the account, rocketmortgage.com/twit to twit in the number two.
[02:07:56.100 --> 02:07:57.100]   Set up an account.
[02:07:57.100 --> 02:08:00.140]   You don't have to go to the attic to find anything.
[02:08:00.140 --> 02:08:04.580]   You just answer a few questions, things you already know, birthday to dress, things like
[02:08:04.580 --> 02:08:05.580]   that.
[02:08:05.580 --> 02:08:09.340]   You give them permission because they have trusted relationships with all the financial
[02:08:09.340 --> 02:08:10.340]   institutions.
[02:08:10.340 --> 02:08:13.860]   You go give them permission to go get out, get the financial information they need.
[02:08:13.860 --> 02:08:18.260]   When they crunch the numbers based on income assets and credit, in most cases within 10
[02:08:18.260 --> 02:08:21.100]   minutes, you'll get loan approval.
[02:08:21.100 --> 02:08:23.660]   They can analyze all the home loan options for which you qualify.
[02:08:23.660 --> 02:08:28.420]   You choose the term, the rate, the down payment, very good rates.
[02:08:28.420 --> 02:08:30.220]   You'll find the one that's just right for you.
[02:08:30.220 --> 02:08:31.700]   Rocket mortgage by Quick and Loans.
[02:08:31.700 --> 02:08:33.180]   You'll apply simply.
[02:08:33.180 --> 02:08:38.140]   You'll understand fully your mortgage confidently and best of all, it takes no time at all and
[02:08:38.140 --> 02:08:39.820]   no rummaging.
[02:08:39.820 --> 02:08:41.300]   No rummaging.
[02:08:41.300 --> 02:08:45.860]   To get started, go to rocketmortgage.com/twit and the number two equal housing lender licensed
[02:08:45.860 --> 02:08:49.940]   in all 50 states and MLS consumer access.org number 30-30.
[02:08:49.940 --> 02:08:51.780]   I want to make that there's slogan.
[02:08:51.780 --> 02:08:54.300]   No rummaging necessary.
[02:08:54.300 --> 02:08:59.580]   Rocketmortgage.com/twit2.
[02:08:59.580 --> 02:09:04.580]   Christina Warren was rummaging around for the type C cable.
[02:09:04.580 --> 02:09:05.580]   Destroyed her office.
[02:09:05.580 --> 02:09:06.780]   Rummaging can be very bad.
[02:09:06.780 --> 02:09:07.780]   I'm glad you...
[02:09:07.780 --> 02:09:08.780]   We should see it.
[02:09:08.780 --> 02:09:09.780]   This is Astrazone.
[02:09:09.780 --> 02:09:17.860]   I had to go to my work office because my home office is like, yeah.
[02:09:17.860 --> 02:09:21.100]   I imagine you throwing things in the air.
[02:09:21.100 --> 02:09:22.100]   Yes.
[02:09:22.100 --> 02:09:23.940]   Going through boxes and trying to find it.
[02:09:23.940 --> 02:09:28.220]   Then after it arrived, I finally found where it was.
[02:09:28.220 --> 02:09:32.820]   It was in a suitcase in place and I was like, "Oh, of course, of course."
[02:09:32.820 --> 02:09:35.420]   We talked on security now about e-fail.
[02:09:35.420 --> 02:09:41.460]   A flaw, not in PGP and S-MIME, but it has that impact of breaking PGP and S-MIME.
[02:09:41.460 --> 02:09:46.940]   If you're using a male client that supports HTML-male, patches are being worked on as
[02:09:46.940 --> 02:09:47.940]   we speak.
[02:09:47.940 --> 02:09:50.020]   This is a big deal.
[02:09:50.020 --> 02:09:53.100]   Actually the problem e-fail was found some months ago.
[02:09:53.100 --> 02:09:57.580]   I know because I got an e-mail from Mailmate, which is the Mac program I use.
[02:09:57.580 --> 02:10:00.740]   He said, "I couldn't say anything about it, but the last update, the update we put out
[02:10:00.740 --> 02:10:04.300]   in March specifically addressed this and we fixed it.
[02:10:04.300 --> 02:10:09.460]   But at the time, they had told us ahead of time, we patched as fast as we could.
[02:10:09.460 --> 02:10:14.140]   I didn't want to say anything because I didn't want to tell anybody else that this thing existed.
[02:10:14.140 --> 02:10:15.500]   Check with your e-mail client.
[02:10:15.500 --> 02:10:16.500]   Make sure it's up to date.
[02:10:16.500 --> 02:10:25.900]   If you want to use PGP or S-MIME for security or signing, you should.
[02:10:25.900 --> 02:10:27.860]   Do you care about Yanny and Laurel?
[02:10:27.860 --> 02:10:28.860]   Do you care?
[02:10:28.860 --> 02:10:31.660]   I was wondering if you were going to do this or anything.
[02:10:31.660 --> 02:10:33.820]   I did not want to do it, I honestly.
[02:10:33.820 --> 02:10:38.620]   It's the audio version of the dress and we all lost interest twice as fast.
[02:10:38.620 --> 02:10:39.620]   Yeah.
[02:10:39.620 --> 02:10:40.620]   Isn't that funny?
[02:10:40.620 --> 02:10:41.620]   Created by high school students.
[02:10:41.620 --> 02:10:42.620]   Well, wait.
[02:10:42.620 --> 02:10:43.620]   Let me, okay.
[02:10:43.620 --> 02:10:48.960]   I don't know if anyone knows this, but so there are two competing high schools or high
[02:10:48.960 --> 02:10:50.620]   school student groups.
[02:10:50.620 --> 02:10:54.900]   So, the New York Times has an article that says it's these one teenagers.
[02:10:54.900 --> 02:10:57.580]   They were in a class, what is it?
[02:10:57.580 --> 02:11:00.660]   Vocabulary.com and then it went to Reddit, then it went to YouTube.
[02:11:00.660 --> 02:11:03.540]   But then Wired's piece were different teenagers.
[02:11:03.540 --> 02:11:05.300]   It was the exact same story.
[02:11:05.300 --> 02:11:06.300]   What?
[02:11:06.300 --> 02:11:08.140]   I don't know if that has ever been solved.
[02:11:08.140 --> 02:11:09.140]   Right.
[02:11:09.140 --> 02:11:10.780]   That is the part of the story that I can't.
[02:11:10.780 --> 02:11:12.300]   Okay, I was going to say, I'm into that.
[02:11:12.300 --> 02:11:13.300]   That I care about.
[02:11:13.300 --> 02:11:14.300]   I don't care about the other stuff.
[02:11:14.300 --> 02:11:15.300]   That I care about.
[02:11:15.300 --> 02:11:18.620]   I want the drama who's actually responsible and who's the imposter.
[02:11:18.620 --> 02:11:19.620]   That's the real story.
[02:11:19.620 --> 02:11:23.900]   Right, because Wired's story is the true story of Yanny versus Laurel and the New York
[02:11:23.900 --> 02:11:26.620]   Times was, but it's different people.
[02:11:26.620 --> 02:11:32.780]   So I want to find out in the end who's the true origin story here.
[02:11:32.780 --> 02:11:36.660]   Do you care which one Yanny hears?
[02:11:36.660 --> 02:11:37.660]   No.
[02:11:37.660 --> 02:11:41.700]   It's all about your ears and what register.
[02:11:41.700 --> 02:11:43.860]   Here's Yanny and his Yanny.
[02:11:43.860 --> 02:11:44.860]   Here's Yanny.
[02:11:44.860 --> 02:11:51.180]   Of course, here's Yanny.
[02:11:51.180 --> 02:11:52.180]   I should play it.
[02:11:52.180 --> 02:11:53.580]   Let me just play it.
[02:11:53.580 --> 02:11:54.580]   What?
[02:11:54.580 --> 02:11:55.580]   No?
[02:11:55.580 --> 02:11:56.580]   Laurel.
[02:11:56.580 --> 02:11:57.580]   Oh, I heard.
[02:11:57.580 --> 02:12:01.580]   Oh, I just heard both.
[02:12:01.580 --> 02:12:02.580]   Laurel.
[02:12:02.580 --> 02:12:06.500]   I was going to say, I heard Yanny this entire time and now I hear Laurel.
[02:12:06.500 --> 02:12:07.500]   So yeah, me too.
[02:12:07.500 --> 02:12:08.500]   Me too.
[02:12:08.500 --> 02:12:12.860]   I think it's probably the Skype algorithm that the store said enough that we're hearing
[02:12:12.860 --> 02:12:13.860]   about.
[02:12:13.860 --> 02:12:17.900]   When I did the episode, I recorded that exact clip and all the time in my headphones, I
[02:12:17.900 --> 02:12:19.300]   heard Laurel, Laurel, Laurel.
[02:12:19.300 --> 02:12:24.780]   When I listened to it back, the episode on my AirPods, I heard Yanny for the first time.
[02:12:24.780 --> 02:12:25.780]   So we did an experiment.
[02:12:25.780 --> 02:12:32.460]   I think this actually was why I thought this is dumb because Yanny is recorded at the
[02:12:32.460 --> 02:12:36.740]   higher pitch and Laurel's at the lower pitch and it depended on the speaker you played
[02:12:36.740 --> 02:12:37.740]   it through.
[02:12:37.740 --> 02:12:39.540]   So when you played it through Tini speakers, you heard Yanny.
[02:12:39.540 --> 02:12:42.140]   When you played it through Basie speakers, you heard Laurel.
[02:12:42.140 --> 02:12:43.140]   Yeah.
[02:12:43.140 --> 02:12:46.820]   Now that I think about it, I think I'd only heard it before, like actually on the audio
[02:12:46.820 --> 02:12:48.220]   on my phone, like that phone speaker.
[02:12:48.220 --> 02:12:50.140]   So you're going to hear Yanny.
[02:12:50.140 --> 02:12:52.540]   And now I've got like the, you know, the headphones.
[02:12:52.540 --> 02:12:53.540]   Exactly.
[02:12:53.540 --> 02:12:57.140]   Like my standard Sony, you know, headphones that are good.
[02:12:57.140 --> 02:12:58.140]   Yup.
[02:12:58.140 --> 02:12:59.140]   Laurel.
[02:12:59.140 --> 02:13:00.140]   Wow.
[02:13:00.140 --> 02:13:03.300]   The chat room's saying I caved.
[02:13:03.300 --> 02:13:07.340]   I wasn't going to do it.
[02:13:07.340 --> 02:13:08.340]   Honest.
[02:13:08.340 --> 02:13:12.740]   Anybody excited about the red phone, the hydrogen one, they've started to make some
[02:13:12.740 --> 02:13:13.740]   announcements.
[02:13:13.740 --> 02:13:14.740]   They're showing it to people.
[02:13:14.740 --> 02:13:17.700]   We're going to, I think we're going to get one up here for a hands on.
[02:13:17.700 --> 02:13:18.700]   It's expensive.
[02:13:18.700 --> 02:13:19.700]   I think $1,200.
[02:13:19.700 --> 02:13:25.700]   But the key is these pogo pins you see at the bottom red, which of course is best known
[02:13:25.700 --> 02:13:29.260]   for its amazing five, four and five K cameras.
[02:13:29.260 --> 02:13:33.580]   They're going to make cameras and other accessories for this phone.
[02:13:33.580 --> 02:13:37.180]   I'm not so excited about the holographic display.
[02:13:37.180 --> 02:13:39.100]   Yeah, that I don't care about.
[02:13:39.100 --> 02:13:43.460]   But if you could have a really good accessory ecosystem for that, where you could, especially
[02:13:43.460 --> 02:13:46.300]   if there's like an adapter so it could work with maybe some of the existing red stuff
[02:13:46.300 --> 02:13:48.900]   or the lenses anyway, like that would be amazing.
[02:13:48.900 --> 02:13:55.180]   Well, I mean, I'm guessing the target market for this is very limited.
[02:13:55.180 --> 02:13:57.860]   But what's the holographic screen?
[02:13:57.860 --> 02:14:03.900]   And I think it's the camera on a particular 3D, it'd be my guess, you know, where it's
[02:14:03.900 --> 02:14:07.260]   like those dopey, you know, it's like the fire phone.
[02:14:07.260 --> 02:14:08.260]   It's like the fire phone.
[02:14:08.260 --> 02:14:09.860]   In fact, I think it's exactly it.
[02:14:09.860 --> 02:14:10.860]   But who knows?
[02:14:10.860 --> 02:14:11.860]   I don't know.
[02:14:11.860 --> 02:14:12.860]   I haven't seen it yet.
[02:14:12.860 --> 02:14:13.860]   It is.
[02:14:13.860 --> 02:14:17.460]   You can get, if you're really crazy, a titanium model for 1600 bucks.
[02:14:17.460 --> 02:14:18.460]   Okay.
[02:14:18.460 --> 02:14:19.460]   Yeah.
[02:14:19.460 --> 02:14:20.460]   I don't know.
[02:14:20.460 --> 02:14:21.460]   Yeah, I think it's not going to be the thing for everyone.
[02:14:21.460 --> 02:14:23.260]   I mean, I guess you're going to have it on a rig all the time.
[02:14:23.260 --> 02:14:27.580]   If you're going to have it connected, you know, to a bunch of heavy equipment and other expensive
[02:14:27.580 --> 02:14:29.420]   things, maybe I don't know.
[02:14:29.420 --> 02:14:32.620]   Here's the thing that might be a little disappointing heavy equipment.
[02:14:32.620 --> 02:14:33.620]   Here's the thing.
[02:14:33.620 --> 02:14:34.620]   Why not?
[02:14:34.620 --> 02:14:35.620]   Yeah.
[02:14:35.620 --> 02:14:38.420]   Here's the thing that might be a little disappointing to people is that while some really interesting
[02:14:38.420 --> 02:14:45.740]   foes like the honor and the Huawei are having trouble getting US carriers, both Verizon
[02:14:45.740 --> 02:14:48.380]   and AT&T are carrying this phone.
[02:14:48.380 --> 02:14:49.380]   Seriously.
[02:14:49.380 --> 02:14:50.380]   Yeah.
[02:14:50.380 --> 02:14:52.820]   I mean, I want to try the P9.
[02:14:52.820 --> 02:14:57.060]   I want to, you know, I mean, it's, I feel like, come on, guys.
[02:14:57.060 --> 02:15:01.020]   Uh, the Mickey Mouse copyright law.
[02:15:01.020 --> 02:15:02.500]   Yeah.
[02:15:02.500 --> 02:15:03.980]   It comes up every few years.
[02:15:03.980 --> 02:15:06.980]   Whatever Mickey Mouse's copyright is about to lapse.
[02:15:06.980 --> 02:15:07.980]   Yeah.
[02:15:07.980 --> 02:15:09.060]   Disney is like, Nope.
[02:15:09.060 --> 02:15:11.180]   Not going to happen.
[02:15:11.180 --> 02:15:12.620]   Every year it gets extended.
[02:15:12.620 --> 02:15:20.220]   The US Congress is now considering extending copyright to 144 years.
[02:15:20.220 --> 02:15:21.220]   Thanks.
[02:15:21.220 --> 02:15:23.300]   So where would that bring us?
[02:15:23.300 --> 02:15:24.300]   I don't know.
[02:15:24.300 --> 02:15:26.300]   Mickey Mouse came out in what?
[02:15:26.300 --> 02:15:27.300]   1927?
[02:15:27.300 --> 02:15:28.300]   I go back to that chart.
[02:15:28.300 --> 02:15:30.340]   Go back to that chart, Leo, cause it, all right.
[02:15:30.340 --> 02:15:31.980]   So it's got to come up in 2023.
[02:15:31.980 --> 02:15:35.780]   So that's, well, we'll have to pass this bill in the next few years.
[02:15:35.780 --> 02:15:37.700]   I can't do the math that quickly.
[02:15:37.700 --> 02:15:38.700]   Okay.
[02:15:38.700 --> 02:15:41.380]   Uh, I think Mickey Mouse was 1923.
[02:15:41.380 --> 02:15:44.100]   They've been extending it ever since 1923.
[02:15:44.100 --> 02:15:49.700]   Originally, I think it was the life of the, uh, creator, then it became the life plus
[02:15:49.700 --> 02:15:57.940]   10 that became, uh, as long as Disney wants to do it, it was called no accident.
[02:15:57.940 --> 02:15:59.940]   The sunny Bono Copyright Act.
[02:15:59.940 --> 02:16:00.940]   Yes.
[02:16:00.940 --> 02:16:04.020]   No, cause that, I mean, that was his whole, I mean, cause he was very much in favor of
[02:16:04.020 --> 02:16:07.940]   that because he'd wanted all of his things protected when he was a congressman.
[02:16:07.940 --> 02:16:12.660]   So it was, uh, in the, uh, the founders during the, uh, in the early days of 1790
[02:16:12.660 --> 02:16:16.420]   Copyright Act, it was 27 years.
[02:16:16.420 --> 02:16:21.340]   And for a long time, it was 40 some years when Mickey Mouse was created in 1928.
[02:16:21.340 --> 02:16:24.620]   At that time, it was 55 years.
[02:16:24.620 --> 02:16:27.500]   Then in 79, it was expanded 98.
[02:16:27.500 --> 02:16:29.780]   It was expanded 2023.
[02:16:29.780 --> 02:16:32.700]   And so that's when we're going to have to come on guys.
[02:16:32.700 --> 02:16:37.220]   And I guess it was, I mean, they must have named it after Sony Bono died.
[02:16:37.220 --> 02:16:41.300]   Oh, maybe, but I, I, I, I, so I think that's, it was named in like his honor, but I think
[02:16:41.300 --> 02:16:43.940]   he'd fought to spend those things.
[02:16:43.940 --> 02:16:48.140]   So I think it was after he, uh, he was in his accidents that I think that's why it was
[02:16:48.140 --> 02:16:49.140]   named after him.
[02:16:49.140 --> 02:16:53.380]   Does anybody care that the Senate voted to extend net neutrality?
[02:16:53.380 --> 02:16:57.580]   It has no, no care when it passes the house.
[02:16:57.580 --> 02:16:58.580]   Yeah.
[02:16:58.580 --> 02:17:02.460]   And even if it passed the house, it's got to be signed on the law by the president who's
[02:17:02.460 --> 02:17:03.460]   surely.
[02:17:03.460 --> 02:17:05.860]   I think if it passed the house, he would sign because I don't think he cares.
[02:17:05.860 --> 02:17:06.860]   He doesn't care.
[02:17:06.860 --> 02:17:09.380]   I think that I think if he could be convinced that it would be a win for him, he would
[02:17:09.380 --> 02:17:12.940]   sign it like I, but, but I think I don't think it's going to pass the house, but I do think
[02:17:12.940 --> 02:17:16.300]   that if it passed the house, he would absolutely sign it and then try to make it look like
[02:17:16.300 --> 02:17:20.420]   he's done something that I want to give you all homework.
[02:17:20.420 --> 02:17:22.780]   Call your member of Congress.
[02:17:22.780 --> 02:17:23.780]   I will.
[02:17:23.780 --> 02:17:24.780]   I promise.
[02:17:24.780 --> 02:17:25.780]   Oh, wait.
[02:17:25.780 --> 02:17:29.700]   Oh, Patrick, my, my state has already passed its own net neutrality thing.
[02:17:29.700 --> 02:17:30.700]   I know.
[02:17:30.700 --> 02:17:31.700]   So it's my city.
[02:17:31.700 --> 02:17:32.940]   Well, my, my state and my city.
[02:17:32.940 --> 02:17:36.540]   So like even if like, wash, I mean, Washington's taking through, but even if they hadn't like
[02:17:36.540 --> 02:17:41.180]   Seattle proper, like last year was like, no, I won.
[02:17:41.180 --> 02:17:43.340]   Sorry for not giving anything over.
[02:17:43.340 --> 02:17:45.380]   This will be an interesting court battle.
[02:17:45.380 --> 02:17:46.380]   Oh, it will.
[02:17:46.380 --> 02:17:50.060]   I mean, that's going to ultimately be the interesting thing is the, that will be like
[02:17:50.060 --> 02:17:56.540]   a Supreme Court, you know, sort of case if, if, you know, the FCC and over the state,
[02:17:56.540 --> 02:18:00.020]   you know, state's provisions, that will, that would actually be really interesting.
[02:18:00.020 --> 02:18:02.100]   But hopefully by that point, this whole thing will be moot.
[02:18:02.100 --> 02:18:07.500]   But somebody said, if you just had Obama come out against it, then Trump would be, yeah,
[02:18:07.500 --> 02:18:09.620]   I'm all for it.
[02:18:09.620 --> 02:18:10.620]   Here's the final story.
[02:18:10.620 --> 02:18:12.620]   We're all podcasters here, right?
[02:18:12.620 --> 02:18:19.620]   You've got the internet history podcast that tech tech memes daily ride home podcast.
[02:18:19.620 --> 02:18:24.980]   Christina's got that fabulous show she does with Cimonde de Rochefort and, and Brianna
[02:18:24.980 --> 02:18:30.140]   Wu called rocket rocket.
[02:18:30.140 --> 02:18:32.660]   Patrick's got French and English podcasts.
[02:18:32.660 --> 02:18:34.540]   Good news.
[02:18:34.540 --> 02:18:35.540]   Ding dong.
[02:18:35.540 --> 02:18:37.540]   The witch is dead.
[02:18:37.540 --> 02:18:40.460]   The audio, which we got a dunning notice from them.
[02:18:40.460 --> 02:18:45.740]   They want a millions of dollars from us because they said they had patented podcasting.
[02:18:45.740 --> 02:18:52.140]   They, the EFF bless you, EFF, the electronic frontier foundation did an inter partase motion.
[02:18:52.140 --> 02:18:56.900]   Got the trademark patent office to overturn it.
[02:18:56.900 --> 02:19:03.860]   In 2013, it was then appealed by personal audio all the way up to the Supreme Court.
[02:19:03.860 --> 02:19:09.980]   Because of May 14th, the Supreme Court has rejected personal audio's petition for review.
[02:19:09.980 --> 02:19:13.060]   Unless the EU wants to take this on, there is no higher court.
[02:19:13.060 --> 02:19:15.700]   It's over.
[02:19:15.700 --> 02:19:18.100]   You save podcasting everybody.
[02:19:18.100 --> 02:19:20.620]   Thanks to people like Adam Corolla too, who actually went to court.
[02:19:20.620 --> 02:19:23.180]   We were prepared to go to court.
[02:19:23.180 --> 02:19:26.020]   When they asked us for, I think two and a half millions, something like that, we said,
[02:19:26.020 --> 02:19:27.020]   no.
[02:19:27.020 --> 02:19:28.380]   And then we never heard from them again.
[02:19:28.380 --> 02:19:31.660]   So thank God because it would have been expensive.
[02:19:31.660 --> 02:19:35.260]   They made a mistake though because it wouldn't cost us that much to defend it.
[02:19:35.260 --> 02:19:41.780]   Just a tip to future patent trolls always ask less than the cost of defending it.
[02:19:41.780 --> 02:19:44.380]   So if it's a million dollars to defend it, which is about what it would have cost us
[02:19:44.380 --> 02:19:49.740]   probably, ask for 9.8, you know, 980,000.
[02:19:49.740 --> 02:19:52.460]   His new podcast is called Extortion 101.
[02:19:52.460 --> 02:19:56.860]   How to be a good patent troll.
[02:19:56.860 --> 02:19:58.620]   This week in extortion, I like that.
[02:19:58.620 --> 02:19:59.620]   There you go.
[02:19:59.620 --> 02:20:00.620]   There you go.
[02:20:00.620 --> 02:20:03.300]   Thank you so much, Christina, for being with us.
[02:20:03.300 --> 02:20:04.300]   I really appreciate it.
[02:20:04.300 --> 02:20:06.220]   It came all the way to work just so you could be here.
[02:20:06.220 --> 02:20:07.220]   Senior Cloud.
[02:20:07.220 --> 02:20:08.220]   I loved it.
[02:20:08.220 --> 02:20:09.220]   Oh, we love having you on.
[02:20:09.220 --> 02:20:12.060]   You're the greatest senior Cloud dev advocate at Microsoft.
[02:20:12.060 --> 02:20:15.220]   Film Girl on the Twitter film underscore Girl.
[02:20:15.220 --> 02:20:16.320]   Catch her on rockets.
[02:20:16.320 --> 02:20:17.320]   What other things?
[02:20:17.320 --> 02:20:19.020]   You're on channel nine, right?
[02:20:19.020 --> 02:20:20.020]   Yeah.
[02:20:20.020 --> 02:20:22.020]   So and that's actually we've got a new YouTube channel.
[02:20:22.020 --> 02:20:26.260]   It's a YouTube.com/MicrosoftDeveloper and that's where all of our developer content is.
[02:20:26.260 --> 02:20:28.500]   It's where all the sessions from build are.
[02:20:28.500 --> 02:20:29.740]   And we're also doing other shows.
[02:20:29.740 --> 02:20:34.780]   There's a really fun show called Five Things with some of our CDAs do where they very briefly
[02:20:34.780 --> 02:20:40.620]   teach you five things about a different type of web language or cool cloud.
[02:20:40.620 --> 02:20:42.380]   Hey, guys, and welcome to.
[02:20:42.380 --> 02:20:43.380]   Oh, that's you.
[02:20:43.380 --> 02:20:44.380]   That's me.
[02:20:44.380 --> 02:20:45.380]   Yeah.
[02:20:45.380 --> 02:20:47.180]   When you go to YouTube there, you literally I'm right there.
[02:20:47.180 --> 02:20:48.180]   That's you.
[02:20:48.180 --> 02:20:49.180]   Nice.
[02:20:49.180 --> 02:20:54.860]   Yeah, no, so that's channel line obviously still has its main website, but we're also
[02:20:54.860 --> 02:20:55.860]   putting all the stuff on that.
[02:20:55.860 --> 02:20:56.860]   I'm glad you're on YouTube.
[02:20:56.860 --> 02:20:58.420]   This makes it very easy to find.
[02:20:58.420 --> 02:20:59.420]   Yeah.
[02:20:59.420 --> 02:21:01.180]   So we know that's where everybody is.
[02:21:01.180 --> 02:21:03.620]   And so it didn't make sense for us to not be there.
[02:21:03.620 --> 02:21:04.860]   And so we've got to both places.
[02:21:04.860 --> 02:21:07.980]   So if you're in a country that doesn't have YouTube or corporate policy or whatever,
[02:21:07.980 --> 02:21:11.500]   we still are not on YouTube as well, but we're trying to put everything that way.
[02:21:11.500 --> 02:21:15.980]   And if you have ideas for types of contents, you know, developer content that you want
[02:21:15.980 --> 02:21:20.060]   to see and, you know, we do non Microsoft centric stuff all the time.
[02:21:20.060 --> 02:21:25.700]   You know, some people are focused on Python or web or, you know, any of those things we
[02:21:25.700 --> 02:21:26.700]   care about.
[02:21:26.700 --> 02:21:32.860]   And let me know and you can see what types of shows we can get created and who we can
[02:21:32.860 --> 02:21:33.860]   talk to.
[02:21:33.860 --> 02:21:37.300]   So I love it that you've got every every talk from a build on that.
[02:21:37.300 --> 02:21:40.220]   That's really, really great.
[02:21:40.220 --> 02:21:41.980]   And I love it that you're the face of it.
[02:21:41.980 --> 02:21:42.980]   You really are, aren't you?
[02:21:42.980 --> 02:21:44.820]   You're like the star of the show.
[02:21:44.820 --> 02:21:48.700]   I mean, it's the big part of my job is to kind of, you know, lead a lot of the strategy
[02:21:48.700 --> 02:21:52.060]   around that and to do interviews with people and to, you know, host a defense and stuff
[02:21:52.060 --> 02:21:53.060]   like that.
[02:21:53.060 --> 02:21:54.060]   Yeah, that's a lot of what I do.
[02:21:54.060 --> 02:21:56.740]   You're so smart.
[02:21:56.740 --> 02:22:00.180]   What a good move from Microsoft.
[02:22:00.180 --> 02:22:01.180]   Thank you, Christina.
[02:22:01.180 --> 02:22:02.180]   Thank you, Leo.
[02:22:02.180 --> 02:22:03.180]   Great to see you.
[02:22:03.180 --> 02:22:05.060]   Thank you, Patrick Bejaw.
[02:22:05.060 --> 02:22:10.100]   Don't forget French spin.com, the Phileus club.
[02:22:10.100 --> 02:22:12.060]   All the stuff he does in French.
[02:22:12.060 --> 02:22:13.980]   It's all there.
[02:22:13.980 --> 02:22:15.380]   I want to listen to pixels.
[02:22:15.380 --> 02:22:16.580]   That's also English language.
[02:22:16.580 --> 02:22:18.100]   I'm very interested.
[02:22:18.100 --> 02:22:21.660]   Yeah, we're going to have an interesting one for you three.
[02:22:21.660 --> 02:22:25.740]   And I guess the French ones, Leo, do you want to learn French?
[02:22:25.740 --> 02:22:26.740]   I should listen.
[02:22:26.740 --> 02:22:27.740]   You want to speak French?
[02:22:27.740 --> 02:22:28.740]   I speak a little.
[02:22:28.740 --> 02:22:29.740]   You know, I studied in four years in high school.
[02:22:29.740 --> 02:22:30.740]   I should.
[02:22:30.740 --> 02:22:32.980]   I could listen and it would build my French, wouldn't it?
[02:22:32.980 --> 02:22:33.980]   Yeah.
[02:22:33.980 --> 02:22:37.660]   Well, that's the thing because it's I mean, Londé VuTech is the one I'm talking about
[02:22:37.660 --> 02:22:39.160]   now.
[02:22:39.160 --> 02:22:40.160]   It's about tech.
[02:22:40.160 --> 02:22:42.460]   So you already know basically half the words.
[02:22:42.460 --> 02:22:43.460]   I know.
[02:22:43.460 --> 02:22:44.460]   No, wait a minute.
[02:22:44.460 --> 02:22:48.380]   Do you call it a computer or an or did not do?
[02:22:48.380 --> 02:22:49.380]   We do often that tell.
[02:22:49.380 --> 02:22:54.260]   It's probably one of the last words that were actually translated in French.
[02:22:54.260 --> 02:22:55.900]   It sounds so anti-weird.
[02:22:55.900 --> 02:22:56.900]   It's a no, they're not too.
[02:22:56.900 --> 02:23:02.140]   It sounds weird because they it's actually to all Dine is well, I guess it's the same
[02:23:02.140 --> 02:23:03.140]   in English.
[02:23:03.140 --> 02:23:04.860]   You order numbers, right?
[02:23:04.860 --> 02:23:07.020]   And that's what basically a computer does.
[02:23:07.020 --> 02:23:10.580]   So the French, the Académie Francais translated it.
[02:23:10.580 --> 02:23:13.820]   But today we don't translate any of those anymore.
[02:23:13.820 --> 02:23:18.180]   I mean, sometimes we do, but they have already come into the language because the internet
[02:23:18.180 --> 02:23:20.100]   makes all this instantaneous.
[02:23:20.100 --> 02:23:21.420]   So we say a smartphone.
[02:23:21.420 --> 02:23:25.340]   We don't really say like intelligent telephone or something like that.
[02:23:25.340 --> 02:23:28.900]   You don't call it a handy or something like that.
[02:23:28.900 --> 02:23:30.660]   No, we don't.
[02:23:30.660 --> 02:23:34.620]   And I guess some people they try to do it for email.
[02:23:34.620 --> 02:23:38.740]   There are specific words for the first stuff like that, but no one really uses it.
[02:23:38.740 --> 02:23:41.260]   So is it over for the Academy Francais?
[02:23:41.260 --> 02:23:42.900]   Do they are not going to?
[02:23:42.900 --> 02:23:44.420]   Oh, no, it's not.
[02:23:44.420 --> 02:23:45.420]   They try.
[02:23:45.420 --> 02:23:52.780]   And I guess they succeed to an extent, but the communication is so it's too fast.
[02:23:52.780 --> 02:23:53.780]   They can't keep up.
[02:23:53.780 --> 02:23:54.780]   Yeah, yeah.
[02:23:54.780 --> 02:23:57.140]   It's a, you know, they're what are they going to do?
[02:23:57.140 --> 02:24:02.380]   Like they're going to try to find a French words for a snap.
[02:24:02.380 --> 02:24:03.780]   The kids are already all snapping.
[02:24:03.780 --> 02:24:04.780]   Too late.
[02:24:04.780 --> 02:24:05.780]   That's it.
[02:24:05.780 --> 02:24:06.780]   They're all snapping.
[02:24:06.780 --> 02:24:09.780]   They're exchanging their like, give me your snap.
[02:24:09.780 --> 02:24:12.500]   That's what I don't know what the kids do.
[02:24:12.500 --> 02:24:17.300]   I do know a little bit about tech and we do, we talk about it in French.
[02:24:17.300 --> 02:24:19.660]   So the only tech is where you want to go.
[02:24:19.660 --> 02:24:20.740]   Frenchspin.com.
[02:24:20.740 --> 02:24:23.220]   Thank you so much, Patrick Bechia.
[02:24:23.220 --> 02:24:25.900]   And Brian, you're the greatest.
[02:24:25.900 --> 02:24:28.060]   Keep your history hat on.
[02:24:28.060 --> 02:24:31.100]   He's the host of the ride home on the tech mean.com.
[02:24:31.100 --> 02:24:32.100]   You could see it.
[02:24:32.100 --> 02:24:33.100]   5 p.m.
[02:24:33.100 --> 02:24:34.100]   Is that 5 p.m.
[02:24:34.100 --> 02:24:38.460]   Pacific or Eastern every Eastern because I'm here in New York, but then, you know, we,
[02:24:38.460 --> 02:24:43.420]   because we want to try to, I would say that it's pretty 50 50 and then 30% of the audience
[02:24:43.420 --> 02:24:45.060]   is overseas so that really does it.
[02:24:45.060 --> 02:24:46.060]   Wow.
[02:24:46.060 --> 02:24:51.660]   You can, you can listen in the morning on your ride in if you want to, but yeah, the idea
[02:24:51.660 --> 02:24:55.980]   is is I'm trying to catch you up on most things that happened.
[02:24:55.980 --> 02:25:01.100]   If something crazy happens, then I will, I'm sure I can run back in and rerecord.
[02:25:01.100 --> 02:25:04.540]   So maybe that the California people would get something a little different than the
[02:25:04.540 --> 02:25:05.540]   New York people.
[02:25:05.540 --> 02:25:07.820]   But we'll do the breaking news here.
[02:25:07.820 --> 02:25:10.420]   You could just keep doing the 5 p.m. thing.
[02:25:10.420 --> 02:25:11.420]   Cool.
[02:25:11.420 --> 02:25:15.980]   I'm so thrilled to have you on and come back really soon.
[02:25:15.980 --> 02:25:16.980]   Okay, Brian, Brian, Brian, Brian, Brian, Brian, Brian, Brian, Brian, Brian, Brian, Brian.
[02:25:16.980 --> 02:25:19.260]   Yeah, really, really enjoyed it.
[02:25:19.260 --> 02:25:21.940]   As they said in the chatroom, we enjoyed your perspective.
[02:25:21.940 --> 02:25:23.380]   We thank you everybody for coming.
[02:25:23.380 --> 02:25:25.340]   We had a great studio audience today.
[02:25:25.340 --> 02:25:30.620]   Some very bored children, a couple of bored spouses, but everybody else is having a good
[02:25:30.620 --> 02:25:32.020]   time, right?
[02:25:32.020 --> 02:25:35.220]   If you want to be a studio, all you have to do is email tickets@twit.tv.
[02:25:35.220 --> 02:25:38.460]   The tickets are free, but we do like to know ahead of time that you come in so we can
[02:25:38.460 --> 02:25:39.460]   put a chair out for you.
[02:25:39.460 --> 02:25:41.140]   We just love having a live audience.
[02:25:41.140 --> 02:25:42.140]   So thank you.
[02:25:42.140 --> 02:25:43.500]   You can also watch live on the stream.
[02:25:43.500 --> 02:25:45.820]   And there's value in that for us too.
[02:25:45.820 --> 02:25:52.260]   If you could watch it at twit.tv/live, join us in the chatroom because I watch the chatroom
[02:25:52.260 --> 02:25:53.540]   like a hawk the whole time.
[02:25:53.540 --> 02:25:56.980]   And as you've heard several times through the show, the chatroom is a great asset to
[02:25:56.980 --> 02:25:57.980]   us.
[02:25:57.980 --> 02:25:58.980]   IRC.twit.tv.
[02:25:58.980 --> 02:26:04.340]   But it's most useful if you're watching during the live programming because then we get feedback
[02:26:04.340 --> 02:26:05.940]   from the live show.
[02:26:05.940 --> 02:26:09.900]   If you can't watch live and I understand everybody's got a life, everything we do is
[02:26:09.900 --> 02:26:18.580]   on demand, audio and video at our website, twit.tv or subscribe in your favorite podcatcher.
[02:26:18.580 --> 02:26:20.860]   And that way you'll get it every week, the minute's available.
[02:26:20.860 --> 02:26:25.180]   We record the show Sunday, 6 p.m. Eastern, 3 p.m. Pacific.
[02:26:25.180 --> 02:26:28.420]   That's noon, not noon, midnight UTC, right?
[02:26:28.420 --> 02:26:29.420]   Is that right?
[02:26:29.420 --> 02:26:30.420]   Is that what it is in Finland right now?
[02:26:30.420 --> 02:26:31.660]   Is it the middle of the night?
[02:26:31.660 --> 02:26:34.500]   Well, in Finland, the sun is coming up now.
[02:26:34.500 --> 02:26:35.500]   It's only 4 a.m.
[02:26:35.500 --> 02:26:36.500]   I am sorry.
[02:26:36.500 --> 02:26:37.500]   You're a Patrick.
[02:26:37.500 --> 02:26:40.740]   Oh my God, I'm glad I didn't know that before.
[02:26:40.740 --> 02:26:43.100]   I feel terrible.
[02:26:43.100 --> 02:26:47.140]   It's the well, that's why I couldn't make it for a few months.
[02:26:47.140 --> 02:26:52.220]   And actually, I don't know if you heard it, but the baby woke up and cried quite a bit
[02:26:52.220 --> 02:26:53.220]   in the middle of the show.
[02:26:53.220 --> 02:26:54.780]   Didn't hear a thing and I'm that what we did.
[02:26:54.780 --> 02:26:57.180]   I admire you as a father for not moving.
[02:26:57.180 --> 02:26:58.980]   For not going to be.
[02:26:58.980 --> 02:27:02.140]   Honey, the baby's crying.
[02:27:02.140 --> 02:27:03.340]   It's three in the morning.
[02:27:03.340 --> 02:27:05.580]   Can you wake up and take a picture?
[02:27:05.580 --> 02:27:07.900]   Honey, you got what he wants.
[02:27:07.900 --> 02:27:08.900]   Not me.
[02:27:08.900 --> 02:27:09.900]   Get in here.
[02:27:09.900 --> 02:27:10.900]   It's pretty.
[02:27:10.900 --> 02:27:11.900]   Yes, it's midnight.
[02:27:11.900 --> 02:27:14.900]   Thank you for staying up late.
[02:27:14.900 --> 02:27:17.260]   I really appreciate it.
[02:27:17.260 --> 02:27:18.260]   Thank you so much, Patrick.
[02:27:18.260 --> 02:27:20.980]   Thank you all for being here and we will see you next time.
[02:27:20.980 --> 02:27:23.140]   Another twit is in the can.
[02:27:23.140 --> 02:27:33.660]   Easy.

