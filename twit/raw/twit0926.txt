;FFMETADATA1
title=Bias, Hallucinations, and Jail Breaks
artist=Leo Laporte, Louise Matsakis, Allyn Malventano, Janko Roettgers
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-05-07
track=926
language=English
genre=Podcast
comment=<p>AI chatbots, Writer&\#039\;s Guild strike, Pluto ad-supported TV</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:07.760]   It's time for Twit this week in Tech. Great panel for you the return of Alan Malvin Tano our former SSD guy from this week in computer hardware
[00:00:07.760 --> 00:00:15.560]   He's still working on SSDs at Soladime. We also have Louise Mazzakis of semaphore.com always great to have Louise on and
[00:00:15.560 --> 00:00:19.280]   Somebody I've been following for years first a giga home
[00:00:19.280 --> 00:00:22.120]   then it
[00:00:22.120 --> 00:00:29.120]   Was it variety than a protocol now? He's got his own newsletter low pass. CC yanko records joins us
[00:00:29.120 --> 00:00:36.240]   We'll be talking about AI of course in that big White House meeting of all of the leaders in AI a judge
[00:00:36.240 --> 00:00:45.640]   Protects a data broker. Oh, no, and what's Google gonna announce at Google? I owe this week that a whole lot more coming up next on Twit
[00:00:45.640 --> 00:00:51.680]   Podcasts you love from people you trust
[00:00:51.680 --> 00:00:54.800]   This is Twit
[00:00:55.800 --> 00:00:57.800]   This
[00:00:57.800 --> 00:01:04.840]   Is Twit this week at Tech episode
[00:01:04.840 --> 00:01:08.800]   926 recorded Sunday, May 7th, 2023
[00:01:08.800 --> 00:01:12.720]   bias hallucinations and jail breaks
[00:01:12.720 --> 00:01:21.400]   This week attack is brought to you by AG1 by Athletic Greens if you're looking for a simpler cost effective supplement routine
[00:01:21.960 --> 00:01:29.720]   AG1 is giving you a free one-year supply of vitamin D and five free travel packs with your first purchase of a subscription
[00:01:29.720 --> 00:01:33.640]   Go to athletic greens calm slash twits and by
[00:01:33.640 --> 00:01:38.760]   Zip recruiter whether you're starting a new business or growing one if you want to be successful
[00:01:38.760 --> 00:01:44.000]   You need the most talented people on your team. That's where zip recruiter comes in right now
[00:01:44.000 --> 00:01:47.720]   You could try it free at zip recruiter calm slash twits
[00:01:47.720 --> 00:01:50.000]   and by
[00:01:50.000 --> 00:01:55.360]   Express VPN stop letting big tech leech on your data freely use this link right now
[00:01:55.360 --> 00:02:03.760]   And you get three extra months free with your one-year plan express VPN calm slash twit to get protected with the VPN rated number one by
[00:02:03.760 --> 00:02:06.440]   CNET tech radar and me
[00:02:06.440 --> 00:02:16.160]   It's time for Twit this week in tech show we cover the latest tech news
[00:02:16.600 --> 00:02:20.160]   I've put together assembled a really good panel for you this week
[00:02:20.160 --> 00:02:22.480]   I had to because last week was so stellar
[00:02:22.480 --> 00:02:26.680]   I have to keep up the standards. It's great to have Alan Malvin tonneau back in the house
[00:02:26.680 --> 00:02:32.720]   Long time hosts at this week in computer hardware our twitch show. He was at Intel
[00:02:32.720 --> 00:02:36.600]   He's an SSD guru Intel spun off his current company
[00:02:36.600 --> 00:02:38.600]   Solidine
[00:02:38.600 --> 00:02:42.560]   Which now makes the Intel SSDs except there
[00:02:43.720 --> 00:02:51.000]   Solidine hi Alan. I liked it so much that I was not part of the spin-off and joined them anyway
[00:02:51.000 --> 00:02:53.920]   Oh, that's interesting. Okay. Yeah
[00:02:53.920 --> 00:02:55.960]   also
[00:02:55.960 --> 00:02:57.520]   Submariner
[00:02:57.520 --> 00:03:01.520]   Naval intelligence officer former NSA analyst did I get that all right?
[00:03:01.520 --> 00:03:04.560]   Just get all over the lifetimes
[00:03:04.560 --> 00:03:12.360]   Because that NS that NSA analyst is helpful these days. It's good to have somebody who's had experience on the inside
[00:03:12.960 --> 00:03:17.120]   Yep, welcome. Alan. It's great to see you also from semaphore.com
[00:03:17.120 --> 00:03:21.600]   Good friend Louise Mizzakis is here. She writes about tech at
[00:03:21.600 --> 00:03:24.920]   Semaphore I just good to see you
[00:03:24.920 --> 00:03:31.000]   Alia, how's it going welcome? Did you ever are you now or have you ever been an NSA analyst?
[00:03:31.000 --> 00:03:38.080]   No, and I have never leaked anything private in a discord server is your first name reality. No, okay
[00:03:38.560 --> 00:03:44.360]   Yeah, that that story is a fascinating story and getting weirder and weirder as time goes by
[00:03:44.360 --> 00:03:47.760]   We'll talk about that a little bit also here not his first time
[00:03:47.760 --> 00:03:49.920]   But at first time in a long time yanko records
[00:03:49.920 --> 00:03:54.960]   Who is you you may remember him from giga home and then variety and then
[00:03:54.960 --> 00:03:58.280]   protocol
[00:03:58.280 --> 00:04:05.880]   That's pretty much it. I get them all there. I think so yanko he has a great new newsletter called low pass
[00:04:06.280 --> 00:04:09.720]   Which has that's a clever name filtering the future
[00:04:09.720 --> 00:04:18.000]   But you've always focused on the streaming and media and as you said the the confluence between technology and media and always great
[00:04:18.000 --> 00:04:22.160]   Topics and great content. So thank you for being here. It's great to see you
[00:04:22.160 --> 00:04:26.840]   I've been you know, I you're here in spirit many shows. We often quote you
[00:04:26.840 --> 00:04:32.200]   So appreciate that. Yeah, yeah always well, you know when variety
[00:04:32.560 --> 00:04:38.120]   Was smart for variety to hire somebody new about technology because you know really is an entertainment magazine
[00:04:38.120 --> 00:04:41.160]   But but the world is changed. I guess we should talk about
[00:04:41.160 --> 00:04:45.920]   The writers strike because that's on and full force started this week
[00:04:45.920 --> 00:04:54.560]   Already, you know, there's a big back and forth between the writers and the TV and motion picture producers
[00:04:54.560 --> 00:04:58.240]   The the big changes that they're striking over are
[00:04:58.960 --> 00:05:05.200]   revenues from streaming which don't trickle down to the writers particularly and AI
[00:05:05.200 --> 00:05:12.040]   The writers are very concerned probably reasonably so I don't know that AI will
[00:05:12.040 --> 00:05:18.720]   Either take their jobs or supplement their jobs and one of the clauses they were hoping for in a new contract was one that prevented
[00:05:18.720 --> 00:05:22.280]   AI from going through their old scripts and creating new scripts
[00:05:22.280 --> 00:05:24.440]   from them
[00:05:24.440 --> 00:05:30.280]   Last week somebody pointed out that how many how many years of the Simpsons are there? There's hundreds of Simpsons scripts
[00:05:30.280 --> 00:05:39.480]   It would it be possible for something like chat GPT to write a new one based on all the old ones anyway the motion picture and television producers
[00:05:39.480 --> 00:05:46.720]   Seem very reluctant to negotiate on AI in fact all they their counterproposal was we'll have a yearly meeting
[00:05:46.720 --> 00:05:50.960]   Let's talk about it once in a while. We'll get together
[00:05:52.320 --> 00:05:59.960]   I I presume you kind of talked to people in the WGA about this. How serious is that concern for about AI?
[00:05:59.960 --> 00:06:02.840]   I think it is serious and I think they also
[00:06:02.840 --> 00:06:09.680]   Sometimes it gets paid off as oh, it's just these people who are afraid that robots are taking a job or something like that
[00:06:09.680 --> 00:06:13.080]   Right, but they are having very valid concerns. I think
[00:06:13.080 --> 00:06:20.240]   One one of them is what happens when like AI goes through old scripts and maybe even rejected scripts
[00:06:20.240 --> 00:06:23.400]   Maybe the students gonna tell you oh, we like what you did
[00:06:23.400 --> 00:06:26.200]   but we're gonna go with something else and that something else happens to be an
[00:06:26.200 --> 00:06:31.280]   I version of a script that's trained on your script so that would kind of suck for writers obviously
[00:06:31.280 --> 00:06:34.760]   But I think the bigger point here is that writers get paid less
[00:06:34.760 --> 00:06:40.440]   For marking up or sort of reworking existing scripts then they get for original works
[00:06:40.440 --> 00:06:45.840]   And I think writers are rightly worried that students might just have AI churn all these
[00:06:47.040 --> 00:06:49.840]   And then say you know, this is not perfect
[00:06:49.840 --> 00:06:55.240]   Why don't you like work on it and then you get paid half of what you would get paid for if we if we took your script?
[00:06:55.240 --> 00:06:59.120]   So that's I think a really well concerned. I could see that actually happen. I
[00:06:59.120 --> 00:07:06.440]   Think there's a serious concern last week. We had Alex Stamos security guru on and his concern is that AI
[00:07:06.440 --> 00:07:10.800]   It is so facile at generating content necessarily accurate content
[00:07:10.800 --> 00:07:16.560]   But but you know kind of convincingly well written content that it's gonna be an
[00:07:16.880 --> 00:07:18.880]   information
[00:07:18.880 --> 00:07:26.840]   Overload that we're gonna have and and the Washington Post had an article this week kind of confirming that saying AI is already
[00:07:26.840 --> 00:07:29.440]   writing books how to books
[00:07:29.440 --> 00:07:31.520]   They're writing recipes
[00:07:31.520 --> 00:07:37.880]   It's already cranking stuff out like crazy. So I think this the writer's guild is rightly concerned
[00:07:37.880 --> 00:07:41.840]   One of the stories that the Washington Post
[00:07:42.320 --> 00:07:49.440]   It's will or amous writing this he does a great job talks about a software developer. Nice name Chris Cowell who wrote a very technical
[00:07:49.440 --> 00:07:52.720]   book for a very small audience
[00:07:52.720 --> 00:08:01.160]   Called automating dev ops with GitLab CI CD pipelines now you probably understand if you understand one of those words mean that is a narrow slice
[00:08:01.160 --> 00:08:05.200]   But three weeks before it came out a
[00:08:05.200 --> 00:08:08.200]   book with exactly the same title
[00:08:08.200 --> 00:08:10.760]   came out and
[00:08:10.840 --> 00:08:17.240]   As far as he could tell that the author Marie Carpos had never written anything before had no presence on the web as
[00:08:17.240 --> 00:08:25.560]   He figured out it was written. There's a book publisher in India an education technology firm called in case stall
[00:08:25.560 --> 00:08:30.000]   That has dozens of books on Amazon each with a different author
[00:08:30.000 --> 00:08:34.840]   Each with five-star Amazon reviews from the same handful of India based reviewers
[00:08:34.840 --> 00:08:38.520]   all apparently AI generated and
[00:08:39.320 --> 00:08:43.480]   That you spend a year writing a book about dev ops and CI CD
[00:08:43.480 --> 00:08:50.400]   With a narrow audience and somebody beats you to the punch of three weeks with an AI generator book. That's not good
[00:08:50.400 --> 00:08:55.520]   Yeah, no, it's it's really bad. I think what you're seeing is sort of AI
[00:08:55.520 --> 00:08:59.800]   Filling up the lowest common denominator
[00:08:59.800 --> 00:09:03.440]   Information on the internet right like it's all the SEO blogs
[00:09:03.800 --> 00:09:07.920]   These books you're seeing on Amazon. That's their Kindle self publishing program, right?
[00:09:07.920 --> 00:09:13.600]   Which we've seen a lot of sort of abuse and you know spammy books fill that section previously
[00:09:13.600 --> 00:09:21.920]   I saw a great tweet earlier this week that was like wouldn't it be so wonderful instead if Google was trying to figure out new ways to
[00:09:21.920 --> 00:09:27.200]   Filter this stuff out, right? It's already been a big problem in their search results for a long time
[00:09:27.200 --> 00:09:29.720]   I would just love if there was a company that was taking the stance
[00:09:29.720 --> 00:09:33.320]   Hey instead of giving you a chat GPT powered bot
[00:09:33.560 --> 00:09:39.680]   We're going to you know emphasize human results, right? And we're gonna make sure that you see you know the best quality
[00:09:39.680 --> 00:09:45.600]   Human generated content as possible. I would switch to a new search engine that was putting an emphasis on that
[00:09:45.600 --> 00:09:47.600]   But instead you're seeing going the other way
[00:09:47.600 --> 00:09:50.480]   Exactly, right? Right
[00:09:50.480 --> 00:09:57.120]   It's getting harder and harder to tell it to whether something's coming from like a bad content farm or from a bad AI, right?
[00:09:57.120 --> 00:10:03.080]   I think Luis you probably have the same issue whenever you write something as people or you break some use or something
[00:10:03.400 --> 00:10:05.400]   People are picking it up. There's
[00:10:05.400 --> 00:10:13.080]   reputable outlets rewriting it picking up doing some on reporting and then there's always these really weird websites that sort of get it right and then at the
[00:10:13.080 --> 00:10:17.920]   End of the article totally goes off the rails and becomes like a weird conspiracy theory and
[00:10:17.920 --> 00:10:24.320]   Until recently I thought it's all just content farms as people in India getting paid like 10 cents to rewrite the story
[00:10:24.320 --> 00:10:30.680]   But I'm starting to suspect that a lot of that stuff has been AI all along and that like half of the Google search results for my
[00:10:30.680 --> 00:10:33.880]   Topsy adjust AI garbage at this point. Yeah
[00:10:33.880 --> 00:10:39.240]   And I think the reason that it's hard to tell between those two things is because these bots were trained on
[00:10:39.240 --> 00:10:41.240]   So much of that stuff, right?
[00:10:41.240 --> 00:10:46.520]   So like they're they're able to really easily generate that type of content because so much of it was already polluting the web
[00:10:46.520 --> 00:10:49.920]   And I think it's just really nefarious and unfortunate that this is
[00:10:49.920 --> 00:10:56.920]   Already starting to happen on a pretty wide scale. I wrote a few weeks ago about how this was starting to happen with online reviews
[00:10:56.920 --> 00:11:03.400]   I encourage everyone to do a Google search for site yelp.com and search as an AI model
[00:11:03.400 --> 00:11:07.280]   That response that you get back from chat GPT and you'll find
[00:11:07.280 --> 00:11:14.800]   Google has cashed all these really funny reviews on Yelp for Biggree is you know for I think it was like a sewage
[00:11:14.800 --> 00:11:23.360]   Service somewhere in California and it's just full of AI generated reviews that you know someone didn't even bother to take the part out where it says
[00:11:23.360 --> 00:11:26.560]   As an AI model. I don't have personal opinions or whatever
[00:11:26.560 --> 00:11:30.920]   This is this is a really fascinating Google search
[00:11:30.920 --> 00:11:35.240]   That you can use on Twitter and a lot of places and what it is is
[00:11:35.240 --> 00:11:43.320]   Frequently chat GPT will start an answer with as an AI language model. So if you search for that
[00:11:43.320 --> 00:11:46.080]   You will find it everywhere
[00:11:46.080 --> 00:11:49.360]   Vice pointed this out
[00:11:49.840 --> 00:11:54.280]   And and they're not even bothering, you know to cut it out
[00:11:54.280 --> 00:11:57.400]   It's just everywhere a
[00:11:57.400 --> 00:12:04.040]   User review for a waste trimmer posted on April 13th contains the entire response to the initial prompt unedited
[00:12:04.040 --> 00:12:10.680]   On Amazon as an AI language model. I mean
[00:12:10.680 --> 00:12:16.800]   It's it's terrifying, but is it but as you point out it's low quality content
[00:12:17.280 --> 00:12:20.880]   So low quality content's always been a problem that content farms and stuff
[00:12:20.880 --> 00:12:26.080]   Maybe we don't have to worry about it because it's just low quality content
[00:12:26.080 --> 00:12:28.760]   you know a
[00:12:28.760 --> 00:12:32.360]   few months back when the AI started to pick up steam I
[00:12:32.360 --> 00:12:38.560]   And I said this to multiple of my friends at the time I went I really feel like we're
[00:12:38.560 --> 00:12:45.440]   At the beginning of some form of renaissance here like this might really be an interesting thing that we get to watch
[00:12:46.000 --> 00:12:48.880]   Unfold and that has slowly been shipped away
[00:12:48.880 --> 00:12:54.160]   By by what you know the types of things that we've just covered here, right?
[00:12:54.160 --> 00:13:00.280]   Like look at how quickly society has proven to us that they're just gonna use this new thing in
[00:13:00.280 --> 00:13:04.320]   Ways that are just you know nefarious or any other?
[00:13:04.320 --> 00:13:11.000]   Opportunistic and I have to point out that while you can now look for as an AI language model
[00:13:12.360 --> 00:13:17.040]   And and immediately eliminate those responses. It's not gonna take long before it fact
[00:13:17.040 --> 00:13:19.040]   Probably people have already figured it out
[00:13:19.040 --> 00:13:25.880]   Right and then someone's gonna need to make an AI model in order to catch the other yeah AI
[00:13:25.880 --> 00:13:30.560]   Yeah, and now it's gonna be this war of AI is just happening behind the scenes, right?
[00:13:30.560 --> 00:13:37.760]   I think I think that's especially true because right now the vast majority of what you're seeing is people using the chat
[00:13:37.760 --> 00:13:43.120]   GPT interface, right? But since the AI opened since the API opened up for the AI
[00:13:43.120 --> 00:13:48.040]   You're starting to see now like all these sorts of third-party apps that are using the same technology
[00:13:48.040 --> 00:13:52.400]   So, you know, I'm sure somebody's gonna come out with a fake review generator, right?
[00:13:52.400 --> 00:13:54.560]   We're automatically strips out that language
[00:13:54.560 --> 00:14:00.000]   Or you know a fake book generator or whatever it is that's optimized for that purpose
[00:14:00.000 --> 00:14:05.000]   I think one really interesting story that I'm gonna be following for sure is how well
[00:14:05.520 --> 00:14:09.720]   Does open AI and the other companies creating these large models?
[00:14:09.720 --> 00:14:17.240]   Do they police their API access right? Are they gonna be cutting off hundreds of apps a day or are these apps gonna last a long time before?
[00:14:17.240 --> 00:14:20.280]   The access is cut off and I think you know
[00:14:20.280 --> 00:14:26.720]   Content moderation stories can be tiresome, but in this case, you know, they're paying for that API access, right?
[00:14:26.720 --> 00:14:30.600]   So I think there's sort of a really direct line to revenues
[00:14:30.600 --> 00:14:36.120]   There's an incentive not to clamp down on the majority of these apps and I think that's gonna be interesting to see
[00:14:36.120 --> 00:14:42.920]   How that plays out how I think on the other go on the other side when you when you talk about these reviews
[00:14:42.920 --> 00:14:47.040]   Maybe it doesn't even matter that that's in there because when you go to Amazon you don't read
[00:14:47.040 --> 00:14:52.040]   5000 reviews you look at this and you say oh, it's 5000 reviews and so one half stars average
[00:14:52.040 --> 00:14:55.180]   It must be good, but you then you missed the part at
[00:14:55.880 --> 00:15:01.720]   900 of those reviews contain that language that says as an AI model. I don't really know what I'm talking about or something like that
[00:15:01.720 --> 00:15:10.280]   Alex runs Stanford's internet observatory, which is basically looking for disinformation. He's concerned about the next
[00:15:10.280 --> 00:15:13.560]   year's presidential election. He says
[00:15:13.560 --> 00:15:17.560]   Bought farms and disinformation are gonna go crazy
[00:15:17.560 --> 00:15:20.560]   Thanks to these new tools
[00:15:20.560 --> 00:15:23.480]   What do we how as users should we?
[00:15:24.440 --> 00:15:26.440]   Deal with this what?
[00:15:26.440 --> 00:15:28.640]   You know as consumers of information
[00:15:28.640 --> 00:15:32.920]   You guys all create information, but as consumers of information
[00:15:32.920 --> 00:15:39.720]   Is there is is there a trick? Is there a tool or should we just forget everything we read?
[00:15:39.720 --> 00:15:42.800]   Yeah, go. What do you think I think oh go ahead? You could start Luis go ahead
[00:15:42.800 --> 00:15:48.000]   Well, I think you're gonna see you already before we had this influx of all this AI content
[00:15:48.000 --> 00:15:54.200]   You already saw a movement towards closed spaces right like closed discord people using signal or
[00:15:54.360 --> 00:16:00.840]   These curated communities and I think you're gonna see more and more of that the same way that now
[00:16:00.840 --> 00:16:07.600]   There's a premium on like handmade ceramics right like handmade jewelry after we saw sort of like the explosion of machines
[00:16:07.600 --> 00:16:08.720]   You know
[00:16:08.720 --> 00:16:14.320]   Turning out all sorts of like you know low-cost junk right like I think you're gonna see that with the internet right now
[00:16:14.320 --> 00:16:19.560]   You're seeing like some publications say like oh, we're like open to experimenting with AI or we're gonna generate
[00:16:19.560 --> 00:16:22.480]   You know AI quizzes like buzzfeed, but I think you're gonna see
[00:16:22.920 --> 00:16:26.000]   Like subscription publications and subscription
[00:16:26.000 --> 00:16:31.480]   You know services online emphasizing we guarantee this is a hundred percent human there's gonna be no bots
[00:16:31.480 --> 00:16:37.860]   Like you know in this discord server or whatever I think the next problem then is that that might lead to a rift right
[00:16:37.860 --> 00:16:43.240]   Or to do to divide some people can afford to pay for all the subscriptions on New York Times and whatnot
[00:16:43.240 --> 00:16:47.040]   To get verified information that has been written by humans
[00:16:47.040 --> 00:16:54.560]   I has been going through an editorial process and then you have the one level lower all those people rewriting those news and
[00:16:54.560 --> 00:16:57.280]   Has a much easier
[00:16:57.280 --> 00:17:00.240]   Like Avenue get in there and wreak havoc
[00:17:00.240 --> 00:17:04.640]   And then if people kind of fought all the subscription fees, that's really troublesome
[00:17:04.640 --> 00:17:08.720]   Yeah, it's a just think about the average person
[00:17:08.720 --> 00:17:12.120]   probably isn't the you know very
[00:17:13.360 --> 00:17:18.160]   Deliberate subscriber and going out to make sure that they're getting the correct sources, right?
[00:17:18.160 --> 00:17:21.240]   So this type of thing going on is gonna catch up
[00:17:21.240 --> 00:17:27.400]   You know the broad net is gonna catch the majority of folks that they're just like oh, I heard about this thing
[00:17:27.400 --> 00:17:30.920]   Let me just Google it and then whatever results they see that's what they look for right?
[00:17:30.920 --> 00:17:36.080]   They're not they're not being careful to be in any sort of any sort of a wild garden or anything like that
[00:17:36.080 --> 00:17:39.480]   You know, so do we have to teach is it now time to teach people?
[00:17:39.680 --> 00:17:45.400]   I guess it has been for a while to be a little bit more discriminant in their media consumption
[00:17:45.400 --> 00:17:48.200]   you know, I'll look at
[00:17:48.200 --> 00:17:50.840]   Cnet none of you have worked for Cnet, right?
[00:17:50.840 --> 00:17:55.840]   We had when this story broke that Cnet was using AI to write its
[00:17:55.840 --> 00:17:58.160]   personal finance
[00:17:58.160 --> 00:18:03.040]   Articles we had the EIC have seen it on Connie Goyama and she had just
[00:18:03.040 --> 00:18:08.160]   You know published a statement saying well, we have humans check it. We're very careful
[00:18:08.160 --> 00:18:12.480]   It's just doing the stuff that this was one one of her big points. These are it's just writing articles
[00:18:12.480 --> 00:18:16.800]   No, nobody wants to write Louise and we Enco don't want to write those articles
[00:18:16.800 --> 00:18:22.920]   So we have a machine write them and then an editor review them actually as it turned out there were still many many mistakes in
[00:18:22.920 --> 00:18:25.040]   These personal finance articles
[00:18:25.040 --> 00:18:27.120]   Cnet
[00:18:27.120 --> 00:18:29.200]   was rumored to and their
[00:18:29.200 --> 00:18:34.520]   You know personal private equity owners red ventures were rumored to be really doubling down on this and
[00:18:35.200 --> 00:18:38.680]   And it looks like red ventures has been planning to use AI written
[00:18:38.680 --> 00:18:45.160]   Content in all of its other sites sites site bank rank bank rate calm really our content farms to some degree
[00:18:45.160 --> 00:18:51.200]   Be sad to see Cnet turn into that but and right after that came out they laid off a bunch of writers
[00:18:51.200 --> 00:18:55.560]   So yeah, yeah on the wall, right? Yeah, we don't need writers. We've got
[00:18:55.560 --> 00:18:58.840]   AI
[00:19:00.760 --> 00:19:06.960]   Should we be I mean, you know, it's funny because there was just a meeting at the White House on Thursday
[00:19:06.960 --> 00:19:13.080]   All the all the big CEOs came in to to meet with the vice president
[00:19:13.080 --> 00:19:19.080]   President Biden came in and tipped his hat and said we need you guys to help
[00:19:19.080 --> 00:19:27.840]   Good and Sundar Pichai was there such an Adela was there a Sam Altman from open AI
[00:19:28.280 --> 00:19:30.480]   anthropic CEO was there
[00:19:30.480 --> 00:19:33.960]   But I mean, what are these guys doing they're not saying
[00:19:33.960 --> 00:19:38.320]   What I mean, this is their business
[00:19:38.320 --> 00:19:44.200]   Yeah, I mean I have to say what was sort of funny about that meeting to me was that
[00:19:44.200 --> 00:19:48.240]   First of all, it was interesting that Apple and meta were not included
[00:19:48.240 --> 00:19:50.600]   so what that sort of left was
[00:19:50.600 --> 00:19:56.240]   Open AI the big company partnering with open AI the people who left open AI to start
[00:19:57.640 --> 00:20:03.840]   Google right like it is sort of a narrow group of people that have a lot of power right now
[00:20:03.840 --> 00:20:10.440]   And I think it's important to push back against that like one of the great framings that I loved of the writers that went on strike was
[00:20:10.440 --> 00:20:13.200]   Why can't the AI replace studio executives?
[00:20:13.200 --> 00:20:17.080]   Right? There you go. There you go. Oh great
[00:20:17.080 --> 00:20:19.320]   Like why does it have to be?
[00:20:19.320 --> 00:20:20.640]   creative job
[00:20:20.640 --> 00:20:26.080]   Yeah, that that people really like to do and that we sort of value that that's coming from a human and has a human perspective
[00:20:26.240 --> 00:20:30.880]   And does not sort of pollute the internet with spam or you know create really?
[00:20:30.880 --> 00:20:35.640]   Reductive, you know narratives. I think when you ask it to generate stories and stuff
[00:20:35.640 --> 00:20:39.880]   It's not only inaccurate, but it's often just like really mid right? It's kind of boring
[00:20:39.880 --> 00:20:44.360]   It's sort of bad because it's just sort of taking an average of everything on the internet
[00:20:44.360 --> 00:20:49.800]   So it's not surprising, but why do we have to replace these things that are so valuable within our culture?
[00:20:49.800 --> 00:20:51.800]   Like why can't it be you know?
[00:20:51.800 --> 00:20:56.280]   Why can't we use the AI to analyze you know budgets for for for studios, right?
[00:20:56.280 --> 00:20:58.800]   Like why does it have to be these really creative jobs?
[00:20:58.800 --> 00:21:02.400]   That end up being things that get replaced
[00:21:02.400 --> 00:21:05.720]   Because I think this technology is really powerful can do a lot
[00:21:05.720 --> 00:21:07.960]   But why you know, I just don't get it
[00:21:07.960 --> 00:21:11.680]   But on the other hand I do think that the editor in chief have seen it has a point that
[00:21:11.680 --> 00:21:14.080]   these
[00:21:14.080 --> 00:21:19.920]   You know this aggregation these sort of like you know low value articles that are just trying to catch
[00:21:19.920 --> 00:21:22.280]   You know a random Google search or whatever
[00:21:22.280 --> 00:21:27.280]   Though that's not what a lot of people envision themselves doing when they want to have a career in journalism, right?
[00:21:27.280 --> 00:21:32.600]   So I think there's some point there, but just automating that process. I'm not sure really
[00:21:32.600 --> 00:21:34.840]   That's just sort of a really sad and narrow
[00:21:34.840 --> 00:21:38.400]   Use of this technology which could be used to do a lot of really cool things
[00:21:38.400 --> 00:21:42.000]   But instead it's being used to write you know SEO articles and spammy books
[00:21:42.000 --> 00:21:48.520]   And the question is like articles that nobody wants to write are those really articles that anybody wants to read right?
[00:21:48.520 --> 00:21:53.720]   Why are they just like clogging up the pipeline and clogging up the Google News results with SEO garbage?
[00:21:53.720 --> 00:21:56.040]   well
[00:21:56.040 --> 00:22:00.520]   Maybe nobody wants to read but people are searching for right. That's why it exists
[00:22:00.520 --> 00:22:03.600]   You're gonna search for bank, you know
[00:22:03.600 --> 00:22:10.360]   What's the best savings account rate and you're gonna get the bank rate calm and be served a bunch of ads and even if the content sucks
[00:22:10.360 --> 00:22:16.480]   They got you with the ads. They're there and you saw them and they get to charge the companies for that
[00:22:16.680 --> 00:22:20.360]   So do I guess I mean I think honestly they don't care if the content sucks
[00:22:20.360 --> 00:22:24.120]   No, it's all about the the ROI for that effort, right?
[00:22:24.120 --> 00:22:30.260]   It's super low effort to just make an AI do a thing and then just throw it up there low effort. That's exactly low effort
[00:22:30.260 --> 00:22:33.680]   I love the picket signs
[00:22:33.680 --> 00:22:37.560]   Writers Guild of course is gonna have the best written picket signs
[00:22:37.560 --> 00:22:40.800]   right
[00:22:40.800 --> 00:22:45.920]   Writers Guild of America AI wrote the sign and then it's crossed out and says this sign
[00:22:46.920 --> 00:22:52.840]   Live from New York on strike my favorite succession without writers is just the apprentice
[00:22:52.840 --> 00:22:59.920]   That one's really great. I told chatbeat GPT to make a sign and it sucked
[00:22:59.920 --> 00:23:05.400]   The forces with us that was probably on May 4th
[00:23:05.400 --> 00:23:08.800]   We have some notes a little shot at producers
[00:23:08.800 --> 00:23:15.020]   No AI the AI obviously a real concern would love a two-day delivery on this
[00:23:15.560 --> 00:23:17.560]   Amazon
[00:23:17.560 --> 00:23:26.000]   I want to get a baseball hat that says writer even though I'm not my brother's career is more stable and he's a magician
[00:23:26.000 --> 00:23:32.280]   Writers we need them
[00:23:32.280 --> 00:23:40.720]   By the way, this is not forget that I mean this this extends to tech journalism too, right?
[00:23:40.720 --> 00:23:45.120]   which used to be my beat you know and and and the written form
[00:23:46.120 --> 00:23:50.240]   Technical detail type info, which is a thing that AI tends to do
[00:23:50.240 --> 00:23:55.520]   Reasonably surprisingly well even if it gets some stuff wrong. It sure sounds like it's right
[00:23:55.520 --> 00:24:04.080]   You know none of this was around and I was already kind of seeing some writing on the wall doing my job back at PC perspective where it was
[00:24:04.080 --> 00:24:07.840]   Okay, there's not a lot of this stuff is moving to really short-form video content
[00:24:07.840 --> 00:24:13.600]   Yeah, people don't seem to have the patience to read a written technical article anymore
[00:24:13.600 --> 00:24:18.560]   They just wanted another five ten minute. How quick can you get me the you know the information side of it?
[00:24:18.560 --> 00:24:24.920]   We've been heading this way for a long time, haven't we? Yeah? Yeah, well, I mean that was so what was already heading that way for a long time
[00:24:24.920 --> 00:24:26.360]   but
[00:24:26.360 --> 00:24:28.840]   Now we have this piled on top of it, right?
[00:24:28.840 --> 00:24:32.880]   It adds even more pressure which is unfortunate right because that you know that's
[00:24:32.880 --> 00:24:36.000]   It is admittedly it's a niche
[00:24:36.000 --> 00:24:41.520]   But there's there are people and it would like to be more educated about a given technical topic and it's
[00:24:42.000 --> 00:24:46.480]   Handy to be able to Google something and get you know an actual good long-form
[00:24:46.480 --> 00:24:52.160]   Article on that technical thing especially in tech in general right especially like computers and whatnot. I
[00:24:52.160 --> 00:24:57.320]   Mean in a way going back to that white house meeting it actually makes sense to invite the folks
[00:24:57.320 --> 00:25:02.400]   Running Google search engine and being in all those things because if anybody can help
[00:25:02.400 --> 00:25:11.000]   Fill out a lot of the garbage and steer people towards high quality content whether that is I generated I assisted or entirely human-made
[00:25:11.240 --> 00:25:16.200]   it is such an it's right and a lot of them have been trying to use AI and
[00:25:16.200 --> 00:25:21.200]   Being chatbots and whatnot for kind of an interface type of question
[00:25:21.200 --> 00:25:28.600]   But the next step is really like how do you revamp that whole thing to make sure that a lot of the garbage doesn't clock up everybody's results
[00:25:28.600 --> 00:25:35.680]   So you Louise had one suggestion for people which is create your own private spaces
[00:25:36.840 --> 00:25:42.280]   That can't be infiltrated by AI, but then that just creates puts you in a filter bubble, right? You don't get to
[00:25:42.280 --> 00:25:46.000]   See stuff outside of your immediate circle
[00:25:46.000 --> 00:25:52.480]   Yeah, it's definitely not the only solution, but I think it's something that people are gonna increasingly crave depending on how much
[00:25:52.480 --> 00:25:56.480]   AI generated content ends up on social media ends up in some of these places
[00:25:56.480 --> 00:26:00.480]   I think people are gonna want to know that they're like talking to a human
[00:26:00.480 --> 00:26:05.400]   I think you see that a lot already for like other parts of the economy that are automated right like people
[00:26:05.400 --> 00:26:09.320]   Oh, yeah, you want to talk to a human they don't want to talk to a customer service bot
[00:26:09.320 --> 00:26:12.980]   They they value companies that where you can call someone nice who will help you
[00:26:12.980 --> 00:26:19.240]   Or you want things that are handmade hand designed that aren't just you know coming out of some big factory somewhere
[00:26:19.240 --> 00:26:22.320]   artisanal writing
[00:26:22.320 --> 00:26:26.280]   I think that's the way things are going it's like you know
[00:26:26.280 --> 00:26:32.920]   How many times do we think that like you know AI like you know AI generated music for example is not going to eliminate
[00:26:33.080 --> 00:26:37.640]   Musicians it might eliminate some types of I think the labels are worried about that a little bit
[00:26:37.640 --> 00:26:40.880]   They shut that Drake song down real quick. Oh, yeah
[00:26:40.880 --> 00:26:44.880]   I mean I think the IP thing is a huge on you know unanswered question
[00:26:44.880 --> 00:26:51.200]   And it'll be fascinating to see how the Supreme Court and some of the lower courts decide on that because that's I think that's one of the big
[00:26:51.200 --> 00:26:53.280]   Concerns that's keeping a lot of
[00:26:53.280 --> 00:26:56.980]   Some companies at least with maybe like more conservative legal teams
[00:26:56.980 --> 00:27:00.400]   From immediately jumping into this stuff right because they're really worried about
[00:27:01.160 --> 00:27:03.160]   copyright complaints
[00:27:03.160 --> 00:27:09.720]   Yeah, and I don't know of course we have no idea these days how the courts will rule but
[00:27:09.720 --> 00:27:14.720]   In the case of the Drake song it was completely original and
[00:27:14.720 --> 00:27:21.440]   So it wasn't like it was samples, which you know we know the case law supports
[00:27:21.440 --> 00:27:28.720]   Writers you know the creators rights in that case, but it wasn't samples. It was generated
[00:27:29.120 --> 00:27:31.120]   It sounded like Drake
[00:27:31.120 --> 00:27:35.240]   Can he could is he copyrighted his voice? I don't think you can your sound
[00:27:35.240 --> 00:27:41.720]   It's gonna be very interesting. I think the legal experts. I've seen of all kind of said
[00:27:41.720 --> 00:27:47.360]   There isn't really a case for preventing this kind of AI mashup
[00:27:47.360 --> 00:27:54.440]   I mean so impersonation and using his likeness right it's not copyright anymore. It goes into like different areas of law
[00:27:54.440 --> 00:28:00.920]   Maybe that's true a lot of the companies have been using a copula right takedown notices, which are really not
[00:28:00.920 --> 00:28:03.320]   meant for something like this right?
[00:28:03.320 --> 00:28:06.080]   Well, it's good. We live in interesting times
[00:28:06.080 --> 00:28:13.480]   And I guess I the best you could say is just tell your friends and neighbors and people who aren't maybe
[00:28:13.480 --> 00:28:19.840]   Paying attention that watch out because there's a lot of gonna be a lot of there's gonna be a flood of fake content
[00:28:20.560 --> 00:28:25.920]   And real content that people say is fake like you must what I see to around and that's part of the problem too
[00:28:25.920 --> 00:28:28.480]   Isn't that that that's the old Soviet flood the zone?
[00:28:28.480 --> 00:28:34.920]   Technologist technique, which is it doesn't matter if all these bot farms say anything believable or credible
[00:28:34.920 --> 00:28:39.960]   It's just you're flooding the zone with crap so that nobody believes anything even the real stuff
[00:28:39.960 --> 00:28:43.200]   We may already be there
[00:28:43.200 --> 00:28:44.560]   frankly
[00:28:44.560 --> 00:28:50.160]   So Google has been slow to jump on this bandwagon. It's interesting. Sundar Pichai was there
[00:28:50.160 --> 00:28:54.800]   They have barred which is kind of unimpressive Wall Street Journal yesterday
[00:28:54.800 --> 00:28:58.200]   Said Google plans to make search more personal
[00:28:58.200 --> 00:29:04.480]   With a I chat and video clips and there's a word they use that I just loathing but I'm seeing it more and more
[00:29:04.480 --> 00:29:07.440]   snackable
[00:29:07.440 --> 00:29:10.480]   Snackable is the big not great choice
[00:29:10.480 --> 00:29:15.680]   Google plans to make its visual engine its search engine more visual
[00:29:15.680 --> 00:29:17.680]   snackable
[00:29:17.680 --> 00:29:23.960]   Personal and human yeah, go you've got to run into snackable before it sounds like quick sounds like quibi to me
[00:29:23.960 --> 00:29:29.840]   Yes, it takes exactly sounds like I mean Google has started with some of that stuff already right for a lot of
[00:29:29.840 --> 00:29:37.960]   Popular topics when you search you get the blue links on top and then you scroll further down and you get sort of cart type of interfaces
[00:29:37.960 --> 00:29:39.640]   a lot of
[00:29:39.640 --> 00:29:40.840]   colorful
[00:29:40.840 --> 00:29:42.840]   snackable things
[00:29:42.920 --> 00:29:48.680]   What I thought was interesting about that story was that it said it's going to allow you to ask more follow-up questions
[00:29:48.680 --> 00:29:55.120]   which helps people to maybe be a little bit more precise or figure out what they're actually looking for I
[00:29:55.120 --> 00:29:57.320]   Actually kind of want sometimes the opposite
[00:29:57.320 --> 00:30:02.320]   I would like Google to ask me what I'm looking for because maybe I'm searching too broadly
[00:30:02.320 --> 00:30:07.400]   And if Google got back to me and said do you want to bake this cake for yourself or for your dog that might actually be really
[00:30:07.400 --> 00:30:11.080]   Very important get that one right? Yeah
[00:30:11.080 --> 00:30:15.480]   Do you have you ever baked a cake for your dog?
[00:30:15.480 --> 00:30:18.680]   my daughter
[00:30:18.680 --> 00:30:22.080]   We we all tried it was not very good
[00:30:22.080 --> 00:30:25.480]   It's quite cute. I
[00:30:25.480 --> 00:30:30.400]   Didn't know you had another word dog cake recipes, but if there aren't AI will be writing them soon
[00:30:30.400 --> 00:30:37.440]   Google search visitors as you say this is from the journal article might be more frequently prompted to ask follow-up questions or
[00:30:38.280 --> 00:30:44.720]   Swipe through visuals like tiktok videos in response to their course. This is by the way the thing that I hate about
[00:30:44.720 --> 00:30:49.360]   Big tech or just I guess the world will live in
[00:30:49.360 --> 00:30:56.080]   You know what you could blame Hollywood. They keep making sequels. Nobody's trying to make something the next big thing
[00:30:56.080 --> 00:30:58.880]   Google's rushing now. They're scared to death
[00:30:58.880 --> 00:31:03.880]   They're rushing now because Microsoft came out with chat GPT and they're worried
[00:31:03.880 --> 00:31:08.000]   They're gonna lose their advantage and search so they're gonna make search more like that and oh by the way
[00:31:08.000 --> 00:31:14.080]   As long as we're doing this let's make it more like tiktok because that's the kids love that and it's it's it's
[00:31:14.080 --> 00:31:16.120]   chasing your tail
[00:31:16.120 --> 00:31:23.560]   The race to the bottom it ends up being a race to the bottom I think and I I understand Google's nerves on this but
[00:31:23.560 --> 00:31:27.040]   Copying the other because yeah
[00:31:27.040 --> 00:31:30.040]   They gotta do it because other people are doing it and then everybody rushes to do it
[00:31:30.040 --> 00:31:33.760]   And then you don't get a good product out of any of them because they're all just trying to get out the door I
[00:31:34.400 --> 00:31:38.800]   Think that tiktok bit is actually really interesting because what are the things I was
[00:31:38.800 --> 00:31:46.880]   Contemplating when you know this huge search bot wave happened is that we're seeing an increasing number of younger people who are actually using
[00:31:46.880 --> 00:31:54.640]   Tiktok as a search platform and the platform is much better for search than say Instagram or other visual based platforms
[00:31:54.640 --> 00:31:59.800]   Which is not surprising because this you know this transition already happened in China, which is where tiktok is from
[00:32:00.480 --> 00:32:07.880]   So I think that's interesting because you're seeing these companies scrambling to make an AI bot when younger users are signaling
[00:32:07.880 --> 00:32:13.360]   Know what we want is maybe something integrated into a social platform or we want that visual content
[00:32:13.360 --> 00:32:15.000]   We don't want to talk to a chatbot
[00:32:15.000 --> 00:32:18.920]   We want to see normal people who are posting videos because you know when you talk to those users
[00:32:18.920 --> 00:32:23.920]   Why are you using tiktok? It's not just oh they only have the attention span for a video
[00:32:23.920 --> 00:32:30.320]   They want to see normal people who are reviewing something or showing them you know advice on where to go on vacation or
[00:32:30.320 --> 00:32:33.560]   You know immediately showing them a video of that recipe, right?
[00:32:33.560 --> 00:32:36.040]   You don't have to scroll past all the SEO garbage
[00:32:36.040 --> 00:32:40.040]   You know 1,000 word introduction to get to the guacamole recipe or whatever
[00:32:40.040 --> 00:32:43.960]   It's just right in front of you and it's a normal person making it and you can decide whether you trust them
[00:32:43.960 --> 00:32:48.440]   Jamerby just gave me a
[00:32:48.440 --> 00:32:51.640]   Really?
[00:32:51.640 --> 00:32:54.480]   Interesting story. This is back from 1988
[00:32:54.480 --> 00:32:57.160]   You have my audio
[00:32:57.160 --> 00:32:59.560]   See if we've got it
[00:32:59.720 --> 00:33:01.960]   Tom weight sued Doritos
[00:33:01.960 --> 00:33:13.280]   That is not Tom weight sounds exactly like him doesn't it?
[00:33:13.280 --> 00:33:19.680]   It's a Doritos ad where they copied his voice and he had a it has a very distinctive voice
[00:33:19.680 --> 00:33:28.840]   He sued and in 1990 won 2.4 7 5 million dollars in damages from Frida la
[00:33:29.760 --> 00:33:34.440]   So I guess you probably did not sue for copyright infringement. Yeah. Well, yeah, that's it
[00:33:34.440 --> 00:33:37.960]   Voice theft
[00:33:37.960 --> 00:33:42.400]   The voice thief is coming
[00:33:42.400 --> 00:33:48.400]   I mean, you know because they're taking advantage of the thing that is unique to that other persona, right?
[00:33:48.400 --> 00:33:52.600]   You're basically stealing and it's not just I mean it was voice back then sure but
[00:33:52.600 --> 00:33:58.000]   Potentially, you know with all these AI things and just where computing is going in general. What's the stop some?
[00:33:58.280 --> 00:34:03.280]   You know, you could make I just saw this short video short movie that was made where
[00:34:03.280 --> 00:34:10.120]   They basically inserted Spock into a Star Trek themed thing, right?
[00:34:10.120 --> 00:34:16.120]   And it was extremely convincing like how many steps away are you from being able to just take an old actor?
[00:34:16.120 --> 00:34:19.280]   You know, even if they're with us no longer with us doesn't matter
[00:34:19.280 --> 00:34:24.920]   Right and just insert them into a thing and now you just have that person in that thing and if you try to we know there limits
[00:34:24.920 --> 00:34:25.840]   Yeah
[00:34:25.840 --> 00:34:29.000]   So I guess Drake could have sued but who do you sue if an AI?
[00:34:29.000 --> 00:34:32.960]   Generates a song with your voice. Who do you go after?
[00:34:32.960 --> 00:34:35.800]   Right turns into a finger pointing game
[00:34:35.800 --> 00:34:44.080]   The person who commissioned the AI to do it. I don't know the person publishes it maybe yeah, you have to go after the publishing
[00:34:44.080 --> 00:34:50.400]   Which is by the way what they successfully did and I think that's why they used emca because they were able to get it taken down everywhere
[00:34:51.240 --> 00:34:58.200]   Yeah platforms don't respond to anything with the same level of speed that they do a DMCA takedowns, right?
[00:34:58.200 --> 00:35:03.000]   They just they well in fact I think most of the time is just automatic, right?
[00:35:03.000 --> 00:35:06.280]   We get hit by that all the time and even though for instance
[00:35:06.280 --> 00:35:13.840]   I'm taking my my life into my hands by playing that Doritos ad I could get taken down by everybody
[00:35:13.840 --> 00:35:16.120]   Wayne's
[00:35:16.120 --> 00:35:17.520]   Doritos
[00:35:17.520 --> 00:35:23.040]   But it was it's in a commentary and a news story about this topic so it's fair
[00:35:23.040 --> 00:35:26.560]   It's for sure fair use, but that doesn't mean I'm gonna go to court to defend it
[00:35:26.560 --> 00:35:28.880]   Right go to file a counterclaim
[00:35:28.880 --> 00:35:35.000]   Well, this is the problem with YouTube as you know that takes months and by then nobody's gonna listen to show because it's it's old so
[00:35:35.000 --> 00:35:38.760]   Generally, I mean
[00:35:38.760 --> 00:35:44.480]   It's a chilling effect we we go through this all the time and I often just don't play stuff
[00:35:45.160 --> 00:35:48.600]   That was that was I think very germane to the topic, right? But
[00:35:48.600 --> 00:35:52.480]   Yeah, absolutely on my to but that doesn't down the street my will
[00:35:52.480 --> 00:35:57.040]   Disagree. Yeah, that doesn't stop people from you know exporting the system, right?
[00:35:57.040 --> 00:36:03.760]   I mean the flip side then is that we're gonna see or we already have started to see artists
[00:36:03.760 --> 00:36:10.160]   License their voice models, right? So grinds. She said for 50 50 50 right? Yeah
[00:36:10.160 --> 00:36:14.240]   I mean that was more of a late night to eat how much is gonna come out of that as a question, but I
[00:36:14.760 --> 00:36:19.360]   Think John legend Google speakers for a while where he would tell
[00:36:19.360 --> 00:36:22.280]   The Google speaker of weather in his voice
[00:36:22.280 --> 00:36:25.280]   Yeah, and I know it was a deal because it expired
[00:36:25.280 --> 00:36:31.960]   So and then but they only had it for a year or whatever. I have Melissa McCarthy in my Amazon
[00:36:31.960 --> 00:36:39.840]   There you go, but they all all time limit because I think the artists or their managers are smarter now
[00:36:39.840 --> 00:36:43.200]   Yeah, we're gonna keep that sort of limited and in the bottle
[00:36:43.200 --> 00:36:46.960]   You don't want to give them the right and perpetuity to do whatever you have they want with you voice obviously
[00:36:46.960 --> 00:36:52.160]   This is of course this always happens when there's a new technology, you know going back to a Ned Ludd in the looms
[00:36:52.160 --> 00:36:57.680]   When there's a new technology, there's displacement there's disruption
[00:36:57.680 --> 00:37:06.080]   That's what Silicon Valley Herald, you know hails is the disruptive force of innovation and there's always winners and losers there
[00:37:06.080 --> 00:37:08.080]   And obviously AI is about to disrupt
[00:37:09.440 --> 00:37:14.800]   Well, I always ask this question. I'll ask it from you because you all because I haven't asked it of you let yet
[00:37:14.800 --> 00:37:20.760]   But is AI gonna disrupt or is it I often think maybe this is just a trick and
[00:37:20.760 --> 00:37:26.360]   It will just it'll wear off and then we'll do the next big thing
[00:37:26.360 --> 00:37:29.760]   Or is this gonna be totally disruptive?
[00:37:29.760 --> 00:37:35.400]   Maybe it's already is it already disruptive Alan? I think I think it's already
[00:37:36.120 --> 00:37:42.440]   Disrupting on multiple levels multiple angles all simultaneously, right? It's not just the topics we've been talking about today
[00:37:42.440 --> 00:37:43.720]   there were
[00:37:43.720 --> 00:37:46.480]   for you know what month or two ago just a
[00:37:46.480 --> 00:37:52.280]   Photo-based AI generated imagery and all the copyright, you know questions
[00:37:52.280 --> 00:38:00.640]   5.1 mid-journey and you can go on Twitter and see a bunch of it's gotten more and more photo realistic to the point where it's almost
[00:38:00.640 --> 00:38:04.920]   I mean you can easily generate an image that looks like a real right
[00:38:04.920 --> 00:38:08.360]   And not only that you know earlier
[00:38:08.360 --> 00:38:12.280]   Earlier it was mentioned about oh, yeah
[00:38:12.280 --> 00:38:16.720]   Well, we're not sure if the API is open or if the companies are gonna lock down certain things or try to limit it that way
[00:38:16.720 --> 00:38:19.880]   This is computing. This is a compute related thing, right?
[00:38:19.880 --> 00:38:25.400]   How many compute related things start in the cloud but then computing compute in general advances to the point where?
[00:38:25.400 --> 00:38:32.560]   You can just have this running on them on a system in your basement or a few years later on your regular desktop system
[00:38:32.560 --> 00:38:37.480]   You can do it on your phone. Yeah on your phone. Yeah, I mean got mid-journey running on a phone, right?
[00:38:37.480 --> 00:38:41.120]   Someone already was I think yes stable diffusion you can run on your phone as well. Yeah
[00:38:41.120 --> 00:38:44.920]   Right well you can run it, but I'll be it with a very limited
[00:38:44.920 --> 00:38:52.840]   Data set right but as things expand you know the data sets that they just trickle down right all that all that expansion just trickles down over time
[00:38:52.840 --> 00:38:58.040]   It always has um so whether you know, I don't I don't see this
[00:38:58.040 --> 00:39:02.440]   I don't see this just going away because it would be too easy to do it
[00:39:02.440 --> 00:39:06.240]   Willy nilly not too long from now on any devices, right?
[00:39:06.240 --> 00:39:11.080]   Right people always overestimate the short-term impact and underestimate the long-term impacts, right?
[00:39:11.080 --> 00:39:13.240]   Right, and I think so the Drake example
[00:39:13.240 --> 00:39:19.800]   I think that will be sorted out in a way where people follow artists and you know on Spotify they will
[00:39:19.800 --> 00:39:26.520]   Verify those types of things that it's only from the real artists, but then there's a whole other area of the music business like when it comes to
[00:39:27.720 --> 00:39:33.560]   licensing music for films oftentimes it's already where you know people music supervisors are looking for
[00:39:33.560 --> 00:39:36.960]   Something that vaguely sounds like the Beatles, but right now
[00:39:36.960 --> 00:39:41.720]   They still license some British band for that in the future. They might just say oh, you know
[00:39:41.720 --> 00:39:46.640]   Let's just generate something that sounds like a Beatles. Here we go. Yeah, right? Yeah, maybe it turns into not
[00:39:46.640 --> 00:39:50.400]   Making the AI not so much model exactly Drake
[00:39:50.400 --> 00:39:56.240]   But just to be close just close enough, but not close enough just drake is enough
[00:39:56.240 --> 00:39:58.240]   Yeah, just drink is enough, right?
[00:39:58.240 --> 00:40:01.440]   Yeah, I think though
[00:40:01.440 --> 00:40:07.240]   Also a lot of the applications are not going to be the really sexy things that people are talking about right now
[00:40:07.240 --> 00:40:10.320]   And those are not going to be the first things like a lot of the
[00:40:10.320 --> 00:40:16.560]   Applications for this stuff is going to be like really boring administrative work or it's going to be in really high tech sectors like biotech
[00:40:16.560 --> 00:40:20.240]   Where you're going to be seeing this technology use pretty quickly
[00:40:20.240 --> 00:40:26.160]   And I totally agree that you know the short-term impacts are probably being overestimated the long-term impacts
[00:40:26.160 --> 00:40:29.120]   Are being underestimated, but I think you're going to see it kind of come into these
[00:40:29.120 --> 00:40:35.320]   You know just like more boring like, you know, like wrote data entry type things before you're going to see
[00:40:35.320 --> 00:40:39.520]   People flocking to an AI Drake concert or whatever
[00:40:39.520 --> 00:40:45.560]   I think a lot of that stuff people are still going to value a human and I also think I'm trying to remember when you know
[00:40:45.560 --> 00:40:48.720]   These things feel scary that it's not predetermined
[00:40:48.720 --> 00:40:52.720]   You know like we as a culture and as a society get to decide how we're going to use this tech
[00:40:52.720 --> 00:40:57.200]   And I think it's important not to accept whatever the narrative is from opening eye or Google that
[00:40:57.200 --> 00:41:00.440]   You know this is going to transform everything and everyone's going to lose their job
[00:41:00.440 --> 00:41:04.880]   Right like that's still the some degree up to us how we want to use this
[00:41:04.880 --> 00:41:11.120]   And I also think the consumer response is going to be really interesting like I don't know if you guys saw but Amnesty International tried using
[00:41:11.120 --> 00:41:17.200]   AI generated images and a human rights campaign and the backlash to that was really intense
[00:41:18.840 --> 00:41:24.120]   Did they say they were doing it? Yeah, they just closed it and it was like really really bad
[00:41:24.120 --> 00:41:28.120]   I mean it was about you know, Colombian. It was about Columbia
[00:41:28.120 --> 00:41:30.640]   civil rights movements there and
[00:41:30.640 --> 00:41:36.760]   They had this woman who was you know being taken away by by police or whatever and shit a Colombian flag and it wasn't the Colombian flag
[00:41:36.760 --> 00:41:38.720]   Like they got it wrong, of course
[00:41:38.720 --> 00:41:44.160]   And that was the first image and it's just like so they didn't have to disclose it because it was kind of obvious
[00:41:44.760 --> 00:41:51.720]   All the refugees have six fingers. Yeah, well, they've fixed by the way. They've pretty much fixed the six finger problem
[00:41:51.720 --> 00:41:58.040]   Here's somebody's using the latest mid journey to generate pictures of presidents as football coaches
[00:41:58.040 --> 00:42:01.800]   And the hands are the hands are pretty good
[00:42:01.800 --> 00:42:08.840]   It's you know, I mean, I don't know. Would you be fooled? Maybe you would some of them were pretty obvious
[00:42:08.840 --> 00:42:11.320]   I might believe that one of Bill Clinton
[00:42:13.600 --> 00:42:15.960]   Reagan's aren't very good Carter's are
[00:42:15.960 --> 00:42:20.280]   Quality there's a feeling quality isn't there. Yeah. Yeah
[00:42:20.280 --> 00:42:24.560]   LBJ is a I don't know. I believe L. B. J. Is a football coach
[00:42:24.560 --> 00:42:28.480]   You're right. There's an uncanny valley. I guess still, huh?
[00:42:28.480 --> 00:42:32.760]   For now at least yeah, yeah, we're getting we're getting better and better and better though
[00:42:32.760 --> 00:42:37.680]   I mean, so it's obvious it's getting better very rapidly. Yeah is the thing. Yeah, right?
[00:42:37.680 --> 00:42:43.720]   This isn't like think about how video game uncanny valley has progressed. It's taken years to get
[00:42:43.720 --> 00:42:48.920]   Really super realistic video game engine graphics, right? But this the AI stuff
[00:42:48.920 --> 00:42:53.040]   It's this this is not that many generations to get to yeah, I know that's pretty funny
[00:42:53.040 --> 00:42:56.660]   The Herbert Hoover is college football coaches actually
[00:42:56.660 --> 00:43:03.960]   The only thing is it's too good quality like it would it should be older. Yeah, this one's believable. So you can see
[00:43:04.880 --> 00:43:07.280]   That it's only that's Woodrow Wilson
[00:43:07.280 --> 00:43:13.960]   Warren G Harding as a college football coach. I don't even have college football. How William Howard Taft
[00:43:13.960 --> 00:43:17.160]   There's some of these are quite believable. I
[00:43:17.160 --> 00:43:21.680]   Don't know. I think we're gonna live in it. We're living in a time now where you can no longer
[00:43:21.680 --> 00:43:25.240]   Yeah, I mean trust what you see
[00:43:25.240 --> 00:43:31.880]   Well, and it is being used behind the scenes a lot just not we don't know it the things that we're noticing here
[00:43:31.880 --> 00:43:38.240]   Well, no, there's I mean programmers programmers have adopted. Yeah extremely quickly. Is it pretty so Alan your code?
[00:43:38.240 --> 00:43:40.240]   You're still a coder. Yeah, I
[00:43:40.240 --> 00:43:43.600]   Dabble, but here's the thing I've I've always coded
[00:43:43.600 --> 00:43:48.600]   Out of necessity, right? I need to do a thing. Okay. Let me learn what I need to learn
[00:43:48.600 --> 00:43:52.960]   You know, I have to I have I know where to look to find the answer right and
[00:43:52.960 --> 00:44:01.860]   You know someone who's much more skilled than me at programming those folks are still using sure the chat GBT's to take care of the
[00:44:01.860 --> 00:44:06.400]   More mundane things and they're the experts and they can yeah, they act as a co-pilot, right?
[00:44:06.400 --> 00:44:09.140]   And it just I just need this thing that does this thing
[00:44:09.140 --> 00:44:14.660]   I can sit here and take 10 minutes do it or I can take you know five seconds to just get that spit out for me
[00:44:14.660 --> 00:44:16.140]   And I know enough about it
[00:44:16.140 --> 00:44:20.660]   Well, I can very quickly review it and and know if it's gonna work or not and tweak it as necessary, right?
[00:44:20.660 --> 00:44:25.760]   It fills in those gaps, but for someone that's at my skill level obviously lower than those experts
[00:44:25.760 --> 00:44:29.340]   But still I know what I want it to do. I know what the system can do
[00:44:29.980 --> 00:44:36.180]   I can at least get 90% of the way there very quickly and if anything learn something along the way
[00:44:36.180 --> 00:44:38.380]   Right like I didn't know how to do that function
[00:44:38.380 --> 00:44:42.380]   But I knew what I wanted done and it's teaching me and at the same time
[00:44:42.380 --> 00:44:47.380]   I'm able to fill in whatever couple of holes that there may be you know to make it work to make it run in the long run
[00:44:47.380 --> 00:44:50.340]   Isn't that what's gonna be the the most useful?
[00:44:50.340 --> 00:44:53.020]   Way to use AI is as a
[00:44:53.020 --> 00:44:59.220]   Collaborator to work in hand in hand with a human not by itself, but hand in hand is that right?
[00:44:59.860 --> 00:45:01.860]   Yeah, and what I've noticed is
[00:45:01.860 --> 00:45:08.300]   What we've just discussed about the fingers and how okay that was wrong so obviously some obviously humans intervened and
[00:45:08.300 --> 00:45:12.740]   Got you know they trained in my crazy on hands. Yeah, and they've trained it like crazy on it
[00:45:12.740 --> 00:45:21.580]   Yeah, made some choices there right the same kind of thing can be applied totally different AI type thing now like Tesla auto pilot for example, right?
[00:45:21.580 --> 00:45:25.660]   The recent versions of autopilot I've noticed
[00:45:26.020 --> 00:45:32.380]   Tend to kind of stay on the opposite and the lane as semi trucks. Oh interesting
[00:45:32.380 --> 00:45:36.300]   I don't think it's doing it because it did it on its own
[00:45:36.300 --> 00:45:43.500]   I'm saying I'm saying avoid those guys. Yeah, it's it's it's a very diss if you're if you own a Tesla on any of the recent firmware
[00:45:43.500 --> 00:45:46.660]   You'll notice it is extremely distinct. It is very purposeful
[00:45:46.660 --> 00:45:51.420]   You're like one car length away from the semi it moves over a little and as soon as you pass its front bumper
[00:45:51.420 --> 00:45:53.500]   It's right back in the middle of the lane, right?
[00:45:53.500 --> 00:45:59.060]   And that's that was very perp, you know, you can tell it's human had to intervene. It didn't figure that out all by itself
[00:45:59.060 --> 00:46:03.860]   I'm sorry, right? It was a very specific programmatic sort of intervention by humans
[00:46:03.860 --> 00:46:08.380]   So you're not just be able to let this thing go you have to like you know intervene a little and teach
[00:46:08.380 --> 00:46:13.200]   You saw was say he doesn't like Tesla self-driving. It's tried to kill him on several occasions
[00:46:13.200 --> 00:46:17.380]   It has it has tried to kill me as well
[00:46:18.180 --> 00:46:23.780]   I haven't had the recent one I had it on my model X and it there was a curve there was a curve on the highway
[00:46:23.780 --> 00:46:29.140]   It always tried to drive into the divider every single time. You just learn. I will say let it do that
[00:46:29.140 --> 00:46:30.940]   well
[00:46:30.940 --> 00:46:36.860]   True, I mean you and I and many other tech folks like to be on the bleeding edge and you know some of us like to sort of lean into
[00:46:36.860 --> 00:46:42.660]   Like investing something yeah, see when it turns into your actual safety though. It's a little yeah, it's a little dicey
[00:46:42.660 --> 00:46:45.900]   But yeah, I've been surprised by
[00:46:46.660 --> 00:46:49.380]   how you know before
[00:46:49.380 --> 00:46:56.340]   When I used to try it every once in a while it would scare the heck out of me and now it's like oh this is
[00:46:56.340 --> 00:47:01.460]   They might be getting close. It's doing things that I would have done normally and that's kind of surprising me
[00:47:01.460 --> 00:47:07.100]   I remember interviewing Ashley Vance. This is back before Elon was an evil genius and was still a genius genius
[00:47:07.100 --> 00:47:12.940]   About his book about Elon Musk and one of the things he said which I think was very true is that
[00:47:13.180 --> 00:47:20.980]   The difference between Tesla say and Ford was that Teslas being run like a Silicon Valley company where you iterate you try you fail and try again
[00:47:20.980 --> 00:47:28.980]   You iterate and Ford is much in them all the you know traditional car companies were much more judicious and cautious
[00:47:28.980 --> 00:47:35.080]   But maybe we maybe with when it comes to AI we need to be a little bit
[00:47:35.080 --> 00:47:42.540]   Loose with the I mean certainly looks like Microsoft traditionally a very cautious company has thrown caution to the wind and said
[00:47:42.540 --> 00:47:49.060]   Let's put this out and see what let's just see what happens with chat GPT or you know being in chat
[00:47:49.060 --> 00:47:56.300]   That was that that was the loosest I think I've ever seen like yeah go on a thing ever yeah, but maybe that's what is that what we need
[00:47:56.300 --> 00:47:59.620]   For AI to succeed. I mean
[00:47:59.620 --> 00:48:02.180]   I don't know
[00:48:02.180 --> 00:48:09.220]   Woz by the way signed that letter there were a number of people signed a letter saying we should including Musk I believe we should pause six months
[00:48:10.260 --> 00:48:14.420]   Which I always interpret is like wait six months so we can catch up and then you can continue
[00:48:14.420 --> 00:48:18.740]   Just give it we just need a heads just a little bit head start
[00:48:18.740 --> 00:48:22.380]   I mean a right Ilana is what recruiting people from deep mind
[00:48:22.380 --> 00:48:32.140]   Right he wants to he's not gonna pause let's put it that way right AI as a fundamental concept is in
[00:48:32.140 --> 00:48:39.580]   Start-up mode. Yes, they're in go fast. Yes. Yes everywhere. Is that risky. Yeah. Yes. I mean
[00:48:39.980 --> 00:48:44.820]   As long as it's not steer in my car. I guess what if it's steering an election
[00:48:44.820 --> 00:48:50.820]   Yes, that's that's the worry right what if you know what it on the head
[00:48:50.820 --> 00:48:57.300]   You know what if the results in 2024? I don't mean like AI will vote
[00:48:57.300 --> 00:49:00.540]   But AI disinformation might prove
[00:49:00.540 --> 00:49:04.620]   It's your Ken influence
[00:49:04.620 --> 00:49:06.740]   Yeah
[00:49:06.740 --> 00:49:09.680]   Did you guys see the join a star in video in the Wall Street Journal where she
[00:49:09.680 --> 00:49:12.940]   Her boys she fooled her bank, right?
[00:49:12.940 --> 00:49:17.540]   I think I think Joseph Cox at mother board did that as well and they were able to fool banks
[00:49:17.540 --> 00:49:22.180]   I mean that's the kind of stuff that I worry about in the short term is like just you know a
[00:49:22.180 --> 00:49:27.500]   Serious increase I told my mom that like we have to have a safe word because if you think I'm calling you like
[00:49:27.500 --> 00:49:31.660]   No, ask the work because I feel like she fooled the bank and her family
[00:49:32.220 --> 00:49:36.540]   Yeah, exactly and I think that that kind of stuff. That's what I'm really terrified about like
[00:49:36.540 --> 00:49:39.700]   I feel like disinformation sometimes can be sort of a nebulous concept
[00:49:39.700 --> 00:49:44.540]   And like what do we mean? What does it look like right like I'm not saying that it's not a danger
[00:49:44.540 --> 00:49:48.820]   But when you see that kind of thing, it's like okay, they can get into your bank account. It's sort of game over I
[00:49:48.820 --> 00:49:55.020]   Mean yeah, but social engineering has existed for a long time people have been playing tricks on banks and
[00:49:55.020 --> 00:50:02.180]   So I think what all of those leads to is just we need better security across the board again
[00:50:02.340 --> 00:50:09.100]   malicious actors whether they humans or AI or a combination of those things stuff just needs to be more secure and
[00:50:09.100 --> 00:50:13.740]   Maybe I will bring us there. Maybe that would be a good good thing good outcome
[00:50:13.740 --> 00:50:17.340]   When you when you combine the potential, you know
[00:50:17.340 --> 00:50:23.020]   That what we mentioned at the beginning of this where we were talking about the reviews and all the reviews spam and everything
[00:50:23.020 --> 00:50:28.120]   you know take the malicious intent of those folks with the power of AI and
[00:50:29.300 --> 00:50:37.140]   Someone who knows social engineering who would typically be a human who was trying to social engineer their way into your bank account now
[00:50:37.140 --> 00:50:39.140]   Back that up with AI. Oh
[00:50:39.140 --> 00:50:45.100]   My goodness, right like that's extremely powerful. That's it's it's learning
[00:50:45.100 --> 00:50:48.340]   It's doing up. It's taking a brief few minutes to Google everything
[00:50:48.340 --> 00:50:55.620]   It could possibly about you and come up with much more solid guesses about any possible security question answer
[00:50:55.860 --> 00:51:02.780]   Right just in an instant, you know potentially on the fly as the call is happening, right? That's you know
[00:51:02.780 --> 00:51:07.540]   That's dangerous, right? We need much better. I mean everything should be
[00:51:07.540 --> 00:51:10.940]   Two factor even when they're talking to you on the phone
[00:51:10.940 --> 00:51:16.580]   And I don't mean like some some companies try to do a second factor to add security, but
[00:51:16.580 --> 00:51:23.540]   I've lost count how many times I didn't have my second factor for example to make changes to like an AT&T
[00:51:23.660 --> 00:51:26.820]   Account or something like that and you add the pin code
[00:51:26.820 --> 00:51:32.340]   But then if you don't know the pin code they can just turn it off right there on the call for you
[00:51:32.340 --> 00:51:38.260]   Right? Maybe they ask your birthday or something so that it doesn't you know it didn't do any good, right?
[00:51:38.260 --> 00:51:42.380]   It's in and it's certainly not gonna get the better of an AI trying to get in
[00:51:42.380 --> 00:51:48.300]   I know we're in deep
[00:51:49.820 --> 00:51:56.940]   Fast yeah, well it means but it means that all of the people on the other end of those transactions need to be on the bus
[00:51:56.940 --> 00:52:02.860]   Right they need to have well which they're not I mean I remember when I'm sure Schwab said oh, yeah
[00:52:02.860 --> 00:52:06.940]   You can use your voice to log in and I thought this doesn't seem like a good idea
[00:52:06.940 --> 00:52:14.420]   Well now no, it's not right terrible idea. Do you have safe words yanko for your family? I
[00:52:14.420 --> 00:52:18.300]   Don't know if you want to go there
[00:52:18.300 --> 00:52:24.380]   I actually I'll have them no I talked about this. I said, you know if you hear from me
[00:52:24.380 --> 00:52:30.620]   And so we worked out a scheme. I don't want to say too much about that, but
[00:52:30.620 --> 00:52:33.300]   Don't give up the password to your luggage Leo
[00:52:33.300 --> 00:52:39.500]   I do have to tell a story a couple years ago something happened to me that no way
[00:52:39.500 --> 00:52:44.980]   I was involved in this at all, but I got a call I had an array number at that time and it was a scam call
[00:52:44.980 --> 00:52:47.180]   but somebody pretended to be
[00:52:47.900 --> 00:52:50.540]   Somebody pretended that they had abducted a family member
[00:52:50.540 --> 00:52:56.860]   Yeah, basically somebody was screaming in the background calling my name and I could figure out really quickly
[00:52:56.860 --> 00:52:59.420]   That was not real because I wasn't living in LA anymore
[00:52:59.420 --> 00:53:06.540]   They didn't know the name of the family number which was sort of important detail in a hostess negotiation situation
[00:53:06.540 --> 00:53:15.220]   But I just imagine at the time for people who may be coming for countries where abductions are more common or
[00:53:15.700 --> 00:53:19.260]   Just in different situations where that could be a real situation
[00:53:19.260 --> 00:53:26.700]   So that could be really crazy now you had a high to it obviously gets even scarier my voice now sounds like it as well
[00:53:26.700 --> 00:53:30.900]   I think it was stories about that recently to where people have been using the very same scam
[00:53:30.900 --> 00:53:37.220]   But with simulated voices it happened to one of our regulars on our shows Larry maggot
[00:53:37.220 --> 00:53:45.660]   Last week published this article in the San Jose Mercury News how I nearly fell for a frightening virtual kidnapping scheme
[00:53:46.660 --> 00:53:53.340]   Oh, good 20 minutes of terror now. This is a guy by the way who's as technical as we are
[00:53:53.340 --> 00:53:58.540]   He actually is the founder of an online safety organization connect safely org
[00:53:58.540 --> 00:54:05.380]   He said the call felt real to me and threatened to separate me not from not from money
[00:54:05.380 --> 00:54:08.020]   But from someone it was his wife. They were used to
[00:54:08.020 --> 00:54:10.860]   simulated voice
[00:54:10.860 --> 00:54:16.140]   He says it began Tuesday morning when my wife drove to San Francisco to visit a friend most days she and I are together at home
[00:54:16.140 --> 00:54:17.780]   Or she's somewhere nearby
[00:54:17.780 --> 00:54:20.820]   He says I was already a little nervous because you know
[00:54:20.820 --> 00:54:24.740]   Last time she drove to San Francisco. She was side-swiped by another car
[00:54:24.740 --> 00:54:30.780]   1237 my cell phone rang with a caller ID that at first glance looked like it was my wife I
[00:54:30.780 --> 00:54:36.820]   Now know it wasn't exactly her numbers same area code same prefix same last number. So it was all but three
[00:54:38.100 --> 00:54:45.220]   At the time, you know, that's that's her because he was worried, right when I heard what I heard was a crying upset woman I
[00:54:45.220 --> 00:54:50.180]   Assume it was my wife because it came from her phone this happened to him last week by the way
[00:54:50.180 --> 00:54:53.420]   I repeatedly asked what the matter was but I couldn't understand her answer
[00:54:53.420 --> 00:54:57.300]   I could imagine was something horrible must have happened then a man came on the phone
[00:54:57.300 --> 00:55:05.420]   Identified himself as a police officer. I'm telling this is worth reading and knowing about and telling family members about to understand this scam goes on
[00:55:05.700 --> 00:55:09.740]   He said he said I'm a cop. I need to verify who I'm talking with
[00:55:09.740 --> 00:55:15.980]   He said well normally I'm gonna refuse a request, but I felt desperate. I gave my name my wife's name
[00:55:15.980 --> 00:55:21.500]   He then admitted he wasn't a police officer, but a member of a drug cartel
[00:55:21.500 --> 00:55:24.620]   Now
[00:55:24.620 --> 00:55:31.060]   Now this is a nightmare. This is a nightmare, and I know Larry. He's sharp. He's like us. He's not gonna fall for this stuff and
[00:55:31.060 --> 00:55:34.340]   Said that he had my wife with him in San Francisco
[00:55:34.340 --> 00:55:40.900]   I don't know how he says he knew she was in San Francisco. Maybe I I don't know maybe said something about our location
[00:55:40.900 --> 00:55:42.900]   Maybe not I wasn't thinking clearly
[00:55:42.900 --> 00:55:47.580]   A security expert I spoke with later explained the technique start with fear
[00:55:47.580 --> 00:55:52.620]   Follow with an authority figure to gain your trust
[00:55:52.620 --> 00:55:57.340]   The cop and then pivot to the threat. I mean this is like a pattern
[00:55:57.340 --> 00:56:02.460]   He says it worked with me even though I primed myself on critical thinking skills
[00:56:03.100 --> 00:56:06.300]   There was enough information to cause me to worry it could be the real thing either way
[00:56:06.300 --> 00:56:08.140]   I knew I was dealing with a criminal
[00:56:08.140 --> 00:56:13.660]   So while I had him on my cell phone he had at least did this I put him on speaker and dialed 911 for my desk phone
[00:56:13.660 --> 00:56:21.140]   I didn't say anything to the 911 operator, but knew they would hang on and listen to the call at one point
[00:56:21.140 --> 00:56:28.780]   The caller told me she'll be okay. Just bring five thousand dollars to a Walmart parking lot in San Jose
[00:56:28.780 --> 00:56:33.220]   I found out later the 911 operator put my local police on the line
[00:56:33.220 --> 00:56:37.100]   So they heard nearly the entire call including the ransom demand and the threat
[00:56:37.100 --> 00:56:40.020]   He told me not to contact anyone asked me if I was alone
[00:56:40.020 --> 00:56:45.420]   I said I was at one point the 911 operator asked me for my wife's phone number, which I whispered into the desk phone
[00:56:45.420 --> 00:56:48.660]   The scammer heard me asked who I was talking to I denied
[00:56:48.660 --> 00:56:53.180]   I was speaking to anyone just said I was talking to myself because I was nervous he then asked me to get in my car I
[00:56:53.180 --> 00:56:56.140]   Didn't but kept him on the phone as long as I could
[00:56:56.660 --> 00:57:02.980]   The car request was to initiate the next step in this the compliance process once you follow one instruction
[00:57:02.980 --> 00:57:07.900]   You're more likely to complete the process and pay the ransom, right?
[00:57:07.900 --> 00:57:11.180]   Did they actually want them to go to the Walmart parking lot?
[00:57:11.180 --> 00:57:14.380]   Yeah, he says we spoke for over 11 minutes
[00:57:14.380 --> 00:57:17.260]   He finally hung up on me probably because he realized I wasn't gonna comply
[00:57:17.260 --> 00:57:23.540]   So anyway, you know, we initially fell for it that is extremely quick thinking to call 911
[00:57:23.980 --> 00:57:27.300]   Yeah, yeah, I mean it's you know again. I would say
[00:57:27.300 --> 00:57:33.100]   Larry is no more vulnerable than any of us here, you know, he's just he's sad right right
[00:57:33.100 --> 00:57:38.980]   So the police came to his house the officer said it's likely a scam
[00:57:38.980 --> 00:57:42.140]   He did say call your wife. There was no answer
[00:57:42.140 --> 00:57:48.580]   He called again and she answered she's fine never in danger the police had texted my wife and
[00:57:49.460 --> 00:57:54.980]   They contacted sftd to dispatch an officer to look for her. So they they took this very seriously
[00:57:54.980 --> 00:57:59.580]   Wow, so we're she doesn't bless flian aware driving
[00:57:59.580 --> 00:58:06.900]   Yeah, she had no idea, right? So it's this article. It seems to be like a public service announcement. Yeah, well
[00:58:06.900 --> 00:58:08.900]   He put it to San Jose Mercury News. I encourage
[00:58:08.900 --> 00:58:14.400]   People read it. I guess they didn't it wasn't really an impersonation of his wife's voice
[00:58:14.400 --> 00:58:18.540]   It was just a scared female voice, but it was enough for and it was close
[00:58:18.540 --> 00:58:23.680]   They must they knew something about him and this is by the way one of the things is really scary these days is
[00:58:23.680 --> 00:58:30.600]   There's enough information out there with data brokers and elsewhere bad guys are able to
[00:58:30.600 --> 00:58:33.960]   Learn enough about you to really kind of fool you
[00:58:33.960 --> 00:58:39.760]   They almost knew his wife's phone number somehow they knew she was in San Francisco or I guess she was in San Francisco
[00:58:39.760 --> 00:58:42.240]   We had a scam happen at work
[00:58:42.880 --> 00:58:48.160]   Where somebody impersonated our CEO and sending messages to everybody not to me?
[00:58:48.160 --> 00:58:51.360]   But to all the employees all her direct reports
[00:58:51.360 --> 00:58:56.240]   And fortunately our staff was smart enough they didn't do anything about it
[00:58:56.240 --> 00:59:02.920]   But I can imagine what the next steps would have been and and this is because because somehow our company structure corporate structure
[00:59:02.920 --> 00:59:08.280]   The names of our employees their emails or phone numbers and at least as email and phone number were available to a bad guy
[00:59:08.280 --> 00:59:12.240]   Who then was able to implement that attack? This is how spearfishing happens now
[00:59:12.240 --> 00:59:14.240]   This is very
[00:59:14.240 --> 00:59:19.840]   Scary stuff and it's your fishing is a tale as old as time. Well, and it you know
[00:59:19.840 --> 00:59:25.680]   For a long time I kind of and I still kind of feel this way like I don't have any privacy fine
[00:59:25.680 --> 00:59:28.640]   So what what's the worst they can do? Well now we're starting to see
[00:59:28.640 --> 00:59:32.640]   That maybe privacy is important for security
[00:59:32.640 --> 00:59:35.480]   If nothing else
[00:59:35.480 --> 00:59:37.640]   Yep
[00:59:38.000 --> 00:59:45.280]   Good news the bad guys. I mean the good guys the good guy bad guys the hackers are gonna are gonna take this on Defcon
[00:59:45.280 --> 00:59:50.760]   Which is coming up this summer in Las Vegas is gonna have an AI village?
[00:59:50.760 --> 01:00:00.360]   Where they've invited hackers to show up dive in find bugs and biases in large language models from open AI Google anthropic and others
[01:00:00.360 --> 01:00:03.160]   they expect thousands
[01:00:03.160 --> 01:00:05.880]   to kind of red team
[01:00:06.480 --> 01:00:09.360]   These AI large language models
[01:00:09.360 --> 01:00:14.120]   So, you know, this has worked very well in the past Fcons had that remember they had the election
[01:00:14.120 --> 01:00:18.000]   machine village where people were as presented with the
[01:00:18.000 --> 01:00:23.120]   electronic voting machines and were able to show how they were hackable and where the
[01:00:23.120 --> 01:00:29.640]   And then didn't they try and sue like the election companies tried to sue. I think Dominion Dominion wasn't too happy as I remember
[01:00:29.640 --> 01:00:33.480]   Anyway, this is good right there
[01:00:34.680 --> 01:00:37.040]   They're not just looking they're gonna look for bias
[01:00:37.040 --> 01:00:40.680]   Hallucinations and jail breaks
[01:00:40.680 --> 01:00:42.520]   I think that's awesome
[01:00:42.520 --> 01:00:47.960]   I mean, I don't know if you guys saw but the other day opening eye launched a bug bounty and I kind of put it, you know
[01:00:47.960 --> 01:00:49.960]   Put the link somewhere to
[01:00:49.960 --> 01:00:54.520]   Write about it in my newsletter the next day and by the time I went back to look at it
[01:00:54.520 --> 01:00:57.080]   They had already paid out like dozens of bugs. Oh, geez
[01:00:57.080 --> 01:00:59.000]   Within like, you know 24 hours
[01:00:59.000 --> 01:01:02.560]   So it showed you that like, you know, I think this kind of red teaming is really important
[01:01:02.560 --> 01:01:06.080]   If you go down to like on the bug crowd website, we'll show you
[01:01:06.080 --> 01:01:09.000]   How many bugs they've already paid out
[01:01:09.000 --> 01:01:14.160]   It should be there. Yeah, that's the bug crowd link. Yeah, so you can see on the right they've already
[01:01:14.160 --> 01:01:17.360]   Rewarded 30
[01:01:17.360 --> 01:01:19.040]   vulnerabilities
[01:01:19.040 --> 01:01:25.280]   Yeah, and you know, that's pretty that's pretty strict and they're also not considering bias or hallucinations or anything
[01:01:25.280 --> 01:01:27.840]   So that's just literally like security problems with the platform
[01:01:29.360 --> 01:01:34.400]   Well, that's the first thing people start doing with chat GPTs is hacking on it
[01:01:34.400 --> 01:01:41.200]   Yeah, I remember none of the good work that those folks are doing are going to stop people from using the AI from nefarious purposes
[01:01:41.200 --> 01:01:44.160]   Right, just point that out. It'll do it more securely
[01:01:44.160 --> 01:01:45.280]   Right
[01:01:45.280 --> 01:01:48.800]   Well, at least yeah, I'm hoping at least people's chat histories are not leaked
[01:01:48.800 --> 01:01:50.720]   You know, we saw that there was already that one
[01:01:50.720 --> 01:01:54.800]   Uh big bug that they found where people could access other people's chat histories
[01:01:54.800 --> 01:01:56.960]   And you know, there's already been stories coming out showing that
[01:01:57.360 --> 01:02:02.640]   People are using this chat bot as a therapist, right? You know, it's it's really worrisome
[01:02:02.640 --> 01:02:07.840]   What what you know that chat history could particularly look like i'm also that reminds me
[01:02:07.840 --> 01:02:09.920]   I'm super interested to see
[01:02:09.920 --> 01:02:14.400]   Will those conversations be used for targeted advertising and how soon?
[01:02:14.400 --> 01:02:18.560]   You know, we already saw snapchat said that they were going to use the outputs from their bot
[01:02:18.560 --> 01:02:23.120]   Or the inputs and the outputs from their their AI bot for advertising
[01:02:23.200 --> 01:02:29.040]   And that's going to be really weird if you have a therapy session with a chat bot and then you see ads based on your therapy session
[01:02:29.040 --> 01:02:34.320]   Yeah, well, this is why uh, microsoft is now offering according to the information
[01:02:34.320 --> 01:02:39.120]   uh professional chat gpt for companies cost 10 times more
[01:02:39.120 --> 01:02:42.720]   But it will be 10 times 10 times more
[01:02:42.720 --> 01:02:45.280]   but uh, which and you know
[01:02:45.280 --> 01:02:52.560]   At least that your inform your corporate information won't leak out sam's son's already told its employees do not you may not use
[01:02:53.120 --> 01:02:56.720]   Uh, an AI to help you with code or anything else. They're worried about leaks
[01:02:56.720 --> 01:03:04.640]   Um, amazon as well and sunset only use our tool. Yeah, you can leak to us. It's okay. Just don't leak to anybody else
[01:03:04.640 --> 01:03:09.200]   All right, let's take a little break. What's your what's your uh, uh
[01:03:09.200 --> 01:03:12.880]   newsletter louis i didn't know you had a newsletter is it through semaphore?
[01:03:12.880 --> 01:03:18.640]   Yeah, so it comes out twice a week. I write it with my colleague read albregati. It's just called semaphore tech
[01:03:19.040 --> 01:03:23.120]   Oh good. Yeah, reads great. Yeah, they've been he's been on the show as well
[01:03:23.120 --> 01:03:31.040]   Semaphore semaphore is great and what is the uh cost of this newsletter this fine? It is absolutely free. Well, I can't believe that
[01:03:31.040 --> 01:03:38.800]   I don't understand the the uh, by the way, I also said this to the folks at protocol. I don't understand the financial
[01:03:38.800 --> 01:03:41.920]   Well, sorry
[01:03:41.920 --> 01:03:48.400]   Well, we have a great uh, we have a great sales team and there are ads in the newsletter. I will say so
[01:03:48.720 --> 01:03:51.120]   Because there aren't ads on the front pages for as I can tell
[01:03:51.120 --> 01:03:58.240]   Yeah, they're in the newsletters. Okay. Good. There you go. So you're supporting the fine semaphore by subscribing
[01:03:58.240 --> 01:04:04.560]   I just subscribed to a yankos new a newsletter low pass filtering the future
[01:04:04.560 --> 01:04:07.360]   uh low pass dot cc
[01:04:07.360 --> 01:04:14.640]   So thank you yankos $80 a year eight bucks a month and uh, i've been a fan of your writing
[01:04:15.120 --> 01:04:19.120]   And your byline every step of the way so i'm glad to get your stuff still
[01:04:19.120 --> 01:04:22.960]   Thank thank you. One more subscriber right there right there
[01:04:22.960 --> 01:04:25.760]   And uh allen, what's your newsletter?
[01:04:25.760 --> 01:04:31.360]   I got out of that beat like four years ago. Don't do that anymore
[01:04:31.360 --> 01:04:33.280]   For you
[01:04:33.280 --> 01:04:39.600]   Listen, I I miss the heck out of it though. It's just that I do all my storage testing internally now
[01:04:40.400 --> 01:04:43.440]   That's too bad because you really were the best resource for ssd
[01:04:43.440 --> 01:04:50.720]   Information and knowledge at pc perspective and we miss kind of really on that but I really you have no idea how much i'm
[01:04:50.720 --> 01:04:55.600]   Estu on that. Yeah, it's just i got a first lights on buddy. You built a
[01:04:55.600 --> 01:04:57.680]   bass array
[01:04:57.680 --> 01:04:59.680]   Discare of ssd's
[01:04:59.680 --> 01:05:03.520]   Right in the beginning ssd's right. How many how big was it how many?
[01:05:03.520 --> 01:05:09.920]   ssd's early on oh man. I did like four of those x25m's back in the day back when it was
[01:05:10.000 --> 01:05:12.480]   I thought you had like hundreds of drives on that thing
[01:05:12.480 --> 01:05:16.720]   Well, I'd have hundreds of drives on my basement. Yeah, but they're not all
[01:05:16.720 --> 01:05:18.560]   Okay
[01:05:18.560 --> 01:05:24.880]   That's just because i'm a crazy person and it just keeps getting bigger and bigger so company is solid i'm sos i d i gm
[01:05:24.880 --> 01:05:31.200]   And uh, yes, allen is always a welcome on this show. I want to tell you a little bit about my new uh health
[01:05:31.200 --> 01:05:34.960]   Regimen, you know, we went went to uh on our vacation
[01:05:35.680 --> 01:05:40.320]   I just said this is it. I am not I started, you know, you get the pill miners and the vitamins and you
[01:05:40.320 --> 01:05:48.240]   You know, we're gonna be gone for like 24 days and that but that's a lot of pill miners. That's when I found this ag
[01:05:48.240 --> 01:05:50.880]   one
[01:05:50.880 --> 01:05:53.760]   And it has really simplified my life like many of course
[01:05:53.760 --> 01:05:59.840]   I I want to supplement with uh with you know the appropriate dietary supplements
[01:05:59.840 --> 01:06:01.120]   But I don't want to
[01:06:01.120 --> 01:06:07.760]   More pills to swallow and I want to sacrifice my taste buds when I found ag one I found something delicious that gives my body
[01:06:07.760 --> 01:06:12.080]   What it craves all in one daily nutritional drink now I started this this morning
[01:06:12.080 --> 01:06:16.080]   So I just saved a little bit just to prove to you this tastes delicious
[01:06:16.080 --> 01:06:25.040]   It's really good. In fact, I've started to crave the taste of it. I really love it. But look what's in here. I mean everything you
[01:06:25.040 --> 01:06:30.640]   Need it's the best all-in-one solution for daily nutrition and by the way, there's no flash in the pan
[01:06:31.280 --> 01:06:33.120]   ag one was founded in 2010
[01:06:33.120 --> 01:06:36.720]   Millions of people as I've been talking about this I've heard you know, I
[01:06:36.720 --> 01:06:43.040]   Approoted. Oh, yeah, I've been doing that for years ag one is the best all-in-one solution for daily nutrition. It saves you time confusion
[01:06:43.040 --> 01:06:46.000]   Saves your money each serving less than three bucks a day
[01:06:46.000 --> 01:06:51.600]   Individual supplements can cost a heck of a lot more. You don't have to tell me that
[01:06:51.600 --> 01:06:58.160]   Plus ag one's daily habit gives you long-term powerful results. It's not just vitamins and minerals
[01:06:58.720 --> 01:07:04.800]   It's prebiotics. It's probiotics 75 high quality minerals vitamins pre and probiotics
[01:07:04.800 --> 01:07:07.760]   One scoop of ag one in the morning covers your entire
[01:07:07.760 --> 01:07:11.440]   daily nutritional basis eat this fill this bottle up
[01:07:11.440 --> 01:07:15.920]   it by the way when you get the the starter kit you get the the bottle you get the
[01:07:15.920 --> 01:07:17.760]   you know the
[01:07:17.760 --> 01:07:23.600]   Package of ag one that's a month's supply plus a canister to put it in a great scoop and right now
[01:07:24.480 --> 01:07:28.800]   In addition to this is by the way metal. This is the nicest scoop ever. I'm gonna end up stealing this
[01:07:28.800 --> 01:07:30.480]   I'm sure for my
[01:07:30.480 --> 01:07:38.160]   My kitchen, but when you sign up now you'll also get with your first subscription a year's supply of vitamin D little dropper
[01:07:38.160 --> 01:07:40.080]   You put right in there
[01:07:40.080 --> 01:07:44.720]   And five free travel packs. I'm a big fan. This is what this is what got me started
[01:07:44.720 --> 01:07:48.320]   Instead of packing, you know three weeks worth of vitamin
[01:07:48.320 --> 01:07:52.560]   Pillminers, this is just everything I need for one day packs very light
[01:07:53.040 --> 01:07:57.200]   It's easy to use you put it in the shaker tastes great. It's a great way to start your day
[01:07:57.200 --> 01:08:02.080]   ag one athletic greens is a climate neutral certified company
[01:08:02.080 --> 01:08:05.840]   They make these in New Zealand in the purest best
[01:08:05.840 --> 01:08:13.200]   Processing facilities ever very very high standards. So you know you're getting the best quality nutrition
[01:08:13.200 --> 01:08:20.240]   Uh, and of course for every purchase they donate to organizations helping to get nutritious food
[01:08:20.640 --> 01:08:25.520]   To kids in need because they believe good nutrition should be available to all
[01:08:25.520 --> 01:08:29.360]   So if you are looking for simpler cost effective supplement routine
[01:08:29.360 --> 01:08:34.480]   That tastes great is a great way to start your day one scoop once a day every day
[01:08:34.480 --> 01:08:36.800]   ag one from athletic greens
[01:08:36.800 --> 01:08:39.600]   And if you go right now at athletic greens.com
[01:08:39.600 --> 01:08:46.800]   Slash twit you'll get a year's supply free of vitamin d and five free travel packs with your first purchase of a subscription
[01:08:47.120 --> 01:08:52.480]   I think you're gonna really like the travel packs if you ever if you ever travel and it is delicious you shake it up
[01:08:52.480 --> 01:08:57.920]   You don't have to put it in warm water you're putting cold water dissolves very easily and it tastes phenomenal
[01:08:57.920 --> 01:09:05.360]   A g one from athletic greens go to athletic greens.com. Slash twit
[01:09:05.360 --> 01:09:07.520]   athletic greens
[01:09:07.520 --> 01:09:08.960]   Dot com
[01:09:08.960 --> 01:09:10.400]   Slash twit
[01:09:10.400 --> 01:09:15.200]   We thank them so much for their support and your support us back by the way when you go to that address
[01:09:16.000 --> 01:09:18.000]   athletic greens dot com
[01:09:18.000 --> 01:09:20.320]   Slash twit
[01:09:20.320 --> 01:09:23.280]   All right, I put away my ag one
[01:09:23.280 --> 01:09:31.120]   All I can think of during the whole ad was uh, Steve Gibson spent an awful lot of time talking about by I know I know
[01:09:31.120 --> 01:09:34.960]   Let me think here you can this dropper is a thousand iu of
[01:09:34.960 --> 01:09:41.440]   One drop is a thousand iu of vitamin d but if you put in two drops or five drops or ten drops you can increase it
[01:09:41.680 --> 01:09:46.720]   I think a thousand's enough, but you definitely want to add that vitamin d that is a and it's an easy way to do it
[01:09:46.720 --> 01:09:48.720]   It's just a little dropper it goes right in there
[01:09:48.720 --> 01:09:53.280]   Uh how predictions we start with the writer's skill, but I didn't really finish it
[01:09:53.280 --> 01:09:58.480]   Yanko predictions is this going to go on the last one went on what a hundred days back in 2007
[01:09:58.480 --> 01:10:06.000]   I mean the longer it goes on the more power the writers have right because right now hollywood still are streaming services
[01:10:06.080 --> 01:10:10.720]   They'll have a bunch of stuff that they've banked and that can use for a while they're coasting once that
[01:10:10.720 --> 01:10:12.720]   Yeah, once that drags on
[01:10:12.720 --> 01:10:17.840]   They're really going to have to wonder like how we're gonna fill this void and what we're gonna do about it
[01:10:17.840 --> 01:10:19.840]   It I think I might preachers
[01:10:19.840 --> 01:10:25.360]   It's gonna impact different companies differently and that might also back influence who wants to settle this how fast
[01:10:25.360 --> 01:10:31.040]   Like netflix seems to be in a better position because they have so much international content these days
[01:10:31.440 --> 01:10:36.480]   And people really dig korean tv shows and and stuff. It's really successful on there
[01:10:36.480 --> 01:10:41.120]   If you're disney if you're hbo or max now, I guess
[01:10:41.120 --> 01:10:48.160]   Harder because you don't have that much of a catalog and you maybe you don't have that many shows queued up
[01:10:48.160 --> 01:10:50.800]   So the longer drags on
[01:10:50.800 --> 01:10:57.120]   Uh the harder it's gonna be for hollywood to get all of that. I think I remember I seemed to remember last time that that happened that for instance
[01:10:57.200 --> 01:11:01.440]   Letterman did a side deal and got the writers back earlier. All right
[01:11:01.440 --> 01:11:06.080]   Um, I can imagine we were talking about this on wednesday on mac pree coelie that apple
[01:11:06.080 --> 01:11:08.800]   especially
[01:11:08.800 --> 01:11:16.240]   Might be incented to say okay. Okay. Okay. Here here have some it's apples throwing money at people anyway have some more money
[01:11:16.240 --> 01:11:21.360]   Uh because we really need this content on the other hand the longer it goes on
[01:11:23.440 --> 01:11:28.960]   Doesn't that when when our favorite shows start going off the air or or not finishing their seasons
[01:11:28.960 --> 01:11:33.920]   Doesn't that start to put pressure on the writers from from fans like are we gonna start saying?
[01:11:33.920 --> 01:11:41.920]   Hey, I you know what happened, you know my show's gone for sure. There's also this point where uh, especially water media
[01:11:41.920 --> 01:11:43.920]   Uh
[01:11:43.920 --> 01:11:45.280]   One of the scurry
[01:11:45.280 --> 01:11:50.960]   They already started to cancel a bunch of shows or not put shows on the body. Yes, they don't seem to mind doing that do they?
[01:11:51.040 --> 01:11:55.520]   Yeah, and they can use this now as a sort of an excuse if something didn't perform completely well
[01:11:55.520 --> 01:11:58.240]   They were like oh well we didn't get to make that other season
[01:11:58.240 --> 01:12:03.440]   So just let's just scan that project and you know, we take a tax right off good for us bad for you, but
[01:12:03.440 --> 01:12:07.040]   Yeah, the evil david zazloff who's running this
[01:12:07.040 --> 01:12:12.400]   Says the writers strike will be ended by a love for working
[01:12:12.400 --> 01:12:15.920]   All right
[01:12:16.080 --> 01:12:18.080]   He is really nasty
[01:12:18.080 --> 01:12:24.160]   Nasty piece of work. I am not at all happy with what's happening at hbo honestly
[01:12:24.160 --> 01:12:26.400]   uh
[01:12:26.400 --> 01:12:32.960]   The whole the whole landscape has shifted dynamically if you think about between what it looks like now and the last time there was a writer strike
[01:12:32.960 --> 01:12:34.560]   Right like you have
[01:12:34.560 --> 01:12:37.680]   There's a totally different set of players at least big players now
[01:12:37.680 --> 01:12:44.080]   Broken valley took over hollywood basically since then. Yeah, so now there's like this different layer of like
[01:12:45.120 --> 01:12:50.960]   Probably less likely to see all of the execs on the one side try to get in the room together and sort of like figure out
[01:12:50.960 --> 01:12:52.560]   Okay, what's the game plan here like now?
[01:12:52.560 --> 01:12:57.920]   It's there all those guys are competing against each other to try to you know, that wasn't you know, you just said about apple
[01:12:57.920 --> 01:13:02.880]   Right, maybe apple just kick him some money earlier what david letterman did like they now you have that many more
[01:13:02.880 --> 01:13:07.120]   Big players that are potentially gonna try to you know solve this
[01:13:07.120 --> 01:13:13.200]   Uh and be the leader of solving it. So maybe that actually makes the strike be shorter
[01:13:13.920 --> 01:13:19.040]   You know, I mean zazlaf got paid 39 million dollars last year. He's not hurting
[01:13:19.040 --> 01:13:23.680]   But these companies do say oh, you know all of this competition
[01:13:23.680 --> 01:13:31.280]   Uh, you know, we're not making the kind of money we used to be uh netflix seem to be struggling for a while
[01:13:31.280 --> 01:13:37.840]   Um, isn't that the counter argument that well you guys want more money and and times are tougher than ever for the streamers
[01:13:37.840 --> 01:13:40.720]   Is that true? Yeah, they just had record profits
[01:13:40.720 --> 01:13:45.680]   It's like I'm just not sure that I believe that like the streaming platforms are doing better than ever
[01:13:45.680 --> 01:13:51.920]   I mean, it's certainly a different business model and I think you know the glory days of tv or maybe over to some extent
[01:13:51.920 --> 01:13:58.560]   But no, I I don't think that that's reasonable and if it was you know, you'd see the people making 30 mind
[01:13:58.560 --> 01:14:03.600]   Million dollars a year being fired right like if you can still make 39 million dollars a year. It's probably okay. Yeah
[01:14:03.600 --> 01:14:07.200]   Two years ago. He made a quarter of a billion dollars. So
[01:14:07.200 --> 01:14:09.920]   You know, he's hurting now. He's
[01:14:10.720 --> 01:14:15.040]   I feel very sorry for him. He's just he's just really doing it for the love of work
[01:14:15.040 --> 01:14:17.040]   That's the love of work. Yes
[01:14:17.040 --> 01:14:21.440]   He just shows up. It doesn't really matter what you pay me. I'm going to do it because I love
[01:14:21.440 --> 01:14:25.040]   I love work. We love I have to say we love though
[01:14:25.040 --> 01:14:31.760]   This modern content world for a while a couple of years ago. We were saying we were in the golden age of television the golden age of
[01:14:31.760 --> 01:14:33.840]   content
[01:14:33.840 --> 01:14:36.400]   Um, we watched a lot of it during the pandemic
[01:14:37.760 --> 01:14:42.400]   They're making more money than ever before is it's is it still the golden age of content?
[01:14:42.400 --> 01:14:48.160]   Now we're in the golden age of a tv series that you might get really like and really enjoy and might be pretty really well
[01:14:48.160 --> 01:14:52.320]   And then it's just gone the next I'm just glad succession has been complete is done
[01:14:52.320 --> 01:14:54.320]   It's in the game
[01:14:54.320 --> 01:15:00.800]   They can't they can't stop my succession because that then I would really yeah be really unhappy about that
[01:15:00.800 --> 01:15:06.960]   Um, yeah, there's actually more there's more to watch now than ever before. I can't even you know, it's
[01:15:07.600 --> 01:15:11.200]   I can't even figure out what I want to watch next. I don't even know there's so much stuff
[01:15:11.200 --> 01:15:17.520]   Uh, well and the shame is that for a lot of the older content your retro
[01:15:17.520 --> 01:15:21.600]   You know, you would go watch some tv show use to watch in the 80s right all these same services
[01:15:21.600 --> 01:15:24.880]   They're licensing. Yeah, well, no, they're not in there. Not
[01:15:24.880 --> 01:15:29.520]   Is there's a lot of them? Yeah, there's there's a certain television series that it happens to be on Netflix right now
[01:15:29.520 --> 01:15:34.320]   I know it's going to be gone next month and there's whole articles that are come out occasionally on here are the next series
[01:15:34.320 --> 01:15:38.560]   They're going to be missing from from Netflix or who or whoever you know next month
[01:15:38.560 --> 01:15:43.040]   And then it turns out they were the only service that had that particular series
[01:15:43.040 --> 01:15:50.240]   So the only way for you to watch it was there and it's just going to be gone until somebody else decides to license it and add it to their library
[01:15:50.240 --> 01:15:54.400]   Yeah, it's not this like growing infinite library. Right. Yeah, actually yeah
[01:15:54.400 --> 01:15:58.240]   We what we all assumed it would be but it's not you had a really good piece
[01:15:58.240 --> 01:16:03.360]   And your low pass newsletter about free tv. That's now the hot thing, right?
[01:16:04.240 --> 01:16:06.720]   Yeah, I mean so that was sort of interesting
[01:16:06.720 --> 01:16:15.920]   I think to to the point here with with all the titles a lot that is coming back or getting revived on these free streaming services or free linear streaming service
[01:16:15.920 --> 01:16:17.920]   There's Pluto out there
[01:16:17.920 --> 01:16:22.080]   There is a lot of these services that are feel a little bit like basic cable
[01:16:22.080 --> 01:16:27.280]   Uh, but stream stuff in a in a linear way and you can switch channels and all those things
[01:16:27.280 --> 01:16:30.080]   But my article focus on sort of the next thing behind that
[01:16:30.400 --> 01:16:36.080]   Because one of the people who founded Pluto one of those early pioneers in free streaming is now building free televisions
[01:16:36.080 --> 01:16:40.560]   So instead of just giving away the content they actually want to give away the tv sets themselves
[01:16:40.560 --> 01:16:47.440]   Wow, where's the demographic for that? That's fascinating. Well, you know people like free stuff and I think the better is oh
[01:16:47.440 --> 01:16:52.000]   The free content worked so well. What if we like take it to the next level?
[01:16:52.000 --> 01:16:58.080]   And in a way if you look at the tv market in general tv's have been getting cheaper for like a decade or so
[01:16:58.480 --> 01:17:05.520]   tv's used to be really expensive and now you can go to a best buyer warm-on by a really good really nice size tv for
[01:17:05.520 --> 01:17:07.440]   $300 sometimes
[01:17:07.440 --> 01:17:11.440]   $250 which is crazy just like five six years ago those prices
[01:17:11.440 --> 01:17:19.040]   And now this this guy earlier porcelain who was a co-founder of Pluto has a new company called tv
[01:17:19.040 --> 01:17:24.480]   Uh, and they want to build this free tv set that's going to be like a regular tv
[01:17:25.280 --> 01:17:28.880]   And then it's going to have a speaker built in sort of like a sauper and then underneath of that
[01:17:28.880 --> 01:17:35.840]   It has a second screen built into the main tv and that second screen is all about showing you advertising non-stop
[01:17:35.840 --> 01:17:41.920]   And maybe every now and then there's going to be a vigil to show you some sports scores or some news headlines and news breaks
[01:17:41.920 --> 01:17:48.000]   But otherwise you watch something maybe you watch netflix on the first screen without ads and then on the second screen
[01:17:48.000 --> 01:17:50.160]   relevant to that
[01:17:50.160 --> 01:17:55.760]   You're going to have some ads pop up and in exchange for enduring that you get the entire tv set for free
[01:17:55.760 --> 01:17:58.400]   That is a wild idea
[01:17:58.400 --> 01:18:03.040]   Do you think people know people hate ads but a free tv that sounds pretty good
[01:18:03.040 --> 01:18:07.760]   I mean that that's the thing everybody says they hate ads but advertising
[01:18:07.760 --> 01:18:11.520]   Still it's billions of dollars, right? And people
[01:18:11.520 --> 01:18:16.480]   Are going back to the point I think where you know, we have all these streaming services
[01:18:16.880 --> 01:18:20.000]   When you bundle them all together it's getting pretty expensive
[01:18:20.000 --> 01:18:25.360]   And especially with recession and maybe people you know, not having as much money as they used to before
[01:18:25.360 --> 01:18:29.280]   Trying to save a little bit more they're going back to some of these free streaming services
[01:18:29.280 --> 01:18:31.840]   So why not give them a free tv as well?
[01:18:31.840 --> 01:18:35.600]   So it is a ticker just like on cnn
[01:18:35.600 --> 01:18:40.160]   That's where the ads will live, but there'll also be information there and other stuff
[01:18:40.160 --> 01:18:45.840]   Right and it's interesting when I when I published this story one of the first responders on mass
[01:18:46.000 --> 01:18:50.640]   It was actually somebody saying time for an ad blocker and then the photo to it was a roll of duct tape
[01:18:50.640 --> 01:18:54.800]   You know that could happen
[01:18:54.800 --> 01:18:59.360]   It's going to be interesting. I asked myself the question like how are they going to pretend
[01:18:59.360 --> 01:19:05.520]   Prevent people from getting these tv's for free and then just doing doing that locking it putting a soundbar in front of it
[01:19:05.520 --> 01:19:07.440]   Yeah, maybe putting a soundbar in front of it
[01:19:07.440 --> 01:19:11.520]   Or maybe just smashing the entire tv for a stupid tick tock challenge or something like that, right?
[01:19:11.520 --> 01:19:13.520]   Putting it on craigslist or whatever
[01:19:15.200 --> 01:19:17.520]   There'll be a contract in there that you have to
[01:19:17.520 --> 01:19:23.040]   I suspect actually that they might it might be a free tv on paper
[01:19:23.040 --> 01:19:25.040]   But maybe they're going to put it in stores
[01:19:25.040 --> 01:19:30.960]   Sell it for $300 and then if you have it active and if you use it you get some somebody
[01:19:30.960 --> 01:19:34.880]   Rebate or something. Yeah, you point out and this kind of blew me away that
[01:19:34.880 --> 01:19:38.880]   Visio only makes about $3 per tv
[01:19:38.880 --> 01:19:42.880]   It's it's crazier. That's the profit $3
[01:19:44.000 --> 01:19:48.880]   And at tv's have been a really thin margin. I mean there's the Samsungs and the LGs
[01:19:48.880 --> 01:19:52.960]   They make really expensive high-end tv sets. They're making money with that, right?
[01:19:52.960 --> 01:19:59.440]   But sort of the mid-level or the lower level the tcl's visio those those tv's that you pick up for $300
[01:19:59.440 --> 01:20:02.080]   On at Walmart or whatever
[01:20:02.080 --> 01:20:07.040]   Those margins has always been a razor thin and they're getting thinner right with
[01:20:07.040 --> 01:20:11.040]   Supply chain shortages shipping getting more expensive during the pandemic
[01:20:12.000 --> 01:20:16.560]   Rocoo has been pretty transparent about they don't make any money with this streaming device
[01:20:16.560 --> 01:20:21.280]   They actually lose money with every dongle that they sell and they're trying to make that back with advertising
[01:20:21.280 --> 01:20:25.200]   Visio is the same thing. They're doing a running ad support a video services
[01:20:25.200 --> 01:20:30.240]   If other companies have their ads on their devices they're getting some kickback from that as well
[01:20:30.240 --> 01:20:32.880]   So that's how they make money. So in a way
[01:20:32.880 --> 01:20:40.160]   You're already getting the subsidized tv because when you buy that visio tv they make the money not on the three dollars
[01:20:41.040 --> 01:20:48.160]   uh, but on the ads that they there's you say that they generate $28 and 30 cents per user
[01:20:48.160 --> 01:20:55.360]   Uh with ads and other fees in the per quarter more than more than a hundred bucks a month a year
[01:20:55.360 --> 01:20:58.080]   per user
[01:20:58.080 --> 01:21:03.920]   So you're already doing that in fact that you know, I have often thought that they put cameras in tv's not so you could do zoom
[01:21:03.920 --> 01:21:06.880]   But so they can see if there's somebody in the room
[01:21:06.880 --> 01:21:10.000]   Uh when they sell the ads
[01:21:10.480 --> 01:21:14.640]   I know that information goes back to advertisers. Yeah, there were four people in the room watching
[01:21:14.640 --> 01:21:19.520]   I mean, I don't know if they need cameras for all of those things
[01:21:19.520 --> 01:21:21.200]   They come on
[01:21:21.200 --> 01:21:23.840]   They all tend to overthink these things a little bit
[01:21:23.840 --> 01:21:28.880]   In the end it doesn't matter quite as much if they say oh on average three people watch everything
[01:21:28.880 --> 01:21:31.280]   That's on our tv that that's good enough for the advertiser
[01:21:31.280 --> 01:21:32.640]   But yeah
[01:21:32.640 --> 01:21:39.520]   Advertising on tv's has long been a big business and traditionally that has been sort of the business of the networks and so forth and a cable operators
[01:21:39.840 --> 01:21:44.320]   Now it's the business of the companies making those tv's so this is going to be called telly
[01:21:44.320 --> 01:21:49.360]   Uh sometime later this year the biggest thing to happen to tv since color
[01:21:49.360 --> 01:21:57.280]   And but we don't know what the actual details as you say it might you might be they'll sell it in a store and it's just going to be more subsidized or
[01:21:57.280 --> 01:22:00.480]   Yeah, they didn't talk to me and it's been completely stealth
[01:22:00.480 --> 01:22:07.520]   Uh, I sort of found out about it through people who knew some things and then I dug up additional details. It's a great scoop
[01:22:09.680 --> 01:22:15.120]   Wild just a wild story. It ties into the story about ad blockers
[01:22:15.120 --> 01:22:20.880]   I mean we're all more concerned than ever and I think more aware than ever certainly our audience more aware than ever uh of
[01:22:20.880 --> 01:22:24.720]   ad brokers or information brokers and ad
[01:22:24.720 --> 01:22:27.440]   tracking and things like that
[01:22:27.440 --> 01:22:29.760]   um
[01:22:29.760 --> 01:22:32.640]   The ftc was suing a company called
[01:22:32.640 --> 01:22:34.880]   kachava
[01:22:34.880 --> 01:22:37.520]   They were a location data broker. They
[01:22:38.480 --> 01:22:41.120]   sold information about where you are
[01:22:41.120 --> 01:22:46.320]   Information they get from your phone presumably to advertisers
[01:22:46.320 --> 01:22:52.560]   kachava according to the New York Times collects more than 90 location data points a day
[01:22:52.560 --> 01:22:57.440]   From about 35 million active mobile device users
[01:22:57.440 --> 01:23:03.440]   And so you may not realize it, but when you have an app and you give it location permissions
[01:23:03.440 --> 01:23:07.840]   It's not just for the apps developer, but it may also be sold on to a broker
[01:23:08.000 --> 01:23:09.840]   The
[01:23:09.840 --> 01:23:16.400]   Location coordinates can reveal where each mobile device has been approximately every 15 minutes federal trade commission
[01:23:16.400 --> 01:23:19.920]   filed a complaint last august saying uh
[01:23:19.920 --> 01:23:22.960]   it it's problematic because
[01:23:22.960 --> 01:23:28.720]   Not only is it track your visit to a store it could track your visits to private locations like churches
[01:23:28.720 --> 01:23:32.000]   mosques synagogues abortion clinic clinics
[01:23:32.000 --> 01:23:36.480]   domestic violence shelters medical centers and homeless shelters
[01:23:37.760 --> 01:23:43.360]   The location data could be used to track not just the dates and times patients visit abortion clinics for instance
[01:23:43.360 --> 01:23:49.600]   But also track the locations of health care professionals who provided medical treatments like abortions and there are many states
[01:23:49.600 --> 01:23:51.920]   Where that would be against the law?
[01:23:51.920 --> 01:23:57.200]   So the ftc sued and lost
[01:23:57.200 --> 01:24:03.600]   On thursday a federal judge in idaho dismissed the lawsuit against kachava
[01:24:04.960 --> 01:24:11.840]   The judge wrote regulators had not provided sufficient evidence to back up their claims that the company was unfairly selling information
[01:24:11.840 --> 01:24:16.560]   On the precise locations and millions of people's mobile phones
[01:24:16.560 --> 01:24:20.000]   They said well, you haven't proven any harm
[01:24:20.000 --> 01:24:25.200]   No, I mean
[01:24:25.200 --> 01:24:32.160]   I think yeah, I mean, I think what's just so unfortunate about this is that lita con who you know, I admire a lot and is trying to do
[01:24:32.160 --> 01:24:33.600]   um
[01:24:33.600 --> 01:24:38.880]   a lot on tech at the ftc this is just such a good example of how you can't really
[01:24:38.880 --> 01:24:46.880]   Do this without legislation, right? Like if we had a national, you know privacy law that lina would be interested with enforcing
[01:24:46.880 --> 01:24:52.320]   I don't think that you would have had that outcome in this case, but the problem is that she's trying to shove
[01:24:52.320 --> 01:24:53.680]   um
[01:24:53.680 --> 01:25:00.640]   You know antiquated laws into a more progressive model of data privacy and we just don't have the legislation to back that up
[01:25:01.200 --> 01:25:04.320]   The founder and chief executive kachava charles manning
[01:25:04.320 --> 01:25:09.200]   Celebrated saying the company has complied with all rules and laws including privacy laws
[01:25:09.200 --> 01:25:17.600]   Uh, now the ftc said it's not a qualif it's qualified victory because the judge did agree
[01:25:17.600 --> 01:25:20.400]   uh
[01:25:20.400 --> 01:25:22.400]   that
[01:25:22.400 --> 01:25:26.960]   Location data could enable third parties to track and harm smartphone view users
[01:25:27.920 --> 01:25:35.040]   He the judge merely said regulators hadn't given him adequate evidence that consumers were actually suffering or were likely to suffer
[01:25:35.040 --> 01:25:41.600]   Substantial harm see fdc said okay. Well, at least he stipulated that it could be harmful now
[01:25:41.600 --> 01:25:45.200]   We just have to prove the harm presumably that means they're going to go back to court
[01:25:45.200 --> 01:25:49.280]   Um and and fight this one, but that's discouraging
[01:25:49.280 --> 01:25:54.560]   Because it's such an ob to me. It seems like such an obvious invasion of privacy
[01:25:54.560 --> 01:25:57.200]   And that's the harm by itself
[01:25:58.160 --> 01:26:00.160]   The judge disagreed
[01:26:00.160 --> 01:26:08.320]   So maybe issues here too is that a lot of the states now, especially when it comes to this abortion laws the restrictive abortion laws
[01:26:08.320 --> 01:26:12.400]   There are some states that now and basically deputize citizens
[01:26:12.400 --> 01:26:16.640]   So if you're a citizen and you suspect a doctor provides
[01:26:16.640 --> 01:26:19.520]   Exactly you can sue them for damages. Yeah, and
[01:26:19.520 --> 01:26:26.960]   You know, it's not too far-fetched to think somebody could buy this location data and then just monetize it by suing a bunch of doctors
[01:26:26.960 --> 01:26:31.200]   Or a bunch of people who go to abortion clinic new kind of ambulance chaser. Yeah. Yeah
[01:26:31.200 --> 01:26:34.320]   um, that's
[01:26:34.320 --> 01:26:36.320]   terrifying
[01:26:36.320 --> 01:26:40.880]   We'll watch obviously we'll watch that with interest because I know our audience is very concerned about privacy
[01:26:40.880 --> 01:26:46.400]   Of of all kinds and that's why I mentioned this in the context of that ad supported tv. I mean
[01:26:46.400 --> 01:26:51.360]   Clearly what you're selling is information about you in order to get a free tv
[01:26:51.360 --> 01:26:53.920]   and some
[01:26:53.920 --> 01:26:55.920]   Very very interesting
[01:26:56.320 --> 01:27:00.320]   Uh, let's see google has turned on past keys
[01:27:00.320 --> 01:27:05.360]   For a passwordless future
[01:27:05.360 --> 01:27:11.520]   I'm I immediately turned it on but there's some concerns about well, maybe maybe this is risky
[01:27:11.520 --> 01:27:14.000]   Uh
[01:27:14.000 --> 01:27:17.120]   It replaces not only your password, but here's two factor
[01:27:17.120 --> 01:27:20.240]   So when I turned it on I went to a page at google
[01:27:20.240 --> 01:27:21.920]   Uh
[01:27:21.920 --> 01:27:25.120]   And made my phone essentially be like a ubiki
[01:27:26.080 --> 01:27:31.680]   The idea is that by unlocking my phone with face ID or touch ID, uh or a pin
[01:27:31.680 --> 01:27:35.760]   And by the way, I probably would not recommend you use this with a pin
[01:27:35.760 --> 01:27:40.400]   Uh, but it couldn't be to use that way you can unlock your google account
[01:27:40.400 --> 01:27:44.000]   And of course the risk is if somebody gets your phone and your pin
[01:27:44.000 --> 01:27:47.920]   Yep
[01:27:47.920 --> 01:27:53.280]   Suddenly they've got your google account along with everything else allen you you think this is a good idea. Would you do this?
[01:27:54.240 --> 01:27:57.680]   It doesn't feel that great to me. Yeah, hot take
[01:27:57.680 --> 01:28:04.880]   Yeah from the former security person. Okay. I'm also worried about like I just think this over reliance
[01:28:04.880 --> 01:28:08.560]   I know that you know security experts will tell you to move away from SMS
[01:28:08.560 --> 01:28:14.240]   But this is still you know an over reliance on your device and your phone and your phone account, right?
[01:28:14.240 --> 01:28:18.640]   And I I just worry about that like you know every time one of these things comes out
[01:28:18.640 --> 01:28:21.520]   I'm like how many iPhones are still in a year, right?
[01:28:21.680 --> 01:28:27.840]   Like I I just feel really bad for people who uh, you know get separated from their device for whatever reason because you know
[01:28:27.840 --> 01:28:30.960]   All your backup codes whatever it is, you know, you might have stored there
[01:28:30.960 --> 01:28:35.200]   It can be really really difficult to get into your accounts. I mean the only
[01:28:35.200 --> 01:28:38.480]   Um way you can safeguard against that is to ensure that you know
[01:28:38.480 --> 01:28:43.200]   You know an iCloud backup right of your phone because I think if you are able to restore from an iCloud backup
[01:28:43.200 --> 01:28:45.440]   You'll have a lot of that stuff, but I don't okay
[01:28:45.440 --> 01:28:47.840]   So this is uh and this is still unclear
[01:28:48.400 --> 01:28:52.000]   but uh the phyto alliance which created this past keys
[01:28:52.000 --> 01:28:53.520]   uh
[01:28:53.520 --> 01:28:55.520]   Protocol has never been
[01:28:55.520 --> 01:28:58.640]   Uh clear about the idea of moving these keys
[01:28:58.640 --> 01:29:02.240]   In fact, they don't really they really have never provided a way of moving these keys
[01:29:02.240 --> 01:29:06.480]   And I think that they see it as a security problem like well if you can move the keys now you got a problem
[01:29:06.480 --> 01:29:09.200]   uh apple
[01:29:09.200 --> 01:29:14.720]   Does not allow you to import or export the keys what apple's doing with the iCloud key as far as I can understand
[01:29:14.720 --> 01:29:17.120]   I keep trying to read these documents and understanding them
[01:29:17.600 --> 01:29:19.600]   Is when you get a new iPhone
[01:29:19.600 --> 01:29:26.400]   The keys will be moved through your iCloud to that phone, but the key is stored in a secure enclave on the hardware
[01:29:26.400 --> 01:29:32.400]   This iCloud the name iCloud is merely that when you get a new device from apple
[01:29:32.400 --> 01:29:35.600]   You can move the keys over as far as I know there is no
[01:29:35.600 --> 01:29:39.760]   And phyto2 doesn't allow for any export import of keys
[01:29:39.760 --> 01:29:45.040]   They see that as a hazard. Well, that's a great way to ensure you never switch to android, isn't it?
[01:29:45.040 --> 01:29:46.640]   Oh
[01:29:46.640 --> 01:29:49.520]   I never thought of that. Oh, yeah, exactly
[01:29:49.520 --> 01:29:53.760]   Uh, it's yeah, and apple has been queried about this
[01:29:53.760 --> 01:29:58.160]   I remember when they announced support but google and apple microsoft all support past keys
[01:29:58.160 --> 01:30:04.560]   But uh in every case, I think the companies you're seeing this is a way to keep you from moving off of their platform
[01:30:04.560 --> 01:30:09.440]   I wonder how it works for sort of unusual devices or even smart TVs
[01:30:09.440 --> 01:30:12.720]   I've didn't read in a situation where I have a i have a google tv
[01:30:13.360 --> 01:30:17.120]   And I have two factor authentication. I have a ub key or like one of those keys
[01:30:17.120 --> 01:30:22.560]   And so it suddenly asked me to put my key into the usb part of my television and press the button
[01:30:22.560 --> 01:30:24.640]   And I had to climb behind my tv basically
[01:30:24.640 --> 01:30:27.040]   To lock it to my google account
[01:30:27.040 --> 01:30:29.680]   It's not a good solution
[01:30:29.680 --> 01:30:33.520]   So, so how is that going to work with uh, sort of well?
[01:30:33.520 --> 01:30:39.200]   I don't think we're ever replacing passwords to be honest with you and that's the problem. I think this is sort of a
[01:30:40.240 --> 01:30:45.840]   Solution for some in some cases, but I don't think it's going to replace passwords. You still need a fallback anyway
[01:30:45.840 --> 01:30:50.400]   Because if you lose your phone, you got to have a fallback so all of your previous
[01:30:50.400 --> 01:30:53.040]   Methods of logging in
[01:30:53.040 --> 01:30:55.040]   Still work. You can't turn them off
[01:30:55.040 --> 01:31:00.640]   That's another trend i'm not thrilled about now is that there's a lot of companies out there that seem to be trying to move
[01:31:00.640 --> 01:31:06.720]   Away towards uh or push their users into oh the next time you sign up you could do passwordless
[01:31:07.280 --> 01:31:10.960]   So amazing and all they're doing is really just sending you a confirmation email
[01:31:10.960 --> 01:31:14.080]   So they've reduced it down to a single factor
[01:31:14.080 --> 01:31:18.800]   For you it's not even what you know anymore. It's just what you have
[01:31:18.800 --> 01:31:22.880]   Yeah, which is your email and that's not very secure at all frankly
[01:31:22.880 --> 01:31:28.080]   Hopefully it is as secure as you can make it, but the average person isn't doing even that
[01:31:28.080 --> 01:31:30.800]   Yeah, for some for some services. It's fine, right?
[01:31:30.800 --> 01:31:31.600]   So
[01:31:31.600 --> 01:31:35.680]   I mean, I know slack does it one could argue that maybe for some slack
[01:31:36.880 --> 01:31:38.880]   Be that good beehive does it your
[01:31:38.880 --> 01:31:42.480]   Yeah, but but for that if you subscribe to my newsletter and I highly recommend you do
[01:31:42.480 --> 01:31:49.120]   But if you log in there and somebody else was going to hack your email to then read my newsletter
[01:31:49.120 --> 01:31:55.440]   Yeah, unlike the scenario right uh one i've seen do it lately is microsoft
[01:31:55.440 --> 01:32:00.400]   Yeah, it's very don't know if that also i don't know if that also carries over to your one drive account
[01:32:00.400 --> 01:32:06.080]   That's a different stake for sure. Yeah, so here's my google pass keys
[01:32:06.800 --> 01:32:08.960]   Page I don't think there's any harm in showing you this
[01:32:08.960 --> 01:32:16.320]   For a long time maybe even without your knowledge and red devices have become pass keys. You can see my google pixel seven
[01:32:16.320 --> 01:32:20.160]   Was it passkey which i've never used as is my s 23?
[01:32:20.160 --> 01:32:25.120]   Ultra those are automatically created when you sign into your google account. Here's the pass keys
[01:32:25.120 --> 01:32:30.560]   I've created and the pass key with the iCloud keychain is in the same blob as
[01:32:30.560 --> 01:32:32.960]   my eubikis
[01:32:32.960 --> 01:32:39.200]   So they're treating this as similar to eubikin. It's true if you if you got my eubik key you wouldn't need anything else
[01:32:39.200 --> 01:32:43.600]   Like well you would you need my password plus a eubik key
[01:32:43.600 --> 01:32:49.280]   Right, I guess that's the biggest difference a pass key. There is no password. It's just the pass key
[01:32:49.280 --> 01:32:53.680]   But in those cases where somebody gets a device of yours and you already have
[01:32:53.680 --> 01:32:58.720]   The browser has already safety password and now it asks you for the second right thing
[01:33:00.000 --> 01:33:05.680]   Well, it doesn't ask you though anymore password managers dash lane and one password say we're going to
[01:33:05.680 --> 01:33:12.800]   Support this but again, this is this issue that the phyto alliance doesn't talk about export import of
[01:33:12.800 --> 01:33:15.280]   password, so I
[01:33:15.280 --> 01:33:19.280]   I guess what you would be doing is replacing your iCloud keychain or your android
[01:33:19.280 --> 01:33:21.760]   device with
[01:33:21.760 --> 01:33:24.720]   One password which i'm not sure is a good idea either
[01:33:24.720 --> 01:33:27.920]   um, so
[01:33:28.400 --> 01:33:33.200]   This is a google's jumped on this and a lot of people jumped on it because no one likes passwords
[01:33:33.200 --> 01:33:35.840]   But I think there's a lot of questions
[01:33:35.840 --> 01:33:40.640]   Google thinks is strong enough that even users who are using its advanced protection program
[01:33:40.640 --> 01:33:44.720]   That's the program i created for members of congress and famous people
[01:33:44.720 --> 01:33:48.000]   Extra secure they can use it too
[01:33:48.000 --> 01:33:55.680]   Instead of the physical security keys google used to require you have two physical security keys with the advanced protection program now you can use pass keys
[01:33:57.280 --> 01:34:04.560]   I I really want to hear from some security queries on this one. I'm not convinced. I mean my my personal, you know
[01:34:04.560 --> 01:34:10.640]   Best known method that i've been using that now that i've settled on is just uh, everything's passwords
[01:34:10.640 --> 01:34:14.560]   It's all made with a password manager, right? I'm using a bit warden
[01:34:14.560 --> 01:34:22.800]   Um for that piece, but then everything that's important has a second factor and the second factor is in no way shape or form tied
[01:34:23.120 --> 01:34:28.400]   Yes to to the email or the anything else. It is a completely separate thing
[01:34:28.400 --> 01:34:32.400]   You know, which is which they weren't even bit warden even tried to make it a little
[01:34:32.400 --> 01:34:37.120]   You know easier for some folks you can add a two factor. No, let's say a bit warden
[01:34:37.120 --> 01:34:41.680]   That's a bad idea. I think I don't do that right and maybe I would do that for for again
[01:34:41.680 --> 01:34:45.440]   Something that offered a two factor if it was just like a blog or something simple, right?
[01:34:45.440 --> 01:34:50.480]   Then it's convenient. I can maybe cross that bridge, but I've just sort of personally made a firewall where
[01:34:51.120 --> 01:34:57.360]   Thou shalt be no two factor. Yeah on the thing that has the passwords, right? It's just its own animal. I'm with you as well
[01:34:57.360 --> 01:34:59.440]   Um, yeah
[01:34:59.440 --> 01:35:02.640]   Unfortunately, a lot of banks require that you use sms
[01:35:02.640 --> 01:35:05.840]   For your second factor. I guess that's better than nothing
[01:35:05.840 --> 01:35:09.040]   That's not much
[01:35:09.040 --> 01:35:12.960]   We're kind of in a mess
[01:35:12.960 --> 01:35:17.680]   All right. I want to take yeah, go ahead. Luis. I know I was just gonna say I agree. Yeah
[01:35:18.960 --> 01:35:22.480]   I want to take a little break come back with more of our esteemed panel
[01:35:22.480 --> 01:35:26.640]   Great to have yanko records low pass.cc for his newsletter
[01:35:26.640 --> 01:35:29.200]   Luis mazzakis semaphor
[01:35:29.200 --> 01:35:32.000]   S-e-m-a-f-o-r
[01:35:32.000 --> 01:35:35.600]   Dot com did you all get free copies of traffic?
[01:35:35.600 --> 01:35:40.720]   No, we did not give you free copies of his book
[01:35:40.720 --> 01:35:46.880]   I he a lot of stuff a PDF of the okay. There you go. That's good. Yeah
[01:35:47.680 --> 01:35:51.760]   Uh, I hear it's very good. Jeff Jarvis has read it and said it's it's really interesting
[01:35:51.760 --> 01:35:55.680]   It's we're gonna get to actually kind of the fallout of all of that in a second
[01:35:55.680 --> 01:35:57.440]   Uh
[01:35:57.440 --> 01:36:01.680]   And of course our dear friend lmao melvin tunnel formerly of this weekend computer hardware
[01:36:01.680 --> 01:36:04.240]   formerly of a submarine
[01:36:04.240 --> 01:36:06.000]   formerly of
[01:36:06.000 --> 01:36:07.760]   the nsa
[01:36:07.760 --> 01:36:12.800]   Now done too many things. I've done many many things now the king of solid state drives
[01:36:12.800 --> 01:36:15.680]   at solidine
[01:36:15.920 --> 01:36:21.600]   Are sure today. Hey, you're my expert. I you know, that's good enough for me. Anyway
[01:36:21.600 --> 01:36:27.600]   Are sure today brought to you by zip recruiter boy when you are and a business and somebody leaves
[01:36:27.600 --> 01:36:30.640]   It's like yikes
[01:36:30.640 --> 01:36:36.080]   And if you're starting a new business or you're growing your business and by the way, if that's the case bravo
[01:36:36.080 --> 01:36:41.920]   But you got to understand and I certainly know this having run my own business now for 18 years
[01:36:41.920 --> 01:36:44.480]   The business is just the people
[01:36:44.960 --> 01:36:46.480]   Who worked there?
[01:36:46.480 --> 01:36:49.440]   If you want to be successful, you got to get the right people
[01:36:49.440 --> 01:36:55.040]   On your team people that fit people with the talents you need people the skills you need people to drive
[01:36:55.040 --> 01:36:59.920]   You need and that's where zip recruiter comes in and I love zip recruiter
[01:36:59.920 --> 01:37:05.440]   We use it here for our hiring and right now you can try it for free at zip recruiter.com/twit
[01:37:05.440 --> 01:37:12.240]   Have you thought about starting your own business whether it's a taco truck or a charity you need to hire
[01:37:12.880 --> 01:37:15.600]   People and this is why you should use zip recruiter
[01:37:15.600 --> 01:37:17.600]   first of all
[01:37:17.600 --> 01:37:23.280]   Because when you post a zip recruiter it goes to a hundred plus job sites all the social networks you're reaching out
[01:37:23.280 --> 01:37:24.960]   casting the
[01:37:24.960 --> 01:37:30.480]   Biggest net possible because you know the right person's out there, but you got to make sure you you reach them where they are
[01:37:30.480 --> 01:37:36.800]   But then you might say well gosh, that's great now. I'm gonna get a million phone calls and my inbox is going to be jam full
[01:37:36.800 --> 01:37:40.400]   No, it all flows into your zip recruiter interface effect
[01:37:40.720 --> 01:37:43.920]   They even reformat all the resumes so you can scan them quickly
[01:37:43.920 --> 01:37:47.600]   They give you screening questions true false multiple choice even essay
[01:37:47.600 --> 01:37:53.760]   Questions so you can eliminate people who just don't fit and then zip recruiter does something really remarkable
[01:37:53.760 --> 01:37:58.800]   They have already a million plus current resumes on file people come to zip recruiter looking for work
[01:37:58.800 --> 01:38:04.960]   They use their powerful matching technology to go through those resumes and find people qualified
[01:38:04.960 --> 01:38:09.120]   for your role the job you're trying to fill and then
[01:38:10.160 --> 01:38:12.160]   They let you send them a personal invite
[01:38:12.160 --> 01:38:17.760]   And by the way, that really works if you look at some of the the prospects that zip recruiter finds
[01:38:17.760 --> 01:38:22.480]   For you and you say oh that person be great that person will be great and you send them an invite man
[01:38:22.480 --> 01:38:26.960]   They are they happy they're flattered they're more likely to apply follow through take the job
[01:38:26.960 --> 01:38:31.760]   Zip recruiter does more too. They give you labels you can put on your listing to help
[01:38:31.760 --> 01:38:35.040]   You tell the message in a quick way because you know what
[01:38:35.040 --> 01:38:37.280]   those those those
[01:38:37.280 --> 01:38:43.040]   Perspective employees they're also scanning the openings. They're going to see these attention grabbing labels
[01:38:43.040 --> 01:38:45.040]   You can say remote work
[01:38:45.040 --> 01:38:51.360]   For instance allowed or training provided. That's a big one people love that or urgent. We need to fill this one fast
[01:38:51.360 --> 01:38:53.840]   They'll really help your job stand out
[01:38:53.840 --> 01:38:57.680]   That invite to apply the attention grabbing labels
[01:38:57.680 --> 01:39:01.440]   The just the process alone makes it recruiter amazing
[01:39:02.320 --> 01:39:05.360]   Let zip recruiter fill all your roles with the right candidates we do
[01:39:05.360 --> 01:39:12.240]   And I have to tell you we get results fast four out of five employers who post on zip recruiter get a quality candidate within the first day
[01:39:12.240 --> 01:39:15.760]   That's such a relief when somebody leaves and you got to fill that position
[01:39:15.760 --> 01:39:19.440]   It's stressful just knowing hey, I got some great candidates
[01:39:19.440 --> 01:39:22.800]   We you know, Lisa will post it breakfast by lunch
[01:39:22.800 --> 01:39:28.800]   Should we go oh we got another one? We got another one? It just it takes some of that stress
[01:39:29.680 --> 01:39:35.440]   Out of your life see for yourself go to this exclusive web address to try zip recruiter for free zip recruiter
[01:39:35.440 --> 01:39:38.320]   Dot com slash twit zip recruiter
[01:39:38.320 --> 01:39:41.520]   Dot com slash twit w I
[01:39:41.520 --> 01:39:43.440]   t
[01:39:43.440 --> 01:39:47.920]   I can tell you from from our personal experience. It is the smartest way to hire
[01:39:47.920 --> 01:39:51.200]   zip recruiter dot com slash twit we thank you zip recruiter
[01:39:51.200 --> 01:39:53.840]   For all that great support we get from you
[01:39:53.840 --> 01:39:56.480]   each
[01:39:56.480 --> 01:39:58.480]   And every time we need to hire
[01:39:59.440 --> 01:40:00.960]   um
[01:40:00.960 --> 01:40:02.960]   We were talking about
[01:40:02.960 --> 01:40:06.240]   Ben Smith's new book traffic
[01:40:06.240 --> 01:40:08.480]   and
[01:40:08.480 --> 01:40:11.120]   what was at the time when when he was at buzzfeed and
[01:40:11.120 --> 01:40:16.080]   And later at other sites the way to you know
[01:40:16.080 --> 01:40:21.680]   Make it in news was to generate traffic with attention grabbing headlines and social media
[01:40:21.680 --> 01:40:24.320]   It seems to be
[01:40:24.720 --> 01:40:29.040]   A little more problematic than it used to be uh we protocol, right?
[01:40:29.040 --> 01:40:34.000]   Uh, I guess giga home in a way in its way and now vice
[01:40:34.000 --> 01:40:40.880]   Apparently said it to be head for bankruptcy according to new york times buzzfeed just shut down their news division
[01:40:40.880 --> 01:40:43.600]   froze it all
[01:40:43.600 --> 01:40:49.440]   Vice was valued at 5.7 billion dollars have been looking hard for a buyer can't find one
[01:40:49.440 --> 01:40:52.800]   Is the online news business in trouble louise?
[01:40:54.400 --> 01:40:58.800]   Good question. Well, I live really close to that vice office and I used to work at vice
[01:40:58.800 --> 01:41:01.440]   and I can say that uh
[01:41:01.440 --> 01:41:09.600]   I think there were a lot of mistakes that were made at these companies just about the business model and how they structured the business
[01:41:09.600 --> 01:41:14.000]   Um, you know, I just watched even in my short time advice, you know over and over again
[01:41:14.000 --> 01:41:20.400]   Sort of pivoting away from things that made money, you know starting a cable news channel when their core demographic was cord cutting more than ever
[01:41:20.800 --> 01:41:27.440]   I don't think it's just about this spiral news model being um, you know, not as good as it seemed potentially
[01:41:27.440 --> 01:41:30.400]   But that was definitely such a specific era
[01:41:30.400 --> 01:41:36.400]   You know, I think of like the personal essays the catchy headlines, but also a lot of like, you know, really hard
[01:41:36.400 --> 01:41:45.440]   Um, you know admirable reporting for sure. Um, and I I think we all have, you know seared into our brains anyone who worked in that era
[01:41:45.440 --> 01:41:48.720]   Uh, these tools one of them was called chart beat
[01:41:49.360 --> 01:41:53.440]   And you could see in real time how many people were clicking on your article and it would sort of
[01:41:53.440 --> 01:41:55.920]   Move to the top of the list
[01:41:55.920 --> 01:42:02.560]   Uh of the website and and yeah, there was nothing that sort of hit the little serotonin receptors in your brain like that and and gocker
[01:42:02.560 --> 01:42:05.520]   And some other workplaces actually had them on screens
[01:42:05.520 --> 01:42:11.200]   Uh that would project into the into the office. You could see you know how you were stacking up against your competitors
[01:42:11.200 --> 01:42:14.480]   And I I definitely think that those sorts of incentives are not always uh
[01:42:14.480 --> 01:42:17.760]   Most positive. Yeah, I don't miss those days. Yeah
[01:42:18.560 --> 01:42:25.680]   Buzzfeed, uh, it said was to rely on facebook too and then when facebook stopped sending links their way it was all over
[01:42:25.680 --> 01:42:27.920]   Um, yeah, definitely
[01:42:27.920 --> 01:42:32.320]   I think that's one reason that semaphore is on us like doubling down on newsletters because we want to own the audience
[01:42:32.320 --> 01:42:38.400]   We want to know who is reading us how we can contact them in the future. Um, we want to be right in your inbox instead of you know
[01:42:38.400 --> 01:42:40.480]   sort of a drive-by on
[01:42:40.480 --> 01:42:45.040]   Uh facebook, right? I think that's totally you don't want uh, you know
[01:42:45.680 --> 01:42:49.840]   One day the platforms change the algorithm and you've totally lost your audience
[01:42:49.840 --> 01:42:53.360]   Yeah, I go you've been through these wars too
[01:42:53.360 --> 01:42:59.040]   Yeah, uh, they're not part of two companies that folded so yeah, I think
[01:42:59.040 --> 01:43:00.800]   Uh
[01:43:00.800 --> 01:43:04.720]   Part of that issue is what luis described the the traffic chasing
[01:43:04.720 --> 01:43:08.960]   I think part of it was also that some of these bets that they made were just really really expensive
[01:43:08.960 --> 01:43:15.600]   With the idea that they would pay off in the future and so they erased a ton of money on those ideas like
[01:43:15.680 --> 01:43:18.160]   making bespoke branded content and
[01:43:18.160 --> 01:43:21.840]   doing crazy video things and so
[01:43:21.840 --> 01:43:26.560]   And if you raise hundreds of millions of dollars or even more in the case of vice, I guess
[01:43:26.560 --> 01:43:30.320]   Uh, at some point you have to be able to pay that back and
[01:43:30.320 --> 01:43:32.080]   that
[01:43:32.080 --> 01:43:34.720]   Sort of forces you even more to chase those those
[01:43:34.720 --> 01:43:41.200]   Audiences and those ad dollars and everything and that that's I think even the bigger promise not that news itself
[01:43:41.360 --> 01:43:46.480]   Isn't can't be run profitably and and they're saying having post is actually
[01:43:46.480 --> 01:43:50.320]   uh profitable, right so um
[01:43:50.320 --> 01:43:55.600]   Us we saying having boats gonna stay around as a news organization because they're profitable
[01:43:55.600 --> 01:44:01.440]   So you can run profitable news organizations, but if you raise so much money that you have to make this
[01:44:01.440 --> 01:44:07.440]   Crazy profitable and you take these crazy bets. It's easy for those things to go wrong
[01:44:07.440 --> 01:44:10.240]   I guess, you know
[01:44:10.640 --> 01:44:15.840]   Even today, uh with what you're doing yanko with a newsletter what semaphore is doing with his newsletter
[01:44:15.840 --> 01:44:17.920]   We're still trying to find a model
[01:44:17.920 --> 01:44:20.480]   For online news. It's not that
[01:44:20.480 --> 01:44:27.520]   I mean, it's just and so I understand the desire to experiment and vice it's I mean motherboard was a really great
[01:44:27.520 --> 01:44:30.240]   source of security news for
[01:44:30.240 --> 01:44:33.120]   Years we quoted it all the time
[01:44:33.120 --> 01:44:36.160]   um, but part of the problem is just that the
[01:44:36.160 --> 01:44:39.600]   Is it just that it's hard to make money in news?
[01:44:40.480 --> 01:44:41.600]   I
[01:44:41.600 --> 01:44:44.560]   Think it's hard to make the kind of money that justifies
[01:44:44.560 --> 01:44:47.360]   You know hundreds of millions of dollars in
[01:44:47.360 --> 01:44:53.040]   Uh, you know venture capital funding and I think that's true for a lot of businesses, right? Like it's not just news. It's true for
[01:44:53.040 --> 01:44:58.400]   Uh ride hailing rights true for food delivery all these things where it's been really hard to turn a profit
[01:44:58.400 --> 01:45:02.640]   Um, and I think that's the problem. I think you can run a profitable news business
[01:45:02.640 --> 01:45:07.120]   Absolutely, but it's about the scale of that and understanding that you know
[01:45:07.280 --> 01:45:13.040]   You're not going to be read by everyone in the country every day, right? Like but you know the messenger's trying it right?
[01:45:13.040 --> 01:45:17.920]   There's another one of these big big operations that has you know incredible aspirations
[01:45:17.920 --> 01:45:20.960]   That's about to launch and they said I think they're gonna hire 500 journalists
[01:45:20.960 --> 01:45:23.440]   So, you know, I'll be praying for them
[01:45:23.440 --> 01:45:26.320]   About trying this again, you know
[01:45:26.320 --> 01:45:31.360]   Problem I like how you said that you're not rooting for them, but you're praying for them
[01:45:33.440 --> 01:45:39.280]   Kind of tells you something some of the problem is uh and Alex kentrowitz has talked about this in his big
[01:45:39.280 --> 01:45:45.520]   Tech newsletter and podcast the over financialization of these of these models that
[01:45:45.520 --> 01:45:52.480]   You know, you have to get vc funding and then the vcs are really pushing you to go to show profit to have an exit
[01:45:52.480 --> 01:45:56.880]   Um, and it's just not a very it's not a very tenable way to create
[01:45:56.880 --> 01:46:00.880]   Value I don't know what the other I don't know what the opposite solution is
[01:46:01.440 --> 01:46:06.480]   You can't bootstrap a Jonah paredi at buzzfeed famously said I know news is never gonna make any money
[01:46:06.480 --> 01:46:10.960]   But it's important so I'm gonna pay for it out of my own pocket and he grew tired of that obviously
[01:46:10.960 --> 01:46:13.840]   Because it just never made any money
[01:46:13.840 --> 01:46:18.640]   Um, well, I don't know what the alternative is but it sounds like it made a lot of money
[01:46:18.640 --> 01:46:22.080]   Yeah, a lot of money. It just didn't make enough money to justify
[01:46:22.080 --> 01:46:28.000]   100 millions of dollars, but yeah news didn't yeah, I think I think that's the that's the problem
[01:46:28.000 --> 01:46:32.080]   I mean, I think busby news did make some money, but yeah, I don't think it made enough to sustain
[01:46:32.080 --> 01:46:35.840]   Uh the scale of the operation that they had right?
[01:46:35.840 --> 01:46:38.160]   He just like he said I just like news and I
[01:46:38.160 --> 01:46:42.640]   And I'm actually in his I'm with him. I like it too. In fact, we you know
[01:46:42.640 --> 01:46:45.280]   We're kind of a parasite on the on the online news
[01:46:45.280 --> 01:46:48.240]   Industry and all we do is talk about your stories
[01:46:48.240 --> 01:46:52.320]   I know
[01:46:52.320 --> 01:46:54.320]   That's okay with you
[01:46:54.800 --> 01:46:58.880]   But I'm smart enough not to try to spend money on enterprise reporting because it's expensive
[01:46:58.880 --> 01:47:05.280]   So I let I let uh, you know venn do it and uh and just piggyback off semaphore and
[01:47:05.280 --> 01:47:07.920]   And and so forth
[01:47:07.920 --> 01:47:10.960]   But we need news and I think that one of the bad things that this
[01:47:10.960 --> 01:47:18.320]   Financialization and finance pressure and and traffic pressure has caused is that a lot of news is just so linkbaity now
[01:47:18.320 --> 01:47:22.560]   Uh, I think it's turning off users. I think we're
[01:47:23.120 --> 01:47:28.640]   Users are not going to want to read it anymore. If I see another list. I don't go to listicles anymore, right?
[01:47:28.640 --> 01:47:31.840]   That was the famous buzzfeed model
[01:47:31.840 --> 01:47:33.840]   Um
[01:47:33.840 --> 01:47:36.000]   If I look at uh, microsoft's news
[01:47:36.000 --> 01:47:40.640]   Service, it's almost entirely linkbait articles
[01:47:40.640 --> 01:47:47.120]   You'll never believe what happened next, uh, you know, um
[01:47:47.120 --> 01:47:50.960]   And I guess that's what drives traffic
[01:47:52.880 --> 01:47:57.280]   Yeah, but it just deludes the the experience. Yeah, greatly. Yeah
[01:47:57.280 --> 01:48:03.760]   Actually, this today is a bad day to look at this because we have two very very big news stories that are dominating
[01:48:03.760 --> 01:48:08.880]   Dominating there's and they're real news. So they're not doing listicles on those but uh
[01:48:08.880 --> 01:48:12.640]   Yeah, normally it's a lot of just kind of crappy stuff
[01:48:12.640 --> 01:48:16.800]   I think we've seen models said work though emerge of a
[01:48:16.800 --> 01:48:19.760]   uh the last couple of years and I wouldn't
[01:48:20.640 --> 01:48:26.240]   Sure, there's these big companies going under but I think in a local news space if you look at local non-profit news sites
[01:48:26.240 --> 01:48:32.000]   They're released sort of providing research and some of the stuff that newspapers used to cover
[01:48:32.000 --> 01:48:37.440]   I hope you're right. We have a great one. We have a great one here in oakland and and broccoli is a familiar one too
[01:48:37.440 --> 01:48:40.720]   Um, and you know smaller publishers
[01:48:40.720 --> 01:48:46.080]   Newsler based publishers and some of the big ones are doing well like the new york times is doing well
[01:48:46.080 --> 01:48:48.320]   So there's examples for things that work
[01:48:48.880 --> 01:48:55.840]   Uh just these big massive bets and glitzy things and raising I think vice raised 400 million dollars from disney alone
[01:48:55.840 --> 01:48:57.040]   Yeah
[01:48:57.040 --> 01:49:00.240]   They were never going to be able to pay that back in a reasonable way
[01:49:00.240 --> 01:49:02.880]   Funny thing is chartbeats still around
[01:49:02.880 --> 01:49:07.440]   And parsley is the other one. Yeah parsley, huh?
[01:49:07.440 --> 01:49:13.600]   Uh, yeah, I just I boy I think news is so important and I think an informed
[01:49:13.600 --> 01:49:16.400]   electorate is so important and
[01:49:17.440 --> 01:49:21.280]   I just hope that there is a way forward and I would hate for it just to be
[01:49:21.280 --> 01:49:24.240]   the new york times in the washington post
[01:49:24.240 --> 01:49:29.760]   Uh and and nothing else so we got to find a way to make this possible
[01:49:29.760 --> 01:49:36.000]   What is your local oakland and berkley? Is it a newspaper? Is it what is it? It's here's a oakland site
[01:49:36.000 --> 01:49:40.720]   Which is a non-profit news organization is doing really good work and covering local stuff really well
[01:49:40.720 --> 01:49:46.240]   In berkeley, it's berkeley site. I know that donnae you have a lais which is
[01:49:46.320 --> 01:49:47.840]   Yeah, elia is awesome
[01:49:47.840 --> 01:49:52.400]   public radio station down there kpcc so they're doing good work and a lot of that is
[01:49:52.400 --> 01:50:00.480]   Um, some of it is advertising based some of this in case of I think elia is that's primarily just greater supported
[01:50:00.480 --> 01:50:05.440]   And so finding the right mixture there and keeping those newsrooms
[01:50:05.440 --> 01:50:11.280]   Manageable in size and not promising the world. I think is a good recipe for for success
[01:50:11.280 --> 01:50:13.120]   Local news is so important in that cress
[01:50:13.120 --> 01:50:15.920]   That's one of the first things that goes away when you have that only can
[01:50:15.920 --> 01:50:21.040]   Support a national enterprise of course berkeley and oakland and la are big metros
[01:50:21.040 --> 01:50:28.800]   Here we are little 50,000 person petaluma, but you know, there's a petaluma patch. There's a local newspaper. I guess there's ways to get this information
[01:50:28.800 --> 01:50:32.640]   You read elia is um louise is that
[01:50:32.640 --> 01:50:37.680]   Yeah, I think it's really important. I mean elia moved to la a few years ago, but I think local news
[01:50:37.680 --> 01:50:42.880]   Um, you know help me learn about and integrate into a new community. Yeah, that's a good point. Yeah
[01:50:42.880 --> 01:50:44.880]   And even just like you know, I think
[01:50:44.880 --> 01:50:48.080]   To the of course like, you know, the really hard news is super important
[01:50:48.080 --> 01:50:53.040]   But I think one thing the new york times la is some of these other publications understand is that people want to know like
[01:50:53.040 --> 01:50:57.920]   What are the latest restaurants? They want they want recipes. They want crossword puzzles like they want sort of like
[01:50:57.920 --> 01:51:04.640]   Lifes on community part traditional lifestyle coverage. I think that stuff's important and I think it's it's it's good when news organizations realize
[01:51:04.640 --> 01:51:08.160]   It's not just like winning the polluters or like which is new princess are you?
[01:51:08.800 --> 01:51:15.360]   Traffic the best la restaurants to eat brunch with your dog to me. That's a good thing. You need to have articles like that
[01:51:15.360 --> 01:51:18.000]   Yeah, crucial information. Yeah
[01:51:18.000 --> 01:51:21.920]   No, this is great how to live with our coyote neighbors
[01:51:21.920 --> 01:51:32.160]   Now this is important stuff do these writers at publications like this? Are they able to make a living? Are they doing this for the love of it?
[01:51:32.160 --> 01:51:36.080]   Do they have to have a job delivering food with uber eats?
[01:51:37.040 --> 01:51:38.800]   I think
[01:51:38.800 --> 01:51:41.920]   I mean, I don't know every example, but I think at least in these examples
[01:51:41.920 --> 01:51:44.880]   Those are real journalists that I've been working for real jobs
[01:51:44.880 --> 01:51:49.840]   Island news organizations in the past and good. Yeah, good. That's very encouraging
[01:51:49.840 --> 01:51:52.880]   Makes me happy
[01:51:52.880 --> 01:51:57.600]   Um google I owe this week
[01:51:57.600 --> 01:52:04.880]   Let me take a break and we'll talk about google I/O because we're kind of running down the clock here and I
[01:52:05.760 --> 01:52:12.240]   Want to give you guys a chance to tell me what you think we're pretty sure in fact, I think google is now admitted. Yes a folding phone
[01:52:12.240 --> 01:52:17.200]   So we'll talk about that and more in just a bit with our wonderful panel
[01:52:17.200 --> 01:52:20.640]   Our show today brought to you by express vpn
[01:52:20.640 --> 01:52:27.600]   We talk about privacy a lot, but you need tools to help protect your privacy tools
[01:52:27.600 --> 01:52:32.480]   Like our sponsor express vpn everybody's talking about you know like chat gpt
[01:52:33.360 --> 01:52:36.240]   Microsoft google investing heavily in ai for search
[01:52:36.240 --> 01:52:41.760]   It's not a surprise. These are the same companies that use the information you give them
[01:52:41.760 --> 01:52:44.400]   to monetize they
[01:52:44.400 --> 01:52:49.440]   They get a cut from the information you see and uh, you might
[01:52:49.440 --> 01:52:52.160]   Notice that one of the things they're doing is
[01:52:52.160 --> 01:52:58.080]   Is creating their own search results with their little robots so that you don't ever leave the site
[01:52:59.360 --> 01:53:04.960]   Uh, I think more and more it's important that you kind of defend yourself against
[01:53:04.960 --> 01:53:12.880]   Uh data brokers. We just talked about kachava about against big tech that wants to mine your information for their ads
[01:53:12.880 --> 01:53:19.120]   A good vpn is worth its weight in gold. That's why I use and recommend express
[01:53:19.120 --> 01:53:22.800]   vpn to put a layer of protection between you
[01:53:22.800 --> 01:53:27.360]   And big tech and little tech the outside world with express vpn
[01:53:27.840 --> 01:53:34.720]   You hide your ip address on all your devices. It's very easy. There's an app for mac windows linux. Ios android
[01:53:34.720 --> 01:53:38.800]   You open the app you press the big button and now you are hidden
[01:53:38.800 --> 01:53:44.000]   Big tech doesn't know it's me. It's sees an express vpn ip address instead
[01:53:44.000 --> 01:53:49.680]   Very easy to use it also encrypts 100 percent of your online traffic
[01:53:49.680 --> 01:53:53.200]   So we were talking last week about the wifi pineapple
[01:53:53.200 --> 01:53:56.160]   very very
[01:53:56.160 --> 01:53:58.160]   Disturbing hardware out there
[01:53:58.160 --> 01:54:01.760]   Express vpn protects against pryonize that way too
[01:54:01.760 --> 01:54:06.960]   And one express vpn subscription covers up to five devices at the same time
[01:54:06.960 --> 01:54:12.000]   So everybody can use it in fact express vpn even sells routers and offers software for routers
[01:54:12.000 --> 01:54:15.280]   So you can put it on your router and and protect the whole house
[01:54:15.280 --> 01:54:20.480]   And express vpn invests in their infrastructure. So it's fast
[01:54:21.040 --> 01:54:27.440]   Fast enough to watch hd video with servers all over the world so you can have a location anywhere in the world that you want
[01:54:27.440 --> 01:54:33.120]   It is the best way to protect your privacy to eliminate geo restrictions
[01:54:33.120 --> 01:54:40.240]   Uh and to uh and to protect yourself from bad guys online express vpn
[01:54:40.240 --> 01:54:45.520]   Stop letting big tech leech your data freely do what I do use express vpn
[01:54:45.520 --> 01:54:48.320]   Go to express vpn.com/twit right now
[01:54:48.320 --> 01:54:51.920]   You can get three extra months free with a one-year plan. That's the best deal
[01:54:51.920 --> 01:54:53.760]   It's not free. It shouldn't be free
[01:54:53.760 --> 01:54:58.800]   You want to pay for your vpn because you don't want to give them any incentive to sell your information express vpn
[01:54:58.800 --> 01:55:03.360]   Bends over backwards to make sure you're private. No logging. They run a
[01:55:03.360 --> 01:55:06.080]   specially designed
[01:55:06.080 --> 01:55:10.320]   Debi and distro that wipes the entire drive every single day
[01:55:10.320 --> 01:55:17.920]   So even if it could write to the hard drive, they can't store that data. They run their custom trusted server
[01:55:18.400 --> 01:55:21.840]   Software which runs out of rand it's sandboxed. It can't write to the drive
[01:55:21.840 --> 01:55:24.320]   By the way, this has all been validated by independent third parties
[01:55:24.320 --> 01:55:28.560]   It was great article if you're interested i'm bleeping computer about how express vpn does what they do
[01:55:28.560 --> 01:55:30.400]   It really works
[01:55:30.400 --> 01:55:31.840]   ex pr
[01:55:31.840 --> 01:55:38.160]   e s s vpn express vpn.com/twit number one rated by cnet tech radar and
[01:55:38.160 --> 01:55:39.840]   me
[01:55:39.840 --> 01:55:41.920]   Express vpn.com
[01:55:41.920 --> 01:55:45.120]   Slash twit get that extra three months with your one-year
[01:55:45.760 --> 01:55:51.520]   Package and a fun week this week on twit we've even made a little movie for your entertainment
[01:55:51.520 --> 01:55:58.320]   Do you do Jason when do you remember back in the day when notification and ringtone volumes were separate on and yeah?
[01:55:58.320 --> 01:56:02.800]   Yeah, I remember that yeah, because it looks like it is back and in testing
[01:56:02.800 --> 01:56:05.440]   It's happening
[01:56:05.440 --> 01:56:11.680]   My guess is whoever the person on the product team at android that hated this around the time a lollipop has left and moved on
[01:56:12.000 --> 01:56:17.200]   Bobby, uh, can we finally split these apart? No, i'm still here. I hate that
[01:56:17.200 --> 01:56:22.560]   No, set yourself a reminder. We'll check back with bobby next year previously on twit
[01:56:22.560 --> 01:56:24.800]   iOS today
[01:56:24.800 --> 01:56:33.040]   I'm joined by matthew kasnilly and we are going to talk pro tips tricks apps and more to bring you up to the best skill level for you
[01:56:33.040 --> 01:56:36.000]   And all of your devices tech news weekly
[01:56:36.000 --> 01:56:39.120]   We start with mia sato from the verge
[01:56:39.520 --> 01:56:42.480]   She joins us talk about generative ai on the voice
[01:56:42.480 --> 01:56:49.440]   Front ai drake ai weekend and the legal implications you can't copyright your voice
[01:56:49.440 --> 01:56:55.680]   You can't copyright your style or your flow or the way drake says something like that's not a copyright problem
[01:56:55.680 --> 01:57:02.000]   That's why the song is so deeply weird. It's like well. There's no work that it was copying this week in google
[01:57:02.000 --> 01:57:04.160]   this morning I got up early and
[01:57:04.160 --> 01:57:06.400]   Signed up for past keys
[01:57:06.400 --> 01:57:11.520]   So now my iPhone is my google login and all I have to do when I want to log into google is
[01:57:11.520 --> 01:57:17.120]   Do the biometrics on the iphone in this case face id may I add something here? Yes
[01:57:17.120 --> 01:57:24.480]   Because you have a workplace account. Oh, right? Is it not work with that? No
[01:57:24.480 --> 01:57:32.800]   Uh, and they keep using ai leo as the announcer on this show. So i'm gonna sue a dad. I'll show you dad
[01:57:34.400 --> 01:57:40.640]   It sounds a little bit like me anyway. Uh, it was a great week this week on twit and I hope you'll join us all week
[01:57:40.640 --> 01:57:43.520]   Long and of course if you're a club twit member
[01:57:43.520 --> 01:57:48.960]   Extra stuff from hands on windows hands on mac and tosh the untitled linux show the gizfiz
[01:57:48.960 --> 01:57:54.320]   Stacey's book club and more if you're not yet a member seven bucks a month add free versions of all our shows
[01:57:54.320 --> 01:58:01.360]   Uh, you wouldn't even be hearing this if you're a member plus access to the discord plus all those free shows
[01:58:01.840 --> 01:58:06.720]   Well, they're not free are they they're seven bucks a month, but that's nothing for all the goodness that you get
[01:58:06.720 --> 01:58:09.760]   Uh, go to twit.tv/club twit and you can
[01:58:09.760 --> 01:58:14.240]   Join we will be doing a live stream from google.io
[01:58:14.240 --> 01:58:19.280]   I think we decided it's 10 a.m. Right 10 a.m. On wednesday 10 a.m. Pacific one p.m. Eastern
[01:58:19.280 --> 01:58:22.960]   Jeff Jarvis and I will uh watch the keynote along with you
[01:58:22.960 --> 01:58:25.600]   And then we'll discuss it all
[01:58:25.600 --> 01:58:28.400]   Uh later on windows weekly and this week in google
[01:58:29.280 --> 01:58:34.000]   Uh, apparently google has finally acknowledged. Yeah, there's gonna be a folding phone and I guess
[01:58:34.000 --> 01:58:36.800]   well, we'll see if they if they
[01:58:36.800 --> 01:58:42.080]   What they say about it at google.io, but they have with video teasers on youtube and twitter
[01:58:42.080 --> 01:58:45.120]   Tease this is it
[01:58:45.120 --> 01:58:48.400]   Uh, looks like it's a two screen phone
[01:58:48.400 --> 01:58:51.440]   Extra wide screen that unfolds and then one on the front
[01:58:51.440 --> 01:58:58.080]   Um, so you get you get both it unfolds into a 7.6 inch display. That's pretty big
[01:58:59.120 --> 01:59:01.840]   Uh, the exterior screen is 5.8 inches
[01:59:01.840 --> 01:59:06.640]   Here's the scary thing the rumors are it's going to be as much as $2,000
[01:59:06.640 --> 01:59:10.240]   Any did any of you use folding phones?
[01:59:10.240 --> 01:59:14.400]   No, i'm trying to figure out who this is for
[01:59:14.400 --> 01:59:21.680]   Uh, with a lot of money. Yeah, well that for sure, you know, somebody wants the coolest looking
[01:59:21.680 --> 01:59:26.720]   Thing, I guess samson's been selling these sf have other companies for some time
[01:59:26.720 --> 01:59:28.960]   um
[01:59:29.600 --> 01:59:31.600]   I don't know maybe it's just I don't know
[01:59:31.600 --> 01:59:36.400]   I think that the sweet spot in the android is the cheap phone not the expensive phone
[01:59:36.400 --> 01:59:41.440]   I think they keep trying to encroach in the space of premium you know how there's well
[01:59:41.440 --> 01:59:48.080]   Maybe some of that but maybe also some of oh look at how much more you can do with this because it has the bigger screen
[01:59:48.080 --> 01:59:54.480]   Like assuming that you're gonna bust out a keyboard or something and use this thing as as if it you know was like a larger
[01:59:55.280 --> 02:00:00.640]   I think that's where they try to start going with these things because what I mean why else would you need the screen to be really that big
[02:00:00.640 --> 02:00:06.160]   Right other than either to show off or to try to be more productive somehow but to be more productive
[02:00:06.160 --> 02:00:09.440]   Okay. Well now you need to do some other ui or interface
[02:00:09.440 --> 02:00:17.040]   Things like to be able to effectively leverage that right? Well, we'll see how google makes the case. I guess on uh
[02:00:17.040 --> 02:00:19.200]   Wednesday
[02:00:19.200 --> 02:00:24.640]   Uh, I wonder if google will address jeffrey hinton the so-called godfather of ai who
[02:00:25.280 --> 02:00:30.400]   Uh quit google so that he could warn of the danger ahead. He does not by the way
[02:00:30.400 --> 02:00:36.720]   Uh think that what we're currently seeing with chat gbt and bard and so forth is dangerous. He's more worried
[02:00:36.720 --> 02:00:38.800]   I don't know about the sci-fi
[02:00:38.800 --> 02:00:41.600]   Future when it's robo cop or something
[02:00:41.600 --> 02:00:46.960]   Is it bad that I was so worried that his photo in that article was a I generate doesn't look real does it?
[02:00:46.960 --> 02:00:52.800]   Do you think it's fake? It's a really nice picture, but it's it will never know anymore. Will we?
[02:00:53.360 --> 02:00:55.360]   Right. See maybe he's making the point right now
[02:00:55.360 --> 02:01:03.840]   It's scary cademets had an article, but he also did interviews with I think the mit technology review
[02:01:03.840 --> 02:01:06.160]   Uh and so forth
[02:01:06.160 --> 02:01:14.000]   Uh, it is a little concerning that people who should I mean he basically created large language models who should you know understand the deep
[02:01:14.000 --> 02:01:17.360]   Uh implications of this are worried about the future
[02:01:20.160 --> 02:01:28.160]   Um, he actually uh, he uh as a graduate student in 1972 was the first to create something called neural networks
[02:01:28.160 --> 02:01:31.520]   Then a professor of computer science at cm u
[02:01:31.520 --> 02:01:35.600]   Um left because he didn't want to take pentagon funding
[02:01:35.600 --> 02:01:38.400]   Went to canada
[02:01:38.400 --> 02:01:39.600]   um
[02:01:39.600 --> 02:01:43.360]   He is uh opposed to artificial intelligence on the battlefield
[02:01:43.360 --> 02:01:49.840]   In canada he built a neural network that could teach itself to identify flowers dogs and cars
[02:01:49.920 --> 02:01:51.760]   I remember that actually
[02:01:51.760 --> 02:01:57.120]   In fact, I think we had one of his students on our show up in canada as I remember
[02:01:57.120 --> 02:02:00.240]   chief scientist, uh, let's see
[02:02:00.240 --> 02:02:05.840]   Google spent 44 million dollars to hire him and his students and acquire their company
[02:02:05.840 --> 02:02:11.920]   Uh, he got the Turing award for their work on no neural networks
[02:02:11.920 --> 02:02:15.520]   But he's a little worried
[02:02:16.480 --> 02:02:20.160]   He says as companies improve their AI systems. They become increasingly dangerous
[02:02:20.160 --> 02:02:26.320]   Look how it was five years ago and how it is now take the difference and propagate it forwards that's scary
[02:02:26.320 --> 02:02:32.000]   He's also I mean
[02:02:32.000 --> 02:02:37.520]   Really respect what what he did there, but he's also esthetic estate 75
[02:02:37.520 --> 02:02:41.280]   Yeah, which is still younger than you know a lot of the politicians we have but
[02:02:41.280 --> 02:02:43.200]   Um
[02:02:43.200 --> 02:02:45.840]   I think everybody's younger than a lot of the politicians we have
[02:02:46.400 --> 02:02:49.920]   Yes, but at this point, you know, maybe he's just too old
[02:02:49.920 --> 02:02:58.160]   I don't know if he's too old, but sort of it's time to retire. Yeah. Yeah, it's an interesting good way to retire to say I
[02:02:58.160 --> 02:03:02.480]   Can't yeah, I resigned protest
[02:03:02.480 --> 02:03:08.240]   He had his i mean if he's having his op and heimer moment you remember when robert op and heimer the physicist
[02:03:08.240 --> 02:03:10.240]   who helped
[02:03:10.240 --> 02:03:16.240]   Perfect the atom bomb first saw he says now I now I am become uh death the stroller of worlds
[02:03:16.240 --> 02:03:21.200]   He was very adamantly afraid of what the atom bomb would bring in that I understand
[02:03:21.200 --> 02:03:24.560]   Uh, that was not reasonable
[02:03:24.560 --> 02:03:28.960]   Yeah, I mean, I don't think his standpoint is I don't think he's nothing genuine
[02:03:28.960 --> 02:03:33.760]   Right just for I don't think right really just embellishing or but I don't see any concrete
[02:03:33.760 --> 02:03:37.520]   Thing to worry about that he's talking about honestly well
[02:03:39.200 --> 02:03:45.840]   Right, but none of us have the perspective that he might have so so there you go. Yeah, you know
[02:03:45.840 --> 02:03:50.080]   It's maybe just maybe food for thought right it's uh something then
[02:03:50.080 --> 02:03:56.000]   His one good point one good point is as hard as he says it's hard to see how you can prevent
[02:03:56.000 --> 02:04:00.880]   Bad actors from using ai for bad things, but that's true of all
[02:04:00.880 --> 02:04:06.560]   Technical advances, right? Oh the same thing about crypto
[02:04:07.760 --> 02:04:09.760]   Cryptography not cryptocurrency
[02:04:09.760 --> 02:04:14.880]   But look at all the ways that we went down the potentials even earlier in this show where
[02:04:14.880 --> 02:04:19.920]   Things could go down very bad roads straight if if unchecked so
[02:04:19.920 --> 02:04:25.280]   Maybe the warning is you know, maybe he's just trying to get the warning out there just to make people
[02:04:25.280 --> 02:04:33.520]   Think will soon or perchai on wednesday take any time to reassure people he's got to explain why google is so far behind
[02:04:35.360 --> 02:04:42.000]   It was just thinking about that because sunday used to go out on on stage at io every year and talk about ai
[02:04:42.000 --> 02:04:48.560]   Right people were just like oh, that's his thing right now. It's everybody's thing and he's kind of behind actually so it sort of turns
[02:04:48.560 --> 02:04:50.720]   Combination as heads it's gonna be interesting for sure
[02:04:50.720 --> 02:04:57.840]   We will be watching jeff javison. I uh 10 a.m. Pacific on wednesday rip for windows weekly and
[02:04:57.840 --> 02:05:01.360]   This week in google google google. I owe me 10th
[02:05:02.320 --> 02:05:05.520]   Used to be something I'd look forward to so with great, you know
[02:05:05.520 --> 02:05:12.480]   Excitement and i've laid the last three google. I was i mean, you know, I guess it was covid, but they've been very disappointing
[02:05:12.480 --> 02:05:16.560]   Go ahead. I'm sorry. Uh, yeah, I have an inkling he may temper it
[02:05:16.560 --> 02:05:21.600]   Maybe with you know if they are a little bit behind as we all can observe. Uh, maybe he
[02:05:21.600 --> 02:05:25.360]   uh strategically tempers the the there you know
[02:05:25.360 --> 02:05:28.080]   Sort of makes it a little bit
[02:05:28.560 --> 02:05:31.600]   Washing over them being behind by saying oh, we're being extremely cautious
[02:05:31.600 --> 02:05:37.440]   Well, that's kind of what the temp cooks position was when they apple quarterly results came out on thursday
[02:05:37.440 --> 02:05:40.640]   um his three word response
[02:05:40.640 --> 02:05:46.160]   I don't know it was more than three words, but uh jason aton writing for ink
[02:05:46.160 --> 02:05:48.720]   Says I liked his three word response
[02:05:48.720 --> 02:05:53.440]   Which is uh, we're going to be deliberate and thoughtful
[02:05:53.440 --> 02:05:56.240]   in how we approach ai
[02:05:56.560 --> 02:05:58.560]   Uh apple way behind
[02:05:58.560 --> 02:06:05.040]   I mean the problem is we don't know if they're behind or not because they're very secretive. We know series way behind
[02:06:05.040 --> 02:06:10.160]   Uh, but who knows apple could have a killer ai just waiting in the wings
[02:06:10.160 --> 02:06:14.160]   I think ai is going to be the very most interesting topic of google. I own we look forward
[02:06:14.160 --> 02:06:16.880]   uh to talking about that
[02:06:16.880 --> 02:06:22.160]   Let's see what else uh before we the halo amazon halo. Bye. Bye
[02:06:23.520 --> 02:06:26.880]   But bye good article in the verge about all of the plans
[02:06:26.880 --> 02:06:30.160]   Amazon had chris welch writing
[02:06:30.160 --> 02:06:37.280]   To celebrity fitness classes and ai powered personal trainer apple watch support
[02:06:37.280 --> 02:06:41.840]   That was all supposed to come out this fall amazon killed it before they got there
[02:06:41.840 --> 02:06:45.840]   Hmm. I had a halo
[02:06:45.840 --> 02:06:48.000]   It was don't be
[02:06:49.360 --> 02:06:53.280]   And well the problem is you had fitbit you had apple watch you had all these
[02:06:53.280 --> 02:06:59.600]   Devices with screens and none of which required you to get naked and take pictures of yourself before you used them
[02:06:59.600 --> 02:07:02.880]   Nor did they have monthly subscription fees
[02:07:02.880 --> 02:07:08.800]   Well fitbit did but you didn't have to I I think it was just not a well thought out
[02:07:08.800 --> 02:07:13.200]   Plan any of you use the halo
[02:07:13.200 --> 02:07:17.280]   Nope, I use it for five minutes
[02:07:17.280 --> 02:07:20.800]   These devices though, it's always really unfortunate when they just you know, oh, yeah
[02:07:20.800 --> 02:07:25.360]   Supporting the platforms. Yeah people probably put a lot of money in there. Yeah
[02:07:25.360 --> 02:07:28.400]   I do uh, I do use your last sponsor
[02:07:28.400 --> 02:07:35.200]   You use what was oh express vpn got to use a vpn for years. Yeah. Yep. Yep. Yep
[02:07:35.200 --> 02:07:38.720]   um
[02:07:38.720 --> 02:07:42.240]   I guess that's pretty much it. I don't
[02:07:42.240 --> 02:07:45.200]   what tell us the latest
[02:07:45.200 --> 02:07:46.240]   greatest
[02:07:46.240 --> 02:07:50.640]   Technology advances in SSD L and melvin tonna. Oh boy anything
[02:07:50.640 --> 02:07:55.680]   You're just lining me up to really seem biased from my own. Oh, go ahead
[02:07:55.680 --> 02:08:01.680]   Let me guess is it coming to us from soladheim synergy 2.0
[02:08:01.680 --> 02:08:06.480]   Listen, if I was still running the beat doing the ssd reviews
[02:08:06.480 --> 02:08:08.160]   I would have been
[02:08:08.160 --> 02:08:15.440]   I would have loved to be able to be the person to write the article that john culter here was able to write in twig town
[02:08:15.680 --> 02:08:18.480]   So what is it? What is why is software matter?
[02:08:18.480 --> 02:08:21.760]   with an ssd? well
[02:08:21.760 --> 02:08:24.640]   As it turns out if you start really digging into
[02:08:24.640 --> 02:08:32.160]   performance of ssd's for various tasks and you start doing really detailed analysis and tracing and seeing okay
[02:08:32.160 --> 02:08:34.800]   Is there any low hanging fruit left?
[02:08:34.800 --> 02:08:38.880]   So we can squeeze out of the system and make this thing quicker
[02:08:38.880 --> 02:08:44.800]   That is not necessarily all about oh, we have to do gen five pci
[02:08:44.800 --> 02:08:47.920]   We have to do this we have to do we have to throw all this hardware at it
[02:08:47.920 --> 02:08:50.480]   Is it possible to squeeze?
[02:08:50.480 --> 02:08:54.640]   Some extra performance out from some optimizations elsewhere in the stack
[02:08:54.640 --> 02:08:58.400]   And as we have discovered the answer to that is yes
[02:08:58.400 --> 02:09:01.040]   And that is in the form of a driver
[02:09:01.040 --> 02:09:06.400]   Which we now make and as a part of this energy 2.0 software that we just happened to launch last Wednesday
[02:09:06.400 --> 02:09:10.240]   So when you get windows you get a standard microsoft nvme driver
[02:09:10.240 --> 02:09:12.720]   Can you replace this with a soladheim?
[02:09:12.720 --> 02:09:14.160]   uh driver
[02:09:14.160 --> 02:09:17.120]   You not only can you but our software does it for you and that's free
[02:09:17.120 --> 02:09:21.760]   And does it make a difference? I mean will it work with other ssd's?
[02:09:21.760 --> 02:09:26.880]   So there's a couple of points to the software. There's the driver piece and the driver piece
[02:09:26.880 --> 02:09:33.280]   Uh has some benefits for there's uh one of our ssd models called a p41 plus
[02:09:33.280 --> 02:09:39.200]   Which is a qlc ssd which has a cache as as most ssd's do especially qlc ssd's
[02:09:39.680 --> 02:09:45.520]   Um and our driver in that case helps that product to be smarter about how it uses the cache
[02:09:45.520 --> 02:09:50.160]   So it's able to do things that a normal ssd can't do because normal ssd's have no idea
[02:09:50.160 --> 02:09:52.160]   What's running around what's running on windows?
[02:09:52.160 --> 02:09:58.240]   Right, so the driver is that thing that can sort of link those two halves together a little bit more better
[02:09:58.240 --> 02:10:01.680]   More benefit in windows than you would say in linux or macOS
[02:10:01.680 --> 02:10:07.360]   Absolutely more benefit in windows. We have to focus on windows. That's where the i mean everybody knows
[02:10:07.360 --> 02:10:09.840]   That's where the bulk of everybody is and we're you know
[02:10:09.840 --> 02:10:14.960]   Our driver guys are working as hard as they can just to get to accomplish what they've accomplished so far
[02:10:14.960 --> 02:10:16.320]   So
[02:10:16.320 --> 02:10:21.840]   Unfortunately, we can't make it, you know work everywhere. We had to focus on the biggest fish right now and that is windows
[02:10:21.840 --> 02:10:22.800]   Um
[02:10:22.800 --> 02:10:29.200]   There's also some other pieces in the driver that are things that you wouldn't actually get a benefit from so much on the linux side
[02:10:29.200 --> 02:10:31.760]   Because there are things that the windows driver just
[02:10:31.760 --> 02:10:35.360]   Microsoft isn't gonna sit there and endlessly optimize their driver
[02:10:35.360 --> 02:10:37.360]   They're just gonna do what they need to do to make it work
[02:10:37.360 --> 02:10:38.400]   Um
[02:10:38.400 --> 02:10:43.920]   We had the extra cycles to burn on making it better and more optimized so we did a bunch of
[02:10:43.920 --> 02:10:49.200]   Optimization on you know on and along those avenues and found some low hanging fruit
[02:10:49.200 --> 02:10:51.680]   And we're able to get some extra gains there as well
[02:10:51.680 --> 02:10:56.080]   So gamers and pc other pc master race people
[02:10:56.080 --> 02:11:00.240]   Of probably long been aware that you can tweak your ssd's to get better performance
[02:11:00.240 --> 02:11:03.680]   Uh samsung offered software to do that for a while the magician
[02:11:04.160 --> 02:11:06.640]   Um, tweak town's conclusion is
[02:11:06.640 --> 02:11:09.360]   This is the successor
[02:11:09.360 --> 02:11:11.200]   Synergy 2.0
[02:11:11.200 --> 02:11:19.040]   It makes they said as much as an 8 percent difference in performance enough so that an inexpensive solid im ssd can outperform
[02:11:19.040 --> 02:11:21.600]   Uh the higher and um
[02:11:21.600 --> 02:11:25.600]   Samsung evo extreme models. That's pretty darn good
[02:11:25.600 --> 02:11:32.000]   Yeah, it's actually I was pleasantly surprised remember I usually tend to look at these things
[02:11:32.640 --> 02:11:35.680]   Objectively even if I happen to work at the company right and part of my job
[02:11:35.680 --> 02:11:37.840]   The company is just to make sure that yeah
[02:11:37.840 --> 02:11:42.320]   Make sure that we're testing the things correctly things like that right and uh, I was
[02:11:42.320 --> 02:11:44.960]   When I came on board and started testing
[02:11:44.960 --> 02:11:50.480]   Testing this new these new drivers even I was kind of like wow, okay. How much how much is it cost?
[02:11:50.480 --> 02:11:53.520]   How much does the driver cost nothing
[02:11:53.520 --> 02:11:57.360]   How much would you pay
[02:11:57.360 --> 02:12:00.800]   The driver is one piece. Uh, it's also there's also a synergy
[02:12:01.840 --> 02:12:08.880]   You uh, UI that's also a part of the package and that gives you your typical toolbox type functionality that you would have seen and with
[02:12:08.880 --> 02:12:11.280]   You know other competing things over the years, right?
[02:12:11.280 --> 02:12:16.880]   Everybody ssd maker tends to have their own little software package you can install to do various things like secure erase and other
[02:12:16.880 --> 02:12:21.120]   You know your other utility type functions of the of a for storage in general, right?
[02:12:21.120 --> 02:12:22.400]   Um
[02:12:22.400 --> 02:12:25.840]   We're doing our best to be more open about that as well
[02:12:25.840 --> 02:12:30.240]   So while the driver is the thing that's really just attached to to benefit our hardware
[02:12:30.880 --> 02:12:32.080]   um
[02:12:32.080 --> 02:12:36.720]   The the UI and then the toolkit type software that we that we are providing
[02:12:36.720 --> 02:12:42.720]   We try to make it as open as we can so for example to secure erase and ssd
[02:12:42.720 --> 02:12:45.200]   Uh, we are
[02:12:45.200 --> 02:12:50.640]   Opening up secure erase most companies only make it work with their own product. We're making it follow the standard
[02:12:50.640 --> 02:12:54.160]   You know nvm e secure erase type operations
[02:12:54.160 --> 02:12:57.360]   So as long as even if it's our competitors ssd
[02:12:57.360 --> 02:13:02.960]   We don't care as long as it conforms to the standard way to secure erase and ssd our software will let the user do it
[02:13:02.960 --> 02:13:06.800]   Right. We're not we're not trying to lock everybody into a to a walled garden
[02:13:06.800 --> 02:13:12.640]   You know, we really want our our software to be like a swiss army knife for doing ssd type things for a long time
[02:13:12.640 --> 02:13:14.960]   You know, I always recommend intel
[02:13:14.960 --> 02:13:17.280]   uh ssd's
[02:13:17.280 --> 02:13:23.680]   Probably because you told me they were the the best this before you even worked for intel and the and that software and the firmware was the best
[02:13:24.320 --> 02:13:28.960]   Uh, this is essentially intel's former ssd division, which they spun off
[02:13:28.960 --> 02:13:37.040]   Yes, these are not new ssd's right these are this is essentially a cute and uh, a continuation
[02:13:37.040 --> 02:13:42.160]   Of all of the work that had been done all of the pedigree from all of the work done back at intel
[02:13:42.160 --> 02:13:44.640]   Right. Just it's the same team. It's all the same folks
[02:13:44.640 --> 02:13:51.040]   Yeah, they just spun off and they you know began this new company this new initiative solidine and
[02:13:51.520 --> 02:13:54.720]   The work continues and you know the the engineers
[02:13:54.720 --> 02:14:00.480]   I don't know what it was but something about that spin-off and that move and everything the the engineering guys and the driver guys like they really just
[02:14:00.480 --> 02:14:06.480]   Something lit and they just started really, you know putting out some some amazing work
[02:14:06.480 --> 02:14:10.080]   So you get this driver software just by when you buy a solidine
[02:14:10.080 --> 02:14:12.560]   ssd
[02:14:12.560 --> 02:14:16.640]   I mean, it's not in the box. It's just there's a link. Oh you can go to the website download it
[02:14:16.640 --> 02:14:21.280]   So if you're using a solid i'm ssd on windows, you should probably get this software because it'll make a difference
[02:14:21.760 --> 02:14:27.840]   You're just leaving performance on the table if I don't right it's just silly. You know the other thing that blows me away is
[02:14:27.840 --> 02:14:35.120]   You know when ssd's first came out and one of the things we always talked to you about on this week in computer hardware was
[02:14:35.120 --> 02:14:38.880]   Well, are they gonna be reliable? They can be robust. They're gonna last as long as spinning drives
[02:14:38.880 --> 02:14:43.200]   They it seems to be are more robust and reliable than spinning drives. Yes
[02:14:43.200 --> 02:14:49.040]   I mean it depends but they're pretty there. No, there's no disadvantage
[02:14:50.240 --> 02:14:52.240]   Is there and there's a lot faster
[02:14:52.240 --> 02:14:54.560]   I mean
[02:14:54.560 --> 02:14:58.480]   Okay, there used to be a disadvantage in the fact of hard drives
[02:14:58.480 --> 02:15:04.960]   You could just keep writing and they would never wear out but modern hard drives actually start to come with an endurance rating of their own
[02:15:04.960 --> 02:15:09.280]   Right as far as how many petabytes you can pass through the the heads in the disc
[02:15:09.280 --> 02:15:16.320]   So that thing that used to be a differentiating factor is kind of no more at this point
[02:15:16.400 --> 02:15:23.120]   So it's there even more on on a level playing field. Well and the price per gigabyte used to be ridiculous for ssd's
[02:15:23.120 --> 02:15:28.080]   I'm see this is a solid. I'm too dare by dry for under a hundred bucks
[02:15:28.080 --> 02:15:34.720]   Yep, mind. Well, I don't know if you I don't know if you remember because you might have not caught the pc perspective podcast
[02:15:34.720 --> 02:15:37.760]   But like for years ryan shrill went on and on
[02:15:37.760 --> 02:15:44.800]   Chanting 10 cents a gig right as a price point that he was hoping that we would get to and the thing you just showed
[02:15:45.360 --> 02:15:47.360]   I've sense again
[02:15:47.360 --> 02:15:49.680]   Yes
[02:15:49.680 --> 02:15:51.680]   Unbelievable. We live in
[02:15:51.680 --> 02:15:55.440]   Amazing times. That's all I can say and thank you for the work you do
[02:15:55.440 --> 02:15:59.120]   You saw me so from now on no more samsung
[02:15:59.120 --> 02:16:02.400]   980 pros
[02:16:02.400 --> 02:16:04.400]   You get the same performance for less
[02:16:04.400 --> 02:16:10.000]   I'm saying it. I trust trust the independent or viewers not me. I work for the company
[02:16:10.000 --> 02:16:15.280]   So don't don't take my word as gospel. I'm pretty guys word. Yeah. Well, that's what the the tweak guy
[02:16:15.520 --> 02:16:17.520]   Said so I believe him
[02:16:17.520 --> 02:16:20.640]   Yeah, I mean, I'm I'm I'm happy to like
[02:16:20.640 --> 02:16:23.680]   I like that I'm working here. Yes, it's yeah
[02:16:23.680 --> 02:16:27.600]   It sucks that I no longer work in the press and no longer get to play in that in that battlefield
[02:16:27.600 --> 02:16:29.040]   but uh
[02:16:29.040 --> 02:16:34.720]   This makes up for it being knowing that I'm working on stuff that really, you know is doing good right nice new
[02:16:34.720 --> 02:16:38.720]   Review john colter just just came out a couple of days ago at tweak town
[02:16:38.720 --> 02:16:40.880]   dot com
[02:16:40.880 --> 02:16:43.600]   Get the get the new solid. I'm synergy
[02:16:44.080 --> 02:16:50.400]   Point oh ssd software when you get you ssd from so the day. Thank you allen. It's great to see you
[02:16:50.400 --> 02:16:57.040]   Someday, I just want you to do a tour of all the stuff on your bookshelves behind you. Oh, we don't have that kind of time
[02:16:57.040 --> 02:17:03.760]   Crazy what's going on back there? There's a lot there. Yeah, well made this I need to I need to prune it keeps growing
[02:17:03.760 --> 02:17:05.760]   I need the fourth be with you
[02:17:05.760 --> 02:17:09.200]   And and made this sith be with you as well
[02:17:09.200 --> 02:17:13.680]   Those are mostly the influence of my lovely wife who likes the star wars. Oh, okay
[02:17:14.160 --> 02:17:16.160]   Oh, okay. She's the geek huh?
[02:17:16.160 --> 02:17:19.600]   Yep, great to see you. Thank you so much for being here
[02:17:19.600 --> 02:17:25.840]   From joining us from the wonderful semaphore and I am not blowing smoke when I say how much I love semaphore
[02:17:25.840 --> 02:17:29.600]   I think you guys are I think this has turned out to be a wonderful
[02:17:29.600 --> 02:17:36.320]   publication especially because of the international coverage and of course the stellar technology coverage from luiz
[02:17:36.320 --> 02:17:38.960]   Mizzocus read albargotti and the team
[02:17:38.960 --> 02:17:43.360]   You're doing a great. I love it. I love it. Thanks leo. I really appreciate your sport. Yeah
[02:17:44.160 --> 02:17:49.680]   Anything you want to plug you've got the newsletter subscribe to the semaphore tech newsletter for sure
[02:17:49.680 --> 02:17:55.520]   Just the newsletter. Yeah, please sign up. It's the best way to get our coverage we try and have
[02:17:55.520 --> 02:17:59.200]   You know a scoop in the u tradition and that's the first place you go
[02:17:59.200 --> 02:18:06.480]   But read it when are you gonna start doing youtube videos? I know they got they've got some youtube video you're gonna do the semaphore youtube videos
[02:18:06.480 --> 02:18:09.280]   Yeah, uh for sure. Yeah our
[02:18:09.840 --> 02:18:16.560]   Video team is amazing. Uh, we're on all the platforms. We're on tiktok or on youtube. Uh, cool. Yeah, it's going well. Nice
[02:18:16.560 --> 02:18:19.200]   So great to see you. Thank you luiz
[02:18:19.200 --> 02:18:23.040]   Appreciate you at mazzocus elmazzocus on twitter
[02:18:23.040 --> 02:18:30.160]   Okay, fine. You notice we didn't mention elon or twitter once on this show
[02:18:30.160 --> 02:18:32.960]   I I'm listening to you folks
[02:18:32.960 --> 02:18:35.920]   I hate it when we do that
[02:18:35.920 --> 02:18:40.880]   Uh, thank you luiz and thank you so much. It's great to have you back on yanko. I won't you won't be a stranger
[02:18:40.880 --> 02:18:47.600]   I I like I said we quote you all the time. We might as we should get you on the show too anytime. Yeah goes new
[02:18:47.600 --> 02:18:48.800]   Uh
[02:18:48.800 --> 02:18:53.920]   newsletter, which I just subscribe to is low pass at low pass
[02:18:53.920 --> 02:18:56.640]   dot cc
[02:18:56.640 --> 02:18:58.640]   and you cover the
[02:18:58.640 --> 02:19:02.160]   confluence of technology and media is that a good way to put it?
[02:19:02.800 --> 02:19:11.040]   Technology media entertainment and smart speakers smart TVs aov are all the fun stuff. Oh, should I ask you about smart speakers?
[02:19:11.040 --> 02:19:18.000]   Are they gonna are are we gonna get chat gpt built into smart speakers soon? Oh, boy
[02:19:18.000 --> 02:19:23.600]   Hey, let me tell you what I know about that have you
[02:19:23.600 --> 02:19:26.720]   So jimmy
[02:19:26.720 --> 02:19:31.840]   A smart speaker man's blaming stuff to you. That's great. That's great. This this this
[02:19:32.160 --> 02:19:35.200]   This uh, this AI model has been trained on the microphone
[02:19:35.200 --> 02:19:39.120]   That nobody knew was on at least he can do it in tom wade's voice
[02:19:39.120 --> 02:19:47.200]   Uh, I am a subscriber. Thank you yanko. Appreciate your being here low pass dot cc
[02:19:47.200 --> 02:19:52.560]   There is a free tier by the way you can you can you can get there that newsletter every single day if you want
[02:19:52.560 --> 02:19:55.280]   Or every thursday, I should say if you want every Thursday. Yeah, yeah
[02:19:55.280 --> 02:19:57.440]   Uh
[02:19:57.440 --> 02:20:01.760]   Thank you all for joining us. We do this week in tech every sunday right after ask the tech guys
[02:20:02.320 --> 02:20:06.320]   We start around 2 pm pacific 5 pm eastern 2100 utc
[02:20:06.320 --> 02:20:09.120]   There's a live you don't have to watch it live by the way
[02:20:09.120 --> 02:20:11.200]   I think people think i'm saying you got to watch it live you don't
[02:20:11.200 --> 02:20:16.160]   In fact, you know most people don't but if you wanted to watch it live of a sunday afternoon
[02:20:16.160 --> 02:20:21.680]   You're not busy doing anything else go to live dot twit.tv. You could chat with us live as you're watching
[02:20:21.680 --> 02:20:27.120]   We have a uh, irc open to all irc dot twit.tv. You don't even need an irc client
[02:20:27.120 --> 02:20:32.480]   Use your web browser irc dot twit.tv. Of course our club twit members get their very own
[02:20:32.480 --> 02:20:35.280]   cool kids channel in our discord
[02:20:35.280 --> 02:20:37.840]   Uh, which is a lot of fun
[02:20:37.840 --> 02:20:39.680]   Uh
[02:20:39.680 --> 02:20:46.560]   They're already voting on the show tile. We didn't mention elan once you know there's a a ai leo in our uh our discord
[02:20:46.560 --> 02:20:49.040]   You can you can talk here. Yeah, I know
[02:20:49.040 --> 02:20:52.000]   I know and he's smarter than I am
[02:20:52.000 --> 02:20:53.840]   he's
[02:20:53.840 --> 02:20:56.160]   Or did they take did you take ai leo out? I don't
[02:20:57.040 --> 02:21:00.640]   Where'd he go? He's gone. No. Oh, I don't know
[02:21:00.640 --> 02:21:03.840]   He's in there somewhere because I know he's he's working
[02:21:03.840 --> 02:21:06.720]   Man how many channels are in that there's a lot of channels
[02:21:06.720 --> 02:21:11.520]   It's a it's a whole community. It's just what you were talking about louise
[02:21:11.520 --> 02:21:13.360]   It's kind of a closed community where you could trust
[02:21:13.360 --> 02:21:16.080]   Uh what you read there because it's good people
[02:21:16.080 --> 02:21:22.400]   In there after the fact we do have on-demand versions of the show ad supported at twit.tv
[02:21:23.040 --> 02:21:27.200]   Uh when you're at the twit this week in tech page, you'll see a link to the youtube channel
[02:21:27.200 --> 02:21:29.680]   That's also available to all
[02:21:29.680 --> 02:21:35.920]   You could see the various podcast clients you can subscribe just by a single click or get the rss feed
[02:21:35.920 --> 02:21:41.120]   Or just search for this week in tech in your favorite podcast client and get it automatically the minutes available
[02:21:41.120 --> 02:21:44.080]   We do audio and video you uh you choose
[02:21:44.080 --> 02:21:50.400]   Pick the one you like best. Thanks everybody for being here. I hope you have a wonderful week
[02:21:50.880 --> 02:21:52.960]   I'll be back on uh, tuesday
[02:21:52.960 --> 02:21:55.920]   With mac break weekly security now and then effect of course wednesday
[02:21:55.920 --> 02:21:59.840]   Don't forget our live coverage of google ios starting at 10am pacific
[02:21:59.840 --> 02:22:04.160]   Uh along with this week in google we'll be talking all about what google announces
[02:22:04.160 --> 02:22:07.440]   This week have a great week. We'll see you next time another twit
[02:22:07.440 --> 02:22:10.000]   This is the camp
[02:22:10.000 --> 02:22:18.000]   Do the twit do the twit all right do the twit baby do the twit all right
[02:22:18.000 --> 02:22:19.340]   - Right, move it.

