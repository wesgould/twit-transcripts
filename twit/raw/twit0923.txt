
[00:00:00.000 --> 00:00:04.560]   Coming up next on This Week in Tech, it's me, Jason Howell, filling in for Leo.
[00:00:04.560 --> 00:00:07.360]   One last time I promised Leo's going to be back next week.
[00:00:07.360 --> 00:00:11.920]   We've got Ant Pruitt, we've got Dan Patterson, and Jason Heiner.
[00:00:11.920 --> 00:00:15.360]   And we have a fantastic episode, lots of news to talk about.
[00:00:15.360 --> 00:00:18.200]   Substack notes, how does it compare to Twitter?
[00:00:18.200 --> 00:00:20.280]   Does it even compare to Twitter at all?
[00:00:20.280 --> 00:00:21.280]   We talk about that.
[00:00:21.280 --> 00:00:27.480]   Also, the rules around AI and how many different governments and countries around the world
[00:00:27.480 --> 00:00:32.640]   are quickly approaching this point where they feel they need to put some guardrails around
[00:00:32.640 --> 00:00:35.400]   the direction of the development of AI.
[00:00:35.400 --> 00:00:36.400]   Is it warranted?
[00:00:36.400 --> 00:00:37.400]   We talk about that.
[00:00:37.400 --> 00:00:40.720]   Morale at meta, not doing so hot.
[00:00:40.720 --> 00:00:43.880]   WWDC and Apple's reality headset.
[00:00:43.880 --> 00:00:46.080]   Are we actually going to see it in a couple of months?
[00:00:46.080 --> 00:00:48.480]   Mark Gurman seems to think that we will.
[00:00:48.480 --> 00:00:51.720]   And computer magazines, we shed a single tear.
[00:00:51.720 --> 00:00:53.000]   They are no more.
[00:00:53.000 --> 00:00:54.080]   This week in Tech is next.
[00:00:54.080 --> 00:00:55.080]   Podcasts you love.
[00:00:55.080 --> 00:00:56.080]   From people you trust.
[00:00:56.080 --> 00:00:57.160]   This is Twitter.
[00:00:57.160 --> 00:00:59.120]   This is Twitter.
[00:00:59.120 --> 00:01:01.080]   This is Twitter.
[00:01:01.080 --> 00:01:10.000]   This is Twitter.
[00:01:10.000 --> 00:01:11.240]   This is Twitter.
[00:01:11.240 --> 00:01:17.240]   This week in Tech, episode 923 recorded Sunday, April 16, 2023.
[00:01:17.240 --> 00:01:20.280]   Intelligence, explosion.
[00:01:20.280 --> 00:01:23.200]   This episode of This Week in Tech is brought to you by Collide.
[00:01:23.200 --> 00:01:29.200]   Collide is a device trust solution that ensures that if a device isn't secure, it can't access
[00:01:29.200 --> 00:01:30.200]   your apps.
[00:01:30.200 --> 00:01:32.560]   It's zero trust for Okta.
[00:01:32.560 --> 00:01:37.280]   Visit collide.com/twit and book a demo today.
[00:01:37.280 --> 00:01:42.080]   And by Cisco Moraki, with employees working in different locations, providing a unified
[00:01:42.080 --> 00:01:45.720]   work experience seems as easy as hurting cats.
[00:01:45.720 --> 00:01:47.640]   How do you rein in so many moving parts?
[00:01:47.640 --> 00:01:50.520]   Well, the Moraki Cloud Managed Network.
[00:01:50.520 --> 00:01:54.160]   And how your organization can make hybrid work work.
[00:01:54.160 --> 00:01:57.640]   Visit moraki.cisco.com/twit.
[00:01:57.640 --> 00:01:59.040]   And by decisions.
[00:01:59.040 --> 00:02:02.280]   Don't let complexity block your company's growth.
[00:02:02.280 --> 00:02:06.160]   Decisions no code rules-driven process automation software.
[00:02:06.160 --> 00:02:11.000]   Provides every tool needed to build custom workflows, empowering you to modernize legacy
[00:02:11.000 --> 00:02:16.920]   systems, ensure regulatory compliance, and renew the customer experience.
[00:02:16.920 --> 00:02:23.400]   Decisions.com/twit to learn how automating anything can change everything.
[00:02:23.400 --> 00:02:25.160]   And by ExpressVPN.
[00:02:25.160 --> 00:02:29.280]   If you don't like big tech tracking you and selling your personal data for profit, it's
[00:02:29.280 --> 00:02:31.280]   time to fight back.
[00:02:31.280 --> 00:02:43.640]   Get three extra months free with a one year package by going to expressvpn.com/twit.
[00:02:43.640 --> 00:02:48.440]   This week in tech week three of Leo's absence.
[00:02:48.440 --> 00:02:52.360]   This week it is I, Jason Howell, sitting in for Leo.
[00:02:52.360 --> 00:02:58.160]   So if you are waiting for Leo to suddenly return in his Dr. Evil chair, don't worry.
[00:02:58.160 --> 00:02:59.800]   Don't have to wait much longer.
[00:02:59.800 --> 00:03:02.880]   Next week's episode, Leo will be back.
[00:03:02.880 --> 00:03:06.520]   And we've got a great episode next week in store for you guys.
[00:03:06.520 --> 00:03:07.760]   But I'm not going to talk about that.
[00:03:07.760 --> 00:03:12.720]   I'm going to talk about our awesome episode that we have today because this is kind of
[00:03:12.720 --> 00:03:19.800]   one of the benefits that we have here when I or Micah or any other person sits down in
[00:03:19.800 --> 00:03:22.200]   the host chair for this week in tech.
[00:03:22.200 --> 00:03:25.600]   We get to decide who do we want to do this show with?
[00:03:25.600 --> 00:03:29.440]   And I got to say today's panel kind of fell together.
[00:03:29.440 --> 00:03:36.800]   I think initially maybe I had already booked you Dan Patterson possibly on this episode.
[00:03:36.800 --> 00:03:39.360]   And that was the beginnings of this.
[00:03:39.360 --> 00:03:43.240]   And first of all, welcome to the show, Dan Patterson.
[00:03:43.240 --> 00:03:44.240]   It's great to be here.
[00:03:44.240 --> 00:03:45.480]   Yeah, it's good.
[00:03:45.480 --> 00:03:52.000]   And also kind of on the heels of some changes in your professional life.
[00:03:52.000 --> 00:03:53.440]   And I'll talk about that for a second.
[00:03:53.440 --> 00:03:54.680]   Tell us what's going on.
[00:03:54.680 --> 00:03:55.680]   Yeah.
[00:03:55.680 --> 00:04:02.560]   So I was in fact, I started my CBS career with Jason Heiner also on this panel years
[00:04:02.560 --> 00:04:03.560]   ago.
[00:04:03.560 --> 00:04:07.920]   And I left CBS to work for a cybersecurity company last summer.
[00:04:07.920 --> 00:04:13.640]   And then was like so many tech people was laid off a couple of weeks ago.
[00:04:13.640 --> 00:04:20.680]   And then I'm like a reprise now writing about AI and conducting interviews with AI thought
[00:04:20.680 --> 00:04:27.640]   leaders for Jason Heiner and doing some work for Fortune magazine for Twit and a few other
[00:04:27.640 --> 00:04:28.640]   outlets.
[00:04:28.640 --> 00:04:33.880]   So it's yeah, I mean, it's so many.
[00:04:33.880 --> 00:04:39.240]   I think it's a huge story, this change in our economy, not just with tech, but the tech
[00:04:39.240 --> 00:04:44.880]   economy has the New York Times had a great story written today by Aaron Griffith about
[00:04:44.880 --> 00:04:47.720]   the changes in the tech economy and in Silicon Valley.
[00:04:47.720 --> 00:04:50.160]   And it's look, it's not something personal to me.
[00:04:50.160 --> 00:04:57.000]   It's impacted so many people that it's existential, but it's also, you know, there are practical
[00:04:57.000 --> 00:05:00.320]   problems to solve and great podcasts that come on like Twit.
[00:05:00.320 --> 00:05:01.600]   Yeah, there you go.
[00:05:01.600 --> 00:05:06.360]   I'm sorry about the changes in force of transition.
[00:05:06.360 --> 00:05:10.640]   No one likes change, but really how we can get you on today.
[00:05:10.640 --> 00:05:12.000]   And that's great to be here.
[00:05:12.000 --> 00:05:13.240]   Yeah, bring it back.
[00:05:13.240 --> 00:05:16.880]   So anyways, my story, Dan, you were booked for this.
[00:05:16.880 --> 00:05:19.240]   Then suddenly I'm hosting this episode.
[00:05:19.240 --> 00:05:21.720]   And then it's very in Heiner.
[00:05:21.720 --> 00:05:24.240]   You come along and I'm like, okay, well, you've got to be on this panel.
[00:05:24.240 --> 00:05:27.440]   So Jason Heiner, ZD net editor in chief.
[00:05:27.440 --> 00:05:30.720]   Great to see you, Jason, just to make things a little bit more complicated because now
[00:05:30.720 --> 00:05:32.800]   there's two Jason's on today's panel.
[00:05:32.800 --> 00:05:33.800]   Yeah, yeah.
[00:05:33.800 --> 00:05:34.800]   A pleasure.
[00:05:34.800 --> 00:05:39.480]   And when you told me, hey, Dan and Ant are going to be on, I'm like, absolutely, if
[00:05:39.480 --> 00:05:42.480]   we'd hang out with all of you all.
[00:05:42.480 --> 00:05:45.760]   But I did say, but I have to let you know, Dan's going to be starting.
[00:05:45.760 --> 00:05:46.760]   Yeah.
[00:05:46.760 --> 00:05:47.760]   You know, we couldn't know that.
[00:05:47.760 --> 00:05:51.040]   We couldn't be that for me.
[00:05:51.040 --> 00:05:56.160]   And so, you know, thank you for letting us, you know, have half the panel here from,
[00:05:56.160 --> 00:05:57.840]   from, from ZD net.
[00:05:57.840 --> 00:06:01.640]   Yeah, always a pleasure and Dan's always been amazing.
[00:06:01.640 --> 00:06:06.480]   I followed Dan for many years before we even became friends and then was lucky enough
[00:06:06.480 --> 00:06:12.720]   to hire him when I worked at CBS Interactive and Tech Republic.
[00:06:12.720 --> 00:06:13.720]   Yes.
[00:06:13.720 --> 00:06:20.480]   And then, and now, you know, here we are again and also just great to get to hang out with
[00:06:20.480 --> 00:06:25.680]   with Dan, with you, Jason, and of course, with our good buddy, Ant Pruitt too.
[00:06:25.680 --> 00:06:26.680]   That's right.
[00:06:26.680 --> 00:06:27.680]   And I'm tired at one point.
[00:06:27.680 --> 00:06:28.680]   I know.
[00:06:28.680 --> 00:06:29.680]   That's true.
[00:06:29.680 --> 00:06:30.680]   That's true.
[00:06:30.680 --> 00:06:34.000]   And this is why I spell out the story because as the pieces started to fall into place,
[00:06:34.000 --> 00:06:37.480]   then I was like, well, Ant's got to be on this episode because he's worth with both
[00:06:37.480 --> 00:06:39.560]   of these guys and knows them very well.
[00:06:39.560 --> 00:06:46.120]   So, Ant Pruitt is here to put that TV slash hands-on photography or hop, but, you know,
[00:06:46.120 --> 00:06:48.920]   club twit manager this week in Google.
[00:06:48.920 --> 00:06:53.400]   I mean, doer of so many things here at Twit and also good friend and it's great to see
[00:06:53.400 --> 00:06:54.400]   you.
[00:06:54.400 --> 00:06:58.440]   And also known as the non-professional here on this week's panel.
[00:06:58.440 --> 00:07:00.200]   Wait a minute.
[00:07:00.200 --> 00:07:02.600]   Why do you call yourself non-professional?
[00:07:02.600 --> 00:07:04.960]   You're a professional podcaster.
[00:07:04.960 --> 00:07:06.280]   That's all I have.
[00:07:06.280 --> 00:07:07.840]   Just the regular guy here.
[00:07:07.840 --> 00:07:11.800]   It's going to sit here and rant every now and then and have a few jaded moments, but
[00:07:11.800 --> 00:07:18.200]   I enjoy talking tech with these guys and I'll be brief, but just want to get it out there.
[00:07:18.200 --> 00:07:21.720]   These two gentlemen, man, they mean so much to me.
[00:07:21.720 --> 00:07:26.800]   Patterson has been a mentor, whether he wants to accept that or not.
[00:07:26.800 --> 00:07:32.200]   He's been a mentor to me for several years now and then I always have much love for Mr.
[00:07:32.200 --> 00:07:36.280]   Heiner because he took a shot on me and said, you know what?
[00:07:36.280 --> 00:07:39.080]   Come over here and give us some stuff to write.
[00:07:39.080 --> 00:07:42.680]   And I was just, oh, I love you guys.
[00:07:42.680 --> 00:07:44.920]   Yeah, same.
[00:07:44.920 --> 00:07:49.960]   You paid that back in spades and like what you're, you know, with the great work you
[00:07:49.960 --> 00:07:55.880]   do, like you came and when we hired you to write some stuff for tech republic and to
[00:07:55.880 --> 00:08:00.720]   work for us, like, you know, you hustled so much and did so much, you know, great work
[00:08:00.720 --> 00:08:06.280]   communicating and so glad that, you know, it opened more doors for you, well deserved
[00:08:06.280 --> 00:08:09.080]   and obviously the work that you're doing it to it now.
[00:08:09.080 --> 00:08:10.080]   You know, awesome as well.
[00:08:10.080 --> 00:08:11.960]   Just writing your wheelhouse too.
[00:08:11.960 --> 00:08:12.960]   Thank you so much.
[00:08:12.960 --> 00:08:14.960]   You're all awesome.
[00:08:14.960 --> 00:08:18.800]   Each and every one of you is awesome.
[00:08:18.800 --> 00:08:22.320]   This is why this is why this panel needed to happen.
[00:08:22.320 --> 00:08:24.920]   Jason Howl and I work together at CNN.
[00:08:24.920 --> 00:08:26.760]   I know that as well.
[00:08:26.760 --> 00:08:27.760]   True.
[00:08:27.760 --> 00:08:31.240]   Well, well, Dan, I guess you were at CBS with here.
[00:08:31.240 --> 00:08:35.240]   I'm trying to think I was at CNET until like 2010.
[00:08:35.240 --> 00:08:38.000]   I think that think September 2010 is when I left.
[00:08:38.000 --> 00:08:40.480]   Were you at CBS when I was at CNET?
[00:08:40.480 --> 00:08:46.360]   I went to I went to tech republic in 2015, CNET in 2018 and CBS.
[00:08:46.360 --> 00:08:51.120]   I was, I was, air quotes embedded at CBS news a couple of months later.
[00:08:51.120 --> 00:08:56.760]   So I worked out of the broadcast center, but I was at I split my time with CNET and then
[00:08:56.760 --> 00:08:58.760]   full of CBS in 2020.
[00:08:58.760 --> 00:08:59.760]   Okay.
[00:08:59.760 --> 00:09:04.680]   So you were a little bit later than my time there, but I kind of I consider you and I
[00:09:04.680 --> 00:09:06.240]   co-workers to a certain degree.
[00:09:06.240 --> 00:09:07.240]   I remember that.
[00:09:07.240 --> 00:09:08.240]   Well, that's CNET.
[00:09:08.240 --> 00:09:11.040]   Like if you if you work for CNET, like you always work for CNET.
[00:09:11.040 --> 00:09:12.560]   Yeah, very true.
[00:09:12.560 --> 00:09:13.560]   Very, very true.
[00:09:13.560 --> 00:09:15.640]   Well, it's great to get you all on here.
[00:09:15.640 --> 00:09:19.400]   We're going to have a lot of fun talking about some of the stories from the past week.
[00:09:19.400 --> 00:09:25.840]   Thought we'd start with sub stack as just what I mean, we could have started with AI.
[00:09:25.840 --> 00:09:29.880]   And honestly, I almost started with AI, but then I was like, I feel like every show starts
[00:09:29.880 --> 00:09:31.040]   with AI these days.
[00:09:31.040 --> 00:09:35.180]   So let's start with something else and then end up in AI because that's just where everything's
[00:09:35.180 --> 00:09:36.580]   headed here.
[00:09:36.580 --> 00:09:39.920]   But sub stack, of course, everybody on this panel, very familiar with it.
[00:09:39.920 --> 00:09:45.160]   Dan, I know that you have a sub stack now kind of as part of your, your new thing.
[00:09:45.160 --> 00:09:50.080]   This is the newsletter platform that everybody's pretty familiar with now has a new feature
[00:09:50.080 --> 00:09:56.120]   called notes, which in, I don't know, in many ways from what I've read, what I've seen of
[00:09:56.120 --> 00:10:01.560]   the interface and everything strikes some similarity with a little social network we're
[00:10:01.560 --> 00:10:05.400]   familiar with Twitter rolled out to users on Tuesday.
[00:10:05.400 --> 00:10:10.640]   Let's just start with that because there are other directions that this goes that get a
[00:10:10.640 --> 00:10:12.040]   little bit more complicated.
[00:10:12.040 --> 00:10:15.120]   But prior to all that, how well, Dan, let's start with that.
[00:10:15.120 --> 00:10:19.640]   With you, because you're the one that's kind of writing content for sub stack, for your
[00:10:19.640 --> 00:10:20.960]   own sub stack.
[00:10:20.960 --> 00:10:21.960]   What do you think of notes?
[00:10:21.960 --> 00:10:23.360]   Have you checked it out?
[00:10:23.360 --> 00:10:27.880]   Do you feel like it's any sort of threat to anything that Twitter or other social networks
[00:10:27.880 --> 00:10:28.960]   have going on?
[00:10:28.960 --> 00:10:29.960]   My guess is no.
[00:10:29.960 --> 00:10:30.960]   I'll start with it.
[00:10:30.960 --> 00:10:32.880]   Yeah, with your second question, and that's no.
[00:10:32.880 --> 00:10:36.520]   And that is, you know, I think we'll get into the details later.
[00:10:36.520 --> 00:10:41.480]   Mechanically, the features on Twitter are pretty easy to mimic and have been for a long
[00:10:41.480 --> 00:10:42.480]   time.
[00:10:42.480 --> 00:10:45.480]   Look, it was built in Ruby on Rails and later in JavaScript.
[00:10:45.480 --> 00:10:49.800]   And it's not a terribly difficult site to recreate mechanically.
[00:10:49.800 --> 00:10:57.480]   But of course, as an information network and somewhat of a social network, it's irreplaceable.
[00:10:57.480 --> 00:11:02.720]   You can copy some of the features and sub stack has not just a tremendous content network,
[00:11:02.720 --> 00:11:03.920]   but a social network.
[00:11:03.920 --> 00:11:09.880]   And so it's really great to use notes and see regular updates from people that I follow
[00:11:09.880 --> 00:11:15.320]   or subscribe to, as well as read the long form content that they produce.
[00:11:15.320 --> 00:11:21.960]   But in terms of being a Twitter replacement, I don't know that anything is or will be,
[00:11:21.960 --> 00:11:24.200]   at least in the short term.
[00:11:24.200 --> 00:11:25.720]   Mm hmm.
[00:11:25.720 --> 00:11:32.280]   Either you, Ant or Jason, have either of you used notes or I don't know.
[00:11:32.280 --> 00:11:36.720]   I'm not the prime candidate because I don't follow a lot of newsletters.
[00:11:36.720 --> 00:11:41.320]   But when I logged into it and took a look at it, I was like, wow, this is a really pretty
[00:11:41.320 --> 00:11:42.320]   Twitter.
[00:11:42.320 --> 00:11:48.440]   It looked just like a Twitter clone, except it was orange and black, instead of blue and
[00:11:48.440 --> 00:11:50.640]   black.
[00:11:50.640 --> 00:11:55.920]   And I wondered about that far as would there be some type of, I don't know, is there anything
[00:11:55.920 --> 00:12:02.880]   infringing far as how Twitter has their service and platform put together versus what sub stack
[00:12:02.880 --> 00:12:08.280]   did here, because you really couldn't tell the two apart unless you look at logos or
[00:12:08.280 --> 00:12:10.960]   color scheme or what have you.
[00:12:10.960 --> 00:12:16.640]   And far as it replaced in Twitter, no, I don't, I don't know if anything's going to do that.
[00:12:16.640 --> 00:12:22.040]   We've been talking about the Twitter alternative, the Instagram alternative, the Facebook alternative
[00:12:22.040 --> 00:12:24.600]   for a decade now.
[00:12:24.600 --> 00:12:27.120]   All those cats are still here for a reason.
[00:12:27.120 --> 00:12:29.040]   It can't be done.
[00:12:29.040 --> 00:12:34.320]   It's easy to jump to the question, will this replace Twitter in this day and age, year
[00:12:34.320 --> 00:12:41.080]   2023, primarily because we've got all the Elon Musk drama to kind of fuel that question.
[00:12:41.080 --> 00:12:42.080]   Right?
[00:12:42.080 --> 00:12:47.240]   It's like, well, Twitter has, doesn't seem like it's been doing itself any favors or favors.
[00:12:47.240 --> 00:12:52.320]   Surely, there's someone that's going to swoop in and take the, you know, take what's left
[00:12:52.320 --> 00:12:54.560]   and divert people over there.
[00:12:54.560 --> 00:13:01.280]   You know, with the exodus of people or so-called exodus of people leaving Twitter, there's still
[00:13:01.280 --> 00:13:06.280]   just as many active users going on day in and day out of me.
[00:13:06.280 --> 00:13:10.560]   And some of those are, some of those are still high profile accounts or celebrities or what
[00:13:10.560 --> 00:13:14.360]   have you because it's fine for broadcasting, if you will.
[00:13:14.360 --> 00:13:16.920]   So they don't go anywhere.
[00:13:16.920 --> 00:13:18.640]   All the ads left, we're not all the ads.
[00:13:18.640 --> 00:13:23.760]   All of the, some of the top brands left because they didn't want to be associated with it.
[00:13:23.760 --> 00:13:28.560]   Twitter's still just taking along because it's just a useful platform for people.
[00:13:28.560 --> 00:13:30.600]   I have some challenges with that.
[00:13:30.600 --> 00:13:36.880]   I mean, like, I don't, I feel like, I mean, Musk is a controversial figure for a lot of
[00:13:36.880 --> 00:13:42.840]   reasons, but I do feel like using it is a little like you're giving tacit permission
[00:13:42.840 --> 00:13:44.920]   to some of his behavior.
[00:13:44.920 --> 00:13:51.600]   And I, you know, I don't know that I come down on this as like a, like a binary.
[00:13:51.600 --> 00:13:56.040]   Like it is absolute one way or the other, but I certainly can empathize with that point
[00:13:56.040 --> 00:13:57.040]   of view.
[00:13:57.040 --> 00:14:02.880]   I deleted all of my tweets just because I auto delete everything and I haven't posted
[00:14:02.880 --> 00:14:04.360]   in several months.
[00:14:04.360 --> 00:14:11.480]   But I, I continue to log in and check it, although I think the, this new algorithm on
[00:14:11.480 --> 00:14:16.200]   the for you section at least is, is kind of made a lot of the, the news features of Twitter
[00:14:16.200 --> 00:14:17.280]   fairly worthless.
[00:14:17.280 --> 00:14:23.760]   It's very hard for me to see the news updates that I was pretty accustomed to.
[00:14:23.760 --> 00:14:24.760]   Yeah.
[00:14:24.760 --> 00:14:25.760]   Yeah.
[00:14:25.760 --> 00:14:31.240]   You know, Twitter, boy, it's a tough one right now because I do think Twitter in one sense
[00:14:31.240 --> 00:14:37.360]   is a, is a, is a caterpillar that's trying to become a butterfly and nobody knows exactly
[00:14:37.360 --> 00:14:41.360]   what that butterfly is or what it looks like or where they're going.
[00:14:41.360 --> 00:14:42.360]   I'm not sure.
[00:14:42.360 --> 00:14:43.360]   Coming up.
[00:14:43.360 --> 00:14:44.360]   Nose.
[00:14:44.360 --> 00:14:46.360]   I think it might be becoming a mom.
[00:14:46.360 --> 00:14:48.040]   It's fair.
[00:14:48.040 --> 00:14:53.560]   I think that the interesting thing that Twitter's developing kind of some sharp elbows, which,
[00:14:53.560 --> 00:15:00.240]   which, you know, it must is renowned for in his other companies too.
[00:15:00.240 --> 00:15:02.360]   You know, he, he has some sharp elbows.
[00:15:02.360 --> 00:15:08.200]   Like if people criticize and he feels like it's unfair, like he, he, like, you know,
[00:15:08.200 --> 00:15:09.880]   hits back twice as hard.
[00:15:09.880 --> 00:15:13.640]   And I think that that that served some of the businesses that he's in well because people
[00:15:13.640 --> 00:15:19.800]   have done a lot of misrepresenting of SpaceX and Tesla and the way that he has essentially
[00:15:19.800 --> 00:15:21.320]   been their PR person.
[00:15:21.320 --> 00:15:25.840]   You know, I'd say it's been one of his main jobs has actually worked well because it,
[00:15:25.840 --> 00:15:30.280]   it's allowed not other companies to pile a lot of fun on those companies that are accomplishing
[00:15:30.280 --> 00:15:33.200]   a lot of things but are disrupting a lot of industries.
[00:15:33.200 --> 00:15:37.080]   And so, you know how you all in the working in the tech industry know how this work.
[00:15:37.080 --> 00:15:40.720]   If you're a big company and you're about to be disrupted, you just make up a lot of,
[00:15:40.720 --> 00:15:45.960]   you know, really sort of kind of shady things about your competitors and try to pile it
[00:15:45.960 --> 00:15:50.600]   on and, and you be the one to frame who they are.
[00:15:50.600 --> 00:15:52.680]   And I think that worked pretty well for Musk.
[00:15:52.680 --> 00:15:57.320]   He didn't allow that to happen successfully on SpaceX and Tesla in many ways.
[00:15:57.320 --> 00:16:02.040]   It's not working out as well on Twitter because Twitter, if you're trying to build and they
[00:16:02.040 --> 00:16:06.000]   say they are, we don't know a whole lot about what they're trying to build other than maybe
[00:16:06.000 --> 00:16:12.240]   this like super app or perhaps a digital town square in the digital town square, which
[00:16:12.240 --> 00:16:16.200]   is what we can see the most now and where the sub stack example comes in.
[00:16:16.200 --> 00:16:18.120]   Sub stack is a great platform.
[00:16:18.120 --> 00:16:22.040]   Having a sharp elbow and trying to elbow them out doesn't help Twitter.
[00:16:22.040 --> 00:16:26.320]   Ultimately, if you're trying to create a digital town square, you want to create a platform
[00:16:26.320 --> 00:16:31.640]   where as many people as many platforms as many others can come and build on top of you
[00:16:31.640 --> 00:16:37.440]   and be invested in you and then have a place on that and how to have some ownership of
[00:16:37.440 --> 00:16:38.440]   it.
[00:16:38.440 --> 00:16:42.800]   It was the early strategy started interrupt, but there was a really strategy to your point,
[00:16:42.800 --> 00:16:47.960]   Jason, was was to be and I mean long before Twitter had an official app, they were just
[00:16:47.960 --> 00:16:48.960]   an API.
[00:16:48.960 --> 00:16:53.520]   So you could, in fact, the first interface was 404 a four send a text message and you
[00:16:53.520 --> 00:16:54.520]   would get that's right.
[00:16:54.520 --> 00:17:00.760]   Then they built a web interface and then they build an API and you could build these apps
[00:17:00.760 --> 00:17:05.000]   on top of it started interrupt you, Jason, but I think your point was perfect.
[00:17:05.000 --> 00:17:06.280]   No, thanks, Dan.
[00:17:06.280 --> 00:17:12.520]   I think that when I read and it could be me reading what I wanted to hear when when musk's
[00:17:12.520 --> 00:17:16.960]   early rhetoric around Twitter was a little bit of like, I'm going to buy it and take
[00:17:16.960 --> 00:17:21.160]   it private and we're going to go back to a little bit of that reality that you just
[00:17:21.160 --> 00:17:22.640]   talked about, Dan.
[00:17:22.640 --> 00:17:26.880]   And so because I think the thing was if we go private, we don't have to respond every
[00:17:26.880 --> 00:17:32.480]   quarter to shareholders and so we can build something for the long term and that whole
[00:17:32.480 --> 00:17:36.880]   digital town square narrative for me, that's what that meant.
[00:17:36.880 --> 00:17:41.920]   Literally, that's not quite what musk meant or maybe we're just not there yet, but I think
[00:17:41.920 --> 00:17:45.720]   like this thing with sub stack and some of these other like sharp elbow moments, they're
[00:17:45.720 --> 00:17:49.560]   really not helping Twitter or musk in where they're going.
[00:17:49.560 --> 00:17:53.240]   I don't think picking fights at the New York Times and other places.
[00:17:53.240 --> 00:17:56.960]   And I think those things are working against them in ways that I think it helped is helped
[00:17:56.960 --> 00:18:04.360]   him and helped Twitter, sorry, Tesla and SpaceX, but don't work very well in sort of the environment
[00:18:04.360 --> 00:18:05.360]   that Twitter is in.
[00:18:05.360 --> 00:18:16.640]   Well, often it feels very reactive and unfought through, like this banning of sub stack links
[00:18:16.640 --> 00:18:22.360]   and mentioning of sub stack on Twitter course, the story there, which I'm not sure if it
[00:18:22.360 --> 00:18:27.120]   was discussed on last week's Twitter, but was that there was this rumor that sub stack
[00:18:27.120 --> 00:18:30.080]   was going to release this product called notes.
[00:18:30.080 --> 00:18:35.160]   And then, you know, sure enough soon after that, any mention of sub stack, any links out
[00:18:35.160 --> 00:18:37.880]   to sub stack from Twitter were being blocked.
[00:18:37.880 --> 00:18:40.360]   And now that's all been undone.
[00:18:40.360 --> 00:18:45.600]   But you know, again, this and this just being one example, but there's a reactivity that
[00:18:45.600 --> 00:18:53.080]   seems that seems to be a part of the perception around the current Twitter and the owner of
[00:18:53.080 --> 00:18:58.600]   Twitter, Elon Musk, and that reactivity can damage a brand.
[00:18:58.600 --> 00:19:03.080]   But then we've also been saying for, you know, ever since Musk took ownership of Twitter
[00:19:03.080 --> 00:19:05.240]   that he is damaging the service.
[00:19:05.240 --> 00:19:06.640]   The service still works.
[00:19:06.640 --> 00:19:08.240]   It's still chugging along.
[00:19:08.240 --> 00:19:09.680]   So you know what I mean?
[00:19:09.680 --> 00:19:14.480]   It's maybe it's a little bit different now than it was before in a number of ways, but
[00:19:14.480 --> 00:19:15.480]   it's still here.
[00:19:15.480 --> 00:19:17.280]   There's people obviously still using it.
[00:19:17.280 --> 00:19:18.600]   I'm the same as you, Dan.
[00:19:18.600 --> 00:19:20.600]   I have my account.
[00:19:20.600 --> 00:19:26.440]   I log in to kind of monitor it from time to time, but my once daily habit of checking
[00:19:26.440 --> 00:19:31.280]   it and actually posting to it, especially as like the extension, like I hardly do that
[00:19:31.280 --> 00:19:32.280]   anymore.
[00:19:32.280 --> 00:19:33.640]   I really don't post to any anymore.
[00:19:33.640 --> 00:19:36.560]   And I'm definitely not checking it on a daily basis.
[00:19:36.560 --> 00:19:42.160]   Does that make something like a sub stack notes, you know, ripe for the picking?
[00:19:42.160 --> 00:19:45.200]   I mean, I'm not a author on sub stack.
[00:19:45.200 --> 00:19:47.440]   I'm also not a regular user of sub stacks.
[00:19:47.440 --> 00:19:49.600]   So maybe I'm just the wrong audience for it.
[00:19:49.600 --> 00:19:54.360]   But I would agree with what you said, and I don't know that it necessarily has any sort
[00:19:54.360 --> 00:19:59.080]   of threat attached to it of being the next Twitter if there could be something.
[00:19:59.080 --> 00:20:01.600]   Well, and then that's a diminish it.
[00:20:01.600 --> 00:20:06.880]   It's just not necessarily for me because I'm not a newsletter.
[00:20:06.880 --> 00:20:07.880]   Yeah.
[00:20:07.880 --> 00:20:11.560]   And the real glue of sub stack is the this ancient technology we called email.
[00:20:11.560 --> 00:20:19.440]   I mean, it is really a platform that is designed for more passive consumption, less engagement.
[00:20:19.440 --> 00:20:26.480]   I know that notes upset engagement for particular writers, but it's really designed to be an
[00:20:26.480 --> 00:20:31.080]   email platform and something that is slow consumption as opposed to Twitter, which is
[00:20:31.080 --> 00:20:33.080]   rapid.
[00:20:33.080 --> 00:20:38.640]   One point I do want to make about Musk and his controversial decisions.
[00:20:38.640 --> 00:20:44.040]   Jason, I agree with the earned media strategy of be controversial and as you said, throw
[00:20:44.040 --> 00:20:49.560]   some sharp elbows, but he has also elevated a lot of hate speech and a lot of white supremacists
[00:20:49.560 --> 00:20:51.400]   are back on that platform.
[00:20:51.400 --> 00:20:55.960]   And there's some challenging stuff that's happening there that gives me a feeling in
[00:20:55.960 --> 00:20:58.760]   my stomach that like, this is not okay.
[00:20:58.760 --> 00:21:04.800]   This stuff that like strategy aside, you know, we could we can look at that and kind of pick
[00:21:04.800 --> 00:21:14.400]   it apart in kind of a fun way, but the way that really terrible stuff has been amplified
[00:21:14.400 --> 00:21:16.880]   is distasteful.
[00:21:16.880 --> 00:21:18.680]   I think that's fair.
[00:21:18.680 --> 00:21:19.680]   Yeah.
[00:21:19.680 --> 00:21:25.000]   Do you think the whole issue with hate speech and all of the horrible content bubbling to
[00:21:25.000 --> 00:21:32.960]   the top is because of the lack of resources that he fired once he got in there and things
[00:21:32.960 --> 00:21:37.560]   are slipping through the cracks a lot easier because the team that's right there now,
[00:21:37.560 --> 00:21:42.720]   they're busy with the whole bunch of things beyond content moderation.
[00:21:42.720 --> 00:21:43.720]   Yeah.
[00:21:43.720 --> 00:21:46.320]   I mean, putting context that's 7,000 employees.
[00:21:46.320 --> 00:21:50.160]   Now they're down to 1,300, 500 of those are engineers.
[00:21:50.160 --> 00:21:53.200]   It is a skeleton crew over there.
[00:21:53.200 --> 00:21:57.680]   And we had some problems with the ZeeDeeNet account and tried to get ahold of people there.
[00:21:57.680 --> 00:21:58.680]   And it took a month.
[00:21:58.680 --> 00:22:00.160]   It took over a month actually.
[00:22:00.160 --> 00:22:01.160]   Wow.
[00:22:01.160 --> 00:22:08.800]   And to get in contact with somebody who could help, you know, everybody had left that there
[00:22:08.800 --> 00:22:10.040]   were there were contacts.
[00:22:10.040 --> 00:22:11.040]   Wow.
[00:22:11.040 --> 00:22:14.200]   And even prior to what you're talking about, I mean, I remember here at Twitter, we've
[00:22:14.200 --> 00:22:19.160]   had a, you know, our run-ins here and there was something that we felt we needed to escalate
[00:22:19.160 --> 00:22:21.640]   to the powers that be at Twitter.
[00:22:21.640 --> 00:22:27.960]   And even prior to the muscification of the service and when they had all of that stuff,
[00:22:27.960 --> 00:22:32.160]   it still took a while to get there, not nearly a month, but it still took a while.
[00:22:32.160 --> 00:22:36.280]   I mean, any of this stuff is incredibly complicated.
[00:22:36.280 --> 00:22:41.360]   And I think this actually ties in with a link that you added, and that does have to do with
[00:22:41.360 --> 00:22:42.560]   substack.
[00:22:42.560 --> 00:22:45.560]   And this is on the side of content moderation, right?
[00:22:45.560 --> 00:22:50.680]   And we're talking a little bit about, you know, how Twitter has approached content moderation.
[00:22:50.680 --> 00:22:52.480]   They're down on staff.
[00:22:52.480 --> 00:22:54.480]   So what are the impacts of that?
[00:22:54.480 --> 00:23:01.600]   Well, the other side of this is that the substack CEO, Chris Best, you know, it's hard to understand
[00:23:01.600 --> 00:23:06.040]   what the values of substack are when it comes to moderation.
[00:23:06.040 --> 00:23:13.440]   And the point behind this is that he had an interview with Neil I Patel from The Verge.
[00:23:13.440 --> 00:23:20.080]   And Neil I really tried to nail him down on the terms of service with substack primarily
[00:23:20.080 --> 00:23:27.840]   because substack, the service itself, taking notes to the side, my understanding is the
[00:23:27.840 --> 00:23:31.560]   CEO says, you know, substack has been a delivery mechanism.
[00:23:31.560 --> 00:23:33.200]   It's a delivery network.
[00:23:33.200 --> 00:23:35.280]   It's not, you know, it's not like a Twitter.
[00:23:35.280 --> 00:23:38.840]   It's not like a social network that needs content moderation.
[00:23:38.840 --> 00:23:42.640]   We give you the platform and it's a little bit more agnostic from there.
[00:23:42.640 --> 00:23:45.800]   Well now, Neil I is saying, okay, well, now you're getting into this other thing, this
[00:23:45.800 --> 00:23:51.080]   notes thing, which is very similar to a Twitter, it's very similar to a social network.
[00:23:51.080 --> 00:23:55.680]   How do you approach content moderation now versus what you were doing then?
[00:23:55.680 --> 00:24:01.080]   And Neil I pointed to a very specific part of the terms of service to say, you know,
[00:24:01.080 --> 00:24:06.760]   to use the example, I can't remember the example that he used to frame around this.
[00:24:06.760 --> 00:24:10.480]   But essentially, here's a really bad thing that's said on your network.
[00:24:10.480 --> 00:24:16.880]   A post that says that there needs to be less brown people in America or that brown.
[00:24:16.880 --> 00:24:20.080]   Yes, something along those lines, exactly.
[00:24:20.080 --> 00:24:26.360]   And Neil, I wanted him to say, yes, that is something that we would remove from the network.
[00:24:26.360 --> 00:24:33.720]   And CEO, I mean, it's really kind of impressive to kind of like listen and read through the
[00:24:33.720 --> 00:24:36.720]   transcript that's here on TechDirt about it.
[00:24:36.720 --> 00:24:41.920]   And no matter what Neil I did, the CEO would not confirm that, even though like any CEO
[00:24:41.920 --> 00:24:47.040]   of any other social network would very quickly probably step forward unless, you know, there
[00:24:47.040 --> 00:24:52.080]   are certain social networks that maybe they wouldn't step forward and say, heck no, not
[00:24:52.080 --> 00:24:53.360]   on my network.
[00:24:53.360 --> 00:24:58.880]   But I'd say that most of them and Twitter included probably would and you're not getting
[00:24:58.880 --> 00:25:00.240]   so much from sub stack.
[00:25:00.240 --> 00:25:04.760]   So I don't know what we, you were the one that made sure that this, you know, made it
[00:25:04.760 --> 00:25:07.200]   into the rundown today.
[00:25:07.200 --> 00:25:08.520]   What are your thoughts on this?
[00:25:08.520 --> 00:25:13.960]   Like, well, I wanted to bring it to the attention of our other hosts because when I saw this,
[00:25:13.960 --> 00:25:18.120]   this message shared, I knew it was going to have a whole lot of people up in arms and
[00:25:18.120 --> 00:25:19.120]   enraged.
[00:25:19.120 --> 00:25:20.120]   I get it.
[00:25:20.120 --> 00:25:24.120]   But instead of just watching the TikTok that was shared, I went and listened to the actual
[00:25:24.120 --> 00:25:28.880]   episode because I found this is the jaded side coming out again.
[00:25:28.880 --> 00:25:35.520]   A lot of things that are published in social media that are set to just induce outrage
[00:25:35.520 --> 00:25:38.400]   tend to be chopped up or a certain way to do that.
[00:25:38.400 --> 00:25:41.400]   And the contents get all thrown over the place, so for sure.
[00:25:41.400 --> 00:25:47.800]   I went back and listened to the whole episode and prior to that segment, Niloi Patel says,
[00:25:47.800 --> 00:25:53.360]   look, I wanted him to answer this question, but I went about it the wrong way.
[00:25:53.360 --> 00:25:58.400]   I should have framed it a little bit differently because the example I gave wasn't a fair example
[00:25:58.400 --> 00:26:00.720]   to him, something like that.
[00:26:00.720 --> 00:26:03.640]   But the problem was he still didn't answer the question.
[00:26:03.640 --> 00:26:09.360]   That's the name because Niloi brought up an example in the terms of service and everything.
[00:26:09.360 --> 00:26:16.640]   And after listening to it, I sort of see where the CEO was coming from because sometimes
[00:26:16.640 --> 00:26:20.280]   they don't want to answer things because they should just keep their mouth shut because
[00:26:20.280 --> 00:26:22.680]   things get worse the more they speak.
[00:26:22.680 --> 00:26:25.800]   And I think that was part of it.
[00:26:25.800 --> 00:26:31.600]   He had PR there whispering in his ear, but he didn't, so he got quiet.
[00:26:31.600 --> 00:26:33.120]   I get that.
[00:26:33.120 --> 00:26:41.360]   But at the same time, black and white hate speech and racism and stuff, yeah, I would
[00:26:41.360 --> 00:26:44.680]   love to have answered, no, that's not going to be allowed.
[00:26:44.680 --> 00:26:49.240]   But again, I'm not the CEO of some tech company out there.
[00:26:49.240 --> 00:26:56.160]   But I wanted to bring it to Mr. Patterson and Mr. Heiner as people in journalism.
[00:26:56.160 --> 00:26:58.760]   What are your thoughts on how Niloi went about it?
[00:26:58.760 --> 00:27:04.640]   Niloi Patel went about it as well as your thoughts on the response from the CEO because
[00:27:04.640 --> 00:27:08.040]   you folks deal with people in those higher positions.
[00:27:08.040 --> 00:27:12.800]   And a lot of times it's tough to get answers out of them to simple yes, no questions.
[00:27:12.800 --> 00:27:13.800]   Yeah, right.
[00:27:13.800 --> 00:27:14.800]   Go ahead, Dave.
[00:27:14.800 --> 00:27:15.800]   I'm waiting for Jason.
[00:27:15.800 --> 00:27:27.680]   But I heard that same answer from every spokesperson at every major tech company for a very long
[00:27:27.680 --> 00:27:28.680]   time.
[00:27:28.680 --> 00:27:35.720]   I can't speak to how Patel conducted the interview because I haven't watched the interview.
[00:27:35.720 --> 00:27:42.520]   But I do think that you need to have an answer for that question because every journalist
[00:27:42.520 --> 00:27:48.000]   is going to ask you about content moderation and especially algorithmic content moderation.
[00:27:48.000 --> 00:27:50.600]   And we certainly see that there are harms.
[00:27:50.600 --> 00:27:58.440]   Maria Ressa, the Nobel Prize winner who is attacked and jailed because of attacks from
[00:27:58.440 --> 00:28:05.000]   the president Duterte on her as a journalist can tell you that there are real harms that
[00:28:05.000 --> 00:28:09.040]   come from algorithmic amplification of hate speech.
[00:28:09.040 --> 00:28:14.120]   Now, we're not saying that necessarily that's what Substack is doing.
[00:28:14.120 --> 00:28:18.480]   But if I were advising him, I would say that you need to have a good answer to this question
[00:28:18.480 --> 00:28:23.160]   because every spokesperson, I mean, let's go back to Cambridge Analytica and Facebook.
[00:28:23.160 --> 00:28:25.720]   Let's extend this to Twitter and other social networks.
[00:28:25.720 --> 00:28:27.560]   So it's not a new problem.
[00:28:27.560 --> 00:28:31.800]   And so if you're going to launch a social network, you need to have an answer to this.
[00:28:31.800 --> 00:28:38.960]   In Tumblr, going back years ago, Tumblr allowed, had a lax policies on what they, on porn
[00:28:38.960 --> 00:28:40.160]   and nudity.
[00:28:40.160 --> 00:28:45.720]   And they changed those not out of pressure from users, but out of pressure from Apple
[00:28:45.720 --> 00:28:48.200]   and the iOS rules.
[00:28:48.200 --> 00:28:54.680]   So you at least need to have an understanding of what your platform allows and what your
[00:28:54.680 --> 00:29:01.720]   answer is going to be to those questions because simply not answering it is, I mean,
[00:29:01.720 --> 00:29:02.720]   yes, problematic.
[00:29:02.720 --> 00:29:03.720]   Yes.
[00:29:03.720 --> 00:29:06.080]   And you're exactly right.
[00:29:06.080 --> 00:29:09.960]   And I think that is a huge part of Neili's point.
[00:29:09.960 --> 00:29:17.160]   And he even says it in the interview is like, you know, these kinds of things, they broadcast
[00:29:17.160 --> 00:29:19.880]   the values of the network that you're using.
[00:29:19.880 --> 00:29:24.040]   And as a user, you want to know that those values align with your values.
[00:29:24.040 --> 00:29:29.480]   And if your terms of service say, this is not OK, then why can't the CEO say this is not
[00:29:29.480 --> 00:29:30.480]   OK?
[00:29:30.480 --> 00:29:33.960]   And flat out that CEO was refusing to say this is not OK.
[00:29:33.960 --> 00:29:37.640]   Meanwhile, you know, I look up the quote, I don't want to say the quote because it's
[00:29:37.640 --> 00:29:41.240]   actually worse than what you, what you even said there, Ant.
[00:29:41.240 --> 00:29:47.280]   But you know, you read a quote like that and you go, yeah, that should not be on a network.
[00:29:47.280 --> 00:29:52.880]   At least it does, at least I as a user have the value system that says that I don't want
[00:29:52.880 --> 00:29:55.080]   to use your network if that is allowed.
[00:29:55.080 --> 00:29:57.000]   And I think that's the point.
[00:29:57.000 --> 00:29:58.560]   There's also a challenge.
[00:29:58.560 --> 00:30:00.600]   I look, I understand it.
[00:30:00.600 --> 00:30:05.320]   Look, it's not new to say that you're just a platform and a content delivery network.
[00:30:05.320 --> 00:30:10.720]   I mean, that's also a pretty poor answer because every social network has said the same thing
[00:30:10.720 --> 00:30:13.560]   going back a very long time.
[00:30:13.560 --> 00:30:16.560]   But these are these are not email protocols, right?
[00:30:16.560 --> 00:30:19.760]   These are not open protocols that are used for delivery.
[00:30:19.760 --> 00:30:23.160]   These are for profit venture backed companies.
[00:30:23.160 --> 00:30:29.000]   And so if you're going to put money in your pocket and you amplify hate speech, then I
[00:30:29.000 --> 00:30:34.280]   would ask that perhaps you have a better answer or I'm going to publish your non-answer.
[00:30:34.280 --> 00:30:35.800]   It's not, and it's not picking on you.
[00:30:35.800 --> 00:30:36.800]   It's not being a jerk.
[00:30:36.800 --> 00:30:40.320]   It's just, and it's not just fundamentals of journalism.
[00:30:40.320 --> 00:30:41.840]   It's what a lot of people want to know.
[00:30:41.840 --> 00:30:43.840]   A lot of your users want to know.
[00:30:43.840 --> 00:30:48.760]   If you are making money on something, why are you also amplifying hate speech?
[00:30:48.760 --> 00:30:52.040]   And I think Nilo is maybe even a little hard on himself.
[00:30:52.040 --> 00:30:56.480]   I think Nilo did an excellent job in this.
[00:30:56.480 --> 00:31:02.640]   He picked this quote to ask this very difficult and challenging question because it's the
[00:31:02.640 --> 00:31:08.720]   fundamental social issue in America, racism, and has been since before America was founded.
[00:31:08.720 --> 00:31:12.120]   And we've struggled with it over the last three years.
[00:31:12.120 --> 00:31:21.640]   This risen to the top of the attention of people and for all of those reasons in America,
[00:31:21.640 --> 00:31:25.040]   and this is an American company, to ask that question is very fair.
[00:31:25.040 --> 00:31:26.640]   And not only that, he pushed him on it.
[00:31:26.640 --> 00:31:29.440]   He didn't let him squirrel out of it once.
[00:31:29.440 --> 00:31:30.680]   He kept pushing him on it.
[00:31:30.680 --> 00:31:31.680]   And it was fair.
[00:31:31.680 --> 00:31:36.080]   He gave him every opportunity to say, this is the kind of thing.
[00:31:36.080 --> 00:31:43.360]   Even just to say, we find it offensive, we don't want things like this here and maybe
[00:31:43.360 --> 00:31:48.480]   explain their platform in that light.
[00:31:48.480 --> 00:31:53.600]   I have heard to Dan's point and others, I've heard, I remember at the founding of Twitter,
[00:31:53.600 --> 00:31:58.160]   early days of Twitter, I remember the founders there saying, we don't want to censor anything.
[00:31:58.160 --> 00:31:59.160]   They started from there.
[00:31:59.160 --> 00:32:01.000]   We just want to be the platform.
[00:32:01.000 --> 00:32:02.520]   That didn't last very long.
[00:32:02.520 --> 00:32:05.000]   It was not a tenable way to be.
[00:32:05.000 --> 00:32:11.480]   It was like Dan said, this is not just a matter of zeros and ones that your platform is on
[00:32:11.480 --> 00:32:12.480]   top of.
[00:32:12.480 --> 00:32:14.280]   It is a content platform.
[00:32:14.280 --> 00:32:19.040]   When you are a content platform, you are responsible for what goes on in that platform.
[00:32:19.040 --> 00:32:25.120]   It doesn't mean it's easy in trying to moderate anything is extremely challenging and complex.
[00:32:25.120 --> 00:32:28.600]   But that's what you sign up for when you create a platform.
[00:32:28.600 --> 00:32:36.160]   I think they, them trying to score out of this one, doesn't generate a whole lot of confidence
[00:32:36.160 --> 00:32:39.520]   that they understand that fundamental fact.
[00:32:39.520 --> 00:32:47.520]   I appreciate y'all bringing up his approach, Neil Iapatel's approach to it and just saying,
[00:32:47.520 --> 00:32:49.560]   he actually did a good job.
[00:32:49.560 --> 00:32:55.960]   But again, as I continue to listen to the whole segment, what it seemed like was the
[00:32:55.960 --> 00:33:04.400]   CEO was struggling with the differentiation between content moderation and everyone's
[00:33:04.400 --> 00:33:07.280]   right to free speech the first minute.
[00:33:07.280 --> 00:33:12.800]   And he actually goes in a little bit more detail about that.
[00:33:12.800 --> 00:33:17.200]   It's sort of sounded like some of the stuff I've heard from the leadership at Facebook
[00:33:17.200 --> 00:33:23.960]   as well as on Twitter saying, you know what, we want our users to be able to speak freely.
[00:33:23.960 --> 00:33:31.920]   But they also hate trying to figure out where that fine line is between overt racism and
[00:33:31.920 --> 00:33:38.920]   hate or things that could come off as hate or ignorance or racism or what have you.
[00:33:38.920 --> 00:33:41.920]   Well, there are a lot of experts started interrupting.
[00:33:41.920 --> 00:33:45.880]   I mean, Elon fired a great expert in this.
[00:33:45.880 --> 00:33:48.240]   He could work with the Atlantic Council.
[00:33:48.240 --> 00:33:50.400]   I mean, there's so many experts on it.
[00:33:50.400 --> 00:33:56.720]   Yes, and you're right, this is a nuanced challenge, especially when we talk about algorithms,
[00:33:56.720 --> 00:33:57.720]   right?
[00:33:57.720 --> 00:34:00.520]   Because algorithms will search surface different things for certain people.
[00:34:00.520 --> 00:34:03.480]   But this is not a problem that doesn't have a solution.
[00:34:03.480 --> 00:34:08.960]   There are experts who can help guide them towards a good solution here.
[00:34:08.960 --> 00:34:13.600]   And again, to not have an answer to that just it kind of rocks confidence.
[00:34:13.600 --> 00:34:18.800]   I wish he had an answer because when I was listening to it, I saw where he was coming
[00:34:18.800 --> 00:34:19.800]   from.
[00:34:19.800 --> 00:34:23.120]   And let me just have y'all step into my shoes for a minute.
[00:34:23.120 --> 00:34:25.400]   I can go outside here in my neighborhood.
[00:34:25.400 --> 00:34:29.720]   You know, I'm the only black family in this neighborhood totally fine with that.
[00:34:29.720 --> 00:34:34.000]   And something can be said to me totally out of the way.
[00:34:34.000 --> 00:34:37.920]   And I can look at it as a couple of ways.
[00:34:37.920 --> 00:34:41.200]   Their racist or their jackasses.
[00:34:41.200 --> 00:34:48.240]   And most of the time, it's option two, you know, but depending on context and depending
[00:34:48.240 --> 00:34:52.880]   on, you know, who generally depending on the context is going to define where there's
[00:34:52.880 --> 00:34:57.120]   something was flat out hate and racism versus someone's just a jerk.
[00:34:57.120 --> 00:35:02.560]   And I think that's what some people in leadership are struggling with when it comes to these
[00:35:02.560 --> 00:35:08.960]   platforms because sometimes people are just jerks online, not necessarily racist.
[00:35:08.960 --> 00:35:12.840]   And free speech, the thing that they didn't, they didn't, it really does come down to free
[00:35:12.840 --> 00:35:16.280]   speech a little bit because that's essentially what the CEO was bringing it back to.
[00:35:16.280 --> 00:35:21.080]   But free speech has its guardrails and it has its limits.
[00:35:21.080 --> 00:35:25.880]   And so, you know, the interview could have, since he kept going back to that, the interview
[00:35:25.880 --> 00:35:31.640]   could have unpacked that a little bit, you know, more as well and say, you know, free
[00:35:31.640 --> 00:35:34.680]   speech absolutists only exist in sound bites.
[00:35:34.680 --> 00:35:36.640]   They don't exist in reality, right?
[00:35:36.640 --> 00:35:39.040]   That is a great sound bite Jason.
[00:35:39.040 --> 00:35:40.040]   That's true.
[00:35:40.040 --> 00:35:45.800]   We'll clip it out and put it, put it as a sound bite in our network.
[00:35:45.800 --> 00:35:46.800]   Sure.
[00:35:46.800 --> 00:35:50.480]   Well, I mean, it should go without saying, but we should also say it to say it all the
[00:35:50.480 --> 00:35:51.480]   time.
[00:35:51.480 --> 00:35:54.280]   Tech companies love to wrap themselves around free speech.
[00:35:54.280 --> 00:35:56.000]   There's nothing to do with free speech.
[00:35:56.000 --> 00:35:58.080]   This is nothing to do with free speech.
[00:35:58.080 --> 00:35:59.840]   Free speech is right.
[00:35:59.840 --> 00:36:01.360]   And you said it.
[00:36:01.360 --> 00:36:05.840]   The government, the First Amendment, the government is not coming in and dictating policy
[00:36:05.840 --> 00:36:06.840]   here.
[00:36:06.840 --> 00:36:11.720]   So, wrapping yourself in free speech and saying free speech over and over is in some ways.
[00:36:11.720 --> 00:36:14.040]   I'm not saying that the CEO did it right.
[00:36:14.040 --> 00:36:17.000]   It is a cop out, but it's also a diversion.
[00:36:17.000 --> 00:36:22.200]   And I know that other tech companies have used it as a specific PR strategy to kind
[00:36:22.200 --> 00:36:27.040]   of get you off of the talking point that you're on or get you off of your line of questioning
[00:36:27.040 --> 00:36:32.440]   and start talking about something that everybody goes, oh, I don't, I'm, I'm for free speech.
[00:36:32.440 --> 00:36:34.640]   You know, I'm just asking you a hard question.
[00:36:34.640 --> 00:36:35.640]   Right.
[00:36:35.640 --> 00:36:36.640]   It's like, in this case.
[00:36:36.640 --> 00:36:37.640]   In the children.
[00:36:37.640 --> 00:36:38.640]   Right.
[00:36:38.640 --> 00:36:39.640]   Exactly.
[00:36:39.640 --> 00:36:40.640]   As a tool.
[00:36:40.640 --> 00:36:41.640]   Yes.
[00:36:41.640 --> 00:36:42.640]   Right.
[00:36:42.640 --> 00:36:46.640]   So, it feels like a species argument at this point, this juncture in history, like, come
[00:36:46.640 --> 00:36:48.520]   on, man, let's look at the last decade.
[00:36:48.520 --> 00:36:52.640]   There have been CEOs ahead of you who have said the same things, repeated the same talking
[00:36:52.640 --> 00:36:53.640]   points.
[00:36:53.640 --> 00:36:54.880]   This is not a free speech issue.
[00:36:54.880 --> 00:36:56.440]   This is a for profit issue.
[00:36:56.440 --> 00:36:58.760]   The bigger you scale, the more money you make.
[00:36:58.760 --> 00:37:02.600]   And so if you're going to scale, then you have to have a better answer about how you
[00:37:02.600 --> 00:37:03.600]   deal with speech.
[00:37:03.600 --> 00:37:04.600]   Yes.
[00:37:04.600 --> 00:37:05.600]   100%.
[00:37:05.600 --> 00:37:06.600]   Excellent summary.
[00:37:06.600 --> 00:37:07.600]   Totally agree.
[00:37:07.600 --> 00:37:13.640]   And it sounds a fantastic first story to dive into.
[00:37:13.640 --> 00:37:15.200]   We are going to take a break.
[00:37:15.200 --> 00:37:21.480]   And then we're going to have many more fantastic stories to dive into when we're done.
[00:37:21.480 --> 00:37:23.680]   So everybody hang tight for the moment.
[00:37:23.680 --> 00:37:28.880]   Let's take a moment and thank the sponsor of this episode of this week in tech.
[00:37:28.880 --> 00:37:32.360]   And that is collide.
[00:37:32.360 --> 00:37:34.760]   So collide has some big news.
[00:37:34.760 --> 00:37:38.480]   So first of all, collide, if you don't know, it's a device trust solution.
[00:37:38.480 --> 00:37:41.240]   It ensures unsecured devices can't access your app.
[00:37:41.240 --> 00:37:42.240]   So there's that.
[00:37:42.240 --> 00:37:43.800]   Well, they got some big news.
[00:37:43.800 --> 00:37:48.680]   If you're an octa user, collide can get your entire fleet to 100%.
[00:37:48.680 --> 00:37:50.440]   Compliance can do that.
[00:37:50.440 --> 00:37:54.000]   Collide patches, one of the major holes in zero trust architecture, and that's device
[00:37:54.000 --> 00:37:55.520]   compliance.
[00:37:55.520 --> 00:38:03.080]   So when you really think about it, your identity provider only lets known devices log into
[00:38:03.080 --> 00:38:04.080]   apps, right?
[00:38:04.080 --> 00:38:05.080]   They're known.
[00:38:05.080 --> 00:38:06.080]   Sure, they can log in.
[00:38:06.080 --> 00:38:11.280]   But just because a device is known doesn't always mean that it's in a secure state.
[00:38:11.280 --> 00:38:12.280]   Is it secured?
[00:38:12.280 --> 00:38:13.280]   You don't actually know that.
[00:38:13.280 --> 00:38:15.400]   Just because it's known, it doesn't tell you that it is.
[00:38:15.400 --> 00:38:19.640]   In fact, plenty of the devices in your fleet probably shouldn't be trusted at all.
[00:38:19.640 --> 00:38:23.200]   Maybe they're running an out of date OS version.
[00:38:23.200 --> 00:38:27.280]   Maybe they've got unencrypted credentials just lying around.
[00:38:27.280 --> 00:38:32.800]   If a device isn't compliant, or if it isn't running the collide agent, it can't access
[00:38:32.800 --> 00:38:37.240]   the organization's SaaS apps or other resources, plain and simple.
[00:38:37.240 --> 00:38:41.840]   Device user can't log into your company's cloud apps until they've actually fixed the
[00:38:41.840 --> 00:38:44.360]   problem on their end.
[00:38:44.360 --> 00:38:50.720]   So a device might be blocked if an employee doesn't have an up-to-date browser, for example.
[00:38:50.720 --> 00:38:55.600]   Using end user remediation actually helps drive your fleet to 100% compliance without
[00:38:55.600 --> 00:38:58.200]   overwhelming your IT team.
[00:38:58.200 --> 00:39:00.960]   So it keeps things a little bit simpler on your end as well.
[00:39:00.960 --> 00:39:06.200]   Now collide, IT teams have no way to solve these compliance issues or to stop insecure
[00:39:06.200 --> 00:39:08.280]   devices from logging in.
[00:39:08.280 --> 00:39:13.400]   With collide, you can set and enforce compliance across your entire fleet.
[00:39:13.400 --> 00:39:18.640]   And it doesn't matter which OS, Mac, Windows, Linux doesn't matter.
[00:39:18.640 --> 00:39:23.880]   Collide is unique in that it makes device compliance part of the authentication process.
[00:39:23.880 --> 00:39:27.840]   So that's when the user logs in with Octa.
[00:39:27.840 --> 00:39:32.320]   It actually alerts them to compliance issues and prevents unsecured devices from logging
[00:39:32.320 --> 00:39:34.000]   in full stop.
[00:39:34.000 --> 00:39:38.780]   It's security you can feel good about because collide puts transparency and respect for
[00:39:38.780 --> 00:39:42.920]   users at the center of their product.
[00:39:42.920 --> 00:39:48.440]   To sum it all up, Collide's method means fewer support tickets, yay, less frustration,
[00:39:48.440 --> 00:39:52.120]   and most importantly, 100% fleet compliance.
[00:39:52.120 --> 00:39:54.400]   That's going to keep everyone safer.
[00:39:54.400 --> 00:39:56.920]   And Collide.com/twit.
[00:39:56.920 --> 00:39:57.920]   And you can learn more.
[00:39:57.920 --> 00:40:00.040]   You can actually book a demo there as well.
[00:40:00.040 --> 00:40:02.640]   That's K-O-L-I-D-E.
[00:40:02.640 --> 00:40:05.760]   That's Collide.com/twit.
[00:40:05.760 --> 00:40:13.080]   And we thank Collide for their support of this week in tech.
[00:40:13.080 --> 00:40:18.560]   What about, I'm going to get off Twitter for a Twitter, I know it was sub-stack, but
[00:40:18.560 --> 00:40:20.720]   everything seems to come back to Twitter these days.
[00:40:20.720 --> 00:40:24.280]   So I'm going to venture away from Twitter a little bit, give ourselves a little Twitter
[00:40:24.280 --> 00:40:27.880]   break, and talk a little bit about AI.
[00:40:27.880 --> 00:40:31.360]   Because before we kind of get into some of this AI news, Dan, you're doing a lot of
[00:40:31.360 --> 00:40:35.360]   thinking and writing about this stuff, right?
[00:40:35.360 --> 00:40:38.920]   Yeah, for Jason, in fact.
[00:40:38.920 --> 00:40:45.080]   For the last month and ongoing, I've been interviewing thought leaders in AI.
[00:40:45.080 --> 00:40:50.680]   So most recently, Ben Gertzel, he developed Sophia the Robot, which is this anthropomorphize
[00:40:50.680 --> 00:40:58.640]   AI, and Pascal Kaufman, who's a neuroscientist in Switzerland.
[00:40:58.640 --> 00:41:04.480]   Philip Rosdale, who was the founder of Second Life, really fascinating personalities.
[00:41:04.480 --> 00:41:13.600]   And everything from the path from these large language models and chat GPT, all the way up
[00:41:13.600 --> 00:41:18.600]   to AGI artificial general intelligence, which is kind of a moving goal post.
[00:41:18.600 --> 00:41:24.200]   But it's the in theory AI that is smarter, smarter than humans.
[00:41:24.200 --> 00:41:30.520]   But also, I mean, definitely keeping with a lot of what is on ZDNet, the practical stuff,
[00:41:30.520 --> 00:41:31.520]   right?
[00:41:31.520 --> 00:41:33.880]   The kitchen table, what does this mean for business?
[00:41:33.880 --> 00:41:36.520]   What does this mean for you at home?
[00:41:36.520 --> 00:41:39.400]   And so a lot of these people are talking to me about like, well, there's going to be
[00:41:39.400 --> 00:41:41.320]   some big changes coming to search.
[00:41:41.320 --> 00:41:45.200]   There are going to be some big changes coming to consumers, for sure.
[00:41:45.200 --> 00:41:50.240]   But business is going to be enterprise business, is going to be changed fundamentally by a
[00:41:50.240 --> 00:41:57.560]   lot of these more practical AI models.
[00:41:57.560 --> 00:41:58.560]   Interesting.
[00:41:58.560 --> 00:42:06.280]   I mean, fascinating place where we are at with AI and get a little confusing because
[00:42:06.280 --> 00:42:08.800]   it's going in all sorts of different directions.
[00:42:08.800 --> 00:42:14.920]   I labeled this taming the AI race because this is just kind of a little chance to see
[00:42:14.920 --> 00:42:21.640]   how everybody is approaching AI when it comes to particularly when it comes to government.
[00:42:21.640 --> 00:42:28.800]   Because increasingly it looks like across the world, there's a lot more attention, a lot
[00:42:28.800 --> 00:42:34.920]   more understanding, maybe not of the technology itself, but of this thought that AI could someday
[00:42:34.920 --> 00:42:40.800]   be so good that what the heck does that mean about the people who are in power, the countries
[00:42:40.800 --> 00:42:46.520]   who are in power, how the lives for the people living within those countries, how are they
[00:42:46.520 --> 00:42:47.520]   impacted?
[00:42:47.520 --> 00:42:50.160]   There's just a number of things happening right now.
[00:42:50.160 --> 00:42:55.820]   The US Commerce Department, this was last Tuesday, requested public comment on creating
[00:42:55.820 --> 00:42:58.720]   accountability measures for AI.
[00:42:58.720 --> 00:43:03.360]   So they're seeking people, and we talked about this a little bit and on this week in Google,
[00:43:03.360 --> 00:43:13.680]   but they're seeking the input of people who understand and know the potential path that
[00:43:13.680 --> 00:43:15.960]   AI is on and what it could lead to.
[00:43:15.960 --> 00:43:23.040]   What are the right questions to be asking so that on a governmental level, they can create,
[00:43:23.040 --> 00:43:27.640]   potentially create guardrails around the development of AI.
[00:43:27.640 --> 00:43:29.040]   And this is just one example.
[00:43:29.040 --> 00:43:35.960]   In Europe, they're creating a task force, Europe's national, the European Data Protection
[00:43:35.960 --> 00:43:47.040]   Board, the EDPP, is looking to get a better understanding of creating this group to really
[00:43:47.040 --> 00:43:49.600]   understand the development of AI.
[00:43:49.600 --> 00:43:50.840]   China's doing the same thing.
[00:43:50.840 --> 00:43:54.720]   They're mandating security reviews for AI.
[00:43:54.720 --> 00:43:58.000]   So I don't know if that means that they're one step ahead.
[00:43:58.000 --> 00:44:02.080]   They're out of the analysis phase and they're more in the reactive phase, but everybody
[00:44:02.080 --> 00:44:10.080]   seems to be understanding that this looming development of AI, where we are right now,
[00:44:10.080 --> 00:44:14.720]   and the explosive growth that it's seen, especially in the last six months, is leading to something
[00:44:14.720 --> 00:44:20.040]   and we don't know what it is, but we want to understand what it is and possibly control
[00:44:20.040 --> 00:44:21.720]   where that is headed.
[00:44:21.720 --> 00:44:25.840]   So Dan, let's start with you because you're writing about this stuff a lot.
[00:44:25.840 --> 00:44:34.560]   Where is your mind when it comes to this movement, this kind of like all encompassing worldwide
[00:44:34.560 --> 00:44:41.440]   movement towards respecting the power of AI and wanting to put guardrails there?
[00:44:41.440 --> 00:44:42.440]   What are your thoughts?
[00:44:42.440 --> 00:44:46.840]   Well, I hope we can regulate it better than we did social media and have a little more
[00:44:46.840 --> 00:44:54.080]   nuanced and sophisticated take by our elected representatives than we have had in the past.
[00:44:54.080 --> 00:44:57.400]   But you've got to take.
[00:44:57.400 --> 00:45:04.240]   Yeah, I wanted to ask you, Dan, because government is wanting to try to figure this stuff out.
[00:45:04.240 --> 00:45:08.240]   You've been speaking with experts, actually, as two things.
[00:45:08.240 --> 00:45:13.840]   From a government standpoint, experts that are out there that you've been speaking to,
[00:45:13.840 --> 00:45:19.680]   are they willing to go to government and try to help figure out a way to do some type of
[00:45:19.680 --> 00:45:24.800]   regulation or set up better guardrails, or are they just saying, "No, we don't want
[00:45:24.800 --> 00:45:29.040]   to get the government involved because when the government gets involved, things get weird
[00:45:29.040 --> 00:45:31.440]   and sometimes really, really screwed up."
[00:45:31.440 --> 00:45:36.480]   Secondly, you mentioned from an enterprise perspective with AI, we've been speaking about
[00:45:36.480 --> 00:45:38.960]   AI for, she's three months now.
[00:45:38.960 --> 00:45:41.720]   That's where it seems like three months.
[00:45:41.720 --> 00:45:48.920]   The stories are either really, really big, "Rah, rah, rah, we're AI is awesome," or "Gloom
[00:45:48.920 --> 00:45:55.600]   and Doom" on the other side of things, never really much of a middle ground.
[00:45:55.600 --> 00:45:59.600]   What on the enterprise side of things is making this something that could be really, really
[00:45:59.600 --> 00:46:03.960]   practical because speaking as a creative, I could see where AI can pop in and make things
[00:46:03.960 --> 00:46:05.600]   better for me.
[00:46:05.600 --> 00:46:12.360]   I have used it recently for things to make it better in a workflow standpoint.
[00:46:12.360 --> 00:46:16.400]   Enterprise, I can't really wrap my head around it, probably because I've been away from it
[00:46:16.400 --> 00:46:22.800]   for quite a while now, but what is AI going to do on the enterprise side of things?
[00:46:22.800 --> 00:46:30.160]   Well, you know, and actually, I'm very curious, when it comes to regulation, I don't know.
[00:46:30.160 --> 00:46:34.640]   To your point about being a creative, in fact, I sent Jason Heiner a text message maybe a
[00:46:34.640 --> 00:46:39.920]   week and a half, two weeks ago about how I'm using chat GPT to write stories.
[00:46:39.920 --> 00:46:42.160]   They're not writing stories for me.
[00:46:42.160 --> 00:46:48.480]   Right, or quality if you try to do that, but a lot of my reporting is interviews with primary
[00:46:48.480 --> 00:46:53.280]   subjects and any reporter can tell you, "Okay, you conduct an interview, but then you have
[00:46:53.280 --> 00:46:58.240]   to often transcribe the interview and then you have to pull out quotes and then you have
[00:46:58.240 --> 00:47:02.160]   to pull out the salient points and then you have to create an outline."
[00:47:02.160 --> 00:47:10.760]   That's the first thing I do is create my outline, but that whole process can take several hours.
[00:47:10.760 --> 00:47:14.640]   For some people, I'm a slow writer, so for some people it can take longer than that.
[00:47:14.640 --> 00:47:22.800]   What I have used chat GPT for is, "Hey, please summarize this transcript, pull out the five
[00:47:22.800 --> 00:47:27.600]   most interesting quotes, and of course you go back into check it and you change it's
[00:47:27.600 --> 00:47:32.440]   not always the best, but it does a pretty good job of finding those quotes, and then pull
[00:47:32.440 --> 00:47:35.400]   out the main points of what this person talked about.
[00:47:35.400 --> 00:47:40.400]   I'm telling you, I've used it for every single story I've written for Jason in that same way,
[00:47:40.400 --> 00:47:41.400]   and it's great.
[00:47:41.400 --> 00:47:42.600]   It saves a ton of time.
[00:47:42.600 --> 00:47:49.480]   It compresses the amount of time I would spend doing those kind of routine maintenance tasks
[00:47:49.480 --> 00:47:51.880]   and allows me to think about stories.
[00:47:51.880 --> 00:47:58.160]   I think write better stories because I'm spending more time writing and less time doing all
[00:47:58.160 --> 00:48:01.200]   of the mechanics.
[00:48:01.200 --> 00:48:07.760]   But I'm curious what Jason has to say, Heiner has to say about the enterprise because that's
[00:48:07.760 --> 00:48:13.160]   going to be a whole huge shift.
[00:48:13.160 --> 00:48:14.680]   Yeah.
[00:48:14.680 --> 00:48:18.480]   It really comes down to one thing primarily, which is automation.
[00:48:18.480 --> 00:48:23.520]   They've been working on this for a long time.
[00:48:23.520 --> 00:48:35.400]   Part of I think what the new level of generative AI could do is we've been able to automate
[00:48:35.400 --> 00:48:40.600]   one little thing here, one little thing here, one little thing here.
[00:48:40.600 --> 00:48:46.640]   I think that this is what we're seeing is the ability to create a string of these automations
[00:48:46.640 --> 00:48:56.600]   that then all of a sudden, you've taken a big chunk of the really repetitive, mind-numbing
[00:48:56.600 --> 00:48:58.960]   work out of it.
[00:48:58.960 --> 00:49:00.600]   We really thought this was going to happen.
[00:49:00.600 --> 00:49:06.320]   It really has been happening with manufacturing for a long time.
[00:49:06.320 --> 00:49:14.280]   But now it's happening with a lot of the knowledge work because we're realizing, chat
[00:49:14.280 --> 00:49:19.120]   TPT is helping us to realize to just the point that Dan did that there's a lot of repetitive
[00:49:19.120 --> 00:49:22.080]   things that are somewhat easy to automate.
[00:49:22.080 --> 00:49:28.800]   In Jan's latest interview, Gertzel says, he says, "What we've discovered is you don't
[00:49:28.800 --> 00:49:35.120]   have to be particularly innovative to automate a lot of knowledge work.
[00:49:35.120 --> 00:49:37.960]   Shockingly, I'm summing it up.
[00:49:37.960 --> 00:49:40.160]   That's like my summing it.
[00:49:40.160 --> 00:49:41.160]   Watch the interview.
[00:49:41.160 --> 00:49:42.160]   It's an awesome interview.
[00:49:42.160 --> 00:49:43.720]   It's only 20 minutes.
[00:49:43.720 --> 00:49:46.040]   You will be thankful that you did.
[00:49:46.040 --> 00:49:50.960]   But I think his point is that there's a lot of opportunities for automation that really
[00:49:50.960 --> 00:49:54.800]   were not on our radar and now we're realizing like that.
[00:49:54.800 --> 00:49:57.680]   I would be interested to hear the ones that you're using.
[00:49:57.680 --> 00:50:02.360]   You alluded to the fact that it's enabled you to streamline a couple things with your creative
[00:50:02.360 --> 00:50:03.360]   workflows.
[00:50:03.360 --> 00:50:11.880]   There's some things inside of Photoshop or inside of the venture resolve that utilizes
[00:50:11.880 --> 00:50:16.760]   AI and the GPU on your computer.
[00:50:16.760 --> 00:50:20.400]   Inside a Photoshop, if you're trying to cut someone out of a background, that used to
[00:50:20.400 --> 00:50:24.760]   be a tedious process of going through and making the selection properly, especially
[00:50:24.760 --> 00:50:29.920]   if they had hair unlike myself and pulling them out cleanly to put them on a different
[00:50:29.920 --> 00:50:30.920]   type of background.
[00:50:30.920 --> 00:50:37.400]   Now it is literally one click and it's done in about three seconds of analysis.
[00:50:37.400 --> 00:50:43.120]   Adobe Sensei, it's AI, will fix it and pull it out for you and it's done.
[00:50:43.120 --> 00:50:49.960]   On the venture resolve, doing that inside of video, it's even more task-intensive there.
[00:50:49.960 --> 00:50:54.040]   With AI, it's literally just clicking drag and it does a selection and pulls it right
[00:50:54.040 --> 00:50:55.800]   out within a couple of seconds.
[00:50:55.800 --> 00:50:58.320]   It's really fascinating.
[00:50:58.320 --> 00:51:00.920]   You're talking about automation in enterprise.
[00:51:00.920 --> 00:51:08.080]   Sort of reminds me of the recent episode of This Week in Enterprise Tech here on Twit TV.
[00:51:08.080 --> 00:51:14.880]   There was a security story where some employees were terminated but yet their credentials were
[00:51:14.880 --> 00:51:19.720]   still available to them down the road months later because there was a checklist that got
[00:51:19.720 --> 00:51:21.640]   missed.
[00:51:21.640 --> 00:51:26.120]   Maybe they couldn't log into this box or server or what have you but they could log into this
[00:51:26.120 --> 00:51:28.480]   one and things like that.
[00:51:28.480 --> 00:51:32.680]   I was thinking about because I haven't seen Microsoft Active Directory in probably 10
[00:51:32.680 --> 00:51:38.000]   years now and I remember we had processes and procedures and going through and making
[00:51:38.000 --> 00:51:40.760]   sure everybody got ticked off.
[00:51:40.760 --> 00:51:45.880]   The boxes got ticked off when people were terminated because there was all types of group
[00:51:45.880 --> 00:51:50.760]   policies and security measures where they could access this folder and that folder and so
[00:51:50.760 --> 00:51:51.760]   forth.
[00:51:51.760 --> 00:51:56.240]   I was like, "I thought you could script a lot of that stuff but apparently you can't script
[00:51:56.240 --> 00:52:05.720]   everything but if AI can come in and help do a bunch of scripts into one, maybe that's
[00:52:05.720 --> 00:52:08.840]   a game changer."
[00:52:08.840 --> 00:52:13.320]   My mind is swirling right now as I think about this because I'm thinking about the example
[00:52:13.320 --> 00:52:17.720]   that you just gave and where removing people in the background, which by the way I can
[00:52:17.720 --> 00:52:21.760]   do on my phone.
[00:52:21.760 --> 00:52:24.120]   That's the power of mission is AI.
[00:52:24.120 --> 00:52:25.600]   TensorFlow.
[00:52:25.600 --> 00:52:39.120]   The fear that a lot of people have about AI eliminating jobs or taking that whole aspect
[00:52:39.120 --> 00:52:40.320]   of things.
[00:52:40.320 --> 00:52:46.320]   When I hear what you're talking about, I hear it as a tool that helps someone who knows
[00:52:46.320 --> 00:52:52.040]   what they're doing do better at their job, do more with their jobs in other ways.
[00:52:52.040 --> 00:52:53.600]   But I can see the flip side of this.
[00:52:53.600 --> 00:53:02.880]   We also have a story a little bit further down the rundown line 63 that focuses on illustrators,
[00:53:02.880 --> 00:53:05.840]   video game illustrators in China.
[00:53:05.840 --> 00:53:09.960]   It's an article on Rest of World, which is a fantastic site.
[00:53:09.960 --> 00:53:19.320]   It just focuses on a handful of examples of where one of the examples is a freelance illustrator.
[00:53:19.320 --> 00:53:25.440]   Used to make $430,000-ish for every video game poster she drew.
[00:53:25.440 --> 00:53:33.120]   Since February, those opportunities have dried up because as you can imagine, as we've seen
[00:53:33.120 --> 00:53:39.520]   from generative AI, doing illustrations like that, kind of come a lot easier to a system
[00:53:39.520 --> 00:53:40.520]   like that.
[00:53:40.520 --> 00:53:43.480]   Now, the work that she's with are thumbnails.
[00:53:43.480 --> 00:53:45.160]   We do it for several of our shows.
[00:53:45.160 --> 00:53:50.680]   Have you used some AI generated art for thumbnails and it looked great.
[00:53:50.680 --> 00:53:55.400]   I get where the people are coming from here in this story.
[00:53:55.400 --> 00:54:01.280]   My suggestion to folks that are in that situation is, "The next move would be to pivot into
[00:54:01.280 --> 00:54:07.560]   figuring out prompts because there is an art to write in those prompts."
[00:54:07.560 --> 00:54:15.360]   I can put in mid-journey or stable diffusion, give me two dogs playing in the field.
[00:54:15.360 --> 00:54:18.480]   It'll give me two dogs running and playing in the field, but someone else can come in
[00:54:18.480 --> 00:54:23.800]   and say, "Hey, how about make it have professional lighting with vibrant green grass and it's
[00:54:23.800 --> 00:54:24.800]   at golden hour."
[00:54:24.800 --> 00:54:30.160]   They knew the nuance of crafting that prompt to make that image look even better.
[00:54:30.160 --> 00:54:33.160]   There's an imaginative quality to it.
[00:54:33.160 --> 00:54:38.680]   I realize I don't have that or I'm not practicing.
[00:54:38.680 --> 00:54:42.880]   When I'm putting together prompts, I haven't done many of them.
[00:54:42.880 --> 00:54:47.760]   Granted, I'd probably get better if I really spent the time understanding these prompts
[00:54:47.760 --> 00:54:48.760]   and everything.
[00:54:48.760 --> 00:54:49.760]   Man, when I do it...
[00:54:49.760 --> 00:54:52.360]   Stories on z-net that can help you with that.
[00:54:52.360 --> 00:54:54.040]   I'm sure there are.
[00:54:54.040 --> 00:54:55.560]   I've read some of the stories.
[00:54:55.560 --> 00:55:00.360]   I'm going to get good at this because I should really know how to do this.
[00:55:00.360 --> 00:55:02.560]   Otherwise, this whole thing is going to pass me by.
[00:55:02.560 --> 00:55:07.240]   Every time I sit down to do it, I'm super underwhelmed with what I come up with.
[00:55:07.240 --> 00:55:12.960]   I just realize, I think what I respect about the people who can write these prompts is it
[00:55:12.960 --> 00:55:19.000]   takes a certain quality of imagination to be able to visualize, at least when we're
[00:55:19.000 --> 00:55:25.640]   talking image-generative AI, you have to visualize in your mind exactly what you want
[00:55:25.640 --> 00:55:30.400]   to see, the details that are important, the style that's important.
[00:55:30.400 --> 00:55:34.160]   Yeah, there's some modifiers in the syntax that if you know how to do those, you're going
[00:55:34.160 --> 00:55:37.120]   to get a higher quality output and that sort of thing.
[00:55:37.120 --> 00:55:41.200]   But you really kind of have to create the image in your mind before you ever put it into
[00:55:41.200 --> 00:55:42.200]   a prompt.
[00:55:42.200 --> 00:55:44.120]   I am very unskilled at that.
[00:55:44.120 --> 00:55:46.040]   That is very apparent to me.
[00:55:46.040 --> 00:55:48.640]   But that's a bad respect for those who can do it.
[00:55:48.640 --> 00:55:54.720]   Really skilled at doing the video game art as the way these illustrators are describing,
[00:55:54.720 --> 00:55:59.360]   they're visualizing that stuff as they put in stylus to the illustrator or what have
[00:55:59.360 --> 00:56:01.720]   you.
[00:56:01.720 --> 00:56:06.840]   I know it's easier for me to say it than done, but take that same imagination and put it
[00:56:06.840 --> 00:56:08.840]   into text.
[00:56:08.840 --> 00:56:12.800]   Don't get yourself not that it is game.
[00:56:12.800 --> 00:56:13.760]   Don't hate on the game.
[00:56:13.760 --> 00:56:18.240]   Just go get better at the game.
[00:56:18.240 --> 00:56:20.720]   The example you gave is a good one.
[00:56:20.720 --> 00:56:25.520]   We used to pay somebody probably $35 to $75 an hour.
[00:56:25.520 --> 00:56:32.400]   It would take them an hour to maybe four hours to do what the AI does in three seconds.
[00:56:32.400 --> 00:56:38.200]   Most of those people, they are who got paid to do that.
[00:56:38.200 --> 00:56:40.240]   That has been kind of a slow burn.
[00:56:40.240 --> 00:56:48.720]   Some of that work has been slowly automated by the software over a number of years here.
[00:56:48.720 --> 00:56:55.280]   What we know is those people have a lot of really important and valuable things to do.
[00:56:55.280 --> 00:56:57.200]   Most of them are still employed.
[00:56:57.200 --> 00:57:03.000]   They're just doing either more work, getting more done faster, or they're also taking on
[00:57:03.000 --> 00:57:06.080]   other work and doing additional work.
[00:57:06.080 --> 00:57:08.040]   We've seen this throughout human history too.
[00:57:08.040 --> 00:57:13.800]   If we zoom out just one level further, every time there's been a new level of automation,
[00:57:13.800 --> 00:57:15.440]   it does two things.
[00:57:15.440 --> 00:57:17.400]   Everybody says there's not going to be any jobs.
[00:57:17.400 --> 00:57:21.080]   From the time an industrial revolution came around.
[00:57:21.080 --> 00:57:24.720]   When cars came around, when the plane came around, all of the stuff.
[00:57:24.720 --> 00:57:28.440]   It just erupts what was normal prior to.
[00:57:28.440 --> 00:57:32.800]   That's scary because it's changed.
[00:57:32.800 --> 00:57:36.960]   We see with layoffs too, when there's a bunch of layoffs like there have been recently, what
[00:57:36.960 --> 00:57:44.280]   we learn often through that process is one of the greatest resources in human history
[00:57:44.280 --> 00:57:45.920]   has been boredom.
[00:57:45.920 --> 00:57:50.480]   Because when people are laid off or don't have work, what do they do?
[00:57:50.480 --> 00:57:52.280]   They typically have time.
[00:57:52.280 --> 00:57:53.880]   They often have ideas.
[00:57:53.880 --> 00:57:56.560]   Now they have time to go work on those ideas.
[00:57:56.560 --> 00:58:04.000]   Those ideas are often what germinates like this next wave of whatever invention, innovation,
[00:58:04.000 --> 00:58:07.120]   even efficiency, all of those things.
[00:58:07.120 --> 00:58:11.120]   We've seen that human beings do do that work.
[00:58:11.120 --> 00:58:13.280]   You're talking about Jason Howell.
[00:58:13.280 --> 00:58:14.880]   They have ideas in their heads.
[00:58:14.880 --> 00:58:20.160]   A lot of times their limitation is resources or time and time and resources are often very
[00:58:20.160 --> 00:58:23.320]   closely connected.
[00:58:23.320 --> 00:58:30.960]   What we see is that there's always a new burst of energy and creativity from very disruptive
[00:58:30.960 --> 00:58:32.280]   moments like this.
[00:58:32.280 --> 00:58:36.440]   I think from that sense, it doesn't mean it's not difficult or challenging or uncomfortable
[00:58:36.440 --> 00:58:37.720]   because it is.
[00:58:37.720 --> 00:58:41.320]   But I think we should draw some hope from that.
[00:58:41.320 --> 00:58:46.400]   That's likely this and what's happening now is likely to power a new wave of creativity
[00:58:46.400 --> 00:58:47.960]   and innovation and invention.
[00:58:47.960 --> 00:58:51.040]   It probably will mean people working less.
[00:58:51.040 --> 00:58:54.880]   It'll probably mean people can get more done in less time.
[00:58:54.880 --> 00:58:57.280]   It may mean for a four day work week.
[00:58:57.280 --> 00:59:02.880]   It might mean not people not having to put as much time in but can still get the same
[00:59:02.880 --> 00:59:07.720]   amount of productivity, perhaps more productivity out of less time.
[00:59:07.720 --> 00:59:11.040]   Those would all be excellent long term benefits.
[00:59:11.040 --> 00:59:16.720]   There's a former CEO in my past, a company I worked at, umpteen years ago.
[00:59:16.720 --> 00:59:23.480]   It was data entry and we moved into doing um the client would do their own data entry
[00:59:23.480 --> 00:59:26.200]   online and just upload it to us.
[00:59:26.200 --> 00:59:31.120]   And I remember all of my co-workers were up in arms like, "Oh my goodness, we're going
[00:59:31.120 --> 00:59:33.400]   to get laid off because they're doing their own work."
[00:59:33.400 --> 00:59:35.120]   And yadda yadda yadda yadda.
[00:59:35.120 --> 00:59:39.000]   And I remember our CEO at the time, that man is so that come awesome.
[00:59:39.000 --> 00:59:41.640]   His track record is just unbelievable.
[00:59:41.640 --> 00:59:44.280]   But he came out to us and he says, "You know what?
[00:59:44.280 --> 00:59:48.280]   Yes, they're going to do that upload and send it to us and we can check it.
[00:59:48.280 --> 00:59:52.240]   But what that's going to do is free you up to do some things with more brain power and
[00:59:52.240 --> 00:59:56.720]   go in and be able to be more analytical and branch out into other things and help us with
[00:59:56.720 --> 01:00:00.400]   these other processes and maybe even build something else and put your name on it because
[01:00:00.400 --> 01:00:04.480]   it's just going to make it better and just free you up to expand all this extra time
[01:00:04.480 --> 01:00:07.840]   that you're going to have to make something bigger and better."
[01:00:07.840 --> 01:00:09.840]   You know, and he nailed it.
[01:00:09.840 --> 01:00:12.120]   You know, it's because things did get better after that.
[01:00:12.120 --> 01:00:17.760]   I'll give you one very recent example because in these conversations, everyone's like, "Well,
[01:00:17.760 --> 01:00:21.920]   can you give me an example of how things are going to change but be okay?"
[01:00:21.920 --> 01:00:30.200]   And to Jason Heiner's point, now it wasn't as hyperbolic or hyped with consumers, but
[01:00:30.200 --> 01:00:38.920]   in the business in B2B tech, when the cloud was introduced, there was so much angst and
[01:00:38.920 --> 01:00:42.960]   worry about this is going to just take every job.
[01:00:42.960 --> 01:00:43.960]   Yeah.
[01:00:43.960 --> 01:00:48.840]   And instead, what it did is allowed companies to scale and add new jobs.
[01:00:48.840 --> 01:00:49.840]   Yep.
[01:00:49.840 --> 01:00:50.840]   Got more engineers.
[01:00:50.840 --> 01:00:52.840]   That's more engineers.
[01:00:52.840 --> 01:00:53.840]   That's more engineers.
[01:00:53.840 --> 01:00:54.840]   Got more engineers.
[01:00:54.840 --> 01:00:55.840]   Okay.
[01:00:55.840 --> 01:01:01.640]   So then go back to now considering all that as extra kind of context behind the initial
[01:01:01.640 --> 01:01:10.800]   story here with all governments around the world feeling like this is a moment in time
[01:01:10.800 --> 01:01:12.880]   where something needs to be done.
[01:01:12.880 --> 01:01:13.880]   We don't know.
[01:01:13.880 --> 01:01:15.800]   We know that AI is important.
[01:01:15.800 --> 01:01:20.240]   We know that it has the potential to change things drastically, but we need to understand
[01:01:20.240 --> 01:01:21.240]   the technology.
[01:01:21.240 --> 01:01:23.880]   We need to figure out how we put in these guardrails.
[01:01:23.880 --> 01:01:28.240]   Like, is that an appropriate response to what you're saying now?
[01:01:28.240 --> 01:01:34.120]   Because if we've gone through this with cloud as one example, like, I mean, I don't know,
[01:01:34.120 --> 01:01:38.760]   you tell me, did the government get freaked out about the potential of the cloud to destroy
[01:01:38.760 --> 01:01:43.760]   jobs and change lives to the point to where they felt they needed to put in serious guardrails
[01:01:43.760 --> 01:01:48.720]   to protect, in some cases, in some people's view, not just jobs, but humanity as a whole,
[01:01:48.720 --> 01:01:51.680]   which personally I feel is pretty far overblown.
[01:01:51.680 --> 01:01:56.040]   But hey, maybe it's not, but I don't know.
[01:01:56.040 --> 01:01:57.240]   Is that an appropriate response?
[01:01:57.240 --> 01:01:58.240]   What do you think?
[01:01:58.240 --> 01:01:59.240]   It is.
[01:01:59.240 --> 01:02:00.240]   It's a good place.
[01:02:00.240 --> 01:02:04.720]   Look, this is a good place for government to step in and play referee, right?
[01:02:04.720 --> 01:02:12.760]   That's often the best place for the government to be, not to play a centralized source for
[01:02:12.760 --> 01:02:18.440]   all of the answers, but a place that helps to set the rules of the game and make sure
[01:02:18.440 --> 01:02:23.880]   people are playing by those rules in ways that are fair and that are not going to have
[01:02:23.880 --> 01:02:33.520]   very negative effects on people or create levels of danger, inequality, violence, all
[01:02:33.520 --> 01:02:35.040]   of those kinds of things.
[01:02:35.040 --> 01:02:39.960]   And there are real dangers with a lot of these generative AI models, right?
[01:02:39.960 --> 01:02:46.040]   You can use them to say, what's the best way to create a bomb and then what's the most
[01:02:46.040 --> 01:02:50.200]   effective place to put it in this building so that I can collapse the whole thing, right?
[01:02:50.200 --> 01:02:54.900]   These getervail AI models that have all of this data, these large language models that
[01:02:54.900 --> 01:02:58.840]   can process all this data, it could give you a very good answer to that.
[01:02:58.840 --> 01:03:07.720]   Now, to be fair, GPT is actively working on putting their own guardrails in place, which
[01:03:07.720 --> 01:03:09.280]   is good.
[01:03:09.280 --> 01:03:16.920]   But I think we don't want to just leave it to the goodwill of companies to do the right
[01:03:16.920 --> 01:03:23.160]   thing, that that's a place where I think the public, us as the public, the whole public,
[01:03:23.160 --> 01:03:28.400]   I think we want to have a say in that and that's where the government comes in, right?
[01:03:28.400 --> 01:03:32.960]   And like being the referee, setting the rules and making sure people are playing by those
[01:03:32.960 --> 01:03:37.600]   rules and for there to be stiff penalties if they aren't because it can have very negative
[01:03:37.600 --> 01:03:46.000]   effects, you know, socially and affect lives and incomes and all of these things.
[01:03:46.000 --> 01:03:54.040]   It's not easy or it's also very complex, but I think these are the things that we want.
[01:03:54.040 --> 01:03:59.280]   This is a good place for government to put its energies to rather than some other things
[01:03:59.280 --> 01:04:03.800]   that sometimes are not very fruitful or efficient.
[01:04:03.800 --> 01:04:10.160]   It'll be interesting to watch how different regions adapt.
[01:04:10.160 --> 01:04:16.400]   I mean, here in the States for sure, and we have an interesting approach to our government
[01:04:16.400 --> 01:04:21.160]   and their sophistication and tech regulation, people have very strong opinions about that.
[01:04:21.160 --> 01:04:29.120]   But a joking aside, look, Ben Gertzel is based or was recently based in Hong Kong and Pascal
[01:04:29.120 --> 01:04:31.240]   Kaufman is based in Switzerland.
[01:04:31.240 --> 01:04:38.360]   Some of the best AI thinkers are in totally disparate regions and those regions, just like
[01:04:38.360 --> 01:04:44.720]   with social media, will have different approaches to AI regulation, but we know with technology
[01:04:44.720 --> 01:04:49.440]   it's diffuse, it will propagate no matter what.
[01:04:49.440 --> 01:04:57.840]   And so the technology that powers this will fundamentally make changes whether we regulate
[01:04:57.840 --> 01:04:59.840]   it or not.
[01:04:59.840 --> 01:05:07.000]   Yeah, there's only so much that we can do to a certain degree about how it's going to
[01:05:07.000 --> 01:05:09.720]   develop for sure.
[01:05:09.720 --> 01:05:10.720]   That is true.
[01:05:10.720 --> 01:05:17.280]   Yeah, and Jason, I believe you had mentioned, you know, open AI kind of throttling themselves.
[01:05:17.280 --> 01:05:19.240]   There's actually a story.
[01:05:19.240 --> 01:05:27.520]   Sam Altman, who is open AI CEO and co-founder, had confirmed that they are not training GPT-5.
[01:05:27.520 --> 01:05:32.640]   At this point, as we know, GPT-3 came out, wow, the socks out of everybody, and then
[01:05:32.640 --> 01:05:38.160]   GPT-4 came out and I don't know how they did it with that, but people were absolutely
[01:05:38.160 --> 01:05:39.160]   blown away.
[01:05:39.160 --> 01:05:44.440]   That's I feel when the conversation of, oh my God, like what have we got ourselves into
[01:05:44.440 --> 01:05:50.240]   started to really ramp up because people saw how damn good GPT-4 was.
[01:05:50.240 --> 01:05:52.720]   And of course, you know, these are for-profit companies.
[01:05:52.720 --> 01:05:56.400]   We kind of assume they're working on their latest and greatest and they might very well
[01:05:56.400 --> 01:06:04.920]   be, but Sam is saying that they're not currently training GPT-5.
[01:06:04.920 --> 01:06:09.680]   So maybe that will calm your nerves a little bit.
[01:06:09.680 --> 01:06:12.000]   I don't know anyone who's really worried about this stuff.
[01:06:12.000 --> 01:06:14.200]   I don't know how far that would go to calm your nerves.
[01:06:14.200 --> 01:06:20.120]   But there he also said that the size of large language models won't matter.
[01:06:20.120 --> 01:06:27.480]   I thought this was kind of interesting because we're so used to in technology talking about
[01:06:27.480 --> 01:06:33.080]   specs and bigger, better, faster, that sort of stuff.
[01:06:33.080 --> 01:06:39.720]   And so of course, a larger data set for large language models would be better than a smaller
[01:06:39.720 --> 01:06:40.720]   one.
[01:06:40.720 --> 01:06:46.400]   But Sam Altman had said he did an interview for the Imagination in Action Event at MIT
[01:06:46.400 --> 01:06:55.080]   just a couple of days ago and kind of likened it to where we are with smartphones, how
[01:06:55.080 --> 01:06:59.360]   for the longest time these things were all about, you know, bigger, better, faster, what's
[01:06:59.360 --> 01:07:02.040]   the speed, what's the memory, blah, blah.
[01:07:02.040 --> 01:07:07.600]   And now we're at a point where that kind of stuff doesn't matter nearly as much.
[01:07:07.600 --> 01:07:09.720]   It's just they're much more efficient.
[01:07:09.720 --> 01:07:11.920]   They're able to do more with less.
[01:07:11.920 --> 01:07:18.080]   And so he sees that being a case with these large language models going forward.
[01:07:18.080 --> 01:07:21.920]   He's a smart guy, so he might be on to something.
[01:07:21.920 --> 01:07:23.920]   But yeah.
[01:07:23.920 --> 01:07:24.920]   All right.
[01:07:24.920 --> 01:07:28.720]   Well, we've got more to discuss here coming up.
[01:07:28.720 --> 01:07:31.120]   But first, let's take a break.
[01:07:31.120 --> 01:07:33.840]   And then when we come back, I don't know, what do you all want to talk about?
[01:07:33.840 --> 01:07:37.440]   I mean, I guess we could stick on the AI beat.
[01:07:37.440 --> 01:07:43.800]   So we can kind of do a little bit of an Oreo cookie because we can take AI and Twitter
[01:07:43.800 --> 01:07:48.800]   and sandwich them together because AI apparently wants to get in or Twitter apparently wants
[01:07:48.800 --> 01:07:50.520]   to get in on the AI game.
[01:07:50.520 --> 01:07:51.920]   So that's coming up here in a moment.
[01:07:51.920 --> 01:07:55.840]   But first, this episode of this week in tech is brought to you by Cisco, Morocco, the
[01:07:55.840 --> 01:08:01.680]   experts in cloud-based networking for hybrid work, whether your employees are working at
[01:08:01.680 --> 01:08:06.440]   home, maybe they're working at a cabin in the mountains or they're sitting in a lounge
[01:08:06.440 --> 01:08:08.560]   chair at the beach working.
[01:08:08.560 --> 01:08:09.560]   That's my favorite.
[01:08:09.560 --> 01:08:13.040]   I would love to be working at a beach in a lounge chair.
[01:08:13.040 --> 01:08:18.280]   Well, a cloud-managed network provides the same exceptional work experience no matter
[01:08:18.280 --> 01:08:19.880]   where they are.
[01:08:19.880 --> 01:08:23.120]   You may as well roll out the welcome mat hybrid work.
[01:08:23.120 --> 01:08:27.040]   It's not going anywhere hybrid work works best in the cloud.
[01:08:27.040 --> 01:08:30.480]   It has its perks for both employees and leaders.
[01:08:30.480 --> 01:08:31.800]   Workers of course can move faster.
[01:08:31.800 --> 01:08:38.280]   It can deliver better results, especially when you're using a cloud-managed network.
[01:08:38.280 --> 01:08:40.320]   Leaders can automate distributed operations.
[01:08:40.320 --> 01:08:47.040]   They can build more sustainable workspaces and you can proactively protect the network.
[01:08:47.040 --> 01:08:54.560]   An IDG, market pulse research report conducted for Meraki highlights top tier opportunities
[01:08:54.560 --> 01:08:56.680]   in supporting hybrid work.
[01:08:56.680 --> 01:09:03.720]   Now hybrid work is a priority for 78% of C-suite executives according to this report.
[01:09:03.720 --> 01:09:09.640]   Leaders want to drive collaboration forward while also staying on top of or in fact boosting
[01:09:09.640 --> 01:09:12.040]   productivity and security.
[01:09:12.040 --> 01:09:15.400]   But as we know hybrid work, it has its challenges, big challenges.
[01:09:15.400 --> 01:09:19.280]   The IDG report raises the red flag about security.
[01:09:19.280 --> 01:09:27.200]   It notes that 48% of leaders report cybersecurity threats as a primary obstacle to improving
[01:09:27.200 --> 01:09:30.160]   workforce experiences.
[01:09:30.160 --> 01:09:35.800]   Always on security monitoring is part of what makes the cloud-managed network so awesome.
[01:09:35.800 --> 01:09:40.520]   IT can use apps from Meraki's vast ecosystem of partners.
[01:09:40.520 --> 01:09:46.400]   These are turn key solutions built to work seamlessly with the Meraki Cloud platform.
[01:09:46.400 --> 01:09:47.480]   And you can do a number of things.
[01:09:47.480 --> 01:09:53.480]   You can asset track, you can use their location analytics and a whole lot more.
[01:09:53.480 --> 01:09:57.840]   And by using these, by using Meraki for all of this, you get insights on how people are
[01:09:57.840 --> 01:09:59.560]   using their workspaces.
[01:09:59.560 --> 01:10:04.320]   So environmental sensors, for example, can track activity and occupancy levels to stay
[01:10:04.320 --> 01:10:07.480]   on top of cleanliness and other things.
[01:10:07.480 --> 01:10:09.000]   You can reserve workspaces.
[01:10:09.000 --> 01:10:16.320]   So based on vacancy and employees' profiles, which you may have heard referred to as hot
[01:10:16.320 --> 01:10:22.760]   desking, locations in restricted environments can be booked in advance and include time-based
[01:10:22.760 --> 01:10:24.600]   door access.
[01:10:24.600 --> 01:10:30.400]   And then mobile device management, that's integrating devices and systems to allow IT
[01:10:30.400 --> 01:10:36.520]   to manage, to update, to troubleshoot company-owned devices, even when the device and employee are
[01:10:36.520 --> 01:10:39.160]   in a remote location.
[01:10:39.160 --> 01:10:44.160]   So in essence, you can turn any space into a place of productivity.
[01:10:44.160 --> 01:10:48.480]   You can empower your organization with the same exceptional experience, no matter where
[01:10:48.480 --> 01:10:53.480]   they work when you're using Meraki and the Cisco suite of technology.
[01:10:53.480 --> 01:10:54.680]   It's really great.
[01:10:54.680 --> 01:10:58.960]   Learn how your organization can make hybrid work work.
[01:10:58.960 --> 01:11:01.720]   Visit Meraki.cisco.com/twit.
[01:11:01.720 --> 01:11:07.600]   That's M-E-R-A-K-I.cisco.com/twit.
[01:11:07.600 --> 01:11:13.360]   And we thank them for their support of this week in tech.
[01:11:13.360 --> 01:11:16.360]   All right, so let's see here.
[01:11:16.360 --> 01:11:24.920]   So earlier this week, Twitter, it was noticed, it was discovered that in a court filing,
[01:11:24.920 --> 01:11:27.520]   that Twitter is no longer Twitter.
[01:11:27.520 --> 01:11:32.480]   I mean, the service is Twitter, but the company that owns Twitter is no longer Twitter.
[01:11:32.480 --> 01:11:35.120]   It's now a company called Xcorp.
[01:11:35.120 --> 01:11:41.600]   So basically, Twitter ceased to exist as an independent company, merged with this new
[01:11:41.600 --> 01:11:43.920]   shell company called Xcorp.
[01:11:43.920 --> 01:11:47.640]   Of course Elon Musk is behind this.
[01:11:47.640 --> 01:11:50.040]   And I don't know, it's kind of interesting.
[01:11:50.040 --> 01:11:54.680]   When I first was reading this and thinking about it, I was thinking of it as like, oh,
[01:11:54.680 --> 01:11:58.800]   so Facebook did this and they became meta.
[01:11:58.800 --> 01:12:01.680]   Google did this and they became Alphabet.
[01:12:01.680 --> 01:12:03.000]   So this is the same thing, right?
[01:12:03.000 --> 01:12:08.080]   And I think that yes, it probably is very similar to that.
[01:12:08.080 --> 01:12:11.240]   But it all seems to hinge on this idea.
[01:12:11.240 --> 01:12:13.480]   Well, it goes in a couple of different directions.
[01:12:13.480 --> 01:12:18.000]   The first direction that we knew prior to the AI stuff is that we knew that Elon Musk
[01:12:18.000 --> 01:12:24.000]   has been talking for a while about this super app, like a WeChat style app that manages
[01:12:24.000 --> 01:12:25.160]   all sorts of things.
[01:12:25.160 --> 01:12:28.520]   It's not just a social network, it's also a payments platform.
[01:12:28.520 --> 01:12:32.720]   Does all these things meant to do everything, that's why they call it an everything app.
[01:12:32.720 --> 01:12:42.160]   And that this move into this structure would enable that to be, would enable that to happen,
[01:12:42.160 --> 01:12:44.440]   essentially.
[01:12:44.440 --> 01:12:50.280]   But we also heard just a couple of days ago that Elon Musk created a new artificial intelligence
[01:12:50.280 --> 01:12:53.520]   company called X.AI.
[01:12:53.520 --> 01:12:56.520]   So apparently Elon Musk loves the letter X.
[01:12:56.520 --> 01:13:00.120]   So many things in his life have X involved.
[01:13:00.120 --> 01:13:02.600]   This is just yet another thing.
[01:13:02.600 --> 01:13:07.440]   Of course, Elon Musk is going to want to get into the AI game.
[01:13:07.440 --> 01:13:10.320]   And this seems to be a part of that.
[01:13:10.320 --> 01:13:13.400]   So I guess we could start there.
[01:13:13.400 --> 01:13:17.400]   How do we feel about Elon Musk making yet another big change to Twitter?
[01:13:17.400 --> 01:13:22.200]   But this one, I don't know, I guess from a business sense, it kind of makes sense.
[01:13:22.200 --> 01:13:23.200]   But what do you guys think?
[01:13:23.200 --> 01:13:26.280]   Didn't you have some beef with open AI?
[01:13:26.280 --> 01:13:27.280]   Well, yeah.
[01:13:27.280 --> 01:13:32.440]   So he was a co-founder, if I'm not mistaken, of open AI.
[01:13:32.440 --> 01:13:35.600]   Back in 2018, is it?
[01:13:35.600 --> 01:13:40.640]   He kind of had a, I don't know if you call it a falling out, but essentially he stepped
[01:13:40.640 --> 01:13:42.480]   away from open AI.
[01:13:42.480 --> 01:13:46.760]   He was concerned about the direction of artificial intelligence.
[01:13:46.760 --> 01:13:48.040]   So yeah, it is.
[01:13:48.040 --> 01:13:55.920]   It's an interesting point to bring up at because he has been one of the people in recent
[01:13:55.920 --> 01:14:03.360]   years sounding alarm bells about the potential of AI and the development of it and being
[01:14:03.360 --> 01:14:09.600]   sometimes going to that extra level, that extra AI-sentience level.
[01:14:09.600 --> 01:14:13.880]   And it'd be easy to kind of write him off as silly as a result of that.
[01:14:13.880 --> 01:14:16.000]   But there's that.
[01:14:16.000 --> 01:14:17.640]   And then now what's happening?
[01:14:17.640 --> 01:14:20.320]   Well, he wants to get in on AI once again.
[01:14:20.320 --> 01:14:23.320]   So I wonder, are his intentions different?
[01:14:23.320 --> 01:14:27.120]   Does he want to do AI in his mind responsibly?
[01:14:27.120 --> 01:14:28.120]   Put that in air quotes?
[01:14:28.120 --> 01:14:29.120]   I don't know.
[01:14:29.120 --> 01:14:33.600]   Do we have any thoughts as far as like what his plan might be, his master plan?
[01:14:33.600 --> 01:14:40.720]   I think to be clear with open AI, as I understand it, and Sam Altman's take on it, you know,
[01:14:40.720 --> 01:14:47.560]   Elon's thing is like, we didn't create open AI so that it could become, you know, controlled
[01:14:47.560 --> 01:14:49.320]   by Microsoft, right?
[01:14:49.320 --> 01:14:50.560]   Or, or another.
[01:14:50.560 --> 01:14:51.560]   Yeah, they're just open.
[01:14:51.560 --> 01:14:53.760]   Yeah, that it was meant to be open.
[01:14:53.760 --> 01:14:54.760]   Yeah.
[01:14:54.760 --> 01:14:59.960]   Now, Sam Altman's take is that when they got to a point in open AI, they realized in order
[01:14:59.960 --> 01:15:05.200]   to get to the level of magnitude and scale they wanted, it was going to take a lot of
[01:15:05.200 --> 01:15:07.880]   servers and a lot of server power.
[01:15:07.880 --> 01:15:12.560]   And they weren't getting the donations for it to really scale it.
[01:15:12.560 --> 01:15:15.520]   They weren't getting the rocket fuel they needed.
[01:15:15.520 --> 01:15:19.480]   And so that's why they had to go to find some corporate partners.
[01:15:19.480 --> 01:15:24.480]   But what he says is they still, the way their board is set up, that the nonprofit board
[01:15:24.480 --> 01:15:31.000]   for open AI still controls the, you know, the ultimate destiny of it.
[01:15:31.000 --> 01:15:38.240]   And so there is some, and Elon wasn't happy with that, still isn't happy with it.
[01:15:38.240 --> 01:15:45.000]   That said, you know, Elon's trying to do this as a in a private company setting with open
[01:15:45.000 --> 01:15:47.600]   with, yeah, Twitter.
[01:15:47.600 --> 01:15:53.840]   So with 1300 employees, it's hard to see a very good trajectory where, you know, they
[01:15:53.840 --> 01:15:59.720]   could make a dent in the universe with it from a Twitter standpoint.
[01:15:59.720 --> 01:16:05.420]   And I think they have enough things on their plate that it doesn't seem very likely to
[01:16:05.420 --> 01:16:08.360]   have a large impact anytime soon.
[01:16:08.360 --> 01:16:12.820]   That said, I'd be surprised if not just, so I kind of think of it as a non-story other
[01:16:12.820 --> 01:16:15.320]   than the fact that they want to get into it.
[01:16:15.320 --> 01:16:18.560]   And I think that's why I think that the company that's starting a company or transforming
[01:16:18.560 --> 01:16:23.400]   their company right now, they don't have AI as part of their mandate, then they're probably,
[01:16:23.400 --> 01:16:27.280]   you know, not thinking of it right, whether you make toilet paper or whether you make,
[01:16:27.280 --> 01:16:33.840]   you know, open platforms to grow the inner, to be a platform for the internet or a super
[01:16:33.840 --> 01:16:34.840]   app.
[01:16:34.840 --> 01:16:36.360]   I mean, that's like a lot of, yeah, go down.
[01:16:36.360 --> 01:16:41.920]   A lot of, a lot of tech stories, the dynamics, the interpersonal dynamics are far more interesting
[01:16:41.920 --> 01:16:50.800]   than the tech dynamics and that relationship that he's had with OpenAI and Microsoft has
[01:16:50.800 --> 01:16:52.440]   been pretty interesting.
[01:16:52.440 --> 01:16:58.120]   What he's referring to with this, he's talked for a long time about AI.
[01:16:58.120 --> 01:17:04.920]   What he's referring to is the paperclip Maximizer, which was a phrase coined by Nick Bostrom
[01:17:04.920 --> 01:17:11.480]   in his book Superintelligence, which was published in 2013 and has kind of defined the
[01:17:11.480 --> 01:17:18.200]   macro AI environment for a long time, not the smaller and more practical business sense,
[01:17:18.200 --> 01:17:22.480]   but there is, and there's kind of a cult of people, maybe I shouldn't use a word cult,
[01:17:22.480 --> 01:17:27.400]   but there is a, there are adherence, well, I misspoke, but there are adherence to this
[01:17:27.400 --> 01:17:30.560]   idea of AI superintelligence.
[01:17:30.560 --> 01:17:38.840]   And that is not just AGI, but the rapid lead, there is an intelligence explosion beyond just
[01:17:38.840 --> 01:17:47.440]   these really useful models and AGI and that artificial superintelligence will be, will
[01:17:47.440 --> 01:17:48.960]   happen rapidly.
[01:17:48.960 --> 01:17:54.080]   And that superintelligence, the values of a machine superintelligence aren't aligned
[01:17:54.080 --> 01:17:55.840]   with human values.
[01:17:55.840 --> 01:18:01.920]   And so the paperclip Maximizer comes in with this idea that, well, if we train an AI to
[01:18:01.920 --> 01:18:08.080]   create paperclips and we say make paperclips at the expense of everything else, make paper
[01:18:08.080 --> 01:18:14.920]   very efficiently, then that AI will have a very narrow value set and it will say, okay,
[01:18:14.920 --> 01:18:15.920]   no problem.
[01:18:15.920 --> 01:18:20.760]   It doesn't matter if people use the paperclips, I am going to atomize everything in the universe
[01:18:20.760 --> 01:18:22.680]   and create paperclips.
[01:18:22.680 --> 01:18:29.200]   It's kind of a ridiculous thought experiment, but there are many people in the valley now
[01:18:29.200 --> 01:18:33.240]   talking about this as an existential threat.
[01:18:33.240 --> 01:18:36.600]   They often don't talk about climate change, but do talk about this.
[01:18:36.600 --> 01:18:39.440]   Yeah, yeah, it's interesting.
[01:18:39.440 --> 01:18:41.040]   Interesting, isn't it?
[01:18:41.040 --> 01:18:46.440]   All right, so then mentioned the super app thing and I can remember this was brought
[01:18:46.440 --> 01:18:50.600]   up way before the actual purchase happened.
[01:18:50.600 --> 01:18:55.800]   And it seemed to be a bit controversial amongst some of the other pundits here at Twit.
[01:18:55.800 --> 01:19:04.040]   And I didn't think it was a bad idea considering we have the likes of the WeChat and so forth
[01:19:04.040 --> 01:19:09.240]   that's going on in Asian, it seems to be working, but I'm not sitting here saying, I want a
[01:19:09.240 --> 01:19:15.180]   Twitter super app and would put my financial information into it, but I don't see anything
[01:19:15.180 --> 01:19:19.560]   wrong with this premise of saying let's have a super app or we can have messages here,
[01:19:19.560 --> 01:19:26.480]   social here, share our, you know, send beer money to our buddy, you know, whatever.
[01:19:26.480 --> 01:19:28.280]   I don't really have it though.
[01:19:28.280 --> 01:19:32.080]   Yeah, it's just getting already like Twitter was like an extra payment layer.
[01:19:32.080 --> 01:19:34.080]   Is that a super app?
[01:19:34.080 --> 01:19:37.920]   I mean, it's like Android or iOS like, yeah, I have an app that does it.
[01:19:37.920 --> 01:19:38.920]   It's called demo.
[01:19:38.920 --> 01:19:39.920]   I have an app that orders cars.
[01:19:39.920 --> 01:19:40.920]   It's called Uber.
[01:19:40.920 --> 01:19:44.120]   It doesn't like if I do it in one app, does that change my experience?
[01:19:44.120 --> 01:19:45.120]   Not really.
[01:19:45.120 --> 01:19:51.080]   I mean, that story kind of misses all of the the subset, like the way China has subsidized
[01:19:51.080 --> 01:19:52.320]   the growth of WeChat.
[01:19:52.320 --> 01:19:55.880]   I mean, sure, I would love to see him try.
[01:19:55.880 --> 01:19:57.160]   And I'm not anti-musque.
[01:19:57.160 --> 01:19:59.520]   Like, okay, great, please try.
[01:19:59.520 --> 01:20:00.680]   I'm just skeptical.
[01:20:00.680 --> 01:20:02.720]   And there's our, it's an operating system.
[01:20:02.720 --> 01:20:04.560]   The super app is an operating system.
[01:20:04.560 --> 01:20:05.560]   Yeah.
[01:20:05.560 --> 01:20:06.720]   It lets you do all of these things.
[01:20:06.720 --> 01:20:12.720]   The super app needs network effects to like it operates on network effects of like people
[01:20:12.720 --> 01:20:15.160]   are in your ecosystem.
[01:20:15.160 --> 01:20:20.320]   And so you just make it easier to do other things in your ecosystem.
[01:20:20.320 --> 01:20:22.200]   And then it just grows naturally.
[01:20:22.200 --> 01:20:26.760]   I think Twitter is going to have a really hard time creating that kind of level of network
[01:20:26.760 --> 01:20:27.760]   effect.
[01:20:27.760 --> 01:20:30.600]   If somebody was going to make the super app, I think it's somebody who already has a
[01:20:30.600 --> 01:20:34.360]   super app, or somebody who already has some of those network effects, or that's going
[01:20:34.360 --> 01:20:40.040]   to create something so much better than something we have now, whether it be a social network,
[01:20:40.040 --> 01:20:48.560]   whether it be a way to send something or start a business and spin up, you know, my
[01:20:48.560 --> 01:20:55.320]   own thing that wouldn't and take payments from other people, something like that, that's
[01:20:55.320 --> 01:20:57.080]   who's going to have the next super app.
[01:20:57.080 --> 01:20:59.080]   I think Twitter is the more suited.
[01:20:59.080 --> 01:21:00.400]   Facebook, give me more suited.
[01:21:00.400 --> 01:21:02.600]   And frankly, Apple probably would too, right?
[01:21:02.600 --> 01:21:03.600]   Yeah.
[01:21:03.600 --> 01:21:10.600]   Now, Apple is on a billion devices if Apple makes a, but what Apple would have to do, which
[01:21:10.600 --> 01:21:15.080]   maybe they do, maybe they don't, but they might considering how fast and how far they've
[01:21:15.080 --> 01:21:19.280]   gone into services is that they might make a hundred dollar phone.
[01:21:19.280 --> 01:21:24.680]   If they make a hundred dollar phone that's in a lot of people's hands, that's just.
[01:21:24.680 --> 01:21:25.920]   They're so smart.
[01:21:25.920 --> 01:21:26.920]   Yeah.
[01:21:26.920 --> 01:21:27.920]   They would write.
[01:21:27.920 --> 01:21:31.480]   They would write me a hundred dollar phone.
[01:21:31.480 --> 01:21:37.760]   That actually, yeah, that actually draws a line to a story that's further down in the
[01:21:37.760 --> 01:21:40.240]   rundown, but that I thought was really interesting.
[01:21:40.240 --> 01:21:44.960]   I, you know, for those who don't know, I host a show on this network called all about Android.
[01:21:44.960 --> 01:21:45.960]   So it's very focused on Android.
[01:21:45.960 --> 01:21:49.960]   I've been doing Android for as long as I've had a smartphone, I've had an Android phone.
[01:21:49.960 --> 01:21:58.400]   And I've always thought of Android devices as the kind of prime device for not much money.
[01:21:58.400 --> 01:22:01.320]   I mean, which is not to say all, all Android phones are like that.
[01:22:01.320 --> 01:22:05.280]   But if you don't have a lot of, a lot of money to spend on a smartphone, chances are you're
[01:22:05.280 --> 01:22:08.360]   probably going to be getting an Android phone.
[01:22:08.360 --> 01:22:13.360]   But there is a story in here from the Wall Street Journal that is just pointing to how
[01:22:13.360 --> 01:22:22.800]   Apple is gradually over time becoming the low cost smartphone pick primarily because,
[01:22:22.800 --> 01:22:26.680]   you know, for a number of reasons, but they hold their value.
[01:22:26.680 --> 01:22:32.640]   They're built to the point to where an older iPhone is still has an excellent chip, you
[01:22:32.640 --> 01:22:38.520]   know, that's comparable to a current chip and all of these things and just how, how then
[01:22:38.520 --> 01:22:47.240]   the time that we're in now is more ripe for people being open to having these devices
[01:22:47.240 --> 01:22:53.000]   longer and being open to using older devices versus where we were five, seven years ago,
[01:22:53.000 --> 01:22:59.200]   where we were worshiping the premium and it was all premium worship, you know.
[01:22:59.200 --> 01:23:02.800]   And now it's like, like, I can only use myself.
[01:23:02.800 --> 01:23:04.800]   I can use myself as an example.
[01:23:04.800 --> 01:23:07.040]   My older daughter, she's 13.
[01:23:07.040 --> 01:23:11.000]   We finally, you know, said, okay, you really want a smartphone.
[01:23:11.000 --> 01:23:15.680]   We'll get a smartphone, but we got rules, you know, around this.
[01:23:15.680 --> 01:23:22.120]   If you want one for free, I, you know, father who is a host of an Android show, I have a
[01:23:22.120 --> 01:23:24.520]   drawer full of phones.
[01:23:24.520 --> 01:23:27.480]   You can use one if you like and you can have it for free.
[01:23:27.480 --> 01:23:29.800]   And she's like, G one for you.
[01:23:29.800 --> 01:23:30.960]   Here's an X is.
[01:23:30.960 --> 01:23:34.400]   Thankfully, I have more modern phones than that.
[01:23:34.400 --> 01:23:38.280]   You would only be like one generation out, let's say.
[01:23:38.280 --> 01:23:42.600]   And she said, no, I want an iPhone, you know, because all of her friends have an iPhone.
[01:23:42.600 --> 01:23:44.200]   And so what did we do?
[01:23:44.200 --> 01:23:47.040]   We went on to what service did we use?
[01:23:47.040 --> 01:23:50.160]   I think actually we ended up buying it through Amazon, but we looked on swap.
[01:23:50.160 --> 01:23:55.560]   We looked on all these other like third party or, you know, second hand refurbished, Apple
[01:23:55.560 --> 01:24:01.600]   refurbished and we ended up getting her an S, an SE, an iPhone SE from 2020.
[01:24:01.600 --> 01:24:05.920]   So it's two years old, but has an excellent, you know, rock and chip in it.
[01:24:05.920 --> 01:24:07.120]   It's going to be great for her.
[01:24:07.120 --> 01:24:08.120]   She loves it.
[01:24:08.120 --> 01:24:09.120]   It's perfect for her.
[01:24:09.120 --> 01:24:14.400]   And it was like 150 bucks, you know, so we're talking this like low end cost.
[01:24:14.400 --> 01:24:18.120]   Like what would she have gotten for $150 in the world of Android?
[01:24:18.120 --> 01:24:19.120]   Probably not.
[01:24:19.120 --> 01:24:20.120]   I hate to say it.
[01:24:20.120 --> 01:24:22.640]   Probably not something as strong and powerful as what she did.
[01:24:22.640 --> 01:24:23.640]   A remote control.
[01:24:23.640 --> 01:24:24.640]   Right.
[01:24:24.640 --> 01:24:30.560]   So anyway, this kind of correlates with that.
[01:24:30.560 --> 01:24:37.520]   Another strength that Apple has going for it is it's becoming a pick for that low mid-tier,
[01:24:37.520 --> 01:24:42.720]   you know, it's finally they're getting those phones, particularly in the refurb or in the
[01:24:42.720 --> 01:24:47.340]   second hand market, you know, those those powerful enough phones are not that expensive
[01:24:47.340 --> 01:24:48.340]   at all.
[01:24:48.340 --> 01:24:49.340]   So I think you're right.
[01:24:49.340 --> 01:24:53.240]   If Apple, I think it's a stretch to think that Apple would release a hundred dollar phone,
[01:24:53.240 --> 01:24:55.000]   but my gosh, they did.
[01:24:55.000 --> 01:24:57.360]   They'd be on the two ninety nine maybe.
[01:24:57.360 --> 01:24:58.360]   Yeah, totally.
[01:24:58.360 --> 01:24:59.360]   Yeah.
[01:24:59.360 --> 01:25:06.400]   Couldn't see it happen at a hundred, but I suppose you never know.
[01:25:06.400 --> 01:25:08.080]   Let's see here.
[01:25:08.080 --> 01:25:10.800]   Speaking of Apple, I like rumor stories.
[01:25:10.800 --> 01:25:14.800]   Some people are like anti rumor stories because they're not real and blah, blah, blah, but
[01:25:14.800 --> 01:25:16.800]   I find them fun.
[01:25:16.800 --> 01:25:18.920]   So I thought maybe you must have goods.
[01:25:18.920 --> 01:25:19.920]   Yeah.
[01:25:19.920 --> 01:25:22.520]   Well, I mean, I just thought this was interesting.
[01:25:22.520 --> 01:25:25.840]   So I got back a week a week ago from a vacation.
[01:25:25.840 --> 01:25:28.000]   I was gone for two and a half weeks.
[01:25:28.000 --> 01:25:33.040]   And so that whole time I was like completely oblivious to anything technology news.
[01:25:33.040 --> 01:25:39.000]   Like I made it a true like vacation from tech news and it was it was wonderful.
[01:25:39.000 --> 01:25:43.000]   But as you all know, the flip side of this is when you get back to work, you got a lot
[01:25:43.000 --> 01:25:44.480]   of catch up to do.
[01:25:44.480 --> 01:25:51.240]   So I wasn't aware that apparently it's at least according to Mark Herman at Bloomberg,
[01:25:51.240 --> 01:25:57.280]   it's pretty much a lock that that we're actually going to finally hear about Apple's virtual
[01:25:57.280 --> 01:26:04.120]   reality or whatever they're calling it, the reality headset at WWDC this year.
[01:26:04.120 --> 01:26:05.360]   So I don't know.
[01:26:05.360 --> 01:26:09.920]   I feel like we've heard this many times before and it keeps not happening, but Garmin seems
[01:26:09.920 --> 01:26:15.160]   pretty solid on the fact that this is going to be the WWDC where you see it period.
[01:26:15.160 --> 01:26:17.120]   He's got a great record.
[01:26:17.120 --> 01:26:19.000]   The timing could not be worse.
[01:26:19.000 --> 01:26:20.720]   I totally agree.
[01:26:20.720 --> 01:26:23.200]   That's why I wanted to talk about it.
[01:26:23.200 --> 01:26:28.120]   If Apple does release this headset and I'm starting to doubt if they will actually release
[01:26:28.120 --> 01:26:29.120]   it.
[01:26:29.120 --> 01:26:34.200]   So not beyond them to just say, you know what, let's pull this crap and not do it.
[01:26:34.200 --> 01:26:41.000]   Wait, because Disney just canceled their whole division that's making content for for metaverse.
[01:26:41.000 --> 01:26:42.000]   They don't.
[01:26:42.000 --> 01:26:47.640]   Meta huge layoffs and yes, even even Mark Zuckerberg is quitting the metaverse.
[01:26:47.640 --> 01:26:51.280]   And if Mark Zuckerberg is quitting the metaverse, then it probably is over.
[01:26:51.280 --> 01:26:53.360]   I mean, he went all in.
[01:26:53.360 --> 01:26:54.360]   He did.
[01:26:54.360 --> 01:26:56.040]   He's the biggest thing you could do with the metaverse.
[01:26:56.040 --> 01:26:58.320]   He changed his name to the metaverse.
[01:26:58.320 --> 01:26:59.320]   I know.
[01:26:59.320 --> 01:27:00.320]   I mean, he burns.
[01:27:00.320 --> 01:27:03.240]   He might as well have burned 10 million, 10 billion dollars.
[01:27:03.240 --> 01:27:04.240]   Oh my goodness.
[01:27:04.240 --> 01:27:05.240]   Yeah.
[01:27:05.240 --> 01:27:07.040]   So Zappa burn is pretty much done with that whole world.
[01:27:07.040 --> 01:27:08.040]   I didn't know this.
[01:27:08.040 --> 01:27:09.400]   He's shipping to AI.
[01:27:09.400 --> 01:27:12.440]   Like they got burned so badly on it and it tanked their stock price.
[01:27:12.440 --> 01:27:15.280]   And I think he's under a lot of pressure, you know, from the company.
[01:27:15.280 --> 01:27:18.080]   I mean, he, he's under as much pressure as he can be under.
[01:27:18.080 --> 01:27:19.080]   He has this controlling.
[01:27:19.080 --> 01:27:20.080]   Yeah.
[01:27:20.080 --> 01:27:22.760]   Everyone I talk to at Facebook is like, it's layoff.
[01:27:22.760 --> 01:27:24.800]   Well, actually nobody's getting worked on.
[01:27:24.800 --> 01:27:27.520]   They're all just sitting around waiting to get laid off.
[01:27:27.520 --> 01:27:29.040]   Oh my goodness.
[01:27:29.040 --> 01:27:30.680]   That's horrible.
[01:27:30.680 --> 01:27:32.040]   That's no fun at all.
[01:27:32.040 --> 01:27:39.840]   So I just can't, I just don't know that I see now Apple could be really take a position
[01:27:39.840 --> 01:27:43.600]   where they're going to say we're going to, we're going to zig when everybody else zags.
[01:27:43.600 --> 01:27:46.960]   They love to do that as everybody knows.
[01:27:46.960 --> 01:27:50.280]   But, you know, this was already sort of it.
[01:27:50.280 --> 01:27:51.520]   They had been ratcheting back.
[01:27:51.520 --> 01:27:52.520]   They've had leaks.
[01:27:52.520 --> 01:27:53.760]   They've leaked stuff to the press.
[01:27:53.760 --> 01:27:58.280]   You know, I think these are pretty clearly leaks in some sense of that they're only going
[01:27:58.280 --> 01:28:02.520]   to have 500,000 units that it's going to charge $3,000.
[01:28:02.520 --> 01:28:06.840]   It's really going to be more of a developer device to get people excited about the metaverse.
[01:28:06.840 --> 01:28:09.480]   So it's Google Glass all over again.
[01:28:09.480 --> 01:28:10.480]   It does, right?
[01:28:10.480 --> 01:28:11.480]   It starts to look like Google Glass.
[01:28:11.480 --> 01:28:16.920]   And I think Apple, I would be more, I would be more, I don't know.
[01:28:16.920 --> 01:28:24.800]   But they're going to actually release something if they had gone in the direction of AR, where
[01:28:24.800 --> 01:28:30.640]   this is a device that needs an iPhone and gives you a little bit of a heads up display
[01:28:30.640 --> 01:28:34.160]   that's powered by the iPhone wirelessly or something.
[01:28:34.160 --> 01:28:35.920]   And it's a third accessory, right?
[01:28:35.920 --> 01:28:38.800]   Because they've had huge success with AirPods' accessory.
[01:28:38.800 --> 01:28:41.040]   They've had huge success with the Apple Watch.
[01:28:41.040 --> 01:28:43.880]   Those are both essentially iPhone accessories.
[01:28:43.880 --> 01:28:47.640]   And another iPhone accessory, that would seem to be a slam dunk.
[01:28:47.640 --> 01:28:50.600]   But I just think the technology's not ready yet for that.
[01:28:50.600 --> 01:28:52.720]   And I think that's what they wanted to do.
[01:28:52.720 --> 01:28:54.240]   And they'd spent a lot of money on it.
[01:28:54.240 --> 01:28:57.720]   And it seems like then they went into the reality thing because it looked like there
[01:28:57.720 --> 01:29:01.960]   was a there, there, and all of a sudden it looks like, you know, everybody has left the
[01:29:01.960 --> 01:29:07.440]   party and if other than HTC and a few like diehards.
[01:29:07.440 --> 01:29:09.560]   But there's not good content.
[01:29:09.560 --> 01:29:13.960]   Even Apple, most of their developer content that they've been focusing on the past like
[01:29:13.960 --> 01:29:16.520]   three to five years has all been AR, not VR.
[01:29:16.520 --> 01:29:24.080]   You know, they've been developing really great developer kits and working closely with developers.
[01:29:24.080 --> 01:29:27.720]   They have a nice ecosystem that's growing in AR.
[01:29:27.720 --> 01:29:30.120]   VR is like a complete unknown.
[01:29:30.120 --> 01:29:32.280]   So they might release something.
[01:29:32.280 --> 01:29:38.040]   Clearly they have something ready to be released and have been demoing it to some people,
[01:29:38.040 --> 01:29:39.640]   some insiders.
[01:29:39.640 --> 01:29:46.160]   But it just seems like this is a really a train wreck rating to happen if they do it.
[01:29:46.160 --> 01:29:49.560]   And this is not something they are such a conservative company.
[01:29:49.560 --> 01:29:53.480]   They are very, very conservative about releasing new things.
[01:29:53.480 --> 01:29:59.000]   We see that everything is so incremental that Apple does now that this would be, I would
[01:29:59.000 --> 01:30:01.240]   be shocked almost surprised.
[01:30:01.240 --> 01:30:05.160]   And I know that's not the popular opinion or the consensus.
[01:30:05.160 --> 01:30:09.280]   But if they actually release this at WWDC, I don't think it's going to happen.
[01:30:09.280 --> 01:30:10.280]   Release or show?
[01:30:10.280 --> 01:30:12.480]   Oh, sorry, show.
[01:30:12.480 --> 01:30:14.200]   Maybe they will show it.
[01:30:14.200 --> 01:30:15.200]   That's true.
[01:30:15.200 --> 01:30:16.320]   That's a good asterisk.
[01:30:16.320 --> 01:30:19.640]   They might show it and say released at the end of the year.
[01:30:19.640 --> 01:30:22.640]   And then that still gives them the out to say at the end of the year, say, you know,
[01:30:22.640 --> 01:30:24.560]   at this still needs some time.
[01:30:24.560 --> 01:30:28.880]   And they might, they might, you know, charging pad this device and just, you know, put it
[01:30:28.880 --> 01:30:31.800]   back under the shelf and nobody ever hears from it.
[01:30:31.800 --> 01:30:37.320]   It ends up like a developer version of the device ends up on eBay in 10 years for some
[01:30:37.320 --> 01:30:38.320]   astronomical amount.
[01:30:38.320 --> 01:30:39.320]   Yeah.
[01:30:39.320 --> 01:30:40.320]   Could happen.
[01:30:40.320 --> 01:30:41.920]   Very, very simple.
[01:30:41.920 --> 01:30:44.560]   Like what Jason just described cost benefit.
[01:30:44.560 --> 01:30:46.360]   Like what is there to gain?
[01:30:46.360 --> 01:30:49.400]   Like what market share is there for them to take?
[01:30:49.400 --> 01:30:54.440]   How many users are there there for that Apple to acquire versus one of the risks and the
[01:30:54.440 --> 01:30:56.360]   risks are tremendous.
[01:30:56.360 --> 01:31:00.360]   So if they just, I mean, they could show it like Jason said and say, hey, it's a developer
[01:31:00.360 --> 01:31:04.080]   preview, you know, there's only going to be a certain amount and we'll iterate on this
[01:31:04.080 --> 01:31:05.080]   product over time.
[01:31:05.080 --> 01:31:06.080]   But why?
[01:31:06.080 --> 01:31:07.080]   Why not just wait?
[01:31:07.080 --> 01:31:10.720]   Why not just wait and release just like Jason said something that's AR.
[01:31:10.720 --> 01:31:11.880]   It's not called VR kit.
[01:31:11.880 --> 01:31:13.440]   It is called AR kit.
[01:31:13.440 --> 01:31:15.240]   Release something that's AR in a few years.
[01:31:15.240 --> 01:31:20.280]   And look, look, your Apple watch and your air parts already are fantastic peripherals
[01:31:20.280 --> 01:31:22.520]   for AR, right?
[01:31:22.520 --> 01:31:27.400]   The, the, um, air pods do great listening and peripheral listening.
[01:31:27.400 --> 01:31:31.440]   One of the problems that you have with everybody has with VR is that you shut out the rest
[01:31:31.440 --> 01:31:32.440]   of the world.
[01:31:32.440 --> 01:31:33.440]   Yeah.
[01:31:33.440 --> 01:31:34.440]   I have an infant toddler.
[01:31:34.440 --> 01:31:38.920]   I can't shut out the rest of my world to like get on a VR call when I could just zoom.
[01:31:38.920 --> 01:31:44.000]   But AR, especially if I have air pods that listen and have haptics and I have a watch
[01:31:44.000 --> 01:31:49.360]   that does haptics and can notify me of events, then suddenly I'm in an environment that's
[01:31:49.360 --> 01:31:55.160]   a lot more conducive to having pair of glasses on my face in a couple of years.
[01:31:55.160 --> 01:32:03.800]   Is it safe to say the only like real market for VR with the headset to what have you is
[01:32:03.800 --> 01:32:09.960]   going to be on the industrial industrial side of things more so than in the consumer entertainment
[01:32:09.960 --> 01:32:10.960]   side?
[01:32:10.960 --> 01:32:16.480]   I'm not even that Microsoft canned, um, hollow lens over a year ago.
[01:32:16.480 --> 01:32:18.760]   So, and they're really, I mean, I covered that.
[01:32:18.760 --> 01:32:23.600]   I had a hollow lens and there was just the, like every use case was just stretching it.
[01:32:23.600 --> 01:32:28.640]   Like there were really cool B things that could happen potentially and looked really
[01:32:28.640 --> 01:32:32.640]   neat but just weren't practical and weren't actually happening.
[01:32:32.640 --> 01:32:33.640]   Yeah.
[01:32:33.640 --> 01:32:38.720]   I mean, same could be said for Google Glass and the Enterprise edition, you know, because
[01:32:38.720 --> 01:32:41.600]   Google that was their pivot was okay.
[01:32:41.600 --> 01:32:46.160]   Well, this isn't really for consumers anymore, but, you know, the enterprise is going to
[01:32:46.160 --> 01:32:47.160]   come in really handy.
[01:32:47.160 --> 01:32:52.320]   And yes, there were occasional stories where they would highlight how it is being used in
[01:32:52.320 --> 01:32:56.680]   the enterprise, but I highly suspect that was not the norm.
[01:32:56.680 --> 01:33:01.000]   You know, that was definitely, you know, the exception to the rule.
[01:33:01.000 --> 01:33:02.000]   Yeah.
[01:33:02.000 --> 01:33:07.880]   And of course, running so far ahead of the reality on this site is terrible.
[01:33:07.880 --> 01:33:10.640]   But, you know, it's just not there yet, right?
[01:33:10.640 --> 01:33:15.480]   The, the ideas are, are farther, much further advanced than the capabilities.
[01:33:15.480 --> 01:33:16.480]   Yes.
[01:33:16.480 --> 01:33:21.360]   I tried the new HTC Vive headset, which really has the latest capabilities.
[01:33:21.360 --> 01:33:23.680]   This is all they do, you know, primarily.
[01:33:23.680 --> 01:33:26.280]   This is the main thing that they do.
[01:33:26.280 --> 01:33:27.720]   They're so focused on it.
[01:33:27.720 --> 01:33:30.720]   They've got the latest technology in this device.
[01:33:30.720 --> 01:33:33.200]   I tried it.
[01:33:33.200 --> 01:33:38.160]   I thought at the time there were some really interesting pieces of it.
[01:33:38.160 --> 01:33:39.840]   It's as far as it could get.
[01:33:39.840 --> 01:33:43.720]   But within 48 hours, I completely forgot about it was moved on to anything else and
[01:33:43.720 --> 01:33:46.000]   really haven't hardly thought about it since, right?
[01:33:46.000 --> 01:33:51.240]   There's just not enough in it for it to be that interesting right now.
[01:33:51.240 --> 01:33:55.720]   And I just have to believe that Apple sees that, that they have a, there's not an echo
[01:33:55.720 --> 01:34:01.160]   chamber there and they know that this is not, this is not good and there's not enough
[01:34:01.160 --> 01:34:03.920]   of it there there and we aren't going to release it.
[01:34:03.920 --> 01:34:05.920]   And I trust Mark Gurman sources.
[01:34:05.920 --> 01:34:10.480]   I think that, you know, Mark Gurman has been, you know, spot on many, many times about these
[01:34:10.480 --> 01:34:11.800]   kinds of things.
[01:34:11.800 --> 01:34:17.240]   But I, you know, we're still two months away from, or almost a month and a half away, a
[01:34:17.240 --> 01:34:21.360]   little over a month and a half away from WWDC, there's still plenty of time for Apple to
[01:34:21.360 --> 01:34:24.400]   pull the plug and say like, word, it's just not enough.
[01:34:24.400 --> 01:34:25.400]   For sure.
[01:34:25.400 --> 01:34:27.760]   And would not be surprised in any way if that happened.
[01:34:27.760 --> 01:34:28.760]   Yeah.
[01:34:28.760 --> 01:34:35.200]   He, he wrote that Apple is, you know, looking at this device as the future beyond iPhone
[01:34:35.200 --> 01:34:41.560]   and iPad, we've heard that before, but which is just, it's really hard for me to envision
[01:34:41.560 --> 01:34:47.160]   that, you know, both of those things are way more palatable for the everyday person.
[01:34:47.160 --> 01:34:53.240]   And if this, if, if you're looking for a device category that is, that begins with those as
[01:34:53.240 --> 01:34:58.800]   the example, we want the next one of these, it's got to be as consumer palatable as those
[01:34:58.800 --> 01:35:00.000]   things.
[01:35:00.000 --> 01:35:05.120]   And the reality is those things, you know, those iPhone, a smartphone fits very easily
[01:35:05.120 --> 01:35:06.120]   into your pocket.
[01:35:06.120 --> 01:35:12.440]   It really does not take much other than cost expense, you know, is the primary thing to
[01:35:12.440 --> 01:35:18.560]   use one of those and the success of the smartphone industry has, has bared that to be true.
[01:35:18.560 --> 01:35:21.600]   Obviously, because everybody has these things now.
[01:35:21.600 --> 01:35:25.000]   I just don't see it when it comes to anything that you're putting on your head.
[01:35:25.000 --> 01:35:29.520]   And maybe I'm short sighted, you know, uh, Mark Zuckerberg sure thought we were short
[01:35:29.520 --> 01:35:30.520]   sighted.
[01:35:30.520 --> 01:35:34.200]   And that's why he named the company meta and look where we're at right now.
[01:35:34.200 --> 01:35:37.960]   Like it really does seem like he's wearing egg on his face, you know, so.
[01:35:37.960 --> 01:35:40.520]   Two lawyers are working hard to roll that back.
[01:35:40.520 --> 01:35:41.520]   Oh boy.
[01:35:41.520 --> 01:35:42.520]   Now, right.
[01:35:42.520 --> 01:35:43.520]   Yeah.
[01:35:43.520 --> 01:35:45.400]   So I don't know.
[01:35:45.400 --> 01:35:46.400]   We'll see.
[01:35:46.400 --> 01:35:48.000]   But anyways, yeah.
[01:35:48.000 --> 01:35:54.120]   And I mean, according to his report also, um, the initial plan, according to Mark Gorman
[01:35:54.120 --> 01:35:59.360]   was that the headset was going to be introduced in March and sold publicly by September.
[01:35:59.360 --> 01:36:01.600]   Well, that obviously did not happen.
[01:36:01.600 --> 01:36:06.320]   Now it's to be introduced to WWDC sold by the end of the year.
[01:36:06.320 --> 01:36:11.400]   Even that I just feel like is just kind of if Apple's actually releasing this thing,
[01:36:11.400 --> 01:36:15.600]   they better know something we don't as far as all this stuff is concerned.
[01:36:15.600 --> 01:36:19.240]   Cause right now it really looks like a very misguided direction in my opinion.
[01:36:19.240 --> 01:36:22.640]   And, and this is coming from somebody I have the meta quest.
[01:36:22.640 --> 01:36:28.280]   I have the meta quest to, and I've spent a lot of time in them, but I've also spent a
[01:36:28.280 --> 01:36:33.200]   lot of time in the last six, eight, 10 months, I'd say, not in them.
[01:36:33.200 --> 01:36:38.280]   You know, it's like, I had this big moment, this big, I can, I can, you know, narrow down
[01:36:38.280 --> 01:36:43.040]   to like a solid year, I'd say year and a half where I was all about it.
[01:36:43.040 --> 01:36:46.280]   And then at some point it just became too much for me.
[01:36:46.280 --> 01:36:51.600]   Like when I go into that world, you know, I, I didn't use to, but now I don't necessarily
[01:36:51.600 --> 01:36:57.160]   feel 100%, like it's not like I need to throw up or something, but I don't feel good.
[01:36:57.160 --> 01:37:00.040]   You know, I don't feel perfect like I did before I put the headset on.
[01:37:00.040 --> 01:37:03.840]   That's enough of the attraction to be like, eh, I don't have the energy for it tonight.
[01:37:03.840 --> 01:37:04.840]   And so it doesn't happen.
[01:37:04.840 --> 01:37:05.840]   Yeah.
[01:37:05.840 --> 01:37:07.000]   So what does that all got that's different than that?
[01:37:07.000 --> 01:37:08.000]   I don't know.
[01:37:08.000 --> 01:37:09.000]   I don't know what they could have.
[01:37:09.000 --> 01:37:15.760]   We did a story at CBS News and we shot the entire story in Horizon Worlds.
[01:37:15.760 --> 01:37:21.440]   Like everything, we shot it just like you would a regular stand-up or a package, but
[01:37:21.440 --> 01:37:24.360]   everything was in Horizon Worlds and VR.
[01:37:24.360 --> 01:37:29.840]   And we even recreated the CBS News Broadcast Center and, you know, they were functioning
[01:37:29.840 --> 01:37:31.360]   camber angles and all that.
[01:37:31.360 --> 01:37:35.440]   And like Jason Howell, it came down to exactly that point.
[01:37:35.440 --> 01:37:38.880]   Like we would get on meetings and go, oh, still a zoom.
[01:37:38.880 --> 01:37:39.880]   Yeah.
[01:37:39.880 --> 01:37:42.600]   Like, oh man, like do I have to get that thing?
[01:37:42.600 --> 01:37:43.600]   I wear glasses.
[01:37:43.600 --> 01:37:47.640]   Like do I got to fit this to my face and then scratch my glasses and then go?
[01:37:47.640 --> 01:37:48.640]   And to what benefit?
[01:37:48.640 --> 01:37:54.520]   I mean, there is this idea of like extra presence and I would agree that when I'm standing
[01:37:54.520 --> 01:37:59.240]   in a room with someone talking to them, that feels better than sitting in front of a screen,
[01:37:59.240 --> 01:38:00.800]   talking to them on a screen.
[01:38:00.800 --> 01:38:06.240]   But I don't know that the standing in a room talking to someone experience is the same
[01:38:06.240 --> 01:38:12.640]   as standing in a virtual room talking to that same person in a virtual way, but realistic
[01:38:12.640 --> 01:38:14.080]   kind of, you know what I mean?
[01:38:14.080 --> 01:38:15.080]   They don't translate.
[01:38:15.080 --> 01:38:18.280]   I don't get the same benefit out of one that I do out of the other.
[01:38:18.280 --> 01:38:19.800]   So then why am I?
[01:38:19.800 --> 01:38:20.800]   I don't know.
[01:38:20.800 --> 01:38:27.040]   We've seen all these Star Trek and Star Wars holograms and we're like, we're almost there
[01:38:27.040 --> 01:38:28.040]   with this.
[01:38:28.040 --> 01:38:29.040]   Like, we're no, we're not.
[01:38:29.040 --> 01:38:30.040]   We're not even close.
[01:38:30.040 --> 01:38:33.840]   And the path there is rough.
[01:38:33.840 --> 01:38:37.480]   And so people are not going to be ready to spend money on it when they have that as their
[01:38:37.480 --> 01:38:41.960]   vision of what it is and could be, you know, when you get to the reality of what it is
[01:38:41.960 --> 01:38:46.400]   today, it just doesn't feel like it's really worth the time or effort.
[01:38:46.400 --> 01:38:47.400]   Yeah.
[01:38:47.400 --> 01:38:49.320]   What is the there there?
[01:38:49.320 --> 01:38:54.120]   We were, you know, this does, we were in a very similar spot with the Apple car, maybe
[01:38:54.120 --> 01:38:55.120]   a half dozen years ago.
[01:38:55.120 --> 01:38:56.120]   Yeah, that's a good point.
[01:38:56.120 --> 01:38:57.120]   You know, right?
[01:38:57.120 --> 01:39:01.640]   It's just the next quarter that we're going to announce it by the end of the year, right?
[01:39:01.640 --> 01:39:02.840]   There seem to be parallels.
[01:39:02.840 --> 01:39:04.760]   Yeah, I think you're absolutely right.
[01:39:04.760 --> 01:39:08.880]   So all of the heads that they've made already are right now they're getting packed into
[01:39:08.880 --> 01:39:14.320]   the trunk of the Apple car, you know, to an unknown location.
[01:39:14.320 --> 01:39:17.000]   Look, it's hard times for tech companies right now.
[01:39:17.000 --> 01:39:19.120]   You got to do what you got to do to save on costs.
[01:39:19.120 --> 01:39:20.960]   So I understand.
[01:39:20.960 --> 01:39:28.040]   Let's take a break and we'll get back and, you know, talk more awesome news stories with
[01:39:28.040 --> 01:39:30.200]   an awesome panel, Dan Patterson.
[01:39:30.200 --> 01:39:32.840]   So great to get the chance to podcast with you, man.
[01:39:32.840 --> 01:39:34.600]   I'm usually booking you for this show.
[01:39:34.600 --> 01:39:41.000]   So it's nice to be able to be on the same show with you, same for Jason Heiner, ZD net,
[01:39:41.000 --> 01:39:44.640]   same for my friend and coworker here at Twit and Pruitt.
[01:39:44.640 --> 01:39:46.400]   This is just a heck of a lot of fun.
[01:39:46.400 --> 01:39:48.640]   So I appreciate you guys being here.
[01:39:48.640 --> 01:39:50.600]   Let me.
[01:39:50.600 --> 01:39:55.600]   This show this episode of this weekend tech is brought to you by decisions.
[01:39:55.600 --> 01:40:01.320]   Decisions gives IT and business experts the tools to automate anything in your company.
[01:40:01.320 --> 01:40:02.320]   Automation.
[01:40:02.320 --> 01:40:04.280]   We've already talked about it a lot on this show, right?
[01:40:04.280 --> 01:40:10.000]   It's so empowering, but the cool thing about decisions, it's all within one no code platform,
[01:40:10.000 --> 01:40:11.160]   no code.
[01:40:11.160 --> 01:40:15.440]   And you can automate the things that you normally do and make your life easier, make
[01:40:15.440 --> 01:40:18.240]   the lives of your employees easier.
[01:40:18.240 --> 01:40:23.480]   This is proven to fix any business process and prepare you to withstand economic uncertainty
[01:40:23.480 --> 01:40:25.920]   in the process.
[01:40:25.920 --> 01:40:30.840]   Recession resilience, it requires a deliberate management of resources.
[01:40:30.840 --> 01:40:33.840]   And you also need the flexibility to adapt at a moment's notice.
[01:40:33.840 --> 01:40:39.120]   Well, the decisions, no code environment makes it easy for your team to collaborate, to build
[01:40:39.120 --> 01:40:46.480]   and adjust workflows, also dynamic forms and decisioning processes that fit your unique
[01:40:46.480 --> 01:40:48.200]   and ever-changing business needs.
[01:40:48.200 --> 01:40:53.360]   This is especially important with today's IT talent shortage.
[01:40:53.360 --> 01:40:55.840]   Decisions process automation software is a complete toolkit.
[01:40:55.840 --> 01:41:02.200]   It allows developers and business users alike to build applications and automations without
[01:41:02.200 --> 01:41:06.760]   requiring any code, like I said, and that's the magic of decisions.
[01:41:06.760 --> 01:41:08.960]   Their no code platform is powerful.
[01:41:08.960 --> 01:41:14.280]   It includes robust rules and workflow engines, a host of pre-built integrations to keep things
[01:41:14.280 --> 01:41:20.960]   easy for you, that you can connect to any legacy system via API, all within a simple drag
[01:41:20.960 --> 01:41:23.640]   and drop visual interface design.
[01:41:23.640 --> 01:41:24.880]   So it's just that simple.
[01:41:24.880 --> 01:41:30.720]   It can be deployed on-prem, it can be deployed in the cloud, if that's your preference.
[01:41:30.720 --> 01:41:36.720]   Companies, as you know, we were all caught flat-footed at the onset of the pandemic.
[01:41:36.720 --> 01:41:38.880]   It really took everybody by surprise.
[01:41:38.880 --> 01:41:42.560]   But decisions customers were fully equipped to respond.
[01:41:42.560 --> 01:41:48.000]   One of the country's largest private banks built an entire PPP loan application process
[01:41:48.000 --> 01:41:51.360]   for small businesses affected by COVID-19.
[01:41:51.360 --> 01:41:53.560]   All it took them was just two days to do it.
[01:41:53.560 --> 01:42:00.000]   They, in essence, were the first to market issuing $1 billion in loans before their competitors
[01:42:00.000 --> 01:42:01.000]   even started.
[01:42:01.000 --> 01:42:03.760]   That's the value being there first, right?
[01:42:03.760 --> 01:42:05.920]   Decisions made that happen.
[01:42:05.920 --> 01:42:12.520]   Decisions lets you customize workflows to automate the small decisions in your workplace
[01:42:12.520 --> 01:42:17.680]   producing faster results with greater accuracy, allowing your team to focus on the important
[01:42:17.680 --> 01:42:19.920]   decisions that they're making.
[01:42:19.920 --> 01:42:23.920]   You can scale your business from there to better serve your customers while reducing
[01:42:23.920 --> 01:42:27.960]   operational costs and saving your team valuable time.
[01:42:27.960 --> 01:42:29.480]   And there's a lot of examples.
[01:42:29.480 --> 01:42:33.560]   This is one excellent example of how decisions automation software can help.
[01:42:33.560 --> 01:42:37.360]   And you can notice it when you're riding in an Otis elevator.
[01:42:37.360 --> 01:42:43.160]   They implemented decisions to run daily pulse checks across their 2 million units operating
[01:42:43.160 --> 01:42:48.840]   globally by finding potential problems before they occur, which is the point they avoid
[01:42:48.840 --> 01:42:49.840]   downtime.
[01:42:49.840 --> 01:42:52.600]   They manage their service technicians efficiently.
[01:42:52.600 --> 01:42:57.320]   So when you're riding on an Otis elevator, you know that you're going to arrive safely
[01:42:57.320 --> 01:43:00.640]   as a result of the decisions on the back end.
[01:43:00.640 --> 01:43:05.200]   As a recession approaches, the durability of a business's foundation will directly impact
[01:43:05.200 --> 01:43:08.000]   its performance and its ability to survive.
[01:43:08.000 --> 01:43:13.000]   So it's important to ask yourself, how strong is your foundation?
[01:43:13.000 --> 01:43:18.480]   Well, decisions automation platform provides a solution to any business challenge.
[01:43:18.480 --> 01:43:20.800]   It automates anything.
[01:43:20.800 --> 01:43:25.560]   And that in essence changes everything for you to improve your company's speed to market,
[01:43:25.560 --> 01:43:28.920]   your financial growth and operational success.
[01:43:28.920 --> 01:43:33.240]   They help industry leaders alleviate bottlenecks and automate pain points in their business.
[01:43:33.240 --> 01:43:36.640]   So you can do what you do best and that's change the world.
[01:43:36.640 --> 01:43:43.320]   To learn more about decisions, no code automation platform and scope your free proof of concept,
[01:43:43.320 --> 01:43:47.680]   all you got to do is visit decisions.com/twit.
[01:43:47.680 --> 01:43:51.520]   That's decisions.com/twit.
[01:43:51.520 --> 01:43:57.400]   And we thank decisions for their support of this week in tech.
[01:43:57.400 --> 01:43:58.800]   Okay.
[01:43:58.800 --> 01:44:01.960]   What else do we have here?
[01:44:01.960 --> 01:44:07.200]   We've done the fun Apple rumor thing.
[01:44:07.200 --> 01:44:11.600]   I know you're going to like this one and the TikTok ban.
[01:44:11.600 --> 01:44:13.520]   Dump, dum, dum.
[01:44:13.520 --> 01:44:15.000]   Yeah, I'm sorry.
[01:44:15.000 --> 01:44:17.440]   I'm sorry I had to do this to you, but hey, it's in the news.
[01:44:17.440 --> 01:44:20.200]   So we should talk about it a little bit.
[01:44:20.200 --> 01:44:25.600]   So on this week in Google, we talked briefly about the story that was in here about progressive
[01:44:25.600 --> 01:44:31.560]   lawmakers starting to kind of change or there's a group of progressive lawmakers that are
[01:44:31.560 --> 01:44:35.080]   really pushing back on this idea of a TikTok ban.
[01:44:35.080 --> 01:44:40.640]   There's news though that came this week that in the state of Montana, this was actually
[01:44:40.640 --> 01:44:42.120]   just on Friday actually.
[01:44:42.120 --> 01:44:48.360]   So just a couple of days ago, they approved a first of its kind bill.
[01:44:48.360 --> 01:44:54.160]   The goal or the target is to ban TikTok across the state.
[01:44:54.160 --> 01:44:59.000]   So is this, I guess this would be the first state in the US to actually put this forward
[01:44:59.000 --> 01:45:04.520]   in a bill and to try and tamp down on a TikTok use in the state?
[01:45:04.520 --> 01:45:08.560]   I don't know what that means for someone if they're VPNing because I have a really hard
[01:45:08.560 --> 01:45:15.000]   time picturing the use of today not using their TikTok.
[01:45:15.000 --> 01:45:16.680]   It's so embedded right now.
[01:45:16.680 --> 01:45:19.520]   But what are your thoughts about this?
[01:45:19.520 --> 01:45:22.840]   Is this so far overblown?
[01:45:22.840 --> 01:45:25.600]   Is there something there?
[01:45:25.600 --> 01:45:28.320]   Is Montana like, do they know something that we don't know?
[01:45:28.320 --> 01:45:29.320]   What do you guys think?
[01:45:29.320 --> 01:45:32.400]   It feels like politics are not so much tech.
[01:45:32.400 --> 01:45:36.320]   I mean, China does similar things, right?
[01:45:36.320 --> 01:45:41.280]   They banned or put contingencies on the use of apps not built in China.
[01:45:41.280 --> 01:45:45.800]   And I mean, this push to, I don't know why I don't have any sources on this, but this
[01:45:45.800 --> 01:45:54.040]   push to ban TikTok feels more like a diplomatic as opposed to a tech issue.
[01:45:54.040 --> 01:46:01.040]   Having said that, I don't put TikTok on my phone.
[01:46:01.040 --> 01:46:02.480]   I don't put WhatsApp on my phone.
[01:46:02.480 --> 01:46:05.480]   But I try to not put social media on my phone in general.
[01:46:05.480 --> 01:46:06.480]   Yeah, that was my next question.
[01:46:06.480 --> 01:46:09.120]   Like how does that line up with other social media?
[01:46:09.120 --> 01:46:15.960]   Are you not putting TikTok on your phone because of the trust issue or is it just social media
[01:46:15.960 --> 01:46:21.400]   in general controlling the data that flows out of your phone in any way possible?
[01:46:21.400 --> 01:46:24.280]   More about the data flowing in.
[01:46:24.280 --> 01:46:26.640]   Look, it's interesting.
[01:46:26.640 --> 01:46:29.600]   I mean, I just don't need more brain bending social media.
[01:46:29.600 --> 01:46:30.600]   Okay.
[01:46:30.600 --> 01:46:34.840]   So it has nothing to do with like, what are they doing with my zeros and ones?
[01:46:34.840 --> 01:46:38.440]   And it's more just like, I want to simplify my life.
[01:46:38.440 --> 01:46:39.440]   I can show you.
[01:46:39.440 --> 01:46:40.440]   I do have those concerns.
[01:46:40.440 --> 01:46:44.560]   I think that there are concerns, especially about key logging and other like what they
[01:46:44.560 --> 01:46:47.920]   can see, like there are definitely concerns, but my concerns are personal.
[01:46:47.920 --> 01:46:52.880]   And that's that like, I think that exposure to social media is something that personal,
[01:46:52.880 --> 01:46:57.680]   like I want to increase my exposure to books and decrease my exposure to social media.
[01:46:57.680 --> 01:46:59.320]   And that's what I try to do.
[01:46:59.320 --> 01:47:03.000]   You know, I'm glad Mr. Patterson is on here.
[01:47:03.000 --> 01:47:07.960]   You know, I know he has gone to different tech events and whatnot all around the world,
[01:47:07.960 --> 01:47:13.520]   including going into, you know, across the pond, if you will, for an event.
[01:47:13.520 --> 01:47:17.840]   And I know as his friend, he has taken certain security measures.
[01:47:17.840 --> 01:47:20.680]   I'm not going to say what they are, but he has taken certain security measures when
[01:47:20.680 --> 01:47:24.480]   he's gone to these events because he's wanting to make sure things are squared away with
[01:47:24.480 --> 01:47:25.680]   his privacy and so forth.
[01:47:25.680 --> 01:47:30.320]   And I've always applied his efforts for that stuff.
[01:47:30.320 --> 01:47:36.480]   But then when stores like this come out and just as he said, this is sounds more political
[01:47:36.480 --> 01:47:38.400]   than tech here.
[01:47:38.400 --> 01:47:45.640]   It pisses me off because TikTok can scrape and be a so-called threat to us from a privacy
[01:47:45.640 --> 01:47:47.600]   stamp on or security stamp.
[01:47:47.600 --> 01:47:53.840]   Well, what have you just as much as any of these homegrown apps here in the US can?
[01:47:53.840 --> 01:47:58.640]   You know, we've had, I don't know, maybe 20 minutes ago where everybody in the country
[01:47:58.640 --> 01:48:06.480]   was talking about just how bad social media was for children and making the children depressed
[01:48:06.480 --> 01:48:11.720]   and wanting to commit suicide or lead into violence because of something they saw on
[01:48:11.720 --> 01:48:15.960]   Instagram or because of something they saw on Facebook or what have you.
[01:48:15.960 --> 01:48:21.560]   But there was never any mentions then of we need to ban Facebook and Instagram, you know.
[01:48:21.560 --> 01:48:26.720]   So it seems really, really political to just sort of single out TikTok when we could say
[01:48:26.720 --> 01:48:30.920]   the same stuff about our very own homegrown apps here.
[01:48:30.920 --> 01:48:33.440]   And likely those apps have them.
[01:48:33.440 --> 01:48:40.040]   I don't want them popping in and saying, you know what, Mr. Pruitt, your son is under
[01:48:40.040 --> 01:48:41.040]   18.
[01:48:41.040 --> 01:48:44.440]   He cannot use Instagram.
[01:48:44.440 --> 01:48:48.720]   And I'm going to tell the government where to stick it because he's my child.
[01:48:48.720 --> 01:48:49.720]   Let me manage that.
[01:48:49.720 --> 01:48:54.320]   And when I hear these different policies, I'm always thinking, okay, I know there are
[01:48:54.320 --> 01:49:01.120]   bad parents out there, but parents can step in and help manage these issues with social
[01:49:01.120 --> 01:49:04.440]   media, in my opinion.
[01:49:04.440 --> 01:49:06.280]   Jason, you know, I worked for Jason.
[01:49:06.280 --> 01:49:12.080]   I've reported from from Darfur from Cairo, the UAE, Kenya.
[01:49:12.080 --> 01:49:16.960]   But Jason, I worked for you when I went to a keve in 2017.
[01:49:16.960 --> 01:49:22.920]   And I mean, I was in retrospect, I'm like, I should have just like breathe.
[01:49:22.920 --> 01:49:25.600]   But like, that was terrifying.
[01:49:25.600 --> 01:49:28.080]   I mean, we the group, I'm not important.
[01:49:28.080 --> 01:49:32.240]   I'm just a schmuck journalist, but the secretary of state was with us and there are other high
[01:49:32.240 --> 01:49:33.320]   level diplomats.
[01:49:33.320 --> 01:49:38.120]   And we were followed by, I'm followed, like literally followed by people who were, I
[01:49:38.120 --> 01:49:40.680]   mean, just thugs.
[01:49:40.680 --> 01:49:45.080]   We were staying right next to FSB and we were live streaming.
[01:49:45.080 --> 01:49:47.400]   This is the Global Cybersecurity Summit.
[01:49:47.400 --> 01:49:50.440]   And they were speaking on cyber policy, high level diplomats.
[01:49:50.440 --> 01:49:57.400]   And so again, I'm a schmuck, but everybody coming out of there, like all of our phones
[01:49:57.400 --> 01:49:59.680]   were being monitored.
[01:49:59.680 --> 01:50:07.040]   And Jason, you know, I used signal, I talked to the team back home, but I took every app
[01:50:07.040 --> 01:50:09.520]   off of my phone that wasn't signal.
[01:50:09.520 --> 01:50:15.360]   And when I spent a couple of weeks in Cairo, training Sudanese migrants, how to use encryption
[01:50:15.360 --> 01:50:17.520]   and mobile devices, I did the same thing.
[01:50:17.520 --> 01:50:22.320]   I took every app off my phone because your data is in that case was being monitored by
[01:50:22.320 --> 01:50:23.320]   the state.
[01:50:23.320 --> 01:50:29.120]   The data that streams out of your app, apps on your phone is the snow joke.
[01:50:29.120 --> 01:50:30.120]   It's no joke.
[01:50:30.120 --> 01:50:34.240]   We should be concerned about data privacy, but to your point, and we should probably be
[01:50:34.240 --> 01:50:39.120]   concerned about those homegrown apps as much as TikTok.
[01:50:39.120 --> 01:50:43.280]   And look, this, it just feels like- >> Quick grandstand and all of them are a miss.
[01:50:43.280 --> 01:50:44.280]   >> Right.
[01:50:44.280 --> 01:50:50.320]   It just feels like maybe this is a little more about diplomacy than it is the app, but
[01:50:50.320 --> 01:50:52.320]   I don't know.
[01:50:52.320 --> 01:50:58.480]   You know, there are not a lot of smart things that have come out of the debate about this
[01:50:58.480 --> 01:51:00.320]   to be clear.
[01:51:00.320 --> 01:51:08.040]   But I did see one, not here, but just in the larger TikTok ban conversation.
[01:51:08.040 --> 01:51:10.400]   I'm sorry to say that.
[01:51:10.400 --> 01:51:16.160]   But one of the smartest things, I did see one senator whose name I'm now forgetting
[01:51:16.160 --> 01:51:22.280]   apologies, who said, you know, the thing is, some of this is just all-in-law.
[01:51:22.280 --> 01:51:28.040]   All visuals, like we could say, okay, TikTok, you have to put your data in Texas, right?
[01:51:28.040 --> 01:51:33.000]   They have this movement where they're gonna like, no, no, we'll store all of the data
[01:51:33.000 --> 01:51:34.080]   in Texas.
[01:51:34.080 --> 01:51:39.080]   And so, and we'll somehow make ourselves feel better that the Chinese government can't,
[01:51:39.080 --> 01:51:41.440]   you know, get at it is easy in Texas.
[01:51:41.440 --> 01:51:47.520]   He says, look, the Chinese government has shown no restraint, whether it's in China, whether
[01:51:47.520 --> 01:51:52.240]   it's in Texas, whether it's in Europe, that they will break in.
[01:51:52.240 --> 01:51:56.960]   Yeah, the senator says, they will, they will try to break in and I'm not confident that
[01:51:56.960 --> 01:52:00.640]   they're not in any of it, you know, at any time that they want.
[01:52:00.640 --> 01:52:03.920]   And they've shown no compunction to have any restraint.
[01:52:03.920 --> 01:52:07.680]   So why would we, you know, we're fooling ourselves that we think that we're gonna be
[01:52:07.680 --> 01:52:10.000]   safer by making them store their data in Texas.
[01:52:10.000 --> 01:52:14.700]   I thought that was actually probably the smartest thing anybody has said in this whole
[01:52:14.700 --> 01:52:18.600]   debate of like, a lot of this is optics, right?
[01:52:18.600 --> 01:52:24.520]   We're trying to make ourselves feel better that the Chinese government not having access
[01:52:24.520 --> 01:52:30.480]   to American data by not allowing TikTok to operate in the United States, we're fooling
[01:52:30.480 --> 01:52:31.480]   ourselves.
[01:52:31.480 --> 01:52:32.480]   It's not gonna solve anything.
[01:52:32.480 --> 01:52:36.440]   I'll add one other thing about social media because I'm also gonna admit that when it comes
[01:52:36.440 --> 01:52:42.360]   to social media, I also have done, I've pulled it back on a lot of it.
[01:52:42.360 --> 01:52:43.960]   I used to do a ton of it.
[01:52:43.960 --> 01:52:50.000]   Dan and I met on social media originally, like 2008, 2009, and then Dan and I did a podcast
[01:52:50.000 --> 01:52:54.640]   in 2018 that I think you can still find out there.
[01:52:54.640 --> 01:52:59.520]   But in 2018, we both had, we both said we were deleting Facebook and Instagram off of
[01:52:59.520 --> 01:53:00.960]   our phones.
[01:53:00.960 --> 01:53:03.760]   We were gonna stop using a lot of Twitter.
[01:53:03.760 --> 01:53:09.120]   And the funny thing is, Dan and I both said, we had this goal because we were using it all
[01:53:09.120 --> 01:53:10.120]   the time every day.
[01:53:10.120 --> 01:53:11.120]   It was part of our job.
[01:53:11.120 --> 01:53:12.720]   We're using hours and hours.
[01:53:12.720 --> 01:53:16.840]   And when you started, I think we had had this conversation before the podcast episode that
[01:53:16.840 --> 01:53:20.720]   when you start adding that up, you realize how much of your life you're advocating to
[01:53:20.720 --> 01:53:21.720]   it.
[01:53:21.720 --> 01:53:25.840]   And it got to the point where it was like, this is a lot of time that's just not, there's
[01:53:25.840 --> 01:53:27.560]   not a good ROI on our time.
[01:53:27.560 --> 01:53:30.680]   So I remember saying on that podcast, I was gonna do one thing for a year.
[01:53:30.680 --> 01:53:35.000]   I was gonna pretty much go semi-retired on social media.
[01:53:35.000 --> 01:53:36.480]   And I was gonna do two things.
[01:53:36.480 --> 01:53:39.760]   I was gonna read more books because I also realized I hadn't read a book in a year at
[01:53:39.760 --> 01:53:44.520]   the time I hadn't read a complete book in one year.
[01:53:44.520 --> 01:53:49.640]   And then I also was, I was using Facebook as a way to keep up with people.
[01:53:49.640 --> 01:53:51.840]   And I was not actually having phone calls with people.
[01:53:51.840 --> 01:53:53.880]   I was not having coffee or lunch.
[01:53:53.880 --> 01:53:57.720]   So I said, in this one year, I'm gonna read books and I'm gonna have real life.
[01:53:57.720 --> 01:54:01.720]   I'm gonna call people and I'm gonna go out and have coffee and lunch with just, you know,
[01:54:01.720 --> 01:54:06.360]   the people that I have the access to do that when I travel and at home.
[01:54:06.360 --> 01:54:10.520]   My life was so much better in that one year than I said it was gonna be a year.
[01:54:10.520 --> 01:54:14.200]   And then I sort of forgot about, you know, whenever the date was that I was supposed
[01:54:14.200 --> 01:54:17.480]   to like go off of this sabbatical, this social media sabbatical.
[01:54:17.480 --> 01:54:22.280]   Because I was like, my life, I'm never going back to, you know, spending hours and hours
[01:54:22.280 --> 01:54:23.720]   a day on social media.
[01:54:23.720 --> 01:54:27.920]   It's just the ROI for that, for my mental health and the quality of life is just not
[01:54:27.920 --> 01:54:28.920]   good enough.
[01:54:28.920 --> 01:54:31.240]   So is that still a constant now at this point?
[01:54:31.240 --> 01:54:32.240]   The same thing.
[01:54:32.240 --> 01:54:33.240]   Yeah.
[01:54:33.240 --> 01:54:36.720]   As you were explaining, I was like, and then after a year, I'm going right back.
[01:54:36.720 --> 01:54:39.760]   But I'm happy that was my day.
[01:54:39.760 --> 01:54:43.680]   I deleted Facebook, not just the app from my phone, but I deleted my Facebook, Instagram
[01:54:43.680 --> 01:54:46.000]   and WhatsApp accounts.
[01:54:46.000 --> 01:54:51.800]   I had to make WhatsApp to talk to some sources a couple months ago, but I've since deleted
[01:54:51.800 --> 01:54:52.800]   it.
[01:54:52.800 --> 01:54:53.800]   Yeah.
[01:54:53.800 --> 01:54:55.240]   Dan, what a step further than I did.
[01:54:55.240 --> 01:54:58.600]   I didn't delete them, but kudos to Dan for calling.
[01:54:58.600 --> 01:55:02.560]   Well, that was, yeah, I just felt like Facebook was a net negative.
[01:55:02.560 --> 01:55:05.760]   And my slash blade on it was pretty high.
[01:55:05.760 --> 01:55:10.200]   Twitter was a net positive for a long time.
[01:55:10.200 --> 01:55:12.520]   And it could be again.
[01:55:12.520 --> 01:55:13.520]   But right.
[01:55:13.520 --> 01:55:15.200]   I mean, anytime I'm a gamer.
[01:55:15.200 --> 01:55:21.360]   So like, if you do it in most major video games, if you pull up your console and you
[01:55:21.360 --> 01:55:27.560]   do slash played like a slash and type played, it'll tell you like World of Warcraft 14 days,
[01:55:27.560 --> 01:55:28.560]   right?
[01:55:28.560 --> 01:55:30.240]   But what is your slash played on social media?
[01:55:30.240 --> 01:55:31.240]   Oh, God.
[01:55:31.240 --> 01:55:34.280]   Do we actually want to know the answer to that question?
[01:55:34.280 --> 01:55:37.040]   It would probably, yeah, probably make it real bad.
[01:55:37.040 --> 01:55:38.680]   And what's the ROI to Jason's point?
[01:55:38.680 --> 01:55:43.240]   Like, like in Twitter, you know, the ROI was pretty, pretty big, pretty substantial.
[01:55:43.240 --> 01:55:49.400]   Like it was absolutely a benefit, but all the other social was like, and even, even
[01:55:49.400 --> 01:55:52.200]   those ads, like it pulling them off my phone.
[01:55:52.200 --> 01:55:53.200]   It sounds silly.
[01:55:53.200 --> 01:55:58.080]   I know people have strong opinions about Amazon and I'm not raising an Amazon debate,
[01:55:58.080 --> 01:56:01.760]   but like I bought a Kindle late last year.
[01:56:01.760 --> 01:56:03.680]   And I mean, it changed my life.
[01:56:03.680 --> 01:56:06.200]   I read, like, I don't touch my phone in bed.
[01:56:06.200 --> 01:56:09.440]   I read every single night for hours.
[01:56:09.440 --> 01:56:11.600]   It's it is the best device.
[01:56:11.600 --> 01:56:15.600]   I would get rid of all of my other devices if I could keep my Kindle.
[01:56:15.600 --> 01:56:18.400]   You know, no flip phone is all you need.
[01:56:18.400 --> 01:56:19.400]   Yeah.
[01:56:19.400 --> 01:56:21.240]   I'm just saying, wait a minute.
[01:56:21.240 --> 01:56:22.240]   Flip phone.
[01:56:22.240 --> 01:56:26.360]   There's a big movement back into flip phones right now.
[01:56:26.360 --> 01:56:28.400]   I was talking about the Motorola Razr.
[01:56:28.400 --> 01:56:29.400]   Okay.
[01:56:29.400 --> 01:56:30.400]   Yeah.
[01:56:30.400 --> 01:56:31.400]   It's great.
[01:56:31.400 --> 01:56:37.840]   The Kindle though, it really is true because it's because of not having that backlight and
[01:56:37.840 --> 01:56:40.640]   also not having any other apps on it.
[01:56:40.640 --> 01:56:45.640]   When I when I did say I was going to go off of, you know, social and spend more time reading,
[01:56:45.640 --> 01:56:46.640]   I did that.
[01:56:46.640 --> 01:56:48.240]   I bought a Kindle as a Kindle Oasis.
[01:56:48.240 --> 01:56:52.560]   It's still the one five years later that I'm reading on today.
[01:56:52.560 --> 01:56:56.280]   And I do I would do a slash plate on that.
[01:56:56.280 --> 01:57:00.640]   And there kind of is a version where you can see here are the books you read so far this
[01:57:00.640 --> 01:57:01.640]   year.
[01:57:01.640 --> 01:57:02.840]   Here's your goal.
[01:57:02.840 --> 01:57:07.200]   And it's kind of it's kind of nice because that then I sort of see I feel like there's
[01:57:07.200 --> 01:57:13.760]   an ROI right there of like I'm or at least I'm fully myself to say like I I deal.
[01:57:13.760 --> 01:57:17.720]   I like the ROI there in terms of reading.
[01:57:17.720 --> 01:57:18.840]   And I also listen to some things.
[01:57:18.840 --> 01:57:21.880]   There's some good other apps.
[01:57:21.880 --> 01:57:27.360]   One of the things that I really like a great trend is the apps like Blink and some others
[01:57:27.360 --> 01:57:32.960]   that do summaries of books because then you can you can get the you can get the summaries
[01:57:32.960 --> 01:57:34.400]   of a lot of things.
[01:57:34.400 --> 01:57:37.840]   And then if you're really into them because most books have one, two, maybe three good
[01:57:37.840 --> 01:57:40.480]   ideas in them and the rest is all evidence.
[01:57:40.480 --> 01:57:45.480]   But if you can get that 15 to 20 minute summary of books, then you can decide, I'd like to
[01:57:45.480 --> 01:57:49.760]   go and either read that or listen to the audio book or whatever.
[01:57:49.760 --> 01:57:54.360]   So I found that to be another thing where the ROI on that feels really, really high.
[01:57:54.360 --> 01:57:58.760]   There's like three or four of these apps now that do that that summarize books into like
[01:57:58.760 --> 01:58:01.800]   15 to 20 minute little snippets.
[01:58:01.800 --> 01:58:04.040]   And I find that another life hacks.
[01:58:04.040 --> 01:58:05.600]   Sorry, I know we're not talking life hacks here.
[01:58:05.600 --> 01:58:06.600]   So back to you.
[01:58:06.600 --> 01:58:13.720]   I think we are because apparently we moved on from from TikTok because I mean it sounds
[01:58:13.720 --> 01:58:19.760]   like we're kind of all in somewhat agreement that maybe this is a little silly and definitely
[01:58:19.760 --> 01:58:22.480]   more political than it is actual technology.
[01:58:22.480 --> 01:58:27.400]   But I appreciate where the conversation has headed because I think I think the wellness
[01:58:27.400 --> 01:58:32.320]   aspect around how we're using technology, there was this movement, you know, four or
[01:58:32.320 --> 01:58:37.240]   five years ago where, you know, Google and Apple were putting all these wellness features
[01:58:37.240 --> 01:58:40.840]   into their phone and there was, you know, a little bit there were the people that that
[01:58:40.840 --> 01:58:44.200]   appreciated having those controls and then the people that were like, don't tell me
[01:58:44.200 --> 01:58:45.480]   how to use my technology.
[01:58:45.480 --> 01:58:50.440]   I have noticed for myself that probably about five or six.
[01:58:50.440 --> 01:58:51.920]   I can't remember when it was exactly.
[01:58:51.920 --> 01:58:54.280]   I want to say it was like five or six years ago.
[01:58:54.280 --> 01:58:58.040]   I left Facebook entirely deleted my account.
[01:58:58.040 --> 01:59:04.760]   It was just I was I realized that when I used Facebook, I came out the other end having,
[01:59:04.760 --> 01:59:07.840]   you know, spend all that time there and I didn't feel better.
[01:59:07.840 --> 01:59:08.840]   I felt worse.
[01:59:08.840 --> 01:59:14.520]   You know, it was just this idea that like I'm subjecting myself to this experience that
[01:59:14.520 --> 01:59:19.560]   on the other side of it, you know, part of it was just it was a very politically heated
[01:59:19.560 --> 01:59:21.840]   moment in time.
[01:59:21.840 --> 01:59:27.760]   And you know, it was just like, I don't why am I spending my time feeling bad?
[01:59:27.760 --> 01:59:29.520]   What would happen if I just removed it?
[01:59:29.520 --> 01:59:34.360]   And you know, a lot of things improved in my life, but but I'm also conflicted because
[01:59:34.360 --> 01:59:39.560]   about a year ago, I signed up for a new Facebook account and came back, not because I wanted
[01:59:39.560 --> 01:59:43.520]   to be exposed to political anything.
[01:59:43.520 --> 01:59:45.680]   I haven't, you know, engaged with anything like that.
[01:59:45.680 --> 01:59:46.680]   Honestly, I don't see it.
[01:59:46.680 --> 01:59:52.120]   So I'm obviously doing something right, but just because I did feel Jason, like I can
[01:59:52.120 --> 01:59:57.760]   appreciate and respect that you were able to remove yourself and maintain all of those
[01:59:57.760 --> 02:00:01.320]   relationships in on a phone call or a sit down or whatever.
[02:00:01.320 --> 02:00:06.920]   I do way more of that now than I did, you know, five or six years ago, no question, but there's
[02:00:06.920 --> 02:00:11.440]   still like a lot of life that I don't see out of the people that I care about.
[02:00:11.440 --> 02:00:16.520]   And that seems to be the only way that I can actually get that connection and just, you
[02:00:16.520 --> 02:00:19.280]   know, to understand what's going on in people's lives.
[02:00:19.280 --> 02:00:23.160]   So my approach to it now after having that distance is very different.
[02:00:23.160 --> 02:00:27.960]   I probably only opened the app a couple of times a week, maybe two times a week.
[02:00:27.960 --> 02:00:32.040]   And when I do, it's very specific, like, what are my friends up to right now?
[02:00:32.040 --> 02:00:33.600]   It's just the good stuff.
[02:00:33.600 --> 02:00:35.120]   And I'm okay with that.
[02:00:35.120 --> 02:00:37.120]   But I need more to mouth for that.
[02:00:37.120 --> 02:00:38.120]   You know, transition.
[02:00:38.120 --> 02:00:39.840]   My wife is still on Facebook.
[02:00:39.840 --> 02:00:45.520]   So my wife tells me when something interesting that I would like to log in or she'll just
[02:00:45.520 --> 02:00:47.200]   screenshot it and text it to me.
[02:00:47.200 --> 02:00:49.880]   Like, here's their life event.
[02:00:49.880 --> 02:00:57.840]   And so, so, yeah, I'm probably, I use Twitter.
[02:00:57.840 --> 02:01:02.160]   But I'm not going to necessarily say I'm on Twitter because I don't respond as much as
[02:01:02.160 --> 02:01:03.160]   I used to.
[02:01:03.160 --> 02:01:04.160]   Yeah.
[02:01:04.160 --> 02:01:11.600]   Even back before the acquisition, I noticed a change in Twitter in my Twitter experience
[02:01:11.600 --> 02:01:15.200]   because I'd opened it up where this in the browser on my phone.
[02:01:15.200 --> 02:01:20.920]   And the fuss for most people was the stuff that was being fed to them was really, you
[02:01:20.920 --> 02:01:24.880]   know, people that are not following and what have you, you know, just like this is none
[02:01:24.880 --> 02:01:26.920]   of this matters to me.
[02:01:26.920 --> 02:01:28.760]   My feed wasn't necessarily like that.
[02:01:28.760 --> 02:01:32.240]   It was a lot of the people that I follow.
[02:01:32.240 --> 02:01:41.440]   But what was being served was the anger and hate, frustration from the people that I follow.
[02:01:41.440 --> 02:01:44.840]   And I got tired of seeing that because it was just depressing, you know.
[02:01:44.840 --> 02:01:51.000]   I didn't get to see all of the viral videos of people having these weird fights in the
[02:01:51.000 --> 02:01:52.200]   street and stuff like that.
[02:01:52.200 --> 02:01:54.200]   Fortunately, I didn't see most of that stuff.
[02:01:54.200 --> 02:01:56.560]   But what I did see was still a bit of a downer.
[02:01:56.560 --> 02:01:57.560]   And I was like, you know what?
[02:01:57.560 --> 02:02:00.000]   I'm just going to go into broadcast mode.
[02:02:00.000 --> 02:02:04.800]   And so for me, if you look at my feed, a lot of myself is automated.
[02:02:04.800 --> 02:02:09.360]   It's just going out all day long because I'm a broadcaster.
[02:02:09.360 --> 02:02:13.440]   And if this, if Twitter is going to get me one more listener to hands on photography,
[02:02:13.440 --> 02:02:18.440]   if it's going to get me one more person to order a print and prove it dot com slash prints,
[02:02:18.440 --> 02:02:21.920]   I'm fine with that.
[02:02:21.920 --> 02:02:24.520]   For the most part, it's just straight up broadcast mode.
[02:02:24.520 --> 02:02:29.160]   And every now and then I go and answer a reply, but not often.
[02:02:29.160 --> 02:02:30.160]   Mm hmm.
[02:02:30.160 --> 02:02:31.160]   Yep.
[02:02:31.160 --> 02:02:33.400]   I totally agree.
[02:02:33.400 --> 02:02:36.160]   My participation has gone down.
[02:02:36.160 --> 02:02:38.160]   And I think I'm okay with it.
[02:02:38.160 --> 02:02:39.160]   Yeah.
[02:02:39.160 --> 02:02:40.160]   And I'm fine.
[02:02:40.160 --> 02:02:45.280]   And even with Instagram, yeah, Instagram has changed for me too.
[02:02:45.280 --> 02:02:51.320]   Now that I think about it, Twitter and Instagram have changed changed again since about November
[02:02:51.320 --> 02:02:52.560]   of last year.
[02:02:52.560 --> 02:02:57.480]   I went into full broadcast campaign mode for my son.
[02:02:57.480 --> 02:03:01.120]   So now if you look at Instagram, I'll look at Twitter, yeah, you'll see some things about
[02:03:01.120 --> 02:03:03.000]   Twitter in there and hands on photography.
[02:03:03.000 --> 02:03:08.120]   But most of the time you're going to see my son in there because I'm just trying to push
[02:03:08.120 --> 02:03:11.720]   him and promote him because I think he's getting screwed when it comes to these dad
[02:03:11.720 --> 02:03:13.480]   gumcologists out there.
[02:03:13.480 --> 02:03:16.280]   But that's another rant for another day.
[02:03:16.280 --> 02:03:20.720]   But yeah, either way, I'm, I use the platforms, but I'm not on the platforms.
[02:03:20.720 --> 02:03:21.720]   Yeah.
[02:03:21.720 --> 02:03:22.720]   I understand.
[02:03:22.720 --> 02:03:23.720]   Instagram and TikTok.
[02:03:23.720 --> 02:03:26.720]   Do you, do you put yourself out for your son on Instagram and Tiam?
[02:03:26.720 --> 02:03:27.720]   Oh, yeah.
[02:03:27.720 --> 02:03:29.240]   And my TikTok has my son on it.
[02:03:29.240 --> 02:03:32.120]   It's, it's not me on there.
[02:03:32.120 --> 02:03:33.120]   It's, it's him.
[02:03:33.120 --> 02:03:34.120]   Yeah.
[02:03:34.120 --> 02:03:35.880]   Does your son have a TikTok account?
[02:03:35.880 --> 02:03:37.320]   No, he does not.
[02:03:37.320 --> 02:03:38.320]   No, he does not.
[02:03:38.320 --> 02:03:40.120]   He is totally curious.
[02:03:40.120 --> 02:03:45.560]   You don't have the answer, but is your son interested in social the way you actually has
[02:03:45.560 --> 02:03:49.800]   two Instagram accounts because he's a, he's a creator himself.
[02:03:49.800 --> 02:03:53.760]   He has a private Instagram account that I don't, I don't have access to, but I don't
[02:03:53.760 --> 02:03:55.120]   care because I trust him.
[02:03:55.120 --> 02:03:57.800]   You know, yes, he's a turd sometimes, but I trust him.
[02:03:57.800 --> 02:04:01.600]   I know he's not going to be doing crazy stuff.
[02:04:01.600 --> 02:04:07.360]   But he has another, his public account where, you know, he does this photography and because
[02:04:07.360 --> 02:04:12.240]   he likes to shoot and he also likes to model and he likes to sew because he's in a fashion
[02:04:12.240 --> 02:04:13.240]   and stuff.
[02:04:13.240 --> 02:04:15.960]   So he, he, he, he does put stuff out there.
[02:04:15.960 --> 02:04:16.960]   All right.
[02:04:16.960 --> 02:04:17.960]   Yeah.
[02:04:17.960 --> 02:04:18.960]   Very cool.
[02:04:18.960 --> 02:04:26.480]   There are ways to use these things that aren't soul crushing thing, you know, experiences.
[02:04:26.480 --> 02:04:28.640]   It's important to remember that.
[02:04:28.640 --> 02:04:33.520]   Let's take quick break and then when we come back from the break, got a couple of like
[02:04:33.520 --> 02:04:37.960]   little bits of stories here and there to round things out with.
[02:04:37.960 --> 02:04:39.280]   But let's take a moment.
[02:04:39.280 --> 02:04:43.160]   Thank the sponsor of this episode of This Week in Tech and that is Express VPN.
[02:04:43.160 --> 02:04:46.600]   I'm a huge fan of Express VPN.
[02:04:46.600 --> 02:04:54.160]   I've been on my phone, had on my computer for years profiling surveillance data harvesting.
[02:04:54.160 --> 02:04:57.200]   Some of this stuff probably sounds pretty familiar based on what we've been talking
[02:04:57.200 --> 02:04:58.200]   about today.
[02:04:58.200 --> 02:05:01.840]   These are a lot of things not to like about the tech giants, right?
[02:05:01.840 --> 02:05:06.120]   But what can you actually do about it when you rely on so many other products?
[02:05:06.120 --> 02:05:10.760]   We don't all have $44 billion to go buying Twitter.
[02:05:10.760 --> 02:05:16.080]   But the good news is you don't have to be a billionaire to take a stand.
[02:05:16.080 --> 02:05:22.200]   I use Express VPN on all my devices, like I said, for less than $7 a month.
[02:05:22.200 --> 02:05:28.240]   You can join me and fight back against all of this data harvesting big tech by using
[02:05:28.240 --> 02:05:30.080]   Express VPN.
[02:05:30.080 --> 02:05:35.760]   And yeah, I mean, you know, of course, the easy example here is you go to a public space,
[02:05:35.760 --> 02:05:41.920]   public Wi-Fi, coffee shop, you know, you're logging on to the Wi-Fi there, prime example
[02:05:41.920 --> 02:05:44.080]   of where you can use VPN.
[02:05:44.080 --> 02:05:50.520]   And if you've ever used VPN before in any public space, you may have noticed like a slow down
[02:05:50.520 --> 02:05:51.920]   in what you're getting.
[02:05:51.920 --> 02:05:55.400]   It's like, oh, well, I turn on the VPN and things just don't seem as snappy.
[02:05:55.400 --> 02:05:58.960]   I mean, that's one of the things that I think I love most about Express VPN is I turn on
[02:05:58.960 --> 02:05:59.960]   that VPN.
[02:05:59.960 --> 02:06:01.800]   And I don't notice a difference.
[02:06:01.800 --> 02:06:04.680]   Like I just don't notice a difference.
[02:06:04.680 --> 02:06:05.680]   It's awesome.
[02:06:05.680 --> 02:06:10.120]   They've got, you know, different different points of presence across the world, across
[02:06:10.120 --> 02:06:11.120]   the map.
[02:06:11.120 --> 02:06:15.840]   So you probably have one very close to you and that's going to keep things going fast.
[02:06:15.840 --> 02:06:18.680]   It's a pretty impressive experience.
[02:06:18.680 --> 02:06:21.120]   How do you think big tech companies make all their money?
[02:06:21.120 --> 02:06:26.200]   They do it by tracking your searches, your video history, everything you click on and
[02:06:26.200 --> 02:06:28.960]   then selling your personal data.
[02:06:28.960 --> 02:06:34.520]   Express VPN helps you anonymize much of your online presence by hiding your IP address,
[02:06:34.520 --> 02:06:41.040]   which if you are to where you probably are, but it's a unique identifier for every device.
[02:06:41.040 --> 02:06:46.200]   This allows big tech to match your activity back to you, right?
[02:06:46.200 --> 02:06:49.880]   It's a very specific target targeted just to you.
[02:06:49.880 --> 02:06:54.400]   And the best part here is how easy it is to use the Express VPN app.
[02:06:54.400 --> 02:06:55.720]   Like I said, I push the button.
[02:06:55.720 --> 02:06:57.920]   It's literally a simple tap.
[02:06:57.920 --> 02:07:03.120]   I just tap one button on my phone, one button on my computer, and I turn it on.
[02:07:03.120 --> 02:07:06.840]   And that's all it takes to keep people out of my business.
[02:07:06.840 --> 02:07:10.840]   If you don't like big tech tracking you and selling your data, your personal data for
[02:07:10.840 --> 02:07:13.440]   profit, it's time to fight back.
[02:07:13.440 --> 02:07:16.440]   Visit ExpressVPN.com/twit.
[02:07:16.440 --> 02:07:20.720]   Right now you'll get three months of ExpressVPN for free.
[02:07:20.720 --> 02:07:23.320]   That's ExpressVPN.com/twit.
[02:07:23.320 --> 02:07:29.000]   One more time for the people in the back row, ExpressVPN.com/twit.
[02:07:29.000 --> 02:07:30.560]   You'll be happy you did it.
[02:07:30.560 --> 02:07:32.600]   I'm telling you it's the VPN to use.
[02:07:32.600 --> 02:07:35.200]   It's the VPN that I trust and I use.
[02:07:35.200 --> 02:07:40.400]   And we thank ExpressVPN for their support of this week in tech.
[02:07:40.400 --> 02:07:42.560]   We had a great week this week on Twit.
[02:07:42.560 --> 02:07:46.280]   San's Leo, but he's coming back so you aren't going to have to wait too much longer.
[02:07:46.280 --> 02:07:48.960]   Let's see what's so great about the last week on Twit.
[02:07:48.960 --> 02:07:53.040]   Yeah, we're hanging out here in Club Twit with my man, Mr. Victor.
[02:07:53.040 --> 02:07:55.720]   I really appreciate all of you all hanging out here with us.
[02:07:55.720 --> 02:07:57.720]   I'm looking here in the discord.
[02:07:57.720 --> 02:08:01.280]   John A says, I have a question for Ant and Victor.
[02:08:01.280 --> 02:08:05.040]   What do you want from Star?
[02:08:05.040 --> 02:08:09.920]   Dude, all yet Twit.
[02:08:09.920 --> 02:08:12.320]   Previously on Twit.
[02:08:12.320 --> 02:08:13.320]   Hands on Windows.
[02:08:13.320 --> 02:08:17.640]   Coming up on Hands on Windows, we're going to take a look at the top five productivity
[02:08:17.640 --> 02:08:19.600]   apps in Windows 11.
[02:08:19.600 --> 02:08:22.960]   And number four is going to really surprise you.
[02:08:22.960 --> 02:08:23.960]   Security now.
[02:08:23.960 --> 02:08:34.080]   Can chat GPT keep a secret that some employees of Samsung semiconductor were using chat GPT
[02:08:34.080 --> 02:08:42.680]   to help in their diagnosis and repair of some problematic proprietary Samsung code.
[02:08:42.680 --> 02:08:50.160]   The only problem was the uploads contained Samsung's sensitive proprietary information.
[02:08:50.160 --> 02:08:55.960]   Samsung warned its employees against using chat GPT in their daily work.
[02:08:55.960 --> 02:09:04.720]   While the company develops its own internal chat GPT like AI for its own private use.
[02:09:04.720 --> 02:09:05.720]   Home Theater Geeks.
[02:09:05.720 --> 02:09:10.360]   I chat with Tomlinson Holman, the TH in THX.
[02:09:10.360 --> 02:09:16.360]   As the show is called Home Theater Geeks, it's the perfect place for you Tom Holman to come
[02:09:16.360 --> 02:09:19.720]   and talk about super geeky stuff.
[02:09:19.720 --> 02:09:21.480]   Where do I join?
[02:09:21.480 --> 02:09:23.000]   Twit.
[02:09:23.000 --> 02:09:26.400]   No membership needed.
[02:09:26.400 --> 02:09:36.800]   I can't hear the letters THX and not think of the big sound effect from the big movies.
[02:09:36.800 --> 02:09:38.800]   That's deep, deep sound.
[02:09:38.800 --> 02:09:40.440]   Wasn't that the phrase for it?
[02:09:40.440 --> 02:09:41.440]   Was that?
[02:09:41.440 --> 02:09:43.800]   Yeah, I don't know.
[02:09:43.800 --> 02:09:46.480]   Did it have a title other than THX?
[02:09:46.480 --> 02:09:48.400]   It had an actual title.
[02:09:48.400 --> 02:09:51.520]   I think it's like deep sound or deep noise or something like that.
[02:09:51.520 --> 02:09:58.800]   Deep note.
[02:09:58.800 --> 02:10:04.240]   Composed by Lucasfilm, sound engineer Dr. James Andy Moore.
[02:10:04.240 --> 02:10:09.600]   First screened 1983 premiere of Star Wars.
[02:10:09.600 --> 02:10:10.600]   What is that?
[02:10:10.600 --> 02:10:11.600]   Is that six?
[02:10:11.600 --> 02:10:12.600]   No.
[02:10:12.600 --> 02:10:14.600]   Return of the Jedi.
[02:10:14.600 --> 02:10:15.600]   Interesting.
[02:10:15.600 --> 02:10:19.240]   I didn't know that that played in the theaters that early.
[02:10:19.240 --> 02:10:22.680]   I thought it was a mid to late 80s thing.
[02:10:22.680 --> 02:10:26.680]   Anyways, certainly didn't play before any football games.
[02:10:26.680 --> 02:10:33.280]   It's a horrible segue, but we all know that YouTube TV is getting the Sunday ticket, the
[02:10:33.280 --> 02:10:38.720]   NFL Sunday ticket, which had stayed, which had called direct TV.
[02:10:38.720 --> 02:10:41.040]   It's home for decades.
[02:10:41.040 --> 02:10:43.520]   It was possible that it was going to go to Apple.
[02:10:43.520 --> 02:10:45.640]   Now it's certainly going to YouTube TV.
[02:10:45.640 --> 02:10:50.720]   Now we have a pricing for it for any of the football fans out there.
[02:10:50.720 --> 02:10:51.720]   I don't know.
[02:10:51.720 --> 02:10:57.520]   Maybe there are not that many, but $349 a year if you want the Sunday ticket package
[02:10:57.520 --> 02:10:59.840]   through YouTube TV.
[02:10:59.840 --> 02:11:04.880]   That's a pretty penny for some NFL football, if I do say so myself.
[02:11:04.880 --> 02:11:06.280]   I don't know if I'll be paying for it.
[02:11:06.280 --> 02:11:10.800]   Well, actually I do know I won't be paying for it, but what about you?
[02:11:10.800 --> 02:11:13.000]   I'm right there with you, bro.
[02:11:13.000 --> 02:11:14.000]   No.
[02:11:14.000 --> 02:11:15.000]   No.
[02:11:15.000 --> 02:11:18.640]   It's no different from when direct TV had it.
[02:11:18.640 --> 02:11:24.880]   From a pricing standpoint, you're getting the football season and you're paying well
[02:11:24.880 --> 02:11:27.120]   over $300 of what have you.
[02:11:27.120 --> 02:11:32.040]   The football season isn't a full year.
[02:11:32.040 --> 02:11:37.720]   Sometimes the game experience isn't necessarily what you would hope it would be because if
[02:11:37.720 --> 02:11:42.480]   you're watching your team play in the other market or what have you, because they weren't
[02:11:42.480 --> 02:11:48.960]   able to, because you're not able to see them in the other regular time, you get the broadcast
[02:11:48.960 --> 02:11:51.680]   team that isn't necessarily the A broadcast team.
[02:11:51.680 --> 02:11:55.720]   Sometimes that screws with the experience.
[02:11:55.720 --> 02:11:57.280]   It's just not worth it.
[02:11:57.280 --> 02:12:00.600]   You get five months out of the year for this package.
[02:12:00.600 --> 02:12:02.920]   You don't even get the full 12 months.
[02:12:02.920 --> 02:12:05.880]   They got to bring something else to the table.
[02:12:05.880 --> 02:12:13.600]   No, it's not comparable, but the price of an MLB.com subscription is $149.
[02:12:13.600 --> 02:12:20.240]   But they have the same challenges, and the same when baseball streams on Apple or on
[02:12:20.240 --> 02:12:25.000]   Amazon or elsewhere that I want my local announcers.
[02:12:25.000 --> 02:12:27.920]   I want to know I have a parasocial relationship with them.
[02:12:27.920 --> 02:12:32.800]   I want to feel like Michael K is saying horrible things in my ears.
[02:12:32.800 --> 02:12:37.080]   When Aaron Judge was pursuing the home run record last year, there was a possibility
[02:12:37.080 --> 02:12:40.040]   that it wouldn't be as hometown anchors calling the game.
[02:12:40.040 --> 02:12:45.400]   No, it's great for Apple and Amazon to help diversify the space, but Google.
[02:12:45.400 --> 02:12:47.200]   A sports sport.
[02:12:47.200 --> 02:12:49.840]   It's a care about this stuff.
[02:12:49.840 --> 02:12:55.200]   When you say 149, is that 149 a year or a month or what?
[02:12:55.200 --> 02:12:56.200]   A year.
[02:12:56.200 --> 02:12:57.200]   A year.
[02:12:57.200 --> 02:12:59.280]   There's still big blackout restrictions.
[02:12:59.280 --> 02:13:02.760]   You can't watch out of market games and teams, at least like the Yankees.
[02:13:02.760 --> 02:13:07.200]   I don't know other teams, but the Yankees through Yes have their own television network,
[02:13:07.200 --> 02:13:11.400]   and they have their own radio network, so they have their own app.
[02:13:11.400 --> 02:13:16.560]   You can listen to Yankee games on MLB, but if you want to watch, you're on Yes.
[02:13:16.560 --> 02:13:17.560]   Interesting.
[02:13:17.560 --> 02:13:23.240]   At the unwinding of all these rights, these old antiquated rights around different kinds
[02:13:23.240 --> 02:13:28.280]   of broadcasting is certainly happening right in front of us.
[02:13:28.280 --> 02:13:34.320]   I think then at the same time, we have the streaming wars, which while the old television
[02:13:34.320 --> 02:13:40.120]   system is still unwinding, we have the streaming wars, which are, I think, about to get apocalyptic
[02:13:40.120 --> 02:13:46.240]   because people are so overstretched on their streaming service subscriptions.
[02:13:46.240 --> 02:13:51.960]   You hear it again and again, consumers talking about it, and I think are ready to start dumping
[02:13:51.960 --> 02:13:55.160]   some subscriptions, and the consumers getting hit pretty hard.
[02:13:55.160 --> 02:14:00.200]   It's been a couple years now, and the drag on that is really strong, and you see that
[02:14:00.200 --> 02:14:03.400]   in the levels of debt rising.
[02:14:03.400 --> 02:14:09.440]   I think certainly the golden age of streaming, or at least paying for streaming subscriptions
[02:14:09.440 --> 02:14:13.160]   is over, and it's tough to see who's going to survive that.
[02:14:13.160 --> 02:14:18.840]   I think it's going to surprise a lot of people, but I think that the companies that are best
[02:14:18.840 --> 02:14:22.280]   set up to win the streaming wars are not necessarily the ones you think of.
[02:14:22.280 --> 02:14:29.480]   I think they are Apple, Amazon, and Google, because Apple and Amazon have subscriptions
[02:14:29.480 --> 02:14:34.520]   that people already pay for and get other stuff, and they're like, "Fine, it's in there.
[02:14:34.520 --> 02:14:42.160]   I'll keep paying for Apple One or my Amazon Prime subscription," and that the fact that
[02:14:42.160 --> 02:14:47.240]   the video is in there, oh, well, that just gives them so much more leverage, operating
[02:14:47.240 --> 02:14:52.120]   leverage over all these other subscription subscriptions, and then you have Google with
[02:14:52.120 --> 02:14:57.160]   YouTube, where they're still sucking up so many of the eyeballs.
[02:14:57.160 --> 02:15:02.040]   That's not really a good analogy that you get that.
[02:15:02.040 --> 02:15:10.640]   They're still capturing a lot of the attention, and so the fact that they are doing it all
[02:15:10.640 --> 02:15:19.000]   the way from now YouTube shorts, which is rapidly drawing some audience from TikTok,
[02:15:19.000 --> 02:15:25.440]   and then on the complete other end, to bring it back to this story, on the complete other
[02:15:25.440 --> 02:15:31.400]   end, you have them now being the host of probably the most valuable subscription in the world,
[02:15:31.400 --> 02:15:41.560]   which is this $5, $350 subscription for the NFL, that people will actually pay for, enough
[02:15:41.560 --> 02:15:44.920]   people will actually pay for that it is insanely profitable.
[02:15:44.920 --> 02:15:50.560]   I think that puts YouTube in a really, really strong position from a viewing standpoint,
[02:15:50.560 --> 02:15:56.440]   and ultimately all these platforms go where the eyeballs go and the winners consolidate
[02:15:56.440 --> 02:15:58.760]   around where the eyeballs are.
[02:15:58.760 --> 02:16:06.360]   Okay, so definitely along these lines, how do we feel about HBO Max becoming just Max?
[02:16:06.360 --> 02:16:11.560]   It's no long HBO, the legacy name, no longer part of the name at all.
[02:16:11.560 --> 02:16:12.560]   You just got to know.
[02:16:12.560 --> 02:16:16.440]   It sounds like the name of an AI.
[02:16:16.440 --> 02:16:19.000]   I figured that was coming.
[02:16:19.000 --> 02:16:21.720]   It feels like the name of an AI.
[02:16:21.720 --> 02:16:23.080]   It's now not HBO Max.
[02:16:23.080 --> 02:16:24.080]   It's the name Max.
[02:16:24.080 --> 02:16:25.080]   Yeah, exactly.
[02:16:25.080 --> 02:16:26.480]   You just tell Max what Max.
[02:16:26.480 --> 02:16:35.200]   I'm interested in watching an action-adventure movie that has this star in it that has rated
[02:16:35.200 --> 02:16:38.840]   this and just show me all the options.
[02:16:38.840 --> 02:16:46.040]   I saw the story via Tech News Weekly, Mr. Sargent, and how we're talking about it because I
[02:16:46.040 --> 02:16:49.080]   didn't see it prior to that.
[02:16:49.080 --> 02:16:52.600]   I get their point, HBO Max, whoever they are.
[02:16:52.600 --> 02:16:58.440]   I get their point by dropping the name because if there's going to be a lot more children-friendly
[02:16:58.440 --> 02:17:05.840]   content there, I guess some parents could be concerned when they put that tag HBO on there,
[02:17:05.840 --> 02:17:14.120]   knowing that, "Yeah, HBO wasn't necessarily kid-friendly for several decades."
[02:17:14.120 --> 02:17:16.320]   It just makes more sense marketing-wise.
[02:17:16.320 --> 02:17:17.960]   Nothing's going to change.
[02:17:17.960 --> 02:17:22.360]   You're still going to have the content that you're used to getting.
[02:17:22.360 --> 02:17:24.000]   It's just a brand-related thing.
[02:17:24.000 --> 02:17:25.000]   It's kind of a silly.
[02:17:25.000 --> 02:17:28.680]   HBO has so much brand equity though.
[02:17:28.680 --> 02:17:29.680]   Yes.
[02:17:29.680 --> 02:17:32.680]   It almost be like Ford saying, "We're not going to call ourselves Fordy, and we're going
[02:17:32.680 --> 02:17:35.880]   to call ourselves Blue Oval."
[02:17:35.880 --> 02:17:38.760]   Nobody's going to know what Blue Oval is, right?
[02:17:38.760 --> 02:17:44.800]   But at the same time, Federal Express has been doing just fine the last couple of decades
[02:17:44.800 --> 02:17:45.800]   as FedEx.
[02:17:45.800 --> 02:17:47.800]   But everybody calls FedEx.
[02:17:47.800 --> 02:17:49.840]   That's a little different.
[02:17:49.840 --> 02:17:50.840]   Close enough.
[02:17:50.840 --> 02:17:53.280]   We all know what that is.
[02:17:53.280 --> 02:17:56.600]   I know the people out there that rebranded in some of them as well.
[02:17:56.600 --> 02:17:59.840]   It does feel like a Zaz love thing.
[02:17:59.840 --> 02:18:06.840]   It feels like you have to know the inside baseball on no pun intended on what's happening
[02:18:06.840 --> 02:18:14.800]   at Warner and those media changes to really understand why this rename and rebrand happened.
[02:18:14.800 --> 02:18:15.800]   Yeah.
[02:18:15.800 --> 02:18:18.320]   It was my big trouble.
[02:18:18.320 --> 02:18:21.760]   Like the rest of those streaming ones we just talked about, they're in big trouble.
[02:18:21.760 --> 02:18:23.720]   Like every major media company.
[02:18:23.720 --> 02:18:24.720]   Yes.
[02:18:24.720 --> 02:18:25.720]   Yes.
[02:18:25.720 --> 02:18:26.720]   They've got to do something.
[02:18:26.720 --> 02:18:30.880]   So to ance point, maybe rebranding is what they need because they need to not be pigeon
[02:18:30.880 --> 02:18:36.400]   holed into what they have been and they need to be seen as something larger in order to
[02:18:36.400 --> 02:18:37.400]   survive.
[02:18:37.400 --> 02:18:38.400]   Yeah, it could be.
[02:18:38.400 --> 02:18:39.400]   Maybe HBO's a boomer brand.
[02:18:39.400 --> 02:18:47.360]   If I want to reruns of Big Bang Theory, I'm not going to look on HBO Max.
[02:18:47.360 --> 02:18:50.840]   I'll probably look on Max before I look on HBO Max.
[02:18:50.840 --> 02:18:52.520]   You might look on Paramount.
[02:18:52.520 --> 02:18:55.600]   No, no, but it's on.
[02:18:55.600 --> 02:18:57.200]   Actually, it's on HBO Max now.
[02:18:57.200 --> 02:18:58.200]   Oh, is it?
[02:18:58.200 --> 02:19:02.200]   I thought it was a Paramount show.
[02:19:02.200 --> 02:19:03.680]   CBS or something.
[02:19:03.680 --> 02:19:07.880]   One of those things, but it's on HBO Max right now if I go and pull it up.
[02:19:07.880 --> 02:19:12.680]   But again, most people, yeah, is it on Max?
[02:19:12.680 --> 02:19:13.680]   Okay.
[02:19:13.680 --> 02:19:14.680]   Yes.
[02:19:14.680 --> 02:19:15.680]   On Max.
[02:19:15.680 --> 02:19:16.680]   Boom is there.
[02:19:16.680 --> 02:19:18.080]   Same way we were doing things with Netflix before it got muddy.
[02:19:18.080 --> 02:19:19.080]   Yeah.
[02:19:19.080 --> 02:19:24.280]   This is a story reminded me of the Netflix name change, which I brought up on T&W.
[02:19:24.280 --> 02:19:25.280]   Oh, right.
[02:19:25.280 --> 02:19:26.280]   Yeah.
[02:19:26.280 --> 02:19:27.280]   Number one, it was great.
[02:19:27.280 --> 02:19:28.280]   Quickster was the name.
[02:19:28.280 --> 02:19:29.280]   Yes.
[02:19:29.280 --> 02:19:33.880]   Of one was going to be the DVD and the other was the Netflix streaming and Quickster was the
[02:19:33.880 --> 02:19:36.480]   DVD and then they were those were the good old days.
[02:19:36.480 --> 02:19:41.080]   Oh, remember when that's what everybody was panicking about what we can't call it.
[02:19:41.080 --> 02:19:42.880]   Netflix anymore.
[02:19:42.880 --> 02:19:44.440]   Yeah.
[02:19:44.440 --> 02:19:46.920]   Did anyone here play word or sorry?
[02:19:46.920 --> 02:19:47.920]   Well, wordle.
[02:19:47.920 --> 02:19:49.480]   Yes, we probably all played world.
[02:19:49.480 --> 02:19:50.880]   Did you ever play hurdle?
[02:19:50.880 --> 02:19:55.200]   It was the game that Spotify acquired the music wordle game.
[02:19:55.200 --> 02:19:57.680]   I did not, sir, but I did run hurdles.
[02:19:57.680 --> 02:19:58.680]   Oh, okay.
[02:19:58.680 --> 02:19:59.680]   You're different.
[02:19:59.680 --> 02:20:05.720]   This is this is not heard H you are, but heard H E A R B hurdle because it's a Spotify
[02:20:05.720 --> 02:20:06.720]   thing.
[02:20:06.720 --> 02:20:11.720]   I do remember when the wordle thing, the wordle like obsession was happening and it seemed
[02:20:11.720 --> 02:20:13.400]   to captivate everybody.
[02:20:13.400 --> 02:20:18.160]   It was like this, there was this gold rush on wordle inspired games and everybody was
[02:20:18.160 --> 02:20:24.160]   buying them up and Spotify's play was to buy this game called hurdle, which would basically
[02:20:24.160 --> 02:20:29.720]   it would play the beginning part of a song five times and you'd have to guess the song
[02:20:29.720 --> 02:20:37.760]   and big surprise that's done that whole, that whole, you know, gold rush has fizzled
[02:20:37.760 --> 02:20:40.520]   out and Spotify is going to be closing it down.
[02:20:40.520 --> 02:20:44.960]   So if you're a big fan of wordle inspired games, you're going to have one less to play.
[02:20:44.960 --> 02:20:45.960]   I don't even know.
[02:20:45.960 --> 02:20:46.960]   I still like podcast.
[02:20:46.960 --> 02:20:48.600]   I still like the wordle.
[02:20:48.600 --> 02:20:51.560]   I just was never wanted to people that shared my score.
[02:20:51.560 --> 02:20:52.560]   Yeah.
[02:20:52.560 --> 02:20:53.560]   Damn dumb day.
[02:20:53.560 --> 02:20:54.560]   Oh, yeah.
[02:20:54.560 --> 02:20:55.560]   I didn't play it ever.
[02:20:55.560 --> 02:20:56.560]   I didn't.
[02:20:56.560 --> 02:20:57.560]   Not once.
[02:20:57.560 --> 02:20:58.560]   I really.
[02:20:58.560 --> 02:20:59.560]   I never played it.
[02:20:59.560 --> 02:21:01.040]   I already have good words, man.
[02:21:01.040 --> 02:21:05.280]   I don't want that to be a game.
[02:21:05.280 --> 02:21:07.520]   Words want you in your sleep already.
[02:21:07.520 --> 02:21:10.120]   Yes, that's true.
[02:21:10.120 --> 02:21:12.520]   I have spent enough of my time with words.
[02:21:12.520 --> 02:21:17.000]   I don't need it to be how I passed the time and enjoyment.
[02:21:17.000 --> 02:21:21.720]   And then I think maybe finally I thought this was an article that could round out the
[02:21:21.720 --> 02:21:23.680]   show.
[02:21:23.680 --> 02:21:32.240]   And of the show, Harry McCracken, the technologizer wrote on technologizer.com about the end
[02:21:32.240 --> 02:21:40.300]   of computer magazines in America because maximum PC and Mac life are both shipping their
[02:21:40.300 --> 02:21:47.960]   last print edition moving to download, moving to digital.
[02:21:47.960 --> 02:21:52.720]   And he says, I'm not, I'm not writing this article because the dead tree versions of
[02:21:52.720 --> 02:21:54.800]   maximum PC and Mac life are no more.
[02:21:54.800 --> 02:22:00.840]   I'm writing it because they were the last two extant US computer magazines that had
[02:22:00.840 --> 02:22:03.360]   managed to cling to life until now.
[02:22:03.360 --> 02:22:09.280]   The computer, he says, with their abandonment of print, the computer magazine era has officially
[02:22:09.280 --> 02:22:13.360]   ended and that just kind of makes me kind of sad.
[02:22:13.360 --> 02:22:14.360]   Computer shop.
[02:22:14.360 --> 02:22:17.800]   I, you know, I didn't realize they were still around.
[02:22:17.800 --> 02:22:18.800]   Yeah.
[02:22:18.800 --> 02:22:21.360]   I mean, I'm not reading them, but.
[02:22:21.360 --> 02:22:25.760]   I thought, whoa, I was the interest in this.
[02:22:25.760 --> 02:22:28.320]   I was like, wow, they were actually still printing.
[02:22:28.320 --> 02:22:29.520]   I had no idea.
[02:22:29.520 --> 02:22:30.680]   That was, yeah.
[02:22:30.680 --> 02:22:34.120]   I just, I mean, people that care is our age group.
[02:22:34.120 --> 02:22:35.120]   No, totally.
[02:22:35.120 --> 02:22:36.280]   Oh, no question.
[02:22:36.280 --> 02:22:40.480]   This is purely an astologist story that I put in there because it reminds me of being
[02:22:40.480 --> 02:22:46.480]   a kid and looking at that fat, like honking, you know, issue of computer shopping.
[02:22:46.480 --> 02:22:51.880]   I mean, it's just like hundreds of pages of storefronts with different, you know, like
[02:22:51.880 --> 02:22:56.080]   specked, you know, machines and, oh, yes, it was the whole thing.
[02:22:56.080 --> 02:23:02.800]   I worked at the ancient history years ago, two decades ago, I worked at Copperfield's
[02:23:02.800 --> 02:23:06.400]   books in Sebastopol and Petaluma.
[02:23:06.400 --> 02:23:08.440]   Oh, I would drive between those two stories.
[02:23:08.440 --> 02:23:09.440]   Oh, yeah, corner.
[02:23:09.440 --> 02:23:15.760]   At the time I was in charge of magazines and with magazine, with all magazines at the time,
[02:23:15.760 --> 02:23:18.760]   that was the early 2000s and magazines were still kicking.
[02:23:18.760 --> 02:23:21.440]   So you'd always over order because they would make up.
[02:23:21.440 --> 02:23:25.920]   And at the end of the week, you have extras and you have to prove to the publisher that
[02:23:25.920 --> 02:23:27.920]   you did not sell those.
[02:23:27.920 --> 02:23:33.840]   And I would sit in the back and strip those, especially those computer magazines.
[02:23:33.840 --> 02:23:38.040]   I would sit in the back of Copperfield's and read the stripped magazine cover to cover
[02:23:38.040 --> 02:23:39.040]   all of them.
[02:23:39.040 --> 02:23:43.240]   And I felt like, wait, I'm reading a computer.
[02:23:43.240 --> 02:23:46.640]   Why don't I just use the internet?
[02:23:46.640 --> 02:23:56.080]   Anyway, it's a non-psych-witter, but I started my computer magazine reading at Copperfield's
[02:23:56.080 --> 02:23:57.080]   very close to you.
[02:23:57.080 --> 02:23:59.080]   It was all your fault, Dan.
[02:23:59.080 --> 02:24:00.080]   What?
[02:24:00.080 --> 02:24:01.080]   I just read it on the internet.
[02:24:01.080 --> 02:24:08.720]   Well, if you worked in a bookstore or a close, I mean, bookstores, I'm sure people are listening
[02:24:08.720 --> 02:24:09.720]   who've worked in bookstores.
[02:24:09.720 --> 02:24:14.320]   And it's a travesty, but stripping books was pretty common.
[02:24:14.320 --> 02:24:17.080]   You'd have way too many mask markets of some sci-fi book.
[02:24:17.080 --> 02:24:18.080]   You totally wanted to read.
[02:24:18.080 --> 02:24:22.640]   So I stripped that cover.
[02:24:22.640 --> 02:24:28.120]   This reminds me of a story to this magazine, the death of the computer magazine.
[02:24:28.120 --> 02:24:33.400]   It reminds me of a story that I've maybe even told on Twitter before of when I was in the
[02:24:33.400 --> 02:24:35.400]   90s, I was in college, right?
[02:24:35.400 --> 02:24:38.240]   When the internet was hitting and I was studying journalism.
[02:24:38.240 --> 02:24:40.480]   And I had these arguments with journals and instructors.
[02:24:40.480 --> 02:24:44.280]   I was like, this internet thing, this is the future.
[02:24:44.280 --> 02:24:50.760]   And they were like, no, they tried this experiment down in Florida with hypercard.
[02:24:50.760 --> 02:24:54.520]   The thing is, look, people just don't want to read on screens.
[02:24:54.520 --> 02:24:59.280]   And I was like, it's instantaneous and it's so much cheaper.
[02:24:59.280 --> 02:25:06.400]   You don't have any trucks going into delivering papers and paper carriers and all of that.
[02:25:06.400 --> 02:25:08.400]   And they're like, it's just not going to happen.
[02:25:08.400 --> 02:25:11.720]   You think of it, you just don't have the serendipity of flipping through things and
[02:25:11.720 --> 02:25:13.480]   being able to see.
[02:25:13.480 --> 02:25:17.000]   And they said, you should just go into it, you should get a job in magazines.
[02:25:17.000 --> 02:25:19.440]   I'm like, no, I wouldn't work on the internet.
[02:25:19.440 --> 02:25:21.440]   This is where the future this is.
[02:25:21.440 --> 02:25:26.320]   And they're like, just take the magazine class and get a job in magazines because you
[02:25:26.320 --> 02:25:27.680]   have a future there.
[02:25:27.680 --> 02:25:30.560]   So now, to be fair, they're right.
[02:25:30.560 --> 02:25:35.400]   If I would have studied magazines, I could have had a job all the way up until 2023 or
[02:25:35.400 --> 02:25:40.920]   something, maybe, actually, probably not, maybe up until 2010 or so.
[02:25:40.920 --> 02:25:44.080]   Yeah, I'd say a few years off that, for sure.
[02:25:44.080 --> 02:25:47.000]   I'm glad I didn't.
[02:25:47.000 --> 02:25:50.000]   I'm glad I sort of walked out and was like, yeah, they don't know what they're talking
[02:25:50.000 --> 02:25:51.000]   about.
[02:25:51.000 --> 02:25:53.000]   My teenage mind, I don't know what the heck they're talking about.
[02:25:53.000 --> 02:25:54.000]   They can't teach me anything.
[02:25:54.000 --> 02:25:57.760]   When, in fact, they could 90% of what they were saying was really excellent.
[02:25:57.760 --> 02:25:59.320]   They just had this 10% where long.
[02:25:59.320 --> 02:26:02.840]   It just happened to be like a really important 10% at that moment.
[02:26:02.840 --> 02:26:03.840]   Yeah.
[02:26:03.840 --> 02:26:04.840]   Yeah.
[02:26:04.840 --> 02:26:13.080]   Well, I, yes, computer shopper, of course, was a magazine that I have fond memories of
[02:26:13.080 --> 02:26:17.400]   just because of the sheer of like, I mean, it was just gigantic.
[02:26:17.400 --> 02:26:23.360]   But my history with computer magazines goes back into the 80s when I had a Commerce 64
[02:26:23.360 --> 02:26:29.080]   and it was all the magazines that you could get then like compute where you get it and
[02:26:29.080 --> 02:26:32.640]   it would have like, you know, it would have some sort of machine language code that you
[02:26:32.640 --> 02:26:36.080]   could enter in manually into your computer and you'd end up with a game on the other
[02:26:36.080 --> 02:26:41.440]   side or you wouldn't and then you'd have to go through line by line and troubleshoot
[02:26:41.440 --> 02:26:43.240]   it and everything.
[02:26:43.240 --> 02:26:47.080]   So yeah, I've got a fondness in my heart for the computer magazine.
[02:26:47.080 --> 02:26:49.920]   So when I saw this article, I was like, oh, okay.
[02:26:49.920 --> 02:26:54.040]   I need to take a couple of minutes to mourn the loss of the computer magazine.
[02:26:54.040 --> 02:26:56.800]   One warm tear rolled down your cheek.
[02:26:56.800 --> 02:26:57.800]   Yeah.
[02:26:57.800 --> 02:27:02.240]   I'll say that one did, but one didn't, but it should have.
[02:27:02.240 --> 02:27:07.600]   If I was more of a computer nerd, a tear would have would have rolled down my cheek.
[02:27:07.600 --> 02:27:12.760]   But anyways, thanks to Harry McCracken for writing a really great article as he does all
[02:27:12.760 --> 02:27:14.280]   the time.
[02:27:14.280 --> 02:27:21.040]   And thanks to you three for being my guests on this episode of This Week in Tech and for
[02:27:21.040 --> 02:27:23.520]   I don't know, making this job easy.
[02:27:23.520 --> 02:27:28.960]   Sometimes I can admit filling Leo's shoes can be a little daunting because he's been
[02:27:28.960 --> 02:27:31.040]   doing this stuff for so long.
[02:27:31.040 --> 02:27:36.840]   So I was like, I gotta have some people that I know I can roll with on this panel and you
[02:27:36.840 --> 02:27:37.840]   guys are great.
[02:27:37.840 --> 02:27:40.000]   So thank you for coming all with me today.
[02:27:40.000 --> 02:27:41.000]   Appreciate it.
[02:27:41.000 --> 02:27:42.000]   Thank you.
[02:27:42.000 --> 02:27:45.200]   Aunt Jason and Jason, great to see you again.
[02:27:45.200 --> 02:27:46.200]   Good to see you.
[02:27:46.200 --> 02:27:47.200]   This was fun.
[02:27:47.200 --> 02:27:48.200]   You did great.
[02:27:48.200 --> 02:27:49.200]   It was a pleasure.
[02:27:49.200 --> 02:27:50.200]   Yeah, Jason.
[02:27:50.200 --> 02:27:51.200]   You did a fantastic job.
[02:27:51.200 --> 02:27:52.200]   Thank you.
[02:27:52.200 --> 02:27:57.320]   And what do you want to leave people with blog.danpatterson.com?
[02:27:57.320 --> 02:28:00.600]   Where do you want to direct people if they want to support you?
[02:28:00.600 --> 02:28:02.080]   Yeah, that's great.
[02:28:02.080 --> 02:28:08.760]   Blog.danpatterson.com is where I have the most recent ZDNet interviews with AI thought
[02:28:08.760 --> 02:28:11.440]   leaders.
[02:28:11.440 --> 02:28:13.000]   This is nothing to do with me.
[02:28:13.000 --> 02:28:20.840]   It's fascinating people talking about this emerging technology that is reshaping lives.
[02:28:20.840 --> 02:28:21.840]   Excellent.
[02:28:21.840 --> 02:28:26.000]   Well, everybody should follow your awesome work there.
[02:28:26.000 --> 02:28:27.000]   Blog.danpatterson.com.
[02:28:27.000 --> 02:28:28.000]   We're just calling for it.
[02:28:28.000 --> 02:28:29.000]   We're just calling for it.
[02:28:29.000 --> 02:28:30.000]   ZDNet.
[02:28:30.000 --> 02:28:31.000]   ZDNet.
[02:28:31.000 --> 02:28:33.400]   And you'll find Dan there as well.
[02:28:33.400 --> 02:28:34.840]   It's great talking with you, Dan.
[02:28:34.840 --> 02:28:35.840]   Thank you.
[02:28:35.840 --> 02:28:36.840]   You too.
[02:28:36.840 --> 02:28:39.480]   And Jason Heiner, editor and chief of ZDNet.
[02:28:39.480 --> 02:28:45.920]   So even if Jason isn't writing the articles that you're seeing, he's certainly very, very
[02:28:45.920 --> 02:28:49.800]   ingrained in the presentation of the articles.
[02:28:49.800 --> 02:28:53.960]   Tell us a little bit about what people can do to find you online and what do you want
[02:28:53.960 --> 02:28:55.520]   to leave people with?
[02:28:55.520 --> 02:28:56.520]   Yeah.
[02:28:56.520 --> 02:29:03.520]   You know, really it's the work of folks like Dan, all of our writers and editors who are
[02:29:03.520 --> 02:29:08.560]   following and on ZDNet are following the most important developments.
[02:29:08.560 --> 02:29:12.960]   Our focus is on disruptive innovation and there is so much of it happening.
[02:29:12.960 --> 02:29:15.360]   AI, of course, leading the charge.
[02:29:15.360 --> 02:29:23.640]   But since the beginning of the year, AI has taken off in such an amazing way and from
[02:29:23.640 --> 02:29:26.240]   a curiosity standpoint of the audience.
[02:29:26.240 --> 02:29:27.560]   People are trying to get their heads around.
[02:29:27.560 --> 02:29:29.440]   What does this mean?
[02:29:29.440 --> 02:29:30.440]   What's happening?
[02:29:30.440 --> 02:29:35.960]   One of the things that I really love about what our team has done is really surrounded
[02:29:35.960 --> 02:29:37.960]   it with a 360 perspective.
[02:29:37.960 --> 02:29:43.440]   We're trying to look at it from every angle, not just repeat the hype and the echo chamber
[02:29:43.440 --> 02:29:49.280]   about what it's doing, but really pulling back and pulling apart the pieces of it and
[02:29:49.280 --> 02:29:53.880]   looking at it and observing it and the work that Dan has done, interviewing really smart
[02:29:53.880 --> 02:29:58.400]   people and bringing those unique perspectives, all three of the interviews that are already
[02:29:58.400 --> 02:30:03.640]   up that Dan has done, all bring a very different perspective to the argument.
[02:30:03.640 --> 02:30:09.320]   And that's a symbol of kind of the work we're doing more broadly on AI and chat GPT.
[02:30:09.320 --> 02:30:14.360]   You can come and you can learn how to use chat GPT, how to make really good prompts.
[02:30:14.360 --> 02:30:17.200]   You can understand how chat GPT works.
[02:30:17.200 --> 02:30:22.320]   We're doing all of those things and understanding AI in the broader spectrum.
[02:30:22.320 --> 02:30:26.360]   And then we're also looking at, of course, what are all the technologies that are happening
[02:30:26.360 --> 02:30:31.480]   the products and what are the most advanced and cutting edge ones and latest things happening
[02:30:31.480 --> 02:30:33.320]   and how can you take advantage of them?
[02:30:33.320 --> 02:30:39.040]   Which ones are worth sort of taking advantage of and bringing in and trying out in your life
[02:30:39.040 --> 02:30:46.040]   from robot vacuums to smartphones to software that you can use to help with your work and
[02:30:46.040 --> 02:30:48.440]   with your life.
[02:30:48.440 --> 02:30:51.440]   So that's what we do zedingit.com.
[02:30:51.440 --> 02:30:56.760]   We're also YouTube, TikTok, Instagram, all of the socials.
[02:30:56.760 --> 02:30:58.840]   You can find lots of great stuff there.
[02:30:58.840 --> 02:31:03.000]   And that's the best thing that I have to offer.
[02:31:03.000 --> 02:31:04.000]   Right on.
[02:31:04.000 --> 02:31:05.000]   Well, thank you, Jason.
[02:31:05.000 --> 02:31:07.720]   Always a pleasure to get the chance to talk with you.
[02:31:07.720 --> 02:31:09.160]   And yeah, thank you.
[02:31:09.160 --> 02:31:10.880]   @jasonheiner, by the way, on Twitter.
[02:31:10.880 --> 02:31:14.520]   But it sounds like you don't, you're not doing too much there.
[02:31:14.520 --> 02:31:15.680]   None of us are.
[02:31:15.680 --> 02:31:18.120]   No, Twitter's dead.
[02:31:18.120 --> 02:31:21.920]   Also, and Pruitt.
[02:31:21.920 --> 02:31:25.080]   Also not doing very much on Twitter, it seems.
[02:31:25.080 --> 02:31:27.440]   And thanks so much for carving out yet another Sunday.
[02:31:27.440 --> 02:31:32.800]   I realized after I asked you to do this, that you were already covering for me on two of
[02:31:32.800 --> 02:31:34.880]   the Sundays while I was out.
[02:31:34.880 --> 02:31:39.480]   And then I felt really bad because I was like, oh man, you're working so hard to enable
[02:31:39.480 --> 02:31:41.000]   me to take a vacation.
[02:31:41.000 --> 02:31:45.640]   So thank you, thank you, thank you for all of your help while I was gone.
[02:31:45.640 --> 02:31:50.240]   And thank you for being on this episode of This Week in Tech because I love podcasting
[02:31:50.240 --> 02:31:51.440]   with you, man.
[02:31:51.440 --> 02:31:52.600]   My pleasure, brother.
[02:31:52.600 --> 02:31:53.600]   My pleasure.
[02:31:53.600 --> 02:31:54.600]   I appreciate you.
[02:31:54.600 --> 02:31:55.600]   Yeah.
[02:31:55.600 --> 02:31:59.920]   Make sure you all watch my show Hands on Photography Twitter at TV slash HOP.
[02:31:59.920 --> 02:32:05.520]   And learn how to be a better photographer and post-processor regardless of the phone
[02:32:05.520 --> 02:32:07.480]   or camera you have.
[02:32:07.480 --> 02:32:13.040]   I'm not going to get into all of the weeds about, well, you need an iPhone 18 plus pro.
[02:32:13.040 --> 02:32:15.360]   No, just grab a camera.
[02:32:15.360 --> 02:32:18.480]   Let's get out there and shoot.
[02:32:18.480 --> 02:32:19.760]   Artist working man and podcast.
[02:32:19.760 --> 02:32:20.760]   That's right.
[02:32:20.760 --> 02:32:21.760]   Yeah, that's right.
[02:32:21.760 --> 02:32:22.760]   That's true.
[02:32:22.760 --> 02:32:23.760]   Keep it up, man.
[02:32:23.760 --> 02:32:25.920]   You're always my lower third, Mr. Jammerby.
[02:32:25.920 --> 02:32:28.840]   You're always such a positive person.
[02:32:28.840 --> 02:32:29.840]   I love it.
[02:32:29.840 --> 02:32:32.200]   Thank you, Anne.
[02:32:32.200 --> 02:32:34.600]   As for, well, first of all, club twit.
[02:32:34.600 --> 02:32:39.600]   You need to know about club twit because it's become really important for us here at
[02:32:39.600 --> 02:32:44.720]   Twit over the last couple of years actually and Pruitt makes a lot of the club twit things
[02:32:44.720 --> 02:32:48.200]   happen behind the scenes as you saw on the promo.
[02:32:48.200 --> 02:32:53.800]   If you saw the promo, you sat down with Victor, one of the editors here.
[02:32:53.800 --> 02:32:56.640]   And so doing lots of really cool stuff on club twit.
[02:32:56.640 --> 02:32:58.160]   But what is club twit?
[02:32:58.160 --> 02:33:04.400]   It's basically, if you want all of our shows without any ads, if you want bonus content
[02:33:04.400 --> 02:33:08.760]   like that interview, there's a hands-on, windows hands-on, Mac.
[02:33:08.760 --> 02:33:12.080]   Scott Wilkinson has the home theater geek show now.
[02:33:12.080 --> 02:33:16.720]   There's a bunch of content exclusive to club twit members.
[02:33:16.720 --> 02:33:22.800]   If you want access to our Discord, which is for members only, then club twits, where
[02:33:22.800 --> 02:33:26.560]   it's at, twit.tv/clubtwit, $7 a month.
[02:33:26.560 --> 02:33:28.000]   See all that stuff.
[02:33:28.000 --> 02:33:34.400]   But not only that, it helps us because as you all know, podcast advertising, there's
[02:33:34.400 --> 02:33:40.120]   been some shaky ground in recent months, not just for us, but in general across the podcast
[02:33:40.120 --> 02:33:41.520]   market.
[02:33:41.520 --> 02:33:47.600]   And so you being a member actually helps support us directly.
[02:33:47.600 --> 02:33:51.160]   And we can't thank you enough for that because it really does help.
[02:33:51.160 --> 02:33:52.160]   So keep in touch.
[02:33:52.160 --> 02:33:53.160]   Yes.
[02:33:53.160 --> 02:33:54.680]   Can I jump in here for a second?
[02:33:54.680 --> 02:33:55.680]   Of course.
[02:33:55.680 --> 02:33:56.680]   All right.
[02:33:56.680 --> 02:34:03.000]   We regarding club twit, yes, we have our $7 a month and we have our annual plan, but
[02:34:03.000 --> 02:34:04.840]   we also have corporate group plans.
[02:34:04.840 --> 02:34:12.400]   So if you folks want to have your IT department join club twit, sign up at Twit.tv/clubtwit.
[02:34:12.400 --> 02:34:14.880]   But we also have family plans.
[02:34:14.880 --> 02:34:22.480]   So you can get a family plan as two seats for $12 and you can also increase those seats.
[02:34:22.480 --> 02:34:29.360]   And you can even get a family annual plan just again, just due to math, $12 times $12,
[02:34:29.360 --> 02:34:32.560]   whatever that math is and increase the seats.
[02:34:32.560 --> 02:34:37.800]   And also folks that are members of club twit, we just had our book club and we're going
[02:34:37.800 --> 02:34:43.280]   to have another book club meeting on June 29th.
[02:34:43.280 --> 02:34:47.800]   So if you're a member right now, please go in and vote on the next book that we're going
[02:34:47.800 --> 02:34:48.960]   to read.
[02:34:48.960 --> 02:34:54.760]   Now we'll have our meeting on June 29th with Mrs. Stacey Higginbotham.
[02:34:54.760 --> 02:34:58.720]   The book club has been a lot of fun and it's been exposing me to a bunch of sci-fi stuff
[02:34:58.720 --> 02:35:01.360]   that I would never read in my own.
[02:35:01.360 --> 02:35:03.640]   You know, it's been a lot of fun.
[02:35:03.640 --> 02:35:07.840]   So make sure you folks check us out and thank you for all of the support of club twit.
[02:35:07.840 --> 02:35:08.840]   Yes, indeed.
[02:35:08.840 --> 02:35:09.840]   Twit.tv/clubtwit.
[02:35:09.840 --> 02:35:12.160]   Thank you, man.
[02:35:12.160 --> 02:35:13.640]   As for this show, twit.tv/twit.
[02:35:13.640 --> 02:35:16.720]   If you go there, you're going to find all the ways to subscribe.
[02:35:16.720 --> 02:35:24.320]   We do the show every Sunday starting at 2pm Pacific, 5pm Eastern, 2100 UTC.
[02:35:24.320 --> 02:35:26.280]   So if you want to watch live, you certainly can.
[02:35:26.280 --> 02:35:31.240]   Twit.tv/live is our live feed with our chats and everything happening in real time.
[02:35:31.240 --> 02:35:35.880]   But really, you need to subscribe and you can do that at twit.tv/twit.
[02:35:35.880 --> 02:35:37.920]   We hope that you will.
[02:35:37.920 --> 02:35:39.240]   And that's all there is to it.
[02:35:39.240 --> 02:35:41.840]   I'm Jason Howell, catch me on all about Android.
[02:35:41.840 --> 02:35:46.120]   Actually I'm filling in for Leo on security now and this week in Google, this next upcoming
[02:35:46.120 --> 02:35:49.480]   week as well as Tech News Weekly and all about Android.
[02:35:49.480 --> 02:35:51.240]   So I'll be really busy.
[02:35:51.240 --> 02:35:54.240]   So I'll be looking for you all there as well.
[02:35:54.240 --> 02:35:56.840]   Thank you for allowing me to crash this party for one week.
[02:35:56.840 --> 02:35:58.600]   You had a lot of fun.
[02:35:58.600 --> 02:36:01.000]   And with that, another twit is in the can.
[02:36:01.000 --> 02:36:01.760]   Thanks everybody.
[02:36:01.760 --> 02:36:05.420]   - [MUSIC PLAYING]
[02:36:05.420 --> 02:36:06.420]   - Do the Twitter.
[02:36:06.420 --> 02:36:07.420]   All right.
[02:36:07.420 --> 02:36:09.420]   Do the Twitter, baby.
[02:36:09.420 --> 02:36:10.420]   Do the Twitter.
[02:36:10.420 --> 02:36:11.420]   All right.

