;FFMETADATA1
title=The Duchess of Sealand
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=773
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.440]   It's time for Twit this week in Tech. Great panel for you. Build that
[00:00:04.440 --> 00:00:10.000]   Wyler from Tech Republic. Patrick Bejaw from FrenchSpin.com and my friend
[00:00:10.000 --> 00:00:14.400]   Denise Howell from this week in law. Lots to talk about. We'll get Denise's take on
[00:00:14.400 --> 00:00:20.520]   the Trump executive order shutting down Twitter and the French offer for
[00:00:20.520 --> 00:00:25.560]   Twitter to move to France. We'll also talk about the House of Representatives.
[00:00:25.560 --> 00:00:29.880]   They're going to vote this week to protect your browser history or will they.
[00:00:29.880 --> 00:00:33.800]   And then finally at the end I want you to stay tuned because we have a one-on-one
[00:00:33.800 --> 00:00:38.440]   conversation with my good friend Barretunde Thurston, author of How to Be Black.
[00:00:38.440 --> 00:00:42.920]   We'll talk about the events of the past week. It's a very important segment and I
[00:00:42.920 --> 00:00:47.360]   want you to stay tuned for that. It's all coming up next on Twit. This week in
[00:00:47.360 --> 00:00:51.680]   Tech comes to you from Twit's LastPass Studios. You're focused on security but
[00:00:51.680 --> 00:00:56.120]   are your employees LastPass can ensure they are by making access and authentication
[00:00:56.120 --> 00:01:02.040]   seamless. Whether they're working in the office or remotely visit LastPass.com/Twit
[00:01:02.040 --> 00:01:05.000]   to learn more.
[00:01:05.000 --> 00:01:25.920]   This is Twit. This is Twit. This week in Tech. Episode 773 recorded Sunday May 31st 2020.
[00:01:25.920 --> 00:01:33.440]   The Duchess of Sea Land. This episode of This Week in Tech is brought to you by
[00:01:33.440 --> 00:01:39.360]   Masterclass. Online classes taught by the world's greatest minds. Get unlimited
[00:01:39.360 --> 00:01:44.000]   access to every Masterclass and as a listener you'll get 15% off the annual
[00:01:44.000 --> 00:01:51.960]   All Access Pass. Go to masterclass.com/Twit and by LastPass. Prepare for the
[00:01:51.960 --> 00:01:57.320]   unexpected in your business with LastPass. Trusted by over 17 million users and
[00:01:57.320 --> 00:02:04.360]   61,000 businesses worldwide. Visit LastPass.com/Twit to find out how they can help you.
[00:02:04.360 --> 00:02:10.440]   And by worldwide technology. You need to maintain your critical operations and
[00:02:10.440 --> 00:02:15.920]   you've got to keep your employees safe while doing so. From idea to execution
[00:02:15.920 --> 00:02:22.160]   WWT can keep you up and running and help you plan for the future. Working with you
[00:02:22.160 --> 00:02:30.560]   every step of the way. Contact WWT today to request a consultation at WWT.com/Twit
[00:02:30.560 --> 00:02:44.040]   to. It's time for Twit this week in Tech. The show we cover the weeks. Tech news with the
[00:02:44.040 --> 00:02:51.960]   best people available. It sounds like faint praise but it's not. You are the
[00:02:51.960 --> 00:02:56.280]   best people. Bill Detweiler, Executive or I'm sorry editor-in-chief at Tech
[00:02:56.280 --> 00:03:01.320]   Republic. Great to have you back. Welcome back Bill from Are you in Louisville? I am
[00:03:01.320 --> 00:03:06.320]   still in Louisville. It's always a pleasure to be here. Good to have you. Also from
[00:03:06.320 --> 00:03:12.400]   Finland. Patrick Beijon. Not Patrick. Patrick is a
[00:03:12.400 --> 00:03:19.680]   emigre from France. You might notice his accent does not sound finish. Are you in
[00:03:19.680 --> 00:03:26.480]   Helsinki? Where in Finland are you? No, I'm in the middle of the forest somewhere along
[00:03:26.480 --> 00:03:35.680]   the coast. It's pretty good. We don't have a lot of people around and we also got an
[00:03:35.680 --> 00:03:43.680]   upgrade to our fiber connection because everyone came around to work from the
[00:03:43.680 --> 00:03:49.640]   countryside. The connection looks great. We always get the best connections with
[00:03:49.640 --> 00:03:56.080]   people not in the US. It drives me nuts. You know, I feel like I talk about this
[00:03:56.080 --> 00:04:00.720]   all the time but I literally when I say the middle of the forest it's really
[00:04:00.720 --> 00:04:08.480]   true. The next house over is probably two kilometers away and the next store is
[00:04:08.480 --> 00:04:16.800]   10 kilometers away and I have 500 down to 500 up. Wow. It's crazy. Nice. I have a
[00:04:16.800 --> 00:04:23.600]   hundred. I'm a little less ridiculous. Also with this Denise Howell. Wonderful to
[00:04:23.600 --> 00:04:28.000]   have her back to these howl.info. Or more hosted this week in law. Hi Denise. Good to
[00:04:28.000 --> 00:04:33.200]   see you. Hi. I only have a hundred down on 10 up to. No fiber here. You're in the
[00:04:33.200 --> 00:04:39.200]   small town called Los Angeles. We're south of there. A little south. Thank fully right now.
[00:04:39.200 --> 00:04:43.520]   Beautiful Orange County. Yeah, you know, and I want to I don't want to ignore what's
[00:04:43.520 --> 00:04:48.240]   going on in the United States right now. It is a terrible time. Not only, you know, on
[00:04:48.240 --> 00:04:54.080]   top of the pandemic and more than a hundred thousand deaths in the US which
[00:04:54.080 --> 00:05:01.400]   is just kind of a mind boggling tragedy over just a few months. Now of course, the
[00:05:01.400 --> 00:05:09.800]   horrific video from Minneapolis of George Floyd being murdered by a police
[00:05:09.800 --> 00:05:18.080]   officer, the unrest that's resulted over the last five nights. It's horrific. We are a
[00:05:18.080 --> 00:05:24.320]   tech network. So it is challenging to talk about tech when so many horrible
[00:05:24.320 --> 00:05:27.800]   non tech things are happening. But we are going to talk about that today. But a
[00:05:27.800 --> 00:05:32.440]   little programming note at the end of the show, I'll let you guys go. And about a
[00:05:32.440 --> 00:05:36.720]   couple hours from now, I'm going to bring an old friend in Baratunde Thurston, the
[00:05:36.720 --> 00:05:40.400]   author of How to Be Black. And I want to just have a little conversation. In fact,
[00:05:40.400 --> 00:05:43.880]   you're welcome all welcome to stay if you'd like. But just a little conversation
[00:05:43.880 --> 00:05:50.040]   with Baratunde about what's going on and and what he can what he can do to help
[00:05:50.040 --> 00:06:00.520]   us in the privileged class be more thoughtful. So and more helpful. So that'll
[00:06:00.520 --> 00:06:03.560]   be at the end of the show. And I'm putting it at the end of the show partly
[00:06:03.560 --> 00:06:06.200]   because Baratunde has his own show right now. Actually, that's the main reason.
[00:06:06.560 --> 00:06:09.680]   Because Baratunde has his own show right now. But also, I think it's appropriate to
[00:06:09.680 --> 00:06:13.480]   kind of put it at a different part of the show. So the people who want to are
[00:06:13.480 --> 00:06:17.800]   trying to avoid and I understand that as well, trying to avoid politics and the
[00:06:17.800 --> 00:06:23.040]   horrors of the world around us and just kind of chill can do that as well. So I
[00:06:23.040 --> 00:06:26.840]   just thought I'd mention that. Although there is quite a bit of it in the tech
[00:06:26.840 --> 00:06:32.120]   news. In fact, I think we should probably start with the battle Royale between the
[00:06:32.120 --> 00:06:38.320]   president of the United States and Twitter. Sorry, you want to talk about
[00:06:38.320 --> 00:06:43.240]   something else? What about that new iPhone? I don't know. I don't I'm trying
[00:06:43.240 --> 00:06:49.720]   folks. I'm trying. So there's an executive order that the president put
[00:06:49.720 --> 00:06:57.600]   out because he there is a strong feeling in some conservative circles that they've
[00:06:57.600 --> 00:07:01.320]   always felt this they've always felt like Hollywood, for instance, was run by
[00:07:01.760 --> 00:07:07.880]   leftist liberals, Jews. They've always felt like and I'm saying that facetiously,
[00:07:07.880 --> 00:07:11.840]   but I'm sorry, that's some of the undercurrent of all that that they always felt
[00:07:11.840 --> 00:07:15.400]   like Silicon Valley is run. In fact, they've said it explicitly. It's run by
[00:07:15.400 --> 00:07:22.920]   liberals, the the L word horrible people. And oddly considering that Twitter has
[00:07:22.920 --> 00:07:29.040]   been a bully pulpit for Trump. Well, since long before he was elected. And
[00:07:29.040 --> 00:07:34.800]   really, he's been the number one place he's used to communicate to people. He
[00:07:34.800 --> 00:07:40.920]   feels like conservatives are censored on those platforms chiefly his executive
[00:07:40.920 --> 00:07:49.560]   order focused on Facebook and Twitter and Google. And they wanted to do
[00:07:49.560 --> 00:07:54.800]   something about it. This happened shortly. And I don't know if it's related,
[00:07:54.880 --> 00:08:00.040]   suspected is after the president was for the first time ever fact checked.
[00:08:00.040 --> 00:08:08.640]   Two tweets about mail in ballots on Tuesday, according to protocol, the
[00:08:08.640 --> 00:08:14.800]   direction from on high was do something. And the something they decided to do an
[00:08:14.800 --> 00:08:19.000]   executive order that was signed on Thursday with the Attorney General
[00:08:19.000 --> 00:08:26.280]   William Barr standing next to the president. I'm going to go to you, Denise,
[00:08:26.280 --> 00:08:32.840]   because you're the lawyer. It seems to me has no legal basis whatsoever.
[00:08:32.840 --> 00:08:41.200]   Yeah, and and the reaction of clamping down on section 230 not only has no
[00:08:41.200 --> 00:08:47.120]   legal basis, but it doesn't really relate to what Twitter did to the
[00:08:47.120 --> 00:08:52.920]   president, right? What section 230 of the Communications Decency Act does is
[00:08:52.920 --> 00:09:02.920]   provide limited immunity for basically online platforms of all kinds to not be
[00:09:02.920 --> 00:09:07.640]   sued when their users do bad things on their platforms. And there are there
[00:09:07.640 --> 00:09:08.640]   exceptions to that.
[00:09:08.640 --> 00:09:12.480]   Actually, let's do let's talk about 236 because I think it's important that we
[00:09:12.480 --> 00:09:14.840]   understand this and I think it's often misinterpreted.
[00:09:15.440 --> 00:09:23.960]   It does not declare that social networks, blogs, internet sites are
[00:09:23.960 --> 00:09:27.480]   common carriers. It's not giving them that kind of protection. The phone
[00:09:27.480 --> 00:09:31.360]   company, for instance, is not liable for what you do on the phone. Postal
[00:09:31.360 --> 00:09:35.960]   service is not liable for what you write in it in a mail. They can't be sued.
[00:09:35.960 --> 00:09:37.520]   But this is different.
[00:09:37.520 --> 00:09:42.320]   This is different. And and it was if you go way back in time to when it was
[00:09:42.320 --> 00:09:47.200]   enacted in the 90s, there was a whole lot of hand ringing about, gee,
[00:09:47.200 --> 00:09:50.360]   this this internet that we've created in the mid 90s that people are
[00:09:50.360 --> 00:09:54.720]   starting to use is great for so many things, but it's also just a cesspool.
[00:09:54.720 --> 00:09:59.800]   And we're really worried about all the pornography and awful things. And it's
[00:09:59.800 --> 00:10:06.040]   section 230 of the Communications Decency Act. So the act itself was geared
[00:10:06.040 --> 00:10:10.520]   towards censoring the internet. And most of the act was thrown out as
[00:10:10.520 --> 00:10:16.960]   unconstitutional, except for this section, which survives. And what it does is,
[00:10:16.960 --> 00:10:19.960]   as you said, it does not create common carrier status, Leo, it just
[00:10:19.960 --> 00:10:27.400]   provides immunity, a safe harbor. So that there was a lot of hand ringing
[00:10:27.400 --> 00:10:31.560]   about that whole, well, we want the internet to be a nicer place. So part of
[00:10:31.560 --> 00:10:36.160]   the goal of section 230 and part of how it's been interpreted by the courts
[00:10:36.560 --> 00:10:42.240]   over all that time that's gone by since it was enacted is to allow websites
[00:10:42.240 --> 00:10:47.640]   to actually take steps to make their platforms less toxic and not to
[00:10:47.640 --> 00:10:53.560]   penalize them for that. So it's not that you're penalized for what goes on on
[00:10:53.560 --> 00:10:57.080]   your website. It's not that you're not going to be penalized for that. It's
[00:10:57.080 --> 00:11:00.800]   going to make it so that you don't get penalized for sense for controlling it.
[00:11:01.840 --> 00:11:07.680]   It's a little different. It's you're not penalized for all the illegal things
[00:11:07.680 --> 00:11:11.640]   that go on on your website. Now, it's not all the illegal things you can be held
[00:11:11.640 --> 00:11:16.200]   liable for some of them, like intellectual property violations or federal
[00:11:16.200 --> 00:11:24.240]   crimes. And recently, FOSTA carved out an exception to the Communications
[00:11:24.240 --> 00:11:30.200]   Decency Act provision we're talking about here, where various sex trafficking
[00:11:30.200 --> 00:11:35.040]   activity was supposed to be not only the responsibility of the people using
[00:11:35.040 --> 00:11:41.880]   the sites to spread that information, but also of the sites themselves for
[00:11:41.880 --> 00:11:47.560]   for being the distribution platform. So so that's really the distinction. Is
[00:11:47.560 --> 00:11:54.120]   it first party material or third party material? And in most instances, sites
[00:11:54.120 --> 00:12:01.480]   are immunized from what constitutes third party material. So, for example, if
[00:12:01.480 --> 00:12:07.680]   President Trump were to go on Twitter and defame someone, which many people
[00:12:07.680 --> 00:12:14.200]   would argue that he may have done at one point or another, that Twitter
[00:12:14.200 --> 00:12:17.920]   would not be liable for that. You would have to go after Trump. That's how
[00:12:17.920 --> 00:12:18.840]   230 works.
[00:12:19.240 --> 00:12:24.080]   So it is distinguished, say, from the First Amendment, the sense of protecting
[00:12:24.080 --> 00:12:25.320]   free speech.
[00:12:25.320 --> 00:12:31.800]   Very much so. Yeah. And for anyone who's interested in a really good discussion
[00:12:31.800 --> 00:12:36.560]   of the, you know, the interplay between Section 230 and free speech, Professor
[00:12:36.560 --> 00:12:44.440]   Eric Goldman is a professor at Santa Clara University. And he's, if not the
[00:12:44.440 --> 00:12:49.200]   foremost authority on Section 230, he's certainly way up there. And he's
[00:12:49.200 --> 00:12:53.920]   done a great piece on the interplay between speech and Section 230. But the
[00:12:53.920 --> 00:13:00.440]   thing, the easy primer to remember about speech, the First Amendment, in order
[00:13:00.440 --> 00:13:04.400]   what it prevents is government interference with speech.
[00:13:04.400 --> 00:13:08.960]   So Twitter's not the government. Facebook's not the government. In limited
[00:13:08.960 --> 00:13:13.280]   circumstances, courts have found that and pretty recently, actually, just last
[00:13:13.280 --> 00:13:20.240]   summer, Trump got slapped because he was blocking people from following his
[00:13:20.240 --> 00:13:27.320]   Twitter account from time to time. And the Court of Appeal, I believe it was
[00:13:27.320 --> 00:13:33.920]   the second circuit, Becky East held, yeah, no, sorry, when the president is
[00:13:33.920 --> 00:13:38.280]   tweeting, that actually creates a public forum. And so it's government action
[00:13:38.280 --> 00:13:40.160]   when he blocks someone from following him.
[00:13:40.160 --> 00:13:45.840]   That is censorship. That's a violation of the First Amendment. So it's, by the
[00:13:45.840 --> 00:13:51.120]   way, it's so, Section 230 has been so widely misinterpreted, misreported by
[00:13:51.120 --> 00:13:56.400]   even the New York Times and, you know, on every television network, it is really
[00:13:56.400 --> 00:14:00.160]   hard to know exactly what it does. It's worth reading it. It's only a few
[00:14:00.160 --> 00:14:06.520]   hundred words. It is a fundamental rule that makes the internet possible.
[00:14:07.240 --> 00:14:11.800]   It's two things, though. It is that first part, which says that no provider or
[00:14:11.800 --> 00:14:15.720]   user of an interactive computer service shall be treated as the publisher or
[00:14:15.720 --> 00:14:20.920]   speaker of any information provided by any other information contact provider.
[00:14:20.920 --> 00:14:24.440]   So that's that, that you're not responsible for stuff that's on your
[00:14:24.440 --> 00:14:29.040]   platform. But the part that is interesting, and the part that may backfire on the
[00:14:29.040 --> 00:14:34.600]   president is the second part, which says, in Section 230, an internet provider can
[00:14:34.600 --> 00:14:39.280]   restrict access to or availability of material that the provider or users
[00:14:39.280 --> 00:14:44.440]   consider to be obscene, lewd, lascivious, filthy, excessively violent, harassing,
[00:14:44.440 --> 00:14:48.520]   or otherwise objectionable, objectionable without being held liable.
[00:14:48.520 --> 00:14:54.440]   So that's what lets them take the bad stuff down.
[00:14:54.440 --> 00:14:58.680]   Right. And that goes back to, remember, the roots of this are we want to make the
[00:14:58.680 --> 00:15:00.440]   internet a nicer place, a better place.
[00:15:01.400 --> 00:15:07.240]   And to the extent that any of that survived constitutional muster, that's what survived.
[00:15:07.240 --> 00:15:12.760]   So it is the law of the land currently. Yes. No question about that. There have been
[00:15:12.760 --> 00:15:16.680]   many attempts to take it down. The Earned Act, I think, is still working its way
[00:15:16.680 --> 00:15:23.800]   through Congress that threatens providers who don't, you know, do some things that
[00:15:23.800 --> 00:15:29.800]   Attorney General Barr wants to, they would lose their Section 230 exemption.
[00:15:29.800 --> 00:15:34.840]   But there have also been other attempts. What does this EO do? Have you read the EO because
[00:15:34.840 --> 00:15:41.960]   I can't bring my... It's really long and it's really difficult to read. And yes, I've read it,
[00:15:41.960 --> 00:15:47.560]   but I've also read, we have in the rundown today, the Verge did a nice line by line
[00:15:47.560 --> 00:15:54.520]   of it, analysis, and so did Professor Goldman on his blog. So for those of you who want the
[00:15:54.520 --> 00:15:59.240]   shorter cut to the chase version, I suggest you go to either of those places.
[00:16:00.040 --> 00:16:02.280]   Okay. To understand what's going on here.
[00:16:02.280 --> 00:16:04.440]   But the upshot is...
[00:16:04.440 --> 00:16:06.360]   He, Goldman, calls it better than the First Amendment.
[00:16:06.360 --> 00:16:12.280]   Better than the First Amendment. As far as protecting the sites.
[00:16:12.280 --> 00:16:13.240]   Right. Right.
[00:16:13.240 --> 00:16:17.720]   Yeah. I hadn't thought about it that way, but yeah, I could see that.
[00:16:17.720 --> 00:16:27.640]   So what did the EO do? Did it... It's not sure what it will do, but what does it want to do?
[00:16:28.680 --> 00:16:34.040]   It wants to punt a bunch of steps. First of all, so bear in mind back to your high school civics
[00:16:34.040 --> 00:16:36.520]   class, separation of powers. Right. Yeah.
[00:16:36.520 --> 00:16:38.280]   Executive branch, judicial branch.
[00:16:38.280 --> 00:16:42.920]   That's a little muddied lately though. I have to point out, but okay. Yeah, in theory, yes.
[00:16:42.920 --> 00:16:51.080]   Legislative branch. So what it's trying to do is undo a law. Well, it knows it can't undo a law
[00:16:51.080 --> 00:16:56.440]   made by the legislative branch that's clearly constitutionally not okay.
[00:16:57.400 --> 00:17:05.400]   So what it's doing is it's tasking various other... So the Justice Department is tasked with certain
[00:17:05.400 --> 00:17:09.880]   things. The FCC is tasked with certain things. The FTC is tasked with certain things.
[00:17:09.880 --> 00:17:14.440]   By the way, there's even a question whether the president can task the FCC with anything.
[00:17:14.440 --> 00:17:15.400]   Right.
[00:17:15.400 --> 00:17:18.680]   He has no legal authority to do that. That's Congress's job.
[00:17:18.680 --> 00:17:25.640]   Right. And in any event, even if we give him the authority to make suggestions,
[00:17:25.640 --> 00:17:32.280]   they don't have to take them. Right. Right. Although, as with many other agencies, he has
[00:17:32.280 --> 00:17:37.480]   the chairman of the FCC who is somewhat of a supporter of his positions, AgitPy.
[00:17:37.480 --> 00:17:44.360]   So it's very possible that AgitPy will take this as a memo that he should act on or do something
[00:17:44.360 --> 00:17:49.160]   about. It's possible, but remember there are other commissioners and they have to vote.
[00:17:49.160 --> 00:17:51.800]   And they're half Democrat, half Republican. Right.
[00:17:52.600 --> 00:17:57.000]   Not that this is a partisan issue. Honestly, it's not a partisan issue. And the irony of it,
[00:17:57.000 --> 00:18:03.560]   as the ACLU's senior legislative counsel Kate Rohan pointed out, is that Trump has been all along a
[00:18:03.560 --> 00:18:12.360]   beneficiary of Section 230. If platforms lose their immunity, it's going to be more difficult for them
[00:18:12.360 --> 00:18:18.760]   to leave Trump's tweets and Facebook posts up. Now, it's interesting because Twitter has not
[00:18:18.760 --> 00:18:23.960]   acted about. This is the first time Twitter has done this to the president. And they didn't even
[00:18:23.960 --> 00:18:28.840]   do it to something which was in some ways more scurrilous. They've now done it again because when
[00:18:28.840 --> 00:18:33.080]   he talked about when the shooting start with the looting starts, shooting starts, they said,
[00:18:33.080 --> 00:18:37.640]   they put it up a warning on that and said, if you want to still want to read this, you have to
[00:18:37.640 --> 00:18:43.080]   click this warning. So they're now doing it more, but they hadn't been doing it much. And
[00:18:43.080 --> 00:18:51.160]   Facebook's decided not to do anything at all. But that's within their current
[00:18:51.160 --> 00:18:59.160]   power according to Section 230. Right. No one has mandated that sites do.
[00:18:59.160 --> 00:19:02.520]   They don't have to do anything. Right. Except for things like
[00:19:02.520 --> 00:19:10.120]   something else that's illegal. Right. So like Fosta, for example, that was aimed at
[00:19:10.120 --> 00:19:14.600]   trafficking. Right. Exactly. So there have been various,
[00:19:14.600 --> 00:19:22.280]   as you were pointing out, people are starting to be upset that lots of bad things can happen on
[00:19:22.280 --> 00:19:27.480]   online platforms. And they're wondering who's responsible for that? Who should be policing that?
[00:19:27.480 --> 00:19:35.800]   And so Congress is starting to draw a map that looks like more and more responsibility is going
[00:19:35.800 --> 00:19:45.240]   to be put on the sites to eliminate fake news. We have some further discussion on that maybe
[00:19:45.240 --> 00:19:52.440]   later in the show. Senator Josh Hawley of Missouri wrote a letter to Jack Dorsey saying,
[00:19:52.440 --> 00:19:56.760]   if you're going to do that to the president, we're going to take away your 230 protection,
[00:19:56.760 --> 00:20:03.240]   which seems to be completely upside down. Yeah, it's upside down to me too.
[00:20:04.040 --> 00:20:07.800]   I don't think the two things. Yeah, it's not the one leads to the other.
[00:20:07.800 --> 00:20:13.160]   It's punishment. Kind of a head scratcher. It's definitely retaliatory. Yeah, for sure.
[00:20:13.160 --> 00:20:20.440]   I don't know if there's any logical way to parse this, but I figured you're the lawyer. We'll let you
[00:20:20.440 --> 00:20:26.600]   try to figure out what's going on. And I think how people react to this somewhat does
[00:20:26.600 --> 00:20:33.480]   break down on ideological lines as much as anything else. Right. And people may get confused
[00:20:33.480 --> 00:20:38.120]   because they'll look at something that Twitter has flagged. And as you've pointed out,
[00:20:38.120 --> 00:20:42.840]   Twitter tends to be pretty hands off about what happens on the platform, except for it does have
[00:20:42.840 --> 00:20:50.760]   guidelines and restrictions trying to reduce harassment and people having terrible issues.
[00:20:50.760 --> 00:20:55.560]   I've complained about Twitter not because of any ideological slant. That's not apparent.
[00:20:55.560 --> 00:21:01.240]   Everybody gets to say whatever they want on Twitter. But just because it's a marvelous
[00:21:01.240 --> 00:21:08.120]   trolling platform, and the president is one of the masters of it, frankly.
[00:21:08.120 --> 00:21:15.720]   And there's some irony in a conservative administration deciding that now is the time to
[00:21:15.720 --> 00:21:21.720]   step in and try and dictate what private enterprise is going to do and how they're going to conduct
[00:21:21.720 --> 00:21:29.080]   business. That seems pretty cognitively dissonant to me, but that's really the crux of what's going
[00:21:29.080 --> 00:21:34.760]   on here. What can our Twitter and Facebook and anybody else entitled to make decisions about
[00:21:34.760 --> 00:21:41.960]   how to run their platform? Well, yeah. And even if your argument is, well, okay, so you're flagging
[00:21:41.960 --> 00:21:46.920]   something that the president posted and saying it needs to be fact checked. Suppose someone posted
[00:21:46.920 --> 00:21:54.040]   that John Adams was the first president of the United States. And would they flag that and say
[00:21:54.040 --> 00:21:58.520]   it needs to be fact checked? No, they don't have to. They can pick and choose what they want to
[00:21:58.520 --> 00:22:05.080]   flag check. Not incumbent upon them to make sure everything on their platform is true.
[00:22:05.080 --> 00:22:12.680]   Patrick, from your point of view in Finland and as a Frenchman, this may all must all seem strange.
[00:22:12.680 --> 00:22:21.320]   For those listening, his head is on his hand. His face palming.
[00:22:22.520 --> 00:22:29.560]   The difference we have, of course, is that we have the benefit of not having everything become a
[00:22:29.560 --> 00:22:38.360]   partisan issue immediately when we talk about any of this. And it's pretty obvious that it's
[00:22:38.360 --> 00:22:49.080]   a giant circus. But the thing that I go to when I push the thinking about all of it
[00:22:50.120 --> 00:22:57.560]   to its conclusion is a concern even by, I think, most neutral standards. Because, of course,
[00:22:57.560 --> 00:23:06.280]   this is a de jerk reaction by President Trump with little, it seems, legal footing. But when
[00:23:06.280 --> 00:23:17.320]   you go to the end, what he wants is to be able to post things on Twitter without the inaccuracies
[00:23:17.320 --> 00:23:29.240]   or the willful fudging of the truth be contradicted or indicated as inaccurate.
[00:23:29.240 --> 00:23:38.040]   And that is, to me, at least fairly concerning. Because the issue isn't really about whether or
[00:23:38.040 --> 00:23:45.640]   not President Trump lies. I think everyone agrees. Most people on the opposing side. And
[00:23:46.520 --> 00:23:54.360]   I think even most people on his side, I think the value to his constituency isn't necessarily
[00:23:54.360 --> 00:24:01.720]   the truth of his arguments, some of it maybe. But everyone knows he is a chauvin. And the idea
[00:24:01.720 --> 00:24:13.240]   that he would want his messages, his speech, to be relayed with no possibility of putting a little
[00:24:13.240 --> 00:24:23.240]   asterisk on next to it saying, maybe this isn't exactly true, is a little bit Orwellian to me.
[00:24:23.240 --> 00:24:28.520]   And I'm using the word I'm trying to be a little bit careful. I'm not saying it's the end,
[00:24:28.520 --> 00:24:37.080]   it's 1984. But the fact that he would want this message to not have the possibility,
[00:24:37.080 --> 00:24:42.360]   even though everyone agrees that there's a lot of inaccuracies in what he says all the time,
[00:24:42.360 --> 00:24:50.680]   I'm being careful here, is a concern. And when I push just a little bit further,
[00:24:50.680 --> 00:24:57.560]   I don't even think he thinks if he's being clever about this. He might not think that this is going
[00:24:57.560 --> 00:25:04.840]   to go anywhere. But what he's doing is waving a giant sword in the face of Mark Zuckerberg
[00:25:04.840 --> 00:25:11.480]   to warn him about doing something like this. Because the election, at least in his mind,
[00:25:11.480 --> 00:25:17.960]   is really going to be played on Facebook as the campaign micro targets as it has four years ago.
[00:25:17.960 --> 00:25:23.720]   And I think that might be the ultimate goal and his fear that if Facebook starts doing things like
[00:25:23.720 --> 00:25:29.720]   that, maybe it's going to affect his reelection chances, which are- I should know fairly high.
[00:25:29.720 --> 00:25:34.680]   Yeah, that's an interesting point of view. Actually, Zuckerberg, who apparently talks
[00:25:34.680 --> 00:25:39.160]   regularly with the president, in fact, even just recently, like yesterday at a phone call
[00:25:39.960 --> 00:25:46.600]   with the president, that both sides says, says, "What do they call it?" Mutually satisfactory,
[00:25:46.600 --> 00:25:53.400]   something like that, was on Fox saying, "I got to give him credit. He has a good point."
[00:25:53.400 --> 00:25:59.800]   He says, "We have a different policy than Twitter on this. I strongly believe Facebook
[00:25:59.800 --> 00:26:05.560]   shouldn't be the arbiter of truth of everything people say online. Private companies probably
[00:26:05.560 --> 00:26:09.720]   shouldn't be. Especially these platform companies shouldn't be in the position of doing that.
[00:26:09.720 --> 00:26:14.920]   I understand what he's saying. Even Twitter doesn't go through every statement on Twitter and
[00:26:14.920 --> 00:26:20.680]   market to or false. And that would be problematic if it did, true or false, according to whom.
[00:26:20.680 --> 00:26:27.080]   And so it's interesting that they have- there are many things that the president has said on
[00:26:27.080 --> 00:26:31.880]   Twitter that are not true. They don't fact check each of them. They pick and choose. And I think
[00:26:31.880 --> 00:26:37.480]   that is- let me ask Bill about this. If you like that somewhat problematic as well, I think Mark's
[00:26:37.480 --> 00:26:43.640]   point is, "How are we going to be- why should we be the ones that decide the truth or false
[00:26:43.640 --> 00:26:50.200]   hood of what's posted online?" And I think this gets to the crux of the issue as it stands now,
[00:26:50.200 --> 00:26:57.400]   because as you were talking about a little earlier, Leo, the reality I think of the internet right
[00:26:57.400 --> 00:27:06.760]   now doesn't necessarily match with the ideology. So in the early 90s when 230 was enacted,
[00:27:06.760 --> 00:27:12.360]   we wanted to be able to protect these companies to allow the internet to flourish. Everything was
[00:27:12.360 --> 00:27:17.720]   kind of new and we wanted to give people a platform without being- those companies without being sued
[00:27:17.720 --> 00:27:26.200]   to death. But now we're in a little bit different space where in order for our democracies to function,
[00:27:26.200 --> 00:27:32.200]   citizens and voters need quality information about what's happening in their world,
[00:27:32.200 --> 00:27:38.680]   about the policies that our elected officials are enacting, so that then we can make educated
[00:27:38.680 --> 00:27:46.120]   decisions about who we want to put into office. And so what a lot of these platforms have become
[00:27:46.120 --> 00:27:53.560]   is not the blogs, not the bulletin board services, not these kind of platforms for discussion,
[00:27:54.120 --> 00:28:00.120]   but they've actually morphed now to become news outlets. And so when you look at the percentage
[00:28:00.120 --> 00:28:06.120]   of people who get there what they would call news from a social media platform,
[00:28:06.120 --> 00:28:15.160]   it's the importance of that news being accurate becomes- is greatly increased,
[00:28:15.160 --> 00:28:22.760]   because as we saw with the 2016 election here in the US, as we've seen with the Brexit election in
[00:28:22.760 --> 00:28:30.040]   the UK, because of the way the platforms operate, it allows for influence, fake information,
[00:28:30.040 --> 00:28:37.800]   for opinions to be- to enter echo chambers and be elevated for people that aren't really who are
[00:28:37.800 --> 00:28:43.960]   busy, who don't have the time, the effort, or the energy to sort of do their own research,
[00:28:43.960 --> 00:28:48.680]   or who are just looking for an outlet that promotes the beliefs that they already have.
[00:28:50.360 --> 00:28:57.160]   We sort of run into problems now because I think these platforms have become something that maybe
[00:28:57.160 --> 00:29:02.760]   they weren't- we didn't think they would be back in the 90s. And there is a double standard. So,
[00:29:02.760 --> 00:29:09.240]   you know, Leo, if you as a content creator, if me, you know, I- I- I- I take Republic and part of
[00:29:09.240 --> 00:29:16.680]   ViacomCBS, a large media organization, if someone goes on the CBS Evening News and puts out something
[00:29:16.680 --> 00:29:22.280]   that's fake, we can be held accountable for that. If you don't disclose something-
[00:29:22.280 --> 00:29:25.240]   You're a publisher. Not a platform. We're a publisher. Yeah.
[00:29:25.240 --> 00:29:29.720]   So, Facebook has said repeatedly, Twitter has said repeatedly, we're not publishers,
[00:29:29.720 --> 00:29:37.320]   we're just hosting this platform and individuals that are posting material to this are actually,
[00:29:37.320 --> 00:29:42.680]   quote-unquote, the publishers. But there's no mechanism to police that and you are right. You
[00:29:42.680 --> 00:29:49.400]   know, I don't know if we want private companies, you know, gauging what is and what is not fact,
[00:29:49.400 --> 00:29:55.080]   gauging what is and what is not protected by the First Amendment. But I think there is a lot of
[00:29:55.080 --> 00:30:01.480]   frustration right now among people of a variety of political persuasions that something needs to
[00:30:01.480 --> 00:30:08.440]   change. Now, what that is, I don't know. Maybe it's- if it's a pullback from Facebook and Twitter
[00:30:08.440 --> 00:30:13.080]   in the pocketbook because I think that's really the only thing that's going to, you know,
[00:30:13.080 --> 00:30:20.280]   change behavior when it comes to corporations. It's so complicated and so difficult.
[00:30:20.280 --> 00:30:25.960]   I don't think that it's appropriate for Facebook or Twitter to become the arbiter of truth because
[00:30:25.960 --> 00:30:31.080]   that's a lot more than they're required. On this- at the same time, I would expect tech
[00:30:31.080 --> 00:30:37.400]   republic, your publication, to try to only publish things that are true and to point people in the
[00:30:37.400 --> 00:30:42.920]   direction of truth. And I'm sure that's your goal. Certainly our goal here at TWIT to get to the truth.
[00:30:42.920 --> 00:30:49.080]   That's a big difference. But it's interesting, you know, Mike Masnick's been very good on this
[00:30:49.080 --> 00:30:55.800]   tech dirt. And he has said that the distinction that published or platform distinction that we're
[00:30:55.800 --> 00:31:01.960]   having such a hard time with is Facebook a platform. Is it a publisher? It seems to be both. He says,
[00:31:01.960 --> 00:31:07.480]   I'm going to read the quote. This publisher versus platform concept is a totally artificial
[00:31:07.480 --> 00:31:14.440]   distinction that has no basis in the law. News publishers like you, Bill, are protected as well
[00:31:14.440 --> 00:31:20.600]   by section 230 of the CDA. All the CDA 230 does is protect a website from being held liable for
[00:31:20.600 --> 00:31:26.120]   user content or moderation choices. It does not cover content created by the company itself.
[00:31:26.840 --> 00:31:34.360]   So you can be both the distinctions platform versus publisher, its content creator or content
[00:31:34.360 --> 00:31:42.920]   intermediary. And so you are both at the same time, aren't you? Yeah, we have blogs. We allow people
[00:31:42.920 --> 00:31:51.240]   to post to our community forums. We do our best to moderate those forums. But we are both a publisher
[00:31:51.240 --> 00:32:00.600]   and a website, a platform protected by 230. I think the issue that we as a society,
[00:32:00.600 --> 00:32:08.200]   as internet consumers, as citizens have to grapple with is where we get our news from.
[00:32:08.200 --> 00:32:13.800]   Because I think if you ask a lot of the average people like, where did you get, where did you
[00:32:13.800 --> 00:32:20.520]   see this story? What's on on Facebook? To them, Facebook is the source for that information.
[00:32:20.520 --> 00:32:28.920]   Whether it's posted by the Washington Post, the New York Times, Twit, Tech Republic, the Verge,
[00:32:28.920 --> 00:32:36.040]   CNET, or their crazy relative who lives in a cabin in the middle of nowhere.
[00:32:36.040 --> 00:32:43.560]   I think sometimes it's a distinction without a difference. And we have to live in the real
[00:32:43.560 --> 00:32:46.840]   world and kind of look at the reality. And I don't know what the answer is.
[00:32:46.840 --> 00:32:50.360]   Well, that's very challenging. It's a hard question to answer.
[00:32:50.360 --> 00:32:51.720]   Yeah, but Denise, I...
[00:32:51.720 --> 00:32:55.880]   Just a failure of education though. I mean, without the failure of education.
[00:32:55.880 --> 00:33:01.720]   Well, it is. So I don't want to have to place the burden on Mark Zuckerberg to tell me what's true
[00:33:01.720 --> 00:33:09.960]   or not. Isn't it the ideal world that both Twitter, Zuckerberg, Facebook are just platforms,
[00:33:09.960 --> 00:33:16.200]   platforms, platforms. They provide a place for people to publish. It's not their obligation or job
[00:33:16.200 --> 00:33:23.720]   to in any way editorialize upon that. It's our job as consumers of the content
[00:33:23.720 --> 00:33:31.000]   to do that. And if you want to, as an editorial function, Bill or me on Twit,
[00:33:31.000 --> 00:33:36.280]   then say, "Hey, you think so? That thing you read on Facebook, that's wrong. That's appropriate
[00:33:36.280 --> 00:33:43.480]   in a good function of the press. But why should Facebook or Twitter be required to do that?
[00:33:43.480 --> 00:33:46.840]   Are they even able to do it or not?
[00:33:46.840 --> 00:33:55.320]   Yeah. So a related legal point to bring up here is that it is so hard and so difficult for the
[00:33:55.320 --> 00:34:02.600]   people tasked with the role of moderating Facebook. I don't know if you remember a while ago, but when
[00:34:04.360 --> 00:34:11.560]   we had terrible terrorist attacks that were going forth live on Facebook and various other things,
[00:34:11.560 --> 00:34:17.960]   right around then is when a lawsuit ensued from the moderators who said, "You know what? I mean,
[00:34:17.960 --> 00:34:25.880]   we are subjected to so much terrible talk with stuff that we have PTSD and various other forms of
[00:34:25.880 --> 00:34:32.520]   mental health issues. Yeah, exactly. We're job in the world. Well, Facebook just recently settled
[00:34:32.520 --> 00:34:39.800]   for $52 million with its moderators. And so there's some acknowledgement that,
[00:34:39.800 --> 00:34:46.600]   yeah, they've got a bad job that Facebook just didn't want to go forward with that lawsuit.
[00:34:46.600 --> 00:34:52.280]   So hard though, because if a terrorist organization is recruiting on Facebook,
[00:34:52.280 --> 00:34:55.400]   you want Facebook to stop that? Yeah. Exactly.
[00:34:55.400 --> 00:34:58.280]   You do. You do. But I think--
[00:34:58.280 --> 00:34:59.880]   Where do we draw the line?
[00:34:59.880 --> 00:35:03.000]   I think these platforms have to pick and choose. And since we're talking about the
[00:35:03.000 --> 00:35:11.880]   First Amendment, that's something that's come up in this executive order and the ramifications
[00:35:11.880 --> 00:35:21.720]   of a retaliatory trying to scale back to 30 as punishment for labeling tweets with various
[00:35:21.720 --> 00:35:26.360]   comments from Twitter or information from Twitter, depending on your perspective,
[00:35:27.160 --> 00:35:34.600]   is that private enterprises, in addition to not being responsible for protecting the First
[00:35:34.600 --> 00:35:38.520]   Amendment on their platform, they actually have First Amendment rights. There is such a thing
[00:35:38.520 --> 00:35:45.720]   as commercial speech. It's entitled to lesser protection than non-commercial speech. But if you
[00:35:45.720 --> 00:35:52.040]   have been paying attention over the last several days to the messages from companies like Nike,
[00:35:52.040 --> 00:36:03.800]   for example, or YouTube, who are putting a moment of we're acknowledging the horrific
[00:36:03.800 --> 00:36:10.040]   events that happened in Minneapolis, and we're providing resources for people to learn more
[00:36:10.040 --> 00:36:14.760]   and contribute. And this is all commercial speech that they are engaged in, as they are
[00:36:14.760 --> 00:36:18.600]   pointing people towards these resources. Can they lie in it?
[00:36:20.040 --> 00:36:26.520]   Can they lie? Can you lie in an ad? The FTC does have false advertising.
[00:36:26.520 --> 00:36:31.240]   Yeah, the FTC does restrict certain things, but not on a speech basis. You just,
[00:36:31.240 --> 00:36:36.840]   yeah, that's misleading, Ed. Right. Stuff. Yeah.
[00:36:36.840 --> 00:36:42.840]   I mean, we've always had restrictions on speech. I mean, in the United States, I mean, everyone loves,
[00:36:42.840 --> 00:36:49.320]   what gets me is everyone says, oh, everything is protected by the first. It's not. You can't
[00:36:49.320 --> 00:36:56.520]   incite violence. You can't go into crowded theater and scream fire. That's the example that my old
[00:36:56.520 --> 00:37:02.440]   law professors used to give us back in college. So there are restrictions on speech. You can't
[00:37:02.440 --> 00:37:09.720]   commit slander. You can't commit libel. So, you know, but I think that gets lost on a lot of people.
[00:37:09.720 --> 00:37:17.480]   Because, you know, they don't know or they don't take the time to know or they only support the
[00:37:17.480 --> 00:37:23.720]   ideology of whatever the person who's saying it, whether or not they're on their team, their side,
[00:37:23.720 --> 00:37:26.280]   they're followed their philosophies.
[00:37:26.280 --> 00:37:35.880]   Patrick, it, yeah, I think we've made it pretty clear that this is an impossible.
[00:37:35.880 --> 00:37:45.080]   Yeah. It's challenging. Right. Yeah. There seems to be no clear solution. And also, as have been made
[00:37:45.080 --> 00:37:54.600]   very eloquently by Bill, this is a key issue that our society is faced with. And potentially,
[00:37:54.600 --> 00:38:02.840]   we think with repercussions on the quality of our democracy, potentially. And
[00:38:02.840 --> 00:38:11.480]   I think that since we don't have a clear solution, it should be and, again, as legally has been
[00:38:11.480 --> 00:38:19.560]   pointed out by Denise, those companies have a right to police the content on their services.
[00:38:19.560 --> 00:38:28.760]   It's pretty clear that both approaches are valid. The approach of saying, well, it's democratically
[00:38:28.760 --> 00:38:35.080]   should be, we shouldn't be policing the truth. And the approach of saying, we think we need to,
[00:38:35.800 --> 00:38:42.280]   in some cases, touch up on the content to indicate when it might be misleading.
[00:38:42.280 --> 00:38:50.360]   And the key here is that both should be allowed to exist. I think both are defensible. Both are
[00:38:50.360 --> 00:39:00.040]   valid. And the idea that we would want to, as a society, prevent one from existing, meaning,
[00:39:00.040 --> 00:39:07.000]   prevent companies from putting the little asterisk, saying, we're not sure about this content,
[00:39:07.000 --> 00:39:14.120]   saying this cannot happen. All content must be required by the government, must be left untouched,
[00:39:14.120 --> 00:39:21.320]   is what is the most concerning. Do we want to, I'm going to put the question back to all of you,
[00:39:21.320 --> 00:39:27.640]   do we want the government to mandate that all social networks cannot touch the content that
[00:39:27.640 --> 00:39:32.200]   is on their platforms, all of them? I don't think that's something.
[00:39:32.200 --> 00:39:39.240]   No, and that's the paradox of the executive order. I think what the president really would
[00:39:39.240 --> 00:39:47.800]   like to say is you can't touch my content. Can you think of a human being on the planet who has a
[00:39:47.800 --> 00:39:55.000]   larger pulpit than the president of the United States? And if you don't like a social media
[00:39:55.000 --> 00:40:01.800]   platform that you're using and you don't like the way that they're moderating or treating what you're
[00:40:01.800 --> 00:40:06.040]   attempting to get out via that platform, if you are the person with that pulpit,
[00:40:06.040 --> 00:40:08.760]   use your pulpit. You don't have to stay on Twitter.
[00:40:08.760 --> 00:40:17.480]   To take the devil's advocate point, it could be argued, I think, that Twitter and Facebook are
[00:40:17.480 --> 00:40:24.200]   two very special social networks that almost have status of like, they're unique. They're so
[00:40:25.000 --> 00:40:36.040]   widely used that I wonder if it could be argued philosophically, not legally, that it's impossible
[00:40:36.040 --> 00:40:41.560]   to not be there if you want to be heard and be present. I think that's the case. I think if you're
[00:40:41.560 --> 00:40:49.480]   president it is. Well, yeah, you can command network time anytime you want. But as you pointed out,
[00:40:49.480 --> 00:40:53.720]   Denise, one of the points of this is to keep, or somebody, maybe Bill, one of the points of this
[00:40:53.720 --> 00:40:58.680]   is to keep Facebook in line so that the president's campaign can proceed.
[00:40:58.680 --> 00:41:04.200]   Everyone used Friendster to, you know, their platforms all the time.
[00:41:04.200 --> 00:41:12.040]   It's yeah. I think that also, though, once you start opinionizing on content on Twitter or Facebook,
[00:41:12.040 --> 00:41:17.480]   you get in this problem where people, nobody's going to be happy with what you say. The
[00:41:17.480 --> 00:41:20.040]   rights are going to say, oh, you shouldn't be doing that. The left's going to say, oh, you should
[00:41:20.040 --> 00:41:27.400]   be doing that. It's a very difficult and narrow line to walk. I don't know how you do it effectively.
[00:41:27.400 --> 00:41:31.640]   And you're doing it so selectively at this point is ridiculous.
[00:41:31.640 --> 00:41:39.400]   So this is the since that question is impossible to answer. I think the most non-partisan way to
[00:41:39.400 --> 00:41:46.760]   look at it is again, that question. Do we want the government to mandate that social networks
[00:41:46.760 --> 00:41:54.440]   cannot touch the content at all? Right. And that question is non-partisan. It isn't.
[00:41:54.440 --> 00:42:00.520]   And if your answer is, yes, we want all social networks to not be able to curate anything and
[00:42:00.520 --> 00:42:06.840]   touch anything. It is, I think, a great concern. Nobody wants that. Everybody wants what the
[00:42:06.840 --> 00:42:13.080]   president wants. They want the social networks to flag stuff they disagree with and promote
[00:42:13.080 --> 00:42:18.840]   stuff they agree with. And that's how section 230 came about. Before it existed, there was a case
[00:42:18.840 --> 00:42:25.880]   that said, hey, if you're going to claim that you have no responsibility for what's going on on
[00:42:25.880 --> 00:42:31.720]   your platform, then you can't be involved at all. You can't moderate. You can't regulate the
[00:42:31.720 --> 00:42:35.960]   content there at all. You have to take a completely hands-off approach. And if you are involved in
[00:42:35.960 --> 00:42:41.640]   moderating, then you're complicit in what's there. It was called the moderator's dilemma.
[00:42:41.640 --> 00:42:46.360]   And section 230 was specifically enacted to eliminate that and make sites feel like,
[00:42:46.360 --> 00:42:53.000]   some of this bad stuff and not be sued. This battle, exactly parallels the battle
[00:42:53.000 --> 00:42:59.880]   that's gone on for years on forums and chat rooms and between trolls and moderators.
[00:42:59.880 --> 00:43:04.360]   It's exactly the same conversation. It's just now that it's out of the White House
[00:43:05.400 --> 00:43:12.840]   instead of the troll house. And you know, there are a number of social networks and services
[00:43:12.840 --> 00:43:18.600]   that have complained about the stifling of their free speech. And they've gone and created
[00:43:18.600 --> 00:43:25.240]   the free speech social network and the free payment system. I wasn't going to mention them.
[00:43:25.240 --> 00:43:34.360]   And you know what content's on there. The worst. We don't want that to become all of Facebook,
[00:43:34.360 --> 00:43:40.120]   at all of Twitter. We want them to be able to, you know, curate at least a little bit.
[00:43:40.120 --> 00:43:46.520]   In a nutshell, this is the paradox of our modern time. These platforms can and continue to be able
[00:43:46.520 --> 00:43:52.600]   to provide a great space for people to discuss the most important issues of the day, the issues
[00:43:52.600 --> 00:43:56.520]   that we may not agree upon, but at least to bring it out in the light and have a discussion and try
[00:43:56.520 --> 00:44:03.800]   to come to a conclusion about it. I mean, they really offer exactly what the founding fathers
[00:44:03.800 --> 00:44:08.840]   wanted in a democracy. And at the same time, they can be used in the most horrific,
[00:44:08.840 --> 00:44:15.560]   horrible, suppressive ways as well. And it's such a paradox. And I don't think you want to
[00:44:15.560 --> 00:44:21.240]   stop one in favor of the other. I don't know if you even can. I don't know what the answer is.
[00:44:21.240 --> 00:44:27.560]   I think that's the answer. But maybe I'm just stupid. I can't think of a way to solve this,
[00:44:27.560 --> 00:44:32.520]   to be honest. And Leo, you and then we haven't even talked about it here, but then there's the
[00:44:32.520 --> 00:44:37.400]   technological answer. Denise brought it up earlier in talking about the people, the
[00:44:37.400 --> 00:44:43.000]   moderators that had the watch, all this horrific content and they were traumatized by it.
[00:44:43.000 --> 00:44:49.080]   So you look at some of these platforms that look at algorithmic ways to filter content,
[00:44:49.080 --> 00:44:57.480]   because imagine how many posts are put on Facebook or put on Twitter, if you had a human actually
[00:44:57.480 --> 00:45:03.240]   look, there is no way to do that without hiring armies and armies of people. I mean, I don't even
[00:45:03.240 --> 00:45:08.760]   know if it's physically possible to do that. So you look at these algorithmic ways, and then you
[00:45:08.760 --> 00:45:15.640]   have to look at, okay, what are the biases that creep into the algorithms? Are there, how effective
[00:45:15.640 --> 00:45:22.040]   are they? So there's a whole nother layer on to the argument beyond just the philosophical.
[00:45:23.000 --> 00:45:29.000]   Yeah. You know, that made me think of a law that just got passed in France, that goes the exact
[00:45:29.000 --> 00:45:40.200]   opposite direction. It mandates that social networks take away within 24 hours of publication or of
[00:45:40.200 --> 00:45:50.840]   flagging any content that is obviously illegal. And the obviously is very important that anyone
[00:45:50.840 --> 00:45:55.480]   would agree is illegal. But it's interesting that in France, we're saying you have to do more,
[00:45:55.480 --> 00:46:01.560]   and you have to find a way we don't care how you do it, moderation or whatever, just hire an army
[00:46:01.560 --> 00:46:11.400]   of moderator if you have to, of moderators. But yeah, in France, they now, it's getting into effect
[00:46:11.400 --> 00:46:17.560]   in July, social networks have to take off the content that is flagged. And that is obviously
[00:46:17.560 --> 00:46:23.000]   illegal within 24 hours. Hmm. Yeah, that's better, honestly. I don't know.
[00:46:23.000 --> 00:46:26.200]   Well, it's all a great experiment. We'll find out. Yeah.
[00:46:26.200 --> 00:46:32.520]   Let us know how it goes back. Yeah, yeah, give us a call back in a few months. Yeah.
[00:46:32.520 --> 00:46:37.800]   Let's take a little break. I do what we're talking about. Happy stuff. There was a very happy event
[00:46:37.800 --> 00:46:44.040]   that we all watched together yesterday. And we'll talk about that. That might be something
[00:46:44.040 --> 00:46:50.200]   to cheer us up in a minute. But first, a word from our sponsor, as they say. And in this case,
[00:46:50.200 --> 00:46:57.480]   it's masterclass. You know, sometimes I, I, and I love masterclass because I can, I can watch it on
[00:46:57.480 --> 00:47:01.480]   my Apple TV. You can watch it on a computer and your phone. And sometimes I just, there's nothing
[00:47:01.480 --> 00:47:07.080]   to us. There's nothing on the air that suits. And I will turn to masterclass. And I invariably
[00:47:07.880 --> 00:47:15.080]   learn something. I'm entertained. I enjoy. And I think a lot of that goes to the 75 plus amazing
[00:47:15.080 --> 00:47:22.840]   people who teach at masterclass. How about learning about the art of negotiation from a former FBI
[00:47:22.840 --> 00:47:30.200]   hostage negotiator, Chris Woss, or presidential history from Doris Kearns Goodwin? How about learning
[00:47:30.200 --> 00:47:37.960]   about interior design from Kelly Wersler, who's a very well-known designer or advertising from
[00:47:37.960 --> 00:47:44.440]   Goodby and Silverstein? You're learning from the very best. As you can see here, I'm watching Steve
[00:47:44.440 --> 00:47:53.400]   Martin's fabulous series on comedy. I am not a funny guy, but Steve sure gave me some great ideas.
[00:47:53.400 --> 00:47:59.880]   I'm never going to dance ballet, but watching Misty Copeland teach you about ballet is phenomenal.
[00:47:59.880 --> 00:48:06.920]   Aaron Sorkin on screenwriting, Samuel L Jackson on acting, Judd Apatow on comedy writing.
[00:48:06.920 --> 00:48:11.800]   I can go on and on. How about Thomas Keller, one of the world's greatest chefs,
[00:48:11.800 --> 00:48:20.200]   or Gary Kasparov, the world chess champion. Teaching you chess. This is the beauty of masterclass.
[00:48:20.200 --> 00:48:26.520]   And some of the new stuff like David Sedaris just started his course. It's so fantastic.
[00:48:26.520 --> 00:48:33.240]   They're beautifully shot. Jody Foster, teaching acting. They're beautifully shot.
[00:48:33.240 --> 00:48:39.560]   They're absolutely entertaining. And yet at the same time,
[00:48:39.560 --> 00:48:47.320]   oh man, I just have to say I love these. I love these. Now, I want you to get involved in masterclass.
[00:48:47.320 --> 00:48:54.360]   We've got a great deal for you. This is a wonderful gift for anybody who is perhaps a graduate.
[00:48:54.360 --> 00:49:02.120]   You don't got any graduates in your life right now, or maybe Father's Day is coming up. Masterclass
[00:49:02.120 --> 00:49:08.120]   lets you learn from the best. You get exclusive access to online classes taught by the masters.
[00:49:08.120 --> 00:49:16.040]   Mexican cooking from Gabriela Kamara. I get so hungry just watching the classes. Look at that.
[00:49:17.560 --> 00:49:23.640]   The editor of Vogue and a winter teaching about fashion, gardening from Ron Finley.
[00:49:23.640 --> 00:49:29.160]   Annie Leibov, it's teaching you how to do portrait photography. It's an immersive learning experience,
[00:49:29.160 --> 00:49:34.680]   classes on all kinds of topics, only the best people teaching. I mean, just amazing.
[00:49:34.680 --> 00:49:39.320]   Each class is broken down into individual video lessons. There's downloadable materials.
[00:49:39.320 --> 00:49:43.960]   You explore your own pace. I like the, you can binge them, of course,
[00:49:45.080 --> 00:49:48.920]   but they're 10 to 15 minutes in length. So all it takes is 10 or 15 minutes.
[00:49:48.920 --> 00:49:53.960]   You get cooking classes. For instance, come with beautiful downloadable guides. They look like a
[00:49:53.960 --> 00:50:01.000]   cookbook. Beautiful high level cookbook. Masterclass is amazing. Masterclass is the way to learn
[00:50:01.000 --> 00:50:07.320]   Chris Hadfield. You want to talk about space, Commander Hadfield's on there. David Lynch.
[00:50:07.320 --> 00:50:14.680]   Some of the people you've heard about and thought about for years. You ever been to Franklin's barbecue
[00:50:14.680 --> 00:50:18.360]   in Austin? Of course not because the line starts at six in the morning,
[00:50:18.360 --> 00:50:23.320]   but you can get Aaron Franklin teaching you had a smoke. Oh, baby.
[00:50:23.320 --> 00:50:30.680]   Oh, I'm a mouse warning. Just looking at him, slicing that brisket. Masterclass, get unlimited
[00:50:30.680 --> 00:50:35.720]   access to every masterclass. And because you're listening to the show, you'll get 15% off the annual
[00:50:35.720 --> 00:50:40.840]   All Access Pass when you go to masterclass.com/twit and really consider this as a gift.
[00:50:41.560 --> 00:50:45.880]   This I tried. I was going to give it to my son when he graduated from college. He said, I already
[00:50:45.880 --> 00:50:57.960]   have it dead. Masterclass.com/twit. 15% off. Gordon Ramsey teaching. Now Gordon could teach either how
[00:50:57.960 --> 00:51:05.560]   to cook or how to yell at cooks. I'm not sure. I have to watch that one. Masterclass.com/twit.
[00:51:05.560 --> 00:51:11.160]   I love it. You will too. What a great gift. And if you've got some time at home these days,
[00:51:12.120 --> 00:51:16.280]   this might be a good way to spend it. Something productive. Learn to shoot three-point
[00:51:16.280 --> 00:51:22.120]   dunks with Steph Curry. Speaking of watching Saturday, did you all watch the launch?
[00:51:22.120 --> 00:51:30.200]   Yes. I feel like we needed something to lift us off this sad planet.
[00:51:30.200 --> 00:51:34.520]   And the other thing that's so cool is, and I don't know if,
[00:51:37.000 --> 00:51:42.280]   I don't know if this is, well, it's 21st century space. First time US has launched
[00:51:42.280 --> 00:51:49.320]   humans into space since 2011. Of course, it's going to look futuristic and modern and straight out of
[00:51:49.320 --> 00:51:55.640]   a sci-fi movie. Now I can't show any video, by the way. I showed it on the tech guy yesterday.
[00:51:55.640 --> 00:52:00.520]   I was, we were showing from the NASA channel, which I believe is a government
[00:52:01.400 --> 00:52:06.840]   channel paid for by our tax dollars. For some reason, National Geographic thinks it owns it.
[00:52:06.840 --> 00:52:13.800]   So National Geographic, if I put any video from the launch, apparently National Geographic,
[00:52:13.800 --> 00:52:18.840]   and I'm not alone, people all over YouTube are complaining about this one, they claim to own
[00:52:18.840 --> 00:52:26.760]   that footage, which we all paid for. But congratulations to Elon Musk. It's a beautiful spacecraft.
[00:52:27.560 --> 00:52:33.400]   Oh, I got to show the, I got to show a picture. I can't show the video, can I show a picture?
[00:52:33.400 --> 00:52:41.560]   Okay, imagine. It's a very good example of how machine moderation doesn't always work perfectly.
[00:52:41.560 --> 00:52:48.520]   Out of control. But isn't, so this is very, I have a feeling, I mean, Elon's got to have some
[00:52:48.520 --> 00:52:54.680]   influence on this. Like the, the astronauts, Doug and Bob, get in the, get in the Dragon X,
[00:52:55.640 --> 00:52:59.320]   and they're all suited up. And then they press a button and just like in a car,
[00:52:59.320 --> 00:53:06.280]   the seats rotate back. So the giant touch screens are right in front of them.
[00:53:06.280 --> 00:53:12.680]   It looks like the inside of a Tesla. All molded, is it plastic? I must be molded plastic. It's
[00:53:12.680 --> 00:53:20.200]   beautiful. The spacesuits alone look like something out of a interstellar or something.
[00:53:22.280 --> 00:53:26.360]   Thought. The thing that got me, like how much light there was coming into the capsule.
[00:53:26.360 --> 00:53:31.880]   Yeah. I mean, I don't, you know, I remember watching videos of the old Apollo launches and
[00:53:31.880 --> 00:53:38.280]   everything looked so cramped and so tight and these tiny windows. But then you looked at the
[00:53:38.280 --> 00:53:44.360]   Dragon capsule and you saw what appeared to be large windows and light coming in everywhere.
[00:53:44.360 --> 00:53:50.600]   Streaming it. It was bright. Yeah. This is, there was also, as soon as they got at zero g,
[00:53:51.480 --> 00:53:58.120]   a little, little dragon started floating through the cabin, which I thought was cool.
[00:53:58.120 --> 00:54:03.480]   That was apparently chosen by the crew's sons. That was their indicator that they were in zero g.
[00:54:03.480 --> 00:54:10.200]   I think they said it was an apatosaurus. It's an apatosaurus. A sequined apatosaurus.
[00:54:10.200 --> 00:54:15.320]   I'm sure that that was a younger son who said that it's really, can I show this?
[00:54:17.960 --> 00:54:24.360]   Okay. I feel like I'm, there's a definite chilling, chilling effect. Thank you, National Geographic.
[00:54:24.360 --> 00:54:31.240]   It, it's pretty cool. Pretty cool. I don't know if it, I think in some ways it could backfire
[00:54:31.240 --> 00:54:35.000]   because there are some small number of people who think the whole thing's fake anyway,
[00:54:35.000 --> 00:54:41.080]   who are going to just use all of this and say, look, obviously, it's completely fake.
[00:54:41.080 --> 00:54:44.520]   I don't think they do that. No, I guess not.
[00:54:46.920 --> 00:54:52.600]   It really was a wonderful thing to watch. I watched it with a friend of mine and we were talking,
[00:54:52.600 --> 00:54:57.960]   we were both watching it while we were streaming, flipping back and forth between either what was
[00:54:57.960 --> 00:55:05.880]   on cable or on YouTube or, and it was really interesting to sort of watch that with a friend
[00:55:05.880 --> 00:55:11.560]   of mine who are both in the space and kind of have a shared experience together and really sort of
[00:55:11.560 --> 00:55:17.640]   see something that we haven't seen in almost a decade. And I grew up with the shuttle,
[00:55:17.640 --> 00:55:27.400]   too young to remember the Apollo missions, but there's still that kind of pull to space.
[00:55:27.400 --> 00:55:36.040]   And if I had the $30 million or whatever it cost to take a private ride to space, I'd be right there.
[00:55:36.600 --> 00:55:42.760]   I would too. I'm a little too old now to do this, but boy, what an amazing thing. Are they aboard?
[00:55:42.760 --> 00:55:47.080]   John would know because he's our space expert. They're aboard the ISS now. The docking was earlier.
[00:55:47.080 --> 00:55:55.240]   It was this morning. Yeah, and it went beautifully. Of course, the Dragon X has autopilot.
[00:55:57.000 --> 00:56:04.200]   I was able to dock autonomously. Really, it's, you know, I have to say,
[00:56:04.200 --> 00:56:14.920]   it's not, you could probably make the case that we don't need to do this or, you know,
[00:56:14.920 --> 00:56:20.840]   maybe we should, the money would be better spent solving our myriad obvious problems on earth.
[00:56:20.840 --> 00:56:26.520]   But I think there is some value to looking to the stars and giving us hope in some of our
[00:56:26.520 --> 00:56:32.680]   darkest hour. And I certainly felt that way yesterday watching the launch. And it was just
[00:56:32.680 --> 00:56:39.320]   quite amazing. This is the inside of the Dragon X capsule, which I swear to God looks just like a
[00:56:39.320 --> 00:56:47.720]   Tesla. And I love that they rode out in Teslas. Yeah. Elon really gets how to put on a show.
[00:56:47.720 --> 00:56:52.760]   Well, he's a bit who's the better marketer? NASA or Elon? They're, you know, really, they're
[00:56:52.760 --> 00:56:59.560]   both pretty darn good. Yeah. That NASA's administrator, Jim, I don't know how to say his last name,
[00:56:59.560 --> 00:57:07.480]   Bridenstein, maybe. Yeah. He was so eloquent and inspiring and just professional and polished.
[00:57:07.480 --> 00:57:13.560]   And I was really impressed by him. And then my other just favorite takeaway was as there,
[00:57:13.560 --> 00:57:20.520]   as he, Jim was being so eloquent and saying diplomatic things about international partners
[00:57:20.520 --> 00:57:28.200]   in response to a press question, Elon quit. The trampoline is working, which refers to the fact
[00:57:28.200 --> 00:57:34.520]   that a senior Russian official in their space program during the middle of all the tensions
[00:57:34.520 --> 00:57:39.960]   between the US and Russia and debate about whether they were still going to take our astronauts up,
[00:57:39.960 --> 00:57:45.880]   said maybe we should use a trampoline to send astronauts to space. Oh, Lord.
[00:57:48.040 --> 00:57:53.880]   The 27 inch touch screens. I think that's a nice touch. Remember those?
[00:57:53.880 --> 00:57:59.000]   And Bill, I am old enough to have gone all the way back to Mercury watching every single launch
[00:57:59.000 --> 00:58:06.120]   with Walt Uncle Walter Cronkite watching the as man goes to the moon. And they had toggle
[00:58:06.120 --> 00:58:11.960]   switches and dials. These things were these were like the craziest. They looked like Soviet Union
[00:58:11.960 --> 00:58:19.640]   power plants. Now it's a lot cleaner. And they even have little Oh, crap handles for the astronauts
[00:58:19.640 --> 00:58:25.000]   with switches on them so they can hold on. Plus, I'm very gratified to see that you no longer have
[00:58:25.000 --> 00:58:35.000]   to have perfect eyesight to be an astronaut. So there's still hope for for us blind old guys.
[00:58:35.000 --> 00:58:40.920]   Well, these guys are terribly young. No, but they are test pilots, which is about the
[00:58:41.720 --> 00:58:46.200]   from my point of view, the highest, highest rank you can achieve in life.
[00:58:46.200 --> 00:58:51.320]   And the farthest away from our yeah, the cool. Yeah, exactly.
[00:58:51.320 --> 00:58:59.320]   You know, I wonder if they have failover systems for these giant touch screens,
[00:58:59.320 --> 00:59:03.640]   because in my experience, those sometimes don't work.
[00:59:05.480 --> 00:59:13.400]   Can you imagine the stresses? I mean, these are just these look like LCD panels. They must be
[00:59:13.400 --> 00:59:20.440]   specially designed to handle the the G forces, the shi shaking and all of that. But I do wonder if
[00:59:20.440 --> 00:59:29.320]   switches might not be more reliable. You know, that that makes me think the the stark difference
[00:59:29.320 --> 00:59:38.600]   between first of all, what you said, you know, it's it's pretty obvious, but it didn't we didn't
[00:59:38.600 --> 00:59:44.360]   need to go to the moon, although there was the space race and the geopolitical aspect of it. But
[00:59:44.360 --> 00:59:50.760]   there were issues on earth back then too, that might have been better to deal with. And it doesn't
[00:59:50.760 --> 00:59:56.440]   mean you can't, you know, think like that, or you'd never do anything at all ever about anything.
[00:59:56.440 --> 01:00:06.200]   Right. But it but it didn't make me think about Bill Gates nuclear power plants projects,
[01:00:06.200 --> 01:00:12.200]   which he is. By the way, the early Apollo spacecraft. I just thought I should know it's not.
[01:00:12.200 --> 01:00:21.000]   That is literally a Soviet Union era nuclear power plant. This is actually a fantastic series of
[01:00:22.120 --> 01:00:29.320]   Soviet control rooms at the designutrust.com. If you love gauges, knobs and dials,
[01:00:29.320 --> 01:00:36.600]   that's impressive. But really, the spacecraft didn't look like that. Go ahead. I'm sorry.
[01:00:36.600 --> 01:00:42.200]   Yeah, exactly. No, exactly. That's that's the whole point. The spacecrafts looked like that.
[01:00:42.200 --> 01:00:48.520]   And the power plants were designed in the 60s and 70s. The power plants were still using today.
[01:00:48.520 --> 01:00:55.240]   And Bill Gates argument is that if we designed power plants today with modern technology and
[01:00:55.240 --> 01:01:02.280]   insight, they are not only incredibly safe, but also, you know, very efficient.
[01:01:02.280 --> 01:01:11.160]   Well, that is one huge difference between the Apollo program and this SpaceX launch is
[01:01:11.160 --> 01:01:18.520]   computers and computer power. And what computers can do. It's very, very different these days.
[01:01:18.520 --> 01:01:24.680]   There's a good there's an actual picture of these. Apollo on the left, black and white.
[01:01:24.680 --> 01:01:29.480]   And then what is that? That is a aircraft, a Boeing aircraft.
[01:01:29.480 --> 01:01:34.440]   A shuttle. That's the shuttle. It looks like an aircraft. And then in the lower right,
[01:01:35.080 --> 01:01:42.760]   the 2001 space Odyssey version, which is the Dragon X capsule. I even think the suits and
[01:01:42.760 --> 01:01:50.520]   the especially the helmets were designed with photogenity in mind. Yeah, Elon admitted to that.
[01:01:50.520 --> 01:01:54.840]   He said, I want to, yeah, he said, I paid a lot of attention. I had a lot of involvement in the
[01:01:54.840 --> 01:02:02.360]   design of the spacesuits. And we want to strike that balance between looking dead-ass cool. And
[01:02:02.360 --> 01:02:07.000]   he didn't say that. No, I think he did actually. I think that's probably exactly what he said.
[01:02:07.000 --> 01:02:13.000]   No, he bet his point was we want to inspire people. We want to inspire kids to go up and be
[01:02:13.000 --> 01:02:19.240]   astronauts. Yeah, I mean, and NASA is very well aware and has always been aware of the fact that
[01:02:19.240 --> 01:02:23.880]   they've got to get funding that is not an automatic thing. And it's, you know, it's a fraction of the
[01:02:23.880 --> 01:02:31.880]   federal budget, but it's often a contentious battle to get funding. And, you know, they're very
[01:02:31.880 --> 01:02:36.920]   aware of public relations, even the stream, which a lot of it came from SpaceX, but it was an
[01:02:36.920 --> 01:02:41.400]   NASA stream. It felt like it was aimed at high schoolers. There was a lot of cool and wow, and
[01:02:41.400 --> 01:02:47.560]   hey, and sure, I'm off for that, you know, do the PR thing. If that's what it takes to get the
[01:02:47.560 --> 01:02:55.000]   funding you need to get into space. And it is dead-ass cool. As long as they work, as long as they're
[01:02:55.000 --> 01:03:01.320]   safe, I don't know. That's well, and that's that was the big worry, right? Because you saw the failure
[01:03:01.320 --> 01:03:08.680]   of the stars prototype right before the launch. So two completely different systems,
[01:03:08.680 --> 01:03:15.960]   one's a prototype, one's been the Falcon 9 and Dragon tested extensively. So I don't think there
[01:03:15.960 --> 01:03:22.920]   was any concern there, but still, you saw that juxtaposition of these two kind of systems. And
[01:03:22.920 --> 01:03:30.200]   gives you a little bit of pause. You want it to work. You want it to be amazing and inspiring and
[01:03:30.200 --> 01:03:38.520]   show us the best of what we can accomplish as opposed to some of the worst of our human nature.
[01:03:38.520 --> 01:03:44.840]   Yeah, yeah, I think that's it. We see a lot of the worst, especially these days. It'd be nice to
[01:03:44.840 --> 01:03:51.400]   see something hopefully to literally lift our eyes to the stars. Not a bad, not a bad thing.
[01:03:52.440 --> 01:04:00.120]   And relatively speaking, not that expensive. Is there video of the Dragon X failure?
[01:04:00.120 --> 01:04:10.760]   It wasn't a Dragon X. It was the Starship. Okay. I have a space-addled crew.
[01:04:10.760 --> 01:04:19.080]   John really loves all this stuff. And I'm with you. Here it is. Yeah, you want to see it in 4K
[01:04:19.080 --> 01:04:30.600]   slow-mo? Maybe not. Oh, Lord above. I had to say, last night I watched Steve Carell and the new
[01:04:30.600 --> 01:04:37.480]   Netflix show, which was it's called the Space Force, which I'm six episodes in. And they and
[01:04:37.480 --> 01:04:43.160]   they feature that explosion or something a lot like it in the first or second episode. So
[01:04:44.280 --> 01:04:49.880]   it's pretty good, isn't it? Yeah, I like it. Yeah. It's far more sort of poignant and thoughtful
[01:04:49.880 --> 01:04:55.640]   than I thought it would be. It's not broad. No. No. Yeah. It's nice. Do they explain why his
[01:04:55.640 --> 01:05:04.600]   wife's in jail at any point? Not yet. Not in Epidiamen, episode six. Nope. She's so funny. She's
[01:05:04.600 --> 01:05:12.280]   so great. It's a great cast. John Malkovich doing TV. Yeah, but he's good. And he's really good.
[01:05:12.280 --> 01:05:17.880]   He is perfectly cast. Yes. So that is it. If you do have Netflix, that is, I think,
[01:05:17.880 --> 01:05:28.120]   worth watching. It's a Space Force. A little political. A little bit. Speaking of Apollo 11,
[01:05:28.120 --> 01:05:35.880]   Google has postponed. I don't know. That's a bad segue. Android 11. It's a particularly bad
[01:05:36.440 --> 01:05:43.000]   segue. We were going to actually stream this unveiling of the next version of Android.
[01:05:43.000 --> 01:05:49.720]   Google's showing, I think, a little bit of sensitivity, decided to postpone it, they say, due to the
[01:05:49.720 --> 01:05:55.400]   unrest in the United States in line of the protests. We're excited to tell you more about
[01:05:55.400 --> 01:06:02.440]   Android 11, but now's not the time to celebrate Google posted on its Android developer's website.
[01:06:03.000 --> 01:06:11.800]   So we will not be streaming that. Although Sony apparently does not have the same
[01:06:11.800 --> 01:06:17.160]   concerns, and they're going to stream a bunch of PlayStation 5 game footage on June 4th,
[01:06:17.160 --> 01:06:24.840]   we'll probably stream that instead. Let's take another break, and we'll talk about,
[01:06:24.840 --> 01:06:30.920]   I wanted to put one really nice, happy story in there, SpaceX. Then we're going to get back
[01:06:30.920 --> 01:06:38.120]   to the depressing stuff. I'm sorry about that. Our show today brought to you by
[01:06:38.120 --> 01:06:45.800]   LastPass is ready to help you take your workforce home. It's always important to have a plan for
[01:06:45.800 --> 01:06:52.760]   the unexpected. Who knew that here we are in almost in June, and you'd have your workforce
[01:06:52.760 --> 01:07:01.880]   working out of the house far away from IT, far away from security. But the good news is LastPass
[01:07:01.880 --> 01:07:06.200]   can be deployed quickly in the midst of any event to make sure your business keeps running smoothly,
[01:07:06.200 --> 01:07:13.000]   and every employee login is secure. I'm really glad that I've been using LastPass personally
[01:07:13.000 --> 01:07:17.720]   for 10 years. I'm really glad we moved to LastPass for the business a few years ago,
[01:07:18.760 --> 01:07:22.680]   because we do have a lot of people working at home. And you know what that means, if they're
[01:07:22.680 --> 01:07:28.280]   working from home, they're logging into our bank accounts, our databases, our websites,
[01:07:28.280 --> 01:07:33.240]   all of the most important stuff, our VPNs, our systems here at work from their house.
[01:07:33.240 --> 01:07:41.720]   Knowing they're using LastPass puts my mind in these, more importantly, it puts Russell's mind
[01:07:41.720 --> 01:07:46.920]   in these, Russell's responsible for IT and security. LastPass does this in a few ways. Of course,
[01:07:46.920 --> 01:07:51.880]   you all know it's the best password manager, and we still need to use passwords a lot.
[01:07:51.880 --> 01:07:58.040]   Enterprise password management makes sure ensures the oversight of shadow IT and enforceable policies
[01:07:58.040 --> 01:08:04.440]   across all password protected accounts. We require for us as two factor, and we have certain minimum
[01:08:04.440 --> 01:08:09.160]   standards for master passwords, things like that. But LastPass has gone beyond just the password
[01:08:09.160 --> 01:08:14.120]   vault. They use single sign on now. In fact, there are 1,200 plus single sign on apps that work
[01:08:14.120 --> 01:08:21.480]   with LastPass. So your employees can log in using their phone, and it gives you an IT a centralized
[01:08:21.480 --> 01:08:27.960]   view. So you have always have complete insight into who has access to what from where. That's really
[01:08:27.960 --> 01:08:32.520]   important with people all over the place. And then there's LastPass's multi-factor
[01:08:32.520 --> 01:08:37.160]   authentication, also a new feature in the LastPass enterprise. Of course, it'll use
[01:08:37.800 --> 01:08:45.320]   biometric like touch ID, face ID, but it will also use contextual factors, geo location,
[01:08:45.320 --> 01:08:50.040]   IP address, to make sure that only the right employees are accessing the right resources.
[01:08:50.040 --> 01:08:56.200]   This is so important, and you will be glad to know LastPass never sends or stores a master
[01:08:56.200 --> 01:09:00.520]   password. If it can't access your data, that means hackers can either. Encryption happens
[01:09:00.520 --> 01:09:06.360]   exclusively at the device level before syncing to LastPass for safe storage. So only users can
[01:09:06.360 --> 01:09:11.960]   decrypt their data. LastPass protects while providing a seamless workflow for your employees.
[01:09:11.960 --> 01:09:16.520]   Account access and passwords can be shared securely between employees, even off-site. Again,
[01:09:16.520 --> 01:09:20.840]   this is very important whether they're in the office or remote. Employees will get secure access
[01:09:20.840 --> 01:09:26.360]   to their work applications with SSO, password management. There's an offline mode, which is great.
[01:09:26.360 --> 01:09:31.880]   Sometimes employees don't have the same great internet service you do at work. So they can still
[01:09:31.880 --> 01:09:37.320]   get their passwords and even multi-factor authentication even when the internet is a little spotty.
[01:09:37.320 --> 01:09:44.520]   LastPass can help make remote work simple and secure. Visit LastPass.com/twit to find out how
[01:09:44.520 --> 01:09:51.480]   they can help you and your business stay productive and secure no matter what. LastPass.com/twit.
[01:09:51.480 --> 01:09:55.880]   We had a great LastPass panel last month. It was so much fun with Steve Gibson and
[01:09:56.840 --> 01:10:02.840]   Jerry Bokell from LogMe in there. See so, Andrew Keen. We're planning the next one in the fall.
[01:10:02.840 --> 01:10:07.240]   If you've got ideas for what you'd like, send me an email or mention it in the chat room. I have
[01:10:07.240 --> 01:10:13.400]   an idea. I haven't run this by LastPass. I want to get a hacker panel. I want to get people who,
[01:10:13.400 --> 01:10:19.080]   you know, and we know a few hackers. We have some friends get some people in. If they want,
[01:10:19.080 --> 01:10:24.840]   they can wear a mask and will change their voice. But I want to get people in and talk about how
[01:10:24.840 --> 01:10:31.640]   hackers attack and how they try to breach your security and what they do. I think that would
[01:10:31.640 --> 01:10:37.400]   be fascinating. We're planning that for September. So stay tuned. Details still to come.
[01:10:37.400 --> 01:10:42.040]   We're talking about the weekend tech with Denise. How always great to see you. You did a great
[01:10:42.040 --> 01:10:47.000]   AMA last week. That was fun. Thank you. That was so fun. Those after-hours things that you're doing
[01:10:47.000 --> 01:10:49.960]   on Fridays are really fun and cool. We're really enjoying them. I don't know if it's,
[01:10:51.080 --> 01:10:55.800]   you know, I mean, it's not a revenue. It's not a money maker, that's for sure. But it's a lot
[01:10:55.800 --> 01:11:00.600]   of fun. I think it's good for our employees and I know it's good for our audience. I think it's
[01:11:00.600 --> 01:11:06.920]   fun. This week we did the YouTube stars, which was really great. We had Father Robert Baliser,
[01:11:06.920 --> 01:11:12.840]   Renee Ritchie, and the return of OMG chat. I used to produce my shows and work for Twitch.
[01:11:12.840 --> 01:11:20.040]   Hey. Remember, of course. Ten years ago, he started a little Minecraft show on Twitch called
[01:11:20.040 --> 01:11:28.520]   OMG Craft. He broke 1 million subscribers this past week on YouTube. So we thought we'd have
[01:11:28.520 --> 01:11:34.120]   him in along with Renee and Robert and talk about YouTube stardom. And then they also played a
[01:11:34.120 --> 01:11:39.240]   kind of a picture-nary game. That was kind of fun. So we've done so many of them. We're going to do
[01:11:39.240 --> 01:11:47.960]   more too. I don't know what we're doing next week, but Fridays, 4 p.m. Pacific 7. Do you know John?
[01:11:47.960 --> 01:11:53.880]   I see your eyebrows going up in it. Yeah, we don't find out till Wednesday. They're planning it.
[01:11:53.880 --> 01:12:01.480]   They're thinking about it. 4 p.m. Pacific 7 p.m. Eastern. We will do that on Friday. And maybe
[01:12:01.480 --> 01:12:06.920]   we'll keep doing it after quarantine's over. I don't know. It's a lot of fun. We did have a good
[01:12:06.920 --> 01:12:14.360]   week. I bet you we have some clips from this week's after hours right here. Previously on Twitch.
[01:12:14.360 --> 01:12:19.160]   I'm going to look down underneath me. What are you doing down there, Padre? Oh, you know,
[01:12:19.160 --> 01:12:26.280]   just doing my thing overseas in the land of, I guess, Karoma. What time is it in the land of
[01:12:26.280 --> 01:12:32.440]   Karoma? It is now 1.06 a.m. So it's, you know, it's just about time for me to start the day.
[01:12:32.440 --> 01:12:38.440]   Hands on Android. I'm going to show you some advanced tips and tricks with Assistant. Things
[01:12:38.440 --> 01:12:42.760]   that you may not have heard of, hidden deeper and in conjunction with some other apps, which I'll
[01:12:42.760 --> 01:12:48.680]   talk about, you can do a whole lot more with the systems. Hands on photography. This week on
[01:12:48.680 --> 01:12:54.200]   Hands on Photography, we're going to do something just a little bit different while we do this
[01:12:54.200 --> 01:13:00.840]   shelter in place photography project. iOS today. There are a lot of people studying at home now.
[01:13:00.840 --> 01:13:08.440]   And a lot of parents sort of having to fill in the gaps. I think it might be good to do an episode
[01:13:08.440 --> 01:13:15.800]   where we talk about how to help them and how to teach them and maybe to sort of refresh yourself
[01:13:15.800 --> 01:13:21.560]   on some of those concepts. This week in Google, this lickable screen can recreate almost any
[01:13:21.560 --> 01:13:26.280]   taste or flavor without eating food. Now see, I need that. They've invented what is being
[01:13:26.280 --> 01:13:33.480]   described as a taste display. I could get this device that would simulate the flavor of my
[01:13:33.480 --> 01:13:38.200]   bean and cheese burrito to it. Technology for your eyes and ear holes.
[01:13:38.200 --> 01:13:46.120]   Jeff Jarvis on this week in Google, he misses his Taco Bell like nobody's business.
[01:13:46.120 --> 01:13:53.320]   So we all got, was it last week of the week before? We all got the Google and Apple API on our
[01:13:53.320 --> 01:13:58.520]   phones. I've got it on my iPhone. I think I've got it on my Google phone as well. The only problem
[01:13:58.520 --> 01:14:05.160]   with it is nobody wants to use it. France said, what are you crazy? Germany back down. They said
[01:14:05.160 --> 01:14:10.440]   eventually they're going to use it. The first use of it, I think is going to be in Switzerland.
[01:14:10.440 --> 01:14:15.560]   They've already started releasing an app in test. I don't think they've released it
[01:14:15.560 --> 01:14:20.520]   to the general populace called exposure notification.
[01:14:20.520 --> 01:14:27.480]   Are they, what are they doing? Oh, Latvia. Oh, Latvia is going to use it too. What are they doing
[01:14:27.480 --> 01:14:35.400]   in Finland? Is there any? I haven't heard about an app in Finland because the
[01:14:35.400 --> 01:14:44.200]   efficiency of the apps is kind of an unknown, especially if you don't have a lot of people
[01:14:44.200 --> 01:14:50.040]   adopting it and no one wants to make it mandatory for understandable reasons.
[01:14:50.040 --> 01:14:56.840]   But you've lost over the French thing. It is completely baffling what is happening with the
[01:14:56.840 --> 01:15:01.480]   French version of the contact tracing app, which is called stop-covid.
[01:15:01.480 --> 01:15:10.920]   They decided to sort of plant their flag, the government, decided to plant their flag on this
[01:15:10.920 --> 01:15:21.240]   as a political/technological battle to kind of fight the dominance of Google and Apple and say,
[01:15:21.240 --> 01:15:26.120]   we don't need to use that version because our version is better. There are two competing
[01:15:26.120 --> 01:15:35.480]   standards, both of which being pretty good in theory, some with good and bad aspects to both.
[01:15:35.480 --> 01:15:44.520]   But the issue is with the French version, since it's not using the validated API by Google and Apple,
[01:15:44.520 --> 01:15:51.000]   it doesn't have free access to the Bluetooth system on iOS.
[01:15:52.520 --> 01:15:58.840]   We've had a few tech publications have had hands-on with the app as it's been tested.
[01:15:58.840 --> 01:16:04.520]   It's releasing tomorrow, I believe. But the access to Bluetooth is very intermittent.
[01:16:04.520 --> 01:16:09.880]   Sometimes when the phone is woken up, it has access to Bluetooth. Sometimes, of course,
[01:16:09.880 --> 01:16:15.800]   when you launch it, it has access to it. But there are many time windows where it doesn't
[01:16:15.800 --> 01:16:22.680]   have access to Bluetooth. So it doesn't communicate the proximity.
[01:16:22.680 --> 01:16:27.000]   Apple's basically made it. You have to use our API or you're not going to have a useful app.
[01:16:27.000 --> 01:16:37.560]   Which is absolutely the case. But for those who don't know, it's ironic because for almost a decade,
[01:16:37.560 --> 01:16:42.760]   we've been saying, oh, the tech giants have too much control. In this specific case,
[01:16:42.760 --> 01:16:51.160]   it's Apple and Google working together who can ensure the governments don't have too much power
[01:16:51.160 --> 01:16:57.560]   with these apps because they could be used. They could be used to track location,
[01:16:57.560 --> 01:17:05.160]   to gather data about you. It could be a doorway to a nightmare surveillance scenario.
[01:17:05.160 --> 01:17:11.640]   Apple and Google did things very right. They implemented a lot of safeguards. It's very well
[01:17:11.640 --> 01:17:15.960]   done there, system. It's well done, although I think a lot of people, including Bruce Schneider,
[01:17:15.960 --> 01:17:24.280]   who I trust, say it's a non-starter only because no app is going to be successful in contact tracing.
[01:17:24.280 --> 01:17:31.960]   You need human contact tracers. Schneider's contention was there are two problems with any app.
[01:17:31.960 --> 01:17:38.120]   False positives, false negatives. False positives because an app that uses Bluetooth or proximity
[01:17:38.120 --> 01:17:45.000]   might make a mistake if you drive by somebody or you're on the other side of a barrier, but they
[01:17:45.000 --> 01:17:51.640]   can't tell, but it will say you've been exposed. That's a problem. There's also the false negative
[01:17:51.640 --> 01:17:57.080]   problem where if not every, but not 100% of everybody's using it, you go in a store,
[01:17:57.080 --> 01:18:00.520]   you can't assume that you haven't been exposed just because your app says,
[01:18:00.520 --> 01:18:05.880]   well, you didn't run into anybody running the app that's tested positive.
[01:18:05.880 --> 01:18:12.440]   So all of this means very low level of trust in these apps, which means a very low level of adoption.
[01:18:12.440 --> 01:18:18.040]   And by the way, in countries like Singapore where they have apps that don't use the API, adoption is
[01:18:18.040 --> 01:18:24.920]   around 20%. It's not going to be very effective. I want to point out a great article by Zach Hall,
[01:18:24.920 --> 01:18:31.720]   9 to 5 Mac. He's keeping it up to date on which states are using it. And only four US states
[01:18:32.760 --> 01:18:41.400]   have said they're going to use the Apple Exposure API, Alabama, North Dakota, South Carolina,
[01:18:41.400 --> 01:18:50.680]   and Virginia. He also talked to health officials at each state. And many states have, for instance,
[01:18:50.680 --> 01:18:57.720]   California says we don't have any updates on apps at all. But if that changes, we'll let you know.
[01:18:57.720 --> 01:19:02.520]   Many states are just not going the app route there instead trying to hire human contact racers.
[01:19:02.520 --> 01:19:07.800]   Which is going to be an even more intrusive conversation than any app.
[01:19:07.800 --> 01:19:13.320]   Yeah, but the contact tracing, the test and trace, which has been put in place in
[01:19:13.320 --> 01:19:19.560]   extensively in Asia, is more intrusive for the person that you know that has been
[01:19:19.560 --> 01:19:27.400]   affected, that has tested positive. And that is important to get information on.
[01:19:27.400 --> 01:19:32.760]   The issue with apps, or the potential issue with apps, would be the mass surveillance
[01:19:32.760 --> 01:19:39.000]   it can implement because everyone should should install them if you want them to be efficient.
[01:19:39.000 --> 01:19:42.680]   I think Google and Apple have done a very good job, right? They don't ever give
[01:19:42.680 --> 01:19:47.000]   anybody else the information. You don't even get the information. It'll say you've been exposed,
[01:19:47.000 --> 01:19:53.080]   it doesn't say to whom. And they don't get your, and so no information is exchanged with anybody.
[01:19:53.080 --> 01:19:59.640]   It's all and it's deleted after a period of time. After 14 days, you get basically
[01:19:59.640 --> 01:20:05.720]   IDs that are renewed every 15 minutes and they don't get your location. They just get the ideas,
[01:20:05.720 --> 01:20:11.560]   the IDs of the phones that you've been, you've come close to the IDU, right? It's not IDU.
[01:20:11.560 --> 01:20:16.840]   No, no, it's not. In that system, it isn't. In the French system, which is using the Robert
[01:20:16.840 --> 01:20:28.120]   Protocol, it is tied to one pseudonymized ID, which then they then can use to run statistical
[01:20:28.120 --> 01:20:34.520]   analysis and things that could be useful. But of course, now they've started releasing the
[01:20:34.520 --> 01:20:43.240]   source code as they had promised, which is very much necessary. We're finding that there are
[01:20:43.240 --> 01:20:48.840]   many security issues in the implementation that they have chosen apparently.
[01:20:48.840 --> 01:20:55.240]   But even beyond that, the issues with these apps are so many, and not everyone has a smartphone,
[01:20:55.240 --> 01:20:57.240]   almost everyone, but when you need to 15-
[01:20:57.240 --> 01:21:02.600]   You don't have to have 100% because you want to bring that are not down, and if you had 70 or 80%
[01:21:02.600 --> 01:21:12.120]   compliance. Yeah, but even if you are at, your target is 50%, there are like 70 to 80% of people
[01:21:12.120 --> 01:21:18.680]   who have smartphones. Out of those, only 70 or so percent have smartphones that can run,
[01:21:18.680 --> 01:21:25.960]   that are on the OS that can run the apps. And so out of those, everyone should install the app
[01:21:25.960 --> 01:21:34.520]   in order to reach that level. But the important thing to keep in mind is that the solution that
[01:21:34.520 --> 01:21:41.000]   Apple and Google are offering is an API that is saying, we don't know if it's going to work,
[01:21:41.000 --> 01:21:47.160]   we don't know if you want to use it, but this is the safest option we can offer.
[01:21:47.160 --> 01:21:52.440]   Now, maybe it doesn't work, but if we don't have that, which is controlled by us and
[01:21:52.440 --> 01:21:59.160]   a dears to the strictest standards of security, then we don't know what's going to happen because
[01:21:59.160 --> 01:22:07.720]   we don't know what other systems or government are going to do with these systems that are implemented.
[01:22:07.720 --> 01:22:12.040]   So it's kind of a safety net, even if it doesn't work, it's important to have it.
[01:22:12.040 --> 01:22:17.480]   But the reality is the thing, it seems I'm not an expert, but everything I've read seems to point
[01:22:17.480 --> 01:22:23.000]   towards the fact that the app is probably not going to help much. And the thing that really helps is
[01:22:23.000 --> 01:22:27.880]   test and trace direct interviews and that kind of thing. Although if the app were willing to violate
[01:22:27.880 --> 01:22:33.080]   privacy, it could be very useful. If a contact trace, no, seriously, if I test positive,
[01:22:33.800 --> 01:22:37.640]   and a contact tracer can come to me and say, "Well, I'd like to know everywhere you've been and
[01:22:37.640 --> 01:22:42.520]   everyone you've run into in the last 14 days." If my phone's been recording that and can hand
[01:22:42.520 --> 01:22:47.160]   that information over, that makes their job a lot more easier and a lot more effective. I don't know
[01:22:47.160 --> 01:22:53.720]   if I could remember everybody I've run into. Yes and no, because it gives you the exact location
[01:22:53.720 --> 01:23:00.760]   you've been, but then you still need to talk to people to ask so you weren't. But they're going
[01:23:00.760 --> 01:23:04.040]   to do that. And contact tracing, the first thing they say is, "Where have you been in the last 14
[01:23:04.040 --> 01:23:09.800]   days?" And if I could say, "Well, it's on my phone." It actually is because I let Google do that.
[01:23:09.800 --> 01:23:18.920]   I'm doing it for a reason. Okay, just to prove your point, and this is not exactly this. It's
[01:23:18.920 --> 01:23:23.960]   apples and oranges here, but it's from Boygenius reported. According to Minnesota Public Safety
[01:23:23.960 --> 01:23:29.080]   Commissioner John Harrington, officials in Minnesota have been using what they describe
[01:23:29.080 --> 01:23:33.800]   without going into much detail as contact tracing in order to build out a picture of
[01:23:33.800 --> 01:23:39.960]   protester affiliations. There you go. So that's the problem with a tool like this.
[01:23:39.960 --> 01:23:45.240]   And there's the slippery slope. There's the slippery slope. The other question I have is,
[01:23:45.240 --> 01:23:50.680]   is whether Apple and Google are going to block apps that don't use their API?
[01:23:50.680 --> 01:23:56.840]   Well, that's France's problem, right? If I write an app as the government of France,
[01:23:57.400 --> 01:24:03.160]   apps on iOS don't have access to Bluetooth. This kind of beakening information. Is that
[01:24:03.160 --> 01:24:08.360]   right, Patrick? Is that the issue? Yeah, exactly. Because Bluetooth is restricted for security and
[01:24:08.360 --> 01:24:15.000]   battery saving issues, especially on iOS. It depends on the version of Android, but some of them have
[01:24:15.000 --> 01:24:26.600]   that too. But Apple and Google have said that only official governmental or official administration
[01:24:26.600 --> 01:24:32.280]   apps could use that kind of technology. So my guess, I haven't looked into this specifically,
[01:24:32.280 --> 01:24:37.960]   but my guess is they're policing the app stores for these kinds of apps. So you can't have
[01:24:37.960 --> 01:24:43.320]   any app doing anything with these because you have security measures on those
[01:24:43.320 --> 01:24:49.560]   devices anyway, right? And this is why, as you said, the French app doesn't work well,
[01:24:49.560 --> 01:24:59.080]   because you can't, by default on the system, use these features. States have already started hiring
[01:24:59.080 --> 01:25:07.880]   tens of thousands of contact tracers. But I have to say, just this conversation illustrates
[01:25:07.880 --> 01:25:14.600]   why I think it's a non-starter in the United States. I don't think we're going to give up
[01:25:14.600 --> 01:25:22.200]   information like that. No, but it's important to distinguish the two. We touch on them earlier.
[01:25:22.200 --> 01:25:27.000]   And surprisingly, we've had these kinds of discussions in France as well from a few people
[01:25:27.000 --> 01:25:37.240]   saying, "Oh, but we're building a database with people who have been sick on COVID-19,
[01:25:37.240 --> 01:25:43.000]   who have tested positive on COVID-19. The government shouldn't know who's been, you know,
[01:25:43.000 --> 01:25:49.160]   we're sort of losing as much as I am very cautious and actually antagonistic towards the apps.
[01:25:49.160 --> 01:25:56.200]   For the people who test positive, this is how you solve it. Public health requires
[01:25:56.200 --> 01:26:03.400]   by my conclusion. Test, trace, and isolate. It's the only thing that works.
[01:26:03.400 --> 01:26:08.520]   Do our behaviors impact whether these apps can work? Do you think?
[01:26:08.520 --> 01:26:13.640]   Well, what do you mean? Like, what give me an example? I mean, like the demonstrations we've seen,
[01:26:13.640 --> 01:26:19.080]   and then the post-demonstration writing that we've seen over the last few days in several large
[01:26:19.080 --> 01:26:24.440]   cities, you know, whether you're talking to your contact tracer, human being, or you're
[01:26:24.440 --> 01:26:29.880]   participating in an app, if you've been around that many people in that close proximity,
[01:26:29.880 --> 01:26:34.440]   does that just flood the system? They can handle it in theory. They can. Yeah.
[01:26:35.080 --> 01:26:41.880]   Yeah. Okay. In theory. But first of all, we don't know a whole, we don't know a hell of a lot of
[01:26:41.880 --> 01:26:49.160]   how the virus spreads. So if I'm, and I don't know how much information is handled, is recorded by
[01:26:49.160 --> 01:26:57.240]   the API. But if I'm in contact with somebody for a second outdoors and that, no, it can
[01:26:57.240 --> 01:27:02.920]   it time it. It times can put a threshold. Yes, you can put a threshold and keep,
[01:27:03.560 --> 01:27:07.800]   basically record the idea of the person you've been in contact with or of the phone,
[01:27:07.800 --> 01:27:13.880]   which your phone has been closed to for 15 minutes, then it tags it. If it wasn't, you know, you can
[01:27:13.880 --> 01:27:18.680]   just. But that's an assumption. See, this is the problem with, honestly, this is the problem with
[01:27:18.680 --> 01:27:24.200]   big tech. And even British Nier says that they solve the problems they can solve. They didn't really,
[01:27:24.200 --> 01:27:29.080]   they based it on assumption. We don't know. Is it 15 minutes? Is it half an hour? Is it five
[01:27:29.080 --> 01:27:33.000]   minutes? We don't know. We actually, and it may depend on what you're doing. What are you doing?
[01:27:33.000 --> 01:27:37.320]   Are you singing or are you my pantomiming makes a difference?
[01:27:37.320 --> 01:27:43.720]   All of these things are true. But again, these systems by Apple and Google were designed because
[01:27:43.720 --> 01:27:49.400]   they knew that this was coming. They knew that governments and health authorities were saying
[01:27:49.400 --> 01:27:56.120]   maybe this could be used to help. And they designed them with the strongest possible security features
[01:27:56.120 --> 01:28:02.840]   while still making everything that these apps would want to do possible as much as possible.
[01:28:02.840 --> 01:28:10.680]   And I am convinced the app, well, it seems most people are convinced the apps are going to do
[01:28:10.680 --> 01:28:18.760]   very little. There was, I think, in Norway or Iceland, a police force that was in charge of the
[01:28:18.760 --> 01:28:24.920]   contact tracing interviews, which also had access to an app that was installed, I think, up to 50 or
[01:28:24.920 --> 01:28:34.120]   60% of the population. And they also said it was negligible. There were one or two cases where it
[01:28:34.120 --> 01:28:39.400]   was like, "Okay, you didn't remember. You went there." And they were using GPS data, which might
[01:28:39.400 --> 01:28:46.920]   be better than Bluetooth. And it was marginal to benefits. But still, it's important to have
[01:28:46.920 --> 01:28:53.080]   those Apple and Google system, which respects privacy. Because if you don't, then everyone
[01:28:53.080 --> 01:29:01.480]   does what they want, and we know what happens. But it doesn't matter. Public health requires
[01:29:01.480 --> 01:29:07.800]   your breach privacy. Denise, do you think we could ever do this in the United States? We don't even
[01:29:07.800 --> 01:29:14.120]   want to wear masks. Right. Very true. EFF has a good article on it where they're pointing out
[01:29:14.120 --> 01:29:20.360]   exactly the conundrum that you are, Leo, that in order to be the most effective app, you're going
[01:29:20.360 --> 01:29:26.920]   to have to have it be centralized and more intrusive. And in order to respect people's privacy, it's
[01:29:26.920 --> 01:29:36.200]   going to be decentralized like the Apple and Google API is mostly, which is more protective
[01:29:36.200 --> 01:29:43.080]   of other concerns, but not as effective in squashing the virus. So it just depends, I guess,
[01:29:43.080 --> 01:29:50.120]   how prevalent the virus is and how scared people are and what sort of trade-offs they're willing
[01:29:50.120 --> 01:29:52.840]   to make. It's a shame that we so little trust our government.
[01:29:52.840 --> 01:29:58.520]   I mean, that's also part of it. It's how much do you trust the government? Do you assume that
[01:29:58.520 --> 01:30:03.880]   the government would have been given this information will then use it in other ways?
[01:30:03.880 --> 01:30:07.560]   Or do you assume your government has beenvolent and he's using it for public health?
[01:30:07.560 --> 01:30:12.040]   If you don't trust the government, you're not going to trust contact tracing. It's just not going to work.
[01:30:12.040 --> 01:30:19.880]   And even if you trust the government generally, people lie, right? I mean, for a variety of reasons.
[01:30:20.280 --> 01:30:28.280]   I mean, they may fear, they may have fear, they may forget, they may not, you know,
[01:30:28.280 --> 01:30:32.840]   they may not provide accurate information which you need during a public health crisis, right?
[01:30:32.840 --> 01:30:36.440]   So it's just like in a criminal investigation, you know, if you go up and you ask somebody,
[01:30:36.440 --> 01:30:43.480]   hey, did you see this? What happens? Were you there? Then that information can be used in ways,
[01:30:43.480 --> 01:30:48.840]   maybe you didn't anticipate, you weren't a subject of the investigation, but you provided evidence.
[01:30:49.560 --> 01:30:56.440]   And then all of a sudden you were caught up in that. So it really is a difficult,
[01:30:56.440 --> 01:31:02.360]   I think it's almost impossible for us to roll this out in the US. I really do from, you know,
[01:31:02.360 --> 01:31:09.320]   looking at the current climate and sort of very divisive political environment that we have right
[01:31:09.320 --> 01:31:15.880]   now. I just don't think there's a lot of interest in allowing the government to watch us any more
[01:31:15.880 --> 01:31:20.840]   than we already do. And so, you know, I just don't see people installing it voluntarily.
[01:31:20.840 --> 01:31:26.120]   And I think it would be very, very difficult for any state or the federal government to mandate
[01:31:26.120 --> 01:31:31.320]   that this be put on phones. I think you'd see a lot of uproar even with the public health crisis.
[01:31:31.320 --> 01:31:37.160]   I think we'd have to be on the edge of just absolute societal breakdown before people would
[01:31:37.160 --> 01:31:41.560]   willingly go along with this. And the closer we get to that, the less likely they are.
[01:31:41.560 --> 01:31:47.800]   It's odd because a hundred thousand people dying in three months seems like a pretty big crisis to
[01:31:47.800 --> 01:31:55.800]   me. No, but really, I don't know. What do you want? A million? The apps don't matter. The apps,
[01:31:55.800 --> 01:32:02.120]   you know, people, they have to be voluntarily installed and people aren't going to install them.
[01:32:02.120 --> 01:32:07.000]   I'm confused whether you're talking about contact tracing via apps. No, I'm not. I'm talking about
[01:32:07.000 --> 01:32:12.280]   human contact tracing. It's a parent. Apps will work. It's very apparent.
[01:32:12.280 --> 01:32:18.600]   And human contact tracing is key. If you don't accept that, then you're going to have a big
[01:32:18.600 --> 01:32:22.840]   problem for a long time. Well, I don't think if the guy comes to the door and says,
[01:32:22.840 --> 01:32:28.760]   you've been in contact with somebody who has tested positive. I need to know everywhere you've
[01:32:28.760 --> 01:32:34.840]   been in the last 14 days and everyone you've seen, I would say at least half of Americans will
[01:32:34.840 --> 01:32:42.120]   slam the door in his face. I'm going to tell you everywhere I've been, except for those two people
[01:32:42.120 --> 01:32:48.280]   that I really don't want you to know about. Right? I mean, yes, or they'll lie. That's better than
[01:32:48.280 --> 01:32:54.840]   ideal situation. We have the real world. Right. So, and I think people would be upset to even
[01:32:54.840 --> 01:33:00.680]   realize that the latest update to their phones operating system has added this cape. It's already
[01:33:00.680 --> 01:33:06.040]   in there. Yeah. Yeah. Well, I'm sure they are. We, when we first started talking about this,
[01:33:06.040 --> 01:33:12.440]   I had a the twit panel just basically said it's a non starter period never installing it.
[01:33:12.440 --> 01:33:17.800]   Now that was a few weeks ago. Maybe people don't know. By the way, here's some good news.
[01:33:17.800 --> 01:33:23.720]   I'm sure Jack Dorsey be glad to hear this. France says Twitter would be welcome if it decides to
[01:33:23.720 --> 01:33:30.280]   leave the United States. Well, not after that law that Patrick mentioned.
[01:33:30.280 --> 01:33:37.080]   I don't think it's going to the US. This is the US, the French junior digital affairs minister.
[01:33:37.080 --> 01:33:43.720]   You're going to have to explain this to me. His name is Cedric O, not EAU O, just the letter O.
[01:33:43.720 --> 01:33:51.080]   Is that his real name? Yeah. Yeah, it is. You can have a one letter name in France.
[01:33:52.520 --> 01:33:59.640]   Well, it's his name. It's his name. Okay. Anyway, he said his offspring with Elon Musk's
[01:33:59.640 --> 01:34:05.640]   offspring and he said this appropriately in an interview on Radio J.
[01:34:05.640 --> 01:34:13.640]   If Twitter would consider at some point that the US circumstances could no longer allow the company
[01:34:13.640 --> 01:34:19.000]   to develop a calling to its values and to continue to expand. And if there would be too much instability
[01:34:19.000 --> 01:34:24.760]   for various reasons, the company would obviously be welcome in Europe, particularly in France.
[01:34:24.760 --> 01:34:31.720]   I don't know if it's that obvious. Yeah, I think, you know, all of these things,
[01:34:31.720 --> 01:34:38.200]   I think Twitter doesn't really want to be friends. No, all of the food and the coffee is better.
[01:34:38.200 --> 01:34:41.880]   But and that's important. They can tell a commute. So it's just a question of right.
[01:34:41.880 --> 01:34:45.560]   They can all tell a good point. Yeah. Jack has said you don't ever have to come back to work.
[01:34:47.080 --> 01:34:52.120]   The company could be anywhere. You know what? If I seriously, if I were Jack, I'd probably consider
[01:34:52.120 --> 01:34:57.320]   moving to somewhere like the Cayman Islands. Well, that's what I was going to say, Leo,
[01:34:57.320 --> 01:35:04.600]   are any of these large multinational corporations actually beholden to any one country anymore?
[01:35:04.600 --> 01:35:10.680]   I mean, you look at they all have to operate in China. They all have to operate in Russia.
[01:35:10.680 --> 01:35:16.520]   They operate in the US. They operate in South America or in the Middle East and the
[01:35:16.520 --> 01:35:22.600]   Arabian Peninsula. They operate everywhere. So I'm not necessarily sure, you know,
[01:35:22.600 --> 01:35:29.880]   an invitation from any one company or a sorry country, you know, is, you know, would be enticing
[01:35:29.880 --> 01:35:35.880]   enough unless it's really low taxes. Right. Well, I'm sure there are countries you could go to,
[01:35:35.880 --> 01:35:42.680]   there'll be no taxes and no rules. And those not France, not France. I know.
[01:35:43.560 --> 01:35:48.600]   I know. Honestly, that's not the best place to go, Jack. What about maybe get one of those
[01:35:48.600 --> 01:35:55.320]   abandoned oil, deep sea oil rigs and start a little country out there? You could have your
[01:35:55.320 --> 01:36:01.800]   Twitter land. Just have your own country. I mean, the thing is, they still, in order to make money,
[01:36:01.800 --> 01:36:07.720]   they have to operate in those kinds of money. And there are ways of restricting it. It's very
[01:36:07.720 --> 01:36:12.280]   difficult to operate in the country if you don't want, if that country doesn't want to operate,
[01:36:12.840 --> 01:36:16.120]   there doesn't want you to whatever happened to C land. Is that still around?
[01:36:16.120 --> 01:36:23.640]   You can, I think still. Oh, yeah, you can still. It's a, it was founded in 1967 in international
[01:36:23.640 --> 01:36:29.880]   waters, seven miles off the shore of Britain as a sovereign principality. And you can still become
[01:36:29.880 --> 01:36:37.080]   a lord or lady of sea land. It's got passports. Maybe this is where Twitter should move.
[01:36:40.440 --> 01:36:45.240]   Weren't they trying to run a data center out of their, yeah, I remember that years ago,
[01:36:45.240 --> 01:36:49.400]   yeah. It doesn't look like it has room for Jack's shower.
[01:36:49.400 --> 01:36:57.960]   Now, if you want to become a counter countist, that'll be 200 pounds. If you want to become a
[01:36:57.960 --> 01:37:04.040]   duke or Duchess, that's 500 pounds. If you want to become a lord or lady, that's only 30 pounds.
[01:37:06.040 --> 01:37:12.840]   That's the difference. I'd say go for the 30 pounds. Honestly, Lord,
[01:37:12.840 --> 01:37:22.600]   La Porte of sea land. Oh boy. Yeah. So that's, I guess there's not much more to say about the
[01:37:22.600 --> 01:37:29.000]   testing. I'm just, I'm really curious what's going to look like. Actually, I don't, I have,
[01:37:29.000 --> 01:37:34.200]   be honest, what is, what is July going to look like in the United States?
[01:37:35.480 --> 01:37:39.080]   I think that's to be determined. You know, you got to start starting to open up.
[01:37:39.080 --> 01:37:44.760]   And, you know, we won't know how that's going to affect the rate of infection
[01:37:44.760 --> 01:37:52.280]   for another probably four weeks. So, you know, we'll have to see they, you know,
[01:37:52.280 --> 01:37:56.120]   you saw the headlines out of that pool party that they had. Oh yeah.
[01:37:56.120 --> 01:38:00.440]   They were like, there were hundreds of people there. Yeah. Somebody tested positive. So,
[01:38:01.080 --> 01:38:07.720]   what's that going to lead to? Yeah. This is something we've been experiencing a little bit. We
[01:38:07.720 --> 01:38:13.400]   reopened, we started reopening both in France and Finland a couple of weeks ago. And a couple of
[01:38:13.400 --> 01:38:19.720]   things. First, people bendy the number about the number of deaths in the US. And of course,
[01:38:19.720 --> 01:38:27.800]   it's a lot. But keep in mind, if you bring it to the ratio to population, it is roughly
[01:38:29.240 --> 01:38:34.920]   in the middle of the pack in Western countries. France has like doubled the number of deaths per
[01:38:34.920 --> 01:38:41.240]   million people compared to the US. So, that's one thing to keep in mind. And of course,
[01:38:41.240 --> 01:38:46.360]   I'm not comparing. It's kind of meaningless say death per thousand. It's 100,000 deaths.
[01:38:46.360 --> 01:38:52.840]   That's 100,000 dead people. People who are otherwise be alive. That's not true.
[01:38:52.840 --> 01:38:57.400]   But, you know, it's awful. But Patrick's point about population is a good one. And I never see it
[01:38:57.400 --> 01:39:02.840]   adequately. We have a giant population. Yeah, we have a whatever proportion of the world's
[01:39:02.840 --> 01:39:09.080]   population and the coronavirus deaths are saying it's worse here than anywhere else or better
[01:39:09.080 --> 01:39:15.720]   here than anywhere else. I'm just saying there hasn't been a lot of acknowledgement that
[01:39:15.720 --> 01:39:20.120]   that's a lot of people. That's almost twice as many people as died in Vietnam.
[01:39:22.760 --> 01:39:29.720]   I didn't mean to launch into that debate. So, two weeks after we started ending the lockdown,
[01:39:29.720 --> 01:39:37.080]   we are not seeing a significant second wave here. Actually, no second wave at all. The decrease
[01:39:37.080 --> 01:39:42.920]   is continuing on O-metrics. And it seems that the factors, at least for now it might change
[01:39:42.920 --> 01:39:48.600]   tomorrow. But the big factors are social distancing is still implemented as much as we can within
[01:39:48.600 --> 01:39:55.160]   the end of the lockdown. The wearing masks, coughing in your arm, all of that. And
[01:39:55.160 --> 01:40:01.320]   test and trace. That is... What is the situation of Finland? Do you have active cases or have you
[01:40:01.320 --> 01:40:08.440]   gone down in almost nothing? Very, very few. Finland woke up at the same time as every other
[01:40:08.440 --> 01:40:16.280]   Western country mid-March. But the infection, the epidemic hadn't reached us too much. So, it was,
[01:40:16.840 --> 01:40:24.520]   I think in the hundreds cases of people sick and hospitalized, and we had maybe 100 death
[01:40:24.520 --> 01:40:32.600]   till total. 320. I'm looking right now. 320. That's a good number. That's a small country.
[01:40:32.600 --> 01:40:40.680]   Yeah, it's probably people. But it's also, the population density is a lot lower.
[01:40:44.520 --> 01:40:48.680]   And I think Leo, we were talking about this. I think the last time I was on, it was sort of early
[01:40:48.680 --> 01:40:55.960]   in the pandemic. And we were kind of wondering what the future of work would look like.
[01:40:55.960 --> 01:41:04.440]   Yeah. And as we move through this. And so, I think that's really, to Patrick's point is,
[01:41:04.440 --> 01:41:09.560]   what is this going to end to yours? What is this going to look like? Are people... A lot of big
[01:41:09.560 --> 01:41:13.000]   companies have already said, you don't have to come back to work. There's no plans for me to
[01:41:13.000 --> 01:41:16.760]   actually go back into an office. But did you go into an office before?
[01:41:16.760 --> 01:41:22.600]   I did, actually. Okay. Yeah, actually, tech republic has some nice offices in Louisville.
[01:41:22.600 --> 01:41:30.760]   We have, and in a small office compared to, say, some of the offices in SF or in New York. But,
[01:41:30.760 --> 01:41:38.120]   how is that going to... What's that going to look like for not just knowledge workers, but actually
[01:41:38.120 --> 01:41:44.920]   people in retail and people in service industries and the healthcare industries. I just think we're
[01:41:44.920 --> 01:41:55.320]   still in for several months of, if not a new normal for the foreseeable future. If not permanently,
[01:41:55.320 --> 01:42:03.160]   we saw it after SARS. We saw a mass culture. And I wish that, unfortunately,
[01:42:04.920 --> 01:42:12.760]   we have some folks in the US who see masks as some kind of equivalent to political correctness
[01:42:12.760 --> 01:42:19.080]   or as a political issue. It should be a public health is a president. It's strange to me how
[01:42:19.080 --> 01:42:25.320]   some people in life get to treat it. Yeah. He told the reporter, "Oh, you're gonna leave your
[01:42:25.320 --> 01:42:35.080]   mask on because you're politically correct." No, that's not exactly why. No, I think we have to do
[01:42:35.080 --> 01:42:40.440]   that. We as a society have to get to a point where we say, "Look, we're gonna make these changes.
[01:42:40.440 --> 01:42:47.240]   Yes, it is different. We don't go into airports the same way as we did 20 years ago."
[01:42:47.240 --> 01:42:55.080]   And that's okay. It's just what we have to do now. And if restaurants have to have a little bit
[01:42:55.080 --> 01:43:02.120]   lower capacity for another year, that's what they have to do. If people can't be crammed together
[01:43:02.120 --> 01:43:08.280]   into an elevator, then that's what we'll have to do. And I think hopefully those measures will
[01:43:08.280 --> 01:43:15.960]   be able to hold that rate of infection flat, even as we do reopen businesses, reopen schools,
[01:43:15.960 --> 01:43:18.120]   people do go back out in public.
[01:43:20.600 --> 01:43:30.920]   It's really interesting what is accepted as obvious here in Europe and what is considered
[01:43:30.920 --> 01:43:37.480]   contentious for you guys in the US. I actually like wearing a mask. I like the fact that it hides my
[01:43:37.480 --> 01:43:46.280]   face ID. I've started using- I looked into that early on, Leo. And there are, again, we need to
[01:43:46.280 --> 01:43:53.160]   consider who's speaking, but there are outlets in China and Russia who say that they can detach
[01:43:53.160 --> 01:43:58.520]   your face even while you're wearing a mask. Google can tell who you are by the way you walk.
[01:43:58.520 --> 01:44:06.680]   There's a laser that can tell who you are by the rhythm of your heart at 100 yards.
[01:44:06.680 --> 01:44:13.240]   So I'm sure it won't take them too long to upgrade that face recognition hardware
[01:44:13.880 --> 01:44:18.520]   so they can tell who you are. Let's take a little break. A few more stories a little later on,
[01:44:18.520 --> 01:44:22.920]   about 15 minutes, 20 minutes. Baratunde is going to join us. I want to talk a little bit about what's
[01:44:22.920 --> 01:44:27.720]   going on in the world around us with the author of How to Be Black and one of our favorite
[01:44:27.720 --> 01:44:35.080]   guests on our shows. Our show today brought to you by Worldwide Technology. I'll always remember
[01:44:35.080 --> 01:44:41.480]   you worldwide technology. The last trip I took before COVID-19, we went early March, we went to
[01:44:41.480 --> 01:44:47.400]   Seattle and I have to say I'm so glad we did because we saw their Advanced Technology Center.
[01:44:47.400 --> 01:44:54.040]   It's mind-boggling. And right now, as people are figuring out what the, it's actually perfect,
[01:44:54.040 --> 01:44:58.200]   Bill, what you were talking about, companies are all over the world are trying to figure out,
[01:44:58.200 --> 01:45:03.000]   what is work going to look like going forward? Worldwide technology is there for you.
[01:45:03.000 --> 01:45:08.040]   They've always been there for enterprise for more than a decade. You want safety for your employees,
[01:45:08.680 --> 01:45:13.960]   but you got to run your critical operations too. You can do both. Worldwide tech can help with it
[01:45:13.960 --> 01:45:21.880]   all. Everything from remote working, home agents, telehealth, even improvised treatment locations.
[01:45:21.880 --> 01:45:27.240]   Yeah, they've got expertise in that. Even remote learning solutions. These guys
[01:45:27.240 --> 01:45:33.400]   are really amazing. WWT can guide you every step of the way. Let's talk about these one by one.
[01:45:33.400 --> 01:45:38.520]   Remote working, of course. Consumer grade technology helps employees connect with colleagues,
[01:45:38.520 --> 01:45:43.800]   and execute tasks as if they were in the office. Organizations can now begin to move from
[01:45:43.800 --> 01:45:48.840]   urgently responding to a need to maybe going, "I stepped beyond those consumer
[01:45:48.840 --> 01:45:53.560]   grade technologies and enhancing the experience of their remote workers." Yeah, we may be in this
[01:45:53.560 --> 01:45:59.160]   for a while, so let's do it right. Home agents, enabling contact centers to distribute agents
[01:45:59.160 --> 01:46:05.320]   to remote locations of their homes, requires deploying technology that satisfies both technical
[01:46:05.320 --> 01:46:11.000]   and process requirements. Your customer service reps can be in their homes. Organizations can
[01:46:11.000 --> 01:46:16.520]   balance the needs of customers with the health and safety of those home agents. There's also
[01:46:16.520 --> 01:46:22.200]   telehealth, another area of expertise with WWT. Health group organizations have to rapidly connect
[01:46:22.200 --> 01:46:27.400]   with patients remotely to provide virtual care. This is something we need more than ever,
[01:46:27.400 --> 01:46:31.800]   and the technology that can empower providers to connect with patients in their homes,
[01:46:32.360 --> 01:46:37.000]   and its subacute care facilities and temporary care clinics, that's an area of expertise too.
[01:46:37.000 --> 01:46:43.720]   They're great at remote learning. Educational institutions at all levels have to adapt to
[01:46:43.720 --> 01:46:49.160]   teaching students from a distance. Don't cobble together a solution. Worldwide tech can help you
[01:46:49.160 --> 01:46:54.360]   with online curriculums, can make sure educators and students have access, can navigate remote
[01:46:54.360 --> 01:47:00.200]   learning tools and systems for you. Look, we're in a new world, and organizations are facing this
[01:47:00.200 --> 01:47:05.720]   world with new challenges. Leaders are struggling to know what technology decisions they should make
[01:47:05.720 --> 01:47:11.320]   to keep their employees safe and productive while still keeping those critical services running.
[01:47:11.320 --> 01:47:18.280]   And worldwide tech is your partner. Communication for your staff, home setups like audio, webcams,
[01:47:18.280 --> 01:47:23.800]   and monitors. Let worldwide tech work out the complexities and provide you with guidance
[01:47:23.800 --> 01:47:28.200]   that will not only help you maintain business critical operations but optimize them for the
[01:47:28.200 --> 01:47:33.000]   future. Worldwide tech is there for you. Don't let anything stand in the way of servicing your
[01:47:33.000 --> 01:47:39.000]   customers or of keeping business running. Contact WWT today to request a consultation.
[01:47:39.000 --> 01:47:50.840]   WWT.com/twit2. Twit and the number two, that's WWT.com/twit2. WWT. Worldwide technology simplifies the complex.
[01:47:51.320 --> 01:48:00.520]   WWT.com/twit2. WWT, enabling critical operations for business continuity. They're here for you now
[01:48:00.520 --> 01:48:05.000]   in the future, just as they have been over the last decade or so. They're really great.
[01:48:05.000 --> 01:48:12.440]   Worldwide technology, we're glad to be partnered with them. Perfect sponsor for these difficult
[01:48:12.440 --> 01:48:19.400]   times. Don't listen, Bill. Microsoft is laying off 50 journalists to replace them with artificial
[01:48:19.400 --> 01:48:25.720]   intelligence. I'm sorry. This has never worked well. Why doesn't Microsoft know that? I mean,
[01:48:25.720 --> 01:48:33.000]   everybody's tried this. Google tried it. Apple tried. Apple ended up hiring more humans, right?
[01:48:33.000 --> 01:48:39.560]   Yeah. It's tough because we kind of touched on this earlier, right? We talked about this
[01:48:39.560 --> 01:48:46.280]   seemingly insurmountable problem of moderating thousands of tweets or thousands of posts on
[01:48:46.280 --> 01:48:52.600]   hundreds of thousands, millions of posts on Facebook, and trying to use algorithms to do this.
[01:48:52.600 --> 01:49:03.560]   Microsoft is going to try the same thing with the homepage there. I don't think it will work very
[01:49:03.560 --> 01:49:10.280]   well. I like to think that I'm not just biased, that that's actually sort of a reflection of
[01:49:11.400 --> 01:49:21.000]   the nature of the task and humans being able to sort of curate those, the stories that people see,
[01:49:21.000 --> 01:49:25.000]   because it does go back to what we were talking about earlier. I think that
[01:49:25.000 --> 01:49:29.960]   whether it's a social platform or whether it's a news page or whatever it is,
[01:49:29.960 --> 01:49:40.040]   algorithms tend to amplify individual behavior. So if I watch something on Facebook, like something
[01:49:40.040 --> 01:49:46.920]   on Facebook, if I read a particular type of story, then algorithms tend to pick up on that and usually
[01:49:46.920 --> 01:49:52.680]   will promote stories like that to me because that increases my engagement interaction
[01:49:52.680 --> 01:49:58.280]   on the sites, which is the purpose of the sites to increase the time that I spend there and the
[01:49:58.280 --> 01:50:05.240]   information that I give them. So there is something valuable in seeing stories maybe that the algorithm
[01:50:05.240 --> 01:50:09.720]   wouldn't promote to me. Now, I haven't seen the inner workings of the algorithm, maybe that will
[01:50:09.720 --> 01:50:15.080]   they'll incorporate some kind of counter programming. It's like, "Oh, you're reading too many of these
[01:50:15.080 --> 01:50:21.480]   stories. We want to bring you this," or maybe it won't have that involved in it. But I still like
[01:50:21.480 --> 01:50:30.200]   the thing that the human brain is often the best arbiter and the best curator of information,
[01:50:30.200 --> 01:50:38.360]   if you know your audience really well. Yeah. Well, it may also be, Microsoft knows better than we do,
[01:50:38.360 --> 01:50:45.160]   that nobody read Microsoft's news. It comes with every version of Windows 10, and it's also
[01:50:45.160 --> 01:50:51.480]   an MSN. And I think it's part of the Bing group. It's possible that they just said, "Yeah, no one's
[01:50:51.480 --> 01:50:57.560]   reading this, so we can get some robots to do it." Give up on it anyway. Save us money.
[01:50:57.560 --> 01:51:05.960]   It's R&D. Yeah, exactly. It'll be this week that the House, so we've been covering these
[01:51:05.960 --> 01:51:16.440]   Patriot Act reauthorization, in particular, this issue about the FBI getting warrantless searches
[01:51:16.440 --> 01:51:23.800]   of our browser history. warrantless, they could just go to your internet service provider, say,
[01:51:23.800 --> 01:51:31.000]   "I'd like Leo's browser history, please." The Senate tried to amend it, failed by one vote,
[01:51:31.000 --> 01:51:37.560]   59 out of 100 senators voted yes. Now the bill is in the House of Representatives,
[01:51:37.560 --> 01:51:41.960]   because you don't have that same super majority rule, it might be a little easier to get this
[01:51:41.960 --> 01:51:48.680]   through something to watch. The sponsors of the Search Privacy Amendment in the House
[01:51:49.320 --> 01:51:57.000]   are Zollofgren and Warren Davidson of Ohio, so that vote will be this week. It's a simple
[01:51:57.000 --> 01:52:06.760]   amendment, but the key is Section 215 that authorities may not seek an order of authorizing
[01:52:06.760 --> 01:52:11.480]   and requiring the production of internet website browsing information or internet search history
[01:52:11.480 --> 01:52:17.400]   information of US persons. They'll need a warrant to do that. Are you following this Denise?
[01:52:18.440 --> 01:52:23.240]   Yeah, somewhat. I mean, a couple of reactions first in the wake of Edward Snowden, do we really
[01:52:23.240 --> 01:52:29.640]   think that they-- Doesn't matter. --care about it. They already know everything, I suppose.
[01:52:29.640 --> 01:52:34.280]   You know, warrants are good and we want them to, whether they're going to--
[01:52:34.280 --> 01:52:41.080]   We would send a message. Listen to it, yes, exactly. So the interesting thing is that three
[01:52:41.080 --> 01:52:46.840]   senators were absent when they did the first vote, where they were off by one vote. So
[01:52:46.840 --> 01:52:54.040]   if those three people who include Lamar Alexander from Tennessee, Heidi Murphy from Washington,
[01:52:54.040 --> 01:53:02.200]   and Bernie Sanders from Vermont. Oh, and actually the war. Ben Sassy, yeah. So if they come back
[01:53:02.200 --> 01:53:07.320]   in, any one of them could be the vote that's needed. Well, we'll see what happens.
[01:53:07.320 --> 01:53:13.400]   The thing that gets me about all these Leon Denise can talk about this probably better than me,
[01:53:13.400 --> 01:53:17.960]   warrants are not that hard to get. Right. I mean, especially in the digital age,
[01:53:17.960 --> 01:53:22.760]   I mean, there are ways you can have a judge at a baseball game that can sign a warrant
[01:53:22.760 --> 01:53:28.600]   electronically on their phone. I mean, the bar, the information that has to be provided to get a
[01:53:28.600 --> 01:53:33.720]   warrant is fairly low. So if you've got a case at all, you know, again, somewhat, you can get a
[01:53:33.720 --> 01:53:41.880]   warrant. But hopefully what it does do is, you know, Edward Snowden aside, is try to sort of
[01:53:41.880 --> 01:53:47.000]   prevent some of the mass surveillance that we saw back during the warrantless wiretapping
[01:53:47.000 --> 01:53:53.320]   scandals we had here in the US. So, you know, hey, it's a move maybe in the right direction.
[01:53:53.320 --> 01:53:56.920]   At least it casts a light on it. Yes. Yes.
[01:53:56.920 --> 01:54:03.400]   One to watch. It doesn't seem kind of minor compared to all the other issues we're facing these days.
[01:54:07.240 --> 01:54:14.120]   Well, let's see something happy. Anybody get HBO Max? Anybody you don't have to pay attention to
[01:54:14.120 --> 01:54:22.520]   this, Patrick? Did they come very happy? You have it? I have HBO Nordic. I don't know how
[01:54:22.520 --> 01:54:29.240]   hard it is. It is an accident. I bet there's a lot of tall, powerfully built blonde people in it.
[01:54:29.240 --> 01:54:36.600]   A lot of the girl with the dragon tattoo. Yeah. I have been watching Vikings. So, yes,
[01:54:36.600 --> 01:54:42.040]   Vikings are pretty good, actually. I like Vikings. That would be on HBO Nordic. That makes sense.
[01:54:42.040 --> 01:54:50.200]   But I think they launched it a few years ago when they were thinking about understanding that
[01:54:50.200 --> 01:54:54.840]   probably at some point they would need to do that. And they thought, which region can we do that
[01:54:54.840 --> 01:55:01.400]   easily without being bogged down in licensing issues? And they figured Scandinavia. They will
[01:55:01.400 --> 01:55:05.480]   speak English anyway. So, let's do that. And it's been working pretty well.
[01:55:05.480 --> 01:55:09.080]   10 Danner wants to know if they call it HBO Royale in France.
[01:55:09.080 --> 01:55:16.760]   Because it's the big Mac called in France. It's called the
[01:55:16.760 --> 01:55:23.800]   Royale with cheese. Royale with cheese. Royale is something else, which is really delicious. So,
[01:55:23.800 --> 01:55:32.520]   the Royale, actually, the issue was all of the burgers from Mac Don'ts were a little bit
[01:55:32.520 --> 01:55:36.920]   too sweet for French tastes. Oh, yeah. We put a little sugar in those.
[01:55:36.920 --> 01:55:45.400]   And so, they designed a French-Pacific burger. They called it the Royale. And it's got pepper
[01:55:45.400 --> 01:55:51.640]   sauce that is really, really nice. Very savory. I remember I got a big Mac in France. It tasted
[01:55:51.640 --> 01:55:58.040]   awful. It was weird. It was weird. Because it didn't have... It wasn't sweet enough, to be honest.
[01:56:01.320 --> 01:56:04.520]   Don't you guys get high fructose corn syrup there in Paris?
[01:56:04.520 --> 01:56:11.000]   Not as much. No. I always say I love Mac Don'ts. And my American friends look at me
[01:56:11.000 --> 01:56:15.800]   completely bewildered. And I realized when I go to the US, I actually don't. It's...
[01:56:15.800 --> 01:56:19.320]   No. It's a different... You don't like our McDonalds. Yeah. It's a different thing.
[01:56:19.320 --> 01:56:23.720]   Well, I will agree with Patrick, the best McDonalds I ever had when I was
[01:56:23.720 --> 01:56:29.400]   not a vegetarian was in Paris. So, you know, completely different than the McDonalds I was
[01:56:29.400 --> 01:56:36.840]   used to here. So... Now I feel guilty for his chewing McDonalds any time I visit the capital of France.
[01:56:36.840 --> 01:56:43.000]   I shall have to try it. Anyway, HBO Mac, I think this is the week that they finally pushed us
[01:56:43.000 --> 01:56:49.320]   to Dam Farr with too many damn subscriptions. And, you know, Quibi might have been the beginning
[01:56:49.320 --> 01:56:57.960]   of the end, but HBO Max is just the end. It's like, no. And now, Peacock from NBC is due out soon.
[01:56:57.960 --> 01:57:04.120]   And it's like, no, enough. Well, it is hard. And, you know, I can't say we've got our own streaming
[01:57:04.120 --> 01:57:09.240]   service, all access, great CBS product put in a promo for that. There you go. But
[01:57:09.240 --> 01:57:15.960]   I wonder about that. I mean, how many streaming services do all of us have? I think I don't really,
[01:57:15.960 --> 01:57:21.960]   I would have to sit here and count them all up that either came with something I have. I got a text.
[01:57:21.960 --> 01:57:27.560]   I have AT&T service for my cell, it was my personal cell phone. And they sent me a text and said,
[01:57:27.560 --> 01:57:35.160]   oh, by the way, as part of your plan, you now get HBO Max free. I was like, oh, okay, don't I already
[01:57:35.160 --> 01:57:41.160]   have go through my cable provider? And then, wait a minute, what's this now thing? What's the
[01:57:41.160 --> 01:57:47.720]   difference between Max and go? And now, which one do I want? I know. It's completely confusing.
[01:57:48.360 --> 01:57:54.280]   And do I already get it for free? Because you don't want to pay for it. If and there's 20 ways to
[01:57:54.280 --> 01:58:00.280]   get it for free. So it's very confusing. Thank you, Ashley, a Scetha at CNET, who actually went
[01:58:00.280 --> 01:58:05.560]   through and ranked every streaming service. Did you know there are more than a hundred?
[01:58:05.560 --> 01:58:15.560]   Oh, wow. Number one Amazon prime, then Hulu Netflix HBO now HBO Max, Disney plus CBS all access,
[01:58:16.280 --> 01:58:24.920]   Peacock, Acorn TV and Showtime in the top 10. But there's also Mubi and High Dive and
[01:58:24.920 --> 01:58:32.920]   Fylo and Crunchyroll and up faith and Fandor and Ginks and Hia.
[01:58:32.920 --> 01:58:43.320]   This is great. Can I defend it for a second? Yes. This is like an American grocery store.
[01:58:45.240 --> 01:58:53.080]   Endless choice. The internet's disintermediation, which means you can have essentially TV channels
[01:58:53.080 --> 01:59:00.600]   for curated niche tastes. You mentioned Crunchyroll. That could, that they tried it many times,
[01:59:00.600 --> 01:59:06.680]   you know, have at least in France, have an anime, Japanese animation based TV channel. It
[01:59:06.680 --> 01:59:13.800]   didn't work. It wasn't possible. It was too expensive. The internet allows the market to support that.
[01:59:13.800 --> 01:59:20.360]   And all of these, I'm sure there will be some form of consolidating for these services. There
[01:59:20.360 --> 01:59:26.840]   might be too many. But there are many that can serve people who wouldn't have access to that
[01:59:26.840 --> 01:59:32.840]   thing that they love in that form. We wanted dis-aggregation. We begged them. We said,
[01:59:32.840 --> 01:59:38.440]   give us Al-A-Kart. It's not quite Al-A-Kart, but give us something. I agree because I subscribe to
[01:59:38.440 --> 01:59:46.840]   like Britbox, which is British TV. I subscribe to the Criterion channel, which is weird old black
[01:59:46.840 --> 01:59:51.320]   and white shows. So, you know, it's nice. Yeah, you're right. It's nice to have what Crunchyroll is what?
[01:59:51.320 --> 01:59:57.240]   Anime. Well, there you go. Probably should be subscribing to that.
[01:59:57.240 --> 02:00:04.840]   I spent all my money getting to do a Duchess ship at the sea land. So I can't do that. If you have,
[02:00:06.840 --> 02:00:14.120]   they didn't tell me Duchess was just for women. Okay. If you had an echo look camera,
[02:00:14.120 --> 02:00:19.800]   I gave it to somebody though. I don't remember who I gave it to. It's going to stop working in July.
[02:00:19.800 --> 02:00:26.600]   This was the camera that goes in your closet, looks at your wardrobe, and tells you if you should
[02:00:26.600 --> 02:00:32.760]   never wear that again. I kept, you know, it didn't really do a good job for me. So I gave it. I feel
[02:00:32.760 --> 02:00:37.240]   like I gave it to Justine or somebody who could use it. Oh, maybe Georgia Dow. I don't remember.
[02:00:37.240 --> 02:00:43.240]   Megan, maybe Megan. I'm going to miss mine. Do you have one? Yes, I have one. And I use it all
[02:00:43.240 --> 02:00:50.040]   the time to post to Instagram. I've never used in any sort of thorough going way their stylist
[02:00:50.040 --> 02:00:53.960]   advice, which is what it was marketed. You know, you would compare a couple of outfits and it
[02:00:53.960 --> 02:01:00.360]   would tell you where that one, not that one. But you have excellent taste. You have such a good
[02:01:00.360 --> 02:01:08.520]   style. That's so sweet of you. No, I just use it as a camera. So you can say, take a picture of me
[02:01:08.520 --> 02:01:13.960]   and post it on Instagram? Not directly from their app, but you save it locally and then you can
[02:01:13.960 --> 02:01:19.960]   post it. Yeah, that's my entire Instagram account for the last two years. It's been courtesy of the
[02:01:19.960 --> 02:01:24.040]   echo look. So I'm going to have to find an alternative. And what are you going to do? Oh,
[02:01:24.040 --> 02:01:29.720]   look at all the this is all echo look. Oh, these, this is amazing. Well, thank you.
[02:01:29.720 --> 02:01:36.280]   That's your whole Instagram. Is this so well, I mean, aside from the occasional,
[02:01:36.280 --> 02:01:42.920]   hey, I'm going to be on Twitter or something. This is so great. What is going on here, though? I
[02:01:42.920 --> 02:01:50.360]   mean, are you, do you want to be an influencer? No, no, no, what's going on here is, you know,
[02:01:50.360 --> 02:01:56.280]   as sort of an adult passion of mine that has taken hold in later life is I really
[02:01:56.280 --> 02:02:04.680]   want people to be aware of the textile waste problem in the US and globally. And so almost
[02:02:04.680 --> 02:02:09.480]   everything I wear is secondhand and yeah, almost everything. So,
[02:02:09.480 --> 02:02:17.400]   Lisa is my wife is so jealous. She loves your style. She thinks you are just so fancy.
[02:02:18.360 --> 02:02:21.800]   But yeah, I'm going to have to talk to answer someone about a good replacement.
[02:02:21.800 --> 02:02:25.480]   Yeah, this is all let go look. So and it's blurring out the background and everything.
[02:02:25.480 --> 02:02:28.520]   And that's all just look that's hysterical. Right.
[02:02:28.520 --> 02:02:34.440]   It has a little light that's built into it. So you don't have to. Yeah, it's,
[02:02:34.440 --> 02:02:39.720]   it's going to be a pain when it doesn't work anymore. But this is not in your closet. I can see.
[02:02:39.720 --> 02:02:45.080]   No, it's not. It's just so the reason that it works for me for this purpose is I just, you know,
[02:02:45.080 --> 02:02:50.040]   it's it's on my way out the door. My office is past my bedroom and as I'm leaving,
[02:02:50.040 --> 02:02:55.000]   I can just stop and take a picture and then I'm done. And then so I just need something that
[02:02:55.000 --> 02:03:02.440]   well, Instagram.com/dehowl. I had no idea. I knew, you know, I could tell you were a fashion
[02:03:02.440 --> 02:03:07.480]   plate, but I had no idea that it was secondhand and this was this was the idea was to save
[02:03:07.480 --> 02:03:11.400]   textiles. That's really cool. That's that's the whole point. Yes. So cool.
[02:03:11.400 --> 02:03:16.120]   Sustainable fashion. Well, near I found somebody who's upset about the look.
[02:03:16.120 --> 02:03:21.880]   It is kind of a. Yeah. But it's my own dank fall, right? You shouldn't ever rely on one platform to
[02:03:21.880 --> 02:03:26.120]   but how much did you? I mean, these were hot. This was expensive. It was like a hundred,
[02:03:26.120 --> 02:03:28.920]   two hundred dollars. It was expensive. Yeah, they were like 200 bucks. Yeah.
[02:03:28.920 --> 02:03:35.320]   Yeah, I am sad. Yeah. But I've gotten my use out of it. I will say that. Yes, you clearly,
[02:03:35.320 --> 02:03:43.240]   you clearly have the big internet hit of the spring of the COVID crisis was John Krasinski's
[02:03:43.240 --> 02:03:50.200]   some good news. It launched like gangbusters, millions of views of every single one of them.
[02:03:50.200 --> 02:03:58.760]   He says, I never intended to keep doing it posted the first episode on March 30th, 17 million views
[02:03:58.760 --> 02:04:06.520]   on that first episode. The virtual prom 6.3 million views. Apparently Krasinski never
[02:04:06.520 --> 02:04:10.840]   intended to keep doing it. He's too busy being Jack televisions Jack Ryan, I guess.
[02:04:10.840 --> 02:04:20.280]   He auctioned it off and your company bought it, Bill. Yeah. Viacom CDS. CBS did buy it. So,
[02:04:20.280 --> 02:04:24.600]   you know, I love Krasinski. So I'll really be interested to see what they do with it.
[02:04:24.600 --> 02:04:28.840]   Well, here's the problem. They bought some good news, but they didn't buy John.
[02:04:28.840 --> 02:04:33.800]   Yeah, that's you will have to see what happens with that. You know, what they have,
[02:04:33.800 --> 02:04:40.280]   you know, there's a lot of really amazing people that do content. And so I'm hopefully they'll come
[02:04:40.280 --> 02:04:45.960]   up with, you know, a great plan. But, you know, we'll have to see, you know, and what's interesting
[02:04:45.960 --> 02:04:54.280]   to me was sort of the flack. I guess that John caught for selling this. I thought that was a little
[02:04:54.280 --> 02:04:58.520]   bit, you know, I just kind of wondered where that was coming from from people. I guess,
[02:04:58.520 --> 02:05:02.120]   you know, he struck such a nerve. People kind of hated to see it stop.
[02:05:02.120 --> 02:05:09.320]   Well, you're not allowed if you're a YouTube success to quit ever. And you can only do more
[02:05:09.320 --> 02:05:15.000]   videos, never fewer. And apparently John didn't know the rules of the internet. So that's what
[02:05:15.000 --> 02:05:20.600]   we're angry. You're not allowed to have your own life. No, dare you, John. How dare you?
[02:05:23.160 --> 02:05:29.560]   Let's see. Oh, and there's one last story. And I think this is a telling of the times.
[02:05:29.560 --> 02:05:35.800]   I've been a Pokemon Go fan since July 2016 when it first came out. I've never gone to a Pokemon
[02:05:35.800 --> 02:05:44.520]   Go Fest. This one I'm going to get to go to because it's virtual Pokemon Go Fest, July 25th to 26th.
[02:05:44.520 --> 02:05:51.000]   You don't have to go to Chicago. You don't have to go anywhere. Just, I don't know what,
[02:05:51.000 --> 02:05:55.240]   just play your game. They've actually done a really good job of making Pokemon Go work better
[02:05:55.240 --> 02:06:02.440]   without leaving the house. It's, in some ways, it's much more fun. I am so glad you were here
[02:06:02.440 --> 02:06:07.640]   Denise Howell. Thank you so much for being here. Denise is at Denise Howell.info. And obviously,
[02:06:07.640 --> 02:06:13.560]   you must follow her on Instagram, D Howell. That's awesome. It's also D Howell on Twitter.
[02:06:13.560 --> 02:06:18.440]   It's so good to be with you all. It's just so good to connect. I love you, Denise. And I miss
[02:06:18.440 --> 02:06:22.680]   you. And you're always a part of the Twit family always. Love you guys too.
[02:06:22.680 --> 02:06:28.520]   Yeah. Also, well, I kind of like Patrick Beisha. I'm not going to say I love you, but I do like
[02:06:28.520 --> 02:06:33.160]   you a lot. Patrick Beisha, FrenchSpin.com. He has English language and French language
[02:06:33.160 --> 02:06:42.280]   shows at FrenchSpin.com. Anything else? Yeah, we just launched the... Sorry, I was just going to ask
[02:06:42.280 --> 02:06:50.680]   you anything new. Well, we just launched a show with our good friend Tom Merritt called Work In Sanity,
[02:06:50.680 --> 02:06:58.120]   which every week gives you a tip and advice of 15 minutes on Mondays about working from home.
[02:06:58.120 --> 02:07:04.360]   Oh, where's that going to be on FrenchSpin or where? Oh, it's just on its own website. Just
[02:07:04.360 --> 02:07:10.600]   Work In Sanity. We launched it last week. So it's very new. Just you can search for that
[02:07:10.600 --> 02:07:16.440]   on your podcast app. And I love the name which I came up with, by the way, Work In Sanity.
[02:07:16.440 --> 02:07:20.840]   Yeah, it's very clever. It's exactly the world we're living in these days.
[02:07:20.840 --> 02:07:25.400]   Work in Sanity. Well, congratulations, congratulations to Tom too, another beloved
[02:07:25.400 --> 02:07:32.600]   former Twit family member. So thank you, Patrick. Great to see you. And thank you, Bill Detweiler. I
[02:07:32.600 --> 02:07:36.600]   knew we were a member of the family, but always glad having you on under-in-chief at Tech Republic.
[02:07:37.480 --> 02:07:42.200]   Took Jason Heiner's place and just great to have you. How are things that they're
[02:07:42.200 --> 02:07:45.480]   calming down a little bit and Louisville, I know, was a little crazy over the last few nights.
[02:07:45.480 --> 02:07:52.920]   We'll have to see hopefully tonight's a little better than it was last night. We do seem to be
[02:07:52.920 --> 02:08:00.680]   getting into the actual peaceful protests more so than some of the violent activity we've had. So
[02:08:00.680 --> 02:08:06.360]   fingers crossed that that that continues. Excellent. Great to have you, Bill. Thank you
[02:08:06.360 --> 02:08:09.560]   all for joining me. As I mentioned, we're going to stick around a little extra. We're going to get
[02:08:09.560 --> 02:08:14.440]   Baratunde Thurston on the line and talk a little bit about what's been going on. I wanted to kind of
[02:08:14.440 --> 02:08:19.400]   keep that as a separate part of the show. But for those of you tuning out, thank you for being here.
[02:08:19.400 --> 02:08:26.440]   We do Twit every Sunday afternoon, 230 Pacific, 530 Eastern. That's 2130 UTC. You can watch live
[02:08:26.440 --> 02:08:32.760]   at Twit.tv/live. You can always download shows. We're on YouTube, of course. We're slackers. We
[02:08:32.760 --> 02:08:38.520]   only do one Twitter week, but we're on it. We also are on our website. A good place to find us
[02:08:38.520 --> 02:08:43.560]   twit.tv. And if you're on the website, you'll see subscribe buttons. It's the best way to get the
[02:08:43.560 --> 02:08:47.720]   show is just subscribe in your favorite podcast application. And that way you'll get it the
[02:08:47.720 --> 02:08:53.400]   minute it's available of a Sunday night for your Monday morning commute from your bedroom to the
[02:08:53.400 --> 02:08:59.800]   living room. Thank you, everybody, for joining us. And we'll see you next time. Another twit is in
[02:08:59.800 --> 02:09:07.080]   the can. Bye bye. So we're back after a little break. I wanted to really address what's going on
[02:09:07.080 --> 02:09:14.360]   in this country. But I didn't know if it fit in with the format of Twit. And I also wanted to get
[02:09:14.360 --> 02:09:21.320]   somebody on who really could speak to this beloved friend. We haven't seen in a while. But I know
[02:09:21.320 --> 02:09:27.160]   who somebody whose heart and mind are in this problem and that's Barretunde Thurston, the author
[02:09:27.160 --> 02:09:33.640]   of how to be black. I almost feel like you need to write a new book how to be white in this world.
[02:09:33.640 --> 02:09:39.960]   Where that is someone else's job. But that book should be written. It needs to be written. It needs
[02:09:39.960 --> 02:09:44.280]   to be read and endorse it. So it's pretty good. Yeah, first of all, let me ask how you are.
[02:09:44.280 --> 02:09:52.840]   Yeah, so thank you for having me on. It's been a while. It has good to hear your voice. I brought
[02:09:52.840 --> 02:10:00.280]   my own super serious microphone. So I hope the audience can understand. I was trying to channel
[02:10:00.280 --> 02:10:06.840]   my inner Leo and just add a little extra bass to a voice that's already filled with bass, of course.
[02:10:06.840 --> 02:10:12.680]   But so it's good to see you. It's good to be back. And I love your audience as well as you.
[02:10:14.280 --> 02:10:23.880]   And so and thanks for asking how I'm doing. I am. Up and down and exhausted and a little excited
[02:10:23.880 --> 02:10:33.240]   and a lot nervous. I think this week, literally the past seven days have been quite exhausting.
[02:10:33.240 --> 02:10:42.600]   For all of us. Yes. And so I think it's sort of and it comes on the heels of exhaustion. Exhaustion.
[02:10:42.600 --> 02:10:48.200]   Right. Like months of exhaustion. Yeah. If not a marathon, a good 800 meter run,
[02:10:48.200 --> 02:10:53.960]   which is like a serious track event for anyone who didn't know that. And then at the end of that
[02:10:53.960 --> 02:11:02.520]   race, coach is like, I need you to do the 200 hurdles. Yeah. Backwards. Right. So just that's
[02:11:02.520 --> 02:11:11.640]   that's kind of where my energy is. It's it's kind of low, a little manic, right, worried. And, and,
[02:11:11.640 --> 02:11:19.960]   and somewhat motivated in terms of the hints of good things I'm seeing, but also just like,
[02:11:19.960 --> 02:11:26.760]   maybe it's not a not a full predictive commitment to like a great outcome of all this either.
[02:11:26.760 --> 02:11:34.040]   Because we've been through so much of this before. It is. I mean, what's interesting is that the
[02:11:34.040 --> 02:11:48.520]   video of George Floyd's murder was so vivid and so clear and so unequivocal that even people like
[02:11:48.520 --> 02:11:57.240]   Janine Pirro on Fox News had to say I'm this is awful. That's appalling. I'm shocked. Even people
[02:11:57.240 --> 02:12:06.840]   who would have normally made excuses couldn't look at it and and say, Oh, well, it was clear.
[02:12:06.840 --> 02:12:13.160]   Yeah. On the other hand, somebody like a person of color of an African American,
[02:12:13.160 --> 02:12:17.720]   especially an African American man living in a big city must have looked at this.
[02:12:18.440 --> 02:12:22.120]   And what was your reaction when you first saw it?
[02:12:22.120 --> 02:12:28.840]   Listen, so, so the way I came to George Floyd's story and his tragic end was actually by way of a
[02:12:28.840 --> 02:12:36.440]   woman named Amy Cooper. And so the news junkies amongst your listeners will already know who I'm
[02:12:36.440 --> 02:12:42.920]   referring to. But there was a there was an interesting coincidence on one Monday ago,
[02:12:42.920 --> 02:12:49.320]   where we actually had two racialized policing events happen in the United States. In the morning,
[02:12:49.320 --> 02:12:56.120]   in Central Park, a black man was bird watching in a section of the park where you're supposed
[02:12:56.120 --> 02:13:02.840]   to keep your dog in a leash so they don't trash the bird life. And a white woman, Amy Cooper,
[02:13:02.840 --> 02:13:09.400]   was with her dog unleashed. And so the man coincidentally named Christian Cooper, no relation,
[02:13:10.120 --> 02:13:14.040]   calls her out says, Hey, you need to put your dog in a leash. And she's like, Nah, I'm not going
[02:13:14.040 --> 02:13:17.480]   to do it. Please put your dog in a leash. I'm not going to do that. They go back and forth.
[02:13:17.480 --> 02:13:23.320]   He starts offering her dog treats, which he has learned as an avid bird watcher. He's literally
[02:13:23.320 --> 02:13:29.000]   like a board member of the New York Audubon Society. If you offer somebody stranger dog treats,
[02:13:29.000 --> 02:13:33.720]   that owner will bring their dog to heel, put the dog in a leash because nobody wants strangers
[02:13:33.720 --> 02:13:39.160]   feeding their dog. So she gets super offended by this. She's demanding that he stop recording
[02:13:39.160 --> 02:13:42.840]   her on the phone, which he had started doing because she was getting super agitated.
[02:13:42.840 --> 02:13:48.520]   She goes up points her finger in his face in a very non socially distant social distance,
[02:13:48.520 --> 02:13:54.120]   adhering fashion and threatens him. I'm going to call the police and tell them that an African
[02:13:54.120 --> 02:13:59.960]   American man is threatening my life, just as a point blanked. And he wasn't. But that's what she
[02:13:59.960 --> 02:14:04.280]   says she was going to do. And if you're black man in America, that's a death threat. That's
[02:14:04.280 --> 02:14:10.360]   that's a potential death sentence, right? And that has been a theoretical outcome and an actual outcome
[02:14:10.360 --> 02:14:16.440]   all too often. I did a whole TED talk just about the phenomenon of white people, largely women,
[02:14:16.440 --> 02:14:22.200]   calling the cops on black people for just living their lives. And it doesn't get more life living
[02:14:22.200 --> 02:14:28.440]   ish than bird watching. Right? You want to talk about non threatening person, really? Yeah,
[02:14:28.440 --> 02:14:35.320]   really? Yeah. So she she offers such such clarity. And then she calls and she puts on a bit of a
[02:14:35.320 --> 02:14:40.120]   performance for the 911 dispatcher. She claims that only as he trying to threaten her, he's threatening
[02:14:40.120 --> 02:14:48.200]   her dog while she is choking the dog on camera. This is all on camera. And then she starts crying a bit,
[02:14:48.200 --> 02:14:54.280]   send the cops immediately is African American. She said it like five times like it was a slur.
[02:14:54.280 --> 02:14:58.040]   African American man is threatening me. He's threatening my life. He's threatening my dogs and the cops
[02:14:58.040 --> 02:15:03.800]   immediately. And that's the video ends with him thanking her. And nothing happened to him
[02:15:03.800 --> 02:15:10.600]   other than he got a lot of sympathy from people all across this country, because this person abused
[02:15:10.600 --> 02:15:17.560]   the power that she has. And the outcome of what she did to him could have been what happened to
[02:15:17.560 --> 02:15:24.200]   George Floyd, which happened that very night. Also Monday, 1000 or so miles away in Minneapolis.
[02:15:24.200 --> 02:15:33.480]   And we have all seen this snuff film, this assassination take of one law enforcement officer
[02:15:33.480 --> 02:15:38.280]   persisting and applying pressure to the human neck with his knee to a man lying in a prone
[02:15:38.280 --> 02:15:46.120]   position handcuffs behind his back for nine minutes. It turns out nine minutes. And three other officers
[02:15:46.120 --> 02:15:50.520]   doing nothing about it, while it's all being filmed while passers by are saying stop that,
[02:15:50.520 --> 02:15:55.240]   you're going to kill him while the man himself is saying, I can't read reminiscent of Eric Garner
[02:15:55.240 --> 02:16:01.800]   in Staten Island a few years ago, who died of an illegal chokehold implemented by officer Pantaleo.
[02:16:01.800 --> 02:16:09.160]   So I it was a Tuesday is when all this stuff was coming out and Wednesdays when it really landed.
[02:16:09.160 --> 02:16:16.680]   And so for me, it was partially, you know, didn't these people see my TED talk? Like I tried to tell
[02:16:16.680 --> 02:16:22.200]   you, you know, like I put it in a format. I had a jokes, right? This sort of ridiculous response.
[02:16:22.200 --> 02:16:29.400]   There was anger, a ton of anger. I couldn't have asked for a better lesson for people who have
[02:16:29.400 --> 02:16:34.840]   refused to listen. The thousands of times this has happened before. Well, here it's all laid
[02:16:34.840 --> 02:16:40.280]   out pretty cleanly for you. And so often, as you pointed out, Leo, the victims in these circumstances
[02:16:40.280 --> 02:16:47.960]   are someone comes up with an excuse for why it was okay for them to be killed. The officer was
[02:16:47.960 --> 02:16:53.160]   threatening, you know, feeling threatened. His life was in danger. It was a split second decision.
[02:16:53.160 --> 02:16:55.560]   The person was armed. They were running. They were moving.
[02:16:55.560 --> 02:16:56.120]   Got that.
[02:16:56.120 --> 02:17:05.960]   And the civil rights movement often moved forward and strategically tried to find
[02:17:05.960 --> 02:17:10.040]   perfect victims because it knew the conscience of white America would refuse to awaken
[02:17:10.040 --> 02:17:17.720]   with anyone less than perfect. And so there were like castings almost to find someone who didn't
[02:17:17.720 --> 02:17:22.680]   have any kind of drama in their past who could be in front of the march, who could be on camera.
[02:17:22.680 --> 02:17:29.880]   And that wasn't the case here. It just happened that way. But yeah, it's sad because it's so not new.
[02:17:29.880 --> 02:17:32.760]   What else is said is so late.
[02:17:32.760 --> 02:17:36.600]   It's close to boredom. It continues because the coroner's report
[02:17:36.600 --> 02:17:39.560]   brought attempted to bring up all sorts of
[02:17:39.560 --> 02:17:49.960]   you know, qualifying conditions, heart condition, diabetes. They even said possible
[02:17:49.960 --> 02:17:55.960]   intoxicants, even though there was no, the medical lab reports aren't out yet and won't be out for
[02:17:55.960 --> 02:18:05.560]   some time. But we know, we know, we know, my politics are very straightforward and your listeners
[02:18:05.560 --> 02:18:09.400]   know that. So we'll come as no surprise, my opinion about the president of this country.
[02:18:09.400 --> 02:18:15.640]   But I think in this regard, it's potentially universally acceptable what I'm about to say.
[02:18:16.440 --> 02:18:21.720]   The president of this country does not behave in a decent way, right? Even if you love his tax plan
[02:18:21.720 --> 02:18:29.320]   and you love his stance on immigrants, he is an indecent being. The way he performs,
[02:18:29.320 --> 02:18:35.560]   the way he holds the office, the language he uses, the way he uses Twitter, the way he bullies victims.
[02:18:35.560 --> 02:18:40.440]   It's just like, it's very hard to look at your child and say, I want you to behave the way the
[02:18:40.440 --> 02:18:44.360]   president of the United States. It doesn't matter what his defenders say. It doesn't matter what
[02:18:44.360 --> 02:18:53.320]   appundant says. Even his biggest fans know, he does not behave well. And so I think the coroner's
[02:18:53.320 --> 02:18:59.640]   going to have a report. There was some delay in the prosecution. We all saw it. I'm seeing videos
[02:18:59.640 --> 02:19:05.080]   of uniform law enforcement officers saying no, I'm seeing a police chief of the police arch
[02:19:05.080 --> 02:19:10.520]   with Black Lives Matter saying no. I love that. And it's so rare for police to break ranks,
[02:19:10.520 --> 02:19:17.480]   right, with their fraternal organization. So we all know what we saw. I think that there are
[02:19:17.480 --> 02:19:24.280]   questions that emerge. What are we going to do? And I think for those who look down on
[02:19:24.280 --> 02:19:29.640]   the actions of that officer, look down on the actions of the Amy Cooper's of the world,
[02:19:29.640 --> 02:19:35.880]   who would call that officer to the scene and unleash him on a Black person for forgery or
[02:19:35.880 --> 02:19:42.040]   bird watching, neither of which of resultant death. Then I think there's a question of what to do
[02:19:42.040 --> 02:19:49.640]   with the power that this country has granted you. And that's what I've started to look at this
[02:19:49.640 --> 02:19:56.840]   week and try to think about for the folks who listen to me. A lot is made about white privilege
[02:19:56.840 --> 02:20:02.600]   and white supremacy. And that triggers a response and a lot of white, I don't feel so privileged.
[02:20:02.600 --> 02:20:08.600]   I don't feel very powerful. And for those who acknowledge it, I don't want it. It's guilty.
[02:20:08.600 --> 02:20:12.440]   I'm not any of the whole white guilt thing. And that gets very useless to feel guilty about
[02:20:12.440 --> 02:20:19.880]   something like that. But here's which truth. This country confers a lot of extra consideration
[02:20:19.880 --> 02:20:27.960]   and higher, you know, rated outcomes to white people in this country in so many different areas.
[02:20:27.960 --> 02:20:36.520]   And this week made it so starkly clear. So what do you do with that power? And signs of hope
[02:20:36.520 --> 02:20:41.880]   that I've seen this week, you use it as a lot of white people did in the protest in Minneapolis
[02:20:41.880 --> 02:20:47.960]   to create a line between the police and the Black protesters, because you know they're less likely
[02:20:47.960 --> 02:20:54.280]   to beat up white people, especially on camera. You use it to send funds to the Minnesota Defense
[02:20:54.280 --> 02:20:59.880]   League and all these bail out organizations across the country, because you know who's more likely
[02:20:59.880 --> 02:21:06.600]   to get arrested. And you take that hyper responsiveness of the system. And you also look at
[02:21:06.600 --> 02:21:13.000]   questioning where that power came from and talking to other white people to start working on this
[02:21:13.000 --> 02:21:18.440]   thing, just like I as a man, I have to talk to other men when I see them acting wrongly toward
[02:21:18.440 --> 02:21:22.120]   women, because it's not going to a woman may not even see that thing going down because the
[02:21:22.120 --> 02:21:27.720]   guy will only say it around me. So if not me, there is no check. And so the work of fixing it
[02:21:27.720 --> 02:21:34.760]   has to keep shifting to those who are benefiting from it being broken in the first place. And so
[02:21:34.760 --> 02:21:37.960]   that's part of how I'm feeling as well and what I've been thinking about.
[02:21:37.960 --> 02:21:45.160]   Yeah, you said something interesting that the civil rights movement wanted to activate the
[02:21:45.160 --> 02:21:53.160]   conscience of white people. And I think that perhaps that's one of the things,
[02:21:53.160 --> 02:22:00.680]   one of the maybe positive things that's happened here. But the problem I think for a lot of us is
[02:22:00.680 --> 02:22:09.560]   we don't know what to do next. Right. Yeah. I'm sure that's a problem for you too. It's not obvious.
[02:22:10.360 --> 02:22:16.840]   It's not obvious. But here's the good news, especially. This is why I love the Twitter audience, Leo.
[02:22:16.840 --> 02:22:22.600]   I mean, I've hung out in the IRCs, you know, like I have seen the tweets. I've gotten so many people
[02:22:22.600 --> 02:22:29.160]   signing up on my mailing list or my text phone number explicitly because of this show. So I know
[02:22:29.160 --> 02:22:33.800]   a certain thing to be true, at least about a part of your audience. You're resourceful.
[02:22:35.160 --> 02:22:40.120]   This of any crowd you were on the internet before everybody else, you know how to Google good,
[02:22:40.120 --> 02:22:45.720]   right? Not the lazy kind. You know how to do real internet research, not just that front page
[02:22:45.720 --> 02:22:51.880]   or the top three or the ad purchase clicks that your aunt and uncle click on. And you can literally
[02:22:51.880 --> 02:23:00.600]   read between the lines. So now is a time to use that ability to find good information. I have
[02:23:00.600 --> 02:23:08.520]   taken a step and created something as a list of resources to start. There are so many people
[02:23:08.520 --> 02:23:14.040]   building lists like these, but on Instagram, which is where I'm kind of digitally living
[02:23:14.040 --> 02:23:20.680]   outside of veritun.com. My bio link goes to a lot of places. There's books, literally a bookshop
[02:23:20.680 --> 02:23:26.600]   that I set up today on bookshop.org, which directs funds to independent booksellers instead of Jeff
[02:23:26.600 --> 02:23:31.720]   Bezos. Well, I think God in the limit say he has enough money. So we've covered Jeff.
[02:23:31.720 --> 02:23:37.640]   Jeff's good. He's good. He's good. That's good for a few lifetimes, which I think is about right.
[02:23:37.640 --> 02:23:43.240]   And now we can support the independent booksellers of this country. And so there are wonderful books
[02:23:43.240 --> 02:23:48.440]   that can help get people up to speed. There are people who do trainings. There's a woman in Rachel
[02:23:48.440 --> 02:23:57.000]   Kargle. She's on Patreon. She's on YouTube. She's everywhere we are as Twitter people. And she has
[02:23:57.000 --> 02:24:05.560]   live streams and curricula and Q&As and multimodal multimedia. There's an organization which is
[02:24:05.560 --> 02:24:11.240]   designed for the white American who wants to do something but just doesn't know what to do.
[02:24:11.800 --> 02:24:19.400]   It's called Showing Up for Racial Justice Surge. And you can type that into the box. And they've got
[02:24:19.400 --> 02:24:26.760]   years in the game of just trying to help folks who want to be helpful but don't know where to start.
[02:24:26.760 --> 02:24:32.360]   So there aren't many excuses. And I think especially with this community,
[02:24:33.080 --> 02:24:42.200]   I know the resourcefulness. So you can get there. It's also a lot of work. And it's a journey.
[02:24:42.200 --> 02:24:47.400]   No one's perfect. I'm not perfect. I'm not a perfect human. I'm not a perfect black person.
[02:24:47.400 --> 02:24:53.240]   Not a perfect man. Not a perfect partner. I'm not aiming for that. It's trying to be better.
[02:24:53.240 --> 02:24:59.560]   And opening up raising that consciousness is going to take a lot of that type of work. There's
[02:24:59.560 --> 02:25:03.560]   people working on the policies, there's all kinds of reforms that we've been reforming. We've been
[02:25:03.560 --> 02:25:11.160]   reforming since we started breaking stuff. That's never ending. The consciousness shift is a lot
[02:25:11.160 --> 02:25:18.280]   of this softer, more intimate, more personal stuff. And it's going to require people where they are
[02:25:18.280 --> 02:25:26.680]   to ask, okay, what can I do? Where am I spending my money? What does my organization look like?
[02:25:27.400 --> 02:25:32.440]   Have we ever had a person of color in this position? Have we ever tried? Not that have we
[02:25:32.440 --> 02:25:36.600]   rejected anyone who happened to walk through the door? Have we intentionally sought them out?
[02:25:36.600 --> 02:25:42.360]   It's not why not? And it's not to beat yourself up and feel like a failure of a woke person.
[02:25:42.360 --> 02:25:47.960]   We're all failed woke people. It's a stupid word to begin with. We're all trying to awaken.
[02:25:47.960 --> 02:25:56.520]   That's the goal is just don't sleep walk. As we awaken, let's ask ourselves where we have control,
[02:25:57.400 --> 02:26:05.400]   where we have influence. What else can we be doing? And the lists are long and there's very
[02:26:05.400 --> 02:26:08.200]   small actions and there's very big gestures and a lot in between.
[02:26:08.200 --> 02:26:17.160]   Easiest thing to do, follow Baratunde on Instagram at Baratunde. His link tree is at
[02:26:17.160 --> 02:26:28.120]   lincrt.oe/baratunde. And I am on your text message chain. I love that. It's not a lot of
[02:26:28.120 --> 02:26:32.680]   messages, but when they come, I always welcome them. I feel like you're talking to me personally.
[02:26:32.680 --> 02:26:36.440]   I am talking to you personally. I only want on it.
[02:26:36.440 --> 02:26:41.800]   No, it's funny. So I kind of launched the number, the first number.
[02:26:41.800 --> 02:26:47.880]   I changed numbers on it. I've got a new number. It's 202-894844.
[02:26:47.880 --> 02:26:57.160]   I've been learning how to use it. So I've got just under a thousand people.
[02:26:57.160 --> 02:27:04.280]   But I don't intimately message with a thousand people. That's crazy. What I do though,
[02:27:04.280 --> 02:27:13.080]   it's I send out alerts to groups of people based on interest and location. I solicited input from
[02:27:13.080 --> 02:27:18.680]   folks. So when the, here's a real story. Last week, when things were popping off in Minnesota,
[02:27:18.680 --> 02:27:24.200]   I sent a message just to my textures who live in the Minnesota area. You kind of drew a circle
[02:27:24.200 --> 02:27:30.920]   around the city, a radius of a certain number of miles offering just my heartfelt best wishes.
[02:27:30.920 --> 02:27:35.320]   It's hard for me and I live in Los Angeles. I can only imagine when it's literally
[02:27:35.320 --> 02:27:40.040]   your hometown, almost your backyard, some cases your actual backyard. That must be tough.
[02:27:40.040 --> 02:27:45.560]   There's anything you want to share, let me know. And I ended up bringing one person,
[02:27:45.560 --> 02:27:49.240]   many wrote back and I ended up bringing him on my live show on Instagram that night.
[02:27:49.240 --> 02:27:55.000]   And he testified to what he had just been through. And it was way more powerful than me
[02:27:55.000 --> 02:28:02.040]   punting. People got to hear from someone on the front lines. I send voter registration deadlines.
[02:28:02.040 --> 02:28:07.560]   I ask and silly gifts on your birthday. It's not all serious politics stuff. I do my race
[02:28:07.560 --> 02:28:15.240]   thing. I do my tech thing. I do my fun thing all the time. So I'm learning how to connect with
[02:28:15.240 --> 02:28:21.960]   people at a certain scale and still be kind of intimate and personal in a way that's a bit
[02:28:21.960 --> 02:28:26.040]   different from a broadcast mailing list. It's early from a venue like Twitter,
[02:28:26.040 --> 02:28:30.600]   both of which have their value. It's really cool. And I have to confess when you
[02:28:30.600 --> 02:28:35.080]   change numbers, I forgot to follow you. So I just have followed you to the new number.
[02:28:35.080 --> 02:28:38.280]   Welcome back to the. I missed your text. Yes.
[02:28:38.280 --> 02:28:40.520]   Graded.
[02:28:40.520 --> 02:28:40.840]   It did.
[02:28:40.840 --> 02:28:47.240]   The great thing about having Baratunde in our life and in the Twit family is that when
[02:28:47.240 --> 02:28:55.480]   things like this happen, we can personalize it. The thing that keeps us apart is the sense of
[02:28:55.480 --> 02:29:01.640]   the other. And all humans do this. If you're not like me, I don't like you. And so it's really
[02:29:01.640 --> 02:29:07.400]   important, I think, for us to have people that aren't like us in our lives so that when things
[02:29:07.400 --> 02:29:15.480]   like this week happen, we understand it and we know it and we love the people that are involved
[02:29:15.480 --> 02:29:18.840]   and we care about them. And I care so much about you, Baratunde. I'm so glad.
[02:29:18.840 --> 02:29:25.800]   Thank you very much, Lee. I'll care for you as well. And I a little pro tip for the listeners,
[02:29:25.800 --> 02:29:31.800]   especially the white listeners who are feeling like you want to reach out to the black person
[02:29:31.800 --> 02:29:40.280]   or people in your lives. First, that is a beautiful instinct to express sympathy and how can I help
[02:29:40.280 --> 02:29:47.160]   and how are you, etc. Also be prepared for people to not want to answer. Just like anyone in a
[02:29:47.160 --> 02:29:51.000]   grieving state, if someone in your family is just that, you've been through a trauma, you're not
[02:29:51.000 --> 02:29:57.560]   necessarily going to move to process that for your friends. And so use that energy and that desire
[02:29:57.560 --> 02:30:02.680]   to help to find someone who's made themselves available to an author who's written a book.
[02:30:02.680 --> 02:30:06.920]   It doesn't take any emotional labor for you to read someone else's book. They've already done
[02:30:06.920 --> 02:30:12.280]   that work. It's been banked so that that scales emotionally in a different way
[02:30:12.280 --> 02:30:18.840]   and talking with your own children and your own spouses and co-workers and colleagues
[02:30:18.840 --> 02:30:24.680]   about what this is bringing up for you. I think it's really, there's a level of intellectual
[02:30:24.680 --> 02:30:30.040]   kind of facts and figures and research that is necessary. But I think more important is for all
[02:30:30.040 --> 02:30:38.600]   of us to get in touch with how we're feeling and to be real about that and to talk about that with
[02:30:38.600 --> 02:30:44.760]   people who we feel safe being vulnerable around and work through it on both levels. The emotional
[02:30:44.760 --> 02:30:51.080]   level and the sort of intellectual policy, like how do we fix this kind of level? Because it's only
[02:30:51.080 --> 02:30:57.080]   when those two come together that we can really move forward in a way that lasts and doesn't kind
[02:30:57.080 --> 02:31:02.520]   of feel good for a few weeks or a few months or even a few years or a whole presidency and then roll
[02:31:02.520 --> 02:31:13.640]   back. I feel better already. Go to Baratunde's world saving bookshop and pick up a book. Probably
[02:31:13.640 --> 02:31:18.840]   the first one you should start with was How to Be Black. I'm a big fan of Tana Hasey coats
[02:31:18.840 --> 02:31:25.640]   between the world and me. Tana, how do you pronounce it? Tana Hasey. Tana Hasey. This is the most
[02:31:25.640 --> 02:31:33.160]   teenage. I learned how to say his name in early 90s. Such a moving book, a National Book Award
[02:31:33.160 --> 02:31:42.120]   winner and it's required reading and anything that we can do to help, we should do to help. We owe
[02:31:42.120 --> 02:31:54.200]   it to our friends. I'm just glad you could be on with this. And Leo, if I might, real-time edit your
[02:31:54.200 --> 02:32:01.160]   statement there, we owe it to ourselves. And ourselves. I think that it's both. It's both.
[02:32:01.160 --> 02:32:07.320]   And I think, again, I'll always try to shift the position because I do a lot of preaching out and
[02:32:07.320 --> 02:32:14.920]   I'm like, am I practicing inside too? And so I don't want to end sexism for the ladies.
[02:32:14.920 --> 02:32:22.680]   If the effect is the same, the reasons don't matter as much, but I think the effect will be
[02:32:22.680 --> 02:32:31.400]   changed by the reasons that motivate us. And we've got to want this world to be just for all.
[02:32:31.400 --> 02:32:38.200]   We've got to want it for the lover that our child might be with. We've got to love it for the boss
[02:32:38.200 --> 02:32:44.360]   we might work for or the employee we might hire. We've got to want it for the health. I mean,
[02:32:44.360 --> 02:32:51.800]   the exhaustion that we're coming off of and we're still in is a pandemic. Public health. We're
[02:32:51.800 --> 02:32:59.320]   all connected. It turns out. And an injury to one is an injury to all in a really practical sense.
[02:32:59.320 --> 02:33:05.080]   It's in the air. We share this air. We share these resources. We share this economy.
[02:33:05.080 --> 02:33:13.080]   And we can share this love. So do it for Black people. I won't deny that yet. Yeah.
[02:33:13.080 --> 02:33:21.080]   Absolutely. Ready for you to jump in the game. But do it for all people too. Do it for humanity.
[02:33:21.800 --> 02:33:25.720]   Because when we come together on that, then you're like, OK, climate change. Let's do this.
[02:33:25.720 --> 02:33:33.560]   Now we got this now. We're on a we've got the Avengers lined up ready to go. It's infinity war.
[02:33:33.560 --> 02:33:39.800]   Let's throw down. We got Black Panther and a Norse God on the same play. Do this.
[02:33:39.800 --> 02:33:45.160]   That's one thing Veritundi's never shied away from the tough topics. It's so great to see you.
[02:33:45.160 --> 02:33:47.000]   Thank you, Frank. I'm glad you're doing well.
[02:33:47.880 --> 02:33:53.960]   Follow him on Instagram. And then in his bio, there's that link to his link tree and a lot of good
[02:33:53.960 --> 02:33:59.880]   resources. There are a lot of good things we can all do for one another and for ourselves.
[02:33:59.880 --> 02:34:05.320]   Thank you, Veritundi. God bless. And I hope we see you soon. Can I just say this out of you,
[02:34:05.320 --> 02:34:09.880]   whether you use it or not, I just it's been a long time. Another twin is in the game.
[02:34:09.880 --> 02:34:13.880]   Easy.
[02:34:13.880 --> 02:34:20.280]   Do the Twitter. All right. Do the Twitter, baby. Do the Twitter. All right. Do the Twitter.
[02:34:20.280 --> 02:34:31.420]   [BLANK_AUDIO]

