;FFMETADATA1
title=The Frightful 5 on the Splinternet
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=638
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.200]   It's time for Twit this week.
[00:00:02.200 --> 00:00:03.800]   In tech, we have a big show for you.
[00:00:03.800 --> 00:00:07.440]   Patrick Bejaud joins us from FrenchSpin.com, Amy Webb,
[00:00:07.440 --> 00:00:10.400]   futurist Michael Nunez from Mashable.
[00:00:10.400 --> 00:00:12.480]   We'll talk about ordering the iPhone X
[00:00:12.480 --> 00:00:15.120]   and what really I think the most interesting conversation
[00:00:15.120 --> 00:00:17.240]   throughout the thread throughout the whole conversation
[00:00:17.240 --> 00:00:19.320]   is what these new products from Apple
[00:00:19.320 --> 00:00:22.360]   and Amazon and Facebook and Google
[00:00:22.360 --> 00:00:24.720]   tell us about their real interests,
[00:00:24.720 --> 00:00:27.440]   their real data interests.
[00:00:27.440 --> 00:00:28.640]   It's next on Twit.
[00:00:29.360 --> 00:00:31.640]   (electronic music)
[00:00:31.640 --> 00:00:33.720]   NetCasts you love.
[00:00:33.720 --> 00:00:35.200]   From people you trust.
[00:00:35.200 --> 00:00:41.240]   This is Twit.
[00:00:41.240 --> 00:00:44.560]   Bandwidth for this week in tech is provided by CashFly
[00:00:44.560 --> 00:00:47.760]   at CACHEFLY.com.
[00:00:47.760 --> 00:00:52.960]   (upbeat music)
[00:00:52.960 --> 00:00:54.920]   This is Twit this week in tech,
[00:00:54.920 --> 00:00:59.920]   episode 638 recorded Sunday, October 29th, 2017,
[00:00:59.920 --> 00:01:03.480]   the frightful five on the spinger nets.
[00:01:03.480 --> 00:01:06.240]   This week in tech is brought to you by Scott Evest,
[00:01:06.240 --> 00:01:08.360]   a full line of functional clothing
[00:01:08.360 --> 00:01:10.200]   that features well-designed pockets
[00:01:10.200 --> 00:01:12.480]   so you can carry all of life's gadgets
[00:01:12.480 --> 00:01:13.880]   without carrying a bag.
[00:01:13.880 --> 00:01:16.800]   For a limited time, go to scottevest.com/twit
[00:01:16.800 --> 00:01:20.040]   and get an extra 25% off your order.
[00:01:20.040 --> 00:01:23.000]   And by JamfNow, Apple Management Software
[00:01:23.000 --> 00:01:25.480]   for Mac, iPad and iPhone devices,
[00:01:25.480 --> 00:01:30.480]   visit Jamfjamf.com/twit to create your free JamfNow account
[00:01:30.480 --> 00:01:34.520]   and manage your first three devices, absolutely free.
[00:01:34.520 --> 00:01:37.400]   And by Carbonite, keep your business safe this year,
[00:01:37.400 --> 00:01:39.920]   protect your business from ransomware and hacker attacks
[00:01:39.920 --> 00:01:43.400]   with automatic data protection solutions from Carbonite.
[00:01:43.400 --> 00:01:46.080]   Try it free at Carbonite.com, use the offer code TWIT
[00:01:46.080 --> 00:01:49.000]   to get two free bonus months if you decide to buy.
[00:01:49.000 --> 00:01:52.440]   And by Blue Apron, now you can expand your wine palette
[00:01:52.440 --> 00:01:55.160]   with Blue Apron's wine boxes.
[00:01:55.160 --> 00:01:56.880]   They're not a box of wine, they're bottles of wine
[00:01:56.880 --> 00:01:57.720]   that come in a box.
[00:01:57.720 --> 00:01:59.840]   They're $25 off your first box
[00:01:59.840 --> 00:02:02.880]   by going to Blue Apron.com/twitwine.
[00:02:02.880 --> 00:02:06.720]   It's time for Twit this week in tech,
[00:02:06.720 --> 00:02:09.640]   the show where we cover the week's tech news.
[00:02:09.640 --> 00:02:11.680]   We're gonna have a lot of fun this week joining us
[00:02:11.680 --> 00:02:14.000]   from Mashable Michael Nunez is here.
[00:02:14.000 --> 00:02:16.040]   It's great to see you, Michael.
[00:02:16.040 --> 00:02:17.080]   - Great to see you as well.
[00:02:17.080 --> 00:02:18.840]   - I'm guessing by the windows in the backdrop,
[00:02:18.840 --> 00:02:22.400]   you are somewhere in the northeast, perhaps New York City.
[00:02:22.400 --> 00:02:23.840]   - Ah, nailed it, yeah.
[00:02:23.840 --> 00:02:26.360]   You've caught me in my apartment in Brooklyn.
[00:02:26.360 --> 00:02:28.240]   - Brooklyn, in fact, I was gonna say Brooklyn,
[00:02:28.240 --> 00:02:29.760]   I should've narrowed it down.
[00:02:29.760 --> 00:02:32.360]   - Yeah, yeah, well, you know, I mean, same difference.
[00:02:32.360 --> 00:02:33.760]   But yeah, I'm glad to be here.
[00:02:33.760 --> 00:02:36.280]   - Those are definitely Brooklyn bricks, I could tell.
[00:02:36.280 --> 00:02:37.120]   I could tell.
[00:02:37.120 --> 00:02:39.040]   Actually, you know, you're in the east coast
[00:02:39.040 --> 00:02:40.280]   'cause this doesn't happen in California
[00:02:40.280 --> 00:02:42.560]   when you have those old fashioned sash windows,
[00:02:42.560 --> 00:02:43.800]   you know, with the counterweights,
[00:02:43.800 --> 00:02:45.840]   and there's lots of layers of paint.
[00:02:45.840 --> 00:02:50.040]   Yeah, that's, to me, that's the east coast.
[00:02:50.040 --> 00:02:50.880]   I grew up in Rhode Island.
[00:02:50.880 --> 00:02:52.640]   I know that look, it's great.
[00:02:52.640 --> 00:02:54.000]   Roger, wonderful to have you, Michael.
[00:02:54.000 --> 00:02:57.120]   Also with us, Amy Webb, she is a futurist.
[00:02:57.120 --> 00:02:59.680]   She is at the Future Today Institute and wrote a book.
[00:02:59.680 --> 00:03:02.960]   In fact, we met when I interviewed her about the signals
[00:03:02.960 --> 00:03:05.400]   are talking on triangulation,
[00:03:05.400 --> 00:03:08.360]   and we just love having Amy on, very smart,
[00:03:08.360 --> 00:03:11.600]   but also with a particular angle.
[00:03:11.600 --> 00:03:13.880]   First of all, she knows Japan very well,
[00:03:13.880 --> 00:03:16.200]   and uses, and I think uses her experiences there
[00:03:16.200 --> 00:03:18.040]   and elsewhere to kind of think about
[00:03:18.040 --> 00:03:19.360]   what's going on in the future.
[00:03:19.360 --> 00:03:20.360]   And that's certainly something we like
[00:03:20.360 --> 00:03:22.000]   to talk about. Welcome, Amy.
[00:03:22.000 --> 00:03:23.000]   Thanks. Thanks for having me back.
[00:03:23.000 --> 00:03:23.840]   Nice to have you.
[00:03:23.840 --> 00:03:26.240]   And our Frenchman, our favorite Frenchman
[00:03:26.240 --> 00:03:27.720]   who haven't had an on and so long
[00:03:27.720 --> 00:03:30.120]   that he's now at a new job in a new country.
[00:03:30.120 --> 00:03:31.960]   Mike Patrick Bezia is here,
[00:03:31.960 --> 00:03:33.080]   frenchspin.com.
[00:03:33.080 --> 00:03:35.320]   He does the Phileas Club is English language podcast,
[00:03:35.320 --> 00:03:38.720]   and Le Honde voutech, his French language podcast.
[00:03:38.720 --> 00:03:40.320]   He's a full-time podcaster,
[00:03:40.320 --> 00:03:41.760]   and he's now in Finland.
[00:03:41.760 --> 00:03:44.480]   He's followed his Swedish wife there.
[00:03:44.480 --> 00:03:46.880]   Yeah, well, my Finnish sweet,
[00:03:46.880 --> 00:03:48.360]   sweetophone wife.
[00:03:48.360 --> 00:03:49.520]   Yeah, indeed.
[00:03:49.520 --> 00:03:51.600]   I'm married a Swedish, a Finnish sweetophone.
[00:03:51.600 --> 00:03:52.800]   I like that. That's good.
[00:03:52.800 --> 00:03:54.920]   (both laugh)
[00:03:54.920 --> 00:03:57.360]   So, and we're expecting a baby now.
[00:03:57.360 --> 00:03:58.680]   So congratulations.
[00:03:58.680 --> 00:03:59.520]   Oh, congratulations.
[00:03:59.520 --> 00:04:03.520]   Will that baby speak is a complete mystery even to us?
[00:04:03.520 --> 00:04:06.120]   'Cause your native tongue, of course, is French.
[00:04:06.120 --> 00:04:07.920]   Her native tongue is Swedish,
[00:04:07.920 --> 00:04:09.800]   but you live in Finland where the native tongue
[00:04:09.800 --> 00:04:12.080]   is obviously Finnish.
[00:04:12.080 --> 00:04:15.120]   I think clearly all three.
[00:04:15.120 --> 00:04:16.960]   I think that's the plan,
[00:04:16.960 --> 00:04:20.080]   but he will not learn Finnish at home.
[00:04:20.080 --> 00:04:22.480]   He will have to learn that in school.
[00:04:22.480 --> 00:04:25.640]   His friends, 'cause his friends mostly speak Finnish.
[00:04:25.640 --> 00:04:28.280]   Well, it depends.
[00:04:28.280 --> 00:04:30.400]   There are many Swedish speakers
[00:04:30.400 --> 00:04:32.520]   in certain parts of the country,
[00:04:32.520 --> 00:04:34.320]   and so if you're in those parts,
[00:04:34.320 --> 00:04:36.600]   probably the friends are gonna be speaking
[00:04:36.600 --> 00:04:38.440]   a lot of Swedish as well.
[00:04:38.440 --> 00:04:39.280]   Yeah.
[00:04:39.280 --> 00:04:40.960]   Okay, this is a pretty eclectic bunch,
[00:04:40.960 --> 00:04:43.000]   and I'm not sure how this is gonna go,
[00:04:43.000 --> 00:04:46.600]   but how many of you were up early Friday morning
[00:04:47.440 --> 00:04:50.680]   would have been 3 a.m. for Amy and Michael,
[00:04:50.680 --> 00:04:52.400]   and I don't know what the hell time would have been
[00:04:52.400 --> 00:04:55.160]   for you, Patrick, ordering an iPhone X.
[00:04:55.160 --> 00:04:57.560]   Patrick, you were ordering an iPhone X.
[00:04:57.560 --> 00:04:58.800]   Yeah, it was 10 a.m.
[00:04:58.800 --> 00:04:59.640]   It was perfect.
[00:04:59.640 --> 00:05:00.880]   Oh, forget it.
[00:05:00.880 --> 00:05:01.920]   There was so much.
[00:05:01.920 --> 00:05:02.760]   Geez.
[00:05:02.760 --> 00:05:05.400]   It's midnight here now when we're doing the show,
[00:05:05.400 --> 00:05:08.360]   but to order an iPhone X, it was great,
[00:05:08.360 --> 00:05:10.920]   except, you know, I was,
[00:05:10.920 --> 00:05:13.120]   I don't really need the phone, so I wasn't crushed,
[00:05:13.120 --> 00:05:18.120]   but I waited until, I had to wait until like 10, eight,
[00:05:18.120 --> 00:05:24.200]   so eight minutes for it to finally refresh for me,
[00:05:24.200 --> 00:05:26.840]   and of course by then it was like five weeks
[00:05:26.840 --> 00:05:29.360]   or six weeks of wait, so.
[00:05:29.360 --> 00:05:31.760]   I think that's, and Amy, you don't feel the need
[00:05:31.760 --> 00:05:34.240]   to get an iPhone X or a...
[00:05:34.240 --> 00:05:37.880]   I don't, I have long since given up on iOS.
[00:05:37.880 --> 00:05:41.640]   I am a happy Samsung Galaxy 8 user, yeah.
[00:05:41.640 --> 00:05:42.600]   Yeah, I love the Note 8.
[00:05:42.600 --> 00:05:44.480]   I just, I have the Note 8, which I really like.
[00:05:44.480 --> 00:05:46.280]   And I've ordered the Pixel XL, which we'll talk about
[00:05:46.280 --> 00:05:49.560]   because what's interesting is with all of these newest phones,
[00:05:49.560 --> 00:05:51.960]   there are some challenges as these phone makers
[00:05:51.960 --> 00:05:53.600]   try to push the envelope,
[00:05:53.600 --> 00:05:56.880]   most notably Samsung last year with the exploding Note 7,
[00:05:56.880 --> 00:05:58.360]   'cause they tried to put more battery in it
[00:05:58.360 --> 00:06:00.520]   and it didn't work out so well.
[00:06:00.520 --> 00:06:02.360]   But now, Google's having some trouble
[00:06:02.360 --> 00:06:04.040]   with the Pixel, we'll talk about that in a little bit.
[00:06:04.040 --> 00:06:06.440]   Michael, did you, do you, do you care?
[00:06:06.440 --> 00:06:10.360]   You know, I was lucky enough to have some coworkers
[00:06:10.360 --> 00:06:12.640]   that were willing to get up at that hour.
[00:06:12.640 --> 00:06:15.360]   So I, myself, didn't set my alarm.
[00:06:15.360 --> 00:06:18.480]   I slept all night and we'll just kind of depend
[00:06:18.480 --> 00:06:21.240]   on some friends to check this device out.
[00:06:21.240 --> 00:06:24.320]   Yeah, yeah, one of my coworkers, Megan Moroney,
[00:06:24.320 --> 00:06:26.560]   got hers, we'll be getting hers Friday,
[00:06:26.560 --> 00:06:29.000]   which is, of course, the official release date.
[00:06:29.000 --> 00:06:30.960]   Mine is two weeks after that.
[00:06:30.960 --> 00:06:34.760]   My wife, so I got up.
[00:06:34.760 --> 00:06:36.120]   I'm sitting in my home office.
[00:06:36.120 --> 00:06:38.320]   I've got a computer screen on the Apple Store,
[00:06:38.320 --> 00:06:40.000]   computer screen on the T-Mobile Store.
[00:06:40.000 --> 00:06:44.280]   I've got an iPod iPad on the iOS Store
[00:06:44.280 --> 00:06:46.000]   and an iPhone on the iOS Store.
[00:06:46.000 --> 00:06:48.480]   I'm refreshing like a monkey.
[00:06:48.480 --> 00:06:49.760]   I'm refreshing, refreshing.
[00:06:49.760 --> 00:06:53.080]   It's midnight, it's 1201, it's 1202, nothing, nothing.
[00:06:53.080 --> 00:06:56.000]   T-Mobile's still selling the upgrade to the iPhone 8,
[00:06:56.000 --> 00:06:58.880]   not to the iPhone 10, refreshing, refreshing.
[00:06:58.880 --> 00:07:00.320]   And when I finally got through,
[00:07:00.320 --> 00:07:01.400]   it wasn't on the Apple Store,
[00:07:01.400 --> 00:07:03.240]   which is the easiest way, usually, to get through.
[00:07:03.240 --> 00:07:04.880]   I got through on the Apple website
[00:07:04.880 --> 00:07:09.340]   and, you know, so I got two to three weeks delay.
[00:07:10.300 --> 00:07:11.780]   My wife stayed in bed.
[00:07:11.780 --> 00:07:14.980]   Her alarm went off at midnight.
[00:07:14.980 --> 00:07:17.220]   She sleepily got her iPhone,
[00:07:17.220 --> 00:07:19.100]   went right in the Apple Store and ordered it
[00:07:19.100 --> 00:07:21.300]   and went back to sleep.
[00:07:21.300 --> 00:07:22.660]   I don't know what I did wrong.
[00:07:22.660 --> 00:07:25.260]   I tried to, oh, I tried hard.
[00:07:25.260 --> 00:07:26.820]   Yeah, I felt the same way.
[00:07:26.820 --> 00:07:29.860]   I was, you know, looking at the great Steve in the sky
[00:07:29.860 --> 00:07:32.380]   and I was thinking, tell me, what did I do wrong?
[00:07:32.380 --> 00:07:35.260]   Eight minutes, that's a natural.
[00:07:35.260 --> 00:07:36.660]   Isn't this-- - But we're very frustrating.
[00:07:36.660 --> 00:07:40.380]   I just find it's brilliant marketing.
[00:07:40.380 --> 00:07:43.180]   There is some debate and it goes back and forth
[00:07:43.180 --> 00:07:45.660]   over whether these shortages,
[00:07:45.660 --> 00:07:47.820]   which has seemed to happen every year,
[00:07:47.820 --> 00:07:49.820]   are contrived for marketing purposes.
[00:07:49.820 --> 00:07:52.140]   I don't, I feel like every company,
[00:07:52.140 --> 00:07:55.060]   Nintendo with the Switch, will make as many
[00:07:55.060 --> 00:07:58.060]   as they possibly can because every phone
[00:07:58.060 --> 00:07:59.860]   you don't sell day one is,
[00:07:59.860 --> 00:08:02.180]   potentially a phone you'll never sell, right?
[00:08:02.180 --> 00:08:03.020]   - No, I agree.
[00:08:03.020 --> 00:08:06.460]   I think it's a common myth.
[00:08:06.460 --> 00:08:09.580]   Unless you're in luxury, like real luxury, you know,
[00:08:09.580 --> 00:08:11.140]   but-- - There's nothing more luxurious
[00:08:11.140 --> 00:08:14.020]   than that. - $1,149 iPhone X.
[00:08:14.020 --> 00:08:16.340]   - Well, that's surprisingly, you know,
[00:08:16.340 --> 00:08:21.340]   there are things like bags that cost three, four times that.
[00:08:21.340 --> 00:08:26.180]   In those cases, you know, the Weevie Tone bag
[00:08:26.180 --> 00:08:27.020]   or something like that.
[00:08:27.020 --> 00:08:30.460]   I think in those cases, scarcity is manufactured.
[00:08:30.460 --> 00:08:32.460]   I don't think it's the case for the tech products
[00:08:32.460 --> 00:08:33.780]   that we enjoy.
[00:08:35.060 --> 00:08:38.260]   And I think that those are completely, you know,
[00:08:38.260 --> 00:08:39.580]   they're myths.
[00:08:39.580 --> 00:08:42.060]   And it especially happens as with the Switch
[00:08:42.060 --> 00:08:44.940]   or the 10 or with these new Pixel 2s
[00:08:44.940 --> 00:08:46.460]   when you've got a new product,
[00:08:46.460 --> 00:08:49.260]   not a refresh of the old product like the iPhone 8,
[00:08:49.260 --> 00:08:51.460]   but a totally new product.
[00:08:51.460 --> 00:08:53.660]   Yeah, and it's not easy to evaluate.
[00:08:53.660 --> 00:08:56.220]   Sorry, go ahead, Nimi.
[00:08:56.220 --> 00:08:57.060]   - Oh, I was just gonna say,
[00:08:57.060 --> 00:08:59.660]   I don't know that that's entirely true.
[00:08:59.660 --> 00:09:02.620]   Because it's not just getting the hardware out the door.
[00:09:02.620 --> 00:09:06.420]   It's, you know, the entire process
[00:09:06.420 --> 00:09:08.580]   and the whole funnel has to scale, right?
[00:09:08.580 --> 00:09:11.580]   So given the problems that we've seen
[00:09:11.580 --> 00:09:16.220]   with some recent products across everybody's different brands,
[00:09:16.220 --> 00:09:17.820]   you know, maybe people are taking a little bit more
[00:09:17.820 --> 00:09:19.500]   of a measured approach.
[00:09:19.500 --> 00:09:23.300]   I'm a cynic and, you know, part of me thinks that
[00:09:23.300 --> 00:09:26.300]   holding off and manufacturing scarcity
[00:09:26.300 --> 00:09:29.140]   in order to increase everybody's thirst and desire
[00:09:29.140 --> 00:09:32.460]   for these products, which we have less and less attention
[00:09:32.460 --> 00:09:34.020]   for because there's just more competition
[00:09:34.020 --> 00:09:37.300]   in the marketplace seems like a viable reason
[00:09:37.300 --> 00:09:39.820]   to make it so difficult to purchase a phone.
[00:09:39.820 --> 00:09:41.740]   But on the other hand, look at what happened
[00:09:41.740 --> 00:09:45.580]   with some of the most recent launches.
[00:09:45.580 --> 00:09:47.900]   Samsung's previous phone had problems.
[00:09:47.900 --> 00:09:51.860]   And, you know, if you've suddenly got all of these people
[00:09:51.860 --> 00:09:54.580]   using a new phone with face recognition
[00:09:54.580 --> 00:09:58.700]   that may or may not work exactly as promised, you know,
[00:09:58.700 --> 00:10:01.580]   and the Pixel 2 having potentially some screen issues,
[00:10:01.580 --> 00:10:03.380]   I mean, there may be reasons to hold out
[00:10:03.380 --> 00:10:06.140]   on the number of products that get released at one time.
[00:10:06.140 --> 00:10:10.500]   - I don't think if Apple makes, you know,
[00:10:10.500 --> 00:10:12.340]   three million phones and some of them,
[00:10:12.340 --> 00:10:14.780]   and they end up not working as advertised,
[00:10:14.780 --> 00:10:17.620]   it's not gonna be a bigger issue than,
[00:10:17.620 --> 00:10:20.060]   a smaller issue than if they make 15.
[00:10:20.060 --> 00:10:21.980]   The difference is gonna be they're gonna have, you know,
[00:10:21.980 --> 00:10:26.820]   X million less in the bank because they didn't sell as many.
[00:10:26.820 --> 00:10:30.500]   I mean, for the switch, I can guarantee you,
[00:10:30.500 --> 00:10:33.140]   it's also kind of scarce, but that is because
[00:10:33.140 --> 00:10:36.540]   of the success of the console, which is somewhat
[00:10:36.540 --> 00:10:38.500]   unexpected. - Unexpected, yeah.
[00:10:38.500 --> 00:10:41.100]   - Yeah, but everybody knows how many iPhones Apple,
[00:10:41.100 --> 00:10:44.340]   I mean, Apple would make 20 million iPhones if it could.
[00:10:44.340 --> 00:10:48.140]   - Yeah, I think whether it's manufactured or not,
[00:10:48.140 --> 00:10:50.460]   it's definitely made me more interested in these products.
[00:10:50.460 --> 00:10:53.860]   So like, you know, the SNES classic is a prime example.
[00:10:53.860 --> 00:10:56.900]   It's a device that I really don't want.
[00:10:56.900 --> 00:10:59.580]   There are better emulators out there.
[00:10:59.580 --> 00:11:01.780]   There are better versions or better ways
[00:11:01.780 --> 00:11:04.420]   to play Super Nintendo games.
[00:11:04.420 --> 00:11:06.500]   As a collector's item, I guess it's kind of neat,
[00:11:06.500 --> 00:11:07.980]   but for the most part, it's like a product
[00:11:07.980 --> 00:11:08.940]   that I would never want.
[00:11:08.940 --> 00:11:11.500]   The scarcity has made me so intrigued by that.
[00:11:11.500 --> 00:11:14.300]   It makes me really wanna play the, you know,
[00:11:14.300 --> 00:11:18.140]   the Lost Star Fox game that's featured on there.
[00:11:18.140 --> 00:11:21.140]   And then also with the iPhone X, you know, again,
[00:11:21.140 --> 00:11:23.660]   it's something that I don't need at all.
[00:11:23.660 --> 00:11:27.220]   You know, I already own an iPhone 7, it works just fine.
[00:11:27.220 --> 00:11:29.260]   But the fact that people are clamoring
[00:11:29.260 --> 00:11:31.300]   for this device, that there's so much intrigue
[00:11:31.300 --> 00:11:35.500]   around the $1,000 phone and that it's already sold out
[00:11:35.500 --> 00:11:38.900]   and, you know, and that delays might spill into next year.
[00:11:38.900 --> 00:11:42.820]   I think that that makes me really interested in,
[00:11:42.820 --> 00:11:45.700]   in playing with the device or, you know,
[00:11:45.700 --> 00:11:46.540]   potentially owning it.
[00:11:46.540 --> 00:11:47.660]   - Well, but you're also in the press.
[00:11:47.660 --> 00:11:50.540]   So you measure that interest as well as,
[00:11:50.540 --> 00:11:54.060]   and assume that that's gonna carry over to Mashable, right?
[00:11:54.060 --> 00:11:56.220]   So you, I mean, I certainly consider this.
[00:11:56.220 --> 00:11:58.460]   If something is, you know, for instance,
[00:11:58.460 --> 00:12:01.660]   I love the essential phone, Andy Rubin's little phone.
[00:12:01.660 --> 00:12:03.140]   And now it's $500.
[00:12:03.140 --> 00:12:05.020]   In fact, if you can get somebody with a friend's and family,
[00:12:05.020 --> 00:12:06.980]   it's $2.99.
[00:12:06.980 --> 00:12:08.220]   If you know somebody was stupid enough,
[00:12:08.220 --> 00:12:11.740]   like me to buy it at full price, you can get it to $2.99.
[00:12:11.740 --> 00:12:15.940]   It's a great phone, but they sold $5,000.
[00:12:15.940 --> 00:12:17.940]   So why would we even cover it?
[00:12:17.940 --> 00:12:19.260]   There's nobody wants it.
[00:12:19.260 --> 00:12:21.340]   I mean, I mean, it's always wise.
[00:12:21.340 --> 00:12:22.860]   - They have some issues to work out.
[00:12:22.860 --> 00:12:24.140]   You know, it's not a perfect phone.
[00:12:24.140 --> 00:12:26.300]   There are some camera issues, I think.
[00:12:26.300 --> 00:12:28.460]   - And even-- - No, no, no, no,
[00:12:28.460 --> 00:12:29.340]   that's a funny thing.
[00:12:29.340 --> 00:12:30.820]   They fixed the camera.
[00:12:30.820 --> 00:12:33.220]   They pushed out three updates.
[00:12:33.220 --> 00:12:36.620]   It is, there is in no respect, is it an inadequate phone?
[00:12:36.620 --> 00:12:40.460]   It's a, for $500 with a Snapdragon 835,
[00:12:40.460 --> 00:12:43.780]   a full bleed screen in a tiny form factor,
[00:12:43.780 --> 00:12:45.540]   titanium and ceramic.
[00:12:45.540 --> 00:12:49.900]   It is in many respects a flagship phone for $500,
[00:12:49.900 --> 00:12:53.220]   but it won't get any coverage because everybody's already
[00:12:53.220 --> 00:12:54.500]   decided.
[00:12:54.500 --> 00:12:55.460]   - Yeah.
[00:12:55.460 --> 00:12:57.980]   Well, it's essentially between just a couple of brands,
[00:12:57.980 --> 00:13:00.420]   but I think that essential also has the problem
[00:13:00.420 --> 00:13:03.140]   with only being available on Sprint, at least
[00:13:03.140 --> 00:13:05.100]   in terms of the carriers that sell it.
[00:13:05.100 --> 00:13:06.580]   You might be able to buy it.
[00:13:06.580 --> 00:13:07.660]   - I bought it unlocked.
[00:13:07.660 --> 00:13:08.820]   Yeah, I bought it unlocked.
[00:13:08.820 --> 00:13:11.340]   But you know, you're right, there's other issues as well.
[00:13:11.340 --> 00:13:14.780]   My point being, editorially, you're not likely to
[00:13:14.780 --> 00:13:19.300]   spend any ink if you used ink covering it
[00:13:19.300 --> 00:13:22.900]   because, well, nobody cares anymore, it's over.
[00:13:22.900 --> 00:13:26.140]   - Yeah, it's just seems that, oh, go ahead.
[00:13:26.140 --> 00:13:28.340]   - Oh, I was gonna say, just, so, so,
[00:13:28.340 --> 00:13:32.260]   why is everybody clamoring for this phone?
[00:13:32.260 --> 00:13:33.780]   - The iPhone 10?
[00:13:33.780 --> 00:13:35.940]   - Yeah, I mean, what's the real,
[00:13:35.940 --> 00:13:38.260]   why are all three of you excited about it?
[00:13:38.260 --> 00:13:40.180]   (laughing)
[00:13:40.180 --> 00:13:41.500]   - Because I was just, I was just,
[00:13:41.500 --> 00:13:44.300]   - I don't think that's a really interesting question.
[00:13:44.300 --> 00:13:46.620]   How else do we propel the consumer economy?
[00:13:46.620 --> 00:13:47.460]   That's why.
[00:13:47.460 --> 00:13:48.860]   (laughing)
[00:13:48.860 --> 00:13:52.860]   - Okay, so, that's, that's the question specifically,
[00:13:52.860 --> 00:13:55.180]   who is actually excited about the iPhone 10?
[00:13:55.180 --> 00:13:57.620]   I'm not, I'm not super excited, I'm gonna get it,
[00:13:57.620 --> 00:13:59.820]   but it's not like, you know, I--
[00:13:59.820 --> 00:14:01.220]   - Is it an obligation?
[00:14:01.220 --> 00:14:04.100]   Like, what was the reason that you purchased it then?
[00:14:04.100 --> 00:14:08.500]   - I have a success currently, and I figured,
[00:14:08.500 --> 00:14:09.380]   you know, it's for work.
[00:14:09.380 --> 00:14:10.860]   - Did you buy an eight new features?
[00:14:10.860 --> 00:14:12.860]   And no, I didn't, I didn't.
[00:14:12.860 --> 00:14:15.100]   So, so, by the way, that's what looks like,
[00:14:15.100 --> 00:14:17.180]   and then this is, you know, nobody Apple's not speaking,
[00:14:17.180 --> 00:14:19.860]   but that's what it looks like anecdotally happened,
[00:14:19.860 --> 00:14:22.620]   which is that a lot Apple put out the eight,
[00:14:22.620 --> 00:14:24.380]   and then a month later, the 10,
[00:14:24.380 --> 00:14:26.540]   announcing them at the same time.
[00:14:26.540 --> 00:14:28.980]   And as you, one almost would predict,
[00:14:28.980 --> 00:14:31.540]   the eight didn't really sell that well.
[00:14:31.540 --> 00:14:33.580]   People waited for the 10.
[00:14:33.580 --> 00:14:36.260]   And to answer your question, I won't,
[00:14:36.260 --> 00:14:39.660]   I like Patrick, because we're tech journalists,
[00:14:39.660 --> 00:14:41.860]   we have to do this, but,
[00:14:41.860 --> 00:14:43.340]   so I'm not sure I would squat,
[00:14:43.340 --> 00:14:44.580]   characterize myself as excited,
[00:14:44.580 --> 00:14:46.060]   but the world is excited.
[00:14:46.060 --> 00:14:50.460]   And they're excited, 'cause it's the latest iPhone.
[00:14:50.460 --> 00:14:52.580]   It's the top of the line iPhone.
[00:14:52.580 --> 00:14:53.420]   And for the first time,
[00:14:53.420 --> 00:14:55.260]   Apple's done some interesting things,
[00:14:55.260 --> 00:14:58.260]   like get rid of the fingerprint reader,
[00:14:58.260 --> 00:15:01.580]   like a full bleed screen that is OLED,
[00:15:01.580 --> 00:15:04.220]   which Apple's never done before.
[00:15:04.220 --> 00:15:06.660]   So there are some actually subject,
[00:15:06.660 --> 00:15:10.660]   objectively unusual and new features in it.
[00:15:10.660 --> 00:15:12.180]   I wasn't excited about the iPhone 8,
[00:15:12.180 --> 00:15:13.300]   which is as far as I could tell,
[00:15:13.300 --> 00:15:14.860]   a duplicate of the seven with a few,
[00:15:14.860 --> 00:15:16.780]   with a faster processor and a,
[00:15:16.780 --> 00:15:19.380]   you know, a few bells and whistles.
[00:15:19.380 --> 00:15:21.820]   I'm just wondering if we've habituated ourselves to,
[00:15:21.820 --> 00:15:23.060]   - Yes.
[00:15:23.060 --> 00:15:25.060]   - Like in a Pavlovian way, almost, right?
[00:15:25.060 --> 00:15:26.500]   Like iPhone 5. - Dude, who iPhones you?
[00:15:26.500 --> 00:15:28.020]   - The announcement, right.
[00:15:28.020 --> 00:15:31.220]   Because on the face of it,
[00:15:31.220 --> 00:15:34.260]   the features that are part of this, of the 10,
[00:15:34.260 --> 00:15:39.220]   aren't really that groundbreaking aside for maybe
[00:15:39.220 --> 00:15:40.500]   the facial recognition.
[00:15:40.500 --> 00:15:44.420]   And I'm kind of surprised that everybody is so excited
[00:15:44.420 --> 00:15:47.020]   about facial recognition without stopping to ask questions
[00:15:47.020 --> 00:15:49.500]   like, okay, well, who owns your face, right?
[00:15:49.500 --> 00:15:52.540]   Like does Apple own your face from here on out?
[00:15:52.540 --> 00:15:53.980]   You know, how does your hat,
[00:15:53.980 --> 00:15:55.900]   where does the data go?
[00:15:55.900 --> 00:15:57.380]   How is it being used?
[00:15:57.380 --> 00:16:01.460]   I'm having a hard time reconciling,
[00:16:01.460 --> 00:16:04.220]   unless it's just like, it's like out of habit now,
[00:16:04.220 --> 00:16:07.620]   we're just excited because this is the latest announcement.
[00:16:07.620 --> 00:16:10.540]   - Yeah, I think that's where the entry comes from, right?
[00:16:10.540 --> 00:16:12.100]   It's like, who, you know,
[00:16:12.100 --> 00:16:15.340]   there are a lot of pieces of technology in the iPhone 10
[00:16:15.340 --> 00:16:16.620]   that no one asked for,
[00:16:16.620 --> 00:16:17.740]   what they're being included.
[00:16:17.740 --> 00:16:22.460]   And it's just, it's about whether this is a fundamental
[00:16:22.460 --> 00:16:25.180]   change to the way that we use our phones,
[00:16:25.180 --> 00:16:28.340]   to the way that we pay for things on our phones.
[00:16:28.340 --> 00:16:31.300]   You know, I think that that's where the interest
[00:16:31.300 --> 00:16:32.500]   and the intrigue lies.
[00:16:32.500 --> 00:16:34.660]   It's not necessarily will this phone sell
[00:16:34.660 --> 00:16:36.700]   as well as the last one.
[00:16:36.700 --> 00:16:39.460]   I think it's, if you look at like,
[00:16:39.460 --> 00:16:42.420]   just even Google search terms over the past 10 years,
[00:16:42.420 --> 00:16:45.460]   you know, the iPhone continues to get more and more popular,
[00:16:45.460 --> 00:16:47.420]   even just based on search.
[00:16:47.420 --> 00:16:49.580]   And I think it's, you know,
[00:16:49.580 --> 00:16:52.460]   it's probably one of the most easily identifiable products
[00:16:52.460 --> 00:16:53.500]   in the world.
[00:16:53.500 --> 00:16:55.300]   And so, yes, the journalist,
[00:16:55.300 --> 00:16:58.260]   it's such an obvious story of intrigue,
[00:16:58.260 --> 00:17:01.820]   because not only is it something that makes a lot of money,
[00:17:01.820 --> 00:17:04.900]   it's something that is radically different
[00:17:04.900 --> 00:17:07.060]   for the first time in many years.
[00:17:07.060 --> 00:17:08.700]   And-- - You and I have a different
[00:17:08.700 --> 00:17:09.980]   definition of radical.
[00:17:09.980 --> 00:17:10.820]   (laughing)
[00:17:10.820 --> 00:17:13.500]   - Yeah, I would disagree with that character.
[00:17:13.500 --> 00:17:14.940]   - No, no, I wanna defend Michael.
[00:17:14.940 --> 00:17:15.780]   - Interesting, interesting.
[00:17:15.780 --> 00:17:18.580]   - It is, so that's kind of what I was saying
[00:17:18.580 --> 00:17:20.900]   at the very beginning, which is we've reached peak phone.
[00:17:20.900 --> 00:17:23.500]   We clearly reached peak phone a couple of years ago.
[00:17:23.500 --> 00:17:28.020]   So now, radically different is things like, you know,
[00:17:28.020 --> 00:17:29.020]   OLED screen.
[00:17:29.020 --> 00:17:32.060]   Or wireless charging, which for Apple
[00:17:32.060 --> 00:17:33.660]   is radically different. - No, no, no.
[00:17:33.660 --> 00:17:34.820]   - No, home button. - No home button.
[00:17:34.820 --> 00:17:35.700]   - It's a visual recognition.
[00:17:35.700 --> 00:17:37.340]   I mean, those are two-- - But when you,
[00:17:37.340 --> 00:17:42.340]   that's a measure of how kind of mature this,
[00:17:42.780 --> 00:17:43.820]   I think, isn't it?
[00:17:43.820 --> 00:17:46.420]   Michael, I measure how mature the technology is,
[00:17:46.420 --> 00:17:49.380]   that these differences now become radical differences.
[00:17:49.380 --> 00:17:51.660]   I mean, the iPhone 7 was radically different
[00:17:51.660 --> 00:17:55.380]   than the BlackBerry it succeeded in my pocket.
[00:17:55.380 --> 00:17:58.460]   - Yeah, I don't, you know, this isn't a technological,
[00:17:58.460 --> 00:18:01.460]   it's not a qualification of like the technological breakthrough.
[00:18:01.460 --> 00:18:04.460]   It's just, this phone will operate much differently
[00:18:04.460 --> 00:18:06.300]   than the phone that you currently use.
[00:18:06.300 --> 00:18:09.380]   So every time you open, every time you look at your phone,
[00:18:09.380 --> 00:18:11.660]   right now you're using a thumbprint scanner,
[00:18:11.660 --> 00:18:14.660]   most likely to unlock your phone.
[00:18:14.660 --> 00:18:16.900]   In the case of the iPhone 10,
[00:18:16.900 --> 00:18:17.980]   that will fundamentally change
[00:18:17.980 --> 00:18:19.660]   because it will open automatically.
[00:18:19.660 --> 00:18:21.940]   And there are other elements like that
[00:18:21.940 --> 00:18:24.500]   that I think will actually change the way
[00:18:24.500 --> 00:18:26.620]   that people are using their phones on a day-to-day basis.
[00:18:26.620 --> 00:18:28.660]   So like, to me, that's what I'm, you know,
[00:18:28.660 --> 00:18:31.580]   that's one of the most interesting pieces of the iPhone 10.
[00:18:31.580 --> 00:18:35.860]   It's how, like, how deeply ingrained are some of these habits
[00:18:35.860 --> 00:18:39.300]   and how quickly will we change them and adopt new ones.
[00:18:39.300 --> 00:18:41.260]   And, you know, are there privacy concerns
[00:18:41.260 --> 00:18:44.980]   about the way that we begin to use our phones?
[00:18:44.980 --> 00:18:46.820]   You know, we're trying to make it a little bit more seamless
[00:18:46.820 --> 00:18:48.780]   and make operations a little bit faster.
[00:18:48.780 --> 00:18:53.260]   But in the process, I think there are certainly privacy concerns
[00:18:53.260 --> 00:18:56.100]   that are raised and--
[00:18:56.100 --> 00:18:57.100]   - Here's--
[00:18:57.100 --> 00:18:59.620]   - Yeah. - Speaking of the Google interest,
[00:18:59.620 --> 00:19:03.300]   I've gone to Google Trends and I search for iPhone.
[00:19:03.300 --> 00:19:04.860]   And this is over a five-year trend.
[00:19:04.860 --> 00:19:07.820]   And there are peaks every fall.
[00:19:07.820 --> 00:19:08.980]   (laughs)
[00:19:08.980 --> 00:19:09.820]   Every-- - Of course.
[00:19:09.820 --> 00:19:11.780]   - In September, there's a peak.
[00:19:11.780 --> 00:19:15.260]   Although the biggest peak was oddly two years ago,
[00:19:15.260 --> 00:19:18.940]   or three years ago, September 2014,
[00:19:18.940 --> 00:19:20.540]   that would have been the iPhone-- - May I phone--
[00:19:20.540 --> 00:19:24.580]   - The 6-6, which was the first big iPhone.
[00:19:24.580 --> 00:19:26.580]   And I think that's probably the last time you could say
[00:19:26.580 --> 00:19:29.500]   that the iPhone was, you know, a significant change.
[00:19:29.500 --> 00:19:33.060]   The peak this year is not significantly higher
[00:19:33.060 --> 00:19:35.140]   than the peak last year.
[00:19:35.140 --> 00:19:39.340]   And there is a secondary peak, which is happening right now,
[00:19:39.340 --> 00:19:41.180]   of course, as people try to figure out
[00:19:41.180 --> 00:19:43.060]   whether they should buy the iPhone X.
[00:19:43.060 --> 00:19:44.660]   - This was some perspective. - I won't say it.
[00:19:44.660 --> 00:19:46.620]   - Go ahead, Amy, go ahead.
[00:19:46.620 --> 00:19:47.820]   - Just for some perspective.
[00:19:47.820 --> 00:19:50.940]   So the facial recognition technology
[00:19:50.940 --> 00:19:52.860]   that we are excited about here in the United States
[00:19:52.860 --> 00:19:56.620]   has been in widespread use in China now for a while.
[00:19:56.620 --> 00:19:58.700]   So-- - Microsoft put it in the connect,
[00:19:58.700 --> 00:20:00.940]   which they just killed.
[00:20:00.940 --> 00:20:02.300]   - I mean, this is three years ago.
[00:20:02.300 --> 00:20:03.140]   It's the same. - Three years ago.
[00:20:03.140 --> 00:20:04.540]   It's the prime sense technology from Israel
[00:20:04.540 --> 00:20:06.100]   that Apple bought.
[00:20:06.100 --> 00:20:06.940]   - Right.
[00:20:06.940 --> 00:20:07.780]   Right, yes.
[00:20:07.780 --> 00:20:11.740]   - So, and there's lots of different--
[00:20:11.740 --> 00:20:14.260]   China's so far ahead of us in a lot of ways.
[00:20:14.260 --> 00:20:17.340]   You know, using your face to pay,
[00:20:17.340 --> 00:20:19.060]   using your face for authentication,
[00:20:19.060 --> 00:20:22.180]   for all these, you know, all different sorts of things.
[00:20:22.180 --> 00:20:24.980]   There are-- but China has a very different attitude
[00:20:24.980 --> 00:20:28.620]   legally and culturally towards personal data
[00:20:28.620 --> 00:20:30.340]   than we do in the United States.
[00:20:30.340 --> 00:20:33.420]   So that's, you know, as we are sort of,
[00:20:33.420 --> 00:20:35.900]   it may feel as though we are embarking
[00:20:35.900 --> 00:20:38.500]   upon this brand new journey with Apple.
[00:20:38.500 --> 00:20:39.540]   And for the first time,
[00:20:39.540 --> 00:20:42.380]   discovering what it's like to do all of this with our faces.
[00:20:42.380 --> 00:20:45.460]   You know, in reality, we're pretty far behind Korea.
[00:20:45.460 --> 00:20:48.340]   - Right. - And China to begin with.
[00:20:48.340 --> 00:20:51.180]   The other thing that I would mention to Leo's excellent point
[00:20:51.180 --> 00:20:52.900]   about reaching peak phone,
[00:20:52.900 --> 00:20:57.580]   all of the modeling that I've done and, you know, with data,
[00:20:57.580 --> 00:20:59.140]   would indicate that this is the beginning
[00:20:59.140 --> 00:21:00.580]   of the end of smartphones.
[00:21:00.580 --> 00:21:03.780]   So-- - Oh, what's next?
[00:21:03.780 --> 00:21:08.780]   - So devices-- so a combination of wearables
[00:21:08.780 --> 00:21:13.620]   that are on our faces and on our hands.
[00:21:13.620 --> 00:21:18.620]   So glasses or sort of a,
[00:21:18.620 --> 00:21:21.060]   like a headset that you wear,
[00:21:21.060 --> 00:21:22.700]   except it points to your eye
[00:21:22.700 --> 00:21:25.580]   rather than comes near your mouth
[00:21:25.580 --> 00:21:29.420]   and a ring or wristband that goes along with it.
[00:21:29.420 --> 00:21:31.620]   It's very likely based on my modeling
[00:21:31.620 --> 00:21:34.180]   that in the year 2027,
[00:21:34.180 --> 00:21:38.500]   we will be at the beginning of AR.
[00:21:38.500 --> 00:21:39.860]   So all of the functionality that,
[00:21:39.860 --> 00:21:41.620]   for the most part that we've got with our phones,
[00:21:41.620 --> 00:21:45.540]   the screens that we hold will become the screens that we wear.
[00:21:45.540 --> 00:21:48.460]   So it's lovely and wonderful
[00:21:48.460 --> 00:21:50.220]   that iPhone is releasing the 10
[00:21:50.220 --> 00:21:52.980]   and I hope everybody really enjoys it.
[00:21:52.980 --> 00:21:59.820]   But, you know, we are gonna see fewer
[00:21:59.820 --> 00:22:02.300]   and fewer true innovations on the phone front
[00:22:02.300 --> 00:22:05.460]   because the real news is that every company, Google,
[00:22:05.460 --> 00:22:08.140]   Facebook, Apple, they're pouring huge amounts
[00:22:08.140 --> 00:22:11.580]   of resources into AR and wearable AR.
[00:22:11.580 --> 00:22:13.340]   - So-- - And Apple is, I'm sure,
[00:22:13.340 --> 00:22:15.020]   doing that as well.
[00:22:15.020 --> 00:22:16.180]   - Yeah, yeah. - Yeah.
[00:22:16.180 --> 00:22:17.380]   - Although, you know,
[00:22:17.380 --> 00:22:21.900]   it's one thing to say, well, the phone is gonna go away
[00:22:21.900 --> 00:22:22.900]   but it's still here today.
[00:22:22.900 --> 00:22:25.700]   So Apple, I mean, this is their big money.
[00:22:25.700 --> 00:22:29.220]   You can also probably fairly say that while Apple's growth
[00:22:29.220 --> 00:22:33.300]   over the last 10 years is entirely powered by the iPhone
[00:22:33.300 --> 00:22:35.500]   that Apple can't expect that to continue forever.
[00:22:35.500 --> 00:22:38.060]   So Apple's gotta be thinking about what's next.
[00:22:38.060 --> 00:22:40.220]   But meanwhile, and this is the innovators dilemma,
[00:22:40.220 --> 00:22:45.220]   they need to mine this gold mine as deep as they can
[00:22:45.220 --> 00:22:48.140]   until they run out, the same runs out of gold
[00:22:48.140 --> 00:22:51.140]   while opening new seams in other areas.
[00:22:51.140 --> 00:22:52.380]   - It's actually working-- - And they're actually working
[00:22:52.380 --> 00:22:54.220]   in your transition. - Yeah.
[00:22:54.220 --> 00:22:55.860]   They're working in AR.
[00:22:55.860 --> 00:22:57.460]   Yeah, I mean, we see AR kit,
[00:22:57.460 --> 00:23:01.620]   which was very surprising in how efficiently it worked.
[00:23:01.620 --> 00:23:03.100]   - Yeah, but it's Adobe.
[00:23:03.100 --> 00:23:03.940]   - Have you played with it?
[00:23:03.940 --> 00:23:05.540]   Well, you don't have an iPhone A.
[00:23:05.540 --> 00:23:07.380]   Every day-- - It works on the six--
[00:23:07.380 --> 00:23:08.540]   - It works on the six-- - Build on the iPhone,
[00:23:08.540 --> 00:23:09.980]   any way. - On the success as well.
[00:23:09.980 --> 00:23:11.220]   - Yeah, but that's today.
[00:23:11.220 --> 00:23:13.820]   So yes, all of this stuff is janky today,
[00:23:13.820 --> 00:23:14.860]   but that's not the point.
[00:23:14.860 --> 00:23:17.420]   The point is all of the investment and the capital,
[00:23:17.420 --> 00:23:19.420]   like the capital investment and the work
[00:23:19.420 --> 00:23:21.860]   and the people who are being hired,
[00:23:21.860 --> 00:23:25.260]   this is the beginning of the end of smartphones.
[00:23:25.260 --> 00:23:27.620]   - How long, what's that timeframe do you think?
[00:23:27.620 --> 00:23:30.540]   Five years? - My models are 10 years.
[00:23:30.540 --> 00:23:33.860]   - So okay, so you're looking in the distant future.
[00:23:33.860 --> 00:23:37.140]   - I mean, that's not my distant future, but yes.
[00:23:37.140 --> 00:23:40.420]   - Well, it's funny 'cause 10 years isn't long in humans,
[00:23:40.420 --> 00:23:42.740]   but it's a long time in technology.
[00:23:42.740 --> 00:23:47.060]   - Well, 10 years from now, the iPhone 10 will be the flip phone
[00:23:47.060 --> 00:23:49.220]   of 2027, right? - Yeah.
[00:23:49.220 --> 00:23:52.460]   - So there will be some people that still have smartphones,
[00:23:52.460 --> 00:23:55.940]   which by today's measure are very sophisticated machines,
[00:23:55.940 --> 00:23:58.780]   but that'll be a smaller group of people
[00:23:58.780 --> 00:24:01.500]   since we will have moved on to devices.
[00:24:01.500 --> 00:24:03.380]   - So the real question for Apple is,
[00:24:03.380 --> 00:24:07.620]   are they gonna be the blackberry in 10 years ago?
[00:24:07.620 --> 00:24:09.100]   - No, see, but I think that's why
[00:24:09.100 --> 00:24:10.580]   that's a slight mischaracterization.
[00:24:10.580 --> 00:24:12.660]   It's because a lot of the technology that's used
[00:24:12.660 --> 00:24:14.180]   in the iPhone will be used in a headset.
[00:24:14.180 --> 00:24:16.060]   So something like a depth sensor,
[00:24:16.060 --> 00:24:20.140]   which is what enables Face ID,
[00:24:20.140 --> 00:24:22.180]   something like that, it's easy to imagine it
[00:24:22.180 --> 00:24:23.540]   in a headset like HoloLens,
[00:24:23.540 --> 00:24:26.380]   or like any augmented reality headset.
[00:24:26.380 --> 00:24:30.500]   But to me, the reason why it's important
[00:24:30.500 --> 00:24:33.260]   to follow the technological side of this
[00:24:33.260 --> 00:24:37.700]   is because the same processors and accessories
[00:24:37.700 --> 00:24:42.420]   that are included in a mobile phone in the iPhone
[00:24:42.420 --> 00:24:45.140]   could be used for this next generation of stuff
[00:24:45.140 --> 00:24:45.980]   that you're talking about.
[00:24:45.980 --> 00:24:49.500]   I think it's probably farther away than 10 years,
[00:24:49.500 --> 00:24:53.180]   just given the rate of innovation over the past five years
[00:24:53.180 --> 00:24:54.140]   that I've seen.
[00:24:54.140 --> 00:24:56.900]   But so I'm a little bit more skeptical than you are
[00:24:56.900 --> 00:24:59.300]   that this will be adopted as quickly as you're saying.
[00:24:59.300 --> 00:25:03.820]   But regardless, the technology that's being used
[00:25:03.820 --> 00:25:08.460]   to sort of push a lot of that forward is mobile technology.
[00:25:08.460 --> 00:25:11.140]   It's all the stuff that's being used in the iPhone.
[00:25:11.140 --> 00:25:13.340]   - Right, I'm not saying one is in exchange of the other.
[00:25:13.340 --> 00:25:14.420]   What I'm saying is,
[00:25:18.380 --> 00:25:21.340]   I would expect to see fewer and fewer major innovations
[00:25:21.340 --> 00:25:25.780]   on mobile devices because the majority of time,
[00:25:25.780 --> 00:25:27.220]   effort, resources, and everything else
[00:25:27.220 --> 00:25:30.820]   is being placed into what comes several years from now.
[00:25:30.820 --> 00:25:33.780]   So it's fine, obviously pay attention to the devices
[00:25:33.780 --> 00:25:35.500]   as they're being launched,
[00:25:35.500 --> 00:25:40.500]   but all of the data and modeling that I've got
[00:25:40.500 --> 00:25:44.500]   indicates pretty strongly that this is the beginning
[00:25:44.500 --> 00:25:45.700]   of the end, which is fine.
[00:25:45.700 --> 00:25:48.020]   - Although, yeah, of course this is the nature of things.
[00:25:48.020 --> 00:25:49.660]   In fact, you hope technology moves along,
[00:25:49.660 --> 00:25:52.020]   otherwise it'll get pretty dull around here.
[00:25:52.020 --> 00:25:54.220]   But what's interesting, when I think Apple and Google
[00:25:54.220 --> 00:25:56.180]   too to a certain degree are doing this interesting
[00:25:56.180 --> 00:26:00.100]   is they are developing these next generation technologies
[00:26:00.100 --> 00:26:02.980]   not merely in parallel with their existing technologies,
[00:26:02.980 --> 00:26:06.060]   but kind of in, for instance, as you pointed out, Michael,
[00:26:06.060 --> 00:26:10.780]   this iPhone X array of sensors in the notch here,
[00:26:10.780 --> 00:26:14.900]   which is basically a connect, is clearly,
[00:26:15.860 --> 00:26:18.340]   in fact, the way they're using it is the most trivial way
[00:26:18.340 --> 00:26:20.500]   possible, they're using it for Face ID,
[00:26:20.500 --> 00:26:22.900]   it's not even being used for augmented reality,
[00:26:22.900 --> 00:26:26.500]   if it were, it would be on the other side of the phone.
[00:26:26.500 --> 00:26:29.420]   But you can imagine that the data that they get,
[00:26:29.420 --> 00:26:32.420]   the information they get, the skills they learn,
[00:26:32.420 --> 00:26:34.300]   for instance, one of the problems with getting this out
[00:26:34.300 --> 00:26:37.260]   is this Romeo and Juliet sensor pair,
[00:26:37.260 --> 00:26:39.500]   they're having a hard time getting those out in quantity,
[00:26:39.500 --> 00:26:42.020]   apparently, according to rumors from the supply chain,
[00:26:42.020 --> 00:26:44.580]   but that's the kinds of problems Apple's solving now,
[00:26:44.580 --> 00:26:46.780]   so that down the road, AirPods, same thing,
[00:26:46.780 --> 00:26:49.140]   or Google with the Google Buds, same thing,
[00:26:49.140 --> 00:26:51.460]   they're developing, and first of all,
[00:26:51.460 --> 00:26:53.060]   they're getting the consumer market ready
[00:26:53.060 --> 00:26:56.620]   for the idea of wearables in your ear,
[00:26:56.620 --> 00:26:58.660]   or the Apple Watch, which is really,
[00:26:58.660 --> 00:27:01.060]   until recently, merely a glorified pedometer,
[00:27:01.060 --> 00:27:05.540]   getting used to the idea of you wearing phone technology,
[00:27:05.540 --> 00:27:07.540]   and then using the data that they gain
[00:27:07.540 --> 00:27:11.260]   and the skills they gain to further this, right?
[00:27:11.260 --> 00:27:15.620]   So, they're not completely parallel tracks,
[00:27:15.620 --> 00:27:17.260]   and so, one of the reasons, then,
[00:27:17.260 --> 00:27:18.380]   to answer your original question,
[00:27:18.380 --> 00:27:20.540]   why we might be excited about it on iPhone X,
[00:27:20.540 --> 00:27:23.900]   is this is a first look at some of these,
[00:27:23.900 --> 00:27:25.700]   particularly the Face ID technology,
[00:27:25.700 --> 00:27:28.020]   which is being used in an admittedly trivial way,
[00:27:28.020 --> 00:27:30.900]   but will be the precursor for something much more important.
[00:27:30.900 --> 00:27:33.820]   - So, I would love to drill down on that,
[00:27:33.820 --> 00:27:36.620]   because I'm so curious to know,
[00:27:36.620 --> 00:27:39.340]   given what I've seen and how Face Recognition Technology,
[00:27:39.340 --> 00:27:41.500]   and not just Face, but Object Recognition,
[00:27:41.500 --> 00:27:44.180]   and Biometric and Bioinformatic Recognition,
[00:27:44.180 --> 00:27:45.740]   is being used elsewhere,
[00:27:45.740 --> 00:27:48.020]   I'm really curious to know how the three of you
[00:27:48.020 --> 00:27:49.820]   plan to use Face Recognition,
[00:27:49.820 --> 00:27:51.780]   and if you're concerned at all.
[00:27:51.780 --> 00:27:52.780]   - Nah.
[00:27:52.780 --> 00:27:54.860]   (laughing)
[00:27:54.860 --> 00:27:56.540]   - Biometrics is interesting.
[00:27:56.540 --> 00:27:58.180]   I'll let you talk in a second.
[00:27:58.180 --> 00:28:00.660]   I just want to say one thing about biometrics.
[00:28:00.660 --> 00:28:04.100]   Unlike other forms of authentication, it's unchangeable.
[00:28:04.100 --> 00:28:06.180]   Your iris pattern, your fingerprint,
[00:28:06.180 --> 00:28:10.740]   your face eye pattern, are kind of permanent.
[00:28:10.740 --> 00:28:12.420]   - So, let me blow your mind.
[00:28:12.420 --> 00:28:13.300]   Yeah, you're right.
[00:28:13.300 --> 00:28:16.540]   Your face is hard to change,
[00:28:16.540 --> 00:28:18.540]   unless you do it intentionally.
[00:28:18.540 --> 00:28:20.300]   - With surgery, yeah.
[00:28:20.300 --> 00:28:22.500]   - That's right, but there's a way,
[00:28:22.500 --> 00:28:27.500]   you know, a lot of the foundation of artificial intelligence,
[00:28:27.500 --> 00:28:31.460]   which underpins a lot of this technology,
[00:28:31.460 --> 00:28:35.780]   has been about teaching machines incorrect and correct,
[00:28:35.780 --> 00:28:38.060]   and part of the way that we teach them correct,
[00:28:38.060 --> 00:28:41.140]   from incorrect, is introducing adversarial information.
[00:28:41.140 --> 00:28:43.500]   So intentionally introducing one teeny tiny thing
[00:28:43.500 --> 00:28:45.060]   that's wrong, like a pixel.
[00:28:45.060 --> 00:28:49.500]   But one thing that we've discovered is that,
[00:28:49.500 --> 00:28:52.300]   your face may be imaluable.
[00:28:52.300 --> 00:28:56.060]   However, we get out of pixel,
[00:28:56.060 --> 00:28:58.660]   through a line of malicious code to your face,
[00:28:58.660 --> 00:29:00.940]   which effectively then hacks your face.
[00:29:00.940 --> 00:29:02.900]   This is why I was asking, like, who owns your face?
[00:29:02.900 --> 00:29:05.020]   - Like locks you out, or within--
[00:29:05.020 --> 00:29:06.260]   - Right, so on the one hand,
[00:29:06.260 --> 00:29:10.060]   it seems like bioinformatics and bioinformation
[00:29:10.060 --> 00:29:12.340]   is much better than two factor, right,
[00:29:12.340 --> 00:29:13.380]   or passwords or anything else,
[00:29:13.380 --> 00:29:15.780]   because you can't, you know,
[00:29:15.780 --> 00:29:18.100]   it's hard to take your face off
[00:29:18.100 --> 00:29:19.940]   and do something else with it,
[00:29:19.940 --> 00:29:21.060]   unless you're in a sci-fi film.
[00:29:21.060 --> 00:29:23.820]   But what I'm trying to say is that there are plenty of ways
[00:29:23.820 --> 00:29:26.700]   using an algorithm and something simple,
[00:29:26.700 --> 00:29:28.100]   like a pixel out of place,
[00:29:28.100 --> 00:29:31.140]   that could lock you out of all of your devices,
[00:29:31.140 --> 00:29:32.380]   or do all kinds of--
[00:29:32.380 --> 00:29:33.220]   - Oh, that's interesting.
[00:29:33.220 --> 00:29:34.060]   - Do various things.
[00:29:34.060 --> 00:29:35.380]   - I hadn't thought of it that way.
[00:29:35.380 --> 00:29:36.300]   I thought of-- - Yeah.
[00:29:36.300 --> 00:29:37.660]   - Usually you think of authentication
[00:29:37.660 --> 00:29:40.020]   being somebody trying to impersonate you.
[00:29:40.020 --> 00:29:42.500]   - Right, and there are all kinds of easy ways.
[00:29:42.500 --> 00:29:44.500]   - Well, it definitely, it introduces new risks,
[00:29:44.500 --> 00:29:46.540]   but it also eliminates a lot of other risks.
[00:29:46.540 --> 00:29:49.380]   So, I mean, I think in the spectrum
[00:29:49.380 --> 00:29:52.220]   of being secure versus unsecure,
[00:29:52.220 --> 00:29:53.460]   it might not be the best choice,
[00:29:53.460 --> 00:29:58.460]   maybe an alphanumeric pin is still the safest.
[00:29:58.460 --> 00:30:01.340]   But it's, I think that like,
[00:30:01.340 --> 00:30:02.580]   a lot of people are gonna use it,
[00:30:02.580 --> 00:30:04.260]   just because it's gonna be really easy.
[00:30:04.260 --> 00:30:06.460]   - People don't use pins 'cause it's too hard.
[00:30:06.460 --> 00:30:08.420]   So, and that's why I thought Touch ID,
[00:30:08.420 --> 00:30:10.940]   and it was really, that was an innovation
[00:30:10.940 --> 00:30:13.020]   because it was so fast and easy,
[00:30:13.020 --> 00:30:15.900]   and it did encourage people to secure their devices.
[00:30:15.900 --> 00:30:19.020]   Most people did not secure their devices before Touch ID.
[00:30:19.020 --> 00:30:20.260]   They didn't wanna do a pin.
[00:30:20.260 --> 00:30:22.380]   It was too much of a pin in the butt.
[00:30:22.380 --> 00:30:25.340]   And I think if people are comfortable with Touch ID,
[00:30:25.340 --> 00:30:28.340]   and with giving their information,
[00:30:28.340 --> 00:30:31.340]   whether or not Apple or others are secure,
[00:30:31.340 --> 00:30:34.420]   people are comfortable giving their fingerprint information
[00:30:34.420 --> 00:30:35.860]   to those companies,
[00:30:35.860 --> 00:30:37.020]   I'm not sure there's gonna be,
[00:30:37.020 --> 00:30:39.500]   I mean, there's gonna be a little bit more resistance
[00:30:39.500 --> 00:30:44.340]   for faces, but not so much that it's gonna
[00:30:44.340 --> 00:30:46.900]   get the technology to tank.
[00:30:46.900 --> 00:30:49.500]   But that being said, I do think,
[00:30:49.500 --> 00:30:53.060]   I mean, we have reached a peak phone,
[00:30:53.060 --> 00:30:55.660]   and all of you have said this in different ways,
[00:30:55.660 --> 00:30:58.340]   but the technologies that we're seeing now
[00:30:58.340 --> 00:31:01.700]   are not as exciting as they were five years ago
[00:31:01.700 --> 00:31:03.540]   in the phone space.
[00:31:03.540 --> 00:31:06.620]   And I think we're all sort of discounting another reason
[00:31:06.620 --> 00:31:10.620]   why we are quote unquote excited about the iPhone X,
[00:31:10.620 --> 00:31:13.700]   and that's because it's just shinier.
[00:31:13.700 --> 00:31:16.940]   It's a bit more expensive, it's a bit different.
[00:31:16.940 --> 00:31:19.540]   It doesn't matter really how it's different.
[00:31:19.540 --> 00:31:24.300]   I was referencing handbags at the beginning of the show.
[00:31:24.300 --> 00:31:29.300]   I think the iPhone X is kind of the slightly more exclusive
[00:31:29.300 --> 00:31:33.500]   handbag of phones, and that aspect,
[00:31:33.500 --> 00:31:37.220]   I'm not just saying this to be, you know, to put it down,
[00:31:37.220 --> 00:31:40.260]   I think the fact that it is a little bit different,
[00:31:40.260 --> 00:31:43.300]   more expensive, whatever, has value as well
[00:31:43.300 --> 00:31:47.580]   for people like us, because just as someone
[00:31:47.580 --> 00:31:52.460]   who likes handbags is not gonna use that 4,000 bucks
[00:31:52.460 --> 00:31:56.540]   handbag better than one that would cost a couple hundred bucks.
[00:31:56.540 --> 00:31:59.900]   The fact that it's new and a little bit different
[00:31:59.900 --> 00:32:03.820]   is gonna bring us joy because we like tech, right?
[00:32:03.820 --> 00:32:07.620]   So I think that is also a reason why we want that phone,
[00:32:07.620 --> 00:32:10.660]   regardless of whether or not it has all of that technology.
[00:32:10.660 --> 00:32:13.940]   I think we rationalize our desire for those things
[00:32:13.940 --> 00:32:17.340]   a little bit with the face ID, which when you think of it,
[00:32:17.340 --> 00:32:19.980]   really, is it gonna be that different from touch ID?
[00:32:19.980 --> 00:32:22.780]   Do we, like, would I be unhappy with touch ID?
[00:32:22.780 --> 00:32:24.220]   I don't think so, it works fine,
[00:32:24.220 --> 00:32:27.540]   and I don't even need to look at it to unlock my phone.
[00:32:27.540 --> 00:32:28.780]   - Right, and that's my point.
[00:32:28.780 --> 00:32:32.980]   My point is that it's a seamless interface,
[00:32:32.980 --> 00:32:35.180]   and because it's a seamless, easy to use interface,
[00:32:35.180 --> 00:32:38.900]   lots of people will use it, and unfortunately,
[00:32:38.900 --> 00:32:40.220]   this is true of the United States,
[00:32:40.220 --> 00:32:43.260]   it's certainly true of Scandinavia and of Europe.
[00:32:43.260 --> 00:32:44.660]   Our government isn't, you know,
[00:32:44.660 --> 00:32:48.060]   governments don't keep up and keep the same pace
[00:32:48.060 --> 00:32:50.580]   with technology as technology would want, right?
[00:32:50.580 --> 00:32:53.380]   Because it develops faster than our laws and regulations
[00:32:53.380 --> 00:32:55.300]   and regulators are able to think about it.
[00:32:55.300 --> 00:32:58.780]   And the problem is we don't have case law,
[00:32:58.780 --> 00:33:01.940]   we haven't defined who owns your face, right?
[00:33:01.940 --> 00:33:05.260]   So we have a technology that is going to rely on it,
[00:33:05.260 --> 00:33:09.020]   we have seen this go into weird directions in China already.
[00:33:09.020 --> 00:33:10.580]   I'll give you an example.
[00:33:10.580 --> 00:33:13.700]   So in China, there is facial recognition technology
[00:33:13.700 --> 00:33:15.540]   and use in a lot of different cities.
[00:33:15.540 --> 00:33:19.620]   In Shungdong, if you cross the street,
[00:33:19.620 --> 00:33:21.580]   there are electronic billboards all over the place,
[00:33:21.580 --> 00:33:26.180]   and if you J-Walk, that image recognition is used
[00:33:26.180 --> 00:33:27.900]   to identify who you are,
[00:33:27.900 --> 00:33:30.940]   and then your personal details are shown as a way
[00:33:30.940 --> 00:33:32.220]   to publicly name you. - Oh geez.
[00:33:32.220 --> 00:33:34.860]   Oh my God, they talk to you on a billboard.
[00:33:34.860 --> 00:33:37.740]   - They do, and not just that then, then.
[00:33:37.740 --> 00:33:39.300]   - That's horrible. - It's your opinion.
[00:33:39.300 --> 00:33:41.580]   Right, your face and your details,
[00:33:41.580 --> 00:33:45.380]   and the fact that you J-Walked is then sent out over Weibo.
[00:33:45.380 --> 00:33:48.140]   - Oh my God, the Chinese social network.
[00:33:48.140 --> 00:33:50.540]   - Oh my God. - So here's the deal.
[00:33:50.540 --> 00:33:51.900]   - I mean, it's China.
[00:33:51.900 --> 00:33:55.060]   - What is, well, but it's, okay, but this is,
[00:33:55.060 --> 00:33:59.460]   my job is to use data to model out what are the implications,
[00:33:59.460 --> 00:34:02.780]   not just like what, not to predict just like what tech is
[00:34:02.780 --> 00:34:04.780]   coming, but what are the second, third, fourth order
[00:34:04.780 --> 00:34:06.980]   implications of that type? - Well, and there's also this,
[00:34:06.980 --> 00:34:08.140]   I think even in the United States,
[00:34:08.140 --> 00:34:09.900]   this assumption that anything that's gathered
[00:34:09.900 --> 00:34:12.780]   when you're in public is kind of public domain,
[00:34:12.780 --> 00:34:15.100]   because you're walking around,
[00:34:15.100 --> 00:34:16.580]   you're on the streets.
[00:34:16.580 --> 00:34:21.580]   So, now I admit that society is starting to think about that
[00:34:21.580 --> 00:34:24.060]   is just 'cause I'm in public,
[00:34:24.060 --> 00:34:26.540]   should you be able to take a picture of me, for instance?
[00:34:26.540 --> 00:34:28.820]   - Right, and now let me tell you how this plays out.
[00:34:28.820 --> 00:34:32.340]   So we haven't quite figured out the who owns your face problem,
[00:34:32.340 --> 00:34:34.620]   but what we do know is that people are angry
[00:34:34.620 --> 00:34:37.020]   for various reasons all around the world,
[00:34:37.020 --> 00:34:39.700]   and have started suing companies like Google and Apple
[00:34:39.700 --> 00:34:40.900]   and Facebook. - In Germany,
[00:34:40.900 --> 00:34:43.820]   they sued them about pictures of street view,
[00:34:43.820 --> 00:34:46.420]   pictures of your house, and we call it blurmany now,
[00:34:46.420 --> 00:34:48.020]   because Google's been required by the courts
[00:34:48.020 --> 00:34:51.900]   to blur out houses at the request of the homeowners.
[00:34:51.900 --> 00:34:55.500]   - Or another way, another term of art is splinternet.
[00:34:55.500 --> 00:34:58.340]   So one of the things, if we don't figure out,
[00:34:58.340 --> 00:35:01.700]   you know, again, we need to push the envelope forward
[00:35:01.700 --> 00:35:04.100]   on technology, but we also need to simultaneously
[00:35:04.100 --> 00:35:07.460]   think about the implications in the present,
[00:35:07.460 --> 00:35:10.460]   because we could very well end up with a sort of
[00:35:10.460 --> 00:35:13.780]   multiverse of internets, where in Germany,
[00:35:13.780 --> 00:35:18.100]   you know, that the worldwide free web suddenly,
[00:35:18.100 --> 00:35:20.860]   you know, must also follow geographic boundaries.
[00:35:20.860 --> 00:35:23.220]   And it's very possible that in the near future,
[00:35:23.220 --> 00:35:25.940]   in like the next 10 years, there'll be a German version
[00:35:25.940 --> 00:35:28.620]   of the internet and a Chinese version of the internet,
[00:35:28.620 --> 00:35:30.940]   and a United States version of the internet,
[00:35:30.940 --> 00:35:33.980]   and a Canadian version, you know, and so forth and so on,
[00:35:33.980 --> 00:35:36.980]   which yes, there are ways to get around,
[00:35:36.980 --> 00:35:39.980]   and yes, you could theoretically still penetrate
[00:35:39.980 --> 00:35:44.340]   and get different versions by using, you know,
[00:35:44.340 --> 00:35:46.100]   IP spoofers and stuff like that.
[00:35:46.100 --> 00:35:49.340]   But for the average person, what that means is,
[00:35:49.340 --> 00:35:51.460]   the way that information moves around is different.
[00:35:51.460 --> 00:35:53.700]   It means the way that you, theoretically,
[00:35:53.700 --> 00:35:56.700]   if you took your iPhone 10 into France,
[00:35:56.700 --> 00:36:00.940]   that maybe the face recognition technology
[00:36:00.940 --> 00:36:01.900]   isn't legal there.
[00:36:01.900 --> 00:36:02.740]   - Shut's off.
[00:36:02.740 --> 00:36:03.580]   - Yeah.
[00:36:03.580 --> 00:36:06.180]   - Yeah, I mean, there are some real,
[00:36:06.180 --> 00:36:09.300]   real important things to be thinking about
[00:36:09.300 --> 00:36:12.980]   while we marvel at the shiny new technology.
[00:36:12.980 --> 00:36:13.820]   That's all I'm saying.
[00:36:13.820 --> 00:36:17.500]   And it's always better to try to think through this
[00:36:17.500 --> 00:36:20.460]   in advance and model out what those scenarios are,
[00:36:20.460 --> 00:36:23.340]   rather than waiting for somebody to murder somebody,
[00:36:23.340 --> 00:36:25.660]   and now somebody wants to a backdoor into the phone
[00:36:25.660 --> 00:36:29.060]   so they can scrape your face off, whatever it might be.
[00:36:29.060 --> 00:36:31.100]   And suddenly we're all making decisions
[00:36:31.100 --> 00:36:34.060]   under significant duress, and politicians
[00:36:34.060 --> 00:36:35.820]   are making these decisions for us,
[00:36:35.820 --> 00:36:38.140]   and they don't know very much about technology.
[00:36:38.140 --> 00:36:39.620]   And it's happening all over the world,
[00:36:39.620 --> 00:36:41.820]   and it becomes very difficult.
[00:36:41.820 --> 00:36:45.500]   - How about this article from a futurist?
[00:36:45.500 --> 00:36:50.500]   You may know Matt Rainin writes on Medium about the,
[00:36:50.500 --> 00:36:54.380]   who owns in augmented reality,
[00:36:54.380 --> 00:36:56.940]   who owns the picture of the house?
[00:36:56.940 --> 00:37:00.660]   Some of the augmented reality apps allow you to put graffiti,
[00:37:00.660 --> 00:37:03.180]   virtual graffiti on a wall.
[00:37:03.180 --> 00:37:05.100]   Let's say you own a restaurant,
[00:37:05.100 --> 00:37:07.620]   somebody comes along and writes, this restaurant sucks.
[00:37:07.620 --> 00:37:10.500]   - That's part of Facebook's plant.
[00:37:10.500 --> 00:37:12.220]   That's one of the things that they announced at FA.
[00:37:12.220 --> 00:37:13.300]   - Yeah, they showed it, yeah.
[00:37:13.300 --> 00:37:14.300]   - They'll need to run it, right, right.
[00:37:14.300 --> 00:37:17.860]   - So, is that, I mean, I guess that's not much different
[00:37:17.860 --> 00:37:18.780]   than a Yelp review.
[00:37:18.780 --> 00:37:22.620]   - It's, I mean, there's definitely a lot of questions
[00:37:22.620 --> 00:37:25.340]   to be asked there, and some of them are already being asked
[00:37:25.340 --> 00:37:26.380]   as you're saying, Amy.
[00:37:26.380 --> 00:37:30.740]   And the splintered internet is already happening.
[00:37:30.740 --> 00:37:31.580]   We have a correct record.
[00:37:31.580 --> 00:37:32.900]   - I think so, thanks to courts.
[00:37:32.900 --> 00:37:36.300]   The right to be forgotten has been heavily debated,
[00:37:36.300 --> 00:37:37.780]   and we've all talked about it,
[00:37:37.780 --> 00:37:42.780]   but some links are now required to disappear
[00:37:42.780 --> 00:37:47.380]   from Google when you search for some things in France,
[00:37:47.380 --> 00:37:51.860]   and some people would like it to be unavailable worldwide
[00:37:51.860 --> 00:37:55.980]   because of the fact that if you go to google.com,
[00:37:55.980 --> 00:37:58.220]   for example, you can still access that link.
[00:37:58.220 --> 00:38:01.820]   So that's causing issues to a lot of people,
[00:38:01.820 --> 00:38:04.820]   but what you're sort of saying, Amy,
[00:38:04.820 --> 00:38:08.140]   if we say it a different way, is you would like,
[00:38:08.140 --> 00:38:14.580]   the Parliament to legislate on technology that hasn't even--
[00:38:14.580 --> 00:38:15.420]   - No, not thank that.
[00:38:15.420 --> 00:38:17.660]   - That's, that's definitely not saying.
[00:38:17.660 --> 00:38:19.060]   - I'm saying it the day before we know.
[00:38:19.060 --> 00:38:20.180]   - No, no, I'm not.
[00:38:20.180 --> 00:38:25.140]   What I'm, no, because I, first of all, as far as I know,
[00:38:25.140 --> 00:38:27.420]   Project Selden never took off for those of you
[00:38:27.420 --> 00:38:29.140]   who read Foundation, and we do not have the intercolors.
[00:38:29.140 --> 00:38:31.100]   - Project Selden, I love it.
[00:38:31.100 --> 00:38:32.260]   - Yeah, we don't have it, you know.
[00:38:32.260 --> 00:38:36.060]   - Harry Selden, he was, they were psychometricians,
[00:38:36.060 --> 00:38:37.700]   psychohistorians, right?
[00:38:37.700 --> 00:38:38.700]   - Right, right.
[00:38:38.700 --> 00:38:39.860]   - I'm gonna have to take back to--
[00:38:39.860 --> 00:38:41.620]   - We don't have a shadow government of academics
[00:38:41.620 --> 00:38:43.860]   and smart, cool geeky people running the universe.
[00:38:43.860 --> 00:38:44.860]   (laughing)
[00:38:44.860 --> 00:38:46.820]   - Let's get that right now.
[00:38:46.820 --> 00:38:52.060]   - Right, so in absence of sort of one government
[00:38:52.060 --> 00:38:54.900]   that rules the planet, which we don't need or want,
[00:38:56.580 --> 00:39:01.180]   you know, if we wind up at a legislative,
[00:39:01.180 --> 00:39:03.340]   if we wind up legislating in every single different country,
[00:39:03.340 --> 00:39:06.620]   we're gonna wind up with all of these different laws anyways.
[00:39:06.620 --> 00:39:09.380]   So I'm not proposing a legislative approach
[00:39:09.380 --> 00:39:11.420]   and that the United States and France
[00:39:11.420 --> 00:39:14.540]   and everybody figures out what their exact rules are.
[00:39:14.540 --> 00:39:19.540]   What I am saying is, we are using more and more technology
[00:39:19.540 --> 00:39:24.020]   and our data that we create using technology
[00:39:24.020 --> 00:39:26.060]   will itself become a natural resource
[00:39:26.060 --> 00:39:27.420]   in the very near future.
[00:39:27.420 --> 00:39:32.780]   And the people who use the technology
[00:39:32.780 --> 00:39:36.140]   don't fully understand the implications of its use.
[00:39:36.140 --> 00:39:39.420]   And we aren't in any way, shape or form,
[00:39:39.420 --> 00:39:43.300]   prepared to have a conversation legally
[00:39:43.300 --> 00:39:46.740]   about things like who owns your face.
[00:39:46.740 --> 00:39:48.660]   And this just happened in the United States
[00:39:48.660 --> 00:39:50.580]   during the San Bernardino shooting.
[00:39:50.580 --> 00:39:53.340]   It was all this back and forth about what the FBI and,
[00:39:55.220 --> 00:39:58.300]   you know, having a back door into the phone,
[00:39:58.300 --> 00:40:03.140]   we, you know, legislation wouldn't have solved that issue.
[00:40:03.140 --> 00:40:05.900]   So I'm not advocating for legislation or regulation.
[00:40:05.900 --> 00:40:08.300]   However, we don't have norms and standards
[00:40:08.300 --> 00:40:10.700]   and there's no interoperability
[00:40:10.700 --> 00:40:14.020]   even at this point between countries on a lot of this stuff.
[00:40:14.020 --> 00:40:17.180]   So, you know, it's fine to keep working on the technology
[00:40:17.180 --> 00:40:20.740]   but we can't decouple what's happening in Silicon Valley
[00:40:20.740 --> 00:40:23.660]   from what happens in the consumer marketplace
[00:40:23.660 --> 00:40:26.060]   and what happens in our respective capitals
[00:40:26.060 --> 00:40:27.940]   all around the world.
[00:40:27.940 --> 00:40:31.740]   There's gotta be, there's gotta be people working together
[00:40:31.740 --> 00:40:33.580]   on all of this in advance.
[00:40:33.580 --> 00:40:35.940]   - Let's take a break.
[00:40:35.940 --> 00:40:36.940]   I love this conversation.
[00:40:36.940 --> 00:40:38.300]   Here we are half an hour in the show.
[00:40:38.300 --> 00:40:39.860]   We got real heavy already.
[00:40:39.860 --> 00:40:42.060]   Thank you for being here.
[00:40:42.060 --> 00:40:43.620]   Amy Webb, she's a futurist.
[00:40:43.620 --> 00:40:46.420]   I think you understand a little bit more about what that
[00:40:46.420 --> 00:40:47.260]   might mean.
[00:40:47.260 --> 00:40:49.300]   You'll find more about her at ameweb.io.
[00:40:49.300 --> 00:40:51.140]   Her book, The Signals are Talking,
[00:40:51.140 --> 00:40:56.140]   tells you her secrets, how you can do what she does,
[00:40:56.140 --> 00:41:00.220]   forecast and take action on tomorrow's trends today.
[00:41:00.220 --> 00:41:01.940]   It's available everywhere books are.
[00:41:01.940 --> 00:41:05.020]   She also, of course, leads the Future Today Institute
[00:41:05.020 --> 00:41:06.500]   and as a speaker.
[00:41:06.500 --> 00:41:09.540]   Also, Patrick Bejaw.
[00:41:09.540 --> 00:41:12.420]   In fact, you had a great conversation on the Phileas Club
[00:41:12.420 --> 00:41:15.580]   about the era of outrage that we live in.
[00:41:15.580 --> 00:41:17.940]   I wouldn't mind talking a little bit about that
[00:41:17.940 --> 00:41:18.940]   when we come back.
[00:41:18.940 --> 00:41:19.940]   That's a great subject.
[00:41:19.940 --> 00:41:23.140]   Patrick is at frenchspin.com
[00:41:23.140 --> 00:41:25.140]   where he leads his English language podcast,
[00:41:25.140 --> 00:41:27.340]   The Phileas Club and his French language podcast,
[00:41:27.340 --> 00:41:30.980]   (speaking in foreign language)
[00:41:30.980 --> 00:41:34.340]   (speaking in foreign language)
[00:41:34.340 --> 00:41:36.700]   My favorite word for a computer.
[00:41:36.700 --> 00:41:38.780]   (speaking in foreign language)
[00:41:38.780 --> 00:41:41.660]   And you've made the transition.
[00:41:41.660 --> 00:41:44.100]   Last time we talked, you were working at Blizzard,
[00:41:44.100 --> 00:41:45.220]   but now you've made the transition.
[00:41:45.220 --> 00:41:47.460]   You're a full-time podcaster.
[00:41:47.460 --> 00:41:49.300]   - Yeah, it's happened a few years ago.
[00:41:49.300 --> 00:41:53.500]   You know, with the wonder of Patreon.
[00:41:53.500 --> 00:41:55.940]   And it's been going pretty well.
[00:41:55.940 --> 00:41:58.980]   It's an interesting life that, you know,
[00:41:58.980 --> 00:42:02.660]   you've known it for a while and I'm not as successful as you,
[00:42:02.660 --> 00:42:07.420]   but being a podcaster full-time is kind of a strange way
[00:42:07.420 --> 00:42:09.620]   to live.
[00:42:09.620 --> 00:42:10.980]   - It's hard to explain to your mother,
[00:42:10.980 --> 00:42:15.940]   but at the same time, I like having control of what we do.
[00:42:15.940 --> 00:42:18.140]   I think there's a certain sound to be said for that.
[00:42:18.140 --> 00:42:21.340]   - That's certainly the great benefit of it.
[00:42:21.340 --> 00:42:24.140]   - Well, support Patrick on Patreon.
[00:42:24.140 --> 00:42:25.340]   What should we search for?
[00:42:25.340 --> 00:42:30.940]   - L'Orond-Boutc is the one that is my primary revenue.
[00:42:30.940 --> 00:42:32.020]   - Good.
[00:42:32.020 --> 00:42:34.420]   Good way to study French.
[00:42:34.420 --> 00:42:36.260]   - Exactly. - How about that?
[00:42:36.260 --> 00:42:38.940]   And from Mashable, it's great to have Michael Nunez
[00:42:38.940 --> 00:42:40.420]   in the house.
[00:42:40.420 --> 00:42:42.940]   He is a editor, technology editor.
[00:42:42.940 --> 00:42:46.260]   He's the A-technology editor, deputy tech editor for tech.
[00:42:46.260 --> 00:42:47.860]   Editor of Tech for Tech.
[00:42:47.860 --> 00:42:49.020]   I don't know what you do.
[00:42:49.020 --> 00:42:50.460]   At Mashable. - At Mashable.
[00:42:50.460 --> 00:42:52.020]   - At Mashable.com.
[00:42:52.020 --> 00:42:55.060]   Deputy tech editor at Mashable.com.
[00:42:55.060 --> 00:42:56.340]   How about that?
[00:42:56.340 --> 00:42:57.500]   - Nice, thank you.
[00:42:57.500 --> 00:42:59.140]   - All right, show today.
[00:42:59.140 --> 00:43:00.460]   You'll know about this then.
[00:43:00.460 --> 00:43:05.460]   If you're in a tech at all, you know the words Scott Evest.
[00:43:05.460 --> 00:43:07.940]   I live, I'm wearing Scott Evest underwear.
[00:43:07.940 --> 00:43:10.020]   I have underwear with pockets right now.
[00:43:10.020 --> 00:43:14.100]   I live in my Scott Evest jackets, shirts, pants,
[00:43:14.100 --> 00:43:18.380]   technology clothing designed for you.
[00:43:18.380 --> 00:43:20.380]   We've been fans of Scott Evest forever.
[00:43:20.380 --> 00:43:23.020]   In fact, if you get a Scott Evest,
[00:43:23.020 --> 00:43:26.140]   I think even to this day, if you get a Scott Evest,
[00:43:26.140 --> 00:43:28.820]   you'll find some pockets inside the pockets,
[00:43:28.820 --> 00:43:31.780]   you'll find some cards with a picture of me
[00:43:31.780 --> 00:43:34.340]   wearing my Scott Evest.
[00:43:34.340 --> 00:43:36.820]   We have twit fleeces, which we love.
[00:43:36.820 --> 00:43:39.260]   Those fleeces are warm, comfortable.
[00:43:39.260 --> 00:43:42.220]   And as with all the Scott Evest clothing,
[00:43:42.220 --> 00:43:45.060]   have strategically placed pockets.
[00:43:45.060 --> 00:43:48.420]   Everybody who knows tech loves Scott Evests.
[00:43:48.420 --> 00:43:49.780]   The best thing for travelers,
[00:43:49.780 --> 00:43:52.340]   I wear my Scott Evest vest.
[00:43:52.340 --> 00:43:54.620]   They actually do make vests with all the,
[00:43:54.620 --> 00:43:57.340]   I think I have 26 pockets in my vest.
[00:43:57.340 --> 00:44:00.740]   That means I, this is a great trick to get through.
[00:44:00.740 --> 00:44:04.620]   Security and to avoid weight limits, I load it up.
[00:44:04.620 --> 00:44:08.940]   I'll put lenses, laptops, iPads, cameras, phones,
[00:44:08.940 --> 00:44:11.100]   everything in my Scott Evest.
[00:44:11.100 --> 00:44:13.900]   And then when you get to the security,
[00:44:13.900 --> 00:44:15.420]   you just put it on the conveyor belt.
[00:44:15.420 --> 00:44:17.220]   That's fine, they don't mind that.
[00:44:17.220 --> 00:44:20.460]   You walk through, nobody weighs your coat.
[00:44:20.460 --> 00:44:23.940]   It's a great way to get all your gear with you
[00:44:23.940 --> 00:44:25.380]   so that you never lose it.
[00:44:25.380 --> 00:44:28.020]   And I have to say, the zippered pockets in the Scott Evest
[00:44:28.020 --> 00:44:31.780]   are great for protecting your wallet.
[00:44:31.780 --> 00:44:35.180]   And the RFID blocking pocket is great for protecting
[00:44:35.180 --> 00:44:40.180]   your secret stuff, whether it's keys or cards,
[00:44:40.620 --> 00:44:45.620]   from snoops, keep that passport free from high tech skimmers,
[00:44:45.620 --> 00:44:50.220]   thieves, everything Scott Evest makes is great products
[00:44:50.220 --> 00:44:52.340]   for men and for women.
[00:44:52.340 --> 00:44:55.540]   Some of the pockets are fantastic.
[00:44:55.540 --> 00:44:57.220]   They have pockets for eyeglasses.
[00:44:57.220 --> 00:44:58.860]   In fact, my eyeglass pocket, I love it,
[00:44:58.860 --> 00:45:03.380]   it has a little, on a clip, a little eyeglass chamois,
[00:45:03.380 --> 00:45:05.620]   so clean my glasses, I love that.
[00:45:05.620 --> 00:45:09.220]   The personal area network links almost all the pockets
[00:45:09.220 --> 00:45:11.660]   to one another so you can keep your iPod
[00:45:11.660 --> 00:45:13.500]   or your iPhone in one pocket,
[00:45:13.500 --> 00:45:15.740]   work, run the headphone wires through it,
[00:45:15.740 --> 00:45:18.140]   have them come out at the neck,
[00:45:18.140 --> 00:45:19.860]   you'll never get tangled again.
[00:45:19.860 --> 00:45:22.980]   It is fantastic and they don't look blocky
[00:45:22.980 --> 00:45:23.900]   if you'll load them up.
[00:45:23.900 --> 00:45:27.220]   I have to say, you don't look like the Pillsbury Doughboy,
[00:45:27.220 --> 00:45:30.620]   you just, you know, you're just carrying everything
[00:45:30.620 --> 00:45:33.260]   that you need, whether you're a photographer,
[00:45:33.260 --> 00:45:36.420]   a tech journalist, or you just like to have a snack
[00:45:36.420 --> 00:45:39.500]   in your breast pocket at all times.
[00:45:39.500 --> 00:45:42.900]   21 pockets in the fleece jacket, removable sleeves,
[00:45:42.900 --> 00:45:44.740]   heavyweight fleece that keeps you warm.
[00:45:44.740 --> 00:45:49.740]   The Quest Fest has 42 pockets, it's teflon treated,
[00:45:49.740 --> 00:45:52.020]   so it's water and stain resistant.
[00:45:52.020 --> 00:45:54.460]   Did I say that you have products for women too?
[00:45:54.460 --> 00:45:56.860]   You bet, some really nice looking stuff
[00:45:56.860 --> 00:45:59.620]   for women and men.
[00:45:59.620 --> 00:46:02.300]   Hoodies, dresses, and skirts even,
[00:46:02.300 --> 00:46:05.340]   that have all the famous Scott Evest features.
[00:46:05.340 --> 00:46:06.500]   It's the best way to travel.
[00:46:06.500 --> 00:46:08.580]   I wouldn't, you see me on a plane,
[00:46:08.580 --> 00:46:11.420]   I will be wearing a Scott Evest, no question about it.
[00:46:11.420 --> 00:46:14.860]   And if you've got a geek that you've got to get a gift for,
[00:46:14.860 --> 00:46:16.780]   this is a great gift.
[00:46:16.780 --> 00:46:18.700]   The geeks don't think about this sometimes,
[00:46:18.700 --> 00:46:20.500]   but look at these, these, believe it or not,
[00:46:20.500 --> 00:46:22.980]   the Deborah dress, it's got,
[00:46:22.980 --> 00:46:25.380]   hover, I love these, the website,
[00:46:25.380 --> 00:46:27.660]   'cause if you hover over the eye,
[00:46:27.660 --> 00:46:31.020]   you can see where the hidden pockets are
[00:46:31.020 --> 00:46:34.940]   in all these outfits, I love this.
[00:46:35.460 --> 00:46:39.820]   Scott, S-C-O-T-T-E, V-E-S-T, ScottieVest.com,
[00:46:39.820 --> 00:46:42.100]   and we've got a deal for you, this is actually fantastic.
[00:46:42.100 --> 00:46:44.100]   Dig your briefcase, your backpack,
[00:46:44.100 --> 00:46:46.140]   your messenger bag, your man purse.
[00:46:46.140 --> 00:46:50.860]   Carry it all in your fabulous ScottieVest clothing,
[00:46:50.860 --> 00:46:54.740]   and you get an extra 25% off right now
[00:46:54.740 --> 00:46:56.820]   when you go to scottieVest.com/twit.
[00:46:56.820 --> 00:46:59.820]   25% off.
[00:46:59.820 --> 00:47:01.300]   I like this, I've got some new stuff,
[00:47:01.300 --> 00:47:03.340]   I think I'm gonna have to go and do some ordering,
[00:47:03.340 --> 00:47:06.860]   I like that sportsman vest that looks good, that's sharp.
[00:47:06.860 --> 00:47:09.220]   I have the Tropaformer, this is what I wore in France
[00:47:09.220 --> 00:47:11.100]   when I was in France this year,
[00:47:11.100 --> 00:47:12.820]   'cause I like the red piping, the styling,
[00:47:12.820 --> 00:47:14.300]   and look at all those pockets.
[00:47:14.300 --> 00:47:16.100]   I love it.
[00:47:16.100 --> 00:47:19.180]   ScottieVest.com/twit,
[00:47:19.180 --> 00:47:20.660]   Scott's a good friend, Scott Jordan,
[00:47:20.660 --> 00:47:22.420]   he was here just the other day with Margot,
[00:47:22.420 --> 00:47:25.100]   his beautiful poodle, I know he's watching,
[00:47:25.100 --> 00:47:29.860]   hi Scott, help a fella out, go to scottieVest.com/twit,
[00:47:29.860 --> 00:47:32.160]   we'll save you 25% right now.
[00:47:33.000 --> 00:47:36.120]   We're having some fun here,
[00:47:36.120 --> 00:47:39.480]   I don't know where to go next with this,
[00:47:39.480 --> 00:47:42.160]   should we go for more controversy?
[00:47:42.160 --> 00:47:44.760]   We're talking about the iPhone X,
[00:47:44.760 --> 00:47:46.740]   I have to show you this.
[00:47:46.740 --> 00:47:53.400]   Apple has, in some ways become a fashion company,
[00:47:53.400 --> 00:47:54.680]   unless of a technology company,
[00:47:54.680 --> 00:47:59.120]   I know every time I say this, it drives Apple fans crazy,
[00:47:59.120 --> 00:48:01.960]   and they say things like, yeah, well what about the A11
[00:48:01.960 --> 00:48:04.520]   bionic chip or the amazing face recognition?
[00:48:04.520 --> 00:48:07.400]   Okay fine, but if you can't get your calculator
[00:48:07.400 --> 00:48:12.160]   to add one plus two plus three, what good is that?
[00:48:12.160 --> 00:48:14.520]   Right?
[00:48:14.520 --> 00:48:16.280]   Okay, maybe they're a technology company,
[00:48:16.280 --> 00:48:19.680]   but they are, this is by the way, this bug,
[00:48:19.680 --> 00:48:23.400]   which is well known in the iPhone,
[00:48:23.400 --> 00:48:25.920]   is because of looks, because of style,
[00:48:25.920 --> 00:48:30.920]   because of the animation, when you press the operator keys,
[00:48:31.040 --> 00:48:33.320]   when you press the fade in and fade out.
[00:48:33.320 --> 00:48:35.480]   Well the problem with that is,
[00:48:35.480 --> 00:48:37.840]   when you're hitting numbers,
[00:48:37.840 --> 00:48:41.520]   if you don't pause for the fade in, fade out,
[00:48:41.520 --> 00:48:43.680]   they won't register properly.
[00:48:43.680 --> 00:48:46.880]   See, it did that right one, plus two plus three equals six,
[00:48:46.880 --> 00:48:50.880]   if I do it slowly, but as soon as I go one plus two plus three,
[00:48:50.880 --> 00:48:56.560]   21, wait a minute, one plus two plus three,
[00:48:56.560 --> 00:49:00.960]   23, one, now that's, I'm not doing it super fast either.
[00:49:01.440 --> 00:49:03.880]   It's just a bug, and now it's an easy bug to fix.
[00:49:03.880 --> 00:49:05.920]   You merely interrupt the animation
[00:49:05.920 --> 00:49:09.840]   when the next operand is hit, but they don't,
[00:49:09.840 --> 00:49:11.880]   they don't seem to understand that.
[00:49:11.880 --> 00:49:13.560]   I don't know, they don't care.
[00:49:13.560 --> 00:49:14.940]   This has been around for a while,
[00:49:14.940 --> 00:49:18.160]   Apple's put out three updates to iOS 11 hasn't fixed it.
[00:49:18.160 --> 00:49:23.720]   So is that really a problem, or is it something
[00:49:23.720 --> 00:49:26.720]   that we wouldn't notice in any other phone,
[00:49:26.720 --> 00:49:27.560]   if it wasn't the time?
[00:49:27.560 --> 00:49:29.540]   I'll tell you why it's a problem, Patrick.
[00:49:29.540 --> 00:49:31.680]   First of all, you should not, at this point,
[00:49:31.680 --> 00:49:33.500]   you should not use the Apple calculator period.
[00:49:33.500 --> 00:49:35.860]   What if I say I'm gonna add up five numbers
[00:49:35.860 --> 00:49:38.360]   on the Apple calculator, and I do it carefully,
[00:49:38.360 --> 00:49:41.460]   so I don't get in the errors, but at one point, one place,
[00:49:41.460 --> 00:49:44.500]   I type it a little too fast, an error is introduced
[00:49:44.500 --> 00:49:47.500]   that I will not see, I will get the wrong answer.
[00:49:47.500 --> 00:49:49.260]   If you're worse, I understand it.
[00:49:49.260 --> 00:49:52.900]   - Well, don't do your taxes with the iPhone calculator then.
[00:49:52.900 --> 00:49:56.780]   - It's a problem because it's a manifestation of a shift
[00:49:57.900 --> 00:50:01.940]   at a company that had been producing world class,
[00:50:01.940 --> 00:50:05.780]   best in class hardware for a very long time.
[00:50:05.780 --> 00:50:10.300]   And I would argue that across, we can argue all day long
[00:50:10.300 --> 00:50:13.020]   about this, but some of the changes to the OS,
[00:50:13.020 --> 00:50:16.860]   some of the changes to something stupid like keynote,
[00:50:16.860 --> 00:50:22.020]   that for whatever reason, design is being,
[00:50:22.020 --> 00:50:26.580]   functionality is being sacrificed for design,
[00:50:26.580 --> 00:50:31.580]   across a lot of the interfaces on Apple's products.
[00:50:31.580 --> 00:50:36.220]   And I think that's a manifestation of an organizational
[00:50:36.220 --> 00:50:38.860]   problem internally within the company.
[00:50:38.860 --> 00:50:40.140]   Stuff should look good certainly,
[00:50:40.140 --> 00:50:41.540]   and we should all enjoy using it,
[00:50:41.540 --> 00:50:45.620]   but if the functionality suffers, then eventually,
[00:50:45.620 --> 00:50:48.540]   you wind up talking about it with Leo in front of a world
[00:50:48.540 --> 00:50:51.020]   in the world by talking, so who wants that?
[00:50:51.020 --> 00:50:52.100]   - I'm not your defend Apple.
[00:50:52.100 --> 00:50:53.540]   - I think it's not happening now,
[00:50:53.540 --> 00:50:57.540]   but we have selective memory, it's happened many,
[00:50:57.540 --> 00:51:00.180]   many, many times over the history of Apple.
[00:51:00.180 --> 00:51:02.260]   - Or Intel, remember Intel made a chip
[00:51:02.260 --> 00:51:04.660]   that couldn't do a floating point math at all.
[00:51:04.660 --> 00:51:07.740]   I mean, these things happen, I understand.
[00:51:07.740 --> 00:51:10.500]   That's a bug which Intel patched and fixed.
[00:51:10.500 --> 00:51:12.740]   I have to agree with Amy, this is more,
[00:51:12.740 --> 00:51:16.900]   yes, it's a bug, but it's a bug that exposes
[00:51:16.900 --> 00:51:18.980]   an underlying flaw in how they're thinking
[00:51:18.980 --> 00:51:20.620]   about what they're doing.
[00:51:20.620 --> 00:51:24.420]   It's not about functionality, it's not about function anymore,
[00:51:24.420 --> 00:51:25.420]   it's about form.
[00:51:25.420 --> 00:51:29.860]   It's the Johnny Ivization of technology,
[00:51:29.860 --> 00:51:32.580]   and I'm not sure I'm crazy about that.
[00:51:32.580 --> 00:51:33.660]   - Well, and this is one of the apps
[00:51:33.660 --> 00:51:34.980]   that's been around since the beginning,
[00:51:34.980 --> 00:51:39.100]   and I would also add that people who praised Apple
[00:51:39.100 --> 00:51:42.020]   for referencing Bronn Design on this calculator,
[00:51:42.020 --> 00:51:43.340]   so it's something that--
[00:51:43.340 --> 00:51:44.180]   - That's a good point.
[00:51:44.180 --> 00:51:47.020]   - You know, dare I say iconic, I've used a lot of--
[00:51:47.020 --> 00:51:49.300]   - No, but you're right, it looks like a Bronn, you're right.
[00:51:49.300 --> 00:51:50.980]   - I forgot about that, yeah.
[00:51:50.980 --> 00:51:53.620]   - So, but this is like one of the,
[00:51:53.620 --> 00:51:56.820]   I would say most iconic apps on the iPhone
[00:51:56.820 --> 00:51:58.060]   because it's been around for so long
[00:51:58.060 --> 00:52:02.740]   and because it uses this iconic, I guess, modern design.
[00:52:02.740 --> 00:52:05.860]   And so it's just a little bit weird to see a flaw
[00:52:05.860 --> 00:52:08.900]   or a blemish, of course, anywhere on an iPhone,
[00:52:08.900 --> 00:52:12.020]   but especially in something that has existed for so long
[00:52:12.020 --> 00:52:16.220]   and that people have actually championed for a few years.
[00:52:16.220 --> 00:52:18.980]   So, I don't know, yeah, it's definitely unusual
[00:52:18.980 --> 00:52:20.860]   to see things like that, and I would say,
[00:52:20.860 --> 00:52:23.180]   I noticed more of them happening on iOS 11
[00:52:23.180 --> 00:52:26.700]   than I have in years past, so, you know,
[00:52:26.700 --> 00:52:29.540]   ever since I've updated, my phone's been just acting
[00:52:29.540 --> 00:52:30.380]   really buggling.
[00:52:30.380 --> 00:52:34.620]   - Yeah, there are lots of cosmetic errors in 11, I think.
[00:52:34.620 --> 00:52:37.420]   - Yeah, exactly, there's just a lot of inconsistencies.
[00:52:37.420 --> 00:52:39.740]   And, you know, even on the 10,
[00:52:39.740 --> 00:52:43.300]   you'll see inconsistencies, you know,
[00:52:45.140 --> 00:52:47.180]   when you mess with that phone.
[00:52:47.180 --> 00:52:49.340]   - I just have to point out, a calculator's the kind of thing
[00:52:49.340 --> 00:52:52.460]   you write in computer 101 in high school.
[00:52:52.460 --> 00:52:55.220]   I mean, it's not a hard thing to get right.
[00:52:55.220 --> 00:52:57.460]   It's just a weird thing to get wrong.
[00:52:57.460 --> 00:52:59.820]   - It also, it's not just that they got it wrong.
[00:52:59.820 --> 00:53:01.740]   I mean, the part that bugs me is that,
[00:53:01.740 --> 00:53:05.220]   was there no QA, like, was there no usability testing
[00:53:05.220 --> 00:53:06.300]   before that shipped?
[00:53:06.300 --> 00:53:08.820]   And if not, then that's a problem.
[00:53:08.820 --> 00:53:10.380]   But if there was, and somebody thought,
[00:53:10.380 --> 00:53:13.020]   well, but it looks cool, so, now--
[00:53:13.020 --> 00:53:14.140]   - Well, let's be fair.
[00:53:14.140 --> 00:53:14.980]   - Let's go. - Make it right out.
[00:53:14.980 --> 00:53:16.500]   - Come on, honestly, you didn't,
[00:53:16.500 --> 00:53:19.420]   do we need to go back to like, mobile me and--
[00:53:19.420 --> 00:53:22.780]   - No, this is current, Patrick, this is current.
[00:53:22.780 --> 00:53:24.540]   No, but that's my point.
[00:53:24.540 --> 00:53:27.780]   It was, they did have significant issues
[00:53:27.780 --> 00:53:30.700]   in their design and in their software,
[00:53:30.700 --> 00:53:33.340]   way before this, and they were much bigger.
[00:53:33.340 --> 00:53:35.060]   I mean, mobile me was a disaster.
[00:53:35.060 --> 00:53:37.060]   iTunes has loaded forever.
[00:53:37.060 --> 00:53:40.100]   We're talking about these now, as if it was new.
[00:53:40.100 --> 00:53:42.180]   They've always had significant issues.
[00:53:42.180 --> 00:53:45.620]   - Oh, okay, so your position as Apple's always sucked.
[00:53:45.620 --> 00:53:48.540]   - My position is that they've never been perfect.
[00:53:48.540 --> 00:53:49.700]   - Oh, good. - And that all of a sudden,
[00:53:49.700 --> 00:53:52.020]   now, just using it. - Just because, you know.
[00:53:52.020 --> 00:53:53.180]   (laughing)
[00:53:53.180 --> 00:53:57.180]   But I don't, I'm not seeing, I mean,
[00:53:57.180 --> 00:54:00.180]   maybe we would need a scientific study on this,
[00:54:00.180 --> 00:54:04.820]   but I don't think I'm seeing more significant issues
[00:54:04.820 --> 00:54:08.540]   in the way Apple designs its software, or even hardware,
[00:54:08.540 --> 00:54:13.540]   then there were 10 years ago, or five years ago,
[00:54:13.540 --> 00:54:16.540]   or 20 years ago, and I, you know,
[00:54:16.540 --> 00:54:20.180]   we could lay out an entire list of gigantic blunders
[00:54:20.180 --> 00:54:23.180]   that either got fixed or they stuck to those,
[00:54:23.180 --> 00:54:26.420]   to their guns in all of those years.
[00:54:26.420 --> 00:54:29.380]   So, yeah, I'm skeptical about,
[00:54:29.380 --> 00:54:32.020]   and maybe I've gotten used to being skeptical about,
[00:54:32.020 --> 00:54:34.820]   oh, now Apple's going down the drain because this or that.
[00:54:34.820 --> 00:54:37.220]   So maybe it's an automatic response for me,
[00:54:37.220 --> 00:54:39.380]   but I'm still, you know,
[00:54:39.380 --> 00:54:42.180]   not sure that it's worse now than it's been before.
[00:54:42.180 --> 00:54:45.300]   - Well, Mobile Me was a big, huge, gigantic blunder.
[00:54:45.300 --> 00:54:49.660]   I guess what I have noticed is that,
[00:54:49.660 --> 00:54:51.580]   the problems are more insidious.
[00:54:51.580 --> 00:54:56.580]   They're more sort of less of a massive catastrophe,
[00:54:56.580 --> 00:55:01.700]   if we put Mobile Me in the massive catastrophe realm.
[00:55:01.700 --> 00:55:05.420]   More sort of like sprinkled throughout all their products.
[00:55:05.420 --> 00:55:08.580]   And it just strikes me that design is being
[00:55:08.580 --> 00:55:15.100]   preferenced over basic functionality.
[00:55:15.100 --> 00:55:19.740]   And it makes me think that is anybody,
[00:55:19.740 --> 00:55:22.780]   you know, Apple used to be the user-centered design,
[00:55:22.780 --> 00:55:25.980]   the human-centered design, you know, capital, right?
[00:55:25.980 --> 00:55:28.420]   It makes me wonder if humans are still at the center
[00:55:28.420 --> 00:55:31.740]   of their, are still the center of gravity for them.
[00:55:31.740 --> 00:55:34.540]   And the way that people use their devices.
[00:55:34.540 --> 00:55:36.620]   They're designing for the sake of design
[00:55:36.620 --> 00:55:39.980]   and not to make it useful for users anymore.
[00:55:39.980 --> 00:55:40.820]   - Maybe.
[00:55:40.820 --> 00:55:45.580]   Maybe, you know, I would say that there have been design
[00:55:45.580 --> 00:55:48.700]   choices that certainly make the application look crisper
[00:55:48.700 --> 00:55:50.900]   and I'll go back again to keynote.
[00:55:50.900 --> 00:55:55.020]   The new UI on the keynote that shipped with High Sierra
[00:55:55.020 --> 00:55:58.540]   certainly looks good, but a lot of,
[00:55:58.540 --> 00:56:01.540]   but it's also not intuitive in any way.
[00:56:01.540 --> 00:56:03.260]   And a lot of the core functionality
[00:56:03.260 --> 00:56:05.980]   and some of the key transitions, just basic things
[00:56:05.980 --> 00:56:07.900]   either got moved or removed.
[00:56:07.900 --> 00:56:11.700]   And so one has to wonder, you know,
[00:56:11.700 --> 00:56:16.500]   why sacrifice basic usability in exchange for looks?
[00:56:16.500 --> 00:56:18.580]   Or, you know, was that a conscious decision?
[00:56:18.580 --> 00:56:21.500]   Or is Apple maybe got folks over in the design shop
[00:56:21.500 --> 00:56:25.060]   that just have more leverage and pull over the usability
[00:56:25.060 --> 00:56:27.500]   people who hopefully are testing products
[00:56:27.500 --> 00:56:28.860]   before they go anywhere?
[00:56:28.860 --> 00:56:29.700]   I don't know.
[00:56:29.700 --> 00:56:31.260]   - That would be a weird circumstance.
[00:56:31.260 --> 00:56:35.460]   If the testors said, hey, came back to the designers
[00:56:35.460 --> 00:56:36.820]   and said, hey, you see this problem?
[00:56:36.820 --> 00:56:37.820]   And they said, yeah, that's fine.
[00:56:37.820 --> 00:56:40.980]   We don't, we don't mind 'cause it looks good.
[00:56:40.980 --> 00:56:44.140]   - Okay, so then what else is to explain the calculator?
[00:56:44.140 --> 00:56:44.980]   - Right.
[00:56:44.980 --> 00:56:49.380]   I mean, either they have bad QA or user interface beats QA,
[00:56:49.380 --> 00:56:52.140]   Trump's QA, either way, it's not good.
[00:56:52.140 --> 00:56:55.260]   - They're curious to see once the new 10 is out,
[00:56:55.260 --> 00:56:57.340]   you know, what interesting, like,
[00:56:57.340 --> 00:56:59.580]   will that be a trend that continues?
[00:56:59.580 --> 00:57:00.420]   I'm curious.
[00:57:00.420 --> 00:57:01.900]   - I won't know 'cause I didn't buy one.
[00:57:01.900 --> 00:57:02.740]   - Yeah.
[00:57:02.740 --> 00:57:05.060]   - So let's, to make it fair,
[00:57:05.060 --> 00:57:06.620]   Apple's not the only one having trouble
[00:57:06.620 --> 00:57:09.380]   with their flagship products.
[00:57:09.380 --> 00:57:10.980]   This is the tweet that started at all
[00:57:10.980 --> 00:57:13.380]   from Alex Dobie at Android Central.
[00:57:13.380 --> 00:57:15.740]   He showed a picture of his Pixel 2 XL
[00:57:15.740 --> 00:57:17.060]   after seven days of use.
[00:57:17.060 --> 00:57:19.540]   And while you have to kind of look hard,
[00:57:19.540 --> 00:57:24.540]   there is burn-in of the back home and recents buttons.
[00:57:24.540 --> 00:57:27.460]   At the bottom, you see the dock as well,
[00:57:27.460 --> 00:57:29.500]   burned into the screen.
[00:57:29.500 --> 00:57:31.620]   And I've talked to many people now.
[00:57:31.620 --> 00:57:33.380]   I talked to somebody yesterday
[00:57:33.380 --> 00:57:36.140]   who got that on his brand new Pixel 2.
[00:57:36.140 --> 00:57:38.420]   He took it out of the box and he saw it.
[00:57:38.420 --> 00:57:40.020]   Google is taking this seriously.
[00:57:40.020 --> 00:57:44.340]   People have contacted Google's support.
[00:57:44.340 --> 00:57:45.900]   The guy I talked to yesterday,
[00:57:45.900 --> 00:57:49.820]   Google was willing to trade in the Pixel 2 for another one,
[00:57:49.820 --> 00:57:51.980]   and yet another one after other problems.
[00:57:51.980 --> 00:57:56.980]   Here's the response from Google and Mario Queroz,
[00:57:57.700 --> 00:57:59.660]   who is their VP product management.
[00:57:59.660 --> 00:58:01.220]   I think it was kind of,
[00:58:01.220 --> 00:58:04.260]   I want you guys to parse this a little bit on me.
[00:58:04.260 --> 00:58:08.420]   First of all, there are two issues with the Pixel 2 XL.
[00:58:08.420 --> 00:58:12.980]   Actually, this one's the Pixel 2 as well as color accuracy.
[00:58:12.980 --> 00:58:15.540]   A number of people, including, I think,
[00:58:15.540 --> 00:58:18.500]   Marcus Brownlee have complained that the color accuracy,
[00:58:18.500 --> 00:58:21.340]   the Pixel seems desaturated a little bit.
[00:58:21.340 --> 00:58:22.580]   Low quality.
[00:58:22.580 --> 00:58:24.780]   Google's response to that is, oh no, well,
[00:58:24.780 --> 00:58:27.020]   that's not a problem because you're just used
[00:58:27.020 --> 00:58:30.180]   to over-bright displays like, say, the Samsung.
[00:58:30.180 --> 00:58:33.740]   And what we're doing is we're making a better-looking,
[00:58:33.740 --> 00:58:36.660]   more accurate display, and so your eyes are conditioned
[00:58:36.660 --> 00:58:39.180]   to these super-bright displays.
[00:58:39.180 --> 00:58:40.180]   You'll get used to it.
[00:58:40.180 --> 00:58:41.780]   This is a high quality,
[00:58:41.780 --> 00:58:44.660]   but then I've also seen posts from color experts who say,
[00:58:44.660 --> 00:58:48.940]   no, no, I tested this phone with color measurement hardware.
[00:58:48.940 --> 00:58:50.340]   It's inaccurate.
[00:58:50.340 --> 00:58:52.540]   So I'm thinking that some of the early,
[00:58:52.540 --> 00:58:55.140]   at least Pixel 2s that came out,
[00:58:55.140 --> 00:58:58.660]   were not calibrated at the factory as they're supposed to be.
[00:58:58.660 --> 00:58:59.820]   That one you can fix.
[00:58:59.820 --> 00:59:00.660]   That one's not the end of the world.
[00:59:00.660 --> 00:59:04.300]   Is that something you can calibrate after the fact or no?
[00:59:04.300 --> 00:59:05.100]   Oh yeah, absolutely.
[00:59:05.100 --> 00:59:06.060]   You can get a color profile.
[00:59:06.060 --> 00:59:08.900]   It's hard to calibrate it accurately without hardware.
[00:59:08.900 --> 00:59:10.860]   At the factory, you have the hardware.
[00:59:10.860 --> 00:59:11.820]   It's all automated.
[00:59:11.820 --> 00:59:15.140]   You put a little screen sucker on, push some buttons.
[00:59:15.140 --> 00:59:16.180]   It makes a profile.
[00:59:16.180 --> 00:59:17.660]   You put it in the software.
[00:59:17.660 --> 00:59:19.140]   The color will be accurate from then on.
[00:59:19.140 --> 00:59:21.540]   It'll be SRGB, which is what Google intends.
[00:59:21.540 --> 00:59:22.700]   After the fact, you're right.
[00:59:22.700 --> 00:59:25.020]   It's a little more tricky because you don't have that hardware,
[00:59:25.020 --> 00:59:26.020]   but you can.
[00:59:26.020 --> 00:59:29.100]   I mean, it depends on how consistent these screens are.
[00:59:29.100 --> 00:59:31.300]   And if they're fairly consistent,
[00:59:31.300 --> 00:59:34.420]   you can at least push out a more accurate color profile.
[00:59:34.420 --> 00:59:36.860]   That's what, by the way, most companies do.
[00:59:36.860 --> 00:59:38.660]   Windows doesn't have color management,
[00:59:38.660 --> 00:59:42.180]   but companies like Apple that do.
[00:59:42.180 --> 00:59:47.180]   I don't know if Apple on Mac OS has custom calibrated
[00:59:47.180 --> 00:59:47.940]   each display.
[00:59:47.940 --> 00:59:50.340]   It gives you a generic, I believe,
[00:59:50.340 --> 00:59:53.860]   a generic color calibration that's pretty accurate
[00:59:53.860 --> 00:59:54.780]   for its displays.
[00:59:54.780 --> 00:59:57.820]   And then, you know, if you want to go further, you can.
[00:59:57.820 --> 00:59:59.740]   That's less of an issue, though.
[00:59:59.740 --> 01:00:02.860]   Then the biggest issue, this burn-in,
[01:00:02.860 --> 01:00:07.860]   which Google calls not burn-in, but differential aging.
[01:00:07.860 --> 01:00:09.820]   (laughs)
[01:00:09.820 --> 01:00:11.380]   That's what you're seeing here on the panel.
[01:00:11.380 --> 01:00:12.500]   Differential aging.
[01:00:12.500 --> 01:00:15.260]   I'm aging a lot faster than you guys are.
[01:00:15.260 --> 01:00:16.420]   Differential aging.
[01:00:16.420 --> 01:00:17.820]   Now, I understand what that means
[01:00:17.820 --> 01:00:19.020]   'cause it's an OLED screen.
[01:00:19.020 --> 01:00:21.900]   And so, the idea is that some of the pixels
[01:00:21.900 --> 01:00:25.780]   that are burning in because they're wearing out
[01:00:25.780 --> 01:00:28.460]   faster than other pixels.
[01:00:28.460 --> 01:00:29.660]   But after a week?
[01:00:29.660 --> 01:00:31.820]   But after a week or no time at all,
[01:00:31.820 --> 01:00:33.820]   that seems a little premature.
[01:00:33.820 --> 01:00:36.100]   Mario says our current investigation of burn-in,
[01:00:36.100 --> 01:00:38.300]   which started as soon as we received the first user report
[01:00:38.300 --> 01:00:42.660]   on October 22nd, confirms that the differential aging
[01:00:42.660 --> 01:00:45.620]   is in line with that of other premium smartphones.
[01:00:45.620 --> 01:00:47.700]   So that's defense one.
[01:00:47.700 --> 01:00:50.300]   We're no worse than the other guys.
[01:00:50.300 --> 01:00:51.300]   Not true.
[01:00:51.300 --> 01:00:53.780]   And should not affect the normal day-to-day user experience
[01:00:53.780 --> 01:00:56.220]   of the Pixel 2 XL, well, that's kind of subjective.
[01:00:56.220 --> 01:01:00.140]   I think some users might say, yeah, I don't want that.
[01:01:00.140 --> 01:01:02.300]   I paid a lot of money for this phone.
[01:01:02.300 --> 01:01:05.460]   I think most users would probably say that they don't want that.
[01:01:05.460 --> 01:01:06.500]   Yeah, so this is--
[01:01:06.500 --> 01:01:07.500]   Okay, but to be fair, you only--
[01:01:07.500 --> 01:01:09.620]   This is surprisingly disingenuous.
[01:01:09.620 --> 01:01:13.780]   But you only see the sort of ghost marks
[01:01:13.780 --> 01:01:16.540]   against a solid grace screen, right?
[01:01:16.540 --> 01:01:19.500]   Yeah, no, you see it on a white screen.
[01:01:19.500 --> 01:01:20.940]   I think really where it would bother people
[01:01:20.940 --> 01:01:24.460]   is if you're watching videos and you start to see that there.
[01:01:24.460 --> 01:01:25.300]   I don't know.
[01:01:25.300 --> 01:01:26.540]   I don't have mine yet.
[01:01:26.540 --> 01:01:28.540]   Of course, that's the first thing I'm going to look at.
[01:01:28.540 --> 01:01:31.700]   The question is, is this a hardware flaw or--
[01:01:31.700 --> 01:01:33.380]   Well, I guess it is a hardware flaw,
[01:01:33.380 --> 01:01:35.500]   but can it be fixed in software?
[01:01:35.500 --> 01:01:38.660]   They didn't use Samsung screens for the Pixel 2 XL.
[01:01:38.660 --> 01:01:41.260]   They used LG screens.
[01:01:41.260 --> 01:01:43.900]   Some of the complaints about the screen color
[01:01:43.900 --> 01:01:46.820]   and the graininess also apply to LG's V30,
[01:01:46.820 --> 01:01:49.140]   which uses the same panel.
[01:01:49.140 --> 01:01:51.940]   I've been told-- I haven't verified this with Samsung,
[01:01:51.940 --> 01:01:55.780]   but I'm told Samsung in its software on its OLED screens
[01:01:55.780 --> 01:02:00.820]   nudges the-- always on screen controls a pixel left
[01:02:00.820 --> 01:02:03.380]   and right periodically to jitters them a little bit
[01:02:03.380 --> 01:02:05.180]   to keep them from burning in.
[01:02:05.180 --> 01:02:07.260]   That's perhaps something Google could do.
[01:02:07.260 --> 01:02:10.500]   Google's response to this was to double their warranty
[01:02:10.500 --> 01:02:12.660]   to two years from one year.
[01:02:12.660 --> 01:02:17.060]   But if they can't fix the phone, that's not much good either.
[01:02:17.060 --> 01:02:26.660]   They say that they're going to, I guess, maybe fix this somehow
[01:02:26.660 --> 01:02:28.980]   with software or--
[01:02:28.980 --> 01:02:31.500]   it says, we use software to safeguard the user experience
[01:02:31.500 --> 01:02:33.220]   and maximize the life of the OLED display
[01:02:33.220 --> 01:02:37.060]   and will make ongoing software updates to optimize further,
[01:02:37.060 --> 01:02:38.620]   unknown whether they can fix this.
[01:02:38.620 --> 01:02:41.620]   That's going to be a big issue going forward
[01:02:41.620 --> 01:02:43.300]   to the Pixel 2 XL.
[01:02:43.300 --> 01:02:47.700]   But the dock at the bottom is-- it's not always on, right?
[01:02:47.700 --> 01:02:49.500]   So it functions like--
[01:02:49.500 --> 01:02:51.460]   It's only on when you're on the lock screen.
[01:02:51.460 --> 01:02:54.060]   Yeah, it's only on when you're on the lock screen.
[01:02:54.060 --> 01:02:57.540]   So I mean, it's strange then that it would happen
[01:02:57.540 --> 01:02:59.740]   that the burn would happen that quickly, right?
[01:02:59.740 --> 01:03:01.580]   Yeah.
[01:03:01.580 --> 01:03:03.140]   I don't know.
[01:03:03.140 --> 01:03:05.140]   I agree with the sentiment.
[01:03:05.140 --> 01:03:07.940]   Because so my-- I love my Samsung,
[01:03:07.940 --> 01:03:10.740]   but this screen is way too bright.
[01:03:10.740 --> 01:03:14.540]   And even at the dimest setting, it's bright.
[01:03:14.540 --> 01:03:16.180]   It doesn't need to be as bright as it is.
[01:03:16.180 --> 01:03:19.900]   You know that you can go in and change the screen mode, right?
[01:03:19.900 --> 01:03:20.500]   I do.
[01:03:20.500 --> 01:03:23.380]   They offer some different--
[01:03:23.380 --> 01:03:27.100]   the cinema is a considerably more muted look.
[01:03:27.100 --> 01:03:29.460]   But even that is too--
[01:03:29.460 --> 01:03:30.500]   It's pretty bright.
[01:03:30.500 --> 01:03:33.180]   And mostly I noticed that at nighttime,
[01:03:33.180 --> 01:03:35.060]   if I'm looking at it in bed right before--
[01:03:35.060 --> 01:03:37.020]   Do you use the night shift that turn on me?
[01:03:37.020 --> 01:03:37.500]   I do.
[01:03:37.500 --> 01:03:39.620]   Yeah, it's still too bright.
[01:03:39.620 --> 01:03:43.060]   Apple's night shift, their version of night shift
[01:03:43.060 --> 01:03:44.700]   is much more--
[01:03:44.700 --> 01:03:45.780]   or I think it's better.
[01:03:45.780 --> 01:03:47.020]   It blocks a lot more of that light.
[01:03:47.020 --> 01:03:49.060]   It's very orange.
[01:03:49.060 --> 01:03:50.980]   So to that point, I think they're right.
[01:03:50.980 --> 01:03:53.340]   It is brighter than it needs to be.
[01:03:53.340 --> 01:03:54.260]   But that's weird.
[01:03:54.260 --> 01:03:55.180]   I don't know what would cost--
[01:03:55.180 --> 01:03:56.620]   That sells phones, by the way.
[01:03:56.620 --> 01:03:58.700]   You know, I mean, that's the same thing with TVs.
[01:03:58.700 --> 01:04:01.140]   You go in a showroom and see a TV.
[01:04:01.140 --> 01:04:04.580]   In fact, when I went to see the Pixel 2 at the Verizon store
[01:04:04.580 --> 01:04:06.980]   because they have them on display,
[01:04:06.980 --> 01:04:09.540]   I noted that both displays were turned up all the way.
[01:04:09.540 --> 01:04:10.660]   Because that sells the phone.
[01:04:10.660 --> 01:04:12.380]   And of course, the first thing you do when you get home
[01:04:12.380 --> 01:04:13.780]   is you turn it down.
[01:04:13.780 --> 01:04:15.420]   Turn it down.
[01:04:15.420 --> 01:04:17.740]   Yeah, I mean, I think that the Pixel 2--
[01:04:17.740 --> 01:04:20.580]   I want to like the Pixel phones more than I have.
[01:04:20.580 --> 01:04:22.100]   So this is the second year where I've just
[01:04:22.100 --> 01:04:27.340]   been a little disappointed with the rate of innovation
[01:04:27.340 --> 01:04:28.460]   on the phone.
[01:04:28.460 --> 01:04:31.780]   And also, hey, can you hear me?
[01:04:31.780 --> 01:04:34.540]   Yeah, your video is frozen.
[01:04:34.540 --> 01:04:35.060]   I think so.
[01:04:35.060 --> 01:04:35.460]   Oh, there we go.
[01:04:35.460 --> 01:04:35.940]   OK.
[01:04:35.940 --> 01:04:36.460]   That's OK.
[01:04:36.460 --> 01:04:37.020]   I don't mind that.
[01:04:37.020 --> 01:04:37.700]   I apologize.
[01:04:37.700 --> 01:04:41.700]   But the point being that these phones have been hyped a lot,
[01:04:41.700 --> 01:04:43.620]   but in fact, their market share is tiny.
[01:04:43.620 --> 01:04:49.540]   I think they own less than 1% of the market share in the US
[01:04:49.540 --> 01:04:51.740]   and probably less globally.
[01:04:51.740 --> 01:04:56.300]   And so, yeah, I mean, I think that it's time
[01:04:56.300 --> 01:05:00.860]   to start considering whether the Google Pixel is a failure
[01:05:00.860 --> 01:05:02.980]   or just a disappointment.
[01:05:02.980 --> 01:05:06.020]   But frankly, it's not selling enough phones
[01:05:06.020 --> 01:05:09.380]   to compete with both Samsung and Apple.
[01:05:09.380 --> 01:05:11.540]   I think they're getting started, though.
[01:05:11.540 --> 01:05:15.300]   I mean, that press release or that article, whatever it is,
[01:05:15.300 --> 01:05:16.980]   is very strangely worded.
[01:05:16.980 --> 01:05:21.500]   Honestly, it seems like my interpretation
[01:05:21.500 --> 01:05:24.540]   would be there are only a few of those that
[01:05:24.540 --> 01:05:28.540]   have been highly publicized because of the--
[01:05:28.540 --> 01:05:30.580]   they were distributed to people in the press.
[01:05:30.580 --> 01:05:33.580]   That's what I'm hoping for them.
[01:05:33.580 --> 01:05:37.540]   And they will make it go away by replacing them.
[01:05:37.540 --> 01:05:39.700]   And that's going to be it, hopefully for them.
[01:05:39.700 --> 01:05:42.260]   But regarding their success, I think
[01:05:42.260 --> 01:05:45.100]   they're going very slowly at first
[01:05:45.100 --> 01:05:48.300]   because they're all in.
[01:05:48.300 --> 01:05:51.700]   They're not selling it in France, for example,
[01:05:51.700 --> 01:05:54.300]   when they did sell the Nexus in France.
[01:05:54.300 --> 01:05:54.860]   Interesting.
[01:05:54.860 --> 01:06:00.380]   But they're-- because they're doing all of it themselves.
[01:06:00.380 --> 01:06:03.340]   I think they're building teams, building relationships,
[01:06:03.340 --> 01:06:06.060]   and they don't launch it internationally
[01:06:06.060 --> 01:06:09.740]   or more internationally if they don't have the proper teams
[01:06:09.740 --> 01:06:14.260]   because they are now a phone selling company as well.
[01:06:14.260 --> 01:06:16.900]   So I think they're playing the long game here.
[01:06:16.900 --> 01:06:20.940]   And I wouldn't judge their performance
[01:06:20.940 --> 01:06:23.700]   on the first couple of years because they've never really
[01:06:23.700 --> 01:06:26.140]   done this themselves before.
[01:06:26.140 --> 01:06:27.420]   They're developing chips.
[01:06:27.420 --> 01:06:33.620]   They're designing head and the phones themselves.
[01:06:33.620 --> 01:06:36.340]   And I think it's unfair as a user
[01:06:36.340 --> 01:06:39.220]   to choose your phone based on sales.
[01:06:39.220 --> 01:06:41.940]   I mean, from a business point of view, I understand--
[01:06:41.940 --> 01:06:44.100]   I'm recommending the current iteration of the Pixel
[01:06:44.100 --> 01:06:47.180]   against something like any of the new iPhones
[01:06:47.180 --> 01:06:49.380]   or something like the Samsung Galaxy Note 8.
[01:06:49.380 --> 01:06:52.340]   I'll tell you why I recommended against other Android phones.
[01:06:52.340 --> 01:06:53.140]   Here's the Note 8.
[01:06:53.140 --> 01:06:55.380]   This has got the last patch on here
[01:06:55.380 --> 01:06:58.020]   is the August Google Security patch.
[01:06:58.020 --> 01:07:00.860]   On a Pixel phone, it would be September and it will soon--
[01:07:00.860 --> 01:07:02.580]   I mean, it'd be October and I'll soon be November.
[01:07:02.580 --> 01:07:05.740]   In other words, this is three months behind.
[01:07:05.740 --> 01:07:10.420]   For security points of view, that's scary in Android land.
[01:07:10.420 --> 01:07:11.020]   That's right.
[01:07:11.020 --> 01:07:13.220]   And Samsung also comes with a bunch of bloatware
[01:07:13.220 --> 01:07:15.020]   that you don't need.
[01:07:15.020 --> 01:07:15.860]   I will say this.
[01:07:15.860 --> 01:07:17.740]   All of this sort of begs the question
[01:07:17.740 --> 01:07:21.460]   because Google has another phone project called Aura.
[01:07:21.460 --> 01:07:22.660]   And Aura is a--
[01:07:22.660 --> 01:07:24.060]   Which was dropped, by the way.
[01:07:24.060 --> 01:07:25.300]   Which was dead, yeah.
[01:07:25.300 --> 01:07:26.340]   I don't think it's totally dead.
[01:07:26.340 --> 01:07:29.220]   You think they're still doing it?
[01:07:29.220 --> 01:07:32.460]   I think they were trying to launch something in the Philippines,
[01:07:32.460 --> 01:07:33.660]   like a test market.
[01:07:33.660 --> 01:07:34.460]   No, no.
[01:07:34.460 --> 01:07:36.660]   After that, they killed it.
[01:07:36.660 --> 01:07:37.860]   That was a failure.
[01:07:37.860 --> 01:07:39.700]   In September, they killed it.
[01:07:39.700 --> 01:07:43.060]   And from my vantage point, it was a pretty smart idea.
[01:07:43.060 --> 01:07:45.420]   Motorola picked up on at least part of it.
[01:07:45.420 --> 01:07:47.420]   So for those who aren't familiar with it,
[01:07:47.420 --> 01:07:49.100]   it was a modular based phone.
[01:07:49.100 --> 01:07:52.540]   So just like the old old school computers,
[01:07:52.540 --> 01:07:54.220]   you would buy the sort of basic unit
[01:07:54.220 --> 01:07:57.020]   and then add different components to snap in and out
[01:07:57.020 --> 01:07:59.580]   so that you weren't having to recycle and renew your phone
[01:07:59.580 --> 01:08:02.020]   every couple of years.
[01:08:02.020 --> 01:08:04.820]   But that you could add in exchange components
[01:08:04.820 --> 01:08:05.980]   and upgrade the phone that way.
[01:08:05.980 --> 01:08:08.460]   Which, again, seems like a really smart idea.
[01:08:08.460 --> 01:08:17.380]   So the question is, if you take a sort of 40,000 foot view,
[01:08:17.380 --> 01:08:19.500]   can Google be a hardware company?
[01:08:19.500 --> 01:08:22.100]   And I don't have the answer to that.
[01:08:22.100 --> 01:08:25.740]   It certainly looks as though they are trying to firmly establish
[01:08:25.740 --> 01:08:28.780]   themselves as a manufacturer of stuff.
[01:08:28.780 --> 01:08:33.140]   What do they spend $2 billion to acquire the HTC Brain Trust?
[01:08:33.140 --> 01:08:33.860]   So the question is--
[01:08:33.860 --> 01:08:35.860]   It's a significant investment.
[01:08:35.860 --> 01:08:36.420]   Yeah.
[01:08:36.420 --> 01:08:39.260]   And are they set up to be that kind of--
[01:08:39.260 --> 01:08:40.380]   Can I do it?
[01:08:40.380 --> 01:08:40.880]   Yeah.
[01:08:40.880 --> 01:08:42.860]   Are Google's faced exactly the same problem
[01:08:42.860 --> 01:08:44.900]   Apples faced with, which is they
[01:08:44.900 --> 01:08:48.860]   have a singularly successful product.
[01:08:48.860 --> 01:08:51.300]   In the case of Google, it's search and advertising
[01:08:51.300 --> 01:08:52.860]   based on search.
[01:08:52.860 --> 01:08:57.700]   And they hope to have a follow up because they know that nothing
[01:08:57.700 --> 01:09:00.460]   lasts forever, just like Apple with the iPhone.
[01:09:00.460 --> 01:09:02.180]   And that's why they've got Waymo.
[01:09:02.180 --> 01:09:05.980]   That's why they've got hardware.
[01:09:05.980 --> 01:09:07.900]   It's very hard to enter the hardware space.
[01:09:07.900 --> 01:09:09.260]   Look at Microsoft's challenges.
[01:09:09.260 --> 01:09:10.780]   Microsoft had made hardware all along,
[01:09:10.780 --> 01:09:12.140]   but they made their first computers,
[01:09:12.140 --> 01:09:15.700]   and they had all sorts of problems in the first year
[01:09:15.700 --> 01:09:17.220]   with the surface.
[01:09:17.220 --> 01:09:18.740]   It's hard to do.
[01:09:18.740 --> 01:09:19.700]   You think the--
[01:09:19.700 --> 01:09:22.340]   Now, I see them intrigued because I looked at the ARRA,
[01:09:22.340 --> 01:09:25.340]   and all I saw was, oh, that's going to be janky.
[01:09:25.340 --> 01:09:28.380]   It's going to be rattly and loose.
[01:09:28.380 --> 01:09:31.900]   And I think most end users said, I don't want to build my phone.
[01:09:31.900 --> 01:09:33.380]   I just want to buy it.
[01:09:33.380 --> 01:09:34.900]   Well, let's find a phone.
[01:09:34.900 --> 01:09:38.300]   The market has gone in the exact opposite direction.
[01:09:38.300 --> 01:09:42.060]   A lot of these phones are actually harder to open up and repair
[01:09:42.060 --> 01:09:44.140]   and replace the smartphone than ever before.
[01:09:44.140 --> 01:09:48.060]   So even the Samsungs, which famously had--
[01:09:48.060 --> 01:09:49.060]   Removable batteries.
[01:09:49.060 --> 01:09:52.180]   Remiss those replaceable batteries.
[01:09:52.180 --> 01:09:52.780]   Yeah, exactly.
[01:09:52.780 --> 01:09:54.980]   Now, they're completely closed systems.
[01:09:54.980 --> 01:09:59.180]   So in the case of Google, Samsung, Apple, and probably
[01:09:59.180 --> 01:10:01.900]   others that I'm forgetting, you have a phone-- oh,
[01:10:01.900 --> 01:10:02.940]   the essential phone.
[01:10:02.940 --> 01:10:04.220]   These are phones that are actually
[01:10:04.220 --> 01:10:08.540]   pretty difficult to open up and mess around with.
[01:10:08.540 --> 01:10:11.140]   And so although I loved the idea of Project ARRA
[01:10:11.140 --> 01:10:15.100]   because in theory, it eliminates E-Waste.
[01:10:15.100 --> 01:10:19.340]   It makes it easier to keep your same device over time.
[01:10:19.340 --> 01:10:25.260]   I've been really amazed at how willing people have been able to just
[01:10:25.260 --> 01:10:27.340]   buy a new phone every year.
[01:10:27.340 --> 01:10:29.700]   And now it's going to be even more interesting with them costing
[01:10:29.700 --> 01:10:32.780]   as much as they do with the Galaxy Note 8 reaching up
[01:10:32.780 --> 01:10:37.140]   into the $1,000 price range and then also with the iPhone 10.
[01:10:37.140 --> 01:10:37.660]   So--
[01:10:37.660 --> 01:10:41.220]   That doesn't have to do the technology only.
[01:10:41.220 --> 01:10:43.020]   I mean, the reason that people are habituated
[01:10:43.020 --> 01:10:47.940]   to buying new phones every two years is because it is literally--
[01:10:47.940 --> 01:10:50.420]   when I went to buy this phone, I wanted to pay cash for it
[01:10:50.420 --> 01:10:51.220]   and just buy it out.
[01:10:51.220 --> 01:10:54.740]   And I had to jump--
[01:10:54.740 --> 01:10:56.420]   my carrier is AT&T.
[01:10:56.420 --> 01:10:56.940]   I had to jump--
[01:10:56.940 --> 01:10:59.860]   Would you ever want to do that, Amy?
[01:10:59.860 --> 01:11:00.260]   Well, yeah.
[01:11:00.260 --> 01:11:04.140]   But the business models are predicated
[01:11:04.140 --> 01:11:09.500]   on at least in the United States throwing out
[01:11:09.500 --> 01:11:11.900]   perfectly good phones.
[01:11:11.900 --> 01:11:12.900]   It's shut up.
[01:11:12.900 --> 01:11:13.660]   It's a couple of years.
[01:11:13.660 --> 01:11:14.940]   It's horrible.
[01:11:14.940 --> 01:11:15.780]   That's right.
[01:11:15.780 --> 01:11:22.620]   So the reason I would argue that the reason that a modular-based system
[01:11:22.620 --> 01:11:26.020]   hasn't yet taken off has a lot to do also
[01:11:26.020 --> 01:11:28.580]   with the way that the contracts are structured and the carriers--
[01:11:28.580 --> 01:11:29.420]   Oh.
[01:11:29.420 --> 01:11:31.940]   So it wasn't end users that didn't want it.
[01:11:31.940 --> 01:11:33.580]   It was the carriers that ended the idea.
[01:11:33.580 --> 01:11:34.340]   Never launched.
[01:11:34.340 --> 01:11:39.300]   Now, there is a Motorola version, similar kind of version out,
[01:11:39.300 --> 01:11:41.820]   that, again, it hasn't taken off.
[01:11:41.820 --> 01:11:43.860]   Now, is the reason that it hasn't taken off?
[01:11:43.860 --> 01:11:46.740]   Because people don't understand the value
[01:11:46.740 --> 01:11:50.060]   of being able to upgrade different parts of the phone
[01:11:50.060 --> 01:11:53.020]   and customize it as it sees fit.
[01:11:53.020 --> 01:11:55.100]   Or because the majority of people still
[01:11:55.100 --> 01:11:57.700]   go into stores to purchase their equipment.
[01:11:57.700 --> 01:11:59.900]   It's like, who killed the electric car?
[01:11:59.900 --> 01:12:00.420]   Yeah.
[01:12:00.420 --> 01:12:00.740]   Yeah, yeah.
[01:12:00.740 --> 01:12:00.940]   Total--
[01:12:00.940 --> 01:12:03.780]   The consumers wanted an electric car, but GM.
[01:12:03.780 --> 01:12:04.700]   That's a great analogy.
[01:12:04.700 --> 01:12:06.700]   Yeah, absolutely.
[01:12:06.700 --> 01:12:07.900]   So I'm going to--
[01:12:07.900 --> 01:12:11.220]   I guess I'm going to try and disagree with every--
[01:12:11.220 --> 01:12:13.820]   You know, all three of us--
[01:12:13.820 --> 01:12:14.780]   --are bringing forth.
[01:12:14.780 --> 01:12:15.300]   Patrick.
[01:12:15.300 --> 01:12:17.300]   Amy.
[01:12:17.300 --> 01:12:19.620]   I think the real reason--
[01:12:19.620 --> 01:12:21.900]   they're making really interesting--
[01:12:21.900 --> 01:12:24.900]   finding interesting solutions to that perceived
[01:12:24.900 --> 01:12:26.860]   jankiness of a modular phone.
[01:12:26.860 --> 01:12:29.140]   They had these super strong magnets
[01:12:29.140 --> 01:12:32.580]   and a bunch of things that were really clever on Project
[01:12:32.580 --> 01:12:33.380]   Era.
[01:12:33.380 --> 01:12:35.580]   But I think the real issue is that there are
[01:12:35.580 --> 01:12:38.660]   solvium problems that people don't want solutions to.
[01:12:38.660 --> 01:12:39.580]   They don't care.
[01:12:39.580 --> 01:12:44.260]   We have a really great parallel with computers nowadays.
[01:12:44.260 --> 01:12:47.620]   You could make all of these same arguments for PCs.
[01:12:47.620 --> 01:12:51.780]   And no one wants a desktop where you can swap parts.
[01:12:51.780 --> 01:12:53.580]   It's commoditized.
[01:12:53.580 --> 01:12:57.060]   And everyone wants a laptop.
[01:12:57.060 --> 01:12:59.220]   And nobody got time to customize their phone.
[01:12:59.220 --> 01:12:59.940]   Yeah, exactly.
[01:12:59.940 --> 01:13:00.460]   Exactly.
[01:13:00.460 --> 01:13:02.780]   And they're doing one of the reasons why people are so
[01:13:02.780 --> 01:13:05.740]   attracts the iPhone versus Android.
[01:13:05.740 --> 01:13:08.060]   And also, if you just look at modular gadgets
[01:13:08.060 --> 01:13:08.860]   across the board--
[01:13:08.860 --> 01:13:10.980]   I mean, they've never truly caught on
[01:13:10.980 --> 01:13:15.540]   because consumers like things to be simple.
[01:13:15.540 --> 01:13:19.220]   They value simplicity over essentially everything else.
[01:13:19.220 --> 01:13:24.220]   OK.
[01:13:24.220 --> 01:13:26.100]   Let's take a little break.
[01:13:26.100 --> 01:13:27.180]   We want to talk about outrage.
[01:13:27.180 --> 01:13:29.060]   And since we did start talking about business,
[01:13:29.060 --> 01:13:33.820]   we should probably mention that the quarterly results were out
[01:13:33.820 --> 01:13:35.220]   for three of the big five.
[01:13:35.220 --> 01:13:37.500]   And what a surprise.
[01:13:37.500 --> 01:13:40.500]   They're just rolling in dough, kids.
[01:13:40.500 --> 01:13:44.100]   The tech industry is going great guns.
[01:13:44.100 --> 01:13:45.700]   And what does that all mean?
[01:13:45.700 --> 01:13:48.620]   We're not a business show, but I think there's something
[01:13:48.620 --> 01:13:50.020]   to be said, including--
[01:13:50.020 --> 01:13:53.500]   we're going to be talking to Scott Galloway about his book
[01:13:53.500 --> 01:13:57.140]   The Four, either Friday-- is it this Friday?
[01:13:57.140 --> 01:13:58.700]   On triangulation.
[01:13:58.700 --> 01:14:02.420]   And he's the professor at your school, I think.
[01:14:02.420 --> 01:14:02.580]   Right?
[01:14:02.580 --> 01:14:03.420]   He's at NYU.
[01:14:03.420 --> 01:14:03.900]   Yeah.
[01:14:03.900 --> 01:14:04.380]   Yeah, yeah.
[01:14:04.380 --> 01:14:06.460]   Who said that, hey, these guys better watch out
[01:14:06.460 --> 01:14:08.740]   because governments are going to start to crack down on them.
[01:14:08.740 --> 01:14:10.820]   They are monopolies.
[01:14:10.820 --> 01:14:12.660]   And I'm not sure I agree with him,
[01:14:12.660 --> 01:14:15.100]   but we can talk about, are these companies too big,
[01:14:15.100 --> 01:14:16.260]   too powerful?
[01:14:16.260 --> 01:14:17.980]   Farhad Manju has been talking about this.
[01:14:17.980 --> 01:14:20.500]   He talks about the big five, Apple, Google, Microsoft,
[01:14:20.500 --> 01:14:23.540]   Amazon, and Facebook.
[01:14:23.540 --> 01:14:27.300]   Dominate in their respective fields.
[01:14:27.300 --> 01:14:29.820]   Kind of, at this point, such strong incumbents,
[01:14:29.820 --> 01:14:32.540]   they'd be hard to beat.
[01:14:32.540 --> 01:14:34.140]   I think this is an interesting topic.
[01:14:34.140 --> 01:14:36.300]   And it ties into those quarterly results,
[01:14:36.300 --> 01:14:40.060]   which are kind of confirming a little bit of that.
[01:14:40.060 --> 01:14:43.620]   Famous Nobel Prize winning economist Joseph Stiglitz
[01:14:43.620 --> 01:14:46.620]   wrote a, I thought, a very interesting piece in the nation.
[01:14:46.620 --> 01:14:49.260]   America has a monopoly problem.
[01:14:49.260 --> 01:14:50.300]   And it's huge.
[01:14:50.300 --> 01:14:53.420]   We're dominated by large corporations
[01:14:53.420 --> 01:14:58.580]   in a way that has failed the many and enriched the few.
[01:14:58.580 --> 01:14:59.260]   Food for thought.
[01:14:59.260 --> 01:15:01.940]   But first, we had a fun week on Twitter.
[01:15:01.940 --> 01:15:04.100]   And we have made a very nice little video.
[01:15:04.100 --> 01:15:08.540]   John, if you could roll the tape, we can watch it.
[01:15:08.540 --> 01:15:10.540]   Previously, on to it.
[01:15:10.540 --> 01:15:15.060]   Have you ever used the calculator in iOS 11?
[01:15:15.060 --> 01:15:17.020]   And then you got to do this kind of quickly.
[01:15:17.020 --> 01:15:20.620]   1 plus 2 plus 3 equals 23.
[01:15:20.620 --> 01:15:22.180]   My goodness.
[01:15:22.180 --> 01:15:24.540]   OK, well, if you want to pay over $1,000
[01:15:24.540 --> 01:15:28.220]   for a calculator that doesn't work, you can do so.
[01:15:28.220 --> 01:15:29.180]   Know how.
[01:15:29.180 --> 01:15:30.860]   The headset that he's using, again,
[01:15:30.860 --> 01:15:32.020]   we took a look at it last week.
[01:15:32.020 --> 01:15:33.820]   It's the Acer AH101.
[01:15:33.820 --> 01:15:35.900]   The Windows Mixed Reality headset.
[01:15:35.900 --> 01:15:37.860]   It's the same price as the Oculus Rift.
[01:15:37.860 --> 01:15:40.420]   I would argue that it's far better.
[01:15:40.420 --> 01:15:43.100]   I can actually use this to help me when I'm doing my 3D designs.
[01:15:43.100 --> 01:15:46.260]   I can actually visualize the sign in 3D space
[01:15:46.260 --> 01:15:48.980]   rather than having to flip the screen back and forth
[01:15:48.980 --> 01:15:49.860]   all the time.
[01:15:49.860 --> 01:15:52.900]   But of course, for Jason, it's all super hot.
[01:15:52.900 --> 01:15:55.420]   That's pretty much all that I ever want to do.
[01:15:55.420 --> 01:15:57.100]   The new screen sabers.
[01:15:57.100 --> 01:15:59.460]   Speaking of Amazon devices, the look is here.
[01:15:59.460 --> 01:16:00.140]   Wow.
[01:16:00.140 --> 01:16:02.300]   This will let you compare two outfits.
[01:16:02.300 --> 01:16:04.220]   And they'll tell you which outfit is better.
[01:16:04.220 --> 01:16:06.620]   It's to replace friends, I guess, and family.
[01:16:06.620 --> 01:16:08.500]   Oh, well, let's see my results.
[01:16:08.500 --> 01:16:09.380]   Oh, it had to do that.
[01:16:09.380 --> 01:16:13.300]   It says, where the suit, the combination of pieces,
[01:16:13.300 --> 01:16:15.020]   is more flattering.
[01:16:15.020 --> 01:16:15.700]   To it.
[01:16:15.700 --> 01:16:18.700]   Technology isn't always pretty, but we are.
[01:16:18.700 --> 01:16:20.180]   [GUNSHOT]
[01:16:20.180 --> 01:16:21.020]   Oh, this is--
[01:16:21.020 --> 01:16:21.460]   There we go.
[01:16:21.460 --> 01:16:23.900]   This is what it would feel like to be inside of a John Woo
[01:16:23.900 --> 01:16:27.980]   film, where everything's really slow motion.
[01:16:27.980 --> 01:16:29.780]   It just makes me want to swallow up.
[01:16:29.780 --> 01:16:31.060]   See, OK.
[01:16:31.060 --> 01:16:32.060]   You just wanted that.
[01:16:32.060 --> 01:16:33.900]   You wanted this super hot.
[01:16:33.900 --> 01:16:36.340]   What can I say?
[01:16:36.340 --> 01:16:37.020]   A lot of fun.
[01:16:37.020 --> 01:16:39.260]   Tune in, of course, Monday through Sunday.
[01:16:39.260 --> 01:16:41.900]   We're here all week long at Twit.tv.
[01:16:41.900 --> 01:16:45.220]   You can watch live at Twit.tv Live or download shows
[01:16:45.220 --> 01:16:48.940]   from our respective shows right from the website
[01:16:48.940 --> 01:16:50.300]   or subscribe.
[01:16:50.300 --> 01:16:52.980]   Search for Twit in your favorite podcast program,
[01:16:52.980 --> 01:16:55.260]   and you'll find a lot of great content.
[01:16:55.260 --> 01:17:00.380]   Our show today brought to you by Jamf, J-A-M-F.
[01:17:00.380 --> 01:17:03.060]   If anybody who runs a shop with a lot of Macs
[01:17:03.060 --> 01:17:06.820]   probably knows about Jamf, and their new solution, Jamf Now.
[01:17:06.820 --> 01:17:10.780]   It's an on-demand mobile device management solution
[01:17:10.780 --> 01:17:14.300]   for iPhones, iPads.
[01:17:14.300 --> 01:17:15.100]   We had this problem.
[01:17:15.100 --> 01:17:16.940]   When you first started business, and you're
[01:17:16.940 --> 01:17:19.820]   giving your employees phones, or tablets, or computers,
[01:17:19.820 --> 01:17:21.780]   it's easy enough to keep track of them.
[01:17:21.780 --> 01:17:23.700]   But as you grow and you buy more tech,
[01:17:23.700 --> 01:17:26.820]   and it's harder to keep track of everyone's Mac and iPhone
[01:17:26.820 --> 01:17:29.300]   and iPad devices, just the other day,
[01:17:29.300 --> 01:17:31.820]   Jon came up with an iPad.
[01:17:31.820 --> 01:17:34.180]   We had no idea who's it was or more importantly,
[01:17:34.180 --> 01:17:35.820]   how to unlock it.
[01:17:35.820 --> 01:17:37.540]   Have we ever-- have we unlocked that?
[01:17:37.540 --> 01:17:38.420]   It's a nice iPad.
[01:17:38.420 --> 01:17:40.860]   It's an-- we don't know.
[01:17:40.860 --> 01:17:41.500]   Darn it.
[01:17:41.500 --> 01:17:43.820]   I wish we'd been using Jamf Now.
[01:17:43.820 --> 01:17:47.220]   Jamf Now lets you secure a lost iPad.
[01:17:47.220 --> 01:17:50.300]   It makes management tasks like deploying Wi-Fi passwords
[01:17:50.300 --> 01:17:50.740]   easier.
[01:17:50.740 --> 01:17:54.300]   I am constantly asking Russell, what's the Wi-Fi password?
[01:17:54.300 --> 01:17:55.380]   Constantly.
[01:17:55.380 --> 01:17:58.380]   He could just push it out to my device, secure company data,
[01:17:58.380 --> 01:18:00.340]   enforce passcodes.
[01:18:00.340 --> 01:18:01.020]   It's very simple.
[01:18:01.020 --> 01:18:01.940]   It's very informal.
[01:18:01.940 --> 01:18:04.820]   In fact, you can set up your free Jamf Now account,
[01:18:04.820 --> 01:18:08.020]   and manage your first three devices free forever
[01:18:08.020 --> 01:18:09.540]   at Jamf.com/twit.
[01:18:09.540 --> 01:18:11.700]   Just adding more devices is just $2 a month.
[01:18:11.700 --> 01:18:14.260]   It's easy.
[01:18:14.260 --> 01:18:17.180]   Remotely configure settings like a password, email, account,
[01:18:17.180 --> 01:18:18.620]   information.
[01:18:18.620 --> 01:18:22.660]   So when you deliver a computer or a tablet or a phone
[01:18:22.660 --> 01:18:26.580]   to your employees, it's all ready to go.
[01:18:26.580 --> 01:18:28.060]   You can protect sensitive information.
[01:18:28.060 --> 01:18:30.660]   You can even lock or wipe a device.
[01:18:30.660 --> 01:18:33.260]   You can centrally deploy apps, view device details.
[01:18:33.260 --> 01:18:35.420]   You'll get a 360 degree view of your inventory.
[01:18:35.420 --> 01:18:38.300]   And the nice thing about this is you don't have to be an IT expert.
[01:18:38.300 --> 01:18:40.300]   You don't need to Russell.
[01:18:40.300 --> 01:18:41.300]   Anybody can do this.
[01:18:41.300 --> 01:18:45.020]   Set up, manage, and protect your Apple devices in minutes
[01:18:45.020 --> 01:18:47.620]   so you can focus on your business instead.
[01:18:47.620 --> 01:18:49.540]   It's Jamf, JAMf.
[01:18:49.540 --> 01:18:50.780]   They've been around for a long time.
[01:18:50.780 --> 01:18:55.060]   They're kind of the kings of managing Apple workplaces.
[01:18:55.060 --> 01:18:59.500]   And if you go to Jamf.com/twit, you can create your free Jamf
[01:18:59.500 --> 01:19:01.820]   now account and manage your first three devices, as I said,
[01:19:01.820 --> 01:19:05.140]   free $2 a month per additional device.
[01:19:05.140 --> 01:19:09.580]   Jamf, J-A-M-F.com/twit.
[01:19:09.580 --> 01:19:13.460]   Patrick Bejave, the Phileus Club, is here.
[01:19:13.460 --> 01:19:16.340]   So nice to see our favorite Frenchman.
[01:19:16.340 --> 01:19:19.380]   Actually, if I say that, Cedric Ingres is going to be
[01:19:19.380 --> 01:19:21.700]   hat-mat at me.
[01:19:21.700 --> 01:19:23.140]   And I can call you our favorite--
[01:19:23.140 --> 01:19:23.980]   I'll calm him down.
[01:19:23.980 --> 01:19:26.620]   Our favorite-- it's funny.
[01:19:26.620 --> 01:19:28.900]   I was in France last month, and everywhere I went with 4
[01:19:28.900 --> 01:19:30.860]   Square, Cedric had created the place.
[01:19:30.860 --> 01:19:33.940]   [LAUGHTER]
[01:19:33.940 --> 01:19:34.900]   He has to do that.
[01:19:34.900 --> 01:19:37.260]   You notice that?
[01:19:37.260 --> 01:19:40.420]   Now, when I go places in Petaluma, I created them.
[01:19:40.420 --> 01:19:43.380]   I'll say, you created this spot eight years ago on 4 Square
[01:19:43.380 --> 01:19:46.060]   and thousands of people have logged in ever since.
[01:19:46.060 --> 01:19:50.340]   Does anybody still use that 4 Square and swarm?
[01:19:50.340 --> 01:19:51.140]   Don't think so.
[01:19:51.140 --> 01:19:52.340]   Yeah.
[01:19:52.340 --> 01:19:52.700]   Kind of a--
[01:19:52.700 --> 01:19:53.420]   Tommy.
[01:19:53.420 --> 01:19:54.700]   Yeah.
[01:19:54.700 --> 01:19:57.340]   Did you see my Amazon look?
[01:19:57.340 --> 01:19:58.900]   This is silly.
[01:19:58.900 --> 01:20:00.100]   This is the silliest thing ever.
[01:20:00.100 --> 01:20:01.300]   It finally came.
[01:20:01.300 --> 01:20:04.500]   First of all, smaller than I thought, it's an Amazon Echo
[01:20:04.500 --> 01:20:07.780]   with a camera and a flash attached to it.
[01:20:07.780 --> 01:20:09.700]   And it doesn't have much of a speaker.
[01:20:09.700 --> 01:20:11.020]   It's such a small speaker.
[01:20:11.020 --> 01:20:12.420]   You can't really hear what it's saying.
[01:20:12.420 --> 01:20:13.620]   That's not its point.
[01:20:13.620 --> 01:20:18.500]   But what you do with it, you have to have a separate app
[01:20:18.500 --> 01:20:19.740]   called the LookApp.
[01:20:19.740 --> 01:20:21.900]   And the idea-- and I'm going to do this, I think--
[01:20:21.900 --> 01:20:26.300]   is every day, you get in front and you say, echo,
[01:20:26.300 --> 01:20:27.580]   take my picture.
[01:20:27.580 --> 01:20:28.740]   And it goes, boop, boop, boop.
[01:20:28.740 --> 01:20:30.220]   And it flashes.
[01:20:30.220 --> 01:20:32.460]   And then it takes your picture.
[01:20:32.460 --> 01:20:35.380]   What's the date, the weather.
[01:20:35.380 --> 01:20:37.260]   And then it has this style check thing here.
[01:20:37.260 --> 01:20:39.340]   So you could take-- this is what I'm wearing today.
[01:20:39.340 --> 01:20:41.700]   And you could take an outfit that you wore the other day.
[01:20:41.700 --> 01:20:44.100]   This is not good for guys like me.
[01:20:44.100 --> 01:20:46.460]   And I could say, which outfit looks better?
[01:20:46.460 --> 01:20:48.660]   And then it checks it.
[01:20:48.660 --> 01:20:49.300]   It's unclear.
[01:20:49.300 --> 01:20:50.900]   I think when Amazon first announced the look,
[01:20:50.900 --> 01:20:54.980]   they said that the way it did it was they would--
[01:20:54.980 --> 01:20:57.380]   so now they're going to notify me when the results are ready.
[01:20:57.380 --> 01:20:59.580]   They would send this to a stylist,
[01:20:59.580 --> 01:21:01.180]   because it doesn't happen right away.
[01:21:01.180 --> 01:21:03.980]   And there would be a combination of computer intelligence
[01:21:03.980 --> 01:21:07.820]   and a stylist rating your outfits.
[01:21:07.820 --> 01:21:12.180]   And then it's told me, it's told me which outfits I should wear.
[01:21:12.180 --> 01:21:15.900]   For instance, for some reason unknown,
[01:21:15.900 --> 01:21:18.540]   it recommends the frumpy sweater vest
[01:21:18.540 --> 01:21:22.740]   look over the stylish suit jacket look.
[01:21:22.740 --> 01:21:24.300]   OK, fine.
[01:21:24.300 --> 01:21:26.780]   Yesterday, I think this was correct.
[01:21:26.780 --> 01:21:28.740]   It recommended the Hawaiian shirt
[01:21:28.740 --> 01:21:31.140]   minus the Hawaiian shirt plus overcoat.
[01:21:31.140 --> 01:21:36.140]   I don't know.
[01:21:36.140 --> 01:21:38.860]   To me, it looks-- but I'm going to do this every day from now on,
[01:21:38.860 --> 01:21:41.500]   because at least I'll have a record of what I wore.
[01:21:41.500 --> 01:21:42.580]   If you're on TV, it's--
[01:21:42.580 --> 01:21:44.580]   how long does it take to make the recommendation?
[01:21:44.580 --> 01:21:46.660]   Can you use it in the morning before you go to work?
[01:21:46.660 --> 01:21:47.180]   Oh, yeah.
[01:21:47.180 --> 01:21:49.180]   In fact, it'll have a-- yeah, it'll have-- see,
[01:21:49.180 --> 01:21:51.740]   I just got my recommendation.
[01:21:51.740 --> 01:21:54.220]   And it does say 70%.
[01:21:54.220 --> 01:21:56.260]   And by the way, it weights it.
[01:21:56.260 --> 01:22:01.220]   So 70% stronger for my current look than the jacket.
[01:22:01.220 --> 01:22:02.460]   And then it tells you what's working.
[01:22:02.460 --> 01:22:05.180]   These colors look better together.
[01:22:05.180 --> 01:22:05.860]   Amy, what is--
[01:22:05.860 --> 01:22:07.140]   I think--
[01:22:07.140 --> 01:22:08.460]   It's AI.
[01:22:08.460 --> 01:22:10.340]   And I think there's some humans.
[01:22:10.340 --> 01:22:12.500]   Yeah, that's what I understood.
[01:22:12.500 --> 01:22:14.820]   So do they have like an army of stylists?
[01:22:14.820 --> 01:22:15.700]   It's pretty fast.
[01:22:15.700 --> 01:22:16.660]   They're waiting.
[01:22:16.660 --> 01:22:18.540]   They-- this was an invite only.
[01:22:18.540 --> 01:22:19.540]   They probably.
[01:22:19.540 --> 01:22:21.060]   Yeah, this was an invite only.
[01:22:21.060 --> 01:22:23.220]   And I just noticed that it was available.
[01:22:23.220 --> 01:22:24.500]   So I ordered it.
[01:22:24.500 --> 01:22:27.380]   Amy, why is Amazon doing this?
[01:22:27.380 --> 01:22:30.300]   Well, so let's think about why Amazon is doing this
[01:22:30.300 --> 01:22:32.660]   within the broader picture of these other companies
[01:22:32.660 --> 01:22:34.940]   we've been talking about that are in the hardware business.
[01:22:34.940 --> 01:22:37.780]   So Amazon is a platform, right?
[01:22:37.780 --> 01:22:40.420]   Amazon, however, is a platform that's gotten itself
[01:22:40.420 --> 01:22:42.780]   into the hardware business and is--
[01:22:42.780 --> 01:22:45.740]   but is in the hardware business in a very broad way.
[01:22:45.740 --> 01:22:49.300]   So Amazon's also making sensors.
[01:22:49.300 --> 01:22:50.580]   And they are--
[01:22:50.580 --> 01:22:52.100]   Oh, I didn't know that.
[01:22:52.100 --> 01:22:53.660]   Yeah, or I don't know if they're physically
[01:22:53.660 --> 01:22:55.500]   manufacturing them, but they have a program now
[01:22:55.500 --> 01:22:58.020]   where they're helping mining companies--
[01:22:58.020 --> 01:23:01.420]   or they're going to be helping mining companies optimize
[01:23:01.420 --> 01:23:11.820]   their roots so that trucks can get more per gallon of mileage.
[01:23:11.820 --> 01:23:15.140]   That's the part of Amazon that's the logistics company, right?
[01:23:15.140 --> 01:23:18.020]   That they're experts in logistics.
[01:23:18.020 --> 01:23:20.940]   Right, and it involves hardware.
[01:23:20.940 --> 01:23:23.300]   And the reason I think this is so interesting
[01:23:23.300 --> 01:23:26.540]   is because the next era of computing,
[01:23:26.540 --> 01:23:29.740]   it will be powered and predicated
[01:23:29.740 --> 01:23:32.340]   on artificial intelligence.
[01:23:32.340 --> 01:23:34.020]   And what's so interesting about that,
[01:23:34.020 --> 01:23:36.260]   within the context of what we were just talking about,
[01:23:36.260 --> 01:23:41.540]   those big companies, are the tech big tech companies,
[01:23:41.540 --> 01:23:43.660]   monopolies, and if you haven't been looking at Farhad's
[01:23:43.660 --> 01:23:47.580]   really, really good columns about this and the times,
[01:23:47.580 --> 01:23:49.100]   you absolutely should, because he's
[01:23:49.100 --> 01:23:51.580]   making some really compelling and interesting arguments.
[01:23:51.580 --> 01:23:56.140]   But there's also a monopoly in the AI space
[01:23:56.140 --> 01:23:58.060]   that we just don't think about.
[01:23:58.060 --> 01:24:00.140]   And there's nine big companies that
[01:24:00.140 --> 01:24:04.540]   control the fate and future of artificial intelligence.
[01:24:04.540 --> 01:24:11.100]   And they include Amazon, Google, Microsoft, Facebook, Apple,
[01:24:11.100 --> 01:24:14.340]   IBM, and then-- I'm forgetting somebody--
[01:24:14.340 --> 01:24:18.580]   and then three companies in China, Tencent, Alibaba, and Baidu.
[01:24:18.580 --> 01:24:21.580]   And most of those companies are also getting--
[01:24:21.580 --> 01:24:23.820]   Google, sorry, I didn't mention Google.
[01:24:23.820 --> 01:24:26.300]   But by necessity, almost, they're also
[01:24:26.300 --> 01:24:27.740]   getting into the hardware space.
[01:24:27.740 --> 01:24:34.260]   So is Amazon trying to be the style mecca of the future?
[01:24:34.260 --> 01:24:37.140]   No, probably not.
[01:24:37.140 --> 01:24:40.900]   Are they trying to use our data in an interesting way
[01:24:40.900 --> 01:24:43.620]   so that we use the platform more?
[01:24:43.620 --> 01:24:45.660]   And ultimately, we spend more money
[01:24:45.660 --> 01:24:47.380]   on their various services?
[01:24:47.380 --> 01:24:48.340]   Yes.
[01:24:48.340 --> 01:24:51.980]   So a lot of the hardware that I think Amazon is producing
[01:24:51.980 --> 01:24:54.420]   is just a different way to connect us to their platform
[01:24:54.420 --> 01:24:57.020]   and services where we'll spend money, which
[01:24:57.020 --> 01:25:01.020]   is a really interesting way of thinking about the work
[01:25:01.020 --> 01:25:03.460]   that they're doing and how it connects to AI and sort
[01:25:03.460 --> 01:25:05.260]   of the motivation behind it.
[01:25:05.260 --> 01:25:07.460]   So that's interesting, because you're not
[01:25:07.460 --> 01:25:10.820]   fearing the data mining aspect of what they're doing so much.
[01:25:10.820 --> 01:25:15.460]   Well, I mean, yeah, we should all be fearing that.
[01:25:15.460 --> 01:25:21.460]   We should all be thinking a lot more about what of our data
[01:25:21.460 --> 01:25:23.860]   is being mined and refined.
[01:25:23.860 --> 01:25:27.700]   Because anybody who puts this in their closet, this Amazon
[01:25:27.700 --> 01:25:31.660]   look, is basically sending--
[01:25:31.660 --> 01:25:35.420]   everything you could possibly know about my wardrobe
[01:25:35.420 --> 01:25:37.460]   to Amazon who sells clothes.
[01:25:37.460 --> 01:25:38.460]   Right.
[01:25:38.460 --> 01:25:39.740]   Now, why are we concerned about that?
[01:25:39.740 --> 01:25:43.060]   I think if you ask the average person whether or not
[01:25:43.060 --> 01:25:45.780]   that if they even knew, half of them don't know.
[01:25:45.780 --> 01:25:47.660]   But if the average person is concerned,
[01:25:47.660 --> 01:25:50.020]   it's most likely because they think somebody is sitting
[01:25:50.020 --> 01:25:50.820]   in a desk at Amazon--
[01:25:50.820 --> 01:25:51.380]   Yeah, snooping.
[01:25:51.380 --> 01:25:52.220]   --like watching them.
[01:25:52.220 --> 01:25:53.020]   Right.
[01:25:53.020 --> 01:25:54.860]   The answer purrifies it.
[01:25:54.860 --> 01:25:56.100]   That's exactly right.
[01:25:56.100 --> 01:25:58.300]   I'm always at great pains to say, no, no, Google's
[01:25:58.300 --> 01:25:59.620]   not reading your email.
[01:25:59.620 --> 01:26:01.180]   There's nobody at Google going, let's
[01:26:01.180 --> 01:26:02.700]   see what Leo's saying these days.
[01:26:02.700 --> 01:26:05.260]   It's all machinery.
[01:26:05.260 --> 01:26:07.580]   But Google is reading our email.
[01:26:07.580 --> 01:26:08.380]   It's just not a person--
[01:26:08.380 --> 01:26:09.380]   Mechanically.
[01:26:09.380 --> 01:26:10.380]   Right.
[01:26:10.380 --> 01:26:11.700]   They're scanning it.
[01:26:11.700 --> 01:26:14.420]   It's the same exact scanning that any company does
[01:26:14.420 --> 01:26:15.940]   for spam keywords.
[01:26:15.940 --> 01:26:18.140]   Except that instead of looking for spam,
[01:26:18.140 --> 01:26:20.300]   they're looking for advertising opportunities.
[01:26:20.300 --> 01:26:22.500]   And that doesn't bother me that much.
[01:26:22.500 --> 01:26:24.660]   Well, it doesn't bother you.
[01:26:24.660 --> 01:26:25.500]   Because you--
[01:26:25.500 --> 01:26:26.980]   It would bother me if Google--
[01:26:26.980 --> 01:26:28.100]   and I don't think they're doing this.
[01:26:28.100 --> 01:26:28.980]   I know they're not doing this.
[01:26:28.980 --> 01:26:31.540]   It would bother me if Google or any company was gathering
[01:26:31.540 --> 01:26:34.620]   information about me, say, let's say, about my eating habits
[01:26:34.620 --> 01:26:36.500]   so that they could sell it to an insurance company
[01:26:36.500 --> 01:26:38.660]   considering offering me life insurance.
[01:26:38.660 --> 01:26:39.860]   That would bother me.
[01:26:39.860 --> 01:26:41.140]   But I don't think that's happening.
[01:26:41.140 --> 01:26:42.820]   In fact, I don't think there's any reason for insurance
[01:26:42.820 --> 01:26:44.900]   companies to do that.
[01:26:44.900 --> 01:26:47.220]   No, but there are certainly reasons for--
[01:26:47.220 --> 01:26:49.420]   I mean, but that will be happening
[01:26:49.420 --> 01:26:51.060]   within the driving realm.
[01:26:51.060 --> 01:26:51.580]   Right?
[01:26:51.580 --> 01:26:52.100]   Ah.
[01:26:52.100 --> 01:26:52.580]   So--
[01:26:52.580 --> 01:26:52.940]   Right.
[01:26:52.940 --> 01:26:53.660]   And--
[01:26:53.660 --> 01:26:58.420]   Yes, because Tesla knows everything that I do in my Tesla.
[01:26:58.420 --> 01:27:03.260]   When I break, how fast I go, they are very openly
[01:27:03.260 --> 01:27:04.140]   collecting that data.
[01:27:04.140 --> 01:27:07.300]   In fact, so much to the point that when we call them to say,
[01:27:07.300 --> 01:27:08.980]   hey, this car is driving funny.
[01:27:08.980 --> 01:27:10.660]   I put it in reverse, but it went forward.
[01:27:10.660 --> 01:27:11.860]   They said, well, let me check.
[01:27:11.860 --> 01:27:12.620]   No, no.
[01:27:12.620 --> 01:27:14.500]   You put it in reverse, it went backward.
[01:27:14.500 --> 01:27:16.060]   They know.
[01:27:16.060 --> 01:27:18.380]   And they don't hide it.
[01:27:18.380 --> 01:27:20.100]   There's a really interesting discussion
[01:27:20.100 --> 01:27:22.460]   that happened a few months ago.
[01:27:22.460 --> 01:27:25.580]   I think it's gone away a little bit now in France,
[01:27:25.580 --> 01:27:28.180]   where, by the way, we have an acronym for the Big Five.
[01:27:28.180 --> 01:27:29.740]   We call them the Gaffam.
[01:27:29.740 --> 01:27:32.380]   So Google Amazon, Facebook, Apple, Microsoft.
[01:27:32.380 --> 01:27:33.820]   They have their name.
[01:27:33.820 --> 01:27:35.020]   Yeah.
[01:27:35.020 --> 01:27:37.740]   The Farhout cars have the frightful five.
[01:27:37.740 --> 01:27:38.260]   Right.
[01:27:38.260 --> 01:27:40.460]   That works, too.
[01:27:40.460 --> 01:27:41.940]   And I would include the other ones,
[01:27:41.940 --> 01:27:43.340]   because they're also important.
[01:27:43.340 --> 01:27:45.500]   Yeah, in the US, we think about these five,
[01:27:45.500 --> 01:27:47.740]   but they're these Chinese companies as well.
[01:27:47.740 --> 01:27:48.260]   And Asian companies.
[01:27:48.260 --> 01:27:50.540]   I mean, the reason we think about them so much
[01:27:50.540 --> 01:27:55.740]   is that they're really big and extra national for us.
[01:27:55.740 --> 01:27:59.100]   And they also don't pay a lot of taxes in France.
[01:27:59.100 --> 01:28:02.180]   So bringing it back to what you were saying earlier, Amy,
[01:28:02.180 --> 01:28:05.900]   that personal data is going to be--
[01:28:05.900 --> 01:28:07.940]   is a natural resource now.
[01:28:07.940 --> 01:28:11.860]   There was a conversation about taxing those companies,
[01:28:11.860 --> 01:28:16.780]   not on the work that they do or on the money that they earn,
[01:28:16.780 --> 01:28:21.500]   but on the data that they use, considering the data
[01:28:21.500 --> 01:28:24.900]   on French citizens, a national resource
[01:28:24.900 --> 01:28:28.220]   that they are mining and using and taxing them accordingly.
[01:28:28.220 --> 01:28:32.140]   It didn't go anywhere so far, but it was being discussed.
[01:28:32.140 --> 01:28:34.620]   One of Farhod's points is, for instance,
[01:28:34.620 --> 01:28:37.500]   if you're a startup, forget about it.
[01:28:37.500 --> 01:28:39.700]   This is one of his first articles on the frightful five,
[01:28:39.700 --> 01:28:43.980]   how frightful five put startups in a lose, lose situation.
[01:28:43.980 --> 01:28:46.020]   And of course, this is the power of monopoly.
[01:28:46.020 --> 01:28:47.500]   You don't have to have an actual monopoly.
[01:28:47.500 --> 01:28:50.300]   I don't think any of these companies have an actual monopoly,
[01:28:50.300 --> 01:28:52.820]   except maybe perhaps Facebook and social media.
[01:28:52.820 --> 01:28:54.420]   But then there's always Twitter and other stuff.
[01:28:54.420 --> 01:28:56.420]   So none of them have an actual monopoly,
[01:28:56.420 --> 01:28:59.020]   but they have enough control and power.
[01:28:59.020 --> 01:29:02.340]   They're big enough that it becomes a real problem
[01:29:02.340 --> 01:29:03.100]   to compete with them.
[01:29:03.100 --> 01:29:07.020]   And maybe that's why we see so few interesting new startups
[01:29:07.020 --> 01:29:07.620]   these days.
[01:29:07.620 --> 01:29:09.620]   They're all crap.
[01:29:09.620 --> 01:29:10.820]   And it's even worse.
[01:29:10.820 --> 01:29:12.300]   It's going to get worse.
[01:29:12.300 --> 01:29:16.020]   There was an article by John Evans on TechCrunch
[01:29:16.020 --> 01:29:22.060]   a few days ago about the fact that the future technologies
[01:29:22.060 --> 01:29:26.660]   like AI and self-driving cars and AR and all of those
[01:29:26.660 --> 01:29:30.620]   require capital and data.
[01:29:30.620 --> 01:29:33.540]   And that is something that is a lot less available
[01:29:33.540 --> 01:29:34.980]   to smaller structures.
[01:29:34.980 --> 01:29:38.540]   So the frightful five are going to be--
[01:29:38.540 --> 01:29:41.340]   basically you have a stranglehold on the future
[01:29:41.340 --> 01:29:42.660]   technologies as well.
[01:29:42.660 --> 01:29:43.620]   So that's definitely--
[01:29:43.620 --> 01:29:45.100]   Well, that's perfectly changing.
[01:29:45.100 --> 01:29:47.300]   So that's-- so here's-- again, people
[01:29:47.300 --> 01:29:48.940]   don't know a lot about this.
[01:29:48.940 --> 01:29:53.020]   But China is the largest foreign direct investor
[01:29:53.020 --> 01:29:57.780]   in all in American technology companies.
[01:29:57.780 --> 01:30:02.220]   And China has invested huge sums of money
[01:30:02.220 --> 01:30:04.340]   and they are into startups as well.
[01:30:04.340 --> 01:30:06.940]   Through venture capital funds or how?
[01:30:06.940 --> 01:30:09.700]   Direct investment purchases?
[01:30:09.700 --> 01:30:10.460]   Private equity.
[01:30:10.460 --> 01:30:11.300]   Private equity.
[01:30:11.300 --> 01:30:13.300]   Through direct investment.
[01:30:13.300 --> 01:30:16.780]   And China, historically, doesn't play fair.
[01:30:16.780 --> 01:30:20.020]   So one of the ways-- they're not just investing
[01:30:20.020 --> 01:30:21.900]   because they're hoping for a 10x return.
[01:30:21.900 --> 01:30:24.420]   They're investing with stipulations
[01:30:24.420 --> 01:30:29.140]   that IP is shared because China has put part of its sovereign
[01:30:29.140 --> 01:30:29.860]   wealth fund.
[01:30:29.860 --> 01:30:31.620]   I mean, China is going all in--
[01:30:31.620 --> 01:30:33.380]   When a nation-state invests in you,
[01:30:33.380 --> 01:30:35.700]   they don't have the same goals as individual investor
[01:30:35.700 --> 01:30:36.300]   would have.
[01:30:36.300 --> 01:30:36.940]   That's right.
[01:30:36.940 --> 01:30:41.820]   And at the moment, we aren't quite sure
[01:30:41.820 --> 01:30:46.860]   whether or not and how China's industrial policies now
[01:30:46.860 --> 01:30:51.180]   may or may not affect treaties and economic agreements
[01:30:51.180 --> 01:30:53.860]   because it's very complicated.
[01:30:53.860 --> 01:30:56.500]   I, for one, welcome our new Chinese overlords.
[01:30:56.500 --> 01:30:59.180]   I don't think--
[01:30:59.180 --> 01:31:00.180]   Is it nefarious?
[01:31:00.180 --> 01:31:01.980]   You think you should be afraid of this?
[01:31:01.980 --> 01:31:04.860]   Well, remember that story about the guy J.
[01:31:04.860 --> 01:31:07.060]   Walking, having his private information
[01:31:07.060 --> 01:31:08.620]   pass over the billboard?
[01:31:08.620 --> 01:31:09.900]   That's fixable.
[01:31:09.900 --> 01:31:10.420]   That's maybe--
[01:31:10.420 --> 01:31:11.860]   Just say that's not--
[01:31:11.860 --> 01:31:15.660]   You and your Hawaiian shirt broadcast all over what--
[01:31:15.660 --> 01:31:19.300]   Well, it better stops J. Walkers.
[01:31:19.300 --> 01:31:20.260]   It sure does.
[01:31:20.260 --> 01:31:22.180]   I think that's a cultural difference.
[01:31:22.180 --> 01:31:24.100]   And I don't think that necessarily
[01:31:24.100 --> 01:31:27.140]   means that that would happen in other countries,
[01:31:27.140 --> 01:31:29.140]   even if they were heavy Chinese influence,
[01:31:29.140 --> 01:31:30.940]   because that's a cultural difference.
[01:31:30.940 --> 01:31:31.980]   It doesn't necessarily.
[01:31:31.980 --> 01:31:32.940]   But what it does signal--
[01:31:32.940 --> 01:31:35.020]   It's China very much has that kind of culture
[01:31:35.020 --> 01:31:37.780]   of a community over individual, right?
[01:31:37.780 --> 01:31:38.260]   That's right.
[01:31:38.260 --> 01:31:38.860]   That's right.
[01:31:38.860 --> 01:31:40.860]   However-- and I lived in China for a while, too.
[01:31:40.860 --> 01:31:43.540]   So that's actually-- it sounds very strange to us
[01:31:43.540 --> 01:31:46.260]   in the United States, but it's not out of the ordinary in China.
[01:31:46.260 --> 01:31:51.500]   But my point is that for people who care about America
[01:31:51.500 --> 01:31:56.380]   continuing to be one of the, if not the global economic superpower,
[01:31:56.380 --> 01:32:00.860]   that could look very different as few as 15 years from now, 10,
[01:32:00.860 --> 01:32:01.860]   15, 20 years--
[01:32:01.860 --> 01:32:04.260]   Look, the American century is over.
[01:32:04.260 --> 01:32:07.220]   Yeah, the American century is over.
[01:32:07.220 --> 01:32:09.660]   We haven't gotten-- we haven't noticed it yet.
[01:32:09.660 --> 01:32:10.740]   Right?
[01:32:10.740 --> 01:32:12.780]   Look at-- I don't know if I would put it like that.
[01:32:12.780 --> 01:32:13.900]   Look at renewables.
[01:32:13.900 --> 01:32:15.820]   Look at renewables.
[01:32:15.820 --> 01:32:19.260]   What we talk about in America is how bad the air is in Beijing.
[01:32:19.260 --> 01:32:21.140]   But really, what's going on is China
[01:32:21.140 --> 01:32:23.340]   is becoming the king of renewable energy, right?
[01:32:23.340 --> 01:32:29.980]   And that's going to be a huge advantage to them,
[01:32:29.980 --> 01:32:35.020]   because solar is cheap compared to petroleum.
[01:32:35.020 --> 01:32:38.140]   Yes, and they also-- we have government officials right now
[01:32:38.140 --> 01:32:40.340]   who are AI deniers.
[01:32:40.340 --> 01:32:40.660]   Yes.
[01:32:40.660 --> 01:32:42.180]   They are climate deniers.
[01:32:42.180 --> 01:32:46.060]   They are secretary of treasury says, oh, automation,
[01:32:46.060 --> 01:32:50.100]   stealing jobs isn't even on our radar, he says.
[01:32:50.100 --> 01:32:54.100]   So there's a strategic advantage in China,
[01:32:54.100 --> 01:32:57.060]   just because their elected officials have
[01:32:57.060 --> 01:33:02.300]   don't let their personal politics get in the way of established facts,
[01:33:02.300 --> 01:33:02.940]   scientific basis.
[01:33:02.940 --> 01:33:05.420]   Oh, no, we were good at that.
[01:33:05.420 --> 01:33:06.900]   We're very good at that.
[01:33:06.900 --> 01:33:10.180]   All right, so this week, Amazon Alphabet, Microsoft,
[01:33:10.180 --> 01:33:12.260]   all announced spectacular earnings.
[01:33:12.260 --> 01:33:15.100]   They beat the street, the expectations that the analysts had,
[01:33:15.100 --> 01:33:16.500]   that meant their stock went up.
[01:33:16.500 --> 01:33:19.420]   In fact, it made Jeff Bezos the richest man in the world again,
[01:33:19.420 --> 01:33:20.460]   yet again.
[01:33:20.460 --> 01:33:23.740]   He gained $10 billion in his personal net worth.
[01:33:23.740 --> 01:33:30.500]   He's now more-- almost $94 billion more than Bill Gates.
[01:33:30.500 --> 01:33:31.860]   But that's not the story, really.
[01:33:31.860 --> 01:33:35.500]   The story is that these companies combined
[01:33:35.500 --> 01:33:39.620]   are worth $2 trillion.
[01:33:39.620 --> 01:33:43.540]   They are incredibly powerful, incredibly successful.
[01:33:43.540 --> 01:33:48.900]   Might as well add Intel, which beat the street as well.
[01:33:48.900 --> 01:33:53.380]   Actually, that's kind of surprising, since I don't think Intel is a big up-and-comer.
[01:33:53.380 --> 01:33:59.980]   But Amazon, total revenue $43 billion in three months.
[01:33:59.980 --> 01:34:05.100]   Amazon Web service, a $4 billion business per quarter.
[01:34:05.100 --> 01:34:09.100]   Alphabet, revenue boosted by a surge of clicks
[01:34:09.100 --> 01:34:13.620]   on Google Ads around the world, particularly in Asia.
[01:34:13.620 --> 01:34:22.620]   Microsoft beat estimates by a significant amount, by about $0.12 a share.
[01:34:22.620 --> 01:34:28.060]   It's a really-- these companies are getting bigger all the time.
[01:34:28.060 --> 01:34:30.260]   And I think Farhud--
[01:34:30.260 --> 01:34:33.820]   I have mixed feelings about Farhud's frightful five point of view,
[01:34:33.820 --> 01:34:36.860]   because it sounds a little bit like Technopanic.
[01:34:36.860 --> 01:34:39.180]   We certainly get a lot of benefits from these companies.
[01:34:39.180 --> 01:34:42.620]   Amazon, Facebook, Intel-- not Intel, Microsoft, Intel.
[01:34:42.620 --> 01:34:43.900]   Why is Intel even in there?
[01:34:43.900 --> 01:34:44.860]   Throw Intel out.
[01:34:44.860 --> 01:34:50.380]   Microsoft, Google, and the Chinese companies you mentioned.
[01:34:50.380 --> 01:34:55.500]   We give some big significant technological benefits from them.
[01:34:55.500 --> 01:34:56.540]   Should we be afraid?
[01:34:56.540 --> 01:34:57.020]   I don't know.
[01:34:57.020 --> 01:34:58.860]   I'm not sure.
[01:34:58.860 --> 01:35:00.420]   Michael, what do you think?
[01:35:00.420 --> 01:35:01.340]   You're on the ground.
[01:35:01.340 --> 01:35:03.300]   You're covering this stuff.
[01:35:03.300 --> 01:35:05.020]   Well, I mean, it's complicated.
[01:35:05.020 --> 01:35:09.300]   I think that the reason why I got into technology journalism
[01:35:09.300 --> 01:35:14.380]   was because 10 to 15 years ago, this stuff looked really exciting.
[01:35:14.380 --> 01:35:20.020]   When my space was still around and social media was still in its infancy,
[01:35:20.020 --> 01:35:21.900]   Web 2.0 was a new thing.
[01:35:21.900 --> 01:35:24.100]   That sounds like 100 years ago.
[01:35:24.100 --> 01:35:26.180]   That sounds like 100 years ago.
[01:35:26.180 --> 01:35:28.780]   You're a stranger things.
[01:35:28.780 --> 01:35:31.140]   Well, now we're in the twilight.
[01:35:31.140 --> 01:35:35.780]   Essentially, these companies that were really young and exciting
[01:35:35.780 --> 01:35:40.500]   and were introducing all of these new ideas and new ways of thinking
[01:35:40.500 --> 01:35:46.340]   into our daily lives ended up growing so big that they have
[01:35:46.340 --> 01:35:48.500]   this inordinate amount of influence
[01:35:48.500 --> 01:35:51.260]   and this inordinate amount of power.
[01:35:51.260 --> 01:35:55.660]   And I think, to Amy's point, there
[01:35:55.660 --> 01:36:00.060]   has to be some element of regulation in lockstep
[01:36:00.060 --> 01:36:02.580]   with the growth of these companies and with the growth of this technology.
[01:36:02.580 --> 01:36:05.500]   The problem is that right now in government,
[01:36:05.500 --> 01:36:09.980]   we're just like antitrust is just like not even a--
[01:36:09.980 --> 01:36:14.460]   it's not a concept that I don't think anyone thinks about ever at this point.
[01:36:14.460 --> 01:36:16.460]   You know, there's so many other fires to put out that--
[01:36:16.460 --> 01:36:19.820]   Yeah, I like how Sprint said, oh, this might be a good time to merge with T-Mobile.
[01:36:19.820 --> 01:36:21.660]   Nobody's looking quick.
[01:36:21.660 --> 01:36:25.860]   Yeah, and I think there is very little corporate oversight at this point.
[01:36:25.860 --> 01:36:28.980]   And so these companies that were already gigantic
[01:36:28.980 --> 01:36:30.740]   are growing even bigger.
[01:36:30.740 --> 01:36:39.020]   And I don't think people realize how bad things could be in the near future.
[01:36:39.020 --> 01:36:41.460]   I mean, something as simple as net neutrality,
[01:36:41.460 --> 01:36:45.420]   you brought up this merger.
[01:36:45.420 --> 01:36:48.460]   That's a huge deal, and it could actually cost consumers a lot.
[01:36:48.460 --> 01:36:55.540]   And there are so many different nodes to attack.
[01:36:55.540 --> 01:36:58.980]   You know, as a smart consumer and as an active citizen,
[01:36:58.980 --> 01:37:03.780]   there are so many battles that we need to fight when it comes to the relationship
[01:37:03.780 --> 01:37:09.060]   between government and technology that it gets hard to sort of pick any one, right?
[01:37:09.060 --> 01:37:10.500]   You know, we can talk about net neutrality.
[01:37:10.500 --> 01:37:15.180]   We can talk about privacy issues, whether the government should be able to snoop
[01:37:15.180 --> 01:37:17.380]   through your text messages or not.
[01:37:17.380 --> 01:37:22.420]   You know, there are so many layers to what's going on right now.
[01:37:22.420 --> 01:37:28.060]   And I think, you know, what I'm seeing is that the technology is rapidly outpacing
[01:37:28.060 --> 01:37:30.900]   the government's ability to regulate any of this stuff.
[01:37:30.900 --> 01:37:36.500]   And so, you know, I guess you just kind of have to do what you want with that information.
[01:37:36.500 --> 01:37:42.140]   But there seems to be no end in sight with especially these big tech companies.
[01:37:42.140 --> 01:37:47.460]   You know, they're posting record profits and record revenues
[01:37:47.460 --> 01:37:51.020]   on what seems to be like a regular basis.
[01:37:51.020 --> 01:37:55.500]   And, you know, and these companies have made several billionaires and several millionaires.
[01:37:55.500 --> 01:38:00.700]   So, I just don't see an end in sight to be totally honest.
[01:38:00.700 --> 01:38:06.060]   I mean, there's certainly a concern and I think they should be...
[01:38:06.060 --> 01:38:08.820]   People shouldn't take their eyes off of them.
[01:38:08.820 --> 01:38:13.140]   But the reality is they're all more or less competing with one another.
[01:38:13.140 --> 01:38:19.260]   So while they are gigantic, there is an element of competition.
[01:38:19.260 --> 01:38:20.740]   And I'm very concerned about them.
[01:38:20.740 --> 01:38:21.740]   Don't get me wrong.
[01:38:21.740 --> 01:38:23.460]   I don't want to make excuses.
[01:38:23.460 --> 01:38:29.380]   But I think it's less of an immediate concern than maybe something to make sure you don't
[01:38:29.380 --> 01:38:35.780]   take your eye off in the next, you know, close time, like one, two, three years.
[01:38:35.780 --> 01:38:40.500]   But if you want to talk about tech monopoly issues, I think the real problem you guys
[01:38:40.500 --> 01:38:45.780]   have in the US is what you were referring to ISPs.
[01:38:45.780 --> 01:38:51.700]   That is a horrible monopoly abuse.
[01:38:51.700 --> 01:38:58.980]   And it's having very real consequences because you don't have the infrastructure outside
[01:38:58.980 --> 01:39:02.060]   of some cities, not even all cities.
[01:39:02.060 --> 01:39:08.340]   You don't have the infrastructure that you need in order to get the whole country to
[01:39:08.340 --> 01:39:10.540]   benefit from the...
[01:39:10.540 --> 01:39:13.460]   I mean, it's not even to benefit from the internet.
[01:39:13.460 --> 01:39:20.820]   If you don't have decent internet connection, then your town, region, country side, whatever,
[01:39:20.820 --> 01:39:22.420]   is not going to develop today.
[01:39:22.420 --> 01:39:23.860]   It's just not going to happen.
[01:39:23.860 --> 01:39:30.060]   And the stranglehold that your ISPs have on the infrastructure is damaging the country.
[01:39:30.060 --> 01:39:33.300]   If you want to talk about monopolies and tech, that's where people should be looking
[01:39:33.300 --> 01:39:34.300]   at.
[01:39:34.300 --> 01:39:35.300]   Really good point.
[01:39:35.300 --> 01:39:36.300]   That is a mess.
[01:39:36.300 --> 01:39:42.100]   And in fact, we're seeing very increased aggression from the big ISPs, the Comcast,
[01:39:42.100 --> 01:39:45.500]   AT&T and Verizon, against municipal Wi-Fi.
[01:39:45.500 --> 01:39:51.020]   They're actively suing their pursuing legislative agendas to keep cities from creating their
[01:39:51.020 --> 01:39:54.060]   own Wi-Fi in areas where they're not well served.
[01:39:54.060 --> 01:39:57.260]   It is a very aggressive...
[01:39:57.260 --> 01:39:59.260]   That is outrageous.
[01:39:59.260 --> 01:40:03.420]   And I have in Finland here an example of...
[01:40:03.420 --> 01:40:09.420]   The government is actually subsidizing, to an extent, the development of municipal communal
[01:40:09.420 --> 01:40:15.980]   associative ISP development.
[01:40:15.980 --> 01:40:26.100]   And so we have one village where my wife's family has their house that dug fiber from
[01:40:26.100 --> 01:40:30.700]   like 20 kilometers away, maybe even more, to the next town over.
[01:40:30.700 --> 01:40:37.380]   And that got a tiny village connected against the better efforts of the incumbent ISPs.
[01:40:37.380 --> 01:40:41.340]   But because the government was supporting them, they managed to do it.
[01:40:41.340 --> 01:40:44.660]   And now you have fiber in the middle of nowhere.
[01:40:44.660 --> 01:40:51.580]   And people, if they wish to do that, they can go and work from those tiny villages that
[01:40:51.580 --> 01:40:55.220]   are dying all over the world.
[01:40:55.220 --> 01:40:57.220]   And if you don't have that, it's not going to happen.
[01:40:57.220 --> 01:41:02.500]   Who is going to go work in any small town that doesn't have fiber today?
[01:41:02.500 --> 01:41:07.300]   Well, and then to kind of bring it back to the earlier conversation, then you see a
[01:41:07.300 --> 01:41:14.660]   company like Facebook propose something like internet.org, which brings an internet to
[01:41:14.660 --> 01:41:18.380]   developing nations to underserved communities.
[01:41:18.380 --> 01:41:20.140]   But it's a special kind of internet.
[01:41:20.140 --> 01:41:22.380]   It's Facebook's internet.
[01:41:22.380 --> 01:41:24.020]   And it isn't the free and open internet.
[01:41:24.020 --> 01:41:25.860]   Talk about the splintering of the internet.
[01:41:25.860 --> 01:41:26.860]   Yeah.
[01:41:26.860 --> 01:41:27.860]   Yeah.
[01:41:27.860 --> 01:41:34.820]   Ben Thompson always astute rights and stratecary about Facebook in this regard.
[01:41:34.820 --> 01:41:42.060]   They are, of course, acquiring now a very popular, I never heard of it, but it's a student social
[01:41:42.060 --> 01:41:46.740]   network called TBH, which of course is the acronym for, to be honest.
[01:41:46.740 --> 01:41:52.020]   It's a polling system that students use, five million downloads, two and a half million
[01:41:52.020 --> 01:41:55.140]   daily active users.
[01:41:55.140 --> 01:42:02.940]   And Ben says the FTC needs to step in and not let Facebook acquire it.
[01:42:02.940 --> 01:42:08.700]   He says they should have stepped in when Facebook tried to acquire WhatsApp and Instagram because
[01:42:08.700 --> 01:42:15.300]   essentially Facebook's strategy is the minute any social network threatens Facebook, acquire
[01:42:15.300 --> 01:42:16.300]   it.
[01:42:16.300 --> 01:42:18.220]   In fact, they weren't able to or copy it.
[01:42:18.220 --> 01:42:21.060]   If they can't acquire it, they tried to acquire Snapchat and couldn't.
[01:42:21.060 --> 01:42:23.300]   So they made sure Instagram was a Snapchat clone.
[01:42:23.300 --> 01:42:27.660]   And in fact, Instagram seems to have stolen some of Snapchat's thunder, at least with an
[01:42:27.660 --> 01:42:30.020]   older audience.
[01:42:30.020 --> 01:42:35.180]   He says this is a failure of the federal government to block this kind of thing.
[01:42:35.180 --> 01:42:41.660]   But maybe because he's based in Taiwan, but I don't know if he understands how unlikely
[01:42:41.660 --> 01:42:45.700]   it is, at least under the current regime, that we're going to see any intervention at
[01:42:45.700 --> 01:42:47.660]   all in this kind of thing.
[01:42:47.660 --> 01:42:52.180]   Certainly not an acquisition of a small company like TBH.
[01:42:52.180 --> 01:42:53.780]   It isn't just the current regime.
[01:42:53.780 --> 01:43:02.100]   So I think one of the great failings of our various government agencies is that we don't
[01:43:02.100 --> 01:43:06.380]   have enough in a lot of ways they don't.
[01:43:06.380 --> 01:43:09.820]   First of all, nobody is planning out far in advance.
[01:43:09.820 --> 01:43:14.780]   There's no substantive data-driven long-term planning in part.
[01:43:14.780 --> 01:43:18.940]   That's because we got Newt Gingrich in the '90s defunded the agency that was responsible
[01:43:18.940 --> 01:43:19.940]   for doing that.
[01:43:19.940 --> 01:43:21.580]   It was called the Office of Technology Assessment.
[01:43:21.580 --> 01:43:26.580]   This was such a brilliant thing that was really helping people, the legislators understand
[01:43:26.580 --> 01:43:28.380]   technology.
[01:43:28.380 --> 01:43:29.380]   It was nonpartisan.
[01:43:29.380 --> 01:43:34.020]   It became the gold standard around the world, and all kinds of countries everywhere else
[01:43:34.020 --> 01:43:36.180]   have that office modeled off of ours.
[01:43:36.180 --> 01:43:43.580]   So what we're left with is a group of rotating commissioners who are political appointees,
[01:43:43.580 --> 01:43:45.780]   and the USPTO, the Patent and Trademark Office.
[01:43:45.780 --> 01:43:52.540]   Unfortunately, a lot of the future of our technology now is being decided by whoever
[01:43:52.540 --> 01:44:03.900]   happens to be the commissioner du jour in those offices who don't have enough restrictions
[01:44:03.900 --> 01:44:10.180]   on how quickly those people can go back and work in the private sector and the lobbying
[01:44:10.180 --> 01:44:12.860]   that they can do after the fact or before the fact.
[01:44:12.860 --> 01:44:17.420]   So then there's kind of a revolving door.
[01:44:17.420 --> 01:44:18.420]   I will say this.
[01:44:18.420 --> 01:44:19.580]   I am a capitalist.
[01:44:19.580 --> 01:44:22.460]   I believe everybody should make money.
[01:44:22.460 --> 01:44:27.500]   I believe everybody should have the ability to make money.
[01:44:27.500 --> 01:44:33.780]   But I also think that the interests of Silicon Valley butt up against necessarily, because
[01:44:33.780 --> 01:44:41.780]   they are commercial entities, that they rub up against the long-term interests of democracy
[01:44:41.780 --> 01:44:42.780]   in a lot of ways.
[01:44:42.780 --> 01:44:47.380]   So, better to say society, the people.
[01:44:47.380 --> 01:44:53.260]   And this is the problem with capitalism is it's a good system, but sometimes the needs
[01:44:53.260 --> 01:44:58.140]   of society are not well served by a pure market.
[01:44:58.140 --> 01:44:59.140]   And it's fine.
[01:44:59.140 --> 01:45:00.140]   It's not a value judgment.
[01:45:00.140 --> 01:45:04.820]   It's just like, you know, they are companies that are publicly traded, right?
[01:45:04.820 --> 01:45:08.700]   So they have different interests than the FTC and the FCC.
[01:45:08.700 --> 01:45:09.700]   Absolutely.
[01:45:09.700 --> 01:45:10.700]   They're profit focused.
[01:45:10.700 --> 01:45:11.700]   That's right.
[01:45:11.700 --> 01:45:20.420]   Value agencies don't have enough sort of long-term, you know, they are looking at like, like,
[01:45:20.420 --> 01:45:27.060]   they are not, first of all, they don't quite get what a lot of these companies do.
[01:45:27.060 --> 01:45:33.620]   And they don't really have the nonpartisan people who are there who can help them make
[01:45:33.620 --> 01:45:34.620]   informed decisions.
[01:45:34.620 --> 01:45:36.980]   And ultimately the commissioner is come and go.
[01:45:36.980 --> 01:45:41.860]   And they come and go back to the private sector where there are commercial interests in place.
[01:45:41.860 --> 01:45:42.860]   So...
[01:45:42.860 --> 01:45:44.460]   Here's one of the things...
[01:45:44.460 --> 01:45:45.900]   Just to be a contrarian for a sake.
[01:45:45.900 --> 01:45:47.820]   Sure, Patrick, go ahead.
[01:45:47.820 --> 01:45:50.020]   Just to be a contrarian for just a second.
[01:45:50.020 --> 01:45:53.660]   And let's put it like that.
[01:45:53.660 --> 01:45:54.740]   I'm a socialist.
[01:45:54.740 --> 01:45:59.100]   I come from a socialist country, not in the term, in the sense that you understand socialism,
[01:45:59.100 --> 01:46:04.580]   but you know, France is certainly on the left end of the political spectrum.
[01:46:04.580 --> 01:46:09.820]   If you want to look at the world, well, the Western world.
[01:46:09.820 --> 01:46:18.180]   But even with everything you're describing, which I agree is very concerning, the reality
[01:46:18.180 --> 01:46:25.660]   is that the tech sector in America has not only outperformed every other tech sector
[01:46:25.660 --> 01:46:34.380]   in the world, but basically driven the world very successfully into beneficial revolutions
[01:46:34.380 --> 01:46:35.900]   in those areas.
[01:46:35.900 --> 01:46:38.100]   And they're all American.
[01:46:38.100 --> 01:46:43.060]   All, well, yeah, mostly American and all in the Silicon Valley.
[01:46:43.060 --> 01:46:44.060]   There...
[01:46:44.060 --> 01:46:52.100]   I feel like I need to say it because it seems like it can't be a random chance, right?
[01:46:52.100 --> 01:46:55.540]   That system has spawned those giants that were successful.
[01:46:55.540 --> 01:46:56.540]   I agree.
[01:46:56.540 --> 01:46:57.540]   And that we...
[01:46:57.540 --> 01:47:00.540]   I was the premise really at the beginning of the conversation as we get great benefits
[01:47:00.540 --> 01:47:03.060]   from Silicon Valley.
[01:47:03.060 --> 01:47:09.300]   But maybe it's time for some judicious intervention.
[01:47:09.300 --> 01:47:15.620]   Just for instance, one of the things Ben says, if the FTC wanted to, and they won't do this,
[01:47:15.620 --> 01:47:21.420]   but it would be great if they made a condition on the acquisition of TBH, that Facebook be
[01:47:21.420 --> 01:47:23.460]   open up the social graph.
[01:47:23.460 --> 01:47:31.220]   So one of the things that is the case is that all social networks kind of lock in your
[01:47:31.220 --> 01:47:32.220]   friend list.
[01:47:32.220 --> 01:47:36.660]   You can't export your friend list from Facebook to another network.
[01:47:36.660 --> 01:47:40.300]   Instagram launched off the back of Twitter.
[01:47:40.300 --> 01:47:43.740]   Many of networks launched off the back of Twitter, but of course the minute Facebook
[01:47:43.740 --> 01:47:46.780]   acquired it, it shut that down.
[01:47:46.780 --> 01:47:52.300]   And all social networks have long since made social graph portability impossible, he writes,
[01:47:52.300 --> 01:47:54.980]   making it much more difficult for competitors to arise.
[01:47:54.980 --> 01:47:58.820]   There are things, there are tweaks, I think you'd agree, Patrick, that government could
[01:47:58.820 --> 01:48:01.900]   make not to disassemble these giant companies.
[01:48:01.900 --> 01:48:07.700]   I don't think that's proper either, but to make them a little more societally aware in
[01:48:07.700 --> 01:48:09.460]   the face of profits.
[01:48:09.460 --> 01:48:11.820]   Hey, let's take a look at that.
[01:48:11.820 --> 01:48:13.340]   Hold that thought Amy.
[01:48:13.340 --> 01:48:15.540]   I want to take a break because we're behind.
[01:48:15.540 --> 01:48:18.060]   And this is such a good conversation and such a great panel.
[01:48:18.060 --> 01:48:19.060]   I want to keep going.
[01:48:19.060 --> 01:48:25.180]   Amy Webb is here futurist from a future Institute, future today Institute.
[01:48:25.180 --> 01:48:31.300]   Also, the author of the signals are talking Patrick Bejah from the Philius Club and Le
[01:48:31.300 --> 01:48:37.380]   Honde vu Tech and from Mashable Deputy Tech Editor Michael Nunez.
[01:48:37.380 --> 01:48:41.660]   We're talking today about backup, carbonite backup.
[01:48:41.660 --> 01:48:42.660]   You've heard the name before.
[01:48:42.660 --> 01:48:44.700]   I've talked about carbonite for more than a decade.
[01:48:44.700 --> 01:48:48.620]   I've used carbonite for that long for home or office, but I want you to take a look at
[01:48:48.620 --> 01:48:51.260]   the new carbonite website because it's a new carbonite.
[01:48:51.260 --> 01:48:57.980]   They have really evolved into so much more than just a backup, an online cloud backup
[01:48:57.980 --> 01:48:58.980]   service.
[01:48:58.980 --> 01:49:04.620]   You'll see the data protection experts and you'll see that in the new website.
[01:49:04.620 --> 01:49:06.060]   You'll see that they've expanded.
[01:49:06.060 --> 01:49:10.700]   They've acquired companies like Double Take and Evolve to give you more high availability
[01:49:10.700 --> 01:49:16.460]   options to create a data platform, a data protection platform that supports all of your
[01:49:16.460 --> 01:49:17.460]   business.
[01:49:17.460 --> 01:49:21.780]   For instance, keeping you from being locked in in any particular cloud, making it easy
[01:49:21.780 --> 01:49:27.980]   to move your data around solutions for high availability.
[01:49:27.980 --> 01:49:32.700]   In a business where downtime is not an option, you can get back in seconds.
[01:49:32.700 --> 01:49:35.580]   End point backup, migration, server backup.
[01:49:35.580 --> 01:49:37.980]   It's all there at carbonite.
[01:49:37.980 --> 01:49:42.940]   There's a plan for you, whether it's just a single per computer in your home or your
[01:49:42.940 --> 01:49:44.860]   big servers in your data center.
[01:49:44.860 --> 01:49:48.380]   They could, yes, data center, backup and disaster recovery.
[01:49:48.380 --> 01:49:49.980]   They do it all.
[01:49:49.980 --> 01:49:52.180]   I want to encourage you to visit carbonite.
[01:49:52.180 --> 01:49:55.700]   If you want to try the individual solutions, just make sure you use the offer code TWIT
[01:49:55.700 --> 01:49:58.140]   to get two months free if you decide to buy.
[01:49:58.140 --> 01:50:03.540]   If you're running a big enterprise or a medium or small enterprise and you can't afford to
[01:50:03.540 --> 01:50:08.260]   lose that very important data that runs your business.
[01:50:08.260 --> 01:50:13.620]   If you want complete protection, all kinds of solutions, including disaster recovery
[01:50:13.620 --> 01:50:17.580]   as a service, I want you to rethink carbonite.
[01:50:17.580 --> 01:50:21.220]   Carbonite.com.
[01:50:21.220 --> 01:50:26.580]   They've got the solutions for you, including their hybrid backup solutions powered by
[01:50:26.580 --> 01:50:27.780]   E-Vault.
[01:50:27.780 --> 01:50:28.940]   I love them.
[01:50:28.940 --> 01:50:29.940]   Carbonite.com.
[01:50:29.940 --> 01:50:32.780]   If you do want to do the free trial, use the offer code TWIT just to let them know you
[01:50:32.780 --> 01:50:34.020]   heard about it here.
[01:50:34.020 --> 01:50:35.500]   You got to back it up to get it back.
[01:50:35.500 --> 01:50:40.140]   Do it right with the data protection specialist, carbonite.com.
[01:50:40.140 --> 01:50:41.540]   Amy Webb, you started.
[01:50:41.540 --> 01:50:44.380]   I hope you didn't forget what we were talking about.
[01:50:44.380 --> 01:50:47.060]   I didn't want, I started interrupt, but I just want to get that in.
[01:50:47.060 --> 01:50:48.060]   Go ahead.
[01:50:48.060 --> 01:50:52.740]   Okay, so here's my thought on this.
[01:50:52.740 --> 01:50:57.300]   What we were talking about, do you need to recap what we were talking about or jump
[01:50:57.300 --> 01:50:58.300]   in?
[01:50:58.300 --> 01:51:01.940]   Well, we're talking about the fact that these frightful five and maybe it's eight if you
[01:51:01.940 --> 01:51:07.460]   include some Chinese companies are so dominant, they certainly, and I think we agree, have
[01:51:07.460 --> 01:51:08.740]   given us a huge benefit.
[01:51:08.740 --> 01:51:13.060]   We have, as Patrick pointed out, the innovation that came out of Silicon Valley has powered
[01:51:13.060 --> 01:51:16.060]   not just American innovation, but the world.
[01:51:16.060 --> 01:51:18.060]   My suggestion is that's fine.
[01:51:18.060 --> 01:51:21.140]   I don't want to ignore that and I want to throw out the baby with the bath water.
[01:51:21.140 --> 01:51:26.700]   We need these companies, but we also, it's appropriate, I think, for judicious governmental
[01:51:26.700 --> 01:51:32.180]   regulation to make sure it's an even fair playing field and innovation can continue.
[01:51:32.180 --> 01:51:33.900]   That's where we left it off.
[01:51:33.900 --> 01:51:35.900]   So here's my thought.
[01:51:35.900 --> 01:51:42.780]   As with any technology, the easier it is for us to use something, the closer that it resembles
[01:51:42.780 --> 01:51:47.700]   magic, the less we think about it.
[01:51:47.700 --> 01:51:54.260]   And that's what Arthur C. Clark would say is the point.
[01:51:54.260 --> 01:51:57.740]   If the technology is like magic because it works well and we don't even have to think
[01:51:57.740 --> 01:51:59.140]   about it, then that's great.
[01:51:59.140 --> 01:52:05.020]   Any sufficiently advanced technology is indistinguishable from magic.
[01:52:05.020 --> 01:52:13.020]   However, we need to, and I'm defining we very broadly here, but I think we all need to spend
[01:52:13.020 --> 01:52:16.300]   a little bit more time thinking about how that magic trick works.
[01:52:16.300 --> 01:52:17.620]   How do they do the magic?
[01:52:17.620 --> 01:52:22.060]   Which is not to say that we all need to go learn how to make a deep neural network or
[01:52:22.060 --> 01:52:31.380]   something like that, but this sort of delta is widening between us, us consumers and us
[01:52:31.380 --> 01:52:39.700]   and the givers of our data and the companies that ultimately mine harvest and refine it
[01:52:39.700 --> 01:52:42.940]   and use it.
[01:52:42.940 --> 01:52:50.500]   And as a result of that, I just wonder that this analogy doesn't quite fit, but almost
[01:52:50.500 --> 01:52:58.500]   in the same way that we establish national parks is a way to preserve land.
[01:52:58.500 --> 01:53:01.940]   I'm wondering if there's a national park solution for our data.
[01:53:01.940 --> 01:53:09.380]   Is there a way for me to somehow either retain all ownership and rights of my data?
[01:53:09.380 --> 01:53:13.140]   So if I'm using somebody else's platform, I still own, and I know that you can download
[01:53:13.140 --> 01:53:17.460]   your data afterwards, but that's not the same, downloading it is not the same thing as owning
[01:53:17.460 --> 01:53:18.460]   it, right?
[01:53:18.460 --> 01:53:24.580]   Because I don't get to see how Russian botnets or something looked at my data when they were
[01:53:24.580 --> 01:53:26.980]   on Facebook, right?
[01:53:26.980 --> 01:53:34.420]   So I'm just wondering, is there a way for me to have sort of portable ownership ability
[01:53:34.420 --> 01:53:35.740]   or some control over my data?
[01:53:35.740 --> 01:53:41.140]   Or if not, maybe there's some kind of governance structure where everybody who's putting data
[01:53:41.140 --> 01:53:45.740]   into the system has some say in how that data is being used.
[01:53:45.740 --> 01:53:49.900]   Now you could argue that that could completely slow down innovation.
[01:53:49.900 --> 01:53:52.100]   It's too complicated.
[01:53:52.100 --> 01:53:57.260]   There's all kinds of arguments against it, but there is no denying that we are getting
[01:53:57.260 --> 01:54:03.860]   further and further away from the sort of core of how our technology works.
[01:54:03.860 --> 01:54:10.180]   And that's important because the more that we press forward with AI, and as we move from
[01:54:10.180 --> 01:54:15.980]   AI, artificial neural intelligence, artificial general intelligence, and all of the technology
[01:54:15.980 --> 01:54:23.660]   and everything else that go along with it, one of the challenges is to me it's concerning
[01:54:23.660 --> 01:54:28.780]   because we have fewer and fewer people making decisions about what the future of our technology
[01:54:28.780 --> 01:54:36.620]   looks like, the corpus, the data sets that are being used, the algorithms that are used
[01:54:36.620 --> 01:54:37.900]   that use that data.
[01:54:37.900 --> 01:54:40.780]   And it's like we're sort of getting further and further removed.
[01:54:40.780 --> 01:54:45.140]   So the National Park illustration doesn't quite fit, but it's in that vein.
[01:54:45.140 --> 01:54:50.340]   I'm just wondering if there's a way for us to truly own our data.
[01:54:50.340 --> 01:54:55.140]   And if not, then let's all give up our data, but get something more meaningful in return
[01:54:55.140 --> 01:55:01.420]   aside from the ability to put a stupid avatar on our Snapchat, whatever is for these five
[01:55:01.420 --> 01:55:03.260]   minutes before something else.
[01:55:03.260 --> 01:55:11.100]   The new data protection law, the GDPR that is being planned is about to...
[01:55:11.100 --> 01:55:12.100]   Is that GDPR?
[01:55:12.100 --> 01:55:13.100]   Yeah.
[01:55:13.100 --> 01:55:14.100]   Yeah.
[01:55:14.100 --> 01:55:18.260]   That's something along the lines of what you're thinking.
[01:55:18.260 --> 01:55:22.460]   So tell me about that, Patrick, because you're there.
[01:55:22.460 --> 01:55:24.140]   Exactly.
[01:55:24.140 --> 01:55:36.700]   Basically it's forcefully making every company care about privacy and data and protecting
[01:55:36.700 --> 01:55:37.700]   the...
[01:55:37.700 --> 01:55:39.820]   Does it give us our data back?
[01:55:39.820 --> 01:55:44.660]   Does it give us control of it or not so much?
[01:55:44.660 --> 01:55:47.660]   It restricts the use of data that I know of.
[01:55:47.660 --> 01:55:48.660]   I'm not an expert.
[01:55:48.660 --> 01:55:53.180]   So that's problematic because I think I agree with what Amy wants, which is we need to be
[01:55:53.180 --> 01:55:55.940]   able to have somehow control it.
[01:55:55.940 --> 01:56:01.580]   So I mean, you control it in the sense that you consent to its users more and you have
[01:56:01.580 --> 01:56:05.900]   more trust in the fact that it is better protected.
[01:56:05.900 --> 01:56:12.740]   But the irony of this, and I'm sorry, I can't find the article, but it was really interesting.
[01:56:12.740 --> 01:56:20.660]   Basically the irony is that it further entrenches companies like Facebook and Google that already
[01:56:20.660 --> 01:56:26.140]   have all of the data and the social graphs and all of those because those are much more
[01:56:26.140 --> 01:56:28.300]   difficult to be shared now.
[01:56:28.300 --> 01:56:35.860]   So it's really hard for another company to come up with those social graphs and if so
[01:56:35.860 --> 01:56:40.780]   clever uses of our data because they can't acquire them as easily.
[01:56:40.780 --> 01:56:48.420]   And so this basically starting with a good intention might end up furthering the power
[01:56:48.420 --> 01:56:50.380]   of those huge companies.
[01:56:50.380 --> 01:56:51.380]   Interesting.
[01:56:51.380 --> 01:56:52.860]   So I don't know.
[01:56:52.860 --> 01:56:55.540]   It requires just a couple of the bullet points.
[01:56:55.540 --> 01:57:00.900]   The highlights requires consent for data, explicit consent for data collected and information
[01:57:00.900 --> 01:57:03.660]   about how it will be used protects children.
[01:57:03.660 --> 01:57:08.580]   There will be a data protection officer who will monitor this.
[01:57:08.580 --> 01:57:16.780]   The GDPR requires something called pseudonymization, which is basically anonymizing data.
[01:57:16.780 --> 01:57:20.820]   Encryption is an example of pseudonymization.
[01:57:20.820 --> 01:57:23.540]   Data breach notifications are required.
[01:57:23.540 --> 01:57:24.820]   That's something we don't have in the US.
[01:57:24.820 --> 01:57:26.620]   We sure ought to have.
[01:57:26.620 --> 01:57:31.100]   There's some, the right to be forgotten is incorporated in it as well.
[01:57:31.100 --> 01:57:38.980]   Although it's a more limited right to erasure, which isn't, I guess, quite as sweeping.
[01:57:38.980 --> 01:57:43.220]   Data portability, just like we were talking about, you should be able to transfer your
[01:57:43.220 --> 01:57:47.860]   personal data from one system to another without being prevented from doing.
[01:57:47.860 --> 01:57:53.580]   So data, I like this data protection by design and by default.
[01:57:53.580 --> 01:57:59.460]   So, so much in the US, we get things like Equifax who does decides to protect your data
[01:57:59.460 --> 01:58:03.300]   after it's already been released to the wild.
[01:58:03.300 --> 01:58:04.980]   All of these things seem like good things.
[01:58:04.980 --> 01:58:07.900]   And yet I think there's considerable concern as certainly in the United States among these
[01:58:07.900 --> 01:58:13.700]   big corporations about GDPR and how to implement it and whether it's going to be burdensome,
[01:58:13.700 --> 01:58:14.700]   whether it's going to be burdensome.
[01:58:14.700 --> 01:58:15.700]   Of course, it'll be burdensome.
[01:58:15.700 --> 01:58:16.700]   But the burden is on.
[01:58:16.700 --> 01:58:17.700]   That's the whole point.
[01:58:17.700 --> 01:58:18.700]   Yeah.
[01:58:18.700 --> 01:58:19.700]   Right.
[01:58:19.700 --> 01:58:20.700]   That's the whole point.
[01:58:20.700 --> 01:58:21.700]   It should be burdensome.
[01:58:21.700 --> 01:58:25.900]   But we also need to be okay being burdened.
[01:58:25.900 --> 01:58:27.900]   I guess that's my point.
[01:58:27.900 --> 01:58:33.140]   You know, I so I have a Tesla and I love the fact that the Tesla can whatever, Auto Park
[01:58:33.140 --> 01:58:37.540]   works and, you know, sell like the pilot function works.
[01:58:37.540 --> 01:58:41.540]   And I don't know exactly how every single component of the car works, but I'm very invested
[01:58:41.540 --> 01:58:47.060]   in learning every single thing that I can about every single update, which is different
[01:58:47.060 --> 01:58:51.140]   than me just getting in the car and turning on autopilot and going somewhere.
[01:58:51.140 --> 01:58:55.620]   Now, I'm a different kind of person because I work with technology for a lot of things.
[01:58:55.620 --> 01:58:59.780]   I mean, like you read before you update, you read all the things it's going to do or you
[01:58:59.780 --> 01:59:01.700]   read the terms of you do really.
[01:59:01.700 --> 01:59:02.700]   Yeah.
[01:59:02.700 --> 01:59:03.700]   Yeah.
[01:59:03.700 --> 01:59:06.660]   I mean, the updates get pushed so it's not like it's hard to sort of do it sort of after
[01:59:06.660 --> 01:59:07.660]   the fact.
[01:59:07.660 --> 01:59:08.660]   Yeah.
[01:59:08.660 --> 01:59:10.420]   I got pushed an update that broke my garage door opener.
[01:59:10.420 --> 01:59:11.780]   I'm not really happy about that.
[01:59:11.780 --> 01:59:16.260]   I called them and it's a widespread problem and they haven't fixed it yet.
[01:59:16.260 --> 01:59:19.340]   I asked the guy, I said, yeah, we've just started putting out just pushing out the fix,
[01:59:19.340 --> 01:59:20.340]   but we do it in bunches.
[01:59:20.340 --> 01:59:22.620]   So you won't have it for a while.
[01:59:22.620 --> 01:59:27.340]   You know, it's you're both exemplifying why we need the governments to be involved in
[01:59:27.340 --> 01:59:28.340]   this.
[01:59:28.340 --> 01:59:32.420]   And I understand I'm talking to mostly American audience and going against what I was sort
[01:59:32.420 --> 01:59:35.780]   of implying by being a contrarian earlier.
[01:59:35.780 --> 01:59:38.700]   But this is the only way these things work.
[01:59:38.700 --> 01:59:45.020]   If you don't have the government, which by the way is by design, this is not going to
[01:59:45.020 --> 01:59:50.620]   be popular in the US, but it is representing the common interest of the people.
[01:59:50.620 --> 01:59:51.620]   That's what it's supposed to do.
[01:59:51.620 --> 01:59:53.620]   It's supposed to private companies.
[01:59:53.620 --> 01:59:54.620]   Yeah.
[01:59:54.620 --> 01:59:56.420]   Here in the US it might work the other way.
[01:59:56.420 --> 01:59:59.420]   But I agree with you.
[01:59:59.420 --> 02:00:01.740]   Amazon, here's a good one.
[02:00:01.740 --> 02:00:03.220]   How do you think about this?
[02:00:03.220 --> 02:00:09.620]   Amazon wants to offer a service that lets the delivery person inside your house because
[02:00:09.620 --> 02:00:11.580]   here's the problem in the United States.
[02:00:11.580 --> 02:00:13.860]   Packages get stolen off of porches all the time.
[02:00:13.860 --> 02:00:15.300]   It's a big problem.
[02:00:15.300 --> 02:00:18.700]   So it's only currently available in areas where Amazon does the delivery.
[02:00:18.700 --> 02:00:24.860]   They're not going to let the UPS or FedEx or the postal service driver in your house.
[02:00:24.860 --> 02:00:31.740]   But you have to spend $250 on a special door lock and the cloud cam, a special camera,
[02:00:31.740 --> 02:00:37.020]   because they want a camera watching the guy or gal as they bake the delivery.
[02:00:37.020 --> 02:00:40.660]   But if you're willing to spend that $250, then you don't have to worry about packages
[02:00:40.660 --> 02:00:41.820]   on your doorstep anymore.
[02:00:41.820 --> 02:00:44.860]   They're just going to let you write in.
[02:00:44.860 --> 02:00:45.860]   Why not?
[02:00:45.860 --> 02:00:48.900]   If you have a house anyway, why not put a giant post box?
[02:00:48.900 --> 02:00:49.900]   Put a box in front of it.
[02:00:49.900 --> 02:00:50.900]   Put a box.
[02:00:50.900 --> 02:00:51.900]   Put a box.
[02:00:51.900 --> 02:00:52.900]   Put a box.
[02:00:52.900 --> 02:00:53.900]   Put a box.
[02:00:53.900 --> 02:00:54.900]   Well, that's a good question.
[02:00:54.900 --> 02:00:55.900]   That's a very good question.
[02:00:55.900 --> 02:00:56.860]   Why wouldn't Amazon just do that?
[02:00:56.860 --> 02:01:01.260]   And I have a thought on that, which is it goes back to data.
[02:01:01.260 --> 02:01:04.980]   So yes, on the one.
[02:01:04.980 --> 02:01:05.980]   There are other ways to solve it.
[02:01:05.980 --> 02:01:10.940]   But what Amazon's most interested in is getting a camera in your house.
[02:01:10.940 --> 02:01:11.940]   And collecting.
[02:01:11.940 --> 02:01:14.660]   I don't think surveilling every single thing that we're doing.
[02:01:14.660 --> 02:01:22.260]   But Amazon is this absolutely one of the smartest companies that we track when it comes to understand
[02:01:22.260 --> 02:01:27.500]   very, very clever use of data, like very clever use of data.
[02:01:27.500 --> 02:01:36.020]   This whole thing with the trucks that I mentioned and coal, I don't think that Amazon is trying
[02:01:36.020 --> 02:01:40.380]   to get into the logistics business specifically to help coal miners.
[02:01:40.380 --> 02:01:46.540]   Isn't it more possible or probable that Amazon assumes that or it's trying to build itself
[02:01:46.540 --> 02:01:51.700]   into the company that will sort of be the cloud for all of the various connected car
[02:01:51.700 --> 02:01:53.200]   companies.
[02:01:53.200 --> 02:01:58.740]   All of these car manufacturers have promised self-driving fleets, but Ford doesn't have
[02:01:58.740 --> 02:02:03.340]   its own, you know, clouds for it.
[02:02:03.340 --> 02:02:04.340]   Right?
[02:02:04.340 --> 02:02:05.340]   It's able to process all of the information.
[02:02:05.340 --> 02:02:07.740]   You don't have the data and you don't have a way to gather it.
[02:02:07.740 --> 02:02:08.740]   You're at a lot.
[02:02:08.740 --> 02:02:09.740]   That's right.
[02:02:09.740 --> 02:02:10.740]   Isn't it possible?
[02:02:10.740 --> 02:02:15.820]   So every time Amazon launches something new in the back of my mind, you know, I treat all
[02:02:15.820 --> 02:02:21.500]   of these product launches as weak signals and weak signals are what futurists use to sort
[02:02:21.500 --> 02:02:22.500]   of spot pad.
[02:02:22.500 --> 02:02:23.500]   Right.
[02:02:23.500 --> 02:02:27.260]   So is this really about making it so that fewer people steal your packages?
[02:02:27.260 --> 02:02:28.260]   Right.
[02:02:28.260 --> 02:02:29.260]   I don't think so.
[02:02:29.260 --> 02:02:30.260]   To me, this is a weak signal.
[02:02:30.260 --> 02:02:31.260]   Same thing with the look.
[02:02:31.260 --> 02:02:32.260]   Yeah.
[02:02:32.260 --> 02:02:33.260]   Yeah.
[02:02:33.260 --> 02:02:34.260]   Same thing with the look.
[02:02:34.260 --> 02:02:36.900]   And what we don't know, because it's not our business and we're not data scientists,
[02:02:36.900 --> 02:02:43.540]   is how these little bits of data, how valuable they can be, you might say, well, you know,
[02:02:43.540 --> 02:02:48.940]   Amazon knows when you got a delivery and whether you were home and who cares.
[02:02:48.940 --> 02:02:53.500]   But what I think is what you just don't understand the great value of gathering this
[02:02:53.500 --> 02:02:58.660]   massive data has and the interesting unique ways they can be used.
[02:02:58.660 --> 02:03:03.820]   This is, I mean, Amazon doesn't want to help me choose what outfit to wear.
[02:03:03.820 --> 02:03:06.340]   There's no percentage in that.
[02:03:06.340 --> 02:03:09.180]   I think they're up to something.
[02:03:09.180 --> 02:03:14.820]   There's a marginal benefit to the company for the Amazon key thing and for the look and
[02:03:14.820 --> 02:03:16.620]   for all of those.
[02:03:16.620 --> 02:03:26.020]   But certainly the data is, I mean, it's the gold and the fuel and the petrol, you know,
[02:03:26.020 --> 02:03:27.420]   the next version of that.
[02:03:27.420 --> 02:03:29.740]   And it keeps coming back, right?
[02:03:29.740 --> 02:03:33.900]   For AI, which is going to put, well, deep learning and AI, which are going to power
[02:03:33.900 --> 02:03:35.340]   everything.
[02:03:35.340 --> 02:03:36.420]   You need data.
[02:03:36.420 --> 02:03:40.260]   If you don't have the data, it just doesn't work.
[02:03:40.260 --> 02:03:46.420]   And so that's something that I think we should be more aware of your very right Amy in,
[02:03:46.420 --> 02:03:52.700]   at least in the tech industry, like tech media isn't aware enough of this, you know,
[02:03:52.700 --> 02:03:54.620]   the importance of data.
[02:03:54.620 --> 02:03:55.620]   Look at this.
[02:03:55.620 --> 02:03:58.060]   I have to do the carbonite ad.
[02:03:58.060 --> 02:04:00.980]   I went to the carbonite website to show the carbonite website.
[02:04:00.980 --> 02:04:06.660]   And now when I read this story about Amazon key on the verge, I get a carbonite ad.
[02:04:06.660 --> 02:04:08.220]   This is a perfect example.
[02:04:08.220 --> 02:04:13.300]   That wouldn't have been there two clicks ago, but it's a perfect example of how, and this
[02:04:13.300 --> 02:04:20.140]   is from Google, how Google, and I'm using the Microsoft Edge browser, but the circle
[02:04:20.140 --> 02:04:23.620]   is like this of data coming and going and flowing.
[02:04:23.620 --> 02:04:27.020]   And, you know, in this case, it's just showing me an ad of something I'm already shown and
[02:04:27.020 --> 02:04:28.020]   interested.
[02:04:28.020 --> 02:04:29.020]   And that's not a bad thing.
[02:04:29.020 --> 02:04:35.580]   But yet it just shows you how free flowing this information is.
[02:04:35.580 --> 02:04:38.140]   I didn't, Google wasn't involved in that transaction yet.
[02:04:38.140 --> 02:04:43.260]   They know exactly where I went and they are in fact putting up an ad key to what I went.
[02:04:43.260 --> 02:04:44.260]   All right.
[02:04:44.260 --> 02:04:45.260]   Let's take one more break.
[02:04:45.260 --> 02:04:48.540]   I, we got to get one more ad in and I have a couple of lightweight things because I don't
[02:04:48.540 --> 02:04:51.820]   want to end on a deep, heavy, profound note.
[02:04:51.820 --> 02:04:53.420]   I'd like to end on a fun note.
[02:04:53.420 --> 02:04:54.620]   How about that?
[02:04:54.620 --> 02:04:58.260]   Although this is kind of heavy and maybe I should do this first.
[02:04:58.260 --> 02:05:03.540]   Roger Stone got banned from Twitter, Twitter, which announced a calendar of new rules is
[02:05:03.540 --> 02:05:05.180]   apparently taking them seriously.
[02:05:05.180 --> 02:05:09.460]   Roger Stone, who is a very outspoken, in fact, you've got to see the documentary, Get Me
[02:05:09.460 --> 02:05:10.900]   Roger Stone.
[02:05:10.900 --> 02:05:18.900]   Fella has decided, decided over the weekend that Don Lemon at CNN was the devil incarnate
[02:05:18.900 --> 02:05:25.340]   and a series of vicious tweets, including saying he should be punished, have gotten
[02:05:25.340 --> 02:05:29.700]   his account banished.
[02:05:29.700 --> 02:05:31.980]   He can, Roger Stone has been suspended.
[02:05:31.980 --> 02:05:35.220]   Twitter is not saying if this is permanent, but his account is suspended.
[02:05:35.220 --> 02:05:40.500]   Stone immediately took to tweeting from the Roger Stone documentary movie account.
[02:05:40.500 --> 02:05:41.980]   So there apparently is a loophole.
[02:05:41.980 --> 02:05:43.620]   In fact, we've known this all along.
[02:05:43.620 --> 02:05:46.420]   You can always create a new account.
[02:05:46.420 --> 02:05:50.200]   So Stone says he's hired one of the best telecommunications lawyers in the country and
[02:05:50.200 --> 02:05:55.500]   will sue Twitter if it's not clear whether they're legal grounds to do so.
[02:05:55.500 --> 02:05:57.140]   He's pretty litigious.
[02:05:57.140 --> 02:05:58.140]   He'll probably do it anyway.
[02:05:58.140 --> 02:06:00.300]   Yeah, he's soothing no matter what, right?
[02:06:00.300 --> 02:06:04.220]   And he says, "I've been inundated on Twitter and this is not a bad point with bloggers threatening
[02:06:04.220 --> 02:06:06.620]   to kill me, my wife, my kids, even my dogs.
[02:06:06.620 --> 02:06:11.700]   Twitter seems unconcerned about that."
[02:06:11.700 --> 02:06:13.860]   So the battle rages.
[02:06:13.860 --> 02:06:19.380]   And one more story for Facebook, and we'll probably talk about this on Wednesday with
[02:06:19.380 --> 02:06:23.220]   Jeff Jarvis because he's our journalism guru.
[02:06:23.220 --> 02:06:31.660]   Facebook is apparently testing keeping publishers, articles out of the stream unless they pay
[02:06:31.660 --> 02:06:33.580]   for it.
[02:06:33.580 --> 02:06:35.060]   This is not in the US yet.
[02:06:35.060 --> 02:06:36.900]   It's in six countries.
[02:06:36.900 --> 02:06:42.020]   They're taking content from publishers and businesses out of the newsfeed.
[02:06:42.020 --> 02:06:48.300]   Those posts will exist in the explore feed unless the publisher pays for placement in
[02:06:48.300 --> 02:06:49.500]   the newsfeed.
[02:06:49.500 --> 02:06:50.980]   Right.
[02:06:50.980 --> 02:06:56.020]   So remember when Facebook was a place to find...
[02:06:56.020 --> 02:06:57.380]   Publish your newspaper.
[02:06:57.380 --> 02:06:58.380]   Use from your friends.
[02:06:58.380 --> 02:07:02.420]   Use from your friends and family and stuff like that.
[02:07:02.420 --> 02:07:07.500]   And then it became a place for Washington Post and Wall Street Journal put articles.
[02:07:07.500 --> 02:07:16.340]   And look at the... this is a tweet from somebody, but he's talking about the drop in organic
[02:07:16.340 --> 02:07:23.580]   reach pages of four times less interactions in countries where the feed has been modified.
[02:07:23.580 --> 02:07:25.740]   You just could see the drop.
[02:07:25.740 --> 02:07:30.660]   So that means the people using Facebook there see more of the people...
[02:07:30.660 --> 02:07:31.660]   Friends and family.
[02:07:31.660 --> 02:07:32.660]   Actually, no.
[02:07:32.660 --> 02:07:33.660]   Yeah, exactly.
[02:07:33.660 --> 02:07:40.260]   I understand the consequence, but it seems like there could be a logical explanation to
[02:07:40.260 --> 02:07:41.260]   this test.
[02:07:41.260 --> 02:07:43.820]   Oh, I understand it.
[02:07:43.820 --> 02:07:50.500]   And just underscores why if you're a business or a publication, using Facebook as your way
[02:07:50.500 --> 02:07:55.300]   of generating traffic is probably not the best idea because after all, Facebook controls
[02:07:55.300 --> 02:07:56.300]   it.
[02:07:56.300 --> 02:07:57.940]   Why would Facebook charge?
[02:07:57.940 --> 02:08:00.380]   That's what we're trying to do.
[02:08:00.380 --> 02:08:02.700]   Well, we live in the capitalist economy.
[02:08:02.700 --> 02:08:07.580]   And if they charge, they'll be like... the way you could put that.
[02:08:07.580 --> 02:08:08.580]   Wait, hang on.
[02:08:08.580 --> 02:08:09.580]   Let me ask this question.
[02:08:09.580 --> 02:08:14.060]   Here's why, because about a year ago I admonished... it doesn't matter.
[02:08:14.060 --> 02:08:19.760]   I was speaking at a conference after writing some stuff, and I said, you know, news organizations
[02:08:19.760 --> 02:08:23.100]   have no longer controlled the means for distribution.
[02:08:23.100 --> 02:08:26.980]   So everybody should get together and decide nobody's publishing anything on Facebook.
[02:08:26.980 --> 02:08:32.860]   And then Facebook will just be a wasteland of friends and family posts and stuff that
[02:08:32.860 --> 02:08:34.820]   nobody wants to read.
[02:08:34.820 --> 02:08:36.020]   They were reluctant to do that.
[02:08:36.020 --> 02:08:43.420]   Now, I'm just anecdotally, we did a study to try to figure out how much of Facebook's
[02:08:43.420 --> 02:08:47.620]   UI is dependent upon quality journalism.
[02:08:47.620 --> 02:08:49.420]   And it turns out there's quite a bit of it.
[02:08:49.420 --> 02:08:50.420]   Oh, really?
[02:08:50.420 --> 02:08:51.420]   Oh, interesting.
[02:08:51.420 --> 02:08:52.420]   Yeah.
[02:08:52.420 --> 02:08:55.620]   So I'm trying to figure out is what net benefit is there to Facebook?
[02:08:55.620 --> 02:08:58.580]   I mean, obviously they know what their leverage is right now.
[02:08:58.580 --> 02:09:01.420]   But news organizations are so strapped if they first...
[02:09:01.420 --> 02:09:02.420]   It's a good point.
[02:09:02.420 --> 02:09:06.380]   ...newzewords pay, which they can't really do, then what's their end game?
[02:09:06.380 --> 02:09:11.220]   Like why... I can't figure out what... why they... why would they be... what do you guys
[02:09:11.220 --> 02:09:12.220]   think?
[02:09:12.220 --> 02:09:13.220]   Why would they be doing that?
[02:09:13.220 --> 02:09:17.580]   Oh, they're just not going to take away the ads.
[02:09:17.580 --> 02:09:22.980]   That's the reason why they... you know, the way they put it in the article is basically
[02:09:22.980 --> 02:09:29.540]   they force you to pay in order for you to appear on your... on people's feeds as if that
[02:09:29.540 --> 02:09:34.620]   was a strategy to, you know, encourage people to pay more.
[02:09:34.620 --> 02:09:41.500]   I think the strategy is to make the feeds more personal, but at the same time, they're
[02:09:41.500 --> 02:09:43.820]   not going to turn away the ad money.
[02:09:43.820 --> 02:09:49.580]   So if you pay... you know, that would mean there's no more revenue for Facebook.
[02:09:49.580 --> 02:09:52.140]   So they keep the ads in there.
[02:09:52.140 --> 02:09:53.140]   And that's...
[02:09:53.140 --> 02:09:54.140]   That's it.
[02:09:54.140 --> 02:09:56.140]   I mean, it's a test, but yeah.
[02:09:56.140 --> 02:09:59.620]   Yeah, I think something else going on.
[02:09:59.620 --> 02:10:06.940]   Yeah, I mean, I think that for, you know, the past decade or a little more than that,
[02:10:06.940 --> 02:10:13.780]   I guess Facebook has made a lot of publications basically addicted to the Facebook news feed
[02:10:13.780 --> 02:10:16.500]   as a source of distribution.
[02:10:16.500 --> 02:10:23.620]   And now they're just... you know, I can't blame them for trying to collect on that addiction,
[02:10:23.620 --> 02:10:24.620]   right?
[02:10:24.620 --> 02:10:28.580]   Because this is a necessity for actually a lot of the largest publishers in the US,
[02:10:28.580 --> 02:10:31.460]   probably even in the world.
[02:10:31.460 --> 02:10:39.340]   And so it's really easy for Facebook to basically just flip a switch and ultimately hurt the
[02:10:39.340 --> 02:10:40.700]   bottom line for these companies.
[02:10:40.700 --> 02:10:46.460]   And so, you know, already a lot of the biggest publishers in New York, let's say, are spending
[02:10:46.460 --> 02:10:52.020]   thousands of dollars on Facebook to promote certain stories and to promote certain social
[02:10:52.020 --> 02:10:55.260]   media campaigns on a monthly basis.
[02:10:55.260 --> 02:10:56.540]   I think it's safe to say.
[02:10:56.540 --> 02:11:05.180]   And so, you know, I would just see this as Facebook sort of, you know, I think that...
[02:11:05.180 --> 02:11:09.940]   Of course, they're looking to charge publishers for distribution because this is one of the
[02:11:09.940 --> 02:11:11.980]   biggest use cases for the network right now.
[02:11:11.980 --> 02:11:17.820]   You know, people are publishing less original content and therefore Facebook's relying on
[02:11:17.820 --> 02:11:20.780]   publishers for a lot of that content.
[02:11:20.780 --> 02:11:26.060]   And one way that you can, you know, quickly increase revenue is by making those publishers
[02:11:26.060 --> 02:11:27.940]   pay to have their content seen.
[02:11:27.940 --> 02:11:35.460]   Because a lot of them already are doing a version of that, which is paying, you know, a smaller
[02:11:35.460 --> 02:11:41.580]   amount to have certain stories promoted and boosted a little bit.
[02:11:41.580 --> 02:11:43.620]   And so, you know, this is just...
[02:11:43.620 --> 02:11:46.020]   It's the same model but just applaud in a different way, in my opinion.
[02:11:46.020 --> 02:11:51.460]   Matthew Inman, the cartoonist at the Oatmeal has a very trenchant point to make.
[02:11:51.460 --> 02:11:55.580]   He published a cartoon reaching people on the internet how it used to be.
[02:11:55.580 --> 02:11:56.580]   Come on over.
[02:11:56.580 --> 02:11:58.340]   I've got some neat stuff here at my website.
[02:11:58.340 --> 02:11:59.340]   What happened?
[02:11:59.340 --> 02:12:00.340]   Well, Facebook.
[02:12:00.340 --> 02:12:04.380]   Actually, following me over there, it'll be easier for us to reach each other where we're
[02:12:04.380 --> 02:12:05.380]   at now.
[02:12:05.380 --> 02:12:06.380]   Hey, I made some new stuff.
[02:12:06.380 --> 02:12:08.380]   Can you show it to my followers?
[02:12:08.380 --> 02:12:12.340]   Facebook's closed the door and says, "Boost this post."
[02:12:12.340 --> 02:12:16.940]   The irony is he posted that cartoon on Facebook.
[02:12:16.940 --> 02:12:19.700]   And then Facebook said, "Get more likes, comments, and shares.
[02:12:19.700 --> 02:12:23.100]   This post is performing better than 95% of the other posts on your page.
[02:12:23.100 --> 02:12:27.860]   Boost this post for $2,000 to reach up to 490,000 people."
[02:12:27.860 --> 02:12:28.860]   I get those all the time.
[02:12:28.860 --> 02:12:30.700]   I get them all the time too.
[02:12:30.700 --> 02:12:31.700]   It's kind of ironic.
[02:12:31.700 --> 02:12:33.740]   I love it.
[02:12:33.740 --> 02:12:40.580]   I do want to entertain the possibility that this is to address the issue of viral political
[02:12:40.580 --> 02:12:41.580]   news.
[02:12:41.580 --> 02:12:42.580]   It could be.
[02:12:42.580 --> 02:12:44.580]   It is a small test in six countries.
[02:12:44.580 --> 02:12:45.580]   It's not an international.
[02:12:45.580 --> 02:12:51.540]   Yeah, but it's also important to remember that all tests begin a small test on Facebook.
[02:12:51.540 --> 02:12:55.780]   But a small test on Facebook can actually mean tens of thousands and potentially millions
[02:12:55.780 --> 02:12:57.340]   of people.
[02:12:57.340 --> 02:12:58.340]   Yeah.
[02:12:58.340 --> 02:13:05.140]   As we saw from those Slovakia and the websites that they're hit hard by that.
[02:13:05.140 --> 02:13:07.860]   They can just context again.
[02:13:07.860 --> 02:13:13.380]   Facebook can absolutely combat misinformation and fake news without charging publisher like
[02:13:13.380 --> 02:13:14.380]   quality journals.
[02:13:14.380 --> 02:13:16.380]   So there are ways to do it.
[02:13:16.380 --> 02:13:17.380]   Yeah.
[02:13:17.380 --> 02:13:18.380]   There's ways to do it.
[02:13:18.380 --> 02:13:19.380]   Let's take a break.
[02:13:19.380 --> 02:13:21.300]   Final thoughts in just a moment with a great panel.
[02:13:21.300 --> 02:13:22.300]   Really fun today.
[02:13:22.300 --> 02:13:28.220]   Michael Nunez, he's deputy tech editor at Mashable, joining us from Brooklyn, New York.
[02:13:28.220 --> 02:13:31.380]   Amy Webb joining us from DC, right, Amy?
[02:13:31.380 --> 02:13:32.380]   Yeah.
[02:13:32.380 --> 02:13:38.140]   Washington DC, she's the author of The Signals are talking a great book on futurists and
[02:13:38.140 --> 02:13:39.620]   how they work.
[02:13:39.620 --> 02:13:44.380]   She's also a futurist herself, as you probably can tell from her astute insights and Patrick
[02:13:44.380 --> 02:13:52.620]   Bejard, our favorite Frenchman, at least one of them from the Phileus Club and of course,
[02:13:52.620 --> 02:13:54.460]   La Jonde voutet.
[02:13:54.460 --> 02:14:00.460]   Our show today brought to you by appropriate for a Frenchman visiting us wine by Blue Apron.
[02:14:00.460 --> 02:14:01.460]   We know Blue Apron.
[02:14:01.460 --> 02:14:06.380]   We talked about Blue Apron, the number one food delivery company in the country, packaging
[02:14:06.380 --> 02:14:11.900]   wonderful ingredients with great recipes to make a delicious dinner.
[02:14:11.900 --> 02:14:16.180]   But now you can expand your wine palette with Blue Apron's wine boxes with Blue Apron.
[02:14:16.180 --> 02:14:21.460]   No wine leftovers because their bottles are a little bit smaller, which is nice because
[02:14:21.460 --> 02:14:24.980]   I always have leftover wine in a regular size bottle.
[02:14:24.980 --> 02:14:27.340]   Lisa and I open up a bottle of Blue Apron wine.
[02:14:27.340 --> 02:14:28.340]   It's delicious.
[02:14:28.340 --> 02:14:29.340]   It's wonderful.
[02:14:29.340 --> 02:14:33.900]   They curate a great collection, great way to distress in the evening, but not wake up
[02:14:33.900 --> 02:14:39.340]   with a hangover the next morning, perfectly sized for two to share over a lovely weeknight
[02:14:39.340 --> 02:14:42.100]   meal, a Blue Apron meal, I hope.
[02:14:42.100 --> 02:14:45.820]   Ten dollars a bottle, that's half the price of what you'd find in the store because of
[02:14:45.820 --> 02:14:46.900]   course they go to the source.
[02:14:46.900 --> 02:14:51.500]   They deliver it directly from the winery, eliminating the costs of a middleman.
[02:14:51.500 --> 02:14:55.060]   Six new wines monthly from all over Napa.
[02:14:55.060 --> 02:14:56.140]   Our favorites are Bordeaux.
[02:14:56.140 --> 02:14:58.900]   We've had some wonderful Bordeaux from Blue Apron.
[02:14:58.900 --> 02:15:02.660]   You customize the box, of course, with the styles and varieties you love.
[02:15:02.660 --> 02:15:04.660]   And Blue Apron includes custom tasting notes.
[02:15:04.660 --> 02:15:10.340]   So just as Blue Apron has taught us all how to cook great new recipes and ingredients,
[02:15:10.340 --> 02:15:15.020]   they're now going to make wine fun by giving you the who, the what, the wear of every wine
[02:15:15.020 --> 02:15:16.020]   they send.
[02:15:16.020 --> 02:15:24.020]   We love this and we're going to get you $25 off your first wine box by going to Blue Apron.com/Twit
[02:15:24.020 --> 02:15:25.020]   Wine.
[02:15:25.020 --> 02:15:26.420]   We've been doing this for some time.
[02:15:26.420 --> 02:15:31.900]   In fact, I think I remember that we convinced them that they should talk about the wine.
[02:15:31.900 --> 02:15:36.060]   For a long time we're talking about the food, of course, we love our Blue Apron boxes.
[02:15:36.060 --> 02:15:38.700]   And Lisa said, you know, you really got to tell people about this wine thing.
[02:15:38.700 --> 02:15:40.300]   It's fantastic.
[02:15:40.300 --> 02:15:47.140]   Blue Apron.com/TwitWine, perfectly sized wines with custom tasting notes, helping you to
[02:15:47.140 --> 02:15:53.940]   discover fabulous new flavors, Blue Apron.com/TwitWine as always Blue Apron, a better way to cook.
[02:15:53.940 --> 02:16:01.140]   And now a better way to imbibe, convince them to launch in Europe.
[02:16:01.140 --> 02:16:02.140]   I want this.
[02:16:02.140 --> 02:16:03.660]   Oh, you need this in Europe.
[02:16:03.660 --> 02:16:04.660]   You need this.
[02:16:04.660 --> 02:16:05.660]   Yes.
[02:16:05.660 --> 02:16:06.820]   I like the smaller bottles.
[02:16:06.820 --> 02:16:07.820]   I know.
[02:16:07.820 --> 02:16:10.540]   A full size is just a...
[02:16:10.540 --> 02:16:11.540]   Don't laugh at me.
[02:16:11.540 --> 02:16:12.540]   I'm American.
[02:16:12.540 --> 02:16:13.540]   I can barely get through.
[02:16:13.540 --> 02:16:15.700]   No, it's not a split.
[02:16:15.700 --> 02:16:16.700]   It's perfect for two.
[02:16:16.700 --> 02:16:18.220]   I don't know what it's a little bigger than the split.
[02:16:18.220 --> 02:16:22.100]   It's like halfway between a split, a half bottle and a full bottle.
[02:16:22.100 --> 02:16:25.460]   It's perfect.
[02:16:25.460 --> 02:16:26.460]   I wonder.
[02:16:26.460 --> 02:16:35.780]   Now, Patrick, let me ask you, when you buy a cheeseburger in Europe, France or Finland,
[02:16:35.780 --> 02:16:40.260]   where is the cheese on the cheeseburger?
[02:16:40.260 --> 02:16:41.260]   We're going to Toki-mo-geez.
[02:16:41.260 --> 02:16:43.260]   We're talking emojis.
[02:16:43.260 --> 02:16:45.940]   It's a big deal here.
[02:16:45.940 --> 02:16:49.620]   I would say the cheese is on top of the patty.
[02:16:49.620 --> 02:16:50.620]   Yes.
[02:16:50.620 --> 02:16:52.140]   But below the South.
[02:16:52.140 --> 02:16:54.140]   Yeah, you put the...
[02:16:54.140 --> 02:16:57.140]   Look, I worked at McDonald's.
[02:16:57.140 --> 02:16:59.060]   I know exactly how you do it.
[02:16:59.060 --> 02:17:02.180]   To all be patty, special sauce, split lettuce, cheese pickles, onions on an sesame seed,
[02:17:02.180 --> 02:17:04.780]   but going from the bottom to the top.
[02:17:04.780 --> 02:17:08.700]   You put the condiments on the meat, then you put the cheese on the meat, the condiments
[02:17:08.700 --> 02:17:10.300]   on top of the cheese, and then the lettuce.
[02:17:10.300 --> 02:17:17.420]   Anyway, for some reason, bizarrely, Google's emoji features the cheese on the bottom.
[02:17:17.420 --> 02:17:23.100]   Now I know this is not the end of the world, but apparently Sundar Pachai thinks so.
[02:17:23.100 --> 02:17:29.460]   The CEO of Google has tweeted after Thomas Beitkel, Beitkel, tweeted, "I think we need
[02:17:29.460 --> 02:17:32.460]   to have a discussion on how Google's burger emoji is placing the cheese underneath the
[02:17:32.460 --> 02:17:34.420]   burger while Apple puts it on the top.
[02:17:34.420 --> 02:17:39.940]   Sundar tweets will drop everything else we are doing and address on Monday."
[02:17:39.940 --> 02:17:42.700]   If folks could agree on a correct way to do this.
[02:17:42.700 --> 02:17:45.140]   Now are you going to tell me it's okay to have the cheese on the bottom?
[02:17:45.140 --> 02:17:52.300]   No, I feel like I read on Mashable for like various reasons.
[02:17:52.300 --> 02:17:55.420]   You're supposed to put the cheese on the...
[02:17:55.420 --> 02:17:56.420]   We've been doing it wrong.
[02:17:56.420 --> 02:17:58.940]   Like you're supposed to put the cheese for some reason on the bottom.
[02:17:58.940 --> 02:17:59.940]   Okay.
[02:17:59.940 --> 02:18:00.940]   Okay.
[02:18:00.940 --> 02:18:02.380]   Well, thank goodness I have somebody for Mashable right here.
[02:18:02.380 --> 02:18:04.380]   Here's how McDonald's does it, interestingly.
[02:18:04.380 --> 02:18:06.460]   You put the cheese underneath the bottom patty.
[02:18:06.460 --> 02:18:09.100]   Yeah, but it's not the case at all.
[02:18:09.100 --> 02:18:10.100]   Yeah.
[02:18:10.100 --> 02:18:11.100]   Yeah.
[02:18:11.100 --> 02:18:12.100]   Yeah.
[02:18:12.100 --> 02:18:14.300]   Well, because you can't do that because then you can't melt the cheese, right?
[02:18:14.300 --> 02:18:18.660]   So if you're doing this properly, you're putting the cheese on top of the patty while
[02:18:18.660 --> 02:18:24.580]   it's on the grill so that the cheese melts and then you can transfer the patty and the
[02:18:24.580 --> 02:18:27.340]   cheese to the bun, dress it up, eat it.
[02:18:27.340 --> 02:18:29.300]   Right, but aren't supposed to flip it and put the cheese on the...
[02:18:29.300 --> 02:18:31.180]   I swear I read that on Mashable.
[02:18:31.180 --> 02:18:37.100]   By the way, the tweet thread in response to Sundar Vichai is awesome.
[02:18:37.100 --> 02:18:43.300]   Here's Facebook's cheeseburger emojis compared to Facebook messengers.
[02:18:43.300 --> 02:18:48.740]   And Caspar Cliptgen points out, "Hey, Dave Marcus, you need to put more sesame seeds
[02:18:48.740 --> 02:18:50.380]   on the messenger bun.
[02:18:50.380 --> 02:18:53.180]   There's only three on the mess...
[02:18:53.180 --> 02:18:57.620]   But David then responds, "Those are poppy seeds, but our cheese is excellent."
[02:18:57.620 --> 02:19:01.100]   And on and on and on.
[02:19:01.100 --> 02:19:03.900]   Cheese always goes on top so it can melt.
[02:19:03.900 --> 02:19:06.500]   Sundar.
[02:19:06.500 --> 02:19:13.620]   I mean, also, by the way, while you're at it, this is not how beer works.
[02:19:13.620 --> 02:19:16.100]   And that actually, I think it makes an excellent point.
[02:19:16.100 --> 02:19:21.380]   The foam is on top of the mug, but there's a spare gap between the top of the beer and
[02:19:21.380 --> 02:19:22.380]   the foam.
[02:19:22.380 --> 02:19:25.020]   Google, "Go home, you're drunk."
[02:19:25.020 --> 02:19:28.340]   That's not how beer works.
[02:19:28.340 --> 02:19:33.020]   This is how their deep learning algorithms told them that beer works.
[02:19:33.020 --> 02:19:34.020]   This is the funniest...
[02:19:34.020 --> 02:19:35.780]   They do everything with alga...
[02:19:35.780 --> 02:19:38.940]   Yeah, it's a very interesting thread.
[02:19:38.940 --> 02:19:45.380]   I don't know where they might have gotten the idea that the cheese goes under the patty.
[02:19:45.380 --> 02:19:46.380]   Hmm.
[02:19:46.380 --> 02:19:49.820]   Oh, wow, that's Danny Mayer.
[02:19:49.820 --> 02:19:51.180]   Was that the Danny Mayer?
[02:19:51.180 --> 02:19:52.180]   Who's Danny Mayer?
[02:19:52.180 --> 02:19:54.020]   Oh, very, very famous.
[02:19:54.020 --> 02:19:55.780]   Oh, from Union Square.
[02:19:55.780 --> 02:19:58.060]   Oh, founder of Shake Shack.
[02:19:58.060 --> 02:19:59.060]   Yeah, yeah.
[02:19:59.060 --> 02:20:02.020]   And he must be a gourmet.
[02:20:02.020 --> 02:20:05.100]   He's got a napkin to his lips and his Twitter avatar.
[02:20:05.100 --> 02:20:07.740]   Yeah, his restaurants are great.
[02:20:07.740 --> 02:20:10.100]   So he's got a diagram.
[02:20:10.100 --> 02:20:14.020]   Apparently, Katie Couric got involved in this as well.
[02:20:14.020 --> 02:20:16.020]   This is crazy.
[02:20:16.020 --> 02:20:17.020]   You know what?
[02:20:17.020 --> 02:20:20.620]   The smaller, the less important, the less relevant the topic, the easier it is for anybody
[02:20:20.620 --> 02:20:22.300]   to have an opinion.
[02:20:22.300 --> 02:20:23.900]   I think that's what's going on.
[02:20:23.900 --> 02:20:27.980]   And by the way, somebody responded to a Google image search, which seems like that should
[02:20:27.980 --> 02:20:30.500]   be definitive.
[02:20:30.500 --> 02:20:37.140]   Apparently, the person responsible for Google's new emojis is a vegetarian teetotaler, according
[02:20:37.140 --> 02:20:39.700]   to Danny and Brooks.
[02:20:39.700 --> 02:20:44.580]   We never solve this conundrum, but I'll be watching with interest tomorrow to see if
[02:20:44.580 --> 02:20:48.140]   Google fixes this problem.
[02:20:48.140 --> 02:20:49.140]   Yes, it is indeed.
[02:20:49.140 --> 02:20:50.140]   I read the OV.
[02:20:50.140 --> 02:20:54.140]   They changed their emoji based on this Twitter thread.
[02:20:54.140 --> 02:21:00.500]   We have, Twitter certainly has a lot of influence in very important things in the world.
[02:21:00.500 --> 02:21:07.500]   It's nice that also sometimes some ridiculous thing happens and becomes a thing.
[02:21:07.500 --> 02:21:12.460]   It seems like it happened like this all the time a few years ago.
[02:21:12.460 --> 02:21:16.940]   And nowadays, it's only horrible horrors that happen on Twitter.
[02:21:16.940 --> 02:21:21.540]   I like that the emoji burger thing became a thing and everyone's jumping in on it.
[02:21:21.540 --> 02:21:28.100]   Of course, you're French, and at the McDonald's, the little royal and the royal with cheese
[02:21:28.100 --> 02:21:30.660]   have no salad.
[02:21:30.660 --> 02:21:35.500]   Because everyone knows in France, the salad comes after the meal, my friends, after.
[02:21:35.500 --> 02:21:40.780]   And the cheese on the royal with cheese is on top and on bottom.
[02:21:40.780 --> 02:21:41.780]   It's both.
[02:21:41.780 --> 02:21:47.300]   Because yes, because the French believe in more cheese.
[02:21:47.300 --> 02:21:48.380]   What is all that white stuff?
[02:21:48.380 --> 02:21:49.860]   Is that mayonnaise?
[02:21:49.860 --> 02:21:52.180]   That's a little secret sauce.
[02:21:52.180 --> 02:21:53.180]   Sichre.
[02:21:53.180 --> 02:22:01.260]   I'm deadly afraid that might mean the secretion sauce in French.
[02:22:01.260 --> 02:22:03.620]   I don't want to go farther than that.
[02:22:03.620 --> 02:22:07.620]   Actually, McDonald's is a lot better in France than it is in the US.
[02:22:07.620 --> 02:22:08.620]   Surprisingly.
[02:22:08.620 --> 02:22:11.180]   Some Americans may disagree.
[02:22:11.180 --> 02:22:12.420]   It doesn't taste the same.
[02:22:12.420 --> 02:22:14.100]   Let's put it that way.
[02:22:14.100 --> 02:22:17.980]   Well, they could disagree, but they would be wrong.
[02:22:17.980 --> 02:22:21.300]   Patrick Fezia, follow him at Frenchspin.com on the Twitter.
[02:22:21.300 --> 02:22:22.300]   He's not Patrick.
[02:22:22.300 --> 02:22:26.460]   I never got the explanation for that.
[02:22:26.460 --> 02:22:27.980]   Why are you not Patrick?
[02:22:27.980 --> 02:22:28.980]   You are Patrick.
[02:22:28.980 --> 02:22:32.380]   Well, a couple of things.
[02:22:32.380 --> 02:22:38.420]   First of all, Patrick was already taken, which is completely, you know, anecdotal in the
[02:22:38.420 --> 02:22:40.060]   reasoning.
[02:22:40.060 --> 02:22:44.460]   The real reason is that it is a brilliant marketing move.
[02:22:44.460 --> 02:22:47.260]   You hear it once and you never forget it.
[02:22:47.260 --> 02:22:48.260]   Not Patrick.
[02:22:48.260 --> 02:22:49.260]   Yeah.
[02:22:49.260 --> 02:22:52.220]   He is for the English language podcast, the Philly is club at Frenchspin.com.
[02:22:52.220 --> 02:22:58.460]   But if you want to listen to a whole on the Voutet in French, Frenchspin.fr, of course.
[02:22:58.460 --> 02:23:00.460]   And he makes his home now in Helsinki.
[02:23:00.460 --> 02:23:01.460]   It's great to have you.
[02:23:01.460 --> 02:23:02.460]   Thank you for joining us.
[02:23:02.460 --> 02:23:03.980]   It's probably four in the morning.
[02:23:03.980 --> 02:23:05.500]   Yeah, it's getting there.
[02:23:05.500 --> 02:23:06.500]   It's two thirty.
[02:23:06.500 --> 02:23:08.060]   But I'm soldering on.
[02:23:08.060 --> 02:23:09.420]   Thank you for staying up late with us.
[02:23:09.420 --> 02:23:10.420]   I really appreciate it.
[02:23:10.420 --> 02:23:11.420]   A pleasure.
[02:23:11.420 --> 02:23:12.900]   Amy Webb always a pleasure.
[02:23:12.900 --> 02:23:16.940]   Go to Amy Webb.io to find out everything Amy is up to.
[02:23:16.940 --> 02:23:20.140]   Just the more we have you on, the more I want to have you on you.
[02:23:20.140 --> 02:23:23.100]   Just so great, so smart.
[02:23:23.100 --> 02:23:28.580]   And I love your perspective, which is a perspective, kind of the long range perspective.
[02:23:28.580 --> 02:23:32.660]   And we're so quotidian in our concerns that it's nice to think about the future a little
[02:23:32.660 --> 02:23:33.660]   bit.
[02:23:33.660 --> 02:23:35.340]   Her book, The Signals is Talking.
[02:23:35.340 --> 02:23:37.500]   The Signals is talking is not the...
[02:23:37.500 --> 02:23:38.500]   She's grammatical.
[02:23:38.500 --> 02:23:40.500]   The Signals are talking.
[02:23:40.500 --> 02:23:42.740]   The Signals is talking is another book entirely.
[02:23:42.740 --> 02:23:45.620]   Why today's Fringe is Tomorrow's Mainstream.
[02:23:45.620 --> 02:23:47.340]   Bestseller available everywhere.
[02:23:47.340 --> 02:23:56.100]   And of course, her company is on the website at Amy Webb, W e b b dot I o.
[02:23:56.100 --> 02:24:04.500]   You could find out about her futurists and her future and tomorrow's future today.
[02:24:04.500 --> 02:24:05.500]   Thank you, Amy.
[02:24:05.500 --> 02:24:06.500]   Great to have you here.
[02:24:06.500 --> 02:24:07.500]   Appreciate it.
[02:24:07.500 --> 02:24:10.580]   And my good friend, glad to have you back.
[02:24:10.580 --> 02:24:17.020]   Please come back anytime from Mashable where he is the Deputy Tech Editor, Mr. Michael
[02:24:17.020 --> 02:24:18.020]   Nunez.
[02:24:18.020 --> 02:24:22.020]   Michael, you held that microphone like a champ for two and a half hours.
[02:24:22.020 --> 02:24:24.380]   Thanks for hanging in there with me.
[02:24:24.380 --> 02:24:30.500]   We're going to send you a headset you don't have to hold on to from now on.
[02:24:30.500 --> 02:24:31.500]   That'd be great.
[02:24:31.500 --> 02:24:32.540]   Thank you all for joining us.
[02:24:32.540 --> 02:24:34.660]   We do this week in Tech every Sunday afternoon.
[02:24:34.660 --> 02:24:39.260]   Really it's fun every week to get together with just the best people and talk about what's
[02:24:39.260 --> 02:24:40.260]   going on.
[02:24:40.260 --> 02:24:44.020]   Try to understand it in a deeper way than the daily headlines.
[02:24:44.020 --> 02:24:48.380]   You can join us around about 3 p.m. Pacific, 6 p.m. Eastern time.
[02:24:48.380 --> 02:24:50.300]   That's 2200 UTC.
[02:24:50.300 --> 02:24:54.980]   And we'll be for one more week and then daylight saving times ends and we'll be at 2300 UTC.
[02:24:54.980 --> 02:24:57.460]   But we'll talk about that next week.
[02:24:57.460 --> 02:25:00.820]   If you can't watch live, and by the way, if you are here live watching twit.tv/live
[02:25:00.820 --> 02:25:04.540]   stream, please join us in the chatroom at IRC.twit.tv.
[02:25:04.540 --> 02:25:06.540]   A great bunch of people in there.
[02:25:06.540 --> 02:25:08.740]   And the background chatter helps you basically.
[02:25:08.740 --> 02:25:10.300]   They write all my jokes.
[02:25:10.300 --> 02:25:14.460]   It's very, very helpful if you come and write jokes for me at IRC.twit.tv.
[02:25:14.460 --> 02:25:15.460]   You can also be in house.
[02:25:15.460 --> 02:25:18.780]   We had some great visitors from all over the world.
[02:25:18.780 --> 02:25:23.540]   If you want to visit, just email tickets at twit.tv so we know you're coming.
[02:25:23.540 --> 02:25:29.220]   We love having people from all over who listen to the show come visit us like Toshi who visits
[02:25:29.220 --> 02:25:35.020]   from Japan, Shrinivas who's from Mumbai, India visiting today from Boston, Massachusetts,
[02:25:35.020 --> 02:25:36.940]   Bob and from Boulder, Colorado, Steve.
[02:25:36.940 --> 02:25:38.940]   Great to have you guys here.
[02:25:38.940 --> 02:25:40.420]   Just email tickets at twit.tv.
[02:25:40.420 --> 02:25:42.820]   We would love to have you in studio with us.
[02:25:42.820 --> 02:25:45.780]   If you can't watch live or be here live, you can always get on demand of all of our
[02:25:45.780 --> 02:25:51.660]   shows easily enough at the website twit.tv or subscribe in your favorite podcatcher.
[02:25:51.660 --> 02:25:53.380]   That way you'll get every episode.
[02:25:53.380 --> 02:25:56.380]   It's getting to be a lot like the end of the year.
[02:25:56.380 --> 02:25:57.380]   I can't believe it.
[02:25:57.380 --> 02:25:58.860]   We're already starting to talk about the end of the year.
[02:25:58.860 --> 02:26:02.100]   We're putting together our best of episodes for all of our Twitch shows.
[02:26:02.100 --> 02:26:04.380]   We would love your help.
[02:26:04.380 --> 02:26:08.620]   It's easy if you remember a moment that you really, you know, oh, that was great whether
[02:26:08.620 --> 02:26:12.700]   it was thought provoking or funny or just weird.
[02:26:12.700 --> 02:26:16.580]   Go to twit.tv/bestof and give us whatever information you know about it.
[02:26:16.580 --> 02:26:19.860]   Even if you just say, "Hey, that time when they did that with the thing and the thing
[02:26:19.860 --> 02:26:22.180]   on the head, that was a great time."
[02:26:22.180 --> 02:26:25.940]   Even that will be helpful because our editors have to work hard to go through, comb through
[02:26:25.940 --> 02:26:28.580]   52 weeks of shows to put together the best of.
[02:26:28.580 --> 02:26:29.580]   We'll have that at the end of the year.
[02:26:29.580 --> 02:26:32.540]   I think it'll be our New Year's Day show, right?
[02:26:32.540 --> 02:26:37.060]   Is it New Year's Eve or New Year's Day on the last Sunday of the year?
[02:26:37.060 --> 02:26:41.460]   Twit.tv/bestof.
[02:26:41.460 --> 02:26:42.460]   Thanks everybody for being here.
[02:26:42.460 --> 02:26:43.460]   It's great to see you.
[02:26:43.460 --> 02:26:44.980]   Come back again next week, will you?
[02:26:44.980 --> 02:26:49.300]   But for now, another twit is in the can.
[02:26:49.300 --> 02:26:53.900]   Bye bye.
[02:26:53.900 --> 02:26:55.640]    Doin' the twit, baby 
[02:26:55.640 --> 02:26:56.480]    Doin' the twit 
[02:26:56.480 --> 02:26:57.480]    All right 

