;FFMETADATA1
title=The Cheese Tax
artist=Leo Laporte, Amy Webb, Phil Libin
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-05-15
track=927
language=English
genre=Podcast
comment=<p>AI ethics, Google IO, Absci, Replika, AM radio in cars, Spotify rejects AI songs</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:11.000]   It's time for Twet This Week in Tech. This is a very special show because we have two people who saw the AI revolution coming and have some very deep insights into it.
[00:00:11.000 --> 00:00:17.000]   Amy Webb, our futurist, is here from the Future Today Institute, the author of The Genesis Machine and The Big Nine.
[00:00:17.000 --> 00:00:28.000]   She's joined by Phil Liben. He started Evernote and is now running an incubator for AI and AI-based companies.
[00:00:28.000 --> 00:00:34.000]   Who knows all about this stuff, we've got a very smart, very interesting show coming up for you.
[00:00:34.000 --> 00:00:38.000]   AI, the primary topic. This week on Tech is Next.
[00:00:38.000 --> 00:00:41.000]   [Music]
[00:00:41.000 --> 00:00:43.000]   Podcasts you love.
[00:00:43.000 --> 00:00:45.000]   From people you trust.
[00:00:45.000 --> 00:00:47.000]   This is Twet.
[00:00:47.000 --> 00:00:54.000]   [Music]
[00:00:54.000 --> 00:01:02.000]   This is Twet. This Week in Tech. Episode 927 recorded Sunday, May 14th, 2023.
[00:01:02.000 --> 00:01:04.000]   The Cheese Tax.
[00:01:04.000 --> 00:01:09.000]   This Week in Tech is brought to you by HelloFresh America's number one meal kit.
[00:01:09.000 --> 00:01:15.000]   Get farm fresh, pre-portioned ingredients and seasonal recipes delivered right to your doorstep.
[00:01:15.000 --> 00:01:21.000]   Skip the grocery store and count on HelloFresh to make home cooking easy, fun and affordable.
[00:01:21.000 --> 00:01:29.000]   Go to hellofresh.com/twits16 and use the code Twet16 for 16 free meals plus free shipping.
[00:01:29.000 --> 00:01:33.000]   And by ExpressVPN.
[00:01:33.000 --> 00:01:36.000]   Stop letting strangers invade your online privacy.
[00:01:36.000 --> 00:01:43.000]   Protect yourself at expressvpn.com/twit for three extra months free with a one-year package.
[00:01:43.000 --> 00:01:51.000]   And by Miro. Miro is your team's online workspace to connect, collaborate and create together.
[00:01:51.000 --> 00:01:56.000]   Tap into a way to map processes, systems and plans with a whole team.
[00:01:56.000 --> 00:02:03.000]   And get your first three boards for free to start creating your best work yet at Miro.com/podcast.
[00:02:03.000 --> 00:02:10.000]   [Music]
[00:02:10.000 --> 00:02:12.000]   It's time for Twet this Week in Tech.
[00:02:12.000 --> 00:02:14.000]   We're going to cover the week's tech news.
[00:02:14.000 --> 00:02:20.000]   And you know when we have a panel of three that it's two very good people.
[00:02:20.000 --> 00:02:26.000]   I always cut the panel down when I have people who have lots of good things to say.
[00:02:26.000 --> 00:02:27.000]   Filippin is here.
[00:02:27.000 --> 00:02:34.000]   Old friend, founder of Evernote, co-founder and CEO of a new, Kurt couldn't be better timing.
[00:02:34.000 --> 00:02:42.000]   New startup incubator called all-turtles.com and you specialize kind of an AI stuff, right Phil?
[00:02:42.000 --> 00:02:43.000]   Yeah.
[00:02:43.000 --> 00:02:46.000]   This was a couple of years ago. You were ahead of the curve.
[00:02:46.000 --> 00:02:51.000]   Yeah, six, seven years ago back before AI was cool.
[00:02:51.000 --> 00:02:54.000]   Now it might be a little too cool.
[00:02:54.000 --> 00:02:55.000]   Way too cool.
[00:02:55.000 --> 00:02:56.000]   Yeah.
[00:02:56.000 --> 00:03:03.000]   Too cool for, and he also does of course, a wonderful app which he's using right now to impress people over video.
[00:03:03.000 --> 00:03:04.000]   Mm-hmm.
[00:03:04.000 --> 00:03:08.000]   It's the best name to mmhmm.app.
[00:03:08.000 --> 00:03:11.000]   Also with us, too cool for school.
[00:03:11.000 --> 00:03:16.000]   The wonderful Amy Webb, author of the Genesis Machine CEO Future Today Institute.
[00:03:16.000 --> 00:03:17.000]   Hi Amy.
[00:03:17.000 --> 00:03:18.000]   Hey.
[00:03:18.000 --> 00:03:20.000]   Great to see you.
[00:03:20.000 --> 00:03:26.000]   I, of course, we had you on when the Genesis Machine came out last year and did a great interview.
[00:03:26.000 --> 00:03:30.000]   And then I had George Church on a few months ago in triangulation.
[00:03:30.000 --> 00:03:31.000]   Yeah.
[00:03:31.000 --> 00:03:35.000]   You talk about him, the father of modern genomics in here.
[00:03:35.000 --> 00:03:37.000]   Yeah, he's a good guy.
[00:03:37.000 --> 00:03:40.000]   Genomics is fascinating. That's what this is all about. Our question.
[00:03:40.000 --> 00:03:41.000]   That one is.
[00:03:41.000 --> 00:03:42.000]   Yeah.
[00:03:42.000 --> 00:03:43.000]   You have a new one?
[00:03:43.000 --> 00:03:44.000]   No.
[00:03:44.000 --> 00:03:50.000]   Next year, I was going to say the last book on AI was suddenly, it went into another printing.
[00:03:50.000 --> 00:04:00.000]   It was already a bestseller, but it went into like two more printings because apparently on Fox News, I didn't know this, but apparently like Janine Pirro.
[00:04:00.000 --> 00:04:05.000]   Janine Pirro, she mentioned you.
[00:04:05.000 --> 00:04:11.000]   Somebody on the show, they were talking about like a couple of weeks ago, generative AI in China.
[00:04:11.000 --> 00:04:12.000]   And I don't know what else.
[00:04:12.000 --> 00:04:16.000]   And somebody turned her and was like, how have you not read this book?
[00:04:16.000 --> 00:04:17.000]   Wow.
[00:04:17.000 --> 00:04:19.000]   And was it the big nine?
[00:04:19.000 --> 00:04:20.000]   Yep.
[00:04:20.000 --> 00:04:26.000]   The big nine that whatever 30 seconds of airtime led to multiple thousands of copies of that book.
[00:04:26.000 --> 00:04:27.000]   Holy cow.
[00:04:27.000 --> 00:04:30.000]   All these sold out, they had to go into more printings.
[00:04:30.000 --> 00:04:35.000]   So for those of you who think you might want to write books, don't write off Fox News.
[00:04:35.000 --> 00:04:36.000]   Yeah.
[00:04:36.000 --> 00:04:37.000]   That's a vehicle.
[00:04:37.000 --> 00:04:38.000]   Wow.
[00:04:38.000 --> 00:04:39.000]   Janine Pirro probably hits.
[00:04:39.000 --> 00:04:40.000]   Yeah.
[00:04:40.000 --> 00:04:42.000]   She probably could sell a few books.
[00:04:42.000 --> 00:04:43.000]   She probably could.
[00:04:43.000 --> 00:04:47.000]   This was the one about, and it's still completely relevant about the big tech companies.
[00:04:47.000 --> 00:04:55.000]   What was great about it, it wasn't just the US, you know, Google and Microsoft and Facebook and Amazon, but it's Baidu and Tencent and Alibaba as well.
[00:04:55.000 --> 00:04:57.000]   And I thought that made it very, very interesting.
[00:04:57.000 --> 00:05:01.000]   And of course AI is a big part of what the Chinese companies, especially Europe do.
[00:05:01.000 --> 00:05:02.000]   It's kind of a black box.
[00:05:02.000 --> 00:05:04.000]   We don't know really.
[00:05:04.000 --> 00:05:11.000]   Well, no, that's the whole, I mean, Google, I guess we'll talk about IO at some point, but no, it's becoming incredibly opaque.
[00:05:11.000 --> 00:05:24.000]   And some of what I, I don't really make predictions, but some of the scenarios that I wrote about, that book is like, I wrote that book six, seven years ago, is now happening pretty close to what I, I said might happen.
[00:05:24.000 --> 00:05:25.000]   So there's that.
[00:05:25.000 --> 00:05:26.000]   Which was?
[00:05:26.000 --> 00:05:39.000]   Just the massive consolidation and a lot of like Amazon trading your privacy for data.
[00:05:39.000 --> 00:05:50.000]   Subsidizing what should be provided, subsidizing what used to be government services or offering them for free in exchange for data.
[00:05:50.000 --> 00:05:57.000]   Some of the early signs of artists, you know, AGI kind of already happening and happening in a conversational way.
[00:05:57.000 --> 00:05:58.000]   So just all that stuff.
[00:05:58.000 --> 00:06:00.000]   And of course nobody took any action.
[00:06:00.000 --> 00:06:02.000]   And now everybody's collectively freaking out.
[00:06:02.000 --> 00:06:04.000]   And we should have paid attention to Amy.
[00:06:04.000 --> 00:06:17.000]   The description on the Heshette, it's from the Heshette book group website is a call to arms about the broken nature of artificial intelligence and the powerful corporations that are turning the human machine relationship on its head.
[00:06:17.000 --> 00:06:19.000]   Yeah, that sounds pretty timely.
[00:06:19.000 --> 00:06:21.000]   Yeah, actually.
[00:06:21.000 --> 00:06:25.000]   And maybe the genomics thing next year, you got it made.
[00:06:25.000 --> 00:06:27.000]   Yeah, it's super great.
[00:06:27.000 --> 00:06:31.000]   I tell people like here's all the bad stuff that's coming and everybody clearly changes.
[00:06:31.000 --> 00:06:34.000]   Yeah, yeah, yeah, and then they all paid attention.
[00:06:34.000 --> 00:06:35.000]   Very effective.
[00:06:35.000 --> 00:06:40.000]   So Google I/O was really kind of Google's attempt to catch up a little bit.
[00:06:40.000 --> 00:06:41.000]   Not because they're behind.
[00:06:41.000 --> 00:06:43.000]   This is what's interesting to me.
[00:06:43.000 --> 00:06:50.000]   And I would like to know you also know a lot about China having spent time there and you've been thinking about it for a long time.
[00:06:50.000 --> 00:07:01.000]   But Microsoft kind of, well, I guess it started really with OpenAI, which Microsoft was a heavy investor in and became an even heavier investor in.
[00:07:01.000 --> 00:07:15.000]   And chat GPT, but even before that, Dolly and Dolly too, and stable diffusion and mid-journey, there was just a sudden Cambrian explosion of consumer facing AI tools.
[00:07:15.000 --> 00:07:23.000]   And of course, the company that we thought would be the leader in all this Google found itself playing catch up this week at Google I/O.
[00:07:23.000 --> 00:07:30.000]   Were they behind Amy or were they just quieter?
[00:07:30.000 --> 00:07:42.000]   Well, I think it's worth noting that the large language models that have made their way into consumer use were in development for quite a while.
[00:07:42.000 --> 00:07:47.000]   And even OpenAI was founded in 2013, something like that.
[00:07:47.000 --> 00:08:03.000]   Okay, so my point is that was founded because Elon and Microsoft and a few others, but I think chiefly Elon, who put in a billion, I believe, said we can't let the big tech companies own AI development.
[00:08:03.000 --> 00:08:09.000]   We've got to have an open AI system where people can have input into it.
[00:08:09.000 --> 00:08:16.000]   And of course, as soon as OpenAI put a chat GPT, they kind of closed it down and said, yeah, and then we're going to try to make some money on this.
[00:08:16.000 --> 00:08:21.000]   Elon had left a few years before it is said because he didn't agree with that strategy, that plan.
[00:08:21.000 --> 00:08:35.000]   And then, you want more recently, Elon and many others have signed a kind of silly letter saying, let's just wait a while, let's just wait six months, put a pause on this, which many interpreted as his attempt to say, let me catch up.
[00:08:35.000 --> 00:08:39.000]   Can you just give me a chance?
[00:08:39.000 --> 00:08:44.000]   He also launched his own truth GPT, maximum truth seeking.
[00:08:44.000 --> 00:08:45.000]   I didn't watch anything.
[00:08:45.000 --> 00:08:47.000]   Wait, he didn't watch anything.
[00:08:47.000 --> 00:08:48.000]   He didn't watch anything.
[00:08:48.000 --> 00:08:50.000]   He said something, got all the attention.
[00:08:50.000 --> 00:08:56.000]   I intentionally didn't cover it because I thought that's just Elon mousin off as usual.
[00:08:56.000 --> 00:08:57.000]   Right.
[00:08:57.000 --> 00:09:01.000]   But the real question is, where are we really in AI?
[00:09:01.000 --> 00:09:02.000]   And where is China in AI?
[00:09:02.000 --> 00:09:03.000]   Where is Google in AI?
[00:09:03.000 --> 00:09:06.000]   Where is Facebook in AI?
[00:09:06.000 --> 00:09:08.000]   And where is Apple in AI?
[00:09:08.000 --> 00:09:13.000]   Because was it Neil Stevenson who said the future is here?
[00:09:13.000 --> 00:09:14.000]   It's just not distributed evenly.
[00:09:14.000 --> 00:09:15.000]   Even later, right?
[00:09:15.000 --> 00:09:16.000]   Yeah.
[00:09:16.000 --> 00:09:21.000]   I mean, so again, I think it's worth noting that AI is a huge umbrella.
[00:09:21.000 --> 00:09:25.000]   And so we're talking about a pretty large language model then, right?
[00:09:25.000 --> 00:09:26.000]   Right.
[00:09:26.000 --> 00:09:28.000]   So it's not new.
[00:09:28.000 --> 00:09:29.000]   What is new?
[00:09:29.000 --> 00:09:37.000]   And I think you actually said it right when you mentioned a Cambrian explosion is that there's now, there's a commercial application.
[00:09:37.000 --> 00:09:45.000]   And the moment that you get a commercial application, that there's this sort of flywheel that gets created with a lot of fast followers.
[00:09:45.000 --> 00:09:51.000]   So Google was absolutely, Google's been working on this as, and I should note, Microsoft is one of our clients.
[00:09:51.000 --> 00:09:58.000]   So Microsoft, for many, many years, pre-open AI, all of this has been in the works.
[00:09:58.000 --> 00:10:06.000]   We've probably, many people have forgotten, but in February of 2020, to be fair, there was a virus going around that had us all a little bit of time.
[00:10:06.000 --> 00:10:16.000]   But in February 2020, OpenAI put out a press release about GPT-2, which was the second gen of their language model.
[00:10:16.000 --> 00:10:21.000]   The first was trained on BookNet, I think it was like 7,000, corpus of like 7,000 books.
[00:10:21.000 --> 00:10:32.000]   The second version was that plus a sense of, I'm assuming, I mean, we don't know, but I'm assuming Reddit and Wikipedia and, you know, whatever else.
[00:10:32.000 --> 00:10:38.000]   The press release, you may remember, said, this is too dangerous for us to release.
[00:10:38.000 --> 00:10:39.000]   So that was three years ago.
[00:10:39.000 --> 00:10:54.000]   And it's not like we have an AI police force roaming around or any, you know, so OpenAI has sort of gone inverse since then, and it's incredibly opaque, as is Google's language models and systems.
[00:10:54.000 --> 00:11:04.000]   Because there's, you know, everybody's, there's like a consolidation happening again, and everybody's concerned about figuring out the way it's, the consumer applications are one thing.
[00:11:04.000 --> 00:11:06.000]   The real money is enterprise.
[00:11:06.000 --> 00:11:12.000]   So what's really happening is everybody's trying to figure out how to scale enterprise applications, and that requires cloud.
[00:11:12.000 --> 00:11:23.000]   So that's partially why you're seeing, you know, Azure and OpenAI getting together and AWS and hugging face and Google making a lot of these announcements a little, or late to the party on the announcements.
[00:11:23.000 --> 00:11:25.000]   But they've certainly had the tech there.
[00:11:25.000 --> 00:11:27.000]   It feels like everybody's scrambling, right?
[00:11:27.000 --> 00:11:28.000]   They see this exploding.
[00:11:28.000 --> 00:11:30.000]   Phil, you use what is it?
[00:11:30.000 --> 00:11:33.000]   Six years ago started all turtles.
[00:11:33.000 --> 00:11:39.000]   And I remember when you started it, it was a kind of an incubator for AI companies, right?
[00:11:39.000 --> 00:11:41.000]   Yeah.
[00:11:41.000 --> 00:11:50.000]   And if we even longer than that, I mean, you remember, I've been on a crusade for like 10 years to get people to refer to Google I/O as Google I/O.
[00:11:50.000 --> 00:11:53.000]   It's just obviously Google I/O.
[00:11:53.000 --> 00:11:55.000]   And no one calls it Google I/O.
[00:11:55.000 --> 00:11:56.000]   You were way ahead of the game.
[00:11:56.000 --> 00:11:57.000]   I know.
[00:11:57.000 --> 00:11:59.000]   I think I've had it.
[00:11:59.000 --> 00:12:01.000]   We're just waiting for people to start calling it Google I/O.
[00:12:01.000 --> 00:12:11.000]   When you started all of this, what did you think the sweet spot was for AI and for investment?
[00:12:11.000 --> 00:12:17.000]   Well, so I was super optimistic about six years ago on conversationally, I in particular.
[00:12:17.000 --> 00:12:22.000]   And I think he's right.
[00:12:22.000 --> 00:12:25.000]   AI is such a giant topic.
[00:12:25.000 --> 00:12:28.000]   But let's talk about specifically like conversational AI.
[00:12:28.000 --> 00:12:36.000]   I wrote a medium article that's still up there somewhere about why I think that bots were going to be a big thing.
[00:12:36.000 --> 00:12:41.000]   And I was excited about it primarily as a user interface paradigm, thinking that this could make technology just much more accessible because humans already developed a way to talk language.
[00:12:41.000 --> 00:12:42.000]   That was the way.
[00:12:42.000 --> 00:12:43.000]   Yeah, exactly.
[00:12:43.000 --> 00:12:48.000]   So, you know, I think that's a good thing.
[00:12:48.000 --> 00:12:51.000]   You know, I think that's a good thing.
[00:12:51.000 --> 00:12:54.000]   You know, I think that's a good thing.
[00:12:54.000 --> 00:12:57.000]   I think that's a good thing.
[00:12:57.000 --> 00:13:00.000]   I think that's a good thing.
[00:13:00.000 --> 00:13:02.000]   I think that's a good thing.
[00:13:02.000 --> 00:13:04.000]   I think that's a good thing.
[00:13:04.000 --> 00:13:06.000]   I think that's a good thing.
[00:13:06.000 --> 00:13:08.000]   I think that's a good thing.
[00:13:08.000 --> 00:13:15.000]   I think that's a good thing.
[00:13:15.000 --> 00:13:18.000]   I think that's a good thing.
[00:13:18.000 --> 00:13:21.000]   I think that's a good thing.
[00:13:21.000 --> 00:13:23.000]   I think that's a good thing.
[00:13:23.000 --> 00:13:25.000]   I think that's a good thing.
[00:13:25.000 --> 00:13:27.000]   I think that's a good thing.
[00:13:27.000 --> 00:13:29.000]   I think that's a good thing.
[00:13:29.000 --> 00:13:32.000]   I think that's a good thing.
[00:13:32.000 --> 00:13:39.000]   I think that's a good thing.
[00:13:39.000 --> 00:13:42.000]   I think that's a good thing.
[00:13:42.000 --> 00:13:45.000]   I think that's a good thing.
[00:13:45.000 --> 00:13:48.000]   I think that's a good thing.
[00:13:48.000 --> 00:13:51.000]   I think that's a good thing.
[00:13:51.000 --> 00:13:54.000]   I think that's a good thing.
[00:13:54.000 --> 00:13:57.000]   I think that's a good thing.
[00:13:57.000 --> 00:14:04.000]   I think that's a good thing.
[00:14:04.000 --> 00:14:07.000]   I think that's a good thing.
[00:14:07.000 --> 00:14:10.000]   I think that's a good thing.
[00:14:10.000 --> 00:14:13.000]   I think that's a good thing.
[00:14:13.000 --> 00:14:16.000]   I think that's a good thing.
[00:14:16.000 --> 00:14:19.000]   I think that's a good thing.
[00:14:19.000 --> 00:14:22.000]   I think that's a good thing.
[00:14:22.000 --> 00:14:25.000]   I think that's a good thing.
[00:14:25.000 --> 00:14:30.600]   We're just so used to associating talking with thinking that we're now ascribing all
[00:14:30.600 --> 00:14:33.000]   sorts of superpowers and miracles to these things.
[00:14:33.000 --> 00:14:43.000]   If somebody's confident, even though they're confidently wrong, we go, "Wow, that's great."
[00:14:43.000 --> 00:14:45.000]   Confidently wrong is sort of my move.
[00:14:45.000 --> 00:14:46.000]   That is the CEO table.
[00:14:46.000 --> 00:14:49.000]   That's what they teach us at CEO school.
[00:14:49.000 --> 00:14:53.000]   Our jobs are the first to be threatened by these things.
[00:14:53.000 --> 00:14:59.000]   I think we are, as an industry, confusing the ability of these things to talk with the ability
[00:14:59.000 --> 00:15:01.000]   for them to actually do anything else.
[00:15:01.000 --> 00:15:02.000]   Are you disappointed?
[00:15:02.000 --> 00:15:04.000]   I'm a smart idea.
[00:15:04.000 --> 00:15:06.000]   Plus, no one says "googlio."
[00:15:06.000 --> 00:15:08.000]   You failed twice.
[00:15:08.000 --> 00:15:10.000]   Amy, is that fair?
[00:15:10.000 --> 00:15:12.000]   Is that accurate?
[00:15:12.000 --> 00:15:17.000]   I think that's a great synopsis of what's happening.
[00:15:17.000 --> 00:15:22.960]   I would say leaders versus industry, but leaders have confused the ability to talk with the
[00:15:22.960 --> 00:15:24.720]   ability to think.
[00:15:24.720 --> 00:15:32.960]   My observation is that those in tech know exactly what's happening.
[00:15:32.960 --> 00:15:34.720]   Leaders ends the media, by the way.
[00:15:34.720 --> 00:15:38.960]   Here's the thing.
[00:15:38.960 --> 00:15:44.440]   I was at TED a couple of weeks ago, and I would say about half of that week, in some form
[00:15:44.440 --> 00:15:51.760]   or another, was devoted to absolute doomsday, apocalyptic, hellscape discussions, conversations,
[00:15:51.760 --> 00:15:54.360]   whatever about generative AI.
[00:15:54.360 --> 00:15:59.320]   Greg Brockman was there, Saul Kahn was there, former vice president, bunch of hedge fund
[00:15:59.320 --> 00:16:00.320]   guys.
[00:16:00.320 --> 00:16:06.280]   Everybody is just reminded me a little bit of flossing your teeth too hard.
[00:16:06.280 --> 00:16:10.760]   Do you ever do a thing where you floss your teeth slightly too hard?
[00:16:10.760 --> 00:16:13.760]   It hurts, but you keep doing it because you kind of like it?
[00:16:13.760 --> 00:16:14.760]   Yeah.
[00:16:14.760 --> 00:16:15.760]   Is that just me?
[00:16:15.760 --> 00:16:18.560]   Everybody now knows me much better than they did before.
[00:16:18.560 --> 00:16:21.080]   I'll give you another analogy that's similar.
[00:16:21.080 --> 00:16:25.360]   When you have a loose tooth, you can't keep your tongue off of it or you keep playing
[00:16:25.360 --> 00:16:26.360]   with it.
[00:16:26.360 --> 00:16:30.920]   Yeah, there was a certain amount of self-flagellation that everybody was really into, right?
[00:16:30.920 --> 00:16:35.960]   How deep down this rabbit hole of absolute horror can we all get to?
[00:16:35.960 --> 00:16:38.160]   It was like none of this matters.
[00:16:38.160 --> 00:16:42.560]   I mean, it doesn't matter in the way that everybody's making it matter.
[00:16:42.560 --> 00:16:50.960]   The issue is that the more of a focus we put on robots that are going to come and kill
[00:16:50.960 --> 00:16:57.680]   us in our sleep, that's a nice distraction because what's actually happening is in the
[00:16:57.680 --> 00:17:02.480]   meantime, so there's a land grab that's happening right now.
[00:17:02.480 --> 00:17:10.840]   Every one of the companies that we deal with, I'm mostly advising C-suites of enormous companies
[00:17:10.840 --> 00:17:12.600]   all around the world.
[00:17:12.600 --> 00:17:16.880]   Suddenly people who have no background on this are having to make, this is a very complex
[00:17:16.880 --> 00:17:19.400]   operating environment for executives.
[00:17:19.400 --> 00:17:23.640]   Suddenly, they're supposed to be able to make decisions about AI.
[00:17:23.640 --> 00:17:25.600]   They're all torqued up about it.
[00:17:25.600 --> 00:17:28.480]   There's this just collective like, "We're going to miss the boat.
[00:17:28.480 --> 00:17:32.680]   Let's give whatever company, all of our data.
[00:17:32.680 --> 00:17:34.600]   Magically, this will solve our problems."
[00:17:34.600 --> 00:17:37.800]   That's kind of what Facebook did with the metaverse.
[00:17:37.800 --> 00:17:38.800]   20 billion.
[00:17:38.800 --> 00:17:41.040]   I don't think so at all.
[00:17:41.040 --> 00:17:44.080]   I think, were you being sarcastic?
[00:17:44.080 --> 00:17:45.960]   No, I was being sincere.
[00:17:45.960 --> 00:17:47.640]   I was serious.
[00:17:47.640 --> 00:17:50.520]   It's kind of the MO for a lot of the tech industry.
[00:17:50.520 --> 00:17:52.360]   What's the next big thing?
[00:17:52.360 --> 00:17:56.160]   Let's jump on it before anybody else.
[00:17:56.160 --> 00:17:57.600]   Sorry, I just said the F word.
[00:17:57.600 --> 00:18:00.040]   I shouldn't have this.
[00:18:00.040 --> 00:18:01.760]   Here's an AI problem.
[00:18:01.760 --> 00:18:04.320]   So, everybody's been at me.
[00:18:04.320 --> 00:18:12.640]   I finally switched back over to an iPhone and this conversational system will not stop.
[00:18:12.640 --> 00:18:14.080]   She's horrible.
[00:18:14.080 --> 00:18:19.240]   By the way, Google did not mention Google Assistant at all.
[00:18:19.240 --> 00:18:22.280]   You mean the duplex thing?
[00:18:22.280 --> 00:18:28.360]   No, the thing on your phone that you talk to that is what most people consider a conversational
[00:18:28.360 --> 00:18:29.360]   AI.
[00:18:29.360 --> 00:18:32.680]   Because I think search is all that matters for them.
[00:18:32.680 --> 00:18:33.680]   I think it's a given that you're not talking about.
[00:18:33.680 --> 00:18:38.040]   It sounds like you're saying that tech industry is running around like a chicken with its head.
[00:18:38.040 --> 00:18:42.520]   No, I'm saying the tech industry may look a little unfocused.
[00:18:42.520 --> 00:18:48.760]   But what's happening, I think, behind the scenes is there is an enormous gold rush right
[00:18:48.760 --> 00:18:51.040]   now and the gold is not the AI.
[00:18:51.040 --> 00:18:52.440]   The gold are all of the enterprise.
[00:18:52.440 --> 00:18:53.440]   It's the enterprise.
[00:18:53.440 --> 00:18:54.440]   It's the businesses.
[00:18:54.440 --> 00:18:55.440]   Right.
[00:18:55.440 --> 00:18:57.160]   You're trying to gobble them up.
[00:18:57.160 --> 00:19:01.440]   Or maybe it's hungry, hungry hippos and you're pushing the game from the...
[00:19:01.440 --> 00:19:02.520]   So I think that's what's happening.
[00:19:02.520 --> 00:19:07.440]   I think that there's this mad dash right now because there's this perceived...
[00:19:07.440 --> 00:19:12.040]   We're all behind the curve and we all need to start using these tools as soon as possible
[00:19:12.040 --> 00:19:13.040]   is what I think happens.
[00:19:13.040 --> 00:19:17.640]   Phil, most of your investments look like they're consumer-focused.
[00:19:17.640 --> 00:19:18.640]   We have quite a mix.
[00:19:18.640 --> 00:19:19.640]   We have consumer.
[00:19:19.640 --> 00:19:20.640]   We have enterprise.
[00:19:20.640 --> 00:19:21.640]   But I think...
[00:19:21.640 --> 00:19:27.000]   Look, I think just like the original gold rush, I agree there's a gold rush.
[00:19:27.000 --> 00:19:32.360]   And what happens in the gold rush back to the actual gold rush is very few people make
[00:19:32.360 --> 00:19:36.720]   money on gold because it turns out there's not that much gold.
[00:19:36.720 --> 00:19:38.440]   People make money right, picks and shovels.
[00:19:38.440 --> 00:19:40.760]   They make money selling stuff to the gold rush.
[00:19:40.760 --> 00:19:44.160]   Which is why Microsoft loves this because they have Azure.
[00:19:44.160 --> 00:19:45.160]   Yeah.
[00:19:45.160 --> 00:19:46.160]   And that's what's happening now.
[00:19:46.160 --> 00:19:47.160]   There's really not...
[00:19:47.160 --> 00:19:51.600]   It's very unclear to me what the actual transformation is.
[00:19:51.600 --> 00:19:58.440]   Again, when I talk to very smart people, to me it's almost a replay of some of the web
[00:19:58.440 --> 00:19:59.920]   three conversations that were saying.
[00:19:59.920 --> 00:20:03.520]   A few years ago, people were very smart people who have a lot of respect for it.
[00:20:03.520 --> 00:20:05.200]   We're telling me about how web three is going to change everything.
[00:20:05.200 --> 00:20:08.640]   And I was like, "I don't understand what you're saying."
[00:20:08.640 --> 00:20:11.440]   I see your face all moving.
[00:20:11.440 --> 00:20:14.360]   And I understand the words that are coming out of it, but they don't form what go here
[00:20:14.360 --> 00:20:17.080]   and thought for me.
[00:20:17.080 --> 00:20:18.600]   And I'm getting a similar sense.
[00:20:18.600 --> 00:20:22.960]   I'm not as confident that AI is quite as empty as web three.
[00:20:22.960 --> 00:20:25.440]   I am still confident that web three is nothing.
[00:20:25.440 --> 00:20:27.760]   AI, obviously there's stuff there.
[00:20:27.760 --> 00:20:33.200]   And I'm still very bullish about the ability of AI in the larger sense to completely transform
[00:20:33.200 --> 00:20:34.400]   everything.
[00:20:34.400 --> 00:20:39.040]   I just think that the current LLM phase, I can't help but think that it's vastly overhyped.
[00:20:39.040 --> 00:20:43.800]   Is the problem it's counterfactual?
[00:20:43.800 --> 00:20:48.480]   I think, again, the best way I can think about the problem, and it's always possible that
[00:20:48.480 --> 00:20:49.480]   I'm wrong about this.
[00:20:49.480 --> 00:20:50.480]   I want to pre-fit with that.
[00:20:50.480 --> 00:20:52.640]   It is entirely possible that I'm just wrong about this.
[00:20:52.640 --> 00:20:54.160]   And I hope I'm wrong about this.
[00:20:54.160 --> 00:20:55.160]   I really do.
[00:20:55.160 --> 00:20:58.800]   I hope that the thing is about to solve all of our problems.
[00:20:58.800 --> 00:21:03.360]   I'm not sure I hope you're wrong because it's scary, but OK.
[00:21:03.360 --> 00:21:11.680]   But I'm kind of seeing this, again, I think we've confused the ability to make it glib,
[00:21:11.680 --> 00:21:13.960]   to make a talk with it, to do something.
[00:21:13.960 --> 00:21:20.680]   When I was a kid, I remember Gary Kasparov said that a computer will never be a human
[00:21:20.680 --> 00:21:21.680]   grandmaster at chess.
[00:21:21.680 --> 00:21:23.160]   I was like 10 years old when he said that.
[00:21:23.160 --> 00:21:25.160]   He was a world champion at the time.
[00:21:25.160 --> 00:21:26.160]   World champion at the time.
[00:21:26.160 --> 00:21:27.760]   Brilliant, brilliant person still is.
[00:21:27.760 --> 00:21:31.880]   And I remember thinking at the time, you don't understand what computers are, and you don't
[00:21:31.880 --> 00:21:33.680]   understand what chess is if you think that.
[00:21:33.680 --> 00:21:39.120]   Because obviously, within a couple of years, he had lost, and now no one would expect humans
[00:21:39.120 --> 00:21:40.120]   to be competitive with chess.
[00:21:40.120 --> 00:21:44.400]   But also, back then, we thought that playing chess was like, you really had to think, and
[00:21:44.400 --> 00:21:48.400]   it was like a deep strategy involved, and now we understand that you could do that mechanically.
[00:21:48.400 --> 00:21:50.840]   And the same thing happened with Go and a bunch of other things.
[00:21:50.840 --> 00:21:51.840]   Well, it turns out.
[00:21:51.840 --> 00:21:54.480]   It was really kind of, to me, very surprising and delightful.
[00:21:54.480 --> 00:21:57.000]   It turns out that talking is the same thing.
[00:21:57.000 --> 00:22:01.120]   It turns out that you can auto-generate in George Orwell in 1984 style.
[00:22:01.120 --> 00:22:05.840]   You can just pull a crank and very compelling words stream out of it.
[00:22:05.840 --> 00:22:10.200]   As long as you don't care if they're accurate or factual.
[00:22:10.200 --> 00:22:11.200]   And a bunch of other things.
[00:22:11.200 --> 00:22:16.080]   But it turns out that just talking, just playing chess doesn't mean that especially
[00:22:16.080 --> 00:22:17.240]   you're thinking about everything.
[00:22:17.240 --> 00:22:18.920]   It's not that special.
[00:22:18.920 --> 00:22:24.640]   And now, we love the fact that we're infatuated with the fact that these things can talk and
[00:22:24.640 --> 00:22:26.280]   draw and do stuff like that.
[00:22:26.280 --> 00:22:31.240]   And we're assigning to them all sorts of other superpowers, which I see no reason to think
[00:22:31.240 --> 00:22:32.240]   that they have.
[00:22:32.240 --> 00:22:33.240]   They're not actually super smart.
[00:22:33.240 --> 00:22:35.160]   They're just super good artists.
[00:22:35.160 --> 00:22:38.760]   Well, I would like to push back a touch.
[00:22:38.760 --> 00:22:43.440]   Because I think if we're just talking about consumer applications, that's one thing.
[00:22:43.440 --> 00:22:47.960]   But Gen AI is doing really interesting things in science.
[00:22:47.960 --> 00:22:55.040]   There's a company called Absai that figured out how to use a zero shot generative AI
[00:22:55.040 --> 00:23:00.880]   system to create, I think, something like 3 million novel combinations of molecules per
[00:23:00.880 --> 00:23:04.240]   week trying to find new therapeutic antibodies.
[00:23:04.240 --> 00:23:09.400]   And the reason that this is significant is because it would take, I don't know how many
[00:23:09.400 --> 00:23:10.840]   scientists.
[00:23:10.840 --> 00:23:17.560]   So it would be impossible for a team of scientists to do this on their own without that type
[00:23:17.560 --> 00:23:19.160]   of assistive tech.
[00:23:19.160 --> 00:23:23.600]   So I actually do think it's the infrastructure that matters right now.
[00:23:23.600 --> 00:23:26.960]   But the thing that we're all talking about, and quite frankly, I'm exhausted listening
[00:23:26.960 --> 00:23:29.760]   to us talk about is not us.
[00:23:29.760 --> 00:23:32.040]   Fill your awesome, obviously.
[00:23:32.040 --> 00:23:37.320]   But just generally, you know, is the same conversations that I heard it Ted.
[00:23:37.320 --> 00:23:39.560]   It's the same conversations I'm hearing everywhere.
[00:23:39.560 --> 00:23:45.680]   There's actually really interesting, really compelling opportunities here.
[00:23:45.680 --> 00:23:48.360]   But there's not in the spaces where people are used to looking.
[00:23:48.360 --> 00:23:50.840]   Well, and they're not chat GPT based.
[00:23:50.840 --> 00:23:52.720]   Well, I'm not talking about chat GPT.
[00:23:52.720 --> 00:23:58.720]   I'm talking about a generative AI system, which is a different thing.
[00:23:58.720 --> 00:24:01.480]   Well, it's related to the chest problem though.
[00:24:01.480 --> 00:24:06.200]   In a constrained environment, you can do amazing things, right?
[00:24:06.200 --> 00:24:11.080]   If you're saying, I want to do gene folding, I can do it much faster with AI.
[00:24:11.080 --> 00:24:13.200]   And Google showed that at Google I/O.
[00:24:13.200 --> 00:24:14.200]   Yeah.
[00:24:14.200 --> 00:24:15.200]   Then we've ever done before.
[00:24:15.200 --> 00:24:17.480]   That's a huge breakthrough.
[00:24:17.480 --> 00:24:19.120]   At some point it's a meaningless label, right?
[00:24:19.120 --> 00:24:20.120]   Like AI.
[00:24:20.120 --> 00:24:21.120]   Yeah.
[00:24:21.120 --> 00:24:22.120]   At some point it's just, it's just like clock chain.
[00:24:22.120 --> 00:24:23.280]   It's like web three.
[00:24:23.280 --> 00:24:24.640]   It's like, it's a buzz.
[00:24:24.640 --> 00:24:26.160]   Well, no, those are just nonsense.
[00:24:26.160 --> 00:24:31.920]   But yeah, but AI covers a range of technologies that are, so Amy's point, very useful and
[00:24:31.920 --> 00:24:33.520]   very useful for very many things.
[00:24:33.520 --> 00:24:34.520]   Yeah.
[00:24:34.520 --> 00:24:35.600]   They're just like, but they've always been there.
[00:24:35.600 --> 00:24:40.880]   It's not like there's been this, the thing that's had a sharp recent breakthrough, right?
[00:24:40.880 --> 00:24:42.240]   Is large language models.
[00:24:42.240 --> 00:24:43.240]   It is chat GPT.
[00:24:43.240 --> 00:24:44.520]   It is things like mid-journey.
[00:24:44.520 --> 00:24:49.080]   It's specifically the thing that shocked all of us in the past six months.
[00:24:49.080 --> 00:24:51.440]   Is there a, is there a, is how glib they've become?
[00:24:51.440 --> 00:24:56.160]   That's taken a massively forward and that's caused all this attention and all the investment.
[00:24:56.160 --> 00:25:00.240]   But that leap forward, the look forward in glib-ness does not actually correspond to
[00:25:00.240 --> 00:25:02.000]   like a significant advance and anything else.
[00:25:02.000 --> 00:25:06.640]   I think all the other stuff has been interesting progress for, you know, for the past decade
[00:25:06.640 --> 00:25:07.640]   and will continue.
[00:25:07.640 --> 00:25:12.120]   And I'm very optimistic about where algorithms are going to go in the next 10 years.
[00:25:12.120 --> 00:25:16.720]   And I'm not super optimistic that large language models are anything but a kind of a, you know,
[00:25:16.720 --> 00:25:18.120]   a diversion from that.
[00:25:18.120 --> 00:25:23.560]   Didn't Demis Hasabas recently talk about the next generation of all of this is not needing
[00:25:23.560 --> 00:25:26.080]   the large language model?
[00:25:26.080 --> 00:25:27.080]   Hopefully.
[00:25:27.080 --> 00:25:28.080]   Yeah.
[00:25:28.080 --> 00:25:29.080]   I think that's a good point.
[00:25:29.080 --> 00:25:30.080]   Yeah.
[00:25:30.080 --> 00:25:32.080]   Because it's not like something that they're like some basic research they're working on.
[00:25:32.080 --> 00:25:33.080]   Hopefully.
[00:25:33.080 --> 00:25:34.080]   So there's a, there's a,
[00:25:34.080 --> 00:25:36.000]   He's the CEO of DeepMind, we should explain.
[00:25:36.000 --> 00:25:38.440]   Is it, it's DeepMind, DeepMind, so brain and brain.
[00:25:38.440 --> 00:25:40.040]   They merged DeepMind with brain.
[00:25:40.040 --> 00:25:41.040]   They merged the two together.
[00:25:41.040 --> 00:25:42.760]   DeepMind kept their branding.
[00:25:42.760 --> 00:25:43.760]   Right.
[00:25:43.760 --> 00:25:45.760]   That's the name of it.
[00:25:45.760 --> 00:25:46.760]   Yeah.
[00:25:46.760 --> 00:25:50.520]   There's a, you know, there was this book that was that came out a few years ago.
[00:25:50.520 --> 00:25:52.120]   Not about this, but it's called Jobs.
[00:25:52.120 --> 00:25:53.800]   I'm trying to remember the author.
[00:25:53.800 --> 00:25:55.640]   That's a great name.
[00:25:55.640 --> 00:25:56.640]   It's a great name.
[00:25:56.640 --> 00:25:58.360]   It's kind of a serious, you know, economics book.
[00:25:58.360 --> 00:26:02.080]   There's a bullet jobs theory is doesn't have much to do with AI.
[00:26:02.080 --> 00:26:04.920]   It basically says that there's many, many, many, many, many, many, many people, entire
[00:26:04.920 --> 00:26:09.840]   industries worth that have jobs that are jobs that like actually the job is just like,
[00:26:09.840 --> 00:26:11.360]   it's just sort of self fulfilling.
[00:26:11.360 --> 00:26:12.960]   It doesn't actually do very much.
[00:26:12.960 --> 00:26:15.000]   It's kind of middle, middle, middle management.
[00:26:15.000 --> 00:26:16.000]   There's a lot of jobs.
[00:26:16.000 --> 00:26:17.760]   And middle and upper and lots of industries.
[00:26:17.760 --> 00:26:18.760]   Yeah.
[00:26:18.760 --> 00:26:20.920]   And they don't actually like produce very much.
[00:26:20.920 --> 00:26:23.200]   And these tend to be pretty high paying jobs.
[00:26:23.200 --> 00:26:26.720]   This is by the way, it's like brilliant David Graber who's passed away.
[00:26:26.720 --> 00:26:29.920]   But yeah, he wrote Dead and the dawn of everything.
[00:26:29.920 --> 00:26:30.920]   Yeah.
[00:26:30.920 --> 00:26:31.920]   Quite an amazing fellow.
[00:26:31.920 --> 00:26:32.920]   Yeah.
[00:26:32.920 --> 00:26:36.560]   It's a, it's a really good book and it, it, it, most of these are white collar pretty
[00:26:36.560 --> 00:26:37.560]   well-hipped paid jobs.
[00:26:37.560 --> 00:26:41.600]   There's very few like both jobs, you know, in construction, like we tend not to pay people
[00:26:41.600 --> 00:26:44.440]   to like, pick holes and fill them up again.
[00:26:44.440 --> 00:26:49.200]   But we, but we pay a lot of people to like fill out forms and like write summaries of
[00:26:49.200 --> 00:26:51.200]   other forms.
[00:26:51.200 --> 00:26:58.680]   And the HR consultants, communication coordinators, telemarketing researchers, corporate lawyers,
[00:26:58.680 --> 00:27:02.560]   and I'd add Douglas, Douglas Adams, phone sanitizers.
[00:27:02.560 --> 00:27:03.560]   That's right.
[00:27:03.560 --> 00:27:04.560]   Yeah.
[00:27:04.560 --> 00:27:05.560]   Yeah.
[00:27:05.560 --> 00:27:13.880]   And there is that danger that we all die of a, of a, of an infection spread by dirty telephone.
[00:27:13.880 --> 00:27:14.880]   That's right.
[00:27:14.880 --> 00:27:15.880]   That's right.
[00:27:15.880 --> 00:27:17.800]   As Douglas Adams predicted.
[00:27:17.800 --> 00:27:21.840]   But what the generative large language models are doing is they're proving to be really
[00:27:21.840 --> 00:27:24.560]   good at jobs that don't need to be done in the first place.
[00:27:24.560 --> 00:27:26.360]   They're at the BS jobs.
[00:27:26.360 --> 00:27:27.360]   Right.
[00:27:27.360 --> 00:27:28.360]   Yes.
[00:27:28.360 --> 00:27:33.320]   And like, and, and I think, I think, I think it would be wonderful if we said this is a
[00:27:33.320 --> 00:27:37.240]   temporary thing to getting rid of the BS jobs, not automating them because I don't need a
[00:27:37.240 --> 00:27:38.240]   computer doing them either.
[00:27:38.240 --> 00:27:39.800]   I just don't need them to be done.
[00:27:39.800 --> 00:27:40.800]   Yeah.
[00:27:40.800 --> 00:27:43.960]   But there's a great danger that, that in fact, instead of doing that, they actually just make
[00:27:43.960 --> 00:27:45.600]   it much worse.
[00:27:45.600 --> 00:27:48.040]   The example to that is, for example, DocuSign.
[00:27:48.040 --> 00:27:49.040]   Right.
[00:27:49.040 --> 00:27:50.040]   I think DocuSign, not AI.
[00:27:50.040 --> 00:27:54.720]   This is a pre AI company, but DocuSign, which I use multiple times a day every day,
[00:27:54.720 --> 00:27:55.720]   right?
[00:27:55.720 --> 00:27:56.720]   So in some sense, it's a great product.
[00:27:56.720 --> 00:28:00.200]   On the other sense, it's horrendous because what does DocuSign do?
[00:28:00.200 --> 00:28:01.200]   Right?
[00:28:01.200 --> 00:28:05.080]   It's easy for people to not read contracts, not read what they're signing.
[00:28:05.080 --> 00:28:09.000]   And so it's like, oh, there used to be this giant bureaucratic thing of, you know, sending
[00:28:09.000 --> 00:28:11.640]   contracts around that it was of giant pain in the ass.
[00:28:11.640 --> 00:28:13.960]   And DocuSign made it easier to just skip over that stuff.
[00:28:13.960 --> 00:28:16.400]   But it hasn't eliminated all the nonsense.
[00:28:16.400 --> 00:28:17.560]   It's just made it easier.
[00:28:17.560 --> 00:28:22.600]   And so I'm kind of afraid that a lot of these, especially enterprise focused LLM applications,
[00:28:22.600 --> 00:28:27.920]   are just going to be like the supersized DocuSign of everything where we entrench the idiocy
[00:28:27.920 --> 00:28:29.760]   and just say, oh, the computers can handle it.
[00:28:29.760 --> 00:28:32.920]   But that just gets us even further away from understanding what's happening.
[00:28:32.920 --> 00:28:34.680]   We're just making us a dumber faster.
[00:28:34.680 --> 00:28:38.080]   Yeah, automate bureaucracy, not get rid of it.
[00:28:38.080 --> 00:28:39.080]   Just make it faster.
[00:28:39.080 --> 00:28:40.080]   Right.
[00:28:40.080 --> 00:28:41.080]   We should just get rid of it.
[00:28:41.080 --> 00:28:42.080]   Yes.
[00:28:42.080 --> 00:28:43.080]   That would be the right thing.
[00:28:43.080 --> 00:28:44.080]   The smart thing to do.
[00:28:44.080 --> 00:28:45.080]   Yeah.
[00:28:45.080 --> 00:28:47.440]   And then we won't need the AI for the large language models.
[00:28:47.440 --> 00:28:48.440]   Right?
[00:28:48.440 --> 00:28:52.320]   Like if someone is going to send me some bullet points, if someone writes a few bullet points
[00:28:52.320 --> 00:28:55.600]   but they want to send me an email, but they want to make it like add more worse to it.
[00:28:55.600 --> 00:28:59.600]   So they ask, you know, Chad GPT to just like convert those bullet points into an email
[00:28:59.600 --> 00:29:00.600]   that they send me.
[00:29:00.600 --> 00:29:01.600]   I don't want that.
[00:29:01.600 --> 00:29:02.600]   Right?
[00:29:02.600 --> 00:29:03.600]   Dear God, I don't want that.
[00:29:03.600 --> 00:29:05.960]   I don't just send me your bullet points if you can't be.
[00:29:05.960 --> 00:29:06.960]   Yeah, pretty soon.
[00:29:06.960 --> 00:29:11.880]   In fact, Google, I think came this close to demonstrating the notion that you would have
[00:29:11.880 --> 00:29:13.400]   an AI write your email.
[00:29:13.400 --> 00:29:15.520]   He would have an AI respond to your email.
[00:29:15.520 --> 00:29:19.760]   Pretty soon you're out of the whole picture and the AI is just talking to one another.
[00:29:19.760 --> 00:29:21.760]   So we practically demonstrated that.
[00:29:21.760 --> 00:29:22.760]   Right.
[00:29:22.760 --> 00:29:25.120]   So we should just get rid of, we should just cut that all out.
[00:29:25.120 --> 00:29:28.360]   Decision points and that whole thing is just gone, not automated.
[00:29:28.360 --> 00:29:29.360]   Right.
[00:29:29.360 --> 00:29:33.880]   But there is something, Walmart is just trying to pull up the article.
[00:29:33.880 --> 00:29:39.200]   Walmart is using a company to do vendor negotiations.
[00:29:39.200 --> 00:29:43.400]   So it is in this case using an AI system and they're only using it.
[00:29:43.400 --> 00:29:44.400]   It was a pilot.
[00:29:44.400 --> 00:29:46.000]   They're using it for shopping carts.
[00:29:46.000 --> 00:29:47.000]   Packed them, P.
[00:29:47.000 --> 00:29:48.400]   Yep, that's it.
[00:29:48.400 --> 00:29:54.800]   So and what's fascinating is, you know, it doesn't mean that humans or procurement officers
[00:29:54.800 --> 00:29:55.800]   are bad.
[00:29:55.800 --> 00:29:57.000]   It's just that we have to go to meetings.
[00:29:57.000 --> 00:29:58.560]   We have to go to the bathroom.
[00:29:58.560 --> 00:30:00.080]   You know, we're pulled away from different things.
[00:30:00.080 --> 00:30:02.000]   We get sick.
[00:30:02.000 --> 00:30:04.280]   And it takes us time to read.
[00:30:04.280 --> 00:30:09.960]   If you have an AI system negotiating on your behalf instead, it just goes much, much,
[00:30:09.960 --> 00:30:10.960]   much faster.
[00:30:10.960 --> 00:30:17.920]   And it turns out, on the other end, something like 75% of the humans, the human negotiators
[00:30:17.920 --> 00:30:20.720]   preferred working with the AI over other humans.
[00:30:20.720 --> 00:30:23.320]   This reminds me of what we've done to the stock markets, right?
[00:30:23.320 --> 00:30:26.600]   I mean, we basically done that in stock markets.
[00:30:26.600 --> 00:30:28.840]   With ETFs, electronic.
[00:30:28.840 --> 00:30:34.260]   Yeah, just, but I mean, it's like a flash, Michael Lewis's book Flash Boys, or Flash
[00:30:34.260 --> 00:30:35.260]   Boys.
[00:30:35.260 --> 00:30:39.360]   Where all of this stuff is happening at the speed of light, no human interaction.
[00:30:39.360 --> 00:30:41.080]   Arbitrage is happening.
[00:30:41.080 --> 00:30:43.760]   It's just, it's all doing it automatically.
[00:30:43.760 --> 00:30:46.120]   It's not like it's intelligent.
[00:30:46.120 --> 00:30:48.760]   I'll just, I'll just, I'll.
[00:30:48.760 --> 00:30:53.160]   I think again, like this is where I think we all get trapped, which is where we go from
[00:30:53.160 --> 00:30:59.640]   abstraction and ambiguity to suddenly like, look, you know, a calculator.
[00:30:59.640 --> 00:31:04.240]   A calculator is a super effective tool that I remember when I was in elementary school,
[00:31:04.240 --> 00:31:05.880]   we weren't supposed to use.
[00:31:05.880 --> 00:31:07.880]   There's a calculator are ubiquitous now.
[00:31:07.880 --> 00:31:08.880]   Right.
[00:31:08.880 --> 00:31:09.880]   In our phones.
[00:31:09.880 --> 00:31:10.880]   You don't know basic math.
[00:31:10.880 --> 00:31:13.680]   A calculator is going to not be super, you're going to come up with an infinite number of
[00:31:13.680 --> 00:31:16.840]   other types of uses that probably don't have to do with math, but it's not going to be
[00:31:16.840 --> 00:31:19.200]   useful for that purpose.
[00:31:19.200 --> 00:31:23.080]   It's made math faster for most people.
[00:31:23.080 --> 00:31:28.840]   So I think that there are plenty of cases where like if it's contract negotiation or I saw
[00:31:28.840 --> 00:31:35.760]   a very, very interesting AI tool out of the Emirates that does some very clever.
[00:31:35.760 --> 00:31:41.120]   It looks through like your team and your company and their skills and then figures
[00:31:41.120 --> 00:31:46.640]   out where their, what percentage of them are deficient for the type of job that they have
[00:31:46.640 --> 00:31:49.120]   and then recommends how they would need to be up skilled.
[00:31:49.120 --> 00:31:52.480]   It's a really, really clever tool.
[00:31:52.480 --> 00:31:59.360]   It would just again, a human could do it, but it's just faster and better for another,
[00:31:59.360 --> 00:32:01.880]   to use an assistive tool, right?
[00:32:01.880 --> 00:32:06.120]   Which also I would just really quickly put in a plug for the Middle East.
[00:32:06.120 --> 00:32:12.240]   There's actually a lot of interesting stuff happening with AI there, including large language
[00:32:12.240 --> 00:32:19.800]   models in Arabic that are launching in UAE and KSA.
[00:32:19.800 --> 00:32:24.920]   And there are supposedly enormous language models in China that are much bigger than
[00:32:24.920 --> 00:32:34.720]   what we've got here with OpenAI and supposedly took fewer, required less power, but of course
[00:32:34.720 --> 00:32:39.120]   they haven't shown anybody what's in them or how they actually work, so who knows?
[00:32:39.120 --> 00:32:47.720]   But it sounds like though we're kind of saying large language models are not the way to go.
[00:32:47.720 --> 00:32:50.040]   Is that what you're saying Phil?
[00:32:50.040 --> 00:32:54.160]   They're not really the important thing in AI.
[00:32:54.160 --> 00:32:55.160]   That's my strong suspicion.
[00:32:55.160 --> 00:32:56.160]   Yes.
[00:32:56.160 --> 00:32:57.160]   I agree.
[00:32:57.160 --> 00:32:58.160]   You agree, Amy?
[00:32:58.160 --> 00:32:59.160]   Strong suspicion.
[00:32:59.160 --> 00:33:01.400]   LLMs are kind of a dead end.
[00:33:01.400 --> 00:33:04.760]   I don't think it's a dead end as much as it's just the shiny...
[00:33:04.760 --> 00:33:09.480]   It's the next stepping stone on the path, right?
[00:33:09.480 --> 00:33:14.680]   But I think the idea is to get us to a point where we don't need those models anymore.
[00:33:14.680 --> 00:33:18.720]   Instead, you may be feeding in new data, which might be your company data, for example.
[00:33:18.720 --> 00:33:20.480]   That's the problem with LLMs right now.
[00:33:20.480 --> 00:33:22.880]   They're frozen in time.
[00:33:22.880 --> 00:33:29.000]   And it requires such power to be real time and keep up that nobody's doing that as far
[00:33:29.000 --> 00:33:30.720]   as I know.
[00:33:30.720 --> 00:33:37.880]   Although, chat GPT is apparently going to this week, they're adding a bunch of plugins
[00:33:37.880 --> 00:33:42.960]   and real time web search, which sounds to me like they are in fact going to start putting
[00:33:42.960 --> 00:33:44.560]   in real time data.
[00:33:44.560 --> 00:33:45.560]   So that's...
[00:33:45.560 --> 00:33:46.680]   I think this is a platform play.
[00:33:46.680 --> 00:33:52.680]   I think what's coming is it's like a platform layer.
[00:33:52.680 --> 00:33:55.080]   I saw a demo of Instacart.
[00:33:55.080 --> 00:33:58.320]   So you ask chat GPT to write you a recipe.
[00:33:58.320 --> 00:33:59.720]   You put in your parameters, right?
[00:33:59.720 --> 00:34:03.720]   So vegetarian, I like spicy food, I've got a whatever allergy.
[00:34:03.720 --> 00:34:06.000]   Spits out a recipe with the instructions.
[00:34:06.000 --> 00:34:10.920]   And then you could say, "Great, go buy me all of that stuff and buy all of it for me."
[00:34:10.920 --> 00:34:16.480]   And it will go to Instacart and populate your shopping cart with all of those things.
[00:34:16.480 --> 00:34:23.640]   And you say, "Deliver it by whatever window, deliver it to my house by four o'clock today."
[00:34:23.640 --> 00:34:26.120]   So this is really interesting.
[00:34:26.120 --> 00:34:31.360]   But if I was a brand, if I was a CPG, I tested this.
[00:34:31.360 --> 00:34:37.160]   And the problem is that there's not a big variety in the brands that come up, like what
[00:34:37.160 --> 00:34:40.000]   auto populates the shopping cart.
[00:34:40.000 --> 00:34:45.920]   So the search problem that news organizations and brands have had, and as a result, to Phil's
[00:34:45.920 --> 00:34:51.640]   point, a job of SEO with apologies to anybody who works in search engine optimization, that
[00:34:51.640 --> 00:34:57.840]   is coming for this next iteration of invisible computing.
[00:34:57.840 --> 00:34:59.640]   How do I get my brand to pop up first?
[00:34:59.640 --> 00:35:03.360]   Because when I did it, I basically got the exact same brand over and over again for all
[00:35:03.360 --> 00:35:05.000]   the stuff that showed up in my cart.
[00:35:05.000 --> 00:35:06.480]   Yeah, that's no good.
[00:35:06.480 --> 00:35:08.080]   I think, but that's early, do you?
[00:35:08.080 --> 00:35:10.000]   Maybe it'd get better.
[00:35:10.000 --> 00:35:14.640]   I think there's kind of two-- we have this rubric at all turtles at our studio.
[00:35:14.640 --> 00:35:18.000]   There's like, sort of, broadly speaking, there's two types of startups.
[00:35:18.000 --> 00:35:22.520]   There's like two types of ideas of tech products, of companies.
[00:35:22.520 --> 00:35:27.160]   There's stuff that solves problems, and then there's stuff that doesn't.
[00:35:27.160 --> 00:35:30.520]   And there's stuff that's just like creates something innovative that's artistic, but it
[00:35:30.520 --> 00:35:32.400]   wasn't a problem.
[00:35:32.400 --> 00:35:34.720]   And I don't know very much about that type.
[00:35:34.720 --> 00:35:37.880]   I really focus on the things that solve problems.
[00:35:37.880 --> 00:35:41.000]   And there's nothing wrong with things that don't solve problems.
[00:35:41.000 --> 00:35:44.840]   There's just beautiful and artistic and unexpectedly delightful.
[00:35:44.840 --> 00:35:45.840]   There's many of those things.
[00:35:45.840 --> 00:35:48.960]   Yeah, mid-journey is cool whether it's valuable or not.
[00:35:48.960 --> 00:35:50.200]   It's still fun to play with.
[00:35:50.200 --> 00:35:51.840]   Well, and so it was Instagram.
[00:35:51.840 --> 00:35:54.800]   Arguably, Instagram didn't solve a problem, but it's still--
[00:35:54.800 --> 00:35:56.280]   Creative planning, but it does solve--
[00:35:56.280 --> 00:35:57.840]   All sorts of things.
[00:35:57.840 --> 00:36:00.640]   It ruined travel for the most of us, but OK.
[00:36:00.640 --> 00:36:07.200]   But I think it's important to not-- I think there's a lot of ideas masquerade as solving
[00:36:07.200 --> 00:36:08.360]   a problem when they're not.
[00:36:08.360 --> 00:36:12.840]   And the way you know that is you just have to ask, is this a real problem?
[00:36:12.840 --> 00:36:15.760]   And usually, problems that are real have been around for a while.
[00:36:15.760 --> 00:36:16.760]   Right.
[00:36:16.760 --> 00:36:17.760]   There's a reason.
[00:36:17.760 --> 00:36:21.320]   Solutions could be new, but the problems have been around for a while.
[00:36:21.320 --> 00:36:28.040]   So if you say, if you were to take every human being on the planet and you rode down
[00:36:28.040 --> 00:36:33.160]   the top 20 problems or the top 100 problems for every single human being on the planet,
[00:36:33.160 --> 00:36:38.440]   in what percentage of those would, it's too difficult for me to find a recipe that I want
[00:36:38.440 --> 00:36:40.200]   and then buy it from Instacart.
[00:36:40.200 --> 00:36:42.040]   Like, where would that rank as a problem?
[00:36:42.040 --> 00:36:45.880]   And I would say, no one in the history of the universe has ever said, you know what
[00:36:45.880 --> 00:36:54.200]   I wish were better, looking at recipes on the internet and then automating by an Instacart.
[00:36:54.200 --> 00:36:56.680]   Like, now it's ever expressed that as a problem.
[00:36:56.680 --> 00:36:58.320]   It is actually a problem for me.
[00:36:58.320 --> 00:36:59.560]   So I'm on the road a bunch.
[00:36:59.560 --> 00:37:00.920]   I tend to cook dinner.
[00:37:00.920 --> 00:37:01.920]   Right.
[00:37:01.920 --> 00:37:02.920]   Often for my family.
[00:37:02.920 --> 00:37:07.080]   And I got to tell you, like on a Tuesday after I've been in 16 hours of meetings, the
[00:37:07.080 --> 00:37:09.920]   last thing I want to do is deal with dinner.
[00:37:09.920 --> 00:37:10.920]   Right.
[00:37:10.920 --> 00:37:13.120]   So yeah, I have a solution to that.
[00:37:13.120 --> 00:37:14.120]   I've been around for that.
[00:37:14.120 --> 00:37:15.120]   I got a solution.
[00:37:15.120 --> 00:37:17.320]   Our sponsor, HelloFresh.
[00:37:17.320 --> 00:37:19.640]   Happy Mother's Day, by the way, Amy.
[00:37:19.640 --> 00:37:21.640]   I should have said that right away.
[00:37:21.640 --> 00:37:24.320]   And this would be a great gift for Mother's Day.
[00:37:24.320 --> 00:37:25.520]   It's not too late.
[00:37:25.520 --> 00:37:29.720]   Our sponsor, HelloFresh, will come back and talk more about AI in just a second.
[00:37:29.720 --> 00:37:31.240]   But you gave me such a good.
[00:37:31.240 --> 00:37:33.880]   Well, I mean, we can go with it.
[00:37:33.880 --> 00:37:35.440]   I cannot ignore it.
[00:37:35.440 --> 00:37:37.960]   We, you know, we got our HelloFresh.
[00:37:37.960 --> 00:37:38.960]   HelloFresh box.
[00:37:38.960 --> 00:37:41.320]   I think it was a couple of weeks ago.
[00:37:41.320 --> 00:37:42.320]   And I made this.
[00:37:42.320 --> 00:37:46.000]   This was really good pork sausage and bell pepper risotto.
[00:37:46.000 --> 00:37:50.200]   HelloFresh delivered all the ingredients I needed and exactly the right proportion.
[00:37:50.200 --> 00:37:53.440]   And then walked me through the preparation.
[00:37:53.440 --> 00:37:56.040]   And it was a great lesson in how to make risotto.
[00:37:56.040 --> 00:37:57.560]   It was fantastic.
[00:37:57.560 --> 00:38:01.520]   And now I've got a whole new thing in my repertoire.
[00:38:01.520 --> 00:38:06.480]   Flavor is in full bloom at HelloFresh because they have added some really wonderful new
[00:38:06.480 --> 00:38:13.040]   tastes, chef crafted, chef crafted recipes with ripe seasonal ingredients delivered right
[00:38:13.040 --> 00:38:14.040]   to your door.
[00:38:14.040 --> 00:38:18.520]   By the way, the peppers in this were, I've never seen such perfect peppers.
[00:38:18.520 --> 00:38:21.280]   The sausage was flavorful and delicious.
[00:38:21.280 --> 00:38:22.720]   It was incredible.
[00:38:22.720 --> 00:38:25.480]   The lemons that garlic everything in here.
[00:38:25.480 --> 00:38:26.480]   This may.
[00:38:26.480 --> 00:38:31.360]   HelloFresh is celebrating Asian American and Pacific Islander heritage months with some
[00:38:31.360 --> 00:38:35.840]   limited time, authentic recipes created in partnership with Serbi Sani.
[00:38:35.840 --> 00:38:40.400]   He's the chef at Tagmo in New York City.
[00:38:40.400 --> 00:38:44.600]   Enjoy a cultural taste to a right in your own kitchen.
[00:38:44.600 --> 00:38:46.880]   Some really mouthwatering.
[00:38:46.880 --> 00:38:48.760]   Amazing things.
[00:38:48.760 --> 00:38:54.040]   Now, we were trained during the pandemic to call for takeout.
[00:38:54.040 --> 00:38:59.200]   And I think it's time to step back a little bit and get HelloFresh instead and start feeding
[00:38:59.200 --> 00:39:06.280]   your family without the high price tag, without all the bad stuff that restaurant food puts
[00:39:06.280 --> 00:39:07.280]   in there.
[00:39:07.280 --> 00:39:10.840]   These are quick and easy meals that are fresh, fast and delicious.
[00:39:10.840 --> 00:39:14.680]   Their new fast and fresh options are ready in 15 minutes or less.
[00:39:14.680 --> 00:39:16.120]   And the quality is superb.
[00:39:16.120 --> 00:39:17.680]   HelloFresh cares about quality.
[00:39:17.680 --> 00:39:21.200]   That's why their seasonal ingredients are picked at peak ripeness and travel from the
[00:39:21.200 --> 00:39:23.760]   farm to your home in less than seven days.
[00:39:23.760 --> 00:39:26.040]   So you know they're fresh and I will vouch for that.
[00:39:26.040 --> 00:39:28.480]   I'm blown away by the quality.
[00:39:28.480 --> 00:39:30.240]   They do more than just delicious dinners.
[00:39:30.240 --> 00:39:34.240]   You can not only take your pick from 40 weekly recipes, which change all the time.
[00:39:34.240 --> 00:39:38.120]   You could choose from more than 100 items to round out your order.
[00:39:38.120 --> 00:39:41.960]   Snacks, lunches, desserts, pantry necessities.
[00:39:41.960 --> 00:39:44.240]   All of a sudden everything you need is coming.
[00:39:44.240 --> 00:39:46.560]   In one box in the delivery day you choose.
[00:39:46.560 --> 00:39:51.800]   You don't have to scour the grocery store for that ingredient or to complete your recipe
[00:39:51.800 --> 00:39:56.240]   because you know everything you need for this recipe is in the box.
[00:39:56.240 --> 00:39:58.040]   Fresh pre-proportion ingredients.
[00:39:58.040 --> 00:40:00.120]   So you have just what you need.
[00:40:00.120 --> 00:40:02.720]   Plus that cuts down on food waste.
[00:40:02.720 --> 00:40:06.920]   Also if you're hosting get together, HelloFresh and HelloFresh Market.
[00:40:06.920 --> 00:40:09.640]   Make dinner time a snap with deliciously easy options.
[00:40:09.640 --> 00:40:15.680]   It'll please everyone at your table from fit and wholesome to Pescatarian to veggie.
[00:40:15.680 --> 00:40:18.920]   You've got a meal plan that suits every lifestyle.
[00:40:18.920 --> 00:40:21.760]   You could swap out proteins and size to your liking effect.
[00:40:21.760 --> 00:40:25.680]   That's one of the things that's great on these address and these recipe cards is they
[00:40:25.680 --> 00:40:29.840]   say, oh you want to try it, you know you want to try a custom recipe.
[00:40:29.840 --> 00:40:33.680]   You can modify it with chicken cutlets instead of the pork that kind of thing.
[00:40:33.680 --> 00:40:38.160]   It's really this is really fun and great.
[00:40:38.160 --> 00:40:42.760]   Save money on your growing to do list with the help of HelloFresh.
[00:40:42.760 --> 00:40:47.720]   It's cheaper than grocery shopping and a lot cheaper than takeout.
[00:40:47.720 --> 00:40:49.000]   You can take my word for that.
[00:40:49.000 --> 00:40:50.440]   We could stop that bad habit.
[00:40:50.440 --> 00:40:52.520]   We all got into during the pandemic.
[00:40:52.520 --> 00:40:53.600]   HelloFresh.
[00:40:53.600 --> 00:41:00.840]   It's America's number one meal kit and if you go right now to HelloFresh.com/twit16.
[00:41:00.840 --> 00:41:04.120]   TWIT16, use the offer code TWIT16.
[00:41:04.120 --> 00:41:07.200]   You'll get 16 free meals.
[00:41:07.200 --> 00:41:08.200]   Wow.
[00:41:08.200 --> 00:41:09.680]   Plus free shipping.
[00:41:09.680 --> 00:41:10.960]   Okay.
[00:41:10.960 --> 00:41:11.960]   That's pretty good.
[00:41:11.960 --> 00:41:16.920]   HelloFresh.com/twit16, the offer code TWIT16.
[00:41:16.920 --> 00:41:23.960]   To get 16 free meals plus free shipping and there is no AI in this recipe card.
[00:41:23.960 --> 00:41:25.880]   It's all human intelligence.
[00:41:25.880 --> 00:41:27.640]   Chef designed.
[00:41:27.640 --> 00:41:29.800]   HelloFresh.com/twit16.
[00:41:29.800 --> 00:41:30.960]   I couldn't resist Amy.
[00:41:30.960 --> 00:41:33.720]   You gave me such a good opening for-
[00:41:33.720 --> 00:41:34.920]   It was.
[00:41:34.920 --> 00:41:36.680]   I had to do it.
[00:41:36.680 --> 00:41:38.400]   Let me ask.
[00:41:38.400 --> 00:41:41.400]   This is probably a dumb question.
[00:41:41.400 --> 00:41:44.720]   You made me think of this when you talked about Gary Kasparov.
[00:41:44.720 --> 00:41:50.320]   I remember interviewing Ray Kurzweil and he's been promoting this idea that the singularity,
[00:41:50.320 --> 00:41:53.560]   is this singularity, first of all, he says he said it would be here by now.
[00:41:53.560 --> 00:41:55.360]   Is this the singularity?
[00:41:55.360 --> 00:41:56.360]   No.
[00:41:56.360 --> 00:41:57.360]   No.
[00:41:57.360 --> 00:42:01.320]   Singularity is when machines become as smart as humans start designing their own stuff.
[00:42:01.320 --> 00:42:06.480]   It's a hockey stick and all of a sudden it's exponential growth.
[00:42:06.480 --> 00:42:07.720]   So this is not the singularity.
[00:42:07.720 --> 00:42:13.000]   But I asked, well, is there something different about the way we think and the way a machine
[00:42:13.000 --> 00:42:14.000]   thinks?
[00:42:14.000 --> 00:42:16.840]   And he says, well, if you can't tell the difference, who cares?
[00:42:16.840 --> 00:42:21.400]   But I think Gary Kasparov had as many humans do this notion, there's something about the
[00:42:21.400 --> 00:42:25.040]   thinking I do in chess that is not mechanistic.
[00:42:25.040 --> 00:42:26.040]   It's not deterministic.
[00:42:26.040 --> 00:42:27.720]   It's not something a machine could do.
[00:42:27.720 --> 00:42:30.160]   He was proven wrong.
[00:42:30.160 --> 00:42:36.080]   You've just said conversation turns out a machine can do exactly as well as a human, maybe
[00:42:36.080 --> 00:42:38.240]   better than most.
[00:42:38.240 --> 00:42:44.440]   But do you think that there's something that we do in our brains that is ineffable that
[00:42:44.440 --> 00:42:47.960]   a machine cannot capture?
[00:42:47.960 --> 00:42:49.520]   So a couple of things.
[00:42:49.520 --> 00:42:52.760]   I think Gary Kasparov went on from that early prediction.
[00:42:52.760 --> 00:42:54.720]   He changes to one of the people.
[00:42:54.720 --> 00:42:58.760]   Well, he really invented, he called it centaur chess, which because he's bad at branding,
[00:42:58.760 --> 00:43:00.080]   but it was kind of a brilliant thing.
[00:43:00.080 --> 00:43:04.600]   The idea was that a computer plus a person was going to be much better chess player,
[00:43:04.600 --> 00:43:05.600]   much more interesting either.
[00:43:05.600 --> 00:43:06.600]   Either by the way themselves.
[00:43:06.600 --> 00:43:09.480]   All modern chess masters play this way.
[00:43:09.480 --> 00:43:11.760]   Yeah, for a while.
[00:43:11.760 --> 00:43:15.960]   But he really invented was what we really, and we believe this had ever known a lot,
[00:43:15.960 --> 00:43:18.840]   we said AI really ought to be the A ought to stand for augmented.
[00:43:18.840 --> 00:43:19.840]   It's augmented intelligence.
[00:43:19.840 --> 00:43:20.840]   It's not artificial.
[00:43:20.840 --> 00:43:22.680]   It's not about making a computer the smarter than a person.
[00:43:22.680 --> 00:43:24.280]   We don't want that.
[00:43:24.280 --> 00:43:26.040]   It's about making the person much smarter.
[00:43:26.040 --> 00:43:31.680]   It's about the person and the computer being much smarter than either could be by themselves.
[00:43:31.680 --> 00:43:33.520]   That's the augmented intelligence vision.
[00:43:33.520 --> 00:43:38.240]   And that by the way is why the singularity probably won't happen, because it's not that
[00:43:38.240 --> 00:43:40.080]   the machines are going to leave us behind.
[00:43:40.080 --> 00:43:42.240]   It's that the machines are going to take us with them.
[00:43:42.240 --> 00:43:45.080]   It's going to be the people plus computers kind of together.
[00:43:45.080 --> 00:43:47.400]   Oh, that's a great thing.
[00:43:47.400 --> 00:43:52.680]   Ray's way of putting that was, well, no, they'll appreciate us because we are their creators.
[00:43:52.680 --> 00:43:55.600]   So they'll treat us like their parents.
[00:43:55.600 --> 00:43:57.360]   Hopefully better than a true product.
[00:43:57.360 --> 00:44:00.080]   We need to stop anthropomorphizing.
[00:44:00.080 --> 00:44:01.080]   I agree.
[00:44:01.080 --> 00:44:03.000]   I hate to be the like, the pragmatics.
[00:44:03.000 --> 00:44:04.440]   The pragmatist in the room.
[00:44:04.440 --> 00:44:06.640]   But listen, there's too much woo woo.
[00:44:06.640 --> 00:44:07.640]   I agree.
[00:44:07.640 --> 00:44:11.160]   And it's not there is, but there's also plenty of examples of learned helplessness.
[00:44:11.160 --> 00:44:18.040]   So there are plenty of ways in which the machines have we are no longer truly in charge
[00:44:18.040 --> 00:44:20.400]   of what's happening because we don't choose to be.
[00:44:20.400 --> 00:44:27.120]   Well, for instance, if I now that I use Google Maps whenever I drive, I can't.
[00:44:27.120 --> 00:44:29.120]   I couldn't drive somewhere with Google Maps.
[00:44:29.120 --> 00:44:30.120]   Right.
[00:44:30.120 --> 00:44:31.120]   So that's a good example.
[00:44:31.120 --> 00:44:32.120]   That's a learned helplessness.
[00:44:32.120 --> 00:44:33.120]   Yeah.
[00:44:33.120 --> 00:44:38.000]   A lot of people can't spell because in Japan, so I think a lot of people who listen know
[00:44:38.000 --> 00:44:43.040]   that I lived in Japan for a really long time, they used to, when you applied for a job,
[00:44:43.040 --> 00:44:47.720]   you had to hand write a particular type of resume because you were being graded on your
[00:44:47.720 --> 00:44:52.480]   hand writing and your knowledge of the character set.
[00:44:52.480 --> 00:44:55.240]   You know, that is sort of less important right now.
[00:44:55.240 --> 00:44:58.840]   And even down to something called a hunko, which was a huge bureaucratic mess, but it
[00:44:58.840 --> 00:45:05.160]   was a stamp that had your name on it and you had to ink it like a red ink.
[00:45:05.160 --> 00:45:09.320]   That is largely not totally obsolete, but a lot of that has gone away.
[00:45:09.320 --> 00:45:12.880]   I was a recent news story that they were going to stop using hunkos in the government.
[00:45:12.880 --> 00:45:14.120]   Actually, I know the guy.
[00:45:14.120 --> 00:45:17.560]   I'm friends with the government official who made that.
[00:45:17.560 --> 00:45:18.560]   Yeah.
[00:45:18.560 --> 00:45:19.560]   That was a big deal.
[00:45:19.560 --> 00:45:25.800]   My point is that we've already in some ways given in, but there are plenty of other cases
[00:45:25.800 --> 00:45:31.560]   where, because the question that you asked is like, I think you were asking about agency.
[00:45:31.560 --> 00:45:32.560]   Yeah.
[00:45:32.560 --> 00:45:40.280]   You know, I'm an endurance cyclist and my bike is outfitted with a computer and sensors
[00:45:40.280 --> 00:45:47.040]   everywhere, which could theoretically allow me to check out when I'm doing a really long
[00:45:47.040 --> 00:45:48.600]   hard ride.
[00:45:48.600 --> 00:45:53.720]   But it causes me to focus and check in more because I'm like, neurotically focused on my
[00:45:53.720 --> 00:45:56.240]   output and my cadence and my speed.
[00:45:56.240 --> 00:46:00.200]   So if anything, it's made me much more present.
[00:46:00.200 --> 00:46:02.640]   And this is a powered technology.
[00:46:02.640 --> 00:46:07.960]   I would say for those metrics, yes, but maybe you're not hearing the birds singing in the
[00:46:07.960 --> 00:46:09.120]   and I'm totally not.
[00:46:09.120 --> 00:46:15.560]   I've got I've got jawbone earphones right here, wearing sound garden and Metallica very loudly.
[00:46:15.560 --> 00:46:16.560]   Okay.
[00:46:16.560 --> 00:46:17.560]   There you go.
[00:46:17.560 --> 00:46:24.800]   But my point is like, it's hard for us to have the infrastructure conversations.
[00:46:24.800 --> 00:46:29.600]   Phil said this earlier and it's worth, I think, double clicking on, you know, in as much as
[00:46:29.600 --> 00:46:38.280]   all of this is a gold rush for oxygen in the room for like having the thought leadership
[00:46:38.280 --> 00:46:42.120]   or the conversations or the attention or whatever.
[00:46:42.120 --> 00:46:46.840]   The large language models, the chat GPT, you know, this is kind of like the thing that
[00:46:46.840 --> 00:46:50.400]   we're interested in right now collectively, but it's not what matters.
[00:46:50.400 --> 00:46:52.440]   The stuff that matters is the infrastructure.
[00:46:52.440 --> 00:46:57.520]   But let me point out that if it weren't for the gold miners, we wouldn't have Levi's, right?
[00:46:57.520 --> 00:47:00.120]   We kind of needed that.
[00:47:00.120 --> 00:47:01.120]   Yeah.
[00:47:01.120 --> 00:47:02.320]   To create these infrastructures.
[00:47:02.320 --> 00:47:03.320]   Oh, yeah.
[00:47:03.320 --> 00:47:05.160]   This is a lot of this is about generating.
[00:47:05.160 --> 00:47:06.160]   So right.
[00:47:06.160 --> 00:47:11.400]   So this is about how the conversation is creating value, but it's really about creating revenue
[00:47:11.400 --> 00:47:14.760]   because because there's an enormous amount of investment needed to push a lot of this
[00:47:14.760 --> 00:47:15.760]   forward.
[00:47:15.760 --> 00:47:20.120]   So the additional search on Google is what one fifth the cost or one second of the cost
[00:47:20.120 --> 00:47:21.120]   right now.
[00:47:21.120 --> 00:47:22.520]   According to Sam Hallman, one tenths.
[00:47:22.520 --> 00:47:23.520]   Yeah.
[00:47:23.520 --> 00:47:24.840]   You know, so it's a big difference.
[00:47:24.840 --> 00:47:31.680]   I remember it being said, I don't know if this is true, that all of after the crash of
[00:47:31.680 --> 00:47:37.480]   a two, the dot com crash of 2000, people said, you know what?
[00:47:37.480 --> 00:47:40.880]   It wasn't so bad because of the infrastructure created.
[00:47:40.880 --> 00:47:44.160]   We were able to do what we've done in the intervening 20 years.
[00:47:44.160 --> 00:47:45.920]   They've said the same thing about the railroads.
[00:47:45.920 --> 00:47:49.360]   In the early days of the railroads, almost all the railroad businesses that created the
[00:47:49.360 --> 00:47:51.960]   Transcontinental Railway failed.
[00:47:51.960 --> 00:47:55.800]   They went bankrupt, but we got the infrastructure.
[00:47:55.800 --> 00:47:57.600]   Maybe we're in that same situation.
[00:47:57.600 --> 00:47:58.600]   It is.
[00:47:58.600 --> 00:48:04.080]   And that's what we ought to be talking about because that's what concerns me.
[00:48:04.080 --> 00:48:08.480]   It's not like I have 10,000 GPUs sitting behind me in the room behind me.
[00:48:08.480 --> 00:48:11.080]   No, Microsoft and Google do.
[00:48:11.080 --> 00:48:12.080]   That's my point.
[00:48:12.080 --> 00:48:16.080]   You need a backbone in order to make a lot of this work.
[00:48:16.080 --> 00:48:21.480]   And therefore, the future of the cloud is to some degree being determined by decisions.
[00:48:21.480 --> 00:48:22.480]   Right?
[00:48:22.480 --> 00:48:23.480]   So it's clear.
[00:48:23.480 --> 00:48:24.480]   Microsoft knows that.
[00:48:24.480 --> 00:48:25.920]   Why else would they put $10 billion?
[00:48:25.920 --> 00:48:30.200]   No, but that's the conversation I wish we were having.
[00:48:30.200 --> 00:48:31.760]   Not the anthropomorphized race.
[00:48:31.760 --> 00:48:32.760]   You know, I agree.
[00:48:32.760 --> 00:48:35.120]   Well, the whole hallucination thing.
[00:48:35.120 --> 00:48:38.000]   And I apologize for bringing it up, but I just thought I'd ask the question, obviously.
[00:48:38.000 --> 00:48:41.840]   No, but I mean, listen, I know it's kind of boring, but I feel like this crowd, I feel
[00:48:41.840 --> 00:48:45.920]   like this crowd, if anybody is willing to listen and to talk about it.
[00:48:45.920 --> 00:48:50.880]   Because it's that heart, that technology that really does matter going forward.
[00:48:50.880 --> 00:48:55.480]   And we've been saying that in all our shows for some time, that stop the anthropomorphization.
[00:48:55.480 --> 00:48:58.080]   It's not hallucinating.
[00:48:58.080 --> 00:49:01.920]   Kevin Ruse, chat GPD did not fall in love with you.
[00:49:01.920 --> 00:49:06.400]   That is a mistake to apply that those human attributes.
[00:49:06.400 --> 00:49:08.600]   No, but it's good for clicks.
[00:49:08.600 --> 00:49:13.520]   And so all the journalists having all these experiments where they tricked, I'm air quoting,
[00:49:13.520 --> 00:49:15.640]   you know, tricked AI into falling.
[00:49:15.640 --> 00:49:21.600]   And then they sort of exhibited, you know, human-like language around love and sex or
[00:49:21.600 --> 00:49:23.160]   racism or whatever else.
[00:49:23.160 --> 00:49:25.880]   It's like silly.
[00:49:25.880 --> 00:49:26.880]   It's a way to get clicks.
[00:49:26.880 --> 00:49:28.160]   I'm so tired of it.
[00:49:28.160 --> 00:49:31.040]   I think there are, I think there's a couple of real issues.
[00:49:31.040 --> 00:49:34.680]   One is just to like, because I hate leaving a big question like this open ended, but you
[00:49:34.680 --> 00:49:37.360]   ask that if there's something ineffable, I think you use the word ineffable.
[00:49:37.360 --> 00:49:38.360]   Yes.
[00:49:38.360 --> 00:49:39.360]   That's a good word.
[00:49:39.360 --> 00:49:41.600]   I'm going to say ineffable three more times today.
[00:49:41.600 --> 00:49:43.160]   It's better than good Leo.
[00:49:43.160 --> 00:49:47.840]   I just want to say, you know, I'm saying good Leo from that.
[00:49:47.840 --> 00:49:48.840]   Thank you, Amy.
[00:49:48.840 --> 00:49:52.000]   Whether there's something ineffable about the way humans think they can't reproduce by
[00:49:52.000 --> 00:49:53.000]   computer.
[00:49:53.000 --> 00:49:55.640]   And I very, very, very, very strongly don't think there is.
[00:49:55.640 --> 00:49:58.680]   I don't think there's anything magical that's happening in the we think that couldn't be
[00:49:58.680 --> 00:50:00.200]   done by a computer.
[00:50:00.200 --> 00:50:04.280]   I don't think that L.L.M.s do you get anywhere near that?
[00:50:04.280 --> 00:50:09.040]   So I think there's absolutely the stuff that's going on when people think that is not anywhere
[00:50:09.040 --> 00:50:12.160]   near what's happening right now with large language models and with AI.
[00:50:12.160 --> 00:50:16.520]   But at some point in the future, sure, I don't believe there's a magical thing that you couldn't
[00:50:16.520 --> 00:50:17.520]   get it to do.
[00:50:17.520 --> 00:50:18.520]   Fair enough.
[00:50:18.520 --> 00:50:19.520]   Is this a stepping stone to that?
[00:50:19.520 --> 00:50:20.520]   Right now.
[00:50:20.520 --> 00:50:21.520]   Isn't this a stepping stone to that?
[00:50:21.520 --> 00:50:25.880]   I don't, I, in some sense, but I'm not sure, as Amy said, whether it's a, it may be a stepping
[00:50:25.880 --> 00:50:28.560]   stone or it actually may be, it may be a wrong turn.
[00:50:28.560 --> 00:50:29.560]   And I don't know.
[00:50:29.560 --> 00:50:30.560]   We'll figure that out.
[00:50:30.560 --> 00:50:31.560]   That's interesting.
[00:50:31.560 --> 00:50:36.160]   We'll take all sorts of long turns and correct and we'll get there sooner or later.
[00:50:36.160 --> 00:50:37.160]   It does all the time.
[00:50:37.160 --> 00:50:43.920]   Somebody made a wonderful, it was on Reddit model of what the earth, what the solar system
[00:50:43.920 --> 00:50:48.080]   would have to look like if Ptolemy was right and everything was revolving around the earth.
[00:50:48.080 --> 00:50:50.320]   And there are all sorts of little spinny.
[00:50:50.320 --> 00:50:57.000]   It was a very complicated model compared to the very simple model of if we rotate around
[00:50:57.000 --> 00:50:58.000]   the sun.
[00:50:58.000 --> 00:50:59.560]   But yeah, but I think it's.
[00:50:59.560 --> 00:51:02.320]   Amy's turns all the time.
[00:51:02.320 --> 00:51:03.320]   That's right.
[00:51:03.320 --> 00:51:04.320]   And we shouldn't discourage that, right?
[00:51:04.320 --> 00:51:05.320]   We give ourselves the freedom to do it.
[00:51:05.320 --> 00:51:06.320]   That's the best.
[00:51:06.320 --> 00:51:10.240]   I think Amy's talking about the infrastructure is deeply interesting.
[00:51:10.240 --> 00:51:13.880]   And I agree that that's actually a really interesting topic.
[00:51:13.880 --> 00:51:18.840]   I think if you look at like things like the gold rush, things like the dot com burst,
[00:51:18.840 --> 00:51:22.800]   things where that created a bunch of infrastructure, they really do two things that are useful
[00:51:22.800 --> 00:51:24.280]   in the future.
[00:51:24.280 --> 00:51:27.560]   And the question is, do any, are either of those two things happening now?
[00:51:27.560 --> 00:51:29.240]   And I don't think they are.
[00:51:29.240 --> 00:51:34.200]   The two things that they typically do is they create infrastructure that is usable by other
[00:51:34.200 --> 00:51:35.360]   things in the future.
[00:51:35.360 --> 00:51:36.360]   Right?
[00:51:36.360 --> 00:51:39.880]   So the gold rush created railroads and turns out that you can use railroads for all sorts
[00:51:39.880 --> 00:51:42.520]   of things other than trying to get the San Francisco to mind for gold.
[00:51:42.520 --> 00:51:49.520]   They were generally useful dot com, a bust, a bubble, which I was a very happy part of
[00:51:49.520 --> 00:51:50.520]   those.
[00:51:50.520 --> 00:51:53.960]   But my first, sort of my first company created a bunch of internet infrastructure, a bunch
[00:51:53.960 --> 00:51:58.280]   of backbone, a bunch of stuff like that that was very useful for all sorts of stuff.
[00:51:58.280 --> 00:52:03.320]   I'm not sure that the current infrastructure being created by AI is actually generally
[00:52:03.320 --> 00:52:06.840]   useful because infrastructure has got more and more specific.
[00:52:06.840 --> 00:52:10.040]   Like you can't really take a lot of the GPUs that were used for like crypto mining, for
[00:52:10.040 --> 00:52:11.320]   example, and repurpose them.
[00:52:11.320 --> 00:52:12.320]   Yeah, that's right.
[00:52:12.320 --> 00:52:13.320]   I'm not sure.
[00:52:13.320 --> 00:52:16.760]   I'm not sure that the infrastructure being created is actually going to be useful.
[00:52:16.760 --> 00:52:21.920]   But the other GPUs and they're like, right, but the other thing that these things did,
[00:52:21.920 --> 00:52:25.440]   which was just as important as they concentrated wealth.
[00:52:25.440 --> 00:52:32.000]   And in the past, you needed concentrated wealth to actually fund massive new leaps forward.
[00:52:32.000 --> 00:52:36.040]   So the gold rush in San Francisco gave us James Lick.
[00:52:36.040 --> 00:52:40.880]   If anyone wants a fascinating story, Google James Lick and the Lick Observatory on this
[00:52:40.880 --> 00:52:44.800]   mountain in San Jose, and there's just a guy who made a lot of money selling pianos and
[00:52:44.800 --> 00:52:48.520]   then chocolates, co-found the Girojerely chocolate and then bought a bunch of real estate and
[00:52:48.520 --> 00:52:51.160]   then went on to fund all sorts of amazing stuff.
[00:52:51.160 --> 00:52:57.400]   And previous to that, we had the Rockefellers and the Morgan's and Walmart and things that
[00:52:57.400 --> 00:52:58.880]   concentrate wealth.
[00:52:58.880 --> 00:53:04.040]   It tended to be in the past that it was these sort of dynasties that had more money than
[00:53:04.040 --> 00:53:08.520]   you would do with it, that would then massively invest in other things and move forward.
[00:53:08.520 --> 00:53:12.720]   We're supposed to, if you just had this wealth, those kind of spread out evenly, wouldn't
[00:53:12.720 --> 00:53:16.160]   have enough of a center of gravity to invest in.
[00:53:16.160 --> 00:53:19.160]   Oh, you just made an argument for billionaires.
[00:53:19.160 --> 00:53:26.200]   Well, billionaires and a lot of the companies that kind of came out of the original.com
[00:53:26.200 --> 00:53:32.120]   bubble like Google, like Yahoo, like Facebook, like these companies made a bunch of money
[00:53:32.120 --> 00:53:35.280]   and maybe the thing that they were working on back then kind of imploded, but they had
[00:53:35.280 --> 00:53:36.280]   a bunch of money.
[00:53:36.280 --> 00:53:37.640]   They funded a bunch of research.
[00:53:37.640 --> 00:53:39.920]   There was a lot of fiber buried.
[00:53:39.920 --> 00:53:44.640]   I'm not sure to what extent that's happening right now, because to Amy's point of the big
[00:53:44.640 --> 00:53:48.720]   nine, if it's really the same companies right now that are doing it, right?
[00:53:48.720 --> 00:53:52.880]   I don't know that we're going to get that much new wealth accumulated in that many new
[00:53:52.880 --> 00:53:53.880]   interest hands.
[00:53:53.880 --> 00:53:57.400]   Maybe, maybe we might and I'm not sure if the infrastructure is that reusable.
[00:53:57.400 --> 00:53:58.400]   So I'm I'm overall.
[00:53:58.400 --> 00:53:59.960]   This is a very, very interesting.
[00:53:59.960 --> 00:54:01.440]   I haven't thought about this before.
[00:54:01.440 --> 00:54:02.440]   You're right.
[00:54:02.440 --> 00:54:04.000]   This is an interesting argument you're making.
[00:54:04.000 --> 00:54:06.960]   That's a novel argument because is your right?
[00:54:06.960 --> 00:54:11.440]   Some of the TPUs, some of the new like at Google, I'll say that.
[00:54:11.440 --> 00:54:12.440]   There was a new shit.
[00:54:12.440 --> 00:54:13.920]   There was like a new chip, right?
[00:54:13.920 --> 00:54:15.400]   And everybody's working on new.
[00:54:15.400 --> 00:54:17.800]   Yeah, they put the tensor chip into their phones.
[00:54:17.800 --> 00:54:18.800]   And it's particularly.
[00:54:18.800 --> 00:54:19.800]   Right.
[00:54:19.800 --> 00:54:23.000]   And Microsoft's working on NPUs as well for for their.
[00:54:23.000 --> 00:54:25.400]   I mean, I don't I don't think we go backwards.
[00:54:25.400 --> 00:54:27.120]   I don't think there's a world without AI.
[00:54:27.120 --> 00:54:31.400]   And I think that's tough shit like these moratoriums on development.
[00:54:31.400 --> 00:54:36.320]   And this is a great again, it's it's a huge distraction, but.
[00:54:36.320 --> 00:54:38.520]   Mortarium was not a development, right?
[00:54:38.520 --> 00:54:39.920]   The moratorium that they asked for.
[00:54:39.920 --> 00:54:41.960]   And actually I take it in good faith.
[00:54:41.960 --> 00:54:45.280]   I don't know about, you know, I know Gary Marcus pretty well who was one of the main
[00:54:45.280 --> 00:54:46.280]   people writing it.
[00:54:46.280 --> 00:54:48.360]   Do not question his motives at all.
[00:54:48.360 --> 00:54:49.360]   I don't think so.
[00:54:49.360 --> 00:54:51.080]   You think it's a good idea that we should have a moratorium?
[00:54:51.080 --> 00:54:52.080]   What was the moratorium on?
[00:54:52.080 --> 00:54:53.080]   Not on development.
[00:54:53.080 --> 00:54:54.080]   The moratorium was not on development.
[00:54:54.080 --> 00:54:56.600]   The moratorium was on was on wise case deployment.
[00:54:56.600 --> 00:54:57.600]   Okay.
[00:54:57.600 --> 00:55:03.160]   And it was what open AI said originally, which is this is too dangerous to realize.
[00:55:03.160 --> 00:55:04.160]   Yes.
[00:55:04.160 --> 00:55:05.960]   And I didn't sign that letter.
[00:55:05.960 --> 00:55:08.920]   I don't think anyone's waiting to see which way I go because I'm genuinely conflicted
[00:55:08.920 --> 00:55:12.720]   by it because the general moratoriums do sound silly to me, but I don't question the good
[00:55:12.720 --> 00:55:13.720]   faith nature of it.
[00:55:13.720 --> 00:55:17.560]   And I think there's a strong argument to be made that we really are putting these things
[00:55:17.560 --> 00:55:21.000]   out to hundreds of millions of people, maybe too quickly.
[00:55:21.000 --> 00:55:25.440]   So I don't think there's a reason to they're being put out because the reinforcement with
[00:55:25.440 --> 00:55:30.320]   human feedback reinforcement makes them better learning with human feedback.
[00:55:30.320 --> 00:55:31.520]   We are that we are they.
[00:55:31.520 --> 00:55:32.520]   Yeah.
[00:55:32.520 --> 00:55:37.880]   I mean, that was open AI's argument was why they released it until we release it to the
[00:55:37.880 --> 00:55:38.880]   public.
[00:55:38.880 --> 00:55:39.880]   We need that input.
[00:55:39.880 --> 00:55:43.080]   Yeah, you know, I don't you know, buy that, but sure.
[00:55:43.080 --> 00:55:48.200]   There are so many there's so many easy better, not easier, but better ways to go about safeguarding
[00:55:48.200 --> 00:55:49.280]   the future and regulation.
[00:55:49.280 --> 00:55:51.080]   They just don't the prop.
[00:55:51.080 --> 00:55:54.600]   This is a I'm not going to go down this rabbit hole because as everybody knows, I won't stop
[00:55:54.600 --> 00:56:00.680]   but like traditional the way that we have regulated in the past does not work for our
[00:56:00.680 --> 00:56:05.200]   present and future states when it comes to certain types of technology.
[00:56:05.200 --> 00:56:07.240]   We just need a different path forward.
[00:56:07.240 --> 00:56:10.680]   There are plenty of different avenues.
[00:56:10.680 --> 00:56:13.040]   You know, how likely is that?
[00:56:13.040 --> 00:56:15.440]   I'm not arguing for the profit for the moratorium.
[00:56:15.440 --> 00:56:17.600]   I don't know whether it's a good or not.
[00:56:17.600 --> 00:56:21.380]   I'm just saying that it's not I don't dismiss it 100% out of hand and I I don't question
[00:56:21.380 --> 00:56:23.640]   the motives for many of the people.
[00:56:23.640 --> 00:56:29.540]   And one of the arguments against it was well, some people will stop, but not everybody.
[00:56:29.540 --> 00:56:35.360]   And that just lets China or whoever or the UAE or whoever continue in their process and.
[00:56:35.360 --> 00:56:39.160]   Can I go back to this thing about what Phil said?
[00:56:39.160 --> 00:56:41.520]   I can't stop thinking about this infrastructure thing.
[00:56:41.520 --> 00:56:42.520]   Go ahead.
[00:56:42.520 --> 00:56:45.560]   So, so here's the argument that I'm spinning out in my head.
[00:56:45.560 --> 00:56:51.440]   So the tools so to be fair, there were like there were the railroad barons, right?
[00:56:51.440 --> 00:56:57.520]   So you can't know, not anybody, not just anybody can like plop a train car on that
[00:56:57.520 --> 00:56:59.760]   rail and just go wherever they want.
[00:56:59.760 --> 00:57:04.680]   However, it is still accessible, knowable technology.
[00:57:04.680 --> 00:57:09.560]   And if you if you are if you have the right, whatever cap amount of capital and where with
[00:57:09.560 --> 00:57:12.720]   all and you can negotiate, you can participate.
[00:57:12.720 --> 00:57:19.520]   What Phil has made me think about is these systems really are designed to further the
[00:57:19.520 --> 00:57:24.680]   next generation of AI, which is all already fairly concentrated and entrenched.
[00:57:24.680 --> 00:57:28.800]   And what's happening is the value creation in the future winds up with an even smaller
[00:57:28.800 --> 00:57:35.320]   group of people because it's not like anybody can just hop in and create new value.
[00:57:35.320 --> 00:57:40.920]   That's a really interesting, so this in this time around the sort of this is general purpose
[00:57:40.920 --> 00:57:41.920]   technology, right?
[00:57:41.920 --> 00:57:47.120]   So the infrastructure layer of the more important GPT, which is general purpose technology,
[00:57:47.120 --> 00:57:52.320]   is not like a pick or a shovel, which is a rudimentary tool that anybody could get.
[00:57:52.320 --> 00:57:56.600]   You may not have the land rights and it's not, you know, it's it's different this time
[00:57:56.600 --> 00:57:59.760]   around because it's much more closed.
[00:57:59.760 --> 00:58:05.920]   Is it though, because we all of this LLM is a well known technology.
[00:58:05.920 --> 00:58:08.120]   Nobody owns it.
[00:58:08.120 --> 00:58:10.760]   It requires a certain amount of hardware, but not an infinite amount.
[00:58:10.760 --> 00:58:17.520]   I mean, stable diffusion was able to do what it was able to do without huge capital investment.
[00:58:17.520 --> 00:58:26.320]   You would need to create and train a new model to do something better than what exists.
[00:58:26.320 --> 00:58:31.760]   I mean, you would have to have enormous amounts of money and you would have to have enormous
[00:58:31.760 --> 00:58:34.600]   access to data, which is being closed off more and more.
[00:58:34.600 --> 00:58:36.240]   That's the other interesting thing.
[00:58:36.240 --> 00:58:38.240]   Regulations could wind up backfiring.
[00:58:38.240 --> 00:58:41.440]   Yeah, because you need to scrape all the public data.
[00:58:41.440 --> 00:58:42.440]   Right.
[00:58:42.440 --> 00:58:46.240]   And what could start happening in Europe is this all of these bills, which are in the
[00:58:46.240 --> 00:58:50.880]   process of passing all's legislation, which was intended to help people could actually
[00:58:50.880 --> 00:58:53.120]   wind up having a reverse effect.
[00:58:53.120 --> 00:58:56.160]   You could actually decrease competition in the future because it's...
[00:58:56.160 --> 00:58:59.480]   Yeah, the Italian regulator blocked Chatshoot to B.E.E.P.s.
[00:58:59.480 --> 00:59:04.760]   for several weeks until they agreed to certain privacy restrictions.
[00:59:04.760 --> 00:59:07.600]   And you're saying that's depleting the...
[00:59:07.600 --> 00:59:12.760]   I'm saying if you spin this out, yeah, if you spin this out in the future, the sort of
[00:59:12.760 --> 00:59:17.400]   unintended consequences that they wind up protecting the company, the company is with
[00:59:17.400 --> 00:59:21.760]   the train models and make it harder for new competition.
[00:59:21.760 --> 00:59:22.960]   Regulatory capture.
[00:59:22.960 --> 00:59:25.280]   They set up a barrier to entry.
[00:59:25.280 --> 00:59:29.840]   But isn't it the case that we are seeing hundreds of companies every week spring up
[00:59:29.840 --> 00:59:32.360]   around these large language models?
[00:59:32.360 --> 00:59:34.440]   They're customers of them.
[00:59:34.440 --> 00:59:38.720]   They're all dependent on Chatshoot to B or Microsoft.
[00:59:38.720 --> 00:59:39.720]   Right.
[00:59:39.720 --> 00:59:42.520]   They're not buying the pick or the shovel.
[00:59:42.520 --> 00:59:47.360]   They are customers who are building applications on top of.
[00:59:47.360 --> 00:59:49.920]   So they're just completely dependent on these companies.
[00:59:49.920 --> 00:59:50.920]   Right.
[00:59:50.920 --> 00:59:55.360]   Again, the analogy to me that makes the most sense is media.
[00:59:55.360 --> 01:00:01.640]   So all of these media organizations just gave away the keys to the kingdom and didn't fundamentally
[01:00:01.640 --> 01:00:05.880]   change their business models so that they could get on to the platform.
[01:00:05.880 --> 01:00:08.960]   And again, I think that...
[01:00:08.960 --> 01:00:11.920]   And it was a brutal mistake.
[01:00:11.920 --> 01:00:16.360]   And I think that's going to happen with brands, with CPG, with...
[01:00:16.360 --> 01:00:21.360]   Yeah.
[01:00:21.360 --> 01:00:26.200]   Well, I'll tell you who's worried about Google using AI in its search results instead of its
[01:00:26.200 --> 01:00:28.840]   spider links.
[01:00:28.840 --> 01:00:30.040]   Neiman Lab has an article.
[01:00:30.040 --> 01:00:31.400]   Google is changing up search.
[01:00:31.400 --> 01:00:33.720]   What does that mean for news publishers?
[01:00:33.720 --> 01:00:38.400]   They're worried it'll decrease the traffic because people will read the summary, the
[01:00:38.400 --> 01:00:41.800]   AI summary and never get to the publisher's sites.
[01:00:41.800 --> 01:00:42.800]   Yeah.
[01:00:42.800 --> 01:00:43.960]   You know what?
[01:00:43.960 --> 01:00:46.760]   Like, let's go back 20 years.
[01:00:46.760 --> 01:00:48.280]   I'm done with news organizations.
[01:00:48.280 --> 01:00:50.880]   I have for the past two decades.
[01:00:50.880 --> 01:00:51.880]   And this is...
[01:00:51.880 --> 01:00:53.040]   I'm just finished.
[01:00:53.040 --> 01:00:54.880]   I'm done having a conversation.
[01:00:54.880 --> 01:00:57.600]   You don't believe they're whining, in other words.
[01:00:57.600 --> 01:01:03.320]   I don't believe they're serious about radically reinventing a business model that makes sense
[01:01:03.320 --> 01:01:04.760]   for the 21st century.
[01:01:04.760 --> 01:01:06.520]   So, look, I mean...
[01:01:06.520 --> 01:01:08.160]   I feel zero sympathy.
[01:01:08.160 --> 01:01:09.000]   And this is my background.
[01:01:09.000 --> 01:01:10.000]   I came out of journalism.
[01:01:10.000 --> 01:01:11.000]   Yeah.
[01:01:11.000 --> 01:01:12.560]   And by the way, this is what we do.
[01:01:12.560 --> 01:01:15.680]   I mean, Twit does not do any journalism.
[01:01:15.680 --> 01:01:22.600]   We merely comment on existing work done by other journalistic entities.
[01:01:22.600 --> 01:01:23.600]   So we are kind of a parasite.
[01:01:23.600 --> 01:01:25.920]   I think Jeff Jarvis and I pretty much disagree on everything.
[01:01:25.920 --> 01:01:29.560]   And I think this is one point that we actually agree on.
[01:01:29.560 --> 01:01:31.240]   I got to get you on with Jeff then.
[01:01:31.240 --> 01:01:32.240]   That sounds like fun.
[01:01:32.240 --> 01:01:33.240]   So you should not.
[01:01:33.240 --> 01:01:34.240]   You should...
[01:01:34.240 --> 01:01:35.240]   That will not be fun for anybody.
[01:01:35.240 --> 01:01:36.880]   Oh, that bad, huh?
[01:01:36.880 --> 01:01:44.920]   But he does agree that publishers are whining because Google has done nothing but drive traffic
[01:01:44.920 --> 01:01:45.920]   to them.
[01:01:45.920 --> 01:01:48.840]   Although, I have to say, this is from Neiman Labs.
[01:01:48.840 --> 01:01:56.000]   This is an example of a barred search where you would get all the information you needed
[01:01:56.000 --> 01:01:58.040]   from the AI-generated synopsis.
[01:01:58.040 --> 01:02:01.160]   But this is a snip-inch graph.
[01:02:01.160 --> 01:02:02.800]   This is a lot more than a snip-it though.
[01:02:02.800 --> 01:02:07.080]   This is enough information that you could stop right there.
[01:02:07.080 --> 01:02:08.080]   Great.
[01:02:08.080 --> 01:02:11.760]   And when they introduced the knowledge graph whatever, seven or eight years ago, they should
[01:02:11.760 --> 01:02:12.960]   have had the conversation then.
[01:02:12.960 --> 01:02:16.960]   And even before then, when they were indexing websites and nobody understood how they should
[01:02:16.960 --> 01:02:18.920]   have had that conversation then.
[01:02:18.920 --> 01:02:26.280]   There's nothing in the Bible or the Constitution or anywhere that guarantees a particular business
[01:02:26.280 --> 01:02:28.320]   model, the right to perpetual existence.
[01:02:28.320 --> 01:02:29.320]   True.
[01:02:29.320 --> 01:02:30.320]   There's nothing that says that like...
[01:02:30.320 --> 01:02:31.320]   Oh, yeah.
[01:02:31.320 --> 01:02:35.200]   If those original sources go away, what's chat GPT or Google's SGE?
[01:02:35.200 --> 01:02:39.680]   What's it going to use for the original sources?
[01:02:39.680 --> 01:02:40.680]   It's...
[01:02:40.680 --> 01:02:42.760]   I mean, that's a very significant problem.
[01:02:42.760 --> 01:02:45.280]   I'm not going to do its own inner problem reporting.
[01:02:45.280 --> 01:02:46.280]   But the...
[01:02:46.280 --> 01:02:48.280]   I think that's a very big problem.
[01:02:48.280 --> 01:02:49.280]   It's a very big question.
[01:02:49.280 --> 01:02:51.640]   But the way forward isn't like...
[01:02:51.640 --> 01:02:56.700]   Isn't in every individual company acting in their narrow short-term self-interest to
[01:02:56.700 --> 01:02:59.080]   avoid the work of reinventing what they do, right?
[01:02:59.080 --> 01:03:04.160]   It's everyone saying, "Oh, but this disrupts technology X, the disrupts the way that I've
[01:03:04.160 --> 01:03:08.360]   been doing things for the past Y years, therefore something's a problem."
[01:03:08.360 --> 01:03:12.360]   That is the nature of every technological motivation is it's going to disrupt a bunch
[01:03:12.360 --> 01:03:14.000]   of stuff.
[01:03:14.000 --> 01:03:18.640]   I'm much more concerned about the larger societal implications than I am with individual...
[01:03:18.640 --> 01:03:19.640]   Yeah.
[01:03:19.640 --> 01:03:24.760]   And as some have pointed out, in fact, in this Neiman article, it also gets rid of all
[01:03:24.760 --> 01:03:28.000]   the sites that are just clickbait.
[01:03:28.000 --> 01:03:29.880]   What time does a Super Bowl start?
[01:03:29.880 --> 01:03:30.880]   Those call...
[01:03:30.880 --> 01:03:36.320]   Those all go away because Google just answers it and then you can move on with your life.
[01:03:36.320 --> 01:03:38.520]   And the regulatory question is interesting.
[01:03:38.520 --> 01:03:39.520]   You know, there's...
[01:03:39.520 --> 01:03:44.040]   You guys know this like the big TikTok meme of the Pelican trying to eat a copibara.
[01:03:44.040 --> 01:03:47.200]   It's just like can't quite get its beak around it at all.
[01:03:47.200 --> 01:03:49.560]   I missed that one, but I get the idea.
[01:03:49.560 --> 01:03:50.560]   Yes.
[01:03:50.560 --> 01:03:54.640]   And you just go for Pelican and Capibari and probably find it.
[01:03:54.640 --> 01:03:59.600]   That's what a lot of this stuff reminds me of, of regulators actually trying to get their
[01:03:59.600 --> 01:04:02.040]   beaks around what to do with this thing.
[01:04:02.040 --> 01:04:03.040]   And they just kind of can't...
[01:04:03.040 --> 01:04:04.680]   They can't deal with it.
[01:04:04.680 --> 01:04:06.200]   And it's a really deep and interesting problem.
[01:04:06.200 --> 01:04:09.440]   I would say as important as exactly.
[01:04:09.440 --> 01:04:12.040]   You've just enriched my life significantly, Phil.
[01:04:12.040 --> 01:04:13.040]   Thank you.
[01:04:13.040 --> 01:04:16.000]   You're welcome.
[01:04:16.000 --> 01:04:17.800]   So what are we going to do?
[01:04:17.800 --> 01:04:20.960]   Maybe we can just take a moment to appreciate this gift.
[01:04:20.960 --> 01:04:27.280]   If you're not watching the video, just imagine a Pelican trying to eat a rather large rodent-like
[01:04:27.280 --> 01:04:30.320]   mammal and not being able to.
[01:04:30.320 --> 01:04:37.600]   And I would love to like engage in this again in good faith because it's easy to like make
[01:04:37.600 --> 01:04:38.600]   fun of regulators.
[01:04:38.600 --> 01:04:43.040]   But like, yeah, to the extent that we need like governments to do something, it's probably
[01:04:43.040 --> 01:04:46.240]   about this like potentially society ending, you know, thing.
[01:04:46.240 --> 01:04:47.640]   I'm not sure what to do.
[01:04:47.640 --> 01:04:53.880]   I feel you, Phil, but I don't know, six years now, multiple meetings at state at DOD with
[01:04:53.880 --> 01:04:54.880]   you.
[01:04:54.880 --> 01:04:55.880]   You know these guys.
[01:04:55.880 --> 01:04:56.880]   You've met with them.
[01:04:56.880 --> 01:04:57.880]   Yeah.
[01:04:57.880 --> 01:05:01.480]   Here, the issue is we cannot get our together in the United States.
[01:05:01.480 --> 01:05:10.160]   The EU's gone a direction that is about regulating, I think to some degree without thinking through
[01:05:10.160 --> 01:05:11.160]   the knock-on effects.
[01:05:11.160 --> 01:05:12.160]   That's pretty clear.
[01:05:12.160 --> 01:05:17.600]   If you go to Europe and try to surf the web, every page is covered with a cookie bin.
[01:05:17.600 --> 01:05:22.680]   Or that you have to click through to no purpose whatsoever.
[01:05:22.680 --> 01:05:31.480]   We lack a long-term vision in this country and we lack good long-term leadership.
[01:05:31.480 --> 01:05:32.480]   That's the issue.
[01:05:32.480 --> 01:05:39.520]   There was a Biden administration, just as an example, issued an executive order on biotech
[01:05:39.520 --> 01:05:41.400]   that literally took.
[01:05:41.400 --> 01:05:44.560]   It was not his original idea that order.
[01:05:44.560 --> 01:05:47.280]   It was for general.
[01:05:47.280 --> 01:05:49.880]   It started with the Obama administration.
[01:05:49.880 --> 01:05:51.360]   He couldn't get it done.
[01:05:51.360 --> 01:05:52.960]   Trump couldn't get it done.
[01:05:52.960 --> 01:05:53.960]   Then it was Biden.
[01:05:53.960 --> 01:05:58.400]   Really, at the end of the day, it doesn't have teeth.
[01:05:58.400 --> 01:06:02.320]   We just don't have one of the things that I've been recommending and trying to stand
[01:06:02.320 --> 01:06:03.320]   up.
[01:06:03.320 --> 01:06:10.040]   So, like 10 years ago, I wrote this whole thing and recommended a sort of office cabinet level
[01:06:10.040 --> 01:06:12.320]   sort of office of the future.
[01:06:12.320 --> 01:06:15.680]   We did have something called the Office of Technology Assessment that got defunded in
[01:06:15.680 --> 01:06:16.680]   the '90s.
[01:06:16.680 --> 01:06:21.440]   And short of that, we just don't have any -- nobody's in charge of long-term vision on
[01:06:21.440 --> 01:06:22.440]   science and tech.
[01:06:22.440 --> 01:06:27.640]   I know everybody's got different groups that they will note, but we need something that's
[01:06:27.640 --> 01:06:30.880]   horizontal that touches all the other agencies.
[01:06:30.880 --> 01:06:32.520]   We don't have that.
[01:06:32.520 --> 01:06:38.280]   We've got a whiplash situation in our government.
[01:06:38.280 --> 01:06:41.160]   And I think we've got people who are going to become apathetic because we've got the
[01:06:41.160 --> 01:06:42.160]   same -- it's like a rerun.
[01:06:42.160 --> 01:06:46.040]   We've got the same people running again for this next go around.
[01:06:46.040 --> 01:06:50.920]   I'm actually, to be honest, more worried about the end of the world with climate change
[01:06:50.920 --> 01:06:54.600]   than I am about whether we're going to -- it's really going to be a race between whether
[01:06:54.600 --> 01:06:59.560]   AI takes over or we all get boiled to death in a giant sea of --
[01:06:59.560 --> 01:07:07.320]   Again, this comes back to -- there's something like -- I will no longer bring up Bitcoin
[01:07:07.320 --> 01:07:12.160]   around friends because it's a little bit like giving them parenting advice.
[01:07:12.160 --> 01:07:17.720]   I can't talk about religion or parenting or like Bitcoin because there's this religious
[01:07:17.720 --> 01:07:19.560]   insanity that takes over with people.
[01:07:19.560 --> 01:07:21.680]   Those people have moved on to AI, I thought.
[01:07:21.680 --> 01:07:23.600]   Well, that's exactly what I was going to say.
[01:07:23.600 --> 01:07:26.680]   So now this AI conversation is completely polarized.
[01:07:26.680 --> 01:07:27.680]   Yeah.
[01:07:27.680 --> 01:07:28.680]   Yeah.
[01:07:28.680 --> 01:07:30.240]   And it's just -- I'm just exhausted.
[01:07:30.240 --> 01:07:35.600]   Well, one of our editors has just pointed out that thanks to AI, we can bleep all the
[01:07:35.600 --> 01:07:36.840]   profanities in this year.
[01:07:36.840 --> 01:07:37.840]   Sorry, I will stop.
[01:07:37.840 --> 01:07:39.800]   I'm like, I'm like not having it today.
[01:07:39.800 --> 01:07:40.800]   I'm so sorry.
[01:07:40.800 --> 01:07:41.800]   We have AI.
[01:07:41.800 --> 01:07:43.960]   Please, swear your little heart out.
[01:07:43.960 --> 01:07:44.960]   I'm sorry, I will stop.
[01:07:44.960 --> 01:07:47.120]   No, it will be automatically bleeped.
[01:07:47.120 --> 01:07:49.280]   It's amazing this technology.
[01:07:49.280 --> 01:07:51.360]   Just bleep, bleep, bleep everywhere.
[01:07:51.360 --> 01:07:54.240]   No, this is such a good conversation.
[01:07:54.240 --> 01:07:58.320]   And frankly, this is the first -- we've been talking about this for months.
[01:07:58.320 --> 01:08:02.560]   It's the first time I really feel like I finally have two's.
[01:08:02.560 --> 01:08:06.280]   Nothing wrong with all of our other posts, but two really smart people have thought about
[01:08:06.280 --> 01:08:09.520]   this is like you've been thinking about this for almost a decade.
[01:08:09.520 --> 01:08:13.280]   Who are kind of insightful into all of this?
[01:08:13.280 --> 01:08:19.440]   It doesn't give me great hope, but also it doesn't -- I'm not worried anymore either,
[01:08:19.440 --> 01:08:20.600]   right?
[01:08:20.600 --> 01:08:23.480]   Because these LLMs aren't going to eat us alive.
[01:08:23.480 --> 01:08:25.680]   Look, I'm optimistic.
[01:08:25.680 --> 01:08:26.880]   I'm just annoyed.
[01:08:26.880 --> 01:08:27.880]   Yes.
[01:08:27.880 --> 01:08:31.880]   I'm like -- In a nutshell, I think that's Amy as well.
[01:08:31.880 --> 01:08:32.880]   Optimistic but annoyed.
[01:08:32.880 --> 01:08:36.240]   Just unless eloquent and fillable.
[01:08:36.240 --> 01:08:38.040]   He's a word, Smith.
[01:08:38.040 --> 01:08:40.640]   You can tell.
[01:08:40.640 --> 01:08:42.120]   So optimistic.
[01:08:42.120 --> 01:08:48.040]   So I'd like to hear from both of you what you think, you know, obviously regulatory
[01:08:48.040 --> 01:08:52.400]   issues and all this other stuff, there's a lot of chaff in the air.
[01:08:52.400 --> 01:08:58.280]   But what do you think in the best case scenario, Amy, this could look like in five or ten
[01:08:58.280 --> 01:08:59.280]   years?
[01:08:59.280 --> 01:09:01.160]   Where are we kind of headed?
[01:09:01.160 --> 01:09:02.160]   I love them.
[01:09:02.160 --> 01:09:04.320]   I love the medicine story.
[01:09:04.320 --> 01:09:06.480]   That's remarkable.
[01:09:06.480 --> 01:09:13.760]   So what I would -- let's put aside geopolitics and the real possibility of yet another A,
[01:09:13.760 --> 01:09:18.240]   a winter because I think a lot of outsized expectations are never going to get fulfilled.
[01:09:18.240 --> 01:09:22.120]   So like let's put that stuff as in regulators talking out of their, you know, whatever.
[01:09:22.120 --> 01:09:24.280]   Let's put that aside.
[01:09:24.280 --> 01:09:30.240]   There are incredible opportunities on the horizon.
[01:09:30.240 --> 01:09:35.960]   So this, this like thing that I mentioned with this company, the company is called Absai.
[01:09:35.960 --> 01:09:36.960]   They're doing really --
[01:09:36.960 --> 01:09:37.960]   So cool.
[01:09:37.960 --> 01:09:38.960]   They're doing really cool stuff, right?
[01:09:38.960 --> 01:09:39.960]   Yes, so cool.
[01:09:39.960 --> 01:09:45.880]   And the possibility of getting to a point where we can shrink what might have been ten
[01:09:45.880 --> 01:09:51.920]   years, it might have taken ten years down to let's say 18 months to get viable candidates
[01:09:51.920 --> 01:09:52.920]   for things.
[01:09:52.920 --> 01:09:54.720]   Well, we saw that with mRNA, right?
[01:09:54.720 --> 01:09:55.720]   I mean, that's --
[01:09:55.720 --> 01:09:56.720]   Sure.
[01:09:56.720 --> 01:09:57.720]   Well --
[01:09:57.720 --> 01:09:58.720]   We got the COVID vaccine so fast.
[01:09:58.720 --> 01:09:59.720]   Exactly.
[01:09:59.720 --> 01:10:06.720]   So the possibility of medication being one size for me, custom for me versus one size fit
[01:10:06.720 --> 01:10:09.680]   all fits all is a real possibility.
[01:10:09.680 --> 01:10:14.560]   This is an issue that's very close to my heart because my mother died, I think, needlessly
[01:10:14.560 --> 01:10:20.360]   of a rare cancer that we just like not a lot of people had it.
[01:10:20.360 --> 01:10:22.600]   So there wasn't a lot of research.
[01:10:22.600 --> 01:10:27.880]   My dad's got Parkinson's and dementia, you know, and the way that we deal with that
[01:10:27.880 --> 01:10:32.440]   in the United States is -- or everywhere is kind of like, let's try to make this situation
[01:10:32.440 --> 01:10:34.360]   as comfortable as possible.
[01:10:34.360 --> 01:10:35.360]   Right.
[01:10:35.360 --> 01:10:42.720]   I think this -- so the thing that I find promising about AI is that it is part of a
[01:10:42.720 --> 01:10:46.400]   Venn diagram that includes biology.
[01:10:46.400 --> 01:10:54.640]   And there's a lot of really awesome possibilities that we then have to think through to make
[01:10:54.640 --> 01:10:59.520]   equitable and to make sure there's not negative knock-on effects like I mentioned medical
[01:10:59.520 --> 01:11:02.120]   deepfakes at the beginning before we started out so what is that?
[01:11:02.120 --> 01:11:03.520]   Yeah, tell me about that.
[01:11:03.520 --> 01:11:04.520]   So, right.
[01:11:04.520 --> 01:11:07.840]   So somebody has already figured out a man in the middle attack -- well, sorry, there's
[01:11:07.840 --> 01:11:08.840]   a couple of things.
[01:11:08.840 --> 01:11:10.120]   Somebody -- not that one.
[01:11:10.120 --> 01:11:18.320]   Somebody's already figured out a way to -- they've demonstrated how to infiltrate a database
[01:11:18.320 --> 01:11:24.160]   that houses scans and to put what looks like a tumor in where there wasn't the four.
[01:11:24.160 --> 01:11:25.160]   Oh, my God.
[01:11:25.160 --> 01:11:29.040]   Or to take one away, which would potentially mean that somebody goes in for chemo that
[01:11:29.040 --> 01:11:31.680]   didn't need it or doesn't, who did.
[01:11:31.680 --> 01:11:37.080]   I was with a bunch of -- it was US -- it was Brazil Week in New York last week.
[01:11:37.080 --> 01:11:42.720]   So there's all the big Brazilian business people and politicians and stuff in the city.
[01:11:42.720 --> 01:11:47.760]   And I was talking to a bunch of them, and I said, you know, and I just kind of thought
[01:11:47.760 --> 01:11:51.840]   of it at the moment, but I was like, what if Lula, the president, there is his name is
[01:11:51.840 --> 01:11:52.840]   Lula?
[01:11:52.840 --> 01:11:59.560]   If Lula, you know, you could destabilize the Brazilian government by deep faking.
[01:11:59.560 --> 01:12:03.400]   You know, every president has an exam once a year.
[01:12:03.400 --> 01:12:04.920]   Every CEO does.
[01:12:04.920 --> 01:12:06.400]   You could destabilize a government.
[01:12:06.400 --> 01:12:11.680]   You could destabilize a company by making it appear as though somebody is sick, who's
[01:12:11.680 --> 01:12:12.680]   not.
[01:12:12.680 --> 01:12:14.160]   And then they go in for treatment.
[01:12:14.160 --> 01:12:18.360]   I mean, you would hope that they have second opinions and whatever else.
[01:12:18.360 --> 01:12:25.080]   But it presents potentially novel security threats that we hadn't thought through before,
[01:12:25.080 --> 01:12:26.080]   which we can manage.
[01:12:26.080 --> 01:12:28.760]   We just have to plan in advance.
[01:12:28.760 --> 01:12:37.720]   Well, I think Amy understands how unlikely it is that we won't do any of that, right?
[01:12:37.720 --> 01:12:42.280]   You know, this is -- our government is a random walk.
[01:12:42.280 --> 01:12:45.680]   And we just bounce off of like posts and --
[01:12:45.680 --> 01:12:52.280]   Look, I think you get like slightly technical.
[01:12:52.280 --> 01:13:00.920]   I think -- and this is stuff that, you know, I think Amy understands better than I do.
[01:13:00.920 --> 01:13:06.160]   But I break things up into a few different categories in terms of AI.
[01:13:06.160 --> 01:13:14.440]   There is a general computational ability that machine learning really help with, but even
[01:13:14.440 --> 01:13:21.240]   ML, even if something is broad as ML is just like a type of, you know, a branch of algorithms.
[01:13:21.240 --> 01:13:23.760]   There's been massive progress on that.
[01:13:23.760 --> 01:13:28.280]   I'm a very early investor in a company called Occam's Razor, for example, which also does
[01:13:28.280 --> 01:13:33.280]   novel therapeutics discovery for Parkinson's and your degenerative diseases by, you know,
[01:13:33.280 --> 01:13:36.800]   reading everything ever published and, you know, suggesting compounds and doing like
[01:13:36.800 --> 01:13:38.280]   really, really cool stuff.
[01:13:38.280 --> 01:13:41.960]   There's massive potential for that.
[01:13:41.960 --> 01:13:45.520]   If you want to call that AI, sure, at some point, right, it's just math.
[01:13:45.520 --> 01:13:49.880]   So I guess we are getting much better at math to, you know, to figure this out.
[01:13:49.880 --> 01:13:53.240]   Well, and our computational models are getting bigger and the data sets are getting bigger
[01:13:53.240 --> 01:13:55.320]   and the speeds are getting faster.
[01:13:55.320 --> 01:13:59.920]   So we can do more than we've ever done before, whether it's AI or math.
[01:13:59.920 --> 01:14:00.920]   Absolutely.
[01:14:00.920 --> 01:14:07.280]   MLMs, and in particular these like generators, right, what the G stands for in JET, GPT,
[01:14:07.280 --> 01:14:13.200]   the T, the transformers are very interesting sub branch of algorithms that can really help
[01:14:13.200 --> 01:14:16.920]   us with a bunch of stuff, including how to like process language.
[01:14:16.920 --> 01:14:22.040]   So a big, a big leap forward that is happening right now is our ability to process huge amounts
[01:14:22.040 --> 01:14:23.800]   of natural language at scale.
[01:14:23.800 --> 01:14:26.520]   So we could take things that were said or written by people.
[01:14:26.520 --> 01:14:27.520]   We can analyze them.
[01:14:27.520 --> 01:14:28.520]   We can data mine them.
[01:14:28.520 --> 01:14:29.520]   We can find all sorts of things.
[01:14:29.520 --> 01:14:35.640]   We combine that with our improved computation models that can have all sorts of positive
[01:14:35.640 --> 01:14:42.880]   results in healthcare, in biology, and all sorts of stuff.
[01:14:42.880 --> 01:14:44.720]   None of that is what's really getting most of the attention.
[01:14:44.720 --> 01:14:48.280]   What's really getting most of the attention is in many ways the least interesting segment
[01:14:48.280 --> 01:14:53.220]   of all this, which is just using it to generate language, using it to like pump out speech
[01:14:53.220 --> 01:14:58.200]   or text or art, just like, you know, blather.
[01:14:58.200 --> 01:15:00.240]   And that stuff, yeah, like that's the sexiest part.
[01:15:00.240 --> 01:15:03.360]   That's like the most impressive part, if you're really not thinking about it very deeply,
[01:15:03.360 --> 01:15:07.320]   because it turns out these systems can generate very glibly lots of language.
[01:15:07.320 --> 01:15:13.880]   And that's like that least impressive part of AI is the thing that's getting 95% of the
[01:15:13.880 --> 01:15:15.800]   hype and attention over the past six months.
[01:15:15.800 --> 01:15:16.800]   And that's the thing.
[01:15:16.800 --> 01:15:21.640]   That's why I say like I'm, I am optimistic, big picture, but I am annoyed as well because
[01:15:21.640 --> 01:15:24.800]   I actually think that like this stuff is not the interesting part.
[01:15:24.800 --> 01:15:26.800]   But it's a good way to turn that into a significant.
[01:15:26.800 --> 01:15:27.800]   That's what happens.
[01:15:27.800 --> 01:15:34.920]   You know, that's what it takes to get a populace that really doesn't understand what's going
[01:15:34.920 --> 01:15:37.720]   on to kind of get excited about it.
[01:15:37.720 --> 01:15:43.360]   I am concerned about your notion that the infrastructure we're developing is not necessarily useful
[01:15:43.360 --> 01:15:44.920]   or germane.
[01:15:44.920 --> 01:15:45.920]   It's worse.
[01:15:45.920 --> 01:15:46.920]   It's worse than that, right?
[01:15:46.920 --> 01:15:51.200]   It's because like the ability, like the, it's hard, it's hard to see the, the, I think
[01:15:51.200 --> 01:15:54.600]   it's going to take a while for us to see the really giant benefits of this.
[01:15:54.600 --> 01:15:58.440]   But I think the, I think the problems, I think the cost we pay immediately, I think like
[01:15:58.440 --> 01:16:01.960]   all of the ways that this is going to break things that is happening now.
[01:16:01.960 --> 01:16:02.960]   Societally.
[01:16:02.960 --> 01:16:03.960]   Right.
[01:16:03.960 --> 01:16:04.960]   Yeah.
[01:16:04.960 --> 01:16:07.960]   Like basically we are looking at this is going to be the year of a tsunami.
[01:16:07.960 --> 01:16:08.960]   It's going to be messy.
[01:16:08.960 --> 01:16:09.960]   Yeah.
[01:16:09.960 --> 01:16:10.960]   Here's what I think.
[01:16:10.960 --> 01:16:11.960]   You're looking at a tsunami of crap right now.
[01:16:11.960 --> 01:16:12.960]   Yeah.
[01:16:12.960 --> 01:16:13.960]   Yeah.
[01:16:13.960 --> 01:16:14.960]   Speaking of tsunami of crap.
[01:16:14.960 --> 01:16:19.440]   So Goldman, I think, put out a report that, I don't know, attempted to list or quantify
[01:16:19.440 --> 01:16:23.200]   the number of jobs that will go away in X amount of time.
[01:16:23.200 --> 01:16:25.080]   Lots of people are doing that right now.
[01:16:25.080 --> 01:16:27.560]   So again, here's what I would do a good job of that report.
[01:16:27.560 --> 01:16:29.920]   These are all the jobs though, right?
[01:16:29.920 --> 01:16:31.640]   Well, but especially right.
[01:16:31.640 --> 01:16:32.640]   Yes and no.
[01:16:32.640 --> 01:16:35.560]   But here's what I think is, here's what I think is plausible.
[01:16:35.560 --> 01:16:41.320]   I think that there are, if it is the case that we are maybe not looking at a recession,
[01:16:41.320 --> 01:16:47.880]   but certainly not looking at a period of strong growth economically and we're coming off of
[01:16:47.880 --> 01:16:48.880]   a wave.
[01:16:48.880 --> 01:16:55.320]   So there's a hundred, 200,000 tech workers displaced out of jobs.
[01:16:55.320 --> 01:17:01.040]   I wonder if, because I know, because I've had conversations with the heads of companies,
[01:17:01.040 --> 01:17:06.600]   one of the questions they're asking us is to build a model showing them which jobs can
[01:17:06.600 --> 01:17:08.640]   go away over the next three to five years.
[01:17:08.640 --> 01:17:10.480]   My answer is we're not going to do that for you.
[01:17:10.480 --> 01:17:14.320]   But I'm concerned that it's making an engine to seed.
[01:17:14.320 --> 01:17:17.120]   Is that not possible?
[01:17:17.120 --> 01:17:21.840]   Because I don't think AI is going to replace all these jobs over the next three to five
[01:17:21.840 --> 01:17:22.840]   years.
[01:17:22.840 --> 01:17:24.840]   I'll give you an example.
[01:17:24.840 --> 01:17:27.360]   So I'm constantly experimenting, right?
[01:17:27.360 --> 01:17:33.160]   So I pulled a P&L from a publicly available hospital and I dumped it into chat GPT and
[01:17:33.160 --> 01:17:35.360]   I used four.
[01:17:35.360 --> 01:17:43.840]   And I asked it to look for an 8% reduction in overhead without challenging whatever patient
[01:17:43.840 --> 01:17:46.600]   outcomes or whatever else.
[01:17:46.600 --> 01:17:52.960]   So it did that, and it did it relatively quickly, but there was nothing in there that was useful.
[01:17:52.960 --> 01:17:58.320]   So it was just sort of like a super generic, if it had been an MBA student in my class,
[01:17:58.320 --> 01:18:02.240]   I would have given them like a D for not coming up with anything tangible.
[01:18:02.240 --> 01:18:05.320]   But if it was a McKinsey consultant, you would have given them a million dollars.
[01:18:05.320 --> 01:18:06.320]   Absolutely.
[01:18:06.320 --> 01:18:09.560]   And chat GPT did it for less.
[01:18:09.560 --> 01:18:10.560]   But here's my concern.
[01:18:10.560 --> 01:18:12.560]   Well, let's go back to McKinsey.
[01:18:12.560 --> 01:18:15.880]   If you guys want to read a great book, it's when McKinsey came to town.
[01:18:15.880 --> 01:18:17.320]   It's very interesting.
[01:18:17.320 --> 01:18:22.440]   But a lot of big box consultants will come in because the problem that they're solving
[01:18:22.440 --> 01:18:25.120]   for is how do we improve our margins?
[01:18:25.120 --> 01:18:29.960]   And the easiest way to, in the very short term, improve a margin is to get rid of headcount.
[01:18:29.960 --> 01:18:35.600]   So what I think could be happening is companies are going to see this sort of permission to
[01:18:35.600 --> 01:18:40.960]   get rid of jobs earlier because AI, well, I'm air quoting again, like AI will do it
[01:18:40.960 --> 01:18:42.120]   better than a human.
[01:18:42.120 --> 01:18:48.000]   AI may be able to imitate a human, but we're not at the point where the insights are there
[01:18:48.000 --> 01:18:49.000]   yet.
[01:18:49.000 --> 01:18:53.880]   So I just, I think that a bunch of companies are going to let go a whole bunch of people
[01:18:53.880 --> 01:18:59.260]   because they think that this is cheaper, better, faster and realize that like their
[01:18:59.260 --> 01:19:01.760]   copper barad.
[01:19:01.760 --> 01:19:06.640]   Should I worry that the guy who is a direct report to Sundar Pichai and is now in charge
[01:19:06.640 --> 01:19:14.560]   of AI ethics at Google was a longtime McKinsey consultant and became partner at McKinsey.
[01:19:14.560 --> 01:19:16.560]   Is that, should I worry?
[01:19:16.560 --> 01:19:17.760]   Should I be scared?
[01:19:17.760 --> 01:19:20.440]   I am not bashing McKinsey.
[01:19:20.440 --> 01:19:22.200]   That is not what's happening here.
[01:19:22.200 --> 01:19:29.440]   But I am now hearing enough of the same conversations and enough of the tops of organizations.
[01:19:29.440 --> 01:19:35.160]   They're looking at these reports as like a green light to reduce headcount.
[01:19:35.160 --> 01:19:36.160]   Yeah.
[01:19:36.160 --> 01:19:39.880]   Well, they'll take whatever they, they'll take any use, right?
[01:19:39.880 --> 01:19:41.960]   That's just an excuse.
[01:19:41.960 --> 01:19:42.960]   That's not.
[01:19:42.960 --> 01:19:46.040]   I think there's, I think there's, there's no such thing as AI ethics, right?
[01:19:46.040 --> 01:19:48.400]   There's only, there's only ethics.
[01:19:48.400 --> 01:19:49.400]   Okay.
[01:19:49.400 --> 01:19:50.400]   Excellent.
[01:19:50.400 --> 01:19:51.400]   Point while taking.
[01:19:51.400 --> 01:19:58.120]   And I, but I think like when we have like, like, when we have a sharp point like this
[01:19:58.120 --> 01:20:02.120]   where things may be changing, like it's a very good reminder that like we haven't necessarily
[01:20:02.120 --> 01:20:05.480]   decided what ethics, what our ethics should be anyway.
[01:20:05.480 --> 01:20:07.640]   And like that would be a healthy thing to take away from all this.
[01:20:07.640 --> 01:20:10.000]   So maybe what do we want as a society?
[01:20:10.000 --> 01:20:12.040]   What are we, like what are the outcomes that we want?
[01:20:12.040 --> 01:20:16.520]   And maybe that's the step of how the regulators can try to do to, to be productive about this
[01:20:16.520 --> 01:20:20.520]   isn't so much figuring out like what to ban and what to restrict, but we can have a discussion
[01:20:20.520 --> 01:20:22.520]   about what are we, what are the outcomes that we want?
[01:20:22.520 --> 01:20:24.920]   Describe the world that we'd like to live in.
[01:20:24.920 --> 01:20:28.400]   That's a good point because we're focused on, that's a great point because we're focused
[01:20:28.400 --> 01:20:31.160]   on outputs, quantity, right?
[01:20:31.160 --> 01:20:33.760]   But outputs, we are not focused on outcomes.
[01:20:33.760 --> 01:20:38.040]   And I think that that tends to happen when business and tech come together.
[01:20:38.040 --> 01:20:39.040]   Yeah.
[01:20:39.040 --> 01:20:42.840]   Like I'd propose, for example, a thing that, that I, that I think could be very dangerous
[01:20:42.840 --> 01:20:48.200]   if we ever slip away from this is if we ever get to a point where no human understands
[01:20:48.200 --> 01:20:53.160]   deeply at a very deep level, how important stuff works.
[01:20:53.160 --> 01:20:56.160]   We've long since gotten to the point where most people don't understand how most things
[01:20:56.160 --> 01:20:57.160]   work.
[01:20:57.160 --> 01:20:58.160]   And that's fine, right?
[01:20:58.160 --> 01:21:02.200]   I really know, like I couldn't tell you exactly how the TV that's, you know, on my wall here
[01:21:02.200 --> 01:21:03.200]   works exactly.
[01:21:03.200 --> 01:21:06.240]   I kind of know a little bit, but I couldn't really describe it to you.
[01:21:06.240 --> 01:21:10.800]   But the people who made the TV could tell you exactly how the thing works.
[01:21:10.800 --> 01:21:15.240]   You know, when I write code, I haven't actually written any code in probably a decade, but
[01:21:15.240 --> 01:21:17.160]   you know, I used to be a programmer.
[01:21:17.160 --> 01:21:21.400]   I couldn't really tell you like how the code that I write and, you know, Java like gets
[01:21:21.400 --> 01:21:25.680]   translated into, into machine code, into ones and zeros.
[01:21:25.680 --> 01:21:29.000]   But I know people who absolutely can exactly.
[01:21:29.000 --> 01:21:32.520]   And I know how I could if I really wanted to put in the work.
[01:21:32.520 --> 01:21:37.640]   I think it's a very important kind of societal norm that we never humans, we never give up
[01:21:37.640 --> 01:21:42.520]   like actually knowing how something works at a very deep level.
[01:21:42.520 --> 01:21:46.520]   And that is, that's definitely something that I feel like we're in the danger of slip away.
[01:21:46.520 --> 01:21:51.200]   Jerry Parnell warned about that in his novel Lucifer's hammer that we would, that we would
[01:21:51.200 --> 01:21:54.520]   get to a state where we didn't understand how our technology worked.
[01:21:54.520 --> 01:21:59.120]   And this is like, yeah, and this is like the, you know, you see this in the, you know, prompt
[01:21:59.120 --> 01:22:01.120]   engineering, you know, phony jobs.
[01:22:01.120 --> 01:22:02.280]   Yeah, you don't need to code.
[01:22:02.280 --> 01:22:03.600]   Just prompt engineer.
[01:22:03.600 --> 01:22:04.600]   Yeah.
[01:22:04.600 --> 01:22:07.760]   I saw a job posting for $300,000 for a prompt engineer.
[01:22:07.760 --> 01:22:08.760]   Yeah.
[01:22:08.760 --> 01:22:10.480]   And this is like, this is waving a dead chicken around.
[01:22:10.480 --> 01:22:12.320]   This is literally superstition, right?
[01:22:12.320 --> 01:22:17.240]   We're literally going to have a class of people who wave chicken who wave dead chickens and
[01:22:17.240 --> 01:22:18.240]   like incantation.
[01:22:18.240 --> 01:22:20.280]   That's good money while you can get it.
[01:22:20.280 --> 01:22:21.280]   Right.
[01:22:21.280 --> 01:22:27.280]   But like, but we don't want, we don't want to make like a cult in this a cultish religion
[01:22:27.280 --> 01:22:30.040]   around our technology around technology would be a bad way.
[01:22:30.040 --> 01:22:31.280]   We don't understand it.
[01:22:31.280 --> 01:22:33.120]   And I think we may be heading there.
[01:22:33.120 --> 01:22:34.120]   We don't have to.
[01:22:34.120 --> 01:22:35.120]   We don't have to get there.
[01:22:35.120 --> 01:22:37.080]   But I think that was that's, we're definitely close.
[01:22:37.080 --> 01:22:42.360]   That's like a very good thing to strive for and probably a very good thing to try to enforce
[01:22:42.360 --> 01:22:48.520]   in, in, in regulations is like somebody needs to understand this at a very deep level.
[01:22:48.520 --> 01:22:54.720]   And, you know, there are people like Jeffrey Hinton and Blake Lemoine who are kind of implying
[01:22:54.720 --> 01:22:59.360]   that it's a black box and you can't understand what's going on inside.
[01:22:59.360 --> 01:23:00.360]   And I think you're right.
[01:23:00.360 --> 01:23:01.760]   And therefore we shouldn't use it then for serious things.
[01:23:01.760 --> 01:23:02.760]   Yeah.
[01:23:02.760 --> 01:23:05.200]   That's not true of every type of thing.
[01:23:05.200 --> 01:23:09.960]   That's only true of like LLMs because of, again, because of how they're, they're created.
[01:23:09.960 --> 01:23:13.920]   And so yes, that to me is an argument for, well, let's not, let's not like before we
[01:23:13.920 --> 01:23:17.760]   wire up the whole world to be run by a thing that we can't fundamentally understand.
[01:23:17.760 --> 01:23:18.760]   Let's not do that.
[01:23:18.760 --> 01:23:20.880]   Yeah, that's a good point.
[01:23:20.880 --> 01:23:25.760]   Let's take a little break just because I have to, but also because you probably need a drink
[01:23:25.760 --> 01:23:27.440]   of water or something.
[01:23:27.440 --> 01:23:28.800]   What a panel.
[01:23:28.800 --> 01:23:29.800]   I was smart.
[01:23:29.800 --> 01:23:35.240]   Jason Hals and I both decided if you've got Phil Libben and Amy Webb on one show, you
[01:23:35.240 --> 01:23:36.600]   don't need anybody else.
[01:23:36.600 --> 01:23:39.000]   I should just step back and let you guys talk.
[01:23:39.000 --> 01:23:41.720]   Phil is at all-turtles.com.
[01:23:41.720 --> 01:23:44.320]   You're turning into a podcast network here.
[01:23:44.320 --> 01:23:46.360]   Look at all the shows you guys are doing.
[01:23:46.360 --> 01:23:47.360]   That's great.
[01:23:47.360 --> 01:23:52.160]   CultureFit talks about racial bias and tech.
[01:23:52.160 --> 01:23:57.160]   Alternals is, I use the word incubator, but you, you like, you prefer studio, software
[01:23:57.160 --> 01:23:58.680]   design studio, I guess.
[01:23:58.680 --> 01:23:59.680]   Yeah.
[01:23:59.680 --> 01:24:00.680]   Yeah.
[01:24:00.680 --> 01:24:05.080]   Lots of portfolio companies, including one that Phil's using right now.
[01:24:05.080 --> 01:24:10.600]   He, believe it or not, is not sitting in front of a beautiful rainforest.
[01:24:10.600 --> 01:24:14.000]   He's probably in some grim, dungy basement somewhere, but mmm.
[01:24:14.000 --> 01:24:15.000]   I'm actually here.
[01:24:15.000 --> 01:24:16.000]   Oh, okay.
[01:24:16.000 --> 01:24:17.000]   In Bentonville.
[01:24:17.000 --> 01:24:18.280]   North, Northwest Arkansas.
[01:24:18.280 --> 01:24:22.080]   Where life works here.
[01:24:22.080 --> 01:24:25.440]   The first Waltons right there, the first Walton's Darkstore.
[01:24:25.440 --> 01:24:27.960]   That's why everything Walmart does.
[01:24:27.960 --> 01:24:31.640]   They're perfect and I can't criticize because I'm literally looking out the window at the
[01:24:31.640 --> 01:24:33.800]  - Are you really in Bentonville?
[01:24:33.800 --> 01:24:34.800]   Seriously?
[01:24:34.800 --> 01:24:35.800]   Yeah.
[01:24:35.800 --> 01:24:36.800]   Like, like that's where you live now?
[01:24:36.800 --> 01:24:37.800]   That's amazing.
[01:24:37.800 --> 01:24:38.800]   That's where I live.
[01:24:38.800 --> 01:24:39.800]   Really?
[01:24:39.800 --> 01:24:40.800]   I didn't know that.
[01:24:40.800 --> 01:24:41.800]   Two blocks from here.
[01:24:41.800 --> 01:24:42.800]   Yeah.
[01:24:42.800 --> 01:24:43.800]   I thought you were in the Bay Area.
[01:24:43.800 --> 01:24:44.800]   It's great.
[01:24:44.800 --> 01:24:45.800]   Nope.
[01:24:45.800 --> 01:24:46.800]   I'm here at Bentonville.
[01:24:46.800 --> 01:24:47.800]   Oh, yeah.
[01:24:47.800 --> 01:24:48.800]   The Crystal Bridges I was there today.
[01:24:48.800 --> 01:24:49.800]   Yeah, yeah.
[01:24:49.800 --> 01:24:51.080]   I've always, I've wanted to go.
[01:24:51.080 --> 01:24:52.080]   I've never been there.
[01:24:52.080 --> 01:24:53.080]   It looks amazing.
[01:24:53.080 --> 01:24:54.080]   Why Bentonville?
[01:24:54.080 --> 01:24:55.880]   Phil, I'm fascinated.
[01:24:55.880 --> 01:24:56.880]   How did that happen?
[01:24:56.880 --> 01:24:59.080]   Do you have family there?
[01:24:59.080 --> 01:25:00.080]   It was kind of random.
[01:25:00.080 --> 01:25:05.920]   We were just looking to just flee, you know, during COVID.
[01:25:05.920 --> 01:25:12.280]   So we thought we'd go to a few different places and yeah, we had some friends here.
[01:25:12.280 --> 01:25:17.040]   They just got the job at, I hope I'm pronouncing it right, Walmart.
[01:25:17.040 --> 01:25:19.240]   I think it's called Walmart.
[01:25:19.240 --> 01:25:20.240]   Walmart.
[01:25:20.240 --> 01:25:21.240]   Walmart.
[01:25:21.240 --> 01:25:22.680]   And they said, yeah, it's nice and quiet.
[01:25:22.680 --> 01:25:23.680]   Come hang out for a couple of months.
[01:25:23.680 --> 01:25:26.960]   So we did that intending to just leave, but we just fell in love with it because it's
[01:25:26.960 --> 01:25:27.960]   pretty amazing.
[01:25:27.960 --> 01:25:30.080]   That's one of the side effects of this pandemic.
[01:25:30.080 --> 01:25:31.840]   Lisa and I both thought the same thing.
[01:25:31.840 --> 01:25:32.840]   I don't have to be here.
[01:25:32.840 --> 01:25:34.360]   I don't have to be anywhere.
[01:25:34.360 --> 01:25:36.000]   We could do our jobs anywhere.
[01:25:36.000 --> 01:25:38.360]   There's high speed internet.
[01:25:38.360 --> 01:25:39.360]   So why?
[01:25:39.360 --> 01:25:40.360]   That's what I was started telling people.
[01:25:40.360 --> 01:25:42.920]   I started thinking that like, well, yeah, I can live anywhere.
[01:25:42.920 --> 01:25:44.160]   So I may as well live somewhere nice.
[01:25:44.160 --> 01:25:45.160]   I can work from anywhere.
[01:25:45.160 --> 01:25:46.600]   So I may as well live somewhere nice.
[01:25:46.600 --> 01:25:50.280]   What I realized is the second half of that is I'm much feeling much better at work because
[01:25:50.280 --> 01:25:51.280]   they live somewhere nice.
[01:25:51.280 --> 01:25:56.160]   Don't you feel like though that you need to be, you know, in person with people at some
[01:25:56.160 --> 01:25:59.120]   point like your, your, your founders don't you like.
[01:25:59.120 --> 01:26:00.640]   So you're flying around a lot.
[01:26:00.640 --> 01:26:05.320]   Not, I mean, not nearly as much as I used to, but yeah, we, you know, people come over
[01:26:05.320 --> 01:26:07.800]   or I go see them and make them fly to Bentonville.
[01:26:07.800 --> 01:26:08.800]   That's it.
[01:26:08.800 --> 01:26:09.800]   That's not.
[01:26:09.800 --> 01:26:10.800]   It's really nice.
[01:26:10.800 --> 01:26:12.360]   Get on one of those Walmart airplanes.
[01:26:12.360 --> 01:26:15.600]   They're, they're coming in every five minutes and you go to Bentonville.
[01:26:15.600 --> 01:26:16.600]   Yeah.
[01:26:16.600 --> 01:26:19.520]   We have a whole like philosophy about what you should do in person, what you should do
[01:26:19.520 --> 01:26:20.520]   on my video.
[01:26:20.520 --> 01:26:21.520]   I want to hear about this.
[01:26:21.520 --> 01:26:22.520]   Yeah.
[01:26:22.520 --> 01:26:23.520]   Would you be in recorded video?
[01:26:23.520 --> 01:26:26.400]   It's, we're trying to deal with that here because I want, I want all my editors and producers
[01:26:26.400 --> 01:26:28.000]   in studio with me.
[01:26:28.000 --> 01:26:29.000]   Yeah.
[01:26:29.000 --> 01:26:34.040]   Of course, I'm sure they hate that idea, but there's no excuse for not being at your
[01:26:34.040 --> 01:26:35.040]   workplace anymore.
[01:26:35.040 --> 01:26:40.200]   I've also, I've also been entirely replaced by, by an AI.
[01:26:40.200 --> 01:26:41.280]   Once ago, there he goes.
[01:26:41.280 --> 01:26:42.280]   He's glitching.
[01:26:42.280 --> 01:26:43.280]   He's glitching.
[01:26:43.280 --> 01:26:44.280]   Yeah.
[01:26:44.280 --> 01:26:49.000]   If you want to glitch to mm mm mm hmm, mm hmm, hmm, mm.app.
[01:26:49.000 --> 01:26:50.720]   Really cool.
[01:26:50.720 --> 01:26:52.320]   Amy Webb is also with us.
[01:26:52.320 --> 01:26:57.800]   She is with the future today Institute newly redesigned beautiful website.
[01:26:57.800 --> 01:27:06.040]   If you want to know more about what they do and what they create and, and Amy's clearly
[01:27:06.040 --> 01:27:09.480]   one of the smartest people in the world.
[01:27:09.480 --> 01:27:15.880]   She's also the author of the Genesis machine, our quest to rewrite life in the age of synthetic
[01:27:15.880 --> 01:27:20.960]   biology and the genie, Pero endorsed the big nine.
[01:27:20.960 --> 01:27:25.360]   That was such a surreal day in my life.
[01:27:25.360 --> 01:27:28.360]   Who's loves the big nine?
[01:27:28.360 --> 01:27:31.760]   You really do.
[01:27:31.760 --> 01:27:32.760]   Great to have you both.
[01:27:32.760 --> 01:27:35.120]   I am honored that you are here.
[01:27:35.120 --> 01:27:38.080]   Our show today brought to you by exp, and I'm honored that you're here to watching the
[01:27:38.080 --> 01:27:39.080]   show.
[01:27:39.080 --> 01:27:40.080]   Thank you.
[01:27:40.080 --> 01:27:41.080]   I bet you're glad you're here too.
[01:27:41.080 --> 01:27:44.440]   Our show today brought to you by Express VPN.
[01:27:44.440 --> 01:27:49.640]   I want you to the next time you go into private mode or incognito mode in your browser, read
[01:27:49.640 --> 01:27:53.360]   the, the pop up, the little fine print that appears, which nobody reads.
[01:27:53.360 --> 01:27:55.080]   Oh, yeah, yeah.
[01:27:55.080 --> 01:28:00.040]   It actually says your activity could still be visible to your employer, your school, your
[01:28:00.040 --> 01:28:02.120]   internet service provider.
[01:28:02.120 --> 01:28:07.600]   In fact, all incognito mode ever does is, is hide what you're doing from somebody on
[01:28:07.600 --> 01:28:09.680]   your computer.
[01:28:09.680 --> 01:28:11.680]   That's not incognito.
[01:28:11.680 --> 01:28:15.000]   If you really want to stop people from seeing the sites you visit, you need to do what I
[01:28:15.000 --> 01:28:16.000]   do.
[01:28:16.000 --> 01:28:18.000]   You need to use Express VPN.
[01:28:18.000 --> 01:28:21.840]   Think about all the times you've used Wi-Fi at a coffee shop, a hotel, even at your parents
[01:28:21.840 --> 01:28:28.440]   house with that Express VPN, every site you visit can be logged by the admin of that network.
[01:28:28.440 --> 01:28:32.600]   Though the big corporation that owns the ISP or Starbucks, you know, Howard Schultz,
[01:28:32.600 --> 01:28:37.720]   writing it all down, even in incognito mode.
[01:28:37.720 --> 01:28:41.160]   I mean, that's not privacy.
[01:28:41.160 --> 01:28:45.200]   What's more, your home internet provider can also see and record your browsing data, even
[01:28:45.200 --> 01:28:47.040]   in incognito mode.
[01:28:47.040 --> 01:28:51.120]   In the US, they're legally allowed to sell that data advertisers.
[01:28:51.120 --> 01:28:53.440]   They could just pass it along.
[01:28:53.440 --> 01:28:56.680]   Express VPN is an app that encrypts all your network data, re-routes it through a network
[01:28:56.680 --> 01:29:03.840]   of secure servers so your private online activity stays just that private.
[01:29:03.840 --> 01:29:05.480]   Express VPN, it works on all your devices.
[01:29:05.480 --> 01:29:07.560]   It's easy to use one button.
[01:29:07.560 --> 01:29:09.720]   You're immediately joining the server that's nearest to you.
[01:29:09.720 --> 01:29:14.640]   And what I got to give them a little plug because Express VPN invests in their architecture.
[01:29:14.640 --> 01:29:15.840]   It's fast.
[01:29:15.840 --> 01:29:19.080]   You can watch HD video through Express VPN.
[01:29:19.080 --> 01:29:21.440]   It doesn't bog you down.
[01:29:21.440 --> 01:29:26.800]   They also invest in IP addresses so your numbers are rotating so you're even more private.
[01:29:26.800 --> 01:29:29.240]   It's not always the same IP address.
[01:29:29.240 --> 01:29:33.200]   One button connects and your browsing activity is secure from prying eyes.
[01:29:33.200 --> 01:29:34.960]   You can even put Express VPN.
[01:29:34.960 --> 01:29:35.960]   They sell routers.
[01:29:35.960 --> 01:29:37.800]   They have very good routers actually.
[01:29:37.800 --> 01:29:39.240]   Or you can put it on certain routers.
[01:29:39.240 --> 01:29:43.920]   You can find out more at the website and then protect the whole house.
[01:29:43.920 --> 01:29:53.440]   Stop letting strangers invade your online privacy.
[01:29:53.440 --> 01:30:13.560]   And you'd like to see what's going on on Twitter for instance.
[01:30:13.560 --> 01:30:20.360]   Speaking of Twitter, they did block in fact a bunch of anti-Airtawan tweeters at the request
[01:30:20.360 --> 01:30:24.800]   of the Turkish government may not have been too much avail because I think they're going
[01:30:24.800 --> 01:30:27.480]   into a runoff election was today.
[01:30:27.480 --> 01:30:30.640]   Twitter does have a new CEO.
[01:30:30.640 --> 01:30:39.560]   Elon Musk has selected Linda Yacareno, formerly in charge of advertising at NBCUniversal to
[01:30:39.560 --> 01:30:46.560]   help Twitter.
[01:30:46.560 --> 01:31:11.280]   This may have the engineers there.
[01:31:11.280 --> 01:31:33.160]   I do find the new hire, the new CEO less curious than everybody else.
[01:31:33.160 --> 01:31:39.040]   This is a person that is not served as a chief executive officer and a CEO function.
[01:31:39.040 --> 01:31:44.040]   It's a pretty specific role with a pretty specific set of duties.
[01:31:44.040 --> 01:31:48.560]   This is somebody who was the head of advertising at NBCU.
[01:31:48.560 --> 01:31:53.800]   So I think this is really just about getting advertisers back onto the platform.
[01:31:53.800 --> 01:31:57.240]   I don't think it's truly about leading the organization into the future.
[01:31:57.240 --> 01:32:00.800]   Elon will continue to fulfill that role.
[01:32:00.800 --> 01:32:05.520]   I've heard a couple of kind of cringy panel conversation.
[01:32:05.520 --> 01:32:13.120]   I heard a cringy panel conversation where she was interviewing him and it was a lot cringy.
[01:32:13.120 --> 01:32:20.520]   So there was a lot of praising and trying like, "I'm a cool girl and you're a cool guy
[01:32:20.520 --> 01:32:22.440]   and we're cool together."
[01:32:22.440 --> 01:32:24.320]   Just a lot of that.
[01:32:24.320 --> 01:32:27.520]   I'm on Mastodon.
[01:32:27.520 --> 01:32:31.240]   I'm on the Twit server.
[01:32:31.240 --> 01:32:32.320]   Thank you on Twit.
[01:32:32.320 --> 01:32:33.320]   That's social.
[01:32:33.320 --> 01:32:34.320]   Good.
[01:32:34.320 --> 01:32:35.320]   That's a good place to be.
[01:32:35.320 --> 01:32:38.920]   The problem is that your friend group is dispersed to the four wins.
[01:32:38.920 --> 01:32:40.040]   My friend group is dispersed.
[01:32:40.040 --> 01:32:45.040]   I'm actually talking to people on the phone again and I don't want to do that.
[01:32:45.040 --> 01:32:49.640]   It's a horrible, I blame Elon for making me talk on the phone.
[01:32:49.640 --> 01:32:50.920]   Yeah, I've said it before.
[01:32:50.920 --> 01:32:52.520]   I think it's sad.
[01:32:52.520 --> 01:33:00.440]   Twitter had a real value despite it's rocky ups and downs and we're kind of losing it
[01:33:00.440 --> 01:33:03.120]   and that's just, I think that's the way it is.
[01:33:03.120 --> 01:33:05.600]   I don't know what else to say about it.
[01:33:05.600 --> 01:33:06.600]   How about you Phil?
[01:33:06.600 --> 01:33:08.600]   Are you still a Twitter?
[01:33:08.600 --> 01:33:12.400]   I mean, I haven't, I don't tweet that much.
[01:33:12.400 --> 01:33:16.360]   I kind of go through periods where it kind of runs hot and cold.
[01:33:16.360 --> 01:33:18.240]   I just got the new Legend of Zelda games.
[01:33:18.240 --> 01:33:20.080]   I'm not going to be feeding much in the next few months.
[01:33:20.080 --> 01:33:21.280]   Have you started?
[01:33:21.280 --> 01:33:23.240]   I just downloaded a Check it out.
[01:33:23.240 --> 01:33:25.240]   I even got that the Legend of Zelda.
[01:33:25.240 --> 01:33:27.920]   You're going to play it on the Switch.
[01:33:27.920 --> 01:33:30.120]   You're not going to play it on a television?
[01:33:30.120 --> 01:33:31.120]   Like a big monitor?
[01:33:31.120 --> 01:33:32.120]   I'm going to both, I think.
[01:33:32.120 --> 01:33:37.800]   I think I've got some flights coming up so I wanted something I could play offline.
[01:33:37.800 --> 01:33:40.520]   We just on the Aztec guys did a little demo.
[01:33:40.520 --> 01:33:42.840]   This game came out Friday.
[01:33:42.840 --> 01:33:45.840]   It is already, in fact I have it right here on my Switch.
[01:33:45.840 --> 01:33:46.840]   There you go.
[01:33:46.840 --> 01:33:47.840]   It's already...
[01:33:47.840 --> 01:33:48.840]   No one wave their Switch around.
[01:33:48.840 --> 01:33:51.000]   Everybody wave your Switch.
[01:33:51.000 --> 01:33:54.400]   It's already on Metacritic and...
[01:33:54.400 --> 01:33:56.440]   It looks amazing.
[01:33:56.440 --> 01:34:01.360]   Yeah, it's already being hailed as the best video game of all time.
[01:34:01.360 --> 01:34:03.160]   I think that may be a little overstatement.
[01:34:03.160 --> 01:34:04.640]   If you're a Zelda fan...
[01:34:04.640 --> 01:34:09.560]   I mean I thought the last one was the greatest pieces of art and culture.
[01:34:09.560 --> 01:34:12.440]   Yeah, it was awesome.
[01:34:12.440 --> 01:34:13.880]   It is, I think it's a...
[01:34:13.880 --> 01:34:16.600]   That's one of the reasons we showed it on Aztec.
[01:34:16.600 --> 01:34:20.920]   Not as a promotion for the game or as an ad for the game, but just you're going to hear
[01:34:20.920 --> 01:34:25.680]   a lot about it in the next few weeks and you perhaps would like to know more about what
[01:34:25.680 --> 01:34:26.680]   it is.
[01:34:26.680 --> 01:34:29.400]   Yeah, I think it's cultural.
[01:34:29.400 --> 01:34:36.520]   I'm not of an age where either Mario or Zelda was in my formative years.
[01:34:36.520 --> 01:34:43.040]   But I do like games and it is such an open world and you could do so much including building.
[01:34:43.040 --> 01:34:44.600]   It's very interesting.
[01:34:44.600 --> 01:34:46.480]   I'll probably play a lot more of it.
[01:34:46.480 --> 01:34:49.480]   I've only gotten to the first few hours of it.
[01:34:49.480 --> 01:34:50.480]   We had our...
[01:34:50.480 --> 01:34:52.920]   Phil, what's the new storyline?
[01:34:52.920 --> 01:34:54.480]   I don't know.
[01:34:54.480 --> 01:34:56.400]   I literally just finished the loading.
[01:34:56.400 --> 01:34:58.760]   So I could tell you a little bit, which is...
[01:34:58.760 --> 01:35:00.920]   It's the same story.
[01:35:00.920 --> 01:35:07.440]   So as you may remember, Link was gone for 100 years, came back.
[01:35:07.440 --> 01:35:15.320]   He hooked up, he solved, got rid of the calamity Ganon, cleansed the world, but got severely
[01:35:15.320 --> 01:35:16.320]   injured.
[01:35:16.320 --> 01:35:18.640]   So he's waking up from his coma.
[01:35:18.640 --> 01:35:24.880]   Zelda's there, they go down into the basement underneath the castle and discover an ancient
[01:35:24.880 --> 01:35:25.880]   civilization.
[01:35:25.880 --> 01:35:30.560]   So it's kind of the precursor civilization to Hyrule.
[01:35:30.560 --> 01:35:31.560]   It isn't.
[01:35:31.560 --> 01:35:38.240]   It's a sequel, but there's a whole new open world, a bunch of new adversaries and puzzles.
[01:35:38.240 --> 01:35:41.240]   It's huge, it's beautiful.
[01:35:41.240 --> 01:35:46.840]   One of the things that happens almost immediately is you get to a high vantage point and you
[01:35:46.840 --> 01:35:50.120]   look around and you can see the world and it's huge.
[01:35:50.120 --> 01:35:51.120]   Oh, wow.
[01:35:51.120 --> 01:35:52.120]   Yeah.
[01:35:52.120 --> 01:35:55.640]   So it's both got an underground and above ground component.
[01:35:55.640 --> 01:35:57.080]   The building is very interesting.
[01:35:57.080 --> 01:36:02.000]   I think a lot of people will be kind of a Minecraft style building mechanism mechanic
[01:36:02.000 --> 01:36:03.120]   that's going to be quite good.
[01:36:03.120 --> 01:36:08.920]   But seriously, literally some of the review, open critic saying best video game ever.
[01:36:08.920 --> 01:36:11.840]   I mean, it's already...
[01:36:11.840 --> 01:36:14.200]   It's only been out for two and a half days.
[01:36:14.200 --> 01:36:15.200]   But...
[01:36:15.200 --> 01:36:16.200]   Need time.
[01:36:16.200 --> 01:36:17.200]   Yeah.
[01:36:17.200 --> 01:36:21.840]   And I think we need like...
[01:36:21.840 --> 01:36:27.440]   I think a lot of this large language model AI stuff reduces the amount of work that it
[01:36:27.440 --> 01:36:30.520]   takes to make something mediocre.
[01:36:30.520 --> 01:36:34.200]   And I think the antidote to that is making stuff that is really exceptional.
[01:36:34.200 --> 01:36:38.120]   I'm very much looking forward to this as being something that I hope is going to live up
[01:36:38.120 --> 01:36:39.640]   to expectations, so be really exceptional.
[01:36:39.640 --> 01:36:41.600]   I really think the last game was.
[01:36:41.600 --> 01:36:46.120]   Like this is, for me, the most optimistic way to look at chat GPT and stuff like that
[01:36:46.120 --> 01:36:48.800]   is it should raise the bar of what it means to be human.
[01:36:48.800 --> 01:36:53.440]   It should raise the bar of what it means to be made by people.
[01:36:53.440 --> 01:36:55.760]   I've already tried to apply this as just my own stuff.
[01:36:55.760 --> 01:36:59.520]   I think like, "Well, if chat GPT could have written this, I can't."
[01:36:59.520 --> 01:37:05.680]   I have to set the bar higher.
[01:37:05.680 --> 01:37:08.560]   And yeah, I really think that this is going to be...
[01:37:08.560 --> 01:37:09.760]   I am very much looking forward.
[01:37:09.760 --> 01:37:12.640]   I could be disappointed, but early reviews are very positive.
[01:37:12.640 --> 01:37:13.640]   I think that's a...
[01:37:13.640 --> 01:37:14.640]   This could be an exceptional thing.
[01:37:14.640 --> 01:37:21.960]   It's an excellent point that one of the things that happens with technologies is that there's
[01:37:21.960 --> 01:37:27.640]   a higher value put on human effort.
[01:37:27.640 --> 01:37:31.200]   It suddenly becomes more valuable, the art that's created by humans, not a machine or
[01:37:31.200 --> 01:37:36.160]   the writing that's created by a human, not a machine, or a game that's created by a human.
[01:37:36.160 --> 01:37:41.160]   And of course, it's not accurate to say that AI is either made by a human or an AI.
[01:37:41.160 --> 01:37:44.720]   Like everything in the future, I think Amy's point is going to be made by humans using
[01:37:44.720 --> 01:37:45.720]   AI.
[01:37:45.720 --> 01:37:48.640]   Like our tools will all become AI infused.
[01:37:48.640 --> 01:37:50.960]   We'll just have to find that balance.
[01:37:50.960 --> 01:37:53.600]   Like what we're seeing right now is not AI infused.
[01:37:53.600 --> 01:37:57.320]   It's just like mediocre crap that things are being turned out.
[01:37:57.320 --> 01:38:01.840]   Very much looking forward to seeing what the next Legend of Zelda that comes out and whatever.
[01:38:01.840 --> 01:38:04.160]   I guess it was five years since the last one.
[01:38:04.160 --> 01:38:08.880]   The next one, five years from now, obviously will be made by people using a tool set that
[01:38:08.880 --> 01:38:11.680]   very much is AI informed.
[01:38:11.680 --> 01:38:13.680]   And there's a potential that it becomes even greater.
[01:38:13.680 --> 01:38:15.280]   But we just we need to find that equilibrium.
[01:38:15.280 --> 01:38:19.000]   We need to find that like, you know, we're not like hand coloring pixels.
[01:38:19.000 --> 01:38:22.720]   We're using modern tools and modern tools will become AI infused.
[01:38:22.720 --> 01:38:27.040]   And hopefully that will lead to even even more beautiful exceptional things.
[01:38:27.040 --> 01:38:31.360]   But what we're seeing right now in this kind of early phase is just like AI generated
[01:38:31.360 --> 01:38:32.360]   mediocrity.
[01:38:32.360 --> 01:38:43.560]   Yeah, as a point of example, Snapchat influencer Karen Marjorie released an AI powered virtual
[01:38:43.560 --> 01:38:49.800]   girlfriend based on her, her voice, her story and it ended up just basically being a sex
[01:38:49.800 --> 01:38:51.960]   chat.
[01:38:51.960 --> 01:38:56.360]   Which I don't think she's too unhappy about because she's charging a buck a minute.
[01:38:56.360 --> 01:39:00.080]   Even though she says, Oh, oh, no, that's not what we wanted.
[01:39:00.080 --> 01:39:05.040]   That's that's an interesting regulatory question.
[01:39:05.040 --> 01:39:08.440]   I wonder how prostitution is actually like legally defined.
[01:39:08.440 --> 01:39:10.000]   Well, how do they?
[01:39:10.000 --> 01:39:11.000]   Well, it's not.
[01:39:11.000 --> 01:39:12.000]   I mean, there's no sexual acts.
[01:39:12.000 --> 01:39:15.720]   It's just it's just a text chat.
[01:39:15.720 --> 01:39:17.920]   Certainly not illegal prostitution.
[01:39:17.920 --> 01:39:23.400]   Well, I'm asking because, you know, in Utah, I'm sure you guys talked about this already
[01:39:23.400 --> 01:39:26.720]   at some point, but Utah changed its privacy.
[01:39:26.720 --> 01:39:27.720]   Yes.
[01:39:27.720 --> 01:39:30.720]   And then I think that's a very interesting question.
[01:39:30.720 --> 01:39:32.720]   I think that's a very interesting question.
[01:39:32.720 --> 01:39:34.720]   I think that's a very interesting question.
[01:39:34.720 --> 01:39:35.720]   I think that's a very interesting question.
[01:39:35.720 --> 01:39:36.720]   I think that's a very interesting question.
[01:39:36.720 --> 01:39:37.720]   I think that's a very interesting question.
[01:39:37.720 --> 01:39:38.720]   I think that's a very interesting question.
[01:39:38.720 --> 01:39:39.720]   I think that's a very interesting question.
[01:39:39.720 --> 01:39:40.720]   I think that's a very interesting question.
[01:39:40.720 --> 01:39:41.720]   I think that's a very interesting question.
[01:39:41.720 --> 01:39:42.720]   I think that's a very interesting question.
[01:39:42.720 --> 01:39:43.720]   I think that's a very interesting question.
[01:39:43.720 --> 01:39:44.720]   I think that's a very interesting question.
[01:39:44.720 --> 01:39:45.720]   I think that's a very interesting question.
[01:39:45.720 --> 01:39:46.720]   I think that's a very interesting question.
[01:39:46.720 --> 01:39:47.720]   I think that's a very interesting question.
[01:39:47.720 --> 01:39:48.720]   I think that's a very interesting question.
[01:39:48.720 --> 01:39:49.720]   I think that's a very interesting question.
[01:39:49.720 --> 01:39:50.720]   I think that's a very interesting question.
[01:39:50.720 --> 01:39:57.720]   I think that's another one of these great areas that the states are going to have a lot of power and we wind up with ever more splinter net versions of.
[01:39:57.720 --> 01:39:59.720]   Actually, it's an interesting point.
[01:39:59.720 --> 01:40:18.720]   If you're a highly conservative state where they're worried about social networks on effect on kids, a sex, AI based sex chat bot might be something that bring down the law.
[01:40:18.720 --> 01:40:22.720]   This is actually an interesting topic.
[01:40:22.720 --> 01:40:26.720]   I was an early investor of replica.
[01:40:26.720 --> 01:40:28.720]   A replica.AI, which was in the news more recently.
[01:40:28.720 --> 01:40:30.720]   That company took a turn.
[01:40:30.720 --> 01:40:32.720]   Well, it kind of didn't.
[01:40:32.720 --> 01:40:35.720]   It's kind of been the idea of replica.
[01:40:35.720 --> 01:40:40.720]   You should have the CEO, Eugne Cudia on your show.
[01:40:40.720 --> 01:40:41.720]   She's fascinating.
[01:40:41.720 --> 01:40:54.720]   The whole point of replica, which we helped with at Alternals many years ago, was that you could have an emotionally resonant connection with a chat bot with an AI actually much easier than a factual one.
[01:40:54.720 --> 01:40:58.720]   You can make an AI that spoke to an emotional connection.
[01:40:58.720 --> 01:41:05.720]   It was more possible to do that than to make one that would give you factual answers about stuff.
[01:41:05.720 --> 01:41:10.720]   They designed this thing to be initially kind of a friend, a companion, a mental health bot.
[01:41:10.720 --> 01:41:13.720]   It was about giving people support and therapy.
[01:41:13.720 --> 01:41:15.720]   It went more recently.
[01:41:15.720 --> 01:41:20.720]   It went into all sorts of kind of erotic and sexual direction.
[01:41:20.720 --> 01:41:22.720]   Some of it intentional, some of it not.
[01:41:22.720 --> 01:41:23.720]   There was a backlash.
[01:41:23.720 --> 01:41:30.720]   They turned a bunch of the sex stuff off, but then users felt really lonely about it because there was a lot of people who actually-
[01:41:30.720 --> 01:41:32.720]   This was their friend?
[01:41:32.720 --> 01:41:33.720]   Yeah.
[01:41:33.720 --> 01:41:39.720]   They restored it, but I think a well-designed version of that is actually quite laudable.
[01:41:39.720 --> 01:41:41.720]   The only embarrassing thing we'd be doing this poorly.
[01:41:41.720 --> 01:41:52.720]   Yeah, like half-assing a sex bot could be quite harmful, but doing it thoughtfully and doing it well is potentially a valuable thing.
[01:41:52.720 --> 01:41:53.720]   I wouldn't-
[01:41:53.720 --> 01:41:54.720]   You think so?
[01:41:54.720 --> 01:41:55.720]   I mean-
[01:41:55.720 --> 01:41:56.720]   I'd check that off hand.
[01:41:56.720 --> 01:41:57.720]   Which is not necessarily what Republicans do.
[01:41:57.720 --> 01:42:01.720]   Well, it just existed in China, what, six, seven years ago?
[01:42:01.720 --> 01:42:02.720]   It was the precursor to TAY.
[01:42:02.720 --> 01:42:03.720]   It was the same code.
[01:42:03.720 --> 01:42:06.720]   Yeah, and we'll make ourselves learn nothing from that experience.
[01:42:06.720 --> 01:42:07.720]   Yeah, okay.
[01:42:07.720 --> 01:42:14.720]   But I mean, there was a bot that did possess- it could remember conversation.
[01:42:14.720 --> 01:42:24.720]   If you had a soccer injury, it had a persona, so I think it was meant to be an older teenage girl.
[01:42:24.720 --> 01:42:30.720]   But there was this research done and most people didn't realize she wasn't a human, and then when they did realize that they had a person.
[01:42:30.720 --> 01:42:35.720]   They did realize that they had created, just like Phil said, this deep connection, that they didn't want to let go.
[01:42:35.720 --> 01:42:38.720]   So they continued chatting with her anyways.
[01:42:38.720 --> 01:42:39.720]   Cool.
[01:42:39.720 --> 01:42:40.720]   It's a little creepy.
[01:42:40.720 --> 01:42:47.720]   I would worry about the long-term- I mean, it's like the pillow brides in Japan.
[01:42:47.720 --> 01:42:53.720]   I mean, I worry a little bit about the long-term impact of such a thing.
[01:42:53.720 --> 01:42:56.720]   Look, I think there's-
[01:42:56.720 --> 01:43:01.720]   Right, we are potentially more lonely than we've ever been, right, as a society.
[01:43:01.720 --> 01:43:02.720]   Yes.
[01:43:02.720 --> 01:43:03.720]   There's all sorts of stats on this.
[01:43:03.720 --> 01:43:04.720]   But is this more solution?
[01:43:04.720 --> 01:43:05.720]   Well, well, hold on.
[01:43:05.720 --> 01:43:08.720]   But we're more connected to other people than we've ever been, right?
[01:43:08.720 --> 01:43:11.720]   So this was always the big lie of Facebook, right?
[01:43:11.720 --> 01:43:13.720]   Facebook was saying, "Oh, we're about connecting people."
[01:43:13.720 --> 01:43:15.720]   And actually turns out that you're not about connecting people.
[01:43:15.720 --> 01:43:22.720]   You're about connecting you with your own biases and getting you out of actual, like, human connection.
[01:43:22.720 --> 01:43:26.720]   And loneliness is not about lack of interaction with other people.
[01:43:26.720 --> 01:43:30.720]   Loneliness is fundamentally about a lack of understanding of yourself, a lack of connection
[01:43:30.720 --> 01:43:36.720]   with yourself, a lack of knowledge and expertise about what your emotional state is, not being
[01:43:36.720 --> 01:43:39.720]   able to name your emotions, not being able to think about those.
[01:43:39.720 --> 01:43:41.720]   And social media has made a lot of that much worse.
[01:43:41.720 --> 01:43:43.720]   So like, yeah, it's kind of creepy.
[01:43:43.720 --> 01:43:46.720]   I mean, it's weird if you make it weird.
[01:43:46.720 --> 01:43:51.720]   But a thoughtful tool that's designed to help people understand themselves a bit better through
[01:43:51.720 --> 01:43:54.720]   conversation with that isn't with another human.
[01:43:54.720 --> 01:43:55.720]   So it's not judging you.
[01:43:55.720 --> 01:44:00.720]   So it isn't going to do anything like potentially harmful is very valuable to many, many people.
[01:44:00.720 --> 01:44:01.720]   Interesting conditions.
[01:44:01.720 --> 01:44:07.720]   You know, Freudian psychotherapy, like traditional, Freudian psychotherapy, the therapist says
[01:44:07.720 --> 01:44:09.720]   very little, right?
[01:44:09.720 --> 01:44:13.720]   It's all about the process of talking it out.
[01:44:13.720 --> 01:44:18.720]   I suppose you could do psychotherapy with a, with a bot.
[01:44:18.720 --> 01:44:24.720]   But I wonder if some of the value of psychotherapies going into a room with a human.
[01:44:24.720 --> 01:44:25.720]   Some.
[01:44:25.720 --> 01:44:27.720]   That was one of the original intensives too.
[01:44:27.720 --> 01:44:29.720]   It was one of the original intensive AI.
[01:44:29.720 --> 01:44:33.720]   There was a chatbot built right, Eliza for therapy.
[01:44:33.720 --> 01:44:38.720]   Eliza was terrible because Eliza had no, I mean, they've gotten better.
[01:44:38.720 --> 01:44:40.720]   Eliza wasn't meant for that, right?
[01:44:40.720 --> 01:44:43.720]   It was just like a, Eliza was a little, it was a toy, right?
[01:44:43.720 --> 01:44:44.720]   It was a toy.
[01:44:44.720 --> 01:44:47.720]   It just said, look, the first bot I ever wrote, and I really think, I think, I think,
[01:44:47.720 --> 01:44:50.720]   I should be known as the godfather of the, of these constituents.
[01:44:50.720 --> 01:44:52.720]   I wrote a bot once many, many years ago.
[01:44:52.720 --> 01:44:55.720]   It was called Rashbot.
[01:44:55.720 --> 01:45:01.720]   And the way Rashbot works is you texted it a picture of your rash and it replied, it replied,
[01:45:01.720 --> 01:45:02.720]   you.
[01:45:02.720 --> 01:45:04.720]   So it did.
[01:45:04.720 --> 01:45:10.720]   Except 10% of the time, 10% of the time randomly, it also added, you had to get that checked out.
[01:45:10.720 --> 01:45:11.720]   Oh, good.
[01:45:11.720 --> 01:45:13.720]   So you would, you would text it a photo of your rash or anything.
[01:45:13.720 --> 01:45:15.720]   Obviously, I was doing no, no image recognition.
[01:45:15.720 --> 01:45:20.720]   And it would reply, you, and sometimes you, you really ought to get that like that.
[01:45:20.720 --> 01:45:26.720]   And I think that was kind of like the MVP product for all of the AI that we have now.
[01:45:26.720 --> 01:45:29.720]   Okay.
[01:45:29.720 --> 01:45:36.720]   Rashbot, when he first said the name, I thought, well, he can't mean R.A.S.H., but I guess
[01:45:36.720 --> 01:45:37.720]   he did.
[01:45:37.720 --> 01:45:38.720]   Yeah.
[01:45:38.720 --> 01:45:39.720]   Yeah.
[01:45:39.720 --> 01:45:40.720]   Yeah.
[01:45:40.720 --> 01:45:41.720]   Was ahead of its time.
[01:45:41.720 --> 01:45:47.720]   I mean, seriously, do you think that psychotherapy could be done by an AI effectively?
[01:45:47.720 --> 01:45:52.720]   I mean, if a Freudian therapist just goes, uh-huh, every five minutes.
[01:45:52.720 --> 01:45:55.720]   Freudian therapy has been discredited.
[01:45:55.720 --> 01:45:56.720]   I think so.
[01:45:56.720 --> 01:45:57.720]   A hundred years ago.
[01:45:57.720 --> 01:45:58.720]   Yeah.
[01:45:58.720 --> 01:45:59.720]   I don't think it's, I don't think it can.
[01:45:59.720 --> 01:46:00.720]   You don't think so.
[01:46:00.720 --> 01:46:06.720]   I've been in cognitive, no, I've been, I've been in cognitive behavioral therapy for several
[01:46:06.720 --> 01:46:07.720]   years.
[01:46:07.720 --> 01:46:08.720]   Yeah.
[01:46:08.720 --> 01:46:12.720]   I started out with an acute issue and then I kind of stayed because it's a little bit like
[01:46:12.720 --> 01:46:13.720]   AA.
[01:46:13.720 --> 01:46:14.720]   Yeah.
[01:46:14.720 --> 01:46:15.720]   It's not, no, it's not fun.
[01:46:15.720 --> 01:46:16.720]   It's hard work.
[01:46:16.720 --> 01:46:21.720]   It's very hard work, but it's like, um, I need to check in regularly.
[01:46:21.720 --> 01:46:22.720]   Your heart.
[01:46:22.720 --> 01:46:26.960]   So here's, here's why I don't think it works because the thing that I needed treatment
[01:46:26.960 --> 01:46:31.240]   for at the beginning is not the same thing that I need right now.
[01:46:31.240 --> 01:46:35.480]   And what I have now with the same guy is learning a little bit about theory and where
[01:46:35.480 --> 01:46:37.200]   some of this is coming from.
[01:46:37.200 --> 01:46:44.160]   I think the problem is if he was a, how would you encode a system to know, to teach it to
[01:46:44.160 --> 01:46:47.240]   evolve with the patient?
[01:46:47.240 --> 01:46:52.200]   And how would you be able to anticipate that this, that what I'm looking now for is more
[01:46:52.200 --> 01:46:57.320]   almost like, uh, instruction on theory of mind and things like that.
[01:46:57.320 --> 01:47:01.640]   What I need is so vastly different from what somebody else at this stage might need that
[01:47:01.640 --> 01:47:07.360]   I think that there was, there's really no way, maybe sort of a extremely short term, very
[01:47:07.360 --> 01:47:11.960]   acute issue that has more limited parameters, but we don't really understand how the mind
[01:47:11.960 --> 01:47:12.960]   works anyways.
[01:47:12.960 --> 01:47:19.320]   I think it'd be, I, this is not where I would push the frontiers of AI.
[01:47:19.320 --> 01:47:22.800]   I think as a tool, it could be, it could be useful for, for some people.
[01:47:22.800 --> 01:47:25.640]   It certainly wouldn't be the same, uh, as doing the UNs.
[01:47:25.640 --> 01:47:29.480]   And I think there's going to be a next generation of therapists that are coming onto the market
[01:47:29.480 --> 01:47:34.000]   right now that we'll grow up with this technology and we'll use AI power tools.
[01:47:34.000 --> 01:47:35.760]   It'll be, again, it'll be sent to our chest.
[01:47:35.760 --> 01:47:36.760]   It'll be augmented intelligence.
[01:47:36.760 --> 01:47:40.560]   It'll be a combination of what is it that we're all looking for?
[01:47:40.560 --> 01:47:41.760]   We're all seeking insights.
[01:47:41.760 --> 01:47:45.120]   This is the word we haven't used yet, but that's what all of this is about.
[01:47:45.120 --> 01:47:49.800]   It's not truly about automating it, things so that we don't have to do them anymore.
[01:47:49.800 --> 01:47:50.800]   Exactly.
[01:47:50.800 --> 01:47:51.800]   Right?
[01:47:51.800 --> 01:47:54.240]   It's about getting to new depths and new insights.
[01:47:54.240 --> 01:48:00.280]   It is, this is maybe the most profound thing about this that, that, that is very much what
[01:48:00.280 --> 01:48:01.280]   you said.
[01:48:01.280 --> 01:48:05.440]   I don't think it is, I don't think anyone needs us to make it easier to do things that we
[01:48:05.440 --> 01:48:08.360]   don't want in the first place.
[01:48:08.360 --> 01:48:11.680]   Reducing the friction to mediocrity is not a virtuous thing.
[01:48:11.680 --> 01:48:12.680]   That's going to make the world worse.
[01:48:12.680 --> 01:48:13.680]   It's not going to make it better.
[01:48:13.680 --> 01:48:18.440]   We don't need to make it simpler to do things that, that are, that are poor quality.
[01:48:18.440 --> 01:48:22.840]   We don't need to reduce the friction of both the bullshit jobs.
[01:48:22.840 --> 01:48:27.560]   So which is what 95% of this stuff is, but maybe to Leo's point, that's always what
[01:48:27.560 --> 01:48:31.560]   the beginning is because you're taking the low hanging fruit, shaking the easy stuff.
[01:48:31.560 --> 01:48:36.120]   If I'm trying to be optimistic about it, it's about saying that after we've sort of washed
[01:48:36.120 --> 01:48:42.400]   through that, which hopefully happens quickly, we wind up using these AI tools to make really
[01:48:42.400 --> 01:48:47.640]   high quality, effective things not easier, just possible.
[01:48:47.640 --> 01:48:49.040]   You've talked about coding.
[01:48:49.040 --> 01:48:51.320]   You've talked about coding, Phil.
[01:48:51.320 --> 01:48:56.520]   A lot of coders are using co-pilot on GitHub and similar tools.
[01:48:56.520 --> 01:48:58.960]   I think temporarily, but yeah.
[01:48:58.960 --> 01:48:59.960]   Okay.
[01:48:59.960 --> 01:49:01.600]   Do you don't think it's of real value?
[01:49:01.600 --> 01:49:05.880]   No, no, I think it's a tremendous value, but I think it's probably a stepping stone to
[01:49:05.880 --> 01:49:07.800]   better computer languages.
[01:49:07.800 --> 01:49:10.240]   They're using it to automate things.
[01:49:10.240 --> 01:49:14.240]   And so the things that are automated are just to be part of the language in the beginning.
[01:49:14.240 --> 01:49:19.040]   So like, you just, like, probably the next generic computer language is constantly evolving.
[01:49:19.040 --> 01:49:22.880]   And my guess is that the next few that come out are going to be deeply informed by things
[01:49:22.880 --> 01:49:23.880]   like co-pilot.
[01:49:23.880 --> 01:49:24.880]   And then you just won't need them.
[01:49:24.880 --> 01:49:26.280]   This is, again, this is what Amy was saying before.
[01:49:26.280 --> 01:49:28.520]   Like we could just like step one.
[01:49:28.520 --> 01:49:30.840]   So you have a bunch of nonsense.
[01:49:30.840 --> 01:49:34.200]   Step one, step two is you bring an AI to automate that nonsense.
[01:49:34.200 --> 01:49:37.680]   And then hopefully we all move on to step three, which is you just get rid of that stuff.
[01:49:37.680 --> 01:49:38.680]   Right.
[01:49:38.680 --> 01:49:39.680]   Not automated.
[01:49:39.680 --> 01:49:40.680]   You just get rid of it.
[01:49:40.680 --> 01:49:44.400]   I'm pretty sure that chat GPT could write rash bot right now.
[01:49:44.400 --> 01:49:45.400]   It could write rash bot.
[01:49:45.400 --> 01:49:46.400]   It couldn't write Zelda.
[01:49:46.400 --> 01:49:47.400]   It couldn't write.
[01:49:47.400 --> 01:49:48.400]   It couldn't write.
[01:49:48.400 --> 01:49:49.400]   Yeah.
[01:49:49.400 --> 01:49:50.400]   No.
[01:49:50.400 --> 01:49:54.480]   Although people working on it could certainly use co-pilot to save some time.
[01:49:54.480 --> 01:49:58.840]   And the writer's guild, which is going on strike among other reasons, one of the reasons
[01:49:58.840 --> 01:50:03.240]   is they don't want AI to be utilized against them.
[01:50:03.240 --> 01:50:07.960]   They fear that, you know, an AI could take all the episodes of the Simpsons.
[01:50:07.960 --> 01:50:11.240]   So they're doing the thing that the AI will never do, which is go on strike, which is
[01:50:11.240 --> 01:50:12.240]   maybe not the best thought.
[01:50:12.240 --> 01:50:13.240]   Right.
[01:50:13.240 --> 01:50:14.240]   Maybe.
[01:50:14.240 --> 01:50:16.040]   I'm going to ask chat GPT what their strategy should be.
[01:50:16.040 --> 01:50:17.040]   Isn't that fantastic?
[01:50:17.040 --> 01:50:18.680]   No, but I go on strike.
[01:50:18.680 --> 01:50:23.200]   So we do a lot of work on shows and with studios.
[01:50:23.200 --> 01:50:29.480]   And I've had a couple of higher level folks very seriously say, are we there yet?
[01:50:29.480 --> 01:50:30.480]   Can we get?
[01:50:30.480 --> 01:50:31.480]   They want to.
[01:50:31.480 --> 01:50:32.480]   Can they say AI systems?
[01:50:32.480 --> 01:50:33.480]   If they could.
[01:50:33.480 --> 01:50:34.480]   Yeah.
[01:50:34.480 --> 01:50:38.880]   They want to because a couple of, again, what are the external forces pressuring them?
[01:50:38.880 --> 01:50:40.800]   There's more consolidation coming.
[01:50:40.800 --> 01:50:45.560]   So the streamers are going to start consolidating down to fewer than there are right now.
[01:50:45.560 --> 01:50:47.880]   There's a glut of content.
[01:50:47.880 --> 01:50:51.880]   And in order to keep subscriptions high, you have to keep pumping things out, but the business
[01:50:51.880 --> 01:50:52.880]   model doesn't work.
[01:50:52.880 --> 01:50:54.400]   It's too expensive.
[01:50:54.400 --> 01:50:55.960]   So they're looking at this.
[01:50:55.960 --> 01:51:01.600]   There's no, I mean, I was, I was thinking through like, is there any franchise where
[01:51:01.600 --> 01:51:05.760]   you could theoretically get a script 80% there?
[01:51:05.760 --> 01:51:10.360]   And it would have to be something formulaic like a law and order, which pulls headlines,
[01:51:10.360 --> 01:51:12.880]   you know, the story concepts for from the headlines.
[01:51:12.880 --> 01:51:19.520]   It's basically the same thing that happens, you know, every show, but we're nowhere, nowhere
[01:51:19.520 --> 01:51:20.520]   close.
[01:51:20.520 --> 01:51:24.640]   That being said, I think I've had meetings with four different studios about scripts
[01:51:24.640 --> 01:51:28.720]   involving everybody wants to write something about AI that's going to come out.
[01:51:28.720 --> 01:51:35.880]   So like brace yourselves in the year 2025 and 2026, there's going to be like more a shows
[01:51:35.880 --> 01:51:38.400]   about AI than any of us want.
[01:51:38.400 --> 01:51:39.400]   Good job.
[01:51:39.400 --> 01:51:42.240]   I think the way I usually put this to people.
[01:51:42.240 --> 01:51:46.320]   And I forget who I first heard to say this is not original by me.
[01:51:46.320 --> 01:51:48.520]   Your job is not going to get replaced by AI.
[01:51:48.520 --> 01:51:51.040]   Your job is going to replace by someone using AI.
[01:51:51.040 --> 01:51:54.840]   Just like if you're refusing to use it, yes, your job is in danger.
[01:51:54.840 --> 01:51:57.320]   You're going to lose your job to someone who is using it.
[01:51:57.320 --> 01:51:59.920]   What would you recommend now?
[01:51:59.920 --> 01:52:02.640]   Or is this stuff just toy and not worth spending time on?
[01:52:02.640 --> 01:52:07.000]   I mean, obviously, if I could make three to a thousand dollars as a year as a prompt engineer,
[01:52:07.000 --> 01:52:08.600]   I might try that.
[01:52:08.600 --> 01:52:15.880]   But what would you say if you were serious about becoming this symbiotic AI human thing?
[01:52:15.880 --> 01:52:16.880]   What would you do right now?
[01:52:16.880 --> 01:52:20.600]   Is there anything you would do right now, Phil, to pursue that?
[01:52:20.600 --> 01:52:25.200]   I'm just trying to get higher and higher quality in everything that I do.
[01:52:25.200 --> 01:52:26.200]   And I'm looking for...
[01:52:26.200 --> 01:52:27.600]   Not worry about AI at this point.
[01:52:27.600 --> 01:52:28.600]   Not worry about it.
[01:52:28.600 --> 01:52:29.600]   Yeah.
[01:52:29.600 --> 01:52:30.600]   Just use...
[01:52:30.600 --> 01:52:31.600]   And then like, and I haven't actually...
[01:52:31.600 --> 01:52:36.280]   I haven't incorporated Chen GPT or AI successfully into my daily workflow because for me, I have
[01:52:36.280 --> 01:52:39.920]   not found the thing that I can do that it would actually make the quality better.
[01:52:39.920 --> 01:52:40.920]   That's exactly right.
[01:52:40.920 --> 01:52:44.360]   I found lots of ways that I could automate crap, but then I just don't do the crap anyway,
[01:52:44.360 --> 01:52:45.880]   and it turns out I don't need to automate it.
[01:52:45.880 --> 01:52:47.720]   The one thing I wouldn't worry about it?
[01:52:47.720 --> 01:52:48.720]   Yeah.
[01:52:48.720 --> 01:52:49.720]   Please.
[01:52:49.720 --> 01:52:55.800]   The one thing I've done with Chen GPT that actually was useful, we had a surprise visit
[01:52:55.800 --> 01:53:00.880]   to Genoa on our vacation, Italy, knew nothing about it, didn't have any guidebooks, asked
[01:53:00.880 --> 01:53:05.800]   Chen GPT to write a three hour walking tour starting at a specific point.
[01:53:05.800 --> 01:53:07.920]   It was quite good.
[01:53:07.920 --> 01:53:11.840]   It didn't take me to any non-existent spots.
[01:53:11.840 --> 01:53:14.800]   But everything else I've done is very toy-like and not useful.
[01:53:14.800 --> 01:53:21.240]   I mean, I have fantasies that maybe Chen GPT could, I could say, tell me, give me rank
[01:53:21.240 --> 01:53:26.560]   the top 10 tech stories of the week, give me summaries for these as my rundown for the
[01:53:26.560 --> 01:53:27.560]   show, for instance.
[01:53:27.560 --> 01:53:30.360]   But it won't do a better job than you would.
[01:53:30.360 --> 01:53:32.600]   Yeah, but it would do it and I wouldn't have to.
[01:53:32.600 --> 01:53:37.320]   So if it was 95% there, it would be happy.
[01:53:37.320 --> 01:53:40.120]   But then the next step of that is in the audience, why am I watching this?
[01:53:40.120 --> 01:53:42.080]   Because I could just ask a few things.
[01:53:42.080 --> 01:53:43.080]   Never mind.
[01:53:43.080 --> 01:53:45.280]   Forget I said that, folks.
[01:53:45.280 --> 01:53:48.240]   Ixne on the etche.
[01:53:48.240 --> 01:53:49.240]   Microsoft is--
[01:53:49.240 --> 01:53:50.240]   Go ahead.
[01:53:50.240 --> 01:53:52.520]   Oh, I was going to say, what's the name of the company?
[01:53:52.520 --> 01:53:59.520]   Some from Chowdrey, former Apple execs, started humanity, human-- what's the name of that company?
[01:53:59.520 --> 01:54:02.800]   Human something?
[01:54:02.800 --> 01:54:04.800]   What does it do?
[01:54:04.800 --> 01:54:08.400]   Well, it was revealed, finally, after all of this time.
[01:54:08.400 --> 01:54:11.240]   He's a former Apple guy.
[01:54:11.240 --> 01:54:13.920]   They're making an invisible interface.
[01:54:13.920 --> 01:54:20.240]   So it's a chatbot, but it's not like the A word or Siri.
[01:54:20.240 --> 01:54:24.240]   It's not a living demo.
[01:54:24.240 --> 01:54:25.800]   So it was a concept.
[01:54:25.800 --> 01:54:31.440]   But the idea is you wear something and you tap it instead of a wake word.
[01:54:31.440 --> 01:54:35.800]   He demoed it a tad and was like, hey, summarize.
[01:54:35.800 --> 01:54:36.960]   I was just in a meeting.
[01:54:36.960 --> 01:54:37.960]   Summarize it.
[01:54:37.960 --> 01:54:40.600]   Tell me the highlights.
[01:54:40.600 --> 01:54:41.880]   Tell me what's happening.
[01:54:41.880 --> 01:54:42.880]   That's it.
[01:54:42.880 --> 01:54:44.920]   Tell me what's the most important stuff of my day.
[01:54:44.920 --> 01:54:47.400]   I have a terrific executive assistant.
[01:54:47.400 --> 01:54:48.400]   She's great.
[01:54:48.400 --> 01:54:51.440]   I cannot put her in my pocket and travel with her everywhere.
[01:54:51.440 --> 01:54:54.440]   So the idea is this is a living--
[01:54:54.440 --> 01:54:55.440]   Well, but--
[01:54:55.440 --> 01:54:56.440]   It's an EA--
[01:54:56.440 --> 01:54:57.800]   It's a booty notification.
[01:54:57.800 --> 01:55:01.040]   The advancement here is the UI, right?
[01:55:01.040 --> 01:55:02.080]   Right, right, right.
[01:55:02.080 --> 01:55:09.800]   So for a long time, we built a model a couple of years ago showing that smartphones had
[01:55:09.800 --> 01:55:10.800]   plateaued.
[01:55:10.800 --> 01:55:14.000]   And I don't care what Google's Pixel Foldy phone does.
[01:55:14.000 --> 01:55:20.000]   We're on the other side of the hill now.
[01:55:20.000 --> 01:55:22.480]   And I've been thinking that glasses would replace it.
[01:55:22.480 --> 01:55:26.880]   Glasses present a bunch of really challenging technical problems, but it makes sense to
[01:55:26.880 --> 01:55:29.000]   me that that's what would be next.
[01:55:29.000 --> 01:55:30.880]   This introduces a different type of UI.
[01:55:30.880 --> 01:55:33.400]   How does he project this under his hand?
[01:55:33.400 --> 01:55:40.200]   So there's a-- the thing that I saw is like this little-- it's a little sort of oval,
[01:55:40.200 --> 01:55:44.240]   like a black oval that seems to have both a projector and a camera.
[01:55:44.240 --> 01:55:48.640]   So the idea would be the demo that he showed, which again I think was staged, but it was
[01:55:48.640 --> 01:55:51.880]   like wife calling and he knew that that was her because he--
[01:55:51.880 --> 01:55:54.440]   He raised his hand and it says Bethany's call.
[01:55:54.440 --> 01:55:55.440]   Right, he saw who it was.
[01:55:55.440 --> 01:55:56.440]   Yeah.
[01:55:56.440 --> 01:55:57.440]   Right.
[01:55:57.440 --> 01:55:58.440]   It all--
[01:55:58.440 --> 01:56:01.120]   By the way, the difference between raising my hand with a phone in it and not in a phone
[01:56:01.120 --> 01:56:02.480]   in it is kind of the minimus.
[01:56:02.480 --> 01:56:03.480]   I mean--
[01:56:03.480 --> 01:56:05.480]   It's-- I mean, yes and no.
[01:56:05.480 --> 01:56:06.480]   Yes and no.
[01:56:06.480 --> 01:56:09.840]   I think it cuts down distractions.
[01:56:09.840 --> 01:56:15.680]   You know, if I've got a phone in my hand and I'm doing one thing, I have so many other
[01:56:15.680 --> 01:56:17.960]   potential things I can then get into.
[01:56:17.960 --> 01:56:23.400]   This is really more sort of single serving, but I'm at-- the thing that I also thought
[01:56:23.400 --> 01:56:27.520]   was really interesting is the input mechanisms visually.
[01:56:27.520 --> 01:56:31.920]   So imagine being able to hold a food item or something up and, you know, and the thing
[01:56:31.920 --> 01:56:35.680]   knows how many calories I've consumed or how much energy I'm going to need for a rash
[01:56:35.680 --> 01:56:36.680]   and I could tell you with--
[01:56:36.680 --> 01:56:37.680]   Or a rash.
[01:56:37.680 --> 01:56:38.680]   There you go.
[01:56:38.680 --> 01:56:39.680]   That makes me help.
[01:56:39.680 --> 01:56:40.680]   My gosh, should I eat this thing or--
[01:56:40.680 --> 01:56:46.440]   He has $100 million in funding from kindred ventures, including Sam Altman.
[01:56:46.440 --> 01:56:51.560]   Yes, so again, I don't know-- I don't know how this thing scales.
[01:56:51.560 --> 01:56:52.560]   Well, I think--
[01:56:52.560 --> 01:56:58.280]   I'm going to guess what's hoping to get rehired by or purchased by Apple because Apple's got
[01:56:58.280 --> 01:57:04.040]   a big problem, which is they're making a honkin' big thing you wear on your head to try and
[01:57:04.040 --> 01:57:07.040]   make this happen and that is not going anywhere.
[01:57:07.040 --> 01:57:13.640]   Well, I just-- I think that this sort of constellation of new UIs that are coming, again, is the more
[01:57:13.640 --> 01:57:17.080]   interesting piece of the AI conversation than that--
[01:57:17.080 --> 01:57:18.080]   That's funny.
[01:57:18.080 --> 01:57:23.680]   Because I thought voice assistants would be that new UI.
[01:57:23.680 --> 01:57:24.680]   And they've failed us miserably.
[01:57:24.680 --> 01:57:27.480]   You were just yelling a Siri a minute ago.
[01:57:27.480 --> 01:57:31.520]   Yeah, I'm really interested in the new UI stuff.
[01:57:31.520 --> 01:57:37.160]   I know this project I talked with him years ago when he started working on this.
[01:57:37.160 --> 01:57:40.520]   And I think it's potentially-- it's a big move.
[01:57:40.520 --> 01:57:42.360]   I mean, who knows whether it'll work out or not.
[01:57:42.360 --> 01:57:44.520]   But it's a really interesting idea.
[01:57:44.520 --> 01:57:47.480]   The projector, your projecting things on your hand, I actually think it's-- if you can
[01:57:47.480 --> 01:57:49.320]   get that working, that's very cool.
[01:57:49.320 --> 01:57:50.320]   I was like, I have this rule--
[01:57:50.320 --> 01:57:53.840]   It doesn't-- you don't need a ton of tech to make that happen, though, right?
[01:57:53.840 --> 01:57:54.840]   No, it isn't.
[01:57:54.840 --> 01:57:55.840]   If it's not going to be visually pretty.
[01:57:55.840 --> 01:57:58.840]   And if it's talking to your phone or something, anyway, it doesn't fit the process.
[01:57:58.840 --> 01:58:00.840]   You still need a phone, right?
[01:58:00.840 --> 01:58:02.840]   And Microsoft-- no, not necessarily.
[01:58:02.840 --> 01:58:05.040]   I think the idea is no phone.
[01:58:05.040 --> 01:58:11.120]   Microsoft, in what, 2002 or 2008 had skin put early, early days, which was--
[01:58:11.120 --> 01:58:12.120]   remember that?
[01:58:12.120 --> 01:58:16.480]   It was like you could sort of project onto your forearm and interact with it?
[01:58:16.480 --> 01:58:17.480]   Oh, yeah.
[01:58:17.480 --> 01:58:19.840]   This has a Qualcomm Snapdragon.
[01:58:19.840 --> 01:58:22.120]   It's running Android.
[01:58:22.120 --> 01:58:23.120]   Right.
[01:58:23.120 --> 01:58:25.640]   And I think it's meant to be a phoneless device, right?
[01:58:25.640 --> 01:58:26.640]   So you don't need the phone?
[01:58:26.640 --> 01:58:32.040]   It is, but the-- I guess like the devices will continue to shrink, right?
[01:58:32.040 --> 01:58:34.560]   The interesting thing is the interface modality.
[01:58:34.560 --> 01:58:38.320]   So sticking out your hand and having a projector on there and then recognizing gestures is
[01:58:38.320 --> 01:58:40.320]   really cool.
[01:58:40.320 --> 01:58:44.000]   And I agree that probably better than something you're wearing, you're strapping out through
[01:58:44.000 --> 01:58:45.200]   your face.
[01:58:45.200 --> 01:58:46.360]   This was sort of my central thing.
[01:58:46.360 --> 01:58:47.640]   I don't know what Apple's going to release.
[01:58:47.640 --> 01:58:52.720]   I'm actually-- I'm hopeful I'm optimistic I'm a giant Apple fanboy, like everyone knows.
[01:58:52.720 --> 01:58:58.480]   But I've said for a long time that like any tech-- any consumer technology that makes
[01:58:58.480 --> 01:59:01.640]   you look stupid while you're using it is not going to work.
[01:59:01.640 --> 01:59:03.560]   As we learned from the segue.
[01:59:03.560 --> 01:59:04.560]   Exactly.
[01:59:04.560 --> 01:59:10.440]   The segue in Google Glass and all of the Facebook meta nonsense.
[01:59:10.440 --> 01:59:15.440]   I should have seen it coming because like literally, you know, I think-- I think second
[01:59:15.440 --> 01:59:19.520]   board's vision was always that you know, he's going to ask people to strap something
[01:59:19.520 --> 01:59:23.680]   the size of a book onto their face and they called it Facebook.
[01:59:23.680 --> 01:59:25.760]   And like we should have seen it coming from a film.
[01:59:25.760 --> 01:59:26.760]   Do you think--
[01:59:26.760 --> 01:59:28.240]   Telegrafted, that far in advance.
[01:59:28.240 --> 01:59:33.360]   Do you think Mark has given up on VR and meta, the metaverse?
[01:59:33.360 --> 01:59:34.360]   I don't know.
[01:59:34.360 --> 01:59:35.360]   I wouldn't know.
[01:59:35.360 --> 01:59:36.360]   I haven't--
[01:59:36.360 --> 01:59:37.360]   No one knows.
[01:59:37.360 --> 01:59:38.360]   But--
[01:59:38.360 --> 01:59:40.800]   One of their earnings call where he basically said that--
[01:59:40.800 --> 01:59:41.800]   Yeah, we're moving AI now.
[01:59:41.800 --> 01:59:42.800]   We're moving a little bit.
[01:59:42.800 --> 01:59:43.800]   Yeah.
[01:59:43.800 --> 01:59:44.800]   It's all AI now.
[01:59:44.800 --> 01:59:48.360]   I'm interested to see what Apple does with it just because like I bet you'll be super
[01:59:48.360 --> 01:59:49.360]   cool.
[01:59:49.360 --> 01:59:55.280]   I think he was going to be my test for especially for using it in a business application.
[01:59:55.280 --> 01:59:57.840]   This is the live and test.
[01:59:57.840 --> 02:00:02.240]   I am not going to use any device to have a meeting or for work if I cannot drink a hot
[02:00:02.240 --> 02:00:04.080]   cup of coffee while wearing it.
[02:00:04.080 --> 02:00:08.080]   If I literally can't do this because it like hits the thing and I spill hot coffee in myself.
[02:00:08.080 --> 02:00:09.080]   Is that an Ember mug?
[02:00:09.080 --> 02:00:12.840]   Is that one of them USB heated mugs you got there?
[02:00:12.840 --> 02:00:13.840]   Or is this a regular mug?
[02:00:13.840 --> 02:00:14.840]   It's a menu.
[02:00:14.840 --> 02:00:15.840]   But this is a new--
[02:00:15.840 --> 02:00:17.840]   You have like five-- what are you drinking over there?
[02:00:17.840 --> 02:00:18.840]   You've noticed.
[02:00:18.840 --> 02:00:19.840]   I've had.
[02:00:19.840 --> 02:00:22.040]   I'm drinking tea.
[02:00:22.040 --> 02:00:24.160]   But it's a really long show.
[02:00:24.160 --> 02:00:25.160]   So I just set myself up.
[02:00:25.160 --> 02:00:30.200]   I'm drinking Earl Grey with lemon and a little bit of honey because I have my voices a little
[02:00:30.200 --> 02:00:31.200]   bit sore.
[02:00:31.200 --> 02:00:32.760]   I can't do the burger mug.
[02:00:32.760 --> 02:00:33.760]   So--
[02:00:33.760 --> 02:00:34.760]   Can't do the burger mug.
[02:00:34.760 --> 02:00:37.480]   So basically when Apple releases their thing, that's just the test.
[02:00:37.480 --> 02:00:40.680]   Like, can you drink a hot cup of coffee?
[02:00:40.680 --> 02:00:45.800]   If you can't, then it's a hard no for any kind of business usage.
[02:00:45.800 --> 02:00:51.480]   Honestly, I feel like Apple and it's got too much momentum to stop.
[02:00:51.480 --> 02:00:55.080]   But I think they've made a horrible mistake.
[02:00:55.080 --> 02:00:56.400]   I'm reserving judgment until I see it.
[02:00:56.400 --> 02:00:57.400]   I'm excited to see it.
[02:00:57.400 --> 02:00:58.400]   I love Apple stuff.
[02:00:58.400 --> 02:00:59.400]   Right.
[02:00:59.400 --> 02:01:00.400]   We'll see.
[02:01:00.400 --> 02:01:04.080]   You don't think what's coming are just a pair of old-school analog looking glasses that
[02:01:04.080 --> 02:01:05.080]   have--
[02:01:05.080 --> 02:01:07.080]   Someday, not in June.
[02:01:07.080 --> 02:01:08.080]   I hope so.
[02:01:08.080 --> 02:01:09.080]   They're going to get something--
[02:01:09.080 --> 02:01:11.920]   In June that is going to look just like the Oculus Rift.
[02:01:11.920 --> 02:01:16.320]   Yeah, if they can release something that's just glasses that I'm actually looking at the
[02:01:16.320 --> 02:01:17.520]   world through clear lenses.
[02:01:17.520 --> 02:01:18.520]   Yeah, that would be awesome.
[02:01:18.520 --> 02:01:19.520]   We all agree.
[02:01:19.520 --> 02:01:20.520]   I'll be amazing.
[02:01:20.520 --> 02:01:21.520]   I'm all over that.
[02:01:21.520 --> 02:01:22.520]   Well, it can.
[02:01:22.520 --> 02:01:28.440]   I'm married to an eye doctor who repeatedly tells me every day why that can't happen.
[02:01:28.440 --> 02:01:35.120]   Well, you wrote in a treatment for a wonderful show that unfortunately got canceled.
[02:01:35.120 --> 02:01:38.040]   People had these glasses and they were sending stuff.
[02:01:38.040 --> 02:01:39.040]   You helped them with the--
[02:01:39.040 --> 02:01:40.040]   Oh, like--
[02:01:40.040 --> 02:01:41.040]   Right?
[02:01:41.040 --> 02:01:42.440]   It was called the first.
[02:01:42.440 --> 02:01:43.440]   I really like that.
[02:01:43.440 --> 02:01:45.240]   It was a lose first streamer.
[02:01:45.240 --> 02:01:46.240]   Yeah.
[02:01:46.240 --> 02:01:48.640]   Our first Bingey streamer.
[02:01:48.640 --> 02:01:52.760]   There is a challenge trying to get the--
[02:01:52.760 --> 02:01:55.440]   Your eye doctor guy says, no, can't happen.
[02:01:55.440 --> 02:01:56.440]   Not going to happen.
[02:01:56.440 --> 02:02:01.200]   You know, I don't know why, but I, for some reason, was like, I want to read some of those
[02:02:01.200 --> 02:02:03.280]   original magic league patents again.
[02:02:03.280 --> 02:02:06.240]   So that was my version of fun last week.
[02:02:06.240 --> 02:02:07.240]   I pulled a couple.
[02:02:07.240 --> 02:02:08.240]   Yeah.
[02:02:08.240 --> 02:02:11.440]   They were just-- they were so far ahead.
[02:02:11.440 --> 02:02:13.120]   Yeah, but they couldn't make it.
[02:02:13.120 --> 02:02:15.880]   They just needed-- they needed 10 years.
[02:02:15.880 --> 02:02:19.640]   It was one of these things where it was like somebody had very interesting ideas.
[02:02:19.640 --> 02:02:24.440]   This was basic research and they just needed to put their heads down for a decade or 15
[02:02:24.440 --> 02:02:26.360]   years and just like work.
[02:02:26.360 --> 02:02:27.360]   We talked about that.
[02:02:27.360 --> 02:02:28.360]   And the market didn't let them do that.
[02:02:28.360 --> 02:02:29.840]   You were on the show at one point.
[02:02:29.840 --> 02:02:30.840]   Yeah.
[02:02:30.840 --> 02:02:34.640]   Yeah, it was the problem is venture capital is it going to give you 10 years to do anything.
[02:02:34.640 --> 02:02:35.640]   Yeah.
[02:02:35.640 --> 02:02:36.640]   Fascinating.
[02:02:36.640 --> 02:02:40.080]   So I'm going to read immediately whatever Apple releases are on its headset.
[02:02:40.080 --> 02:02:42.920]   I'm going to have you on June 12.
[02:02:42.920 --> 02:02:44.480]   We'll have you on the show.
[02:02:44.480 --> 02:02:48.320]   Because I didn't find it.
[02:02:48.320 --> 02:02:51.120]   If I can't drink coffee, I'm not going to be in a meeting.
[02:02:51.120 --> 02:02:56.320]   So if wearing this means that I can choose between seeing 3D representations of my coworkers
[02:02:56.320 --> 02:02:57.680]   and having coffee, that is natural.
[02:02:57.680 --> 02:02:58.680]   Apple's big innovation.
[02:02:58.680 --> 02:02:59.920]   Well, they'll have legs.
[02:02:59.920 --> 02:03:00.920]   And have legs.
[02:03:00.920 --> 02:03:01.920]   Yeah.
[02:03:01.920 --> 02:03:02.920]   Apple needs to do something like this.
[02:03:02.920 --> 02:03:04.720]   Maybe like a sippy cup as well.
[02:03:04.720 --> 02:03:07.040]   Maybe they can come up with a sippy cup.
[02:03:07.040 --> 02:03:14.000]   Well, you know, you could just have one of those backpacks with a little thing just attached
[02:03:14.000 --> 02:03:15.000]   and then you'd be all set.
[02:03:15.000 --> 02:03:16.000]   Camelback.
[02:03:16.000 --> 02:03:17.000]   Camelback.
[02:03:17.000 --> 02:03:18.000]   If Apple releases it because I'm not going third party.
[02:03:18.000 --> 02:03:19.000]   You need an apple camel.
[02:03:19.000 --> 02:03:20.000]   Yeah.
[02:03:20.000 --> 02:03:21.000]   They need an apple camel.
[02:03:21.000 --> 02:03:22.000]   Yeah.
[02:03:22.000 --> 02:03:31.200]   There is a kind of bizarre TV show on free v. Amazon Prime's free ad supported channel
[02:03:31.200 --> 02:03:32.360]   called DREEDDuty.
[02:03:32.360 --> 02:03:35.080]   Oh, I've heard about that.
[02:03:35.080 --> 02:03:36.080]   It's interesting.
[02:03:36.080 --> 02:03:37.080]   It's hilarious.
[02:03:37.080 --> 02:03:40.160]   One guy is not in on the joke.
[02:03:40.160 --> 02:03:43.000]   He thinks he's on a regular jury and a regular trial.
[02:03:43.000 --> 02:03:47.640]   Everybody else, the judge, the bailiff and all the other jurors are actors, including
[02:03:47.640 --> 02:03:50.240]   James Marsden as himself.
[02:03:50.240 --> 02:03:56.880]   But there's one guy who is a, what do they call those guys who are trying to do body modifications
[02:03:56.880 --> 02:03:58.800]   for the future?
[02:03:58.800 --> 02:04:02.560]   There's one of the jurors is that kind of guy and he's wearing a Camelback.
[02:04:02.560 --> 02:04:07.560]   He said, but the ideal would be have some sort of bladder that you insert into the body
[02:04:07.560 --> 02:04:12.560]   that contains all the water and he is another one that has all the nutrition he needs.
[02:04:12.560 --> 02:04:13.760]   He's got any way.
[02:04:13.760 --> 02:04:15.360]   Yeah, it's pretty funny.
[02:04:15.360 --> 02:04:20.000]   The joke doesn't make it through all eight episodes or whatever, but it's the first few
[02:04:20.000 --> 02:04:21.000]   are humorous.
[02:04:21.000 --> 02:04:24.320]   Since we very slightly sidetracked, did you guys start silo yet?
[02:04:24.320 --> 02:04:25.680]   Yeah, I love it.
[02:04:25.680 --> 02:04:26.680]   I hadn't read the books.
[02:04:26.680 --> 02:04:28.320]   Did you read the books?
[02:04:28.320 --> 02:04:31.200]   A friend of mine read the books and said that he works at AWS.
[02:04:31.200 --> 02:04:32.200]   He's a super nerd.
[02:04:32.200 --> 02:04:34.120]   I said the books were amazing.
[02:04:34.120 --> 02:04:35.120]   He loved the books.
[02:04:35.120 --> 02:04:37.400]   I feel like I should have read the books.
[02:04:37.400 --> 02:04:40.320]   Now I'm watching the show is going to spoil the books and the books are always better,
[02:04:40.320 --> 02:04:44.200]   in my opinion, than any motion picture TV show.
[02:04:44.200 --> 02:04:45.200]   But it's pretty good.
[02:04:45.200 --> 02:04:46.800]   I missed the most recent episode.
[02:04:46.800 --> 02:04:47.800]   I got to watch it.
[02:04:47.800 --> 02:04:50.280]   Apparently it takes quite a turn.
[02:04:50.280 --> 02:04:51.280]   That's on Apple, right?
[02:04:51.280 --> 02:04:52.280]   Yeah.
[02:04:52.280 --> 02:04:53.280]   Yeah.
[02:04:53.280 --> 02:04:54.280]   I think Apple's struggling with their content.
[02:04:54.280 --> 02:04:56.040]   They're studio content.
[02:04:56.040 --> 02:04:57.280]   Yeah, it's interesting.
[02:04:57.280 --> 02:05:01.720]   For my point of view, there's the new, the big whatever machine I tried a little bit of
[02:05:01.720 --> 02:05:02.720]   that.
[02:05:02.720 --> 02:05:05.200]   I think that there's this formula they're following.
[02:05:05.200 --> 02:05:07.600]   They have the world's largest catalog of music.
[02:05:07.600 --> 02:05:09.400]   There's a lot of interstitials that I think are.
[02:05:09.400 --> 02:05:11.040]   A lot of music at all of their shows.
[02:05:11.040 --> 02:05:12.040]   You can tell they're singing.
[02:05:12.040 --> 02:05:14.040]   Yeah, and it's just like...
[02:05:14.040 --> 02:05:15.200]   There's some of the soundtracks.
[02:05:15.200 --> 02:05:17.920]   It's the Zelda problem.
[02:05:17.920 --> 02:05:21.880]   It's all formula, not enough human.
[02:05:21.880 --> 02:05:24.320]   That's what HBO did so well in the early days.
[02:05:24.320 --> 02:05:26.760]   I despair of the future for HBO.
[02:05:26.760 --> 02:05:28.440]   But in the early days, they were famous.
[02:05:28.440 --> 02:05:30.960]   They would say, "We're giving you money.
[02:05:30.960 --> 02:05:32.200]   We're not going to give you notes.
[02:05:32.200 --> 02:05:34.000]   Just do what you want to do."
[02:05:34.000 --> 02:05:35.000]   And they got the sopranos.
[02:05:35.000 --> 02:05:37.080]   I mean, amazing stuff.
[02:05:37.080 --> 02:05:39.400]   I have to say succession.
[02:05:39.400 --> 02:05:41.880]   Now you work in business.
[02:05:41.880 --> 02:05:44.480]   I think succession is the best TV show I've ever seen.
[02:05:44.480 --> 02:05:52.760]   I have firsthand experience with a lot of those storylines and the people who were...
[02:05:52.760 --> 02:05:54.440]   And I will tell you that...
[02:05:54.440 --> 02:05:55.440]   Exactly.
[02:05:55.440 --> 02:06:01.600]   I think I'm wondering, actually, if there was somebody on the inside leaking things.
[02:06:01.600 --> 02:06:08.480]   They apparently did have some Murdoch info.
[02:06:08.480 --> 02:06:09.960]   I think there had to be.
[02:06:09.960 --> 02:06:10.960]   Yeah.
[02:06:10.960 --> 02:06:17.280]   But I wish I had time to play games.
[02:06:17.280 --> 02:06:21.440]   At most, I've got 45 minutes of free time occasionally or a half hour.
[02:06:21.440 --> 02:06:22.440]   And I just...
[02:06:22.440 --> 02:06:23.440]   I can't get into Zelda for...
[02:06:23.440 --> 02:06:24.440]   You have to release...
[02:06:24.440 --> 02:06:26.440]   I need like a block of time.
[02:06:26.440 --> 02:06:32.440]   You have to ration your amusements because you have important duties.
[02:06:32.440 --> 02:06:33.440]   I do.
[02:06:33.440 --> 02:06:34.440]   I need a new...
[02:06:34.440 --> 02:06:35.440]   You know what might help.
[02:06:35.440 --> 02:06:36.440]   I just want to show you something.
[02:06:36.440 --> 02:06:38.440]   One of our sponsors is TryHelp.
[02:06:38.440 --> 02:06:40.440]   Our show today brought to you by Miro.
[02:06:40.440 --> 02:06:44.440]   We've been playing with this a little bit and I am very impressed with it.
[02:06:44.440 --> 02:06:45.440]   But I'm challenged.
[02:06:45.440 --> 02:06:50.440]   I don't know exactly how to describe what Miro is.
[02:06:50.440 --> 02:06:52.440]   If you have a team, you're working on something.
[02:06:52.440 --> 02:06:53.440]   Maybe the team's distributed.
[02:06:53.440 --> 02:06:55.440]   Some are in Bentonville, Arkansas.
[02:06:55.440 --> 02:06:57.440]   Some are in Rome.
[02:06:57.440 --> 02:06:59.440]   You've got multiple tools.
[02:06:59.440 --> 02:07:03.440]   You know, you're using a design tool and a typing tool.
[02:07:03.440 --> 02:07:04.440]   Maybe some AI.
[02:07:04.440 --> 02:07:10.440]   And all these brilliant ideas, they're kind of getting spread out and losing information along the way.
[02:07:10.440 --> 02:07:19.440]   Miro, the whole idea of Miro is it's a collaborative visual platform that brings all the great work together no matter where you are.
[02:07:19.440 --> 02:07:25.440]   Whether you're working from home in a hybrid workspace, your team might be in different time zones.
[02:07:25.440 --> 02:07:28.440]   It's got it all in one place.
[02:07:28.440 --> 02:07:31.440]   One single source of truth that is so flexible.
[02:07:31.440 --> 02:07:34.440]   It can be anything you need.
[02:07:34.440 --> 02:07:37.440]   It democratizes collaboration and input.
[02:07:37.440 --> 02:07:39.440]   It's great for Zoom meetings.
[02:07:39.440 --> 02:07:41.440]   They actually have a timer and ice breakers, all sorts of tools.
[02:07:41.440 --> 02:07:43.440]   They're infinite shared boards.
[02:07:43.440 --> 02:07:48.440]   Give product teams a perpetual space where they can drag and drop insights and date another day.
[02:07:48.440 --> 02:07:49.440]   Insights and data.
[02:07:49.440 --> 02:07:50.440]   Nothing's lost.
[02:07:50.440 --> 02:07:52.440]   You can zoom in or zoom out.
[02:07:52.440 --> 02:07:54.440]   They cover so many use cases.
[02:07:54.440 --> 02:07:55.440]   You can build visual assets.
[02:07:55.440 --> 02:08:06.440]   You could present findings or on brainstorms with cross functional teams, can ban boards, voting tools, live reactions, sticky notes, even a timer.
[02:08:06.440 --> 02:08:08.440]   You can express yourself.
[02:08:08.440 --> 02:08:12.440]   It's so hard for me to describe what Miro does.
[02:08:12.440 --> 02:08:17.440]   Maybe the best thing to do is to go to Miro.com/podcast.
[02:08:17.440 --> 02:08:18.440]   Sign up.
[02:08:18.440 --> 02:08:20.440]   Your first three boards are free.
[02:08:20.440 --> 02:08:27.440]   Then check out the Miroverse because this is what people who are using Miro have done with it.
[02:08:27.440 --> 02:08:30.440]   It will give you a very good idea.
[02:08:30.440 --> 02:08:33.440]   You don't even have to, you know, you could just look at it.
[02:08:33.440 --> 02:08:36.440]   Here's a Harry Potter retrospective from the UK government.
[02:08:36.440 --> 02:08:40.440]   I don't quite understand that.
[02:08:40.440 --> 02:08:43.440]   TikTok marketing strategy.
[02:08:43.440 --> 02:08:44.440]   Team mapping.
[02:08:44.440 --> 02:08:46.440]   Product vision.
[02:08:46.440 --> 02:08:48.440]   Midnight sailboat retrospective.
[02:08:48.440 --> 02:08:50.440]   It goes on and on.
[02:08:50.440 --> 02:08:52.440]   Oh, here's one for Mother's Day.
[02:08:52.440 --> 02:08:53.440]   But this is the point of it.
[02:08:53.440 --> 02:08:57.440]   There's even games that people have designed in Miro.
[02:08:57.440 --> 02:09:00.440]   Miro can be whatever you need it to be.
[02:09:00.440 --> 02:09:03.440]   It integrates with dozens of other third party tools.
[02:09:03.440 --> 02:09:07.440]   We're using it with Zapier because we use Zapier to trigger stuff.
[02:09:07.440 --> 02:09:10.440]   We use it with Google Docs because we use a lot of Google Docs.
[02:09:10.440 --> 02:09:16.440]   Miro users report saving up to 80 hours per user per year by streamlining conversations,
[02:09:16.440 --> 02:09:18.440]   by cutting down meetings.
[02:09:18.440 --> 02:09:23.440]   But more importantly than the time saved, the product you create is that much better because
[02:09:23.440 --> 02:09:29.440]   your team is staying connected to real time information, a single source of truth.
[02:09:29.440 --> 02:09:33.440]   It gives project managers and product leads a bird's eye view of the whole project.
[02:09:33.440 --> 02:09:35.440]   So nothing slips through the crack.
[02:09:35.440 --> 02:09:38.440]   Nothing I say will really communicate what this can do for you.
[02:09:38.440 --> 02:09:39.440]   You just got to try it.
[02:09:39.440 --> 02:09:42.440]   So get your first three boards absolutely free.
[02:09:42.440 --> 02:09:52.440]   You start working better at MiroMiro.com/podcastmiro.com/podcast.
[02:09:52.440 --> 02:09:57.440]   And the whole world is available for you.
[02:09:57.440 --> 02:09:59.440]   You can do an amazing thing with Miro.
[02:09:59.440 --> 02:10:03.440]   We had an amazing week this week at Twit.
[02:10:03.440 --> 02:10:06.440]   We've made a little movie for you.
[02:10:06.440 --> 02:10:07.440]   Enjoy.
[02:10:07.440 --> 02:10:11.440]   You, Jeez, are such a fun and simple way of expressing yourself.
[02:10:11.440 --> 02:10:15.440]   So we thought, wouldn't it be cool to bring them to your wallpaper?
[02:10:15.440 --> 02:10:16.440]   No, no.
[02:10:16.440 --> 02:10:17.440]   So would have loved you all paper.
[02:10:17.440 --> 02:10:18.440]   It's YouTube.
[02:10:18.440 --> 02:10:19.440]   Oh, this is awful.
[02:10:19.440 --> 02:10:20.440]   This is awful.
[02:10:20.440 --> 02:10:21.440]   This is awful.
[02:10:21.440 --> 02:10:22.440]   This is awful.
[02:10:22.440 --> 02:10:25.440]   You get one of these a year, Google.
[02:10:25.440 --> 02:10:27.440]   Previously on Twit.
[02:10:27.440 --> 02:10:28.440]   Twit news.
[02:10:28.440 --> 02:10:31.440]   We're sitting down with Dave Burke, VP of Engineering,
[02:10:31.440 --> 02:10:36.440]   and Andrew and Samir Samat, VP of Product Management.
[02:10:36.440 --> 02:10:39.440]   I think what we wanted to do today was to showcase not just the technology,
[02:10:39.440 --> 02:10:44.440]   but also how the technology comes into each and every one of those products that we all use.
[02:10:44.440 --> 02:10:45.440]   And what can it do for me?
[02:10:45.440 --> 02:10:50.440]   Actually, the hard part in my view is like, how do you make this approach of all, right?
[02:10:50.440 --> 02:10:52.440]   And relatable and fit for purpose.
[02:10:52.440 --> 02:10:54.440]   Mac break weekly.
[02:10:54.440 --> 02:11:00.440]   Apple is bringing Final Cut Pro and Logic Pro to the iPad.
[02:11:00.440 --> 02:11:02.440]   You will have to buy it anew.
[02:11:02.440 --> 02:11:05.440]   In fact, it's a subscription this week in space.
[02:11:05.440 --> 02:11:09.440]   Strange sounds recorded high in Earth's atmosphere from space.com.
[02:11:09.440 --> 02:11:10.440]   Tell us.
[02:11:10.440 --> 02:11:19.440]   Yeah, the San DIA National Labs scientists, they launched a solar powered balloon up like 31 miles above our planet.
[02:11:19.440 --> 02:11:27.440]   And what they found is this really, really strange series of sounds that repeat every few hours
[02:11:27.440 --> 02:11:30.440]   and no one knows where it comes from.
[02:11:30.440 --> 02:11:32.440]   That's clearly aliens, right, Rod?
[02:11:32.440 --> 02:11:34.440]   Okay, wait, let me see if I can reproduce this.
[02:11:34.440 --> 02:11:35.440]   Hold on a second.
[02:11:35.440 --> 02:11:36.440]   How's that sound?
[02:11:36.440 --> 02:11:40.440]   It's probably my beard against the microphone.
[02:11:40.440 --> 02:11:42.440]   It's a microphone.
[02:11:42.440 --> 02:11:48.440]   Actually, it reminds me that for about 15 years, there were audible click sounds in radio broadcasts and other things.
[02:11:48.440 --> 02:11:56.440]   Turns out it was some Russian giant Russian antenna that they were used to spy on people.
[02:11:56.440 --> 02:11:58.440]   It was causing radio interference.
[02:11:58.440 --> 02:11:59.440]   It wasn't aliens.
[02:11:59.440 --> 02:12:03.440]   I think the odds are it's probably not aliens.
[02:12:03.440 --> 02:12:08.440]   I hope you enjoyed this week and I hope you will continue to watch all of our shows.
[02:12:08.440 --> 02:12:10.440]   Of course, our club Twit members get special access.
[02:12:10.440 --> 02:12:17.440]   No ads, no promos, no spy, no, you know, ad monitoring technologies.
[02:12:17.440 --> 02:12:20.440]   Just the shows all clean, pure.
[02:12:20.440 --> 02:12:21.440]   You know what we should do?
[02:12:21.440 --> 02:12:24.440]   We should leave the swear words in for the club members.
[02:12:24.440 --> 02:12:27.440]   So they can hear what it really sounded like.
[02:12:27.440 --> 02:12:29.440]   Club Twit is $7 a month.
[02:12:29.440 --> 02:12:30.440]   Ad-free versions of all the shows.
[02:12:30.440 --> 02:12:32.440]   Special programming you don't put it on anywhere else.
[02:12:32.440 --> 02:12:34.440]   That's how this week in space got started.
[02:12:34.440 --> 02:12:39.440]   Home theater geeks is right now in the club and it's, you know, it's birth pangs.
[02:12:39.440 --> 02:12:43.440]   We've got the hands-on Macintosh with Micah, hands-on windows with Paul.
[02:12:43.440 --> 02:12:45.440]   We've got an untitled Linux show, all sorts of stuff.
[02:12:45.440 --> 02:12:47.440]   The great club Twit discord.
[02:12:47.440 --> 02:12:49.440]   I think it's the best seven bucks you'll ever spend.
[02:12:49.440 --> 02:12:55.440]   And it really helps us do, I think, more of what we want to do for you.
[02:12:55.440 --> 02:12:57.440]   You know, ad support's nice.
[02:12:57.440 --> 02:12:59.440]   Listener supports even better.
[02:12:59.440 --> 02:13:02.440]   Twit.tv/glovetwit.
[02:13:02.440 --> 02:13:08.440]   There is a version of the show from Alia G with all the swear words.
[02:13:08.440 --> 02:13:09.440]   I'm an imammer.
[02:13:09.440 --> 02:13:10.440]   You are.
[02:13:10.440 --> 02:13:11.440]   You hang out there once in a while.
[02:13:11.440 --> 02:13:12.440]   I love seeing you in there.
[02:13:12.440 --> 02:13:13.440]   It's wonderful.
[02:13:13.440 --> 02:13:16.440]   You know, I would love to get all the smart.
[02:13:16.440 --> 02:13:19.440]   In fact, Phil, I'll send you a complimentary membership too.
[02:13:19.440 --> 02:13:22.440]   But I'd love to get all those smart people in here.
[02:13:22.440 --> 02:13:26.040]   It's fun because there's something about a social network where people have paid seven
[02:13:26.040 --> 02:13:27.040]   bucks to be there.
[02:13:27.040 --> 02:13:28.840]   They just have an investment in it.
[02:13:28.840 --> 02:13:31.440]   It's like, you know, a private park.
[02:13:31.440 --> 02:13:34.440]   You don't get the messing around stuff as much.
[02:13:34.440 --> 02:13:36.440]   It's really great.
[02:13:36.440 --> 02:13:40.440]   AM radio is on its last legs, wash and post.
[02:13:40.440 --> 02:13:44.440]   This broke my heart because I'm a long time radio guy.
[02:13:44.440 --> 02:13:45.440]   End of a love affair.
[02:13:45.440 --> 02:13:47.440]   AM radio being removed from many cars.
[02:13:47.440 --> 02:13:51.440]   Ford, the MW Volkswagen, Tesla.
[02:13:51.440 --> 02:13:56.440]   They kind of have the excuse well, the electric motors interfere with the AM.
[02:13:56.440 --> 02:14:00.440]   But really it's, you know, why put it in if people are going to be using their smart phones?
[02:14:00.440 --> 02:14:05.440]   It does raise an issue because AM radio is used for safety announcements, emergency announcements.
[02:14:05.440 --> 02:14:10.440]   And there are plenty of places where you can't get a cell phone signal.
[02:14:10.440 --> 02:14:18.440]   So I imagine there'll be quite a bit of lobbying from broadcasters to keep it alive.
[02:14:18.440 --> 02:14:21.440]   Do you guys ever listen to the radio, Amy?
[02:14:21.440 --> 02:14:28.440]   I, yes, I listen to college usually whenever I'm in a new place, I listen to a college radio station.
[02:14:28.440 --> 02:14:29.440]   Much better, isn't it?
[02:14:29.440 --> 02:14:30.440]   Yeah.
[02:14:30.440 --> 02:14:31.440]   Good.
[02:14:31.440 --> 02:14:33.440]   Like I get good, you know, introduced to new music that way.
[02:14:33.440 --> 02:14:38.440]   But I was actually on an AM radio show, trying to remember the name of it.
[02:14:38.440 --> 02:14:40.440]   It was a national show in the middle of the night.
[02:14:40.440 --> 02:14:42.440]   Oh, our, yeah, coast to coast.
[02:14:42.440 --> 02:14:43.440]   Coast to coast.
[02:14:43.440 --> 02:14:44.440]   Coast to coast.
[02:14:44.440 --> 02:14:47.440]   Our bells old show now of George Norrie hosts it.
[02:14:47.440 --> 02:14:48.440]   Yep.
[02:14:48.440 --> 02:14:49.440]   I was on coast to coast.
[02:14:49.440 --> 02:14:51.440]   Do you ask you about aliens?
[02:14:51.440 --> 02:14:53.440]   It was mostly calling people.
[02:14:53.440 --> 02:14:58.440]   So I was on from like 2 AM to 4 AM or something when I was launching the last book.
[02:14:58.440 --> 02:15:00.440]   And that's quite a booking.
[02:15:00.440 --> 02:15:01.440]   Wow.
[02:15:01.440 --> 02:15:03.440]   Well, here's what it was.
[02:15:03.440 --> 02:15:08.440]   It was a lot of people who were really worried about vaccines.
[02:15:08.440 --> 02:15:09.440]   Yeah.
[02:15:09.440 --> 02:15:13.440]   And they listen, if you're up in the middle of the night, you're worried about something.
[02:15:13.440 --> 02:15:18.440]   You know, you're, you're probably anxious and it occurred to me.
[02:15:18.440 --> 02:15:20.440]   So Zelda comes into play.
[02:15:20.440 --> 02:15:29.440]   So listening to everybody, it occurred to me that they all must feel like everybody was really talking down to them and just treating them like they were dumb.
[02:15:29.440 --> 02:15:32.440]   And I think when you're genuinely anxious and afraid.
[02:15:32.440 --> 02:15:34.440]   That is not the right response.
[02:15:34.440 --> 02:15:39.440]   So I was like, actually, one guy who had called in, I was like, do you ever play Zelda?
[02:15:39.440 --> 02:15:40.440]   And he was like, yeah, I play Zelda.
[02:15:40.440 --> 02:15:42.440]   And I was like, okay.
[02:15:42.440 --> 02:15:53.440]   Um, this is, COVID is a little, and the mRNA vaccines are a little bit like, you know, you're out roaming around Hyrule and you meet, what are those blobs?
[02:15:53.440 --> 02:15:59.440]   I can't remember the name of them, but the blob comes by and like the first time you see the blob, you don't know, is this food?
[02:15:59.440 --> 02:16:00.440]   Is this a good blob?
[02:16:00.440 --> 02:16:01.440]   Is it a bad blob?
[02:16:01.440 --> 02:16:03.440]   Maybe, you know, hard to tell.
[02:16:03.440 --> 02:16:07.440]   And since you can't recognize what it is, you don't know what to do with it.
[02:16:07.440 --> 02:16:09.440]   You're missing the instructions.
[02:16:09.440 --> 02:16:19.440]   Once you have the instructions, like, oh, that blob is actually a bad thing and it's going to kill you, you know, which weapon and your quiver to pull out and to use to defend yourself against it.
[02:16:19.440 --> 02:16:23.440]   That is what the messenger RNA vaccine does for you.
[02:16:23.440 --> 02:16:25.440]   They're actually called blobs, by the way.
[02:16:25.440 --> 02:16:26.440]   That's the official.
[02:16:26.440 --> 02:16:35.440]   Yeah, so I mean, it was like, and we just had this moment where he, I heard he was like, nobody ever, why don't they just tell us that?
[02:16:35.440 --> 02:16:36.440]   Yeah.
[02:16:36.440 --> 02:16:37.440]   I don't know.
[02:16:37.440 --> 02:16:43.440]   And it just, it, um, so I mostly just listened to people being really worried and telling them like, your fears are totally valid.
[02:16:43.440 --> 02:16:45.440]   Like, you know, like, I get it.
[02:16:45.440 --> 02:16:47.440]   This is hard stuff.
[02:16:47.440 --> 02:16:49.440]   Um, so it was very good.
[02:16:49.440 --> 02:16:50.440]   That.
[02:16:50.440 --> 02:16:51.440]   Yeah.
[02:16:51.440 --> 02:16:53.440]   That's, I guess that's true.
[02:16:53.440 --> 02:16:58.440]   I guess that's true.
[02:16:58.440 --> 02:16:59.440]   If AM radio dies, it's because broadcasters have killed it, basically.
[02:16:59.440 --> 02:17:00.440]   Right.
[02:17:00.440 --> 02:17:02.440]   I'm surprised that it's AM in not FM, right?
[02:17:02.440 --> 02:17:06.440]   Because like, you would think that like, AMX is more useful.
[02:17:06.440 --> 02:17:08.440]   But I would have expected it to go first.
[02:17:08.440 --> 02:17:09.440]   Yeah.
[02:17:09.440 --> 02:17:11.440]   It's like, we don't need radio for like higher quality.
[02:17:11.440 --> 02:17:15.440]   And AM gives you the range and it's like, an information and it works everywhere.
[02:17:15.440 --> 02:17:18.440]   In the apocalypse, you'd rather, you'd rather have a good game.
[02:17:18.440 --> 02:17:19.440]   That's a good point.
[02:17:19.440 --> 02:17:20.440]   Why aren't they taking out?
[02:17:20.440 --> 02:17:23.440]   I think it's a good point.
[02:17:23.440 --> 02:17:24.440]   Yeah.
[02:17:24.440 --> 02:17:25.440]   I think it's a good point.
[02:17:25.440 --> 02:17:26.440]   I think it's a good point.
[02:17:26.440 --> 02:17:27.440]   Yeah.
[02:17:27.440 --> 02:17:28.440]   I think it's a good point.
[02:17:28.440 --> 02:17:29.440]   I think it's a good point.
[02:17:29.440 --> 02:17:30.440]   I think it's a good point.
[02:17:30.440 --> 02:17:31.440]   I think it's a good point.
[02:17:31.440 --> 02:17:32.440]   I think it's a good point.
[02:17:32.440 --> 02:17:33.440]   I think it's a good point.
[02:17:33.440 --> 02:17:34.440]   I think it's a good point.
[02:17:34.440 --> 02:17:35.440]   I think it's a good point.
[02:17:35.440 --> 02:17:36.440]   I think it's a good point.
[02:17:36.440 --> 02:17:37.440]   I think it's a good point.
[02:17:37.440 --> 02:17:38.440]   I think it's a good point.
[02:17:38.440 --> 02:17:39.440]   I think it's a good point.
[02:17:39.440 --> 02:17:40.440]   I think it's a good point.
[02:17:40.440 --> 02:17:41.440]   I think it's a good point.
[02:17:41.440 --> 02:17:42.440]   I think it's a good point.
[02:17:42.440 --> 02:17:43.440]   I think it's a good point.
[02:17:43.440 --> 02:17:44.440]   I think it's a good point.
[02:17:44.440 --> 02:17:45.440]   I think it's a good point.
[02:17:45.440 --> 02:17:50.440]   I think it's a good point.
[02:17:50.440 --> 02:17:57.440]   I think it's a good point.
[02:17:57.440 --> 02:18:00.440]   I think it's a good point.
[02:18:00.440 --> 02:18:03.440]   I think it's a good point.
[02:18:03.440 --> 02:18:06.440]   I think it's a good point.
[02:18:06.440 --> 02:18:09.440]   I think it's a good point.
[02:18:09.440 --> 02:18:10.440]   I think it's a good point.
[02:18:10.440 --> 02:18:11.440]   I think it's a good point.
[02:18:11.440 --> 02:18:12.440]   I think it's a good point.
[02:18:12.440 --> 02:18:13.440]   I think it's a good point.
[02:18:13.440 --> 02:18:15.440]   I think it's a good point.
[02:18:15.440 --> 02:18:16.440]   I think it's a good point.
[02:18:16.440 --> 02:18:17.440]   I think it's a good point.
[02:18:17.440 --> 02:18:19.440]   I think it's a good point.
[02:18:19.440 --> 02:18:20.440]   I think it's a good point.
[02:18:20.440 --> 02:18:21.440]   I think it's a good point.
[02:18:21.440 --> 02:18:22.440]   I think it's a good point.
[02:18:22.440 --> 02:18:23.440]   I think it's a good point.
[02:18:23.440 --> 02:18:24.440]   I think it's a good point.
[02:18:24.440 --> 02:18:25.440]   I think it's a good point.
[02:18:25.440 --> 02:18:26.440]   I think it's a good point.
[02:18:26.440 --> 02:18:27.440]   I think it's a good point.
[02:18:27.440 --> 02:18:28.440]   I think it's a good point.
[02:18:28.440 --> 02:18:29.440]   I think it's a good point.
[02:18:29.440 --> 02:18:30.440]   I think it's a good point.
[02:18:30.440 --> 02:18:31.440]   I think it's a good point.
[02:18:31.440 --> 02:18:32.440]   I think it's a good point.
[02:18:32.440 --> 02:18:33.440]   I think it's a good point.
[02:18:33.440 --> 02:18:34.440]   I think it's a good point.
[02:18:34.440 --> 02:18:35.440]   I think it's a good point.
[02:18:35.440 --> 02:18:36.440]   I think it's a good point.
[02:18:36.440 --> 02:18:37.440]   I think it's a good point.
[02:18:37.440 --> 02:18:38.440]   I think it's a good point.
[02:18:38.440 --> 02:18:39.440]   I think it's a good point.
[02:18:39.440 --> 02:18:40.440]   I think it's a good point.
[02:18:40.440 --> 02:18:41.440]   I think it's a good point.
[02:18:41.440 --> 02:18:42.440]   I think it's a good point.
[02:18:42.440 --> 02:18:43.440]   I think it's a good point.
[02:18:43.440 --> 02:18:44.440]   I think it's a good point.
[02:18:44.440 --> 02:18:45.440]   I think it's a good point.
[02:18:45.440 --> 02:18:46.440]   I think it's a good point.
[02:18:46.440 --> 02:18:47.440]   I think it's a good point.
[02:18:47.440 --> 02:18:48.440]   I think it's a good point.
[02:18:48.440 --> 02:18:49.440]   I think it's a good point.
[02:18:49.440 --> 02:18:50.440]   I think it's a good point.
[02:18:50.440 --> 02:18:51.440]   I think it's a good point.
[02:18:51.440 --> 02:18:52.440]   I think it's a good point.
[02:18:52.440 --> 02:18:53.440]   I think it's a good point.
[02:18:53.440 --> 02:18:54.440]   I think it's a good point.
[02:18:54.440 --> 02:18:55.440]   I think it's a good point.
[02:18:55.440 --> 02:18:56.440]   I think it's a good point.
[02:18:56.440 --> 02:18:57.440]   I think it's a good point.
[02:18:57.440 --> 02:18:58.440]   I think it's a good point.
[02:18:58.440 --> 02:18:59.440]   I think it's a good point.
[02:18:59.440 --> 02:19:00.440]   I think it's a good point.
[02:19:00.440 --> 02:19:01.440]   I think it's a good point.
[02:19:01.440 --> 02:19:02.440]   I think it's a good point.
[02:19:02.440 --> 02:19:03.440]   I think it's a good point.
[02:19:03.440 --> 02:19:04.440]   I think it's a good point.
[02:19:04.440 --> 02:19:05.440]   I think it's a good point.
[02:19:05.440 --> 02:19:06.440]   I think it's a good point.
[02:19:06.440 --> 02:19:07.440]   I think it's a good point.
[02:19:07.440 --> 02:19:08.440]   I think it's a good point.
[02:19:08.440 --> 02:19:09.440]   I think it's a good point.
[02:19:09.440 --> 02:19:10.440]   I think it's a good point.
[02:19:10.440 --> 02:19:11.440]   I think it's a good point.
[02:19:11.440 --> 02:19:12.440]   I think it's a good point.
[02:19:12.440 --> 02:19:13.440]   I think it's a good point.
[02:19:13.440 --> 02:19:14.440]   I think it's a good point.
[02:19:14.440 --> 02:19:15.440]   I think it's a good point.
[02:19:15.440 --> 02:19:16.440]   I think it's a good point.
[02:19:16.440 --> 02:19:17.440]   I think it's a good point.
[02:19:17.440 --> 02:19:18.440]   I think it's a good point.
[02:19:18.440 --> 02:19:19.440]   I think it's a good point.
[02:19:19.440 --> 02:19:20.440]   I think it's a good point.
[02:19:20.440 --> 02:19:21.440]   I think it's a good point.
[02:19:21.440 --> 02:19:22.440]   I think it's a good point.
[02:19:22.440 --> 02:19:23.440]   I think it's a good point.
[02:19:23.440 --> 02:19:24.440]   I think it's a good point.
[02:19:24.440 --> 02:19:25.440]   I think it's a good point.
[02:19:25.440 --> 02:19:26.440]   I think it's a good point.
[02:19:26.440 --> 02:19:27.440]   I think it's a good point.
[02:19:27.440 --> 02:19:28.440]   I think it's a good point.
[02:19:28.440 --> 02:19:29.440]   I think it's a good point.
[02:19:29.440 --> 02:19:30.440]   I think it's a good point.
[02:19:30.440 --> 02:19:31.440]   I think it's a good point.
[02:19:31.440 --> 02:19:32.440]   I think it's a good point.
[02:19:32.440 --> 02:19:33.440]   I think it's a good point.
[02:19:33.440 --> 02:19:34.440]   I think it's a good point.
[02:19:34.440 --> 02:19:35.440]   I think it's a good point.
[02:19:35.440 --> 02:19:36.440]   I think it's a good point.
[02:19:36.440 --> 02:19:37.440]   I think it's a good point.
[02:19:37.440 --> 02:19:38.440]   I think it's a good point.
[02:19:38.440 --> 02:19:39.440]   I think it's a good point.
[02:19:39.440 --> 02:19:40.440]   I think it's a good point.
[02:19:40.440 --> 02:19:41.440]   I think it's a good point.
[02:19:41.440 --> 02:19:42.440]   I think it's a good point.
[02:19:42.440 --> 02:19:43.440]   I think it's a good point.
[02:19:43.440 --> 02:19:45.440]   I think it's a good point.
[02:19:45.440 --> 02:19:46.440]   I think it's a good point.
[02:19:46.440 --> 02:19:47.440]   I think it's a good point.
[02:19:47.440 --> 02:19:48.440]   I think it's a good point.
[02:19:48.440 --> 02:19:49.440]   I think it's a good point.
[02:19:49.440 --> 02:19:50.440]   I think it's a good point.
[02:19:50.440 --> 02:19:51.440]   I think it's a good point.
[02:19:51.440 --> 02:19:52.440]   I think it's a good point.
[02:19:52.440 --> 02:19:53.440]   I think it's a good point.
[02:19:53.440 --> 02:19:54.440]   I think it's a good point.
[02:19:54.440 --> 02:19:55.440]   I think it's a good point.
[02:19:55.440 --> 02:19:56.440]   I think it's a good point.
[02:19:56.440 --> 02:19:57.440]   I think it's a good point.
[02:19:57.440 --> 02:19:58.440]   I think it's a good point.
[02:19:58.440 --> 02:19:59.440]   I think it's a good point.
[02:19:59.440 --> 02:20:00.440]   I think it's a good point.
[02:20:00.440 --> 02:20:01.440]   I think it's a good point.
[02:20:01.440 --> 02:20:02.440]   I think it's a good point.
[02:20:02.440 --> 02:20:03.440]   I think it's a good point.
[02:20:03.440 --> 02:20:04.440]   I think it's a good point.
[02:20:04.440 --> 02:20:05.440]   I think it's a good point.
[02:20:05.440 --> 02:20:06.440]   I think it's a good point.
[02:20:06.440 --> 02:20:11.940]   I think it's a horrible, horrible thing that we are asking locally elected officials to
[02:20:11.940 --> 02:20:12.940]   do a person in this case.
[02:20:12.940 --> 02:20:17.700]   In this case, in California, it was friends of the earth who lobbied the California Department
[02:20:17.700 --> 02:20:24.240]   of Pesticide Regulation, presumably people who would know enough science, and they convinced
[02:20:24.240 --> 02:20:28.240]   them to withdraw the research application, and then they published a press release that
[02:20:28.240 --> 02:20:29.240]   victory.
[02:20:29.240 --> 02:20:30.240]   Who knows?
[02:20:30.240 --> 02:20:35.440]   I mean, listen, we don't know the political...
[02:20:35.440 --> 02:20:38.440]   What was happening on the back end?
[02:20:38.440 --> 02:20:42.320]   There are solutions.
[02:20:42.320 --> 02:20:47.680]   There are knock-on effects, and there are people who talk about genetically modified
[02:20:47.680 --> 02:20:54.000]   as though that is some type of ultimate, horrific thing.
[02:20:54.000 --> 02:21:00.240]   In the United States at this point, it is very difficult to consume something that has
[02:21:00.240 --> 02:21:05.400]   not in some way been modified.
[02:21:05.400 --> 02:21:06.400]   It's very...
[02:21:06.400 --> 02:21:11.880]   Again, it sort of mirrors some of the talk about AI, which is just completely salacious
[02:21:11.880 --> 02:21:17.280]   and over the top, and we're not taking five seconds to be reasonable.
[02:21:17.280 --> 02:21:18.280]   Which makes me crazy.
[02:21:18.280 --> 02:21:22.960]   Well, you're going to have your mosquitoes in the central valley, and you can thank the
[02:21:22.960 --> 02:21:24.960]   face of the earth.
[02:21:24.960 --> 02:21:27.520]   Yeah, I don't know.
[02:21:27.520 --> 02:21:32.840]   Spotify has ejected thousands of AI-made songs.
[02:21:32.840 --> 02:21:36.280]   And by the way, they're all terrible.
[02:21:36.280 --> 02:21:37.800]   But this is a flaw.
[02:21:37.800 --> 02:21:42.320]   People can gamify Spotify by...
[02:21:42.320 --> 02:21:46.200]   There's a whole bunch of songs about poop, for instance, on Spotify, because they know
[02:21:46.200 --> 02:21:50.720]   kids will ask Alexa for it as soon as they find out that they can ask for a poop song
[02:21:50.720 --> 02:21:53.760]   they will, and they make money on it.
[02:21:53.760 --> 02:21:56.400]   Now that I know what I'm saying.
[02:21:56.400 --> 02:21:58.960]   Yeah, you can't wait till you try that out.
[02:21:58.960 --> 02:22:00.520]   Tens of thousands of songs.
[02:22:00.520 --> 02:22:01.520]   Can I ask you a question now?
[02:22:01.520 --> 02:22:03.920]   But why is that bad?
[02:22:03.920 --> 02:22:04.920]   Why is that bad?
[02:22:04.920 --> 02:22:07.000]   No, I'm not saying...
[02:22:07.000 --> 02:22:08.080]   Did I say it was bad?
[02:22:08.080 --> 02:22:09.080]   I'm just saying that.
[02:22:09.080 --> 02:22:11.520]   No, I met with Spotify, kill this.
[02:22:11.520 --> 02:22:13.000]   Why are they removing them?
[02:22:13.000 --> 02:22:14.000]   What's the damage?
[02:22:14.000 --> 02:22:18.640]   I mean, it's taking money away from legitimate artists, I guess.
[02:22:18.640 --> 02:22:20.440]   Yeah, I don't know.
[02:22:20.440 --> 02:22:26.600]   And I mean, I realize that I literally haven't listened to real new music with any regularity
[02:22:26.600 --> 02:22:28.840]   post like 1997.
[02:22:28.840 --> 02:22:34.360]   That being said, a lot of music, let's face it, kind of is interchangeable.
[02:22:34.360 --> 02:22:39.560]   Can I recommend, if you want to catch up on all of this stuff, watch Eurovision, which
[02:22:39.560 --> 02:22:44.720]   for the first time was broadcasting United States on Peacock this year.
[02:22:44.720 --> 02:22:50.680]   You will be amazed at how bad some of the music is.
[02:22:50.680 --> 02:22:52.240]   I know spoilers here.
[02:22:52.240 --> 02:22:56.240]   I'm sorry if you saw that, who won and all of that.
[02:22:56.240 --> 02:23:01.760]   But this was something I watched last night and I was thinking, "This is why the US music
[02:23:01.760 --> 02:23:04.560]   industry is so dominant worldwide."
[02:23:04.560 --> 02:23:06.800]   I think it's rich that Spotify...
[02:23:06.800 --> 02:23:07.800]   Let's talk for a second.
[02:23:07.800 --> 02:23:13.080]   I forget who did this study, but somebody looked at how Spotify is putting together the
[02:23:13.080 --> 02:23:16.040]   sort of recommended daily lists.
[02:23:16.040 --> 02:23:17.960]   And they're not random.
[02:23:17.960 --> 02:23:24.080]   So if you look across multiple accounts, everybody's basically being recommended the
[02:23:24.080 --> 02:23:26.640]   same songs over and over again.
[02:23:26.640 --> 02:23:31.440]   So I don't care what anybody says, the discoverability.
[02:23:31.440 --> 02:23:34.800]   You see this across different platforms in the digital arena.
[02:23:34.800 --> 02:23:40.480]   The same thing happens with Search, the same thing happens with music.
[02:23:40.480 --> 02:23:45.400]   It's not as though everybody truly is getting a random assortment of enhanced, if you click
[02:23:45.400 --> 02:23:47.240]   on "enhance" in Spotify.
[02:23:47.240 --> 02:23:54.040]   So what does it matter if people are making AI?
[02:23:54.040 --> 02:23:55.040]   Yeah.
[02:23:55.040 --> 02:24:01.240]   I've been listening to the Cheese Tax song on a continuous loop from TikTok.
[02:24:01.240 --> 02:24:03.040]   Just cheese tax.
[02:24:03.040 --> 02:24:04.600]   Cheese tax?
[02:24:04.600 --> 02:24:05.600]   Cheese tax.
[02:24:05.600 --> 02:24:06.600]   It's a dog thing.
[02:24:06.600 --> 02:24:08.760]   I have a new dog.
[02:24:08.760 --> 02:24:10.040]   Is there a cheese tax?
[02:24:10.040 --> 02:24:11.040]   Cheese tax.
[02:24:11.040 --> 02:24:12.040]   But I'd be worried about this.
[02:24:12.040 --> 02:24:13.040]   If I'm regulating.
[02:24:13.040 --> 02:24:18.200]   You've got to pay the cheese tax every time you're cooking when the cheese comes out.
[02:24:18.200 --> 02:24:19.200]   This is it?
[02:24:19.200 --> 02:24:21.960]   The rules are the rules and the facts are the facts.
[02:24:21.960 --> 02:24:26.560]   And when the cheese draw opens, you've got to pay the tax.
[02:24:26.560 --> 02:24:27.560]   Okay.
[02:24:27.560 --> 02:24:29.560]   Does your dog like that song?
[02:24:29.560 --> 02:24:32.360]   My dog enforces the cheese tax.
[02:24:32.360 --> 02:24:33.360]   It's extremely...
[02:24:33.360 --> 02:24:36.960]   He gets a little bit of whatever cheese comes out of the cheese drawer.
[02:24:36.960 --> 02:24:37.960]   The dog gets.
[02:24:37.960 --> 02:24:38.960]   Okay.
[02:24:38.960 --> 02:24:39.960]   That's fair.
[02:24:39.960 --> 02:24:40.960]   There's thousands.
[02:24:40.960 --> 02:24:41.960]   That's fair.
[02:24:41.960 --> 02:24:42.960]   Tens of thousands of these clips.
[02:24:42.960 --> 02:24:43.960]   Yeah.
[02:24:43.960 --> 02:24:47.760]   This is why I hope TikTok will never get banned.
[02:24:47.760 --> 02:24:50.360]   Mr. Phil Libbon, thank you so much for spending some time with us.
[02:24:50.360 --> 02:24:54.040]   I hope you enjoyed your five cups of tea.
[02:24:54.040 --> 02:24:55.040]   You didn't run out.
[02:24:55.040 --> 02:24:56.040]   Three.
[02:24:56.040 --> 02:24:57.040]   Only three.
[02:24:57.040 --> 02:24:58.040]   Only three.
[02:24:58.040 --> 02:24:59.040]   Oh, good.
[02:24:59.040 --> 02:25:00.040]   You didn't run out.
[02:25:00.040 --> 02:25:04.080]   Did we make it through the entire show without a tea shortage?
[02:25:04.080 --> 02:25:06.280]   I think it's hysterical that you noticed that, Amy.
[02:25:06.280 --> 02:25:07.280]   I'm very impressed.
[02:25:07.280 --> 02:25:10.800]   Well, I've got a bunch of vessels on my desk too, but I've been drinking black coffee out
[02:25:10.800 --> 02:25:13.160]   of this one and Topochiko out of this one.
[02:25:13.160 --> 02:25:14.160]   Perfect.
[02:25:14.160 --> 02:25:15.160]   Perfect.
[02:25:15.160 --> 02:25:17.440]   You cannot make a Molotov cocktail out of Topochiko, by the way.
[02:25:17.440 --> 02:25:18.440]   I've learned that.
[02:25:18.440 --> 02:25:21.040]   Those are the glasses too thick.
[02:25:21.040 --> 02:25:23.200]   Yeah, no, it doesn't burn.
[02:25:23.200 --> 02:25:27.200]   Someday you'll have to tell us why you know that.
[02:25:27.200 --> 02:25:32.280]   Next time I'm on, I'll be plugging my new restaurant, which is called Bentoville.
[02:25:32.280 --> 02:25:33.960]   Bentoville in beautiful.
[02:25:33.960 --> 02:25:34.960]   In Bentonville.
[02:25:34.960 --> 02:25:35.960]   Oh, that's a great idea.
[02:25:35.960 --> 02:25:36.960]   That's not cool.
[02:25:36.960 --> 02:25:40.440]   Are you going to start a restaurant with all of Bento boxes in Bentonville?
[02:25:40.440 --> 02:25:41.440]   Yep.
[02:25:41.440 --> 02:25:42.440]   It's called Bentoville.
[02:25:42.440 --> 02:25:43.440]   It's all happening.
[02:25:43.440 --> 02:25:44.440]   I love that.
[02:25:44.440 --> 02:25:48.400]   We have all the pop-ups are sold out, so you can't get anything right now.
[02:25:48.400 --> 02:25:50.160]   And this is your restaurant?
[02:25:50.160 --> 02:25:53.720]   So, Bill, well, I have partners who know what they're doing.
[02:25:53.720 --> 02:25:58.320]   The problem, unfortunately, when I search for Bentoville is Google's too smart and says,
[02:25:58.320 --> 02:25:59.800]   "Oh, you mean Bentonville."
[02:25:59.800 --> 02:26:01.920]   Yeah, but that's going to change.
[02:26:01.920 --> 02:26:02.920]   I love it.
[02:26:02.920 --> 02:26:04.240]   I think that sounds so cool.
[02:26:04.240 --> 02:26:07.560]   The world is going to come to Bentoville.
[02:26:07.560 --> 02:26:10.040]   Mr. Bentobox, Bill Libben.
[02:26:10.040 --> 02:26:12.440]   His app is mm-hmm.
[02:26:12.440 --> 02:26:13.440]   It's mmhmm.app.
[02:26:13.440 --> 02:26:15.760]   Got to try it.
[02:26:15.760 --> 02:26:18.720]   Backend Windows, you can present with it.
[02:26:18.720 --> 02:26:20.160]   You can change your backgrounds with it.
[02:26:20.160 --> 02:26:26.920]   It makes every Zoom call better, or WebEx or Teams, or meet your video companion.
[02:26:26.920 --> 02:26:28.320]   Thank you, Phil.
[02:26:28.320 --> 02:26:29.320]   So smart.
[02:26:29.320 --> 02:26:32.920]   I just love having smart people on the show because then I could sit back, relax, and
[02:26:32.920 --> 02:26:35.880]   enjoy the ride.
[02:26:35.880 --> 02:26:39.160]   Bentonville, that just blows my mind.
[02:26:39.160 --> 02:26:40.160]   All turn the economy.
[02:26:40.160 --> 02:26:41.160]   I'll show you around.
[02:26:41.160 --> 02:26:43.080]   Yeah, I will.
[02:26:43.080 --> 02:26:48.040]   Amy Webb, just great to have you.
[02:26:48.040 --> 02:26:54.120]   The book is The Genesis Machine, our quest to rewrite life in the age of synthetic biology,
[02:26:54.120 --> 02:26:57.000]   future today, Institute, is her company.
[02:26:57.000 --> 02:26:58.000]   What do you want to plug?
[02:26:58.000 --> 02:26:59.000]   Anything?
[02:26:59.000 --> 02:27:05.120]   Yeah, I mean, I guess if you're interested in AI, the book that I wrote previous to this
[02:27:05.120 --> 02:27:08.080]   one is The Big Nine, and it's still really great.
[02:27:08.080 --> 02:27:09.480]   Janine Perro's favorite.
[02:27:09.480 --> 02:27:12.920]   Janine Perro's endorsed it.
[02:27:12.920 --> 02:27:14.840]   Actually, it's a great book.
[02:27:14.840 --> 02:27:19.760]   I have all your books, and I brought this one out because it's the most recent, but I
[02:27:19.760 --> 02:27:22.200]   should have brought The Big Nine out to next time I'll know better.
[02:27:22.200 --> 02:27:27.440]   Had I been watching Fox News, I would have known how tech titans and their thinking
[02:27:27.440 --> 02:27:28.800]   machines could warp humanity.
[02:27:28.800 --> 02:27:30.000]   It came out four years ago.
[02:27:30.000 --> 02:27:31.160]   It was prescient.
[02:27:31.160 --> 02:27:36.520]   Both these people knew what was coming long before I did.
[02:27:36.520 --> 02:27:38.080]   Thank you so much for being here, both of you.
[02:27:38.080 --> 02:27:44.000]   We do Twitter every Sunday afternoon, 2 p.m. Pacific, 5 p.m. Eastern, 2100 UTC.
[02:27:44.000 --> 02:27:44.840]   Thank you for watching.
[02:27:44.840 --> 02:27:51.880]   If you're watching live, you can chat live at irc.twit.tv or in our club, Twit Discord.
[02:27:51.880 --> 02:27:54.640]   After the fact, the show's available at the website, Twit.tv.
[02:27:54.640 --> 02:27:57.720]   And when you get there, you'll also notice a link to the YouTube channel.
[02:27:57.720 --> 02:27:59.680]   You can watch it there.
[02:27:59.680 --> 02:28:04.440]   There's also a link to various podcast players, or just search for this week in tech or Twit
[02:28:04.440 --> 02:28:05.800]   on your favorite podcast player.
[02:28:05.800 --> 02:28:06.800]   Subscribe.
[02:28:06.800 --> 02:28:07.800]   Exactly.
[02:28:07.800 --> 02:28:11.800]   The minute it's done, just in time for your Monday morning commute.
[02:28:11.800 --> 02:28:13.640]   Thank you everybody.
[02:28:13.640 --> 02:28:14.640]   Have a great week.
[02:28:14.640 --> 02:28:15.880]   We'll see you next time.
[02:28:15.880 --> 02:28:17.920]   Another Twit is in the can.
[02:28:17.920 --> 02:28:18.920]   All right.
[02:28:18.920 --> 02:28:19.920]   This is amazing.
[02:28:19.920 --> 02:28:19.920]   [Music]
[02:28:19.920 --> 02:28:20.920]   [Music]
[02:28:20.920 --> 02:28:27.920]   [Music]
[02:28:27.920 --> 02:28:28.760]   - That's it.
[02:28:28.760 --> 02:28:29.580]   - All right.

