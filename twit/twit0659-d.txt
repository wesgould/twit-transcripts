;FFMETADATA1
title=A Game of Hold My Beer
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=659
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100
Failed to align segment (" You're going to use the same password over and over."): backtrack failed, resorting to original...
Failed to align segment (""): no characters in this segment found in model dictionary, resorting to original...
Failed to align segment (" 21%."): no characters in this segment found in model dictionary, resorting to original...
Start time: 1.36
End time: 30.53
Speaker: SPEAKER_08
Transcript:  It's time for twit this week in tech. We've got a great panel for you. Denise Howell joins us from this week in law from the register.co.uk the news editor Ian Thompson, Harry McCracken from fast company. And of course, big the big story Facebook Cambridge Analytica, the Uber self driving car who really was at fault. And a whole lot more. It's a it's a big day today. Lots of stories. Oh, yeah, let's not forget what Congress did what Congress did. It's all coming up next on Twitter.  This is twit.

Start time: 34.04
End time: 36.64
Speaker: SPEAKER_06
Transcript:  Netcasts you love from people you trust.

Start time: 41.43
End time: 133.94
Speaker: SPEAKER_08
Transcript:  This is twit.  Bandwidth for this week in tech is provided by cashfly at C A C H E F L Y dot com.  This is twit this week in tech episode six hundred fifty nine recorded Sunday, March 25th, 2018. A game of hold my beer.  This week in tech is brought to you by Kubernetes engine on Google Cloud Platform. Learn more at G dot C O slash G E T G K E today.  And by last pass. Join over 13 million last pass users, including me, and start managing and securing your passwords today.  Learn more at last pass dot com slash twit and by blue apron, the number one fresh ingredient and recipe delivery service in the country.  Check out this week's menu and get thirty dollars off your first order with free shipping by going to blue apron dot com slash twit and by zip recruiter hiring.  Zip recruiter has revolutionized how you do it. Their technology identifies people with the right experience and invites them to apply to your job.  Try it free today at zip recruiter dot com slash twit.  It's time for twit this week in tech, the show where we cover the week's tech news. Did anything at all happen this week? I think it's going to be very slow, but it doesn't matter because we have an entertaining crew here for you.  Ian Thompson from the register. Now, new title for you. What is your new title?

Start time: 135.00
End time: 143.82
Speaker: SPEAKER_07
Transcript:  I'm a new set of sir for the register. Very nice. Yes, I've become corporate and management since. Yeah, I mean, I might have to stop wearing a t shirt to work, but yeah, I'm sure that.

Start time: 145.00
End time: 232.00
Speaker: SPEAKER_08
Transcript:  I think whatever you're wearing looks very British and you should keep it up.  Turn up in a bowler hat and you're. I don't know. Doesn't it look like. I don't know. It feels like. I don't know. You're playing rugby or something. I don't know what it is. It's very British. Rugby is the beautiful game.  It is the beautiful game, but it's a beautiful game. Unless unless your knees. If you're a pair of knees, rugby is not the beautiful game or teeth or collarbone.  Any other body part you might mention Harry McCracken, the technologizers also here. Harry is technology editor at fast company.  We've got some big names in here. Hi, Harry. Good to see you. Great to be here. Always a pleasure to have you on.  And from right to left, I guess the next would be Denise Howell from this week in law. Hello, Denise.  Hi, Leo. Hi, Harry. Hi, Ian. Good to see you all.  We bring our legal counsel in because we're going to be talking about the law. Stop us all getting sued.  I look lousy and orange. So we've been we've been talking beforehand and there's really three big stories.  We'll cover a number of them, but the three big stories are going to be Facebook slash Cambridge Analytica.  That really heated up. It broke last week, but it really heated up this most recent week.  In fact, today, full page article in the New York Times in the UK was in the Observer, which is where the story broke.  Mark Zuckerberg apologizing. He didn't apologize. Not apologizing. He's not good at apologizing.

Start time: 233.00
End time: 240.98
Speaker: SPEAKER_05
Transcript:  He said he was sorry, but that's not exactly the same thing. Oh, I thought that was an apology.  I don't know the way he said it. I think it's hard to apologize without using the words I apologize.

Start time: 241.26
End time: 348.98
Speaker: SPEAKER_08
Transcript:  It's more like I'm sorry this happened to me. Yeah. Yeah.  Sorry we got found out. Sorry we got discovered.  We'll also talk about and I'm glad to have some legal support on this one.  Congress, our do nothing Congress got very active last week and snuck some stuff in.  They passed a bill that ostensibly is against sex trafficking. Who's in favor of that?  But it has a much larger impact on blogs, on Craigslist, on Reddit, FOSTA and SESTA.  And they snuck in at the last minute to the budget appropriations bill.  Page 2000, what was it? 2,201 out of a 2,232 page bill.  They snuck in the Cloud Act, which they've been trying to get passed through Congress.  And oddly enough, Microsoft and Google and Facebook have been trying to get passed through Congress all this time.  Because it lets them off the hook. So we'll talk about that. Exactly.  We talked last week about Uber's self-driving vehicle, sadly killing a pedestrian.  Well now it looks like there may be evidence it's Uber's fault.  We'll get into that as well. Let's start with Facebook though and Mark Zuckerberg's pseudo apology.  I tried to do the timeline in this Cambridge Analytica thing, but this is a really fast changing story.  I'll paint general pictures and you can correct me if I get it wrong.  This Mother Jones article implies that Cambridge Analytica overstated its capabilities in order to fleece candidates for money.  Including Ted Cruz, who was one of the first candidates to use Cambridge Analytica.  The story is this was a company created out of another British company called SLC.  What is SLC's job? They're still around.

Start time: 350.04
End time: 360.00
Speaker: SPEAKER_07
Transcript:  They're a general data monitoring and data merchant.  It's kind of like a cross between public relations, marketing, advertising.

Start time: 361.06
End time: 364.00
Speaker: SPEAKER_08
Transcript:  And they have many Tory politicians in Britain as clients.

Start time: 365.00
End time: 404.00
Speaker: SPEAKER_07
Transcript:  I wouldn't surprise me if they were Labour and Liberal Democrat politicians as well.  They were already being investigated by the UK Parliament over fears that they'd involve themselves in the Brexit vote.  Indeed one of their staffers put out a press release saying we are going to be working with a leave campaign for Brexit.  And schedule a press conference and then it got pulled very quickly.  And it was like no, that was someone being over enthusiastic. We haven't actually done any business with them.  But as we've seen in the investigations over the week, they're on video as saying, oh yes we can not invoice you as from ourselves.  We can find another dummy company, we can invoice you from that and that way you get plausible deniability.

Start time: 404.64
End time: 498.00
Speaker: SPEAKER_08
Transcript:  They were doing something that really has been going on for decades.  The idea of trying to understand the public and their interests and their weak spots, their pressure points.  For the purpose of marketing by observing behaviour.  The problem is that over the last decade computers and big databases and information treasure troves like Google and Facebook have become such rich sources of this information.  That these companies have, or maybe not, the contention is these companies have in effect weaponised, this is what Tim Berners-Lee, the creator of the web said, have weaponised these surveillance mechanisms to really take advantage of them.  And I think from SLC's point of view, Christopher Wiley the whistleblower who outed Cambridge Analytica started at SLC and went along with Andrew Nix and others to work at Cambridge Analytica.  I think where Cambridge Analytica's genesis came from is this study that showed up in the proceedings of the National Academy of Sciences of the US of A in February 2013.  This is a five year old study.  What happened was they took 58,000 volunteers, they got from their volunteers their Facebook likes, detailed demographic profiles and then gave them a bunch of psychometric tests.  And were able to come up with some kind of surprising correlations.  And this gave marketers a little chill down their spine. Oh my god!

Start time: 499.02
End time: 499.96
Speaker: SPEAKER_07
Transcript:  Somebody's applying science to witchcraft.

Start time: 501.00
End time: 580.00
Speaker: SPEAKER_08
Transcript:  Facebook likes are pretty widely available and it turns out can be fairly predictive of lots of things.  Race, gender, politics, intelligence. Here are some of the things that according to this article your Facebook likes can tell you with very high accuracy.  The best predictors of high intelligence include likes for these categories.  Thunderstorms, the Colbert Report, that's how old this is, science and curly fries.  Curly fries, these are high but not just predictors, highly correlated like 80% correlation.  Good predictors of low intelligence, Sephora, I love being a mom, Harley-Date, I'm sorry Denise but actually somebody when we did, I mentioned this on Monday,  this week on Google and Stacey said boy that's very sexist on both of those.  But also Harley-Davidson and the country group Lady Antebellum.  But you can do it to predict sexual orientation with an 88% accuracy.  It turns out if you like Mac Cosmetics and Wicked the Musical, you're probably gay. I like both, I use both.  Is that something you'd like to tell us?  But the point is it's not perfect. And in fact people lie to Facebook, there's other reasons why it's not perfect.

Start time: 581.00
End time: 590.88
Speaker: SPEAKER_05
Transcript:  I just identify as African American. You can go in and see what they tell advertisers and they say that my cultural affinity is African American.  Isn't that interesting?

Start time: 592.00
End time: 598.88
Speaker: SPEAKER_08
Transcript:  And I'm not sure why.  They may though not be completely forthright with you, right, because they also don't want people to know how much information can be deduced from your Facebook likes.

Start time: 600.02
End time: 616.00
Speaker: SPEAKER_07
Transcript:  I think it comes down to how you use social media. I mean if you're just going on social media to report exactly what's happening to you every day of your life, then it might be fairly accurate.  But most people I think who use it are only putting up a very small section of their life that they just want other people to know about.  So I don't think it's entirely...

Start time: 617.10
End time: 620.84
Speaker: SPEAKER_08
Transcript:  But come on, you press the like button from time to time.  Oh yeah, sure.

Start time: 622.22
End time: 630.00
Speaker: SPEAKER_05
Transcript:  I have almost never liked a brand or a thing. I like comments from my friends.  Do you like Wu Tang Clan?  Never expressed an opinion one way or the other.

Start time: 631.00
End time: 659.00
Speaker: SPEAKER_08
Transcript:  Shaq? Never said.  How about being confused after waking up from naps? Did you ever like that group?  That means you're straight.  That's a strong predictor of male heterosexuality if you like waking up confused from naps.  I don't know why, and that's the point of this. And that's why big data is interesting because it's not intuitive. It's correlations. It's just correlations.  If you like Hello Kitty, do you like Hello Kitty, anybody? If you like Hello Kitty...

Start time: 660.00
End time: 661.00
Speaker: SPEAKER_01
Transcript:  I mean it's hard to hate Hello Kitty.

Start time: 661.74
End time: 860.00
Speaker: SPEAKER_08
Transcript:  No, but if you actually press the like button on Hello Kitty, that's the key.  Yeah, you're right. This isn't just like I like Hello Kitty. Who doesn't?  If you actively liked Hello Kitty on Facebook, you tend to be high on openness, but low on conscientiousness, agreeableness, and emotional stability.  Now, I don't know... I mean, anyway, that's the way it is.  I believe Christopher Wiley and the folks at Cambridge Analytica saw this study and said, hmm, because marketers are always looking for ways to find soft spots in people to target.  And they actually created their own psychological test. A guy named Alexander Kogan, who, Cambridge University is very anxious to point out,  yes, he was a teacher here, but no, we did not approve this study. We rejected it. And he went over to the Soviet Union, to former Soviet Union, Russia, and some Russian university and got them to approve it.  But he made a psychological, similar to this study, a psychological test, advertised it on Facebook, got people to take it. He said, well, pay you. They got a token that paid you.  But he also, because the test somehow was linked to Facebook, was able to get, and this was in the days when Facebook not only gave you, gave your information to these quizzes,  and this was an off Facebook quiz, but somehow he got access to that. And this was when Friends of Friends were still in act. Facebook turned that off a few years ago.  But he ended up, after getting, I think, I can't remember, 70,000 questionnaires, he got a quarter, no, it was 270,000 questionnaires, but he got 50 million people out of that because of the Friends of Friends.  That database, which normally Facebook holds on tight to, because after all, this is their business model, it's not that they don't want that information, it's they want to keep it to themselves so they can sell it.  But Cambridge Analytica, because it was an academic study, they claimed, was able to get it out of Facebook, and apparently kept it for a long time, even though they swore to Mark Zuckerberg, I promise we deleted that.  And Mark said, well, it was a legal document, they must have. Nevertheless, used that information on 50 million voters. And here's, and by the way, that's exactly Facebook's business model. That's how Facebook works.  That's how it works today. The only thing that they did wrong from Facebook's point of view is kept the data instead of having to buy it from Facebook.  But here's maybe something that you and I should be more considerate about, considerate more of a problem, is that they then apparently maybe gave it to the Internet Research Agency, the Russian arm of Trollfarm, and that information was used by them to create, and I don't, you know, I see this again and again, somebody just said it in the chat room,  Oh, Obama did the same thing. It's just because it's Trump that did it. That's not the problem so much. Every campaign does this. In fact, the first Cambridge Analytica customer was Ted Cruz.  But the Russians might have used it to sow dissent and discord in the United States by creating opposing groups, all sorts of fake stuff. And that's, I think, of interest, if a foreign interest tried to create polarization in this country using this information.  In any event, the chickens have come home to roost. Mark Zuckerberg did interviews. He doesn't like doing interviews on CNN. Recode with the New York Times. Did he ever say, did he ever say I apologize?

Start time: 861.00
End time: 876.98
Speaker: SPEAKER_05
Transcript:  As far as I know, he has not used that word. He's certainly expressed regret and said they messed up and that they're taking actions to prevent it from happening again. But I think he said that we failed you and if we can't keep your trust, then we don't deserve to.

Start time: 877.08
End time: 916.96
Speaker: SPEAKER_08
Transcript:  Alex Stamos, Facebook's, I think, widely regarded, highly regarded security chief. He was at Yahoo. I think people like him. He's left Facebook saying, I tried to explain to the executives.  He's on his way out apparently. Yeah, August will be his. And that's mostly a transitional thing. He had a team of 120 people now reduced to three. So he's not really...  He hasn't really been doing anything for a while. But he says, I tried to convince Facebook's executives that Russian trolls were using this information and they didn't listen.  I think they listened. They just didn't care.  Yeah, well that's kind of my question. Isn't, Denise, isn't this Facebook's business model? This is what they do. Why is anybody surprised? What's wrong with this picture?

Start time: 919.10
End time: 1039.94
Speaker: SPEAKER_01
Transcript:  Well, I think we have to go to the... We have to back the timeline up a little further back than you even went. In fact, we could back the timeline up all the way back to the inception of Facebook when they first unleashed themselves from the universities where they were helping people hook up, right?  And opened to the general public and their business model then was, we want as many users as possible. We don't know how we're going to make money off of this yet.  But we know that if we create a great service where people like to come use our service, they like to come back, they trust us, they are willingly handing over all this information, then we can build a business model on that.  So over the course of the history of the company, its priorities have changed. It used to look after the interests of its users because that's what it needed at the time when it started, right? It really, really wanted to build that huge millions and millions of customer base.  And then it flipped, right? Once it decided its business model was to sell all that information to advertisers in ways that hadn't been done before, then the advertisers began to take precedence as to whose interests were going to be put first.  And that's where we find ourselves now. So as we've watched that progression over the years, privacy watchdogs like, for example, the Electronic Privacy Information Center, another mark to throw into the mix here, Mark Rotenberg, not Zuckerberg, heads Epic.  And he is so ticked off at the FTC right now. If you haven't seen his piece in Techonomy that was published this week, it is well worth checking out.

Start time: 1040.26
End time: 1047.00
Speaker: SPEAKER_08
Transcript:  Mark's one of the good guys. And Epic is a really important group that protects our interests online. So what did Mark say?

Start time: 1048.36
End time: 1187.00
Speaker: SPEAKER_01
Transcript:  He details there how, you know, we started to sound the alarm about this shift in priorities back in 2009.  Well, we all, but haven't we all been doing this?  Right. And they were, but it was the complaint that Epic filed that ultimately led to the consent decree, the settlement on Facebook's part with the FTC.  As part of that settlement, the FTC states, Facebook represented that third party apps that users installed would have access only to user information that they needed to operate.  In fact, the apps could access nearly all of users' personal data, data the apps didn't need.  That was that was all going back to the original Epic complaint in 2009.  So Epic continued to, you know, bang the drum as the years went on, went back in in 2012 and said, you know, FTC, you really need to be holding these companies' feet to the fire for these settlements that we reached.  And the FTC didn't do all that it could. And we find ourselves here now.  So, but what I come back to is, you initially asked about the business model.  I think if Facebook's going to dig itself out of this, if it's going to get to the point where Tim Berners-Lee wants them to get, where they fix this, they have to rethink that business model.  And it was somewhat disappointing to hear Sheryl Sandberg this week say, oh no, you know, our business model is just fine.  Well, maybe it is, but at minimum they have to shift the priorities around again so that user data is something that they respect and they understand that people aren't waiting, waiting, waiting into their privacy policies and data policies.  And figuring out, oh gee, I guess if I clicked that I did let Cambridge Analytica do all this stuff that I wouldn't have wanted them to do.  I mean, clearly the users who were affected by this would have liked some actually comprehensible disclosure as to what was happening.  And many of them, if not the majority of them, probably would have said no.  So I think we have to get to the point where people understand what's going on here.

Start time: 1188.36
End time: 1200.68
Speaker: SPEAKER_08
Transcript:  Yeah, the only reason I brought up all those Hello Kitty Facebook Live things is to point out that you may be leaking all sorts of information just by using Facebook that you're not really aware of.  And Facebook certainly is not in their interest to disclose this.

Start time: 1201.18
End time: 1208.68
Speaker: SPEAKER_05
Transcript:  I always thought that if I liked Hello Kitty that was kind of meaningless. Not that I've liked Hello Kitty, but if I liked whatever, I thought it was meaningless.

Start time: 1209.02
End time: 1236.00
Speaker: SPEAKER_07
Transcript:  And taking a quiz on Facebook is just fun.  The quiz thing really irritates the bejesus out of me because yes, I mean, they build to you as like, oh, it's fun.  Find out which Star Trek captain you would be or whatever.  And people didn't realize that in taking that quiz, not only were they handing over their own personal information, but up until 2014, they were handling over handing over all their friends personal information as well.  And I've got relatives and friends on Facebook who take these quizzes generally and invite you to kind of, you have no control over that.

Start time: 1236.38
End time: 1254.00
Speaker: SPEAKER_08
Transcript:  It's a larger problem, though, isn't it?  I mean, the term that's widely used now is surveillance capitalism.  And it isn't just Facebook.  This is Google's business model as well.  Yeah, true.  Amazon certainly spies on us in every way possible so they can offer us individual.

Start time: 1254.04
End time: 1260.00
Speaker: SPEAKER_07
Transcript:  I mean, you look at the amount of decisions made when you book a flight online and the amount of things that they can grab out from you.

Start time: 1260.56
End time: 1272.08
Speaker: SPEAKER_08
Transcript:  And again, as I said at the very beginning, nothing new.  This has been going on since the 1950s, but it's become, to use Tim Berners-Lee's word, weaponized because of big data and big computers.  Yeah.

Start time: 1273.93
End time: 1301.96
Speaker: SPEAKER_01
Transcript:  So yeah, definitely.  And it's not just I mean, it's it's down to the the way in which consent is extracted from us, the graphical way.  Consider whenever you're asked to release information, how you get a big blue button that says, yes, I'd like to comply.  And then maybe there's some gray text down underneath that is opting you out.  You know, I mean, they're they're manipulating people at very fundamental levels.

Start time: 1302.20
End time: 1313.51
Speaker: SPEAKER_05
Transcript:  I really feel like on Facebook when they give you a yes or no, they should be in the same typeface, same amount of emphasis.  They should not try to steer you in one direction or another.  There shouldn't be a little frowny face next to they.  I don't want to do that.

Start time: 1314.06
End time: 1320.98
Speaker: SPEAKER_08
Transcript:  I feel like Facebook's done a lot worse, though.  I mean, they when Duterte became the I don't know what the term is.

Start time: 1321.51
End time: 1322.16
Speaker: SPEAKER_07
Transcript:  Presidente for life.  Yeah.

Start time: 1323.14
End time: 1411.00
Speaker: SPEAKER_08
Transcript:  It's dictator, a strong man in the Philippines.  He did it with the help of Facebook.  And then when he won, Facebook came in and gave them so-called white glove service to help Duterte stay in power.  They shut down the free press in the Philippines and used Facebook Live to stream his speeches.  Facebook would say, well, we don't want to pick sides in a political battle.  What is what?  And I don't even just want to blame Facebook because I think Facebook, although well, as long as we're piling on, let's just let's pile on a little bit more.  Sandy Parakilis, who was for a couple of years platform operations manager at Facebook, has stepped forward.  He was responsible for policing data breaches by 30 third party developers between 2011 and 2012.  He said he is to this is a Guardian story.  Warren, senior executives of the company, its lax approach to data protection risked a major breach.  He said he believes hundreds of millions of Americans have been affected by this.  Cambridge Analytica is just the tip of the iceberg.  I would just say I'm not at all surprised.  This is what Facebook does.  What do you expect Facebook to do?  This is what this is the business of Facebook.  How could Facebook create a business model that doesn't involve selling our information to the highest bidder?  Facebook says we're going to do a forensic analysis of every app ever written.

Start time: 1411.02
End time: 1414.94
Speaker: SPEAKER_07
Transcript:  Yeah, let me guess it'll come back saying.  How are you going to do that?

Start time: 1415.30
End time: 1434.59
Speaker: SPEAKER_05
Transcript:  I mean, it could be far more anonymized than it is.  Zuckerberg explained, which is true, that a lot of this goes back to when Facebook decided to be a platform  and opened up the ability for third parties to integrate themselves with the Facebook platform.  If that aspect of Facebook didn't exist, less of this would happen.  Does it even matter?

Start time: 1435.06
End time: 1450.92
Speaker: SPEAKER_08
Transcript:  There's a Delete Facebook movement.  You can go to DeleteFacebook.com to learn how to do it.  Does it even matter, though, if people do that?  Wouldn't everybody?  Somebody said, even if 10% of the United States deleted its Facebook page,  it's more than made up for in one day by the rest of the world joining Facebook.

Start time: 1451.04
End time: 1454.70
Speaker: SPEAKER_05
Transcript:  And there's no particular evidence that there is a mass migration out of Facebook yet?

Start time: 1455.10
End time: 1474.00
Speaker: SPEAKER_08
Transcript:  I actually did delete my account, and last night my wife said,  you know, there's a side effect to this you may not realize.  I am now married to nobody.  On Facebook it says Lisa LaPorte is married, but it doesn't say to whom because I don't exist on Facebook.  And that made me feel so bad, sweetie.  I logged back in and reactivated my account.

Start time: 1475.30
End time: 1536.90
Speaker: SPEAKER_01
Transcript:  And I kind of feel like it's counterproductive if what we want them to do is to fix this  and to realize the potential of what Zuckerberg talks about,  connecting everyone and having this amazing experience,  then maybe quitting isn't the right protest step.  Maybe we should take a page from these youngsters that we've seen this weekend making their voices heard  and really have people make their voices heard.  What would happen to Facebook if the web rallied around this the way they did about opposing SOPA and PIPA a few years back  and had a day where people didn't use Facebook or even, you know, they didn't interact.  They didn't like or love or laugh or on any of the posts.  They checked in, they got their news, but they didn't react.  That would have a huge impact, I would think.  It would send a message to Facebook.  Yeah, it might.

Start time: 1537.68
End time: 1559.84
Speaker: SPEAKER_05
Transcript:  And I mean, despite everything, I don't agree with the folks who say that Facebook inherently makes you miserable  and is inherently pernicious and should go away.  I think Facebook, along with this infinite number of problems, also has an infinite number of ways  in which it has potential to make the world a better place.  I don't think Zuckerberg is insincere when he talks about that.  That is also true.

Start time: 1560.80
End time: 1594.80
Speaker: SPEAKER_01
Transcript:  And Leo, you asked about the ad model and I think sort of cast it in a way that it's a foregone conclusion that it's a negative.  But I think when Facebook began tinkering around with all this, we all thought maybe they're really going to figure this out.  Maybe they're going to give me ads I want to see and that are useful and that are not trying to manipulate my political opinions  and in ways that are underhanded.  I mean, when we see political ads on TV, which we do all the time, we know what we're getting into there.

Start time: 1595.14
End time: 1605.56
Speaker: SPEAKER_08
Transcript:  Well, partly because the law requires they be identified by the purchaser of the ad time, which the law does not require of digital media.  I think that's one thing we can change right now.

Start time: 1606.24
End time: 1625.00
Speaker: SPEAKER_01
Transcript:  Yeah. And if you saw Zuckerberg's interview on CNN, and I assume he said this elsewhere, when he was sort of referencing,  maybe we should be regulated more. It seemed like that was the direction he was going in, that there is an ad transparency bill that's been sort of sitting around Congress for a while that hasn't been enacted.

Start time: 1625.14
End time: 1630.00
Speaker: SPEAKER_08
Transcript:  I think it's Mark Warner's bill, right? Senator Warner from Virginia.

Start time: 1631.04
End time: 1673.00
Speaker: SPEAKER_01
Transcript:  Yes. And John McCain, I believe, is the only Republican who's bought into it.  But it seems to me when I hear that, and I hear him, you know, inviting regulation of that kind, but then putting money into opposing, for example,  the California proposition that we may see on the ballot, the California Consumer Privacy Act.  It seems like that could be a really nice opportunity for California to once again lead the way in privacy law, be the tail that wags the dog,  and get a lot of other, get compliance that affects a lot of people that don't live in California.

Start time: 1673.42
End time: 1685.98
Speaker: SPEAKER_08
Transcript:  There you see the response from Facebook and Google. They're spending big bucks, according to the Sacramento Bee, to fight the California data privacy measure.  They understand that their business model is surveillance capitalism. They don't want any limitation on that.

Start time: 1686.02
End time: 1766.90
Speaker: SPEAKER_01
Transcript:  Yeah, I think they're, you know, if they don't want to just punt and lose their business model altogether,  I think they're going to have to pay attention to this because they're either going to be regulated or I do think that the users have sort of said times up on this.  That there's a lot of concern, especially I would think among young people who are not happy about where net neutrality is gone in the last year or so.  And if they become educated and informed enough to know that they are being manipulated by these companies when they don't have to,  when there are ways in which you can do an ad system that is very lucrative, that's not violating people's expectations and consent,  that they're going to take action. And maybe this bill in California is the first move toward that kind of step.  I almost feel like Zuckerberg's embracing of the ad transparency bill in Congress is sort of an entree to be able to go to advertisers and say,  gee, you know, we've been accommodating you guys for so long, but now we're getting regulated and I'm sorry, we're going to have to scale back.  And it's really not our fault. Congress is making us do it. So maybe that's a step in the right direction, too.

Start time: 1767.64
End time: 1774.00
Speaker: SPEAKER_08
Transcript:  What should what should we do as users? Should we stay on Facebook and try to change things from within?

Start time: 1774.04
End time: 1780.00
Speaker: SPEAKER_07
Transcript:  One of the most depressing tweets I saw this week was, that's it. I'm deleting Facebook, shifting to Instagram.

Start time: 1780.36
End time: 1791.92
Speaker: SPEAKER_08
Transcript:  And what's that? I don't think it makes any difference to Facebook or to my personal privacy, whether I delete Facebook or not.

Start time: 1792.96
End time: 1821.00
Speaker: SPEAKER_07
Transcript:  It's a gesture. It's not effective. I'm not so sure. I'm starting to think, you know, traditionally social networks have done that thing where they grow, grow, grow,  and then something better comes along and they die on the vine. And it happens to Friendster, it happens to MySpace.  And we always thought it wouldn't probably wouldn't happen to Facebook because they got scale. You know, they've got over a billion users.  It's going to be very hard to dislodge that. But something like this. And, you know, they were very keen throughout the entire week to make it about this is not a data breach.

Start time: 1821.36
End time: 1843.62
Speaker: SPEAKER_08
Transcript:  It's not a data breach and not they don't like the word breach. Yeah. But really, they should embrace it because breach implies that they didn't want it to happen.  But then they did eventually say it was a breach of trust. Yes, very carefully put, which was sort of a breach in the sense that somebody hacked Facebook and stole the data.  They hacked the Social Engineering Act. It's such a legal distinction.

Start time: 1844.06
End time: 1880.74
Speaker: SPEAKER_01
Transcript:  Well, if it's a data breach, you know, California, among other states require you to advise users, send out letters and say, oh, gee, sorry, we've not do that.  So if you're a data has been subject to a breach and here are the steps you can take and there's a whole lot of transparency that's required around a breach.  So if we're not going to call it a breach that has legal significance and there's at least one professor out there, Ido Kilvati, who published at TechCrunch on this, that thinks we should amend those data breach laws to encompass something like this.

Start time: 1882.32
End time: 1945.00
Speaker: SPEAKER_08
Transcript:  Sometimes I feel like maybe it's just me. It's so overwhelming. And it's just I give up. It's like I'm just go home and watch Gilligan's Island.  I don't. It's like what can we had Cory Doctorow on the new screensaver yesterday and Cory always cheers me up a little bit.  So there's a number of things we can do. One, there are little small leveraged things you can do in Congress that would help in a lot of this.  For instance, Congress has made it so that it's difficult or impossible. Correct me on this, Denise, to sue over these kinds of things.  They require it's possible for a company like Equifax to require binding arbitration instead of a lawsuit over a data breach.  And they always do pretty much.  And he said if we could change that law just to allow us to sue these companies, that would go a long way.  You know, hit them in the pocketbook. That would go a long way to making a difference. Is that the case, Denise?

Start time: 1945.90
End time: 1968.00
Speaker: SPEAKER_01
Transcript:  Completely agree. Yeah, that Cory's right on about that.  And the reason that binding arbitration is in so many of these terms of service that we have with various companies that we do business with is that litigation is so much more expensive.  So if you're if you're taking that cost savings off the table for them, I think it definitely makes a difference.

Start time: 1968.74
End time: 1984.96
Speaker: SPEAKER_08
Transcript:  But then also, and I'm sure they would argue this has all sorts of unintended consequences. You get lots of frivolous lawsuits.  You tie up the courts. If you could if you could say only good lawsuits, that would be good. Right.  Only lawsuits with merit. That would be good. Right. But you can't. It's not how it works.

Start time: 1985.54
End time: 1996.00
Speaker: SPEAKER_01
Transcript:  This proposed California privacy law would allow consumers to sue businesses for security breaches of consumer data, even if the consumers cannot prove injury.

Start time: 1996.10
End time: 2000.00
Speaker: SPEAKER_08
Transcript:  Oh, that's another one. Yeah. That's right. You have to prove that it hurt.

Start time: 2000.32
End time: 2010.00
Speaker: SPEAKER_07
Transcript:  That was a big problem with the NSA spying case because they initially threw out cases.  Well, you can't prove that the NSA was spying on you. Therefore, you can't bring a lawsuit against them.

Start time: 2011.40
End time: 2020.00
Speaker: SPEAKER_01
Transcript:  Standing is it happened to you.  Damages or harm mean that you were actually hurt by it and you have to be able to palpably show that.

Start time: 2020.50
End time: 2031.02
Speaker: SPEAKER_08
Transcript:  And that is, by the way, that's a kind of a standing law or a point of point of view in the United States. Right.  That there's no if there's not actual damages, you have you have no cause.

Start time: 2034.19
End time: 2041.86
Speaker: SPEAKER_01
Transcript:  Right. Well, this law would it's there.  There's always some form of damage if you've been injured. It's just a quite summer easier to hurt my feelings.

Start time: 2043.39
End time: 2055.94
Speaker: SPEAKER_05
Transcript:  Right. Hurt my feelings. Facebook.  Although there is a layer of this we haven't discussed yet, which is whether what Cambridge Analytica does is actually all that valuable to their customers.  Well, there's there's always been that.

Start time: 2056.00
End time: 2068.00
Speaker: SPEAKER_08
Transcript:  And the Cambridge Analytica may be overstating the value of what they're doing.  And they took credit for Trump winning. And in fact, maybe they didn't have anything to do with it.  And on and on and on. There's a Mother Jones article that implied it was basically a Ponzi scheme.

Start time: 2068.38
End time: 2099.00
Speaker: SPEAKER_05
Transcript:  Yes. And the Mercer family who funded Cambridge Analytica and also gives money to a lot of Republican candidates.  The money kind of cycles from the Mercer's to the campaign who really has to give money to Cambridge Analytica because the Mercer's tell them to.  Which is good for the Mercer's. And it's not it's just not entirely clear whether this is quite as powerful in its form as used by companies like Cambridge Analytica.  As Cambridge Analytica, which seems to like to make up claims about what they're doing, says it is.

Start time: 2099.72
End time: 2113.00
Speaker: SPEAKER_08
Transcript:  Would you agree, though, that if this information got to the troll farm at the Internet Research Agency, it could their use and manipulation of Facebook, not merely with ads, although ads for sure, but creating phony groups, creating phony YouTube videos,

Start time: 2114.14
End time: 2122.06
Speaker: SPEAKER_05
Transcript:  that maybe this could be much more damaging. When it's used for something that that dirty, whether or not it's effective is less on my mind.

Start time: 2123.52
End time: 2143.36
Speaker: SPEAKER_08
Transcript:  And now we know Kevin Poulsen's article in the Daily Beast. The Daily Beast figured out the Guccifer 2.0 was a hacker.  That was really good. Who leaked a lot of this information to WikiLeaks. The lone hacker. Not a lone hacker.  In fact, he is an officer of the Russian military intelligence director, the GRU.

Start time: 2144.00
End time: 2149.14
Speaker: SPEAKER_05
Transcript:  He forgot to turn on his VPN one time. Just like the drug-prone Robbins. He came from inside the house.

Start time: 2150.50
End time: 2206.42
Speaker: SPEAKER_07
Transcript:  Yeah, well, I spoke to somebody at a security conference a couple of weeks ago, and they'd spoken to Guccifer.  And they were like, I'm pretty sure it's two or three people. And there's no way they're Romanian, because I threw some Romanian slang at them.  They weren't going for it. They didn't understand what I was talking about. So it doesn't surprise me that it was coming from the GRU.  It also doesn't surprise me they didn't use the VR VPN, because everybody slips up sooner or later.  We saw it with Robert Roberts. We saw it with everyone else. Everyone mucks up. But all the same, oh, we were talking to Guccifer.  We've now discovered their Russian military intelligence, but we didn't work with the Russians. I'm having problems reconciling those those facts.  It's clear now that the Russians were playing a part. How influential it was and who knew about it at the time.  I think that's going to be the big question to answer. That was a very eloquent shrug, Leo.  We're never going to know, are we?

Start time: 2207.30
End time: 2240.74
Speaker: SPEAKER_08
Transcript:  Yeah, I mean, clearly that's not a good thing. And the Russians are not our friends.  And they've been able to use these tools against us.  I don't know what we should do about it.  Corey also said, and I think this is a good message as well, that as geeks, we should promote and maybe even create ourselves social networks that don't rely on surveillance.

Start time: 2242.00
End time: 2248.00
Speaker: SPEAKER_07
Transcript:  Do you remember from about 18 18 months ago that was going to be the next big thing and it just kind of with it.  What happened to Mastodon?

Start time: 2252.12
End time: 2252.39
Speaker: SPEAKER_08
Transcript:  Yeah.

Start time: 2253.26
End time: 2256.50
Speaker: SPEAKER_01
Transcript:  OK, so question for you all. Do you know anyone under 30 who uses Facebook?

Start time: 2257.83
End time: 2258.80
Speaker: SPEAKER_05
Transcript:  No, they all use Instagram.

Start time: 2259.62
End time: 2266.80
Speaker: SPEAKER_08
Transcript:  Well, that's a good enough. Although you're not giving them the same information by posting a picture that you would be with.  I don't know. My nieces use it.

Start time: 2267.06
End time: 2285.96
Speaker: SPEAKER_07
Transcript:  But then when we were down the pub, they explained that they have a Facebook account which is linked to their parents Facebook account.  But they very seldom use it. It's kind of like the camouflage account, if you like.  That's what they want their parents to know.  And then they go to Snapchat or Instagram or kick or a bunch of other services to actually converse that way.

Start time: 2286.83
End time: 2303.14
Speaker: SPEAKER_08
Transcript:  Well, I think we turns out kids were smarter than we realized.  They use Snapchat too, right? Where they feel like they're not giving away this stuff.  They sensed somehow that this wasn't a good idea.  And meanwhile, us older farts kind of say, it's good. I'm going to meet my high school girlfriend.  Yeah.

Start time: 2304.10
End time: 2304.84
Speaker: SPEAKER_04
Transcript:  She's still hot.

Start time: 2305.62
End time: 2314.00
Speaker: SPEAKER_01
Transcript:  Right. That's why I feel like Facebook and Google and everyone else who's managing our data and doing what did you call it, Leo?

Start time: 2314.65
End time: 2315.96
Speaker: SPEAKER_08
Transcript:  Surveillance capitalism.

Start time: 2316.02
End time: 2350.98
Speaker: SPEAKER_01
Transcript:  Surveillance capitalism. That if they're not getting the message that, you know, it's not just that their current base of users is upset about it.  Their future base of users is just steering clear.  Yeah.  And lawmakers are bearing down on them from state, federal and international fronts.  Did you all read the Guardian story where the information commissioner's office in the UK was storming Cambridge Analytica in their jackets that look like DEA jackets here in the US?

Start time: 2351.50
End time: 2388.29
Speaker: SPEAKER_07
Transcript:  It was the most slow motion storming you've ever seen though.  Because this broke on the Friday and then the information commissioner said, OK, well, we're going to go in.  We should have a search warrant organized by about Tuesday.  And then it came out that Facebook had sent their own forensic team in there.  And they're like, OK, well, we'll get it the next day.  And they went in on Friday, at which point all they're going to find is a bunch of wiped servers and a slightly shredded memo saying, remember, chaps, everybody obey the law.  And, you know, they missed their chance completely.  It was a shameful display by the ICO.  I want to give credit.  I can't give a photo op though.

Start time: 2389.08
End time: 2389.20
Speaker: UNKNOWN
Transcript:  Yeah.

Start time: 2390.00
End time: 2400.31
Speaker: SPEAKER_08
Transcript:  I like it that they have a period at the end of ICO.  Oh, come on. Punctuation's important.  ICO, period.  Or full stop.  Enforcement or full stop.  We are the ICO.  Full stop.

Start time: 2401.90
End time: 2409.98
Speaker: SPEAKER_01
Transcript:  It's a reminder that each state of the United States and the federal government could have their own equivalent of that before too long.  Even if their...

Start time: 2410.00
End time: 2462.00
Speaker: SPEAKER_08
Transcript:  I should credit to Harvard academic Shoshana Zuboff for the term surveillance capitalism.  Not the first use according to Wikipedia.  John Bellamy Foster and Robert McChesney wrote an article using that term in Monthly Review.  But Zuboff is writing a book called Surveillance Capitalism.  Here's how she defines it.  She says surveillance capitalism emerged due to the coupling of the vast powers of the digital, which we've all talked about, with the...  I love this.  The radical indifference and intrinsic narcissism of the financial capitalism and its neoliberal vision that have dominated commerce for at least three decades, especially in the Anglo economies.  And I think the Russians might have looked at us and said, oh, we found your weak spot.  You all made fun of us for communism.  But capitalism has its own problems.

Start time: 2462.42
End time: 2465.73
Speaker: SPEAKER_07
Transcript:  Oh, capitalism has plenty of problems.  Yes.  But I mean, this is...

Start time: 2466.24
End time: 2494.94
Speaker: SPEAKER_08
Transcript:  She says Google is to surveillance capitalism what Ford was to mass production.  They perfected it.  They perfected it, later adopted by Facebook and others.  It uses illegible mechanism.  I like that word, illegible mechanisms.  In other words, invisible mechanisms of extraction, commodification, control of behavior to produce new markets of behavioral prediction and modification.  She's brilliant.

Start time: 2495.00
End time: 2497.33
Speaker: SPEAKER_01
Transcript:  She's Shana Zuboff, by the way.  I can't wait to read this book when it comes out.

Start time: 2498.72
End time: 2500.80
Speaker: SPEAKER_08
Transcript:  We'll get her on triangulation, of course, immediately.

Start time: 2502.38
End time: 2516.73
Speaker: SPEAKER_01
Transcript:  She wrote a book probably more than a decade ago now called The Support Economy that basically forecasted everything that Amazon has become, first of all, and all our personal assistance and everything else.  She's really smart.

Start time: 2517.00
End time: 2556.58
Speaker: SPEAKER_08
Transcript:  Google's chief economist Hal Varian identified the four key features in the logic of surveillance capitalism.  One, the drive toward more and more data extraction and analysis.  Two, the development of new contractual forms using computer monitoring and automation.  Three, the desire to personalize and customize the services offered to users of digital platforms.  And finally, the use of technological infrastructure to carry out continual experiments on its users and consumers.  If I could pick the one thing that most bothers me about this is I feel like a guinea pig.  I feel like a lab rat.

Start time: 2557.00
End time: 2567.45
Speaker: SPEAKER_07
Transcript:  You already are though, because I mean Facebook, you remember Facebook got caught out doing this thing where they had an academic study where they would alter people's news feeds and see if they could get them to get depressed or.  We know that.

Start time: 2568.00
End time: 2577.46
Speaker: SPEAKER_08
Transcript:  Facebook even published a study that said too much Facebook gets you depressed unless, boy this is a little self-serving of Facebook, you put more content on Facebook, then you won't be as depressed.

Start time: 2578.32
End time: 2598.59
Speaker: SPEAKER_07
Transcript:  How fortunate that was for them.  But I mean we know this stuff works.  We know that you can alter people's perceptions by altering the news feed.  What I do find somewhat of a stretch is whether or not you could actually change someone's political opinion using these kind of techniques because that seems to be something fairly deep rooted.  I don't even think that's as much of a problem.

Start time: 2601.12
End time: 2653.00
Speaker: SPEAKER_08
Transcript:  Yeah, because I think that really we talk a lot about fake news and how it, but I think it was embraced by people who already believed it.  It was just a self-reinforcing mechanism.  That's not what bothers me so much as the way it can be used and it has apparently been used by Russia to see discord and dissent and polarization.  And if there's anything that really has kind of frustrated the Republican experiment, lowercase R experiment that the United States is, it's this polarization that we've all observed over the last 10 years where the two sides don't even talk to one another.  And that seems particularly pernicious if a foreign entity which doesn't like the liberal West decides, hey we can really screw with them.  That's the ultimate, I mean literally the word trolling, that's the ultimate trolling.  We don't care what the result is, we just want to throw chaos into the mix.

Start time: 2653.64
End time: 2681.98
Speaker: SPEAKER_07
Transcript:  Well, it was the Russians who first invented the word disinformation, if you look back in the 1920s.  But yeah, I'm seeing the same thing happen in the UK at the moment.  Last time I was over there, everyone's tremendously polarized around are you a Brexiteer or are you a Remainer?  And over here, the level of tribalism in politics is just damning.  I mean there are people who won't even have friends who are of a different school of persuasion and that's how you come around it.  We need to get over that. Absolutely.

Start time: 2682.30
End time: 2698.56
Speaker: SPEAKER_01
Transcript:  And as far as changing people's minds go, I agree with you that I don't think that it's going to necessarily alter your political opinion.  But what it might do is get you from a point of apathy and wanting to just stay home and watch Gilligan's Island, right, to going out and voting.

Start time: 2699.24
End time: 2719.42
Speaker: SPEAKER_08
Transcript:  That's a good point. And we do know that there were voter suppression ads as well that were very effective apparently in suppressing the vote.  But again, long tradition of voter suppression in a variety of different ways in the US political elections.  By the fact that certainly on the democratic side, the candidate wasn't really very...

Start time: 2720.20
End time: 2728.82
Speaker: SPEAKER_07
Transcript:  A lot of people are like, well I'll hold my nose and vote for Hillary, but I don't really feel...  And if you've got that kind of level amongst everyone's Democrats, it's easier to say, well maybe you should just say it.

Start time: 2729.14
End time: 2779.00
Speaker: SPEAKER_08
Transcript:  So maybe that's really what it is. It can't change your mind, but it can tilt you, it can push you over. If you're tilting, it'll push you over the top maybe.  Anyway, to me again, I feel like a lab rat. I feel like...  And I think this is really the potential negative of this. Look, we love technology. That's why this network exists, that's why I do what I do.  I love the potential for technology, I love playing with it, I think it's great in so many ways.  And so I don't want it to become this tool to abuse us. I don't want it to become used against us.  And so we need to find ways to make it so that it isn't used against us and can be used for good.  And frankly, I think you made the point. Facebook can be used for good. It's not inherently evil.  It's just that it's so sad when it's used in this terrible way.

Start time: 2779.42
End time: 2788.00
Speaker: SPEAKER_05
Transcript:  I do think looking a tiny bit on the bright side, Mark Zuckerberg has always been terrified of it all going away and something else taking over from Facebook.

Start time: 2788.08
End time: 2789.66
Speaker: SPEAKER_08
Transcript:  He knows because he did it to iSpace.

Start time: 2790.02
End time: 2795.98
Speaker: SPEAKER_05
Transcript:  Which is why he bought Instagram and WhatsApp. And I think that might be a little bit of a mediating factor in terms of him.

Start time: 2796.00
End time: 2799.00
Speaker: SPEAKER_08
Transcript:  He's a smart guy! Maybe he can find a solution.

Start time: 2799.40
End time: 2818.76
Speaker: SPEAKER_05
Transcript:  Because there is a scenario where the stuff that's happening does lead to people pulling back on Facebook use and not trusting that and advertisers no longer wanting to be associated with it.  And that might push them in a direction of doing some smart things that would lead people to say, Facebook is taking this seriously and is doing the right things.

Start time: 2819.92
End time: 2830.90
Speaker: SPEAKER_07
Transcript:  Yeah, I think it certainly does kill his presidential ambitions for the short.  The very chance that we're going to sort of, yeah, we'll let Mark Zuckerberg run for president.  And he's got all the Facebook data.

Start time: 2831.02
End time: 2831.82
Speaker: SPEAKER_08
Transcript:  Never say never though.

Start time: 2832.14
End time: 2834.92
Speaker: SPEAKER_05
Transcript:  He is already a pretty darn powerful person.

Start time: 2835.76
End time: 2840.00
Speaker: SPEAKER_08
Transcript:  That's what everybody said. Why would he want to be president? He's got much more power running Facebook. He's global.

Start time: 2840.38
End time: 2849.52
Speaker: SPEAKER_05
Transcript:  And this is going to get more evidence that Facebook is more or less a country unto itself and it has powers that in the past you would have associated with countries rather than technology companies.

Start time: 2850.84
End time: 2862.04
Speaker: SPEAKER_01
Transcript:  Right. Leo, you said a moment ago, is there going to be some upstart social network that addresses all these problems and comes to users and say, we know you, we respect you, we can do both those things.

Start time: 2863.32
End time: 2871.00
Speaker: SPEAKER_08
Transcript:  Wouldn't you jump? If there were an alternative to Facebook, I would jump in a moment. I do jump. I jumped on Mastodon, but nobody I knew followed.

Start time: 2871.52
End time: 2878.00
Speaker: SPEAKER_07
Transcript:  So I went on to try and make it work and that didn't work either.  It's just so do you think that's possible, Denise?

Start time: 2878.02
End time: 2919.52
Speaker: SPEAKER_01
Transcript:  Well, one little data point. I don't know.  I've been watching this show on the Science Channel called Silicon Valley, the Untold Story.  It's just this three episode whirlwind tour through Silicon Valley from the 50s until today.  And the theme that they come back to over and over again is impermanence and how people leave and start something else that completely disrupts what came before, disrupts the company they worked at before.  So one little data point that I thought was interesting in the middle of all this is that Facebook's information security officer left.  Twitter's information security officer left. I don't know. Maybe they'll all get together and do this.

Start time: 2920.36
End time: 2984.34
Speaker: SPEAKER_08
Transcript:  I saw a post on Twitter. Dave Morin, the guy who created Path, said, you know, maybe we should get the old gang back together.  He sold it to a Chinese, Japanese, Korean company.  Maybe we should get the old gang. Would there be an interest in?  And by the way, that's why I joined Path. It was limited to 50 people. It didn't seem to have the same kind of intense surveillance behind it.  He said, maybe we should get the old gang back. I hope, Dave, anything we can do to support and help that.  And Mastodon Gargron, the guy who created Mastodon, wrote a medium post saying, don't forget there are social networks out there that aren't trying to.  Yeah. Now, it's funny because Twitter, which I don't think really is very good at surveillance capitalism and certainly doesn't have as much information,  but it has other problems that make it less, you know, not the utopia we thought it would be.  It's it's painful to go there, too. It can be it can be ugly. And at the same time, huge value. Right. Yeah.  Huge value. Google. Go ahead and try to live without Google. Yeah, you can.

Start time: 2985.26
End time: 2994.58
Speaker: SPEAKER_01
Transcript:  Well, I feel like this week has been a huge boon to both of them. Right.  Mike Elgin is talking about how it's the resurgence of Google Plus. Oh, no. Really? Yeah. Really?

Start time: 2996.26
End time: 3000.00
Speaker: SPEAKER_07
Transcript:  I love I love Mike Daly, but he does have a bit of a blind spot. He's listening right now.

Start time: 3000.34
End time: 3007.88
Speaker: SPEAKER_01
Transcript:  I'm sure. Mike, really? Google Plus. Really? But realistically, I feel like a lot of people are looking at Twitter in a new light. Yeah.

Start time: 3008.28
End time: 3031.30
Speaker: SPEAKER_05
Transcript:  Yeah. Yeah. I mean, Twitter for a long time, Twitter got beat up more than anybody else for not dealing with stuff very well.  And now we know that Facebook is not dealing with it well. YouTube, which you haven't really mentioned, has all kinds of all sorts of.  Oh, yeah. Huge. Google. I mean, Google. Other than YouTube, Google is not blown up quite as much, but it probably will at some point. Right.

Start time: 3032.34
End time: 3041.00
Speaker: SPEAKER_07
Transcript:  I think for a number of reasons, both this Facebook story and also the Uber story, there are a lot of people at Google just going,  Wow, we dodged some bullets this week because this could have been really ugly for us.

Start time: 3041.08
End time: 3070.00
Speaker: SPEAKER_08
Transcript:  But what I'd really love them to do is to say, now, let's figure out a way to make something of value that doesn't make people feel like lab rats.  Yeah. Because I think the world is ready for that. And I think that people would jump on that.  And Mark Zuckerberg, that is the real existential threat to Facebook. And you could do it too, Mark.  You know, you could save Facebook. There are ways to do this. If anything good comes out of this, it'll be that.

Start time: 3071.39
End time: 3074.00
Speaker: SPEAKER_07
Transcript:  We don't have to burn the house down. Just do some improvements on it.

Start time: 3074.38
End time: 3077.94
Speaker: SPEAKER_08
Transcript:  We recognize there's value to it. Let's see how we can make it work.

Start time: 3079.63
End time: 3084.00
Speaker: SPEAKER_07
Transcript:  It would be nice if it works out. I think what they're worried about is the share price, because that's what really got Facebook.

Start time: 3084.12
End time: 3116.85
Speaker: SPEAKER_08
Transcript:  Well, that puts you in a death spiral, right? Because if your share price plummets, which you did Facebook last fifty billion dollars in market value,  then you can't attract the best talent because you don't have shares that are worth anything.  And then there's a death spiral because then you can't get good people and it gets worse and worse and worse.  I don't know if they're at that. I doubt they're at that point. I think their stock price has recovered somewhat.  I imagine it will continue to do so.  I don't feel equipped to cover these stories, to be honest with you.  I just...

Start time: 3118.89
End time: 3120.00
Speaker: SPEAKER_01
Transcript:  If you're not, Leo, who is?

Start time: 3120.04
End time: 3131.00
Speaker: SPEAKER_08
Transcript:  I got into tech because I thought I could write reviews of software and get free computers. I didn't know.  I didn't know. I thought this is fun. This is the toy store, right?

Start time: 3131.28
End time: 3155.52
Speaker: SPEAKER_07
Transcript:  Well, do you remember the Hacker Manifesto back in the mid-90s?  The internet was going to bring all these diverse groups together. We'd all talk. We'd understand each other.  I was rereading it last week when all this broke.  And it was just like, I wish it had worked out that way. But instead we've got polarized.  We've got crammed into narrower and narrower silos.

Start time: 3156.06
End time: 3242.76
Speaker: SPEAKER_08
Transcript:  Well, one of the tenets of the Hacker Manifesto was information wants to be free.  And in fact, that's to me the most important thing.  And that means not siloed, not owned by any company, not gathered to be used against you.  It wants to be free. And that's the real promise of the internet.  And you know, it's still hugely valuable.  I think now every day five or six times of premises, of things that I would have in the past had to go to a library and research,  which meant I could do maybe one every six months.  Now I can in several times a day go out, find out something, do some research on something. And that's amazing.  It's an enormously valuable tool.  Huge. I remember my father-in-law, who was a science professor.  We gave him for the first time, he's passed away since, but an iPad with the elements program on it and some other science programs on it.  He was a science teacher. And he said if Copernicus had had this, Copernicus spent most of his life grinding lenses  so that he could observe the heavens, so that he could come up with the theory that the earth revolves around the sun.  If he had something like this, he wouldn't have had to grind all those lenses.  He could get the data and then spend his energy coming up with theses.  And that's the world we live in today. The data is now freely available. It is not a huge cost.  And I think kids growing up today have a huge opportunity, if they will use it, to become great synthesizers.

Start time: 3243.44
End time: 3278.98
Speaker: SPEAKER_07
Transcript:  But you've got to be able to handle data critically. This is the thing.  That's right. Absolutely.  We used to have a class at school called Media Studies when I was growing up where you've seen as an easy option,  but it's actually quite tough when you work into it where you watch TV programs and talk about newspaper articles  and you work out why it's written the way it's written.  And I think if we could have some kind of social media education, either coming from parents or, God help us from the schools,  I'm not quite sure that would work, but I mean some kind of parents can educate their kids about,  this is what social media is about, this is what they take from you, this is what they expect in return, and these are the benefits.  Absolutely. It's an education issue.

Start time: 3280.06
End time: 3331.03
Speaker: SPEAKER_01
Transcript:  And getting back to shifting business models again, what you guys are talking about, information wants to be free.  When the blogging movement first took off, one of its tenants were, we can fact check your ass.  Yes.  It becomes really hard to fact check and get your fact checking seen and vetted and taken as, oh yeah, you really took that down  when algorithms are fighting you in getting your information out there.  And I think that's one of Facebook's challenges.  And when you throw it back to why Google is successful all these years later, it's because their algorithms in their original search product  were providing you with the actual relevant information you were looking for.  They were not providing you with what was promoted or paid for or otherwise.  Absolutely right.

Start time: 3332.68
End time: 3579.51
Speaker: SPEAKER_08
Transcript:  And that segues right into our first ad of the show, an hour and a half in.  And guess who it's from? Google.  Our episode this week brought to you by Google Cloud, the Google Cloud Platform, which, hey, really is amazing.  For anybody who wants to create the next social network, create the next great thing, when you're building an application,  you know the Google Cloud Platform is the place to be.  And this particular episode brought to you by the Kubernetes Engine on Google Cloud Platform.  Fast, secure, always evolving with the Kubernetes Engine, developers can easily deploy containerized apps on a fully managed service  from Google Cloud Platform.  Nobody does it better than Google.  They scale.  They've been running production workloads in containers for more than 10 years.  Oh, sure.  You just heard about it, but they've been doing it for more than 10 years and they have built the best of what they learn into Kubernetes Engine.  Kubernetes Engine combines automatic scaling, updates, reliable self-healing infrastructure.  That's all the kind of thing you really need.  But it also has open source flexibility.  So you cut down your development cycles.  You can move from idea to production quickly and reliably.  I remember when my friend Kevin Rose started Digg.  He had to get a co-location, buy a server, put it in the cage, set the whole thing up.  The next great application is out there and Kubernetes Engine is ready to handle it with scheduling deployment for your workloads to maximize resource optimization.  You can focus on your apps.  It auto scales.  So when you succeed, no problem.  Those increased demands go up.  And by the way, quiet periods.  It scales down.  You save money.  That is really nice.  Google is the only hyperscale cloud provider to offer an SLA for clusters running on Kubernetes.  It's also backed by experts on Google's security and reliability engineering team, HIPAA compliant, PCI compliant.  And of course, it's open.  So you're never going to get locked in with Kubernetes Engine.  You're free to take your workloads out, run them anywhere Kubernetes is supported.  Learn more about the great implementation of the Kubernetes Engine at g.co.  slash get G.K.E.  today.  That's G.co.  slash G.E.T.  G.K.E.  Get Google Kubernetes Engine.  G.co.  slash G.E.T.  Get G.K.E.  That's the short URL.  Google Cloud Platform.  That's a really good example of the really empowering capabilities of these companies.  They can really empower people to do some amazing things.  It's why everybody, well, not everybody, but learn to code, learn to code.  I think, you know, great media literacy.  It's good.  But if you have any interest in coding, learn to code.  The sky's the limit.  We need people who are aware and alert and smart and media literate and are thinking about this stuff to go out there and write the next great app and do it ethically.  I love that.  All right.  Moving along.  Thank you, Denise, for being here.  We brought you here actually to talk about another issue which is very hot.  All of a sudden Congress, which hasn't been able to do anything, is getting ready for, I want to say, siesta.  They're getting ready for the session to end.  And so they thought, well, we better get to work and do a few things.  Obviously they didn't want the government shut down.  So they passed a, what was it, a 1.3, 1.6 trillion dollar budget.  And on page, what was it, 2,024 of this massive omnibus bill, they stuck in a rider that put through the Cloud Act.  So first of all, maybe I better get the lawyer here.  Explain to us, this thing has been hanging around, the Cloud Act's been hanging around for at least a year.  What is the Cloud Act?

Start time: 3580.98
End time: 3647.41
Speaker: SPEAKER_01
Transcript:  It's been hanging around and it's been sort of put on the fast track because of a case before the Supreme Court right now involving Microsoft.  Is this the Irish server case?  Yes.  So this is the Irish server case where Microsoft is saying that they don't have to turn over information to law enforcement related to a drug crime investigation because the information resides on a server that is completely under Microsoft's control but is in Ireland.  So they have taken that case all the way up to the Supreme Court.  So the Cloud Act was argued, yeah, I think just last month, we should have a decision on it soon.  But right there in the chambers, as it was being argued, were lawmakers who are proponents of the Cloud Act as though to emphasize to the Supreme Court members that, you know, we may enact something that makes this completely moot for you.  Well, guess what?  They did.

Start time: 3649.24
End time: 3756.00
Speaker: SPEAKER_08
Transcript:  By sneaking it into the budget, getting it passed and signed, it is the law.  Right.  Let me tell you, this cloud stands for, I love it.  It's a retronym.  The Clarifying Overseas Use of Data Act.  What it does, it creates an explicit provision for, and by the way, thanks to Orrin Hatch, Lindsay Graham, Coons and Whitehouse, the four senators who created the Cloud Act.  It creates an explicit provision for US law enforcement, whether it's the chief of police down the road in Butboy, Utah, or, you know, the National Security, well, I guess it's the FBI, to access or ICE, to access the contents of a wire or electronic communication and any record or other information about a person, regardless of where they live or where that information is.  The information is located on the globe.  Let's say an Irish national and their email is on a Microsoft server in Ireland, for instance.  Furthermore, they don't need a warrant.  US police could compel a service provider, Google, Facebook, Snapchat, to hand over a user's content and metadata, even if it's stored in a foreign country, without following that foreign country's privacy law.  Furthermore, the president can enter into, quote, executive agreements, in other words, no oversight, with foreign governments that would allow those governments to acquire users' data stored in the other country without following each other's privacy laws.  So China could come to Google and say, our Chinese national, maybe a dissenter, had a conversation with Leo that's on your servers.  We would like that.

Start time: 3756.00
End time: 3766.64
Speaker: SPEAKER_07
Transcript:  Well, there is one caveat to that.  They have to tick a box saying we respect human rights.  It's the pinky swear version of sort of doing this.  Oh, my God.

Start time: 3767.58
End time: 3779.70
Speaker: SPEAKER_08
Transcript:  And by the way, this bill passed without being marked up, without oversight, without.  In fact, it was having trouble getting passed in the previously.  It just snuck in at 8 p.m. on Wednesday.

Start time: 3780.24
End time: 3793.60
Speaker: SPEAKER_07
Transcript:  This is what I don't get.  This 2000 plus page bill.  Two thousand two hundred and thirty two page bill.  PM on a Wednesday.  And you have to vote this tomorrow.  And they voted for it on Thursday morning.  This is how nobody can have read that bill.

Start time: 3794.30
End time: 3819.00
Speaker: SPEAKER_01
Transcript:  And it really strikes me as a strange thing for Congress to enact in the first instance.  It really feels more like a treaty.  Right. We're getting some accommodations from other countries in exchange for the ability to empower U.S.  police to go to their countries and grab data regardless if it's a U.S.  persons or not, no matter where it's stored.  So how how is that really something that we get to legislate?

Start time: 3819.50
End time: 3830.57
Speaker: SPEAKER_08
Transcript:  It even bypasses American law on American soil.  So you know, your private data can be sent to a foreign country without a warrant, without any regard.  Without any. Yes.

Start time: 3832.50
End time: 3842.00
Speaker: SPEAKER_05
Transcript:  So is there a good idea here or is this fundamentally a bad idea?  If the idea was explored and discussed and and the kinks were worked out.

Start time: 3842.79
End time: 3849.98
Speaker: SPEAKER_08
Transcript:  Well, what's weird is that I think Microsoft supported this.  I think Google. So I think this was supported eventually.  Initially they opposed it.  And then they took them off the hook.

Start time: 3850.10
End time: 3856.98
Speaker: SPEAKER_07
Transcript:  Yeah, they changed a few key words, which basically reduces their legal liability to zero.  So they like they're like, yeah, OK, we can live with that.

Start time: 3857.34
End time: 3873.98
Speaker: SPEAKER_01
Transcript:  Companies like that like the certainty that this gives it also gives them cover in the way I was saying.  Maybe Mark Zuckerberg wants to be regulated.  So he has cover.  They can say, oops, sorry, we, you know, we would protect your data on our foreign servers, but we can't anymore because Congress said no.

Start time: 3874.00
End time: 3890.64
Speaker: SPEAKER_08
Transcript:  So supported the Cloud Act.  We I mean, these are companies who claim to fight for us.  Yeah.  And yet this is a clear example of them saying, yeah, we fight for you.  But go ahead, pass that bill so we don't have to.

Start time: 3891.81
End time: 3897.90
Speaker: SPEAKER_07
Transcript:  So Congress has tied our hands.  Whoa, whoa, is me.  Yes, of course you can have that data.  So it's just it's shame.

Start time: 3898.30
End time: 3903.97
Speaker: SPEAKER_08
Transcript:  Apple supported it.  Yeah.  Microsoft supported it.

Start time: 3905.24
End time: 3932.76
Speaker: SPEAKER_07
Transcript:  And you remember when Microsoft when the Dublin case first broke and Microsoft were all up in arms about this, just like we will fight for your rights.  They instructed the court to find them in contempt because they weren't prepared to play along with this.  And it was basically because this they knew this would throw an enormous curveball into their into their cloud business.  And now Congress has acted on it and they can say, well, you know, we tried and such is life.  But I guess we're just going to have to obey what Congress says.  Here's the letterhead.

Start time: 3933.34
End time: 3950.60
Speaker: SPEAKER_08
Transcript:  Here's the letterhead sent to Senator Hatch, Graham Coons and Whitehouse in support of the Cloud Act.  Apple, Facebook, Google, Microsoft and oath.  I mean, that's it.  Right. I'm surprised Comcast isn't on there, but they probably supported it, too.  Well, keeping the head down often that neutral.

Start time: 3951.54
End time: 3973.72
Speaker: SPEAKER_01
Transcript:  Yeah. Getting back to Harry's question, I mean, there are some practical realities here that this bill recognizes that, you know, it really makes no difference to Microsoft where its servers are.  It has control over all of that data.  Yeah. And and I think that's what frustrated the lawmakers here or the prosecutors in the drug case that well.  Well, yeah, let me put myself in Microsoft shoes.

Start time: 3974.91
End time: 4010.14
Speaker: SPEAKER_08
Transcript:  This is look at the nature of the way technology works.  It is our data. Your data is going to be all over the place.  We don't want to have to figure out interlocking data protections, data rules of all these different countries is impossible for us.  So we just want need some simple rule that cuts through all of this.  Yeah, I understand that.  I mean, I do understand, especially if you can just, by the way, just add some language that lets us off the hook.  That would be nice.  It's good. Done deal.  So when Apple says your personal data belongs to you.

Start time: 4011.44
End time: 4015.14
Speaker: SPEAKER_07
Transcript:  Well, in certain circumstances, terms and conditions may apply.

Start time: 4016.24
End time: 4028.08
Speaker: SPEAKER_01
Transcript:  I just I think that your mileage may vary.  Yes, exactly.  Can we go ahead and just declare that this whole bill acronym thing has gotten completely terrible.

Start time: 4029.54
End time: 4033.00
Speaker: SPEAKER_07
Transcript:  Oh, the Patriot Act was the worst one protecting America.

Start time: 4033.54
End time: 4063.00
Speaker: SPEAKER_08
Transcript:  Just like they come up with a name and then they retro nim it so that it makes.  The EFF says the Cloud Act is a new proposed backdoor to our data, which bypasses the fourth amendment protections to communications privacy.  The ACLU called the sinister piece of legislation that threatens activists abroad individuals here in the U.S.  would empower the attorney general in new and disturbing ways.  And it supports foreign governments going after dissidents in the United States.

Start time: 4063.00
End time: 4065.87
Speaker: SPEAKER_05
Transcript:  It does seem really worrisome for activists in the U.S. who might well tick off countries.

Start time: 4072.34
End time: 4082.96
Speaker: SPEAKER_08
Transcript:  Opposed by Amnesty International, People for the American Way, Human Rights Watch, the National Association of Criminal Defense Lawyers all said that this bill is a bad idea.

Start time: 4083.20
End time: 4106.90
Speaker: SPEAKER_07
Transcript:  Well, there's no oversight. This is the thing.  I mean, if the executive decides to conduct one of these deals with another country, the only way you'd find out about it is if somebody leaks about it.  I don't believe as in and you can correct me if I'm wrong on this, but I don't believe the legislation requires the government to say who has these agreements with.  And that's tremendously worrying.  And if you're a foreign activist who's based over here.

Start time: 4107.58
End time: 4122.98
Speaker: SPEAKER_08
Transcript:  Well, and if you're a foreign activist, you probably are going to stop using any American products.  Period. Yeah. Right.  Because you no longer have any protections.  So what should you do?  You should start. You should invest in encryption.  Get signal yourself on your signal on signal.

Start time: 4123.14
End time: 4125.08
Speaker: SPEAKER_07
Transcript:  Yeah, it seems to be the gold standard these days.  Yeah.

Start time: 4127.71
End time: 4128.00
Speaker: SPEAKER_08
Transcript:  And you sure.

Start time: 4128.20
End time: 4149.98
Speaker: SPEAKER_01
Transcript:  Well, now would be a really good time to donate to EFF because you know, their lawyers are looking at this.  Do you think there's a way to fight it?  Well, I mean, if they can figure out a way that Congress has exceeded its authority in unconstitutional ways here, then yeah, you know, it's definitely something I'm sure is being examined.

Start time: 4150.00
End time: 4163.24
Speaker: SPEAKER_07
Transcript:  I mean, the courts took down large sections of the Communications Decency Act.  They've done an awful lot of stuff in protecting encryption.  So it does offer a way forward.  And yeah, the EFF are solid and well worth contributing to.

Start time: 4164.78
End time: 4168.68
Speaker: SPEAKER_08
Transcript:  Ray Ozzie, by the way, he's turned on us.

Start time: 4169.58
End time: 4171.70
Speaker: SPEAKER_01
Transcript:  It remind me why we know who Ray Ozzie is.

Start time: 4172.00
End time: 4182.94
Speaker: SPEAKER_08
Transcript:  He created Lotus Notes, right?  Then went to Microsoft.  Microsoft CTO for a little while.  He's the king of kind of shared work environments, right?

Start time: 4183.28
End time: 4188.86
Speaker: SPEAKER_07
Transcript:  He was the guy who was supposed to bring Microsoft into the cloud, but they enough people at Microsoft hated his guts that they forced him out.

Start time: 4189.00
End time: 4191.82
Speaker: SPEAKER_05
Transcript:  He created Groove, which was a collaboration platform.

Start time: 4192.72
End time: 4264.00
Speaker: SPEAKER_08
Transcript:  Widely agreed to be a genius.  Maybe not so much in this respect.  In fact, he and others like him have agreed with the FBI and the Justice Department because, of course, the FBI and the Justice Department don't like it that there's stuff on phones they can't get to.  Although I have to say we know we've seen this report from the Harvard Berkman Center going dark that in fact they have more ways of spying on us than ever before.  They don't.  I mean, the problem is that your phone has everything has your life in here.  And I think needs some specialized protection over and above, you know, just your computer.  I mean, this is you put everything in this phone.  So the Justice Department says, here's what we how about this?  What if there were a mechanism to access the data on your phone that could be engineered without weakening the devices security against hacking some sort of secure enclave that contains the encryption key that would only be accessible to law enforcement?  With the help of a company.  And then we could then you know, we could just go to Apple and say, OK, look, it's a special case.  Could you unlock that phone?  What an amazing idea.

Start time: 4264.00
End time: 4299.00
Speaker: SPEAKER_07
Transcript:  I'm amazed no one's thoughts of that one.  I mean, seriously, we're going to let we're going to set this up and then trust that the government will look after this encryption key and ask the Office of Personnel Management how good they are at that kind of stuff.  Ask the NSA how good it is at keeping its private secrets private.  Or we're going to trust companies like Apple and Google.  I mean, people.  Yes, an encryption backdoor is a fantastic is a great idea in some circumstances, but you can't build a backdoor into a system that no one else can find.  And if you tell people it's there, they will move heaven and earth to find it.

Start time: 4299.41
End time: 4355.22
Speaker: SPEAKER_08
Transcript:  So I agree.  And math says this is the case.  But there are people like Ray Ozzie who he said, quote, This issue is not going away.  And we think that we need to have some constructive dialogue rather than saying it can't be done.  And I understand that people like Ozzie, Stephen Savage, who's a computer science professor at UCSD, Ernie Burkel, former chief security officer at Intel.  These three have been meeting at MIT to try to find a solution.  I understand that they they're probably patriots.  They want to protect the nation.  They feel like the terrorists can use these technologies to operate unimpeded and that law enforcement should have.  And I agree with all of that.  But I do think that there's a really substantive question.  Is it possible to crack this stuff in a way that doesn't give bad guys access to my stuff as well?

Start time: 4356.20
End time: 4379.98
Speaker: SPEAKER_01
Transcript:  Well, as I read this, you guys correct me if I'm wrong, but it seems like the distinction that they're working toward here is maybe having a unique key for each device that doesn't unlock the entire operating system for every other device.  Right. And that's what would be subpoenaed.  So does that make you feel any more comfortable?

Start time: 4380.69
End time: 4399.98
Speaker: SPEAKER_05
Transcript:  And it lives on the device.  Yes.  Yeah, it's a little bit like how Apple Pay works where there's a secure enclave that nobody can get at except for Apple Pay.  This would be a secure enclave that nobody could get at except Apple.  Right. And a government would go to Apple to get at it.

Start time: 4400.42
End time: 4439.23
Speaker: SPEAKER_08
Transcript:  Although, as we can see with the Cloud Act, the standards for what the government has to do to get Apple to agree have shifted dramatically.  Yeah.  And if they could just say, hey, by the way, what's that key again?  Now, I agree with you, Denise.  There's a real benefit to the fact that it's done on a per foam basis.  I think that's a good protection.  And actually, you know, Steve Gibson suggested exactly this solution.  And he said, I think this is a reasonable compromise.  I hope I'm not putting words in your mouth, Steve.  But he said this was some months ago.  This would be a solution that would, you know, at least be limited in its risks.

Start time: 4441.28
End time: 4446.00
Speaker: SPEAKER_01
Transcript:  And I guess it has a precedent.  The New York Times article talks about Symphony in the banking world.

Start time: 4446.44
End time: 4488.76
Speaker: SPEAKER_08
Transcript:  Right.  Symphony, which is an encrypted messaging system for banks, several banks agreed to give copies of their Symphony keys to law firms because Symphony keeps a copy of encrypted data on its servers.  That arrangement created a backup means for investigators to gain access to the messages if necessary.  Christopher Ray, the FBI director, former FBI director, said at the end, the data is he's former.  I can't remember. It's changed so fast.  I think so. At the end, is he still the director?  OK. At the end, the data and Symphony was still secure, still encrypted, but also accessible to the regulators so they could do their jobs.  So OK, so can we so maybe this is a solution that's OK.

Start time: 4490.63
End time: 4500.90
Speaker: SPEAKER_05
Transcript:  It's moving in the right direction, it sounds like.  It's not a master key to every iPhone on the planet.  It's a key to a particular iPhone.  I am still of the opinion that, well, yeah.

Start time: 4501.62
End time: 4507.98
Speaker: SPEAKER_07
Transcript:  I don't know. The FBI managed perfectly well for the first hundred years of its history.  They survived without this.  Yeah. And it just seems like, yeah.

Start time: 4508.00
End time: 4518.98
Speaker: SPEAKER_08
Transcript:  I feel like this is somehow a privileged device.  The data we put in here, it's like accessing my brain.  And that should be protected. And this should be protected.  I'm not an American.

Start time: 4519.00
End time: 4531.73
Speaker: SPEAKER_01
Transcript:  Well, the other distinction that Ray Ozzie makes in talking about this is I'm only working on the device problem, how we unlock the device.  If things are encrypted on the device, I'm not working on that.  Good.  Or at least not for now.

Start time: 4532.36
End time: 4552.92
Speaker: SPEAKER_08
Transcript:  So, but I can tell you that the very next thing that happens.  Yeah. Right.  The FBI says, oh, crap, everybody's using Signal.  We need a back door in Signal, by the way.  And now, oh, well, it's a little easier because we got this.  We could do that.  So I just, it's a slippery slope and I just, I'm not, I'm a guinnet.

Start time: 4554.08
End time: 4578.74
Speaker: SPEAKER_07
Transcript:  Yeah. No, it's the same with Sester and Fester.  It's the start of a slippery slope.  Slippery slope.  Oh, okay. We'll make this exception for child traffickers.  Even though the law itself is probably going to make them more prone to being abused.  It's like, well, next time it'll be, well, there's the terrorist, the terrorist.  That's a good one.  And then before you know it, you're left with nothing.

Start time: 4579.06
End time: 4591.96
Speaker: SPEAKER_08
Transcript:  Let's save. Well, let's get to Sester and Fester next.  But I want to take a little break.  It's great to have Harry McCracken here, Fast Company.  How's things going? Everything good?  Things are great.  You were, you and Marie were down at South By.  Did you enjoy that?

Start time: 4592.00
End time: 4595.35
Speaker: SPEAKER_05
Transcript:  I actually set it out because I had too many colleagues who wanted to go.  You sent Marie and you stayed home.

Start time: 4597.33
End time: 4624.65
Speaker: SPEAKER_08
Transcript:  She's gotten so much trouble down there.  You didn't know anything about that, did you?  No, I'm teasing.  It was a lot of fun though. It was really fun.  We didn't run into each other.  Nice to have you here, Ian.  Congratulations on your promotion, Ian Thompson, news editor at theregister.co.uk.  Which is still a great.  You guys, it's great.  Everybody else is like suffering and you guys are beefing up the staff.  Yeah, yeah.  It seems to be working well.

Start time: 4626.54
End time: 4631.92
Speaker: SPEAKER_07
Transcript:  It's, I think there's something to be said for being slightly snarky and cynical.  Love it.  Love it.

Start time: 4632.35
End time: 4639.92
Speaker: SPEAKER_08
Transcript:  You know what?  It's an antidote to all of the just overwhelming sludge that we're crawling through here.

Start time: 4640.00
End time: 4643.29
Speaker: SPEAKER_07
Transcript:  It does make us popular with companies, but what the hell?  We're not here to be popular.  Right.

Start time: 4644.02
End time: 4650.78
Speaker: SPEAKER_08
Transcript:  That's my attitude, exactly.  And the wonderful Denise Howell.  You've got to listen to This Week in Law every Friday.  What time?

Start time: 4652.91
End time: 4656.94
Speaker: SPEAKER_01
Transcript:  11 o'clock Pacific time, 1800 UTC.  You sound like somebody I know.

Start time: 4657.02
End time: 4658.85
Speaker: SPEAKER_04
Transcript:  A mere two hours of your time is all we ask.

Start time: 4660.18
End time: 4661.98
Speaker: SPEAKER_01
Transcript:  Oh, it's so good.  It's a must listen.

Start time: 4662.06
End time: 4681.76
Speaker: SPEAKER_08
Transcript:  If you love this stuff, I mean, this is where you get the deep dive and you understand it  and you have great people on it and I just, I always love it.  Thank you for all for being here.  We had a fun week this week talking about all this stuff.  I think we have a little mini movie that we've prepared for your enjoyment.  Watch.  Previously on Twitter.  Could be the end of Facebook in some ways, right?  What do you think?

Start time: 4682.36
End time: 4686.86
Speaker: SPEAKER_03
Transcript:  We need to have something like Facebook.  Do we?  Do we?  Yeah, I do think we do.

Start time: 4687.00
End time: 4688.00
Speaker: SPEAKER_00
Transcript:  This Week in Google.

Start time: 4688.34
End time: 4694.00
Speaker: SPEAKER_08
Transcript:  This is Mark Zuckerberg's comment.  He said, we have a responsibility to protect your data.  If we can't, we don't deserve to serve you.

Start time: 4694.02
End time: 4703.96
Speaker: SPEAKER_04
Transcript:  I'm disappointed that all he did was deal with the tactical details of Cambridge Analytica.  There's a much bigger issue here about what the public responsibility of these platforms is.  Tech News Weekly.

Start time: 4704.50
End time: 4712.96
Speaker: SPEAKER_03
Transcript:  This week, pedestrian Elaine Herzberg was killed after stepping into the path of a self-driving Uber.  Sam, describe a high level of what that dash cam video showed.

Start time: 4713.58
End time: 4724.80
Speaker: SPEAKER_00
Transcript:  It actually tells quite a different story from what we first heard from the police on Monday.  Contrary to what the police said, this appears to have been an entirely avoidable accident.  iOS Today.  This is really, I think this is kind of cool.

Start time: 4725.00
End time: 4728.76
Speaker: SPEAKER_03
Transcript:  We croak.  I brought this up at many a cocktail party over the last week.

Start time: 4729.38
End time: 4731.80
Speaker: SPEAKER_08
Transcript:  Oh, you must be a real welcome guest.

Start time: 4732.28
End time: 4733.66
Speaker: SPEAKER_04
Transcript:  You know, we're all going to die.

Start time: 4734.80
End time: 4738.71
Speaker: SPEAKER_02
Transcript:  Like, oh, I have this app that reminds me six times a day that I'm going to die.  What do you think of that?

Start time: 4739.00
End time: 4741.00
Speaker: SPEAKER_00
Transcript:  Tweet, the happiest place on earth.

Start time: 4741.26
End time: 4748.00
Speaker: SPEAKER_08
Transcript:  How many parties a week do you go to?  I haven't been to a party in months.  I'm sure you were invited to the party.

Start time: 4748.00
End time: 4752.85
Speaker: SPEAKER_02
Transcript:  It was the school fundraiser that I brought up to many different people, probably.  I'm sure you were invited to that.  Yeah, my brother died.

Start time: 4755.75
End time: 5025.00
Speaker: SPEAKER_08
Transcript:  That's a reminder of death, isn't it?  Our show today brought to you by somebody I can celebrate that I use, we use at work, and we're really happy.  I've been trying to get these guys as a sponsor literally since Tweet started LastPass.  LastPass keeps your everybody loves LastPass.  If you're not using it, you better get there and use it.  Keeps your passwords organized, secure, right at your fingertips.  It solves the fundamental problem that passwords present.  You've got to remember a bunch of them.  You've got to keep them secure.  But the problem is, if you've got to remember a bunch of them, you're going to keep them secure.  You're going to pick passwords you can remember.  You're going to use bad passwords.  Your niece's name, the name of your dog and the birthday of your child.  That's a terrible password.  With LastPass, you let it generate long, obscure, unmemorable passwords.  Store them securely, fully encrypted.  All you have to remember is one password, your master password.  LastPass does the rest.  You don't want to reuse passwords.  You want long.  I make my passwords.  I'm not going to tell you the length because part of my security through obscurity is not using the standard lengths.  I make them like 17 or 23.  I like prime numbers.  And then, so long, 58, long.  Then LastPass automatically remembers and fills in your passwords anytime, anywhere.  Not just on your computer, on your browser, but on your mobile device too.  In fact, with the new Android 8.1, LastPass is so great now.  It just fills it in so easily.  And then you've got a LastPass vault.  That's encrypted on LastPass servers.  They don't have the key.  Only you have the key.  Strong encryption using some really sophisticated techniques.  I don't know if you remember, but Steve Gibson actually got a deep dive from the creator of LastPass, Joe Segrist, and was so impressed.  He started using it.  So impressed by the technologies that LastPass is using to protect you.  And we use LastPass Enterprise.  And I got to tell you, this is great.  In fact, we like LastPass so much.  As a benefit to people who work at Twitter, and we've done this for the last few years, they get a free LastPass account in addition to the LastPass Enterprise.  And they can merge, they can keep the two together.  That's what I do.  So I have access to all the company passwords and all of my personal passwords.  It doesn't merge them.  It's separate.  But I log into one vault and I have it all.  Eighty-one percent of breaches are caused by weak passwords.  Gotta use LastPass.  It protects every password in your business without slowing down your employees.  AES 256-bit encryption to protect against man-in-the-middle attacks.  They use PBKDDF, whatever the heck that is.  Anyway, I can go on and on.  You should actually listen.  Steve Gibson did a really good segment on LastPass where he explains what they do.  They've got products for homes, for individuals, for families.  We use LastPass for families.  This is, by the way, I love this feature.  There's a death benefit.  We were talking about death a second ago.  I can designate and I have my wife.  If something happens to me, I hear about this all the time when geeks die and their family doesn't know how to get into any of their stuff.  She will have access to my LastPass.  So the way it works is she requests it of LastPass.  She says, I'm the designated survivor.  She sends them a message.  They send me an email to the email address I specified with LastPass.  And then I can say, if I don't respond after a week, I'm dead.  I actually made it two days.  If I don't respond after two days, I'm dead.  Give her the password.  That is huge.  It really gave me peace of mind.  LastPass premium for personal use.  LastPass families for your whole family.  We use that all the time.  Family sharing is fabulous.  LastPass teams for teams are 50 or less.  There's a solution for you.  Very affordable.  You got to try it.  At work and at home.  Fix your password woes with LastPass.  It is the number one most preferred password manager.  It's what I use.  It's what we use at Twitter.  Learn more at lastpass.com slash twit.  There it is.  It is literally the first program I install on any new device or computer.  But you know why that is?  Because then I can log into all the rest.  Lastpass.com slash twit.  I actually put passwords in there, driver's license, all the documents that I want to keep track of,  but I don't want anybody else to have access to because they're important privacy documents.  I keep them in LastPass.

Start time: 5025.00
End time: 5025.82
Speaker: SPEAKER_01
Transcript:  Oh, that's a great idea.

Start time: 5026.14
End time: 5054.64
Speaker: SPEAKER_08
Transcript:  Yeah.  And you know what else I do with LastPass, which is great.  I use a separate authenticator.  But the QR code that you use, I put that in LastPass.  Because that's a safe place to store that so I can set up the authenticator on a new device.  Episode 256 of Security Now.  If you want to watch our LastPass dissection.  All right.  Sasta Festa.  Festa Festa.

Start time: 5057.14
End time: 5058.51
Speaker: SPEAKER_01
Transcript:  Sesta Fosta.

Start time: 5060.61
End time: 5061.84
Speaker: SPEAKER_08
Transcript:  They're not the same or they are?

Start time: 5062.00
End time: 5070.77
Speaker: SPEAKER_01
Transcript:  These acronyms could actually make sense.  Fight Online Sex Trafficking Act or Stop Online Sex Trafficking Act.  The Fosta don't make sense, but their acronyms do.

Start time: 5072.16
End time: 5076.27
Speaker: SPEAKER_07
Transcript:  Yeah, I mean the idea...  Fosta was the House, Sesta was the Senate.  Nice.

Start time: 5077.00
End time: 5100.67
Speaker: SPEAKER_08
Transcript:  And the Reconcile Bill was passed.  Has it yet been signed into law by President Trump?  Yes, it has.  So it's the law?  It is the law.  So the name implies and something nobody wants, which is good.  Stop enabling sex traffickers.  Yeah, who would be against that?  The problem is it might have go much farther than that.  Denise, explain.

Start time: 5102.16
End time: 5113.42
Speaker: SPEAKER_01
Transcript:  Well, we've already seen the fallout of it.  Just in the day since it was signed into law, Craigslist used to have personals where lots of lots of people have not...

Start time: 5114.28
End time: 5124.71
Speaker: SPEAKER_08
Transcript:  That's not sex trafficking.  That's me saying, hey, I was at a bar and I saw a beautiful girl with a red rose and what's your name?  I missed you.  Like that.

Start time: 5125.44
End time: 5126.72
Speaker: SPEAKER_01
Transcript:  Yes, just like that.

Start time: 5127.10
End time: 5128.66
Speaker: SPEAKER_08
Transcript:  There are also sex workers on there.

Start time: 5129.40
End time: 5136.76
Speaker: SPEAKER_01
Transcript:  In the wake of passing this, Craigslist has said, we're so sorry, but they're the first fallout of this act.

Start time: 5137.00
End time: 5149.17
Speaker: SPEAKER_08
Transcript:  We don't dare have a personal section because we would be liable under S...  Can I just call it S-Tuh?  We would be liable.  And what is the liability?

Start time: 5151.73
End time: 5155.64
Speaker: SPEAKER_01
Transcript:  Well, I mean, the liability is for sex trafficking.

Start time: 5156.10
End time: 5159.00
Speaker: SPEAKER_08
Transcript:  So it's as if Craigslist was the sex trafficker.

Start time: 5159.65
End time: 5229.98
Speaker: SPEAKER_01
Transcript:  Yes, exactly.  So Section 230 of the Communications Decency Act is instrumental to how the web as we know it works, how people can have interactions with one another on the web.  Some totally legal, some not, but the sites where they're having those interactions are by and large not responsible unless it's a criminal, a federal criminal law that's being violated.  Or of course, what's carved out of Section 230 as well is intellectual property violations.  So to the extent any of this...  What we're concerned about are things that were federal crimes already.  Section 230 already was drafted perfectly comfortably to accommodate that.  It didn't provide any shelter to sites for people who were engaging in federal crimes on their platforms.  And they were thus incentivized to prevent that kind of activity.  But now...  By the way, sex trafficking, is that prostitution?  Well, that's a good...  I mean, there's a lot of...

Start time: 5230.65
End time: 5232.59
Speaker: SPEAKER_08
Transcript:  I'm not in favor of sex slaves, but...  Right.

Start time: 5234.11
End time: 5249.82
Speaker: SPEAKER_01
Transcript:  It's different, right?  Sex trafficking is what you just described.  I think people engaged in, being forced to engage in sexual activities without their consent and being dragooned into things they don't want to do.

Start time: 5250.08
End time: 5259.76
Speaker: SPEAKER_08
Transcript:  But you're saying Section 230 did not protect a blog comment section or Reddit or...  It didn't protect them.  What's that?

Start time: 5260.08
End time: 5263.22
Speaker: SPEAKER_01
Transcript:  Federal crimes were already carved out.  Now, state crimes were not.  So...

Start time: 5264.95
End time: 5268.56
Speaker: SPEAKER_08
Transcript:  So it's state sex trafficking this protects.  Right, right, right.

Start time: 5269.14
End time: 5285.98
Speaker: SPEAKER_01
Transcript:  So what this winds up doing is, yes, potentially putting...  It gives a very broad definition of what constitutes things that are going to be prevented and enforceable and prosecutable here.

Start time: 5287.20
End time: 5327.68
Speaker: SPEAKER_08
Transcript:  So Craigslist was worried enough that they said, well, we're not even going to take the chance.  So that's one problem, is you're going to see people just say, well, we're not going to take the chance.  In fact, I think some Reddit subreddits were closed as well for the same reason.  But there's also the issue, and we were talking about how somebody should come along and disintermediate Facebook by making a better privacy-focused Facebook.  Facebook has the resources, perhaps, to moderate and to search for and to get rid of anything that would violate SESTA-FOSTA.  Yes.  STA.  STA.  But if you're the new Facebook, if you're the little guy, you may not.  And so this...  Mike Godwin said this actually kills innovation too.  It keeps you from getting a next Facebook.

Start time: 5328.00
End time: 5354.00
Speaker: SPEAKER_07
Transcript:  Senator Ron Wyden said exactly the same thing on the Senate floor.  I mean, he actually wrote Section 230, so he knows what he's talking about.  And he was saying it was shameful that the tech companies are supporting this because they know they've got the armies of lawyers needed and moderators that they can actually protect themselves with.  But if you're a startup, your liability bill has just gone through the ceiling if you're having any kind of public forums on there.  So the big tech companies like this because it's an anticompetitive move.

Start time: 5356.41
End time: 5357.37
Speaker: SPEAKER_08
Transcript:  Here's a quote.  Right.

Start time: 5358.00
End time: 5377.90
Speaker: SPEAKER_01
Transcript:  Inflation is incredibly expensive.  And the problem is, as you pointed out, Leo, there is a difference between sex trafficking and sexual discourse.  And the one will get curtailed in service of protecting liability from the other.

Start time: 5378.34
End time: 5388.00
Speaker: SPEAKER_08
Transcript:  And there is a legitimate concern for sex workers who have been using things like Craigslist.  I mean, that certainly goes on there to get off the streets and to get out of being pimped.

Start time: 5388.12
End time: 5420.96
Speaker: SPEAKER_07
Transcript:  Study after study has shown it's actually safer for sex workers to go online rather than to get business online rather than actually having to go out into the street.  Because you're not, as you say, you're not out there.  You're much more capable of running your own business yourself.  And you're not putting yourself in physical risk.  There's even a sort of a group of sex workers who combine into a database of people who they know are weirdos and can be blocked based on their email address.  So, I mean, this does undoubtedly make people safer if you're a sex worker is going online.

Start time: 5421.24
End time: 5429.98
Speaker: SPEAKER_05
Transcript:  And it becomes less if they're not on Craigslist the whole thing, it becomes more shadowy.  Harder to find the people who do need to be in trouble for what they're doing.

Start time: 5430.04
End time: 5483.44
Speaker: SPEAKER_08
Transcript:  That's an interesting point.  Alex Levy is a professor at Notre Dame.  He's quoted in Gizmodo.  He teaches.  I mean, this guy teaches a class in human trafficking and human markets.  He wrote the war on Internet platforms is pageantry, a kind of theater designed to satisfy people's need to identify and fight bad guys without regard to nuance or long term outcome.  But from a tactical standpoint, it's more than a distraction.  Censoring these platforms means forfeiting a resource that naturally facilitates the recovery of victims.  Section 230 doesn't cause lawlessness.  It creates a space in which many things, including lawless behavior, come to light.  And it's in that light that multitudes of organizations and people have taken proactive steps to usher victims to safety and apprehend their abusers.  It pushes that into the shadows.

Start time: 5484.44
End time: 5497.72
Speaker: SPEAKER_07
Transcript:  Well, they go onto the dark web now and there's no and there's no way to track them.  If they're actually on this on Craigslist or on other open sites, that would give the police and a wealth of information as to how to track these people down.  And it's just made the situation worse.

Start time: 5498.02
End time: 5513.90
Speaker: SPEAKER_08
Transcript:  So it's bad for dangerous for sex workers.  It pushes bad behavior out of the light, pushes victims out of the light.  It has a chilling effect on legitimate enterprises like Craigslist and Reddit.  So clearly it's a bad law.  Right.

Start time: 5514.12
End time: 5525.00
Speaker: SPEAKER_01
Transcript:  And it's a harbinger of maybe bad law to come because Section 230 has been really unpopular among our current lawmakers and prosecutors.

Start time: 5525.30
End time: 5531.98
Speaker: SPEAKER_08
Transcript:  Because they don't like the fact that it protects them against what they think is defamatory speech.  Yes, exactly.

Start time: 5534.04
End time: 5556.98
Speaker: SPEAKER_01
Transcript:  All manner of bad things can go on between people under Section 230 and have a site like Facebook or Google or smaller sites be protected as long as it's not a federal crime or IP infringement.  So defamation would be one thing.  So this could be used to chill speech.  Definitely.

Start time: 5557.26
End time: 5560.00
Speaker: SPEAKER_07
Transcript:  Oh, yeah. First, they came for the sex traffickers and I said nothing for I was not a sex trafficker.

Start time: 5561.18
End time: 5565.00
Speaker: SPEAKER_08
Transcript:  I don't think that was the intent of that poem.  But anyway, I get the point.

Start time: 5565.74
End time: 5593.98
Speaker: SPEAKER_07
Transcript:  It's very interesting that they've used exactly the same excuse in various states.  States have introduced laws where tech companies are mandated to put porn filters onto their devices.  And if you want it taken out, you have to pay a $20 fee per year.  Australia does this.  There's like five or six states in the US that have just introduced laws.  It's terrible.  It's all, every single one of them has used the same excuse.  It's to combat sex traffickers.  And this seems to be the excuse to Jordan.  I'm wondering who's coming up with this.

Start time: 5594.06
End time: 5623.90
Speaker: SPEAKER_08
Transcript:  Well, child abuse is working for a while, but now we've got another one.  So we got to go.  But I also OK, I'm going to argue on their side here.  On the other hand, I mean, we need tools to fight sex trafficking and child abuse.  And you've got to give us credit.  We're not going to overstep our bounds here.  Because that never happens.  Well, OK, I couldn't even say it with a straight face.  So everybody has said the courts will weigh in on this.  Denise, will the courts protect Section 230?

Start time: 5625.63
End time: 5664.00
Speaker: SPEAKER_01
Transcript:  Yes, they might.  There might be parts of this that are, again, deemed unconstitutional  or beyond Congress's scope of authority in one way or another.  And again, EFF and others will probably be leading the charge on  picking at the viability of this law and the Cloud Act.  And in both instances, right, it's like we're just going to grant ourselves  this authority in service of a noble goal.  And as you say, Leo, there's sort of this tacit implication.  We would never abuse that authority, but it's really broad authority.

Start time: 5664.06
End time: 5683.88
Speaker: SPEAKER_08
Transcript:  Well, so now we just hope that the court that there will certainly be actions,  although it's interesting that Craigslist didn't fight it.  They just said, yeah, OK, fine.  Well, I think they were making a stand more than...  Oh, you think so?  But I'm sure they make a lot of money on those persons, right?  It's an important part of their business.  Are they free?  They're free.

Start time: 5684.14
End time: 5686.56
Speaker: SPEAKER_07
Transcript:  Oh, that's right.  There's job listings that they make money from.

Start time: 5688.35
End time: 5699.95
Speaker: SPEAKER_05
Transcript:  And Craigslist is also an itty bitty company without the resources  to police this stuff in the way that large ones might.  They can't hire 10,000 people like Facebook can.  But I mean, also, look at the amount of sex workers are on Tinder and Bumble

Start time: 5701.08
End time: 5714.97
Speaker: SPEAKER_07
Transcript:  and that sort of thing.  That's going to have to come into play sooner or later.  They've got to be aware of this.  We've been contacting them for comment on it since this came up,  and they're being very, very quiet about it.  But I suspect their legal department is quietly having kittens  over the prospect of being sued.

Start time: 5716.00
End time: 5738.62
Speaker: SPEAKER_08
Transcript:  So I think the other thing we probably should point out is it probably  won't do anything to fight sex traffic.  Because honestly, that's not going to have that effect.  Do you think that they were cynically, the lawmakers were cynically saying,  well, what we really want is a bill that will keep those people from saying  bad things about us and we'll just put it in the guise of sex trafficking?

Start time: 5739.76
End time: 5768.94
Speaker: SPEAKER_01
Transcript:  I wouldn't go that far, but I do think only two senators voted against this bill.  It was a 1972 vote with Rand Paul and Ron Wyden being the only two no votes.  I just think it's really hard for a senator to say, yeah,  I voted against this bill that was designed to curb sex trafficking.  How do you go back to your constituents, justify that,  unless you're good at explaining why and who really is that good  or wants to take that time?

Start time: 5769.48
End time: 5798.00
Speaker: SPEAKER_07
Transcript:  I think that's why the bill was named in its way.  So if somebody trying to get it through could say,  well, you want to go back to your constituents and say,  no, I voted against a law that would go against sex traffickers.  It's blatantly cynical.  I think, to be honest, most of them, if Congress,  if we've seen anything from Congress's level of tech-savvyness,  most of them probably didn't really realize the implications of what they were voting for,  but that seems to be very much the run of course in Congress these days.

Start time: 5798.60
End time: 5806.97
Speaker: SPEAKER_05
Transcript:  And Ron Wyden is one guy who kind of knows what he's doing when it comes to tech,  and Ron Paul is a guy who's very comfortable with going the other way.  Doesn't mind.

Start time: 5808.20
End time: 5833.00
Speaker: SPEAKER_08
Transcript:  I think there is, and really this is, you can attribute a lot of what's going on in Congress right now  to the power of these attack ads that members of Congress know they're going to go home,  they're going to face opponents in their primary who are going to run attack ads that say,  Senator is in favor of sex trafficking, and those will work.  And so it always comes down to, frankly, an electorate that's willing to fall for that.

Start time: 5833.52
End time: 5858.36
Speaker: SPEAKER_01
Transcript:  Yeah, and in service of your cynicism argument,  Professor Eric Goldman from the University of Santa Clara,  who knows more about Section 230 of the Communications Decency Act than anyone except,  saving maybe Ron Wyden, went and testified before the Senate about this.  And so it's not like they weren't gathering information from people who did understand it  and could explain it clearly to them why this was a bad idea.

Start time: 5859.95
End time: 5861.94
Speaker: SPEAKER_07
Transcript:  Yeah, it's one of those things.

Start time: 5862.10
End time: 5876.00
Speaker: SPEAKER_08
Transcript:  You're not going to have the chance to explain it.  There's going to be an attack ad with a big black screen that says,  Senator Wyden's in favor of sex trafficking.  And then that's that. You don't have a chance to explain it.

Start time: 5876.08
End time: 5879.00
Speaker: SPEAKER_05
Transcript:  With an ugly black and white photograph of him looking shadowy.

Start time: 5879.14
End time: 5883.00
Speaker: SPEAKER_08
Transcript:  Ask Senator Wyden why he likes sex trafficking so much.

Start time: 5883.52
End time: 5895.00
Speaker: SPEAKER_07
Transcript:  Maybe a hand with a pair of handcuffs just putting money into his back pocket.  You know how these things work. They're tasteless as all hell, and it just,  again, it comes back to the dumbing down of politics.

Start time: 5897.11
End time: 5903.16
Speaker: SPEAKER_08
Transcript:  Ultimately, it's our responsibility. We're the electorate.  It's happening in every country.  It is. That's how Brexit happened.  Yeah.

Start time: 5904.18
End time: 5918.00
Speaker: SPEAKER_07
Transcript:  And we now know.  You remember, I came on the show the weekend after Brexit,  and I predicted Trump would win.  Did you really? I blanked that part out.  Yeah. Trust me, I was having a discussion.

Start time: 5918.00
End time: 5920.44
Speaker: SPEAKER_08
Transcript:  And there, November 8th, saying, oh, that Ian Thompson, what does he know?

Start time: 5921.37
End time: 5927.58
Speaker: SPEAKER_07
Transcript:  What does he know?  Hey, I won a bottle of scotch based on that bet.  Did you really?  Yeah. It turns out I needed to drink it that night, but you know.

Start time: 5928.74
End time: 5934.00
Speaker: SPEAKER_08
Transcript:  You're not the first Brit to say that. John Oliver said that also.  He said, after going through Brexit, I had a feeling.

Start time: 5934.22
End time: 5940.00
Speaker: SPEAKER_07
Transcript:  Yeah. It was something was up. And the old rules don't seem to apply.

Start time: 5940.02
End time: 5948.98
Speaker: SPEAKER_08
Transcript:  Come on, guys. You've got to start reading, learning, and taking your job as electors seriously.  Being a voter is an important thing to do.

Start time: 5949.86
End time: 5953.96
Speaker: SPEAKER_01
Transcript:  I was in London the morning after the vote. I've never seen so many shell shocked people.

Start time: 5954.38
End time: 5955.64
Speaker: SPEAKER_08
Transcript:  Really? Was that same thing?

Start time: 5956.18
End time: 5956.59
Speaker: UNKNOWN
Transcript:  Oh, God.

Start time: 5957.32
End time: 5984.69
Speaker: SPEAKER_07
Transcript:  I know. Even amongst the winners, though, when Boris Johnson got...  He couldn't believe it.  He came out of his house and saw the assembled press crew at 8 o'clock in the morning after the vote,  and just had this look on his face of just, what have I done?  Wow. It's going to happen.  Yeah.  Wow.  I mean, that was the plus side of Trump getting elected, though, because for three months,  all my friends over here were just like, ah, you see, you can screw things up, too.  America said, hold my beer and watch this.

Start time: 5988.50
End time: 6146.00
Speaker: SPEAKER_08
Transcript:  Wow. What a world. What a world.  Let's take a little break on that note, Denise. I want to talk about food.  This will make me feel better. Denise, did you do Blue Apron? I think you did. Yeah.  Yeah.  Isn't it great? I love it.  Yeah. And still subscribe.  We get it. We have Blue Apron in our fridge. I might go home tonight.  I can't remember what it is, but it's guaranteed to be something delicious.  Blue Apron is the number one fresh ingredient and recipe delivery service in the country.  Here's how it works. You go to blueapron.com slash twit.  By the way, if you do that, you're going to get $30 off your first order and free shipping.  Blueapron.com slash twit.  Take a look at what's on the menu. They have lots of different items.  You pick, well, two, three, or four recipes, depending on what you want, what fits your schedule.  There's 12 to pick from. Things like quick bucatini with broccoli and pecorino cheese.  I want that now, right now. Pan fried chicken breast with sweet and tangy zucchini.  Italian style shrimp and sweet peppers over fregola sarda pasta.  Parmesan crusted steaks with mashed potatoes and broccoli.  You choose the one you want. They have two plans, one for a couple, one for a family of four.  The family of four version has kind of more kid-friendly ingredients.  By the way, what you're not going to get is a precooked meal that you steam or you microwave.  You're going to get exactly the ingredients fresh, beautiful.  They pick produce better than anybody. It's going to be beautiful.  Everything you need, right down to the clove of garlic, the rib of celery, the little tiny bottle of soy sauce.  Whatever it is, it's in the recipe.  And then you cook it. And kids love this. You go, oh, we got everything. Let's cook.  And it takes you just, you know, 45 minutes.  The house fills with these amazing, beautiful scents of things like ginger.  I noticed that Blue Apron, I think they do this on purpose, has beautiful aromatic ingredients that really make your nose go,  I can't wait. Dinner's coming. They're so good.  Kids get excited about it.  And, you know, I have to say, it's so easy. We live such busy lives. We're so swamped.  I think a lot of times you go home, you get something to go or you go to the fast food.  Look, with Blue Apron, you're not just having burgers for dinner.  You're making short rib burgers with a hoppy cheddar sauce on a pretzel bun  or seared steaks and thyme pan sauce with mashed potatoes, green beans and crispy shallots.  Oh my God. Blue Apron, look at you. You got to try this.  Incredible ingredients, chef designed recipes.  Always something new.  Fresh ingredients. A meal you make yourself from scratch.  And by the way, once you do it, I don't know if this is your experience.  A, learning about new. You probably know all these ingredients, but I didn't know.

Start time: 6146.32
End time: 6152.38
Speaker: SPEAKER_01
Transcript:  I really know. They throw in some really cool sort of fairy eggplants.  From culture kind of ingredients.

Start time: 6153.24
End time: 6164.00
Speaker: SPEAKER_08
Transcript:  Frigola sarda. What is that? So then I know how to do it.  And I tend to make these recipes because I was getting a little stale.  I was getting a little bit of a rut, making the same thing over and over.  Now I have a much bigger repertoire. It really is fun.

Start time: 6164.97
End time: 6172.05
Speaker: SPEAKER_01
Transcript:  I love it for kids. Their recipes are really easy to follow.  Sometimes I'll just say, hey, you're making dinner.  Really? And does he do it?  Yeah, here you go. The easier ones, the pasta ones. Sure. Why not?

Start time: 6173.44
End time: 6284.15
Speaker: SPEAKER_08
Transcript:  That's awesome. Blue Apron is giving you $30 off your first order and free shipping.  Go to blueapron.com slash twit.  Blue, if you haven't done this, do it.  Check out this week's menu and get $30 off with free shipping.  Blueapron.com slash twit.  Blue Apron is a better way to cook.  All right. Uber.  So I, you know, when Lisa told me about this Uber car, a very sad story.  Tempe, Arizona, Uber self-driving vehicle with a safety driver at the helm,  but in autonomous mode, 10 p.m., homeless pedestrian walking her bicycle,  steps out into a four-lane, and they have these in Arizona, really.  These are city streets, but they're four lanes and the cars are whizzing down,  steps out and gets hit and killed. Very sad.  She had her bicycle and she had bags on the bicycle. She gets hit and killed.  But my first reaction was, oh, well, clearly the pedestrian walked out from between a car  or was in some way invisible because, no, even my car, not in autonomous mode,  would see and jam on the brakes if something like that happened  because almost all cars now have collision avoidance and they would jam on the brakes.  There was no evidence that the brakes had slowed down.  The safety driver said, I didn't see anything until a crash happened.  Then we saw the video and the Tempe police initially said, yeah, this is unavoidable.  There's no way anybody could avoid this.  But as you saw, Sam Abouelsameed and others, Brad Templeton, who I highly respect,  is a big advocate of autonomous vehicles.  He's talked about how this is going to change the world and it's a great thing.  He said he was skeptical. Sam was skeptical.  We looked at the video. A few things came to mind.  First of all, maybe the least important, the car was traveling five miles above the speed limit.  Now, we all do that.

Start time: 6285.28
End time: 6291.90
Speaker: SPEAKER_07
Transcript:  That's actually been altered now. It was on a 45-mile-an-hour road and it was going 40.  Oh, it was going slower than the speed limit.  It was going slightly slower.

Start time: 6292.26
End time: 6342.00
Speaker: SPEAKER_08
Transcript:  As it should in the dark with visibility's bad.  Exactly.  That's one thing a self-driving vehicle should do.  The safety driver in the video was not watching.  She was looking down at her phone.  And laughing.  But, and this is what Brad pointed out, and this is a big deal,  you can clearly see in the video five seconds before the car strikes the pedestrian,  the lights off the pedestrian sneakers, he said a LIDAR system can see in the dark.  It should have seen this.  This is clearly a failure of Uber's autonomous vehicle.  And I have to say, I agree now seeing more of this,  because even my Tesla, not in auto driving mode,  and many modern cars, would have at least braked if it had five seconds.

Start time: 6343.49
End time: 6376.94
Speaker: SPEAKER_07
Transcript:  And this is the thing, autonomous vehicles are being sold to us  as this will make life so much simpler, safer, and we'll have less traffic.  And in fact, the reason people are struggling,  and the reason Uber is rushing to develop this technology  is because its business model doesn't make sense without it.  So I think this kind of gives light to the whole autonomous driving is going to be safer,  because these cars have done a pathetic amount of time on the roads,  and they're already killing people.  Or somebody has died in it.  So I don't think the technology could have worked.  I mean, most LIDAR systems would have picked that up.

Start time: 6377.18
End time: 6440.41
Speaker: SPEAKER_08
Transcript:  There's some, we will know, because the National Transportation Safety Board is investigating.  The Tempe police have now referred it for criminal investigation,  so they're obviously taking this more seriously.  The family members of the pedestrian have now retained counsel and are expected to sue.  So this will come out.  And at least one thing about autonomous vehicles, there's lots of recording.  But one thing Brad speculated, maybe the LIDAR was turned off.  It's not unusual in testing for LIDAR to be turned off in testing.  And what he said is this is not appropriate.  Maybe it should be, the input of the LIDAR should be somehow disabled.  That's bizarre.  And maybe Uber has less of a record for all of this.  According to the New York Times, Waymo, the self-driving car project at Google,  its cars went on average of 5,600 miles before the driver had to take control, the safety driver.  As of March, Uber was struggling to meet its target of 13 miles.  5,600 for Waymo, 13 for Google.

Start time: 6444.00
End time: 6450.98
Speaker: SPEAKER_07
Transcript:  Well, Google's been very careful about this.  And what's the figure for humans with that?

Start time: 6451.64
End time: 6481.92
Speaker: SPEAKER_08
Transcript:  Well, yeah, I mean, of course, and one of the things I worry about,  I don't want autonomous vehicles to be slammed inappropriately.  I mean, at some point, people are, this is going to happen because there are unavoidable circumstances.  Even a very fast autonomous system can't stop a car in zero feet  because it's physics.  And we know autonomous vehicles will ultimately save lives.  But if there is any evidence that Uber, and who wouldn't be surprised to find this out,  did not do everything right, then they...

Start time: 6483.24
End time: 6489.68
Speaker: SPEAKER_07
Transcript:  No, I mean, they're a highly reputable company who've never played fast and be fallucid with the rules at all.  I don't know what you're hinting at.

Start time: 6490.00
End time: 6492.94
Speaker: SPEAKER_08
Transcript:  Uber has halted its autonomous vehicle testing, obviously.

Start time: 6493.08
End time: 6509.00
Speaker: SPEAKER_07
Transcript:  Yeah, but it's interesting that under California law,  all the self-driving car companies who have their cars on the roads  have to file a report if there's any kind of accident.  Now, under the Arizona law, they don't have to do that.  Yeah, Arizona really wanted this to happen.

Start time: 6509.58
End time: 6512.88
Speaker: SPEAKER_05
Transcript:  That's one of the reasons why they're testing in Arizona, because it's a little bit more loosey-goosey.

Start time: 6513.36
End time: 6531.98
Speaker: SPEAKER_07
Transcript:  Yeah. I mean, I'm working on a piece, so I can't say too much about it,  but there are other self-driving car companies who are going to be falling afoul of this as well.  I mean, it was always going to happen that an autonomous vehicle kills somebody.  I mean, it's the way of it.  You know, was it 30,000 people get killed in cars every year in the US?

Start time: 6532.52
End time: 6538.90
Speaker: SPEAKER_08
Transcript:  If you cut that in half, if autonomous vehicles only killed 15,000 people a year,  that would be a plus, but it would surely look bad.

Start time: 6539.46
End time: 6545.10
Speaker: SPEAKER_07
Transcript:  But again, I think there were people at Waymo who were just like,  we dodged a bullet, it wasn't us that did the first one.  Maybe.  Right.

Start time: 6546.54
End time: 6560.00
Speaker: SPEAKER_01
Transcript:  And the way in which Uber may have dodged a bullet here is there seems to have been some sort of fault  on the part of Elaine Hertzberg, who was tragically killed here.  But when you watch the video, she does seem to have, you know, jaywalked.

Start time: 6560.74
End time: 6580.00
Speaker: SPEAKER_08
Transcript:  She's not at the crosswalk. It's a dark night.  She's walking into the middle of four lanes of traffic in a 45-mile-an-hour zone.  You know, that's not the best behavior.  And yet, it seems to me now, and apparently many experts in this,  that this is the kind of thing in an autonomous vehicle that just should not happen to us.

Start time: 6580.08
End time: 6583.88
Speaker: SPEAKER_05
Transcript:  It's an edge case, but most accidents involve edge cases.  They're all edge cases.

Start time: 6584.00
End time: 6595.92
Speaker: SPEAKER_07
Transcript:  I mean, I've got to admit, the first time I saw that video, I thought, you know,  if I'd have been driving, I probably would have hit her too.  But you're supposed to have all this whiz-bang technology,  which is going to save people lives on the road.

Start time: 6596.02
End time: 6624.98
Speaker: SPEAKER_08
Transcript:  And as Brad pointed out,  she was carrying a big metal black...  If you lighten up the picture so that you can actually see what's going on,  he says his line is, it certainly looks bad for Uber.  They had five seconds.  Yeah.  It's kind of hard to see because it's a dim video, but they had time.  And they should have seen it.  The lidar should have seen it.  And so, there's big questions.  We'll watch with that.

Start time: 6625.40
End time: 6632.48
Speaker: SPEAKER_05
Transcript:  And it seems to open up questions about what the safety drivers should be doing.  Not looking at her phone.  If in fact safety is why they're in there.

Start time: 6633.37
End time: 6655.56
Speaker: SPEAKER_08
Transcript:  You know, I white-knuckle it.  When I Tesla, which, you know, it has autopilot, it's not autonomous.  It has autopilot. It's just a fancy cruise control.  But I got my hands on the wheel.  Tesla makes you keep your hands on the wheel.  I pay attention because one of the problems is when you're letting a car drive,  is you're not paying attention.  It takes you a few seconds just to figure out what's going on.  What's going on.  Not enough time to stop the vehicle.  So you need to be watching.

Start time: 6657.53
End time: 6676.45
Speaker: SPEAKER_07
Transcript:  No, this is it.  I mean, if you've got...  This is kind of what freaked people out when Google started testing their driverless cars  with no pedals or steering wheels.  And that's been kind of scaled back a bit.  But yeah, if you're going to have a safety driver in there,  they've got to be doing their job and making sure that it's as safe as it can be.  And that means sitting there with your hands on the wheel,  with your feet near or on the pedals.

Start time: 6677.84
End time: 6726.96
Speaker: SPEAKER_08
Transcript:  And I have to say, there's no reason why a car should not be driving a little slower  than a car with a human at the wheel.  I can get there 30 seconds later.  The Uber...  Brad says the Uber was traveling at what he calls the upper range of speeds  at which it's safe to drive non-freeway with just the LIDAR.  He calls it the valley of danger.  Uber knows it.  It's about as fast as you should go.  You can do it.  Even so, some cars like to go a bit slower approaching crosswalks, marked or not.  Using the LIDAR, their perception system should have a pretty good impression of her.  By 50 meters, 2.7 seconds.  And by then it should have at least hit the brakes.  It didn't.  Yeah.  Right.  So there's something going on.  There's more to this.  He says Uber needs to say why this did not happen.  Was the LIDAR turned off?

Start time: 6727.06
End time: 6744.89
Speaker: SPEAKER_07
Transcript:  You do feel for the new CEO, Uber, because he's been faced with endless,  you know, with three or four big, big things that have come up.  Yeah, before you actually got here, we kind of did this thing.  It's a wee bit on the illegal side or it's a wee bit on the distasteful side.  And by all accounts, Daira Khosrow Shahi is a really good guy.

Start time: 6746.00
End time: 6764.92
Speaker: SPEAKER_08
Transcript:  No, no, solid bloke.  Yeah.  And in fact, according to the New York Times, he considered killing the self-driving program at Uber  when he became CEO and was convinced not to because, frankly,  it's the only way Uber makes any sense at all is if they don't have to pay drivers.  Because otherwise, there's just no way to make money.  Their business model doesn't work.

Start time: 6766.07
End time: 6783.00
Speaker: SPEAKER_07
Transcript:  At the moment, I think it's something like 40 percent of the cost of an Uber ride  is covered by the venture capitalist who's funding the company.  And you can't burn through cash like that forever.  So they're desperate to make self-driving cars work.  And that does kind of, given Uber's record,  beg the question as to whether or not they cut corners at just the wrong time.

Start time: 6783.06
End time: 6793.76
Speaker: SPEAKER_08
Transcript:  You've talked a lot about this on I Know This Week in Law and the issues with self-driving vehicles.  Who's responsible if an accident happens?  Is there a consensus?  Do we understand this well enough or is this still in flux?

Start time: 6794.41
End time: 6856.00
Speaker: SPEAKER_01
Transcript:  I think it's still in flux.  And I think this case is a really good example of how our negligence laws.  I've always kind of thought, oh, it might change everything, that it's a self-driving car.  But I think our negligence laws as they exist today are probably very well capable of handling this kind of incident.  Arizona is a comparative fault state.  So if this case were to be tried, the lawyers who represent Ms. Hertzberg have said they expect it to settle.  But if this case were to be tried, there would be evidence put in about the degree of fault that she had for having stepped out at that particular place and time.  And there would be an extensive investigation, just like there would be in any car accident,  as to the fault of the person or entity piloting the car.  So I think the evidence would look a little bit different.  But the liability equation would probably be something that our courts are used to dealing with.

Start time: 6857.28
End time: 6862.00
Speaker: SPEAKER_08
Transcript:  Is there any legal precedent or legal language covering self-driving vehicles at all?

Start time: 6862.00
End time: 6887.98
Speaker: SPEAKER_01
Transcript:  I don't think that the negligence law at all has been, you know, modified to accommodate so that, oh, you have higher responsibilities.  If you're testing self-driving cars, that might be something that states think about in the wake of this.  And certainly if we had an example where it seemed like the victim was not at fault at all, where they're crossing in a crosswalk at a green light and they get smacked into.

Start time: 6888.26
End time: 6894.00
Speaker: SPEAKER_08
Transcript:  And who would be responsible in that case? The company operating the vehicle?

Start time: 6894.10
End time: 6915.76
Speaker: SPEAKER_01
Transcript:  I would think so. Yeah. If there's no fault at all on the part of the pedestrian.  Does the safety driver, does that person have any?  The safety driver is protected, I think, by the virtue of their being an employee.  But if they're doing something they're not supposed to do.

Start time: 6916.34
End time: 6924.00
Speaker: SPEAKER_08
Transcript:  It's like a trained engineer, you know, texting and then the train has an accident. That engineer is liable. I'm sorry.

Start time: 6924.48
End time: 6933.98
Speaker: SPEAKER_01
Transcript:  Right, of course. But they, you know, that engineer is not going to be able to pay very much.  Right. Yeah.  So, you know, it's the company who is on the hook for whatever.

Start time: 6934.00
End time: 6941.00
Speaker: SPEAKER_08
Transcript:  Well, expect many more stories like this over the next 10 years. I think this is going to be one of the areas where we're going to hear a lot.

Start time: 6941.22
End time: 6969.90
Speaker: SPEAKER_07
Transcript:  Yeah. I mean, the fact is the freeways are the roads and the roads are an inherently dangerous place.  You're driving a ton and a half or two tons of metal at very high speeds with contact patches on a tire about yay big.  And this stuff is going to happen. And also just on a personal note, freeway driving over here is terrifying for a grip.  Because if you leave everybody's on the right, I don't understand that. What's going on?  There's that. But also if you leave a decent stopping distance in front of you with the car in front, two people try and cram themselves in.

Start time: 6971.05
End time: 6977.60
Speaker: SPEAKER_08
Transcript:  Because I do that. I really try to keep two seconds between me and the car in front of me.  And everybody else says that an opportunity to just jam in the middle.

Start time: 6978.20
End time: 6981.98
Speaker: SPEAKER_01
Transcript:  But that's OK. Wow. That used to be just an L.A. thing. Now it's a Northern California thing.

Start time: 6982.04
End time: 6992.05
Speaker: SPEAKER_07
Transcript:  Yeah. You should try New Jersey. I mean, I don't even drive on New Jersey roads anymore.  Boston, where I learned to drive. Oh, good grief. You must have nerves of steel.  No, I'm a terrible driver and obnoxious.

Start time: 6993.00
End time: 6994.84
Speaker: SPEAKER_05
Transcript:  How is it in Phoenix, Matt?

Start time: 6995.28
End time: 7010.00
Speaker: SPEAKER_08
Transcript:  People see if in Arizona, they have lots of space and the roads are wide and there's usually not that much traffic on the roads.  In fact, this is a perfect place to test self-driving vehicles, one would think if people would just stop walking in the middle of the road.

Start time: 7010.00
End time: 7035.76
Speaker: SPEAKER_01
Transcript:  I hate to make light of this because it's a tragedy.  It's a human tragedy.  It is a human tragedy. But just harkening back to what Ian said earlier about the UK and the United States being in a game of Hold My Beer,  I will comment that Ashley Vance, who wrote the book on Elon Musk, tweeted on March 19th,  This Hold My Beer game that Facebook and Uber are playing is next level.

Start time: 7036.22
End time: 7085.98
Speaker: SPEAKER_08
Transcript:  Oh, yeah. Ashley's been great. His Twitter foo has been very high in the last four weeks.  Travis Kalanick going, wow, I dodged that bullet. He's got a new business. They rehab real estate.  He's the CEO of a company formerly called Cloud Kitchens.  Of course, Travis has a little bit of money from the Uber thing.  And so he has sunk $150 million into it and buys out all the other investors.  And I think he's renaming it to City Storage Systems, which is not nearly as impressive as Cloud Kitchens.  But they focus on, I don't even know what this means, repurposing distressed real estate assets like parking lots or abandoned strip malls  and turning them into spaces suited for new industries such as food delivery or online retail.

Start time: 7087.10
End time: 7092.00
Speaker: SPEAKER_07
Transcript:  Property company. You know, it's just I've got some spare cash around. Let's put it into property. That's always a safe bet, I guess.

Start time: 7092.93
End time: 7093.98
Speaker: SPEAKER_05
Transcript:  Doesn't sound terribly glamorous.

Start time: 7094.95
End time: 7124.98
Speaker: SPEAKER_08
Transcript:  Yeah, maybe he's decided it could not to be so glamorous.  Yeah, maybe. You know, it's you know, who else has evolved? Sky Dayton.  There's a name we haven't heard in a while. Mr. Earthlink.  Mr. Earthlink. Oh, good grief. He was the Scientologist who founded and ran Earthlink for a long time.  I guess he's got he's an investor now. He's a co-founder.  I had no idea a Scientologist in Cratered Earthlink. I think he was. Yeah, wasn't he? Am I wrong?  He was indeed. Yeah, good grief. His first name is a hint.

Start time: 7127.66
End time: 7135.07
Speaker: SPEAKER_07
Transcript:  Sorry, sorry. I know he's a very powerful so Zinu be praised. Zinu be praised. Praise Zinu.  And give him your money.

Start time: 7136.60
End time: 7265.84
Speaker: SPEAKER_08
Transcript:  Our show today brought to you by ZipRecruiter. If you're hiring, you got to know about ZipRecruiter.  They bring you the talent. I love this idea. We've talked about ZipRecruiter before.  You know, one post on ZipRecruiter goes to 100 plus job sites plus social networks.  So more people will see your application. All those applicants don't go to your inbox or your phone.  They go into the great ZipRecruiter interface. But I don't think we've emphasized enough the intelligence,  the platform that's in ZipRecruiter that brings the candidates to you.  See, ZipRecruiter looks learns what you're looking for. And then, of course, they have millions of resumes.  They identify people with the right experience and they actually invite them. They say, hey, Leo's got an opening.  They invite them. It's like they're headhunting for you.  These invitations have totally revolutionized how you find your next hire.  In fact, 80% of employers who post a job on ZipRecruiter get a quality candidate through the site in just one day.  80% in one day. ZipRecruiter doesn't stop there. They even spotlight the strongest applications.  So you never miss a great match. Here, look here. Look, these guys great.  It's like they're working for you. The right candidates are out there. We know that.  Let ZipRecruiter make your job a lot easier. You've got the most important job in the business.  You're literally hiring the stuff the business is made out of. Do it right with ZipRecruiter.  You could try it free right now. Businesses of all sizes trust ZipRecruiter for their hiring needs.  Just go to ZipRecruiter.com slash twit for your free trial. ZipRecruiter.com slash twit.  It is literally the smartest way to hire. ZipRecruiter.com slash twit.  And they're very smart because they decided to use twit to get to you.  So thank them for that too. ZipRecruiter.com slash twit.  GDPR just a little more than a month away, two months away.  Yeah. May 25th.  May 25th.  Yep.  And a lot of-  You've had two years to prepare?  And they are completely unprepared.  Completely unprepared.  This is the general data protection privacy regulation.  Yes, something like that.  I mean it's the-  It's probably something in French.

Start time: 7266.46
End time: 7275.80
Speaker: SPEAKER_07
Transcript:  It's 21 chapters of DenseEU legislation because you know, never use one chapter when 21 will do when it comes to being, to writing these laws up.

Start time: 7276.16
End time: 7280.84
Speaker: SPEAKER_08
Transcript:  What do you think Denise? I mean talk about the law looming large over tech.

Start time: 7282.72
End time: 7318.92
Speaker: SPEAKER_01
Transcript:  Yeah, absolutely. I mean we're already starting to see notices come through from companies like Google for example.  I got some email this week that was related to their- is it GDPR?  Yeah.  PDR, GDPR compliance.  So you know, they've definitely been preparing for it for some time, but you know, I hope that the message that they're taking away from it aside from everything they have to do to comply in the EU is you know, there but for the grace of Congress.  We definitely could move more in that direction.

Start time: 7319.00
End time: 7334.60
Speaker: SPEAKER_08
Transcript:  Who knows what could happen?  That's the problem. It's not predictable.  Somebody has found child abuse imagery in the Bitcoin blockchain.  If you have a Bitcoin wallet on your computer, you have that imagery on your computer. Are you liable?

Start time: 7335.78
End time: 7344.68
Speaker: SPEAKER_07
Transcript:  It's a very good- it was a very interesting piece of research that you could play to put- I mean not just child abuse images, but I mean you could put pretty much any bit of data in there.

Start time: 7345.92
End time: 7381.96
Speaker: SPEAKER_08
Transcript:  It's an open source distributed ledger. All Bitcoin transactions are in the ledger. It's hundreds of gigabytes now.  And if you want to have- maintain your own Bitcoin wallet on your service as opposed to using Coinbase or something, you have to download it all.  I know because I have a wallet on my computer. What I didn't know is that there are 1600 files stored in that blockchain.  People have figured out a way to stick something in there. Of the files, at least eight were of sexual content, according to Aachen University, Germany, RWTH Aachen University.  One thought to be an image of child abuse and two that contain links to child abuse content.

Start time: 7384.71
End time: 7387.00
Speaker: SPEAKER_01
Transcript:  Right, which is an important distinction, correct?

Start time: 7387.86
End time: 7392.00
Speaker: SPEAKER_08
Transcript:  I don't know. You're the lawyer. Should I delete this ledger from my hard drive? Am I-

Start time: 7393.18
End time: 7396.78
Speaker: SPEAKER_07
Transcript:  Let me take those Bitcoins off your hands, Leo. Someone's going to take the risk.

Start time: 7397.74
End time: 7402.00
Speaker: SPEAKER_08
Transcript:  If you can figure out the password, they're yours, my friend. I locked it down and I don't remember.

Start time: 7402.08
End time: 7415.00
Speaker: SPEAKER_01
Transcript:  Bitcoin did a release on this where they say this is a lot of smoke but not much fire.  And people have been criticizing Bitcoin and its operation and the way the blockchain works from the get-go.

Start time: 7415.08
End time: 7426.00
Speaker: SPEAKER_08
Transcript:  Ian Earpohl sent out a memo, I guess, in 2015 saying this is the possibility malware could be injected into the blockchain and nothing could wipe it because the blockchain has to live forever.

Start time: 7427.38
End time: 7434.80
Speaker: SPEAKER_01
Transcript:  Right. And one of my listeners at Twill wrote in exactly what Ian was saying. You could put anything in there. You could put people's credit information, personal data.

Start time: 7436.64
End time: 7446.01
Speaker: SPEAKER_08
Transcript:  But I wouldn't- I mean, I'm not liable for this stuff that's stuck in the wallet just because I have it on my hard drive. I don't have to worry about that, I presume.  Your Honor, I had no idea. Right?

Start time: 7448.28
End time: 7451.18
Speaker: SPEAKER_01
Transcript:  Yeah, I don't want to have to make that case to law enforcement.  No.

Start time: 7452.40
End time: 7457.42
Speaker: SPEAKER_08
Transcript:  All right, I'm deleting my wallet. I can't get it anyway. That's just weird.

Start time: 7458.96
End time: 7481.00
Speaker: SPEAKER_07
Transcript:  It was a really interesting bit of research because it did- it's looking- okay, this is just a personal view, but it's looking more and more like Bitcoin is kind of like- it was the valiant first attempt.  The next generation is going to be better and the generation after that will probably get it right.  You know, it's the old version three problem. You never buy any Microsoft product until it gets to version three. I think we're going to see the same with online currencies.

Start time: 7481.06
End time: 7484.00
Speaker: SPEAKER_05
Transcript:  And will Bitcoin be Bitcoin 3.0 or is it-

Start time: 7484.52
End time: 7487.79
Speaker: SPEAKER_07
Transcript:  Ah, well, if it's still in existence, yes.  Jack Dorsey says-

Start time: 7489.08
End time: 7502.64
Speaker: SPEAKER_08
Transcript:  Jack Dorsey says it will be.  Well, not Bitcoin per se, but some cryptocurrency will be the default currency in the world within 10 years. Forget the euro, forget the dollar.  He didn't say Bitcoin despite that headline.  He said he thinks it will be Bitcoin.

Start time: 7503.06
End time: 7503.20
Speaker: SPEAKER_05
Transcript:  He does?

Start time: 7504.33
End time: 7515.60
Speaker: SPEAKER_08
Transcript:  Yes.  Oh, you're right. The world ultimately will have a single currency. The internet will have a single currency. I personally believe it will be Bitcoin.  That tells me Jack Dorsey has a few Bitcoin in his wallet.

Start time: 7516.38
End time: 7520.00
Speaker: SPEAKER_05
Transcript:  And he's CEO of Square, so this is an important question for him.

Start time: 7520.00
End time: 7524.96
Speaker: SPEAKER_08
Transcript:  Yeah. Square recently added the ability to buy and sell Bitcoin in the Square app.

Start time: 7528.93
End time: 7534.94
Speaker: SPEAKER_01
Transcript:  Well, how about the other story about hacking or backdooring the Bitcoin wallet by the 15-year-old?

Start time: 7535.54
End time: 7549.44
Speaker: SPEAKER_08
Transcript:  I love that. Isn't that a great story? So this was a unhackable wallet. That should be a giveaway.  As soon as somebody says it's unhackable, it will be hacked. And the funny thing is this time it was hacked by a 15-year-old.

Start time: 7551.14
End time: 7556.98
Speaker: SPEAKER_07
Transcript:  I should just say that as a specialized in security, you did have to have physical access to the wallet to do this.

Start time: 7557.48
End time: 7582.31
Speaker: SPEAKER_08
Transcript:  Okay, but if I lost my wallet and you found it and you picked it up, it's a French ledger, it's called, or the French would say, leger.  I don't know.  It is malg. You found malg.  Well, a 15-year-old from the UK posted on his personal blog, Slimer Sheed, he said, I can get into it.  300-byte proof of concept code.

Start time: 7584.88
End time: 7592.86
Speaker: SPEAKER_07
Transcript:  I do applaud the youngster for his skills, though.  They need to get him down to DEF CON next time around so we can talk to everyone about how it was done.

Start time: 7593.12
End time: 7598.96
Speaker: SPEAKER_08
Transcript:  A lot of people do this, have hardware, physical hardware, or Bitcoin wallets. Is that a common...  I know a couple of people with them.

Start time: 7599.00
End time: 7601.00
Speaker: SPEAKER_05
Transcript:  I think they've said that they've sold millions worth of these.

Start time: 7601.46
End time: 7610.90
Speaker: SPEAKER_08
Transcript:  Wow. That seems risky to me. Like, if you lost that, of course I lost my password and I... same problem.

Start time: 7613.00
End time: 7618.76
Speaker: SPEAKER_07
Transcript:  Yes, it's... well, you know, it's the old adage of why do you rob banks? That's where the money is.

Start time: 7620.14
End time: 7643.68
Speaker: SPEAKER_08
Transcript:  I feel like Bitcoin has some structural issues that are going to keep it from becoming the world's currency in 10 years.  It costs a lot of money. What is it? $40 now? Or I guess it's gone back down to $20 to do a Bitcoin transaction.  It's slow because, you know, it's a slow process. The proof of work is now using more power than the country of Poland.  It's getting silly at this point.  I just feel like there's a long way to go on that.

Start time: 7644.24
End time: 7649.64
Speaker: SPEAKER_01
Transcript:  I don't know. How much power does it take to make paper money and coin money?  That's a good point. A lot, I would guess.

Start time: 7650.00
End time: 7653.92
Speaker: SPEAKER_08
Transcript:  I could see getting rid of that.  I would love it if we could get rid of the penny.

Start time: 7654.14
End time: 7659.32
Speaker: SPEAKER_07
Transcript:  Yes, so start with pennies.  Those things cost more than a penny to make and they're utterly useless.

Start time: 7660.40
End time: 7661.15
Speaker: SPEAKER_05
Transcript:  Nickels too, maybe?

Start time: 7662.02
End time: 7666.98
Speaker: SPEAKER_08
Transcript:  Let's get rid of dimes while we're at it. Who needs quarters? Oh, you need it for the laundry. I'm sorry, never mind.

Start time: 7667.42
End time: 7668.25
Speaker: UNKNOWN
Transcript:  And parking meters.

Start time: 7669.24
End time: 7671.30
Speaker: SPEAKER_07
Transcript:  I was about to show my age and say, and telephone boxes.

Start time: 7672.50
End time: 7690.00
Speaker: SPEAKER_08
Transcript:  What is that, Daddy? What's a telephone box?  That's the antiquated Ian Thompson. He is news editor at theregister.co.uk.  The antiquated Ian Thompson. Thank you.  He is not a number. Well, actually he is. 796 apparently.  Oh.  What number is that? Why does it say 796? Is that your prisoner number?

Start time: 7690.56
End time: 7697.03
Speaker: SPEAKER_07
Transcript:  No, no, this was just basically a shirt I threw on. I was up late watching the Grand Prix last night.  Oh, in the mood. You're in the mood. Formula One.

Start time: 7699.04
End time: 7701.70
Speaker: SPEAKER_08
Transcript:  Oh, yes. Oh, yes. Where is it?

Start time: 7702.00
End time: 7709.00
Speaker: SPEAKER_07
Transcript:  Australia at the moment.  How fun.  Yeah, so the timing was just about right too. I didn't have to get up at a stupid o'clock in the morning to watch it.

Start time: 7709.10
End time: 7721.02
Speaker: SPEAKER_08
Transcript:  We had last fall a hotel on the hairpin curve at Monaco.  Oh, I've walked that route.  Yeah, I just want to go and watch this. They go under the tunnels.

Start time: 7722.52
End time: 7730.00
Speaker: SPEAKER_07
Transcript:  When the, I've forgotten the name of the corner now, but yes, I've walked down that and I thought, I didn't want to do that in a standard car, let alone in a Formula One car.

Start time: 7730.06
End time: 7749.00
Speaker: SPEAKER_08
Transcript:  I took videos of people trying to navigate it at 12 miles an hour.  Yeah, I'd love to see that someday. That would be a lot of fun.  Harry McCracken, the technologizer. I want to call you that. I don't care if you're a technology editor at Fast Company, you'll always be the technologizer to me.  Harry, he's at Harry McCracken on the Twitter.

Start time: 7749.06
End time: 7763.74
Speaker: SPEAKER_05
Transcript:  Hey, I want to put in a plug for the Apple event on Tuesday.  You're going?  I'm going. I'll be in Chicago.  We didn't even talk about that.  And that will be covering it for our news section at FastCompany.com.  Good.  They're not streaming it, are they?  They're not streaming it, so the only way to see it will be to come in early.

Start time: 7764.00
End time: 7766.65
Speaker: SPEAKER_07
Transcript:  What are they ashamed of?  Well, I don't know. Do you think there will be big announcements?

Start time: 7769.99
End time: 7791.00
Speaker: SPEAKER_05
Transcript:  Well, if you're an iPad fan like I am, you'll be excited.  There'll be an education low-cost iPad.  And there is this really interesting war between Google and Apple over the education market and Chromebooks, such as IAN's, have really done well.  Partially because they're easy to manage and there are a lot of good things to be said from them as an education device.  Oh, I see. Admins love them.  But also because the price point is attractive.

Start time: 7791.38
End time: 7797.82
Speaker: SPEAKER_08
Transcript:  Yes. And they have a keyboard.  And they have a keyboard.  And children don't throw them like frisbees.

Start time: 7798.10
End time: 7801.00
Speaker: SPEAKER_05
Transcript:  And they're pretty sturdy. And if you do break one, it's okay.

Start time: 7801.22
End time: 7812.78
Speaker: SPEAKER_08
Transcript:  And honestly, if you're in school, how many years are you going to get out of an iPad?  One or two? That's a lot of money to spend for something that's going to be obsoleted so quickly.  Apple really has a long way to go to make this an education product.  But we'll see. Tuesday.

Start time: 7813.06
End time: 7816.00
Speaker: SPEAKER_05
Transcript:  If they have pencil support, that's really cool.

Start time: 7816.22
End time: 7820.00
Speaker: SPEAKER_08
Transcript:  I think that's what the invitation.  That would be an interactive thing, which you can't get on a cheap Chromebook.

Start time: 7820.00
End time: 7820.41
Speaker: SPEAKER_05
Transcript:  Right. Yeah.

Start time: 7821.02
End time: 7821.51
Speaker: SPEAKER_08
Transcript:  Strong glass.

Start time: 7823.02
End time: 7823.10
Speaker: SPEAKER_01
Transcript:  Yes.

Start time: 7824.24
End time: 7827.00
Speaker: SPEAKER_08
Transcript:  Strong. Rubber. Make them out of rubber all the way.

Start time: 7827.18
End time: 7831.00
Speaker: SPEAKER_05
Transcript:  You can get them in the Pixelbook, which is a really expensive Chromebook, but a cool one.

Start time: 7832.09
End time: 7834.80
Speaker: SPEAKER_08
Transcript:  The Pixelbook has a pen, but so does the Samsung $500 Samsung Plus.

Start time: 7835.00
End time: 7842.46
Speaker: SPEAKER_07
Transcript:  Yeah, I reviewed that and I wasn't particularly impressed.  Oh, I loved it. Really?  I've purchased two now.  But for the prime, they're charging, what, like $500 for it?

Start time: 7844.25
End time: 7859.47
Speaker: SPEAKER_08
Transcript:  $500. So what? How much do you pay for that?  A lot more.  Yeah, okay, but...  He's got a Pixel, the original Pixel, it looks like.  Yeah, it's the second generation.  No, it's V1.  V1. Pixelbook. No, no, they don't call it Pixelbook. Chromebook. What do they call it?  Chromebook.

Start time: 7860.00
End time: 7862.86
Speaker: SPEAKER_05
Transcript:  It's called the Pixelbook now.  Is it now the Pixelbook?  The Pixelbook is a Chromebook.

Start time: 7863.04
End time: 7871.30
Speaker: SPEAKER_08
Transcript:  Yeah, no, I have the new Pixelbook. I really like it, but it's a little pricey for a Chromebook.  Denise Howell, what does your son use in school? Does he use a Chromebook?

Start time: 7872.70
End time: 7886.00
Speaker: SPEAKER_01
Transcript:  No, they have Chromebooks at the school that are sort of for communal use, but my son was in the last class that didn't get their own to take home.  So they can use them at school, but he has his own computer at home that he uses.

Start time: 7886.10
End time: 7888.00
Speaker: SPEAKER_08
Transcript:  I bet he's well equipped. I don't think there's any...

Start time: 7888.75
End time: 7889.33
Speaker: SPEAKER_01
Transcript:  He's not bad.

Start time: 7890.16
End time: 7944.84
Speaker: SPEAKER_08
Transcript:  But the schools here had Chromebooks and they took them back and they got a grant and they bought iPads and I just thought it was a terrible, regressive move.  I wasn't happy about it at all. The only thing that they really used them for is to make movies, because they had a camera so they'd be making movies.  But it didn't seem like there wasn't a lot of curriculum. I didn't feel like it was a really good move.  Google has Classroom, they've got Google Docs, they have so many things you can do on a Chromebook. I don't know.  Well, we'll see. Tuesday. Denise Howell, she's on This Week in Law every Friday. DeniseHowell.info is her personal website. What is your tagline there?  Oh, you'd have to remind me. It's something about...  It's pretty funny about cats, I think. Eight out of ten owners who expressed a preference said their cats preferred this lawyer.  Denise, does that get you a lot of business? I'm just curious.

Start time: 7945.90
End time: 7948.44
Speaker: SPEAKER_01
Transcript:  You know, I do fine. I don't know if that's what's doing it.

Start time: 7949.40
End time: 7958.48
Speaker: SPEAKER_08
Transcript:  It's the cat lovers. It's the cat lovers. You do a great job on This Week in Law and you're my personal attorney from now on.

Start time: 7959.38
End time: 7961.00
Speaker: SPEAKER_01
Transcript:  I had a bot generate that and I was happy with it.

Start time: 7961.63
End time: 8067.82
Speaker: SPEAKER_08
Transcript:  Really? A bot generated that?  Yes.  Wow, that's kind of an interesting sideline. Thank you, Denise. Thank you very much, Harry. Thank you, of course, Ian.  It's always great to have all three of you on the show.  And thank you, most of all, for watching. We do This Week in Tech every Saturday afternoon, 3 p.m. Pacific, 6 p.m. Eastern time, 2200 UTC.  If you want to stop by and watch live, please do. You can watch it at twit.tv slash live.  If you're using the Roku app, Roku changed their API. That app is no longer functional.  But the good news is there are many ways you could still watch live on our Roku, including the YouTube app.  I think that's my favorite. Just search for the Twit channel on YouTube and you can watch us live on your big screen TV.  There are, of course, Twit apps for all the phones and devices. Lots of them out there.  Get one of those or come to our website, twit.tv slash live. Watch there.  You can also join us in the chat room. Nice people. Did I say Saturday? It's Sunday.  Did I say Saturday? It's Sunday, 3 p.m. Pacific, 6 p.m. Eastern time. I apologize. I hope I didn't confuse you.  Saturday we do the screensavers. Sunday we do Twit. Chat room is at irc.twit.tv.  And that's their job, is to keep me from saying the wrong day, among many others.  If you can't and if you want to be in studio, we love having in-studio visitors.  We've got Marie and we got Mike and we got Matt from Phoenix. Nice to have you. Please come say hi.  All you have to do is email tickets at twit.tv. There's no charge. We'd love to have you.  If you can't be here in studio, if you can't watch live, don't worry. On demand.  It's a podcast, folks. On demand, always available audio and yes, video at twit.tv or wherever you get your podcast.  Just subscribe and that way you'll get each and every week.  Thank you, everybody, for being here and we'll see you next time. Another Twit is in the can.

Start time: 8073.14
End time: 8077.82
Speaker: UNKNOWN
Transcript:  All right. Doing the Twit, baby. Doing the Twit. All right. Doing the Twit.

