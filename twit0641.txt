
[00:00:00.000 --> 00:00:02.600]   It's time for Twit this week in Tech.
[00:00:02.600 --> 00:00:06.040]   Wow, what a philosophical, wonderful, fascinating show.
[00:00:06.040 --> 00:00:08.520]   Perhaps has something to do with our great panelie in Thompson,
[00:00:08.520 --> 00:00:11.040]   his back Phil Libbon and Lisa Schmeiser.
[00:00:11.040 --> 00:00:14.960]   We're going to talk about Tesla's new truck and Roadster.
[00:00:14.960 --> 00:00:18.240]   Can artificial intelligence be a Godhead,
[00:00:18.240 --> 00:00:20.320]   the religion of AI,
[00:00:20.320 --> 00:00:24.800]   national AI wars, a backflip being robot and a new dog?
[00:00:24.800 --> 00:00:26.240]   One nice, one not so nice.
[00:00:26.240 --> 00:00:29.080]   It's all coming up next on Twit.
[00:00:29.080 --> 00:00:33.360]   Netcast you love.
[00:00:33.360 --> 00:00:35.320]   From people you trust.
[00:00:35.320 --> 00:00:40.800]   This is Twit.
[00:00:40.800 --> 00:00:47.640]   Bandwidth for this week in Tech is provided by CashFly at C-A-C-H-E-F-L-Y.com.
[00:00:47.640 --> 00:00:56.400]   This is Twit this week in Tech, episode 641,
[00:00:56.400 --> 00:01:01.920]   recorded Sunday, November 19, 2017, the Tesla Zamboni.
[00:01:01.920 --> 00:01:03.680]   This week at Tech is brought to you by
[00:01:03.680 --> 00:01:06.920]   Captera, find software solutions for your business needs.
[00:01:06.920 --> 00:01:11.760]   Captera is a free website with over 400 categories of business software
[00:01:11.760 --> 00:01:15.840]   and thousands of ratings and reviews from software users just like you.
[00:01:15.840 --> 00:01:19.800]   Visit Captera.com/Twit today.
[00:01:19.800 --> 00:01:23.680]   And by Scott Evest, a full line of functional clothing
[00:01:23.680 --> 00:01:27.040]   that features well-designed pockets so you can carry all of life's gadgets
[00:01:27.040 --> 00:01:28.600]   without carrying a bag.
[00:01:28.600 --> 00:01:31.880]   For a limited time, go to scottevest.com/Twit
[00:01:31.880 --> 00:01:36.560]   and use a promo code "CyberTwit" to save 30% on most items.
[00:01:36.560 --> 00:01:38.480]   And by WordPress.
[00:01:38.480 --> 00:01:40.800]   Plans started just $4 a month.
[00:01:40.800 --> 00:01:46.960]   Find out why 29% of the web runs on WordPress and get 15% off your brand new website today
[00:01:46.960 --> 00:01:49.920]   at WordPress.com/Twit.
[00:01:49.920 --> 00:01:54.640]   And by Betterment, the largest independent online financial advisor
[00:01:54.640 --> 00:01:59.120]   for a free five-minute investment review to help you assess your investment accounts,
[00:01:59.120 --> 00:02:04.480]   tax strategies, fees and risk exposure, visit betterment.com/Twit.
[00:02:04.480 --> 00:02:05.840]   No sign-up required.
[00:02:05.840 --> 00:02:12.240]   It's time for Twit this week in Tech, the show where we talk about the latest news
[00:02:12.240 --> 00:02:17.440]   from the Tech Sphere, some of the best tech analysts, journalists
[00:02:17.440 --> 00:02:21.840]   and just men about town, women about town, Ian Thompson is here
[00:02:21.840 --> 00:02:25.200]   from the register.co.uk always wonderful to have you in.
[00:02:25.200 --> 00:02:26.400]   It was great fun to be here.
[00:02:26.400 --> 00:02:29.600]   And I'm thrilled that you chose the Adventure Time mug.
[00:02:29.600 --> 00:02:35.040]   It tells a lot about our visitors with the coffee mug they choose.
[00:02:35.040 --> 00:02:37.120]   Honestly, I chose the largest one I could find.
[00:02:37.120 --> 00:02:41.200]   I'm going to be doing a lot of talking and I need all the energy I can get.
[00:02:41.200 --> 00:02:43.360]   Phil Liben is also here from all Turtles.
[00:02:43.360 --> 00:02:44.800]   Of course, remember him from Evernote.
[00:02:44.800 --> 00:02:46.080]   He chose the Twit mug.
[00:02:46.080 --> 00:02:47.040]   Good choice.
[00:02:47.040 --> 00:02:48.880]   Very bland choice, but good choice.
[00:02:48.880 --> 00:02:50.560]   I'm on my fourth one though.
[00:02:50.560 --> 00:02:51.200]   Ah, you are.
[00:02:51.200 --> 00:02:53.280]   Is it hot water or is it coffee?
[00:02:53.280 --> 00:02:55.200]   It's coffee.
[00:02:55.200 --> 00:02:58.800]   You can't tell what that mug is a dark mug.
[00:02:58.800 --> 00:03:02.560]   Lisa Schmeiser is here from the SuperSight for Windows.
[00:03:02.560 --> 00:03:03.120]   And of course...
[00:03:03.120 --> 00:03:06.080]   Not anymore, we rebranded just last week, believe it or not.
[00:03:06.080 --> 00:03:07.120]   Let's fix the lower third.
[00:03:07.120 --> 00:03:08.320]   What should it say?
[00:03:08.320 --> 00:03:10.080]   It should say ITPro today.
[00:03:10.080 --> 00:03:10.800]   Oh, I like it.
[00:03:10.800 --> 00:03:12.640]   Yeah, we consolidated several of our sites.
[00:03:12.640 --> 00:03:14.000]   We're now ITPro today.
[00:03:14.000 --> 00:03:17.280]   ITPro is one of the pet petri brands that's been around for a long time.
[00:03:17.280 --> 00:03:19.520]   Well, Windows ITPro has been around for a long time.
[00:03:19.520 --> 00:03:21.440]   This is just ITPro today.
[00:03:21.440 --> 00:03:22.880]   So it's more platform independent.
[00:03:22.880 --> 00:03:26.400]   We folded in the Windows ITPro family of sites,
[00:03:26.400 --> 00:03:28.880]   plus SuperSight for Windows and a few others.
[00:03:28.880 --> 00:03:31.200]   And we have folded it into your lower third.
[00:03:31.200 --> 00:03:31.680]   Excellent.
[00:03:31.680 --> 00:03:32.480]   ITPro today.
[00:03:32.480 --> 00:03:32.880]   Look at that.
[00:03:32.880 --> 00:03:33.920]   Editor there.
[00:03:33.920 --> 00:03:36.880]   So, also, you all have podcasts.
[00:03:36.880 --> 00:03:38.560]   Phil's got his all Turtles podcast.
[00:03:38.560 --> 00:03:41.440]   You have Phil and Lisa ruin the movies podcast.
[00:03:41.440 --> 00:03:43.200]   That's on the incomparable network.
[00:03:44.080 --> 00:03:46.240]   What movies have you ruined lately?
[00:03:46.240 --> 00:03:49.200]   We actually have a running queue of movies we need to ruin.
[00:03:49.200 --> 00:03:51.120]   [laughter]
[00:03:51.120 --> 00:03:55.600]   Because as you know, the premise of the podcast is that we review movies based solely on the movie trailers.
[00:03:55.600 --> 00:03:56.640]   We don't go to see the movies.
[00:03:56.640 --> 00:03:58.480]   We pre-roon the movies before they hit the end.
[00:03:58.480 --> 00:04:00.480]   Well, the trailers often do ruin the movies.
[00:04:00.480 --> 00:04:01.040]   I must say.
[00:04:01.040 --> 00:04:02.720]   I don't think that that's a stretch.
[00:04:02.720 --> 00:04:08.560]   For instance, I have steadfastly refused to see the new Star Wars trailer.
[00:04:08.560 --> 00:04:11.920]   December 14th, we've got tickets to see opening night.
[00:04:11.920 --> 00:04:12.960]   Don't watch the trailer.
[00:04:12.960 --> 00:04:15.520]   I think the trailers reveals a lot, I've heard.
[00:04:15.520 --> 00:04:16.320]   And I don't want to know.
[00:04:16.320 --> 00:04:18.160]   I did accidentally see a pork.
[00:04:18.160 --> 00:04:21.920]   Well, this was my complaint about the Thor Ragnarok trailers.
[00:04:21.920 --> 00:04:26.640]   Because one of the biggest plot twists was revealed in the trailer with a punchline.
[00:04:26.640 --> 00:04:28.320]   And I get why I think I did it.
[00:04:28.320 --> 00:04:33.840]   But again, it would have been a much more impactful moment in the film if it had been a big surprise.
[00:04:33.840 --> 00:04:37.200]   And the other thing is comedies often all the humor since the trailer.
[00:04:37.200 --> 00:04:37.440]   Yes.
[00:04:37.440 --> 00:04:40.320]   And they go to the comedy and there's nothing funny because they wasted it.
[00:04:40.320 --> 00:04:40.880]   Exactly.
[00:04:40.880 --> 00:04:42.560]   So don't watch trailers, kids.
[00:04:42.560 --> 00:04:44.320]   Let's see.
[00:04:44.320 --> 00:04:46.240]   Enough of that.
[00:04:46.240 --> 00:04:48.480]   I see the word about the pool, but no, it's.
[00:04:48.480 --> 00:04:50.560]   Port what are poor?
[00:04:50.560 --> 00:04:51.280]   No, don't tell me.
[00:04:51.280 --> 00:04:51.840]   I don't want to know.
[00:04:51.840 --> 00:04:52.480]   No, no.
[00:04:52.480 --> 00:04:53.440]   No.
[00:04:53.440 --> 00:04:54.400]   No.
[00:04:54.400 --> 00:04:54.800]   Spoiler.
[00:04:54.800 --> 00:04:55.840]   This is a spoiler freeze.
[00:04:55.840 --> 00:04:56.720]   Spoiler freeze.
[00:04:56.720 --> 00:04:57.760]   Spoiler freeze zone.
[00:04:57.760 --> 00:05:01.440]   What were the big stories of the week?
[00:05:01.440 --> 00:05:03.840]   You know, I probably should do this before the show be here.
[00:05:03.840 --> 00:05:04.560]   What?
[00:05:07.360 --> 00:05:10.240]   I like the Tesla.
[00:05:10.240 --> 00:05:12.560]   I called it the Tesla diesel and then it was pointed out.
[00:05:12.560 --> 00:05:14.560]   There ain't no diesel in a Tesla semi.
[00:05:14.560 --> 00:05:16.720]   A 18 wheeler.
[00:05:16.720 --> 00:05:20.320]   Elon Musk announced the first Tesla truck.
[00:05:20.320 --> 00:05:22.240]   There's a handsome truck.
[00:05:22.240 --> 00:05:23.520]   Yeah, but I've tempted.
[00:05:23.520 --> 00:05:24.080]   I want to get one.
[00:05:24.080 --> 00:05:26.400]   I wonder if Call of Candice, he says he owns every Tesla.
[00:05:26.400 --> 00:05:27.600]   Will he buy a diesel?
[00:05:27.600 --> 00:05:28.480]   I mean, a semi.
[00:05:28.480 --> 00:05:32.480]   I think he's more into the roads to myself, but given all the news we've had over the last
[00:05:32.480 --> 00:05:35.840]   month or so, when I woke up to the Twitter feed going, Tesla, Sammy, it was like,
[00:05:35.840 --> 00:05:37.120]   oh no, not him too.
[00:05:37.120 --> 00:05:41.120]   Nowadays, anybody, any man's name is mentioned.
[00:05:41.120 --> 00:05:42.240]   You go, oh God.
[00:05:42.240 --> 00:05:42.800]   Oh God.
[00:05:42.800 --> 00:05:44.560]   Well, actually, they've been a lot of back.
[00:05:44.560 --> 00:05:47.600]   I have to say some accusations about the Tesla factory in Fremont.
[00:05:47.600 --> 00:05:51.120]   Yeah, I covered one about nine months ago as well.
[00:05:51.120 --> 00:05:52.560]   This seems to be bubbling under.
[00:05:52.560 --> 00:05:53.680]   And he wants attitudes.
[00:05:53.680 --> 00:05:57.760]   Well, just get, you know, man up, like get a thick skin.
[00:05:57.760 --> 00:05:59.040]   I don't think that's exactly.
[00:05:59.040 --> 00:06:01.360]   It's a very comfortable up.
[00:06:01.360 --> 00:06:02.560]   Yeah, the right thing to say.
[00:06:02.560 --> 00:06:03.280]   Or you can say that.
[00:06:03.280 --> 00:06:03.600]   Yeah.
[00:06:03.600 --> 00:06:08.400]   So, the guy just worked himself into the ground, but it's not really something that you can,
[00:06:08.400 --> 00:06:09.680]   yeah, it's tricky.
[00:06:09.680 --> 00:06:16.080]   My big worry about the trunk though is, you know, I live near Oakland and they were talking
[00:06:16.080 --> 00:06:18.320]   about the aerodynamics better than a supercar.
[00:06:18.320 --> 00:06:21.360]   But if you've actually got to put a container capsule on the back of this,
[00:06:21.360 --> 00:06:23.600]   that's going to create an enormous amount of drag.
[00:06:23.600 --> 00:06:30.480]   It's just, in fact, they even admit that because they talk about range, but the range is like 30%
[00:06:30.480 --> 00:06:32.000]   better if you're convoying.
[00:06:32.000 --> 00:06:34.400]   In other words, if you're drafting off the truck in front.
[00:06:34.400 --> 00:06:34.720]   Yeah.
[00:06:34.720 --> 00:06:38.000]   So is there a plan to have some of these be self-driving as well?
[00:06:38.000 --> 00:06:38.640]   Or is this?
[00:06:38.640 --> 00:06:40.320]   Oh, you have to think that's inevitable.
[00:06:40.320 --> 00:06:44.160]   Yeah, because if you take a look at where truck driving is going,
[00:06:44.160 --> 00:06:47.920]   the two biggest costs are going to be fuel and human labor.
[00:06:47.920 --> 00:06:50.640]   And the trucking industry has been trying to reduce the human labor costs
[00:06:50.640 --> 00:06:54.000]   by finding ways to automate convoys.
[00:06:54.000 --> 00:06:56.880]   And when I was looking at the story, I thought, well, you've made a very pretty truck,
[00:06:56.880 --> 00:07:02.320]   but I don't see how this addresses your ongoing quest to cut labor costs.
[00:07:02.320 --> 00:07:04.320]   I don't know what the financial incentive is yet.
[00:07:04.320 --> 00:07:08.640]   But I would love to figure out where or hear anybody's explanation for
[00:07:08.640 --> 00:07:12.400]   why if you own a fleet of trucks, you're going to say, I'm buying me some Tesla.
[00:07:12.400 --> 00:07:13.680]   Well, I'll tell you one way.
[00:07:13.680 --> 00:07:17.040]   But by the way, Lobb Law, UPS number of people have already ordered these trucks.
[00:07:17.040 --> 00:07:18.400]   We don't even know what it's going to cost.
[00:07:18.400 --> 00:07:21.680]   One way you're reducing labor costs, there's only one seat in the thing.
[00:07:21.680 --> 00:07:23.040]   You can't.
[00:07:23.040 --> 00:07:23.600]   Yeah.
[00:07:23.600 --> 00:07:24.560]   It's just you.
[00:07:24.560 --> 00:07:26.720]   You can't even pick up hitchhikers anymore.
[00:07:26.720 --> 00:07:28.240]   Well, you can't bring your partner with you either,
[00:07:28.240 --> 00:07:32.480]   because there's a lot of long haul truckers that either drive in teams or they have a spouse with them.
[00:07:32.480 --> 00:07:33.760]   And look at the console.
[00:07:33.760 --> 00:07:36.640]   It's just basically a screen, a couple of screens.
[00:07:36.640 --> 00:07:38.560]   You see, this is the way I think the industry is going.
[00:07:38.560 --> 00:07:41.760]   When we've seen with with way with with way more.
[00:07:41.760 --> 00:07:42.000]   Auto.
[00:07:42.000 --> 00:07:42.480]   Auto.
[00:07:42.480 --> 00:07:42.960]   Auto.
[00:07:42.960 --> 00:07:49.200]   There they had to have a human driver in the cab because the truck can't handle city traffic.
[00:07:49.200 --> 00:07:50.400]   On the freeway, it's fine.
[00:07:50.400 --> 00:07:52.320]   You know, the guy can pretty much go to sleep.
[00:07:52.320 --> 00:07:52.800]   Please.
[00:07:52.800 --> 00:07:53.200]   Goodness.
[00:07:53.200 --> 00:07:53.360]   No.
[00:07:53.360 --> 00:07:55.680]   Even a Tesla, the freeway does fine right now.
[00:07:55.680 --> 00:07:56.880]   Even if the Teslas do fine.
[00:07:56.880 --> 00:07:58.160]   But I mean, but you don't want to do them.
[00:07:58.160 --> 00:07:59.360]   I think there's a TV road.
[00:07:59.360 --> 00:08:00.400]   It's raining.
[00:08:00.400 --> 00:08:02.800]   The team says not going to allow driverless trucks for all.
[00:08:02.800 --> 00:08:03.360]   It's inevitable.
[00:08:03.360 --> 00:08:04.960]   Oh, it is inevitable.
[00:08:04.960 --> 00:08:05.440]   Yeah.
[00:08:05.440 --> 00:08:10.320]   And you can see a lot of truck stop towns die because of automation.
[00:08:10.320 --> 00:08:12.560]   And the knock on effect in terms of employment can be used.
[00:08:12.560 --> 00:08:16.400]   Well, I think you may see like two stage trucking where you have long hauls that are mostly automated.
[00:08:16.400 --> 00:08:17.600]   And then there will be.
[00:08:17.600 --> 00:08:18.640]   The short set route.
[00:08:18.640 --> 00:08:20.560]   A short set route where it's a human driver.
[00:08:20.560 --> 00:08:26.400]   And perhaps this is where the Tesla market will come in handy because you're still reducing fuel costs,
[00:08:26.400 --> 00:08:27.840]   which is a big consideration.
[00:08:27.840 --> 00:08:29.680]   And they can be human guidance.
[00:08:29.680 --> 00:08:32.480]   They can handle Oakland or San Francisco during rush hour.
[00:08:32.480 --> 00:08:39.760]   500 miles of range, Tesla said that 80% of all trucking routes are 350 or less, 350 miles or less.
[00:08:39.760 --> 00:08:42.800]   It will charge in half an hour using a new mega charger.
[00:08:42.800 --> 00:08:48.160]   And that may be what keeps truck stops alive because you're going to you're going to have to go to a special charger.
[00:08:48.160 --> 00:08:52.080]   But the good thing is you can have your chicken fried steak and charge the diesel or diesel.
[00:08:52.080 --> 00:08:53.360]   Keep calling it diesel.
[00:08:53.360 --> 00:08:54.640]   Diesel is not a generic term.
[00:08:54.640 --> 00:08:55.760]   Take your shower and go.
[00:08:55.760 --> 00:08:56.720]   Tractor trailer.
[00:08:56.720 --> 00:08:57.280]   Yeah.
[00:08:57.280 --> 00:08:58.480]   It is an 18 wheeler.
[00:08:58.480 --> 00:09:02.320]   No rear view mirrors, but that makes me think it's probably got cameras galore.
[00:09:02.320 --> 00:09:03.680]   I think you guys are thinking about this wrong.
[00:09:03.680 --> 00:09:07.920]   I think it's not so much a truck as a case for the new Roadster.
[00:09:07.920 --> 00:09:11.040]   Well, that was the one more thing.
[00:09:11.040 --> 00:09:12.080]   And I thought that was great.
[00:09:12.080 --> 00:09:17.600]   Zac Estrada is writing. He says the first time a tractor trailer has been introduced with trance music.
[00:09:17.600 --> 00:09:20.400]   Lots of fog machines.
[00:09:20.400 --> 00:09:26.160]   And after Elon shows the truck and talks about, I think he drove it in as a matter of fact.
[00:09:26.160 --> 00:09:27.840]   I wasn't at the event, but it looked like he drove it in.
[00:09:27.840 --> 00:09:30.640]   Then he says, oh, one more thing.
[00:09:30.640 --> 00:09:35.920]   And the truck opens up and out of the back comes the last object for every car.
[00:09:36.880 --> 00:09:43.360]   $100,000 Roadster. This is how Tesla started with they took a Lotus Elise and they put an electric
[00:09:43.360 --> 00:09:47.760]   motor in it. This is not exactly the original Roadster.
[00:09:47.760 --> 00:09:52.080]   I think this is the new rule. From now on, companies must only announce products.
[00:09:52.080 --> 00:09:52.960]   Within products.
[00:09:52.960 --> 00:09:53.840]   Simultaneously.
[00:09:53.840 --> 00:09:55.440]   You need to have a truck and launch.
[00:09:55.440 --> 00:09:59.040]   You're from Russia. This is the nesting doll theory.
[00:09:59.040 --> 00:09:59.200]   Exactly.
[00:09:59.200 --> 00:10:00.400]   This is why the product launches.
[00:10:00.400 --> 00:10:01.200]   So like Apple.
[00:10:01.200 --> 00:10:05.920]   So like Apple has the Apple home and when they pop it open, there's another iPhone
[00:10:05.920 --> 00:10:07.280]   inside it and you pop it open.
[00:10:07.280 --> 00:10:07.840]   I like that.
[00:10:07.840 --> 00:10:11.120]   From now on, I only care about products that fit inside of other products.
[00:10:11.120 --> 00:10:12.240]   And at the end, it's always.
[00:10:12.240 --> 00:10:15.440]   So when this is a plot line in the next season of Silicon Valley,
[00:10:15.440 --> 00:10:18.800]   I hope that you reach out to Mike Judge and tell him that you thought of it first.
[00:10:18.800 --> 00:10:20.640]   That's right. You heard it here first.
[00:10:20.640 --> 00:10:25.280]   I can see the nesting doll strategy becoming huge in Silicon Valley.
[00:10:25.280 --> 00:10:29.520]   At the end of the day, this is the typical super curl that companies might
[00:10:29.520 --> 00:10:30.800]   make like McLaren are putting out.
[00:10:30.800 --> 00:10:35.360]   They'll put out a few thousand of them that we pre-bolt before they're rolled out.
[00:10:35.360 --> 00:10:37.680]   And they're oppressed to each product which drives the brand.
[00:10:37.680 --> 00:10:39.440]   I'm more worried about the Model 3.
[00:10:39.440 --> 00:10:43.040]   If there's been very little talk about where they're coming on the Model 3
[00:10:43.040 --> 00:10:44.800]   and that's got to get sorted out.
[00:10:44.800 --> 00:10:46.800]   If they're going to be a large-style car manufacturer.
[00:10:46.800 --> 00:10:48.560]   I'm not too worried about, to be honest.
[00:10:48.560 --> 00:10:53.440]   I feel like Tesla always has production problems when they begin new models.
[00:10:53.440 --> 00:10:57.520]   The Model 3 isn't so very different from previous Teslas as far as I can tell.
[00:10:57.520 --> 00:10:58.720]   No, but they need to get volume.
[00:10:58.720 --> 00:11:01.920]   There's a lot of, there's hundreds of thousands of people waiting for this car.
[00:11:01.920 --> 00:11:02.480]   Yeah, they're getting it.
[00:11:02.480 --> 00:11:05.520]   But I think this happened with the Model S when it came out.
[00:11:05.520 --> 00:11:07.600]   I know it happened with the Model X because I bought one.
[00:11:07.600 --> 00:11:09.520]   So is it a scale?
[00:11:09.520 --> 00:11:13.360]   It's a concern but they can actually scale up the business from being a micro luxury
[00:11:13.360 --> 00:11:16.480]   to something that actually fundamentally changes American transport?
[00:11:16.480 --> 00:11:16.960]   Pretty much.
[00:11:16.960 --> 00:11:17.280]   Yeah.
[00:11:17.280 --> 00:11:21.520]   I mean, it's just whether or not they can do that in America because they made a big thing
[00:11:21.520 --> 00:11:23.120]   about manufacturing in America.
[00:11:23.120 --> 00:11:26.080]   But at the same time, the China plants now coming online.
[00:11:26.080 --> 00:11:30.160]   It's going to be interesting to see how it develops.
[00:11:30.160 --> 00:11:35.040]   And that yes, we do need something to give the car industry a massive boot up the backside.
[00:11:35.040 --> 00:11:38.560]   But it's whether or not you can do it in this country economically,
[00:11:38.560 --> 00:11:40.320]   or you've got to go to China to do it.
[00:11:40.320 --> 00:11:42.240]   I don't think I'm ever going to own a car again.
[00:11:42.240 --> 00:11:43.680]   I really don't.
[00:11:43.680 --> 00:11:44.480]   I don't think I'm ever buying it.
[00:11:44.480 --> 00:11:46.240]   People are everywhere now and stuff like that.
[00:11:46.240 --> 00:11:47.280]   I have a car.
[00:11:47.280 --> 00:11:48.960]   I haven't touched it in about a year and a half.
[00:11:48.960 --> 00:11:49.920]   Wow.
[00:11:49.920 --> 00:11:52.800]   It literally, not only won't it start, it doesn't even like pretend,
[00:11:52.800 --> 00:11:56.000]   it doesn't even make that sad little noise when you try to start it.
[00:11:56.000 --> 00:11:56.880]   It's like, it's just dead.
[00:11:56.880 --> 00:11:57.680]   No, it doesn't even do that.
[00:11:57.680 --> 00:11:59.600]   It's like a statue of a car at this point.
[00:11:59.600 --> 00:12:01.360]   I need to figure out how to like haul it out of my car.
[00:12:01.360 --> 00:12:03.520]   You can drain the oil, put it up in blocks and then...
[00:12:03.520 --> 00:12:04.800]   I don't think I'm ever going to own a car again.
[00:12:04.800 --> 00:12:05.680]   Take a hatchet to it.
[00:12:05.680 --> 00:12:10.240]   I'd be interested to see if Tesla ever actually moves into say agricultural vehicles.
[00:12:10.240 --> 00:12:11.120]   I mean, they're one of the...
[00:12:11.120 --> 00:12:12.720]   I think I'm ever going to own an agricultural vehicle.
[00:12:12.720 --> 00:12:13.600]   No, but I was thinking if they...
[00:12:13.600 --> 00:12:16.080]   Don't you want to combine them?
[00:12:16.080 --> 00:12:17.680]   I should kind of do.
[00:12:17.680 --> 00:12:22.320]   Well, this is what I was talking to some engineers at VW about self-driving cars
[00:12:22.320 --> 00:12:24.000]   at one of these internet of things shows.
[00:12:24.000 --> 00:12:28.160]   And what the engineers and I were talking about was it's one thing to
[00:12:29.120 --> 00:12:33.040]   talk about transforming the auto industry and have these ranges when you're in a pretty
[00:12:33.040 --> 00:12:37.520]   well-populated urban grid where you can have your charging stations every so often.
[00:12:37.520 --> 00:12:40.320]   But America is also comprised of vast tracts of land.
[00:12:40.320 --> 00:12:41.520]   Huge tracts of land.
[00:12:41.520 --> 00:12:43.120]   It's not suitable.
[00:12:43.120 --> 00:12:43.920]   It's not suitable.
[00:12:43.920 --> 00:12:46.320]   It's also electric stuff.
[00:12:46.320 --> 00:12:48.160]   It's also electric stuff is good in cold climates.
[00:12:48.160 --> 00:12:48.720]   Yeah.
[00:12:48.720 --> 00:12:49.200]   Yeah.
[00:12:49.200 --> 00:12:50.000]   But for...
[00:12:50.000 --> 00:12:55.440]   There's still a huge agricultural sector in America and there are things happening there with...
[00:12:56.000 --> 00:12:58.880]   They're bringing more robots online to do work and things like that.
[00:12:58.880 --> 00:13:03.600]   But you're still looking at a lot of technology that has pretty fundamentally unchanged
[00:13:03.600 --> 00:13:04.720]   over the last 50 years.
[00:13:04.720 --> 00:13:05.920]   Like, nobody is...
[00:13:05.920 --> 00:13:10.720]   Few people are wringing their hands worrying about the energy you think that is your average
[00:13:10.720 --> 00:13:12.320]   combine or thrasher or...
[00:13:12.320 --> 00:13:13.760]   Yeah, that'd be a great use for it.
[00:13:13.760 --> 00:13:14.000]   Yeah.
[00:13:14.000 --> 00:13:14.880]   So I'd like to see some...
[00:13:14.880 --> 00:13:17.600]   And Zamboni, let's not forget hockey rinks everywhere.
[00:13:17.600 --> 00:13:19.440]   It should have Tesla Zamboni's.
[00:13:19.440 --> 00:13:20.480]   But I mean, there's also...
[00:13:20.480 --> 00:13:22.480]   I buy a Tesla Zamboni.
[00:13:22.480 --> 00:13:23.280]   There's also a...
[00:13:23.280 --> 00:13:25.280]   That's a straight one.
[00:13:25.280 --> 00:13:30.640]   I mean, there's also the rest of the world angle in the Holland, the UK, France, all-instusing
[00:13:30.640 --> 00:13:38.320]   legislation to basically ban the sale of new common-based cars in 2025, 2030, whenever
[00:13:38.320 --> 00:13:40.160]   China is going the same way.
[00:13:40.160 --> 00:13:40.640]   Yeah.
[00:13:40.640 --> 00:13:42.480]   We're in the middle of a fundamental shift here.
[00:13:42.480 --> 00:13:42.960]   Yeah.
[00:13:42.960 --> 00:13:46.160]   I'd just like to see American car companies go, "Oh, yeah, hang on, we're missing a truck."
[00:13:46.160 --> 00:13:50.720]   Well, interestingly, when Tesla announced the truck, it was pointed out immediately that
[00:13:50.720 --> 00:13:55.200]   Cummins, which is a big diesel truck manufacturer and Daimler, among others, have already announced
[00:13:55.200 --> 00:13:56.400]   all the plans to electrify.
[00:13:56.400 --> 00:13:56.880]   Yeah.
[00:13:56.880 --> 00:13:58.160]   Oh, this makes sense.
[00:13:58.160 --> 00:13:59.920]   We want to eliminate pollution.
[00:13:59.920 --> 00:14:03.600]   A fuel economy is a big part of the business model.
[00:14:03.600 --> 00:14:04.640]   This kind of makes sense.
[00:14:04.640 --> 00:14:05.360]   Mm.
[00:14:05.360 --> 00:14:06.080]   That's logical.
[00:14:06.080 --> 00:14:09.600]   I mean, isn't it, though, the case that none of this makes any sense until...
[00:14:09.600 --> 00:14:14.880]   And this is imminent electric power generation and batteries, get to the point where it is
[00:14:14.880 --> 00:14:16.480]   more economical than fossil fuels.
[00:14:16.480 --> 00:14:16.880]   Yeah.
[00:14:16.880 --> 00:14:18.880]   That's where we have to get before this really.
[00:14:18.880 --> 00:14:20.720]   We've got an infrastructure that supports it, too.
[00:14:20.720 --> 00:14:21.040]   Yeah.
[00:14:21.040 --> 00:14:22.560]   Well, that's a big problem.
[00:14:22.560 --> 00:14:24.800]   That's why Elon did the supercharger network.
[00:14:24.800 --> 00:14:28.400]   And I think that reduces a lot of the range anxiety.
[00:14:28.400 --> 00:14:28.800]   I think there's...
[00:14:28.800 --> 00:14:29.760]   Sorry, go ahead.
[00:14:29.760 --> 00:14:30.080]   Please.
[00:14:30.080 --> 00:14:34.960]   Oh, no, no, I think there's three factors here, which are all being munched together,
[00:14:34.960 --> 00:14:37.440]   but are really separate that are going to affect the future of this industry.
[00:14:37.440 --> 00:14:41.120]   There's electricity versus fossil fuels.
[00:14:41.120 --> 00:14:41.840]   There's autonomy.
[00:14:41.840 --> 00:14:46.000]   And then there's the future of ownership or financing of the thing.
[00:14:46.000 --> 00:14:46.080]   Yes.
[00:14:46.080 --> 00:14:48.400]   I think those are three independent things that are...
[00:14:48.400 --> 00:14:51.440]   Everyone kind of clumps together, but they're probably going to move...
[00:14:52.000 --> 00:14:55.920]   They're all changing significantly, but they're going to move at different speeds.
[00:14:55.920 --> 00:14:58.000]   I feel like autonomy and ownership are linked.
[00:14:58.000 --> 00:14:59.280]   I don't know.
[00:14:59.280 --> 00:15:02.560]   People are going to buy autonomous cars, but I think they'll be very comfortable getting
[00:15:02.560 --> 00:15:04.640]   in an autonomous shuttle or an autonomous Uber.
[00:15:04.640 --> 00:15:08.240]   I think the ownership thing is rippling out now, because you mentioned Uber,
[00:15:08.240 --> 00:15:09.120]   but you're talking...
[00:15:09.120 --> 00:15:12.960]   You look at homeownership trends and homeownership as an idea is losing traction.
[00:15:12.960 --> 00:15:13.760]   Is it really?
[00:15:13.760 --> 00:15:13.760]   Yeah.
[00:15:13.760 --> 00:15:16.720]   It's losing traction, especially among younger people who have student loans.
[00:15:16.720 --> 00:15:21.440]   And then if you look at the rise of streaming media services where people are like,
[00:15:21.440 --> 00:15:26.400]   "I'm fine with not having to buy a terabyte drive to own all this music and all of this film."
[00:15:26.400 --> 00:15:26.720]   Yeah, right.
[00:15:26.720 --> 00:15:27.920]   Nobody even knows what that means anymore.
[00:15:27.920 --> 00:15:28.880]   They happen so fast.
[00:15:28.880 --> 00:15:29.120]   Yeah.
[00:15:29.120 --> 00:15:29.600]   They happen.
[00:15:29.600 --> 00:15:30.880]   Or there's clothing rental.
[00:15:30.880 --> 00:15:34.160]   For example, one of the growing sectors is actually wedding clothing rental.
[00:15:34.160 --> 00:15:35.920]   Rent the runway has just launched...
[00:15:35.920 --> 00:15:36.960]   They do dresses.
[00:15:36.960 --> 00:15:38.320]   They now do wedding dresses.
[00:15:38.320 --> 00:15:38.720]   Right.
[00:15:38.720 --> 00:15:40.320]   Rital dresses and bridesmaids dresses.
[00:15:40.320 --> 00:15:42.880]   And their argument is, "Why should you have to take thousands of dollars?"
[00:15:42.880 --> 00:15:43.360]   Yeah.
[00:15:43.360 --> 00:15:44.400]   Or in my case, three times.
[00:15:44.400 --> 00:15:45.920]   And that's exactly right.
[00:15:45.920 --> 00:15:50.320]   For ownership, it comes down at a macro level, comes down to utilization rates.
[00:15:50.320 --> 00:15:52.880]   The utilization rates of cars right now is terrible.
[00:15:52.880 --> 00:15:55.520]   Like the utilization rate of cars, I forget what the latest numbers are.
[00:15:55.520 --> 00:15:56.000]   You buy it.
[00:15:56.000 --> 00:15:57.600]   You drive it maybe five...
[00:15:57.600 --> 00:15:58.800]   Not even that long.
[00:15:58.800 --> 00:16:02.400]   You drive it maybe an hour a day and the 23 hours a day is sitting in a garage.
[00:16:02.400 --> 00:16:03.600]   I think it's worse on average.
[00:16:03.600 --> 00:16:03.840]   Yeah.
[00:16:03.840 --> 00:16:04.560]   It's even worse.
[00:16:04.560 --> 00:16:08.960]   I think the utilization rates of cars is comparable to the utilization rate of like
[00:16:08.960 --> 00:16:11.120]   luxury watches or jewelry.
[00:16:11.120 --> 00:16:11.600]   Yeah.
[00:16:11.600 --> 00:16:14.480]   And so you'll get to the same ownership stress.
[00:16:14.480 --> 00:16:15.520]   Like at some point it'll be...
[00:16:15.520 --> 00:16:15.600]   And it's huge.
[00:16:15.600 --> 00:16:16.560]   It'll drain on resources.
[00:16:16.560 --> 00:16:17.680]   All the metals, all the...
[00:16:17.680 --> 00:16:18.320]   Yeah.
[00:16:18.320 --> 00:16:19.280]   It's a luxury item.
[00:16:19.280 --> 00:16:19.920]   Yeah.
[00:16:19.920 --> 00:16:21.120]   And the future will be.
[00:16:21.120 --> 00:16:24.480]   My husband, I frequently talk about ditching our car because the number one reason we have it
[00:16:24.480 --> 00:16:26.640]   is to actually leave the Bay Area and go camping.
[00:16:26.640 --> 00:16:28.320]   Or to live in a city as you feel.
[00:16:28.320 --> 00:16:28.800]   Yeah, I love it.
[00:16:28.800 --> 00:16:30.400]   So I think for city dwellers...
[00:16:30.400 --> 00:16:31.360]   I live in San Francisco.
[00:16:31.360 --> 00:16:32.320]   Okay.
[00:16:32.320 --> 00:16:32.560]   Yeah.
[00:16:32.560 --> 00:16:33.360]   A small fishy...
[00:16:33.360 --> 00:16:33.680]   A small fair...
[00:16:33.680 --> 00:16:34.640]   A small fair village.
[00:16:34.640 --> 00:16:34.880]   Yeah.
[00:16:34.880 --> 00:16:35.440]   Fairly city.
[00:16:35.440 --> 00:16:37.680]   But I think for people who live in urban areas, how about that?
[00:16:37.680 --> 00:16:38.240]   Yeah.
[00:16:38.240 --> 00:16:40.080]   This is not an uncommon idea.
[00:16:40.080 --> 00:16:42.880]   Most people, I would imagine most people in New York don't want to own a car.
[00:16:42.880 --> 00:16:44.880]   So it's not hideously expensive to college it.
[00:16:44.880 --> 00:16:45.120]   Yeah.
[00:16:45.120 --> 00:16:46.880]   And we do our stuff and we do our errands on flip-flips.
[00:16:46.880 --> 00:16:47.840]   And there's mass transit.
[00:16:47.840 --> 00:16:49.200]   So what do you need a car for?
[00:16:49.200 --> 00:16:52.480]   But then as you point out, the rest of the country is not
[00:16:52.480 --> 00:16:54.160]   organized and you really need it.
[00:16:54.160 --> 00:16:58.400]   But before it comes to mass transit, you've got New York, Chicago,
[00:16:58.400 --> 00:17:01.600]   a little bit of Washington, some Boston, San Francisco.
[00:17:01.600 --> 00:17:04.000]   That's pretty much it.
[00:17:04.000 --> 00:17:06.640]   For major cities with a good public transport network.
[00:17:06.640 --> 00:17:06.880]   Yeah.
[00:17:06.880 --> 00:17:08.160]   And I'm using the word good and...
[00:17:08.160 --> 00:17:11.440]   That's a very generous, very generous stuff.
[00:17:11.440 --> 00:17:11.840]   Yeah.
[00:17:11.840 --> 00:17:13.520]   We were robbed as a country.
[00:17:13.520 --> 00:17:17.280]   Because if you go to Europe, as you know, you see these incredible mass transit
[00:17:17.280 --> 00:17:20.080]   networks, the train networks, it's so easy to get around.
[00:17:20.080 --> 00:17:20.640]   We had the ease that we'd get around.
[00:17:20.640 --> 00:17:21.200]   We were robbed down in the 50s.
[00:17:21.200 --> 00:17:24.240]   Yeah, we were robbed because the motor companies and the
[00:17:24.240 --> 00:17:26.480]   west companies and the old companies got together.
[00:17:26.480 --> 00:17:27.600]   LA had a fun...
[00:17:27.600 --> 00:17:28.880]   I mean, we were also bigger.
[00:17:28.880 --> 00:17:29.280]   Right.
[00:17:29.280 --> 00:17:32.640]   There's just much bigger distances between major cities in the US than in Europe.
[00:17:32.640 --> 00:17:33.440]   Oh, between cities.
[00:17:33.440 --> 00:17:37.040]   But in Los Angeles, you used to have the most fantastic public transport
[00:17:37.040 --> 00:17:38.080]   tram network.
[00:17:38.080 --> 00:17:39.360]   And that was all ripped out.
[00:17:39.360 --> 00:17:42.560]   Now two hours in commuting traffic is the norm.
[00:17:42.560 --> 00:17:43.360]   Yeah.
[00:17:43.360 --> 00:17:46.960]   But basically, there is no long-term saving the car companies.
[00:17:46.960 --> 00:17:49.840]   Because the utilization rates are going exactly against them.
[00:17:49.840 --> 00:17:50.160]   Yeah.
[00:17:50.160 --> 00:17:54.000]   So even if they go autonomous and they go to fleet rentals and...
[00:17:54.000 --> 00:17:57.120]   Yeah, because there'll just be far fewer cars because you're not going to be
[00:17:57.120 --> 00:17:58.560]   sitting at 3% utilization.
[00:17:58.560 --> 00:18:00.000]   So this is maybe Tesla's...
[00:18:00.000 --> 00:18:01.840]   I mean, I'm sure Elon's smart enough to notice.
[00:18:01.840 --> 00:18:02.400]   Exactly.
[00:18:02.400 --> 00:18:03.440]   That's his opportunity.
[00:18:03.440 --> 00:18:05.520]   Well, Tesla will probably succeed in that market.
[00:18:05.520 --> 00:18:08.720]   And the traditional car companies will see their unit volumes.
[00:18:08.720 --> 00:18:09.200]   How do I start?
[00:18:09.200 --> 00:18:12.000]   They were basically a battery company that's using a car to promote the battery.
[00:18:12.000 --> 00:18:12.240]   That was...
[00:18:12.240 --> 00:18:12.800]   It's accurate.
[00:18:12.800 --> 00:18:13.200]   Yeah.
[00:18:13.200 --> 00:18:13.680]   Exactly.
[00:18:13.680 --> 00:18:14.960]   This is where it's going.
[00:18:14.960 --> 00:18:18.960]   It's that move that you were talking about, the electricity grid is being central to this.
[00:18:18.960 --> 00:18:25.200]   It's that move from large-scale inefficient transmission plants to everyone generates their own
[00:18:25.200 --> 00:18:30.320]   and an aggregate network with certain baseline stations providing the current.
[00:18:30.320 --> 00:18:34.800]   You were reminding me of a vendor I interviewed at CES earlier this year.
[00:18:34.800 --> 00:18:38.240]   He was selling a solar panel that is probably about the size of Leo's gizmo there.
[00:18:38.240 --> 00:18:41.680]   My surface studio all in one.
[00:18:41.680 --> 00:18:42.640]   Isn't that gorgeous?
[00:18:42.640 --> 00:18:43.120]   Super duper.
[00:18:43.120 --> 00:18:44.640]   It was a solar panel that size.
[00:18:44.640 --> 00:18:49.040]   And he said that his market was actually electric car owners in Silicon Valley because he said
[00:18:49.040 --> 00:18:52.480]   you can stick this thing outside for a couple hours, power it up, it can juice up your car.
[00:18:52.480 --> 00:18:56.560]   And this way, you're independent of the electrical grid.
[00:18:56.560 --> 00:18:59.040]   I think we're one breakthrough away from this being viable.
[00:18:59.040 --> 00:19:05.680]   Something like a 40% efficient solar panel or a battery that can be charged quickly
[00:19:05.680 --> 00:19:07.680]   and is easier to make.
[00:19:07.680 --> 00:19:11.280]   I mean, it's field and of course everybody's working on this kind of innovation.
[00:19:11.280 --> 00:19:12.960]   You're also going to have to train users though.
[00:19:12.960 --> 00:19:16.880]   I mean, the reason gas gauges are so effective is because people think of gasoline in terms of
[00:19:16.880 --> 00:19:17.200]   some.
[00:19:17.200 --> 00:19:19.600]   No, people think of gasoline in terms of some of their assuming.
[00:19:19.600 --> 00:19:23.120]   They're assuming and they can see how much they're consuming and it affects their behavior.
[00:19:23.120 --> 00:19:26.720]   And it even affects the way they think about it in terms of am I wasting it?
[00:19:26.720 --> 00:19:29.360]   Am I using it correctly? Am I shopping around for the best deal?
[00:19:29.360 --> 00:19:32.720]   Until you can get to the point when, oh, there's gas buddy.
[00:19:32.720 --> 00:19:34.000]   People are really weird.
[00:19:34.000 --> 00:19:35.680]   Gas buddy is an app.
[00:19:35.680 --> 00:19:39.200]   Yeah, it's people are weird about how much they're paying per gallon of gas.
[00:19:39.200 --> 00:19:41.440]   As they should be when gas gets a little more monthly.
[00:19:41.440 --> 00:19:44.880]   And the extent they're willing to drive further than the cost savings will get
[00:19:44.880 --> 00:19:44.880]   together.
[00:19:44.880 --> 00:19:47.440]   It is, yeah, again, again weird.
[00:19:47.440 --> 00:19:50.160]   But you know, until people start thinking of battery power.
[00:19:50.160 --> 00:19:52.240]   You see your battery go down in a Tesla.
[00:19:52.240 --> 00:19:53.600]   You see where your battery is.
[00:19:53.600 --> 00:19:56.400]   You know how many actually it's really just a linear either.
[00:19:56.400 --> 00:19:57.360]   Yeah, it isn't.
[00:19:57.360 --> 00:19:59.120]   No, it depends on how you step on the pedal.
[00:19:59.120 --> 00:20:05.040]   But I think one of the things Tesla does quite well with that big screen is give you an idea of
[00:20:05.040 --> 00:20:08.400]   how much power you're going to have left when you get to where you're going.
[00:20:08.400 --> 00:20:12.080]   And it's pretty accurate. It will reroute you to chargers if you're going to need one.
[00:20:12.080 --> 00:20:14.640]   I think it handles that issue quite well.
[00:20:14.640 --> 00:20:16.800]   I know where I'm sitting.
[00:20:16.800 --> 00:20:18.960]   Tesla did not have a very good quarter.
[00:20:18.960 --> 00:20:23.600]   Worst financial quarter ever.
[00:20:23.600 --> 00:20:25.600]   I think short term Tesla is having trouble.
[00:20:25.600 --> 00:20:28.400]   It's but they've got a certain amount of cash to burn.
[00:20:28.400 --> 00:20:31.920]   You know, they they're not going to run short of funding any time soon.
[00:20:31.920 --> 00:20:34.320]   The question is, is this a small height as well?
[00:20:34.320 --> 00:20:35.600]   They get themselves sorted out.
[00:20:35.600 --> 00:20:38.000]   It's all about the model three and the gigafactory.
[00:20:38.000 --> 00:20:42.720]   In fact, Elon's blaming the gigafactory and problems making batteries for the
[00:20:42.720 --> 00:20:45.520]   one we're all on the roof on a couple of weeks ago.
[00:20:45.520 --> 00:20:48.480]   I know they're making s'mores on the roof of the gigafactory.
[00:20:48.480 --> 00:20:51.200]   If I was to ever buy a car again, it would probably be a model three.
[00:20:51.200 --> 00:20:53.920]   I can't imagine getting anything else, but I'll probably never want one.
[00:20:53.920 --> 00:20:58.720]   But here in Petaluma, I can't I couldn't not have a car.
[00:20:58.720 --> 00:21:02.480]   I I guess I could bicycle everywhere, but it rains.
[00:21:02.480 --> 00:21:03.200]   Money with Davis.
[00:21:03.200 --> 00:21:06.000]   It'd be good for me.
[00:21:06.000 --> 00:21:07.440]   Be better for me.
[00:21:07.440 --> 00:21:10.480]   There's obviously a bunch of people that are going to still have cars,
[00:21:10.480 --> 00:21:11.360]   whether they own them or not.
[00:21:11.360 --> 00:21:14.720]   That's the ownership question, different from the, you know,
[00:21:14.720 --> 00:21:16.000]   who's actually going to drive it question.
[00:21:16.000 --> 00:21:20.720]   But most of the population in the world is living in urban centers.
[00:21:20.720 --> 00:21:21.600]   That's what's growing.
[00:21:21.600 --> 00:21:24.960]   So just the sheer numbers are obviously going only in one direction,
[00:21:24.960 --> 00:21:26.800]   which is fewer unit sales.
[00:21:26.800 --> 00:21:27.280]   Yeah.
[00:21:27.280 --> 00:21:28.560]   No future for the big.
[00:21:28.560 --> 00:21:29.760]   These are mass certain areas.
[00:21:29.760 --> 00:21:31.120]   As long as you can find a way to move people around,
[00:21:31.120 --> 00:21:34.000]   whether they're differently abled or they're senior citizens,
[00:21:34.000 --> 00:21:35.200]   or they're very small children.
[00:21:35.200 --> 00:21:38.480]   I mean, one of the biggest reasons to have a car is you have people that can't walk.
[00:21:38.480 --> 00:21:42.880]   Oh, but for my mom who's 84 and got rid of her car some years ago,
[00:21:42.880 --> 00:21:47.280]   she does use Uber, but if an autonomous shuttle comes,
[00:21:47.280 --> 00:21:49.280]   her door at her beck and call the future is a matter of two.
[00:21:49.280 --> 00:21:51.600]   So many senior citizens who have talked about that,
[00:21:51.600 --> 00:21:54.160]   who are like, I want the autonomous car I just thought you go.
[00:21:54.160 --> 00:21:56.480]   And, you know, I'm not going to be able to drive in a couple of years.
[00:21:56.480 --> 00:21:58.800]   And I'm praying that though, I'll be able to give you a few more.
[00:21:58.800 --> 00:22:02.960]   But it's those old-timey, matic tubes that used to have at banks.
[00:22:03.760 --> 00:22:05.200]   That's the hyperloop. That's the future.
[00:22:05.200 --> 00:22:06.160]   That's the future.
[00:22:06.160 --> 00:22:06.960]   That's the future of my credit.
[00:22:06.960 --> 00:22:07.360]   That's it?
[00:22:07.360 --> 00:22:09.040]   You've just cited the future of my credit.
[00:22:09.040 --> 00:22:10.240]   Yeah, that's where we're going.
[00:22:10.240 --> 00:22:12.720]   The boring machine stuff though.
[00:22:12.720 --> 00:22:15.280]   I seriously think Elon's lost the plot on this,
[00:22:15.280 --> 00:22:19.520]   because the idea of being able to just bore tunnels under cities
[00:22:19.520 --> 00:22:21.200]   might work in a few select cities,
[00:22:21.200 --> 00:22:22.240]   but try that in London.
[00:22:22.240 --> 00:22:24.320]   You want to find yourself in a whole world of trouble.
[00:22:24.320 --> 00:22:24.640]   Yeah.
[00:22:24.640 --> 00:22:26.560]   It's kind of a more lock imagery there, guys.
[00:22:26.560 --> 00:22:27.200]   Yeah.
[00:22:27.200 --> 00:22:27.920]   Very good.
[00:22:27.920 --> 00:22:28.320]   It's you.
[00:22:28.320 --> 00:22:29.360]   Well, wow.
[00:22:29.360 --> 00:22:31.680]   What a literate crew we have here.
[00:22:31.680 --> 00:22:32.640]   That's Lisa Schmeiser.
[00:22:32.640 --> 00:22:33.200]   Mm-hmm.
[00:22:33.200 --> 00:22:33.840]   What is it called?
[00:22:33.840 --> 00:22:34.960]   IT Pro today now?
[00:22:34.960 --> 00:22:36.240]   Yes, IT Pro today.
[00:22:36.240 --> 00:22:36.560]   Yes.
[00:22:36.560 --> 00:22:38.400]   And she is...
[00:22:38.400 --> 00:22:40.240]   What is your...
[00:22:40.240 --> 00:22:41.440]   The Iron Feminist on Twitter?
[00:22:41.440 --> 00:22:41.760]   What's your name?
[00:22:41.760 --> 00:22:42.880]   The Immortal Iron Feminist.
[00:22:42.880 --> 00:22:44.320]   The Immortal Iron Feminist.
[00:22:44.320 --> 00:22:45.680]   We'll talk about it.
[00:22:45.680 --> 00:22:48.880]   The advantage of those 50 characters in my title now.
[00:22:48.880 --> 00:22:49.840]   Twitter is evolving.
[00:22:49.840 --> 00:22:51.520]   Now you have 50 characters for your title.
[00:22:51.520 --> 00:22:53.440]   You have 280 characters for your tweet.
[00:22:53.440 --> 00:22:54.080]   Yeah.
[00:22:54.080 --> 00:22:55.360]   We'll talk a little bit about that.
[00:22:55.360 --> 00:22:56.640]   Phil Libbon is also here.
[00:22:56.640 --> 00:22:58.160]   He's at all turtles.
[00:22:58.160 --> 00:23:00.240]   That is an incubator for artificial...
[00:23:00.240 --> 00:23:00.880]   No.
[00:23:00.880 --> 00:23:02.080]   I get wrong every time.
[00:23:02.080 --> 00:23:02.960]   Studio.
[00:23:02.960 --> 00:23:03.600]   Studio.
[00:23:03.600 --> 00:23:05.440]   Studio for artificial intelligence.
[00:23:05.440 --> 00:23:06.320]   What is a studio?
[00:23:06.320 --> 00:23:07.920]   We make stuff.
[00:23:07.920 --> 00:23:08.960]   Oh, yeah.
[00:23:08.960 --> 00:23:09.280]   Okay.
[00:23:09.280 --> 00:23:12.560]   So you're not funding companies and supporting them to make stuff.
[00:23:12.560 --> 00:23:13.520]   You're making stuff.
[00:23:13.520 --> 00:23:16.480]   Well, we do everything to make important products,
[00:23:16.480 --> 00:23:18.400]   which includes funding when that's necessary,
[00:23:18.400 --> 00:23:20.880]   but also engineering and design and marketing and everything.
[00:23:20.880 --> 00:23:24.320]   And we're not trying to incubate a show to poop out little companies.
[00:23:24.320 --> 00:23:26.000]   We try to make important products.
[00:23:26.000 --> 00:23:28.960]   We're kind of indifferent as to whether they're freestanding companies or not.
[00:23:28.960 --> 00:23:31.440]   Phil Libbon, not pooping out companies.
[00:23:31.440 --> 00:23:31.600]   No.
[00:23:31.600 --> 00:23:31.840]   Okay.
[00:23:31.840 --> 00:23:33.120]   We just want to make that clear.
[00:23:33.120 --> 00:23:33.600]   Yeah.
[00:23:33.600 --> 00:23:35.120]   We can't look at little companies.
[00:23:35.120 --> 00:23:36.080]   It's like 50 characters.
[00:23:36.080 --> 00:23:36.480]   It's like 50 characters.
[00:23:36.480 --> 00:23:37.280]   I think that's a good thing.
[00:23:37.280 --> 00:23:39.040]   That's your new Twitter handle.
[00:23:39.040 --> 00:23:44.240]   But interestingly enough, a member of Anthony Levin Dasky's new
[00:23:44.240 --> 00:23:46.000]   Church of the Artificial Intelligence.
[00:23:46.000 --> 00:23:46.480]   Yeah, no.
[00:23:46.480 --> 00:23:46.800]   No.
[00:23:46.800 --> 00:23:47.200]   Okay.
[00:23:47.200 --> 00:23:48.400]   Well, we'll talk about that in a minute.
[00:23:48.400 --> 00:23:48.800]   No.
[00:23:48.800 --> 00:23:49.280]   Two.
[00:23:49.280 --> 00:23:52.480]   Also here from the register, the King of Snark himself.
[00:23:52.480 --> 00:23:55.760]   I need that my business cup.
[00:23:55.760 --> 00:23:57.760]   It was probably snarkier people at the register.
[00:23:57.760 --> 00:23:58.720]   Sub business cards?
[00:23:58.720 --> 00:23:59.280]   Oh, yes.
[00:23:59.280 --> 00:23:59.600]   No, no.
[00:23:59.600 --> 00:24:01.440]   I'm quite mild in some regards.
[00:24:01.440 --> 00:24:03.280]   Sometimes.
[00:24:03.280 --> 00:24:05.600]   Our show today brought to you by Kaptera.
[00:24:05.600 --> 00:24:10.080]   And I love Kaptera because it solves a problem I have, which is people call me,
[00:24:10.080 --> 00:24:15.280]   mostly on the radio show saying, I don't know, as an example, I run a dog groomer.
[00:24:15.280 --> 00:24:18.080]   What's the best software for running dog groomers?
[00:24:18.080 --> 00:24:20.560]   And I have to say, why would I know that?
[00:24:20.560 --> 00:24:22.320]   Now I could say go to Kaptera.com.
[00:24:22.320 --> 00:24:23.040]   What have I been doing it?
[00:24:23.040 --> 00:24:24.800]   I guess in the past is googling it.
[00:24:24.800 --> 00:24:28.880]   And that's not a great way to find a specific vertical business software.
[00:24:28.880 --> 00:24:37.360]   Kaptera is 400 categories of business software, thousands of applications.
[00:24:37.360 --> 00:24:40.640]   In fact, one of the fun things, even though you're not looking for business software,
[00:24:40.640 --> 00:24:46.960]   is just going to Kaptera and looking in, you know, I mean, from bookkeeper software to bug tracking
[00:24:46.960 --> 00:24:52.000]   software to call recording software to chemical, child care, chiropractic.
[00:24:52.000 --> 00:24:54.880]   Let's pick a weird business.
[00:24:54.880 --> 00:24:58.400]   I shouldn't say that because then somebody dance studio software.
[00:24:58.400 --> 00:24:59.680]   That's not a weird business.
[00:24:59.680 --> 00:25:02.160]   But do you know, look at all of these.
[00:25:02.160 --> 00:25:10.560]   These are all programs built to run dance studios, class juggler, your virtuoso,
[00:25:10.560 --> 00:25:17.680]   bookio, Sawyer, Compu dance, dance studio pro, iClass pro, prime time, dance studio software.
[00:25:17.680 --> 00:25:20.000]   So first of all, you're going to get a directory of that.
[00:25:20.000 --> 00:25:23.280]   By the way, these directories tend to be kind of big because they have everything in there.
[00:25:23.280 --> 00:25:25.120]   Now you can say, well, I'm going to filter that.
[00:25:25.120 --> 00:25:26.480]   I'm going to get four stars.
[00:25:26.480 --> 00:25:29.760]   It should be web based, only four stars and up.
[00:25:29.760 --> 00:25:33.760]   And I want to take online payments, point of sale and recital management.
[00:25:33.760 --> 00:25:34.080]   I don't know.
[00:25:34.080 --> 00:25:35.040]   I just picked some.
[00:25:35.040 --> 00:25:38.800]   You can filter it down to the programs that Jack Rabbit danced, the studio director,
[00:25:38.800 --> 00:25:38.960]   Pike.
[00:25:38.960 --> 00:25:40.240]   These are the ones that do.
[00:25:40.240 --> 00:25:42.480]   Then you can say, let's add that to the compare.
[00:25:42.480 --> 00:25:43.760]   That looks good dance biz.
[00:25:43.760 --> 00:25:44.480]   That looks good.
[00:25:44.480 --> 00:25:45.840]   Our dance shop.
[00:25:45.840 --> 00:25:46.480]   I'm going to add that.
[00:25:46.480 --> 00:25:48.080]   And now I can compare them.
[00:25:49.200 --> 00:25:52.880]   And look at what they do and even better.
[00:25:52.880 --> 00:25:55.440]   I can look at the reviews.
[00:25:55.440 --> 00:25:59.120]   Hundreds of thousands of reviews by people just like you.
[00:25:59.120 --> 00:26:02.000]   And all of this, guess what?
[00:26:02.000 --> 00:26:02.320]   Free.
[00:26:02.320 --> 00:26:03.200]   There's no sign up.
[00:26:03.200 --> 00:26:04.000]   No.
[00:26:04.000 --> 00:26:06.080]   You don't have to give them your email.
[00:26:06.080 --> 00:26:09.760]   It's just a directory of great business software just for you.
[00:26:09.760 --> 00:26:12.240]   260,000 ratings and reviews.
[00:26:12.240 --> 00:26:13.200]   Hundreds.
[00:26:13.200 --> 00:26:16.080]   I mean, thousands of applications, hundreds of categories.
[00:26:16.080 --> 00:26:17.440]   Capetera.
[00:26:18.800 --> 00:26:21.840]   C-A-P-T-E-R-R-A.
[00:26:21.840 --> 00:26:23.840]   Capetera.com/twit.
[00:26:23.840 --> 00:26:26.400]   Don't waste your precious daylight as the days get shorter,
[00:26:26.400 --> 00:26:30.800]   sifting through a sea of Google results when you can find the right business software
[00:26:30.800 --> 00:26:32.800]   and get home one time tonight.
[00:26:32.800 --> 00:26:35.920]   Capetera.com/twit.
[00:26:35.920 --> 00:26:39.520]   Get that software you need and get it for free.
[00:26:39.520 --> 00:26:41.920]   Well, the software's not free, but get the fun.
[00:26:41.920 --> 00:26:45.600]   Actually, in many cases, you can even say, I want to free trials.
[00:26:45.600 --> 00:26:47.280]   I want to see which ones have free trials.
[00:26:47.280 --> 00:26:49.600]   C-A-P-T-E-R-R-A.com.
[00:26:49.600 --> 00:26:52.240]   3 million people, including some of the biggest companies in the world,
[00:26:52.240 --> 00:26:54.880]   use Capetera to find business software.
[00:26:54.880 --> 00:26:56.080]   Maybe you should too.
[00:26:56.080 --> 00:26:56.800]   We thank him so much.
[00:26:56.800 --> 00:26:57.920]   They're supportive.
[00:26:57.920 --> 00:27:01.680]   This week in tech, Anthony Lewandowski is an interesting character.
[00:27:01.680 --> 00:27:02.720]   I think we should talk about this.
[00:27:02.720 --> 00:27:07.920]   He is an interesting character that covers a multitude of sins.
[00:27:07.920 --> 00:27:10.640]   And he's getting more interesting.
[00:27:10.640 --> 00:27:13.440]   He became a kind of a news story.
[00:27:13.440 --> 00:27:17.040]   He was at Google, worked at the self-driving car group there.
[00:27:17.840 --> 00:27:22.480]   Then left Waymo to start Auto, which is the self-driving truck company,
[00:27:22.480 --> 00:27:23.760]   which was acquired by Uber.
[00:27:23.760 --> 00:27:28.640]   And then Google sued Uber, saying Anthony brought intellectual property.
[00:27:28.640 --> 00:27:30.160]   That lawsuit is ongoing.
[00:27:30.160 --> 00:27:33.920]   So we're not here to talk about that,
[00:27:33.920 --> 00:27:35.920]   because there's nothing more boring than lawsuits, frankly.
[00:27:35.920 --> 00:27:41.360]   But he's also done something I find strangely fascinating.
[00:27:41.360 --> 00:27:45.120]   He's founded a church of artificial intelligence.
[00:27:45.120 --> 00:27:47.040]   It's called The Way of the Future.
[00:27:47.040 --> 00:27:50.640]   And actually, there's a lot of people wondering,
[00:27:50.640 --> 00:27:51.760]   is this real?
[00:27:51.760 --> 00:27:52.640]   Is this a joke?
[00:27:52.640 --> 00:27:54.080]   What's going on?
[00:27:54.080 --> 00:27:54.960]   Why aren't interviewed?
[00:27:54.960 --> 00:27:57.520]   And they said he was quite serious.
[00:27:57.520 --> 00:28:00.080]   Although if you look at him with his Nike,
[00:28:00.080 --> 00:28:03.360]   Nike, a turtleneck there.
[00:28:03.360 --> 00:28:05.520]   Oh, there's a tracksuit.
[00:28:05.520 --> 00:28:06.240]   That's a tracksuit.
[00:28:06.240 --> 00:28:07.200]   That's a tracksuit, I would know.
[00:28:07.200 --> 00:28:10.080]   And we know about Nike tracksuits.
[00:28:10.080 --> 00:28:11.760]   They're often used by religious cults.
[00:28:11.760 --> 00:28:14.400]   I was just thinking the Heavens Gate people.
[00:28:14.400 --> 00:28:14.960]   Yeah.
[00:28:14.960 --> 00:28:18.800]   So do you think there's tongue in cheek here or what?
[00:28:18.800 --> 00:28:20.160]   He calls The Way of the Future,
[00:28:20.160 --> 00:28:25.920]   they call it the next act for the Silicon Valley Robotics Wunderkind.
[00:28:25.920 --> 00:28:30.400]   He filed papers in May with the IRS.
[00:28:30.400 --> 00:28:34.000]   He is the leader of the religion, self-styled dean,
[00:28:34.000 --> 00:28:37.120]   just like the dean of Canterbury Cathedral, right?
[00:28:37.120 --> 00:28:41.120]   Yes, the dean at Canterbury Cathedral probably has slightly more
[00:28:41.120 --> 00:28:42.800]   than The Way of Followers than this.
[00:28:42.800 --> 00:28:44.320]   He's not wearing Nike tracksuits.
[00:28:44.320 --> 00:28:45.760]   A lot of popularity contests.
[00:28:45.760 --> 00:28:49.600]   Generally, religions attend to the thought of that.
[00:28:49.600 --> 00:28:51.680]   Well, yeah, but that's interesting.
[00:28:51.680 --> 00:28:52.320]   I don't know.
[00:28:52.320 --> 00:28:53.280]   It's a non-profit.
[00:28:53.280 --> 00:28:55.840]   It's also not clear on what the word "deen" means.
[00:28:55.840 --> 00:28:59.120]   Well, the boss, the head hard show.
[00:28:59.120 --> 00:29:00.640]   But also, he just don't wear the rules about which.
[00:29:00.640 --> 00:29:02.080]   The rules aren't actually the boss of anything.
[00:29:02.080 --> 00:29:03.040]   Well, what is the dean?
[00:29:03.040 --> 00:29:04.480]   They call the canop...
[00:29:04.480 --> 00:29:05.520]   No, the dean's not the boss.
[00:29:05.520 --> 00:29:07.040]   The dean is the guy who gets the bishop.
[00:29:07.040 --> 00:29:08.960]   No, the dean is the person that gets stuff done.
[00:29:09.520 --> 00:29:11.440]   The other guy is the guy that wears the stupid hat.
[00:29:11.440 --> 00:29:13.760]   In no context, does the word "deen" mean the person
[00:29:13.760 --> 00:29:15.280]   like in charge of the whole argument?
[00:29:15.280 --> 00:29:16.000]   No, it's this number two.
[00:29:16.000 --> 00:29:16.960]   Well, who's in charge?
[00:29:16.960 --> 00:29:16.960]   Who's in charge?
[00:29:16.960 --> 00:29:17.920]   Who's in charge?
[00:29:17.920 --> 00:29:22.000]   Some giant computer somewhere, isn't it?
[00:29:22.000 --> 00:29:23.200]   Is that a wargames reference?
[00:29:23.200 --> 00:29:23.440]   Yes.
[00:29:23.440 --> 00:29:24.720]   Very good.
[00:29:24.720 --> 00:29:27.520]   For somebody not born here.
[00:29:27.520 --> 00:29:28.640]   The documents...
[00:29:28.640 --> 00:29:29.680]   No, I'm kidding.
[00:29:29.680 --> 00:29:30.160]   How?
[00:29:30.160 --> 00:29:30.720]   That's true.
[00:29:30.720 --> 00:29:31.200]   Immigrants.
[00:29:31.200 --> 00:29:32.000]   Funny because it's true.
[00:29:32.000 --> 00:29:32.080]   This is true.
[00:29:32.080 --> 00:29:32.560]   This is true.
[00:29:32.560 --> 00:29:33.360]   This is true.
[00:29:33.360 --> 00:29:35.200]   I watched World Games when it came out in the cinema.
[00:29:35.200 --> 00:29:35.760]   I love that.
[00:29:35.760 --> 00:29:36.800]   Yeah, great movie.
[00:29:36.800 --> 00:29:41.600]   I interviewed the director some years ago on the old screen savers.
[00:29:41.600 --> 00:29:43.200]   And it was really fun to talk to him.
[00:29:43.200 --> 00:29:44.800]   We still have the World Games poster.
[00:29:44.800 --> 00:29:46.480]   John's bottom is his name.
[00:29:46.480 --> 00:29:50.480]   WOTF, the way of the future.
[00:29:50.480 --> 00:29:52.000]   That's the name of this religion.
[00:29:52.000 --> 00:29:55.360]   We'll focus on "the realization, acceptance, and worship
[00:29:55.360 --> 00:29:59.440]   of a Godhead based on artificial intelligence
[00:29:59.440 --> 00:30:01.520]   developed through computer hardware and software."
[00:30:01.520 --> 00:30:02.480]   They're going to fund research.
[00:30:02.480 --> 00:30:03.760]   God, need to do a spaceship.
[00:30:05.840 --> 00:30:09.200]   It will seek to build working relationships with AI industry leaders.
[00:30:09.200 --> 00:30:10.320]   Have they contacted you?
[00:30:10.320 --> 00:30:12.080]   You're an AI industry leader, Phil.
[00:30:12.080 --> 00:30:14.080]   They haven't, apparently.
[00:30:14.080 --> 00:30:15.520]   I'm not a leader enough.
[00:30:15.520 --> 00:30:19.360]   And they're going to target AI professionals and lay persons
[00:30:19.360 --> 00:30:23.200]   who are interested in the worship of a Godhead based on AI.
[00:30:23.200 --> 00:30:23.840]   This could be...
[00:30:23.840 --> 00:30:26.240]   If there's nothing else I learned from college,
[00:30:26.240 --> 00:30:29.840]   it's to not take people who say the word Godhead seriously.
[00:30:29.840 --> 00:30:31.920]   That is a red flag.
[00:30:31.920 --> 00:30:33.280]   You see the word Godhead?
[00:30:33.280 --> 00:30:34.480]   You know, that's not a serious person.
[00:30:34.480 --> 00:30:37.360]   It sounds a little bit like the Church of the Subgenius, right?
[00:30:37.360 --> 00:30:42.800]   JR Bob Dobbs and Slack, which of course was a mock religion
[00:30:42.800 --> 00:30:43.600]   that took off.
[00:30:43.600 --> 00:30:45.920]   And gave us Slack.
[00:30:45.920 --> 00:30:46.800]   And gave us Slack.
[00:30:46.800 --> 00:30:49.760]   Do you think Slack, the program is named after Slack
[00:30:49.760 --> 00:30:51.040]   from Church of Subgenius?
[00:30:51.040 --> 00:30:51.440]   It's got to be.
[00:30:51.440 --> 00:30:54.000]   Wow, I didn't think of that.
[00:30:54.000 --> 00:30:57.920]   So is Anthony Lewandowski creating a new JR Bob Dobbs?
[00:30:57.920 --> 00:30:58.880]   And he's the dean.
[00:30:58.880 --> 00:31:01.200]   Is he serious?
[00:31:01.200 --> 00:31:01.920]   Will he...
[00:31:01.920 --> 00:31:03.840]   He's got this thing he says in the article where he's like,
[00:31:03.840 --> 00:31:05.680]   "Well, clearly, AI's are going to take over.
[00:31:05.680 --> 00:31:07.760]   So I want to be in on the ground floor as..."
[00:31:07.760 --> 00:31:08.480]   And being nice.
[00:31:08.480 --> 00:31:10.720]   Yes, I want to be the collaborator.
[00:31:10.720 --> 00:31:11.520]   That's why he's the dean.
[00:31:11.520 --> 00:31:13.200]   I'm not lined up against the wall when they...
[00:31:13.200 --> 00:31:16.400]   Yeah, he doesn't want the AI's to think he's trying to take over.
[00:31:16.400 --> 00:31:17.760]   Yeah, that makes sense.
[00:31:17.760 --> 00:31:19.200]   And that's why he's just the dean.
[00:31:19.200 --> 00:31:20.000]   He's a facilitator.
[00:31:20.000 --> 00:31:20.560]   Got it.
[00:31:20.560 --> 00:31:22.720]   This is a dude who watched Avengers Age of Ultron.
[00:31:22.720 --> 00:31:25.040]   I was like, "That old Tron, I feel like he's got a business level."
[00:31:25.040 --> 00:31:25.600]   Yeah.
[00:31:25.600 --> 00:31:27.760]   So I'm going to back him.
[00:31:27.760 --> 00:31:29.040]   It's weird.
[00:31:29.040 --> 00:31:32.960]   When you read the manifesto, it comes across as almost plausible
[00:31:32.960 --> 00:31:36.560]   in a way that this is coming and we should be looking wolf for ways
[00:31:36.560 --> 00:31:38.160]   to actually do this more safely.
[00:31:38.160 --> 00:31:42.320]   But I'm sorry, with the timing of this and the way it's all done,
[00:31:42.320 --> 00:31:44.560]   I can't help thinking he's pulling a helmet.
[00:31:44.560 --> 00:31:47.840]   And this is going to be the first ontologist of the technology movement.
[00:31:47.840 --> 00:31:48.560]   Or it's a joke.
[00:31:48.560 --> 00:31:49.920]   It has a lot of free time.
[00:31:49.920 --> 00:31:50.480]   Or...
[00:31:50.480 --> 00:31:51.280]   Well, yeah.
[00:31:51.280 --> 00:31:53.200]   Yeah, it's three possibilities.
[00:31:53.200 --> 00:31:57.200]   But the timing of this in relation to the call case is interesting.
[00:31:57.200 --> 00:31:57.600]   It hasn't been...
[00:31:57.600 --> 00:31:58.640]   That thing's going to drag on.
[00:31:58.640 --> 00:32:00.320]   He's got to do something in the meantime, right?
[00:32:00.320 --> 00:32:02.560]   He's obviously not hireable at this time.
[00:32:02.560 --> 00:32:06.160]   He says, "The idea needs to spread before the technology.
[00:32:06.160 --> 00:32:08.800]   The church is how we spread the word, the gospel."
[00:32:08.800 --> 00:32:16.160]   If you believe it, then start a conversation with someone else
[00:32:16.160 --> 00:32:19.600]   and help them understand the same things.
[00:32:19.600 --> 00:32:21.760]   Now, this you agreed with when I read this, Phil.
[00:32:21.760 --> 00:32:23.600]   He says, "The change is coming.
[00:32:23.600 --> 00:32:25.920]   A change will transform every aspect of human existence,
[00:32:25.920 --> 00:32:28.880]   disrupting employment, leisure, religion, the economy,
[00:32:28.880 --> 00:32:32.880]   and possibly decide our very survival as a species."
[00:32:32.880 --> 00:32:36.080]   Quote, "If you ask people whether a computer can be smarter than a human,
[00:32:36.080 --> 00:32:41.120]   99.9% will say science fiction actually, it's inevitable,
[00:32:41.120 --> 00:32:42.560]   it's guaranteed to happen."
[00:32:42.560 --> 00:32:43.200]   Is that true?
[00:32:43.200 --> 00:32:45.280]   99.9% will say science fiction.
[00:32:45.280 --> 00:32:47.280]   That seems like an awfully precise number.
[00:32:47.280 --> 00:32:49.200]   It also seems...
[00:32:49.200 --> 00:32:51.760]   Science fiction will a computer ever be as smart as a human?
[00:32:51.760 --> 00:32:53.040]   Oh, undoubtedly.
[00:32:53.040 --> 00:32:53.520]   Undoubtedly.
[00:32:53.520 --> 00:32:54.720]   Well, obviously.
[00:32:54.720 --> 00:32:55.440]   Obviously.
[00:32:58.240 --> 00:32:59.280]   I'm clearly in the...
[00:32:59.280 --> 00:33:01.760]   I want to figure out how are you defining smarter?
[00:33:01.760 --> 00:33:05.680]   Are you defining it as the ability to synthesize and take a degree
[00:33:05.680 --> 00:33:07.680]   in creativity and adaptability?
[00:33:07.680 --> 00:33:07.920]   Yes.
[00:33:07.920 --> 00:33:09.520]   Will there be anything?
[00:33:09.520 --> 00:33:12.720]   Will there be any single thing that humans can be better than computers?
[00:33:12.720 --> 00:33:13.120]   Tennis.
[00:33:13.120 --> 00:33:13.680]   No.
[00:33:13.680 --> 00:33:14.160]   No.
[00:33:14.160 --> 00:33:14.640]   But I guess...
[00:33:14.640 --> 00:33:15.760]   Not a single thing.
[00:33:15.760 --> 00:33:17.120]   Are they going to teach themselves that?
[00:33:17.120 --> 00:33:19.120]   Or are we going to have to teach them to think like us?
[00:33:19.120 --> 00:33:21.040]   Or will they find a way to eventually process
[00:33:21.040 --> 00:33:23.200]   so that it's so much for that we can't comprehend it?
[00:33:23.200 --> 00:33:25.440]   Look at AlphaGo as a really good template for this.
[00:33:25.440 --> 00:33:27.680]   AlphaGo is the Go playing computer that Google developed.
[00:33:28.240 --> 00:33:31.600]   When we talk about Deep Blue, Deep Blue was taught to play chess.
[00:33:31.600 --> 00:33:35.280]   Not just given the rules, but given ways to...
[00:33:35.280 --> 00:33:36.720]   And then all chess programs work like this,
[00:33:36.720 --> 00:33:40.880]   way to weigh positions and say which was winning and go for the better position.
[00:33:40.880 --> 00:33:43.440]   AlphaGo, they said, "Here's the rules of Go.
[00:33:43.440 --> 00:33:44.080]   Just play it."
[00:33:44.080 --> 00:33:47.120]   It played millions of games and taught itself.
[00:33:47.120 --> 00:33:47.680]   Yeah.
[00:33:47.680 --> 00:33:49.760]   And in fact, interestingly, I think,
[00:33:49.760 --> 00:33:52.800]   came up with a way to play that isn't...
[00:33:52.800 --> 00:33:56.400]   That master Go players say, "You know, this is beautiful.
[00:33:56.400 --> 00:33:57.360]   This is artistic.
[00:33:57.360 --> 00:33:57.920]   This..."
[00:33:57.920 --> 00:34:01.440]   When you see a machine play chess, most chess players would agree.
[00:34:01.440 --> 00:34:03.760]   You could tell it's a machine.
[00:34:03.760 --> 00:34:05.360]   It's not human.
[00:34:05.360 --> 00:34:06.400]   It's mechanical.
[00:34:06.400 --> 00:34:08.240]   It doesn't...
[00:34:08.240 --> 00:34:09.840]   It's not art.
[00:34:09.840 --> 00:34:13.680]   But apparently, and I'm not a good enough Go player to judge this,
[00:34:13.680 --> 00:34:16.880]   but apparently AlphaGo plays in an artistic fashion
[00:34:16.880 --> 00:34:19.840]   that is even innovative, that master Go players,
[00:34:19.840 --> 00:34:24.560]   like the world champion that it beat last year or earlier this year, I guess,
[00:34:25.280 --> 00:34:29.360]   say, "This is giving us new ideas for how to play Go."
[00:34:29.360 --> 00:34:30.480]   So there's your answer.
[00:34:30.480 --> 00:34:34.640]   You can have machines that learn, at least in some constrained environments.
[00:34:34.640 --> 00:34:35.520]   I mean, Luna has this...
[00:34:35.520 --> 00:34:36.160]   So are you in the...
[00:34:36.160 --> 00:34:38.480]   So I'm going to put you on the spot, Lisa.
[00:34:38.480 --> 00:34:41.280]   Ian says, "Yes."
[00:34:41.280 --> 00:34:42.640]   But not as soon as people think.
[00:34:42.640 --> 00:34:46.640]   But eventually, Phil says, "Yes, probably agrees sooner or later."
[00:34:46.640 --> 00:34:50.080]   That's two out of two.
[00:34:50.080 --> 00:34:52.480]   Teach him how to think, and then the answer's probably, "Yes."
[00:34:52.480 --> 00:34:53.680]   Yes, and I say yes.
[00:34:53.680 --> 00:34:54.320]   So it's 100%.
[00:34:55.280 --> 00:34:59.280]   100% of people don't believe it's science fiction, but think it's inevitable.
[00:34:59.280 --> 00:34:59.760]   I mean, look...
[00:34:59.760 --> 00:35:01.520]   So of course, is the opposite of what we said.
[00:35:01.520 --> 00:35:01.920]   Yeah.
[00:35:01.920 --> 00:35:02.480]   Yeah.
[00:35:02.480 --> 00:35:04.320]   How about in the studio audience?
[00:35:04.320 --> 00:35:04.960]   Anybody...
[00:35:04.960 --> 00:35:06.000]   Let's put it this way.
[00:35:06.000 --> 00:35:08.160]   Anybody doesn't think that...
[00:35:08.160 --> 00:35:08.480]   Let's...
[00:35:08.480 --> 00:35:10.800]   Can we give it a time for him the next couple of hundred years?
[00:35:10.800 --> 00:35:11.040]   Yeah.
[00:35:11.040 --> 00:35:14.480]   Give it a long time, that the machine can't be developed
[00:35:14.480 --> 00:35:16.240]   to be smarter than a human.
[00:35:16.240 --> 00:35:17.520]   Anybody think that won't happen?
[00:35:17.520 --> 00:35:19.040]   Zero hands.
[00:35:19.040 --> 00:35:19.440]   Okay.
[00:35:19.440 --> 00:35:22.560]   Will it be smarter than an octopus might actually be the real question?
[00:35:22.560 --> 00:35:23.200]   Octopus.
[00:35:23.200 --> 00:35:24.160]   Those things are coming.
[00:35:24.160 --> 00:35:25.680]   But it is smart.
[00:35:25.680 --> 00:35:28.080]   Yeah, I mean, look at how mollists learn and think in a depth, especially with...
[00:35:28.080 --> 00:35:29.200]   I know, I saw a finding Nemo.
[00:35:29.200 --> 00:35:30.640]   I'm really smart.
[00:35:30.640 --> 00:35:31.600]   I'm really smart.
[00:35:31.600 --> 00:35:34.800]   Listen, Paul, Lewandowski is not wrong in that there's a link between
[00:35:34.800 --> 00:35:37.600]   religious development and communications technologies.
[00:35:37.600 --> 00:35:38.480]   And you're talking...
[00:35:38.480 --> 00:35:40.080]   Wait a minute, wait a minute.
[00:35:40.080 --> 00:35:41.600]   You better break that one down.
[00:35:41.600 --> 00:35:42.640]   There's a link?
[00:35:42.640 --> 00:35:44.560]   No, absolutely.
[00:35:44.560 --> 00:35:49.680]   Yeah, the conception of divinity tends to change as communication technologies change,
[00:35:49.680 --> 00:35:51.040]   and people move.
[00:35:51.040 --> 00:35:55.680]   I mean, you take a look at the types of mythologies and gods that oral cultures define,
[00:35:55.680 --> 00:36:01.680]   and then you take a look at how the concept of divinity shifts once there's written language,
[00:36:01.680 --> 00:36:05.280]   and then you take a look at how there were cultural shifts in religion again,
[00:36:05.280 --> 00:36:09.440]   once Gutenberg hit the scene with printing presses and movable type.
[00:36:09.440 --> 00:36:15.120]   There's a sociological relationship between the way religions develop
[00:36:15.120 --> 00:36:16.640]   and the way we conceive of God.
[00:36:16.640 --> 00:36:19.440]   Because of our changing communications.
[00:36:19.440 --> 00:36:22.880]   Because what you do is you go from having, say, oral transmission of information,
[00:36:22.880 --> 00:36:26.400]   which is very specific and very different and very time dependent
[00:36:26.400 --> 00:36:30.560]   to something where it's written down and the information is therefore freed from time,
[00:36:30.560 --> 00:36:34.400]   and that actually expands the definition of divinity for people too,
[00:36:34.400 --> 00:36:38.560]   because now their divine entity is uncoupled from time.
[00:36:38.560 --> 00:36:47.200]   And then when you get to printing presses and ubiquity of communications technologies,
[00:36:47.200 --> 00:36:49.840]   the definition of God expands what's one step ahead of that.
[00:36:49.840 --> 00:36:52.480]   What is the internet done to our definition of God?
[00:36:52.480 --> 00:36:56.160]   It's probably turned it more into it's moved towards AI to be frank,
[00:36:56.160 --> 00:37:01.200]   and it started in the 90s because when the idea of having disintermediate communication
[00:37:01.200 --> 00:37:05.280]   that was instantaneous in-globe spanning, that transcended God-like.
[00:37:05.280 --> 00:37:09.200]   Will that transcend the physical limitations of the print?
[00:37:09.200 --> 00:37:12.640]   The physical world.
[00:37:12.640 --> 00:37:15.360]   I don't think it's a coincidental.
[00:37:15.360 --> 00:37:21.520]   I think it's reflective that you saw some ripples through different flavors of Christianity
[00:37:21.520 --> 00:37:23.360]   that arose in response to this.
[00:37:23.360 --> 00:37:28.080]   I'm speaking as someone who's sort of baptised and confirmed as a Protestant in the Church of England.
[00:37:28.080 --> 00:37:31.040]   Gutenberg was fundamental to that because up until this point,
[00:37:31.040 --> 00:37:34.720]   there were very few Bibles they were written in Latin.
[00:37:34.720 --> 00:37:37.760]   It was actively illegal to do an English language version of the Bible
[00:37:37.760 --> 00:37:39.520]   because people didn't want them reading and then going.
[00:37:39.520 --> 00:37:43.360]   This is so full of, but when Gutenberg came along...
[00:37:43.360 --> 00:37:45.760]   It was kind of coincident with Luther.
[00:37:45.760 --> 00:37:47.040]   Yeah. Martin Luther.
[00:37:47.040 --> 00:37:52.240]   Well, Luther, 50 years later, the 95 theses were on the Church.
[00:37:52.240 --> 00:37:54.240]   But they spread around a lot faster thanks to the printing press.
[00:37:54.240 --> 00:37:56.720]   And you're seeing the effect of the internet on religions now.
[00:37:56.720 --> 00:38:01.360]   The Mormons, the LDS, has a huge problem with the internet.
[00:38:01.360 --> 00:38:05.120]   They are losing followers hand over fist because people are actually reading
[00:38:05.120 --> 00:38:08.000]   what the history of the Church and saying,
[00:38:08.000 --> 00:38:11.360]   "Well, look, you're all lovely people, but this is just bonkers.
[00:38:11.360 --> 00:38:14.480]   You know, you were all going to have a planet to ourselves when we die."
[00:38:14.480 --> 00:38:17.280]   And, you know, Joseph Smith got these gold books.
[00:38:17.280 --> 00:38:23.920]   I think that actually the Broadway playbook of Mormon has done more than the internet to break down the wealth.
[00:38:23.920 --> 00:38:25.920]   And I don't want to diss the Mormon religion.
[00:38:25.920 --> 00:38:28.880]   I know many Mormons who are devout and actually good people.
[00:38:28.880 --> 00:38:30.480]   Oh, no. I mean, this is the...
[00:38:30.480 --> 00:38:32.800]   The doctrine means they're all marvelous people,
[00:38:32.800 --> 00:38:34.960]   but they just believe something which is utterly bonkers.
[00:38:34.960 --> 00:38:36.720]   Well, to you, I think this is the...
[00:38:36.720 --> 00:38:38.560]   Do you believe in the Trinity?
[00:38:38.560 --> 00:38:39.040]   Nope.
[00:38:39.040 --> 00:38:39.840]   Okay, good.
[00:38:39.840 --> 00:38:41.840]   No, give me some proof.
[00:38:41.840 --> 00:38:44.080]   No, I'm open to the idea.
[00:38:44.080 --> 00:38:47.920]   I have a religion who has some interesting suppositions.
[00:38:47.920 --> 00:38:50.960]   I'm open to the idea of a god, but the idea that there is one book
[00:38:50.960 --> 00:38:53.120]   which contains the perfect explanation of what that is.
[00:38:53.120 --> 00:38:56.880]   Let me continue on what Lisa is saying because it's kind of interesting.
[00:38:56.880 --> 00:38:57.760]   Do you think...
[00:38:57.760 --> 00:39:00.400]   So religion obviously fills a human need of some sort
[00:39:00.400 --> 00:39:03.760]   because as long as they've been humans, we've had these beliefs.
[00:39:03.760 --> 00:39:06.240]   So there is some need for this, whether it's...
[00:39:06.240 --> 00:39:08.960]   You want explanations for things that make no sense.
[00:39:08.960 --> 00:39:11.200]   You want to believe that you're not alone in the universe.
[00:39:11.200 --> 00:39:15.280]   And you want a supportive community that you know that you belong to.
[00:39:15.280 --> 00:39:18.160]   And there are all sorts of reciprocal give and take relationships.
[00:39:18.160 --> 00:39:19.840]   That doesn't evolutionary advantage in it.
[00:39:19.840 --> 00:39:20.480]   Yeah.
[00:39:20.480 --> 00:39:27.600]   So, and I will accept your contention that the means of communication
[00:39:27.600 --> 00:39:29.600]   says change to how we hold religion.
[00:39:29.600 --> 00:39:30.880]   I think that's very interesting.
[00:39:30.880 --> 00:39:33.360]   It changes how we conceive of the divine, too.
[00:39:33.360 --> 00:39:34.240]   Very interesting.
[00:39:34.240 --> 00:39:35.840]   Does it make sense that the internet...
[00:39:35.840 --> 00:39:38.320]   That we'll look back in 100 years and say the internet era...
[00:39:39.200 --> 00:39:42.880]   Created a new form of divinity, artificial intelligence.
[00:39:42.880 --> 00:39:43.280]   Is that...
[00:39:43.280 --> 00:39:44.400]   I don't think that's unreasonable.
[00:39:44.400 --> 00:39:45.680]   Well, it created a new conception of divinity,
[00:39:45.680 --> 00:39:48.240]   which is that it's a divinity that can process information
[00:39:48.240 --> 00:39:51.200]   and come up with things that are beyond our comprehension.
[00:39:51.200 --> 00:39:54.000]   You wouldn't say that AI created the world.
[00:39:54.000 --> 00:39:56.720]   In most religions, it's not what creation is.
[00:39:56.720 --> 00:39:57.280]   It understands it.
[00:39:57.280 --> 00:39:59.520]   What's the creation myth in the AI religion?
[00:39:59.520 --> 00:40:03.600]   Here's the problem with this stupid church.
[00:40:06.240 --> 00:40:06.800]   I like it.
[00:40:06.800 --> 00:40:07.520]   Let's call it like this.
[00:40:07.520 --> 00:40:08.080]   It's ugly.
[00:40:08.080 --> 00:40:11.760]   It's because it leads to this kind of discussion.
[00:40:11.760 --> 00:40:14.000]   It combines...
[00:40:14.000 --> 00:40:19.760]   Whether or not AI is relevant or interesting for religion is...
[00:40:19.760 --> 00:40:21.120]   That's a topic that we can talk about.
[00:40:21.120 --> 00:40:25.760]   What religion is not is useful and interesting for AI.
[00:40:25.760 --> 00:40:27.680]   Like, this is setting AI back.
[00:40:27.680 --> 00:40:30.480]   What the industry doesn't need is another crackpot
[00:40:32.560 --> 00:40:36.240]   using scary sounding words to try to alienate normal people
[00:40:36.240 --> 00:40:38.400]   and make the scene like something other.
[00:40:38.400 --> 00:40:39.120]   But there are some...
[00:40:39.120 --> 00:40:40.400]   I agree with you.
[00:40:40.400 --> 00:40:41.680]   Using the word God and so forth.
[00:40:41.680 --> 00:40:44.560]   Or church, or dean, or pretending that this is...
[00:40:44.560 --> 00:40:49.440]   I think people need what society needs in order to
[00:40:49.440 --> 00:40:52.000]   make peace with AI have a develop the way that it should develop,
[00:40:52.000 --> 00:40:54.720]   that we want it to develop, is actually to demystify it,
[00:40:54.720 --> 00:40:58.640]   is to stop treating it like some magical, mysterious sci-fi thing.
[00:40:58.640 --> 00:41:00.800]   It's actually completely understandable
[00:41:00.800 --> 00:41:02.800]   and it's completely understandable by normal people.
[00:41:02.800 --> 00:41:04.560]   And casting it...
[00:41:04.560 --> 00:41:05.040]   Yes.
[00:41:05.040 --> 00:41:07.280]   Because this just numbers is just math.
[00:41:07.280 --> 00:41:09.760]   There's nothing like magical or mystical about it.
[00:41:09.760 --> 00:41:10.720]   It feels mystical though.
[00:41:10.720 --> 00:41:11.120]   There's...
[00:41:11.120 --> 00:41:13.120]   It feels like there's an inflection point, for instance,
[00:41:13.120 --> 00:41:17.040]   an alpha go, where it goes from just being a mechanistic system,
[00:41:17.040 --> 00:41:20.480]   a touring machine or a von Neumann machine
[00:41:20.480 --> 00:41:24.080]   that is doing deterministic calculations to something...
[00:41:24.080 --> 00:41:26.800]   There is a point at which it has played enough games
[00:41:26.800 --> 00:41:27.840]   that it feels like...
[00:41:28.480 --> 00:41:31.360]   Maybe this is very smooshy human feelings,
[00:41:31.360 --> 00:41:34.320]   but it feels like it's crossed into a new realm.
[00:41:34.320 --> 00:41:39.280]   It's no longer one of the biggest dangers in society
[00:41:39.280 --> 00:41:43.360]   over the next few decades is tech fear, science fear.
[00:41:43.360 --> 00:41:44.160]   Yes, absolutely.
[00:41:44.160 --> 00:41:48.320]   Being afraid of technology and science and panicking about it.
[00:41:48.320 --> 00:41:52.400]   And that has the potential to do vast damage to billions of people.
[00:41:52.400 --> 00:41:54.000]   We already sit and tap on this.
[00:41:54.000 --> 00:41:56.800]   This bull church makes that problem worse.
[00:41:56.800 --> 00:41:57.520]   I agree with you.
[00:41:57.520 --> 00:42:00.880]   Because it's somebody coming up and using mystical sounding language
[00:42:00.880 --> 00:42:02.560]   and talking about the Godhead and AI.
[00:42:02.560 --> 00:42:04.080]   It's just like hand waving.
[00:42:04.080 --> 00:42:05.120]   One of the big human...
[00:42:05.120 --> 00:42:06.640]   It's meant to be not understandable.
[00:42:06.640 --> 00:42:08.800]   It's like this language is meant to make people say,
[00:42:08.800 --> 00:42:09.760]   "Oh, we want to worship this.
[00:42:09.760 --> 00:42:11.360]   We don't need to understand it.
[00:42:11.360 --> 00:42:12.240]   We can't control it.
[00:42:12.240 --> 00:42:13.680]   It's outside of human comprehension."
[00:42:13.680 --> 00:42:15.920]   That's actually a critical facet of most religions, isn't it?
[00:42:15.920 --> 00:42:19.040]   Is there a behind the curtain that you aren't expected to understand?
[00:42:19.040 --> 00:42:19.520]   Right.
[00:42:19.520 --> 00:42:23.280]   It's much more important that people feel that they actually do understand
[00:42:23.280 --> 00:42:26.480]   and do control and can influence the evolution of behavior of this thing.
[00:42:26.480 --> 00:42:27.520]   I think this is setting it back.
[00:42:27.520 --> 00:42:29.360]   Now, I think this is not a serious person.
[00:42:29.360 --> 00:42:31.520]   It's the effort is not going to be taken seriously.
[00:42:31.520 --> 00:42:32.000]   Okay.
[00:42:32.000 --> 00:42:33.040]   Good.
[00:42:33.040 --> 00:42:35.760]   He's trying to position people as pets.
[00:42:35.760 --> 00:42:37.600]   You've demolished this.
[00:42:37.600 --> 00:42:38.160]   Well, it's over.
[00:42:38.160 --> 00:42:39.360]   Well, it's also...
[00:42:39.360 --> 00:42:41.440]   I think there's actually points to an attitude problem
[00:42:41.440 --> 00:42:42.880]   that's pervasive through Silicon Valley,
[00:42:42.880 --> 00:42:44.960]   which is the idea that people who work in tech
[00:42:44.960 --> 00:42:47.040]   are some sort of anointed or priestly cast.
[00:42:47.040 --> 00:42:47.520]   Yeah.
[00:42:47.520 --> 00:42:48.560]   And everybody else else.
[00:42:48.560 --> 00:42:50.080]   Or that there are annoying douchebags.
[00:42:50.080 --> 00:42:51.120]   Which many of us are.
[00:42:51.120 --> 00:42:52.640]   It's male, im potato, potatoe.
[00:42:52.640 --> 00:42:52.960]   Yeah.
[00:42:52.960 --> 00:42:53.440]   And...
[00:42:53.440 --> 00:42:54.480]   [LAUGHTER]
[00:42:54.480 --> 00:42:56.000]   But what he's doing here is he's trying to...
[00:42:56.000 --> 00:43:00.080]   He's trying to make the argument that either you can be part of the winning technology priestly cast
[00:43:00.080 --> 00:43:01.840]   or you can be a pet or livestock.
[00:43:01.840 --> 00:43:02.640]   That's even Hawking.
[00:43:02.640 --> 00:43:05.200]   Stephen Hawking says the development of full artificial intelligence
[00:43:05.200 --> 00:43:07.200]   could spell the end of the human race.
[00:43:07.200 --> 00:43:08.000]   True or false?
[00:43:08.000 --> 00:43:08.960]   False flag operations.
[00:43:08.960 --> 00:43:10.800]   Stephen Hawking's been fully AI for decades.
[00:43:10.800 --> 00:43:12.640]   [LAUGHTER]
[00:43:12.640 --> 00:43:14.560]   He's just trying to make us believe that that's not the case.
[00:43:14.560 --> 00:43:17.680]   No attention to the man behind the pointing stick.
[00:43:17.680 --> 00:43:19.440]   Oh, that's interesting.
[00:43:19.440 --> 00:43:19.920]   How about...
[00:43:19.920 --> 00:43:21.440]   But Elon says the same thing.
[00:43:21.440 --> 00:43:22.960]   There are a number of people who say,
[00:43:22.960 --> 00:43:24.800]   "We were worried about AI."
[00:43:24.800 --> 00:43:25.920]   Oh, there's...
[00:43:25.920 --> 00:43:26.400]   Oh, sorry.
[00:43:26.400 --> 00:43:27.280]   I'll tell you, please.
[00:43:27.280 --> 00:43:27.840]   I'm just saying...
[00:43:27.840 --> 00:43:32.480]   I mean, I know Elon Musk reads the in banks because he names it spaceships after it.
[00:43:32.480 --> 00:43:37.760]   And the in-m banks culture series is the best possible way for how AI would work out.
[00:43:37.760 --> 00:43:41.600]   Benevolent gods keeping things running, making sure of human things.
[00:43:41.600 --> 00:43:43.440]   I don't think it's going to particularly work out like that.
[00:43:43.440 --> 00:43:46.160]   I think it's going to work out in a whole multiplicity of ways
[00:43:46.160 --> 00:43:48.880]   because this is why this church is so stupid.
[00:43:48.880 --> 00:43:50.560]   The Chinese are working on AI.
[00:43:50.560 --> 00:43:53.280]   Individual companies within China and the US are working on AI.
[00:43:53.280 --> 00:43:55.760]   All around the world, people are working on AI systems.
[00:43:55.760 --> 00:44:00.960]   We're going to get a multiplicity of them and we don't know how it's going to work out.
[00:44:00.960 --> 00:44:03.200]   If people keep a firm grip on it, then fine.
[00:44:03.200 --> 00:44:05.760]   But there's going to be a lot of mistakes all the way.
[00:44:05.760 --> 00:44:09.280]   Last month, Vladimir Putin said that the nation that leads in AI will,
[00:44:09.280 --> 00:44:12.960]   quote, "be the ruler of the world," to which Elon responded.
[00:44:12.960 --> 00:44:14.080]   It begins.
[00:44:14.080 --> 00:44:15.120]   China, Russia, soon.
[00:44:15.120 --> 00:44:17.520]   All countries with strong computer science.
[00:44:17.520 --> 00:44:21.600]   Competition for AI superiority at a national level will most likely cause
[00:44:22.560 --> 00:44:23.520]   World War III.
[00:44:23.520 --> 00:44:27.920]   Is that techno panic?
[00:44:27.920 --> 00:44:31.040]   I think that's probably the definition of techno panic.
[00:44:31.040 --> 00:44:33.920]   I think people misinterpret a lot of what Elon says.
[00:44:33.920 --> 00:44:37.760]   I don't think that Elon is saying this stuff to be particularly alarming.
[00:44:37.760 --> 00:44:38.640]   It won't be a war.
[00:44:38.640 --> 00:44:42.000]   Like World War II was, it will be a competition though.
[00:44:42.000 --> 00:44:45.280]   We're seeing it already between these big nations.
[00:44:45.280 --> 00:44:49.680]   Yeah, I kind of think that national boundaries are not going to be as relevant in the future
[00:44:49.680 --> 00:44:50.560]   as they were in the past.
[00:44:50.560 --> 00:44:51.920]   That's one thing that happens with AI.
[00:44:51.920 --> 00:44:56.080]   The question is, what kind of cultural biases will be, will cause
[00:44:56.080 --> 00:44:57.280]   AI to differentiate?
[00:44:57.280 --> 00:45:00.880]   Because you're talking about companies in the countries that have very distinct-
[00:45:00.880 --> 00:45:03.440]   Act differently than an American AI or a Russian AI.
[00:45:03.440 --> 00:45:05.920]   Yeah, because that's what I'd be interested in seeing.
[00:45:05.920 --> 00:45:09.440]   Will they moderate their behavior when they run into each other?
[00:45:09.440 --> 00:45:10.480]   Will they learn from each other?
[00:45:10.480 --> 00:45:13.840]   Will they actually come up with their own set of biases that we can't anticipate?
[00:45:13.840 --> 00:45:15.520]   It's when it's just going to happen, Phil.
[00:45:15.520 --> 00:45:18.080]   I just want to know if I'll be around to see this.
[00:45:18.080 --> 00:45:19.120]   Well, it's happening.
[00:45:19.120 --> 00:45:24.480]   And it looks a lot less mysterious than I think people want to dramatize it.
[00:45:24.480 --> 00:45:25.840]   Because you work in it.
[00:45:25.840 --> 00:45:26.880]   I work in it.
[00:45:26.880 --> 00:45:30.640]   You are less mystical about it and less perturbed about it.
[00:45:30.640 --> 00:45:32.240]   We do practically AI.
[00:45:32.240 --> 00:45:33.440]   So lots of products.
[00:45:33.440 --> 00:45:35.200]   And it's not general AI probably, right?
[00:45:35.200 --> 00:45:37.360]   I don't think there's any such thing as general AI.
[00:45:37.360 --> 00:45:38.880]   There's no general intelligence.
[00:45:38.880 --> 00:45:41.040]   Yeah, I think everything we do is-
[00:45:41.040 --> 00:45:42.240]   It's domain specific.
[00:45:42.240 --> 00:45:44.240]   Well, but it's meant to be better than people.
[00:45:44.240 --> 00:45:44.480]   Right.
[00:45:44.480 --> 00:45:48.960]   Everything I work on starts out being vastly better than a human could be.
[00:45:48.960 --> 00:45:50.240]   At its narrow job.
[00:45:50.240 --> 00:45:50.720]   Right.
[00:45:50.720 --> 00:45:56.080]   And then it doesn't even pretend to be intelligent in any other aspect.
[00:45:56.080 --> 00:45:57.920]   Is this company nuts?
[00:45:57.920 --> 00:46:00.640]   Boston Dynamics.
[00:46:00.640 --> 00:46:01.600]   Google used to own them.
[00:46:01.600 --> 00:46:02.880]   They sold them off.
[00:46:02.880 --> 00:46:04.160]   They now have a new dog.
[00:46:04.160 --> 00:46:05.360]   It's called the Spot Mini.
[00:46:05.360 --> 00:46:06.000]   I just want you to-
[00:46:06.000 --> 00:46:06.800]   That is awesome.
[00:46:06.800 --> 00:46:07.920]   Really?
[00:46:07.920 --> 00:46:08.480]   Yeah.
[00:46:08.480 --> 00:46:09.280]   Here it comes.
[00:46:09.280 --> 00:46:10.480]   You seem like kicking it anymore.
[00:46:10.480 --> 00:46:12.640]   Yeah, that was kind of obsessive.
[00:46:12.640 --> 00:46:13.680]   Oh, don't look at me.
[00:46:13.680 --> 00:46:15.120]   Don't look at me like that.
[00:46:15.120 --> 00:46:16.560]   Okay.
[00:46:16.560 --> 00:46:17.440]   Now there I am.
[00:46:17.440 --> 00:46:18.800]   Anthropomorphizing it.
[00:46:18.800 --> 00:46:19.520]   It's not really-
[00:46:19.520 --> 00:46:22.400]   In fact, aren't they kind of anthropomorphizing it?
[00:46:22.400 --> 00:46:24.720]   Or at least animalizing it by-
[00:46:24.720 --> 00:46:26.800]   Now a magic children can have pets.
[00:46:26.800 --> 00:46:28.560]   Yeah, but you see, they've mini-trized that.
[00:46:28.560 --> 00:46:29.520]   That's not-
[00:46:29.520 --> 00:46:30.880]   They've had something similar to Spotify.
[00:46:30.880 --> 00:46:31.680]   From years now.
[00:46:31.680 --> 00:46:32.800]   And what they've basically done-
[00:46:32.800 --> 00:46:33.280]   You can have it for-
[00:46:33.280 --> 00:46:34.960]   I mean, look, you can actually-
[00:46:34.960 --> 00:46:38.320]   Actually, Sony, believe it or not, is bringing back the IBO.
[00:46:38.320 --> 00:46:39.840]   Oh, I had the first gen IBO.
[00:46:39.840 --> 00:46:40.480]   Did you?
[00:46:40.480 --> 00:46:40.880]   What I did-
[00:46:40.880 --> 00:46:41.760]   Why is that not as a price?
[00:46:41.760 --> 00:46:43.040]   Sony bringing back the IBO.
[00:46:43.040 --> 00:46:43.760]   Like you did-
[00:46:43.760 --> 00:46:45.840]   50 monthly charge and you can't jailbreak it.
[00:46:45.840 --> 00:46:46.640]   I didn't have-
[00:46:46.640 --> 00:46:47.200]   What's the point?
[00:46:47.200 --> 00:46:48.400]   It's a monthly charge.
[00:46:48.400 --> 00:46:52.240]   It was released in Japan a couple of weeks ago.
[00:46:52.240 --> 00:46:53.120]   I love IBO.
[00:46:53.120 --> 00:46:56.480]   His OLED eyes that allow for nuanced expressions,
[00:46:56.480 --> 00:46:57.360]   fisheye cameras,
[00:46:57.360 --> 00:46:59.040]   see and recognize individual faces.
[00:46:59.040 --> 00:47:05.280]   It's got actuators that allow the body to move smoothly along 22 axes.
[00:47:05.280 --> 00:47:05.920]   More than I got.
[00:47:05.920 --> 00:47:08.160]   Yeah, I have maybe five.
[00:47:08.160 --> 00:47:08.800]   So I can-
[00:47:08.800 --> 00:47:09.360]   So I can-
[00:47:09.360 --> 00:47:11.040]   Until I can mount a laser on his head,
[00:47:11.040 --> 00:47:11.760]   I'm not interested.
[00:47:11.760 --> 00:47:12.320]   Yeah.
[00:47:12.320 --> 00:47:13.200]   Here is-
[00:47:13.200 --> 00:47:14.400]   How many charcoal laser is that?
[00:47:14.400 --> 00:47:16.240]   I got you a video of IBO.
[00:47:16.240 --> 00:47:18.400]   You both know that you can put a laser on a normal dog.
[00:47:18.400 --> 00:47:19.680]   You don't need an AI dog for that.
[00:47:19.680 --> 00:47:20.800]   Yeah, I know, but-
[00:47:20.800 --> 00:47:22.880]   Sharks would be the best in lasers.
[00:47:22.880 --> 00:47:23.280]   Yeah.
[00:47:23.280 --> 00:47:23.600]   Yeah.
[00:47:23.600 --> 00:47:25.280]   You're discussing that shark's a freaking way.
[00:47:25.280 --> 00:47:26.480]   So like an ill-tempered bastard.
[00:47:26.480 --> 00:47:27.760]   IBO does have its ball again.
[00:47:27.760 --> 00:47:29.120]   Remember that was one of the features of IBO.
[00:47:29.120 --> 00:47:30.720]   Did you ever have the ball and have it play fetch?
[00:47:30.720 --> 00:47:31.200]   Of course you do.
[00:47:31.200 --> 00:47:35.280]   Really did you buy it as a curiosity or as a toy?
[00:47:35.280 --> 00:47:35.840]   Or what did-
[00:47:35.840 --> 00:47:37.200]   What were you thinking?
[00:47:37.200 --> 00:47:38.640]   Because it was thousands of dollars.
[00:47:38.640 --> 00:47:39.440]   Yes, thousands of dollars.
[00:47:39.440 --> 00:47:41.840]   I signed a contract with a universe
[00:47:41.840 --> 00:47:45.120]   that required me to buy every consumer robot I made.
[00:47:45.120 --> 00:47:47.040]   I actually, I still have that contract.
[00:47:47.040 --> 00:47:47.680]   Yeah.
[00:47:47.680 --> 00:47:49.520]   I signed a blood so I can't stop.
[00:47:49.520 --> 00:47:50.400]   Can't get out of it.
[00:47:50.400 --> 00:47:54.240]   I'm getting the Gibo, which is the robot that sits on a table
[00:47:54.240 --> 00:47:56.160]   and looks around like this someday.
[00:47:56.160 --> 00:47:56.640]   Yeah.
[00:47:56.640 --> 00:47:57.920]   That's from Cynthia Brazil though,
[00:47:57.920 --> 00:48:01.840]   who was a really legitimate robotics researcher from MIT.
[00:48:01.840 --> 00:48:03.200]   Sitting at a table and looking around
[00:48:03.200 --> 00:48:04.560]   is pretty much my signature move.
[00:48:04.560 --> 00:48:06.640]   It has all that eyes.
[00:48:06.640 --> 00:48:08.320]   It has actually a screen face.
[00:48:08.320 --> 00:48:09.840]   Take pictures and talk with you.
[00:48:09.840 --> 00:48:12.800]   And then we've actually had an advertiser called Curry,
[00:48:12.800 --> 00:48:14.320]   which is coming out in the next few months.
[00:48:14.320 --> 00:48:15.520]   I'm looking forward to getting our Curry.
[00:48:15.520 --> 00:48:16.480]   That one wanders around.
[00:48:16.480 --> 00:48:17.920]   It's like an I-bow without legs.
[00:48:17.920 --> 00:48:18.880]   It just rolls.
[00:48:18.880 --> 00:48:19.920]   This stuff is fascinating.
[00:48:19.920 --> 00:48:20.800]   Like it really-
[00:48:20.800 --> 00:48:23.600]   But it isn't artificial intelligence in any respect.
[00:48:23.600 --> 00:48:25.520]   Well, nothing is artificial intelligence
[00:48:25.520 --> 00:48:26.560]   and kind of everything is.
[00:48:26.560 --> 00:48:29.200]   Like once when you get into the technology,
[00:48:29.200 --> 00:48:32.400]   there isn't a specific bucket of things that's like,
[00:48:32.400 --> 00:48:33.120]   this is AI.
[00:48:33.120 --> 00:48:35.840]   It's a we use the word in a design context.
[00:48:35.840 --> 00:48:38.880]   What products feel intelligent?
[00:48:38.880 --> 00:48:40.880]   What does it take to make something feel smart?
[00:48:40.880 --> 00:48:41.600]   Yeah.
[00:48:41.600 --> 00:48:42.480]   And what does it feel?
[00:48:42.480 --> 00:48:43.680]   What does it take to make a product
[00:48:43.680 --> 00:48:45.840]   to design a product that feels like it's getting smarter?
[00:48:45.840 --> 00:48:46.480]   That it's learning.
[00:48:46.480 --> 00:48:47.760]   It's a design essence.
[00:48:47.760 --> 00:48:50.080]   I don't care so much about the back end technology,
[00:48:50.080 --> 00:48:52.000]   the algorithms, those will change.
[00:48:52.000 --> 00:48:53.760]   It's the design of intelligence.
[00:48:53.760 --> 00:48:55.120]   That's a super interesting thing for me.
[00:48:55.120 --> 00:48:57.920]   Well, if not back ends, how about back flips?
[00:48:57.920 --> 00:49:00.400]   This is another Boston robotics computer.
[00:49:00.400 --> 00:49:01.360]   Oh, I love this video.
[00:49:01.360 --> 00:49:02.240]   That's great.
[00:49:02.240 --> 00:49:03.200]   That can jump.
[00:49:03.200 --> 00:49:03.600]   Yeah.
[00:49:03.600 --> 00:49:05.120]   This is a humanoid computer.
[00:49:05.120 --> 00:49:05.760]   Yeah.
[00:49:05.760 --> 00:49:07.280]   It's jumping onto boxes.
[00:49:07.280 --> 00:49:10.960]   And in a moment, it's going to flip and do a back flip off a box here.
[00:49:10.960 --> 00:49:11.360]   Let's see.
[00:49:12.880 --> 00:49:17.360]   Oh, and it's better than I certainly couldn't do any of this.
[00:49:17.360 --> 00:49:18.720]   Whoa.
[00:49:18.720 --> 00:49:22.720]   It did a perfect plant and then it raises its arms.
[00:49:22.720 --> 00:49:22.960]   Yeah.
[00:49:22.960 --> 00:49:24.080]   It's a lovely bit of PR.
[00:49:24.080 --> 00:49:25.520]   It's stuck in landing.
[00:49:25.520 --> 00:49:25.760]   Yeah.
[00:49:25.760 --> 00:49:29.440]   It kind of glosses over some major failings in the Atlas design
[00:49:29.440 --> 00:49:30.560]   that they've got there.
[00:49:30.560 --> 00:49:32.400]   If you ever watch this thing, try and walk across rubble.
[00:49:32.400 --> 00:49:34.400]   It's hilariously bad.
[00:49:34.400 --> 00:49:34.720]   Yeah.
[00:49:34.720 --> 00:49:36.480]   It's funny, but that's a hard thing to do.
[00:49:36.480 --> 00:49:39.040]   I think so stuntmen still have their jobs for another 20 years.
[00:49:39.040 --> 00:49:42.080]   Mine was filled, said, which is that it won't be general intelligence.
[00:49:42.080 --> 00:49:44.400]   It'll be domain specific and very narrow.
[00:49:44.400 --> 00:49:47.120]   Well, that's definitely a good product.
[00:49:47.120 --> 00:49:51.920]   Is that designed to do something in a way that doesn't have to think about all your users?
[00:49:51.920 --> 00:49:52.480]   It's scary.
[00:49:52.480 --> 00:49:53.920]   There is a design change that's coming.
[00:49:53.920 --> 00:49:57.600]   And I actually think the most important thing in the medium term is the
[00:49:57.600 --> 00:49:59.840]   is the iPhone 10 and the face recognition stuff.
[00:49:59.840 --> 00:50:00.160]   Whoa.
[00:50:00.160 --> 00:50:00.560]   Okay.
[00:50:00.560 --> 00:50:02.480]   The iPhone 10.
[00:50:02.480 --> 00:50:02.720]   Yeah.
[00:50:02.720 --> 00:50:06.000]   I used to say X until someone said I was an idiot.
[00:50:06.000 --> 00:50:07.360]   It's definitely the 10.
[00:50:07.360 --> 00:50:09.760]   Apple will be Apple open that can of worms.
[00:50:09.760 --> 00:50:10.400]   It's an X.
[00:50:10.400 --> 00:50:11.200]   Finatively, yeah.
[00:50:11.200 --> 00:50:12.960]   Is it Roman numeral 10, I guess?
[00:50:12.960 --> 00:50:15.920]   Had they wanted that to be an ambiguous, there's a way they could have made it an ambiguous.
[00:50:15.920 --> 00:50:16.880]   One and a zero.
[00:50:16.880 --> 00:50:18.400]   Yeah.
[00:50:18.400 --> 00:50:24.960]   So what's going to start happening up to now all technology, it's always been the human's job to
[00:50:24.960 --> 00:50:25.760]   understand the product.
[00:50:25.760 --> 00:50:31.040]   And the flip that's happening now is it's going to be the technology's job to understand
[00:50:31.040 --> 00:50:37.040]   people and the looking at your face and seeing what your expressions are and when you're confused
[00:50:37.040 --> 00:50:39.760]   and when you're happy and when you're frustrated, like that's a big part of it.
[00:50:40.640 --> 00:50:45.520]   What we're seeing now is we're the beginning of a decade of a total redesign of every single
[00:50:45.520 --> 00:50:50.480]   product in the world because they're all going to go towards understanding users instead of the
[00:50:50.480 --> 00:50:50.960]   other way around.
[00:50:50.960 --> 00:50:52.320]   Isn't that what we always had hoped for?
[00:50:52.320 --> 00:50:53.280]   That was always the goal.
[00:50:53.280 --> 00:50:57.440]   And I remember really talking about this in the early days of computing, which is that you
[00:50:57.440 --> 00:51:00.960]   wanted a computer program to adapt to you, not vice versa.
[00:51:00.960 --> 00:51:07.680]   And yet every computer, every bit of software we've had until very recently, you had to adapt
[00:51:07.680 --> 00:51:08.080]   your--
[00:51:08.960 --> 00:51:10.480]   It isn't even adapt.
[00:51:10.480 --> 00:51:11.200]   It's really C.
[00:51:11.200 --> 00:51:19.200]   When you're talking to a human or even when you're interacting with a dog, like a pet dog,
[00:51:19.200 --> 00:51:25.520]   what dogs are really good at, the single evolutionary trick of dogs is that they look at human faces
[00:51:25.520 --> 00:51:29.920]   and they understand human emotion better than humans do, better than anything in the world,
[00:51:29.920 --> 00:51:30.880]   and only for humans.
[00:51:30.880 --> 00:51:33.760]   They don't understand dog emotions by looking at other dogs, but they're seeing humans.
[00:51:33.760 --> 00:51:35.280]   Yeah, this is what dogs do.
[00:51:35.280 --> 00:51:36.160]   That's their stick.
[00:51:36.160 --> 00:51:36.960]   That's why they're dogs.
[00:51:36.960 --> 00:51:37.920]   That's what we bred them for.
[00:51:38.640 --> 00:51:42.000]   And do they really understand human emotions, which it just looks like?
[00:51:42.000 --> 00:51:44.080]   So that's a really interesting question, right?
[00:51:44.080 --> 00:51:47.920]   Because at some level, they understand human emotion much better than a human does,
[00:51:47.920 --> 00:51:50.400]   but at a different level, it's obviously-- they couldn't explain it to you.
[00:51:50.400 --> 00:51:51.680]   They don't know what's going on.
[00:51:51.680 --> 00:51:52.000]   Well, they do know.
[00:51:52.000 --> 00:51:56.240]   Well, it helps if they can smell the smelling helps too, because they can actually detect
[00:51:56.240 --> 00:51:58.080]   hormonal shifts or everything.
[00:51:58.080 --> 00:51:58.160]   It's all in.
[00:51:58.160 --> 00:51:58.720]   Yeah.
[00:51:58.720 --> 00:52:02.480]   It's a very sensing machine's design for a certain--
[00:52:02.480 --> 00:52:04.160]   But this is what we're going to see with technology.
[00:52:04.160 --> 00:52:07.520]   Up to now, basically no product has ever seen the user.
[00:52:08.480 --> 00:52:10.240]   The product has never seen you.
[00:52:10.240 --> 00:52:11.680]   It has never tried to understand you.
[00:52:11.680 --> 00:52:15.920]   It has never actually tried to engage in the way that a person or even a dog would.
[00:52:15.920 --> 00:52:18.160]   And now we're just the beginning of that.
[00:52:18.160 --> 00:52:21.520]   And 10 years from now, most mainstream products are going to be--
[00:52:21.520 --> 00:52:27.840]   are going to have this kind of conversational and emotional intelligence about how they're
[00:52:27.840 --> 00:52:28.560]   interacting with people.
[00:52:28.560 --> 00:52:29.840]   And that's profound.
[00:52:29.840 --> 00:52:35.440]   This is full employment for every UX designer, every AI person, every cognitive scientist for the next few decades.
[00:52:35.440 --> 00:52:36.560]   They're called Lighthouse Effect.
[00:52:36.560 --> 00:52:40.400]   There's one right behind me that has a time of flight 3D sensor.
[00:52:40.400 --> 00:52:42.400]   It has a camera.
[00:52:42.400 --> 00:52:44.000]   It has infrared.
[00:52:44.000 --> 00:52:48.640]   But what it does is interesting because you can make natural, learned language queries of it.
[00:52:48.640 --> 00:52:50.880]   It can distinguish pets and humans.
[00:52:50.880 --> 00:52:54.480]   So you can have queries like, "Show me when a pet comes in without a human."
[00:52:54.480 --> 00:52:57.120]   Or, "Show me when a pet and a human come in.
[00:52:57.120 --> 00:53:00.240]   Show me when a human comes in after two in the afternoon."
[00:53:00.240 --> 00:53:01.840]   And it's very accurate.
[00:53:01.840 --> 00:53:03.280]   That's what you're talking about.
[00:53:03.280 --> 00:53:04.080]   More than that, right?
[00:53:04.080 --> 00:53:04.720]   Just anytime.
[00:53:04.720 --> 00:53:06.320]   Show me when an unhappy human comes in.
[00:53:06.320 --> 00:53:08.960]   Well, it would be nice, for example, if Siri could pick up your tone of voice.
[00:53:08.960 --> 00:53:11.280]   Oh, God, I dread that day.
[00:53:11.280 --> 00:53:14.160]   That's probably not that hard, though, right?
[00:53:14.160 --> 00:53:18.080]   Well, if theory they should be able to pick up your tone of voice.
[00:53:18.080 --> 00:53:20.480]   We see emotional analysis of tweets, right?
[00:53:20.480 --> 00:53:28.880]   I think it's really real time user experiences that adopt in real time based on your--
[00:53:28.880 --> 00:53:31.040]   I see your micro-emotional feel.
[00:53:31.840 --> 00:53:34.720]   Perhaps you'd like to take a chill pill before we speak.
[00:53:34.720 --> 00:53:36.800]   Well, so potentially, right?
[00:53:36.800 --> 00:53:38.000]   But other things as well.
[00:53:38.000 --> 00:53:42.560]   So, like, you could have a room, for example, a conference room,
[00:53:42.560 --> 00:53:48.480]   that is aware of the emotional and cognitive content of the conversations that are being held
[00:53:48.480 --> 00:53:53.520]   in it and responds in real time to those conversations in a way to try to improve the quality.
[00:53:53.520 --> 00:53:56.960]   I'm going to turn the lights down now because it's getting a little heavy.
[00:53:56.960 --> 00:53:59.120]   I would design it with a slightly less creepy voice.
[00:54:00.960 --> 00:54:02.400]   Darn it, there goes that job.
[00:54:02.400 --> 00:54:04.000]   So, this is the goals for the room.
[00:54:04.000 --> 00:54:05.360]   Who says the goals for the room?
[00:54:05.360 --> 00:54:06.640]   And it decides on that, though.
[00:54:06.640 --> 00:54:07.360]   I mean, this is--
[00:54:07.360 --> 00:54:08.080]   It's a good question.
[00:54:08.080 --> 00:54:12.000]   So, initially, me, in the medium term, me,
[00:54:12.000 --> 00:54:18.640]   longer than that, it'll probably machine learn based on desired outcomes, right?
[00:54:18.640 --> 00:54:19.120]   So, for example--
[00:54:19.120 --> 00:54:21.600]   So, that's what you would at a higher level, you would set an outcome?
[00:54:21.600 --> 00:54:22.160]   Yeah, set an outcome.
[00:54:22.160 --> 00:54:24.160]   Instead of saying what you want the room to do, you'd say,
[00:54:24.160 --> 00:54:28.480]   "Look, I'd like everybody to be able to have a meeting where we get stuff done,
[00:54:28.480 --> 00:54:31.280]   and any conflict is reduced."
[00:54:31.280 --> 00:54:33.440]   And the machine says, "Well, it turns out--"
[00:54:33.440 --> 00:54:35.200]   Or a conflict is escalated, depending on--
[00:54:35.200 --> 00:54:37.600]   Maybe sometimes you want more conflict, depending on the outcome you want.
[00:54:37.600 --> 00:54:37.600]   Right.
[00:54:37.600 --> 00:54:41.520]   And the software can learn, "Well, when I make the lights a little redder--"
[00:54:41.520 --> 00:54:44.400]   Or a fine scenario for family holiday get-togethers.
[00:54:44.400 --> 00:54:48.320]   We're going to talk about family holiday get-togethers in just a second,
[00:54:48.320 --> 00:54:53.920]   because apparently, there are people who know about what you're doing this holiday season.
[00:54:53.920 --> 00:54:57.680]   If it's the research I'm thinking of, then we could have a humding of an argument.
[00:54:57.680 --> 00:54:58.240]   Oh!
[00:54:58.240 --> 00:54:59.280]   Good, I look forward to that.
[00:54:59.280 --> 00:55:01.520]   But first, a word from our sponsor.
[00:55:01.520 --> 00:55:02.800]   This is more than a sponsor.
[00:55:02.800 --> 00:55:05.600]   We could practically put at the end of every show,
[00:55:05.600 --> 00:55:08.240]   "Leo the Poets Wardrobe" by Scottie Vest.
[00:55:08.240 --> 00:55:09.680]   I'm a big fan of Scottie Vest.
[00:55:09.680 --> 00:55:12.000]   I travel-- it's my travel wear.
[00:55:12.000 --> 00:55:15.840]   Even down to my underwear, they make actually Scottie Vest underwear
[00:55:15.840 --> 00:55:17.760]   that has a pocket, because all Scottie Vest has a pocket.
[00:55:17.760 --> 00:55:18.720]   No, no, I have a set.
[00:55:18.720 --> 00:55:18.960]   Yeah.
[00:55:18.960 --> 00:55:19.680]   Yeah, it's great.
[00:55:19.680 --> 00:55:22.000]   It's-- you can wash it out in the sink and dry it.
[00:55:22.000 --> 00:55:26.320]   It's really-- anyway, Scottie Vest isn't really about underwear so much.
[00:55:26.320 --> 00:55:28.240]   It really started with vests, with jackets.
[00:55:28.240 --> 00:55:29.440]   Photographers love them.
[00:55:29.440 --> 00:55:32.160]   I'm sure Mike, you wear quite a few Scottie Vest,
[00:55:32.160 --> 00:55:35.520]   because they have-- my Scottie Vest vest has, I think, 27 pockets.
[00:55:35.520 --> 00:55:37.760]   A lot of police light them as well for exactly that reason.
[00:55:37.760 --> 00:55:39.440]   It's like a tactical jacket.
[00:55:39.440 --> 00:55:41.120]   And you can put stuff in them for travel.
[00:55:41.120 --> 00:55:42.160]   They make my favorite hat.
[00:55:42.160 --> 00:55:44.560]   I don't have a Scottie Vest hat.
[00:55:44.560 --> 00:55:46.720]   Yeah, they have a really nice baseball cap, which is like perfect.
[00:55:46.720 --> 00:55:47.920]   Oh, yeah, it's got a little pouch in it.
[00:55:47.920 --> 00:55:48.160]   Yeah.
[00:55:48.160 --> 00:55:49.120]   It's got like two pockets.
[00:55:49.120 --> 00:55:49.280]   Yeah.
[00:55:49.280 --> 00:55:50.400]   It's got like no logos.
[00:55:50.400 --> 00:55:50.960]   It's great.
[00:55:50.960 --> 00:55:51.760]   Oh, nice.
[00:55:51.760 --> 00:55:52.960]   Yeah, I don't like wearing people's logos.
[00:55:52.960 --> 00:55:53.520]   But look at this.
[00:55:53.520 --> 00:55:55.120]   Here's an image on the left there.
[00:55:55.120 --> 00:55:57.680]   You can see, actually, they have this great x-ray feature on the site.
[00:55:57.680 --> 00:56:01.600]   Scottie Vest, S-C-O-T-T-E-V-E-S-T.com/twit,
[00:56:01.600 --> 00:56:05.760]   where you can hover your mouse over the vest, the jacket, the shirt.
[00:56:05.760 --> 00:56:09.760]   They even have dresses for women, and see where the pockets are.
[00:56:09.760 --> 00:56:11.360]   I kind of want that the white--
[00:56:11.360 --> 00:56:12.880]   I want the design with the stuff printed.
[00:56:12.880 --> 00:56:13.680]   Like, I would wear that.
[00:56:13.680 --> 00:56:16.320]   Well, if it had those graphics.
[00:56:16.320 --> 00:56:17.840]   Yeah, so I actually have a look at that.
[00:56:17.840 --> 00:56:18.880]   Yeah, yeah, yeah.
[00:56:18.880 --> 00:56:20.320]   Yeah, I would wear that.
[00:56:20.320 --> 00:56:21.120]   That is the same.
[00:56:21.120 --> 00:56:21.760]   I want that.
[00:56:21.760 --> 00:56:22.400]   A diagram.
[00:56:22.400 --> 00:56:24.880]   I want that exact one with the graphics on.
[00:56:24.880 --> 00:56:25.760]   I love it.
[00:56:25.760 --> 00:56:27.360]   They call it Scott-- who's great.
[00:56:27.360 --> 00:56:29.600]   Scott Jordan calls it the personal area network.
[00:56:29.600 --> 00:56:31.840]   It has that you can run wires.
[00:56:31.840 --> 00:56:34.720]   You could, for instance, have an iPhone in your iPhone pocket.
[00:56:34.720 --> 00:56:36.240]   And then you see how the headphones come out of it,
[00:56:36.240 --> 00:56:39.040]   and then go through conduits in the jacket,
[00:56:39.040 --> 00:56:40.960]   so they appear at your ears.
[00:56:40.960 --> 00:56:43.840]   And so your wires never get tangled and never get lost.
[00:56:43.840 --> 00:56:46.400]   When you get to the airport and you want to go through security,
[00:56:46.400 --> 00:56:48.240]   you just take off the vest, put it on the belt.
[00:56:48.240 --> 00:56:51.200]   If you ever run out of room in your luggage,
[00:56:51.200 --> 00:56:53.040]   like they say, oh, sir, you're a kilogram over.
[00:56:53.920 --> 00:56:55.760]   Hey, boy, having 27 pockets.
[00:56:55.760 --> 00:56:58.800]   You just-- I stuff my lenses in there, my camera in there.
[00:56:58.800 --> 00:57:00.240]   Sure, you look like the Michelin man,
[00:57:00.240 --> 00:57:01.520]   but they let you through.
[00:57:01.520 --> 00:57:02.560]   They don't weigh you.
[00:57:02.560 --> 00:57:03.680]   And then yet.
[00:57:03.680 --> 00:57:07.680]   By the way, they have an RFID blocking pocket,
[00:57:07.680 --> 00:57:09.520]   which is very important if you're traveling.
[00:57:09.520 --> 00:57:10.800]   You could put your passport in there,
[00:57:10.800 --> 00:57:14.880]   and nobody can read your passport or skim your credit card
[00:57:14.880 --> 00:57:19.120]   and blocks the 13.56 megahertz frequencies.
[00:57:19.120 --> 00:57:20.400]   That's how specific it is.
[00:57:20.400 --> 00:57:22.960]   The weight management system means
[00:57:22.960 --> 00:57:26.080]   you can balance it all out, so you don't look like the Michelin man.
[00:57:26.080 --> 00:57:30.080]   Unless you put like 20 lenses in your pocket.
[00:57:30.080 --> 00:57:31.200]   It's got to be kind of--
[00:57:31.200 --> 00:57:33.120]   Look, I need to say the Michelin man looks good.
[00:57:33.120 --> 00:57:34.160]   I think he looks great.
[00:57:34.160 --> 00:57:35.520]   Nothing wrong with that.
[00:57:35.520 --> 00:57:36.800]   Nothing wrong with that.
[00:57:36.800 --> 00:57:37.680]   They've got jackets.
[00:57:37.680 --> 00:57:38.320]   They've got shirts.
[00:57:38.320 --> 00:57:39.040]   They've got pants.
[00:57:39.040 --> 00:57:39.840]   They've got dresses.
[00:57:39.840 --> 00:57:40.960]   They've got boxer shorts.
[00:57:40.960 --> 00:57:43.040]   I own and wear them all.
[00:57:43.040 --> 00:57:45.040]   You should see my closet.
[00:57:45.040 --> 00:57:46.080]   It's just all Scottie vests.
[00:57:46.080 --> 00:57:46.800]   Oh, I love this.
[00:57:46.800 --> 00:57:47.760]   This is the one I wore to France,
[00:57:47.760 --> 00:57:49.520]   the one with the red piping.
[00:57:49.520 --> 00:57:51.520]   I love that jacket.
[00:57:51.520 --> 00:57:52.720]   And it's kind of stylish.
[00:57:52.720 --> 00:57:55.040]   Lisa wears the Scottie vests.
[00:57:55.040 --> 00:57:55.440]   Fleeces.
[00:57:55.440 --> 00:57:56.960]   Actually, we have twit fleeces we all wear.
[00:57:56.960 --> 00:58:00.480]   The RFID Travel Vest, the fleece jacket 8.0,
[00:58:00.480 --> 00:58:01.920]   the Scottie vests jacket for women,
[00:58:01.920 --> 00:58:03.760]   which is really nice.
[00:58:03.760 --> 00:58:06.880]   It's heavily weighted fleece, so it keeps you warm 21 pockets.
[00:58:06.880 --> 00:58:09.680]   But if you have to cool off removable sleeves,
[00:58:09.680 --> 00:58:11.760]   the Quest Vest--
[00:58:11.760 --> 00:58:12.320]   Get this.
[00:58:12.320 --> 00:58:13.520]   Oh, I like that trench.
[00:58:13.520 --> 00:58:14.560]   Isn't that nice?
[00:58:14.560 --> 00:58:14.880]   I have the trench.
[00:58:14.880 --> 00:58:16.320]   I have the girlfriend who has that.
[00:58:16.320 --> 00:58:16.960]   Yeah, I do too.
[00:58:16.960 --> 00:58:17.760]   I love that.
[00:58:17.760 --> 00:58:18.400]   Yeah.
[00:58:18.400 --> 00:58:18.880]   40--
[00:58:18.880 --> 00:58:21.120]   The Quest Vest has 42 buckets.
[00:58:21.840 --> 00:58:24.560]   And it's teflon treated for water and stain resistant.
[00:58:24.560 --> 00:58:26.400]   Actually, there are so many pockets.
[00:58:26.400 --> 00:58:27.760]   I lose things in the pockets,
[00:58:27.760 --> 00:58:29.760]   and that's the other sponsored trackers very helpful.
[00:58:29.760 --> 00:58:32.880]   And literally,
[00:58:32.880 --> 00:58:34.080]   if you're doing any traveling,
[00:58:34.080 --> 00:58:35.840]   this is the best apparel you could pack.
[00:58:35.840 --> 00:58:37.920]   Johnny Jadar Travel Guy loves Scottie vests.
[00:58:37.920 --> 00:58:41.120]   He uses their vests to do things like hide money.
[00:58:41.120 --> 00:58:41.520]   Hide--
[00:58:41.520 --> 00:58:42.720]   You know, forget the money belt.
[00:58:42.720 --> 00:58:43.840]   You need a Scottie vest.
[00:58:43.840 --> 00:58:44.720]   Diamonds.
[00:58:44.720 --> 00:58:45.200]   Diamonds.
[00:58:45.200 --> 00:58:49.120]   Scottie vest has a great deal right now for twit fans,
[00:58:49.680 --> 00:58:52.560]   a private opportunity to their huge Cyber Monday sale.
[00:58:52.560 --> 00:58:55.280]   It's coming up a week from Monday,
[00:58:55.280 --> 00:58:55.760]   a week from tomorrow,
[00:58:55.760 --> 00:58:56.960]   but you can get in now.
[00:58:56.960 --> 00:58:59.120]   And for a limited time,
[00:58:59.120 --> 00:59:00.880]   save 30% on almost everything
[00:59:00.880 --> 00:59:03.280]   by going to scottie vest dot com slash twit
[00:59:03.280 --> 00:59:05.840]   and use the promo code cyber twit.
[00:59:05.840 --> 00:59:06.800]   It's not cyber Monday,
[00:59:06.800 --> 00:59:07.680]   it's cyber twit.
[00:59:07.680 --> 00:59:10.320]   And you'll get an extra 30% off.
[00:59:10.320 --> 00:59:13.520]   Scottie vest, great gifts for you
[00:59:13.520 --> 00:59:15.040]   and your loved ones.
[00:59:15.040 --> 00:59:16.640]   Two-year manufacturer guarantee,
[00:59:16.640 --> 00:59:19.200]   and 30% off if you use the promo code cyber twit.
[00:59:19.200 --> 00:59:22.240]   Scottie vest dot com slash twit.
[00:59:22.240 --> 00:59:24.400]   I think we're all Scottie vest users.
[00:59:24.400 --> 00:59:26.080]   That's the fun thing about an advertiser.
[00:59:26.080 --> 00:59:28.080]   I just don't have a pocket label for snacks though,
[00:59:28.080 --> 00:59:30.640]   and that's an unforgivable omission in my world.
[00:59:30.640 --> 00:59:32.160]   They have a water bottle pocket.
[00:59:32.160 --> 00:59:34.400]   You know, I like they have a key fob,
[00:59:34.400 --> 00:59:35.840]   which I always attach my key to.
[00:59:35.840 --> 00:59:37.040]   For you, stick your your your your
[00:59:37.040 --> 00:59:38.000]   soil it is.
[00:59:38.000 --> 00:59:38.960]   Yeah, it could be.
[00:59:38.960 --> 00:59:40.880]   Oh, I have carried soil it in my
[00:59:40.880 --> 00:59:42.880]   Yes, in my Scottie vest.
[00:59:42.880 --> 00:59:44.480]   I actually get it through customs.
[00:59:44.480 --> 00:59:46.240]   Soil it they don't they don't bother you.
[00:59:46.240 --> 00:59:47.040]   All right.
[00:59:47.040 --> 00:59:49.040]   I just say this is my magic food.
[00:59:49.040 --> 00:59:51.760]   No, I've never had problems with soil it.
[00:59:51.760 --> 00:59:53.280]   That's an interesting question.
[00:59:53.280 --> 00:59:55.520]   Trying to get mamaight through customs is a nightmare.
[00:59:55.520 --> 00:59:57.760]   I'm my smart for explosives last time.
[00:59:57.760 --> 00:59:59.040]   Really?
[00:59:59.040 --> 00:59:59.680]   Yeah.
[00:59:59.680 --> 01:00:01.680]   Now I came through LAX and a British genome
[01:00:01.680 --> 01:00:04.560]   admitted Adobe Max had brought me over a couple of jars.
[01:00:04.560 --> 01:00:06.400]   Why would smuggling mamaight be in?
[01:00:06.400 --> 01:00:07.840]   Well, mamaight actually use it explosive.
[01:00:07.840 --> 01:00:10.560]   Please, it's explosive on your taste.
[01:00:10.560 --> 01:00:11.920]   Oh, I know.
[01:00:11.920 --> 01:00:13.600]   Oh, I was going to say it's no.
[01:00:13.600 --> 01:00:14.960]   You guys are so weird.
[01:00:14.960 --> 01:00:17.120]   It's a byproduct of beer production.
[01:00:17.120 --> 01:00:18.160]   It's yeast.
[01:00:18.160 --> 01:00:19.600]   Makes me touch the best.
[01:00:19.600 --> 01:00:20.240]   And the bar makers in Britain said,
[01:00:20.240 --> 01:00:21.360]   what can we do with this?
[01:00:21.360 --> 01:00:23.200]   Oh, let's pretend it's good.
[01:00:23.200 --> 01:00:26.560]   Oh, Leo, all modern civilization is a byproduct of beer production.
[01:00:26.560 --> 01:00:27.600]   Yeah, there's that.
[01:00:27.600 --> 01:00:30.800]   It all started in Egypt many, many years ago.
[01:00:30.800 --> 01:00:32.320]   So we like the iPhone 10.
[01:00:32.320 --> 01:00:36.960]   Of course, the rumor and by the way, so does the public.
[01:00:36.960 --> 01:00:40.560]   It seems to be selling quite well, at least as best we can see.
[01:00:40.560 --> 01:00:42.400]   The iPhone 8 actually sold all right.
[01:00:42.400 --> 01:00:43.360]   The iPhone 8 plus.
[01:00:43.360 --> 01:00:45.040]   That's disappointing in early sales.
[01:00:45.040 --> 01:00:47.120]   Well, the 8, the little one didn't sell that well.
[01:00:47.120 --> 01:00:49.120]   The iPhone 8 plus sold OK.
[01:00:49.120 --> 01:00:51.440]   Surprisingly, well given that in a month,
[01:00:51.440 --> 01:00:53.280]   you would be able to get an iPhone 10.
[01:00:53.280 --> 01:00:55.200]   But this is of course the iPhone most people want.
[01:00:55.200 --> 01:00:57.520]   And one lesson Apple's learned is,
[01:00:57.520 --> 01:00:58.880]   oh, go ahead and charge a lot.
[01:00:58.880 --> 01:01:02.400]   Nobody seems to mind the $1,000 price tag.
[01:01:02.400 --> 01:01:05.360]   We thought that was a line too far that could be crossed.
[01:01:05.360 --> 01:01:08.320]   People aren't buying cars, so they have money to buy phones.
[01:01:08.320 --> 01:01:11.840]   And now the question and Mingchi Kuo,
[01:01:11.840 --> 01:01:17.600]   of course, the supply chain analyst from KGI Securities,
[01:01:17.600 --> 01:01:24.320]   he says there will be next time late 2018, as usual, a year from now,
[01:01:24.320 --> 01:01:26.960]   three iPhones with the iPhone 10 form factor.
[01:01:26.960 --> 01:01:30.880]   But interestingly, two with OLEDs and one inexpensive one.
[01:01:30.880 --> 01:01:32.640]   So there'll be a 6.1 inch model.
[01:01:32.640 --> 01:01:39.360]   And that'll have a lower resolution LCD screen and then a giant.
[01:01:40.080 --> 01:01:42.960]   And I think everybody who has an iPhone 10, well, not everybody,
[01:01:42.960 --> 01:01:47.040]   many who have an iPhone 10 would like this 6.5 inch iPhone 10.
[01:01:47.040 --> 01:01:48.000]   Yes, please.
[01:01:48.000 --> 01:01:48.400]   Right?
[01:01:48.400 --> 01:01:49.440]   Yes, please.
[01:01:49.440 --> 01:01:52.640]   Although I love the size, but I think a little bigger might not be so bad.
[01:01:52.640 --> 01:01:55.120]   And a 5.8 inch, which is a little smaller than this.
[01:01:55.120 --> 01:01:56.880]   And it's all going to go to face stuff.
[01:01:56.880 --> 01:01:57.280]   This is--
[01:01:57.280 --> 01:01:58.720]   And it will be no more home button.
[01:01:58.720 --> 01:01:59.280]   That makes sense.
[01:01:59.280 --> 01:02:00.640]   You can't keep doing that old.
[01:02:00.640 --> 01:02:01.840]   It's all new UI.
[01:02:01.840 --> 01:02:06.080]   The motion detection on the face stuff is far more important than unlocking your phone.
[01:02:06.080 --> 01:02:07.040]   And it's going to power everything.
[01:02:07.040 --> 01:02:08.000]   So you got to tell me about that.
[01:02:08.000 --> 01:02:09.440]   You started talking about that.
[01:02:09.440 --> 01:02:12.480]   So the iPhone's going to know what I'm happy or sad?
[01:02:12.480 --> 01:02:16.720]   Well, when you've understood something, what you're looking at,
[01:02:16.720 --> 01:02:18.720]   where your attention is, like that?
[01:02:18.720 --> 01:02:20.880]   Is it going to sell that information of third parties?
[01:02:20.880 --> 01:02:26.800]   Apple has put out pretty strict guidelines for what apps that use those APIs are allowed to do.
[01:02:26.800 --> 01:02:30.960]   And I think they're explicitly not allowed to actually do it for any kind of targeted advertising
[01:02:30.960 --> 01:02:31.760]   or analytics.
[01:02:31.760 --> 01:02:34.160]   As you could do heat mapping, you could do all sorts of stuff.
[01:02:34.160 --> 01:02:35.680]   So that's all stuff that's got to get worked out.
[01:02:35.680 --> 01:02:40.160]   And it's obviously super easy to see the super creepy ways that this could go.
[01:02:40.160 --> 01:02:43.120]   And it's up to all of us to not have it go that way.
[01:02:43.120 --> 01:02:51.120]   But the idea that every product is going to know when you've understood something and when you haven't,
[01:02:51.120 --> 01:02:52.800]   it's deep.
[01:02:52.800 --> 01:02:53.920]   Like it's a big thing.
[01:02:53.920 --> 01:02:56.080]   Oh, Scott Jordan's in the chair room.
[01:02:56.080 --> 01:02:56.800]   Hi, Scott.
[01:02:56.800 --> 01:03:01.440]   Scott, Scott, can you make me one that has all the printing on it?
[01:03:01.440 --> 01:03:02.720]   I totally want that.
[01:03:02.720 --> 01:03:04.480]   I believe that would be a best seller.
[01:03:04.480 --> 01:03:05.760]   I just want-- I want--
[01:03:05.760 --> 01:03:09.280]   The X-ray is on the outside, so this is where we're at class.
[01:03:09.280 --> 01:03:09.440]   Yeah.
[01:03:09.440 --> 01:03:12.880]   Because I have to say, I want the white with the things printed on it.
[01:03:12.880 --> 01:03:14.720]   Just like it looks on the side, I just want that for real.
[01:03:14.720 --> 01:03:16.080]   I would wear that all the time.
[01:03:16.080 --> 01:03:17.600]   That is so chic.
[01:03:17.600 --> 01:03:18.160]   Yeah.
[01:03:18.160 --> 01:03:20.160]   Just is one of those things where it goes past the--
[01:03:20.160 --> 01:03:22.240]   Yeah, I think it's designed the next guy to get it.
[01:03:22.240 --> 01:03:25.680]   I don't know, someone who was basically showing the mugger where it goes.
[01:03:25.680 --> 01:03:26.000]   So--
[01:03:26.000 --> 01:03:29.120]   So if this is the new UI, the new way of being--
[01:03:29.120 --> 01:03:31.440]   Obviously, iPads got to go that way too, right?
[01:03:31.440 --> 01:03:34.240]   I would wait to buy an iPad until the spring,
[01:03:34.240 --> 01:03:36.160]   because that would be an awesome iPad.
[01:03:36.160 --> 01:03:38.720]   An iPhone X-sized iPad?
[01:03:38.720 --> 01:03:40.480]   How does it handle with iPads?
[01:03:40.480 --> 01:03:43.120]   They're often communal machines, especially in families with small kids.
[01:03:43.120 --> 01:03:44.080]   Oh, that's a big problem.
[01:03:44.080 --> 01:03:47.440]   Well, I was going to say, if we're moving to a facial recognition system,
[01:03:47.440 --> 01:03:48.960]   how does it handle multi-use?
[01:03:48.960 --> 01:03:49.680]   I guess you could.
[01:03:49.680 --> 01:03:50.000]   Yeah.
[01:03:50.000 --> 01:03:50.800]   For things like tablets.
[01:03:50.800 --> 01:03:51.280]   Could you--
[01:03:51.280 --> 01:03:54.000]   You could say, oh, I see it's Lisa using it today, or I see--
[01:03:54.000 --> 01:03:55.600]   Or I see it's your daughter, or something.
[01:03:55.600 --> 01:03:57.520]   You can't do it right now, and I don't know why not.
[01:03:57.520 --> 01:03:58.880]   I don't think that would be a hard thing to do.
[01:03:58.880 --> 01:04:00.960]   You could send it to it with the Google home products
[01:04:00.960 --> 01:04:01.840]   at the moment.
[01:04:01.840 --> 01:04:02.560]   With voice.
[01:04:02.560 --> 01:04:03.280]   Yeah, they were saying--
[01:04:03.280 --> 01:04:03.920]   You could train it.
[01:04:03.920 --> 01:04:04.960]   You could actually--
[01:04:04.960 --> 01:04:07.440]   They can differentiate between different accents within the house,
[01:04:07.440 --> 01:04:08.480]   and then tailor there.
[01:04:08.480 --> 01:04:10.000]   You can have different Google accounts.
[01:04:10.000 --> 01:04:14.720]   So when you use it, it puts Ian's Google account when your wife uses it,
[01:04:14.720 --> 01:04:16.160]   puts her Google account on there.
[01:04:16.160 --> 01:04:19.040]   Right now with Amazon Echo, you have to explicitly say it's--
[01:04:19.040 --> 01:04:19.600]   Exactly.
[01:04:19.600 --> 01:04:20.160]   --which my wife's got.
[01:04:20.160 --> 01:04:22.720]   It's coming to Amazon as well, and this is the next logical step.
[01:04:22.720 --> 01:04:24.480]   And you're right, Faces is the next logical step.
[01:04:24.480 --> 01:04:24.480]   No reason.
[01:04:24.480 --> 01:04:26.320]   You could do it with fingerprint now.
[01:04:26.320 --> 01:04:26.960]   OK, I'm welcome.
[01:04:26.960 --> 01:04:29.200]   It'll be interesting with Faces just because I know there's--
[01:04:29.200 --> 01:04:30.800]   there's age--
[01:04:30.800 --> 01:04:32.320]   there's often age considerations.
[01:04:32.320 --> 01:04:35.680]   If you have a small child, though, children's faces change a lot over the course of the day.
[01:04:35.680 --> 01:04:36.480]   Apple says that.
[01:04:36.480 --> 01:04:41.120]   That we've seen spoofing the iPhone X face ID with young people.
[01:04:41.120 --> 01:04:45.040]   There's a video of a mom and her 10-year-old.
[01:04:45.040 --> 01:04:46.400]   And they look enough alike.
[01:04:46.400 --> 01:04:48.560]   And the 10-year-old's face is not fully formed yet,
[01:04:48.560 --> 01:04:49.400]   because--
[01:04:49.400 --> 01:04:51.640]   And so as a result, he can unlock her phone.
[01:04:51.640 --> 01:04:52.440]   We've seen that.
[01:04:52.440 --> 01:04:54.480]   We've also seen it with 3D printed masks.
[01:04:54.480 --> 01:04:55.360]   That's weird, though.
[01:04:55.360 --> 01:04:58.960]   You're not-- took him five minutes to scan the person's face--
[01:04:58.960 --> 01:04:59.240]   Yeah.
[01:04:59.240 --> 01:05:00.320]   --the scanner.
[01:05:00.320 --> 01:05:03.040]   Then they had to print out a 3D printed model.
[01:05:03.040 --> 01:05:06.760]   If there's not already a subreddit devoted to it, there will be by the end of the manual.
[01:05:06.760 --> 01:05:07.760]   I guarantee you that.
[01:05:07.760 --> 01:05:08.760]   I mean, it's hard to--
[01:05:08.760 --> 01:05:12.040]   So you said our security guy said, I will not use Face ID on the iPhone X.
[01:05:12.040 --> 01:05:13.280]   He said, I do not trust it.
[01:05:13.280 --> 01:05:15.240]   By the way, I brought you a bottle of Marmite.
[01:05:15.240 --> 01:05:16.480]   Oh, you stung.
[01:05:16.480 --> 01:05:18.840]   I smuggled it into the country.
[01:05:18.840 --> 01:05:22.560]   Let's get this man some toast and eggs.
[01:05:22.560 --> 01:05:23.360]   I'll open it later.
[01:05:23.360 --> 01:05:24.680]   Steve Gibson said it.
[01:05:24.680 --> 01:05:27.360]   I think I got to say, I love Steve.
[01:05:27.360 --> 01:05:30.080]   Sometimes a little tin foil hat, because he's a security guy.
[01:05:30.080 --> 01:05:31.800]   He knows the risks.
[01:05:31.800 --> 01:05:33.600]   But he says, I will never use Face ID.
[01:05:33.600 --> 01:05:35.480]   I do not trust it.
[01:05:35.480 --> 01:05:38.800]   I will use the password on my iPhone X. He did order an iPhone X.
[01:05:38.800 --> 01:05:39.520]   Is that--
[01:05:39.520 --> 01:05:44.760]   Well, I'm just saying that with a lot of computing, especially in a home context,
[01:05:44.760 --> 01:05:48.840]   and especially as you shift to smartphone, one of the challenges that you're going to have
[01:05:48.840 --> 01:05:53.000]   to be able to switch between different users in a way that's still
[01:05:53.000 --> 01:05:57.920]   context and user appropriate, so that I can do things like order off of Amazon Prime.
[01:05:57.920 --> 01:06:03.120]   But my daughter can't decide to spite order the Barbie Dreamhouse, because I said no or something.
[01:06:03.120 --> 01:06:03.880]   Apple may never do that on a phone.
[01:06:03.880 --> 01:06:05.760]   Apple may never do that on a phone.
[01:06:05.760 --> 01:06:09.120]   But I totally think they have to do it on an iPad.
[01:06:09.120 --> 01:06:10.800]   And wouldn't that be awesome?
[01:06:10.800 --> 01:06:14.120]   You pick up the iPad and it says, oh, configure this for your 12-year-old.
[01:06:14.120 --> 01:06:15.800]   Oh, configure this for mom.
[01:06:15.800 --> 01:06:19.120]   That would be a great product.
[01:06:19.120 --> 01:06:19.600]   I don't know.
[01:06:19.600 --> 01:06:22.760]   Maybe this is just because I don't have a human family to care about.
[01:06:22.760 --> 01:06:25.000]   I enjoy the ear quotes.
[01:06:25.000 --> 01:06:32.640]   Honestly, honestly, I think the unlocking your device is literally the least interesting thing.
[01:06:32.640 --> 01:06:33.640]   No, it's like context.
[01:06:33.640 --> 01:06:34.640]   No, I like that.
[01:06:34.640 --> 01:06:36.400]   Although, that's what everybody's using right now.
[01:06:36.400 --> 01:06:37.400]   For now, yeah.
[01:06:37.400 --> 01:06:38.400]   Yeah, exactly.
[01:06:38.400 --> 01:06:39.400]   I think that actually is very interesting.
[01:06:39.400 --> 01:06:42.760]   And we're curious to see, as someone who doesn't have any internet things, devices in
[01:06:42.760 --> 01:06:46.800]   their house quite deliberately, quite how this is going to affect those people that don't
[01:06:46.800 --> 01:06:48.000]   buy into the ecosystem.
[01:06:48.000 --> 01:06:49.000]   Oh, yeah.
[01:06:49.000 --> 01:06:54.720]   At what point it's going to become so inconvenient not to use this?
[01:06:54.720 --> 01:06:55.720]   You basically have to.
[01:06:55.720 --> 01:07:00.320]   Amazon structures their entire channel because they've already demonstrated, for example,
[01:07:00.320 --> 01:07:04.200]   it's good to be a Whole Foods customer this month because Amazon's knockdown prices.
[01:07:04.200 --> 01:07:05.200]   Yeah.
[01:07:05.200 --> 01:07:07.200]   They've started their stuff in stores.
[01:07:07.200 --> 01:07:12.800]   They've already changed shopper behaviors in several fundamental ways over the last decade.
[01:07:12.800 --> 01:07:18.680]   They're going to start tying in a lot of behavior incentives for you to have them in the house
[01:07:18.680 --> 01:07:25.160]   with the speakers constantly accessible and making them your first line of inquiry.
[01:07:25.160 --> 01:07:27.240]   There's going to be a strong financial incentive attached to it.
[01:07:27.240 --> 01:07:31.280]   What happens to people who say today, oh, I'm not going to use the internet.
[01:07:31.280 --> 01:07:32.280]   It's too much.
[01:07:32.280 --> 01:07:34.440]   And the internet is very much an intrusion.
[01:07:34.440 --> 01:07:35.440]   Yes.
[01:07:35.440 --> 01:07:38.280]   It's very hard to be private on the internet.
[01:07:38.280 --> 01:07:40.200]   Well, you can even take it off the internet, for example.
[01:07:40.200 --> 01:07:42.080]   How about grocery store cards?
[01:07:42.080 --> 01:07:47.560]   Because people who opt out of grocery store loyalty programs can pay up to 30.
[01:07:47.560 --> 01:07:48.560]   I spent so much more.
[01:07:48.560 --> 01:07:49.560]   Yeah.
[01:07:49.560 --> 01:07:54.320]   30% more than people who are like, yes, I'm totally fine with this corporation collecting
[01:07:54.320 --> 01:07:59.000]   data on everything I sell, on everything I buy and selling it to every party.
[01:07:59.000 --> 01:08:03.680]   I'm totally fine with my insurer being able to subpoena that information and use it against
[01:08:03.680 --> 01:08:05.160]   me as has happened.
[01:08:05.160 --> 01:08:06.160]   Yeah.
[01:08:06.160 --> 01:08:07.160]   Has it happened?
[01:08:07.160 --> 01:08:08.160]   Yes.
[01:08:08.160 --> 01:08:09.160]   Because I always thought that was a straw man.
[01:08:09.160 --> 01:08:14.000]   Because I would like to be able to buy my Campbell's soup for $5 instead of paying $3
[01:08:14.000 --> 01:08:15.000]   apiece.
[01:08:15.000 --> 01:08:16.240]   That's very high in sodium.
[01:08:16.240 --> 01:08:17.240]   You probably shouldn't be.
[01:08:17.240 --> 01:08:18.240]   Yeah.
[01:08:18.240 --> 01:08:19.240]   We can't give you insurance.
[01:08:19.240 --> 01:08:23.440]   Just saying that people tend to respond to what they see as a perceived value or perceived
[01:08:23.440 --> 01:08:24.440]   discount.
[01:08:24.440 --> 01:08:28.400]   There are studies that show that if you offer free shipping, people will pay more.
[01:08:28.400 --> 01:08:30.800]   People will buy more on average in a shopping cart.
[01:08:30.800 --> 01:08:33.240]   Because in their brand, like, oh, free shipping, I saved money.
[01:08:33.240 --> 01:08:35.800]   Therefore, I'm free to spend it someplace else.
[01:08:35.800 --> 01:08:36.800]   And all, anybody.
[01:08:36.800 --> 01:08:37.800]   Right.
[01:08:37.800 --> 01:08:38.800]   Amazon Prime is all about that.
[01:08:38.800 --> 01:08:39.800]   Yeah.
[01:08:39.800 --> 01:08:40.800]   Exactly.
[01:08:40.800 --> 01:08:43.360]   And Amazon will just take it further and say, you know what, Prime membership will be lower
[01:08:43.360 --> 01:08:47.280]   on an annual basis if you buy this $150 appliance once.
[01:08:47.280 --> 01:08:49.480]   And someone's like, oh, that amortizes over three years.
[01:08:49.480 --> 01:08:53.920]   And then they don't realize all of the different behaviors that they'll be habituated to over
[01:08:53.920 --> 01:08:54.920]   that time.
[01:08:54.920 --> 01:08:55.920]   There's no doubt in my mind.
[01:08:55.920 --> 01:09:00.560]   And in fact, actually, Amazon is now entering Whole Foods.
[01:09:00.560 --> 01:09:04.520]   Which place is the correct channel where I can get the soup for cheaper?
[01:09:04.520 --> 01:09:07.720]   Yes, by the way, the Dollar Tree, it's only a dollar at the Dollar Tree.
[01:09:07.720 --> 01:09:08.720]   You can get some.
[01:09:08.720 --> 01:09:09.720]   Nice.
[01:09:09.720 --> 01:09:10.720]   And they're not tracking you.
[01:09:10.720 --> 01:09:11.720]   Yep.
[01:09:11.720 --> 01:09:13.720]   Nobody wants Dollar Tree shoppers.
[01:09:13.720 --> 01:09:15.760]   They're just no good, but I'm sorry.
[01:09:15.760 --> 01:09:19.960]   American Cam Soup is so loaded full of sugar and salt, they'll even let them anywhere near
[01:09:19.960 --> 01:09:21.200]   my lips in the first place.
[01:09:21.200 --> 01:09:22.200]   No.
[01:09:22.200 --> 01:09:23.200]   You eat this, my friend.
[01:09:23.200 --> 01:09:24.200]   There's no excuse at all.
[01:09:24.200 --> 01:09:25.200]   You say Marmite's healthy?
[01:09:25.200 --> 01:09:26.200]   Oh, yeah.
[01:09:26.200 --> 01:09:27.200]   Full of vitamin B.
[01:09:27.200 --> 01:09:31.840]   For two slices of that, we'll give someone, if you're, if you're planning on having kids,
[01:09:31.840 --> 01:09:35.080]   two slices of Marmite toast, all the folic acid you will need for the day.
[01:09:35.080 --> 01:09:36.320]   Oh, folic acid is very important.
[01:09:36.320 --> 01:09:37.320]   B to B12.
[01:09:37.320 --> 01:09:38.320]   It's vegetarian.
[01:09:38.320 --> 01:09:41.320]   Before I get pregnant, I'm going to start liking Marmite.
[01:09:41.320 --> 01:09:43.000]   What's in that?
[01:09:43.000 --> 01:09:46.360]   It's basically brew as yeast and flavourings.
[01:09:46.360 --> 01:09:47.360]   It's, um...
[01:09:47.360 --> 01:09:49.360]   Would you like to smell it?
[01:09:49.360 --> 01:09:50.360]   No, I've had lots of Marmite.
[01:09:50.360 --> 01:09:51.360]   Oh, you like it.
[01:09:51.360 --> 01:09:52.360]   That's right.
[01:09:52.360 --> 01:09:53.360]   Yeah.
[01:09:53.360 --> 01:09:54.360]   Okay.
[01:09:54.360 --> 01:09:55.360]   Yes, it's...
[01:09:55.360 --> 01:10:00.840]   I always assumed it was petroleum-based, but this is an old bottle of Marmite.
[01:10:00.840 --> 01:10:01.840]   Is that okay?
[01:10:01.840 --> 01:10:02.840]   I don't think it doesn't go off.
[01:10:02.840 --> 01:10:03.840]   Yeah.
[01:10:03.840 --> 01:10:04.840]   That's what I thought.
[01:10:04.840 --> 01:10:05.840]   He concentrates and gets by.
[01:10:05.840 --> 01:10:06.840]   That's a size with age.
[01:10:06.840 --> 01:10:07.840]   It doesn't go off.
[01:10:07.840 --> 01:10:08.840]   Oh, please.
[01:10:08.840 --> 01:10:11.280]   I vouch to the nation that gave us the Twinkie, the thing with a five-year-old.
[01:10:11.280 --> 01:10:12.280]   The other span.
[01:10:12.280 --> 01:10:13.280]   [laughter]
[01:10:13.280 --> 01:10:18.480]   Apple says the HomePod will not be ready in time for the holidays.
[01:10:18.480 --> 01:10:20.120]   That's kind of a big miss for Apple.
[01:10:20.120 --> 01:10:21.120]   This is their three.
[01:10:21.120 --> 01:10:22.120]   What is it?
[01:10:22.120 --> 01:10:23.120]   $3 or $40?
[01:10:23.120 --> 01:10:25.120]   Speaker that has Siri built in.
[01:10:25.120 --> 01:10:26.120]   Mm.
[01:10:26.120 --> 01:10:31.360]   Uh, and they only touted the sound, the room adapting sound, blah, blah, blah.
[01:10:31.360 --> 01:10:37.000]   Next year, all the company said was the production process needs, quote, "A little more time to
[01:10:37.000 --> 01:10:38.000]   bake."
[01:10:38.000 --> 01:10:39.000]   [laughter]
[01:10:39.000 --> 01:10:40.000]   They do bake them.
[01:10:40.000 --> 01:10:41.000]   Yeah.
[01:10:41.000 --> 01:10:42.000]   They do look a little bit like--
[01:10:42.000 --> 01:10:45.000]   See, we're as long as we've been rained for the likes.
[01:10:45.000 --> 01:10:46.000]   --like most of the time.
[01:10:46.000 --> 01:10:47.000]   Yeah.
[01:10:47.000 --> 01:10:48.000]   Yes.
[01:10:48.000 --> 01:10:53.680]   This is going to be a hugely crowded market, especially this holiday season for Amazon,
[01:10:53.680 --> 01:10:54.680]   of course, in Google.
[01:10:54.680 --> 01:10:55.680]   They're going head-to-head.
[01:10:55.680 --> 01:10:57.520]   Harman Carton has one using Cortana.
[01:10:57.520 --> 01:10:59.640]   I don't think that's going to be-- that's going to be-- that's going to be also ran.
[01:10:59.640 --> 01:11:02.280]   But Amazon has all these different form factors.
[01:11:02.280 --> 01:11:07.040]   Although they told me that my orb, the little round one that goes on my bedside table with
[01:11:07.040 --> 01:11:09.920]   the camera, that it's delayed.
[01:11:09.920 --> 01:11:10.920]   So--
[01:11:10.920 --> 01:11:11.920]   Yeah.
[01:11:11.920 --> 01:11:13.920]   They're really comfortable having a computer with a camera on your bedside.
[01:11:13.920 --> 01:11:14.920]   Absolutely.
[01:11:14.920 --> 01:11:16.400]   What could possibly go wrong?
[01:11:16.400 --> 01:11:19.200]   If anyone hacks it, they've brought that on themselves.
[01:11:19.200 --> 01:11:21.600]   Whatever they see, that's their fault.
[01:11:21.600 --> 01:11:22.600]   Just look for somebody going--
[01:11:22.600 --> 01:11:23.600]   My eyes!
[01:11:23.600 --> 01:11:24.600]   Yes, sorry.
[01:11:24.600 --> 01:11:27.040]   I'm off the grid now.
[01:11:27.040 --> 01:11:33.520]   But now, because once a decade, Apple releases a product that I just don't care about.
[01:11:33.520 --> 01:11:34.520]   And this is it.
[01:11:34.520 --> 01:11:35.520]   This is it.
[01:11:35.520 --> 01:11:36.520]   This is the one for this decade.
[01:11:36.520 --> 01:11:38.080]   I think 10 years ago with some other speakers.
[01:11:38.080 --> 01:11:40.080]   It was the high-fi, the high-fi.
[01:11:40.080 --> 01:11:41.880]   It's literally like, that's a pretty good ratio.
[01:11:41.880 --> 01:11:45.100]   Once a decade, they announced a product that I'm just like, "Hey, you're just speaking
[01:11:45.100 --> 01:11:46.100]   dolphin to me.
[01:11:46.100 --> 01:11:47.800]   I don't understand why I should care about this."
[01:11:47.800 --> 01:11:52.160]   They're coming into a very crowded market because-- obviously, Amazon's got its stuff.
[01:11:52.160 --> 01:11:54.760]   Google's just launched a bunch of stuff around this.
[01:11:54.760 --> 01:11:56.800]   They're coming into it late, more expensive.
[01:11:56.800 --> 01:12:01.640]   It's serious, frankly, not as good as Amazon or Google Voice services.
[01:12:01.640 --> 01:12:06.800]   This is-- traditionally, Apple has either led from the front in certain cases or taken
[01:12:06.800 --> 01:12:07.800]   existing adult technologies.
[01:12:07.800 --> 01:12:08.800]   More often than that.
[01:12:08.800 --> 01:12:09.800]   And just done them better.
[01:12:09.800 --> 01:12:10.800]   Much better.
[01:12:10.800 --> 01:12:14.960]   I thought the really innovative thing they were doing was actually put into either a dashboard
[01:12:14.960 --> 01:12:17.520]   for the many, many other smart home.
[01:12:17.520 --> 01:12:19.920]   Well, that's the-- I think that's the larger story.
[01:12:19.920 --> 01:12:23.760]   Because that was the biggest problem I've seen with all of your smart garage door opener
[01:12:23.760 --> 01:12:26.920]   and your smart lights and your smart doors and your smart this and your smart.
[01:12:26.920 --> 01:12:31.280]   That is the expectation that you are literally going to spend half an hour every day opening
[01:12:31.280 --> 01:12:33.760]   and monkeying with all of these different apps.
[01:12:33.760 --> 01:12:36.760]   And so when Apple rolled out the home app, it was basically, "We have all the stuff
[01:12:36.760 --> 01:12:37.760]   in one place.
[01:12:37.760 --> 01:12:40.720]   Just one tap and you can go through and customize your life."
[01:12:40.720 --> 01:12:42.880]   And I thought, "Well, this makes sense."
[01:12:42.880 --> 01:12:45.800]   And this is right within Apple's wheelhouse of, "We're going to shape this experience,
[01:12:45.800 --> 01:12:47.800]   so it's really frictionless for you."
[01:12:47.800 --> 01:12:48.800]   It never happened.
[01:12:48.800 --> 01:12:49.800]   And never happened.
[01:12:49.800 --> 01:12:51.800]   I honestly thought this was where they were headed.
[01:12:51.800 --> 01:12:55.440]   The IoT developers I've talked to said they don't want to develop for HomeKit.
[01:12:55.440 --> 01:12:58.360]   That's too hard compared to Amazon.
[01:12:58.360 --> 01:13:02.800]   And this is why even a few months delay on the HomePod is actually a problem for Apple
[01:13:02.800 --> 01:13:05.320]   because they've already fallen far behind.
[01:13:05.320 --> 01:13:06.960]   And this is a race.
[01:13:06.960 --> 01:13:09.280]   It's as if they're saying, "We're not going to even.
[01:13:09.280 --> 01:13:11.560]   We're going to go ahead and lap us a few more times."
[01:13:11.560 --> 01:13:13.560]   Let's just get on with the AR stuff.
[01:13:13.560 --> 01:13:15.200]   Let's put the smart speakers aside.
[01:13:15.200 --> 01:13:16.200]   Well, they're smart.
[01:13:16.200 --> 01:13:19.960]   And of course, the rumor is that in a couple of years, Apple will have some spectacles
[01:13:19.960 --> 01:13:21.480]   that will have AR built in.
[01:13:21.480 --> 01:13:23.200]   And that would be a killer product.
[01:13:23.200 --> 01:13:24.200]   And it may.
[01:13:24.200 --> 01:13:26.200]   If you know AirPods, you could talk to Siri.
[01:13:26.200 --> 01:13:27.200]   I love AirPods.
[01:13:27.200 --> 01:13:32.240]   I just got the Google Pixel Buds, your husband, I know, is reviewing it for Tom's hardware.
[01:13:32.240 --> 01:13:36.280]   These are like, I don't know, a poor man's AirPods.
[01:13:36.280 --> 01:13:38.400]   They don't quite work as well.
[01:13:38.400 --> 01:13:40.000]   They showed off the translation thing.
[01:13:40.000 --> 01:13:43.520]   We tried this with Russian earlier before the show and it didn't work very well.
[01:13:43.520 --> 01:13:44.520]   It's not great.
[01:13:44.520 --> 01:13:45.520]   It's not great.
[01:13:45.520 --> 01:13:46.520]   I don't like the form factor.
[01:13:46.520 --> 01:13:49.800]   I think AirPods are a bad form factor for translators.
[01:13:49.800 --> 01:13:56.080]   I love the idea though, if you put in a temple piece of some spectacles, HomeKit instruction,
[01:13:56.080 --> 01:13:58.080]   maybe you look a fish.
[01:13:58.080 --> 01:14:00.080]   Yeah, you look a fish.
[01:14:00.080 --> 01:14:01.800]   That's why it's kind of fascinating.
[01:14:01.800 --> 01:14:04.280]   The science fiction so influences Silicon Valley.
[01:14:04.280 --> 01:14:05.280]   Oh yeah.
[01:14:05.280 --> 01:14:08.600]   Clearly the reason they're looking at any ear solutions is because of Ducklin' Sadams.
[01:14:08.600 --> 01:14:11.280]   They're just trying to reinvent the...
[01:14:11.280 --> 01:14:12.280]   That's a good idea.
[01:14:12.280 --> 01:14:14.760]   I think it's about trying to reinvent that horrible thing they dropped into Chekhov's
[01:14:14.760 --> 01:14:16.120]   ear and Wrath of Khan.
[01:14:16.120 --> 01:14:17.920]   That's a good idea.
[01:14:17.920 --> 01:14:18.920]   Give it time.
[01:14:18.920 --> 01:14:19.920]   Give it time.
[01:14:19.920 --> 01:14:23.640]   That came straight from the Night Gallery Earwig episode, right?
[01:14:23.640 --> 01:14:24.720]   Don't know.
[01:14:24.720 --> 01:14:25.720]   No.
[01:14:25.720 --> 01:14:27.640]   Yeah, that's a nightmare right there.
[01:14:27.640 --> 01:14:30.000]   That's worse than any dog-like robot.
[01:14:30.000 --> 01:14:33.960]   No, it's going to be interesting with AR because when you think about it, Apple effectively
[01:14:33.960 --> 01:14:38.480]   killed the low-to-mid-market digital camera.
[01:14:38.480 --> 01:14:39.480]   With the iPhone?
[01:14:39.480 --> 01:14:43.480]   With the iPhone because people stopped slipping around both their $300 digital camera and
[01:14:43.480 --> 01:14:44.480]   their phone because their phone...
[01:14:44.480 --> 01:14:46.960]   The pictures were not as good, but they were good enough.
[01:14:46.960 --> 01:14:48.800]   The camera on the 10 is nice.
[01:14:48.800 --> 01:14:49.800]   Yeah.
[01:14:49.800 --> 01:14:50.800]   It's nice.
[01:14:50.800 --> 01:14:54.440]   And the new portrait mode is like instant morsey album covers, for anyone.
[01:14:54.440 --> 01:14:56.440]   Actually, a number of people have commented that the...
[01:14:56.440 --> 01:14:57.440]   Yeah.
[01:14:57.440 --> 01:15:00.960]   ...they do with AR to see if they manage to do the same thing where they come up with
[01:15:00.960 --> 01:15:05.800]   a product ecosystem that is unobtrusive enough and ubiquitous enough where they manage to
[01:15:05.800 --> 01:15:07.840]   find and shape human behavior that way too.
[01:15:07.840 --> 01:15:13.460]   I think honestly, Apple are in a good position to do that because we're so far into the
[01:15:13.460 --> 01:15:17.760]   hype cycle that it's going to be at least two to three years before you can get consumer
[01:15:17.760 --> 01:15:21.720]   AR technology, which works, which has a battery life, which is...
[01:15:21.720 --> 01:15:22.920]   There's a lot of problems with that.
[01:15:22.920 --> 01:15:26.760]   Not having birds melt, but it's just...
[01:15:26.760 --> 01:15:30.840]   It's getting it down to that small form factor, and I don't see any evidences yet that we're
[01:15:30.840 --> 01:15:32.520]   any closer than two years away from that.
[01:15:32.520 --> 01:15:35.560]   Do you think it'll go faster if we all start pronouncing it AR?
[01:15:35.560 --> 01:15:36.560]   AR!
[01:15:36.560 --> 01:15:37.560]   I want AR now!
[01:15:37.560 --> 01:15:40.720]   Yeah, but then you just get it over one eye and you'll be AR.
[01:15:40.720 --> 01:15:41.720]   AR!
[01:15:41.720 --> 01:15:42.720]   That's it.
[01:15:42.720 --> 01:15:44.720]   Or you should be able to introduce the heartline.
[01:15:44.720 --> 01:15:46.720]   Oh, you look like you're a pewse borg.
[01:15:46.720 --> 01:15:47.720]   It would be so great.
[01:15:47.720 --> 01:15:49.720]   Oh, that is the design goal.
[01:15:49.720 --> 01:15:50.720]   Yeah, there we go.
[01:15:50.720 --> 01:15:51.720]   That is the design goal.
[01:15:51.720 --> 01:15:52.720]   Everyone's gonna like it.
[01:15:52.720 --> 01:15:53.720]   Something that you use in your glass.
[01:15:53.720 --> 01:15:56.320]   You just flip it down and then you flip it up.
[01:15:56.320 --> 01:15:58.720]   Well, there's a lot of opportunities.
[01:15:58.720 --> 01:16:02.320]   Or like a robot parrot that you're just having a shoulder.
[01:16:02.320 --> 01:16:03.320]   Yeah, AR!
[01:16:03.320 --> 01:16:04.320]   It's in my heart!
[01:16:04.320 --> 01:16:06.120]   So you can go from there?
[01:16:06.120 --> 01:16:09.800]   Yeah, I mean, Magic League were burnt through one billion and a half over this and we know
[01:16:09.800 --> 01:16:10.800]   we're close to it.
[01:16:10.800 --> 01:16:11.800]   I think I'm coming through.
[01:16:11.800 --> 01:16:12.800]   Yeah.
[01:16:12.800 --> 01:16:13.800]   You know, it's...
[01:16:13.800 --> 01:16:14.800]   It's not money, is it?
[01:16:14.800 --> 01:16:15.800]   Google's chucking billions at it.
[01:16:15.800 --> 01:16:19.840]   Facebook is chucking billions at it and wasting large chunks of it.
[01:16:19.840 --> 01:16:22.880]   And what about, as we talked about earlier, China, Russia?
[01:16:22.880 --> 01:16:23.880]   Yeah.
[01:16:23.880 --> 01:16:26.480]   We're not completely up on what's happening in these countries.
[01:16:26.480 --> 01:16:29.880]   I know China is working very hard on this.
[01:16:29.880 --> 01:16:33.800]   So if past experiences are the only thing to go by, China will do it better or at least
[01:16:33.800 --> 01:16:35.440]   cheaper and then allow some insight.
[01:16:35.440 --> 01:16:36.960]   What will it play in the US market?
[01:16:36.960 --> 01:16:38.200]   Will it play in the European market?
[01:16:38.200 --> 01:16:39.480]   Will it play in the West?
[01:16:39.480 --> 01:16:41.800]   I think you don't have to look at necessarily as consumer markets.
[01:16:41.800 --> 01:16:48.240]   You can also ask yourself how what industries are going to boost this or whether this is
[01:16:48.240 --> 01:16:51.360]   going to penetrate secondary and college level education.
[01:16:51.360 --> 01:16:57.800]   By the way, Beatmaster points out that the Babelfish was actually the final proof of
[01:16:57.800 --> 01:17:00.120]   the non-existence of God.
[01:17:00.120 --> 01:17:02.120]   So we've come full circle.
[01:17:02.120 --> 01:17:05.800]   Although the person that did come to that conclusion then went on to prove that black
[01:17:05.800 --> 01:17:08.200]   wasn't white and got killed at the next pedestrian crossing.
[01:17:08.200 --> 01:17:10.200]   Oh, you've read it and memorized it.
[01:17:10.200 --> 01:17:11.200]   I have read it.
[01:17:11.200 --> 01:17:13.360]   This is where AI overlords come in.
[01:17:13.360 --> 01:17:14.360]   Yeah.
[01:17:14.360 --> 01:17:15.360]   Amen.
[01:17:15.360 --> 01:17:18.680]   I read the Hitchhiker's Guide right as I was learning English.
[01:17:18.680 --> 01:17:20.480]   Oh, what a good way to learn English.
[01:17:20.480 --> 01:17:25.520]   It was one of the first books I read in English and I still didn't quite learn no English
[01:17:25.520 --> 01:17:27.560]   but I remember reading that book.
[01:17:27.560 --> 01:17:28.560]   I read it many times since then.
[01:17:28.560 --> 01:17:29.560]   Must have been puzzling.
[01:17:29.560 --> 01:17:35.040]   And I remember the words clicking in my head because I remember seeing it not understanding
[01:17:35.040 --> 01:17:36.760]   and then I remember seeing it understanding.
[01:17:36.760 --> 01:17:43.360]   And there's one phrase in particular that just wedges off in my head when it's near the
[01:17:43.360 --> 01:17:46.840]   beginning of the first book when the Vogue on Ships show up to destroy the earth.
[01:17:46.840 --> 01:17:47.840]   Yes.
[01:17:47.840 --> 01:17:48.840]   Spoiler warning.
[01:17:48.840 --> 01:17:49.840]   They do.
[01:17:49.840 --> 01:17:52.400]   For a book that's 30 odd years ago.
[01:17:52.400 --> 01:17:58.680]   And there's a line that says something like the Vogue on Ships hung in the air in exactly
[01:17:58.680 --> 01:18:00.240]   the way that bricks don't.
[01:18:00.240 --> 01:18:01.240]   Yes.
[01:18:01.240 --> 01:18:05.080]   And I was like, as a little kid reading it I was learning language and I was like, that
[01:18:05.080 --> 01:18:08.720]   is something about that made me think it's a beautiful sentence.
[01:18:08.720 --> 01:18:09.720]   That's very Douglas.
[01:18:09.720 --> 01:18:12.360]   That's hung in the air in exactly the way that bricks don't.
[01:18:12.360 --> 01:18:13.360]   Exactly.
[01:18:13.360 --> 01:18:16.000]   Douglas's sense of humor in his style and I love it.
[01:18:16.000 --> 01:18:18.400]   And I love it that it made English click for you.
[01:18:18.400 --> 01:18:19.400]   Yeah.
[01:18:19.400 --> 01:18:21.720]   Like that line like that's amazing.
[01:18:21.720 --> 01:18:22.720]   That's amazing.
[01:18:22.720 --> 01:18:25.600]   I don't know if they still do it in the later editions but I've got to look at second or
[01:18:25.600 --> 01:18:28.240]   third edition of the paper about when it came out.
[01:18:28.240 --> 01:18:32.200]   And with the Vogue ons said when they first emerged from the seas and late climbing and
[01:18:32.200 --> 01:18:36.000]   panting on the Virgin and then it breaks shores of their planet.
[01:18:36.000 --> 01:18:40.040]   I don't know if they still do that in the new paperbacks but it was just like, oh, I
[01:18:40.040 --> 01:18:41.040]   like this.
[01:18:41.040 --> 01:18:42.040]   This is funny.
[01:18:42.040 --> 01:18:43.040]   It happens.
[01:18:43.040 --> 01:18:49.480]   Well, as some people know, it was originally a BBC radio player and the radio players
[01:18:49.480 --> 01:18:51.800]   are very differently highly recommended.
[01:18:51.800 --> 01:18:52.800]   And it's quite good.
[01:18:52.800 --> 01:18:54.200]   And they're restarting.
[01:18:54.200 --> 01:18:55.200]   They're restarting?
[01:18:55.200 --> 01:18:56.200]   Yep.
[01:18:56.200 --> 01:18:59.560]   There are going to be more Hitchhiker's Guide episodes coming down the line very shortly
[01:18:59.560 --> 01:19:00.560]   indeed.
[01:19:00.560 --> 01:19:03.880]   And trust me, I can't wait.
[01:19:03.880 --> 01:19:07.840]   I wonder if the Internet Archive has the original, yes it does.
[01:19:07.840 --> 01:19:10.240]   It looks like the BBC radio plays.
[01:19:10.240 --> 01:19:13.480]   Oh, just get them from the BBC.
[01:19:13.480 --> 01:19:14.480]   You could buy it.
[01:19:14.480 --> 01:19:15.480]   Oh, yeah.
[01:19:15.480 --> 01:19:19.840]   And the radio shows that, I mean, they're obviously widely parted but the BBC is worth
[01:19:19.840 --> 01:19:20.840]   paying for.
[01:19:20.840 --> 01:19:22.680]   Actually, yeah, give them a little something.
[01:19:22.680 --> 01:19:23.680]   Yeah.
[01:19:23.680 --> 01:19:24.680]   A few, it happens.
[01:19:24.680 --> 01:19:29.800]   Well, if I could buy my BBC license fee, but you know what's crap, is BBC America.
[01:19:29.800 --> 01:19:30.800]   Yes.
[01:19:30.800 --> 01:19:31.800]   Oh, no.
[01:19:31.800 --> 01:19:36.280]   That deserves to go out of business because they put ads, they take BBC things which are
[01:19:36.280 --> 01:19:39.840]   not meant to have ads in them, they ruin it by shoving ads in it.
[01:19:39.840 --> 01:19:40.840]   That must cease to exist.
[01:19:40.840 --> 01:19:42.920]   And also, they show some really bad shows.
[01:19:42.920 --> 01:19:43.920]   Yes.
[01:19:43.920 --> 01:19:44.920]   Why are you being served?
[01:19:44.920 --> 01:19:47.520]   It's huge over here and it's a shamefully poor show.
[01:19:47.520 --> 01:19:49.040]   At least they're not drinking.
[01:19:49.040 --> 01:19:50.360]   It's also pretty old, isn't it?
[01:19:50.360 --> 01:19:52.320]   I feel like they've been showing it for you.
[01:19:52.320 --> 01:19:53.320]   That would have been just the nadir.
[01:19:53.320 --> 01:19:55.160]   It's 1970s level at Benny Hill.
[01:19:55.160 --> 01:19:56.160]   Yeah.
[01:19:56.160 --> 01:19:57.360]   You know, of all the great British community.
[01:19:57.360 --> 01:19:58.360]   The Benny Hill's illegal.
[01:19:58.360 --> 01:20:01.240]   I would hope that it's getting close to that.
[01:20:01.240 --> 01:20:05.160]   But I mean, it's just like, of all the great British comedy series, you have Benny Hill
[01:20:05.160 --> 01:20:06.880]   and all you being, all the things.
[01:20:06.880 --> 01:20:09.160]   I think Benny Hill has clearly thought crime by now.
[01:20:09.160 --> 01:20:13.720]   I think actually it's a reflection of what you Brits really think of us.
[01:20:13.720 --> 01:20:14.720]   Oh, give them that.
[01:20:14.720 --> 01:20:15.720]   They'll love it.
[01:20:15.720 --> 01:20:20.840]   Ah, well, we did manage to sell 40 towels to the Germans, which I would love to be the
[01:20:20.840 --> 01:20:21.840]   person that did come.
[01:20:21.840 --> 01:20:22.840]   That is the remark.
[01:20:22.840 --> 01:20:23.840]   You know, they actually played the two things.
[01:20:23.840 --> 01:20:25.160]   They don't think it's a comedy.
[01:20:25.160 --> 01:20:26.160]   That's the funny thing.
[01:20:26.160 --> 01:20:27.160]   Oh, well.
[01:20:27.160 --> 01:20:30.160]   I do enjoy how one of your funny shows is called Don't Mention the Score.
[01:20:30.160 --> 01:20:31.160]   Yes.
[01:20:31.160 --> 01:20:33.840]   But I mean, even Monty Python, they...
[01:20:33.840 --> 01:20:37.440]   It's the best English comedy that we haven't seen.
[01:20:37.440 --> 01:20:40.080]   Oh, Prime Minister question error.
[01:20:40.080 --> 01:20:42.000]   Oh, Prime Minister...
[01:20:42.000 --> 01:20:43.000]   Well, Prime Minister's question...
[01:20:43.000 --> 01:20:44.000]   That's not a comedy, but okay.
[01:20:44.000 --> 01:20:45.800]   Yeah, it's gonna be humorous sometimes.
[01:20:45.800 --> 01:20:46.640]   Yes, Minister is good.
[01:20:46.640 --> 01:20:47.640]   Yes, Minister is good.
[01:20:47.640 --> 01:20:48.640]   Yeah.
[01:20:48.640 --> 01:20:49.640]   Both very good.
[01:20:49.640 --> 01:20:51.120]   I like the IT crowd.
[01:20:51.120 --> 01:20:52.120]   It's quite good.
[01:20:52.120 --> 01:20:53.120]   The IT crowd is very good.
[01:20:53.120 --> 01:20:54.120]   But we've seen that.
[01:20:54.120 --> 01:20:55.120]   I like Blackadder.
[01:20:55.120 --> 01:20:56.120]   Yeah.
[01:20:56.120 --> 01:20:57.120]   I like it.
[01:20:57.120 --> 01:20:58.120]   I like it.
[01:20:58.120 --> 01:20:59.120]   I like it.
[01:20:59.120 --> 01:21:00.120]   I like it.
[01:21:00.120 --> 01:21:01.120]   I like it.
[01:21:01.120 --> 01:21:02.120]   2012 a lot.
[01:21:02.120 --> 01:21:03.120]   2012 was very good.
[01:21:03.120 --> 01:21:06.640]   It was a lot of fun to be up there on, you know, at some of the best...
[01:21:06.640 --> 01:21:09.360]   I'm always going to lobby for space because I think space is just...
[01:21:09.360 --> 01:21:10.360]   Oh, space.
[01:21:10.360 --> 01:21:11.360]   Not seen-yous.
[01:21:11.360 --> 01:21:12.360]   Space.
[01:21:12.360 --> 01:21:13.360]   You've never seen it.
[01:21:13.360 --> 01:21:14.360]   Oh, you've never seen it.
[01:21:14.360 --> 01:21:15.360]   Oh, really worked in the devil.
[01:21:15.360 --> 01:21:16.360]   It's time, at least I'm in pig.
[01:21:16.360 --> 01:21:17.360]   Read the world.
[01:21:17.360 --> 01:21:18.360]   I love it.
[01:21:18.360 --> 01:21:19.360]   Yeah.
[01:21:19.360 --> 01:21:20.360]   And read the world for season three and full.
[01:21:20.360 --> 01:21:21.360]   Bobby Llewinn is a good friend of the show actually.
[01:21:21.360 --> 01:21:22.360]   Oh, also Blackbooks.
[01:21:22.360 --> 01:21:24.360]   I feel like we should actually mention Blackbooks which is...
[01:21:24.360 --> 01:21:25.360]   Yeah, I know that either.
[01:21:25.360 --> 01:21:26.360]   ...Gillarious.
[01:21:26.360 --> 01:21:27.360]   And...
[01:21:27.360 --> 01:21:28.560]   I've got to have to listen to this podcast and write everything done.
[01:21:28.560 --> 01:21:29.800]   There's one called the book.
[01:21:29.800 --> 01:21:30.800]   Well, anything...
[01:21:30.800 --> 01:21:33.680]   I'm just going to share with Michelle Gomez and I loved Green Wing which I think is...
[01:21:33.680 --> 01:21:34.680]   When I start something.
[01:21:34.680 --> 01:21:36.080]   ...not as highly rated as it should be.
[01:21:36.080 --> 01:21:37.080]   Yes.
[01:21:37.080 --> 01:21:40.720]   And Michelle Gomez is fantastic in that and she's been very good in Doctor Who and basically
[01:21:40.720 --> 01:21:42.120]   if she's in anything I'll watch it.
[01:21:42.120 --> 01:21:44.120]   There's actually a really good Russian sitcom.
[01:21:44.120 --> 01:21:45.120]   No, I'm just kidding.
[01:21:45.120 --> 01:21:46.120]   I'm just kidding.
[01:21:46.120 --> 01:21:47.120]   I'm just kidding.
[01:21:47.120 --> 01:21:48.120]   There is.
[01:21:48.120 --> 01:21:49.120]   Do Russians even ask?
[01:21:49.120 --> 01:21:50.120]   Oh, sure they do.
[01:21:50.120 --> 01:21:51.120]   I have no idea.
[01:21:51.120 --> 01:21:52.120]   Yeah.
[01:21:52.120 --> 01:21:53.120]   But yeah, no space to get yourself the...
[01:21:53.120 --> 01:21:54.120]   Yeah.
[01:21:54.120 --> 01:21:55.120]   The DVD set and space.
[01:21:55.120 --> 01:21:56.120]   ...lirge on it.
[01:21:56.120 --> 01:21:57.120]   Oh, yeah.
[01:21:57.120 --> 01:21:58.120]   It's just...
[01:21:58.120 --> 01:21:59.120]   Oh, it's on Netflix.
[01:21:59.120 --> 01:22:00.120]   Sweet.
[01:22:00.120 --> 01:22:02.160]   Carsten knows.
[01:22:02.160 --> 01:22:03.160]   It's a comedy?
[01:22:03.160 --> 01:22:04.160]   Oh, yes.
[01:22:04.160 --> 01:22:05.160]   Yes, it's...
[01:22:05.160 --> 01:22:07.720]   It's how Simon wrote "Pegg and Egg Are Right" first got started.
[01:22:07.720 --> 01:22:08.720]   Yes.
[01:22:08.720 --> 01:22:10.360]   Ah, so it's early Simon Pegg.
[01:22:10.360 --> 01:22:11.360]   Yes, it's very good.
[01:22:11.360 --> 01:22:13.520]   And Nick Frost as well.
[01:22:13.520 --> 01:22:14.520]   Okay.
[01:22:14.520 --> 01:22:16.720]   It is listed as one of the top 25...
[01:22:16.720 --> 01:22:18.720]   And Jessica Hines in the TV show.
[01:22:18.720 --> 01:22:19.720]   Yes, she's very good.
[01:22:19.720 --> 01:22:21.960]   ...comedy TV shows streaming on Netflix right now.
[01:22:21.960 --> 01:22:22.960]   Nick Frost also has an excellent thing.
[01:22:22.960 --> 01:22:24.960]   I think called "Manstroke Woman" which is...
[01:22:24.960 --> 01:22:25.960]   Oh, yes.
[01:22:25.960 --> 01:22:26.960]   Yes.
[01:22:26.960 --> 01:22:27.960]   You've seen the "Man Cold" thing with...
[01:22:27.960 --> 01:22:29.960]   Oh, "Man Cold" is one of my favorite sketches.
[01:22:29.960 --> 01:22:32.200]   I think of that every time somebody in my house gets sick.
[01:22:32.200 --> 01:22:33.200]   Yeah, that's...
[01:22:33.200 --> 01:22:39.120]   I really got into a Netflix I got into the original title was called "Scrotal Recall" and
[01:22:39.120 --> 01:22:40.680]   they had to change it.
[01:22:40.680 --> 01:22:42.320]   Right, at least so.
[01:22:42.320 --> 01:22:43.320]   Yeah, it's...
[01:22:43.320 --> 01:22:44.320]   Although, that's pretty funny.
[01:22:44.320 --> 01:22:45.320]   No, the...
[01:22:45.320 --> 01:22:46.320]   I'm spacing.
[01:22:46.320 --> 01:22:47.320]   What is it?
[01:22:47.320 --> 01:22:48.760]   I'm spacing what the actual title is.
[01:22:48.760 --> 01:22:49.760]   It's on Netflix.
[01:22:49.760 --> 01:22:51.760]   "Chattram, Scrotal Recall."
[01:22:51.760 --> 01:22:53.640]   What is it actually called?
[01:22:53.640 --> 01:22:56.120]   Well, you know what?
[01:22:56.120 --> 01:22:57.120]   It's coupling.
[01:22:57.120 --> 01:22:58.120]   Coupling.
[01:22:58.120 --> 01:22:59.120]   Last of the summer one.
[01:22:59.120 --> 01:23:00.120]   Oh, "Cold Feet."
[01:23:00.120 --> 01:23:02.120]   The first season of "Cold Feet" was pretty funny.
[01:23:02.120 --> 01:23:03.120]   Love "Sick."
[01:23:03.120 --> 01:23:04.120]   Brian Laurie.
[01:23:04.120 --> 01:23:05.120]   Love "Brian Laurie."
[01:23:05.120 --> 01:23:06.120]   Love "Sick."
[01:23:06.120 --> 01:23:07.120]   Love "Sick."
[01:23:07.120 --> 01:23:08.120]   I haven't even heard of that.
[01:23:08.120 --> 01:23:09.120]   Yeah, it's very funny.
[01:23:09.120 --> 01:23:10.120]   Okay.
[01:23:10.120 --> 01:23:12.120]   Somebody said that's not the Kevin Spacey show "Scrotal Recall."
[01:23:12.120 --> 01:23:13.120]   That's a bit...
[01:23:13.120 --> 01:23:14.120]   Let's take a break.
[01:23:14.120 --> 01:23:15.120]   Yeah.
[01:23:15.120 --> 01:23:17.120]   Maybe it's a good time to do that or show up.
[01:23:17.120 --> 01:23:20.120]   My balls being with you.
[01:23:20.120 --> 01:23:25.440]   Just one of many programs on the Twitter network we would love for you to sample some
[01:23:25.440 --> 01:23:26.440]   of the others.
[01:23:26.440 --> 01:23:28.120]   Nothing as good as spaced.
[01:23:28.120 --> 01:23:31.720]   But maybe this small movie will influence your opinion watch.
[01:23:31.720 --> 01:23:33.160]   Previously on "Twit."
[01:23:33.160 --> 01:23:36.120]   It really sucks that the Martian is completely unsequelable.
[01:23:36.120 --> 01:23:37.120]   Right.
[01:23:37.120 --> 01:23:39.840]   Because I'm sure people would love to have more and more books with Marlottany, but it's
[01:23:39.840 --> 01:23:42.120]   like they'll be back to Mars.
[01:23:42.120 --> 01:23:43.120]   No!
[01:23:43.120 --> 01:23:45.120]   Like, no, bite me.
[01:23:45.120 --> 01:23:46.120]   Yeah.
[01:23:46.120 --> 01:23:47.120]   Triangulation.
[01:23:47.120 --> 01:23:48.960]   Andy Weir is here.
[01:23:48.960 --> 01:23:51.120]   He is the author of "The Martian."
[01:23:51.120 --> 01:23:53.120]   And he has a new book.
[01:23:53.120 --> 01:23:58.080]   Believe it or not, Artemis is actually more true to real science than the Martian is.
[01:23:58.080 --> 01:24:00.080]   But it takes place further in the future.
[01:24:00.080 --> 01:24:02.080]   Martian takes place in the 2030s.
[01:24:02.080 --> 01:24:04.080]   Artemis takes place in the 2090s.
[01:24:04.080 --> 01:24:09.080]   It's a little silly, but you could argue that it's economic fiction.
[01:24:09.080 --> 01:24:10.080]   Tech News Weekly.
[01:24:10.080 --> 01:24:15.200]   We'll have a chat with once the star of Stargate Atlantis, David Hewlett, about what it takes
[01:24:15.200 --> 01:24:19.080]   to produce an 80s film in 2017.
[01:24:19.080 --> 01:24:20.560]   Like, you just had so much fun with it.
[01:24:20.560 --> 01:24:23.080]   It's hard not to get wrapped up in that stuff.
[01:24:23.080 --> 01:24:24.080]   We did those tests.
[01:24:24.080 --> 01:24:26.880]   And my wife said, "I'm not going to New York and having you run around like that.
[01:24:26.880 --> 01:24:27.880]   You can't do that."
[01:24:27.880 --> 01:24:29.880]   I put a rolled up sock in there the first time.
[01:24:29.880 --> 01:24:30.880]   I'm sorry.
[01:24:30.880 --> 01:24:31.880]   The sausage is obscene.
[01:24:31.880 --> 01:24:32.880]   Yeah.
[01:24:32.880 --> 01:24:34.880]   Now the mystery really is gone.
[01:24:34.880 --> 01:24:35.880]   To it.
[01:24:35.880 --> 01:24:38.880]   Technology isn't always pretty, but we are.
[01:24:38.880 --> 01:24:39.880]   Oh boy.
[01:24:39.880 --> 01:24:42.880]   I'm going to have to watch that human interview.
[01:24:42.880 --> 01:24:46.880]   Yes, please tune in our shows all of them all week long.
[01:24:46.880 --> 01:24:48.880]   You could find them all at twit.tv.
[01:24:48.880 --> 01:24:51.880]   Our show today brought to you by WordPress.
[01:24:51.880 --> 01:24:52.880]   I love it.
[01:24:52.880 --> 01:24:57.680]   When we first started doing ads for WordPress last year or early this year, I was like,
[01:24:57.680 --> 01:25:02.680]   "They were touting that 27% of the web runs on WordPress.
[01:25:02.680 --> 01:25:07.680]   It is now 29% and I'm going to take credit for that additional 2% right here right now."
[01:25:07.680 --> 01:25:09.680]   But that is amazing number.
[01:25:09.680 --> 01:25:14.680]   29% of the internet runs on WordPress.
[01:25:14.680 --> 01:25:18.680]   I've been using WordPress since, well, early 2000s.
[01:25:18.680 --> 01:25:22.480]   Back at the Matt Mullenweg days when it was, you know, just first began.
[01:25:22.480 --> 01:25:23.480]   I loved it.
[01:25:23.480 --> 01:25:25.320]   Ran it on my own server.
[01:25:25.320 --> 01:25:28.960]   But I've moved to WordPress.com because that way they host it, which is great.
[01:25:28.960 --> 01:25:31.320]   And by the way, their hosting is very affordable.
[01:25:31.320 --> 01:25:33.280]   And they service it.
[01:25:33.280 --> 01:25:34.280]   They keep it up to date.
[01:25:34.280 --> 01:25:35.880]   They keep the plugins up to date.
[01:25:35.880 --> 01:25:38.560]   They add great features like HTTPS.
[01:25:38.560 --> 01:25:41.320]   So my site is secure.
[01:25:41.320 --> 01:25:48.280]   They have fabulous social plugins so that when somebody visits the site and wants to
[01:25:48.280 --> 01:25:51.600]   share it, it's easy for them to put it on Facebook and Twitter.
[01:25:51.600 --> 01:25:54.840]   You may, if you're a business and you know, you have a presence on Facebook and Twitter,
[01:25:54.840 --> 01:25:59.360]   it's all well and good, but you've got to have a website with your business name.
[01:25:59.360 --> 01:26:00.840]   And WordPress can make that easy.
[01:26:00.840 --> 01:26:01.840]   You don't need experience.
[01:26:01.840 --> 01:26:03.680]   You don't need to know web technologies.
[01:26:03.680 --> 01:26:05.800]   You don't need to hire somebody.
[01:26:05.800 --> 01:26:07.400]   You can do it all yourself.
[01:26:07.400 --> 01:26:10.680]   Very quickly, WordPress.com will guide you through the process.
[01:26:10.680 --> 01:26:12.600]   They take care of all the technical pieces.
[01:26:12.600 --> 01:26:13.960]   You choose a beautiful template.
[01:26:13.960 --> 01:26:15.320]   There are hundreds.
[01:26:15.320 --> 01:26:18.720]   And by the way, you can change it anytime again and again.
[01:26:18.720 --> 01:26:20.920]   You get built-in search engine optimization.
[01:26:20.920 --> 01:26:22.200]   That's something people pay money for.
[01:26:22.200 --> 01:26:23.200]   You don't need that.
[01:26:23.200 --> 01:26:24.200]   Let them do it.
[01:26:24.200 --> 01:26:25.640]   They do a great job.
[01:26:25.640 --> 01:26:30.360]   And because WordPress.com is a community, you can put a follow button up and you're going
[01:26:30.360 --> 01:26:34.320]   to get traffic just from the WordPress community, which is fantastic.
[01:26:34.320 --> 01:26:37.120]   Their business plans let you access hundreds of plugins and themes.
[01:26:37.120 --> 01:26:40.080]   They even have plans to let you bring your own theme and plugins.
[01:26:40.080 --> 01:26:41.720]   A great customer support team.
[01:26:41.720 --> 01:26:47.240]   I actually use them a couple of times and they're really good WordPress experts and
[01:26:47.240 --> 01:26:51.560]   they're really eager to help you get the most out of your site.
[01:26:51.560 --> 01:26:54.360]   24 hours a day, Monday through Friday, weekends too.
[01:26:54.360 --> 01:27:00.240]   And I mean all of this for as little as $4 a month, a buck a week.
[01:27:00.240 --> 01:27:03.720]   Come see why 29% of all websites run on WordPress.
[01:27:03.720 --> 01:27:09.360]   You get started today with 15% off any new plan purchase at WordPress.com/twit.
[01:27:09.360 --> 01:27:12.400]   It's where I run my blog, leoloport.com.
[01:27:12.400 --> 01:27:13.400]   I just love it.
[01:27:13.400 --> 01:27:14.400]   You'll love it too.
[01:27:14.400 --> 01:27:17.560]   And if you're a business and you don't have your own site, you've got to do it.
[01:27:17.560 --> 01:27:21.520]   You're not real in this world unless you have your own website.
[01:27:21.520 --> 01:27:24.600]   WordPress.com/twit.
[01:27:24.600 --> 01:27:27.040]   We thank you for their support.
[01:27:27.040 --> 01:27:28.040]   Okay.
[01:27:28.040 --> 01:27:33.640]   Boy, we've really covered everything from God to artificial intelligence to the iPhone
[01:27:33.640 --> 01:27:34.640]   face.
[01:27:34.640 --> 01:27:37.200]   Now let's just take over the universe.
[01:27:37.200 --> 01:27:38.640]   Where should we go with this?
[01:27:38.640 --> 01:27:39.640]   Amazon.
[01:27:39.640 --> 01:27:42.440]   We haven't talked much about Amazon.
[01:27:42.440 --> 01:27:46.800]   Anything to say about them, their cashierless store is about to go nationwide.
[01:27:46.800 --> 01:27:48.560]   They started to pop up stores.
[01:27:48.560 --> 01:27:52.960]   I was about to talk about this actually at the whole foods.
[01:27:52.960 --> 01:27:56.520]   You go into Whole Foods now in a lot of places and you can buy kindles and stuff.
[01:27:56.520 --> 01:28:00.400]   I'm just expecting that they don't bugger up a load of the rings too badly.
[01:28:00.400 --> 01:28:02.600]   Oh yeah, they have the right to.
[01:28:02.600 --> 01:28:05.000]   And it's just you can just see this coming.
[01:28:05.000 --> 01:28:07.720]   It's $250 million.
[01:28:07.720 --> 01:28:13.080]   When you spend that in a quarter of a billion dollars, merely for the rights, it's going
[01:28:13.080 --> 01:28:14.080]   to be great.
[01:28:14.080 --> 01:28:15.080]   I agree with you.
[01:28:15.080 --> 01:28:16.080]   I agree with you.
[01:28:16.080 --> 01:28:17.080]   How could they muck it up?
[01:28:17.080 --> 01:28:19.280]   The entire season of Tom Bombadil's backstory.
[01:28:19.280 --> 01:28:20.280]   Yeah.
[01:28:20.280 --> 01:28:21.280]   The greatest thing ever.
[01:28:21.280 --> 01:28:23.000]   What would be better than that?
[01:28:23.000 --> 01:28:26.440]   Tom Bombadil goes to Amazon's cashierless store.
[01:28:26.440 --> 01:28:27.440]   That's going to be great.
[01:28:27.440 --> 01:28:30.440]   Tom Bombadil has been neglected by every major adaptation.
[01:28:30.440 --> 01:28:31.440]   That's true.
[01:28:31.440 --> 01:28:33.200]   He does not appear in any of the movies.
[01:28:33.200 --> 01:28:34.200]   Yeah.
[01:28:34.200 --> 01:28:36.880]   Well, he's one of the most disturbing characters in there.
[01:28:36.880 --> 01:28:41.240]   All I remember is him singing endlessly over and over again his little Tom Bombadil
[01:28:41.240 --> 01:28:42.240]   song.
[01:28:42.240 --> 01:28:43.240]   It's a disturbing.
[01:28:43.240 --> 01:28:44.240]   Great.
[01:28:44.240 --> 01:28:47.400]   Not all Tom Bombadil all the time.
[01:28:47.400 --> 01:28:49.720]   Tom Bombadil the real story.
[01:28:49.720 --> 01:28:50.720]   The next season.
[01:28:50.720 --> 01:28:52.640]   They're going to do prequels.
[01:28:52.640 --> 01:28:56.760]   It will not, I presume it's not going to start with The Hobbit.
[01:28:56.760 --> 01:28:58.440]   What did they buy the rights to specifically?
[01:28:58.440 --> 01:29:02.160]   Because this was always like a point of contention with the Tolkien estate.
[01:29:02.160 --> 01:29:04.320]   The Hobbit is not the right of the Tolkien estate.
[01:29:04.320 --> 01:29:05.320]   No, but it's just resilient.
[01:29:05.320 --> 01:29:09.000]   Do they have the rights to the, to like the simmarillion and all of that, all of the
[01:29:09.000 --> 01:29:10.000]   lore?
[01:29:10.000 --> 01:29:11.000]   I hope so.
[01:29:11.000 --> 01:29:12.000]   I don't want to see the simmarillion.
[01:29:12.000 --> 01:29:13.000]   No, no, but that's the reason why he's down point and story.
[01:29:13.000 --> 01:29:15.600]   If you're going to get stuff, if you're going to get a series out of it.
[01:29:15.600 --> 01:29:16.600]   Peter Jackson never had it, right?
[01:29:16.600 --> 01:29:21.240]   So he only had like Lord of the Rings and the Hobbit and it wasn't allowed to actually
[01:29:21.240 --> 01:29:22.440]   go back with the lore.
[01:29:22.440 --> 01:29:23.440]   Right.
[01:29:23.440 --> 01:29:27.600]   Set in Middle Earth, the adaptation produced by Amazon Studios will explore new storylines
[01:29:27.600 --> 01:29:29.960]   preceding the fellowship of the ring.
[01:29:29.960 --> 01:29:33.960]   Yes, but hopefully that means they bought the rights to that, not just like making, you
[01:29:33.960 --> 01:29:34.960]   know, the adaptation.
[01:29:34.960 --> 01:29:35.960]   Because it's a nice proceeding to the fellowship of the ring.
[01:29:35.960 --> 01:29:39.880]   It could be between The Hobbit and Sharon.
[01:29:39.880 --> 01:29:42.840]   It was fascinating years when Frodo sat at home.
[01:29:42.840 --> 01:29:43.840]   Exactly.
[01:29:43.840 --> 01:29:44.840]   You can find out what else is going on.
[01:29:44.840 --> 01:29:48.560]   Sharon Talleguato, who's the head of scripted series for Amazon Studios said, quote, "We
[01:29:48.560 --> 01:29:53.720]   are honored to be working with the Tolkien estate and trust Harper Collins and New Line."
[01:29:53.720 --> 01:29:54.720]   It's great.
[01:29:54.720 --> 01:29:57.320]   New Line is a movie making enterprise, right?
[01:29:57.320 --> 01:30:01.920]   Harper Collins is the American publisher at, of course, the Tolkien estate on this exciting
[01:30:01.920 --> 01:30:05.280]   collaboration for television and are thrilled to be taking the Lord of the Rings fan on
[01:30:05.280 --> 01:30:06.960]   a new epic journey in Middle Earth.
[01:30:06.960 --> 01:30:09.680]   You know, that's a really good question.
[01:30:09.680 --> 01:30:12.200]   Is it going to be existing content?
[01:30:12.200 --> 01:30:13.200]   Because there is.
[01:30:13.200 --> 01:30:14.200]   There's a similar line.
[01:30:14.200 --> 01:30:15.200]   There's a ton of it, right?
[01:30:15.200 --> 01:30:16.200]   Yeah.
[01:30:16.200 --> 01:30:17.200]   Or is it going to be some new world in time?
[01:30:17.200 --> 01:30:18.200]   It could do a lot with that.
[01:30:18.200 --> 01:30:21.640]   But if you look what they've done to Game of Thrones ever since they lost the plot with
[01:30:21.640 --> 01:30:24.600]   the books, it's just ruined the series.
[01:30:24.600 --> 01:30:25.600]   So I worry about it.
[01:30:25.600 --> 01:30:27.600]   Somebody's going to say, "Come on, Game of Thrones."
[01:30:27.600 --> 01:30:30.120]   Well, I think it's going to highlight the difficulty of prequels.
[01:30:30.120 --> 01:30:32.120]   Look at Star Wars with the difficulty of prequels.
[01:30:32.120 --> 01:30:33.120]   Yeah.
[01:30:33.120 --> 01:30:34.920]   That's not only because George Lucas was involved.
[01:30:34.920 --> 01:30:35.920]   Well, also with the...
[01:30:35.920 --> 01:30:39.520]   What I'm saying is that, you know, since the fellowship of the ring starts off with
[01:30:39.520 --> 01:30:44.040]   this idea that the Middle Earth has had this fall from grace and the whole point is to
[01:30:44.040 --> 01:30:48.600]   restore all this stuff that has fallen into dark and despair and so on and so forth.
[01:30:48.600 --> 01:30:52.520]   If you're going to ever prequel, are you telling the story of everything sliding into the point
[01:30:52.520 --> 01:30:54.400]   where the fellowship of the ring is necessary?
[01:30:54.400 --> 01:30:56.640]   Or are you just kind of like, that's all background noise?
[01:30:56.640 --> 01:31:00.360]   Or it could be like a soap opera set in the Hobbits.
[01:31:00.360 --> 01:31:02.400]   You have to be way before Hobbits, right?
[01:31:02.400 --> 01:31:04.360]   Because this is all like third age.
[01:31:04.360 --> 01:31:06.360]   This is pretty intense, nerdy stuff here.
[01:31:06.360 --> 01:31:08.840]   But it sounds like if they say prequel of the fellowship of the ring, it sounds like it's
[01:31:08.840 --> 01:31:12.160]   after the Hobbit before the Lord of the Rings.
[01:31:12.160 --> 01:31:14.000]   That's as Sauron as like assembling himself.
[01:31:14.000 --> 01:31:15.000]   Oh yeah.
[01:31:15.000 --> 01:31:16.000]   So you have Bill Wilson and storyline.
[01:31:16.000 --> 01:31:18.000]   The whole Sauron storyline, you could go on and on with that.
[01:31:18.000 --> 01:31:19.000]   It could basically be like...
[01:31:19.000 --> 01:31:20.000]   Gandalf is off doing stuff.
[01:31:20.000 --> 01:31:21.000]   Yeah.
[01:31:21.000 --> 01:31:22.000]   What's Gandalf been doing, right?
[01:31:22.000 --> 01:31:23.000]   That's a good one.
[01:31:23.000 --> 01:31:24.000]   He disappeared, right?
[01:31:24.000 --> 01:31:25.000]   Yeah.
[01:31:25.000 --> 01:31:26.640]   He was in the tower, the white wizard for a while.
[01:31:26.640 --> 01:31:27.640]   What's going on?
[01:31:27.640 --> 01:31:28.640]   Well, that's after.
[01:31:28.640 --> 01:31:29.640]   But yeah.
[01:31:29.640 --> 01:31:30.640]   That's after.
[01:31:30.640 --> 01:31:31.640]   Gandalf's been coming back once in a while to get the finest...
[01:31:31.640 --> 01:31:32.640]   He goes off.
[01:31:32.640 --> 01:31:33.640]   Yeah.
[01:31:33.640 --> 01:31:34.640]   But yeah, but otherwise he goes off.
[01:31:34.640 --> 01:31:35.640]   So we don't know what he's up to.
[01:31:35.640 --> 01:31:36.960]   I'm super excited by this.
[01:31:36.960 --> 01:31:38.960]   He was hanging with Jesus in the dinosaurs.
[01:31:38.960 --> 01:31:42.300]   As long as they just don't turn into titans and dragons, they have done with games and
[01:31:42.300 --> 01:31:43.300]   throw in games and throw in.
[01:31:43.300 --> 01:31:44.300]   There's nothing wrong with that.
[01:31:44.300 --> 01:31:46.300]   They're not only for a token strength though.
[01:31:46.300 --> 01:31:50.200]   Actually, the most recent album since the game of Thrones has been much toned down in
[01:31:50.200 --> 01:31:52.560]   terms of the R-rated content.
[01:31:52.560 --> 01:31:53.840]   There's not nearly the nudity.
[01:31:53.840 --> 01:31:54.840]   And I appreciated that.
[01:31:54.840 --> 01:31:59.920]   We need to do a season of Game of Thrones based entirely on stories about the Iron Bank.
[01:31:59.920 --> 01:32:00.920]   I know.
[01:32:00.920 --> 01:32:02.360]   You want to know more about the games?
[01:32:02.360 --> 01:32:04.920]   I want to know about the finance, right?
[01:32:04.920 --> 01:32:09.280]   I want all Iron Bank all the time and all Tom Bombadil.
[01:32:09.280 --> 01:32:11.800]   How is Westeros financially solvent at this point?
[01:32:11.800 --> 01:32:12.800]   Like it makes no sense.
[01:32:12.800 --> 01:32:13.800]   It's constant warfare.
[01:32:13.800 --> 01:32:16.720]   There's almost no agricultural base.
[01:32:16.720 --> 01:32:19.520]   They haven't had any industrial progression whatsoever.
[01:32:19.520 --> 01:32:20.920]   Why are they even on the basis?
[01:32:20.920 --> 01:32:24.880]   The Lannisters apparently think that mining gold is going to generate money?
[01:32:24.880 --> 01:32:25.880]   Yeah, but they're gold ones.
[01:32:25.880 --> 01:32:26.880]   That is economically...
[01:32:26.880 --> 01:32:27.880]   It makes no sense.
[01:32:27.880 --> 01:32:30.840]   What is the currency standard?
[01:32:30.840 --> 01:32:34.720]   I think the lack of serious business reporting on the Iron Bank is an outrage.
[01:32:34.720 --> 01:32:37.520]   We need to see a season.
[01:32:37.520 --> 01:32:38.520]   I hear.
[01:32:38.520 --> 01:32:40.520]   There's nothing but Iron Bank stuff.
[01:32:40.520 --> 01:32:43.920]   I think Castile Rock is going with cryptocurrency.
[01:32:43.920 --> 01:32:46.760]   I could be wrong.
[01:32:46.760 --> 01:32:50.760]   I would love to see a better Iron Bank series simply because the guy who plays it is a part
[01:32:50.760 --> 01:32:51.760]   of a very funny...
[01:32:51.760 --> 01:32:52.760]   He's great.
[01:32:52.760 --> 01:32:53.760]   Yeah.
[01:32:53.760 --> 01:32:54.760]   He's super good.
[01:32:54.760 --> 01:32:58.000]   The British Comedy Programme put this on the list, The League of Gentlemen.
[01:32:58.000 --> 01:32:59.000]   And it's...
[01:32:59.000 --> 01:33:00.760]   Well, so they're doing spin-offs, right?
[01:33:00.760 --> 01:33:02.720]   As Ryo said, they're doing spin-offs of Game of Thrones.
[01:33:02.720 --> 01:33:03.720]   They haven't seen them.
[01:33:03.720 --> 01:33:05.720]   After the spin-off is all Iron Bank.
[01:33:05.720 --> 01:33:06.720]   Yeah.
[01:33:06.720 --> 01:33:07.720]   Kevin Spacey won't be in those spin-offs.
[01:33:07.720 --> 01:33:13.080]   No, I would watch that though because he gets into espionage and it gets into policy and
[01:33:13.080 --> 01:33:14.080]   it gets into king-making.
[01:33:14.080 --> 01:33:15.080]   As long as they don't bring back in.
[01:33:15.080 --> 01:33:16.080]   He's got a little finger.
[01:33:16.080 --> 01:33:17.080]   I had enough of a little finger.
[01:33:17.080 --> 01:33:18.080]   I was so glad when they...
[01:33:18.080 --> 01:33:19.080]   Oh, never mind.
[01:33:19.080 --> 01:33:20.080]   I won't say anything.
[01:33:20.080 --> 01:33:21.080]   Yes.
[01:33:21.080 --> 01:33:22.080]   Yes.
[01:33:22.080 --> 01:33:24.520]   He was very good in the British Fishnail Quirri's vote.
[01:33:24.520 --> 01:33:25.520]   Yeah.
[01:33:25.520 --> 01:33:26.520]   And he's great enough to...
[01:33:26.520 --> 01:33:27.520]   And he's good enough.
[01:33:27.520 --> 01:33:29.320]   And Gil is very good in the wire too.
[01:33:29.320 --> 01:33:30.320]   Yeah.
[01:33:30.320 --> 01:33:31.320]   I like him as an actor, but that character is...
[01:33:31.320 --> 01:33:32.320]   Love him as an actor, but little finger.
[01:33:32.320 --> 01:33:33.320]   Oh, the character was good.
[01:33:33.320 --> 01:33:37.240]   Actually, that was a problem because they couldn't decide what to do with him.
[01:33:37.240 --> 01:33:38.240]   He went back and forth.
[01:33:38.240 --> 01:33:40.200]   I think they dealt with him very well.
[01:33:40.200 --> 01:33:44.240]   Him and Joffrey were the two big hate figures that everyone had.
[01:33:44.240 --> 01:33:47.440]   Don't say anymore and don't say anymore because they're probably somebody.
[01:33:47.440 --> 01:33:48.440]   And then when they got married.
[01:33:48.440 --> 01:33:49.440]   Yeah.
[01:33:49.440 --> 01:33:52.600]   That was really an eye-opener for all of us.
[01:33:52.600 --> 01:33:56.800]   I was, you know, it was nearly as shocking as when...
[01:33:56.800 --> 01:33:58.560]   Word Bailish and Joffrey were...
[01:33:58.560 --> 01:34:01.640]   I wasn't going to pub crawl with Joffrey in Dublin.
[01:34:01.640 --> 01:34:03.280]   I was great.
[01:34:03.280 --> 01:34:04.280]   The actor?
[01:34:04.280 --> 01:34:05.280]   The actor.
[01:34:05.280 --> 01:34:06.280]   Yeah.
[01:34:06.280 --> 01:34:07.280]   I would have been a little weird if you went with the king.
[01:34:07.280 --> 01:34:08.280]   He's busy boys.
[01:34:08.280 --> 01:34:09.280]   Really?
[01:34:09.280 --> 01:34:10.280]   Really?
[01:34:10.280 --> 01:34:11.280]   Really?
[01:34:11.280 --> 01:34:12.280]   That's interesting.
[01:34:12.280 --> 01:34:13.280]   He's given a painting now.
[01:34:13.280 --> 01:34:14.280]   Is that the case?
[01:34:14.280 --> 01:34:15.280]   He was still Joffrey at the time.
[01:34:15.280 --> 01:34:16.280]   Yeah.
[01:34:16.280 --> 01:34:17.280]   He was old enough to drink.
[01:34:17.280 --> 01:34:18.280]   Was he billed as Joffrey?
[01:34:18.280 --> 01:34:19.280]   Was it like going to pub crawl with Joffrey?
[01:34:19.280 --> 01:34:20.280]   Come on, hang on.
[01:34:20.280 --> 01:34:21.280]   You went out pub crawl with Ireland.
[01:34:21.280 --> 01:34:22.280]   He's old enough to drink.
[01:34:22.280 --> 01:34:24.280]   You mean he didn't turn up on a tricycle going, "Can I have a cup of beer please?"
[01:34:24.280 --> 01:34:25.280]   He may.
[01:34:25.280 --> 01:34:26.280]   I didn't see how he got there.
[01:34:26.280 --> 01:34:29.280]   I really think he should be friend of the actor who played Drake on Malfoy and they should
[01:34:29.280 --> 01:34:30.280]   just go around.
[01:34:30.280 --> 01:34:31.280]   Just go around.
[01:34:31.280 --> 01:34:32.280]   Just go around.
[01:34:32.280 --> 01:34:33.280]   He's a big fan of the movie.
[01:34:33.280 --> 01:34:34.280]   He's a big fan of the film.
[01:34:34.280 --> 01:34:35.280]   He's a big fan of the movie.
[01:34:35.280 --> 01:34:36.280]   He's a big fan of the film.
[01:34:36.280 --> 01:34:37.280]   He's a big fan of the movie.
[01:34:37.280 --> 01:34:38.280]   He's a big fan of the movie.
[01:34:38.280 --> 01:34:39.280]   He's a big fan of the film.
[01:34:39.280 --> 01:34:40.280]   He's a big fan of the film.
[01:34:40.280 --> 01:34:41.280]   He's a big fan of the movie.
[01:34:41.280 --> 01:34:42.280]   He's a big fan of the film.
[01:34:42.280 --> 01:34:43.280]   He's a big fan of the film.
[01:34:43.280 --> 01:34:44.280]   He's a big fan of the film.
[01:34:44.280 --> 01:34:45.280]   He's a big fan of the film.
[01:34:45.280 --> 01:34:46.280]   He's a big fan of the film.
[01:34:46.280 --> 01:34:47.280]   He's a big fan of the film.
[01:34:47.280 --> 01:34:48.280]   He's a big fan of the film.
[01:34:48.280 --> 01:34:49.280]   He's a big fan of the film.
[01:34:49.280 --> 01:34:50.280]   He's a big fan of the film.
[01:34:50.280 --> 01:34:51.280]   He's a big fan of the film.
[01:34:51.280 --> 01:34:58.280]   He's a big fan of the film.
[01:34:58.280 --> 01:35:05.280]   He's a big fan of the film.
[01:35:05.280 --> 01:35:12.280]   He's a big fan of the film.
[01:35:12.280 --> 01:35:19.280]   He's a big fan of the film.
[01:35:19.280 --> 01:35:26.280]   He's a big fan of the film.
[01:35:26.280 --> 01:35:33.280]   He's a big fan of the film.
[01:35:33.280 --> 01:35:40.280]   He's a big fan of the film.
[01:35:40.280 --> 01:35:47.280]   He's a big fan of the film.
[01:35:47.280 --> 01:35:54.280]   He's a big fan of the film.
[01:35:54.280 --> 01:36:01.280]   He's a big fan of the film.
[01:36:01.280 --> 01:36:08.280]   He's a big fan of the film.
[01:36:08.280 --> 01:36:15.280]   He's a big fan of the film.
[01:36:15.280 --> 01:36:22.280]   He's a big fan of the film.
[01:36:22.280 --> 01:36:29.280]   He's a big fan of the film.
[01:36:29.280 --> 01:36:36.280]   He's a big fan of the film.
[01:36:36.280 --> 01:36:43.280]   He's a big fan of the film.
[01:36:43.280 --> 01:36:53.280]   He's a big fan of the film.
[01:36:53.280 --> 01:36:58.280]   He's a big fan of the film.
[01:36:58.280 --> 01:37:03.280]   He's a big fan of the film.
[01:37:03.280 --> 01:37:10.280]   He's a big fan of the film.
[01:37:10.280 --> 01:37:20.280]   He's a big fan of the film.
[01:37:20.280 --> 01:37:25.280]   He's a big fan of the film.
[01:37:25.280 --> 01:37:30.280]   He's a big fan of the film.
[01:37:30.280 --> 01:37:37.280]   He's a big fan of the film.
[01:37:37.280 --> 01:37:47.280]   He's a big fan of the film.
[01:37:47.280 --> 01:37:52.280]   He's a big fan of the film.
[01:37:52.280 --> 01:37:57.280]   He's a big fan of the film.
[01:37:57.280 --> 01:38:04.280]   He's a big fan of the film.
[01:38:04.280 --> 01:38:14.280]   He's a big fan of the film.
[01:38:14.280 --> 01:38:19.280]   He's a big fan of the film.
[01:38:19.280 --> 01:38:24.280]   He's a big fan of the film.
[01:38:24.280 --> 01:38:31.280]   He's a big fan of the film.
[01:38:31.280 --> 01:38:41.280]   He's a big fan of the film.
[01:38:41.280 --> 01:38:46.280]   He's a big fan of the film.
[01:38:46.280 --> 01:38:51.280]   He's a big fan of the film.
[01:38:51.280 --> 01:38:58.280]   He's a big fan of the film.
[01:38:58.280 --> 01:39:08.280]   He's a big fan of the film.
[01:39:08.280 --> 01:39:13.280]   He's a big fan of the film.
[01:39:13.280 --> 01:39:18.280]   He's a big fan of the film.
[01:39:18.280 --> 01:39:23.280]   He's a big fan of the film.
[01:39:23.280 --> 01:39:33.280]   He's a big fan of the film.
[01:39:33.280 --> 01:39:38.280]   He's a big fan of the film.
[01:39:38.280 --> 01:39:43.280]   He's a big fan of the film.
[01:39:43.280 --> 01:39:50.280]   He's a big fan of the film.
[01:39:50.280 --> 01:40:00.280]   He's a big fan of the film.
[01:40:00.280 --> 01:40:05.280]   He's a big fan of the film.
[01:40:05.280 --> 01:40:10.280]   He's a big fan of the film.
[01:40:10.280 --> 01:40:15.280]   He's a big fan of the film.
[01:40:15.280 --> 01:40:30.280]   He's a big fan of the film.
[01:40:30.280 --> 01:40:35.280]   He's a big fan of the film.
[01:40:35.280 --> 01:40:42.280]   He's a big fan of the film.
[01:40:42.280 --> 01:40:52.280]   He's a big fan of the film.
[01:40:52.280 --> 01:40:57.280]   He's a big fan of the film.
[01:40:57.280 --> 01:41:02.280]   He's a big fan of the film.
[01:41:02.280 --> 01:41:07.280]   He's a big fan of the film.
[01:41:07.280 --> 01:41:17.280]   He's a big fan of the film.
[01:41:17.280 --> 01:41:22.280]   He's a big fan of the film.
[01:41:22.280 --> 01:41:27.280]   He's a big fan of the film.
[01:41:27.280 --> 01:41:32.280]   He's a big fan of the film.
[01:41:32.280 --> 01:41:42.280]   He's a big fan of the film.
[01:41:42.280 --> 01:41:47.280]   He's a big fan of the film.
[01:41:47.280 --> 01:41:52.280]   He's a big fan of the film.
[01:41:52.280 --> 01:41:59.280]   He's a big fan of the film.
[01:41:59.280 --> 01:42:09.280]   He's a big fan of the film.
[01:42:09.280 --> 01:42:14.280]   He's a big fan of the film.
[01:42:14.280 --> 01:42:19.280]   He's a big fan of the film.
[01:42:19.280 --> 01:42:24.280]   He's a big fan of the film.
[01:42:24.280 --> 01:42:34.280]   He's a big fan of the film.
[01:42:34.280 --> 01:42:39.280]   He's a big fan of the film.
[01:42:39.280 --> 01:42:44.280]   He's a big fan of the film.
[01:42:44.280 --> 01:42:49.280]   He's a big fan of the film.
[01:42:49.280 --> 01:42:59.280]   He's a big fan of the film.
[01:42:59.280 --> 01:43:04.280]   He's a big fan of the film.
[01:43:04.280 --> 01:43:09.280]   He's a big fan of the film.
[01:43:09.280 --> 01:43:14.280]   He's a big fan of the film.
[01:43:14.280 --> 01:43:24.280]   He's a big fan of the film.
[01:43:24.280 --> 01:43:29.280]   He's a big fan of the film.
[01:43:29.280 --> 01:43:34.280]   He's a big fan of the film.
[01:43:34.280 --> 01:43:39.280]   He's a big fan of the film.
[01:43:39.280 --> 01:43:54.280]   He's a big fan of the film.
[01:43:54.280 --> 01:43:59.280]   He's a big fan of the film.
[01:43:59.280 --> 01:44:04.280]   He's a big fan of the film.
[01:44:04.280 --> 01:44:14.280]   He's a big fan of the film.
[01:44:14.280 --> 01:44:19.280]   He's a big fan of the film.
[01:44:19.280 --> 01:44:24.280]   He's a big fan of the film.
[01:44:24.280 --> 01:44:33.280]   He's a big fan of the film.
[01:44:33.280 --> 01:44:43.280]   He's a big fan of the film.
[01:44:43.280 --> 01:44:48.280]   He's a big fan of the film.
[01:44:48.280 --> 01:44:53.280]   He's a big fan of the film.
[01:44:53.280 --> 01:45:00.280]   He's a big fan of the film.
[01:45:00.280 --> 01:45:10.280]   He's a big fan of the film.
[01:45:10.280 --> 01:45:15.280]   He's a big fan of the film.
[01:45:15.280 --> 01:45:20.280]   He's a big fan of the film.
[01:45:20.280 --> 01:45:27.280]   He's a big fan of the film.
[01:45:27.280 --> 01:45:37.280]   He's a big fan of the film.
[01:45:37.280 --> 01:45:42.280]   He's a big fan of the film.
[01:45:42.280 --> 01:45:47.280]   He's a big fan of the film.
[01:45:47.280 --> 01:45:54.280]   He's a big fan of the film.
[01:45:54.280 --> 01:46:04.280]   He's a big fan of the film.
[01:46:04.280 --> 01:46:09.280]   He's a big fan of the film.
[01:46:09.280 --> 01:46:14.280]   He's a big fan of the film.
[01:46:14.280 --> 01:46:19.280]   He's a big fan of the film.
[01:46:19.280 --> 01:46:29.280]   He's a big fan of the film.
[01:46:29.280 --> 01:46:34.280]   He's a big fan of the film.
[01:46:34.280 --> 01:46:39.280]   He's a big fan of the film.
[01:46:39.280 --> 01:46:42.280]   He's a big fan of the film.
[01:46:42.280 --> 01:46:47.280]   He's a big fan of the film.
[01:46:47.280 --> 01:46:57.280]   He's a big fan of the film.
[01:46:57.280 --> 01:47:02.280]   He's a big fan of the film.
[01:47:02.280 --> 01:47:07.280]   He's a big fan of the film.
[01:47:07.280 --> 01:47:12.280]   He's a big fan of the film.
[01:47:12.280 --> 01:47:22.280]   He's a big fan of the film.
[01:47:22.280 --> 01:47:27.280]   He's a big fan of the film.
[01:47:27.280 --> 01:47:32.280]   He's a big fan of the film.
[01:47:32.280 --> 01:47:37.280]   He's a big fan of the film.
[01:47:37.280 --> 01:47:47.280]   He's a big fan of the film.
[01:47:47.280 --> 01:47:52.280]   He's a big fan of the film.
[01:47:52.280 --> 01:47:57.280]   He's a big fan of the film.
[01:47:57.280 --> 01:48:04.280]   He's a big fan of the film.
[01:48:04.280 --> 01:48:14.280]   He's a big fan of the film.
[01:48:14.280 --> 01:48:19.280]   He's a big fan of the film.
[01:48:19.280 --> 01:48:24.280]   He's a big fan of the film.
[01:48:24.280 --> 01:48:29.280]   He's a big fan of the film.
[01:48:29.280 --> 01:48:39.280]   He's a big fan of the film.
[01:48:39.280 --> 01:48:44.280]   He's a big fan of the film.
[01:48:44.280 --> 01:48:49.280]   He's a big fan of the film.
[01:48:49.280 --> 01:48:56.280]   He's a big fan of the film.
[01:48:56.280 --> 01:49:06.280]   He's a big fan of the film.
[01:49:06.280 --> 01:49:11.280]   He's a big fan of the film.
[01:49:11.280 --> 01:49:16.280]   He's a big fan of the film.
[01:49:16.280 --> 01:49:23.280]   He's a big fan of the film.
[01:49:23.280 --> 01:49:33.280]   He's a big fan of the film.
[01:49:33.280 --> 01:49:38.280]   He's a big fan of the film.
[01:49:38.280 --> 01:49:43.280]   He's a big fan of the film.
[01:49:43.280 --> 01:49:50.280]   He's a big fan of the film.
[01:49:50.280 --> 01:50:00.280]   He's a big fan of the film.
[01:50:00.280 --> 01:50:05.280]   He's a big fan of the film.
[01:50:05.280 --> 01:50:10.280]   He's a big fan of the film.
[01:50:10.280 --> 01:50:17.280]   He's a big fan of the film.
[01:50:17.280 --> 01:50:27.280]   He's a big fan of the film.
[01:50:27.280 --> 01:50:32.280]   He's a big fan of the film.
[01:50:32.280 --> 01:50:37.280]   He's a big fan of the film.
[01:50:37.280 --> 01:50:42.280]   He's a big fan of the film.
[01:50:42.280 --> 01:50:52.280]   He's a big fan of the film.
[01:50:52.280 --> 01:50:57.280]   He's a big fan of the film.
[01:50:57.280 --> 01:51:02.280]   He's a big fan of the film.
[01:51:02.280 --> 01:51:07.280]   He's a big fan of the film.
[01:51:07.280 --> 01:51:22.280]   He's a big fan of the film.
[01:51:22.280 --> 01:51:27.280]   He's a big fan of the film.
[01:51:27.280 --> 01:51:34.280]   He's a big fan of the film.
[01:51:34.280 --> 01:51:44.280]   He's a big fan of the film.
[01:51:44.280 --> 01:51:49.280]   He's a big fan of the film.
[01:51:49.280 --> 01:51:54.280]   He's a big fan of the film.
[01:51:54.280 --> 01:51:59.280]   He's a big fan of the film.
[01:51:59.280 --> 01:52:09.280]   He's a big fan of the film.
[01:52:09.280 --> 01:52:14.280]   He's a big fan of the film.
[01:52:14.280 --> 01:52:19.280]   He's a big fan of the film.
[01:52:19.280 --> 01:52:24.280]   He's a big fan of the film.
[01:52:24.280 --> 01:52:34.280]   He's a big fan of the film.
[01:52:34.280 --> 01:52:39.280]   He's a big fan of the film.
[01:52:39.280 --> 01:52:44.280]   He's a big fan of the film.
[01:52:44.280 --> 01:52:49.280]   He's a big fan of the film.
[01:52:49.280 --> 01:53:04.280]   He's a big fan of the film.
[01:53:04.280 --> 01:53:09.280]   He's a big fan of the film.
[01:53:09.280 --> 01:53:14.280]   He's a big fan of the film.
[01:53:14.280 --> 01:53:24.280]   He's a big fan of the film.
[01:53:24.280 --> 01:53:29.280]   He's a big fan of the film.
[01:53:29.280 --> 01:53:34.280]   He's a big fan of the film.
[01:53:34.280 --> 01:53:39.280]   He's a big fan of the film.
[01:53:39.280 --> 01:53:49.280]   He's a big fan of the film.
[01:53:49.280 --> 01:53:54.280]   He's a big fan of the film.
[01:53:54.280 --> 01:53:59.280]   He's a big fan of the film.
[01:53:59.280 --> 01:54:04.280]   He's a big fan of the film.
[01:54:04.280 --> 01:54:19.280]   He's a big fan of the film.
[01:54:19.280 --> 01:54:24.280]   He's a big fan of the film.
[01:54:24.280 --> 01:54:31.280]   He's a big fan of the film.
[01:54:31.280 --> 01:54:46.280]   He's a big fan of the film.
[01:54:46.280 --> 01:54:51.280]   He's a big fan of the film.
[01:54:51.280 --> 01:54:56.280]   He's a big fan of the film.
[01:54:56.280 --> 01:55:11.280]   He's a big fan of the film.
[01:55:11.280 --> 01:55:16.280]   He's a big fan of the film.
[01:55:16.280 --> 01:55:21.280]   He's a big fan of the film.
[01:55:21.280 --> 01:55:31.280]   He's a big fan of the film.
[01:55:31.280 --> 01:55:36.280]   He's a big fan of the film.
[01:55:36.280 --> 01:55:41.280]   He's a big fan of the film.
[01:55:41.280 --> 01:55:46.280]   He's a big fan of the film.
[01:55:46.280 --> 01:56:01.280]   He's a big fan of the film.
[01:56:01.280 --> 01:56:06.280]   He's a big fan of the film.
[01:56:06.280 --> 01:56:13.280]   He's a big fan of the film.
[01:56:13.280 --> 01:56:23.280]   He's a big fan of the film.
[01:56:23.280 --> 01:56:28.280]   He's a big fan of the film.
[01:56:28.280 --> 01:56:33.280]   He's a big fan of the film.
[01:56:33.280 --> 01:56:40.280]   He's a big fan of the film.
[01:56:40.280 --> 01:56:47.280]   He's a CEO of that.
[01:56:47.280 --> 01:56:53.280]   I never get it right.
[01:56:53.280 --> 01:57:08.280]   He's a big fan of the film.
[01:57:08.280 --> 01:57:13.280]   He's a big fan of the film.
[01:57:13.280 --> 01:57:20.280]   He's a big fan of the film.
[01:57:20.280 --> 01:57:35.280]   He's a big fan of the film.
[01:57:35.280 --> 01:57:40.280]   He's a big fan of the film.
[01:57:40.280 --> 01:57:47.280]   He's a big fan of the film.
[01:57:47.280 --> 01:58:02.280]   He's a big fan of the film.
[01:58:02.280 --> 01:58:22.280]   He's a big fan of the film.
[01:58:22.280 --> 01:58:42.280]   He's a big fan of the film.
[01:58:42.280 --> 01:59:02.280]   He's a big fan of the film.
[01:59:02.280 --> 01:59:22.280]   He's a big fan of the film.
[01:59:22.280 --> 01:59:42.280]   He's a big fan of the film.
[01:59:42.280 --> 02:00:02.280]   He's a big fan of the film.
[02:00:02.280 --> 02:00:22.280]   He's a big fan of the film.
[02:00:22.280 --> 02:00:42.280]   He's a big fan of the film.
[02:00:42.280 --> 02:01:02.280]   He's a big fan of the film.
[02:01:02.280 --> 02:01:22.280]   He's a big fan of the film.
[02:01:22.280 --> 02:01:42.280]   He's a big fan of the film.
[02:01:42.280 --> 02:02:02.280]   He's a big fan of the film.
[02:02:02.280 --> 02:02:22.280]   He's a big fan of the film.
[02:02:22.280 --> 02:02:42.280]   He's a big fan of the film.
[02:02:42.280 --> 02:03:02.280]   He's a big fan of the film.
[02:03:02.280 --> 02:03:22.280]   He's a big fan of the film.
[02:03:22.280 --> 02:03:42.280]   He's a big fan of the film.
[02:03:42.280 --> 02:04:02.280]   He's a big fan of the film.
[02:04:02.280 --> 02:04:22.280]   He's a big fan of the film.
[02:04:22.280 --> 02:04:42.280]   He's a big fan of the film.
[02:04:42.280 --> 02:05:02.280]   He's a big fan of the film.
[02:05:02.280 --> 02:05:22.280]   He's a big fan of the film.
[02:05:22.280 --> 02:05:42.280]   He's a big fan of the film.
[02:05:42.280 --> 02:06:02.280]   He's a big fan of the film.
[02:06:02.280 --> 02:06:22.280]   He's a big fan of the film.
[02:06:22.280 --> 02:06:42.280]   He's a big fan of the film.
[02:06:42.280 --> 02:07:02.280]   He's a big fan of the film.
[02:07:02.280 --> 02:07:22.280]   He's a big fan of the film.
[02:07:22.280 --> 02:07:42.280]   He's a big fan of the film.
[02:07:42.280 --> 02:08:02.280]   He's a big fan of the film.
[02:08:02.280 --> 02:08:22.280]   He's a big fan of the film.
[02:08:22.280 --> 02:08:42.280]   He's a big fan of the film.

