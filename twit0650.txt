
[00:00:00.000 --> 00:00:01.180]   (upbeat music)
[00:00:01.180 --> 00:00:02.480]   It's time for "Twent" this week in tech.
[00:00:02.480 --> 00:00:03.340]   When we got a great show,
[00:00:03.340 --> 00:00:05.720]   Lindsey Turretine, editor-in-chief of "C-Net" is here,
[00:00:05.720 --> 00:00:07.640]   Tim Stevens from "Roadshow,"
[00:00:07.640 --> 00:00:09.920]   Georgia Dow from "I-More,"
[00:00:09.920 --> 00:00:14.160]   and we're gonna go head-to-head toe-to-toe on privacy.
[00:00:14.160 --> 00:00:16.320]   I'll give you a tease, a little hint.
[00:00:16.320 --> 00:00:18.720]   I'm gonna get knocked out badly.
[00:00:18.720 --> 00:00:21.280]   We'll also talk about big stories from CES
[00:00:21.280 --> 00:00:23.960]   and the Detroit auto show self-driving cars,
[00:00:23.960 --> 00:00:27.240]   electric vehicles, creepy robots,
[00:00:27.240 --> 00:00:30.400]   home pods, yellow nails, ice racing.
[00:00:30.400 --> 00:00:31.960]   It's all next!
[00:00:31.960 --> 00:00:32.800]   A twit.
[00:00:32.800 --> 00:00:37.880]   Net-cast, you love.
[00:00:37.880 --> 00:00:39.360]   From people you trust.
[00:00:39.360 --> 00:00:45.400]   This is "Twit."
[00:00:45.400 --> 00:00:48.680]   Bandwidth for this week in tech is provided by cashfly
[00:00:48.680 --> 00:00:51.920]   at cachefly.com.
[00:00:57.080 --> 00:00:58.280]   This is "Twit."
[00:00:58.280 --> 00:01:00.960]   This week in tech, episode 650,
[00:01:00.960 --> 00:01:05.120]   recorded Sunday, January 21st, 2018.
[00:01:05.120 --> 00:01:07.280]   Frumpy Rump.
[00:01:07.280 --> 00:01:09.960]   This week in tech is brought to you by Qwip.
[00:01:09.960 --> 00:01:13.360]   Make a fresh start this year with a Qwip electric toothbrush.
[00:01:13.360 --> 00:01:15.560]   It cleans like a premium electric brush
[00:01:15.560 --> 00:01:17.120]   at a fraction of the cost.
[00:01:17.120 --> 00:01:21.600]   Visit getquip.com/twit to get your first refill pack free
[00:01:21.600 --> 00:01:24.680]   when you purchase any Qwip electric toothbrush.
[00:01:24.680 --> 00:01:26.920]   And by zipper-cruder,
[00:01:26.920 --> 00:01:29.040]   looking for your next great hire zipper-cruder
[00:01:29.040 --> 00:01:32.440]   offers simple tools and powerful matching technologies
[00:01:32.440 --> 00:01:36.040]   to find you qualified candidates efficiently and effectively.
[00:01:36.040 --> 00:01:37.840]   It's the smartest way to hire.
[00:01:37.840 --> 00:01:41.440]   Try it free at zipper-cruder.com/twit.
[00:01:41.440 --> 00:01:44.360]   And by rocket mortgage, from quick and loans,
[00:01:44.360 --> 00:01:46.040]   home plays a big role in your life.
[00:01:46.040 --> 00:01:48.640]   That's why quick and loans created rocket mortgage.
[00:01:48.640 --> 00:01:50.560]   It lets you apply simply and understand
[00:01:50.560 --> 00:01:52.440]   the entire mortgage process fully
[00:01:52.440 --> 00:01:54.120]   so you can be confident you're getting
[00:01:54.120 --> 00:01:55.280]   the right mortgage for you.
[00:01:55.280 --> 00:01:59.360]   Get started at rocket mortgage.com/twit2.
[00:01:59.360 --> 00:02:02.400]   And by tracker, a coin-sized tracking device
[00:02:02.400 --> 00:02:03.880]   that pairs with your smartphone
[00:02:03.880 --> 00:02:06.640]   and keeps you from losing your most valued possessions.
[00:02:06.640 --> 00:02:11.000]   Visit the tracker.com/twit to save 20% off any order.
[00:02:11.000 --> 00:02:15.240]   It's time for Twit this week in tech.
[00:02:15.240 --> 00:02:18.080]   This show we cover the week's tech news.
[00:02:18.080 --> 00:02:20.760]   We got kind of an almost all CBS panels.
[00:02:20.760 --> 00:02:23.440]   It's CBS and Canada together.
[00:02:23.440 --> 00:02:25.320]   It's a great combination.
[00:02:25.320 --> 00:02:26.440]   It's a great combination.
[00:02:26.440 --> 00:02:27.480]   Lindsay Turrentine is here.
[00:02:27.480 --> 00:02:30.720]   Haven't had her on the show in so long and I apologize.
[00:02:30.720 --> 00:02:33.680]   I'm thrilled to have you, editor-in-chief at cnet.com
[00:02:33.680 --> 00:02:34.600]   in studio.
[00:02:34.600 --> 00:02:35.440]   Thank you.
[00:02:35.440 --> 00:02:36.520]   I love being here.
[00:02:36.520 --> 00:02:37.880]   And not sick.
[00:02:37.880 --> 00:02:39.040]   Not sick.
[00:02:39.040 --> 00:02:40.800]   Easy drive.
[00:02:40.800 --> 00:02:42.360]   Wow, this is great.
[00:02:42.360 --> 00:02:46.160]   We'll talk about CES, absolutely, in just a little bit.
[00:02:46.160 --> 00:02:47.840]   But we have other things to talk about.
[00:02:47.840 --> 00:02:51.280]   Tim Stevens is here also editor at large at Roadshow,
[00:02:51.280 --> 00:02:53.040]   which is of course on cnet.com.
[00:02:53.040 --> 00:02:54.040]   Hi, Tim.
[00:02:54.040 --> 00:02:54.920]   How's it going, Leo?
[00:02:54.920 --> 00:02:56.480]   I could tell by the badges behind you,
[00:02:56.480 --> 00:02:59.000]   you've been to a few CES's.
[00:02:59.000 --> 00:03:01.200]   Yeah, I've had a few badges just the past couple of weeks,
[00:03:01.200 --> 00:03:02.200]   as a matter of fact.
[00:03:02.200 --> 00:03:04.200]   And then the Detroit Auto Show.
[00:03:04.200 --> 00:03:04.760]   That's right.
[00:03:04.760 --> 00:03:06.600]   Yeah, I just got back from there the other day.
[00:03:06.600 --> 00:03:08.880]   And then, yeah, it's been a long couple of weeks,
[00:03:08.880 --> 00:03:10.800]   but I'm glad we're going for a couple of days.
[00:03:10.800 --> 00:03:11.360]   Welcome back.
[00:03:11.360 --> 00:03:11.920]   And--
[00:03:11.920 --> 00:03:12.400]   Thank you.
[00:03:12.400 --> 00:03:15.000]   We will have lots to talk about, I know.
[00:03:15.000 --> 00:03:18.560]   But before I start the conversation, let's also say hi.
[00:03:18.560 --> 00:03:20.280]   And welcome back to Georgia Dow.
[00:03:20.280 --> 00:03:23.520]   I haven't seen Georgia in the ages from imore.com.
[00:03:23.520 --> 00:03:25.000]   Our mom passed a little while ago.
[00:03:25.000 --> 00:03:27.960]   We're all our deep love and condolences.
[00:03:27.960 --> 00:03:30.760]   And we're just glad this is your first show sense, right?
[00:03:30.760 --> 00:03:32.880]   It's my first podcast period.
[00:03:32.880 --> 00:03:35.160]   So I'm rusty.
[00:03:35.160 --> 00:03:37.480]   Oh, well, no, we're thrilled to have you.
[00:03:37.480 --> 00:03:38.840]   Always, always thrilled.
[00:03:38.840 --> 00:03:43.200]   Georgia is, besides being a great gadget hound and a writer
[00:03:43.200 --> 00:03:45.320]   for iMore, a psychotherapist.
[00:03:45.320 --> 00:03:48.400]   So if any of you should feel the need at any time,
[00:03:48.400 --> 00:03:50.640]   just scream.
[00:03:50.640 --> 00:03:53.280]   No more CES.
[00:03:53.280 --> 00:03:54.200]   You feel free.
[00:03:54.200 --> 00:03:56.920]   Actually, Georgia, you're the only person beside you and me
[00:03:56.920 --> 00:03:57.760]   didn't go to CES.
[00:03:57.760 --> 00:03:58.520]   These two--
[00:03:58.520 --> 00:03:59.680]   No, I didn't go this year.
[00:03:59.680 --> 00:04:00.680]   What did we--
[00:04:00.680 --> 00:04:02.680]   [MUSIC PLAYING]
[00:04:02.680 --> 00:04:05.080]   So we talked about it last week, of course, a little bit,
[00:04:05.080 --> 00:04:06.520]   as well.
[00:04:06.520 --> 00:04:09.400]   It struck me that I didn't miss anything, but am I wrong?
[00:04:09.400 --> 00:04:11.080]   You missed a blackout.
[00:04:11.080 --> 00:04:13.680]   Besides that.
[00:04:13.680 --> 00:04:15.640]   That is the first time I can think of the CES
[00:04:15.640 --> 00:04:16.480]   suffered a blackout.
[00:04:16.480 --> 00:04:16.960]   That's it.
[00:04:16.960 --> 00:04:19.640]   And I have to say, and this is going to sound self-congratulatory.
[00:04:19.640 --> 00:04:20.280]   It's not really.
[00:04:20.280 --> 00:04:21.120]   It was just exciting.
[00:04:21.120 --> 00:04:24.080]   I was backstage at the CNET stage when that happened.
[00:04:24.080 --> 00:04:25.200]   No, that's in the South Hall, isn't it?
[00:04:25.200 --> 00:04:26.080]   It's in the South Hall.
[00:04:26.080 --> 00:04:29.680]   And we had people out on the floor doing live hits already
[00:04:29.680 --> 00:04:31.640]   from Central Hall, where the power went out.
[00:04:31.640 --> 00:04:34.360]   So we just immediately turned into like CNN.
[00:04:34.360 --> 00:04:35.120]   Blackout Central.
[00:04:35.120 --> 00:04:35.600]   Blackout Central.
[00:04:35.600 --> 00:04:36.920]   We were growing--
[00:04:36.920 --> 00:04:38.800]   I was standing here in front of an out--
[00:04:38.800 --> 00:04:40.680]   blacked out TV set.
[00:04:40.680 --> 00:04:41.200]   Yes.
[00:04:41.200 --> 00:04:42.440]   And it was--
[00:04:42.440 --> 00:04:42.880]   But I really--
[00:04:42.880 --> 00:04:44.080]   --relatively exciting.
[00:04:44.080 --> 00:04:45.200]   That's the big hole.
[00:04:45.200 --> 00:04:51.080]   That's where the LG curvy TV portal was.
[00:04:51.080 --> 00:04:52.640]   And all these stuff just were--
[00:04:52.640 --> 00:04:52.980]   Yep.
[00:04:52.980 --> 00:04:54.320]   The whole Samsung booth went down.
[00:04:54.320 --> 00:04:57.320]   There were a few booths with their own generators.
[00:04:57.320 --> 00:04:59.200]   Like this boat that was all lit up.
[00:04:59.200 --> 00:05:00.560]   But who's going to come visit that?
[00:05:00.560 --> 00:05:01.560]   It was very strange.
[00:05:01.560 --> 00:05:02.600]   Make your way through the bitch's left.
[00:05:02.600 --> 00:05:05.720]   It was just uncanny and ironic, right, of all places.
[00:05:05.720 --> 00:05:06.840]   It is.
[00:05:06.840 --> 00:05:09.240]   It is, especially since this was the CES where the--
[00:05:09.240 --> 00:05:11.160]   for instance, showed a collared bathroom
[00:05:11.160 --> 00:05:14.240]   that without electricity is completely useless.
[00:05:14.240 --> 00:05:14.760]   Correct.
[00:05:14.760 --> 00:05:15.560]   That's true.
[00:05:15.560 --> 00:05:16.240]   That's a good point.
[00:05:16.240 --> 00:05:16.600]   Yeah.
[00:05:16.600 --> 00:05:17.080]   Yeah.
[00:05:17.080 --> 00:05:20.280]   So in a way, it's kind of showing--
[00:05:20.280 --> 00:05:21.280]   How ridiculously--
[00:05:21.280 --> 00:05:23.200]   --dividend and vulnerability of the grid.
[00:05:23.200 --> 00:05:23.720]   Yes.
[00:05:23.720 --> 00:05:25.120]   This is vulnerable.
[00:05:25.120 --> 00:05:27.280]   There were some companies talking about solar power.
[00:05:27.280 --> 00:05:28.480]   And you know they were just sitting back,
[00:05:28.480 --> 00:05:30.800]   rubbing their hand thinking, oh, yes, come to us.
[00:05:30.800 --> 00:05:32.000]   We're going to make some more technology.
[00:05:32.000 --> 00:05:32.000]   Yeah.
[00:05:32.000 --> 00:05:32.960]   Make some more money today.
[00:05:32.960 --> 00:05:33.760]   Yeah.
[00:05:33.760 --> 00:05:35.160]   Yeah.
[00:05:35.160 --> 00:05:36.960]   It did seem like, though, that--
[00:05:36.960 --> 00:05:37.920]   were there some breakthroughs?
[00:05:37.920 --> 00:05:39.240]   I guess--
[00:05:39.240 --> 00:05:42.200]   I mean, in TVs, we could talk about Samsung's micro LED.
[00:05:42.200 --> 00:05:42.640]   Sure.
[00:05:42.640 --> 00:05:44.240]   I mean, the wall, right?
[00:05:44.240 --> 00:05:46.800]   The idea of this modular wall-based television
[00:05:46.800 --> 00:05:48.000]   was exciting.
[00:05:48.000 --> 00:05:49.680]   You're not going to be able to afford it for a long time.
[00:05:49.680 --> 00:05:53.000]   It's a cool idea that you can be able to expand and contract
[00:05:53.000 --> 00:05:56.120]   a television based on what you need for your house.
[00:05:56.120 --> 00:05:57.560]   If you want--
[00:05:57.560 --> 00:05:59.160]   There's just been a size of your wall.
[00:05:59.160 --> 00:06:00.880]   In my opinion, that's a spin.
[00:06:00.880 --> 00:06:03.480]   Because they can't get them-- the micro LEDs
[00:06:03.480 --> 00:06:06.960]   are very, very small LEDs, just as they sound like red, green,
[00:06:06.960 --> 00:06:07.880]   and blue.
[00:06:07.880 --> 00:06:10.280]   And they combine together to make a pixel.
[00:06:10.280 --> 00:06:12.760]   But they can't get them small enough.
[00:06:12.760 --> 00:06:15.480]   The only way you can make a 40 TV out of micro LEDs
[00:06:15.480 --> 00:06:17.960]   is to make it 146 inches.
[00:06:17.960 --> 00:06:20.360]   So they made a virtue out of this,
[00:06:20.360 --> 00:06:22.200]   because they can't do anything smaller.
[00:06:22.200 --> 00:06:25.960]   So they made a virtue, oh, well, it's modular.
[00:06:25.960 --> 00:06:27.200]   Yeah, that's true, because otherwise,
[00:06:27.200 --> 00:06:28.960]   we'd be able to get it in the door.
[00:06:28.960 --> 00:06:30.080]   That's true.
[00:06:30.080 --> 00:06:33.280]   I mean, the thing is, once it's that large, though,
[00:06:33.280 --> 00:06:36.400]   you're not going to be able to appreciate a much better
[00:06:36.400 --> 00:06:37.800]   resolution than that without getting--
[00:06:37.800 --> 00:06:38.480]   Oh, I agree.
[00:06:38.480 --> 00:06:40.360]   Unless you're very, very close to it.
[00:06:40.360 --> 00:06:41.320]   They were 8K.
[00:06:41.320 --> 00:06:41.560]   Scrantz.
[00:06:41.560 --> 00:06:42.520]   There were 8Ks.
[00:06:42.520 --> 00:06:43.360]   Crazy.
[00:06:43.360 --> 00:06:43.720]   Oh, that's crazy.
[00:06:43.720 --> 00:06:44.720]   Insane.
[00:06:44.720 --> 00:06:46.720]   So C and 16K, though, surprisingly.
[00:06:46.720 --> 00:06:47.320]   What?
[00:06:47.320 --> 00:06:48.400]   Stop it.
[00:06:48.400 --> 00:06:49.360]   No.
[00:06:49.360 --> 00:06:49.720]   Really?
[00:06:49.720 --> 00:06:51.280]   The deal is coming soon.
[00:06:51.280 --> 00:06:52.840]   Oh, man.
[00:06:52.840 --> 00:06:55.040]   The LG Roll-Up TV, though, that was really impressive.
[00:06:55.040 --> 00:06:57.000]   Unfortunately, they didn't have that on the show floor.
[00:06:57.000 --> 00:06:59.600]   But we've all known that OLEDs are flexible,
[00:06:59.600 --> 00:07:00.720]   and they can be rolled up.
[00:07:00.720 --> 00:07:03.680]   And we've seen a lot of concepts of small flexible displays
[00:07:03.680 --> 00:07:04.880]   from companies like Samsung.
[00:07:04.880 --> 00:07:07.040]   But for LG to roll out a 4K, I think
[00:07:07.040 --> 00:07:09.680]   it was 65 inch if I remember correctly.
[00:07:09.680 --> 00:07:10.440]   Roll-Up TV.
[00:07:10.440 --> 00:07:13.880]   Roll-Up to a tube like a window blind kind of thing.
[00:07:13.880 --> 00:07:15.360]   And that was really, really impressive.
[00:07:15.360 --> 00:07:16.840]   I was just disappointed they didn't have it
[00:07:16.840 --> 00:07:18.360]   on the show floor going all day long.
[00:07:18.360 --> 00:07:19.200]   It was really cool.
[00:07:19.200 --> 00:07:21.320]   The one thing to note about it, though--
[00:07:21.320 --> 00:07:24.360]   and David Katzmer from CNET took a really close look at it.
[00:07:24.360 --> 00:07:25.440]   He was super impressed.
[00:07:25.440 --> 00:07:27.680]   He had the same thing to say that Tim just said,
[00:07:27.680 --> 00:07:31.400]   but they would not reveal the mechanism into which it
[00:07:31.400 --> 00:07:32.600]   was rolling.
[00:07:32.600 --> 00:07:34.040]   So to see how it's a box.
[00:07:34.040 --> 00:07:38.160]   But you couldn't actually see where it was going.
[00:07:38.160 --> 00:07:40.160]   It was probably like two people's
[00:07:40.160 --> 00:07:42.640]   hands just kind of rolling it up regularly.
[00:07:42.640 --> 00:07:44.640]   We don't know how tight the roll was, I guess,
[00:07:44.640 --> 00:07:45.640]   is what I'm saying.
[00:07:45.640 --> 00:07:48.520]   We couldn't see whether it was really tight,
[00:07:48.520 --> 00:07:50.680]   and it was kind of like a window shade or whether it was this--
[00:07:50.680 --> 00:07:52.000]   I suspect it was a very big--
[00:07:52.000 --> 00:07:53.360]   It was a big thing to lose.
[00:07:53.360 --> 00:07:54.440]   A very loose--
[00:07:54.440 --> 00:07:55.440]   --stealing behinds.
[00:07:55.440 --> 00:07:56.520]   --cool.
[00:07:56.520 --> 00:07:57.760]   Still.
[00:07:57.760 --> 00:07:59.760]   Scott Wilkinson, our home theater guy,
[00:07:59.760 --> 00:08:04.080]   said I'm wondering what you think that 70 inches is the new 55
[00:08:04.080 --> 00:08:04.640]   or 60.
[00:08:04.640 --> 00:08:07.680]   That TVs are going to get bigger.
[00:08:07.680 --> 00:08:09.360]   I think that that's just sort of a truism.
[00:08:09.360 --> 00:08:10.840]   It's like the Moore's Law TV.
[00:08:10.840 --> 00:08:11.960]   They just get bigger.
[00:08:11.960 --> 00:08:14.160]   And we all have this weird sci-fi expectation
[00:08:14.160 --> 00:08:17.000]   that eventually TVs are going to fill our whole wall.
[00:08:17.000 --> 00:08:18.560]   We just seem to want that.
[00:08:18.560 --> 00:08:20.160]   I'm not sure exactly what--
[00:08:20.160 --> 00:08:22.840]   So most of us now just watch on this.
[00:08:22.840 --> 00:08:23.360]   I know.
[00:08:23.360 --> 00:08:24.040]   That's so crazy.
[00:08:24.040 --> 00:08:26.520]   I was just watching the football game on my iPad.
[00:08:26.520 --> 00:08:27.240]   It's over now.
[00:08:27.240 --> 00:08:30.560]   So I don't put it away because that's rude.
[00:08:30.560 --> 00:08:32.480]   But yeah, I think a lot of people--
[00:08:32.480 --> 00:08:33.800]   I look at my son's generation.
[00:08:33.800 --> 00:08:37.440]   They don't care if it's 70 inches or 10 inches.
[00:08:37.440 --> 00:08:39.120]   They don't seem to care that much.
[00:08:39.120 --> 00:08:41.000]   They may, though, when they start to buy houses.
[00:08:41.000 --> 00:08:41.920]   Yeah.
[00:08:41.920 --> 00:08:43.840]   It's an old person thing.
[00:08:43.840 --> 00:08:45.360]   You want to sit in comfy chairs?
[00:08:45.360 --> 00:08:48.200]   [LAUGHTER]
[00:08:48.200 --> 00:08:49.920]   So how about cars?
[00:08:49.920 --> 00:08:51.920]   This is one of the things that's weird about CES.
[00:08:51.920 --> 00:08:53.920]   It's right before the auto show.
[00:08:53.920 --> 00:08:56.160]   And yet there's a lot of automotive technology.
[00:08:56.160 --> 00:08:57.960]   Is CES right, Tim?
[00:08:57.960 --> 00:08:58.680]   Yeah, absolutely.
[00:08:58.680 --> 00:09:00.120]   There's a huge amount of stuff there.
[00:09:00.120 --> 00:09:02.560]   And a lot of companies, US CES, is kind of a launching point
[00:09:02.560 --> 00:09:04.160]   for their Detroit show, actually.
[00:09:04.160 --> 00:09:08.520]   So they'll announce some technology in Vegas at CES.
[00:09:08.520 --> 00:09:10.880]   For example, Mercedes-Benz showed off their new infotainment
[00:09:10.880 --> 00:09:12.600]   system at Vegas.
[00:09:12.600 --> 00:09:15.640]   And then they went into Detroit and showed off the new G-Class,
[00:09:15.640 --> 00:09:16.760]   their big new SUV.
[00:09:16.760 --> 00:09:18.320]   So that way they can get their tech story,
[00:09:18.320 --> 00:09:20.520]   and they can get that angle covered in Vegas.
[00:09:20.520 --> 00:09:23.240]   And then they can get their big metal story done
[00:09:23.240 --> 00:09:24.720]   into Detroit as well without kind of stepping
[00:09:24.720 --> 00:09:25.560]   on their own toes.
[00:09:25.560 --> 00:09:27.800]   Because in Detroit, not that many people
[00:09:27.800 --> 00:09:29.880]   would really care about a new infotainment system.
[00:09:29.880 --> 00:09:31.280]   And in Vegas, certainly not that many people
[00:09:31.280 --> 00:09:33.320]   would care about what is ultimately
[00:09:33.320 --> 00:09:36.480]   a new version of their ancient giant SUV.
[00:09:36.480 --> 00:09:38.640]   So that way they can basically target their story
[00:09:38.640 --> 00:09:41.160]   at the appropriate media and kind of cover both bases
[00:09:41.160 --> 00:09:42.160]   pretty well.
[00:09:42.160 --> 00:09:45.040]   But there were quite a few automotive announcements at CES.
[00:09:45.040 --> 00:09:46.840]   Biden was probably the biggest one,
[00:09:46.840 --> 00:09:48.960]   new Chinese manufacturing with a relatively low cost
[00:09:48.960 --> 00:09:50.400]   all-electric SUV.
[00:09:50.400 --> 00:09:52.680]   Hopefully, coming to the US in 2020, fingers crossed.
[00:09:52.680 --> 00:09:54.960]   We'll see.
[00:09:54.960 --> 00:09:58.280]   So electrics are interesting because you've got, of course,
[00:09:58.280 --> 00:09:58.760]   Tesla.
[00:09:58.760 --> 00:10:02.320]   But then you now have the Nissan Leaf, which
[00:10:02.320 --> 00:10:05.720]   is a big seller, the Bolt from BOLT, from Chevy,
[00:10:05.720 --> 00:10:07.320]   which is a big seller.
[00:10:07.320 --> 00:10:09.760]   And every car manufacturer--
[00:10:09.760 --> 00:10:11.040]   I'm curious.
[00:10:11.040 --> 00:10:12.120]   They've announced electrics.
[00:10:12.120 --> 00:10:14.720]   But do they see this as the future of the market?
[00:10:14.720 --> 00:10:17.200]   Or doing this for regulatory reasons?
[00:10:17.200 --> 00:10:19.000]   There are countries like Norway where it fits
[00:10:19.000 --> 00:10:19.520]   not electric.
[00:10:19.520 --> 00:10:22.040]   You won't be able to sell it in a few years.
[00:10:22.040 --> 00:10:23.240]   What's driving electric?
[00:10:23.240 --> 00:10:23.960]   Is it demand?
[00:10:23.960 --> 00:10:25.240]   I don't think it's demand.
[00:10:25.240 --> 00:10:26.440]   It's definitely not demand.
[00:10:26.440 --> 00:10:28.480]   In fact, I was just looking at some numbers
[00:10:28.480 --> 00:10:29.480]   not that long ago.
[00:10:29.480 --> 00:10:33.120]   And the average incentive price on top of an EV
[00:10:33.120 --> 00:10:35.280]   is in the $10,000 to $15,000 range.
[00:10:35.280 --> 00:10:37.320]   It's basically how much dealers need to knock off
[00:10:37.320 --> 00:10:39.360]   the price of the car to move those.
[00:10:39.360 --> 00:10:42.000]   So for example, the Chevy Bolt EV, the electric car,
[00:10:42.000 --> 00:10:44.240]   outsold the Volt to their plug-in hybrid last year,
[00:10:44.240 --> 00:10:46.360]   which is on the surface a pretty great story.
[00:10:46.360 --> 00:10:49.560]   But they had to knock off somewhere around $12,000, $13,000
[00:10:49.560 --> 00:10:51.080]   on every Bolt to get them out the door.
[00:10:51.080 --> 00:10:52.960]   And that's on top of any federal incentives.
[00:10:52.960 --> 00:10:55.720]   So there's a lot of deals like that going on
[00:10:55.720 --> 00:10:57.720]   to get these things out the door.
[00:10:57.720 --> 00:11:00.240]   And the expectation is that they could be the future.
[00:11:00.240 --> 00:11:02.200]   But there are actually a lot of executives
[00:11:02.200 --> 00:11:02.920]   that are skeptical.
[00:11:02.920 --> 00:11:05.640]   It was a KPMG study from a few months back
[00:11:05.640 --> 00:11:07.640]   where I think it was on the order of 60% or 70%
[00:11:07.640 --> 00:11:09.880]   of automotive executives think that battery
[00:11:09.880 --> 00:11:11.040]   electrics are actually a fad.
[00:11:11.040 --> 00:11:12.640]   And they're actually going to go away.
[00:11:12.640 --> 00:11:14.840]   So that's pretty interesting.
[00:11:14.840 --> 00:11:17.840]   Right now they're allowing these automotive companies
[00:11:17.840 --> 00:11:20.480]   to basically increase their average fuel economy
[00:11:20.480 --> 00:11:22.600]   and ultimately reduce the fleet emissions
[00:11:22.600 --> 00:11:23.840]   of all their vehicles.
[00:11:23.840 --> 00:11:24.680]   So they get credits.
[00:11:24.680 --> 00:11:27.480]   And ultimately that allows them to build big trucks
[00:11:27.480 --> 00:11:29.440]   or sports cars or whatever else they want to do.
[00:11:29.440 --> 00:11:31.520]   But they're having to incentivize those EVs a lot
[00:11:31.520 --> 00:11:32.560]   in order to get them out the door.
[00:11:32.560 --> 00:11:34.360]   I'm hoping that that changes this year
[00:11:34.360 --> 00:11:36.000]   with the Model 3 coming out in mass.
[00:11:36.000 --> 00:11:37.680]   But that remains to be seen.
[00:11:37.680 --> 00:11:39.920]   Part of the problem is a gas is cheap right now.
[00:11:39.920 --> 00:11:43.320]   Gas is super cheap and it's still not easy to charge.
[00:11:43.320 --> 00:11:45.200]   It's not all my friends who drive electric vehicles.
[00:11:45.200 --> 00:11:46.400]   I have quite a few because I live in Berkeley.
[00:11:46.400 --> 00:11:49.200]   That's like, you know, I don't do yoga
[00:11:49.200 --> 00:11:50.120]   and I don't have an electric car.
[00:11:50.120 --> 00:11:50.960]   So I'm going to get kicked out.
[00:11:50.960 --> 00:11:52.920]   At least you eat granola and wear burka stocks.
[00:11:52.920 --> 00:11:53.760]   So you're halfway there.
[00:11:53.760 --> 00:11:55.040]   Granola.
[00:11:55.040 --> 00:11:56.040]   Yes.
[00:11:56.040 --> 00:11:58.120]   (laughing)
[00:11:58.120 --> 00:12:00.600]   But it's still this kind of crazy like,
[00:12:00.600 --> 00:12:02.040]   wait, where's there a whole foods?
[00:12:02.040 --> 00:12:03.040]   I got to charge.
[00:12:03.040 --> 00:12:05.560]   And this is just taking too long.
[00:12:05.560 --> 00:12:06.760]   I'm not going to make my appointment.
[00:12:06.760 --> 00:12:08.120]   I think it still happens a lot
[00:12:08.120 --> 00:12:09.520]   and it stresses people out.
[00:12:09.520 --> 00:12:10.960]   Charging fast with the barrier.
[00:12:10.960 --> 00:12:12.520]   I mean, if you're used to going to the gas station
[00:12:12.520 --> 00:12:14.320]   and filling up in a few minutes,
[00:12:14.320 --> 00:12:16.080]   you're going to be charging for,
[00:12:16.080 --> 00:12:18.320]   if it's a supercharger, half an hour.
[00:12:18.320 --> 00:12:20.440]   And if it's not, long, long time.
[00:12:20.440 --> 00:12:22.200]   Yeah, so that was actually one of the cool things
[00:12:22.200 --> 00:12:25.680]   we saw at CDS was the first production ready car
[00:12:25.680 --> 00:12:27.000]   with a solid state battery.
[00:12:27.000 --> 00:12:28.560]   That was the new Fisker Emotion,
[00:12:28.560 --> 00:12:31.840]   which is a beautiful four-door kind of sporty sedan thing.
[00:12:31.840 --> 00:12:33.920]   But we've heard a lot of talk about solid state batteries.
[00:12:33.920 --> 00:12:35.680]   And Fisker was the first one to actually show us a car
[00:12:35.680 --> 00:12:36.840]   that had one in it.
[00:12:36.840 --> 00:12:40.920]   And in nine minutes, that car will do 125 miles with charging,
[00:12:40.920 --> 00:12:42.600]   which is about two hours of driving.
[00:12:42.600 --> 00:12:45.240]   So you figure if your crew's done the highway every two hours,
[00:12:45.240 --> 00:12:47.720]   maybe stop, get a coffee, use the restroom.
[00:12:47.720 --> 00:12:49.720]   And in that time, you'd get another 120 miles of rain.
[00:12:49.720 --> 00:12:51.560]   So in theory, that's pretty cool.
[00:12:51.560 --> 00:12:53.280]   And we're expecting to see something similar
[00:12:53.280 --> 00:12:55.520]   from Porsche this year a little later on.
[00:12:55.520 --> 00:12:57.840]   But solid state batteries could really change the industry
[00:12:57.840 --> 00:13:00.280]   and that could get us the charging rate that we need,
[00:13:00.280 --> 00:13:01.560]   but we're still going to need those charges of course.
[00:13:01.560 --> 00:13:02.400]   - That's really cool.
[00:13:02.400 --> 00:13:04.960]   Tim, what did you think of the Qualcomm demo of the,
[00:13:04.960 --> 00:13:08.520]   it was basically like wireless charging for your car, right?
[00:13:08.520 --> 00:13:09.720]   That was what I got out of it.
[00:13:09.720 --> 00:13:12.000]   - Yeah, they put it in the freeway and you charge as you drive.
[00:13:12.000 --> 00:13:13.160]   - Well, just like as you park,
[00:13:13.160 --> 00:13:16.000]   you just parked over the charging pad.
[00:13:16.000 --> 00:13:17.200]   - They're actually working on both.
[00:13:17.200 --> 00:13:19.640]   So the technology is called Halo and it's,
[00:13:19.640 --> 00:13:21.040]   the basic idea is that yeah,
[00:13:21.040 --> 00:13:23.440]   you'd be able to deploy this within a parking spot
[00:13:23.440 --> 00:13:25.280]   at a grocery store or something like that.
[00:13:25.280 --> 00:13:28.240]   And it actually will do fast DC charging wirelessly,
[00:13:28.240 --> 00:13:29.080]   which is pretty cool.
[00:13:29.080 --> 00:13:30.680]   So you just park your car over it.
[00:13:30.680 --> 00:13:32.280]   And I think the loss rate is only,
[00:13:32.280 --> 00:13:34.960]   it's less than 5% if I can recall correctly.
[00:13:34.960 --> 00:13:36.920]   So it's actually not that much worse
[00:13:36.920 --> 00:13:38.400]   than actually just plugging the car in directly,
[00:13:38.400 --> 00:13:40.000]   but they did do a prototype last year.
[00:13:40.000 --> 00:13:43.120]   They said, I think it was 500 meters of road in Paris
[00:13:43.120 --> 00:13:44.640]   that they had lined with these things
[00:13:44.640 --> 00:13:46.960]   and they were actually able to get a car
[00:13:46.960 --> 00:13:48.760]   to charge while it was driving.
[00:13:48.760 --> 00:13:50.720]   It takes a lot of work to make that work
[00:13:50.720 --> 00:13:52.920]   because usually with these wireless charging systems,
[00:13:52.920 --> 00:13:56.440]   you have to have like a direct PDP kind of connection
[00:13:56.440 --> 00:13:57.280]   sort of thing.
[00:13:57.280 --> 00:13:58.360]   But they're actually able to make it work
[00:13:58.360 --> 00:13:59.400]   as the car was driving.
[00:13:59.400 --> 00:14:02.080]   So in theory, if there was a city with enough money,
[00:14:02.080 --> 00:14:04.040]   and I'm guessing it would cost a whole lot of money,
[00:14:04.040 --> 00:14:05.880]   they could make major roads,
[00:14:05.880 --> 00:14:07.200]   wirelessly charging vehicles.
[00:14:07.200 --> 00:14:08.680]   So you'd have buses cruising along,
[00:14:08.680 --> 00:14:11.200]   just pulling electricity right out of the ground.
[00:14:11.200 --> 00:14:12.360]   - Seems like they would have to work with that.
[00:14:12.360 --> 00:14:14.960]   That'd be a toll road kind of situation.
[00:14:14.960 --> 00:14:16.480]   - Probably sell, yeah, absolutely.
[00:14:16.480 --> 00:14:18.040]   - You could sell technology going anywhere
[00:14:18.040 --> 00:14:19.680]   'cause that's another alternative
[00:14:19.680 --> 00:14:22.300]   where you have faster fill-ups, I think.
[00:14:22.300 --> 00:14:24.240]   - Absolutely.
[00:14:24.240 --> 00:14:25.560]   So we saw that was another thing.
[00:14:25.560 --> 00:14:28.240]   I see, yes, Hyundai showed off their fuel cell electric car.
[00:14:28.240 --> 00:14:30.200]   In fact, Antoine Goodwin on our team drove one
[00:14:30.200 --> 00:14:32.640]   from San Francisco down to Las Vegas
[00:14:32.640 --> 00:14:35.120]   for the unveiling of the car, which was pretty cool.
[00:14:35.120 --> 00:14:36.120]   The advantage of fuel cells,
[00:14:36.120 --> 00:14:37.800]   well, they're basically electric cars,
[00:14:37.800 --> 00:14:39.160]   but rather than having a big battery,
[00:14:39.160 --> 00:14:42.200]   they pull their electricity from an onboard hydrogen tank.
[00:14:42.200 --> 00:14:44.000]   And you can refill those in about five minutes,
[00:14:44.000 --> 00:14:44.840]   give or take.
[00:14:44.840 --> 00:14:47.080]   So yeah, in theory, you could basically get a full amount
[00:14:47.080 --> 00:14:50.040]   of range from compressed liquid hydrogen
[00:14:50.040 --> 00:14:51.040]   in about the same amount of time
[00:14:51.040 --> 00:14:52.920]   it would take you to fill up your car.
[00:14:52.920 --> 00:14:55.760]   Range on those, does tend to be in the 250 to 300 mile range.
[00:14:55.760 --> 00:14:56.600]   So that's plenty.
[00:14:56.600 --> 00:14:59.240]   And again, no emissions other than water vapor.
[00:14:59.240 --> 00:15:01.000]   We've seen more and more companies working on it,
[00:15:01.000 --> 00:15:03.720]   both Honda and Toyota have been kind of taking the lead here,
[00:15:03.720 --> 00:15:05.880]   but Hyundai now has this car as well,
[00:15:05.880 --> 00:15:06.800]   which is pretty great to see.
[00:15:06.800 --> 00:15:08.040]   And essentially, Aquana and I still looking
[00:15:08.040 --> 00:15:09.840]   a little crossover SUV.
[00:15:09.840 --> 00:15:11.920]   But they're actually, the problem is much bigger.
[00:15:11.920 --> 00:15:14.640]   If I have an EV, in fact, I do have an EV,
[00:15:14.640 --> 00:15:16.360]   I could just plug it in my garage and charge it up.
[00:15:16.360 --> 00:15:18.200]   And in the next day, it's fully charged.
[00:15:18.200 --> 00:15:20.320]   So if I don't go further than the range of my car,
[00:15:20.320 --> 00:15:22.200]   I don't even need to think about charging.
[00:15:22.200 --> 00:15:23.720]   With a hydrogen system,
[00:15:23.720 --> 00:15:25.600]   you don't probably have a hydrogen filling station
[00:15:25.600 --> 00:15:26.600]   in your garage.
[00:15:26.600 --> 00:15:27.880]   So that point, you need to find one.
[00:15:27.880 --> 00:15:30.840]   And there are, I think, maybe a dozen or two in New York
[00:15:30.840 --> 00:15:33.040]   States, which is a pretty big state in California.
[00:15:33.040 --> 00:15:36.400]   There's more, but still, they're very, very hard to find.
[00:15:36.400 --> 00:15:38.520]   And getting those things deployed in metropolitan areas
[00:15:38.520 --> 00:15:41.240]   is actually pretty difficult, because hydrogen is, of course,
[00:15:41.240 --> 00:15:42.000]   rather explosive.
[00:15:42.000 --> 00:15:43.920]   So there needs to be some regulation on that front too.
[00:15:43.920 --> 00:15:46.200]   So the potential is there, the emissions are great.
[00:15:46.200 --> 00:15:46.880]   The range is great.
[00:15:46.880 --> 00:15:48.880]   It solves a lot of the EV problems.
[00:15:48.880 --> 00:15:50.800]   And the nice thing is it kind of piggybacks
[00:15:50.800 --> 00:15:53.040]   on EV technology from a drive train standpoint.
[00:15:53.040 --> 00:15:54.240]   So that's nice too.
[00:15:54.240 --> 00:15:56.080]   But distribution there is actually even harder
[00:15:56.080 --> 00:15:58.160]   than on the EV side, because you can just
[00:15:58.160 --> 00:16:00.520]   put a charging station pretty much anywhere you want to.
[00:16:00.520 --> 00:16:02.120]   You can't really store a big tank of hydrogen
[00:16:02.120 --> 00:16:03.240]   anywhere you want to.
[00:16:03.240 --> 00:16:07.440]   Georgia, how much is gas expensive in Canada?
[00:16:07.440 --> 00:16:11.840]   Somebody's saying, in Vancouver, it's $5.25 a gallon.
[00:16:11.840 --> 00:16:13.080]   No, it's expensive here.
[00:16:13.080 --> 00:16:14.720]   We pay a lot for our gas, and we're
[00:16:14.720 --> 00:16:15.960]   one of the major producers.
[00:16:15.960 --> 00:16:19.320]   So, you know, just a ton of sense.
[00:16:19.320 --> 00:16:22.360]   It's gone down, but it's still really expensive in Canada.
[00:16:22.360 --> 00:16:23.360]   I wonder why--
[00:16:23.360 --> 00:16:24.960]   And especially in Quebec.
[00:16:24.960 --> 00:16:29.960]   Is that the government trying to disincentivize people
[00:16:29.960 --> 00:16:34.120]   to use gas, or why is it cheap here in expensive in Canada?
[00:16:34.120 --> 00:16:35.760]   I bet it has to taxation.
[00:16:35.760 --> 00:16:36.280]   Yeah.
[00:16:36.280 --> 00:16:38.240]   I think that it's not just even taxation.
[00:16:38.240 --> 00:16:40.880]   I think that it's agreements that we have of, you know,
[00:16:40.880 --> 00:16:42.680]   how many distillation systems we're
[00:16:42.680 --> 00:16:44.520]   going to have here versus overseas.
[00:16:44.520 --> 00:16:47.080]   And we really pretty much sell off all of our crude
[00:16:47.080 --> 00:16:51.680]   and then buy it back, which, again, the logic is flawed.
[00:16:51.680 --> 00:16:55.280]   But so a lot of that has to do with us buying back our own.
[00:16:55.280 --> 00:16:56.960]   I mean, I'm not against the idea,
[00:16:56.960 --> 00:16:59.680]   because I think you do want to encourage the switch
[00:16:59.680 --> 00:17:00.800]   to renewables.
[00:17:00.800 --> 00:17:01.280]   Sure.
[00:17:01.280 --> 00:17:03.440]   And it's one of the things that's held it back here.
[00:17:03.440 --> 00:17:03.920]   Yeah.
[00:17:03.920 --> 00:17:09.120]   But if gas prices are low, you know, everything costs less,
[00:17:09.120 --> 00:17:11.040]   because everything's delivered by gas, drug.
[00:17:11.040 --> 00:17:12.960]   And I'm kind of assuming-- and I'm-- this
[00:17:12.960 --> 00:17:15.400]   is going to reveal my Canada ignorance,
[00:17:15.400 --> 00:17:19.320]   but that public transportation systems are probably not
[00:17:19.320 --> 00:17:21.240]   much better than they are in the US, just because
[00:17:21.240 --> 00:17:25.120]   of the population density and size of the country.
[00:17:25.120 --> 00:17:26.200]   So that is interesting.
[00:17:26.200 --> 00:17:27.000]   Yeah.
[00:17:27.000 --> 00:17:27.800]   Yeah, it's true.
[00:17:27.800 --> 00:17:29.400]   We do have-- there's certain areas
[00:17:29.400 --> 00:17:31.720]   that have a really great public transport.
[00:17:31.720 --> 00:17:35.200]   But our road systems also are really pitiful.
[00:17:35.200 --> 00:17:37.160]   And in Quebec, they're probably one of the worst
[00:17:37.160 --> 00:17:38.680]   in the country.
[00:17:38.680 --> 00:17:40.280]   Just there's potholes everywhere, which--
[00:17:40.280 --> 00:17:40.800]   Oh, you have--
[00:17:40.800 --> 00:17:42.280]   which is climate related, right?
[00:17:42.280 --> 00:17:42.800]   I mean--
[00:17:42.800 --> 00:17:44.600]   It's not just climate related.
[00:17:44.600 --> 00:17:46.880]   It's actually also about--
[00:17:46.880 --> 00:17:50.320]   they found out all kinds of nefarious things with--
[00:17:50.320 --> 00:17:53.240]   they've chosen to give contracts to people for 20 years.
[00:17:53.240 --> 00:17:55.520]   And so they build roads that are actually
[00:17:55.520 --> 00:17:57.480]   poor road systems, and the concrete is not
[00:17:57.480 --> 00:18:01.320]   made to last for as long as the contract is
[00:18:01.320 --> 00:18:04.200]   so that people are constantly renewing the road systems
[00:18:04.200 --> 00:18:05.160]   because of that.
[00:18:05.160 --> 00:18:08.040]   So there's a lot of stuff that kind of happens.
[00:18:08.040 --> 00:18:08.840]   Canada's great.
[00:18:08.840 --> 00:18:09.800]   I really love Canada.
[00:18:09.800 --> 00:18:13.440]   But there's another side to that as well.
[00:18:13.440 --> 00:18:15.200]   We should mention George is in Montreal.
[00:18:15.200 --> 00:18:17.320]   With your blue background, I think, Carson,
[00:18:17.320 --> 00:18:18.360]   we should work on something.
[00:18:18.360 --> 00:18:21.360]   Let's put fish or something behind your shoes.
[00:18:21.360 --> 00:18:24.280]   I can do-- you're on a blue screen, Georgia.
[00:18:24.280 --> 00:18:26.080]   I don't know if you know about blue screens.
[00:18:26.080 --> 00:18:28.040]   I think you could be careful here.
[00:18:28.040 --> 00:18:31.000]   We're going to see what we could do to mess with you a little bit.
[00:18:31.000 --> 00:18:32.480]   Mess with you a little bit.
[00:18:32.480 --> 00:18:36.480]   That fisker looks really cool, but that's going to be--
[00:18:36.480 --> 00:18:39.960]   first of all, I mean, I remember the first fisker.
[00:18:39.960 --> 00:18:43.200]   Wasn't that a horrible flop and all sorts of problems?
[00:18:43.200 --> 00:18:47.400]   A friend of mine drove one, eh?
[00:18:47.400 --> 00:18:48.400]   Who's telling stories?
[00:18:48.400 --> 00:18:50.480]   Like, he couldn't get out of the car once.
[00:18:50.480 --> 00:18:51.480]   Wow.
[00:18:51.480 --> 00:18:52.480]   Yeah.
[00:18:52.480 --> 00:18:54.120]   It was not a resounding success.
[00:18:54.120 --> 00:18:55.840]   It burned down a few people's homes.
[00:18:55.840 --> 00:18:58.240]   It was not exactly what you want out of your EV.
[00:18:58.240 --> 00:19:01.560]   But Henrik Fisker is actually a renowned designer.
[00:19:01.560 --> 00:19:03.400]   He's done a lot of the most beautiful cars on the road.
[00:19:03.400 --> 00:19:04.880]   Oh, the most gorgeous.
[00:19:04.880 --> 00:19:05.920]   I want this car.
[00:19:05.920 --> 00:19:06.400]   Yeah.
[00:19:06.400 --> 00:19:06.960]   It is beautiful.
[00:19:06.960 --> 00:19:08.040]   Except for that little grill in the nose,
[00:19:08.040 --> 00:19:10.560]   which I'm not particularly fond of, I must be honest.
[00:19:10.560 --> 00:19:11.760]   But otherwise, yeah, it is gorgeous.
[00:19:11.760 --> 00:19:13.560]   It's got suicide doors, which is one of the things
[00:19:13.560 --> 00:19:14.760]   that we love to see in concert cars,
[00:19:14.760 --> 00:19:16.880]   but they never actually come to production.
[00:19:16.880 --> 00:19:18.520]   But it is a great looking car.
[00:19:18.520 --> 00:19:20.000]   And hopefully, a lot of lessons learned
[00:19:20.000 --> 00:19:23.480]   from the Fisker Karma, which was the car that came before.
[00:19:23.480 --> 00:19:24.960]   And that car actually is still available.
[00:19:24.960 --> 00:19:26.240]   Another company bought the technology
[00:19:26.240 --> 00:19:27.640]   and brought it out with bigger batteries.
[00:19:27.640 --> 00:19:29.360]   And a few other tweaks.
[00:19:29.360 --> 00:19:31.680]   Unfortunately, it still just not a very good car.
[00:19:31.680 --> 00:19:32.320]   It's really--
[00:19:32.320 --> 00:19:33.600]   We're hopeful this one will be better.
[00:19:33.600 --> 00:19:33.880]   We'll see.
[00:19:33.880 --> 00:19:37.680]   This one looks to me like a Tesla mixed with an offer of mayo.
[00:19:37.680 --> 00:19:38.760]   Yes.
[00:19:38.760 --> 00:19:40.040]   It's like they had a baby.
[00:19:40.040 --> 00:19:42.400]   And then it had a really cool--
[00:19:42.400 --> 00:19:43.320]   Is that real?
[00:19:43.320 --> 00:19:44.760]   That's weird.
[00:19:44.760 --> 00:19:45.760]   I can't wait.
[00:19:45.760 --> 00:19:47.640]   So how much did they say a price?
[00:19:47.640 --> 00:19:49.560]   I don't think they gave us a formal price on this,
[00:19:49.560 --> 00:19:51.520]   but we're definitely talking hundreds of thousands of dollars
[00:19:51.520 --> 00:19:52.720]   to get into something like that.
[00:19:52.720 --> 00:19:54.600]   Yeah, if you have to ask, you can't afford it.
[00:19:54.600 --> 00:19:55.880]   Tim, do you think that you took a car?
[00:19:55.880 --> 00:19:57.440]   I don't know about the back end.
[00:19:57.440 --> 00:20:00.400]   The rump is a little bit more frumpy than the front side.
[00:20:00.400 --> 00:20:01.200]   Rumpy rump.
[00:20:01.200 --> 00:20:02.240]   It is a frumpy rump.
[00:20:02.240 --> 00:20:03.240]   A little bit.
[00:20:03.240 --> 00:20:06.120]   When you're driving towards you, you're like, wow, that's really--
[00:20:06.120 --> 00:20:06.920]   And then it's going away.
[00:20:06.920 --> 00:20:09.440]   You're like, mm, mm.
[00:20:09.440 --> 00:20:10.440]   Frumpy rump.
[00:20:10.440 --> 00:20:12.360]   That is the frontiest rump I've ever seen.
[00:20:12.360 --> 00:20:14.360]   [LAUGHTER]
[00:20:14.360 --> 00:20:16.040]   Now you spoiled it for me.
[00:20:16.040 --> 00:20:16.720]   You like it?
[00:20:16.720 --> 00:20:18.280]   Do you like it?
[00:20:18.280 --> 00:20:19.200]   I like it.
[00:20:19.200 --> 00:20:20.720]   I guess I just like a frumpy rump.
[00:20:20.720 --> 00:20:22.480]   [LAUGHTER]
[00:20:22.480 --> 00:20:24.320]   OK, I like a frumpy rump.
[00:20:24.320 --> 00:20:25.480]   They're the show title right now.
[00:20:25.480 --> 00:20:26.560]   I think so.
[00:20:26.560 --> 00:20:28.720]   $2,000 to reserve it.
[00:20:28.720 --> 00:20:30.600]   Oh, base price, $129,000.
[00:20:30.600 --> 00:20:32.880]   It's in the Tesla range.
[00:20:32.880 --> 00:20:34.360]   This is like a Tesla.
[00:20:34.360 --> 00:20:37.120]   You never get the base model of any of these, though, right?
[00:20:37.120 --> 00:20:41.160]   So you probably write it closer to $200,000.
[00:20:41.160 --> 00:20:42.800]   I don't know, though.
[00:20:42.800 --> 00:20:44.520]   After the last fisker, it's like--
[00:20:44.520 --> 00:20:45.920]   Well, that was my question.
[00:20:45.920 --> 00:20:48.280]   I feel like fisker had a ton of name recognition
[00:20:48.280 --> 00:20:49.760]   around the time that Tesla was also
[00:20:49.760 --> 00:20:51.080]   gaining name recognition.
[00:20:51.080 --> 00:20:52.440]   They were kind of neck and necked,
[00:20:52.440 --> 00:20:55.120]   Tesla a little bit ahead, and then everything went wrong
[00:20:55.120 --> 00:20:56.800]   for fisker and kind of fell off the map.
[00:20:56.800 --> 00:20:58.680]   It seems very hard to dig out of that.
[00:20:58.680 --> 00:21:02.320]   But I don't know, Tim, do you think they've kept their panache?
[00:21:02.320 --> 00:21:03.800]   I don't think that this is a car
[00:21:03.800 --> 00:21:06.320]   that they're expecting to sell in large volumes anyway.
[00:21:06.320 --> 00:21:08.520]   So ultimately, I think he's got a sort of clientele
[00:21:08.520 --> 00:21:10.120]   that's going to be very loyal to his design,
[00:21:10.120 --> 00:21:11.360]   more so than anything.
[00:21:11.360 --> 00:21:14.440]   So I think that'll help to get that first couple hundred
[00:21:14.440 --> 00:21:16.080]   sold that he's looking for beyond that.
[00:21:16.080 --> 00:21:17.160]   That's the big question.
[00:21:17.160 --> 00:21:20.440]   And how reliable that solid-state battery technology can be
[00:21:20.440 --> 00:21:23.360]   is my big question, because this is still cutting-edge technology.
[00:21:23.360 --> 00:21:26.280]   Again, we're waiting to see some major manufacturer bring out.
[00:21:26.280 --> 00:21:27.680]   There's been a lot of rumors that Portia's
[00:21:27.680 --> 00:21:29.960]   electric Model S fighter, which we think we'll
[00:21:29.960 --> 00:21:31.640]   see in just a few months.
[00:21:31.640 --> 00:21:34.000]   We'll actually be using the same sort of technology,
[00:21:34.000 --> 00:21:35.200]   but we're waiting to see on that front.
[00:21:35.200 --> 00:21:37.080]   So I think that's where I would be most skeptical.
[00:21:37.080 --> 00:21:39.000]   And I definitely wouldn't want to be at the cutting edge
[00:21:39.000 --> 00:21:41.440]   of that reservation queue myself.
[00:21:41.440 --> 00:21:43.000]   But I'm definitely eager to see how the thing drives.
[00:21:43.000 --> 00:21:44.920]   And hopefully, we'll get a chance to take a spin
[00:21:44.920 --> 00:21:45.960]   and won this year.
[00:21:45.960 --> 00:21:47.320]   I'm not going to go anyway.
[00:21:47.320 --> 00:21:51.120]   I think this is a bad time to make any plans or reservations
[00:21:51.120 --> 00:21:54.280]   for an electric car, because it seems to me every company
[00:21:54.280 --> 00:21:56.760]   is announcing there's going to be a Cadillac,
[00:21:56.760 --> 00:21:58.160]   there's going to be a Jaguar, there's
[00:21:58.160 --> 00:22:01.000]   going to be multiple outies and Volkswagen's.
[00:22:01.000 --> 00:22:01.480]   Right?
[00:22:01.480 --> 00:22:02.480]   Yeah, definitely.
[00:22:02.480 --> 00:22:05.200]   It also seems like a great time to buy a used electric car.
[00:22:05.200 --> 00:22:06.280]   Yes.
[00:22:06.280 --> 00:22:06.840]   Absolutely.
[00:22:06.840 --> 00:22:07.840]   As a second car.
[00:22:07.840 --> 00:22:08.840]   It's a good point.
[00:22:08.840 --> 00:22:10.600]   Depreciation on those is absolutely astronomical.
[00:22:10.600 --> 00:22:12.880]   In some belief, you can get a last generation
[00:22:12.880 --> 00:22:15.480]   leaf for less than $10,000 at this point,
[00:22:15.480 --> 00:22:17.480]   even if it only has maybe 20,000 miles on it.
[00:22:17.480 --> 00:22:19.760]   It's outrageous how quickly these things drop.
[00:22:19.760 --> 00:22:22.560]   And that's encouraging if you're looking for a second-hand car.
[00:22:22.560 --> 00:22:24.560]   But the problem is that actually is really heavily
[00:22:24.560 --> 00:22:27.840]   disincentivizing anybody who's thinking about buying one new,
[00:22:27.840 --> 00:22:29.920]   because of course, you want that car to be worth something
[00:22:29.920 --> 00:22:31.080]   when you trade it back in again.
[00:22:31.080 --> 00:22:34.480]   So lease rates are generally pretty appealing for EVs,
[00:22:34.480 --> 00:22:36.000]   actually relatively affordable.
[00:22:36.000 --> 00:22:38.040]   We got a Kia Soul EV a few months ago,
[00:22:38.040 --> 00:22:41.080]   and they took almost half of the MSRP off,
[00:22:41.080 --> 00:22:43.280]   just to get us in a short term lease in that thing,
[00:22:43.280 --> 00:22:45.640]   just because they were desperate to move them.
[00:22:45.640 --> 00:22:48.240]   A lot of car companies know that these new EVs are coming
[00:22:48.240 --> 00:22:50.920]   with more range and the new leaf, for example,
[00:22:50.920 --> 00:22:53.400]   on the Model 3 now that's actually in production.
[00:22:53.400 --> 00:22:56.120]   And that's just going to drive the prices of the existing EVs
[00:22:56.120 --> 00:22:58.080]   down quite a bit.
[00:22:58.080 --> 00:22:59.040]   Interesting.
[00:22:59.040 --> 00:23:01.200]   Well, it's an interesting world.
[00:23:01.200 --> 00:23:02.720]   I want to ask you-- we're going to take a break,
[00:23:02.720 --> 00:23:04.120]   but I wanted to ask you when I come back
[00:23:04.120 --> 00:23:05.960]   about self-driving vehicles.
[00:23:05.960 --> 00:23:07.720]   As well, Tim Stevens is here, of course,
[00:23:07.720 --> 00:23:10.720]   host of Roadshow on CNET, editor at large--
[00:23:10.720 --> 00:23:14.080]   I'm sorry, Roadshow at CNET, and our car guy,
[00:23:14.080 --> 00:23:16.560]   but he's also a great gadget guy and a tech guy too.
[00:23:16.560 --> 00:23:17.480]   Let's see, turn it on.
[00:23:17.480 --> 00:23:20.480]   Editor in chief of CNET who literally knows everything.
[00:23:20.480 --> 00:23:23.560]   Well, that's Litten figuratively knows her.
[00:23:23.560 --> 00:23:24.400]   I don't know.
[00:23:24.400 --> 00:23:25.720]   See, now I'm thinking she's an editor.
[00:23:25.720 --> 00:23:26.880]   She's going to read--
[00:23:26.880 --> 00:23:30.960]   by the way, if you haven't seen the post yet, oh my god.
[00:23:30.960 --> 00:23:32.360]   We haven't.
[00:23:32.360 --> 00:23:34.320]   Anybody's in journalism.
[00:23:34.320 --> 00:23:36.960]   This will make you stand up and salute the flag.
[00:23:36.960 --> 00:23:37.680]   You'll go, yes.
[00:23:37.680 --> 00:23:39.280]   You're going to make me stressed out whenever
[00:23:39.280 --> 00:23:40.920]   I watch journalism, movies, or TV shows.
[00:23:40.920 --> 00:23:42.360]   I just get stressed out the whole time,
[00:23:42.360 --> 00:23:43.400]   because I keep thinking, how would I--
[00:23:43.400 --> 00:23:44.240]   It's too close to you.
[00:23:44.240 --> 00:23:44.920]   It's a Spielberg.
[00:23:44.920 --> 00:23:45.440]   It is.
[00:23:45.440 --> 00:23:46.440]   It's a Spielberg.
[00:23:46.440 --> 00:23:47.280]   It's a Spielberg.
[00:23:47.280 --> 00:23:48.680]   There are no good news rooms.
[00:23:48.680 --> 00:23:50.160]   It's a Spielberg.
[00:23:50.160 --> 00:23:52.600]   It just reminds you what you should be doing at that moment.
[00:23:52.600 --> 00:23:53.100]   Maybe.
[00:23:53.100 --> 00:23:54.120]   I didn't let it spill bombs.
[00:23:54.120 --> 00:23:55.560]   I feel like I got to get to work.
[00:23:55.560 --> 00:23:56.080]   Yeah.
[00:23:56.080 --> 00:23:57.800]   Yeah, I should do more of this.
[00:23:57.800 --> 00:23:59.280]   It's just-- you can't enjoy.
[00:23:59.280 --> 00:24:01.320]   Tom Hanks has been Bradley.
[00:24:01.320 --> 00:24:05.880]   Meryl Streep is a great-- a gram, Catherine Graham.
[00:24:05.880 --> 00:24:09.000]   What's funny is you've got these superstars.
[00:24:09.000 --> 00:24:10.720]   And then early in the scene in the movie,
[00:24:10.720 --> 00:24:12.440]   they're sitting having breakfast.
[00:24:12.440 --> 00:24:15.640]   And maybe it's me looking at them going, Tom Hanks,
[00:24:15.640 --> 00:24:16.720]   Meryl Streep.
[00:24:16.720 --> 00:24:19.520]   But I get the feeling in their heads, they were thinking,
[00:24:19.520 --> 00:24:20.880]   Tom Hanks and Meryl Streep.
[00:24:20.880 --> 00:24:23.920]   They were like a little--
[00:24:23.920 --> 00:24:25.840]   and they were almost overplaying it
[00:24:25.840 --> 00:24:29.360]   like, we're the greatest actors of our generation.
[00:24:29.360 --> 00:24:31.520]   But it's still, the story is so good.
[00:24:31.520 --> 00:24:34.320]   And it does make you feel really good about journalism.
[00:24:34.320 --> 00:24:35.840]   So I don't know why I brought it up,
[00:24:35.840 --> 00:24:37.160]   but you're editor and chief.
[00:24:37.160 --> 00:24:38.120]   Because we're journalists.
[00:24:38.120 --> 00:24:38.880]   Because we care.
[00:24:38.880 --> 00:24:39.400]   You should go.
[00:24:39.400 --> 00:24:39.880]   You should go.
[00:24:39.880 --> 00:24:40.240]   It's a good man.
[00:24:40.240 --> 00:24:41.840]   Have you seen it, Georgia?
[00:24:41.840 --> 00:24:44.520]   No, and anything that has to do with my work,
[00:24:44.520 --> 00:24:46.120]   I can't really enjoy either.
[00:24:46.120 --> 00:24:47.120]   So I fully understand it.
[00:24:47.120 --> 00:24:49.120]   Anything that has psychology in it that I'm doing--
[00:24:49.120 --> 00:24:49.640]   Oh, yeah.
[00:24:49.640 --> 00:24:51.080]   --working at the same time.
[00:24:51.080 --> 00:24:52.280]   I think that this one, though, might--
[00:24:52.280 --> 00:24:55.960]   I like the feel good kind of triumph and difficulty.
[00:24:55.960 --> 00:24:56.360]   I don't know.
[00:24:56.360 --> 00:24:56.560]   I'll see.
[00:24:56.560 --> 00:24:57.840]   It's a Spielberg.
[00:24:57.840 --> 00:25:01.280]   So you know, it's not going to be complex.
[00:25:01.280 --> 00:25:04.880]   It's going to be a great story with the ties up
[00:25:04.880 --> 00:25:07.080]   and with the bow at the end.
[00:25:07.080 --> 00:25:08.640]   No moral ambiguity.
[00:25:08.640 --> 00:25:10.960]   It even has all great--
[00:25:10.960 --> 00:25:12.080]   No spoilers, Leo.
[00:25:12.080 --> 00:25:13.280]   No spoilers.
[00:25:13.280 --> 00:25:15.560]   You might know what happened, but OK.
[00:25:15.560 --> 00:25:16.840]   It is, after all, history.
[00:25:16.840 --> 00:25:18.880]   It's not like it's a true story.
[00:25:18.880 --> 00:25:22.480]   But it even has, in a throwback to the classic movies,
[00:25:22.480 --> 00:25:24.560]   it's got the presses.
[00:25:24.560 --> 00:25:25.600]   The guy hits the press.
[00:25:25.600 --> 00:25:26.600]   The press's turn.
[00:25:26.600 --> 00:25:27.560]   You see the presses?
[00:25:27.560 --> 00:25:28.800]   They throw them on the truck.
[00:25:28.800 --> 00:25:30.920]   The trucks come bouncing out of the factory,
[00:25:30.920 --> 00:25:31.920]   throwing the newspaper.
[00:25:31.920 --> 00:25:32.400]   Oh, my gosh.
[00:25:32.400 --> 00:25:34.120]   Does anybody else stop the presses?
[00:25:34.120 --> 00:25:35.240]   No, almost, though.
[00:25:35.240 --> 00:25:36.480]   The presses are stopped.
[00:25:36.480 --> 00:25:37.840]   They yell start the presses.
[00:25:37.840 --> 00:25:38.360]   OK.
[00:25:38.360 --> 00:25:38.960]   How about that?
[00:25:38.960 --> 00:25:39.480]   That's--
[00:25:39.480 --> 00:25:39.920]   How about that?
[00:25:39.920 --> 00:25:41.480]   You're good.
[00:25:41.480 --> 00:25:43.960]   It's unexpected.
[00:25:43.960 --> 00:25:45.160]   It's a great movie.
[00:25:45.160 --> 00:25:46.080]   It's a great movie.
[00:25:46.080 --> 00:25:46.880]   I really enjoyed it.
[00:25:46.880 --> 00:25:48.080]   Now I'm going to brush my teeth.
[00:25:48.080 --> 00:25:50.480]   So if you don't mind, we're going to take a little break.
[00:25:50.480 --> 00:25:56.120]   While Leo pays attention to his dental hygiene,
[00:25:56.120 --> 00:25:58.520]   they're all looking at me strangely.
[00:25:58.520 --> 00:26:00.640]   This is our sponsor for the day.
[00:26:00.640 --> 00:26:01.920]   Quip.
[00:26:01.920 --> 00:26:03.320]   Look at this.
[00:26:03.320 --> 00:26:05.080]   Now everybody-- well, you probably
[00:26:05.080 --> 00:26:07.440]   should know that an electric toothbrush is
[00:26:07.440 --> 00:26:09.080]   a better way to brush your teeth.
[00:26:09.080 --> 00:26:10.800]   Not just for lazy people.
[00:26:10.800 --> 00:26:12.760]   My daughter says, I'm not going to get an electric toothbrush.
[00:26:12.760 --> 00:26:14.160]   That's for lazy people.
[00:26:14.160 --> 00:26:17.640]   Well, but you're going to get better checkups
[00:26:17.640 --> 00:26:19.720]   because you brush your teeth better.
[00:26:19.720 --> 00:26:22.120]   This quip toothbrush is fabulous.
[00:26:22.120 --> 00:26:23.520]   30 seconds per sector.
[00:26:23.520 --> 00:26:25.880]   It times you gives you a little beep.
[00:26:25.880 --> 00:26:30.720]   But the best part of it is it comes by mail.
[00:26:30.720 --> 00:26:34.960]   You get the refills, the resupplies of the heads automatically.
[00:26:34.960 --> 00:26:37.280]   It is a beautiful electric toothbrush.
[00:26:37.280 --> 00:26:38.280]   And look at this.
[00:26:38.280 --> 00:26:39.440]   It has a little strip here.
[00:26:39.440 --> 00:26:41.720]   You actually adhere this to your mirror
[00:26:41.720 --> 00:26:43.600]   or to your counter somewhere.
[00:26:43.600 --> 00:26:45.400]   And the toothbrush sits there so you never forget
[00:26:45.400 --> 00:26:46.760]   to brush your teeth.
[00:26:46.760 --> 00:26:47.440]   Isn't that nice?
[00:26:47.440 --> 00:26:48.600]   How do you charge it?
[00:26:48.600 --> 00:26:49.760]   Ah, you don't have to charge it.
[00:26:49.760 --> 00:26:52.320]   It's got a double A battery in it.
[00:26:52.320 --> 00:26:53.840]   So that's even better, right?
[00:26:53.840 --> 00:26:55.560]   You don't have to just--
[00:26:55.560 --> 00:26:56.440]   I'm glad you asked that.
[00:26:56.440 --> 00:26:57.800]   You just put it on your mirror.
[00:26:57.800 --> 00:27:01.120]   And then you will get automatically.
[00:27:01.120 --> 00:27:03.520]   You'll get the new heads every three months.
[00:27:03.520 --> 00:27:04.960]   See, everybody says, oh, I got to remember
[00:27:04.960 --> 00:27:06.800]   to change my toothbrush, but never does it.
[00:27:06.800 --> 00:27:08.320]   This way you get it automatically.
[00:27:08.320 --> 00:27:10.280]   You also get tips I confess.
[00:27:10.280 --> 00:27:11.200]   How old am I?
[00:27:11.200 --> 00:27:13.360]   I should know how to brush my teeth, right?
[00:27:13.360 --> 00:27:16.480]   You get tips on dental care that I didn't even know.
[00:27:16.480 --> 00:27:20.440]   It's backed by over 10,000 dental professionals.
[00:27:20.440 --> 00:27:23.040]   For instance, I'm reading the instructions
[00:27:23.040 --> 00:27:24.720]   that says, don't rinse after you brush.
[00:27:24.720 --> 00:27:26.680]   I always rinse after I brush.
[00:27:26.680 --> 00:27:29.240]   Because, of course, that's how the fluoride
[00:27:29.240 --> 00:27:31.880]   is getting in your teeth, dummy.
[00:27:31.880 --> 00:27:34.800]   So even here I am at my age, learning how to brush my teeth
[00:27:34.800 --> 00:27:35.960]   better.
[00:27:35.960 --> 00:27:38.200]   It is fabulous, by the way, toothpaste.
[00:27:38.200 --> 00:27:38.960]   Tastes great.
[00:27:38.960 --> 00:27:41.000]   Gives you a nice minty flavor.
[00:27:41.000 --> 00:27:42.560]   Strengthens your teeth.
[00:27:42.560 --> 00:27:44.760]   And of course, it's good for your teeth.
[00:27:44.760 --> 00:27:48.000]   It's got sodium monofluorophosphate or whatever
[00:27:48.000 --> 00:27:51.360]   that is to strengthen your teeth
[00:27:51.360 --> 00:27:53.400]   and make sure you don't get cavities.
[00:27:53.400 --> 00:27:55.400]   They've combined dentistry and design
[00:27:55.400 --> 00:27:57.840]   to make a better electric toothbrush.
[00:27:57.840 --> 00:28:00.000]   And you get new brush heads every three months
[00:28:00.000 --> 00:28:04.440]   for just $5, including free shipping worldwide.
[00:28:04.440 --> 00:28:05.080]   I love that.
[00:28:05.080 --> 00:28:08.440]   Most toothbrushes do not cannot claim
[00:28:08.440 --> 00:28:11.400]   to be one of Time Magazine's best inventions of the year.
[00:28:11.400 --> 00:28:12.560]   Quip can.
[00:28:12.560 --> 00:28:16.560]   QUIP, it starts at $25.
[00:28:16.560 --> 00:28:19.280]   That's a lot less than those big names.
[00:28:19.280 --> 00:28:21.600]   It works just as well or even better.
[00:28:21.600 --> 00:28:23.080]   I think it's more convenient.
[00:28:23.080 --> 00:28:25.560]   And if you go to getquip.com/twit right now,
[00:28:25.560 --> 00:28:28.640]   you'll get your first refill pack free when you purchase
[00:28:28.640 --> 00:28:30.280]   any quip-electric toothbrush.
[00:28:30.280 --> 00:28:34.200]   Here's a tip for moms, Lindsay.
[00:28:34.200 --> 00:28:36.280]   Your kids are-- are they old enough to have sleepovers yet?
[00:28:36.280 --> 00:28:37.440]   Oh, yeah.
[00:28:37.440 --> 00:28:38.800]   Kids never bring their toothbrush to sleep.
[00:28:38.800 --> 00:28:39.800]   That's true.
[00:28:39.800 --> 00:28:41.240]   So we got a couple extra heads.
[00:28:41.240 --> 00:28:42.040]   You put it in my own.
[00:28:42.040 --> 00:28:43.120]   Yes.
[00:28:43.120 --> 00:28:44.520]   We got a couple extra heads.
[00:28:44.520 --> 00:28:46.240]   And we put their names on them.
[00:28:46.240 --> 00:28:48.040]   And when they come for the sleepover,
[00:28:48.040 --> 00:28:50.440]   they'd end your toothbrushes right there.
[00:28:50.440 --> 00:28:52.240]   It's awesome.
[00:28:52.240 --> 00:28:54.320]   And then they go home with Minty Fresh Breath.
[00:28:54.320 --> 00:28:59.760]   Get your first refill pack free, getquip.com/twit.
[00:28:59.760 --> 00:29:01.240]   I just love the packaging and everything.
[00:29:01.240 --> 00:29:02.120]   It's just very modern.
[00:29:02.120 --> 00:29:07.240]   Getquip, G-E-T-Q-U-I-P.com/twit.
[00:29:07.240 --> 00:29:09.080]   And yes, they have a little instruction pamphlet
[00:29:09.080 --> 00:29:13.360]   that I learned finally had a brush my teeth right.
[00:29:13.360 --> 00:29:15.200]   Getquip.
[00:29:15.200 --> 00:29:16.040]   Don't rinse.
[00:29:16.040 --> 00:29:17.560]   I'm just all about the battery.
[00:29:17.560 --> 00:29:20.360]   I don't like having to plug in a charger in my house.
[00:29:20.360 --> 00:29:21.760]   Right.
[00:29:21.760 --> 00:29:23.880]   If you have no space or no plugs,
[00:29:23.880 --> 00:29:24.600]   that's really nice.
[00:29:24.600 --> 00:29:25.560]   You're going camping.
[00:29:25.560 --> 00:29:26.060]   Yeah.
[00:29:26.060 --> 00:29:26.720]   I don't have to worry.
[00:29:26.720 --> 00:29:29.160]   It's great for-- in the battery last months,
[00:29:29.160 --> 00:29:31.680]   I mean, it's not like it runs out or anything.
[00:29:31.680 --> 00:29:32.960]   It's not wasteful.
[00:29:32.960 --> 00:29:36.200]   If you wanted to, you could put a rechargeable in there.
[00:29:36.200 --> 00:29:37.520]   Sometimes I'll do that.
[00:29:37.520 --> 00:29:38.840]   But yeah, really nice.
[00:29:38.840 --> 00:29:40.480]   Read really, really good.
[00:29:40.480 --> 00:29:41.760]   Getquip.com/twit.
[00:29:41.760 --> 00:29:43.680]   OK, enough of that.
[00:29:43.680 --> 00:29:44.360]   Gonna put away.
[00:29:44.360 --> 00:29:47.200]   I was going to brush my teeth, but I thought no.
[00:29:47.200 --> 00:29:48.320]   You were going to do it on air?
[00:29:48.320 --> 00:29:50.080]   Yeah, I did it last time.
[00:29:50.080 --> 00:29:52.800]   Well, if you were alone here, maybe you'd be different.
[00:29:52.800 --> 00:29:55.320]   Yeah, I didn't want to upset you.
[00:29:55.320 --> 00:29:56.080]   I can handle it.
[00:29:56.080 --> 00:29:56.600]   Yeah.
[00:29:56.600 --> 00:29:58.000]   Well--
[00:29:58.000 --> 00:29:59.320]   You didn't want to traumatize you.
[00:29:59.320 --> 00:30:01.840]   It's a little intimate.
[00:30:01.840 --> 00:30:03.640]   I have shaved on this show.
[00:30:03.640 --> 00:30:07.160]   I have brushed my teeth on this show.
[00:30:07.160 --> 00:30:09.160]   I'm not going to say what happened with the one-shot Charlie
[00:30:09.160 --> 00:30:09.640]   butt wipes.
[00:30:09.640 --> 00:30:11.200]   But that's enough of that.
[00:30:11.200 --> 00:30:12.720]   We're going to move on right now.
[00:30:12.720 --> 00:30:13.720]   Whoo.
[00:30:13.720 --> 00:30:14.560]   Whoo.
[00:30:14.560 --> 00:30:15.560]   Whoo.
[00:30:15.560 --> 00:30:16.060]   Oh.
[00:30:16.060 --> 00:30:17.060]   Oh, I guess we--
[00:30:17.060 --> 00:30:19.660]   We locked out today.
[00:30:19.660 --> 00:30:24.420]   Hey, what's the status of self-driving cars, Tim?
[00:30:24.420 --> 00:30:27.900]   Is we seeing any advance here?
[00:30:27.900 --> 00:30:31.340]   I have to say, having had driven a Tesla now for over a year,
[00:30:31.340 --> 00:30:36.140]   year and a half, that's not autopilot.
[00:30:36.140 --> 00:30:37.500]   No, it's not.
[00:30:37.500 --> 00:30:38.880]   It's a nice cruise control.
[00:30:38.880 --> 00:30:40.620]   It's a nice cruise control.
[00:30:40.620 --> 00:30:42.540]   Right, Tesla's was the best for a little while,
[00:30:42.540 --> 00:30:45.100]   but they're not the best any longer right now.
[00:30:45.100 --> 00:30:48.180]   In terms of what's available to the mass market, GM Supercruise
[00:30:48.180 --> 00:30:50.580]   is the closest thing we have to autonomy.
[00:30:50.580 --> 00:30:54.020]   With that in the new Cadillac CT6 and soon coming to other--
[00:30:54.020 --> 00:30:56.500]   excuse me, coming to other Cadlax,
[00:30:56.500 --> 00:30:59.020]   you'll be able to actually take your hands off the wheel
[00:30:59.020 --> 00:31:00.540]   and get on the highway.
[00:31:00.540 --> 00:31:03.380]   So basically, on designated roads that that Cadillac knows
[00:31:03.380 --> 00:31:05.380]   has mapped in high enough resolution,
[00:31:05.380 --> 00:31:07.020]   and conditions where the car can actually
[00:31:07.020 --> 00:31:09.660]   see the lines on the road, you get on the dial
[00:31:09.660 --> 00:31:12.180]   light, you press a button, the steering wheel turns blue,
[00:31:12.180 --> 00:31:13.540]   and then you can take your hands off the wheel,
[00:31:13.540 --> 00:31:14.740]   and you are good to go.
[00:31:14.740 --> 00:31:16.700]   You still need to pay attention, but you can look away for,
[00:31:16.700 --> 00:31:19.580]   I think, upwards of 20 seconds sometimes.
[00:31:19.580 --> 00:31:22.060]   So you can't be just reading a book or something like that,
[00:31:22.060 --> 00:31:24.820]   but the car will take care of itself and do a pretty good job.
[00:31:24.820 --> 00:31:26.980]   How does it look like you know when you need to pay attention?
[00:31:26.980 --> 00:31:29.380]   Like when it's like, hey, hey, hey, Tesla.
[00:31:29.380 --> 00:31:32.180]   It has a whole series of escalating alerts to the point
[00:31:32.180 --> 00:31:33.580]   where it actually will slow down.
[00:31:33.580 --> 00:31:35.540]   Yeah.
[00:31:35.540 --> 00:31:39.060]   For Cadillac, it blinks at you very, very noticeably,
[00:31:39.060 --> 00:31:40.940]   and sounds very loud, chime as well.
[00:31:40.940 --> 00:31:41.900]   So it'll give you a warning.
[00:31:41.900 --> 00:31:44.180]   I think it's probably about a 20 or 30-second warning saying,
[00:31:44.180 --> 00:31:47.100]   we're coming to the end of the road that I know how to map,
[00:31:47.100 --> 00:31:49.380]   or if the conditions are changing, if the weather's bad,
[00:31:49.380 --> 00:31:50.860]   suddenly you can't see very well.
[00:31:50.860 --> 00:31:52.060]   It'll give you a pretty good warning,
[00:31:52.060 --> 00:31:54.340]   and it'll start to chime at some point, the car will eventually
[00:31:54.340 --> 00:31:57.020]   slow down and stop if you do not respond at some point.
[00:31:57.020 --> 00:31:58.620]   So that actually does a pretty good job at the handover,
[00:31:58.620 --> 00:32:00.540]   and it has an infrared camera actually watching you as well
[00:32:00.540 --> 00:32:02.100]   to make sure that you are paying attention.
[00:32:02.100 --> 00:32:04.340]   Now you are able to take control of the car back.
[00:32:04.340 --> 00:32:06.740]   So this is an expensive option in a high-end Cadillac,
[00:32:06.740 --> 00:32:09.300]   but presumably GM is going to migrate it to some other--
[00:32:09.300 --> 00:32:12.500]   will they put it like in the bold, for instance?
[00:32:12.500 --> 00:32:14.940]   We should definitely expect to see it across the entire GM line.
[00:32:14.940 --> 00:32:17.540]   And Nissan's actually got a really nice system in the leaf
[00:32:17.540 --> 00:32:18.620]   called ProPile Assist.
[00:32:18.620 --> 00:32:19.980]   That's not quite that comprehensive.
[00:32:19.980 --> 00:32:21.860]   You can't take your hands off the wheel.
[00:32:21.860 --> 00:32:23.460]   But I was speaking with a Nissan engineer
[00:32:23.460 --> 00:32:25.140]   in last week in Detroit, actually.
[00:32:25.140 --> 00:32:28.260]   And they plan to have at least be able to automatically
[00:32:28.260 --> 00:32:30.820]   change lanes on their own by next year.
[00:32:30.820 --> 00:32:32.860]   And by 2020, they want to have this ProPile system
[00:32:32.860 --> 00:32:35.540]   be able to actually stop automatically at intersections.
[00:32:35.540 --> 00:32:37.540]   Right now, if you're behind somebody and that person stops,
[00:32:37.540 --> 00:32:39.020]   the car will stop and follow them.
[00:32:39.020 --> 00:32:41.580]   So it will stop at intersections if you can kind of follow
[00:32:41.580 --> 00:32:42.580]   somebody else.
[00:32:42.580 --> 00:32:44.740]   But in 2020, they say the system will be good enough
[00:32:44.740 --> 00:32:47.780]   to be able to stop at a red light or stop at a stop sign
[00:32:47.780 --> 00:32:49.460]   and proceed through all its own.
[00:32:49.460 --> 00:32:51.860]   At CES, hosting a panel about autonomous driving
[00:32:51.860 --> 00:32:53.500]   and Tim helped me prepare for this a lot.
[00:32:53.500 --> 00:32:54.900]   But one of the interesting things
[00:32:54.900 --> 00:32:58.060]   is that there was a product guy from Nissan on the panel.
[00:32:58.060 --> 00:33:02.660]   And his whole platform for autonomous driving
[00:33:02.660 --> 00:33:04.860]   was-- and it seems like Nissan is actually
[00:33:04.860 --> 00:33:07.540]   doing this in practice as it's all going to be incremental.
[00:33:07.540 --> 00:33:09.220]   We're not going to just one day wake up.
[00:33:09.220 --> 00:33:11.900]   And it's going to be like January 1, 2025.
[00:33:11.900 --> 00:33:13.620]   We're all on an autonomous--
[00:33:13.620 --> 00:33:14.540]   That makes sense.
[00:33:14.540 --> 00:33:15.820]   It's like a dairy thing.
[00:33:15.820 --> 00:33:16.460]   We've got to jush.
[00:33:16.460 --> 00:33:17.100]   We've got to jush.
[00:33:17.100 --> 00:33:18.100]   Yeah.
[00:33:18.100 --> 00:33:19.100]   We've got to get comfortable.
[00:33:19.100 --> 00:33:20.460]   It's actually a very different approach
[00:33:20.460 --> 00:33:22.300]   than a lot of other companies like Waymo, for example,
[00:33:22.300 --> 00:33:24.260]   were jumping right to level four, level five,
[00:33:24.260 --> 00:33:27.140]   which is where there's basically no driver controls.
[00:33:27.140 --> 00:33:29.260]   And there's different arguments, one way or the other.
[00:33:29.260 --> 00:33:30.580]   On one hand, it kind of makes sense
[00:33:30.580 --> 00:33:32.220]   to be iterative because at that point,
[00:33:32.220 --> 00:33:34.980]   you can kind of progressively make things better and smarter.
[00:33:34.980 --> 00:33:38.820]   But on the other hand, that gray area between truly driverless
[00:33:38.820 --> 00:33:41.300]   and all human driven is actually pretty complicated
[00:33:41.300 --> 00:33:43.500]   because now not only do you need to make the car able to drive
[00:33:43.500 --> 00:33:45.940]   itself some of the time, but it has to actually watch
[00:33:45.940 --> 00:33:48.300]   the driver and hand control back over to the driver.
[00:33:48.300 --> 00:33:50.580]   And that is actually really, really complicated to know
[00:33:50.580 --> 00:33:52.580]   if I'm paying attention, if I'm in a state where I'm
[00:33:52.580 --> 00:33:54.620]   ready to take control of the car back,
[00:33:54.620 --> 00:33:57.540]   what's my heart rate, are my eyes open, and my drunk.
[00:33:57.540 --> 00:34:00.100]   The car needs to figure out all that before it can safely hand
[00:34:00.100 --> 00:34:01.060]   control back to you.
[00:34:01.060 --> 00:34:03.780]   And that problem right there, if you make a car that's fully
[00:34:03.780 --> 00:34:05.420]   driverless, you don't have to worry about that at all.
[00:34:05.420 --> 00:34:07.180]   And so that's why we see a lot of companies like Waymo
[00:34:07.180 --> 00:34:09.380]   saying, forget the whole intermediary steps.
[00:34:09.380 --> 00:34:10.860]   We're just going to make cars that don't even have
[00:34:10.860 --> 00:34:11.660]   steering wheels.
[00:34:11.660 --> 00:34:13.100]   And then we won't even have to worry about doing that.
[00:34:13.100 --> 00:34:15.060]   I cannot understand that because one of the problems
[00:34:15.060 --> 00:34:19.220]   with this half autonomy is the time it takes for you
[00:34:19.220 --> 00:34:21.540]   to kind of readjust to driving.
[00:34:21.540 --> 00:34:24.940]   So if you're letting the Tesla or the Leaf drive or the
[00:34:24.940 --> 00:34:28.020]   Cadillac drive, and you suddenly need to take control,
[00:34:28.020 --> 00:34:30.540]   unless you've been really paying attention as if you
[00:34:30.540 --> 00:34:35.420]   were driving, there's a few seconds to just rock what's
[00:34:35.420 --> 00:34:35.820]   going on.
[00:34:35.820 --> 00:34:36.300]   Where am I?
[00:34:36.300 --> 00:34:37.180]   What's happening?
[00:34:37.180 --> 00:34:37.980]   And that's another--
[00:34:37.980 --> 00:34:39.020]   And the most more dangerous.
[00:34:39.020 --> 00:34:40.620]   Yeah, I think it's more dangerous, exactly.
[00:34:40.620 --> 00:34:44.420]   Because you're getting used to being in a car and not really
[00:34:44.420 --> 00:34:46.060]   paying attention.
[00:34:46.060 --> 00:34:49.180]   That midpoint becomes then when you need to pay attention,
[00:34:49.180 --> 00:34:52.980]   by the time that's milliseconds that it takes for your brain
[00:34:52.980 --> 00:34:56.340]   to kind of be fully activated, that's a really dangerous set
[00:34:56.340 --> 00:34:56.780]   of time.
[00:34:56.780 --> 00:34:58.940]   So you should either be paying attention or the car should
[00:34:58.940 --> 00:35:00.540]   be able to deal with it.
[00:35:00.540 --> 00:35:04.060]   And that midpoint, I think that people end up--
[00:35:04.060 --> 00:35:07.500]   we already think that we can check our cell phone, do this,
[00:35:07.500 --> 00:35:09.260]   eat something, chat with this person,
[00:35:09.260 --> 00:35:11.780]   sing, dance around in the car, which is already using up
[00:35:11.780 --> 00:35:13.060]   a certain amount of our attention.
[00:35:13.060 --> 00:35:14.940]   So yeah, I agree with you, Leo.
[00:35:14.940 --> 00:35:17.260]   It's kind of one of those things where it might be even more
[00:35:17.260 --> 00:35:18.420]   dangerous to have an endpoint.
[00:35:18.420 --> 00:35:20.940]   The thing I like about the incremental progress is that
[00:35:20.940 --> 00:35:23.900]   people still will have to learn how to drive.
[00:35:23.900 --> 00:35:26.340]   Because I think there's this sort of weird--
[00:35:26.340 --> 00:35:29.060]   Yeah, once it's all self-driving, you'll never learn.
[00:35:29.060 --> 00:35:30.820]   But maybe you need to.
[00:35:30.820 --> 00:35:31.700]   I'm not sure why.
[00:35:31.700 --> 00:35:33.260]   There's no steering wheel.
[00:35:33.260 --> 00:35:34.700]   I think it's going to be a long time.
[00:35:34.700 --> 00:35:35.980]   It's going to be a very long time.
[00:35:35.980 --> 00:35:39.620]   I don't want to have to drive a buggy.
[00:35:39.620 --> 00:35:40.300]   Well, seriously--
[00:35:40.300 --> 00:35:42.460]   I can't get a horse to move in the right direction.
[00:35:42.460 --> 00:35:42.980]   That's true.
[00:35:42.980 --> 00:35:44.460]   But I think it's going to be a very--
[00:35:44.460 --> 00:35:47.260]   if people only buy new cars every seven years,
[00:35:47.260 --> 00:35:49.220]   there's going to be this very long transition time
[00:35:49.220 --> 00:35:50.300]   where people are still driving--
[00:35:50.300 --> 00:35:52.460]   I mean, heck, people are still driving manual transmissions.
[00:35:52.460 --> 00:35:54.380]   We're teaching our 15-year-old a drive, of course.
[00:35:54.380 --> 00:35:54.820]   Sure.
[00:35:54.820 --> 00:35:57.180]   And so you want to be able to--
[00:35:57.180 --> 00:36:01.460]   I'm driving my old whatever on the highway.
[00:36:01.460 --> 00:36:03.580]   And I get stuck or something happens.
[00:36:03.580 --> 00:36:07.020]   I want my son to be able to hop up to the front seat and drive.
[00:36:07.020 --> 00:36:08.100]   It just feels like a bicycle you just got.
[00:36:08.100 --> 00:36:08.620]   Wait a minute.
[00:36:08.620 --> 00:36:10.060]   You really think about that?
[00:36:10.060 --> 00:36:13.180]   What if I have a stroke and my son has to take over?
[00:36:13.180 --> 00:36:14.700]   Like, he's your co-pilot?
[00:36:14.700 --> 00:36:17.140]   More like, what if he just needs to drive my car?
[00:36:17.140 --> 00:36:17.700]   I don't know why.
[00:36:17.700 --> 00:36:18.300]   For whatever reason.
[00:36:18.300 --> 00:36:19.220]   For whatever reason.
[00:36:19.220 --> 00:36:20.100]   How old is he?
[00:36:20.100 --> 00:36:21.020]   He's 14 now.
[00:36:21.020 --> 00:36:21.940]   He's never learning to drive.
[00:36:21.940 --> 00:36:23.100]   Actually, this is all mood.
[00:36:23.100 --> 00:36:23.100]   I know.
[00:36:23.100 --> 00:36:24.100]   I'm not going to let him learn.
[00:36:24.100 --> 00:36:25.260]   I'm just going to say.
[00:36:25.260 --> 00:36:26.780]   You don't want your son to drive.
[00:36:26.780 --> 00:36:27.300]   Trust me.
[00:36:27.300 --> 00:36:28.940]   You do not want him to learn how to drive.
[00:36:28.940 --> 00:36:30.580]   It's a bad, bad idea.
[00:36:30.580 --> 00:36:31.100]   We are lucky.
[00:36:31.100 --> 00:36:33.580]   My son didn't want to drive until he was 18.
[00:36:33.580 --> 00:36:36.380]   My daughter, 15 and 1/2, which is the youngest you can do it
[00:36:36.380 --> 00:36:39.620]   in California, she was at DMV.
[00:36:39.620 --> 00:36:42.220]   Yeah, she wasted no time.
[00:36:42.220 --> 00:36:44.460]   And she's, by the way, terrible driver.
[00:36:44.460 --> 00:36:45.620]   Oh.
[00:36:45.620 --> 00:36:47.020]   And my son's a great driver.
[00:36:47.020 --> 00:36:47.660]   So I don't know if there's--
[00:36:47.660 --> 00:36:50.060]   Do you're hoping to have longer to practice?
[00:36:50.060 --> 00:36:51.820]   You're hoping to practice more years to practice.
[00:36:51.820 --> 00:36:52.700]   Really soon.
[00:36:52.700 --> 00:36:53.300]   Soon.
[00:36:53.300 --> 00:36:54.340]   Oh, man.
[00:36:54.340 --> 00:36:55.300]   The sooner the--
[00:36:55.300 --> 00:36:58.580]   Well, but isn't that really the argument for autonomous
[00:36:58.580 --> 00:37:02.780]   is that regardless of how bad autonomous is,
[00:37:02.780 --> 00:37:04.780]   humans are worse.
[00:37:04.780 --> 00:37:05.380]   I agree.
[00:37:05.380 --> 00:37:05.380]   Certainly.
[00:37:05.380 --> 00:37:08.180]   The vast majority of crashes that we see are human cause.
[00:37:08.180 --> 00:37:09.580]   Human error.
[00:37:09.580 --> 00:37:09.900]   Right.
[00:37:09.900 --> 00:37:12.260]   But those are-- we're so used to that at this point.
[00:37:12.260 --> 00:37:14.060]   The people just go, oh, that's a shame and move on.
[00:37:14.060 --> 00:37:17.380]   But when robot cars make a mistake and kill people,
[00:37:17.380 --> 00:37:20.100]   as we've seen with the unfortunate autopilot accident,
[00:37:20.100 --> 00:37:22.860]   but previously, it becomes a major story.
[00:37:22.860 --> 00:37:25.100]   And it's going to be a major story for a very long time.
[00:37:25.100 --> 00:37:28.500]   And people won't give robot cars the same kind of--
[00:37:28.500 --> 00:37:29.940]   slack that they cut human beings.
[00:37:29.940 --> 00:37:32.820]   So it is going to be a major adjustment for a lot of people,
[00:37:32.820 --> 00:37:34.980]   despite the fact that these cars are already much safer
[00:37:34.980 --> 00:37:36.740]   than human drivers by and large.
[00:37:36.740 --> 00:37:38.580]   There's going to be a lot of trust built up
[00:37:38.580 --> 00:37:41.020]   before people really trust them to be on the road
[00:37:41.020 --> 00:37:43.700]   and be commuting next to somebody who's watching a movie,
[00:37:43.700 --> 00:37:45.060]   reading a book, or taking a nap.
[00:37:45.060 --> 00:37:47.420]   Georgia, how do you get people--
[00:37:47.420 --> 00:37:49.220]   how would you, as a psychologist,
[00:37:49.220 --> 00:37:54.580]   approach getting people used to the idea of getting in a car
[00:37:54.580 --> 00:37:57.220]   without a steering wheel or a brake pedal?
[00:37:57.220 --> 00:37:57.900]   I mean--
[00:37:57.900 --> 00:38:00.020]   I think that without a steering wheel and a brake pedal,
[00:38:00.020 --> 00:38:02.860]   I think, as Lindsay said, the incremental is easier for us
[00:38:02.860 --> 00:38:06.180]   to cope with and not feel that type of anxiety.
[00:38:06.180 --> 00:38:09.980]   I feel that an autonomous car would be much safer,
[00:38:09.980 --> 00:38:11.660]   yet I would not feel comfortable driving
[00:38:11.660 --> 00:38:15.060]   without having the-- if I wanted to take over,
[00:38:15.060 --> 00:38:16.260]   that I couldn't.
[00:38:16.260 --> 00:38:18.780]   And in the end, what happens is habituation.
[00:38:18.780 --> 00:38:22.220]   When we see something enough after a while,
[00:38:22.220 --> 00:38:24.980]   even if it's scary or dangerous, our brains go,
[00:38:24.980 --> 00:38:25.300]   you know what?
[00:38:25.300 --> 00:38:26.860]   I guess it's not that bad, because I've
[00:38:26.860 --> 00:38:27.860]   seen it all--
[00:38:27.860 --> 00:38:29.540]   So you said we could get used to it?
[00:38:29.540 --> 00:38:30.540]   --or much traumatized.
[00:38:30.540 --> 00:38:32.540]   We will get used to it, for sure.
[00:38:32.540 --> 00:38:34.620]   But with everything, there's a certain amount of anxiety.
[00:38:34.620 --> 00:38:37.380]   Whenever we don't deal well with change,
[00:38:37.380 --> 00:38:39.580]   we deal well with incremental change.
[00:38:39.580 --> 00:38:43.180]   But large jumps in change causes a great amount of anxiety.
[00:38:43.180 --> 00:38:45.980]   And unfortunately, we are less and less adaptive
[00:38:45.980 --> 00:38:47.020]   as we get older.
[00:38:47.020 --> 00:38:49.020]   So when we're young, and if we've grown up with something,
[00:38:49.020 --> 00:38:50.380]   it's very comfortable for us.
[00:38:50.380 --> 00:38:54.020]   But as we get older, our ability to change environments
[00:38:54.020 --> 00:38:57.220]   and the status quo becomes much more difficult for us.
[00:38:57.220 --> 00:39:00.020]   So I think that also it will be the younger generations that
[00:39:00.020 --> 00:39:02.180]   will find this and kind of embrace it
[00:39:02.180 --> 00:39:04.500]   and want to try something new and cool.
[00:39:04.500 --> 00:39:06.660]   And those of us that are older that are not really
[00:39:06.660 --> 00:39:08.700]   into technology will be like, you know what?
[00:39:08.700 --> 00:39:10.980]   I'm just going to stick with the steering wheel and brake
[00:39:10.980 --> 00:39:12.900]   and take my chances.
[00:39:12.900 --> 00:39:16.020]   It seems like it would be better to do it instantly.
[00:39:16.020 --> 00:39:17.100]   Because--
[00:39:17.100 --> 00:39:19.260]   Especially for the truth.
[00:39:19.260 --> 00:39:20.780]   Well, there's lots of reasons.
[00:39:20.780 --> 00:39:22.660]   I mean, the incremental approach
[00:39:22.660 --> 00:39:24.940]   is going to be more traffic depth.
[00:39:24.940 --> 00:39:26.380]   So it's going to be more fatalities.
[00:39:26.380 --> 00:39:28.580]   And there's going to, in fact, be an increase in fatalities
[00:39:28.580 --> 00:39:32.540]   from the bad intersection between self-driving cars
[00:39:32.540 --> 00:39:34.660]   and humans and the takeover.
[00:39:34.660 --> 00:39:35.540]   I mean, it's interesting.
[00:39:35.540 --> 00:39:37.620]   This happened in--
[00:39:37.620 --> 00:39:43.580]   I'm told by professional pilots in aeronautics and airplanes,
[00:39:43.580 --> 00:39:45.380]   civilian aviation.
[00:39:45.380 --> 00:39:51.060]   Airbus makes a far more automated plane than Boeing does.
[00:39:51.060 --> 00:39:53.780]   And pilots in the United States, particularly United
[00:39:53.780 --> 00:39:55.980]   Airlines, didn't want to get air buses.
[00:39:55.980 --> 00:39:58.580]   They didn't want to get the pilot out of the loop.
[00:39:58.580 --> 00:39:59.580]   Yeah, interesting.
[00:39:59.580 --> 00:40:01.100]   I mean, I think they all think they're going to have
[00:40:01.100 --> 00:40:02.940]   to land a plane on the plane, right?
[00:40:02.940 --> 00:40:04.100]   And it could happen.
[00:40:04.100 --> 00:40:04.780]   But it's--
[00:40:04.780 --> 00:40:06.060]   And it could happen.
[00:40:06.060 --> 00:40:09.380]   And probably an Airbus would not do a good job of that.
[00:40:09.380 --> 00:40:10.740]   But Sully would.
[00:40:10.740 --> 00:40:14.020]   Nevertheless, in the vast majority of situations,
[00:40:14.020 --> 00:40:16.260]   humans make-- even pilots make mistakes.
[00:40:16.260 --> 00:40:19.900]   There have been accidents that where the pilot's taken over
[00:40:19.900 --> 00:40:22.900]   and done the wrong thing and actually caused the plane
[00:40:22.900 --> 00:40:27.060]   to crash where an Airbus wouldn't have let them do that.
[00:40:27.060 --> 00:40:30.060]   And so it's the same thing as happening.
[00:40:30.060 --> 00:40:32.660]   And this is really the issue of self-driving versus human,
[00:40:32.660 --> 00:40:35.940]   self-flying planes versus human planes.
[00:40:35.940 --> 00:40:38.060]   I think that there's another point as well, which
[00:40:38.060 --> 00:40:42.100]   is that our economy is strongly driven, if I can say that,
[00:40:42.100 --> 00:40:45.020]   by the fact that we drive our own cars, the amount of people
[00:40:45.020 --> 00:40:47.980]   that are in taxis and that are fixing cars
[00:40:47.980 --> 00:40:50.020]   and that are dealing with that and driving schools.
[00:40:50.020 --> 00:40:53.260]   So a lot of people's jobs depend on us being the ones
[00:40:53.260 --> 00:40:53.860]   to drive that.
[00:40:53.860 --> 00:40:57.420]   And I think that if we just went straight to autonomous,
[00:40:57.420 --> 00:41:01.180]   I don't think that the economy would be able to readjust fast
[00:41:01.180 --> 00:41:01.660]   enough.
[00:41:01.660 --> 00:41:04.420]   There'd be a lot of people out of work because of that.
[00:41:04.420 --> 00:41:04.980]   You just go to--
[00:41:04.980 --> 00:41:06.140]   That's really true.
[00:41:06.140 --> 00:41:06.660]   Yeah.
[00:41:06.660 --> 00:41:08.260]   I should-- there's a lot of potential
[00:41:08.260 --> 00:41:10.460]   to create a lot of new jobs.
[00:41:10.460 --> 00:41:12.420]   Once we go to autonomous cars, at that point,
[00:41:12.420 --> 00:41:14.540]   the expectation is a lot of us will be using ride sharing
[00:41:14.540 --> 00:41:15.540]   services a lot more.
[00:41:15.540 --> 00:41:18.500]   So even if I have a car in my driveway in the morning,
[00:41:18.500 --> 00:41:19.660]   that takes me to work in the morning,
[00:41:19.660 --> 00:41:21.900]   that car might not be sitting there in the driveway.
[00:41:21.900 --> 00:41:25.180]   Because right now, 95% of the time that we own our cars,
[00:41:25.180 --> 00:41:27.980]   our cars are sitting in a driveway, sitting in a parking lot,
[00:41:27.980 --> 00:41:28.740]   not doing anything.
[00:41:28.740 --> 00:41:30.340]   Fire-inship is the not a thing.
[00:41:30.340 --> 00:41:30.840]   Yeah.
[00:41:30.840 --> 00:41:34.500]   And there's also the need to fix those cars.
[00:41:34.500 --> 00:41:35.060]   Right.
[00:41:35.060 --> 00:41:37.380]   If we go to autonomy, at that point,
[00:41:37.380 --> 00:41:39.860]   the idea is that ride sharing will be so popular that, ultimately,
[00:41:39.860 --> 00:41:41.220]   that my car will take me to work,
[00:41:41.220 --> 00:41:42.540]   and then my car might shuttle people
[00:41:42.540 --> 00:41:45.500]   from the train station to their jobs all day long.
[00:41:45.500 --> 00:41:47.540]   And then the car will be back and ready to take me home again.
[00:41:47.540 --> 00:41:48.820]   And then all night long, it might again
[00:41:48.820 --> 00:41:50.940]   be out during ride sharing, that kind of thing.
[00:41:50.940 --> 00:41:53.940]   So utilization will go up such an incredibly huge amount
[00:41:53.940 --> 00:41:55.940]   that our cars will actually need a lot more servicing
[00:41:55.940 --> 00:41:58.420]   than before, which means being an auto mechanic,
[00:41:58.420 --> 00:42:00.180]   where I should become a lot more important,
[00:42:00.180 --> 00:42:02.380]   because we're in a lot more cars that need to be used.
[00:42:02.380 --> 00:42:04.860]   There's going to be a lot of new services like cleaning our cars,
[00:42:04.860 --> 00:42:07.340]   because we're not going in and out of our cars.
[00:42:07.340 --> 00:42:09.900]   We're seeing this kind of cottage industry of people
[00:42:09.900 --> 00:42:12.380]   who go in and clean cars coming up that's really not
[00:42:12.380 --> 00:42:13.100]   been there before.
[00:42:13.100 --> 00:42:14.660]   That is quite interesting.
[00:42:14.660 --> 00:42:16.700]   There are opportunities, but I can't imagine it will
[00:42:16.700 --> 00:42:20.340]   it will recompense for the millions of people who drive trucks,
[00:42:20.340 --> 00:42:21.260]   cabs.
[00:42:21.260 --> 00:42:21.860]   Sure.
[00:42:21.860 --> 00:42:25.380]   I think that we're dealing with like efficiency versus economy.
[00:42:25.380 --> 00:42:28.540]   And I think that because of the fact that the cars will not
[00:42:28.540 --> 00:42:31.460]   be run going too fast, not have as many bumps and bumps.
[00:42:31.460 --> 00:42:33.620]   I don't think that they'll both be able to track.
[00:42:33.620 --> 00:42:35.580]   And I think that a lot of the jobs that it'll create
[00:42:35.580 --> 00:42:37.140]   will be more high end jobs.
[00:42:37.140 --> 00:42:39.740]   Well, that's not necessarily a bad thing, though.
[00:42:39.740 --> 00:42:41.740]   And also think about the time in the car.
[00:42:41.740 --> 00:42:43.380]   This is what I spent a lot of time thinking about.
[00:42:43.380 --> 00:42:45.420]   If your car is completely autonomous,
[00:42:45.420 --> 00:42:47.340]   you have this contained place.
[00:42:47.340 --> 00:42:48.540]   You're going from point A to point B,
[00:42:48.540 --> 00:42:50.380]   you're a captive audience.
[00:42:50.380 --> 00:42:53.260]   I mean, maybe that is when you meet with your therapist.
[00:42:53.260 --> 00:42:53.740]   Oh.
[00:42:53.740 --> 00:42:57.420]   Or you can have meetings in your car.
[00:42:57.420 --> 00:42:57.860]   You can--
[00:42:57.860 --> 00:42:59.340]   Actually, I have to say, listen to the audio books.
[00:42:59.340 --> 00:43:01.900]   I could get a facial in my car.
[00:43:01.900 --> 00:43:02.900]   I'm joking.
[00:43:02.900 --> 00:43:05.620]   So there'll be a whole category of mobile professions,
[00:43:05.620 --> 00:43:09.060]   like mobile aesthetician, mobile psychotherapist.
[00:43:09.060 --> 00:43:11.660]   You'll pull up, you'll pick them up, they'll get in,
[00:43:11.660 --> 00:43:14.460]   and they'll talk to you while you're in your living room,
[00:43:14.460 --> 00:43:15.140]   basically.
[00:43:15.140 --> 00:43:16.140]   Sure.
[00:43:16.140 --> 00:43:17.580]   It's a more private living room.
[00:43:17.580 --> 00:43:20.780]   Or maybe because of 5G, you're actually having that meeting.
[00:43:20.780 --> 00:43:21.620]   It's like a telemedic.
[00:43:21.620 --> 00:43:25.300]   You're meeting your doctor, because you're in a private place.
[00:43:25.300 --> 00:43:27.220]   Having-- you're on a call with your doctor.
[00:43:27.220 --> 00:43:30.260]   Probably the case that the economy will create--
[00:43:30.260 --> 00:43:35.340]   I mean, it may be a net zero or not.
[00:43:35.340 --> 00:43:39.820]   But even if it is a net zero change in employment,
[00:43:39.820 --> 00:43:41.980]   it's going to be highly disruptive.
[00:43:41.980 --> 00:43:42.700]   The people--
[00:43:42.700 --> 00:43:45.980]   Well, it depends on how quickly we transition.
[00:43:45.980 --> 00:43:47.580]   If we transition slowly, I don't think
[00:43:47.580 --> 00:43:49.820]   that there's going to be a problem as more jobs are created
[00:43:49.820 --> 00:43:52.380]   as some jobs deal, and people are going to learn to adapt.
[00:43:52.380 --> 00:43:53.940]   If it happens really quickly, that's
[00:43:53.940 --> 00:43:57.300]   when we end up with a large amount of people that are like,
[00:43:57.300 --> 00:43:58.460]   I don't know what to do.
[00:43:58.460 --> 00:43:59.220]   I'm fully worried.
[00:43:59.220 --> 00:44:00.340]   I get angry.
[00:44:00.340 --> 00:44:02.060]   And then we end up with a lot of backlash
[00:44:02.060 --> 00:44:04.820]   against a certain field because of that.
[00:44:04.820 --> 00:44:06.180]   Right.
[00:44:06.180 --> 00:44:08.180]   It's going to happen more quickly than most people think.
[00:44:08.180 --> 00:44:10.860]   There are fully driverless cars in Phoenix right now.
[00:44:10.860 --> 00:44:12.900]   Waymo is testing them there.
[00:44:12.900 --> 00:44:15.500]   GM has the cruise AV that they want to have on the streets
[00:44:15.500 --> 00:44:19.340]   of San Francisco, available for public use by next year.
[00:44:19.340 --> 00:44:21.820]   And once people get to experience these cars,
[00:44:21.820 --> 00:44:24.420]   they're probably going to want to be using them more.
[00:44:24.420 --> 00:44:26.220]   So I think that those things will come very quickly.
[00:44:26.220 --> 00:44:27.380]   And then, yeah, at that point, it
[00:44:27.380 --> 00:44:28.220]   will be very disruptive.
[00:44:28.220 --> 00:44:29.300]   And ultimately, I don't think we really
[00:44:29.300 --> 00:44:31.860]   have a good idea or a good understanding of exactly what
[00:44:31.860 --> 00:44:33.620]   it will do to that industry.
[00:44:33.620 --> 00:44:34.700]   And if there are any more, I mean,
[00:44:34.700 --> 00:44:36.660]   if you have an hour commute right now,
[00:44:36.660 --> 00:44:38.300]   you probably wouldn't be too cool with that.
[00:44:38.300 --> 00:44:40.740]   But if you're our commute, as you hanging out
[00:44:40.740 --> 00:44:43.140]   in the backseat of the car, napping or getting work done
[00:44:43.140 --> 00:44:45.060]   or reading a book or doing whatever,
[00:44:45.060 --> 00:44:46.980]   most people would probably be OK with that.
[00:44:46.980 --> 00:44:49.900]   And so that means maybe we'll see this kind of rush out
[00:44:49.900 --> 00:44:52.700]   of cities again, moving more to the suburbs again,
[00:44:52.700 --> 00:44:54.500]   which would be contrast to the organization we've
[00:44:54.500 --> 00:44:56.820]   been seeing a lot lately, which so that could have some major
[00:44:56.820 --> 00:44:58.940]   changes to everything.
[00:44:58.940 --> 00:44:59.940]   Yeah.
[00:44:59.940 --> 00:45:01.340]   And how have they been--
[00:45:01.340 --> 00:45:03.820]   sorry, how have they been dealing with weather situations
[00:45:03.820 --> 00:45:04.380]   in snow?
[00:45:04.380 --> 00:45:05.860]   I live in Canada.
[00:45:05.860 --> 00:45:08.900]   There's a lot of snow and how they work in the--
[00:45:08.900 --> 00:45:10.460]   Yeah, it's definitely difficult.
[00:45:10.460 --> 00:45:12.820]   That's exactly why Waymo is testing in Phoenix,
[00:45:12.820 --> 00:45:15.300]   because there's not a whole lot of weather to worry about there.
[00:45:15.300 --> 00:45:16.620]   But it is definitely an issue.
[00:45:16.620 --> 00:45:19.220]   Ford is the only one I've heard of that's really put a lot
[00:45:19.220 --> 00:45:21.220]   of effort into testing in the snow.
[00:45:21.220 --> 00:45:23.460]   They showed off a concept of their autonomous cars
[00:45:23.460 --> 00:45:26.060]   that actually ignores the lines in the road.
[00:45:26.060 --> 00:45:29.700]   And they use LiDAR scanners like most cars do.
[00:45:29.700 --> 00:45:31.820]   And they scan the trees and the buildings
[00:45:31.820 --> 00:45:34.020]   and basically the surroundings rather than actually
[00:45:34.020 --> 00:45:34.860]   mapping the roads.
[00:45:34.860 --> 00:45:37.580]   So even if the road is completely occluded by snow,
[00:45:37.580 --> 00:45:39.900]   the car can know exactly where it is on the road,
[00:45:39.900 --> 00:45:41.300]   which is pretty cool.
[00:45:41.300 --> 00:45:43.460]   And LiDAR scanners are actually good enough right now.
[00:45:43.460 --> 00:45:45.060]   One engineer told me that in a blizzard,
[00:45:45.060 --> 00:45:47.100]   they can count the snowflakes in a blizzard.
[00:45:47.100 --> 00:45:47.620]   So--
[00:45:47.620 --> 00:45:49.860]   They can see better than a human can, for sure.
[00:45:49.860 --> 00:45:51.620]   Yeah, so ultimately, blizzards, that kind of thing,
[00:45:51.620 --> 00:45:53.460]   really aren't that problematic, especially when you
[00:45:53.460 --> 00:45:55.980]   have a car with radar and LiDAR and other sensors.
[00:45:55.980 --> 00:45:58.260]   The big problem, though, is when that stuff starts to build up
[00:45:58.260 --> 00:46:00.780]   ice and snow getting on sensors is a problem.
[00:46:00.780 --> 00:46:03.180]   So companies like DPG are working on and special coatings
[00:46:03.180 --> 00:46:04.580]   that will kind of shrug off that stuff.
[00:46:04.580 --> 00:46:06.220]   It's a challenge for sure that they have an exactly
[00:46:06.220 --> 00:46:08.460]   quite figured out, but I'm optimistic they will produce some.
[00:46:08.460 --> 00:46:11.740]   It'd be a big market for LiDAR heaters coming in.
[00:46:11.740 --> 00:46:12.660]   LiDAR heaters.
[00:46:12.660 --> 00:46:14.340]   Invest in LiDAR heaters.
[00:46:14.340 --> 00:46:16.340]   You have a little LiDAR cozy that you can
[00:46:16.340 --> 00:46:17.420]   speak on there before you get on there.
[00:46:17.420 --> 00:46:17.940]   LiDAR cozy.
[00:46:17.940 --> 00:46:17.940]   Nice.
[00:46:17.940 --> 00:46:19.500]   Keep it warm.
[00:46:19.500 --> 00:46:23.540]   So, Tim, chat room was wondering, what about security?
[00:46:23.540 --> 00:46:25.340]   Are there people talking about security at CES
[00:46:25.340 --> 00:46:26.980]   in the auto show about--
[00:46:26.980 --> 00:46:28.900]   because we've seen cars--
[00:46:28.900 --> 00:46:32.660]   famous hacks of cars where people remotely take over
[00:46:32.660 --> 00:46:34.860]   and accelerate you into a wall and all sorts of things.
[00:46:34.860 --> 00:46:37.380]   Are car manufacturers paying more attention to this now?
[00:46:37.380 --> 00:46:39.100]   They definitely are, but most of them actually
[00:46:39.100 --> 00:46:40.580]   aren't talking about it too much.
[00:46:40.580 --> 00:46:43.220]   I think they're still taking the big corporation idea
[00:46:43.220 --> 00:46:45.340]   that talking about security puts a target on your back.
[00:46:45.340 --> 00:46:47.460]   So by and large, these companies are not really
[00:46:47.460 --> 00:46:48.780]   being that forthright about it.
[00:46:48.780 --> 00:46:52.340]   Jim, if you companies actually is going out there
[00:46:52.340 --> 00:46:55.300]   and working with hacking communities with white hat
[00:46:55.300 --> 00:46:58.540]   hackers, with things like bug bounties and things like that.
[00:46:58.540 --> 00:47:00.140]   But that's pretty rare.
[00:47:00.140 --> 00:47:02.700]   But actually, in Detroit last week,
[00:47:02.700 --> 00:47:04.740]   John Chen from Blackberry had a keynote speech.
[00:47:04.740 --> 00:47:05.860]   And they actually spent the whole time
[00:47:05.860 --> 00:47:08.700]   showing off a developer tool designed explicitly
[00:47:08.700 --> 00:47:11.900]   for basically being integrated into your build process
[00:47:11.900 --> 00:47:13.500]   and scanning for any vulnerabilities
[00:47:13.500 --> 00:47:15.140]   within automotive code.
[00:47:15.140 --> 00:47:16.260]   It's called Jarvis.
[00:47:16.260 --> 00:47:18.500]   And they basically showed off how you can integrate it
[00:47:18.500 --> 00:47:19.700]   within your build system.
[00:47:19.700 --> 00:47:21.220]   And within a matter of minutes, it
[00:47:21.220 --> 00:47:22.940]   will scan over all your automotive code
[00:47:22.940 --> 00:47:25.180]   and look for common vulnerabilities
[00:47:25.180 --> 00:47:27.340]   that your average mobile developer might be familiar with.
[00:47:27.340 --> 00:47:29.300]   But automotive developers typically
[00:47:29.300 --> 00:47:31.460]   are used to working within very close systems.
[00:47:31.460 --> 00:47:33.860]   So for them, this is kind of a new world.
[00:47:33.860 --> 00:47:36.100]   So it's actually really interesting for John Chen
[00:47:36.100 --> 00:47:39.260]   and his colleagues to stand up in front of an audience
[00:47:39.260 --> 00:47:42.180]   of mostly automotive journalists and talk about a developer
[00:47:42.180 --> 00:47:44.940]   tool, something that you would integrate into your make files.
[00:47:44.940 --> 00:47:47.140]   And they spent a long time going into exactly how much
[00:47:47.140 --> 00:47:49.620]   this can make your cars code more safe.
[00:47:49.620 --> 00:47:51.180]   So that's actually a pretty interesting approach.
[00:47:51.180 --> 00:47:53.420]   And there is a lot of work going on in that area.
[00:47:53.420 --> 00:47:55.260]   But again, a lot of us going on behind closed doors,
[00:47:55.260 --> 00:47:56.660]   which I think is still pretty unfortunate.
[00:47:56.660 --> 00:47:59.340]   It's interesting to see Blackberry doing that.
[00:47:59.340 --> 00:48:01.340]   They've really found a second life.
[00:48:01.340 --> 00:48:05.180]   They bought a real-time operating system called QNXQNX
[00:48:05.180 --> 00:48:06.100]   some years ago.
[00:48:06.100 --> 00:48:09.540]   And is that becoming the dominant car platform?
[00:48:09.540 --> 00:48:10.220]   It definitely is.
[00:48:10.220 --> 00:48:12.860]   And that was one of the bigger announcements coming out of CES
[00:48:12.860 --> 00:48:14.740]   was partnership with NVIDIA.
[00:48:14.740 --> 00:48:18.220]   NVIDIA is now making their GPUs become kind of the de facto
[00:48:18.220 --> 00:48:20.660]   platform for a lot of these autonomous cars, actually,
[00:48:20.660 --> 00:48:22.940]   because the floating-point math used for video games
[00:48:22.940 --> 00:48:25.100]   is actually very similar to the floating-point math used
[00:48:25.100 --> 00:48:27.100]   for autonomous cars.
[00:48:27.100 --> 00:48:31.020]   And so QNX now, which is this very bulletproof low-level
[00:48:31.020 --> 00:48:33.380]   operating system that used in nuclear submarines and power
[00:48:33.380 --> 00:48:36.140]   plants and S16s and stuff like that,
[00:48:36.140 --> 00:48:38.780]   is now becoming the de facto operating system for a lot
[00:48:38.780 --> 00:48:39.780]   of these things, too.
[00:48:39.780 --> 00:48:43.180]   Is Ford out of that business with Microsoft cars?
[00:48:43.180 --> 00:48:44.580]   Microsoft is out.
[00:48:44.580 --> 00:48:47.300]   Yeah, ultimately, the new sync does no longer
[00:48:47.300 --> 00:48:48.180]   uses Microsoft's technology.
[00:48:48.180 --> 00:48:49.780]   Microsoft is still working in automotive.
[00:48:49.780 --> 00:48:50.540]   They have some partners.
[00:48:50.540 --> 00:48:52.500]   But ultimately, no longer with Ford.
[00:48:52.500 --> 00:48:53.740]   Yeah, that was a major announcement.
[00:48:53.740 --> 00:48:56.140]   And I think a big loss for Microsoft.
[00:48:56.140 --> 00:48:59.500]   And then I know my car uses Ubuntu Linux.
[00:48:59.500 --> 00:49:00.940]   I have a Tesla.
[00:49:00.940 --> 00:49:05.460]   I don't imagine there's anybody else using Linux in their cars.
[00:49:05.460 --> 00:49:08.980]   Actually, Linux Automotive Group is doing a lot of great things
[00:49:08.980 --> 00:49:09.340]   as well.
[00:49:09.340 --> 00:49:11.140]   They're working on autonomy quite readily.
[00:49:11.140 --> 00:49:12.820]   And there's a lot of Linux in cars as well,
[00:49:12.820 --> 00:49:15.020]   but they're going to tends to be hidden deeper down below the surface.
[00:49:15.020 --> 00:49:17.380]   Again, a lot of manufacturers aren't really that keen
[00:49:17.380 --> 00:49:19.580]   to talk about what's running on their cars, to be honest with you.
[00:49:19.580 --> 00:49:21.460]   But there's a lot of Android hiding in these systems, too.
[00:49:21.460 --> 00:49:24.340]   We're seeing Android and actually will be running natively
[00:49:24.340 --> 00:49:25.900]   on a dashboard, which is pretty exciting.
[00:49:25.900 --> 00:49:27.220]   So no more Android all.
[00:49:27.220 --> 00:49:29.500]   You'll just have Google Home running your dashboard.
[00:49:29.500 --> 00:49:31.860]   I think it's really interesting that NVIDIA is--
[00:49:31.860 --> 00:49:34.500]   they were talking also about self-driving stuff.
[00:49:34.500 --> 00:49:36.060]   I didn't understand why they were in it.
[00:49:36.060 --> 00:49:36.700]   Now I get it.
[00:49:36.700 --> 00:49:40.180]   GPUs are useful in this environment.
[00:49:40.180 --> 00:49:43.140]   And because they're so good at making their chips efficient,
[00:49:43.140 --> 00:49:45.300]   this was actually a really interesting point.
[00:49:45.300 --> 00:49:47.620]   I had Dennis Shapiro from NVIDIA on a panel.
[00:49:47.620 --> 00:49:48.860]   I did in Detroit last week.
[00:49:48.860 --> 00:49:51.060]   And somebody asked, you know, what kind of power consumption
[00:49:51.060 --> 00:49:51.940]   are you talking about?
[00:49:51.940 --> 00:49:55.060]   And Danny said that you're talking typically about 30 watts
[00:49:55.060 --> 00:49:57.540]   for one of these autonomous GPUs to power a car.
[00:49:57.540 --> 00:50:00.380]   And 30 watts is really not that much power.
[00:50:00.380 --> 00:50:03.100]   You would probably need two of those for a redundancy
[00:50:03.100 --> 00:50:03.820]   sort of situation.
[00:50:03.820 --> 00:50:07.860]   But even so, less than 100 watts is what they're aiming for anyway,
[00:50:07.860 --> 00:50:10.340]   when they have a production-ready version of these GPUs.
[00:50:10.340 --> 00:50:12.020]   And that's essentially relatively low,
[00:50:12.020 --> 00:50:14.500]   because you think about autonomous car and EV.
[00:50:14.500 --> 00:50:16.540]   You want to have all your battery juice going
[00:50:16.540 --> 00:50:18.780]   to actually make you get to where you want to go.
[00:50:18.780 --> 00:50:20.980]   You don't have it burning away at the GPU sitting in your trunk.
[00:50:20.980 --> 00:50:22.340]   So that was actually really interesting.
[00:50:22.340 --> 00:50:24.180]   So now if we just get all those Bitcoin miners
[00:50:24.180 --> 00:50:27.340]   to stop buying up all the NVIDIA video cards,
[00:50:27.340 --> 00:50:30.700]   we can put them in our cars and things will be better.
[00:50:30.700 --> 00:50:31.980]   We were talking about employment.
[00:50:31.980 --> 00:50:35.700]   If you're a 7/11 clerk, this might scare you a little bit.
[00:50:35.700 --> 00:50:38.900]   Amazon Go opens tomorrow.
[00:50:38.900 --> 00:50:40.780]   This is in the Amazon headquarters.
[00:50:40.780 --> 00:50:42.060]   It's the first.
[00:50:42.060 --> 00:50:44.540]   But the idea is there's no humans.
[00:50:44.540 --> 00:50:48.180]   You walk in, it recognizes you, I presume, from your phone.
[00:50:48.180 --> 00:50:49.900]   You pick up what you want.
[00:50:49.900 --> 00:50:53.540]   You walk out, cameras register what you bought
[00:50:53.540 --> 00:50:55.820]   on the shelves and other places,
[00:50:55.820 --> 00:50:58.980]   and charge you for it on, I guess, Android Pay,
[00:50:58.980 --> 00:51:00.500]   or what they call Google Pay now.
[00:51:00.500 --> 00:51:01.940]   Yeah, this is--
[00:51:01.940 --> 00:51:03.340]   Or Amazon Pay, maybe.
[00:51:03.340 --> 00:51:05.540]   You have to register your phone when you walk in.
[00:51:05.540 --> 00:51:08.180]   OK, so you have to say, here's my phone, you scan it.
[00:51:08.180 --> 00:51:10.500]   Now your phone is matched to you.
[00:51:10.500 --> 00:51:13.020]   And then presumably, the system with all the cameras
[00:51:13.020 --> 00:51:16.740]   in the store are taking pictures of you watching where you go.
[00:51:16.740 --> 00:51:18.380]   There are sensors on the products.
[00:51:18.380 --> 00:51:20.860]   I mean, this is like a Vegas minibar, right?
[00:51:20.860 --> 00:51:22.940]   If you take the wine off the shelf, you bought it.
[00:51:22.940 --> 00:51:24.340]   Right, but what was interesting about it
[00:51:24.340 --> 00:51:26.340]   was that apparently in this demo store,
[00:51:26.340 --> 00:51:28.300]   there were still quite a few employees standing around
[00:51:28.300 --> 00:51:31.100]   to do things like ID checks on the alcohol section.
[00:51:31.100 --> 00:51:31.780]   Yeah, of course.
[00:51:31.780 --> 00:51:32.860]   Interesting.
[00:51:32.860 --> 00:51:34.500]   And if you have any experience, and this
[00:51:34.500 --> 00:51:36.900]   is really the basic version of this,
[00:51:36.900 --> 00:51:39.220]   it's not that crazy when you think about self-checkout
[00:51:39.220 --> 00:51:41.220]   at Safeway or what have you.
[00:51:41.220 --> 00:51:42.700]   But I don't know how often you've done that.
[00:51:42.700 --> 00:51:45.540]   I'm forced to do it all the time.
[00:51:45.540 --> 00:51:47.780]   It's a pain in the butt.
[00:51:47.780 --> 00:51:50.980]   There are always checkers having to come over and help people
[00:51:50.980 --> 00:51:53.060]   with their transactions for all kinds of reasons.
[00:51:53.060 --> 00:51:55.540]   And those already existing systems
[00:51:55.540 --> 00:51:58.020]   don't let you take alcohol through them usually.
[00:51:58.020 --> 00:52:01.660]   So I think it's a proof of concept.
[00:52:01.660 --> 00:52:03.460]   And maybe a little bit getting people
[00:52:03.460 --> 00:52:06.660]   used to the idea that you don't have to interact with a human
[00:52:06.660 --> 00:52:09.900]   because kind of like the autonomous driving conversation,
[00:52:09.900 --> 00:52:12.460]   it's something that people will feel uncomfortable with
[00:52:12.460 --> 00:52:14.340]   for a long time until they do it,
[00:52:14.340 --> 00:52:16.540]   and they see those charges pop up easily.
[00:52:16.540 --> 00:52:18.220]   And they'll just be like, wait, I can just come in and go,
[00:52:18.220 --> 00:52:20.220]   how are you going to know and what's going to be fun?
[00:52:20.220 --> 00:52:21.540]   Why are you tracking me?
[00:52:21.540 --> 00:52:24.940]   And as I think they're just getting people used to this idea.
[00:52:24.940 --> 00:52:27.300]   I, that's really you think people are going to have a hard time
[00:52:27.300 --> 00:52:28.700]   because I can't wait.
[00:52:28.700 --> 00:52:31.380]   I think that's exactly what I would love for a corner market.
[00:52:31.380 --> 00:52:33.340]   I just walk in, take what I want and leave.
[00:52:33.340 --> 00:52:36.420]   I think people are not going to like the tracking part of it.
[00:52:36.420 --> 00:52:37.740]   Oh, come on, people get over it.
[00:52:37.740 --> 00:52:39.020]   And I think that they're going to for a little while feel like
[00:52:39.020 --> 00:52:39.860]   they're stealing things.
[00:52:39.860 --> 00:52:41.020]   If you're carrying--
[00:52:41.020 --> 00:52:41.660]   That would be it.
[00:52:41.660 --> 00:52:42.180]   That would be it.
[00:52:42.180 --> 00:52:45.020]   Lindsay is that I would worry that it didn't actually
[00:52:45.020 --> 00:52:47.540]   check the sensor and I've now just stolen a whole bunch of stuff.
[00:52:47.540 --> 00:52:48.740]   That's even better.
[00:52:48.740 --> 00:52:49.540]   Check it.
[00:52:49.540 --> 00:52:50.740]   What are you crazy?
[00:52:50.740 --> 00:52:51.580]   That's even better.
[00:52:51.580 --> 00:52:52.900]   I'm chasing you down the block.
[00:52:52.900 --> 00:52:54.700]   I feel bad for the actual thieves that are like,
[00:52:54.700 --> 00:52:56.860]   I stole this and they're like, no, actually,
[00:52:56.860 --> 00:52:57.820]   it's been charged to you anyway.
[00:52:57.820 --> 00:53:01.740]   One of the New York Times reporter tried to steal,
[00:53:01.740 --> 00:53:05.620]   actually tried to shoplift, and he got charged for it anyway.
[00:53:05.620 --> 00:53:09.420]   So don't-- but I think that that's a bonus.
[00:53:09.420 --> 00:53:11.020]   I think there-- maybe it's a guy thing.
[00:53:11.020 --> 00:53:11.860]   I don't know.
[00:53:11.860 --> 00:53:12.700]   I think-- I don't know.
[00:53:12.700 --> 00:53:14.980]   A lot of guys would say, hey, if they don't charge
[00:53:14.980 --> 00:53:16.060]   me, they don't charge me.
[00:53:16.060 --> 00:53:17.860]   I'm just going to take it.
[00:53:17.860 --> 00:53:19.860]   I can't believe you would feel guilty about that.
[00:53:19.860 --> 00:53:20.980]   I think that people are just going to be--
[00:53:20.980 --> 00:53:22.740]   it's like light.
[00:53:22.740 --> 00:53:24.340]   Like we were talking about earlier,
[00:53:24.340 --> 00:53:26.220]   people feel afraid of change.
[00:53:26.220 --> 00:53:27.740]   This is a big change.
[00:53:27.740 --> 00:53:28.660]   What if it doesn't work?
[00:53:28.660 --> 00:53:30.140]   It just takes time for people to get used to it.
[00:53:30.140 --> 00:53:32.580]   Although I will say that the idea of not having to show people
[00:53:32.580 --> 00:53:34.940]   what I'm buying is great.
[00:53:34.940 --> 00:53:35.300]   Right?
[00:53:35.300 --> 00:53:36.620]   But it's all tracked, right?
[00:53:36.620 --> 00:53:38.660]   So it's not really-- they know what you're buying.
[00:53:38.660 --> 00:53:39.820]   Oh, but they look at it.
[00:53:39.820 --> 00:53:42.220]   You don't think they don't know what you're buying at Walmart?
[00:53:42.220 --> 00:53:43.140]   I mean, come on.
[00:53:43.140 --> 00:53:45.380]   Something really embarrassing.
[00:53:45.380 --> 00:53:46.580]   Someone can now-- OK.
[00:53:46.580 --> 00:53:47.540]   I won't--
[00:53:47.540 --> 00:53:48.540]   Hey, cash.
[00:53:48.540 --> 00:53:51.740]   It would be like, they now know that you bought whatever
[00:53:51.740 --> 00:53:52.900]   cream for whatever--
[00:53:52.900 --> 00:53:53.460]   I think--
[00:53:53.460 --> 00:53:54.620]   They know that anyway.
[00:53:54.620 --> 00:53:58.300]   Embarrassment is a very 20th century idea.
[00:53:58.300 --> 00:53:59.220]   It's gone.
[00:53:59.220 --> 00:54:00.500]   There's no more embarrassment.
[00:54:00.500 --> 00:54:01.620]   It's over.
[00:54:01.620 --> 00:54:03.060]   Just everybody knows everything.
[00:54:03.060 --> 00:54:05.620]   I'm mostly embarrassed about the cheap ros.
[00:54:05.620 --> 00:54:06.580]   Actually, you should do it.
[00:54:06.580 --> 00:54:07.580]   Yeah, it's embarrassing.
[00:54:07.580 --> 00:54:08.740]   Don't buy that there.
[00:54:08.740 --> 00:54:10.580]   So Nick Wingfield, who was writing the article,
[00:54:10.580 --> 00:54:13.500]   said he tried to trick the camera system by wrapping
[00:54:13.500 --> 00:54:18.580]   a shopping bag around $4.35, 4-pack of vanilla soda
[00:54:18.580 --> 00:54:21.540]   while it was still on his shelf, then tucked it under his arm
[00:54:21.540 --> 00:54:22.980]   and walked out of the store.
[00:54:22.980 --> 00:54:24.620]   He still got charged.
[00:54:24.620 --> 00:54:25.660]   Sorry, Nick.
[00:54:25.660 --> 00:54:26.180]   Let's try.
[00:54:26.180 --> 00:54:27.580]   Thanks for shopping.
[00:54:27.580 --> 00:54:31.820]   You just bought $4.35 with a dry soda that you didn't want.
[00:54:31.820 --> 00:54:34.180]   Was there an RF idea as part of this?
[00:54:34.180 --> 00:54:35.020]   I don't know.
[00:54:35.020 --> 00:54:37.420]   I didn't see that in this piece.
[00:54:37.420 --> 00:54:38.740]   No, they implied it was all for cameras.
[00:54:38.740 --> 00:54:42.660]   Just cameras, your location in the store,
[00:54:42.660 --> 00:54:45.300]   and added to shots of you.
[00:54:45.300 --> 00:54:49.180]   I feel bad about people who work at 7-Eleven stores.
[00:54:49.180 --> 00:54:50.020]   But--
[00:54:50.020 --> 00:54:51.860]   because I mean, really--
[00:54:51.860 --> 00:54:53.140]   Or any stores.
[00:54:53.140 --> 00:54:54.620]   Or bodegas.
[00:54:54.620 --> 00:54:56.900]   We are going to see fewer and fewer entry level jobs
[00:54:56.900 --> 00:54:57.900]   in general, right?
[00:54:57.900 --> 00:55:02.340]   Drivers, Uber drivers, convenience stores.
[00:55:02.340 --> 00:55:04.700]   But what is more convenient than have--
[00:55:04.700 --> 00:55:06.620]   and you could put little ones of these everywhere.
[00:55:06.620 --> 00:55:07.980]   You don't have to put wine in them.
[00:55:07.980 --> 00:55:09.380]   You just put little ones of them everywhere,
[00:55:09.380 --> 00:55:12.460]   and you just walk in, grab something, and go.
[00:55:12.460 --> 00:55:14.540]   I think that's the future.
[00:55:14.540 --> 00:55:15.460]   I'm going to predict this--
[00:55:15.460 --> 00:55:18.140]   I still have to walk in though, Leo.
[00:55:18.140 --> 00:55:20.940]   Like, that's the problem is that I don't want to have to walk in.
[00:55:20.940 --> 00:55:22.700]   Once I've walked in, I actually prefer
[00:55:22.700 --> 00:55:25.020]   to have a cashier than doing a self-checkout.
[00:55:25.020 --> 00:55:25.580]   I do--
[00:55:25.580 --> 00:55:25.940]   I don't know.
[00:55:25.940 --> 00:55:27.020]   I like the process.
[00:55:27.020 --> 00:55:27.980]   I might be the only one.
[00:55:27.980 --> 00:55:29.220]   I don't like the self-checkouts.
[00:55:29.220 --> 00:55:30.580]   But that's mostly because they don't work.
[00:55:30.580 --> 00:55:31.580]   If I hadn't, they don't work very well.
[00:55:31.580 --> 00:55:35.060]   They delivered me all the stuff, then I'm all in.
[00:55:35.060 --> 00:55:36.100]   Well, we do that too.
[00:55:36.100 --> 00:55:37.060]   I mean, I do that too.
[00:55:37.060 --> 00:55:39.740]   I literally have an Amazon delivery every single day,
[00:55:39.740 --> 00:55:40.500]   several usually.
[00:55:40.500 --> 00:55:41.420]   I think it's going to be a hybrid.
[00:55:41.420 --> 00:55:42.860]   I think at some point, it may turn out
[00:55:42.860 --> 00:55:45.820]   to be sort of a luxury experience to have a checkout.
[00:55:45.820 --> 00:55:46.820]   Right?
[00:55:46.820 --> 00:55:47.420]   It's going to be--
[00:55:47.420 --> 00:55:49.900]   It's like in Oregon and New Jersey, where you pump your cash.
[00:55:49.900 --> 00:55:50.180]   Right.
[00:55:50.180 --> 00:55:51.860]   You'd be like, well, not Oregon, not anymore.
[00:55:51.860 --> 00:55:52.380]   Not anymore.
[00:55:52.380 --> 00:55:52.900]   Not anymore.
[00:55:52.900 --> 00:55:54.380]   Those poor people in Oregon going to do,
[00:55:54.380 --> 00:55:55.060]   they have to figure out--
[00:55:55.060 --> 00:55:55.660]   They're so confused.
[00:55:55.660 --> 00:55:56.540]   It's so organic.
[00:55:56.540 --> 00:55:57.420]   What do I--
[00:55:57.420 --> 00:55:58.260]   You mean, wait a minute.
[00:55:58.260 --> 00:55:59.740]   I've got to put a hose in my car.
[00:55:59.740 --> 00:56:01.820]   What are you talking about?
[00:56:01.820 --> 00:56:03.740]   That's why they need electric vehicles.
[00:56:03.740 --> 00:56:05.140]   They need electric vehicles.
[00:56:05.140 --> 00:56:06.540]   And autonomous ones, too.
[00:56:06.540 --> 00:56:07.420]   No, I'm very excited.
[00:56:07.420 --> 00:56:08.620]   I think this is a great idea.
[00:56:08.620 --> 00:56:11.340]   But again, I hate saying that because I feel bad about people
[00:56:11.340 --> 00:56:13.660]   who will be losing their jobs, because maybe not.
[00:56:13.660 --> 00:56:14.220]   I don't know.
[00:56:14.220 --> 00:56:15.860]   I'm trying to be optimistic about this,
[00:56:15.860 --> 00:56:17.260]   because I wanted to get the same thing.
[00:56:17.260 --> 00:56:18.580]   But somebody has a stock.
[00:56:18.580 --> 00:56:20.220]   Somebody has to polish everything.
[00:56:20.220 --> 00:56:21.300]   There may be somebody who's there
[00:56:21.300 --> 00:56:22.660]   as sort of a concierge.
[00:56:22.660 --> 00:56:25.180]   Your job is a convenience store polisher?
[00:56:25.180 --> 00:56:25.860]   Polisher.
[00:56:25.860 --> 00:56:26.860]   I don't know.
[00:56:26.860 --> 00:56:28.500]   But maybe this-- maybe this is a little bit of a--
[00:56:28.500 --> 00:56:29.500]   Wait a minute.
[00:56:29.500 --> 00:56:32.860]   Do they polish things at convenience stores?
[00:56:32.860 --> 00:56:33.380]   They do.
[00:56:33.380 --> 00:56:34.260]   They guess they'd have to.
[00:56:34.260 --> 00:56:35.540]   They've got to wipe those fingerprints out
[00:56:35.540 --> 00:56:36.540]   and they're absolutely--
[00:56:36.540 --> 00:56:38.140]   They're going to look shiny.
[00:56:38.140 --> 00:56:38.620]   Modern.
[00:56:38.620 --> 00:56:39.500]   If you think about the number of people
[00:56:39.500 --> 00:56:41.780]   who actually work at a grocery store, department store,
[00:56:41.780 --> 00:56:44.140]   and who are actually rigging up customers,
[00:56:44.140 --> 00:56:45.700]   it's not 100% buying a mean.
[00:56:45.700 --> 00:56:47.220]   So there certainly will be a lot of other things
[00:56:47.220 --> 00:56:49.260]   to do at these stores, I would think.
[00:56:49.260 --> 00:56:51.860]   But yeah, I'm super excited about the potential as well.
[00:56:51.860 --> 00:56:53.260]   There are definitely times where
[00:56:53.260 --> 00:56:55.340]   just going to a gas station and needing a couple things,
[00:56:55.340 --> 00:56:56.540]   it would certainly be nice to just walk in,
[00:56:56.540 --> 00:56:58.500]   grab it, walk out again, rather than standing in line,
[00:56:58.500 --> 00:57:00.780]   waiting for everybody else to get the coffee order,
[00:57:00.780 --> 00:57:01.460]   write that kind of thing.
[00:57:01.460 --> 00:57:02.380]   Certainly would be very nice.
[00:57:02.380 --> 00:57:03.340]   Should we be worried?
[00:57:03.340 --> 00:57:06.340]   You got young kids, I got young kids.
[00:57:06.340 --> 00:57:08.700]   Georgia has young kids.
[00:57:08.700 --> 00:57:11.140]   I don't know, Tim, do you have young kids?
[00:57:11.140 --> 00:57:12.220]   I don't want to leave you out if you--
[00:57:12.220 --> 00:57:12.740]   Oh, dogs.
[00:57:12.740 --> 00:57:13.900]   You've dogs.
[00:57:13.900 --> 00:57:16.340]   Should we be worried about when they--
[00:57:16.340 --> 00:57:20.740]   I mean, they're in the workforce in seven, 10 years.
[00:57:20.740 --> 00:57:22.460]   Will there be jobs for these kids?
[00:57:22.460 --> 00:57:23.420]   I think there will be.
[00:57:23.420 --> 00:57:26.500]   What I'm hoping is that they then use this moment
[00:57:26.500 --> 00:57:28.140]   to find their own jobs, right?
[00:57:28.140 --> 00:57:31.020]   And so that hopefully that my kids, once they're in college,
[00:57:31.020 --> 00:57:32.580]   they're looking at opportunities and they're saying,
[00:57:32.580 --> 00:57:36.220]   oh, I am going to be an autonomous vehicle cleaner.
[00:57:36.220 --> 00:57:36.720]   Right?
[00:57:36.720 --> 00:57:37.580]   I'm going to work for the new--
[00:57:37.580 --> 00:57:38.900]   Seven or 11, Paul is sure.
[00:57:38.900 --> 00:57:41.060]   I mean, that's probably a really gross job.
[00:57:41.060 --> 00:57:42.740]   Don't do it.
[00:57:42.740 --> 00:57:47.180]   But there may be some entrepreneurial moments for them.
[00:57:47.180 --> 00:57:49.020]   And there's still a lot to do.
[00:57:49.020 --> 00:57:51.380]   Like, my yard is still not--
[00:57:51.380 --> 00:57:53.420]   there are no robots yet to trim my trees.
[00:57:53.420 --> 00:57:54.700]   Oh, that's just a moment to do that.
[00:57:54.700 --> 00:57:56.060]   There's-- well, sure.
[00:57:56.060 --> 00:57:57.340]   We're self- trimming trees.
[00:57:57.340 --> 00:57:58.340]   I think there's a--
[00:57:58.340 --> 00:57:59.900]   Chris Bresman.
[00:57:59.900 --> 00:58:02.380]   Self- trimming trees.
[00:58:02.380 --> 00:58:03.260]   I don't know.
[00:58:03.260 --> 00:58:06.020]   It's so funny because people who are in this field
[00:58:06.020 --> 00:58:09.500]   are kind of science fiction fans, many of us.
[00:58:09.500 --> 00:58:12.460]   On the one hand, we want to live in the sci-fi future.
[00:58:12.460 --> 00:58:15.300]   But the reality of it may not be quite so wonderful.
[00:58:15.300 --> 00:58:16.340]   And I don't know.
[00:58:16.340 --> 00:58:17.340]   I mean, I just don't know.
[00:58:17.340 --> 00:58:18.420]   I think it's inexorable.
[00:58:18.420 --> 00:58:20.660]   I think it's kind of hard to say, stop.
[00:58:20.660 --> 00:58:22.420]   We've got to keep the entry-level jobs.
[00:58:22.420 --> 00:58:23.260]   That's not going to happen.
[00:58:23.260 --> 00:58:24.660]   Well, I actually feel like--
[00:58:24.660 --> 00:58:26.860]   I was thinking about this a lot the other day.
[00:58:26.860 --> 00:58:29.380]   At what point are we actually living in the sci-fi future?
[00:58:29.380 --> 00:58:30.380]   I mean, we live there now.
[00:58:30.380 --> 00:58:30.780]   I think so.
[00:58:30.780 --> 00:58:31.780]   And we feel like--
[00:58:31.780 --> 00:58:33.460]   I'm always talking to my room.
[00:58:33.460 --> 00:58:35.540]   But today, I walk into my office.
[00:58:35.540 --> 00:58:37.340]   I say, echo, turn on the lights.
[00:58:37.340 --> 00:58:38.340]   Yeah.
[00:58:38.340 --> 00:58:39.380]   You live there now.
[00:58:39.380 --> 00:58:39.880]   Yeah.
[00:58:39.880 --> 00:58:42.580]   We live in some ways we are living there now.
[00:58:42.580 --> 00:58:47.660]   I still have a lot of work that needs to be done in my life.
[00:58:47.660 --> 00:58:49.180]   Humans still need to be there.
[00:58:49.180 --> 00:58:50.180]   Humans still need to be there.
[00:58:50.180 --> 00:58:52.500]   I also spend a lot of time thinking about the Amazon warehouse
[00:58:52.500 --> 00:58:53.980]   because they're on the forefront of this thing.
[00:58:53.980 --> 00:58:55.500]   But that's a terrible job.
[00:58:55.500 --> 00:58:56.700]   Well, it's a terrible job.
[00:58:56.700 --> 00:58:58.300]   This is what makes me worry.
[00:58:58.300 --> 00:59:00.980]   This is what makes me take the negative position.
[00:59:00.980 --> 00:59:05.020]   They have the Kiva robots that drive the stock towers up.
[00:59:05.020 --> 00:59:06.260]   They pick--
[00:59:06.260 --> 00:59:08.820]   they drive to the right shelf.
[00:59:08.820 --> 00:59:12.100]   They park this little robot with the pallets on it
[00:59:12.100 --> 00:59:14.660]   right in front of the item that needs to be grabbed.
[00:59:14.660 --> 00:59:17.260]   The humans who work there, their only job
[00:59:17.260 --> 00:59:20.740]   is to intercept these robots, reach onto the stock shelf,
[00:59:20.740 --> 00:59:24.500]   and put the item on the cart because there aren't robots
[00:59:24.500 --> 00:59:27.860]   with really well-articulated fingers yet.
[00:59:27.860 --> 00:59:31.060]   Essentially, because robots don't have good hands.
[00:59:31.060 --> 00:59:33.620]   And so we're paying humans to be hands.
[00:59:33.620 --> 00:59:34.380]   And that is it.
[00:59:34.380 --> 00:59:35.340]   They don't think.
[00:59:35.340 --> 00:59:36.540]   They don't move.
[00:59:36.540 --> 00:59:37.700]   All they do is grab in place.
[00:59:37.700 --> 00:59:39.500]   And this is what upsets me.
[00:59:39.500 --> 00:59:42.460]   We've become just the meat bags.
[00:59:42.460 --> 00:59:43.220]   So what should we--
[00:59:43.220 --> 00:59:44.700]   I mean, what do you--
[00:59:44.700 --> 00:59:46.060]   on the one hand, you can see--
[00:59:46.060 --> 00:59:47.860]   We are the cautionary tale, right?
[00:59:47.860 --> 00:59:49.180]   Well, but on the one hand, you can say--
[00:59:49.180 --> 00:59:49.980]   This is Black Mirror.
[00:59:49.980 --> 00:59:52.660]   Let's eliminate bad jobs.
[00:59:52.660 --> 00:59:56.580]   And then maybe humans will let machines do the jobs that
[00:59:56.580 --> 01:00:00.220]   are terrible and let humans do better jobs.
[01:00:00.220 --> 01:00:01.620]   Maybe that will happen.
[01:00:01.620 --> 01:00:02.300]   I don't know.
[01:00:02.300 --> 01:00:03.220]   There are some economists--
[01:00:03.220 --> 01:00:04.740]   Do we need to be doing these generative living at that point,
[01:00:04.740 --> 01:00:05.740]   which I think is a question that--
[01:00:05.740 --> 01:00:06.540]   But yeah, exactly.
[01:00:06.540 --> 01:00:09.260]   Sure, there are economists who optimistically think
[01:00:09.260 --> 01:00:11.420]   that the work week is just going to turn into a 20-hour work
[01:00:11.420 --> 01:00:12.420]   week.
[01:00:12.420 --> 01:00:13.820]   Well, if the robots do all the hard stuff,
[01:00:13.820 --> 01:00:16.540]   maybe we can just work less and get paid the same amount.
[01:00:16.540 --> 01:00:19.060]   I think that that is not going to happen.
[01:00:19.060 --> 01:00:20.380]   No, because employers aren't going
[01:00:20.380 --> 01:00:23.300]   to want to pay people for working less, right?
[01:00:23.300 --> 01:00:27.020]   Unless we end up having just a basic cost of living
[01:00:27.020 --> 01:00:30.860]   that just happens for everyone, or we tax the robots.
[01:00:30.860 --> 01:00:32.060]   That's not going to happen.
[01:00:32.060 --> 01:00:33.220]   Yeah.
[01:00:33.220 --> 01:00:33.740]   Yeah.
[01:00:33.740 --> 01:00:34.240]   Yeah.
[01:00:34.240 --> 01:00:35.740]   And as somebody pointed out--
[01:00:35.740 --> 01:00:36.420]   There's no consensus.
[01:00:36.420 --> 01:00:38.940]   He has chops at voting robots.
[01:00:38.940 --> 01:00:42.860]   You know, I think it's time for robots to get the right to vote.
[01:00:42.860 --> 01:00:43.380]   Absolutely.
[01:00:43.380 --> 01:00:44.380]   Absolutely.
[01:00:44.380 --> 01:00:46.380]   [LAUGHTER]
[01:00:46.380 --> 01:00:48.140]   Do you think the Supreme Court in 100 years
[01:00:48.140 --> 01:00:49.860]   will be debating that?
[01:00:49.860 --> 01:00:52.140]   They'll just Supreme Court D robots?
[01:00:52.140 --> 01:00:54.140]   If they are, they're already voting.
[01:00:54.140 --> 01:00:55.140]   They're already voting.
[01:00:55.140 --> 01:00:59.660]   So my daughter, who is 11 the other day,
[01:00:59.660 --> 01:01:03.180]   said something that was not very nice to Alexa.
[01:01:03.180 --> 01:01:04.220]   I mean, it wasn't terrible.
[01:01:04.220 --> 01:01:06.380]   She just-- I don't know, said shut up or something like that.
[01:01:06.380 --> 01:01:08.100]   And I said, hey, don't talk that way to Alexa,
[01:01:08.100 --> 01:01:09.580]   because I don't want her to be rude.
[01:01:09.580 --> 01:01:11.220]   And she looked at me and she goes, mom--
[01:01:11.220 --> 01:01:12.380]   She's a machine, mom?
[01:01:12.380 --> 01:01:14.300]   She says, Alexa's not an AI.
[01:01:14.300 --> 01:01:16.100]   Alexa's just a robot.
[01:01:16.100 --> 01:01:17.500]   Oh, she's smart.
[01:01:17.500 --> 01:01:18.700]   Sophisticated.
[01:01:18.700 --> 01:01:20.140]   That was her new on--
[01:01:20.140 --> 01:01:21.660]   Very sophisticated.
[01:01:21.660 --> 01:01:25.180]   But it was interesting to me that culturally, if she felt
[01:01:25.180 --> 01:01:27.700]   that Alexa actually had--
[01:01:27.700 --> 01:01:29.140]   Then maybe she would worry?
[01:01:29.140 --> 01:01:32.100]   Maybe, if Alexa had any kind of artificial intelligence,
[01:01:32.100 --> 01:01:34.180]   that that includes emotions.
[01:01:34.180 --> 01:01:35.980]   And we should worry about that.
[01:01:35.980 --> 01:01:39.060]   It was a trippy moment.
[01:01:39.060 --> 01:01:40.460]   That's good.
[01:01:40.460 --> 01:01:42.740]   It has actually-- she's very sophisticated.
[01:01:42.740 --> 01:01:44.500]   That is a very sophisticated distinction.
[01:01:44.500 --> 01:01:47.660]   There is a mannequin going around
[01:01:47.660 --> 01:01:48.820]   that has facial features.
[01:01:48.820 --> 01:01:50.260]   Jimmy Fallon interviewed her.
[01:01:50.260 --> 01:01:54.380]   Now, we actually-- she did some-- we saw her at CES.
[01:01:54.380 --> 01:01:55.380]   The scene at Folk's Dead.
[01:01:55.380 --> 01:01:57.740]   We watched her take her first steps.
[01:01:57.740 --> 01:01:58.820]   Well, Sophia is her name.
[01:01:58.820 --> 01:01:59.700]   Sophia.
[01:01:59.700 --> 01:02:01.540]   Was it creepy as happy?
[01:02:01.540 --> 01:02:02.140]   Oh, yeah.
[01:02:02.140 --> 01:02:02.660]   Yeah.
[01:02:02.660 --> 01:02:06.260]   But they're building it as artificial intelligence as--
[01:02:06.260 --> 01:02:07.380]   and it's not.
[01:02:07.380 --> 01:02:09.460]   It is a robot.
[01:02:09.460 --> 01:02:12.700]   The humans are telling it exactly what to say, what to do.
[01:02:12.700 --> 01:02:15.900]   In fact, there was an article this week about the head of AI
[01:02:15.900 --> 01:02:17.020]   at Facebook.
[01:02:17.020 --> 01:02:19.540]   He says, I hate Sophia.
[01:02:19.540 --> 01:02:20.740]   I hate it.
[01:02:20.740 --> 01:02:22.820]   It is from Hanson, robotics.
[01:02:22.820 --> 01:02:27.860]   And he says, it is not-- this is Jan Lecun.
[01:02:27.860 --> 01:02:31.820]   And he's-- he's head of AI research at Facebook.
[01:02:31.820 --> 01:02:35.300]   He says, the whole thing is complete BS.
[01:02:35.300 --> 01:02:37.420]   Sophia is to artificial intelligence
[01:02:37.420 --> 01:02:41.060]   as press to digitization as to real magic.
[01:02:41.060 --> 01:02:47.380]   It's not-- and I have to say, I kind of agree.
[01:02:47.380 --> 01:02:49.180]   And I think your daughter knows the difference, which
[01:02:49.180 --> 01:02:50.260]   is what's really interesting.
[01:02:50.260 --> 01:02:50.760]   Yeah.
[01:02:50.760 --> 01:02:51.820]   She knows the difference.
[01:02:51.820 --> 01:02:53.220]   And it is really interesting.
[01:02:53.220 --> 01:02:56.140]   And yet, Sophia-- and whatever she is,
[01:02:56.140 --> 01:02:57.900]   you could say it's real or not.
[01:02:57.900 --> 01:03:00.940]   But she is part of getting us used to the idea
[01:03:00.940 --> 01:03:02.860]   that there are going to be robots who interact with you
[01:03:02.860 --> 01:03:05.340]   on a human level to the extent that Chrissy Teigen is
[01:03:05.340 --> 01:03:07.540]   getting in Twitter battles with her about her makeup.
[01:03:07.540 --> 01:03:09.860]   Yeah, but she's not getting in Twitter battles with Sophia.
[01:03:09.860 --> 01:03:12.620]   She's getting in Twitter battles with a human pretending
[01:03:12.620 --> 01:03:13.260]   to be Sophia.
[01:03:13.260 --> 01:03:14.340]   But it's convincing us.
[01:03:14.340 --> 01:03:14.860]   It's very important.
[01:03:14.860 --> 01:03:15.820]   It's convincing us.
[01:03:15.820 --> 01:03:16.320]   Yeah.
[01:03:16.320 --> 01:03:17.460]   It's an important distinction, though, right?
[01:03:17.460 --> 01:03:17.860]   Oh, sure.
[01:03:17.860 --> 01:03:18.360]   Yeah.
[01:03:18.360 --> 01:03:20.380]   There's a human writing everything Sophia says.
[01:03:20.380 --> 01:03:21.140]   Absolutely.
[01:03:21.140 --> 01:03:21.340]   Yeah.
[01:03:21.340 --> 01:03:24.260]   She's more of an AI, like a robotic puppet.
[01:03:24.260 --> 01:03:24.820]   She's a puppet.
[01:03:24.820 --> 01:03:26.260]   And she is an actual AI.
[01:03:26.260 --> 01:03:26.760]   Yeah.
[01:03:26.760 --> 01:03:27.260]   Right.
[01:03:27.260 --> 01:03:29.120]   But I love it, because I think your daughter knows
[01:03:29.120 --> 01:03:31.060]   there is a difference between a robot and an AI.
[01:03:31.060 --> 01:03:31.560]   Yeah.
[01:03:31.560 --> 01:03:32.540]   And you can yell at robots.
[01:03:32.540 --> 01:03:34.660]   I think it's interesting that she thinks you can't yell at AI.
[01:03:34.660 --> 01:03:36.820]   That's what-- that was so strange to me.
[01:03:36.820 --> 01:03:38.020]   You can't yell at AI.
[01:03:38.020 --> 01:03:40.940]   Because AI will know that you're yelling.
[01:03:40.940 --> 01:03:43.080]   Well, maybe she's preparing for the future with the robots
[01:03:43.080 --> 01:03:43.540]   that they're revenge.
[01:03:43.540 --> 01:03:46.020]   Absolutely.
[01:03:46.020 --> 01:03:47.940]   Let's take a break on that note.
[01:03:47.940 --> 01:03:49.360]   If you're looking for work--
[01:03:49.360 --> 01:03:52.560]   [LAUGHTER]
[01:03:52.560 --> 01:03:53.720]   Oh, boy.
[01:03:53.720 --> 01:03:55.520]   This might be an ad for you.
[01:03:55.520 --> 01:03:57.560]   Actually, if you're looking for your next great job,
[01:03:57.560 --> 01:03:59.000]   zippracruder.com.
[01:03:59.000 --> 01:04:03.040]   And if you're a hiring person, an HR person,
[01:04:03.040 --> 01:04:05.240]   zippracruder is the absolute place
[01:04:05.240 --> 01:04:08.720]   to go to find the best candidates.
[01:04:08.720 --> 01:04:10.520]   Everybody in every business should
[01:04:10.520 --> 01:04:13.520]   know that hiring is the most important thing, simply
[01:04:13.520 --> 01:04:15.400]   the most important thing you do.
[01:04:15.400 --> 01:04:16.160]   You're managers.
[01:04:16.160 --> 01:04:17.160]   You know.
[01:04:17.160 --> 01:04:20.580]   The people you hire make or break a company.
[01:04:20.580 --> 01:04:22.080]   A company is just made of people.
[01:04:22.080 --> 01:04:23.580]   So make your hiring process better.
[01:04:23.580 --> 01:04:27.060]   Hire the best people faster, more efficiently,
[01:04:27.060 --> 01:04:27.920]   with zippracruder.
[01:04:27.920 --> 01:04:29.960]   No one has done a better job of transforming
[01:04:29.960 --> 01:04:32.180]   how you find the right talent than a zippracruder.
[01:04:32.180 --> 01:04:34.480]   First of all, one post on zippracruder
[01:04:34.480 --> 01:04:37.120]   posts your job to over 100 of the web's leading job board.
[01:04:37.120 --> 01:04:39.680]   So right there, you're reaching more people.
[01:04:39.680 --> 01:04:40.440]   That's important.
[01:04:40.440 --> 01:04:41.880]   The right person's out there.
[01:04:41.880 --> 01:04:43.960]   But do you know where they are, which job board
[01:04:43.960 --> 01:04:45.000]   they're looking at?
[01:04:45.000 --> 01:04:47.200]   But zippracruder goes a step farther.
[01:04:47.200 --> 01:04:49.680]   They actively look for the most qualified candidates
[01:04:49.680 --> 01:04:51.000]   and then invite them to apply.
[01:04:51.000 --> 01:04:53.560]   It's like having a headhunter working for you.
[01:04:53.560 --> 01:04:55.480]   And then when they get the applications,
[01:04:55.480 --> 01:04:58.760]   they automatically review them and identify the top candidates
[01:04:58.760 --> 01:05:00.320]   so you never miss a great match.
[01:05:00.320 --> 01:05:03.360]   And those candidates don't roll into your personal inbox.
[01:05:03.360 --> 01:05:04.880]   They don't call you at work.
[01:05:04.880 --> 01:05:07.040]   They all go into the zippracruder interface,
[01:05:07.040 --> 01:05:09.800]   which makes it easy to screen them, rate them,
[01:05:09.800 --> 01:05:11.560]   and hire the right person fast.
[01:05:11.560 --> 01:05:15.280]   Zippracruder doesn't depend on hiring on the right candidates
[01:05:15.280 --> 01:05:16.160]   finding you.
[01:05:16.160 --> 01:05:17.840]   It finds them.
[01:05:17.840 --> 01:05:19.960]   And I'll tell you, this is the stat that proves it.
[01:05:19.960 --> 01:05:24.320]   80% of employers, 80% who post on zippracruder
[01:05:24.320 --> 01:05:28.680]   get a quality candidate through the site in just one day.
[01:05:28.680 --> 01:05:29.440]   It's that fast.
[01:05:29.440 --> 01:05:32.960]   Zippracruder, the smartest way to hire-- find out today
[01:05:32.960 --> 01:05:34.960]   why zippracruder has been used by businesses
[01:05:34.960 --> 01:05:38.200]   in every industry, businesses of every size.
[01:05:38.200 --> 01:05:38.840]   We've used it.
[01:05:38.840 --> 01:05:41.560]   It's amazing to find the most qualified job
[01:05:41.560 --> 01:05:43.280]   candidates with immediate results.
[01:05:43.280 --> 01:05:46.040]   And right now, you could post on zippracruder free.
[01:05:46.040 --> 01:05:50.680]   Just go to zippracruder.com/twit, zippracruder.com/twit.
[01:05:50.680 --> 01:05:55.480]   The smartest way to hire zippracruder.com/twit.
[01:05:55.480 --> 01:05:56.680]   We thank them for their support.
[01:05:56.680 --> 01:06:00.320]   And I think that between the robot and this grocery store,
[01:06:00.320 --> 01:06:01.320]   there's going to be a lot of people
[01:06:01.320 --> 01:06:02.680]   looking for work in the near future.
[01:06:02.680 --> 01:06:03.680]   Sure.
[01:06:03.680 --> 01:06:06.000]   Actually, warm up your typing.
[01:06:06.000 --> 01:06:07.200]   Seriously?
[01:06:07.200 --> 01:06:10.960]   That's probably a good area to be in this job placement market,
[01:06:10.960 --> 01:06:12.160]   right?
[01:06:12.160 --> 01:06:13.880]   Who made the money during the gold rush?
[01:06:13.880 --> 01:06:14.640]   Leave eyes.
[01:06:14.640 --> 01:06:15.960]   As long as somebody's hiring.
[01:06:15.960 --> 01:06:17.760]   And then leave eyes had--
[01:06:17.760 --> 01:06:19.360]   That was-- there was a lot of people buying blue keys.
[01:06:19.360 --> 01:06:20.920]   There was not enough labor.
[01:06:20.920 --> 01:06:23.080]   It was actually not enough labor in that case.
[01:06:23.080 --> 01:06:25.760]   And there were a lot of people.
[01:06:25.760 --> 01:06:28.240]   This may be the opposite, but as long as you're
[01:06:28.240 --> 01:06:31.440]   making money in both directions, it'll work.
[01:06:31.440 --> 01:06:34.800]   I think it's time to start thinking about the future
[01:06:34.800 --> 01:06:38.440]   as an employee, prospective employee, or employer.
[01:06:38.440 --> 01:06:39.840]   Actually, your kids, our kids should
[01:06:39.840 --> 01:06:42.480]   start thinking about the future as what businesses should we
[01:06:42.480 --> 01:06:43.640]   start, right?
[01:06:43.640 --> 01:06:44.640]   Absolutely.
[01:06:44.640 --> 01:06:46.720]   Yeah, there is some talk that this generation that's
[01:06:46.720 --> 01:06:51.640]   the age of our children is called the founder's generation,
[01:06:51.640 --> 01:06:53.920]   which I kind of raise my eyebrows at because they're not
[01:06:53.920 --> 01:06:55.000]   old enough to be founders yet.
[01:06:55.000 --> 01:06:57.280]   Maybe it's more about how they have to be.
[01:06:57.280 --> 01:06:58.640]   That's all chiefs know Indians.
[01:06:58.640 --> 01:07:00.080]   Somebody has to be an employee.
[01:07:00.080 --> 01:07:00.760]   The robots.
[01:07:00.760 --> 01:07:01.480]   Oh, yeah, that's right.
[01:07:01.480 --> 01:07:03.240]   The AI employees.
[01:07:03.240 --> 01:07:06.440]   Make sure the company you found is staffed by robots.
[01:07:06.440 --> 01:07:08.720]   The economy is just going to get so big because we'll
[01:07:08.720 --> 01:07:10.760]   all be founders with our AI minions.
[01:07:10.760 --> 01:07:13.280]   You know who's not sorry he fired somebody.
[01:07:13.280 --> 01:07:16.920]   Sundar Pichai CEO of Google, they asked him at this event
[01:07:16.920 --> 01:07:19.240]   that Rico did with MSNBC.
[01:07:19.240 --> 01:07:23.480]   Cara Swisher and Ari Melber from MSNBC,
[01:07:23.480 --> 01:07:27.440]   interviewing Sundar Pichai, Susan Wojcicki of YouTube.
[01:07:27.440 --> 01:07:30.000]   And of course, it's Cara.
[01:07:30.000 --> 01:07:33.200]   Cara's going to always ask the tough questions.
[01:07:33.200 --> 01:07:36.360]   Pichai talking about James Damore, the employee who wrote
[01:07:36.360 --> 01:07:39.920]   the memo saying, why are we working so hard to hire women?
[01:07:39.920 --> 01:07:43.520]   They don't like technology.
[01:07:43.520 --> 01:07:47.120]   Said he does not regret firing Damore.
[01:07:47.120 --> 01:07:49.640]   He only regrets people thought of it as political.
[01:07:49.640 --> 01:07:53.720]   Because Damore's suing saying that Google discriminates
[01:07:53.720 --> 01:07:55.880]   against conservative employees.
[01:07:55.880 --> 01:07:58.960]   Pichai said, I regret that people misunderstand
[01:07:58.960 --> 01:08:01.520]   that we made this for a political belief when we're
[01:08:01.520 --> 01:08:04.080]   in other-- it's important for the women at Google
[01:08:04.080 --> 01:08:06.560]   and all the people at Google, we want
[01:08:06.560 --> 01:08:08.880]   to make an inclusive environment.
[01:08:08.880 --> 01:08:09.280]   I mean, I cannot--
[01:08:09.280 --> 01:08:10.200]   I do not regret it.
[01:08:10.200 --> 01:08:11.480]   I think that's sort of a distinction
[01:08:11.480 --> 01:08:13.280]   without a difference, honestly.
[01:08:13.280 --> 01:08:15.920]   I mean, I think that's all they've said all the way along
[01:08:15.920 --> 01:08:17.440]   is this is just the wrong thing to do.
[01:08:17.440 --> 01:08:18.920]   If you want to interpret that as political--
[01:08:18.920 --> 01:08:21.520]   I think what he's saying is, if people interpret that
[01:08:21.520 --> 01:08:23.680]   as political, that's a bummer.
[01:08:23.680 --> 01:08:25.840]   But we've always said it was just the right thing to do.
[01:08:25.840 --> 01:08:28.480]   But they're being sued saying Google discriminates
[01:08:28.480 --> 01:08:31.440]   against white male conservatives.
[01:08:31.440 --> 01:08:32.440]   Yeah.
[01:08:32.440 --> 01:08:35.560]   I mean, all they have to do is look at the number of--
[01:08:35.560 --> 01:08:36.080]   White males?
[01:08:36.080 --> 01:08:38.400]   White men employed by Google.
[01:08:38.400 --> 01:08:40.360]   And probably many of them Republicans.
[01:08:40.360 --> 01:08:42.400]   I mean, I have no idea what their political stance is.
[01:08:42.400 --> 01:08:45.160]   And I don't think they actually really care.
[01:08:45.160 --> 01:08:46.680]   They're not-- I'm sure they're not--
[01:08:46.680 --> 01:08:48.720]   I mean, Google can track anything they want.
[01:08:48.720 --> 01:08:50.480]   But I'm sure they're not really tracking that.
[01:08:50.480 --> 01:08:54.720]   If you look statistically at the employment roles at Google,
[01:08:54.720 --> 01:08:58.520]   I cannot believe the claim that white men are being discriminated.
[01:08:58.520 --> 01:08:59.520]   Yeah, that's--
[01:08:59.520 --> 01:09:01.520]   I might have a few white men at Google.
[01:09:01.520 --> 01:09:02.520]   There's a few.
[01:09:02.520 --> 01:09:03.520]   That's a good point.
[01:09:03.520 --> 01:09:04.520]   That's actually a good point.
[01:09:04.520 --> 01:09:08.520]   And I don't even think that he fired him because of the fact
[01:09:08.520 --> 01:09:09.480]   that it was political.
[01:09:09.480 --> 01:09:11.160]   I think that it was just really bad press
[01:09:11.160 --> 01:09:12.240]   and made Google look bad.
[01:09:12.240 --> 01:09:14.400]   And because of that, they lost a whole bunch of money
[01:09:14.400 --> 01:09:17.200]   and got-- again, it's all about branding.
[01:09:17.200 --> 01:09:19.440]   And so he costs them a whole bunch of money.
[01:09:19.440 --> 01:09:21.280]   And if you cost your company a whole bunch of money,
[01:09:21.280 --> 01:09:22.720]   you're probably going to get fired.
[01:09:22.720 --> 01:09:25.000]   Susan Wojcicki did say it personally
[01:09:25.000 --> 01:09:26.560]   heard her feelings what they more wrote.
[01:09:26.560 --> 01:09:28.360]   She was insulted by it.
[01:09:28.360 --> 01:09:29.040]   It is insulting.
[01:09:29.040 --> 01:09:31.000]   And the other thing that Sundar is saying
[01:09:31.000 --> 01:09:35.360]   is it makes it harder for us to hire women if it seems
[01:09:35.360 --> 01:09:39.140]   that we are discriminating against women
[01:09:39.140 --> 01:09:40.480]   or that we are full of employees who
[01:09:40.480 --> 01:09:43.280]   are going to create a hostile work environment for them.
[01:09:43.280 --> 01:09:45.920]   And Google has, like many Silicon Valley companies, been
[01:09:45.920 --> 01:09:48.960]   actually working very hard to create a more inclusive
[01:09:48.960 --> 01:09:49.480]   environment.
[01:09:49.480 --> 01:09:51.440]   And if you think about it from a business standpoint,
[01:09:51.440 --> 01:09:52.360]   that makes a lot of sense.
[01:09:52.360 --> 01:09:55.120]   Women spend more money in the United States than men.
[01:09:55.120 --> 01:09:59.200]   So if you have a diverse staff, you're
[01:09:59.200 --> 01:10:02.560]   more likely to create a product that people will buy
[01:10:02.560 --> 01:10:03.520]   and spend money on.
[01:10:03.520 --> 01:10:06.680]   And it's just from a business standpoint, it makes sense.
[01:10:06.680 --> 01:10:08.400]   I mean, I think you can make a really strong case
[01:10:08.400 --> 01:10:10.400]   that Silicon Valley would make better products if it had
[01:10:10.400 --> 01:10:11.760]   a more diverse workforce.
[01:10:11.760 --> 01:10:13.480]   Or a diverse in all ways.
[01:10:13.480 --> 01:10:16.000]   The more you represent the public,
[01:10:16.000 --> 01:10:18.200]   the better you can make products that the public will use.
[01:10:18.200 --> 01:10:19.240]   We've said that for years.
[01:10:19.240 --> 01:10:22.240]   One of the reasons computing technology technology
[01:10:22.240 --> 01:10:25.520]   is so hard to use is because it's designed by engineers.
[01:10:25.520 --> 01:10:27.840]   Be sure-- it'd be nice if you had a few normal people
[01:10:27.840 --> 01:10:28.680]   in the doors as well.
[01:10:28.680 --> 01:10:30.480]   I mean, obviously, engineers have to design it,
[01:10:30.480 --> 01:10:32.960]   but they don't have a really good grasp of what normal people
[01:10:32.960 --> 01:10:35.520]   go through to try to figure out how to use this stuff.
[01:10:35.520 --> 01:10:37.120]   They go, well, it's obvious.
[01:10:37.120 --> 01:10:40.000]   You just click the mouse, you do the thing, and the--
[01:10:40.000 --> 01:10:41.640]   and people go, huh?
[01:10:41.640 --> 01:10:44.800]   So obviously, a little bit of diversity wouldn't hurt.
[01:10:44.800 --> 01:10:47.560]   Hey, we may be getting a HomePod pretty soon.
[01:10:47.560 --> 01:10:51.720]   FCC approval for the Apple device
[01:10:51.720 --> 01:10:55.240]   that was supposed to come out at Christmas and didn't.
[01:10:55.240 --> 01:10:59.120]   Supplier in Ventech is it has apparently
[01:10:59.120 --> 01:11:01.880]   shipped the first million HomePods.
[01:11:01.880 --> 01:11:05.120]   They expect to ship as many as a dozen--
[01:11:05.120 --> 01:11:06.200]   10 to 12 million more.
[01:11:06.200 --> 01:11:09.400]   This is according to the Taipei Times.
[01:11:09.400 --> 01:11:13.280]   And that would make sense since it's about four to six weeks
[01:11:13.280 --> 01:11:14.800]   into the--
[01:11:14.800 --> 01:11:17.160]   it's going to be about four to six weeks into the year.
[01:11:17.160 --> 01:11:19.120]   That's about when they would hit the store shelf.
[01:11:19.120 --> 01:11:20.760]   So if you've been waiting--
[01:11:20.760 --> 01:11:24.880]   did it seem like it's CES that Apple and Microsoft
[01:11:24.880 --> 01:11:27.240]   were kind of in the backseat for voice assistance,
[01:11:27.240 --> 01:11:29.640]   and that was really Google and Amazon's show?
[01:11:29.640 --> 01:11:31.240]   Sure, for sure.
[01:11:31.240 --> 01:11:35.080]   I mean, well, Microsoft is essentially off.
[01:11:35.080 --> 01:11:36.080]   Their off-the-tape.
[01:11:36.080 --> 01:11:37.320]   That chip has sailed.
[01:11:37.320 --> 01:11:38.960]   A few people really like Cortana, but it's just--
[01:11:38.960 --> 01:11:43.320]   I think I heard one headline at CES that Cortana showed up
[01:11:43.320 --> 01:11:45.840]   in one thermostat.
[01:11:45.840 --> 01:11:46.560]   That was it.
[01:11:46.560 --> 01:11:49.280]   And that was the gas, the glass, you know what it is?
[01:11:49.280 --> 01:11:51.200]   I really want to meet somebody who's like,
[01:11:51.200 --> 01:11:52.480]   I'm going to go buy the Cortana thermostat.
[01:11:52.480 --> 01:11:54.840]   I need that Cortana thermostat.
[01:11:54.840 --> 01:11:56.520]   So even the glass people said, we're
[01:11:56.520 --> 01:11:58.280]   going to put Google Assistant in it later.
[01:11:58.280 --> 01:11:59.280]   Sure, yeah.
[01:11:59.280 --> 01:12:00.280]   Don't worry.
[01:12:00.280 --> 01:12:00.680]   Don't worry.
[01:12:00.680 --> 01:12:02.280]   They did get them a headline.
[01:12:02.280 --> 01:12:05.680]   It was-- what's crazy about this story to me
[01:12:05.680 --> 01:12:11.760]   is that the plans to ship 10 to 12 million units
[01:12:11.760 --> 01:12:14.840]   seems really optimistic to me, because that
[01:12:14.840 --> 01:12:17.280]   is going to be an expensive piece of hardware.
[01:12:17.280 --> 01:12:20.880]   And the reason that so many people own digital assistant
[01:12:20.880 --> 01:12:23.520]   speakers now is because the price point came so far down.
[01:12:23.520 --> 01:12:25.680]   You're going to echo dot for essentially 40 bucks.
[01:12:25.680 --> 01:12:26.200]   40 bucks.
[01:12:26.200 --> 01:12:28.560]   And the Google little mini is the same price.
[01:12:28.560 --> 01:12:30.640]   Same price, or it was during the holidays.
[01:12:30.640 --> 01:12:33.120]   And Apple is going to come out with a premium product.
[01:12:33.120 --> 01:12:34.360]   I bought a Google Home Max.
[01:12:34.360 --> 01:12:36.920]   They gave me a small, a medium Google Home.
[01:12:36.920 --> 01:12:38.640]   They're so anxious to get out in the market.
[01:12:38.640 --> 01:12:39.680]   Yeah, they just want those numbers.
[01:12:39.680 --> 01:12:41.080]   It's like magazine subscriptions.
[01:12:41.080 --> 01:12:41.800]   Where do you get them?
[01:12:41.800 --> 01:12:43.760]   And you don't want them because they
[01:12:43.760 --> 01:12:46.880]   need to say that there are a lot of power there.
[01:12:46.880 --> 01:12:49.760]   I think Apple's probably not going to ship as many of these
[01:12:49.760 --> 01:12:50.280]   as they want.
[01:12:50.280 --> 01:12:53.600]   However, I don't think they're as far behind as everybody else
[01:12:53.600 --> 01:12:56.480]   says they are, because people use Siri.
[01:12:56.480 --> 01:12:57.880]   They're already using it on their phone.
[01:12:57.880 --> 01:12:58.600]   They're already using it.
[01:12:58.600 --> 01:12:59.840]   It's already baked into there.
[01:12:59.840 --> 01:13:02.440]   In fact, because so many people use iPhones,
[01:13:02.440 --> 01:13:05.520]   I actually think that Siri is way more a part of people's lives
[01:13:05.520 --> 01:13:06.760]   than we all give it credit for.
[01:13:06.760 --> 01:13:08.440]   That may be true.
[01:13:08.440 --> 01:13:10.760]   In mobile, probably people mostly use Siri.
[01:13:10.760 --> 01:13:12.840]   At home, they're using Amazon.
[01:13:12.840 --> 01:13:15.080]   Google is, of course, well positioned,
[01:13:15.080 --> 01:13:17.440]   but not yet taking the market.
[01:13:17.440 --> 01:13:20.440]   The Google claims 60 million activations over Christmas.
[01:13:20.440 --> 01:13:21.280]   Yeah, and I think they are.
[01:13:21.280 --> 01:13:22.200]   They are putting it.
[01:13:22.200 --> 01:13:24.520]   So Google showed up at CES in a very big way
[01:13:24.520 --> 01:13:26.400]   that they haven't for many, many years.
[01:13:26.400 --> 01:13:28.600]   And it was all their entire installation
[01:13:28.600 --> 01:13:30.440]   was about getting you to use the Google Assistant.
[01:13:30.440 --> 01:13:31.400]   Right.
[01:13:31.400 --> 01:13:35.720]   It's going to be hard, even if Apple has this hub.
[01:13:35.720 --> 01:13:38.080]   It's going to be, I think, a tough pitch, though,
[01:13:38.080 --> 01:13:39.840]   even to Apple fanatics.
[01:13:39.840 --> 01:13:42.440]   And I'm an Apple fanatic because we already
[01:13:42.440 --> 01:13:45.680]   picked an Amazon Echo, right?
[01:13:45.680 --> 01:13:47.480]   We already have something.
[01:13:47.480 --> 01:13:51.160]   And we don't know yet exactly how Apple is going to position
[01:13:51.160 --> 01:13:53.400]   this, but I would put a lot of money
[01:13:53.400 --> 01:13:55.400]   against the idea that Apple is going
[01:13:55.400 --> 01:13:58.360]   to say this is the best speaker you can get.
[01:13:58.360 --> 01:13:59.960]   It sounds better than anything else.
[01:13:59.960 --> 01:14:02.520]   It looks more beautiful.
[01:14:02.520 --> 01:14:04.280]   And it's better than Sonos.
[01:14:04.280 --> 01:14:06.720]   But I think Sonos itself has proven
[01:14:06.720 --> 01:14:08.000]   that that's a narrow market.
[01:14:08.000 --> 01:14:08.960]   Yeah.
[01:14:08.960 --> 01:14:10.080]   It's over for Sonos.
[01:14:10.080 --> 01:14:11.400]   It's got to be over for Sonos.
[01:14:11.400 --> 01:14:11.840]   I'm sorry.
[01:14:11.840 --> 01:14:13.280]   I love Sonos.
[01:14:13.280 --> 01:14:17.480]   I have highly invested in Sonos in every room here
[01:14:17.480 --> 01:14:19.280]   and everywhere but home.
[01:14:19.280 --> 01:14:21.160]   But it's just got to be over for Sonos.
[01:14:21.160 --> 01:14:23.720]   They came too late to the game and they didn't add enough
[01:14:23.720 --> 01:14:24.480]   capability.
[01:14:24.480 --> 01:14:27.000]   I wouldn't be surprised if Sonos sold itself at some point.
[01:14:27.000 --> 01:14:28.520]   They're privately on company, right?
[01:14:28.520 --> 01:14:29.840]   Yeah, Apple should buy it.
[01:14:29.840 --> 01:14:33.080]   Well, or Amazon should buy them.
[01:14:33.080 --> 01:14:36.840]   Because Amazon actually doesn't really have a premium play.
[01:14:36.840 --> 01:14:40.000]   I have the Sonos 1, which is a little Sonos speaker
[01:14:40.000 --> 01:14:42.160]   that they put Amazon's echo in.
[01:14:42.160 --> 01:14:43.400]   But it's not a complete echo.
[01:14:43.400 --> 01:14:44.680]   Amazon won't let them do--
[01:14:44.680 --> 01:14:48.080]   I presume it's Amazon that won't let them do a full echo
[01:14:48.080 --> 01:14:48.560]   capability.
[01:14:48.560 --> 01:14:50.600]   For instance, I can't listen to my audiobooks on it.
[01:14:50.600 --> 01:14:52.200]   So I'm not going to buy it.
[01:14:52.200 --> 01:14:53.480]   I'm not going to buy the other one.
[01:14:53.480 --> 01:14:55.400]   Look at Amazon bots, Sonos.
[01:14:55.400 --> 01:14:58.720]   They're nearly what I could.
[01:14:58.720 --> 01:15:00.320]   But really, do you need--
[01:15:00.320 --> 01:15:02.680]   speakers are a commodity now.
[01:15:02.680 --> 01:15:05.480]   Anybody can make a good speaker system.
[01:15:05.480 --> 01:15:06.720]   It's off the shelf.
[01:15:06.720 --> 01:15:09.160]   It's not hard to do, is it?
[01:15:09.160 --> 01:15:12.080]   And people aren't saying, well, I have to have that finely tuned
[01:15:12.080 --> 01:15:13.080]   B&O speaker.
[01:15:13.080 --> 01:15:15.200]   I mean, you're not competing at that level.
[01:15:15.200 --> 01:15:17.720]   You're just competing with all the other guys.
[01:15:17.720 --> 01:15:20.480]   It feels like that's-- what is Sonos selling
[01:15:20.480 --> 01:15:22.920]   that isn't generic at this point?
[01:15:22.920 --> 01:15:24.360]   At this point, it's just sound quality.
[01:15:24.360 --> 01:15:24.880]   That's it.
[01:15:24.880 --> 01:15:25.440]   Yeah, that's so nice.
[01:15:25.440 --> 01:15:26.840]   And Apple's going to try to compete against that.
[01:15:26.840 --> 01:15:28.640]   But again, like you're saying, it's not enough.
[01:15:28.640 --> 01:15:29.320]   It's not enough.
[01:15:29.320 --> 01:15:30.240]   Probably not enough for Apple.
[01:15:30.240 --> 01:15:31.920]   Even Amazon doesn't really need it.
[01:15:31.920 --> 01:15:32.880]   Why would you buy Sonos?
[01:15:32.880 --> 01:15:33.600]   You just make one.
[01:15:33.600 --> 01:15:35.400]   This is good as a Sonos.
[01:15:35.400 --> 01:15:38.000]   That's not so hard to do.
[01:15:38.000 --> 01:15:40.040]   All right.
[01:15:40.040 --> 01:15:42.480]   And by the way, first alert, put Amazon's echo
[01:15:42.480 --> 01:15:44.400]   into your smoke alarm.
[01:15:44.400 --> 01:15:44.900]   Sure.
[01:15:44.900 --> 01:15:46.080]   So you put a smoke alarm.
[01:15:46.080 --> 01:15:47.520]   If you're building a new house, you
[01:15:47.520 --> 01:15:48.560]   have to put in a smoke alarm.
[01:15:48.560 --> 01:15:49.760]   And at least you do in California.
[01:15:49.760 --> 01:15:50.840]   I presume in most places.
[01:15:50.840 --> 01:15:52.560]   You have to put a smoke alarm in every room.
[01:15:52.560 --> 01:15:53.960]   Might as well have a B echo enabled.
[01:15:53.960 --> 01:15:54.960]   And now, echo's in every room.
[01:15:54.960 --> 01:15:55.240]   Boom.
[01:15:55.240 --> 01:15:55.600]   Sure.
[01:15:55.600 --> 01:15:56.840]   The price point's pretty high.
[01:15:56.840 --> 01:15:57.800]   Oh, is it?
[01:15:57.800 --> 01:15:58.560]   Yeah.
[01:15:58.560 --> 01:16:00.960]   Considering that you can buy a decent smoke alarm
[01:16:00.960 --> 01:16:02.160]   for $5.
[01:16:02.160 --> 01:16:02.720]   $2.
[01:16:02.720 --> 01:16:04.800]   Basically, that's a little bit of a hard sell.
[01:16:04.800 --> 01:16:06.320]   But it's really interesting, especially
[01:16:06.320 --> 01:16:08.640]   if you want to strategically place it.
[01:16:08.640 --> 01:16:10.720]   And we also saw Phillips with Alexa
[01:16:10.720 --> 01:16:12.840]   and light bulbs at the show.
[01:16:12.840 --> 01:16:15.240]   So then they have Hughes with Alexa built in.
[01:16:15.240 --> 01:16:15.560]   Yeah.
[01:16:15.560 --> 01:16:16.960]   So you can actually just talk to your ceiling
[01:16:16.960 --> 01:16:18.240]   in all kinds of ways.
[01:16:18.240 --> 01:16:21.480]   I think you're just going to cease these devices everywhere.
[01:16:21.480 --> 01:16:24.400]   What is-- we talked earlier about privacy with the cars.
[01:16:24.400 --> 01:16:26.080]   What is the privacy?
[01:16:26.080 --> 01:16:28.920]   What do you think, Georgia, people?
[01:16:28.920 --> 01:16:31.560]   I can't get my finger on this one,
[01:16:31.560 --> 01:16:33.360]   because we have a lot of viewers who
[01:16:33.360 --> 01:16:35.120]   are very concerned about privacy.
[01:16:35.120 --> 01:16:40.440]   In fact, a lot of hosts that are tin foil hat wearers.
[01:16:40.440 --> 01:16:41.800]   I, on the other hand, am completely
[01:16:41.800 --> 01:16:43.200]   in the other end of the spectrum,
[01:16:43.200 --> 01:16:45.920]   because I want all the convenience and functionality
[01:16:45.920 --> 01:16:50.720]   that giving up privacy, like walking into that store--
[01:16:50.720 --> 01:16:52.160]   it doesn't bother me.
[01:16:52.160 --> 01:16:56.720]   So Georgia, I'm going to assume you know some normal people.
[01:16:56.720 --> 01:17:00.640]   Or at least have your finger on the pulse of normals.
[01:17:00.640 --> 01:17:03.320]   Are normal people concerned about this?
[01:17:03.320 --> 01:17:06.000]   I think that a lot of people are concerned about their privacy,
[01:17:06.000 --> 01:17:07.840]   but I don't think people are concerned enough.
[01:17:07.840 --> 01:17:09.400]   I think that people will usually
[01:17:09.400 --> 01:17:11.960]   pick convenience over privacy.
[01:17:11.960 --> 01:17:14.960]   And I think that most people go by the thought of, well,
[01:17:14.960 --> 01:17:16.240]   if I'm not doing anything wrong, what
[01:17:16.240 --> 01:17:18.160]   does it really matter if they're harvesting whatever
[01:17:18.160 --> 01:17:20.800]   information that they want from us?
[01:17:20.800 --> 01:17:23.840]   In the end, we get so used to doing something
[01:17:23.840 --> 01:17:26.560]   that it doesn't just create any of our threat levels to that.
[01:17:26.560 --> 01:17:31.480]   But what we do is the most important set of information
[01:17:31.480 --> 01:17:34.720]   that we have, and to have that being sold off to anyone,
[01:17:34.720 --> 01:17:38.560]   in the end, they can find out all of our pattern recognition
[01:17:38.560 --> 01:17:39.720]   and figure out where you're going to be
[01:17:39.720 --> 01:17:42.320]   where and who you're seeing and what you're doing because of that.
[01:17:42.320 --> 01:17:44.960]   So I might be one of those tin foil hat.
[01:17:44.960 --> 01:17:45.920]   I think you are.
[01:17:45.920 --> 01:17:47.000]   Congratulations.
[01:17:47.000 --> 01:17:48.640]   But I think it's really--
[01:17:48.640 --> 01:17:50.360]   I think that it's something that's really important.
[01:17:50.360 --> 01:17:52.440]   I think that that is something that we are selling off
[01:17:52.440 --> 01:17:53.200]   for convenience.
[01:17:53.200 --> 01:17:54.480]   And I don't believe that we should.
[01:17:54.480 --> 01:17:57.080]   I think that if you choose to send that off,
[01:17:57.080 --> 01:18:00.960]   at least get something of value for it, not just convenience,
[01:18:00.960 --> 01:18:04.680]   which ends up being us saying complacency and laziness,
[01:18:04.680 --> 01:18:06.200]   which I get something of value.
[01:18:06.200 --> 01:18:08.920]   I get a lot of value for this.
[01:18:08.920 --> 01:18:12.160]   But most of the things that this can offer you,
[01:18:12.160 --> 01:18:14.960]   you could have a little bit less offer to you.
[01:18:14.960 --> 01:18:17.640]   And I think that that's what I'm excited about the HomePod,
[01:18:17.640 --> 01:18:19.960]   is that I'm going to be able to have something
[01:18:19.960 --> 01:18:22.480]   that I feel right now with Apple as it's set, who
[01:18:22.480 --> 01:18:23.800]   knows if it gets sold.
[01:18:23.800 --> 01:18:26.280]   But that my information is least secure.
[01:18:26.280 --> 01:18:28.360]   And so maybe I'm not going to be able to do as many things
[01:18:28.360 --> 01:18:31.680]   as I can with it, but that I don't have to worry about someone
[01:18:31.680 --> 01:18:34.640]   harvesting all of my information of what I do.
[01:18:34.640 --> 01:18:36.160]   And I think that that's really important.
[01:18:36.160 --> 01:18:38.880]   And we know that there's no way that they
[01:18:38.880 --> 01:18:42.000]   can keep this safe and secure from all of the data breaches
[01:18:42.000 --> 01:18:43.360]   that we've already seen.
[01:18:43.360 --> 01:18:46.240]   So it makes me feel a little bit more comfortable.
[01:18:46.240 --> 01:18:48.920]   I see very little evidence that people who use Apple products
[01:18:48.920 --> 01:18:52.440]   are somehow magically more protected than others.
[01:18:52.440 --> 01:18:55.800]   I think that most people I've talked to and a lot of people
[01:18:55.800 --> 01:19:00.360]   do bring up frequently, oh, don't those speakers freak you out?
[01:19:00.360 --> 01:19:01.920]   They're listening to you all the time.
[01:19:01.920 --> 01:19:04.240]   And I say, do you think that your phone is not?
[01:19:04.240 --> 01:19:06.080]   You carry a smartphone.
[01:19:06.080 --> 01:19:06.880]   It's done.
[01:19:06.880 --> 01:19:07.080]   Right.
[01:19:07.080 --> 01:19:08.640]   You're already carrying that technology around on you.
[01:19:08.640 --> 01:19:09.840]   I've got a microphone on you.
[01:19:09.840 --> 01:19:13.040]   GPS always on, always connected camera.
[01:19:13.040 --> 01:19:16.800]   What I think should happen is that whenever you turn on a service
[01:19:16.800 --> 01:19:20.280]   that is going to be passively collecting data on what you do,
[01:19:20.280 --> 01:19:24.000]   where you go any time you opt into that service,
[01:19:24.000 --> 01:19:29.640]   you should get a very bluntly, very short worded warning
[01:19:29.640 --> 01:19:34.400]   that says once you plug in this Alexa,
[01:19:34.400 --> 01:19:38.120]   there will be a record of your voice at all times.
[01:19:38.120 --> 01:19:38.800]   Warning.
[01:19:38.800 --> 01:19:40.400]   Kind of like a cigarette warning.
[01:19:40.400 --> 01:19:42.120]   I think that that should be-- but that
[01:19:42.120 --> 01:19:43.920]   is going to have to be legislated.
[01:19:43.920 --> 01:19:46.600]   And people probably will choose that convenience
[01:19:46.600 --> 01:19:47.960]   because they're used to it.
[01:19:47.960 --> 01:19:50.360]   But at least they know.
[01:19:50.360 --> 01:19:52.680]   And very clearly, because right now it's just so hard
[01:19:52.680 --> 01:19:54.280]   to understand what's happening.
[01:19:54.280 --> 01:19:56.080]   It's kind of like how everybody says back to Facebook
[01:19:56.080 --> 01:19:57.560]   is listening to you.
[01:19:57.560 --> 01:19:59.800]   Like listening to your actual conversations.
[01:19:59.800 --> 01:20:01.600]   But nobody has actually been able to prove it.
[01:20:01.600 --> 01:20:03.760]   See, well, one of the convene, I drive a Tesla.
[01:20:03.760 --> 01:20:05.960]   And so that means Elon Musk knows everywhere I go,
[01:20:05.960 --> 01:20:06.680]   everything I do.
[01:20:06.680 --> 01:20:08.080]   He knows when I put my foot on the brake.
[01:20:08.080 --> 01:20:09.720]   He knows when I put my foot on the accelerator.
[01:20:09.720 --> 01:20:11.200]   And they don't even hide it.
[01:20:11.200 --> 01:20:13.120]   I even know this because we called and said,
[01:20:13.120 --> 01:20:16.240]   I think the call went backwards when I put my foot on the accelerator.
[01:20:16.240 --> 01:20:18.240]   And they said, well, let's check.
[01:20:18.240 --> 01:20:18.720]   Let me see.
[01:20:18.720 --> 01:20:19.920]   What was the time?
[01:20:19.920 --> 01:20:21.400]   No, no, I went forwards.
[01:20:21.400 --> 01:20:27.280]   I mean, so I've given up all privacy, all privacy.
[01:20:27.280 --> 01:20:29.120]   You also do this for a living, though.
[01:20:29.120 --> 01:20:32.760]   You're on screen almost all moments as well.
[01:20:32.760 --> 01:20:34.720]   So probably for you, it's a little bit more harmful to that.
[01:20:34.720 --> 01:20:35.240]   I don't mind.
[01:20:35.240 --> 01:20:36.160]   Yeah, yeah.
[01:20:36.160 --> 01:20:38.360]   But I'm just wondering what the harm--
[01:20:38.360 --> 01:20:40.240]   I'm trying to find out what the harm.
[01:20:40.240 --> 01:20:41.920]   I say this every time.
[01:20:41.920 --> 01:20:44.840]   I understand conceptually, oh, yeah,
[01:20:44.840 --> 01:20:46.120]   you shouldn't give up privacy.
[01:20:46.120 --> 01:20:48.160]   But what is the actual harm we're talking?
[01:20:48.160 --> 01:20:49.880]   What's going to happen to you?
[01:20:49.880 --> 01:20:53.360]   The actual harm is that if you choose to go for office
[01:20:53.360 --> 01:20:55.640]   or you're going to be someone that's
[01:20:55.640 --> 01:20:58.560]   going to be going against whatever
[01:20:58.560 --> 01:21:00.680]   is the government of the time to that,
[01:21:00.680 --> 01:21:02.200]   that they can use all of the information
[01:21:02.200 --> 01:21:04.800]   that they have against you or control that.
[01:21:04.800 --> 01:21:05.680]   And they do.
[01:21:05.680 --> 01:21:08.480]   We've already seen that leaks of information come out,
[01:21:08.480 --> 01:21:11.160]   and it can destroy people's careers, what they're doing,
[01:21:11.160 --> 01:21:12.480]   where they're going to that.
[01:21:12.480 --> 01:21:14.360]   And so in the end, you end up being
[01:21:14.360 --> 01:21:16.360]   able to be much more easily manipulated
[01:21:16.360 --> 01:21:18.400]   because your information is going out there.
[01:21:18.400 --> 01:21:20.800]   And often you aren't unaware of that.
[01:21:20.800 --> 01:21:22.160]   I think it's the opposite.
[01:21:22.160 --> 01:21:23.560]   I think it's the end of embarrassment,
[01:21:23.560 --> 01:21:25.200]   as I was saying earlier.
[01:21:25.200 --> 01:21:27.440]   If everybody knows everything about everybody,
[01:21:27.440 --> 01:21:29.720]   then you can't be embarrassed by it.
[01:21:29.720 --> 01:21:30.160]   It hasn't.
[01:21:30.160 --> 01:21:32.160]   That's not the truth, though, right now, right?
[01:21:32.160 --> 01:21:34.160]   People's political careers and people that
[01:21:34.160 --> 01:21:37.280]   are opposition to thing have been destroyed because of this.
[01:21:37.280 --> 01:21:39.160]   So it's not right yet, right?
[01:21:39.160 --> 01:21:41.040]   I don't worry if I trip, and that's on video,
[01:21:41.040 --> 01:21:44.200]   because if I trip really to get that to be interesting at all,
[01:21:44.200 --> 01:21:44.720]   I have to like--
[01:21:44.720 --> 01:21:46.080]   Should their career be destroyed?
[01:21:46.080 --> 01:21:48.320]   --be paid on to make it be interesting.
[01:21:48.320 --> 01:21:50.640]   I mean, all you're saying is that the public learns
[01:21:50.640 --> 01:21:54.160]   about bad behavior and decides not to vote for that person.
[01:21:54.160 --> 01:21:55.760]   But it might not even be bad behavior.
[01:21:55.760 --> 01:21:58.000]   Often it can be political suicide, something
[01:21:58.000 --> 01:22:02.840]   that's embarrassing or ridiculous or effeminate or--
[01:22:02.840 --> 01:22:04.080]   But that's up to the voters.
[01:22:04.080 --> 01:22:05.440]   --to the mainstream.
[01:22:05.440 --> 01:22:06.600]   That's up to the voters.
[01:22:06.600 --> 01:22:09.760]   I think the voters get to decide whether that's
[01:22:09.760 --> 01:22:10.840]   suitable or not.
[01:22:10.840 --> 01:22:12.920]   That seems like there also be some sort of exposed fact,
[01:22:12.920 --> 01:22:17.080]   though, like persecution or prosecution, especially
[01:22:17.080 --> 01:22:20.720]   in countries where there is a little bit of a thought
[01:22:20.720 --> 01:22:21.960]   police kind of world.
[01:22:21.960 --> 01:22:23.840]   And if you are saying things that you
[01:22:23.840 --> 01:22:25.520]   think are private behind closed doors,
[01:22:25.520 --> 01:22:29.200]   and then later they are misinterpreted, or for instance,
[01:22:29.200 --> 01:22:31.960]   imagine you're having a conversation in your home
[01:22:31.960 --> 01:22:35.920]   and you are sarcastically or jokingly talking about like,
[01:22:35.920 --> 01:22:37.560]   well, we should murder that person.
[01:22:37.560 --> 01:22:39.160]   You're joking.
[01:22:39.160 --> 01:22:42.920]   But a jury may not know that if that's out of context later.
[01:22:42.920 --> 01:22:45.440]   It's somebody-- there are all kinds of scenarios
[01:22:45.440 --> 01:22:46.920]   we can't foresee.
[01:22:46.920 --> 01:22:49.800]   But when we're essentially being electively surveilled,
[01:22:49.800 --> 01:22:51.040]   it is a little--
[01:22:51.040 --> 01:22:51.880]   it's unnerving.
[01:22:51.880 --> 01:22:52.960]   We haven't seen where this is going.
[01:22:52.960 --> 01:22:55.480]   I feel like a lot of this is speculative, though.
[01:22:55.480 --> 01:22:56.520]   That occurs a lot of--
[01:22:56.520 --> 01:22:59.760]   Do you think that we shouldn't have a right to privacy, Ben?
[01:22:59.760 --> 01:23:00.960]   Oh, you have a right to privacy.
[01:23:00.960 --> 01:23:02.800]   You have the right to pursue privacy anyway you want to.
[01:23:02.800 --> 01:23:04.320]   But do we give that up to use?
[01:23:04.320 --> 01:23:05.240]   You don't get to tell--
[01:23:05.240 --> 01:23:07.240]   Should we give that up to use technology?
[01:23:07.240 --> 01:23:08.440]   Should that be the cause?
[01:23:08.440 --> 01:23:09.720]   No, no.
[01:23:09.720 --> 01:23:12.480]   If you choose not to get on the internet, not to use a cell
[01:23:12.480 --> 01:23:14.400]   phone, not to have microphones in your home,
[01:23:14.400 --> 01:23:15.880]   I have no problem with that.
[01:23:15.880 --> 01:23:18.240]   But I don't want anybody to tell me not to do it.
[01:23:18.240 --> 01:23:20.960]   And I don't want people to shut down businesses that do it.
[01:23:20.960 --> 01:23:22.440]   And that's what's happening.
[01:23:22.440 --> 01:23:25.800]   I think that I should be able to expect the right to privacy
[01:23:25.800 --> 01:23:30.040]   and still be able to use a phone cell phone in a certain manner
[01:23:30.040 --> 01:23:31.640]   whichever that might be, right?
[01:23:31.640 --> 01:23:32.760]   I think that's bloody.
[01:23:32.760 --> 01:23:35.680]   Them harvesting my information for profit.
[01:23:35.680 --> 01:23:38.000]   That's like saying I should be able to walk a tightrope
[01:23:38.000 --> 01:23:38.880]   without falling off.
[01:23:38.880 --> 01:23:40.120]   I mean, this is--
[01:23:40.120 --> 01:23:41.000]   Well, if you practice.
[01:23:41.000 --> 01:23:45.440]   If you're carrying a smartphone, I think that's--
[01:23:45.440 --> 01:23:47.120]   I think there's no way--
[01:23:47.120 --> 01:23:50.520]   I don't care if it's an Apple smartphone or not.
[01:23:50.520 --> 01:23:52.560]   You're presuming that you're somehow
[01:23:52.560 --> 01:23:53.840]   able to do that privately.
[01:23:53.840 --> 01:23:54.760]   You're not.
[01:23:54.760 --> 01:23:56.000]   Period.
[01:23:56.000 --> 01:23:57.840]   Doesn't require Apple to cooperate.
[01:23:57.840 --> 01:23:59.160]   You're just not.
[01:23:59.160 --> 01:24:00.760]   So--
[01:24:00.760 --> 01:24:02.040]   I just want people to know that.
[01:24:02.040 --> 01:24:02.520]   All I can have--
[01:24:02.520 --> 01:24:03.200]   Oh, I agree.
[01:24:03.200 --> 01:24:05.240]   We should tell people that.
[01:24:05.240 --> 01:24:06.240]   But what's the choice?
[01:24:06.240 --> 01:24:07.320]   Not be on the internet?
[01:24:07.320 --> 01:24:08.880]   Not carry a smartphone?
[01:24:08.880 --> 01:24:09.760]   Sure.
[01:24:09.760 --> 01:24:12.520]   I mean, if people may make that choice, not many.
[01:24:12.520 --> 01:24:13.560]   I don't think there's a half--
[01:24:13.560 --> 01:24:14.600]   I think Georgia--
[01:24:14.600 --> 01:24:15.880]   I understand what you're saying, but I don't think there's
[01:24:15.880 --> 01:24:16.520]   a half choice.
[01:24:16.520 --> 01:24:17.480]   I don't think you could say.
[01:24:17.480 --> 01:24:20.520]   Well, I'd like to use all these technologies and be private.
[01:24:20.520 --> 01:24:23.840]   By its very nature, it's not private.
[01:24:23.840 --> 01:24:26.320]   And it doesn't require Apple to be complicit or Google
[01:24:26.320 --> 01:24:27.560]   to be complicit.
[01:24:27.560 --> 01:24:30.240]   It's-- it's-- you know, I mean, if a government wants
[01:24:30.240 --> 01:24:32.760]   to eavesdrop on you, there's nothing to stop them.
[01:24:32.760 --> 01:24:37.320]   They just-- they just authorized the Pfizer Act again.
[01:24:37.320 --> 01:24:39.680]   There's nothing to stop you.
[01:24:39.680 --> 01:24:41.840]   So you know, I mean, you're carrying a microphone
[01:24:41.840 --> 01:24:43.280]   in your pocket and you're saying, well, I don't
[01:24:43.280 --> 01:24:44.280]   want anybody to listen.
[01:24:44.280 --> 01:24:49.560]   What are you going to do?
[01:24:49.560 --> 01:24:49.920]   I don't know.
[01:24:49.920 --> 01:24:50.960]   I think we're screwed.
[01:24:50.960 --> 01:24:52.920]   [LAUGHTER]
[01:24:52.920 --> 01:24:55.040]   But I don't think it's-- I don't feel screwed, but Tim,
[01:24:55.040 --> 01:24:57.920]   do you have an opinion you want to weigh in?
[01:24:57.920 --> 01:24:59.400]   I was just going to add that I actually
[01:24:59.400 --> 01:25:01.360]   added the trend of the arson culture and selfies.
[01:25:01.360 --> 01:25:02.920]   Everyone on the internet did this week.
[01:25:02.920 --> 01:25:05.040]   And I shot to the number of people who replied to me and said,
[01:25:05.040 --> 01:25:06.880]   oh, but now Google knows what you look like.
[01:25:06.880 --> 01:25:07.400]   Yes.
[01:25:07.400 --> 01:25:08.040]   And you know, I mean--
[01:25:08.040 --> 01:25:08.560]   Exactly.
[01:25:08.560 --> 01:25:10.160]   --I mean, of course Google knows what I look like.
[01:25:10.160 --> 01:25:11.400]   Come on, are you kidding me?
[01:25:11.400 --> 01:25:13.320]   What is I going to do with that?
[01:25:13.320 --> 01:25:15.720]   To this point, there's definitely a lack of awareness,
[01:25:15.720 --> 01:25:18.320]   I think, of what these companies already know about you.
[01:25:18.320 --> 01:25:21.120]   So I think some better legislation around disclaimers
[01:25:21.120 --> 01:25:22.400]   would be nice.
[01:25:22.400 --> 01:25:23.920]   But I think ultimately a lot of people
[01:25:23.920 --> 01:25:25.920]   have a lot to learn in terms of what companies already
[01:25:25.920 --> 01:25:27.120]   know about you.
[01:25:27.120 --> 01:25:29.040]   That's the name known for years, by the way.
[01:25:29.040 --> 01:25:31.200]   Here's one thing not to do if you--
[01:25:31.200 --> 01:25:33.080]   this happened on Friday.
[01:25:33.080 --> 01:25:37.160]   Guy passed out behind the wheel of his Tesla,
[01:25:37.160 --> 01:25:38.960]   got stopped by the CHP.
[01:25:38.960 --> 01:25:43.000]   He said, oh, don't worry, I'm on autopilot.
[01:25:43.000 --> 01:25:45.600]   He was at the CHP and the California Highway Patrol
[01:25:45.600 --> 01:25:48.320]   was at great pains to tweet this, arrested and charged
[01:25:48.320 --> 01:25:50.720]   the suspicion of DOI car towed.
[01:25:50.720 --> 01:25:54.440]   No, it did not drive itself to the tow yard.
[01:25:54.440 --> 01:25:55.120]   So just--
[01:25:55.120 --> 01:25:57.040]   Another case where people don't understand
[01:25:57.040 --> 01:25:59.640]   what their technology is doing, autopilot is not--
[01:25:59.640 --> 01:26:00.640]   it's self-driving cars.
[01:26:00.640 --> 01:26:00.960]   It's not autopilot.
[01:26:00.960 --> 01:26:03.160]   It's probably poorly named.
[01:26:03.160 --> 01:26:05.240]   But to be fair, the guy's blood alcohol level
[01:26:05.240 --> 01:26:06.560]   was twice the legal limit.
[01:26:06.560 --> 01:26:09.200]   So his judgment was a little impaired anyway.
[01:26:09.200 --> 01:26:10.000]   Sure.
[01:26:10.000 --> 01:26:13.200]   I wonder, if you get a DUI in a lot of places in California,
[01:26:13.200 --> 01:26:14.680]   you have to have a--
[01:26:14.680 --> 01:26:16.920]   and this is probably true in lots of places--
[01:26:16.920 --> 01:26:19.880]   you have to have a breathalyzer installed on your ignition.
[01:26:19.880 --> 01:26:21.120]   Yeah, I think that's a great thing.
[01:26:21.120 --> 01:26:24.440]   And I sort of wonder if that's just going to happen.
[01:26:24.440 --> 01:26:27.760]   If once that technology becomes cheaper and easier to install,
[01:26:27.760 --> 01:26:29.960]   insurers are simply going to say,
[01:26:29.960 --> 01:26:31.880]   your rate is going to be twice as expensive unless you
[01:26:31.880 --> 01:26:32.920]   have this on your car.
[01:26:32.920 --> 01:26:34.680]   You know what's interesting about that Google--
[01:26:34.680 --> 01:26:36.720]   And they just get their child to drive on it,
[01:26:36.720 --> 01:26:38.040]   your cat or something.
[01:26:38.040 --> 01:26:39.240]   You see where I'm going.
[01:26:39.240 --> 01:26:41.240]   As long as you can go to McDonald's.
[01:26:41.240 --> 01:26:43.600]   What?
[01:26:43.600 --> 01:26:46.880]   Every eight-year-old knows how to get to McDonald's.
[01:26:46.880 --> 01:26:49.280]   Actually, what was I going to say now?
[01:26:49.280 --> 01:26:50.480]   I'm sorry.
[01:26:50.480 --> 01:26:51.440]   You got me laughing.
[01:26:51.440 --> 01:26:54.440]   The cat is the cat talk, right?
[01:26:54.440 --> 01:26:55.880]   The cat driving the car.
[01:26:55.880 --> 01:26:58.720]   Tootses.
[01:26:58.720 --> 01:26:59.480]   Let's take a break.
[01:26:59.480 --> 01:27:01.960]   And I'll think of what I was going to--
[01:27:01.960 --> 01:27:02.720]   Sorry.
[01:27:02.720 --> 01:27:03.400]   --the mind brain.
[01:27:03.400 --> 01:27:04.200]   It's going.
[01:27:04.200 --> 01:27:07.560]   This is why I need a self-driving vehicle.
[01:27:07.560 --> 01:27:10.600]   My mind needs a self-driving vehicle.
[01:27:10.600 --> 01:27:12.520]   Our show today brought to you by Rocket Mortgage
[01:27:12.520 --> 01:27:13.320]   from Quick and Loans.
[01:27:13.320 --> 01:27:16.120]   Folks, if you are in the market for a new home,
[01:27:16.120 --> 01:27:18.560]   unless you're one of the 1/10 of 1%
[01:27:18.560 --> 01:27:20.480]   who's going to pay for this in cash,
[01:27:20.480 --> 01:27:22.160]   you're probably going to get a home loan, right?
[01:27:22.160 --> 01:27:23.880]   Can I make a recommendation?
[01:27:23.880 --> 01:27:26.040]   Do it from the best lender in the country.
[01:27:26.040 --> 01:27:28.800]   Number one in customer satisfaction.
[01:27:28.800 --> 01:27:31.920]   For eight years running, it's quick and loans.
[01:27:31.920 --> 01:27:34.400]   And especially because they've made a loan product that
[01:27:34.400 --> 01:27:36.240]   is designed for us, the geeks.
[01:27:36.240 --> 01:27:38.400]   It's called Rocket Mortgage.
[01:27:38.400 --> 01:27:42.520]   But I didn't know and I learned it's kind of interesting
[01:27:42.520 --> 01:27:45.760]   is that originally, Quick and Loans was called,
[01:27:45.760 --> 01:27:47.600]   I think, Rock Financial.
[01:27:47.600 --> 01:27:51.000]   When Dan Gilbert founded it in the 1990s,
[01:27:51.000 --> 01:27:53.880]   he sold it to Quick and they renamed it Quick and Loans.
[01:27:53.880 --> 01:27:56.160]   And then three years later, he said, I'm buying this back.
[01:27:56.160 --> 01:27:57.560]   I want to run this company.
[01:27:57.560 --> 01:27:59.000]   But they kept the name.
[01:27:59.000 --> 01:28:00.800]   But remember, Rock Financial, I think
[01:28:00.800 --> 01:28:03.040]   Rocket Mortgage is Dan Gilbert's way of saying,
[01:28:03.040 --> 01:28:04.960]   we're back, baby.
[01:28:04.960 --> 01:28:05.560]   I love it.
[01:28:05.560 --> 01:28:06.120]   I love the name.
[01:28:06.120 --> 01:28:11.360]   It also is a pun because it lifts the burden of the home loan.
[01:28:11.360 --> 01:28:13.520]   It also brings you the home loan process
[01:28:13.520 --> 01:28:17.440]   into the 21st century because you can do it all on your phone
[01:28:17.440 --> 01:28:20.240]   or your tablet or your computer in a matter of minutes.
[01:28:20.240 --> 01:28:22.000]   You don't have to go to the bank.
[01:28:22.000 --> 01:28:24.960]   You don't have to sit there while the bank--
[01:28:24.960 --> 01:28:26.560]   I don't know what it is about loan officers.
[01:28:26.560 --> 01:28:28.920]   And if you're a loan officer, I'm sure you're a nice person.
[01:28:28.920 --> 01:28:32.240]   But they're chatty.
[01:28:32.240 --> 01:28:33.520]   Maybe they don't get talked to a lot.
[01:28:33.520 --> 01:28:34.240]   I don't know.
[01:28:34.240 --> 01:28:36.320]   It takes a long time.
[01:28:36.320 --> 01:28:39.440]   They show you their fancy calculator and they do the ammer.
[01:28:39.440 --> 01:28:41.000]   And they look at this is what your--
[01:28:41.000 --> 01:28:42.640]   is this is what your payment would be.
[01:28:42.640 --> 01:28:44.560]   And then you're not even done.
[01:28:44.560 --> 01:28:45.600]   And this happened to us.
[01:28:45.600 --> 01:28:47.160]   We didn't go to--
[01:28:47.160 --> 01:28:47.920]   I wish we had.
[01:28:47.920 --> 01:28:49.480]   They didn't have Rock and Mortgage four years ago
[01:28:49.480 --> 01:28:50.680]   when they sent our buyer a house.
[01:28:50.680 --> 01:28:52.840]   We went to the big bank.
[01:28:52.840 --> 01:28:54.840]   The one lender bigger than Quick and Loans,
[01:28:54.840 --> 01:28:57.280]   the one with the stage coach and the buggy whips.
[01:28:57.280 --> 01:29:02.600]   And so then you go home and they said, OK,
[01:29:02.600 --> 01:29:03.720]   we need some paperwork.
[01:29:03.720 --> 01:29:07.320]   OK, we got the pay stubs and the bank statements.
[01:29:07.320 --> 01:29:09.960]   Every three or four days they'd come back and say, OK,
[01:29:09.960 --> 01:29:11.720]   now we need this.
[01:29:11.720 --> 01:29:13.040]   Now we need that.
[01:29:13.040 --> 01:29:14.680]   A month later, we're going on vacation.
[01:29:14.680 --> 01:29:15.360]   We say, are you done?
[01:29:15.360 --> 01:29:16.600]   No.
[01:29:16.600 --> 01:29:17.400]   We're on vacation.
[01:29:17.400 --> 01:29:19.600]   Faxing stuff to the bank.
[01:29:19.600 --> 01:29:23.400]   We had to get Lisa's sister Debbie to go to the file cabinet.
[01:29:23.400 --> 01:29:25.120]   Fax more stuff.
[01:29:25.120 --> 01:29:27.200]   We almost lost the house.
[01:29:27.200 --> 01:29:30.760]   Fast forward, the 21st century, Rock and Mortgage.
[01:29:30.760 --> 01:29:33.480]   You can do the whole thing on your phone in minutes.
[01:29:33.480 --> 01:29:35.640]   You don't have to go to the attic or anything
[01:29:35.640 --> 01:29:38.200]   because you just go on the phone, answer the questions
[01:29:38.200 --> 01:29:40.880]   you already know, name, address, birth date.
[01:29:40.880 --> 01:29:42.880]   I think you have to do social, of course.
[01:29:42.880 --> 01:29:45.040]   And then they have trusted financial relationships,
[01:29:45.040 --> 01:29:46.680]   trusted relationships with all the financial institutions.
[01:29:46.680 --> 01:29:49.200]   So they pull all the information with your permission
[01:29:49.200 --> 01:29:51.240]   that they need, crunch the numbers
[01:29:51.240 --> 01:29:53.480]   based on income assets and credit.
[01:29:53.480 --> 01:29:55.520]   They will offer you the loans for which you qualify
[01:29:55.520 --> 01:29:57.080]   right there in minutes.
[01:29:57.080 --> 01:29:58.880]   And then you say, you choose a down payment.
[01:29:58.880 --> 01:29:59.680]   You choose a term.
[01:29:59.680 --> 01:30:00.200]   You choose the rate.
[01:30:00.200 --> 01:30:01.560]   You say, this is the loan for me.
[01:30:01.560 --> 01:30:05.720]   And then three minutes later, you're approved.
[01:30:05.720 --> 01:30:06.200]   That's it.
[01:30:06.200 --> 01:30:06.640]   It's done.
[01:30:06.640 --> 01:30:07.920]   You're done.
[01:30:07.920 --> 01:30:10.320]   It's entirely online, entirely on your phone,
[01:30:10.320 --> 01:30:10.800]   if you want.
[01:30:10.800 --> 01:30:12.000]   Apply simply.
[01:30:12.000 --> 01:30:14.080]   Understand fully and mortgage confidently
[01:30:14.080 --> 01:30:16.520]   with Rocket Mortgage from Quick and Loans.
[01:30:16.520 --> 01:30:17.720]   This is really a revolution.
[01:30:17.720 --> 01:30:18.520]   And I'm not surprised.
[01:30:18.520 --> 01:30:20.560]   Dan Gilbert's a smart guy.
[01:30:20.560 --> 01:30:23.720]   They're revitalizing downtown Detroit, really cool company.
[01:30:23.720 --> 01:30:24.800]   And they've got a great product.
[01:30:24.800 --> 01:30:28.440]   Rocket Mortgage.com/Twit2 is the URL.
[01:30:28.440 --> 01:30:30.560]   You could go there now, set up the account, get started.
[01:30:30.560 --> 01:30:32.360]   The next time you're in an open house,
[01:30:32.360 --> 01:30:33.480]   it'll take even less time.
[01:30:33.480 --> 01:30:36.120]   You could just say, hold it up to the realtor.
[01:30:36.120 --> 01:30:37.720]   We're approved.
[01:30:37.720 --> 01:30:39.840]   If the realtor is stuck in the 19th century,
[01:30:39.840 --> 01:30:41.680]   you press there's a big button on it that says,
[01:30:41.680 --> 01:30:43.400]   print a letter out for your realtor.
[01:30:43.400 --> 01:30:45.920]   And you can print a letter and say, OK, now it's
[01:30:45.920 --> 01:30:47.080]   in printed paper form.
[01:30:47.080 --> 01:30:48.200]   Is that better?
[01:30:48.200 --> 01:30:52.000]   Rocket Mortgage.com/Twit2==HousingLender_Liciss and all 50
[01:30:52.000 --> 01:30:55.920]   states and MLS consumer access.org number 30, 30.
[01:30:55.920 --> 01:30:58.800]   Rocket Mortgage from Quick and Loans.
[01:30:58.800 --> 01:31:01.000]   We thank them for their support.
[01:31:01.000 --> 01:31:03.520]   Hey, we had a great week this week on Twit.
[01:31:03.520 --> 01:31:07.080]   We made a little video, summarizing the highlights watch.
[01:31:07.080 --> 01:31:08.880]   Previously on Twit.
[01:31:08.880 --> 01:31:10.720]   I can tell you my people magazine headline
[01:31:10.720 --> 01:31:11.360]   that I like the most.
[01:31:11.360 --> 01:31:12.000]   Yes.
[01:31:12.000 --> 01:31:13.440]   Shoo goo goo roo.
[01:31:13.440 --> 01:31:16.560]   Lyman Van Leep cures tethered, tennies toes
[01:31:16.560 --> 01:31:18.040]   with sheer stick-tuitiveness.
[01:31:18.040 --> 01:31:20.800]   Oh my god, I love that.
[01:31:20.800 --> 01:31:24.040]   When I am practicing my microphone technique,
[01:31:24.040 --> 01:31:26.440]   I shall proclaim Shoo goo goo roo Lyman Van
[01:31:26.440 --> 01:31:28.160]   Leep, Clures, Cures tet.
[01:31:28.160 --> 01:31:29.640]   You see, you can't even say it.
[01:31:29.640 --> 01:31:34.080]   This week in computer hardware, NVIDIA's big format gaming
[01:31:34.080 --> 01:31:37.240]   display initiative where they were showing off HDR enabled,
[01:31:37.240 --> 01:31:39.920]   65 inch 4K, 120 Hertz panels.
[01:31:39.920 --> 01:31:42.640]   It basically looks like a TV on a desk,
[01:31:42.640 --> 01:31:43.920]   and you're sitting at the desk.
[01:31:43.920 --> 01:31:44.760]   Sounds great.
[01:31:44.760 --> 01:31:45.280]   Yeah.
[01:31:45.280 --> 01:31:47.560]   So realize you're going--
[01:31:47.560 --> 01:31:50.800]   You're like watching a tennis match, but tonight we're shooting at you.
[01:31:50.800 --> 01:31:52.360]   The new screen sabers.
[01:31:52.360 --> 01:31:55.120]   Edward Snowden has a new app for Android,
[01:31:55.120 --> 01:32:00.560]   turning your old Android device into a portable security
[01:32:00.560 --> 01:32:01.120]   system.
[01:32:01.120 --> 01:32:02.800]   That's Haven.
[01:32:02.800 --> 01:32:05.200]   How can we protect journalists, really,
[01:32:05.200 --> 01:32:06.800]   from physical intrusions?
[01:32:06.800 --> 01:32:08.240]   In the world, there's so many people
[01:32:08.240 --> 01:32:12.560]   who are affected by theft, by physical assault,
[01:32:12.560 --> 01:32:16.240]   by having people come into their spaces without them knowing it.
[01:32:16.240 --> 01:32:20.480]   You see, I found some strange person looking at my stuff.
[01:32:20.480 --> 01:32:22.000]   Oh, he's caught this little--
[01:32:22.000 --> 01:32:22.520]   I know him.
[01:32:22.520 --> 01:32:23.560]   --this little guy.
[01:32:23.560 --> 01:32:24.280]   --twit.
[01:32:24.280 --> 01:32:25.920]   Say hello to the NSA.
[01:32:25.920 --> 01:32:28.200]   They're listening.
[01:32:28.200 --> 01:32:31.160]   But we welcome them, because, hey, every listener counts.
[01:32:31.160 --> 01:32:36.800]   Patrick Norton, somebody said, is that Patrick Norton behind that big beard?
[01:32:36.800 --> 01:32:37.280]   Yes, it is.
[01:32:37.280 --> 01:32:39.240]   He said he's starting to look like a mountain man.
[01:32:39.240 --> 01:32:41.320]   He'll be on the new screen savers next Saturday.
[01:32:41.320 --> 01:32:45.680]   You can see how the beard has progressed.
[01:32:45.680 --> 01:32:47.280]   So it looks like Apple--
[01:32:47.280 --> 01:32:48.840]   I'm not sure I really understand this,
[01:32:48.840 --> 01:32:51.840]   but it looks like Apple is going to, with a new tax bill,
[01:32:51.840 --> 01:32:53.960]   be required to repatriate.
[01:32:53.960 --> 01:32:55.320]   It's $200-- what is it?
[01:32:55.320 --> 01:33:00.280]   $30 billion international piggy bank.
[01:33:00.280 --> 01:33:04.040]   It'll cost it $38 billion in taxes, 15%.
[01:33:04.040 --> 01:33:08.240]   But Apple's already saying all the fun things it's going to do,
[01:33:08.240 --> 01:33:12.640]   contributing $350 billion to the US economy of the next five years.
[01:33:12.640 --> 01:33:19.600]   They gave bonuses to everybody, $2,500 bonuses to all $125,000 or will.
[01:33:19.600 --> 01:33:21.520]   It was all director level and below.
[01:33:21.520 --> 01:33:23.920]   Yeah, I don't think a director should get a bonus.
[01:33:23.920 --> 01:33:24.200]   Right.
[01:33:24.200 --> 01:33:26.200]   Well, what was interesting was that these are RSUs.
[01:33:26.200 --> 01:33:28.000]   So they still, in a way, connected it--
[01:33:28.000 --> 01:33:29.040]   Stretastic stock units.
[01:33:29.040 --> 01:33:29.520]   Right.
[01:33:29.520 --> 01:33:31.760]   Which is essentially, if you don't know,
[01:33:31.760 --> 01:33:33.120]   they're just giving you the stock.
[01:33:33.120 --> 01:33:34.440]   It's not the option to buy the stock.
[01:33:34.440 --> 01:33:35.720]   It's just, here's some--
[01:33:35.720 --> 01:33:36.800]   Well, that's a good thing.
[01:33:36.800 --> 01:33:39.840]   Yeah, so it's still attached to performance, to a certain degree.
[01:33:39.840 --> 01:33:41.880]   As it should be, then, employers are incentive.
[01:33:41.880 --> 01:33:44.840]   It's 300-- I did the math-- $300 million
[01:33:44.840 --> 01:33:46.400]   in restricted stock units.
[01:33:46.400 --> 01:33:48.120]   That's a lot of stock units.
[01:33:48.120 --> 01:33:50.800]   They also say they're going to build a second campus.
[01:33:50.800 --> 01:33:51.760]   That's what they say.
[01:33:51.760 --> 01:33:53.960]   They're working on something in Reno, right now?
[01:33:53.960 --> 01:33:54.520]   Really?
[01:33:54.520 --> 01:33:55.960]   Yeah, I think so.
[01:33:55.960 --> 01:33:57.520]   There's some--
[01:33:57.520 --> 01:33:58.720]   Now, forgive my ignorance.
[01:33:58.720 --> 01:34:00.440]   I think it might be call center related.
[01:34:00.440 --> 01:34:00.920]   Oh, yeah, yeah.
[01:34:00.920 --> 01:34:02.400]   It sure makes sense.
[01:34:02.400 --> 01:34:02.840]   As well.
[01:34:02.840 --> 01:34:05.440]   So it's not other spaceship campus right next door or something
[01:34:05.440 --> 01:34:05.800]   like that.
[01:34:05.800 --> 01:34:06.520]   No, no.
[01:34:06.520 --> 01:34:10.200]   Really good article by Horace Dediu,
[01:34:10.200 --> 01:34:16.120]   and as always various, Dediu, on Asymco about the Apple cash
[01:34:16.120 --> 01:34:17.720]   and how it all works.
[01:34:17.720 --> 01:34:20.000]   And what I didn't really grok--
[01:34:20.000 --> 01:34:25.160]   by the way, Apple has $268 billion, $895 million in cash
[01:34:25.160 --> 01:34:27.320]   as of the end of September.
[01:34:27.320 --> 01:34:28.880]   But really, that money belongs to--
[01:34:28.880 --> 01:34:31.040]   I didn't realize this to the shareholders.
[01:34:31.040 --> 01:34:34.200]   It's not considered Apple's to do whatever it wants with,
[01:34:34.200 --> 01:34:36.560]   unless it invests it in something that will make more money
[01:34:36.560 --> 01:34:37.240]   down the road.
[01:34:37.240 --> 01:34:40.400]   Which is interesting, and explains why Apple seems
[01:34:40.400 --> 01:34:42.000]   to sometimes be struggling to figure out
[01:34:42.000 --> 01:34:43.280]   what to do with its money.
[01:34:43.280 --> 01:34:46.240]   It's not typical for-- it's very unusual.
[01:34:46.240 --> 01:34:50.400]   Most businesses keep very little cash on their books.
[01:34:50.400 --> 01:34:54.280]   Apple's cash amounts to about 30% of its market capitalization.
[01:34:54.280 --> 01:34:56.960]   Part of that is, weirdly enough, they've
[01:34:56.960 --> 01:34:59.840]   taken $100 billion in loans.
[01:34:59.840 --> 01:35:02.120]   And that's because they couldn't get the money in the US.
[01:35:02.120 --> 01:35:04.800]   So they borrowed against the international money
[01:35:04.800 --> 01:35:10.120]   to buy back stock, to pay dividends, to pay back shareholders.
[01:35:10.120 --> 01:35:13.720]   So presumably, I guess they'll pay those loans off.
[01:35:13.720 --> 01:35:14.920]   Interesting.
[01:35:14.920 --> 01:35:17.880]   And then buy some stuff.
[01:35:17.880 --> 01:35:19.320]   They'll be able to use their own money.
[01:35:19.320 --> 01:35:20.680]   They can with their own money.
[01:35:20.680 --> 01:35:24.200]   And then you'll be glad you bought Apple stock,
[01:35:24.200 --> 01:35:25.800]   because I guess they'll have to give the rest of it
[01:35:25.800 --> 01:35:27.880]   to the shareholders.
[01:35:27.880 --> 01:35:28.520]   So--
[01:35:28.520 --> 01:35:31.160]   We'll see or use it to invest in other products,
[01:35:31.160 --> 01:35:33.440]   although I think it could go either way.
[01:35:33.440 --> 01:35:37.760]   I mean, if we were being a little doomsday about the HomePod,
[01:35:37.760 --> 01:35:40.040]   but it seems like Apple's actually--
[01:35:40.040 --> 01:35:42.040]   I mean, Apple is essentially a hardware company,
[01:35:42.040 --> 01:35:44.160]   not a software company, unlike something like Google,
[01:35:44.160 --> 01:35:46.280]   which is essentially a software company, not a hardware
[01:35:46.280 --> 01:35:48.720]   company or a data company.
[01:35:48.720 --> 01:35:55.240]   And Apple, as these form factors get more and more sort of flat,
[01:35:55.240 --> 01:35:57.560]   they seem to be running out of things to make.
[01:35:57.560 --> 01:36:02.920]   I'm waiting for the car.
[01:36:02.920 --> 01:36:04.400]   We have to spend it.
[01:36:04.400 --> 01:36:05.720]   They've got to spend it on something.
[01:36:05.720 --> 01:36:06.840]   It would be interesting.
[01:36:06.840 --> 01:36:08.400]   They are not big on acquisitions.
[01:36:08.400 --> 01:36:12.200]   And actually, I think this is actually prudent fiscal
[01:36:12.200 --> 01:36:13.960]   management acquisitions.
[01:36:13.960 --> 01:36:19.520]   We've seen-- look at AOL Time Warner and Compaq--
[01:36:19.520 --> 01:36:20.520]   who bought Compaq?
[01:36:20.520 --> 01:36:21.120]   IBM?
[01:36:21.120 --> 01:36:22.120]   No.
[01:36:22.120 --> 01:36:22.520]   HP.
[01:36:22.520 --> 01:36:22.920]   That's right.
[01:36:22.920 --> 01:36:24.200]   Compaq HP.
[01:36:24.200 --> 01:36:27.120]   These big acquisitions rarely pay out,
[01:36:27.120 --> 01:36:29.480]   because of the culture clash.
[01:36:29.480 --> 01:36:33.480]   And Apple's got such a strong corporate culture
[01:36:33.480 --> 01:36:34.760]   that they don't want to dilute.
[01:36:34.760 --> 01:36:35.800]   They don't want to mess up.
[01:36:35.800 --> 01:36:37.040]   It's hard to management.
[01:36:37.040 --> 01:36:39.000]   Managers, people talked about them buying Disney.
[01:36:39.000 --> 01:36:42.920]   But Disney has 140,000 employees.
[01:36:42.920 --> 01:36:45.800]   How do you mesh those companies without completely changing
[01:36:45.800 --> 01:36:46.680]   Apple?
[01:36:46.680 --> 01:36:48.520]   That wouldn't be a good idea.
[01:36:48.520 --> 01:36:51.880]   So Horace, this is such a good arc.
[01:36:51.880 --> 01:36:53.200]   I feel like I learned a lot.
[01:36:53.200 --> 01:36:56.280]   He says, Apple doesn't buy business models, which
[01:36:56.280 --> 01:36:58.640]   is what often you buy, essentially, when you buy a company.
[01:36:58.640 --> 01:37:01.360]   You buy their business model, or you buy their customers,
[01:37:01.360 --> 01:37:03.000]   or you buy their cash flow.
[01:37:03.000 --> 01:37:05.200]   That's not-- Apple doesn't need any of that.
[01:37:05.200 --> 01:37:06.840]   They've got a great business model.
[01:37:06.840 --> 01:37:09.400]   They have an infinite number of customers.
[01:37:09.400 --> 01:37:12.360]   And they certainly don't need cash flow, obviously.
[01:37:12.360 --> 01:37:15.800]   So acquisition isn't probably the right thing either.
[01:37:15.800 --> 01:37:16.240]   Well, thank you.
[01:37:16.240 --> 01:37:17.560]   What they're going to end up doing
[01:37:17.560 --> 01:37:18.920]   is buying back shares.
[01:37:18.920 --> 01:37:20.240]   Ultimately, that helps shareholders,
[01:37:20.240 --> 01:37:23.880]   because it means that one shares worth more,
[01:37:23.880 --> 01:37:25.640]   and paid dividends to the degree they can.
[01:37:25.640 --> 01:37:27.240]   Although, he says something interesting.
[01:37:27.240 --> 01:37:30.000]   The tax code in this country discourages companies
[01:37:30.000 --> 01:37:30.800]   from paying dividends.
[01:37:30.800 --> 01:37:33.360]   It's not a good tax move.
[01:37:33.360 --> 01:37:35.640]   I think that Apple, if they make any acquisitions,
[01:37:35.640 --> 01:37:38.240]   it's going to be in the content world.
[01:37:38.240 --> 01:37:40.720]   Boy, they're hitting it out of the park with content,
[01:37:40.720 --> 01:37:43.440]   at least on paper, aren't they?
[01:37:43.440 --> 01:37:45.680]   Well, and I think that they've maybe not given up,
[01:37:45.680 --> 01:37:50.280]   but with the exception of Apple Music,
[01:37:50.280 --> 01:37:53.960]   they are just, in my opinion, not as good at services
[01:37:53.960 --> 01:37:55.360]   as many of their competitors.
[01:37:55.360 --> 01:37:59.600]   And so they may turn to the things that they do know.
[01:37:59.600 --> 01:38:01.000]   They know media.
[01:38:01.000 --> 01:38:03.120]   They know how to make things attractive.
[01:38:03.120 --> 01:38:07.600]   They know how to create a force field of appeal.
[01:38:07.600 --> 01:38:09.000]   And you look at what they've done with Beats.
[01:38:09.000 --> 01:38:11.000]   I think that Beats was way less about the hardware
[01:38:11.000 --> 01:38:13.560]   than it was about the music relationships.
[01:38:13.560 --> 01:38:16.480]   So I think they'll continue to progress in that.
[01:38:16.480 --> 01:38:18.120]   Although the hardware has worked out OK.
[01:38:18.120 --> 01:38:18.880]   It's worked out OK.
[01:38:18.880 --> 01:38:22.040]   Turns out that's a high margin market, isn't it?
[01:38:22.040 --> 01:38:22.640]   Headphones.
[01:38:22.640 --> 01:38:25.320]   It is, and yet, I think that they still
[01:38:25.320 --> 01:38:27.960]   work very separately with the same company.
[01:38:27.960 --> 01:38:28.480]   Yeah.
[01:38:28.480 --> 01:38:29.720]   And Beats really still seems like it's not--
[01:38:29.720 --> 01:38:31.120]   They're very, very different.
[01:38:31.120 --> 01:38:31.760]   Yeah.
[01:38:31.760 --> 01:38:32.960]   Yeah, that's smart.
[01:38:32.960 --> 01:38:37.360]   So last year, they were going to spend $1 billion on content.
[01:38:37.360 --> 01:38:40.800]   They hired two executives from Sony, right?
[01:38:40.800 --> 01:38:44.200]   And now what's interesting is they've made some really
[01:38:44.200 --> 01:38:47.640]   interesting acquisitions.
[01:38:47.640 --> 01:38:49.680]   We talked about this on Tuesday.
[01:38:49.680 --> 01:38:51.560]   Carson, do you have the--
[01:38:51.560 --> 01:38:59.480]   They hired-- or they're going to do a show with Drew
[01:38:59.480 --> 01:39:00.400]   Barramore, right?
[01:39:00.400 --> 01:39:02.600]   Is that right?
[01:39:02.600 --> 01:39:03.600]   Oh, I can't remember it.
[01:39:03.600 --> 01:39:03.840]   You know what?
[01:39:03.840 --> 01:39:06.640]   I'll go back to the Mac break weekly,
[01:39:06.640 --> 01:39:09.280]   because we had it all on the rundown for Mac break weekly.
[01:39:09.280 --> 01:39:12.040]   But they have three or four really interesting shows.
[01:39:12.040 --> 01:39:14.640]   There's a science fiction show they're going to do.
[01:39:14.640 --> 01:39:18.800]   These all seem like premier kind of HBO quality, even better
[01:39:18.800 --> 01:39:23.000]   than Netflix, really, or Amazon quality stuff.
[01:39:23.000 --> 01:39:24.800]   I'm very impressed with what they're doing.
[01:39:24.800 --> 01:39:26.800]   My question is, though, what are they going to do with it?
[01:39:26.800 --> 01:39:29.120]   It's not-- doesn't fit into Apple Music.
[01:39:29.120 --> 01:39:32.720]   Are they going to start a new service?
[01:39:32.720 --> 01:39:33.960]   Apple TV?
[01:39:33.960 --> 01:39:36.200]   Are they going to merge Apple Music into something
[01:39:36.200 --> 01:39:38.640]   like Apple Media?
[01:39:38.640 --> 01:39:41.120]   What do they do with all this stuff?
[01:39:41.120 --> 01:39:42.880]   That's what I've been wondering as well.
[01:39:42.880 --> 01:39:45.080]   If they're just going to sell it for a premium on iTunes,
[01:39:45.080 --> 01:39:48.200]   which seems unlikely, it's pretty difficult to create
[01:39:48.200 --> 01:39:50.200]   an audience for premium content by charging for it
[01:39:50.200 --> 01:39:52.640]   in front and not bundling it into a service like Netflix.
[01:39:52.640 --> 01:39:54.320]   So it almost seems like they'd have
[01:39:54.320 --> 01:39:57.440]   to have some kind of a premium channel of content offerings.
[01:39:57.440 --> 01:39:59.920]   Or maybe it's a free if you buy an Apple TV.
[01:39:59.920 --> 01:40:01.160]   They are a hardware company.
[01:40:01.160 --> 01:40:03.680]   I don't think you'll get enough money.
[01:40:03.680 --> 01:40:06.200]   I don't think you get anything for free at all.
[01:40:06.200 --> 01:40:10.560]   So they have a reboot of Steven Spielberg's amazing stories.
[01:40:10.560 --> 01:40:14.360]   They've ordered a space drama from Battlestar Galactica creator
[01:40:14.360 --> 01:40:16.520]   Ronald Moore, Reese Witherspoon.
[01:40:16.520 --> 01:40:17.440]   That's who I was trying to remember.
[01:40:17.440 --> 01:40:18.720]   It's her production company.
[01:40:18.720 --> 01:40:19.600]   It's her production company.
[01:40:19.600 --> 01:40:23.880]   But she and Jennifer Anston are going to be in it as well.
[01:40:23.880 --> 01:40:26.520]   They had such a flop with carpool karaoke.
[01:40:26.520 --> 01:40:29.640]   And that horrible planet of the apps.
[01:40:29.640 --> 01:40:31.160]   Yeah, that was a little stretch.
[01:40:31.160 --> 01:40:32.640]   That was really bad.
[01:40:32.640 --> 01:40:34.560]   But notice they're not promoting it at all.
[01:40:34.560 --> 01:40:36.800]   I think they were-- I think they were regrouping.
[01:40:36.800 --> 01:40:38.800]   I think they took a step backward and said, whoops.
[01:40:38.800 --> 01:40:41.320]   They seem to have done fairly well with their music coverage,
[01:40:41.320 --> 01:40:41.520]   right?
[01:40:41.520 --> 01:40:43.360]   The stuff that Dr. Dre has been involved in.
[01:40:43.360 --> 01:40:44.040]   Absolutely.
[01:40:44.040 --> 01:40:45.760]   That has all been really interesting.
[01:40:45.760 --> 01:40:49.720]   And so when they go to the people who really-- they have already,
[01:40:49.720 --> 01:40:53.520]   who are deep in an industry, it seems to go pretty well.
[01:40:53.520 --> 01:40:57.720]   I mean, look at it.
[01:40:57.720 --> 01:40:59.680]   It's completely-- they've got the money.
[01:40:59.680 --> 01:41:02.720]   A billion dollars is nothing.
[01:41:02.720 --> 01:41:03.680]   That's what Netflix is spending.
[01:41:03.680 --> 01:41:04.160]   $8 billion.
[01:41:04.160 --> 01:41:06.600]   That would be nothing.
[01:41:06.600 --> 01:41:09.200]   And they clearly now have the people
[01:41:09.200 --> 01:41:13.560]   they need to make the deals, the really impressive deals.
[01:41:13.560 --> 01:41:17.200]   I don't understand exactly where they stick it.
[01:41:17.200 --> 01:41:18.040]   I don't know.
[01:41:18.040 --> 01:41:19.640]   I think-- it'll be really interesting to see
[01:41:19.640 --> 01:41:22.280]   what they do with iTunes as a concept of "Rami."
[01:41:22.280 --> 01:41:24.520]   Tim, you brought that up.
[01:41:24.520 --> 01:41:26.040]   Will they sell content through iTunes?
[01:41:26.040 --> 01:41:27.960]   Will they just expand Apple music?
[01:41:27.960 --> 01:41:30.000]   Will that just be-- I mean, when we start
[01:41:30.000 --> 01:41:34.640]   to see a little bit more overlap between iOS and Mac OS,
[01:41:34.640 --> 01:41:37.280]   will that start to just kind of bubble up?
[01:41:37.280 --> 01:41:41.680]   I frankly cannot believe that iTunes still exists.
[01:41:41.680 --> 01:41:42.200]   It's so far.
[01:41:42.200 --> 01:41:42.960]   The way that it does right now.
[01:41:42.960 --> 01:41:44.760]   And I have a kind of a funny story,
[01:41:44.760 --> 01:41:46.600]   and I can't name names because frankly, I don't even
[01:41:46.600 --> 01:41:47.640]   remember them.
[01:41:47.640 --> 01:41:50.320]   But many, many, many, many, many, many years ago,
[01:41:50.320 --> 01:41:53.080]   I was on a call with a product manager from Apple
[01:41:53.080 --> 01:41:56.200]   who was-- we were talking about something completely different.
[01:41:56.200 --> 01:41:58.200]   And I just said, well, when iTunes goes away,
[01:41:58.200 --> 01:41:59.760]   because it's kind of a house.
[01:41:59.760 --> 01:42:01.400]   And this person was like, I do not
[01:42:01.400 --> 01:42:02.480]   know what you were talking about.
[01:42:02.480 --> 01:42:04.440]   It is a wonderful piece of software.
[01:42:04.440 --> 01:42:05.840]   In robotic voice, just like that?
[01:42:05.840 --> 01:42:07.840]   Kind of.
[01:42:07.840 --> 01:42:08.680]   I mean, not exactly.
[01:42:08.680 --> 01:42:11.520]   They were being serious, but I was like, are you really?
[01:42:11.520 --> 01:42:13.320]   I mean, this isn't on the record.
[01:42:13.320 --> 01:42:13.880]   Yeah.
[01:42:13.880 --> 01:42:14.720]   You can tell me the truth.
[01:42:14.720 --> 01:42:15.480]   You can tell me the truth.
[01:42:15.480 --> 01:42:16.320]   You know that it's not--
[01:42:16.320 --> 01:42:17.320]   But that was how long ago?
[01:42:17.320 --> 01:42:17.840]   He's right.
[01:42:17.840 --> 01:42:18.760]   Oh, probably five years ago?
[01:42:18.760 --> 01:42:20.080]   Yeah, so they didn't go away.
[01:42:20.080 --> 01:42:20.920]   It did not go away.
[01:42:20.920 --> 01:42:21.960]   It should be going away.
[01:42:21.960 --> 01:42:22.800]   It's really interesting.
[01:42:22.800 --> 01:42:24.280]   Why does an Apple do something better?
[01:42:24.280 --> 01:42:26.240]   There's so many-- they can afford great programmers.
[01:42:26.240 --> 01:42:27.560]   They can make something so much better.
[01:42:27.560 --> 01:42:32.080]   Maybe that's what this will go to some brand new--
[01:42:32.080 --> 01:42:37.920]   they've got-- the Apple TV is still a hobby, right?
[01:42:37.920 --> 01:42:41.960]   It's not-- it can't be a big product for them at this point.
[01:42:41.960 --> 01:42:43.240]   I think it's a platform.
[01:42:43.240 --> 01:42:44.120]   And I think it's kind of--
[01:42:44.120 --> 01:42:46.160]   But it's a platform, then you do develop for it.
[01:42:46.160 --> 01:42:46.800]   Sure.
[01:42:46.800 --> 01:42:48.920]   And I think that's happening concurrent with Apple Music.
[01:42:48.920 --> 01:42:51.000]   And then hopefully learning comes in the home kit.
[01:42:51.000 --> 01:42:54.280]   And then it all comes together.
[01:42:54.280 --> 01:42:55.000]   Hopefully.
[01:42:55.000 --> 01:42:57.240]   So this is the story I forgot and I wanted to talk about,
[01:42:57.240 --> 01:42:58.400]   because we were talking about privacy.
[01:42:58.400 --> 01:43:00.160]   I got distracted.
[01:43:00.160 --> 01:43:01.720]   There was a big football game last week.
[01:43:01.720 --> 01:43:02.600]   I don't know if you know.
[01:43:02.600 --> 01:43:03.680]   The Viking Saints game.
[01:43:03.680 --> 01:43:04.640]   And it was insane.
[01:43:04.640 --> 01:43:06.880]   The last quarter, the lead shifted how many times?
[01:43:06.880 --> 01:43:09.280]   I didn't see it because I was working, but four or five
[01:43:09.280 --> 01:43:12.000]   times in the last quarter, last few minutes even.
[01:43:12.000 --> 01:43:15.440]   And in the very last play of the game,
[01:43:15.440 --> 01:43:18.160]   the Vikings who are down by one point
[01:43:18.160 --> 01:43:22.760]   throw a long bomb, a mistackled guy
[01:43:22.760 --> 01:43:25.800]   runs in for a touchdown game over.
[01:43:25.800 --> 01:43:31.080]   Apple watches, told fans, you might be having heart issues
[01:43:31.080 --> 01:43:32.240]   in great numbers.
[01:43:32.240 --> 01:43:34.680]   You know, they put that feature into the new Apple
[01:43:34.680 --> 01:43:36.240]   watches, where if your heart rate goes up
[01:43:36.240 --> 01:43:40.040]   and you're not moving, they think maybe you're
[01:43:40.040 --> 01:43:42.160]   having a heart attack.
[01:43:42.160 --> 01:43:46.760]   And apparently many people in the stadium tweeted
[01:43:46.760 --> 01:43:49.560]   or Instagrammed this image, Apple Watch
[01:43:49.560 --> 01:43:51.160]   thinks I'm having a heart attack.
[01:43:51.160 --> 01:43:52.920]   Oh my gosh.
[01:43:52.920 --> 01:43:56.120]   During the game.
[01:43:56.120 --> 01:43:57.600]   And they kind of were.
[01:43:57.600 --> 01:44:00.360]   And they maybe they were.
[01:44:00.360 --> 01:44:01.280]   Maybe they were.
[01:44:01.280 --> 01:44:03.560]   I think Patriots fans might have had the same experience
[01:44:03.560 --> 01:44:04.280]   earlier today.
[01:44:04.280 --> 01:44:05.280]   I don't know.
[01:44:05.280 --> 01:44:07.080]   How funny is that?
[01:44:07.080 --> 01:44:09.680]   Now, is that Georgia invasion of privacy?
[01:44:09.680 --> 01:44:10.960]   Or is that great news?
[01:44:10.960 --> 01:44:12.520]   That's helping them be healthy.
[01:44:12.520 --> 01:44:12.960]   That's helpful.
[01:44:12.960 --> 01:44:14.920]   I think in that way, it's helpful for people.
[01:44:14.920 --> 01:44:17.160]   I don't think that they should be posting it, you know,
[01:44:17.160 --> 01:44:18.680]   for themselves on the internet.
[01:44:18.680 --> 01:44:20.480]   Well, they get to do that.
[01:44:20.480 --> 01:44:23.360]   But to send it back to you to say, listen, check this out.
[01:44:23.360 --> 01:44:25.480]   There might be something wrong.
[01:44:25.480 --> 01:44:27.160]   Or someone's having a panic attack.
[01:44:27.160 --> 01:44:28.520]   You might be having a panic attack.
[01:44:28.520 --> 01:44:30.000]   And if they could tell the difference between that
[01:44:30.000 --> 01:44:31.800]   would be really wonderful.
[01:44:31.800 --> 01:44:34.840]   Because for some people, you know, you're not actually
[01:44:34.840 --> 01:44:36.440]   aware, you're like, oh, I'm actually OK.
[01:44:36.440 --> 01:44:39.120]   And you're not, which is helpful to be able to have something
[01:44:39.120 --> 01:44:41.080]   that most people are wearing their watch to monitor
[01:44:41.080 --> 01:44:42.960]   how they're doing and for safety.
[01:44:42.960 --> 01:44:45.000]   So like, it's not great for telling time.
[01:44:45.000 --> 01:44:46.960]   It's great for all kinds of other things.
[01:44:46.960 --> 01:44:50.040]   But as a time device, it, you know, it falls a little bit short.
[01:44:50.040 --> 01:44:51.080]   I've gotten over it.
[01:44:51.080 --> 01:44:52.280]   I've gotten over it.
[01:44:52.280 --> 01:44:53.440]   I have to apologize.
[01:44:53.440 --> 01:44:55.560]   Because for the first two years of the Apple Watch,
[01:44:55.560 --> 01:44:56.760]   I said this thing is a flop.
[01:44:56.760 --> 01:44:57.640]   I didn't mean commercially.
[01:44:57.640 --> 01:45:00.920]   I meant, technically, this is a useless gadget.
[01:45:00.920 --> 01:45:02.160]   It's a side car.
[01:45:02.160 --> 01:45:03.600]   It's not.
[01:45:03.600 --> 01:45:05.520]   But I wear it every day now.
[01:45:05.520 --> 01:45:07.000]   And it's the series.
[01:45:07.000 --> 01:45:08.280]   It's because of the series three.
[01:45:08.280 --> 01:45:11.560]   I think they finally got good enough.
[01:45:11.560 --> 01:45:13.080]   It got fast enough.
[01:45:13.080 --> 01:45:14.160]   It got smart enough.
[01:45:14.160 --> 01:45:16.400]   And gosh darn it, people like it.
[01:45:16.400 --> 01:45:18.480]   I just don't want to charge it.
[01:45:18.480 --> 01:45:19.760]   That's my only problem.
[01:45:19.760 --> 01:45:23.160]   You know, I got to the point now where going to bed
[01:45:23.160 --> 01:45:28.000]   involves attaching a large number of things to the power.
[01:45:28.000 --> 01:45:29.440]   I don't want to go into the details,
[01:45:29.440 --> 01:45:31.800]   but a number of things into the power.
[01:45:31.800 --> 01:45:33.880]   And that's just part of my night ritual.
[01:45:33.880 --> 01:45:35.200]   I wash my face, I brush my teeth,
[01:45:35.200 --> 01:45:36.760]   and I plug everything in.
[01:45:36.760 --> 01:45:38.880]   And in the morning, it's good again.
[01:45:38.880 --> 01:45:39.720]   You don't do it.
[01:45:39.720 --> 01:45:40.880]   I don't mind, Lindsay, because it's not--
[01:45:40.880 --> 01:45:42.080]   I don't have to actually plug it in.
[01:45:42.080 --> 01:45:44.400]   I'm much more annoyed with my phone,
[01:45:44.400 --> 01:45:46.560]   because I have to find where to plug it in.
[01:45:46.560 --> 01:45:48.880]   I think that I'm going to place it onto the counter,
[01:45:48.880 --> 01:45:50.720]   get one of the ones that do that.
[01:45:50.720 --> 01:45:55.760]   But because I could just kind of get sucked into the magnet
[01:45:55.760 --> 01:45:56.480]   charging on like--
[01:45:56.480 --> 01:45:57.040]   It does.
[01:45:57.040 --> 01:45:59.080]   It almost makes me feel a little bit happy.
[01:45:59.080 --> 01:46:00.040]   Yeah, that's the noise.
[01:46:00.040 --> 01:46:01.280]   It is very satisfying.
[01:46:01.280 --> 01:46:03.440]   I like thinking about the human factors meetings,
[01:46:03.440 --> 01:46:05.200]   where people were like, does it click?
[01:46:05.200 --> 01:46:07.560]   Is it satisfying when it pops onto the charger?
[01:46:07.560 --> 01:46:10.120]   My problem was that I travel a lot
[01:46:10.120 --> 01:46:12.280]   and taking that charger with me and setting it up
[01:46:12.280 --> 01:46:12.920]   every single time.
[01:46:12.920 --> 01:46:15.320]   And they're expensive, so it's not universal.
[01:46:15.320 --> 01:46:16.800]   It's not like you can just kind of pick one up
[01:46:16.800 --> 01:46:17.760]   at the Walgreens.
[01:46:17.760 --> 01:46:18.600]   Yeah, that's a good point.
[01:46:18.600 --> 01:46:19.480]   No, that's true.
[01:46:19.480 --> 01:46:19.840]   That's true.
[01:46:19.840 --> 01:46:21.240]   We need the charging mats, and we can just
[01:46:21.240 --> 01:46:22.600]   place them all onto the charging mats.
[01:46:22.600 --> 01:46:24.840]   I do wireless charging with my iPhone now.
[01:46:24.840 --> 01:46:27.000]   And that really-- not plugging it in your right
[01:46:27.000 --> 01:46:29.320]   is a big improvement.
[01:46:29.320 --> 01:46:32.840]   The watch, I have one of those little rubber Elgato--
[01:46:32.840 --> 01:46:33.600]   is it Elgato?
[01:46:33.600 --> 01:46:34.640]   I can't remember--
[01:46:34.640 --> 01:46:35.480]   Max.
[01:46:35.480 --> 01:46:36.760]   It looks like the old classic Mac.
[01:46:36.760 --> 01:46:37.800]   I put it in there.
[01:46:37.800 --> 01:46:38.520]   And it's just so cute.
[01:46:38.520 --> 01:46:40.000]   It makes me happy.
[01:46:40.000 --> 01:46:41.040]   Oh, that's it.
[01:46:41.040 --> 01:46:42.040]   Yeah.
[01:46:42.040 --> 01:46:42.880]   Yeah.
[01:46:42.880 --> 01:46:43.880]   But I just used the charging--
[01:46:43.880 --> 01:46:45.000]   It rewards you with its cuteness.
[01:46:45.000 --> 01:46:46.960]   Yes.
[01:46:46.960 --> 01:46:49.160]   Somebody pointed out, look, you shouldn't call it
[01:46:49.160 --> 01:46:50.600]   a smart watch or watch anymore.
[01:46:50.600 --> 01:46:51.720]   It's not really a watch.
[01:46:51.720 --> 01:46:52.280]   It's not.
[01:46:52.280 --> 01:46:54.040]   Smartphone hasn't been a phone in ages.
[01:46:54.040 --> 01:46:55.320]   How many phone calls do you make?
[01:46:55.320 --> 01:46:57.040]   Although the funny thing is that when I first--
[01:46:57.040 --> 01:46:59.000]   when I first tested Apple Watch, now,
[01:46:59.000 --> 01:47:01.680]   years ago, I haven't worn a watch for a very long time
[01:47:01.680 --> 01:47:02.960]   because of my smartphone.
[01:47:02.960 --> 01:47:05.840]   And I remember being so charmed by the fact
[01:47:05.840 --> 01:47:08.960]   that I could look at my wrist and see what time it was.
[01:47:08.960 --> 01:47:09.960]   Look, it tells me--
[01:47:09.960 --> 01:47:10.960]   Look, it tells me--
[01:47:10.960 --> 01:47:11.960]   Oh, my god.
[01:47:11.960 --> 01:47:13.280]   People are like, what's the best thing about this?
[01:47:13.280 --> 01:47:14.600]   I was like, um--
[01:47:14.600 --> 01:47:16.120]   Now it tells you if you're having a heart attack.
[01:47:16.120 --> 01:47:17.880]   All the time.
[01:47:17.880 --> 01:47:18.560]   Even better.
[01:47:18.560 --> 01:47:20.640]   I mean, wouldn't you-- if you had the choice between knowing
[01:47:20.640 --> 01:47:22.360]   it's two in the afternoon or you're having a heart attack,
[01:47:22.360 --> 01:47:24.240]   wouldn't you rather know you're having a heart attack?
[01:47:24.240 --> 01:47:25.720]   That's really useful.
[01:47:25.720 --> 01:47:27.360]   It's pretty-- that is useful.
[01:47:27.360 --> 01:47:29.600]   I also like just the SOS feature.
[01:47:29.600 --> 01:47:31.000]   My kids use it.
[01:47:31.000 --> 01:47:34.320]   We've given our kids the watch.
[01:47:34.320 --> 01:47:36.360]   I was completely against it.
[01:47:36.360 --> 01:47:39.440]   But in the end, my husband was like, listen,
[01:47:39.440 --> 01:47:40.480]   it's a really good gift.
[01:47:40.480 --> 01:47:41.720]   It'll save us a whole bunch of money.
[01:47:41.720 --> 01:47:45.320]   And the series is pretty much useless right now.
[01:47:45.320 --> 01:47:47.000]   So I was like, you're right in a couple of years.
[01:47:47.000 --> 01:47:48.000]   You're not going to be able to--
[01:47:48.000 --> 01:47:49.000]   Can you?
[01:47:49.000 --> 01:47:50.000]   But they use the Breathe app.
[01:47:50.000 --> 01:47:51.760]   My little kids, when they're getting too anxious and upset,
[01:47:51.760 --> 01:47:53.840]   I have them just sit and use the Breathe app.
[01:47:53.840 --> 01:47:55.680]   And they said they're using the Breathe app.
[01:47:55.680 --> 01:47:56.680]   It's so sweet.
[01:47:56.680 --> 01:47:58.840]   And they like it?
[01:47:58.840 --> 01:47:59.760]   They do like it.
[01:47:59.760 --> 01:48:02.320]   Because it taps your wrist as you're breathing in.
[01:48:02.320 --> 01:48:03.080]   They do.
[01:48:03.080 --> 01:48:03.920]   They sit there.
[01:48:03.920 --> 01:48:04.800]   It's adorable.
[01:48:04.800 --> 01:48:07.680]   And so I'm like, OK, fair enough, fair enough.
[01:48:07.680 --> 01:48:08.800]   That is very cute.
[01:48:08.800 --> 01:48:10.320]   Wow.
[01:48:10.320 --> 01:48:12.640]   I think what happens with the watch--
[01:48:12.640 --> 01:48:13.160]   it's right.
[01:48:13.160 --> 01:48:15.880]   I mean, I'm just observing myself.
[01:48:15.880 --> 01:48:17.440]   For everybody, it might be a different thing.
[01:48:17.440 --> 01:48:20.280]   But you find the thing-- there is a killer app.
[01:48:20.280 --> 01:48:21.400]   And it's different for everybody.
[01:48:21.400 --> 01:48:23.040]   But once you find a killer app--
[01:48:23.040 --> 01:48:24.560]   I guess it's true of all technology.
[01:48:24.560 --> 01:48:26.320]   Once you find that one app that you go,
[01:48:26.320 --> 01:48:27.800]   I have to have this.
[01:48:27.800 --> 01:48:30.680]   So I have an app called Drafts that lets me dictate,
[01:48:30.680 --> 01:48:33.600]   just dump mind stuff into it.
[01:48:33.600 --> 01:48:35.800]   And so I don't have to remember-- if I tell you I'm
[01:48:35.800 --> 01:48:38.240]   going to do something, I just go do that thing.
[01:48:38.240 --> 01:48:39.200]   I said I would do.
[01:48:39.200 --> 01:48:40.160]   And it all goes in there.
[01:48:40.160 --> 01:48:41.840]   And at the end of the day, it's on my iPad.
[01:48:41.840 --> 01:48:43.320]   And I can process it all.
[01:48:43.320 --> 01:48:46.160]   And that single app, that plus the workout stuff,
[01:48:46.160 --> 01:48:47.840]   I keep track of my--
[01:48:47.840 --> 01:48:50.440]   it's the only thing that really does a good heart job of heart
[01:48:50.440 --> 01:48:51.680]   rate, except for a chest band.
[01:48:51.680 --> 01:48:53.240]   It's really very accurate.
[01:48:53.240 --> 01:48:54.200]   That's enough.
[01:48:54.200 --> 01:48:57.320]   And then-- and I'm sure that Apple planned this.
[01:48:57.320 --> 01:49:01.080]   So then I had to start using my iPhone and my iPad.
[01:49:01.080 --> 01:49:03.200]   So then I had to start using my iMac.
[01:49:03.200 --> 01:49:04.640]   So then I had to get an iMac Pro.
[01:49:04.640 --> 01:49:06.320]   And all of a sudden--
[01:49:06.320 --> 01:49:07.040]   You had to--
[01:49:07.040 --> 01:49:07.720]   You're back in the ecosystem.
[01:49:07.720 --> 01:49:08.720]   --and back in the ecosystem.
[01:49:08.720 --> 01:49:09.640]   Talk to you back in.
[01:49:09.640 --> 01:49:10.640]   They sucked me.
[01:49:10.640 --> 01:49:12.240]   Pulling me back in.
[01:49:12.240 --> 01:49:13.440]   Just when you thought you were out.
[01:49:13.440 --> 01:49:14.080]   I did.
[01:49:14.080 --> 01:49:15.240]   I was using Windows 10.
[01:49:15.240 --> 01:49:16.200]   It's convenient.
[01:49:16.200 --> 01:49:16.800]   Yeah.
[01:49:16.800 --> 01:49:19.000]   But there is-- and this is not a good thing.
[01:49:19.000 --> 01:49:20.760]   I know this intellectually.
[01:49:20.760 --> 01:49:24.800]   Locking in ecosystem, locking is bad for consumers.
[01:49:24.800 --> 01:49:26.960]   Maybe this is like the privacy thing.
[01:49:26.960 --> 01:49:28.360]   It's like a warm bath.
[01:49:28.360 --> 01:49:29.440]   And you just might as well--
[01:49:29.440 --> 01:49:29.960]   It's a warm bath.
[01:49:29.960 --> 01:49:30.480]   You just never know.
[01:49:30.480 --> 01:49:33.680]   It's like the frog in the boiling pot of water.
[01:49:33.680 --> 01:49:35.240]   Just flow it out.
[01:49:35.240 --> 01:49:37.000]   You never know when you're supposed to jump out.
[01:49:37.000 --> 01:49:38.160]   Just boil me.
[01:49:38.160 --> 01:49:39.640]   I don't care.
[01:49:39.640 --> 01:49:42.600]   Yeah, ecosystem lock is going to be bad when it forces you
[01:49:42.600 --> 01:49:44.160]   into the car ecosystem.
[01:49:44.160 --> 01:49:44.720]   Oh, it's terrible.
[01:49:44.720 --> 01:49:46.280]   I mean, this is where--
[01:49:46.280 --> 01:49:50.640]   Like when you start to make 20, 30, $40,000 decisions
[01:49:50.640 --> 01:49:53.400]   based on your ecosystem, Tim knows a lot about this.
[01:49:53.400 --> 01:49:53.880]   I do.
[01:49:53.880 --> 01:49:54.880]   I do.
[01:49:54.880 --> 01:49:56.280]   I mean, oh, yeah.
[01:49:56.280 --> 01:49:56.800]   That's true.
[01:49:56.800 --> 01:49:57.320]   Did you do that, Tim?
[01:49:57.320 --> 01:49:59.600]   Did you buy into an ecosystem today?
[01:49:59.600 --> 01:50:00.240]   I know.
[01:50:00.240 --> 01:50:02.600]   But there was some interesting news this week of BMW
[01:50:02.600 --> 01:50:05.280]   basically putting a paywall around a carplay this week,
[01:50:05.280 --> 01:50:08.360]   which is pretty shocking stuff.
[01:50:08.360 --> 01:50:09.000]   No, wait a minute.
[01:50:09.000 --> 01:50:10.560]   The carplay's free.
[01:50:10.560 --> 01:50:10.960]   Ooh.
[01:50:10.960 --> 01:50:11.440]   Right.
[01:50:11.440 --> 01:50:12.720]   Look at this.
[01:50:12.720 --> 01:50:15.120]   Why would be-- well, first of all, BMW charges you
[01:50:15.120 --> 01:50:19.120]   for anything, a new cigarette lighter on your BMW's $5,000.
[01:50:19.120 --> 01:50:20.360]   I mean, that's just BMW.
[01:50:20.360 --> 01:50:21.520]   If you buy a BMW, you do that.
[01:50:21.520 --> 01:50:22.160]   But why?
[01:50:22.160 --> 01:50:25.200]   What was their rationale for charging for carplay?
[01:50:25.200 --> 01:50:27.600]   It's unclear if indeed there is any.
[01:50:27.600 --> 01:50:29.480]   This-- that should turn into a bit of a big story
[01:50:29.480 --> 01:50:34.160]   because BMW Canada representative told a Canadian journalist
[01:50:34.160 --> 01:50:36.160]   that Apple was actually changing their rates.
[01:50:36.160 --> 01:50:39.320]   And so therefore, BMW is simply following suit.
[01:50:39.320 --> 01:50:41.520]   But that's actually turned out to be untrue and BMW
[01:50:41.520 --> 01:50:42.760]   since retracted that statement.
[01:50:42.760 --> 01:50:45.120]   But right now, if you want carplay in your BMW,
[01:50:45.120 --> 01:50:48.160]   you have to pay $300 for an additional accessory.
[01:50:48.160 --> 01:50:48.800]   A one-time fee.
[01:50:48.800 --> 01:50:51.800]   And at that point, carplay is free as it should be.
[01:50:51.800 --> 01:50:54.240]   Going forward, though, BMW, at least in the US,
[01:50:54.240 --> 01:50:56.320]   will give you one year free, but then it'll
[01:50:56.320 --> 01:51:00.280]   be $80 per year after that for carplay.
[01:51:00.280 --> 01:51:02.760]   Or BMW says this will actually save a lot of people money.
[01:51:02.760 --> 01:51:03.360]   What?
[01:51:03.360 --> 01:51:06.160]   Because a lot of BMW people have short-term leases.
[01:51:06.160 --> 01:51:08.360]   So therefore, they only have the car for two or three years.
[01:51:08.360 --> 01:51:10.200]   And therefore, they'd save money.
[01:51:10.200 --> 01:51:12.320]   But still, it definitely rubbed me the wrong way.
[01:51:12.320 --> 01:51:13.960]   And a lot of other people are the wrong way, too,
[01:51:13.960 --> 01:51:18.400]   that BMW is effectively putting up a paywall around carplay,
[01:51:18.400 --> 01:51:22.320]   which is the free service effectively to users.
[01:51:22.320 --> 01:51:24.880]   So if I buy a Chevy Bolt, which has carplay,
[01:51:24.880 --> 01:51:27.920]   do I pay extra for the carplay capability?
[01:51:27.920 --> 01:51:29.120]   You'd pay nothing at all.
[01:51:29.120 --> 01:51:31.040]   And in fact, you get Android Auto on there as well,
[01:51:31.040 --> 01:51:32.520]   which BMW does not support.
[01:51:32.520 --> 01:51:34.520]   They don't offer Android Auto?
[01:51:34.520 --> 01:51:35.120]   Not at all.
[01:51:35.120 --> 01:51:36.520]   No, BMW does not offer Android Auto.
[01:51:36.520 --> 01:51:37.240]   It's very common.
[01:51:37.240 --> 01:51:39.400]   If one-- if car company-- I understand,
[01:51:39.400 --> 01:51:41.360]   if I offer its carplay, they offer Android Auto as well,
[01:51:41.360 --> 01:51:41.860]   right?
[01:51:41.860 --> 01:51:42.840]   That's just kind of--
[01:51:42.840 --> 01:51:43.800]   Right.
[01:51:43.800 --> 01:51:45.640]   The technology implementation details
[01:51:45.640 --> 01:51:46.920]   are very similar across the two.
[01:51:46.920 --> 01:51:48.880]   You're basically putting a thin client in there
[01:51:48.880 --> 01:51:51.240]   that then the phone kind of provides the videos for.
[01:51:51.240 --> 01:51:53.680]   So if you can do one, you can technically do the other.
[01:51:53.680 --> 01:51:56.440]   But a lot of manufacturers don't like to bring Android Auto
[01:51:56.440 --> 01:51:59.000]   in the car because Google actually asks for a lot of information
[01:51:59.000 --> 01:52:00.520]   about what you're doing, which gets us back
[01:52:00.520 --> 01:52:02.520]   to the privacy discussion.
[01:52:02.520 --> 01:52:04.440]   Google is asking for details about where you are,
[01:52:04.440 --> 01:52:05.880]   and how fast you're going, how long you stay there,
[01:52:05.880 --> 01:52:06.840]   that kind of thing.
[01:52:06.840 --> 01:52:09.480]   Apple doesn't really ask that much in exchange.
[01:52:09.480 --> 01:52:13.440]   So auto manufacturers have been more happy to work with Apple.
[01:52:13.440 --> 01:52:15.360]   But there is a licensing fee involved.
[01:52:15.360 --> 01:52:18.960]   The MFI program made for iPhone or made for iPod back in the day.
[01:52:18.960 --> 01:52:20.280]   But that's a one-time fee.
[01:52:20.280 --> 01:52:21.720]   So there is no recurring fee.
[01:52:21.720 --> 01:52:23.720]   And I spoke with a lot of different folks
[01:52:23.720 --> 01:52:26.680]   and a lot of different places to kind of trace this down
[01:52:26.680 --> 01:52:29.040]   and try to find out why BMW would make this change.
[01:52:29.040 --> 01:52:31.400]   And as far as I can tell, there's no plan,
[01:52:31.400 --> 01:52:33.240]   or there's no change in fee going on.
[01:52:33.240 --> 01:52:36.880]   It's just BMW moving to a services-based payment, which--
[01:52:36.880 --> 01:52:38.640]   sorry to say we're going to see a lot of in the automotive
[01:52:38.640 --> 01:52:39.480]   industry going forward.
[01:52:39.480 --> 01:52:41.240]   A lot of companies are going to try to find ways
[01:52:41.240 --> 01:52:43.400]   to make recurring revenue off of people,
[01:52:43.400 --> 01:52:47.240]   rather than just that upfront payment that we've had this far.
[01:52:47.240 --> 01:52:53.400]   You mentioned, Tim, that you used that Google Arts and Entertainment--
[01:52:53.400 --> 01:52:54.920]   what is it called? Arts and culture.
[01:52:54.920 --> 01:52:55.920]   Arts and culture.
[01:52:55.920 --> 01:52:57.400]   We all did, right?
[01:52:57.400 --> 01:52:58.000]   Did you do it?
[01:52:58.000 --> 01:52:59.880]   Everybody could take a picture of yourself.
[01:52:59.880 --> 01:53:01.560]   I actually just did it on someone else's phone,
[01:53:01.560 --> 01:53:03.360]   because I didn't feel like installing it.
[01:53:03.360 --> 01:53:04.640]   Yeah, and that's really it, too.
[01:53:04.640 --> 01:53:06.680]   Once you do it, it's like, I'm done.
[01:53:06.680 --> 01:53:08.320]   Now, Google's like, what does Lindsay look like?
[01:53:08.320 --> 01:53:08.960]   Oh, my gosh.
[01:53:08.960 --> 01:53:09.480]   Oh, no.
[01:53:09.480 --> 01:53:11.960]   But she now knows that I'm totally confusing Google about me.
[01:53:11.960 --> 01:53:14.920]   But the weird thing about me is that, for some reason,
[01:53:14.920 --> 01:53:18.320]   even though I look like a generic lady,
[01:53:18.320 --> 01:53:22.320]   it will only return profile images.
[01:53:22.320 --> 01:53:23.520]   We don't know why.
[01:53:23.520 --> 01:53:25.560]   All of them are like this, even if I take a photo.
[01:53:25.560 --> 01:53:26.880]   That's interesting.
[01:53:26.880 --> 01:53:29.880]   Here's what Google thinks I look like.
[01:53:29.880 --> 01:53:31.960]   I'm so flattered.
[01:53:31.960 --> 01:53:33.200]   It's a very old man.
[01:53:33.200 --> 01:53:35.760]   Actually, after a while, I looked at it,
[01:53:35.760 --> 01:53:37.720]   and I thought, actually, that's probably exactly
[01:53:37.720 --> 01:53:38.920]   how I look in my age.
[01:53:38.920 --> 01:53:40.120]   Oh, in like 30 years.
[01:53:40.120 --> 01:53:41.280]   No, that's too much.
[01:53:41.280 --> 01:53:41.840]   I admit it.
[01:53:41.840 --> 01:53:43.400]   I look like an old guy.
[01:53:43.400 --> 01:53:43.840]   That's like--
[01:53:43.840 --> 01:53:46.240]   I was a portrait of a gentleman from the North Carolina
[01:53:46.240 --> 01:53:47.240]   Museum of Art.
[01:53:47.240 --> 01:53:47.720]   So--
[01:53:47.720 --> 01:53:48.480]   Oh, that's good.
[01:53:48.480 --> 01:53:49.080]   Good on you.
[01:53:49.080 --> 01:53:49.720]   That's all right.
[01:53:49.720 --> 01:53:50.080]   Yeah.
[01:53:50.080 --> 01:53:52.280]   So this is the thing that's interesting.
[01:53:52.280 --> 01:53:52.520]   And here's--
[01:53:52.520 --> 01:53:54.280]   Yeah, I had just a scary picture, though.
[01:53:54.280 --> 01:53:56.000]   You see, mine was just really--
[01:53:56.000 --> 01:53:56.480]   Oh, really?
[01:53:56.480 --> 01:53:57.560]   I'm like really scary.
[01:53:57.560 --> 01:53:57.920]   I'm like, oh, my god.
[01:53:57.920 --> 01:54:00.000]   Oh, you look like Michael Jackson.
[01:54:00.000 --> 01:54:01.760]   Is that-- that's just-- it's not good.
[01:54:01.760 --> 01:54:03.240]   I was like, OK, I'm done with this.
[01:54:03.240 --> 01:54:04.840]   But yeah, it's a pretty close--
[01:54:04.840 --> 01:54:07.880]   OK, admittedly, that's a scary picture.
[01:54:07.880 --> 01:54:08.320]   Yeah.
[01:54:08.320 --> 01:54:09.040]   But it kind of looks like it.
[01:54:09.040 --> 01:54:10.280]   That's why meth is bad.
[01:54:10.280 --> 01:54:12.880]   I'm like, that was a warning.
[01:54:12.880 --> 01:54:14.240]   It's brutally honest.
[01:54:14.240 --> 01:54:15.440]   It's brutally honest.
[01:54:15.440 --> 01:54:16.040]   It's brutally honest.
[01:54:16.040 --> 01:54:17.440]   It's like, OK, won't two drugs.
[01:54:17.440 --> 01:54:22.720]   So did you know that this app is illegal in Texas and Illinois?
[01:54:22.720 --> 01:54:23.280]   Yeah.
[01:54:23.280 --> 01:54:26.600]   Because of that facial recognition technology.
[01:54:26.600 --> 01:54:29.720]   If you in Texas and Illinois, it refuses to do it.
[01:54:29.720 --> 01:54:30.240]   Yeah.
[01:54:30.240 --> 01:54:31.120]   That's interesting.
[01:54:31.120 --> 01:54:34.280]   Now let me ask you privacy advocates.
[01:54:34.280 --> 01:54:35.960]   Is that good?
[01:54:35.960 --> 01:54:37.840]   It depends on what Google's doing with the pictures you're
[01:54:37.840 --> 01:54:38.680]   taking and the data.
[01:54:38.680 --> 01:54:39.320]   It's accumulating.
[01:54:39.320 --> 01:54:40.280]   We don't know.
[01:54:40.280 --> 01:54:43.200]   Tell us what apps we can--
[01:54:43.200 --> 01:54:46.440]   and by the way, why do they let me do Snapchat and Facebook?
[01:54:46.440 --> 01:54:49.360]   I just want-- again, I just want in this case, Google,
[01:54:49.360 --> 01:54:52.280]   instead of just giving you this fun thing, to say, hey,
[01:54:52.280 --> 01:54:53.280]   we're giving you this fun thing.
[01:54:53.280 --> 01:54:55.680]   Here's what we do with the data.
[01:54:55.680 --> 01:54:57.360]   Yeah, that would be great.
[01:54:57.360 --> 01:55:01.680]   I don't care if they have my face.
[01:55:01.680 --> 01:55:03.520]   I mean, I gave my face.
[01:55:03.520 --> 01:55:04.360]   I took--
[01:55:04.360 --> 01:55:07.280]   Yeah, but Leo, you're not-- it's not a fair assessment.
[01:55:07.280 --> 01:55:07.800]   Well, who?
[01:55:07.800 --> 01:55:10.160]   You pretty much live online.
[01:55:10.160 --> 01:55:12.200]   I guess if you're in the witness protection program--
[01:55:12.200 --> 01:55:13.520]   You're selling yourself.
[01:55:13.520 --> 01:55:15.640]   Yeah, no, I guess if you're a witness protection program
[01:55:15.640 --> 01:55:19.920]   or you're a woman who's being stalked in an abusive relationship,
[01:55:19.920 --> 01:55:20.440]   I guess--
[01:55:20.440 --> 01:55:21.080]   Or you're--
[01:55:21.080 --> 01:55:23.000]   But it's not like Google's going to call the guy up
[01:55:23.000 --> 01:55:24.480]   and say, I found your wife.
[01:55:24.480 --> 01:55:25.480]   Well, there's people that--
[01:55:25.480 --> 01:55:29.560]   My sister, who's a DA, an ADM, like she doesn't want people
[01:55:29.560 --> 01:55:30.680]   to know which is--
[01:55:30.680 --> 01:55:31.520]   Which she looks like.
[01:55:31.520 --> 01:55:33.200]   Well, I have to do is go to court
[01:55:33.200 --> 01:55:34.520]   and you can see her.
[01:55:34.520 --> 01:55:36.080]   Yeah, but--
[01:55:36.080 --> 01:55:40.240]   There's also cases of ex-husband's or abusive people
[01:55:40.240 --> 01:55:43.160]   or people that have gone through trauma that are being tracked
[01:55:43.160 --> 01:55:43.520]   or stopped.
[01:55:43.520 --> 01:55:44.520]   I understand that.
[01:55:44.520 --> 01:55:47.280]   By sometimes-- so there is some reasons why I think
[01:55:47.280 --> 01:55:49.520]   the right to be able to be forgotten is something
[01:55:49.520 --> 01:55:50.960]   that we have to take a look in.
[01:55:50.960 --> 01:55:51.520]   Then you should--
[01:55:51.520 --> 01:55:53.920]   You shouldn't use this app.
[01:55:53.920 --> 01:55:55.520]   That's not a fair assessment, though.
[01:55:55.520 --> 01:55:56.520]   That's not a fair thing.
[01:55:56.520 --> 01:55:58.520]   Companies should not be given free rein
[01:55:58.520 --> 01:56:00.800]   with what they do with our information.
[01:56:00.800 --> 01:56:02.760]   And right now, it's the Wild West.
[01:56:02.760 --> 01:56:03.320]   It's great.
[01:56:03.320 --> 01:56:05.600]   There's definitely some need for legislation
[01:56:05.600 --> 01:56:08.120]   to be able to say what they're doing and why.
[01:56:08.120 --> 01:56:10.040]   And that should be clearly stated.
[01:56:10.040 --> 01:56:11.520]   OK.
[01:56:11.520 --> 01:56:12.520]   I think the only alternative, though,
[01:56:12.520 --> 01:56:14.160]   is that Google charges us for this app
[01:56:14.160 --> 01:56:16.440]   and nobody would pay for this, which it would mean
[01:56:16.440 --> 01:56:17.960]   we wouldn't have the app in the first place.
[01:56:17.960 --> 01:56:18.560]   So--
[01:56:18.560 --> 01:56:19.360]   They don't only use--
[01:56:19.360 --> 01:56:19.920]   They don't really use--
[01:56:19.920 --> 01:56:21.920]   They don't really use--
[01:56:21.920 --> 01:56:24.320]   Like, we don't even use really good apps that are worth us
[01:56:24.320 --> 01:56:27.000]   spending the money to be able to use them.
[01:56:27.000 --> 01:56:27.600]   And then--
[01:56:27.600 --> 01:56:30.720]   Oh, God, I dread a future where states--
[01:56:30.720 --> 01:56:35.000]   And by the way, individually, outlaw stuff,
[01:56:35.000 --> 01:56:38.040]   because some legislators decided this was a bad idea,
[01:56:38.040 --> 01:56:39.280]   and all of a sudden--
[01:56:39.280 --> 01:56:39.800]   I mean, that's--
[01:56:39.800 --> 01:56:41.720]   I think usually they don't understand it.
[01:56:41.720 --> 01:56:44.320]   The trouble with legislation is that it's very hard
[01:56:44.320 --> 01:56:47.280]   for legislative bodies to understand the implications
[01:56:47.280 --> 01:56:47.680]   and the future--
[01:56:47.680 --> 01:56:49.240]   Well, and this is impossible, in force.
[01:56:49.240 --> 01:56:49.680]   Of the decision.
[01:56:49.680 --> 01:56:52.960]   So everybody in Illinois, if you've got Snapchat, Facebook,
[01:56:52.960 --> 01:56:55.320]   or any photo app on your program--
[01:56:55.320 --> 01:56:58.600]   Oh, by the way, Face ID, you must turn that off now
[01:56:58.600 --> 01:57:00.840]   because it's illegal.
[01:57:00.840 --> 01:57:03.720]   That's crazy talk.
[01:57:03.720 --> 01:57:04.640]   It sounds like--
[01:57:04.640 --> 01:57:05.080]   I think Apple--
[01:57:05.080 --> 01:57:06.600]   You want to test this?
[01:57:06.600 --> 01:57:09.000]   Snapchat should just say, yeah, not available in Texas
[01:57:09.000 --> 01:57:09.840]   in Illinois.
[01:57:09.840 --> 01:57:10.840]   See along that law lines.
[01:57:10.840 --> 01:57:12.520]   Well, I think that Google was just being--
[01:57:12.520 --> 01:57:13.680]   maybe overly cautious.
[01:57:13.680 --> 01:57:14.720]   Maybe they just felt like this--
[01:57:14.720 --> 01:57:15.180]   I don't know.
[01:57:15.180 --> 01:57:17.280]   I don't know what Google thought, but maybe they were like--
[01:57:17.280 --> 01:57:18.680]   But maybe that's what Google was--
[01:57:18.680 --> 01:57:19.960]   They actually might have been doing that.
[01:57:19.960 --> 01:57:21.760]   They might have been saying like, well, OK.
[01:57:21.760 --> 01:57:22.880]   All right, you want to test that law--
[01:57:22.880 --> 01:57:24.280]   I'll call your blood, because this isn't that important.
[01:57:24.280 --> 01:57:24.880]   Anyway.
[01:57:24.880 --> 01:57:25.360]   Yeah.
[01:57:25.360 --> 01:57:26.520]   See how that-- you feel.
[01:57:27.520 --> 01:57:29.520]   I don't know.
[01:57:29.520 --> 01:57:32.520]   It's-- all right.
[01:57:32.520 --> 01:57:33.520]   [LAUGHTER]
[01:57:33.520 --> 01:57:36.520]   I obviously am not in the mainstream on this.
[01:57:36.520 --> 01:57:37.520]   I'll acknowledge that.
[01:57:37.520 --> 01:57:38.520]   I don't get it.
[01:57:38.520 --> 01:57:39.520]   I just don't get it.
[01:57:39.520 --> 01:57:41.520]   It doesn't make any sense to me.
[01:57:41.520 --> 01:57:44.520]   And because nobody's making you use this app, I understand--
[01:57:44.520 --> 01:57:46.520]   Most people don't know.
[01:57:46.520 --> 01:57:47.520]   They don't know what they're doing.
[01:57:47.520 --> 01:57:50.520]   There should be laws against the state putting cameras
[01:57:50.520 --> 01:57:53.520]   every freak in wear and using face recognition.
[01:57:53.520 --> 01:57:55.520]   And by the way, they do this very effectively
[01:57:55.520 --> 01:57:57.520]   in Russia and China.
[01:57:57.520 --> 01:57:58.520]   And they can tell--
[01:57:58.520 --> 01:58:01.520]   It was fascinating piece on what they can do in China.
[01:58:01.520 --> 01:58:04.520]   They can tell if you're in Beijing where you've gone,
[01:58:04.520 --> 01:58:05.520]   where you've been, what you've done.
[01:58:05.520 --> 01:58:06.520]   I understand.
[01:58:06.520 --> 01:58:07.520]   There should be rules against that.
[01:58:07.520 --> 01:58:10.520]   That happens here with license plates.
[01:58:10.520 --> 01:58:11.520]   Yeah, I'm saying.
[01:58:11.520 --> 01:58:14.520]   There aren't any rules against that in the United States.
[01:58:14.520 --> 01:58:17.520]   Well, Leo, why are you now saying that in public places,
[01:58:17.520 --> 01:58:20.520]   the government shouldn't have cameras yet in your private--
[01:58:20.520 --> 01:58:22.520]   If I want to take a picture of myself--
[01:58:22.520 --> 01:58:23.520]   Totally OK.
[01:58:23.520 --> 01:58:24.520]   It is, because I gave--
[01:58:24.520 --> 01:58:25.520]   I did it.
[01:58:25.520 --> 01:58:26.520]   I took it.
[01:58:26.520 --> 01:58:27.520]   I bought the camera.
[01:58:27.520 --> 01:58:29.520]   I put the app on my phone and I did it.
[01:58:29.520 --> 01:58:31.520]   You didn't have to walk down that road.
[01:58:31.520 --> 01:58:32.520]   I chose to do it.
[01:58:32.520 --> 01:58:35.520]   Meanwhile, the government does not restrict it anyway.
[01:58:35.520 --> 01:58:37.520]   It's right to take pictures of your face,
[01:58:37.520 --> 01:58:41.520]   match them up in a database that's corrupt and poorly run
[01:58:41.520 --> 01:58:44.520]   and produces all sorts of false positives.
[01:58:44.520 --> 01:58:46.520]   There's no limit on that.
[01:58:46.520 --> 01:58:48.520]   But the government can also subpoena anything.
[01:58:48.520 --> 01:58:51.520]   So it's kind of, I think, again, it's like there's no actual
[01:58:51.520 --> 01:58:53.520]   boundary between what you elect into
[01:58:53.520 --> 01:58:54.520]   and what you--
[01:58:54.520 --> 01:58:55.520]   That's a good point.
[01:58:55.520 --> 01:58:58.520]   But it's disingenuous of the state of Illinois and Texas
[01:58:58.520 --> 01:59:01.520]   governments to say, well, you better not do that.
[01:59:01.520 --> 01:59:04.520]   When they're doing it anyway all the time, I mean, to me,
[01:59:04.520 --> 01:59:08.520]   that's actually more of a problem because they're basically
[01:59:08.520 --> 01:59:11.520]   got a monopoly on face recognition.
[01:59:11.520 --> 01:59:12.520]   Mm.
[01:59:12.520 --> 01:59:13.520]   Mm.
[01:59:13.520 --> 01:59:14.520]   Mm.
[01:59:14.520 --> 01:59:15.520]   All right.
[01:59:15.520 --> 01:59:16.520]   All right.
[01:59:16.520 --> 01:59:17.520]   I know I'm not in the mainstream.
[01:59:17.520 --> 01:59:18.520]   I'm not in the mainstream.
[01:59:18.520 --> 01:59:19.520]   Let's take a break.
[01:59:19.520 --> 01:59:20.520]   We got more to do.
[01:59:20.520 --> 01:59:21.520]   We got a great panel.
[01:59:21.520 --> 01:59:22.520]   I'm so glad you're here.
[01:59:22.520 --> 01:59:25.520]   I'm the president of the American Tech School of Illinois.
[01:59:25.520 --> 01:59:29.520]   I'm the president of the American Tech School of Illinois.
[01:59:29.520 --> 01:59:32.520]   I'm the president of the American Tech School of Illinois.
[01:59:32.520 --> 01:59:35.520]   And I'm the president of the American Tech School of Illinois.
[01:59:35.520 --> 01:59:37.520]   I'm the president of the American Tech School of Illinois.
[01:59:37.520 --> 01:59:39.520]   I'm the president of the American Tech School of Illinois.
[01:59:39.520 --> 01:59:41.520]   I'm the president of the American Tech School of Illinois.
[01:59:41.520 --> 01:59:44.520]   And I'm the president of the American Tech School of Illinois.
[01:59:44.520 --> 01:59:46.520]   And I'm the president of the American Tech School of Illinois.
[01:59:46.520 --> 01:59:48.520]   I'm the president of the American Tech School of Illinois.
[01:59:48.520 --> 01:59:50.520]   I'm the president of the American Tech School of Illinois.
[01:59:50.520 --> 01:59:52.520]   I'm the president of the American Tech School of Illinois.
[01:59:52.520 --> 01:59:54.520]   I'm the president of the American Tech School of Illinois.
[01:59:54.520 --> 01:59:56.520]   I'm the president of the American Tech School of Illinois.
[01:59:56.520 --> 01:59:58.520]   I'm the president of the American Tech School of Illinois.
[01:59:58.520 --> 02:00:00.520]   I'm the president of the American Tech School of Illinois.
[02:00:00.520 --> 02:00:02.520]   I'm the president of the American Tech School of Illinois.
[02:00:02.520 --> 02:00:03.520]   I'm the president of the American Tech School of Illinois.
[02:00:03.520 --> 02:00:04.520]   I'm the president of the American Tech School of Illinois.
[02:00:04.520 --> 02:00:05.520]   I'm the president of the American Tech School of Illinois.
[02:00:05.520 --> 02:00:06.520]   I'm the president of the American Tech School of Illinois.
[02:00:06.520 --> 02:00:07.520]   I'm the president of the American Tech School of Illinois.
[02:00:07.520 --> 02:00:08.520]   I'm the president of the American Tech School of Illinois.
[02:00:08.520 --> 02:00:09.520]   I'm the president of the American Tech School of Illinois.
[02:00:09.520 --> 02:00:10.520]   I'm the president of the American Tech School of Illinois.
[02:00:10.520 --> 02:00:11.520]   I'm the president of the American Tech School of Illinois.
[02:00:11.520 --> 02:00:18.520]   This is the cleaning your nails selfie.
[02:00:18.520 --> 02:00:20.520]   I know because my daughter does a lot of nail selfies.
[02:00:20.520 --> 02:00:24.520]   Tim Stevens is all I do not do nail selfies.
[02:00:24.520 --> 02:00:25.520]   Tim Stevens is also here.
[02:00:25.520 --> 02:00:28.520]   He does ice driving, ice racing selfies.
[02:00:28.520 --> 02:00:30.520]   Have you been ice racing this winter?
[02:00:30.520 --> 02:00:34.520]   I was on the lake about three hours ago as a matter of fact.
[02:00:34.520 --> 02:00:35.520]   So, yes, I was racing today.
[02:00:35.520 --> 02:00:38.520]   Where can I see some videos of this?
[02:00:38.520 --> 02:00:40.520]   This was the first of the season, wasn't it?
[02:00:40.520 --> 02:00:41.520]   Tim, did I see that this morning?
[02:00:41.520 --> 02:00:42.520]   Was your first race?
[02:00:42.520 --> 02:00:43.520]   It was the first.
[02:00:43.520 --> 02:00:46.520]   Yeah, I shared a video on Twitter, Leo, a couple hours ago.
[02:00:46.520 --> 02:00:49.520]   Yeah, this was the first racing weekend.
[02:00:49.520 --> 02:00:51.520]   First, I've done it like two years, so it was good to get back out there.
[02:00:51.520 --> 02:00:52.520]   Oh, really?
[02:00:52.520 --> 02:00:53.520]   I got it.
[02:00:53.520 --> 02:00:54.520]   Second and a third today.
[02:00:54.520 --> 02:00:55.520]   So, not too bad.
[02:00:55.520 --> 02:00:56.520]   I just got to get back out.
[02:00:56.520 --> 02:00:59.520]   Now, do you ice race on skates, on a car, running?
[02:00:59.520 --> 02:01:00.520]   How do you do this?
[02:01:00.520 --> 02:01:02.520]   In a Subaru, actually.
[02:01:02.520 --> 02:01:06.520]   So, I was out on Lake Elgonquin in Wales, New York, in my Subaru.
[02:01:06.520 --> 02:01:14.520]   And do you modify in any way this vehicle so that it will race better on ice?
[02:01:14.520 --> 02:01:16.520]   We run street legal tires.
[02:01:16.520 --> 02:01:18.520]   We're required to buy regulations.
[02:01:18.520 --> 02:01:21.520]   So, they are studded tires, but there's street legal studded tires.
[02:01:21.520 --> 02:01:23.520]   They're not very big, gnarly studs.
[02:01:23.520 --> 02:01:27.520]   My car's got some upgrades on it, but by and large, it's pretty much the stock 2004 STI.
[02:01:27.520 --> 02:01:30.520]   Wow, let me let's watch the video, kids.
[02:01:30.520 --> 02:01:33.520]   Ice racing, round one, heat one.
[02:01:33.520 --> 02:01:34.520]   It's 360 videos.
[02:01:34.520 --> 02:01:35.520]   You can click and scroll around.
[02:01:35.520 --> 02:01:36.520]   Oh, no.
[02:01:36.520 --> 02:01:38.520]   What did you use for that?
[02:01:38.520 --> 02:01:39.520]   I was a gear 360.
[02:01:39.520 --> 02:01:43.520]   It came with my no-date, so I was playing around with it today.
[02:01:43.520 --> 02:01:44.520]   Oh, neat.
[02:01:44.520 --> 02:01:45.520]   This is figure.
[02:01:45.520 --> 02:01:47.520]   You're right there in the cockpit.
[02:01:47.520 --> 02:01:49.520]   Put on your daydream and you can...
[02:01:49.520 --> 02:01:50.520]   Holy cow.
[02:01:50.520 --> 02:01:51.520]   That's amazing.
[02:01:51.520 --> 02:01:52.520]   You did this today.
[02:01:52.520 --> 02:01:53.520]   Yeah, that was me.
[02:01:53.520 --> 02:01:54.520]   This is up to noon.
[02:01:54.520 --> 02:01:58.520]   It's almost like he knows something about technology and cars.
[02:01:58.520 --> 02:01:59.520]   I think this should be out loud.
[02:01:59.520 --> 02:02:01.520]   You should never be allowed to do this.
[02:02:01.520 --> 02:02:02.520]   You see my exhaust.
[02:02:02.520 --> 02:02:03.520]   You ran into a car.
[02:02:03.520 --> 02:02:06.520]   Is the car ran into you or you ran into a car?
[02:02:06.520 --> 02:02:09.520]   No, so I'm fighting for second place.
[02:02:09.520 --> 02:02:11.520]   That second place ahead of me.
[02:02:11.520 --> 02:02:13.520]   I don't know how long I want to watch this video for, but...
[02:02:13.520 --> 02:02:14.520]   Forever.
[02:02:14.520 --> 02:02:18.520]   I basically passed him right before the line anyway, which I got very excited when I did it.
[02:02:18.520 --> 02:02:19.520]   This is great.
[02:02:19.520 --> 02:02:23.520]   Well, let's jump ahead to the photo finish.
[02:02:23.520 --> 02:02:26.520]   I think you got to go back a little bit.
[02:02:26.520 --> 02:02:27.520]   Oh, here we go.
[02:02:27.520 --> 02:02:28.520]   Here we go.
[02:02:28.520 --> 02:02:29.520]   Here we go.
[02:02:29.520 --> 02:02:30.520]   Here we go.
[02:02:30.520 --> 02:02:31.520]   Here we go.
[02:02:31.520 --> 02:02:32.520]   There's that already.
[02:02:32.520 --> 02:02:33.520]   You already win.
[02:02:33.520 --> 02:02:35.520]   Yeah, I'm not sure where to actually choose.
[02:02:35.520 --> 02:02:37.520]   I might want to jump back a little bit more.
[02:02:37.520 --> 02:02:39.520]   It's a back a little bit more.
[02:02:39.520 --> 02:02:40.520]   Here we go.
[02:02:40.520 --> 02:02:41.520]   Here we go.
[02:02:41.520 --> 02:02:42.520]   Oh, this is exciting.
[02:02:42.520 --> 02:02:43.520]   Oh, I'm excited.
[02:02:43.520 --> 02:02:44.520]   Oh, by the way, not a lot of traction.
[02:02:44.520 --> 02:02:45.520]   That car is kind of going...
[02:02:45.520 --> 02:02:48.520]   Yeah, so we're going on a straight side.
[02:02:48.520 --> 02:02:49.520]   Oh!
[02:02:49.520 --> 02:02:50.520]   Oh!
[02:02:50.520 --> 02:02:51.520]   Bye-bye!
[02:02:51.520 --> 02:02:52.520]   Goodbye!
[02:02:52.520 --> 02:02:53.520]   Goodbye!
[02:02:53.520 --> 02:02:55.520]   That was great to mine.
[02:02:55.520 --> 02:02:58.520]   I love it that you did 360.
[02:02:58.520 --> 02:03:01.520]   That is so much fun.
[02:03:01.520 --> 02:03:03.520]   We were all ice racing with you.
[02:03:03.520 --> 02:03:04.520]   Thanks for coming to the ride.
[02:03:04.520 --> 02:03:05.520]   Round one heat run.
[02:03:05.520 --> 02:03:07.520]   Second place.
[02:03:07.520 --> 02:03:09.520]   Given the silver medal, ladies and gentlemen.
[02:03:09.520 --> 02:03:10.520]   I started last.
[02:03:10.520 --> 02:03:11.520]   I was very proud of second place.
[02:03:11.520 --> 02:03:12.520]   Wow.
[02:03:12.520 --> 02:03:13.520]   That's very cool.
[02:03:13.520 --> 02:03:17.520]   Lake Algonquin in Wells, New York, if you want to go out to the next meet.
[02:03:17.520 --> 02:03:19.520]   That might be cooler than yellow nails.
[02:03:19.520 --> 02:03:20.520]   I mean, I don't know.
[02:03:20.520 --> 02:03:21.520]   I don't know.
[02:03:21.520 --> 02:03:22.520]   You're both up there.
[02:03:22.520 --> 02:03:24.520]   He should race with yellow nails.
[02:03:24.520 --> 02:03:25.520]   I have yellow helmet.
[02:03:25.520 --> 02:03:26.520]   I don't know.
[02:03:26.520 --> 02:03:29.520]   Also, Georgia Dow, she's here.
[02:03:29.520 --> 02:03:32.520]   We love Georgia and we are so thrilled to see you.
[02:03:32.520 --> 02:03:34.520]   Don't forget Georgia's anxiety videos.
[02:03:34.520 --> 02:03:37.520]   That sounds a little weird when I say that.
[02:03:37.520 --> 02:03:40.520]   Anxiety-videos.com.
[02:03:40.520 --> 02:03:41.520]   She does some great videos.
[02:03:41.520 --> 02:03:46.520]   Not just about anxiety, but getting sleep, getting comfortable.
[02:03:46.520 --> 02:03:53.520]   All sorts of interesting stuff that you can just download these DVDs, boundaries and consequences.
[02:03:53.520 --> 02:03:54.520]   Parenting.
[02:03:54.520 --> 02:03:55.520]   Parenting.
[02:03:55.520 --> 02:03:56.520]   We all need that.
[02:03:56.520 --> 02:03:58.520]   We all need that.
[02:03:58.520 --> 02:04:01.520]   Go to anxiety-videos.com.
[02:04:01.520 --> 02:04:08.520]   Help, as they say, is just a click away.
[02:04:08.520 --> 02:04:09.520]   I hope these are doing really well.
[02:04:09.520 --> 02:04:10.520]   These are just really...
[02:04:10.520 --> 02:04:11.520]   They do well.
[02:04:11.520 --> 02:04:12.520]   They do well.
[02:04:12.520 --> 02:04:13.520]   Good.
[02:04:13.520 --> 02:04:17.520]   Because not everybody can get a good therapist or can find one or afford one.
[02:04:17.520 --> 02:04:23.720]   Sometimes people are shy too and don't feel comfortable going in, so you can kind of take
[02:04:23.720 --> 02:04:26.320]   care of things that you can take care of yourself.
[02:04:26.320 --> 02:04:28.320]   Or some people do it in adjunct, right?
[02:04:28.320 --> 02:04:32.320]   They want to see someone and then they can keep this and take it home whenever they're feeling.
[02:04:32.320 --> 02:04:33.320]   I am so...
[02:04:33.320 --> 02:04:36.320]   I actually post my therapy sessions on Instagram.
[02:04:36.320 --> 02:04:37.320]   Do you really?
[02:04:37.320 --> 02:04:38.320]   No.
[02:04:38.320 --> 02:04:39.320]   No.
[02:04:39.320 --> 02:04:40.320]   I don't.
[02:04:40.320 --> 02:04:41.320]   What is it?
[02:04:41.320 --> 02:04:43.320]   I just said he doesn't care about privacy.
[02:04:43.320 --> 02:04:46.320]   That's going one step too far.
[02:04:46.320 --> 02:04:47.320]   I'm not more than crazy.
[02:04:47.320 --> 02:04:48.320]   I could.
[02:04:48.320 --> 02:04:49.320]   No.
[02:04:49.320 --> 02:04:54.320]   There's a lot of issues that are important to talk about because that gets rid of stigma.
[02:04:54.320 --> 02:04:55.320]   No stigma.
[02:04:55.320 --> 02:04:56.320]   We all have our stuff, right?
[02:04:56.320 --> 02:04:58.640]   We all have different things we need to deal with.
[02:04:58.640 --> 02:05:01.960]   If we dealt with them more openly, I think that people would realize that we're all just
[02:05:01.960 --> 02:05:02.960]   human.
[02:05:02.960 --> 02:05:03.960]   Yeah.
[02:05:03.960 --> 02:05:04.960]   I was telling my daughter that.
[02:05:04.960 --> 02:05:08.800]   Again, 22 years old or 25 years old, just talking about boys and stuff.
[02:05:08.800 --> 02:05:10.160]   I said, "You know what?
[02:05:10.160 --> 02:05:11.760]   There's no perfect kid boy.
[02:05:11.760 --> 02:05:13.880]   We're all F'd up."
[02:05:13.880 --> 02:05:16.480]   It's just as you get in a relationship with somebody, you just realize, "Well, this is
[02:05:16.480 --> 02:05:19.320]   how they're F'd up and I'm going to be able to work around that.
[02:05:19.320 --> 02:05:20.320]   That's fine."
[02:05:20.320 --> 02:05:21.320]   Right?
[02:05:21.320 --> 02:05:22.320]   Am I wrong?
[02:05:22.320 --> 02:05:24.120]   Are any of you in a perfect relationship?
[02:05:24.120 --> 02:05:25.120]   Nobody's perfect.
[02:05:25.120 --> 02:05:29.360]   It would be boring if you were married to someone or you were yourself were perfect.
[02:05:29.360 --> 02:05:33.320]   Well, you'd feel like crap if you were perfect and married to a normal person.
[02:05:33.320 --> 02:05:34.320]   Right.
[02:05:34.320 --> 02:05:35.320]   It just would.
[02:05:35.320 --> 02:05:36.320]   It doesn't happen.
[02:05:36.320 --> 02:05:38.680]   That would be its own annoyance is having someone that was perfect.
[02:05:38.680 --> 02:05:40.480]   That would be what would be perfect.
[02:05:40.480 --> 02:05:41.480]   Important lesson.
[02:05:41.480 --> 02:05:44.000]   It does not happen.
[02:05:44.000 --> 02:05:46.320]   We all have our weirdnesses.
[02:05:46.320 --> 02:05:49.520]   And one person's perfect might be another person's totally F'd up.
[02:05:49.520 --> 02:05:50.520]   That is a good point.
[02:05:50.520 --> 02:05:51.520]   That is a good point.
[02:05:51.520 --> 02:05:52.920]   Actually, that's an excellent point.
[02:05:52.920 --> 02:05:53.920]   Yeah.
[02:05:53.920 --> 02:05:54.920]   It's a good question.
[02:05:54.920 --> 02:05:55.920]   It's what makes us interesting.
[02:05:55.920 --> 02:05:56.920]   It's what makes us interesting.
[02:05:56.920 --> 02:05:57.920]   We're different.
[02:05:57.920 --> 02:05:58.920]   It makes us interesting.
[02:05:58.920 --> 02:05:59.920]   It is.
[02:05:59.920 --> 02:06:01.920]   It is, I guess.
[02:06:01.920 --> 02:06:02.920]   [laughter]
[02:06:02.920 --> 02:06:03.920]   Totally.
[02:06:03.920 --> 02:06:10.160]   We'll be back one more segment to come before we go though.
[02:06:10.160 --> 02:06:13.440]   I want to tell you about Tracker because you, the last thing you want to do, you've got
[02:06:13.440 --> 02:06:16.880]   a busy life, you've got lots to do, you've got places to go, people to see, the last thing
[02:06:16.880 --> 02:06:22.080]   you want to do is spend the next hour looking for your keys so you can go.
[02:06:22.080 --> 02:06:24.800]   But how many of us do that all the time?
[02:06:24.800 --> 02:06:26.640]   It's like stop the insanity.
[02:06:26.640 --> 02:06:28.360]   There is a better way.
[02:06:28.360 --> 02:06:32.360]   If there's stuff you know you have, but you cannot find, you need Tracker.
[02:06:32.360 --> 02:06:33.680]   Tracker goes on your keys.
[02:06:33.680 --> 02:06:34.680]   It goes in your purse.
[02:06:34.680 --> 02:06:35.680]   It goes in your suitcase.
[02:06:35.680 --> 02:06:36.680]   It goes on your bicycle.
[02:06:36.680 --> 02:06:37.960]   It goes in your headphones.
[02:06:37.960 --> 02:06:41.000]   Goes on your remote control.
[02:06:41.000 --> 02:06:42.440]   Anything that you misplace.
[02:06:42.440 --> 02:06:45.240]   And by the way, the new Tracker Pixel, the lightest Tracker ever.
[02:06:45.240 --> 02:06:48.520]   It's so small about the size of a dime weighs absolutely nothing.
[02:06:48.520 --> 02:06:51.640]   And that means you can put it on anything.
[02:06:51.640 --> 02:06:52.640]   It pairs to your phone.
[02:06:52.640 --> 02:06:53.640]   It's a Bluetooth tracking device.
[02:06:53.640 --> 02:06:54.640]   It's not GPS.
[02:06:54.640 --> 02:06:55.960]   I should be clear about this.
[02:06:55.960 --> 02:06:56.960]   It's a Bluetooth tracking device.
[02:06:56.960 --> 02:06:57.960]   So you pair it to your phone.
[02:06:57.960 --> 02:06:59.640]   Your phone has GPS.
[02:06:59.640 --> 02:07:03.360]   If you, there are a number of ways it helps you find stuff.
[02:07:03.360 --> 02:07:05.160]   First of all, the two-way separation alerts.
[02:07:05.160 --> 02:07:08.600]   If you leave your Tracker, let's say my Tracker is on my keys, which it is.
[02:07:08.600 --> 02:07:11.240]   When I leave my keys behind the phone makes noises.
[02:07:11.240 --> 02:07:12.840]   Say, "Hey, you left your keys."
[02:07:12.840 --> 02:07:14.320]   And I go, "Oh shoot, I left my keys."
[02:07:14.320 --> 02:07:15.320]   Same thing the other way around.
[02:07:15.320 --> 02:07:17.240]   Leave my phone behind.
[02:07:17.240 --> 02:07:18.960]   Keys make noise.
[02:07:18.960 --> 02:07:23.440]   If you can't find your keys, you open up the Tracker app on your phone, you press the
[02:07:23.440 --> 02:07:25.520]   button and the keys make a very loud noise.
[02:07:25.520 --> 02:07:26.880]   And with the Pixel, they have lights.
[02:07:26.880 --> 02:07:27.880]   So they light up.
[02:07:27.880 --> 02:07:31.040]   And if it's fallen down the couch or whatever, it's easier to find under the bed.
[02:07:31.040 --> 02:07:32.120]   Really easy to find.
[02:07:32.120 --> 02:07:34.680]   I put these by the way on my AirPods.
[02:07:34.680 --> 02:07:37.280]   Because I actually bought a second pair of AirPods.
[02:07:37.280 --> 02:07:38.560]   Because I thought I lost them.
[02:07:38.560 --> 02:07:39.680]   And they were under the bed the whole time.
[02:07:39.680 --> 02:07:41.680]   So now I have Tracker on my AirPods.
[02:07:41.680 --> 02:07:42.680]   They're on the case.
[02:07:42.680 --> 02:07:43.680]   They're not there.
[02:07:43.680 --> 02:07:44.680]   Yeah, not on the...
[02:07:44.680 --> 02:07:46.480]   It looks like they'd probably then be like earrings.
[02:07:46.480 --> 02:07:47.480]   Yeah.
[02:07:47.480 --> 02:07:50.360]   Yeah, although, wait a minute now, that's a stylish look.
[02:07:50.360 --> 02:07:51.360]   I think I might do that.
[02:07:51.360 --> 02:07:52.360]   Could be fast.
[02:07:52.360 --> 02:07:54.360]   I didn't even think about that.
[02:07:54.360 --> 02:07:55.360]   Yeah.
[02:07:55.360 --> 02:07:56.360]   Dangles.
[02:07:56.360 --> 02:07:57.360]   You also have a button on the Tracker.
[02:07:57.360 --> 02:08:00.920]   So if you can't find your phone, and that happens all the time, right?
[02:08:00.920 --> 02:08:01.920]   It's not like you...
[02:08:01.920 --> 02:08:05.400]   In this case, it's not like you left it somewhere far away.
[02:08:05.400 --> 02:08:06.400]   It's on the kitchen counter.
[02:08:06.400 --> 02:08:08.120]   And you just forgot where you put it.
[02:08:08.120 --> 02:08:11.200]   You press the button on the Tracker, the phone, even if it's silenced, go, "Yeah, I'm here."
[02:08:11.200 --> 02:08:13.200]   It's not worth firing up the fine my iPhone.
[02:08:13.200 --> 02:08:14.200]   You just press the button, it's there.
[02:08:14.200 --> 02:08:15.200]   You got it.
[02:08:15.200 --> 02:08:16.280]   It simplifies your life.
[02:08:16.280 --> 02:08:22.640]   Now, the larger issue is if you do your keys fall out of your pocket, and you lose them,
[02:08:22.640 --> 02:08:26.880]   and then somebody picks them up and takes them somewhere, now you open up the phone,
[02:08:26.880 --> 02:08:28.280]   the Tracker shows the last...
[02:08:28.280 --> 02:08:30.120]   Because Bluetooth, it's only good for 150 feet.
[02:08:30.120 --> 02:08:32.160]   So it shows you the last place it saw them.
[02:08:32.160 --> 02:08:33.680]   You go there, they're not there.
[02:08:33.680 --> 02:08:36.120]   This is where Tracker really sings.
[02:08:36.120 --> 02:08:38.280]   It's the world's largest crowd locate network.
[02:08:38.280 --> 02:08:40.640]   See, there's five million trackers out there.
[02:08:40.640 --> 02:08:43.800]   So that means there's millions of Tracker users.
[02:08:43.800 --> 02:08:44.800]   And there...
[02:08:44.800 --> 02:08:46.040]   It's like they're on your team.
[02:08:46.040 --> 02:08:48.040]   And they don't have to do anything.
[02:08:48.040 --> 02:08:49.040]   You don't have to do anything.
[02:08:49.040 --> 02:08:50.560]   You're always participating in this.
[02:08:50.560 --> 02:08:54.360]   Whenever you walk by somebody's keys, your phone says, "Because it's Bluetooth-ely.
[02:08:54.360 --> 02:08:57.200]   I see keys or I see a Tracker device, actually.
[02:08:57.200 --> 02:08:58.200]   I see a Tracker device.
[02:08:58.200 --> 02:08:59.200]   It's a...
[02:08:59.200 --> 02:09:00.200]   What's the serial number?
[02:09:00.200 --> 02:09:01.200]   Okay."
[02:09:01.200 --> 02:09:05.840]   And then it pings your phone and says, "Your tracker was just seeing your keys are here."
[02:09:05.840 --> 02:09:06.840]   That is fantastic.
[02:09:06.840 --> 02:09:07.840]   It works really, really well.
[02:09:07.840 --> 02:09:10.120]   In fact, if you've got a Tracker, you can test it.
[02:09:10.120 --> 02:09:11.240]   It's not on by default.
[02:09:11.240 --> 02:09:14.680]   It's only when you lose something, you turn on the notifications, but turn it on now.
[02:09:14.680 --> 02:09:16.880]   And every time you walk by your keys, you get a notification.
[02:09:16.880 --> 02:09:17.880]   There's your keys.
[02:09:17.880 --> 02:09:18.880]   There's your keys.
[02:09:18.880 --> 02:09:19.880]   It really works.
[02:09:19.880 --> 02:09:23.720]   Tracker's one of the few Bluetooth trackers that has a replaceable battery.
[02:09:23.720 --> 02:09:26.400]   Even on the tiny Tracker pixel, that's huge.
[02:09:26.400 --> 02:09:29.160]   That means you don't have to throw it out when it wears out.
[02:09:29.160 --> 02:09:31.160]   You can keep it going for a long, long time.
[02:09:31.160 --> 02:09:32.160]   They're very affordable.
[02:09:32.160 --> 02:09:34.800]   They even have a 30-day money back guarantee.
[02:09:34.800 --> 02:09:36.040]   And we've got an even better deal.
[02:09:36.040 --> 02:09:41.960]   If you go to the Tracker.com/twit, you'll save 20% on any order.
[02:09:41.960 --> 02:09:43.040]   On any order.
[02:09:43.040 --> 02:09:45.800]   If you go load up on Tracker, get the Tracker Bravo.
[02:09:45.800 --> 02:09:48.600]   That's the Aluminized, Anidized, Aluminized.
[02:09:48.600 --> 02:09:50.520]   You can get that monogram.
[02:09:50.520 --> 02:09:52.720]   You can even print a picture on it.
[02:09:52.720 --> 02:09:54.280]   Or get the Tracker pixel, the new one.
[02:09:54.280 --> 02:09:57.920]   There's lots of accessories, lanyards and pitons and things.
[02:09:57.920 --> 02:10:01.720]   T-H-E-T-R-A-C-K-R, the Tracker.com/twit.
[02:10:01.720 --> 02:10:03.120]   20% off.
[02:10:03.120 --> 02:10:04.360]   Boom.
[02:10:04.360 --> 02:10:05.360]   Just like that.
[02:10:05.360 --> 02:10:08.040]   The Tracker.com/twit.
[02:10:08.040 --> 02:10:12.240]   I've used that code because I'm always buying more trackers because I'm always losing more
[02:10:12.240 --> 02:10:13.240]   stuff.
[02:10:13.240 --> 02:10:17.160]   Think of things all the time to put a Tracker on.
[02:10:17.160 --> 02:10:19.360]   Thank you, Tracker, for your support of this week in Tech.
[02:10:19.360 --> 02:10:21.440]   And thank you for a great panel.
[02:10:21.440 --> 02:10:22.440]   I get you.
[02:10:22.440 --> 02:10:23.440]   We still have so many stories.
[02:10:23.440 --> 02:10:24.440]   I don't--
[02:10:24.440 --> 02:10:28.960]   It was a really surprisingly busy week for the third week, second week of January.
[02:10:28.960 --> 02:10:29.960]   Yeah.
[02:10:29.960 --> 02:10:31.880]   Normally it's to be quiet because C.S. is over.
[02:10:31.880 --> 02:10:32.880]   Interesting.
[02:10:32.880 --> 02:10:35.200]   Amazon Prime monthly price going up 20%.
[02:10:35.200 --> 02:10:37.040]   Yeah, only if you do it by month.
[02:10:37.040 --> 02:10:38.040]   But if you--
[02:10:38.040 --> 02:10:40.040]   You're still 99 bucks if you do the year.
[02:10:40.040 --> 02:10:42.480]   Yeah, and if you're a student, I think it's 49.
[02:10:42.480 --> 02:10:44.040]   Why do they do the monthly?
[02:10:44.040 --> 02:10:46.400]   I think it's to discourage people from doing the monthly.
[02:10:46.400 --> 02:10:47.400]   Oh, yeah.
[02:10:47.400 --> 02:10:50.760]   Because I think what people do is sign up for it, buy a bunch of stuff, and then dump
[02:10:50.760 --> 02:10:51.760]   it.
[02:10:51.760 --> 02:10:52.760]   Of course.
[02:10:52.760 --> 02:10:54.760]   And they want you to sign up for the recurring annual.
[02:10:54.760 --> 02:10:55.760]   Duh.
[02:10:55.760 --> 02:10:56.760]   I think that's fine.
[02:10:56.760 --> 02:10:57.760]   You're absolutely right.
[02:10:57.760 --> 02:11:01.200]   Actually, more and more when I buy stuff, I buy a year because I don't like all those
[02:11:01.200 --> 02:11:03.160]   little charges every month.
[02:11:03.160 --> 02:11:04.160]   Sure.
[02:11:04.160 --> 02:11:05.720]   It's like they always give you a break.
[02:11:05.720 --> 02:11:06.720]   Yeah.
[02:11:06.720 --> 02:11:07.720]   I figure I'm going to use this.
[02:11:07.720 --> 02:11:08.720]   And I don't.
[02:11:08.720 --> 02:11:12.000]   That's a good way to get my money, I guess.
[02:11:12.000 --> 02:11:19.600]   Amazon has narrowed its HQ search down to 20 cities, 19 in the US, and Toronto.
[02:11:19.600 --> 02:11:21.400]   That'd be interesting.
[02:11:21.400 --> 02:11:22.400]   My fingers are crossed.
[02:11:22.400 --> 02:11:26.200]   Yeah, they've not known for their international presence.
[02:11:26.200 --> 02:11:29.800]   So I think-- actually, I'm rooting for Toronto.
[02:11:29.800 --> 02:11:30.800]   That would be really great.
[02:11:30.800 --> 02:11:32.280]   Toronto could be cool.
[02:11:32.280 --> 02:11:37.520]   I was fascinated by the number of sort of-- this wasn't as interesting as I had hoped
[02:11:37.520 --> 02:11:38.520]   in New York City.
[02:11:38.520 --> 02:11:39.520]   New York City?
[02:11:39.520 --> 02:11:42.520]   Yeah, New York, Austin, Los Angeles, Dallas.
[02:11:42.520 --> 02:11:43.520]   It's the obvious.
[02:11:43.520 --> 02:11:44.520]   Yeah.
[02:11:44.520 --> 02:11:48.360]   Miami was interesting to me just because of climate change.
[02:11:48.360 --> 02:11:51.000]   Miami seems like it would be a pretty risky place to put big animals.
[02:11:51.000 --> 02:11:52.000]   That's what, yeah.
[02:11:52.000 --> 02:11:53.320]   I was thinking the same thing.
[02:11:53.320 --> 02:11:56.400]   They say Miami's going to be underwater in a few decades.
[02:11:56.400 --> 02:11:57.400]   Exactly.
[02:11:57.400 --> 02:11:59.000]   So I'm ruling out Miami right now.
[02:11:59.000 --> 02:12:00.000]   Yeah.
[02:12:00.000 --> 02:12:04.320]   I kind of was thinking of retiring to Key Biscayne, which is in Miami.
[02:12:04.320 --> 02:12:05.320]   Just stilts.
[02:12:05.320 --> 02:12:07.480]   Yeah, well, I just figured I'd build a higher house.
[02:12:07.480 --> 02:12:12.640]   Or you buy inland a little bit and becomes waterfront property down the road.
[02:12:12.640 --> 02:12:13.640]   Sure.
[02:12:13.640 --> 02:12:15.280]   Is that crazy?
[02:12:15.280 --> 02:12:18.760]   You just have to figure out how much ice is going to melt.
[02:12:18.760 --> 02:12:22.480]   YouTube has changed its policy for monetization.
[02:12:22.480 --> 02:12:27.400]   You now have to have 4,000 views a month and 1,000 subscribers.
[02:12:27.400 --> 02:12:29.400]   I guess that's OK.
[02:12:29.400 --> 02:12:32.920]   It sounds like, though, this is going to hurt a lot of new, young and up and coming.
[02:12:32.920 --> 02:12:33.920]   People aren't going to be able to monetize.
[02:12:33.920 --> 02:12:37.520]   Well, the logic, if I understood it correctly, was really weird to me.
[02:12:37.520 --> 02:12:40.000]   It was like, this is how we're going to keep hate speech off.
[02:12:40.000 --> 02:12:41.680]   Yeah, that I understand at all.
[02:12:41.680 --> 02:12:47.080]   It was sort of, I guess, the implication was that people who use YouTube for hate-related
[02:12:47.080 --> 02:12:49.360]   discourse are very popular.
[02:12:49.360 --> 02:12:52.480]   They just don't do very well.
[02:12:52.480 --> 02:12:53.480]   But how much money can you make?
[02:12:53.480 --> 02:12:54.480]   I don't think I really get it.
[02:12:54.480 --> 02:12:59.760]   If you only have 1,000 subscribers or less and you have few views, you're not making
[02:12:59.760 --> 02:13:00.760]   any money anyway.
[02:13:00.760 --> 02:13:01.760]   What is the deal?
[02:13:01.760 --> 02:13:02.760]   Yeah.
[02:13:02.760 --> 02:13:05.640]   I'm not sure it does seem like it is going to do anything.
[02:13:05.640 --> 02:13:07.480]   It seems like a very blunt, forced way to do that.
[02:13:07.480 --> 02:13:08.480]   Oh, sorry.
[02:13:08.480 --> 02:13:09.480]   Hate speech.
[02:13:09.480 --> 02:13:11.880]   We're not going to give you your $0.23 in ad revenue this month.
[02:13:11.880 --> 02:13:12.880]   Yeah, I'm not.
[02:13:12.880 --> 02:13:13.880]   I don't get it.
[02:13:13.880 --> 02:13:15.120]   I don't get it.
[02:13:15.120 --> 02:13:16.120]   You know what?
[02:13:16.120 --> 02:13:19.040]   At this point, they want to-- they need a PR win.
[02:13:19.040 --> 02:13:21.440]   It's just going to throw stuff at the wall.
[02:13:21.440 --> 02:13:23.320]   Bigger stuff going on at Facebook.
[02:13:23.320 --> 02:13:25.960]   And this probably we could have spent a long time discussing that.
[02:13:25.960 --> 02:13:27.400]   We don't need to get into it.
[02:13:27.400 --> 02:13:31.320]   But as you know, they said they're going to de-emphasize brands.
[02:13:31.320 --> 02:13:33.680]   They're going to re-emphasize friends.
[02:13:33.680 --> 02:13:41.320]   And their newest thing is they're going to let the users decide what's-- they're basically
[02:13:41.320 --> 02:13:42.880]   going to be Yelp for news.
[02:13:42.880 --> 02:13:46.480]   They're going to let the users decide what's high quality news.
[02:13:46.480 --> 02:13:49.480]   Yeah, so this is so strange to me.
[02:13:49.480 --> 02:13:52.760]   It's like to fight fake news.
[02:13:52.760 --> 02:13:56.120]   We're going to let everybody tell us what they think is.
[02:13:56.120 --> 02:13:58.960]   The people who post fake news, was that fake news?
[02:13:58.960 --> 02:13:59.960]   And then decide.
[02:13:59.960 --> 02:14:00.960]   Yeah.
[02:14:00.960 --> 02:14:01.960]   OK, now decide.
[02:14:01.960 --> 02:14:03.240]   So they basically--
[02:14:03.240 --> 02:14:08.240]   Zach basically said, we thought about policing it ourselves, but we thought that that was
[02:14:08.240 --> 02:14:09.240]   obviously problematic.
[02:14:09.240 --> 02:14:11.080]   So then we thought to go into a panel of experts.
[02:14:11.080 --> 02:14:12.080]   But that seems biased.
[02:14:12.080 --> 02:14:15.360]   So instead, we're going to you and we're letting the general public decide what is a
[02:14:15.360 --> 02:14:16.840]   trust whether a news source.
[02:14:16.840 --> 02:14:21.600]   And the thing that's so upsetting about this to me is that obviously politicians on both
[02:14:21.600 --> 02:14:26.200]   sides have an incentive to discredit press.
[02:14:26.200 --> 02:14:27.200]   Right.
[02:14:27.200 --> 02:14:30.800]   And that gives them even more of an incentive to discredit press.
[02:14:30.800 --> 02:14:35.960]   So that their constituents will then essentially downvote the press they don't like.
[02:14:35.960 --> 02:14:42.120]   Good article on BuzzFeed saying, "Crowdsourced ratings work for toasters, but not news."
[02:14:42.120 --> 02:14:44.560]   And they have the data to back it up.
[02:14:44.560 --> 02:14:50.600]   Well, and it took a very long time for Amazon to effectively deal with user reviews.
[02:14:50.600 --> 02:14:51.600]   It's still not a part of this.
[02:14:51.600 --> 02:14:52.600]   Right.
[02:14:52.600 --> 02:14:53.600]   And look at Yelp.
[02:14:53.600 --> 02:14:55.200]   I mean, it's almost impossible.
[02:14:55.200 --> 02:15:03.760]   So the authors of this Alan Dennis from BuzzFeed and Tino Kim and Tricia Moravec, apparently
[02:15:03.760 --> 02:15:09.760]   they say there's a research paper they did entitled, "Bine the Stars the Effects of News
[02:15:09.760 --> 02:15:15.440]   Source Ratings on Fake News and Social Media" deals exactly with this particular technique.
[02:15:15.440 --> 02:15:19.560]   Research shows users don't trust other users.
[02:15:19.560 --> 02:15:26.440]   So they studied 590 Facebook users in the United States and they believe that expert ratings
[02:15:26.440 --> 02:15:29.760]   would be more credible than user ratings because experts are more likely to be objective and
[02:15:29.760 --> 02:15:30.760]   check the facts.
[02:15:30.760 --> 02:15:32.760]   Of course they are.
[02:15:32.760 --> 02:15:36.120]   Anyway, user ratings easily manipulated really?
[02:15:36.120 --> 02:15:37.120]   Really?
[02:15:37.120 --> 02:15:40.000]   Is that happen ever?
[02:15:40.000 --> 02:15:42.960]   You know, Facebook's fun to watch.
[02:15:42.960 --> 02:15:44.400]   And look for your email from Twitter.
[02:15:44.400 --> 02:15:51.080]   They're going to email 677,775 people in the US who followed, retweeted or liked to tweet
[02:15:51.080 --> 02:15:58.400]   from a Russian government agency posing as something else, like the internet trolls of
[02:15:58.400 --> 02:16:00.840]   the internet research agency.
[02:16:00.840 --> 02:16:02.360]   I'm looking for- I hope I get that email.
[02:16:02.360 --> 02:16:04.360]   I kind of want it too.
[02:16:04.360 --> 02:16:05.360]   I want it.
[02:16:05.360 --> 02:16:06.360]   I just, yeah.
[02:16:06.360 --> 02:16:07.360]   If you don't get it, then you know that you were good.
[02:16:07.360 --> 02:16:09.760]   You didn't retweet, retweet, follow.
[02:16:09.760 --> 02:16:10.760]   I just want to know what that's actually for now.
[02:16:10.760 --> 02:16:11.760]   Give it to people that get this.
[02:16:11.760 --> 02:16:12.760]   Will they really care?
[02:16:12.760 --> 02:16:13.760]   No.
[02:16:13.760 --> 02:16:14.760]   Yeah, is there like, tweet shaving?
[02:16:14.760 --> 02:16:15.760]   And-
[02:16:15.760 --> 02:16:16.760]   What's the intent here?
[02:16:16.760 --> 02:16:17.760]   Yes, exactly.
[02:16:17.760 --> 02:16:18.760]   What's the intent?
[02:16:18.760 --> 02:16:19.760]   No.
[02:16:19.760 --> 02:16:20.760]   Why does it-
[02:16:20.760 --> 02:16:21.760]   I can't-
[02:16:21.760 --> 02:16:25.520]   I think that a lot of what happened with the actual fake news, the sort of Russia-based
[02:16:25.520 --> 02:16:34.360]   propaganda news, was that it was actually used to inflame people who found the news upsetting.
[02:16:34.360 --> 02:16:40.200]   So not so much like, hey, look what this person is doing in politics, don't vote for
[02:16:40.200 --> 02:16:41.200]   them.
[02:16:41.200 --> 02:16:44.800]   It's just like putting into people's feeds.
[02:16:44.800 --> 02:16:48.520]   So if you're extremely liberal, putting something inflamatory conservative in your
[02:16:48.520 --> 02:16:51.360]   feed so that you get even more upset and polarized.
[02:16:51.360 --> 02:16:52.360]   Right.
[02:16:52.360 --> 02:16:54.960]   So people may have been manipulated in ways that they didn't quite understand.
[02:16:54.960 --> 02:16:57.320]   They're pushing your buttons is what they're doing.
[02:16:57.320 --> 02:16:58.320]   Yeah.
[02:16:58.320 --> 02:16:59.320]   That's what truth is.
[02:16:59.320 --> 02:17:00.320]   Yeah.
[02:17:00.320 --> 02:17:03.140]   I think we got a rapid, I'm sorry to say, I hate to rap it because we're having so much
[02:17:03.140 --> 02:17:06.520]   fun, but you and you guys are so great and there's so much more to talk about, but you
[02:17:06.520 --> 02:17:08.440]   know, all good things must come to an end.
[02:17:08.440 --> 02:17:11.880]   I have to go home and post my face everywhere.
[02:17:11.880 --> 02:17:14.880]   Post your therapy sessions.
[02:17:14.880 --> 02:17:15.880]   Yes.
[02:17:15.880 --> 02:17:19.880]   Oh, kind of go down and get those out of the basement, got the tapes, put them all up
[02:17:19.880 --> 02:17:20.880]   online.
[02:17:20.880 --> 02:17:22.160]   And then I'm going to ask Georgia what she thinks.
[02:17:22.160 --> 02:17:23.480]   Georgia Dow always a pleasure.
[02:17:23.480 --> 02:17:24.800]   I'm so thrilled to have you.
[02:17:24.800 --> 02:17:30.840]   Senior editor, imore.com at Georgia underscore Dowanxiety-videos.com.
[02:17:30.840 --> 02:17:33.480]   We didn't manage to put anything in your blue room.
[02:17:33.480 --> 02:17:34.960]   No, no fish.
[02:17:34.960 --> 02:17:35.960]   I was disappointed.
[02:17:35.960 --> 02:17:36.960]   It's time.
[02:17:36.960 --> 02:17:38.960]   Maybe we'll do it in post.
[02:17:38.960 --> 02:17:39.960]   In post?
[02:17:39.960 --> 02:17:41.680]   I won't even notice them.
[02:17:41.680 --> 02:17:42.680]   You'll never know.
[02:17:42.680 --> 02:17:46.360]   I'll just have to do video of me grabbing stuff so that it then makes sense.
[02:17:46.360 --> 02:17:48.720]   Maybe I shouldn't because that might be inappropriate.
[02:17:48.720 --> 02:17:53.000]   Yeah, or I should have randomly just said, hey, love the place, Georgia, or hey, watch
[02:17:53.000 --> 02:17:54.560]   out for that lion and it.
[02:17:54.560 --> 02:17:55.560]   Right, right.
[02:17:55.560 --> 02:17:59.280]   And then we have to like the saw to be like, like, ducking.
[02:17:59.280 --> 02:18:00.280]   It's great to have you.
[02:18:00.280 --> 02:18:01.280]   Did you move?
[02:18:01.280 --> 02:18:02.520]   I did move.
[02:18:02.520 --> 02:18:03.520]   I did move.
[02:18:03.520 --> 02:18:04.520]   I'm now in the hat.
[02:18:04.520 --> 02:18:05.520]   Like there's boxes everywhere.
[02:18:05.520 --> 02:18:08.320]   But I moved into the place.
[02:18:08.320 --> 02:18:09.320]   Yeah, I do.
[02:18:09.320 --> 02:18:10.320]   I do.
[02:18:10.320 --> 02:18:13.400]   Is this one of two VR rooms that you will have in the new place?
[02:18:13.400 --> 02:18:15.040]   No, this one's just my office.
[02:18:15.040 --> 02:18:21.760]   I'm next door to the VR areas and the movie room is in between.
[02:18:21.760 --> 02:18:27.120]   So we have the chairs, the movie chairs that are in between that separate the two VR areas.
[02:18:27.120 --> 02:18:28.680]   So it works out pretty well.
[02:18:28.680 --> 02:18:30.160]   I want that house.
[02:18:30.160 --> 02:18:32.880]   She's got a man cave in her house.
[02:18:32.880 --> 02:18:33.880]   I...
[02:18:33.880 --> 02:18:36.080]   A woman cave, I guess.
[02:18:36.080 --> 02:18:37.080]   Person cave.
[02:18:37.080 --> 02:18:38.080]   I like it too.
[02:18:38.080 --> 02:18:39.080]   We both like it.
[02:18:39.080 --> 02:18:40.080]   It's really cool.
[02:18:40.080 --> 02:18:42.520]   My hubby does all of the setting up of that.
[02:18:42.520 --> 02:18:44.320]   So they like...
[02:18:44.320 --> 02:18:48.320]   For those who don't know, they like VR so much, they didn't want to fight over who gets to
[02:18:48.320 --> 02:18:49.320]   use the VR.
[02:18:49.320 --> 02:18:50.320]   So they have...
[02:18:50.320 --> 02:18:51.320]   Stereo VR.
[02:18:51.320 --> 02:18:52.320]   They have their own VR rooms.
[02:18:52.320 --> 02:18:53.320]   We VR together.
[02:18:53.320 --> 02:18:54.920]   It's a bonding experience.
[02:18:54.920 --> 02:18:55.920]   Really?
[02:18:55.920 --> 02:18:58.560]   They're moving in their early 90s about that.
[02:18:58.560 --> 02:19:00.600]   That's Stephen King saying a thing.
[02:19:00.600 --> 02:19:02.600]   This is Facebook of the future.
[02:19:02.600 --> 02:19:03.600]   That's true.
[02:19:03.600 --> 02:19:04.600]   This is where we go.
[02:19:04.600 --> 02:19:05.600]   Red round.
[02:19:05.600 --> 02:19:06.600]   Red round.
[02:19:06.600 --> 02:19:07.600]   That is Tim's...
[02:19:07.600 --> 02:19:08.600]   Not that one.
[02:19:08.600 --> 02:19:09.600]   Okay.
[02:19:09.600 --> 02:19:10.600]   That is Tim.
[02:19:10.600 --> 02:19:11.600]   No, he wasn't.
[02:19:11.600 --> 02:19:12.600]   No, no, no.
[02:19:12.600 --> 02:19:13.600]   Congratulations.
[02:19:13.600 --> 02:19:18.440]   I can't believe earlier today this man was driving a Subaru on a frozen lake and came in
[02:19:18.440 --> 02:19:20.200]   and started talking to us about...
[02:19:20.200 --> 02:19:21.200]   And God's second place.
[02:19:21.200 --> 02:19:23.000]   And God's second place.
[02:19:23.000 --> 02:19:24.000]   And round one.
[02:19:24.000 --> 02:19:25.000]   Where's the metal?
[02:19:25.000 --> 02:19:26.000]   We won't have to see the metal.
[02:19:26.000 --> 02:19:27.000]   It should be behind you.
[02:19:27.000 --> 02:19:29.200]   Oh, they're all in the garage showroom.
[02:19:29.200 --> 02:19:30.200]   I'll bring him next time.
[02:19:30.200 --> 02:19:31.200]   Yeah, what are you doing?
[02:19:31.200 --> 02:19:32.200]   Next time.
[02:19:32.200 --> 02:19:36.040]   All of a sudden have trophies all around behind them.
[02:19:36.040 --> 02:19:37.040]   I stick him in the garage.
[02:19:37.040 --> 02:19:38.040]   Oh, he's so humble.
[02:19:38.040 --> 02:19:39.040]   Oh.
[02:19:39.040 --> 02:19:40.040]   So you should put them...
[02:19:40.040 --> 02:19:42.600]   Yeah, you've got to clear one of those cubbies.
[02:19:42.600 --> 02:19:43.600]   Yeah.
[02:19:43.600 --> 02:19:46.160]   That's how my toys are though.
[02:19:46.160 --> 02:19:47.160]   Trophies are toys too.
[02:19:47.160 --> 02:19:48.160]   It's just ways too.
[02:19:48.160 --> 02:19:49.160]   I like that.
[02:19:49.160 --> 02:19:50.160]   This is why we love Tim.
[02:19:50.160 --> 02:19:51.160]   He's very unassuming.
[02:19:51.160 --> 02:19:52.160]   He's very humble.
[02:19:52.160 --> 02:19:56.560]   Editor at large for a road show, a CNET production.
[02:19:56.560 --> 02:19:57.960]   Great to have you here.
[02:19:57.960 --> 02:20:01.440]   And of course, is Lindsay your boss?
[02:20:01.440 --> 02:20:02.440]   She is my boss.
[02:20:02.440 --> 02:20:06.320]   So hopefully I've been on my best behavior all the past two and a half hours now.
[02:20:06.320 --> 02:20:08.240]   And you'll be sending in your grades later.
[02:20:08.240 --> 02:20:10.480]   I'm going forward to that being a little bit.
[02:20:10.480 --> 02:20:12.360]   Great to have Lindsay for a boss.
[02:20:12.360 --> 02:20:13.360]   My gosh.
[02:20:13.360 --> 02:20:14.360]   It is pretty great.
[02:20:14.360 --> 02:20:15.360]   It's pretty great.
[02:20:15.360 --> 02:20:16.360]   I need some CNETs.
[02:20:16.360 --> 02:20:19.840]   It's great to have you on the show and get you a studio.
[02:20:19.840 --> 02:20:21.000]   It was really fun.
[02:20:21.000 --> 02:20:22.080]   I feel privileged.
[02:20:22.080 --> 02:20:25.760]   We do this show every Sunday afternoon with the best people in tech.
[02:20:25.760 --> 02:20:26.760]   It's so much fun.
[02:20:26.760 --> 02:20:29.240]   It's really like a coffee clatch.
[02:20:29.240 --> 02:20:33.080]   One of these days I'm going to get a pickle barrel, a checkerboard.
[02:20:33.080 --> 02:20:34.080]   I don't know.
[02:20:34.080 --> 02:20:35.080]   An old dog named Blue.
[02:20:35.080 --> 02:20:36.080]   I don't know.
[02:20:36.080 --> 02:20:38.080]   I don't know where we're going with this one.
[02:20:38.080 --> 02:20:39.080]   I don't know.
[02:20:39.080 --> 02:20:40.080]   You feel like we're hanging out on the porch.
[02:20:40.080 --> 02:20:41.080]   Yeah.
[02:20:41.080 --> 02:20:42.080]   Pickle barrel.
[02:20:42.080 --> 02:20:44.520]   You know, you just sit around the pickle barrel, smoke your corn cob pipe.
[02:20:44.520 --> 02:20:45.520]   Did I know of?
[02:20:45.520 --> 02:20:46.520]   We need rocking chairs.
[02:20:46.520 --> 02:20:47.520]   Just go, yeah.
[02:20:47.520 --> 02:20:48.520]   What about that Google?
[02:20:48.520 --> 02:20:49.520]   Oh, yeah.
[02:20:49.520 --> 02:20:50.520]   Again, aren't they?
[02:20:50.520 --> 02:20:53.520]   Remember when this little show is in VR?
[02:20:53.520 --> 02:20:54.520]   That's privacy.
[02:20:54.520 --> 02:20:55.520]   Yeah.
[02:20:55.520 --> 02:20:56.520]   What about that privacy?
[02:20:56.520 --> 02:20:58.920]   Leo, I can't believe what I saw.
[02:20:58.920 --> 02:21:00.520]   You were doing earlier today.
[02:21:00.520 --> 02:21:02.160]   What?
[02:21:02.160 --> 02:21:08.280]   We do this show in public every Sunday, 3 p.m. Pacific, 6 p.m. Eastern, 2300 UTC.
[02:21:08.280 --> 02:21:10.680]   We love it if you watch live, twit.tv/live.
[02:21:10.680 --> 02:21:15.160]   But if you do, please join us in the chat room so we can get your feedback and your comments
[02:21:15.160 --> 02:21:17.080]   and frankly so I can steal your jokes.
[02:21:17.080 --> 02:21:18.080]   That's IRC.
[02:21:18.080 --> 02:21:19.080]   I do.
[02:21:19.080 --> 02:21:20.080]   No, seriously.
[02:21:20.080 --> 02:21:22.080]   These guys are my joke writers.
[02:21:22.080 --> 02:21:24.880]   IRC.twit.tv.
[02:21:24.880 --> 02:21:26.440]   We also love it if you're in studio.
[02:21:26.440 --> 02:21:29.760]   We had some nice studio audience visiting us today.
[02:21:29.760 --> 02:21:31.520]   Thank you guys.
[02:21:31.520 --> 02:21:33.360]   If you want to be in studio, do let us know.
[02:21:33.360 --> 02:21:37.600]   We don't like surprises.
[02:21:37.600 --> 02:21:39.240]   Tickets at twit.tv.
[02:21:39.240 --> 02:21:42.440]   It sounds so paranoid, but that's what they asked me to say.
[02:21:42.440 --> 02:21:43.440]   Tickets at twit.tv.
[02:21:43.440 --> 02:21:46.040]   That's why we put a chair out for you.
[02:21:46.040 --> 02:21:49.600]   We had a visitor today from Silicon Valley and from Honolulu.
[02:21:49.600 --> 02:21:51.760]   It's so nice to have you both.
[02:21:51.760 --> 02:21:55.280]   We also want you to, if you would, take our survey.
[02:21:55.280 --> 02:21:56.280]   We do this once a year.
[02:21:56.280 --> 02:21:58.400]   It's that time again, beginning of the year.
[02:21:58.400 --> 02:21:59.400]   Twit.tv/survey.
[02:21:59.400 --> 02:22:00.400]   It's our audience survey.
[02:22:00.400 --> 02:22:01.760]   Just a few, I don't know.
[02:22:01.760 --> 02:22:03.000]   It's about four minutes, five minutes.
[02:22:03.000 --> 02:22:05.080]   Just a few questions about you.
[02:22:05.080 --> 02:22:08.640]   Help us understand better what you listen to, what you like, what you don't like.
[02:22:08.640 --> 02:22:10.040]   And also, I'll be frank.
[02:22:10.040 --> 02:22:14.640]   It helps us talk about you to advertisers because we don't collect information any other way
[02:22:14.640 --> 02:22:15.640]   about you.
[02:22:15.640 --> 02:22:18.200]   So we don't, we respect your privacy.
[02:22:18.200 --> 02:22:21.160]   Regardless of my opinion about my privacy, we respect your privacy.
[02:22:21.160 --> 02:22:23.160]   We don't collect it in any way.
[02:22:23.160 --> 02:22:28.040]   We won't use your email address if you provide it in the form.
[02:22:28.040 --> 02:22:29.480]   But it was very helpful for us.
[02:22:29.480 --> 02:22:33.880]   Twit.tv/survey would do this once a year just to help us a little bit understand our
[02:22:33.880 --> 02:22:34.880]   audience.
[02:22:34.880 --> 02:22:37.400]   So I appreciate you for doing that.
[02:22:37.400 --> 02:22:38.880]   I think there's nothing more to say.
[02:22:38.880 --> 02:22:43.040]   Oh, well, I should say if you can't be here in person or on the stream, you can always
[02:22:43.040 --> 02:22:46.560]   download a copy of the show from twit.tv.
[02:22:46.560 --> 02:22:52.560]   More shows available in video and audio, on demand, a ton of stuff.
[02:22:52.560 --> 02:22:55.000]   Please subscribe to the shows you like the best.
[02:22:55.000 --> 02:22:56.000]   That's a good signal for us.
[02:22:56.000 --> 02:22:59.840]   When we see somebody has a subscription that gives us more information about the stuff
[02:22:59.840 --> 02:23:00.840]   you want to see more of.
[02:23:00.840 --> 02:23:03.160]   So that's good to have.
[02:23:03.160 --> 02:23:06.240]   Subscribe at twit.tv or on your favorite podcast application.
[02:23:06.240 --> 02:23:11.240]   Wherever you watch, we'll know because we're watching you every moment.
[02:23:11.240 --> 02:23:12.240]   No.
[02:23:12.240 --> 02:23:14.240]   It's not creepy.
[02:23:14.240 --> 02:23:15.240]   That's not creepy.
[02:23:15.240 --> 02:23:16.240]   No, out of it.
[02:23:16.240 --> 02:23:17.240]   I know what your face looks like.
[02:23:17.240 --> 02:23:18.240]   No, I don't.
[02:23:18.240 --> 02:23:19.240]   I don't.
[02:23:19.240 --> 02:23:22.680]   Unless you posted it on Instagram, then I might.
[02:23:22.680 --> 02:23:24.680]   Thanks for being here.
[02:23:24.680 --> 02:23:25.680]   And I'm following you.
[02:23:25.680 --> 02:23:27.160]   Yay, see, I don't know.
[02:23:27.160 --> 02:23:28.160]   Thanks for being here.
[02:23:28.160 --> 02:23:29.160]   We'll see you next time.
[02:23:29.160 --> 02:23:30.160]   Another twit.
[02:23:30.160 --> 02:23:31.160]   It's in the can.
[02:23:31.160 --> 02:23:32.160]   Do the twit.
[02:23:32.160 --> 02:23:33.160]   All right.
[02:23:33.160 --> 02:23:34.160]   Do the twit, baby.
[02:23:34.160 --> 02:23:35.160]   Do the twit.
[02:23:35.160 --> 02:23:36.160]   All right.
[02:23:36.160 --> 02:23:37.160]   Do the twit.
[02:23:37.160 --> 02:23:38.160]   All right.
[02:23:38.160 --> 02:23:38.160]   Do the twit.
[02:23:38.160 --> 02:23:39.160]   Alright.
[02:23:39.160 --> 02:23:40.160]   You in the twins.

