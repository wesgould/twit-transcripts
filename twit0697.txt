
[00:00:00.000 --> 00:00:04.300]   It's time for Twit this weekend Tech.
[00:00:04.300 --> 00:00:05.300]   What a great panel this week.
[00:00:05.300 --> 00:00:09.800]   Harry McCracken, the technologizer's here from Wired Magazine, the great Paris Martino
[00:00:09.800 --> 00:00:12.800]   Michael Nunez from Mashable.
[00:00:12.800 --> 00:00:19.360]   We'll start off by talking about Elon Musk, then self-propagating negativity, the amorphous
[00:00:19.360 --> 00:00:28.080]   ball of rage in France, the super micro audit was Bloomberg completely wrong in YouTube,
[00:00:28.080 --> 00:00:32.000]   and the most hated video of all time.
[00:00:32.000 --> 00:00:35.080]   It's all coming up next on Twit.
[00:00:35.080 --> 00:00:38.960]   Netcast you love.
[00:00:38.960 --> 00:00:44.280]   From people you trust.
[00:00:44.280 --> 00:00:52.360]   This is Twit.
[00:00:52.360 --> 00:00:59.680]   This is Twit this week at Tech, Episode 697, recorded Sunday, December 16, 2018.
[00:00:59.680 --> 00:01:03.000]   The Big Leak Kabal.
[00:01:03.000 --> 00:01:05.840]   This week at Tech is brought to you by Avnet.
[00:01:05.840 --> 00:01:11.080]   Avnet and non-impossible labs created an historic event that the life is beautiful music festival,
[00:01:11.080 --> 00:01:15.720]   a first of its kind, a live concert that helped the deaf and hearing communities experience
[00:01:15.720 --> 00:01:18.480]   music together in a whole new way.
[00:01:18.480 --> 00:01:21.880]   Visit Avnet.com/musicone to see the journey.
[00:01:21.880 --> 00:01:25.000]   And by Blue Apron.
[00:01:25.000 --> 00:01:27.480]   Blue Apron makes cooking fun and easy.
[00:01:27.480 --> 00:01:33.000]   Check out this week's menu and get your first three meals free at Blue Apron.com/Twit.
[00:01:33.000 --> 00:01:37.480]   And by Rocket Mortgage from Quick and Loans, introducing rate shield approval if you're
[00:01:37.480 --> 00:01:39.040]   in the market to buy a home.
[00:01:39.040 --> 00:01:42.400]   Rate shield approval locks up your rate for up to 90 days while you shop.
[00:01:42.400 --> 00:01:44.240]   It's a real game changer.
[00:01:44.240 --> 00:01:48.920]   Learn more and get started at rocketmortgage.com/Twit2.
[00:01:48.920 --> 00:01:50.400]   And by Molecule.
[00:01:50.400 --> 00:01:55.360]   Molecule is the world's first molecular air purifier that reduces symptoms for allergy
[00:01:55.360 --> 00:01:56.760]   and asthma sufferers.
[00:01:56.760 --> 00:02:07.040]   For $75 off your first order, visit Molecule.com and enter the promo code TWIT75.
[00:02:07.040 --> 00:02:10.080]   It's time for Twit this week in Tech to show what we get together with the best minds and
[00:02:10.080 --> 00:02:16.680]   technology to try to figure out what the heck is going on in this weekend.
[00:02:16.680 --> 00:02:17.680]   Tech.
[00:02:17.680 --> 00:02:22.680]   Congratulations with me.
[00:02:22.680 --> 00:02:24.680]   It's been Studio Harry McCracken.
[00:02:24.680 --> 00:02:25.680]   Thank you Harry.
[00:02:25.680 --> 00:02:26.680]   Great to be here.
[00:02:26.680 --> 00:02:27.680]   You came up in the rain.
[00:02:27.680 --> 00:02:28.680]   Marie is here.
[00:02:28.680 --> 00:02:29.680]   I think I saw her briefly.
[00:02:29.680 --> 00:02:30.680]   She's wandering your hallway as I think.
[00:02:30.680 --> 00:02:31.680]   Don't.
[00:02:31.680 --> 00:02:32.680]   She can borrow my bike if she wants.
[00:02:32.680 --> 00:02:33.680]   You see my e-bike?
[00:02:33.680 --> 00:02:34.680]   Do you like that?
[00:02:34.680 --> 00:02:37.560]   The Super 73 came.
[00:02:37.560 --> 00:02:40.400]   It's not a bike, it's got pedals.
[00:02:40.400 --> 00:02:44.920]   But what it really is is an electric moped that's disguised as a bicycle.
[00:02:44.920 --> 00:02:48.640]   You can't see it because of where it is, but it's Harry can see it.
[00:02:48.640 --> 00:02:50.600]   It's really fun.
[00:02:50.600 --> 00:02:52.840]   Also with us, we've got to welcome him back.
[00:02:52.840 --> 00:02:54.240]   We haven't seen Michael in a while.
[00:02:54.240 --> 00:03:00.160]   Michael Nunez, Deputy Tech Editor at Mashable, in the Mashable offices in beautiful Manhattan.
[00:03:00.160 --> 00:03:01.160]   Hello Michael.
[00:03:01.160 --> 00:03:02.560]   Hey, thanks for having me.
[00:03:02.560 --> 00:03:03.560]   Good to see you.
[00:03:03.560 --> 00:03:08.200]   We want to welcome a new member of the panel, but I'm thrilled to have her because her
[00:03:08.200 --> 00:03:13.680]   name has come up many times in the last months on Twit.
[00:03:13.680 --> 00:03:15.040]   She's a staff writer at Wired.
[00:03:15.040 --> 00:03:16.040]   Paris Martin, though.
[00:03:16.040 --> 00:03:21.200]   Remember that lashify story we were talking about, the Instagram influencers?
[00:03:21.200 --> 00:03:22.200]   That's Paris.
[00:03:22.200 --> 00:03:23.480]   She's done great work.
[00:03:23.480 --> 00:03:27.080]   If you just look at her stuff on Wired, it's great to have you.
[00:03:27.080 --> 00:03:29.080]   Thank you for joining us.
[00:03:29.080 --> 00:03:30.080]   I don't know.
[00:03:30.080 --> 00:03:31.880]   I almost don't know where to begin.
[00:03:31.880 --> 00:03:36.080]   There's so many boring stories in the week.
[00:03:36.080 --> 00:03:39.440]   I could start with the crazy story of Elon Musk.
[00:03:39.440 --> 00:03:40.720]   Oh, let's do it.
[00:03:40.720 --> 00:03:43.640]   It's a Wired story.
[00:03:43.640 --> 00:03:46.440]   This is not yours, I should say.
[00:03:46.440 --> 00:03:49.320]   Charles Duhigg, who also does a great job.
[00:03:49.320 --> 00:03:55.400]   Dr. Elon and Mr. Musk, life inside Tesla's production hell.
[00:03:55.400 --> 00:03:56.600]   I'm a fan of Tesla.
[00:03:56.600 --> 00:03:58.680]   I drive a Model X.
[00:03:58.680 --> 00:04:03.920]   But now after reading this, I'm really second-guessing whether I should buy another Tesla.
[00:04:03.920 --> 00:04:07.880]   He sounds worse than Steve Jobs, if that's possible.
[00:04:07.880 --> 00:04:12.280]   He sounds the same in person as he is on Twitter, which isn't always true of everybody.
[00:04:12.280 --> 00:04:13.280]   Yes.
[00:04:13.280 --> 00:04:15.200]   Crazy Elon on Twitter?
[00:04:15.200 --> 00:04:17.360]   He's like that.
[00:04:17.360 --> 00:04:26.160]   Duhigg's talking about the Gigafactory and problems of the Gigafactory.
[00:04:26.160 --> 00:04:29.800]   About 10 o'clock on a Saturday evening, an angry Musk was examining one of the production
[00:04:29.800 --> 00:04:33.480]   lines mechanized modules trying to figure out what was wrong.
[00:04:33.480 --> 00:04:37.280]   When the young excited engineer that he talks about earlier in the article was brought over
[00:04:37.280 --> 00:04:39.120]   to assist him, the guy was really excited.
[00:04:39.120 --> 00:04:46.160]   I'm going to meet Elon, I'm here, I'm working at the Gigafactory.
[00:04:46.160 --> 00:04:49.560]   He'd been living out of a suitcase, putting in 13-hour days, seven days a week over the
[00:04:49.560 --> 00:04:51.520]   last year was his first real job.
[00:04:51.520 --> 00:04:55.680]   And now a colleague had tracked him down to say, "Elon Musk wanted his help."
[00:04:55.680 --> 00:04:58.360]   So he came running over.
[00:04:58.360 --> 00:04:59.520]   Musk shouts at him, "Hey, buddy."
[00:04:59.520 --> 00:05:02.040]   He doesn't even know his name.
[00:05:02.040 --> 00:05:03.040]   This doesn't work.
[00:05:03.040 --> 00:05:06.280]   Musk shouted at the engineer, according to someone who heard the conversation, "Did you
[00:05:06.280 --> 00:05:07.680]   do this?"
[00:05:07.680 --> 00:05:08.680]   The engineer was taking it back.
[00:05:08.680 --> 00:05:10.760]   He never met Musk before.
[00:05:10.760 --> 00:05:12.040]   Musk didn't even know the engineer's name.
[00:05:12.040 --> 00:05:16.000]   The young man wasn't certain what exactly Musk was asking him.
[00:05:16.000 --> 00:05:17.480]   Or why he sounded so angry.
[00:05:17.480 --> 00:05:22.080]   You mean, program the robot the engineer said or designed that tool.
[00:05:22.080 --> 00:05:24.120]   "Did you F& do this?"
[00:05:24.120 --> 00:05:26.120]   Musk asked him again.
[00:05:26.120 --> 00:05:29.880]   "I'm not sure what you're referring to," the engineer replied apologetic.
[00:05:29.880 --> 00:05:32.160]   "You're an F& idiot!"
[00:05:32.160 --> 00:05:33.160]   Musk shouted back.
[00:05:33.160 --> 00:05:35.480]   "Get the F out and don't come back!"
[00:05:35.480 --> 00:05:36.480]   And he was fired.
[00:05:36.480 --> 00:05:39.000]   Gone.
[00:05:39.000 --> 00:05:43.880]   The entire conversation had lasted less than a minute.
[00:05:43.880 --> 00:05:46.000]   And on and on and on.
[00:05:46.000 --> 00:05:54.480]   And it really makes you wonder, "Holy cow, how can this company survive this guy?"
[00:05:54.480 --> 00:05:56.480]   Thoughts?
[00:05:56.480 --> 00:05:57.480]   Yeah.
[00:05:57.480 --> 00:06:01.480]   What really stood out to me is, "Yeah, how can this company survive this guy?"
[00:06:01.480 --> 00:06:08.000]   Where would Tesla be if it was a place that was not a living nightmare like this?
[00:06:08.000 --> 00:06:12.440]   I feel like we often have this, or the tech community at large, kind of idealize these
[00:06:12.440 --> 00:06:18.560]   eccentric genius CEO personalities, whether it's Elon Musk or Steve Jobs.
[00:06:18.560 --> 00:06:25.920]   But it seems like Tesla engineers or just as a workplace in general, it would be much
[00:06:25.920 --> 00:06:27.840]   better and perhaps more productive.
[00:06:27.840 --> 00:06:31.240]   All these people didn't have to feel like they were walking on eggshells.
[00:06:31.240 --> 00:06:38.760]   Every single minute working with this eccentric madman.
[00:06:38.760 --> 00:06:39.760]   I have to...
[00:06:39.760 --> 00:06:40.760]   I want to honor him.
[00:06:40.760 --> 00:06:41.760]   I think you're going to do that, Michael.
[00:06:41.760 --> 00:06:42.760]   So I'll let you.
[00:06:42.760 --> 00:06:46.400]   But I mean, Tesla might not even exist without Elon Musk, too, right?
[00:06:46.400 --> 00:06:47.400]   Well, yeah.
[00:06:47.400 --> 00:06:48.960]   I mean, I think it's great.
[00:06:48.960 --> 00:06:52.560]   Elon Musk, but one part that's sort of at this article is that a lot of people praised
[00:06:52.560 --> 00:06:55.000]   Elon Musk for being a great boss for...
[00:06:55.000 --> 00:06:56.000]   A visionary.
[00:06:56.000 --> 00:06:57.000]   A visionary.
[00:06:57.000 --> 00:06:58.000]   Yeah.
[00:06:58.000 --> 00:07:00.440]   The things that he brought to Tesla and the things that he did with Tesla cannot be
[00:07:00.440 --> 00:07:07.920]   understated, but like where would Tesla be if his team felt support or even less than
[00:07:07.920 --> 00:07:11.000]   support, just the fact that they weren't going to be fired if they happened to have
[00:07:11.000 --> 00:07:13.560]   a conversation with their CEO?
[00:07:13.560 --> 00:07:15.160]   Go ahead, Michael.
[00:07:15.160 --> 00:07:16.160]   Right.
[00:07:16.160 --> 00:07:17.160]   Yeah.
[00:07:17.160 --> 00:07:18.160]   I think...
[00:07:18.160 --> 00:07:19.440]   Well, Tesla absolutely wouldn't exist without Elon.
[00:07:19.440 --> 00:07:24.840]   I think he often gets a little bit more credit than he deserves because I think Tesla realistically
[00:07:24.840 --> 00:07:28.520]   doesn't have a very big edge in any of the technologies that they're producing.
[00:07:28.520 --> 00:07:31.560]   So a lot of people can make electric engines.
[00:07:31.560 --> 00:07:33.600]   Tesla does batteries really well.
[00:07:33.600 --> 00:07:34.600]   That's their big bet.
[00:07:34.600 --> 00:07:37.880]   So if Tesla can make batteries better than anyone in the world, that company will continue
[00:07:37.880 --> 00:07:41.920]   to exist if they can't, then they'll lose their edge.
[00:07:41.920 --> 00:07:45.200]   And so it's a lot of people getting really excited about battery technology, which is
[00:07:45.200 --> 00:07:47.000]   kind of funny to me.
[00:07:47.000 --> 00:07:51.680]   But I think ultimately Tesla is a company that's kind of a cult of personality.
[00:07:51.680 --> 00:07:54.240]   People go there to work for Elon Musk.
[00:07:54.240 --> 00:07:59.240]   The people that I've talked to that have worked closely with Elon have said that it's extremely
[00:07:59.240 --> 00:08:05.840]   difficult, that he tends to choose favorites and he quickly tires of those favorites and
[00:08:05.840 --> 00:08:09.560]   finds ways to excise them from the company or from his inner circle.
[00:08:09.560 --> 00:08:12.560]   And it's really...
[00:08:12.560 --> 00:08:13.920]   It's an anxiety-ridden company.
[00:08:13.920 --> 00:08:16.320]   I think it sounds very exhausting.
[00:08:16.320 --> 00:08:20.680]   But also the thing that I've learned over the course of my career is that companies can
[00:08:20.680 --> 00:08:21.840]   operate this way.
[00:08:21.840 --> 00:08:26.840]   Unfortunately, this is just bad leadership or it's a type of leadership.
[00:08:26.840 --> 00:08:32.680]   And some people tend to rule things based on fear and based on whether people are drinking
[00:08:32.680 --> 00:08:37.240]   the Kool-Aid, whether people really buy into the leader's idea.
[00:08:37.240 --> 00:08:39.840]   So Steve Jobs would be a prime example of that.
[00:08:39.840 --> 00:08:43.680]   I've certainly worked at places that are run like that.
[00:08:43.680 --> 00:08:44.680]   But...
[00:08:44.680 --> 00:08:46.640]   Was Pete Cashmore that kind of crazy?
[00:08:46.640 --> 00:08:47.640]   No.
[00:08:47.640 --> 00:08:48.640]   Pete Cashmore is...
[00:08:48.640 --> 00:08:50.840]   He is the smallest guy I've ever met.
[00:08:50.840 --> 00:08:51.840]   Yeah.
[00:08:51.840 --> 00:08:53.560]   Yeah, he is extremely nice.
[00:08:53.560 --> 00:08:57.640]   But there are other places definitely in media that are run by these types of personalities.
[00:08:57.640 --> 00:08:59.600]   And I'm sure even in the...
[00:08:59.600 --> 00:09:04.360]   Maybe an easier way to say this is during the golden era of magazines, I think you saw
[00:09:04.360 --> 00:09:11.280]   even bigger personalities that were running a lot of big magazines.
[00:09:11.280 --> 00:09:14.400]   But yeah, my point is that...
[00:09:14.400 --> 00:09:15.400]   You think Thomas Edison...
[00:09:15.400 --> 00:09:19.840]   Somebody in the chair, I'm saying, do you think Thomas Edison was as hard to work for as
[00:09:19.840 --> 00:09:21.840]   Steve Jobs or Elon Musk?
[00:09:21.840 --> 00:09:24.640]   On a bad day, I would imagine so.
[00:09:24.640 --> 00:09:25.640]   But...
[00:09:25.640 --> 00:09:29.240]   You know, I wonder if there's a toxic culture that is unique to Silicon Valley.
[00:09:29.240 --> 00:09:30.880]   Because I mean...
[00:09:30.880 --> 00:09:35.920]   So one response from Tesla's general counsel to this Wired story, there are a lot of people
[00:09:35.920 --> 00:09:40.960]   outside Tesla who will sort of sugarcoat what they actually think of your performance
[00:09:40.960 --> 00:09:44.560]   or of an issue because they don't want to have that hard conversation.
[00:09:44.560 --> 00:09:48.400]   Musk is someone who puts a lot of effort into forcing himself to be fully honest.
[00:09:48.400 --> 00:09:54.320]   This is what Reed Hastings is kind of dinged for sometimes at Netflix for radical honesty.
[00:09:54.320 --> 00:09:58.400]   There's such a thing as being radically honest, but there's also such a thing as pointlessly
[00:09:58.400 --> 00:10:00.280]   treating people terribly.
[00:10:00.280 --> 00:10:05.640]   I think it seems like Tesla did not really deny the substance of anything in this story.
[00:10:05.640 --> 00:10:07.760]   They just said that the context wasn't always there.
[00:10:07.760 --> 00:10:08.760]   That's how we run it.
[00:10:08.760 --> 00:10:09.760]   I mean, more and more...
[00:10:09.760 --> 00:10:14.200]   I've been reading all these stories about CBS, which seemed to have a culture of thinking
[00:10:14.200 --> 00:10:15.200]   that...
[00:10:15.200 --> 00:10:20.680]   Several levels below Moonves, the theory was that if you were talented, it was okay if
[00:10:20.680 --> 00:10:22.920]   you treated your colleagues horribly.
[00:10:22.920 --> 00:10:27.880]   And more and more, I think there is no excuse for cultures that allow anybody, including
[00:10:27.880 --> 00:10:29.760]   the CEO, to treat other people horribly.
[00:10:29.760 --> 00:10:36.280]   And that is a distinct thing for making hard decisions or holding people to a high standard
[00:10:36.280 --> 00:10:37.280]   of competence.
[00:10:37.280 --> 00:10:38.780]   Yeah, I also...
[00:10:38.780 --> 00:10:42.840]   I wouldn't conflate radical transparency with exactly what Harry just said.
[00:10:42.840 --> 00:10:44.160]   You know, treating people poorly.
[00:10:44.160 --> 00:10:45.160]   So I've worked at...
[00:10:45.160 --> 00:10:48.360]   You know, Gawker Media embraced radical transparency.
[00:10:48.360 --> 00:10:53.040]   Bridgewater Capital of Hedge Fund is famously run by a guy named Ray Dalio, who embraces
[00:10:53.040 --> 00:10:54.040]   radical transparency.
[00:10:54.040 --> 00:10:57.840]   Both of those places are difficult to work at.
[00:10:57.840 --> 00:11:03.920]   And you might be confronted with some of your weaknesses regularly, but I don't think that's
[00:11:03.920 --> 00:11:07.120]   the same type of thing as throwing a tantrum.
[00:11:07.120 --> 00:11:11.120]   It sounds like Elon Musk is motivated by...
[00:11:11.120 --> 00:11:14.880]   I think he lets his emotions get the best of him more frequently than he'd probably like
[00:11:14.880 --> 00:11:15.880]   to admit.
[00:11:15.880 --> 00:11:19.200]   I mean, if Elon was a guy on the assembly line, he would not be allowed to treat people as
[00:11:19.200 --> 00:11:20.680]   badly as he's allowed to treat people.
[00:11:20.680 --> 00:11:24.280]   Yeah, but we have heard stories about racism and other problems on the assembly line as
[00:11:24.280 --> 00:11:25.280]   well.
[00:11:25.280 --> 00:11:26.280]   There's...
[00:11:26.280 --> 00:11:27.280]   You know, it starts at the top, but...
[00:11:27.280 --> 00:11:30.000]   And I'm not saying Elon's a racist, but there are problems on these...
[00:11:30.000 --> 00:11:34.640]   Which is one reason why you don't want people at the top to behave like Jackassas, because
[00:11:34.640 --> 00:11:36.120]   it becomes part of your culture.
[00:11:36.120 --> 00:11:37.120]   Yeah.
[00:11:37.120 --> 00:11:43.080]   Especially because it kind of strengthens this culture where there is no transparency, where
[00:11:43.080 --> 00:11:49.640]   perhaps any employee who I guess is an Elon could not bring up an issue that they see or
[00:11:49.640 --> 00:11:50.640]   be able to...
[00:11:50.640 --> 00:11:51.640]   Let's say...
[00:11:51.640 --> 00:11:55.560]   Tesla hires some really smart people.
[00:11:55.560 --> 00:12:00.320]   And if any of them had perhaps an idea that conflicted with a direct order, I guess from
[00:12:00.320 --> 00:12:06.200]   Elon or someone above them, this sort of culture negates any opportunity for them to bring up
[00:12:06.200 --> 00:12:14.840]   what they think could I guess be one of the next best Tesla features.
[00:12:14.840 --> 00:12:20.360]   The 60 Minutes piece on Sunday was pretty negative, right?
[00:12:20.360 --> 00:12:22.120]   I mean, that's kind of...
[00:12:22.120 --> 00:12:25.000]   And he even Musk himself pointed out that, you know, 10 years ago they did a piece of
[00:12:25.000 --> 00:12:26.400]   me and I was the Messiah.
[00:12:26.400 --> 00:12:27.400]   What happened?
[00:12:27.400 --> 00:12:28.400]   Well, he's...
[00:12:28.400 --> 00:12:30.360]   He's a much greater antagonist.
[00:12:30.360 --> 00:12:31.360]   Yeah.
[00:12:31.360 --> 00:12:36.160]   And he made more provocative comments in this interview purposely, I think.
[00:12:36.160 --> 00:12:38.640]   Well, he said, I talked to him for five hours.
[00:12:38.640 --> 00:12:41.440]   They put the most provocative 15 Minutes on.
[00:12:41.440 --> 00:12:42.440]   That's TV.
[00:12:42.440 --> 00:12:43.440]   That's TV.
[00:12:43.440 --> 00:12:46.520]   You got to be careful what you say.
[00:12:46.520 --> 00:12:50.800]   Elon says he's been misunderstood and mistreated by journalists.
[00:12:50.800 --> 00:12:51.800]   And maybe he has.
[00:12:51.800 --> 00:12:52.800]   And I do...
[00:12:52.800 --> 00:12:54.640]   And I bought a Tesla.
[00:12:54.640 --> 00:12:59.520]   I don't know if there would be an electric car industry if he hadn't paved the way.
[00:12:59.520 --> 00:13:07.280]   And of course now governments trying to eliminate emissions are trying to move towards electric.
[00:13:07.280 --> 00:13:08.600]   So his timing was good.
[00:13:08.600 --> 00:13:11.200]   You can't say it's entirely him.
[00:13:11.200 --> 00:13:14.240]   But I think he is a visionary and I think he did have a vision and I think he wanted
[00:13:14.240 --> 00:13:15.240]   to do good things.
[00:13:15.240 --> 00:13:19.360]   And this reminds me a lot of Steve Jobs who had both bad and good, right?
[00:13:19.360 --> 00:13:21.160]   We were old enough to live through that.
[00:13:21.160 --> 00:13:25.640]   He was certainly accused of very similar behavior at his worst.
[00:13:25.640 --> 00:13:26.640]   Yeah.
[00:13:26.640 --> 00:13:27.640]   Oh, very similar.
[00:13:27.640 --> 00:13:31.160]   I mean he also had days where he held people to an impossibly high standard and it was
[00:13:31.160 --> 00:13:35.960]   a great thing and he probably did it in a way that was not terrible, just hard.
[00:13:35.960 --> 00:13:36.960]   Yeah.
[00:13:36.960 --> 00:13:38.840]   And I don't really mean to pick on Elon.
[00:13:38.840 --> 00:13:44.280]   I'm more interested in if there's this sense in Silicon Valley that you have to have a
[00:13:44.280 --> 00:13:46.640]   toxic culture to get things done.
[00:13:46.640 --> 00:13:47.640]   I mean people felt like...
[00:13:47.640 --> 00:13:52.000]   And people flocked around Steve Jobs and put up with the difficulty of working at Apple
[00:13:52.000 --> 00:13:53.480]   because they thought they were changing the world.
[00:13:53.480 --> 00:13:57.360]   I imagine it's the same as SpaceX and Tesla.
[00:13:57.360 --> 00:14:00.320]   They're there because you're making a big difference and you're going to put up with
[00:14:00.320 --> 00:14:05.880]   the tough conditions because that's maybe what it takes to make something insanely great.
[00:14:05.880 --> 00:14:09.160]   Yeah, I tend to disagree with that.
[00:14:09.160 --> 00:14:10.560]   I mean, look at...
[00:14:10.560 --> 00:14:16.400]   It's just like try to imagine Tim Cook doing what we've just read about Elon Musk in that
[00:14:16.400 --> 00:14:22.480]   wired feature story or think about Sundar Pichai acting in the same way.
[00:14:22.480 --> 00:14:28.640]   I think you can run a competitive business and be a reasonable human being.
[00:14:28.640 --> 00:14:35.400]   And there's so many examples of balanced tech leaders, Bill Gates, I think is not running
[00:14:35.400 --> 00:14:38.440]   through manufacturing facilities and yelling at people.
[00:14:38.440 --> 00:14:45.200]   I'm sure there were periods at Microsoft that were very difficult and had high demands on
[00:14:45.200 --> 00:14:46.200]   a lot of the employees.
[00:14:46.200 --> 00:14:51.480]   But I think at the end of the day, Elon seems a little bit more self-absorbed than these
[00:14:51.480 --> 00:14:52.480]   guys.
[00:14:52.480 --> 00:14:54.840]   And I think he wants it to be a cult of personality.
[00:14:54.840 --> 00:15:00.320]   He wants to be seen as someone that has a lot of money and a lot of brilliant ideas.
[00:15:00.320 --> 00:15:06.600]   And those two things may be true, but I think he's also become annoyed that he's being held
[00:15:06.600 --> 00:15:08.280]   accountable for his actions.
[00:15:08.280 --> 00:15:15.240]   I mean, to say that in the 60 Minutes interview, for example, to say, "I do not respect the
[00:15:15.240 --> 00:15:20.000]   SEC after you've been investigated by them."
[00:15:20.000 --> 00:15:25.000]   You may not respect it, but they have the power to find you $20 million for a tweet.
[00:15:25.000 --> 00:15:31.440]   It's harder to be with the stuff if you're not a founder or a pseudo-founder as Elon is
[00:15:31.440 --> 00:15:32.440]   with the Tesla.
[00:15:32.440 --> 00:15:34.240]   And you're actually fantastic.
[00:15:34.240 --> 00:15:38.840]   I mean, Tim Cook, there are tons of stories about Tim Cook not being warm and fuzzy and
[00:15:38.840 --> 00:15:42.640]   being hard-nosed and incredibly demanding.
[00:15:42.640 --> 00:15:46.840]   But I haven't heard anything about the bleed-in to him being unpleasant for the sake of being
[00:15:46.840 --> 00:15:48.320]   unpleasant.
[00:15:48.320 --> 00:15:55.120]   Hugh Higg also has an article in The Atlantic this month about American rage and the unstalled
[00:15:55.120 --> 00:15:59.440]   story of how anger became the dominant emotion in our politics and personal lives.
[00:15:59.440 --> 00:16:01.000]   And we were talking before the show began.
[00:16:01.000 --> 00:16:02.000]   I think we've lost Paris.
[00:16:02.000 --> 00:16:03.800]   We're going to try to get her back.
[00:16:03.800 --> 00:16:04.800]   What happened?
[00:16:04.800 --> 00:16:05.800]   We lost her.
[00:16:05.800 --> 00:16:06.800]   We're trying to get her back.
[00:16:06.800 --> 00:16:07.800]   Just as I said.
[00:16:07.800 --> 00:16:08.800]   We lost her.
[00:16:08.800 --> 00:16:09.800]   We're trying to get her back.
[00:16:09.800 --> 00:16:14.120]   But we were talking before the show, poor Paris has to, the nature of her work at Wired
[00:16:14.120 --> 00:16:22.120]   is to visit Twitter regularly, even 4chan and all the other pockets of rage on the internet.
[00:16:22.120 --> 00:16:25.760]   It really does feel like we're more angry these days.
[00:16:25.760 --> 00:16:29.640]   And is it, I also feel like it's somewhat because we've been given tools.
[00:16:29.640 --> 00:16:31.440]   Anger has been weaponized by places like Twitter.
[00:16:31.440 --> 00:16:32.440]   You like Twitter, though.
[00:16:32.440 --> 00:16:33.440]   You're not a-
[00:16:33.440 --> 00:16:36.760]   I acknowledge all of Twitter's problems and have written about a lot of them.
[00:16:36.760 --> 00:16:40.240]   My own little bubble within Twitter is a pleasant place to be.
[00:16:40.240 --> 00:16:41.720]   It's not something I do, gradually.
[00:16:41.720 --> 00:16:42.720]   Yeah.
[00:16:42.720 --> 00:16:43.720]   I totally like it.
[00:16:43.720 --> 00:16:44.720]   It's horrible.
[00:16:44.720 --> 00:16:45.720]   You admit.
[00:16:45.720 --> 00:16:48.220]   Yes, I totally acknowledge the fact that I like Twitter does not mean that everybody
[00:16:48.220 --> 00:16:53.160]   likes Twitter or even can like Twitter.
[00:16:53.160 --> 00:16:54.240]   It's like any other medium.
[00:16:54.240 --> 00:16:55.240]   It's very complex.
[00:16:55.240 --> 00:16:58.920]   And it depends on how you intersect with it and who you're talking to and the topics
[00:16:58.920 --> 00:17:03.640]   you talk about and who you are and who people perceive you to be.
[00:17:03.640 --> 00:17:06.040]   Paris, when you were gone, I'm glad we got you back.
[00:17:06.040 --> 00:17:10.240]   We were talking about Duhek's other article on this month in Atlantic about the real
[00:17:10.240 --> 00:17:13.680]   roots of American rage and how angry everybody is.
[00:17:13.680 --> 00:17:19.320]   And I mentioned that you have to live in the cesspools of Anger on the Internet.
[00:17:19.320 --> 00:17:22.120]   Yes, every day.
[00:17:22.120 --> 00:17:25.640]   On Twitter, Reddit, Vote, GAB, 4chan.
[00:17:25.640 --> 00:17:26.640]   It's the real-
[00:17:26.640 --> 00:17:28.640]   You're hanging out on GAB, too?
[00:17:28.640 --> 00:17:29.640]   You got to.
[00:17:29.640 --> 00:17:30.640]   I mean, I tried to hang out there.
[00:17:30.640 --> 00:17:36.520]   I have a couple of tools that monitor GAB for a couple of key words, but I think a large
[00:17:36.520 --> 00:17:41.760]   part about writing about online extremism is just keeping tabs in a variety of different
[00:17:41.760 --> 00:17:45.920]   parts of the Internet and generally the worst parts of the Internet.
[00:17:45.920 --> 00:17:50.240]   Is it your sense that it's worse today than it's been or is it the...
[00:17:50.240 --> 00:17:53.280]   I mean, I think that it's...
[00:17:53.280 --> 00:17:54.520]   That's a hard question.
[00:17:54.520 --> 00:18:01.840]   And I think it's because when we say worse, we can see worse things more easily.
[00:18:01.840 --> 00:18:07.240]   But I think that these bad things have always been there or these awful conversations and
[00:18:07.240 --> 00:18:09.320]   this hate has always existed.
[00:18:09.320 --> 00:18:15.720]   It's just now that because of the proliferation of social media and the ease of access we have
[00:18:15.720 --> 00:18:21.360]   to all these different mediums, these sort of groups and hateful rhetoric that they
[00:18:21.360 --> 00:18:27.840]   spouse, they are much more high-profile and you can access them by just searching.
[00:18:27.840 --> 00:18:33.920]   I think if you go back to news groups in the '80s or '90s, you will find parallels, but
[00:18:33.920 --> 00:18:36.840]   a tiny percentage of the people in the world were in news groups.
[00:18:36.840 --> 00:18:39.640]   They weren't the least bit mainstream.
[00:18:39.640 --> 00:18:40.640]   Yeah, exactly.
[00:18:40.640 --> 00:18:43.640]   And they were for people who weren't aware of it and it did not become national news
[00:18:43.640 --> 00:18:48.560]   stories and the President of the United States wasn't diving in to it in the way that happens
[00:18:48.560 --> 00:18:49.560]   today.
[00:18:49.560 --> 00:18:55.480]   Yeah, I mean, I didn't even consider the President element, but I think, yeah, without a doubt,
[00:18:55.480 --> 00:18:57.120]   it's easier to broadcast these ideas.
[00:18:57.120 --> 00:19:03.320]   I think it's easier to recruit vulnerable members of society to some of these causes.
[00:19:03.320 --> 00:19:09.320]   I think it's spilling over into the real world now so people are acting on these conspiracies
[00:19:09.320 --> 00:19:14.760]   such as pizza gate or probably other better examples.
[00:19:14.760 --> 00:19:21.960]   But to me, it seems, yeah, QAnon, I think without a doubt, the hate is rising in America.
[00:19:21.960 --> 00:19:27.480]   I think that it's more accepted than it's been, at least in my lifetime.
[00:19:27.480 --> 00:19:28.480]   Here's my theory.
[00:19:28.480 --> 00:19:31.280]   I want to try this out on you all.
[00:19:31.280 --> 00:19:39.160]   You know, when you go, there's signs on bridges in some bridges in this country that when
[00:19:39.160 --> 00:19:43.720]   troops are marched across bridges to break step, not to march in lockstep, because their
[00:19:43.720 --> 00:19:49.240]   step will resonate and actually cause the bridge to get more and more resonated and
[00:19:49.240 --> 00:19:50.560]   break it.
[00:19:50.560 --> 00:19:55.080]   Whereas if they don't march in lockstep, there's no resonance created.
[00:19:55.080 --> 00:20:01.000]   My theory is that anger, and I do talk about this in Atlantic article, anger is self-reinforcing.
[00:20:01.000 --> 00:20:05.320]   It's not something that there are things you can do that get it out of your system and
[00:20:05.320 --> 00:20:12.760]   then it goes away, but anger and revenge create a vicious circle that they amplify
[00:20:12.760 --> 00:20:14.560]   themselves.
[00:20:14.560 --> 00:20:16.160]   That's always been the case.
[00:20:16.160 --> 00:20:23.600]   But what my theory is that there's a resonating frequency, the Twitter and Facebook, and to
[00:20:23.600 --> 00:20:28.320]   a lesser degree, because they're lesser used GAB and places like that, are acting as a
[00:20:28.320 --> 00:20:30.280]   resonating agent.
[00:20:30.280 --> 00:20:34.640]   They're getting it to amplify to the breaking point.
[00:20:34.640 --> 00:20:36.520]   It's not that the anger hasn't always existed.
[00:20:36.520 --> 00:20:41.840]   It's that there are now in our society things that are amplifying it to the point where
[00:20:41.840 --> 00:20:46.680]   it's actually becoming a more and more rapid cycle of anger.
[00:20:46.680 --> 00:20:48.360]   Isn't that what you see on Twitter?
[00:20:48.360 --> 00:20:50.400]   I mean, social media amplifies everything.
[00:20:50.400 --> 00:20:53.840]   Actually social media often amplifies things in a positive way.
[00:20:53.840 --> 00:20:57.680]   I think people just don't pay quite as much attention to it because it's nothing to worry
[00:20:57.680 --> 00:21:04.560]   about an arguably it can amplify lies and hate more powerfully than it amplifies things.
[00:21:04.560 --> 00:21:08.720]   The problem is we're attracted to this.
[00:21:08.720 --> 00:21:17.840]   So systems like Facebook that are engineered to reinforce that behavior, to increase engagement
[00:21:17.840 --> 00:21:22.200]   are automatically reinforcing it because that's what engages you.
[00:21:22.200 --> 00:21:26.960]   Yeah, I mean, people stay on longer if they think that they're among a group of people
[00:21:26.960 --> 00:21:28.840]   that think just like them.
[00:21:28.840 --> 00:21:30.640]   And that's been one thing that I have.
[00:21:30.640 --> 00:21:35.200]   Why doesn't somebody do a happy news site or a happy news newspaper?
[00:21:35.200 --> 00:21:36.440]   Because no one will read it.
[00:21:36.440 --> 00:21:40.840]   Because no one will read it because that's not what people love to be outraged.
[00:21:40.840 --> 00:21:42.000]   People love to be outraged.
[00:21:42.000 --> 00:21:43.400]   People love to be furious.
[00:21:43.400 --> 00:21:47.320]   People, and I think stems because they like feeling like they're part of this kind of
[00:21:47.320 --> 00:21:48.320]   in group.
[00:21:48.320 --> 00:21:52.120]   That's one thing that I've noticed when I've been studying, I guess, online conspiracies
[00:21:52.120 --> 00:21:58.720]   and online extremism is that the ones that stay around for the longest seem to share the
[00:21:58.720 --> 00:22:03.760]   same note that their adherence have this sense of community.
[00:22:03.760 --> 00:22:06.800]   It's them against the world or some other group.
[00:22:06.800 --> 00:22:07.800]   Yes.
[00:22:07.800 --> 00:22:12.400]   I guess in the case of QAnon, why I think that it has existed for almost a year now,
[00:22:12.400 --> 00:22:20.280]   or I guess over a year now, is that it, the same factor of it is that, okay, everybody
[00:22:20.280 --> 00:22:21.720]   out in the world is foolish.
[00:22:21.720 --> 00:22:22.720]   They don't understand.
[00:22:22.720 --> 00:22:24.280]   It's the deep state.
[00:22:24.280 --> 00:22:28.320]   We are the only ones that are knowledgeable, and we are the smart ones.
[00:22:28.320 --> 00:22:29.320]   All conspiracies do.
[00:22:29.320 --> 00:22:31.120]   Putting yourself, yeah.
[00:22:31.120 --> 00:22:32.120]   Yeah.
[00:22:32.120 --> 00:22:34.680]   That's why they continue to stay around.
[00:22:34.680 --> 00:22:37.640]   That's why also when it comes to these sort of hate groups, it's an in-group out-group
[00:22:37.640 --> 00:22:38.640]   thing.
[00:22:38.640 --> 00:22:44.760]   And these social media platforms reward that primal part in our brain that wants to constantly
[00:22:44.760 --> 00:22:50.240]   be engaged in a war for all eternity, a war between us and others.
[00:22:50.240 --> 00:22:52.880]   It's way more exciting than real life.
[00:22:52.880 --> 00:22:58.080]   If you are existing in this fake fantasy world that's constantly got you amped up, where
[00:22:58.080 --> 00:23:04.640]   you're constantly fighting against deep, shadowy, deep-stake forces or against some
[00:23:04.640 --> 00:23:09.280]   other race that you think is inferior in trying to take away your rights in the case
[00:23:09.280 --> 00:23:17.360]   of, I guess, white supremacist groups, things like that, these platforms are built to pry
[00:23:17.360 --> 00:23:18.360]   into our deepest urges.
[00:23:18.360 --> 00:23:20.400]   So is there something we should do about this?
[00:23:20.400 --> 00:23:24.000]   I mean, I don't think we should shut down Twitter and Facebook.
[00:23:24.000 --> 00:23:25.480]   Should Facebook take response?
[00:23:25.480 --> 00:23:29.840]   Facebook's not going to put good news in the newsfeed because that's not their business.
[00:23:29.840 --> 00:23:33.480]   Well, they sort of have moved a tiny bit in that direction because they said they're going
[00:23:33.480 --> 00:23:35.560]   to put less news in your newsfeed.
[00:23:35.560 --> 00:23:36.560]   More local.
[00:23:36.560 --> 00:23:40.200]   And give you more stuff from your friends and family, which could be awful.
[00:23:40.200 --> 00:23:42.200]   But at least there's a decent chance it's not the worst.
[00:23:42.200 --> 00:23:47.200]   But you know, the side effect that that was is in that announcement, I think Mark Zuckerberg
[00:23:47.200 --> 00:23:50.560]   whoever it was, the press release, when they said they're going to de-emphasize local
[00:23:50.560 --> 00:23:55.240]   news, they said, instead, we're going to promote, yeah, like you said, post-firmary
[00:23:55.240 --> 00:23:58.560]   family friends, but also things in groups.
[00:23:58.560 --> 00:24:01.840]   And do you know what, since January when they made this announcement has become more
[00:24:01.840 --> 00:24:05.800]   the hottest places for disinformation and extremism, Facebook groups?
[00:24:05.800 --> 00:24:06.800]   Yeah.
[00:24:06.800 --> 00:24:09.320]   That's where the Giais-John protests in France came at us.
[00:24:09.320 --> 00:24:10.880]   We talked about this last week.
[00:24:10.880 --> 00:24:11.880]   This is a perfect example.
[00:24:11.880 --> 00:24:14.240]   I know French people are always angry, right?
[00:24:14.240 --> 00:24:20.040]   I mean, there's a multi-hundred, multi-century history of barricades and riots.
[00:24:20.040 --> 00:24:21.040]   In fact, they-
[00:24:21.040 --> 00:24:24.600]   And there are some genuine points for the Giais-John protests, but there's also a large
[00:24:24.600 --> 00:24:30.440]   part of it that has arisen just from unbridled rage, conspiracy theories that has come out
[00:24:30.440 --> 00:24:33.600]   of these Facebook groups with hundreds of thousands.
[00:24:33.600 --> 00:24:35.960]   One of them even has over a million members.
[00:24:35.960 --> 00:24:36.960]   Yeah.
[00:24:36.960 --> 00:24:39.320]   And they're local because they're all by De Parmeau.
[00:24:39.320 --> 00:24:44.200]   But the thing I find interesting is this is always- so this is a pretty-
[00:24:44.200 --> 00:24:49.000]   Actually, France is probably a really good test tube for this because they've always
[00:24:49.000 --> 00:24:53.680]   had protests, though it grand boulevards or Paris were designed so they couldn't be barricaded.
[00:24:53.680 --> 00:24:54.680]   That's why there's boulevards.
[00:24:54.680 --> 00:24:58.800]   That's why Hausmann built the boulevards, is so that they could easily un-barricate them
[00:24:58.800 --> 00:25:02.360]   in troops could march in and break up the protests.
[00:25:02.360 --> 00:25:06.720]   But I mean, if you've ever seen Les Misérables, you know, I mean, this is a long-standing
[00:25:06.720 --> 00:25:07.920]   history.
[00:25:07.920 --> 00:25:13.360]   But what's unique about the yellow jacket protests is that if you ask somebody, they
[00:25:13.360 --> 00:25:16.600]   can't really pinpoint the reason for their anger.
[00:25:16.600 --> 00:25:17.600]   It's like a lot.
[00:25:17.600 --> 00:25:19.600]   It's a generalized upset.
[00:25:19.600 --> 00:25:23.560]   Yeah, it's an amorphous kind of ball of rage.
[00:25:23.560 --> 00:25:28.400]   It ostensibly began because it was tied to the gas-type tax hike that was supposed to
[00:25:28.400 --> 00:25:29.400]   come in January.
[00:25:29.400 --> 00:25:33.800]   But then Macron said, okay, we're doing with it or at least going to suspend it for six
[00:25:33.800 --> 00:25:34.800]   months.
[00:25:34.800 --> 00:25:40.880]   And then two days later, France had the biggest protest it had ever related to these- the
[00:25:40.880 --> 00:25:45.600]   Gégionne yellow bests and it has continued to spiral.
[00:25:45.600 --> 00:25:48.000]   I wonder.
[00:25:48.000 --> 00:25:52.840]   If there- I don't know how you would fix this because it is a human tendency.
[00:25:52.840 --> 00:25:56.080]   And social networks and the internet in general reflect humans.
[00:25:56.080 --> 00:25:57.080]   Good and bad.
[00:25:57.080 --> 00:25:58.080]   We know that.
[00:25:58.080 --> 00:26:00.200]   I don't know if you- does it need a fix?
[00:26:00.200 --> 00:26:01.440]   Does it need a cure?
[00:26:01.440 --> 00:26:07.640]   Maybe- I always thought when- and Harry and I are of the era where we were very optimistic
[00:26:07.640 --> 00:26:09.240]   in the '90s about the internet.
[00:26:09.240 --> 00:26:13.020]   The internet was going to change the world for the good because it was friction-free
[00:26:13.020 --> 00:26:14.020]   commerce.
[00:26:14.020 --> 00:26:16.960]   People would be able to express themselves, no more gatekeepers.
[00:26:16.960 --> 00:26:20.000]   It was a democratizing medium.
[00:26:20.000 --> 00:26:24.800]   And one of the things I said is sunlight is the best disinfectant that yes, they're
[00:26:24.800 --> 00:26:28.680]   hate groups and they're hidden and they're under rocks.
[00:26:28.680 --> 00:26:32.680]   And so the internet's going to give these- draw these people out in the open and then
[00:26:32.680 --> 00:26:34.000]   sunlight would disinfect.
[00:26:34.000 --> 00:26:37.400]   We would all- maybe that's what's going on maybe now, right?
[00:26:37.400 --> 00:26:38.840]   Maybe we've all as a society.
[00:26:38.840 --> 00:26:45.640]   We're just seeing the snakes and vipers under the rocks and saying, "Oh, that's horrible."
[00:26:45.640 --> 00:26:49.880]   And maybe now over the next decade or two, things will get better.
[00:26:49.880 --> 00:26:50.880]   Is that possible?
[00:26:50.880 --> 00:26:54.080]   Well, I think- I think-
[00:26:54.080 --> 00:26:55.240]   Okay, Paris-
[00:26:55.240 --> 00:26:56.240]   That's right.
[00:26:56.240 --> 00:26:57.240]   Michael, let's do Paris-
[00:26:57.240 --> 00:26:58.240]   Sure.
[00:26:58.240 --> 00:26:59.240]   Okay.
[00:26:59.240 --> 00:27:03.800]   I mean, sunlight disinfects, yes, and there are a lot of cases where yes, we want to expose
[00:27:03.800 --> 00:27:11.880]   these sort of ills, but also sunlight amplifies and giving a spotlight to something exposes
[00:27:11.880 --> 00:27:13.760]   it to more people.
[00:27:13.760 --> 00:27:17.920]   And while- this is one thing I've been thinking about a bit as this year is finished, we've
[00:27:17.920 --> 00:27:24.880]   gotten so much better as society, as journalists, as platforms at identifying disinformation
[00:27:24.880 --> 00:27:26.440]   or extremism.
[00:27:26.440 --> 00:27:33.560]   And- but we haven't figured out yet how to identify it and deal with it in a way that
[00:27:33.560 --> 00:27:39.320]   doesn't unduly amplify it or unduly show it to other people.
[00:27:39.320 --> 00:27:44.760]   Because if there's a large trend, I think, in the industry of tech journalism or internet
[00:27:44.760 --> 00:27:49.200]   journalism, whatever you want to call it, of people going through 4chan or some awful
[00:27:49.200 --> 00:27:54.480]   part of the internet and finding a bunch of, I don't know, let's say Nazis doing terrible
[00:27:54.480 --> 00:27:58.440]   Nazi things or crazy conspiracy theorists and screen-shiting it, tweeting it out or
[00:27:58.440 --> 00:27:59.800]   writing a story about it.
[00:27:59.800 --> 00:28:05.360]   And while readers or viewers will be like, wow, that's crazy or laugh at it, there's also
[00:28:05.360 --> 00:28:10.160]   a large sect of other people who see it and go, hmm, what is this QAnon thing?
[00:28:10.160 --> 00:28:15.000]   Let me Google it and then end up falling into the rabbit hole themselves or same with other
[00:28:15.000 --> 00:28:16.560]   extremist beliefs.
[00:28:16.560 --> 00:28:23.760]   We have to find the middle line somewhere between disinfecting and overexposing.
[00:28:23.760 --> 00:28:24.760]   Michael?
[00:28:24.760 --> 00:28:30.040]   So I guess I think about this in slightly different terms.
[00:28:30.040 --> 00:28:37.920]   I guess, for me, it's all about the click-based economy and how Twitter and Facebook basically
[00:28:37.920 --> 00:28:42.080]   encouraged a lot of different companies to grow over the course of five to ten years.
[00:28:42.080 --> 00:28:48.520]   So I think that right now, the issue is that if you see a post on the Facebook news feed
[00:28:48.520 --> 00:28:53.400]   from the New York Times and then you see another one from something called the Denver Guardian,
[00:28:53.400 --> 00:29:00.360]   which was a domain that was once registered by Russian operatives, then the layman person
[00:29:00.360 --> 00:29:04.440]   or the average person cannot tell the difference between the New York Times article and that
[00:29:04.440 --> 00:29:05.520]   Denver Guardian article.
[00:29:05.520 --> 00:29:08.680]   They don't understand why one is more credible than the other.
[00:29:08.680 --> 00:29:14.280]   There's inherent literacy problems in the US and frankly just education problems.
[00:29:14.280 --> 00:29:21.040]   And so I think people, because Facebook treats those links equally, people are reading this
[00:29:21.040 --> 00:29:26.480]   type of information and thinking that it looks the same as something that's legitimate and
[00:29:26.480 --> 00:29:29.360]   therefore there must be some truth to it.
[00:29:29.360 --> 00:29:33.080]   And a lot of people have a hard time discerning what's fake from what's real, obviously.
[00:29:33.080 --> 00:29:36.560]   Well, we don't want to go back to gatekeepers and say, "Well, the only real news is from
[00:29:36.560 --> 00:29:43.680]   the media centers of the country, LA and New York and everything else is fake news, Denver,
[00:29:43.680 --> 00:29:49.440]   what could possibly good come out of Denver or your blog or Mashable or...
[00:29:49.440 --> 00:29:50.440]   Yeah.
[00:29:50.440 --> 00:29:54.320]   Well, so we don't want to go back to those days where they were a handful of media or
[00:29:54.320 --> 00:29:55.560]   do we?
[00:29:55.560 --> 00:29:59.400]   I think that the invisible hand will solve this problem and that ultimately you're going
[00:29:59.400 --> 00:30:02.320]   to have to pay money for good information.
[00:30:02.320 --> 00:30:07.240]   And these websites that, you know, this is maybe a little bit weird for someone that's
[00:30:07.240 --> 00:30:13.080]   working at Mashable to say, but a lot of these new media companies that deliver free information
[00:30:13.080 --> 00:30:15.560]   will likely be less trusted over the course of time.
[00:30:15.560 --> 00:30:17.160]   There are definitely some exceptions to the rule.
[00:30:17.160 --> 00:30:22.120]   I think the larger publishers that have a history of breaking news and having good information
[00:30:22.120 --> 00:30:28.080]   may continue to live on, but there are dozens and dozens of websites that have millions
[00:30:28.080 --> 00:30:33.160]   of followers on Facebook that exist through clicks right now.
[00:30:33.160 --> 00:30:35.520]   And I think that that will matter less in the future.
[00:30:35.520 --> 00:30:37.200]   I think that the people...
[00:30:37.200 --> 00:30:38.480]   I vote with my dollars.
[00:30:38.480 --> 00:30:43.280]   So I pay for a subscription to Wired to ours, Technica, to the New York Times, the Washington
[00:30:43.280 --> 00:30:46.720]   Post, to the information.
[00:30:46.720 --> 00:30:52.720]   So it's not in the sense of a traditional gatekeeper, but I reward the ones that I think are quality
[00:30:52.720 --> 00:30:55.520]   because I know it costs money to make good journalism.
[00:30:55.520 --> 00:30:56.600]   Maybe that's what we need to do.
[00:30:56.600 --> 00:30:57.600]   Well, yeah.
[00:30:57.600 --> 00:31:00.720]   And the people that really care about news typically subscribe to...
[00:31:00.720 --> 00:31:01.720]   We're a minority.
[00:31:01.720 --> 00:31:06.080]   In least one or multiple publishers.
[00:31:06.080 --> 00:31:07.080]   So I think...
[00:31:07.080 --> 00:31:09.000]   And that's been the case for a very long time in history.
[00:31:09.000 --> 00:31:10.840]   I think that will...
[00:31:10.840 --> 00:31:14.440]   Free news is going away from what I can tell.
[00:31:14.440 --> 00:31:18.520]   I don't think that the click-based economy will continue to exist.
[00:31:18.520 --> 00:31:21.960]   And I think the newsfeed will change to reflect that ultimately.
[00:31:21.960 --> 00:31:27.280]   But right now, the issue is that Facebook is presenting this information in the same
[00:31:27.280 --> 00:31:28.280]   exact way.
[00:31:28.280 --> 00:31:32.760]   A New York Times article looks the exact same as something that is written on some random
[00:31:32.760 --> 00:31:36.160]   blog that has been quickly created by a Russian operative.
[00:31:36.160 --> 00:31:38.200]   And so that allowed for misinformation to spread.
[00:31:38.200 --> 00:31:40.240]   It continues to this day.
[00:31:40.240 --> 00:31:44.320]   And I think ultimately people will realize, as used to be the case, when I was...
[00:31:44.320 --> 00:31:48.120]   Growing up in the 90s on AOL, you can't trust everything on the internet.
[00:31:48.120 --> 00:31:49.280]   And so people will...
[00:31:49.280 --> 00:31:51.680]   That will become the presiding mantra of the newsfeed.
[00:31:51.680 --> 00:31:53.800]   You can't trust everything on Facebook.
[00:31:53.800 --> 00:31:58.360]   And so ultimately the people that want good information are going to pay for subscriptions
[00:31:58.360 --> 00:32:04.120]   to the Times or the Washington Post or the Journal or the Information or Wired magazine.
[00:32:04.120 --> 00:32:10.480]   Or I think a lot of these digitally native new media publishers will also slowly move
[00:32:10.480 --> 00:32:11.800]   behind a paywall.
[00:32:11.800 --> 00:32:16.880]   And ask for... there will be a premium version of these services that give people more insight
[00:32:16.880 --> 00:32:19.160]   or more information.
[00:32:19.160 --> 00:32:21.880]   So ultimately, if you want good info, you're going to have to pay for it.
[00:32:21.880 --> 00:32:23.880]   And I think that's basically the case right now.
[00:32:23.880 --> 00:32:28.640]   I think if you talk to people that are making decisions based on the news, like some of
[00:32:28.640 --> 00:32:31.680]   that works at a hedge fund or on Wall Street, they're surprised for a lot of...
[00:32:31.680 --> 00:32:32.680]   They're paid for their information.
[00:32:32.680 --> 00:32:33.680]   Yeah, they...
[00:32:33.680 --> 00:32:34.680]   Absolutely.
[00:32:34.680 --> 00:32:37.400]   That's why you can get a 150 bucks a year for the Wall Street Journal because it makes
[00:32:37.400 --> 00:32:38.400]   you money.
[00:32:38.400 --> 00:32:40.200]   That's why Mr. Bloomberg is a billionaire.
[00:32:40.200 --> 00:32:41.200]   Yeah.
[00:32:41.200 --> 00:32:42.200]   Exactly right.
[00:32:42.200 --> 00:32:44.200]   I mean, I think...
[00:32:44.200 --> 00:32:45.480]   You've been in publishing for ages.
[00:32:45.480 --> 00:32:48.400]   I mean, you've seen the ups and downs of this area.
[00:32:48.400 --> 00:32:54.240]   Yes, but the ups lasted a long time and the downward slope has lasted for a long time.
[00:32:54.240 --> 00:32:56.240]   It's been a quick down, though, I gotta say.
[00:32:56.240 --> 00:33:00.680]   I do feel like the people who are willing to pay for information are ready.
[00:33:00.680 --> 00:33:02.520]   They're already pretty savvy.
[00:33:02.520 --> 00:33:04.760]   Consumers of information by definition.
[00:33:04.760 --> 00:33:06.120]   And a tiny minority.
[00:33:06.120 --> 00:33:07.120]   And a tiny minority.
[00:33:07.120 --> 00:33:11.680]   And I'm a little scared that the masses will have even less access to good information
[00:33:11.680 --> 00:33:14.000]   if everything goes behind a paywall.
[00:33:14.000 --> 00:33:15.000]   That's a very good point.
[00:33:15.000 --> 00:33:21.320]   One of the reasons we're free and it was Mike Elgin who catalyzed this for me is for
[00:33:21.320 --> 00:33:22.320]   democratic reasons.
[00:33:22.320 --> 00:33:25.840]   I want everybody to get the information that we offer.
[00:33:25.840 --> 00:33:26.840]   So we do...
[00:33:26.840 --> 00:33:29.120]   I've always done advertising supported media.
[00:33:29.120 --> 00:33:34.760]   I don't know what the future for advertising supported media is because the problem...
[00:33:34.760 --> 00:33:35.760]   I'll tell you why.
[00:33:35.760 --> 00:33:38.040]   It's hard for advertising supported media.
[00:33:38.040 --> 00:33:46.200]   Google and Facebook is if you are an advertiser and you can get the kind of granular information
[00:33:46.200 --> 00:33:52.680]   Facebook and Google offers about your ad buy and it's very efficient, it just makes it
[00:33:52.680 --> 00:33:56.360]   much less interesting to buy media.
[00:33:56.360 --> 00:34:01.400]   And I think that we're already seeing a flight of dollars from media into Google and Facebook
[00:34:01.400 --> 00:34:03.240]   and they're eating it all up.
[00:34:03.240 --> 00:34:04.480]   And this is something really...
[00:34:04.480 --> 00:34:07.200]   I mean this is inside baseball but I'll say it anyway.
[00:34:07.200 --> 00:34:14.220]   This is a cause for concern because what that means is the economics of it are incentivizing
[00:34:14.220 --> 00:34:21.200]   privacy invasion, surveillance capitalism if you will, and de-incentivizing reporting
[00:34:21.200 --> 00:34:23.920]   information services.
[00:34:23.920 --> 00:34:27.880]   And the flight of dollars to Google started like 15 years ago.
[00:34:27.880 --> 00:34:28.880]   It's accelerated.
[00:34:28.880 --> 00:34:30.920]   We're lucky.
[00:34:30.920 --> 00:34:33.200]   We have a good advertiser base.
[00:34:33.200 --> 00:34:38.760]   But I worry, I really do because hey you can get Facebook cheap.
[00:34:38.760 --> 00:34:42.520]   I am looking for reasons for optimism at least.
[00:34:42.520 --> 00:34:46.960]   The Facebooks and the YouTubes of the world are acknowledging the embarrassment responsibility
[00:34:46.960 --> 00:34:50.120]   which pretty much was not true a couple of years ago.
[00:34:50.120 --> 00:34:53.560]   They're not going to change their medium or their model.
[00:34:53.560 --> 00:34:58.720]   No, but they are doing things like de-platforming people like Alex Jones which actually...
[00:34:58.720 --> 00:35:01.400]   Alex Jones has more trouble reaching the masses.
[00:35:01.400 --> 00:35:04.960]   It's actually harder for him to cause pain for people.
[00:35:04.960 --> 00:35:07.520]   And Paris, I think you worry about this too.
[00:35:07.520 --> 00:35:12.320]   I worry about these big companies deciding what's good and what's not good, what's wholesome
[00:35:12.320 --> 00:35:15.160]   and what's not wholesome, which people should hear and what they shouldn't hear.
[00:35:15.160 --> 00:35:19.000]   We can probably all agree that not all but many of us can agree that infowards is not
[00:35:19.000 --> 00:35:20.480]   a good source.
[00:35:20.480 --> 00:35:24.680]   But I don't know if I want Google to decide what news I should get.
[00:35:24.680 --> 00:35:25.680]   You think...
[00:35:25.680 --> 00:35:28.880]   Yeah, I think it stems from the fact that when we're talking about the Facebooks and
[00:35:28.880 --> 00:35:36.480]   Google's world, we view them not as companies with their own judgment and rules and editorial
[00:35:36.480 --> 00:35:43.200]   control baked into their terms of service since their beginning, but as essential interfaces
[00:35:43.200 --> 00:35:48.920]   of the internet, essential components and parts of how we see the world and view and
[00:35:48.920 --> 00:35:50.840]   experience information.
[00:35:50.840 --> 00:35:56.880]   And when it gets to a size, something of that size, all of the questions and answers
[00:35:56.880 --> 00:36:02.080]   become much more difficult to even sort out because, okay, yes, it is...
[00:36:02.080 --> 00:36:06.720]   I think it's good that places like Facebook or YouTube have deep platforms, someone like
[00:36:06.720 --> 00:36:07.720]   Alex Jones or...
[00:36:07.720 --> 00:36:09.360]   Yeah, we can all agree with that.
[00:36:09.360 --> 00:36:10.800]   We can agree on that.
[00:36:10.800 --> 00:36:16.360]   But it's also like when you give agree that these companies should have that power, what
[00:36:16.360 --> 00:36:21.480]   stops them from deep platforming someone else or doing something in error or creating new
[00:36:21.480 --> 00:36:25.280]   rules about what sort of content they think is acceptable or not.
[00:36:25.280 --> 00:36:28.360]   And people who love Alex Jones feel totally disenfranchised by that.
[00:36:28.360 --> 00:36:32.760]   Of course, Sundar Pichai, the CEO of Google, was called on the carpet in Washington, DC
[00:36:32.760 --> 00:36:35.840]   this week, spoke to Congress.
[00:36:35.840 --> 00:36:37.320]   So Loughgren, who is his...
[00:36:37.320 --> 00:36:40.560]   I think his member of Congress said, what she's using...
[00:36:40.560 --> 00:36:43.600]   She was using this as a faux example.
[00:36:43.600 --> 00:36:48.160]   Why is it that when people search for idiot pictures of Donald Trump show up?
[00:36:48.160 --> 00:36:52.600]   And Pichai said, "Well, but that's us not editorializing.
[00:36:52.600 --> 00:36:58.280]   If you wanted us to stop that, we would have to editorialize."
[00:36:58.280 --> 00:37:05.000]   I think just before we hopped on this call, I was looking at the conspiracy theory subreddit
[00:37:05.000 --> 00:37:08.480]   on Reddit just as one does for work.
[00:37:08.480 --> 00:37:14.120]   And the big topic of the day there was the fact that if you entered in Google, Hillary
[00:37:14.120 --> 00:37:20.800]   Clinton EM, as you're supposed to type like emails, no results related to Hillary Clinton
[00:37:20.800 --> 00:37:23.640]   emails will pop up in the suggested search bar.
[00:37:23.640 --> 00:37:28.680]   But if you enter the same into all the other search engines, then email related things
[00:37:28.680 --> 00:37:29.680]   will happen.
[00:37:29.680 --> 00:37:32.560]   But then somebody in the comments points it out, if you do the same with Donald Trump,
[00:37:32.560 --> 00:37:33.560]   are you?
[00:37:33.560 --> 00:37:35.680]   Nothing related to Russia pops up in the suggestions.
[00:37:35.680 --> 00:37:41.400]   And it's because Google is trying to walk this line between showing the content that
[00:37:41.400 --> 00:37:45.160]   people are actually searching for and the things that are actually happening.
[00:37:45.160 --> 00:37:46.160]   But that's troubling.
[00:37:46.160 --> 00:37:47.800]   And not offending people.
[00:37:47.800 --> 00:37:54.680]   If Google were just doing search results unedited, I think Hillary Clinton EM should turn up more
[00:37:54.680 --> 00:37:57.720]   than emoji and employment, which is all I get.
[00:37:57.720 --> 00:37:58.720]   Yeah.
[00:37:58.720 --> 00:37:59.720]   Yeah.
[00:37:59.720 --> 00:38:00.720]   That's all you get.
[00:38:00.720 --> 00:38:01.720]   Yeah.
[00:38:01.720 --> 00:38:02.720]   So that's editorializing.
[00:38:02.720 --> 00:38:03.720]   It is editorializing.
[00:38:03.720 --> 00:38:04.720]   It is editorializing.
[00:38:04.720 --> 00:38:06.880]   It's totally cleaned up.
[00:38:06.880 --> 00:38:08.720]   They've taken out all the stuff.
[00:38:08.720 --> 00:38:12.720]   Google is acknowledging to take some of them pleasant stuff out of complete.
[00:38:12.720 --> 00:38:13.720]   Yeah.
[00:38:13.720 --> 00:38:17.400]   Because I forget so many stories about what if like when if you search black people, then
[00:38:17.400 --> 00:38:22.680]   it'll pull up black people and some racial slur or some sort of gorillas.
[00:38:22.680 --> 00:38:27.120]   And so they made it impossible to search for gorillas or black people.
[00:38:27.120 --> 00:38:29.280]   That I understand.
[00:38:29.280 --> 00:38:34.800]   But this is the problem is if you're a search engine in theory, you without bias reflect
[00:38:34.800 --> 00:38:38.920]   the state of the internet, which is biased, which is biased.
[00:38:38.920 --> 00:38:42.360]   That's why you get idiot.
[00:38:42.360 --> 00:38:47.080]   But on the other hand, do we want Google to decide what's true and that's not they
[00:38:47.080 --> 00:38:49.960]   wield huge amount of power?
[00:38:49.960 --> 00:38:51.360]   Well, if it's the only item.
[00:38:51.360 --> 00:38:56.800]   The internet is Google from in most cases, at least people use Google.
[00:38:56.800 --> 00:38:57.800]   Yeah.
[00:38:57.800 --> 00:39:03.080]   I think that the idea that there is now becoming some editorializing or that there's some editorial
[00:39:03.080 --> 00:39:06.800]   procedure from places like Google and Facebook is just a little bit silly because they've
[00:39:06.800 --> 00:39:08.720]   always been editorializing.
[00:39:08.720 --> 00:39:13.280]   I think in my, you know, the thing that I've sort of advocated for is algorithmic transparency.
[00:39:13.280 --> 00:39:16.920]   I think people deserve to understand why they're seeing things in the newsfeed, why they're
[00:39:16.920 --> 00:39:19.120]   seeing things in Google search.
[00:39:19.120 --> 00:39:25.120]   And that would, that would, I guess, resolve some of the anxiety around this.
[00:39:25.120 --> 00:39:29.640]   But, you know, but the reality is that even before deplatforming, even before a lot of
[00:39:29.640 --> 00:39:36.840]   this stuff, Facebook was already algorithmically demoting and prioritizing certain bits of information.
[00:39:36.840 --> 00:39:42.000]   So, you know, in the case of some of the reporting I did in 2016, we found that they were giving
[00:39:42.000 --> 00:39:45.720]   preferential treatment to 10 publishers, 10 national news publishers.
[00:39:45.720 --> 00:39:50.120]   So the example that I often use is in the case of the Laquan McDonald shooting.
[00:39:50.120 --> 00:39:51.480]   There was a famous shooting in Chicago.
[00:39:51.480 --> 00:39:52.840]   It became national news.
[00:39:52.840 --> 00:39:59.800]   If the Chicago Tribune or if WGN or if the Chicago Sun Times all award winning publishers,
[00:39:59.800 --> 00:40:05.160]   if any of those publishers covered that shooting and broke news on that scandal, it wasn't
[00:40:05.160 --> 00:40:10.200]   allowed to show up in the trending newsfeed because it hadn't been covered by one of 10
[00:40:10.200 --> 00:40:15.120]   of these national publishers, which included the Wall Street Journal, New York Times, CNN,
[00:40:15.120 --> 00:40:18.320]   and a lot of East Coast, East Coast entities.
[00:40:18.320 --> 00:40:21.480]   And so the reason why I was interested in that was because I grew up in Chicago.
[00:40:21.480 --> 00:40:24.360]   I expected to work at the Chicago Tribune my entire life.
[00:40:24.360 --> 00:40:30.280]   And I saw this as part of the reason why local publishers were dying, places like the Baltimore
[00:40:30.280 --> 00:40:32.520]   Sun, the Denver Post, the Chicago Tribune.
[00:40:32.520 --> 00:40:41.080]   There's so many good city newspapers across the country that have had advertising dollars
[00:40:41.080 --> 00:40:48.600]   taken from them because of Facebook's machine, because of the way that Facebook runs its
[00:40:48.600 --> 00:40:50.120]   business.
[00:40:50.120 --> 00:40:53.920]   And so in my opinion, it sort of led to the death of a lot of these newspapers and publishers
[00:40:53.920 --> 00:41:00.160]   because they were never allowed to be featured in this trending newsfeed.
[00:41:00.160 --> 00:41:03.120]   And so Facebook was giving preferential treatment to these publishers.
[00:41:03.120 --> 00:41:09.600]   I think the bigger, I don't think it's wrong for Facebook to have made that decision, but
[00:41:09.600 --> 00:41:11.120]   they didn't disclose it, right?
[00:41:11.120 --> 00:41:16.240]   They weren't upfront with the fact that unless things reached one of these 10 publishers,
[00:41:16.240 --> 00:41:17.560]   it would not be allowed to trend.
[00:41:17.560 --> 00:41:18.560]   And so that's just one example.
[00:41:18.560 --> 00:41:20.040]   These decisions are constant.
[00:41:20.040 --> 00:41:22.080]   They're being made all the time.
[00:41:22.080 --> 00:41:26.480]   They can't be transparent because they're algorithmic.
[00:41:26.480 --> 00:41:30.240]   We see all the time people saying, look at artificial intelligence as being mistrained.
[00:41:30.240 --> 00:41:33.120]   You can't identify people of color because it's all trained on white people.
[00:41:33.120 --> 00:41:37.320]   And so it's got a huge false positive rate among black people.
[00:41:37.320 --> 00:41:40.840]   I mean, it's an endless story.
[00:41:40.840 --> 00:41:42.920]   This is the nature of the world we live in.
[00:41:42.920 --> 00:41:45.800]   Is it just though that it's always been the nature of the world we live in and we just
[00:41:45.800 --> 00:41:47.320]   now can see it?
[00:41:47.320 --> 00:41:53.480]   Or that's the fundamental question for this whole segment, which we're going to wrap soon.
[00:41:53.480 --> 00:41:54.740]   Is this unusual?
[00:41:54.740 --> 00:41:58.920]   Is this something new or is this just we know about it now because of better.
[00:41:58.920 --> 00:42:03.600]   I think there's this idea that there's this some sort of neutral platform that we're
[00:42:03.600 --> 00:42:05.280]   supposed to return back to.
[00:42:05.280 --> 00:42:06.280]   It doesn't exist.
[00:42:06.280 --> 00:42:07.280]   There was never did.
[00:42:07.280 --> 00:42:13.760]   It's because these social media companies and tech companies from Silicon Valley, especially
[00:42:13.760 --> 00:42:18.240]   kind of pride themselves in the early days on the free and open internet and being pro
[00:42:18.240 --> 00:42:20.440]   free speech, whatever that meant.
[00:42:20.440 --> 00:42:26.360]   But being kind of a technological digital version of the public square when in reality,
[00:42:26.360 --> 00:42:29.160]   they're as far from that as possible.
[00:42:29.160 --> 00:42:35.280]   And so people are trying to return back to some unrealistic and possible idea.
[00:42:35.280 --> 00:42:36.280]   It's nostalgia.
[00:42:36.280 --> 00:42:38.200]   It's the it's a lie.
[00:42:38.200 --> 00:42:40.200]   It's all I was never.
[00:42:40.200 --> 00:42:41.440]   The fault is a lie.
[00:42:41.440 --> 00:42:42.440]   It's all a lie.
[00:42:42.440 --> 00:42:45.640]   The old gatekeeper is also a million problems.
[00:42:45.640 --> 00:42:47.840]   They were worse and they were less transparent.
[00:42:47.840 --> 00:42:50.840]   I would not go back to that area even if we could.
[00:42:50.840 --> 00:42:51.840]   You worked it this week.
[00:42:51.840 --> 00:42:52.840]   You worked it a time.
[00:42:52.840 --> 00:42:53.840]   Time.
[00:42:53.840 --> 00:42:54.840]   I'm sorry.
[00:42:54.840 --> 00:42:58.840]   A great news magazine that's had its ups and downs and right in parallel with all of
[00:42:58.840 --> 00:43:00.840]   this, you know, and.
[00:43:00.840 --> 00:43:01.840]   They're having some.
[00:43:01.840 --> 00:43:03.600]   They're having an up right now now that Mark Benney off.
[00:43:03.600 --> 00:43:04.600]   Yes.
[00:43:04.600 --> 00:43:05.600]   Bottom.
[00:43:05.600 --> 00:43:08.360]   He's actually willing to invest in time, which time Warner was not when I was.
[00:43:08.360 --> 00:43:11.880]   That's still a weird model where people like Jeff Bezos buys a Washington Post or Ben
[00:43:11.880 --> 00:43:14.520]   off buys time magazine.
[00:43:14.520 --> 00:43:16.000]   It's the it's the patronage model.
[00:43:16.000 --> 00:43:22.880]   We're going back to Venice in the 16th century where the rich people decided what media outlets
[00:43:22.880 --> 00:43:26.640]   exist purely because they're willing to spend lose money on them.
[00:43:26.640 --> 00:43:28.320]   I don't know if that's a good model either.
[00:43:28.320 --> 00:43:30.880]   I don't know what it's it's hard to say.
[00:43:30.880 --> 00:43:33.280]   There's somebody in the chat room.
[00:43:33.280 --> 00:43:35.960]   I presume native geek is a native American.
[00:43:35.960 --> 00:43:41.640]   Here she writes the protests in North Dakota over the XL, the Keystone Pipeline went on for
[00:43:41.640 --> 00:43:43.920]   months before anybody paid attention.
[00:43:43.920 --> 00:43:48.960]   Facebook was pivotal in getting native Americans from across the country to protest that pipeline.
[00:43:48.960 --> 00:43:53.000]   So that's I mean, you can for every negative, there's also a positive counter example for
[00:43:53.000 --> 00:43:54.480]   all of this.
[00:43:54.480 --> 00:43:59.160]   So it's very difficult to say what the right thing to do is all I all I think we ever do
[00:43:59.160 --> 00:44:00.160]   here.
[00:44:00.160 --> 00:44:03.000]   That's a little frustrating to me is observe.
[00:44:03.000 --> 00:44:05.320]   We never we've not yet come up with a solution.
[00:44:05.320 --> 00:44:06.400]   Try as we might.
[00:44:06.400 --> 00:44:08.840]   Let's take a little bit of a break.
[00:44:08.840 --> 00:44:10.160]   Michael Nunez is here for Mashable.
[00:44:10.160 --> 00:44:12.640]   It's great to have you deputy tech editor.
[00:44:12.640 --> 00:44:14.040]   We're going to talk some tech.
[00:44:14.040 --> 00:44:16.800]   Actually, I'm going to let you get prepared because we're going to talk about that super
[00:44:16.800 --> 00:44:18.400]   micro story.
[00:44:18.400 --> 00:44:20.800]   There's new news.
[00:44:20.800 --> 00:44:21.800]   Also Paris Martin.
[00:44:21.800 --> 00:44:23.280]   No, she's at Wired.
[00:44:23.280 --> 00:44:28.760]   Staff writer there does if you get a chance to just search her byline on Wired.
[00:44:28.760 --> 00:44:30.200]   Great story after great story.
[00:44:30.200 --> 00:44:34.720]   You're really doing fantastic job and speaking of great stories.
[00:44:34.720 --> 00:44:40.000]   He's the original technologist, Harry McCracken technology editor at Fast Company.
[00:44:40.000 --> 00:44:42.760]   I show today brought to you by Avnet.
[00:44:42.760 --> 00:44:43.960]   You may not know the name.
[00:44:43.960 --> 00:44:46.640]   I bet you do actually AVNET.
[00:44:46.640 --> 00:44:51.400]   They make great electronic components, but they want to tell this is I'm really happy
[00:44:51.400 --> 00:44:53.520]   to do this for the holiday season.
[00:44:53.520 --> 00:45:00.480]   I want to thank AVNET for doing this a happy story to make you feel better about the world.
[00:45:00.480 --> 00:45:07.440]   How does a groundbreaking technology incubator debut its latest idea for solving life's absurdities
[00:45:07.440 --> 00:45:13.040]   by leveraging an end to end ecosystem that turns ideas into marketable products?
[00:45:13.040 --> 00:45:16.480]   There's a company called Not Impossible Labs.
[00:45:16.480 --> 00:45:18.400]   I love the name Not Impossible Labs.
[00:45:18.400 --> 00:45:22.080]   They had an idea for a technology that could change live music.
[00:45:22.080 --> 00:45:27.200]   They wanted to bring the experience of a concert to a group of people who traditionally couldn't
[00:45:27.200 --> 00:45:30.120]   enjoy it, the deaf community.
[00:45:30.120 --> 00:45:32.400]   We love live music and we take it for granted.
[00:45:32.400 --> 00:45:38.360]   It's an amazing experience and this is important all the senses.
[00:45:38.360 --> 00:45:44.240]   The challenge lies in making concerts and live music more inclusive for the deaf.
[00:45:44.240 --> 00:45:49.720]   With AVNET as their guide, Not Impossible Labs, their idea evolved to one of the most
[00:45:49.720 --> 00:45:54.760]   sophisticated wearables on the market, helping bring a shared live concert experience to
[00:45:54.760 --> 00:45:56.000]   some great people.
[00:45:56.000 --> 00:46:01.560]   You can see this if you go to the website AVNET, AVNET.com/music1.
[00:46:01.560 --> 00:46:03.600]   It's music in the number one.
[00:46:03.600 --> 00:46:05.560]   It's called Music Not Impossible.
[00:46:05.560 --> 00:46:10.920]   The product allows deaf and hearing concert goers alike to literally feel live music through
[00:46:10.920 --> 00:46:16.560]   advanced vibration technology and then experience it together for the first time.
[00:46:16.560 --> 00:46:18.360]   Dance, baby!
[00:46:18.360 --> 00:46:20.400]   These wearables are truly wearables.
[00:46:20.400 --> 00:46:26.560]   It's a vest, components that said vibrations through the ankles, wrists and chest.
[00:46:26.560 --> 00:46:30.720]   The hearing able to receive music vibrations through the ears, but attendees who are deaf
[00:46:30.720 --> 00:46:34.760]   and wearing these wearables got the same vibrations throughout other parts of the body,
[00:46:34.760 --> 00:46:38.800]   allowing many of them to respond live music alongside everybody else.
[00:46:38.800 --> 00:46:41.720]   For many people, this is so cool.
[00:46:41.720 --> 00:46:45.200]   It was a first for them, an innovation that literally opened a whole new world of music
[00:46:45.200 --> 00:46:46.600]   exploration.
[00:46:46.600 --> 00:46:51.480]   For those who don't hear it in a traditional sense, AVNET and Not Impossible Labs revealed
[00:46:51.480 --> 00:46:52.480]   music.
[00:46:52.480 --> 00:46:56.720]   Not Impossible at the Life is Beautiful Music Festival in Las Vegas was a big hit.
[00:46:56.720 --> 00:47:00.840]   This kind of innovation was brought to the finish line because of AVNET.
[00:47:00.840 --> 00:47:05.720]   On this holiday season, instead of AVNET touting their wearables and they do great stuff,
[00:47:05.720 --> 00:47:08.240]   they thought it would be great if you could just share this story.
[00:47:08.240 --> 00:47:12.160]   Feel the rhythm at AVNET.com/music1.
[00:47:12.160 --> 00:47:13.160]   AVNET.com/music1.
[00:47:13.160 --> 00:47:20.160]   That's a great holiday story we thank AVNET for their support and for, I think this is
[00:47:20.160 --> 00:47:21.160]   really cool.
[00:47:21.160 --> 00:47:24.360]   I don't think we've ever had an advertiser do this and I thank you AVNET.
[00:47:24.360 --> 00:47:28.160]   Instead of flogging their wares, which I would be happy to flog because they're good
[00:47:28.160 --> 00:47:32.880]   wares, they decided to talk about some good that has happened in the world.
[00:47:32.880 --> 00:47:35.600]   See it does not make us feel better now.
[00:47:35.600 --> 00:47:37.640]   Super Micro.
[00:47:37.640 --> 00:47:41.200]   We go back a few months to the Bloomberg story, which was a conundrum to everybody.
[00:47:41.200 --> 00:47:45.240]   Actually, I defended Bloomberg because I think Bloomberg is a great journalistic organization.
[00:47:45.240 --> 00:47:50.400]   I know people who know the two authors of the Super Micro story, the editor in chief who
[00:47:50.400 --> 00:47:55.800]   was supervised that story over a long period of time has great credentials, came from the
[00:47:55.800 --> 00:47:58.560]   economist, really knows his stuff.
[00:47:58.560 --> 00:48:05.360]   I couldn't think of any way that was 17 anonymous sources that Bloomberg business we could
[00:48:05.360 --> 00:48:06.360]   have gotten this wrong.
[00:48:06.360 --> 00:48:07.360]   The big hack.
[00:48:07.360 --> 00:48:14.160]   Super story on Bloomberg, how China used a tiny chip to infiltrate US companies.
[00:48:14.160 --> 00:48:22.800]   We always thought, well, if this is true, there'll be evidence somehow we'll find out.
[00:48:22.800 --> 00:48:28.680]   Well, Super Micro went out and asked for an audit.
[00:48:28.680 --> 00:48:34.480]   They went to a company that specializes in going through this kind of stuff, Nardello
[00:48:34.480 --> 00:48:37.200]   and company.
[00:48:37.200 --> 00:48:42.120]   Nardello specifically tested samples of the motherboards supplied to, as in the Bloomberg
[00:48:42.120 --> 00:48:43.560]   story, Apple and Amazon.
[00:48:43.560 --> 00:48:49.960]   Remember, Amazon's Elemental Apple servers that supposedly had this rice sized chip that
[00:48:49.960 --> 00:48:53.560]   was phoning home, phoning the Chinese government.
[00:48:53.560 --> 00:48:57.560]   Nardello tested older motherboards alongside current versions.
[00:48:57.560 --> 00:49:01.760]   And according to Super Micro found no evidence of spy chips on any of them.
[00:49:01.760 --> 00:49:06.240]   The company also checked design files and software to see if there was evidence of tampering
[00:49:06.240 --> 00:49:09.120]   with either, but found nothing there either.
[00:49:09.120 --> 00:49:15.400]   Now, you remember that the US intelligence community, Apple, Amazon strongly denied the
[00:49:15.400 --> 00:49:16.400]   story.
[00:49:16.400 --> 00:49:17.920]   Tim Cook asked for a retraction.
[00:49:17.920 --> 00:49:21.360]   Bloomberg has stood by the story all this time.
[00:49:21.360 --> 00:49:28.360]   Super Micro now says they're reviewing their legal options.
[00:49:28.360 --> 00:49:30.760]   What do you make of this one?
[00:49:30.760 --> 00:49:33.080]   This is the other shoe.
[00:49:33.080 --> 00:49:35.440]   Now I'm going to have to step back and say, how did this happen?
[00:49:35.440 --> 00:49:38.400]   How could Bloomberg get this so wrong, Harry?
[00:49:38.400 --> 00:49:44.960]   Well, Bloomberg reported on this audit and had to say that Bloomberg has said in the past
[00:49:44.960 --> 00:49:46.120]   it stands by its story.
[00:49:46.120 --> 00:49:50.360]   I don't believe Bloomberg has really responded to this audit specifically.
[00:49:50.360 --> 00:49:52.760]   No, I mean, it came out Wednesday.
[00:49:52.760 --> 00:49:58.760]   So they're probably thinking about what they're going to say.
[00:49:58.760 --> 00:50:03.200]   Bloomberg hasn't had all that much to say about its story other than it stands by it.
[00:50:03.200 --> 00:50:08.960]   And I've seen little evidence that anybody else who's looked at this has concluded that
[00:50:08.960 --> 00:50:11.440]   Bloomberg is probably right.
[00:50:11.440 --> 00:50:15.240]   So I think it's a challenge for people like us to form opinions on this just because at
[00:50:15.240 --> 00:50:18.040]   least I'm not an expert on this stuff.
[00:50:18.040 --> 00:50:20.640]   And I want to be careful about being positive.
[00:50:20.640 --> 00:50:22.440]   I know who's right here.
[00:50:22.440 --> 00:50:23.440]   But you--
[00:50:23.440 --> 00:50:26.120]   Well, what you are an expert on is the editorial process.
[00:50:26.120 --> 00:50:27.960]   True.
[00:50:27.960 --> 00:50:35.320]   And from everything I know, which mostly comes from movies, I mean, I saw the post.
[00:50:35.320 --> 00:50:37.320]   That's it.
[00:50:37.320 --> 00:50:38.320]   You're a journalist.
[00:50:38.320 --> 00:50:39.320]   I'm an expert.
[00:50:39.320 --> 00:50:41.400]   All the presidents, man, I saw that.
[00:50:41.400 --> 00:50:47.240]   Remember, during Watergate and all the presidents, man and Woodward and Bernstein, they had to,
[00:50:47.240 --> 00:50:54.440]   first of all, their editor in chief said, look, you're going to have to have two stories
[00:50:54.440 --> 00:50:56.960]   for every-- two sources for everything.
[00:50:56.960 --> 00:50:59.200]   You know, they vetted everything.
[00:50:59.200 --> 00:51:02.520]   You've got to be sure this kid-- let me see your notebooks.
[00:51:02.520 --> 00:51:05.120]   17 sources in this story.
[00:51:05.120 --> 00:51:08.800]   And presumably, Bloomberg was doing exactly the same thing.
[00:51:08.800 --> 00:51:09.800]   Yes.
[00:51:09.800 --> 00:51:15.520]   And when you were reporting on something so explosive, the bar goes up even higher.
[00:51:15.520 --> 00:51:19.120]   And Bloomberg is not a like some new fan, gold, funky blog.
[00:51:19.120 --> 00:51:20.120]   Right.
[00:51:20.120 --> 00:51:21.120]   Extremely old school.
[00:51:21.120 --> 00:51:22.120]   Yes.
[00:51:22.120 --> 00:51:24.920]   So we-- so this is why it's a puzzle.
[00:51:24.920 --> 00:51:29.400]   Here's my theory, Paris, and I'll let you and Michael jump in anytime you want.
[00:51:29.400 --> 00:51:35.080]   But my theory is that Bloomberg got-- make a super-- Bloomberg business week got played.
[00:51:35.080 --> 00:51:39.640]   That's-- that somebody in the-- there has been, and we'll talk about some of the other
[00:51:39.640 --> 00:51:44.000]   stories too, a concerted drumbeat against China.
[00:51:44.000 --> 00:51:48.520]   And I would-- if you want to interpret this as a blow to the supply chain, as a blow to
[00:51:48.520 --> 00:51:56.760]   the security of the supply chain that originates most of it in China, and we know the US government,
[00:51:56.760 --> 00:52:01.080]   particularly the administration, is not a fan of trade with China.
[00:52:01.080 --> 00:52:07.520]   You can almost put together a scenario where 17 intelligence officials-- in a concerted
[00:52:07.520 --> 00:52:12.960]   fashion-- now, of course, Bloomberg said there were also people from Amazon and Apple involved
[00:52:12.960 --> 00:52:13.960]   in this.
[00:52:13.960 --> 00:52:20.200]   So a concerted fashion lied to Bloomberg business week, tricked them into publishing this story
[00:52:20.200 --> 00:52:23.120]   because it served their political agenda.
[00:52:23.120 --> 00:52:24.760]   Is that possible?
[00:52:24.760 --> 00:52:31.160]   I'll go first here, because I've-- you know, people have said similar things about stories
[00:52:31.160 --> 00:52:32.160]   that I've written.
[00:52:32.160 --> 00:52:39.400]   I think that when you have 17 people confirming a lot of the same information, I don't know
[00:52:39.400 --> 00:52:42.840]   that all 17 confirmed that this bug was implanted.
[00:52:42.840 --> 00:52:44.960]   But certainly there was an issue around this, right?
[00:52:44.960 --> 00:52:52.000]   I think that seems-- to me, it seems pretty-- I'm pretty certain of that.
[00:52:52.000 --> 00:52:55.600]   I think what I'm sort of watching before-- I think Harry makes a great point.
[00:52:55.600 --> 00:52:57.440]   It's hard to say anything definitively.
[00:52:57.440 --> 00:53:01.360]   I think that if there's a lawsuit brought against Bloomberg, that will bring the truth
[00:53:01.360 --> 00:53:02.360]   out.
[00:53:02.360 --> 00:53:04.880]   And if there isn't, I think that also suggests that Bloomberg got the story right.
[00:53:04.880 --> 00:53:12.000]   So for me, I'm kind of using the legal case as the litmus test for the validity of this
[00:53:12.000 --> 00:53:16.120]   article because if they absolutely-- if they misreported the facts, if they got everything
[00:53:16.120 --> 00:53:21.360]   wrong, then there should be a pretty easy case to make legally against Bloomberg from
[00:53:21.360 --> 00:53:23.640]   all of the companies mentioned in that story.
[00:53:23.640 --> 00:53:30.520]   However, if Bloomberg is even somewhat true, that the reporting is somewhat true, then I
[00:53:30.520 --> 00:53:35.520]   think that a lot of these companies will have a hard time making a case against Bloomberg.
[00:53:35.520 --> 00:53:39.840]   And so if no lawsuit comes of this, then I think it suggests that there is a lot of
[00:53:39.840 --> 00:53:40.840]   truth in the reporting that--
[00:53:40.840 --> 00:53:41.840]   Oh, that's interesting.
[00:53:41.840 --> 00:53:42.840]   --from that story.
[00:53:42.840 --> 00:53:44.840]   So for me, that's kind of the barometer, I guess.
[00:53:44.840 --> 00:53:45.840]   So there's a third shoe.
[00:53:45.840 --> 00:53:48.680]   I don't know why, but there's a third shoe that's going to have to drop and that's a
[00:53:48.680 --> 00:53:49.680]   lawsuit.
[00:53:49.680 --> 00:53:50.680]   Paris--
[00:53:50.680 --> 00:53:51.680]   Yeah, I totally agree.
[00:53:51.680 --> 00:53:56.760]   I mean, I think that I've been following this as well and been kind of perplexed as
[00:53:56.760 --> 00:54:02.160]   well as silent in a bit because generally from an outlet like Bloomberg and the two great
[00:54:02.160 --> 00:54:07.520]   reporters that bought Cobyline to this 17 sources, a story of this magnitude, especially
[00:54:07.520 --> 00:54:14.400]   one that was in print, it doesn't seem likely that all the different layers of fact-checking
[00:54:14.400 --> 00:54:21.040]   and legal editorial work that went into this would have ended up misrepresenting the truth
[00:54:21.040 --> 00:54:24.280]   in an egregious fashion.
[00:54:24.280 --> 00:54:30.520]   But also, all these companies have been so defiant about this.
[00:54:30.520 --> 00:54:36.140]   But I think the, like Michael said, the one thing that we will have to see is whether
[00:54:36.140 --> 00:54:42.100]   or not a legal case is brought because, OK, if what Apple and Super Micro and whatnot
[00:54:42.100 --> 00:54:50.560]   are saying is true, if the Bloomberg story has no merit, then it's a pretty easy lawsuit
[00:54:50.560 --> 00:54:54.040]   because the only defense is that it's true.
[00:54:54.040 --> 00:54:57.680]   So if they can prove that it's not in some way, I don't see why they wouldn't have already
[00:54:57.680 --> 00:55:05.680]   brought a lawsuit or why they had just called for a retraction rather than something stronger.
[00:55:05.680 --> 00:55:11.140]   But I don't think we have enough facts to judge it as outsiders.
[00:55:11.140 --> 00:55:16.660]   Is Bloomberg Business Week hampered in their defense though in a lawsuit because these 17
[00:55:16.660 --> 00:55:18.980]   sources are anonymous?
[00:55:18.980 --> 00:55:25.060]   And by the way, you're all reporters, you know, oftentimes anonymous sources just as
[00:55:25.060 --> 00:55:29.700]   at Watergate are critical to a story.
[00:55:29.700 --> 00:55:33.180]   Antanimity alone does not impeach them.
[00:55:33.180 --> 00:55:37.920]   But that's partly because of a strong editorial process that backs it up.
[00:55:37.920 --> 00:55:40.760]   But it's appropriate for reporters to keep those sources anonymous.
[00:55:40.760 --> 00:55:47.120]   You can understand why in a national defense story that would be not surprising.
[00:55:47.120 --> 00:55:48.520]   But does it hamper their legal cases?
[00:55:48.520 --> 00:55:51.360]   It's hard for them to go defend themselves in court if they could say, well, we can't
[00:55:51.360 --> 00:55:52.960]   tell you who said this.
[00:55:52.960 --> 00:55:54.840]   Well, it shouldn't matter.
[00:55:54.840 --> 00:56:00.480]   So I've used anonymous sources and have been accused of similar things of being tricked
[00:56:00.480 --> 00:56:02.880]   into telling a story that wasn't true.
[00:56:02.880 --> 00:56:03.880]   Which is wrong.
[00:56:03.880 --> 00:56:07.700]   I think that the source doesn't matter.
[00:56:07.700 --> 00:56:09.740]   It's the material that they present to you.
[00:56:09.740 --> 00:56:15.460]   So as long as Bloomberg has enough material to show that the claims made in the story
[00:56:15.460 --> 00:56:16.460]   are true.
[00:56:16.460 --> 00:56:20.960]   So it could be emails, it could be internal documents, it could be photos of the server,
[00:56:20.960 --> 00:56:25.900]   any of those things, then it ultimately doesn't matter who delivered that information.
[00:56:25.900 --> 00:56:32.380]   But I would add one small caveat to this, which is I think that this culture of non-disclosure
[00:56:32.380 --> 00:56:40.000]   has really caused a lot more anonymous sourcing than I would prefer, really, especially in
[00:56:40.000 --> 00:56:42.060]   the case of technology news.
[00:56:42.060 --> 00:56:46.340]   So all of these big companies, these employees have to sign non-disclosure agreements when
[00:56:46.340 --> 00:56:47.540]   they walk into the building.
[00:56:47.540 --> 00:56:51.860]   And so they're often not able to talk about their experience, even if they want to on
[00:56:51.860 --> 00:56:52.860]   the record.
[00:56:52.860 --> 00:56:57.440]   So in many cases, when you see an anonymous source in a story, they're often legally hampered
[00:56:57.440 --> 00:56:59.620]   from revealing their name.
[00:56:59.620 --> 00:57:05.100]   And so at least in my experience, there have been people that have said, like I would love
[00:57:05.100 --> 00:57:10.740]   to come forward, I just don't want to be sued or have any legal retribution pursued from
[00:57:10.740 --> 00:57:13.460]   a company like Facebook or something like that.
[00:57:13.460 --> 00:57:19.060]   So I think what irritates me is that these non-disclosure seem antithetical to the First
[00:57:19.060 --> 00:57:20.060]   Amendment.
[00:57:20.060 --> 00:57:22.540]   It seems like almost unconstitutional.
[00:57:22.540 --> 00:57:23.540]   You're muzzling citizens.
[00:57:23.540 --> 00:57:28.260]   Well, if you're a whistleblower, you're protected, right?
[00:57:28.260 --> 00:57:32.900]   Well, remember, Edward Snowden did not have enough confidence in whistleblower regulations
[00:57:32.900 --> 00:57:36.820]   to trust the military.
[00:57:36.820 --> 00:57:39.980]   He got out of the country as fast as he could.
[00:57:39.980 --> 00:57:43.460]   Yeah, I think in many cases, I wouldn't trust it.
[00:57:43.460 --> 00:57:44.460]   I'll say that.
[00:57:44.460 --> 00:57:47.940]   If I was a whistleblower, I would not trust those protections because I think there are
[00:57:47.940 --> 00:57:51.420]   plenty of examples where whistleblowers are ostracized and challenged.
[00:57:51.420 --> 00:57:52.420]   And--
[00:57:52.420 --> 00:57:56.980]   So I mean, if you're legally challenged, then you can be blackballed by the industry.
[00:57:56.980 --> 00:58:02.900]   I think a lot of times I will talk to people and say, OK, I can probably speak with you
[00:58:02.900 --> 00:58:04.460]   about this legally.
[00:58:04.460 --> 00:58:09.380]   My non-disclosure doesn't apply to this, but I don't want my name associated with an article
[00:58:09.380 --> 00:58:14.220]   on this topic because future employers will just happen to see my name when they Google
[00:58:14.220 --> 00:58:18.340]   it in an article about this and decide I'm not trustworthy or that I will leak to the
[00:58:18.340 --> 00:58:20.140]   press in the future.
[00:58:20.140 --> 00:58:24.060]   And it kind of creates this culture of silence, especially in the tech world.
[00:58:24.060 --> 00:58:25.060]   Does this--
[00:58:25.060 --> 00:58:26.060]   OK.
[00:58:26.060 --> 00:58:27.060]   I'll just wait and see.
[00:58:27.060 --> 00:58:32.460]   I'd love to say Bloomberg come up with a meteor response.
[00:58:32.460 --> 00:58:33.460]   They need to do something.
[00:58:33.460 --> 00:58:36.100]   To all the flack they've caught, which they have not really done.
[00:58:36.100 --> 00:58:41.700]   It may be the case that there wasn't a widespread hack, that there were a handful of boards,
[00:58:41.700 --> 00:58:43.300]   which would actually make sense.
[00:58:43.300 --> 00:58:47.580]   You're going to target-- if you're the Chinese government, you're doing industrial espionage
[00:58:47.580 --> 00:58:51.420]   or military espionage because some of these boards, by the way, the Amazon elemental boards
[00:58:51.420 --> 00:58:56.500]   were used by the Department of Defense for-- during drone strikes, things like that.
[00:58:56.500 --> 00:58:58.340]   So if you're going to do that, you're going to target them.
[00:58:58.340 --> 00:59:02.380]   You're going to say, this board's going to-- the Department of Defense, let's put it on
[00:59:02.380 --> 00:59:03.580]   that one.
[00:59:03.580 --> 00:59:07.180]   So it doesn't, to me, it doesn't disqualify the story because we haven't been able to
[00:59:07.180 --> 00:59:08.940]   produce a board that has that problem.
[00:59:08.940 --> 00:59:12.660]   It might be-- there's six of them.
[00:59:12.660 --> 00:59:17.540]   And also, the underlying truth of the story is a given by everybody who knows anything about
[00:59:17.540 --> 00:59:18.540]   the supply chain.
[00:59:18.540 --> 00:59:19.540]   We do it to them.
[00:59:19.540 --> 00:59:20.980]   They do it to us.
[00:59:20.980 --> 00:59:23.380]   There's all kinds of stuff going on.
[00:59:23.380 --> 00:59:28.900]   Even the Snowden revelations had some hardware hacks that the NSA had perpetrated on other
[00:59:28.900 --> 00:59:29.900]   countries.
[00:59:29.900 --> 00:59:30.900]   So we know this goes on.
[00:59:30.900 --> 00:59:34.540]   So really, I think there's a much tougher question than they're lying it, which is,
[00:59:34.540 --> 00:59:39.300]   what the hell do we do if we can't trust stuff made in China?
[00:59:39.300 --> 00:59:40.300]   Because that's where--
[00:59:40.300 --> 00:59:44.580]   Have to pay for things made, not in China, which will be an expensive answer, I guess.
[00:59:44.580 --> 00:59:51.020]   The great article I read today about Steve Jobs' futile attempt to make Apple computers
[00:59:51.020 --> 00:59:55.060]   in the United States, he tried like the Dickens.
[00:59:55.060 --> 00:59:57.060]   First he wanted to make Max-- remember he had that?
[00:59:57.060 --> 00:59:59.580]   He was a great believer in the next factory.
[00:59:59.580 --> 01:00:02.100]   He did it with Max, and then with next.
[01:00:02.100 --> 01:00:03.700]   And then Tim Cook comes along.
[01:00:03.700 --> 01:00:08.060]   It's no surprise, as his successor, he's the chief operating officer, what is Tim Cook's
[01:00:08.060 --> 01:00:09.060]   expertise?
[01:00:09.060 --> 01:00:11.380]   Supply chain in Asia.
[01:00:11.380 --> 01:00:13.660]   And all the manufacturing moves offshore.
[01:00:13.660 --> 01:00:18.180]   That's how he became a CEO, because he was great at that kind of stuff.
[01:00:18.180 --> 01:00:20.820]   So it doesn't seem likely that we're going to get off.
[01:00:20.820 --> 01:00:22.220]   We're hooked on that one.
[01:00:22.220 --> 01:00:28.260]   Well GoPro is still going to manufacture in Asia, but not for stuff coming back to the
[01:00:28.260 --> 01:00:31.140]   US because of the tariff situation.
[01:00:31.140 --> 01:00:33.220]   That's more tariff, though, than the worry about stuff.
[01:00:33.220 --> 01:00:34.220]   Yes, that's not security thing.
[01:00:34.220 --> 01:00:39.300]   But apparently GoPro, which they're not making things in iPhone volume, they're making things
[01:00:39.300 --> 01:00:43.860]   in quite high volume, feels that it's capable of doing that.
[01:00:43.860 --> 01:00:44.860]   We shall see.
[01:00:44.860 --> 01:00:46.620]   It could be the end of the company.
[01:00:46.620 --> 01:00:54.660]   There was an interesting twist to the Huawei case.
[01:00:54.660 --> 01:01:00.460]   I'm thinking I'm going upstream when I'm saying this, but I worry that this Huawei story is
[01:01:00.460 --> 01:01:05.020]   not actually a technical issue or a defense story, but a political story.
[01:01:05.020 --> 01:01:10.020]   That it's one more arm of this attempt to discredit China.
[01:01:10.020 --> 01:01:15.860]   President Trump has sort of more or less acknowledged it as I think it sounds like he's
[01:01:15.860 --> 01:01:18.460]   talking about how they'll roll it into the trade deal.
[01:01:18.460 --> 01:01:20.660]   Yeah, I know you keep screwing up this.
[01:01:20.660 --> 01:01:23.740]   You got to stop talking about this stuff.
[01:01:23.740 --> 01:01:25.060]   So Germany.
[01:01:25.060 --> 01:01:33.380]   Now, by the way, US, Australia, England, the five eyes are saying we're not going to use
[01:01:33.380 --> 01:01:38.060]   Huawei gear in our Canada in our 5G networks.
[01:01:38.060 --> 01:01:45.300]   Germany says, no, we're going to use Huawei gear.
[01:01:45.300 --> 01:01:51.220]   The Germany's technology watchdog says it can't find any evidence that Huawei is a security
[01:01:51.220 --> 01:01:54.220]   risk.
[01:01:54.220 --> 01:01:59.460]   The Arnie Schoenbaum, who's the head of Germany's federal office for information security,
[01:01:59.460 --> 01:02:06.660]   told Der Spiegel for such serious decisions like a ban, you need proof.
[01:02:06.660 --> 01:02:11.780]   We can't find any evidence we examined all of Huawei's products in our lab.
[01:02:11.780 --> 01:02:16.900]   All three of Germany's major telcos use Huawei equipment.
[01:02:16.900 --> 01:02:23.140]   John Ledger of T-Mobile says we're going to be able to get regulatory approval for the
[01:02:23.140 --> 01:02:27.980]   T-Mobile Sprint merger because we promise not to use Huawei gear.
[01:02:27.980 --> 01:02:29.100]   Is this politics?
[01:02:29.100 --> 01:02:30.220]   Is this tech?
[01:02:30.220 --> 01:02:33.060]   Is this what's going on?
[01:02:33.060 --> 01:02:37.020]   It could be a preemptive measure, right?
[01:02:37.020 --> 01:02:44.700]   I don't know definitively why some of this stuff is happening, but it doesn't seem that
[01:02:44.700 --> 01:02:51.540]   far-fetched to me that there would be interest from the Chinese government to use all of
[01:02:51.540 --> 01:03:01.620]   its resources, including one of its largest electronics companies to further the ruling
[01:03:01.620 --> 01:03:02.620]   parties.
[01:03:02.620 --> 01:03:03.620]   Sure.
[01:03:03.620 --> 01:03:05.580]   We know Huawei has tight connections to the Chinese military.
[01:03:05.580 --> 01:03:10.180]   It was founded by former Chinese military officer.
[01:03:10.180 --> 01:03:11.180]   That seems like an inherent reason to me.
[01:03:11.180 --> 01:03:15.700]   It seems to me you have to find, just as with Super Micro, you've got to find the bug.
[01:03:15.700 --> 01:03:18.340]   You've got to find the flaw.
[01:03:18.340 --> 01:03:21.260]   I'm presuming that Germany has some pretty good technologists.
[01:03:21.260 --> 01:03:24.180]   They can't find anything wrong with it.
[01:03:24.180 --> 01:03:28.740]   Is this just a preemptive thing that could be?
[01:03:28.740 --> 01:03:33.220]   To me, it seems I'm okay with that type of reasoning.
[01:03:33.220 --> 01:03:38.260]   An all-out ban, so a country-wide ban is pretty darn extreme, but I think it's okay for defense
[01:03:38.260 --> 01:03:42.740]   contractors to be prohibited from using equipment made overseas.
[01:03:42.740 --> 01:03:44.260]   That seems very reasonable to me.
[01:03:44.260 --> 01:03:48.020]   Especially it seems foolish not to do that, in my opinion.
[01:03:48.020 --> 01:03:52.660]   Especially because if you don't find something now that doesn't mean that later something
[01:03:52.660 --> 01:03:58.860]   might come up, especially if I guess then a foreign entity knows it has easy access and
[01:03:58.860 --> 01:04:05.180]   access channel into another country's defense systems, then why wouldn't they in the future
[01:04:05.180 --> 01:04:06.940]   perhaps take advantage of that?
[01:04:06.940 --> 01:04:13.660]   Huawei, on their part, is offering full access to their source code and hardware.
[01:04:13.660 --> 01:04:18.380]   They're proposing the establishment of a National Cyber Security Evaluation Center to test the
[01:04:18.380 --> 01:04:22.900]   security credentials of technologies being implemented into critical infrastructure products.
[01:04:22.900 --> 01:04:27.460]   They're trying to persuade Australia and others to start using Huawei gear.
[01:04:27.460 --> 01:04:31.140]   Canada's pulling a billion dollars worth of Huawei gear out of their network.
[01:04:31.140 --> 01:04:33.740]   A great cost.
[01:04:33.740 --> 01:04:40.380]   Huawei also pitched US lawmakers saying, "Hey, look, we'll open it all up."
[01:04:40.380 --> 01:04:44.500]   We live in difficult times.
[01:04:44.500 --> 01:04:47.500]   I'm counting on you guys.
[01:04:47.500 --> 01:04:51.500]   You guys got to figure this out.
[01:04:51.500 --> 01:04:56.900]   I guess what's the cost to the average US consumer?
[01:04:56.900 --> 01:04:59.060]   What's the cost to the even the US economy?
[01:04:59.060 --> 01:05:02.860]   I don't see the incentive for pushing this deal through.
[01:05:02.860 --> 01:05:04.660]   You say, "Why take the chance?"
[01:05:04.660 --> 01:05:05.660]   Why take the chance?
[01:05:05.660 --> 01:05:06.660]   Yeah.
[01:05:06.660 --> 01:05:07.660]   I mean, I truly don't think-
[01:05:07.660 --> 01:05:10.100]   Because Huawei's the number one 5G manufacturer in the world.
[01:05:10.100 --> 01:05:17.460]   If we want to roll out 5G fast and don't want to use Huawei gear or maybe you don't want
[01:05:17.460 --> 01:05:20.540]   to use any Chinese gear, we ain't getting 5G.
[01:05:20.540 --> 01:05:22.340]   There's no one in the US that makes 5G gear.
[01:05:22.340 --> 01:05:27.860]   I think that- They're the number two smartphone manufacturer in the world after Samsung.
[01:05:27.860 --> 01:05:30.580]   This is not some little tiny company.
[01:05:30.580 --> 01:05:35.180]   This is a huge global corporation.
[01:05:35.180 --> 01:05:41.180]   Are they doing anything that a company?
[01:05:41.180 --> 01:05:42.180]   For example, software-
[01:05:42.180 --> 01:05:45.820]   I would say you have to find the evidence that they're doing something first before
[01:05:45.820 --> 01:05:48.420]   you accuse them of doing something.
[01:05:48.420 --> 01:05:49.420]   I understand.
[01:05:49.420 --> 01:05:53.580]   It makes sense that they would do something.
[01:05:53.580 --> 01:05:59.500]   Didn't US intelligence accuse Huawei of spying on US citizens earlier this year, like in
[01:05:59.500 --> 01:06:01.820]   January or February?
[01:06:01.820 --> 01:06:07.900]   I thought that there was an intelligence report that said that Huawei and ZTE were guilty
[01:06:07.900 --> 01:06:10.980]   of spying on US consumers.
[01:06:10.980 --> 01:06:17.260]   There was a brief scare about this before the ban came.
[01:06:17.260 --> 01:06:18.780]   There were a couple of warnings.
[01:06:18.780 --> 01:06:25.100]   There was the one warning from the US Department of Commerce that ZTE and Huawei gear should
[01:06:25.100 --> 01:06:28.540]   not be used.
[01:06:28.540 --> 01:06:33.540]   Here this year, you might be talking about this, the US intelligence community, including
[01:06:33.540 --> 01:06:39.060]   the heads of the FBI, CIA, and the NSA, said don't use Huawei phones.
[01:06:39.060 --> 01:06:44.180]   Chris Ray, the FBI director at the time, said the government was deeply concerned about the
[01:06:44.180 --> 01:06:49.180]   risks of allowing any company or entity that is beholden to foreign governments that don't
[01:06:49.180 --> 01:06:53.140]   share our values to gain positions of power inside our telecommunications network.
[01:06:53.140 --> 01:06:55.500]   That's more like, well, we worried.
[01:06:55.500 --> 01:06:58.060]   Yeah, so I guess there still wasn't any evidence.
[01:06:58.060 --> 01:06:59.060]   It's reasonable concern.
[01:06:59.060 --> 01:07:01.060]   It seems totally fair.
[01:07:01.060 --> 01:07:02.060]   Okay.
[01:07:02.060 --> 01:07:06.580]   You're going to have a more obligation to buy any piece of equipment from anybody in particular
[01:07:06.580 --> 01:07:09.740]   if you find it to be an unsettling prospect.
[01:07:09.740 --> 01:07:10.740]   Yeah.
[01:07:10.740 --> 01:07:13.100]   Well, but now there's an outright ban.
[01:07:13.100 --> 01:07:14.100]   I think it's better to be a fan, right?
[01:07:14.100 --> 01:07:15.100]   I'm sorry.
[01:07:15.100 --> 01:07:17.140]   Yeah, the US lawmakers have banned it.
[01:07:17.140 --> 01:07:20.140]   And I didn't mean to catch you off, Paris.
[01:07:20.140 --> 01:07:21.140]   Oh, sorry.
[01:07:21.140 --> 01:07:25.500]   I was saying I think it's better in these situations to be proactive rather than reactive.
[01:07:25.500 --> 01:07:31.340]   I think if the situation was reversed and that these companies or groups went ahead with
[01:07:31.340 --> 01:07:37.340]   Huawei technology and then a couple of years down the road, we found out that some of it
[01:07:37.340 --> 01:07:41.780]   maybe, even if it was just a small part of it, was compromised in some way.
[01:07:41.780 --> 01:07:48.180]   We all would be up in arms over the fact that they ever let any of it in and that in these
[01:07:48.180 --> 01:07:53.180]   sort of cases, we should allow intelligence organizations or entities, whether they're
[01:07:53.180 --> 01:07:59.460]   governments or specific mergers and companies to decide what sort of channels they want
[01:07:59.460 --> 01:08:02.940]   to use and not use.
[01:08:02.940 --> 01:08:08.220]   I mean, the funny thing is that isn't it true that Russian intelligence had infiltrated,
[01:08:08.220 --> 01:08:12.220]   I feel like I read this earlier a couple of months ago, that Russian intelligence had
[01:08:12.220 --> 01:08:17.020]   infiltrated energy companies or something like that.
[01:08:17.020 --> 01:08:20.900]   Isn't it true that basically my point here is that maybe there are greater risks than
[01:08:20.900 --> 01:08:29.300]   Huawei that are already the vulnerabilities that are already being used and exposed, I
[01:08:29.300 --> 01:08:30.300]   guess.
[01:08:30.300 --> 01:08:38.260]   So if you buy a Cisco switch, a nice American company is made in China.
[01:08:38.260 --> 01:08:41.660]   If you're buying it in Russia, it'll be made in Russia, by the way, because the Russians
[01:08:41.660 --> 01:08:45.500]   aren't going to use the Chinese equipment.
[01:08:45.500 --> 01:08:48.140]   They shut down their Mexican plants.
[01:08:48.140 --> 01:08:53.540]   I would bet there's not much networking gear you could buy that's not made in China.
[01:08:53.540 --> 01:09:02.060]   Does this, I mean, prudence would dictate we shouldn't buy anything made in China, right?
[01:09:02.060 --> 01:09:03.860]   As just average citizens?
[01:09:03.860 --> 01:09:07.740]   Yeah, or the government or to telcos or?
[01:09:07.740 --> 01:09:09.780]   I mean, I understand the threat.
[01:09:09.780 --> 01:09:13.260]   You could not only have espionage, industrial and surveillance.
[01:09:13.260 --> 01:09:17.060]   In fact, we're going to talk about that story in a second because apparently there's evidence
[01:09:17.060 --> 01:09:20.420]   Chinese have had a long standing program to do that.
[01:09:20.420 --> 01:09:22.260]   But there's also the threat of cyber warfare.
[01:09:22.260 --> 01:09:23.540]   They shut your networks down.
[01:09:23.540 --> 01:09:24.620]   So I understand the threat.
[01:09:24.620 --> 01:09:25.620]   I'm not saying that they're.
[01:09:25.620 --> 01:09:30.420]   I mean, I think if we're talking about a perfect world where you'd want to be perfectly secure,
[01:09:30.420 --> 01:09:34.540]   of course, yeah, you definitely want to be making everything in a controlled warehouse
[01:09:34.540 --> 01:09:37.540]   that only you have access to.
[01:09:37.540 --> 01:09:40.060]   But of course, that is not feasible.
[01:09:40.060 --> 01:09:41.060]   It's not practical.
[01:09:41.060 --> 01:09:42.900]   It's not something that's ever going to happen.
[01:09:42.900 --> 01:09:48.580]   So companies and governmental entities are going to have to pick and choose where they
[01:09:48.580 --> 01:09:50.940]   want to minimize risks.
[01:09:50.940 --> 01:09:56.900]   And as Mr. Jeff or is it Mr. Giff points out in our chat room, it's exactly what other
[01:09:56.900 --> 01:10:00.020]   countries go through when buying American gear.
[01:10:00.020 --> 01:10:01.020]   Yeah.
[01:10:01.020 --> 01:10:02.020]   Right.
[01:10:02.020 --> 01:10:03.020]   I would hate.
[01:10:03.020 --> 01:10:04.020]   I would hate.
[01:10:04.020 --> 01:10:10.660]   Look, I'm a liberal and I mean in the traditional sense of liberty free, I think the world is
[01:10:10.660 --> 01:10:18.300]   a better place when there's free trade where borders go down, where people trust one another.
[01:10:18.300 --> 01:10:23.700]   And I would hate to see in a liberal world, which we're moving towards where every country
[01:10:23.700 --> 01:10:28.020]   has a big barrier up against every other country and doesn't trust any other country
[01:10:28.020 --> 01:10:31.060]   because to me that leads to world war.
[01:10:31.060 --> 01:10:33.540]   And I don't think we can afford another world war.
[01:10:33.540 --> 01:10:36.380]   So that's my point of view.
[01:10:36.380 --> 01:10:41.100]   Maybe that's a highly impractical point of view, but that's my point of view.
[01:10:41.100 --> 01:10:44.580]   I worry that this is the wrong direction to be going.
[01:10:44.580 --> 01:10:46.900]   I generally agree with that.
[01:10:46.900 --> 01:10:53.620]   I think that most people on the panel agree that open borders and free trade is generally
[01:10:53.620 --> 01:10:54.740]   regarded better.
[01:10:54.740 --> 01:10:57.340]   But obviously there are no simple answers to this.
[01:10:57.340 --> 01:11:02.340]   I think that one of the things that I tell my friends who didn't go to college who are
[01:11:02.340 --> 01:11:07.780]   often tricked by misinformation is that there are very rarely, and this is also part of
[01:11:07.780 --> 01:11:08.780]   adulthood.
[01:11:08.780 --> 01:11:13.700]   I think there are rarely simple answers to these very complicated questions.
[01:11:13.700 --> 01:11:18.420]   And so although it would be great to just buy and sell from whoever you want across
[01:11:18.420 --> 01:11:25.860]   the world, there are more social dynamics going on that frankly we probably don't even fully
[01:11:25.860 --> 01:11:27.020]   understand.
[01:11:27.020 --> 01:11:32.860]   And so I think it makes sense to me to be as open as possible, but if you're a government
[01:11:32.860 --> 01:11:37.940]   contractor or a government agency, I think that it makes sense to source from a place
[01:11:37.940 --> 01:11:42.540]   like South Korea, which is a strong ally, has one of the largest US army bases in the
[01:11:42.540 --> 01:11:51.300]   world, et cetera, et cetera, and has a strong history of supporting US priorities, I guess,
[01:11:51.300 --> 01:12:00.340]   versus commissioning work from a foreign agent, whether it be China or Russia or anywhere
[01:12:00.340 --> 01:12:01.340]   else in the world.
[01:12:01.340 --> 01:12:07.540]   I think it's generally just safer to work with allies and with people that have historically
[01:12:07.540 --> 01:12:16.500]   supported US customs and US international strategy or whatever you want to call it.
[01:12:16.500 --> 01:12:23.700]   But to me, it feels like maybe I'm defending something that I typically wouldn't defend,
[01:12:23.700 --> 01:12:28.260]   but it seems to me like there are plenty of reasonable alternatives.
[01:12:28.260 --> 01:12:33.020]   And I'm not saying we should completely shut down trade with China.
[01:12:33.020 --> 01:12:37.900]   That's not my stance, but it seems okay to forbid government agencies from purchasing
[01:12:37.900 --> 01:12:43.300]   from this one supplier, so long as there are viable alternatives that allies can provide.
[01:12:43.300 --> 01:12:48.820]   And my rough understanding is that South Korea does have a lot of manufacturing facilities
[01:12:48.820 --> 01:12:51.820]   that can do a lot of the things that Chinese suppliers can do.
[01:12:51.820 --> 01:12:59.300]   It's sometimes a little bit more costly, but things like OLED screens and something conductor
[01:12:59.300 --> 01:13:08.740]   manufacturing, that stuff does take place in countries that have long supported US political
[01:13:08.740 --> 01:13:13.300]   procedure or US geopolitical strategy.
[01:13:13.300 --> 01:13:21.620]   So to me, if that's a viable option, then we should absolutely choose that over an unknown.
[01:13:21.620 --> 01:13:24.740]   I think it's just the most rational decision.
[01:13:24.740 --> 01:13:31.780]   You have the W no versus the W don't, I guess, or whatever that saying is.
[01:13:31.780 --> 01:13:32.780]   I think it's better.
[01:13:32.780 --> 01:13:35.660]   I think a better decision would be to make it evidence-based.
[01:13:35.660 --> 01:13:41.300]   I think it's the idea of the world being overly dependent on one company for 5G is unsettling.
[01:13:41.300 --> 01:13:42.300]   But it is.
[01:13:42.300 --> 01:13:44.820]   There's Sony and Erickson, but they're fraction.
[01:13:44.820 --> 01:13:49.900]   I'd much rather there will be several choices based in several different countries.
[01:13:49.900 --> 01:13:56.580]   And I don't know, I feel like I would rather see this be evidence-based rather than say,
[01:13:56.580 --> 01:13:59.220]   well, they've got to be up to something, so we're just not going to buy this.
[01:13:59.220 --> 01:14:04.780]   So if you were the head of a large telecommunications company, would you buy Huawei equipment or?
[01:14:04.780 --> 01:14:09.100]   I think I would, but I would make sure we audited it heavily.
[01:14:09.100 --> 01:14:10.860]   Now, is that a false presumption?
[01:14:10.860 --> 01:14:12.820]   Is it impossible to audit this stuff?
[01:14:12.820 --> 01:14:16.820]   No, actually, maybe that's the more reasonable solution here.
[01:14:16.820 --> 01:14:21.420]   Yeah, and perhaps I assume that these companies are not doing this blindly.
[01:14:21.420 --> 01:14:24.860]   I assume that there's some sort of audit or investigation that we as the public are not
[01:14:24.860 --> 01:14:25.860]   hearing about.
[01:14:25.860 --> 01:14:26.860]   Yeah.
[01:14:26.860 --> 01:14:30.980]   But that's my problem is there's this political thing that says, no, don't do it, don't do
[01:14:30.980 --> 01:14:31.980]   it.
[01:14:31.980 --> 01:14:36.660]   And there's, I think, a much more pragmatic, and because we don't make this stuff in the
[01:14:36.660 --> 01:14:39.300]   United States, there's a much more pragmatic point of view.
[01:14:39.300 --> 01:14:42.420]   I mean, sure, maybe we should build up our manufacturing capacity.
[01:14:42.420 --> 01:14:47.460]   But for right now, a more pragmatic thing to do would be buy it and audit it.
[01:14:47.460 --> 01:14:50.460]   And if it's clean, use it.
[01:14:50.460 --> 01:14:54.420]   How do you look for something, you know, you guys know that the way that hackers work,
[01:14:54.420 --> 01:15:00.300]   the way that hackers think, how do you search for something that, like, how do you look
[01:15:00.300 --> 01:15:02.300]   for the thing that you that that?
[01:15:02.300 --> 01:15:03.620]   Well, look at what he's done.
[01:15:03.620 --> 01:15:08.300]   I mean, he opens sourcing their software and saying, look, here's, we're going to be open
[01:15:08.300 --> 01:15:09.300]   about this.
[01:15:09.300 --> 01:15:10.620]   That's all they can do, right?
[01:15:10.620 --> 01:15:16.620]   And to me, that gives me some confidence that they are not in fact, do it up to something.
[01:15:16.620 --> 01:15:19.900]   It seems like there's a strong case that we haven't seen evidence of Huawei doing anything
[01:15:19.900 --> 01:15:21.660]   fishy as of now.
[01:15:21.660 --> 01:15:24.580]   So now I'm worried that it's a political thing.
[01:15:24.580 --> 01:15:26.260]   And it's not in our best interest.
[01:15:26.260 --> 01:15:27.500]   But again, who knows?
[01:15:27.500 --> 01:15:30.140]   I don't remember like Spectre and Meltdown though.
[01:15:30.140 --> 01:15:36.100]   I mean, that those were vulnerabilities that were available that were open for years, right?
[01:15:36.100 --> 01:15:40.900]   And so and how did those go missed is, you know, was how there's a difference between
[01:15:40.900 --> 01:15:46.780]   a flaw and intentional back doors and things like that.
[01:15:46.780 --> 01:15:47.780]   That's true.
[01:15:47.780 --> 01:15:53.420]   But I guess the, you know, the intelligence community looks for flaws that are unknown,
[01:15:53.420 --> 01:15:54.420]   right?
[01:15:54.420 --> 01:15:59.460]   And you think maybe the Chinese put something in there knowing that down the road, they'd
[01:15:59.460 --> 01:16:03.500]   be able to exploit it, but figuring we wouldn't discover it.
[01:16:03.500 --> 01:16:04.900]   No, that's okay.
[01:16:04.900 --> 01:16:05.900]   So that answers my question.
[01:16:05.900 --> 01:16:09.580]   Is it possible to say this is safe and maybe it's not?
[01:16:09.580 --> 01:16:16.220]   And I mean, I think that the ultimate Trojan horse is present something that looks to be
[01:16:16.220 --> 01:16:17.580]   entirely safe.
[01:16:17.580 --> 01:16:22.380]   You have the open source software and whatnot, but you have somehow found a backdoor in that
[01:16:22.380 --> 01:16:26.620]   is hiding within that once it is deployed within the highest levels of US government
[01:16:26.620 --> 01:16:34.420]   and the agencies of all of your entities of your enemies, then you exploited.
[01:16:34.420 --> 01:16:41.940]   And I mean, I assume that these, I'm generally very skeptical of all government agencies,
[01:16:41.940 --> 01:16:48.660]   but I assume that making a decision like this that is going to affect their bottom line,
[01:16:48.660 --> 01:16:53.100]   they have to have some sort of reasoning behind it, and especially given the reports
[01:16:53.100 --> 01:16:56.620]   that we've gone over earlier and that we've seen over the past couple of years concerning
[01:16:56.620 --> 01:17:02.900]   Huawei and similar companies that there's probably, they're probably not just doing
[01:17:02.900 --> 01:17:08.740]   this for no apparent reason because for a lot of these companies or agencies purchasing
[01:17:08.740 --> 01:17:13.940]   technology from China from one of the best manufacturers of 5G technology as well as
[01:17:13.940 --> 01:17:21.220]   other different venues seems like the natural choice and would benefit them in numerous
[01:17:21.220 --> 01:17:23.260]   ways.
[01:17:23.260 --> 01:17:24.260]   Let's take a break.
[01:17:24.260 --> 01:17:30.340]   When we come back, more evidence of Chinese hacking in the United States and a long term
[01:17:30.340 --> 01:17:31.340]   problem.
[01:17:31.340 --> 01:17:40.020]   We'll also talk about new text message rules from the FCC that could cause problems.
[01:17:40.020 --> 01:17:43.900]   Also some good news in California, it looks like they're not going to tax your SMS message
[01:17:43.900 --> 01:17:44.900]   messages.
[01:17:44.900 --> 01:17:46.620]   We'll talk about that lots more.
[01:17:46.620 --> 01:17:49.820]   I went in another Facebook exploit.
[01:17:49.820 --> 01:17:54.740]   Our show today brought to you by, well, I can't decide, Panko Crested Chicken and Maple
[01:17:54.740 --> 01:17:58.580]   Dipping Sauce with Roasted Brussels Sprouts and Sweet Potatoes, or maybe you look to
[01:17:58.580 --> 01:18:04.260]   me like more like a creamy saffron risotto with Sicilian style roasted cauliflower.
[01:18:04.260 --> 01:18:05.260]   I do?
[01:18:05.260 --> 01:18:09.340]   Well, I'll tell you what we made the other day that was amazing.
[01:18:09.340 --> 01:18:15.380]   Weird steaks and loaded mashed potatoes with roasted broccoli and guajillo honey sauce.
[01:18:15.380 --> 01:18:17.380]   Now that's a Harry McCracken.
[01:18:17.380 --> 01:18:18.380]   Sounds good.
[01:18:18.380 --> 01:18:20.900]   We're talking about blue apron.
[01:18:20.900 --> 01:18:23.860]   Blue apron makes cooking fun and easy.
[01:18:23.860 --> 01:18:25.300]   It is awesome.
[01:18:25.300 --> 01:18:28.660]   Every Wednesday we get our blue apron box and we just look forward to it every week.
[01:18:28.660 --> 01:18:32.700]   What we do is we get that I'm talking we, me and Lisa, we go online, we look at the
[01:18:32.700 --> 01:18:35.820]   blue apron menus, lovely, lovely choices.
[01:18:35.820 --> 01:18:41.420]   A variety of diets, many vegetarian options, kid friendly meals too.
[01:18:41.420 --> 01:18:44.780]   That's why we did the seared steaks, Michael loves steak.
[01:18:44.780 --> 01:18:48.100]   With the holidays fast approaching meal prep is the last thing you want to worry about
[01:18:48.100 --> 01:18:49.860]   shopping, planning.
[01:18:49.860 --> 01:18:55.420]   What if, what if it was just like you go home and there's everything you made to do it,
[01:18:55.420 --> 01:18:59.100]   make it, everything you need to make it delicious, wholesome meal.
[01:18:59.100 --> 01:19:03.180]   That's what blue apron does, create meals that are beautiful, so beautiful.
[01:19:03.180 --> 01:19:05.420]   You'll share them on social media.
[01:19:05.420 --> 01:19:09.020]   You can choose meals that are quick, 20 minutes, choose meals that take a little bit longer.
[01:19:09.020 --> 01:19:10.900]   I actually like to cook.
[01:19:10.900 --> 01:19:11.900]   I love it.
[01:19:11.900 --> 01:19:15.260]   And since they've they've they've done all the work for me, I always, you know, just
[01:19:15.260 --> 01:19:17.420]   pick by flavor, frankly.
[01:19:17.420 --> 01:19:21.180]   There are lots of videos and time saving tips on the website.
[01:19:21.180 --> 01:19:23.220]   Blue apron takes a chore out of meal prep.
[01:19:23.220 --> 01:19:27.020]   The website mobile app make it easy to plan your meals every week.
[01:19:27.020 --> 01:19:29.420]   You don't have to ever worry about forgotten ingredients.
[01:19:29.420 --> 01:19:34.140]   I don't have to run to the store because, oh, no, I need some sage.
[01:19:34.140 --> 01:19:35.740]   In fact, what's great is there's no waste either.
[01:19:35.740 --> 01:19:41.220]   You always get exactly what you need to make the dish, but no more.
[01:19:41.220 --> 01:19:42.980]   If you need one carrot, you get one carrot.
[01:19:42.980 --> 01:19:45.980]   Oh, man, you're going to learn it to me.
[01:19:45.980 --> 01:19:52.900]   It's been a cooking school in a box, new techniques, new recipes, even new ingredients.
[01:19:52.900 --> 01:19:57.540]   I had no idea what Guajillo honey sauce was or how it tasted until I made it.
[01:19:57.540 --> 01:20:00.940]   And let me tell you, it tastes delicious.
[01:20:00.940 --> 01:20:02.980]   Blue apron.
[01:20:02.980 --> 01:20:03.980]   It's time.
[01:20:03.980 --> 01:20:05.620]   I know you've heard me talk about it before.
[01:20:05.620 --> 01:20:08.260]   I know your mouth is watering right now.
[01:20:08.260 --> 01:20:12.860]   Perhaps I could interest you in some Korean style popcorn chicken with jasmine rice and
[01:20:12.860 --> 01:20:14.020]   roasted broccoli.
[01:20:14.020 --> 01:20:15.020]   Yes.
[01:20:15.020 --> 01:20:16.220]   Mm hmm.
[01:20:16.220 --> 01:20:17.380]   I want you to try it out.
[01:20:17.380 --> 01:20:18.860]   Get your first three meals free.
[01:20:18.860 --> 01:20:23.060]   Check out this week's menu at blueapron.com/twit.
[01:20:23.060 --> 01:20:24.060]   Blue apron.
[01:20:24.060 --> 01:20:26.660]   Oh, man, I'm so hungry now.
[01:20:26.660 --> 01:20:31.340]   Blueapron.com/twit.
[01:20:31.340 --> 01:20:33.980]   Blue aprons a better way to cook.
[01:20:33.980 --> 01:20:39.220]   We actually often do blue apron Sunday nights because I work late and what's nice to have
[01:20:39.220 --> 01:20:42.940]   a meal ready for me when I get home and I just whip it up.
[01:20:42.940 --> 01:20:45.500]   So good.
[01:20:45.500 --> 01:20:47.780]   A new Facebook bug, well, they just never end.
[01:20:47.780 --> 01:20:51.020]   Facebook is the gift that just keeps on giving this one.
[01:20:51.020 --> 01:20:54.700]   It's another related bug to this whole API.
[01:20:54.700 --> 01:21:02.540]   Several Facebook made the announcement after hosting a pop up privacy experience in New
[01:21:02.540 --> 01:21:03.780]   York's Bryant Park.
[01:21:03.780 --> 01:21:07.780]   Oh, man, how did I miss that?
[01:21:07.780 --> 01:21:14.860]   They said, oh, oh, by the way, several third party apps had, quote, "access to a broader
[01:21:14.860 --> 01:21:18.300]   set of photos than usual for 12 days in September."
[01:21:18.300 --> 01:21:24.860]   6.8 million users, many photos which they had not posted that were private, there's the
[01:21:24.860 --> 01:21:28.540]   pop up, the privacy, Facebook privacy experience.
[01:21:28.540 --> 01:21:31.860]   It's probably gone by now.
[01:21:31.860 --> 01:21:37.140]   When someone gives permission for an app to access their photos on Facebook, we usually
[01:21:37.140 --> 01:21:40.020]   only grant the app access to photos people share on their timeline.
[01:21:40.020 --> 01:21:45.820]   In this case, a bug potentially gave developers access to other photos.
[01:21:45.820 --> 01:21:49.140]   A few apps, 1,500 apps.
[01:21:49.140 --> 01:21:52.100]   Oh, man.
[01:21:52.100 --> 01:21:58.260]   It also let those apps access photos posted in stories and marketplace and on the user's
[01:21:58.260 --> 01:22:00.180]   timeline.
[01:22:00.180 --> 01:22:03.340]   Even photos you uploaded but didn't post.
[01:22:03.340 --> 01:22:07.140]   It'll be working with affected developers to help them delete the photos from impacted
[01:22:07.140 --> 01:22:08.140]   users.
[01:22:08.140 --> 01:22:11.820]   Oh, ye go vault.
[01:22:11.820 --> 01:22:13.300]   I just tell you this is a warning.
[01:22:13.300 --> 01:22:16.100]   I'm not angry about it because I don't use Facebook anymore.
[01:22:16.100 --> 01:22:20.820]   I think one big lesson is look and see what apps have access to your Facebook.
[01:22:20.820 --> 01:22:22.340]   Yes, and take the ones out.
[01:22:22.340 --> 01:22:26.820]   When I checked, I had 300, most of which I couldn't remember ever having given them
[01:22:26.820 --> 01:22:30.900]   permission and some of which didn't exist anymore.
[01:22:30.900 --> 01:22:36.580]   You should grind it down to the ones where you know them, you want them to have access
[01:22:36.580 --> 01:22:38.420]   and delete everything else.
[01:22:38.420 --> 01:22:41.020]   Yeah, I think that's great advice.
[01:22:41.020 --> 01:22:44.020]   I have kind of just given up on the idea of privacy at this point.
[01:22:44.020 --> 01:22:45.580]   I feel like as long as I have-
[01:22:45.580 --> 01:22:46.860]   That's the other option.
[01:22:46.860 --> 01:22:50.340]   I mean, truthfully, there's no way to manage this anymore.
[01:22:50.340 --> 01:22:51.860]   Yeah, a little bit.
[01:22:51.860 --> 01:22:55.900]   I mean, the thing is I'm sharing a lot less but I guess as long as I have the Facebook
[01:22:55.900 --> 01:23:00.860]   app on my phone and as long as I'm sort of participating even as infrequently as I do,
[01:23:00.860 --> 01:23:05.300]   I just assume that everything could go public at any moment.
[01:23:05.300 --> 01:23:10.660]   Anything that I do on my phone, any browsing history, any like, there's so much
[01:23:10.660 --> 01:23:16.780]   attached to my Facebook account that I've just kind of given up on the idea of privacy.
[01:23:16.780 --> 01:23:21.140]   The other thing I'll add to this is that, you know, so I joined Facebook before photos
[01:23:21.140 --> 01:23:22.140]   were even a thing.
[01:23:22.140 --> 01:23:23.380]   This was in college.
[01:23:23.380 --> 01:23:27.180]   And I remember photos arriving while I was in my freshman year of college.
[01:23:27.180 --> 01:23:30.580]   Wait a minute, you joined Facebook back when it was like just universities only?
[01:23:30.580 --> 01:23:31.580]   Yes, exactly.
[01:23:31.580 --> 01:23:32.580]   Yes, exactly.
[01:23:32.580 --> 01:23:33.580]   At the time.
[01:23:33.580 --> 01:23:35.060]   That's how I accessed the website.
[01:23:35.060 --> 01:23:36.060]   Yeah, the Facebook.
[01:23:36.060 --> 01:23:37.060]   Oh my God.
[01:23:37.060 --> 01:23:38.740]   You're an old timer.
[01:23:38.740 --> 01:23:39.740]   It was awesome.
[01:23:39.740 --> 01:23:41.780]   I mean, again, you described this earlier.
[01:23:41.780 --> 01:23:46.340]   People were really optimistic about these democratizing tools and it was this really
[01:23:46.340 --> 01:23:47.340]   exciting thing.
[01:23:47.340 --> 01:23:50.260]   And truthfully, I really liked Facebook for a very long time.
[01:23:50.260 --> 01:23:53.980]   I guess, and now my relationship has changed quite a bit.
[01:23:53.980 --> 01:23:56.660]   But this was something that I obsessed over and a lot of people did.
[01:23:56.660 --> 01:24:02.180]   And when photos arrived to Facebook, this was a huge freaking deal on college campuses
[01:24:02.180 --> 01:24:07.740]   because suddenly you could upload photo albums with your friends from all these parties
[01:24:07.740 --> 01:24:12.740]   around campus or activities, I guess, and share them with people.
[01:24:12.740 --> 01:24:17.420]   And you could also see who was going to, who was likely to show up at certain parties
[01:24:17.420 --> 01:24:19.820]   and who you might meet and that sort of thing.
[01:24:19.820 --> 01:24:25.340]   And it's amazing because there was so much excitement around uploading photo albums
[01:24:25.340 --> 01:24:30.460]   and giving each photo a caption and tagging your friends at a certain point.
[01:24:30.460 --> 01:24:37.060]   And now I feel like people so infrequently publish albums on Facebook.
[01:24:37.060 --> 01:24:40.660]   It's done a complete 180.
[01:24:40.660 --> 01:24:45.260]   So now I rarely see people posting photo albums.
[01:24:45.260 --> 01:24:47.180]   And it's just really hard to justify at this point.
[01:24:47.180 --> 01:24:54.940]   I see more people, at least of my age group, that are heading to the cloud and basically
[01:24:54.940 --> 01:25:00.580]   using software like Google Photos or using iCloud and using that as an archive for all
[01:25:00.580 --> 01:25:06.100]   of their photos and then sort of disseminating the photos one by one via text or via messaging
[01:25:06.100 --> 01:25:07.580]   platforms as they wish.
[01:25:07.580 --> 01:25:09.740]   So for example, I was out with a friend this weekend.
[01:25:09.740 --> 01:25:12.900]   We were hanging out with his two twin daughters.
[01:25:12.900 --> 01:25:16.020]   And we took a bunch of photos and we were at a diner and all of this stuff.
[01:25:16.020 --> 01:25:19.100]   At the end of the day, we just shared a couple of photos via text.
[01:25:19.100 --> 01:25:23.380]   In the past, we would have posted those things on Facebook and that would have been a way
[01:25:23.380 --> 01:25:24.380]   of sharing them with each other.
[01:25:24.380 --> 01:25:27.660]   I think personal sharing is really taken off.
[01:25:27.660 --> 01:25:33.060]   Yeah, the social dynamic has changed a lot for me personally.
[01:25:33.060 --> 01:25:34.060]   Interesting.
[01:25:34.060 --> 01:25:35.060]   I hadn't thought about that.
[01:25:35.060 --> 01:25:37.380]   And people around me, how about you, Paris?
[01:25:37.380 --> 01:25:38.380]   Do you?
[01:25:38.380 --> 01:25:39.700]   Yeah, I mean, I think it's the same.
[01:25:39.700 --> 01:25:44.140]   I took a trip up the other weekend with some friends to a cabin and at the end of it, we
[01:25:44.140 --> 01:25:50.620]   all created a shared Apple photos, an iPhoto's album and then added and commented there.
[01:25:50.620 --> 01:25:58.980]   I think it also, it's representative of this shift from public open platforms to kind of
[01:25:58.980 --> 01:26:07.060]   more semi-private controlled spaces because we're all realizing that a totally open public
[01:26:07.060 --> 01:26:11.300]   internet or social network experience is a modern hellscape.
[01:26:11.300 --> 01:26:14.540]   And we'd rather be around the people that we choose to be around.
[01:26:14.540 --> 01:26:19.900]   A bunch of some people are using Facebook Messenger though to do that.
[01:26:19.900 --> 01:26:21.380]   Somewhat more private sharing.
[01:26:21.380 --> 01:26:22.380]   Yeah.
[01:26:22.380 --> 01:26:24.460]   Which means Facebook's still got access to all the info.
[01:26:24.460 --> 01:26:25.460]   Yeah.
[01:26:25.460 --> 01:26:28.940]   I mean, there are things that default public like Facebook and things that default private
[01:26:28.940 --> 01:26:32.540]   like Google Photos and Apple Photos.
[01:26:32.540 --> 01:26:36.860]   Well and you know, I'll add that if I would have posted some of those photos on Facebook,
[01:26:36.860 --> 01:26:38.980]   I think it would have been a little bit offensive.
[01:26:38.980 --> 01:26:43.340]   So there's this new, I think there's a new- Offensive to the people whose pictures they
[01:26:43.340 --> 01:26:44.340]   were.
[01:26:44.340 --> 01:26:47.260]   Like, are you sharing my, our private stuff to the public?
[01:26:47.260 --> 01:26:48.260]   Yeah.
[01:26:48.260 --> 01:26:51.540]   And like, oh, I look like, I look like garbage in that photo or, you know, they would phrase
[01:26:51.540 --> 01:26:54.060]   it in a way that like they were uncomfortable with the way that they look.
[01:26:54.060 --> 01:26:55.580]   There's a button with Facebook to do that.
[01:26:55.580 --> 01:26:59.580]   I was so shocked my wife sent me that she said, take that picture down.
[01:26:59.580 --> 01:27:00.580]   I find it offensive.
[01:27:00.580 --> 01:27:01.580]   Oh really?
[01:27:01.580 --> 01:27:02.580]   That's hilarious.
[01:27:02.580 --> 01:27:03.580]   Yeah.
[01:27:03.580 --> 01:27:04.580]   There's a button on Facebook for that.
[01:27:04.580 --> 01:27:05.580]   Yeah, you could, this is a real concern.
[01:27:05.580 --> 01:27:07.900]   You can make it so it doesn't show up in a profile.
[01:27:07.900 --> 01:27:09.460]   I use that all the time.
[01:27:09.460 --> 01:27:10.900]   Oh, see, there you go.
[01:27:10.900 --> 01:27:11.900]   Yeah.
[01:27:11.900 --> 01:27:14.300]   I find it so weird, but sorry, I continue.
[01:27:14.300 --> 01:27:15.780]   Well, no, you're exactly right.
[01:27:15.780 --> 01:27:16.980]   I mean, it's a faux pas.
[01:27:16.980 --> 01:27:25.020]   It is a social faux pas to post photos of your friends without asking first, I think.
[01:27:25.020 --> 01:27:30.980]   And it's because the value of privacy is much greater than it was in 2001 or 2005 when
[01:27:30.980 --> 01:27:33.100]   I was in college.
[01:27:33.100 --> 01:27:37.220]   So yeah, so it's, that to me is really interesting.
[01:27:37.220 --> 01:27:41.820]   This shift in the value of privacy and how, and what moments we decide to share and what
[01:27:41.820 --> 01:27:45.900]   moments we decide to even share privately.
[01:27:45.900 --> 01:27:50.380]   And just like the dissemination of this, of this, you could call it data, I guess, but
[01:27:50.380 --> 01:27:54.980]   the dissemination of these photos, I think is, it represents the change.
[01:27:54.980 --> 01:27:57.540]   It's a changing dynamic that people have, the changing relationship that people have
[01:27:57.540 --> 01:28:01.100]   with Facebook because Facebook used to be all about photos.
[01:28:01.100 --> 01:28:03.340]   That was the whole purpose of Facebook.
[01:28:03.340 --> 01:28:07.740]   Really, when I was in college, it was about taking funny photos on your point and shoot
[01:28:07.740 --> 01:28:09.580]   at an Instagram for though, right?
[01:28:09.580 --> 01:28:10.580]   Is an Instagram replacement?
[01:28:10.580 --> 01:28:11.580]   Yeah.
[01:28:11.580 --> 01:28:12.580]   It is.
[01:28:12.580 --> 01:28:13.580]   It is.
[01:28:13.580 --> 01:28:14.580]   I mean, you guys have.
[01:28:14.580 --> 01:28:18.660]   Do you guys have a public Instagram or a private Instagram?
[01:28:18.660 --> 01:28:23.540]   So I've always been the biggest advocate for public Instagram because I thought private
[01:28:23.540 --> 01:28:28.940]   ones were dumb, but then over the past week, I wrote a story about the in-sale community
[01:28:28.940 --> 01:28:31.140]   and some other people and started getting some rassing messages.
[01:28:31.140 --> 01:28:35.740]   So I had to take mine private for the first time, which has actually been kind of fantastic
[01:28:35.740 --> 01:28:36.740]   really.
[01:28:36.740 --> 01:28:40.100]   Because now I look through the people who are following me.
[01:28:40.100 --> 01:28:41.700]   I deleted all the ones that didn't know.
[01:28:41.700 --> 01:28:45.060]   And now when I post a story, it's just just to my friends.
[01:28:45.060 --> 01:28:46.060]   Just to my friends.
[01:28:46.060 --> 01:28:47.060]   Yep.
[01:28:47.060 --> 01:28:48.060]   Great.
[01:28:48.060 --> 01:28:49.060]   I love it.
[01:28:49.060 --> 01:28:50.060]   I'm not going back.
[01:28:50.060 --> 01:28:51.060]   My wife does that too.
[01:28:51.060 --> 01:28:54.100]   It's a changing culture a little bit.
[01:28:54.100 --> 01:29:00.260]   Well, and I think there's a level of intellectualism and a level of like, what is it?
[01:29:00.260 --> 01:29:07.260]   Classism, I guess that the people that are wealthiest and smartest are sharing the least,
[01:29:07.260 --> 01:29:08.860]   at least anecdotally in my life.
[01:29:08.860 --> 01:29:14.100]   There was a big New York Times article about how Silicon Valley says don't use this crap.
[01:29:14.100 --> 01:29:16.300]   I don't want our kids using this.
[01:29:16.300 --> 01:29:20.940]   And Steve Jobs wouldn't let his kids use iPads.
[01:29:20.940 --> 01:29:21.940]   Founders of Facebook?
[01:29:21.940 --> 01:29:22.940]   No?
[01:29:22.940 --> 01:29:25.140]   Yeah, I mean, it's interesting, isn't it?
[01:29:25.140 --> 01:29:26.980]   They may be where the canaries in the coal mine.
[01:29:26.980 --> 01:29:30.540]   They were the first to realize the dangers of all this.
[01:29:30.540 --> 01:29:31.540]   Yeah.
[01:29:31.540 --> 01:29:33.420]   And of course, there are exceptions to the rule.
[01:29:33.420 --> 01:29:35.940]   Kim Kardashian shows us a lot of her life, I guess.
[01:29:35.940 --> 01:29:39.580]   I heard the cadessions it was over, that that's over.
[01:29:39.580 --> 01:29:40.580]   Somebody told me.
[01:29:40.580 --> 01:29:42.580]   Oh, that their reign is over.
[01:29:42.580 --> 01:29:44.780]   Their reign of terror is over.
[01:29:44.780 --> 01:29:45.780]   Oh, wow.
[01:29:45.780 --> 01:29:47.380]   I see I might be a little bit out of it.
[01:29:47.380 --> 01:29:50.020]   I don't know how you know, but I just haven't.
[01:29:50.020 --> 01:29:52.060]   I don't know because I don't ever see them.
[01:29:52.060 --> 01:29:56.740]   I should point out, I know people watch the shows regularly know this, but in August,
[01:29:56.740 --> 01:30:00.300]   killed Facebook, Twitter, Instagram, Tumblr, completely.
[01:30:00.300 --> 01:30:01.300]   Oh, wow.
[01:30:01.300 --> 01:30:03.020]   I deactivate all the accounts.
[01:30:03.020 --> 01:30:05.220]   I have never been happier.
[01:30:05.220 --> 01:30:06.220]   I don't...
[01:30:06.220 --> 01:30:07.220]   Do you ever miss them?
[01:30:07.220 --> 01:30:11.540]   Yeah, well, so what's interesting is I used to know everything that was going on and,
[01:30:11.540 --> 01:30:13.940]   you know, like I knew everything going on with you, Harry, you'd post these...
[01:30:13.940 --> 01:30:14.940]   You were talking about my Instagram?
[01:30:14.940 --> 01:30:19.540]   Yeah, you and Marie would post these great antique technology pictures and stuff like
[01:30:19.540 --> 01:30:20.540]   that.
[01:30:20.540 --> 01:30:26.780]   So, I like that and I miss it, but I also realize that it's kind of a false intimacy
[01:30:26.780 --> 01:30:32.060]   to think that you know about people because you're following their social feeds.
[01:30:32.060 --> 01:30:35.540]   And that really, the only real intimacy is when I'm here and you're talking, we're talking
[01:30:35.540 --> 01:30:38.300]   to each other, or even on Skype.
[01:30:38.300 --> 01:30:43.740]   I mean, I think that's real, but I think that there's this phony sense that you know you're
[01:30:43.740 --> 01:30:47.060]   keeping up with your friends because you see them on Facebook and Instagram.
[01:30:47.060 --> 01:30:48.300]   I don't miss.
[01:30:48.300 --> 01:30:49.300]   I really don't miss.
[01:30:49.300 --> 01:30:51.740]   And I really didn't like the FOMO stuff.
[01:30:51.740 --> 01:30:52.740]   Yeah.
[01:30:52.740 --> 01:30:54.580]   You know, you see these...
[01:30:54.580 --> 01:30:59.980]   And you've written some brilliant stuff, Paris, on the Facebook influencers and, you know,
[01:30:59.980 --> 01:31:00.980]   the whole culture...
[01:31:00.980 --> 01:31:02.500]   I'm sorry, Instagram.
[01:31:02.500 --> 01:31:08.100]   The whole culture on Instagram has become a little weird and creepy and show-offy and
[01:31:08.100 --> 01:31:09.100]   I don't like it.
[01:31:09.100 --> 01:31:10.100]   Yeah.
[01:31:10.100 --> 01:31:13.700]   Nothing is really real and it's...
[01:31:13.700 --> 01:31:18.540]   But yet people kind of judge it as if what they're seeing is real.
[01:31:18.540 --> 01:31:19.540]   I don't know.
[01:31:19.540 --> 01:31:24.700]   I think this is perhaps a story in the New York Times a couple of months ago where it
[01:31:24.700 --> 01:31:30.700]   was this one penthouse apartment in Manhattan that was rented out by influencers by the
[01:31:30.700 --> 01:31:35.340]   hour or day or week where they could go and take beautiful photos in like a luxurious
[01:31:35.340 --> 01:31:38.900]   living room or a men-der bathroom.
[01:31:38.900 --> 01:31:44.460]   And then add that as if it's their own home where they're taking photos in.
[01:31:44.460 --> 01:31:50.460]   And I thought of that kind of crystallized in the fact that nothing about Instagram or
[01:31:50.460 --> 01:31:54.420]   for any of these sort of platforms that we look at is real.
[01:31:54.420 --> 01:31:55.420]   And even...
[01:31:55.420 --> 01:31:58.500]   I mean, I've written pretty extensively about influencers.
[01:31:58.500 --> 01:32:03.180]   But I think even when you're talking about your friends, there's this artifice that's
[01:32:03.180 --> 01:32:06.380]   inherent in every action that you have online.
[01:32:06.380 --> 01:32:10.580]   You've seen the boyfriends of Instagram feed, haven't you?
[01:32:10.580 --> 01:32:12.420]   Oh, I have not.
[01:32:12.420 --> 01:32:14.500]   Oh my God.
[01:32:14.500 --> 01:32:20.260]   It's just a bunch of pictures of people taking pictures.
[01:32:20.260 --> 01:32:21.740]   The behind the scenes...
[01:32:21.740 --> 01:32:25.740]   Oh, no, I know the Instagram boyfriend meme, yeah.
[01:32:25.740 --> 01:32:26.900]   The behind...
[01:32:26.900 --> 01:32:30.780]   The trouble people go to get the angles, the shot.
[01:32:30.780 --> 01:32:36.980]   I have to try and submit this because I...so, Wired's New York Offices are in the World
[01:32:36.980 --> 01:32:42.500]   Trade Center and I swear every single day when I go to work, there are five, maybe more
[01:32:42.500 --> 01:32:48.340]   people trying to get really cool shots near the World Trade Center or the Oculus building
[01:32:48.340 --> 01:32:53.260]   with their gaggle of friends around them trying to take photos.
[01:32:53.260 --> 01:32:54.940]   Just to get the angle, man.
[01:32:54.940 --> 01:32:55.940]   Wow.
[01:32:55.940 --> 01:33:01.780]   My friend Chris Markford, who does a photo thing on the radio show, actually is starting
[01:33:01.780 --> 01:33:05.500]   a campaign and he says that Jackson Hole, Wyoming does this.
[01:33:05.500 --> 01:33:11.180]   No hashtags because one of the problems with Instagram is these beautiful places or even
[01:33:11.180 --> 01:33:16.740]   just the front of the Wired building are becoming uninhabitable because of all these
[01:33:16.740 --> 01:33:19.620]   people Instagramming their experience.
[01:33:19.620 --> 01:33:21.940]   Hashtags are the worst in general.
[01:33:21.940 --> 01:33:26.700]   So Jackson Hole, Wyoming has like the hashtag, no hashtags, please or don't say where you
[01:33:26.700 --> 01:33:28.300]   took this picture because it's just...
[01:33:28.300 --> 01:33:29.380]   Don't tag it, yeah.
[01:33:29.380 --> 01:33:31.540]   Don't tag it.
[01:33:31.540 --> 01:33:38.740]   And I have to say going to a concert has been ruined for me because not so much, I mean,
[01:33:38.740 --> 01:33:42.220]   it was bad enough people raising, you know, taking pictures the whole time, worse with
[01:33:42.220 --> 01:33:44.660]   an iPad, blocking your view.
[01:33:44.660 --> 01:33:51.260]   But the last few concerts I've been to, there are people, they're in the front row sometimes.
[01:33:51.260 --> 01:33:54.340]   There's a person singing his heart out.
[01:33:54.340 --> 01:34:01.460]   Somebody turns around, turns their back to the singer and takes a selfie of themselves
[01:34:01.460 --> 01:34:03.340]   with a singer behind them.
[01:34:03.340 --> 01:34:05.740]   I'll give you one more on that.
[01:34:05.740 --> 01:34:10.380]   I was at a concert a couple of months ago and the person in front of me had their phone
[01:34:10.380 --> 01:34:14.300]   at the entire time because they had FaceTimed in a bunch of their friends, the concert
[01:34:14.300 --> 01:34:19.060]   and had them piled up on FaceTime, a higher multi-hour concert.
[01:34:19.060 --> 01:34:22.940]   We know your life is fantastic, you don't have to post it.
[01:34:22.940 --> 01:34:23.940]   Yeah.
[01:34:23.940 --> 01:34:27.380]   The concert's tried to even try to place that stuff anymore because a couple of years ago
[01:34:27.380 --> 01:34:30.940]   I went to one where somebody was doing that next to me and security came down and told
[01:34:30.940 --> 01:34:31.940]   them to cut it out.
[01:34:31.940 --> 01:34:32.940]   It really depends.
[01:34:32.940 --> 01:34:39.180]   We saw a train and he actually, people were, I was stunned, throwing their phones up on
[01:34:39.180 --> 01:34:43.020]   the stage so he could take a selfie and then throw it back at them.
[01:34:43.020 --> 01:34:45.180]   He like incorporated it into the show.
[01:34:45.180 --> 01:34:46.180]   What?
[01:34:46.180 --> 01:34:47.180]   That's incredible.
[01:34:47.180 --> 01:34:48.180]   Crazy!
[01:34:48.180 --> 01:34:53.940]   It was kind of disruptive to his singing because he's like doing this the whole time.
[01:34:53.940 --> 01:34:57.940]   But I saw a spring scene on Broadway last week, closed on Saturday, close yesterday.
[01:34:57.940 --> 01:35:01.300]   We got in there at the last minute and they're very clear.
[01:35:01.300 --> 01:35:04.060]   You may not take out your phone but they understand the culture.
[01:35:04.060 --> 01:35:07.620]   So they say, "At the end of the show, we will turn up the house lights.
[01:35:07.620 --> 01:35:09.020]   Bruce will be taking his bows.
[01:35:09.020 --> 01:35:11.780]   You may then take a picture."
[01:35:11.780 --> 01:35:16.660]   They were, boy, if somebody, I saw, it was a little disruptive.
[01:35:16.660 --> 01:35:20.620]   Ushers with flashlights, if anybody took their phone out of their pocket, they were
[01:35:20.620 --> 01:35:34.720]   going, "N
[01:35:34.720 --> 01:35:51.320]   years ago."
[01:35:51.320 --> 01:36:02.760]   I'm going to get this costume next to Halloween.
[01:36:02.760 --> 01:36:05.640]   This is the influencer Halloween costume set.
[01:36:05.640 --> 01:36:11.600]   Everybody should have this sport bra, yoga pants, perfectly clean white sneakers, black
[01:36:11.600 --> 01:36:13.120]   baseball caps sunglasses.
[01:36:13.120 --> 01:36:14.120]   This is it.
[01:36:14.120 --> 01:36:15.120]   I think the baseball cap-
[01:36:15.120 --> 01:36:16.600]   I think it doesn't come with the Instagram boyfriend.
[01:36:16.600 --> 01:36:18.680]   That's probably an added $69.
[01:36:18.680 --> 01:36:19.680]   Extra.
[01:36:19.680 --> 01:36:20.680]   Yeah.
[01:36:20.680 --> 01:36:25.680]   Here's the definition of the Instagram boyfriend.
[01:36:25.680 --> 01:36:31.520]   Age 24 occupation, full-time boyfriend, interest, whatever Bay wants, description, castrated
[01:36:31.520 --> 01:36:32.520]   by love.
[01:36:32.520 --> 01:36:35.160]   He's now more of an employee than a boyfriend.
[01:36:35.160 --> 01:36:37.880]   I want to take a break.
[01:36:37.880 --> 01:36:41.120]   We have lots more to talk about.
[01:36:41.120 --> 01:36:42.600]   Harry McCracken is here.
[01:36:42.600 --> 01:36:45.840]   He is technology editor at Fest Company.
[01:36:45.840 --> 01:36:46.840]   Always a welcome guest.
[01:36:46.840 --> 01:36:47.840]   Great to have you.
[01:36:47.840 --> 01:36:48.840]   When you come, you bring your wife, Marie.
[01:36:48.840 --> 01:36:49.840]   We love Marie.
[01:36:49.840 --> 01:36:51.640]   We love you, Marie.
[01:36:51.640 --> 01:36:58.520]   And then Marie brings @photo, Michael O'Donnell, who is the king of the Silicon Valley photographers.
[01:36:58.520 --> 01:37:03.680]   Did you get a shot of me on my new Super73 fake electric bike?
[01:37:03.680 --> 01:37:04.680]   No?
[01:37:04.680 --> 01:37:06.000]   Wow, you missed a bet.
[01:37:06.000 --> 01:37:08.400]   You haven't had the camera out today.
[01:37:08.400 --> 01:37:11.000]   You're just here for just sitting in the audience.
[01:37:11.000 --> 01:37:12.000]   Unbelievable.
[01:37:12.000 --> 01:37:15.360]   Getting ready for next year, just apply to travel the world.
[01:37:15.360 --> 01:37:17.000]   Oh, you want to be an influencer?
[01:37:17.000 --> 01:37:20.680]   No, I'm going to take photography of a family traveling.
[01:37:20.680 --> 01:37:23.160]   Oh, that would be a cool job.
[01:37:23.160 --> 01:37:24.160]   That would be a very fun--
[01:37:24.160 --> 01:37:26.160]   You're going to be their Instagram boyfriend.
[01:37:26.160 --> 01:37:27.160]   Yep.
[01:37:27.160 --> 01:37:28.160]   [LAUGHTER]
[01:37:28.160 --> 01:37:31.160]   He's applied to be an Instagram boyfriend.
[01:37:31.160 --> 01:37:34.480]   The photos will be incredible.
[01:37:34.480 --> 01:37:36.200]   That's something very rich people do.
[01:37:36.200 --> 01:37:41.560]   I know this because Catherine had a family in New York, very wealthy family.
[01:37:41.560 --> 01:37:43.720]   She'd go out periodically and photograph them.
[01:37:43.720 --> 01:37:49.960]   They hire really good professional photographers because it's so hard to do all those selfies.
[01:37:49.960 --> 01:37:51.720]   Just have somebody do it for you.
[01:37:51.720 --> 01:37:54.960]   But we got a position in the hand so it looks like they took you.
[01:37:54.960 --> 01:37:55.960]   Oh, to you?
[01:37:55.960 --> 01:37:59.080]   You make him do this every time you take a picture and just get the hand out of the frame?
[01:37:59.080 --> 01:38:01.080]   I'm going to zoom in.
[01:38:01.080 --> 01:38:02.080]   [LAUGHTER]
[01:38:02.080 --> 01:38:03.080]   Whoo!
[01:38:03.080 --> 01:38:06.920]   Swamp Rat says he went to see the monkeys in 1986.
[01:38:06.920 --> 01:38:08.080]   Good on you, Swamp Rat.
[01:38:08.080 --> 01:38:10.560]   There was a no pictures policy.
[01:38:10.560 --> 01:38:14.080]   He took a few with his Instamatic Anyway.
[01:38:14.080 --> 01:38:17.680]   Somebody from the Louisiana Tech Union board walked up to me, reminded me about the no pictures
[01:38:17.680 --> 01:38:21.840]   policy and then winked and said, "Don't worry, I didn't see anything."
[01:38:21.840 --> 01:38:24.760]   See, an Instamatic could be fine.
[01:38:24.760 --> 01:38:25.760]   Take all the pictures you want.
[01:38:25.760 --> 01:38:28.560]   It doesn't have a big, bright screen.
[01:38:28.560 --> 01:38:33.640]   Our show today brought to you by Rocket Mortgage from Quick and Loans introducing something
[01:38:33.640 --> 01:38:34.720]   really cool.
[01:38:34.720 --> 01:38:38.360]   We've talked before about Rocket Mortgage because people like us--
[01:38:38.360 --> 01:38:39.960]   I'm kind of an introvert.
[01:38:39.960 --> 01:38:44.480]   I don't want to go to a bank and fill out an application and have to dress up and pretend
[01:38:44.480 --> 01:38:46.520]   I don't need a loan to get a home loan.
[01:38:46.520 --> 01:38:48.120]   I actually don't even want to put pants on.
[01:38:48.120 --> 01:38:50.480]   I just want to do it on my couch.
[01:38:50.480 --> 01:38:53.960]   That's what Rocket Mortgage-- believe it or not, Quick and Loans knows.
[01:38:53.960 --> 01:38:55.360]   That's what I love about Quick and Loans.
[01:38:55.360 --> 01:38:56.360]   They're kind of techy.
[01:38:56.360 --> 01:38:57.760]   They understand this.
[01:38:57.760 --> 01:39:01.360]   They are the number one mortgage lender in the country according to customer satisfaction
[01:39:01.360 --> 01:39:02.360]   results.
[01:39:02.360 --> 01:39:08.360]   Nine years in a row, JD Power says, "Number one, nine years in a row of customer satisfaction."
[01:39:08.360 --> 01:39:09.720]   That's for a mortgage origination.
[01:39:09.720 --> 01:39:11.240]   It's a approval process.
[01:39:11.240 --> 01:39:16.520]   Number one in customer satisfaction and mortgage servicing, five years in a row.
[01:39:16.520 --> 01:39:18.360]   They've been really dominant.
[01:39:18.360 --> 01:39:21.280]   They also number one lender in the country as of last year.
[01:39:21.280 --> 01:39:22.280]   That's really great.
[01:39:22.280 --> 01:39:23.680]   Congratulations, Quick and Loans.
[01:39:23.680 --> 01:39:25.200]   It's things like Rocket Mortgage.
[01:39:25.200 --> 01:39:26.840]   I think they make the difference.
[01:39:26.840 --> 01:39:28.560]   A thinking about the customer experience.
[01:39:28.560 --> 01:39:30.560]   Rocket Mortgage is entirely online.
[01:39:30.560 --> 01:39:32.760]   You go to rocketmortgage.com/twit2.
[01:39:32.760 --> 01:39:34.840]   I know you're probably not getting a home loan right now.
[01:39:34.840 --> 01:39:35.840]   You're busy.
[01:39:35.840 --> 01:39:37.200]   Some people are.
[01:39:37.200 --> 01:39:39.560]   You wouldn't hurt to go sign up for it.
[01:39:39.560 --> 01:39:40.960]   Rocketmortgage.com/twit2.
[01:39:40.960 --> 01:39:44.520]   That speeds up the process because let's say you're then in an open house.
[01:39:44.520 --> 01:39:48.680]   Maybe you're driving around looking at Christmas lights and suddenly you say, "I see a for-sales
[01:39:48.680 --> 01:39:49.760]   sign."
[01:39:49.760 --> 01:39:52.480]   You go look at the house, you say, "We need a loan.
[01:39:52.480 --> 01:39:53.680]   Now you're ready."
[01:39:53.680 --> 01:39:58.040]   The three-step power buying process begins and you won't believe how easy it is.
[01:39:58.040 --> 01:40:02.280]   First, you go to the website, rocketmortgage.com/twit2.
[01:40:02.280 --> 01:40:08.320]   There are a few simple questions.
[01:40:08.320 --> 01:40:09.880]   You can get a loan.
[01:40:09.880 --> 01:40:10.880]   These are the rates you can get.
[01:40:10.880 --> 01:40:11.800]   These are the terms you can get.
[01:40:11.800 --> 01:40:13.320]   These are the down payment, etc.
[01:40:13.320 --> 01:40:18.480]   You choose the perfect loan for you and you now get pre-qualified approval.
[01:40:18.480 --> 01:40:23.680]   That was in most cases less than 10 minutes and involved no research on your part.
[01:40:23.680 --> 01:40:29.080]   You didn't have to look up your dog's social security number or get pay stubs or anything.
[01:40:29.080 --> 01:40:34.200]   Basic stuff, name, address, beta, birth, social, that kind of thing.
[01:40:34.200 --> 01:40:37.440]   Then step two happens kind of automatically.
[01:40:37.440 --> 01:40:41.680]   Quick and loans verifies your income assets and credit and within 24 hours in most cases,
[01:40:41.680 --> 01:40:43.800]   they're going to give you what they call verified approval.
[01:40:43.800 --> 01:40:45.000]   You now get a letter.
[01:40:45.000 --> 01:40:46.000]   You give your realtor.
[01:40:46.000 --> 01:40:47.600]   You give the seller that says, "They're good for it.
[01:40:47.600 --> 01:40:48.600]   They got the loan.
[01:40:48.600 --> 01:40:49.600]   They're qualified.
[01:40:49.600 --> 01:40:53.160]   You have the strength of a cash buyer."
[01:40:53.160 --> 01:40:57.480]   That makes a huge difference when you're buying a house.
[01:40:57.480 --> 01:41:01.320]   Once you're verified, you also qualify for something that I think is very timely.
[01:41:01.320 --> 01:41:02.320]   Rates are going up.
[01:41:02.320 --> 01:41:06.440]   You notice perhaps that gives you a little anxiety, a little spilt because you're searching around
[01:41:06.440 --> 01:41:07.440]   for a house.
[01:41:07.440 --> 01:41:09.120]   You go, "Oh, God, we got to buy a house.
[01:41:09.120 --> 01:41:10.120]   Quick, the rate's going to go up.
[01:41:10.120 --> 01:41:11.840]   It's going to cost us a lot more."
[01:41:11.840 --> 01:41:14.600]   Maybe you buy a house that's not perfect.
[01:41:14.600 --> 01:41:15.920]   I want you to buy the perfect house.
[01:41:15.920 --> 01:41:16.920]   So does quick and loans.
[01:41:16.920 --> 01:41:19.680]   That's why rate shield approval.
[01:41:19.680 --> 01:41:23.840]   Once you get the verified approval, they lock your rate for up to 90 days.
[01:41:23.840 --> 01:41:25.000]   Why are you shopping?
[01:41:25.000 --> 01:41:26.920]   You got three months.
[01:41:26.920 --> 01:41:29.440]   Even if rates go up, yours will not.
[01:41:29.440 --> 01:41:30.680]   If rates go down, years will go down.
[01:41:30.680 --> 01:41:32.080]   So you win either way.
[01:41:32.080 --> 01:41:34.040]   But your rate cannot go up.
[01:41:34.040 --> 01:41:38.200]   That's exactly what you'd expect from one America's largest mortgage lender and best.
[01:41:38.200 --> 01:41:43.640]   And the one that really thinks about you, go to rocketmortgage.com/twit2 and get started.
[01:41:43.640 --> 01:41:48.320]   And then when you find that house, you can get the process underway.
[01:41:48.320 --> 01:41:51.920]   Rate shield approval is only valid on certain 30-year purchase transactions.
[01:41:51.920 --> 01:41:56.080]   Additional conditions or exclusions may apply based on quick and loans data in comparison
[01:41:56.080 --> 01:42:01.280]   to public data records, equal housing lender, licenses in all 50 states, and MLSconsumeraccess.org
[01:42:01.280 --> 01:42:04.360]   number, 30-30.
[01:42:04.360 --> 01:42:06.600]   All you have to remember is this.
[01:42:06.600 --> 01:42:09.720]   Rocketmortgage.com/twit2, we thank you so much.
[01:42:09.720 --> 01:42:15.840]   They've been such a great supporter all year long and we wish them the best in this holiday
[01:42:15.840 --> 01:42:16.840]   season.
[01:42:16.840 --> 01:42:22.520]   I didn't mention this, but Facebook has some explaining to do.
[01:42:22.520 --> 01:42:25.520]   Actually, Facebook, you've got some explaining to do.
[01:42:25.520 --> 01:42:31.040]   The Irish Data Protection Commission, which oversees Facebook's compliance with the European
[01:42:31.040 --> 01:42:37.800]   regulations, probably GDPR, said yesterday or Friday that it had launched a statutory
[01:42:37.800 --> 01:42:44.120]   inquiry into Facebook after receiving multiple reports of data breaches affecting the company.
[01:42:44.120 --> 01:42:50.720]   So the fine on this could be billions, could be huge.
[01:42:50.720 --> 01:42:55.120]   Four percent of their annual worldwide revenue.
[01:42:55.120 --> 01:42:58.800]   That profit, revenue.
[01:42:58.800 --> 01:43:01.280]   So there's that.
[01:43:01.280 --> 01:43:04.600]   I got that going for me.
[01:43:04.600 --> 01:43:05.600]   I also have a portal.
[01:43:05.600 --> 01:43:06.600]   I have two of them.
[01:43:06.600 --> 01:43:08.560]   If anybody wants to buy them, I cannot get them returned.
[01:43:08.560 --> 01:43:11.920]   Facebook said you could have 30 days to return them.
[01:43:11.920 --> 01:43:15.800]   I said, I sent them an email to support saying, "I want to return them."
[01:43:15.800 --> 01:43:16.800]   They said, "Why?"
[01:43:16.800 --> 01:43:18.720]   I said, "I don't want them."
[01:43:18.720 --> 01:43:22.320]   And that's the last I heard.
[01:43:22.320 --> 01:43:24.120]   They said, "Send us your invoice.
[01:43:24.120 --> 01:43:25.080]   Your number."
[01:43:25.080 --> 01:43:26.320]   They said nothing.
[01:43:26.320 --> 01:43:31.080]   And the 30 days runs out in four days.
[01:43:31.080 --> 01:43:32.080]   What do I do?
[01:43:32.080 --> 01:43:33.080]   I'm going to keep them.
[01:43:33.080 --> 01:43:35.480]   Do you have them plugged into you?
[01:43:35.480 --> 01:43:36.480]   Is that not?
[01:43:36.480 --> 01:43:37.480]   No, of course not.
[01:43:37.480 --> 01:43:38.480]   What am I crazy?
[01:43:38.480 --> 01:43:41.560]   Put a high-def camera in my house with Facebook on the other end.
[01:43:41.560 --> 01:43:43.440]   Zuck wants to know what's going on.
[01:43:43.440 --> 01:43:44.440]   Wait a minute.
[01:43:44.440 --> 01:43:45.640]   I can play words with friends?
[01:43:45.640 --> 01:43:46.640]   Okay.
[01:43:46.640 --> 01:43:50.400]   There's no other device you can do that on.
[01:43:50.400 --> 01:43:55.040]   I found myself, I can't believe this, recommending it on the radio show to a caller.
[01:43:55.040 --> 01:43:56.520]   I said, "Yes, today."
[01:43:56.520 --> 01:44:00.320]   But he had the perfect, well, he said, "My 85-year-old mother has a hard time with Skype
[01:44:00.320 --> 01:44:05.960]   and FaceTime is too complicated, but she's got relatives in China.
[01:44:05.960 --> 01:44:06.960]   What should we do?"
[01:44:06.960 --> 01:44:09.280]   I said, "Get a Facebook portal."
[01:44:09.280 --> 01:44:11.160]   Because all your faces are on this.
[01:44:11.160 --> 01:44:15.440]   First of all, it's a really nice picture frame with your Facebook photo albums that you
[01:44:15.440 --> 01:44:18.240]   don't upload anymore.
[01:44:18.240 --> 01:44:19.840]   It has all the people that you can call.
[01:44:19.840 --> 01:44:23.040]   You could choose like five people or whatever that you call.
[01:44:23.040 --> 01:44:25.240]   She just pushes the button and you appear.
[01:44:25.240 --> 01:44:28.960]   Plus, you can play words with friends with her.
[01:44:28.960 --> 01:44:31.680]   Facebook's, I guess, portal is such a success.
[01:44:31.680 --> 01:44:34.120]   I didn't know this, should have.
[01:44:34.120 --> 01:44:42.480]   It came out of their building eight, which is their advanced technology stuff, right?
[01:44:42.480 --> 01:44:44.480]   ATAP.
[01:44:44.480 --> 01:44:45.480]   They've now renamed it.
[01:44:45.480 --> 01:44:46.880]   I guess it was such a success.
[01:44:46.880 --> 01:44:49.760]   The building eight is now called Portal.
[01:44:49.760 --> 01:44:53.680]   They moved the more experimental stuff into a different lab.
[01:44:53.680 --> 01:44:55.040]   So Portal is a big deal for them.
[01:44:55.040 --> 01:44:58.480]   More tied to the AR Inviter, VR, parts of the company.
[01:44:58.480 --> 01:45:03.400]   We renamed the building 18 Portal after that device launch.
[01:45:03.400 --> 01:45:06.080]   So maybe, I don't know.
[01:45:06.080 --> 01:45:08.600]   I can send you one.
[01:45:08.600 --> 01:45:09.600]   I got two.
[01:45:09.600 --> 01:45:12.320]   I stupidly bought two because I thought, "Well, I need one on each end.
[01:45:12.320 --> 01:45:13.320]   You don't.
[01:45:13.320 --> 01:45:14.320]   You can just use Facebook Messenger."
[01:45:14.320 --> 01:45:15.320]   I don't know.
[01:45:15.320 --> 01:45:16.320]   Maybe I'll keep using it.
[01:45:16.320 --> 01:45:17.320]   It has a little...
[01:45:17.320 --> 01:45:18.320]   They're actually fun.
[01:45:18.320 --> 01:45:19.320]   I mean, I...
[01:45:19.320 --> 01:45:22.240]   Yeah, we've done remote...
[01:45:22.240 --> 01:45:23.720]   I have to teleconference for work.
[01:45:23.720 --> 01:45:26.600]   We have some writers on the West Coast.
[01:45:26.600 --> 01:45:28.600]   We've used a Microsoft...
[01:45:28.600 --> 01:45:34.040]   One of the large Microsoft service, like whiteboard-style devices.
[01:45:34.040 --> 01:45:37.120]   Yeah, and we've tried all sorts of things.
[01:45:37.120 --> 01:45:43.160]   Google Meet, by far and away, the most popular teleconferencing tool was the Facebook portal
[01:45:43.160 --> 01:45:47.800]   because they had the best filters so people could...
[01:45:47.800 --> 01:45:48.800]   It was just more fun.
[01:45:48.800 --> 01:45:50.440]   It's a very, very fun conversation.
[01:45:50.440 --> 01:45:52.840]   The camera actually tracks people in the room.
[01:45:52.840 --> 01:45:59.840]   So based on who's speaking, it'll find the face that the audio is coming from.
[01:45:59.840 --> 01:46:01.200]   And yeah, it was actually really fun.
[01:46:01.200 --> 01:46:06.080]   I think my advice to people that are considering getting this device would be to just quarantine
[01:46:06.080 --> 01:46:07.880]   it, put it in a space that...
[01:46:07.880 --> 01:46:08.880]   There you go.
[01:46:08.880 --> 01:46:09.880]   In an office.
[01:46:09.880 --> 01:46:15.880]   I mean, I know that not everyone has a dedicated office room, but if you can find a room that
[01:46:15.880 --> 01:46:18.760]   isn't too revealing, that isn't very high traffic, I think it's a very high-tech, but
[01:46:18.760 --> 01:46:24.200]   I think it's a really cool tool to have in your arsenal in terms of just reaching people
[01:46:24.200 --> 01:46:26.600]   across the country or across the world.
[01:46:26.600 --> 01:46:28.960]   Just put on it like a socket.
[01:46:28.960 --> 01:46:31.960]   Yeah, unplug it whenever you're not using it.
[01:46:31.960 --> 01:46:32.960]   Then put it back in.
[01:46:32.960 --> 01:46:33.960]   Yeah, minor unplug.
[01:46:33.960 --> 01:46:34.960]   Yeah.
[01:46:34.960 --> 01:46:35.960]   Great project.
[01:46:35.960 --> 01:46:36.960]   But the terrible timing.
[01:46:36.960 --> 01:46:37.960]   Yeah.
[01:46:37.960 --> 01:46:38.960]   Yeah.
[01:46:38.960 --> 01:46:43.480]   It's kind of sad because it is actually, technologically, it's a great product.
[01:46:43.480 --> 01:46:45.520]   I mean, it's cool.
[01:46:45.520 --> 01:46:47.040]   And they did delay it a little while.
[01:46:47.040 --> 01:46:48.040]   It sounds like out of concern.
[01:46:48.040 --> 01:46:50.040]   It's right after it was same rate.
[01:46:50.040 --> 01:46:51.040]   Yeah.
[01:46:51.040 --> 01:46:52.040]   Yeah.
[01:46:52.040 --> 01:46:53.040]   You have you used...
[01:46:53.040 --> 01:46:55.520]   So Michael, you've used the Surface Hub, the new Surface Hub?
[01:46:55.520 --> 01:46:57.360]   Yeah, we have massive.
[01:46:57.360 --> 01:47:00.440]   I think this looks really nice.
[01:47:00.440 --> 01:47:02.760]   It's cool.
[01:47:02.760 --> 01:47:09.200]   I mean, it's probably a little more powerful than we need for our purposes.
[01:47:09.200 --> 01:47:11.600]   We basically just use it to teleconference.
[01:47:11.600 --> 01:47:13.280]   Oh, yeah, if you're just talking.
[01:47:13.280 --> 01:47:16.680]   It's more for like collaborating and stuff like that.
[01:47:16.680 --> 01:47:17.680]   Yeah.
[01:47:17.680 --> 01:47:22.560]   Generally speaking, we don't present in those types of ways, I guess.
[01:47:22.560 --> 01:47:25.200]   But I think it could work for some offices.
[01:47:25.200 --> 01:47:26.200]   Yeah, definitely.
[01:47:26.200 --> 01:47:30.720]   As a presenting tool, I think it looks...
[01:47:30.720 --> 01:47:32.720]   It theoretically would be really great.
[01:47:32.720 --> 01:47:34.360]   They're putting a lot of money into this.
[01:47:34.360 --> 01:47:35.360]   It's not available.
[01:47:35.360 --> 01:47:38.840]   I think the new ones coming out when Harry next year or early next year.
[01:47:38.840 --> 01:47:41.680]   They pre-announced it by a lot several months ago.
[01:47:41.680 --> 01:47:43.640]   They showed it though.
[01:47:43.640 --> 01:47:46.640]   Paul Therott and Mary Jofoli, I think, played with it.
[01:47:46.640 --> 01:47:50.120]   It was at Ignite at one of the Microsoft conferences and they were very impressed.
[01:47:50.120 --> 01:47:51.800]   It did the rotating thing that it does.
[01:47:51.800 --> 01:47:54.240]   I'm glad they're sticking with this idea.
[01:47:54.240 --> 01:47:55.880]   I think there's business here.
[01:47:55.880 --> 01:48:03.160]   Cisco must have made millions on those expensive rooms that they sold the conference rooms to
[01:48:03.160 --> 01:48:04.160]   people.
[01:48:04.160 --> 01:48:05.160]   This looks really interesting.
[01:48:05.160 --> 01:48:10.160]   I mean, it's basically a PC with a giant super screen.
[01:48:10.160 --> 01:48:11.160]   I know.
[01:48:11.160 --> 01:48:14.560]   Yeah, so I guess my criticism of this would be that you probably don't need touchscreen
[01:48:14.560 --> 01:48:17.720]   on such a large monitor.
[01:48:17.720 --> 01:48:22.520]   At least in the case of our office, Google Meet works really well.
[01:48:22.520 --> 01:48:25.280]   It used to be Google Hangouts.
[01:48:25.280 --> 01:48:26.280]   I don't know.
[01:48:26.280 --> 01:48:33.080]   I think that most offices, from my perspective, would be better off with a very large HDTV
[01:48:33.080 --> 01:48:34.960]   connected to Google Meet.
[01:48:34.960 --> 01:48:38.840]   There's little hubs that you can put behind the television set.
[01:48:38.840 --> 01:48:40.960]   But you don't need the touch screen.
[01:48:40.960 --> 01:48:44.240]   Most people don't need those touch screen capabilities.
[01:48:44.240 --> 01:48:45.600]   And they don't need that pen either.
[01:48:45.600 --> 01:48:48.360]   I think that's just kind of extraneous for most.
[01:48:48.360 --> 01:48:49.360]   Facebook's got me.
[01:48:49.360 --> 01:48:51.200]   I'm going to, my mom loves to play words with friends.
[01:48:51.200 --> 01:48:55.960]   I think I'm going to send one to her and we can play portal play.
[01:48:55.960 --> 01:48:57.160]   That is pretty awesome.
[01:48:57.160 --> 01:48:59.560]   She loves words with friends.
[01:48:59.560 --> 01:49:04.600]   And then it'll zoom in on your victory dance according to a CNET.
[01:49:04.600 --> 01:49:09.720]   I keep pulling me in.
[01:49:09.720 --> 01:49:10.720]   This is the problem.
[01:49:10.720 --> 01:49:11.720]   I deactivated my Facebook account.
[01:49:11.720 --> 01:49:13.680]   I had to reactivate it to buy the portal.
[01:49:13.680 --> 01:49:15.440]   That's why I haven't got the email.
[01:49:15.440 --> 01:49:18.640]   I was too polite to ask you why you had a portal if you were no longer on Facebook.
[01:49:18.640 --> 01:49:19.640]   To review it.
[01:49:19.640 --> 01:49:23.080]   Because I do it for you, the audience.
[01:49:23.080 --> 01:49:24.560]   I buy a lot of stupid stuff.
[01:49:24.560 --> 01:49:25.560]   Let me tell you.
[01:49:25.560 --> 01:49:26.560]   Like an e-bike.
[01:49:26.560 --> 01:49:27.560]   No, just show again.
[01:49:27.560 --> 01:49:29.560]   Oh, my e-bike is awesome.
[01:49:29.560 --> 01:49:30.560]   Did you see the...
[01:49:30.560 --> 01:49:31.560]   Wait a minute.
[01:49:31.560 --> 01:49:33.200]   I actually have a bone to pick with you.
[01:49:33.200 --> 01:49:37.600]   It sounds like you have a throttle on your e-bike, which gives people like me who use
[01:49:37.600 --> 01:49:40.360]   a pedal assist e-bike a bad name.
[01:49:40.360 --> 01:49:44.760]   But I'm probably getting way in the weeds here.
[01:49:44.760 --> 01:49:47.440]   You've like shoot down on a throttle on them.
[01:49:47.440 --> 01:49:49.520]   This is awesome.
[01:49:49.520 --> 01:49:50.520]   That's pretty cool.
[01:49:50.520 --> 01:49:52.480]   This is the Super 73.
[01:49:52.480 --> 01:49:55.520]   It looks basically...
[01:49:55.520 --> 01:49:58.800]   I don't think they really mean it to be an e-bike.
[01:49:58.800 --> 01:50:03.760]   I think their goal with this bike is to look like an old motorcycle.
[01:50:03.760 --> 01:50:06.200]   Because the batteries where the tank would be.
[01:50:06.200 --> 01:50:07.720]   It looks like an old motorcycle.
[01:50:07.720 --> 01:50:08.720]   Yeah, it does.
[01:50:08.720 --> 01:50:09.720]   It looks very cool.
[01:50:09.720 --> 01:50:12.120]   Do you have to pedal though?
[01:50:12.120 --> 01:50:13.120]   No.
[01:50:13.120 --> 01:50:15.320]   You can, but you don't have to.
[01:50:15.320 --> 01:50:16.960]   So it's basically like a motor.
[01:50:16.960 --> 01:50:19.360]   It's a lie to a little police.
[01:50:19.360 --> 01:50:20.760]   Yeah, that pedals.
[01:50:20.760 --> 01:50:23.360]   I have an issue with that, Leo.
[01:50:23.360 --> 01:50:25.880]   So what I use is something called a pedal assist e-bike.
[01:50:25.880 --> 01:50:27.840]   It also has a large battery.
[01:50:27.840 --> 01:50:29.880]   And you have to move the pedals.
[01:50:29.880 --> 01:50:30.880]   You can turn...
[01:50:30.880 --> 01:50:33.880]   It has three levels of pedal assist or none.
[01:50:33.880 --> 01:50:38.400]   Nice, nice.
[01:50:38.400 --> 01:50:43.400]   I have also some rad power pedal assist.
[01:50:43.400 --> 01:50:45.320]   Normal, they're really more normal.
[01:50:45.320 --> 01:50:46.320]   But this is just...
[01:50:46.320 --> 01:50:47.320]   I got this because of...
[01:50:47.320 --> 01:50:48.320]   I mean, that's pretty awesome.
[01:50:48.320 --> 01:50:49.320]   I got to say.
[01:50:49.320 --> 01:50:52.480]   I got it because Casey and I have to have it too.
[01:50:52.480 --> 01:50:53.480]   Nice.
[01:50:53.480 --> 01:50:54.480]   Yeah, those are illegal.
[01:50:54.480 --> 01:50:56.000]   Those types of bikes are illegal in New York City.
[01:50:56.000 --> 01:50:57.960]   You're not supposed to have a...
[01:50:57.960 --> 01:50:58.960]   Oh, really?
[01:50:58.960 --> 01:50:59.960]   Yeah, you're supposed to...
[01:50:59.960 --> 01:51:01.960]   It's strictly pedal assist if you have your throttle.
[01:51:01.960 --> 01:51:02.960]   But on Arferser.
[01:51:02.960 --> 01:51:03.960]   Orpherser.
[01:51:03.960 --> 01:51:04.960]   But on Arferser.
[01:51:04.960 --> 01:51:06.880]   It's got pedals and it's pedal assist.
[01:51:06.880 --> 01:51:08.880]   And it only goes 20 miles an hour, Arferser.
[01:51:08.880 --> 01:51:12.040]   But it's true it's the last hour, Arferser.
[01:51:12.040 --> 01:51:13.040]   Yes.
[01:51:13.040 --> 01:51:14.640]   But I didn't see your feet move, LaPorte.
[01:51:14.640 --> 01:51:16.680]   No, because I'm very strong.
[01:51:16.680 --> 01:51:18.200]   One pedal is all it takes.
[01:51:18.200 --> 01:51:20.400]   It was a big pedal.
[01:51:20.400 --> 01:51:24.000]   Yeah, it's an issue right now in the bike lanes in New York.
[01:51:24.000 --> 01:51:25.760]   Yes, because they're basically electric motors.
[01:51:25.760 --> 01:51:27.760]   There's three bike lanes in New York City.
[01:51:27.760 --> 01:51:29.320]   Not such a big issue in pedal to the Mys.
[01:51:29.320 --> 01:51:31.600]   No, here they let me ride all over the place.
[01:51:31.600 --> 01:51:32.600]   That's awesome.
[01:51:32.600 --> 01:51:34.720]   I will say it looks really, really cool.
[01:51:34.720 --> 01:51:35.720]   Is that cool?
[01:51:35.720 --> 01:51:36.720]   It looks awesome.
[01:51:36.720 --> 01:51:38.840]   That's an example of--
[01:51:38.840 --> 01:51:40.000]   That's the handle wipe.
[01:51:40.000 --> 01:51:41.000]   Yeah, look at that.
[01:51:41.000 --> 01:51:42.000]   Yeah, that's cool.
[01:51:42.000 --> 01:51:43.000]   And they're trying to look like a motorcycle.
[01:51:43.000 --> 01:51:45.040]   Is it crashed like a motorcycle or like a bike bike bike?
[01:51:45.040 --> 01:51:45.800]   Is it what?
[01:51:45.800 --> 01:51:47.280]   Crash?
[01:51:47.280 --> 01:51:48.440]   Like a motorcycle?
[01:51:48.440 --> 01:51:49.880]   Is it priced like a motorcycle or?
[01:51:49.880 --> 01:51:51.200]   It's $2300.
[01:51:51.200 --> 01:51:52.680]   All right.
[01:51:52.680 --> 01:51:56.480]   So electric bikes tend to be very expensive, right?
[01:51:56.480 --> 01:51:59.720]   What do you ride, Michael?
[01:51:59.720 --> 01:52:01.800]   It's from a company called Priority.
[01:52:01.800 --> 01:52:04.960]   And the bicycle is called the Embark.
[01:52:04.960 --> 01:52:06.040]   And it's a few bikes.
[01:52:06.040 --> 01:52:07.880]   I am like an e-bike fanatic.
[01:52:07.880 --> 01:52:09.360]   If I lived-- I mean, if I had a commute,
[01:52:09.360 --> 01:52:11.600]   I would do it on an e-bike.
[01:52:11.600 --> 01:52:12.600]   Yeah, same.
[01:52:12.600 --> 01:52:14.080]   That's really the goal.
[01:52:14.080 --> 01:52:16.720]   So for me, it's about an hour long bike ride.
[01:52:16.720 --> 01:52:17.880]   And I actually love doing it.
[01:52:17.880 --> 01:52:18.880]   Yeah, because it's good for you.
[01:52:18.880 --> 01:52:20.160]   Like, you'll probably sweaty and gross.
[01:52:20.160 --> 01:52:21.680]   So yeah, this is the one I need.
[01:52:21.680 --> 01:52:22.200]   It's beautiful.
[01:52:22.200 --> 01:52:24.160]   And the thing is, you're still pedaling.
[01:52:24.160 --> 01:52:26.120]   You still get plenty of exercise.
[01:52:26.120 --> 01:52:29.680]   But when there's a hill, you don't have to kill yourself.
[01:52:29.680 --> 01:52:30.400]   That's exactly right.
[01:52:30.400 --> 01:52:32.320]   Yeah, so for the bridge, I crossed from Brooklyn
[01:52:32.320 --> 01:52:33.440]   into Manhattan.
[01:52:33.440 --> 01:52:34.440]   Oh, wow.
[01:52:34.440 --> 01:52:36.280]   I can just cruise over the bridge.
[01:52:36.280 --> 01:52:37.640]   It is so, so nice.
[01:52:37.640 --> 01:52:40.120]   So this has been-- I mean, biking in general
[01:52:40.120 --> 01:52:41.760]   has been a huge game changer for me.
[01:52:41.760 --> 01:52:46.760]   It's just added a lot of happiness to my life, basically.
[01:52:46.760 --> 01:52:50.200]   But this new e-bike thing is like, this is the new frontier.
[01:52:50.200 --> 01:52:51.680]   So I just got it in the fall.
[01:52:51.680 --> 01:52:53.680]   I haven't had a chance to ride it too much in the winter.
[01:52:53.680 --> 01:52:56.240]   But I'm really expecting to get around most places
[01:52:56.240 --> 01:52:58.960]   in the spring and summer using that bike.
[01:52:58.960 --> 01:53:00.040]   I think it's a revolution.
[01:53:00.040 --> 01:53:01.640]   And I think it's good for the climate.
[01:53:01.640 --> 01:53:02.920]   I think it's good for the environment.
[01:53:02.920 --> 01:53:04.720]   I hope that more people go--
[01:53:04.720 --> 01:53:06.760]   I have big solar panels on the roof,
[01:53:06.760 --> 01:53:08.920]   so I'm generating my own electricity.
[01:53:08.920 --> 01:53:09.400]   I love it.
[01:53:09.400 --> 01:53:10.840]   I think it's great.
[01:53:10.840 --> 01:53:16.920]   Meanwhile, people are vandalizing Waymo self-driving vehicles.
[01:53:16.920 --> 01:53:19.400]   They're all over Arizona, I guess.
[01:53:19.400 --> 01:53:25.200]   And what is underreported, but Arizona Central has a story.
[01:53:25.200 --> 01:53:27.560]   Police have responded to dozens of calls regarding people
[01:53:27.560 --> 01:53:33.200]   threatening and harassing the Waymo self-driving vans.
[01:53:33.200 --> 01:53:35.320]   They start the story of Waymo self-driving
[01:53:35.320 --> 01:53:37.160]   and cruise through a Chandler neighborhood August 1st
[01:53:37.160 --> 01:53:40.240]   when a test driver, Michael Pallows, saw something startling
[01:53:40.240 --> 01:53:43.120]   as he sat behind the wheel, a bearded man in shorts,
[01:53:43.120 --> 01:53:46.760]   aiming a handgun at him as he passed the driveway.
[01:53:46.760 --> 01:53:49.000]   21 interactions documented by Chandler
[01:53:49.000 --> 01:53:50.920]   police over the last two years where people have
[01:53:50.920 --> 01:53:53.960]   harassed the autonomous vehicles and their human test
[01:53:53.960 --> 01:53:56.880]   drivers tires slash when stopped at traffic,
[01:53:56.880 --> 01:53:59.920]   rocks thrown one Jeep was responsible for forcing the vans
[01:53:59.920 --> 01:54:01.520]   off the road six times.
[01:54:01.520 --> 01:54:07.720]   Why are people so angry about self-driving vehicles?
[01:54:07.720 --> 01:54:09.800]   I think that story says that the guy with the gun
[01:54:09.800 --> 01:54:12.200]   was suffering from demand shafts, at least according
[01:54:12.200 --> 01:54:12.840]   to his wife.
[01:54:12.840 --> 01:54:15.200]   But I do worry, this foreshadow is
[01:54:15.200 --> 01:54:18.040]   what the world will be like a few years from now
[01:54:18.040 --> 01:54:20.920]   of self-driving vehicles are everywhere.
[01:54:20.920 --> 01:54:21.480]   Well, don't--
[01:54:21.480 --> 01:54:23.400]   I mean, it reminds me of the same sort of backlash
[01:54:23.400 --> 01:54:26.200]   faced by the nightscape security bots.
[01:54:26.200 --> 01:54:27.640]   But that wasn't what I was going to bring up.
[01:54:27.640 --> 01:54:28.240]   People kick them.
[01:54:28.240 --> 01:54:31.200]   Yeah, someone throwing scooters under the river.
[01:54:31.200 --> 01:54:32.480]   [INTERPOSING VOICES]
[01:54:32.480 --> 01:54:34.640]   He's the least.
[01:54:34.640 --> 01:54:37.200]   So this may be the theme to the show,
[01:54:37.200 --> 01:54:41.400]   but there is a backlash against technology, isn't there?
[01:54:41.400 --> 01:54:42.400]   Yeah.
[01:54:42.400 --> 01:54:46.320]   I mean, without a doubt, I think you could characterize this year
[01:54:46.320 --> 01:54:50.080]   2018 as a--
[01:54:50.080 --> 01:54:53.720]   there's been a kind of a tide change or a tectonic shift
[01:54:53.720 --> 01:54:57.080]   in the way that people interact with technology.
[01:54:57.080 --> 01:54:57.920]   Sorry?
[01:54:57.920 --> 01:55:00.320]   You got to wonder why a guy with dementia had a handgun,
[01:55:00.320 --> 01:55:03.080]   but that's Arizona for you.
[01:55:03.080 --> 01:55:04.120]   Marriott--
[01:55:04.120 --> 01:55:06.000]   New York Times, Marriott Data Breach.
[01:55:06.000 --> 01:55:07.160]   Remember that one?
[01:55:07.160 --> 01:55:09.680]   Oh, yeah, it was only two weeks ago.
[01:55:09.680 --> 01:55:12.740]   Half a billion records of people who
[01:55:12.740 --> 01:55:15.920]   use the Starwood Group reservation system,
[01:55:15.920 --> 01:55:18.560]   including, in some cases, passports.
[01:55:18.560 --> 01:55:19.680]   People like me?
[01:55:19.680 --> 01:55:20.400]   Man me?
[01:55:20.400 --> 01:55:20.920]   Who didn't?
[01:55:20.920 --> 01:55:22.040]   I'm like, Starwood Gold.
[01:55:22.040 --> 01:55:22.540]   Yeah.
[01:55:22.540 --> 01:55:23.040]   Yeah.
[01:55:23.040 --> 01:55:23.400]   Me too.
[01:55:23.400 --> 01:55:24.360]   Half a billion people.
[01:55:24.360 --> 01:55:25.960]   That's quite a lot of people.
[01:55:25.960 --> 01:55:29.000]   Turns out, according to the New York Times, which is quoting,
[01:55:29.000 --> 01:55:32.840]   of course, our intelligence agencies,
[01:55:32.840 --> 01:55:35.920]   that the hack came from China.
[01:55:35.920 --> 01:55:37.640]   And actually, the larger story, it
[01:55:37.640 --> 01:55:42.760]   was part of a large four-year intelligence-gathering effort.
[01:55:42.760 --> 01:55:45.280]   You remember when the Office of Personnel Management,
[01:55:45.280 --> 01:55:48.040]   the US government's OPM, was hacked?
[01:55:48.040 --> 01:55:51.200]   That was part of it.
[01:55:51.200 --> 01:55:53.520]   Apparently, the Chinese--
[01:55:53.520 --> 01:55:55.160]   even though they deny this, by the way,
[01:55:55.160 --> 01:55:58.040]   I should give them fair reporting here.
[01:55:58.040 --> 01:56:00.680]   China firmly opposes all forms of cyber-attacking cracks
[01:56:00.680 --> 01:56:02.840]   down on accordance with the law.
[01:56:02.840 --> 01:56:05.040]   If offered evidence, the relevant Chinese departments
[01:56:05.040 --> 01:56:07.320]   will carry out investigations according to the law.
[01:56:07.320 --> 01:56:11.280]   Apparently, according to the anonymous sources
[01:56:11.280 --> 01:56:17.280]   in this article, the Chinese started doing this in 2014
[01:56:17.280 --> 01:56:24.520]   because they wanted to gather information about spies.
[01:56:24.520 --> 01:56:27.800]   It wasn't about you and me, but they
[01:56:27.800 --> 01:56:31.320]   wanted to know about Chinese nationals in the United States
[01:56:31.320 --> 01:56:32.800]   where they traveled.
[01:56:32.800 --> 01:56:37.120]   They wanted to know about other people.
[01:56:37.120 --> 01:56:39.600]   They were kind of keeping-- remember the Anthem hack?
[01:56:39.600 --> 01:56:42.760]   American Health Insurance Company, Anthem?
[01:56:42.760 --> 01:56:45.680]   Also, part of that, they wanted Social Security Numbers
[01:56:45.680 --> 01:56:47.400]   and patients' medical histories.
[01:56:47.400 --> 01:56:55.120]   It's another story that I guess I credit.
[01:56:55.120 --> 01:56:56.400]   I don't know.
[01:56:56.400 --> 01:57:00.840]   The data that was taken is not shown up on the dark web,
[01:57:00.840 --> 01:57:03.400]   which people say suggest it's a state actor.
[01:57:03.400 --> 01:57:06.600]   It doesn't indicate it's China, particularly.
[01:57:06.600 --> 01:57:10.240]   But it might mean it's not some random individual.
[01:57:10.240 --> 01:57:11.920]   And it's also--
[01:57:11.920 --> 01:57:13.480]   Oh, go ahead.
[01:57:13.480 --> 01:57:16.720]   Oh, I mean, the data, in this case,
[01:57:16.720 --> 01:57:19.440]   while it is not, I guess, Social Security Numbers
[01:57:19.440 --> 01:57:21.640]   or something that, in and of itself,
[01:57:21.640 --> 01:57:23.880]   you think of as a large security risk,
[01:57:23.880 --> 01:57:26.640]   any number of identifying information
[01:57:26.640 --> 01:57:29.640]   can be used to identify you in other means
[01:57:29.640 --> 01:57:32.240]   or find out more about a particular person
[01:57:32.240 --> 01:57:34.960]   or their habits online if you're also combining that
[01:57:34.960 --> 01:57:37.720]   with other information from data brokers or perhaps
[01:57:37.720 --> 01:57:40.880]   like anonymized information, the sort of information
[01:57:40.880 --> 01:57:47.480]   that an entity could collect from a hack of, yeah, Marriott,
[01:57:47.480 --> 01:57:51.040]   while it could seem innocuous in certain lights
[01:57:51.040 --> 01:57:55.000]   when combined with other pools of information or data.
[01:57:55.000 --> 01:57:59.280]   It can reveal people's identities.
[01:57:59.280 --> 01:58:01.960]   There was a truce.
[01:58:01.960 --> 01:58:06.400]   We thought we had a truce with China over cyber warfare
[01:58:06.400 --> 01:58:11.320]   and cyber espionage back in 2015.
[01:58:11.320 --> 01:58:13.760]   They apparently agreed not to do it.
[01:58:13.760 --> 01:58:15.120]   And we agreed not to do it to them.
[01:58:15.120 --> 01:58:18.600]   On the other hand, General Clapper, James Clapper,
[01:58:18.600 --> 01:58:21.280]   the former head of the Defense Intelligence Agency,
[01:58:21.280 --> 01:58:25.280]   said, hey, well, if we could do it, we'd do it to them.
[01:58:25.280 --> 01:58:30.720]   So maybe it's just tit for tat, as they say.
[01:58:30.720 --> 01:58:33.080]   Chinese were also interested in getting information
[01:58:33.080 --> 01:58:35.840]   about American security officials.
[01:58:35.840 --> 01:58:38.360]   Anything you can get is going to be useful.
[01:58:38.360 --> 01:58:40.440]   So there is that.
[01:58:40.440 --> 01:58:42.440]   I'll throw that into the mix.
[01:58:42.440 --> 01:58:46.200]   Well, here's the question I wanted to ask,
[01:58:46.200 --> 01:58:49.040]   which is what are people supposed to do in these cases?
[01:58:49.040 --> 01:58:53.280]   This isn't the first gigantic hack that's been committed.
[01:58:53.280 --> 01:58:56.280]   There was a target at one point,
[01:58:56.280 --> 01:58:58.000]   I think Home Depot is in there.
[01:58:58.000 --> 01:58:59.000]   Equifax.
[01:58:59.000 --> 01:59:01.040]   Yahoo, Tumblr, everything.
[01:59:01.040 --> 01:59:02.960]   Yeah, so Michael, they're supposed to do what you do,
[01:59:02.960 --> 01:59:04.040]   which is throw up your hands and say,
[01:59:04.040 --> 01:59:06.840]   I give up on privacy, right?
[01:59:06.840 --> 01:59:09.040]   I mean, is that what you did?
[01:59:09.040 --> 01:59:10.560]   It is to a certain extent.
[01:59:10.560 --> 01:59:12.240]   Yeah, I think like, you know, there are definitely best
[01:59:12.240 --> 01:59:13.080]   practices, right?
[01:59:13.080 --> 01:59:17.160]   You're supposed to, I don't know, it helps to change your password
[01:59:17.160 --> 01:59:20.240]   every so often or have varying levels of,
[01:59:20.240 --> 01:59:23.920]   of, of, of, sorry, of,
[01:59:23.920 --> 01:59:27.840]   Well, supposedly those companies are also supposed to do things
[01:59:27.840 --> 01:59:28.800]   like that.
[01:59:28.800 --> 01:59:30.440]   Marriott did none of them.
[01:59:30.440 --> 01:59:33.320]   They had the worst security practices ever.
[01:59:33.320 --> 01:59:35.440]   And so that's the other thing I think we need to do,
[01:59:35.440 --> 01:59:37.320]   which is hold these companies feet to the fire.
[01:59:37.320 --> 01:59:39.520]   That's what GDPR does.
[01:59:39.520 --> 01:59:41.760]   But I think we should have something in the United States too,
[01:59:41.760 --> 01:59:45.800]   that if a company breaches its responsibility to protect our data
[01:59:45.800 --> 01:59:49.560]   in a really gross way, like Marriott did,
[01:59:49.560 --> 01:59:51.800]   that they should pay a severe fine.
[01:59:51.800 --> 01:59:52.760]   That should be a penalty.
[01:59:52.760 --> 01:59:56.360]   Equifax not only got off scot-free, they made money on it.
[01:59:56.360 --> 01:59:57.960]   Yeah, yeah, I'm very well said.
[01:59:57.960 --> 01:59:59.760]   I mean, I would, I completely agree with that.
[01:59:59.760 --> 02:00:03.080]   The sad thing is that there is no piece of legislation
[02:00:03.080 --> 02:00:04.840]   that I've seen that nobody's doing it.
[02:00:04.840 --> 02:00:07.640]   Is, is it? Yeah, I mean, it just weren't years away from this.
[02:00:07.640 --> 02:00:09.640]   It's, it's pathetic.
[02:00:09.640 --> 02:00:11.600]   Well, congratulations.
[02:00:11.600 --> 02:00:15.160]   I have to offer a congratulations to YouTube Rewind.
[02:00:15.160 --> 02:00:20.440]   It is now the most hated video on YouTube.
[02:00:20.440 --> 02:00:22.880]   Finally beat Justin Bieber's baby.
[02:00:22.880 --> 02:00:29.720]   The Rewind every, every year, the Rewind.
[02:00:29.720 --> 02:00:31.680]   Well, you, you cover this stuff in Mashable.
[02:00:31.680 --> 02:00:35.280]   What tell us the Rewind celebrates YouTube creators, right?
[02:00:35.280 --> 02:00:38.600]   And this year, it started as it often does with a celebrity.
[02:00:38.600 --> 02:00:41.840]   In this case, Will Smith, who used YouTube to stream
[02:00:41.840 --> 02:00:45.240]   jumping off a cliff with a bungee cord,
[02:00:45.240 --> 02:00:48.000]   or a, no, I guess he jumped out of a helicopter with a,
[02:00:48.000 --> 02:00:48.840]   yep, here, let me.
[02:00:48.840 --> 02:00:51.240]   Night and Marquez Brownlee.
[02:00:51.240 --> 02:00:53.360]   And Marquez getting a shout out.
[02:00:53.360 --> 02:00:58.240]   They're all in a bus.
[02:00:58.240 --> 02:00:59.080]   There's Marquez.
[02:00:59.080 --> 02:01:00.400]   No, why wouldn't I ask for it?
[02:01:00.400 --> 02:01:01.600]   Quit horsing around.
[02:01:01.600 --> 02:01:03.600]   Oh, oh, that's, what's his name?
[02:01:03.600 --> 02:01:04.200]   Ninja.
[02:01:04.200 --> 02:01:05.200]   Ninja.
[02:01:05.200 --> 02:01:06.480]   You said jumping music?
[02:01:06.480 --> 02:01:07.480]   Jumping music.
[02:01:07.480 --> 02:01:08.760]   'Cause you know why?
[02:01:08.760 --> 02:01:11.200]   'Cause they're in a Fortnite bus,
[02:01:11.200 --> 02:01:12.720]   and they're gonna have to jump out.
[02:01:12.720 --> 02:01:13.720]   What up?
[02:01:13.720 --> 02:01:17.480]   So even though Google has, YouTube has nothing to do
[02:01:17.480 --> 02:01:21.080]   with Fortnite, they're gonna jump on the Fortnite bandwagon.
[02:01:21.080 --> 02:01:22.080]   I'm gonna thank you.
[02:01:22.080 --> 02:01:22.920]   Bye, Ninja.
[02:01:22.920 --> 02:01:25.320]   (laughing)
[02:01:25.320 --> 02:01:28.120]   You know what this really is,
[02:01:28.120 --> 02:01:31.560]   is this makes the creators, the YouTube stars happy, right?
[02:01:31.560 --> 02:01:33.960]   They get this day in the sunshine.
[02:01:33.960 --> 02:01:37.000]   Marquez Brownlee's gonna be just jumping up and down.
[02:01:37.000 --> 02:01:39.000]   But for sure.
[02:01:39.000 --> 02:01:40.920]   Well, yeah, I don't think actually most,
[02:01:40.920 --> 02:01:42.680]   I think most of the creators were unhappy
[02:01:42.680 --> 02:01:43.520]   with the way that this came out.
[02:01:43.520 --> 02:01:45.280]   Were they embarrassed?
[02:01:45.280 --> 02:01:46.560]   I think many of them were,
[02:01:46.560 --> 02:01:49.080]   and then the other issue is that
[02:01:49.080 --> 02:01:51.680]   a lot of the most popular creators were not featured.
[02:01:51.680 --> 02:01:54.200]   So people like Logan Paul and--
[02:01:54.200 --> 02:01:55.200]   Pooty pie.
[02:01:55.200 --> 02:01:56.720]   Logan Paul would have been, yeah,
[02:01:56.720 --> 02:01:58.720]   both of those I feel like would have been a little controversial
[02:01:58.720 --> 02:01:59.880]   had they been included.
[02:01:59.880 --> 02:02:02.120]   Yeah, both of them were sanctioned by YouTube
[02:02:02.120 --> 02:02:03.880]   for stuff they did this year.
[02:02:03.880 --> 02:02:05.320]   Right, Alex Jones.
[02:02:05.320 --> 02:02:06.160]   Alex Jones.
[02:02:06.160 --> 02:02:07.000]   No, but--
[02:02:07.000 --> 02:02:07.840]   But look at this.
[02:02:07.840 --> 02:02:09.920]   This is why the creators were unhappy.
[02:02:09.920 --> 02:02:11.840]   'Cause before when they did it, they were happy.
[02:02:11.840 --> 02:02:14.880]   Then they saw 13 million thumbs down
[02:02:14.880 --> 02:02:18.760]   on a video that has 139 million views.
[02:02:18.760 --> 02:02:21.880]   Let's see, I think Marquez had a response here.
[02:02:21.880 --> 02:02:24.760]   Let's see what the problem with YouTube rewind.
[02:02:24.760 --> 02:02:27.640]   Is this Marquez's video?
[02:02:27.640 --> 02:02:31.920]   Yeah, I think he talks a little bit about it behind the scenes.
[02:02:31.920 --> 02:02:32.760]   Oh, he's amazing.
[02:02:32.760 --> 02:02:33.880]   YouTube rewind.
[02:02:33.880 --> 02:02:36.080]   Every year, the biggest celebration
[02:02:36.080 --> 02:02:38.760]   of the top stuff on YouTube that year,
[02:02:38.760 --> 02:02:41.560]   the biggest creators, the best videos,
[02:02:41.560 --> 02:02:43.320]   the memes, everything.
[02:02:43.320 --> 02:02:44.400]   At least we thought.
[02:02:44.400 --> 02:02:45.680]   A lot of people have been asking me about this.
[02:02:45.680 --> 02:02:46.960]   You may have noticed about a week ago,
[02:02:46.960 --> 02:02:49.640]   YouTube rewind 2018 was uploaded,
[02:02:49.640 --> 02:02:52.800]   and it's not exactly doing too hot.
[02:02:52.800 --> 02:02:54.880]   In fact, it's currently, in just a few days,
[02:02:54.880 --> 02:02:58.640]   the second most disliked video ever uploaded--
[02:02:58.640 --> 02:02:59.480]   No number one.
[02:02:59.480 --> 02:03:00.320]   All time.
[02:03:00.320 --> 02:03:02.800]   First place being Justin Bieber's baby.
[02:03:02.800 --> 02:03:05.880]   And it's on pace to pass that probably pretty soon.
[02:03:05.880 --> 02:03:08.640]   So the most outwardly hated video ever uploaded to this site.
[02:03:08.640 --> 02:03:10.560]   Can I just ask really asking a quick question.
[02:03:10.560 --> 02:03:15.760]   Is he wearing advertising on his jacket?
[02:03:15.760 --> 02:03:16.880]   Oh, God.
[02:03:16.880 --> 02:03:17.720]   He'd be smart actually.
[02:03:17.720 --> 02:03:18.560]   He'd be smart actually.
[02:03:18.560 --> 02:03:19.400]   He'd better be.
[02:03:19.400 --> 02:03:20.240]   Yeah.
[02:03:20.240 --> 02:03:22.440]   He's definitely getting paid for that if it's not something.
[02:03:22.440 --> 02:03:24.400]   I would be an I-er him.
[02:03:24.400 --> 02:03:25.800]   Yeah, I mean--
[02:03:25.800 --> 02:03:27.240]   It's like a NASCAR driver.
[02:03:27.240 --> 02:03:28.640]   He's like a NASCAR driver.
[02:03:28.640 --> 02:03:29.480]   He's a $1,000.
[02:03:29.480 --> 02:03:30.600]   A chess grandmaster.
[02:03:30.600 --> 02:03:32.520]   A car car car car car car car car car car car car car car car car.
[02:03:32.520 --> 02:03:35.320]   Why don't I have anything to wear on?
[02:03:35.320 --> 02:03:36.160]   I need some badges.
[02:03:36.160 --> 02:03:38.080]   That's why the eBay bike is there, right?
[02:03:38.080 --> 02:03:38.880]   It's the eBay.
[02:03:38.880 --> 02:03:39.960]   I wish they were paying me.
[02:03:39.960 --> 02:03:40.720]   I was paying them.
[02:03:40.720 --> 02:03:42.200]   I asked your son.
[02:03:42.200 --> 02:03:43.040]   Yeah, that's my son.
[02:03:43.040 --> 02:03:44.920]   My son's doing sales for us right now.
[02:03:44.920 --> 02:03:49.000]   So Mark has, I guess, said a lot of people were--
[02:03:49.000 --> 02:03:50.840]   I have to think that the number of dislikes
[02:03:50.840 --> 02:03:54.400]   had something to do with their repudiation
[02:03:54.400 --> 02:03:56.120]   of the YouTube rewind.
[02:03:56.120 --> 02:03:57.040]   It's worth watching.
[02:03:57.040 --> 02:03:59.800]   It's got 139 million views.
[02:03:59.800 --> 02:04:00.760]   It's an honor.
[02:04:00.760 --> 02:04:01.600]   It's an honor.
[02:04:01.600 --> 02:04:03.320]   To be fair, it is a terrible video.
[02:04:03.320 --> 02:04:06.560]   It is one of the worst things I've seen on YouTube all year.
[02:04:06.560 --> 02:04:07.280]   Well, wait a minute.
[02:04:07.280 --> 02:04:07.800]   Wait a minute.
[02:04:07.800 --> 02:04:08.520]   No, it's not.
[02:04:08.520 --> 02:04:10.320]   It's not even close.
[02:04:10.320 --> 02:04:11.520]   Let's be honest.
[02:04:11.520 --> 02:04:12.040]   That's true.
[02:04:12.040 --> 02:04:13.160]   That's a pretty--
[02:04:13.160 --> 02:04:14.440]   There are some bad things on YouTube.
[02:04:14.440 --> 02:04:17.000]   That is the, I mean, close to the worst videos on YouTube.
[02:04:17.000 --> 02:04:18.360]   Oh, my God.
[02:04:18.360 --> 02:04:18.800]   Yeah.
[02:04:18.800 --> 02:04:19.160]   OK.
[02:04:19.160 --> 02:04:19.720]   Fair point.
[02:04:19.720 --> 02:04:20.760]   Fair point.
[02:04:20.760 --> 02:04:23.440]   But it was-- I think it was just--
[02:04:23.440 --> 02:04:26.280]   it wasn't representative of what the network is.
[02:04:26.280 --> 02:04:30.360]   I think to say that it's a celebration of the creators
[02:04:30.360 --> 02:04:33.040]   and of some of the most popular things on YouTube,
[02:04:33.040 --> 02:04:34.600]   while turning a blind eye--
[02:04:34.600 --> 02:04:38.880]   turning a very obvious blind eye to all of this smudgy,
[02:04:38.880 --> 02:04:42.480]   smudgy, terrible stuff, I think that was the reason
[02:04:42.480 --> 02:04:43.400]   why people were outraged.
[02:04:43.400 --> 02:04:46.680]   It just simply did not represent what was happening on the network.
[02:04:46.680 --> 02:04:49.080]   They wanted Alex Jones jumping out of a bus.
[02:04:49.080 --> 02:04:49.440]   That--
[02:04:49.440 --> 02:04:49.960]   Yeah.
[02:04:49.960 --> 02:04:53.040]   You wanted 7 million videos of Gordon Peterson
[02:04:53.040 --> 02:04:56.200]   defeating some feminist.
[02:04:56.200 --> 02:04:56.560]   Goodness.
[02:04:56.560 --> 02:04:58.040]   Well, it also was a little too long.
[02:04:58.040 --> 02:05:00.520]   I think that it just went on and on and on and on.
[02:05:00.520 --> 02:05:03.200]   And so it was trying to be everything to everyone.
[02:05:03.200 --> 02:05:06.720]   And actually, it just failed to do anything well.
[02:05:06.720 --> 02:05:11.720]   Even the MKBHD cameo.
[02:05:11.720 --> 02:05:15.800]   He explains in his video that he shot over 15 minutes
[02:05:15.800 --> 02:05:18.120]   of this Fortnite scene.
[02:05:18.120 --> 02:05:20.120]   And they ended up using a very small portion of it.
[02:05:20.120 --> 02:05:22.480]   Cazillion Musk's defense too, wasn't it?
[02:05:22.480 --> 02:05:24.240]   I just wanted to ask that the rest of this show
[02:05:24.240 --> 02:05:27.160]   is brought to you by the Welsh National Government
[02:05:27.160 --> 02:05:29.400]   and the Leaks Council.
[02:05:29.400 --> 02:05:30.120]   Nice.
[02:05:30.120 --> 02:05:30.620]   Nice.
[02:05:30.620 --> 02:05:31.520]   What do you think?
[02:05:31.520 --> 02:05:32.040]   Good.
[02:05:32.040 --> 02:05:32.520]   Yeah.
[02:05:32.520 --> 02:05:33.040]   I love it.
[02:05:33.040 --> 02:05:34.320]   I hope we're getting paid well, yeah.
[02:05:34.320 --> 02:05:37.640]   [LAUGHTER]
[02:05:37.640 --> 02:05:41.880]   I don't know if there's that much money in Leaks.
[02:05:41.880 --> 02:05:43.200]   I love Leaks for what it's worth.
[02:05:43.200 --> 02:05:44.200]   Oh, good.
[02:05:44.200 --> 02:05:45.000]   Oh, see?
[02:05:45.000 --> 02:05:45.520]   They're great.
[02:05:45.520 --> 02:05:46.400]   We're Leaks.
[02:05:46.400 --> 02:05:47.800]   We love Leaks.
[02:05:47.800 --> 02:05:48.800]   Leaks are great.
[02:05:48.800 --> 02:05:50.640]   These are big Leak cabal here, yeah.
[02:05:50.640 --> 02:05:51.160]   Yeah.
[02:05:51.160 --> 02:05:52.640]   Leaks.
[02:05:52.640 --> 02:05:53.600]   Eat more Leaks.
[02:05:53.600 --> 02:05:54.480]   Yeah, Leaks.
[02:05:54.480 --> 02:05:56.120]   Yay Leaks.
[02:05:56.120 --> 02:05:57.120]   Leaks.
[02:05:57.120 --> 02:06:01.080]   [LAUGHTER]
[02:06:01.080 --> 02:06:04.280]   Microsoft Teams is growing ridiculously fast.
[02:06:04.280 --> 02:06:06.880]   This actually is a little bit of a surprise.
[02:06:06.880 --> 02:06:09.880]   So Slack bought Hip Chat.
[02:06:09.880 --> 02:06:10.480]   The Atlassian product.
[02:06:10.480 --> 02:06:12.360]   They bought up Slack some of the IP of Hip Chat.
[02:06:12.360 --> 02:06:12.920]   Yeah.
[02:06:12.920 --> 02:06:13.320]   Well, that's right.
[02:06:13.320 --> 02:06:14.080]   Atlassian's so long.
[02:06:14.080 --> 02:06:15.480]   Which sort of dwindling--
[02:06:15.480 --> 02:06:17.120]   Hip Chat is winding it down.
[02:06:17.120 --> 02:06:19.120]   And Slack is number one, right?
[02:06:19.120 --> 02:06:20.960]   Microsoft would like to get into that business.
[02:06:20.960 --> 02:06:23.200]   So they decided to give away.
[02:06:23.200 --> 02:06:25.080]   You're not going to take me seriously while I'm wearing this,
[02:06:25.080 --> 02:06:26.360]   are you?
[02:06:26.360 --> 02:06:27.320]   They decided to--
[02:06:27.320 --> 02:06:29.200]   You just look so authoritative.
[02:06:29.200 --> 02:06:30.440]   I'm taking off the leak.
[02:06:30.440 --> 02:06:31.600]   I don't care well as Leak.
[02:06:31.600 --> 02:06:33.080]   It kind of looks like a crown to me.
[02:06:33.080 --> 02:06:33.600]   Yeah.
[02:06:33.600 --> 02:06:35.080]   Anyways.
[02:06:35.080 --> 02:06:36.760]   [LAUGHTER]
[02:06:36.760 --> 02:06:39.200]   It actually is-- it's an helmet that I'm
[02:06:39.200 --> 02:06:41.760]   going to wear riding my e-bike down the street
[02:06:41.760 --> 02:06:43.400]   and see if anybody points a gun at me.
[02:06:43.400 --> 02:06:44.840]   For visibility, yeah.
[02:06:44.840 --> 02:06:45.560]   Yeah.
[02:06:45.560 --> 02:06:47.960]   For visibility.
[02:06:47.960 --> 02:06:50.600]   The beard sheds, unfortunately.
[02:06:50.600 --> 02:06:53.400]   So we had to decide-- we were using Hip Chat.
[02:06:53.400 --> 02:06:54.160]   We had to decide.
[02:06:54.160 --> 02:06:55.640]   And Hip Chat was a sponsor.
[02:06:55.640 --> 02:06:57.480]   And so we switched to Slack, which became a sponsor.
[02:06:57.480 --> 02:06:59.720]   So that's a good thing.
[02:06:59.720 --> 02:07:01.080]   But Slack is not free.
[02:07:01.080 --> 02:07:02.840]   Slack has a free tier, but really--
[02:07:02.840 --> 02:07:04.840]   You're doing a good free tier.
[02:07:04.840 --> 02:07:05.760]   That's-- we use a free tier.
[02:07:05.760 --> 02:07:07.320]   We don't use a paid version.
[02:07:07.320 --> 02:07:11.120]   But Microsoft Teams is really free and apparently growing.
[02:07:11.120 --> 02:07:11.880]   Like Teams are--
[02:07:11.880 --> 02:07:13.520]   I think also is tiered and you have
[02:07:13.520 --> 02:07:14.880]   to pay for the really good stuff.
[02:07:14.880 --> 02:07:15.480]   Oh, OK.
[02:07:15.480 --> 02:07:16.440]   But they did.
[02:07:16.440 --> 02:07:17.840]   They gave it a lot more away from it.
[02:07:17.840 --> 02:07:19.960]   They came out with a aggressively good free tier
[02:07:19.960 --> 02:07:24.360]   after not having had super amounts of traction.
[02:07:24.360 --> 02:07:27.240]   But I bet you, if you ask all the cool kids,
[02:07:27.240 --> 02:07:29.360]   they're using Slack.
[02:07:29.360 --> 02:07:29.720]   Cool kids?
[02:07:29.720 --> 02:07:31.200]   Yes.
[02:07:31.200 --> 02:07:31.960]   Yeah, I'm--
[02:07:31.960 --> 02:07:33.440]   Oh, you ever used Slack.
[02:07:33.440 --> 02:07:34.440]   Yeah.
[02:07:34.440 --> 02:07:34.840]   I'm not going to--
[02:07:34.840 --> 02:07:35.240]   I'm not going to--
[02:07:35.240 --> 02:07:35.240]   I'm not going to--
[02:07:35.240 --> 02:07:36.280]   --just the most robust right now.
[02:07:36.280 --> 02:07:42.720]   And I think some of these tools will be democratized.
[02:07:42.720 --> 02:07:46.640]   I think that Slack doesn't really do anything that unique.
[02:07:46.640 --> 02:07:49.320]   It's just instant messaging with emojis, basically.
[02:07:49.320 --> 02:07:52.080]   And also, their embeds are really nice,
[02:07:52.080 --> 02:07:53.760]   like the inline chat embed.
[02:07:53.760 --> 02:07:57.480]   So if I see a YouTube clip and I drop that in the chat,
[02:07:57.480 --> 02:07:59.240]   it will embed that video so that someone
[02:07:59.240 --> 02:08:00.880]   doesn't have to click out to YouTube.
[02:08:00.880 --> 02:08:03.160]   We can just watch the video within the chat.
[02:08:03.160 --> 02:08:05.960]   It's fantastic for a newsroom that is like really, especially
[02:08:05.960 --> 02:08:10.360]   a digitally native newsroom that is super important for tweets,
[02:08:10.360 --> 02:08:13.920]   for YouTube, for basically any of this content that
[02:08:13.920 --> 02:08:17.320]   were digesting throughout the day.
[02:08:17.320 --> 02:08:20.200]   But I also just don't think that that's that unique.
[02:08:20.200 --> 02:08:22.120]   At the end of the day, Facebook and Microsoft
[02:08:22.120 --> 02:08:25.280]   and all these companies that employ hundreds of engineers
[02:08:25.280 --> 02:08:29.800]   can just as easily create an instant messaging platform
[02:08:29.800 --> 02:08:31.880]   within embeds.
[02:08:31.880 --> 02:08:35.480]   And my expectation is that it's going
[02:08:35.480 --> 02:08:38.760]   to become a very competitive marketplace.
[02:08:38.760 --> 02:08:41.960]   Because from what I read, Slack was talking about potentially
[02:08:41.960 --> 02:08:43.200]   going public next year.
[02:08:43.200 --> 02:08:46.560]   And if that's the case, I think it's similar to Uber
[02:08:46.560 --> 02:08:51.680]   in that what the technology aspect isn't all that unique.
[02:08:51.680 --> 02:08:53.840]   What's really good about their business right now
[02:08:53.840 --> 02:08:55.880]   is that they've scaled really well.
[02:08:55.880 --> 02:08:57.560]   They have a lot of people that have
[02:08:57.560 --> 02:08:59.240]   bought into that system.
[02:08:59.240 --> 02:09:01.640]   But it will be easy for competitors
[02:09:01.640 --> 02:09:05.840]   to create copycat versions of those businesses.
[02:09:05.840 --> 02:09:09.920]   And so I think that there's inherently a lot of risk
[02:09:09.920 --> 02:09:13.400]   with something like Slack or Uber, I guess,
[02:09:13.400 --> 02:09:14.680]   from a business standpoint.
[02:09:14.680 --> 02:09:17.760]   I don't think that they do anything that unique.
[02:09:17.760 --> 02:09:20.280]   It's not impossible for Facebook or Microsoft
[02:09:20.280 --> 02:09:23.120]   to create something similar, in my opinion.
[02:09:23.120 --> 02:09:25.720]   Slack did a good job of giving it a lot of polish,
[02:09:25.720 --> 02:09:27.560]   which is harder than it looks.
[02:09:27.560 --> 02:09:28.880]   I mean, it is elegant.
[02:09:28.880 --> 02:09:29.720]   It's not nice.
[02:09:29.720 --> 02:09:31.240]   That chat wasn't archrival for a while,
[02:09:31.240 --> 02:09:33.840]   but Slack was a lot more polished and pleasant to use.
[02:09:33.840 --> 02:09:34.280]   Fun.
[02:09:34.280 --> 02:09:38.000]   And I am glad to see that Microsoft is doing OK,
[02:09:38.000 --> 02:09:40.560]   just because I think it's healthy for Slack to have
[02:09:40.560 --> 02:09:43.600]   at least one major archrival.
[02:09:43.600 --> 02:09:47.160]   Facebook has Facebook at work, which they're certainly still--
[02:09:47.160 --> 02:09:48.760]   Does anybody use that?
[02:09:48.760 --> 02:09:50.000]   They certainly do some big user.
[02:09:50.000 --> 02:09:51.320]   Facebook employees, too.
[02:09:51.320 --> 02:09:51.960]   Mashable.
[02:09:51.960 --> 02:09:54.240]   Mashable uses VDO yet.
[02:09:54.240 --> 02:09:55.240]   Oh.
[02:09:55.240 --> 02:09:56.240]   Interesting.
[02:09:56.240 --> 02:09:57.640]   I'd be able to have that huge band.
[02:09:57.640 --> 02:09:59.640]   Yeah.
[02:09:59.640 --> 02:10:02.320]   I'd like to announce that right after 60 minutes this evening
[02:10:02.320 --> 02:10:04.680]   is on CBS.
[02:10:04.680 --> 02:10:08.520]   It's the all-new Freaks and Leaks, a show--
[02:10:08.520 --> 02:10:09.800]   No.
[02:10:09.800 --> 02:10:11.200]   Hill Leaks Blues.
[02:10:11.200 --> 02:10:13.000]   No.
[02:10:13.000 --> 02:10:14.160]   We're going to take a little break.
[02:10:14.160 --> 02:10:14.600]   Live Leaks.
[02:10:14.600 --> 02:10:15.760]   Live Leaks.
[02:10:15.760 --> 02:10:17.640]   [LAUGHTER]
[02:10:17.640 --> 02:10:22.680]   TMZ presents Live Leaks.
[02:10:22.680 --> 02:10:24.880]   Do we have a--
[02:10:24.880 --> 02:10:26.040]   No promo this week.
[02:10:26.040 --> 02:10:26.440]   Oh, God.
[02:10:26.440 --> 02:10:29.520]   You were going to get me out of this.
[02:10:29.520 --> 02:10:30.640]   That's all, folks.
[02:10:31.640 --> 02:10:32.640]   [LAUGHTER]
[02:10:32.640 --> 02:10:36.920]   Hey, actually, I want to say a good thing about face recognition.
[02:10:36.920 --> 02:10:37.880]   I had no idea.
[02:10:37.880 --> 02:10:42.840]   Do you know that Taylor Swift has lots of stalkers?
[02:10:42.840 --> 02:10:44.560]   I don't even want to say a number,
[02:10:44.560 --> 02:10:46.320]   but actually, I guess I'm not surprised.
[02:10:46.320 --> 02:10:46.880]   Hundreds.
[02:10:46.880 --> 02:10:48.640]   Hundreds.
[02:10:48.640 --> 02:10:50.040]   So they've used-- they start--
[02:10:50.040 --> 02:10:53.560]   Security for Taylor Swift at California's Rose Bowl last May
[02:10:53.560 --> 02:11:00.280]   used face recognition to spot the stalkers.
[02:11:00.280 --> 02:11:05.040]   Hundreds of images of known Taylor Swift stalkers.
[02:11:05.040 --> 02:11:07.080]   The kiosk was set up to show highlights
[02:11:07.080 --> 02:11:09.440]   of the singer's rehearsals.
[02:11:09.440 --> 02:11:12.200]   Seekerly recorded the faces of onlookers
[02:11:12.200 --> 02:11:16.520]   which were sent to a command post across the country in Nashville.
[02:11:16.520 --> 02:11:19.040]   Everybody who went by would stop and stare at the software
[02:11:19.040 --> 02:11:21.280]   would start working according to chief security officer
[02:11:21.280 --> 02:11:23.040]   at Live Entertainment Security Company, Oakview,
[02:11:23.040 --> 02:11:26.560]   a group who personally attended the event.
[02:11:26.560 --> 02:11:28.240]   It's unknown if any of the footage were kept,
[02:11:28.240 --> 02:11:30.320]   or even if I identified any real stalkers
[02:11:30.320 --> 02:11:31.800]   or what happened if it did.
[02:11:31.800 --> 02:11:34.640]   But this is actually confirming something
[02:11:34.640 --> 02:11:37.320]   we've been hearing for some time, which is that stadiums routinely
[02:11:37.320 --> 02:11:40.680]   use face recognition to recognize rowdy fans, fans that
[02:11:40.680 --> 02:11:42.560]   have been kicked out in the past.
[02:11:42.560 --> 02:11:46.920]   A man if I were Taylor Swift, you bet I'd use that.
[02:11:46.920 --> 02:11:49.520]   Yeah, I think this is actually a pretty common practice at this point.
[02:11:49.520 --> 02:11:53.480]   I remember earlier this year in China,
[02:11:53.480 --> 02:11:58.000]   the Chinese police used facial recognition to identify a...
[02:11:58.000 --> 02:11:59.400]   Jay Walker's.
[02:11:59.400 --> 02:12:01.400]   No, no, no, it was at a concert.
[02:12:01.400 --> 02:12:04.680]   It was like it was a sea of 60,000 people,
[02:12:04.680 --> 02:12:07.720]   and they were able to find this man that was accused
[02:12:07.720 --> 02:12:09.480]   of economic crimes or something like that.
[02:12:09.480 --> 02:12:11.920]   So I just remember the headline being,
[02:12:11.920 --> 02:12:16.560]   that this person was found in a sea of 60,000 people
[02:12:16.560 --> 02:12:17.640]   at a concert.
[02:12:17.640 --> 02:12:20.960]   And also, I believe Ticketmaster announced that they were going
[02:12:20.960 --> 02:12:26.320]   to be embracing more facial recognition or surveillance
[02:12:26.320 --> 02:12:28.840]   technology at concerts in the US.
[02:12:28.840 --> 02:12:32.320]   So I think basically my view on this is,
[02:12:32.320 --> 02:12:35.000]   if you're going to a concert, you're probably
[02:12:35.000 --> 02:12:39.520]   being surveilled to an extreme degree at this point.
[02:12:39.520 --> 02:12:43.040]   I think it's actually becoming fairly common.
[02:12:43.040 --> 02:12:45.880]   And I guess I'm OK with that.
[02:12:45.880 --> 02:12:47.120]   Yeah, I mean...
[02:12:47.120 --> 02:12:50.120]   I'm interested to see how this works from an advertising
[02:12:50.120 --> 02:12:52.720]   perspective though, because when it comes to concerts,
[02:12:52.720 --> 02:12:56.680]   I've always been a bit wary or kind of interested in the
[02:12:56.680 --> 02:13:01.440]   overlap of security, technology, and advertising ever since.
[02:13:01.440 --> 02:13:03.840]   I went to a concert earlier this year,
[02:13:03.840 --> 02:13:07.000]   I think the Arctic Monkeys, and the only way that I could
[02:13:07.000 --> 02:13:09.960]   use my ticket that I purchased was to download an app
[02:13:09.960 --> 02:13:11.400]   by the company Access.
[02:13:11.400 --> 02:13:14.320]   And of course, and I also had forgotten to,
[02:13:14.320 --> 02:13:16.200]   I don't know, bring headphones in the subway over.
[02:13:16.200 --> 02:13:19.480]   So I decided to read the privacy policy for the app on my way
[02:13:19.480 --> 02:13:19.800]   over.
[02:13:19.800 --> 02:13:24.120]   And I realized that essentially it was taking a log of like
[02:13:24.120 --> 02:13:27.480]   everything your phone was doing everywhere you went,
[02:13:27.480 --> 02:13:30.720]   where you moved within once you were using your ticket.
[02:13:30.720 --> 02:13:34.200]   And I assume that if these stadiums are deploying facial
[02:13:34.200 --> 02:13:38.400]   recognition technology, then so they were taking that
[02:13:38.400 --> 02:13:40.720]   information and selling it to advertisers.
[02:13:40.720 --> 02:13:43.840]   So I assume that facial recognition technology will go
[02:13:43.840 --> 02:13:44.720]   there too.
[02:13:44.720 --> 02:13:48.480]   They'll know that, oh, you looked at Coca-Cola possibly
[02:13:48.480 --> 02:13:53.600]   getting like a freezy for three minutes, then decided not to.
[02:13:53.600 --> 02:13:54.840]   Oh my god.
[02:13:54.840 --> 02:13:56.880]   You know what, this is going to be the new normal.
[02:13:56.880 --> 02:14:00.280]   And so the only real advice is download the app, use it to
[02:14:00.280 --> 02:14:03.200]   get in, and then delete it immediately.
[02:14:03.200 --> 02:14:05.680]   Yeah, don't give it any permissions, have your phone
[02:14:05.680 --> 02:14:07.360]   in airplane mode.
[02:14:07.360 --> 02:14:09.400]   Cheese Louise, that's terrible.
[02:14:09.400 --> 02:14:10.640]   Crazy.
[02:14:10.640 --> 02:14:11.600]   That's terrible.
[02:14:11.600 --> 02:14:15.720]   You had to show a barcode that was constantly refreshing,
[02:14:15.720 --> 02:14:17.240]   kind of like an authenticator apps.
[02:14:17.240 --> 02:14:18.880]   There was no way to get around it.
[02:14:18.880 --> 02:14:21.280]   It was ostensibly supposed to stop ticket scalping.
[02:14:21.280 --> 02:14:22.600]   Yeah, that's the excuse.
[02:14:22.600 --> 02:14:23.200]   It's a valid.
[02:14:23.200 --> 02:14:23.560]   Yeah.
[02:14:23.560 --> 02:14:26.480]   But of course, the number of things it was pulling from you
[02:14:26.480 --> 02:14:28.880]   at the same time was the same.
[02:14:28.880 --> 02:14:31.240]   To get into Springsteen, we had to use-- oh no, it was the
[02:14:31.240 --> 02:14:32.600]   Harry Potter show.
[02:14:32.600 --> 02:14:35.160]   We had to use the Ticketmaster app, which didn't work by the
[02:14:35.160 --> 02:14:35.320]   way.
[02:14:35.320 --> 02:14:38.680]   I ended up getting tickets printed anyway.
[02:14:38.680 --> 02:14:40.960]   I bet you anything.
[02:14:40.960 --> 02:14:43.240]   I should look at the permissions on that Ticketmaster app.
[02:14:43.240 --> 02:14:46.200]   I bet you anything is doing exactly the same thing.
[02:14:46.200 --> 02:14:48.840]   Anytime you have to download an app, it probably is.
[02:14:48.840 --> 02:14:49.640]   So annoying.
[02:14:49.640 --> 02:14:51.880]   That's the New York Times just came out with that piece
[02:14:51.880 --> 02:14:54.880]   over this week about all the different apps that are
[02:14:54.880 --> 02:14:57.920]   tracking your location and selling it at any given point.
[02:14:57.920 --> 02:15:00.720]   I've been saying this for some time, though.
[02:15:00.720 --> 02:15:04.280]   Because I've seen a lot of apps, like Product Hunt
[02:15:04.280 --> 02:15:05.440]   released an app.
[02:15:05.440 --> 02:15:07.000]   There was kind of a goofy app called SIP.
[02:15:07.000 --> 02:15:07.640]   You remember this?
[02:15:07.640 --> 02:15:08.800]   It's still out.
[02:15:08.800 --> 02:15:10.200]   It would give you one headline a day.
[02:15:10.200 --> 02:15:12.960]   Because you can't drink the news river, so SIP from the
[02:15:12.960 --> 02:15:13.480]   news straw.
[02:15:13.480 --> 02:15:14.720]   One little--
[02:15:14.720 --> 02:15:17.720]   but I don't think that was the point of that app.
[02:15:17.720 --> 02:15:20.560]   I increasingly think people will do anything to get an app on
[02:15:20.560 --> 02:15:24.920]   your phone, because the data they get is so valuable that
[02:15:24.920 --> 02:15:27.240]   they're just going to do anything.
[02:15:27.240 --> 02:15:29.240]   Makeup apps?
[02:15:29.240 --> 02:15:30.560]   It's very sad.
[02:15:30.560 --> 02:15:31.840]   Let's take--
[02:15:31.840 --> 02:15:33.520]   we've been keeping you guys so long, and I appreciate it.
[02:15:33.520 --> 02:15:35.280]   We do have one big story.
[02:15:35.280 --> 02:15:36.400]   I want to do this.
[02:15:36.400 --> 02:15:38.640]   And then we'll end with that big story.
[02:15:38.640 --> 02:15:42.000]   We set back in the outback, as Signal calls it.
[02:15:42.000 --> 02:15:45.680]   But first a word from the fresh air filling this studio right
[02:15:45.680 --> 02:15:46.400]   now.
[02:15:46.400 --> 02:15:47.920]   Smells good, doesn't it, Marie?
[02:15:47.920 --> 02:15:49.000]   Smells good in here.
[02:15:49.000 --> 02:15:50.160]   Yeah, yeah.
[02:15:50.160 --> 02:15:53.280]   You know I cooked bacon in here just an hour before the show.
[02:15:53.280 --> 02:15:54.720]   And you can't tell, can you?
[02:15:54.720 --> 02:15:56.400]   Because we've got a molecule.
[02:15:56.400 --> 02:16:00.840]   Molecule is the world's first molecular air purifier that
[02:16:00.840 --> 02:16:03.840]   reduces symptoms for allergy and asthma sufferers.
[02:16:03.840 --> 02:16:06.960]   We got a molecule first at home, because Lisa used to wake
[02:16:06.960 --> 02:16:08.920]   up with headaches, because we live in the country, and there's
[02:16:08.920 --> 02:16:10.520]   pollen and dust and stuff.
[02:16:10.520 --> 02:16:12.280]   And all of a sudden, after we moved, she started getting
[02:16:12.280 --> 02:16:13.320]   headaches, so we got the molecule.
[02:16:13.320 --> 02:16:14.560]   Headaches went away.
[02:16:14.560 --> 02:16:17.200]   And I know it was a molecule, because when we stay overnight
[02:16:17.200 --> 02:16:18.720]   somewhere, the headaches come back.
[02:16:18.720 --> 02:16:23.400]   This is an amazing technology.
[02:16:23.400 --> 02:16:26.880]   It replaces 50 years of antiquated air purification
[02:16:26.880 --> 02:16:27.960]   technology.
[02:16:27.960 --> 02:16:31.560]   That help a filter that's in most of these things,
[02:16:31.560 --> 02:16:35.320]   that's been used to clean your air since 1940.
[02:16:35.320 --> 02:16:37.160]   There haven't really been major innovation since.
[02:16:37.160 --> 02:16:40.000]   It's just trapping stuff.
[02:16:40.000 --> 02:16:41.680]   Molecule destroys it.
[02:16:41.680 --> 02:16:44.560]   It's called Pico technology, photo electrochemical
[02:16:44.560 --> 02:16:45.960]   oxidation.
[02:16:45.960 --> 02:16:51.440]   It was created and funded by the EPA, tested by real people
[02:16:51.440 --> 02:16:53.840]   verified by third parties in university labs, like the
[02:16:53.840 --> 02:16:56.920]   University of South Florida's Center for Biological
[02:16:56.920 --> 02:17:00.040]   Defense, the University of Minnesota's Particle
[02:17:00.040 --> 02:17:02.080]   Calibration Lab.
[02:17:02.080 --> 02:17:04.080]   It goes so much farther than a HEPA filter.
[02:17:04.080 --> 02:17:08.280]   First of all, it doesn't just trap allergens.
[02:17:08.280 --> 02:17:09.520]   It eliminates them.
[02:17:09.520 --> 02:17:13.640]   It destroys them, not just allergens, mold bacteria, even
[02:17:13.640 --> 02:17:17.440]   tiny viruses and airborne chemicals, VOCs, formaldehyde
[02:17:17.440 --> 02:17:22.360]   from the carpet, paint fumes, all of that captured, trapped,
[02:17:22.360 --> 02:17:23.520]   destroyed.
[02:17:23.520 --> 02:17:25.800]   Pollutants 1,000 times smaller than those
[02:17:25.800 --> 02:17:27.880]   a HEPA filter can catch.
[02:17:27.880 --> 02:17:28.760]   We liked it so much.
[02:17:28.760 --> 02:17:30.560]   We got one for our son, and we have two of them.
[02:17:30.560 --> 02:17:32.920]   And I tell you, when we had the fires up here and all that
[02:17:32.920 --> 02:17:35.040]   smoke, that molecule saved our lives.
[02:17:35.040 --> 02:17:37.560]   We got one in the studio too.
[02:17:37.560 --> 02:17:38.320]   I have them everywhere.
[02:17:38.320 --> 02:17:39.520]   I love the molecule.
[02:17:39.520 --> 02:17:40.160]   It's beautiful.
[02:17:40.160 --> 02:17:41.760]   It's clean, sleek design.
[02:17:41.760 --> 02:17:44.000]   They set the kind of, say, the apple of air purifiers.
[02:17:44.000 --> 02:17:46.560]   It's got a solid aluminum shell.
[02:17:46.560 --> 02:17:48.960]   You can also get a filter subscription service, so you
[02:17:48.960 --> 02:17:52.360]   just get filters automatically.
[02:17:52.360 --> 02:17:54.480]   We have it connected to our Wi-Fi network so I can control
[02:17:54.480 --> 02:17:56.280]   it for my phone.
[02:17:56.280 --> 02:17:59.160]   And it will automatically order new filters that way, which
[02:17:59.160 --> 02:18:01.040]   is awesome when it needs them.
[02:18:01.040 --> 02:18:02.000]   But you don't have to do that.
[02:18:02.000 --> 02:18:03.880]   It's got a button on top that completely controls it.
[02:18:03.880 --> 02:18:04.960]   It is really awesome.
[02:18:04.960 --> 02:18:07.360]   Just gets everything out of the air.
[02:18:07.360 --> 02:18:08.480]   It smells great.
[02:18:08.480 --> 02:18:11.160]   And even if it's a giant room like our studio,
[02:18:11.160 --> 02:18:12.600]   the molecule works great.
[02:18:12.600 --> 02:18:14.320]   $75 off your first order.
[02:18:14.320 --> 02:18:16.080]   They have a silent mode.
[02:18:16.080 --> 02:18:20.320]   And then if you do make bacon, you turn on the boost mode
[02:18:20.320 --> 02:18:23.000]   and she cleans the air out so fast.
[02:18:23.000 --> 02:18:25.800]   $75 off your first order, but don't delay because these
[02:18:25.800 --> 02:18:27.480]   are selling out fast.
[02:18:27.480 --> 02:18:31.600]   Molecule is M-O-L-E-K-U-L-E.com and use a promo code
[02:18:31.600 --> 02:18:34.640]   TWIT 75 to get $75 off.
[02:18:34.640 --> 02:18:42.080]   Molecule M-O-L-E-K-U-L-E.com, offer code TWIT 75.
[02:18:42.080 --> 02:18:43.720]   Australia this week passed.
[02:18:43.720 --> 02:18:46.640]   It was a last minute bill passed by the Australian Parliament
[02:18:46.640 --> 02:18:53.760]   before they recessed the Assistance and Access Bill,
[02:18:53.760 --> 02:18:55.920]   which is given people fits.
[02:18:55.920 --> 02:18:58.480]   This is from Signal.
[02:18:58.480 --> 02:19:00.680]   But I've seen blog posts from one password now
[02:19:00.680 --> 02:19:02.600]   in many other companies.
[02:19:02.600 --> 02:19:07.240]   Because the bill requires that a company that has encryption
[02:19:07.240 --> 02:19:10.680]   can offer the encrypted data and clear text
[02:19:10.680 --> 02:19:14.760]   upon the request of law enforcement in Australia.
[02:19:14.760 --> 02:19:16.080]   A backdoor.
[02:19:16.080 --> 02:19:19.480]   Signal says, well, we can't do that because we can't read
[02:19:19.480 --> 02:19:20.200]   your message.
[02:19:20.200 --> 02:19:22.160]   We don't have any access to your message.
[02:19:22.160 --> 02:19:27.840]   But the Australian law would require them
[02:19:27.840 --> 02:19:31.600]   to put a backdoor and signal so that they could give them
[02:19:31.600 --> 02:19:34.000]   their severe penalties if they don't.
[02:19:34.000 --> 02:19:35.280]   This data.
[02:19:35.280 --> 02:19:39.400]   Ironically, according to Signal, the Prime Minister of Australia,
[02:19:39.400 --> 02:19:41.160]   Malcolm Turnbull, uses Signal.
[02:19:41.160 --> 02:19:42.920]   Members of government everywhere use Signal,
[02:19:42.920 --> 02:19:44.120]   because it's private.
[02:19:44.120 --> 02:19:50.760]   One of the ways, according to Signal,
[02:19:50.760 --> 02:19:53.560]   that the Assistance and Access Bill is particularly terrible,
[02:19:53.560 --> 02:19:57.120]   lies in its potential to isolate Australians from the services
[02:19:57.120 --> 02:19:59.200]   they depend on and use every day.
[02:19:59.200 --> 02:20:02.920]   Like Signal, maybe like your password vault.
[02:20:02.920 --> 02:20:05.440]   Over time, users may find a growing number of apps
[02:20:05.440 --> 02:20:06.800]   no longer behave as expected.
[02:20:06.800 --> 02:20:10.320]   New apps might never launch in Australia at all.
[02:20:10.320 --> 02:20:14.200]   It doesn't seem like smart politics.
[02:20:14.200 --> 02:20:16.200]   So it isn't yet an effect.
[02:20:16.200 --> 02:20:18.080]   And I think there's perhaps some hope
[02:20:18.080 --> 02:20:19.880]   that the courts in Australia will overturn it
[02:20:19.880 --> 02:20:23.320]   or maybe the parliament will get its mind back.
[02:20:23.320 --> 02:20:28.600]   I use an Australian mail provider, Fast Mail, which I love.
[02:20:28.600 --> 02:20:31.080]   But as soon as I read this, I looked at means
[02:20:31.080 --> 02:20:34.240]   to change my mail away from Fast Mail.
[02:20:34.240 --> 02:20:35.440]   What do you think, Harry?
[02:20:35.440 --> 02:20:38.600]   I mean, the argument is that back doors
[02:20:38.600 --> 02:20:42.080]   are always incredibly dangerous because nobody is smart enough
[02:20:42.080 --> 02:20:45.720]   to build a backdoor that they can be positive,
[02:20:45.720 --> 02:20:48.320]   won't get intruded on by a bad guy.
[02:20:48.320 --> 02:20:49.120]   And that seems to--
[02:20:49.120 --> 02:20:50.120]   That's the problem.
[02:20:50.120 --> 02:20:50.640]   Compelling argument.
[02:20:50.640 --> 02:20:52.200]   I'm not against catching terrorists,
[02:20:52.200 --> 02:20:55.680]   but I don't want this bill to make us all less safe,
[02:20:55.680 --> 02:20:57.200]   and that's what we think it would do.
[02:20:57.200 --> 02:20:58.560]   They don't have a fundamental objection
[02:20:58.560 --> 02:21:02.400]   to certain scenarios in which governments can look at this stuff.
[02:21:02.400 --> 02:21:04.960]   But it's really dangerous from a security standpoint,
[02:21:04.960 --> 02:21:07.640]   according to almost every security expert I've ever
[02:21:07.640 --> 02:21:08.920]   seen chime in on this.
[02:21:08.920 --> 02:21:10.000]   Yeah.
[02:21:10.000 --> 02:21:13.200]   And I'll be-- look, I want to help law enforcement,
[02:21:13.200 --> 02:21:15.960]   but they already have amazing surveillance capabilities.
[02:21:15.960 --> 02:21:18.440]   This last little bit of dark pixel matter,
[02:21:18.440 --> 02:21:23.320]   and there's giant HD panopticon, is not
[02:21:23.320 --> 02:21:26.520]   critical to them doing their job, I don't think.
[02:21:26.520 --> 02:21:28.440]   And it makes us all less safe.
[02:21:28.440 --> 02:21:31.560]   Proton mail, same thing.
[02:21:31.560 --> 02:21:36.280]   What's wrong with the assistance and access law?
[02:21:36.280 --> 02:21:37.720]   The Australian government is really
[02:21:37.720 --> 02:21:41.040]   a leader in encryption back doors.
[02:21:41.040 --> 02:21:44.280]   So this is basically breaks encryption permanently.
[02:21:44.280 --> 02:21:46.680]   It goes farther than even Russia,
[02:21:46.680 --> 02:21:49.600]   which with bans encryption, Turkey bans encryption,
[02:21:49.600 --> 02:21:51.160]   China bans encryption.
[02:21:51.160 --> 02:21:52.680]   This says, oh, you're going to have encryption
[02:21:52.680 --> 02:21:55.080]   as long as we can see what you're still in.
[02:21:55.080 --> 02:21:56.400]   So it's not encryption.
[02:21:56.400 --> 02:21:58.440]   It's worse than banning it.
[02:21:58.440 --> 02:22:01.880]   At least when you ban it, you don't know.
[02:22:01.880 --> 02:22:03.680]   And the problem is it doesn't just impact us.
[02:22:03.680 --> 02:22:07.040]   It impacts if Apple says, well, well, we want to sell phones
[02:22:07.040 --> 02:22:10.560]   in Australia, we guess we got to put it back door in the iPhone.
[02:22:10.560 --> 02:22:13.800]   That impacts every single iPhone user in the world.
[02:22:13.800 --> 02:22:15.600]   And Apple and Google and Microsoft
[02:22:15.600 --> 02:22:16.960]   have all come out against this.
[02:22:16.960 --> 02:22:18.320]   Yeah.
[02:22:18.320 --> 02:22:21.480]   I just wanted to raise the issue.
[02:22:21.480 --> 02:22:23.600]   If you're-- we have many Australian listeners.
[02:22:23.600 --> 02:22:26.160]   I'm sure you're just as upset about this as we are.
[02:22:26.160 --> 02:22:27.840]   Contact your member of parliament.
[02:22:27.840 --> 02:22:35.040]   Yeah, I mean, I think it seems to be just another example
[02:22:35.040 --> 02:22:36.760]   of the people who are making the laws.
[02:22:36.760 --> 02:22:39.480]   Do you not understand the effects
[02:22:39.480 --> 02:22:41.280]   that these sort of rulings are going
[02:22:41.280 --> 02:22:43.680]   to have at the community at large?
[02:22:43.680 --> 02:22:47.360]   Well, it may seem like on one level,
[02:22:47.360 --> 02:22:51.280]   a way to help law enforcement.
[02:22:51.280 --> 02:22:53.920]   I'm not sure of the makeup of Australia's parliament
[02:22:53.920 --> 02:22:56.120]   or if they're experiencing similar issues.
[02:22:56.120 --> 02:22:59.120]   But I know that in the US, this is the common refrain
[02:22:59.120 --> 02:23:01.520]   whenever there's another social media filtering hearing
[02:23:01.520 --> 02:23:03.760]   is that the people who are asking the questions
[02:23:03.760 --> 02:23:06.280]   or have the power to make these decisions
[02:23:06.280 --> 02:23:10.160]   from a governmental standpoint do not understand
[02:23:10.160 --> 02:23:12.720]   the technology that they wield this power over.
[02:23:12.720 --> 02:23:14.000]   Please don't do it.
[02:23:14.000 --> 02:23:15.240]   It makes us all less safe.
[02:23:15.240 --> 02:23:15.600]   That's all.
[02:23:15.600 --> 02:23:16.440]   Just don't do it.
[02:23:16.440 --> 02:23:17.640]   It's bad for Australia.
[02:23:17.640 --> 02:23:19.800]   It's bad for the world.
[02:23:19.800 --> 02:23:23.320]   The FCC, speaking of bad for America,
[02:23:23.320 --> 02:23:28.040]   has decided to change the rules governing text messaging,
[02:23:28.040 --> 02:23:33.280]   making text messaging services not a telecommunications service
[02:23:33.280 --> 02:23:36.880]   but a publishing service, which gives telecoms
[02:23:36.880 --> 02:23:40.560]   the right to censor text messages.
[02:23:40.560 --> 02:23:46.280]   In 2007 Verizon blocked texts from narrow,
[02:23:46.280 --> 02:23:48.920]   an abortion rights group.
[02:23:48.920 --> 02:23:51.120]   That sounds pretty political.
[02:23:51.120 --> 02:23:54.720]   We've seen other situations where this has happened.
[02:23:54.720 --> 02:24:00.240]   The telcos are not notorious for their civil rights focus.
[02:24:00.240 --> 02:24:03.120]   Giving them this power to block text messages
[02:24:03.120 --> 02:24:05.200]   they disagree with is very dangerous.
[02:24:05.200 --> 02:24:08.640]   Again, another one to write your member of Congress.
[02:24:08.640 --> 02:24:10.960]   Theoretically, it gives them the ability
[02:24:10.960 --> 02:24:13.360]   to deal with spamming text.
[02:24:13.360 --> 02:24:13.800]   Bands.
[02:24:13.800 --> 02:24:14.840]   That's the pretext.
[02:24:14.840 --> 02:24:15.960]   Which doesn't sound like a bad thing,
[02:24:15.960 --> 02:24:18.960]   but the EFF says that it's not what
[02:24:18.960 --> 02:24:21.480]   they're going to help on that front.
[02:24:21.480 --> 02:24:23.600]   This is the same thing they did for net neutrality.
[02:24:23.600 --> 02:24:26.120]   They reclassified ISPs as information services
[02:24:26.120 --> 02:24:30.120]   rather than telecommunications company, common carriers.
[02:24:30.120 --> 02:24:32.320]   Just a little update on the FCC.
[02:24:32.320 --> 02:24:34.960]   We like to do that every week.
[02:24:34.960 --> 02:24:36.480]   They're feeding us a lot of news.
[02:24:36.480 --> 02:24:37.520]   I've got to let you guys go.
[02:24:37.520 --> 02:24:38.520]   You've been very patient.
[02:24:38.520 --> 02:24:40.360]   It's been a long show, but it's been so much fun.
[02:24:40.360 --> 02:24:43.200]   I didn't want to stop Harry McCracken.
[02:24:43.200 --> 02:24:44.280]   Love your stuff.
[02:24:44.280 --> 02:24:45.240]   Technologizer.
[02:24:45.240 --> 02:24:45.840]   We call him.
[02:24:45.840 --> 02:24:48.400]   He's a technology editor at Fast Company at Harry McCracken
[02:24:48.400 --> 02:24:49.240]   on Twitter.
[02:24:49.240 --> 02:24:50.160]   Anything you want to plug?
[02:24:50.160 --> 02:24:51.320]   Anything you're--
[02:24:51.320 --> 02:24:54.280]   We're taking off the last week of the year pretty much.
[02:24:54.280 --> 02:24:55.920]   So right now, we're putting together
[02:24:55.920 --> 02:24:59.320]   all these great stories, recapping in 2018
[02:24:59.320 --> 02:25:01.240]   and looking ahead to 2019.
[02:25:01.240 --> 02:25:02.680]   So while I'm off enjoying myself,
[02:25:02.680 --> 02:25:04.040]   we're going to have a ton of good stuff on the site
[02:25:04.040 --> 02:25:05.040]   to check out.
[02:25:05.040 --> 02:25:07.000]   You've been doing some great long form pieces
[02:25:07.000 --> 02:25:08.560]   on Fast Company that I've--
[02:25:08.560 --> 02:25:11.960]   especially your specialty, of course, history.
[02:25:11.960 --> 02:25:12.960]   The history of technology.
[02:25:12.960 --> 02:25:14.360]   It's fun to get to write about that stuff.
[02:25:14.360 --> 02:25:15.360]   Such great stuff.
[02:25:15.360 --> 02:25:16.920]   And we do it because people love it.
[02:25:16.920 --> 02:25:17.440]   Yeah.
[02:25:17.440 --> 02:25:18.240]   Well, I do.
[02:25:18.240 --> 02:25:22.440]   Thank you, fastcompany.com.
[02:25:22.440 --> 02:25:26.120]   Thrill to have Paris Martyno on the show this week.
[02:25:26.120 --> 02:25:27.280]   Your first time, how was it?
[02:25:27.280 --> 02:25:28.440]   Was it OK?
[02:25:28.440 --> 02:25:29.200]   Oh, it was great.
[02:25:29.200 --> 02:25:29.760]   Yeah.
[02:25:29.760 --> 02:25:29.760]   That's fantastic.
[02:25:29.760 --> 02:25:31.120]   We would love to have you back.
[02:25:31.120 --> 02:25:32.880]   Well, somebody's got to give you
[02:25:32.880 --> 02:25:36.760]   a reward for living in the cesspool of the internet.
[02:25:36.760 --> 02:25:38.080]   And I don't mean wired.
[02:25:38.080 --> 02:25:41.080]   I mean, all those other places you cover in Wired.
[02:25:41.080 --> 02:25:44.680]   She writes about the dark side of the net.
[02:25:44.680 --> 02:25:47.960]   And a lot of great stuff at Wired.com.
[02:25:47.960 --> 02:25:49.040]   You do such great work.
[02:25:49.040 --> 02:25:51.120]   I'm so glad we could have you here this week.
[02:25:51.120 --> 02:25:51.640]   Appreciate it.
[02:25:51.640 --> 02:25:53.000]   Thank you.
[02:25:53.000 --> 02:25:55.760]   And of course, my friend Michael Nunez, deputy tech editor
[02:25:55.760 --> 02:25:58.360]   at Mashable, who actually came into work.
[02:25:58.360 --> 02:26:01.120]   Did you ride your bike there?
[02:26:01.120 --> 02:26:01.600]   I did not.
[02:26:01.600 --> 02:26:02.480]   It was raining out.
[02:26:02.480 --> 02:26:03.160]   So--
[02:26:03.160 --> 02:26:04.080]   I'm good.
[02:26:04.080 --> 02:26:05.560]   Yeah, so it was an ideal condition.
[02:26:05.560 --> 02:26:08.600]   And I want to send you home over the bridge in the dark.
[02:26:08.600 --> 02:26:10.600]   No, no, I don't want to ride it.
[02:26:10.600 --> 02:26:12.040]   I ride that in the dark.
[02:26:12.040 --> 02:26:13.280]   Thank you for being here, Michael.
[02:26:13.280 --> 02:26:14.520]   We really appreciate it.
[02:26:14.520 --> 02:26:17.960]   Michael F. Nunez on the Twitter.
[02:26:17.960 --> 02:26:19.400]   What's your Twitter handle, Paris?
[02:26:19.400 --> 02:26:20.880]   Is it Paris Martynau?
[02:26:20.880 --> 02:26:22.080]   Yeah, it's Paris Martynau.
[02:26:22.080 --> 02:26:24.240]   Don't send her nasty stuff.
[02:26:24.240 --> 02:26:25.320]   Send her nice stuff.
[02:26:25.320 --> 02:26:26.800]   Say she was smart.
[02:26:26.800 --> 02:26:28.600]   Say she did a good job.
[02:26:28.600 --> 02:26:30.280]   She sounded great.
[02:26:30.280 --> 02:26:32.000]   Her microphone was fantastic.
[02:26:32.000 --> 02:26:32.520]   Whatever.
[02:26:32.520 --> 02:26:34.560]   Say nice things.
[02:26:34.560 --> 02:26:36.040]   See, they can't bug me anymore.
[02:26:36.040 --> 02:26:39.520]   I must be the only person with a Twitter following bigger
[02:26:39.520 --> 02:26:42.760]   than half a million people who just walked away from it.
[02:26:42.760 --> 02:26:44.120]   Everybody thinks I'm nuts.
[02:26:44.120 --> 02:26:44.600]   Probably, yeah.
[02:26:44.600 --> 02:26:46.520]   That's nuts.
[02:26:46.520 --> 02:26:47.640]   It used to be 600.
[02:26:47.640 --> 02:26:49.480]   By the way, it used to be 600, 11,000.
[02:26:49.480 --> 02:26:50.560]   It's now 511,000.
[02:26:50.560 --> 02:26:52.480]   I don't know why, maybe because I walked away.
[02:26:52.480 --> 02:26:53.760]   I don't know.
[02:26:53.760 --> 02:26:56.680]   Maybe.
[02:26:56.680 --> 02:26:58.080]   Hey, it's really great to have you all.
[02:26:58.080 --> 02:26:59.600]   Thank you for being here.
[02:26:59.600 --> 02:27:01.320]   We are going to be back next week, right?
[02:27:01.320 --> 02:27:02.560]   We have a show or no?
[02:27:02.560 --> 02:27:03.040]   Yes.
[02:27:03.040 --> 02:27:07.320]   OK, next week, actually, that's going to be our big--
[02:27:07.320 --> 02:27:09.840]   that's the show where I'm drinking heavily and smoking
[02:27:09.840 --> 02:27:11.600]   cigars.
[02:27:11.600 --> 02:27:15.760]   We got the Louis XIII's Cognac, the 100-year-old Cognac.
[02:27:15.760 --> 02:27:18.160]   We're going to bust it out.
[02:27:18.160 --> 02:27:19.680]   It'll be our year-end episode.
[02:27:19.680 --> 02:27:21.440]   Michael, you better come because I think we're going
[02:27:21.440 --> 02:27:22.960]   to need evidence.
[02:27:22.960 --> 02:27:24.920]   I was going to say I feel like I really missed out.
[02:27:24.920 --> 02:27:27.080]   I was talking to Michael on the autographer.
[02:27:27.080 --> 02:27:28.440]   But yeah, you missed out.
[02:27:28.440 --> 02:27:30.120]   It's no way we haven't figured out
[02:27:30.120 --> 02:27:32.360]   how to do the Skype Cognac thing yet.
[02:27:32.360 --> 02:27:34.120]   But if you're ever in--
[02:27:34.120 --> 02:27:35.320]   Next year, next year.
[02:27:35.320 --> 02:27:37.280]   I will save some for you, OK?
[02:27:37.280 --> 02:27:39.040]   I will save a sip for you.
[02:27:39.040 --> 02:27:39.400]   Thank you.
[02:27:39.400 --> 02:27:41.640]   Smoking jacket, check.
[02:27:41.640 --> 02:27:42.400]   Get it ready.
[02:27:42.400 --> 02:27:45.840]   That's the December 23 edition of this week and take.
[02:27:45.840 --> 02:27:47.320]   The following week will be our best of.
[02:27:47.320 --> 02:27:48.560]   We always have fun doing that.
[02:27:48.560 --> 02:27:50.720]   Some of the best clips from the year gone by.
[02:27:50.720 --> 02:27:53.400]   And then we'll be back the week after.
[02:27:53.400 --> 02:27:56.240]   So it's going to be fun.
[02:27:56.240 --> 02:27:58.880]   We do Twitch normally every Sunday afternoon, 3 p.m.
[02:27:58.880 --> 02:28:01.040]   Pacific, 6 p.m. Eastern.
[02:28:01.040 --> 02:28:02.360]   That's 2300 UTC.
[02:28:02.360 --> 02:28:04.080]   You can't watch us do it live.
[02:28:04.080 --> 02:28:07.640]   There's a live video and audio stream at twit.tv/live.
[02:28:07.640 --> 02:28:09.760]   And then you get all the bloopers, all the stuff
[02:28:09.760 --> 02:28:12.160]   we have to cut out of the final version.
[02:28:12.160 --> 02:28:13.400]   Leo wearing a leak hat.
[02:28:13.400 --> 02:28:17.360]   Things like you're going to cut that out, right?
[02:28:17.360 --> 02:28:18.720]   No.
[02:28:18.720 --> 02:28:21.000]   You can also get on-demand versions that apparently do
[02:28:21.000 --> 02:28:23.560]   include the leak hat at twit.tv.
[02:28:23.560 --> 02:28:26.520]   In fact, all our shows are available on demand at twit.tv.
[02:28:26.520 --> 02:28:29.880]   Better yet, if you subscribe in your favorite podcast
[02:28:29.880 --> 02:28:33.480]   application, you will get it automatically.
[02:28:33.480 --> 02:28:34.840]   Just look for this week in tech.
[02:28:34.840 --> 02:28:37.120]   You can even ask your smart device.
[02:28:37.120 --> 02:28:40.000]   Hey, Echo, play this week in tech podcast.
[02:28:40.000 --> 02:28:41.760]   And it'll play the most recent version.
[02:28:41.760 --> 02:28:43.920]   You can just enjoy it yourself.
[02:28:43.920 --> 02:28:44.920]   It's great.
[02:28:44.920 --> 02:28:46.480]   Thank you for being here.
[02:28:46.480 --> 02:28:47.400]   We'll see you next time.
[02:28:47.400 --> 02:28:50.000]   Another twit for our special Cognac edition.
[02:28:50.000 --> 02:28:51.000]   Another twit.
[02:28:51.000 --> 02:28:52.360]   It's in the can.
[02:28:52.360 --> 02:28:53.360]   John shaking his head.
[02:28:53.360 --> 02:28:54.360]   No.
[02:28:54.360 --> 02:28:55.800]   [LAUGHTER]
[02:28:55.800 --> 02:28:56.640]   Do the twit.
[02:28:56.640 --> 02:28:57.640]   All right.
[02:28:57.640 --> 02:28:59.480]   Do the twit, baby.
[02:28:59.480 --> 02:29:02.060]   (upbeat music)

