
[00:00:00.000 --> 00:00:03.920]   It's time for Twit this week in Tech. Great show ahead for you. We're going to talk about
[00:00:03.920 --> 00:00:08.240]   DEF CON. It's coming up this weekend. Roberto Baldwin is on his way. He's from Engadget.
[00:00:08.240 --> 00:00:15.200]   Greg Farrow is just back from the IETF meetings. We'll talk about the internet engineering ahead
[00:00:15.200 --> 00:00:21.360]   and what's ahead for TLS. 1.3. Mike Elkins also here. Our digital Nomad. Lots to talk about,
[00:00:21.360 --> 00:00:27.520]   including artificial reality, virtual reality, artificial intelligence, self-driving cars,
[00:00:27.520 --> 00:00:31.520]   and Audi's with Facebook on the screen. It's all coming up next. Hunt to it.
[00:00:31.520 --> 00:00:37.920]   Netcast you love. From people you trust.
[00:00:37.920 --> 00:00:50.640]   This is Twit. Bandwidth for this weekend Tech is provided by CashFly at CACHEFLY.com.
[00:00:55.840 --> 00:01:03.200]   This is Twit this week in Tech, Episode 624, recorded Sunday, July 23, 2017.
[00:01:03.200 --> 00:01:09.200]   There's a real RG-Bargie going on. This weekend Tech is brought to you by Rocket Mortgage
[00:01:09.200 --> 00:01:13.280]   from Quick and Loans. Home plays a big role in your life. That's why Quick and Loans created
[00:01:13.280 --> 00:01:18.080]   Rocket Mortgage. It lets you apply simply and understand the entire mortgage process fully.
[00:01:18.080 --> 00:01:23.520]   So you can be confident you're getting the right mortgage for you. Get started at rocketmortgage.com/twit2
[00:01:24.160 --> 00:01:29.200]   and buy Blue Apron, the number one fresh ingredient and recipe delivery service in the country.
[00:01:29.200 --> 00:01:33.040]   See what's on the menu this weekend? Get three meals free with your first purchase
[00:01:33.040 --> 00:01:36.560]   and free shipping by going to Blue Apron.com/twit.
[00:01:36.560 --> 00:01:43.920]   And buy Audible. Sign up for the Gold Plus 1 plan to get two free books and a 30-day free trial
[00:01:43.920 --> 00:01:51.280]   at audible.com/twit2. And buy Betterment, the largest independent online financial advisor.
[00:01:51.280 --> 00:01:56.640]   For one low, transparent fee, Betterment gives you personalized advice and invests your money.
[00:01:56.640 --> 00:02:02.480]   For a limited time you can get up to one year managed free, learn more at Betterment.com/twit.
[00:02:02.480 --> 00:02:10.080]   It's time for Twit this week in Tech, the show where we cover the week's Tech News.
[00:02:10.080 --> 00:02:14.720]   With the best tech journalists in the biz, we got a great panel for you today.
[00:02:14.720 --> 00:02:19.280]   Because we're coming off of Black Hat and going into Defcon and Roberto Baldwin is here. He's
[00:02:19.280 --> 00:02:23.680]   actually literally going into Defcon. He's got the Queen with. From Engadget, very nicely done.
[00:02:23.680 --> 00:02:28.320]   I haven't set up my burners yet though. You don't want to bring real phones, real equipment.
[00:02:28.320 --> 00:02:34.160]   Well, not to the floor. I bring it to Vegas and then at night I take it out of its Faraday
[00:02:34.160 --> 00:02:39.520]   bag and then I... That's smart. That's Mike Elgin to your left. Nice to see you. Mike back
[00:02:39.520 --> 00:02:46.320]   from his digital nomadicness. Yes, we're in Italy, Spain, Morocco and France. It was wonderful.
[00:02:46.320 --> 00:02:50.080]   Three months or so and changed. And you're back visiting the family, but are you going to go back
[00:02:50.080 --> 00:02:53.600]   on the road? Yes. You're going to be going to Barcelona next month. That's right. We're going to
[00:02:53.600 --> 00:02:57.280]   go to Barcelona and then probably going to go to Italy and France again. This time we're going to
[00:02:57.280 --> 00:03:04.560]   have my son and his wife with us and his granddaughter. Princess Squishy Face. Princess Squishy Face.
[00:03:04.560 --> 00:03:10.400]   In Morocco. Yeah. We're not going to bring her to Morocco. We're going to introduce her to a nicer
[00:03:10.400 --> 00:03:16.720]   place which is Provence, which she will love because she loves French things. Yeah. Like all those
[00:03:16.720 --> 00:03:20.800]   French things. She's eight months old. But like all the French things my wife brought her, a little
[00:03:20.800 --> 00:03:27.760]   tiny dresses and stuff like that. Becoming Nomad. It's gastronomad.net. All right. Yeah.
[00:03:27.760 --> 00:03:32.960]   We got to get... Can we put that on the lower third? Because I want to give a plug to Mike and
[00:03:32.960 --> 00:03:39.360]   Emery's business because you can go. There's, I think last time I saw room for one more couple.
[00:03:39.360 --> 00:03:44.480]   One more couple. In Barcelona. That's right. And we have assembled the most incredible
[00:03:44.480 --> 00:03:50.080]   team of experts. The best bread baker in Spain is going to give us a personal tour and class.
[00:03:50.080 --> 00:03:56.880]   We're going to learn how to make sangrian and paella. We're going to be so close. We're going to be
[00:03:56.880 --> 00:04:01.120]   in Monte Carlo at that time. We went to Monte Carlo for the first time. Like three or four weeks ago.
[00:04:01.120 --> 00:04:05.120]   I know. And we were looking at your pictures with great interest. It's for real. Yeah. It's really cool.
[00:04:05.120 --> 00:04:11.120]   It's the Vegas of the Riviera. It's almost like inception. It's so mountainous and
[00:04:11.120 --> 00:04:16.320]   steep and vertical. It's amazing. Can't wait. Hey, look who's here from the UK, Greg Farro and
[00:04:16.320 --> 00:04:20.240]   the Packet Pushers network. Good day. Hi, how's it going? Nice to see you, Greg.
[00:04:20.240 --> 00:04:25.920]   Yep. Sorry. I can't be there in person. But I just flew back from Prague on the weekend. So
[00:04:25.920 --> 00:04:31.760]   it was available for this. Thrills. You mean you came back just for the show? Yeah, pretty much.
[00:04:31.760 --> 00:04:37.520]   I was in the IETF was happening in Prague. The people who actually make the standards that
[00:04:37.520 --> 00:04:43.200]   drive the internet. And I had a hanging around there with the people who, these are the nerdy
[00:04:43.200 --> 00:04:49.280]   nerds, if you can imagine that. Really? What is that like the IETF meetings? That must be it.
[00:04:49.280 --> 00:04:55.760]   Well, I mean, for normal people, they would walk in there and go like, there is no fashion crime
[00:04:55.760 --> 00:05:00.640]   not being committed. That's just starting with that. Oh, yeah. Sandals with socks. That's nothing.
[00:05:00.640 --> 00:05:06.480]   This this goes way beyond that. Yeah. Oh, this goes way beyond that. You know, like a 20 year old
[00:05:06.480 --> 00:05:12.480]   suit coat with a 35 year old t-shirt underneath. It's just got some arcane, you know, like there
[00:05:12.480 --> 00:05:18.640]   was one guy who was wearing a bright fluorescent orange trousers, big baggy ones, and a bright,
[00:05:18.640 --> 00:05:23.040]   yellow shirt. Nice. And he was he was generously proportioned in every way.
[00:05:24.800 --> 00:05:30.240]   It was kind of like a flag walking past it. How do you get on getting this group? I mean,
[00:05:30.240 --> 00:05:37.440]   who are these people? Most of them started out like the older people have been there sort of 30
[00:05:37.440 --> 00:05:41.840]   or 40 years building the standards since the 1960s and 1970s. Are these citizens? Are they
[00:05:41.840 --> 00:05:47.840]   university types? They've all gone through various incarnations. Anybody can participate.
[00:05:47.840 --> 00:05:53.120]   The IETF is a volunteer effort. So anybody who wants to can jump in and start participating in
[00:05:53.120 --> 00:05:57.600]   the standards process, you join the mailing lists, you build up your karma points by
[00:05:57.600 --> 00:06:03.840]   contributing stuff to people and telling them what telling them about stuff. And then as you
[00:06:03.840 --> 00:06:07.760]   build up more points, you can take you can participate in more and more stuff. So right now,
[00:06:07.760 --> 00:06:15.040]   a lot of the people who participate are employees of Huawei, Google, somewhere there from Comcast
[00:06:15.040 --> 00:06:18.960]   and so forth. Cisco has a number of people. That's depressing with these big companies,
[00:06:18.960 --> 00:06:24.480]   but that makes sense. Well, these people are often very unaligned. They may be working for the
[00:06:24.480 --> 00:06:29.200]   companies, but their best intentions are by and large in the interests of the world,
[00:06:29.200 --> 00:06:34.080]   or the whole, or the community as the whole. And these people really work hard to do things like
[00:06:34.080 --> 00:06:37.920]   encryption in the internet. We'll talk more about that later on, but they're the people that
[00:06:37.920 --> 00:06:43.600]   are responsible for the new TLS 1.3 standard and trying to prevent companies like Google from
[00:06:43.600 --> 00:06:49.920]   sniffing your traffic as it flies in as it's in motion. And they're making standards to be able to
[00:06:49.920 --> 00:06:55.280]   improve data centers. They're making standards to improve when connectivity or 5G or metropolitan
[00:06:55.280 --> 00:07:01.520]   man's and just so many different areas. It's not just networking now. They're also doing
[00:07:01.520 --> 00:07:07.200]   browsers and stuff like that. So it's been a real effort to try and bring this around.
[00:07:07.200 --> 00:07:12.400]   It's just fascinating to see it all because it's all volunteers getting in a room and
[00:07:12.400 --> 00:07:17.920]   basically hashing out the things that they can't sort out on the mailing list. So if you have
[00:07:17.920 --> 00:07:20.960]   something on a mailing list and you can't agree, then they'll take it to the conference and then
[00:07:20.960 --> 00:07:25.120]   you hash it out in person. So there's all these people just huddled around arguing over this
[00:07:25.120 --> 00:07:31.120]   arcane computer science. And it's just the most fun thing to be around. Just you can smell the
[00:07:31.120 --> 00:07:35.680]   nerd smoke coming off as you move around. Well, and you because you have a fairly deep
[00:07:35.680 --> 00:07:39.920]   understanding of this stuff, it's probably meaningful to you. A lot of people would just be nonsense
[00:07:39.920 --> 00:07:45.360]   what they're saying. Yeah, well, I'm not I'm not when you're as smart as any of them. So please don't
[00:07:45.360 --> 00:07:51.360]   make that comparison. You're smarter than me. So that puts me even I'll be like a lizard there.
[00:07:51.360 --> 00:07:56.800]   I feel like I wonder if these of course, there's no battles harder fought than battles over.
[00:07:56.800 --> 00:08:02.240]   The less important something is the worse the battle is. How hard fought are these are these
[00:08:02.240 --> 00:08:07.600]   battles? Are they are they vicious? Or do these guys and I presume as mostly guys have a sense
[00:08:08.480 --> 00:08:12.400]   of the importance of what they're doing and really how much this means to the world that
[00:08:12.400 --> 00:08:19.920]   may be a more important meeting than the G20 summit. I think it's very collegiate. So the first
[00:08:19.920 --> 00:08:26.000]   thing to do is it's very collegiate. Yeah. Yes, very community battles may be hard for it, but
[00:08:26.000 --> 00:08:32.880]   that's unusual. And often battles aren't fought on company lines. They're fought on technology
[00:08:32.880 --> 00:08:38.080]   lines. Like I believe my technology is the best way forward. And people go around and try to
[00:08:38.080 --> 00:08:45.200]   convince other people that my vision of technology is the right way. And the people who have been
[00:08:45.200 --> 00:08:52.000]   doing it for the longest who have the most karma will tend to be listened to more. So as a rule,
[00:08:52.000 --> 00:08:56.400]   there's sort of a stabilization between people just entering into the process. And those people
[00:08:56.400 --> 00:09:01.600]   who've been participating and contributing for 20, 30, 40 years, I interviewed conducted a
[00:09:01.600 --> 00:09:07.120]   YouTube interview with Sue has Susan has. She's the chair of the routing working group,
[00:09:07.120 --> 00:09:10.960]   which is the one that does all of the routing for the internet backbone, the BGP and autonomous
[00:09:10.960 --> 00:09:16.960]   systems and that sort of stuff, among other things. And she was telling me how back in the 1970s and
[00:09:16.960 --> 00:09:21.760]   60s, how what it was like when we started doing these technologies for the first time and telling
[00:09:21.760 --> 00:09:27.520]   stories about how we built these routers or why we did this, because she was there writing that
[00:09:27.520 --> 00:09:32.800]   code just fascinating to talk to. Are there are there? How many? How many women are there?
[00:09:32.800 --> 00:09:39.440]   It's pretty well attended. There might be five, maybe 10%. Oh, that's good. I mean,
[00:09:39.440 --> 00:09:44.480]   it should be 50, obviously, but that's better than nothing. It's very difficult to go to these
[00:09:44.480 --> 00:09:50.880]   conferences. The ITF meets in person three times a year, usually meets somewhere in the Americas,
[00:09:50.880 --> 00:09:55.360]   and then in Europe, and then in Asia somewhere, although they've stopped going to the US because
[00:09:55.360 --> 00:10:03.120]   of the change in the visa policy, it meant that a lot of people couldn't attend these meetings,
[00:10:03.120 --> 00:10:08.480]   so they've shifted all of the meetings from the US to Canada, which is disappointing. I feel sorry
[00:10:08.480 --> 00:10:13.680]   for everybody in the US. The idea behind the ITF is to make it as participatory as possible,
[00:10:13.680 --> 00:10:19.520]   and as much as anybody can get in, and to sort of move it outside the US cuts you off from a market.
[00:10:19.520 --> 00:10:24.400]   So, and I know they were very sad about that, but it's not fair to people coming in from Africa or
[00:10:24.400 --> 00:10:31.440]   the Middle East to not be able to attend. Yeah, no kidding. Well, we'll get more about some of the
[00:10:31.440 --> 00:10:37.920]   goings on there, but I'm glad you're here, Greg. And how about you at DEF CON? Are you looking
[00:10:37.920 --> 00:10:44.720]   forward, Roberto, to anything particular or? I like DEF CON a lot. It's fun. It's really fun.
[00:10:44.720 --> 00:10:49.840]   It sort of puts you, you know, it keeps you on your toes the entire time because you can't go in
[00:10:49.840 --> 00:10:56.160]   with your sort of regular digital life. Right. What happens is you go in with regular digital
[00:10:56.160 --> 00:11:00.160]   life and then within 20 minutes you're on the wall of sheep. Yeah, you're on the wall of sheep,
[00:11:00.160 --> 00:11:06.960]   or just someone hacks you, and it's a great way to sort of show you the potential of the future
[00:11:06.960 --> 00:11:12.640]   of like how much we have to protect our privacy, protect our security. I mean,
[00:11:12.640 --> 00:11:18.240]   everything that's happening there, there's potential for it to be what the real world would be like if
[00:11:19.440 --> 00:11:24.640]   companies, if people don't get their, you know, get it together. Right. And so,
[00:11:24.640 --> 00:11:29.680]   which they won't, which they won't. Well, individuals getting it together is that's,
[00:11:29.680 --> 00:11:34.960]   you can only say, hey, use different passwords every time, use two factor authentication every time.
[00:11:34.960 --> 00:11:40.000]   And, you know, you can just keep telling people that doesn't mean they're going to do it.
[00:11:40.000 --> 00:11:42.640]   Right. Don't worry, the government will take care of our privacy. Yeah. And so when,
[00:11:42.640 --> 00:11:46.320]   and then that's the other thing. So it's kind of behooves the, you know, the companies that
[00:11:46.320 --> 00:11:50.560]   these secur, these researchers and hackers are talking about, and you know, these systems that
[00:11:50.560 --> 00:11:55.520]   they're talking about and saying, hey, you know, we found this issue, fix it. Right. And that's,
[00:11:55.520 --> 00:12:00.240]   that's really what, what kind of, we found something wrong, fix it. Right. We're going to
[00:12:00.240 --> 00:12:05.120]   shame you into fixing it. Now, before DEF CON is black hat, and that was this weekend,
[00:12:05.120 --> 00:12:11.280]   what is the difference between the two black hat is more of a sort of corporate event. You have,
[00:12:11.280 --> 00:12:15.520]   instead of getting individual talks, you get usually, you usually end up getting talks from
[00:12:15.520 --> 00:12:22.080]   security companies. And they kind of talk about what they're thinking is like networking, you
[00:12:22.080 --> 00:12:27.040]   know, everyone, where everyone dresses like this with a tie at black hat, and then they go back
[00:12:27.040 --> 00:12:31.840]   to their room and they put their t-shirt on and they like, let their, you know, they comb out like
[00:12:31.840 --> 00:12:37.600]   the stuff in their hair that makes it one color. They let themselves be themselves at DEF CON.
[00:12:37.600 --> 00:12:42.320]   DEF CON is more people call hacker, hacker summer camp.
[00:12:42.320 --> 00:12:47.520]   Where will we learn more about exploits? Do hackers bring their exploits to DEF CON to
[00:12:47.520 --> 00:12:53.040]   talk about or do they do it more black hat? Because I know we always praise ourselves this time of
[00:12:53.040 --> 00:13:01.680]   year. Yeah, I mean, I think we're going to learn more from DEF CON because the people who are
[00:13:01.680 --> 00:13:05.280]   presenting are presenting as individuals. Right. So there isn't the concern about, oh, well, the
[00:13:05.280 --> 00:13:08.880]   company behind me or this behind me or have to worry about, you know, my boss thinks.
[00:13:08.880 --> 00:13:16.400]   And so you have that, I think that mentality actually breeds, you know, a better, a better
[00:13:16.400 --> 00:13:20.960]   presentation. Yeah. And it's a bit more fun, it's a bit more loose.
[00:13:20.960 --> 00:13:28.720]   We're also seeing a transition, Leo, where we used to only see security vulnerabilities
[00:13:28.720 --> 00:13:33.760]   announced at the conferences, the hacker conferences, so that you could build up some,
[00:13:33.760 --> 00:13:38.880]   you know, hacker cred, you know, get some ego stroking going on. But increasingly, we're seeing
[00:13:38.880 --> 00:13:43.200]   bug bounties being paid. There's so much money in this now, you don't hold on to it.
[00:13:43.200 --> 00:13:47.040]   No, you want to, yeah, you want to put that out. And so a lot of the, some of the talks
[00:13:47.040 --> 00:13:52.480]   of black hat and a DEF CON are about things that have been disheartened already. Yeah. But
[00:13:52.480 --> 00:13:56.720]   they started to sort of talk about the journey and they talk about how they found it out. And I
[00:13:56.720 --> 00:14:03.040]   think the journey is really the more interesting part, like, how did you find out about this? How
[00:14:03.040 --> 00:14:07.280]   did you, you know, reverse engineer this malware? How did you figure out that, you know,
[00:14:07.280 --> 00:14:13.360]   is DEF CON going to suffer from the same kind of visa issues that the IETF?
[00:14:13.360 --> 00:14:18.240]   There's suffered from, I mean, I would imagine there are a few hackers who say, I'm not coming to
[00:14:18.240 --> 00:14:23.280]   this. Yeah. Yeah. You know, I, I, I've talked, I've asked them about that and they don't have any
[00:14:23.280 --> 00:14:27.920]   sort of official, but they have, you know, there are people who have said on Twitter that we're
[00:14:27.920 --> 00:14:31.920]   not going to go to DEF CON this year because we're concerned about, you know, visas, we're
[00:14:31.920 --> 00:14:36.160]   concerned about the border patrol, like looking at our phones or, you know, concerned about,
[00:14:36.160 --> 00:14:40.560]   you know, there's a lot of things that said, that seems to have been a little bit overblown. I
[00:14:40.560 --> 00:14:44.800]   think we were very worried about that. But Mike, you've traveled into an out of the country. I have
[00:14:44.800 --> 00:14:50.560]   also in the last few months, it was a snap for me. Yes. It's, it's a largely theoretical. It's
[00:14:50.560 --> 00:14:54.480]   mostly targeted. You could, you could happen to, you know, it's happened to Pete. Right. Yeah,
[00:14:54.480 --> 00:14:58.880]   it's happened to journalists. It's probably worse if you're Muslim, you're a brown, you're somebody
[00:14:58.880 --> 00:15:04.320]   who doesn't look like you and me, Mike. But it does, it does, and then the border patrol says it's a
[00:15:04.320 --> 00:15:09.040]   very, very small percentage of people that we do this to. So maybe, it may be just the risk of that
[00:15:09.040 --> 00:15:13.520]   as opposed to the certainty. That's a, that's a calm and reasoned approach. The other approach
[00:15:13.520 --> 00:15:19.280]   is to consider the fact that should there be a horrible incident of some kind, believe me,
[00:15:19.280 --> 00:15:24.160]   everything again, locked in there, you wrote a good article on that. Just we're one bomb away
[00:15:24.160 --> 00:15:28.960]   from a, from a full on laptop ban where the laptop will be banned from the cargo hold,
[00:15:28.960 --> 00:15:33.040]   not the, and the cabin. We're going right now. The opposite direction. This week,
[00:15:33.040 --> 00:15:38.160]   the laptop ban was completely lifted. The last few airlines that were still, these are middle,
[00:15:38.160 --> 00:15:44.080]   it was always weird to me that there were middle Eastern airlines coming from the middle east,
[00:15:44.080 --> 00:15:50.160]   not American owned airlines coming from the middle east. As if, if you're a bad guy,
[00:15:50.160 --> 00:15:53.920]   why would you say, well, no, I got to fly on them or it's, I can't fly. I can't fly.
[00:15:53.920 --> 00:15:56.960]   I only fly. Have you seen the video for United? I'm not flying.
[00:15:56.960 --> 00:16:03.040]   Those guys drag people off the planes. It's too dangerous. No, but seriously, a band like that
[00:16:03.040 --> 00:16:08.880]   can only be effective if it's universal. Otherwise, if you've got a laptop bomb,
[00:16:08.880 --> 00:16:13.920]   you just don't fly those five airlines. Or just drive or take a train or a boat to another
[00:16:13.920 --> 00:16:18.720]   country and get on the plane there. It's probably based on one of two things. One of them is that
[00:16:18.720 --> 00:16:24.080]   they handed Morocco or whatever a list of things that they want them to do at the airport.
[00:16:24.080 --> 00:16:26.720]   They're not doing that. They say, well, we're going to make you do it.
[00:16:26.720 --> 00:16:30.320]   That I understand. The other one is from Mongeord and Cairo, Istanbul,
[00:16:30.320 --> 00:16:37.040]   Jitta and Riyadh and Saudi Arabia, Kuwait City, Casablanca, Morocco, Doha, Dubai and Abu Dhabi.
[00:16:37.040 --> 00:16:41.440]   Those certain airports. Those airports. Yeah, you can presume maybe there's security issues in
[00:16:41.440 --> 00:16:45.600]   those airports. But the other thing is that it's very likely is that the leader in this space is
[00:16:45.600 --> 00:16:52.160]   Israel. Israel has better intel. They have stronger airport security. They've really
[00:16:52.160 --> 00:16:56.080]   excelled in this area. And they're probably pushing the US to do certain things in the US
[00:16:56.080 --> 00:16:59.920]   going, OK, that makes a lot of sense. We actually don't know what's happening behind the scenes.
[00:16:59.920 --> 00:17:05.680]   I think we think because of Trump's leaking to the Russians in the Oval Office that it was,
[00:17:05.680 --> 00:17:10.320]   in fact, Israeli intelligence that drove the laptop. Yeah. And a huge amount of the
[00:17:10.800 --> 00:17:17.520]   airline-based anti-terror sort of spy info comes from Israel because they're all over that.
[00:17:17.520 --> 00:17:22.880]   I mean, they do a great job. Right. Yeah. Right. Yeah. You can't come in and out of my son traveled
[00:17:22.880 --> 00:17:30.560]   from Cairo to Tel Aviv. That's got to be. Yeah. And he was in an interrogation room for six hours.
[00:17:30.560 --> 00:17:34.160]   Really? Yeah. No. What are you doing in Cairo? It's interesting, though. They rely, I'm told,
[00:17:34.160 --> 00:17:38.720]   the Israeli security forces rely on that. They rely on personal interviews more than anything
[00:17:38.720 --> 00:17:44.800]   else. That's, they feel like if they talk to you, they'll catch you. Yes. Right. And it's a-
[00:17:44.800 --> 00:17:50.160]   It's very expensive, manpower-wise. Yes, it is. Yes, it is. But they're apparently very good at it.
[00:17:50.160 --> 00:17:55.600]   So, you know, and you come into a major US airport, you come into LAX or something like that, and it's
[00:17:55.600 --> 00:18:01.120]   just this cattle being herded through this mass process. You know, Bruce Schnier has always said
[00:18:01.120 --> 00:18:07.760]   it's security theater, not true security. Yeah. And it's obvious because if, for instance, you're
[00:18:07.760 --> 00:18:14.720]   flying first class, which incidentally the 9/11 terrorists flew first class, you are often-
[00:18:14.720 --> 00:18:18.720]   Might as well catch in all the miles. You're often exempted from those searches.
[00:18:18.720 --> 00:18:24.080]   Every time I fly a business or first, I get pre-TSA pre-and I don't even have to take my
[00:18:24.080 --> 00:18:30.320]   shoes off, let alone take my laptop out. But that's- So it's theater. Yes. Yeah. And what they don't
[00:18:30.320 --> 00:18:36.400]   want to do is disadvantage wealthy people who are paying them the most. They do not want them to
[00:18:36.400 --> 00:18:40.320]   start flying. But that's where the terrorists fly. It's where the first class. First class.
[00:18:40.320 --> 00:18:43.680]   Because they're close to the front of the plane. It was easier for them to hijack the plane.
[00:18:43.680 --> 00:18:48.480]   There was a wonderful cartoon that I saw a couple of years ago where airport security says,
[00:18:48.480 --> 00:18:52.240]   "Oh, this bottle of water may be a bomb. I'm going to put it in this trash can with all
[00:18:52.240 --> 00:18:55.360]   these other potential- The other ball of the people are. Well, the people are. Well, the people are.
[00:18:55.360 --> 00:19:00.400]   The most with the largest concentration of people are outside of a plane is at the TSA.
[00:19:00.400 --> 00:19:04.880]   I'm all for security. I want us to be safe. I don't want anything bad to happen. Anybody in
[00:19:04.880 --> 00:19:11.920]   flight, certainly not me or my family. But I also don't think that security theater is the right way
[00:19:11.920 --> 00:19:17.040]   to go. And we've come a long, long way. I mean, Leo, you and I are old enough to remember when
[00:19:17.040 --> 00:19:21.680]   you could just get on a plane in a pistol and go to Cuba. It happened all the time. You have
[00:19:21.680 --> 00:19:25.120]   hijacked planes all the time. Absolutely. Those are the days. Those are the days.
[00:19:25.120 --> 00:19:31.440]   It's so simple then. There's two things I think I could add to this. One is when I first started
[00:19:31.440 --> 00:19:36.400]   traveling to Las Vegas four or five years ago, I always been traveling Virgin Atlantic for quite
[00:19:36.400 --> 00:19:41.440]   some time. There used to be three flights a day in and out of Vegas, most days of the week.
[00:19:41.440 --> 00:19:50.720]   Now there's one plane a day and that plane is not full. So the impact of the visa changes and the
[00:19:50.720 --> 00:19:55.760]   increased border security, from my point of view, is definitely had an impact. I can only get one
[00:19:55.760 --> 00:20:01.440]   plane a day, but that plane is empty, which improves my life a little bit. The second thing is, is that
[00:20:01.440 --> 00:20:07.200]   keep in mind that the computer systems that customs are using that and they're all interlinked.
[00:20:07.200 --> 00:20:11.360]   What they really do is a deep data search on you before you get on the plane based on when you
[00:20:11.360 --> 00:20:16.720]   bought the ticket. How did you pay for it? What address did you put on that? Is it your home address?
[00:20:17.600 --> 00:20:24.560]   Your passport, of course, is the key of the data. So don't underestimate the ability of the computers
[00:20:24.560 --> 00:20:29.600]   that customs is using to identify potential people because it's still certainly true that when I go
[00:20:29.600 --> 00:20:34.400]   to board the plane, there are people being picked out of the queue by name and then searched.
[00:20:34.400 --> 00:20:39.440]   There's definitely a different approach. That actually happened to me coming back to the US.
[00:20:39.440 --> 00:20:45.440]   I was randomly selected. I had to walk literally like a half a mile to the other side of the airport
[00:20:45.440 --> 00:20:51.360]   and wait. It was just an extensive frisking and bag search and all that kind of stuff.
[00:20:51.360 --> 00:20:55.440]   They did that to me on the way back from Spain and I only had to walk like from here to there.
[00:20:55.440 --> 00:20:59.840]   It was like, as everyone's getting on the plane, I'm the last person on the plane.
[00:20:59.840 --> 00:21:03.920]   They're like, "Oh, you go over there." Has anybody on this panel or any of our visitors from overseas
[00:21:03.920 --> 00:21:07.840]   have any of them had the thing we're all feared of that they're going to say, "Give me your phone,
[00:21:07.840 --> 00:21:13.280]   unlock it, and I'll be back in half an hour. Has that happened to anybody?" I have a phone that
[00:21:13.280 --> 00:21:17.920]   looks exactly like this phone that is my international travel. You have a dummy phone. I have a dummy phone.
[00:21:17.920 --> 00:21:24.080]   All it has is Netflix and Amazon Prime so I can watch movies. Like I said, I am all for security.
[00:21:24.080 --> 00:21:30.240]   In fact, I would prefer to see us do Israeli-style security if it was effective than kind of an
[00:21:30.240 --> 00:21:36.960]   ineffectual theater. That's a waste of time for everybody if it doesn't make you safer.
[00:21:36.960 --> 00:21:44.000]   I'm super concerned about it because we live in a gadget-centric world. We and everybody
[00:21:44.000 --> 00:21:49.120]   listening to this. If they go after laptops and tablets and you can't travel with those,
[00:21:49.120 --> 00:21:55.600]   that's going to just... I can't even imagine. They're going to have to have processes
[00:21:55.600 --> 00:22:04.160]   to detect whatever it is they're afraid of into the future because we have to be able to work on
[00:22:04.160 --> 00:22:09.120]   airplanes. This is what you worried about is the minute something bad happens. A laptop
[00:22:09.120 --> 00:22:15.200]   actually does take a plane down. All of these bands will immediately go into effect and nobody's
[00:22:15.200 --> 00:22:22.320]   going to get any work done in the sky. I'm not sure I'm against that idea. I'll bring a book.
[00:22:22.320 --> 00:22:27.280]   You pull your laptop out and they scan it. Well, remember when you used to...
[00:22:27.280 --> 00:22:32.640]   When you said turn it on... Oh, you said turn it on. Remember, if it's a bomb, you're not going to be
[00:22:32.640 --> 00:22:38.720]   able to get a screen. Well, see, this is the problem. So with miniaturization...
[00:22:38.720 --> 00:22:43.680]   No, I mean, just mean if you put a bomb in this MacBook Air, it's unlikely that it would also boot.
[00:22:43.680 --> 00:22:47.920]   Oh, you could have this MacBook Air might just be powered by an iPhone inside.
[00:22:47.920 --> 00:22:52.160]   The rest of it is C4. Obviously, I shouldn't be arguing this one.
[00:22:52.160 --> 00:22:54.720]   You should see some of the Defcon badges I'm looking at right now.
[00:22:54.720 --> 00:22:57.120]   Oh, yeah. What is the badge? Did you get your badge yet?
[00:22:57.120 --> 00:23:00.240]   You don't get your badge till you get to Defcon. The official badge, you don't get to get to
[00:23:00.240 --> 00:23:03.120]   Defcon. There's always a puzzle, right? There's always something going on.
[00:23:03.120 --> 00:23:06.560]   Yeah, there's always something going on. This year, it sounds like they're not going to be, but the badge
[00:23:06.560 --> 00:23:12.000]   community is insane. So I'm actually working on an article talking about the badge community and
[00:23:12.000 --> 00:23:18.240]   talking to badge makers and sending me stuff. I've gotten two badges already and one of...
[00:23:18.240 --> 00:23:24.400]   It's got a screen on it and it looks like Bender. It's just amazing.
[00:23:24.400 --> 00:23:31.200]   Like, what... And most of the badge is because of the design. I mean, the badge could be this big,
[00:23:31.200 --> 00:23:36.800]   with just a screen on it, but it's like these very nice designs that you wear and I'm a nerd,
[00:23:36.800 --> 00:23:41.280]   which is definitely... They're very cool. We talked to the guy who's on that committee for
[00:23:41.280 --> 00:23:48.080]   badges. Oh, I can't remember. Awesome. A triangulation, but it was a long time hacker, one of the greats.
[00:23:49.280 --> 00:23:55.920]   All right. Anyway, that's all kind of futures. I could talk about the past. Did you know that
[00:23:55.920 --> 00:24:08.000]   Amazon was started 22 years ago this week? Well, I do now. Yes, you do. 1995. And what a long way
[00:24:08.000 --> 00:24:14.320]   we've come. And 22 years from now, they'll be the only company. I was playing with the Amazon
[00:24:14.320 --> 00:24:18.160]   shopping app. We've been showing this all week because I didn't know this, but they've hidden
[00:24:18.720 --> 00:24:25.120]   all these additional tools in the Amazon shopping app. Did you know that? If you go into Amazon and
[00:24:25.120 --> 00:24:30.960]   you go into the hamburger menu on the left, there's something new called programs and features. Oh,
[00:24:30.960 --> 00:24:37.040]   maybe it's... Maybe they've taken it away from me. Maybe it's on Android only. But... Oh, no,
[00:24:37.040 --> 00:24:41.280]   there it is. Programs and features. And there's a treasure truck. Do you know about the treasure
[00:24:41.280 --> 00:24:47.680]   truck? Yeah. Comes to your town. It has only one item on it. You sign up for the treasure truck.
[00:24:47.680 --> 00:24:52.640]   They can use a smaller truck. It was just for my... Do you have boxes I get sometimes when I
[00:24:52.640 --> 00:24:57.280]   order a dongle and I can do a box like this? They do that with marketing. That's the treasure box.
[00:24:57.280 --> 00:25:03.280]   Amazon Spark, which is a strange social network ever, is basically Instagram for shopping. Speaking
[00:25:03.280 --> 00:25:08.880]   of talking about the past, if you recall, when Google+ first launched in 2011 or whatever it was,
[00:25:08.880 --> 00:25:14.000]   they had a news service called Sparks, I think. Oh, really? Yeah. I don't remember. And so for them
[00:25:14.000 --> 00:25:18.080]   to take that name, since obviously nobody's using it,
[00:25:18.080 --> 00:25:24.480]   well, in the way they bury it and the stuff on it, I wonder if anybody even... Yeah, why would you
[00:25:24.480 --> 00:25:29.760]   spend any time there? I did the outfit. You have the now the new outfit compare. We should do this.
[00:25:29.760 --> 00:25:32.960]   We should do this. Unfortunately, you need to have two outfits. Do you bring two outfits?
[00:25:32.960 --> 00:25:40.880]   I try to do this. You can't do it with different people. But I did it on Wednesday
[00:25:40.880 --> 00:25:44.960]   with a nice outfit and a creepy outfit and the creepy outfit one. So what it does is it rates
[00:25:44.960 --> 00:25:48.720]   your... You take a picture of yourself with two different outfits and it says, "You should wear that
[00:25:48.720 --> 00:25:53.200]   today." Yeah. That's built in. This is going to be part of the new Amazon look when it comes out.
[00:25:53.200 --> 00:25:58.560]   That's the camera echo. The machine learning needs more data. Any other more data. Maybe that's
[00:25:58.560 --> 00:26:03.440]   what's going on. This has nothing to do with my look. How perfect would that have been to go to
[00:26:03.440 --> 00:26:08.640]   Greg's conference? There's in his meeting. See, that's... Orange pants at a yellow shirt.
[00:26:09.200 --> 00:26:13.440]   How about a yellow shirt and orange pants? Not just orange, but for wrestling.
[00:26:13.440 --> 00:26:22.080]   It wasn't enough to just wear open-toed sandals. It had to be fantastic. No fashion crime,
[00:26:22.080 --> 00:26:28.080]   uncommitted. I had a really interesting revelation the other day. I went to Santana Row in San Jose
[00:26:28.080 --> 00:26:33.120]   and was walking around and knew it was late-ish at night. I told my wife, I was like, "Remember,
[00:26:33.120 --> 00:26:37.920]   there was a Barnes & Noble there. There was a big, beautiful Barnes & Noble. Walked another block
[00:26:37.920 --> 00:26:42.480]   and there was an Amazon store opening." Yeah. And I thought, "Well, this is Paradise and they put
[00:26:42.480 --> 00:26:46.720]   up a bookstore." Yeah, they ran the bookstores out of business so that they could open up
[00:26:46.720 --> 00:26:53.360]   bookstores. Here's the... This is on business inside of the original Amazon 1995 web page.
[00:26:53.360 --> 00:26:57.200]   Remember when all web pages... Oh! Oh! Remember when all web pages were gray?
[00:26:57.200 --> 00:27:01.280]   Oh, yeah. Those are even times. We didn't have white. I guess. I don't know why.
[00:27:01.280 --> 00:27:04.880]   It's easy on the eyes. I'm sure someone's like, "You know what's easier on the eyes gray."
[00:27:05.440 --> 00:27:10.320]   Look at that. It says, "If you explore just one thing, make it our personal notification service."
[00:27:10.320 --> 00:27:14.160]   We think it's very cool. One million titles, consistently low prices.
[00:27:14.160 --> 00:27:20.400]   Remember when one million titles was like, "Wow! That's way more than the book." It's everything.
[00:27:20.400 --> 00:27:24.160]   It's everything in the world. Everything is here. Every book ever printed.
[00:27:24.160 --> 00:27:26.640]   They did call themselves. Except for the book you were looking for. That's how it worked.
[00:27:26.640 --> 00:27:30.240]   You were like, "The Earth's biggest bookstores." Here is this.
[00:27:30.240 --> 00:27:37.040]   Did you see that Amazon was getting up in antitrust? They're now saying that government is starting
[00:27:37.040 --> 00:27:41.200]   to look very closely at what Amazon's doing as far as an antitrust because if it gets much bigger...
[00:27:41.200 --> 00:27:46.000]   I think they should. They should. Yeah. At the same time, I kind of have to admire Jeff and his
[00:27:46.000 --> 00:27:52.960]   ambition. Clearly, he just goes back on Amazon. He's truly... He's the most visionary. He's up
[00:27:52.960 --> 00:27:58.560]   there with Elon Musk. He's a true visionary and a very, very disciplined visionary. Elon Musk is
[00:27:58.560 --> 00:28:06.400]   like, "Oh, it's all a simulation. Still a visionary." Jeff Bezos doesn't go into flights of fancy.
[00:28:06.400 --> 00:28:10.720]   He is focused, focused, focused on running everybody else out of business and stealing the business.
[00:28:10.720 --> 00:28:15.120]   I say that like it's a bad thing. It's actually a mixed bad thing.
[00:28:15.120 --> 00:28:17.600]   Well, it's kind of a bad thing. But it's a mixed bad thing.
[00:28:17.600 --> 00:28:22.160]   We get some benefit from it. I mean, I order... This week, I probably ordered 10,
[00:28:22.160 --> 00:28:28.320]   15 things on Amazon. I get boxes every single day from Amazon. It's basically my shopping now.
[00:28:28.320 --> 00:28:32.960]   It's where I get out of my stuff. I'll go and I'm like, "No, I'm going to Amazon."
[00:28:32.960 --> 00:28:37.120]   It's like, "Yeah, I go to the store and I want a specific thing. I hate shopping, first of all.
[00:28:37.120 --> 00:28:41.360]   And I want a specific thing when I go to the store and they never have it. And I'm just like,
[00:28:41.360 --> 00:28:44.480]   "Well, I guess Amazon's getting the money and I get on my phone and look it up."
[00:28:44.480 --> 00:28:50.640]   Ship. I ordered some little thing. It was a $17 thing and they said, "It'll arrive Sunday."
[00:28:50.640 --> 00:28:55.760]   I know. I ordered it Saturday. I know. I'm like, "Wait, ordering it on Saturday and it's arriving on
[00:28:55.760 --> 00:28:59.360]   Sunday." I don't understand how it works. And it arrived early. And it's like, "Wow."
[00:28:59.360 --> 00:29:03.200]   And yet you do worry as they eat more and more of the world. Is they buy whole foods? Is they
[00:29:03.200 --> 00:29:06.560]   eat more and more of the world? The pretty soon there will be no other retailers.
[00:29:06.560 --> 00:29:13.280]   Right now we choose to buy from Amazon. In 2025, we probably won't have the choice.
[00:29:13.280 --> 00:29:17.440]   Maybe. Well, keep in mind that shopping at Walmart is not exactly a pleasant experience.
[00:29:17.440 --> 00:29:22.640]   Well, it isn't. There aren't people, though. It's funny because we're nerds. We obviously look at
[00:29:22.640 --> 00:29:28.320]   us. We obviously shop online. I got these fine Dickies suspenders at Amazon.
[00:29:28.320 --> 00:29:34.480]   But I think there are a lot of people. I talk to people all the time. I say, "No, but I like to
[00:29:34.480 --> 00:29:38.640]   shop. I like to go to a store. I like to look at merchandise." So here's one that kind of blew me
[00:29:38.640 --> 00:29:44.720]   away because I always thought Amazon is mostly a US market. We made a really good friend in Morocco
[00:29:44.720 --> 00:29:48.720]   and she's just obsessed with reading and books. I want to show you my book collection. She has
[00:29:48.720 --> 00:29:52.000]   this big book collection. Where are you getting these books? Because they're all in English. She's
[00:29:52.000 --> 00:29:58.080]   like, amazon.com. I'm like in Morocco. She's like, "Oh, yeah." Wow. And they deliver. They probably
[00:29:58.080 --> 00:30:04.560]   has delivery and overnight delivery and all of a sudden. It's amazing. Sears. Sears, which was
[00:30:04.560 --> 00:30:10.720]   the Amazon of its day. Sears had the opportunity to be Amazon. When I was a kid, I remember we would
[00:30:10.720 --> 00:30:15.680]   go to my grandmother's house and she would give us the Sears catalog. Boom. The Christmas toy catalog.
[00:30:15.680 --> 00:30:20.640]   Boom. Get get us. And we'd look through it and all I want this and then she would order it and
[00:30:20.640 --> 00:30:26.240]   then it would show up. Sears had the opportunity and they blew it. Sears has closed 200 Sears and
[00:30:26.240 --> 00:30:32.160]   Kmart stores this year alone. They have lost about $10 billion in the last few years. And now
[00:30:32.160 --> 00:30:39.520]   they've announced they're going to start selling Sears Kenmore appliances on Amazon. And those
[00:30:39.520 --> 00:30:47.120]   appliances will run the A letter, the A word. How we say it? I just said we echo. I say
[00:30:47.120 --> 00:30:52.880]   echo or Amazon voice services. They're going to add Kenmore is adding echo functionality.
[00:30:52.880 --> 00:30:58.800]   Wow. And I imagine it a dash button too because if you have a washer dryer and you don't,
[00:30:58.800 --> 00:31:03.440]   and you don't want to have soap, you just push the button and guess where that button doesn't go to
[00:31:03.440 --> 00:31:11.360]   Sears. It is an amazing thing. It's hard. I would imagine it's hard, first of all,
[00:31:11.360 --> 00:31:16.480]   in the current political climate, I don't think there'll be much antitrust action anyway.
[00:31:17.440 --> 00:31:22.240]   But they're busy. They're busy. All these Goldman Sachs guys, the last thing they want to do is
[00:31:22.240 --> 00:31:27.440]   shut down company. But even so, it would be hard to prosecute Amazon because
[00:31:27.440 --> 00:31:31.760]   Amazon can rightly say, well, we have competition in every single market. There's a store on every
[00:31:31.760 --> 00:31:36.960]   corner. How could you say we have a monopoly? People shop with us. We dominate because we're
[00:31:36.960 --> 00:31:41.840]   better. And most of our products are from other companies selling in products. They're smart
[00:31:41.840 --> 00:31:46.400]   that way, right? Because you're right. They're empowering Sears. But if we can get back to this
[00:31:46.400 --> 00:31:51.040]   refrigerator thing, I think what they're really gearing toward is the future of fridges where
[00:31:51.040 --> 00:31:55.760]   it's not just like barcodes or whatever, where they'll have AI looking at what's in the fridge and
[00:31:55.760 --> 00:31:59.120]   determining what you're low on, what you need more of. And in fact, they're working on it in
[00:31:59.120 --> 00:32:03.920]   their secret, well, not secret, they're not so secret. They're store at their headquarters,
[00:32:03.920 --> 00:32:08.960]   grocery store, where you supposedly are supposed to take out products without paying. And the way
[00:32:08.960 --> 00:32:14.320]   they do that is a combination of like you pick something up that the shelf has a scale on it. So
[00:32:14.320 --> 00:32:18.080]   knows that, oh, it's this much lighter. That's about the amount of a yogurt. It looks at the
[00:32:18.080 --> 00:32:22.880]   person's history because using face recognition, that person buys a lot of yogurt. It looked like
[00:32:22.880 --> 00:32:28.720]   yogurt in the AI. They just picked up yogurt. Like so it's using all these different sensors and
[00:32:28.720 --> 00:32:33.920]   AI together to figure out what food products are that people are moving up. That is going straight
[00:32:33.920 --> 00:32:40.800]   into refrigerators. Are we giving them too much power when we use echoes, when we shop in the
[00:32:40.800 --> 00:32:45.840]   Amazon stores? Obviously, all of these things are almost, they probably lost leaders in order to
[00:32:45.840 --> 00:32:52.160]   gain data. And once they have all that data, then they can eat the world. Should we be thinking now
[00:32:52.160 --> 00:32:56.400]   about not participating? Greg, you're in the UK. You don't even have the choice, right?
[00:32:56.400 --> 00:33:04.640]   I think the thing about Amazon ultimately is the same as what we have with Google and Facebook
[00:33:04.640 --> 00:33:07.520]   and Netflix. They just have so much data they can out compete.
[00:33:08.160 --> 00:33:13.360]   That's right. They have a huge advantage. There's at some point an insurmountable advantage to a
[00:33:13.360 --> 00:33:18.880]   certain extent. It's all about the data. And the second thing is that these companies are
[00:33:18.880 --> 00:33:24.320]   technology companies, whereas Sears and Walmart made the mistake of thinking that they were
[00:33:24.320 --> 00:33:28.400]   management companies or retail companies. What were they thinking?
[00:33:28.400 --> 00:33:34.960]   And they didn't sort of understand the trend that technology is really so the mistake I made
[00:33:34.960 --> 00:33:39.520]   about Amazon five, 10 years ago when I was up on, I used to wander around Wall Street talking to
[00:33:39.520 --> 00:33:46.400]   investors. And discussing this point was, I made the mistake of thinking that Amazon was a retail
[00:33:46.400 --> 00:33:51.520]   company or whatever. And it turns out that they're absolutely an utterly a technology company,
[00:33:51.520 --> 00:33:56.960]   and they're building a technology platform that can do sell anything. It is a fulfillment business.
[00:33:56.960 --> 00:34:02.800]   For example, McDonald's Corporation, this is something I learned a while ago, is actually not a fast
[00:34:02.800 --> 00:34:07.680]   food company. It's a real estate trust. And the secret to McDonald's success is every time they
[00:34:07.680 --> 00:34:15.280]   open a store, they open a man. They opened the land that the property is on. And so McDonald's
[00:34:15.280 --> 00:34:19.840]   is a real estate company. It's a real estate trust, right? And the fast foods kind of justifies
[00:34:19.840 --> 00:34:24.000]   owning the portfolio and generates enough money to fund the purchase of more and more land.
[00:34:24.000 --> 00:34:29.920]   We learned that about movie company, movie theaters here in Petaluma, because the movie theater,
[00:34:29.920 --> 00:34:34.320]   we had one movie theater some years ago, and they were sitting on a lot of land, which they sold
[00:34:34.320 --> 00:34:38.720]   for a shopping mall. And they're, we're not in the movie business, see ya. They didn't open another
[00:34:38.720 --> 00:34:43.760]   theater, they just moved on. So that's not an unusual radio station. A lot of people think radio,
[00:34:43.760 --> 00:34:49.200]   terrestrial broadcast radio is really about real estate. The land, the station zone, and their
[00:34:49.200 --> 00:34:54.320]   towers as opposed, because they certainly aren't making any money, you know, not much money in
[00:34:54.320 --> 00:34:58.880]   broadcasting. There's good money in owning, weirdly, there's good money in owning radio stations right
[00:34:58.880 --> 00:35:03.280]   now, because they have a right to a certain amount of frequency. Every radio station has a license.
[00:35:03.280 --> 00:35:11.440]   There are new standards coming up in the 5G networking, so the successor to 4G.
[00:35:11.440 --> 00:35:16.800]   That needs those radio frequencies to operate. And there's a consortium of rich people like
[00:35:16.800 --> 00:35:22.240]   Michael Dell and, you know, the high big names. They're buying spectrum, buying radio stations.
[00:35:22.240 --> 00:35:26.240]   They're buying up radio stations right the way across America, so they can then sell them on
[00:35:26.240 --> 00:35:31.200]   later on into a 5G. So another kind of real estate, isn't it? It is. If you own that spectrum,
[00:35:31.200 --> 00:35:36.240]   you wouldn't even sell it, you'd rent it. So you do like you do in London where the land is owned
[00:35:36.240 --> 00:35:40.960]   on 100 year lease. At the end of 100 years, the license comes back to you and then you still own
[00:35:40.960 --> 00:35:45.840]   it. And in fact, the Chinese government owns many of the radio stations in the United States,
[00:35:45.840 --> 00:35:50.160]   including in Washington, DC. Yeah, this is something that Reuters
[00:35:50.160 --> 00:35:54.080]   broke up in a couple of years ago. It's kind of an interesting story that they'll be. Well, they're
[00:35:54.080 --> 00:35:58.000]   going to be able to cash in more because Clear Channel, one of the biggest radio stations in the
[00:35:58.000 --> 00:36:04.880]   US, has so much debt that they're clearly in play over the next year. They cannot pay back their debt.
[00:36:04.880 --> 00:36:11.120]   In comes China. Yep. But I don't think it's an investment. I think it's pro-Beijing propaganda
[00:36:11.120 --> 00:36:17.440]   type of thing. On these Washington, DC stations? Oh, yeah, WCRW, which is the Washington, DC one,
[00:36:17.440 --> 00:36:22.400]   but they own many across the US and across around the world. But they have, I think they have
[00:36:23.120 --> 00:36:30.720]   33 stations in the US owned by the Chinese government. So kind of interesting.
[00:36:30.720 --> 00:36:36.480]   That's where I listen to podcasts. I hear media. Yeah. That's why I do podcasts. I ostensibly
[00:36:36.480 --> 00:36:41.680]   have a career in radio. American owners. I think it's better to work in podcasts these days.
[00:36:41.680 --> 00:36:47.360]   I hear media who I just close your work for. The radio show I do is owned by their
[00:36:47.360 --> 00:36:54.560]   syndicating our premier networks, has $20 billion in debt and really no way to pay it off.
[00:36:54.560 --> 00:36:59.840]   Yeah. So there's a reckoning day coming very soon for them. They just repainted their building
[00:36:59.840 --> 00:37:06.640]   where I work. Paint's cheap. Come on, guys. There's a problem. They're selling it. They're getting
[00:37:06.640 --> 00:37:10.960]   it all nice and pretty. You don't want to look like you've got $20 billion. Yeah, that's right.
[00:37:10.960 --> 00:37:14.640]   All right, let's take a break. We'll come back with more. Greg Farrow is here for the
[00:37:14.640 --> 00:37:21.360]   Packet Pushers Network. A great podcast and his two beers shows on YouTube are also fascinating.
[00:37:21.360 --> 00:37:25.760]   I think I was watching just a little bit of before the show. He has one beer when he writes it and
[00:37:25.760 --> 00:37:30.080]   another beer when he delivers it. That's right. Well, sometimes it might be more, but you'll never
[00:37:30.080 --> 00:37:35.200]   know. You didn't tell you how big the beer is. It's good English here. So it's okay. Thank you
[00:37:35.200 --> 00:37:38.800]   for being here, Greg. We like that. And thank you for coming back early from Prague. That's a
[00:37:39.360 --> 00:37:45.120]   Prague is a beautiful city. Oh, gosh, I am definitely going to go back there real soon.
[00:37:45.120 --> 00:37:49.680]   Yeah, I think didn't didn't didn't didn't check. Practically invented beer.
[00:37:49.680 --> 00:37:56.960]   Bill Sherman's are going to get mad. Yeah. They make a fine pills near though. Yeah. Also with
[00:37:56.960 --> 00:38:02.480]   this Mike Elgin from, well, lots of places you see his writing computer world and fast company
[00:38:02.480 --> 00:38:08.880]   all over the place. But don't forget. Astronomad.net last chance. One lucky couple.
[00:38:08.880 --> 00:38:11.920]   That's right. It's going to get to join Mike in a mirror and some of the best chefs,
[00:38:11.920 --> 00:38:16.480]   including the best baker in Spain. That's right. In September. That's right. September.
[00:38:16.480 --> 00:38:22.000]   And it's going to be a week of spectacular foodie adventure. How many couples total?
[00:38:22.000 --> 00:38:26.240]   It's going to be eight. So you've sold out seven and now you've got room for one more couple.
[00:38:26.240 --> 00:38:30.560]   Six people and then one more couple. So it's a very small group. Oh, six people. Yeah.
[00:38:30.560 --> 00:38:35.360]   Oh, wow. Six people we have now and then we're going to have eight. So it's a very, very small
[00:38:35.360 --> 00:38:40.160]   group plus, plus Amir and I and we have some people helping us. And so it's a very, very small group.
[00:38:40.160 --> 00:38:45.360]   Kevin and Nadia will be there. They will as well. And Princess Squishy face. She's going to help
[00:38:45.360 --> 00:38:50.080]   be very helpful. I'm sure. She's the most the cutest baby you've ever seen. But we're going to
[00:38:50.080 --> 00:38:54.320]   really, really under by the time this week is over, we're going to understand. Top us. I can't
[00:38:54.320 --> 00:38:58.160]   wait. We're going to understand Spanish wine. We're going to really. We're going to do this.
[00:38:58.160 --> 00:39:02.400]   We can't do this one and Feds. I think we can't. But yeah, we got some coming up that we're going
[00:39:02.400 --> 00:39:08.240]   to do. Maybe next year, I really like this also here from Engadget formerly of TMZ, the 30-mile zone.
[00:39:08.240 --> 00:39:15.200]   It's so long ago. I know, but I love TMS. I love TMZ. Roberto Baldwin. That's where you
[00:39:15.200 --> 00:39:20.480]   that's where you made your bones, right? It is. It is. I learned a lot about writing and writing
[00:39:20.480 --> 00:39:25.520]   in a way that doesn't get you sued. Yeah. I learned a lot of useful skills. No, it's a very useful
[00:39:25.520 --> 00:39:30.640]   still is and I learned a lot about sitting down with a lawyer while writing. Yeah. Like what you
[00:39:30.640 --> 00:39:33.840]   can and cannot say. Like they were like, "Rob, you write this." And then I would write it and I would
[00:39:33.840 --> 00:39:36.880]   show it. I would sit with a lawyer and I would sit with a lawyer and say, "I would like everyone to
[00:39:36.880 --> 00:39:41.280]   tell me a day or a day and then Harvey would come in and have an argument over like one word."
[00:39:41.280 --> 00:39:45.600]   He wanted to do it. He was like, "What about this word?" We can't say this word about this word.
[00:39:45.600 --> 00:39:49.200]   I'm like, "Well, what about this?" Yeah, I was. Because Harvey is a lawyer, actually. Harvey, yes,
[00:39:49.200 --> 00:39:54.160]   or was a lawyer. I don't know how that works. But yeah, he went to law school. I need, I need,
[00:39:54.160 --> 00:39:58.640]   I need to, because there's many of the time where I wish I had a counselor. I could whisper in,
[00:39:59.520 --> 00:40:01.920]   "No, I'm sorry. I can't say that." Yeah.
[00:40:01.920 --> 00:40:07.600]   Leo, that's it. You can't say that. Yeah, I learned a lot about what I can and cannot say.
[00:40:07.600 --> 00:40:12.400]   That's actually very helpful. Very useful media. I'm sorry. So you can't tell us what it is.
[00:40:12.400 --> 00:40:15.360]   Yeah, I'm sorry. And it doesn't come up much it in gadget. I don't think probably.
[00:40:15.360 --> 00:40:18.480]   Yeah, there's not a lot of writing about whether or not someone's on cocaine or not.
[00:40:18.480 --> 00:40:23.200]   Can't say it was cocaine because you didn't have a, did the police have it? Have they done a test?
[00:40:23.200 --> 00:40:26.240]   It's just white powder. It's vicious white powder. All around his mouth.
[00:40:26.240 --> 00:40:29.440]   It could have been sugar donuts. We don't know. Could have been sugar donuts. Could have been
[00:40:29.440 --> 00:40:32.880]   donut. He was a cop. Yeah, probably. Yeah. He seemed a little excitable.
[00:40:32.880 --> 00:40:39.200]   Donuts are underrated, I think. Well, apparently there are people in our audience who feel exactly
[00:40:39.200 --> 00:40:46.160]   the same way. Do they have donuts in Germany? They do. I just check. Because I don't want to go there,
[00:40:46.160 --> 00:40:51.760]   if they don't. Our show today brought to you by Rocket Mortgage. Quick and Lones,
[00:40:51.760 --> 00:40:56.080]   the best mortgage lender in the country. Just a great company, very techno savvy.
[00:40:56.080 --> 00:41:01.440]   I can tell you, it sounds like hyperbole. It sounds like I'm making something up when I say
[00:41:01.440 --> 00:41:06.160]   the best lender in the country. But if you go to their website, you go to rocketmortgage.com/twit
[00:41:06.160 --> 00:41:11.520]   and the number two and scroll down a little bit, you'll see all of those JD Power Customer
[00:41:11.520 --> 00:41:17.040]   Satisfaction Awards. See, I'm not making it up. Number one in customer satisfaction for primary
[00:41:17.040 --> 00:41:22.000]   mortgage origination for the last seven years. And I'm sure 2017, they're going to add that little
[00:41:22.000 --> 00:41:26.400]   trophy there. And number one for mortgage servicing for the last three years. So they really are the
[00:41:26.400 --> 00:41:31.360]   best lender in the country. And because they're tech savvy, they decided that they could make a
[00:41:31.360 --> 00:41:39.120]   better experience for us geeks. The mortgage approval process is not, it's practically a 19th
[00:41:39.120 --> 00:41:46.080]   century process. You go to a bank, you bring a box of paperwork, you'll maybe even fax stuff.
[00:41:46.720 --> 00:41:51.760]   It's primitive. And it takes a long time. The last loan we got when we bought our house three
[00:41:51.760 --> 00:41:55.760]   years ago, Lisa and I took, we just checked, it took three months, three months. And we were
[00:41:55.760 --> 00:42:01.920]   still faxing stuff. Like from, they kept saying move more, rocket mortgage wasn't around then,
[00:42:01.920 --> 00:42:08.080]   from now on, goodbye paperwork, hello rocket mortgage. And it's transparent. You get the confidence
[00:42:08.080 --> 00:42:13.040]   you need when you're buying a home or refinancing, you can do that there. You'll know the details,
[00:42:13.040 --> 00:42:17.920]   you'll be confident you're getting the right mortgage for you. You could provide all of the paperwork,
[00:42:17.920 --> 00:42:23.680]   everything online with a touch of a button you can do on your phone. And because it's all computerized,
[00:42:23.680 --> 00:42:27.760]   they crunch those numbers fast, they analyze all the options they're going to give you the best
[00:42:27.760 --> 00:42:34.000]   possible loan based on assets, income, credit, just for you. And they're going to do that not in
[00:42:34.000 --> 00:42:37.600]   months, not in weeks, not in days, they're going to do that in minutes right there while you're
[00:42:37.600 --> 00:42:43.360]   that you could be at an open house, applying for a loan rocket mortgage by quick and loans, apply
[00:42:43.360 --> 00:42:50.320]   simply, understand fully, and then mortgage confidently to get started, go to rocket mortgage.com/twit
[00:42:50.320 --> 00:42:56.320]   and the number two equal housing lender licensed in all 50 states and MLS consumer access.org
[00:42:56.320 --> 00:43:02.960]   number 3030 rocket mortgage.com/twit and the number two, and we thank quick and loans and
[00:43:02.960 --> 00:43:08.160]   rocket mortgage so much for their support. They've been with us more than a year now,
[00:43:08.160 --> 00:43:14.400]   great support of this weekend tech and all of our shows. I loved this mic drop moment. Tech Crunch
[00:43:14.400 --> 00:43:19.040]   interviewed a guy named Rodney Brooks, the founding director. This guy's got cred,
[00:43:19.040 --> 00:43:23.920]   founding director of MIT's Computer Science and Artificial Intelligence Lab. He was co-founder of
[00:43:23.920 --> 00:43:30.240]   iRobot, the Roomba company, re-think robotics. Connie Loizos of Tech Crunch was interviewing him
[00:43:30.240 --> 00:43:34.880]   and asked him about Elon Musk. Remember, Elon's been saying lately, we got to do something. The
[00:43:34.880 --> 00:43:42.880]   AI is going to take over the world. She said, Elon Musk, you're writing a book on AI, so I have to
[00:43:42.880 --> 00:43:48.240]   ask you, Rodney, Elon Musk expressed this again this past weekend that AI is, in Elon's words,
[00:43:48.240 --> 00:43:54.560]   an existential threat to humans. Like the guy's seen all the Terminator movies, he knows.
[00:43:54.560 --> 00:43:59.760]   Agree or disagree, Rodney Brooks says, there are quite a few people out there who said that AI
[00:43:59.760 --> 00:44:06.160]   is an existential threat. Stephen Hawking, astronomer Royal Martin Reese, who's written a book about it,
[00:44:06.160 --> 00:44:12.640]   they all share a common thread in that they don't work in AI. For those who work in AI,
[00:44:12.640 --> 00:44:18.480]   we know how hard it is to actually get anything to work through the product level. I think people
[00:44:18.480 --> 00:44:26.000]   were on the ground with AI, realized that it's primitive. Even things like Amazon's Echo or
[00:44:26.000 --> 00:44:31.840]   Siri, he says, the reason people, including Elon, make this mistake, when you see somebody do
[00:44:31.840 --> 00:44:37.120]   something like play chess or a go master, and we understand what's involved because we understand
[00:44:37.120 --> 00:44:42.320]   how humans think. We understand the competence and the training and the brains that it takes.
[00:44:42.320 --> 00:44:47.520]   But when you see, for instance, Deep Mind's Alpha Go beat the Chinese Go champion,
[00:44:47.520 --> 00:44:53.760]   you think, oh, this machine is so smart, it can do anything. He said, but I was with Deep Mind and
[00:44:53.760 --> 00:44:56.960]   London about three weeks ago, and they admitted things could easily have gone very wrong. It can't
[00:44:56.960 --> 00:45:04.560]   do anything. It's really good in playing Go. It's a machine. If you've used any of the sort of bots,
[00:45:04.560 --> 00:45:10.160]   remember messenger bots? If you've used any of those, you know that AI is still sort of not
[00:45:10.160 --> 00:45:15.920]   so far. It's a lysa level, right? It's not sophisticated. So, and then she said, well,
[00:45:15.920 --> 00:45:21.520]   Elon's point is we need to regulate it before it gets smart. It's not smart. Yes, I know it's not
[00:45:21.520 --> 00:45:25.200]   smart, but it's going to get smart. We got to regulate it now. Rodney Brooks says, so you're
[00:45:25.200 --> 00:45:30.240]   going to regulate now well. And this is very, I thought of you, Greg Farah, when I read this logical
[00:45:30.240 --> 00:45:36.400]   refutation, he says, first, if you're going to have a regulation now, either it applies to something
[00:45:36.400 --> 00:45:41.920]   and changes something in the world that exists, or it doesn't apply to anything. If it doesn't
[00:45:41.920 --> 00:45:47.280]   apply to anything, what the hell is the regulation for, right? So tell me what behavior you want to
[00:45:47.280 --> 00:45:52.480]   change, Elon. You can't write a regulation today because you don't have an example
[00:45:52.480 --> 00:45:57.840]   of what you want to prevent. And then this is the mic drop moment. By the way, Rodney Brooks goes on,
[00:45:57.840 --> 00:46:02.480]   let's talk about regulation on self-driving Teslas because that's an issue. And that's
[00:46:02.480 --> 00:46:08.880]   driven by AI. Self-driving vehicles are all driven by AI. AI is huge. Without AI, self-driving cars
[00:46:08.880 --> 00:46:14.240]   don't work. I think what Brooks's point is you can't trust a Tesla to drive yourself.
[00:46:14.240 --> 00:46:18.640]   They maybe should be regulating that because that is something that's in the world.
[00:46:18.640 --> 00:46:25.040]   I actually wonder if Elon Musk is being misinterpreted here. I hear him talking about
[00:46:25.040 --> 00:46:32.240]   the social impact and the impact of big data much more than as a film of AI.
[00:46:32.240 --> 00:46:35.440]   And we've kind of talked about that. I mean, when you say Amazon is going to eat the world,
[00:46:35.440 --> 00:46:40.400]   Sears is out of business. That's a lot of jobs. That's a lot of people who aren't going to be
[00:46:40.400 --> 00:46:46.160]   working. That's right. If you think about artificial intelligence in, let's try and
[00:46:46.160 --> 00:46:50.960]   bring some practicality around this. They're talking about truck drivers being removed from
[00:46:50.960 --> 00:46:56.400]   the roads within 30 years. And I think it's something like 7% of jobs in the US are actually
[00:46:56.400 --> 00:47:02.320]   truck drives. So you're going to have, so that's one. You're talking about manufacturing jobs.
[00:47:02.320 --> 00:47:08.480]   There was an article a few weeks back about an entire iron manufacturing facility and it employs
[00:47:08.480 --> 00:47:15.280]   10 people because it's fully automated using robots. And artificial intelligence will continue to
[00:47:15.280 --> 00:47:19.840]   do that. I think we're also seeing articles come out about artificial intelligence. Your point
[00:47:19.840 --> 00:47:24.720]   about chatbots is well taken. But keep in mind that chatbots don't actually involve real money.
[00:47:24.720 --> 00:47:29.200]   So nobody's actually doing real science. I was investing in chatbots.
[00:47:29.200 --> 00:47:34.560]   Chatbots, I'm like, no, no. Because you guys use them once. They're so annoying.
[00:47:34.560 --> 00:47:38.400]   Yes. And then you start using them. Have a look at the guy who did the website with the
[00:47:38.400 --> 00:47:45.920]   parking tickets and the speeding fines using ML or AI like activities to basically put lawyers
[00:47:45.920 --> 00:47:51.440]   out of business. So if it starts taking out the middle class, the professional middle class,
[00:47:51.440 --> 00:47:56.320]   as well as the working class, you actually have something that is right for disruption in society
[00:47:56.320 --> 00:48:02.320]   because you're going to have massive social disruption if artificial intelligence replaces
[00:48:02.320 --> 00:48:06.640]   those people. And I think this is the point that Elon is trying to make. It's not that
[00:48:06.640 --> 00:48:11.040]   some machine is going to take over the world. Although that's certainly possible. I mean,
[00:48:11.040 --> 00:48:15.600]   you look at what Petra did and not Petra, you know, the outbreak of the war cry.
[00:48:15.600 --> 00:48:16.160]   Yes.
[00:48:16.160 --> 00:48:22.080]   Those are, I actually see those as like cyber warfare. I think Russia was literally, you know,
[00:48:22.080 --> 00:48:30.320]   detonating a piece of cyber technology in Romania, sorry, in wherever it was Ukraine,
[00:48:30.320 --> 00:48:35.760]   as a bit like an atomic test back in the 1960s, you know, you let off the atomic bomb to say,
[00:48:35.760 --> 00:48:41.280]   hey, I've got atomic bombs to tell you, you know, blah, blah, blah. I think artificial intelligence
[00:48:41.280 --> 00:48:47.200]   could be used in combination with cyber warfare to have an enormous impact and that what he's
[00:48:47.200 --> 00:48:50.800]   tried to say to the lawmakers or to the government is you need to wake up to this.
[00:48:50.800 --> 00:48:56.960]   There is massive social change coming driven by technology and probably simplifying it to AI
[00:48:56.960 --> 00:49:01.840]   because your average senator is a bit of an idiot, you know, their whole reason to try is pretty
[00:49:01.840 --> 00:49:07.360]   straightforward and that you need to be planning for these things. You need to start legislating,
[00:49:07.360 --> 00:49:12.880]   not to prevent AI, but to work out how you're going to run the country in, you know, 20 years
[00:49:12.880 --> 00:49:16.400]   time when 50% of your population doesn't have to work for a living.
[00:49:16.400 --> 00:49:20.720]   I have been using this one bot. I have been using called trim. Have you ever heard of this one?
[00:49:20.720 --> 00:49:26.240]   This is a financial bot that you give, you have to give it access to your finances, but then it,
[00:49:26.240 --> 00:49:30.240]   you can show this. I don't think there's anything in here, but, and then it will show you,
[00:49:30.240 --> 00:49:35.680]   it will tell you about transactions, but it also shows you about recurring transactions and,
[00:49:35.680 --> 00:49:42.000]   and gives you the chance to cancel the transaction. So, you chit chat with the bot. You say, hey,
[00:49:42.000 --> 00:49:45.360]   how much money do I have in my savings account? I don't want to do a snap. There we go.
[00:49:45.360 --> 00:49:49.920]   Just tell you. Yeah, which is kind of interesting. I mean, so, but this isn't really a bot.
[00:49:49.920 --> 00:49:54.480]   No. This is, I don't know. It's not AI. That's true. The biggest, the funny thing to me about
[00:49:54.480 --> 00:49:59.840]   fear over AI is that it is potentially scary because of the power it gives to people.
[00:49:59.840 --> 00:50:04.800]   I would trust the morality of a person of a AI more than the morality of a, at least
[00:50:04.800 --> 00:50:09.600]   is predictable. Right. So, we always think, we do this in biology too. There's cloning and all that
[00:50:09.600 --> 00:50:13.280]   kind of stuff. Oh, what should the US do? What should the UK do? What should Germany do?
[00:50:13.280 --> 00:50:21.600]   What is North Korea going to do with AI? What is, you know what I mean? So, I'm, people scare me a
[00:50:21.600 --> 00:50:25.280]   lot more. You know, I'll have to stay ahead. You have to stay ahead of them. I mean,
[00:50:25.280 --> 00:50:32.240]   DARPA last year at DEF CON, they had the cyber challenge, where they had AI machines battling
[00:50:32.240 --> 00:50:37.200]   each other, basically hacking each other. And they're like, we have to do this because other
[00:50:37.200 --> 00:50:40.720]   nation states are going to do this. So, we're having hackers work on this now, but DARPA is
[00:50:40.720 --> 00:50:46.160]   working on basically AI hackers because if they don't, you're going to follow behind and you're
[00:50:46.160 --> 00:50:51.520]   going to lose to Russia, to North Korea, to whoever, to random, you know, usually it's a nation state
[00:50:51.520 --> 00:50:55.520]   if you're talking about that much power. But you have to, you know, you have to have somebody
[00:50:55.520 --> 00:50:59.840]   who is researching and working on this because if you don't, someone else's.
[00:50:59.840 --> 00:51:04.800]   Elon sent this picture to Rodney Brooks as his response. It's a picture of him. Just after Amber
[00:51:04.800 --> 00:51:10.480]   heard, kissed him. I don't know. Maybe that's just a simulation. What gives us a difference?
[00:51:10.480 --> 00:51:15.200]   By the way, would Harvey allow that? I don't know. I'm assuming there's lipstick on the cheek.
[00:51:15.200 --> 00:51:18.960]   She's got red lipstick. It seems like the shades match. I'm just a circumstantial.
[00:51:18.960 --> 00:51:22.560]   Circumstantial. There doesn't seem to be any donut powder available at all.
[00:51:22.560 --> 00:51:30.720]   To me, I think that the most culture-changing impact of AI will simply be the augmentation of
[00:51:30.720 --> 00:51:35.680]   human. We're going to interact with our computers through AI virtual assistant.
[00:51:35.680 --> 00:51:41.040]   I feel like that's kind of what we do with Echo already. And it's primitive, but it sure is nice.
[00:51:41.040 --> 00:51:46.240]   But as it gets less primitive, it's going to be amazing when the AI can figure out what we mean,
[00:51:46.240 --> 00:51:48.800]   no matter how we say it. In no way is that threatening.
[00:51:48.800 --> 00:51:55.040]   Again, it's only threatening when a killer ropethrony is using it.
[00:51:55.040 --> 00:51:59.840]   Yeah, it's only threatening how you use it. I mean, if you're like, "Oh, well, Amazon is not going to
[00:51:59.840 --> 00:52:04.400]   do something crazy." And then Jeff leaves the company because he retires and then
[00:52:04.400 --> 00:52:10.240]   random somebody comes and says, "Well, that is all that data sitting there is a supervillain."
[00:52:10.240 --> 00:52:14.560]   Yeah, the other way that these can be misused, virtual assistants are going to affect culture,
[00:52:14.560 --> 00:52:20.160]   is the new rock movie, which rock and Siri are starring in a new movie coming out, which the
[00:52:20.160 --> 00:52:23.920]   rock said on Twitter is going to, quote, "drop tomorrow," unquote. I don't know what drop means.
[00:52:23.920 --> 00:52:30.320]   From now on, there's a man. He just announced an upcoming movie with Siri, really?
[00:52:30.320 --> 00:52:36.080]   Yeah, really. So in the poster, there's all this crazy stuff happening with the rock in the
[00:52:36.080 --> 00:52:41.760]   middle holding an iPhone. It's called the Rock X-Siri Dominate the Day. This is a joke.
[00:52:41.760 --> 00:52:50.000]   Oh, it's Apple's doing it. It's Apple. Well, Apple's in Apple. This is just this is Apple
[00:52:50.000 --> 00:52:56.080]   marketing. Watch the full film at youtube.com/apple. It's an ad. We're all going to watch it, Leo.
[00:52:56.080 --> 00:52:59.360]   I'm going to watch it right now. Siri's so bad. I just don't think it's not.
[00:52:59.360 --> 00:53:03.760]   If the movie is based on the rock just getting in the rock or silver and over and over again,
[00:53:03.760 --> 00:53:08.160]   because Siri is so bad, then yeah, I would totally watch that because Siri is not good.
[00:53:08.160 --> 00:53:12.160]   I'd like to see the rock crush Siri. He's just like, "Come on, Siri!"
[00:53:12.160 --> 00:53:17.120]   Lisa and I have a game because we'll ask Siri stuff and it's just the most...
[00:53:20.240 --> 00:53:27.920]   Yesterday, I was going to a concert in San Jose and I was looking for a place to eat.
[00:53:27.920 --> 00:53:32.240]   And I said, "As you're supposed to say, can I get restaurants near me?"
[00:53:32.240 --> 00:53:38.560]   I wonder, does anybody in Apple actually use this? So it says, "Yes, here's the restaurant
[00:53:38.560 --> 00:53:45.760]   near me. It shows me a list, flashes it briefly, and then starts one by one reading the names of
[00:53:45.760 --> 00:53:51.680]   the restaurant and saying, "Would you like me to contact them?" It hid the list and I know,
[00:53:51.680 --> 00:53:57.200]   and then it goes to the next one. No, and it's like you can't... See, the information's there,
[00:53:57.200 --> 00:54:03.040]   but it hit it and now it's bothering me with this meaningless conversation. I just want locations.
[00:54:03.040 --> 00:54:07.920]   I'm just looking for ratings. You do it on Google with Google Assistant. It works exactly as expected.
[00:54:07.920 --> 00:54:12.640]   You feel like nobody used this. I feel like it's gotten worse. I remember...
[00:54:12.640 --> 00:54:18.160]   It was just an app and it was great. And Apple keeps saying Siri, every time they get on stage,
[00:54:18.160 --> 00:54:23.200]   Siri's better than ever. The funny thing is that Apple is really good with AI in improving products.
[00:54:23.200 --> 00:54:28.080]   Personally, I think, and this is going to be controversial in the chat room probably,
[00:54:28.080 --> 00:54:33.920]   I personally think that the most elegant consumer electronics product that ever existed is AirPods.
[00:54:33.920 --> 00:54:37.120]   I agree. I really like these. It doesn't take it out. It stops. It knows if it's...
[00:54:37.120 --> 00:54:42.560]   They're not perfect, but they're pretty cool. So that's AI behind the scenes, knowing
[00:54:42.560 --> 00:54:48.160]   if it's in your ear or not, knowing if... And it's just... That's what Apple's really good at.
[00:54:48.160 --> 00:54:49.680]   Greg, what's your favorite AI?
[00:54:49.680 --> 00:54:56.400]   Gee. You can't use a lot of this stuff, right? You don't have Echo in the UK... Or no,
[00:54:56.400 --> 00:55:01.120]   you do have Echo now in the Google home. We have Echo. We have Siri. I have... I haven't bothered
[00:55:01.120 --> 00:55:06.400]   with the Echo because if it's Amazon technology, it'll be second rate. Anything like the Amazon
[00:55:06.400 --> 00:55:11.280]   Fire or my Kindle, my Kindle's a piece of junk. I deeply regret buying that thing. What a waste
[00:55:11.280 --> 00:55:17.360]   of time that was. So I tried using Siri and it doesn't work either. I've tried using it. I've
[00:55:17.360 --> 00:55:22.000]   got a bunch of Philip Hewlights and stuff, and I OT and all that sort of stuff. And that is just
[00:55:22.000 --> 00:55:27.440]   such an arcane experience. If I didn't have 25 years of IT behind me, I wouldn't be able to make
[00:55:27.440 --> 00:55:33.120]   those... I know. I often wonder how normal people live these days. Yeah. Well, they buy an IT device
[00:55:33.120 --> 00:55:37.200]   and it doesn't work and they get pissed and they never use it again. And by the way, what's the
[00:55:37.200 --> 00:55:44.080]   deal with Siri's British accent? That is not an accent. Do you use a British accent of Siri?
[00:55:44.080 --> 00:55:49.600]   No, I use the Australian accent. The Australian is way better. It's just enough like Australian
[00:55:49.600 --> 00:55:52.800]   that it doesn't work for British, but it's just enough British that it doesn't work for
[00:55:52.800 --> 00:55:58.240]   Australians. So maybe that's what I'm doing wrong. The British accent, it's like a combination of
[00:55:58.240 --> 00:56:04.880]   London neighborhoods or something. I gotta... Now I have to hear this. And it's really
[00:56:04.880 --> 00:56:09.040]   unpolished. Unlike the American and Australian ones. It's like jumps around in a term. Well,
[00:56:09.040 --> 00:56:13.360]   the male and the female British voices are quite different too. Yeah, the female one is the one
[00:56:13.360 --> 00:56:21.680]   that I heard that is just so off. Is it under UK? Is it under English? English, Ireland, United
[00:56:21.680 --> 00:56:27.360]   Kingdom. Okay. Change language. Okay. So you also need to be careful here with the term AI when most
[00:56:27.360 --> 00:56:31.600]   of what we're doing today is machine learning. So machine learning is where you just pass some
[00:56:31.600 --> 00:56:36.400]   statistical analysis on the machine. But doesn't machine learning fall under the umbrella of AI
[00:56:36.400 --> 00:56:41.600]   as long as I'm with... No, AI is very different to machine learning. Machine learning is you pass
[00:56:41.600 --> 00:56:46.080]   it through a certain set of mathematical equations, usually basic statistical analysis, but there's
[00:56:46.080 --> 00:56:51.360]   more advanced... It's a precursor to AI. It is an AI by itself. I always thought it was... Yeah,
[00:56:51.360 --> 00:56:56.320]   and eventually to build AI, you have to be able to build neural networks or networks that are
[00:56:56.320 --> 00:57:03.360]   where the data associations can be made dynamically and quite fluidly. And we're not there yet. There's
[00:57:03.360 --> 00:57:08.880]   no live applications around artificial intelligence. Most of what we have today is ML. So just be
[00:57:08.880 --> 00:57:15.040]   patient. There's many years to go because there's not enough AI mathematicians around to make those
[00:57:15.040 --> 00:57:20.080]   algorithms run yet. But most importantly, is there's not enough data. So if you're a research
[00:57:20.080 --> 00:57:26.000]   scientist or if you're a post grad or a master student, you can have all the theory about artificial
[00:57:26.000 --> 00:57:30.720]   intelligence. But until you've got a data set to train the AI on, you've got nothing. I think
[00:57:30.720 --> 00:57:36.160]   it's got all the data. It has to be said, though, that your point of view is in the minority. I
[00:57:36.160 --> 00:57:41.120]   think the majority of people consider artificial intelligence to be an umbrella term that includes
[00:57:41.120 --> 00:57:46.480]   lots of things, including machine learning. But I think Greg's point is well taken, especially
[00:57:46.480 --> 00:57:50.880]   in this conversation about how smart is AI. Because machine learning isn't smart at all,
[00:57:51.840 --> 00:57:59.440]   it's just big data. And if you extract the information, if you separate the two,
[00:57:59.440 --> 00:58:05.120]   then we really are in the early stages of AI. It really is barely doing anything. Let me just
[00:58:05.120 --> 00:58:10.080]   ask, this is the British series, just curious what she sounds or he, right? She's the male voice.
[00:58:10.080 --> 00:58:13.440]   The female voice is the one that I heard that's completely out of whack.
[00:58:13.440 --> 00:58:15.440]   Okay, well, let me use the female voice then.
[00:58:15.440 --> 00:58:17.600]   I'm sorry, your virtual assistant.
[00:58:18.320 --> 00:58:21.440]   Oh, she sounds quite nice. I kind of like her.
[00:58:21.440 --> 00:58:28.240]   I'm Siri, your virtual assistant. But that's a complicated question that where they string
[00:58:28.240 --> 00:58:33.680]   together, show me restaurants near me. This is the one. Okay, here's what I found.
[00:58:33.680 --> 00:58:38.560]   Oh, now it's giving me the list. Oh, it's just because it's on the camera.
[00:58:38.560 --> 00:58:41.040]   Yeah, you bastard. I hate you guts.
[00:58:41.040 --> 00:58:46.480]   Sometimes I like to save like stupid things that Siri tells me. Like, sorry, Robbie,
[00:58:46.480 --> 00:58:49.920]   I wasn't able to find any door locks this time. I asked to play a song.
[00:58:49.920 --> 00:58:53.440]   Oh, I know. But it was by the war locks. So that's, you know, to be fair.
[00:58:53.440 --> 00:59:01.840]   I think that Siri often says some of the funniest things I've ever heard.
[00:59:01.840 --> 00:59:06.960]   Unintentionally, because she's so goofy. Hey, there is a new movie coming to town.
[00:59:06.960 --> 00:59:10.320]   We were watching the trailer. I thought we'd play it again here in just a second. Ready,
[00:59:10.320 --> 00:59:14.880]   player one. Everybody loved the Ernest Cline novel. Spielberg is making the movie.
[00:59:14.880 --> 00:59:19.600]   I want to take a look at the trailer. And that can launch a conversation about virtual reality,
[00:59:19.600 --> 00:59:23.520]   what Apple is doing with augmented reality. Scoville's really gone off the deep end this
[00:59:23.520 --> 00:59:28.640]   week. He really has. Oh, what did he say this time? He says, Siri's God is, I think, his new thing.
[00:59:28.640 --> 00:59:34.880]   He's crazy. He thinks Apple's going to eat the world. Oh, he's been talking. He and I got into it
[00:59:34.880 --> 00:59:42.080]   on Facebook about he says that in September, Apple is going to kill virtual reality.
[00:59:43.040 --> 00:59:46.320]   And let's say augmented reality, but you said virtual reality.
[00:59:46.320 --> 00:59:54.720]   Well, it's going to be Pokemon. They're going to invent the category. They're not going to
[00:59:54.720 --> 00:59:58.640]   destroy it. They're going to create it. All right. Well, we'll find out in a moment
[00:59:58.640 --> 01:00:04.640]   when we return, but first a word from dinner. Dinner is, I tell you what I,
[01:00:04.640 --> 01:00:11.280]   I actually really owe a debt of gratitude to Blue Apron. I have rediscovered cooking
[01:00:11.280 --> 01:00:18.320]   with Blue Apron. It is, it is, I can't, I can't express this more vividly this.
[01:00:18.320 --> 01:00:24.240]   So it seems simple. It's the number one fresh ingredient recipe delivery service in the country.
[01:00:24.240 --> 01:00:27.840]   So you get, we get a box once a week. We choose, we go online, we go to the menus,
[01:00:27.840 --> 01:00:32.480]   we choose the three meals we're going to get. You can get it for a couple or for a family,
[01:00:32.480 --> 01:00:36.160]   a four and they have more kid-friendly ingredients. But let's just look, see. So,
[01:00:36.160 --> 01:00:41.200]   sweet and spicy beef sounds good. Crispy chicken tenders and hot honey, salmon and
[01:00:41.200 --> 01:00:45.600]   freaky salad. I don't even know what freaky is. So you pick the three, you get the box,
[01:00:45.600 --> 01:00:49.120]   everything's fresh, everything's perfect. They have really figured out how to do
[01:00:49.120 --> 01:00:55.760]   the delivery, the processing, because you never get like a old leaf or anything. But you started
[01:00:55.760 --> 01:01:00.000]   hitting the nail on the head the way you feel when you're cooking. Exactly. You feel like you're
[01:01:00.000 --> 01:01:04.640]   chef. So isn't, so I, you know, we always focus on the mechanics of this. You get the box, you get
[01:01:04.640 --> 01:01:09.840]   exactly the right thing and blah, blah, blah. But it's like you have a sous chef who's done all the
[01:01:09.840 --> 01:01:14.320]   prep work. You have exactly what you need. And then you start cooking and you're going to be able
[01:01:14.320 --> 01:01:18.320]   to do this in half an hour, 40 minutes and you're, the room starts to, you feel like a chef, the
[01:01:18.320 --> 01:01:23.680]   room starts to fill with these amazing aromas. You become extremely arrogant and you start yelling
[01:01:23.680 --> 01:01:29.920]   people. Oh, get that from here. I'll sue for you. I say, no, it is really, and by the way, we've
[01:01:29.920 --> 01:01:33.120]   learned how to make things that we make again and again, there's a salad now that we make again.
[01:01:33.120 --> 01:01:37.200]   And actually a mirror. It was like when we came over to dinner the other time at Amira,
[01:01:37.200 --> 01:01:41.600]   she was making a kale salad. It was so good. And we've made that over and over again because we saw
[01:01:41.600 --> 01:01:45.360]   her make it, we learned, and that became a part of our repertoire. And that's what happens with
[01:01:45.360 --> 01:01:50.080]   Blue Apron. These things become a part of your repertoire. They don't repeat. So you, they only
[01:01:50.080 --> 01:01:55.760]   repeat recipes at a minimum of once a year. The ingredients are perfect as every bit is good,
[01:01:55.760 --> 01:01:59.760]   if not better, than you would get if you did the shopping yourself. Actually, they do a better job
[01:01:59.760 --> 01:02:04.480]   of picking the produce and the meats and everything. And by the way, the meat's phenomenal cuts.
[01:02:04.480 --> 01:02:08.160]   Fresh, fresh delivery. It's an refrigerated box.
[01:02:08.160 --> 01:02:14.160]   On the menu, seared chicken and creamy pasta salad with summer squash and sweet peppers.
[01:02:14.160 --> 01:02:21.040]   Creamy shrimp rolls with quick pickles and sweet potato wedges. We had this the other day,
[01:02:21.040 --> 01:02:25.920]   chili butter steaks. It's chili butter. They have a chili pepper sauce. It's from Italy that
[01:02:25.920 --> 01:02:31.440]   you mix in with the butter. With Parmesan potatoes and spinach, it was a hit, a huge hit. I've
[01:02:31.440 --> 01:02:36.320]   wilted more spinach with Blue Apron than I ever had before. I've become the fan of the wilted spinach.
[01:02:36.320 --> 01:02:41.760]   They have partnerships with 150 local farms, fisheries, and ranchers across the US.
[01:02:41.760 --> 01:02:46.400]   So you're getting the best food. Their freshness guarantees promises everything arrives ready to
[01:02:46.400 --> 01:02:51.520]   cook or they'll make it right. They've sent me emails. This is a company that does customer
[01:02:51.520 --> 01:02:57.840]   service right. They've sent me emails saying you're going to get yellow tomatoes instead of red ones
[01:02:57.840 --> 01:03:02.800]   because the yellow were better. Just wanted to let you know. And it's that kind of thing.
[01:03:02.800 --> 01:03:08.480]   Look, you've got to try this. For getting a family excited about cooking, for cooking for
[01:03:08.480 --> 01:03:13.440]   your sweetheart, for date night, there's nothing better. Blue Apron, check out this week's menu.
[01:03:13.440 --> 01:03:17.200]   And by the way, you're going to get three meals free with your first purchase and free shipping
[01:03:17.200 --> 01:03:23.920]   when you go to Blue Apron.com/twit. We actually cook two and give one to our, to at least his parents,
[01:03:23.920 --> 01:03:29.200]   my in-laws. It's more than their older. Oh, yeah, easily. We feed a family three with every meal.
[01:03:29.200 --> 01:03:34.000]   But they love it. So we pick one that's going to be that they're going to love. And it's all
[01:03:34.000 --> 01:03:39.760]   ready to go and they just love it. Blue, it's a great gift. Blue Apron.com/twit or do it for yourself.
[01:03:39.760 --> 01:03:45.040]   It's a gift for you. It's a better way to cook and I promise you, I think you know, if you've been
[01:03:45.040 --> 01:03:48.720]   listening, if you listen to our shows, I've been talking about cooking a lot more lately.
[01:03:48.720 --> 01:03:54.080]   It's because of Blue Apron. Thank you, Blue Apron. We also been talking a lot about VR and AR.
[01:03:54.080 --> 01:04:01.280]   That's because of Robert Scoble. Curse you, Robert Scoble. He is all about apple going to be this
[01:04:01.280 --> 01:04:04.880]   new iPhone. It's going to be the most amazing. He's actually doing a better job than apple and
[01:04:04.880 --> 01:04:11.120]   getting me excited about the new iPhone. Yes. Yeah. And I really am super excited about AR kit
[01:04:11.120 --> 01:04:15.040]   and what that's going to bring about. We've already seen some demos. It's fantastic. Very impressive.
[01:04:15.040 --> 01:04:20.480]   What people have been able to just cobble together very quickly. So this is to me, this is what really
[01:04:20.480 --> 01:04:26.000]   gets you inspired. Comic Con was this week. And Steven Spielberg came with a trailer for his new
[01:04:26.000 --> 01:04:30.400]   movie, the one based on Ernest Klein's novel that I know all of you read because it was the Geek
[01:04:30.400 --> 01:04:36.880]   novel of the year a couple of years ago. Never run it. Ready, player one. It's a horrible nerd.
[01:04:36.880 --> 01:04:42.480]   Leave immediately. I'll tell you what. Just read this. Okay. So everyone. Okay. So this is what I know.
[01:04:42.480 --> 01:04:47.920]   You like nostalgia? Read this book. No, no, no, no, no. Read the first page. That's all. Okay.
[01:04:47.920 --> 01:04:53.440]   And then if you can put it down, put it down. You won't. Yeah. Yeah. You haven't read it from
[01:04:53.440 --> 01:04:58.240]   Antointe. Yeah, no, I love the book as well. Oh, yeah. I thought that unlike a lot of books like
[01:04:58.240 --> 01:05:02.720]   that, the sort of lose pace in the middle. Oh, no, nice to make it run all the way through. So the
[01:05:02.720 --> 01:05:08.560]   writing was just outstanding. So here is the trailer. And I have to say, I'm pretty excited about this
[01:05:08.560 --> 01:05:16.080]   from Ready Player One. And watch, Mike pointed this out to me, watch how they, how Steven Spielberg
[01:05:16.080 --> 01:05:21.200]   dramatizes the experience of VR. Because that's a hard thing to do. Yeah. In our day, it was,
[01:05:21.200 --> 01:05:26.960]   they had to dramatize smoking marijuana, right? Like, right? Or the acid trip. And they would make
[01:05:26.960 --> 01:05:34.000]   the camera go, ooh, I did a pretty good job on lawn mower. Actually, that was the VR movie of
[01:05:34.000 --> 01:05:41.120]   all time. Well, compare this to lawn mower. Man, sure. I live here in Columbus, Ohio. No lawns
[01:05:41.120 --> 01:05:46.480]   in the future, though. No. In 2045, it's still ranked the fastest growing city on earth. But
[01:05:46.480 --> 01:05:53.360]   sure doesn't seem like Columbus, Ohio. They called our generation the missing millions.
[01:05:54.880 --> 01:05:59.920]   Missing not because we went anywhere. There's nowhere left to go.
[01:05:59.920 --> 01:06:10.080]   Nowhere. Except the Oasis. This is putting on the VR goggles. In 2035, we're gonna still have to wear
[01:06:10.080 --> 01:06:15.600]   those crap visors. I'm disappointed. Everyone's gonna have to pack the air on the mattress. So now
[01:06:15.600 --> 01:06:20.720]   he's in VR. People are flying around. That's actually from the book. Dancing. Is he dancing?
[01:06:22.080 --> 01:06:27.520]   This is Spielberg's idea. So there are 80s things like that was Iron Giant. There are 80s
[01:06:27.520 --> 01:06:31.680]   eye-cons and references as there were in the book, of course.
[01:06:31.680 --> 01:06:43.360]   The best VR game, though, is coming up. They're gonna do this driving game where it shows the
[01:06:43.360 --> 01:06:49.760]   guys walking into, you know, a kind of weird minimalist environment. It's an organized,
[01:06:49.760 --> 01:06:54.960]   very organized team video game playing in VR. And then it shows their VR experience of this,
[01:06:54.960 --> 01:06:59.840]   which looks like, you know, fast and furious. And their robots in VR. Yeah. And there's apparently
[01:06:59.840 --> 01:07:06.000]   there's Russian VR as well. Thank God. You know, any more prog rock in VR? I mean, come on.
[01:07:06.000 --> 01:07:11.440]   If anything should be VR as prog rock. And with some sticks.
[01:07:16.320 --> 01:07:20.720]   Unfortunately, I think every movie now has to have all this car chase crash.
[01:07:20.720 --> 01:07:28.000]   It explodes. What if that turns into video game tie-ins? Yeah. That's a lot of scenes in mod movies
[01:07:28.000 --> 01:07:33.200]   that get lifted. So he rips off the visor goes, "Hmm, it's too much." I've been doing this all the
[01:07:33.200 --> 01:07:39.520]   time, but that time was too much. Is this the VR game of the year? It's not till next year,
[01:07:39.520 --> 01:07:44.480]   so don't get too excited. It's the perfect novel for a movie because you can have live action,
[01:07:44.480 --> 01:07:50.080]   which is very interesting. You have CGI, which is necessary based on the subject matter.
[01:07:50.080 --> 01:07:56.400]   And it's just a fascinating story with lots of nostalgia. So the public generally can, you know,
[01:07:56.400 --> 01:08:00.800]   the kids will love it and their parents will love it because of the nostalgia. It's just,
[01:08:00.800 --> 01:08:04.880]   it's a perfect, even when I was reading it, I'm like, oh, somebody's got to make this movie.
[01:08:04.880 --> 01:08:07.840]   Everyone's while you read a novel that was clearly written with the idea of,
[01:08:07.840 --> 01:08:09.840]   you know, we're going to option this sucker. Yeah.
[01:08:11.680 --> 01:08:15.200]   Maybe more now than ever. So can that whole thing's going to be,
[01:08:15.200 --> 01:08:18.720]   actors are going to be in front of a green screen? They're not going to have to go out on site.
[01:08:18.720 --> 01:08:24.160]   They're not going to have to come to work. They'll digitize them and then say, "Thanks very much.
[01:08:24.160 --> 01:08:28.400]   We'll send you the royalties." Yeah. But I mean, already they're doing things like,
[01:08:28.400 --> 01:08:31.920]   I would imagine that almost all of that movie, if not all of it, was done in front of a green
[01:08:31.920 --> 01:08:34.880]   screen, everything else will CGI it, which makes them incredibly cheap to make.
[01:08:34.880 --> 01:08:39.920]   Not only cheap to make, but very accurate. You know exactly how much money you're going to spend
[01:08:39.920 --> 01:08:45.440]   to make a movie. And this is one of the impacts of technology is that in movies,
[01:08:45.440 --> 01:08:48.320]   we never used to know how much it would take. You know, you go out on site and
[01:08:48.320 --> 01:08:52.960]   what is it that there's a story in the Matrix, they could only get the studios to give them 20
[01:08:52.960 --> 01:08:58.640]   million to make the whole of the Matrix. So what they did was spent 20 million making the first 15
[01:08:58.640 --> 01:09:02.720]   minutes and then went back to the studio and said, "Here's the first 15 minutes. We want the rest
[01:09:02.720 --> 01:09:09.680]   of the 200 million." That's a good trick. And so I think we're seeing a transition here as if
[01:09:09.680 --> 01:09:16.240]   you can put, if you can turn video production or movie production into a factory production line,
[01:09:16.240 --> 01:09:19.520]   you get control of your costs and you know what things start to look at. And that's what we're
[01:09:19.520 --> 01:09:24.480]   seeing with Netflix and Amazon Video. There's a transition there where a lot of the CGI is,
[01:09:24.480 --> 01:09:28.880]   you know, definitely able to be produced at a much lower cost than going out with a camera and
[01:09:28.880 --> 01:09:34.160]   taking shots of someone. I wanted to show that trailer because that was the folly of the first,
[01:09:34.160 --> 01:09:38.880]   you know, the prequel, the Star Wars prequels. Like, well, we're just going to make everything
[01:09:38.880 --> 01:09:44.480]   CGI. Yeah. And everything CGI is fine. So let's. But you have to have, you know, you need to have a
[01:09:44.480 --> 01:09:49.360]   good, you know, you have Steven Spielberg attached to this, you have a good actors attached to it
[01:09:49.360 --> 01:09:53.520]   without that without a story, without a good director, without good actors, you're going to end up
[01:09:53.520 --> 01:09:57.360]   with the prequels. One of the strangest things about Rogue One was that one of the characters,
[01:09:57.360 --> 01:10:01.440]   presumably the actor died or something, but they, one of the characters was just CGI. Two of them
[01:10:01.440 --> 01:10:07.840]   were two of them. Okay. And and and. It was poorly done since CGI, but if you didn't know ahead of
[01:10:07.840 --> 01:10:11.680]   time, you might have missed it. But once you knew and looked at Emperor Tarkin, or I think it was
[01:10:11.680 --> 01:10:18.000]   Tarkin, it was like, it was surreal. And then and then Carrie Fisher's part was CGI because she's
[01:10:18.000 --> 01:10:22.560]   obviously was a little bit far in the movie. But but also, sorry, I don't mean to ruin it.
[01:10:22.560 --> 01:10:27.760]   Darn it. I was gonna. So but but the other the other weird CGI thing was Tron. That was one of the
[01:10:27.760 --> 01:10:34.480]   things that ruined Tron was they showed the young actor was Jeff Bridges. And that just did not work.
[01:10:34.480 --> 01:10:38.800]   It's the uncanny valley. Yeah, you're still need people. You still they still look like like,
[01:10:38.800 --> 01:10:44.400]   Oh, God. Oh, so the other reason I showed that trailer is I want to show the glossy Hollywood vision
[01:10:44.400 --> 01:10:50.160]   of AR and VR compared to well, this is an AR kit demo that was posted on YouTube. Everybody's
[01:10:50.160 --> 01:10:58.000]   very excited. Not exactly ready for the very excited. I mean, how cool is that? Well, this is
[01:10:58.000 --> 01:11:01.360]   real, right? This is you're actually going to be able to do this presumably when the new iPhone
[01:11:01.360 --> 01:11:07.360]   comes out. One of the interesting things is she's using an HTC Vive with AR kit. And in fact, it's
[01:11:07.360 --> 01:11:15.120]   that painting in 3D space is amazing that whatever that thing is character doing the drawing is not
[01:11:15.120 --> 01:11:22.160]   amazing. It's a blob. It's a blob. It's a blob. It's not as creepy as Jeff Bridges. That's by the way,
[01:11:22.160 --> 01:11:26.000]   I'm sure why it looks like that. If they'd made it look like her, it would have been creepy as hell.
[01:11:26.000 --> 01:11:32.000]   Yeah. So I want to show a video. I think Anthony has it. This is a this is an experiential
[01:11:32.000 --> 01:11:40.640]   marketing gimmick video made by somebody in I think, Peru. And they're handing out
[01:11:40.640 --> 01:11:45.120]   in trap people stuck in traffic. So they're getting goggles. Right. Yeah. And they're giving
[01:11:45.120 --> 01:11:49.200]   the app and all that kind of stuff. And this is this is a representation of what they see. Now,
[01:11:49.200 --> 01:11:53.200]   this is how it's going to transform advertising. Oh my God. She's driving down a normal highway,
[01:11:53.200 --> 01:12:00.000]   but there's a giant hose. So that's AR. This is as there's a of inflatable flamingo.
[01:12:00.000 --> 01:12:04.880]   Placement of virtual objects in real space. And these and you're saying these will be marketing
[01:12:04.880 --> 01:12:11.760]   objects. This will be a soccer. This is this is a this is a pretty glossy version of what AR kit
[01:12:11.760 --> 01:12:16.640]   is going to bring us in like next year. We're going to see virtual stuff. It's not going to be that
[01:12:16.640 --> 01:12:22.400]   good, but it's going to be good. And things will be placed in space that will be mind blown.
[01:12:22.400 --> 01:12:28.880]   Well, an Apple will you know, just goables credit Apple will have a huge lead in this. I mean,
[01:12:28.880 --> 01:12:35.200]   yeah, AR kit is clearly something that developers like and it's easy enough to use. It'll be like
[01:12:35.200 --> 01:12:38.880]   everything else. They'll have they'll dominate with their app store. And these are apps that
[01:12:38.880 --> 01:12:43.440]   we're going to be downloading. But Apple's app store didn't end all other app stores. It
[01:12:43.440 --> 01:12:47.600]   basically, you know, they they help create the market. And then the play store is even bigger
[01:12:47.600 --> 01:12:52.080]   than the app store. And there's a thriving market on Android and even other platforms.
[01:12:52.080 --> 01:12:56.720]   So his his point is it's going to kill everything else that's VR. That is not going to happen.
[01:12:56.720 --> 01:13:05.200]   Now they're going to they're going to push augmented reality directly into the mainstream very rapidly,
[01:13:05.200 --> 01:13:10.000]   which I think will be great for other companies doing AR. And I think that VR is going to be kind
[01:13:10.000 --> 01:13:16.400]   of like a kind of a back burner thing. It's sort of like mobile games versus console games.
[01:13:16.400 --> 01:13:20.560]   Console games are amazing, but a tiny minority of people actually play console games.
[01:13:20.560 --> 01:13:25.760]   They are going to be the Google Glass. Nobody wants to shove a thing on the front of their face.
[01:13:25.760 --> 01:13:31.040]   And the same reason that people didn't want. Well, it's only a matter of time before all that
[01:13:31.040 --> 01:13:36.560]   stuff on your face is undetectable. And yeah, and I think Apple will get into the market at that
[01:13:36.560 --> 01:13:41.200]   point. But but all this work that's being done in AR for looking through your phone and tablet
[01:13:41.200 --> 01:13:46.240]   will be applicable to the world of glasses. This is the to me, the most exciting thing that people
[01:13:46.240 --> 01:13:51.200]   under appreciate, which is that all this development work, I mean, it's a lot of work to create these
[01:13:51.200 --> 01:13:56.640]   virtual experiences and so on. All you need is glasses to project what's happening on the phone
[01:13:56.640 --> 01:14:00.880]   onto those glasses. And you can take your pick. You can look at on the phone. You can look at
[01:14:00.880 --> 01:14:07.360]   glasses. Yeah, we use the way where we're years and years away from that. Transmitting to do
[01:14:07.360 --> 01:14:14.320]   a sufficient virtual reality you need about. I think it's 30 or 40. Sorry, I think it's sort of
[01:14:14.320 --> 01:14:20.080]   like five to 10 gigabits per second and streaming that over a Bluetooth style connection, our personal
[01:14:20.080 --> 01:14:24.720]   area network isn't going to happen any time in the next one. Again, I don't I don't even want to
[01:14:24.720 --> 01:14:28.160]   talk about virtual reality because that that's like not that's gonna be nothing compared to
[01:14:28.160 --> 01:14:32.880]   augmented reality. And there are going to be a million solutions for this already. There's a $99
[01:14:32.880 --> 01:14:36.560]   set of glasses that are ridiculous. You're not going to wear them in public and all that kind of
[01:14:36.560 --> 01:14:43.840]   stuff. But what they do is they'd simply reflect virtual objects onto a curved piece of plastic that
[01:14:43.840 --> 01:14:50.160]   is part of the goggles. What are those called? But it's $99 to have an augmented reality experience.
[01:14:50.160 --> 01:14:54.480]   Whether they'll be rejected like Google Glass was. So Google Glass was definitely seen as
[01:14:54.480 --> 01:15:01.520]   privacy intrusion. It was uncool. And I can remember being in bars in San Francisco, people
[01:15:01.520 --> 01:15:05.520]   be walking around with Google Glasses and we'd they'd be being verbally abused. Well, that was
[01:15:05.520 --> 01:15:09.360]   only in San Francisco. There was literally no other place on earth where that took place.
[01:15:09.360 --> 01:15:15.120]   Or own Google Glass. No one else is wearing Google Glasses.
[01:15:15.120 --> 01:15:19.360]   This is a Columbus, Ohio, a lot of Google Glass floating. No, no, you would have abused it.
[01:15:19.360 --> 01:15:23.600]   We Greg, I think we had actually had this argument last time we did this show together,
[01:15:23.600 --> 01:15:30.160]   which is that this this idea, the Google Glass tried and failed. And now they've come out of
[01:15:30.160 --> 01:15:35.760]   nowhere with this new product. That's a false narrative put out by the tech press, which felt
[01:15:37.280 --> 01:15:40.800]   journalists don't make enough money to spend $1,500. They felt excluded through price.
[01:15:40.800 --> 01:15:44.560]   And so they went about. I know all the people I know who had Google Glass were journalists.
[01:15:44.560 --> 01:15:51.360]   Well, I spent money. I worked at Wired at the time and like four people on staff spent their
[01:15:51.360 --> 01:15:55.760]   own money. But they were ultimately they were unimpressed. But they wore them and then they're
[01:15:55.760 --> 01:15:59.840]   like kind of and they've given to me and I'm like, this is this is not comfortable. So here's what
[01:15:59.840 --> 01:16:04.160]   I could here's what actually happened. You like it better when you spend $1,500 for it. I want to
[01:16:04.160 --> 01:16:09.280]   point out. Oh, you have to like it. You at least have to tell your spouse you like it. But anyway,
[01:16:09.280 --> 01:16:13.680]   here's what actually happened. Google came out with an extraordinarily
[01:16:13.680 --> 01:16:18.560]   innovative beta program. They said not only want to try the hardware,
[01:16:18.560 --> 01:16:22.800]   we also want to pay a fortune for it. And we want you to tell us how you're using it. We have no
[01:16:22.800 --> 01:16:27.360]   idea how you're supposed to use this thing. They did it for a while. The one thing that made the
[01:16:27.360 --> 01:16:30.800]   narrative weird is that the very end they said, you know, okay, we're opening up the public. The
[01:16:30.800 --> 01:16:37.040]   public said, I don't think so. And then they admitted weird is when the initial video showed
[01:16:37.040 --> 01:16:42.240]   actual augmented reality, the initial Google last video was actual augmented reality. And I spoke
[01:16:42.240 --> 01:16:46.880]   to augmented reality researchers and they're like, that's not possible. And then it came out and it
[01:16:46.880 --> 01:16:51.520]   wasn't actual. And it was a screen in front of your face. Yeah, it was a screen that you had to do
[01:16:51.520 --> 01:16:57.840]   this. But but there actually was augmented reality. For example, the magic lens app, which started
[01:16:57.840 --> 01:17:03.520]   out on the phone, which trans translates signs into a new language right before your eyes using
[01:17:03.520 --> 01:17:07.840]   the same typeface. That was real augmented reality. But anyway, back to back to my point.
[01:17:07.840 --> 01:17:12.160]   Somebody saying something interesting, though, gray Raven in the chatroom. 10 years ago, people
[01:17:12.160 --> 01:17:16.960]   staring at six inch phones would have looked really stupid. Right? They still do. I remember when I
[01:17:16.960 --> 01:17:22.080]   first experienced this, I was in Europe and saw people walking around looking at phones that
[01:17:22.080 --> 01:17:27.760]   that is crazy. But now it's accepted. It's everywhere. It's universal so much so that you don't notice
[01:17:27.760 --> 01:17:33.120]   it. Well, what will we all get used to wearing some sort of augmented reality? I don't think we'll
[01:17:33.120 --> 01:17:36.960]   have to. I think they look nice. They have to look nice and they have to work. Right. And I think
[01:17:36.960 --> 01:17:42.320]   Google's folly was they created this over problem. They almost promised they under delivered. What
[01:17:42.320 --> 01:17:47.200]   did they promise? They promised real augmented reality. They showed a video and they're like,
[01:17:47.200 --> 01:17:51.920]   this is Google Glass. It's coming. They had a gut seed up demo exactly like every product that
[01:17:51.920 --> 01:17:56.240]   has ever existed. It was unrealistically that video. It wasn't even like slightly unrealized.
[01:17:56.240 --> 01:18:00.400]   It was incredibly unrealistic compared to the actual product they gave out as beta.
[01:18:00.400 --> 01:18:05.920]   And then they looked out of a point. I think Google's mistake was, as you said, over promising,
[01:18:05.920 --> 01:18:10.400]   over delivering. But it was also a step too far at a social level. People wearing them down the
[01:18:10.400 --> 01:18:15.520]   street could take a photo at a time when people, there was no Instagram, there was no Snapchat.
[01:18:15.520 --> 01:18:19.280]   This is my point is, when the snap goggles came along and that was fine.
[01:18:19.280 --> 01:18:24.320]   This was the Google Glass. This is my point. This is this time around. It could work.
[01:18:24.320 --> 01:18:28.640]   And I have to say Roberto has a little bit of a point here. Like, oh, here the Google Glass is
[01:18:28.640 --> 01:18:33.440]   coming. This is awesome. He's going to put on his glass and oh, look. Nobody who wears flip-lops
[01:18:33.440 --> 01:18:37.280]   could afford it either. Well, you could only afford flip-lops after buying exactly. So,
[01:18:37.280 --> 01:18:42.880]   this reminds me of Microsoft's HoloLens demos, which always imply a full field of you.
[01:18:42.880 --> 01:18:48.000]   You're interacting with the world around you. This is what we want. This isn't even something
[01:18:48.000 --> 01:18:54.000]   Google Glass could have done. But I guess what you're saying is once you understand that what
[01:18:54.000 --> 01:18:58.960]   you're seeing isn't in front of you, but is in fact over your right eyebrow, it's not so far off.
[01:18:58.960 --> 01:19:04.640]   Epson had their their their industrial glasses, basically. Well, Google glasses,
[01:19:04.640 --> 01:19:08.880]   that was the point is that Stephen Levy and Wired wrote a long article about how Google Glass is
[01:19:08.880 --> 01:19:12.240]   now widely used in in in manufacturing. They make their planes.
[01:19:12.240 --> 01:19:17.840]   It makes their planes. But my point is that all this stuff we're talking about happened under
[01:19:17.840 --> 01:19:22.560]   Google X. It was a research project. And then at the end of it, they said, okay, now we're going to
[01:19:22.560 --> 01:19:27.520]   productize it. And everybody said, oh, it failed. No, they was a research project. They knew good
[01:19:27.520 --> 01:19:30.800]   productize it. Now the product is out and it's amazing. And they know what it's called.
[01:19:30.800 --> 01:19:36.080]   Well, it is a little object lesson in not doing this kind of over promising years before you can
[01:19:36.080 --> 01:19:40.560]   actually deliver something like that. I mean, I wore some glasses the other day for for bike
[01:19:40.560 --> 01:19:45.520]   riding like a week or so ago. And they wasn't exactly this, but it was pretty close. I'm like,
[01:19:45.520 --> 01:19:49.520]   okay, this heads up display. Yeah, it heads up display. They had I mean, and the company who
[01:19:49.520 --> 01:19:56.480]   built it actually builds the the the heads up displays for jets for people wearing those giant.
[01:19:56.480 --> 01:20:01.600]   Yeah, so I'm like, okay, now, okay, I can see how this is working. This is great. This makes this
[01:20:01.600 --> 01:20:07.600]   makes sense. I like I like the idea. And the niche sort of market where it's like for for for
[01:20:07.600 --> 01:20:13.200]   manufacturing, for industrial uses, that's where they are started and where it's going to be for a
[01:20:13.200 --> 01:20:17.200]   while. But now they're like, okay, well, we're doing this for bicycles. And you can see, you know,
[01:20:17.200 --> 01:20:20.720]   there's there's sort of a little bit for motorcycles. There's sort of a little bit for here and there.
[01:20:20.720 --> 01:20:25.600]   Scuba. And I think augmented before AR before VR, VR, I don't even want to talk about VR, but
[01:20:25.600 --> 01:20:32.640]   AR is going to be huge. But you still have to get over that initial. We have to make something
[01:20:32.640 --> 01:20:36.640]   that people want to wear. Right. And that's when it comes to that. I've been predicting this for a
[01:20:36.640 --> 01:20:41.920]   while, but within like six or seven years, eight years, when you go to get your glasses, they'll
[01:20:41.920 --> 01:20:44.960]   say, well, you want the smart ones or the dumb ones, you know, when you get pick your frames,
[01:20:44.960 --> 01:20:48.560]   you're like, well, okay, give me the smart ones. Okay. So here's a series of options. Okay,
[01:20:48.560 --> 01:20:52.880]   I'll take those extra $200. And then you just you'll get little screens kind of like Google
[01:20:52.880 --> 01:20:56.320]   Glass, but there won't be the big boom that comes out in front of your face. They'll be
[01:20:56.320 --> 01:21:02.800]   practically indistinguishable from ordinary glasses. There's a company that a startup called
[01:21:02.800 --> 01:21:08.400]   what are they called a view glasses VUE glasses in San Francisco. They don't do screens. They don't
[01:21:08.400 --> 01:21:12.800]   have a camera, but it's little blinking lights and and and the most important part of all,
[01:21:12.800 --> 01:21:17.360]   which I think is really underrepresented is the audio part of augmented reality.
[01:21:17.360 --> 01:21:23.440]   Bone conduction is the killer app for smart glasses, I think. Yeah. Because you want you want to be
[01:21:23.440 --> 01:21:26.720]   talking your virtual assistant all day. There's another technology that's never worked very well
[01:21:26.720 --> 01:21:31.200]   for me. Does it work for you? Can you actually that's not a glass worked kind of it did use bone
[01:21:31.200 --> 01:21:36.480]   conduction. Yeah. It only was on one side whereas VUE glasses are on both sides. But the point is
[01:21:36.480 --> 01:21:41.680]   that we're going to be wanted. So here's a problem nobody worries about. We want to talk to our
[01:21:41.680 --> 01:21:47.360]   virtual assistant all day, our future selves five years from now. What technology gives us
[01:21:47.360 --> 01:21:53.840]   all day, you know, battery life and all that kind of stuff, not, you know, AirPods lies what two hours
[01:21:53.840 --> 01:21:58.960]   or something like that. Your phone does though. But that's not, that's not, you know, that's not
[01:21:58.960 --> 01:22:03.440]   augmented is not on your I think that's the why Apple's doing everything on the phone. It really
[01:22:03.440 --> 01:22:08.960]   seems apparent. Let me as someone who builds infrastructure for as a profession. Let me tell
[01:22:08.960 --> 01:22:12.800]   you that there's about as much chance of this happening as a pet getting in at not burning in
[01:22:12.800 --> 01:22:18.640]   health. What happened? Because it's just not about 5G. Would that make it possible? No, no, no.
[01:22:18.640 --> 01:22:24.400]   You're talking like 5G is yes in theory the 5G will give you 10 gigabits per second, but it's
[01:22:24.400 --> 01:22:28.880]   going to be shared with everybody. And if you want to be able to save battery life, you have to
[01:22:28.880 --> 01:22:34.240]   shift a very low bandwidth connections as soon as you're low bandwidth. No AR. It's probably the case
[01:22:34.240 --> 01:22:38.960]   that what was the what was the increase in data use when the iPhone came out? It was 10 or 20
[01:22:38.960 --> 01:22:43.920]   fold is probably the case that before the iPhone came out AT&T's engineer said, well, there's no way
[01:22:43.920 --> 01:22:48.720]   we could we could support that. And they're sorry, but they did they made a work of it, but
[01:22:48.720 --> 01:22:56.880]   it's took years. So if these glasses need 5G and 5G is at best five years away, maybe longer, it's a
[01:22:56.880 --> 01:23:02.240]   little hard to tell. And even if it rolls out, it's not going to roll out everywhere in just a few
[01:23:02.240 --> 01:23:07.040]   years, it's going to be a five to 20 year rollout. So now you'll be in the situation. It'll be back
[01:23:07.040 --> 01:23:12.400]   like when the iPhones first came out. And you could use your iPhone on edge everywhere in the
[01:23:12.400 --> 01:23:18.080]   country, but 3G only in a couple of capital cities. And that's going to hold your adoption back of
[01:23:18.080 --> 01:23:22.880]   these sorts of things. But Greg, there's not there's not one thing that is going to come out at some
[01:23:22.880 --> 01:23:28.080]   point in the future. I'm in a reality in a in a rudimentary form, a rudimentary form is already
[01:23:28.080 --> 01:23:34.160]   here. Yeah, and it's already breaking. Did you Pokemon Go Fest was Saturday in Grant Park in Chicago?
[01:23:34.160 --> 01:23:37.760]   And people were fear. That's why I was avoiding Chicago. People were furious.
[01:23:37.760 --> 01:23:46.400]   20,000. Yeah, it's 20,000 Pokemon Go players gathered. And there the internet didn't work.
[01:23:46.400 --> 01:23:50.720]   World largest gathering of virgins. Yeah, people when Niantic's president, John Hankey,
[01:23:50.720 --> 01:23:56.000]   took the stage. Hey, I like Pokemon go. It took the stage. He was greeted by an audience a few
[01:23:56.000 --> 01:24:02.080]   thousand deep many of them chanting fix your game. We can't play some more. Greg, this is from Tech
[01:24:02.080 --> 01:24:07.120]   Crunch some more aggressive attendees approach the stage to personally shout their discontent.
[01:24:07.120 --> 01:24:13.120]   And Niantic says we're going to give you refunds in the form of $100 of pokey coins.
[01:24:13.120 --> 01:24:18.400]   That has no idea about infrastructure. The Niantic is it.
[01:24:18.400 --> 01:24:23.920]   Never worked out how to scale up their servers. Their game was successful in the had outages.
[01:24:23.920 --> 01:24:28.720]   It's true. Is this a surprise? Remember when Pokemon go launched a year ago? What short memories
[01:24:28.720 --> 01:24:34.880]   do we have? I mean, these guys must be idiots. I mean, seriously, you know, when you have a
[01:24:34.880 --> 01:24:40.160]   conference with 30 to 50,000 people at it, the first thing that you do is you go and get the
[01:24:40.160 --> 01:24:45.840]   mobile phone companies to bring the mobile towers to your event. Here's John Hankey coming on stage.
[01:24:45.840 --> 01:24:52.320]   Even work it out. Oh, it's hard. Great job. Oh, man, this got to hurt. You're right, though.
[01:24:52.320 --> 01:24:56.320]   You're exactly right, Greg. If it was ever a company to expect this to happen to you.
[01:24:56.320 --> 01:25:01.200]   Hey, everybody. Welcome to technology companies. They should realize that if you put 30,000 people
[01:25:01.200 --> 01:25:05.600]   in a park, you're going to overload the base stations that surround the park. Music festivals
[01:25:05.600 --> 01:25:10.160]   they travel with towers. Well, you know, when Renee Richie was at this, we'll actually talk to
[01:25:10.160 --> 01:25:14.880]   him on Tuesday and find out what it really looks like. But he said, you know, it said that in
[01:25:14.880 --> 01:25:18.960]   Grant Park, you're lucky if you get one bar. I don't know how they're going to handle tens of
[01:25:18.960 --> 01:25:24.320]   thousands of and they just didn't and they did anything. Well, I thought I said at the time,
[01:25:24.320 --> 01:25:28.880]   they'll probably have trucks coming out. Just as you said, Greg, that the T-Mobile and Sprint and
[01:25:28.880 --> 01:25:33.920]   Verizon ATT will probably have seen them do it at South by Southwest, bring out cell tower trucks.
[01:25:33.920 --> 01:25:36.800]   They'll only come if you talk to them. You book them. You have to book them. You know, there's
[01:25:36.800 --> 01:25:41.920]   some people who have to pay them. You know, I've worked. I did a we did a podcast with the people
[01:25:41.920 --> 01:25:46.880]   from Live Nation, you know, the people who run the concerts festivals. Yeah. And I was talking to
[01:25:46.880 --> 01:25:51.520]   them about the infrastructure build out. They spend the entire year. So for six months of the
[01:25:51.520 --> 01:25:57.120]   conferences run, they use the tech and the next six months, they spend rebuilding it and reworking
[01:25:57.120 --> 01:26:01.840]   the kit and the fiber optic cables and everything so that they can actually run a data network at
[01:26:01.840 --> 01:26:07.200]   these events. It's a serious. It's just I've just got this vision of Niantic being like a
[01:26:07.200 --> 01:26:10.320]   bunch of kids going, it's all in the cloud. Don't worry about it. We don't need to know anything
[01:26:10.320 --> 01:26:15.120]   about infrastructure. No, it's true. Blah, blah, blah. AWS will take care of it. Boom. Done.
[01:26:15.120 --> 01:26:20.640]   Done. You got you want to make people mad, get them to come to Chicago with a promise of rare
[01:26:20.640 --> 01:26:25.840]   Pokemon. There's people there who have everything but, you know, a you known or a Heracross and they
[01:26:25.840 --> 01:26:31.440]   went and then the worst thing happened, which is it pops up that Pokemon on the screen. You're
[01:26:31.440 --> 01:26:39.440]   ready to catch it and the thing goes, I mean, that's how you make people really as dumb as it
[01:26:39.440 --> 01:26:42.800]   sounds. That's how you make people really, really. It's like going to a music festival, not being
[01:26:42.800 --> 01:26:47.520]   any music. Hey, I got to tell you though, things can happen. We were going to a live nation concert.
[01:26:47.520 --> 01:26:52.240]   At least an eye on Thursday. We had tickets to see matchbox 20 and the counting crows. A fine.
[01:26:52.240 --> 01:26:57.920]   A fine middle of the road. I'm going to be a good guest and not saying.
[01:26:57.920 --> 01:27:04.880]   Good middle of the hey, I'm 60 years old. The fact I'm going to see a rock band of any kind is
[01:27:04.880 --> 01:27:09.680]   impressive, right? Well, I mean, either of those bands rock, rock ish, rock light.
[01:27:10.400 --> 01:27:13.920]   And everybody's talking anyway, I don't like being told what to do. I'm not going to put my hands
[01:27:13.920 --> 01:27:18.080]   in the air. We're practically waving like I just don't care. You're not the boss. I do care.
[01:27:18.080 --> 01:27:23.120]   You're not my dad. Well, you'll be glad to know we practically there. We're walking down the
[01:27:23.120 --> 01:27:27.840]   street. Some lady drives by going, the cats are big dance off. And we went what? And we.
[01:27:27.840 --> 01:27:34.160]   It turns out the Shoreline amphitheater, which is down in next right literally next door to Google
[01:27:34.160 --> 01:27:40.080]   in Mountain View is built on landfill. And it turns out the ramp that would allow them to offload
[01:27:40.080 --> 01:27:46.400]   their gear onto the stage had crumbled. And they couldn't get on the stage. So they canceled the
[01:27:46.400 --> 01:27:52.400]   concert. Wow. Entirely. And thank you, live nation. I think we, Lisa had a tweet at them before she
[01:27:52.400 --> 01:27:56.880]   could get a refund. I, I, man, as you say, we missed nothing because that would have been,
[01:27:56.880 --> 01:28:01.040]   no, if you enjoy the music, I'm not, I'm trying not to be a jerk anymore about music.
[01:28:01.040 --> 01:28:06.960]   Wait, wait, you don't have a matchbox 20 tribute band? I do not have a matchbox. I'm shocked. You
[01:28:06.960 --> 01:28:13.680]   do, you do who? But you prince? We have a, I had a, I had a prince band. I had a Bowie band.
[01:28:13.680 --> 01:28:21.360]   I currently have a divo band. Oh, divo. Divo. That's a band I would go see. And in need of a drummer,
[01:28:21.360 --> 01:28:27.120]   which is tough because yeah, yeah. And then I have like, why is it hard to get a drummer?
[01:28:27.120 --> 01:28:32.400]   Well, it's hard to find drummers who, okay, you have to have three things. Find out a, they have
[01:28:32.400 --> 01:28:36.880]   to say they're a drummer, be the actual, actual, be able to keep time. Keep time is a time.
[01:28:36.880 --> 01:28:41.280]   It's a lot of people can like to do, to do, to do that. They can do that, but they can't keep
[01:28:41.280 --> 01:28:46.480]   you. I can do a paradiddle, but you get out. And then own drums. Oh, so you have to have all three
[01:28:46.480 --> 01:28:51.920]   and all of that. It's like the Holy Trinity. And the keeping time part is extremely crucial for
[01:28:51.920 --> 01:28:58.080]   a divo band. Yeah, it's very mechanical. It is very, yes. And the, you're dealing with like seven,
[01:28:58.080 --> 01:29:02.720]   eight times and five, eight time and then three, four time all in the same song. So you have to be
[01:29:02.720 --> 01:29:07.120]   able to have, you know, I think you need a matchbox 20 tribute band. No, I don't think I do.
[01:29:07.120 --> 01:29:11.600]   Like the general band, we just sort of cover everything we have like, we do like LCD sound
[01:29:11.600 --> 01:29:16.880]   system and like 90, like 2000s. We did 90 show told me should hire Dave Grohl, but he won't do it.
[01:29:16.880 --> 01:29:22.160]   He won't call me back. I swear to God. He was perfect. Yeah, he knows his drums.
[01:29:23.680 --> 01:29:29.040]   He has probably has a drum kit. I'm sure he is like, yeah, wasn't his daughter played the drums at
[01:29:29.040 --> 01:29:33.040]   a recent event? He brought her out. She's like nine years old. She baled on the drums.
[01:29:33.040 --> 01:29:37.680]   Nice. Yeah. We're having Dave Grohl's your dad. Yeah. As your, as your drum teacher, it's got to be
[01:29:37.680 --> 01:29:41.440]   amazing. I think it's hard. Because what are you going to do? Become a drummer?
[01:29:41.440 --> 01:29:46.720]   They're going to be a accountant. She's not always good work. She's not as good as Dave Grohl.
[01:29:46.720 --> 01:29:51.520]   Yeah. No, there are there are drummers who are in multiple bands in other cities who get paid
[01:29:51.520 --> 01:29:56.480]   to show up for practice because finding a drummer is difficult. So I'm going to ignore the fact that
[01:29:56.480 --> 01:30:00.720]   you've now dised my musical tastes and call me a nerd for liking Pokemon Go. I'm just going to
[01:30:00.720 --> 01:30:05.040]   ignore it. I didn't say that. That was him. Oh, sorry. That was me. I just do. I didn't call you. Greg,
[01:30:05.040 --> 01:30:11.200]   is there anything you'd like to say right now before we move on? No. I just think the hard one.
[01:30:11.200 --> 01:30:16.800]   This idea of drummers and why don't you just use a drum machine for goodness. No, just use
[01:30:16.800 --> 01:30:22.560]   automating. They're very rhythm. And secondly, if you're going to produce, if you're going to be
[01:30:22.560 --> 01:30:28.800]   into weird music, expect to live a weird life. He does. I do live a weird life. He totally lives
[01:30:28.800 --> 01:30:33.360]   a weird life. No, no, I own drum machines. I have a couple of drum machines. It's not the same as
[01:30:33.360 --> 01:30:38.480]   an excellent drummer. Let's just do this. This is why AI is not going to be a great.
[01:30:38.480 --> 01:30:42.160]   I suspect the drum machine be a lot more practical right now. Because it's always
[01:30:42.160 --> 01:30:48.320]   quantizing stuff. I want to quantize. You lose the swing of a drum. Yeah, you can't. It's
[01:30:48.320 --> 01:30:54.640]   inflexible. Yeah. And you can kind of like kind of cheat it, but I'm not very good at this. Let's do
[01:30:54.640 --> 01:31:07.440]   some more. I got it. Leo, you're hired. I can do this. Let's have a little snare.
[01:31:11.280 --> 01:31:20.400]   Oh, we not been. We are demo. I'm sure they that's seven eight. That's seven eight to know
[01:31:20.400 --> 01:31:28.240]   these are drummer. Really? Yeah, that's seven eight. Hey, let's talk about audible.com. I am an
[01:31:28.240 --> 01:31:35.120]   audible lover. Audible is, of course, the place everybody knows the place to get your audio books.
[01:31:35.120 --> 01:31:42.880]   I am. I am just finished the Letterman biography, which I totally enjoyed. And now I'm starting.
[01:31:42.880 --> 01:31:49.040]   I don't know if I'm going to recommend this a book called Kristin Lavrens' Daughter,
[01:31:49.040 --> 01:31:57.360]   which is from the Nobel Prize winning Signeuxd Unset. It is a famous Norwegian story
[01:31:59.600 --> 01:32:07.280]   of thousands of pages from the Middle Ages. But she wrote it in the 20s. And I was told this is
[01:32:07.280 --> 01:32:11.200]   if you're if you're into Norway, you got to read this book. And who isn't in the Norway?
[01:32:11.200 --> 01:32:16.720]   And who isn't into Norway won the Nobel Prize for Literature in 1928. Anyway, the point is
[01:32:16.720 --> 01:32:22.320]   classics, modern. So how about ready player one? Wait a minute, we're going to get you a copy
[01:32:22.320 --> 01:32:27.840]   of ready player one, because this is a great, I think Will Wheaton does the audio book.
[01:32:27.840 --> 01:32:32.000]   This would be great for you, Robert. How else are you going to complain that the movie wasn't as
[01:32:32.000 --> 01:32:36.000]   good as a book? That's true. You have a good make-up of a point.
[01:32:36.000 --> 01:32:41.840]   One of these was my own channel. Parzival TV, broadcasting obscure eclectic crap.
[01:32:41.840 --> 01:32:47.280]   24/7, 365. Ernest Klein's latest is also their Armada.
[01:32:47.280 --> 01:32:52.240]   There actually are two versions of ready player one. You can get it in German too.
[01:32:53.280 --> 01:32:57.840]   This is for people who want to study German a really great way to learn here.
[01:32:57.840 --> 01:33:00.960]   Just a little bit of this. I do say the stacks in German.
[01:33:00.960 --> 01:33:04.480]   It is in Fondavut, Nataan. And who are both these Argon Fudders?
[01:33:04.480 --> 01:33:07.280]   Yeah, I think he's got a great voice. I would just listen to this one.
[01:33:07.280 --> 01:33:10.000]   What's David Nathan up to you right now?
[01:33:10.000 --> 01:33:13.120]   Right. And my name is Argon Fudders.
[01:33:13.120 --> 01:33:15.440]   I'm not a good guy. I'm not a good guy.
[01:33:15.440 --> 01:33:19.840]   Right? Am I am I am? Anyway, Audible does have multilingual stuff.
[01:33:19.840 --> 01:33:24.400]   You can also learn a language if you wanted to learn German or French or a variety of languages.
[01:33:24.400 --> 01:33:30.320]   They've got the greatest language courses on here. It's all part of your Audible subscription,
[01:33:30.320 --> 01:33:35.200]   which makes it really amazing. Did you know that this month is the 200th anniversary of Jane
[01:33:35.200 --> 01:33:40.560]   Austin's death? I did. Did you really? Well, there's a lot of stuff going on around it.
[01:33:40.560 --> 01:33:45.920]   Yeah, my wife is a Jane Austin freak. They have a ton of Jane Austin on here.
[01:33:45.920 --> 01:33:50.240]   I was just looking at Al Franken's newest giant of the Senate. Of course, he narrates that.
[01:33:50.240 --> 01:33:55.040]   That's a great boat myself with my own two hands. It's funny.
[01:33:55.040 --> 01:34:01.280]   Sorry, that's not true. I got that from my official Senate website. We really should change that.
[01:34:01.280 --> 01:34:06.240]   Let me let me start over. He says he comes from humble beginnings. He's the first person
[01:34:06.240 --> 01:34:14.480]   in his family to own a pasta maker. If you want audio books from the best readers,
[01:34:14.480 --> 01:34:19.200]   the best books Kevin Hart's got a new book. Oh, I got to read that. Trevor Noah's Born at Crime
[01:34:19.200 --> 01:34:25.120]   is amazing. Here's what you do. You go to audible.com/twitthenumber2. We're going to sign you up for the
[01:34:25.120 --> 01:34:33.840]   gold plus one plan. You get two free books and a 30 day free trial of audible.com at audible.com/twit
[01:34:33.840 --> 01:34:41.760]   and the number two. It's a great way to get two books to try out and then one credit per month
[01:34:41.760 --> 01:34:49.200]   after that. That is often enough, but I have to say picking the first two books is going to be
[01:34:49.200 --> 01:34:54.880]   the biggest challenge. There's so many great books. Audible.com. One of our chatters was telling
[01:34:54.880 --> 01:35:01.440]   me about this. Emily likes a gentleman in Moscow, a novel. She says these Amor Tulls books are
[01:35:01.440 --> 01:35:06.960]   really, really great. That's on my list. I listen to Audible like crazy. Let me see what else is on
[01:35:06.960 --> 01:35:11.840]   my library. Oh, I'm listening to David Sidaris' new one. He narrates that. It's his diary's
[01:35:11.840 --> 01:35:20.080]   theft by finding. Before we travel, we always get books. I read the Galapagos Natural History
[01:35:20.080 --> 01:35:24.720]   and Turn Right at Machu Picchu. I love to read these books before I go. This is a great discovery.
[01:35:24.720 --> 01:35:29.040]   I found this. If you love the John the Carrey stuff and I do and I've read all the novels,
[01:35:29.040 --> 01:35:36.640]   I've seen all the movies, but this is BBC full cast dramatizations of the complete George Smiley
[01:35:36.640 --> 01:35:43.600]   books. It is beautifully done. I'll play a little bit of this for you. It's very subtle.
[01:35:43.600 --> 01:35:49.120]   The last veil was removed, George blinked and blushed and understood. He'd imagined,
[01:35:49.120 --> 01:35:54.400]   so he said, fellowships under life, related to the literary obscurities of 17th century.
[01:35:54.400 --> 01:36:03.760]   I loved it. Are you a fan of Stephen Fry and John Bird? Here's the complete BBC
[01:36:03.760 --> 01:36:09.520]   4 radio comedy series called Absolute Power. Do you know this, Greg? I love Stephen Fry.
[01:36:09.520 --> 01:36:17.840]   No, it's a really, it's quite old. I think it's what are government media relations actually.
[01:36:17.840 --> 01:36:23.840]   But what makes allowances you are after all a bank manager? This is a yes prime minister.
[01:36:23.840 --> 01:36:28.400]   Like a creepy human. Yeah, it's very, it's dry, but I love it.
[01:36:28.400 --> 01:36:34.320]   Here's what you do. You go to audible.com/twit the number two. Get two books to start and then
[01:36:34.320 --> 01:36:41.840]   a book a month in the audible gold plus one plan. Highly recommended audible.com/twit
[01:36:41.840 --> 01:36:48.880]   and the number two. This is new. I'll mind it play how Claude Shannon invented the information age.
[01:36:48.880 --> 01:37:00.960]   I'm interested. We are talking Twit with three of the most intelligent panelists on the show.
[01:37:00.960 --> 01:37:04.880]   And I always love it when we get Greg Ferro from the Packet Pushers Network on. He's
[01:37:05.680 --> 01:37:13.760]   a network engineer and a brilliant fellow ethereal mind on the Twitter. Roberto Baldwin, always a
[01:37:13.760 --> 01:37:20.160]   pleasure too. He's fun. He's smart and he mocks my music taste. He of course is a regular adding
[01:37:20.160 --> 01:37:25.360]   gadget. And from around the world, our digital nomad. And back again.
[01:37:25.360 --> 01:37:30.320]   Gastron Nomad.net from Mike Elgin back home. Yeah. Nice to have you.
[01:37:30.320 --> 01:37:36.640]   Actually, Corey Booker, the senator from New Jersey has called for some government oversight
[01:37:36.640 --> 01:37:43.040]   on Amazon and Google and tech giants. So there is some sentiment. I just saw this on
[01:37:43.040 --> 01:37:49.920]   recode. They did a, they're doing an episode with Corey. He says, "The consolidation that's
[01:37:49.920 --> 01:37:56.160]   happening all over this country is not a positive trend." He said that the Republicans have been
[01:37:56.160 --> 01:38:02.960]   really slacking in terms of asserting consumer protections. And he says that a number of key
[01:38:02.960 --> 01:38:09.520]   federal agency leaders who oversee issues of antitrust are dangerous. He really wants to
[01:38:09.520 --> 01:38:16.080]   start paying attention to this. I worry when I hear that, on the one hand, I don't disagree that
[01:38:16.080 --> 01:38:20.480]   you got to keep it on these guys. I worry at Congress' ability to regulate these guys.
[01:38:20.480 --> 01:38:25.920]   It's problematic because the thing that makes them potentially anti-competitive are secret
[01:38:25.920 --> 01:38:31.920]   algorithms that nobody can see. And it makes me throw up on my mouth a little bit to think about
[01:38:31.920 --> 01:38:36.720]   a government requiring them to really feel those. To reveal those. And it also makes me
[01:38:36.720 --> 01:38:40.640]   sick that they can get away with murder behind the scenes with these algorithms.
[01:38:40.640 --> 01:38:43.760]   Facebook is another company that does this stuff. You don't know what they're doing.
[01:38:43.760 --> 01:38:48.720]   Are they racially profiling? Are they? You'll never know.
[01:38:48.720 --> 01:38:53.040]   Yeah. And all you know is that companies like Amazon and Google succeed wildly.
[01:38:53.040 --> 01:38:58.800]   And in the case, this is more of a European antitrust obsession. But Google
[01:38:58.800 --> 01:39:06.400]   has competitors. And those competitors don't rank. They both A) don't rank as well on Google
[01:39:06.400 --> 01:39:11.520]   searches as Google's stuff does. And B) they're not as successful. Are they not as successful because
[01:39:11.520 --> 01:39:16.480]   they don't rank? Or do they? You don't mean you don't know what's really going on if they're
[01:39:16.480 --> 01:39:22.880]   really cheating or not. They're just so, and these algorithm-based companies, they change
[01:39:22.880 --> 01:39:28.240]   their algorithms a thousand times a day. And so it's like there's no
[01:39:28.240 --> 01:39:32.560]   crisis. There's no tangible evidence that they're doing this because it's gone.
[01:39:32.560 --> 01:39:38.560]   And Amazon's already playing around with a thing called dynamic pricing where they charge
[01:39:38.560 --> 01:39:42.640]   you according your ability to pay. That is so weird, isn't it? Yeah.
[01:39:42.640 --> 01:39:48.400]   Yeah. Well, they've always monked around with book prices. Who knows why or how they're doing
[01:39:48.400 --> 01:39:53.280]   it, what the criteria is? I mean, it's crazy. Well, there's been automated book pricing for some
[01:39:53.280 --> 01:39:59.200]   time. But I think Ben Thompson over at Stratecary put it together in that the American government's
[01:39:59.200 --> 01:40:05.040]   going to struggle to regulate Amazon because the way that the government measures anti-competitive
[01:40:05.040 --> 01:40:11.920]   or monopolistic behavior is whether prices are cheaper or not. So they would the American
[01:40:11.920 --> 01:40:17.520]   government system says if the price of the goods rise, then that's monopolistic behavior.
[01:40:17.520 --> 01:40:22.800]   And we will then leap in and regulate it immediately. If the price that the customer is paying is less,
[01:40:22.800 --> 01:40:28.000]   then that's not monopolistic behavior. Now, it doesn't matter if Amazon was able to take over
[01:40:28.000 --> 01:40:33.200]   100% of the market under the current legal framework, as long as the price is, you know,
[01:40:33.200 --> 01:40:37.840]   this is Google's advantage, by the way, because Google has a monopoly, but it's selling something
[01:40:37.840 --> 01:40:43.600]   for free. So therefore, it can never be a monopoly unless something is prosecuted in the EU, but not
[01:40:43.600 --> 01:40:46.320]   so much here in the United States, you have to see the decline of Amazon. Because in the EU,
[01:40:46.320 --> 01:40:51.760]   they measure your exact behavior, not the outcome of the behavior. I think what Jeff Jarvis had told
[01:40:51.760 --> 01:40:57.520]   me that the difference in antitrust regulation is here, they're designed to protect the consumer,
[01:40:57.520 --> 01:41:02.160]   and in Europe, they're designed to protect other companies, to protect competition,
[01:41:02.160 --> 01:41:07.040]   which I think is a little more subtle and probably a better way of doing it,
[01:41:07.600 --> 01:41:12.800]   because price isn't the only way of measuring damage to consumers. The bizarre thing about that
[01:41:12.800 --> 01:41:18.080]   in Europe is that, especially as far as Google is concerned, is that Google is a monopoly in
[01:41:18.080 --> 01:41:25.760]   Europe and it isn't elsewhere. European citizens voluntarily choose to use Google,
[01:41:25.760 --> 01:41:29.840]   and it's super easy to use something else, like most other, you know, go to Russia and it's all
[01:41:29.840 --> 01:41:34.400]   Yandax. They're market share, Google's market share is what, 70 something percent?
[01:41:34.400 --> 01:41:40.800]   60 or something. Yeah, it's much lower. Right. And so it's really a kind of bizarre thing,
[01:41:40.800 --> 01:41:45.440]   you know, the citizens who the government in the EU is supposed to represent have chosen
[01:41:45.440 --> 01:41:51.920]   to give Google monopoly power. And so it's, I just think it's a bizarre.
[01:41:51.920 --> 01:41:58.080]   Oh, that's a problem in general. We love Amazon. We love Google. We don't see the problem
[01:41:59.760 --> 01:42:07.040]   until it's too late that they could pose. So you just came back, Roberto, just came back from
[01:42:07.040 --> 01:42:12.160]   Barcelona and the Audi event there. In fact, you've got a good article in
[01:42:12.160 --> 01:42:20.720]   Gagit here about self-driving boredom. You were looking at Audi's autonomous efforts.
[01:42:20.720 --> 01:42:25.760]   They have a, it's not in this effort. Yeah, it's the 25th hours with their colony,
[01:42:25.760 --> 01:42:30.240]   which is somebody in the marketing team and thought, Hey, I got an idea. It's 25th hour because
[01:42:30.240 --> 01:42:34.160]   they're giving, they're saying, you know, we're going to get an extra hour of our day. We're
[01:42:34.160 --> 01:42:37.840]   going to get an extra hour of our day. That's not how time works. It's just that you're not.
[01:42:37.840 --> 01:42:42.560]   But you know, they're, but they have to, you know, companies like Audi, companies like
[01:42:42.560 --> 01:42:47.040]   BMW, they're both working on it. But other companies need to figure out how are we going to sell
[01:42:47.040 --> 01:42:51.840]   self-driving cars? Yeah. Because if the car drives for you, then you're not worried about
[01:42:51.840 --> 01:42:55.120]   horsepower. You're not worried about torque. You're not worried about steering. Right.
[01:42:55.120 --> 01:42:59.920]   Now you're basically selling a home entertainment system. It's a living in a car. Yeah. How do you
[01:42:59.920 --> 01:43:05.680]   sell that now? And so they actually built this. This is a room where people sit and they've got
[01:43:05.680 --> 01:43:10.800]   screens that show the city going bus by them. And they're what they're testing. They've got a
[01:43:10.800 --> 01:43:17.280]   big thing on his head. They're just the amount of the amount of what the information. Is it too
[01:43:17.280 --> 01:43:21.520]   much? Is it not enough? What keeps you intrigued? What keeps you involved while you're driving?
[01:43:21.520 --> 01:43:25.520]   Their fear is that people will become bored to tears. Well, if you get in a car and it's just
[01:43:25.520 --> 01:43:28.720]   like shooting ads at you, then you're going to tell your friends, you know what? I bought this new
[01:43:28.720 --> 01:43:34.480]   Audi, this new autonomous Audi, and it sucks because I'm getting ads from Whole Foods. I'm getting
[01:43:34.480 --> 01:43:39.680]   ads from Apple. It's just it's so much about doing that about any ads would be kind of annoying.
[01:43:39.680 --> 01:43:42.880]   Yeah. So any ads would be kind of annoying, but they're trying to figure out like that
[01:43:42.880 --> 01:43:47.280]   that secret sauce of giving you information. Like how much information do you want? Because
[01:43:47.280 --> 01:43:53.520]   some people want an ad. Like I've bought things from Instagram, which seems ridiculous. In my
[01:43:53.520 --> 01:43:57.920]   mind, I'm like, ads are dumb, ads are stupid, ads pay my bills, but they're horrible. And then I'm
[01:43:57.920 --> 01:44:03.200]   on Instagram and I'm all the time. Oh man, I really need those speakers. I really need those
[01:44:03.200 --> 01:44:06.800]   coach. I need. Yeah. And that's, you know, that's what we're trying to figure out. In addition to
[01:44:06.800 --> 01:44:13.440]   the like how much like how much information they'll tell you about your day, you know,
[01:44:13.440 --> 01:44:17.360]   the news, you want a lot of news, you want little news. And so just trying to figure out,
[01:44:17.360 --> 01:44:21.600]   they're not trying to make a better environment for people. They're seeing what they can get away
[01:44:21.600 --> 01:44:26.480]   with. I think they've got it completely backwards. Audi wants to sell cars to consumers. And then
[01:44:26.480 --> 01:44:29.520]   after they bought one of these self-driving cars, they're going to get ads and they're trying to
[01:44:29.520 --> 01:44:33.520]   figure out what kind of ads how many they got it completely backwards. I think that we're going to
[01:44:33.520 --> 01:44:38.720]   get to a point where rides are free and you pay for it by watching ads. Oh, that's different.
[01:44:38.720 --> 01:44:43.200]   Reality. It's going to be a great virtual reality platform. Nothing else to do. You put on the goggles,
[01:44:43.200 --> 01:44:46.560]   whatever it takes. But you don't need the goggles because you have the glass on. I mean,
[01:44:46.560 --> 01:44:50.320]   it's yeah, that's the holiday. Isn't it? But the stuff on my face again, the virtual reality,
[01:44:50.320 --> 01:44:55.280]   like a nice warm computer on my face does not feel like a good time. It'll be all of the above,
[01:44:55.280 --> 01:45:00.240]   all of the above, but especially at I think it'll start at influencer events. So if there's a trade
[01:45:00.240 --> 01:45:04.800]   show, anybody with a badge will get free rides anywhere they want in the city and they'll get
[01:45:04.800 --> 01:45:10.880]   peppered with advertising related to the topic of the concert. So Roberto got put in this thing.
[01:45:10.880 --> 01:45:14.400]   So I got put in it and it was actually they were like, well, if you get
[01:45:14.400 --> 01:45:19.680]   motion sickness, tell us. Oh, really? And it actually felt like you were just sitting stationary,
[01:45:19.680 --> 01:45:24.000]   but because of all the screens are moving, you're brain seeing movement. Your brain
[01:45:24.000 --> 01:45:29.600]   seeing move is so you feel like you're moving. But yeah, sick. No, I don't usually get the
[01:45:29.600 --> 01:45:33.280]   opportunity for you there. Yeah, basically you're basically inside a virtual reality.
[01:45:33.280 --> 01:45:36.560]   You said you started to reach for your phone. You do because it's just like,
[01:45:36.560 --> 01:45:43.760]   well, and also the phone inside the view. Yeah. And so what it comes down to is people are going to
[01:45:43.760 --> 01:45:49.440]   have whatever's on their phone, that's what's going to be on the screen. Yeah, that's what we
[01:45:49.440 --> 01:45:54.640]   want. You want what's on your phone there will be doing face, and would you get ads? Maybe if you
[01:45:54.640 --> 01:45:57.360]   get ads, you'll probably get Facebook ads, you'll probably get Instagram ads, you'll probably get
[01:45:57.360 --> 01:46:02.480]   which what you are used to already. And you know, ad, but Audi, you know, these companies need to
[01:46:02.480 --> 01:46:06.160]   do this research because they have to figure out what people are going to want before they want.
[01:46:06.160 --> 01:46:12.000]   I need to tell them what they're giving people ads. They were using ads as bad as
[01:46:12.000 --> 01:46:16.640]   your thought. Half windows. You can look out the windows as the world goes by. And if you want to
[01:46:16.640 --> 01:46:20.480]   look at your phone, look at the phone. Microsoft windows? No, people don't want that. No one
[01:46:20.480 --> 01:46:23.760]   looks at the window anymore. I'm driving this people in the car. I'm like, look a deer. And they're
[01:46:23.760 --> 01:46:31.680]   like, wait, what? Well, you missed the deer. That's me driving. Why does Audi think you need
[01:46:31.680 --> 01:46:36.160]   something on the screen? It's not that Audi thinks it's that we think it because you know why?
[01:46:36.160 --> 01:46:39.280]   Because when you when we finish this, you're going to pick up your phone and do this.
[01:46:39.280 --> 01:46:44.880]   Horrible reality is that Audi is not going to survive this transition. They're like, we don't
[01:46:44.880 --> 01:46:49.760]   need Audi's anymore. You know, self-driving cars are going to be generic. So Greg, you're
[01:46:49.760 --> 01:46:54.080]   saying they have an expensive value. Yeah, there will. There has to be a point of differentiation.
[01:46:54.080 --> 01:46:59.040]   If you go and have any Uber car and it's a black something, something, something, or maybe it's
[01:46:59.040 --> 01:47:03.840]   something that Uber manufactures itself. So now Audi differentiates itself. The
[01:47:03.840 --> 01:47:08.240]   pleasure of driving is one way car companies do it, right? But you won't be doing that.
[01:47:08.240 --> 01:47:11.280]   Well, there's going to be the niche market of drivers. Most people don't
[01:47:11.280 --> 01:47:16.240]   like driving. We can't let people drive if it's autonomous because they'll screw it up.
[01:47:16.240 --> 01:47:20.560]   It's too dangerous. We can't let people drive. They'll have to go to special places.
[01:47:20.560 --> 01:47:26.400]   Yep. Imagine what's going to happen to these cars. Well, everyone's going to bully these
[01:47:26.400 --> 01:47:30.000]   self-driving cars because the self-driving car will get out of your way. If I'm driving,
[01:47:30.000 --> 01:47:35.680]   let's say this is true. And here's a self-driving car. I know this self-driving car is going to stop
[01:47:35.680 --> 01:47:39.200]   if I cut it off. Right. So you cut it off. And that's the Apple self-driving car.
[01:47:39.200 --> 01:47:43.120]   This is the Apple self-driving car. Somebody did that to me yesterday. I'm driving my Tesla.
[01:47:43.120 --> 01:47:49.040]   They were driving a truck with a horse trailer on it. They decided they didn't want to be in that
[01:47:49.040 --> 01:47:53.120]   lane. They just pulled in front of me. They figured, oh, he'll stop. The Tesla will stop.
[01:47:53.120 --> 01:47:56.160]   I'm pretty sure that's what they... Because it was the weirdest vehicle in my life.
[01:47:56.160 --> 01:47:57.040]   But they might have seen you.
[01:47:57.040 --> 01:48:01.760]   No, no, no. They pulled... And they almost hit me. And I really thought,
[01:48:01.760 --> 01:48:05.920]   oh, that's because... You know why? Because in that truck, we're the best self-driving vehicles
[01:48:05.920 --> 01:48:11.680]   horses. Yeah. What do you do? So when somebody's riding a horse, what screens do you show him?
[01:48:11.680 --> 01:48:17.520]   Yeah. I think... I think what you're seeing here is economically, there's a massive gap opening
[01:48:17.520 --> 01:48:22.320]   up between who's going to manufacture electric cars, who's going to manufacture self-driving cars.
[01:48:22.320 --> 01:48:27.680]   And if you have self-driving cars, are they all going to be run by ride-sharing services like
[01:48:27.680 --> 01:48:32.720]   Uber and Lyft and so on and so forth? What do you need car ownership for at that point?
[01:48:32.720 --> 01:48:38.880]   Yeah. Well, I mean, I'm still car in the future. I'm in the car now. And I would welcome a ride-sharing
[01:48:38.880 --> 01:48:43.120]   service that could just come and pick me up from my door. So I don't have to call a taxi and wait
[01:48:43.120 --> 01:48:47.840]   for it to arrive or go and catch the bus. I think that's a great thing. But if you're a car manufacturer
[01:48:47.840 --> 01:48:52.320]   and you're in a business where you're making five to ten billion dollars a year shipping cars,
[01:48:52.320 --> 01:48:58.560]   you want to be able to have a spectrum of choices. So you have companies like Audi and
[01:48:58.560 --> 01:49:04.960]   Chrysler and Ford investing in ride-hailing services. And at the same time, they're putting out cars
[01:49:04.960 --> 01:49:10.560]   that consumers might want to buy directly. So self-driving cars, but ones that consumers might
[01:49:10.560 --> 01:49:15.280]   want to pay a premium for. And I think they're going to try all the different possibilities in
[01:49:15.280 --> 01:49:18.960]   the market. And we don't know what the fine... It's a bit like Google Glasses, right? Google
[01:49:18.960 --> 01:49:24.480]   Glasses 2 comes out. Is it going to be a success or a failure? We don't know. I can point at things
[01:49:24.480 --> 01:49:30.720]   like the infrastructure isn't there. So they do all of them. Big companies aren't smart enough
[01:49:30.720 --> 01:49:35.440]   to do the one thing that's right. They do all the things and wait to see which one of those
[01:49:35.440 --> 01:49:39.440]   little treasures, little nuggets of happiness floats to the top and keeps them winning.
[01:49:39.440 --> 01:49:43.200]   So I think we already kind of have an experience. Anybody who lives in an urban area of what it would
[01:49:43.200 --> 01:49:47.760]   be like not to own a car because Uber is cheaper, faster and easier, isn't it?
[01:49:47.760 --> 01:49:54.000]   We drove into San Francisco, it was easier to park. But here's an... And just take Uber. The problem
[01:49:54.000 --> 01:49:58.400]   with Uber is the driver. People feel uncomfortable with stranger... Yeah, I don't want to keep it in there.
[01:49:58.400 --> 01:50:03.840]   That's why self-driving cars and Uber makes a lot of sense. The issue with self-driving car still
[01:50:03.840 --> 01:50:09.920]   is that it's not removing that extra car on the road because people are still getting in one person
[01:50:09.920 --> 01:50:14.560]   in one car and they're going from point A to point B. There's more than 500 Ubers on the road in
[01:50:14.560 --> 01:50:19.840]   San Francisco. It hasn't reduced congestion. It's increased congestion. Exactly. And so there's
[01:50:19.840 --> 01:50:23.360]   still... We're always driving. Everyone's talking about self-driving cars. I'm like, well, we still have
[01:50:23.360 --> 01:50:27.600]   to look at the infrastructure of public transportation, which is, you know what, you can get a lot of
[01:50:27.600 --> 01:50:31.200]   people on a bus. And if the bus is self-driving, that's great because the bus is... And this is
[01:50:31.200 --> 01:50:37.040]   what's going to... This is the first version of autonomy is going to be vehicles along a very
[01:50:37.040 --> 01:50:42.320]   narrow route. They're on a route. They're like slot cars, basically. And they're on this route and
[01:50:42.320 --> 01:50:48.640]   it's going to be probably Ubers, most likely, and probably public transportation. But the...
[01:50:48.640 --> 01:50:53.280]   Oh, it's all just keep buying cars or we're just going to have a lot of cars. If you just
[01:50:53.280 --> 01:50:56.720]   replace the cars that people are driving with cars that no one is driving, we're still going to have
[01:50:56.720 --> 01:51:00.240]   congestion. You know, algorithms are great and we're going to be able to move a little bit quicker.
[01:51:00.240 --> 01:51:06.240]   But it still needs to be a figure out of reduction in vehicles on the road.
[01:51:06.240 --> 01:51:10.000]   So the way that's going to work is you're going to see in the capital cities like London and New
[01:51:10.000 --> 01:51:15.200]   York with the congestion zones, where unless you're registered or it'd be out of a permit to drive
[01:51:15.200 --> 01:51:20.080]   in the center of those cities, the obvious thing that they're going to do is to say only self-driving
[01:51:20.080 --> 01:51:25.040]   cars from ride-hailing services inside of that congestion zone. And that'll prove that out.
[01:51:25.040 --> 01:51:30.480]   And then you'll see... They're already talking here in London about turning parking garages
[01:51:30.480 --> 01:51:36.560]   into charging stations for these self-driving vehicles. But they're still giving up all that
[01:51:36.560 --> 01:51:40.320]   and all that real estate to vehicles. No, the... I mean, the... I mean, the... I mean, the...
[01:51:40.320 --> 01:51:42.720]   I mean, the... I mean, the... I mean, the... It shows that it's an eight to... No, you're wrong.
[01:51:42.720 --> 01:51:47.600]   There's an eight to one compression ratio. Once you shift to self-driving ride-hailing services,
[01:51:47.600 --> 01:51:52.240]   for every eight cars on the road today will be replaced by one self-driving car.
[01:51:52.240 --> 01:51:57.840]   But you're talking about decades into the future. Two decades. Probably more like three.
[01:51:57.840 --> 01:52:01.760]   If you're talking about everyone having it, we're probably 15 years out before we even get to just
[01:52:01.760 --> 01:52:06.240]   the crappy one. The previous one's still worth it. In the congestion zones inside the cities,
[01:52:06.240 --> 01:52:10.960]   we have the pollution problem. They have the health, too much diesel, too much whatever.
[01:52:10.960 --> 01:52:15.120]   It's probably going to happen within five years. There's a tipping point of...
[01:52:15.120 --> 01:52:20.960]   There won't be appropriate self-driving vehicles in five years to really...
[01:52:20.960 --> 01:52:25.360]   You're not going to have self-driving vehicles in a higher, right? It's going to be in the capital
[01:52:25.360 --> 01:52:31.120]   cities where there's high density and there are more complex issues than there are in somewhere else,
[01:52:31.120 --> 01:52:35.280]   right? You're going to see those. You'll see them in Singapore and Hong Kong and London.
[01:52:35.280 --> 01:52:38.800]   No, I mean, it makes more sense for self-driving vehicles. The situation is a massive problem.
[01:52:38.800 --> 01:52:43.920]   And the government's... Although, you know what? You still see a ton of bicycles in Beijing.
[01:52:43.920 --> 01:52:47.440]   I think bicycles might be a better choice for a lot of these urban areas.
[01:52:47.440 --> 01:52:52.960]   Get some exercise, no pollution. Or the bus. Or you could walk. Or take the subway.
[01:52:52.960 --> 01:52:56.560]   Or take a segue. Anyway, so before we move on, we talk about Audi.
[01:52:56.560 --> 01:53:01.120]   I really think Apple has the right idea for the future of manufacturing self-driving cars.
[01:53:01.120 --> 01:53:06.960]   They're working with an Austrian contract manufacturer called Magnus Steyer. And they've
[01:53:06.960 --> 01:53:11.360]   based their whole business off of Foxconn. And so I think Apple is making a big bet that...
[01:53:11.360 --> 01:53:14.960]   We won't make them. They'll just... It won't matter who makes them. They'll be designing
[01:53:14.960 --> 01:53:20.720]   California, whatever. And then some contract manufacturer will make them. They'll be given
[01:53:20.720 --> 01:53:25.680]   the brand of whoever designed them. And they'll churn them out like nothing. They'll cost very
[01:53:25.680 --> 01:53:31.120]   little. And they'll be mostly consumer electronics experiences. But how would the cost of having a
[01:53:31.120 --> 01:53:38.160]   third party build it be different from having Ford build it? Well, talk about Ford, Audi,
[01:53:38.160 --> 01:53:43.120]   whatever. I mean, to a certain extent, car manufacturing has become very contract manufacturer-like.
[01:53:43.120 --> 01:53:46.720]   The parts are all manufacturers. You're saying it's commoditized. It's going to be commoditized.
[01:53:46.720 --> 01:53:50.640]   Right. It won't be... It won't get an Audi. Who cares about an Audi? Exactly. It doesn't...
[01:53:50.640 --> 01:53:54.400]   The brand's not going to mean anything. Look what happened with Apple, with the iPhone.
[01:53:54.400 --> 01:53:57.360]   They came out with the iPhone, they mass-produced these things. I bet you, you can't tell me what
[01:53:57.360 --> 01:54:05.600]   the number one buggy whip manufacturer was in 1892. Buggy's unlimited. He's better than I thought.
[01:54:05.600 --> 01:54:11.920]   Yeah, no, I'm sorry. Turn that the wheel in the middle. Everything about the electric car industry
[01:54:11.920 --> 01:54:19.040]   is accelerated economics. It all runs off a cliff. Like I was talking somebody in the fuel in the
[01:54:19.040 --> 01:54:26.240]   petrol industry, petrol stations. And they are predicting that if just 30% of sales of petrol
[01:54:26.240 --> 01:54:34.160]   sales shrink by 30%, the entire distribution market for fuel collapses. Because it needs a
[01:54:34.160 --> 01:54:38.800]   certain density, it needs a certain number of fuel stations. And if 30% of the fuel stations
[01:54:38.800 --> 01:54:43.920]   go away because 30% less petrol is being sold, that whole distribution chain just falls apart.
[01:54:43.920 --> 01:54:49.200]   And that market goes into freefall. And at that point, it becomes more effective to have
[01:54:49.200 --> 01:54:53.840]   electric cars because fuel has to rise in price just because of the cost of distribution.
[01:54:53.840 --> 01:54:58.480]   That's fuel in general. All kinds of fuels. Right. I mean, we're starting to see a transition.
[01:54:58.480 --> 01:55:04.160]   People are worried about climate change. It may turn out that we go to renewables faster than we thought.
[01:55:05.520 --> 01:55:13.440]   But that cliff, there's a singularity where solar is always cheaper than alternative sources in
[01:55:13.440 --> 01:55:16.640]   Russia. Why would you do anything else? We're approaching it.
[01:55:16.640 --> 01:55:21.280]   With the storage technologies, it's really driving that. As soon as you're able to store that,
[01:55:21.280 --> 01:55:27.280]   with the power wall, Mercedes is making something as well, the storage. Once you can store the
[01:55:27.280 --> 01:55:32.960]   energy and use it anytime, then suddenly you're like, "Oh, well, now solar power isn't... You
[01:55:32.960 --> 01:55:36.640]   don't have an argument against really solar power." That's what Hawaii is moving to, because they
[01:55:36.640 --> 01:55:41.600]   don't really have a choice because they're pricing your electric, your kilowatt per hour,
[01:55:41.600 --> 01:55:46.720]   rises and falls based on what the oil prices are because they're all using diesel.
[01:55:46.720 --> 01:55:48.080]   Yeah. That's interesting.
[01:55:48.080 --> 01:55:52.800]   And the mother of all venture capitalists is, of course, strongly backing solar, the Pentagon,
[01:55:52.800 --> 01:55:58.480]   of course. They go to these sunny places and they have to have these convoys going through
[01:55:58.480 --> 01:56:04.400]   Afghanistan and elsewhere protecting these giant oil tankers because the tanks and the cars and
[01:56:04.400 --> 01:56:09.360]   all the stuff that you need for a presence like what the Pentagon has, what the US military has
[01:56:09.360 --> 01:56:15.840]   in Afghanistan, is just you need trillions of gallons of gas and the sun is beating down
[01:56:15.840 --> 01:56:21.120]   every day on everything and it's free energy. What about the chat rooms bringing out these
[01:56:21.120 --> 01:56:26.320]   disruptions, causes disruptions in tax revenue? How do local... When you lose gasoline, you lose
[01:56:26.320 --> 01:56:35.120]   gas taxes. How do we just solve that by writing new laws or how sticky is that thick wicked?
[01:56:35.120 --> 01:56:37.520]   Just to... I said that for you, Greg.
[01:56:37.520 --> 01:56:46.720]   There's a whole bunch of things that has to happen, but if you look at what they do here in the UK,
[01:56:46.720 --> 01:56:53.840]   they charge you road tax, which is to own a car, you have to have a tax to put it on the road,
[01:56:53.840 --> 01:56:58.720]   as well as your insurance, plus the government licensing fees, plus so forth and so on.
[01:56:58.720 --> 01:57:02.480]   Most of the government revenue though for running the roads comes in the form of fuel,
[01:57:02.480 --> 01:57:05.520]   so the taxes on the fuel here, which are very high. That's very early in the US actually.
[01:57:05.520 --> 01:57:11.680]   So if you're not... You still need the roads and building out that infrastructure is not cheap,
[01:57:11.680 --> 01:57:17.600]   so what do you do? Tax electricity? That will be popular. Yeah.
[01:57:17.600 --> 01:57:23.040]   So it'll be an interesting challenge, but that's... All everything we talk about is a
[01:57:23.040 --> 01:57:29.040]   disruption and proposes challenges for all the existing companies. What happens to all the garages
[01:57:29.040 --> 01:57:34.080]   and mechanics? Keep in mind that electric car needs... What is it? One-fifth of servicing of an
[01:57:34.080 --> 01:57:37.440]   ordinary challenge? It's changed the oil. You know, she's probably about a Tesla year ago and
[01:57:37.440 --> 01:57:42.080]   I'm supposed to bring it in. I don't know. You don't even have to change the batteries as much.
[01:57:42.080 --> 01:57:46.000]   It's not the batteries, it's the brakes. It's because of regenerative regeneration.
[01:57:46.000 --> 01:57:49.040]   Yeah. Your brakes don't go out as obviously. If you're changing the wheel,
[01:57:50.320 --> 01:57:54.080]   and most of the batteries are rated at over 300,000 miles in this car. You're mostly going to be
[01:57:54.080 --> 01:57:59.200]   upgrading the processor for the VR. Well, I get updates all the time, but yeah,
[01:57:59.200 --> 01:58:04.080]   hardware upgrades. Yeah. Well, I guess we got to... I don't know, realign your wheels again.
[01:58:04.080 --> 01:58:08.480]   Let's take a little break. We're almost done here. We've been talking for two hours here. We
[01:58:08.480 --> 01:58:13.120]   might want to wrap this up, but first let's... It's hard to stop when we're having fun like this,
[01:58:13.120 --> 01:58:17.440]   but let's first take a look at some of the things that happened this week on Twitch.
[01:58:18.480 --> 01:58:24.160]   Previously on Twitch. Took a picture of you. It says 54 year old man wearing a hat looking happy.
[01:58:24.160 --> 01:58:27.360]   I took six years off my age because I was wearing the hat sideways. Yes.
[01:58:27.360 --> 01:58:34.080]   Tech News today. Axios says Google, Apple and Amazon have spent record amounts in lobbying
[01:58:34.080 --> 01:58:38.880]   this year. I think because there's so much uncertainty, I am guessing these companies
[01:58:38.880 --> 01:58:42.800]   feel like they have to protect themselves by getting as much opinion out there. And as many
[01:58:42.800 --> 01:58:47.440]   people work in the system as possible. We'll see our one in the chat room says why lobby,
[01:58:47.440 --> 01:58:50.720]   by politicians direct from Amazon. They felt everything.
[01:58:50.720 --> 01:59:00.400]   Yesterday was World Emoji Day. Very big deal. Also, we were reminded yesterday of a very sad fact.
[01:59:00.400 --> 01:59:05.920]   Once Androido comes to fruition, we'll be saying goodbye to those beautiful yellow blobs.
[01:59:05.920 --> 01:59:11.920]   Well, now we're getting these circular guys that look like everybody else's. So there goes
[01:59:11.920 --> 01:59:16.240]   our individualism, but I'm not bitter. I guess we'll be okay with it.
[01:59:16.240 --> 01:59:22.800]   To it. The happiest place on earth. The sweat face emoji is going to completely change with the
[01:59:22.800 --> 01:59:29.440]   new emoji in Androido. It's going to mean sickly as I typically use it for being anxious or nervous,
[01:59:29.440 --> 01:59:34.000]   which is pretty much every minute of my life. Especially when Jason's not here.
[01:59:34.000 --> 01:59:40.080]   Florence, the great Florence, I had a lot of fun on this week in, I'm sorry, all about Android,
[01:59:40.080 --> 01:59:44.720]   not this week in Android. Meghan Moroni, we have a big week ahead. What's on the docket?
[01:59:44.720 --> 01:59:52.080]   White and black hat hackers will descend on steamy hot Las Vegas this week for black hat.
[01:59:52.080 --> 01:59:58.400]   The conference started in 1997, which means it turns 20 years old. And my advice for you is to
[01:59:58.400 --> 02:00:05.520]   watch out for pineapples. And I'm not talking about their prickly skin. If constantly protecting
[02:00:05.520 --> 02:00:10.080]   yourself from getting hacked isn't your jam, you might be more interested in the wearable
[02:00:10.080 --> 02:00:15.520]   technologies conference, which will happen this week in San Francisco. And finally, Amazon,
[02:00:15.520 --> 02:00:21.200]   Facebook, PayPal and AMD will all release earnings this week. Alphabet will release their quarterly
[02:00:21.200 --> 02:00:25.760]   earnings as well. And because of changes in regulation, we expect them to break out YouTube
[02:00:25.760 --> 02:00:30.320]   earnings separately from Google search and advertising that happens for the first time.
[02:00:30.320 --> 02:00:35.120]   And that could be interesting. That is a look at a few of the things we'll be tracking in the coming
[02:00:35.120 --> 02:00:41.440]   week. Join Jason Howell and me on Tech News Today, every weekday of 4PM Pacific, 7PM Eastern here
[02:00:41.440 --> 02:00:46.000]   on Twitter.tv. I have a feeling the wearables conference won't be quite as big this year.
[02:00:46.000 --> 02:00:50.400]   I don't know. Is that a category anymore? Is that funny how these things kind of...
[02:00:50.400 --> 02:00:55.040]   I've never been a fan of wearables. I put them on there like, oh, this is cool. I mean, like,
[02:00:55.040 --> 02:00:57.840]   after two weeks, I take it off and I put it over here. And then...
[02:00:57.840 --> 02:01:01.920]   It's on your dresser. It's over here for the next two years. And I'm like, oh, this thing.
[02:01:03.680 --> 02:01:07.280]   Yeah, I just... I can never... I tried to be excited about wearables.
[02:01:07.280 --> 02:01:11.680]   It was a category. People were excited about it. Oh, yeah. Intel put through some money at it.
[02:01:11.680 --> 02:01:16.480]   By the way, they've just closed that division down. Yeah. And the basis is gone. Is it me or does
[02:01:16.480 --> 02:01:21.040]   it not... Does that what Intel does? They have this huge initiative. They talk about it. You read a
[02:01:21.040 --> 02:01:26.720]   bunch of stuff and they're like, oh, we're closing it down. Again? No, that's everything. Intel technology
[02:01:26.720 --> 02:01:32.400]   really wasn't as good as anybody else's. They... Their mobile strategy hadn't supported the technology.
[02:01:32.400 --> 02:01:38.240]   So they didn't shut it down. I don't... I'm pretty sure that Intel, like every other big company,
[02:01:38.240 --> 02:01:42.400]   goes and sticks its finger in someone else's pie. And if it doesn't work, it pulls its finger back
[02:01:42.400 --> 02:01:48.400]   out again. But Intel's mobile strategy hasn't been very good. It's low power stuff is still
[02:01:48.400 --> 02:01:54.320]   struggling. Even their intelligence and image recognition chips are still not as good as somebody
[02:01:54.320 --> 02:01:59.680]   else's. So it's hard to see how Intel comes back and participates in the mobile or the wearer's
[02:01:59.680 --> 02:02:05.200]   market offering silicon. And ultimately, its business is selling silicon, right? So that's what it does.
[02:02:05.200 --> 02:02:07.520]   Well, thank God the PC market is really on the go.
[02:02:07.520 --> 02:02:15.440]   Nevermind. It's fell 10% again. Yeah, another 10%. Yeah. Microsos making money, but it's on the
[02:02:15.440 --> 02:02:19.680]   cloud. And actually, Microsoft's quarterly results came out this weekend for the first time ever.
[02:02:19.680 --> 02:02:24.480]   They made more money on office subscriptions than they did on selling office software outright.
[02:02:24.480 --> 02:02:29.360]   And that's definitely a bellwether for the future of software. In fact, who even
[02:02:29.360 --> 02:02:35.120]   remembers going to a software store and buying a box of software and taking it home? That seems
[02:02:35.120 --> 02:02:39.200]   so primitive now. I did it, but I don't remember. I remember buying Windows XP because my wife
[02:02:39.200 --> 02:02:43.840]   took a math class that she needed to have XP. I had to have XP so I bought Windows XP so we
[02:02:43.840 --> 02:02:48.800]   could bootcamp it up. The last time I tried to buy software, I tried to buy Microsoft office about
[02:02:48.800 --> 02:02:53.920]   four or five years ago. And the sticker shock, I went into a Staples, it was like 700 bucks or
[02:02:53.920 --> 02:02:58.560]   something. And I thought, I'm not buying that. I can live with that office. I'm not buying it.
[02:02:58.560 --> 02:03:02.640]   I'll use last year's office. And that was the end of that. And now I do subscribe for eight bucks
[02:03:02.640 --> 02:03:08.720]   a month to office 365. It's a better deal. I subscribed to Photoshop and Lightroom. It's just
[02:03:08.720 --> 02:03:14.080]   it's a natural way to do it. Let's take a little break. We have a few more minutes with our great
[02:03:14.080 --> 02:03:20.800]   guests, our panel today, Mike Elgin. And don't forget gastronomad.net if you're interested in
[02:03:20.800 --> 02:03:25.600]   spending some time with Mike in the mirror in Barcelona. Barcelona, we're going to teach people
[02:03:25.600 --> 02:03:30.560]   how to live everywhere and eat everything. Sounds wonderful. Roberto Baldwin here from Engadget.
[02:03:30.560 --> 02:03:36.240]   And is your is your band performing anywhere soon? Or are you waiting for that drummer?
[02:03:36.240 --> 02:03:40.880]   We are waiting for a drummer from one band. The other band is gearing up for a big wedding.
[02:03:40.880 --> 02:03:45.200]   Can't you have that drummer drummer the other? I already asked them. I have two drummers in
[02:03:45.200 --> 02:03:49.200]   the main band. You do. And I asked them and I said, Hey, do you have any of you want to
[02:03:49.200 --> 02:03:52.800]   have two drummers? You have an excess of drummers. Are you a drummer? Yeah.
[02:03:52.800 --> 02:03:58.800]   But not good enough for that. He's got to be the front man. He's a front man. But I'm not
[02:03:58.800 --> 02:04:03.040]   just not good enough. Did you see Kid Rock might be running for a chorus? Yeah, I saw that.
[02:04:03.040 --> 02:04:09.760]   Finally, somebody responsible. You don't have a Kid Rock tribute band. No, no, I have the furthest
[02:04:09.760 --> 02:04:15.280]   thing from a Kid Rock tribute band. Senator Kid Rock. Well, initially, like when you went to
[02:04:15.280 --> 02:04:18.480]   Kid Rock running, like you went to URL, like Warner Brothers, Kid Rock.
[02:04:18.480 --> 02:04:23.120]   And like, this is just a PR thing. Well, I mean, that's how our current president ended up.
[02:04:23.120 --> 02:04:25.360]   Yeah, sometimes you just can do it.
[02:04:25.360 --> 02:04:27.920]   Which is like a goal for it. Wow, you won. So,
[02:04:27.920 --> 02:04:29.920]   Oh, dear. Now what? Yeah.
[02:04:29.920 --> 02:04:36.720]   Our show today brought to you by Betterment, the largest independent online financial advisor.
[02:04:36.720 --> 02:04:41.920]   You know, you know, you've got to do finances. It's the grown up thing. You've got to save for
[02:04:41.920 --> 02:04:47.120]   the future for your college, you know, your kid's college bills or you want to buy that house or
[02:04:47.120 --> 02:04:53.520]   hey, someday you're going to retire. Betterment is designed to help improve your long-term returns
[02:04:53.520 --> 02:04:57.840]   and lower your taxes for everything from retirement planning to building wealth,
[02:04:57.840 --> 02:05:02.320]   all the financial goals. They use very advanced sophisticated investment strategies.
[02:05:02.320 --> 02:05:09.760]   And of course, the latest technology, a quarter of a million people use Betterment to get better
[02:05:09.760 --> 02:05:14.800]   advice based on the information you share with Betterment. They'll make tailored recommendations.
[02:05:14.800 --> 02:05:18.800]   How much to invest? Yes, they'll even tell you how much how much risks to take
[02:05:18.800 --> 02:05:23.440]   the type of investment account you should have. And on average, this is important. Betterment's
[02:05:23.440 --> 02:05:31.200]   tax coordinated portfolio can increase portfolio value by as much as 15%, 15% over 30 years.
[02:05:31.200 --> 02:05:34.480]   Betterment gives you a clear view of your net worth when you sink your outside accounts.
[02:05:35.440 --> 02:05:40.720]   Another reason to choose Betterment, low transparent advisory fees, much lower than traditional services,
[02:05:40.720 --> 02:05:47.200]   only 0.25% on assets under management. And if you'd like to get access to a certified financial
[02:05:47.200 --> 02:05:53.040]   planner or a licensed financial expert, you pay a little more, but just 0.4 to 0.5%. It really is
[02:05:53.040 --> 02:05:57.440]   a great deal. Betterment, they care about you keeping your money and your data secure.
[02:05:57.440 --> 02:06:01.280]   That's why they have advanced data encryption and login protection and they care about growing
[02:06:01.280 --> 02:06:05.920]   your money. Investing involves risk, but for a limited time, you can get up to a year managed
[02:06:05.920 --> 02:06:12.000]   free at betterment.com/twit. Betterment.com/twit.
[02:06:12.000 --> 02:06:16.240]   Rethink what your money can do. Betterment is a better way.
[02:06:16.240 --> 02:06:20.480]   Kid Rock, Senator Rock, are you going to call him Senator Rock?
[02:06:20.480 --> 02:06:23.520]   I'm not going to call him Senator Kid. Senator Kid?
[02:06:23.520 --> 02:06:27.760]   He said, yeah, well, that's right. The other rock. The other rock. What do you go by is?
[02:06:28.480 --> 02:06:34.320]   His real name is what? Richie? Richie. Richie Rock? Richie Rock? Richie something.
[02:06:34.320 --> 02:06:40.000]   Really? Yeah, Kid Rock for Senator. Let's see what it looks like right now. Are you scared?
[02:06:40.000 --> 02:06:45.760]   Yeah, at this point, Pimp of the Nation. At this point, it does look promotional. But as you say,
[02:06:45.760 --> 02:06:50.720]   you never know. And also, kids, those of you beginning musical careers out there,
[02:06:50.720 --> 02:06:54.240]   don't have your name be kid because eventually you'll be in your 60s.
[02:06:54.240 --> 02:06:59.360]   Yeah, Sonic Youth is not really. When they broke up, they were definitely not used.
[02:06:59.360 --> 02:07:04.480]   Sonic Youth. So, quick thing, Leo, and I just want to throw this in there because I think it's
[02:07:04.480 --> 02:07:09.040]   such a cool thing. Before the show began, before we began recording, we were talking about command
[02:07:09.040 --> 02:07:14.720]   lines and that sort of thing. And I mentioned that my soul uses a thing for the Mac OS that
[02:07:14.720 --> 02:07:20.000]   gives you super powerful thing. It's called Alfred. Oh, I love Alfred. Alfred. I know Alfred,
[02:07:20.000 --> 02:07:24.240]   I've known Alfred for a long time. Alfred is a good friend of mine at AlfredApp.com.
[02:07:24.240 --> 02:07:28.960]   Yeah, if you like the command line, what Alfred is kind of like launch bar was the original one,
[02:07:28.960 --> 02:07:35.360]   and you can now do it with just built-in Apple stuff, Sherlock and so forth. But Alfred is really
[02:07:35.360 --> 02:07:40.640]   great if you use a command line because you can do those command line chores from your command
[02:07:40.640 --> 02:07:47.280]   space, your hotkeys. No, I love Alfred. In fact, I'm a power pack owner on Alfred. So, yes, thank you
[02:07:47.280 --> 02:07:54.800]   for that. I had asked. I use launch bar. Launch bar is still around? Oh, yeah. Yeah, launch bar is
[02:07:54.800 --> 02:08:02.080]   - that was the original App that got Sherlock. No, I'm not too sure. I mean, it's always gone on
[02:08:02.080 --> 02:08:06.000]   to do some really amazing things for me and some of the automations that I can trigger. He still
[02:08:06.000 --> 02:08:11.360]   makes it. That's nice. You might be thinking of quick silver, Leo. No, I know about quick silver
[02:08:11.360 --> 02:08:16.880]   and I know that went away, but no launch bar. I for some reason thought of it just been superseded
[02:08:16.880 --> 02:08:21.840]   by objective development. Yeah, that's a good no. I love launch bar. Another good
[02:08:21.840 --> 02:08:25.920]   choice. Very similar to Alfred and quick silver. Yes, 6-1/2 a dozen of the other options.
[02:08:25.920 --> 02:08:30.560]   Quick silver went open source and you could still use quick silver. I don't know the state of development.
[02:08:30.560 --> 02:08:32.480]   I had problems with it. I had problems with it. I found it to be stable.
[02:08:32.480 --> 02:08:36.400]   I don't know how he's turned into Mac break weekly, but it did somehow.
[02:08:36.400 --> 02:08:44.240]   I've got one thing I'd like to raise. When I was at the ITF last week, there's a working group
[02:08:44.240 --> 02:08:49.600]   draft. This is part of the ITF is that they're responsible for the encryption algorithms and
[02:08:49.600 --> 02:08:54.480]   the SSL. They're a little locked that appears in your web browser and the encryption algorithms
[02:08:54.480 --> 02:09:00.080]   and the protocols that go with that. There's a new draft coming out around TLS 1.3. Now,
[02:09:00.080 --> 02:09:07.280]   TLS 1.3 is about making sure that operators can't intercept your traffic as it crosses their networks
[02:09:07.280 --> 02:09:13.120]   and then sell your data for money. I dropped a note in here. I found the link. This is from
[02:09:13.120 --> 02:09:22.160]   ad age is the website back in 2015 talking about this being a $24 billion market for selling your
[02:09:22.160 --> 02:09:28.560]   private data as it crosses the network backbones. The ITF has been putting together a working group
[02:09:28.560 --> 02:09:33.680]   on increasing encryption on the carriers. They wanted to get more people to jump in
[02:09:33.680 --> 02:09:39.760]   and join the conversation. Now, I'm not so sure how many telco nerds or carrier nerds or crypto
[02:09:39.760 --> 02:09:45.360]   nerds are out there. If you are, it would be really great if you could go along and participate in the
[02:09:45.360 --> 02:09:51.520]   draft that's happening around this, which is the effect of pervasive encryption on operators.
[02:09:51.520 --> 02:09:58.720]   I'll drop it into the chat room. We had Vince Surf on a triangulation that one of the
[02:09:58.720 --> 02:10:02.080]   fathers of the internet. I once asked him, if you were going to do it all over again,
[02:10:02.080 --> 02:10:07.120]   designing TCP/IP and the internet, what would you do differently? He said pervasive encryption.
[02:10:08.240 --> 02:10:13.040]   They really didn't think that would be necessary. Now, of course, it's obvious that it would be.
[02:10:13.040 --> 02:10:17.040]   There's been some real Argy Barge gone on here. Apparently, some of the big carriers,
[02:10:17.040 --> 02:10:22.400]   some of the... They don't want this at all, right? No, they really opposed to it, very strongly
[02:10:22.400 --> 02:10:28.800]   opposed. There's a lot of very unusual stuff happening here where the carriers are putting
[02:10:28.800 --> 02:10:33.200]   pressure on the companies that employ the people who are doing this draft in the ITF.
[02:10:34.400 --> 02:10:38.800]   This is a big company, a big company saying, "Where your customer here,
[02:10:38.800 --> 02:10:42.800]   you really should get your person on the ITF to drop this because we are opposed to it."
[02:10:42.800 --> 02:10:46.560]   It's interesting because you would think Google would be opposed to it, but I think that they're
[02:10:46.560 --> 02:10:51.920]   engineering a heritage. Google's not opposed because they capture it all in the browser.
[02:10:51.920 --> 02:10:56.720]   They get it from Prime directly. Yeah, you say it from the thing.
[02:10:56.720 --> 02:11:04.240]   You know. What I'm trying to stop here is even when we use TLS 1.2, which is reasonably modern,
[02:11:04.240 --> 02:11:10.320]   there's still a lot of data leakage in the header. A lot of the HTTP requests and the URL is not hidden.
[02:11:10.320 --> 02:11:15.120]   So that's interesting because I just assumed, well, of course, in the United States, the law
[02:11:15.120 --> 02:11:20.080]   that protected you from your internet service provider, stooping on you, was allowed to lapse.
[02:11:20.080 --> 02:11:25.360]   The regulation was overturned at the FCC by the new administration. I just assumed that,
[02:11:25.360 --> 02:11:30.480]   well, the good news is my Google searches at least are not visible to my ISP, but that's not always true.
[02:11:31.520 --> 02:11:37.120]   No, well, some of it is and some of it isn't. So it's not as simple to say.
[02:11:37.120 --> 02:11:40.400]   That's good to know. Some of the metadata and so forth.
[02:11:40.400 --> 02:11:44.800]   Yeah, there's a certain amount of the URL is still shown and the header is not fully encrypted.
[02:11:44.800 --> 02:11:49.280]   So the next generation of TLS 1.3 will be a make pervasive encryption.
[02:11:49.280 --> 02:11:54.480]   So this is actually kind of a nice example of how when government fails
[02:11:54.480 --> 02:11:58.800]   to protect your privacy, technology can step in and do that.
[02:12:00.400 --> 02:12:04.800]   It is, except it literally comes down to the shoulders of about five people
[02:12:04.800 --> 02:12:08.240]   who are working in the working group and are being
[02:12:08.240 --> 02:12:12.720]   well, they all die mysteriously in the next six weeks, then we'll know.
[02:12:12.720 --> 02:12:16.640]   Well, these people actually having their threatened to have their jobs taken away from them and
[02:12:16.640 --> 02:12:22.640]   stuff like this, which is really not cool. And this is all volunteer work, although some of
[02:12:22.640 --> 02:12:27.040]   it's funded by companies who make it possible for these people to volunteer and so forth.
[02:12:27.040 --> 02:12:30.720]   So who are the big adversaries to this? People like Verizon or?
[02:12:30.720 --> 02:12:36.880]   Yes, Verizon, AT&T, various governments, they want to be able to monitor the data.
[02:12:36.880 --> 02:12:41.040]   Chinese government, perhaps the Russians don't want to see pervasive encryption.
[02:12:41.040 --> 02:12:43.600]   If they vanish, I'd look at the Russians.
[02:12:43.600 --> 02:12:49.520]   What about there's also a lot of companies who specialize in tapping this data and collecting
[02:12:49.520 --> 02:12:54.000]   it and analyzing it. So Cambridge Analytics, for example, which was recently was part of the
[02:12:54.000 --> 02:12:58.000]   Fiori in the government and so forth. If we start taking away their data sources,
[02:12:58.000 --> 02:13:01.600]   they get a little bit poopy. So these fights still go on.
[02:13:01.600 --> 02:13:10.800]   What's also said is that while the engineers at the ITF understand the stakes and the relevant
[02:13:10.800 --> 02:13:12.560]   details, I think end users.
[02:13:12.560 --> 02:13:13.680]   This is a real end user.
[02:13:13.680 --> 02:13:17.520]   It's really something that happens in the back room. Nobody, this happens.
[02:13:17.520 --> 02:13:21.760]   When it comes out, everybody's going to go, "Oh, isn't that what you always did?"
[02:13:21.760 --> 02:13:25.360]   And the answer is no. Data is not safe.
[02:13:25.360 --> 02:13:28.400]   And that's exactly why Vincerf wanted to build big data at the internet.
[02:13:28.400 --> 02:13:32.000]   Because that is not even a question. That's just how the internet would have worked.
[02:13:32.000 --> 02:13:36.480]   Well, we couldn't do it 30 years ago. We just didn't have enough CPU to run the crypto.
[02:13:36.480 --> 02:13:39.520]   It was built by a bunch of hippies who thought that everybody would just be nice.
[02:13:39.520 --> 02:13:40.480]   I think it's more than that.
[02:13:40.480 --> 02:13:40.480]   Nice.
[02:13:40.480 --> 02:13:42.320]   Actually, everybody.
[02:13:42.320 --> 02:13:43.440]   Everyone's going to be cool, right?
[02:13:43.440 --> 02:13:44.000]   It'd be cool.
[02:13:44.000 --> 02:13:48.160]   No, I was far simpler. It was like, we could barely make this work.
[02:13:48.160 --> 02:13:49.920]   How the hell would we add encryption to it?
[02:13:49.920 --> 02:13:51.680]   It really wasn't.
[02:13:51.680 --> 02:13:57.600]   They didn't think about encryption because they literally stumbled from problem to problem to
[02:13:57.600 --> 02:13:59.600]   problem right through the boot cycle.
[02:13:59.600 --> 02:14:06.720]   No, it wasn't anything as glorious as hippies striving for freedom. That was completely irrelevant.
[02:14:06.720 --> 02:14:10.640]   Greg, always a pleasure having you on. Thank you for staying up a little late with us.
[02:14:10.640 --> 02:14:15.760]   You can head out for the local and have another one of them pints that you seem to like so much.
[02:14:15.760 --> 02:14:19.520]   You'll find Greg at Packet.
[02:14:19.520 --> 02:14:23.120]   pushers.net. That's his great network with lots of really good podcasts.
[02:14:23.120 --> 02:14:29.120]   He's on the Internet of the real mind, both as the website and as his Twitter handle.
[02:14:29.120 --> 02:14:33.120]   Thank you, Greg. It's always a pleasure to have you on. I really appreciate it.
[02:14:33.120 --> 02:14:34.320]   Thanks so much. What a privilege.
[02:14:34.320 --> 02:14:35.040]   Yeah.
[02:14:35.040 --> 02:14:38.640]   Roberto Baldwin, he came all the way from Prague.
[02:14:38.640 --> 02:14:40.000]   No, that was great.
[02:14:40.000 --> 02:14:40.640]   That wasn't me.
[02:14:40.640 --> 02:14:41.280]   Yeah.
[02:14:41.280 --> 02:14:44.400]   He came all the way from his Devo cover band just to be with us tonight.
[02:14:44.400 --> 02:14:46.480]   All the way from Akron, Ohio.
[02:14:46.480 --> 02:14:48.080]   That's how dedicated we are.
[02:14:48.640 --> 02:14:49.680]   Is that where you're from, Akron?
[02:14:49.680 --> 02:14:50.880]   No, no, that's where Devo is from.
[02:14:50.880 --> 02:14:52.560]   Oh, but Devo is from California.
[02:14:52.560 --> 02:14:54.640]   You're a hippie.
[02:14:54.640 --> 02:14:55.680]   I'm a hippie.
[02:14:55.680 --> 02:14:56.560]   I'm probably a hippie.
[02:14:56.560 --> 02:14:56.960]   Appreciate it.
[02:14:56.960 --> 02:14:57.440]   I don't have a choice.
[02:14:57.440 --> 02:14:58.800]   That's in the water.
[02:14:58.800 --> 02:14:59.760]   Thank you, Roberto.
[02:14:59.760 --> 02:15:02.000]   Catch his work at Engadget.com, of course.
[02:15:02.000 --> 02:15:06.560]   How's it been, by the way, the new ownership and the...
[02:15:06.560 --> 02:15:07.600]   You know what?
[02:15:07.600 --> 02:15:08.720]   Did that change anything?
[02:15:08.720 --> 02:15:10.960]   You're owned by Verizon then.
[02:15:10.960 --> 02:15:11.840]   We're owned by Verizon.
[02:15:11.840 --> 02:15:13.440]   You know what? I don't think...
[02:15:13.440 --> 02:15:14.960]   Can you write bad stuff about Verizon?
[02:15:14.960 --> 02:15:17.280]   Oh, I write bad stuff out of Verizon all the time.
[02:15:18.080 --> 02:15:22.800]   You know, when Verizon bought AOL, there were a few sites that wrote,
[02:15:22.800 --> 02:15:27.440]   "Oh, how can we trust TechCrunch and Gagit too?"
[02:15:27.440 --> 02:15:30.080]   I'm going to say, "Cause you're not just..."
[02:15:30.080 --> 02:15:34.160]   When you write that sort of crap, you're not attacking a publication.
[02:15:34.160 --> 02:15:36.160]   You're attacking individual reporters.
[02:15:36.160 --> 02:15:40.000]   You're saying that these reporters don't have the journalistic integrity
[02:15:40.000 --> 02:15:44.160]   to see that Verizon is a crappy company.
[02:15:44.160 --> 02:15:44.640]   And it is.
[02:15:46.000 --> 02:15:47.360]   It works for a horrible company.
[02:15:47.360 --> 02:15:48.560]   They just kept busted.
[02:15:48.560 --> 02:15:50.240]   You are gating bandwidth to Netflix.
[02:15:50.240 --> 02:15:52.960]   No, and so I talk crap about Verizon as much as I possibly can.
[02:15:52.960 --> 02:15:53.440]   Good.
[02:15:53.440 --> 02:15:54.240]   If they fire me, I don't...
[02:15:54.240 --> 02:15:54.960]   Whatever.
[02:15:54.960 --> 02:15:55.920]   No, that's right.
[02:15:55.920 --> 02:15:58.880]   Your integrity is more valuable than your job.
[02:15:58.880 --> 02:16:02.160]   But for the most part, they've had zero input on anything we do.
[02:16:02.160 --> 02:16:03.600]   No one ever said anything.
[02:16:03.600 --> 02:16:04.800]   No one's ever come to...
[02:16:04.800 --> 02:16:07.200]   TechCrunch and Engadget don't seem in any way, you know, kind of...
[02:16:07.200 --> 02:16:10.880]   Yeah, no one's ever said anything to me, to anyone on the staff.
[02:16:10.880 --> 02:16:14.000]   So, I think they know better, especially in the world of Twitter,
[02:16:14.000 --> 02:16:15.040]   because everybody...
[02:16:15.040 --> 02:16:18.240]   If you shut somebody up on a site, then they just jump on Twitter.
[02:16:18.240 --> 02:16:22.160]   And now you have every tech reporter, every reporter in the world being like,
[02:16:22.160 --> 02:16:22.960]   "Look what you did."
[02:16:22.960 --> 02:16:23.360]   Yeah.
[02:16:23.360 --> 02:16:23.920]   Yeah.
[02:16:23.920 --> 02:16:24.400]   Yeah.
[02:16:24.400 --> 02:16:25.520]   Verizon, can you hear me now?
[02:16:25.520 --> 02:16:27.280]   Can you hear me now?
[02:16:27.280 --> 02:16:30.640]   And Mike Elgin, where were we?
[02:16:30.640 --> 02:16:31.840]   Computer world, of course.
[02:16:31.840 --> 02:16:33.200]   Computer world, that's company.
[02:16:33.200 --> 02:16:37.360]   I follow you on Facebook and Google+, it seems like you're on Facebook a lot.
[02:16:37.360 --> 02:16:38.400]   Yeah, occasionally.
[02:16:38.400 --> 02:16:41.680]   Mostly Google+, in Twitter these days, a little bit of Instagram.
[02:16:42.560 --> 02:16:47.760]   And yeah, a fast company, computer world, I just finished the second of the last chapter
[02:16:47.760 --> 02:16:49.440]   of the book, Gastronomad.
[02:16:49.440 --> 02:16:50.400]   Oh, you're writing a book?
[02:16:50.400 --> 02:16:50.720]   Yes.
[02:16:50.720 --> 02:16:52.240]   And that's going to be...
[02:16:52.240 --> 02:16:53.520]   Oh, I can't wait to read it.
[02:16:53.520 --> 02:16:55.360]   I can't wait for you to read it.
[02:16:55.360 --> 02:16:55.920]   Awesome.
[02:16:55.920 --> 02:16:59.680]   It's a lot of fun to write and should be out soon.
[02:16:59.680 --> 02:17:00.640]   This is a lifestyle.
[02:17:00.640 --> 02:17:00.960]   I would...
[02:17:00.960 --> 02:17:01.120]   Yeah.
[02:17:01.120 --> 02:17:04.800]   If we could just get internet, fast internet everywhere,
[02:17:04.800 --> 02:17:07.520]   I would just pack up the podcast gear and hit the road.
[02:17:07.520 --> 02:17:08.160]   Imagine if you...
[02:17:08.160 --> 02:17:08.800]   There's no reason I...
[02:17:08.800 --> 02:17:10.880]   Put the entire company into a self-driving car.
[02:17:10.880 --> 02:17:11.280]   Yeah.
[02:17:11.280 --> 02:17:12.240]   You could go anywhere.
[02:17:12.240 --> 02:17:12.960]   We're an industry.
[02:17:12.960 --> 02:17:16.960]   You could record your podcast without his new podcast version of the A8.
[02:17:16.960 --> 02:17:20.160]   Is there just the new A8 podcast?
[02:17:20.160 --> 02:17:21.440]   This is 15 years.
[02:17:21.440 --> 02:17:22.800]   There's a lot of those.
[02:17:22.800 --> 02:17:23.840]   That's what it's going to be.
[02:17:23.840 --> 02:17:26.320]   You're going to buy cars or versions.
[02:17:26.320 --> 02:17:28.560]   Like, oh, you're going to get the businessman version.
[02:17:28.560 --> 02:17:31.920]   You're going to get the social media butterfly version.
[02:17:31.920 --> 02:17:32.880]   You're going to get the...
[02:17:32.880 --> 02:17:33.840]   I do want that.
[02:17:33.840 --> 02:17:34.960]   I do want to get the...
[02:17:34.960 --> 02:17:36.560]   I have a kid rock version.
[02:17:36.560 --> 02:17:37.680]   You're going to get the D-Bo version.
[02:17:37.680 --> 02:17:38.640]   That's actually...
[02:17:38.640 --> 02:17:40.960]   For what it's worth, Leo, that's what we do at the Packet Pushes.
[02:17:40.960 --> 02:17:44.720]   We go to the events like the IETF and record 8 to 10 podcasts
[02:17:44.720 --> 02:17:46.560]   with the people inside these organizations.
[02:17:46.560 --> 02:17:47.200]   No, I know you do.
[02:17:47.200 --> 02:17:47.840]   It's awesome.
[02:17:47.840 --> 02:17:48.640]   Yeah.
[02:17:48.640 --> 02:17:49.760]   And I hate that.
[02:17:49.760 --> 02:17:51.520]   That's not the road I'm talking about.
[02:17:51.520 --> 02:17:55.040]   I want to get in a boat, frankly.
[02:17:55.040 --> 02:17:57.840]   I want to be an Academy-Ran in the middle of the ocean in Putt.
[02:17:57.840 --> 02:17:59.280]   What is that going to happen, Greg?
[02:17:59.280 --> 02:18:02.880]   When we're going to have high speed internet globally, worldwide.
[02:18:02.880 --> 02:18:06.480]   Well, at the moment, it's hard to see that that would ever happen.
[02:18:06.480 --> 02:18:08.240]   There's not enough spectrum.
[02:18:08.240 --> 02:18:09.520]   And it's all about latency.
[02:18:09.520 --> 02:18:11.200]   You can't change the speed of light.
[02:18:11.200 --> 02:18:11.600]   So...
[02:18:11.600 --> 02:18:15.600]   What about those project loon and the balloons that Google was talking about?
[02:18:15.600 --> 02:18:16.080]   Oh, yeah.
[02:18:16.080 --> 02:18:20.400]   Akila, the killer drone that Facebook was going to do.
[02:18:20.400 --> 02:18:21.440]   Any of that can happen?
[02:18:21.440 --> 02:18:24.240]   I don't think so.
[02:18:24.240 --> 02:18:27.200]   It's a little hard to tell at the moment.
[02:18:27.200 --> 02:18:28.880]   Like, it's possible that they are,
[02:18:28.880 --> 02:18:31.840]   but I think there's software that coordinates where the balloons are.
[02:18:31.840 --> 02:18:34.640]   And then the question is, where's your antenna pointing to?
[02:18:34.640 --> 02:18:36.080]   Is it here?
[02:18:36.080 --> 02:18:36.560]   Yeah.
[02:18:36.560 --> 02:18:37.040]   Is it there?
[02:18:37.040 --> 02:18:38.240]   Well, on a boat is here and there.
[02:18:38.240 --> 02:18:40.880]   It's going to say that my tombstone, I was going to move,
[02:18:40.880 --> 02:18:42.240]   but I was waiting for better internet.
[02:18:42.240 --> 02:18:45.440]   You just need a big fiber octa cable on a big spool.
[02:18:45.440 --> 02:18:49.280]   We're stuck in total, I think.
[02:18:49.280 --> 02:18:51.360]   You're all back in as you go back to shore.
[02:18:51.360 --> 02:18:53.360]   They're worst places to be stuck, frankly.
[02:18:53.360 --> 02:18:54.320]   No, it's not so bad.
[02:18:54.320 --> 02:18:56.640]   Hey, thank you all for being here.
[02:18:56.640 --> 02:19:00.720]   We do tweet every Sunday afternoon, 3 p.m. Pacific, 6 p.m. Eastern time.
[02:19:00.720 --> 02:19:04.320]   That's 2200 UTC in the evening if you're in the UK.
[02:19:04.320 --> 02:19:06.240]   That's why I thank Greg for staying up late.
[02:19:06.240 --> 02:19:07.520]   But I hope you will join us.
[02:19:07.520 --> 02:19:09.920]   If you join us live, don't forget to go to the chat room
[02:19:09.920 --> 02:19:13.600]   and pipe up in there at irc.twit.tv.
[02:19:13.600 --> 02:19:14.640]   You can also join us live.
[02:19:14.640 --> 02:19:18.480]   We have some great visitors from Oakland, from Germany,
[02:19:18.480 --> 02:19:20.240]   from London, England, and Columbus, Ohio.
[02:19:20.240 --> 02:19:22.160]   Thank you for visiting from all over the world.
[02:19:22.160 --> 02:19:23.200]   People come to see us.
[02:19:23.200 --> 02:19:25.120]   All you have to do is email tickets@twit.tv.
[02:19:25.120 --> 02:19:26.240]   We'll put out a chair for you.
[02:19:26.240 --> 02:19:29.120]   Make sure you're comfortable, offer you snacks.
[02:19:29.120 --> 02:19:32.800]   If you can't be here in person, if you can't be here during the live stream,
[02:19:32.800 --> 02:19:36.080]   you can always download on demand versions of all of our shows.
[02:19:36.080 --> 02:19:37.600]   Audio and video.
[02:19:37.600 --> 02:19:40.080]   The website is twit.tv.
[02:19:40.080 --> 02:19:43.440]   For this show, I think it's twit.tv/thisweekintech.
[02:19:43.440 --> 02:19:44.080]   Something like that.
[02:19:44.080 --> 02:19:45.280]   It's a longer URL.
[02:19:45.280 --> 02:19:45.840]   You'll find it.
[02:19:45.840 --> 02:19:47.440]   Just go to the front page and click shows.
[02:19:47.440 --> 02:19:49.840]   You can also, of course, subscribe.
[02:19:49.840 --> 02:19:52.240]   However, whatever podcast program you use,
[02:19:52.240 --> 02:19:54.000]   just look for Twit and subscribe.
[02:19:54.000 --> 02:19:56.080]   That way you'll get every episode every week.
[02:19:56.080 --> 02:20:00.000]   And you can start your week right with a little dose of tech talk.
[02:20:00.000 --> 02:20:00.720]   Thanks for being here.
[02:20:00.720 --> 02:20:01.600]   We'll see you next time.
[02:20:01.600 --> 02:20:03.920]   Another Twit is in the can.
[02:20:03.920 --> 02:20:04.320]   Bye-bye.
[02:20:05.280 --> 02:20:06.320]   Bye-bye.
[02:20:06.320 --> 02:20:08.320]   [applause]
[02:20:08.320 --> 02:20:19.760]   [music]

