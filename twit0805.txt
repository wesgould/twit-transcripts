
[00:00:00.000 --> 00:00:06.600]   It's time for Twit This Week in Tech. A very difficult week, but we have three really
[00:00:06.600 --> 00:00:12.960]   great people who will help us understand everything. Amy Webb, our futurist, Dan Gilmour, an
[00:00:12.960 --> 00:00:19.920]   expert on journalism and media literacy, and from the big technology podcast and newsletter,
[00:00:19.920 --> 00:00:25.360]   Alex Cantrowitz. We're going to talk, of course, about the bands of the president on pretty
[00:00:25.360 --> 00:00:31.720]   much every social media network. Is this the right direction to go? Is Big Tech too powerful?
[00:00:31.720 --> 00:00:40.440]   Who's in charge and why? We'll talk about something Amy is an expert on. China nationalizing
[00:00:40.440 --> 00:00:46.280]   one of its biggest corporations, Alibaba. And what that really means will celebrate the
[00:00:46.280 --> 00:00:52.840]   birthday of Wikipedia and the iPhone. There's lots to talk about and Twit is next.
[00:00:52.840 --> 00:00:55.480]   [Music]
[00:00:55.480 --> 00:01:01.840]   Podcasts you love from people you trust. This is Twit.
[00:01:01.840 --> 00:01:07.560]   [Music]
[00:01:07.560 --> 00:01:17.040]   This is Twit. This Week in Tech. Episode 805 recorded Sunday January 10th, 2021. Come together
[00:01:17.040 --> 00:01:23.400]   by Voltron. This Week in Tech is brought to you by Barracuda. Did you know that 91% of
[00:01:23.400 --> 00:01:30.000]   all cyber attacks start with an email to uncover the threats hiding in your office 365 account?
[00:01:30.000 --> 00:01:39.720]   Get a secure and free email threat scan at barracuda.com/twit. And by ExpressVPN. Stop
[00:01:39.720 --> 00:01:45.280]   handing over your personal data to ISPs who mine your activity and sell off your information
[00:01:45.280 --> 00:01:52.440]   for three extra months free with a one year package. Go to expressvpn.com/twit. And by
[00:01:52.440 --> 00:01:58.160]   Mint Mobile switching to Mint Mobile's premium wireless service is the easiest way to save
[00:01:58.160 --> 00:02:04.240]   this year. Maximize your savings with plans starting at just $15 a month and get the plans
[00:02:04.240 --> 00:02:13.120]   shipped to your door for free at mintmobile.com/twit. And by the TriCaster 2 Elite from New Tech,
[00:02:13.120 --> 00:02:17.520]   the most complete live production system on the planet. There's a TriCaster for every
[00:02:17.520 --> 00:02:23.440]   production including yours including ours. Go to new tech.com/tricaster for an interactive
[00:02:23.440 --> 00:02:34.480]   guide that will help you choose which TriCaster is right for you.
[00:02:34.480 --> 00:02:38.640]   It's time for Twit this Week in Tech. The show where we talk about the week's tech news and
[00:02:38.640 --> 00:02:45.120]   oh boy is there it's their news. But I have the perfect panel assembled for this particular
[00:02:45.120 --> 00:02:51.760]   wild week. Let's start with Amy Webb who is a futurist with the Future Today Institute
[00:02:51.760 --> 00:02:59.280]   author of a number of really good books about big tech. Amyweb.io and Amy I have
[00:02:59.280 --> 00:03:07.520]   my Future Today Magic 8 Ball with me in case we need it if it should come up. You did a
[00:03:07.520 --> 00:03:13.360]   great calendar and I forgot to bring it in but it's beautiful for 2021. You hired 12 different
[00:03:13.360 --> 00:03:19.120]   illustrators to talk about trends, future trends and illustrate them and it's really beautiful.
[00:03:19.120 --> 00:03:25.040]   I just really love it. Is that available somewhere? Yeah, most of our stuff is free. So we made a
[00:03:25.040 --> 00:03:31.680]   speculative travel calendar so it's a speculative design futures project. Every month is a different
[00:03:31.680 --> 00:03:39.040]   country so it looks like an old school beautiful 1950s travel poster collection. But if you look
[00:03:39.040 --> 00:03:48.320]   closely every single month tells us something about technology, biology things about the future
[00:03:48.320 --> 00:03:55.440]   and it starts January as a trip to nowhere. These airlines are now flying trips to nowhere.
[00:03:56.800 --> 00:04:02.400]   For wealthy people. Anyhow, so it's all available on our Future Today Institute website. You can
[00:04:02.400 --> 00:04:06.960]   download the whole thing. All the travel posters, the calendar. Cool. That's really cool. Future
[00:04:06.960 --> 00:04:14.640]   Today Institute.com. Also want to welcome back. It's been a long time but I still consider him
[00:04:14.640 --> 00:04:24.560]   one of the premier experts on media today. Dan Gilmore, a long time reporter at the Mercury News
[00:04:24.560 --> 00:04:29.440]   who's now kind of focused on media literacy. Hi, Dan. It's great to see you again.
[00:04:29.440 --> 00:04:34.960]   Fine to be back. It has been a long time. It's been too long and I apologize but we
[00:04:34.960 --> 00:04:41.840]   have a thrill to have you on. And again, perfect timing. And we also want to welcome some of the
[00:04:41.840 --> 00:04:48.480]   brand new longtime tech reporter Alex Cantrowitz who now is doing his own thing on substech.
[00:04:48.480 --> 00:04:54.240]   Bigtechnology.substack.com. It's also the big technology podcast. It is great to meet you,
[00:04:54.240 --> 00:04:58.720]   Alex. I've seen you many times on Tech News Weekly and I'm so glad we could get you on Twitch as well.
[00:04:58.720 --> 00:05:04.080]   Thank you, Leo. Yeah, it's a thrill here. And when I was starting out to do my own thing,
[00:05:04.080 --> 00:05:09.120]   I was trying to learn from the best. And I read one of the formative documents for me was reading
[00:05:09.120 --> 00:05:16.560]   your guide on how to build a successful podcast. And I really kept that in mind. And so I appreciate
[00:05:16.560 --> 00:05:19.760]   your advice there and it's a thrill to be on with you right now. It's very kind of you. And
[00:05:19.760 --> 00:05:25.680]   you'll really appreciate it. SQL, how to do a failing podcast in. I'm going on the way on that.
[00:05:25.680 --> 00:05:32.160]   That's it. Turns out pretty easy. No, it's just been it's been it's not failing but it's been a
[00:05:32.160 --> 00:05:38.000]   tough year. Thanks to COVID and advertisers being very, I don't think it's just COVID. In fact,
[00:05:38.000 --> 00:05:44.080]   I've noticed as the year went on that they got more and more nervous about the future. And while
[00:05:44.080 --> 00:05:50.400]   I don't think stopping advertising is probably the best way to face the future, I understand
[00:05:50.400 --> 00:05:55.040]   budgets are tight and a lot of businesses are going, yeah, where can we cut the first thing you
[00:05:55.040 --> 00:06:01.120]   cut? The first thing you cut is a podcast. I would say smart advertisers should play some
[00:06:01.120 --> 00:06:06.720]   podcast and particularly here, you don't get a more intimate relationship with people who are
[00:06:06.720 --> 00:06:11.440]   consuming media outside of podcasts. So you know that and I know that. That's right.
[00:06:12.800 --> 00:06:18.320]   And we do fortunately, we do have a number of number of very loyal advertisers who feel the
[00:06:18.320 --> 00:06:24.240]   same way. Thank you. We're glad that we could we could still do this show. I do not, you know,
[00:06:24.240 --> 00:06:30.320]   I'm just going to say a caveat upfront. While we occasionally get political on all of our shows,
[00:06:30.320 --> 00:06:37.200]   we're not here to be political. We're not here to flog a political viewpoint.
[00:06:38.400 --> 00:06:45.040]   But more and more recently, over the last five years, for sure, there's been an intersection
[00:06:45.040 --> 00:06:50.960]   between politics and technology. And it's hard to avoid that, especially because this was the week
[00:06:50.960 --> 00:06:58.000]   that the president of the United States got himself banned on Twitter, Facebook, Spotify,
[00:06:58.000 --> 00:07:06.400]   YouTube, even Pinterest and TikTok for inflammatory comments. And I guess, I guess the first thing,
[00:07:08.240 --> 00:07:14.320]   I would like to ask all three of you. I mean, I think that there's probably no question
[00:07:14.320 --> 00:07:25.840]   that when anybody president or not says things that incite violence and really cause
[00:07:25.840 --> 00:07:34.000]   problems, that it's not unreasonable to censure them in some way. And Twitter has done that
[00:07:34.000 --> 00:07:40.800]   with with warning messages and now completely banning the president. But it also raises, in some
[00:07:40.800 --> 00:07:50.640]   people's minds, the specter of big tech suddenly weighing in on political issues. And and it really
[00:07:50.640 --> 00:07:56.960]   brings home the power big tech has, we saw stories that the presidents, people, press people,
[00:07:56.960 --> 00:08:02.320]   were casting around, where can we put his videos? We don't, there's nowhere we could put his videos.
[00:08:02.320 --> 00:08:09.840]   And maybe they were looking perhaps at conservative alternatives like Gabb and Parler. But even those
[00:08:09.840 --> 00:08:15.680]   alternatives are slowly being shut down. Both Gabb and Parler were under threat. Parler ended up
[00:08:15.680 --> 00:08:20.560]   being pulled from the Google App Store first and then later from the Apple App Store,
[00:08:20.560 --> 00:08:26.640]   which by the way caused it to be the number one app as hundreds of thousands of people downloaded
[00:08:26.640 --> 00:08:32.240]   it quickly before it got banned. And now the latest is at midnight tonight Pacific Time.
[00:08:32.240 --> 00:08:37.680]   Amazon Web Services says we're going to pull Parler down because there's too many
[00:08:37.680 --> 00:08:46.400]   posts like the one that says let's all kill the vice president. We don't feel right serving that.
[00:08:46.400 --> 00:08:51.920]   So let me start with you Alex, because you cover big technology. This is something big technology
[00:08:51.920 --> 00:09:02.320]   is resisted. But all, you know, all the resistance was over after Wednesday. What's your take on that?
[00:09:02.320 --> 00:09:07.680]   Well, look, there's there's a reason why they resisted this. They knew that if they were going
[00:09:07.680 --> 00:09:16.000]   to make this move, they were going to form a good amount of antitrust sentiment in the House and
[00:09:16.000 --> 00:09:21.120]   the Senate in a way that they hadn't previously and they were already on the hot seat. You know,
[00:09:21.120 --> 00:09:27.760]   for being mildly interventionist, not taking action like removing the president's Twitter account.
[00:09:27.760 --> 00:09:32.080]   And I think what they're looking at right now is something pretty serious for their business,
[00:09:32.080 --> 00:09:38.960]   which is that you have the House Democrats and the Senate who basically got turned onto antitrust
[00:09:38.960 --> 00:09:44.720]   after the 2016 election where they felt that big tech had too much power and blamed it for
[00:09:44.720 --> 00:09:50.960]   the election of Donald Trump and had taken antitrust investigations against big tech very seriously.
[00:09:51.280 --> 00:09:56.000]   I mean, really looking deeply into this stuff and getting further than Congress had ever gotten
[00:09:56.000 --> 00:10:02.320]   before. So you already had that brewing with congressional Democrats. Now you're going to have
[00:10:02.320 --> 00:10:09.200]   them joined with a group of Republicans who are furious over what's happened. And they knew that
[00:10:09.200 --> 00:10:13.680]   this would happen. And that's why they resisted it. That's why you saw moments where Trump would say,
[00:10:13.680 --> 00:10:19.200]   you know, when the looting starts, the shooting starts or any other form of, you know, attempts
[00:10:19.200 --> 00:10:24.080]   to delegitimize the 2020 election, they let that side. It was only until this moment where they
[00:10:24.080 --> 00:10:28.480]   realized there would be, you know, basically impossible for them to face their employees and to face
[00:10:28.480 --> 00:10:34.560]   their board if they allowed him to stay on. They waited until this moment. So it was almost as if
[00:10:34.560 --> 00:10:40.080]   their hand was forced at the very last minute here. They obviously took action to ban him.
[00:10:40.080 --> 00:10:45.760]   Facebook might let him back on. But I think ultimately what the result of this is is a very strong
[00:10:45.760 --> 00:10:52.960]   bipartisan push for serious new antitrust legislation that's going to come for these companies.
[00:10:52.960 --> 00:10:59.040]   And yeah, I don't think anybody disputes that if somebody is advocating violence,
[00:10:59.040 --> 00:11:03.440]   particularly the violent takeover of Congress, that that's something that should not be
[00:11:03.440 --> 00:11:09.680]   permitted on these things. Right, Dan? I mean, this is very challenging. Mike Masnick wrote a
[00:11:09.680 --> 00:11:15.360]   really interesting piece, his headline, not easy, not unreasonable, not censorship,
[00:11:15.360 --> 00:11:23.600]   the decision to ban Trump from Twitter, which, you know, the problem is the goalposts are constantly
[00:11:23.600 --> 00:11:32.800]   moving. Yeah. Well, Twitter had for a long time what we might call the presidential exception
[00:11:33.360 --> 00:11:39.600]   to the rules. Basically, he's the president. So we're going to let him break the rules.
[00:11:39.600 --> 00:11:47.600]   It's not on reason. I you can make a case. And I had been assuming that on January 20th,
[00:11:47.600 --> 00:11:55.440]   we were going to learn about a new ex presidential exception. But Twitter for its
[00:11:56.560 --> 00:12:04.160]   own reasons and their private company, they can do what they want. That is to say they're not the
[00:12:04.160 --> 00:12:12.240]   government. And it's certainly justifiable on many grounds that they said enough is enough
[00:12:12.240 --> 00:12:19.440]   and stopped it. The issue and what Alex was talking about is really relevant. This is
[00:12:20.720 --> 00:12:27.760]   my question. And while I certainly agree that the companies have the right to edit, to
[00:12:27.760 --> 00:12:35.520]   manage their spaces any way they want, the question I have is do we really want
[00:12:35.520 --> 00:12:44.160]   a small number of giant enterprises to decide what is allowed in the way of speech
[00:12:44.160 --> 00:12:50.240]   in the United States in the giant public square we call the internet. And they do more
[00:12:50.240 --> 00:13:01.360]   than not make those decisions. I worry a lot about the consequences of the brigade of do something
[00:13:01.360 --> 00:13:07.360]   people who have been screaming at platforms to do something about all this. And when they get
[00:13:07.360 --> 00:13:13.600]   their way, they may long in the long run wonder why they want to quite that much intervention.
[00:13:15.680 --> 00:13:21.120]   We're not thinking this stuff through. And I'm quite worried about it. I mean, look,
[00:13:21.120 --> 00:13:27.520]   it's not as if the president of the United States doesn't have. It's a bully pulpit, as Teddy
[00:13:27.520 --> 00:13:33.200]   Frisvelt said. He has plenty of ways he can express his opinion. There's a lot of conservative
[00:13:33.200 --> 00:13:38.480]   talk radio, they're conservative television networks. There are plenty of places he can tell the
[00:13:38.480 --> 00:13:43.120]   world what he thinks. I had mixed feelings too, because at least when he's tweeting,
[00:13:43.120 --> 00:13:47.200]   that gives you some insight into what his thinking is and what he's planning and what he's up to.
[00:13:47.200 --> 00:13:56.560]   In some ways, going dark on the president at this point is a scary thought, not a reassuring
[00:13:56.560 --> 00:14:01.360]   thought. And yet at the same time, I think it's completely reasonable if you're a social network
[00:14:01.360 --> 00:14:07.120]   to say, yeah, but you can't incite there are certain things even the president can't do. And
[00:14:07.120 --> 00:14:12.560]   the violent overthrow of the American government is definitely in that category. Amy, what do you
[00:14:12.560 --> 00:14:20.560]   think? I have a lot of thoughts. I bet you do. We all do, obviously. I do. Let me start,
[00:14:20.560 --> 00:14:26.160]   because I think disclosures, in my case, are important by saying I am politically independent.
[00:14:26.160 --> 00:14:31.600]   We do quite a bit of work with different federal government agencies and with the military.
[00:14:31.600 --> 00:14:36.800]   And obviously with lots of other different companies. So with that being said,
[00:14:39.360 --> 00:14:49.520]   so Alex mentioned the antitrust enthusiasm in the wake of the 2016 elections. And it actually
[00:14:49.520 --> 00:14:57.840]   predates that. So Elizabeth Warren and some others have been charging now for years that
[00:14:57.840 --> 00:15:04.480]   our big tech companies have amassed a considerable amount of power and wield tremendous influence.
[00:15:05.280 --> 00:15:14.240]   If this was an ecosystem in the natural world, we would relate the big tech companies to a
[00:15:14.240 --> 00:15:20.640]   keystone species, meaning that these companies have outsized influence now on the places in which
[00:15:20.640 --> 00:15:26.080]   they operate and the things that they do have serious consequences and reverberations.
[00:15:27.920 --> 00:15:38.320]   So as I see it, we have three core problems related to what happened last week and they're all tech
[00:15:38.320 --> 00:15:44.000]   tech related. So the first core problem, Dan mentioned we've got a relatively small group of
[00:15:44.000 --> 00:15:48.720]   people who are making decisions for all of us. And that is true that that group of people is
[00:15:48.720 --> 00:15:54.080]   based in the Valley. We also have a small group of people in Washington, D.C., making decisions
[00:15:54.080 --> 00:16:01.600]   that impact all of us. And that's the Supreme Court. There's a tension inherent between the
[00:16:01.600 --> 00:16:07.600]   decisions that are being made in response to the evolving nature of our technology,
[00:16:07.600 --> 00:16:15.200]   speech and society by the technology companies and the static laws of the United States,
[00:16:15.200 --> 00:16:22.080]   laws that are not intended to change. There's this tension between the two. And resolving that
[00:16:22.080 --> 00:16:27.200]   tension I think is not just critical to get through where we are right now, but it's going to have
[00:16:27.200 --> 00:16:37.120]   to be the cornerstone of how law evolves over the next couple of years. Because if that doesn't happen,
[00:16:37.120 --> 00:16:41.520]   then the mechanisms by which we make a lot of decisions no longer make sense. So part of what's
[00:16:41.520 --> 00:16:50.400]   happening right now is an indiscriminate use of terms of service. The big long post that Zuckerberg
[00:16:50.400 --> 00:16:56.240]   relayed the letter from Twitter, what they are doing is trying to make legalistic
[00:16:56.240 --> 00:17:02.400]   arguments having to do with the terms of service. And the problem is that there was a study from
[00:17:02.400 --> 00:17:08.960]   Carnegie Mellon a couple of years ago showing that if we stopped to read all of the terms of
[00:17:08.960 --> 00:17:17.680]   service that are presented to us, it would take 76 work days. So eight hours a day, 15 work weeks
[00:17:17.680 --> 00:17:25.600]   because the median length of privacy policies is around 2,500 words. Now, if you stop, and that's
[00:17:25.600 --> 00:17:31.120]   just so if you assume that you are reading the TOS and you're thinking about what you are agreeing
[00:17:31.120 --> 00:17:39.120]   to, right? You could even go further and show that in terms of work lost in exchange for us
[00:17:39.120 --> 00:17:43.920]   reading all of these terms of service, it's like hundreds of billions of dollars. And my point is
[00:17:43.920 --> 00:17:49.920]   that tech companies are making up the interpretations of these terms of service as they go along,
[00:17:49.920 --> 00:17:57.920]   which is in conflict with the laws that they are intended to respond to. So indiscriminate
[00:17:57.920 --> 00:18:04.960]   application of terms of service to some extent is how we got to now. And ultimately, that's what
[00:18:04.960 --> 00:18:11.920]   took some of these key figures off of the networks. But in some way, they don't matter because it's
[00:18:11.920 --> 00:18:18.480]   not just Trump who is the problem here, it's algorithmic determinism. It's the systems that are
[00:18:18.480 --> 00:18:24.880]   designed to pull content not just from him, but now from all of these other places. So Trump may
[00:18:24.880 --> 00:18:30.720]   be gone and AWS may have taken parlor offline, but the problem is the infrastructure supporting all
[00:18:30.720 --> 00:18:36.080]   of this. And I will say one last thing, and that has to do with world building, because all of this
[00:18:36.080 --> 00:18:42.480]   reached a sort of fever pitch in the past couple of days. And again, I say this as somebody who's
[00:18:42.480 --> 00:18:49.280]   politically independent, this, the people surrounding Donald Trump have done a masterful job of world
[00:18:49.280 --> 00:18:56.800]   building. They created something called Antifa. They created something called QAnon, which may
[00:18:56.800 --> 00:19:01.680]   have been bubbling up here and there, you know, may have had little fits and spurts, but certainly
[00:19:01.680 --> 00:19:09.680]   if they created it, they certainly fostered it. This Antifa, first of all, it makes me crazy that
[00:19:09.680 --> 00:19:16.000]   the way that it's being pronounced is Antifa. There is no long sound in the word Antifa, but
[00:19:16.000 --> 00:19:25.040]   whatever. The point is that it's short for anti-fascist, Antifa, and whatever. It doesn't matter.
[00:19:25.040 --> 00:19:30.960]   It's made up anyway. So here's why it does matter, because I was at the State Department
[00:19:31.840 --> 00:19:40.000]   in 2012 through 2016, as were others, talking about intentional misinformation and how this has
[00:19:40.000 --> 00:19:45.200]   significant next-order outcomes, and it would be good to red team some of this in advance to
[00:19:45.200 --> 00:19:50.640]   think these things through. And it just didn't happen. So we really have to think hard, because
[00:19:50.640 --> 00:19:56.480]   just taking some of these people offline and taking parlor and gab, you know, out of the play store,
[00:19:57.200 --> 00:20:00.800]   there are mirror sights already. Like somebody created a mirror sight of a subreddit that it
[00:20:00.800 --> 00:20:06.160]   got in takedown that this doesn't- >> It's virtually impossible to eliminate this stuff.
[00:20:06.160 --> 00:20:11.200]   >> That's right. That's right. So now we have three core problems that are unfortunately,
[00:20:11.200 --> 00:20:18.400]   for my vantage point, not going away. And the compounding effect here is that we've got a
[00:20:18.400 --> 00:20:24.800]   transition of power. It's cold in a lot of places where Americans live, and we have no holidays to
[00:20:24.800 --> 00:20:27.760]   look forward to. >> January's always sucks.
[00:20:27.760 --> 00:20:32.080]   But this tells us, and we've got COVID that's- >> Yeah.
[00:20:32.080 --> 00:20:38.560]   >> So we have to really think through not just what happened last week in a vacuum, but also,
[00:20:38.560 --> 00:20:45.920]   the mechanisms that we have to deal with some of this no longer make sense is the challenge.
[00:20:45.920 --> 00:20:50.560]   >> Well, and despite all the rules of engagement in the terms of service,
[00:20:51.600 --> 00:20:58.480]   it doesn't seem to be enforced equitably across the board. We all have experience of
[00:20:58.480 --> 00:21:05.520]   Twitter being used for blatantly illegal threats and so forth. And it's very difficult to get
[00:21:05.520 --> 00:21:16.720]   those taken down, if at all. It almost feels like the rules are applied, and this is Mike
[00:21:16.720 --> 00:21:22.560]   Masnick's point. They're applied completely ad hoc at will. And in a way, that's more scary,
[00:21:22.560 --> 00:21:28.240]   because it's Jack Dorsey and Mark Zuckerberg and their elk who are deciding things that are
[00:21:28.240 --> 00:21:34.160]   pretty important. And yet, I don't know of any other mechanism to do this. Dan,
[00:21:34.160 --> 00:21:41.760]   is you focus on media literacy? Is that a solution? Sometimes people say, well,
[00:21:41.760 --> 00:21:49.760]   he can't blame the platforms, blame the people using them. >> Well, let me start off by a
[00:21:49.760 --> 00:22:00.400]   disclosure or two on my own. I have been vehemently worried about Trump since before he was president
[00:22:00.400 --> 00:22:07.120]   and loudly and publicly warning about the danger that I thought he represented to the republic.
[00:22:07.840 --> 00:22:15.040]   And secondly, the work that we're doing at Arizona State University News collab, which is
[00:22:15.040 --> 00:22:23.760]   the thing I work on, has gotten some funding from social media companies like Facebook.
[00:22:23.760 --> 00:22:30.800]   And we are involved in a Defense Department project and some other things. So, my disclosure
[00:22:30.800 --> 00:22:39.920]   is all on my website and I welcome people to look at that. If we think about it again
[00:22:39.920 --> 00:22:51.600]   from the notion that this ad hoc response to these problems that are coming up, it's exactly
[00:22:51.600 --> 00:22:58.320]   right. Mike Masnick, whose tech dirt site is must read from my perspective. He's correct that
[00:22:58.320 --> 00:23:08.160]   basically companies wait for a big enough, loud enough, public enough problem and then do something
[00:23:08.160 --> 00:23:16.000]   about that. Because it's impossible to moderate that scale. It can't be done. There is literally
[00:23:16.000 --> 00:23:24.160]   no way to do it except through bad AI that doesn't work well. It's incredibly hard. So,
[00:23:25.440 --> 00:23:32.720]   it is natural that they're going to wait for the biggest problem to show up and then do something
[00:23:32.720 --> 00:23:38.080]   about that. And meanwhile, a lot of things that are much worse don't get noticed except by the
[00:23:38.080 --> 00:23:44.400]   people who become the victims of those. The Myanmar Hunsa, president Duterte in the Philippines,
[00:23:44.400 --> 00:23:50.560]   Modi in India, the leader of the Taliban, the leader of Iran all have a bully pulpit on Twitter.
[00:23:51.120 --> 00:23:59.440]   Still, it's so Facebook. But what I'm as worried about rules that would basically drive out almost
[00:23:59.440 --> 00:24:08.000]   everything that's interesting as well as the bad stuff. And we've got to be cautious about how
[00:24:08.000 --> 00:24:15.280]   we try to attack this. And by the way, no one's mentioned the role, the pernicious role of big
[00:24:15.280 --> 00:24:24.080]   journalism, as I call it, which does not just include the right-wing networks like Fox and others.
[00:24:24.080 --> 00:24:34.320]   But keep in mind, for years until quite recently, our biggest journalism organizations
[00:24:34.320 --> 00:24:40.800]   happily parroted what serial liars were telling them every day. And occasionally,
[00:24:40.800 --> 00:24:44.880]   say, by the way, that's not true. And doing it because it was good for ratings, I might point out.
[00:24:44.880 --> 00:24:51.360]   Well, not of any ideology. I think that's part of it. I think there's also
[00:24:51.360 --> 00:25:00.080]   journalistic norms, which are clearly out of date when one side of a, it's not a debate. It's,
[00:25:00.080 --> 00:25:06.720]   you know, there's something that's real and something that's not. But when when party in power says,
[00:25:08.000 --> 00:25:16.560]   the sky is green. There's no journalistic obligation to publish that or to broadcast that. And
[00:25:16.560 --> 00:25:23.520]   yet the norms of journalism were hacked against the republic. And there's very good data now from
[00:25:23.520 --> 00:25:31.600]   a Harvard study and from a Cornell study that's strongly suggest that more the public got more
[00:25:31.600 --> 00:25:38.000]   misinformation via regular television than it did from the social network.
[00:25:38.000 --> 00:25:38.560]   Absolutely.
[00:25:38.560 --> 00:25:46.560]   In the last year or so, it's not conclusive data, but it's pretty persuasive to me.
[00:25:46.560 --> 00:25:55.760]   I think you lump it all together into one kind of modern, and I think you could say social media
[00:25:55.760 --> 00:26:02.720]   is, and mainstream media, there's this, it's a modern condition of just kind of a barrage
[00:26:02.720 --> 00:26:10.880]   of material factual and nonfactual that makes it almost impossible to have any
[00:26:10.880 --> 00:26:14.400]   reasonable discourse, even among people who disagree.
[00:26:14.400 --> 00:26:21.280]   When the Wall Street Journal's editorial board says Trump should resign, the media interpretation
[00:26:21.280 --> 00:26:28.000]   is that, well, Rupert Murdoch has turned against him. I mean, he wields a lot of power as probably
[00:26:28.000 --> 00:26:33.680]   as much as Twitter and Facebook does. Kevin Rus writing in the New York Times, in pulling
[00:26:33.680 --> 00:26:40.720]   Trump's megaphone Twitter shows where power now lies. And I think that's to me, that's
[00:26:40.720 --> 00:26:48.240]   really where the tech conversation goes. When you talk about big tech, Alex,
[00:26:49.760 --> 00:26:53.920]   what is the context? Are you saying, what is your feeling? Are they too powerful?
[00:26:53.920 --> 00:26:57.760]   There's certainly you could point to a lot of good that has come from big tech.
[00:26:57.760 --> 00:27:02.160]   I don't want to throw out the baby with the bathwater.
[00:27:02.160 --> 00:27:05.840]   Or should we? What do you do? I can't wait.
[00:27:05.840 --> 00:27:10.080]   I mean, that's the question that we're going to be focusing on, at least for the next two years,
[00:27:10.080 --> 00:27:16.080]   and probably longer, given that the antitrust cases are now underway against Facebook and Google
[00:27:16.080 --> 00:27:19.840]   at the FTC and the Department of Justice. And believe me, after this week,
[00:27:19.840 --> 00:27:23.760]   Apple and Amazon are definitely on their way for cases of their own.
[00:27:23.760 --> 00:27:31.600]   So, look, I don't view it. These are tough questions without simple answers. Are they too
[00:27:31.600 --> 00:27:36.320]   powerful? Yes, no doubt about it. These are enormous companies. I mean, if you think about it,
[00:27:36.320 --> 00:27:42.400]   as we've been through this pandemic, Facebook, Amazon, Apple, Google, and Microsoft have made up,
[00:27:42.400 --> 00:27:50.240]   at times, 25% or more, of the S&P 500. That's supposed to be a long, very diverse portfolio of
[00:27:50.240 --> 00:27:54.480]   the companies that really make up the American economy. So, what does it mean when there are five
[00:27:54.480 --> 00:28:00.640]   companies that end up taking up a quarter of that space? It means that the government can't let
[00:28:00.640 --> 00:28:07.600]   them fail in certain ways, because if these companies were to seriously decline, then you could end up
[00:28:07.600 --> 00:28:13.040]   taking a chunk out of the market. So, it means that even when there is accountability, it has to be
[00:28:13.040 --> 00:28:17.040]   done with a scalpel, right? It can't be done with a sledgehammer. That's my problem. I don't know
[00:28:17.040 --> 00:28:21.760]   where there is a solution for this. Amy wrote the book, The Big Nine, which includes, besides those
[00:28:21.760 --> 00:28:31.280]   five, 10 cent bidu in Alibaba. What is that changed by China's censorship of Alibaba?
[00:28:31.280 --> 00:28:36.560]   Well, this is, I got to tell you, to me, I mean, as disastrous as what we saw in the
[00:28:36.560 --> 00:28:42.320]   Capitol last week, the news story that really went under reported and unnoticed was China's
[00:28:42.320 --> 00:28:48.240]   provocative move just after Christmas to nationalize Alibaba. For those of you not familiar with
[00:28:48.240 --> 00:28:56.960]   Alibaba, it's an enormous conglomerate on par with Amazon in many ways. But let me give you a quick-
[00:28:56.960 --> 00:28:59.040]   And Jack Ma disappeared.
[00:29:01.840 --> 00:29:07.920]   I don't know that he went disappeared. I don't think the government disappeared him,
[00:29:07.920 --> 00:29:11.520]   but he certainly had the good sense to shut up and lay low.
[00:29:11.520 --> 00:29:18.800]   This was the beginning of the catastrophic scenario that I wrote about in The Big Nine,
[00:29:18.800 --> 00:29:24.640]   and that I turns my stomach to see we are inching closer to every day. But here's a really
[00:29:24.640 --> 00:29:31.440]   interesting insight into Alibaba. So, just before double 11, which is singles day, so November 11,
[00:29:31.440 --> 00:29:37.200]   every year, big shopping holiday across China. It's a sort of national holiday. It's also a time
[00:29:37.200 --> 00:29:45.920]   when Alibaba rolls out some of its new AI. Last year, Alibaba, some of the new AI systems were
[00:29:45.920 --> 00:29:52.000]   using a new much more sophisticated type of predictive analytics to understand in advance,
[00:29:52.000 --> 00:29:56.560]   not just based on prior shopping habits, but all types of other data sources, which are now being
[00:29:56.560 --> 00:30:03.680]   collected by a panopticon, as well as behavioral data. All of those data were collected to try
[00:30:03.680 --> 00:30:10.160]   to predict in advance what people would shop for on that day. I remember reading that story as
[00:30:10.160 --> 00:30:15.920]   like a human interest story. Isn't that cool? Isn't that interesting? Yeah, well, it's really
[00:30:15.920 --> 00:30:23.600]   not though. They were able to predict with 80% accuracy, 500,000 people's orders, they pre-packed
[00:30:23.600 --> 00:30:29.520]   those boxes and got them ready for shipment. So, from an e-commerce point of view, what a win.
[00:30:29.520 --> 00:30:33.920]   But think if you stop for a moment and ask yourself what it took to make that happen.
[00:30:33.920 --> 00:30:39.920]   And now that this is no longer in as much as it was an independent company because it was domiciled
[00:30:39.920 --> 00:30:48.480]   in China, the fact that this is now nationalized should be, it should send shivers up your spine.
[00:30:48.480 --> 00:30:56.480]   And now let's go one step further and say, what if Parler instead of Azure and Google and Amazon
[00:30:56.480 --> 00:31:00.640]   won't allow it to happen? What if it went over to Ollie Cloud? Oh, wow.
[00:31:00.640 --> 00:31:05.520]   Would have to try to host it Parler. It would be a, so now here's- Or the Russian equivalent,
[00:31:05.520 --> 00:31:11.760]   which I'm sure exists. China would be a more realistic equivalent, but here's the interesting
[00:31:11.760 --> 00:31:18.000]   thing. It would be a provocation, but it's not exactly a digital act of war. I mean, it is
[00:31:18.480 --> 00:31:25.040]   totally plausible that all of these sites and people that are getting kicked off of our networks
[00:31:25.040 --> 00:31:32.400]   wind up on a Chinese network, which we wouldn't be able to access. And then think of the data
[00:31:32.400 --> 00:31:38.480]   stores. There's something called Siphius. But if you're using that would prevent something like
[00:31:38.480 --> 00:31:46.000]   that in the US. The Committee for Foreign Investment in the US. The cognitive dissonance of all of
[00:31:46.000 --> 00:31:54.320]   the Parler crowd fleeing to China virtually is like, whoa, it makes my headspace.
[00:31:54.320 --> 00:32:00.000]   Well, but they may not know. I mean, if somebody say Ollie Baba to any, like they're not going to-
[00:32:00.000 --> 00:32:03.840]   Half the people who are clicking on Russia Today's story is a little green box with an
[00:32:03.840 --> 00:32:06.640]   R&B. I don't know. That's right. No idea. It's part of the Kremlin.
[00:32:06.640 --> 00:32:14.240]   Yeah. And Amy is right about the way about the- Most Americans have no idea the power and
[00:32:14.240 --> 00:32:22.080]   breadth of the big companies in China. And I had never expected Jack Ma to run a foul of the
[00:32:22.080 --> 00:32:27.600]   government because he had been so much in league with the government for so long. I was totally
[00:32:27.600 --> 00:32:36.880]   shocked that that happened. He went too far. Well, China is doing its best to anger everybody
[00:32:36.880 --> 00:32:42.400]   right now who's outside of China. It's amazing. What did Jack Ma do, Amy?
[00:32:43.040 --> 00:32:48.240]   This was not Jack Ma doing anything. This is so much like our big tech make up a disproportionate
[00:32:48.240 --> 00:32:54.880]   part of the S&P in the United States. The economic prowess of not just Ollie Baba, but again,
[00:32:54.880 --> 00:32:58.800]   this is a huge ecosystem. So there's something called Ant Financial. There's many other companies
[00:32:58.800 --> 00:33:06.800]   within the umbrella that wields so much economic power that in some ways they are a threat
[00:33:07.920 --> 00:33:13.440]   to the CCP, to the Chinese government. So I don't think this was like Jack Ma ticked off Xi Jinping.
[00:33:13.440 --> 00:33:20.480]   I think this was a response to the enormous power that Ollie Baba on its own can wield
[00:33:20.480 --> 00:33:28.720]   and the sensitive and hyper-advanced computer vision, AI systems that are under its purview
[00:33:28.720 --> 00:33:33.840]   made it a target for the Chinese government. So at one level, that's the way.
[00:33:33.840 --> 00:33:44.000]   Interesting. The US government has been unwilling to do anything about the hegemony of tech
[00:33:44.000 --> 00:33:50.080]   in this country. And the Chinese government said, wait a minute, this can't go on. They're
[00:33:50.080 --> 00:33:54.480]   getting too powerful. That's such an interesting contrast. It's such a great contradiction.
[00:33:54.480 --> 00:33:58.320]   It has no parallel, but it's interesting. Go ahead, Alex.
[00:33:58.880 --> 00:34:05.280]   I mean, it's a fascinating parallel that both in China and the US, governments are viewing these
[00:34:05.280 --> 00:34:10.160]   companies as more powerful in some ways than the government or so powerful that they need to be
[00:34:10.160 --> 00:34:15.760]   reckoned with. And you have what happened in China. And now you look at Facebook and Twitter,
[00:34:15.760 --> 00:34:20.320]   for instance, in Google and Amazon shutting down the president's account or pulling
[00:34:20.320 --> 00:34:26.400]   the ability for sites that are populated by his supporters to sustain themselves.
[00:34:27.040 --> 00:34:31.440]   And you start to look at it as we're still kind of young in the internet age. I mean,
[00:34:31.440 --> 00:34:36.640]   it's a very small sliver of human history. And we're already at the point where there's now this
[00:34:36.640 --> 00:34:41.680]   unbelievable tension between these big companies and the government. I mean, it's a fascinating
[00:34:41.680 --> 00:34:47.120]   thing. And this is definitely, I mean, we're probably just at the beginning of the push and pull
[00:34:47.120 --> 00:34:49.680]   between governments and companies nowhere near the end.
[00:34:49.680 --> 00:34:53.840]   The problem I have, and I want it, we're going to take a break. So I'm going to give you
[00:34:53.840 --> 00:34:58.960]   all a minute or two to come up with a solution. Because that's the problem I have. You split up
[00:34:58.960 --> 00:35:05.760]   Facebook, you just create more Facebook's. It's like, it's like, Hydra, you're not solving the
[00:35:05.760 --> 00:35:10.560]   problem, you're multiplying the problem. You can't force them to divest Instagram and what's
[00:35:10.560 --> 00:35:17.600]   happened, even if you did. What does that accomplish? Well, it's almost for me. And we spent, we've
[00:35:17.600 --> 00:35:22.560]   been for the last at least two years. This has been one of the big conversations, Amy,
[00:35:22.560 --> 00:35:26.320]   you know, we've had this conversation many times on many of our shows.
[00:35:26.320 --> 00:35:32.640]   But for me, that's the most intractable problem is what is the solution? So I'll give each of you
[00:35:32.640 --> 00:35:40.000]   a little time to think of how you would solve this. And if there is a solution, because I'm
[00:35:40.000 --> 00:35:47.600]   baffled. And we'll talk about it when we come back. We have a great panel for maybe one of the most
[00:35:47.600 --> 00:35:56.640]   momentous weeks for Big Tech ever. Amy Webb is here from the Future Today Institute. And again,
[00:35:56.640 --> 00:36:01.600]   highly recommend her book on the Big Nine, which includes the five American companies,
[00:36:01.600 --> 00:36:08.880]   but Tencent and Alibaba and Baidu as well. And it's very timely. Alex Cantrowitz is here. His
[00:36:08.880 --> 00:36:14.800]   podcast is called Big Tech, big technology. And his newsletters at bigtechnology.substack.com
[00:36:15.600 --> 00:36:21.520]   and a media specialist, Dan Gilmour. It's been a long time. I'm so glad to have you back.
[00:36:21.520 --> 00:36:26.400]   Three big minds. Maybe we can come up with an answer that this little mind has
[00:36:26.400 --> 00:36:32.720]   time figuring out. Our show today brought to you by Veracuda. You know the name. If you know
[00:36:32.720 --> 00:36:37.840]   security, Veracuda is a provider of cloud enabled enterprise grade security solutions
[00:36:37.840 --> 00:36:44.480]   to protect you, your email, your networks, your data, your applications. Let's just think about
[00:36:44.480 --> 00:36:54.880]   email for a second. 91% of all cyber attacks start in an email. I'm reminded of the big aluminum
[00:36:54.880 --> 00:37:01.360]   company, North Kydro, which got an innocent email from one of its customers in Italy
[00:37:01.360 --> 00:37:05.440]   with an attachment. The attachment when the customer senate was benign,
[00:37:05.440 --> 00:37:11.120]   but email hijacking meant that the bad guys were able to replace that attachment with malware.
[00:37:11.840 --> 00:37:16.640]   An executive at North Kydro opened it. All of their operations were set down for weeks
[00:37:16.640 --> 00:37:22.000]   due to ransomware. That's one email, one bad attachment, an email you expected with an
[00:37:22.000 --> 00:37:27.440]   attachment you expected from a customer you knew. And imagine now you have all your employees are
[00:37:27.440 --> 00:37:32.400]   out there. They're working from home. Every one of them gets email hundreds every day.
[00:37:32.400 --> 00:37:40.240]   Layden with spearfishing ransomware account takeover conversation hijacking. One click is
[00:37:40.240 --> 00:37:43.920]   all it takes to cost you money, customers to cost you your reputation.
[00:37:43.920 --> 00:37:50.320]   I think I've probably convinced you to do something about this. The researchers at Veracuda have
[00:37:50.320 --> 00:37:57.440]   noticed a spike of 667% in coronavirus related spearfishing attacks. They impersonate the World
[00:37:57.440 --> 00:38:03.600]   Health Organization, the CDC. They use domain spoofing. They'll promise employees information
[00:38:03.600 --> 00:38:11.760]   relating to COVID-19. Maybe our new company policy opened this PDF. And of course, it's a fishing scam.
[00:38:11.760 --> 00:38:18.000]   You need Barracuda total email protection includes all in one email security. You also get back up
[00:38:18.000 --> 00:38:24.560]   in archiving something you need in a lot of businesses by law. You get AI based protection
[00:38:24.560 --> 00:38:30.320]   from spearfishing account takeover and business email compromise. Why AI based because hackers
[00:38:30.320 --> 00:38:35.680]   don't sit still these attacks evolve. They modify. They're constantly changing. And so your protection
[00:38:35.680 --> 00:38:41.840]   has to be smart enough to update itself all the time to notice if something seems suspicious.
[00:38:41.840 --> 00:38:47.120]   They've got a great automated incidents response system. And that's important because of course,
[00:38:47.120 --> 00:38:52.160]   the faster you respond to these attacks, the better able to stop them you are. You also get
[00:38:52.160 --> 00:38:57.360]   security awareness training. So your workforce, your employees, your frontline can be your first
[00:38:57.360 --> 00:39:02.800]   line of defense against attacks. I've got a deal for you to uncover the threats hiding in your
[00:39:02.800 --> 00:39:09.840]   inbox and get a secure free email threat scan of your office 365 account completely risk free
[00:39:09.840 --> 00:39:19.760]   at barracuda.com/twit B A double R A bara C U D A like the fish with the pointy teeth barracuda.com
[00:39:19.760 --> 00:39:25.680]   slash to it uncover the threats hiding your inbox a free threat scan. I know probably some of
[00:39:25.680 --> 00:39:32.160]   you saying I don't want to know you need but trust me you need to know Barracuda your journey secured
[00:39:32.160 --> 00:39:37.840]   unfortunately they have a good solution if you do find anything nefarious in there Barracuda.
[00:39:37.840 --> 00:39:43.040]   Thank you Barracuda for your support of this week in tech and thank you dear listener for
[00:39:43.040 --> 00:39:48.080]   using that address that lets them know you you heard it here barracuda.com/twit
[00:39:49.120 --> 00:39:54.880]   All right is this crazy Amy for me to say what's the solution? Is that reductionist?
[00:39:54.880 --> 00:39:59.600]   It's not I should have been paying attention to Barracuda but instead I was thinking through.
[00:39:59.600 --> 00:40:03.760]   No, no that's fine. That was your job. That was your assignment. What do we do?
[00:40:03.760 --> 00:40:12.160]   Right so typically you know when we wind up in these complex situations with that are multivariate
[00:40:12.160 --> 00:40:17.920]   that roll out in different ways into the future we look for a historic analog of some kind.
[00:40:17.920 --> 00:40:22.160]   I was just trying to think of one I've been trying to think of one and I can't come back to the
[00:40:22.160 --> 00:40:31.120]   problem. There's never been anything like Facebook. The power that Mark Zuckerberg one guy wields
[00:40:31.120 --> 00:40:38.880]   controls the entire media universe and that's just one platform is I think parallel.
[00:40:38.880 --> 00:40:45.840]   Right so I mean he owned because of his shares he has a lot of say but this is not just about
[00:40:45.840 --> 00:40:50.560]   people and entities if it was a people and entity problem it would be an easier problem to solve.
[00:40:50.560 --> 00:40:55.440]   This is people entities and code so the challenge at least you know Twitter is a
[00:40:55.440 --> 00:40:59.360]   different situation but there's a lot of Frankenstein code floating around Facebook and
[00:40:59.360 --> 00:41:04.320]   I've had a lot of conversations with folks there about creating different hierarchies and
[00:41:04.320 --> 00:41:10.960]   taxonomies and deprioritizing things and I think the bottom line when I hear like
[00:41:10.960 --> 00:41:15.440]   you know yes yes we've thought about that but we really don't know how to adjust the products.
[00:41:15.440 --> 00:41:19.680]   I think that they're probably right. I think they probably do not know. I'm sure that there
[00:41:19.680 --> 00:41:25.040]   are billions of lines of code at this point. So I think we can stipulate that there's no
[00:41:25.040 --> 00:41:28.960]   technological solution Facebook could employ. Well I think there could be. That's what I.
[00:41:28.960 --> 00:41:35.120]   It's one of the things that I think that the solution going forward there's nothing is immediate.
[00:41:35.120 --> 00:41:43.040]   So any true immediate near term action I think is going to lead to any action take.
[00:41:44.320 --> 00:41:48.960]   There has to be action taken by the technology companies because this reached fever pitch
[00:41:48.960 --> 00:41:56.320]   and because they're facing after January 20th significant action. The problem is to take those
[00:41:56.320 --> 00:42:03.600]   actions is going to further. It's going to further upset a lot of the people who are currently angry
[00:42:03.600 --> 00:42:08.400]   and they're going to find other places where that anger foments and it leads to additional
[00:42:08.400 --> 00:42:13.840]   problems. So at the moment I think the red team response is not a tech one. I think it's an
[00:42:13.840 --> 00:42:20.800]   actual security apparatus one for the next couple of months. But then what do we do?
[00:42:20.800 --> 00:42:27.440]   And I keep coming back to infrastructure in the book in the big nine I make a pretty I think clear
[00:42:27.440 --> 00:42:35.120]   argument for why AI needs to be thought of as a public good. And I've been having conversations
[00:42:35.120 --> 00:42:43.120]   with people about how we could treat our transmission. So is there a way to create an open source
[00:42:43.120 --> 00:42:51.280]   architecture upon which some of these websites would sit using immutable records so that
[00:42:51.280 --> 00:42:58.320]   at least there was some additional control beyond Facebook Supreme Court whatever it's called
[00:42:58.320 --> 00:43:05.920]   and beyond Jack deciding finally today to make a difference that the social networks would have
[00:43:05.920 --> 00:43:12.800]   shared infrastructure. That's obviously a longer play. I think Tim Berners-Lee is working on
[00:43:12.800 --> 00:43:18.960]   something that's decentralized. And so I keep coming to that point and then I remember
[00:43:18.960 --> 00:43:24.960]   actually decentralization is potentially worse. Because we write.
[00:43:24.960 --> 00:43:32.080]   Let a million flowers bloom. Yeah. So that leads me to my I work on really long term systems level
[00:43:32.080 --> 00:43:41.440]   change and nothing changes until we depoliticize tech and do that intentionally. So I last October
[00:43:41.440 --> 00:43:50.080]   I was at the Senate talking about a new policy a new a national strategic foresight office.
[00:43:50.080 --> 00:43:56.400]   We do not do that desperately. We need that. I've been working really hard for two years to get that
[00:43:56.400 --> 00:44:04.400]   up and running because if we if we you know the the challenges that the laws that people are using
[00:44:04.400 --> 00:44:12.560]   to support the everything that's misinformation bad photos bad stuff that's being sent they're
[00:44:12.560 --> 00:44:17.520]   using the first amendment. And the problem is that the first amendment was written in the days
[00:44:17.520 --> 00:44:21.680]   before there. I mean I guess there technically was an algorithm but it hadn't been operationalized
[00:44:21.680 --> 00:44:26.960]   and like productized on the internet yet. Right. So that's the problem and that's a problem that's
[00:44:26.960 --> 00:44:35.520]   going to take much longer to solve longer than the hours and weeks we have ahead of what might be
[00:44:35.520 --> 00:44:43.520]   another violent attack. I'm very concerned about January 20th. Yeah. I think the only well
[00:44:43.520 --> 00:44:48.640]   basically Washington DC is going to be a battleground and you're going to have troops.
[00:44:48.640 --> 00:44:54.320]   I mean I'm just terrified. And I think there not only is there going to be violence I think
[00:44:54.320 --> 00:44:57.760]   there's gonna be deaths. I'm very scared about what's going to happen in a week.
[00:44:57.760 --> 00:45:04.560]   But that's another that's another topic that also has no obvious and easy solution. What about the
[00:45:04.560 --> 00:45:12.080]   free speech solution? The only solution to bad speech is more free speech. Dan is that a is that
[00:45:12.080 --> 00:45:24.240]   reasonable? More speech. Well like like all simple solutions it's incomplete and sometimes
[00:45:24.240 --> 00:45:33.280]   wrong. I do tend to be a free speech First Amendment absolutist. I do think that
[00:45:33.280 --> 00:45:42.080]   we to address your original question what you know what do we do? I don't think there's anything
[00:45:42.080 --> 00:45:51.280]   wrong with breaking up to in whatever way possible the power of the companies that now dominate
[00:45:51.280 --> 00:46:00.320]   everything. And yes that would probably create lots of spin-offs and which are already starting
[00:46:00.320 --> 00:46:08.480]   to happen because of the actions that the big companies are taking as we speak. I think that's
[00:46:08.480 --> 00:46:16.960]   okay that there's going to be a thousand flowers blooming. I don't it doesn't mean that I'm happy
[00:46:16.960 --> 00:46:22.880]   about the fact that there will be more and more individual spaces for bad people to gather and
[00:46:22.880 --> 00:46:31.440]   plan and do things. But we have we have to decide whether we believe at some fundamental level in
[00:46:31.440 --> 00:46:40.320]   free speech that's loathsome but not at the point where it's advocating the overthrow
[00:46:40.320 --> 00:46:46.320]   governments in ways where people are actually plotting to do things and saying let's go and
[00:46:46.320 --> 00:46:51.600]   kill these people. I think anybody would agree that's too far but prior restraint
[00:46:51.600 --> 00:46:58.880]   is something that just doesn't have a place in this country. We should keep that in mind.
[00:46:58.880 --> 00:47:06.400]   We on the I think redecentralization is a great idea.
[00:47:06.400 --> 00:47:13.360]   Well that's something like Federation or you know Facebook published an API and there were
[00:47:13.360 --> 00:47:19.920]   a thousand Facebooks that confederated something like that. Well that was the infrastructure.
[00:47:19.920 --> 00:47:25.680]   Facebook that's what you were talking about. Yeah and Amy's what Amy's talking about and working
[00:47:25.680 --> 00:47:32.640]   on is really important and a good idea. I think that there's and I've been in touch with these
[00:47:32.640 --> 00:47:39.840]   people for years. There's a in the in whole you know places around the valley Silicon Valley.
[00:47:39.840 --> 00:47:44.400]   People are working on redecentralizing technologies and doing some fabulous stuff.
[00:47:44.400 --> 00:47:48.320]   Mostly young people say this is crazy the system that exists.
[00:47:48.320 --> 00:47:51.440]   That at least eliminates Mark Zuckerberg's threat.
[00:47:51.440 --> 00:48:00.640]   That's the longest of long-term plays because Facebook's so entrenched. But the notion that we
[00:48:00.640 --> 00:48:07.200]   further entrenched them by making laws that make them the default everything is that's even worse
[00:48:07.200 --> 00:48:14.640]   from my point of view. And I will come back to I wanted to say that I do think
[00:48:14.640 --> 00:48:22.240]   redecentralizing is a fine idea. It's hard but people were kind of. And the final one of the fixes
[00:48:22.240 --> 00:48:27.840]   is that we have to stop obsessing solely about the supply
[00:48:27.840 --> 00:48:36.400]   bad as it may be of information and the inundation of bad stuff. And we have to not just upgrade
[00:48:36.400 --> 00:48:43.920]   the information supply but we have to upgrade us the demand. We have to do better at that and that
[00:48:43.920 --> 00:48:50.400]   I would say that because that's what I work on. But I think there's not nearly enough attention
[00:48:50.400 --> 00:48:55.680]   being paid to that side of it. Do you think it's possible that people can be taught
[00:48:59.280 --> 00:49:07.200]   better content moderation literacy? Obviously you do. That's what you're pursuing. I just
[00:49:07.200 --> 00:49:12.160]   I fear that human nature is such that we really look for things that confirm our beliefs.
[00:49:12.160 --> 00:49:18.320]   And and and that's the problem I feel with tools like Facebook is is they foster that they make it
[00:49:18.320 --> 00:49:23.760]   very easy for you to create a bubble of that that's sympathetic to your beliefs and that's that.
[00:49:25.520 --> 00:49:29.520]   Can I just before we move? I'm sorry go ahead. Go ahead, Amy.
[00:49:29.520 --> 00:49:35.040]   I don't want to move away from the breaking up big tech. Okay.
[00:49:35.040 --> 00:49:41.840]   Yeah, let's do that. So we can come back to it but I. No, no, go ahead. Decentralize. What was it?
[00:49:41.840 --> 00:49:50.400]   It's so I want to make the conditions for competition better. That's a good way to do it.
[00:49:50.400 --> 00:49:54.480]   I want there to be more competition. And you're waking up the companies is very hard maybe
[00:49:54.480 --> 00:50:02.880]   impossible but create conditions under which more competition is possible. And that may include
[00:50:02.880 --> 00:50:10.320]   restricting a lot of what's current winners are doing. And it's very hard but we have to have
[00:50:10.320 --> 00:50:16.560]   more competition from my perspective or we have no chance in solving it. It does. It does though.
[00:50:16.560 --> 00:50:21.920]   There's the problem is the network effect. I mean, we have tried Federation when we use a
[00:50:21.920 --> 00:50:26.240]   federated Twitter called Mastodon but because nobody's on it, nobody uses it.
[00:50:26.240 --> 00:50:32.400]   And so you could create a thousand Facebooks but there needs to be but if all your friends are
[00:50:32.400 --> 00:50:39.280]   on Facebook 202 that's the one you're going to use. So I feel like the network effect is working
[00:50:39.280 --> 00:50:51.200]   against decentralization. Amy, go ahead. So right. So here my. I think that centralization is
[00:50:51.200 --> 00:50:58.000]   decentralization is somewhat of an inevitability but not for tiny many, many,
[00:50:58.000 --> 00:51:03.520]   tiny communities. Otherwise key base would be bigger than it is right now. I think it has more to
[00:51:03.520 --> 00:51:08.000]   do with how the infrastructure evolves. But I wanted to come back to this notion of breaking
[00:51:08.000 --> 00:51:13.600]   up big tech because inevitably this is what is discussed when we talk about the future of
[00:51:13.600 --> 00:51:20.640]   these technology companies and in parallel, the breaking up of the baby bells is usually the
[00:51:20.640 --> 00:51:27.760]   reference that's made. We cannot. It would be a problem for our national security if we break
[00:51:27.760 --> 00:51:33.600]   these if we force these companies to be broken up. And the reason for that is because the United
[00:51:33.600 --> 00:51:40.160]   States for the past several administrations has not has either actively not invested in basic
[00:51:40.160 --> 00:51:46.720]   science and research or basic research and science and technology or they've defunded a lot of the
[00:51:46.720 --> 00:51:55.040]   departments that would be doing this. Absent of large scale funding into critical areas of development
[00:51:55.040 --> 00:52:03.040]   for things like not just artificial intelligence, but also space and the batteries that go into
[00:52:03.040 --> 00:52:09.360]   our autonomous vehicles and next frontier technologies like synthetic biology. If the government is
[00:52:09.360 --> 00:52:16.640]   not going to pour money, lots and lots of money into these things that at a level that keeps
[00:52:16.640 --> 00:52:22.720]   pace with what other like China is doing, then this causes us a problem in the longer term.
[00:52:22.720 --> 00:52:27.200]   At the moment, it is just these companies in part because they're so well capitalized
[00:52:27.200 --> 00:52:31.760]   that are doing that work, but they are doing that work in service of their own products and
[00:52:31.760 --> 00:52:37.760]   services because the market spanks them otherwise when it's revealed how much R&D spend there is.
[00:52:37.760 --> 00:52:43.920]   So we can't break. I mean, there are logistical reasons the federal government runs on AWS.
[00:52:43.920 --> 00:52:48.240]   I don't know how you're going to break apart Amazon in any meaningful way because so much
[00:52:48.240 --> 00:52:57.040]   of that company is so interconnected. But my question is like, who is going to do the work?
[00:52:57.040 --> 00:53:03.680]   Who is going to fund the work? And I'm not saying that the big tech companies are doing it now is
[00:53:03.680 --> 00:53:09.520]   good either. They're essentially just working with a handful of universities. It's an incestuous,
[00:53:10.320 --> 00:53:15.200]   very narrow homogenous group of people who are building all of these things. But the point is,
[00:53:15.200 --> 00:53:20.400]   if they don't, it's not going to happen at the same level. And that is going to put us at a
[00:53:20.400 --> 00:53:27.120]   longer term strategic disadvantage. So breaking up big tech alone doesn't solve our problem.
[00:53:27.120 --> 00:53:36.160]   Figuring out a way to incent them to do better on behalf of all of us and probably financial
[00:53:36.160 --> 00:53:42.080]   incentives are going to be the right way to go. While simultaneously making more of the systems
[00:53:42.080 --> 00:53:49.360]   interoperable, forcing parts of them to be public for code bases to be public. It doesn't
[00:53:49.360 --> 00:53:55.040]   detract from the bottom line. We have to take a much more sophisticated approach. Alex earlier
[00:53:55.040 --> 00:54:01.280]   talked about a scalpel versus a hammer. And this is one of those like, the blunt instrument
[00:54:01.280 --> 00:54:06.080]   here is the hammer, right? That's breaking them up. The scalpel approach is going to serve us
[00:54:06.080 --> 00:54:10.960]   better in the longer term. I don't know if this is something we can innovate our way out of though.
[00:54:10.960 --> 00:54:19.200]   I mean, the innovation is in policy. These companies have become so politicized. And to be fair,
[00:54:19.200 --> 00:54:28.640]   it's their own fault. But because this is also sort of this hydra where we have systems and
[00:54:28.640 --> 00:54:33.360]   services that are not interoperable, we've got companies that are so big and wield so much
[00:54:33.360 --> 00:54:38.640]   economic might, they effectively act as nation states. I mean, for God's sake, Microsoft has its
[00:54:38.640 --> 00:54:49.120]   own corporate foreign policy group that it has poached from state. So we have wicked challenging
[00:54:49.120 --> 00:54:56.720]   problems, sort of demand creative approaches. Otherwise, we're going to wind up in court
[00:54:56.720 --> 00:55:00.560]   arguing about all of this and going backwards.
[00:55:00.560 --> 00:55:04.720]   Well, we're certainly going to do that. That's right, Alex. That's already a given.
[00:55:04.720 --> 00:55:08.480]   This is all going to be in court. We will be in court lawyers. Yeah.
[00:55:08.480 --> 00:55:13.440]   Yeah. I mean, Leo, I think what you're getting at here, right? So there's two big questions that
[00:55:13.440 --> 00:55:20.960]   we have. One is, how can we incentivize an environment of competition in our country so that these
[00:55:20.960 --> 00:55:27.520]   companies do not consolidate control in the way that they have? And we have smaller firms that
[00:55:27.520 --> 00:55:32.240]   are able to come up and start taking a little bit of their market share and then maybe building
[00:55:32.240 --> 00:55:36.800]   bigger things at a result of it versus now where they don't have the ability to compete. And that's
[00:55:36.800 --> 00:55:42.960]   definitely a worthy conversation. But I think this week, what really demands to be foregrounded is,
[00:55:42.960 --> 00:55:48.240]   how do we put our society back together? Because that's the issue that we've had with some of these
[00:55:49.680 --> 00:55:55.680]   platforms. And you've talked about it, how people end up going into their echo chambers and
[00:55:55.680 --> 00:56:01.920]   basically forming another version of reality that's disconnected with what's actually happening
[00:56:01.920 --> 00:56:08.480]   in the world. I have family members I can't talk to because they operate on a different set of
[00:56:08.480 --> 00:56:13.280]   facts. Exactly. And so when we talk about this discussion, like, should we break up Facebook,
[00:56:13.280 --> 00:56:17.360]   I think we should be decentralized. I think we really need to think about the second order
[00:56:17.360 --> 00:56:24.400]   effects, which is that what network does take off and does it become a partisan network like a
[00:56:24.400 --> 00:56:28.960]   GAB? And is there a tendency to that because of the network effect? I mean, right now, people
[00:56:28.960 --> 00:56:33.600]   are saying, what's app? Oh my God, Dapples revealed all the privacy issues. Let's all use
[00:56:33.600 --> 00:56:38.160]   signal. Not not going to happen. Everybody you talk to is on WhatsApp, you're going to still
[00:56:38.160 --> 00:56:43.040]   use WhatsApp. Yeah, well, I mean, signal is now the number one app in the app store. So there's
[00:56:43.040 --> 00:56:47.440]   a lot of people are trying it. Yeah, but if people are trying it, but my family and friends aren't
[00:56:47.440 --> 00:56:52.240]   on it. Well, that's I mean, I've done that. It's great. Love signal. Nobody I knows on it.
[00:56:52.240 --> 00:56:56.800]   Correct. Look, and I think we're getting at the same point, which is that we need to consider
[00:56:56.800 --> 00:57:01.760]   the second order of effects. And that history doesn't end with a ban because people when you
[00:57:01.760 --> 00:57:07.840]   zap somebody off your social platform, you know, they don't just go away. They're still have access
[00:57:07.840 --> 00:57:13.840]   to each unit. The browser again, they might go somewhere else. And so, and actually, you know,
[00:57:13.840 --> 00:57:19.040]   if you think about Facebook, they've been slow and irresponsible for a large part of their history.
[00:57:19.040 --> 00:57:23.360]   So I think that, you know, rather than saying, let's break Facebook up, I think we need to focus
[00:57:23.360 --> 00:57:27.760]   on some of the policies that Facebook could implement to end up bringing us back to that
[00:57:27.760 --> 00:57:31.440]   shared version of reality. They know how to structure their newsfeed. And Dan, I think,
[00:57:31.440 --> 00:57:37.520]   I backed this up in a way that will not divide people by having them direct their clicks to
[00:57:37.520 --> 00:57:42.960]   partisan and sensationalize news. They can direct their newsfeed to a place where,
[00:57:42.960 --> 00:57:47.920]   you know, there are news news organizations that have some sentiment or some attachment of the
[00:57:47.920 --> 00:57:53.440]   truth that people go to. And they can change the recommendation algorithms to make sure that,
[00:57:53.440 --> 00:57:56.720]   you know, people are not going to they did this right before the election to.
[00:57:57.600 --> 00:58:02.960]   That people aren't going to groups that end up in the next conspiracy theory. We know many people,
[00:58:02.960 --> 00:58:06.480]   you know, did that, you know, did that and ended up finding their way to things like
[00:58:06.480 --> 00:58:11.360]   QAnon through Facebook recommendations. But what our society facing now is more than technology
[00:58:11.360 --> 00:58:15.120]   problem, which is something I don't think should be lost in this discussion, right? Because,
[00:58:15.120 --> 00:58:19.920]   you know, while technology might have helped, you know, I helped that horse leave the barn,
[00:58:19.920 --> 00:58:24.960]   that horse is out. And I think a major part of this solution is we need a people part of this
[00:58:24.960 --> 00:58:31.120]   solution as well, where, you know, I'm hoping and, you know, I think a realist will say this is
[00:58:31.120 --> 00:58:36.880]   unlikely. But the optimistic take is that maybe the fever breaks, you know, from an event like this,
[00:58:36.880 --> 00:58:41.920]   people realize that this stuff is just totally unacceptable. We have to preserve democracy and
[00:58:41.920 --> 00:58:48.080]   that the elements of the country that, you know, led up or helped build up to, you know, what we
[00:58:48.080 --> 00:58:52.960]   were what we experienced over the past week, you know, aren't given the free reign and the,
[00:58:52.960 --> 00:58:56.880]   oh, what could be the worst thing that happens perspective that we've had so far. We need people
[00:58:56.880 --> 00:59:03.680]   who are, you know, who are political leaders or media leaders to steer us back to like a world
[00:59:03.680 --> 00:59:08.640]   where people can talk to each other like you, you should be able to talk to your, to your relatives
[00:59:08.640 --> 00:59:14.160]   and to your friends and about politics. And okay, part of the reason of that is we can't do that
[00:59:14.160 --> 00:59:19.680]   is because of tech. But you know, these tech companies amplify people. And it's up to the people
[00:59:19.680 --> 00:59:24.880]   who are making these statements. They have a responsibility, you know, to step up to the game
[00:59:24.880 --> 00:59:28.800]   and realize how close we were to the disruption of the counting of the votes for the president of the
[00:59:28.800 --> 00:59:33.760]   United States. You know, potentially, who knows they could have burned those votes if they got into
[00:59:33.760 --> 00:59:38.240]   the hall at the right time and to steer us back from the brink. That's what we demand from our
[00:59:38.240 --> 00:59:42.880]   leadership right now. And I think that's, you know, just as if not more important as the
[00:59:42.880 --> 00:59:48.160]   technology solutions that we're seeking to implement. I suppose you can look at certainly
[00:59:48.720 --> 00:59:54.560]   recent history as a pendulum swing back and forth. And each time the pendulum goes too far in one
[00:59:54.560 --> 00:59:59.040]   direction, it swings back in the other direction. I fear though that at some point it's going to
[00:59:59.040 --> 01:00:07.280]   swing so far, it's going to get stuck. I'm feeling we could be where we are right now boy. I just
[01:00:07.280 --> 01:00:16.160]   I feel like this is very a very difficult thing to solve. And you know, you could say that Alex
[01:00:16.160 --> 01:00:20.880]   said, Oh, it's up to Facebook to fix their newsfeed. But that just says, in a way, well, it's up to
[01:00:20.880 --> 01:00:27.040]   Mark Zuckerberg to solve this. Let's let's go risk. Yeah, I mean, three out of that, we have a lot of
[01:00:27.040 --> 01:00:32.240]   power. That gives me heat. Yeah, but whether or not we were we like it, he has that power. That's true.
[01:00:32.240 --> 01:00:37.680]   We need the help of the platforms in the short term. There's no question for a short and medium
[01:00:37.680 --> 01:00:45.120]   term that that they're going to have an outsized role. And we should be asking a lot of them. And
[01:00:45.120 --> 01:00:51.280]   Alex is exactly right. They could tweak the newsfeed tomorrow to do things that would be better for
[01:00:51.280 --> 01:01:02.320]   all of us. And but possibly not as good for the bottom line at Facebook. And there's an any number
[01:01:02.320 --> 01:01:09.920]   of things that they Twitter and other social media companies could do to help us improve
[01:01:10.640 --> 01:01:22.080]   the conversations and to make them more civil and to downgrade the BS and to upgrade things that
[01:01:22.080 --> 01:01:30.320]   have demonstrated some sort of, you know, interest in truth and things like that. But we have to just
[01:01:30.320 --> 01:01:35.120]   put a lot of pressure on them. And even then they're going to do what they decide.
[01:01:36.320 --> 01:01:43.120]   Is it simplistic to me, Amy, to say it's the problem is algorithms. And you know, I think people,
[01:01:43.120 --> 01:01:47.360]   when they joined Facebook, a lot of people, I thought, Oh, this is great. It's going to be me
[01:01:47.360 --> 01:01:53.520]   looking at a feed of the things my family and friends are doing. And that's that. But I left
[01:01:53.520 --> 01:01:57.040]   Facebook because it pretty clearly became something other than that because of the
[01:01:57.040 --> 01:02:05.360]   evil algorithm. YouTube, the algorithm pushes people into extremism. Is the is it too simplistic
[01:02:05.360 --> 01:02:12.640]   to say? Well, that's that's because the algorithm optimizes for profit, not for quality.
[01:02:12.640 --> 01:02:19.040]   So I'll answer that question and then a note on YouTube. And then I actually just thought of a
[01:02:19.040 --> 01:02:25.680]   solution for Facebook. Oh, good. So I knew if we let you think long enough, you'd solve this.
[01:02:25.680 --> 01:02:31.360]   My answer to your question, is it too simplistic to just blame it on the algorithms? The answer is
[01:02:31.360 --> 01:02:37.200]   no, that's exactly what the problem is. Now it's it's non algorithmic determinism on its own human
[01:02:37.200 --> 01:02:42.880]   still play a role. And both Alex and Dan have made this point clearly. There are plenty of
[01:02:42.880 --> 01:02:52.080]   interventions that that would amount to small tweaks that may not shift the experience so drastically
[01:02:52.080 --> 01:02:57.520]   that we at least alleviate or let out some of the steam, right, from the pot that's boiling. So
[01:02:57.520 --> 01:03:02.400]   that's that's one thing. YouTube hasn't been brought up at all. And again,
[01:03:02.400 --> 01:03:09.120]   Trump has been the most visual and vocal presence on Facebook and on
[01:03:09.120 --> 01:03:13.840]   Twitter. And that's what gets talked about a lot. And I know, Ben and got taken off of YouTube,
[01:03:13.840 --> 01:03:19.200]   the algorithm, YouTube's personalization algorithms are far better,
[01:03:19.200 --> 01:03:26.240]   far better than than everybody else's. And and I think deserves much more scrutiny than it's
[01:03:26.880 --> 01:03:34.160]   receiving. And you know, that that would I would actually start there. I wouldn't start
[01:03:34.160 --> 01:03:36.240]   with Facebook and Twitter, but
[01:03:36.240 --> 01:03:44.000]   to tell Google, stop optimizing for profit. Just stop it. Okay, so here's but that that
[01:03:44.000 --> 01:03:48.080]   leads into the answer. So I was trying to think through like, I'm always asking the question,
[01:03:48.080 --> 01:03:52.320]   what would it take for X to be why? So I was thinking as I was listening to Alex and Dan,
[01:03:52.320 --> 01:03:56.720]   what would it take for these companies to make to change their minds? Or you know,
[01:03:56.720 --> 01:04:00.960]   and the problem is that individuals, we often lump everybody together. There's plenty of
[01:04:00.960 --> 01:04:05.120]   there are plenty of people working at these companies who disagree with the policies of
[01:04:05.120 --> 01:04:09.120]   these companies, but their employees, their, yeah, that's actually one of the stories this
[01:04:09.120 --> 01:04:15.040]   week is petitions by Twitter, Google, Facebook employees, all saying, you know, take down Trump,
[01:04:15.040 --> 01:04:21.760]   whether that I don't know. But I have some very good friends working at these companies.
[01:04:22.240 --> 01:04:25.920]   Who are there because they are trying to make a difference, which is why they say.
[01:04:25.920 --> 01:04:31.040]   And anyway, what would it take for Facebook to take down the content that it knows is
[01:04:31.040 --> 01:04:36.800]   salacious in the news feed? And that's what led me to kind of an interesting idea, which is,
[01:04:36.800 --> 01:04:41.360]   you know, it would be tedious, but not impossible to build
[01:04:43.600 --> 01:04:51.920]   many, many permutations of financial models that hinge on different, different variables. So,
[01:04:51.920 --> 01:04:58.800]   so for example, QAnon is kind of a big topic, but within that, and I haven't seen the data on
[01:04:58.800 --> 01:05:06.240]   this, but my assumption is there's probably, you know, a thousand sub keywords related to QAnon
[01:05:06.240 --> 01:05:10.880]   that tend to get posted within. So if you built a regression, you would probably, I'm not going to
[01:05:10.880 --> 01:05:19.280]   at any rate. There's probably a bunch of data. So what would be interesting is to try to quantify
[01:05:19.280 --> 01:05:26.960]   what the value of that post is. What's the actual monetary value of posts that use,
[01:05:26.960 --> 01:05:34.800]   you know, X number of terms related to these incendiary terms, right? So QAnon, Antifa,
[01:05:35.360 --> 01:05:42.560]   you know, Trump, whatever. So going at the content versus the user, and then if you have those models
[01:05:42.560 --> 01:05:48.480]   built, you would be able to project how much actual loss there might be. And I wonder if,
[01:05:48.480 --> 01:05:53.200]   I mean, I'm sure there would be some loss, right? But if you were able to be, like,
[01:05:53.200 --> 01:05:57.360]   if Facebook could be transparent and say we built these, we built these economic models,
[01:05:57.360 --> 01:06:04.000]   I free like, like, there are plenty of quant funds that do this exact same thing. So they could
[01:06:04.000 --> 01:06:08.640]   partner with a quant fund to actually do this for them, but to build those models, run the
[01:06:08.640 --> 01:06:14.560]   scenarios, and then be able to say, listen, we're about, you know, in a couple of days, we're going
[01:06:14.560 --> 01:06:23.120]   to start policing. Policing is not the right word. We're going to start treating this content
[01:06:23.120 --> 01:06:29.920]   differently. And this is our projected impact on our bottom line. And we believe it's going to cost
[01:06:29.920 --> 01:06:35.520]   us X amount of dollars to do this. We're doing it because we believe it's for the best.
[01:06:35.520 --> 01:06:41.680]   I wonder, you know, and we're asking for investors to see the bigger picture here and to not, you know,
[01:06:41.680 --> 01:06:48.560]   I wonder if they're, rather than everybody speculating, I wonder if there was an actual model built.
[01:06:48.560 --> 01:06:56.320]   And if they used a quant fund model that idea to do it, and then they were totally transparent
[01:06:56.320 --> 01:07:02.320]   about the results and gave everybody time, you know, I wonder and said like, this is, this is,
[01:07:02.320 --> 01:07:07.200]   we're going to do this from now on. I bet that would help not hurt them. And it might solve our
[01:07:07.200 --> 01:07:13.440]   problems. That was super complicated. Well, good story. Yeah. Wall Street loves a good story.
[01:07:13.440 --> 01:07:18.640]   And that's a good story. So that might, that might be a really productive way to proceed.
[01:07:18.640 --> 01:07:28.160]   Our show today brought to you by Express VPN. There, I think when we talk about a VPN to you guys,
[01:07:28.160 --> 01:07:32.160]   you know what a VPN is, and you even know why you might want to use it for privacy,
[01:07:32.160 --> 01:07:38.400]   because internet service providers and carriers can see everything you're doing and often
[01:07:38.400 --> 01:07:43.600]   log it and sell it to marketers. We also do it for security because that guy in the hoodie in the
[01:07:43.600 --> 01:07:49.600]   coffee shop, he also could see everything you do. And maybe he's gotten nefarious intent. You might
[01:07:49.600 --> 01:07:55.760]   even do it because it allows you to emerge onto the public internet at an IP address, not associated
[01:07:55.760 --> 01:08:00.320]   with you in a country, maybe the one you don't live in. And so if you've watched everything on
[01:08:00.320 --> 01:08:06.400]   Netflix and who hasn't by this time, but you like Dr. Who, you just fire up Express VPN. It's a single
[01:08:06.400 --> 01:08:10.160]   button you say, I want to be in London right now, then refresh Netflix. Suddenly all the
[01:08:10.160 --> 01:08:15.520]   Dr Who episodes are available in Netflix, UK, or Netflix, Japan, where you can watch as much anime
[01:08:15.520 --> 01:08:21.280]   as you can take. That's kind of cool. One of the reasons you can do this is because Express VPN
[01:08:21.280 --> 01:08:27.280]   invests in their network. They have they're in almost 100 countries, and they make sure that
[01:08:27.280 --> 01:08:32.560]   there are enough IP addresses, they rotate them enough so that you're really your privacy is really
[01:08:32.560 --> 01:08:40.000]   protected. And they're fast enough. Look at this speed. So that you can watch HD video. It is, it is,
[01:08:40.000 --> 01:08:44.640]   in fact, a lot of us keep Express VPN running all the time, you could put it on your router,
[01:08:44.640 --> 01:08:51.120]   protect your whole home. And you won't even know that Express VPN is running. It is great for privacy,
[01:08:51.120 --> 01:08:57.280]   it's great for security, it's great for watching content. And I have to say one of the things I
[01:08:57.280 --> 01:09:03.120]   really like and the reason I recommend Express VPN is it is fully private. There are free and
[01:09:03.120 --> 01:09:08.880]   inexpensive VPN providers that don't protect your privacy that don't respect you. That's not
[01:09:08.880 --> 01:09:13.520]   Express VPN. And we know that because they get regular audits, a price, water house cooper, who
[01:09:13.520 --> 01:09:18.560]   check their privacy policy, make sure they live up to that. They even check this trusted server
[01:09:18.560 --> 01:09:24.320]   technology Express VPN uses when you use an Express VPN VPN server, wherever it is in the world,
[01:09:24.320 --> 01:09:29.200]   it spins up the trusted server, runs in RAM, can't write to hard drive, it's completely
[01:09:29.200 --> 01:09:35.280]   sandboxed. And when you leave it goes away, so does every record of your visit, it's completely
[01:09:35.280 --> 01:09:39.760]   private. That's the kind of thing you really want in price, water house cooper says, yes,
[01:09:39.760 --> 01:09:47.120]   that works exactly as I just described it. It's probably why I seen it wired. And so many
[01:09:47.120 --> 01:09:51.760]   other people say Express VPN is the number one VPN service out there. It's the only one I use.
[01:09:52.560 --> 01:09:56.480]   So stop handing over your personal data to ISPs and other tech giants who
[01:09:56.480 --> 01:10:02.400]   mine your activity, sell off your information, protect yourself. With the only VPN I trust to
[01:10:02.400 --> 01:10:11.520]   keep me private online visit Express VPN dot com slash twit, e X P R E S S VPN dot com slash
[01:10:11.520 --> 01:10:19.440]   twit in the extra three months free with a one year package. It is the way to go express VPN dot
[01:10:19.440 --> 01:10:24.160]   com slash twit. We thank him so much for their support of this week and Tekken. And thank you
[01:10:24.160 --> 01:10:29.440]   for supporting us by using that URL. It's they've got a 30 day money back guarantee too. I should
[01:10:29.440 --> 01:10:36.400]   throw that in. So you know that no risk trying it out. Express VPN. Dan, somebody in the chat room
[01:10:36.400 --> 01:10:42.880]   brought this up. What if what if we brought back the fairness doctrine? Would that solve the media
[01:10:42.880 --> 01:10:51.760]   problem the news problem? No, no, it's it's it's so completely
[01:10:51.760 --> 01:11:04.320]   inadequate and wrong for the internet age that it's and it was I was never my favorite thing even
[01:11:04.320 --> 01:11:09.040]   when it existed with a limited number of broadcasters, which is what it was about.
[01:11:09.040 --> 01:11:14.800]   Right. Presumption that the airways are a limited quantity owned by the public. Therefore, they
[01:11:14.800 --> 01:11:22.480]   have responsibility and we're going to make sure they do this. It you know, it was always a good idea
[01:11:22.480 --> 01:11:31.360]   in a sort of conceptual way, but hard to make work. But when you have even even with all the power
[01:11:31.360 --> 01:11:39.200]   of the big companies, there's still a lot of ways to be heard and to speak. And the idea that a
[01:11:39.200 --> 01:11:43.840]   fairness doctrine is somehow needed again, it seems crazy to me, especially because you know,
[01:11:43.840 --> 01:11:50.560]   you have billions of things posted every day on Facebook. We're going to go and try to.
[01:11:50.560 --> 01:11:55.840]   It's okay. It's from an earlier era when there was just two points of view, Republican or Democrat,
[01:11:55.840 --> 01:12:02.640]   and you just balance it. And of course, and that's one reason that traditional media earned a lot
[01:12:02.640 --> 01:12:09.760]   of the suspicion that has fallen upon them in the last couple of generations is that the notion
[01:12:09.760 --> 01:12:16.480]   that there were two balance and equally valid viewpoints for anything has always been kind of loony.
[01:12:16.480 --> 01:12:25.520]   And people who had views that were outside of those constrained right center and left center
[01:12:25.840 --> 01:12:34.800]   places had no outlet, no one talking to them. And, you know, I'm to put it mildly not a fan of
[01:12:34.800 --> 01:12:42.800]   Rush Limbaugh, but he filled a major gap in the media ecosystem, did it in an evil way, but there
[01:12:42.800 --> 01:12:50.640]   was so well, the change talk radio forever. He sure did. But it was a and again, I'm, you know,
[01:12:50.640 --> 01:13:02.560]   I loathe what he did. But I recognize that he had filled a gap that was just huge. And no one's done
[01:13:02.560 --> 01:13:09.600]   that very well on the left. But their whole point is that I don't like constrained media ecosystems.
[01:13:09.600 --> 01:13:16.720]   I like the ability of people to publish, to say what they want to say and to possibly reach an
[01:13:16.720 --> 01:13:24.960]   audience rather than having to filter through a couple of or three or at most four networks
[01:13:24.960 --> 01:13:30.560]   run by old white men. That's crazy. Yeah, no, that's a very good point. Corporate media is not
[01:13:30.560 --> 01:13:36.960]   doing anybody a favor. Leo, can I just jump in with your idea for a break? Go ahead.
[01:13:36.960 --> 01:13:41.520]   Yeah. Okay. One thing I've been standing on the table at with the technology is that we're never
[01:13:41.520 --> 01:13:46.000]   going to solve anything if we look only at the outputs and we need to really look at the machine,
[01:13:46.000 --> 01:13:50.960]   which means that it's so easy to look at a bad post or a user you don't like and say,
[01:13:50.960 --> 01:13:55.920]   if you hit the band button or if you remove this from your service, you're going to solve all the
[01:13:55.920 --> 01:14:00.000]   problems. And that's why I think like over the past weekend, you know, so many people celebrating
[01:14:00.000 --> 01:14:05.520]   the ban of Donald Trump as if it's the end of all problems in the world are misguided. I think
[01:14:05.520 --> 01:14:10.960]   what we really need to do is to kind of look at how these platforms operate. And that can be the
[01:14:10.960 --> 01:14:15.600]   newsfeed algorithm. And we did see ahead of the election, Facebook and Twitter basically showed
[01:14:15.600 --> 01:14:20.560]   us exactly, you know what they believe is wrong with their service. But you know, for one reason or
[01:14:20.560 --> 01:14:27.280]   another, they don't think it's possible to make change permanently. So they did start directing
[01:14:27.280 --> 01:14:32.480]   people in the newsfeed to fact based reporting. This was a noted newsfeed change that they announced
[01:14:32.480 --> 01:14:37.520]   where they thought it would be there be less tension. So, you know, if they have the more
[01:14:37.520 --> 01:14:43.280]   outrageous stuff, they might get more engagement, which is good for their business model. But for
[01:14:43.280 --> 01:14:46.880]   some reason, they decided to switch it back after the election, which to me was
[01:14:46.880 --> 01:14:52.400]   remarkable. They turned off group recommendations. So we know that that's an issue because if they're
[01:14:52.400 --> 01:14:57.360]   trying to have a normal political system and they turn off group recommendations, what does that
[01:14:57.360 --> 01:15:02.400]   tell us about group recommendations? And then finally, I don't think there's been anywhere close
[01:15:02.400 --> 01:15:07.920]   to enough discussion of the damage that's done by the share button and the retweet button,
[01:15:07.920 --> 01:15:13.040]   which allows information and travel at lightning speed, you know, through these networks that
[01:15:13.040 --> 01:15:18.080]   number one means misinformation is going to go quickly. But number two, it changes the way that
[01:15:18.080 --> 01:15:23.520]   people pass along information because when you don't need a hesitate, when you just hit a button
[01:15:23.520 --> 01:15:28.240]   and you can share it to people, you're going to share the thing like you mentioned that confirms
[01:15:28.240 --> 01:15:33.920]   or validates your belief and plays to emotion and not to thought. And there are studies that
[01:15:33.920 --> 01:15:38.800]   show that when you need when you pause, you take a moment to think about what you're sharing,
[01:15:38.800 --> 01:15:43.680]   you're going to share things based off of whether they make sense for your followers, not in terms of
[01:15:43.680 --> 01:15:48.320]   or your friends, not in terms of based off of how they make you feel. And I think this entire
[01:15:48.320 --> 01:15:53.680]   instinctual emotional based sharing, where people are willing to share something that says, you know,
[01:15:53.680 --> 01:15:58.720]   Hillary Clinton is a space alien because it sort of validates their thoughts about Hillary Clinton.
[01:15:58.720 --> 01:16:04.000]   That is incredibly damaging. And if you have to copy and paste that into the compose box
[01:16:04.000 --> 01:16:08.080]   and actually put your avatar next to it, not somebody else's, it's a completely different
[01:16:08.080 --> 01:16:13.440]   calculation. But for some reason, whenever we have these discussions, we'll leave that completely out.
[01:16:13.440 --> 01:16:18.160]   And I think that if there's one thing we're going to do moving forward is we start to focus on the
[01:16:18.160 --> 01:16:22.720]   machine, especially the share in the retweet button, but also the recommendations and the
[01:16:22.720 --> 01:16:28.080]   algorithm itself. And that will probably get us into a much better place than thinking about who
[01:16:28.080 --> 01:16:33.120]   we're going to ban or what content we don't want on the platforms. I agree. Can I ask you a really
[01:16:33.120 --> 01:16:39.520]   quick question, Alex? And I know you've got to do commercial. But what if so, what if Twitter no
[01:16:39.520 --> 01:16:46.800]   longer showed externally metrics? So you knew how many followers you had, but you could not,
[01:16:46.800 --> 01:16:53.760]   you could not visually see anybody else's followers or the number of likes or we retweets.
[01:16:53.760 --> 01:16:59.680]   If you took the extra step to go into your dashboard and see what got retweeted and liked,
[01:16:59.680 --> 01:17:04.480]   but basically all those metrics were stripped out of public view. I wonder if that would,
[01:17:04.480 --> 01:17:11.840]   that seems like a tweak that could potentially help maybe.
[01:17:11.840 --> 01:17:15.280]   Yeah, that is something that's absolutely worth testing. I mean, the way that these
[01:17:15.280 --> 01:17:20.000]   platforms make these decisions is they test with the limited number of users and they see how that
[01:17:20.000 --> 01:17:24.640]   impacts the way their service works. And if they like the results, then they roll it out.
[01:17:24.640 --> 01:17:29.280]   And to me, it's extraordinary that they haven't tested what you're talking about in
[01:17:29.280 --> 01:17:34.560]   particular. And we already seen actually a version of this where Instagram
[01:17:34.560 --> 01:17:39.360]   stopped publicly displaying likes. And I think that that changed. I mean, obviously,
[01:17:39.360 --> 01:17:43.520]   they decided to roll it out permanently, say like what it was going, but it didn't kill the business.
[01:17:43.520 --> 01:17:48.320]   And it certainly changed the experience where all of a sudden, you're not feeling bad about how
[01:17:48.320 --> 01:17:53.520]   many likes you get every time you post. And it's not about more. It's about the point you're not
[01:17:53.520 --> 01:17:58.800]   retweeting simply because you see a bunch of people retweeting and you feel like part of
[01:17:58.800 --> 01:18:02.400]   if you remove that incentive, I don't think that changes the the
[01:18:02.400 --> 01:18:07.120]   it'd be interesting. I wonder if Twitter just did that for a couple of weeks. What would
[01:18:07.120 --> 01:18:11.920]   I don't think it changes the Twitter did introduce the you haven't read that yet.
[01:18:11.920 --> 01:18:19.360]   Morning. All that's doing is that's like that's like everybody's like, yeah, no, no,
[01:18:19.360 --> 01:18:24.800]   yeah, no kidding. Not only that, they also they also stopped the native retweet ahead of the
[01:18:24.800 --> 01:18:29.360]   election, but they said, Oh, well, people retweeted less. So that experiment didn't work. So they
[01:18:29.360 --> 01:18:34.800]   put it back. Nice. But like, come on, wrong. Saying that every single decision they make is based off
[01:18:34.800 --> 01:18:40.080]   of the health of the platform, conversational health. And it's really freaking interesting
[01:18:40.080 --> 01:18:44.640]   that when they evaluated how well that experiment worked, they didn't say whether it improved or
[01:18:44.640 --> 01:18:49.600]   hurt conversational. They said, well, people retweeted less. It sort of goes to show you exactly
[01:18:49.600 --> 01:18:55.440]   what's going on inside that platform. It's about the incentives to keep engagement up and not about
[01:18:55.440 --> 01:18:58.960]   the incentives to make the conversation. Precisely. We're letting them off the hook.
[01:18:58.960 --> 01:19:04.320]   Yeah. So we need to in the pantheon. Dan's been trying to get a word edge. Hold on a second. Go
[01:19:04.320 --> 01:19:11.360]   ahead, Dan. Yeah. I think the idea of putting a whole bunch of speed bumps into the process of
[01:19:11.360 --> 01:19:20.160]   sharing and posting is a great one. And they should, as Alex said, be a be testing this like crazy.
[01:19:20.160 --> 01:19:26.160]   There's no evidence I've seen that they are. But there's all sorts of things that they should be
[01:19:26.160 --> 01:19:34.640]   experimenting with. And I think that the Twitter experiment was quite successful from a social
[01:19:34.640 --> 01:19:44.000]   point of view, not obviously from a business one. And Facebook's ability to change things
[01:19:44.000 --> 01:19:52.720]   pretty quickly is extraordinary. And the thing is that if what I've been arguing or
[01:19:52.720 --> 01:20:00.640]   trying to nudge Facebook toward personally is give users more control over their own
[01:20:00.640 --> 01:20:06.640]   newsfeed and give us dashboards to make some of our own decisions in the system. The usual
[01:20:06.640 --> 01:20:15.440]   answer is, oh, no one will do that. Well, why don't you try? I think we have so much help that we
[01:20:15.440 --> 01:20:23.440]   need from the platforms and as far as yet not nearly enough being provided. But it's perfectly
[01:20:23.440 --> 01:20:27.600]   possible. I'm hopeful that the government scrutiny is going to make them sit back and think, well,
[01:20:28.400 --> 01:20:32.800]   OK, maybe we need to. We do need to do some of these things, even if it doesn't increase engagement,
[01:20:32.800 --> 01:20:38.880]   maybe that's not the only metric, maybe not getting broken up by the House Judiciary Committee on
[01:20:38.880 --> 01:20:44.080]   subcommittee on any trust. That would be a good goal as well. Amy, go ahead. I'm just going to say,
[01:20:44.080 --> 01:20:49.120]   I think it's worth noting, especially given who we are each talking to today,
[01:20:49.120 --> 01:20:57.200]   Twitter by volume is a fraction of Facebook. Their share value is a fraction of Amazon,
[01:20:57.200 --> 01:21:02.960]   of Apple. Well, their usership is a fraction of it too.
[01:21:02.960 --> 01:21:10.080]   Right. That's what I'm saying. And by any way you slice it, market cap,
[01:21:10.080 --> 01:21:18.880]   number of users, the amplification in Twitter happens because there's a concentration of people
[01:21:18.880 --> 01:21:25.520]   in tech, politics and media. So I think it's worth, I mean, I think that is worth thinking
[01:21:25.520 --> 01:21:32.240]   through because Twitter as a company is not what I would call it. I'm looking at their share price
[01:21:32.240 --> 01:21:37.760]   right now. That's not the place where I would sink a bunch of money into, and yet they have
[01:21:37.760 --> 01:21:43.280]   this unbelievable influence. And I think the insight there is that this isn't platform alone,
[01:21:43.280 --> 01:21:49.200]   but it's people. I mean, in as much as Twitter is enabling misinformation to spread,
[01:21:49.200 --> 01:21:55.120]   people in positions of power are helping them do that. I am guilty of the same. I have
[01:21:55.120 --> 01:21:59.280]   certainly spread incendiary content more than once without really thinking through
[01:21:59.280 --> 01:22:05.680]   how others in my network are going to treat that information. So all of us bear some
[01:22:05.680 --> 01:22:12.160]   responsibility here. The platforms are making poor choices that are in response to market
[01:22:12.160 --> 01:22:18.080]   demands, but we're all complicit in this. There's no Twitter user that hasn't
[01:22:18.080 --> 01:22:24.720]   regret tweeted at some point or the other. I mean, I like how Maggie, Maggie Haberman calls it
[01:22:24.720 --> 01:22:29.200]   a anger video game where the reach tweets are at the points. And I think there's some real truth
[01:22:29.200 --> 01:22:34.400]   to that. It's really set up that way. All right, let's take a little break only because we have to,
[01:22:34.400 --> 01:22:40.880]   but this is a great conversation. It will continue with our wonderful guests. First time on, Alex,
[01:22:40.880 --> 01:22:44.640]   but I'm really glad to have you on. We'll have you back lots more Alex Cantrowitz.
[01:22:44.640 --> 01:22:50.800]   Bigtechnology.substack.com and the big technology podcast. Old friend Dan Gilmore, who is, of course,
[01:22:51.360 --> 01:22:56.480]   a great guy to listen to in all of this stuff. Longtime media watcher, dangilmore.com
[01:22:56.480 --> 01:23:09.600]   at dangilmore on the Twitter and Amy Webb. The future is talking at the Future Today Institute.
[01:23:09.600 --> 01:23:15.280]   Wait, let me just shake the, what should we do about Facebook Future Today Institute Magic 8 Ball?
[01:23:16.080 --> 01:23:26.560]   It says, maybe small bets. I don't like it. I want another one. Call FTI. I like that one.
[01:23:26.560 --> 01:23:34.240]   All right, so today, oh, hey, before we start the ad, I just want to mention it is survey time.
[01:23:34.240 --> 01:23:38.880]   We do this at the beginning of every year. We don't collect information about you. We're not
[01:23:38.880 --> 01:23:44.720]   able to. We don't want to. We know you don't want us to, but advertisers want to know. We want to know
[01:23:45.440 --> 01:23:49.520]   what you're interested in, what you like, how much money you make, things like that,
[01:23:49.520 --> 01:23:54.240]   your education level. So we do this survey once you're completely voluntary. You don't have to
[01:23:54.240 --> 01:24:00.800]   answer any question if you don't want to, but it is very helpful for us. It'll take about, I don't
[01:24:00.800 --> 01:24:04.960]   know, five, 10, 15 minutes of your time. Depends on how much you think about the answers.
[01:24:05.520 --> 01:24:12.800]   Twit.tv/survey21. We really appreciate it if you take that survey. Thank you.
[01:24:12.800 --> 01:24:23.040]   I don't want to just kind of, you know, beat a dead horse. That seems rude. I don't want to keep
[01:24:23.040 --> 01:24:27.520]   going on on this subject. If we've said everything that needs to be said, it's clearly a thorny issue.
[01:24:27.520 --> 01:24:32.720]   They're breaking news. Oh, no. No, no. Yeah. So this is kind of interesting. Just as we've
[01:24:32.720 --> 01:24:39.760]   all been talking, Stripe is no longer processed. Stripe is a canceled Trump, so they're not taking
[01:24:39.760 --> 01:24:47.120]   any more payments. So that's kind of interesting, right? Because that's the social media infrastructure
[01:24:47.120 --> 01:24:51.600]   has to do with information, and that's important. But gutting the architecture of what make these
[01:24:51.600 --> 01:24:57.040]   websites work and able to collect money, that I think is a bigger problem for Trump
[01:24:58.160 --> 01:25:06.080]   than being pulled off of Facebook and Twitter. That's the process payments. That's the porn hub
[01:25:06.080 --> 01:25:16.400]   punishment. If you can't use credit cards on a site, that really, that's actually more significant
[01:25:16.400 --> 01:25:21.280]   than getting pushed off Twitter or Facebook. That means you can't raise money.
[01:25:21.280 --> 01:25:26.960]   Yeah, and it's interesting. So does that kind of would be interesting to get Dan and Alex's take
[01:25:26.960 --> 01:25:33.760]   on this, but could we say that that is information? Is a digital payment? Does that fall within the
[01:25:33.760 --> 01:25:37.680]   realm of information? I feel like that's something the Supreme Court could weigh in on and say,
[01:25:37.680 --> 01:25:43.680]   you know what? That is a violation. That is a First Amendment. Well, it's not First Amendment
[01:25:43.680 --> 01:25:48.560]   because it's not government. But you're taking away this guy's right to raise money
[01:25:50.160 --> 01:25:58.400]   in a political campaign. Talk about destroying a campaign. What if they did that to the next
[01:25:58.400 --> 01:26:01.920]   person running for president? That's a big deal. What do you think, Alex?
[01:26:01.920 --> 01:26:06.880]   I mean, like these are, you know, a stripe isn't the only payment processor off. I think it would be
[01:26:06.880 --> 01:26:12.640]   much more impactful if let's say Visa or MasterCard said. Well, I wonder if they'll follow.
[01:26:12.640 --> 01:26:17.280]   I mean, now they did. They just did. They did. They did. They're out as well.
[01:26:17.280 --> 01:26:22.560]   So Citibank, they're no longer accepting money from any PACs. So they're not specifically targeting
[01:26:22.560 --> 01:26:28.880]   Republicans or Trump, but they are no longer accepting any money for the time being for
[01:26:28.880 --> 01:26:33.200]   like, like, with the Citizens United. But the Citizens United, you can still use a credit card
[01:26:33.200 --> 01:26:37.920]   to donate to Trump. Can you? If he moves to a new software? Yeah, I mean, they would have to
[01:26:37.920 --> 01:26:42.880]   re-engineer their site and get off his writing kind of something else. So this is, I want to call
[01:26:42.880 --> 01:26:48.400]   death knelt to the campaign and to his ability to raise money. It's a temporary situation. I mean,
[01:26:48.400 --> 01:26:56.800]   even with GAB, you saw GAB was able to build its own servers and it's running today. And I think
[01:26:56.800 --> 01:27:03.760]   in the 24 hours after the storming of the Capitol, they had 2.2 million visits, according to the head
[01:27:03.760 --> 01:27:08.240]   of the site told me while I was working on a story for big technology about that. So like,
[01:27:08.240 --> 01:27:15.120]   these are setbacks, but it's not like, you know, Trump not being on Twitter and Facebook isn't the
[01:27:15.120 --> 01:27:20.560]   end of his ability to communicate. He can find a certain point, set up his own website with his
[01:27:20.560 --> 01:27:25.920]   own servers and communicate that way, similar with the payment processing. But it is, you know,
[01:27:25.920 --> 01:27:31.040]   it is definitely a setback and we'll take time for them to work through. Yeah, I do recommend your
[01:27:31.040 --> 01:27:39.040]   latest sub stack newsletter where you talk about alternative social networks. As Trump gets banned
[01:27:39.040 --> 01:27:44.960]   from mainstream networks, there are many places like GAB that are still there. And boy, some of
[01:27:44.960 --> 01:27:51.440]   the stuff posted on his sites is horrific. Well, these are the second order effects. And by the way,
[01:27:51.440 --> 01:27:59.520]   GAB's CEO said he had a phone call with a very special person yesterday. That was. But given that
[01:27:59.520 --> 01:28:04.720]   parlor is going down and like the only other option might be Telegram or some other secondary
[01:28:04.720 --> 01:28:11.920]   networks, I would not be surprised to see a big migration to GAB, which is already set up
[01:28:11.920 --> 01:28:16.800]   outside of the tech giants control largely. So keep an eye out for that.
[01:28:16.800 --> 01:28:23.040]   Yeah, wow. I mean, the Supreme Court in the Citizens United did say money is speech.
[01:28:24.560 --> 01:28:32.720]   So if you cut off the air supply for a political campaign, you're cutting off their speech.
[01:28:32.720 --> 01:28:40.400]   I think it's, it's, you know, your ability to spend money to support a cause is speech, your ability
[01:28:40.400 --> 01:28:45.600]   to collect his money through a specific platform. I'm trusting. Yeah, I don't believe it's okay.
[01:28:45.600 --> 01:28:50.880]   Very interesting. Thank you, Amy, for that breaking news. Holy cow.
[01:28:52.480 --> 01:28:55.920]   Very interesting. We live in interesting times, not necessarily a good thing.
[01:28:55.920 --> 01:29:05.120]   You know, we talk about the platform power. What about the power of just a small number of payment
[01:29:05.120 --> 01:29:11.920]   systems to decide who's allowed to make money? Exactly. Yeah. And WikiLeaks was basically
[01:29:11.920 --> 01:29:19.920]   very close to shut down back in the days when it was before 2016. I'm not a fan of what they've
[01:29:19.920 --> 01:29:28.800]   done recently, but they were doing something important for a long time and were essentially
[01:29:28.800 --> 01:29:38.320]   taken out of the fundraising world by companies that had no, you know, they didn't really answer
[01:29:38.320 --> 01:29:42.880]   to anybody except somebody calling them from Washington saying, we'd really like you to take
[01:29:43.520 --> 01:29:49.520]   WikiLeaks off of your systems. That's, that's pretty dangerous too. Yeah.
[01:29:49.520 --> 01:29:54.880]   On a brighter note, it is the 20th anniversary of Wikipedia,
[01:29:54.880 --> 01:29:59.360]   which, I mean, if you're going to point to one thing that the internet does well,
[01:29:59.360 --> 01:30:05.760]   that's the one, right? I don't, I don't think so. No, I thought, geez, you know,
[01:30:05.760 --> 01:30:10.400]   fraught with peril, letting the community edit and encyclopedia, but you think,
[01:30:10.400 --> 01:30:15.200]   yeah, Wikipedia, an amazing poster child for crowdsourced internet.
[01:30:15.200 --> 01:30:18.400]   Not when you're being targeted for harassment,
[01:30:18.400 --> 01:30:25.360]   Brianna and I can both give you long stories of how I'm, you know, I didn't establish my
[01:30:25.360 --> 01:30:30.400]   Wikipedia page. I don't know who did. I can tell you that it's, thankfully,
[01:30:30.400 --> 01:30:36.400]   and I love everybody listening. So I know you are not going to go in and start monkeying on it
[01:30:36.400 --> 01:30:42.320]   again. But after my first book came out, there was quite a bit of back and forth and like
[01:30:42.320 --> 01:30:50.720]   vandalism, you know, and I mean, just really bad, like really horrible stuff that was being
[01:30:50.720 --> 01:30:55.520]   posted and there's no, like I'm the source. I am the primary source. Could I at least take that
[01:30:55.520 --> 01:31:01.680]   stuff off? No, no, you don't get to edit your page. Yeah. Right. Okay, never mind.
[01:31:03.600 --> 01:31:08.240]   I like it. But then I haven't had too much vandalism. There's been a little bit, but nothing too
[01:31:08.240 --> 01:31:14.000]   malicious. I feel like Wikipedia. Well, all right, never mind. I'll take, I'll take that back.
[01:31:14.000 --> 01:31:19.440]   I think it's a great idea. I think there are like, like everything, I think there are ways to
[01:31:19.440 --> 01:31:24.080]   improve. And there's got, you know, so that it is not so when you are in a situation and people
[01:31:24.080 --> 01:31:29.840]   are targeting you in some way, that it is not impossible to deal with.
[01:31:31.360 --> 01:31:38.720]   So I don't disagree with any of that. And I think Wikipedia is a miracle that gets better and better
[01:31:38.720 --> 01:31:47.440]   and better. And we can always improve and people running it are aware of a lot of these issues.
[01:31:47.440 --> 01:31:53.600]   Maybe don't move as fast as we would like them to on some things. But man, I think the world would
[01:31:53.600 --> 01:32:02.080]   be vastly worse off without Wikipedia than what we have with it. I love it. But I didn't know about
[01:32:02.080 --> 01:32:08.000]   all that abuse you and Brianna had. So that's not so good. Our show today brought to you by Mint Mobile.
[01:32:08.000 --> 01:32:14.640]   I got my my iPhone SE from Mint Mobile. This is an amazing deal. The SE is $15 a month. My Mint Mobile
[01:32:14.640 --> 01:32:20.960]   unlimited nationwide talk and text and data. I think I get the three gigs of data month.
[01:32:21.600 --> 01:32:29.440]   15 bucks. So the total package 30 bucks a month. That's amazing. One of the ways Mint Mobile does
[01:32:29.440 --> 01:32:33.280]   it, they don't have stores. They don't spend money renting stores. They don't have store fronts.
[01:32:33.280 --> 01:32:40.240]   It's an all online wireless service selling you the same service you get with a premium wireless plan
[01:32:40.240 --> 01:32:47.600]   but starting at $15 a month. That is an amazing deal. All their plans come with unlimited talk
[01:32:47.600 --> 01:32:52.480]   and text and high speed data delivered on the nation's largest 5G network.
[01:32:52.480 --> 01:32:56.560]   Team mobile. You can use your own phone. Bring your own phone. They'll send you a sim free.
[01:32:56.560 --> 01:33:01.280]   You put it in the phone. You keep your contacts. They'll even port your old phone number over.
[01:33:01.280 --> 01:33:06.400]   They also sell phones like that iPhone SE with a great price. And if you're not 100% satisfied,
[01:33:06.400 --> 01:33:11.520]   Mint Mobile has you covered with their seven day money back guarantee. Switch now to Mint Mobile.
[01:33:11.520 --> 01:33:17.760]   Get premium wireless service starting at just $15 a month. Actually, I liked it so much
[01:33:17.760 --> 01:33:22.400]   because I'm a T-Mobile customer. I thought, you know, why am I paying 70, 80 bucks a month?
[01:33:22.400 --> 01:33:27.040]   For some I can get for less. I ended up paying 300 bucks for a year of Mint Mobile
[01:33:27.040 --> 01:33:32.640]   for 12 gigabytes a month. That's 25 bucks a month. For 12 gigabytes,
[01:33:32.640 --> 01:33:39.520]   unlimited nationwide talk and text. That's a great deal. Here's what you do. Go to mintmobile.com/tuit.
[01:33:39.520 --> 01:33:48.880]   Pick the plan you like and stop paying all those huge phone bills. I feel like every phone I use
[01:33:48.880 --> 01:33:56.480]   should be Mint Mobile. Why am I paying more? Mintmobile.com/tuit. Cut your wireless bill to as low as $15 a month
[01:33:56.480 --> 01:34:04.160]   and get the plan shipped to your door for free. Mintmobile.com/tuit. This is an idea whose
[01:34:04.800 --> 01:34:12.320]   time has come, I think. Mintmobile.com/tuit. Thank you, Mint Mobile, for supporting the show.
[01:34:12.320 --> 01:34:20.160]   Okay, how about this? The iPhone, 13 years old, at least the announcement of the iPhone,
[01:34:20.160 --> 01:34:28.560]   13 years ago this weekend. Who hates the iPhone here? Anybody? Okay. I don't use Apple stuff, so...
[01:34:31.200 --> 01:34:36.960]   Yeah, but your Android phone is based on the iPhone. So, you know, I mean, it's basically a copy of the
[01:34:36.960 --> 01:34:42.640]   iPhone. You want to hear a great story about the iPhone announcement? Yeah. Were you there? I was
[01:34:42.640 --> 01:34:48.320]   there. I was not there, but I can tell you who heard about the iPhone at the same time that I did.
[01:34:48.320 --> 01:34:56.080]   It was the executive leaders at Blackberry. Oh, dear. How did they react to that? Did they say
[01:34:56.080 --> 01:35:01.440]   over in trouble? It's a wonderful story. So, the CEO at the time was supposedly on a treadmill.
[01:35:01.440 --> 01:35:06.800]   And the others came in and said, "You're never going to believe this," and they're trying to
[01:35:06.800 --> 01:35:12.400]   explain this iPhone. They see the commercial. And when they finally got their hands on one,
[01:35:12.400 --> 01:35:20.080]   they cracked it open and said, "My God, it's a computer." Yeah. And still refused to see a future
[01:35:20.080 --> 01:35:26.720]   in which a phone would be something more than for office, email, and calendars, and people would
[01:35:26.720 --> 01:35:32.400]   respond to something without buttons. That was purely haptic. It really is amazing how the reaction
[01:35:32.400 --> 01:35:38.560]   of Blackberry, Microsoft, and others really underplayed what was about to happen. Clearly,
[01:35:38.560 --> 01:35:45.040]   a revolution in hindsight, we know a revolution in computing, maybe the most significant revolution
[01:35:45.040 --> 01:35:52.080]   since the personal computer, it killed Blackberry, essentially. In fact, my last phone before my
[01:35:52.080 --> 01:35:58.000]   first iPhone in 2007 was a Blackberry Pearl. I loved that physical keyboard and all that, but
[01:35:58.000 --> 01:36:01.040]   I never bought another one after that, and I think I was, I'm not alone.
[01:36:01.040 --> 01:36:09.920]   Apple will probably attempt the same thing in a couple of years. I have a feeling Hyundai may
[01:36:09.920 --> 01:36:15.600]   have made a strategic mistake by announcing that they were talking with Apple about making a car.
[01:36:15.600 --> 01:36:22.640]   Apple's not notoriously forgiving in cases like that. Hyundai's stock went up 19%.
[01:36:22.640 --> 01:36:28.960]   And then they kind of retracted saying, "Oh, Apple's in discussion with a lot of automakers.
[01:36:28.960 --> 01:36:36.320]   It's not just us." But it's interesting because we've heard the drumbeat about Apple making a car
[01:36:36.320 --> 01:36:43.120]   now suddenly ramp up over the last couple of weeks. So this Hyundai story is kind of interesting.
[01:36:43.120 --> 01:36:48.160]   Clearly, Apple doesn't have a manufacturing capability to manufacture a car. They'd have to go to somebody
[01:36:48.160 --> 01:36:56.320]   who makes cars, but an Apple car is an interesting product, probably not till 2024 or 2025, maybe
[01:36:56.320 --> 01:37:03.280]   later, depending on which rumor mill you listen to. I want to see Tim Cook on stage saying,
[01:37:04.720 --> 01:37:09.680]   "It's an internet communicator. It's a phone. It's a transportation
[01:37:09.680 --> 01:37:13.600]   device. Internet communicator phone transportation device."
[01:37:13.600 --> 01:37:21.360]   And being Apple, they'll decide where you go. Yes, that's right. It is presumed to be an electric
[01:37:21.360 --> 01:37:28.960]   vehicle, self-driving, probably unclear when you'd have it. Of course, Apple's not commenting.
[01:37:28.960 --> 01:37:34.240]   Also, strong rumors that Apple and Facebook will do augmented reality glasses of some kind of
[01:37:34.240 --> 01:37:38.240]   this year. Facebook's already kind of in that business having bought Oculus.
[01:37:38.240 --> 01:37:45.600]   There have been significant rumors that perhaps, let's see, Andrew Bosworth of Facebook says,
[01:37:45.600 --> 01:37:52.400]   "It's smart glasses that they're building in relationship with Ray Ban will arrive sooner than
[01:37:52.400 --> 01:37:59.440]   later and will not necessarily be." Oh my God. Now, I'm trying to go to this story on Bloomberg,
[01:38:00.480 --> 01:38:05.200]   and Bloomberg has decided I'm a robot. So I'm clicking this, I'm not a robot,
[01:38:05.200 --> 01:38:11.360]   and it still thinks I'm a robot. Bloomberg, I was going to give you a nice plug for that story.
[01:38:11.360 --> 01:38:18.480]   I'm not a robot, Bloomberg. You see, the robots have really taken over. They really...
[01:38:18.480 --> 01:38:19.520]   The robots have totally...
[01:38:19.520 --> 01:38:28.000]   Amy, what do you think of the dancing dog and the Boston Dynamics robots doing ballet moves?
[01:38:28.800 --> 01:38:36.080]   So I will tell you, I watch a lot of robots. I certainly thought that was funny. I showed it
[01:38:36.080 --> 01:38:41.200]   to my daughter and who loves ballet and robots and thought it was really cool. And then,
[01:38:41.200 --> 01:38:46.560]   for some reason, I remembered peanut butter jelly time. Do you guys remember that?
[01:38:46.560 --> 01:38:50.000]   Yeah, peanut butter jelly time. So I was like, yeah. So I showed... I was like,
[01:38:50.000 --> 01:38:55.600]   "Oh, wouldn't it be funny if the Boston Dynamics robot did something with that?" And I went from
[01:38:55.600 --> 01:39:02.480]   this video to that GIF and song, and she was like old-your-weird old technology.
[01:39:02.480 --> 01:39:05.120]   She was not at all interested peanut butter jelly time. She was not amused.
[01:39:05.120 --> 01:39:07.440]   No. But did she enjoy the robots? Yeah.
[01:39:07.440 --> 01:39:14.400]   She thought that it was not real. She thought that it was some kind of CGI or animation.
[01:39:14.400 --> 01:39:17.680]   See, that's the problem with her generation. They're never going to know what's real or not,
[01:39:17.680 --> 01:39:20.880]   because everything... I thought it was... No, the fact that she was skeptical,
[01:39:20.880 --> 01:39:24.880]   I thought was amazing. She didn't just see this and thought it was... I mean, I was so proud.
[01:39:24.880 --> 01:39:29.600]   I was like, "Great." And she wasn't brought up on, you know, Terminator movies. And so she didn't
[01:39:29.600 --> 01:39:33.600]   look at this and go, "Oh my God, we're all... It's done. We're done. We're through. It's over."
[01:39:33.600 --> 01:39:36.480]   She's still pretty young, so we haven't explored.
[01:39:36.480 --> 01:39:45.440]   I see this robot bearing arms and dancing its way over to me, and I find it a little bit scary.
[01:39:45.440 --> 01:39:49.120]   Oh my God, it's got a jaw. It's got a mouth. Oh my God.
[01:39:49.120 --> 01:39:53.360]   Yeah, well, you know what? It can't open up a door. Yeah, it can. That's why the jaw is there.
[01:39:53.920 --> 01:39:59.040]   It can turn. It's like... I've seen a lot of robot fails of trying to actually...
[01:39:59.040 --> 01:40:04.080]   Oh sure, we humans reassure ourselves by pretending that it can't open a door.
[01:40:04.080 --> 01:40:11.360]   It'll get there if it's not there yet. Yeah. Who needs to be able to open a door when you're
[01:40:11.360 --> 01:40:18.400]   holding a 50 caliber? Exactly. There are other ways to open those doors. Actually, this is amazing
[01:40:18.400 --> 01:40:25.520]   that this 350 pound device can stand on one leg and balance is pretty impressive. I have to say.
[01:40:25.520 --> 01:40:33.360]   This is, I think, partly designed to make people feel better after they saw the robot in Singapore,
[01:40:33.360 --> 01:40:39.280]   going after people if they're not wearing masks. That sort of freaked a lot of people out.
[01:40:39.280 --> 01:40:46.560]   It's all scary. What the hell is that, you know? So why is Boston Dynamics? Why are they having
[01:40:46.560 --> 01:40:53.840]   such a hard time finding a... They keep getting acquired and unacquired? Yeah, so Google owned
[01:40:53.840 --> 01:40:58.960]   them and then the Google probably wisely decided having killer military robots was not a good part
[01:40:58.960 --> 01:41:04.640]   in the portfolio. So they sold it to SoftBank and SoftBank just sold it as well. SoftBank, right?
[01:41:04.640 --> 01:41:09.840]   All kinds of issues. They go to Hyundai? Yeah, I think you might be right. Maybe they're going
[01:41:09.840 --> 01:41:13.040]   to Hyundai to work on the software. Maybe it's the Apple robot. By the way, I...
[01:41:13.040 --> 01:41:18.640]   Davey, Apple's working on it. Having reported on Apple and spoken with the engineers who've
[01:41:18.640 --> 01:41:23.280]   worked on that car, I don't think that thing's going to happen. I don't think that's ever
[01:41:23.280 --> 01:41:30.000]   going to roll off the assembly line, period. That's a big deal to get in the car business.
[01:41:30.000 --> 01:41:38.400]   Although, who's the richest man in the world right now? Yeah. Elon Musk. That's bizarre. He made
[01:41:38.400 --> 01:41:45.920]   $37 billion last week to suddenly... And sadly, he doesn't have a home. He's homeless because he
[01:41:45.920 --> 01:41:53.760]   got rid of his home and his child is named something unpronounceable. He has several children. That's not
[01:41:53.760 --> 01:42:02.480]   the right... Oh, he has adult children. Yeah. Yeah. But he is worth $188 billion on paper,
[01:42:03.280 --> 01:42:08.400]   on paper. And when he said you should use Signal, what happened, Alex?
[01:42:08.400 --> 01:42:14.720]   No. It became number one. Through the roof. It's almost as if Elon has a little bit of
[01:42:14.720 --> 01:42:21.360]   influence among people who are interested in emerging technology. Yeah. I don't know. I owned
[01:42:21.360 --> 01:42:29.520]   a Tesla for a while, but it was kind of like that Boston dynamic robot. I felt like my wife,
[01:42:29.520 --> 01:42:35.840]   at one point Lisa said, "This car is in beta and I don't trust it. It's going to kill me."
[01:42:35.840 --> 01:42:41.040]   It would be veering off the highway and the door... It had those gall-wing doors that kept
[01:42:41.040 --> 01:42:46.080]   closing on her head because they never put sensors on the bottom part. So it was very
[01:42:46.080 --> 01:42:50.640]   careful to open them and not hit the wall. But if you had your head underneath it, it would stand
[01:42:50.640 --> 01:42:56.240]   back. So I had at some point in them in my model, Alex, I had to announce the doors are closing.
[01:42:56.240 --> 01:43:02.960]   Stand clear, stand clear. I just bought an Audi Etron Sportback.
[01:43:02.960 --> 01:43:09.360]   Oh, did you? I'm so jealous. Yeah. It's like driving a spaceship.
[01:43:09.360 --> 01:43:13.520]   It's much bigger than I thought it was going to be. The electronics are...
[01:43:13.520 --> 01:43:26.000]   There are lots of features and functions. But ultimately, it's got a permanent nanny state.
[01:43:26.000 --> 01:43:31.840]   If I put cruise control on and I set it five to 10 miles above the speed limit,
[01:43:31.840 --> 01:43:38.320]   every time it's computer vision senses a new speed limit sign, it slows... It will slow the
[01:43:38.320 --> 01:43:43.600]   car back down to what is stated. Oh, interesting. There's really... You can't
[01:43:43.600 --> 01:43:52.080]   easily override all of the following rules. I was thinking of... It's a perfect example of
[01:43:52.080 --> 01:43:58.000]   something that you bought that you don't own. Yeah. For sure. For sure. I bought... Well,
[01:43:58.000 --> 01:44:06.080]   and I also didn't buy it. I don't buy cars. I lease them. But yeah, I'm... Yeah, and it's the
[01:44:06.080 --> 01:44:13.360]   amount of data that it wants to collect is pervasive. It wants all... Yeah, which I said no to. So...
[01:44:13.360 --> 01:44:17.760]   Tesla started that. That makes it a little bit more early on. We got the Tesla. I could have sworn
[01:44:17.760 --> 01:44:22.960]   I was in reverse, but the car went forward. And it happened a couple of times, once to me, once
[01:44:22.960 --> 01:44:27.360]   to my wife. So we called Tesla and they said, "Well, according to the log, you were in drive."
[01:44:27.360 --> 01:44:36.240]   You know? Yeah, we know exactly what you do. And they control the log production. They control...
[01:44:36.240 --> 01:44:44.560]   I mean, you have to trust them on that. There's no acting out. Cars, of course, have had black
[01:44:44.560 --> 01:44:50.880]   boxes for a long time before Tesla. So the data stuff has gone back in fairly long time.
[01:44:50.880 --> 01:44:58.880]   And it is pervasive and it's disgusting. Yeah. That said, if you want an all-electric vehicle,
[01:44:58.880 --> 01:45:03.840]   which I wanted, there's not a lot. And I wasn't going to buy the... I'm getting the Mustang, baby.
[01:45:03.840 --> 01:45:09.040]   The Fisker... No, don't get the Fisker. No, I'll get the Fisker.
[01:45:12.160 --> 01:45:16.640]   It's not the Scissor Company. It's not ready. It's not ready.
[01:45:16.640 --> 01:45:22.160]   But this, this e-tron is... You actually might like it a lot if you guys are in the market.
[01:45:22.160 --> 01:45:28.480]   Well, I'm getting the Mustang Lockheed any day now, but I think maybe... The GT is beautiful.
[01:45:28.480 --> 01:45:34.160]   It's a nice car. It's a nice car. That's the... What the Porsche Taycon is based on.
[01:45:34.160 --> 01:45:39.360]   Yeah. This Audi is really underpowered. I think... Well, here's a tech-related thing.
[01:45:40.000 --> 01:45:43.600]   We have yet to solve the battery issue and because the battery capacity on these things...
[01:45:43.600 --> 01:45:51.040]   That's, I think, the big, big advantage that Tesla has over everybody else, the battery.
[01:45:51.040 --> 01:45:59.120]   The range on this Audi is only 150 miles and it's drastically underpowered. It has the space for a
[01:45:59.120 --> 01:46:03.920]   bigger... I mean, an engine is... You don't need more space, right? So it could have a bigger,
[01:46:03.920 --> 01:46:10.960]   better engine in it, but the battery won't support it. And the battery technology is a lot of what's
[01:46:10.960 --> 01:46:18.480]   holding back the, I think, explosion of EVs. Yeah. John Lasseter, the director at Pixar, had a Fisker,
[01:46:18.480 --> 01:46:24.560]   Fisker, one of the early ones. And I remember his wife told me, "Don't get it. We drove up to the
[01:46:24.560 --> 01:46:34.480]   Academy Awards and we couldn't get out because the car, the malfunctioned. We had a roll down
[01:46:34.480 --> 01:46:42.960]   with us and climb out." So again, probably not a good choice. There's a car, there's a car, a
[01:46:42.960 --> 01:46:50.240]   story of a car gone wild. This finally, during the pandemic finally caved and bought a car,
[01:46:50.240 --> 01:46:54.880]   I had an owned a car in a decade or so. What do you need one in the pandemic for? You're not going
[01:46:54.880 --> 01:46:58.720]   anywhere, Alex? Well, the point is, yeah, I mean, you're not going to take... I was taking public
[01:46:58.720 --> 01:47:02.000]   transportation everywhere and you had to do that. Oh, you don't want to do that. Yeah. And I was
[01:47:02.000 --> 01:47:08.320]   like, all right. Yeah. So I went back to the future, 2007, Mazda Miata got it from South
[01:47:08.320 --> 01:47:13.840]   Philippines, San Francisco. Perfect. It's been a blast and a pleasure. And that car does not log
[01:47:13.840 --> 01:47:20.720]   your presence at all. As far as we know, it's about as technologically dumb as it gets.
[01:47:20.720 --> 01:47:24.240]   So we'll see how long it lasts. I love those when those came out. When those came out.
[01:47:24.240 --> 01:47:30.160]   Oh. Oh. That was good. It's what you want. Wow. It's a beater. It's a beater. It's a good
[01:47:30.160 --> 01:47:33.600]   looking beater. Well, you're being logged by others in the Bay Area. Yeah, that's true.
[01:47:33.600 --> 01:47:37.600]   That's camera. I also have an iPhone in my, you know, in the car while I'm driving. So
[01:47:37.600 --> 01:47:43.440]   maybe I'm helping Apple with our development. The car is the least, honestly, the car tracking
[01:47:43.440 --> 01:47:49.680]   me is the absolute least of my concerns. I've got, you know, we're all surrounded by so much
[01:47:49.680 --> 01:47:54.880]   technology. There is no way not to be surveilled. It always cracks me up. I had a guy call.
[01:47:54.880 --> 01:48:01.680]   It was an older gentleman on the radio show today. A lot older. He said, my Yahoo weather
[01:48:01.680 --> 01:48:10.480]   doesn't remember my favorite places. And I've said, what browser using? He says, I'm using fire
[01:48:10.480 --> 01:48:17.680]   Fox. And he apparently, because the news site that he read had too many ads, he turned on cookie
[01:48:17.680 --> 01:48:24.320]   blocking, which of course has nothing to do with ads. But smart idea. Yeah, people are, you know,
[01:48:24.320 --> 01:48:28.320]   that cookie thing. And then, but you got to point out, if you're carrying a smartphone,
[01:48:28.320 --> 01:48:32.960]   you're carrying a device that has built in GPS and always on internet connection, a microphone
[01:48:32.960 --> 01:48:40.640]   and a camera. And in my case, LiDAR, that's the perfect spy device. Yeah, who cares what cookies?
[01:48:40.640 --> 01:48:48.480]   Who cares? Yeah. Who cares? Hey, let's let's take another little break. We have a little video.
[01:48:48.480 --> 01:48:54.000]   Fun week this week on Twitch, we've combined some of the highlights to make this little movie for
[01:48:54.000 --> 01:49:00.080]   your enjoyment watch. Well, today was fun because of Mulchfest. You bring your Christmas tree. Oh,
[01:49:00.080 --> 01:49:04.720]   and they have huge machine. Oh, that's great. And they run it through and you can bring a shopping
[01:49:04.720 --> 01:49:11.440]   bag. Give you a bag of mulch in December. It's really hard to reassemble it. But it takes up so
[01:49:11.440 --> 01:49:21.840]   much space. It's much more compact. Previously on Twitch, security now. Swatting goes IOT. The FBI
[01:49:21.840 --> 01:49:30.320]   wrote to warn users of smart home devices with cameras and voice capabilities to use complex,
[01:49:30.320 --> 01:49:37.440]   unique passwords and enable two-factor authentication to help protect against swatting attacks.
[01:49:37.440 --> 01:49:43.360]   This week in Google, reverse engineering the source code. Yes, source code of the Pfizer
[01:49:44.160 --> 01:49:51.120]   COVID vaccine. And it is fascinating. Wait, why does the vaccine have a source code? That's what's
[01:49:51.120 --> 01:50:01.760]   fascinating. Okay. All about Android. This is a remake of the Commodore 64 called VC64. This
[01:50:01.760 --> 01:50:06.640]   just gives me so much joy. I've actually literally played it, played hours of it since I've had it.
[01:50:06.640 --> 01:50:12.000]   That's how like geeked out I am on this thing. So, Twitch. Or if you're on a Commodore 64,
[01:50:12.000 --> 01:50:19.200]   it's Twitch, eight, one. Don't forget to use the peak command. I suppose we should probably talk
[01:50:19.200 --> 01:50:27.520]   about section 230. I don't, you know, I don't feel I don't really like TikTok. It's like as the
[01:50:27.520 --> 01:50:33.120]   as the Trump presidency winds down, I wonder there's these little things kind of hanging out in the
[01:50:33.120 --> 01:50:37.200]   world like TikTok was supposed to be shut down. I don't think it's going to be shut down. Although
[01:50:37.200 --> 01:50:43.440]   maybe now that it banned the president, it will be I don't know. I also feel like all of the attacks
[01:50:43.440 --> 01:50:51.760]   on section 230, they've just kind of gone up and smoke. Even Ajit Pai, the current chairman of the FCC
[01:50:51.760 --> 01:50:58.240]   on his last days has said, I'm not going to I think about section 230. Section 230, as some have said,
[01:50:58.240 --> 01:51:05.840]   is the the what is it? 23 words that created the internet. 60 minutes did a piece on it a couple
[01:51:05.840 --> 01:51:12.880]   weeks ago that was completely wrong. Got it completely wrong, which is a shame. If you want to know more
[01:51:12.880 --> 01:51:21.200]   about section 230, Reid Mike Masnick's tector where he explains what it means. But it is really
[01:51:21.200 --> 01:51:31.360]   very simple. It it means that. Well, there's there's really two parts. No provider or user of an
[01:51:31.360 --> 01:51:37.440]   interactive computer service. She'll be treated as the publisher or speaker of any information
[01:51:37.440 --> 01:51:41.600]   provided by another information content provider. In other words, you can't be sued.
[01:51:41.600 --> 01:51:51.520]   Twitter, Facebook can't be sued for anything's other people post. I can't be sued for people
[01:51:51.520 --> 01:51:57.120]   things people post on my chat room. I can't be sued for people things people post in our forums
[01:51:57.120 --> 01:52:03.440]   or comments on our web page, even though I run those sites, I'm not liable for what people post
[01:52:03.440 --> 01:52:10.240]   there. If you got rid of that, I'd have to take the chat room down. I'd have to take the forums down.
[01:52:10.240 --> 01:52:15.360]   I'd have to take comments down because I don't want to be liable for something somebody says on
[01:52:15.360 --> 01:52:23.040]   those sites. I think it's pretty clear. We need section 230. Is it at risk?
[01:52:25.040 --> 01:52:29.840]   Anybody have any thoughts on this? I don't think it's at risk at all. I think we should cut through
[01:52:29.840 --> 01:52:35.600]   some of the bullcrap, which is that the Republicans have been threatening to revoke section 230
[01:52:35.600 --> 01:52:40.240]   because they don't like the way they've been moderated. And that's why you see every time
[01:52:40.240 --> 01:52:46.560]   there's a label put on Trump's tweets back when he was able to tweet, he would tweet section 230,
[01:52:46.560 --> 01:52:51.680]   revoke section 230, because they didn't want the platforms to be touching their stuff. But that
[01:52:51.680 --> 01:52:56.640]   was a complete misunderstanding of what 230 is. Yes, exactly, exactly. Totally misunderstanding.
[01:52:56.640 --> 01:53:01.440]   But they knew they know they're lawmakers. They know exactly what the law, maybe except for Trump,
[01:53:01.440 --> 01:53:06.800]   they had a good idea of what that law was supposed to do. The threat of revoking it
[01:53:06.800 --> 01:53:12.640]   was supposed to intimidate these platforms to not touch their content. Because, like you said,
[01:53:12.640 --> 01:53:17.280]   if your if section 230 went away, you'd have to take down their chat room. It doesn't, it's not a
[01:53:17.280 --> 01:53:22.880]   pretty world for if your Facebook or your Google and section 230 is gone. And now all of a sudden,
[01:53:22.880 --> 01:53:28.880]   you're liable, you're liable for everything that's posted. So essentially, this was a toothless
[01:53:28.880 --> 01:53:33.120]   death threat that people who didn't like the way that these companies were moderating content.
[01:53:33.120 --> 01:53:40.080]   That was like what they would hold above their head. But now, we're moving into a world where the
[01:53:40.080 --> 01:53:45.520]   Democrats control the White House, the Senate, and the House of Representatives. And you'll note
[01:53:46.480 --> 01:53:53.360]   that very few of them have come out and said we need to either revoke or amend this law. So
[01:53:53.360 --> 01:53:59.600]   for the next two years, at least, but probably longer, there is not a high likelihood of anything
[01:53:59.600 --> 01:54:04.240]   happening to 230. And honestly, I feel sometimes like I'm the only person who's like, this is actually
[01:54:04.240 --> 01:54:12.560]   a good law, because you shouldn't hold companies liable for what's said by other people on their
[01:54:12.560 --> 01:54:19.040]   forums online, assuming that they do some form of good faith content moderation to keep
[01:54:19.040 --> 01:54:25.200]   illegal things like child porn off of their sites. In fact, it gives you the right to moderate as
[01:54:25.200 --> 01:54:30.400]   well, which is really important. You don't want to take away Twitter's right to moderate.
[01:54:30.400 --> 01:54:38.560]   Exactly right. And so to me, I think that we're about to see the end of this bad faith use of
[01:54:38.560 --> 01:54:44.080]   holding section 230 over these platforms, head to have them lay off the partisan,
[01:54:44.080 --> 01:54:48.800]   either parties messaging. I think that's over.
[01:54:48.800 --> 01:54:54.960]   May I vehemently disagree with what Alex just said?
[01:54:54.960 --> 01:54:56.160]   Oh, good. Yes, of course.
[01:54:56.160 --> 01:54:59.120]   But then somebody who hates section 230.
[01:54:59.120 --> 01:55:06.880]   He said when he said Democrats, basically don't care. I'll tell you two Democrats who have come
[01:55:06.880 --> 01:55:14.800]   out and said in one case that he would repeal it completely. That is the president elect Biden
[01:55:14.800 --> 01:55:25.920]   specifically called out 234 repeal. And the soon to be vice president led a campaign in California
[01:55:25.920 --> 01:55:35.440]   to have cynical campaign in California to carve a giant hole in section 230, charging people with
[01:55:35.440 --> 01:55:38.720]   things she knew were legal despite.
[01:55:38.720 --> 01:55:40.320]   Oh, I did not know about this.
[01:55:40.320 --> 01:55:46.560]   Yeah, I'll send you some info on that. I think the Democrats are.
[01:55:46.560 --> 01:55:48.400]   This was when she was.
[01:55:48.400 --> 01:55:55.360]   Anything more likely to carve to poke a hole in it than the Republicans were,
[01:55:55.360 --> 01:56:00.960]   because Alex is absolutely right that the Republicans had no idea what it did or purported not to.
[01:56:02.320 --> 01:56:09.280]   But I think the Democrats are at least as big a danger in this case.
[01:56:09.280 --> 01:56:12.240]   And the thing that's important to keep in mind,
[01:56:12.240 --> 01:56:20.640]   this is not a law written to protect Twitter and Facebook and Google, which didn't exist
[01:56:20.640 --> 01:56:26.160]   at the time the law was written. It's a law written to promote free speech.
[01:56:26.160 --> 01:56:33.920]   That's the point of the law. And we keep losing that in the debate that's not really a debate.
[01:56:33.920 --> 01:56:41.360]   So I'm exceedingly worried and we almost lost it this time because of,
[01:56:41.360 --> 01:56:47.600]   it can easily be tossed into some omnibus legislation because Congress doesn't pass
[01:56:47.600 --> 01:56:52.640]   like individual laws anymore. I think we're in real danger and very soon.
[01:56:54.720 --> 01:57:01.760]   Harris was losing it. Of section 230 being repealed or gutted.
[01:57:01.760 --> 01:57:07.520]   Harris, when she was California Attorney General was going after the revenge porn
[01:57:07.520 --> 01:57:12.320]   industry, which honestly is a good thing, but the way.
[01:57:12.320 --> 01:57:13.120]   It's worth going after.
[01:57:13.120 --> 01:57:22.400]   Yeah, but the way she was doing it was to peel away the legal protections that keep online platforms
[01:57:23.440 --> 01:57:29.040]   immune. And that was misguided to say the least. It was the wrong direction to take.
[01:57:29.040 --> 01:57:33.920]   Even though it was a popular position, a position I agree with, I think that's the wrong way to go
[01:57:33.920 --> 01:57:41.600]   about it. I believe that Biden's anti-230 comments were offhand. And Jeff Jarvis says that he's
[01:57:41.600 --> 01:57:48.480]   talked to people who have been advising the Biden transition that they have talked with him and
[01:57:48.480 --> 01:57:55.280]   explained why that's a bad idea. So it remains to be seen. I think both Harris and Biden clearly
[01:57:55.280 --> 01:57:59.760]   acted against 230 in the past, but let's hope cooler heads prevail.
[01:57:59.760 --> 01:58:03.760]   Yeah, that's my response to Dan. I think there are things you say as a candidate.
[01:58:03.760 --> 01:58:09.040]   And then it's a completely different ballgame when you're actually there running the country
[01:58:09.040 --> 01:58:14.320]   and you speak with your congressional leaders who have been doing something different.
[01:58:14.320 --> 01:58:18.160]   Is it reasonable to say, as some have said,
[01:58:18.640 --> 01:58:23.040]   well, let's obviously we don't want to repeal it, but maybe it needs to be fixed,
[01:58:23.040 --> 01:58:27.680]   strengthened. How would you strengthen it, Amy?
[01:58:27.680 --> 01:58:36.800]   So again, the central challenge we have in our democracy is that our laws are not responsive,
[01:58:36.800 --> 01:58:44.720]   meaning we can't recalibrate them. We have to create a new law to mitigate the problems in
[01:58:44.720 --> 01:58:52.640]   interpreting that law, given the changing rate. Our system is set up to not have a lot of
[01:58:52.640 --> 01:58:57.520]   gray area, and you want to know what's acceptable and not acceptable. So
[01:58:57.520 --> 01:59:11.120]   if section 230 doesn't exist or is weakened in some way, and also everybody wants the big tech
[01:59:11.120 --> 01:59:17.200]   companies to be broken up or to create a situation where there is profound competition, meaning
[01:59:17.200 --> 01:59:25.360]   there are lots of niche micro sites and lots more distribution of content,
[01:59:25.360 --> 01:59:31.600]   then all of those brand new organizations become vulnerable because they're going to be tiny.
[01:59:31.600 --> 01:59:35.440]   They're not going to be lawyered up. And if somebody tries to sue them,
[01:59:37.520 --> 01:59:43.440]   so Facebook is in other words, Facebook is a little bit safer because it has
[01:59:43.440 --> 01:59:48.800]   enough resources to fight those kinds of actions. It would be all the startups trying to replace
[01:59:48.800 --> 01:59:56.320]   Facebook. That's exactly right. Mike, so again, we can't look at any of these issues in a vacuum.
[01:59:56.320 --> 02:00:05.360]   We have to look at all of these as levers that the push pull of the levers
[02:00:06.480 --> 02:00:13.360]   on how our democracy works because all of these, you know, repealing, not repealing,
[02:00:13.360 --> 02:00:16.640]   strengthening, weakening, they all have these reverberating next-order effects,
[02:00:16.640 --> 02:00:21.680]   which could impact some of the very same things that people are fighting, you know, fighting over
[02:00:21.680 --> 02:00:32.480]   it, just adjacently related areas. Well, trust me, if this comes back to haunt us,
[02:00:32.480 --> 02:00:37.120]   we will be talking about it and fighting to save it because I'm of the opinion that without
[02:00:37.120 --> 02:00:43.520]   230 be very hard to, for us to go forward, then we'd certainly have to shut down all public
[02:00:43.520 --> 02:00:48.400]   comments on all the things we do. And that's a big deal because I can, unlike Mark Zuckerberg,
[02:00:48.400 --> 02:00:54.160]   I can't really afford to fight a lawsuit. It's a really important point that you and Amy make.
[02:00:54.160 --> 02:00:59.040]   When we had GDPR rollout, there was this, you know, view that it was going to apply equally,
[02:00:59.040 --> 02:01:04.560]   but of course, the bigger tech companies are the ones that had the amount of resources necessary
[02:01:04.560 --> 02:01:09.680]   to be able to comply with that law. And that's privacy law in Europe. So what you ended up having
[02:01:09.680 --> 02:01:13.760]   was, you know, Facebook and Google worked fine in Europe, but if you try to go to some of the local
[02:01:13.760 --> 02:01:18.960]   new sites or smaller new sites, they just shut down in Europe because they couldn't operate them.
[02:01:18.960 --> 02:01:23.040]   So considering the second-order effects here is extremely important. And I'm glad we've
[02:01:23.040 --> 02:01:29.840]   discussed it. Yeah. Yeah. Well, if law, if new laws entrench the people that were worried about,
[02:01:29.840 --> 02:01:34.960]   that's obviously something to reconsider. The opposite of what we've been talking about in this
[02:01:34.960 --> 02:01:40.080]   show. Yeah. Somebody said I should start twit.ru. That's not a bad idea.
[02:01:40.080 --> 02:01:47.200]   You can go over with parlor to the loud, loud, super-
[02:01:47.200 --> 02:01:54.000]   great person who was, I think, kicked out of chat. He said something stupid about George Floyd.
[02:01:54.000 --> 02:01:58.960]   I was kicked out of chat, which we have every right to do and Section 230 protects us. He said,
[02:01:58.960 --> 02:02:04.400]   I'm going to sue you. I'm going to sue you. I said, dude, go ahead, because of Section 230.
[02:02:04.400 --> 02:02:11.200]   If there's no Section 230, I have to take that very, very seriously. And so no more banning in
[02:02:11.200 --> 02:02:16.720]   chat, no more, none of that. Everything goes. Oh boy, I would shut it down immediately.
[02:02:16.720 --> 02:02:22.320]   Actually, I've already taken, I shouldn't admit this. I've already taken steps to distance chat.
[02:02:22.320 --> 02:02:28.880]   We no longer run the server. It's not on our servers. It's a community run, which is good.
[02:02:28.880 --> 02:02:34.000]   So it's, I'm putting it at arm's length and it's partly because of that.
[02:02:34.000 --> 02:02:41.520]   Are you talking about the IRC? Yeah. I love them, but I don't want to go to jail for it.
[02:02:42.400 --> 02:02:46.960]   Yeah. I mean, it just goes to show you that running a forum or running up
[02:02:46.960 --> 02:02:52.640]   with user generated content is probably one of the more difficult things that you can do on the
[02:02:52.640 --> 02:02:58.400]   internet. We talked about, how do we get Mark Zuckerberg or Jack Dorsey to do the right thing or
[02:02:58.400 --> 02:03:04.160]   make their platforms a place where everybody can come together and be happy. But once you start
[02:03:04.160 --> 02:03:08.720]   to run a forum like that, we all right, you realize he's your said and done. Do you have a forum,
[02:03:08.720 --> 02:03:14.560]   Alex? I mean, I have comments on my sub stack newsletter and they become active at times.
[02:03:14.560 --> 02:03:20.800]   Oh, yeah. And I just banned someone for the first time this week and I sort of went through the
[02:03:20.800 --> 02:03:25.280]   same thing where it was like, wait a second, this is my, you know, it's my living room here on the
[02:03:25.280 --> 02:03:32.320]   internet and you're calling me a liar. So I've always felt that way. Yeah. Yeah. Yeah. So see you.
[02:03:32.320 --> 02:03:36.880]   See you. And that is not illegal. I mean, it's a, you have the right to ban people to kick
[02:03:36.880 --> 02:03:41.200]   totally to censor people. But they also have the right to sue you if you don't have the
[02:03:41.200 --> 02:03:47.360]   sec, the immunity section 230 provides you. Yeah. So here's to that law. Here's to here's to 230.
[02:03:47.360 --> 02:03:52.160]   I'll drink to that. Our show today brought to you quite literally by our brand new Tricaster. I want
[02:03:52.160 --> 02:04:00.240]   to give some real props to a new tech and their new Tricaster to elite. John is loving it. All of
[02:04:00.240 --> 02:04:04.800]   our engineers are loving it. It's the most complete live production system on the planet.
[02:04:04.800 --> 02:04:11.200]   We use one tenth of its capabilities, but we still love our Tricasters. Twit wouldn't exist
[02:04:11.200 --> 02:04:16.000]   without the Tricaster. When we started streaming video, of course, we chose Tricaster. And now
[02:04:16.000 --> 02:04:21.520]   we're using the latest and greatest. It's more than a live video production system. It's a, it's a,
[02:04:21.520 --> 02:04:27.760]   it's an all encompassing digital media solution. People can use to create content for internet,
[02:04:27.760 --> 02:04:35.840]   mobile and television distribution. It's used in so many places. Our new board that board you see
[02:04:35.840 --> 02:04:40.480]   right there has an instant replay button. We're trying to figure out how we can use that.
[02:04:40.480 --> 02:04:44.800]   Maybe the next time I make a, it has 10, nine instant replay buttons. Wow.
[02:04:44.800 --> 02:04:52.480]   It's software driven IP native technology that gives you the capability, connectivity,
[02:04:52.480 --> 02:04:58.720]   control to take on any sort of digital media production. And it's portable. I mean, we've got
[02:04:58.720 --> 02:05:03.040]   a rack mounted, but, but we've also taken it on the road. It's amazing when you consider the
[02:05:03.040 --> 02:05:09.840]   sheer variety of features that new tech offers with their Tricaster series. No other live production
[02:05:09.840 --> 02:05:15.600]   solution enables digital workflows like the Tricaster to elite. There's live call connect,
[02:05:15.600 --> 02:05:19.920]   which we're going to, we haven't started using, but I'm excited about that. It lets you connect
[02:05:19.920 --> 02:05:25.040]   to anyone, anywhere on any device with high quality video and audio. You can use Skype,
[02:05:25.040 --> 02:05:31.680]   Microsoft Teams, Zoom, meeting, Slack, everything. There's the integrated live set technology. You
[02:05:31.680 --> 02:05:37.120]   may not realize it, but I'm actually at home in my jammies. But thanks to new techs, Tricaster to
[02:05:37.120 --> 02:05:42.880]   elite, I look like I'm in a studio dress. No, I am. We don't, that's one feature we probably
[02:05:42.880 --> 02:05:47.680]   should use more of it. But the live set technology is amazing. And you get full access to a premium
[02:05:47.680 --> 02:05:53.920]   library of virtual sets. So you can create a simulated environment that suits your style and
[02:05:53.920 --> 02:05:58.320]   your brand and men. They have such a good matte technology built in. It's so amazing.
[02:05:58.320 --> 02:06:02.640]   There don't leave out the live graphics creator plugin. Either you can design animated titles,
[02:06:02.640 --> 02:06:06.480]   motion graphics. I'm sure Anthony's going to start doing some stuff with that looping effects.
[02:06:06.480 --> 02:06:12.160]   And you can do it with creative cloud. Photo, the lower third you're seeing was created in Photoshop.
[02:06:12.160 --> 02:06:16.800]   And then import it directly into your Tricaster live production system. It's one of the reasons
[02:06:16.800 --> 02:06:21.280]   our fonts look so much better now because we're using Photoshop. Live graphics creator, let's
[02:06:21.280 --> 02:06:27.680]   produce and present spectacular live graphics faster and easier than ever before. We're just
[02:06:27.680 --> 02:06:32.720]   tech, we're just scratching the surface with this new Tricaster to elite. And when you're ready to
[02:06:32.720 --> 02:06:36.560]   take your production live, that's when we really get excited. That's what we do. You can stream it
[02:06:36.560 --> 02:06:41.200]   live to your choice of new media platforms, the convenient presets for Facebook live,
[02:06:41.200 --> 02:06:49.280]   Microsoft Azure, Twitch, YouTube live and more. It's flexible. It's powerful. It's easy to use.
[02:06:49.280 --> 02:06:54.960]   But most importantly, Tricaster to elite frees you from the boundaries of live video production.
[02:06:54.960 --> 02:07:00.880]   The skies, the limit, it's transformative. It's better than broadcast. And we love it. Thank you,
[02:07:00.880 --> 02:07:07.920]   new tech for our Tricaster to elite. Go to new tech, any W T E K dot com slash Tricaster.
[02:07:07.920 --> 02:07:12.160]   There's an interactive guide that'll help you choose the Tricaster right for you.
[02:07:12.160 --> 02:07:17.200]   We're pretty happy to have the top of the line. I got to say the Tricaster to elite new tech,
[02:07:17.200 --> 02:07:25.360]   any W T E K dot com slash Tricaster. And I think, you know, as we assimilate the capabilities of
[02:07:25.360 --> 02:07:30.080]   this, you're going to see more and more of the Tricaster to elites features be used. We're
[02:07:30.080 --> 02:07:35.520]   really excited about the NDI, the network digital interface that replaces SDI and like gives us
[02:07:35.520 --> 02:07:42.000]   all sorts of capability. It's really, really fantastic. New tech dot com slash Tricaster.
[02:07:42.000 --> 02:07:49.360]   Thank you, new tech. We are eternally grateful. You know what? I bet there's about to be a bunch
[02:07:49.360 --> 02:07:55.840]   of lawsuits though, because there was so much community sleuthing. I don't know what to call it.
[02:07:57.040 --> 02:08:06.800]   The days since the stampede on the capital, you know, all of these people and organizations are
[02:08:06.800 --> 02:08:12.880]   going through meticulously every single image and every single person to identify everybody who
[02:08:12.880 --> 02:08:21.440]   took part in that. And they're doing that really effectively and funneling that back up to law
[02:08:21.440 --> 02:08:26.960]   enforcement, which is a good thing. But I just wonder how many people are now going to turn around
[02:08:26.960 --> 02:08:31.520]   like a whole bunch of people just got put on a permanent no fly list. There are all these videos
[02:08:31.520 --> 02:08:37.680]   now going around showing the people who were who attacked the capital at the airport finding
[02:08:37.680 --> 02:08:42.960]   out that they are no longer allowed to fly. So they're all of these additional, this is an entire
[02:08:42.960 --> 02:08:50.720]   like swirl of media that's generated. And I guess technically they were in a public space.
[02:08:50.720 --> 02:08:56.240]   This is probably having more to do with, I don't think section 230 in our center, maybe
[02:08:56.240 --> 02:09:02.000]   in our intersection some ways. But my point is there's a whole bunch of people who were about to
[02:09:02.000 --> 02:09:06.400]   find out that they've lost a lot of their privileges and are going to be arrested and a bunch of
[02:09:06.400 --> 02:09:11.040]   other things because they were identified even if they had their faces mostly covered up.
[02:09:11.040 --> 02:09:15.520]   Yeah, well, just a point, you know, next time you want to take over the capital building,
[02:09:15.520 --> 02:09:20.720]   don't take selfies. And remember, there's a lot of cameras around, a lot of them.
[02:09:22.000 --> 02:09:27.040]   This is probably the best documented attempt, coup attempt ever in history.
[02:09:27.040 --> 02:09:33.680]   I have a friend who works. So I've got some friends who work for the capital police and and.
[02:09:33.680 --> 02:09:41.120]   Oh, I'd love to hear their story. So, yes, I don't think I should tell it.
[02:09:41.120 --> 02:09:48.720]   But I will say this. One of them said, a military coup. We think of a coup
[02:09:50.160 --> 02:09:57.280]   as having the sort of person in charge direct the military to do whatever they want.
[02:09:57.280 --> 02:10:05.360]   But in effect, directing them the National Guard to not act was also a use of military forces.
[02:10:05.360 --> 02:10:13.440]   It's a good point. I mean, it was a coup in a different way by methodically and actively
[02:10:13.440 --> 02:10:16.400]   not having anybody show up putting.
[02:10:16.960 --> 02:10:21.040]   Maryland's governor, Hogan is hopping man. He said, I kept trying off from the National Guard.
[02:10:21.040 --> 02:10:27.520]   As he should be, he should be pissed. Yeah. Yeah. There's a there's a whole lot that we're still
[02:10:27.520 --> 02:10:37.280]   yet to learn about all of these things. And I I I think Amy's points are really important. One
[02:10:37.280 --> 02:10:50.800]   about people being identified. And for all the sort of quiet pleasure I get in bad people being
[02:10:50.800 --> 02:10:57.120]   called to account for what they've done, I'm a little worried about some of the follow on effects
[02:10:57.120 --> 02:11:06.800]   of the notion that we can that everything we do in public, even if it's not meant to be done
[02:11:07.280 --> 02:11:12.480]   might someday be used against us. And then the other thing, which is that again and again
[02:11:12.480 --> 02:11:18.400]   in these situations, people who are actually not there have been accused of having been there
[02:11:18.400 --> 02:11:25.360]   going back a long time, but the Boston bombing a few years ago is such a case.
[02:11:25.360 --> 02:11:35.840]   We have to be really careful that we that the tiring and feathering that we might want to do
[02:11:36.720 --> 02:11:40.560]   first of all, don't do it to people who weren't there. But
[02:11:40.560 --> 02:11:53.520]   at what point is the punishment disproportionate? And we're not we don't know yet in a lot of the
[02:11:53.520 --> 02:12:01.600]   situations and we have to be it's the wrong time to be making laws is when you're in a total anger
[02:12:02.560 --> 02:12:12.240]   phase as as the Patriot Act showed and we we I think this is related to the sort of
[02:12:12.240 --> 02:12:22.320]   shoot then aim things we do on social media. We should try to find ways to take a breath
[02:12:22.320 --> 02:12:30.960]   and not overreact instantly. And in many of these cases we're not overreacting. But I'm concerned
[02:12:30.960 --> 02:12:42.240]   about our you know, there's a certain level of anger that people get and then stop seeing clearly.
[02:12:42.240 --> 02:12:45.760]   We've talked about this before, Amy. I mean, you were talking about it earlier,
[02:12:45.760 --> 02:12:53.120]   government is intentionally slow and slow moving and deliberative. And while that's difficult
[02:12:53.120 --> 02:12:57.760]   and when it comes to regulating technology, which moves at about 10 times faster speed,
[02:12:58.480 --> 02:13:04.880]   they're not such it's not such a bad idea to to. Well, no, I mean, look at the the rancor and the
[02:13:04.880 --> 02:13:11.440]   the emotional rollercoaster that we've been on, which has been challenging for everybody,
[02:13:11.440 --> 02:13:17.840]   regardless of what your politics are happened because we faced such an abrupt and decisive change
[02:13:17.840 --> 02:13:23.360]   between the Obama administration and the Trump administration. You know, that amount of change
[02:13:23.360 --> 02:13:28.720]   in government and all of the again, that had next order consequences. So all of the other
[02:13:28.720 --> 02:13:37.040]   changes that that entailed became really difficult for us to sort of deal with in society.
[02:13:37.040 --> 02:13:45.440]   So we don't want government. We don't want that to happen. We don't want this level of rancor and
[02:13:46.480 --> 02:13:54.160]   you know, political we want this amount of political rancor to infiltrate every single aspect of
[02:13:54.160 --> 02:14:01.920]   our daily lives, whether that is how technology is being used or stuff having to do with cars or,
[02:14:01.920 --> 02:14:09.440]   you know, whatever. So we actually want our government to move deliberately, slowly,
[02:14:10.400 --> 02:14:16.640]   with less politics being a part of it. So I think the challenge for us going forward is figuring
[02:14:16.640 --> 02:14:24.800]   out how to slow everything down. Literally, as we've been talking, I'm Nancy Pelosi just sent a
[02:14:24.800 --> 02:14:31.520]   letter that is now posted on the speaker's website saying that the house is voting that they've
[02:14:31.520 --> 02:14:38.720]   basically called on Pence to respond within 24 hours to a resolution calling to invoke the 25th
[02:14:38.720 --> 02:14:42.320]   amendment. So I don't know if it works that way, but okay.
[02:14:42.320 --> 02:14:48.400]   But the fact that that is, I don't think the point of that was Pence. I think the point of that
[02:14:48.400 --> 02:14:56.960]   was a public display via using the media and social media. And what does that do? It further
[02:14:56.960 --> 02:15:04.320]   inflames everybody. So we're operating, government isn't, we need to be more thoughtful. I mean,
[02:15:04.320 --> 02:15:09.920]   I know this sounds like common sense, but we don't act this way. We all need to be much more thoughtful
[02:15:09.920 --> 02:15:15.600]   and think through the consequences of the decisions or non-decisions that we're making.
[02:15:15.600 --> 02:15:21.120]   Last thing, because I mentioned Capitol Police. So I'm friends with the person who used to be the
[02:15:21.120 --> 02:15:30.720]   chief police for Washington, D.C. And she was telling me about the challenge in Washington, D.C.
[02:15:30.720 --> 02:15:35.920]   There are, I think, this number may not be right, but I think there's something like 27 different
[02:15:35.920 --> 02:15:42.880]   police forces all operating in the district, which means that depending on like between where they
[02:15:42.880 --> 02:15:47.840]   were standing on the Capitol and the sidewalk across the street, there may be three or four
[02:15:47.840 --> 02:15:55.120]   different jurisdictions. And they slice and dice that very specifically, which means that if
[02:15:55.840 --> 02:16:01.520]   there is a threat, if something is about to happen and you know, because you've been seeing it bubble
[02:16:01.520 --> 02:16:07.040]   up, that you can't have a federation of 27 different police forces that are just going to
[02:16:07.040 --> 02:16:14.000]   magically come together like Voltron and save the day. You need a higher authority, you need more
[02:16:14.000 --> 02:16:23.760]   leadership. And in the absence of that leadership, one has to ask, if you're not putting government
[02:16:23.760 --> 02:16:29.280]   to work in a positive way, then again, all we're left with is that it was deliberate.
[02:16:29.280 --> 02:16:31.840]   It was deliberate. I mean, how sad is that for?
[02:16:31.840 --> 02:16:39.440]   It may have an unintended consequence, because I think there's now a bit of a drumbeat to make D.C.
[02:16:39.440 --> 02:16:46.480]   estate, which would solve some of those problems, but that would also guarantee two more democratic
[02:16:46.480 --> 02:16:51.840]   senators. So that would be kind of. And we need to change the flag, which nobody wants to do.
[02:16:51.840 --> 02:16:55.440]   Nobody wants to do that. That's a big thing. Yeah. That's a that's a lot of work.
[02:16:55.440 --> 02:16:59.520]   Some I'll end with a couple of good news points before we take a break.
[02:16:59.520 --> 02:17:08.400]   SolarWinds has hired Chris Krebs as a consultant. Krebs, of course, the former Cyber Security Chief,
[02:17:08.400 --> 02:17:14.960]   who got fired because he said the election was flawless. But I think that's a good sign. And in
[02:17:14.960 --> 02:17:21.200]   the State Department is setting up a new Bureau for Cyber Security and emerging technologies. I don't
[02:17:21.200 --> 02:17:27.280]   know if this is going to be what you were advocating with this Office of Strategic Planning, but it's
[02:17:27.280 --> 02:17:35.680]   a step in the right direction. CSET will help lead diplomatic efforts around security,
[02:17:35.680 --> 02:17:41.120]   including working to prevent cyber conflicts with potentially adversarial nations. At this point,
[02:17:41.120 --> 02:17:46.400]   a diplomatic solution to the SolarWinds hack seems like the probably the only reasonable
[02:17:47.040 --> 02:17:54.320]   approach. There's certainly no technical solution. It's pretty much the worst hack anybody's ever seen.
[02:17:54.320 --> 02:18:01.840]   So a couple of, I think, good steps in the right direction. Interesting article on
[02:18:01.840 --> 02:18:09.200]   Substack as well by Matt Stoller, his big newsletter, where he talks about the real problem at SolarWinds
[02:18:09.200 --> 02:18:20.080]   was the fact that it was bought by a private equity company, Orlando Bravo, which immediately,
[02:18:20.080 --> 02:18:25.680]   as has happens with private equity companies, cut costs, cut security, moved its programmers
[02:18:25.680 --> 02:18:35.840]   to Eastern Europe, and in effect, created a real threat to a security company that offers
[02:18:35.840 --> 02:18:43.360]   software is used by millions of private firms. So that's another kind of interesting issue.
[02:18:43.360 --> 02:18:50.160]   That's amazing. I didn't hear that piece. I would equity kind of wrecks everything it touches.
[02:18:50.160 --> 02:18:55.840]   Yeah. And you don't want it to be touching things relating to national security,
[02:18:55.840 --> 02:19:03.200]   because it wrecks. I'm really surprised. It touches. I'm shocked that that sale was allowed to go
[02:19:03.200 --> 02:19:06.800]   through. Well, what Stoller suggests is that when you
[02:19:06.800 --> 02:19:13.280]   procure, certainly in government, but anytime you procure a security solution,
[02:19:13.280 --> 02:19:19.840]   you might want to consider who owns it and what their track record is. This is one of the reasons
[02:19:19.840 --> 02:19:28.000]   SolarWinds update server's password was SolarWinds 1, 2, 3. It wasn't well secured. And as a result,
[02:19:29.040 --> 02:19:36.640]   essentially, every government agency has been hacked and many other companies, private and
[02:19:36.640 --> 02:19:42.240]   public. They should have gone with password. That would have been password is much more effective.
[02:19:42.240 --> 02:19:46.000]   Yeah, that's the space balls password. Right with a zero instead of a row.
[02:19:46.000 --> 02:19:51.040]   But it's good they added the 123 though. Oh, yeah. A little bit.
[02:19:51.040 --> 02:19:56.000]   Well, some, some. I have to try that to see if I get into my face, my Bitcoin wall.
[02:19:56.640 --> 02:20:01.280]   Maybe I might have used that actually. Bitcoin, what is it now? $40,000?
[02:20:01.280 --> 02:20:09.760]   Down to $37. Damn it. I knew I should have sold. It's great article in Bloomberg. Day trader
[02:20:09.760 --> 02:20:17.360]   heaven arrives as Tesla, Bitcoin and stock options surge. Everybody looks like they're geniuses
[02:20:17.360 --> 02:20:23.840]   till it crashes. But I'm not going to say anything. Actually, somebody, I saw somebody say,
[02:20:23.840 --> 02:20:29.280]   somebody like I thought was intelligent, say, oh, now Bitcoin can never crash. It's only up, up, up.
[02:20:29.280 --> 02:20:36.160]   Now there's a method. I'm really glad to see there's a way to go short.
[02:20:36.160 --> 02:20:43.360]   Oh, short Bitcoin, huh? Well, we're by puts anyway. I mean, this is a great,
[02:20:43.360 --> 02:20:50.000]   great opportunity to make some money. Yeah. It's so volatile. Yeah, it's so volatile.
[02:20:51.280 --> 02:20:57.200]   We always end the show with the obituaries. The father of fiber optics passed. He lived to a
[02:20:57.200 --> 02:21:05.840]   ripe old 94, Narendra S. Kapani, a physicist and entrepreneur who did more than anyone to make
[02:21:05.840 --> 02:21:12.000]   optical research a priority in government and corporate budgets. It's kind of, this is a
[02:21:12.000 --> 02:21:18.800]   picture of him in 1955, is very early work. And it's kind of amazing, really. The amount of data,
[02:21:19.760 --> 02:21:27.120]   glass fibers can carry when you pass light down it is astounding. He was doing that in 1955.
[02:21:27.120 --> 02:21:36.000]   It's in 1955. Yeah. It's really amazing. Yes, I know. I know. He was at the Imperial College
[02:21:36.000 --> 02:21:43.040]   London. He entered graduate school there in '52. For decades, researchers across Europe,
[02:21:43.040 --> 02:21:46.480]   I'm reading from the New York Times obituary, had been studying ways to transmit light through
[02:21:46.480 --> 02:21:52.480]   flexible glass fibers. But a host of technical challenges had set them back. He persuaded
[02:21:52.480 --> 02:21:59.600]   Harold Hopkins to hire him as a research assistant. Hopkins, as a theoretician, provided the ideas,
[02:21:59.600 --> 02:22:06.480]   Dr. Kapani figured out the practical side. And in 1954, they published in the journal Nature
[02:22:06.480 --> 02:22:12.880]   demonstrating how to bundle thousands of impossibly thin glass fibers together and then connect them
[02:22:13.440 --> 02:22:19.840]   and to end. Yeah. 1954. That's amazing. Yeah. That's amazing. Yeah. And a good reminder that it
[02:22:19.840 --> 02:22:26.800]   sometimes it's a good reminder for us to be patient because groundbreaking technology
[02:22:26.800 --> 02:22:34.320]   takes time to operationalize and bring to the market. Yeah. Who is it that said,
[02:22:34.320 --> 02:22:40.080]   new technologies both take longer than you expect and happen faster than you think,
[02:22:40.640 --> 02:22:43.920]   or longer than you think and faster than you expect. Yeah. I've heard flavors of that out of
[02:22:43.920 --> 02:22:53.120]   the gates and many of it. Yeah. Yeah. Along those lines, Amy, we're still waiting for a lot of the
[02:22:53.120 --> 02:22:58.000]   internet access providers in the United States to discover fun. Yeah. Yeah. Some day, they'll
[02:22:58.000 --> 02:23:03.360]   wonderful when they'll discover it and boy, oh boy, that'll be amazing. Quibi, which raised
[02:23:03.360 --> 02:23:10.880]   $1.75 billion and then failed in six months, has sold off its assets to Roku for less than $100
[02:23:10.880 --> 02:23:16.480]   million. According to our aesthetic, I think I got a little money back. I don't know what,
[02:23:16.480 --> 02:23:21.840]   I have no idea what Roku wants to create a Quibi channel, I think. The good news is there are
[02:23:21.840 --> 02:23:29.360]   Quibi producers, documentarians and such, who were cut off with the failure of Quibi in the middle
[02:23:29.360 --> 02:23:34.400]   of their production. This will give them some opportunity to finish what they were doing,
[02:23:34.400 --> 02:23:39.200]   which is a good thing. So Roku bought not only the existing Quibi library, but anything in
[02:23:39.200 --> 02:23:47.200]   production now. This is how we know that 2021 is going to be a good year. Yeah. Because if Quibi
[02:23:47.200 --> 02:23:51.840]   can be revived in some form this year, then things are looking up for us.
[02:23:51.840 --> 02:23:57.120]   Even better. They might have democratic stability. Who knows? And this pandemic.
[02:23:57.120 --> 02:24:01.040]   And then, of course, we'll be able to realize Quibi's initial vision,
[02:24:01.040 --> 02:24:06.560]   which is we watched those short little on a TV finally, but even without it, I think it would be
[02:24:06.560 --> 02:24:11.200]   fine. There was some good stuff on Quibi. I just, what they did not buy by the way,
[02:24:11.200 --> 02:24:14.080]   Roku did not get the technology that let you switch the time.
[02:24:14.080 --> 02:24:19.200]   Well, that's whatever that magic, they had a name for it. I don't remember what that was.
[02:24:19.200 --> 02:24:21.760]   That was the innovation. That was actually pretty slick.
[02:24:21.760 --> 02:24:24.800]   Yeah, that nobody cared about. Yeah. All right.
[02:24:25.920 --> 02:24:28.560]   That's creative destruction. I can really get behind.
[02:24:28.560 --> 02:24:34.880]   Hey, what a great panel you are. I thank you for spending some time, a lot of time with us,
[02:24:34.880 --> 02:24:41.840]   discussing some of the hardest issues in technology today and with a plumb and smarts
[02:24:41.840 --> 02:24:47.440]   and you made it fun. So thank you, Amy Webb. It's always a thrill to have you on.
[02:24:47.440 --> 02:24:51.920]   I really appreciate everything you do. Her books are... Can I say a thanks? Oh, sorry.
[02:24:51.920 --> 02:24:58.960]   Let me give you your plug. Then you can say anything you want. Her books are at AmyWeb.io,
[02:24:58.960 --> 02:25:05.840]   the big nine, the signals are talking. And her first book, "Data a Love Story,
[02:25:05.840 --> 02:25:12.080]   How I Cracked the Online Dating Code to Meet My Match." Yes, she's happily married and has a
[02:25:12.080 --> 02:25:17.360]   lovely daughter. That's all three of those are well worth reading. Of course, her company is the
[02:25:17.360 --> 02:25:26.160]   Future Today Institute. You can go there, FutureTodayInstitute.com and play with what is one of the most
[02:25:26.160 --> 02:25:33.840]   coolest things on any website, which is this little tool that lets you predict the future by tying
[02:25:33.840 --> 02:25:39.760]   together disparate threads. And then you can also get the calendar, which I highly recommend.
[02:25:39.760 --> 02:25:45.200]   There's the threads. We can drag these around and show what they do and all of that stuff.
[02:25:45.200 --> 02:25:50.800]   That's got to be it. Maybe that's in the menu. No, that's it. It's not somewhat interactive.
[02:25:50.800 --> 02:25:55.040]   We're building out more functionality. Good.
[02:25:55.040 --> 02:26:02.640]   But actually, thank you for having me on as always. But I also want to say thank you to
[02:26:02.640 --> 02:26:08.640]   everybody who works in IT, who I know is listening. You've got a rough couple of weeks ahead of you.
[02:26:08.640 --> 02:26:13.680]   I consider you to be first responders. I know that you don't get the thanks that you deserve.
[02:26:14.560 --> 02:26:19.120]   Because you were the ones who are keeping everything going. So I just wanted to say,
[02:26:19.120 --> 02:26:23.600]   I just wanted to know that you are appreciated and that I thank you for all of the work that
[02:26:23.600 --> 02:26:30.480]   you're doing. So well said. And man, if you're an IT in the Capitol building and you know that
[02:26:30.480 --> 02:26:36.240]   the evil made problem to a thousand, you know, all of those machines, people fled their offices
[02:26:36.240 --> 02:26:40.960]   without logging out, without shutting down. And you had people playing with them and you have no
[02:26:40.960 --> 02:26:48.080]   idea what's been compromised, what hasn't. I know you're going to have a tough couple of weeks.
[02:26:48.080 --> 02:26:52.000]   So thank you. That's really great. Thank you, Amy. We always forget that.
[02:26:52.000 --> 02:26:57.680]   It's great to see you, Dan Gilmore. It's been too long. We'll get you back very soon.
[02:26:57.680 --> 02:27:04.880]   Dangilmore.com. He's at Arizona State working on media literacy, making people smarter.
[02:27:07.040 --> 02:27:11.920]   Is that at ASU's Walter Cronkite school or where is that? Yes, it is.
[02:27:11.920 --> 02:27:18.320]   Oh man, that's a great place. Walter Cronkite school of journalism and mass communication
[02:27:18.320 --> 02:27:24.000]   in the lab there. Really great. Dangilmore.com. Thank you, Dan.
[02:27:24.000 --> 02:27:28.800]   It's great to be with you again and we'll do it again. I will. We will soon.
[02:27:28.800 --> 02:27:32.480]   You know, it's funny. We got a new producer, Jason Howell, and he said,
[02:27:32.480 --> 02:27:36.880]   you know, you haven't had Dan Gilmore in a while. He said, what? So I was very pleased.
[02:27:36.880 --> 02:27:40.160]   I don't, you know, these things, they slipped my mind. That's what we have producers for
[02:27:40.160 --> 02:27:46.000]   because my mind is just a bunch of mush. And Jason also said, and by the way,
[02:27:46.000 --> 02:27:53.200]   you got to get Alex on too. Alex Cantorwitz, his big technology newsletter is at bigtechnology.substack.com.
[02:27:53.200 --> 02:27:57.600]   How much does that cost to subscribe to that? It's free for now.
[02:27:57.600 --> 02:28:01.360]   What? If you want to add support, it's free to subscribe.
[02:28:02.480 --> 02:28:07.520]   And we'll see how things go down the line. You've worked. Tell me some of the places you've
[02:28:07.520 --> 02:28:13.760]   worked. You had a lot of these big name tech sites. Yeah, I was part of the original team that
[02:28:13.760 --> 02:28:19.840]   came over as BuzzFeed started its Bureau here in Silicon Valley and was at ad age before that.
[02:28:19.840 --> 02:28:25.280]   And previously, I actually spent some time working in tech. I was selling ad tech before I made my
[02:28:25.280 --> 02:28:31.200]   way into journalism. So anybody out there wants to jam about order management systems for public
[02:28:31.200 --> 02:28:36.240]   insurance. I'd be happy to talk with you. It's been great to meet you, Alex. And you were great to
[02:28:36.240 --> 02:28:40.240]   have on the show. We'll have you back real soon, Alex Cantorwitz, big technology that
[02:28:40.240 --> 02:28:45.920]   substack.com and search for his big technology podcast. You just had, I really want to listen to
[02:28:45.920 --> 02:28:51.920]   the one you just did with Kevin Russe. Yeah. That is very timely. The making of a YouTube
[02:28:51.920 --> 02:28:57.760]   radical. Kevin talks about how YouTube can radicalize. And then you had Mark Ledwich
[02:28:57.760 --> 02:29:03.280]   taking issue saying, you know, my research shows that YouTube does not radicalize.
[02:29:03.280 --> 02:29:09.120]   What a great topic. Yeah, it was super fun. Yeah. Mark was basically like telling Kevin,
[02:29:09.120 --> 02:29:15.040]   I don't agree with what you're saying. And I will debate you about it, but only if we do it on big
[02:29:15.040 --> 02:29:20.080]   technology podcasts. So perfect. It's only a couple months old, but it was a good sign. And
[02:29:20.080 --> 02:29:25.680]   we got some new good shows coming up this upcoming week. I had Ryan Mack from BuzzFeed News, former
[02:29:25.680 --> 02:29:30.400]   colleague of mine on, and then the Twitter ban of Trump broke in the middle of that discussion.
[02:29:30.400 --> 02:29:33.760]   So our reaction is on that. Wow. We kind of give some live
[02:29:33.760 --> 02:29:38.320]   live updates about what it means and where things go from here. You also interview my personal
[02:29:38.320 --> 02:29:44.880]   peloton instructor, Emma Lovewell. Emma Lovewell, I mean, she's unbelievable. I take all her classes.
[02:29:44.880 --> 02:29:50.800]   I take all her classes. Every time I love her classes. Live well, love well, she says at the end
[02:29:50.800 --> 02:29:59.440]   of them and I'm going, I catch my breath. So I'll be listening to that one big technology podcast.
[02:29:59.440 --> 02:30:04.320]   It's on Apple podcast and everywhere you get your podcast. Of course, we are too. Don't forget to
[02:30:04.320 --> 02:30:09.520]   take our survey. We really appreciate that. We do Twitter every Sunday afternoon, 230 Pacific
[02:30:09.520 --> 02:30:16.240]   530 Eastern, 2230 UTC. If you like to watch the sausage being made, we do have a behind the scenes
[02:30:16.240 --> 02:30:22.240]   audio and video feed of everything we do. Unedited, unexpergated. The truth comes out
[02:30:22.240 --> 02:30:28.880]   sometimes before and after the shows. You can watch that at twit.tv/live. If you're watching live,
[02:30:28.880 --> 02:30:35.200]   you can watch live with a lot of other people watching live in our arms length chat room at irc.
[02:30:35.200 --> 02:30:43.040]   TV. They're not really arms length. They're just, I can almost reach them. Twit, irc.twit.tv.
[02:30:43.040 --> 02:30:48.480]   All of our shows for everything we do available on our website, of course, twit.tv. There's a
[02:30:48.480 --> 02:30:53.920]   YouTube channel for every show, including this one this week in tech on YouTube. And of course,
[02:30:53.920 --> 02:30:59.840]   it's a podcast so you can subscribe audio or video. Use your favorite podcast client and
[02:30:59.840 --> 02:31:04.240]   subscribe. That way you'll get it automatically every Sunday evening after the show's over.
[02:31:04.240 --> 02:31:11.600]   Thank you everybody for being here. Good luck next week. Please stay safe. Wear your mask. Wash
[02:31:11.600 --> 02:31:17.920]   your hands socially distance because we can't afford to lose another one of you. So please,
[02:31:17.920 --> 02:31:21.040]   we'll see you next week. Another twit. This is amazing.
[02:31:21.040 --> 02:31:30.960]   [Music]
[02:31:30.960 --> 02:31:54.500]   [ Silence ]

