
[00:00:00.000 --> 00:00:02.160]   Hey, it's time for Twit this week in Tech.
[00:00:02.160 --> 00:00:04.480]   Ashley Esqueda is here from CNET.
[00:00:04.480 --> 00:00:05.960]   First time since her baby was born.
[00:00:05.960 --> 00:00:07.800]   Denise Howell is here.
[00:00:07.800 --> 00:00:11.040]   And from Bloomberg, their tech editor, Nate Langston,
[00:00:11.040 --> 00:00:15.340]   we've got lots to talk about, including the confusion in Iowa.
[00:00:15.340 --> 00:00:19.600]   The NYPD dropping their notebooks for an iPhone app,
[00:00:19.600 --> 00:00:23.520]   GeForce Now, Hue, Lightbulbs, and Deep Fakes.
[00:00:23.520 --> 00:00:25.400]   Deep Fakes on the internet.
[00:00:25.400 --> 00:00:28.280]   It's all coming up next on Twit.
[00:00:28.280 --> 00:00:31.280]   This week in Tech comes to you from our Twit Last Pass
[00:00:31.280 --> 00:00:32.160]   studios.
[00:00:32.160 --> 00:00:35.080]   You're focused on security, but are your employees?
[00:00:35.080 --> 00:00:38.920]   Last Pass can ensure they are, by making access and authentication
[00:00:38.920 --> 00:00:39.680]   seamless.
[00:00:39.680 --> 00:00:44.920]   Visit lasspass.com/twit to learn more.
[00:00:44.920 --> 00:00:47.240]   Podcasts you love.
[00:00:47.240 --> 00:00:49.760]   From people you trust.
[00:00:49.760 --> 00:00:51.160]   This is Twit.
[00:00:51.160 --> 00:00:53.960]   [MUSIC PLAYING]
[00:00:53.960 --> 00:00:58.440]   [MUSIC PLAYING]
[00:00:58.440 --> 00:01:00.960]   This is Twit this week at Tech.
[00:01:00.960 --> 00:01:06.280]   Episode 757, recorded Sunday, February 9, 2020.
[00:01:06.280 --> 00:01:09.280]   My fridge killed my Apple TV.
[00:01:09.280 --> 00:01:11.920]   This week at Tech is brought to you by Casper.
[00:01:11.920 --> 00:01:15.480]   Casper is a sleep brand that makes expertly designed products
[00:01:15.480 --> 00:01:18.520]   to help you get your best rest, one night at a time.
[00:01:18.520 --> 00:01:21.560]   Get $100 towards select mattresses by visiting
[00:01:21.560 --> 00:01:27.000]   casper.com/twit1 and using the promo code Twit1 at check.
[00:01:27.000 --> 00:01:28.400]   And by Mint Mobile.
[00:01:28.400 --> 00:01:30.920]   Mint Mobile provides the same premium network coverage
[00:01:30.920 --> 00:01:33.120]   you're used to, but at a fraction of the cost,
[00:01:33.120 --> 00:01:35.280]   because everything's online.
[00:01:35.280 --> 00:01:37.440]   Mint Mobile makes it easy to cut your wireless bill down
[00:01:37.440 --> 00:01:40.680]   to just $15 a month with your three-month introductory plan.
[00:01:40.680 --> 00:01:45.320]   Get the plan shipped to your door free at mintmobile.com/twit.
[00:01:45.320 --> 00:01:47.680]   And by Epson's Eco Tank printers,
[00:01:47.680 --> 00:01:50.440]   now you can kiss expensive cartridges.
[00:01:50.440 --> 00:01:52.800]   Goodbye, the Epson Eco Tank printer
[00:01:52.800 --> 00:01:54.360]   comes with a ridiculous amount of ink.
[00:01:54.360 --> 00:01:55.960]   Just fill and chill.
[00:01:55.960 --> 00:02:00.280]   Check out ecotankprinners@epson.com/eco tankleo.
[00:02:00.280 --> 00:02:02.880]   And by LinkedIn marketing solutions.
[00:02:02.880 --> 00:02:05.720]   LinkedIn can help you speak to the right professionals
[00:02:05.720 --> 00:02:07.280]   at the right time.
[00:02:07.280 --> 00:02:09.880]   Right now, get a free $100 LinkedIn ad credit
[00:02:09.880 --> 00:02:11.560]   to launch your first campaign.
[00:02:11.560 --> 00:02:14.120]   Just visit LinkedIn.com/twit.
[00:02:19.200 --> 00:02:21.760]   It's time for Twit.
[00:02:21.760 --> 00:02:25.320]   The show we cover the week's tech news
[00:02:25.320 --> 00:02:27.640]   with a panel of esteemed journalists
[00:02:27.640 --> 00:02:30.960]   welcoming back Nate Langson, a tech editor for Bloomberg.
[00:02:30.960 --> 00:02:34.000]   He's at a London where Tropical Storms Ciara
[00:02:34.000 --> 00:02:37.280]   is bringing 80 mile an hour winds
[00:02:37.280 --> 00:02:41.120]   and 100 degree temperatures to the Emerald.
[00:02:41.120 --> 00:02:44.000]   No, it's not that warm, I would guess Nate.
[00:02:44.000 --> 00:02:45.680]   - It is not that warm, although,
[00:02:45.680 --> 00:02:48.840]   but I'll be honest, I haven't been outside at all today.
[00:02:48.840 --> 00:02:51.960]   Things are falling over, things are flying through the air.
[00:02:51.960 --> 00:02:54.440]   The cat took one look outside and was like,
[00:02:54.440 --> 00:02:56.200]   "It's just noped his way out of there."
[00:02:56.200 --> 00:02:58.160]   - Not going out there, nope.
[00:02:58.160 --> 00:03:02.320]   - Yeah, you said your wife's greenhouse got creamed.
[00:03:02.320 --> 00:03:04.040]   - Yeah, it did.
[00:03:04.040 --> 00:03:06.800]   This morning, I was just making breakfast, making coffee,
[00:03:06.800 --> 00:03:09.040]   and I pointed out the window and I said,
[00:03:09.040 --> 00:03:12.800]   "Kate, your greenhouse has appeared to have moved."
[00:03:12.800 --> 00:03:15.280]   And yeah, it's on its side at the moment.
[00:03:15.280 --> 00:03:16.600]   There was no point doing anything with it
[00:03:16.600 --> 00:03:18.840]   until the storm comes down, so it's just outside.
[00:03:18.840 --> 00:03:20.520]   - It's the neighbor's problem now.
[00:03:20.520 --> 00:03:21.520]   (laughs)
[00:03:21.520 --> 00:03:23.800]   - It is, as is the fence, which is also blown down
[00:03:23.800 --> 00:03:24.720]   on the side of the house.
[00:03:24.720 --> 00:03:28.000]   - Oh gosh, well stay inside, stay dry, stay warm.
[00:03:28.000 --> 00:03:29.080]   I know it's the middle of the night,
[00:03:29.080 --> 00:03:31.360]   but I appreciate you staying up late with us.
[00:03:31.360 --> 00:03:36.000]   Also with us, Denise Howell, longtime host of This Week in Law
[00:03:36.000 --> 00:03:38.440]   and triangulation, both of which have now been canceled.
[00:03:38.440 --> 00:03:42.080]   So I'm sorry, Denise, I'm glad you at least
[00:03:42.080 --> 00:03:43.960]   come back on Twitter for me.
[00:03:43.960 --> 00:03:46.400]   - Of course, I mean, I hope I'm not a bad lecture.
[00:03:46.400 --> 00:03:50.000]   - It's not you, it's me, believe me, it's me.
[00:03:50.000 --> 00:03:53.080]   Denise Howell, is that Denise Howell, not info?
[00:03:53.080 --> 00:03:54.240]   That's a nice one.
[00:03:54.240 --> 00:03:58.440]   Do you still do the bags and baggage, whatever?
[00:03:58.440 --> 00:04:00.680]   - The baggage was my blog a long time ago.
[00:04:00.680 --> 00:04:03.720]   I was one of the very first lawyers in the world,
[00:04:03.720 --> 00:04:06.080]   I think, that had a web blog.
[00:04:06.080 --> 00:04:08.520]   And that was in the early 2000s.
[00:04:08.520 --> 00:04:11.240]   And no, bag and baggage retired a long time ago,
[00:04:11.240 --> 00:04:15.560]   but it's out there on Interwebs, if anyone's interested.
[00:04:15.560 --> 00:04:18.960]   - Yeah, actually now some bag and baggage store has
[00:04:18.960 --> 00:04:20.640]   as the URL.
[00:04:20.640 --> 00:04:23.840]   - Probably, yeah, literally.
[00:04:23.840 --> 00:04:26.600]   So there you go, great to have you, Denise.
[00:04:26.600 --> 00:04:29.040]   And boy, we gotta welcome Ashley Aschetha back
[00:04:29.040 --> 00:04:30.560]   to our microphones.
[00:04:30.560 --> 00:04:33.880]   It's been a little bit of a time, she's been busy.
[00:04:33.880 --> 00:04:36.360]   I believe there is a child involved.
[00:04:36.360 --> 00:04:41.320]   - Somehow someone allowed me to have a child.
[00:04:41.320 --> 00:04:43.200]   Yeah, they let me take it right out of the hospital.
[00:04:43.200 --> 00:04:44.040]   It was amazing.
[00:04:44.040 --> 00:04:46.280]   I'm a human 3D printer.
[00:04:46.280 --> 00:04:48.160]   I made a person inside me.
[00:04:48.160 --> 00:04:49.160]   It's incredible.
[00:04:49.160 --> 00:04:51.800]   - Congratulations, that's so great.
[00:04:51.800 --> 00:04:55.480]   Little Wolfgang is now, he said seven months old.
[00:04:55.480 --> 00:04:57.320]   - Yeah, he's almost eight months old.
[00:04:57.320 --> 00:04:58.920]   - So it's been a long time.
[00:04:58.920 --> 00:04:59.760]   - A little lot of teethes.
[00:04:59.760 --> 00:05:01.040]   - Oh, he's got teeth.
[00:05:01.040 --> 00:05:03.400]   - Oh, he's close to crawling, but he's cool.
[00:05:03.400 --> 00:05:05.720]   He's like, it's weird, it's like a little computer.
[00:05:05.720 --> 00:05:07.520]   You see him downloading everything all the time,
[00:05:07.520 --> 00:05:08.480]   it's amazing.
[00:05:08.480 --> 00:05:09.920]   - I know you're a senior producer,
[00:05:09.920 --> 00:05:12.680]   it's seen where you're able to take some time off.
[00:05:12.680 --> 00:05:16.040]   - Yeah, I gotta give credit where it's due.
[00:05:16.040 --> 00:05:20.440]   CBS Interactive has a very generous family leave policy.
[00:05:20.440 --> 00:05:23.520]   So I was offered, I think about four months.
[00:05:23.520 --> 00:05:24.360]   It's pretty good.
[00:05:24.360 --> 00:05:25.200]   - That's great.
[00:05:25.200 --> 00:05:28.000]   And I gave you nine months, so I'm even more generous.
[00:05:28.000 --> 00:05:29.320]   - I know, I know.
[00:05:29.320 --> 00:05:32.120]   I was so tired for a while, and I was just like,
[00:05:32.120 --> 00:05:34.120]   I'm sorry Carson, I just have no energy.
[00:05:34.120 --> 00:05:35.000]   I just can't live.
[00:05:35.000 --> 00:05:38.760]   And then at the end, oh my God, I did E3,
[00:05:38.760 --> 00:05:40.120]   nine and a half months pregnant.
[00:05:40.120 --> 00:05:41.720]   - Oh my God.
[00:05:41.720 --> 00:05:43.200]   - That might have been a mistake.
[00:05:43.200 --> 00:05:44.520]   - Oh my God.
[00:05:44.520 --> 00:05:47.360]   - But my feet looked like bread loaps,
[00:05:47.360 --> 00:05:49.240]   it was just, they were so swollen.
[00:05:49.240 --> 00:05:51.840]   But yeah, and then two weeks after that,
[00:05:51.840 --> 00:05:56.440]   I had my son, and so it was a long nine months,
[00:05:56.440 --> 00:05:59.480]   but fortunately, I can't complain, no complications.
[00:05:59.480 --> 00:06:02.960]   My doctor said it was a very boring pregnancy,
[00:06:02.960 --> 00:06:03.800]   which is a good thing.
[00:06:03.800 --> 00:06:06.120]   - Perfect, boring is good in this context.
[00:06:06.120 --> 00:06:08.440]   Well, congratulations, I think that's wonderful.
[00:06:08.440 --> 00:06:09.800]   - Thank you.
[00:06:09.800 --> 00:06:12.560]   It's hard to believe that the Iowa caucus
[00:06:12.560 --> 00:06:13.960]   was less than a week ago.
[00:06:13.960 --> 00:06:19.240]   But honestly, that might be the big tech story of the week
[00:06:19.240 --> 00:06:21.880]   is the reliance of the Iowa Democratic Party
[00:06:21.880 --> 00:06:25.440]   upon an app which had little testing, less training,
[00:06:25.440 --> 00:06:28.480]   and in fact failed on the night of the caucus
[00:06:28.480 --> 00:06:30.680]   to the point where they really didn't get great results
[00:06:30.680 --> 00:06:32.380]   until just a few days ago.
[00:06:32.380 --> 00:06:36.760]   Charlie Wartzell's quote I think in the New York Times
[00:06:36.760 --> 00:06:40.360]   was "App Shadow," which is the company that made the
[00:06:40.360 --> 00:06:43.480]   App Shadows Iowa Failure was a dangerous combination
[00:06:43.480 --> 00:06:47.840]   of techno utopianism and laziness,
[00:06:47.840 --> 00:06:50.120]   which led to slapdash software engineering,
[00:06:50.120 --> 00:06:51.920]   procurement, and deployment.
[00:06:51.920 --> 00:06:55.960]   Blaming everybody involved, particularly I think,
[00:06:55.960 --> 00:06:59.280]   actually this is not the story,
[00:06:59.280 --> 00:07:00.920]   I have clicked the wrong link here.
[00:07:00.920 --> 00:07:03.760]   But this is, but particularly the Iowa Democratic Party
[00:07:03.760 --> 00:07:07.880]   or rolling out an app that it just, it's funny,
[00:07:07.880 --> 00:07:10.720]   it's a combination of both techno utopianism thinking,
[00:07:10.720 --> 00:07:14.520]   well, technology is gonna solve this and make this perfect
[00:07:14.520 --> 00:07:17.120]   and fear that it would get hacked to the point
[00:07:17.120 --> 00:07:19.120]   where they didn't wanna show anybody.
[00:07:19.120 --> 00:07:21.960]   And of course, as a result, the app didn't get vetted,
[00:07:21.960 --> 00:07:24.720]   didn't get checked, didn't get trialed,
[00:07:24.720 --> 00:07:27.240]   and nobody got any training in it.
[00:07:27.240 --> 00:07:30.000]   You couldn't have a worse failure.
[00:07:30.000 --> 00:07:33.440]   Nate, you're watching this from abroad.
[00:07:33.440 --> 00:07:36.720]   Are you laughing at America right now?
[00:07:36.720 --> 00:07:40.520]   No, not at all.
[00:07:40.520 --> 00:07:42.160]   To be perfectly honest, I don't know how much
[00:07:42.160 --> 00:07:43.760]   of this story made it over here.
[00:07:43.760 --> 00:07:45.400]   Oh, interesting.
[00:07:45.400 --> 00:07:48.960]   It was news to me when I saw it in the rundown today.
[00:07:48.960 --> 00:07:52.240]   My, the only point of comparison I had is that when
[00:07:52.240 --> 00:07:55.600]   the conservative party here about a year,
[00:07:55.600 --> 00:07:57.920]   maybe two years ago, had an event,
[00:07:57.920 --> 00:08:00.960]   had an app that could be used,
[00:08:00.960 --> 00:08:05.960]   and all the, the Tory politicians' details were basically
[00:08:05.960 --> 00:08:09.560]   accidentally made available to the public through it,
[00:08:09.560 --> 00:08:12.240]   which was a pretty terrible little blooper.
[00:08:12.240 --> 00:08:14.200]   I don't know if that's the same thing that's going on here,
[00:08:14.200 --> 00:08:15.040]   but.
[00:08:15.040 --> 00:08:17.640]   Well, in a way, I find that interesting
[00:08:17.640 --> 00:08:19.720]   that there wasn't any coverage.
[00:08:19.720 --> 00:08:23.000]   It must seem puzzling to every other country
[00:08:23.000 --> 00:08:25.400]   that our presidential elections,
[00:08:25.400 --> 00:08:27.920]   which I think are fairly important to every other country,
[00:08:27.920 --> 00:08:32.240]   begin with the strangest ritual in high school gymnasiums
[00:08:32.240 --> 00:08:36.120]   in the state of Iowa called a caucus.
[00:08:36.120 --> 00:08:38.840]   Denise, you must have looked at this.
[00:08:38.840 --> 00:08:42.080]   I mean, you're at least following this in the US
[00:08:42.080 --> 00:08:44.880]   and thought this is as bad as it can get.
[00:08:44.880 --> 00:08:47.880]   Yeah, I mean, I'm not gonna pretend to understand
[00:08:47.880 --> 00:08:49.120]   the whole caucus system.
[00:08:49.120 --> 00:08:52.040]   I don't live in Iowa and never have,
[00:08:52.040 --> 00:08:57.200]   but just the fact that an app was used
[00:08:57.200 --> 00:09:01.800]   to count votes is indicative of how Wild West,
[00:09:01.800 --> 00:09:06.800]   the relationship of technology with US elections remains.
[00:09:06.800 --> 00:09:11.800]   The electronic voting systems are certainly
[00:09:11.800 --> 00:09:13.920]   not ironclad by any means.
[00:09:13.920 --> 00:09:18.080]   And in fact, it's sport at hacker conferences
[00:09:18.080 --> 00:09:21.040]   to set up voting as where.
[00:09:21.040 --> 00:09:25.120]   Show exactly how many ways you can penetrate
[00:09:25.120 --> 00:09:27.040]   the electronic voting systems.
[00:09:27.040 --> 00:09:30.360]   So I wasn't surprised at all about this,
[00:09:30.360 --> 00:09:33.400]   but I was probably in a minority.
[00:09:33.400 --> 00:09:37.000]   I think a lot of people think that we can actually
[00:09:37.000 --> 00:09:39.240]   use technology to manage our elections
[00:09:39.240 --> 00:09:41.720]   and we're far from it.
[00:09:41.720 --> 00:09:43.080]   You know, as you point out,
[00:09:43.080 --> 00:09:45.760]   hackers for a long time pointed out,
[00:09:45.760 --> 00:09:50.280]   as security experts that electronic voting is risky.
[00:09:50.280 --> 00:09:51.600]   Apps is even riskier,
[00:09:51.600 --> 00:09:54.040]   internet voting is risky is still.
[00:09:54.040 --> 00:09:55.640]   But one thing that everybody agrees on,
[00:09:55.640 --> 00:09:56.800]   Ed Felton talked about this,
[00:09:56.800 --> 00:10:00.080]   he's the Princeton professor who was one of the first,
[00:10:00.080 --> 00:10:02.000]   his team to crack voting machines,
[00:10:02.000 --> 00:10:04.400]   I think was the Die Bold Voting Machines.
[00:10:04.400 --> 00:10:08.640]   He said, well, at least in Iowa, there was a paper trail.
[00:10:08.640 --> 00:10:09.880]   And that's what took so long.
[00:10:09.880 --> 00:10:12.880]   They had written votes from every caucus goer
[00:10:12.880 --> 00:10:14.200]   and they were able to tally them.
[00:10:14.200 --> 00:10:16.680]   So they were able to get a decent tally.
[00:10:16.680 --> 00:10:19.240]   But I wish the New York Times had an article
[00:10:19.240 --> 00:10:22.040]   later in the week, a couple of days ago,
[00:10:22.040 --> 00:10:24.600]   where they said that even the results
[00:10:24.600 --> 00:10:28.080]   we're getting from Iowa now are inconsistent and crazy.
[00:10:28.080 --> 00:10:30.200]   They're clearly, you look at them, they're wrong.
[00:10:30.200 --> 00:10:31.920]   They don't pass a sanity check.
[00:10:31.920 --> 00:10:34.840]   So you can't even really blame the app at this point.
[00:10:34.840 --> 00:10:36.400]   You gotta blame the process.
[00:10:36.400 --> 00:10:39.480]   And it's kind of embarrassing for a party that says,
[00:10:39.480 --> 00:10:41.920]   we want, of course, the president and the Republicans
[00:10:41.920 --> 00:10:43.520]   immediately jumped on it saying,
[00:10:43.520 --> 00:10:46.400]   they can't run a caucus, how are they gonna run the country?
[00:10:46.400 --> 00:10:48.400]   - Yeah.
[00:10:48.400 --> 00:10:49.960]   - I think this, it's really interesting if you look at
[00:10:49.960 --> 00:10:52.400]   what Estonia has been doing for quite a while.
[00:10:52.400 --> 00:10:53.240]   - They're very digital.
[00:10:53.240 --> 00:10:54.120]   - I think, yeah.
[00:10:54.120 --> 00:10:59.120]   I think it was as far back as maybe 2005, 2006,
[00:10:59.120 --> 00:11:03.520]   something like that, that the citizens were able to vote
[00:11:03.520 --> 00:11:05.720]   online, they still do it.
[00:11:05.720 --> 00:11:07.400]   I don't know if the majority do,
[00:11:07.400 --> 00:11:12.120]   but I think it's pretty much law that everyone has
[00:11:12.120 --> 00:11:12.960]   to be able to.
[00:11:12.960 --> 00:11:15.200]   Everyone has these digital identity cards
[00:11:15.200 --> 00:11:16.640]   and I think there's like a little reader thing
[00:11:16.640 --> 00:11:18.760]   that you insert it at your computer,
[00:11:18.760 --> 00:11:19.920]   log in and cast your vote.
[00:11:19.920 --> 00:11:22.280]   And it's all done like that.
[00:11:24.000 --> 00:11:26.480]   - A lot of people roll the Estonia out as being an example
[00:11:26.480 --> 00:11:27.800]   of how things could be done,
[00:11:27.800 --> 00:11:29.360]   but it's been doing it for so many years now
[00:11:29.360 --> 00:11:31.280]   that I kind of think, yeah, maybe that is how we should
[00:11:31.280 --> 00:11:32.120]   be doing it.
[00:11:32.120 --> 00:11:34.480]   - Well, but Nate, I gotta point out,
[00:11:34.480 --> 00:11:37.800]   you may remember that the Estonian digital identity card
[00:11:37.800 --> 00:11:42.400]   had a crypto flaw and had to do reissued last year.
[00:11:42.400 --> 00:11:44.440]   - Right.
[00:11:44.440 --> 00:11:46.400]   - I'm doing that for 300 million,
[00:11:46.400 --> 00:11:50.640]   or however many voting Americans we have is really difficult.
[00:11:50.640 --> 00:11:53.360]   And out, well, I think this goes back to,
[00:11:53.360 --> 00:11:56.200]   I mean, like we talked about this previously, Leo,
[00:11:56.200 --> 00:11:58.880]   like when it comes back to this,
[00:11:58.880 --> 00:12:02.160]   you know, lack of an ethical reckoning
[00:12:02.160 --> 00:12:04.360]   in software development, where, you know,
[00:12:04.360 --> 00:12:05.720]   we have these people who say,
[00:12:05.720 --> 00:12:08.920]   I can solve this problem that, you know,
[00:12:08.920 --> 00:12:12.560]   is right now very analog and I can make an app
[00:12:12.560 --> 00:12:15.000]   like Facebook to bring people together or whatever it is
[00:12:15.000 --> 00:12:18.840]   without them really, truly considering the ethical
[00:12:18.840 --> 00:12:22.360]   ramifications of worst case use scenarios and bad actors
[00:12:22.360 --> 00:12:26.600]   and, you know, how it can be weaponized against its own
[00:12:26.600 --> 00:12:27.680]   actual user base.
[00:12:27.680 --> 00:12:30.440]   And so this is the same problem, right?
[00:12:30.440 --> 00:12:31.880]   So it's something that they said,
[00:12:31.880 --> 00:12:33.960]   oh, well, we can do this and we can, you know,
[00:12:33.960 --> 00:12:37.280]   make the caucus easier for you to report all your results
[00:12:37.280 --> 00:12:40.560]   without really thinking about worst case scenarios here.
[00:12:40.560 --> 00:12:44.080]   I mean, you know, and that, it also speaks to the fact
[00:12:44.080 --> 00:12:45.680]   that the phone lines were so jammed.
[00:12:45.680 --> 00:12:49.400]   I mean, expect, be prepared and expect for that app
[00:12:49.400 --> 00:12:50.640]   to not work.
[00:12:50.640 --> 00:12:52.280]   And, well, that's exactly what happened.
[00:12:52.280 --> 00:12:54.840]   They had a manual that they'd used for years,
[00:12:54.840 --> 00:12:57.280]   manual reporting method with the phone.
[00:12:57.280 --> 00:13:00.680]   And they were so bullish about this app that they said,
[00:13:00.680 --> 00:13:03.600]   we're going to make those lines be the tech support lines
[00:13:03.600 --> 00:13:04.440]   for the app.
[00:13:04.440 --> 00:13:05.720]   Yeah, not good.
[00:13:05.720 --> 00:13:08.240]   So when the app failed, the lines got jammed
[00:13:08.240 --> 00:13:10.360]   and then people couldn't manually report,
[00:13:10.360 --> 00:13:12.480]   they were on hold for hours.
[00:13:12.480 --> 00:13:13.600]   Yeah, it was actually--
[00:13:13.600 --> 00:13:16.760]   You kept that guy on Wolf Blitzer, who was on hold
[00:13:16.760 --> 00:13:18.760]   and complaining about it to Wolf Blitzer.
[00:13:18.760 --> 00:13:21.840]   And then he kept saying, I have to go because they picked up
[00:13:21.840 --> 00:13:24.240]   and then they hung up on him and he had to start the caucus
[00:13:24.240 --> 00:13:24.680]   all over.
[00:13:24.680 --> 00:13:25.360]   It was so bad.
[00:13:25.360 --> 00:13:26.560]   It was so bad.
[00:13:26.560 --> 00:13:29.720]   What's great is it's something anybody who's ever used tech
[00:13:29.720 --> 00:13:32.920]   and completely identified with the long hold on a tech support
[00:13:32.920 --> 00:13:35.120]   line and then getting on that phone.
[00:13:35.120 --> 00:13:36.760]   Your call's important to us.
[00:13:36.760 --> 00:13:39.520]   Your number 5,072--
[00:13:39.520 --> 00:13:41.360]   Leo, that literally happened to me last week
[00:13:41.360 --> 00:13:42.480]   with the Motorola Razr.
[00:13:42.480 --> 00:13:45.160]   I called asking, where is my phone?
[00:13:45.160 --> 00:13:50.000]   Because I had ordered it to do an unboxing on launch day.
[00:13:50.000 --> 00:13:52.760]   And they were like, we let us call you back.
[00:13:52.760 --> 00:13:55.120]   And I'm like, they're like, we'll call you right back
[00:13:55.120 --> 00:13:56.200]   within five minutes.
[00:13:56.200 --> 00:13:57.320]   I'm like, great.
[00:13:57.320 --> 00:14:00.280]   And I had had like five cups of coffee that morning.
[00:14:00.280 --> 00:14:00.760]   Oh, boy.
[00:14:00.760 --> 00:14:01.760]   And I really had to pee.
[00:14:01.760 --> 00:14:03.080]   Oh, boy.
[00:14:03.080 --> 00:14:05.720]   35 minutes later, I was like, I'm going to die.
[00:14:05.720 --> 00:14:07.600]   And so I went to the bathroom and came back.
[00:14:07.600 --> 00:14:08.080]   And they called.
[00:14:08.080 --> 00:14:09.800]   And sure enough, they called me.
[00:14:09.800 --> 00:14:11.160]   I was like, well, I give up.
[00:14:11.160 --> 00:14:12.920]   That means they have cameras in the house, Ashley.
[00:14:12.920 --> 00:14:15.080]   I just want to--
[00:14:15.080 --> 00:14:15.960]   Is she in the bathroom yet?
[00:14:15.960 --> 00:14:16.920]   OK, call now.
[00:14:16.920 --> 00:14:17.440]   Call now.
[00:14:17.440 --> 00:14:19.960]   It's even worse because it's tech support combined
[00:14:19.960 --> 00:14:21.760]   with a state agency, right?
[00:14:21.760 --> 00:14:22.280]   So--
[00:14:22.280 --> 00:14:22.760]   So bad.
[00:14:22.760 --> 00:14:26.960]   --and we talk about a collision of miserable situations.
[00:14:26.960 --> 00:14:29.000]   I don't know if you've tried to make an appointment
[00:14:29.000 --> 00:14:31.200]   at the California DMV recently.
[00:14:31.200 --> 00:14:31.720]   Oh, god.
[00:14:31.720 --> 00:14:32.960]   But that's an hour on the phone.
[00:14:32.960 --> 00:14:33.440]   At least.
[00:14:33.440 --> 00:14:33.960]   Oh, my god.
[00:14:33.960 --> 00:14:34.760]   That's the worst.
[00:14:34.760 --> 00:14:36.280]   You don't do it online?
[00:14:36.280 --> 00:14:37.160]   You try to call them?
[00:14:37.160 --> 00:14:38.360]   Well, you can do it online.
[00:14:38.360 --> 00:14:38.880]   Oh.
[00:14:38.880 --> 00:14:40.600]   But it's-- yeah, it's very hit or miss.
[00:14:40.600 --> 00:14:41.600]   Very hit or miss.
[00:14:41.600 --> 00:14:44.240]   And well, and like Leo, you were just saying,
[00:14:44.240 --> 00:14:47.800]   you know, the Republicans in the GOP, you know, like Trump
[00:14:47.800 --> 00:14:49.520]   and everybody's like, oh, how are they
[00:14:49.520 --> 00:14:50.400]   going to run a country?
[00:14:50.400 --> 00:14:51.880]   They can't even run this caucus.
[00:14:51.880 --> 00:14:52.400]   Really?
[00:14:52.400 --> 00:14:54.760]   I mean, the really bad thing was all
[00:14:54.760 --> 00:14:57.440]   of the disinformation that was getting put out there,
[00:14:57.440 --> 00:14:59.280]   while they were trying to figure out what was going on,
[00:14:59.280 --> 00:15:01.680]   people were like, oh, it's rigged.
[00:15:01.680 --> 00:15:05.440]   It's, you know, people's nerves are so on edge.
[00:15:05.440 --> 00:15:08.640]   That would even the slightest thing goes wrong in this election
[00:15:08.640 --> 00:15:09.280]   year.
[00:15:09.280 --> 00:15:11.680]   Like, people immediately go to those conspiracy theories.
[00:15:11.680 --> 00:15:13.800]   They immediately start spreading.
[00:15:13.800 --> 00:15:15.400]   You know, well, what if this happened?
[00:15:15.400 --> 00:15:16.920]   It's just really dangerous.
[00:15:16.920 --> 00:15:18.080]   It's so scary.
[00:15:18.080 --> 00:15:18.840]   Well, we won't know.
[00:15:18.840 --> 00:15:21.000]   And I saw-- I was still seeing on Reddit.
[00:15:21.000 --> 00:15:22.520]   Oh, this was a hack.
[00:15:22.520 --> 00:15:25.240]   And, you know, all sorts of conspiracy theories.
[00:15:25.240 --> 00:15:27.120]   And I guess we-- you know, who knows?
[00:15:27.120 --> 00:15:30.880]   And you've learned-- if we've learned nothing from the last few
[00:15:30.880 --> 00:15:34.160]   years, it's to expect that to--
[00:15:34.160 --> 00:15:35.680]   I mean, the lesson of 2019--
[00:15:35.680 --> 00:15:36.400]   But they don't expect that.
[00:15:36.400 --> 00:15:36.760]   That's the problem.
[00:15:36.760 --> 00:15:37.960]   No.
[00:15:37.960 --> 00:15:42.880]   The lesson of 2019 was techno-utopianism is dead.
[00:15:42.880 --> 00:15:43.880]   Yeah.
[00:15:43.880 --> 00:15:47.560]   And yet, people are still looking at technology for solutions.
[00:15:47.560 --> 00:15:49.680]   That's a great Atlantic article.
[00:15:49.680 --> 00:15:54.000]   The disinformation is going to be the crux of this year's.
[00:15:54.000 --> 00:15:57.280]   And every future election that we have.
[00:15:57.280 --> 00:16:00.200]   The problem with Iowa, aside from just the intricacies
[00:16:00.200 --> 00:16:04.120]   of its caucus system, which I'm sure that it has its benefits
[00:16:04.120 --> 00:16:05.520]   and there are reasons why I--
[00:16:05.520 --> 00:16:07.480]   It's old form democracy.
[00:16:07.480 --> 00:16:09.160]   You and your neighbors get together, right?
[00:16:09.160 --> 00:16:10.960]   It's like a talk war or something.
[00:16:10.960 --> 00:16:12.840]   But the thing is they go first, and there's
[00:16:12.840 --> 00:16:16.400]   all this sort of national significance associated
[00:16:16.400 --> 00:16:18.960]   with it that the person who wins in Iowa often
[00:16:18.960 --> 00:16:21.080]   goes on to become the nominee.
[00:16:21.080 --> 00:16:24.560]   And since we don't know who won or if we do,
[00:16:24.560 --> 00:16:28.680]   it's subject to question.
[00:16:28.680 --> 00:16:30.440]   It's difficult.
[00:16:30.440 --> 00:16:34.440]   Here in California, we moved up our primary.
[00:16:34.440 --> 00:16:36.800]   We used to go late, so we would have no impact.
[00:16:36.800 --> 00:16:38.480]   Now we're March 3rd.
[00:16:38.480 --> 00:16:41.440]   And my ballot, my vote by mail ballot, actually arrived.
[00:16:41.440 --> 00:16:42.080]   I think it was yesterday.
[00:16:42.080 --> 00:16:43.080]   I already voted.
[00:16:43.080 --> 00:16:43.560]   It was, too.
[00:16:43.560 --> 00:16:43.560]   Yeah.
[00:16:43.560 --> 00:16:45.520]   I already filled it out.
[00:16:45.520 --> 00:16:48.200]   I saw-- actually, I saw that headline.
[00:16:48.200 --> 00:16:51.440]   And I declined to read it because I thought I'm already
[00:16:51.440 --> 00:16:52.560]   depressed.
[00:16:52.560 --> 00:16:53.560]   It's really scary.
[00:16:53.560 --> 00:16:55.200]   And I shared it with--
[00:16:55.200 --> 00:16:57.520]   I shared it on Facebook specifically
[00:16:57.520 --> 00:17:01.160]   to get it to people who are affected by things like that
[00:17:01.160 --> 00:17:04.240]   every single day on social media.
[00:17:04.240 --> 00:17:07.360]   The premise is get ready because this ain't nothing.
[00:17:07.360 --> 00:17:13.000]   And we're going to be in a nine month cycle of mystery,
[00:17:13.000 --> 00:17:16.320]   disinformation, propaganda, lies, truths.
[00:17:16.320 --> 00:17:19.280]   And of course, the end result of all of that
[00:17:19.280 --> 00:17:21.720]   is that everybody just throws up their hands and says,
[00:17:21.720 --> 00:17:22.720]   I give up.
[00:17:22.720 --> 00:17:23.240]   Right.
[00:17:23.240 --> 00:17:24.320]   It's obfuscation.
[00:17:24.320 --> 00:17:24.680]   And that's--
[00:17:24.680 --> 00:17:25.400]   That's the goal.
[00:17:25.400 --> 00:17:26.880]   That's the goal of disinformation.
[00:17:26.880 --> 00:17:29.920]   It's not to convince you that something isn't true.
[00:17:29.920 --> 00:17:33.280]   It's to make you question the thing that is true.
[00:17:33.280 --> 00:17:34.760]   And that's the only goal.
[00:17:34.760 --> 00:17:37.960]   And it's really scary because there
[00:17:37.960 --> 00:17:43.080]   are lots of people out there who are not--
[00:17:43.080 --> 00:17:47.720]   I've seen even my very media savvy friends fall victim
[00:17:47.720 --> 00:17:49.160]   to this, where they'll share something.
[00:17:49.160 --> 00:17:50.880]   And I'm like, this has been debunked.
[00:17:50.880 --> 00:17:52.720]   Please stop posting this.
[00:17:52.720 --> 00:17:55.400]   Please don't use this place as a source.
[00:17:55.400 --> 00:18:00.240]   And it captures almost everyone at some point.
[00:18:00.240 --> 00:18:02.880]   And it's just-- it's really--
[00:18:02.880 --> 00:18:08.960]   Man, it's going to be a very wild election year this year.
[00:18:08.960 --> 00:18:09.480]   Is there--
[00:18:09.480 --> 00:18:13.040]   I feel like, though, that people are beginning to distrust
[00:18:13.040 --> 00:18:15.080]   getting their information from social media?
[00:18:15.080 --> 00:18:18.640]   I mean, I think younger generations are.
[00:18:18.640 --> 00:18:19.920]   But I mean--
[00:18:19.920 --> 00:18:20.720]   Are they?
[00:18:20.720 --> 00:18:21.600]   My parents are--
[00:18:21.600 --> 00:18:22.080]   Are they?
[00:18:22.080 --> 00:18:23.520]   They don't get it from Facebook anymore.
[00:18:23.520 --> 00:18:25.240]   They get to get news on Facebook.
[00:18:25.240 --> 00:18:27.200]   My parents, my friends' parents,
[00:18:27.200 --> 00:18:29.280]   I mean, they all get their news on Facebook.
[00:18:29.280 --> 00:18:30.080]   So the young--
[00:18:30.080 --> 00:18:30.280]   The young--
[00:18:30.280 --> 00:18:30.720]   The young--
[00:18:30.720 --> 00:18:34.160]   They get it from Instagram or from Snapchat or TikTok.
[00:18:34.160 --> 00:18:37.680]   And that's all getting permeated with this information
[00:18:37.680 --> 00:18:38.520]   as well.
[00:18:38.520 --> 00:18:40.000]   It's just-- it's all-- it's really bad.
[00:18:40.000 --> 00:18:43.280]   And I think-- I think at some point,
[00:18:43.280 --> 00:18:44.800]   there's going to have to be--
[00:18:44.800 --> 00:18:46.120]   I mean, we see it in pockets.
[00:18:46.120 --> 00:18:47.480]   And I think it's a really good idea.
[00:18:47.480 --> 00:18:51.320]   I mean, when I went to college for broadcast journalism,
[00:18:51.320 --> 00:18:55.400]   I had to take classes that were literally critical thinking
[00:18:55.400 --> 00:18:56.240]   in mass media.
[00:18:56.240 --> 00:18:58.440]   And it was how to spot propaganda,
[00:18:58.440 --> 00:19:01.280]   how to fact check things, how to do.
[00:19:01.280 --> 00:19:03.720]   And I think, regardless of whether you're a journalism
[00:19:03.720 --> 00:19:06.880]   student or not, because the news is all around us
[00:19:06.880 --> 00:19:10.840]   in every single thing we do because it's always connected,
[00:19:10.840 --> 00:19:13.360]   I think our education system has to change
[00:19:13.360 --> 00:19:16.480]   to teach people otherwise.
[00:19:16.480 --> 00:19:19.040]   Well, that's not going to fix anything in 2020.
[00:19:19.040 --> 00:19:19.520]   What--
[00:19:19.520 --> 00:19:20.840]   No, of course not.
[00:19:20.840 --> 00:19:23.200]   But if that's the fundamental crux of the problem--
[00:19:23.200 --> 00:19:24.840]   Maybe in 2030.
[00:19:24.840 --> 00:19:26.400]   But for the next year--
[00:19:26.400 --> 00:19:27.400]   Probably far than that.
[00:19:27.400 --> 00:19:27.960]   Yeah.
[00:19:27.960 --> 00:19:29.840]   So what do we do?
[00:19:29.840 --> 00:19:31.720]   Denise, what's your prescription?
[00:19:31.720 --> 00:19:34.320]   Is there anything we could do in the next nine months
[00:19:34.320 --> 00:19:38.760]   to avoid this kind of information overload?
[00:19:38.760 --> 00:19:41.160]   I think having conversations like the one we're having right
[00:19:41.160 --> 00:19:44.560]   now and having them with our friends and family
[00:19:44.560 --> 00:19:48.400]   and people that we come into contact with
[00:19:48.400 --> 00:19:50.280]   is really important.
[00:19:50.280 --> 00:19:52.760]   My personal anecdotal sense is I do
[00:19:52.760 --> 00:19:56.920]   feel like my group of folks around me
[00:19:56.920 --> 00:19:59.360]   are very distrustful of information
[00:19:59.360 --> 00:20:00.760]   that they get on social media.
[00:20:00.760 --> 00:20:02.280]   And I'm grateful for it.
[00:20:02.280 --> 00:20:03.400]   And I do think--
[00:20:03.400 --> 00:20:05.000]   Does that leave a void?
[00:20:05.000 --> 00:20:07.960]   Doesn't that leave a void?
[00:20:07.960 --> 00:20:10.800]   So where are they getting information from?
[00:20:10.800 --> 00:20:14.000]   Hopefully from actual journalists.
[00:20:14.000 --> 00:20:16.280]   And where would we find those?
[00:20:16.280 --> 00:20:18.840]   Well, I guess that's subject to debate as well.
[00:20:18.840 --> 00:20:21.600]   I mean, the pollution continues up the food chain.
[00:20:21.600 --> 00:20:22.680]   Yeah.
[00:20:22.680 --> 00:20:26.160]   I mean, the problem with 24-hour news channels
[00:20:26.160 --> 00:20:28.960]   is they have to obsess.
[00:20:28.960 --> 00:20:29.920]   If they entertain.
[00:20:29.920 --> 00:20:31.320]   If they entertain, they have to keep--
[00:20:31.320 --> 00:20:34.200]   And so I was talking to my wife the other day.
[00:20:34.200 --> 00:20:36.920]   She said, I can't watch it because it's
[00:20:36.920 --> 00:20:38.880]   the same story over and over again.
[00:20:38.880 --> 00:20:41.200]   Isn't anything else happening?
[00:20:41.200 --> 00:20:43.600]   And you don't get the sense that anything else is happening.
[00:20:43.600 --> 00:20:46.200]   By the way, I hear Brexit happened.
[00:20:46.200 --> 00:20:47.680]   Nate, did that happen?
[00:20:47.680 --> 00:20:48.240]   It did.
[00:20:48.240 --> 00:20:49.800]   I heard that.
[00:20:49.800 --> 00:20:51.760]   I'm afraid to say so.
[00:20:51.760 --> 00:20:53.520]   Eventually, it did take place.
[00:20:53.520 --> 00:20:57.000]   Is there a similar issue in Great Britain with--
[00:20:57.000 --> 00:20:59.920]   I think there is with this information?
[00:20:59.920 --> 00:21:03.320]   I don't-- for some reason, I feel like somebody
[00:21:03.320 --> 00:21:04.040]   living on--
[00:21:04.040 --> 00:21:07.400]   Stoughton on the Thames is not reading Facebook
[00:21:07.400 --> 00:21:11.200]   as much as the people in Iowa, or is that not the case?
[00:21:11.200 --> 00:21:13.360]   If there is such a tab.
[00:21:13.360 --> 00:21:14.120]   Yeah.
[00:21:14.120 --> 00:21:16.480]   I mean, people are reading the news here.
[00:21:16.480 --> 00:21:19.400]   I think part of the problem that we had with Brexit and online
[00:21:19.400 --> 00:21:22.760]   is that people discounted the older vote.
[00:21:22.760 --> 00:21:27.440]   I think people expected more young people to come out
[00:21:27.440 --> 00:21:29.000]   and to register and to vote.
[00:21:29.000 --> 00:21:32.200]   And they probably did, just not enough of them.
[00:21:32.200 --> 00:21:35.720]   And as you were saying, where are younger people getting
[00:21:35.720 --> 00:21:36.880]   their news from?
[00:21:36.880 --> 00:21:41.280]   From what I hear, not being massively young anymore,
[00:21:41.280 --> 00:21:43.760]   unfortunately, is that a huge number of people
[00:21:43.760 --> 00:21:47.760]   are getting news and getting their awarenesses broadened
[00:21:47.760 --> 00:21:52.080]   from these influences, whether it's on YouTube
[00:21:52.080 --> 00:21:53.560]   on Snapchat or TikTok.
[00:21:53.560 --> 00:21:57.680]   They have an incredible power over not only what young people
[00:21:57.680 --> 00:22:00.920]   hear, but I think also what young people discuss
[00:22:00.920 --> 00:22:02.720]   with each other.
[00:22:02.720 --> 00:22:04.960]   I think one of the positives of Brexit,
[00:22:04.960 --> 00:22:08.280]   and it's a silver lining, rather than anything else
[00:22:08.280 --> 00:22:11.400]   exposed, is that I think a lot of younger people have now
[00:22:11.400 --> 00:22:16.080]   been empowered to make a change in the future.
[00:22:16.080 --> 00:22:18.360]   And hopefully that will be the case.
[00:22:18.360 --> 00:22:19.960]   But until then, I think we've just
[00:22:19.960 --> 00:22:23.480]   got to basically keep fighting these--
[00:22:23.480 --> 00:22:27.000]   the disinformation fires, as we've been talking about so far.
[00:22:27.000 --> 00:22:29.600]   Nate, do you have you ever heard--
[00:22:29.600 --> 00:22:34.600]   I forget the survey or the study that was done.
[00:22:34.600 --> 00:22:36.920]   It was a while ago.
[00:22:36.920 --> 00:22:42.560]   It was about Gen Z and even a little bit younger than that,
[00:22:42.560 --> 00:22:47.800]   asking how they felt or how they found a news.
[00:22:47.800 --> 00:22:50.800]   And the majority of respondents said,
[00:22:50.800 --> 00:22:53.640]   if the news is big enough, it will find me.
[00:22:53.640 --> 00:22:55.640]   They don't actually go looking for news.
[00:22:55.640 --> 00:22:58.200]   They wait for it to hit them.
[00:22:58.200 --> 00:23:00.960]   And I thought that was really interesting.
[00:23:00.960 --> 00:23:03.640]   I think that is absolutely, completely believable.
[00:23:03.640 --> 00:23:07.120]   I think the caveat to that is that through what filter
[00:23:07.120 --> 00:23:08.360]   is the news reaching them.
[00:23:08.360 --> 00:23:09.840]   Because a lot of first impressions,
[00:23:09.840 --> 00:23:13.000]   they think make a huge difference.
[00:23:13.000 --> 00:23:15.720]   And I think that's where a lot of the confusion sometimes
[00:23:15.720 --> 00:23:16.960]   comes from.
[00:23:16.960 --> 00:23:19.320]   I think it's why social media companies have had such an
[00:23:19.320 --> 00:23:23.080]   important role to play in fighting even just misinformation.
[00:23:23.080 --> 00:23:25.960]   Because a lot of people will just see and share based on
[00:23:25.960 --> 00:23:27.840]   headlines alone, as we know.
[00:23:27.840 --> 00:23:30.400]   And I think if that initial headline is not necessarily
[00:23:30.400 --> 00:23:34.120]   conveying the big news in the fairest, almost balanced way,
[00:23:34.120 --> 00:23:36.640]   or have you, then you're getting--
[00:23:36.640 --> 00:23:38.280]   that's spreading.
[00:23:38.280 --> 00:23:39.480]   But I can totally believe it.
[00:23:39.480 --> 00:23:42.280]   Yeah, although I haven't seen that particular study.
[00:23:42.280 --> 00:23:43.640]   I also wonder if--
[00:23:43.640 --> 00:23:50.400]   so you can say, oh, we'll pay attention to the news and do
[00:23:50.400 --> 00:23:53.920]   good sources and blah, blah, blah, or only look for the news
[00:23:53.920 --> 00:23:54.960]   when it comes to you.
[00:23:54.960 --> 00:23:59.840]   But that's not the only source of misinformation and
[00:23:59.840 --> 00:24:03.960]   propaganda that a lot of it is subtle if you watch TikTok
[00:24:03.960 --> 00:24:04.960]   or influencers.
[00:24:04.960 --> 00:24:06.640]   They're not giving you news stories.
[00:24:06.640 --> 00:24:11.760]   But they are influencing your way of thinking in a subtle,
[00:24:11.760 --> 00:24:15.560]   less detectable way in a way that is more difficult to
[00:24:15.560 --> 00:24:17.680]   defend against.
[00:24:17.680 --> 00:24:19.040]   Does that make sense?
[00:24:19.040 --> 00:24:23.480]   And so I feel like we're actually generating a culture
[00:24:23.480 --> 00:24:24.360]   through--
[00:24:24.360 --> 00:24:27.440]   unconsciously, through these tools like TikTok and
[00:24:27.440 --> 00:24:30.600]   Instagram and Twitter and Facebook.
[00:24:30.600 --> 00:24:33.560]   And we don't really know what we're getting.
[00:24:33.560 --> 00:24:34.360]   Look at this.
[00:24:34.360 --> 00:24:38.560]   Mike Bloomberg, paying influencers to make him seem cool.
[00:24:38.560 --> 00:24:40.840]   This really terrifies me.
[00:24:40.840 --> 00:24:43.280]   Because there's a gap.
[00:24:43.280 --> 00:24:46.720]   There's exactly what you're talking about.
[00:24:46.720 --> 00:24:50.920]   A lot of particularly younger folks get their information
[00:24:50.920 --> 00:24:53.080]   through their social media feeds.
[00:24:53.080 --> 00:24:54.240]   They're skeptical about it.
[00:24:54.240 --> 00:24:59.440]   And they have a rubric for judging whether something is an
[00:24:59.440 --> 00:25:00.280]   ad or not.
[00:25:00.280 --> 00:25:04.000]   And that's whether they're seeing hashtag ad on that post.
[00:25:04.000 --> 00:25:10.120]   And unfortunately, there is this gray area where campaigns
[00:25:10.120 --> 00:25:14.480]   working with influencers recognize how important it is.
[00:25:14.480 --> 00:25:17.800]   And if you read that Daily Beast piece about Bloomberg,
[00:25:17.800 --> 00:25:19.000]   that he's targeting--
[00:25:19.000 --> 00:25:21.280]   they point out the irony that he's got the biggest war
[00:25:21.280 --> 00:25:26.880]   chest of really any or many of the candidates.
[00:25:26.880 --> 00:25:31.360]   And he is targeting these micro, micro influencers.
[00:25:31.360 --> 00:25:34.920]   People with 1,000 or 5,000 followers.
[00:25:34.920 --> 00:25:40.320]   And he's using an app called Tribe, which is how those people
[00:25:40.320 --> 00:25:43.360]   find sponsors for their sponsor.
[00:25:43.360 --> 00:25:46.000]   It's an agency for influencers.
[00:25:46.000 --> 00:25:47.080]   Right, exactly.
[00:25:47.080 --> 00:25:49.160]   It's not just the Kim Kardashian's of the world.
[00:25:49.160 --> 00:25:50.200]   They have their own--
[00:25:50.200 --> 00:25:51.520]   they have their people.
[00:25:51.520 --> 00:25:53.480]   This is for little influencers.
[00:25:53.480 --> 00:25:56.960]   It's something that's going to look more organic.
[00:25:56.960 --> 00:26:00.560]   Get behind our-- they're going to pay $150 a post.
[00:26:00.560 --> 00:26:08.560]   And how and whether that payment is disclosed seems very fuzzy.
[00:26:08.560 --> 00:26:11.840]   And there's actually a journalist over at the morning
[00:26:11.840 --> 00:26:16.080]   consult, Sam-- oh, I'm forgetting Sam's last name--
[00:26:16.080 --> 00:26:21.560]   who wrote about this back in the latter part of last year,
[00:26:21.560 --> 00:26:22.880]   Sam Saban.
[00:26:22.880 --> 00:26:26.320]   And she reached out to the Federal Election Commission
[00:26:26.320 --> 00:26:28.160]   and said, what's going on with this?
[00:26:28.160 --> 00:26:30.920]   Of course, we're going to see candidates reaching out
[00:26:30.920 --> 00:26:32.920]   to influencers and wanting to get them on board
[00:26:32.920 --> 00:26:35.160]   and wanting to pay them for posts.
[00:26:35.160 --> 00:26:36.680]   What do you think about that?
[00:26:36.680 --> 00:26:39.400]   And the FEC did not get back to the morning consult
[00:26:39.400 --> 00:26:40.280]   or Sam Saban.
[00:26:40.280 --> 00:26:41.000]   They have no way to--
[00:26:41.000 --> 00:26:42.640]   They're like, what is Instagram?
[00:26:42.640 --> 00:26:44.520]   They have no way--
[00:26:44.520 --> 00:26:45.960]   no way to regulate it.
[00:26:45.960 --> 00:26:47.080]   It's not-- no.
[00:26:47.080 --> 00:26:50.000]   It's not an ad, exactly.
[00:26:50.000 --> 00:26:51.600]   It's not-- I mean--
[00:26:51.600 --> 00:26:53.600]   I think the FTC might beg to differ.
[00:26:53.600 --> 00:26:58.120]   Whenever you're being paid to say a particular thing,
[00:26:58.120 --> 00:26:59.920]   even if you're coming up with a language for it,
[00:26:59.920 --> 00:27:01.680]   I think the FTC would judge it an ad.
[00:27:01.680 --> 00:27:03.920]   And I think the UK's equivalent--
[00:27:03.920 --> 00:27:07.560]   But does the FTC have the enforcement capability
[00:27:07.560 --> 00:27:09.360]   to find all of this stuff?
[00:27:09.360 --> 00:27:09.920]   And--
[00:27:09.920 --> 00:27:12.000]   Well, they have rules on the books.
[00:27:12.000 --> 00:27:14.760]   Whether they're going to go out and enforce is another story.
[00:27:14.760 --> 00:27:16.960]   But the rules are pretty toothy.
[00:27:16.960 --> 00:27:21.600]   $15,000 per violation is a good disincentive for people
[00:27:21.600 --> 00:27:26.400]   who are trying to earn $150 opposed.
[00:27:26.400 --> 00:27:27.400]   Right.
[00:27:27.400 --> 00:27:30.720]   They have enforced it here in Britain.
[00:27:30.720 --> 00:27:34.880]   There have been people who have full foul of not disclosing
[00:27:34.880 --> 00:27:37.480]   either that something was sponsored or paid
[00:27:37.480 --> 00:27:41.480]   and more worryingly are promoting things
[00:27:41.480 --> 00:27:43.600]   with alleged medical benefits.
[00:27:43.600 --> 00:27:46.400]   Often it's things like weight loss, things along those lines.
[00:27:46.400 --> 00:27:48.280]   And they're not disclosing that actually,
[00:27:48.280 --> 00:27:50.040]   whether these trials have been done or not,
[00:27:50.040 --> 00:27:52.800]   whether they've been done on a big enough sample,
[00:27:52.800 --> 00:27:55.160]   these things are not making their way through the messaging
[00:27:55.160 --> 00:27:59.280]   that an influencer is perhaps naively not even cognizant
[00:27:59.280 --> 00:28:01.680]   of needing to share with their audience.
[00:28:01.680 --> 00:28:04.000]   But there's definitely been fines issued here.
[00:28:04.000 --> 00:28:07.920]   And I'm sure that's not a unique case to Britain.
[00:28:07.920 --> 00:28:10.360]   I can't remember the numbers, but the FTC enforcement
[00:28:10.360 --> 00:28:13.440]   division is not--
[00:28:13.440 --> 00:28:15.920]   it's not replete with people.
[00:28:15.920 --> 00:28:17.280]   Yeah, it's not robust.
[00:28:17.280 --> 00:28:19.520]   Yeah, it's not a robust agency.
[00:28:19.520 --> 00:28:21.960]   No, but it's not afraid to write letters and point out
[00:28:21.960 --> 00:28:24.320]   when it has big public.
[00:28:24.320 --> 00:28:25.680]   I would suspect that their focus more--
[00:28:25.680 --> 00:28:26.920]   Examples of stepping gone wrong.
[00:28:26.920 --> 00:28:28.280]   The fire festival comes to mind.
[00:28:28.280 --> 00:28:31.560]   Yeah, but they're going to be focused more on drug campaigns
[00:28:31.560 --> 00:28:35.640]   and stuff than Mike Bloomberg hiring for $150.
[00:28:35.640 --> 00:28:38.760]   Some Instagrammer to say, hey, Mike's pretty cool.
[00:28:38.760 --> 00:28:39.560]   What a bargain.
[00:28:39.560 --> 00:28:44.360]   I mean, if Kim Kardashian charges $2 million
[00:28:44.360 --> 00:28:48.360]   for an Instagram ad, I mean, that's over--
[00:28:48.360 --> 00:28:52.880]   that's like $13,000 micro influencers you could buy off
[00:28:52.880 --> 00:28:54.800]   for the same amount of money, right?
[00:28:54.800 --> 00:28:58.320]   This is from the Daily Beast for a fixed $150 fee
[00:28:58.320 --> 00:28:59.320]   through Tribe.
[00:28:59.320 --> 00:29:02.200]   The Bloomberg campaign is pitching micro influencers,
[00:29:02.200 --> 00:29:06.200]   someone who has from one to 100,000 followers,
[00:29:06.200 --> 00:29:08.760]   to create original content, quote,
[00:29:08.760 --> 00:29:11.640]   that tells us why Mike Bloomberg is the electable candidate who
[00:29:11.640 --> 00:29:15.840]   can rise above the fray, work across the aisle so all Americans
[00:29:15.840 --> 00:29:19.000]   feel heard and respected.
[00:29:19.000 --> 00:29:21.360]   That's a subtle message.
[00:29:21.360 --> 00:29:26.000]   I think if we look back at the last few campaigns,
[00:29:26.000 --> 00:29:30.080]   we can see this trend towards better and better, more and more
[00:29:30.080 --> 00:29:34.320]   effectively using the internet for getting elected.
[00:29:34.320 --> 00:29:38.280]   Obama started with Facebook in 2012.
[00:29:38.280 --> 00:29:42.080]   Trump got very good at using Brad Parscale and his team
[00:29:42.080 --> 00:29:44.400]   figured out how to use Facebook.
[00:29:44.400 --> 00:29:46.080]   But I wouldn't be in 2016.
[00:29:46.080 --> 00:29:49.120]   I wouldn't be at all surprised if in 2020
[00:29:49.120 --> 00:29:51.760]   that the winner is the one who can figure out
[00:29:51.760 --> 00:29:56.480]   how to use micro influencers.
[00:29:56.480 --> 00:29:57.920]   And the thing is-- and this was my point--
[00:29:57.920 --> 00:30:01.840]   is you may have a great defense against that headline
[00:30:01.840 --> 00:30:04.680]   that Grandpa shared on Facebook.
[00:30:04.680 --> 00:30:07.240]   But it's so subtle, you might not have
[00:30:07.240 --> 00:30:10.840]   that kind of defense against some Instagram person publishing.
[00:30:10.840 --> 00:30:12.000]   Well, it's an opinion, right?
[00:30:12.000 --> 00:30:13.120]   It's an opinion.
[00:30:13.120 --> 00:30:15.600]   Oh, this is why I think he's a great candidate.
[00:30:15.600 --> 00:30:17.120]   And don't we do this as humans?
[00:30:17.120 --> 00:30:19.040]   And don't we do this as humans?
[00:30:19.040 --> 00:30:21.360]   Just think about your movie pics.
[00:30:21.360 --> 00:30:23.560]   Aren't you heavily influenced what movie you go to see
[00:30:23.560 --> 00:30:25.360]   by what your friends said about it?
[00:30:25.360 --> 00:30:26.360]   Right.
[00:30:26.360 --> 00:30:26.920]   Right.
[00:30:26.920 --> 00:30:29.400]   I think it's no different to elect a president.
[00:30:29.400 --> 00:30:31.800]   I mean, it's a different magnitude of importance,
[00:30:31.800 --> 00:30:33.120]   but that doesn't mean we're going to treat it
[00:30:33.120 --> 00:30:33.840]   any differently.
[00:30:33.840 --> 00:30:34.360]   Right.
[00:30:34.360 --> 00:30:38.880]   And you can only say, oh, I don't care for this candidate.
[00:30:38.880 --> 00:30:39.560]   Here's why.
[00:30:39.560 --> 00:30:43.960]   But the thing is, if someone likes them,
[00:30:43.960 --> 00:30:46.480]   if someone's on board for somebody,
[00:30:46.480 --> 00:30:48.680]   a lot of times it's very hard to dissuade them,
[00:30:48.680 --> 00:30:50.560]   regardless of what argument you can make,
[00:30:50.560 --> 00:30:53.680]   because it is very subjective.
[00:30:53.680 --> 00:30:59.080]   It's has to do with people's personal opinions about stuff.
[00:30:59.080 --> 00:31:03.120]   I think it can't be a very far stretch for the FEC
[00:31:03.120 --> 00:31:05.800]   to say, look, if the FTC is going to expect you,
[00:31:05.800 --> 00:31:07.880]   if you're handed a pair of false eyelashes
[00:31:07.880 --> 00:31:10.400]   and you go on and you post about how great they are,
[00:31:10.400 --> 00:31:12.960]   and you're expected to put hashtag out on that,
[00:31:12.960 --> 00:31:15.720]   if you're being paid $150 by Michael Bloomberg,
[00:31:15.720 --> 00:31:18.400]   you're expected to put hashtag out on that too.
[00:31:18.400 --> 00:31:20.960]   But we really need someone to come out and give that guidance,
[00:31:20.960 --> 00:31:27.360]   because I think the people who ingest this material
[00:31:27.360 --> 00:31:30.400]   are used to those signposts, helping them judge
[00:31:30.400 --> 00:31:31.720]   what it is they're looking at.
[00:31:31.720 --> 00:31:33.440]   And absent those signposts, they're
[00:31:33.440 --> 00:31:36.400]   going to think, oh, this is organic and real.
[00:31:36.400 --> 00:31:37.400]   Yeah.
[00:31:37.400 --> 00:31:42.040]   And so really, this is the story.
[00:31:42.040 --> 00:31:43.960]   One of the things we've been talking about for the last two
[00:31:43.960 --> 00:31:46.000]   years a lot, probably ad nauseam,
[00:31:46.000 --> 00:31:49.760]   is this story of how technology has--
[00:31:49.760 --> 00:31:53.240]   and social media have really weaponized these kind of things.
[00:31:53.240 --> 00:31:56.640]   And given people tools that never existed before,
[00:31:56.640 --> 00:31:59.160]   propaganda tools that never existed before.
[00:31:59.160 --> 00:32:04.920]   I always think about Herman Gebels, who in the Third Reich
[00:32:04.920 --> 00:32:07.320]   figured out that radio, this brand new medium of radio,
[00:32:07.320 --> 00:32:08.840]   would be a perfect propaganda tool.
[00:32:08.840 --> 00:32:11.840]   And the Nazis were very effective in using radio.
[00:32:11.840 --> 00:32:15.000]   And now we have new tools.
[00:32:15.000 --> 00:32:18.480]   And I don't think anybody would use radio.
[00:32:18.480 --> 00:32:20.280]   All right, if you're going to go historical,
[00:32:20.280 --> 00:32:22.840]   I have to go historical and bring up Howard Dean, who
[00:32:22.840 --> 00:32:25.400]   was the first guy who used the internet in a campaign.
[00:32:25.400 --> 00:32:27.640]   For fundraising, and very effectively.
[00:32:27.640 --> 00:32:28.160]   Right.
[00:32:28.160 --> 00:32:33.080]   In 2004, he had the first campaign website.
[00:32:33.080 --> 00:32:34.080]   Yeah.
[00:32:34.080 --> 00:32:37.280]   Did he really use the fund raised and had volunteer
[00:32:37.280 --> 00:32:38.040]   recruitment?
[00:32:38.040 --> 00:32:40.080]   Look how recent that was, though.
[00:32:40.080 --> 00:32:40.520]   Yeah.
[00:32:40.520 --> 00:32:41.000]   Right?
[00:32:41.000 --> 00:32:43.520]   That was what 10, 12 years ago?
[00:32:43.520 --> 00:32:47.120]   Things changed so rapidly.
[00:32:47.120 --> 00:32:49.440]   You know, I mean, it was--
[00:32:49.440 --> 00:32:51.040]   we kind of absorbed smartphones.
[00:32:51.040 --> 00:32:55.120]   But really, Obama was the first president to have a smartphone.
[00:32:55.120 --> 00:32:56.840]   I remember that was a big deal when he switched away
[00:32:56.840 --> 00:32:57.840]   from a blackberry.
[00:32:57.840 --> 00:32:58.200]   Yeah.
[00:32:58.200 --> 00:33:00.880]   That was like a-- that was a huge thing.
[00:33:00.880 --> 00:33:02.040]   Whoa.
[00:33:02.040 --> 00:33:04.960]   And now it's going to be Mike Bloomberg, the first president
[00:33:04.960 --> 00:33:07.560]   to use micro-influencers.
[00:33:07.560 --> 00:33:09.600]   Holy cow.
[00:33:09.600 --> 00:33:12.080]   You said to you something interesting, you got my attention.
[00:33:12.080 --> 00:33:15.000]   Ashley, you have the new Motorola razor.
[00:33:15.000 --> 00:33:16.600]   Have you-- did you get it?
[00:33:16.600 --> 00:33:17.080]   No.
[00:33:17.080 --> 00:33:17.580]   No.
[00:33:17.580 --> 00:33:18.080]   I don't know what it is.
[00:33:18.080 --> 00:33:18.880]   You still haven't.
[00:33:18.880 --> 00:33:19.360]   I don't have it.
[00:33:19.360 --> 00:33:20.960]   I have a theory, actually.
[00:33:20.960 --> 00:33:21.520]   I have a theory.
[00:33:21.520 --> 00:33:22.520]   So--
[00:33:22.520 --> 00:33:24.200]   They don't want you to have it because--
[00:33:24.200 --> 00:33:24.720]   Here's the thing.
[00:33:24.720 --> 00:33:26.520]   CNET did get one.
[00:33:26.520 --> 00:33:28.600]   It ended up in our Louisville office,
[00:33:28.600 --> 00:33:29.440]   which is pretty cool.
[00:33:29.440 --> 00:33:30.800]   At least we got one.
[00:33:30.800 --> 00:33:34.840]   And we actually put it in our-- we have a folding machine.
[00:33:34.840 --> 00:33:36.800]   And we put the galaxy fold in there a while back.
[00:33:36.800 --> 00:33:39.280]   And I think it was like 120 plus 1,000 folds.
[00:33:39.280 --> 00:33:40.040]   I love it.
[00:33:40.040 --> 00:33:43.040]   The CNET has a folding machine.
[00:33:43.040 --> 00:33:43.560]   Oh, yeah.
[00:33:43.560 --> 00:33:46.560]   We get serious about our folding screens.
[00:33:46.560 --> 00:33:51.640]   And we did a live stream on launch day, last week,
[00:33:51.640 --> 00:33:54.000]   where we put the razor to the test,
[00:33:54.000 --> 00:33:55.680]   and we put it in that machine.
[00:33:55.680 --> 00:33:58.600]   And to be fair, it is not a machine
[00:33:58.600 --> 00:34:00.760]   that is calibrated in the same way.
[00:34:00.760 --> 00:34:03.120]   I'm curious as to what Motorola's process is.
[00:34:03.120 --> 00:34:04.440]   They did tweet at us, and they're like,
[00:34:04.440 --> 00:34:07.760]   this is not calibrated the same way we test our phones
[00:34:07.760 --> 00:34:08.360]   and all that stuff.
[00:34:08.360 --> 00:34:09.200]   It's so weird.
[00:34:09.200 --> 00:34:11.520]   Humans are not calibrated machines.
[00:34:11.520 --> 00:34:13.600]   You're going to open it kind of weird anyway.
[00:34:13.600 --> 00:34:16.040]   But it is a very good folding machine.
[00:34:16.040 --> 00:34:19.280]   And so it made it about 27,000 cycles.
[00:34:19.280 --> 00:34:21.560]   And then they said, we got to stop.
[00:34:21.560 --> 00:34:22.760]   It's definitely breaking down.
[00:34:22.760 --> 00:34:24.320]   But yeah, that's our folding machine right there.
[00:34:24.320 --> 00:34:26.200]   I mean, just open and close it.
[00:34:26.200 --> 00:34:27.200]   We do a big live stream.
[00:34:27.200 --> 00:34:30.920]   It just goes until we feel it's not going to work anymore.
[00:34:30.920 --> 00:34:32.960]   27,000 folds.
[00:34:32.960 --> 00:34:34.400]   That's a lot.
[00:34:34.400 --> 00:34:35.040]   It's a lot.
[00:34:35.040 --> 00:34:36.720]   But it's not 120,000.
[00:34:36.720 --> 00:34:39.720]   I mean, the Galaxy Fold did--
[00:34:39.720 --> 00:34:43.680]   I think it was 124,000 volts.
[00:34:43.680 --> 00:34:49.200]   Well, if you're going to open and close it 10, 20 times a day.
[00:34:49.200 --> 00:34:50.160]   Right.
[00:34:50.160 --> 00:34:51.760]   So I mean, yeah, I mean--
[00:34:51.760 --> 00:34:55.000]   And again, we're not perfectly calibrated machines.
[00:34:55.000 --> 00:34:57.160]   So I argue that this is like a--
[00:34:57.160 --> 00:34:58.160]   it's a reasonable test.
[00:34:58.160 --> 00:35:01.200]   Like I said, it's not factory quality calibration.
[00:35:01.200 --> 00:35:03.400]   But we do this school.
[00:35:03.400 --> 00:35:04.880]   We do say it's like four hours long.
[00:35:04.880 --> 00:35:05.960]   So skip toward the end.
[00:35:05.960 --> 00:35:07.320]   But they did get one.
[00:35:07.320 --> 00:35:09.000]   And that's Lexi Savitas.
[00:35:09.000 --> 00:35:12.520]   She's checking out the devices, showing off the old one.
[00:35:12.520 --> 00:35:13.840]   But there it is.
[00:35:13.840 --> 00:35:14.960]   We've got the timer.
[00:35:14.960 --> 00:35:16.680]   We've got that thing.
[00:35:16.680 --> 00:35:18.120]   We've got the whole thing going.
[00:35:18.120 --> 00:35:19.560]   And no, that's in real--
[00:35:19.560 --> 00:35:20.320]   I think that's in real time.
[00:35:20.320 --> 00:35:21.320]   This is in real time, yeah.
[00:35:21.320 --> 00:35:22.000]   It's just fast.
[00:35:22.000 --> 00:35:22.880]   Yeah, we have it fold--
[00:35:22.880 --> 00:35:23.520]   Well, it's definitely--
[00:35:23.520 --> 00:35:24.360]   27,000.
[00:35:24.360 --> 00:35:25.520]   That is pretty quick.
[00:35:25.520 --> 00:35:26.520]   That is really quick.
[00:35:26.520 --> 00:35:27.440]   That is really, really quick.
[00:35:27.440 --> 00:35:27.440]   I'm not even--
[00:35:27.440 --> 00:35:29.000]   I'm not even just to the audio.
[00:35:29.000 --> 00:35:31.920]   There's something very zen about watching this folding machine.
[00:35:31.920 --> 00:35:32.880]   Let me turn on the audio.
[00:35:32.880 --> 00:35:34.800]   It is very weird.
[00:35:34.800 --> 00:35:36.160]   Can you turn up my audio?
[00:35:36.160 --> 00:35:38.800]   Can you hear it?
[00:35:38.800 --> 00:35:41.680]   It looks a little bit like a robot learning how to upload.
[00:35:41.680 --> 00:35:42.800]   Like to clap, yeah.
[00:35:42.800 --> 00:35:43.880]   Close your eyes.
[00:35:43.880 --> 00:35:44.880]   I'm not going to be learning how to clap.
[00:35:44.880 --> 00:35:46.280]   It sounds like a spanking machine to me.
[00:35:46.280 --> 00:35:48.120]   I'm sorry.
[00:35:48.120 --> 00:35:48.720]   You might ask--
[00:35:48.720 --> 00:35:50.200]   I'd say it's like a robot clap machine.
[00:35:50.200 --> 00:35:54.320]   Leo, how do you know what a spanking machine sounds like?
[00:35:54.320 --> 00:35:55.560]   No, no one asked.
[00:35:55.560 --> 00:35:57.560]   [LAUGHTER]
[00:35:57.560 --> 00:35:59.920]   I just want to say that when I was at Yale,
[00:35:59.920 --> 00:36:01.360]   there was a secret society.
[00:36:01.360 --> 00:36:02.800]   Well, I'll stop there.
[00:36:02.800 --> 00:36:04.760]   [LAUGHTER]
[00:36:04.760 --> 00:36:05.840]   That is hysterical.
[00:36:05.840 --> 00:36:11.040]   So at 27,000, it just-- what happened?
[00:36:11.040 --> 00:36:13.000]   It just-- I think they pulled it out of the machine,
[00:36:13.000 --> 00:36:13.600]   because they were--
[00:36:13.600 --> 00:36:14.120]   We don't--
[00:36:14.120 --> 00:36:15.760]   We started to smoke.
[00:36:15.760 --> 00:36:18.040]   We actually said we don't make it go until it's
[00:36:18.040 --> 00:36:20.600]   about the batteries in danger or anything like that,
[00:36:20.600 --> 00:36:24.280]   because we don't want to put anybody in actual harm's way.
[00:36:24.280 --> 00:36:26.800]   So I believe there was a point where they said, OK,
[00:36:26.800 --> 00:36:27.560]   we've got to call it.
[00:36:27.560 --> 00:36:29.000]   We've got to pull this thing out.
[00:36:29.000 --> 00:36:32.440]   Well, to be completely fair, now that people have this,
[00:36:32.440 --> 00:36:34.600]   a lot of people are saying it's not--
[00:36:34.600 --> 00:36:36.160]   it's falling apart in my hands.
[00:36:36.160 --> 00:36:36.680]   Not great.
[00:36:36.680 --> 00:36:37.840]   Yeah.
[00:36:37.840 --> 00:36:42.120]   This is the inherent issue with new technologies.
[00:36:42.120 --> 00:36:45.160]   I mean, it's cool to be an early adopter,
[00:36:45.160 --> 00:36:46.960]   but you also have to kind of accept
[00:36:46.960 --> 00:36:49.840]   that maybe you're not going to get the highest quality thing.
[00:36:49.840 --> 00:36:52.440]   You might just get a really cool futuristic thing that
[00:36:52.440 --> 00:36:54.720]   will be better and better every iteration after.
[00:36:54.720 --> 00:36:59.080]   It really does feel like the idea of a screen that folds.
[00:36:59.080 --> 00:37:00.640]   I want a tablet that folds, right?
[00:37:00.640 --> 00:37:05.560]   Where it's like an iPad mini that folds in half
[00:37:05.560 --> 00:37:10.760]   into a phone, like an iPhone max on the back of that.
[00:37:10.760 --> 00:37:12.320]   So there's two different approaches.
[00:37:12.320 --> 00:37:15.360]   One is that it's a normal-sized phone that becomes a tablet.
[00:37:15.360 --> 00:37:19.080]   The other is that it's a normal-sized fold that folds
[00:37:19.080 --> 00:37:20.400]   into like a pocket square.
[00:37:20.400 --> 00:37:20.960]   So smaller.
[00:37:20.960 --> 00:37:21.600]   Yeah, yeah.
[00:37:21.600 --> 00:37:22.600]   In fact--
[00:37:22.600 --> 00:37:25.160]   The flip was-- is it the flip?
[00:37:25.160 --> 00:37:25.920]   I think it's a--
[00:37:25.920 --> 00:37:26.440]   Yeah, and there's a rumor.
[00:37:26.440 --> 00:37:27.520]   It looks like a little compact.
[00:37:27.520 --> 00:37:28.320]   It's real cute.
[00:37:28.320 --> 00:37:31.560]   Tuesday, Samsung's going to have an event in San Francisco.
[00:37:31.560 --> 00:37:32.600]   Yeah, I saw that.
[00:37:32.600 --> 00:37:33.360]   That's the rumor.
[00:37:33.360 --> 00:37:33.920]   That's the rumor.
[00:37:33.920 --> 00:37:34.960]   There's a rumor they're going to do that.
[00:37:34.960 --> 00:37:36.600]   And I think Huawei has one they've already--
[00:37:36.600 --> 00:37:38.800]   that we've seen leaked video of.
[00:37:38.800 --> 00:37:40.840]   But Samsung, it folds into a little--
[00:37:40.840 --> 00:37:42.640]   It looks like a little makeup compact.
[00:37:42.640 --> 00:37:43.520]   Like, that's the rumor.
[00:37:43.520 --> 00:37:45.040]   It's going to be a little tiny square.
[00:37:45.040 --> 00:37:46.120]   That's an interesting question.
[00:37:46.120 --> 00:37:48.880]   Would you rather have a regular-sized phone that
[00:37:48.880 --> 00:37:51.400]   folds out to a tablet size?
[00:37:51.400 --> 00:37:53.880]   I think I wouldn't mind having a little pocket phone that
[00:37:53.880 --> 00:37:54.400]   becomes a tablet.
[00:37:54.400 --> 00:37:57.840]   Yeah, I think a lot of people who don't
[00:37:57.840 --> 00:38:01.320]   have that power user case would probably enjoy
[00:38:01.320 --> 00:38:02.920]   having a smaller device.
[00:38:02.920 --> 00:38:04.720]   I mean, we see how many people still
[00:38:04.720 --> 00:38:07.000]   wish for the days of the iPhone SE.
[00:38:07.000 --> 00:38:09.360]   Like, oh, man, I really love the original size iPhone.
[00:38:09.360 --> 00:38:11.560]   It was the perfect size.
[00:38:11.560 --> 00:38:14.080]   And so there are those people out there.
[00:38:14.080 --> 00:38:16.400]   And I think a lot of casual users
[00:38:16.400 --> 00:38:18.120]   would be very happy to have a phone like that that
[00:38:18.120 --> 00:38:20.360]   folds up real small and slips into their pocket
[00:38:20.360 --> 00:38:24.520]   or just a little clutch purse or something like that.
[00:38:24.520 --> 00:38:25.720]   I mean, I don't blame them.
[00:38:25.720 --> 00:38:26.640]   That would be cool.
[00:38:26.640 --> 00:38:30.440]   Hey, Nate, I didn't mention this, but you work for Bloomberg.
[00:38:30.440 --> 00:38:33.200]   Do you have to recuse yourself if we do a story with Mike
[00:38:33.200 --> 00:38:35.080]   Bloomberg in it?
[00:38:35.080 --> 00:38:37.720]   I mean, I did, but only because I didn't really
[00:38:37.720 --> 00:38:39.280]   have anything to say.
[00:38:39.280 --> 00:38:40.680]   I mean, we don't really hear anything.
[00:38:40.680 --> 00:38:42.280]   You didn't get a memo from Mike saying,
[00:38:42.280 --> 00:38:45.400]   don't talk about me behind my back or anything like that.
[00:38:45.400 --> 00:38:45.920]   OK.
[00:38:45.920 --> 00:38:47.120]   No, I didn't.
[00:38:47.120 --> 00:38:50.320]   He'll get us $100, $50 check later this year.
[00:38:50.320 --> 00:38:52.120]   He does seem like somebody you could trust
[00:38:52.120 --> 00:38:55.320]   and would be able to run the country in a good way,
[00:38:55.320 --> 00:38:56.920]   because he's a technocrat.
[00:38:56.920 --> 00:39:00.840]   He wouldn't have let this Iowa fiasco happen.
[00:39:00.840 --> 00:39:02.240]   I have no idea.
[00:39:02.240 --> 00:39:03.920]   But to be honest, so little of that
[00:39:03.920 --> 00:39:07.960]   makes its way into our London Bureau, to be honest,
[00:39:07.960 --> 00:39:11.720]   that it doesn't really affect us on a daily basis.
[00:39:11.720 --> 00:39:15.240]   Have you ever met Mike?
[00:39:15.240 --> 00:39:18.040]   By the way, when I do that, I'm not talking about his height.
[00:39:18.040 --> 00:39:22.520]   But have you ever met Mike?
[00:39:22.520 --> 00:39:23.840]   I haven't met him.
[00:39:23.840 --> 00:39:29.200]   I did walk past him in our breakout area once at our London
[00:39:29.200 --> 00:39:31.600]   office and almost built coffee on him.
[00:39:31.600 --> 00:39:32.120]   That's good.
[00:39:32.120 --> 00:39:38.080]   But he's always trailed by several very serious looking
[00:39:38.080 --> 00:39:39.520]   people with earpieces.
[00:39:39.520 --> 00:39:41.160]   So yes.
[00:39:41.160 --> 00:39:42.480]   You know.
[00:39:42.480 --> 00:39:44.960]   But I do see him in the office from time to time over here.
[00:39:44.960 --> 00:39:49.680]   Let's take a little break.
[00:39:49.680 --> 00:39:53.120]   And we won't talk about politics anymore.
[00:39:53.120 --> 00:39:55.720]   Although it's really so fun to talk about.
[00:39:55.720 --> 00:39:58.520]   It's hard not to.
[00:39:58.520 --> 00:40:02.800]   I'll show you by my mattress.
[00:40:02.800 --> 00:40:05.320]   Man, that mattress takes a pounding.
[00:40:05.320 --> 00:40:08.600]   The Casper-- oh, I love my Casper, an online retailer
[00:40:08.600 --> 00:40:11.400]   of premium mattresses for a fraction of the cost.
[00:40:11.400 --> 00:40:13.720]   And I think by now, everybody knows the story.
[00:40:13.720 --> 00:40:16.280]   Casper took a look at the mattress businesses.
[00:40:16.280 --> 00:40:20.600]   This is ripe for reinvention.
[00:40:20.600 --> 00:40:22.760]   Look at the markup mattress stores take.
[00:40:22.760 --> 00:40:24.240]   They double the cost of the mattress.
[00:40:24.240 --> 00:40:25.800]   If we could sell direct to people,
[00:40:25.800 --> 00:40:29.560]   why we could save them hundreds, nay thousands of dollars.
[00:40:29.560 --> 00:40:32.640]   But there was one little problem.
[00:40:32.640 --> 00:40:35.560]   People like to lie on a mattress before they buy it.
[00:40:35.560 --> 00:40:38.880]   That's silly because you can't really-- oh, that's nice.
[00:40:38.880 --> 00:40:42.040]   You can't really tell from lying on a mattress
[00:40:42.040 --> 00:40:43.640]   in a mattress store if you're going to like that when
[00:40:43.640 --> 00:40:44.440]   you go to bed on it.
[00:40:44.440 --> 00:40:46.160]   So Casper has a much better way.
[00:40:46.160 --> 00:40:49.680]   You get to lie on that mattress to try it for 100 nights.
[00:40:49.680 --> 00:40:51.600]   At any time, in those first 100 nights,
[00:40:51.600 --> 00:40:54.440]   if it doesn't exactly fit your needs,
[00:40:54.440 --> 00:40:56.760]   you call them up to come and get it and refund you every penny.
[00:40:56.760 --> 00:40:59.120]   It will cost you absolutely nothing.
[00:40:59.120 --> 00:41:04.880]   That is a, I believe, the new hybrid Casper hybrid mattress.
[00:41:04.880 --> 00:41:09.680]   I actually have multiple Casper mattresses.
[00:41:09.680 --> 00:41:11.920]   I had the original Casper, which is awesome,
[00:41:11.920 --> 00:41:14.200]   that combines multiple supportive memory
[00:41:14.200 --> 00:41:17.240]   foams for a quality sleep surface, just the right amount
[00:41:17.240 --> 00:41:18.680]   of sink and bounce.
[00:41:18.680 --> 00:41:22.040]   And as with all Casper mattresses, it's breathable.
[00:41:22.040 --> 00:41:23.200]   That means you sleep cool.
[00:41:23.200 --> 00:41:28.080]   Even if you have a cat sleeping next to you, as I often do,
[00:41:28.080 --> 00:41:30.080]   you're never hot in a Casper mattress.
[00:41:30.080 --> 00:41:32.600]   It lets you easily regulate your body temperature.
[00:41:32.600 --> 00:41:36.080]   20,000 reviews, an average of 4.8 stars.
[00:41:36.080 --> 00:41:38.160]   That's on Casper Amazon Google.
[00:41:38.160 --> 00:41:40.360]   Casper is becoming the internet's favorite mattress.
[00:41:40.360 --> 00:41:42.400]   Now they've added some new ones.
[00:41:42.400 --> 00:41:45.040]   There's the Wave, which features a patent-pending premium
[00:41:45.040 --> 00:41:47.680]   support system that mirrors the natural shape
[00:41:47.680 --> 00:41:49.000]   of your body.
[00:41:49.000 --> 00:41:51.000]   There's the essential, a streamlined design,
[00:41:51.000 --> 00:41:53.440]   and a price that won't keep you up at night.
[00:41:53.440 --> 00:41:56.440]   And the hybrid, which combines the pressure relief
[00:41:56.440 --> 00:41:59.560]   of award-winning foam with durable yet gentle springs.
[00:41:59.560 --> 00:42:00.320]   I like the hybrid.
[00:42:00.320 --> 00:42:02.960]   That's the one we've got now.
[00:42:02.960 --> 00:42:06.240]   Because I'm a big guy.
[00:42:06.240 --> 00:42:09.400]   And I don't know if you've noticed big guys or gals,
[00:42:09.400 --> 00:42:11.720]   but sometimes when you get out of bed,
[00:42:11.720 --> 00:42:15.640]   the bed gives way underneath you when you fall out of bed.
[00:42:15.640 --> 00:42:21.800]   I love the firm edge on the hybrid, those durable springs.
[00:42:21.800 --> 00:42:24.080]   And they still give you the give and the support.
[00:42:24.080 --> 00:42:26.080]   But they also have a little bit of firmer edge,
[00:42:26.080 --> 00:42:27.000]   which is very nice.
[00:42:27.000 --> 00:42:29.280]   Actually, I just-- all my Caspers.
[00:42:29.280 --> 00:42:31.280]   I love all my Caspers.
[00:42:31.280 --> 00:42:33.600]   And by the way, there are plenty of other products
[00:42:33.600 --> 00:42:36.880]   there like my Casper pillow, which if I get in bed
[00:42:36.880 --> 00:42:39.080]   and my Casper pillow is missing, I yell at my wife.
[00:42:39.080 --> 00:42:40.320]   I say, where's my pillow?
[00:42:40.320 --> 00:42:41.640]   I need my pillow.
[00:42:41.640 --> 00:42:43.520]   And the great Casper sheets.
[00:42:43.520 --> 00:42:46.400]   And of course, the Casper glow, that lamp that wakes you up
[00:42:46.400 --> 00:42:50.600]   slowly, 100 nights, risk-free sleep on a trial.
[00:42:50.600 --> 00:42:52.160]   You can be sure of your purchase.
[00:42:52.160 --> 00:42:54.440]   They also offer free shipping and painless returns
[00:42:54.440 --> 00:42:56.520]   in the US and Canada.
[00:42:56.520 --> 00:42:58.680]   Casper, get a Casper mattress today.
[00:42:58.680 --> 00:43:01.280]   Save $100 towards select mattresses
[00:43:01.280 --> 00:43:05.160]   by visiting Casper.com/twit1 and using the offer code
[00:43:05.160 --> 00:43:06.400]   "Twit1" at checkout.
[00:43:06.400 --> 00:43:11.320]   $100 off select mattresses by going to Casper.com/twit1
[00:43:11.320 --> 00:43:15.400]   and entering the code "Twit1" at checkout terms and conditions.
[00:43:15.400 --> 00:43:16.320]   Apply.
[00:43:16.320 --> 00:43:20.080]   Thank you, Casper, for supporting our programs.
[00:43:20.080 --> 00:43:21.600]   Thank you for supporting our programs
[00:43:21.600 --> 00:43:23.880]   by using that offer code and that special address.
[00:43:23.880 --> 00:43:25.240]   That way, you let them know you saw it here.
[00:43:25.240 --> 00:43:29.680]   Casper.com/twit1, promo code "Twit1."
[00:43:29.680 --> 00:43:32.480]   Where else should we go with this one?
[00:43:32.480 --> 00:43:34.640]   Is this a sign of the times?
[00:43:34.640 --> 00:43:38.360]   I thought this was kind of a little bit of a sad story.
[00:43:38.360 --> 00:43:40.680]   The New York Police Department, famous-- you've seen it
[00:43:40.680 --> 00:43:42.320]   on a million crime shows--
[00:43:42.320 --> 00:43:47.720]   for their notebooks, their handwritten activity logs.
[00:43:47.720 --> 00:43:51.000]   They've been using those since the 1800s.
[00:43:51.000 --> 00:43:54.600]   They are going to phase them out for--
[00:43:54.600 --> 00:43:57.160]   get this-- an iPhone app.
[00:43:57.160 --> 00:44:02.320]   Officers will have department issued iPhone apps,
[00:44:02.320 --> 00:44:05.160]   and they'll be doing all their logging in the app
[00:44:05.160 --> 00:44:07.280]   instead of the notebook.
[00:44:07.280 --> 00:44:10.080]   I feel like that's kind of sad.
[00:44:10.080 --> 00:44:11.680]   It's kind of the end of an era.
[00:44:11.680 --> 00:44:14.480]   Not to mention how hard it is to type on that Apple keyboard
[00:44:14.480 --> 00:44:16.760]   when you're chasing it.
[00:44:16.760 --> 00:44:18.480]   An armed felon.
[00:44:18.480 --> 00:44:20.240]   Well, they'll use Siri, right?
[00:44:20.240 --> 00:44:21.080]   They'll just make a thing.
[00:44:21.080 --> 00:44:21.600]   Oh, that'll work.
[00:44:21.600 --> 00:44:22.600]   That'll work.
[00:44:22.600 --> 00:44:23.600]   Wonderful.
[00:44:23.600 --> 00:44:27.400]   And that Siri being so accurate will misinterpret
[00:44:27.400 --> 00:44:29.280]   the evidence, and then it'll be used in--
[00:44:29.280 --> 00:44:30.840]   You bring it to court.
[00:44:30.840 --> 00:44:33.760]   I recorded it at this time.
[00:44:33.760 --> 00:44:37.040]   Siri, tell me what I recorded at that time.
[00:44:37.040 --> 00:44:40.040]   Look what I found on the internet about that.
[00:44:40.040 --> 00:44:43.800]   Thanks a lot, Siri.
[00:44:43.800 --> 00:44:44.760]   Anyway.
[00:44:44.760 --> 00:44:47.520]   This sounds a lot like the Iowa caucus using an app.
[00:44:47.520 --> 00:44:48.520]   I have to say.
[00:44:48.520 --> 00:44:50.000]   Yeah, Siri.
[00:44:50.000 --> 00:44:50.800]   Very--
[00:44:50.800 --> 00:44:55.680]   I stopped using notebooks, paper notebooks in journalism
[00:44:55.680 --> 00:44:56.400]   several years ago.
[00:44:56.400 --> 00:44:59.480]   I just went fully digital, just all in not a single piece
[00:44:59.480 --> 00:45:00.360]   of paper anymore.
[00:45:00.360 --> 00:45:01.320]   Really?
[00:45:01.320 --> 00:45:04.440]   You don't use the same as the reporters in a notebook anymore?
[00:45:04.440 --> 00:45:06.040]   Nope, I don't at all.
[00:45:06.040 --> 00:45:06.920]   I have them.
[00:45:06.920 --> 00:45:08.760]   They're there, but I never, ever use them.
[00:45:08.760 --> 00:45:12.960]   I just either dictate things, or I use--
[00:45:12.960 --> 00:45:15.080]   if I've got my iPad, I'll write with the Apple pen
[00:45:15.080 --> 00:45:15.720]   soil in.
[00:45:15.720 --> 00:45:16.440]   When I worked--
[00:45:16.440 --> 00:45:17.080]   And things like that.
[00:45:17.080 --> 00:45:20.960]   When I worked for a network, I was told to not only use the
[00:45:20.960 --> 00:45:23.600]   notebook, write everything down, because that's a legal
[00:45:23.600 --> 00:45:25.000]   document.
[00:45:25.000 --> 00:45:25.520]   Right?
[00:45:25.520 --> 00:45:26.000]   Yeah.
[00:45:26.000 --> 00:45:26.360]   And it's a--
[00:45:26.360 --> 00:45:27.080]   Yeah, we can test.
[00:45:27.080 --> 00:45:28.000]   --the quotes or whatever.
[00:45:28.000 --> 00:45:30.120]   You've got it in your notebook.
[00:45:30.120 --> 00:45:30.760]   Absolutely.
[00:45:30.760 --> 00:45:34.680]   And you would keep notebooks for a couple of years or longer.
[00:45:34.680 --> 00:45:36.600]   But I've got every note.
[00:45:36.600 --> 00:45:39.840]   I've been a reporter for nearly 15 years now, and I've got
[00:45:39.840 --> 00:45:42.960]   every note pretty much that I've certainly written in the
[00:45:42.960 --> 00:45:45.280]   last 10 years.
[00:45:45.280 --> 00:45:48.080]   All digital, all archived.
[00:45:48.080 --> 00:45:50.640]   What about using-- I really like the feature now in the
[00:45:50.640 --> 00:45:53.040]   modern Android 10.
[00:45:53.040 --> 00:45:55.120]   They have a voice recorder that transcribes and does a
[00:45:55.120 --> 00:45:56.040]   very good job.
[00:45:56.040 --> 00:45:56.560]   What about that?
[00:45:56.560 --> 00:45:58.160]   Could you use that?
[00:45:58.160 --> 00:45:58.600]   I do.
[00:45:58.600 --> 00:46:03.400]   I don't use that specifically, but I do run non-sensitive
[00:46:03.400 --> 00:46:08.880]   interviews through an AI powered transcription service that
[00:46:08.880 --> 00:46:10.920]   doesn't reach human people.
[00:46:10.920 --> 00:46:13.320]   But obviously, there's always a chance that it could, so for
[00:46:13.320 --> 00:46:19.280]   anything that's actually sensitive, that never leaves
[00:46:19.280 --> 00:46:19.960]   my device.
[00:46:19.960 --> 00:46:24.040]   The Google thing, as I remember, the new recorder is
[00:46:24.040 --> 00:46:25.960]   device-based.
[00:46:25.960 --> 00:46:26.880]   But you're right.
[00:46:26.880 --> 00:46:28.000]   I mean, there's no--
[00:46:28.000 --> 00:46:31.440]   I use dictation on Gboard on my iPhone.
[00:46:31.440 --> 00:46:33.080]   So I use the Google keyboard.
[00:46:33.080 --> 00:46:33.920]   And let it dictate.
[00:46:33.920 --> 00:46:34.520]   Yeah.
[00:46:34.520 --> 00:46:34.920]   Yeah.
[00:46:34.920 --> 00:46:37.200]   It's really good.
[00:46:37.200 --> 00:46:40.240]   Yeah, just Nick in chat says, I guarantee you they will all
[00:46:40.240 --> 00:46:43.160]   use voice memos, and I think that's probably spot on.
[00:46:43.160 --> 00:46:47.040]   And I think also, there's going to be a good, healthy number
[00:46:47.040 --> 00:46:49.360]   of New York police officers that still write things down.
[00:46:49.360 --> 00:46:50.640]   They keep their memo book.
[00:46:50.640 --> 00:46:52.600]   They're in a notebook, and then transfer it to the app, and
[00:46:52.600 --> 00:46:53.520]   they have both.
[00:46:53.520 --> 00:46:57.000]   What you lose is, as an example, this image from the
[00:46:57.000 --> 00:47:00.920]   New York Times, this is Officer Sean McGill's memo book, the
[00:47:00.920 --> 00:47:05.240]   first police officer to arrive at the World Trade Center on 9/11.
[00:47:05.240 --> 00:47:08.880]   And so that's a historic document.
[00:47:08.880 --> 00:47:12.760]   And there is something in the written paper that cannot be
[00:47:12.760 --> 00:47:15.360]   replaced with the kind of antiseptic digital
[00:47:15.360 --> 00:47:18.280]   transcription or typing.
[00:47:18.280 --> 00:47:20.360]   I guess an audio recording might be even better.
[00:47:20.360 --> 00:47:23.960]   But I remember when Nicholson Baker was fighting--
[00:47:23.960 --> 00:47:26.840]   he's one of my favorite novelists.
[00:47:26.840 --> 00:47:28.680]   He was fighting the move.
[00:47:28.680 --> 00:47:31.320]   Libraries all over the world are doing this, replacing their
[00:47:31.320 --> 00:47:35.080]   handwritten card catalogs with digital card catalogs.
[00:47:35.080 --> 00:47:38.040]   And he was bemoaning it because he said the notations, the
[00:47:38.040 --> 00:47:40.800]   things that are written on those cards by generations of
[00:47:40.800 --> 00:47:43.360]   librarians are valuable information.
[00:47:43.360 --> 00:47:44.080]   And you're losing it.
[00:47:44.080 --> 00:47:46.360]   You're throwing it away.
[00:47:46.360 --> 00:47:50.480]   And I went back to my alma mater a few years ago, and
[00:47:50.480 --> 00:47:54.200]   where there used to be these great, beautiful, gothic
[00:47:54.200 --> 00:47:57.920]   cavernous rooms with giant wooden card catalogs.
[00:47:57.920 --> 00:48:00.800]   It felt like miles of them, completely replaced by
[00:48:00.800 --> 00:48:02.240]   terminals.
[00:48:02.240 --> 00:48:05.520]   And Baker says, yeah, those catalogs, they move them to a
[00:48:05.520 --> 00:48:07.720]   storage facility, and then eventually they just destroy
[00:48:07.720 --> 00:48:08.280]   them.
[00:48:08.280 --> 00:48:08.760]   They're gone.
[00:48:08.760 --> 00:48:09.280]   But you know what?
[00:48:09.280 --> 00:48:12.240]   I mean, digital information and devices, they capture a
[00:48:12.240 --> 00:48:14.320]   different type of metadata.
[00:48:14.320 --> 00:48:17.520]   I would call that almost metadata.
[00:48:17.520 --> 00:48:19.640]   If there's an main note card and somebody screlled
[00:48:19.640 --> 00:48:22.120]   something down the edge or something like that.
[00:48:22.120 --> 00:48:23.520]   It's basically metadata.
[00:48:23.520 --> 00:48:26.320]   And devices have that kind of metadata.
[00:48:26.320 --> 00:48:28.600]   It might be anything from the GPS location.
[00:48:28.600 --> 00:48:33.000]   It could be the speed of devices traveling at.
[00:48:33.000 --> 00:48:38.480]   I think Uber uses a system now and has in the US for a
[00:48:38.480 --> 00:48:41.960]   little while that can detect crashes or impact if both
[00:48:41.960 --> 00:48:45.160]   driver and passenger cars stop at the same time.
[00:48:45.160 --> 00:48:47.240]   That might indicate that there's been a crash or an
[00:48:47.240 --> 00:48:48.760]   accident of some kind.
[00:48:48.760 --> 00:48:53.440]   And I think that if a police department is using a device,
[00:48:53.440 --> 00:48:56.640]   it is actually potentially capturing more than just the
[00:48:56.640 --> 00:48:59.080]   notes themselves and the voice memos.
[00:48:59.080 --> 00:49:02.040]   There's a whole range of things that even if they're not
[00:49:02.040 --> 00:49:03.840]   capturing them now, they potentially could.
[00:49:03.840 --> 00:49:06.960]   And maybe that would be useful in solving cases down the
[00:49:06.960 --> 00:49:07.320]   line.
[00:49:07.320 --> 00:49:11.160]   Well, and you're a proof, an example of the proof is it
[00:49:11.160 --> 00:49:14.840]   because you are very content with your digital notes now,
[00:49:14.840 --> 00:49:15.360]   right?
[00:49:15.360 --> 00:49:18.240]   They're everybody's good as the handwritten ones you did?
[00:49:18.240 --> 00:49:18.840]   They are.
[00:49:18.840 --> 00:49:19.280]   Yeah.
[00:49:19.280 --> 00:49:21.440]   I mean, I haven't been caught out.
[00:49:21.440 --> 00:49:23.200]   You know, there are always exceptions, obviously, as
[00:49:23.200 --> 00:49:26.200]   a journalist, very different to a police officer.
[00:49:26.200 --> 00:49:29.480]   You know, anytime I'm recording somebody, I'm always asking
[00:49:29.480 --> 00:49:30.440]   them upfront.
[00:49:30.440 --> 00:49:32.680]   You know, I'm never recording anyone, even if it's just
[00:49:32.680 --> 00:49:34.280]   getting transcribed, I always ask.
[00:49:34.280 --> 00:49:36.720]   And sometimes it's, you don't even want to put that
[00:49:36.720 --> 00:49:40.480]   question out there because you don't want to worry a source
[00:49:40.480 --> 00:49:42.720]   potentially so you don't record anything.
[00:49:42.720 --> 00:49:46.360]   And but then I have the iPad and I use the pencil almost as a
[00:49:46.360 --> 00:49:47.360]   body language tool.
[00:49:47.360 --> 00:49:48.600]   Like, look, I'm not recording.
[00:49:48.600 --> 00:49:49.600]   I'm writing notes.
[00:49:49.600 --> 00:49:50.560]   I'm taking notes.
[00:49:50.560 --> 00:49:54.800]   And so far, no one has ever had an issue with that because the
[00:49:54.800 --> 00:49:57.160]   whole relationship is built on trust anyway.
[00:49:57.160 --> 00:49:59.800]   But it'd be interesting to see how it evolves in New York.
[00:49:59.800 --> 00:50:01.080]   I don't think we're doing that here.
[00:50:01.080 --> 00:50:09.840]   Somebody was asking on the radio show if he should use USB
[00:50:09.840 --> 00:50:13.160]   chargers in the airport and other places.
[00:50:13.160 --> 00:50:14.560]   I said, absolutely.
[00:50:14.560 --> 00:50:19.360]   Not NBC News has given it a name, Juice Jacking.
[00:50:19.360 --> 00:50:23.520]   And they have an article why you should avoid public phone
[00:50:23.520 --> 00:50:25.920]   charging stations.
[00:50:25.920 --> 00:50:30.160]   Apparently, and they're everywhere now.
[00:50:30.160 --> 00:50:32.040]   You know, in fact, this was the best thing in the world is
[00:50:32.040 --> 00:50:34.800]   airports and everywhere started adding these places you could
[00:50:34.800 --> 00:50:36.360]   plug in your phone.
[00:50:36.360 --> 00:50:37.280]   Wow, that's great.
[00:50:37.280 --> 00:50:39.880]   Well, Matt, now apparently you shouldn't use those.
[00:50:39.880 --> 00:50:43.240]   I carry with me a USB condom.
[00:50:43.240 --> 00:50:47.800]   It is a device that only lets a charger prevents data from
[00:50:47.800 --> 00:50:50.440]   passing through the USB port.
[00:50:50.440 --> 00:50:51.000]   And I guess--
[00:50:51.000 --> 00:50:53.560]   And how guaranteed is that?
[00:50:53.560 --> 00:50:58.520]   I mean, I'm not a massive security expert, but I do
[00:50:58.520 --> 00:51:00.440]   want to know how rigorous the testing is.
[00:51:00.440 --> 00:51:01.720]   Yeah, that's the one I use.
[00:51:01.720 --> 00:51:02.600]   The port of power.
[00:51:02.600 --> 00:51:03.960]   Well, I don't know.
[00:51:03.960 --> 00:51:04.800]   It's made in China.
[00:51:04.800 --> 00:51:05.880]   It must be good.
[00:51:05.880 --> 00:51:12.240]   The chargers I've seen though, just you don't have any
[00:51:12.240 --> 00:51:14.200]   control over what they're plugging into.
[00:51:14.200 --> 00:51:17.680]   It's just a cord coming out of the wall with a link.
[00:51:17.680 --> 00:51:21.720]   Oh, oh, that's what I would argue is if you're at an
[00:51:21.720 --> 00:51:25.920]   airport and you can't see what the cable's plugged into,
[00:51:25.920 --> 00:51:27.400]   don't plug your phone into that.
[00:51:27.400 --> 00:51:28.560]   I didn't realize they're doing that.
[00:51:28.560 --> 00:51:31.160]   So it's not a USB port.
[00:51:31.160 --> 00:51:32.600]   It's a cable.
[00:51:32.600 --> 00:51:33.120]   Yes.
[00:51:33.120 --> 00:51:33.760]   But don't plug into those.
[00:51:33.760 --> 00:51:35.240]   It's like free charging here.
[00:51:35.240 --> 00:51:36.920]   Look at that at Burbank Airport.
[00:51:36.920 --> 00:51:39.920]   They have these are free charging provided by Channel 7
[00:51:39.920 --> 00:51:41.280]   News or whatever.
[00:51:41.280 --> 00:51:43.520]   Wow, so I run it because Channel 7 News right here is
[00:51:43.520 --> 00:51:44.760]   saying don't use them.
[00:51:44.760 --> 00:51:48.520]   But yeah, it's in there cables that come out of a
[00:51:48.520 --> 00:51:49.600]   kind of a box.
[00:51:49.600 --> 00:51:50.440]   Oh, that's great.
[00:51:50.440 --> 00:51:51.960]   That's terrible.
[00:51:51.960 --> 00:51:52.880]   Why would you plug in there?
[00:51:52.880 --> 00:51:55.800]   Just carry your own wallboard and cable.
[00:51:55.800 --> 00:51:56.600]   Is this like--
[00:51:56.600 --> 00:51:57.920]   Yeah, bring your own plug.
[00:51:57.920 --> 00:51:58.800]   There's outlets around.
[00:51:58.800 --> 00:51:59.200]   Come on.
[00:51:59.200 --> 00:52:01.680]   Yeah, well, there's not as many outlets as there are
[00:52:01.680 --> 00:52:04.240]   charging cables, unfortunately.
[00:52:04.240 --> 00:52:07.040]   The story that accompanied this, the video that
[00:52:07.040 --> 00:52:11.240]   accompanied this was pretty funny because they set up a
[00:52:11.240 --> 00:52:14.760]   sort of a fake charging station where they just wanted to
[00:52:14.760 --> 00:52:16.960]   see if people would come along and use it.
[00:52:16.960 --> 00:52:21.200]   And they had a hacker positioned somewhere.
[00:52:21.200 --> 00:52:24.040]   They put it in a park and they had someone across the park
[00:52:24.040 --> 00:52:26.360]   who, as the people were charging their phone, they're
[00:52:26.360 --> 00:52:29.920]   going through, they're doing e-commerce transactions where
[00:52:29.920 --> 00:52:32.480]   they're putting in their credit card, they're checking
[00:52:32.480 --> 00:52:35.120]   their email, they're putting in passwords for things, and
[00:52:35.120 --> 00:52:36.760]   the guy is logging everything.
[00:52:36.760 --> 00:52:40.800]   And then they bring him over to the poor person
[00:52:40.800 --> 00:52:45.400]   charging and say, hey, look, this is what we were able to find out.
[00:52:45.400 --> 00:52:47.320]   Oh, my god.
[00:52:47.320 --> 00:52:48.480]   Look at this thing.
[00:52:48.480 --> 00:52:49.480]   It's suspicious.
[00:52:49.480 --> 00:52:52.240]   I mean, if I sat down in a park and there was a fast, free
[00:52:52.240 --> 00:52:55.120]   charging thing, that you just--
[00:52:55.120 --> 00:52:58.480]   But if you just printed out a decal that was the city's logo.
[00:52:58.480 --> 00:53:00.000]   Oh, yeah, put the city's logo on it.
[00:53:00.000 --> 00:53:00.520]   Yeah.
[00:53:00.520 --> 00:53:01.360]   That's safe.
[00:53:01.360 --> 00:53:02.480]   Not people don't know.
[00:53:02.480 --> 00:53:03.000]   They wouldn't know.
[00:53:03.000 --> 00:53:04.520]   I'm sorry to give that idea to anybody.
[00:53:04.520 --> 00:53:05.200]   Don't do that.
[00:53:05.200 --> 00:53:06.400]   I'm going to jump.
[00:53:06.400 --> 00:53:08.440]   Don't do that.
[00:53:08.440 --> 00:53:09.840]   I'm sure they already thought of it.
[00:53:09.840 --> 00:53:10.840]   Oh, that's a stir.
[00:53:10.840 --> 00:53:11.840]   Yeah, sure.
[00:53:11.840 --> 00:53:13.240]   But yeah, I mean, it's--
[00:53:13.240 --> 00:53:15.840]   it would be very difficult for someone to discern if it was
[00:53:15.840 --> 00:53:19.680]   something that looked relatively official.
[00:53:19.680 --> 00:53:22.400]   It's not a handwritten sign that says free candy.
[00:53:22.400 --> 00:53:24.560]   This looks like San Diego.
[00:53:24.560 --> 00:53:27.560]   It's right outside the Nimitz or whatever the carrier is.
[00:53:27.560 --> 00:53:30.200]   The ones I've seen are in ski lodges.
[00:53:30.200 --> 00:53:35.440]   And I think people's security radar is at a low threshold.
[00:53:35.440 --> 00:53:39.000]   If you've come into a ski lodge for lunch, you're low on juice.
[00:53:39.000 --> 00:53:42.880]   You want to make sure you have your music flowing for the rest of the afternoon or
[00:53:42.880 --> 00:53:43.880]   whatever.
[00:53:43.880 --> 00:53:44.880]   Honestly.
[00:53:44.880 --> 00:53:46.840]   And you see this series of cables coming up.
[00:53:46.840 --> 00:53:47.840]   Yeah.
[00:53:47.840 --> 00:53:49.640]   Well, you're going to think, oh, thank goodness.
[00:53:49.640 --> 00:53:52.440]   Yeah, even if you know there's a risk, you're going, oh, I don't care.
[00:53:52.440 --> 00:53:53.640]   I need the power anyway.
[00:53:53.640 --> 00:53:54.640]   Yeah.
[00:53:54.640 --> 00:54:00.360]   I'm just-- I'll just-- and I should point out even though that in this instance, they had
[00:54:00.360 --> 00:54:02.560]   people-- they were watching what people were doing.
[00:54:02.560 --> 00:54:06.640]   There's no reason why they couldn't just suck all the data out of your phone.
[00:54:06.640 --> 00:54:10.240]   You don't have to use the phone for them to get data out of it.
[00:54:10.240 --> 00:54:11.240]   Yeah.
[00:54:11.240 --> 00:54:13.360]   OK, that's good to know.
[00:54:13.360 --> 00:54:14.360]   Don't get juice jacks.
[00:54:14.360 --> 00:54:17.760]   I suppose that's going to be the benefit of wireless charging, right?
[00:54:17.760 --> 00:54:20.760]   Because that's not going to be as possible.
[00:54:20.760 --> 00:54:21.760]   Right.
[00:54:21.760 --> 00:54:23.640]   I'm sure someone will figure it out.
[00:54:23.640 --> 00:54:29.520]   Well, if you can hack my Hue light bulbs, you can hack anything.
[00:54:29.520 --> 00:54:36.600]   Apparently, Hue light bulbs, which had a compromise, didn't patch it quite right.
[00:54:36.600 --> 00:54:40.240]   So if you have Philips Hue light bulbs, you might want to check to make sure you've got
[00:54:40.240 --> 00:54:42.360]   the right-- the latest firmware on it.
[00:54:42.360 --> 00:54:46.200]   I'm not going to read that number because it's long.
[00:54:46.200 --> 00:54:48.520]   They didn't fix it quite right.
[00:54:48.520 --> 00:54:51.600]   Checkpoint security discovered.
[00:54:51.600 --> 00:54:54.240]   And told Hue, by the way, they told Philips in November.
[00:54:54.240 --> 00:54:57.760]   So the patch came out mid last month.
[00:54:57.760 --> 00:55:02.560]   If you got the patch-- who knew, first of all, that they could over the air patch your
[00:55:02.560 --> 00:55:03.560]   light bulbs.
[00:55:03.560 --> 00:55:04.560]   I didn't know that.
[00:55:04.560 --> 00:55:05.560]   Oh, yeah.
[00:55:05.560 --> 00:55:06.560]   Yeah.
[00:55:06.560 --> 00:55:07.560]   You shouldn't get light bulbs.
[00:55:07.560 --> 00:55:09.640]   It can't be patched over the air.
[00:55:09.640 --> 00:55:10.640]   So true.
[00:55:10.640 --> 00:55:12.920]   What a world we live in.
[00:55:12.920 --> 00:55:14.400]   What an internet of things.
[00:55:14.400 --> 00:55:16.040]   Good lord.
[00:55:16.040 --> 00:55:22.160]   Does it surprise any that these Hue bulbs are relatively compromiseable?
[00:55:22.160 --> 00:55:26.520]   It seems like mine work when they want to work and they constantly do the flashing at
[00:55:26.520 --> 00:55:28.320]   you thing.
[00:55:28.320 --> 00:55:29.320]   All the time.
[00:55:29.320 --> 00:55:30.320]   All the time.
[00:55:30.320 --> 00:55:31.320]   All the time.
[00:55:31.320 --> 00:55:32.320]   I just--
[00:55:32.320 --> 00:55:36.280]   What exactly-- again, I'm not a security expert either.
[00:55:36.280 --> 00:55:41.960]   So what exactly would the benefit be to hacking my light bulbs?
[00:55:41.960 --> 00:55:42.960]   Is it--
[00:55:42.960 --> 00:55:45.040]   OK, well, you could turn them off.
[00:55:45.040 --> 00:55:46.120]   You could turn them down.
[00:55:46.120 --> 00:55:47.120]   You could make them red.
[00:55:47.120 --> 00:55:48.120]   No, the real issue.
[00:55:48.120 --> 00:55:50.800]   It makes me think I have a ghost in the house.
[00:55:50.800 --> 00:55:51.800]   Yes.
[00:55:51.800 --> 00:55:52.800]   Yeah, there you go.
[00:55:52.800 --> 00:55:53.800]   See, Leo, you've been hacked.
[00:55:53.800 --> 00:55:54.800]   What?
[00:55:54.800 --> 00:55:58.360]   No, the real issue is that could be a gateway into the rest of your network.
[00:55:58.360 --> 00:55:59.440]   To your hope, to your smart.
[00:55:59.440 --> 00:56:00.440]   OK.
[00:56:00.440 --> 00:56:01.440]   See, that makes sense.
[00:56:01.440 --> 00:56:04.680]   But it just-- yeah, it's just so odd to me to have to warn people.
[00:56:04.680 --> 00:56:06.320]   If you have a hue, it might get hacked.
[00:56:06.320 --> 00:56:07.320]   I know.
[00:56:07.320 --> 00:56:08.320]   I know.
[00:56:08.320 --> 00:56:09.320]   In Mr. Robot--
[00:56:09.320 --> 00:56:16.800]   That makes hackers compromised a character's entire smart home, including her light bulbs,
[00:56:16.800 --> 00:56:22.640]   such that they then basically stole her lovely loft dwelling, moved in, made it their headquarters.
[00:56:22.640 --> 00:56:23.640]   Oh.
[00:56:23.640 --> 00:56:25.240]   She didn't want to have it.
[00:56:25.240 --> 00:56:28.680]   Yes, she thought she had a ghost or whatever, so she left.
[00:56:28.680 --> 00:56:30.680]   They gaslighted her, literally.
[00:56:30.680 --> 00:56:31.680]   Yes.
[00:56:31.680 --> 00:56:32.680]   Mm-hmm.
[00:56:32.680 --> 00:56:33.680]   OK.
[00:56:33.680 --> 00:56:40.280]   And I just throw into that my Samsung refrigerator, which I bought in 2015, for the first time
[00:56:40.280 --> 00:56:45.920]   since 2015, three days ago, did a software update, firmware update, first time.
[00:56:45.920 --> 00:56:46.920]   Thank God.
[00:56:46.920 --> 00:56:48.480]   Is it working better now?
[00:56:48.480 --> 00:56:51.160]   I can't really tell.
[00:56:51.160 --> 00:56:55.960]   You don't have-- tell me you don't have a refrigerator with the browser in the door.
[00:56:55.960 --> 00:56:57.360]   It-- yes.
[00:56:57.360 --> 00:56:58.560]   Wait, can you browse?
[00:56:58.560 --> 00:57:00.280]   No, it does little apps.
[00:57:00.280 --> 00:57:05.560]   So if you want to give it your passwords for your social media accounts and things--
[00:57:05.560 --> 00:57:07.560]   You could Instagram from the fridge.
[00:57:07.560 --> 00:57:09.560]   I'm not going to do that.
[00:57:09.560 --> 00:57:12.000]   It's a French to Graham.
[00:57:12.000 --> 00:57:13.680]   There was, and I bet you it's this refrigerator.
[00:57:13.680 --> 00:57:18.240]   There was a problem with a Samsung refrigerator that had an internet explorer browser in it
[00:57:18.240 --> 00:57:20.280]   and couldn't be updated.
[00:57:20.280 --> 00:57:24.600]   So they said you can't-- they stopped-- you can't use the browser in your refrigerator.
[00:57:24.600 --> 00:57:29.120]   This is very much like what has been an ongoing story over the last few weeks.
[00:57:29.120 --> 00:57:33.400]   It started with Sonos, where they were going to-- they said, well, look, we're going to--
[00:57:33.400 --> 00:57:36.480]   the old speakers were not-- you're not going to be able to use them anymore after-- we're
[00:57:36.480 --> 00:57:40.680]   not going to give you any more updates because-- and people were upset because they thought
[00:57:40.680 --> 00:57:41.680]   they bought speakers.
[00:57:41.680 --> 00:57:43.920]   Instead, they realized they bought a computer.
[00:57:43.920 --> 00:57:48.280]   And that's really what's happening is you're seeing these hardware devices that, you know,
[00:57:48.280 --> 00:57:50.320]   in years gone by a speaker was a speaker.
[00:57:50.320 --> 00:57:52.520]   It didn't need firmware updates.
[00:57:52.520 --> 00:57:58.800]   But now, because we put hardware and software into the speaker, you can literally have a
[00:57:58.800 --> 00:58:05.760]   speaker that no longer works at all in any fashion after being bricked by Sonos.
[00:58:05.760 --> 00:58:10.640]   Then we heard this week, Elon Musk's Tesla.
[00:58:10.640 --> 00:58:14.360]   This is a very odd story, but it's related.
[00:58:14.360 --> 00:58:17.400]   Tesla sold through its used auction--
[00:58:17.400 --> 00:58:18.400]   Oh, he heard about that.
[00:58:18.400 --> 00:58:25.680]   --a Model S. And on the sticker, the Model S was equipped with $8,000 worth of self-driving
[00:58:25.680 --> 00:58:30.680]   features, the used car dealer that bought it, had the sticker.
[00:58:30.680 --> 00:58:35.600]   They sold it on because that's what they do to somebody named Alex.
[00:58:35.600 --> 00:58:36.600]   Alex bought it.
[00:58:36.600 --> 00:58:37.600]   He got the sticker.
[00:58:37.600 --> 00:58:40.920]   He thought he was getting these $8,000 worth of self-driving features.
[00:58:40.920 --> 00:58:47.440]   Then Tesla pushes down a firmware update that turned off the self-driving features.
[00:58:47.440 --> 00:58:50.520]   When asked, they said, well, Alex didn't pay for those.
[00:58:50.520 --> 00:58:56.240]   So they literally took $8,000 worth of value out of that car with a firmware update.
[00:58:56.240 --> 00:59:00.540]   Well, didn't Leo-- didn't they say that it was actually originally that the listing
[00:59:00.540 --> 00:59:06.720]   was incorrect and that they said the original owner did not actually pay for those things?
[00:59:06.720 --> 00:59:08.080]   Oh, was that their excuse?
[00:59:08.080 --> 00:59:10.320]   Yeah, I think so.
[00:59:10.320 --> 00:59:17.680]   They said they did some-- every now and again, they do these checks where they see if there's
[00:59:17.680 --> 00:59:25.360]   any mistakenly activated enhanced autopilot and full self-driving on certain cars and
[00:59:25.360 --> 00:59:26.680]   then they disable it.
[00:59:26.680 --> 00:59:35.960]   I think that they said that they had done some sort of check or some sort of-- I don't know,
[00:59:35.960 --> 00:59:39.560]   they're just looking for basically things that have been mistakenly activated.
[00:59:39.560 --> 00:59:41.560]   I think that was their excuse.
[00:59:41.560 --> 00:59:48.280]   Yeah, it's not great.
[00:59:48.280 --> 00:59:54.400]   Here's the sticker that came from Tesla at the auction.
[00:59:54.400 --> 01:00:01.920]   So it says, "An enhanced autopilot $5,000, full self-driving capability, that's laughable,
[01:00:01.920 --> 01:00:04.520]   $3,000."
[01:00:04.520 --> 01:00:06.080]   So that's Tesla's mistake.
[01:00:06.080 --> 01:00:08.040]   They should give him-- yeah, that's their mistake.
[01:00:08.040 --> 01:00:11.480]   They should give-- they should activate those things on that car.
[01:00:11.480 --> 01:00:16.080]   And the used car dealer paid for it and Alex paid for it.
[01:00:16.080 --> 01:00:23.360]   Yeah, Tesla said we didn't audit and yes, as part of that audit, we removed the autopilot
[01:00:23.360 --> 01:00:25.320]   feature.
[01:00:25.320 --> 01:00:30.560]   So they're saying it was the original customer that didn't pay for it.
[01:00:30.560 --> 01:00:31.560]   Yeah.
[01:00:31.560 --> 01:00:33.000]   Huh.
[01:00:33.000 --> 01:00:37.880]   And what did it have ever expired for the original user?
[01:00:37.880 --> 01:00:44.880]   Or is it $8,000 worth that lasts for 24 months or something and then goes away?
[01:00:44.880 --> 01:00:46.720]   Or is it just ongoing?
[01:00:46.720 --> 01:00:49.240]   You've paid for those features, so you've got them forever.
[01:00:49.240 --> 01:00:51.880]   I have a Model 3, so I can speak to this.
[01:00:51.880 --> 01:00:54.680]   So I paid-- I did not pay for full self-driving.
[01:00:54.680 --> 01:00:57.560]   I paid for enhanced autopilot.
[01:00:57.560 --> 01:01:01.160]   And so it is a feature that you turn on.
[01:01:01.160 --> 01:01:02.360]   You can buy it later.
[01:01:02.360 --> 01:01:03.680]   Yeah, you can buy it later.
[01:01:03.680 --> 01:01:09.360]   It's more expensive than if you buy it at the time of purchase.
[01:01:09.360 --> 01:01:14.280]   But you basically add that to the price of your car and you can roll that into the loan
[01:01:14.280 --> 01:01:17.680]   or whatever what have you lease.
[01:01:17.680 --> 01:01:21.440]   And it is always available to you.
[01:01:21.440 --> 01:01:28.400]   However, I will say that Tesla has sort of changed.
[01:01:28.400 --> 01:01:32.320]   If I'm not mistaken, Tesla's sort of changed the definition of what is included in full
[01:01:32.320 --> 01:01:36.280]   self-driving and what is included in enhanced autopilot.
[01:01:36.280 --> 01:01:42.840]   So like, some in now is a-- I think it's considered more of a full self-driving feature, but before
[01:01:42.840 --> 01:01:49.560]   it was sort of a beta feature that was available with enhanced autopilot drivers, it's very
[01:01:49.560 --> 01:01:52.720]   kind of fluid, which is weird.
[01:01:52.720 --> 01:01:57.440]   But the original question is like, if you pay for it once, it's always activated and
[01:01:57.440 --> 01:01:59.960]   that seems to be the case.
[01:01:59.960 --> 01:02:04.880]   And my question would be, is can you then, if you were to sell that Model 3, and let's
[01:02:04.880 --> 01:02:08.720]   just say there's a Model 4 and you go and buy one of those, and you say, well, I want
[01:02:08.720 --> 01:02:11.440]   to move these features to this new Model.
[01:02:11.440 --> 01:02:13.960]   Oh, no, I think they'd let you do that at all.
[01:02:13.960 --> 01:02:17.240]   Because if it's like a license thing, these things are rarely transferable anyway.
[01:02:17.240 --> 01:02:18.240]   It's usually just--
[01:02:18.240 --> 01:02:20.240]   Well, that's what's going on.
[01:02:20.240 --> 01:02:22.920]   You think you're buying hardware, you're buying software.
[01:02:22.920 --> 01:02:25.080]   You're buying a computer.
[01:02:25.080 --> 01:02:28.560]   And that's where we've gotten-- and it's the same thing with Sonos.
[01:02:28.560 --> 01:02:31.920]   And that's in a way you could say this is the same thing with a Hue light bulb.
[01:02:31.920 --> 01:02:33.080]   It's why people are confused.
[01:02:33.080 --> 01:02:39.200]   I thought I bought a light bulb, but it's really a computer that has to be updated.
[01:02:39.200 --> 01:02:43.240]   I think there's my ask the tech guy talking about that Sonos thing.
[01:02:43.240 --> 01:02:44.840]   I think this is the problem.
[01:02:44.840 --> 01:02:50.480]   And by the way, I have some sympathy for software companies, especially iOS software
[01:02:50.480 --> 01:02:54.640]   companies that sell software for $5 and are expected to support it for the rest of your
[01:02:54.640 --> 01:02:55.640]   life.
[01:02:55.640 --> 01:02:56.640]   Right.
[01:02:56.640 --> 01:02:58.320]   You know, they've got to run a business.
[01:02:58.320 --> 01:03:02.360]   And so I understand why companies are like Adobe and Microsoft are moving to subscription
[01:03:02.360 --> 01:03:03.360]   models.
[01:03:03.360 --> 01:03:07.720]   But at the same time, it's a car.
[01:03:07.720 --> 01:03:08.720]   Yeah.
[01:03:08.720 --> 01:03:09.720]   It's a car.
[01:03:09.720 --> 01:03:12.160]   I mean, that's why I didn't buy full self-driving.
[01:03:12.160 --> 01:03:14.640]   Because so many people are like, are you going to get full self-driving?
[01:03:14.640 --> 01:03:18.880]   And I'm like, California, we can't even make appointments at the DMV.
[01:03:18.880 --> 01:03:22.960]   How are we going to have laws for full self-driving next five to ten years?
[01:03:22.960 --> 01:03:24.280]   That whole full self-driving thing was like not--
[01:03:24.280 --> 01:03:25.800]   It was really a weird--
[01:03:25.800 --> 01:03:28.600]   I don't want to say scam, but it was a weird thing because--
[01:03:28.600 --> 01:03:30.840]   You just kind of given a loan to Tesla, right?
[01:03:30.840 --> 01:03:35.320]   You're giving them money saying when and if you ever get this capability, I'll get it.
[01:03:35.320 --> 01:03:36.320]   I'll take it.
[01:03:36.320 --> 01:03:39.080]   But there's no-- like it's-- yeah, it's just very--
[01:03:39.080 --> 01:03:41.040]   You got nothing for it, right?
[01:03:41.040 --> 01:03:42.040]   You were paying--
[01:03:42.040 --> 01:03:43.200]   It was like a seat license.
[01:03:43.200 --> 01:03:45.640]   It was the right to get it later.
[01:03:45.640 --> 01:03:46.640]   The right to get it later.
[01:03:46.640 --> 01:03:52.360]   And if it needed hardware, which some of the older cars will need hardware upgrades, then
[01:03:52.360 --> 01:03:53.680]   you would get that as well.
[01:03:53.680 --> 01:03:56.480]   So-- but I mean, I love my Model 3.
[01:03:56.480 --> 01:03:58.440]   It's the best car I've ever owned.
[01:03:58.440 --> 01:03:59.520]   It's my favorite thing.
[01:03:59.520 --> 01:04:05.200]   I mean, someone literally tweeted me today saying they're watching us in his Model 3,
[01:04:05.200 --> 01:04:07.200]   which is hilarious.
[01:04:07.200 --> 01:04:09.560]   It's watching the show in his Model 3.
[01:04:09.560 --> 01:04:11.560]   Is that a good idea?
[01:04:11.560 --> 01:04:12.560]   Is that a good idea?
[01:04:12.560 --> 01:04:13.560]   Well, you have to be--
[01:04:13.560 --> 01:04:14.560]   You got to be a car.
[01:04:14.560 --> 01:04:18.560]   So he says he's watching us in the car while his wife shops.
[01:04:18.560 --> 01:04:19.560]   Oh, OK.
[01:04:19.560 --> 01:04:20.560]   OK.
[01:04:20.560 --> 01:04:23.640]   But yeah, like it's-- it is a great car, but on that same note, there are--
[01:04:23.640 --> 01:04:28.040]   certain things where I'm just like, I just-- I don't see the value in that.
[01:04:28.040 --> 01:04:31.120]   And I can't-- I can't bring myself-- pay for it.
[01:04:31.120 --> 01:04:36.600]   And I-- you know, maybe I'll get full self-driving when we actually have, you know, full self-driving
[01:04:36.600 --> 01:04:41.720]   laws that allow me to use those things fully, legally on the roads.
[01:04:41.720 --> 01:04:46.000]   And also, I just don't trust human drivers a lot of the time.
[01:04:46.000 --> 01:04:47.560]   So I use enhanced autopilot.
[01:04:47.560 --> 01:04:48.560]   Love autopilot.
[01:04:48.560 --> 01:04:50.560]   It's the best in LA traffic.
[01:04:50.560 --> 01:04:52.800]   You can't beat it.
[01:04:52.800 --> 01:04:57.040]   And-- but at the end of the day, I just-- I can't-- I just-- I feel bad for that guy who
[01:04:57.040 --> 01:04:58.040]   bought the car.
[01:04:58.040 --> 01:05:01.160]   And I feel like if the sticker said that it was there, Tesla should make it right and
[01:05:01.160 --> 01:05:05.840]   they should give this guy what was on the sticker, regardless of whether the original
[01:05:05.840 --> 01:05:06.840]   owner paid for it.
[01:05:06.840 --> 01:05:07.840]   Yeah.
[01:05:07.840 --> 01:05:11.320]   Well, the upshot of his bad PR for Tesla, because people-- whether Tesla's in the right
[01:05:11.320 --> 01:05:15.760]   or not have it in their mind, now that Tesla can revoke a feature at will.
[01:05:15.760 --> 01:05:16.760]   Right.
[01:05:16.760 --> 01:05:18.720]   And that's not a good message.
[01:05:18.720 --> 01:05:19.720]   Mm-hmm.
[01:05:19.720 --> 01:05:20.720]   Yeah.
[01:05:20.720 --> 01:05:21.720]   Take a break.
[01:05:21.720 --> 01:05:23.640]   Ashley Aschetha is here.
[01:05:23.640 --> 01:05:28.160]   She is, of course, senior editor at CNET, senior producer at CNET at Ashley Aschetha
[01:05:28.160 --> 01:05:30.480]   on the Twitter, you figure out a spelling.
[01:05:30.480 --> 01:05:36.600]   It took me literally four or five appearances before I could pronounce it properly.
[01:05:36.600 --> 01:05:37.600]   It's ESQEDA.
[01:05:37.600 --> 01:05:38.600]   ESQ-U-E-D-A.
[01:05:38.600 --> 01:05:39.600]   See?
[01:05:39.600 --> 01:05:40.600]   I still don't know.
[01:05:40.600 --> 01:05:41.600]   All right.
[01:05:41.600 --> 01:05:42.600]   2020 is going to be your year.
[01:05:42.600 --> 01:05:44.600]   It's going to be the year.
[01:05:44.600 --> 01:05:49.520]   I finally pronounce Ashley's name properly.
[01:05:49.520 --> 01:05:52.400]   So with this, Denise Howell, nice Anglo-Saxon name.
[01:05:52.400 --> 01:05:53.760]   I can pronounce that.
[01:05:53.760 --> 01:05:55.560]   Denise Howell.info is her website.
[01:05:55.560 --> 01:05:56.760]   Great to see you, Denise.
[01:05:56.760 --> 01:05:58.200]   Great to see you too.
[01:05:58.200 --> 01:06:00.960]   Yeah, I miss having you around here.
[01:06:00.960 --> 01:06:05.920]   And also with us from the UK, Nate Langson, from Bloomberg.
[01:06:05.920 --> 01:06:08.320]   He's tech editor over there, UK TechShow.
[01:06:08.320 --> 01:06:15.520]   He does a podcast with literally, I think, the best podcast name ever, text message,
[01:06:15.520 --> 01:06:17.600]   T-E-C-H apostrophe-S, text message.
[01:06:17.600 --> 01:06:18.600]   I love it.
[01:06:18.600 --> 01:06:19.600]   I am very fluttered.
[01:06:19.600 --> 01:06:22.360]   That's such a good name.
[01:06:22.360 --> 01:06:23.360]   Thank you.
[01:06:23.360 --> 01:06:24.360]   It makes more sense when it's written down.
[01:06:24.360 --> 01:06:29.520]   That's why the website is UK TechShow.com because I got fed up of saying text message,
[01:06:29.520 --> 01:06:31.080]   T-E-C-H apostrophe-S message.
[01:06:31.080 --> 01:06:32.560]   And they go, "Oh, right.
[01:06:32.560 --> 01:06:33.560]   Oh, I see."
[01:06:33.560 --> 01:06:37.680]   Oh, that's clever, which is why the website is different from the show name.
[01:06:37.680 --> 01:06:38.680]   It's for that reason.
[01:06:38.680 --> 01:06:39.960]   But I appreciate the compliment.
[01:06:39.960 --> 01:06:44.040]   It's less literal than this week in tech, which is, I mean, it does what it says on
[01:06:44.040 --> 01:06:45.840]   the tin.
[01:06:45.840 --> 01:06:48.560]   But I don't know why I'm talking--
[01:06:48.560 --> 01:06:49.560]   I like it, text message.
[01:06:49.560 --> 01:06:52.360]   It's what we talk about on the show is text message.
[01:06:52.360 --> 01:06:54.560]   And I think it's a good-- it's a nice pun.
[01:06:54.560 --> 01:06:55.560]   I like it.
[01:06:55.560 --> 01:06:56.560]   Thank you.
[01:06:56.560 --> 01:07:05.200]   Our show today is brought to you by Deadpool's cell phone company, Mint Mobile.
[01:07:05.200 --> 01:07:09.480]   Honest to God, look at the website, mintmobile.com/twit.
[01:07:09.480 --> 01:07:12.080]   Right there, there he is.
[01:07:12.080 --> 01:07:15.160]   Ryan Reynolds, he is apparently an owner of Mint Mobile.
[01:07:15.160 --> 01:07:17.440]   Oh, there's a lot of other reasons I love Mint Mobile.
[01:07:17.440 --> 01:07:23.640]   Besides the Minty Green Fox, Mint Mobile is the same premium network coverage you're
[01:07:23.640 --> 01:07:27.120]   used to at a fraction of the cost.
[01:07:27.120 --> 01:07:28.120]   Because everything's online.
[01:07:28.120 --> 01:07:29.120]   They don't have stores.
[01:07:29.120 --> 01:07:30.640]   They don't have to worry about it.
[01:07:30.640 --> 01:07:35.320]   Mint Mobile is what we call an MVNO, mobile virtual network operator.
[01:07:35.320 --> 01:07:39.240]   If you get great T-Mobile service in your neck of the woods, you'll get the same service
[01:07:39.240 --> 01:07:40.880]   for Mint Mobile.
[01:07:40.880 --> 01:07:43.480]   I pay $25 a month for Mint Mobile.
[01:07:43.480 --> 01:07:45.800]   And I'm getting there top of the line service.
[01:07:45.800 --> 01:07:49.680]   With Mint Mobile, you'll stop paying for unlimited data you never use.
[01:07:49.680 --> 01:07:55.000]   Plans come with three, eight, or 12 gigabytes a month of 4G LTE data.
[01:07:55.000 --> 01:07:58.840]   The $15 a month plan is the one to start with, that three month introductory plan.
[01:07:58.840 --> 01:08:00.400]   $15 a month.
[01:08:00.400 --> 01:08:04.160]   And every plan comes with unlimited nationwide talk and text.
[01:08:04.160 --> 01:08:05.160]   It is such a relief.
[01:08:05.160 --> 01:08:08.560]   You can port your number over, which means you don't even have to change numbers.
[01:08:08.560 --> 01:08:10.720]   You can use your existing phone, put the sim in.
[01:08:10.720 --> 01:08:12.480]   They'll mail you the sim for free.
[01:08:12.480 --> 01:08:14.600]   Or you can get a phone from them.
[01:08:14.600 --> 01:08:18.080]   It works with any unlocked GSM compatible device.
[01:08:18.080 --> 01:08:20.520]   I have to say it is fantastic.
[01:08:20.520 --> 01:08:24.600]   What they're saving on retail locations, they're passing on to you.
[01:08:24.600 --> 01:08:27.640]   I liked it so much after I did the introductory plan.
[01:08:27.640 --> 01:08:31.400]   I went for the 12 gigabyte a month yearly plan.
[01:08:31.400 --> 01:08:34.440]   I paid $300, paid for the whole year ahead of time.
[01:08:34.440 --> 01:08:36.240]   That means $25 a month.
[01:08:36.240 --> 01:08:37.480]   12 gigabytes a month.
[01:08:37.480 --> 01:08:39.120]   I never go through all of that.
[01:08:39.120 --> 01:08:42.440]   With all the streaming, with everything I do, never get through that.
[01:08:42.440 --> 01:08:44.840]   And nationwide talk, unlimited text.
[01:08:44.840 --> 01:08:46.560]   $25.
[01:08:46.560 --> 01:08:48.040]   Why would you do anything else?
[01:08:48.040 --> 01:08:49.280]   Ditch your wireless bill.
[01:08:49.280 --> 01:08:51.520]   Start saving with Mint Mobile.
[01:08:51.520 --> 01:08:56.160]   To get your new wireless plan, just $15 a month with your three month introductory plan.
[01:08:56.160 --> 01:08:57.840]   And get the plan shipped to your door for free.
[01:08:57.840 --> 01:09:00.240]   No Ryan Reynolds will not deliver it to you.
[01:09:00.240 --> 01:09:01.240]   Sorry.
[01:09:01.240 --> 01:09:04.040]   Mint Mobile.com/Twit.
[01:09:04.040 --> 01:09:10.000]   Although now that I think about it, I realize the Mint Mobile Fox looks just like Ryan Reynolds.
[01:09:10.000 --> 01:09:13.120]   See there's a little resemblance there.
[01:09:13.120 --> 01:09:17.120]   Mintmobile.com/Twit.
[01:09:17.120 --> 01:09:19.920]   Thank you so much, Mint Mobile, for supporting our show.
[01:09:19.920 --> 01:09:21.880]   And thank you for supporting it by going to that address.
[01:09:21.880 --> 01:09:26.800]   Mintmobile.com/Twit.
[01:09:26.800 --> 01:09:28.040]   Big reorg.
[01:09:28.040 --> 01:09:32.080]   This is one of those stories that is not sexy.
[01:09:32.080 --> 01:09:34.080]   But we got to talk about it because it's big news.
[01:09:34.080 --> 01:09:35.480]   Big reorg at Microsoft.
[01:09:35.480 --> 01:09:39.200]   They do this every year.
[01:09:39.200 --> 01:09:44.720]   This year they took their hardware guy, chief product officer, Panos Panay.
[01:09:44.720 --> 01:09:45.880]   He's the guy you see.
[01:09:45.880 --> 01:09:50.680]   I'm sure Ashley you've been to a bunch of these events with Panos showing off surface.
[01:09:50.680 --> 01:09:54.440]   And he's got this certain way about him.
[01:09:54.440 --> 01:09:55.440]   Yeah.
[01:09:55.440 --> 01:09:56.440]   Yeah.
[01:09:56.440 --> 01:09:58.520]   How would you describe Panos Panay?
[01:09:58.520 --> 01:10:03.080]   I mean, I think he's very cool.
[01:10:03.080 --> 01:10:04.080]   He's enthusiastic.
[01:10:04.080 --> 01:10:05.080]   Yeah.
[01:10:05.080 --> 01:10:11.120]   He's like, he really, I feel like his enthusiasm is very infectious.
[01:10:11.120 --> 01:10:12.120]   Some people.
[01:10:12.120 --> 01:10:13.120]   Like people.
[01:10:13.120 --> 01:10:18.880]   He talks about a product I get, like I get excited about it, even though I don't necessarily
[01:10:18.880 --> 01:10:21.600]   have a lot of Windows products or Microsoft products.
[01:10:21.600 --> 01:10:24.080]   Makes you want them.
[01:10:24.080 --> 01:10:25.280]   You want them when he talks about them.
[01:10:25.280 --> 01:10:27.400]   You're just like, man, this guy's like pumped.
[01:10:27.400 --> 01:10:28.400]   And he's just cool.
[01:10:28.400 --> 01:10:33.800]   Like he seems, I've never spoken to him or interviewed him, but he just seems cool.
[01:10:33.800 --> 01:10:37.400]   Like, kind of like a Craig Federighi type, like an apple.
[01:10:37.400 --> 01:10:38.400]   Yeah.
[01:10:38.400 --> 01:10:39.400]   Cool guy.
[01:10:39.400 --> 01:10:40.400]   Yeah.
[01:10:40.400 --> 01:10:46.160]   Well, there was a rumor that Panos, actually was a rumor he might be going to Apple.
[01:10:46.160 --> 01:10:50.840]   I don't know how credible that rumor was, but there was a rumor that Panos was leaving
[01:10:50.840 --> 01:10:52.800]   Microsoft.
[01:10:52.800 --> 01:10:58.960]   And that might explain why he has gotten, I think is kind of a promotion.
[01:10:58.960 --> 01:11:03.920]   He has now, he will no longer be just hardware.
[01:11:03.920 --> 01:11:06.280]   He's going to be software.
[01:11:06.280 --> 01:11:14.400]   He is now running Microsoft's OXO, their experiences and devices unit, which means, well, I guess
[01:11:14.400 --> 01:11:16.400]   it's a promotion, right?
[01:11:16.400 --> 01:11:17.400]   I guess so.
[01:11:17.400 --> 01:11:18.400]   Sounds like it.
[01:11:18.400 --> 01:11:19.400]   Yeah.
[01:11:19.400 --> 01:11:22.900]   I mean, is this one of those situations where you kind of like, kind of like leak that news
[01:11:22.900 --> 01:11:26.440]   in the hopes that they'll like give you more stuff to do where you're like, oh, I heard
[01:11:26.440 --> 01:11:34.200]   a rumor that Panos is going to leave like actors when they, when they, when their reps
[01:11:34.200 --> 01:11:39.360]   drop a rumor that they're being considered for a part in the hopes that they'll, it'll
[01:11:39.360 --> 01:11:42.200]   push the casting director over the edge and cast them.
[01:11:42.200 --> 01:11:43.880]   It's a long standing technique.
[01:11:43.880 --> 01:11:45.320]   It happens in every industry.
[01:11:45.320 --> 01:11:46.320]   Yeah.
[01:11:46.320 --> 01:11:50.000]   I've got a better offer.
[01:11:50.000 --> 01:11:56.400]   The weirdest thing is before he takes over, he won't be taking over until this fall,
[01:11:56.400 --> 01:12:02.120]   because next month he and his family are going to do a semester at sea.
[01:12:02.120 --> 01:12:05.800]   He's going to take the summer off and get on a boat.
[01:12:05.800 --> 01:12:06.720]   Okay.
[01:12:06.720 --> 01:12:11.120]   So, I think Joe B was going to do now.
[01:12:11.120 --> 01:12:12.120]   Huh?
[01:12:12.120 --> 01:12:13.920]   I thought that's what Joe B was going to do now.
[01:12:13.920 --> 01:12:15.160]   Oh, is that Joe B?
[01:12:15.160 --> 01:12:16.160]   Yeah.
[01:12:16.160 --> 01:12:17.160]   Oh, I'm confused.
[01:12:17.160 --> 01:12:18.160]   Okay.
[01:12:18.160 --> 01:12:19.600]   So I'm, I'm sorry.
[01:12:19.600 --> 01:12:20.600]   You're right.
[01:12:20.600 --> 01:12:21.600]   Thank you for correcting me.
[01:12:21.600 --> 01:12:24.360]   Carson Bondi, always on top of the stories.
[01:12:24.360 --> 01:12:25.560]   I'm confused because honestly.
[01:12:25.560 --> 01:12:30.320]   Joe B is Joe Bell, Joe Bell Fiore, who's the guy with the Beatles haircut.
[01:12:30.320 --> 01:12:31.320]   Yeah.
[01:12:31.320 --> 01:12:36.120]   And another enthusiastic, Microsoft part is in.
[01:12:36.120 --> 01:12:39.800]   He is, he's currently heading the Windows experience business.
[01:12:39.800 --> 01:12:43.400]   He's going to move to the office side of the house, but before he does that, he's going
[01:12:43.400 --> 01:12:49.040]   to see for a few months, which he did last year too.
[01:12:49.040 --> 01:12:52.900]   And then Panos Panay, I guess he's going to go right to work.
[01:12:52.900 --> 01:12:53.900]   It's very confusing.
[01:12:53.900 --> 01:12:54.900]   Personally, he wrote.
[01:12:54.900 --> 01:12:57.240]   I'm very excited to lead the Windows client for Microsoft.
[01:12:57.240 --> 01:13:01.960]   Doesn't that say the Windows client for Microsoft?
[01:13:01.960 --> 01:13:04.720]   This sounds like a demotion for Windows.
[01:13:04.720 --> 01:13:07.800]   It's just the Windows client.
[01:13:07.800 --> 01:13:09.800]   Kind of downplaying it, right?
[01:13:09.800 --> 01:13:10.800]   Yeah.
[01:13:10.800 --> 01:13:12.300]   I'm excited to lead Windows.
[01:13:12.300 --> 01:13:16.080]   This used to be, yeah, this used to be the big product.
[01:13:16.080 --> 01:13:17.440]   Yeah, that was it.
[01:13:17.440 --> 01:13:18.440]   It's not anymore.
[01:13:18.440 --> 01:13:20.240]   It's just a, it's just a client.
[01:13:20.240 --> 01:13:22.160]   Wow.
[01:13:22.160 --> 01:13:25.940]   We will make the Windows client experience better for the entire PC ecosystem, designing
[01:13:25.940 --> 01:13:27.500]   hardware and software together.
[01:13:27.500 --> 01:13:30.980]   We'll enable us to do a better job on our long-term Windows bets.
[01:13:30.980 --> 01:13:34.940]   It is kind of interesting because for a long time, it felt like Microsoft's hardware, the
[01:13:34.940 --> 01:13:38.700]   Surface line, which was by the way, the first time Microsoft's ever made computers, they
[01:13:38.700 --> 01:13:42.500]   started that a few years ago, was kind of a hobby.
[01:13:42.500 --> 01:13:47.020]   Was there nowhere anywhere near the size of the big PC manufacturers like HP, Dell and
[01:13:47.020 --> 01:13:49.940]   Lenovo?
[01:13:49.940 --> 01:13:51.700]   But now they're tying it to Windows.
[01:13:51.700 --> 01:13:52.700]   So that's my question.
[01:13:52.700 --> 01:13:58.920]   Is this a boost for hardware or a demotion for Windows?
[01:13:58.920 --> 01:13:59.920]   Nobody cares.
[01:13:59.920 --> 01:14:00.920]   It's corporate insiders.
[01:14:00.920 --> 01:14:02.220]   Well, that's a little both, right?
[01:14:02.220 --> 01:14:03.640]   It's like good, good.
[01:14:03.640 --> 01:14:05.960]   It's always good to have teams communicating with each other.
[01:14:05.960 --> 01:14:09.760]   And if you can do that by putting one person in charge of two different teams, this is
[01:14:09.760 --> 01:14:10.760]   a good thing.
[01:14:10.760 --> 01:14:17.120]   It's only going to make those vertically integrated Windows Microsoft products better.
[01:14:17.120 --> 01:14:23.680]   I'll tell you what it looks like to me, the continuing process of Microsoft, of deprecating
[01:14:23.680 --> 01:14:29.240]   anything that's not Azure, that really the future for Microsoft is the cloud.
[01:14:29.240 --> 01:14:34.160]   And what such an Adella, CEO, has said time and time again is we don't care what our customers
[01:14:34.160 --> 01:14:39.200]   use as long as they use our cloud.
[01:14:39.200 --> 01:14:43.480]   And I honestly think that that's more of this, that they're really doubling down on
[01:14:43.480 --> 01:14:44.480]   the cloud.
[01:14:44.480 --> 01:14:47.320]   And to me, it's a demotion for Windows.
[01:14:47.320 --> 01:14:50.520]   It's saying, yeah, it's just one more thing that we do.
[01:14:50.520 --> 01:14:56.120]   And they used to even say the best experience of the Microsoft cloud will be with Microsoft
[01:14:56.120 --> 01:14:57.120]   products.
[01:14:57.120 --> 01:14:59.120]   They don't even say that anymore.
[01:14:59.120 --> 01:15:00.120]   Wow.
[01:15:00.120 --> 01:15:02.120]   All right.
[01:15:02.120 --> 01:15:03.120]   Nobody cares about Windows.
[01:15:03.120 --> 01:15:06.240]   I don't know why I bring this up.
[01:15:06.240 --> 01:15:07.240]   Used to be the...
[01:15:07.240 --> 01:15:08.240]   I just...
[01:15:08.240 --> 01:15:13.960]   Leo, how long do you think that Windows is going to be this thing that powers the corporate
[01:15:13.960 --> 01:15:14.960]   world?
[01:15:14.960 --> 01:15:15.960]   Yeah, that's a good question.
[01:15:15.960 --> 01:15:16.960]   Yeah.
[01:15:16.960 --> 01:15:19.560]   Honestly, I think the future is not in Windows or Mac.
[01:15:19.560 --> 01:15:23.120]   It's in thin clients that will...
[01:15:23.120 --> 01:15:25.400]   The cloud is where it's at, right?
[01:15:25.400 --> 01:15:27.680]   All your stuff is in the cloud.
[01:15:27.680 --> 01:15:30.080]   All your programs are running in the cloud.
[01:15:30.080 --> 01:15:35.200]   We went from running QuickBooks on a machine to running QuickBooks online.
[01:15:35.200 --> 01:15:36.200]   I think we're not alone.
[01:15:36.200 --> 01:15:40.680]   I think most corporations do that now, which means your software isn't running on your
[01:15:40.680 --> 01:15:41.680]   computer.
[01:15:41.680 --> 01:15:43.760]   It's running in the cloud, which means it doesn't matter whether you're using Windows
[01:15:43.760 --> 01:15:45.480]   Mac and iPad.
[01:15:45.480 --> 01:15:47.400]   Android doesn't matter.
[01:15:47.400 --> 01:15:49.040]   Gamings, I'll move to that too.
[01:15:49.040 --> 01:15:52.600]   Gamings with Microsoft has their XCloud, which is going to be cloud gaming.
[01:15:52.600 --> 01:15:54.600]   Google announced Stadia.
[01:15:54.600 --> 01:15:55.600]   Sony has...
[01:15:55.600 --> 01:15:56.600]   Yeah.
[01:15:56.600 --> 01:15:57.600]   What does Sony call that?
[01:15:57.600 --> 01:15:58.600]   They bought Geikai.
[01:15:58.600 --> 01:15:59.600]   What do they call it?
[01:15:59.600 --> 01:16:00.600]   Well, they have PlayStation Now.
[01:16:00.600 --> 01:16:01.600]   It's just PlayStation Now.
[01:16:01.600 --> 01:16:02.600]   Yeah, PlayStation Now.
[01:16:02.600 --> 01:16:06.520]   And then NVIDIA just officially launched their...
[01:16:06.520 --> 01:16:07.520]   GeForce Now.
[01:16:07.520 --> 01:16:08.520]   Their cloud service.
[01:16:08.520 --> 01:16:11.520]   Yeah, GeForce Now, which is apparently very good.
[01:16:11.520 --> 01:16:14.000]   I've been using it for a long time.
[01:16:14.000 --> 01:16:17.680]   I have an NVIDIA Shield, and I didn't want to buy No Man's Sky.
[01:16:17.680 --> 01:16:20.640]   I was playing No Man's Sky on the GeForce Now.
[01:16:20.640 --> 01:16:21.640]   It's out of beta.
[01:16:21.640 --> 01:16:22.640]   That's great.
[01:16:22.640 --> 01:16:25.400]   It's $5 a month.
[01:16:25.400 --> 01:16:30.400]   And my experience, if you have decent bandwidth, it's very easy, very good, fast.
[01:16:30.400 --> 01:16:33.680]   I mean, so that's an interesting question.
[01:16:33.680 --> 01:16:35.120]   Are they going to talk about that at E3?
[01:16:35.120 --> 01:16:38.240]   Is it the end of console gaming?
[01:16:38.240 --> 01:16:40.520]   So it's definitely not the end of console gaming.
[01:16:40.520 --> 01:16:43.880]   I think there are lots of arguments to be made for physical media.
[01:16:43.880 --> 01:16:47.640]   So for example, if you remember when the Xbox One was originally announced, there was
[01:16:47.640 --> 01:16:55.280]   this big uproar about this online check that the Xbox One was going to do every 24 hours
[01:16:55.280 --> 01:16:56.280]   or so.
[01:16:56.280 --> 01:16:59.040]   Some Microsoft executive soon demoted, said it.
[01:16:59.040 --> 01:17:01.080]   You wouldn't be able to use it a submarine.
[01:17:01.080 --> 01:17:02.080]   Right.
[01:17:02.080 --> 01:17:07.200]   And that made some very important people very mad, which is the troops who often are in
[01:17:07.200 --> 01:17:13.640]   places where they would like to enjoy maybe some leisure time at any point and could
[01:17:13.640 --> 01:17:16.000]   not do so with an Xbox One.
[01:17:16.000 --> 01:17:17.000]   So things like that.
[01:17:17.000 --> 01:17:20.360]   You know, there are just some people who don't have that connectivity.
[01:17:20.360 --> 01:17:27.440]   There are lots of places in America, even that don't have really good internet service
[01:17:27.440 --> 01:17:28.960]   still, which is kind of amazing.
[01:17:28.960 --> 01:17:30.880]   I mean, we have a huge country though.
[01:17:30.880 --> 01:17:36.880]   So it is very difficult to blanket that in super fast ISPs.
[01:17:36.880 --> 01:17:44.600]   And so I think we're not quite at consoles or dead, but I do think that there's continually
[01:17:44.600 --> 01:17:52.560]   going to be this push for more and more cloud, more and more digital downloads, online gaming.
[01:17:52.560 --> 01:17:58.120]   It will also be very interesting to see if not this console generation because they have
[01:17:58.120 --> 01:18:03.080]   to start working on them for years prior to launch.
[01:18:03.080 --> 01:18:09.640]   Next console generation, what that looks like in the way of access to games.
[01:18:09.640 --> 01:18:13.480]   Maybe it's not, maybe you don't own any games.
[01:18:13.480 --> 01:18:18.040]   Maybe it's just you do something like Xbox Game Pass where you pay a certain amount of
[01:18:18.040 --> 01:18:19.040]   money.
[01:18:19.040 --> 01:18:24.000]   Maybe it's 60 bucks every two months as opposed to buying one 60 dollar game every month and
[01:18:24.000 --> 01:18:27.480]   you get all the new releases that month or something like that.
[01:18:27.480 --> 01:18:28.480]   Who knows?
[01:18:28.480 --> 01:18:31.600]   Gaming is in a very interesting place right now, especially console gaming.
[01:18:31.600 --> 01:18:37.920]   So it will be very interesting to see how all of the console wars play out, especially
[01:18:37.920 --> 01:18:45.320]   now that Microsoft is making what amounts to, I mean to me anyway, a mini ITX machine.
[01:18:45.320 --> 01:18:47.200]   It's a little computer.
[01:18:47.200 --> 01:18:49.120]   So it's not really a console.
[01:18:49.120 --> 01:18:50.440]   It's a tiny computer.
[01:18:50.440 --> 01:18:53.120]   But would anybody use it for anything besides games?
[01:18:53.120 --> 01:18:57.600]   Didn't Microsoft kind of lose its way with the original Xbox One by saying, oh, it's
[01:18:57.600 --> 01:18:58.760]   a TV?
[01:18:58.760 --> 01:18:59.760]   Was that the one?
[01:18:59.760 --> 01:19:04.440]   It leaned into this, it's an entertainment center for your living room.
[01:19:04.440 --> 01:19:14.680]   But I am very curious how Microsoft separates having your own PC and having an Xbox Series
[01:19:14.680 --> 01:19:20.200]   X because in essence now that they have cross purchase and cross play.
[01:19:20.200 --> 01:19:25.520]   So if you buy Halo on PC, you can play it on your Xbox and vice versa.
[01:19:25.520 --> 01:19:29.480]   So if you already have a PC, what is your incentive to get an Xbox?
[01:19:29.480 --> 01:19:33.080]   Like I want to know what the answer to that question is and I hope they answer it and
[01:19:33.080 --> 01:19:38.200]   probably fingers crossed they will at E3.
[01:19:38.200 --> 01:19:41.320]   Kind of in a fit turnabout is fair play.
[01:19:41.320 --> 01:19:47.300]   Last year the Navy launched the USS Colorado, its newest nuclear powered attack submarine
[01:19:47.300 --> 01:19:50.000]   with Xbox controllers.
[01:19:50.000 --> 01:19:55.040]   Which makes sense because I bet you they get a lot of sailors who really can play a great
[01:19:55.040 --> 01:19:56.040]   Xbox game.
[01:19:56.040 --> 01:19:57.040]   Of course.
[01:19:57.040 --> 01:19:59.000]   They're really comfortable controllers.
[01:19:59.000 --> 01:20:00.000]   That's right.
[01:20:00.000 --> 01:20:05.840]   It's the first attack submarine where sailors use an Xbox controller to maneuver the photonics
[01:20:05.840 --> 01:20:09.400]   masks which replaced periscopes.
[01:20:09.400 --> 01:20:14.920]   Other submarines have joysticks using commercial off the shelf technology saves money and young
[01:20:14.920 --> 01:20:18.920]   sailors report to the submarine knowing how to use it.
[01:20:18.920 --> 01:20:22.520]   I mean, you just already know how to use the controller.
[01:20:22.520 --> 01:20:23.840]   It's inherently built into you.
[01:20:23.840 --> 01:20:27.040]   If you've ever played a video game, it's like, oh, here's controller.
[01:20:27.040 --> 01:20:28.040]   You know what that is.
[01:20:28.040 --> 01:20:29.040]   You don't have to be trained.
[01:20:29.040 --> 01:20:34.280]   Did anyone else just go straight to enders game in their head?
[01:20:34.280 --> 01:20:35.280]   Yes.
[01:20:35.280 --> 01:20:36.280]   Okay.
[01:20:36.280 --> 01:20:39.080]   So how do I destroy the enemy press Y X?
[01:20:39.080 --> 01:20:43.920]   Wow, that's pretty wild.
[01:20:43.920 --> 01:20:46.480]   So that's the US Navy's response to you.
[01:20:46.480 --> 01:20:49.840]   You can't use an Xbox on a submarine.
[01:20:49.840 --> 01:20:51.600]   Hold my beer.
[01:20:51.600 --> 01:20:53.600]   Yeah.
[01:20:53.600 --> 01:20:58.880]   Do you think Microsoft used to make a version of Windows XP for submarines specifically?
[01:20:58.880 --> 01:20:59.880]   Or did I make that up?
[01:20:59.880 --> 01:21:00.880]   That sounds right.
[01:21:00.880 --> 01:21:04.720]   Someone was like, Windows XP for submarines or something.
[01:21:04.720 --> 01:21:07.520]   Some of those submarines are probably still using Windows XP.
[01:21:07.520 --> 01:21:08.520]   You're probably not right.
[01:21:08.520 --> 01:21:09.680]   That's probably true.
[01:21:09.680 --> 01:21:15.760]   By the way, what was it last month, January 14th, Windows 7, end of life, right?
[01:21:15.760 --> 01:21:17.880]   Microsoft does the last patch Tuesday.
[01:21:17.880 --> 01:21:19.120]   They say that's it.
[01:21:19.120 --> 01:21:21.800]   No more patches were done.
[01:21:21.800 --> 01:21:27.000]   One week later, turns out if you use, was it stretched to fit for your wallpaper?
[01:21:27.000 --> 01:21:28.000]   It turns black.
[01:21:28.000 --> 01:21:29.000]   Oh, well, okay.
[01:21:29.000 --> 01:21:30.760]   We'll fix that.
[01:21:30.760 --> 01:21:34.880]   Now there's a new one, a new Windows 7 bugs cropping up.
[01:21:34.880 --> 01:21:39.600]   I saw this on Reddit, a bunch of people saying, I try to shut down Windows 7.
[01:21:39.600 --> 01:21:42.600]   It says, you don't have permission to shut down.
[01:21:42.600 --> 01:21:45.920]   It's a pop up up.
[01:21:45.920 --> 01:21:49.360]   You can't shut it down.
[01:21:49.360 --> 01:21:53.720]   No word yet if Microsoft will put out one more patch.
[01:21:53.720 --> 01:21:56.640]   When they said the end of life for Windows 7, I said, that's okay because they've been
[01:21:56.640 --> 01:21:58.800]   patching it for 10 years.
[01:21:58.800 --> 01:22:01.240]   It's probably pretty solid.
[01:22:01.240 --> 01:22:03.240]   What was I wrong?
[01:22:03.240 --> 01:22:04.240]   Holy cow.
[01:22:04.240 --> 01:22:05.920]   Don't have permission.
[01:22:05.920 --> 01:22:08.080]   Microsoft will never let you shut down Windows.
[01:22:08.080 --> 01:22:09.080]   Sorry.
[01:22:09.080 --> 01:22:10.800]   You cannot shut it down.
[01:22:10.800 --> 01:22:13.440]   This is 100% a 2001 space.
[01:22:13.440 --> 01:22:14.440]   I was easy to trade.
[01:22:14.440 --> 01:22:15.440]   I'm sorry.
[01:22:15.440 --> 01:22:16.440]   I can't do that.
[01:22:16.440 --> 01:22:17.440]   Sorry, Dave.
[01:22:17.440 --> 01:22:18.440]   I can't shut down.
[01:22:18.440 --> 01:22:24.360]   The people that make those power cables that go with the three pins that you use in your
[01:22:24.360 --> 01:22:29.440]   old catals and things, they're just sitting back thinking, this is great for us.
[01:22:29.440 --> 01:22:32.200]   This is great.
[01:22:32.200 --> 01:22:34.840]   Yeah, I mean, I don't know what to say.
[01:22:34.840 --> 01:22:36.400]   I guess Microsoft will fix it.
[01:22:36.400 --> 01:22:37.880]   I don't know.
[01:22:37.880 --> 01:22:39.360]   I don't know.
[01:22:39.360 --> 01:22:40.880]   At what point did they just go, I'm sorry.
[01:22:40.880 --> 01:22:41.880]   We said last up.
[01:22:41.880 --> 01:22:42.880]   I told you.
[01:22:42.880 --> 01:22:43.880]   No.
[01:22:43.880 --> 01:22:44.880]   No.
[01:22:44.880 --> 01:22:45.880]   At what point does that happen?
[01:22:45.880 --> 01:22:47.520]   I'm just so curious about that.
[01:22:47.520 --> 01:22:50.920]   It's like you were talking about with app developers and stuff.
[01:22:50.920 --> 01:22:55.280]   It's like you pay your money and people expect this stuff to be updated forever and then
[01:22:55.280 --> 01:22:57.680]   they get mad when it gets end to life.
[01:22:57.680 --> 01:22:58.680]   Yeah.
[01:22:58.680 --> 01:23:01.680]   But at what point did they say, no, we got to draw a line here and like no more updates.
[01:23:01.680 --> 01:23:03.280]   Well, you could still get an update.
[01:23:03.280 --> 01:23:06.920]   You just have to pay for it a lot.
[01:23:06.920 --> 01:23:09.120]   And I love this.
[01:23:09.120 --> 01:23:11.040]   It doubles every year.
[01:23:11.040 --> 01:23:14.080]   So it starts out a relatively reasonable amount.
[01:23:14.080 --> 01:23:15.080]   I can't remember what it is.
[01:23:15.080 --> 01:23:16.120]   It's hundreds of dollars.
[01:23:16.120 --> 01:23:18.160]   And then it doubles and doubles again.
[01:23:18.160 --> 01:23:23.000]   And as everybody knows, that means in a few years it's an infinite amount of money.
[01:23:23.000 --> 01:23:24.000]   Yeah.
[01:23:24.000 --> 01:23:25.000]   Yeah.
[01:23:25.000 --> 01:23:26.000]   It's changing.
[01:23:26.000 --> 01:23:30.080]   I showed it a brought to you by speaking of an infinite amount of money.
[01:23:30.080 --> 01:23:32.080]   It's time for another ad.
[01:23:32.080 --> 01:23:37.440]   Actually, you know, before I do the ad, we should show the very nice little small film.
[01:23:37.440 --> 01:23:42.840]   I know it's Academy Awards night and I was hoping we could get this in before the awards,
[01:23:42.840 --> 01:23:48.960]   maybe make it eligible for something best promo for a podcast network watch.
[01:23:48.960 --> 01:23:50.640]   Preach on to it.
[01:23:50.640 --> 01:23:54.800]   I was at the New York Times travel show and someone came up to me and said, I love listening
[01:23:54.800 --> 01:23:57.560]   to you on Leo LaPorte and Leo LaPorte makes tech sexy.
[01:23:57.560 --> 01:23:59.440]   I was like, see, that's the problem.
[01:23:59.440 --> 01:24:05.520]   I got into this for the groupies, but they're all middle aged men tech news weekly and video
[01:24:05.520 --> 01:24:10.560]   is finally opening the doors to the public for its own cloud gaming service called GeForce.
[01:24:10.560 --> 01:24:12.040]   Now you've probably heard of it.
[01:24:12.040 --> 01:24:17.240]   GeForce now, they say, not only can you try this thing for free right now for an hour
[01:24:17.240 --> 01:24:23.640]   at a time, pay just $5 a month to basically bring a vast collection of your existing PC
[01:24:23.640 --> 01:24:27.320]   games, whether you bought them on Steam, whether you bought them on Apple.
[01:24:27.320 --> 01:24:28.320]   Windows weekly.
[01:24:28.320 --> 01:24:33.960]   As the show began this morning, big news out of Microsoft, the annual reorg has happened.
[01:24:33.960 --> 01:24:38.240]   Panos now is going to head up Windows and devices.
[01:24:38.240 --> 01:24:45.640]   And Joe B, who has been running Windows experiences on the client, is going to move to office.
[01:24:45.640 --> 01:24:46.840]   Security now.
[01:24:46.840 --> 01:24:52.480]   Richard Stalman's free software foundation has asked Microsoft, and this is a term I was
[01:24:52.480 --> 01:24:59.760]   not familiar with, Leo, to upcycle Windows 7 by releasing it to the public.
[01:24:59.760 --> 01:25:03.520]   To who's a house of laughter from Reddit.
[01:25:03.520 --> 01:25:04.520]   Exactly.
[01:25:04.520 --> 01:25:05.520]   To it.
[01:25:05.520 --> 01:25:06.520]   We read the tech news.
[01:25:06.520 --> 01:25:10.680]   Until you don't have to.
[01:25:10.680 --> 01:25:14.120]   Our show today brought to you by Epson, the amazing Epson Eco Tank printers.
[01:25:14.120 --> 01:25:15.640]   Epson deserves a lot of credit.
[01:25:15.640 --> 01:25:21.200]   If you want to talk about courage in the tech industry, the courage to say goodbye to ink
[01:25:21.200 --> 01:25:25.920]   cartridges in the printer industry, that's unheard of.
[01:25:25.920 --> 01:25:30.000]   Thank goodness the Epson Eco Tank printer became available all over the world.
[01:25:30.000 --> 01:25:32.360]   You can now kiss expensive ink cartridges.
[01:25:32.360 --> 01:25:33.360]   Goodbye.
[01:25:33.360 --> 01:25:40.120]   I'm going to go to the store in 2022 to get more ink.
[01:25:40.120 --> 01:25:42.080]   By the way, really nice bottles.
[01:25:42.080 --> 01:25:43.520]   They're color coded.
[01:25:43.520 --> 01:25:44.520]   They're keyed.
[01:25:44.520 --> 01:25:46.040]   You can't put them in the wrong port.
[01:25:46.040 --> 01:25:47.040]   You can't spill them.
[01:25:47.040 --> 01:25:48.400]   They're very easy to do.
[01:25:48.400 --> 01:25:54.160]   When you go to the store in 2022 to get another set, you can say to the store, "Hey, office
[01:25:54.160 --> 01:25:55.160]   supply store.
[01:25:55.160 --> 01:25:56.200]   I'll see you in two years.
[01:25:56.200 --> 01:25:58.320]   I'll see you in 2024."
[01:25:58.320 --> 01:25:59.840]   That's awesome.
[01:25:59.840 --> 01:26:01.360]   Super sized, easy to fill tanks.
[01:26:01.360 --> 01:26:04.560]   You're never going to hassle with buying or changing ink cartridges again.
[01:26:04.560 --> 01:26:06.200]   It's changing the way people print.
[01:26:06.200 --> 01:26:07.680]   Less frustration.
[01:26:07.680 --> 01:26:08.880]   More time to get things done.
[01:26:08.880 --> 01:26:12.480]   If you're in business, more time to get more business done at home, I have two of them
[01:26:12.480 --> 01:26:13.480]   at home.
[01:26:13.480 --> 01:26:14.480]   I love them.
[01:26:14.480 --> 01:26:15.480]   It's great for business, great for home print.
[01:26:15.480 --> 01:26:19.760]   All your business reports, your tax time's coming up, your tax forms.
[01:26:19.760 --> 01:26:23.320]   Just do it in color too, the beautiful color.
[01:26:23.320 --> 01:26:28.200]   There are a whole bunch of Epson Eco Tank printers, multi-function printers to match your
[01:26:28.200 --> 01:26:30.600]   needs to match your wallet.
[01:26:30.600 --> 01:26:34.360]   Whenever you're thinking about printing, add the Eco Tank printer to your shopping list
[01:26:34.360 --> 01:26:36.520]   so you can just fill and chill.
[01:26:36.520 --> 01:26:37.960]   That's what Shaquille O'Neal says.
[01:26:37.960 --> 01:26:38.960]   Fill and chill.
[01:26:38.960 --> 01:26:41.440]   Transform the way your home or office prints.
[01:26:41.440 --> 01:26:42.960]   Do away with that of ink, frustration.
[01:26:42.960 --> 01:26:46.600]   Go to epsin.com/ecotankleo.
[01:26:46.600 --> 01:26:47.600]   epsin.
[01:26:47.600 --> 01:26:48.600]   E-P-S-O-N.
[01:26:48.600 --> 01:26:49.600]   com/ecotankleo.
[01:26:49.600 --> 01:26:51.600]   Epsin Eco Tank printers just fill and chill.
[01:26:51.600 --> 01:26:58.840]   Epsin, exceed your vision.
[01:26:58.840 --> 01:27:01.040]   There were a lot of tech companies.
[01:27:01.040 --> 01:27:04.800]   Did Epson have a, I don't think they had a Super Bowl ad, Rocket Mortgage did that weird
[01:27:04.800 --> 01:27:08.320]   ad with Jason Momoa taking off his muscles.
[01:27:08.320 --> 01:27:09.520]   That kind of creeped me out.
[01:27:09.520 --> 01:27:10.520]   I loved it.
[01:27:10.520 --> 01:27:11.520]   Did you?
[01:27:11.520 --> 01:27:13.520]   It was a really memorable.
[01:27:13.520 --> 01:27:17.680]   Do you remember who the ad was for?
[01:27:17.680 --> 01:27:18.680]   It's for me.
[01:27:18.680 --> 01:27:19.680]   It's Jason Momoa.
[01:27:19.680 --> 01:27:20.680]   I was in it.
[01:27:20.680 --> 01:27:24.560]   He's for me, because Aquaman is in it.
[01:27:24.560 --> 01:27:27.600]   There's a great YouTube video I recommend on the making of it.
[01:27:27.600 --> 01:27:28.600]   They show how they did it.
[01:27:28.600 --> 01:27:33.540]   They had a skinny guy and they had to match his movements and then they would use green
[01:27:33.540 --> 01:27:37.280]   screen and other CGI technologies too.
[01:27:37.280 --> 01:27:40.800]   Because obviously he couldn't really take off his muscles.
[01:27:40.800 --> 01:27:44.560]   Is it similar to the way that they did Captain America, right?
[01:27:44.560 --> 01:27:48.600]   With Steve Rogers was a little guy.
[01:27:48.600 --> 01:27:49.600]   That's pretty cool.
[01:27:49.600 --> 01:27:56.640]   I mean, another Academy Award nominated film tonight is Irishman in which they make Robert
[01:27:56.640 --> 01:28:02.920]   De Niro, Joe Pesci and Al Pacino look like they're in their 40s when they're really
[01:28:02.920 --> 01:28:05.960]   in their 70s and 80s.
[01:28:05.960 --> 01:28:06.960]   What did you think of that?
[01:28:06.960 --> 01:28:07.960]   Did it work?
[01:28:07.960 --> 01:28:10.960]   Did that CGI work?
[01:28:10.960 --> 01:28:14.320]   I didn't actually see it, but it sounds to me like they just did.
[01:28:14.320 --> 01:28:16.160]   It was a little freaky.
[01:28:16.160 --> 01:28:17.480]   Well, it was deep fakes.
[01:28:17.480 --> 01:28:18.480]   You're right.
[01:28:18.480 --> 01:28:19.480]   Yeah.
[01:28:19.480 --> 01:28:23.640]   And the other weird thing is, I guess the director Martin Scorsese said, the problem
[01:28:23.640 --> 01:28:26.400]   is the acting is in the eyes, so we can't change the eyes.
[01:28:26.400 --> 01:28:30.560]   So they all had old eyes and young faces, which was very weird.
[01:28:30.560 --> 01:28:33.440]   And then the other thing that was very hard to do, and I guess they brought in movement
[01:28:33.440 --> 01:28:37.800]   coaches, but you don't move the same when you're in your 80s as you did when you were
[01:28:37.800 --> 01:28:38.800]   in your 40s.
[01:28:38.800 --> 01:28:39.800]   Yes.
[01:28:39.800 --> 01:28:43.080]   So there's one scene where Robert De Niro has taken a hand gonna throw it in the bay as
[01:28:43.080 --> 01:28:46.240]   one does to get rid of the evidence.
[01:28:46.240 --> 01:28:52.640]   And he has to go over these rocks and you just want to leap out and say, careful grandpa,
[01:28:52.640 --> 01:28:57.800]   he's just a little, he's just, you just can tell he's not a 40 year old guy.
[01:28:57.800 --> 01:28:58.800]   Yeah.
[01:28:58.800 --> 01:29:05.960]   I argue that de-aging if we had had it for the Godfather would have not had the great
[01:29:05.960 --> 01:29:09.400]   Robert De Niro performance that we had in Godfather 2.
[01:29:09.400 --> 01:29:10.680]   Well, I'm saying.
[01:29:10.680 --> 01:29:15.680]   And remember, maybe you don't remember, but I do, that in Godfather 1, the studio did
[01:29:15.680 --> 01:29:21.480]   not want Coppola the cast Martin Marlin Brando as the Godfather because he was in his 40s.
[01:29:21.480 --> 01:29:23.440]   He was a young guy.
[01:29:23.440 --> 01:29:29.320]   And they forced him to do a test sequence with Brando.
[01:29:29.320 --> 01:29:32.160]   It was the scene where he's in the olive oil company and they're trying to convince
[01:29:32.160 --> 01:29:36.200]   him to deal drugs and he does his thank you, but no thank you.
[01:29:36.200 --> 01:29:43.040]   And they actually shot that to prove to the studio he can look old, but that was all makeup.
[01:29:43.040 --> 01:29:47.000]   And that's actually, that's an interesting point because in the old days, if you think
[01:29:47.000 --> 01:29:51.440]   about movies like, remember Dustin Hoffman, Little Big Man where he had to become a 100
[01:29:51.440 --> 01:29:55.200]   year old guy and it didn't, it didn't, it's makeup.
[01:29:55.200 --> 01:29:56.200]   It's makeup.
[01:29:56.200 --> 01:29:57.200]   It's makeup.
[01:29:57.200 --> 01:29:59.200]   But we suspended disbelief.
[01:29:59.200 --> 01:30:07.120]   We go, well, yeah, it's, it's because it's really Dustin in his 30s wearing a lot of makeup.
[01:30:07.120 --> 01:30:08.880]   Too bad makeup doesn't work the other way.
[01:30:08.880 --> 01:30:09.880]   Yeah.
[01:30:09.880 --> 01:30:11.680]   You can't de-age people.
[01:30:11.680 --> 01:30:13.040]   Can you, maybe you can.
[01:30:13.040 --> 01:30:17.920]   I think you could probably, you could probably do, which is like you, you age yourself up
[01:30:17.920 --> 01:30:23.000]   just a little bit with makeup so that when you do the, when you do the good makeup and
[01:30:23.000 --> 01:30:27.320]   good lighting, like more fill lighting and stuff, it would, it would look like a bigger
[01:30:27.320 --> 01:30:28.800]   swing that it actually was.
[01:30:28.800 --> 01:30:29.800]   How long?
[01:30:29.800 --> 01:30:32.400]   But I thought like, you know, De Niro, that had such a great performance in Godfather
[01:30:32.400 --> 01:30:34.160]   II as a young Marl Frento.
[01:30:34.160 --> 01:30:35.160]   Right.
[01:30:35.160 --> 01:30:36.160]   Right.
[01:30:36.160 --> 01:30:39.320]   So it's like that to me is like, I, I point to that every time people go, wow, de-aging.
[01:30:39.320 --> 01:30:41.640]   And I'm like, what about in Star Wars?
[01:30:41.640 --> 01:30:48.960]   Do you think that they, I don't know how they got Princess Leia in the latest Star Wars?
[01:30:48.960 --> 01:30:54.640]   Well, wasn't it that most of those, if you saw the movie, it was, it had, she, all of
[01:30:54.640 --> 01:30:58.160]   her lines were very generic where it was like, I can't stand to lose you again.
[01:30:58.160 --> 01:30:59.760]   Never underestimate a droid.
[01:30:59.760 --> 01:31:05.440]   It was like, there were very non-specific lines and you could tell if, if you're in production,
[01:31:05.440 --> 01:31:07.760]   you could really tell that they rode around a lot of that.
[01:31:07.760 --> 01:31:08.760]   Okay.
[01:31:08.760 --> 01:31:16.440]   But, but they did one sequence that de-aged her and Mark Hamill to the events of shortly
[01:31:16.440 --> 01:31:18.040]   after Return of the Jedi.
[01:31:18.040 --> 01:31:19.800]   Was it believable?
[01:31:19.800 --> 01:31:21.120]   It was okay.
[01:31:21.120 --> 01:31:26.200]   It wasn't great, but it was also kind of like, it was quick enough that it was like, okay,
[01:31:26.200 --> 01:31:27.200]   that was fine.
[01:31:27.200 --> 01:31:34.280]   But if they had used it the way that they did with Grand Moff Tarkin in Rogue One, I, I,
[01:31:34.280 --> 01:31:36.240]   I did, I would not have enjoyed that.
[01:31:36.240 --> 01:31:37.240]   That was really weird.
[01:31:37.240 --> 01:31:39.640]   I didn't think Commander Tarkin was believable at all.
[01:31:39.640 --> 01:31:40.640]   Very, very weird.
[01:31:40.640 --> 01:31:42.240]   Yeah, it was very, very weird.
[01:31:42.240 --> 01:31:43.480]   That was, that was from whole cloth, right?
[01:31:43.480 --> 01:31:44.480]   They didn't have any footage.
[01:31:44.480 --> 01:31:45.480]   Yeah.
[01:31:45.480 --> 01:31:48.120]   They had a stand in it and then they just, yeah.
[01:31:48.120 --> 01:31:50.600]   It's just very, it's a very strange.
[01:31:50.600 --> 01:31:53.960]   How long before these deep fakes, because that's really what they are.
[01:31:53.960 --> 01:32:00.320]   How, yeah, that's, how long before they're good enough that we could be fooled.
[01:32:00.320 --> 01:32:02.440]   They're not fooling us now, right?
[01:32:02.440 --> 01:32:03.440]   Yeah.
[01:32:03.440 --> 01:32:04.440]   Some kind.
[01:32:04.440 --> 01:32:05.440]   I think they are.
[01:32:05.440 --> 01:32:06.440]   They are?
[01:32:06.440 --> 01:32:07.440]   Yeah.
[01:32:07.440 --> 01:32:11.120]   I think people, just like people have that short attention span and only share a headline,
[01:32:11.120 --> 01:32:16.520]   if that, that real quick kind of, you know, real quick at a glance.
[01:32:16.520 --> 01:32:18.040]   Oh my God, can you believe this?
[01:32:18.040 --> 01:32:19.200]   Like person said this thing?
[01:32:19.200 --> 01:32:24.840]   Like I, I think deep fakes are going to get pretty, pretty scary soon.
[01:32:24.840 --> 01:32:30.640]   Nate, you, you, you think, what, have you been fooled Nate?
[01:32:30.640 --> 01:32:32.040]   I, well, I don't know.
[01:32:32.040 --> 01:32:33.040]   Maybe I have.
[01:32:33.040 --> 01:32:34.040]   Good answer.
[01:32:34.040 --> 01:32:37.040]   Maybe I have.
[01:32:37.040 --> 01:32:38.040]   That's part of the problem.
[01:32:38.040 --> 01:32:42.200]   But the thing that I always interest, that interests me with this is, is actually the
[01:32:42.200 --> 01:32:47.800]   psychological aspect, which is a lot of the time we see things that we want to see and
[01:32:47.800 --> 01:32:49.040]   we want to believe.
[01:32:49.040 --> 01:32:54.680]   And so often I think that a deep fake only has to be good enough for somebody to believe
[01:32:54.680 --> 01:32:56.720]   what they're seeing is accurate.
[01:32:56.720 --> 01:33:00.520]   I think it's different in films because there's a, there are other elements of illusion of
[01:33:00.520 --> 01:33:06.040]   play there, but with, with, with politics in particular, not to go back to the start
[01:33:06.040 --> 01:33:07.640]   of the show again.
[01:33:07.640 --> 01:33:11.200]   I think that's where it gets particularly worrying because then they don't have to be
[01:33:11.200 --> 01:33:15.720]   absolutely perfect for somebody who wants to believe what they're seeing is true, to
[01:33:15.720 --> 01:33:19.920]   believe that it is true and then to spread it as if it was.
[01:33:19.920 --> 01:33:21.840]   And for that, it doesn't need perfection.
[01:33:21.840 --> 01:33:25.480]   And that's why I think we're kind of already there and why it's already scary.
[01:33:25.480 --> 01:33:32.720]   Don't attempt too much, use it subtly, use it as in just the right place.
[01:33:32.720 --> 01:33:33.720]   Yeah.
[01:33:33.720 --> 01:33:34.720]   Yeah.
[01:33:34.720 --> 01:33:36.040]   And you know what, people aren't very good.
[01:33:36.040 --> 01:33:40.640]   I have to, you know what, I have to agree with you because I, I watch videos all the
[01:33:40.640 --> 01:33:45.880]   time on Reddit and elsewhere where I think that must be made up.
[01:33:45.880 --> 01:33:46.880]   And but I don't know.
[01:33:46.880 --> 01:33:51.120]   Deep, deep fake detection is certainly a growth industry right now.
[01:33:51.120 --> 01:33:59.960]   Certainly Google's, what do they call it, the, their political arm has come up with a thing
[01:33:59.960 --> 01:34:05.640]   called assembler that can detect modified photos, but they, but they're going to, and
[01:34:05.640 --> 01:34:10.000]   they're going to give it to journalists, but they're not going to give it to us.
[01:34:10.000 --> 01:34:11.480]   Be nice if we had that.
[01:34:11.480 --> 01:34:15.520]   It just reminds me of that really old meme.
[01:34:15.520 --> 01:34:18.240]   This is a shop I can tell by the pixels.
[01:34:18.240 --> 01:34:21.080]   But you can, right?
[01:34:21.080 --> 01:34:22.840]   The pixels will tell you, won't they?
[01:34:22.840 --> 01:34:23.920]   I guess so.
[01:34:23.920 --> 01:34:25.440]   I mean, yeah.
[01:34:25.440 --> 01:34:26.440]   No.
[01:34:26.440 --> 01:34:27.440]   Some, I don't know.
[01:34:27.440 --> 01:34:28.440]   I don't know.
[01:34:28.440 --> 01:34:30.200]   I want to come up with a point where someone will find a way around, you know.
[01:34:30.200 --> 01:34:35.280]   I wanted to believe that my refrigerator finally updated its firm years, firmware after five
[01:34:35.280 --> 01:34:39.040]   years and the chat room pointed out to me quickly that I may have been wrong.
[01:34:39.040 --> 01:34:40.040]   What?
[01:34:40.040 --> 01:34:41.040]   It didn't?
[01:34:41.040 --> 01:34:44.600]   Well, I mean, it certainly appears that it did, but all they have to do is put something
[01:34:44.600 --> 01:34:47.280]   up that says, Hey, we've, right.
[01:34:47.280 --> 01:34:51.960]   What's updated, we fixed it.
[01:34:51.960 --> 01:34:57.800]   I saw a Reddit, maybe, maybe, maybe I found one.
[01:34:57.800 --> 01:35:03.080]   I blame you, Carson, for following maybe, maybe, maybe there's, there was on Reddit, there used
[01:35:03.080 --> 01:35:06.600]   to be a subreddit called Yes, Yes, Yes.
[01:35:06.600 --> 01:35:08.520]   And then one called No, No, No.
[01:35:08.520 --> 01:35:09.520]   But the problem with Yes, Yes, Yes.
[01:35:09.520 --> 01:35:12.280]   And No, No, No is you'd know how the video was going to end.
[01:35:12.280 --> 01:35:13.280]   Yes, Yes, Yes.
[01:35:13.280 --> 01:35:15.400]   It was always going to end like, wow, it worked.
[01:35:15.400 --> 01:35:17.960]   No, No, No, No, it was always going to end in disaster.
[01:35:17.960 --> 01:35:23.960]   But maybe, maybe, maybe combines both, but cleverly eliminates any indication about which way
[01:35:23.960 --> 01:35:26.600]   it's going to go.
[01:35:26.600 --> 01:35:28.680]   Let me see.
[01:35:28.680 --> 01:35:32.240]   Let me see if I can find the maybe, maybe maybe I was looking at, I showed it to a number
[01:35:32.240 --> 01:35:34.920]   of people and they said, that's, that's a fake.
[01:35:34.920 --> 01:35:37.280]   And I thought, Oh, maybe it is.
[01:35:37.280 --> 01:35:38.680]   I can't, I just don't know.
[01:35:38.680 --> 01:35:40.480]   I can't tell.
[01:35:40.480 --> 01:35:41.800]   I don't know if I'm going to be able to find it.
[01:35:41.800 --> 01:35:48.680]   It was, it was a claim to be a Russian dash cam footage of a little kid wandering into
[01:35:48.680 --> 01:35:50.640]   the street in the snow.
[01:35:50.640 --> 01:35:55.440]   Maybe you could find it, Carsten and, and getting rescued.
[01:35:55.440 --> 01:35:57.440]   And I thought it was real.
[01:35:57.440 --> 01:35:59.400]   Do you remember that one?
[01:35:59.400 --> 01:36:03.160]   You think that was real?
[01:36:03.160 --> 01:36:05.960]   And then Lisa said, that's not real.
[01:36:05.960 --> 01:36:08.120]   Yeah, yeah, yeah.
[01:36:08.120 --> 01:36:09.120]   Yeah.
[01:36:09.120 --> 01:36:11.440]   That was real?
[01:36:11.440 --> 01:36:13.560]   Are you sure?
[01:36:13.560 --> 01:36:15.200]   How do we know?
[01:36:15.200 --> 01:36:16.200]   How can we be?
[01:36:16.200 --> 01:36:17.200]   How can we tell?
[01:36:17.200 --> 01:36:18.200]   All right.
[01:36:18.200 --> 01:36:19.800]   Well, if they can find it, I'll show it.
[01:36:19.800 --> 01:36:21.880]   Otherwise you just have to look for it.
[01:36:21.880 --> 01:36:23.440]   It has a good ending.
[01:36:23.440 --> 01:36:25.120]   But again, I feel like it's a faked ending.
[01:36:25.120 --> 01:36:28.320]   I don't know if it's a real good ending.
[01:36:28.320 --> 01:36:34.600]   IBM Marriott and Mickey Mouse walk into a court.
[01:36:34.600 --> 01:36:43.720]   They're all going after section 230, which is Denise Howell's favorite section.
[01:36:43.720 --> 01:36:44.720]   Denise, I'm so glad you're here.
[01:36:44.720 --> 01:36:45.720]   Everybody's favorite section.
[01:36:45.720 --> 01:36:47.440]   Well, it should be.
[01:36:47.440 --> 01:36:49.560]   People who hate the technology.
[01:36:49.560 --> 01:36:51.560]   Hate the internet.
[01:36:51.560 --> 01:36:52.560]   Yes.
[01:36:52.560 --> 01:36:58.280]   Apparently, Marriott has asked Congress to, we're talking about section 230 of the Communications
[01:36:58.280 --> 01:37:01.280]   Decency Act of 1996.
[01:37:01.280 --> 01:37:03.120]   So it's a pretty old law.
[01:37:03.120 --> 01:37:07.200]   Disney is fighting to stop the law's spread abroad according to the New York Times.
[01:37:07.200 --> 01:37:09.040]   Marriott has asked Congress to amend it.
[01:37:09.040 --> 01:37:11.840]   IBM is a plan to slim it down.
[01:37:11.840 --> 01:37:15.760]   And of course, many members of Congress are going after section 230.
[01:37:15.760 --> 01:37:19.000]   Denise is going to have to explain what section 230 does.
[01:37:19.000 --> 01:37:29.280]   So section 230 gives anybody who enables others to publish things on their platform.
[01:37:29.280 --> 01:37:31.560]   A safe harbor.
[01:37:31.560 --> 01:37:32.560]   It protects them.
[01:37:32.560 --> 01:37:37.560]   If you're Facebook, you can't be sued for what somebody posts on Facebook.
[01:37:37.560 --> 01:37:39.440]   With important limitations.
[01:37:39.440 --> 01:37:45.520]   Now, the analog for this is the phone company's not, if I make a phone call and I say I'm
[01:37:45.520 --> 01:37:50.360]   plotting to overthrow the government or something, the phone company's not liable for that.
[01:37:50.360 --> 01:37:51.360]   Correct.
[01:37:51.360 --> 01:37:52.960]   They're a common carrier.
[01:37:52.960 --> 01:37:53.960]   Yes.
[01:37:53.960 --> 01:37:56.280]   And that nobody disputes that.
[01:37:56.280 --> 01:38:00.160]   How could you sue the phone company for something I said on a phone call?
[01:38:00.160 --> 01:38:05.600]   And that was in 1996, the theory behind section 230 is well, these companies, we can't, if
[01:38:05.600 --> 01:38:11.400]   Leo has a chat room, we can't hold him responsible for what somebody says in the chat room.
[01:38:11.400 --> 01:38:12.400]   Right?
[01:38:12.400 --> 01:38:17.040]   Well, I mean, there were more if you delve into the history of the law, et cetera.
[01:38:17.040 --> 01:38:22.840]   It wasn't so much that they were trying to create a safe harbor to protect folks like
[01:38:22.840 --> 01:38:25.160]   Leo or Google.
[01:38:25.160 --> 01:38:33.320]   What they wanted to do was, section 230 was part of an act that then got determined to
[01:38:33.320 --> 01:38:34.320]   be unconstitutional.
[01:38:34.320 --> 01:38:36.600]   The CPA is unconstitutional?
[01:38:36.600 --> 01:38:43.240]   Yes, the Communications Decency Act wanted to censor the internet.
[01:38:43.240 --> 01:38:44.240]   Oh.
[01:38:44.240 --> 01:38:46.240]   And that didn't fly.
[01:38:46.240 --> 01:38:54.480]   But what section 230 was designed so that people would actually police their sites and
[01:38:54.480 --> 01:39:06.920]   have some engagement and ownership, have guidelines in user restrictions, et cetera,
[01:39:06.920 --> 01:39:10.880]   so that it wouldn't be the Wild West.
[01:39:10.880 --> 01:39:14.360]   That's why there are limited protections.
[01:39:14.360 --> 01:39:19.720]   And we want to encourage people to be proactive about managing their sites without holding
[01:39:19.720 --> 01:39:21.880]   them liable.
[01:39:21.880 --> 01:39:28.480]   And yet, built into the law were a couple of important restrictions.
[01:39:28.480 --> 01:39:34.160]   Anything that is a federal crime, you're not going to have any protection for that.
[01:39:34.160 --> 01:39:39.000]   So you still have to make sure that people aren't using your site to carry out federal
[01:39:39.000 --> 01:39:40.280]   crimes.
[01:39:40.280 --> 01:39:46.680]   And surprise, surprise, anything that is a federal intellectual property violation,
[01:39:46.680 --> 01:39:48.600]   you're not going to have protection for that.
[01:39:48.600 --> 01:39:56.520]   So the entertainment industry got their protections in there at the inception of this law.
[01:39:56.520 --> 01:40:03.480]   So it's somewhat ironic, as it's pointed out in this New York Times article that Disney
[01:40:03.480 --> 01:40:12.560]   is one of the strong lobbyists seeking to restrict section 230 when really their interests
[01:40:12.560 --> 01:40:14.720]   are covered.
[01:40:14.720 --> 01:40:20.160]   So he says you can go on YouTube or Google or any other platform and infringe their
[01:40:20.160 --> 01:40:24.560]   intellectual property and let users do that.
[01:40:24.560 --> 01:40:29.240]   Yeah, we know that because we get taken down on YouTube all the time for what is in fact
[01:40:29.240 --> 01:40:31.400]   not a violation, but a fair use.
[01:40:31.400 --> 01:40:32.400]   That's right.
[01:40:32.400 --> 01:40:33.840]   So we read what Disney is complaining about.
[01:40:33.840 --> 01:40:43.680]   It's more about terrorism and things, trafficking, very sudden things that have very loose connection
[01:40:43.680 --> 01:40:45.880]   to Disney's actual business.
[01:40:45.880 --> 01:40:47.560]   That's how you do it.
[01:40:47.560 --> 01:40:51.400]   If you're going to go after a law, you never say because it's bad for our business, you
[01:40:51.400 --> 01:40:53.080]   say think of the children.
[01:40:53.080 --> 01:40:54.080]   It's bad.
[01:40:54.080 --> 01:40:55.240]   Think of women and children.
[01:40:55.240 --> 01:40:57.160]   It's bad for them.
[01:40:57.160 --> 01:40:58.400]   That's how you convince people.
[01:40:58.400 --> 01:41:03.120]   Marriott, according to the Times, wants to make it harder for A or B and B to fight local
[01:41:03.120 --> 01:41:05.120]   hotel laws.
[01:41:05.120 --> 01:41:11.240]   IBM wants consumer online services just to be more responsible for content on their sites.
[01:41:11.240 --> 01:41:13.160]   They couldn't find a real reason for IBM.
[01:41:13.160 --> 01:41:15.400]   I suspect it has to do with libel.
[01:41:15.400 --> 01:41:22.240]   But as people like Joe Biden has said that 230 told the New York Times should be revoked
[01:41:22.240 --> 01:41:24.040]   immediately.
[01:41:24.040 --> 01:41:29.840]   Lindsey Graham and Brian Schatz both have bills in Congress to increase liability.
[01:41:29.840 --> 01:41:33.480]   The Justice Department is conducting a review of the law.
[01:41:33.480 --> 01:41:38.640]   How important is 230 to preserve the Internet as we know it?
[01:41:38.640 --> 01:41:39.640]   Facebook and Twitter.
[01:41:39.640 --> 01:41:43.040]   I don't think we need to defend Facebook and Twitter.
[01:41:43.040 --> 01:41:46.680]   If Facebook and Twitter were responsible for everything posted on there, it'd be hard
[01:41:46.680 --> 01:41:48.440]   for them to continue to do business.
[01:41:48.440 --> 01:41:56.160]   But it'd be even harder for me to have to police my chatroom or my comments.
[01:41:56.160 --> 01:42:00.240]   Think about defamation, which is something that is within the safe harbor.
[01:42:00.240 --> 01:42:02.720]   Think about how much defamation happens online.
[01:42:02.720 --> 01:42:08.600]   How many times somebody says something false and harmful about something else?
[01:42:08.600 --> 01:42:14.160]   If you are Facebook or Twitter or Instagram or you monitoring the Twitter chatroom...
[01:42:14.160 --> 01:42:16.160]   That's everything on Twitter.
[01:42:16.160 --> 01:42:17.160]   Exactly.
[01:42:17.160 --> 01:42:20.520]   I mean, how do you operate if you're responsible?
[01:42:20.520 --> 01:42:22.160]   All of you two are comments.
[01:42:22.160 --> 01:42:24.000]   That's all YouTube comments.
[01:42:24.000 --> 01:42:26.080]   Somebody said it, and I think this is actually true.
[01:42:26.080 --> 01:42:30.240]   Twitter is the Internet's comments section.
[01:42:30.240 --> 01:42:31.760]   That's exactly what Twitter is.
[01:42:31.760 --> 01:42:38.000]   It's just with exactly the same level of quality.
[01:42:38.000 --> 01:42:44.520]   But at the same time, I think I understand why people wish to regulate Twitter and Facebook.
[01:42:44.520 --> 01:42:47.480]   But I don't know if you can without breaking the Internet.
[01:42:47.480 --> 01:42:48.480]   Right.
[01:42:48.480 --> 01:42:55.520]   Well, two years ago, there was another exception that was added legislatively to Section 230.
[01:42:55.520 --> 01:42:56.520]   And that was...
[01:42:56.520 --> 01:42:58.560]   You remember Cesta and Fosta?
[01:42:58.560 --> 01:42:59.560]   Yes.
[01:42:59.560 --> 01:43:03.240]   So now, in addition to...
[01:43:03.240 --> 01:43:05.240]   There's no safe harbor for federal crimes.
[01:43:05.240 --> 01:43:12.120]   There's no safe harbor if your users are engaging in intellectual property violations, federal
[01:43:12.120 --> 01:43:15.040]   intellectual property violations.
[01:43:15.040 --> 01:43:20.760]   And now there's an exception for certain sex trafficking offenses as well.
[01:43:20.760 --> 01:43:21.760]   Highly controversial.
[01:43:21.760 --> 01:43:29.720]   But you can see that there's a trend in Washington to say, "Hey, these tech giants that have made
[01:43:29.720 --> 01:43:34.600]   these vast fortunes out of letting this conduct take place.
[01:43:34.600 --> 01:43:41.000]   They can't just sit back and profit off terrible things.
[01:43:41.000 --> 01:43:42.440]   We're not going to let them do that anymore."
[01:43:42.440 --> 01:43:49.120]   So there's a lot of sentiment in Washington, as you pointed out, Joe Biden and others to
[01:43:49.120 --> 01:43:50.360]   rein that in.
[01:43:50.360 --> 01:43:54.680]   But where this all winds up, Leo, I don't know.
[01:43:54.680 --> 01:43:58.840]   And it is complicated because on the face of it, you would say, "Well, I'm not for sex
[01:43:58.840 --> 01:43:59.840]   trafficking."
[01:43:59.840 --> 01:44:02.560]   So Cesta and Fosta seem like a good idea.
[01:44:02.560 --> 01:44:08.320]   The upshot of it has been to drive sex workers underground and make their jobs more dangerous,
[01:44:08.320 --> 01:44:10.840]   not less dangerous.
[01:44:10.840 --> 01:44:15.480]   I don't know if it's helped with trafficking.
[01:44:15.480 --> 01:44:21.920]   Is it Denise, do you think it's something that needs to continually have additional carve-outs
[01:44:21.920 --> 01:44:25.160]   based on abuse of larger platforms?
[01:44:25.160 --> 01:44:26.720]   Is it something that...
[01:44:26.720 --> 01:44:31.520]   For example, we've all probably read that story about that poor four-year-old who died of
[01:44:31.520 --> 01:44:40.040]   the flu because his mother was going on anti-vaxxer groups on Facebook who discouraged her from
[01:44:40.040 --> 01:44:42.280]   seeking medical treatment for him.
[01:44:42.280 --> 01:44:44.040]   Don't use Tammo flu.
[01:44:44.040 --> 01:44:45.040]   It's a communist conspiracy.
[01:44:45.040 --> 01:44:46.040]   They said, "Don't use that."
[01:44:46.040 --> 01:44:47.040]   Right.
[01:44:47.040 --> 01:44:49.040]   And so she said, "No, I don't want Tammo flu to be done."
[01:44:49.040 --> 01:44:51.840]   Put potatoes in his socks or whatever.
[01:44:51.840 --> 01:45:03.280]   But yeah, is it that we continue to have Section 230 and carve out pieces of it and say, "Hey,
[01:45:03.280 --> 01:45:07.200]   you can't have this type of misinformation or disinformation on your platform or you'll
[01:45:07.200 --> 01:45:09.200]   be held liable."
[01:45:09.200 --> 01:45:12.480]   Do you think that there is a good solution here?
[01:45:12.480 --> 01:45:17.360]   I'm very curious as to your opinion because you're very well read in this arena.
[01:45:17.360 --> 01:45:18.360]   It's also...
[01:45:18.360 --> 01:45:20.640]   She's an attorney and this is her field.
[01:45:20.640 --> 01:45:21.640]   Yeah.
[01:45:21.640 --> 01:45:29.080]   And I've watched this law go through its permutations over the years and the way that it has been
[01:45:29.080 --> 01:45:35.680]   used as a shield and the way that others now want to make these exceptions into sort of
[01:45:35.680 --> 01:45:42.760]   a sword as this New York Times article points out and wield it against the technology industry
[01:45:42.760 --> 01:45:45.360]   when they have other interests.
[01:45:45.360 --> 01:45:50.280]   And to answer your question, Ashley, I think that what you've described is what we will
[01:45:50.280 --> 01:45:57.000]   continue to see happen because that tends to be how our lawmakers function, that they
[01:45:57.000 --> 01:46:02.960]   react to things that their constituents are concerned about in the moment.
[01:46:02.960 --> 01:46:08.440]   And I think that's how we got Cesta and Fosta, winning their way.
[01:46:08.440 --> 01:46:13.160]   It was Fosta that was actually added.
[01:46:13.160 --> 01:46:14.320]   So that's how that happened.
[01:46:14.320 --> 01:46:20.200]   And I think that, yeah, we would probably see something about disinformation as another
[01:46:20.200 --> 01:46:21.320]   carve out.
[01:46:21.320 --> 01:46:28.400]   But what gives me pause is I really don't think that's necessarily a great way to legislate
[01:46:28.400 --> 01:46:34.080]   to go piecemeal and in a reactionary fashion.
[01:46:34.080 --> 01:46:36.080]   So again, I think there's just...
[01:46:36.080 --> 01:46:40.760]   There's so much concern and hand-ringing about these issues that I don't think that
[01:46:40.760 --> 01:46:45.600]   Section 230 is in a real strong, secure place right now.
[01:46:45.600 --> 01:46:46.800]   And certainly there's a lot...
[01:46:46.800 --> 01:46:52.080]   I think that politicians are getting a lot of capital out of talking about how we want
[01:46:52.080 --> 01:46:56.360]   more responsibility from these platforms.
[01:46:56.360 --> 01:47:02.080]   So I think that you're on to something that we will continue to see things sort of pick
[01:47:02.080 --> 01:47:07.400]   away at the safe harbor, but I don't necessarily like that situation.
[01:47:07.400 --> 01:47:08.400]   Yeah.
[01:47:08.400 --> 01:47:10.800]   And irony is that both...
[01:47:10.800 --> 01:47:16.000]   I don't know if it's true, but chatroom saying both Google and Twitter are in favor of limiting
[01:47:16.000 --> 01:47:21.080]   230 because it protects them because the small guys can't compete because they at least
[01:47:21.080 --> 01:47:22.080]   have the resources.
[01:47:22.080 --> 01:47:23.080]   Right, right.
[01:47:23.080 --> 01:47:24.080]   Ah, great.
[01:47:24.080 --> 01:47:31.280]   It's hard to win.
[01:47:31.280 --> 01:47:32.280]   I will...
[01:47:32.280 --> 01:47:35.280]   We're going to take a break and I have found the...
[01:47:35.280 --> 01:47:38.920]   I will ask the panel, "Is this a fake or not a fake?"
[01:47:38.920 --> 01:47:43.280]   I have found the video from maybe, maybe, maybe you could tell me.
[01:47:43.280 --> 01:47:45.240]   And then I don't know the answer.
[01:47:45.240 --> 01:47:49.160]   We already have one person in the studio says, "It's absolutely real.
[01:47:49.160 --> 01:47:51.040]   I will let you all be the judge of that."
[01:47:51.040 --> 01:47:56.200]   But first of all, from our good friends, I've always said LinkedIn is the last good social
[01:47:56.200 --> 01:47:57.200]   network.
[01:47:57.200 --> 01:48:02.000]   It's the last place you can go and not be bombarded with garbage.
[01:48:02.000 --> 01:48:03.240]   It is a great place.
[01:48:03.240 --> 01:48:06.960]   If you're a professional, you got to live on LinkedIn.
[01:48:06.960 --> 01:48:11.440]   And if you're a business, you really ought to look at LinkedIn for your advertising.
[01:48:11.440 --> 01:48:17.120]   I showed you they brought to you by LinkedIn marketing solutions, time and places, everything,
[01:48:17.120 --> 01:48:18.840]   especially in marketing.
[01:48:18.840 --> 01:48:21.200]   But today we're bombarded, aren't we?
[01:48:21.200 --> 01:48:23.840]   Millions of messages a minute, not enough hours in the day.
[01:48:23.840 --> 01:48:25.200]   How do you get people's attention?
[01:48:25.200 --> 01:48:30.280]   How do you reach the influencers that are most important to your brand?
[01:48:30.280 --> 01:48:31.440]   There's an easy way.
[01:48:31.440 --> 01:48:32.440]   LinkedIn.
[01:48:32.440 --> 01:48:37.280]   You speak to the right professionals at the right time very efficiently.
[01:48:37.280 --> 01:48:38.280]   Who's on LinkedIn?
[01:48:38.280 --> 01:48:40.520]   62 million decision makers.
[01:48:40.520 --> 01:48:46.520]   And LinkedIn marketing gives you the tools you need to get to exactly the right business
[01:48:46.520 --> 01:48:49.320]   leaders who are relevant to your brand.
[01:48:49.320 --> 01:48:54.240]   With LinkedIn ads, you can make sure your messages are getting through to those people.
[01:48:54.240 --> 01:48:55.240]   I've got an example.
[01:48:55.240 --> 01:48:59.280]   Rod Strathar, who is director of digital and social center of excellence with Lenovo.
[01:48:59.280 --> 01:49:00.280]   A quote from Rod.
[01:49:00.280 --> 01:49:04.760]   He said, "With LinkedIn, we're seeing a lift of 17% in brand favorability."
[01:49:04.760 --> 01:49:07.600]   We're already looking at how we can extend this into other markets.
[01:49:07.600 --> 01:49:13.920]   Lenovo was able to tailor its content to enhance its engagement with its audience.
[01:49:13.920 --> 01:49:15.640]   And you know you're going to exist.
[01:49:15.640 --> 01:49:18.280]   It's so often the case when you buy it.
[01:49:18.280 --> 01:49:21.600]   Say you buy an ad on a social network.
[01:49:21.600 --> 01:49:25.400]   You don't know where the environment your ad is going to live in.
[01:49:25.400 --> 01:49:29.920]   Within LinkedIn, you know it's going to live in a positive, valuable environment.
[01:49:29.920 --> 01:49:32.480]   And it's very efficient, very affordable.
[01:49:32.480 --> 01:49:36.520]   Even small and medium businesses are making the most out of LinkedIn ads to get their voices
[01:49:36.520 --> 01:49:40.080]   heard, to get their messages resonating with the audience.
[01:49:40.080 --> 01:49:41.440]   LinkedIn ads drive traffic.
[01:49:41.440 --> 01:49:43.600]   They drive engagement.
[01:49:43.600 --> 01:49:49.240]   That means visits to your landing pages, registrations to events, downloads of thought
[01:49:49.240 --> 01:49:54.680]   leadership content with precise targeting small and medium sized businesses could speak
[01:49:54.680 --> 01:49:56.480]   to the people who matter.
[01:49:56.480 --> 01:50:00.800]   They're in the right environment talking to the people who matter and you could do it
[01:50:00.800 --> 01:50:02.440]   so efficiently.
[01:50:02.440 --> 01:50:05.040]   LinkedIn ads help small businesses get big results.
[01:50:05.040 --> 01:50:06.120]   You should try it for yourself.
[01:50:06.120 --> 01:50:08.520]   In fact, we're going to make it really easy.
[01:50:08.520 --> 01:50:12.200]   They're giving us a $100 ad credit to give you.
[01:50:12.200 --> 01:50:15.760]   Launch your first campaign at LinkedIn.com/twit.
[01:50:15.760 --> 01:50:21.160]   You'll be amazed at how efficient these buys are, how much you can get for $100.
[01:50:21.160 --> 01:50:24.920]   LinkedIn.com/twit terms and conditions apply.
[01:50:24.920 --> 01:50:27.200]   We think LinkedIn is a great way to market.
[01:50:27.200 --> 01:50:32.640]   We use it and we think everybody, especially small businesses, should take a look.
[01:50:32.640 --> 01:50:35.640]   High quality, great place to be.
[01:50:35.640 --> 01:50:37.480]   LinkedIn marketing solutions.
[01:50:37.480 --> 01:50:40.480]   Go to LinkedIn.com/twit.
[01:50:40.480 --> 01:50:41.480]   Here it is.
[01:50:41.480 --> 01:50:46.720]   This is, I will refresh this page so we can start at the beginning here.
[01:50:46.720 --> 01:50:49.360]   Are you ready?
[01:50:49.360 --> 01:50:50.360]   Ready to?
[01:50:50.360 --> 01:50:51.360]   Here's a little boy.
[01:50:51.360 --> 01:50:52.360]   He's running in the road.
[01:50:52.360 --> 01:50:55.760]   He's from Russian dash cam footage.
[01:50:55.760 --> 01:50:56.760]   Everybody's getting out.
[01:50:56.760 --> 01:50:57.760]   They're all worried.
[01:50:57.760 --> 01:51:01.880]   Car comes up, picks the kid up by the collar and drives off.
[01:51:01.880 --> 01:51:03.720]   Saves him.
[01:51:03.720 --> 01:51:04.720]   Fake or real?
[01:51:04.720 --> 01:51:07.960]   Why buy that is real?
[01:51:07.960 --> 01:51:08.960]   Really?
[01:51:08.960 --> 01:51:10.120]   Gets him out of the road.
[01:51:10.120 --> 01:51:11.520]   It worked.
[01:51:11.520 --> 01:51:13.320]   It's very dangerous, by the way, folks.
[01:51:13.320 --> 01:51:15.000]   Do not do this.
[01:51:15.000 --> 01:51:15.880]   Do not do this.
[01:51:15.880 --> 01:51:21.680]   If you slipped and dropped the kid, you just run right over him.
[01:51:21.680 --> 01:51:22.680]   I don't know.
[01:51:22.680 --> 01:51:23.800]   I believe everything except the kid.
[01:51:23.800 --> 01:51:25.440]   The kid's surprisingly colorful.
[01:51:25.440 --> 01:51:28.000]   That's what I was thinking.
[01:51:28.000 --> 01:51:29.000]   Yeah.
[01:51:29.000 --> 01:51:30.000]   What do you think?
[01:51:30.000 --> 01:51:31.000]   Real?
[01:51:31.000 --> 01:51:32.000]   Fake.
[01:51:32.000 --> 01:51:33.000]   And how would you know?
[01:51:33.000 --> 01:51:34.000]   What do you think, Nate?
[01:51:34.000 --> 01:51:35.560]   That's what I don't think I can tell.
[01:51:35.560 --> 01:51:36.560]   Maybe that's not...
[01:51:36.560 --> 01:51:37.840]   I don't know why it would be fake.
[01:51:37.840 --> 01:51:38.840]   Right.
[01:51:38.840 --> 01:51:41.000]   See, that's like one, I don't know why it would be fake.
[01:51:41.000 --> 01:51:47.040]   And also, if it is fake, what is the passenger in that car picking up in that child's place
[01:51:47.040 --> 01:51:48.520]   that they were replaced with a kid?
[01:51:48.520 --> 01:51:50.000]   It could be a small animal.
[01:51:50.000 --> 01:51:51.920]   It could be a loaf of bread.
[01:51:51.920 --> 01:51:55.040]   I think the thing that does make it a little more credible is at the very end when the
[01:51:55.040 --> 01:51:58.480]   mother comes and says, "Oh God, and grabs the kid away."
[01:51:58.480 --> 01:52:02.160]   "Jeez, Louise, what do you do?"
[01:52:02.160 --> 01:52:05.280]   I just feel like it would be much more like if this was fake, I mean, you'd make it more
[01:52:05.280 --> 01:52:06.280]   exciting, right?
[01:52:06.280 --> 01:52:07.440]   You could put it somewhere else.
[01:52:07.440 --> 01:52:08.440]   But shouldn't the kid...
[01:52:08.440 --> 01:52:12.520]   Shouldn't the guy just get out of the car and help the kid back to the sidewalk?
[01:52:12.520 --> 01:52:14.560]   Well, it's cold outside, Leo.
[01:52:14.560 --> 01:52:15.560]   Jasey.
[01:52:15.560 --> 01:52:18.960]   It's very Russian.
[01:52:18.960 --> 01:52:19.960]   Very Russian.
[01:52:19.960 --> 01:52:20.960]   Yeah, it got out.
[01:52:20.960 --> 01:52:23.320]   He's like, "All right, what's going on here?"
[01:52:23.320 --> 01:52:24.560]   Here's my question.
[01:52:24.560 --> 01:52:25.560]   Wait, okay.
[01:52:25.560 --> 01:52:26.560]   Am I to understand that...
[01:52:26.560 --> 01:52:28.360]   Oh, he's the driver.
[01:52:28.360 --> 01:52:29.360]   Okay.
[01:52:29.360 --> 01:52:30.360]   So the driver is on...
[01:52:30.360 --> 01:52:31.360]   Oh, is the driver on the right?
[01:52:31.360 --> 01:52:32.360]   No.
[01:52:32.360 --> 01:52:33.360]   I don't think...
[01:52:33.360 --> 01:52:34.360]   Is this guy...
[01:52:34.360 --> 01:52:36.480]   Well, because this guy in the bus gets out on the...
[01:52:36.480 --> 01:52:39.480]   Our traditional driver side, he is not the driver.
[01:52:39.480 --> 01:52:41.120]   So I assume it's right-hand drive.
[01:52:41.120 --> 01:52:42.920]   Oh, so maybe it's not Russia.
[01:52:42.920 --> 01:52:44.000]   Oh, you're right.
[01:52:44.000 --> 01:52:45.000]   He's not the boy.
[01:52:45.000 --> 01:52:46.000]   You're smart, Ashley.
[01:52:46.000 --> 01:52:47.840]   I didn't know he wasn't the driver, but you're right.
[01:52:47.840 --> 01:52:50.120]   Either he is not the driver or the car.
[01:52:50.120 --> 01:52:52.120]   He's lost control of the vehicle.
[01:52:52.120 --> 01:52:54.960]   Yeah, the car is just a self-driving traffic left.
[01:52:54.960 --> 01:52:55.960]   It's a self-driving...
[01:52:55.960 --> 01:52:56.960]   It is a little icy.
[01:52:56.960 --> 01:52:57.960]   It's not a self-driving car.
[01:52:57.960 --> 01:52:58.960]   I hate it.
[01:52:58.960 --> 01:52:59.960]   I hate it.
[01:52:59.960 --> 01:53:00.960]   I paid for that.
[01:53:00.960 --> 01:53:02.560]   Yeah, if you bought it used, he paid for FSD, for sure.
[01:53:02.560 --> 01:53:03.880]   It hasn't been deactivated yet.
[01:53:03.880 --> 01:53:04.880]   All right.
[01:53:04.880 --> 01:53:06.120]   This happened in 2017.
[01:53:06.120 --> 01:53:07.120]   I don't know.
[01:53:07.120 --> 01:53:08.680]   It's from the We Love Russia YouTube channel.
[01:53:08.680 --> 01:53:09.680]   All right.
[01:53:09.680 --> 01:53:10.680]   How about this one?
[01:53:10.680 --> 01:53:11.680]   All right.
[01:53:11.680 --> 01:53:12.680]   You can tell...
[01:53:12.680 --> 01:53:13.680]   All right.
[01:53:13.680 --> 01:53:14.680]   Fake or not fake?
[01:53:14.680 --> 01:53:15.680]   It's gonna tell if this is a deep-free or not.
[01:53:15.680 --> 01:53:16.680]   So this...
[01:53:16.680 --> 01:53:17.680]   I love this story.
[01:53:17.680 --> 01:53:27.200]   So this guy, Dennis Sexy, not his real name, has upscaled the first motion picture.
[01:53:27.200 --> 01:53:28.560]   You recognize it right away, Ashley.
[01:53:28.560 --> 01:53:29.560]   I'm impressed.
[01:53:29.560 --> 01:53:30.560]   I did.
[01:53:30.560 --> 01:53:33.480]   The scared people in the theater, they thought of no train coming at him.
[01:53:33.480 --> 01:53:36.360]   Yeah, I've repeated that story many times.
[01:53:36.360 --> 01:53:39.960]   I think there is actually no evidence that people ran out of the theater screaming.
[01:53:39.960 --> 01:53:41.440]   I think they were just astonished.
[01:53:41.440 --> 01:53:44.960]   I would imagine it would just be an incredible thing to see for the first time.
[01:53:44.960 --> 01:53:49.560]   This is a very famous movie, very short movie.
[01:53:49.560 --> 01:53:54.160]   The first motion picture, I'm trying to find the details about it.
[01:53:54.160 --> 01:53:55.160]   I don't have it off the top of my head.
[01:53:55.160 --> 01:53:56.640]   It's by the Lumiere Brothers.
[01:53:56.640 --> 01:53:57.640]   Okay.
[01:53:57.640 --> 01:53:59.400]   The Lumiere Brothers made this...
[01:53:59.400 --> 01:54:01.400]   What, in 1890, 1910?
[01:54:01.400 --> 01:54:02.400]   1896.
[01:54:02.400 --> 01:54:04.040]   Okay.
[01:54:04.040 --> 01:54:08.240]   So it's very early motion pictures of a train arriving in a station.
[01:54:08.240 --> 01:54:10.400]   People getting in and out.
[01:54:10.400 --> 01:54:14.520]   What's cool about this is somebody has upscaled this, improved it.
[01:54:14.520 --> 01:54:18.200]   In fact, does a great job of...
[01:54:18.200 --> 01:54:19.200]   You can see...
[01:54:19.200 --> 01:54:21.600]   Actually keep showing it because you can see the source versus the upscaled.
[01:54:21.600 --> 01:54:23.800]   They do a half and half.
[01:54:23.800 --> 01:54:32.000]   4K video, 60 frames a second using computer software to upscale the first movie.
[01:54:32.000 --> 01:54:33.560]   His real name is not Dennis Sexy.
[01:54:33.560 --> 01:54:38.720]   It's Denise Shirev, but I like Dennis Sexy.
[01:54:38.720 --> 01:54:43.880]   He took the arrival of a train at Laschostat and upscaled it.
[01:54:43.880 --> 01:54:46.760]   It was originally 640x480.
[01:54:46.760 --> 01:54:48.720]   I find that even hard to believe, but it is filmed.
[01:54:48.720 --> 01:54:51.720]   So I guess it could be 20 frames a second.
[01:54:51.720 --> 01:54:56.120]   It was one of the first attempts at 3D film.
[01:54:56.120 --> 01:55:00.720]   Not completely believable at the time, but he used...
[01:55:00.720 --> 01:55:05.640]   Denise used a mix of neural networks from Giggle Pixel AI in a technique called Depth-Aware
[01:55:05.640 --> 01:55:11.320]   Video Frame Interpolation to not only upscale the resolution, but increase its frame rate
[01:55:11.320 --> 01:55:14.360]   to 60 frames a second.
[01:55:14.360 --> 01:55:16.760]   That's the amazing part to me is the increasing the frame rate.
[01:55:16.760 --> 01:55:17.760]   That's incredible.
[01:55:17.760 --> 01:55:19.880]   Tripling the frame rate is just...
[01:55:19.880 --> 01:55:21.120]   That's amazing.
[01:55:21.120 --> 01:55:22.120]   Pretty cool.
[01:55:22.120 --> 01:55:25.560]   Wasn't it upscaled from an already upscaled video?
[01:55:25.560 --> 01:55:28.080]   It was, which actually probably makes it harder.
[01:55:28.080 --> 01:55:29.080]   No, you're right.
[01:55:29.080 --> 01:55:35.480]   It was coming from an already upscaled digitized version of the same film.
[01:55:35.480 --> 01:55:39.160]   People messing with this Lumiere Brothers film for a long, long time.
[01:55:39.160 --> 01:55:41.120]   I want to see that color version of it.
[01:55:41.120 --> 01:55:42.120]   Do you want to see the color?
[01:55:42.120 --> 01:55:43.120]   Yeah, that's awesome.
[01:55:43.120 --> 01:55:44.120]   I'll show you that.
[01:55:44.120 --> 01:55:45.120]   I'll show you that.
[01:55:45.120 --> 01:55:47.120]   This is the colorized version.
[01:55:47.120 --> 01:55:48.640]   How love old colorized stuff.
[01:55:48.640 --> 01:55:49.640]   De-oldified.
[01:55:49.640 --> 01:55:50.640]   De-oldified.
[01:55:50.640 --> 01:55:51.640]   Wow.
[01:55:51.640 --> 01:55:52.640]   Wow.
[01:55:52.640 --> 01:55:53.640]   What's kind of cool...
[01:55:53.640 --> 01:55:56.280]   You see, that to me is more impressive.
[01:55:56.280 --> 01:55:57.280]   It's amazing.
[01:55:57.280 --> 01:55:59.520]   I love that stuff.
[01:55:59.520 --> 01:56:00.520]   Yeah.
[01:56:00.520 --> 01:56:01.520]   Yeah.
[01:56:01.520 --> 01:56:05.240]   I mean, actually, like on white one, I prefer the old one because I feel like I'm watching
[01:56:05.240 --> 01:56:07.920]   something that's made at the time it was made.
[01:56:07.920 --> 01:56:08.920]   Yeah, but it's...
[01:56:08.920 --> 01:56:11.520]   Yes, it's looking like a Hollywood film.
[01:56:11.520 --> 01:56:12.520]   Yeah.
[01:56:12.520 --> 01:56:13.520]   I love...
[01:56:13.520 --> 01:56:14.520]   This gives me more.
[01:56:14.520 --> 01:56:18.920]   I love seeing, knowing that these people are dressed in what they thought was normal
[01:56:18.920 --> 01:56:19.920]   clothing.
[01:56:19.920 --> 01:56:20.920]   I love...
[01:56:20.920 --> 01:56:23.520]   You can never wear that in Southern California.
[01:56:23.520 --> 01:56:24.520]   He is dying.
[01:56:24.520 --> 01:56:26.480]   Look at that lady.
[01:56:26.480 --> 01:56:30.060]   I just love the idea that this is actually...
[01:56:30.060 --> 01:56:36.920]   These are how factory workers dressed in the 1890s with petticoats and big hats with frills
[01:56:36.920 --> 01:56:38.880]   and that's just wild.
[01:56:38.880 --> 01:56:39.880]   To me.
[01:56:39.880 --> 01:56:42.560]   Haven't you watched Dickinson on Apple TV Plus?
[01:56:42.560 --> 01:56:43.760]   They all dress like this.
[01:56:43.760 --> 01:56:44.760]   Yeah, no.
[01:56:44.760 --> 01:56:45.760]   And they all...
[01:56:45.760 --> 01:56:47.560]   I got this on my list, on my list.
[01:56:47.560 --> 01:56:48.560]   Yeah.
[01:56:48.560 --> 01:56:49.560]   Look, there's...
[01:56:49.560 --> 01:56:51.560]   There's Hailey Steinfeld over there.
[01:56:51.560 --> 01:56:54.560]   Hey, did you guys watch the new...
[01:56:54.560 --> 01:56:55.560]   Do you think...
[01:56:55.560 --> 01:56:56.560]   Yes, I did.
[01:56:56.560 --> 01:56:57.560]   What do you think?
[01:56:57.560 --> 01:56:58.560]   I totally did.
[01:56:58.560 --> 01:57:02.160]   Well, let's just make sure we're talking about the same new thing.
[01:57:02.160 --> 01:57:05.600]   There's a lot of new stuff in the world, but you mean the new game thing on Apple TV?
[01:57:05.600 --> 01:57:06.600]   Yeah.
[01:57:06.600 --> 01:57:07.600]   It's their...
[01:57:07.600 --> 01:57:08.600]   Silicon Valley.
[01:57:08.600 --> 01:57:09.600]   What is it?
[01:57:09.600 --> 01:57:11.560]   Mythic Quest, Raven's something banquet.
[01:57:11.560 --> 01:57:12.560]   It's great.
[01:57:12.560 --> 01:57:13.560]   You liked it.
[01:57:13.560 --> 01:57:14.560]   I love it.
[01:57:14.560 --> 01:57:17.560]   Yeah, but I mean, I'm a huge RPG nerd as well.
[01:57:17.560 --> 01:57:22.740]   And so for me, it's just like all these little tropes that are planted there, clearly by someone
[01:57:22.740 --> 01:57:26.040]   who knows what they're talking about in the writing room.
[01:57:26.040 --> 01:57:27.040]   It's great.
[01:57:27.040 --> 01:57:28.040]   The first episode in itself.
[01:57:28.040 --> 01:57:29.040]   Brother Blizzard.
[01:57:29.040 --> 01:57:30.040]   Just sums everything up.
[01:57:30.040 --> 01:57:31.040]   Yeah, yeah, yeah.
[01:57:31.040 --> 01:57:32.040]   Is it just like Blizzard?
[01:57:32.040 --> 01:57:33.040]   I feel...
[01:57:33.040 --> 01:57:35.040]   He's like, I just feel personally attacked by this show.
[01:57:35.040 --> 01:57:36.040]   It's funny.
[01:57:36.040 --> 01:57:38.760]   He's like, it's really funny, but oh my God, it's just...
[01:57:38.760 --> 01:57:39.760]   Wow.
[01:57:39.760 --> 01:57:44.480]   This is the thing that's really funny about it is that even though it's not explicitly
[01:57:44.480 --> 01:57:51.240]   made mentioned in the series, is that the game, the fictional game in the series is obviously
[01:57:51.240 --> 01:57:55.600]   a free to play game that's monetized by in-app.
[01:57:55.600 --> 01:57:57.720]   Yeah, they talk about loot boxes.
[01:57:57.720 --> 01:57:59.240]   They talk about loot boxes.
[01:57:59.240 --> 01:58:03.960]   They talk about getting people to pay for these things, but in a way that you tend not
[01:58:03.960 --> 01:58:08.560]   to hear people talk about in games that are paid up front and have optional in-app things.
[01:58:08.560 --> 01:58:13.880]   So I think it's very much of the time and will probably date quite badly in the future,
[01:58:13.880 --> 01:58:16.840]   but it's a brilliantly written, very, very fast show.
[01:58:16.840 --> 01:58:17.840]   I love it.
[01:58:17.840 --> 01:58:18.840]   Mike.
[01:58:18.840 --> 01:58:19.840]   Hey, can I ask a question?
[01:58:19.840 --> 01:58:20.840]   Yes.
[01:58:20.840 --> 01:58:25.400]   How was the opportunity missed to launch that game on iOS, on iPad?
[01:58:25.400 --> 01:58:26.400]   Oh, I know.
[01:58:26.400 --> 01:58:27.400]   Capital Arcade.
[01:58:27.400 --> 01:58:28.400]   Maybe this coming.
[01:58:28.400 --> 01:58:29.400]   Maybe.
[01:58:29.400 --> 01:58:30.400]   Maybe.
[01:58:30.400 --> 01:58:32.440]   I mean, if it's popular, I guess maybe they would consider launching it if it was a popular
[01:58:32.440 --> 01:58:33.440]   season.
[01:58:33.440 --> 01:58:41.000]   I would absolutely love it, but it's kind of like it feels like an amalgam of just every
[01:58:41.000 --> 01:58:46.480]   trope in MMORPGs and free to play, you know, that you see in the game world.
[01:58:46.480 --> 01:58:50.200]   But they obviously had to make a game because a lot of the interstitials and things in the
[01:58:50.200 --> 01:58:51.200]   establishing shots.
[01:58:51.200 --> 01:58:52.200]   Yeah.
[01:58:52.200 --> 01:58:56.800]   I thought they might have just used, I don't know, Skyrim or something to do it, but maybe-
[01:58:56.800 --> 01:58:58.840]   The modding, yeah, modding another game.
[01:58:58.840 --> 01:59:03.520]   Truthfully, with Unity these days or the Unreal Engine, you probably could with a minimum
[01:59:03.520 --> 01:59:05.000]   of effort create those scenes.
[01:59:05.000 --> 01:59:07.920]   I mean, you don't have to actually have gameplay, you just need cutscenes.
[01:59:07.920 --> 01:59:08.920]   Yeah.
[01:59:08.920 --> 01:59:11.200]   I'll just use some kind of like, mission, I'm on a private server.
[01:59:11.200 --> 01:59:12.200]   Yeah.
[01:59:12.200 --> 01:59:13.200]   Yeah.
[01:59:13.200 --> 01:59:14.200]   I'd love it.
[01:59:14.200 --> 01:59:15.360]   I never thought of that, actually, but you're right.
[01:59:15.360 --> 01:59:17.160]   They should totally have released that game.
[01:59:17.160 --> 01:59:18.160]   Who should have been the game?
[01:59:18.160 --> 01:59:20.360]   That's the game on Apple Arcade that you should launch, right?
[01:59:20.360 --> 01:59:22.680]   That's like, I mean, get your corporate synergy going.
[01:59:22.680 --> 01:59:23.680]   Let's go, people.
[01:59:23.680 --> 01:59:24.680]   Then the live with love done.
[01:59:24.680 --> 01:59:26.480]   The selection on their in-app purchases.
[01:59:26.480 --> 01:59:27.480]   Yeah.
[01:59:27.480 --> 01:59:28.480]   Yeah.
[01:59:28.480 --> 01:59:31.280]   So, I'm going to enjoy the send up of PewDiePie.
[01:59:31.280 --> 01:59:37.800]   There's a 14-year-old YouTuber who apparently reeled such power that they're all watching
[01:59:37.800 --> 01:59:44.080]   his for his review on 10 books because he can make or break a $100 million game.
[01:59:44.080 --> 01:59:48.240]   This 14-year-old who's yelling at his mom, "I'm streaming here!"
[01:59:48.240 --> 01:59:50.240]   What is he?
[01:59:50.240 --> 01:59:52.480]   What's his name is Pootie Shoe?
[01:59:52.480 --> 01:59:53.480]   Pootie Shoe, yeah.
[01:59:53.480 --> 01:59:54.480]   Something like that.
[01:59:54.480 --> 01:59:55.480]   Yeah.
[01:59:55.480 --> 01:59:56.480]   I'm just rooting for his downfall.
[01:59:56.480 --> 01:59:57.480]   Oh, he's so hard.
[01:59:57.480 --> 01:59:59.360]   I'm going to see that kid taken down.
[01:59:59.360 --> 02:00:01.720]   Oh, this is so horrible.
[02:00:01.720 --> 02:00:02.720]   All right.
[02:00:02.720 --> 02:00:05.680]   I don't mean to wish ill on children of course.
[02:00:05.680 --> 02:00:07.880]   No, well, it's a fictional child.
[02:00:07.880 --> 02:00:09.880]   All right, good.
[02:00:09.880 --> 02:00:11.600]   Did you, you haven't seen it, Ashley?
[02:00:11.600 --> 02:00:15.880]   No, I've seen the, I watched the first episode of it.
[02:00:15.880 --> 02:00:16.880]   Yeah, I did.
[02:00:16.880 --> 02:00:20.840]   I only watched the first episode, but I did, I had mixed feelings, but there were enough
[02:00:20.840 --> 02:00:25.520]   really laugh at loud moments and there was enough, you know, of that, recognizing it
[02:00:25.520 --> 02:00:27.440]   that I thought, "Oh, that was a good inside joke."
[02:00:27.440 --> 02:00:28.440]   Yeah.
[02:00:28.440 --> 02:00:30.040]   I kind of enjoyed it.
[02:00:30.040 --> 02:00:33.560]   I'm not sure I like the people in it much.
[02:00:33.560 --> 02:00:34.560]   I'm dying to see it now.
[02:00:34.560 --> 02:00:35.560]   I haven't seen it.
[02:00:35.560 --> 02:00:38.120]   I think my refrigerator knocked my Apple TV offline.
[02:00:38.120 --> 02:00:39.120]   Oh, hell.
[02:00:39.120 --> 02:00:42.560]   I think you're probably right.
[02:00:42.560 --> 02:00:46.480]   Ladies and gentlemen, I want to thank this panel.
[02:00:46.480 --> 02:00:47.680]   You guys are great.
[02:00:47.680 --> 02:00:51.520]   Nate is surviving Storm C. Ciali, Ciara.
[02:00:51.520 --> 02:00:53.520]   I'm not laughing.
[02:00:53.520 --> 02:00:54.520]   The wind is blowing.
[02:00:54.520 --> 02:01:00.960]   I think the night is, it's pretty much a gothic horror show in Nate World.
[02:01:00.960 --> 02:01:03.040]   So thank you, Nate Langston, for being here.
[02:01:03.040 --> 02:01:09.600]   It is Nate World, also known as the depths of Hertfordshire, which is where I live.
[02:01:09.600 --> 02:01:12.120]   The depths of Hertfordshire.
[02:01:12.120 --> 02:01:14.480]   That's a phrase you don't hear all the time.
[02:01:14.480 --> 02:01:16.520]   Yes, it's true.
[02:01:16.520 --> 02:01:19.040]   Nate's podcast is text message.
[02:01:19.040 --> 02:01:23.800]   You'll find it at UKtechshow.com, the UK-focused technology podcast.
[02:01:23.800 --> 02:01:27.040]   You know it's UK-focused because he spells "focused wrong."
[02:01:27.040 --> 02:01:30.240]   You're silly, Britt.
[02:01:30.240 --> 02:01:31.760]   Yeah, it's true.
[02:01:31.760 --> 02:01:35.360]   Or as he might say, you spell it right.
[02:01:35.360 --> 02:01:36.360]   Yeah.
[02:01:36.360 --> 02:01:37.360]   Yeah.
[02:01:37.360 --> 02:01:38.360]   Thank you, Nate.
[02:01:38.360 --> 02:01:39.360]   It's great to have you here.
[02:01:39.360 --> 02:01:40.360]   I appreciate it.
[02:01:40.360 --> 02:01:41.360]   Thank you very much, Leo.
[02:01:41.360 --> 02:01:42.360]   The wonderful Denise Howell.
[02:01:42.360 --> 02:01:44.720]   You'll find her at Denise Howell.info.
[02:01:44.720 --> 02:01:47.400]   And I hope more on Twit than ever before.
[02:01:47.400 --> 02:01:50.200]   We're sorry to have lost your presence on triangulation.
[02:01:50.200 --> 02:01:52.800]   I'm sorry you lost the show.
[02:01:52.800 --> 02:01:53.800]   Congratulations.
[02:01:53.800 --> 02:01:56.920]   Always been one of my favorite shows on the network and I miss it.
[02:01:56.920 --> 02:02:02.800]   Well, so if you, I think what we're going to do is just whenever we got somebody we really
[02:02:02.800 --> 02:02:05.520]   have to talk to, we'll just resurrect it.
[02:02:05.520 --> 02:02:09.600]   So if you've got somebody you just have to talk to, just let us know and we'll do an
[02:02:09.600 --> 02:02:10.600]   episode.
[02:02:10.600 --> 02:02:11.600]   Cool.
[02:02:11.600 --> 02:02:15.040]   I think Carsten emailed me that Steven Levy's got a new book coming out and Steven says,
[02:02:15.040 --> 02:02:16.560]   "I have to do it."
[02:02:16.560 --> 02:02:18.280]   And I'm not going to say no to Steven Levy.
[02:02:18.280 --> 02:02:20.920]   So I think we're going to do triangulation specials.
[02:02:20.920 --> 02:02:21.920]   Yay.
[02:02:21.920 --> 02:02:23.840]   So let us know.
[02:02:23.840 --> 02:02:26.680]   His new book sounds fascinating.
[02:02:26.680 --> 02:02:29.760]   I can't wait to talk to him.
[02:02:29.760 --> 02:02:30.760]   Are we set up to do that?
[02:02:30.760 --> 02:02:32.520]   Yeah, that'll be in March.
[02:02:32.520 --> 02:02:33.520]   In March.
[02:02:33.520 --> 02:02:37.160]   About three or four weeks from now.
[02:02:37.160 --> 02:02:38.160]   Okay.
[02:02:38.160 --> 02:02:39.560]   Yeah, that'll be, what's the name of his new book?
[02:02:39.560 --> 02:02:40.560]   Do you know?
[02:02:40.560 --> 02:02:42.960]   Off the top of your head, I don't remember.
[02:02:42.960 --> 02:02:44.160]   It's not out yet, I think.
[02:02:44.160 --> 02:02:45.160]   So we'll have to just--
[02:02:45.160 --> 02:02:47.440]   Not out till first week in March.
[02:02:47.440 --> 02:02:48.440]   Yeah.
[02:02:48.440 --> 02:02:49.440]   Cool.
[02:02:49.440 --> 02:02:50.920]   Thank you so much, Ashley.
[02:02:50.920 --> 02:02:51.920]   Oh, it's called Facebook.
[02:02:51.920 --> 02:02:54.360]   Oh, it's about Facebook.
[02:02:54.360 --> 02:02:55.360]   That's right.
[02:02:55.360 --> 02:02:59.480]   He was embedded at Mark Zuckerberg's side.
[02:02:59.480 --> 02:03:00.640]   This should be very interesting.
[02:03:00.640 --> 02:03:03.000]   So we couldn't say no to that.
[02:03:03.000 --> 02:03:06.560]   So yeah, so if Denise, you get somebody you really want, don't hesitate.
[02:03:06.560 --> 02:03:07.560]   We'll get one.
[02:03:07.560 --> 02:03:08.560]   Cool.
[02:03:08.560 --> 02:03:09.560]   Yeah.
[02:03:09.560 --> 02:03:13.080]   Ashley, it is so great to see you, senior producer at CNET.
[02:03:13.080 --> 02:03:15.320]   Motherhood has done wonders for you.
[02:03:15.320 --> 02:03:17.720]   I'm glad you have a gothic child.
[02:03:17.720 --> 02:03:20.640]   It's a delight.
[02:03:20.640 --> 02:03:24.360]   And we're just thrilled that you're here and come back again very soon.
[02:03:24.360 --> 02:03:25.360]   All three of you.
[02:03:25.360 --> 02:03:26.360]   Thanks for having me back.
[02:03:26.360 --> 02:03:27.360]   Yeah.
[02:03:27.360 --> 02:03:29.760]   It's been a minute, so it's nice to come back and hang out.
[02:03:29.760 --> 02:03:30.760]   It's been a long time.
[02:03:30.760 --> 02:03:31.760]   Yeah.
[02:03:31.760 --> 02:03:32.760]   Yeah.
[02:03:32.760 --> 02:03:37.840]   We do Twitter every Sunday afternoon, 230 Pacific, 530 Eastern, 2230 UTC.
[02:03:37.840 --> 02:03:42.200]   That means you can watch it live if you're up at that hour, which you probably are if
[02:03:42.200 --> 02:03:44.040]   you're in the Western Hemisphere.
[02:03:44.040 --> 02:03:48.600]   All you have to do is go to twitter.tv/live, live audio and video made available.
[02:03:48.600 --> 02:03:49.600]   You can be in Studio 2.
[02:03:49.600 --> 02:03:52.640]   We had a great studio audience visiting us from all over.
[02:03:52.640 --> 02:03:56.760]   Matthew's from London, from Tom is from...
[02:03:56.760 --> 02:03:57.760]   Where?
[02:03:57.760 --> 02:03:58.760]   L.A.
[02:03:58.760 --> 02:03:59.760]   San Jose.
[02:03:59.760 --> 02:04:00.760]   San Jose.
[02:04:00.760 --> 02:04:03.440]   That's right.
[02:04:03.440 --> 02:04:05.360]   And also from Santa Clara.
[02:04:05.360 --> 02:04:06.360]   I have the cards here.
[02:04:06.360 --> 02:04:09.160]   I don't know why I'm trying to memorize this.
[02:04:09.160 --> 02:04:12.040]   We've got Robert and Christina visiting.
[02:04:12.040 --> 02:04:13.040]   Great to have you.
[02:04:13.040 --> 02:04:14.520]   It's Robert's 50th today.
[02:04:14.520 --> 02:04:15.520]   Happy birthday, Robert.
[02:04:15.520 --> 02:04:22.640]   If you want to celebrate a special occasion, just email tickets at twitter.tv.
[02:04:22.640 --> 02:04:27.880]   And we will have your server bring you a cake with a candle in it a little later on.
[02:04:27.880 --> 02:04:31.520]   No, sorry.
[02:04:31.520 --> 02:04:33.440]   If you can't watch live...
[02:04:33.440 --> 02:04:37.320]   By the way, I should mention if you're watching live, join the chatroom irc.twit.tv.
[02:04:37.320 --> 02:04:41.920]   But if you can't watch live on demand versions of the show available at the website twitter.tv,
[02:04:41.920 --> 02:04:43.320]   we have a YouTube channel.
[02:04:43.320 --> 02:04:46.560]   And of course your best bet is subscribe.
[02:04:46.560 --> 02:04:48.120]   Subscribe.
[02:04:48.120 --> 02:04:49.560]   Pick your favorite podcast app.
[02:04:49.560 --> 02:04:52.000]   Subscribe to the audio or the video stream.
[02:04:52.000 --> 02:04:53.720]   That way you'll always have it.
[02:04:53.720 --> 02:04:56.160]   You know, you get in the car on Monday morning and you go, "What can I listen to?"
[02:04:56.160 --> 02:04:58.360]   "Oh yes, there's a brand new Twit."
[02:04:58.360 --> 02:04:59.680]   We appreciate if you do that.
[02:04:59.680 --> 02:05:01.960]   Thank you very much for being here everybody.
[02:05:01.960 --> 02:05:03.840]   Have a wonderful evening and we'll see you next time.
[02:05:03.840 --> 02:05:04.840]   Another Twit.
[02:05:04.840 --> 02:05:05.840]   Is it a chance?
[02:05:05.840 --> 02:05:06.840]   This is amazing.
[02:05:06.840 --> 02:05:13.840]   Oh, one thing.
[02:05:13.840 --> 02:05:21.840]   Oh my God.
[02:05:21.840 --> 02:05:23.440]   I love this bib.
[02:05:23.440 --> 02:05:24.840]   It says "loved."
[02:05:24.840 --> 02:05:25.840]   Oh, yeah.
[02:05:25.840 --> 02:05:29.640]   Oh, look at those blue eyes.
[02:05:29.640 --> 02:05:31.840]   Oh my goodness.
[02:05:31.840 --> 02:05:33.840]   He smiled at us.
[02:05:33.840 --> 02:05:34.840]   He's a very happy smiley.
[02:05:34.840 --> 02:05:36.840]   First podcast appearance.
[02:05:36.840 --> 02:05:37.840]   It is.
[02:05:37.840 --> 02:05:39.840]   Hold hand at this.
[02:05:39.840 --> 02:05:41.840]   Oh, that's so cute.
[02:05:41.840 --> 02:05:44.680]   I bet Ashley you don't let Wolfie in that room though.
[02:05:44.680 --> 02:05:46.920]   There's all those figures on the shelf he would love.
[02:05:46.920 --> 02:05:49.520]   He doesn't come in here very often.
[02:05:49.520 --> 02:05:52.360]   Today was the first time he'd ever been in this room.
[02:05:52.360 --> 02:05:53.360]   Yeah.
[02:05:53.360 --> 02:05:54.360]   Yeah, he says, "What is this?
[02:05:54.360 --> 02:05:56.720]   Where did all these toys come from?"
[02:05:56.720 --> 02:05:57.720]   Bye-bye.
[02:05:57.720 --> 02:05:58.720]   See, bye-bye.
[02:05:58.720 --> 02:05:59.720]   Oh my God.
[02:05:59.720 --> 02:06:02.640]   Mommy, you have a room full of toys and you never told me.
[02:06:02.640 --> 02:06:03.640]   I know.
[02:06:03.640 --> 02:06:04.640]   Oh my gosh.
[02:06:04.640 --> 02:06:05.640]   All the cool toys too.
[02:06:05.640 --> 02:06:06.640]   All the good stuff.
[02:06:06.640 --> 02:06:07.640]   Bye-bye.
[02:06:07.640 --> 02:06:08.640]   It's a step baby.
[02:06:08.640 --> 02:06:09.640]   The step baby.
[02:06:09.640 --> 02:06:10.640]   Bye-bye, Wolf gang.
[02:06:10.640 --> 02:06:11.640]   Bye-bye.
[02:06:11.640 --> 02:06:12.640]   Bye-bye, darling.
[02:06:12.640 --> 02:06:13.640]   Oh, what a sweetie.
[02:06:13.640 --> 02:06:14.640]   Oh, yeah.

