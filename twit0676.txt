
[00:00:00.000 --> 00:00:02.000]   [MUSIC]
[00:00:02.000 --> 00:00:03.360]   >> I'm exhausted.
[00:00:03.360 --> 00:00:04.200]   It's time for Twit.
[00:00:04.200 --> 00:00:05.800]   We have an amazing show.
[00:00:05.800 --> 00:00:08.440]   Greg Farrow is back from the Packet Pusher Network,
[00:00:08.440 --> 00:00:12.080]   our futurist Amy Webb from the Signals are talking.
[00:00:12.080 --> 00:00:14.320]   We are going to talk about everything from voting machine
[00:00:14.320 --> 00:00:17.760]   and vulnerabilities to the real reason why Intel and
[00:00:17.760 --> 00:00:20.760]   the MacBook Pro are a match made in hell.
[00:00:20.760 --> 00:00:22.400]   At least this is hot as hell.
[00:00:22.400 --> 00:00:26.280]   And China's secret plans, it's all coming up next on Twit.
[00:00:26.280 --> 00:00:29.440]   [MUSIC]
[00:00:29.440 --> 00:00:31.560]   >> Netcast you love.
[00:00:31.560 --> 00:00:32.840]   >> From people you trust.
[00:00:32.840 --> 00:00:36.680]   [MUSIC]
[00:00:36.680 --> 00:00:39.040]   >> This is Twit.
[00:00:39.040 --> 00:00:42.540]   >> Bandwidth for this weekend tech is provided by cash fly at
[00:00:42.540 --> 00:00:45.360]   cachefly.com.
[00:00:45.360 --> 00:00:50.600]   [MUSIC]
[00:00:50.600 --> 00:00:52.920]   This is Twit this weekend tech.
[00:00:52.920 --> 00:00:58.360]   Episode 676 recorded Sunday, July 22nd, 2018,
[00:00:58.360 --> 00:01:00.640]   following a sleep as the robots wake up.
[00:01:00.640 --> 00:01:04.160]   This week, attack is brought to you by Quip,
[00:01:04.160 --> 00:01:06.760]   the first subscription electric toothbrush accepted by the
[00:01:06.760 --> 00:01:08.600]   American Dental Association.
[00:01:08.600 --> 00:01:12.440]   Visit getquip.com/twit to get your first refill pack free when
[00:01:12.440 --> 00:01:14.760]   you purchase any Quip electric toothbrush.
[00:01:14.760 --> 00:01:17.800]   And buy a rocket mortgage from Quicken Loans, introducing
[00:01:17.800 --> 00:01:21.000]   rate shield approval if you're in the market to buy a home.
[00:01:21.000 --> 00:01:23.880]   Rate shield approval locks up your rate for up to 90 days while
[00:01:23.880 --> 00:01:24.760]   you shop.
[00:01:24.760 --> 00:01:25.760]   It's a real game changer.
[00:01:25.760 --> 00:01:29.880]   Learn more and get started at rocketmortgage.com/twit2.
[00:01:29.880 --> 00:01:34.400]   And buy stamps.com by imprint real US postage the instant you
[00:01:34.400 --> 00:01:36.480]   need it right from your desk.
[00:01:36.480 --> 00:01:38.440]   To get our special offer go to stamps.com,
[00:01:38.440 --> 00:01:41.320]   click the microphone and enter Twit.
[00:01:41.320 --> 00:01:43.680]   And buy the Ring Video Doorbell.
[00:01:43.680 --> 00:01:45.560]   Stop crying before it happens.
[00:01:45.560 --> 00:01:47.760]   And I'll make your neighborhood safer with Ring.
[00:01:47.760 --> 00:01:52.560]   Go to Ring.com/twit and get it to $150 off a Ring of Security Kit.
[00:01:52.560 --> 00:01:55.680]   [MUSIC]
[00:01:55.680 --> 00:01:57.520]   It's time for Twit this week in Tech, the show where we get
[00:01:57.520 --> 00:02:00.480]   together with some of the best, brightest, most fun people
[00:02:00.480 --> 00:02:03.600]   in technology and talk about the week's tech news.
[00:02:03.600 --> 00:02:06.960]   And we decided today, normally we have a four person panel,
[00:02:06.960 --> 00:02:10.480]   we decided today we are going to do three because I have two
[00:02:10.480 --> 00:02:11.400]   of my favorite people.
[00:02:11.400 --> 00:02:15.080]   And I want to give them equal time to talk, talk, talk,
[00:02:15.080 --> 00:02:18.360]   starting with Amy Webb, who I just adore.
[00:02:18.360 --> 00:02:21.800]   Amy, oh, you've redecorated.
[00:02:21.800 --> 00:02:23.240]   Oh, I'm at home today.
[00:02:23.240 --> 00:02:24.480]   Oh, normally.
[00:02:24.480 --> 00:02:25.720]   Yeah, different stuff behind me.
[00:02:25.720 --> 00:02:29.640]   We see her whiteboard predicting the future.
[00:02:29.640 --> 00:02:34.600]   She is a futurist, which is a very handy person to know.
[00:02:34.600 --> 00:02:38.040]   She is a founder of the Institute for the Future
[00:02:38.040 --> 00:02:41.200]   and the author of the fabulous Signals
[00:02:41.200 --> 00:02:44.600]   are talking why today's fringe is tomorrow's mainstream.
[00:02:44.600 --> 00:02:48.760]   I'm happy to see a Washington Post bestseller,
[00:02:48.760 --> 00:02:52.320]   a winner of the 2017 Gold Axiom Award, Fast Company's
[00:02:52.320 --> 00:02:55.600]   Best Books of 2016, Amazon's Best Books of 2016.
[00:02:55.600 --> 00:02:58.960]   Congratulations, you did very well with this.
[00:02:58.960 --> 00:02:59.800]   Thank you.
[00:02:59.800 --> 00:03:00.800]   Nice job.
[00:03:00.800 --> 00:03:01.560]   Thanks.
[00:03:01.560 --> 00:03:02.120]   Nice job.
[00:03:02.120 --> 00:03:06.240]   Her future today Institute is at futuretodayinstitute.com.
[00:03:06.240 --> 00:03:06.880]   I got the name wrong.
[00:03:06.880 --> 00:03:09.160]   Futuretodayinstitute.com.
[00:03:09.160 --> 00:03:10.320]   Thank you, Amy, for being here.
[00:03:10.320 --> 00:03:13.200]   We were talking before the show because Amy lived in Japan.
[00:03:13.200 --> 00:03:15.600]   How many years did you live in Japan?
[00:03:15.600 --> 00:03:17.760]   I was in Japan for six in China for one.
[00:03:17.760 --> 00:03:18.560]   Wow.
[00:03:18.560 --> 00:03:20.280]   And actually, the beginning of the book,
[00:03:20.280 --> 00:03:23.400]   The Signals Are Talking, talks about how, by living in Japan,
[00:03:23.400 --> 00:03:25.320]   you were able to get a sense of what the future was going
[00:03:25.320 --> 00:03:29.200]   to look like because it was already happening in Akihabara.
[00:03:29.200 --> 00:03:30.760]   And you told me not to go to Akihabara,
[00:03:30.760 --> 00:03:33.120]   and that was good advice.
[00:03:33.120 --> 00:03:34.320]   But I love Tokyo.
[00:03:34.320 --> 00:03:35.400]   I had a great time anyway.
[00:03:35.400 --> 00:03:36.400]   Thank you for all the tips.
[00:03:36.400 --> 00:03:36.960]   I'm happy you.
[00:03:36.960 --> 00:03:37.320]   Yeah.
[00:03:37.320 --> 00:03:39.080]   It was so much fun.
[00:03:39.080 --> 00:03:41.200]   Also with us, another favorite of mine.
[00:03:41.200 --> 00:03:43.640]   And I love it when Greg Farrow is in studio.
[00:03:43.640 --> 00:03:44.400]   It's good to be here.
[00:03:44.400 --> 00:03:46.840]   From the Packet Pusher's network, Ethereal Mind.
[00:03:46.840 --> 00:03:48.440]   I've been staying away from traveling.
[00:03:48.440 --> 00:03:49.520]   But if I'm here, I'm here.
[00:03:49.520 --> 00:03:50.120]   I'm coming.
[00:03:50.120 --> 00:03:53.960]   I beg you, please always make an effort to get here.
[00:03:53.960 --> 00:03:55.560]   And between these two, I mean, there's
[00:03:55.560 --> 00:03:59.240]   no reason to have another person because you guys know it all.
[00:03:59.240 --> 00:04:01.560]   And I just wanted to make sure that there was no question
[00:04:01.560 --> 00:04:03.520]   that if you had something to say, you would get to--
[00:04:03.520 --> 00:04:06.640]   I'm just going to shut up and let you talk.
[00:04:06.640 --> 00:04:07.920]   I had two of my favorite people, so this
[00:04:07.920 --> 00:04:08.440]   can be a lot of--
[00:04:08.440 --> 00:04:10.160]   It's a pretty high bar.
[00:04:10.160 --> 00:04:12.240]   No, not for you, Greg.
[00:04:12.240 --> 00:04:14.680]   Truthfully, we don't really need Amy.
[00:04:14.680 --> 00:04:15.960]   warn me.
[00:04:15.960 --> 00:04:17.120]   Greg has his own podcast.
[00:04:17.120 --> 00:04:18.320]   Thank god.
[00:04:18.320 --> 00:04:20.000]   What brings you out here?
[00:04:20.000 --> 00:04:22.120]   I was out for Network Field Day, which
[00:04:22.120 --> 00:04:25.320]   is an event where we do seven vendors in three days.
[00:04:25.320 --> 00:04:28.640]   We get briefings, and we jump in a limo and drive between--
[00:04:28.640 --> 00:04:31.280]   Is this something you do for your company?
[00:04:31.280 --> 00:04:31.880]   It's a group.
[00:04:31.880 --> 00:04:33.480]   It's an influencer program.
[00:04:33.480 --> 00:04:35.360]   I work mostly in Enterprise IT.
[00:04:35.360 --> 00:04:37.360]   So my podcast, the Packet Pusher's podcast,
[00:04:37.360 --> 00:04:40.600]   is about Enterprise IT data networking and Enterprise IT
[00:04:40.600 --> 00:04:41.200]   technology--
[00:04:41.200 --> 00:04:42.560]   You go see Cisco?
[00:04:42.560 --> 00:04:43.720]   Yeah, not this time around.
[00:04:43.720 --> 00:04:44.080]   We saw--
[00:04:44.080 --> 00:04:46.440]   It's just about their back doors.
[00:04:46.440 --> 00:04:48.440]   I've been fairly critical about Cisco's back doors.
[00:04:48.440 --> 00:04:49.520]   Holy cow.
[00:04:49.520 --> 00:04:52.120]   They just discovered it with a sixth back door?
[00:04:52.120 --> 00:04:54.520]   Well, they do seem to have a systemic problem.
[00:04:54.520 --> 00:04:57.440]   They must have pretty bad development practices internally
[00:04:57.440 --> 00:04:59.120]   in that part of the company.
[00:04:59.120 --> 00:05:00.040]   Some parts--
[00:05:00.040 --> 00:05:01.600]   Fifth back door.
[00:05:01.600 --> 00:05:04.280]   But a hard-coded static back door.
[00:05:04.280 --> 00:05:05.000]   And we actually--
[00:05:05.000 --> 00:05:06.720]   In a particular product.
[00:05:06.720 --> 00:05:09.320]   They always come in one particular family of product,
[00:05:09.320 --> 00:05:11.080]   and it shows that that part of the company
[00:05:11.080 --> 00:05:13.480]   seems to have a fairly obsolete way of doing business.
[00:05:13.480 --> 00:05:15.440]   Is the Cisco policy sweet?
[00:05:15.440 --> 00:05:16.920]   Yeah, it's part of the DNA center.
[00:05:16.920 --> 00:05:18.640]   It's all based around Tomcat.
[00:05:18.640 --> 00:05:21.400]   I think we use this here.
[00:05:21.400 --> 00:05:24.080]   Would it be something that you would use as a--
[00:05:24.080 --> 00:05:24.920]   No.
[00:05:24.920 --> 00:05:27.120]   No, it's very much an Enterprise IT.
[00:05:27.120 --> 00:05:28.720]   Well, we use Cisco filters.
[00:05:28.720 --> 00:05:31.960]   I mean, when I get blocked from my porn sites,
[00:05:31.960 --> 00:05:33.160]   it's always Cisco's.
[00:05:33.160 --> 00:05:34.920]   That'll be their security products.
[00:05:34.920 --> 00:05:35.420]   Yeah.
[00:05:35.420 --> 00:05:36.920]   Umbrella, it's called Umbrella.
[00:05:36.920 --> 00:05:39.240]   Umbrella, yeah, which is their online proxy service.
[00:05:39.240 --> 00:05:40.880]   Damn, you're Umbrella.
[00:05:40.880 --> 00:05:42.240]   That's a company.
[00:05:42.240 --> 00:05:44.600]   And then Cisco's really an organization,
[00:05:44.600 --> 00:05:45.800]   like lots of big companies, which
[00:05:45.800 --> 00:05:47.640]   has dozens of companies inside it.
[00:05:47.640 --> 00:05:49.160]   And some of them are good companies, and some of them
[00:05:49.160 --> 00:05:50.520]   are bad companies.
[00:05:50.520 --> 00:05:53.560]   This weakness has been the same business unit over and over,
[00:05:53.560 --> 00:05:54.920]   delivering pretty substandard--
[00:05:54.920 --> 00:05:57.120]   Five backdoors in five months.
[00:05:57.120 --> 00:05:57.880]   Yep.
[00:05:57.880 --> 00:05:58.400]   Yep.
[00:05:58.400 --> 00:06:00.280]   Which sort of says there's something wrong with that business
[00:06:00.280 --> 00:06:00.840]   unit, doesn't it?
[00:06:00.840 --> 00:06:01.840]   Yeah.
[00:06:01.840 --> 00:06:04.720]   I always think it's interesting when a company discovers
[00:06:04.720 --> 00:06:06.040]   a back door.
[00:06:06.040 --> 00:06:08.080]   You put that in their quotes, didn't you?
[00:06:08.080 --> 00:06:09.520]   Well, yeah, so it's my--
[00:06:09.520 --> 00:06:10.520]   [LAUGHTER]
[00:06:10.520 --> 00:06:15.480]   I mean, it seems odd to me that unless the organization is
[00:06:15.480 --> 00:06:18.880]   just completely misaligned, and there's no communication,
[00:06:18.880 --> 00:06:23.560]   but it seems odd that a company would discover
[00:06:23.560 --> 00:06:25.520]   a back door five times.
[00:06:25.520 --> 00:06:30.640]   It's like the-- it's like how Symantec discovered.
[00:06:30.640 --> 00:06:33.400]   This is a current story, but it goes back in time.
[00:06:33.400 --> 00:06:39.000]   In 2006, there was an election machine company
[00:06:39.000 --> 00:06:41.640]   that was putting remote access-- PC anywhere,
[00:06:41.640 --> 00:06:43.640]   a semantic product-- on their voting machines.
[00:06:43.640 --> 00:06:45.880]   Now, these weren't the machines that voters used.
[00:06:45.880 --> 00:06:47.920]   These were machines that counted the votes.
[00:06:47.920 --> 00:06:52.960]   So the fact that these machines were not only on the internet,
[00:06:52.960 --> 00:06:56.160]   but had remote access software running on them--
[00:06:56.160 --> 00:06:56.920]   Yep.
[00:06:56.920 --> 00:06:58.320]   Now, this is really scary, because PC
[00:06:58.320 --> 00:07:01.360]   anywhere is about as secure as an open front door.
[00:07:01.360 --> 00:07:03.280]   Well, that's what made me think of this, because it turns out,
[00:07:03.280 --> 00:07:06.640]   in that time from 2006, the PC anywhere source code
[00:07:06.640 --> 00:07:08.040]   had been leaked out to hackers.
[00:07:08.040 --> 00:07:08.720]   That's right.
[00:07:08.720 --> 00:07:12.640]   But Symantec didn't tell anybody to 2012.
[00:07:12.640 --> 00:07:14.520]   So let me ask you a question, though.
[00:07:14.520 --> 00:07:16.920]   So let me play devil's advocate.
[00:07:16.920 --> 00:07:17.880]   So I'm Symantec.
[00:07:17.880 --> 00:07:20.360]   I'm Cisco, whatever.
[00:07:20.360 --> 00:07:23.520]   What other way is there, once you discover a vulnerability,
[00:07:23.520 --> 00:07:28.360]   to get firmware to a consumer like my dad, right, who is just
[00:07:28.360 --> 00:07:29.760]   not super tech savvy?
[00:07:29.760 --> 00:07:34.760]   And many people don't find and install updates
[00:07:34.760 --> 00:07:35.680]   on a regular basis.
[00:07:35.680 --> 00:07:39.920]   So isn't there a reasonable case
[00:07:39.920 --> 00:07:43.240]   to keep creating those back doors so that there's
[00:07:43.240 --> 00:07:45.480]   a way to ship the firmware updates?
[00:07:45.480 --> 00:07:46.960]   Well, the challenge should be is that if you're
[00:07:46.960 --> 00:07:48.640]   going to make a product like that,
[00:07:48.640 --> 00:07:50.920]   PC anywhere was a bodge to fix a bodge.
[00:07:50.920 --> 00:07:54.920]   So these election machines were often made out of Windows 95
[00:07:54.920 --> 00:07:55.920]   or Windows 2000s.
[00:07:55.920 --> 00:07:58.000]   Yeah, this was 2000 to 2006.
[00:07:58.000 --> 00:07:59.120]   So very likely something happened.
[00:07:59.120 --> 00:08:01.920]   There was no remote access tools in the operating system,
[00:08:01.920 --> 00:08:04.480]   and you had to apply something after the fact.
[00:08:04.480 --> 00:08:07.320]   My reading of the situation is that those voting machines aren't
[00:08:07.320 --> 00:08:08.440]   used very often.
[00:08:08.440 --> 00:08:10.640]   And it's on a high value contract.
[00:08:10.640 --> 00:08:13.680]   It was at the time the number one voting machine maker
[00:08:13.680 --> 00:08:14.160]   in the US.
[00:08:14.160 --> 00:08:15.360]   Yeah, but they're not big business.
[00:08:15.360 --> 00:08:17.280]   Like it's only like a tens of millions of dollars.
[00:08:17.280 --> 00:08:17.800]   Oh, yeah.
[00:08:17.800 --> 00:08:19.640]   Yeah, you're right, because it's just the county seat that
[00:08:19.640 --> 00:08:19.840]   has it.
[00:08:19.840 --> 00:08:22.960]   Yeah, see, the fundamental problem with IT security
[00:08:22.960 --> 00:08:25.520]   is that the IT security has to cost less than the thing
[00:08:25.520 --> 00:08:27.120]   you're protecting.
[00:08:27.120 --> 00:08:30.040]   But most IT security costs a whole lot of money.
[00:08:30.040 --> 00:08:32.360]   Like security companies are charging so much cash
[00:08:32.360 --> 00:08:35.600]   for their products and expecting to make so much money.
[00:08:35.600 --> 00:08:38.080]   And that people just don't buy it and deploy it.
[00:08:38.080 --> 00:08:42.320]   And I just point out, if you were going to pick one system
[00:08:42.320 --> 00:08:44.200]   that you wanted to be secure and reliable,
[00:08:44.200 --> 00:08:47.000]   it would be the system that counts votes.
[00:08:47.000 --> 00:08:49.480]   Yeah, but that's not got a dollar value in it.
[00:08:49.480 --> 00:08:52.080]   No, but it's not a value.
[00:08:52.080 --> 00:08:52.680]   Here's a 50-minute dollar.
[00:08:52.680 --> 00:08:55.760]   It also highlights the fact that all technology
[00:08:55.760 --> 00:08:57.920]   in the United States serves two masters.
[00:08:57.920 --> 00:09:00.680]   So it serves DC, and it serves Wall Street.
[00:09:00.680 --> 00:09:04.040]   And the problem is that most often
[00:09:04.040 --> 00:09:08.480]   the avenue upon which technology proceeds
[00:09:08.480 --> 00:09:10.720]   as it's getting better and better runs counter
[00:09:10.720 --> 00:09:13.160]   to the best long-term interests of--
[00:09:13.160 --> 00:09:14.200]   Oh, they're listening.
[00:09:14.200 --> 00:09:15.640]   Sorry, they're listening.
[00:09:15.640 --> 00:09:16.640]   It was Russia.
[00:09:16.640 --> 00:09:18.560]   Russia's calling, telling me to shut up.
[00:09:18.560 --> 00:09:19.880]   I'm letting their secrets.
[00:09:19.880 --> 00:09:22.320]   You raised an interesting point, because I did on Friday,
[00:09:22.320 --> 00:09:24.720]   I had an interview with Adam Fisher, who's got a new book
[00:09:24.720 --> 00:09:26.120]   called A Valley of Genius.
[00:09:26.120 --> 00:09:30.280]   It's interviews with 200 of the really interesting people
[00:09:30.280 --> 00:09:33.040]   who created Silicon Valley over the last 20 years.
[00:09:33.040 --> 00:09:35.360]   I mean, all the names, and it covers everything
[00:09:35.360 --> 00:09:38.480]   from Atari to Twitter and everything in between.
[00:09:38.480 --> 00:09:40.200]   It's a really wonderful book.
[00:09:40.200 --> 00:09:41.880]   He made an interesting observation.
[00:09:41.880 --> 00:09:46.560]   And maybe what you just said, it follows on what you just
[00:09:46.560 --> 00:09:51.360]   said, Amy, which is that until recently it was--
[00:09:51.360 --> 00:09:53.000]   you nailed it--
[00:09:53.000 --> 00:09:55.440]   was finance and government.
[00:09:55.440 --> 00:09:57.440]   It was Wall Street in Washington.
[00:09:57.440 --> 00:09:59.520]   Because we were talking about why New York City hates
[00:09:59.520 --> 00:10:02.320]   Silicon Valley, why the New York Times hates Silicon Valley.
[00:10:02.320 --> 00:10:04.280]   He says, it's becoming Silicon Valley that
[00:10:04.280 --> 00:10:06.800]   drives the agenda these days.
[00:10:06.800 --> 00:10:12.120]   And it's one of the reasons you see such antipathy,
[00:10:12.120 --> 00:10:15.160]   especially from East Coast media,
[00:10:15.160 --> 00:10:17.200]   to-- I like the Wall Street Journal in the New York Times--
[00:10:17.200 --> 00:10:18.480]   toward Silicon Valley.
[00:10:18.480 --> 00:10:21.800]   And why you see so many stories about programmers
[00:10:21.800 --> 00:10:24.120]   and all these morons.
[00:10:24.120 --> 00:10:26.960]   I remember this story The New York Times
[00:10:26.960 --> 00:10:30.880]   did about the blockchain houses where these silly people
[00:10:30.880 --> 00:10:32.320]   have Bitcoin Avenue.
[00:10:32.320 --> 00:10:34.360]   I mean, all of these people aren't that.
[00:10:34.360 --> 00:10:35.360]   They are that.
[00:10:35.360 --> 00:10:37.120]   They are certainly vulnerable.
[00:10:37.120 --> 00:10:38.080]   However--
[00:10:38.080 --> 00:10:41.200]   But they're also stupid and childish and immature.
[00:10:41.200 --> 00:10:43.720]   They have very little ground in the philosophy of--
[00:10:43.720 --> 00:10:44.720]   You read it from the X--
[00:10:44.720 --> 00:10:47.040]   I think it was in Vanity Fair.
[00:10:47.040 --> 00:10:48.640]   No, it was New York Magazine.
[00:10:48.640 --> 00:10:51.360]   Anyway, about the programmer culture that started Facebook.
[00:10:51.360 --> 00:10:54.160]   And they just-- there was one massive kegger, basically,
[00:10:54.160 --> 00:10:55.960]   for the first five years of Facebook.
[00:10:55.960 --> 00:10:57.400]   You could argue it still is today.
[00:10:57.400 --> 00:10:58.400]   It may still be.
[00:10:58.400 --> 00:11:00.440]   The point is, is that a lot of these organizations
[00:11:00.440 --> 00:11:04.080]   are run in this moral isolation, where--
[00:11:04.080 --> 00:11:05.680]   and there's not even any acknowledgment
[00:11:05.680 --> 00:11:07.200]   that humanity is actually a thing.
[00:11:07.200 --> 00:11:08.840]   And they're in there doing their thing and saying,
[00:11:08.840 --> 00:11:10.480]   the technology will change the world.
[00:11:10.480 --> 00:11:12.800]   And we believe that any technology we give you
[00:11:12.800 --> 00:11:15.360]   will make your life so much vastly better,
[00:11:15.360 --> 00:11:17.480]   while conveniently forgetting that most people don't want it,
[00:11:17.480 --> 00:11:19.840]   don't need it, didn't ask for it, don't--
[00:11:19.840 --> 00:11:22.920]   And I'll add one onto that.
[00:11:22.920 --> 00:11:26.280]   So there's more isolation than just a bunch of people
[00:11:26.280 --> 00:11:27.840]   who think that they can change the world.
[00:11:27.840 --> 00:11:29.840]   It's a homogenous group of people.
[00:11:29.840 --> 00:11:31.280]   So it's a homogenous group of people
[00:11:31.280 --> 00:11:36.280]   trying to make the world better in their image, which never
[00:11:36.280 --> 00:11:40.280]   aligns with everybody else's idea of what a great world is.
[00:11:40.280 --> 00:11:42.160]   He uses an example, he said--
[00:11:42.160 --> 00:11:42.520]   Good, great.
[00:11:42.520 --> 00:11:45.520]   Adam Fisher said, look at 3D printers.
[00:11:45.520 --> 00:11:48.200]   Silicon Valley decided, oh, this is the next big thing,
[00:11:48.200 --> 00:11:50.760]   probably because they read a lot of science fiction that
[00:11:50.760 --> 00:11:52.760]   involve printing your next car.
[00:11:52.760 --> 00:11:55.120]   Or maybe they watch the replicator in Star Trek.
[00:11:55.120 --> 00:11:56.440]   It said, oh, I want that.
[00:11:56.440 --> 00:11:58.440]   So they decided it's the next big thing.
[00:11:58.440 --> 00:12:04.320]   It becomes a big product category.
[00:12:04.320 --> 00:12:06.360]   Middle America goes out and buys it,
[00:12:06.360 --> 00:12:09.440]   uses it for one week, says this is crappy,
[00:12:09.440 --> 00:12:11.320]   puts it in the closet.
[00:12:11.320 --> 00:12:12.840]   It's dead by now, right?
[00:12:12.840 --> 00:12:14.480]   It's a dead category by now.
[00:12:14.480 --> 00:12:15.680]   It'll still be used by industry.
[00:12:15.680 --> 00:12:16.680]   It is important.
[00:12:16.680 --> 00:12:19.400]   I think part of that is whenever Silicon Valley
[00:12:19.400 --> 00:12:22.000]   decides or that group of people--
[00:12:22.000 --> 00:12:23.160]   But this is his point.
[00:12:23.160 --> 00:12:25.600]   They decide what we are going to get the next day.
[00:12:25.600 --> 00:12:28.080]   But they're also very good at feeding a hype machine.
[00:12:28.080 --> 00:12:29.120]   They're creating hype, right?
[00:12:29.120 --> 00:12:32.600]   And unlike companies of old who would go out and do ad
[00:12:32.600 --> 00:12:35.520]   campaigns on television, and it would be this slow build
[00:12:35.520 --> 00:12:39.520]   as the marketing campaigns rolled out, Silicon Valley's--
[00:12:39.520 --> 00:12:41.400]   technology people, not just Silicon Valley,
[00:12:41.400 --> 00:12:42.960]   is a global thing right now.
[00:12:42.960 --> 00:12:46.080]   I like to think of Silicon Valley as just one of many places
[00:12:46.080 --> 00:12:47.160]   where innovation is happening.
[00:12:47.160 --> 00:12:49.720]   I don't know if Silicon Valley is being the only place
[00:12:49.720 --> 00:12:50.480]   where it happens.
[00:12:50.480 --> 00:12:52.200]   They're very good at hacking the marketing
[00:12:52.200 --> 00:12:54.080]   to get the thing happening very, very quickly.
[00:12:54.080 --> 00:12:57.360]   So the 3D printing thing, yes, it did capture a zeitgeist
[00:12:57.360 --> 00:12:58.960]   or a piece of the imagination.
[00:12:58.960 --> 00:13:02.240]   But they're also very good at getting a grassroots movement
[00:13:02.240 --> 00:13:03.240]   behind it.
[00:13:03.240 --> 00:13:05.240]   So they're able to reach out and get the--
[00:13:05.240 --> 00:13:08.320]   and that feeds its own feedback loop.
[00:13:08.320 --> 00:13:12.080]   So I don't know that they're also self-believing,
[00:13:12.080 --> 00:13:13.880]   but they're also able to tap into grassroots.
[00:13:13.880 --> 00:13:17.080]   Where before New York Times was the tastemaker?
[00:13:17.080 --> 00:13:19.160]   They would go out and say, this looks really cool.
[00:13:19.160 --> 00:13:20.480]   And there'd be one or two journalists
[00:13:20.480 --> 00:13:21.520]   who'd publish an article.
[00:13:21.520 --> 00:13:23.720]   And because they're standing on a platform with the loudest
[00:13:23.720 --> 00:13:26.920]   mega, loud halo, they get to make the taste.
[00:13:26.920 --> 00:13:29.280]   And those people are peeved because they're no longer doing that.
[00:13:29.280 --> 00:13:30.360]   That's right.
[00:13:30.360 --> 00:13:33.960]   So a lot of those Hollywood too, right?
[00:13:33.960 --> 00:13:38.080]   I don't know if I agree with that assessment entirely.
[00:13:38.080 --> 00:13:40.880]   I don't think that the New York Times is ticked off
[00:13:40.880 --> 00:13:44.320]   that they are no longer the ones sort of anointing what
[00:13:44.320 --> 00:13:46.840]   is a trend or what the cool things are.
[00:13:46.840 --> 00:13:48.840]   And therefore, they've gone after the Valley.
[00:13:48.840 --> 00:13:50.680]   I think that the Valley has had a pass
[00:13:50.680 --> 00:13:53.360]   for a very, very long time.
[00:13:53.360 --> 00:13:56.880]   And a lot of the technology, as it's been developed,
[00:13:56.880 --> 00:13:59.040]   has sort of gone unchecked.
[00:13:59.040 --> 00:14:00.920]   And we are in the midst of a reckoning.
[00:14:00.920 --> 00:14:02.080]   So the--
[00:14:02.080 --> 00:14:05.560]   I mean, that's why we wind up with Facebook problems
[00:14:05.560 --> 00:14:06.520]   and Twitter problems.
[00:14:06.520 --> 00:14:07.280]   It's never one thing.
[00:14:07.280 --> 00:14:10.360]   I think it's a bit of a loss of control and loss.
[00:14:10.360 --> 00:14:13.440]   There is though right now, would you
[00:14:13.440 --> 00:14:19.520]   agree a tension between the media elite in the Northeast,
[00:14:19.520 --> 00:14:22.280]   Washington DC, and the technologists,
[00:14:22.280 --> 00:14:25.240]   whether they're in Silicon Valley or Seattle, Washington?
[00:14:25.240 --> 00:14:26.320]   I would take that a step further.
[00:14:26.320 --> 00:14:28.280]   I would also point out that Old Capital
[00:14:28.280 --> 00:14:29.680]   is also competing in the temperature.
[00:14:29.680 --> 00:14:31.320]   That might even be a better way to characterize it.
[00:14:31.320 --> 00:14:32.920]   Old Capital and Old Power.
[00:14:32.920 --> 00:14:33.320]   That's right.
[00:14:33.320 --> 00:14:34.400]   Old Capital and Old Power.
[00:14:34.400 --> 00:14:37.680]   So you're looking at an alignment of media,
[00:14:37.680 --> 00:14:40.760]   industrialists, the government, which is all--
[00:14:40.760 --> 00:14:42.800]   And you know what really pisses them off?
[00:14:42.800 --> 00:14:45.720]   The Donald Trump was able to use social media, particularly
[00:14:45.720 --> 00:14:49.720]   in Facebook, to galvanize a group of voters that never
[00:14:49.720 --> 00:14:52.360]   voted before and win the presidential election.
[00:14:52.360 --> 00:14:55.360]   That just cheeses everybody off, except Silicon Valley.
[00:14:55.360 --> 00:14:56.480]   There was an article this week saying
[00:14:56.480 --> 00:14:59.640]   he is the largest political advertiser in Facebook today.
[00:14:59.640 --> 00:15:00.080]   To this day.
[00:15:00.080 --> 00:15:01.120]   Right now, even.
[00:15:01.120 --> 00:15:01.720]   So--
[00:15:01.720 --> 00:15:02.520]   To this day.
[00:15:02.520 --> 00:15:03.020]   Yeah.
[00:15:03.020 --> 00:15:03.520]   So you know--
[00:15:03.520 --> 00:15:05.080]   But it furthers a point.
[00:15:05.080 --> 00:15:07.640]   I mean, Stanford did a study.
[00:15:07.640 --> 00:15:10.880]   Recently, they interviewed, I think, 600 people, 600
[00:15:10.880 --> 00:15:12.760]   different developers on their political views
[00:15:12.760 --> 00:15:15.720]   and religious views and everything else.
[00:15:15.720 --> 00:15:18.280]   And obviously, 600 is not statistically relevant,
[00:15:18.280 --> 00:15:19.960]   given how many people work in Silicon Valley.
[00:15:19.960 --> 00:15:21.400]   But it was a pretty good cross-section.
[00:15:21.400 --> 00:15:24.080]   And all of those viewpoints were homogenous.
[00:15:24.080 --> 00:15:27.400]   I'm not saying I'm excited that Donald Trump is the number one
[00:15:27.400 --> 00:15:28.840]   political spender on Facebook.
[00:15:28.840 --> 00:15:34.040]   And I'm not super elated with how the world has turned out
[00:15:34.040 --> 00:15:35.320]   at this particular moment in time.
[00:15:35.320 --> 00:15:42.520]   But it just sort of goes to show that we talk about media
[00:15:42.520 --> 00:15:43.880]   elites all the time.
[00:15:43.880 --> 00:15:46.160]   We don't often talk about tech elites.
[00:15:46.160 --> 00:15:50.200]   And there is very much an elite ideology and culture
[00:15:50.200 --> 00:15:53.040]   and set of ideas that are alive and well.
[00:15:53.040 --> 00:15:54.000]   I think that's true.
[00:15:54.000 --> 00:15:55.040]   It's fairly homogeneous.
[00:15:55.040 --> 00:15:56.600]   I mean, I think that's accurate.
[00:15:56.600 --> 00:15:59.080]   There are the Peter Teals of the world
[00:15:59.080 --> 00:16:01.080]   who are completely run-catter to that.
[00:16:01.080 --> 00:16:03.880]   But it's the same with industrialists or old media.
[00:16:03.880 --> 00:16:05.480]   They're all homogenous too.
[00:16:05.480 --> 00:16:08.440]   They're not embracing of new groups or new things.
[00:16:08.440 --> 00:16:10.120]   The ability of the old organizations
[00:16:10.120 --> 00:16:12.440]   to be non-homogeneous was limited.
[00:16:12.440 --> 00:16:13.160]   I do wonder--
[00:16:13.160 --> 00:16:14.680]   I do wonder-- and actually, Amy, you probably
[00:16:14.680 --> 00:16:17.160]   have some data on this because this is right up your alley.
[00:16:17.160 --> 00:16:22.000]   But I do wonder if the power of social media these days,
[00:16:22.000 --> 00:16:23.680]   particularly Twitter and Facebook,
[00:16:23.680 --> 00:16:27.880]   was not fully understood and embraced.
[00:16:27.880 --> 00:16:29.600]   And because it allowed an outsider
[00:16:29.600 --> 00:16:33.800]   to march right into Washington, DC,
[00:16:33.800 --> 00:16:36.760]   that is a huge shift that was not foreseen or understood
[00:16:36.760 --> 00:16:38.680]   by anybody.
[00:16:38.680 --> 00:16:40.400]   I think a number of people were going to--
[00:16:40.400 --> 00:16:40.680]   Go ahead.
[00:16:40.680 --> 00:16:42.080]   Let Amy.
[00:16:42.080 --> 00:16:45.240]   So Evelyn and I, a year and a half ago,
[00:16:45.240 --> 00:16:46.560]   were both at a meeting.
[00:16:46.560 --> 00:16:48.600]   Somebody asked him whether or not,
[00:16:48.600 --> 00:16:50.240]   as they were developing Twitter,
[00:16:50.240 --> 00:16:55.640]   they ever saw the wave at Twitter unfolded
[00:16:55.640 --> 00:16:58.040]   as a remote possibility.
[00:16:58.040 --> 00:17:00.080]   And the answer was-- and to be fair,
[00:17:00.080 --> 00:17:02.520]   he's responsible to shareholders.
[00:17:02.520 --> 00:17:04.720]   But the answer was, we've always just
[00:17:04.720 --> 00:17:06.720]   been working on the product.
[00:17:06.720 --> 00:17:08.920]   And from my vantage point, that answer
[00:17:08.920 --> 00:17:15.160]   was either willfully-- he was either lying,
[00:17:15.160 --> 00:17:20.200]   or he literally-- and the team literally never
[00:17:20.200 --> 00:17:21.520]   thought about the future, which to me
[00:17:21.520 --> 00:17:25.120]   seems unlikely given what they had done previously,
[00:17:25.120 --> 00:17:27.440]   or they didn't care.
[00:17:27.440 --> 00:17:29.400]   I have a fourth point of view.
[00:17:29.400 --> 00:17:31.600]   OK.
[00:17:31.600 --> 00:17:33.480]   And I really believe this is true.
[00:17:33.480 --> 00:17:35.800]   I think Eve, who founded Twitter,
[00:17:35.800 --> 00:17:38.080]   I think Zuck, who founded Facebook,
[00:17:38.080 --> 00:17:40.280]   and all of the engineers who work
[00:17:40.280 --> 00:17:43.960]   are doing what engineers do without a point of view,
[00:17:43.960 --> 00:17:47.400]   they're optimizing the software.
[00:17:47.400 --> 00:17:51.320]   They're optimizing for the priority
[00:17:51.320 --> 00:17:52.760]   for both Twitter and Facebook, which
[00:17:52.760 --> 00:17:55.480]   is engagement and attention, because they sell advertising
[00:17:55.480 --> 00:17:56.320]   against it.
[00:17:56.320 --> 00:17:58.080]   Actually, I don't know what Twitter does against it,
[00:17:58.080 --> 00:18:00.040]   but that's what they want.
[00:18:00.040 --> 00:18:03.440]   And this is how engineers are.
[00:18:03.440 --> 00:18:07.600]   They're not really thinking about who benefits or anything.
[00:18:07.600 --> 00:18:09.440]   In fact, if anything, Facebook says it's good,
[00:18:09.440 --> 00:18:11.120]   because everybody connects, that's good.
[00:18:11.120 --> 00:18:11.600]   OK.
[00:18:11.600 --> 00:18:12.480]   Stipulated.
[00:18:12.480 --> 00:18:15.200]   How do we maximize for attention?
[00:18:15.200 --> 00:18:16.280]   Incredibly not easy for people.
[00:18:16.280 --> 00:18:17.800]   That's a really important insight.
[00:18:17.800 --> 00:18:20.080]   That's an incredibly important insight,
[00:18:20.080 --> 00:18:22.880]   because there is a push-pull tension
[00:18:22.880 --> 00:18:26.320]   between what is exacting and what is optimizing.
[00:18:26.320 --> 00:18:27.560]   And that is the push-pull that you
[00:18:27.560 --> 00:18:31.320]   see between traditional media and new media--
[00:18:31.320 --> 00:18:31.880]   You better explain that.
[00:18:31.880 --> 00:18:33.440]   --or media in the valley.
[00:18:33.440 --> 00:18:35.200]   What's exacting?
[00:18:35.200 --> 00:18:39.120]   So all of technology now is about optimization.
[00:18:39.120 --> 00:18:43.400]   So within the realm of AI, there is no singular--
[00:18:43.400 --> 00:18:49.400]   no systems are being designed to produce exact answers.
[00:18:49.400 --> 00:18:53.600]   In solutions, they are being designed to optimize.
[00:18:53.600 --> 00:18:56.080]   And there are many, many reasons for that.
[00:18:56.080 --> 00:18:57.160]   But I think that that--
[00:18:57.160 --> 00:18:57.520]   Yeah.
[00:18:57.520 --> 00:18:59.400]   Something like Google Homes--
[00:18:59.400 --> 00:19:01.760]   because when you ask Google Home for a factual,
[00:19:01.760 --> 00:19:04.040]   it has to give you the answer.
[00:19:04.040 --> 00:19:04.680]   That's exacting.
[00:19:04.680 --> 00:19:06.680]   Well, no, it has to give you an answer.
[00:19:06.680 --> 00:19:08.760]   It doesn't have to give you the answer.
[00:19:08.760 --> 00:19:10.160]   And the answer doesn't have to be correct.
[00:19:10.160 --> 00:19:12.040]   It just has to give you an answer to be successful.
[00:19:12.040 --> 00:19:15.560]   Because the engineers optimize for speed, of responsiveness,
[00:19:15.560 --> 00:19:16.160]   they opt in--
[00:19:16.160 --> 00:19:17.520]   Right, but that's the question, though.
[00:19:17.520 --> 00:19:18.360]   That was the insight.
[00:19:18.360 --> 00:19:19.960]   Who are they optimizing for?
[00:19:19.960 --> 00:19:21.480]   What are they optimizing for?
[00:19:21.480 --> 00:19:24.840]   And the challenge is that typically what's being optimized for
[00:19:24.840 --> 00:19:28.880]   is whatever gets to market fast, whatever that KPI--
[00:19:28.880 --> 00:19:30.080]   Or the business priority--
[00:19:30.080 --> 00:19:32.800]   Or the business priority of the company,
[00:19:32.800 --> 00:19:36.520]   and if it's Facebook's case, Facebook's entire priority
[00:19:36.520 --> 00:19:39.800]   is increasing the amount of time you spend on Facebook, right?
[00:19:39.800 --> 00:19:41.360]   That's like number one, job one.
[00:19:41.360 --> 00:19:44.200]   So when they created the news feed, and ever since,
[00:19:44.200 --> 00:19:47.640]   they've been optimizing for engagement, for attention.
[00:19:47.640 --> 00:19:49.160]   And I don't think they're evil.
[00:19:49.160 --> 00:19:50.640]   In fact, I think they're good engineers.
[00:19:50.640 --> 00:19:52.840]   They've done a damn fine job.
[00:19:52.840 --> 00:19:54.120]   But somebody--
[00:19:54.120 --> 00:19:57.800]   Nobody, Zuck on down, has thought about, well, what do you
[00:19:57.800 --> 00:19:58.800]   get when you do that?
[00:19:58.800 --> 00:19:59.880]   Implications.
[00:19:59.880 --> 00:20:00.800]   Implications.
[00:20:00.800 --> 00:20:01.800]   Yeah, they've never--
[00:20:01.800 --> 00:20:03.480]   I think-- remember when Twitter first started--
[00:20:03.480 --> 00:20:05.320]   Engineers are not in the implications.
[00:20:05.320 --> 00:20:06.400]   They're in the solutions.
[00:20:06.400 --> 00:20:06.960]   They're probably--
[00:20:06.960 --> 00:20:08.040]   They're focused on the goals.
[00:20:08.040 --> 00:20:08.880]   They're problems.
[00:20:08.880 --> 00:20:11.120]   And the problem is that when we first started out with Twitter,
[00:20:11.120 --> 00:20:12.640]   remember how it used to crash all the time,
[00:20:12.640 --> 00:20:13.800]   and we couldn't do anything?
[00:20:13.800 --> 00:20:15.800]   So the engineers were actually spent most of their time
[00:20:15.800 --> 00:20:17.360]   trying to keep the site up.
[00:20:17.360 --> 00:20:19.640]   And then all of a sudden, when the system was able to scale,
[00:20:19.640 --> 00:20:21.560]   they redeveloped all the software and got it all working,
[00:20:21.560 --> 00:20:23.000]   and then added some features.
[00:20:23.000 --> 00:20:24.880]   Engagement grew.
[00:20:24.880 --> 00:20:26.320]   So they cycled on engagement.
[00:20:26.320 --> 00:20:28.400]   Now we're working our way through what engagement is.
[00:20:28.400 --> 00:20:30.040]   And suddenly we're saying, oh, you've
[00:20:30.040 --> 00:20:31.160]   got to start thinking morally.
[00:20:31.160 --> 00:20:33.360]   Well, I bet there's a whole bunch of engineers inside
[00:20:33.360 --> 00:20:35.680]   of these companies thinking about what is morally acceptable.
[00:20:35.680 --> 00:20:38.920]   But the challenge is, moral acceptability
[00:20:38.920 --> 00:20:40.400]   is not easy to determine.
[00:20:40.400 --> 00:20:41.880]   Nor do you go under what's happening.
[00:20:41.880 --> 00:20:42.880]   Nor do you go under what's happening.
[00:20:42.880 --> 00:20:43.320]   Yeah.
[00:20:43.320 --> 00:20:44.880]   For profit.
[00:20:44.880 --> 00:20:45.400]   What happened?
[00:20:45.400 --> 00:20:48.240]   It might optimize to avoid getting going to jail
[00:20:48.240 --> 00:20:51.000]   or getting called into Congress.
[00:20:51.000 --> 00:20:52.720]   But if you think about your smartphone,
[00:20:52.720 --> 00:20:54.960]   if you think about every single video game,
[00:20:54.960 --> 00:20:57.720]   really every product of Silicon Valley
[00:20:57.720 --> 00:20:59.360]   is a product of engineering minds
[00:20:59.360 --> 00:21:01.800]   attempting to solve a problem and optimize--
[00:21:01.800 --> 00:21:03.320]   Let's keep a sense of balance here, right?
[00:21:03.320 --> 00:21:06.640]   Remember that cars were invented roughly in the 1920s,
[00:21:06.640 --> 00:21:08.760]   and they used to kill people at a phenomenal rate
[00:21:08.760 --> 00:21:09.600]   for a long period of time--
[00:21:09.600 --> 00:21:10.160]   It's so hard.
[00:21:10.160 --> 00:21:10.160]   --until--
[00:21:10.160 --> 00:21:12.720]   By the way, a million people a year in the world wide.
[00:21:12.720 --> 00:21:13.280]   Exactly.
[00:21:13.280 --> 00:21:14.640]   And then there's a lot of people--
[00:21:14.640 --> 00:21:17.440]   it took a couple of decades before the law came around
[00:21:17.440 --> 00:21:18.800]   and started saying, you have to consider
[00:21:18.800 --> 00:21:20.440]   the moral impact of these vehicles.
[00:21:20.440 --> 00:21:23.920]   And they forced drivers to walk around with a person
[00:21:23.920 --> 00:21:26.200]   waving a flag in front so the horses wouldn't--
[00:21:26.200 --> 00:21:27.600]   A lot of good that did.
[00:21:27.600 --> 00:21:30.200]   But I mean, these things take time.
[00:21:30.200 --> 00:21:31.040]   But this also--
[00:21:31.040 --> 00:21:31.960]   And it's also iterative.
[00:21:31.960 --> 00:21:32.400]   --it's also iterative.
[00:21:32.400 --> 00:21:34.720]   It's a hundred years ago that we had the flagmen.
[00:21:34.720 --> 00:21:36.240]   It's a process of awakening.
[00:21:36.240 --> 00:21:37.120]   Take time.
[00:21:37.120 --> 00:21:38.920]   It's a century.
[00:21:38.920 --> 00:21:40.080]   I do wish I was in the years where--
[00:21:40.080 --> 00:21:41.680]   That's another interesting insight.
[00:21:41.680 --> 00:21:44.800]   And again, it goes to what and who
[00:21:44.800 --> 00:21:45.960]   are we optimizing for.
[00:21:45.960 --> 00:21:51.640]   So the year 1917 is the year that the very first flying car
[00:21:51.640 --> 00:21:52.600]   patent got filed.
[00:21:55.080 --> 00:22:00.080]   And this is relevant because Google's big secret flying car
[00:22:00.080 --> 00:22:03.880]   project finally went public, like two, three weeks ago.
[00:22:03.880 --> 00:22:08.680]   And it's an example of innovation imitation.
[00:22:08.680 --> 00:22:12.280]   So we keep coming up with what we feel and think
[00:22:12.280 --> 00:22:14.320]   are novel solutions, big ideas.
[00:22:14.320 --> 00:22:17.160]   We're optimizing for what people need right now.
[00:22:17.160 --> 00:22:21.400]   And somehow we forget that we're just inventing--
[00:22:21.400 --> 00:22:24.560]   we're constantly reinventing the same solutions
[00:22:24.560 --> 00:22:26.600]   to those sort of same problems.
[00:22:26.600 --> 00:22:29.800]   And every year, since 1917, somebody
[00:22:29.800 --> 00:22:33.800]   has invented and flown a flying car.
[00:22:33.800 --> 00:22:35.440]   But this time it's going to work.
[00:22:35.440 --> 00:22:35.940]   Yeah.
[00:22:35.940 --> 00:22:37.360]   But this time it's going to work.
[00:22:37.360 --> 00:22:40.000]   But I mean, this is sort of the--
[00:22:40.000 --> 00:22:42.400]   I think this is a--
[00:22:42.400 --> 00:22:45.400]   this is sort of what happens when we're optimizing all the time.
[00:22:45.400 --> 00:22:48.600]   This is not Larry Page's flying car.
[00:22:48.600 --> 00:22:50.640]   Well, but I mean--
[00:22:50.640 --> 00:22:51.240]   So this is--
[00:22:51.240 --> 00:22:51.640]   By the way--
[00:22:51.640 --> 00:22:53.240]   --it's like we keep coming back to it over and over.
[00:22:53.240 --> 00:22:55.240]   And I talked about this also.
[00:22:55.240 --> 00:22:57.680]   Adam Fisher, author of "Valley of Gods."
[00:22:57.680 --> 00:23:00.880]   Because there's a guy that's very little known
[00:23:00.880 --> 00:23:01.600]   in Silicon Valley.
[00:23:01.600 --> 00:23:02.760]   In fact, I'm going to test you guys.
[00:23:02.760 --> 00:23:05.280]   You ever hear of Scott Hassan?
[00:23:05.280 --> 00:23:05.880]   I haven't.
[00:23:05.880 --> 00:23:09.040]   He's the third founder of Google.
[00:23:09.040 --> 00:23:12.720]   When Larry Page and Sergey Brin were graduate students,
[00:23:12.720 --> 00:23:15.120]   they were going for their PhDs.
[00:23:15.120 --> 00:23:19.200]   They came up with this as research for a PhD.
[00:23:19.200 --> 00:23:20.160]   That was their goal.
[00:23:20.160 --> 00:23:22.080]   They were trying to build a business.
[00:23:22.080 --> 00:23:24.880]   Scott Hassan was the guy assigned to the computer lab
[00:23:24.880 --> 00:23:28.200]   so that the PhD students could have some code written.
[00:23:28.200 --> 00:23:31.200]   So they go to Scott and say, can you write this search thing?
[00:23:31.200 --> 00:23:33.720]   And it was Scott who basically wrote the original Google
[00:23:33.720 --> 00:23:36.440]   algorithms and in fact said, you know,
[00:23:36.440 --> 00:23:37.400]   this could be a business.
[00:23:37.400 --> 00:23:39.200]   Larry and Sergey said, who wants to do a business?
[00:23:39.200 --> 00:23:39.720]   There's excitement.
[00:23:39.720 --> 00:23:40.920]   There's out the vista.
[00:23:40.920 --> 00:23:41.920]   There's no need.
[00:23:41.920 --> 00:23:44.600]   Today, Android, as we know it sits at the--
[00:23:44.600 --> 00:23:50.280]   Sorry, little side talk there from the thank you internet.
[00:23:50.280 --> 00:23:53.840]   So anyway, Scott Hassan has an interesting theory.
[00:23:53.840 --> 00:23:56.000]   He's still around, very smart guy.
[00:23:56.000 --> 00:23:58.280]   He says, we're not doing self-driving cars.
[00:23:58.280 --> 00:23:59.600]   He said, we're going to skip that.
[00:23:59.600 --> 00:24:02.280]   It's to a regulatory nightmare because every single locality
[00:24:02.280 --> 00:24:03.240]   has to approve it.
[00:24:03.240 --> 00:24:04.360]   Insurers have to go for it.
[00:24:04.360 --> 00:24:06.880]   And users have to be willing to get in a car with no steering
[00:24:06.880 --> 00:24:07.560]   wheel.
[00:24:07.560 --> 00:24:08.880]   That's going to be a problem.
[00:24:08.880 --> 00:24:12.160]   You know what's cool?
[00:24:12.160 --> 00:24:13.560]   Flying cars.
[00:24:13.560 --> 00:24:14.400]   He said, thanks to--
[00:24:14.400 --> 00:24:17.440]   Now that we have autonomy, batteries are getting good enough
[00:24:17.440 --> 00:24:19.160]   so that the efficiency of a flying car
[00:24:19.160 --> 00:24:22.000]   is the same as a car on the ground.
[00:24:22.000 --> 00:24:24.280]   And autonomy solves the problem of flying cars
[00:24:24.280 --> 00:24:25.960]   because you don't want humans flying around
[00:24:25.960 --> 00:24:27.880]   in these flying cars because if you think
[00:24:27.880 --> 00:24:30.040]   you're wreck on the freeway is bad, what about a wreck
[00:24:30.040 --> 00:24:32.680]   100 feet over your head?
[00:24:32.680 --> 00:24:35.640]   But once we have safe, autonomous flying vehicles,
[00:24:35.640 --> 00:24:39.400]   it's one regulatory agency, one, the FAA.
[00:24:39.400 --> 00:24:41.240]   That's it.
[00:24:41.240 --> 00:24:43.160]   He's of the opinion that in the next 20 years,
[00:24:43.160 --> 00:24:45.000]   you're going to completely skip autonomous vehicles
[00:24:45.000 --> 00:24:45.880]   and go to flying cars.
[00:24:45.880 --> 00:24:46.640]   You're the futurist.
[00:24:46.640 --> 00:24:48.880]   You say no way, Amy Webb.
[00:24:48.880 --> 00:24:49.880]   Yeah.
[00:24:49.880 --> 00:24:53.480]   So for many of the same reasons, none of the regulatory
[00:24:53.480 --> 00:24:55.200]   infrastructure is in place.
[00:24:55.200 --> 00:24:57.560]   We would have to effectively create invisible highways
[00:24:57.560 --> 00:25:00.360]   in the sky, the sense of an avoid technology that
[00:25:00.360 --> 00:25:02.000]   would be required for any of--
[00:25:02.000 --> 00:25:04.640]   for a flying car to work in a city with buildings.
[00:25:04.640 --> 00:25:07.160]   A lot cheaper to build an invisible highway in the sky
[00:25:07.160 --> 00:25:09.520]   than a single freeway on the ground.
[00:25:09.520 --> 00:25:10.280]   Sure.
[00:25:10.280 --> 00:25:11.040]   A lot cheaper.
[00:25:11.040 --> 00:25:12.400]   But I know because--
[00:25:12.400 --> 00:25:15.440]   Plus, imagine, no cars on the freeway.
[00:25:15.440 --> 00:25:18.120]   It would be a bicycle and pedestrian paradise.
[00:25:18.120 --> 00:25:19.720]   But you're in California.
[00:25:19.720 --> 00:25:20.480]   You're in California.
[00:25:20.480 --> 00:25:22.520]   I'm on the East Coast, where we are surrounded
[00:25:22.520 --> 00:25:24.200]   by enormously tall buildings.
[00:25:24.200 --> 00:25:25.520]   Well, you fly over.
[00:25:25.520 --> 00:25:26.520]   You can't.
[00:25:26.520 --> 00:25:30.160]   And you're up in the airspace of actual--
[00:25:30.160 --> 00:25:31.840]   So the challenge here is you have to create--
[00:25:31.840 --> 00:25:33.160]   I thought a little bit about this.
[00:25:33.160 --> 00:25:35.960]   When you follow the planes, if you create highways,
[00:25:35.960 --> 00:25:37.360]   you have to have clear space underneath.
[00:25:37.360 --> 00:25:39.880]   Because when they crash, you can't crash into buildings.
[00:25:39.880 --> 00:25:40.960]   So what are you going to do?
[00:25:40.960 --> 00:25:42.480]   Run them over the top of existing highways,
[00:25:42.480 --> 00:25:44.400]   or you're going to run them over clear ground.
[00:25:44.400 --> 00:25:47.600]   So that means you would actually have to go and clear in a city.
[00:25:47.600 --> 00:25:49.760]   You would have to clear flying lanes.
[00:25:49.760 --> 00:25:50.560]   That's interesting.
[00:25:50.560 --> 00:25:52.680]   Or you go underground.
[00:25:52.680 --> 00:25:53.040]   Oh.
[00:25:53.040 --> 00:25:53.960]   So this is where--
[00:25:53.960 --> 00:25:54.440]   Boring.
[00:25:54.440 --> 00:25:54.960]   That's so boring.
[00:25:54.960 --> 00:25:56.160]   I know everybody loves--
[00:25:56.160 --> 00:25:57.760]   Everybody-- yes, it's so boring.
[00:25:57.760 --> 00:26:00.200]   And I know everybody loves to make fun of Elon Musk.
[00:26:00.200 --> 00:26:01.360]   The boring company.
[00:26:01.360 --> 00:26:02.960]   Yep.
[00:26:02.960 --> 00:26:05.160]   That-- right, so-- but to me, this
[00:26:05.160 --> 00:26:07.380]   is such a great conversation to tee off
[00:26:07.380 --> 00:26:09.040]   any other conversation we have tonight.
[00:26:09.040 --> 00:26:12.680]   Because it's a good reminder that a lot of the technology that
[00:26:12.680 --> 00:26:15.560]   feels new and fresh, or the tech-related problems that we
[00:26:15.560 --> 00:26:17.040]   have, are cyclical.
[00:26:17.040 --> 00:26:19.920]   And somebody's already done a lever of it.
[00:26:19.920 --> 00:26:22.240]   And it just feels novel and new.
[00:26:22.240 --> 00:26:23.280]   Look at the London Underground.
[00:26:23.280 --> 00:26:25.360]   It was built in the 1820s.
[00:26:25.360 --> 00:26:29.320]   And they used to have horses dragging the carts along the tunnels.
[00:26:29.320 --> 00:26:31.920]   And then later, they had coal-powered trains.
[00:26:31.920 --> 00:26:34.320]   Can you imagine being in an underground tunnel
[00:26:34.320 --> 00:26:39.360]   with coal-fired steam trains dragging the trains along?
[00:26:39.360 --> 00:26:40.880]   And just how about that would have been?
[00:26:40.880 --> 00:26:42.080]   My favorite-- let's see.
[00:26:42.080 --> 00:26:42.600]   I don't know.
[00:26:42.600 --> 00:26:43.160]   This isn't it.
[00:26:43.160 --> 00:26:45.000]   There's pictures of the London Underground.
[00:26:45.000 --> 00:26:46.720]   There were like little living rooms.
[00:26:46.720 --> 00:26:47.220]   Yes.
[00:26:47.220 --> 00:26:48.800]   And of people going on the line--
[00:26:48.800 --> 00:26:49.300]   Yeah.
[00:26:49.300 --> 00:26:50.320]   How about this?
[00:26:50.320 --> 00:26:51.400]   How about this?
[00:26:51.400 --> 00:26:56.120]   The tube going in-- this is basically a mining cart.
[00:26:56.120 --> 00:26:56.720]   Yes.
[00:26:56.720 --> 00:26:59.200]   That still exists in Washington, DC.
[00:26:59.200 --> 00:26:59.760]   What?
[00:26:59.760 --> 00:27:00.480]   Oh, that's under--
[00:27:00.480 --> 00:27:02.040]   You're-- that's in Congress, right?
[00:27:02.040 --> 00:27:02.320]   They have--
[00:27:02.320 --> 00:27:02.800]   That's right.
[00:27:02.800 --> 00:27:04.520]   So you can take--
[00:27:04.520 --> 00:27:05.440]   I mean, not a tourist.
[00:27:05.440 --> 00:27:07.200]   You have to be there for a reason.
[00:27:07.200 --> 00:27:10.280]   But there is a little tiny train that looks just like that.
[00:27:10.280 --> 00:27:11.080]   I've ridden it.
[00:27:11.080 --> 00:27:12.080]   It's really cool.
[00:27:12.080 --> 00:27:14.880]   It gets members of Congress from the Capitol
[00:27:14.880 --> 00:27:17.680]   building to their offices in that.
[00:27:17.680 --> 00:27:19.480]   So yeah, I take your point, Amy.
[00:27:19.480 --> 00:27:23.760]   The idea of a tunnel is a 250-year-old idea coming back to life.
[00:27:23.760 --> 00:27:25.440]   But I do want another--
[00:27:25.440 --> 00:27:26.400]   If--
[00:27:26.400 --> 00:27:29.960]   Flying cars are deeply impractical because, yes,
[00:27:29.960 --> 00:27:31.240]   there's only one regulation bodies.
[00:27:31.240 --> 00:27:32.840]   And quite often, these--
[00:27:32.840 --> 00:27:36.680]   By the way, a regulation body that loves flying.
[00:27:36.680 --> 00:27:38.600]   Look how they've treated drones.
[00:27:38.600 --> 00:27:40.320]   They love this stuff.
[00:27:40.320 --> 00:27:42.400]   They want you to fly their full car.
[00:27:42.400 --> 00:27:45.560]   The challenge here is, of course, is that these people have
[00:27:45.560 --> 00:27:47.520]   these really rose-colored glasses.
[00:27:47.520 --> 00:27:49.760]   They want to believe that these things that they come up
[00:27:49.760 --> 00:27:51.680]   with these ideas are actually going to work.
[00:27:51.680 --> 00:27:53.320]   And yet, they very rarely do.
[00:27:53.320 --> 00:27:54.640]   Who's Paddington Station in 18th?
[00:27:54.640 --> 00:27:55.680]   Look at self-driving cars.
[00:27:55.680 --> 00:27:58.640]   Was it like five years ago when the first self-driving cars came out?
[00:27:58.640 --> 00:28:00.160]   And we were all supposed to be in self-driving car.
[00:28:00.160 --> 00:28:01.480]   Actually, no.
[00:28:01.480 --> 00:28:04.200]   The very first self-driving car was launched by--
[00:28:04.200 --> 00:28:09.840]   it was a GE and RCA project in the 1950s.
[00:28:09.840 --> 00:28:13.440]   And it was an electric car that drove around a self-contained track.
[00:28:13.440 --> 00:28:16.840]   So again, we feel like it's our generation that invented
[00:28:16.840 --> 00:28:18.760]   the self-driving car, and we're not.
[00:28:18.760 --> 00:28:20.600]   That's a technology that's 70 years old.
[00:28:20.600 --> 00:28:23.640]   But we are now, for the first time in a position
[00:28:23.640 --> 00:28:26.480]   where it could actually work because of autonomy,
[00:28:26.480 --> 00:28:29.920]   because of batteries, because of new efficiencies
[00:28:29.920 --> 00:28:31.120]   and manufacture.
[00:28:31.120 --> 00:28:33.400]   It's still going to take decades.
[00:28:33.400 --> 00:28:35.440]   I think a lot of this has to do with the way
[00:28:35.440 --> 00:28:36.840]   that we tell stories.
[00:28:36.840 --> 00:28:39.280]   And the stories that we tell about transportation
[00:28:39.280 --> 00:28:42.920]   are visceral because we have seen flying cars and movies,
[00:28:42.920 --> 00:28:46.360]   and we have seen cool people movers.
[00:28:46.360 --> 00:28:51.360]   What we have never seen are super lightning fast, super far
[00:28:51.360 --> 00:28:55.320]   underground systems that get us around.
[00:28:55.320 --> 00:28:57.400]   And so we don't have a story.
[00:28:57.400 --> 00:28:58.480]   We don't have a framework.
[00:28:58.480 --> 00:28:59.600]   And so it's hard for us to--
[00:28:59.600 --> 00:29:00.520]   I think they're challenges.
[00:29:00.520 --> 00:29:03.240]   And there's also great expense to digging holes.
[00:29:03.240 --> 00:29:04.480]   Sure.
[00:29:04.480 --> 00:29:05.440]   Not so much.
[00:29:05.440 --> 00:29:06.720]   Well, compared to--
[00:29:06.720 --> 00:29:08.520]   Oh my god, lost into the big dig.
[00:29:08.520 --> 00:29:10.120]   Oh my god, we're in nightmare.
[00:29:10.120 --> 00:29:11.120]   Yeah.
[00:29:11.120 --> 00:29:13.240]   But there's also political issues.
[00:29:13.240 --> 00:29:14.880]   So you've got your airspace overhead,
[00:29:14.880 --> 00:29:17.920]   which seems like it's politically more tenable.
[00:29:17.920 --> 00:29:20.800]   You've compared to the nightmare on the ground,
[00:29:20.800 --> 00:29:24.320]   under the ground, I don't know, because I don't think--
[00:29:24.320 --> 00:29:28.560]   what would be the use cases besides wells and oil?
[00:29:28.560 --> 00:29:30.120]   I can't wait.
[00:29:30.120 --> 00:29:31.880]   And I invite you both.
[00:29:31.880 --> 00:29:33.280]   Twit 1,000.
[00:29:33.280 --> 00:29:35.640]   You can come in your Uber flying car.
[00:29:35.640 --> 00:29:37.520]   You'll be up here in half an hour.
[00:29:37.520 --> 00:29:39.040]   Amy, you'll be here in a few hours,
[00:29:39.040 --> 00:29:40.800]   because you're all the way from the East Coast.
[00:29:40.800 --> 00:29:44.320]   You can take your tunnel if you want.
[00:29:44.320 --> 00:29:48.360]   Actually, aren't we supposed to be building a tunnel to LA
[00:29:48.360 --> 00:29:49.720]   from San Francisco?
[00:29:49.720 --> 00:29:51.080]   I thought that was--
[00:29:51.080 --> 00:29:53.640]   Elon Musk's boring company is doing pilot projects.
[00:29:53.640 --> 00:29:56.080]   No, but I thought that there was already--
[00:29:56.080 --> 00:29:57.080]   maybe not.
[00:29:57.080 --> 00:29:58.240]   I have a lot of people--
[00:29:58.240 --> 00:29:59.240]   I have a lot of people.
[00:29:59.240 --> 00:29:59.920]   --a high-speed tunnel.
[00:29:59.920 --> 00:30:01.800]   If it's going to happen, it's going to happen in Japan.
[00:30:01.800 --> 00:30:02.960]   Yeah.
[00:30:02.960 --> 00:30:05.600]   They invent the bullet train in the '60s.
[00:30:05.600 --> 00:30:07.160]   Yeah.
[00:30:07.160 --> 00:30:09.240]   There's a couple of other investors moving into the market
[00:30:09.240 --> 00:30:09.760]   as well.
[00:30:09.760 --> 00:30:11.840]   Now that he's proven out the concept early movies--
[00:30:11.840 --> 00:30:13.520]   I still think that's--
[00:30:13.520 --> 00:30:16.840]   there's so many problems with boring holes in the ground.
[00:30:16.840 --> 00:30:19.080]   There's so many more problems with flying cars.
[00:30:19.080 --> 00:30:22.000]   No, it's the future.
[00:30:22.000 --> 00:30:22.480]   All it's--
[00:30:22.480 --> 00:30:24.280]   I saw it on the Jetsons.
[00:30:24.280 --> 00:30:25.160]   You've seen people--
[00:30:25.160 --> 00:30:27.520]   you've seen a couple of self-driving cars have accidents,
[00:30:27.520 --> 00:30:30.000]   and we've seen the world have apocalyptic fits over that.
[00:30:30.000 --> 00:30:31.840]   Imagine what happens when a half a dozen flying cars
[00:30:31.840 --> 00:30:32.360]   are the last--
[00:30:32.360 --> 00:30:33.720]   No, because autonomy's going to solve that.
[00:30:33.720 --> 00:30:34.680]   Go ahead, Amy.
[00:30:34.680 --> 00:30:36.680]   I want to play a quick game of, would you rather?
[00:30:36.680 --> 00:30:37.680]   Oh, yeah.
[00:30:37.680 --> 00:30:38.180]   Yeah.
[00:30:38.180 --> 00:30:41.760]   Would you rather have your only mode of transportation
[00:30:41.760 --> 00:30:45.720]   be flying car, autonomous car, or car
[00:30:45.720 --> 00:30:48.880]   that is part of an underground system?
[00:30:48.880 --> 00:30:51.680]   Flying car, like in a heartbeat.
[00:30:51.680 --> 00:30:52.680]   Imagine.
[00:30:52.680 --> 00:30:53.960]   I pull out my-- by the way, this is why
[00:30:53.960 --> 00:30:56.040]   Uber's valued at billions of dollars.
[00:30:56.040 --> 00:30:57.320]   I pull out my phone.
[00:30:57.320 --> 00:30:58.720]   I hit the Uber app.
[00:30:58.720 --> 00:31:00.400]   I said, I need to get to the city.
[00:31:00.400 --> 00:31:01.800]   It lands right here.
[00:31:01.800 --> 00:31:02.320]   I get in.
[00:31:02.320 --> 00:31:03.160]   There's no driver.
[00:31:03.160 --> 00:31:06.640]   It's just a pod, a drone pod.
[00:31:06.640 --> 00:31:07.480]   And I go.
[00:31:07.480 --> 00:31:09.120]   And then I get out, and it goes away.
[00:31:09.120 --> 00:31:11.120]   There's no parking.
[00:31:11.120 --> 00:31:13.080]   There's no traffic.
[00:31:13.080 --> 00:31:15.400]   It's assuming there's a landing space.
[00:31:15.400 --> 00:31:17.000]   And what if there's no other traffic around it
[00:31:17.000 --> 00:31:18.520]   for 100 meters in any direction?
[00:31:18.520 --> 00:31:19.920]   We already got helipads and all the hard rides.
[00:31:19.920 --> 00:31:21.440]   There's no hurricane, no wind.
[00:31:21.440 --> 00:31:23.600]   OK, what if there's no updrafts or downdrafts?
[00:31:23.600 --> 00:31:24.320]   Have you ever been in a--
[00:31:24.320 --> 00:31:25.560]   Come here in a hurricane.
[00:31:25.560 --> 00:31:27.000]   Have you ever--
[00:31:27.000 --> 00:31:27.720]   Sorry.
[00:31:27.720 --> 00:31:28.240]   I won't.
[00:31:28.240 --> 00:31:29.480]   Have you ever been called off?
[00:31:29.480 --> 00:31:32.160]   Have you ever been in a light aircraft flying
[00:31:32.160 --> 00:31:32.680]   in a thermal--
[00:31:32.680 --> 00:31:33.160]   Yes.
[00:31:33.160 --> 00:31:34.000]   --over the desert?
[00:31:34.000 --> 00:31:34.500]   It's not fun.
[00:31:34.500 --> 00:31:36.360]   When it goes into a 500-foot downdraft,
[00:31:36.360 --> 00:31:37.200]   and you literally just--
[00:31:37.200 --> 00:31:38.120]   I wasn't a glider.
[00:31:38.120 --> 00:31:38.520]   Yep.
[00:31:38.520 --> 00:31:40.120]   Was the last flight of the day, because instead
[00:31:40.120 --> 00:31:41.680]   of getting too windy, we can't fly anymore.
[00:31:41.680 --> 00:31:42.640]   But you, you come on up.
[00:31:42.640 --> 00:31:43.640]   Yeah.
[00:31:43.640 --> 00:31:44.480]   [LAUGHTER]
[00:31:44.480 --> 00:31:45.400]   So you're the nightmare.
[00:31:45.400 --> 00:31:47.440]   You only fly in that once.
[00:31:47.440 --> 00:31:49.760]   I was flying in a little tiny F-28
[00:31:49.760 --> 00:31:50.680]   over the Australian desert.
[00:31:50.680 --> 00:31:51.040]   Imagine that.
[00:31:51.040 --> 00:31:51.680]   No engine.
[00:31:51.680 --> 00:31:52.040]   Yeah.
[00:31:52.040 --> 00:31:53.040]   And you're in a glider.
[00:31:53.040 --> 00:31:53.240]   Yeah.
[00:31:53.240 --> 00:31:53.880]   And it's doing that.
[00:31:53.880 --> 00:31:54.760]   Yeah, I know.
[00:31:54.760 --> 00:31:57.040]   So I think flying cars are complete rubbish.
[00:31:57.040 --> 00:31:58.560]   I want a flying car.
[00:31:58.560 --> 00:31:59.400]   OK, would you rub?
[00:31:59.400 --> 00:32:00.160]   Leo's flying car.
[00:32:00.160 --> 00:32:01.520]   Greg, what are you?
[00:32:01.520 --> 00:32:04.320]   I'm an autonomous car on the ground.
[00:32:04.320 --> 00:32:07.200]   The point of reason is that underground will have limited
[00:32:07.200 --> 00:32:08.360]   stops and destinations.
[00:32:08.360 --> 00:32:10.480]   So it won't be where I am.
[00:32:10.480 --> 00:32:14.520]   So if you're saying that I want a flying vehicle for suburban
[00:32:14.520 --> 00:32:16.200]   work, well, then you have to assume
[00:32:16.200 --> 00:32:19.360]   that you have enough space to fly down and for it to park.
[00:32:19.360 --> 00:32:20.800]   And you have to have safety zones around it.
[00:32:20.800 --> 00:32:21.760]   That means no trees.
[00:32:21.760 --> 00:32:23.880]   It's the size of a flying car.
[00:32:23.880 --> 00:32:25.120]   A car parking space.
[00:32:25.120 --> 00:32:26.640]   You can land anywhere.
[00:32:26.640 --> 00:32:28.000]   But if you've got a nice breeze,
[00:32:28.000 --> 00:32:29.160]   it'll blow you into a tree.
[00:32:29.160 --> 00:32:29.840]   And then you're dead.
[00:32:29.840 --> 00:32:30.520]   No.
[00:32:30.520 --> 00:32:31.440]   No.
[00:32:31.440 --> 00:32:33.920]   Drones can handle hurricanes.
[00:32:33.920 --> 00:32:34.960]   You've seen drones handled.
[00:32:34.960 --> 00:32:36.320]   But I also go like this.
[00:32:36.320 --> 00:32:37.960]   And you're willing to let them fall out of the sky.
[00:32:37.960 --> 00:32:38.560]   You'll get used to that.
[00:32:38.560 --> 00:32:40.480]   They're not going to have to shift your balance
[00:32:40.480 --> 00:32:41.400]   around inside the vehicle.
[00:32:41.400 --> 00:32:43.400]   All right, let's try it with a studio audience.
[00:32:43.400 --> 00:32:44.680]   Would you rather-- let me see.
[00:32:44.680 --> 00:32:47.560]   Those of you who would rather have a flying car--
[00:32:47.560 --> 00:32:50.640]   two, three-- those of you who would rather go in a hole
[00:32:50.640 --> 00:32:52.160]   in the ground--
[00:32:52.160 --> 00:32:53.160]   none.
[00:32:53.160 --> 00:32:53.960]   Hole.
[00:32:53.960 --> 00:32:54.840]   Zero.
[00:32:54.840 --> 00:32:57.240]   Those of you who would like an autonomous vehicle.
[00:32:57.240 --> 00:32:58.000]   That's the most.
[00:32:58.000 --> 00:32:58.560]   That's four.
[00:32:58.560 --> 00:32:59.480]   You win.
[00:32:59.480 --> 00:33:01.200]   So lack of foresight.
[00:33:01.200 --> 00:33:03.400]   I would definitely go in a hole in the ground
[00:33:03.400 --> 00:33:04.280]   if it's for long haul.
[00:33:04.280 --> 00:33:06.120]   Instead of getting in a plane, I will go in a hole
[00:33:06.120 --> 00:33:06.640]   in the ground.
[00:33:06.640 --> 00:33:08.040]   I mean, when I'm in New York, I take the subway.
[00:33:08.040 --> 00:33:08.760]   I love the subway.
[00:33:08.760 --> 00:33:09.280]   That's right.
[00:33:09.280 --> 00:33:10.880]   And if you said to me there's a way
[00:33:10.880 --> 00:33:13.000]   to get from San Francisco to Portland or whatever
[00:33:13.000 --> 00:33:15.560]   in a hole in the ground, then I'm all up for that.
[00:33:15.560 --> 00:33:18.520]   But getting in-- the challenges around flying cars
[00:33:18.520 --> 00:33:19.360]   are just impractical.
[00:33:19.360 --> 00:33:21.200]   You're literally going to have a problem in the States
[00:33:21.200 --> 00:33:24.000]   by saying I have to build a landing spot in everybody's
[00:33:24.000 --> 00:33:24.840]   back garden.
[00:33:24.840 --> 00:33:26.440]   And most people don't have their--
[00:33:26.440 --> 00:33:28.360]   One of the things-- one of the models
[00:33:28.360 --> 00:33:31.200]   that I've been building is around the next economic centers.
[00:33:31.200 --> 00:33:33.440]   And given what we know to be true about micro climates
[00:33:33.440 --> 00:33:35.360]   and extreme weather changes and everything else,
[00:33:35.360 --> 00:33:38.680]   it's highly likely that sometime within the next 50 years,
[00:33:38.680 --> 00:33:41.080]   our new economic centers in the US
[00:33:41.080 --> 00:33:44.400]   would be places more like Denver and Wichita.
[00:33:44.400 --> 00:33:46.840]   Lots of room for flying cars to lands in Denver.
[00:33:46.840 --> 00:33:48.480]   And then who's going to complain about the noise?
[00:33:48.480 --> 00:33:49.520]   You know how loud those things are?
[00:33:49.520 --> 00:33:50.760]   Yeah, the noise is an issue.
[00:33:50.760 --> 00:33:51.400]   Yeah.
[00:33:51.400 --> 00:33:54.400]   Noise, trees, parking spots.
[00:33:54.400 --> 00:33:57.360]   And then if the weather's bad, you can't fly in a car.
[00:33:57.360 --> 00:34:00.880]   Don't you wish that we could just jump ahead 20 years
[00:34:00.880 --> 00:34:01.760]   and see.
[00:34:01.760 --> 00:34:03.680]   But other than those small little things,
[00:34:03.680 --> 00:34:06.600]   you can imagine this one because--
[00:34:06.600 --> 00:34:07.760]   My answer is no.
[00:34:07.760 --> 00:34:08.600]   What?
[00:34:08.600 --> 00:34:09.880]   You're a futurist.
[00:34:09.880 --> 00:34:11.880]   Your whole career is based on that.
[00:34:11.880 --> 00:34:14.040]   Because I am so concerned about the decisions that
[00:34:14.040 --> 00:34:15.240]   are being made today.
[00:34:15.240 --> 00:34:17.120]   I don't want to go 20 years into the future
[00:34:17.120 --> 00:34:18.640]   because I don't want to live in that place.
[00:34:18.640 --> 00:34:20.680]   So my hope is that we make better decisions now.
[00:34:20.680 --> 00:34:22.240]   Oh, you see where we're headed.
[00:34:22.240 --> 00:34:23.720]   [LAUGHTER]
[00:34:23.720 --> 00:34:26.280]   I do not want to go to there.
[00:34:26.280 --> 00:34:27.840]   I'm not talking about scooters, by the way,
[00:34:27.840 --> 00:34:30.120]   because gosh, those things are like rubbish.
[00:34:30.120 --> 00:34:31.320]   Oh, you're going to see scooters?
[00:34:31.320 --> 00:34:32.080]   For e-scooters.
[00:34:32.080 --> 00:34:34.400]   In the interim, you're going to see e-scooters everywhere.
[00:34:34.400 --> 00:34:36.120]   I've just been in Santa's A.
[00:34:36.120 --> 00:34:37.080]   They're everywhere.
[00:34:37.080 --> 00:34:38.280]   And they're just all over the place.
[00:34:38.280 --> 00:34:39.880]   It's like litter.
[00:34:39.880 --> 00:34:40.920]   Totally.
[00:34:40.920 --> 00:34:42.000]   Yeah, I can't imagine.
[00:34:42.000 --> 00:34:43.520]   I saw that first in Santa Monica,
[00:34:43.520 --> 00:34:45.280]   where this all began with bird.
[00:34:45.280 --> 00:34:47.400]   It seemed like a good idea, but it was great in practice.
[00:34:47.400 --> 00:34:48.320]   It's just like--
[00:34:48.320 --> 00:34:49.680]   Yeah, because they just leave them.
[00:34:49.680 --> 00:34:50.080]   The idea is--
[00:34:50.080 --> 00:34:52.520]   People leave them in the middle of the footpath,
[00:34:52.520 --> 00:34:55.080]   not just at the edge or near the building.
[00:34:55.080 --> 00:34:57.400]   They leave them in the middle.
[00:34:57.400 --> 00:34:58.600]   They just hop off.
[00:34:58.600 --> 00:34:59.080]   Hop off.
[00:34:59.080 --> 00:34:59.920]   Let it fall over.
[00:34:59.920 --> 00:35:00.760]   It's just lying there.
[00:35:00.760 --> 00:35:03.080]   I watched somebody leave one right in the middle of the door
[00:35:03.080 --> 00:35:04.760]   to Chipotle the other day.
[00:35:04.760 --> 00:35:05.600]   I'm just sitting there.
[00:35:05.600 --> 00:35:07.120]   Are these like city bike scooters?
[00:35:07.120 --> 00:35:08.600]   I haven't seen these out in New York yet.
[00:35:08.600 --> 00:35:10.200]   Yeah, you will.
[00:35:10.200 --> 00:35:10.720]   Great.
[00:35:10.720 --> 00:35:10.720]   Wonderful.
[00:35:10.720 --> 00:35:11.560]   It's another thing.
[00:35:11.560 --> 00:35:11.920]   It's another thing.
[00:35:11.920 --> 00:35:12.560]   For us to complain about.
[00:35:12.560 --> 00:35:13.640]   It's the next big thing.
[00:35:13.640 --> 00:35:15.480]   Yeah.
[00:35:15.480 --> 00:35:16.880]   I'll show you--
[00:35:16.880 --> 00:35:17.920]   Let me see.
[00:35:17.920 --> 00:35:20.400]   And then we're going to take a break.
[00:35:20.400 --> 00:35:23.480]   I wish I had made more hours for this show,
[00:35:23.480 --> 00:35:25.440]   because you guys are so much fun.
[00:35:25.440 --> 00:35:28.840]   So Amy, what if you could just visit the future for an hour
[00:35:28.840 --> 00:35:30.080]   and then come back?
[00:35:30.080 --> 00:35:31.800]   I don't mean you have to live there.
[00:35:31.800 --> 00:35:33.280]   Yeah, no, I think about it all the time.
[00:35:33.280 --> 00:35:37.960]   No?
[00:35:37.960 --> 00:35:39.920]   I don't.
[00:35:39.920 --> 00:35:43.000]   You really think it's going to be that bad?
[00:35:43.000 --> 00:35:45.520]   No, I worry about my own.
[00:35:45.520 --> 00:35:48.440]   So the answer is if I suffered no psychological damage,
[00:35:48.440 --> 00:35:50.960]   if I didn't move the future back, right?
[00:35:50.960 --> 00:35:52.760]   And if the pollution doesn't kill me.
[00:35:52.760 --> 00:35:54.600]   Right.
[00:35:54.600 --> 00:35:57.240]   And the only way I would do it is if I went so far
[00:35:57.240 --> 00:36:00.280]   into the future that none of my current family members
[00:36:00.280 --> 00:36:02.960]   would be alive because it would be impractical.
[00:36:02.960 --> 00:36:05.160]   So I don't want to jump ahead 20 years and like--
[00:36:05.160 --> 00:36:05.640]   And then you'd also--
[00:36:05.640 --> 00:36:07.240]   Or maybe I could go to the future and not
[00:36:07.240 --> 00:36:10.080]   encounter anybody that I know and just look at tech.
[00:36:10.080 --> 00:36:13.120]   But it'd be no point in going forward to the future in New York
[00:36:13.120 --> 00:36:14.840]   without seeing what's happening in London
[00:36:14.840 --> 00:36:17.360]   and then somewhere rural and then in Bangladesh
[00:36:17.360 --> 00:36:19.040]   in the Philippines, because the future's never
[00:36:19.040 --> 00:36:20.040]   evenly distributed.
[00:36:20.040 --> 00:36:21.000]   Of course.
[00:36:21.000 --> 00:36:21.440]   So you--
[00:36:21.440 --> 00:36:22.080]   Of course.
[00:36:22.080 --> 00:36:25.600]   But then like, who's got that kind of time?
[00:36:25.600 --> 00:36:26.680]   No, I don't.
[00:36:26.680 --> 00:36:28.600]   Especially with our underground tunnels.
[00:36:28.600 --> 00:36:30.560]   Yeah, that's right.
[00:36:30.560 --> 00:36:32.120]   Are you good?
[00:36:32.120 --> 00:36:33.960]   If I could get to it, I'm going to find it somewhere.
[00:36:33.960 --> 00:36:37.440]   There's a picture of me and one of those bird scooters
[00:36:37.440 --> 00:36:38.520]   in San Francisco.
[00:36:38.520 --> 00:36:40.160]   And it's exactly as you say.
[00:36:40.160 --> 00:36:44.040]   They are literally litter.
[00:36:44.040 --> 00:36:46.200]   I have to go back in time.
[00:36:46.200 --> 00:36:47.800]   It's easy to go back in time, by the way.
[00:36:47.800 --> 00:36:51.040]   It's very hard to go forward in time these days.
[00:36:51.040 --> 00:36:53.200]   Are the scooters right?
[00:36:53.200 --> 00:36:55.080]   The lime scooters are lime green.
[00:36:55.080 --> 00:36:57.880]   And they have an interesting feature, which they had to disable,
[00:36:57.880 --> 00:37:00.240]   that if somebody touches them without actually booking
[00:37:00.240 --> 00:37:04.480]   the scooter, they go, stand back, stand back.
[00:37:04.480 --> 00:37:06.040]   Police are being called now.
[00:37:06.040 --> 00:37:10.200]   Stand back, stand back, which is extremely--
[00:37:10.200 --> 00:37:12.560]   If you think car alarms are bad.
[00:37:12.560 --> 00:37:13.040]   Yeah.
[00:37:13.040 --> 00:37:14.840]   Extremely annoying.
[00:37:14.840 --> 00:37:17.200]   Especially when they're in the middle of the footpath
[00:37:17.200 --> 00:37:18.440]   and you stand on them to walk past them.
[00:37:18.440 --> 00:37:19.800]   Yeah, they had to disable that feature.
[00:37:19.800 --> 00:37:23.160]   That is-- turned out to be a bad idea.
[00:37:23.160 --> 00:37:24.160]   Especially since--
[00:37:24.160 --> 00:37:25.160]   On the flip side.
[00:37:25.160 --> 00:37:26.680]   The scooters had no way to call the police.
[00:37:26.680 --> 00:37:28.600]   So it was a good idea in the sense
[00:37:28.600 --> 00:37:32.680]   that 60% of all journeys in the city
[00:37:32.680 --> 00:37:34.160]   are actually less than two kilometers.
[00:37:34.160 --> 00:37:34.760]   Well, that's the thing.
[00:37:34.760 --> 00:37:35.680]   It makes perfect sense.
[00:37:35.680 --> 00:37:36.520]   It does make sense.
[00:37:36.520 --> 00:37:37.520]   What?
[00:37:37.520 --> 00:37:38.440]   How about, like, walk?
[00:37:38.440 --> 00:37:39.440]   No.
[00:37:39.440 --> 00:37:41.440]   [LAUGHTER]
[00:37:41.440 --> 00:37:43.360]   It was on my mind, but that's hardly silly.
[00:37:43.360 --> 00:37:44.000]   It's like a long--
[00:37:44.000 --> 00:37:45.440]   Silicon Valley can't take a profit.
[00:37:45.440 --> 00:37:46.560]   Can't take a margin on walking.
[00:37:46.560 --> 00:37:47.800]   It's no margin on walking.
[00:37:47.800 --> 00:37:50.200]   There's no margin in walking.
[00:37:50.200 --> 00:37:51.840]   So there's no reason to market walking.
[00:37:51.840 --> 00:37:52.600]   Actually.
[00:37:52.600 --> 00:37:53.880]   Nobody's going to come up with--
[00:37:53.880 --> 00:37:56.120]   You know what's making me walk a lot these days?
[00:37:56.120 --> 00:37:56.880]   Pokemon go.
[00:37:56.880 --> 00:37:59.200]   There is a margin of walking.
[00:37:59.200 --> 00:38:01.440]   I've got kilometers under my belt
[00:38:01.440 --> 00:38:03.680]   trying to catch these silly things.
[00:38:03.680 --> 00:38:04.200]   All right.
[00:38:04.200 --> 00:38:05.600]   Let's talk about something that's good for you.
[00:38:05.600 --> 00:38:06.360]   Brushing your teeth.
[00:38:06.360 --> 00:38:07.520]   You know you have to brush your teeth.
[00:38:07.520 --> 00:38:09.360]   Everybody has to brush their teeth, right?
[00:38:09.360 --> 00:38:13.000]   You're doing it wrong, by the way, in many cases.
[00:38:13.000 --> 00:38:14.800]   I got a quip, and I learned.
[00:38:14.800 --> 00:38:17.120]   I learned quip is the first subscription
[00:38:17.120 --> 00:38:20.080]   electric toothbrush accepted by the American Dental
[00:38:20.080 --> 00:38:21.840]   Association.
[00:38:21.840 --> 00:38:23.240]   You're not brushing long enough.
[00:38:23.240 --> 00:38:25.760]   You forget to change your brush head.
[00:38:25.760 --> 00:38:29.960]   Maybe you're buying those Bluetooth toothbrushes
[00:38:29.960 --> 00:38:31.920]   that play music in your mouth.
[00:38:31.920 --> 00:38:32.840]   This quip is great.
[00:38:32.840 --> 00:38:33.840]   It's an electric toothbrush.
[00:38:33.840 --> 00:38:37.000]   It's a fraction of the cost of those fancy brushes.
[00:38:37.000 --> 00:38:38.800]   It's 25 bucks.
[00:38:38.800 --> 00:38:39.920]   They have different models.
[00:38:39.920 --> 00:38:42.880]   But the starts at $25 is what I use.
[00:38:42.880 --> 00:38:43.720]   It's fantastic.
[00:38:43.720 --> 00:38:44.840]   There's no charger.
[00:38:44.840 --> 00:38:47.360]   It uses a AAA battery, which is fantastic.
[00:38:47.360 --> 00:38:48.560]   It lasts for months.
[00:38:48.560 --> 00:38:50.520]   It's a perfect travel brush.
[00:38:50.520 --> 00:38:52.920]   It also has a great case that with a little sticky pad on it,
[00:38:52.920 --> 00:38:54.040]   you could just put right up on the mirror
[00:38:54.040 --> 00:38:56.960]   so you don't forget to brush like that.
[00:38:56.960 --> 00:38:59.840]   And you'll automatically get, by subscription,
[00:38:59.840 --> 00:39:02.200]   new brush heads on a dentist recommended schedule
[00:39:02.200 --> 00:39:03.120]   every three months.
[00:39:03.120 --> 00:39:05.240]   And that's just $5 for the new heads,
[00:39:05.240 --> 00:39:07.240]   including free shipping worldwide.
[00:39:07.240 --> 00:39:10.520]   Quip is awesome.
[00:39:10.520 --> 00:39:12.880]   And it's so affordable, we actually got an extra quip
[00:39:12.880 --> 00:39:14.760]   for the sleepovers, 'cause kids, for some reason,
[00:39:14.760 --> 00:39:16.040]   never bring toothbrushes.
[00:39:16.040 --> 00:39:18.680]   So we just got some heads, put their names on it.
[00:39:18.680 --> 00:39:20.200]   Then when they come, they get their quip.
[00:39:20.200 --> 00:39:21.480]   Everybody loves a quip.
[00:39:21.480 --> 00:39:23.720]   Oprah put it on her O-list.
[00:39:23.720 --> 00:39:26.000]   Time magazine said it was one of the best inventions
[00:39:26.000 --> 00:39:27.080]   of the year.
[00:39:27.080 --> 00:39:28.960]   And of course, the ADA approval doesn't hurt.
[00:39:28.960 --> 00:39:31.200]   Plus, they're backed by a network of over 20,000 dentists
[00:39:31.200 --> 00:39:32.400]   and hygienists.
[00:39:32.400 --> 00:39:34.240]   And hundreds of thousands of happy brushes
[00:39:34.240 --> 00:39:36.040]   use quip every day.
[00:39:36.040 --> 00:39:38.960]   I love our quip.
[00:39:38.960 --> 00:39:42.400]   Go to getquip.com/twit.
[00:39:42.400 --> 00:39:45.280]   G-E-T-Q-U-I-P.
[00:39:45.280 --> 00:39:46.960]   Dot com/twit.
[00:39:46.960 --> 00:39:49.080]   It starts at $25.
[00:39:49.080 --> 00:39:51.880]   Get your first refill pack free
[00:39:51.880 --> 00:39:54.680]   when you purchase any quip electric toothbrush.
[00:39:54.680 --> 00:39:57.040]   So you'll have nice fresh brush heads.
[00:39:57.040 --> 00:40:00.080]   Getquip.com/twit.
[00:40:00.080 --> 00:40:02.680]   This is a good idea.
[00:40:02.680 --> 00:40:04.600]   Thank you, quip, for supporting Twit.
[00:40:04.600 --> 00:40:06.680]   And thank you, Twits, for supporting Quip.
[00:40:06.680 --> 00:40:09.120]   Goes round, comes round.
[00:40:09.120 --> 00:40:13.000]   So I see you using a MacBook.
[00:40:13.000 --> 00:40:16.440]   Yep. From 2015 at the latest.
[00:40:16.440 --> 00:40:18.120]   2012. 2012.
[00:40:18.120 --> 00:40:19.320]   Yep.
[00:40:19.320 --> 00:40:20.800]   You like it?
[00:40:20.800 --> 00:40:21.840]   Yeah, it's still working.
[00:40:21.840 --> 00:40:23.720]   Although I'm starting to do a little bit of video lately.
[00:40:23.720 --> 00:40:24.880]   I've been doing some YouTube's.
[00:40:24.880 --> 00:40:25.680]   Yeah, slow for that.
[00:40:25.680 --> 00:40:28.120]   Mostly images of me drinking beer,
[00:40:28.120 --> 00:40:29.720]   which is a common habit of mine.
[00:40:29.720 --> 00:40:32.000]   So I know you're going to mock me.
[00:40:32.000 --> 00:40:35.080]   But Apple's new MacBook Pro came out
[00:40:35.080 --> 00:40:37.720]   with the i9 process of the first MacBook
[00:40:37.720 --> 00:40:41.840]   to support as much as 32 gigabytes of DDR4 RAM.
[00:40:41.840 --> 00:40:45.920]   Not just slowpoke LD, LP DDR3.
[00:40:45.920 --> 00:40:47.520]   No DDR4 RAM.
[00:40:47.520 --> 00:40:48.640]   Yes.
[00:40:48.640 --> 00:40:53.720]   And its own dedicated graphics card, a Radeon 560,
[00:40:53.720 --> 00:40:56.200]   and for a mere $800 additional,
[00:40:56.200 --> 00:40:58.320]   you're gonna get this.
[00:40:58.320 --> 00:41:02.840]   A thermally controlled tower
[00:41:02.840 --> 00:41:06.240]   that has a additional graphics processing unit.
[00:41:06.240 --> 00:41:09.240]   An eGPU that connects via Thunderbolt 3.
[00:41:09.240 --> 00:41:10.520]   To give it even more.
[00:41:10.520 --> 00:41:11.760]   That's not what that is.
[00:41:11.760 --> 00:41:12.720]   Yeah.
[00:41:12.720 --> 00:41:14.200]   That's the new garbage can?
[00:41:14.200 --> 00:41:15.360]   It's the new garbage can,
[00:41:15.360 --> 00:41:17.520]   but guess what? It's not even a computer.
[00:41:17.520 --> 00:41:18.520]   It's just a GPU.
[00:41:18.520 --> 00:41:20.800]   It's just the graphics processor.
[00:41:20.800 --> 00:41:24.960]   There's been a lot of talk about it
[00:41:24.960 --> 00:41:27.920]   being an overheating monster.
[00:41:27.920 --> 00:41:31.880]   Dave Lee, who's a YouTuber,
[00:41:31.880 --> 00:41:33.360]   was the first to report this.
[00:41:33.360 --> 00:41:35.800]   He said, what he did is he compiled
[00:41:35.800 --> 00:41:38.840]   or rendered a Premiere Pro video file.
[00:41:38.840 --> 00:41:40.080]   Took about half an hour to render.
[00:41:40.080 --> 00:41:42.560]   And he noticed that as it got,
[00:41:42.560 --> 00:41:45.480]   as the i9, six core processor,
[00:41:45.480 --> 00:41:46.280]   and by the way, I fix it,
[00:41:46.280 --> 00:41:47.880]   says that the new MacBook Pros
[00:41:47.880 --> 00:41:50.600]   have a completely unchanged thermal cooling system
[00:41:50.600 --> 00:41:51.960]   from previous Mac Pros.
[00:41:51.960 --> 00:41:54.320]   So no attempt was made to accommodate
[00:41:54.320 --> 00:41:58.040]   a chip that is 50% hotter and uses much more power.
[00:41:58.040 --> 00:42:01.320]   He noticed that as a, you know, five minutes in,
[00:42:01.320 --> 00:42:03.280]   the GPU is suddenly going boom,
[00:42:03.280 --> 00:42:06.080]   down way, way, way down below spec.
[00:42:06.080 --> 00:42:08.960]   It's a 2.9 gigahertz i9 processor
[00:42:08.960 --> 00:42:11.240]   that peaks out at 4.8 gigahertz.
[00:42:11.240 --> 00:42:14.400]   It was operating at one gigahertz, two gigahertz.
[00:42:14.400 --> 00:42:17.760]   And it did that for the next half hour it flapped.
[00:42:17.760 --> 00:42:19.440]   It was slow.
[00:42:19.440 --> 00:42:22.000]   He said, oh man, then he put it,
[00:42:22.000 --> 00:42:23.560]   then he did something I would not recommend.
[00:42:23.560 --> 00:42:24.840]   He put it in the freezer.
[00:42:24.840 --> 00:42:29.040]   - For that whole thing. - And it was like 25% faster.
[00:42:29.040 --> 00:42:32.240]   - You know, the thing that strikes me about all of this is,
[00:42:32.240 --> 00:42:35.280]   people seem to forget that they're running a laptop.
[00:42:35.280 --> 00:42:36.800]   Do you remember the days of the Logable?
[00:42:36.800 --> 00:42:38.040]   And how many sacrifices would you get from?
[00:42:38.040 --> 00:42:40.720]   - You could have a desktop replacement laptop
[00:42:40.720 --> 00:42:44.200]   that would be 12 pounds and have a jet engine in the back,
[00:42:44.200 --> 00:42:45.760]   blowing the hot air.
[00:42:45.760 --> 00:42:46.760]   - Yeah.
[00:42:46.760 --> 00:42:48.320]   I mean, I kind of wanted,
[00:42:48.320 --> 00:42:50.160]   I mean, last time I was here, you and I talked,
[00:42:50.160 --> 00:42:51.240]   I was here in January, I said,
[00:42:51.240 --> 00:42:53.280]   Intel doesn't have any CPUs.
[00:42:53.280 --> 00:42:57.440]   There probably won't be another release of MacBooks in 2018.
[00:42:57.440 --> 00:42:58.280]   So I got that wrong.
[00:42:58.280 --> 00:42:59.120]   - You were wrong.
[00:42:59.120 --> 00:42:59.960]   - I was wrong.
[00:42:59.960 --> 00:43:01.520]   But I still think I'm right because this is showing you
[00:43:01.520 --> 00:43:04.080]   that this chip is not fit for laptops.
[00:43:04.080 --> 00:43:04.920]   - Well, right?
[00:43:04.920 --> 00:43:06.240]   So my point is that this I know--
[00:43:06.240 --> 00:43:07.960]   - I want to report my own experience
[00:43:07.960 --> 00:43:12.960]   because I foolishly spent more than $4,000 on an I9.
[00:43:12.960 --> 00:43:15.200]   - Excellent, let's see how that works.
[00:43:15.200 --> 00:43:17.880]   - And I got the Intel, they have a little widget
[00:43:17.880 --> 00:43:20.240]   that shows clock speed, but I also have I-stat menus
[00:43:20.240 --> 00:43:21.480]   which shows clock speed.
[00:43:21.480 --> 00:43:24.440]   And a couple of things I want to report.
[00:43:24.440 --> 00:43:28.560]   First of all, there's a number of different things you can do.
[00:43:28.560 --> 00:43:31.760]   One of the reasons I wanted to be faster was for Lightroom.
[00:43:31.760 --> 00:43:35.800]   'Cause I use Lightroom, I like Avid and not good,
[00:43:35.800 --> 00:43:37.160]   but an Avid amateur photographer.
[00:43:37.160 --> 00:43:38.280]   And I use Adobe's Lightroom,
[00:43:38.280 --> 00:43:40.480]   which is the worst piece of software ever.
[00:43:40.480 --> 00:43:44.920]   Because it's so slow, it's mostly Lewis scripts.
[00:43:44.920 --> 00:43:47.680]   So it's not the most performant thing ever.
[00:43:47.680 --> 00:43:50.000]   And what was really getting me is that you'd import
[00:43:50.000 --> 00:43:53.320]   a bunch of raw photos, 42 megabyte files.
[00:43:53.320 --> 00:43:55.720]   And you'd want to go pretty quickly saying,
[00:43:55.720 --> 00:43:58.120]   yeah, that one's good, no, no, no, yeah, that one's good.
[00:43:58.120 --> 00:44:01.760]   And it would take three or four seconds to go to each image.
[00:44:01.760 --> 00:44:05.360]   So on that use, this thing is great.
[00:44:05.360 --> 00:44:06.200]   - Yep.
[00:44:06.200 --> 00:44:08.360]   - In fact, with Lightroom running in the background,
[00:44:08.360 --> 00:44:10.840]   rendering pre-rendering images and stuff,
[00:44:10.840 --> 00:44:13.040]   the CPU goes up to about four gigahertz
[00:44:13.040 --> 00:44:14.280]   and kind of stays there.
[00:44:14.280 --> 00:44:15.120]   - Yeah.
[00:44:15.120 --> 00:44:15.960]   - Nice.
[00:44:15.960 --> 00:44:17.080]   Now admittedly, when you're doing that,
[00:44:17.080 --> 00:44:19.400]   the battery life goes down to about an hour.
[00:44:19.400 --> 00:44:22.400]   And the keyboard, which is the hottest part,
[00:44:22.400 --> 00:44:25.880]   we got one of those instant read thermometers,
[00:44:25.880 --> 00:44:28.840]   the keyboard goes up to 108 degrees,
[00:44:28.840 --> 00:44:31.520]   which is a little toasty for typing.
[00:44:31.520 --> 00:44:34.160]   So admittedly, that's a problem.
[00:44:35.280 --> 00:44:38.120]   If you do, and we ran Handbrake, for instance,
[00:44:38.120 --> 00:44:41.080]   which is smart enough to use all six CPUs, peg 'em out,
[00:44:41.080 --> 00:44:44.800]   doesn't use the GPU, but use all six CPUs to render video.
[00:44:44.800 --> 00:44:46.520]   And yeah, you bet.
[00:44:46.520 --> 00:44:49.040]   It goes up and then goes down because it's too hot.
[00:44:49.040 --> 00:44:49.880]   - Too hot, sorry.
[00:44:49.880 --> 00:44:51.560]   - And in fact, if you look at the temperature,
[00:44:51.560 --> 00:44:55.120]   it goes up to 100 Celsius and stops.
[00:44:55.120 --> 00:44:56.560]   And as soon as it gets to 100 Celsius,
[00:44:56.560 --> 00:44:58.960]   everything slows down because it doesn't want to go over 100.
[00:44:58.960 --> 00:45:01.640]   - So it should have been, so what we should have seen
[00:45:01.640 --> 00:45:04.760]   was the Core i9 was meant to be a 10 nanometer,
[00:45:04.760 --> 00:45:05.600]   a skylake processor.
[00:45:05.600 --> 00:45:07.360]   - A smaller, more efficient cooler.
[00:45:07.360 --> 00:45:08.960]   - Small and narrow cooler.
[00:45:08.960 --> 00:45:11.880]   And what's happened here is that in my opinion,
[00:45:11.880 --> 00:45:15.480]   is Apple has sort of caved to the public opinion and said,
[00:45:15.480 --> 00:45:17.920]   "We need to bring a new MacBook Pro to market."
[00:45:17.920 --> 00:45:20.160]   And all the work they've done with Intel's processors,
[00:45:20.160 --> 00:45:22.600]   which have been delayed, isn't able to be brought forward.
[00:45:22.600 --> 00:45:25.080]   - You could almost blame Intel for this, frankly.
[00:45:25.080 --> 00:45:26.680]   Although Apple should have said,
[00:45:26.680 --> 00:45:27.800]   "No, we're not gonna do it."
[00:45:27.800 --> 00:45:31.640]   I have to say though, as certain kinds of users,
[00:45:31.640 --> 00:45:32.480]   this is a great thing.
[00:45:32.480 --> 00:45:34.560]   I'm very happy to lightroom performance
[00:45:34.560 --> 00:45:38.160]   when I'm just doing normal things, browsing, email,
[00:45:38.160 --> 00:45:39.520]   nothing heavy duty.
[00:45:39.520 --> 00:45:41.840]   The clock speed does go down about two gigahertz
[00:45:41.840 --> 00:45:42.760]   or one and a half gigahertz,
[00:45:42.760 --> 00:45:44.360]   and I get eight hours of battery life.
[00:45:44.360 --> 00:45:46.440]   Just as much, just as I would want.
[00:45:46.440 --> 00:45:48.280]   So it is a great laptop.
[00:45:48.280 --> 00:45:51.280]   And when I'm compiling software, which is very bursty,
[00:45:51.280 --> 00:45:54.200]   very fast, runs programs very fast.
[00:45:54.200 --> 00:45:55.840]   It's only when you're doing stuff that--
[00:45:55.840 --> 00:45:57.200]   - That's the open fast.
[00:45:57.200 --> 00:45:59.760]   - Everything opens fast, everything feels snappy.
[00:45:59.760 --> 00:46:02.480]   I love this machine, they kind of fix the keyboard,
[00:46:02.480 --> 00:46:03.440]   it's not perfect.
[00:46:03.440 --> 00:46:05.080]   But I'm very happy now.
[00:46:05.080 --> 00:46:10.080]   I probably shouldn't have spent $700 on the GPU.
[00:46:10.080 --> 00:46:11.760]   - I didn't realize that was blind, so I love it.
[00:46:11.760 --> 00:46:12.920]   - What is that intended to be used for?
[00:46:12.920 --> 00:46:15.240]   - Well, you plug it in and then it all floats
[00:46:15.240 --> 00:46:16.800]   the internal GPU.
[00:46:16.800 --> 00:46:18.600]   And this thing is like a chimney.
[00:46:18.600 --> 00:46:20.640]   You feel the heat coming rising off of this.
[00:46:20.640 --> 00:46:22.480]   And in theory, I guess, this was Apple
[00:46:22.480 --> 00:46:24.360]   probably knowing that the I9 was gonna have
[00:46:24.360 --> 00:46:26.880]   thermal throttling saying, "Well, if you're really,
[00:46:26.880 --> 00:46:28.640]   "I mean, you probably look it."
[00:46:28.640 --> 00:46:30.440]   You're using a laptop that's this thin
[00:46:30.440 --> 00:46:33.080]   to try to render 4K video.
[00:46:33.080 --> 00:46:34.960]   You probably, I wouldn't recommend it.
[00:46:34.960 --> 00:46:37.560]   Maybe you get the eGPU, maybe okay, it's okay.
[00:46:37.560 --> 00:46:39.520]   Really, you should be using a desktop PC
[00:46:39.520 --> 00:46:42.800]   or a desktop replacement from another company.
[00:46:42.800 --> 00:46:43.640]   But you wanna use--
[00:46:43.640 --> 00:46:46.800]   - The problem is there's just the average person--
[00:46:46.800 --> 00:46:48.240]   - They should not get this.
[00:46:48.240 --> 00:46:49.720]   - Yeah, the good news is they're not gonna,
[00:46:49.720 --> 00:46:51.480]   'cause it's 4,000.
[00:46:51.480 --> 00:46:53.480]   Actually, you can get up to $7,000.
[00:46:53.480 --> 00:46:54.800]   - Yeah, this is-- - Really try.
[00:46:54.800 --> 00:46:56.480]   - I think the thing that's here, what we're seeing
[00:46:56.480 --> 00:46:58.640]   is we're reaching the limits of our current engineering.
[00:46:58.640 --> 00:47:02.920]   Intel has been unable to make the transition 14 to 10.
[00:47:02.920 --> 00:47:04.520]   Haven't been able to bring out a GPU
[00:47:04.520 --> 00:47:07.120]   that Apple can get into the thermal envelope of the chassis.
[00:47:07.120 --> 00:47:11.120]   Now, I think it's possible that Apple had a new chassis design
[00:47:11.120 --> 00:47:12.320]   with a new thermal capability.
[00:47:12.320 --> 00:47:13.640]   - Yeah, this is a four-year-old chassis.
[00:47:13.640 --> 00:47:15.200]   - This is a four-year-old unibody Mac.
[00:47:15.200 --> 00:47:16.360]   It's not new.
[00:47:16.360 --> 00:47:18.440]   So, it just sort of feels like they--
[00:47:18.440 --> 00:47:19.280]   - Four, three, maybe.
[00:47:19.280 --> 00:47:20.960]   - Said, "This is the only decision we can make
[00:47:20.960 --> 00:47:22.800]   "and put the I9 inside of the existing box."
[00:47:22.800 --> 00:47:24.560]   - And for some people like me, I'm happy.
[00:47:24.560 --> 00:47:25.400]   - Yep.
[00:47:25.400 --> 00:47:27.280]   - Because for the kind of work I do, it's fine.
[00:47:27.280 --> 00:47:28.120]   - It is.
[00:47:28.120 --> 00:47:30.440]   - If you're a video editor or you're gonna do long renders
[00:47:30.440 --> 00:47:31.840]   or you're gonna do long,
[00:47:31.840 --> 00:47:34.840]   I tried my Fusion Pro Studio software
[00:47:34.840 --> 00:47:36.360]   rendering a 360.
[00:47:36.360 --> 00:47:41.360]   Don't, because it'll actually be slower than a slower machine.
[00:47:41.360 --> 00:47:42.760]   - I think the thing to think about here
[00:47:42.760 --> 00:47:44.280]   is it's not so much a CPU performance
[00:47:44.280 --> 00:47:45.280]   that's gonna take us forward.
[00:47:45.280 --> 00:47:47.720]   The I9 does have better memory performance
[00:47:47.720 --> 00:47:48.920]   'cause it's using the faster memory.
[00:47:48.920 --> 00:47:49.760]   It's got a new memory.
[00:47:49.760 --> 00:47:50.600]   - I love the hardware.
[00:47:50.600 --> 00:47:51.440]   I wish we were Optane.
[00:47:51.440 --> 00:47:52.280]   Why is it not Optane?
[00:47:52.280 --> 00:47:53.560]   - Oh, there's a big, long story.
[00:47:53.560 --> 00:47:54.920]   I should send you a link about that.
[00:47:54.920 --> 00:47:57.720]   Basically, Intel hasn't been able to make Optane work.
[00:47:57.720 --> 00:47:59.000]   - This is the cross-point technology.
[00:47:59.000 --> 00:47:59.840]   - Why not?
[00:47:59.840 --> 00:48:04.400]   - The jury's kind of out and I wouldn't like to get
[00:48:04.400 --> 00:48:05.760]   too much into the weeds about it
[00:48:05.760 --> 00:48:08.040]   because partly it's not loaded into the front of my head
[00:48:08.040 --> 00:48:09.640]   but also because it's incredibly involved.
[00:48:09.640 --> 00:48:12.760]   But it looks like Intel just hasn't been able to get
[00:48:12.760 --> 00:48:15.840]   the technology to run it, to produce enough of it
[00:48:15.840 --> 00:48:17.400]   and to bring the price down.
[00:48:17.400 --> 00:48:19.480]   And also because they made a decision to bring it out
[00:48:19.480 --> 00:48:23.320]   as a sort of an SSD replacement, like faster than SSD.
[00:48:23.320 --> 00:48:26.000]   And when they brought it out, they tried to make it run
[00:48:26.000 --> 00:48:29.360]   as a SATA controller instead of moving to NVMe.
[00:48:29.360 --> 00:48:32.160]   - Which means the bottle there, 'cause is the I/O.
[00:48:32.160 --> 00:48:33.120]   - Yeah, which meant that--
[00:48:33.120 --> 00:48:33.960]   - Can't keep up.
[00:48:33.960 --> 00:48:36.280]   - And the commitment, when we talked in the middle
[00:48:36.280 --> 00:48:38.360]   of last year and I came on the show and told you all about it
[00:48:38.360 --> 00:48:40.720]   and said how excited I was and it was gonna be great.
[00:48:40.720 --> 00:48:42.720]   And they were talking like a thousand times increase
[00:48:42.720 --> 00:48:45.480]   in performance and a hundred times,
[00:48:45.480 --> 00:48:47.040]   all these fantastic numbers.
[00:48:47.040 --> 00:48:50.400]   And they came out with a 10 times faster than SSD
[00:48:50.400 --> 00:48:54.160]   at 100 times, sorry, it was a 100 times faster
[00:48:54.160 --> 00:48:55.840]   but 10 times the price of an SSD.
[00:48:55.840 --> 00:48:57.800]   Well, that's not what we were promised.
[00:48:57.800 --> 00:48:59.120]   - It's why the theory.
[00:48:59.120 --> 00:49:01.480]   - Yeah, and so they just weren't able to execute
[00:49:01.480 --> 00:49:02.400]   on the product.
[00:49:02.400 --> 00:49:04.680]   And there was a partnership between Intel and Micron
[00:49:04.680 --> 00:49:07.120]   to manufacture this with cross licensing agreement.
[00:49:07.120 --> 00:49:10.080]   And Micron is now, that partnership has now been dissolved
[00:49:10.080 --> 00:49:11.680]   and Micron has completely stopped making
[00:49:11.680 --> 00:49:13.080]   the technology completely.
[00:49:13.080 --> 00:49:13.920]   - Wow.
[00:49:13.920 --> 00:49:15.360]   - So it's only Intel and Intel's fab
[00:49:15.360 --> 00:49:17.880]   that makes those obtain chips has now shut down.
[00:49:17.880 --> 00:49:18.720]   - Oh no.
[00:49:18.720 --> 00:49:20.040]   - So they've stopped making them completely.
[00:49:20.040 --> 00:49:22.440]   - Does this comport with your theory, Amy Webb?
[00:49:22.440 --> 00:49:23.760]   - So it does.
[00:49:23.760 --> 00:49:28.760]   So this sounds like, so here's my thinking.
[00:49:29.200 --> 00:49:33.400]   So if it's true that Apple's working on its own chips, right?
[00:49:33.400 --> 00:49:37.160]   And if it's true that Apple is with the 2020 product suite
[00:49:37.160 --> 00:49:41.040]   release intending to forego Intel and instead have its own--
[00:49:41.040 --> 00:49:42.800]   - Oh, I don't know if it's gonna be by 2020.
[00:49:42.800 --> 00:49:45.240]   You thinking that soon, next year or two years?
[00:49:45.240 --> 00:49:46.760]   - That's what everybody's, I mean,
[00:49:46.760 --> 00:49:48.080]   it's like a year and a half, right?
[00:49:48.080 --> 00:49:50.360]   So if we assume that sometime between now
[00:49:50.360 --> 00:49:52.400]   and the next 24 months, is it possible
[00:49:52.400 --> 00:49:55.560]   that the engineering team was kind of like, you know,
[00:49:55.560 --> 00:49:58.680]   let's-- - But we can do one last Intel.
[00:49:58.680 --> 00:50:01.480]   - Yeah, and just sort of like not work super hard on it.
[00:50:01.480 --> 00:50:03.480]   - And by the way, everybody will go,
[00:50:03.480 --> 00:50:04.920]   oh, this thing sucks.
[00:50:04.920 --> 00:50:09.040]   So that when we come out with our A15--
[00:50:09.040 --> 00:50:09.880]   - That's exactly right.
[00:50:09.880 --> 00:50:11.360]   That's what I said. - We jump on it.
[00:50:11.360 --> 00:50:12.200]   - That's right.
[00:50:12.200 --> 00:50:13.040]   - I think you're exactly right, Amy.
[00:50:13.040 --> 00:50:14.560]   - And everybody say like, you know what?
[00:50:14.560 --> 00:50:17.240]   We don't really miss the, like the Intel thing makes sense now.
[00:50:17.240 --> 00:50:19.600]   We see that they're leaving, it's, you know.
[00:50:19.600 --> 00:50:22.040]   - You're exactly right. - Makes that decision.
[00:50:22.040 --> 00:50:23.200]   - You're exactly right.
[00:50:23.200 --> 00:50:24.040]   It's obvious, I don't know,
[00:50:24.040 --> 00:50:25.400]   the only thing I would maybe quibble with is
[00:50:25.400 --> 00:50:27.200]   I don't know if it'll be by as fast as 2020.
[00:50:27.200 --> 00:50:31.120]   It's a devilishly hard thing to do to make a processor that fast.
[00:50:31.120 --> 00:50:34.240]   - Everybody's working on their own silicon at the moment.
[00:50:34.240 --> 00:50:37.080]   So-- - I feel so bad for Intel.
[00:50:37.080 --> 00:50:38.960]   You know, Brian Krasanna just now obvious.
[00:50:38.960 --> 00:50:41.800]   He didn't get fired 'cause of his daliances.
[00:50:41.800 --> 00:50:43.240]   He got fired 'cause he screwed.
[00:50:43.240 --> 00:50:47.520]   - That was really, that was a strange announcement, I thought.
[00:50:47.520 --> 00:50:49.160]   - It's like this, the pain don't attach the fact
[00:50:49.160 --> 00:50:51.080]   that the company's falling apart.
[00:50:51.080 --> 00:50:53.280]   It's because he has sex with an underling.
[00:50:53.280 --> 00:50:54.440]   That's what it is.
[00:50:54.440 --> 00:50:56.200]   You'll love that, they'll eat this up.
[00:50:56.200 --> 00:50:58.400]   - I can see it, I know the strategy meeting
[00:50:58.400 --> 00:51:00.160]   in the board room there where the PR people said,
[00:51:00.160 --> 00:51:02.840]   well, you know, if you said it was a sex scandal,
[00:51:02.840 --> 00:51:05.240]   they'd never report on this other stuff.
[00:51:05.240 --> 00:51:07.200]   - Divergence, absolutely.
[00:51:07.200 --> 00:51:10.400]   So, I mean, that's my crazy conspiracy theory,
[00:51:10.400 --> 00:51:11.680]   but I stand by it.
[00:51:11.680 --> 00:51:13.600]   - I mean, obviously, Intel is not in trouble.
[00:51:13.600 --> 00:51:15.640]   They're making money, like, gobs of money,
[00:51:15.640 --> 00:51:18.400]   but they does feel like they've had some engineering
[00:51:18.400 --> 00:51:20.800]   challenges, they can't get to 10 nanometers,
[00:51:20.800 --> 00:51:21.960]   which is a big deal.
[00:51:21.960 --> 00:51:24.320]   - Well, as other companies can, why can't Intel?
[00:51:24.320 --> 00:51:26.120]   - Well, Samsung's down to seven nanometers.
[00:51:26.120 --> 00:51:28.000]   - Yeah, for what you see.
[00:51:28.000 --> 00:51:29.560]   - So, the chips in your latest iPhone
[00:51:29.560 --> 00:51:31.640]   are all seven nanometer, all those ARM CPUs.
[00:51:31.640 --> 00:51:32.560]   - So, what's...
[00:51:32.560 --> 00:51:34.960]   - It's just they chose a different technology,
[00:51:34.960 --> 00:51:37.400]   so the actual material science behind it,
[00:51:37.400 --> 00:51:39.200]   Samsung went for a particular path,
[00:51:39.200 --> 00:51:41.560]   and Intel went down another, they chose a different...
[00:51:41.560 --> 00:51:43.680]   The material...
[00:51:43.680 --> 00:51:45.200]   - The first time the Intel has gone down
[00:51:45.200 --> 00:51:47.000]   the wrong work in the road.
[00:51:47.000 --> 00:51:48.920]   - No, they've done it before and managed to get away with it.
[00:51:48.920 --> 00:51:50.320]   However, the good news is that...
[00:51:50.320 --> 00:51:52.240]   - Well, titanium never really did take off.
[00:51:52.240 --> 00:51:54.720]   - Well, that wasn't a manufacturing process.
[00:51:54.720 --> 00:51:57.200]   That was a failure to read the market.
[00:51:57.200 --> 00:51:59.680]   So, I mean, titanium was doomed pretty much after,
[00:51:59.680 --> 00:52:01.840]   as soon as it hit the market, we all knew that.
[00:52:01.840 --> 00:52:03.160]   But I think the other side of it is,
[00:52:03.160 --> 00:52:04.640]   is that Intel's gonna be fine.
[00:52:04.640 --> 00:52:06.320]   We saw in the last quarter,
[00:52:06.320 --> 00:52:08.280]   IDC announced the numbers saying...
[00:52:08.280 --> 00:52:09.320]   - PC sales are up.
[00:52:09.320 --> 00:52:10.720]   - PC sales are up, 3%.
[00:52:10.720 --> 00:52:12.400]   - Not a lot, not something to, you know,
[00:52:12.400 --> 00:52:13.960]   you're not gonna eat cake,
[00:52:13.960 --> 00:52:15.720]   but at least you're not gonna be sweating.
[00:52:15.720 --> 00:52:18.640]   - No, so there is some hope that Intel's got a buffer
[00:52:18.640 --> 00:52:21.280]   to be able to survive, but we are also seeing,
[00:52:21.280 --> 00:52:24.240]   like for example, Broadcom has been growing strongly
[00:52:24.240 --> 00:52:26.840]   in recent years and making a number of acquisitions
[00:52:26.840 --> 00:52:28.120]   trying to do. - Making arm processors.
[00:52:28.120 --> 00:52:28.960]   - Yep.
[00:52:28.960 --> 00:52:31.840]   And now we've seen them take over computer associates.
[00:52:31.840 --> 00:52:33.960]   - I think it's be unfair to call what Apple's about
[00:52:33.960 --> 00:52:35.320]   to do an arm processor though.
[00:52:35.320 --> 00:52:37.000]   I know they have an arm architectural license,
[00:52:37.000 --> 00:52:39.640]   but it seems to me Apple, custom silicon,
[00:52:39.640 --> 00:52:42.880]   one of the advantages Apple has is when they're designing
[00:52:42.880 --> 00:52:46.520]   the chips for their own hardware, in their own use cases,
[00:52:46.520 --> 00:52:49.560]   they can optimize in ways that a general purpose chip
[00:52:49.560 --> 00:52:50.400]   just cannot.
[00:52:51.920 --> 00:52:54.320]   - I'm surprised it took them this long.
[00:52:54.320 --> 00:52:57.120]   I mean, I know that it's like separate.
[00:52:57.120 --> 00:52:59.040]   - They wish they don't have a lot of--
[00:52:59.040 --> 00:53:00.760]   - Earlier, I guarantee you.
[00:53:00.760 --> 00:53:01.680]   - You can see-- - Well, I mean,
[00:53:01.680 --> 00:53:04.360]   the other thing is they're also, so Apple,
[00:53:04.360 --> 00:53:07.200]   so everybody's also producing their own silicon for AI
[00:53:07.200 --> 00:53:09.240]   and their own frameworks.
[00:53:09.240 --> 00:53:12.600]   So it just seems like this is a natural sort of next
[00:53:12.600 --> 00:53:14.520]   inflection point where everybody's moving toward,
[00:53:14.520 --> 00:53:16.640]   and I'm kind of surprised it took this long.
[00:53:16.640 --> 00:53:18.160]   - You can see Apple's side link up to it though.
[00:53:18.160 --> 00:53:20.240]   You're familiar with the T2 chip in the new MacBook.
[00:53:20.240 --> 00:53:21.080]   - Yes.
[00:53:21.080 --> 00:53:23.400]   - The chip now has the NVMe bus.
[00:53:23.400 --> 00:53:25.040]   It's now doing all of the crypto ops.
[00:53:25.040 --> 00:53:26.880]   It's now doing a number of GPU ops.
[00:53:26.880 --> 00:53:29.240]   - It's a big reason why this is a faster price.
[00:53:29.240 --> 00:53:31.200]   Computer is because a lot of that stuff's offloaded.
[00:53:31.200 --> 00:53:33.200]   - That's right, memory handling.
[00:53:33.200 --> 00:53:34.800]   There's a lot of stuff in that T2 chip.
[00:53:34.800 --> 00:53:37.800]   That's Apple's side-ling up to its own CPU.
[00:53:37.800 --> 00:53:40.600]   Now, that's not to say that Apple will have its own,
[00:53:40.600 --> 00:53:45.360]   but my sense of it is, is that Apple's side-ling up to this,
[00:53:45.360 --> 00:53:47.040]   practicing, practicing, building.
[00:53:47.040 --> 00:53:49.400]   Apple never goes and does something in a big
[00:53:49.400 --> 00:53:52.280]   and commits the company to technology like that.
[00:53:52.280 --> 00:53:56.560]   But I could see the next MacBook Air being an ARM CPU.
[00:53:56.560 --> 00:53:58.520]   Not the MacBook Pros, not the MacBooks,
[00:53:58.520 --> 00:53:59.360]   but the ARM.
[00:53:59.360 --> 00:54:00.840]   - In a way, Microsoft's shifting towards that.
[00:54:00.840 --> 00:54:02.800]   Do you see Windows on ARM?
[00:54:02.800 --> 00:54:04.440]   You're seeing the low end.
[00:54:04.440 --> 00:54:08.040]   I have an HP NVX2, which is a Windows 10 on ARM.
[00:54:08.040 --> 00:54:10.000]   It's a pig, it's really slow,
[00:54:10.000 --> 00:54:13.440]   but maybe the next, the 845 will be a little faster.
[00:54:13.440 --> 00:54:14.280]   - Yep.
[00:54:14.280 --> 00:54:16.080]   - There's some room there at the bottom end.
[00:54:16.080 --> 00:54:17.720]   - There's plenty of room in the ARM CPUs
[00:54:17.720 --> 00:54:20.640]   to improve the memory performance and the disk handling
[00:54:20.640 --> 00:54:23.480]   and the way that it transfers data between itself,
[00:54:23.480 --> 00:54:25.400]   like the Numa architecture is on board.
[00:54:25.400 --> 00:54:27.240]   If you have multiple threads,
[00:54:27.240 --> 00:54:29.200]   how the threads communicate with each other.
[00:54:29.200 --> 00:54:31.920]   - So I always felt like ARM was inherently gonna be slower
[00:54:31.920 --> 00:54:34.800]   because of its choice of kind of a more of a risk architecture.
[00:54:34.800 --> 00:54:35.960]   - Yep.
[00:54:35.960 --> 00:54:37.840]   - That's just kind of the nature of the beast.
[00:54:37.840 --> 00:54:39.520]   Is that the case?
[00:54:39.520 --> 00:54:40.440]   - That's my sense of it.
[00:54:40.440 --> 00:54:42.640]   It'll always have less forward horsepower
[00:54:42.640 --> 00:54:44.680]   because it's just not clocking so fast.
[00:54:44.680 --> 00:54:46.680]   Because they're focusing on power consumption.
[00:54:46.680 --> 00:54:47.520]   - They're very efficient. - And thermal,
[00:54:47.520 --> 00:54:51.240]   on very efficient, hence the MacBook Air idea.
[00:54:51.240 --> 00:54:52.280]   - Could you solve the,
[00:54:52.280 --> 00:54:53.360]   yeah, start with an Air because it's a low end.
[00:54:53.360 --> 00:54:56.200]   - Start with an Air, low end, don't make too many promises
[00:54:56.200 --> 00:54:57.560]   and then iterate for a few years.
[00:54:57.560 --> 00:54:59.680]   - If you solve the problem the way Intel solved the problem
[00:54:59.680 --> 00:55:02.240]   with more cores, I mean, that's basically what Intel did
[00:55:02.240 --> 00:55:03.960]   is said, well, we can't get them smaller,
[00:55:03.960 --> 00:55:06.520]   we can't get them faster, so let's just give you more of them.
[00:55:06.520 --> 00:55:08.920]   - Well, we can't, yes, that's exactly where it's gonna go.
[00:55:08.920 --> 00:55:09.760]   - What ARM will do the same thing.
[00:55:09.760 --> 00:55:10.600]   - And you're gonna have,
[00:55:10.600 --> 00:55:12.240]   gonna have speculative thread execution, you know,
[00:55:12.240 --> 00:55:13.080]   and-- - That's what got it
[00:55:13.080 --> 00:55:15.760]   until it's in trouble though, with Spectre and Meltdown.
[00:55:15.760 --> 00:55:17.880]   - And those things, that's because they're using
[00:55:17.880 --> 00:55:20.080]   Spectre to thread it, they're pre-executing.
[00:55:20.080 --> 00:55:21.760]   But instead of having just six cores,
[00:55:21.760 --> 00:55:23.080]   and like a hyperthreaded architecture,
[00:55:23.080 --> 00:55:26.840]   we've got six cores or 12 cores, have 128.
[00:55:26.840 --> 00:55:27.960]   - Oh! - Right?
[00:55:27.960 --> 00:55:30.440]   - So we now have network adapters that go into desktops
[00:55:30.440 --> 00:55:33.240]   that have 128 ARM cores plus an FPGA
[00:55:33.240 --> 00:55:35.120]   and costs less than a thousand bucks.
[00:55:35.120 --> 00:55:36.880]   - Really? - Yeah, so if you're building a data center,
[00:55:36.880 --> 00:55:39.400]   like if I'm out working with a customer building a data center,
[00:55:39.400 --> 00:55:42.840]   we're putting smart necks in that has 128 core ARM--
[00:55:42.840 --> 00:55:44.080]   - Wow. - And an FPGA.
[00:55:44.080 --> 00:55:45.640]   - Because we've seen that in GPUs as well,
[00:55:45.640 --> 00:55:46.480]   haven't we?
[00:55:46.480 --> 00:55:47.600]   There are many core, multi-core GPUs.
[00:55:47.600 --> 00:55:50.080]   - And that also is the other place where there's so much room
[00:55:50.080 --> 00:55:52.080]   to be made from, and this is where metal comes in,
[00:55:52.080 --> 00:55:53.160]   Apple's Metal.
[00:55:53.160 --> 00:55:54.720]   So you now are talking to an API--
[00:55:54.720 --> 00:55:56.640]   - They own the whole stack, they have it all.
[00:55:56.640 --> 00:55:57.480]   - Yeah, you talk to the metal,
[00:55:57.480 --> 00:55:59.040]   and then what's the GPU underneath that?
[00:55:59.040 --> 00:56:00.560]   You don't care. - You don't care.
[00:56:00.560 --> 00:56:01.400]   - They're gonna optimize,
[00:56:01.400 --> 00:56:02.400]   and then they did the ARM thing.
[00:56:02.400 --> 00:56:04.720]   So we saw in the last keynote,
[00:56:04.720 --> 00:56:07.000]   they're starting to produce an ARM layer
[00:56:07.000 --> 00:56:09.200]   by bringing the apps from the iPhone to run on the Mac,
[00:56:09.200 --> 00:56:11.080]   just a small subset.
[00:56:11.080 --> 00:56:11.920]   It'll be interesting. - And the T2
[00:56:11.920 --> 00:56:14.840]   is an ARM chip. - Yep, yes.
[00:56:14.840 --> 00:56:15.800]   - Probably heading on the way. - So in a way,
[00:56:15.800 --> 00:56:17.600]   this is a hybrid. - It'd be more than an ARM,
[00:56:17.600 --> 00:56:20.000]   it would be an ARM with a bunch of other subprocessors.
[00:56:20.000 --> 00:56:22.840]   Like, they'd have things in the die,
[00:56:22.840 --> 00:56:25.640]   like IO chips and memory bus handlers and so forth.
[00:56:25.640 --> 00:56:28.000]   - So basically what I bought,
[00:56:28.000 --> 00:56:30.440]   and I bought it for you folks,
[00:56:30.440 --> 00:56:32.680]   'cause I wanted to test it so I could tell you,
[00:56:32.680 --> 00:56:33.920]   is an interim product.
[00:56:33.920 --> 00:56:37.800]   It's kind of the last gasp for Intel architecture.
[00:56:37.800 --> 00:56:40.760]   I bet you you're right on Mac OS.
[00:56:40.760 --> 00:56:43.960]   And they're putting it out for a high-end audience
[00:56:43.960 --> 00:56:45.360]   and wants performance,
[00:56:45.360 --> 00:56:48.360]   understands a little bit that maybe you need something crazy
[00:56:48.360 --> 00:56:50.040]   like this EGPU,
[00:56:50.040 --> 00:56:52.000]   and you're at it's gonna cost you thousands of dollars.
[00:56:52.000 --> 00:56:54.880]   But for the main market, can they wait though?
[00:56:54.880 --> 00:56:56.920]   And I think that's probably what you were saying, Amy,
[00:56:56.920 --> 00:56:58.600]   was they just couldn't wait.
[00:56:58.600 --> 00:57:00.000]   They had to do something, but 2020.
[00:57:00.000 --> 00:57:03.200]   - Well, I think that there's also this,
[00:57:03.200 --> 00:57:06.480]   so this goes back to the incredibly smart point
[00:57:06.480 --> 00:57:08.200]   that Greg had made earlier,
[00:57:08.200 --> 00:57:09.200]   which is,
[00:57:09.200 --> 00:57:12.800]   tech companies now are beholden
[00:57:12.800 --> 00:57:15.440]   to legacy capital in some way.
[00:57:15.440 --> 00:57:19.000]   And so I think there's this incredible pressure
[00:57:19.000 --> 00:57:22.160]   to keep putting something into market,
[00:57:22.160 --> 00:57:25.920]   even though it may not be required or necessary.
[00:57:25.920 --> 00:57:27.160]   So think of all the companies that are--
[00:57:27.160 --> 00:57:29.240]   - Apple's been pretty resistant to that.
[00:57:29.240 --> 00:57:32.560]   I mean, that Mac Pro you're looking at
[00:57:32.560 --> 00:57:35.560]   has been updated in five years.
[00:57:35.560 --> 00:57:36.840]   - Okay.
[00:57:36.840 --> 00:57:37.760]   - It's had monotweaks.
[00:57:37.760 --> 00:57:40.080]   - The question is, did the new computer,
[00:57:40.080 --> 00:57:42.160]   like did this stuff need to exist?
[00:57:42.160 --> 00:57:44.040]   And I don't know that the answer is yes.
[00:57:44.040 --> 00:57:45.480]   And if you look at all these different--
[00:57:45.480 --> 00:57:47.280]   - I think they felt a lot of pressure to--
[00:57:47.280 --> 00:57:49.160]   - Well, that's what I'm talking about, right?
[00:57:49.160 --> 00:57:50.920]   And if you look at a lot of different companies,
[00:57:50.920 --> 00:57:53.360]   including Microsoft that are maybe late
[00:57:53.360 --> 00:57:54.640]   to the smart speaker market
[00:57:54.640 --> 00:57:57.840]   and are rushing to put out a smart speaker,
[00:57:57.840 --> 00:58:01.560]   the question is, does it make sense to focus on a smart--
[00:58:01.560 --> 00:58:03.800]   whatever it is,
[00:58:03.800 --> 00:58:06.040]   to let people know that you're still around,
[00:58:06.040 --> 00:58:08.080]   whatever, to appease expectations,
[00:58:08.080 --> 00:58:10.200]   which I realize is a core part of business,
[00:58:10.200 --> 00:58:13.800]   or to put those resources into building whatever is next.
[00:58:13.800 --> 00:58:15.600]   And I think that that's how we wind up
[00:58:15.600 --> 00:58:19.840]   with a lot of surprisingly bad technology.
[00:58:19.840 --> 00:58:21.840]   - Yeah, look what Apple's done with the iPhone though.
[00:58:21.840 --> 00:58:26.200]   They have stuck to a very consistent year over year schedule.
[00:58:26.200 --> 00:58:28.600]   Incremental improvements, yes, but in every case,
[00:58:28.600 --> 00:58:29.440]   something--
[00:58:29.440 --> 00:58:30.280]   - There's always a--
[00:58:30.280 --> 00:58:31.400]   - Well, that's a different model though.
[00:58:31.400 --> 00:58:34.080]   That's a different model because people don't have to lease,
[00:58:34.080 --> 00:58:35.800]   you know, you're not paying a third party
[00:58:35.800 --> 00:58:37.600]   to use your laptop.
[00:58:37.600 --> 00:58:39.520]   So the phone doesn't work.
[00:58:39.520 --> 00:58:41.760]   - And I think that there's a core difference.
[00:58:41.760 --> 00:58:44.840]   And none of us, even, you know,
[00:58:44.840 --> 00:58:47.080]   I bought my phone outright.
[00:58:47.080 --> 00:58:50.400]   But essentially I'm leasing a product.
[00:58:50.400 --> 00:58:51.560]   - Right, yeah. - Right?
[00:58:51.560 --> 00:58:53.120]   The product is the ability for me
[00:58:53.120 --> 00:58:54.720]   to use mobile connectivity.
[00:58:54.720 --> 00:58:56.720]   - And in fact, there's good reason for them to do it yearly
[00:58:56.720 --> 00:58:58.440]   because people are on a two year contract.
[00:58:58.440 --> 00:58:59.600]   There's a turnover over here.
[00:58:59.600 --> 00:59:00.520]   - No, that's gone.
[00:59:00.520 --> 00:59:03.040]   The days of renting phones from carriers and telcos
[00:59:03.040 --> 00:59:03.880]   is pretty much over.
[00:59:03.880 --> 00:59:04.880]   - Well, I know they're not subsidized,
[00:59:04.880 --> 00:59:06.600]   but I still think that people are specticated--
[00:59:06.600 --> 00:59:07.440]   - Well, but there are--
[00:59:07.440 --> 00:59:08.280]   - Phone every two years,
[00:59:08.280 --> 00:59:09.440]   whether they need one or not.
[00:59:09.440 --> 00:59:10.280]   - No.
[00:59:10.280 --> 00:59:11.120]   - No.
[00:59:11.120 --> 00:59:13.760]   - That just justifies you to pay monthly.
[00:59:13.760 --> 00:59:16.280]   When I broke my phone for the 15th time
[00:59:16.280 --> 00:59:20.360]   because these stupid Samsungs are made out of rice paper
[00:59:20.360 --> 00:59:23.680]   and screens are constantly shattering.
[00:59:23.680 --> 00:59:26.800]   So I just tried to pay outright and it was impossible.
[00:59:26.800 --> 00:59:30.080]   - They want you by month to month, yeah.
[00:59:30.080 --> 00:59:31.640]   - No, they want you to pay a monthly fee.
[00:59:31.640 --> 00:59:33.680]   I mean, they're essentially in the United States
[00:59:33.680 --> 00:59:36.400]   trying to push nudge everybody into a monthly--
[00:59:36.400 --> 00:59:37.960]   - No, it's a good compelling nudge
[00:59:37.960 --> 00:59:40.320]   because they say you only pay monthly
[00:59:40.320 --> 00:59:41.800]   and you get a new phone when the new one comes out,
[00:59:41.800 --> 00:59:42.640]   you get a new one on a Mac.
[00:59:42.640 --> 00:59:44.280]   - Sure, but it also solves the companies
[00:59:44.280 --> 00:59:47.480]   for making any significant improvements in their devices.
[00:59:47.480 --> 00:59:48.320]   All we're seeing are--
[00:59:48.320 --> 00:59:50.120]   - Any subscriptions of us put you at risk of that.
[00:59:50.120 --> 00:59:52.280]   Any time you sign up for the subscription service,
[00:59:52.280 --> 00:59:54.880]   you're actually incentivizing the maker
[00:59:54.880 --> 00:59:56.600]   to not give you anything new.
[00:59:56.600 --> 00:59:57.920]   - Because you'll-- - Wait a minute, wait a minute.
[00:59:57.920 --> 01:00:01.000]   - See, sales go down as a result.
[01:00:01.000 --> 01:00:02.200]   - Why is that?
[01:00:02.200 --> 01:00:03.040]   What--
[01:00:03.040 --> 01:00:04.520]   - So because--
[01:00:04.520 --> 01:00:06.920]   - They get an annuity, they get income.
[01:00:06.920 --> 01:00:07.760]   They're happy.
[01:00:07.760 --> 01:00:09.080]   They deliver something new or not.
[01:00:09.080 --> 01:00:11.120]   - Oh. - Right, so it's the same reason
[01:00:11.120 --> 01:00:13.720]   that we don't have 5G in a lot of places
[01:00:13.720 --> 01:00:15.560]   around the United States 'cause there's been no--
[01:00:15.560 --> 01:00:16.880]   - There's no competition.
[01:00:16.880 --> 01:00:17.720]   - You're being--
[01:00:17.720 --> 01:00:20.400]   We missed you being sarcastic with us, didn't we?
[01:00:20.400 --> 01:00:21.640]   - No, I wasn't being certain.
[01:00:21.640 --> 01:00:23.480]   Well, I don't know what I was being.
[01:00:23.480 --> 01:00:24.400]   - It's not about--
[01:00:24.400 --> 01:00:25.720]   Well, there is no 5G yet.
[01:00:25.720 --> 01:00:26.720]   - Stupid.
[01:00:26.720 --> 01:00:27.880]   - I wanna come back.
[01:00:27.880 --> 01:00:29.360]   There is no 5G standard yet.
[01:00:29.360 --> 01:00:30.880]   That's still an evolving thing.
[01:00:30.880 --> 01:00:31.840]   That's why they're talking about
[01:00:31.840 --> 01:00:32.680]   four or four and a half.
[01:00:32.680 --> 01:00:33.680]   - I think Japan would--
[01:00:33.680 --> 01:00:34.520]   - Would you say?
[01:00:34.520 --> 01:00:35.360]   - Disagree.
[01:00:35.360 --> 01:00:36.760]   - Does Japan have 5G?
[01:00:36.760 --> 01:00:40.040]   - Japan has put considerable resources
[01:00:40.040 --> 01:00:42.120]   into getting a 5G network completely up
[01:00:42.120 --> 01:00:43.400]   and running ahead of the Olympics.
[01:00:43.400 --> 01:00:45.760]   - Does 5G mean anything?
[01:00:45.760 --> 01:00:47.120]   - Yes, it does.
[01:00:47.120 --> 01:00:49.400]   Comcast will tell you no.
[01:00:49.400 --> 01:00:51.360]   And other providers in the United States will tell you no.
[01:00:51.360 --> 01:00:53.560]   - We should be honest, when AT&T and T-Mobile say
[01:00:53.560 --> 01:00:55.760]   we've got 5G in New York City,
[01:00:55.760 --> 01:00:59.200]   that's not the same 5G Japan's putting in.
[01:00:59.200 --> 01:01:02.000]   - 5G is a very loose collection
[01:01:02.000 --> 01:01:05.360]   of hundreds of possible different modes and operations.
[01:01:05.360 --> 01:01:07.120]   - Some of them-- - I think the name says it all.
[01:01:07.120 --> 01:01:09.080]   It stands for fifth generation.
[01:01:09.080 --> 01:01:11.080]   - Yeah, but it's-- - It's like LTE,
[01:01:11.080 --> 01:01:12.640]   long-term evolution.
[01:01:12.640 --> 01:01:14.680]   I mean, these names kind of are giveaways.
[01:01:14.680 --> 01:01:17.480]   - Yeah, well, the 4G LTE was very much designed
[01:01:17.480 --> 01:01:20.960]   to do voice over and data to handsets.
[01:01:20.960 --> 01:01:24.440]   5G is much more about building mobile broadband.
[01:01:24.440 --> 01:01:26.880]   So instead of running cable or fiber optic to the home,
[01:01:26.880 --> 01:01:29.640]   you can use the wireless spectrum to do it.
[01:01:29.640 --> 01:01:31.920]   They're also talking about being able to deploy micro cells.
[01:01:31.920 --> 01:01:33.400]   So there's a whole bunch of standards in there
[01:01:33.400 --> 01:01:34.680]   to do more-- - Yeah, there's a huge issue though,
[01:01:34.680 --> 01:01:38.400]   because you need more sites, lots more sites,
[01:01:38.400 --> 01:01:40.520]   and don't they all have to have high-speed back halls?
[01:01:40.520 --> 01:01:42.520]   - Well, if you're doing mobile broadband,
[01:01:42.520 --> 01:01:44.120]   and that's a very high duty cycle,
[01:01:44.120 --> 01:01:45.480]   so it's consuming up the bandwidth,
[01:01:45.480 --> 01:01:48.200]   you need to be able to have a lot more spectrum available,
[01:01:48.200 --> 01:01:49.960]   but there's no spectrum to have
[01:01:49.960 --> 01:01:51.840]   unless you turn off all of the analog TV sessions.
[01:01:51.840 --> 01:01:53.640]   - This is where Japan has a huge advantage,
[01:01:53.640 --> 01:01:55.920]   very dense population. - It's a very dense film.
[01:01:55.920 --> 01:01:58.880]   - And a sort of autocratic approach to technology.
[01:01:58.880 --> 01:02:02.760]   - And people with CDMA, like when I lived,
[01:02:02.760 --> 01:02:06.240]   when I was living there, there was a question about CDMA
[01:02:06.240 --> 01:02:09.000]   versus, there was a question about standards.
[01:02:09.000 --> 01:02:10.320]   - Yes.
[01:02:10.320 --> 01:02:13.080]   - And because it's Japan, they just sort of decided
[01:02:13.080 --> 01:02:15.040]   that this is now what everybody is doing.
[01:02:15.040 --> 01:02:16.800]   And there wasn't a lot of political wrangling
[01:02:16.800 --> 01:02:18.360]   and technology as far as I'm concerned,
[01:02:18.360 --> 01:02:21.120]   Japan is not politicized the way that it is in the US.
[01:02:21.120 --> 01:02:26.120]   So they are fully planning on deploying
[01:02:26.120 --> 01:02:28.720]   a fifth generation set of standards.
[01:02:28.720 --> 01:02:30.200]   - Yes, but there are many--
[01:02:30.200 --> 01:02:31.840]   - Many standards inside of 5G,
[01:02:31.840 --> 01:02:32.680]   and it's actually a lot of things.
[01:02:32.680 --> 01:02:34.920]   One of them is low-raw, an NB-IOT,
[01:02:34.920 --> 01:02:36.200]   that's extremely low-powered,
[01:02:36.200 --> 01:02:38.720]   so you'll be able to use a little button battery
[01:02:38.720 --> 01:02:40.920]   to run a device for 10 years.
[01:02:40.920 --> 01:02:42.440]   So that's not an IOT--
[01:02:42.440 --> 01:02:43.640]   - That's for the IOT crap.
[01:02:43.640 --> 01:02:44.560]   - That's for the IOT crap.
[01:02:44.560 --> 01:02:46.360]   So instead of having this massive battery in here
[01:02:46.360 --> 01:02:48.360]   that has to be recharged every day,
[01:02:48.360 --> 01:02:50.720]   the current 4G standards are an attempt to be low-power,
[01:02:50.720 --> 01:02:52.120]   but they're really not.
[01:02:52.120 --> 01:02:53.240]   So now we're seeing--
[01:02:53.240 --> 01:02:54.880]   - So we'll change our phones too?
[01:02:54.880 --> 01:02:55.720]   - Yes, absolutely.
[01:02:55.720 --> 01:02:58.280]   - We'll potentially be able to use less battery power
[01:02:58.280 --> 01:02:59.360]   because if you're closed,
[01:02:59.360 --> 01:03:01.960]   it'll probably be unchanged.
[01:03:01.960 --> 01:03:04.640]   Smartphones will continue as is maybe a little bit less power.
[01:03:04.640 --> 01:03:05.480]   - But faster.
[01:03:05.480 --> 01:03:07.440]   - Faster for short bursts.
[01:03:07.440 --> 01:03:10.640]   - What it will do is help usher in glasses.
[01:03:10.640 --> 01:03:13.600]   So the models that we've built show that this is the beginning
[01:03:13.600 --> 01:03:14.920]   of the end of smartphones.
[01:03:14.920 --> 01:03:17.880]   So 2018, we've reached the apex and it's all--
[01:03:17.880 --> 01:03:19.960]   - It's not about IOT, it's about wearables.
[01:03:19.960 --> 01:03:20.800]   - It's also about things like--
[01:03:20.800 --> 01:03:21.640]   - It's about--
[01:03:21.640 --> 01:03:23.400]   - And it's about wearables too.
[01:03:23.400 --> 01:03:24.800]   - But it's also about autonomous cars,
[01:03:24.800 --> 01:03:26.160]   carving out channels that work for--
[01:03:26.160 --> 01:03:28.400]   So the actual 5G edge will be intelligently
[01:03:28.400 --> 01:03:29.240]   providing information--
[01:03:29.240 --> 01:03:30.080]   - Or--
[01:03:30.080 --> 01:03:30.920]   - To what autonomous cars?
[01:03:30.920 --> 01:03:32.320]   - Flying cars.
[01:03:32.320 --> 01:03:33.480]   (laughing)
[01:03:33.480 --> 01:03:36.720]   'Cause there's no 5G in tunnels under the ground.
[01:03:36.720 --> 01:03:38.320]   (laughing)
[01:03:38.320 --> 01:03:40.240]   I'm just saying.
[01:03:40.240 --> 01:03:41.080]   All right, let's take a break.
[01:03:41.080 --> 01:03:46.240]   I love having Greg Farro on from the Packet Pusher's network.
[01:03:46.240 --> 01:03:47.880]   It's great, thank you for coming out here.
[01:03:47.880 --> 01:03:49.600]   Greg's normally based in the UK.
[01:03:49.600 --> 01:03:50.440]   - Yeah.
[01:03:50.440 --> 01:03:52.840]   - He's out here for something way over my head,
[01:03:52.840 --> 01:03:54.880]   but we just got him to come up.
[01:03:54.880 --> 01:03:57.920]   And Amy Webb, who has never been in our studio,
[01:03:57.920 --> 01:03:59.520]   will someday come visit us.
[01:03:59.520 --> 01:04:00.520]   - I will someday be there.
[01:04:00.520 --> 01:04:02.440]   - Please, 'cause I wanna meet you.
[01:04:02.440 --> 01:04:04.760]   But we love having you on too.
[01:04:04.760 --> 01:04:07.680]   Are you in, I can never remember, you're in DC?
[01:04:07.680 --> 01:04:09.400]   - I'm in Baltimore today.
[01:04:09.400 --> 01:04:10.240]   - Baltimore.
[01:04:10.240 --> 01:04:11.760]   - So sometimes I'm in New York,
[01:04:11.760 --> 01:04:14.040]   sometimes I'm in Baltimore today's Baltimore.
[01:04:14.040 --> 01:04:16.520]   - She's always in the place.
[01:04:16.520 --> 01:04:19.400]   She's a futurist, her book, "The Signals Are Talking"
[01:04:19.400 --> 01:04:20.800]   is a must read.
[01:04:20.800 --> 01:04:23.880]   She was foolish enough to tell you how she does her craft,
[01:04:23.880 --> 01:04:25.440]   so you could be a futurist too.
[01:04:26.400 --> 01:04:28.480]   Even though I'm a little worried that we're talking
[01:04:28.480 --> 01:04:30.320]   to a futurist who says I wouldn't wanna see
[01:04:30.320 --> 01:04:32.920]   what it's gonna be like in 20 years.
[01:04:32.920 --> 01:04:33.980]   That scares me.
[01:04:33.980 --> 01:04:36.800]   - I wanna just put this out there,
[01:04:36.800 --> 01:04:38.880]   'cause I know that I go dark quickly.
[01:04:38.880 --> 01:04:39.880]   (laughing)
[01:04:39.880 --> 01:04:42.600]   So I'm a quantitator futurist.
[01:04:42.600 --> 01:04:45.040]   My job is to look at data and to model out
[01:04:45.040 --> 01:04:46.520]   risk and opportunity scenarios.
[01:04:46.520 --> 01:04:48.800]   And at the moment, most of what I see is risk,
[01:04:48.800 --> 01:04:50.480]   but that being said--
[01:04:50.480 --> 01:04:51.720]   - That's probably normal, right?
[01:04:51.720 --> 01:04:52.880]   - Right?
[01:04:52.880 --> 01:04:54.040]   - It's probably normal.
[01:04:54.040 --> 01:04:55.720]   I am an optimist.
[01:04:55.720 --> 01:05:00.720]   So I see doom, but I believe that we have the opportunity
[01:05:00.720 --> 01:05:04.200]   to create change to make a better world.
[01:05:04.200 --> 01:05:05.320]   - If you were a futurist in--
[01:05:05.320 --> 01:05:06.160]   - I have hope.
[01:05:06.160 --> 01:05:09.960]   - In 1929 or 1939, you would have been pretty bleak too.
[01:05:09.960 --> 01:05:13.760]   1917. - I mean, yeah, that would have been,
[01:05:13.760 --> 01:05:14.920]   those were my people.
[01:05:14.920 --> 01:05:15.760]   - My peeps.
[01:05:15.760 --> 01:05:16.600]   (laughing)
[01:05:16.600 --> 01:05:20.520]   - Right before the 1939 Industrial War Complex
[01:05:20.520 --> 01:05:21.920]   researchers.
[01:05:21.920 --> 01:05:23.560]   So no, as a woman--
[01:05:23.560 --> 01:05:25.400]   - And they didn't wanna go 20 years in the future either,
[01:05:25.400 --> 01:05:29.120]   but had they known, it would have been Dwight D. Eisenhower
[01:05:29.120 --> 01:05:30.720]   and the birth of the middle class,
[01:05:30.720 --> 01:05:32.280]   they would have said, "Hey, cool!"
[01:05:32.280 --> 01:05:36.280]   - Yeah, I mean, listen, there are plenty of different ways
[01:05:36.280 --> 01:05:38.320]   to look at the present.
[01:05:38.320 --> 01:05:40.760]   And so not all is doom and gloom.
[01:05:40.760 --> 01:05:42.640]   I'm, what I'm mostly concerned about
[01:05:42.640 --> 01:05:44.800]   are the decisions that we are making.
[01:05:44.800 --> 01:05:48.000]   And I just feel like we're not always making decisions
[01:05:48.000 --> 01:05:50.000]   in the right way with the right amount of information.
[01:05:50.000 --> 01:05:53.000]   - Actually, I feel like we're not making decisions at all.
[01:05:53.000 --> 01:05:53.840]   - Sure.
[01:05:53.840 --> 01:05:55.120]   - That we're really deferring everything.
[01:05:55.120 --> 01:05:56.520]   We're kicking the can down the road.
[01:05:56.520 --> 01:05:58.360]   Nobody wants to be the one who said,
[01:05:58.360 --> 01:06:00.000]   "Hey, we gotta tighten the belt.
[01:06:00.000 --> 01:06:02.080]   Hey, we gotta cut emissions.
[01:06:02.080 --> 01:06:04.600]   Hey, you can't get a cover."
[01:06:04.600 --> 01:06:06.600]   - And I think that that is, to some extent,
[01:06:06.600 --> 01:06:08.480]   part of our American culture.
[01:06:08.480 --> 01:06:10.120]   We are nowists.
[01:06:10.120 --> 01:06:12.600]   We think very much about right now,
[01:06:12.600 --> 01:06:14.240]   in every part of our life,
[01:06:14.240 --> 01:06:18.560]   we are incentivized to work on short-term,
[01:06:18.560 --> 01:06:22.160]   even if that means long-term damage.
[01:06:22.160 --> 01:06:25.320]   And it's not like that in other places around the world.
[01:06:25.320 --> 01:06:27.240]   It's definitely like Scandinavian countries,
[01:06:27.240 --> 01:06:30.840]   parts of Europe, they think longer-term.
[01:06:30.840 --> 01:06:32.760]   - One of the interesting things about living in England,
[01:06:32.760 --> 01:06:33.800]   like I grew up in Australia,
[01:06:33.800 --> 01:06:36.520]   which is like America, relatively young country.
[01:06:36.520 --> 01:06:37.600]   But when you go to England,
[01:06:37.600 --> 01:06:39.880]   and you, like my house that I used to live in
[01:06:39.880 --> 01:06:41.280]   was built in the 1750s.
[01:06:41.280 --> 01:06:42.520]   - What?
[01:06:42.520 --> 01:06:43.360]   - Right.
[01:06:43.360 --> 01:06:44.200]   - You were living in every day.
[01:06:44.200 --> 01:06:46.760]   - We didn't even have a country in the 1750s.
[01:06:46.760 --> 01:06:47.600]   - Exactly.
[01:06:47.600 --> 01:06:48.440]   - We were rear rebels.
[01:06:48.440 --> 01:06:50.000]   - It is certainly, if you live in Europe
[01:06:50.000 --> 01:06:52.920]   or around Northern Africa, it's very hard
[01:06:52.920 --> 01:06:54.240]   to have a short-term perspective.
[01:06:54.240 --> 01:06:56.080]   And I do think that's one of the great culture clashes
[01:06:56.080 --> 01:06:57.720]   between the US and Europe.
[01:06:57.720 --> 01:07:00.360]   Europe's got this sense of a thousand years,
[01:07:00.360 --> 01:07:02.440]   and America's got this sense of,
[01:07:02.440 --> 01:07:04.240]   almost a hundred years of history,
[01:07:04.240 --> 01:07:05.160]   or something, you know?
[01:07:05.160 --> 01:07:08.200]   - Well, and then combine that with our fetishizing
[01:07:08.200 --> 01:07:10.160]   individualism versus collectivism.
[01:07:10.160 --> 01:07:11.440]   - Oh, don't start me on that.
[01:07:11.440 --> 01:07:15.400]   - That really is kind of a recipe for mindless hedonism,
[01:07:15.400 --> 01:07:17.160]   because it's all about the individual's rights,
[01:07:17.160 --> 01:07:18.880]   and the individual what the individual wants.
[01:07:18.880 --> 01:07:20.880]   - Yes, personal freedom versus society.
[01:07:20.880 --> 01:07:21.720]   - Personal freedom.
[01:07:21.720 --> 01:07:23.040]   - But we masquerade.
[01:07:23.040 --> 01:07:26.920]   We even, as we, I think, acknowledge that
[01:07:26.920 --> 01:07:30.520]   we feel very strongly that we have a good idea
[01:07:30.520 --> 01:07:33.680]   of what collectively those individuals should want.
[01:07:33.680 --> 01:07:37.920]   So I think we go to ourselves into thinking
[01:07:37.920 --> 01:07:42.480]   that we are planning for what's best for the good
[01:07:42.480 --> 01:07:45.560]   of everybody when we really are using ourselves
[01:07:45.560 --> 01:07:47.320]   as the focal point.
[01:07:47.320 --> 01:07:49.680]   - Leo, when you were in Japan, I'm off on a tangent.
[01:07:49.680 --> 01:07:50.520]   I'll stop.
[01:07:50.520 --> 01:07:51.360]   - No, I love it.
[01:07:51.360 --> 01:07:52.200]   Keep going.
[01:07:52.200 --> 01:07:53.040]   - But when you were in Japan,
[01:07:53.040 --> 01:07:55.840]   a lot of those temples that you went into,
[01:07:55.840 --> 01:07:59.040]   most of the temples in Japan have a wood reserve.
[01:07:59.040 --> 01:08:01.920]   So whatever wood makes up the ceilings,
[01:08:01.920 --> 01:08:04.680]   somewhere there is a reserve pile of wood
[01:08:04.680 --> 01:08:07.480]   that has been, you know, if it's bamboo,
[01:08:07.480 --> 01:08:09.480]   it was likely planted by somebody,
[01:08:09.480 --> 01:08:12.280]   some monk who works there centuries ago,
[01:08:12.280 --> 01:08:16.560]   harvested by somebody 70 to a hundred years ago,
[01:08:16.560 --> 01:08:19.280]   and then the labor of harvesting and prepping that wood
[01:08:19.280 --> 01:08:21.720]   and making sure that it would be, you know,
[01:08:21.720 --> 01:08:24.760]   sort of there at the ready when it was most needed,
[01:08:24.760 --> 01:08:28.240]   was probably done by somebody who died like 50 years ago.
[01:08:28.240 --> 01:08:31.920]   And to me, that illustrates a core difference
[01:08:31.920 --> 01:08:35.280]   between how, especially in Asia, there's a,
[01:08:35.280 --> 01:08:37.000]   there's serious long-term thinking,
[01:08:37.000 --> 01:08:39.880]   'cause that permeates throughout the government.
[01:08:39.880 --> 01:08:44.760]   It tech Sony, you know, like every major technology company
[01:08:44.760 --> 01:08:49.760]   in China, Korea, and Japan have very, very long-term plans
[01:08:49.760 --> 01:08:54.840]   that they allow it to be sort of tweaked as time moves on,
[01:08:54.840 --> 01:08:58.680]   but, you know, it's a radically different way of thinking
[01:08:58.680 --> 01:09:00.960]   that we don't have in the United States,
[01:09:00.960 --> 01:09:03.120]   and the thing that concerns me is that we're the ones
[01:09:03.120 --> 01:09:04.840]   at the moment building the future.
[01:09:04.840 --> 01:09:07.720]   But we are doing so, thinking squarely about the present.
[01:09:07.720 --> 01:09:10.120]   - Instead of having Louis the 13th cognac
[01:09:10.120 --> 01:09:12.480]   that takes seven generations of wine makers
[01:09:12.480 --> 01:09:14.520]   to make over a hundred years,
[01:09:14.520 --> 01:09:16.840]   we've got beer that's stale in three weeks.
[01:09:16.840 --> 01:09:19.920]   - Yeah, but it's America, God bless it.
[01:09:19.920 --> 01:09:22.080]   - Yeah, I mean, the beer that I drink in the UK,
[01:09:22.080 --> 01:09:24.880]   for example, takes usually four to six weeks to make,
[01:09:24.880 --> 01:09:26.360]   but the lager that I hear takes three days.
[01:09:26.360 --> 01:09:27.200]   - What do you drink?
[01:09:27.200 --> 01:09:28.040]   - You gotta tell us what you do.
[01:09:28.040 --> 01:09:29.200]   - Real ale. - But you're...
[01:09:29.200 --> 01:09:31.360]   - I have a local bar near me that is real ale.
[01:09:31.360 --> 01:09:33.200]   It's called the Sanford Inn.
[01:09:33.200 --> 01:09:35.560]   The front bump, they have front pumps
[01:09:35.560 --> 01:09:38.680]   and of the front beer pumps, there's only eight of them.
[01:09:38.680 --> 01:09:40.360]   Three of them are always the same
[01:09:40.360 --> 01:09:41.720]   and the other five are always different.
[01:09:41.720 --> 01:09:42.760]   So every time I go into the pub,
[01:09:42.760 --> 01:09:46.480]   I get a different real beer, like real ale, not beer,
[01:09:46.480 --> 01:09:48.680]   but beer, not the stuff you get over here which is great.
[01:09:48.680 --> 01:09:50.000]   - Yeah, I had your real ale
[01:09:50.000 --> 01:09:52.040]   and I had your real bangers in mash too.
[01:09:52.040 --> 01:09:53.680]   I don't know. (laughs)
[01:09:53.680 --> 01:09:56.600]   - If you come up to, you got a little damn.
[01:09:56.600 --> 01:09:58.440]   - We got lager kneaders over here.
[01:09:58.440 --> 01:10:01.440]   We got real beer. - That stuff's manufactured
[01:10:01.440 --> 01:10:03.680]   to the point of like... - Oh, yeah, yeah, yeah.
[01:10:03.680 --> 01:10:05.640]   That's, that's, that's, yeah, yeah.
[01:10:05.640 --> 01:10:08.080]   - East, mold and hops, it's all the same.
[01:10:08.080 --> 01:10:10.040]   - I want something that's to make one person
[01:10:10.040 --> 01:10:11.440]   in a building. - What person?
[01:10:11.440 --> 01:10:13.360]   - Yeah. - I was just in Ireland
[01:10:13.360 --> 01:10:16.640]   and went to the old Bushmills whiskey distillery
[01:10:16.640 --> 01:10:18.200]   'cause I'm a whiskey drinker.
[01:10:18.200 --> 01:10:20.960]   And I had some good whiskey there,
[01:10:20.960 --> 01:10:23.160]   but they were very, very insistent on like,
[01:10:23.160 --> 01:10:25.320]   this is, this is the big difference between
[01:10:25.320 --> 01:10:27.800]   an Irish whiskey and a whiskey made somewhere else
[01:10:27.800 --> 01:10:31.240]   and it's, you know, very few ingredients and...
[01:10:31.240 --> 01:10:32.520]   - Yeah, all that sort of stuff.
[01:10:32.520 --> 01:10:34.240]   Now I wanna come back to one thing on the T2.
[01:10:34.240 --> 01:10:35.560]   We're talking about the T2 chip.
[01:10:35.560 --> 01:10:36.560]   - Hold on, hold that thought.
[01:10:36.560 --> 01:10:37.400]   - All right.
[01:10:37.400 --> 01:10:38.400]   - We will come back to that
[01:10:38.400 --> 01:10:41.160]   and much more including a $5 billion fine
[01:10:41.160 --> 01:10:43.040]   for Google in the European Union.
[01:10:43.040 --> 01:10:47.640]   But first, a word from our sponsor, Rocket Mortgage.
[01:10:47.640 --> 01:10:49.120]   I love Rocket Mortgage.
[01:10:49.120 --> 01:10:51.720]   They are, they're geeks.
[01:10:51.720 --> 01:10:53.440]   They're geeks, dang, Gilbert and his crew.
[01:10:53.440 --> 01:10:54.320]   They're quick and loans.
[01:10:54.320 --> 01:10:55.480]   They love technology.
[01:10:55.480 --> 01:10:58.480]   They are very consumer focused in that kind of silica.
[01:10:58.480 --> 01:10:59.720]   They're in Detroit, but they're kind of that
[01:10:59.720 --> 01:11:02.440]   in Silicon Valley way of really thinking about
[01:11:02.440 --> 01:11:05.160]   what, how can we serve our customers?
[01:11:05.160 --> 01:11:07.240]   And that's where they came up with Rocket Mortgage.
[01:11:07.240 --> 01:11:09.840]   They realized that the mortgage approval process
[01:11:09.840 --> 01:11:11.960]   was kind of stuck in the 19th century.
[01:11:11.960 --> 01:11:13.360]   You know, you had to go to a bank,
[01:11:13.360 --> 01:11:15.840]   you had to get paperwork and application.
[01:11:15.840 --> 01:11:17.240]   They made it all online.
[01:11:17.240 --> 01:11:19.560]   That was huge.
[01:11:19.560 --> 01:11:20.760]   They're always paying attention though.
[01:11:20.760 --> 01:11:21.880]   And one of the things they've noticed
[01:11:21.880 --> 01:11:23.840]   is as interest rates are going up,
[01:11:23.840 --> 01:11:26.440]   we haven't seen interest rates go up in quite some time.
[01:11:26.440 --> 01:11:28.880]   They're going up now and they realized,
[01:11:28.880 --> 01:11:31.880]   oh, you know, there's some anxiety
[01:11:31.880 --> 01:11:33.200]   to buying a home these days
[01:11:33.200 --> 01:11:35.560]   because as you're looking for a house,
[01:11:35.560 --> 01:11:36.640]   there's pressure.
[01:11:36.640 --> 01:11:39.200]   This is the most important decision you'll ever make.
[01:11:39.200 --> 01:11:40.400]   The biggest check you'll ever write.
[01:11:40.400 --> 01:11:42.280]   And there's this pressure to get done now
[01:11:42.280 --> 01:11:43.240]   because interest rates are going up
[01:11:43.240 --> 01:11:44.920]   and it's going to cost more next month.
[01:11:44.920 --> 01:11:46.720]   So they've come up with a great solution.
[01:11:46.720 --> 01:11:49.960]   They call it the power buying process.
[01:11:49.960 --> 01:11:50.800]   Here's how it works.
[01:11:50.800 --> 01:11:54.480]   You go to rocketmortgage.com/twit2.
[01:11:54.480 --> 01:11:56.240]   As always with Rocket Mortgage,
[01:11:56.240 --> 01:11:58.440]   they'll verify your income and assets and credit.
[01:11:58.440 --> 01:11:59.840]   You don't have to get paperwork.
[01:11:59.840 --> 01:12:01.040]   You don't fill out an application.
[01:12:01.040 --> 01:12:02.680]   You just do it all online.
[01:12:02.680 --> 01:12:04.040]   They have trusted relationships
[01:12:04.040 --> 01:12:05.440]   with all the financial institutions.
[01:12:05.440 --> 01:12:06.440]   As soon as you give them permission,
[01:12:06.440 --> 01:12:08.320]   they get the information they need.
[01:12:08.320 --> 01:12:09.760]   It happens fast.
[01:12:09.760 --> 01:12:12.600]   No more than 24 hours to get you verified approval.
[01:12:12.600 --> 01:12:13.720]   Now that's a big deal.
[01:12:13.720 --> 01:12:16.320]   The verified approval means
[01:12:16.320 --> 01:12:20.120]   you now are basically equivalent to a cash buyer.
[01:12:20.120 --> 01:12:22.200]   So when a seller's selling the house
[01:12:22.200 --> 01:12:24.520]   and they're talking to somebody with verified approval
[01:12:24.520 --> 01:12:25.360]   versus somebody that says,
[01:12:25.360 --> 01:12:26.440]   yeah, we're going to get the loan.
[01:12:26.440 --> 01:12:27.640]   We want this house.
[01:12:27.640 --> 01:12:30.520]   You got the leg up by far.
[01:12:30.520 --> 01:12:31.400]   They know you're good for it.
[01:12:31.400 --> 01:12:33.600]   You've already got the loan basically.
[01:12:33.600 --> 01:12:34.840]   Now, once you're verified,
[01:12:34.840 --> 01:12:37.040]   you qualify for the all new exclusive
[01:12:37.040 --> 01:12:38.120]   rate shield approval.
[01:12:38.120 --> 01:12:41.200]   They will lock your rate up for up to three months.
[01:12:41.200 --> 01:12:43.440]   So no more anxiety about rates going up.
[01:12:43.440 --> 01:12:45.680]   Whatever rate you get when you go there, right?
[01:12:45.680 --> 01:12:46.840]   And go there now, right?
[01:12:46.840 --> 01:12:48.760]   Rocketmortgage.com/twit2.
[01:12:48.760 --> 01:12:51.560]   That's your rate for up to 90 days.
[01:12:51.560 --> 01:12:53.680]   With one exception, if rates go down, your rate goes down.
[01:12:53.680 --> 01:12:55.080]   It just won't go up.
[01:12:55.080 --> 01:12:56.560]   So you win either way.
[01:12:56.560 --> 01:12:57.840]   It's exactly what you'd expect
[01:12:57.840 --> 01:12:59.320]   from America's Best Mortgage Linder.
[01:12:59.320 --> 01:13:02.960]   Rocketmortgage.com/twit2.
[01:13:02.960 --> 01:13:04.480]   And now I'll give you the legal stuff.
[01:13:04.480 --> 01:13:06.800]   Rate shield approval is only valid on 30 years.
[01:13:06.800 --> 01:13:09.000]   Certain 30-year purchase transactions,
[01:13:09.000 --> 01:13:11.440]   additional conditions or exclusions may apply based
[01:13:11.440 --> 01:13:12.840]   on quick and loans data in comparison
[01:13:12.840 --> 01:13:14.240]   to public data records.
[01:13:14.240 --> 01:13:16.080]   Equal housing lender, that one's important.
[01:13:16.080 --> 01:13:18.160]   License in all 50 states, you bet.
[01:13:18.160 --> 01:13:21.880]   And MLSconsumeraccess.org number 30, 30.
[01:13:21.880 --> 01:13:22.880]   Here's what you need to remember.
[01:13:22.880 --> 01:13:25.920]   Rocketmortgage.com/twit2.
[01:13:25.920 --> 01:13:30.000]   It's the best way to get your new home or refi to.
[01:13:30.000 --> 01:13:31.840]   Rocketmortgage.com/twit2.
[01:13:31.840 --> 01:13:33.720]   We thank them so much for their support.
[01:13:33.720 --> 01:13:36.120]   They've told us they're coming back next year.
[01:13:36.120 --> 01:13:37.360]   Thank you, Rocketmortgage.
[01:13:37.360 --> 01:13:38.200]   And you know what?
[01:13:38.200 --> 01:13:39.680]   Thank you for supporting Rocketmortgage.
[01:13:39.680 --> 01:13:42.280]   I've talked to a lot of you who've used Rocketmortgage now.
[01:13:42.280 --> 01:13:43.400]   And we appreciate that.
[01:13:43.400 --> 01:13:45.040]   That helps twiddle on.
[01:13:45.040 --> 01:13:47.680]   We're talking about the week's tech news.
[01:13:47.680 --> 01:13:50.240]   And we've got two of my favorite people to do it.
[01:13:50.240 --> 01:13:52.440]   And I think by now, if you're still listening,
[01:13:52.440 --> 01:13:53.640]   if you're not, you probably disagree.
[01:13:53.640 --> 01:13:54.920]   But if you're still listening,
[01:13:54.920 --> 01:13:58.520]   you agree that Greg Farrow from PacketPushersNetwork
[01:13:58.520 --> 01:14:01.000]   and Amy Webb from the Signals are Talking.
[01:14:01.000 --> 01:14:04.760]   AmyWeb.io are really, I just,
[01:14:04.760 --> 01:14:06.480]   I always feel smarter after these shows.
[01:14:06.480 --> 01:14:07.320]   So thank you.
[01:14:07.320 --> 01:14:09.680]   I really, it's great to talk to you both.
[01:14:09.680 --> 01:14:11.360]   What did you want to say about the T2?
[01:14:11.360 --> 01:14:13.240]   So T2 is kind of like a--
[01:14:13.240 --> 01:14:16.480]   This is the little arm chip that Apple started
[01:14:16.480 --> 01:14:19.000]   by putting in their iPhone, right?
[01:14:19.000 --> 01:14:20.080]   Yeah. The T1.
[01:14:20.080 --> 01:14:20.920]   Yep.
[01:14:20.920 --> 01:14:22.840]   And they put it in the iMac Pro.
[01:14:22.840 --> 01:14:24.320]   FaceTime and a few other differences.
[01:14:24.320 --> 01:14:25.720]   Fingerprint.
[01:14:25.720 --> 01:14:28.040]   And now they put it in these new MacBook Pros.
[01:14:28.040 --> 01:14:28.840]   It's a T2.
[01:14:28.840 --> 01:14:29.360]   T2.
[01:14:29.360 --> 01:14:32.440]   So Intel this week acquired a company called EASIC.
[01:14:32.440 --> 01:14:36.640]   So normally when you build processors, CPUs or ASICs,
[01:14:36.640 --> 01:14:38.840]   there's two types of ways of doing one is an ASIC
[01:14:38.840 --> 01:14:40.280]   where you fully design everything
[01:14:40.280 --> 01:14:41.720]   and then you hard wire everything
[01:14:41.720 --> 01:14:43.360]   and then you send it off to the factory.
[01:14:43.360 --> 01:14:45.880]   These are widely used for Bitcoin miners, for instance,
[01:14:45.880 --> 01:14:47.880]   'cause you can make a very dedicated, very specific--
[01:14:47.880 --> 01:14:48.280]   That's right.
[01:14:48.280 --> 01:14:50.040]   So a GPU is an instance of an ASIC.
[01:14:50.040 --> 01:14:50.440]   OK.
[01:14:50.440 --> 01:14:51.520]   But ASICs are more general.
[01:14:51.520 --> 01:14:54.120]   So normally you have ASICs to do your NVMe controller
[01:14:54.120 --> 01:14:55.640]   or your memory controller.
[01:14:55.640 --> 01:14:57.040]   They're very specific.
[01:14:57.040 --> 01:14:58.040]   In fact, that's what it says
[01:14:58.040 --> 01:14:58.880]   where application is specific.
[01:14:58.880 --> 01:15:00.120]   Specific integrated circuit.
[01:15:00.120 --> 01:15:00.640]   Right.
[01:15:00.640 --> 01:15:02.040]   And at the other end, there's another type
[01:15:02.040 --> 01:15:04.320]   of chip called an FPGA floating point gate array,
[01:15:04.320 --> 01:15:09.880]   which is a sort of an ASIC that has these programmable junctions
[01:15:09.880 --> 01:15:11.520]   which you can break or make.
[01:15:11.520 --> 01:15:11.880]   Right.
[01:15:11.880 --> 01:15:12.960]   You can add them up.
[01:15:12.960 --> 01:15:14.680]   And you can actually load firmware into them
[01:15:14.680 --> 01:15:16.560]   to run general purpose applications.
[01:15:16.560 --> 01:15:19.200]   We saw about a year ago, there was a flash in a pan
[01:15:19.200 --> 01:15:21.640]   about putting FPGAs onto the servers
[01:15:21.640 --> 01:15:24.360]   so that the cloud companies could program
[01:15:24.360 --> 01:15:26.960]   in specific functions into the CPU.
[01:15:26.960 --> 01:15:29.320]   That doesn't seem to have picked up a lot of momentum.
[01:15:29.320 --> 01:15:31.520]   But these EASICs is somewhere in the middle.
[01:15:31.520 --> 01:15:32.080]   That is there.
[01:15:32.080 --> 01:15:35.920]   Actually, ASICs that can be have software loaded into them
[01:15:35.920 --> 01:15:37.880]   so that they can turn them into different things.
[01:15:37.880 --> 01:15:38.480]   Oh, that's interesting.
[01:15:38.480 --> 01:15:39.560]   This is one ASIC.
[01:15:39.560 --> 01:15:41.320]   It's called a structured ASIC.
[01:15:41.320 --> 01:15:44.560]   So an ASIC takes about 18 months to bring to market.
[01:15:44.560 --> 01:15:46.800]   From the time you start designing until you can go through
[01:15:46.800 --> 01:15:49.160]   the design phase, about 18 months.
[01:15:49.160 --> 01:15:51.400]   An FPGA is you can take an FPGA off the shelf
[01:15:51.400 --> 01:15:52.720]   and just start loading modules into it.
[01:15:52.720 --> 01:15:53.880]   You've got yourself in.
[01:15:53.880 --> 01:15:56.480]   And it's very complicated to make an FPGA program, I believe.
[01:15:56.480 --> 01:15:57.960]   So whatever.
[01:15:57.960 --> 01:16:01.080]   And these EASICs give you sort of like ASIC capabilities,
[01:16:01.080 --> 01:16:03.240]   but in a three to six month timeline.
[01:16:03.240 --> 01:16:04.560]   And this is where the T2 is.
[01:16:04.560 --> 01:16:07.120]   It's kind of a chip that allows you to be reprogrammed.
[01:16:07.120 --> 01:16:09.800]   You load an app into it and it runs a touch bar.
[01:16:09.800 --> 01:16:12.320]   Or you load an app into it to an MvUmE,
[01:16:12.320 --> 01:16:16.280]   or a crypto processor, or a low-power drive controller,
[01:16:16.280 --> 01:16:17.680]   or a RAM controller.
[01:16:17.680 --> 01:16:20.080]   So you can get these very specific,
[01:16:20.080 --> 01:16:24.040]   a highly optimized customized tasks.
[01:16:24.040 --> 01:16:25.400]   But from a single chip.
[01:16:25.400 --> 01:16:26.000]   From one chip.
[01:16:26.000 --> 01:16:27.440]   Without doing a whole lot of really good.
[01:16:27.440 --> 01:16:30.560]   So the T2 is running new software all the time
[01:16:30.560 --> 01:16:32.280]   or is it pre-programmed from the factory?
[01:16:32.280 --> 01:16:33.280]   It's a little hard when we don't know.
[01:16:33.280 --> 01:16:36.160]   I think the T2 is much more of a programmable,
[01:16:36.160 --> 01:16:37.520]   like this structured ASIC.
[01:16:37.520 --> 01:16:39.560]   I'll put a link into the chat room.
[01:16:39.560 --> 01:16:40.720]   And I'll send it.
[01:16:40.720 --> 01:16:42.800]   It'll be in the show notes if you want to read up more about it.
[01:16:42.800 --> 01:16:44.640]   But I think this is Intel realizing
[01:16:44.640 --> 01:16:45.760]   that this is the new future.
[01:16:45.760 --> 01:16:47.440]   This dedicated ASIC.
[01:16:47.440 --> 01:16:50.040]   But people don't want to build custom ASICs for everything
[01:16:50.040 --> 01:16:51.880]   and go through a two year cycle.
[01:16:51.880 --> 01:16:53.320]   There has to be a middle-ground--
[01:16:53.320 --> 01:16:55.560]   Does it replace the CPU or do we still
[01:16:55.560 --> 01:16:57.360]   have to have this general purpose brain?
[01:16:57.360 --> 01:16:58.760]   Well, and I wanted to notice that.
[01:16:58.760 --> 01:17:01.960]   Mean it's complementary or competitive with edge computing
[01:17:01.960 --> 01:17:02.440]   then.
[01:17:02.440 --> 01:17:04.320]   I see it as complementary in the sense
[01:17:04.320 --> 01:17:06.800]   that these processors will run alongside CPUs.
[01:17:06.800 --> 01:17:09.360]   But you'll be able to use them for dedicated tasks.
[01:17:09.360 --> 01:17:10.560]   So you'll still have the CPUs on that.
[01:17:10.560 --> 01:17:11.960]   The T2 is exactly that.
[01:17:11.960 --> 01:17:15.080]   The T2 does everything from the secure enclave to fingerprint
[01:17:15.080 --> 01:17:16.840]   to it's got a DSP for FaceTime.
[01:17:16.840 --> 01:17:17.320]   Yep, that's right.
[01:17:17.320 --> 01:17:20.960]   But you still have it until i7 or i5 or i9.
[01:17:20.960 --> 01:17:21.840]   That's doing the general purpose--
[01:17:21.840 --> 01:17:23.200]   That's doing the general purpose stuff.
[01:17:23.200 --> 01:17:24.480]   And I think we're seeing--
[01:17:24.480 --> 01:17:25.680]   we're already seeing this ability.
[01:17:25.680 --> 01:17:28.520]   The CPUs reached its maximum capability.
[01:17:28.520 --> 01:17:29.880]   We can't clock them faster.
[01:17:29.880 --> 01:17:31.320]   We can't make them bigger.
[01:17:31.320 --> 01:17:33.440]   We're struggling to make them smaller.
[01:17:33.440 --> 01:17:36.320]   So the way that we do things is we offload functions to GPUs.
[01:17:36.320 --> 01:17:37.560]   We have network adapters.
[01:17:37.560 --> 01:17:38.720]   We have memory--
[01:17:38.720 --> 01:17:42.600]   Is that what you mean by edge computing, Amy?
[01:17:42.600 --> 01:17:48.720]   Edge-- so the idea is that the source of the data
[01:17:48.720 --> 01:17:53.480]   and the application that the data uses are closer together.
[01:17:53.480 --> 01:17:58.200]   And it's a divergence from the John von Neumann architecture.
[01:17:58.200 --> 01:18:01.520]   We may be talking about things in parallel.
[01:18:01.520 --> 01:18:03.800]   Edge is a way to bring processing to the edge.
[01:18:03.800 --> 01:18:05.240]   So the idea is that the network only has
[01:18:05.240 --> 01:18:05.760]   finite--
[01:18:05.760 --> 01:18:07.960]   Smart devices as opposed to a centralized--
[01:18:07.960 --> 01:18:08.440]   That's right.
[01:18:08.440 --> 01:18:09.400]   So you know, you know, you--
[01:18:09.400 --> 01:18:10.400]   This has been the evolution of computing.
[01:18:10.400 --> 01:18:11.440]   It produces latency.
[01:18:11.440 --> 01:18:12.600]   It's been the evolution of computing.
[01:18:12.600 --> 01:18:13.600]   Right, I mean, it's--
[01:18:13.600 --> 01:18:15.400]   If it was computing from the 50s, where you had a mainframe
[01:18:15.400 --> 01:18:17.520]   and terminals, then we had client server computing.
[01:18:17.520 --> 01:18:19.440]   That's the document architecture, right.
[01:18:19.440 --> 01:18:20.880]   And that hasn't changed at all.
[01:18:20.880 --> 01:18:23.200]   So this is sort of the next architectural approach.
[01:18:23.200 --> 01:18:24.560]   Edge computing is much more likely
[01:18:24.560 --> 01:18:28.240]   to be aligned very closely with a central function.
[01:18:28.240 --> 01:18:31.120]   So the central function will have very tight control over it.
[01:18:31.120 --> 01:18:32.920]   And it will have apps and software--
[01:18:32.920 --> 01:18:35.200]   You still have a master control unit.
[01:18:35.200 --> 01:18:36.880]   Yeah.
[01:18:36.880 --> 01:18:39.160]   Without the MCU.
[01:18:39.160 --> 01:18:40.560]   We used to call the master slavery.
[01:18:40.560 --> 01:18:42.280]   We're not allowed to say that so much these days.
[01:18:42.280 --> 01:18:45.640]   But there has to be a master and uncontrolled edge.
[01:18:45.640 --> 01:18:47.720]   So instead of the edge being what it is today,
[01:18:47.720 --> 01:18:49.840]   your laptop, which is you can put anything on it
[01:18:49.840 --> 01:18:52.080]   and you have control of it, the edge in the future
[01:18:52.080 --> 01:18:53.360]   will be much more like this.
[01:18:53.360 --> 01:18:57.520]   Much restricted and administered from a central control point.
[01:18:57.520 --> 01:18:59.000]   And you'll also see it.
[01:18:59.000 --> 01:19:00.960]   You know the little green boxes on the side of the road
[01:19:00.960 --> 01:19:02.120]   as you walk around?
[01:19:02.120 --> 01:19:03.680]   You'll be seeing those--
[01:19:03.680 --> 01:19:05.440]   Instead of a mobile phone tower having
[01:19:05.440 --> 01:19:07.200]   a bunch of analog equipment driving it,
[01:19:07.200 --> 01:19:09.400]   it'll just be a bunch of x86 servers.
[01:19:09.400 --> 01:19:10.920]   And the apps will be deployed from somewhere
[01:19:10.920 --> 01:19:12.040]   in the central of the cloud.
[01:19:12.040 --> 01:19:13.520]   That's how I see the edge.
[01:19:13.520 --> 01:19:16.480]   Or cars will be little mini data centers.
[01:19:16.480 --> 01:19:17.160]   That will be your edge.
[01:19:17.160 --> 01:19:18.680]   Edge isn't going to do you any good.
[01:19:18.680 --> 01:19:20.400]   I'm afraid to stop.
[01:19:20.400 --> 01:19:21.400]   Please.
[01:19:21.400 --> 01:19:23.320]   You realize they cannot allow this?
[01:19:23.320 --> 01:19:25.960]   I like to go against a cutie with your maid of.
[01:19:25.960 --> 01:19:28.400]   I'm warning you, you're entering a big Arab.
[01:19:28.400 --> 01:19:31.360]   I'm going to have to put you in the game, Bred.
[01:19:31.360 --> 01:19:32.400]   That's the voice.
[01:19:32.400 --> 01:19:33.080]   That's the voice.
[01:19:33.080 --> 01:19:34.200]   Master control.
[01:19:34.200 --> 01:19:37.640]   You're going to be troubled by the fear I ran out of you.
[01:19:37.640 --> 01:19:38.080]   I'm sorry.
[01:19:38.080 --> 01:19:39.280]   I live in science fiction.
[01:19:39.280 --> 01:19:40.280]   I'm not the one--
[01:19:40.280 --> 01:19:41.680]   Tron's one of my favorite movies ever.
[01:19:41.680 --> 01:19:42.680]   Is it?
[01:19:42.680 --> 01:19:43.200]   Oh, yeah.
[01:19:43.200 --> 01:19:44.720]   You have such poor taste.
[01:19:44.720 --> 01:19:45.220]   What?
[01:19:45.220 --> 01:19:45.720]   No.
[01:19:45.720 --> 01:19:48.680]   Why is it one of your favorite movies?
[01:19:48.680 --> 01:19:49.920]   Jeff Bridges' Tron.
[01:19:49.920 --> 01:19:51.440]   The young Jeff Bridges' Tron.
[01:19:51.440 --> 01:19:52.120]   Not the old Jeff Bridges' Tron.
[01:19:52.120 --> 01:19:53.120]   Yeah, original Tron.
[01:19:53.120 --> 01:19:54.640]   Not whatever happened.
[01:19:54.640 --> 01:19:55.640]   Whatever that was.
[01:19:55.640 --> 01:19:56.640]   Yeah.
[01:19:56.640 --> 01:19:58.640]   The one with the Frisbee.
[01:19:58.640 --> 01:20:00.640]   [LAUGHTER]
[01:20:00.640 --> 01:20:05.360]   Original, like very cool retro colorful, interesting,
[01:20:05.360 --> 01:20:06.360]   inside the machine.
[01:20:06.360 --> 01:20:08.800]   Did you get chills when I played the voice of Master
[01:20:08.800 --> 01:20:09.320]   control?
[01:20:09.320 --> 01:20:11.120]   Did that bring back your childhood?
[01:20:11.120 --> 01:20:11.640]   It did.
[01:20:11.640 --> 01:20:12.160]   Yeah.
[01:20:12.160 --> 01:20:13.160]   The cuddle looks up.
[01:20:13.160 --> 01:20:14.720]   This is the voice of Master control.
[01:20:14.720 --> 01:20:19.440]   I think we'll find some--
[01:20:19.440 --> 01:20:20.560]   it's funny how--
[01:20:20.560 --> 01:20:22.440]   actually, one of the things I love about Twit
[01:20:22.440 --> 01:20:26.080]   and about what we do and what I do is that it ranges everywhere
[01:20:26.080 --> 01:20:31.440]   from the functions of A6 and FPGAs to the politics
[01:20:31.440 --> 01:20:35.200]   of the European Union and everywhere in between.
[01:20:35.200 --> 01:20:38.320]   And that's really how technology has permeated our world.
[01:20:38.320 --> 01:20:43.520]   The EU has decided that Google has violated antitrust
[01:20:43.520 --> 01:20:48.320]   regulations because of how it handles Android.
[01:20:48.320 --> 01:20:54.160]   And as find Google, $5 billion, well, it's 4.2 million euros.
[01:20:54.160 --> 01:20:55.840]   So whatever that is, $1.
[01:20:55.840 --> 01:21:01.080]   And says, you have 90 days to knock it off.
[01:21:01.080 --> 01:21:05.840]   Now, I bet you both have strong opinions about this.
[01:21:05.840 --> 01:21:08.120]   I wouldn't say strong opinions.
[01:21:08.120 --> 01:21:09.120]   Opinion at all?
[01:21:09.120 --> 01:21:10.120]   Yeah, a little bit.
[01:21:10.120 --> 01:21:13.680]   I think you could say lots and lots of things about this
[01:21:13.680 --> 01:21:16.880]   that I think the first of all is the gap between US
[01:21:16.880 --> 01:21:19.760]   and any competitive or US antitrust and the vision
[01:21:19.760 --> 01:21:22.800]   that Europeans have of antitrust.
[01:21:22.800 --> 01:21:24.560]   So tell me how we diverge.
[01:21:24.560 --> 01:21:28.280]   So the way that I understand US antitrust, obviously,
[01:21:28.280 --> 01:21:30.840]   is that it's focused on whether the consumer is affected
[01:21:30.840 --> 01:21:32.000]   financially.
[01:21:32.000 --> 01:21:34.000]   It's to protect consumers, basically.
[01:21:34.000 --> 01:21:36.280]   Both antitrust laws are to protect consumers,
[01:21:36.280 --> 01:21:42.440]   but the US is purely focused on whether the consumer has been
[01:21:42.440 --> 01:21:43.520]   suffered financially.
[01:21:43.520 --> 01:21:44.720]   So did it cost you more?
[01:21:44.720 --> 01:21:47.000]   Did they get together and do something to charge you more
[01:21:47.000 --> 01:21:48.920]   for a product that could have been cheaper?
[01:21:48.920 --> 01:21:51.160]   And in this case, Android is free.
[01:21:51.160 --> 01:21:53.240]   So there will never be an antitrust against Google
[01:21:53.240 --> 01:21:55.480]   around this because the antitrust law is clear.
[01:21:55.480 --> 01:21:57.600]   There has to be a monetary problem here.
[01:21:57.600 --> 01:22:00.440]   The consumer has to be protected, but only financially,
[01:22:00.440 --> 01:22:02.120]   not emotionally or morally.
[01:22:02.120 --> 01:22:03.400]   How is it different in the EU?
[01:22:03.400 --> 01:22:05.680]   In the EU, they take a much more systemic view of the place.
[01:22:05.680 --> 01:22:07.880]   And they say, if you're controlling the market
[01:22:07.880 --> 01:22:11.760]   in any form, monetarily, societally, or dominating
[01:22:11.760 --> 01:22:13.440]   the market by making--
[01:22:13.440 --> 01:22:16.080]   So even if Google gives away Android for free,
[01:22:16.080 --> 01:22:18.720]   you could still be abusing the market.
[01:22:18.720 --> 01:22:19.320]   So I've heard you--
[01:22:19.320 --> 01:22:20.720]   Even if the consumer gets the benefit of having
[01:22:20.720 --> 01:22:22.360]   your product for free.
[01:22:22.360 --> 01:22:25.160]   You used to be in the EU now you're in Britain, but--
[01:22:25.160 --> 01:22:27.440]   We haven't left yet.
[01:22:27.440 --> 01:22:29.640]   I've heard it a little differently,
[01:22:29.640 --> 01:22:31.160]   but maybe this is the same thing.
[01:22:31.160 --> 01:22:33.160]   In the US antitrust laws is designed
[01:22:33.160 --> 01:22:34.200]   to protect consumers.
[01:22:34.200 --> 01:22:35.680]   In the EU antitrust laws are designed
[01:22:35.680 --> 01:22:38.640]   to protect competition amongst companies.
[01:22:38.640 --> 01:22:39.240]   Yes.
[01:22:39.240 --> 01:22:41.640]   To keep it a competitive environment for companies.
[01:22:41.640 --> 01:22:44.680]   And that's a moral dimension or a political dimension
[01:22:44.680 --> 01:22:47.920]   in that any company should be able to enter the market.
[01:22:47.920 --> 01:22:49.000]   So this is where Google and Facebook--
[01:22:49.000 --> 01:22:50.760]   And ultimately consumers benefit from that.
[01:22:50.760 --> 01:22:53.400]   And this is where Google and Facebook can be so dominant.
[01:22:53.400 --> 01:22:56.480]   It's because there's no way anybody can enter the market,
[01:22:56.480 --> 01:22:58.680]   because they're giving it away for free.
[01:22:58.680 --> 01:22:59.200]   So--
[01:22:59.200 --> 01:23:01.160]   Yeah, except this-- well, the reason it bothers me--
[01:23:01.160 --> 01:23:05.440]   so there's three different areas that the EU is upset about.
[01:23:05.440 --> 01:23:08.920]   One is that Google bundles Chrome and they play store
[01:23:08.920 --> 01:23:10.400]   into Android.
[01:23:10.400 --> 01:23:14.200]   The two is that they pressure companies that are selling
[01:23:14.200 --> 01:23:16.720]   Android devices to pick either the free Android, which
[01:23:16.720 --> 01:23:21.200]   has no Google services, or the Google services Android.
[01:23:21.200 --> 01:23:22.720]   You can't do both.
[01:23:22.720 --> 01:23:24.840]   And the third problem, which is somewhat gone away,
[01:23:24.840 --> 01:23:27.480]   because Google stopped doing this practice in 2014.
[01:23:27.480 --> 01:23:30.040]   They pay some companies more than other companies.
[01:23:30.040 --> 01:23:31.880]   They give companies sweetheart deals for Android,
[01:23:31.880 --> 01:23:33.880]   things like that.
[01:23:33.880 --> 01:23:38.520]   It strikes me that if you want to talk anti-competitive,
[01:23:38.520 --> 01:23:39.360]   look at the iPhone.
[01:23:39.360 --> 01:23:41.080]   There's no free version of the iPhone.
[01:23:41.080 --> 01:23:43.040]   They bundle whatever the hell they want into it.
[01:23:43.040 --> 01:23:45.600]   You have to use the Apple Store.
[01:23:45.600 --> 01:23:48.360]   If you're a developer, you have to give Apple 30%
[01:23:48.360 --> 01:23:50.760]   of your revenue, period.
[01:23:50.760 --> 01:23:54.840]   And there's no other companies making Apple phones.
[01:23:54.840 --> 01:23:57.000]   Only Apple can make an Apple phone.
[01:23:57.000 --> 01:23:59.240]   On the other hand, we have Android,
[01:23:59.240 --> 01:24:00.600]   which is free of open source.
[01:24:00.600 --> 01:24:03.520]   Anybody, including Amazon, a deadly competitor to Google,
[01:24:03.520 --> 01:24:06.240]   makes an Android device by forking Android.
[01:24:06.240 --> 01:24:08.520]   And they pay Google nothing to do it.
[01:24:08.520 --> 01:24:13.000]   Google, I think, seems to me, is being really--
[01:24:13.000 --> 01:24:13.880]   I like it there.
[01:24:13.880 --> 01:24:14.720]   It's open source.
[01:24:14.720 --> 01:24:16.680]   They've given away an operating system.
[01:24:16.680 --> 01:24:20.080]   There are more than 1,000 Android phone manufacturers.
[01:24:20.080 --> 01:24:21.960]   They get to choose what they want to do.
[01:24:21.960 --> 01:24:23.160]   Do they want the Google services?
[01:24:23.160 --> 01:24:24.440]   If they choose the Google services,
[01:24:24.440 --> 01:24:25.920]   it's because their customers say,
[01:24:25.920 --> 01:24:27.840]   I don't want an Android phone that doesn't have Google Play
[01:24:27.840 --> 01:24:29.080]   Store and Chrome on it.
[01:24:29.080 --> 01:24:32.200]   So it seems to me, Google's being the good guy here.
[01:24:32.200 --> 01:24:33.680]   And that's Sundar Pachai's contention.
[01:24:33.680 --> 01:24:36.600]   He says, Google and Android have created more choice.
[01:24:36.600 --> 01:24:37.760]   He's the CEO of Google.
[01:24:37.760 --> 01:24:38.840]   Not less.
[01:24:38.840 --> 01:24:39.680]   OK, sorry, Amy.
[01:24:39.680 --> 01:24:40.000]   Go ahead.
[01:24:40.000 --> 01:24:40.440]   What do you think?
[01:24:40.440 --> 01:24:41.720]   No, well, so this is where--
[01:24:41.720 --> 01:24:44.200]   because I agree with you.
[01:24:44.200 --> 01:24:48.880]   But this is also where our lawmakers, again,
[01:24:48.880 --> 01:24:51.120]   not thinking about the future and the laws
[01:24:51.120 --> 01:24:55.240]   that they create run counter to the practical realities
[01:24:55.240 --> 01:24:56.920]   of the technology and the ecosystems
[01:24:56.920 --> 01:24:58.600]   that the technology relies on.
[01:24:58.600 --> 01:25:01.280]   So Apple is a completely closed environment.
[01:25:01.280 --> 01:25:03.880]   And because it's a proprietary closed environment,
[01:25:03.880 --> 01:25:07.480]   and you are buying a thing that that closed environment
[01:25:07.480 --> 01:25:11.720]   operates under somehow they don't rise
[01:25:11.720 --> 01:25:14.040]   to the level of antitrust.
[01:25:14.040 --> 01:25:18.080]   Well, to be fair, in the EU, there are only 25% of the market
[01:25:18.080 --> 01:25:18.880]   here in the United States.
[01:25:18.880 --> 01:25:21.120]   Sure, and that ultimately--
[01:25:21.120 --> 01:25:21.960]   That's it.
[01:25:21.960 --> 01:25:23.360]   It finishes there, right?
[01:25:23.360 --> 01:25:25.680]   Apple is not a monopolistic.
[01:25:25.680 --> 01:25:27.120]   Because you're--
[01:25:27.120 --> 01:25:28.840]   Because here's the thing.
[01:25:28.840 --> 01:25:30.720]   But here's the thing, because, again,
[01:25:30.720 --> 01:25:34.720]   so my area of research is artificial intelligence.
[01:25:34.720 --> 01:25:40.960]   And so Google is constantly making its platform open.
[01:25:40.960 --> 01:25:42.040]   Anybody can build on it.
[01:25:42.040 --> 01:25:44.680]   So TensorFlow is out there.
[01:25:44.680 --> 01:25:45.520]   It's their framework.
[01:25:45.520 --> 01:25:47.840]   Anybody can build on it and do whatever they want.
[01:25:47.840 --> 01:25:52.320]   And TensorFlow is a big chunk of GitHub.
[01:25:52.320 --> 01:25:54.200]   But GitHub is owned by Microsoft.
[01:25:54.200 --> 01:25:56.640]   And TensorFlow, in order to basically--
[01:25:56.640 --> 01:25:58.520]   there's just a limited number of frameworks
[01:25:58.520 --> 01:26:01.080]   and a limited number of training sets and algorithms
[01:26:01.080 --> 01:26:04.280]   that are practically in use anyways.
[01:26:04.280 --> 01:26:08.560]   And so I think that the better conversation to be having
[01:26:08.560 --> 01:26:13.720]   right now is, what does antitrust mean in an age in which
[01:26:13.720 --> 01:26:15.560]   platforms--
[01:26:15.560 --> 01:26:17.720]   antitrust makes a lot of sense when
[01:26:17.720 --> 01:26:19.920]   you're talking about hamburgers.
[01:26:19.920 --> 01:26:21.680]   It makes a lot less sense when you're
[01:26:21.680 --> 01:26:24.760]   talking about very, very sophisticated, complicated
[01:26:24.760 --> 01:26:27.920]   technologies that are layered.
[01:26:27.920 --> 01:26:29.880]   And you might draw the parallel.
[01:26:29.880 --> 01:26:32.800]   Microsoft got into trouble with Explorer.
[01:26:32.800 --> 01:26:34.480]   Do you remember a long time ago?
[01:26:34.480 --> 01:26:35.800]   Yeah, but again--
[01:26:35.800 --> 01:26:37.840]   But Microsoft was the dominant provider.
[01:26:37.840 --> 01:26:41.640]   They had 80% or 90% of the desktop market.
[01:26:41.640 --> 01:26:45.440]   And politicians and politics generally, or societal moves,
[01:26:45.440 --> 01:26:47.200]   are focused on the number of people impact.
[01:26:47.200 --> 01:26:50.880]   So what you really say is that the thing the EU doesn't like
[01:26:50.880 --> 01:26:52.960]   is Google's just too damn big.
[01:26:52.960 --> 01:26:54.760]   Now, what they-- in this particular case,
[01:26:54.760 --> 01:26:56.320]   this is where the law starts to--
[01:26:56.320 --> 01:26:57.720]   the actual practical law.
[01:26:57.720 --> 01:27:01.000]   Law says the number of people impacted.
[01:27:01.000 --> 01:27:02.000]   Apple might make a decision--
[01:27:02.000 --> 01:27:03.680]   True, by the way, in the United States,
[01:27:03.680 --> 01:27:05.800]   that you're not subject to antitrust law.
[01:27:05.800 --> 01:27:07.520]   If you don't have a dominant position,
[01:27:07.520 --> 01:27:09.920]   if you don't have a monopoly position, it's the same thing.
[01:27:09.920 --> 01:27:11.400]   Here, you have to be big enough.
[01:27:11.400 --> 01:27:12.120]   You have to be big enough.
[01:27:12.120 --> 01:27:15.080]   Have a significant percentage of the market per person.
[01:27:15.080 --> 01:27:18.440]   So in the US, the monopoly condition
[01:27:18.440 --> 01:27:22.080]   only comes in when the consumer suffers financial loss.
[01:27:22.080 --> 01:27:25.600]   The EU law says, we must allow new companies to emerge.
[01:27:25.600 --> 01:27:28.680]   That is, if somebody wants to enter this market,
[01:27:28.680 --> 01:27:31.560]   they must be able to unseat the dominant player
[01:27:31.560 --> 01:27:33.040]   through fair and reasonable competition.
[01:27:33.040 --> 01:27:34.200]   So that's what you're saying, Amy,
[01:27:34.200 --> 01:27:38.440]   is because they're platforms, it doesn't make sense at this point
[01:27:38.440 --> 01:27:40.920]   to say that somebody should come along and unseat Google.
[01:27:40.920 --> 01:27:42.400]   That doesn't even make sense.
[01:27:42.400 --> 01:27:45.440]   What I'm saying is that the EU, I think, is very prescient
[01:27:45.440 --> 01:27:49.000]   when it is thinking about the future of data and technology
[01:27:49.000 --> 01:27:49.960]   and people.
[01:27:49.960 --> 01:27:53.120]   Don't forget, they had a 1990-- they were early in the days
[01:27:53.120 --> 01:27:56.680]   of the internet with a very, very early data protection
[01:27:56.680 --> 01:27:58.560]   that launched in 1995.
[01:27:58.560 --> 01:28:01.560]   On balance, we have deleted GDPR has been a benefit
[01:28:01.560 --> 01:28:03.560]   for everybody in the world.
[01:28:03.560 --> 01:28:04.480]   I would disagree.
[01:28:04.480 --> 01:28:06.680]   Oh.
[01:28:06.680 --> 01:28:10.440]   Because there are plenty of legitimate news websites
[01:28:10.440 --> 01:28:13.040]   with legit-- like the Chicago Tribune, which
[01:28:13.040 --> 01:28:16.240]   couldn't be seen in areas around Europe.
[01:28:16.240 --> 01:28:18.040]   It's because they're dopey.
[01:28:18.040 --> 01:28:19.000]   Well, it--
[01:28:19.000 --> 01:28:19.760]   Yes, it now.
[01:28:19.760 --> 01:28:20.000]   I mean--
[01:28:20.000 --> 01:28:21.120]   No, honestly, so what?
[01:28:21.120 --> 01:28:22.120]   Who cares?
[01:28:22.120 --> 01:28:23.640]   If it's a Chicago Tribune and they
[01:28:23.640 --> 01:28:26.120]   don't want to show their content in Europe, so what?
[01:28:26.120 --> 01:28:28.600]   Because it creates a fragmented information space.
[01:28:28.600 --> 01:28:30.400]   We've already got an ad segmented into it.
[01:28:30.400 --> 01:28:30.800]   I agree.
[01:28:30.800 --> 01:28:32.760]   We've already have a fragmented information space.
[01:28:32.760 --> 01:28:33.680]   It's called Google.
[01:28:33.680 --> 01:28:35.240]   He who ranks first wins.
[01:28:35.240 --> 01:28:36.800]   Everybody else loses.
[01:28:36.800 --> 01:28:39.840]   But the problem is if somebody-- their IP is being blocked
[01:28:39.840 --> 01:28:43.280]   and you're not getting in there through a side door.
[01:28:43.280 --> 01:28:45.960]   You are going there directly outside of search.
[01:28:45.960 --> 01:28:48.520]   The problem that I have is that I've
[01:28:48.520 --> 01:28:51.120]   been talking a lot about the emergence of splinter nets.
[01:28:51.120 --> 01:28:52.800]   We have a fractured splinter net.
[01:28:52.800 --> 01:28:53.920]   Yeah, we don't want that.
[01:28:53.920 --> 01:28:54.440]   Yeah.
[01:28:54.440 --> 01:28:56.640]   We don't want that, but we're already seeing that.
[01:28:56.640 --> 01:28:57.200]   So it might point--
[01:28:57.200 --> 01:28:59.360]   Well, and frankly, we're seeing it because Facebook
[01:28:59.360 --> 01:29:01.720]   is the internet for-- seems to be the internet for a lot of people.
[01:29:01.720 --> 01:29:01.720]   Sure.
[01:29:01.720 --> 01:29:03.480]   We're seeing that in lots of different places.
[01:29:03.480 --> 01:29:07.120]   And within the EU, the GDPR is being implemented differently
[01:29:07.120 --> 01:29:08.880]   depending on the local regulators.
[01:29:08.880 --> 01:29:11.400]   But here's the thing with the antitrust suit
[01:29:11.400 --> 01:29:15.520]   and the massive fine and with Google.
[01:29:15.520 --> 01:29:19.960]   The problem is the intent of the laws, I think,
[01:29:19.960 --> 01:29:20.920]   makes sense.
[01:29:20.920 --> 01:29:23.400]   It's the execution and implementation
[01:29:23.400 --> 01:29:26.160]   because you have to have something executable, which
[01:29:26.160 --> 01:29:30.120]   means you've got to be thinking about what can we do right now,
[01:29:30.120 --> 01:29:31.960]   is where the problems are.
[01:29:31.960 --> 01:29:33.520]   And the way--
[01:29:33.520 --> 01:29:36.080]   It also has to be said that Google had plenty of chance
[01:29:36.080 --> 01:29:37.960]   to not let this happen.
[01:29:37.960 --> 01:29:38.800]   So it was called--
[01:29:38.800 --> 01:29:40.160]   What is it supposed to do?
[01:29:40.160 --> 01:29:41.440]   Google chose this.
[01:29:41.440 --> 01:29:43.360]   Google chose to be in this position.
[01:29:43.360 --> 01:29:45.520]   So what was Google's fork?
[01:29:45.520 --> 01:29:47.400]   What could they have done?
[01:29:47.400 --> 01:29:49.960]   The European Commission told them very clearly
[01:29:49.960 --> 01:29:51.320]   you need to do these things.
[01:29:51.320 --> 01:29:54.000]   It refused to be less successful.
[01:29:54.000 --> 01:29:57.080]   No, it was told that it had to share access to its platform,
[01:29:57.080 --> 01:29:59.200]   allow other search engines to a mute.
[01:29:59.200 --> 01:30:00.800]   They knew that from the Microsoft.
[01:30:00.800 --> 01:30:02.800]   Remember the Bing thing on top of the Microsoft?
[01:30:02.800 --> 01:30:04.280]   I would point out on the iPhone--
[01:30:04.280 --> 01:30:06.600]   It's not like the competition wasn't telegraphed to them.
[01:30:06.600 --> 01:30:08.080]   It wasn't like slap.
[01:30:08.080 --> 01:30:09.080]   Here's a fine.
[01:30:09.080 --> 01:30:10.880]   They've been told for five years that if they didn't--
[01:30:10.880 --> 01:30:13.040]   I can't replace Safari for as default browser.
[01:30:13.040 --> 01:30:15.080]   I can't replace screenboard as default launcher.
[01:30:15.080 --> 01:30:17.920]   No, the default is always Safari on iPhone.
[01:30:17.920 --> 01:30:21.040]   On Android, I can have it be Microsoft's launcher.
[01:30:21.040 --> 01:30:22.360]   I can have it the Edge browser.
[01:30:22.360 --> 01:30:23.600]   I can have Bing be the search.
[01:30:23.600 --> 01:30:25.560]   It's a trivially easy thing to do.
[01:30:25.560 --> 01:30:27.320]   In fact, Sonja Pichai on his blog post
[01:30:27.320 --> 01:30:30.560]   shows how you could do that in under 25 seconds.
[01:30:30.560 --> 01:30:32.880]   So I dropped out of law school before I started.
[01:30:32.880 --> 01:30:35.520]   So I am not speaking with any legal authority.
[01:30:35.520 --> 01:30:38.240]   I mean, you can't drop out of something before you started.
[01:30:38.240 --> 01:30:40.960]   I made an alternate decision not to go.
[01:30:40.960 --> 01:30:42.760]   So you got in and you were going to go
[01:30:42.760 --> 01:30:45.080]   and then you said, I'll screw it.
[01:30:45.080 --> 01:30:47.280]   It was a weird moment in my life that I--
[01:30:47.280 --> 01:30:47.920]   No, no.
[01:30:47.920 --> 01:30:50.080]   And you're not regretting that one bit, Amy Webb.
[01:30:50.080 --> 01:30:50.840]   Not at all.
[01:30:50.840 --> 01:30:52.440]   I would have been a horrible lawyer.
[01:30:52.440 --> 01:30:55.440]   The happiest lawyers I know are lapsed lawyers.
[01:30:55.440 --> 01:30:59.280]   Yeah, it would have not been good for everybody.
[01:30:59.280 --> 01:31:00.720]   But my point is--
[01:31:00.720 --> 01:31:01.200]   OK.
[01:31:01.200 --> 01:31:06.680]   My point is this is, again, so like the law--
[01:31:06.680 --> 01:31:07.960]   I understand intent.
[01:31:07.960 --> 01:31:10.560]   But the way that when you're dealing with legal frameworks,
[01:31:10.560 --> 01:31:12.480]   you have to have stuff in writing.
[01:31:12.480 --> 01:31:14.880]   And the problem is that the stuff in writing
[01:31:14.880 --> 01:31:18.360]   applies in a meaningful way to Google Apple.
[01:31:18.360 --> 01:31:19.440]   It doesn't.
[01:31:19.440 --> 01:31:21.800]   Even though Apple, I would argue, is violating the spirit
[01:31:21.800 --> 01:31:23.080]   of the law, not Google.
[01:31:23.080 --> 01:31:23.560]   Yes.
[01:31:23.560 --> 01:31:24.920]   Yes.
[01:31:24.920 --> 01:31:26.440]   So the problem is an implementation.
[01:31:26.440 --> 01:31:30.400]   And the problem is, by the time that the moment
[01:31:30.400 --> 01:31:35.000]   to fine rolls around, you've got too many people making
[01:31:35.000 --> 01:31:38.440]   decisions, and nobody looking at the farther term--
[01:31:38.440 --> 01:31:40.560]   Well, and that is what we saw with Microsoft,
[01:31:40.560 --> 01:31:44.520]   is that by the time this whole process finished,
[01:31:44.520 --> 01:31:45.520]   it was done--
[01:31:45.520 --> 01:31:46.040]   It's a really--
[01:31:46.040 --> 01:31:47.040]   It was over anyway.
[01:31:47.040 --> 01:31:47.560]   It was purely around.
[01:31:47.560 --> 01:31:48.560]   So the question's made a lot faster.
[01:31:48.560 --> 01:31:53.040]   My point is, Google chose to be in breach of the law here.
[01:31:53.040 --> 01:31:54.920]   The European Commission told them long before they--
[01:31:54.920 --> 01:31:55.960]   So answer any question.
[01:31:55.960 --> 01:31:58.280]   What path should Google have gone down?
[01:31:58.280 --> 01:31:59.960]   Google is the dominant player.
[01:31:59.960 --> 01:32:02.200]   The law is clear that they have to share access
[01:32:02.200 --> 01:32:04.440]   to their platform in this circumstance.
[01:32:04.440 --> 01:32:05.880]   Apple is not the way--
[01:32:05.880 --> 01:32:09.080]   I can't say-- you can't say Apple should do this too,
[01:32:09.080 --> 01:32:10.480]   because it is not the dominant player.
[01:32:10.480 --> 01:32:13.080]   Apple has carefully judged its sense in the market.
[01:32:13.080 --> 01:32:16.400]   It does not want to ever be the majority supplier of handsets.
[01:32:16.400 --> 01:32:18.080]   And so you don't have to go--
[01:32:18.080 --> 01:32:19.080]   I understand that.
[01:32:19.080 --> 01:32:20.600]   --and up your platform if you choose to be 25% of the month.
[01:32:20.600 --> 01:32:22.520]   But I guess how Google's platform not open.
[01:32:22.520 --> 01:32:24.120]   What do you want, source code?
[01:32:24.120 --> 01:32:24.760]   Well, you have it.
[01:32:24.760 --> 01:32:25.280]   You actually--
[01:32:25.280 --> 01:32:27.680]   When you're the dominant player, the rules are different.
[01:32:27.680 --> 01:32:28.560]   So what should they do?
[01:32:28.560 --> 01:32:29.040]   That's different.
[01:32:29.040 --> 01:32:30.080]   What's wrong about that, right?
[01:32:30.080 --> 01:32:31.800]   Just because you're the 75% market.
[01:32:31.800 --> 01:32:34.640]   I guess what I'm saying is, again, so like--
[01:32:34.640 --> 01:32:37.280]   believe me, I don't like what I'm seeing Google
[01:32:37.280 --> 01:32:38.680]   doing a lot of different spaces.
[01:32:38.680 --> 01:32:39.160]   But in this particular case--
[01:32:39.160 --> 01:32:41.120]   We're not saying Google's perfect by any.
[01:32:41.120 --> 01:32:43.080]   No, no, but in this particular case,
[01:32:43.080 --> 01:32:46.160]   I don't understand how they could have remedied
[01:32:46.160 --> 01:32:48.280]   the current situation without--
[01:32:48.280 --> 01:32:51.560]   You say any handmaker who wants to put the Play Store
[01:32:51.560 --> 01:32:55.160]   and Chrome on their handsets go have at it,
[01:32:55.160 --> 01:32:56.440]   you're welcome to do so.
[01:32:56.440 --> 01:32:58.280]   And then they'll build another operating system,
[01:32:58.280 --> 01:32:59.160]   like the one they're building.
[01:32:59.160 --> 01:33:00.360]   I can't remember the name of it.
[01:33:00.360 --> 01:33:00.880]   Fuchsia.
[01:33:00.880 --> 01:33:02.840]   And maybe that's a different one that's closed,
[01:33:02.840 --> 01:33:05.040]   but it'll actually have a different set of market share.
[01:33:05.040 --> 01:33:05.720]   So it would be--
[01:33:05.720 --> 01:33:07.080]   As if now, Fuchsia's open.
[01:33:07.080 --> 01:33:07.680]   Yeah.
[01:33:07.680 --> 01:33:10.040]   Maybe it won't be in the future in reaction to this.
[01:33:10.040 --> 01:33:11.640]   Doc.com work on it.
[01:33:11.640 --> 01:33:13.680]   But the point is, is that Google's
[01:33:13.680 --> 01:33:17.760]   been slapped with a $5 billion fine that they chose to take,
[01:33:17.760 --> 01:33:18.680]   rather than to fit in the--
[01:33:18.680 --> 01:33:20.120]   Well, no, they're appealing this.
[01:33:20.120 --> 01:33:23.000]   Yeah, they'll appeal it, but it's still less than 5% of cash--
[01:33:23.000 --> 01:33:25.680]   You say they chose by their actions, rather than--
[01:33:25.680 --> 01:33:28.040]   What they're effectively saying by never having fixed that
[01:33:28.040 --> 01:33:31.120]   before today is that the $5 billion is worth it.
[01:33:31.120 --> 01:33:31.920]   And by the way--
[01:33:31.920 --> 01:33:33.000]   They're making more than $5 billion.
[01:33:33.000 --> 01:33:34.080]   It's probably in Google's interest
[01:33:34.080 --> 01:33:36.400]   to give away the Play Store, which they make money on,
[01:33:36.400 --> 01:33:38.240]   and the Chrome browser, which way they make money on.
[01:33:38.240 --> 01:33:40.160]   You could make the case that the Play Store is necessary
[01:33:40.160 --> 01:33:41.640]   to secure Android.
[01:33:41.640 --> 01:33:44.120]   People should not be-- the success of Apple's phone
[01:33:44.120 --> 01:33:46.880]   has been the App Store, and the fact that apps are safe,
[01:33:46.880 --> 01:33:48.120]   malware-free.
[01:33:48.120 --> 01:33:51.400]   Android is a train smash of a security app.
[01:33:51.400 --> 01:33:54.040]   Everything on-- you can't find an Android phone that's
[01:33:54.040 --> 01:33:55.280]   basically not malware.
[01:33:55.280 --> 01:33:57.480]   If Google had closed sourced Android
[01:33:57.480 --> 01:33:59.800]   and made themselves like Apple, the only maker
[01:33:59.800 --> 01:34:04.400]   of Android headsets, would that have been compliant?
[01:34:04.400 --> 01:34:04.960]   Who knows?
[01:34:04.960 --> 01:34:08.680]   Would the world be a better place with 800,000 CD and phones?
[01:34:08.680 --> 01:34:10.760]   So the question more likely is if Google
[01:34:10.760 --> 01:34:12.240]   had have closed sourced it, would we
[01:34:12.240 --> 01:34:15.720]   see other operating systems from companies like Samsung?
[01:34:15.720 --> 01:34:16.240]   Yeah, Tizen.
[01:34:16.240 --> 01:34:17.800]   There would be more platforms in the market.
[01:34:17.800 --> 01:34:19.040]   Yeah, Tizen.
[01:34:19.040 --> 01:34:20.000]   Well, whatever it would be.
[01:34:20.000 --> 01:34:20.640]   Maybe.
[01:34:20.640 --> 01:34:21.840]   What about Palm OS?
[01:34:21.840 --> 01:34:22.360]   Palm OS.
[01:34:22.360 --> 01:34:25.120]   What about Q and X from the BlackBerry operating system?
[01:34:25.120 --> 01:34:25.920]   Why about all the others?
[01:34:25.920 --> 01:34:28.560]   All great winners in the marketplace of ideas.
[01:34:28.560 --> 01:34:30.360]   And they were all killed because Google gave away--
[01:34:30.360 --> 01:34:32.440]   No, they were killed because they sucked.
[01:34:32.440 --> 01:34:34.120]   Yeah, I would agree.
[01:34:34.120 --> 01:34:35.080]   I don't think they died because--
[01:34:35.080 --> 01:34:37.600]   They were killed because Android was free.
[01:34:37.600 --> 01:34:40.120]   Microsoft Windows tried to license its operating system.
[01:34:40.120 --> 01:34:41.120]   You have a point.
[01:34:41.120 --> 01:34:42.640]   BlackBerry tried to license its operating system.
[01:34:42.640 --> 01:34:43.840]   Palm OS was trying to be licensed.
[01:34:43.840 --> 01:34:49.200]   Microsoft killed Netscape by giving away Internet Explorer.
[01:34:49.200 --> 01:34:50.440]   So that's a kind of dumping.
[01:34:50.440 --> 01:34:51.360]   Yep.
[01:34:51.360 --> 01:34:52.360]   So that may be the point.
[01:34:52.360 --> 01:34:53.160]   So you say Google--
[01:34:53.160 --> 01:34:53.560]   Google has been--
[01:34:53.560 --> 01:34:54.480]   --with all of its competitors.
[01:34:54.480 --> 01:34:55.560]   --and created it as a--
[01:34:55.560 --> 01:34:57.160]   So once it killed all of its competitors,
[01:34:57.160 --> 01:34:58.880]   it's now the dominant player.
[01:34:58.880 --> 01:35:02.760]   And it now dictates to people what can happen on Android.
[01:35:02.760 --> 01:35:03.680]   That is a monopolist position.
[01:35:03.680 --> 01:35:04.680]   All right, you can bid me.
[01:35:04.680 --> 01:35:05.180]   Right?
[01:35:05.180 --> 01:35:05.840]   Has it convinced you any--
[01:35:05.840 --> 01:35:07.120]   And all the other platforms were extinguished.
[01:35:07.120 --> 01:35:08.120]   That's a very good question.
[01:35:08.120 --> 01:35:08.320]   Sure.
[01:35:08.320 --> 01:35:09.920]   Now, I concur.
[01:35:09.920 --> 01:35:13.040]   And effectively, Google has said $5 billion
[01:35:13.040 --> 01:35:14.920]   against a known thing that's coming.
[01:35:14.920 --> 01:35:17.040]   That's worth doing on making more than $5 billion.
[01:35:17.040 --> 01:35:18.320]   Their plan was, oh, good.
[01:35:18.320 --> 01:35:19.920]   We found this open source-- which they didn't write, by the way.
[01:35:19.920 --> 01:35:22.960]   This open source, Andy Rubin's open source operating system
[01:35:22.960 --> 01:35:24.120]   for phones called Android.
[01:35:24.120 --> 01:35:24.800]   They bought it.
[01:35:24.800 --> 01:35:25.480]   Yep.
[01:35:25.480 --> 01:35:26.320]   They said, you know what?
[01:35:26.320 --> 01:35:27.360]   You know what would be cool.
[01:35:27.360 --> 01:35:29.080]   We could use this to generate more ad revenue.
[01:35:29.080 --> 01:35:33.520]   If we dumped this and made it on every phone possible,
[01:35:33.520 --> 01:35:37.720]   it is totally dominant globally except in the US, then what do we--
[01:35:37.720 --> 01:35:39.560]   No, it is now.
[01:35:39.560 --> 01:35:40.760]   It's 60% of the US.
[01:35:40.760 --> 01:35:42.240]   Even in the US, OK.
[01:35:42.240 --> 01:35:44.280]   Yeah.
[01:35:44.280 --> 01:35:44.960]   Wow.
[01:35:44.960 --> 01:35:48.280]   Then no one will write anything to compete with us.
[01:35:48.280 --> 01:35:50.520]   And then, who else is going to spend $1 billion
[01:35:50.520 --> 01:35:51.920]   writing a smart online writing system?
[01:35:51.920 --> 01:35:53.080]   OK, so again, then that to me.
[01:35:53.080 --> 01:35:55.200]   So I get the intent.
[01:35:55.200 --> 01:35:59.800]   But again, this is where I think our legal frameworks
[01:35:59.800 --> 01:36:04.000]   and our expectations for how our open markets work
[01:36:04.000 --> 01:36:07.520]   don't mesh with the realities of everyday practical use.
[01:36:07.520 --> 01:36:13.240]   So what the European Commission wants is for good Android.
[01:36:13.240 --> 01:36:16.440]   Oh, go ahead, Amy.
[01:36:16.440 --> 01:36:20.120]   So we can't, on the one hand, bitch at Google
[01:36:20.120 --> 01:36:23.240]   for keeping its stuff under lock and key.
[01:36:23.240 --> 01:36:25.880]   And then also, bitch at them for making it open.
[01:36:25.880 --> 01:36:31.080]   And I realize that incentivizes others away
[01:36:31.080 --> 01:36:34.240]   from creating their own systems because one
[01:36:34.240 --> 01:36:39.720]   exists that superior and it's being given away for free.
[01:36:39.720 --> 01:36:42.480]   But then what would be the next option?
[01:36:42.480 --> 01:36:46.240]   Google abused its monopolistic position
[01:36:46.240 --> 01:36:48.560]   to give away its operating system to extinguish all
[01:36:48.560 --> 01:36:49.360]   competition.
[01:36:49.360 --> 01:36:51.400]   And the answer to extinguish the competition
[01:36:51.400 --> 01:36:54.720]   is then abused its dominant power to force, to make money.
[01:36:54.720 --> 01:36:57.480]   The next best choice is to not make as much money.
[01:36:57.480 --> 01:37:00.840]   And no company is designed to make that choice.
[01:37:00.840 --> 01:37:03.800]   So these steps are designed to allow another competitor
[01:37:03.800 --> 01:37:04.800]   to enter the market.
[01:37:04.800 --> 01:37:07.760]   Now, OK, I can't replace Android, so they
[01:37:07.760 --> 01:37:10.360]   have to open up their platforms so that other people,
[01:37:10.360 --> 01:37:14.640]   companies can enter the market and through browsers or ads
[01:37:14.640 --> 01:37:16.800]   or apps or whatever.
[01:37:16.800 --> 01:37:19.640]   And Google knew it was coming and they can't whine and go,
[01:37:19.640 --> 01:37:21.400]   oh, you're happy?
[01:37:21.400 --> 01:37:22.480]   Because they knew it was coming.
[01:37:22.480 --> 01:37:24.560]   And that's always going to come since 1995
[01:37:24.560 --> 01:37:28.600]   because this is exactly the Microsoft Windows search engine
[01:37:28.600 --> 01:37:31.000]   and browser and the whole thing all over again.
[01:37:31.000 --> 01:37:33.600]   The 1995 Windows 95 Internet Explorer.
[01:37:33.600 --> 01:37:35.400]   I think actually you've convinced me.
[01:37:35.400 --> 01:37:37.800]   Yeah.
[01:37:37.800 --> 01:37:38.400]   Interesting.
[01:37:38.400 --> 01:37:43.600]   On the other hand, the eggs are cracked.
[01:37:43.600 --> 01:37:46.160]   There's nothing we can do to put the egg back in the shell.
[01:37:46.160 --> 01:37:48.760]   No.
[01:37:48.760 --> 01:37:51.560]   Fuchsia-- I wonder what the plan is then for Fuchsia.
[01:37:51.560 --> 01:37:54.440]   It sounds like what the Fuchsia team, which, by the way,
[01:37:54.440 --> 01:37:56.160]   is now 100 engineers strong.
[01:37:56.160 --> 01:38:00.760]   This is Google's new one operating system to rule them all.
[01:38:00.760 --> 01:38:02.640]   It sounds like their plan is to literally make
[01:38:02.640 --> 01:38:07.960]   this operating system run phones, IoT devices, smart speakers,
[01:38:07.960 --> 01:38:08.960]   even Chromebooks.
[01:38:08.960 --> 01:38:12.520]   This is the single operating system of the future.
[01:38:12.520 --> 01:38:14.080]   It is currently open source.
[01:38:14.080 --> 01:38:15.400]   In fact, people have contributed to it,
[01:38:15.400 --> 01:38:18.280]   which means I think it kind of has to stay open source.
[01:38:18.280 --> 01:38:20.080]   Is that going to be dumped on the world
[01:38:20.080 --> 01:38:23.360]   and then Microsoft Windows and Mac OS go away?
[01:38:23.360 --> 01:38:25.880]   And everybody uses Fuchsia and then Google dominates.
[01:38:25.880 --> 01:38:26.560]   Who knows?
[01:38:26.560 --> 01:38:28.000]   Total world global domination.
[01:38:28.000 --> 01:38:28.960]   Don't tell me what to tell.
[01:38:28.960 --> 01:38:31.840]   How many companies can afford to put hundreds of engineers
[01:38:31.840 --> 01:38:34.680]   on a project for year in year out?
[01:38:34.680 --> 01:38:38.200]   So then we ought to be asking the question right now.
[01:38:38.200 --> 01:38:41.600]   Does that run afoul of antitrust?
[01:38:41.600 --> 01:38:43.360]   Only policies.
[01:38:43.360 --> 01:38:46.280]   Well, but again, so I guess that we want--
[01:38:46.280 --> 01:38:48.480]   This is part of the problem is that all of this stuff
[01:38:48.480 --> 01:38:53.680]   is reactionary instead of pro-lores reactionary.
[01:38:53.680 --> 01:38:55.760]   You can't-- to be prior restraint.
[01:38:55.760 --> 01:38:59.480]   Now, the enforcement of that law is reactionary.
[01:38:59.480 --> 01:39:04.320]   The development of the law ought to be using strategic foresight
[01:39:04.320 --> 01:39:07.800]   and thinking about how it might be--
[01:39:07.800 --> 01:39:10.120]   It's hard to do though when a company is willing to eat
[01:39:10.120 --> 01:39:12.000]   a $5 billion fine.
[01:39:12.000 --> 01:39:14.560]   It's just kind of hard to think of something you could do
[01:39:14.560 --> 01:39:15.640]   to discourage that interview.
[01:39:15.640 --> 01:39:18.120]   And so that my point is the EU had to have known that.
[01:39:18.120 --> 01:39:19.200]   They had to have known that.
[01:39:19.200 --> 01:39:20.480]   So what's the--
[01:39:20.480 --> 01:39:22.680]   It takes time for the wheels to turn.
[01:39:22.680 --> 01:39:23.920]   And the law always reacts.
[01:39:23.920 --> 01:39:25.600]   I mean, technology moves so much faster
[01:39:25.600 --> 01:39:28.520]   than the legal environment, framework can.
[01:39:28.520 --> 01:39:32.000]   This is why Amy doesn't want to live in 20 years in the future.
[01:39:32.000 --> 01:39:34.360]   It's going to be dominated by Fuchsia.
[01:39:34.360 --> 01:39:34.840]   Yep.
[01:39:34.840 --> 01:39:36.200]   It's a hideous color.
[01:39:36.200 --> 01:39:37.360]   The interesting corollary here is
[01:39:37.360 --> 01:39:40.080]   that if apple does get to be more than 50% of hand sets
[01:39:40.080 --> 01:39:44.080]   in Europe, it will have a problem.
[01:39:44.080 --> 01:39:46.200]   Maybe that's why they're not trying to.
[01:39:46.200 --> 01:39:48.760]   Maybe they-- you know what's actually a smart strategy.
[01:39:48.760 --> 01:39:49.840]   We'll just scream the--
[01:39:49.840 --> 01:39:52.080]   scream the-- skim the cream.
[01:39:52.080 --> 01:39:53.160]   We'll cream the skim.
[01:39:53.160 --> 01:39:53.800]   Yes.
[01:39:53.800 --> 01:39:55.400]   There's two different meanings there.
[01:39:55.400 --> 01:39:57.920]   We'll skim the cream.
[01:39:57.920 --> 01:40:02.320]   And just make a high-end device that is high-profit,
[01:40:02.320 --> 01:40:04.840]   sell it to a minority of users.
[01:40:04.840 --> 01:40:05.800]   And we're happy.
[01:40:05.800 --> 01:40:07.120]   Apple is not preventing other people
[01:40:07.120 --> 01:40:09.080]   from entering the market, where in this case, Google
[01:40:09.080 --> 01:40:11.080]   clearly is because they're giving away the--
[01:40:11.080 --> 01:40:13.240]   You know, couldn't you have made the same argument
[01:40:13.240 --> 01:40:14.880]   about BlackBerry 10 years ago?
[01:40:14.880 --> 01:40:17.360]   BlackBerry had 98% global market share.
[01:40:17.360 --> 01:40:20.400]   BlackBerry had, because of its security features,
[01:40:20.400 --> 01:40:24.480]   overwhelmingly, the federal contracts.
[01:40:24.480 --> 01:40:26.520]   I mean, nobody else could compete against BlackBerry.
[01:40:26.520 --> 01:40:31.080]   Did BlackBerry face similar antitrust lawsuits and arguments?
[01:40:31.080 --> 01:40:33.520]   I don't know, but I suspect they would have eventually.
[01:40:33.520 --> 01:40:34.440]   It was too quick.
[01:40:34.440 --> 01:40:36.960]   But they were-- it was over before it started.
[01:40:36.960 --> 01:40:37.480]   Yeah.
[01:40:37.480 --> 01:40:38.640]   But it wasn't, though.
[01:40:38.640 --> 01:40:40.960]   But at the same time, we also had Nokia headsets.
[01:40:40.960 --> 01:40:42.840]   Everybody else had their own operating systems.
[01:40:42.840 --> 01:40:45.040]   It wasn't like you had to buy BlackBerry's--
[01:40:45.040 --> 01:40:46.840]   OK, it was actually a farmer, a dominant.
[01:40:46.840 --> 01:40:47.640]   That's right.
[01:40:47.640 --> 01:40:50.200]   So here's what I'm getting at, because I actually disagree.
[01:40:50.200 --> 01:40:52.720]   I think that if you wanted to do a certain kind of business,
[01:40:52.720 --> 01:40:55.960]   you had to have a BlackBerry because of the security protocols.
[01:40:55.960 --> 01:40:57.200]   So here's what I'm getting at.
[01:40:57.200 --> 01:41:01.040]   I think a lot of regulators and companies and countries
[01:41:01.040 --> 01:41:03.160]   around the world don't like Google.
[01:41:03.160 --> 01:41:04.720]   I think that they don't like--
[01:41:04.720 --> 01:41:06.880]   Isn't it funny how Silicon Valley gets a warning when
[01:41:06.880 --> 01:41:08.360]   they don't get loved?
[01:41:08.360 --> 01:41:09.920]   Well, I mean--
[01:41:09.920 --> 01:41:10.320]   I just--
[01:41:10.320 --> 01:41:12.480]   But they're doing this for us.
[01:41:12.480 --> 01:41:12.920]   What?
[01:41:12.920 --> 01:41:13.440]   Who?
[01:41:13.440 --> 01:41:14.560]   Google's doing--
[01:41:14.560 --> 01:41:15.720]   They're doing it because they love us.
[01:41:15.720 --> 01:41:17.960]   All they want to do is put the world's information
[01:41:17.960 --> 01:41:18.880]   at your fingertips.
[01:41:18.880 --> 01:41:20.520]   What's so wrong about that?
[01:41:20.520 --> 01:41:21.640]   Yeah.
[01:41:21.640 --> 01:41:26.680]   I think, again, there's got to be a way going forward for us
[01:41:26.680 --> 01:41:32.120]   to think about what's best for us for humanity, right,
[01:41:32.120 --> 01:41:37.080]   in a real sense, and to somehow depoliticize
[01:41:37.080 --> 01:41:40.200]   the regulatory framework of technology.
[01:41:40.200 --> 01:41:41.360]   This isn't political.
[01:41:41.360 --> 01:41:42.760]   This has been coming for 10 years.
[01:41:42.760 --> 01:41:44.240]   It's all even.
[01:41:44.240 --> 01:41:45.880]   This is like World Cup level.
[01:41:45.880 --> 01:41:48.200]   This has been through multiple governments.
[01:41:48.200 --> 01:41:50.440]   This court case has existed across multiple--
[01:41:50.440 --> 01:41:51.440]   I think you're right, Amy.
[01:41:51.440 --> 01:41:53.960]   But I think the only reason you think that's even possible
[01:41:53.960 --> 01:41:55.560]   is because you live in Japan for so long.
[01:41:55.560 --> 01:41:56.720]   Google was really excited to--
[01:41:56.720 --> 01:41:59.320]   That's too much something that could ever happen, right?
[01:41:59.320 --> 01:42:01.120]   Google was really excited two years ago
[01:42:01.120 --> 01:42:04.320]   when the European commissioner for justice actually changed.
[01:42:04.320 --> 01:42:05.600]   They thought they were going to get away with it.
[01:42:05.600 --> 01:42:07.760]   They got Margaret Vestiger instead.
[01:42:07.760 --> 01:42:08.260]   Yeah.
[01:42:08.260 --> 01:42:09.000]   And she went after the--
[01:42:09.000 --> 01:42:10.160]   But at the same time, the EU parliament--
[01:42:10.160 --> 01:42:11.320]   There's no politics here.
[01:42:11.320 --> 01:42:12.920]   This is the law.
[01:42:12.920 --> 01:42:13.800]   Hang on, dude.
[01:42:13.800 --> 01:42:16.000]   At the same time, the EU parliament
[01:42:16.000 --> 01:42:18.880]   is trying to introduce legislation that
[01:42:18.880 --> 01:42:21.240]   would regulate robots.
[01:42:21.240 --> 01:42:21.760]   Oh, wait.
[01:42:21.760 --> 01:42:23.440]   We don't know about that one.
[01:42:23.440 --> 01:42:23.940]   Yes.
[01:42:23.940 --> 01:42:26.880]   And at the same time, we've got the US government legislating
[01:42:26.880 --> 01:42:29.440]   that old data in any American company in the whole world
[01:42:29.440 --> 01:42:30.880]   belongs to the US government.
[01:42:30.880 --> 01:42:32.880]   So let me come back to the robots for a moment.
[01:42:32.880 --> 01:42:34.680]   The reason that that is short-sighted,
[01:42:34.680 --> 01:42:36.680]   because robots are containers.
[01:42:36.680 --> 01:42:41.040]   They are containers for code that works inside them
[01:42:41.040 --> 01:42:43.040]   and on behalf of them and for other things.
[01:42:43.040 --> 01:42:46.600]   So the problem is that we've got--
[01:42:46.600 --> 01:42:48.520]   to some extent, a fundamental misunderstanding
[01:42:48.520 --> 01:42:51.240]   of how a lot of different technologies work.
[01:42:51.240 --> 01:42:55.880]   But a lot of this has become politicized in a way
[01:42:55.880 --> 01:42:56.680]   that doesn't make sense.
[01:42:56.680 --> 01:42:59.760]   And I'm usually not defending Google.
[01:42:59.760 --> 01:43:03.960]   But I think in this particular case,
[01:43:03.960 --> 01:43:07.840]   the EU has a history of politicizing technology,
[01:43:07.840 --> 01:43:10.760]   misunderstanding the implications of that technology,
[01:43:10.760 --> 01:43:12.520]   and writing weird laws.
[01:43:12.520 --> 01:43:14.680]   That is not unreasonable.
[01:43:14.680 --> 01:43:16.360]   I don't think it's unreasonable for the EU
[01:43:16.360 --> 01:43:20.080]   to at least contemplate a law of robotics.
[01:43:20.080 --> 01:43:24.280]   It's not yet codified, but is it a bad thing to think about?
[01:43:24.280 --> 01:43:25.960]   This is what they--
[01:43:25.960 --> 01:43:28.040]   this is from Sebit.
[01:43:28.040 --> 01:43:30.480]   EU delegate Maddie Delveau reports
[01:43:30.480 --> 01:43:33.120]   that commission is currently working on EU-wide laws
[01:43:33.120 --> 01:43:35.240]   of robotics.
[01:43:35.240 --> 01:43:36.040]   One option--
[01:43:36.040 --> 01:43:37.040]   And what they are trying.
[01:43:37.040 --> 01:43:38.360]   --could be to assign robots the status
[01:43:38.360 --> 01:43:42.200]   of electronic personality for damage compensation.
[01:43:42.200 --> 01:43:44.440]   So if your autonomous vehicle runs over somebody,
[01:43:44.440 --> 01:43:47.080]   the robot is responsible.
[01:43:47.080 --> 01:43:48.480]   The other thing they're considering
[01:43:48.480 --> 01:43:53.960]   is mandating a kill switch for robots.
[01:43:53.960 --> 01:43:57.360]   This sounds exactly like laws around cars, another technology.
[01:43:57.360 --> 01:43:58.040]   Well, that's--
[01:43:58.040 --> 01:43:59.600]   --you've got to have a license to drive a car.
[01:43:59.600 --> 01:44:00.760]   The car has to be safe.
[01:44:00.760 --> 01:44:03.000]   And if you run into somebody who's legally responsible,
[01:44:03.000 --> 01:44:05.240]   there's no difference between this and laws around cars.
[01:44:05.240 --> 01:44:07.080]   I think it's an appropriate thing to consider.
[01:44:07.080 --> 01:44:08.240]   Go ahead, Amy.
[01:44:08.240 --> 01:44:09.600]   Well, there is a pretty big difference,
[01:44:09.600 --> 01:44:14.400]   because nowhere in that proposal are they defining
[01:44:14.400 --> 01:44:16.280]   where the chain of custody is.
[01:44:16.280 --> 01:44:19.440]   So are we-- is it the person who created the algorithm,
[01:44:19.440 --> 01:44:22.720]   the person who created the malware, the person--
[01:44:22.720 --> 01:44:24.720]   these are the problems that I'm talking about.
[01:44:24.720 --> 01:44:25.960]   Yeah, the same with the car.
[01:44:25.960 --> 01:44:28.200]   The manufacturer has a responsibility to make the car,
[01:44:28.200 --> 01:44:29.200]   but the person--
[01:44:29.200 --> 01:44:29.200]   --in certain case.
[01:44:29.200 --> 01:44:31.960]   --for driving it is responsible for the results
[01:44:31.960 --> 01:44:33.120]   of their actions when driving the car.
[01:44:33.120 --> 01:44:35.440]   If the car manufacturer creates a faulty petrol tank
[01:44:35.440 --> 01:44:37.200]   and it blows up, I should be responsible.
[01:44:37.200 --> 01:44:37.800]   But that's point of--
[01:44:37.800 --> 01:44:38.840]   It depends on the situation.
[01:44:38.840 --> 01:44:41.800]   But we're talking about intellectual property.
[01:44:41.800 --> 01:44:43.480]   And the problem with intellectual property
[01:44:43.480 --> 01:44:46.560]   is that we have radically different viewpoints on it,
[01:44:46.560 --> 01:44:49.080]   depending on where in the world you are.
[01:44:49.080 --> 01:44:53.320]   And the bigger problem is globalization.
[01:44:53.320 --> 01:44:55.680]   So everybody's doing business with everybody else.
[01:44:55.680 --> 01:44:57.200]   The intellectual property laws don't even
[01:44:57.200 --> 01:44:59.920]   come close to meshing.
[01:44:59.920 --> 01:45:02.840]   Well, white bo's trying to fix that.
[01:45:02.840 --> 01:45:04.080]   And what they're going to make-- they're
[01:45:04.080 --> 01:45:05.560]   going to make it the US law.
[01:45:05.560 --> 01:45:06.520]   That's the law.
[01:45:06.520 --> 01:45:08.520]   That's the IP.
[01:45:08.520 --> 01:45:10.240]   This is interesting.
[01:45:10.240 --> 01:45:13.040]   So I should point out, they're considering--
[01:45:13.040 --> 01:45:13.800]   they're talking.
[01:45:13.800 --> 01:45:15.480]   This is the beginning stages.
[01:45:15.480 --> 01:45:16.600]   There's no law yet.
[01:45:16.600 --> 01:45:18.200]   And this is a good time to weigh in,
[01:45:18.200 --> 01:45:20.880]   if you're a roboticist on what these laws should be.
[01:45:20.880 --> 01:45:25.280]   They have apparently agreed on what makes a robot.
[01:45:25.280 --> 01:45:26.480]   You want to know?
[01:45:26.480 --> 01:45:27.680]   Let's hear it.
[01:45:27.680 --> 01:45:29.520]   There's five things.
[01:45:29.520 --> 01:45:32.520]   The acquisition of autonomy through sensors
[01:45:32.520 --> 01:45:34.560]   or by exchanging data with its environment
[01:45:34.560 --> 01:45:37.400]   and trading and analyzing of that data.
[01:45:37.400 --> 01:45:38.880]   So it's an autonomous device.
[01:45:38.880 --> 01:45:39.640]   It's not a remote control.
[01:45:39.640 --> 01:45:41.280]   Unlike a car, which is a control door.
[01:45:41.280 --> 01:45:42.200]   It's not remote control.
[01:45:42.200 --> 01:45:43.440]   It's not, yeah.
[01:45:43.440 --> 01:45:45.880]   Number two-- and this is an optional criterion--
[01:45:45.880 --> 01:45:48.560]   self-learning from experience and by interaction,
[01:45:48.560 --> 01:45:50.400]   machine learning.
[01:45:50.400 --> 01:45:52.880]   Number three, at least a minor physical support.
[01:45:52.880 --> 01:45:55.360]   That's to distinguish a robot from, say, software.
[01:45:55.360 --> 01:45:57.600]   That's to be something physical.
[01:45:57.600 --> 01:45:59.720]   The adaptation of its behavior and actions
[01:45:59.720 --> 01:46:02.600]   to the environment, it's kind of like--
[01:46:02.600 --> 01:46:04.160]   In other words, it has a feedback loop.
[01:46:04.160 --> 01:46:05.800]   Something goes wrong at self-adapting.
[01:46:05.800 --> 01:46:09.480]   And the absence of life in the biological sense.
[01:46:09.480 --> 01:46:09.920]   Right.
[01:46:09.920 --> 01:46:13.400]   And so again, the thing that we should be concerned about
[01:46:13.400 --> 01:46:15.000]   is recursive self-improvement.
[01:46:15.000 --> 01:46:16.800]   The thing that we should be concerned about
[01:46:16.800 --> 01:46:22.600]   are genetic algorithms that have heritability.
[01:46:22.600 --> 01:46:24.880]   The thing that I am not concerned about
[01:46:24.880 --> 01:46:30.640]   is the cool, as-mo-looking robot and whether or not
[01:46:30.640 --> 01:46:32.400]   it has a kill switch.
[01:46:32.400 --> 01:46:36.840]   So again, you got to think this stuff through.
[01:46:36.840 --> 01:46:40.600]   And I understand that the piece of this that's tangible for us
[01:46:40.600 --> 01:46:45.440]   is Google Chrome is the car, is the robot.
[01:46:45.440 --> 01:46:48.480]   But there are issues here that are far more sophisticated
[01:46:48.480 --> 01:46:53.160]   than a quick update to Asimov's three laws of robotics
[01:46:53.160 --> 01:46:54.560]   plus the zero-width law.
[01:46:54.560 --> 01:46:56.160]   I hope they're acting that way.
[01:46:56.160 --> 01:46:57.000]   I mean, at least--
[01:46:57.000 --> 01:46:58.160]   It's not like they're about to let you just--
[01:46:58.160 --> 01:46:58.640]   God bless them.
[01:46:58.640 --> 01:47:00.680]   They didn't just say, hey, let's just use Asimov's law.
[01:47:00.680 --> 01:47:01.960]   Those are good enough.
[01:47:01.960 --> 01:47:03.200]   But I've seen people do that.
[01:47:03.200 --> 01:47:04.200]   I've seen--
[01:47:04.200 --> 01:47:08.000]   Even Asimov said that these laws are logically--
[01:47:08.000 --> 01:47:10.640]   Because of his short science fiction story, he wrote.
[01:47:10.640 --> 01:47:11.000]   He said, yeah.
[01:47:11.000 --> 01:47:11.680]   It's a bullshit magazine.
[01:47:11.680 --> 01:47:13.360]   They're logically inconsistent.
[01:47:13.360 --> 01:47:14.880]   They don't even-- you couldn't--
[01:47:14.880 --> 01:47:15.560]   Anyway.
[01:47:15.560 --> 01:47:16.280]   This isn't law.
[01:47:16.280 --> 01:47:19.400]   This is announcing that things-- it's a bit like this cool case
[01:47:19.400 --> 01:47:20.920]   that Google has just--
[01:47:20.920 --> 01:47:23.480]   Everybody has said that we should be thinking about this.
[01:47:23.480 --> 01:47:25.520]   It's been 10 years in the making.
[01:47:25.520 --> 01:47:26.560]   It's not like it was--
[01:47:26.560 --> 01:47:28.040]   But the lawsuit is important.
[01:47:28.040 --> 01:47:29.520]   The outcome of that lawsuit is important.
[01:47:29.520 --> 01:47:31.200]   Whether or not Google pays is important.
[01:47:31.200 --> 01:47:34.240]   These crazy laws of robotics, they use them important.
[01:47:34.240 --> 01:47:36.680]   Because it becomes part of that conversation.
[01:47:36.680 --> 01:47:39.400]   And ultimately, the layer cake that makes up
[01:47:39.400 --> 01:47:42.200]   our incredibly convoluted legal frameworks that
[01:47:42.200 --> 01:47:44.600]   govern everything from technology to commerce
[01:47:44.600 --> 01:47:48.560]   to human rights and liberties as they relate to technology.
[01:47:48.560 --> 01:47:50.360]   So stuff is important.
[01:47:50.360 --> 01:47:51.400]   Oh, I agree.
[01:47:51.400 --> 01:47:53.160]   I think we're at all in agreement on that.
[01:47:53.160 --> 01:47:53.480]   Yeah.
[01:47:53.480 --> 01:47:54.560]   But at least we're talking about it.
[01:47:54.560 --> 01:47:55.000]   Yeah.
[01:47:55.000 --> 01:47:57.520]   Like at least the EU's taking the position and open it up
[01:47:57.520 --> 01:47:57.960]   for discussion.
[01:47:57.960 --> 01:47:59.960]   Well, on the EU's talking about it,
[01:47:59.960 --> 01:48:03.480]   is there a process for feedback or request for comments,
[01:48:03.480 --> 01:48:04.200]   anything like that?
[01:48:04.200 --> 01:48:05.280]   I wonder.
[01:48:05.280 --> 01:48:05.880]   Sure.
[01:48:05.880 --> 01:48:07.360]   Or are they just sitting in a room somewhere
[01:48:07.360 --> 01:48:08.120]   in a mountain in Switzerland?
[01:48:08.120 --> 01:48:09.280]   Anybody can go and participate.
[01:48:09.280 --> 01:48:10.440]   Actually, they're not in Switzerland.
[01:48:10.440 --> 01:48:12.680]   Make submissions to your MEP and away you go.
[01:48:12.680 --> 01:48:14.520]   You know, living in Brussels for any length of time
[01:48:14.520 --> 01:48:16.720]   make anybody crazy.
[01:48:16.720 --> 01:48:17.440]   Oh, just kidding.
[01:48:17.440 --> 01:48:19.240]   They got good beer and chocolate.
[01:48:19.240 --> 01:48:21.080]   Our show today brought you by Stamps.com.
[01:48:21.080 --> 01:48:24.840]   One place you don't have to go anymore if you are a Stamps.com
[01:48:24.840 --> 01:48:26.640]   customer, the post office.
[01:48:26.640 --> 01:48:27.640]   Nothing wrong with it.
[01:48:27.640 --> 01:48:28.680]   I love the post office.
[01:48:28.680 --> 01:48:30.040]   I love my mailman.
[01:48:30.040 --> 01:48:34.280]   But wouldn't it be cool if you could buy and print postage
[01:48:34.280 --> 01:48:36.480]   from your desk with your computer, your printer?
[01:48:36.480 --> 01:48:38.480]   We're not talking on postage meter.
[01:48:38.480 --> 01:48:39.880]   You got everything you need already.
[01:48:39.880 --> 01:48:41.360]   No special ink.
[01:48:41.360 --> 01:48:42.320]   No special hardware.
[01:48:42.320 --> 01:48:44.040]   Just your printer, your computer.
[01:48:44.040 --> 01:48:46.880]   In fact, with Stamps.com, you can access all the amazing
[01:48:46.880 --> 01:48:51.240]   services of the post office right from your desk 24/7.
[01:48:51.240 --> 01:48:54.480]   When it's convenient for you, you can buy and print
[01:48:54.480 --> 01:48:57.280]   official US postage for any letter, any package, any class
[01:48:57.280 --> 01:48:59.320]   of mail using your own computer and printer.
[01:48:59.320 --> 01:49:01.160]   And then the mail carrier comes and picks it up.
[01:49:01.160 --> 01:49:02.960]   I was at the post office the other day.
[01:49:02.960 --> 01:49:04.360]   All I wanted to do, I had a package.
[01:49:04.360 --> 01:49:09.360]   All I wanted to do, they have this new kiosk, no human.
[01:49:09.360 --> 01:49:11.200]   And it was a little bit of a line, so I didn't want to wait
[01:49:11.200 --> 01:49:11.360]   there.
[01:49:11.360 --> 01:49:13.560]   So I went to the kiosk and there's some woman.
[01:49:13.560 --> 01:49:15.760]   All I wanted to do is I had one thing I wanted to print a
[01:49:15.760 --> 01:49:17.400]   stamp for and mail it.
[01:49:17.400 --> 01:49:21.040]   She had a stack, this tall of Manila envelopes.
[01:49:21.040 --> 01:49:22.880]   She was obviously doing some sort of business.
[01:49:22.880 --> 01:49:26.600]   Was there for half an hour weighing pressing buttons.
[01:49:26.600 --> 01:49:30.200]   I almost came up to her and said, you know, if you went to
[01:49:30.200 --> 01:49:33.960]   Stamps.com and use the offer code TWIT, you could do all this
[01:49:33.960 --> 01:49:36.560]   at your desk in minutes.
[01:49:36.560 --> 01:49:38.240]   No equipment, no long term commitments.
[01:49:38.240 --> 01:49:41.120]   They even will send you a digital scale that automatically
[01:49:41.120 --> 01:49:43.680]   calculates the exact postage.
[01:49:43.680 --> 01:49:46.880]   They'll help you decide the best class of mail every time.
[01:49:46.880 --> 01:49:49.160]   You don't have to type anything in.
[01:49:49.160 --> 01:49:52.440]   They'll take the address from the address book or the, you
[01:49:52.440 --> 01:49:55.160]   know, if you're an Amazon seller, Etsy, eBay seller, it'll
[01:49:55.160 --> 01:49:57.320]   just get it from the website, it gets your return address, it
[01:49:57.320 --> 01:50:00.360]   can put your logo on there, click print mail, you're done.
[01:50:00.360 --> 01:50:01.360]   It could not be easier.
[01:50:01.360 --> 01:50:02.440]   We use it.
[01:50:02.440 --> 01:50:04.920]   Highly recommend it.
[01:50:04.920 --> 01:50:07.480]   If you do any kind of mailing, but especially if you're a
[01:50:07.480 --> 01:50:10.120]   seller, it's so, you know, the impression that you make is
[01:50:10.120 --> 01:50:10.560]   so important.
[01:50:10.560 --> 01:50:13.840]   Stamps.com really makes you look like a pro organization.
[01:50:13.840 --> 01:50:15.240]   That's some of the biggest companies in the world, do
[01:50:15.240 --> 01:50:16.360]   Stamps.com.
[01:50:16.360 --> 01:50:17.600]   You should too.
[01:50:17.600 --> 01:50:19.680]   Go to Stamps.com.
[01:50:19.680 --> 01:50:21.080]   You can do like we do.
[01:50:21.080 --> 01:50:22.560]   Enjoy the Stamps.com service.
[01:50:22.560 --> 01:50:23.600]   We've got a special offer.
[01:50:23.600 --> 01:50:26.080]   It includes up to $55 free postage.
[01:50:26.080 --> 01:50:28.880]   That digital scale I mentioned, you get a four week trial.
[01:50:28.880 --> 01:50:30.800]   Did you hear what I just said?
[01:50:30.800 --> 01:50:34.280]   I'm giving you $55 free postage here, folks.
[01:50:34.280 --> 01:50:36.040]   Go to Stamps.com.
[01:50:36.040 --> 01:50:38.120]   Click on the microphone at the top of the home page and
[01:50:38.120 --> 01:50:39.600]   enter Twit.
[01:50:39.600 --> 01:50:42.200]   Stamps.com offer code Twit.
[01:50:42.200 --> 01:50:43.320]   Man, I don't know why.
[01:50:43.320 --> 01:50:45.280]   I should have just gone up to that lady.
[01:50:45.280 --> 01:50:48.120]   Said, "Man, I could help you."
[01:50:48.120 --> 01:50:49.960]   And you could help me.
[01:50:49.960 --> 01:50:52.400]   Stamps.com offer code Twit.
[01:50:52.400 --> 01:50:55.080]   Actually, Leo, you should have just come to work and
[01:50:55.080 --> 01:50:57.560]   used to Stamps.com thing.
[01:50:57.560 --> 01:50:58.960]   What was I saying?
[01:50:58.960 --> 01:51:00.760]   Thank you.
[01:51:00.760 --> 01:51:02.640]   You know, I still have a post office box, so I'd like to
[01:51:02.640 --> 01:51:04.760]   go in there once in a while, and I like the mail people.
[01:51:04.760 --> 01:51:05.760]   I thought that'd be fun.
[01:51:05.760 --> 01:51:06.640]   I used to have one of those.
[01:51:06.640 --> 01:51:08.880]   I used to really enjoy going to the PO box.
[01:51:08.880 --> 01:51:09.320]   You did?
[01:51:09.320 --> 01:51:10.520]   Are you being facetious?
[01:51:10.520 --> 01:51:11.440]   No, no, no, I used to--
[01:51:11.440 --> 01:51:11.960]   It's fun.
[01:51:11.960 --> 01:51:12.680]   You have the key.
[01:51:12.680 --> 01:51:13.120]   You go in.
[01:51:13.120 --> 01:51:15.280]   Special key says US government property.
[01:51:15.280 --> 01:51:16.320]   Do not duplicate.
[01:51:16.320 --> 01:51:18.120]   And I used to be able to ignore my mail.
[01:51:18.120 --> 01:51:19.000]   That was awesome.
[01:51:19.000 --> 01:51:20.200]   Well, you know what's really sad.
[01:51:20.200 --> 01:51:22.240]   You enjoyed going to the post office to take a
[01:51:22.240 --> 01:51:22.880]   look at that, right?
[01:51:22.880 --> 01:51:24.120]   Yeah, when I lived--
[01:51:24.120 --> 01:51:25.200]   What planet are you from?
[01:51:25.200 --> 01:51:26.760]   No, it's fun.
[01:51:26.760 --> 01:51:32.400]   Our post office is-- WPA era's got murals, beautiful old
[01:51:32.400 --> 01:51:33.960]   brass boxes.
[01:51:33.960 --> 01:51:36.240]   I think some of the mail workers are actually as old as
[01:51:36.240 --> 01:51:37.320]   the building.
[01:51:37.320 --> 01:51:38.280]   And you go in there--
[01:51:38.280 --> 01:51:39.760]   I knew my mail guy.
[01:51:39.760 --> 01:51:40.560]   He was a really nice guy.
[01:51:40.560 --> 01:51:41.000]   It's awesome.
[01:51:41.000 --> 01:51:41.960]   I love Maricares.
[01:51:41.960 --> 01:51:43.840]   And then you go in and you have this really old key and
[01:51:43.840 --> 01:51:45.320]   you open the box.
[01:51:45.320 --> 01:51:46.560]   Now what's really sad--
[01:51:46.560 --> 01:51:48.360]   I'm really sad-- is the only way the postal service
[01:51:48.360 --> 01:51:49.800]   continues is by junk mail.
[01:51:49.800 --> 01:51:51.720]   So my box is loaded.
[01:51:51.720 --> 01:51:55.240]   And they even know this because there's the box.
[01:51:55.240 --> 01:51:56.240]   There's the table.
[01:51:56.240 --> 01:51:57.680]   There's the recycling bin.
[01:51:57.680 --> 01:51:59.880]   You just take the stuff out of the box, put it on the
[01:51:59.880 --> 01:52:01.200]   table, throw it in the recycling bin.
[01:52:01.200 --> 01:52:03.360]   For all I know, they take the recycling bin, put it back
[01:52:03.360 --> 01:52:05.440]   in the box.
[01:52:05.440 --> 01:52:09.320]   It reminds me, I went speaking of Mao because you said
[01:52:09.320 --> 01:52:11.360]   you were a Maoist, right, Amy?
[01:52:11.360 --> 01:52:12.520]   No, I did not.
[01:52:12.520 --> 01:52:15.600]   You said you were a Maoist.
[01:52:15.600 --> 01:52:16.320]   For a minute, I heard Maoist.
[01:52:16.320 --> 01:52:18.080]   In fact, I've been censored in China.
[01:52:18.080 --> 01:52:18.720]   I found out.
[01:52:18.720 --> 01:52:19.560]   What?
[01:52:19.560 --> 01:52:20.400]   A Maoist.
[01:52:20.400 --> 01:52:21.040]   Oh, yeah.
[01:52:21.040 --> 01:52:21.560]   Why?
[01:52:21.560 --> 01:52:22.560]   Well, you're lucky.
[01:52:22.560 --> 01:52:24.320]   Well, mine.
[01:52:24.320 --> 01:52:29.520]   Because of the insights that I have shared about why China
[01:52:29.520 --> 01:52:33.800]   is doing what it's doing with AI and how it's collaborating.
[01:52:33.800 --> 01:52:38.240]   And the book that I'm writing now that's coming out soon
[01:52:38.240 --> 01:52:41.160]   is the story of the United States and China and AI.
[01:52:41.160 --> 01:52:42.120]   Well, get us banned.
[01:52:42.120 --> 01:52:43.480]   We got to be banned in China, too.
[01:52:43.480 --> 01:52:46.120]   What tell us all?
[01:52:46.120 --> 01:52:50.160]   What is-- because I'm really-- first of all, and I think
[01:52:50.160 --> 01:52:53.520]   I've said this before, and I've been laughed at, mocked,
[01:52:53.520 --> 01:52:55.680]   especially by people who know about China.
[01:52:55.680 --> 01:52:59.760]   I feel like the 20th century was the American century.
[01:52:59.760 --> 01:53:01.600]   I think we all agree.
[01:53:01.600 --> 01:53:04.080]   I feel like the 21st century is the Chinese century.
[01:53:04.080 --> 01:53:05.040]   Absolutely.
[01:53:05.040 --> 01:53:06.080]   There's no question.
[01:53:06.080 --> 01:53:07.320]   And yeah, for sure.
[01:53:07.320 --> 01:53:08.520]   Yeah, China is--
[01:53:08.520 --> 01:53:09.040]   I agree with Amy.
[01:53:09.040 --> 01:53:11.600]   Because of long-term thinking, the Chinese are taking
[01:53:11.600 --> 01:53:14.200]   25-year and 50-year perspectives,
[01:53:14.200 --> 01:53:16.240]   while the Western societies are looking at things
[01:53:16.240 --> 01:53:19.000]   quarter by quarter, or maybe for as long as three years
[01:53:19.000 --> 01:53:19.680]   of a democracy.
[01:53:19.680 --> 01:53:20.640]   They're still us in, though, Amy.
[01:53:20.640 --> 01:53:21.480]   What are they thinking?
[01:53:21.480 --> 01:53:22.720]   What's the plan?
[01:53:22.720 --> 01:53:26.200]   So this would take a little while to explain.
[01:53:26.200 --> 01:53:26.720]   But the plan--
[01:53:26.720 --> 01:53:29.200]   They don't want anybody in China to know about.
[01:53:29.200 --> 01:53:30.360]   I don't know.
[01:53:30.360 --> 01:53:31.720]   The plan's been around since 1949.
[01:53:31.720 --> 01:53:32.640]   But they're banning you.
[01:53:32.640 --> 01:53:36.400]   So you must have said something they didn't like.
[01:53:36.400 --> 01:53:40.840]   My perspective on this is that she's--
[01:53:40.840 --> 01:53:43.640]   so there is a president now in power, effectively, for life.
[01:53:43.640 --> 01:53:44.760]   Yeah.
[01:53:44.760 --> 01:53:46.680]   That--
[01:53:46.680 --> 01:53:48.560]   That was kind of shocking when she did that.
[01:53:48.560 --> 01:53:50.560]   I thought that was not what I--
[01:53:50.560 --> 01:53:52.200]   So Xi Jinping is president for life.
[01:53:52.200 --> 01:53:54.280]   There are a number of different--
[01:53:54.280 --> 01:53:57.040]   he is a stalwart within the party.
[01:53:57.040 --> 01:53:59.000]   He is beloved within the party.
[01:53:59.000 --> 01:54:01.120]   Is he a criminal school communist?
[01:54:01.120 --> 01:54:02.280]   Very old school.
[01:54:02.280 --> 01:54:03.680]   There are a number of initiatives.
[01:54:03.680 --> 01:54:06.440]   But he's very smart because he understands
[01:54:06.440 --> 01:54:07.920]   how to be on a public stage.
[01:54:07.920 --> 01:54:11.040]   So he is sort of the opposite of being sort of isolationist.
[01:54:11.040 --> 01:54:13.600]   And there are a number of initiatives.
[01:54:13.600 --> 01:54:16.320]   The Belt and Road Initiative is sort of being pitched
[01:54:16.320 --> 01:54:17.600]   as an infrastructure product--
[01:54:17.600 --> 01:54:18.600]   That's kind of interesting.
[01:54:18.600 --> 01:54:21.840]   You go to Beijing and they have these Belt Roads all the way
[01:54:21.840 --> 01:54:24.520]   around in concentric circles.
[01:54:24.520 --> 01:54:26.040]   This is a little different than--
[01:54:26.040 --> 01:54:28.040]   so this is like getting the--
[01:54:28.040 --> 01:54:29.560]   Is that what they're doing or something else?
[01:54:29.560 --> 01:54:30.080]   Yeah.
[01:54:30.080 --> 01:54:32.280]   It's sort of being described as an infrastructure
[01:54:32.280 --> 01:54:34.840]   and trade along what used to be the old Silk Road.
[01:54:34.840 --> 01:54:35.840]   Oh.
[01:54:35.840 --> 01:54:36.840]   Right.
[01:54:36.840 --> 01:54:37.360]   Right.
[01:54:37.360 --> 01:54:41.160]   However, a lot of the pilot companies, the '68 pilot companies,
[01:54:41.160 --> 01:54:44.360]   many of which are in Africa and spread out around,
[01:54:44.360 --> 01:54:46.440]   that have signed up for this are getting
[01:54:46.440 --> 01:54:51.360]   an export of China's social credit score pilot system.
[01:54:51.360 --> 01:54:52.360]   Oh.
[01:54:52.360 --> 01:54:56.200]   A lot of China's surveillance methods.
[01:54:56.200 --> 01:54:58.760]   So from my vantage point, China is spreading
[01:54:58.760 --> 01:55:01.560]   its brand of social capitalism--
[01:55:01.560 --> 01:55:05.760]   or not capitalism-- sorry, its brand of communism.
[01:55:05.760 --> 01:55:08.080]   Surveillance communism.
[01:55:08.080 --> 01:55:10.480]   Authoritarian communism.
[01:55:10.480 --> 01:55:15.400]   In a way to me that is breathtaking.
[01:55:15.400 --> 01:55:18.760]   And because it's autocratic, I mean,
[01:55:18.760 --> 01:55:22.440]   there are sweeping changes sort of being pushed through.
[01:55:22.440 --> 01:55:25.680]   So for everybody who looks at China and says, whatever,
[01:55:25.680 --> 01:55:28.360]   they've tried stuff before, this is copy-paste culture.
[01:55:28.360 --> 01:55:29.400]   That's what they're good at.
[01:55:29.400 --> 01:55:30.480]   Copy-paste.
[01:55:30.480 --> 01:55:32.720]   They are missing an entirely different story
[01:55:32.720 --> 01:55:36.400]   that's unfolding in front of our very eyes, which
[01:55:36.400 --> 01:55:41.160]   is that China has very much transformed from copy-paste
[01:55:41.160 --> 01:55:46.080]   to innovating and spreading some radical ideas in ways
[01:55:46.080 --> 01:55:48.120]   that I think we're all going to find very uncomfortable
[01:55:48.120 --> 01:55:49.880]   if we don't do something about it.
[01:55:49.880 --> 01:55:51.280]   Wow.
[01:55:51.280 --> 01:55:54.280]   So what's the name of the book and when's it come out?
[01:55:54.280 --> 01:55:56.800]   The name of the book is the Big Nine.
[01:55:56.800 --> 01:55:58.920]   The Big Nine refer to the nine companies
[01:55:58.920 --> 01:56:01.480]   that control the future of artificial intelligence,
[01:56:01.480 --> 01:56:04.600]   six are in the United States, three are in China,
[01:56:04.600 --> 01:56:06.240]   and it comes out early next year.
[01:56:06.240 --> 01:56:06.880]   Can't wait.
[01:56:06.880 --> 01:56:08.400]   And it's already been banned in China.
[01:56:08.400 --> 01:56:09.760]   So there you go.
[01:56:09.760 --> 01:56:11.520]   They haven't even read it.
[01:56:11.520 --> 01:56:12.840]   They bought out the contract.
[01:56:12.840 --> 01:56:16.360]   A publisher they're forked out over quite a bit of money
[01:56:16.360 --> 01:56:17.960]   in order to bury the book.
[01:56:17.960 --> 01:56:18.760]   Oh.
[01:56:18.760 --> 01:56:20.040]   And we didn't know that at the time.
[01:56:20.040 --> 01:56:23.640]   So my publisher now is dealing with the--
[01:56:23.640 --> 01:56:26.480]   That's what the National Acquire does, right?
[01:56:26.480 --> 01:56:26.960]   What is it called?
[01:56:26.960 --> 01:56:30.160]   Capture and catch and hold.
[01:56:30.160 --> 01:56:33.920]   Plumping and catch and hold journalism.
[01:56:33.920 --> 01:56:34.720]   Catch what you want to do.
[01:56:34.720 --> 01:56:35.160]   Catch and bury.
[01:56:35.160 --> 01:56:36.120]   Keep what you kill.
[01:56:36.120 --> 01:56:38.800]   Keep what you kill.
[01:56:38.800 --> 01:56:39.480]   Wow.
[01:56:39.480 --> 01:56:40.280]   That's interesting.
[01:56:40.280 --> 01:56:41.600]   Well, I can't wait to read that.
[01:56:41.600 --> 01:56:43.480]   We'll have you on when the book comes out.
[01:56:43.480 --> 01:56:46.160]   And we could spend an hour or two talking about it
[01:56:46.160 --> 01:56:47.120]   on the triangulation.
[01:56:47.120 --> 01:56:50.040]   That sounds really, really interesting.
[01:56:50.040 --> 01:56:51.600]   But it's really, really-- for me,
[01:56:51.600 --> 01:56:56.280]   this is one of those big stories that gets overlooked a lot,
[01:56:56.280 --> 01:56:57.720]   which is unfortunate.
[01:56:57.720 --> 01:57:00.120]   It means that we're stuck in the weeds paying attention
[01:57:00.120 --> 01:57:03.800]   to, I think, the wrong things too often.
[01:57:03.800 --> 01:57:05.960]   You mean in the US or--
[01:57:05.960 --> 01:57:06.680]   Just in general.
[01:57:06.680 --> 01:57:12.360]   I mean, the Amazon store going down during Prime Day.
[01:57:12.360 --> 01:57:13.640]   That's a big story.
[01:57:13.640 --> 01:57:14.800]   I was just about to talk about that.
[01:57:14.800 --> 01:57:15.320]   Well, it is, is it that?
[01:57:15.320 --> 01:57:17.560]   Yeah, I mean, I know that's why I swear.
[01:57:17.560 --> 01:57:18.280]   A second way.
[01:57:18.280 --> 01:57:19.200]   No, I'm not actually--
[01:57:19.200 --> 01:57:21.160]   I was actually literally-- as you said,
[01:57:21.160 --> 01:57:24.520]   that's scrolling past it so we didn't have to talk about it.
[01:57:24.520 --> 01:57:25.440]   It is on the rundown.
[01:57:25.440 --> 01:57:27.040]   But we put everything on there.
[01:57:27.040 --> 01:57:29.320]   So here's something I find interesting.
[01:57:29.320 --> 01:57:31.320]   So the two things happened last week.
[01:57:31.320 --> 01:57:33.680]   So the Amazon store went down during Prime Day.
[01:57:33.680 --> 01:57:36.040]   And everybody talked about it, I think,
[01:57:36.040 --> 01:57:37.920]   in part because of the visuals.
[01:57:37.920 --> 01:57:40.160]   So you were getting pictures of people.
[01:57:40.160 --> 01:57:41.720]   And it was funny to talk about it.
[01:57:41.720 --> 01:57:43.760]   There's another story that was equally, I think,
[01:57:43.760 --> 01:57:46.280]   funny to talk about that made the rounds on Reddit.
[01:57:46.280 --> 01:57:48.200]   But I didn't hear anybody else talk about it.
[01:57:48.200 --> 01:57:50.480]   Somebody discovered that in Google Translate,
[01:57:50.480 --> 01:57:53.680]   if you type in seemingly random letters over and over again,
[01:57:53.680 --> 01:57:55.040]   you get verses from the Bible.
[01:57:55.040 --> 01:57:56.720]   It's just correct.
[01:57:56.720 --> 01:57:57.280]   Yeah.
[01:57:57.280 --> 01:57:59.280]   And it still works.
[01:57:59.280 --> 01:58:01.640]   I was monkeying around with it earlier today.
[01:58:01.640 --> 01:58:05.880]   I was typing in a bunch of stuff in Somali.
[01:58:05.880 --> 01:58:08.880]   And it's still spitting out really weird stuff.
[01:58:08.880 --> 01:58:11.880]   Now they say this is because the AI in Translate
[01:58:11.880 --> 01:58:17.040]   was trained in many texts, including religious texts.
[01:58:17.040 --> 01:58:20.000]   Because they're the only ones translated into obscure languages.
[01:58:20.000 --> 01:58:22.000]   Let me just try it here.
[01:58:22.000 --> 01:58:23.760]   Translate.
[01:58:23.760 --> 01:58:26.520]   Type the letters A, A, G over and over again,
[01:58:26.520 --> 01:58:28.480]   like 19 times into Google Translate.
[01:58:28.480 --> 01:58:29.800]   OK.
[01:58:29.800 --> 01:58:35.040]   The example I saw in Reddit was Maori and Dog Dog Dog.
[01:58:35.040 --> 01:58:38.640]   But A, G, and what language should I do?
[01:58:38.640 --> 01:58:39.480]   Somali.
[01:58:39.480 --> 01:58:39.960]   Somali.
[01:58:39.960 --> 01:58:42.400]   Just A, G, like just copy paste.
[01:58:42.400 --> 01:58:44.480]   Is that a Somali word?
[01:58:44.480 --> 01:58:45.720]   No.
[01:58:45.720 --> 01:58:48.200]   But if you do that, just like copy paste that like four times.
[01:58:48.200 --> 01:58:49.000]   And then add with it.
[01:58:49.000 --> 01:58:49.200]   OK.
[01:58:49.200 --> 01:58:50.760]   So now--
[01:58:50.760 --> 01:58:52.080]   I didn't put spaces in.
[01:58:52.080 --> 01:58:53.520]   Is that matter?
[01:58:53.520 --> 01:58:53.720]   Let me--
[01:58:53.720 --> 01:58:54.040]   That's OK.
[01:58:54.040 --> 01:58:55.640]   I think you've got--
[01:58:55.640 --> 01:58:58.000]   go in.
[01:58:58.000 --> 01:58:58.280]   Yeah.
[01:58:58.280 --> 01:58:59.280]   So it'll keep--
[01:58:59.280 --> 01:59:01.000]   Now, but I have to change this to--
[01:59:01.000 --> 01:59:03.040]   it says Irish was detected.
[01:59:03.040 --> 01:59:03.880]   Yeah, yeah.
[01:59:03.880 --> 01:59:07.360]   So I want to change this to Somali.
[01:59:07.360 --> 01:59:10.400]   And see, the aggregation of the ages to be aglons
[01:59:10.400 --> 01:59:12.800]   to the ages of the ages of the beginning of the ages of the beginning--
[01:59:12.800 --> 01:59:16.720]   So add on just like a single A to the end of that A, G.
[01:59:16.720 --> 01:59:17.440]   That will change it?
[01:59:17.440 --> 01:59:18.280]   Without the space.
[01:59:18.280 --> 01:59:19.120]   Without the space.
[01:59:19.120 --> 01:59:20.400]   Without the space.
[01:59:20.400 --> 01:59:21.760]   Yeah.
[01:59:21.760 --> 01:59:25.520]   So this is-- maybe I haven't put too many A, G's in.
[01:59:25.520 --> 01:59:26.920]   Let's cut them out a little bit.
[01:59:26.920 --> 01:59:30.680]   Numbers, 420 names, Native Agent Losses, LTO Nation.
[01:59:30.680 --> 01:59:31.600]   It's nonsense.
[01:59:31.600 --> 01:59:33.400]   I mean, I took screenshots on my phone
[01:59:33.400 --> 01:59:37.200]   because they fixed it since then, I bet you, right?
[01:59:37.200 --> 01:59:38.560]   So what we were getting--
[01:59:38.560 --> 01:59:40.880]   and we can show the article on Motherboard,
[01:59:40.880 --> 01:59:42.680]   which shows Dog Dog Dog, which has been--
[01:59:42.680 --> 01:59:46.640]   oh, here's the A, G. So if you put in Dog Dog Dog and Maori,
[01:59:46.640 --> 01:59:49.640]   bunch of times, doomsday clock is three minutes at 12.
[01:59:49.640 --> 01:59:52.680]   We are experiencing characters and dramatic developments
[01:59:52.680 --> 01:59:55.080]   in the world, which indicate we are increasingly approaching
[01:59:55.080 --> 01:59:58.120]   the end of times and Jesus' return.
[01:59:58.120 --> 01:59:59.360]   So here's what I found in the--
[01:59:59.360 --> 02:00:01.280]   As a result, the number of the members of the tribe
[02:00:01.280 --> 02:00:03.640]   of the sons of Gershine were 150,000.
[02:00:03.640 --> 02:00:04.640]   But you know what this is?
[02:00:04.640 --> 02:00:06.680]   I recognize this.
[02:00:06.680 --> 02:00:11.800]   This is very, very common in the way
[02:00:11.800 --> 02:00:15.680]   sometimes artificial intelligence works with stuff.
[02:00:15.680 --> 02:00:18.200]   And there's a name for it, which I can't remember.
[02:00:18.200 --> 02:00:20.560]   But there's a name for the technique, which separates
[02:00:20.560 --> 02:00:22.280]   phrases out.
[02:00:22.280 --> 02:00:24.200]   And it is exactly what you'd expect
[02:00:24.200 --> 02:00:26.680]   to see when you put in nonsense.
[02:00:26.680 --> 02:00:27.200]   My names?
[02:00:27.200 --> 02:00:31.000]   Although, obviously, it was heavily trained on religious texts.
[02:00:31.000 --> 02:00:32.800]   So that's interesting.
[02:00:32.800 --> 02:00:35.720]   And from my vantage point, the interesting piece
[02:00:35.720 --> 02:00:37.720]   of that story beyond like, it's really funny.
[02:00:37.720 --> 02:00:42.200]   If you type in Dog, you get religious texts on the other end.
[02:00:42.200 --> 02:00:44.280]   But what's interesting-- the part of that story
[02:00:44.280 --> 02:00:45.920]   that should have been interesting is,
[02:00:45.920 --> 02:00:48.880]   we don't have a list of ingredients.
[02:00:48.880 --> 02:00:49.360]   So there's--
[02:00:49.360 --> 02:00:51.040]   Well, that's always the issue with AI, right?
[02:00:51.040 --> 02:00:51.760]   Who's training it?
[02:00:51.760 --> 02:00:53.920]   What biases are being introduced?
[02:00:53.920 --> 02:00:54.280]   Unknown.
[02:00:54.280 --> 02:00:56.800]   But even like bias aside, it just highlighted
[02:00:56.800 --> 02:00:58.360]   that maybe we're at a point where it'd
[02:00:58.360 --> 02:01:01.800]   be good to have the sort of list of like a nutritional label,
[02:01:01.800 --> 02:01:02.920]   right?
[02:01:02.920 --> 02:01:05.440]   These are-- they're called Markov chains.
[02:01:05.440 --> 02:01:07.480]   That's what I was trying to remember.
[02:01:07.480 --> 02:01:12.400]   And it's commonly-- in fact, there's a--
[02:01:12.400 --> 02:01:15.320]   I know about this because programmers sometimes
[02:01:15.320 --> 02:01:21.600]   use it to play with as a way of generating real sounding text
[02:01:21.600 --> 02:01:25.000]   from random snips of things.
[02:01:25.000 --> 02:01:28.640]   And there's actually a Twitter feed of Markov chains
[02:01:28.640 --> 02:01:34.800]   using open source texts and things like poetry.
[02:01:34.800 --> 02:01:36.080]   And it's actually quite interesting
[02:01:36.080 --> 02:01:38.360]   because it is pretty stuff that sounds realistic.
[02:01:38.360 --> 02:01:40.200]   But it's really just-- it's almost
[02:01:40.200 --> 02:01:42.320]   as if you cut up a bunch of stuff,
[02:01:42.320 --> 02:01:44.880]   the little slips of paper through it in a hat and pulled it out.
[02:01:44.880 --> 02:01:47.520]   It would sound like-- the whole point of this
[02:01:47.520 --> 02:01:49.040]   is it would sound like real speech,
[02:01:49.040 --> 02:01:49.640]   except it's not.
[02:01:49.640 --> 02:01:50.400]   It's nonsense.
[02:01:50.400 --> 02:01:52.560]   I've used that a couple of times.
[02:01:52.560 --> 02:01:54.720]   It's a great way to write proposals.
[02:01:54.720 --> 02:01:57.400]   I've used that because it's going to stay like--
[02:01:57.400 --> 02:01:59.040]   Yeah, once upon a time I was on a conference
[02:01:59.040 --> 02:02:00.240]   and we were talking-- and we decided
[02:02:00.240 --> 02:02:03.440]   to make up this technology as a joke.
[02:02:03.440 --> 02:02:04.640]   Fiber channel over ethernet.
[02:02:04.640 --> 02:02:05.080]   Fiber channel over ethernet.
[02:02:05.080 --> 02:02:07.080]   Yeah, you just take all those--
[02:02:07.080 --> 02:02:08.480]   those words as marketing terms.
[02:02:08.480 --> 02:02:10.280]   Yeah, we took a bunch of these and just fed them in.
[02:02:10.280 --> 02:02:12.880]   And we got this-- we declared that the new standard was--
[02:02:12.880 --> 02:02:15.760]   and we got journalists calling us to ask us about the technology
[02:02:15.760 --> 02:02:18.200]   and whether it was real and how soon it was going to come along.
[02:02:18.200 --> 02:02:19.920]   And we had a wild hour time.
[02:02:19.920 --> 02:02:20.600]   This is-- this is--
[02:02:20.600 --> 02:02:21.960]   Fiber channel over token ring.
[02:02:21.960 --> 02:02:25.360]   Karen Meyer wrote this as a demonstration of closure
[02:02:25.360 --> 02:02:26.480]   that I was reading the other day.
[02:02:26.480 --> 02:02:28.320]   And she's actually created a Twitter account,
[02:02:28.320 --> 02:02:31.960]   functional e-lear markup mashups of Edward Lear's
[02:02:31.960 --> 02:02:35.320]   nonsense songs and functional programming snippets.
[02:02:35.320 --> 02:02:37.880]   And it makes sense in a weird kind of way.
[02:02:37.880 --> 02:02:38.440]   So that's probably what--
[02:02:38.440 --> 02:02:40.440]   I'd like to have programming once and I'll tell you
[02:02:40.440 --> 02:02:42.040]   it's going to come out like that today.
[02:02:42.040 --> 02:02:44.160]   [LAUGHTER]
[02:02:44.160 --> 02:02:45.800]   What a wonderful noise there'll be.
[02:02:45.800 --> 02:02:48.040]   And at night by the languages type system.
[02:02:48.040 --> 02:02:49.760]   Yes, exactly.
[02:02:49.760 --> 02:02:52.920]   That sounds like a real cut to me.
[02:02:52.920 --> 02:02:55.240]   And when the sieve turned round and round and everyone said,
[02:02:55.240 --> 02:02:57.960]   if we only live, we too will go to see in a sieve.
[02:02:57.960 --> 02:02:59.360]   Mm-hmm.
[02:02:59.360 --> 02:03:00.360]   Uh-huh.
[02:03:00.360 --> 02:03:01.800]   So yeah, so I guess--
[02:03:01.800 --> 02:03:03.400]   it's funny that you picked up on that, Amy,
[02:03:03.400 --> 02:03:05.800]   because I also picked up on that.
[02:03:05.800 --> 02:03:08.400]   And I thought that was really interesting.
[02:03:08.400 --> 02:03:10.000]   No one at Google has stepped forward
[02:03:10.000 --> 02:03:11.280]   to say what it means.
[02:03:11.280 --> 02:03:14.200]   But a professor at Harvard, Andrew Rush, who studies
[02:03:14.200 --> 02:03:16.680]   natural language processing and computer translation,
[02:03:16.680 --> 02:03:20.400]   said normally internal quality filters
[02:03:20.400 --> 02:03:23.720]   would catch this kind of manipulation
[02:03:23.720 --> 02:03:27.240]   if we're intentionally done by, say, editors or disgruntled
[02:03:27.240 --> 02:03:28.760]   Google employees.
[02:03:28.760 --> 02:03:31.360]   He says more likely that the strange translations are
[02:03:31.360 --> 02:03:33.800]   related to a change Google Translate made several years
[02:03:33.800 --> 02:03:36.800]   ago when it started to use neural machine translation.
[02:03:36.800 --> 02:03:39.440]   We've all seen those weird dreamscapes
[02:03:39.440 --> 02:03:43.000]   that neural networks create of artwork and so forth.
[02:03:43.000 --> 02:03:45.840]   This just seems like it's an out effect.
[02:03:45.840 --> 02:03:46.320]   Who cares?
[02:03:46.320 --> 02:03:47.520]   It's a hallucination.
[02:03:47.520 --> 02:03:49.120]   That's what he says.
[02:03:49.120 --> 02:03:52.800]   I guess my point is it says something about optimizing
[02:03:52.800 --> 02:03:54.680]   and quality assurance and control.
[02:03:54.680 --> 02:03:57.040]   And you've got lots of different groups of people
[02:03:57.040 --> 02:03:59.040]   working on different projects.
[02:03:59.040 --> 02:04:00.000]   So silly, can there be--
[02:04:00.000 --> 02:04:01.600]   There's no such thing as quality control.
[02:04:01.600 --> 02:04:03.640]   It's just make it and ship it and damn it.
[02:04:03.640 --> 02:04:06.080]   I mean, but I think we're at a point where we
[02:04:06.080 --> 02:04:10.760]   ought to start consumers and even not people who
[02:04:10.760 --> 02:04:11.560]   are super plugged in.
[02:04:11.560 --> 02:04:15.080]   But like, all of us, it's our data that makes the systems run.
[02:04:15.080 --> 02:04:17.840]   And I think that we ought to at some point--
[02:04:17.840 --> 02:04:19.720]   and just going back to the you for a moment,
[02:04:19.720 --> 02:04:22.680]   I actually think that was the spirit of the GDPR.
[02:04:22.680 --> 02:04:24.560]   I think that that was part of what
[02:04:24.560 --> 02:04:27.920]   was sort of underlying the text.
[02:04:27.920 --> 02:04:31.400]   It just wasn't made explicit in a meaningful way,
[02:04:31.400 --> 02:04:33.400]   maybe because at the time nobody thought to--
[02:04:33.400 --> 02:04:34.640]   The best part of this is that the--
[02:04:34.640 --> 02:04:35.520]   The point of this is that they're going to be called out
[02:04:35.520 --> 02:04:36.560]   looking stupid.
[02:04:36.560 --> 02:04:38.560]   Yeah.
[02:04:38.560 --> 02:04:39.600]   That's it.
[02:04:39.600 --> 02:04:42.480]   Google's response to this is, yeah, you put in nonsense.
[02:04:42.480 --> 02:04:43.560]   You're going to get nonsense out.
[02:04:43.560 --> 02:04:44.240]   What did you want?
[02:04:44.240 --> 02:04:45.240]   Move on.
[02:04:45.240 --> 02:04:47.360]   [LAUGHTER]
[02:04:47.360 --> 02:04:49.040]   I could care.
[02:04:49.040 --> 02:04:51.960]   I guess-- so my job is to look for patterns.
[02:04:51.960 --> 02:04:53.800]   So this happened at the same--
[02:04:53.800 --> 02:04:55.000]   well, but there were a couple of things
[02:04:55.000 --> 02:04:56.800]   that happened at the same time.
[02:04:56.800 --> 02:04:59.440]   So that happened.
[02:04:59.440 --> 02:05:00.240]   Did you guys see you?
[02:05:00.240 --> 02:05:03.320]   It's not like a translation system is a flying car.
[02:05:03.320 --> 02:05:04.440]   If you feed in garbage--
[02:05:04.440 --> 02:05:05.440]   But the same technology--
[02:05:05.440 --> 02:05:06.040]   There's no main event.
[02:05:06.040 --> 02:05:08.400]   --kind of crashing into a mountain.
[02:05:08.400 --> 02:05:09.280]   Hang on for a second.
[02:05:09.280 --> 02:05:12.160]   So that happened at the same time that Move Mirror was
[02:05:12.160 --> 02:05:13.720]   announced by Google.
[02:05:13.720 --> 02:05:19.800]   So this is similar to the face matching fine art program
[02:05:19.800 --> 02:05:22.720]   that launched last year.
[02:05:22.720 --> 02:05:25.600]   So this is now where you can stand in front of your webcam
[02:05:25.600 --> 02:05:29.920]   and move your body around and your body will render as many,
[02:05:29.920 --> 02:05:33.120]   many, like one of 80,000 images.
[02:05:33.120 --> 02:05:34.640]   And they sort of--
[02:05:34.640 --> 02:05:37.080]   It creates like a quick movie of all these--
[02:05:37.080 --> 02:05:38.040]   Carson, I have it here.
[02:05:38.040 --> 02:05:39.200]   Just go to-- yeah.
[02:05:39.200 --> 02:05:39.880]   Yeah.
[02:05:39.880 --> 02:05:41.800]   I'm having a hard time describing it.
[02:05:41.800 --> 02:05:43.840]   So you want to do it?
[02:05:43.840 --> 02:05:45.000]   No, obviously, I don't want to do it.
[02:05:45.000 --> 02:05:46.200]   But my reason that I--
[02:05:46.200 --> 02:05:46.800]   Because I'll do it.
[02:05:46.800 --> 02:05:49.560]   Well, so the question is--
[02:05:49.560 --> 02:05:50.960]   This is a one person experiment.
[02:05:50.960 --> 02:05:53.760]   This is not just about making it so that people
[02:05:53.760 --> 02:05:55.520]   have fun on the internet.
[02:05:55.520 --> 02:05:59.600]   They are collecting information so that their systems can
[02:05:59.600 --> 02:06:01.160]   learn and train in real time.
[02:06:01.160 --> 02:06:05.160]   They need more real time data to capture gesture in motion.
[02:06:05.160 --> 02:06:06.880]   So which I don't have a problem with.
[02:06:06.880 --> 02:06:09.080]   I just wish that it was disclosed in a way.
[02:06:09.080 --> 02:06:10.400]   And I don't have a problem that Google
[02:06:10.400 --> 02:06:16.240]   use the Bible to train because it's easy, right?
[02:06:16.240 --> 02:06:19.240]   That there's structured data and there's a lot of it
[02:06:19.240 --> 02:06:20.440]   in lots of different languages.
[02:06:20.440 --> 02:06:21.800]   So that makes sense.
[02:06:21.800 --> 02:06:24.320]   But let's just explain what's going on here
[02:06:24.320 --> 02:06:26.240]   rather than brushing it off.
[02:06:26.240 --> 02:06:28.080]   Leo, you look great.
[02:06:28.080 --> 02:06:31.400]   I would not have done it.
[02:06:31.400 --> 02:06:33.040]   I keep getting the same four people.
[02:06:33.040 --> 02:06:34.320]   I don't know what I'm doing wrong.
[02:06:34.320 --> 02:06:36.320]   But I keep getting those women sitting in the line.
[02:06:36.320 --> 02:06:38.440]   Is this what Google thinks I look like?
[02:06:38.440 --> 02:06:40.600]   Yay!
[02:06:40.600 --> 02:06:41.760]   All right.
[02:06:41.760 --> 02:06:44.160]   I've done my move mirror, so thank you very much.
[02:06:44.160 --> 02:06:47.200]   Well, and what you've done is you've just created some--
[02:06:47.200 --> 02:06:49.760]   you've been part of the giant mechanical Turk.
[02:06:49.760 --> 02:06:53.160]   That is everybody using Google's cool services and products.
[02:06:53.160 --> 02:06:55.520]   But did I give them anything of value there?
[02:06:55.520 --> 02:06:56.840]   Yeah, you absolutely did.
[02:06:56.840 --> 02:06:58.400]   You're helping train the system.
[02:06:58.400 --> 02:06:59.800]   Now we're going to get to the discussion
[02:06:59.800 --> 02:07:02.400]   where more Google should pay me for my data.
[02:07:02.400 --> 02:07:03.480]   Well, but--
[02:07:03.480 --> 02:07:06.480]   Well, at least I guess the point is just there's
[02:07:06.480 --> 02:07:07.720]   a lot happening.
[02:07:07.720 --> 02:07:10.280]   And actually, Google says here that they're not sending any
[02:07:10.280 --> 02:07:11.440]   images back to the server.
[02:07:11.440 --> 02:07:13.440]   No, no, Google is saying-- so this is important.
[02:07:13.440 --> 02:07:17.200]   The fine print is that they are not saving and sending the
[02:07:17.200 --> 02:07:18.440]   images back.
[02:07:18.440 --> 02:07:21.280]   But machine learning algorithms don't need to store the
[02:07:21.280 --> 02:07:23.560]   information.
[02:07:23.560 --> 02:07:25.880]   They can extract the data from it and save that.
[02:07:25.880 --> 02:07:29.360]   They are sending something back, just not those images.
[02:07:29.360 --> 02:07:32.120]   So again, again, I don't have a problem with this.
[02:07:32.120 --> 02:07:34.720]   But if we look at--
[02:07:34.720 --> 02:07:36.680]   my point is that we have checked out.
[02:07:36.680 --> 02:07:40.160]   We are falling asleep just as the machines are waking up.
[02:07:40.160 --> 02:07:41.560]   Oh, god.
[02:07:41.560 --> 02:07:42.080]   Oh, god.
[02:07:42.080 --> 02:07:43.360]   This is a point.
[02:07:43.360 --> 02:07:46.440]   Like, the little-- wow, Google translates spit out some
[02:07:46.440 --> 02:07:49.240]   weird apocalyptic doomsday cult sentences.
[02:07:49.240 --> 02:07:50.800]   Ha, ha, isn't that funny.
[02:07:50.800 --> 02:07:54.760]   And Google's saying, obviously, this is a glitch.
[02:07:54.760 --> 02:07:56.200]   And when we all move on.
[02:07:56.200 --> 02:07:59.200]   Or Google launching Magic Move and people sort of thinking
[02:07:59.200 --> 02:08:03.080]   it's cool, these things are all connected, is my point.
[02:08:03.080 --> 02:08:05.920]   And they are part of the fabric that we are all in
[02:08:05.920 --> 02:08:09.680]  extricably tied into, literally, physically tied into,
[02:08:09.680 --> 02:08:10.880]   because of our data.
[02:08:10.880 --> 02:08:16.640]   And I just think that we ought to be more aware and awake
[02:08:16.640 --> 02:08:17.560]   to what's happening.
[02:08:17.560 --> 02:08:20.120]   Amy, you're going dark on us again.
[02:08:20.120 --> 02:08:20.720]   Sorry.
[02:08:20.720 --> 02:08:21.080]   Terrified.
[02:08:21.080 --> 02:08:21.920]   I'll perk back up.
[02:08:21.920 --> 02:08:22.960]   No, no, perk up.
[02:08:22.960 --> 02:08:26.240]   I've got something that'll lighten up your day.
[02:08:26.240 --> 02:08:29.320]   The Ring Spotlight Cam.
[02:08:29.320 --> 02:08:30.000]   We're going to take a break.
[02:08:30.000 --> 02:08:32.040]   Come back more with our great panel.
[02:08:32.040 --> 02:08:32.600]   Amy, when--
[02:08:32.600 --> 02:08:33.920]   Because that's motion capture yourself.
[02:08:33.920 --> 02:08:34.760]   Yeah.
[02:08:34.760 --> 02:08:36.120]   But it doesn't go-- well, actually,
[02:08:36.120 --> 02:08:38.000]   it does set it back to the servers.
[02:08:38.000 --> 02:08:41.080]   And-- but not for the same reasons.
[02:08:41.080 --> 02:08:43.160]   And it's also great to have Greg Ferro here
[02:08:43.160 --> 02:08:44.360]   from the Packet Pushers Network.
[02:08:44.360 --> 02:08:46.680]   Our show today brought to you by Ring.
[02:08:46.680 --> 02:08:47.920]   They Ring Video Doorbell.
[02:08:47.920 --> 02:08:49.560]   I've already told you how great this is.
[02:08:49.560 --> 02:08:52.480]   It is a way of keeping an eye on what's going on outside
[02:08:52.480 --> 02:08:54.480]   my front door, whether somebody's ringing the doorbell.
[02:08:54.480 --> 02:08:57.240]   And then it brings it on my phone, no matter where I am in
[02:08:57.240 --> 02:08:58.920]   the world and I could talk to them.
[02:08:58.920 --> 02:09:02.120]   I had on Saturday, yesterday on the radio show, I had a mail
[02:09:02.120 --> 02:09:04.760]   carrier call, a woman who delivers mail.
[02:09:04.760 --> 02:09:06.920]   And she said, I bought a Ring Video Doorbell because
[02:09:06.920 --> 02:09:09.960]   everywhere I go, every time I'm delivering mail,
[02:09:09.960 --> 02:09:11.240]   I see these doorbells.
[02:09:11.240 --> 02:09:14.520]   The other day, I was walked up to deliver a package.
[02:09:14.520 --> 02:09:16.520]   The woman said, I can't talk to you right now.
[02:09:16.520 --> 02:09:17.280]   I'm not here right now.
[02:09:17.280 --> 02:09:18.480]   I'm in Hawaii.
[02:09:18.480 --> 02:09:19.480]   But I'll tell you what.
[02:09:19.480 --> 02:09:21.200]   Give that to my neighbor.
[02:09:21.200 --> 02:09:22.800]   And I'll pick it up when I get back.
[02:09:22.800 --> 02:09:24.640]   And the mail lady was so blown away.
[02:09:24.640 --> 02:09:27.040]   She said, I went out and got a Ring Video Doorbell.
[02:09:27.040 --> 02:09:29.000]   She also mentioned she doesn't have internet access,
[02:09:29.000 --> 02:09:30.080]   so it's not that useful.
[02:09:30.080 --> 02:09:35.280]   But if you have internet access, this is an amazing thing.
[02:09:35.280 --> 02:09:36.640]   You literally get--
[02:09:36.640 --> 02:09:37.320]   I love it.
[02:09:37.320 --> 02:09:39.200]   We know who's on our doorstep.
[02:09:39.200 --> 02:09:40.360]   It actually has motion sensors.
[02:09:40.360 --> 02:09:42.840]   So even if somebody walks around in front of your house,
[02:09:42.840 --> 02:09:45.000]   you set the area that it's monitoring.
[02:09:45.000 --> 02:09:46.240]   You'll get that feedback.
[02:09:46.240 --> 02:09:49.240]   And you can talk to anybody at your front door so you can
[02:09:49.240 --> 02:09:50.040]   say, get away from there.
[02:09:50.040 --> 02:09:52.360]   Or you could say, leave the package behind the bush.
[02:09:52.360 --> 02:09:54.640]   You could say, well, it's great.
[02:09:54.640 --> 02:09:56.320]   You can know when not to answer the door.
[02:09:56.320 --> 02:09:58.000]   How many times is a doorbell ring?
[02:09:58.000 --> 02:09:59.680]   And you want to kind of like peek out through the curtains
[02:09:59.680 --> 02:10:03.000]   and make sure it's not that annoying neighbor.
[02:10:03.000 --> 02:10:04.080]   All the time.
[02:10:04.080 --> 02:10:06.640]   Now I just look at my phone and go,
[02:10:06.640 --> 02:10:08.720]   I can answer that one.
[02:10:08.720 --> 02:10:09.080]   You know what?
[02:10:09.080 --> 02:10:12.160]   You could do you go on and say, I'm not here right now.
[02:10:12.160 --> 02:10:13.560]   Go away.
[02:10:13.560 --> 02:10:16.360]   I'm out of town for the next year.
[02:10:16.360 --> 02:10:18.160]   Now, just like the Ring Video Doorbell,
[02:10:18.160 --> 02:10:19.480]   you got your Ring Floodlight Cam.
[02:10:19.480 --> 02:10:20.600]   It's a motion activated.
[02:10:20.600 --> 02:10:21.240]   Well, it's a floodlight.
[02:10:21.240 --> 02:10:23.320]   You've seen those before the side of the house.
[02:10:23.320 --> 02:10:25.440]   You walk out, the light goes on, so you can empty the trash.
[02:10:25.440 --> 02:10:25.840]   That's nice.
[02:10:25.840 --> 02:10:26.720]   But what if you added?
[02:10:26.720 --> 02:10:27.840]   What if you did this?
[02:10:27.840 --> 02:10:29.320]   First of all, make it LEDs.
[02:10:29.320 --> 02:10:30.360]   So they're never going to burn out.
[02:10:30.360 --> 02:10:30.840]   They're bright.
[02:10:30.840 --> 02:10:31.280]   They're clear.
[02:10:31.280 --> 02:10:32.120]   They're great.
[02:10:32.120 --> 02:10:35.280]   Then add the camera, the speaker, the microphone,
[02:10:35.280 --> 02:10:37.720]   the motion sensor from the Ring Video Doorbell.
[02:10:37.720 --> 02:10:38.960]   Now the light comes on.
[02:10:38.960 --> 02:10:39.880]   You can see who's there.
[02:10:39.880 --> 02:10:42.280]   You get notified no matter where you are anywhere in the world.
[02:10:42.280 --> 02:10:43.040]   You can talk to them.
[02:10:43.040 --> 02:10:44.120]   You can listen to them.
[02:10:44.120 --> 02:10:47.680]   And if it's a bad guy and they don't get out of the way,
[02:10:47.680 --> 02:10:49.960]   you just press a button on your phone.
[02:10:49.960 --> 02:10:53.840]   And 110 decibel alarm goes off and they go running.
[02:10:53.840 --> 02:10:56.080]   This is so great.
[02:10:56.080 --> 02:10:58.240]   We have them all around the house.
[02:10:58.240 --> 02:10:59.080]   It's home security.
[02:10:59.080 --> 02:11:00.800]   We use it to monitor our cats too.
[02:11:00.800 --> 02:11:01.840]   It's good for that.
[02:11:01.840 --> 02:11:03.360]   I like to look at the Twitter feed on Ring
[02:11:03.360 --> 02:11:06.120]   because they have all sorts of stuff.
[02:11:06.120 --> 02:11:09.600]   Here's a three in the morning.
[02:11:09.600 --> 02:11:11.160]   Three in the morning.
[02:11:11.160 --> 02:11:14.960]   This is the video from the Ring Spotlight Cam.
[02:11:14.960 --> 02:11:16.920]   This guy, three in the morning, they're going up to their car.
[02:11:16.920 --> 02:11:18.840]   He's in his car.
[02:11:18.840 --> 02:11:19.680]   And now--
[02:11:19.680 --> 02:11:21.080]   Get out of my car.
[02:11:21.080 --> 02:11:23.280]   He says, get out of my car.
[02:11:23.280 --> 02:11:25.040]   Hey, get out of my car.
[02:11:25.040 --> 02:11:26.080]   I'm calling the cops.
[02:11:26.080 --> 02:11:26.920]   And they go running.
[02:11:26.920 --> 02:11:28.400]   Yeah, that's crazy.
[02:11:28.400 --> 02:11:30.040]   They go running.
[02:11:30.040 --> 02:11:31.520]   Is that not awesome?
[02:11:31.520 --> 02:11:35.080]   If you go to Twitter.com/ring, you can watch these videos.
[02:11:35.080 --> 02:11:36.680]   They're phenomenal.
[02:11:36.680 --> 02:11:38.040]   Stop crying before it happens.
[02:11:38.040 --> 02:11:41.480]   Make your neighborhood safer with Ring.
[02:11:41.480 --> 02:11:43.120]   Ring.com/twit.
[02:11:43.120 --> 02:11:47.800]   You can get up to $150 off a Ring of Security kit.
[02:11:47.800 --> 02:11:49.680]   When you go to Ring.com/twit.
[02:11:49.680 --> 02:11:52.040]   Ring.com/twit.
[02:11:52.040 --> 02:11:55.600]   Save it to $150 on your Ring of Security kit.
[02:11:55.600 --> 02:11:57.680]   One, two, or three Spotlight cams on the Ring video door
[02:11:57.680 --> 02:12:00.880]   bell you get the choice of the original Ring, the new really
[02:12:00.880 --> 02:12:02.800]   sweet Ring with it.
[02:12:02.800 --> 02:12:03.520]   It's all HD.
[02:12:03.520 --> 02:12:06.920]   It's awesome.
[02:12:06.920 --> 02:12:08.760]   OK, we got to cheer everybody up.
[02:12:08.760 --> 02:12:12.680]   We can't leave on a bad note.
[02:12:12.680 --> 02:12:15.480]   Is it good news that Jeff Bezos is now the richest man
[02:12:15.480 --> 02:12:18.960]   who ever walked the earth, $150 billion?
[02:12:18.960 --> 02:12:19.960]   I'm more concerned that we're seeing--
[02:12:19.960 --> 02:12:20.800]   That's a lot of money.
[02:12:20.800 --> 02:12:21.720]   That's a lot of money.
[02:12:21.720 --> 02:12:22.760]   It's also a distortion.
[02:12:22.760 --> 02:12:24.360]   I mean, it's a lot of money.
[02:12:24.360 --> 02:12:25.000]   That's a lot of money.
[02:12:25.000 --> 02:12:26.000]   And it's such a distortion.
[02:12:26.000 --> 02:12:28.240]   It takes us back to Rockefeller era,
[02:12:28.240 --> 02:12:31.600]   where those people had so much money that politics and society
[02:12:31.600 --> 02:12:33.560]   was beholden to the individual.
[02:12:33.560 --> 02:12:35.120]   They could make a decision.
[02:12:35.120 --> 02:12:36.360]   And we've already seen that.
[02:12:36.360 --> 02:12:39.400]   Amazon this week announced they were going to make switches.
[02:12:39.400 --> 02:12:41.840]   And Cisco's share price dropped by $10 billion.
[02:12:41.840 --> 02:12:43.480]   And Amazon's went up by $11 billion.
[02:12:43.480 --> 02:12:46.280]   Amazon buys pill pack, the online pharmacy.
[02:12:46.280 --> 02:12:49.560]   Takes billions of dollars off the table for CVS, Walgreens,
[02:12:49.560 --> 02:12:50.400]   all these--
[02:12:50.400 --> 02:12:51.520]   because it's like, well, if Amazon's
[02:12:51.520 --> 02:12:52.520]   going to do it, you're history.
[02:12:52.520 --> 02:12:54.320]   Those companies fell by $19 billion.
[02:12:54.320 --> 02:12:56.080]   But Amazon's share price went up by $19 billion.
[02:12:56.080 --> 02:12:58.600]   Effectively, they bought that company for nothing.
[02:12:58.600 --> 02:12:59.120]   Oh, wow.
[02:12:59.120 --> 02:13:00.640]   So that goes back to these--
[02:13:00.640 --> 02:13:03.760]   They literally bought that company for zero money.
[02:13:03.760 --> 02:13:05.440]   Not only did they get their share price--
[02:13:05.440 --> 02:13:07.800]   as soon as they announced they were buying a short pill pack,
[02:13:07.800 --> 02:13:09.360]   their share price went up by the value of a buy-out.
[02:13:09.360 --> 02:13:10.960]   Same amount of the beauty squared down.
[02:13:10.960 --> 02:13:12.520]   And more importantly, their competitors
[02:13:12.520 --> 02:13:13.720]   went down by much as well.
[02:13:13.720 --> 02:13:15.560]   So they already have a--
[02:13:15.560 --> 02:13:18.200]   So we're seeing sort of-- this is where, again,
[02:13:18.200 --> 02:13:22.520]   Amazon in US competitive antitrust law,
[02:13:22.520 --> 02:13:24.120]   the consumer is getting a good deal.
[02:13:24.120 --> 02:13:26.320]   They're not being financially disadvantaged.
[02:13:26.320 --> 02:13:28.400]   Therefore, it will continue.
[02:13:28.400 --> 02:13:30.040]   It's more than that in the United States.
[02:13:30.040 --> 02:13:33.360]   So Amazon has very, very smartly and strategically
[02:13:33.360 --> 02:13:37.800]   made acquisitions and across different specters.
[02:13:37.800 --> 02:13:40.840]   So they don't own any controlling stake in any one
[02:13:40.840 --> 02:13:41.520]   industry.
[02:13:41.520 --> 02:13:42.480]   That's right.
[02:13:42.480 --> 02:13:43.960]   Which is brilliant.
[02:13:43.960 --> 02:13:45.200]   But they're also smart.
[02:13:45.200 --> 02:13:47.800]   So they can also sit down in front of any trust courts
[02:13:47.800 --> 02:13:50.800]   and say, we reduce the price on this thing.
[02:13:50.800 --> 02:13:52.360]   We improved market access.
[02:13:52.360 --> 02:13:54.440]   We improved consumer value.
[02:13:54.440 --> 02:13:57.120]   Therefore, it's in the interests of the society
[02:13:57.120 --> 02:13:59.880]   to let us continue to bring this product to market.
[02:13:59.880 --> 02:14:02.240]   There's no way an antitrust will ever stick.
[02:14:02.240 --> 02:14:03.480]   So I was in the city.
[02:14:03.480 --> 02:14:08.120]   I was in New York while getting a lunch
[02:14:08.120 --> 02:14:11.680]   at the little stall where I like to get lunch sometimes.
[02:14:11.680 --> 02:14:12.320]   What kind of food?
[02:14:12.320 --> 02:14:13.120]   Is it halal?
[02:14:13.120 --> 02:14:14.440]   What do you get?
[02:14:14.440 --> 02:14:15.880]   No, it's not halal.
[02:14:15.880 --> 02:14:16.640]   I love halal.
[02:14:16.640 --> 02:14:17.640]   Don't like halal.
[02:14:17.640 --> 02:14:22.080]   It was a super unhealthy Italian hokey.
[02:14:22.080 --> 02:14:23.080]   Oh, yeah, baby.
[02:14:23.080 --> 02:14:24.080]   Oh, man.
[02:14:24.080 --> 02:14:25.880]   Oh, I love those.
[02:14:25.880 --> 02:14:27.840]   That's the thing I miss the most about moving out here
[02:14:27.840 --> 02:14:29.440]   is Italian subs.
[02:14:29.440 --> 02:14:30.680]   There's a really unhealthy--
[02:14:30.680 --> 02:14:31.040]   It's just a hell of a hell of a hell of a hell of a hell
[02:14:31.040 --> 02:14:32.040]   of a hell of a hell of a hell of a hell of a hell of a hell
[02:14:32.040 --> 02:14:33.040]   of a hell of a hell of a hell of a hell of a hell of a hell.
[02:14:33.040 --> 02:14:33.400]   It's just going to have the good stuff.
[02:14:33.400 --> 02:14:33.840]   Where is it?
[02:14:33.840 --> 02:14:35.040]   So it's like in the--
[02:14:35.040 --> 02:14:37.440]   So the Plaza Hotel in Midtown beneath there
[02:14:37.440 --> 02:14:39.640]   was like this food hall sort of place.
[02:14:39.640 --> 02:14:40.440]   So I'm down there.
[02:14:40.440 --> 02:14:41.560]   Oh, I've been in that food hall.
[02:14:41.560 --> 02:14:42.720]   I love that food hall.
[02:14:42.720 --> 02:14:43.480]   Yeah, yeah.
[02:14:43.480 --> 02:14:46.640]   So I'm getting my sandwich in between meetings.
[02:14:46.640 --> 02:14:48.680]   And I look over and I see this person.
[02:14:48.680 --> 02:14:52.000]   And I'm like, holy cow, that's Jeff Bezos.
[02:14:52.000 --> 02:14:52.600]   What the hell is it?
[02:14:52.600 --> 02:14:54.800]   And he was with another person.
[02:14:54.800 --> 02:14:56.560]   But there were no security guards.
[02:14:56.560 --> 02:14:59.360]   And then I'm like, no, that makes absolutely no sense.
[02:14:59.360 --> 02:15:01.480]   Why would he just be hanging out?
[02:15:01.480 --> 02:15:03.040]   Nobody else seems to know who he is.
[02:15:03.040 --> 02:15:03.480]   But I mean--
[02:15:03.480 --> 02:15:05.400]   It's a kidnapping target.
[02:15:05.400 --> 02:15:08.160]   So I'm kind of looking.
[02:15:08.160 --> 02:15:10.200]   I'm trying not to stare at all.
[02:15:10.200 --> 02:15:13.400]   But I'm like, now that's got to be him.
[02:15:13.400 --> 02:15:14.880]   He has a very particular look.
[02:15:14.880 --> 02:15:16.520]   He has a very distinctive--
[02:15:16.520 --> 02:15:18.800]   If you laugh, you're going to know,
[02:15:18.800 --> 02:15:20.320]   because there's nobody laughs like this.
[02:15:20.320 --> 02:15:21.320]   It was him.
[02:15:21.320 --> 02:15:22.720]   It was him.
[02:15:22.720 --> 02:15:23.360]   It was totally him.
[02:15:23.360 --> 02:15:25.800]   But what was interesting is he went around--
[02:15:25.800 --> 02:15:29.480]   so here's the world's richest man ever.
[02:15:29.480 --> 02:15:35.520]   And he's-- I think most people would have absolutely no idea
[02:15:35.520 --> 02:15:36.560]   what he looks like.
[02:15:36.560 --> 02:15:38.960]   If they saw him in person, most people
[02:15:38.960 --> 02:15:40.520]   would have no idea who he is.
[02:15:40.520 --> 02:15:44.520]   And I bet if he went out and asked people who is Jeff Bezos,
[02:15:44.520 --> 02:15:47.760]   most people wouldn't even know what that name is.
[02:15:47.760 --> 02:15:48.240]   So--
[02:15:48.240 --> 02:15:50.600]   I mean, Mark Zuckerberg probably couldn't go out in public.
[02:15:50.600 --> 02:15:52.440]   Bill Gates couldn't go out in public.
[02:15:52.440 --> 02:15:53.760]   No, but Bezos--
[02:15:53.760 --> 02:15:54.040]   But Bezos--
[02:15:54.040 --> 02:15:54.960]   But to me, that's like--
[02:15:54.960 --> 02:15:56.480]   But that's kind of like part of the end.
[02:15:56.480 --> 02:15:56.480]   Was he doing?
[02:15:56.480 --> 02:15:58.520]   Was he getting lunch?
[02:15:58.520 --> 02:15:59.000]   I don't know.
[02:15:59.000 --> 02:16:00.440]   He was wandering around the food court with me.
[02:16:00.440 --> 02:16:00.880]   What the-- no.
[02:16:00.880 --> 02:16:01.880]   No.
[02:16:01.880 --> 02:16:02.380]   No.
[02:16:02.380 --> 02:16:03.080]   That was-- I know.
[02:16:03.080 --> 02:16:03.080]   I know.
[02:16:03.080 --> 02:16:03.600]   I know.
[02:16:03.600 --> 02:16:04.600]   But it was him.
[02:16:04.600 --> 02:16:04.600]   I know.
[02:16:04.600 --> 02:16:05.120]   She was totally him.
[02:16:05.120 --> 02:16:06.480]   He's worth $100 billion.
[02:16:06.480 --> 02:16:09.360]   He should be in a Sherman tank.
[02:16:09.360 --> 02:16:12.440]   I mean, he was worth slightly less at that point.
[02:16:12.440 --> 02:16:12.960]   Oh, that's--
[02:16:12.960 --> 02:16:14.960]   Oh, like $140 billion or something.
[02:16:14.960 --> 02:16:15.640]   Oh, that's--
[02:16:15.640 --> 02:16:16.120]   It's me.
[02:16:16.120 --> 02:16:19.560]   Like, that's such a good window.
[02:16:19.560 --> 02:16:20.880]   Because that's so Amazon, right?
[02:16:20.880 --> 02:16:24.040]   We know very, very little about this company.
[02:16:24.040 --> 02:16:27.400]   We know very little outside of the acquisition announcements
[02:16:27.400 --> 02:16:29.560]   about what the grand plan is.
[02:16:29.560 --> 02:16:31.880]   As somebody who is a--
[02:16:31.880 --> 02:16:35.800]   From my point, Jeff Bezos is probably
[02:16:35.800 --> 02:16:42.920]   our most interesting and most capable long-term future.
[02:16:42.920 --> 02:16:44.960]   I think you're in America.
[02:16:44.960 --> 02:16:46.120]   So I play--
[02:16:46.120 --> 02:16:48.560]   You know, I disagree very strongly.
[02:16:48.560 --> 02:16:50.800]   There's two things that Jeff Bezos has done.
[02:16:50.800 --> 02:16:53.640]   He's done a massive job of avoiding tax
[02:16:53.640 --> 02:16:54.880]   because he's never made a profit
[02:16:54.880 --> 02:16:56.840]   and he's used government taxes effectively
[02:16:56.840 --> 02:16:58.320]   to grow his business.
[02:16:58.320 --> 02:16:58.960]   That's the first thing.
[02:16:58.960 --> 02:16:59.760]   So he's never--
[02:16:59.760 --> 02:17:02.880]   By the way, that's something here in America we celebrate.
[02:17:02.880 --> 02:17:03.760]   Yeah, exactly.
[02:17:03.760 --> 02:17:05.720]   So he's all of the money, all of the growth
[02:17:05.720 --> 02:17:07.200]   that he's managed to achieve out of his company
[02:17:07.200 --> 02:17:09.160]   has been done because he's never paid taxes.
[02:17:09.160 --> 02:17:10.760]   So all of the infrastructure, all the roads,
[02:17:10.760 --> 02:17:12.320]   all the buildings, all the people,
[02:17:12.320 --> 02:17:14.600]   all the social services, Amazon has contributed
[02:17:14.600 --> 02:17:15.440]   effectively to his--
[02:17:15.440 --> 02:17:16.680]   He doesn't pay the people all that well either.
[02:17:16.680 --> 02:17:17.920]   He doesn't pay people very all that well
[02:17:17.920 --> 02:17:19.840]   and they're expected to work extremely hard.
[02:17:19.840 --> 02:17:23.800]   Average life of an Amazon employee is 18 months.
[02:17:23.800 --> 02:17:25.560]   They just don't last for very long.
[02:17:25.560 --> 02:17:27.320]   He's going to run an out of people to employ--
[02:17:27.320 --> 02:17:30.880]   So how is that making him not a long-term strategic thinker
[02:17:30.880 --> 02:17:31.240]   about it?
[02:17:31.240 --> 02:17:34.040]   Because all he's doing is he's an expert at exploiting the system.
[02:17:34.040 --> 02:17:35.920]   All he's managed to do is hack the system.
[02:17:35.920 --> 02:17:38.520]   He hasn't actually created a genuine innovation.
[02:17:38.520 --> 02:17:41.000]   He ability to sell something slightly more efficiently
[02:17:41.000 --> 02:17:42.880]   than it did before is not an innovation.
[02:17:42.880 --> 02:17:43.880]   It's not a disadvantage.
[02:17:43.880 --> 02:17:46.560]   I don't know why he had a small percentage
[02:17:46.560 --> 02:17:49.200]   of every financial transaction in the United States of America.
[02:17:49.200 --> 02:17:49.920]   He's done right extra.
[02:17:49.920 --> 02:17:51.560]   I think it-- they're patents.
[02:17:51.560 --> 02:17:53.360]   I look at some of their crazy experiments
[02:17:53.360 --> 02:17:55.520]   and some of the stuff that they're doing with mines
[02:17:55.520 --> 02:17:56.760]   and looking at sensor data.
[02:17:56.760 --> 02:17:59.000]   I think Amazon has a--
[02:17:59.000 --> 02:18:02.200]   I think he is brilliant and I think he has a long-term plan
[02:18:02.200 --> 02:18:04.480]   that he is executing on whether or not that's
[02:18:04.480 --> 02:18:05.280]   apparent to all of us.
[02:18:05.280 --> 02:18:06.560]   It's not an innovation plan.
[02:18:06.560 --> 02:18:08.400]   It's an exploitation plan.
[02:18:08.400 --> 02:18:09.320]   I don't know.
[02:18:09.320 --> 02:18:12.360]   All his company sucks in vast amounts of open source data
[02:18:12.360 --> 02:18:16.280]   and then spits it back at people as AWS to make a profit
[02:18:16.280 --> 02:18:19.360]   but gives nothing back to open source.
[02:18:19.360 --> 02:18:23.880]   So it's not like he's not genuinely contributing to society.
[02:18:23.880 --> 02:18:25.160]   He's taking from society.
[02:18:25.160 --> 02:18:26.400]   Well, how about Elon Musk?
[02:18:26.400 --> 02:18:27.960]   Is he contributing to society?
[02:18:27.960 --> 02:18:30.080]   He's trying and he's at least he's a lot more transparent
[02:18:30.080 --> 02:18:32.120]   than Amazon is.
[02:18:32.120 --> 02:18:33.400]   Here's my headline of the week.
[02:18:33.400 --> 02:18:35.880]   This is the best headline for the whole week.
[02:18:35.880 --> 02:18:38.920]   Tesla boss Elon Musk has reached an agreement
[02:18:38.920 --> 02:18:42.000]   with a Colorado potter who accused the billionaire
[02:18:42.000 --> 02:18:44.800]   of using his farting unicorn without permission.
[02:18:44.800 --> 02:18:48.080]   Yeah, but they looked pretty similar.
[02:18:48.080 --> 02:18:49.640]   Do you see it?
[02:18:49.640 --> 02:18:52.360]   You see one farting unicorn, you see them all.
[02:18:52.360 --> 02:18:54.960]   I mean, it's a ridiculous farting unicorn.
[02:18:54.960 --> 02:18:55.800]   I mean, that's--
[02:18:55.800 --> 02:19:00.160]   It's pretty darn close to the original, if you ask me.
[02:19:00.160 --> 02:19:02.480]   Well, apparently there's been a settlement.
[02:19:05.080 --> 02:19:06.800]   I'm happy to report that we have reached an agreement
[02:19:06.800 --> 02:19:08.560]   with Tesla that resolves our issues in a way
[02:19:08.560 --> 02:19:10.880]   that everyone feels good about it says the potter.
[02:19:10.880 --> 02:19:13.040]   It's clear there were some misunderstandings
[02:19:13.040 --> 02:19:14.400]   that led to this escalating.
[02:19:14.400 --> 02:19:15.840]   I'm just glad that everything's been cleared up.
[02:19:15.840 --> 02:19:17.400]   I've always been a Tesla fan
[02:19:17.400 --> 02:19:19.320]   and I'm looking back to getting pots,
[02:19:19.320 --> 02:19:21.880]   getting back to making pots and selling them in,
[02:19:21.880 --> 02:19:24.080]   including a link, my online store.
[02:19:24.080 --> 02:19:26.920]   - I don't know, Elon Musk is his PR people
[02:19:26.920 --> 02:19:30.640]   or lack of PR people is certainly caused him some headaches
[02:19:30.640 --> 02:19:31.640]   over the past couple of weeks.
[02:19:31.640 --> 02:19:33.880]   - Yeah, we talked about it last week.
[02:19:33.880 --> 02:19:35.040]   Just bizarre.
[02:19:35.040 --> 02:19:37.200]   You know what?
[02:19:37.200 --> 02:19:40.000]   You got the right idea.
[02:19:40.000 --> 02:19:42.360]   What was his explanation for the peto comment?
[02:19:42.360 --> 02:19:43.200]   - Yeah.
[02:19:43.200 --> 02:19:44.840]   That part, I didn't hear.
[02:19:44.840 --> 02:19:47.400]   I heard the explanation for the submarine
[02:19:47.400 --> 02:19:48.560]   and the building was submarine.
[02:19:48.560 --> 02:19:49.440]   - Oh, I think that's fine.
[02:19:49.440 --> 02:19:50.680]   He thought it was a good idea.
[02:19:50.680 --> 02:19:53.600]   When he leapt into action, he did something.
[02:19:53.600 --> 02:19:54.440]   You can't knock it.
[02:19:54.440 --> 02:19:56.880]   - Yeah, the comment though was, I don't know.
[02:19:56.880 --> 02:19:58.280]   - The peto comment, I don't know.
[02:19:58.280 --> 02:20:02.360]   - The whole thing, like at what point does somebody go over
[02:20:02.360 --> 02:20:05.640]   and offer something to, there are teams of experts
[02:20:05.640 --> 02:20:09.640]   in Thailand, well-trained, highly expert cave divers
[02:20:09.640 --> 02:20:11.360]   who know exactly what they're doing.
[02:20:11.360 --> 02:20:13.680]   At exactly what point did they ask him for help?
[02:20:13.680 --> 02:20:14.840]   Or exactly what point?
[02:20:14.840 --> 02:20:17.240]   - They didn't, he just said, I got a submarine.
[02:20:17.240 --> 02:20:18.880]   - Yep, now he told, he said publicly.
[02:20:18.880 --> 02:20:20.280]   - So he made a judge, I mean, he's been in that he asked
[02:20:20.280 --> 02:20:21.120]   for help.
[02:20:21.120 --> 02:20:21.960]   - Well, the company did ask him.
[02:20:21.960 --> 02:20:23.080]   - The people there are incompetent
[02:20:23.080 --> 02:20:24.080]   and he should leap into action.
[02:20:24.080 --> 02:20:25.160]   - No, he was asked.
[02:20:25.160 --> 02:20:26.000]   He was asked.
[02:20:26.000 --> 02:20:26.840]   - By who?
[02:20:26.840 --> 02:20:27.760]   - Well, he said that he was asked.
[02:20:27.760 --> 02:20:29.640]   - Yes, he said he was asked, right?
[02:20:29.640 --> 02:20:30.720]   There are people involved.
[02:20:30.720 --> 02:20:32.120]   They flew in teams of divers.
[02:20:32.120 --> 02:20:36.120]   They had over 80 divers, highly trained expert cave divers
[02:20:36.120 --> 02:20:38.080]   on the spot, right?
[02:20:38.080 --> 02:20:40.120]   Moving in and out of those caves, they did tie.
[02:20:40.120 --> 02:20:41.360]   - I think it's a simple thing.
[02:20:41.360 --> 02:20:42.480]   - It's a simple thing.
[02:20:42.480 --> 02:20:43.440]   - Yeah, it's a simple thing.
[02:20:43.440 --> 02:20:46.080]   - The problem is Twitter, just get off Twitter.
[02:20:46.080 --> 02:20:48.000]   You keep saying Twitter's good, Amy Webb.
[02:20:48.000 --> 02:20:48.960]   Have you changed your tune?
[02:20:48.960 --> 02:20:50.280]   - What are you talking about?
[02:20:50.280 --> 02:20:51.440]   (laughing)
[02:20:51.440 --> 02:20:52.560]   She's selling books.
[02:20:52.560 --> 02:20:53.560]   - No, you don't like Twitter.
[02:20:53.560 --> 02:20:54.400]   - That is my Twitter.
[02:20:54.400 --> 02:20:55.240]   - That is my tag.
[02:20:55.240 --> 02:20:57.320]   - That is absolutely misinformation.
[02:20:57.320 --> 02:20:58.160]   - Fake news.
[02:20:58.160 --> 02:20:59.480]   - That is fake news.
[02:20:59.480 --> 02:21:01.360]   - I've literally never said Facebook is good.
[02:21:01.360 --> 02:21:04.440]   In fact, I have almost completely stopped using Twitter
[02:21:04.440 --> 02:21:06.400]   over the past several weeks.
[02:21:06.400 --> 02:21:07.800]   I went completely off of the media.
[02:21:07.800 --> 02:21:09.840]   - You and me and Maggie Haberman, like this.
[02:21:09.840 --> 02:21:12.120]   - And I just don't see a point in coming back.
[02:21:12.120 --> 02:21:13.440]   - No more Twitter.
[02:21:13.440 --> 02:21:15.880]   Twitter makes people do stupid things.
[02:21:15.880 --> 02:21:16.880]   - Yeah.
[02:21:16.880 --> 02:21:18.280]   - I just want to think,
[02:21:18.280 --> 02:21:20.280]   the next thing, Silicon Valley,
[02:21:20.280 --> 02:21:22.080]   a lot of people in Silicon Valley think
[02:21:22.080 --> 02:21:23.680]   they have solutions for the entire world.
[02:21:23.680 --> 02:21:26.760]   And I sort of see what Elon did with the tie cave thing
[02:21:26.760 --> 02:21:28.400]   as sort of a simple thing. - It's very Silicon Valley.
[02:21:28.400 --> 02:21:29.560]   - It is very Silicon Valley.
[02:21:29.560 --> 02:21:31.640]   - I've done these things. - The smartest man in the room.
[02:21:31.640 --> 02:21:32.960]   - Smartest man in the room, I can help.
[02:21:32.960 --> 02:21:34.520]   - So I'm going to go there and make it better.
[02:21:34.520 --> 02:21:36.360]   - You can't knock him for trying to help.
[02:21:36.360 --> 02:21:38.200]   It's not like he got actively gotten away.
[02:21:38.200 --> 02:21:39.040]   - No, he's not.
[02:21:39.040 --> 02:21:40.640]   - You don't go and help unless you ask.
[02:21:40.640 --> 02:21:42.800]   I mean, the tie government, if nobody else is helping.
[02:21:42.800 --> 02:21:44.240]   - If somebody falls on the street,
[02:21:44.240 --> 02:21:45.560]   you go and help him up.
[02:21:45.560 --> 02:21:46.400]   - Yeah.
[02:21:46.400 --> 02:21:47.840]   But if there's 20 people there helping,
[02:21:47.840 --> 02:21:49.440]   you don't go in and help him up.
[02:21:49.440 --> 02:21:51.080]   - Well, you don't nudge other people out of the way.
[02:21:51.080 --> 02:21:52.520]   So let me in. - And that's my point,
[02:21:52.520 --> 02:21:54.200]   exactly, that's exactly what he did.
[02:21:54.200 --> 02:21:55.040]   - You think he did that?
[02:21:55.040 --> 02:21:55.880]   - Here's a better question.
[02:21:55.880 --> 02:22:00.200]   - There's no problem. - Let me ask you a question.
[02:22:00.200 --> 02:22:04.120]   - If you had done all of this and made all of the preparations
[02:22:04.120 --> 02:22:06.480]   without announcing an even on Twitter
[02:22:06.480 --> 02:22:10.440]   and had shown up and had contributed to helping everybody out,
[02:22:10.440 --> 02:22:12.360]   would we have the same negative reaction?
[02:22:12.360 --> 02:22:13.480]   And my answer is no.
[02:22:13.480 --> 02:22:15.360]   I think the problem that we have
[02:22:15.360 --> 02:22:18.040]   is that we all somehow feel compelled
[02:22:18.040 --> 02:22:21.480]   to announce our, you know, to announce our philanthropy.
[02:22:21.480 --> 02:22:23.440]   - And where do you announce that?
[02:22:23.440 --> 02:22:25.440]   - On Twitter. - On Twitter, right.
[02:22:25.440 --> 02:22:28.600]   - So Twitter is the four-chan of this generation.
[02:22:28.600 --> 02:22:29.440]   It is a baby. - It is.
[02:22:29.440 --> 02:22:31.120]   Let's be that way.
[02:22:31.120 --> 02:22:32.240]   - It's a bane.
[02:22:32.240 --> 02:22:34.560]   - I don't have that problem with Twitter, but then I'm not--
[02:22:34.560 --> 02:22:35.400]   - You will.
[02:22:35.400 --> 02:22:37.400]   - I'm not volunteering to help people that don't need helping.
[02:22:37.400 --> 02:22:39.440]   - You're gonna say something on Twitter, you regret.
[02:22:39.440 --> 02:22:40.480]   I promise you.
[02:22:40.480 --> 02:22:43.720]   Maybe tonight. - I mean, it's just,
[02:22:43.720 --> 02:22:45.560]   there's a compulsion, you know?
[02:22:45.560 --> 02:22:48.360]   So again, like, because, so we have to ask ourselves,
[02:22:48.360 --> 02:22:51.800]   'cause a lot of our conversation today has been around
[02:22:51.800 --> 02:22:56.240]   the evil, like the people, the sort of know-it-alls
[02:22:56.240 --> 02:22:59.640]   of Silicon Valley and them trying to skirt rules
[02:22:59.640 --> 02:23:00.480]   or whatever.
[02:23:00.480 --> 02:23:03.960]   And again, I just come back to, I don't think,
[02:23:03.960 --> 02:23:07.440]   I think a lot of these people and a lot of these companies
[02:23:07.440 --> 02:23:09.200]   have our best interests at heart.
[02:23:09.200 --> 02:23:10.240]   I really do. - Yes.
[02:23:10.240 --> 02:23:12.400]   - I don't think they're our folks out there.
[02:23:12.400 --> 02:23:13.240]   - This is a big question.
[02:23:13.240 --> 02:23:14.840]   - I don't, Elon Musk is done with SpaceX
[02:23:14.840 --> 02:23:16.880]   and with Tesla and Solar TV.
[02:23:16.880 --> 02:23:19.360]   - And I think we wouldn't be driving electric vehicles
[02:23:19.360 --> 02:23:20.960]   if Tesla hadn't made them cool again.
[02:23:20.960 --> 02:23:23.800]   - No question, I am not attacking the man personally.
[02:23:23.800 --> 02:23:25.400]   I'm saying in this particular case,
[02:23:25.400 --> 02:23:28.320]   he overstepped the mark and he should be criticized for that.
[02:23:28.320 --> 02:23:29.800]   He was not welcomed.
[02:23:29.800 --> 02:23:30.800]   He was not invited.
[02:23:30.800 --> 02:23:32.760]   The Thai government was in action.
[02:23:32.760 --> 02:23:35.840]   It had brought in dozens of divers and experts
[02:23:35.840 --> 02:23:38.000]   from around the world to help it with the rescue.
[02:23:38.000 --> 02:23:39.720]   It was not doing nothing. - It was missing context.
[02:23:39.720 --> 02:23:41.160]   - Not needed.
[02:23:41.160 --> 02:23:43.040]   - We're missing context and this is why,
[02:23:43.040 --> 02:23:45.520]   so again, like, so he's got some kids.
[02:23:45.520 --> 02:23:46.600]   We have no idea.
[02:23:46.600 --> 02:23:48.640]   It's entirely plausible that one of his kids
[02:23:48.640 --> 02:23:50.120]   almost drowned at one point.
[02:23:50.120 --> 02:23:51.400]   - I'm a mom, right?
[02:23:51.400 --> 02:23:54.240]   And I'm a mom of a young kid
[02:23:54.240 --> 02:23:55.920]   and hearing what was happening,
[02:23:55.920 --> 02:23:57.200]   there was a part of me that was like,
[02:23:57.200 --> 02:23:58.240]   ah, you know, is there a--
[02:23:58.240 --> 02:24:00.400]   - Yeah, but you tried to fly to another country
[02:24:00.400 --> 02:24:05.400]   and undermine a government-led solution to a major issue
[02:24:05.400 --> 02:24:07.080]   and pretend that you're-- - I know, I think--
[02:24:07.080 --> 02:24:08.680]   - That's not acceptable behavior.
[02:24:08.680 --> 02:24:10.520]   That's not an adult in action.
[02:24:10.520 --> 02:24:12.040]   That's a child throwing a tent.
[02:24:12.040 --> 02:24:14.280]   - And not here to defend Elon Musk.
[02:24:14.280 --> 02:24:15.560]   All I'm saying is--
[02:24:15.560 --> 02:24:16.560]   - I'm not attacking him.
[02:24:16.560 --> 02:24:19.400]   - I'm saying that is not mature behavior.
[02:24:19.400 --> 02:24:22.320]   - Sir, so what I'm saying is that a lot of this
[02:24:22.320 --> 02:24:26.040]   is playing out now over on Twitter.
[02:24:26.040 --> 02:24:28.880]   And what I was just trying to say is,
[02:24:28.880 --> 02:24:31.600]   it's entirely plausible 'cause we weren't there
[02:24:31.600 --> 02:24:34.960]   that he was asked for some help
[02:24:34.960 --> 02:24:37.800]   because of, you know, he was already set up to do some of that.
[02:24:37.800 --> 02:24:39.160]   - He tweeted that, although I noticed that,
[02:24:39.160 --> 02:24:40.720]   that tweet has now been deleted.
[02:24:40.720 --> 02:24:42.640]   So he said he was repeating-- - Who knows?
[02:24:42.640 --> 02:24:45.000]   The problem is that stuff gets hit on Twitter.
[02:24:45.000 --> 02:24:47.200]   - And then the rest of us spend our time.
[02:24:47.200 --> 02:24:50.680]   - Who asked him, the secretary of this junior,
[02:24:50.680 --> 02:24:52.760]   you know, one of his buddies who happened to be--
[02:24:52.760 --> 02:24:54.080]   - You know what, great wait a minute.
[02:24:54.080 --> 02:24:54.920]   - A holiday's like, you know--
[02:24:54.920 --> 02:24:56.240]   - You might be right, you might be wrong.
[02:24:56.240 --> 02:24:57.640]   We don't know, that's the real--
[02:24:57.640 --> 02:24:59.240]   - Look, he's a CEO out of our business--
[02:24:59.240 --> 02:25:00.400]   - And that's my really health festival companies.
[02:25:00.400 --> 02:25:02.120]   It would have to talk to the-- - Well, entitled--
[02:25:02.120 --> 02:25:03.640]   - Comments on it. - Yeah.
[02:25:03.640 --> 02:25:07.680]   - And it's the vitriol that comes out in that commentary
[02:25:07.680 --> 02:25:09.920]   which is creating this like environment
[02:25:09.920 --> 02:25:12.680]   where we're having these stupid arguments with each other.
[02:25:12.680 --> 02:25:14.860]   - He did, there is this post,
[02:25:16.400 --> 02:25:18.280]   a request from Richard Stanton,
[02:25:18.280 --> 02:25:19.520]   we're worried about the smallest lab,
[02:25:19.520 --> 02:25:21.760]   please keep working on the capsule details.
[02:25:21.760 --> 02:25:26.760]   This is Elon's, you know, apparent, you know, request.
[02:25:26.760 --> 02:25:29.640]   So, I don't know.
[02:25:29.640 --> 02:25:32.480]   You know, I don't think we have to weigh in on that.
[02:25:32.480 --> 02:25:33.320]   - No.
[02:25:33.320 --> 02:25:35.040]   - I like Elon, I like what they're doing.
[02:25:35.040 --> 02:25:38.560]   It cost the pedal comment, cost Tesla, that and other things.
[02:25:38.560 --> 02:25:40.400]   Two billion dollars in market value.
[02:25:40.400 --> 02:25:42.760]   It was a very expensive comment.
[02:25:42.760 --> 02:25:43.960]   Get off Twitter!
[02:25:43.960 --> 02:25:45.000]   (laughing)
[02:25:45.000 --> 02:25:47.920]   - I think that's the best advice
[02:25:47.920 --> 02:25:49.880]   from this entire episode is just like,
[02:25:49.880 --> 02:25:51.480]   we all need to get off Twitter.
[02:25:51.480 --> 02:25:52.320]   - Get off Twitter. - Get off Twitter.
[02:25:52.320 --> 02:25:53.160]   - A lot of value out of Twitter, right?
[02:25:53.160 --> 02:25:54.160]   - Get off the Pokemon Go,
[02:25:54.160 --> 02:25:55.000]   'cause you can't-- - The two-inch sword
[02:25:55.000 --> 02:25:57.120]   or anything. - I don't get the value out
[02:25:57.120 --> 02:25:59.240]   that I used to and I'm one of the earliest users
[02:25:59.240 --> 02:26:01.840]   and I was a power user for a long time
[02:26:01.840 --> 02:26:05.080]   and I just don't get the value out of it that I used to.
[02:26:05.080 --> 02:26:07.720]   - Okay, and the flip side of that is he gets a lot of value
[02:26:07.720 --> 02:26:09.560]   and a lot of exposure, he's got a lot of followers,
[02:26:09.560 --> 02:26:11.440]   he tweets something out and he can sell a hell
[02:26:11.440 --> 02:26:13.000]   of a lot of flying throws for zero effort.
[02:26:13.000 --> 02:26:14.520]   - You know what really is interesting?
[02:26:14.520 --> 02:26:15.360]   - What's wrong with that?
[02:26:15.360 --> 02:26:18.360]   - Is that the thing that people are most willing
[02:26:18.360 --> 02:26:20.920]   to argue about and vociferously defends,
[02:26:20.920 --> 02:26:23.560]   they're always the smallest things,
[02:26:23.560 --> 02:26:25.600]   the things that are the least important.
[02:26:25.600 --> 02:26:28.320]   And I think that that's something we gotta keep in mind.
[02:26:28.320 --> 02:26:30.520]   This doesn't really matter in the scheme of things.
[02:26:30.520 --> 02:26:31.360]   - Doesn't.
[02:26:31.360 --> 02:26:36.280]   - Ladies and gentlemen, I'm gonna call this one.
[02:26:36.280 --> 02:26:37.840]   (laughing)
[02:26:37.840 --> 02:26:38.680]   - What are you gonna call it?
[02:26:38.680 --> 02:26:42.000]   - I don't know yet, but I'll think of something.
[02:26:42.000 --> 02:26:43.640]   Amy Webb, God bless you,
[02:26:43.640 --> 02:26:45.400]   I can't wait to read the new book,
[02:26:45.400 --> 02:26:46.480]   gotta read the old book.
[02:26:46.480 --> 02:26:49.200]   First though, folks, the signals are talking.
[02:26:49.200 --> 02:26:53.360]   Amy Webb.io, she is keeping an eye on the future
[02:26:53.360 --> 02:26:54.560]   so we don't have to.
[02:26:54.560 --> 02:26:58.120]   Thank you, Amy, for being here.
[02:26:58.120 --> 02:26:59.920]   - Thank you. - You're just always fun.
[02:26:59.920 --> 02:27:01.240]   - You too, Greg Ferro.
[02:27:01.240 --> 02:27:02.960]   Man, I love both of you.
[02:27:02.960 --> 02:27:05.520]   I knew, see aren't we smart?
[02:27:05.520 --> 02:27:06.480]   Carson, you were right.
[02:27:06.480 --> 02:27:09.200]   Don't put another person on this table.
[02:27:09.200 --> 02:27:11.640]   They'll never get a word in edgewise.
[02:27:11.640 --> 02:27:12.480]   - Sorry about that.
[02:27:12.480 --> 02:27:13.320]   (laughing)
[02:27:13.320 --> 02:27:14.600]   - You're fine to debate with.
[02:27:14.600 --> 02:27:15.960]   No, this was good. - All of us, baby.
[02:27:15.960 --> 02:27:16.840]   - I learned a ton.
[02:27:16.840 --> 02:27:19.680]   I have a notebook full of ideas after today.
[02:27:19.680 --> 02:27:20.520]   This was awesome.
[02:27:20.520 --> 02:27:21.880]   - E-A-6, okay?
[02:27:21.880 --> 02:27:23.440]   E-A-6.
[02:27:23.440 --> 02:27:25.800]   Greg Ferro, you can hear more Greg Ferro
[02:27:25.800 --> 02:27:28.280]   on his podcast at the Packet Pushers Network,
[02:27:28.280 --> 02:27:29.720]   or follow him on Twitter.
[02:27:29.720 --> 02:27:32.000]   He apparently thinks it's a good thing.
[02:27:32.000 --> 02:27:34.120]   Ethereal mind is his Twitter handle.
[02:27:34.120 --> 02:27:36.000]   - It's not, it's snarky, but you know?
[02:27:36.000 --> 02:27:37.760]   - Snarky, but delicious.
[02:27:37.760 --> 02:27:39.720]   Just like all of Twitter.
[02:27:39.720 --> 02:27:40.720]   You guys are so great.
[02:27:40.720 --> 02:27:41.600]   - There's your title.
[02:27:41.600 --> 02:27:43.920]   - Yes, snarky, but delicious.
[02:27:43.920 --> 02:27:45.000]   - There you go.
[02:27:45.000 --> 02:27:47.320]   - Oh, no, there were so many good titles
[02:27:47.320 --> 02:27:48.680]   floating across this one, I'll tell you.
[02:27:48.680 --> 02:27:50.440]   This is gonna be a hard one to pick.
[02:27:50.440 --> 02:27:51.560]   Thank you so much for being here.
[02:27:51.560 --> 02:27:53.840]   We do this week in tech every Sunday afternoon
[02:27:53.840 --> 02:27:54.680]   in 3 p.m. Pacific.
[02:27:54.680 --> 02:27:56.680]   You actually could watch it live if you wanted.
[02:27:56.680 --> 02:27:59.360]   6 p.m. Eastern time, 2,200 UTC.
[02:27:59.360 --> 02:28:00.200]   Couple of ways to do that.
[02:28:00.200 --> 02:28:01.040]   You could be in the studio.
[02:28:01.040 --> 02:28:03.760]   I had a great studio audience put up with our snarky,
[02:28:03.760 --> 02:28:07.480]   but delicious show for the last two and a half hours.
[02:28:07.480 --> 02:28:09.800]   It's okay, gotta warn you, it's a long haul,
[02:28:09.800 --> 02:28:10.920]   but you can go to the bathroom.
[02:28:10.920 --> 02:28:11.920]   We don't lock in.
[02:28:11.920 --> 02:28:12.760]   - Yeah.
[02:28:12.760 --> 02:28:14.880]   - Now we allow you to go to the bathroom.
[02:28:14.880 --> 02:28:16.360]   Just email tickets@twit.tv
[02:28:16.360 --> 02:28:18.360]   so we can make sure we have a chair for you.
[02:28:18.360 --> 02:28:20.200]   You can also watch the live stream,
[02:28:20.200 --> 02:28:21.920]   twit.tv/live.
[02:28:21.920 --> 02:28:23.360]   There's live audio and video there
[02:28:23.360 --> 02:28:25.960]   from a variety of sources you pick the source you like.
[02:28:25.960 --> 02:28:27.720]   You can hum along.
[02:28:27.720 --> 02:28:28.680]   You can also, if you're gonna do that,
[02:28:28.680 --> 02:28:30.080]   you might as well be in the chatroom too,
[02:28:30.080 --> 02:28:32.320]   'cause that's where the fun kids are.
[02:28:32.320 --> 02:28:33.480]   They are great.
[02:28:33.480 --> 02:28:35.280]   In fact, some of the best lines from the show today
[02:28:35.280 --> 02:28:38.960]   that I didn't use, 'cause games in the chatroom.
[02:28:38.960 --> 02:28:40.720]   Just great stuff in here.
[02:28:40.720 --> 02:28:45.200]   IRC.twit.tv.
[02:28:45.200 --> 02:28:49.040]   IRC.twit.tv.
[02:28:49.040 --> 02:28:52.160]   If you can't watch live, you can download shows,
[02:28:52.160 --> 02:28:53.960]   audio or video from twit.tv.
[02:28:53.960 --> 02:28:54.800]   That's our website.
[02:28:54.800 --> 02:28:56.920]   Every show's there on demand.
[02:28:56.920 --> 02:28:59.280]   Subscribe in your favorite podcast application
[02:28:59.280 --> 02:29:01.560]   so you get it the minute it's available every week.
[02:29:01.560 --> 02:29:04.120]   That way you'll have it for your Monday commute.
[02:29:04.120 --> 02:29:06.720]   Or, you know what I love more and more.
[02:29:06.720 --> 02:29:08.640]   I feel like I listened to more podcasts now
[02:29:08.640 --> 02:29:12.480]   than ever before thanks to these voice assistants.
[02:29:12.480 --> 02:29:14.560]   You can say, "Echo, listen to Twit."
[02:29:14.560 --> 02:29:16.240]   Or, actually I think for this one,
[02:29:16.240 --> 02:29:18.640]   you have to say, "Listen to this weekend tech."
[02:29:18.640 --> 02:29:20.760]   Or, you could do it with your Google Home
[02:29:20.760 --> 02:29:25.760]   or your Amazon or your Siri or your whatever.
[02:29:25.760 --> 02:29:28.080]   Just you could be in an empty room
[02:29:28.080 --> 02:29:30.440]   and just shout it and see if it plays.
[02:29:30.440 --> 02:29:32.240]   Listen to this weekend tech.
[02:29:32.240 --> 02:29:33.400]   Thank you for listening.
[02:29:33.400 --> 02:29:34.760]   Thank you for being here.
[02:29:34.760 --> 02:29:35.640]   We'll see you next week.
[02:29:35.640 --> 02:29:36.480]   Another Twit.
[02:29:36.480 --> 02:29:37.800]   - This is amazing.
[02:29:37.800 --> 02:29:40.380]   (upbeat music)
[02:29:40.380 --> 02:29:42.960]   (upbeat music)
[02:29:42.960 --> 02:29:46.000]   Do it in the tweet, baby. Do it in the tweet.
[02:29:46.000 --> 02:29:47.840]   Alright. Do it in the tweet.

