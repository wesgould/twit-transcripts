;FFMETADATA1
title=Laundry Folding as a Service
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=505
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.320]   It's time for Twig this week at Google Jeff's here.
[00:00:02.320 --> 00:00:03.320]   Stacey's here.
[00:00:03.320 --> 00:00:04.640]   We're all back together again.
[00:00:04.640 --> 00:00:06.960]   We're going to talk about Google I/O, what to expect.
[00:00:06.960 --> 00:00:09.560]   Google's undersea cable network.
[00:00:09.560 --> 00:00:12.800]   And why Facebook asked you for your email password
[00:00:12.800 --> 00:00:14.320]   when you first joined?
[00:00:14.320 --> 00:00:16.280]   It's all coming up next on Twig.
[00:00:16.280 --> 00:00:21.440]   Netcast you love.
[00:00:21.440 --> 00:00:22.840]   From people you trust.
[00:00:22.840 --> 00:00:27.880]   This is Twig.
[00:00:27.880 --> 00:00:34.880]   This week in Google, episode 505, recorded Wednesday, April 24, 2019.
[00:00:34.880 --> 00:00:36.880]   Bondry folding as a service.
[00:00:36.880 --> 00:00:39.880]   This week in Google is brought to you by Envoy.
[00:00:39.880 --> 00:00:44.880]   Envoy builds beautiful modern software to help businesses elevate the physical workplace experience.
[00:00:44.880 --> 00:00:49.880]   With Envoy visitors, you can greet guests with a welcoming sleek iPad service.
[00:00:49.880 --> 00:00:52.880]   Envoy is brought to you by Envoy.
[00:00:52.880 --> 00:00:56.880]   Envoy builds beautiful modern software to help businesses elevate the physical workplace experience.
[00:00:56.880 --> 00:01:01.880]   This week, iPad sign-in app while still protecting your people, property and ideas.
[00:01:01.880 --> 00:01:05.880]   Start your free trial today at Envoy.com/twig.
[00:01:05.880 --> 00:01:09.880]   It's time for Twig this week in Google, the show where we talk about the ladies from Google.
[00:01:09.880 --> 00:01:11.880]   And man, the gang is back.
[00:01:11.880 --> 00:01:16.880]   I'm so happy to welcome Stacey Higginbotham back to our microphones.
[00:01:16.880 --> 00:01:17.880]   We missed you.
[00:01:17.880 --> 00:01:20.880]   She's in her temporary location.
[00:01:20.880 --> 00:01:21.880]   That's right.
[00:01:21.880 --> 00:01:22.880]   A discloast.
[00:01:22.880 --> 00:01:23.880]   A temporary.
[00:01:23.880 --> 00:01:24.880]   A discloast.
[00:01:24.880 --> 00:01:25.880]   For a month.
[00:01:25.880 --> 00:01:28.880]   The breakfast tacos are still on.
[00:01:28.880 --> 00:01:30.880]   The breakfast tacos.
[00:01:30.880 --> 00:01:35.880]   You really should be stocking up right now.
[00:01:35.880 --> 00:01:36.880]   Oh, I am.
[00:01:36.880 --> 00:01:37.880]   Believe me.
[00:01:37.880 --> 00:01:39.880]   It's enchilada's breakfast tacos and brisket.
[00:01:39.880 --> 00:01:40.880]   Ooh, encaso.
[00:01:40.880 --> 00:01:41.880]   Yes.
[00:01:41.880 --> 00:01:42.880]   My arteries will not make me.
[00:01:42.880 --> 00:01:49.880]   You could probably bring all of this to Seattle and the Seattleans would be so grateful that you introduced food.
[00:01:49.880 --> 00:01:51.880]   I have really thought about this.
[00:01:51.880 --> 00:01:53.880]   But then I'd have to quit my actual tech job.
[00:01:53.880 --> 00:01:54.880]   I'd have to just become a restaurateur.
[00:01:54.880 --> 00:01:56.880]   Yeah, we don't want to do that.
[00:01:56.880 --> 00:01:57.880]   Nope.
[00:01:57.880 --> 00:01:59.880]   And purely selfish, on my part.
[00:01:59.880 --> 00:02:02.880]   Seattle's begging for Stacey's place.
[00:02:02.880 --> 00:02:04.880]   Also here, Jeff Jarvis.
[00:02:04.880 --> 00:02:11.880]   He is, of course, the Leonard Towne Professor for journalistic innovation at the Craig NoMark graduate school of journalism at the city university of New York.
[00:02:11.880 --> 00:02:14.880]   Blogger@buzzmachine.com author of many wonderful books.
[00:02:14.880 --> 00:02:16.880]   Like what would Google do?
[00:02:16.880 --> 00:02:18.880]   Welcome again, Jeff.
[00:02:18.880 --> 00:02:19.880]   Thank you.
[00:02:19.880 --> 00:02:20.880]   Thank you.
[00:02:20.880 --> 00:02:21.880]   Former TV player.
[00:02:21.880 --> 00:02:22.880]   Good to have the gang together.
[00:02:22.880 --> 00:02:23.880]   Yeah.
[00:02:23.880 --> 00:02:25.880]   This is the way it's supposed to be.
[00:02:25.880 --> 00:02:27.880]   It's been fun, but this is the way it's supposed to be.
[00:02:27.880 --> 00:02:34.880]   Let's start with undersea cables just to make sure Stacey comes back.
[00:02:34.880 --> 00:02:37.880]   We can't.
[00:02:37.880 --> 00:02:40.880]   I think I owe, but OK.
[00:02:40.880 --> 00:02:41.880]   Undersea cables.
[00:02:41.880 --> 00:02:42.880]   Yeah, no, no.
[00:02:42.880 --> 00:02:43.880]   I want you to be happy, Stacey.
[00:02:43.880 --> 00:02:46.880]   Stacey has a thing for undersea cables.
[00:02:46.880 --> 00:02:51.880]   Because I only because I can do my Sebastian, the crab impression.
[00:02:51.880 --> 00:02:53.880]   No, I will not see.
[00:02:53.880 --> 00:02:54.880]   Why?
[00:02:54.880 --> 00:02:59.880]   First of all, I should probably ask you, what is it about undersea cables you like so much?
[00:02:59.880 --> 00:03:02.880]   Because that's how you get the internet around the world.
[00:03:02.880 --> 00:03:07.880]   Because anything backhaul is inherently like necessary and not sexy to anyone.
[00:03:07.880 --> 00:03:09.880]   So I, of course, love it.
[00:03:09.880 --> 00:03:11.880]   That's how that's like the technology I love.
[00:03:11.880 --> 00:03:12.880]   It's it's infrastructure.
[00:03:12.880 --> 00:03:13.880]   You love infrastructure.
[00:03:13.880 --> 00:03:17.880]   You love getting down into the nitty gritty.
[00:03:17.880 --> 00:03:19.880]   So into the weeds or the photons.
[00:03:19.880 --> 00:03:26.880]   I'm not sure I'm thrilled about the idea of Google or any company like Google, Facebook
[00:03:26.880 --> 00:03:30.880]   would be in there having so much influence on the infrastructure.
[00:03:30.880 --> 00:03:33.880]   But I guess, you know, like you love your phone company?
[00:03:33.880 --> 00:03:35.880]   Yeah, it's like Sprint, you know, or.
[00:03:35.880 --> 00:03:37.880]   Yeah, this is they did this.
[00:03:37.880 --> 00:03:42.880]   They started doing this back in like 2008 or maybe even earlier because it was all part
[00:03:42.880 --> 00:03:47.880]   of their, I mean, again, what is their business is delivering bits at the lowest possible
[00:03:47.880 --> 00:03:48.880]   cost.
[00:03:48.880 --> 00:03:49.880]   Right.
[00:03:49.880 --> 00:03:51.880]   Why I give money to telcos who hated them.
[00:03:51.880 --> 00:03:54.880]   Yeah, no, and Jeff makes a great point.
[00:03:54.880 --> 00:04:01.880]   I'd rather Google did it than a telco, which is what I guess who normally builds these.
[00:04:01.880 --> 00:04:08.880]   So this is an image from your favorite place, Telah Geography, SubmarineCableMap.com.
[00:04:08.880 --> 00:04:16.880]   This is the Curie cable, a 10,000 kilometer cable that goes from Los Angeles to Valparaiso,
[00:04:16.880 --> 00:04:21.880]   which is just a Northwest of Chile's capital Santiago.
[00:04:21.880 --> 00:04:25.880]   And it is fully Google owned, which is unique.
[00:04:25.880 --> 00:04:27.880]   These are very expensive to build.
[00:04:27.880 --> 00:04:29.880]   And this is the first one that Google's ever built.
[00:04:29.880 --> 00:04:36.880]   It has invested in several others along with other telcos, Facebook and other companies.
[00:04:36.880 --> 00:04:41.880]   So that's the first unique thing about Curie.
[00:04:41.880 --> 00:04:46.880]   And then it's doing Unity, which goes from Japan to Ritondo Beach.
[00:04:46.880 --> 00:04:48.880]   No, it already did Unity.
[00:04:48.880 --> 00:04:49.880]   Oh, that's the old one.
[00:04:49.880 --> 00:04:51.880]   So the new one is Curie.
[00:04:51.880 --> 00:04:52.880]   Curie is it?
[00:04:52.880 --> 00:04:53.880]   Unity was a while back.
[00:04:53.880 --> 00:04:54.880]   2010.
[00:04:54.880 --> 00:04:55.880]   Yeah, yeah.
[00:04:55.880 --> 00:04:56.880]   Yes.
[00:04:56.880 --> 00:05:00.880]   The Southeast Asia Japan cable, which was 2013.
[00:05:00.880 --> 00:05:03.880]   But these are not all Google owned, I guess.
[00:05:03.880 --> 00:05:05.880]   Those are the ones that Google was part ownership.
[00:05:05.880 --> 00:05:06.880]   Yeah.
[00:05:06.880 --> 00:05:11.880]   And just to give you an idea of who puts these cables in, it's owned by Google, but also
[00:05:11.880 --> 00:05:17.880]   Globe Telecom, KDDI, telecom in Indonesia, Singapore's telecom, Singtel, China Telecom,
[00:05:17.880 --> 00:05:22.880]   China Mobile, TOT, Chungwa cable, and Brunei International Gateway.
[00:05:22.880 --> 00:05:25.880]   So that's the Southeast Asia cable.
[00:05:25.880 --> 00:05:31.880]   Faster, which went into service a few years ago from Oregon to Taiwan, coned by a bunch
[00:05:31.880 --> 00:05:34.880]   of Asian telecoms, of course.
[00:05:34.880 --> 00:05:35.880]   And Time.com.
[00:05:35.880 --> 00:05:38.880]   That's not Time magazine, isn't it?
[00:05:38.880 --> 00:05:39.880]   No.
[00:05:39.880 --> 00:05:40.880]   Time magazine.
[00:05:40.880 --> 00:05:41.880]   That's dead.
[00:05:41.880 --> 00:05:42.880]   Well, it's not dead.
[00:05:42.880 --> 00:05:43.880]   Time is down.
[00:05:43.880 --> 00:05:45.380]   They actually had a great conference.
[00:05:45.380 --> 00:05:48.440]   This Time 100 conference had everybody.
[00:05:48.440 --> 00:05:51.880]   This is the new D conference, isn't it?
[00:05:51.880 --> 00:05:52.880]   Is it?
[00:05:52.880 --> 00:05:53.880]   Really?
[00:05:53.880 --> 00:05:54.880]   Yeah.
[00:05:54.880 --> 00:05:57.880]   I was amazed at the people who were at this Time 100 conference.
[00:05:57.880 --> 00:05:59.880]   Because they gave them a reward.
[00:05:59.880 --> 00:06:01.880]   They are telling them a little bit.
[00:06:01.880 --> 00:06:05.880]   And they're also trying to appear human to the rest of the world.
[00:06:05.880 --> 00:06:07.880]   Well, I'll tell you the, yeah.
[00:06:07.880 --> 00:06:09.880]   But, you know, they are humans.
[00:06:09.880 --> 00:06:16.680]   Well, they want to hobnob with people outside of tech companies and make themselves more
[00:06:16.680 --> 00:06:19.880]   approachable for people to counter some of the backlash.
[00:06:19.880 --> 00:06:27.560]   Well, it worked for me because Tim Cook, who spoke at it, said some things I really liked.
[00:06:27.560 --> 00:06:29.840]   And it did humanize him.
[00:06:29.840 --> 00:06:33.800]   He said and revealed, I think for the first time, I didn't know this, that Google, for
[00:06:33.800 --> 00:06:37.200]   instance, gives no money to candidates unlike any other tech company.
[00:06:37.200 --> 00:06:38.920]   Apple, I should say, not Google.
[00:06:38.920 --> 00:06:41.440]   Apple gives no money to tech candidates.
[00:06:41.440 --> 00:06:45.920]   Apple does not have a PAC or is not a member of a PAC, a political action committee.
[00:06:45.920 --> 00:06:49.640]   Because Tim Cook says that would be wrong.
[00:06:49.640 --> 00:06:51.360]   I thought that was pretty cool.
[00:06:51.360 --> 00:06:53.480]   It's also because they're Apple.
[00:06:53.480 --> 00:06:54.480]   People come to the technical level.
[00:06:54.480 --> 00:06:55.480]   They don't have to.
[00:06:55.480 --> 00:06:56.480]   Right.
[00:06:56.480 --> 00:06:58.080]   They don't have to.
[00:06:58.080 --> 00:07:00.880]   He also called for the regulation of the tech industry.
[00:07:00.880 --> 00:07:05.240]   And in fact, I really liked what he said about that.
[00:07:05.240 --> 00:07:11.120]   He said, let me see if I can find the quote.
[00:07:11.120 --> 00:07:14.280]   I'm a deeply free market person and mindset.
[00:07:14.280 --> 00:07:18.880]   I believe that some unexpected things can happen in regulation.
[00:07:18.880 --> 00:07:21.800]   I think that we all have to be intellectually honest and we have to admit that what we're
[00:07:21.800 --> 00:07:27.440]   doing isn't working and that technology needs to be regulated.
[00:07:27.440 --> 00:07:29.440]   I don't disagree with that.
[00:07:29.440 --> 00:07:31.520]   And I think that that's fairly honest.
[00:07:31.520 --> 00:07:36.360]   That's not so other technology CEOs say that because it's inevitable.
[00:07:36.360 --> 00:07:38.360]   And so we better call for it.
[00:07:38.360 --> 00:07:39.720]   And if we do, maybe we can control it.
[00:07:39.720 --> 00:07:43.120]   I don't feel like Tim Cook's doing that.
[00:07:43.120 --> 00:07:51.200]   There are too many examples where no rails have resulted in great damage to society.
[00:07:51.200 --> 00:07:55.040]   I've been on the regulation for a rails for competitors.
[00:07:55.040 --> 00:07:56.040]   Yeah.
[00:07:56.040 --> 00:07:59.200]   You can be right about both things.
[00:07:59.200 --> 00:08:00.200]   Yeah.
[00:08:00.200 --> 00:08:06.040]   I do believe that Apple has stood up and said privacy is important.
[00:08:06.040 --> 00:08:11.320]   And they remember they fought the FBI actually had something interesting to say about that.
[00:08:11.320 --> 00:08:15.880]   The FBI, Sam Bernadino case where the FBI went to Apple and said, please decrypt the
[00:08:15.880 --> 00:08:19.520]   guy, the shooter's cell phone and Apple said no.
[00:08:19.520 --> 00:08:22.640]   Tim says, I wish that case would have gone to court to be honest.
[00:08:22.640 --> 00:08:24.480]   It was dropped the day before.
[00:08:24.480 --> 00:08:28.960]   And now after the inspector general reports have come out, our worst fears have been confirmed
[00:08:28.960 --> 00:08:32.600]   that it was a very rigged case to begin with.
[00:08:32.600 --> 00:08:35.120]   And so I think this was not the government's finest hour.
[00:08:35.120 --> 00:08:37.280]   I have personally- How was it a rigged case?
[00:08:37.280 --> 00:08:38.680]   I don't know that.
[00:08:38.680 --> 00:08:39.680]   I don't either.
[00:08:39.680 --> 00:08:44.640]   I'm going to have to look at the inspector general's report, but it sounds like what he's
[00:08:44.640 --> 00:08:49.040]   saying and what we kind of thought at the time was that this was a cherry-picked case.
[00:08:49.040 --> 00:08:53.000]   They didn't really need this information, but they thought that this was their best shot
[00:08:53.000 --> 00:08:58.720]   at getting a judgment that would allow them to decrypt phones in the future.
[00:08:58.720 --> 00:09:03.240]   Yes, they wanted a high-profile thing that people would not get cranky about.
[00:09:03.240 --> 00:09:08.360]   So you could say, oh, they're being a privacy and they're like, but terrorists.
[00:09:08.360 --> 00:09:09.360]   It's a terrorist.
[00:09:09.360 --> 00:09:10.360]   Okay.
[00:09:10.360 --> 00:09:11.360]   Yeah.
[00:09:11.360 --> 00:09:12.760]   So I thought that was good.
[00:09:12.760 --> 00:09:16.600]   And by the way, that was all at this time 100 summit.
[00:09:16.600 --> 00:09:18.520]   So who owns time if they don't exist?
[00:09:18.520 --> 00:09:19.520]   No, no, no.
[00:09:19.520 --> 00:09:20.520]   I do.
[00:09:20.520 --> 00:09:21.520]   Time ache doesn't exist.
[00:09:21.520 --> 00:09:22.920]   Oh, many off on time magazine.
[00:09:22.920 --> 00:09:30.080]   But you were asking in the context of who would home a cable?
[00:09:30.080 --> 00:09:32.440]   It was a long, deep rat hole we went down.
[00:09:32.440 --> 00:09:34.360]   Yes, it was, but it was your rat holes.
[00:09:34.360 --> 00:09:35.360]   All yours.
[00:09:35.360 --> 00:09:38.160]   It's still a tech rat hole because Benioff.
[00:09:38.160 --> 00:09:44.560]   So boom, although time.com is time magazine, so I don't know.
[00:09:44.560 --> 00:09:45.560]   I'm confused.
[00:09:45.560 --> 00:09:47.160]   I'm confused.
[00:09:47.160 --> 00:09:51.800]   I was not impressed with the 100.
[00:09:51.800 --> 00:09:52.800]   As such BS.
[00:09:52.800 --> 00:09:53.800]   They are.
[00:09:53.800 --> 00:09:54.800]   Yeah.
[00:09:54.800 --> 00:09:57.360]   As you know, and Jeff's been with this is for just been in these rooms.
[00:09:57.360 --> 00:10:00.520]   I'm sure when these lists have been created, it's really a great way to publicize a magazine
[00:10:00.520 --> 00:10:01.520]   to get attention.
[00:10:01.520 --> 00:10:03.760]   I've been in the room when these are decided.
[00:10:03.760 --> 00:10:04.760]   Oh, you have to.
[00:10:04.760 --> 00:10:06.320]   I used to work at fortune.
[00:10:06.320 --> 00:10:07.320]   They make this.
[00:10:07.320 --> 00:10:08.320]   Yeah.
[00:10:08.320 --> 00:10:09.720]   I used to make listed Gigo.
[00:10:09.720 --> 00:10:12.480]   Some of these people are important.
[00:10:12.480 --> 00:10:15.600]   Some of them are celebrities.
[00:10:15.600 --> 00:10:18.040]   Jared Kushner was one of them.
[00:10:18.040 --> 00:10:19.520]   No doubt he's important.
[00:10:19.520 --> 00:10:24.360]   Nancy Pelosi also important, but some of them may be less.
[00:10:24.360 --> 00:10:25.880]   These are advertisers.
[00:10:25.880 --> 00:10:26.880]   Yeah.
[00:10:26.880 --> 00:10:27.880]   Tyra Banks, really?
[00:10:27.880 --> 00:10:28.880]   Shut your mouth.
[00:10:28.880 --> 00:10:29.880]   Yeah.
[00:10:29.880 --> 00:10:30.880]   Yeah.
[00:10:30.880 --> 00:10:31.880]   Yeah.
[00:10:31.880 --> 00:10:32.880]   I was in the room.
[00:10:32.880 --> 00:10:33.880]   I was in the room.
[00:10:33.880 --> 00:10:34.880]   I was in the room.
[00:10:34.880 --> 00:10:35.880]   I was in the room.
[00:10:35.880 --> 00:10:40.320]   CEO of Microsoft, Melinda Gates, Bill's wife, who's been very influential.
[00:10:40.320 --> 00:10:41.960]   Kanye.
[00:10:41.960 --> 00:10:47.360]   Anyway, it's an interesting, interesting list.
[00:10:47.360 --> 00:10:48.360]   You're right.
[00:10:48.360 --> 00:10:51.360]   These things are really about promotional.
[00:10:51.360 --> 00:10:52.360]   Yeah.
[00:10:52.360 --> 00:10:53.360]   Yeah.
[00:10:53.360 --> 00:10:58.920]   And they're also just kind of New York magazine made lists, the lingua franca magazines.
[00:10:58.920 --> 00:10:59.920]   Oh, really?
[00:10:59.920 --> 00:11:01.520]   It was also, it was that who started that?
[00:11:01.520 --> 00:11:02.520]   New York.
[00:11:02.520 --> 00:11:03.520]   Yeah, New York really did.
[00:11:03.520 --> 00:11:06.960]   Well, when it was part of the hair X, a viral tribute, I mean.
[00:11:06.960 --> 00:11:13.440]   And then, so I was in the room at People when they had a cover of Mel Gibson.
[00:11:13.440 --> 00:11:18.280]   And my mentor, the editor, Pat Ryan, was just struggling to have any damn cover billing
[00:11:18.280 --> 00:11:20.040]   to say, "What do you say about Mel Gibson at the time?"
[00:11:20.040 --> 00:11:22.560]   Now there's plenty to say, but then there wasn't much to say.
[00:11:22.560 --> 00:11:25.600]   And she'd finally just, just out of nowhere kind of said, "Oh, let's call him a sexy
[00:11:25.600 --> 00:11:26.600]   man alive."
[00:11:26.600 --> 00:11:28.120]   Oh, that started that.
[00:11:28.120 --> 00:11:29.120]   That was like a franchise.
[00:11:29.120 --> 00:11:30.120]   Yeah.
[00:11:30.120 --> 00:11:32.800]   It was just trying to fill in a cover bill.
[00:11:32.800 --> 00:11:33.800]   It was, "Ugh."
[00:11:33.800 --> 00:11:34.800]   All right.
[00:11:34.800 --> 00:11:35.800]   Wow.
[00:11:35.800 --> 00:11:36.800]   I was there.
[00:11:36.800 --> 00:11:37.800]   I love those covers.
[00:11:37.800 --> 00:11:42.240]   What do you say about Mel Gibson?
[00:11:42.240 --> 00:11:44.240]   Well, now you say that...
[00:11:44.240 --> 00:11:52.880]   Mel, you mean, "I'm not somebody drunk, but that's another matter."
[00:11:52.880 --> 00:11:55.880]   Here's another one for you, Stacey, because we missed you so much.
[00:11:55.880 --> 00:11:56.880]   Oh.
[00:11:56.880 --> 00:11:57.880]   "Shacard."
[00:11:57.880 --> 00:11:58.880]   Oh, yes.
[00:11:58.880 --> 00:12:01.240]   I didn't see the TED Talk, but I was supposed to look up.
[00:12:01.240 --> 00:12:02.240]   I looked this up yet.
[00:12:02.240 --> 00:12:03.240]   Oh, okay.
[00:12:03.240 --> 00:12:04.240]   Good.
[00:12:04.240 --> 00:12:08.880]   It will be, though, a TED Talk which demonstrates this "Shacard" technology, which is a wearable
[00:12:08.880 --> 00:12:09.880]   technology.
[00:12:09.880 --> 00:12:12.120]   It was announced four years ago.
[00:12:12.120 --> 00:12:13.120]   A long time ago.
[00:12:13.120 --> 00:12:16.160]   I just want to point that out.
[00:12:16.160 --> 00:12:25.520]   Ivan Pujerev, Puperev, who is at the ATP Google's Advanced Technology and Products Division,
[00:12:25.520 --> 00:12:31.880]   projects, I'm sorry, division, showed off his Levi jacket, the "Shacard" jacket.
[00:12:31.880 --> 00:12:39.680]   This is an interesting graphic involving vegetables, shoes, laptops.
[00:12:39.680 --> 00:12:40.680]   I'm not sure.
[00:12:40.680 --> 00:12:44.200]   A day in the life of Gemma, everything we touch.
[00:12:44.200 --> 00:12:45.200]   Oh, I get it.
[00:12:45.200 --> 00:12:49.200]   It's an image of everything she touched during the day.
[00:12:49.200 --> 00:12:50.200]   Interesting.
[00:12:50.200 --> 00:12:53.960]   Oh, she's got the tray of Indian spices.
[00:12:53.960 --> 00:12:55.680]   I bet her cooking is delicious.
[00:12:55.680 --> 00:13:04.160]   I bet looking at this because a pogo stick, a skateboard, a hairdryer, I bet she's a kid.
[00:13:04.160 --> 00:13:06.040]   But this was all the stuff she ate.
[00:13:06.040 --> 00:13:07.040]   I don't know.
[00:13:07.040 --> 00:13:08.040]   Oh, ate or made?
[00:13:08.040 --> 00:13:09.040]   I mean, if you touched it...
[00:13:09.040 --> 00:13:11.120]   She's a chef, I don't know.
[00:13:11.120 --> 00:13:12.120]   You don't need to be a chef.
[00:13:12.120 --> 00:13:14.960]   This is just every home cook who's every...
[00:13:14.960 --> 00:13:15.960]   Six fish?
[00:13:15.960 --> 00:13:19.480]   Yeah, I mean, we don't know how big those fish are.
[00:13:19.480 --> 00:13:22.560]   Judging by the size, comparing it to other stuff, they could be like sardines.
[00:13:22.560 --> 00:13:23.560]   Two short chokes.
[00:13:23.560 --> 00:13:24.560]   Yeah, they could be sardines.
[00:13:24.560 --> 00:13:25.560]   You're right.
[00:13:25.560 --> 00:13:26.560]   We don't.
[00:13:26.560 --> 00:13:27.560]   Haring.
[00:13:27.560 --> 00:13:30.200]   She's well off, whoever she is.
[00:13:30.200 --> 00:13:31.200]   She's got that in her eye.
[00:13:31.200 --> 00:13:32.200]   She's very well off.
[00:13:32.200 --> 00:13:34.400]   She has expensive gear.
[00:13:34.400 --> 00:13:39.000]   And a jacket that snitches on her, I think, is the impression I'm getting from this image.
[00:13:39.000 --> 00:13:44.840]   So, I think this was a photo essay taken from an artist.
[00:13:44.840 --> 00:13:50.200]   And I think, actually, what they're probably showing is how you can imbue everyday objects
[00:13:50.200 --> 00:13:52.120]   with touch using check card...
[00:13:52.120 --> 00:13:55.720]   Sorry, imbue everyday objects with technology with check card.
[00:13:55.720 --> 00:13:59.800]   Yeah, he actually used his Levi's jacket to control his presentation.
[00:13:59.800 --> 00:14:00.800]   Right.
[00:14:00.800 --> 00:14:05.600]   Which, having played with touch technology, that's this kind of gimmicky stuff, that's
[00:14:05.600 --> 00:14:07.480]   exactly what it's good for.
[00:14:07.480 --> 00:14:11.480]   I once saw Mick Fleetwood do an entire drum solo on his clothes.
[00:14:11.480 --> 00:14:12.480]   Yes.
[00:14:12.480 --> 00:14:17.200]   I want to see Karsten replace the tricaster with his jacket.
[00:14:17.200 --> 00:14:19.640]   That sounds true.
[00:14:19.640 --> 00:14:21.160]   So, I thought...
[00:14:21.160 --> 00:14:22.160]   Wonderful.
[00:14:22.160 --> 00:14:27.440]   When they talked about check card way back in 2015, when they talked about it at I/O
[00:14:27.440 --> 00:14:30.640]   or wherever it was, I actually thought it was neat.
[00:14:30.640 --> 00:14:31.640]   Imagine if you...
[00:14:31.640 --> 00:14:34.080]   Because it's touch sensitive and it's...
[00:14:34.080 --> 00:14:35.600]   I think it's fabric.
[00:14:35.600 --> 00:14:38.840]   It's a built-in fabric using silver or something.
[00:14:38.840 --> 00:14:39.840]   Right.
[00:14:39.840 --> 00:14:45.120]   And what you can do, imagine that on your couch, on the arm of your couch or your favorite
[00:14:45.120 --> 00:14:46.120]   chair.
[00:14:46.120 --> 00:14:49.080]   It's just built in and it links into whatever your TV remote is.
[00:14:49.080 --> 00:14:51.040]   And then suddenly you don't have that remote.
[00:14:51.040 --> 00:14:53.920]   You just kind of run your hands on your couch.
[00:14:53.920 --> 00:14:59.200]   But then you have to remember all the places to touch or not to touch if you don't want
[00:14:59.200 --> 00:15:00.560]   the jail to change.
[00:15:00.560 --> 00:15:06.400]   Well, so the idea is eventually you'd have fabric that you could interact with that way.
[00:15:06.400 --> 00:15:13.360]   And that's actually as we bring design that is in view more things with technology, having
[00:15:13.360 --> 00:15:21.040]   the devices understand your intentions is going to be the next or a next big area of
[00:15:21.040 --> 00:15:22.040]   design.
[00:15:22.040 --> 00:15:23.040]   Almost.
[00:15:23.040 --> 00:15:24.040]   It should be the first thing you do.
[00:15:24.040 --> 00:15:27.040]   Well, but you've got to build it and see what happens with...
[00:15:27.040 --> 00:15:30.000]   That's the stage they're at right now, clearly.
[00:15:30.000 --> 00:15:36.280]   Chris Anderson, who runs Ted, came on stage apparently and asked Puparev whether there
[00:15:36.280 --> 00:15:41.840]   needs to be some kind of contract ensuring this data isn't abused since it potentially
[00:15:41.840 --> 00:15:46.240]   creates the biggest effort surveillance network for Google or another company.
[00:15:46.240 --> 00:15:51.360]   It's unfortunate that we've gotten this position now where we don't, and understandably, don't
[00:15:51.360 --> 00:15:53.000]   trust these companies.
[00:15:53.000 --> 00:15:57.000]   And it could slow down this kind of innovation because people just don't...
[00:15:57.000 --> 00:15:58.000]   I think that's good.
[00:15:58.000 --> 00:15:59.000]   I think that's...
[00:15:59.000 --> 00:16:00.000]   Do we have that conversation?
[00:16:00.000 --> 00:16:01.000]   So we have that conversation.
[00:16:01.000 --> 00:16:03.040]   As long as we continue to do it, right?
[00:16:03.040 --> 00:16:04.040]   We are.
[00:16:04.040 --> 00:16:05.560]   I mean, that's where we're...
[00:16:05.560 --> 00:16:12.120]   I don't think it's crazy to be like, "I'm sure when generals came up against the first
[00:16:12.120 --> 00:16:16.400]   people who had guns or maybe arrows, they were like, "Holy cow, this is a new technology.
[00:16:16.400 --> 00:16:19.920]   We've got to figure out what this is, how we adapt to it."
[00:16:19.920 --> 00:16:22.400]   It's not crazy for us to have that same thing.
[00:16:22.400 --> 00:16:27.080]   And I'm not saying this technology is as deadly, but it's definitely got similar types of
[00:16:27.080 --> 00:16:31.880]   repercussions in terms of effects.
[00:16:31.880 --> 00:16:32.880]   So...
[00:16:32.880 --> 00:16:34.160]   Yeah.
[00:16:34.160 --> 00:16:38.960]   This is where I'm on Jeff's side of this whole moral panic thing because sometimes that
[00:16:38.960 --> 00:16:44.600]   could be used to shut stuff down instead of just say, "Let's really talk about the issues
[00:16:44.600 --> 00:16:45.600]   here."
[00:16:45.600 --> 00:16:48.600]   I think that's what we do well, and that's why it's great to have both of you kind of
[00:16:48.600 --> 00:16:54.240]   fighting back and forth on this because it's reasonable to consider it.
[00:16:54.240 --> 00:16:56.280]   It's not crazy to press pause.
[00:16:56.280 --> 00:17:00.400]   If you shut something down for a little while and then you recognize...
[00:17:00.400 --> 00:17:01.800]   I mean, I've kicked things...
[00:17:01.800 --> 00:17:03.320]   I'm sure everyone's done this.
[00:17:03.320 --> 00:17:07.640]   I've kicked things out of my house and then two weeks later I'm like, "Dude, that was
[00:17:07.640 --> 00:17:08.640]   dumb.
[00:17:08.640 --> 00:17:09.640]   I really need this."
[00:17:09.640 --> 00:17:12.800]   And then you realize where you need it most and you start flipping rules.
[00:17:12.800 --> 00:17:19.520]   Then don't you worry, as I do, that now, "Oh, the only reason I'm hooked, I can't get
[00:17:19.520 --> 00:17:20.520]   rid of it."
[00:17:20.520 --> 00:17:25.840]   Because I do need it because it's a benefit, because I'm hooked on this stuff.
[00:17:25.840 --> 00:17:27.960]   Don't you worry about that?
[00:17:27.960 --> 00:17:28.960]   No.
[00:17:28.960 --> 00:17:29.960]   We got hooked on books.
[00:17:29.960 --> 00:17:33.920]   I mean, TV was going to be awful to us.
[00:17:33.920 --> 00:17:35.920]   We watch things.
[00:17:35.920 --> 00:17:36.920]   It's okay.
[00:17:36.920 --> 00:17:37.920]   It's okay.
[00:17:37.920 --> 00:17:39.400]   I would argue though.
[00:17:39.400 --> 00:17:42.720]   Let's just throw out this TV example for a second there because Americans are now watching
[00:17:42.720 --> 00:17:44.920]   like five hours and 42 minutes of...
[00:17:44.920 --> 00:17:45.920]   Isn't that funny?
[00:17:45.920 --> 00:17:46.920]   They're not really.
[00:17:46.920 --> 00:17:47.920]   Not what you would have found it.
[00:17:47.920 --> 00:17:50.680]   I would have predicted that would go down and internet viewing would go up and it's
[00:17:50.680 --> 00:17:51.680]   not the case.
[00:17:51.680 --> 00:17:53.240]   Both have gone up.
[00:17:53.240 --> 00:17:57.960]   Well, overall television, overall video content viewing has gone up.
[00:17:57.960 --> 00:18:03.960]   The slice of it happening on mobile and non-television networks is also increasing.
[00:18:03.960 --> 00:18:06.040]   We're still watching a lot of traditional TV.
[00:18:06.040 --> 00:18:07.040]   We're still watching TV.
[00:18:07.040 --> 00:18:09.280]   But it's not growing.
[00:18:09.280 --> 00:18:12.000]   What we're watching on traditional TV is not growing.
[00:18:12.000 --> 00:18:13.000]   I know.
[00:18:13.000 --> 00:18:14.000]   We're still watching a lot.
[00:18:14.000 --> 00:18:17.400]   If I may, I think there's a...
[00:18:17.400 --> 00:18:18.400]   How much?
[00:18:18.400 --> 00:18:19.400]   Four hours a day.
[00:18:19.400 --> 00:18:23.720]   I think it's roughly four hours a day or three hours and 52 minutes.
[00:18:23.720 --> 00:18:24.720]   I do that.
[00:18:24.720 --> 00:18:25.720]   I do that.
[00:18:25.720 --> 00:18:30.520]   But if I may speak on behalf of the great unwashed masses, I think it's a little hotty
[00:18:30.520 --> 00:18:34.520]   to say, "Oh, all these people are doing all that and it's just awful for them."
[00:18:34.520 --> 00:18:37.080]   People do what they want to do in their lives and it's okay.
[00:18:37.080 --> 00:18:39.880]   Well, I'm not saying it as those people.
[00:18:39.880 --> 00:18:42.320]   I am doing it too much.
[00:18:42.320 --> 00:18:46.320]   Well, yes, your judgment is too much, but others are doing it to what they ever want
[00:18:46.320 --> 00:18:47.320]   to do.
[00:18:47.320 --> 00:18:50.160]   I'm not trying to tell somebody to stop watching TV.
[00:18:50.160 --> 00:18:55.680]   I would argue that television is the opiate of the masses.
[00:18:55.680 --> 00:18:56.680]   No, that's religion.
[00:18:56.680 --> 00:18:59.480]   But I would say that there are repercussions to television.
[00:18:59.480 --> 00:19:00.480]   I don't think...
[00:19:00.480 --> 00:19:02.960]   We are distracting ourselves to death.
[00:19:02.960 --> 00:19:04.680]   Not just that, but think about...
[00:19:04.680 --> 00:19:08.360]   I look at someone like my father who watches a lot of Fox News and...
[00:19:08.360 --> 00:19:10.360]   Well, there's that.
[00:19:10.360 --> 00:19:15.600]   I think the things that television brings out or certain types of television, and I'm
[00:19:15.600 --> 00:19:17.760]   not trying to throw Fox News under the bus.
[00:19:17.760 --> 00:19:18.760]   There are programs on the outside.
[00:19:18.760 --> 00:19:23.800]   No, I would say all 24-hour news is bad for us.
[00:19:23.800 --> 00:19:30.240]   Some's worse than others, but any 24-hour news channel has an imperative to ramp up everything
[00:19:30.240 --> 00:19:32.560]   because they need to fill the time.
[00:19:32.560 --> 00:19:35.400]   Everything becomes important, even the smallest detail.
[00:19:35.400 --> 00:19:39.880]   And I know because I remember having to fill 12 hours a day on tech TV.
[00:19:39.880 --> 00:19:44.000]   And that's the result of it is you go, "Every story is a big story."
[00:19:44.000 --> 00:19:48.880]   So I'm not going to drag you through this, but I wrote a long and esoteric.
[00:19:48.880 --> 00:19:51.800]   See, I confess that to watch out.
[00:19:51.800 --> 00:19:52.800]   Post today about...
[00:19:52.800 --> 00:19:56.240]   I think I mentioned this in the past.
[00:19:56.240 --> 00:20:00.400]   It was inspired by this book, "How History Guess Things Wrong, The Neuroscience of Our
[00:20:00.400 --> 00:20:01.400]   Addiction to Stories."
[00:20:01.400 --> 00:20:02.400]   Oh, yeah.
[00:20:02.400 --> 00:20:03.600]   And I got to read that.
[00:20:03.600 --> 00:20:04.600]   That's a really...
[00:20:04.600 --> 00:20:05.600]   It's quite a thing.
[00:20:05.600 --> 00:20:06.600]   So I think...
[00:20:06.600 --> 00:20:07.600]   So I think...
[00:20:07.600 --> 00:20:11.360]   In this post today, which is on the rundown, right, mid-it's long and esoteric, but you
[00:20:11.360 --> 00:20:14.720]   can plug it anyway.
[00:20:14.720 --> 00:20:19.120]   I go through his rationale about this.
[00:20:19.120 --> 00:20:22.600]   And part of what he says in there is we love stories.
[00:20:22.600 --> 00:20:26.160]   Part of the reason we're addicted to stories is a story with conflict gets the oxytocin
[00:20:26.160 --> 00:20:27.160]   going.
[00:20:27.160 --> 00:20:28.160]   Yep.
[00:20:28.160 --> 00:20:29.160]   Yep.
[00:20:29.160 --> 00:20:32.520]   And I went into various of the academic...
[00:20:32.520 --> 00:20:40.440]   Not that I'm an academic, but I try to play one on TV or on podcasts.
[00:20:40.440 --> 00:20:42.960]   Search engines for papers and started reading through it.
[00:20:42.960 --> 00:20:43.960]   It's fascinating.
[00:20:43.960 --> 00:20:51.320]   They're doing experiments about whether oxytocin, if injected or up the nose, makes you more
[00:20:51.320 --> 00:20:55.240]   or less likely to connect with other people.
[00:20:55.240 --> 00:21:00.280]   Just by the way, we're not talking oxycontin, which is an addictive opiate and a real problem
[00:21:00.280 --> 00:21:07.120]   in the essence of taking oxytocin, which is they call it the love hormone, which is manufactured
[00:21:07.120 --> 00:21:08.120]   by your own body.
[00:21:08.120 --> 00:21:09.120]   Yeah.
[00:21:09.120 --> 00:21:10.960]   And you're not actually going to have an orgasm.
[00:21:10.960 --> 00:21:13.360]   Or nipple, so it makes you feel good.
[00:21:13.360 --> 00:21:18.080]   Or hello after you give birth or when you cuddle a baby or even when you pet a dog.
[00:21:18.080 --> 00:21:22.160]   When baby's nurse, it causes the phone to write.
[00:21:22.160 --> 00:21:27.760]   When baby's nurse, the stimulating the nipple, that stimulates oxytocin.
[00:21:27.760 --> 00:21:31.160]   I still have to get it right.
[00:21:31.160 --> 00:21:32.160]   And you feel love.
[00:21:32.160 --> 00:21:33.880]   And that's a...
[00:21:33.880 --> 00:21:36.840]   If you think about a great hormone, but it is also an addictive hormone.
[00:21:36.840 --> 00:21:37.840]   It's a pleasure hormone.
[00:21:37.840 --> 00:21:38.840]   Right.
[00:21:38.840 --> 00:21:41.720]   So things are set up and structured for this.
[00:21:41.720 --> 00:21:45.160]   Now you could imagine using it for good in the sense that...
[00:21:45.160 --> 00:21:47.360]   So this is a great line I quoted.
[00:21:47.360 --> 00:21:48.520]   Hold on, let me get this.
[00:21:48.520 --> 00:21:49.520]   I love this.
[00:21:49.520 --> 00:21:50.520]   I love this part.
[00:21:50.520 --> 00:21:51.520]   Medium.
[00:21:51.520 --> 00:21:57.040]   So in fact, this story says it's not attachment that's going on.
[00:21:57.040 --> 00:21:59.840]   It's addiction to oxytocin.
[00:21:59.840 --> 00:22:01.520]   That we interpret as attachment.
[00:22:01.520 --> 00:22:02.520]   I love my baby.
[00:22:02.520 --> 00:22:04.000]   No, no, I just want more oxytocin.
[00:22:04.000 --> 00:22:06.960]   How do we get here from the card?
[00:22:06.960 --> 00:22:11.200]   Did I feel like I fell asleep for a minute?
[00:22:11.200 --> 00:22:12.680]   No, no, it's moving fast.
[00:22:12.680 --> 00:22:14.280]   And I'm going to keep it moving fast.
[00:22:14.280 --> 00:22:17.160]   No, no, because you've got to go get queso or something.
[00:22:17.160 --> 00:22:18.160]   I can't remember.
[00:22:18.160 --> 00:22:19.160]   But there's...
[00:22:19.160 --> 00:22:20.160]   No, no, no, sorry.
[00:22:20.160 --> 00:22:21.160]   I was like, wait a second.
[00:22:21.160 --> 00:22:23.560]   Did I just have a dark, elliptic moment?
[00:22:23.560 --> 00:22:24.560]   No, no.
[00:22:24.560 --> 00:22:25.560]   You were a rebel.
[00:22:25.560 --> 00:22:26.560]   Everybody listen to the show.
[00:22:26.560 --> 00:22:27.560]   You were the rabbit hole.
[00:22:27.560 --> 00:22:31.040]   Yeah, everybody listen to the show has had exactly the same experience.
[00:22:31.040 --> 00:22:33.360]   So thank you, Stacey, for mirroring our audience.
[00:22:33.360 --> 00:22:34.360]   Let me show the conversation.
[00:22:34.360 --> 00:22:39.640]   Let me show the conversation just a little bit to stay in the same rut.
[00:22:39.640 --> 00:22:41.400]   Yes, it was Harvard.
[00:22:41.400 --> 00:22:46.880]   Yeah, HAVA is in EU votes to create giant biometrics database.
[00:22:46.880 --> 00:22:54.320]   They're going to create the common identity repository, CIR, which is fingerprints DNA.
[00:22:54.320 --> 00:23:00.360]   And the idea is it'll be used across borders by law enforcement everywhere in the EU.
[00:23:00.360 --> 00:23:03.720]   Boy, you really, that was a headlight.
[00:23:03.720 --> 00:23:04.720]   You're a moxie toasted to that.
[00:23:04.720 --> 00:23:07.720]   I don't think it could possibly go wrong.
[00:23:07.720 --> 00:23:10.920]   And so all I always do is attempting, of course, correction.
[00:23:10.920 --> 00:23:13.720]   But I feel at this point, a little whiplash.
[00:23:13.720 --> 00:23:18.240]   I'm just going back to things that could possibly go wrong.
[00:23:18.240 --> 00:23:20.160]   Okay, what could possibly go wrong?
[00:23:20.160 --> 00:23:22.760]   So I really want to know what could possibly go wrong.
[00:23:22.760 --> 00:23:25.040]   I got something that could possibly go wrong.
[00:23:25.040 --> 00:23:27.640]   If you made queso out of Cheez-Its.
[00:23:27.640 --> 00:23:28.640]   Ew.
[00:23:28.640 --> 00:23:30.640]   How would you get it?
[00:23:30.640 --> 00:23:31.640]   It's called...
[00:23:31.640 --> 00:23:32.640]   What?
[00:23:32.640 --> 00:23:36.040]   Cheez-It, queso fondido.
[00:23:36.040 --> 00:23:42.960]   Liven up your snacking routine with some zesty, Cheez-It, queso fondido, baked, snacked crackers.
[00:23:42.960 --> 00:23:45.960]   So they're going to get Cheez-Its out of queso, not queso out of Cheez-Its.
[00:23:45.960 --> 00:23:48.000]   We're putting the queso back in Cheez-Its.
[00:23:48.000 --> 00:23:49.960]   Yeah, that's just giving me.
[00:23:49.960 --> 00:23:50.960]   Does it stuff with queso?
[00:23:50.960 --> 00:23:53.000]   I bet it's like pepper jack cheese.
[00:23:53.000 --> 00:23:54.000]   Wait, wait, wait, wait.
[00:23:54.000 --> 00:23:56.400]   Is it just queso flavored or is there like...
[00:23:56.400 --> 00:23:58.760]   It pops out in your mouth.
[00:23:58.760 --> 00:24:00.880]   This is the Weasel word.
[00:24:00.880 --> 00:24:04.760]   It's inspired by the taste of Chili's peppers and scrumptious cheese.
[00:24:04.760 --> 00:24:09.040]   But I was hoping when you bite into the Cheez-It, you get a little burst of queso.
[00:24:09.040 --> 00:24:11.080]   Ooh, like a cheesy lava cake?
[00:24:11.080 --> 00:24:12.080]   Yeah.
[00:24:12.080 --> 00:24:17.240]   Or that gum that you chew and it's got inside, it's got, instead of it's got fruit flavors.
[00:24:17.240 --> 00:24:18.720]   It's got cheese.
[00:24:18.720 --> 00:24:20.480]   It's got queso inside.
[00:24:20.480 --> 00:24:21.480]   My cat.
[00:24:21.480 --> 00:24:23.480]   Oh my God.
[00:24:23.480 --> 00:24:24.480]   Okay.
[00:24:24.480 --> 00:24:27.800]   Can I ask one off-purpose question?
[00:24:27.800 --> 00:24:30.200]   I'm asking this of the queso queen.
[00:24:30.200 --> 00:24:31.960]   Would you make a keto queso?
[00:24:31.960 --> 00:24:34.840]   In other words, a non-carb queso.
[00:24:34.840 --> 00:24:35.840]   Because I could eat that.
[00:24:35.840 --> 00:24:37.840]   Isn't cheese not...
[00:24:37.840 --> 00:24:40.440]   It is, but there's flour in it, isn't there or something like that?
[00:24:40.440 --> 00:24:41.440]   Oh, no.
[00:24:41.440 --> 00:24:42.440]   That's like a...
[00:24:42.440 --> 00:24:43.440]   No, yeah.
[00:24:43.440 --> 00:24:45.320]   That's a Beckamil sauce or something.
[00:24:45.320 --> 00:24:46.320]   Oh, okay.
[00:24:46.320 --> 00:24:48.520]   No, yeah, you're fine.
[00:24:48.520 --> 00:24:54.480]   You put milk in your quesos to thin it so it's not like you got to get the right consistency.
[00:24:54.480 --> 00:24:57.160]   Heavy cream, cheese, and jalapenos is all you need.
[00:24:57.160 --> 00:24:58.400]   Oh, well, there you go.
[00:24:58.400 --> 00:24:59.400]   You're fine.
[00:24:59.400 --> 00:25:00.400]   Heavy cream?
[00:25:00.400 --> 00:25:01.400]   Well...
[00:25:01.400 --> 00:25:02.400]   Is all the...
[00:25:02.400 --> 00:25:03.400]   Is all the...
[00:25:03.400 --> 00:25:04.400]   No, curp cream.
[00:25:04.400 --> 00:25:05.400]   That's curp free.
[00:25:05.400 --> 00:25:06.880]   No, velvety is not free.
[00:25:06.880 --> 00:25:07.880]   No, velvety is not free.
[00:25:07.880 --> 00:25:10.320]   No, velvety is not really cheese, but it is the key to most...
[00:25:10.320 --> 00:25:12.120]   Tex mexie quesos.
[00:25:12.120 --> 00:25:17.760]   So, heavy whipping cream is keto friendly and that's why I choose heavy whipping cream.
[00:25:17.760 --> 00:25:18.760]   That...
[00:25:18.760 --> 00:25:21.880]   I choose it because it's totally so good.
[00:25:21.880 --> 00:25:22.880]   It is keto.
[00:25:22.880 --> 00:25:23.880]   I found a keto queso.
[00:25:23.880 --> 00:25:25.520]   There you go.
[00:25:25.520 --> 00:25:26.520]   Okay.
[00:25:26.520 --> 00:25:28.040]   Sorry about that digression.
[00:25:28.040 --> 00:25:30.040]   Back to the undersea cables.
[00:25:30.040 --> 00:25:33.040]   No, we went on to jacquard.
[00:25:33.040 --> 00:25:35.040]   And it parsed in jacquard.
[00:25:35.040 --> 00:25:37.040]   Hold back to my post for the awakening.
[00:25:37.040 --> 00:25:40.360]   Let's go back to Mark Zuckerberg's podcast.
[00:25:40.360 --> 00:25:42.760]   I wonder what he's talking about now.
[00:25:42.760 --> 00:25:45.760]   That wasn't even the show.
[00:25:45.760 --> 00:25:48.120]   That wasn't even the show yet.
[00:25:48.120 --> 00:25:49.120]   Okay, okay, okay.
[00:25:49.120 --> 00:25:50.120]   I've got to get control.
[00:25:50.120 --> 00:25:52.120]   I've got to get control.
[00:25:52.120 --> 00:25:53.120]   Um...
[00:25:53.120 --> 00:25:54.120]   The glaya.
[00:25:54.120 --> 00:25:55.120]   It's coming up.
[00:25:55.120 --> 00:25:59.040]   It's coming up, but we're going to be doing some fun stuff for Google I/O.
[00:25:59.040 --> 00:26:02.800]   So that will be a week from Tuesday, right?
[00:26:02.800 --> 00:26:03.800]   Almost...
[00:26:03.800 --> 00:26:04.800]   May 7th.
[00:26:04.800 --> 00:26:05.800]   Yeah, May 7th.
[00:26:05.800 --> 00:26:07.040]   Jeff is going to be there.
[00:26:07.040 --> 00:26:09.480]   Jason Howell is going to be there.
[00:26:09.480 --> 00:26:12.040]   Florence I/O is going to be there.
[00:26:12.040 --> 00:26:14.400]   However, this is how we're going to do it.
[00:26:14.400 --> 00:26:15.720]   We will do the keynote.
[00:26:15.720 --> 00:26:19.560]   It's I think 10 a.m. Pacific 1 p.m. Eastern, but you'll have to check the schedule for
[00:26:19.560 --> 00:26:20.560]   that.
[00:26:20.560 --> 00:26:23.480]   We will do a live stream of the keynote on Tuesday.
[00:26:23.480 --> 00:26:28.200]   And then Jeff's going to actually brave the Golden Gate Bridge and drive up.
[00:26:28.200 --> 00:26:30.200]   I'm going to put on...
[00:26:30.200 --> 00:26:34.440]   He's going to close his eyes when he drives over the bridge.
[00:26:34.440 --> 00:26:38.440]   And we'll do a special twig the following day.
[00:26:38.440 --> 00:26:40.440]   Getting nervous even now.
[00:26:40.440 --> 00:26:43.880]   Stacey, you're not going to I/O this year, are you?
[00:26:43.880 --> 00:26:45.880]   I am in California.
[00:26:45.880 --> 00:26:50.280]   I'm going to Anaheim, unfortunately, and then I'm in San Jose on the 9th.
[00:26:50.280 --> 00:26:51.280]   Oh, you just missed it.
[00:26:51.280 --> 00:26:52.440]   I'm going to be on a plane during the keynote.
[00:26:52.440 --> 00:26:53.440]   I know.
[00:26:53.440 --> 00:26:54.440]   I'm terrible.
[00:26:54.440 --> 00:26:57.920]   I'm just flying all over California and bypassing Petaluma.
[00:26:57.920 --> 00:26:58.920]   Yeah, as usual.
[00:26:58.920 --> 00:26:59.920]   That's okay.
[00:26:59.920 --> 00:27:03.000]   If you were in the Bay Area, I would totally be...
[00:27:03.000 --> 00:27:07.000]   I believe this is considered part of the Bay Area, but I may be...
[00:27:07.000 --> 00:27:08.000]   Really?
[00:27:08.000 --> 00:27:09.800]   It's so far.
[00:27:09.800 --> 00:27:15.560]   The northernmost finger of the San Francisco Bay is right out our door.
[00:27:15.560 --> 00:27:16.560]   You got a point.
[00:27:16.560 --> 00:27:17.560]   You got a point.
[00:27:17.560 --> 00:27:18.560]   Okay, you went.
[00:27:18.560 --> 00:27:23.520]   And Jeff, I have planned out a route that doesn't involve bridges if you'd like me to
[00:27:23.520 --> 00:27:24.520]   send that.
[00:27:24.520 --> 00:27:25.520]   It goes through Reno.
[00:27:25.520 --> 00:27:26.520]   Don't listen to him.
[00:27:26.520 --> 00:27:30.120]   It only goes through Stockton and Sacramento.
[00:27:30.120 --> 00:27:31.120]   You can go around.
[00:27:31.120 --> 00:27:32.120]   Sacramento.
[00:27:32.120 --> 00:27:33.120]   Go around.
[00:27:33.120 --> 00:27:34.520]   No bridges involved.
[00:27:34.520 --> 00:27:40.480]   So when I was at the examiner, a reporter named Lynn Ludlow, it's a man's name, got challenged
[00:27:40.480 --> 00:27:44.000]   on his expense account for not having to receipt for the bridge toll.
[00:27:44.000 --> 00:27:45.680]   So he said, "You're right.
[00:27:45.680 --> 00:27:46.680]   You caught me."
[00:27:46.680 --> 00:27:49.520]   And he tracked that and charged for the mileage.
[00:27:49.520 --> 00:27:51.520]   That's clever.
[00:27:51.520 --> 00:27:52.520]   That's clever.
[00:27:52.520 --> 00:27:55.040]   Anyway, we're excited about Google I/O.
[00:27:55.040 --> 00:27:57.040]   What do you think we're going to see at Google I/O?
[00:27:57.040 --> 00:27:58.040]   Will they hand out?
[00:27:58.040 --> 00:28:01.880]   I think the rumor is we're going to see the new inexpensive Pixel phone.
[00:28:01.880 --> 00:28:02.880]   Exciting.
[00:28:02.880 --> 00:28:03.880]   Although...
[00:28:03.880 --> 00:28:04.880]   Yes.
[00:28:04.880 --> 00:28:10.840]   ...for one day only yesterday, Google was offering half off the Pixel 3.
[00:28:10.840 --> 00:28:15.080]   Oh, I should have taken that up because I'm about to need a new phone.
[00:28:15.080 --> 00:28:16.680]   Oh, missed it.
[00:28:16.680 --> 00:28:19.280]   No, I'm six months from a new phone.
[00:28:19.280 --> 00:28:20.280]   I do.
[00:28:20.280 --> 00:28:21.680]   Actually, you didn't miss it.
[00:28:21.680 --> 00:28:24.880]   $200 off the Pixel 3 and 3 XL right now.
[00:28:24.880 --> 00:28:25.880]   So...
[00:28:25.880 --> 00:28:26.880]   But it was even more.
[00:28:26.880 --> 00:28:28.280]   I think it was even more, as I remember.
[00:28:28.280 --> 00:28:29.280]   500 bucks a whole.
[00:28:29.280 --> 00:28:30.280]   Yeah.
[00:28:30.280 --> 00:28:35.720]   Still, these are usually when you see a company doing this kind of deep discount, it means...
[00:28:35.720 --> 00:28:37.880]   There are a bunch of multiple places.
[00:28:37.880 --> 00:28:38.880]   Yeah.
[00:28:38.880 --> 00:28:40.760]   ...are on the way.
[00:28:40.760 --> 00:28:48.400]   So I don't... what we had thought is this low-cost Pixel 3A would be not a replacement, but in
[00:28:48.400 --> 00:28:52.520]   addition to a less expensive version of the phone trying to broaden the market.
[00:28:52.520 --> 00:28:53.840]   This Pixel 3 has been successful.
[00:28:53.840 --> 00:28:56.640]   The Pixel 2 did not sell a whole lot of phones.
[00:28:56.640 --> 00:29:01.120]   In fact, it sold the equivalent of one weeks of iPhone sales all year long.
[00:29:01.120 --> 00:29:03.440]   I still have my two and I'm very happy with it.
[00:29:03.440 --> 00:29:05.120]   I love my two.
[00:29:05.120 --> 00:29:06.120]   I'm looking for it, right?
[00:29:06.120 --> 00:29:07.120]   Oh, it's right here in my pocket.
[00:29:07.120 --> 00:29:08.680]   You know what?
[00:29:08.680 --> 00:29:13.760]   The best part was I could remember, I forgot actually, about the squeeze function and then
[00:29:13.760 --> 00:29:16.160]   I squeezed my phone and it was like, "Ooh!"
[00:29:16.160 --> 00:29:17.160]   I'm gonna stop there.
[00:29:17.160 --> 00:29:18.160]   Oh, they're here.
[00:29:18.160 --> 00:29:19.160]   They're here.
[00:29:19.160 --> 00:29:20.160]   The family came home.
[00:29:20.160 --> 00:29:21.160]   Yay!
[00:29:21.160 --> 00:29:22.160]   They didn't abandon me.
[00:29:22.160 --> 00:29:24.160]   Oh, they love me still.
[00:29:24.160 --> 00:29:26.000]   Oh, I'm not stuck here.
[00:29:26.000 --> 00:29:27.000]   Just stay-see.
[00:29:27.000 --> 00:29:28.360]   Oh, she's no fun.
[00:29:28.360 --> 00:29:30.840]   She just talks at the wall for hours.
[00:29:30.840 --> 00:29:33.240]   Oh, they have the dog with them?
[00:29:33.240 --> 00:29:34.760]   The dog is announcing their arrival.
[00:29:34.760 --> 00:29:35.760]   I thought maybe...
[00:29:35.760 --> 00:29:37.920]   She's announcing their arrival.
[00:29:37.920 --> 00:29:41.240]   She's crazy right now because thunderstorms and the move, she's just...
[00:29:41.240 --> 00:29:42.480]   A book with the moronic and all the kids.
[00:29:42.480 --> 00:29:43.800]   You know what's an all to do, yeah.
[00:29:43.800 --> 00:29:48.160]   Isn't it fun how animals kind of sense that something disruptive is happening and they...
[00:29:48.160 --> 00:29:50.280]   Well, we put her in a new house.
[00:29:50.280 --> 00:29:51.280]   Yeah, she...
[00:29:51.280 --> 00:29:54.080]   But did she know before the move that she could kind of sense that this is not good
[00:29:54.080 --> 00:29:55.080]   whatever's going on?
[00:29:55.080 --> 00:29:56.080]   I think so.
[00:29:56.080 --> 00:29:57.080]   Our cats.
[00:29:57.080 --> 00:29:58.080]   Lots of boxes.
[00:29:58.080 --> 00:29:59.080]   Yeah, our cats know that.
[00:29:59.080 --> 00:30:02.960]   When we're going on a trip, they know when the suitcases come out, they pee in them just
[00:30:02.960 --> 00:30:05.280]   to make sure we don't go anywhere.
[00:30:05.280 --> 00:30:09.040]   At least that's how I'm interpreting it.
[00:30:09.040 --> 00:30:10.800]   I like my dog even more now.
[00:30:10.800 --> 00:30:12.720]   Yeah, he doesn't pee in your suitcase.
[00:30:12.720 --> 00:30:14.480]   Isn't that awesome?
[00:30:14.480 --> 00:30:15.920]   So Android Q.
[00:30:15.920 --> 00:30:16.920]   Yes.
[00:30:16.920 --> 00:30:19.880]   We're currently on Pi.
[00:30:19.880 --> 00:30:22.680]   Q which will be QuinceCham.
[00:30:22.680 --> 00:30:23.680]   We don't know.
[00:30:23.680 --> 00:30:26.000]   We can't think of what a dessert begins with Q.
[00:30:26.000 --> 00:30:28.720]   What features do we die for?
[00:30:28.720 --> 00:30:29.720]   There are...
[00:30:29.720 --> 00:30:31.000]   You know, I was thinking the other day, Leo.
[00:30:31.000 --> 00:30:32.240]   We used to on this show.
[00:30:32.240 --> 00:30:36.280]   We used to mark a new version of Chrome.
[00:30:36.280 --> 00:30:38.280]   We don't say that anymore because we're up to 74.
[00:30:38.280 --> 00:30:40.080]   Well, it's more first on.
[00:30:40.080 --> 00:30:41.080]   But we used to do that, right?
[00:30:41.080 --> 00:30:42.080]   Yeah, we did.
[00:30:42.080 --> 00:30:44.080]   I think we're there with the damned...
[00:30:44.080 --> 00:30:45.080]   I feel that way.
[00:30:45.080 --> 00:30:46.960]   Like, Android's pretty much cooked.
[00:30:46.960 --> 00:30:48.680]   It's pretty much done.
[00:30:48.680 --> 00:30:51.760]   So here is an example article from Android Police.
[00:30:51.760 --> 00:30:55.680]   Google's considering replacing Android's back button with a gesture in Android Q.
[00:30:55.680 --> 00:30:57.480]   That, I'm not looking forward to it.
[00:30:57.480 --> 00:30:58.480]   I don't like.
[00:30:58.480 --> 00:30:59.480]   No.
[00:30:59.480 --> 00:31:00.480]   I thought they'd already done this.
[00:31:00.480 --> 00:31:05.200]   Don't you have gestures that go back in Android P?
[00:31:05.200 --> 00:31:10.240]   You're thinking of selecting all the previous things menu.
[00:31:10.240 --> 00:31:11.240]   Yeah.
[00:31:11.240 --> 00:31:14.040]   Well, and also I'm thinking of my Galaxy S10 Plus which has...
[00:31:14.040 --> 00:31:15.040]   The square button is not the square button.
[00:31:15.040 --> 00:31:16.040]   It's all gestures.
[00:31:16.040 --> 00:31:18.680]   So I take on my S10 Plus.
[00:31:18.680 --> 00:31:22.560]   I don't have any indicators at the bottom at all.
[00:31:22.560 --> 00:31:25.400]   If I swipe up from the middle, that's home.
[00:31:25.400 --> 00:31:31.320]   If I swipe up from the right, that's previous, and I love that close all button, even though
[00:31:31.320 --> 00:31:32.880]   it's not necessary.
[00:31:32.880 --> 00:31:36.880]   If I swipe up from the left, it's normally it's the previous thing I did.
[00:31:36.880 --> 00:31:39.520]   Since I haven't done anything previously, I just closed everything.
[00:31:39.520 --> 00:31:40.520]   It's nothing.
[00:31:40.520 --> 00:31:45.880]   But gestures are big on Android, and I think that this isn't surprising.
[00:31:45.880 --> 00:31:47.960]   So this is the video, Android Q.
[00:31:47.960 --> 00:31:53.160]   You know, I still can't get the gesture that says what's open on your Android now.
[00:31:53.160 --> 00:31:54.600]   I keep on messing that up every time.
[00:31:54.600 --> 00:31:55.600]   I can't get that one right.
[00:31:55.600 --> 00:31:57.200]   Yeah, just what you just saw me do it on the...
[00:31:57.200 --> 00:31:58.200]   Now that's an S2 feature.
[00:31:58.200 --> 00:31:59.200]   Do it again.
[00:31:59.200 --> 00:32:00.200]   You swipe up on the right.
[00:32:00.200 --> 00:32:05.680]   So you think of these three buttons as being below the screen, so they're kind of invisible.
[00:32:05.680 --> 00:32:06.680]   And the swipe...
[00:32:06.680 --> 00:32:11.040]   And you can change this on the Samsung, but the swipe is...
[00:32:11.040 --> 00:32:12.640]   So you swipe up from the middle for home.
[00:32:12.640 --> 00:32:14.680]   So let me go to the wrong page.
[00:32:14.680 --> 00:32:15.680]   The swipe of the middle is home.
[00:32:15.680 --> 00:32:16.680]   Well, that wasn't for home.
[00:32:16.680 --> 00:32:18.160]   It's one where you find everything that you have.
[00:32:18.160 --> 00:32:19.640]   Yeah, I'm showing you all three of them.
[00:32:19.640 --> 00:32:23.760]   This one on the right is what you did previously, but I don't have any recently what you use
[00:32:23.760 --> 00:32:26.640]   to use, but I don't know if that's Android or so.
[00:32:26.640 --> 00:32:27.640]   Oh, you see that?
[00:32:27.640 --> 00:32:28.640]   No, here's the problem.
[00:32:28.640 --> 00:32:29.640]   That's not standard.
[00:32:29.640 --> 00:32:33.920]   This is another thing I really love about the S10.
[00:32:33.920 --> 00:32:35.520]   See that little chat head?
[00:32:35.520 --> 00:32:38.400]   A Facebook pioneered these little pop-up notifications.
[00:32:38.400 --> 00:32:42.920]   Now with the S10, I can have every notification be a little pop-up, and I tap it to open it
[00:32:42.920 --> 00:32:47.360]   up, which is great, or I can throw it away by swiping it off the screen somewhere.
[00:32:47.360 --> 00:32:49.160]   I'd rather have fewer notifications.
[00:32:49.160 --> 00:32:50.920]   Well, that's what I do.
[00:32:50.920 --> 00:32:51.920]   So that's Lisa.
[00:32:51.920 --> 00:32:55.480]   See, I see a little picture of Lisa, so I know that that's a text message from Lisa, which
[00:32:55.480 --> 00:32:57.520]   is why, by the way, I'm not opening it.
[00:32:57.520 --> 00:33:00.480]   Okay, she's yelling at me or something.
[00:33:00.480 --> 00:33:02.720]   She thanks you for that.
[00:33:02.720 --> 00:33:06.080]   Okay, let me now see what she's in.
[00:33:06.080 --> 00:33:10.040]   Oh, well, actually, let me show you, because I also have one from the New York Times.
[00:33:10.040 --> 00:33:15.440]   So when I tap Lisa's head, it's opened, and so I can see there's also a news thing.
[00:33:15.440 --> 00:33:17.200]   I will tap that to open it.
[00:33:17.200 --> 00:33:22.720]   And then it opens as a window, which I can make full screen, or I can shrink away, or
[00:33:22.720 --> 00:33:23.800]   I can close away.
[00:33:23.800 --> 00:33:28.520]   I think that's a really nice, that's a Samsung UI feature.
[00:33:28.520 --> 00:33:32.520]   But apparently that will also be in queue, or something similar to that will also be
[00:33:32.520 --> 00:33:33.520]   in queue.
[00:33:33.520 --> 00:33:39.560]   This is the, just if you want to see the gesture, this is the version queue gesture.
[00:33:39.560 --> 00:33:40.840]   Do you see that?
[00:33:40.840 --> 00:33:42.880]   He just swiped up.
[00:33:42.880 --> 00:33:43.880]   He's swiping left, right.
[00:33:43.880 --> 00:33:46.120]   I don't know what he's doing now.
[00:33:46.120 --> 00:33:47.120]   Which is happening.
[00:33:47.120 --> 00:33:50.680]   Unless you swipe from the edge of the screen to go back, just as you would on iPhone 10.
[00:33:50.680 --> 00:33:54.600]   iPhone kind of did pioneer this, they had it.
[00:33:54.600 --> 00:33:59.120]   So it's not enabled by default in the current version of queue.
[00:33:59.120 --> 00:34:02.320]   You have to use an some ADB command.
[00:34:02.320 --> 00:34:03.420]   This is geeky.
[00:34:03.420 --> 00:34:06.080]   Some ADB commands to turn that on.
[00:34:06.080 --> 00:34:08.760]   That's something very, and XDA developers.
[00:34:08.760 --> 00:34:11.200]   Oh, what is that?
[00:34:11.200 --> 00:34:12.200]   Command lines.
[00:34:12.200 --> 00:34:16.040]   ADB shell settings put global prototype enabled one.
[00:34:16.040 --> 00:34:20.440]   ADB shell settings put global squix.
[00:34:20.440 --> 00:34:24.840]   I love how Stacy is writing her own pot for the dog.
[00:34:24.840 --> 00:34:26.680]   Wait, where do you do that?
[00:34:26.680 --> 00:34:27.760]   Where do you go to do that?
[00:34:27.760 --> 00:34:37.080]   So ADB, which is the debugging interface, you plug your phone into Windows or a computer,
[00:34:37.080 --> 00:34:40.920]   and then you can run the ADB program, which you have to download separately, and then
[00:34:40.920 --> 00:34:43.720]   you can issue commands.
[00:34:43.720 --> 00:34:45.040]   That's how you also root it.
[00:34:45.040 --> 00:34:46.360]   You've probably done it because if you've ever--
[00:34:46.360 --> 00:34:50.440]   I've rooted my phone, but I wasn't like 100% sure what I was doing.
[00:34:50.440 --> 00:34:54.560]   Kevin was helping me because he's so nice.
[00:34:54.560 --> 00:34:59.880]   I don't normally care that much about the rumors about what the next version of Android
[00:34:59.880 --> 00:35:04.920]   is going to do because they may or may not do it.
[00:35:04.920 --> 00:35:05.920]   It's one of those.
[00:35:05.920 --> 00:35:06.920]   You can spend a lot of energy.
[00:35:06.920 --> 00:35:09.800]   I got this from doing an Apple podcast for so many years.
[00:35:09.800 --> 00:35:14.360]   You can spend a lot of energy on rumors that never happen.
[00:35:14.360 --> 00:35:18.640]   Well, and it feels like-- I'm thinking about the things in tech right now that I'm excited
[00:35:18.640 --> 00:35:22.160]   about, and they are not things on my phone.
[00:35:22.160 --> 00:35:25.560]   Part of it's because I'm an IoT person, but I'm even thinking about things like VR, and
[00:35:25.560 --> 00:35:27.560]   I'm kind of like--
[00:35:27.560 --> 00:35:28.560]   Well, interesting.
[00:35:28.560 --> 00:35:34.240]   NAB, the big National Association of Broadcasters Conference, which ended last week.
[00:35:34.240 --> 00:35:40.360]   In years previous, last few years, augmented reality, virtual reality, huge, not at all,
[00:35:40.360 --> 00:35:42.320]   hardly at all.
[00:35:42.320 --> 00:35:44.120]   And I think that that portends something.
[00:35:44.120 --> 00:35:45.480]   Now, F8's coming up.
[00:35:45.480 --> 00:35:46.880]   Are you going to go to F8?
[00:35:46.880 --> 00:35:47.880]   No, I'm not now.
[00:35:47.880 --> 00:35:48.880]   Not now.
[00:35:48.880 --> 00:35:49.880]   Not now.
[00:35:49.880 --> 00:35:54.720]   They will announce their new standalone Oculus.
[00:35:54.720 --> 00:35:58.400]   So people are still making these things, but I think that the interest in it has somewhat
[00:35:58.400 --> 00:35:59.760]   died out.
[00:35:59.760 --> 00:36:03.640]   It's just a lot of the shagas.
[00:36:03.640 --> 00:36:07.840]   It's like 3D on TVs and in movies.
[00:36:07.840 --> 00:36:08.840]   Yeah.
[00:36:08.840 --> 00:36:10.600]   It was a big thing, but nobody--
[00:36:10.600 --> 00:36:11.600]   It didn't--
[00:36:11.600 --> 00:36:13.560]   What do we call quadrifonic?
[00:36:13.560 --> 00:36:15.680]   Yeah, quadrif-- actually, that's back, though.
[00:36:15.680 --> 00:36:18.160]   We all have Saran, Dobbly Saran now.
[00:36:18.160 --> 00:36:19.160]   That's true.
[00:36:19.160 --> 00:36:24.680]   Bob Heil, the host of our Ham Show, invented the joystick that the Hu used in quadrifini
[00:36:24.680 --> 00:36:31.960]   to pan the music all around the concert hall.
[00:36:31.960 --> 00:36:32.960]   That's a claim to fame.
[00:36:32.960 --> 00:36:38.480]   Also, I mean, as long as we're talking claim to fame, he also invented the mouth organ
[00:36:38.480 --> 00:36:39.960]   that Peter Frampton used and--
[00:36:39.960 --> 00:36:42.960]   [SINGING]
[00:36:42.960 --> 00:36:43.960]   Wow.
[00:36:43.960 --> 00:36:44.960]   Yeah.
[00:36:44.960 --> 00:36:45.960]   He tells the story.
[00:36:45.960 --> 00:36:46.960]   It's a great story.
[00:36:46.960 --> 00:36:47.960]   Peter Frampton's wife came to him.
[00:36:47.960 --> 00:36:50.000]   Bob was a well-known rock and roll sound guy.
[00:36:50.000 --> 00:36:53.920]   Came to him and said, "I want to give Peter something for his birthday.
[00:36:53.920 --> 00:36:55.480]   Do you have any ideas?"
[00:36:55.480 --> 00:36:57.560]   And Bob said, "Yeah, I can do this."
[00:36:57.560 --> 00:37:02.920]   And he built this thing that takes the mouth cavity, plays the sound from the--
[00:37:02.920 --> 00:37:06.360]   from the guitar into the mouth cavity through a hollow tube.
[00:37:06.360 --> 00:37:12.000]   And then you keep your mouth near the microphone and go like that.
[00:37:12.000 --> 00:37:14.800]   And the sound comes alive.
[00:37:14.800 --> 00:37:17.600]   And Frampton came alive as a result.
[00:37:17.600 --> 00:37:18.600]   Wow.
[00:37:18.600 --> 00:37:22.280]   That is some geeky stuff.
[00:37:22.280 --> 00:37:24.280]   Should we talk about Twitter?
[00:37:24.280 --> 00:37:25.280]   Sure.
[00:37:25.280 --> 00:37:26.280]   Jack Dorely.
[00:37:26.280 --> 00:37:29.440]   Was that only in the Oval Office yesterday?
[00:37:29.440 --> 00:37:30.960]   Hold on.
[00:37:30.960 --> 00:37:32.880]   Was that all we expect from I/O?
[00:37:32.880 --> 00:37:39.560]   It's a major ADDs week.
[00:37:39.560 --> 00:37:41.200]   It's just a squirrel.
[00:37:41.200 --> 00:37:42.200]   Squirrel.
[00:37:42.200 --> 00:37:43.920]   Well, what else might we see at I/O?
[00:37:43.920 --> 00:37:48.160]   I don't actually have any more stories about what we might see at I/O.
[00:37:48.160 --> 00:37:51.360]   There's questions about like, are we going to do stuff with the assistant?
[00:37:51.360 --> 00:37:55.000]   Yeah, so duplex getting better, more assistant voices.
[00:37:55.000 --> 00:37:56.000]   Oh my God.
[00:37:56.000 --> 00:37:58.080]   They'll probably show more of duplex.
[00:37:58.080 --> 00:38:03.320]   Although remember how much heat they got from showing duplex making restaurant reservations
[00:38:03.320 --> 00:38:04.680]   last year.
[00:38:04.680 --> 00:38:09.520]   And then everybody said, ah, you ought to tell people when you're doing that.
[00:38:09.520 --> 00:38:12.920]   And apparently they are rolling it out still.
[00:38:12.920 --> 00:38:13.920]   I don't know.
[00:38:13.920 --> 00:38:17.920]   I think, you know, sometimes I don't want Google to announce new things because I know
[00:38:17.920 --> 00:38:19.920]   they're just going to kill them.
[00:38:19.920 --> 00:38:20.920]   Yep.
[00:38:20.920 --> 00:38:21.920]   It's just--
[00:38:21.920 --> 00:38:23.160]   I think it's important to start those conversations.
[00:38:23.160 --> 00:38:25.320]   I think that's kind of the nice thing about duplex.
[00:38:25.320 --> 00:38:26.320]   But I'm sorry.
[00:38:26.320 --> 00:38:28.000]   I'm not torturing my dog, you guys.
[00:38:28.000 --> 00:38:29.840]   She's just scared of thunderstorms.
[00:38:29.840 --> 00:38:30.840]   And she's just going to whine.
[00:38:30.840 --> 00:38:31.840]   And she's like a little hucky.
[00:38:31.840 --> 00:38:33.520]   Is it thundering, thunder and lightning?
[00:38:33.520 --> 00:38:35.520]   Very, very frightening right now.
[00:38:35.520 --> 00:38:36.520]   It is.
[00:38:36.520 --> 00:38:37.520]   It is.
[00:38:37.520 --> 00:38:39.960]   And there's no buffer.
[00:38:39.960 --> 00:38:43.560]   She's usually-- I mean, I can usually shut the door, but I'm in an apartment only for
[00:38:43.560 --> 00:38:44.560]   like a few weeks.
[00:38:44.560 --> 00:38:45.560]   Sorry, y'all.
[00:38:45.560 --> 00:38:46.560]   No, no, apologize.
[00:38:46.560 --> 00:38:47.560]   Sorry.
[00:38:47.560 --> 00:38:55.480]   We love the sounds of life in our podcast because frankly, we love podcasts unlike the
[00:38:55.480 --> 00:39:01.200]   Washington Post columnist who freaking hates podcasts.
[00:39:01.200 --> 00:39:02.200]   Did you see that today?
[00:39:02.200 --> 00:39:03.200]   No, I didn't see this one.
[00:39:03.200 --> 00:39:05.200]   Is there another rundown?
[00:39:05.200 --> 00:39:06.200]   It should be.
[00:39:06.200 --> 00:39:10.920]   If it isn't, let me find it because-- yeah, no, I didn't put it in the rundown.
[00:39:10.920 --> 00:39:11.920]   Dummy.
[00:39:11.920 --> 00:39:13.920]   We spent about two seconds on Twitter.
[00:39:13.920 --> 00:39:16.520]   No, we're getting back to that.
[00:39:16.520 --> 00:39:17.520]   Believe me.
[00:39:17.520 --> 00:39:18.520]   There's a lot more.
[00:39:18.520 --> 00:39:19.520]   We hit the back button a lot.
[00:39:19.520 --> 00:39:20.520]   There's a lot more.
[00:39:20.520 --> 00:39:21.520]   We hit the back button.
[00:39:21.520 --> 00:39:22.520]   We did.
[00:39:22.520 --> 00:39:24.760]   We hit the back button this one.
[00:39:24.760 --> 00:39:33.280]   This is more like-- I don't know what this is.
[00:39:33.280 --> 00:39:41.240]   This is more like a minor digression to make time for Stacey's dog to get over its fits.
[00:39:41.240 --> 00:39:48.400]   Our podcasts, Killing Music or Just Wasting Our Time-- this is your choice-- Killing Music
[00:39:48.400 --> 00:39:50.920]   or Wasting Time, which is a music thing, right?
[00:39:50.920 --> 00:39:55.360]   It's a pop music critic named Chris Richards.
[00:39:55.360 --> 00:39:59.720]   He says, "I'm about to kick a hornet's nest, and if this were a podcast, you would now
[00:39:59.720 --> 00:40:04.520]   hear the crunch of a boot perforating a hive following by--" I don't know what podcasts
[00:40:04.520 --> 00:40:05.520]   he listens to.
[00:40:05.520 --> 00:40:08.720]   Have we ever sweetened our podcast with side effects like that?
[00:40:08.720 --> 00:40:10.680]   I don't think so.
[00:40:10.680 --> 00:40:13.200]   He says, "He thinks podcasts are--" Radio Lab.
[00:40:13.200 --> 00:40:15.280]   Radio Lab is what he's talking about, yeah.
[00:40:15.280 --> 00:40:16.280]   This is the problem.
[00:40:16.280 --> 00:40:17.480]   He listens to the wrong podcast.
[00:40:17.480 --> 00:40:21.520]   I think they're tedious and say-me and sedative, and when I'm feeling especially cranky, I consider
[00:40:21.520 --> 00:40:24.960]   them an enemy of music.
[00:40:24.960 --> 00:40:29.440]   Most podcasts are conversations for people to eavesdrop on, yes?
[00:40:29.440 --> 00:40:34.080]   Oh, now this is where he's really cutting-- cutting to the quick.
[00:40:34.080 --> 00:40:42.600]   Recorded talk that precludes real-life talk about real life.
[00:40:42.600 --> 00:40:45.240]   What does that mean?
[00:40:45.240 --> 00:40:48.440]   Zombie talk about podcasts.
[00:40:48.440 --> 00:40:51.640]   Also I like music.
[00:40:51.640 --> 00:40:57.560]   He also-- although some of the things he says, I kind of like.
[00:40:57.560 --> 00:41:04.320]   He says, "I hate podcasts because of how they sound.
[00:41:04.320 --> 00:41:09.400]   Forget the lousy microphones and the dinky interstitial stock music.
[00:41:09.400 --> 00:41:16.960]   The thing that derails most podcasts--" and as the king of derailed podcasts, I'm going
[00:41:16.960 --> 00:41:17.960]   to pay attention here.
[00:41:17.960 --> 00:41:18.960]   What is that?
[00:41:18.960 --> 00:41:20.160]   It's the blab.
[00:41:20.160 --> 00:41:24.280]   There are two kinds, more or less-- and by the way, I do not disagree with them on that.
[00:41:24.280 --> 00:41:25.280]   The first is that soft--
[00:41:25.280 --> 00:41:26.280]   What are we doing right now?
[00:41:26.280 --> 00:41:27.280]   Yeah.
[00:41:27.280 --> 00:41:28.280]   Well, we're doing the second kind.
[00:41:28.280 --> 00:41:33.160]   The first is that soft inquisitive staccato, popularized by hourglass in this American
[00:41:33.160 --> 00:41:34.160]   life.
[00:41:34.160 --> 00:41:37.320]   Jeff Jarvis was a normal man on a normal day.
[00:41:37.320 --> 00:41:41.680]   As he walked down the sidewalk, headed to the end.
[00:41:41.680 --> 00:41:44.560]   The source from which so much pod voice appears to have sprung.
[00:41:44.560 --> 00:41:50.440]   The second podcast mode-- this is ours, by the way-- is performative in a different way,
[00:41:50.440 --> 00:41:55.600]   and you hear it on most round table podcasts, a tone that people use at parties when they
[00:41:55.600 --> 00:42:00.280]   want to be heard by people that aren't necessarily talking to.
[00:42:00.280 --> 00:42:01.560]   And it's pretty much one or the other.
[00:42:01.560 --> 00:42:07.680]   Be podcasted too in a cozy, overly considered way, or be podcasted in a hasty or less considered
[00:42:07.680 --> 00:42:08.680]   way.
[00:42:08.680 --> 00:42:11.480]   I prefer to be the hasty or less considered podcast.
[00:42:11.480 --> 00:42:13.720]   That's club hasty and less considered.
[00:42:13.720 --> 00:42:14.720]   Whoo-hoo!
[00:42:14.720 --> 00:42:17.800]   Guilty as charged.
[00:42:17.800 --> 00:42:22.400]   So I can't completely disagree with them, except for the part where podcasts are killing
[00:42:22.400 --> 00:42:23.400]   music.
[00:42:23.400 --> 00:42:26.280]   I hope we're not killing music, and I don't think we're killing music.
[00:42:26.280 --> 00:42:32.640]   I think there's a place for people who can listen to a good podcast, or for a lot of
[00:42:32.640 --> 00:42:36.760]   people a good podcast, is listening in on your friends have a conversation about something.
[00:42:36.760 --> 00:42:39.480]   I think that's what our shows are, at least we're trying to be.
[00:42:39.480 --> 00:42:45.080]   Yeah, and I think there's a place for that, especially if you don't have friends who are
[00:42:45.080 --> 00:42:49.200]   necessarily interested in the same things you are.
[00:42:49.200 --> 00:42:50.200]   I don't know.
[00:42:50.200 --> 00:42:54.240]   I listen to people talk-- I listen to podcasts about pop culture and stuff, because I don't
[00:42:54.240 --> 00:42:57.280]   really have a lot of friends where I can sit there and talk about the latest in fashion
[00:42:57.280 --> 00:42:58.280]   with.
[00:42:58.280 --> 00:43:02.040]   And I like them.
[00:43:02.040 --> 00:43:03.040]   Podcasts are just--
[00:43:03.040 --> 00:43:04.040]   And I still talk to real people.
[00:43:04.040 --> 00:43:05.040]   One more way.
[00:43:05.040 --> 00:43:14.160]   And in this case, a very, I think, democratic egalitarian way of sharing ideas and voices,
[00:43:14.160 --> 00:43:18.960]   in case of podcast, voices that are often not heard elsewhere.
[00:43:18.960 --> 00:43:20.720]   And they're very intimate.
[00:43:20.720 --> 00:43:25.040]   And I think a lot of people, certainly, are listening, are listening in the car when they're
[00:43:25.040 --> 00:43:29.280]   on their way to work.
[00:43:29.280 --> 00:43:30.560]   We compete with audiobooks.
[00:43:30.560 --> 00:43:32.240]   You could say audiobooks are killing music.
[00:43:32.240 --> 00:43:36.840]   We compete with all kinds of different ear entertainment.
[00:43:36.840 --> 00:43:37.840]   And I think that's good.
[00:43:37.840 --> 00:43:43.040]   You have a lot more choices when you're sitting in your car stuck on the 404 than you used
[00:43:43.040 --> 00:43:44.280]   to, which is great.
[00:43:44.280 --> 00:43:48.440]   But listening to radio voices telling you the next song was awful.
[00:43:48.440 --> 00:43:49.600]   Broadcast Radio was terrible.
[00:43:49.600 --> 00:43:50.600]   It needed to replace it.
[00:43:50.600 --> 00:43:51.600]   I don't know.
[00:43:51.600 --> 00:43:52.600]   Radio killed music.
[00:43:52.600 --> 00:43:53.600]   Radio killed music.
[00:43:53.600 --> 00:43:54.600]   Radio killed music.
[00:43:54.600 --> 00:43:55.600]   Radio killed music.
[00:43:55.600 --> 00:43:56.600]   No, I agree.
[00:43:56.600 --> 00:43:57.600]   I agree.
[00:43:57.600 --> 00:43:58.600]   Yeah.
[00:43:58.600 --> 00:43:59.600]   Yeah.
[00:43:59.600 --> 00:44:02.880]   I used to like-- there was a morning show I used to like here.
[00:44:02.880 --> 00:44:04.040]   They don't do it anymore.
[00:44:04.040 --> 00:44:07.240]   But I always thought those were kind of silly fun.
[00:44:07.240 --> 00:44:08.240]   But that's just--
[00:44:08.240 --> 00:44:09.240]   Listen to Howard Stern.
[00:44:09.240 --> 00:44:15.240]   Now, would you pay 50 pounds for a podcast recorded on a wax cylinder?
[00:44:15.240 --> 00:44:16.240]   No.
[00:44:16.240 --> 00:44:17.240]   No.
[00:44:17.240 --> 00:44:18.960]   Hello internet.
[00:44:18.960 --> 00:44:26.400]   This podcast, two minutes in duration, stored in a solid cardboard tube.
[00:44:26.400 --> 00:44:35.800]   Yeah, one cylinder per order because, well, they're hard to make.
[00:44:35.800 --> 00:44:37.520]   They're using these giant--
[00:44:37.520 --> 00:44:38.520]   A gamma phone.
[00:44:38.520 --> 00:44:39.520]   Yeah.
[00:44:39.520 --> 00:44:42.520]   On the 3rd of April, 2019.
[00:44:42.520 --> 00:44:43.520]   OK.
[00:44:43.520 --> 00:44:44.520]   Hi there, everyone.
[00:44:44.520 --> 00:44:53.000]   This is Brady recording live onto a wax cylinder record to promote our Hello internet episode,
[00:44:53.000 --> 00:44:56.080]   which is going to be released on a wax cylinder.
[00:44:56.080 --> 00:45:00.880]   Ironically, they are also recording it with high quality audio and video equipment across
[00:45:00.880 --> 00:45:03.320]   the room so that we can see it on YouTube.
[00:45:03.320 --> 00:45:06.000]   Anyway, I like that.
[00:45:06.000 --> 00:45:07.480]   I think it's funny that they did it.
[00:45:07.480 --> 00:45:10.440]   I wouldn't pay 50 pounds for it.
[00:45:10.440 --> 00:45:12.880]   Not even to send it to Locker now?
[00:45:12.880 --> 00:45:15.200]   Did I send it to Chris Perilla?
[00:45:15.200 --> 00:45:18.400]   You think you'd enjoy it?
[00:45:18.400 --> 00:45:21.800]   He's the one who broke the Edison cylinder.
[00:45:21.800 --> 00:45:22.800]   Yeah.
[00:45:22.800 --> 00:45:24.840]   I know I got the reference.
[00:45:24.840 --> 00:45:29.680]   Although I'm sad to say that that was a hoax, the whole thing.
[00:45:29.680 --> 00:45:34.760]   Although you can see it and watch it on YouTube if you'd like to relive it.
[00:45:34.760 --> 00:45:39.240]   By the way, he also says this guy, writing the Washington Post, like it or not, podcasts
[00:45:39.240 --> 00:45:44.360]   are invading a sound world near you, the marquee names of podcasting, Pod Save America, the
[00:45:44.360 --> 00:45:47.040]   Joe Button podcast, and terrible.
[00:45:47.040 --> 00:45:49.040]   Thanks for asking.
[00:45:49.040 --> 00:45:50.480]   I'm familiar with one of them.
[00:45:50.480 --> 00:45:53.640]   And many others are currently appearing on actual marquee's bringing the discussions in
[00:45:53.640 --> 00:45:57.640]   a nightclubs and theaters usually dedicated to live music.
[00:45:57.640 --> 00:45:58.640]   How dare they?
[00:45:58.640 --> 00:46:02.680]   Okay, I will say I went to a wait, wait, don't tell me broadcast.
[00:46:02.680 --> 00:46:03.680]   Awesome, right?
[00:46:03.680 --> 00:46:04.680]   Awesome.
[00:46:04.680 --> 00:46:05.680]   Awesome.
[00:46:05.680 --> 00:46:06.680]   It's so much fun.
[00:46:06.680 --> 00:46:07.680]   Was Carl Castle still there?
[00:46:07.680 --> 00:46:09.840]   Or he had passed away?
[00:46:09.840 --> 00:46:10.840]   He had passed by then.
[00:46:10.840 --> 00:46:11.840]   Yeah.
[00:46:11.840 --> 00:46:12.840]   Well, maybe no.
[00:46:12.840 --> 00:46:18.400]   I would have liked to have seen Carl Castle before he took the big walk to the radio microphone
[00:46:18.400 --> 00:46:21.800]   in the sky or whatever it is we're headed for.
[00:46:21.800 --> 00:46:24.120]   All right, take a break.
[00:46:24.120 --> 00:46:27.160]   Then we will talk about Twitter.
[00:46:27.160 --> 00:46:32.560]   Then we will talk about I'm actually saving my outrage because there's some outrageous
[00:46:32.560 --> 00:46:34.400]   Facebook stuff outrageous.
[00:46:34.400 --> 00:46:38.920]   Can we also talk about the galaxy fold or is that crazy?
[00:46:38.920 --> 00:46:40.160]   No, we could.
[00:46:40.160 --> 00:46:41.160]   We really should.
[00:46:41.160 --> 00:46:42.160]   Yeah.
[00:46:42.160 --> 00:46:47.560]   Because I was going to go to the T-Mobile store on Friday and buy one.
[00:46:47.560 --> 00:46:52.160]   Samsung saved my $2,000.
[00:46:52.160 --> 00:46:54.320]   Thank you, Samsung.
[00:46:54.320 --> 00:46:56.680]   And AOC.
[00:46:56.680 --> 00:47:00.800]   She's quitting Facebook and she says you should too.
[00:47:00.800 --> 00:47:01.800]   Socialist.
[00:47:01.800 --> 00:47:07.680]   Meanwhile, I'm going to show you how you come to our studio.
[00:47:07.680 --> 00:47:10.280]   When you come to our studio and you welcome, we have an open studio.
[00:47:10.280 --> 00:47:11.360]   We love a studio audience.
[00:47:11.360 --> 00:47:16.120]   Email tickets@twit.tv so we know to put a chair out for you.
[00:47:16.120 --> 00:47:21.280]   In the past, when you've come to our studio, we have counted you a fairly long double-sided
[00:47:21.280 --> 00:47:25.840]   document, a legal disclaimer and required that you read through it, but sign it before
[00:47:25.840 --> 00:47:27.240]   you enter our studio.
[00:47:27.240 --> 00:47:35.520]   We have simplified the process considerably thanks to Envoy, which is an awesome iPad app
[00:47:35.520 --> 00:47:38.040]   designed for signing people.
[00:47:38.040 --> 00:47:42.800]   This is Envoy visitors.
[00:47:42.800 --> 00:47:43.800]   They have other apps as well.
[00:47:43.800 --> 00:47:49.040]   In fact, there's a delivery app that the delivery guy, the UPS or FedEx guy, can say,
[00:47:49.040 --> 00:47:50.840]   you know, show what he's delivered.
[00:47:50.840 --> 00:47:55.360]   And then it notifies the employee that their package is at the front desk.
[00:47:55.360 --> 00:47:58.480]   This notifies the employee that their guests have arrived.
[00:47:58.480 --> 00:47:59.720]   It's nice to see it has our logo.
[00:47:59.720 --> 00:48:01.200]   You tap to sign in.
[00:48:01.200 --> 00:48:03.960]   You put in your name.
[00:48:03.960 --> 00:48:08.960]   It's completely replacing all that paperwork we used to do or the paper logbook that's completely
[00:48:08.960 --> 00:48:10.080]   useless.
[00:48:10.080 --> 00:48:11.520]   You could sign in unlimited visitors.
[00:48:11.520 --> 00:48:15.680]   You can, if you want, connect it to a printer print customized badges.
[00:48:15.680 --> 00:48:17.520]   You could set up VIP names.
[00:48:17.520 --> 00:48:22.520]   You could say, if Jeff Jarvis signs in, please sound the alarm and get everybody down the
[00:48:22.520 --> 00:48:24.360]   hall to welcome them.
[00:48:24.360 --> 00:48:28.480]   It will even, for security reasons, notify administrators about unattended guests.
[00:48:28.480 --> 00:48:30.440]   When visitors arrive, they sign in on the kiosk.
[00:48:30.440 --> 00:48:32.000]   They enter their details.
[00:48:32.000 --> 00:48:35.720]   They sign any legal documents and DAs right on the iPad.
[00:48:35.720 --> 00:48:38.400]   Envoy will then notify their host automatically.
[00:48:38.400 --> 00:48:43.080]   It uses all the tools you probably already use, like Slack or Box or Salesforce Chatter
[00:48:43.080 --> 00:48:45.760]   or G Suite.
[00:48:45.760 --> 00:48:48.920]   So your employee knows a guest is there so you don't have to call them or anything.
[00:48:48.920 --> 00:48:51.040]   They come down the hall.
[00:48:51.040 --> 00:48:56.360]   By the way, no more awkward meetings coming down the hall, not expecting somebody in there.
[00:48:56.360 --> 00:48:57.360]   He isn't a lobby.
[00:48:57.360 --> 00:48:59.520]   You don't have to worry about that either.
[00:48:59.520 --> 00:49:01.040]   You'll boost your office efficiency.
[00:49:01.040 --> 00:49:05.040]   You'll add to security without intimidating your visitors.
[00:49:05.040 --> 00:49:08.640]   In fact, visitors, we know because we've used this for a while now.
[00:49:08.640 --> 00:49:09.960]   Love it.
[00:49:09.960 --> 00:49:16.640]   It's a great way to make your office both more welcoming and more secure.
[00:49:16.640 --> 00:49:21.800]   And it works for all companies of all kinds, even those in highly regulated industries
[00:49:21.800 --> 00:49:25.280]   because it meets security and compliance requirements.
[00:49:25.280 --> 00:49:31.240]   So many companies, Nike, you see the Nike Sush Mazda, Weight Watchers, Pinterest, Slack,
[00:49:31.240 --> 00:49:33.720]   L'Oreal and Twit.
[00:49:33.720 --> 00:49:38.600]   We all use Envoy to welcome over 30 million visitors so far.
[00:49:38.600 --> 00:49:42.760]   Protect your people, your property and your ideas, the welcoming way with Envoy.
[00:49:42.760 --> 00:49:47.520]   10,000 people, 10,000 workplaces use Envoy Worldwide to make a great first impression.
[00:49:47.520 --> 00:49:50.360]   I love it because we didn't have to buy additional hardware.
[00:49:50.360 --> 00:49:52.440]   It runs in a normal iPad.
[00:49:52.440 --> 00:49:54.000]   Envoy is the software.
[00:49:54.000 --> 00:49:58.040]   And you just put it on any iPad and most of us have a few iPads lying around.
[00:49:58.040 --> 00:50:01.920]   Envoy.com/Twit, start your free trial today.
[00:50:01.920 --> 00:50:06.200]   Welcome to the modern lobby with Envoy Envoy Envoy.
[00:50:06.200 --> 00:50:08.280]   Envoy.com/Twit.
[00:50:08.280 --> 00:50:09.480]   Thank you Envoy for your support.
[00:50:09.480 --> 00:50:11.120]   Thank you for supporting us.
[00:50:11.120 --> 00:50:15.040]   By using that URL, if you want to check out Envoy, it really is great.
[00:50:15.040 --> 00:50:16.120]   Just love it.
[00:50:16.120 --> 00:50:21.360]   I've been actually looking for something to do this ever since we started having a guard
[00:50:21.360 --> 00:50:24.600]   at the front desk and we have a big sign in process.
[00:50:24.600 --> 00:50:28.720]   You have to these days, unfortunately, for legal and security reasons.
[00:50:28.720 --> 00:50:31.680]   But this really simplifies it.
[00:50:31.680 --> 00:50:34.040]   Galaxy Fold.
[00:50:34.040 --> 00:50:39.160]   Man, it was so funny because the first initial thought says the Galaxy Fold started to go
[00:50:39.160 --> 00:50:45.280]   out to reviewers like Steve Kovac and Mark Hess Brownlee, Joanna Stern at the Journal
[00:50:45.280 --> 00:50:48.960]   and Dieter Bohn, Mark Gurman at Bloomberg.
[00:50:48.960 --> 00:50:51.960]   The first day they go, "Oh, I get it.
[00:50:51.960 --> 00:50:52.960]   I get it.
[00:50:52.960 --> 00:50:57.120]   This is exactly what you want a phone to be simple and small for the basic stuff, but
[00:50:57.120 --> 00:51:01.280]   you can open it up and get a nice 7+ inch screen."
[00:51:01.280 --> 00:51:03.680]   That was day one.
[00:51:03.680 --> 00:51:04.680]   Day two.
[00:51:04.680 --> 00:51:07.560]   It broke.
[00:51:07.560 --> 00:51:09.320]   I mean, for different reasons.
[00:51:09.320 --> 00:51:17.960]   So Mark Hess Brownlee and Steven Kovac removed a plastic coating on top of it, which I don't
[00:51:17.960 --> 00:51:22.280]   blame them for doing when I got my Galaxy S10 and many Samsung phones.
[00:51:22.280 --> 00:51:24.440]   They have a pre-installed screen protector.
[00:51:24.440 --> 00:51:27.160]   I don't like screen protectors, so I peel them off.
[00:51:27.160 --> 00:51:29.160]   I did that on my S10.
[00:51:29.160 --> 00:51:30.880]   Turns out that wasn't a screen protector.
[00:51:30.880 --> 00:51:33.680]   Well, it was in a way.
[00:51:33.680 --> 00:51:36.760]   Well, it literally protected the screen.
[00:51:36.760 --> 00:51:40.040]   Here's a fix this article about this.
[00:51:40.040 --> 00:51:41.040]   It was necessary.
[00:51:41.040 --> 00:51:43.200]   See, the problem with flexible screens, they have to be OLED screens.
[00:51:43.200 --> 00:51:48.520]   They're very fragile, so they have a layer of plastic on top of it, but they're also
[00:51:48.520 --> 00:51:49.520]   sealing it.
[00:51:49.520 --> 00:51:52.680]   This is the other problem with the Galaxy Fold according to IFixit.
[00:51:52.680 --> 00:51:58.360]   When you close it, when you see that hinge, you can see that the screen is not fully sealed.
[00:51:58.360 --> 00:52:00.280]   That was the problem Dieter Bohn had at the verge.
[00:52:00.280 --> 00:52:02.440]   He said, "We think what happened.
[00:52:02.440 --> 00:52:07.920]   When we take pictures for our reviews, we use a little molding putty underneath the device
[00:52:07.920 --> 00:52:09.680]   to hold it in place or hold it up.
[00:52:09.680 --> 00:52:14.920]   We think that got into the hinge and then that caused a protrusion on the screen."
[00:52:14.920 --> 00:52:19.360]   In any event, when you send out, I don't know what they sent out, a couple of dozen,
[00:52:19.360 --> 00:52:21.120]   we didn't get one, but I know a lot of people didn't.
[00:52:21.120 --> 00:52:23.200]   In fact, Renee Richie said they had two mobile nations.
[00:52:23.200 --> 00:52:28.040]   When you send out, say, a couple of dozen phones and half of them get broken in the first
[00:52:28.040 --> 00:52:30.480]   two days, you know you have a problem.
[00:52:30.480 --> 00:52:33.720]   Samsung said, "We're not going to launch that phone Friday.
[00:52:33.720 --> 00:52:37.360]   We're going to step back and figure out what's going on."
[00:52:37.360 --> 00:52:38.960]   How many they made?
[00:52:38.960 --> 00:52:42.360]   I was impressed because Samsung originally was like, "We're not delaying the launch."
[00:52:42.360 --> 00:52:46.600]   Then a few hours later, they're like, "Oh, yeah, okay, thank you."
[00:52:46.600 --> 00:52:47.600]   They had two.
[00:52:47.600 --> 00:52:48.600]   Well, that's what everyone...
[00:52:48.600 --> 00:52:50.040]   Because that's what I thought.
[00:52:50.040 --> 00:52:51.720]   I was like, "Wow, this is a...
[00:52:51.720 --> 00:52:52.720]   I mean, it's pretty bad."
[00:52:52.720 --> 00:52:56.320]   You don't want the Note 7 thing all over again, right?
[00:52:56.320 --> 00:52:57.320]   No.
[00:52:57.320 --> 00:53:00.320]   They had to recall every single Note 7.
[00:53:00.320 --> 00:53:02.880]   These phones weren't blowing up, but yeah, it was still bad.
[00:53:02.880 --> 00:53:05.080]   You would still have to recall it if you found it.
[00:53:05.080 --> 00:53:06.080]   You would have to...
[00:53:06.080 --> 00:53:07.960]   Especially at $2,000 of money.
[00:53:07.960 --> 00:53:08.960]   Yeah.
[00:53:08.960 --> 00:53:14.080]   So, AT&T representatives say June 13th.
[00:53:14.080 --> 00:53:16.040]   So it's not that long.
[00:53:16.040 --> 00:53:19.480]   It's a month and a half later.
[00:53:19.480 --> 00:53:21.200]   Samsung says, "No, we haven't set a date."
[00:53:21.200 --> 00:53:23.360]   But AT&T is adamant.
[00:53:23.360 --> 00:53:32.640]   So, I think it's reasonable that AT&T thinks it's going to be then, but Samsung wants to
[00:53:32.640 --> 00:53:38.040]   reserve the right not to ship it if they think it'll be any problem.
[00:53:38.040 --> 00:53:39.040]   Save me to Grant.
[00:53:39.040 --> 00:53:43.200]   That's all I'm saying, because I was going to go into T-Mobile on Friday and buy one,
[00:53:43.200 --> 00:53:45.440]   because I wanted to talk about it.
[00:53:45.440 --> 00:53:50.320]   I was kind of excited because it is the first really new form factor for a phone we've seen
[00:53:50.320 --> 00:53:51.640]   in a while.
[00:53:51.640 --> 00:53:52.840]   I was super excited.
[00:53:52.840 --> 00:53:54.680]   I was so bummed when I started seeing this.
[00:53:54.680 --> 00:54:00.360]   I was like, "Ah, I really want to see new form factor."
[00:54:00.360 --> 00:54:02.520]   I really thought they had it right, I guess.
[00:54:02.520 --> 00:54:06.760]   I was like Samsung wouldn't put out something that wasn't engineered and considered.
[00:54:06.760 --> 00:54:08.280]   This isn't like...
[00:54:08.280 --> 00:54:10.200]   What was Andy Rubin's company?
[00:54:10.200 --> 00:54:12.080]   I can't think of it right now.
[00:54:12.080 --> 00:54:13.080]   Essential.
[00:54:13.080 --> 00:54:14.080]   Essential.
[00:54:14.080 --> 00:54:15.080]   This is a Samsung.
[00:54:15.080 --> 00:54:18.480]   They know what they're doing, but no.
[00:54:18.480 --> 00:54:20.920]   That is a bit of a question.
[00:54:20.920 --> 00:54:24.360]   How could you not see this coming?
[00:54:24.360 --> 00:54:26.000]   Did you never let it out of the lab?
[00:54:26.000 --> 00:54:27.200]   Didn't you give it to employees?
[00:54:27.200 --> 00:54:28.200]   Apple does this.
[00:54:28.200 --> 00:54:30.080]   We know because employees have lost phones.
[00:54:30.080 --> 00:54:34.000]   Other companies do it to take into the wild and use for several months before you ship
[00:54:34.000 --> 00:54:35.360]   the phone.
[00:54:35.360 --> 00:54:36.640]   They must have.
[00:54:36.640 --> 00:54:37.800]   It's puzzling.
[00:54:37.800 --> 00:54:40.600]   Is it just because the cover was taken off by people?
[00:54:40.600 --> 00:54:41.600]   No.
[00:54:41.600 --> 00:54:42.600]   No.
[00:54:42.600 --> 00:54:43.600]   There was another one too.
[00:54:43.600 --> 00:54:44.600]   There was several.
[00:54:44.600 --> 00:54:45.600]   I thought there were a lot.
[00:54:45.600 --> 00:54:51.960]   The turn started to peel hers back at the Wall Street Journal and then said maybe not.
[00:54:51.960 --> 00:54:58.000]   Mark Hiz Brownlee and Stephen Kovac said yes, we took it off, but Dieter Bohn did not and
[00:54:58.000 --> 00:55:00.200]   he still got damaged.
[00:55:00.200 --> 00:55:01.600]   Mark Gurman didn't.
[00:55:01.600 --> 00:55:03.400]   Mark Gurman didn't.
[00:55:03.400 --> 00:55:04.400]   There's a problem.
[00:55:04.400 --> 00:55:09.880]   The problem is probably that stuff can get in under the OLED screen.
[00:55:09.880 --> 00:55:12.960]   It is super fragile.
[00:55:12.960 --> 00:55:22.800]   It's super fragile and you don't want anything to get in there.
[00:55:22.800 --> 00:55:25.920]   One of the things Samsung said initially is we're going to put a big warning on there.
[00:55:25.920 --> 00:55:29.680]   We should have told these guys, there's a debate about whether they told the reviewers
[00:55:29.680 --> 00:55:33.360]   not to, but we're going to make sure that everybody gets this phone.
[00:55:33.360 --> 00:55:34.920]   No, don't remove that.
[00:55:34.920 --> 00:55:36.760]   That's part of the screen.
[00:55:36.760 --> 00:55:42.360]   By the way, does it lock open?
[00:55:42.360 --> 00:55:45.160]   Oh, I don't know.
[00:55:45.160 --> 00:55:47.160]   Oh, when you're working with it?
[00:55:47.160 --> 00:55:48.160]   I don't think so.
[00:55:48.160 --> 00:55:51.640]   When you want to watch a movie or something, you have to hold both sides open.
[00:55:51.640 --> 00:55:53.360]   I always wondered that.
[00:55:53.360 --> 00:55:54.360]   I don't know.
[00:55:54.360 --> 00:55:57.720]   I also read a number of commentaries about the fact that everybody has a different way
[00:55:57.720 --> 00:55:58.720]   of closing it.
[00:55:58.720 --> 00:56:00.720]   There's no obvious way of closing it.
[00:56:00.720 --> 00:56:02.880]   Do you just be outside?
[00:56:02.880 --> 00:56:07.520]   Or, well, some people might do it like that.
[00:56:07.520 --> 00:56:14.480]   Samsung, when they have its testing machine, in the testing machine, it's even pressure
[00:56:14.480 --> 00:56:17.920]   along the entire machine.
[00:56:17.920 --> 00:56:19.560]   It's not a good...
[00:56:19.560 --> 00:56:20.560]   People are not going to do that.
[00:56:20.560 --> 00:56:21.560]   They're going to whack it closed.
[00:56:21.560 --> 00:56:22.560]   Look at how...
[00:56:22.560 --> 00:56:27.200]   I have a case like I have here, which is a case of cover.
[00:56:27.200 --> 00:56:30.640]   There's lots of ways of closing that case, and I probably use all of them.
[00:56:30.640 --> 00:56:34.320]   If I'm holding the phone like that, I might do that.
[00:56:34.320 --> 00:56:38.400]   These test machines probably didn't catch this potential problem.
[00:56:38.400 --> 00:56:41.120]   In any event, I feel bad for Samsung.
[00:56:41.120 --> 00:56:42.120]   I've said this before.
[00:56:42.120 --> 00:56:43.600]   I love the S10 Plus.
[00:56:43.600 --> 00:56:45.040]   It's the best phone I've ever had.
[00:56:45.040 --> 00:56:46.640]   It's just incredible.
[00:56:46.640 --> 00:56:47.800]   I was really hopeful.
[00:56:47.800 --> 00:56:49.520]   I was willing to set...
[00:56:49.520 --> 00:56:51.120]   Of course, I have a budget for this.
[00:56:51.120 --> 00:56:57.360]   This is my job, but I was willing to put aside $2,000 to get this phone.
[00:56:57.360 --> 00:57:05.000]   I was willing to support you spending $2,000 just to see.
[00:57:05.000 --> 00:57:06.000]   So there you go.
[00:57:06.000 --> 00:57:07.840]   Anything else you want to stay see about it?
[00:57:07.840 --> 00:57:08.840]   It's sad.
[00:57:08.840 --> 00:57:11.000]   Yeah, I'm just disappointed.
[00:57:11.000 --> 00:57:12.000]   Yeah.
[00:57:12.000 --> 00:57:13.000]   It's not over.
[00:57:13.000 --> 00:57:14.000]   It's sad.
[00:57:14.000 --> 00:57:15.340]   You know what else is disappointing?
[00:57:15.340 --> 00:57:19.640]   Sprint settled with AT&T over 5GE.
[00:57:19.640 --> 00:57:25.760]   AT&T is continuing to use the 5GE label on their phones.
[00:57:25.760 --> 00:57:32.200]   This suit saying it's blatantly misleading, but at some point settled.
[00:57:32.200 --> 00:57:33.200]   I don't know why they settled.
[00:57:33.200 --> 00:57:36.240]   I don't know why they sued if they planned to settle.
[00:57:36.240 --> 00:57:41.160]   They didn't settle in a way that kept AT&T from using it.
[00:57:41.160 --> 00:57:44.440]   I think it hurts AT&T, honestly.
[00:57:44.440 --> 00:57:46.440]   Although...
[00:57:46.440 --> 00:57:47.440]   Yeah.
[00:57:47.440 --> 00:57:48.440]   But no, it didn't.
[00:57:48.440 --> 00:57:52.400]   I mean, remember, again, HSPA+ that did not hurt them.
[00:57:52.400 --> 00:57:54.440]   People aren't that techie.
[00:57:54.440 --> 00:57:57.520]   I just had someone ask me about, "What was it?"
[00:57:57.520 --> 00:58:02.760]   Oh, my cable installer today was telling me he's like, "Oh, you know..."
[00:58:02.760 --> 00:58:04.520]   Cable installer finally came?
[00:58:04.520 --> 00:58:05.760]   Finally came, yes.
[00:58:05.760 --> 00:58:07.960]   I saw the dog waiting for the cable installer.
[00:58:07.960 --> 00:58:09.360]   Sorry, but this is more important news.
[00:58:09.360 --> 00:58:10.360]   Was that on Twitter?
[00:58:10.360 --> 00:58:12.000]   I saw the dog waiting for the installer.
[00:58:12.000 --> 00:58:14.000]   I miss all the good stuff.
[00:58:14.000 --> 00:58:15.000]   Yeah.
[00:58:15.000 --> 00:58:17.840]   Yeah, he was supposed to come or the cable person was supposed to come Monday and they
[00:58:17.840 --> 00:58:18.840]   never did.
[00:58:18.840 --> 00:58:19.840]   And then we...
[00:58:19.840 --> 00:58:20.840]   That's a good thing to use.
[00:58:20.840 --> 00:58:24.160]   By the way, that is one of the best reasons to have a Twitter account.
[00:58:24.160 --> 00:58:25.160]   Stuff like that.
[00:58:25.160 --> 00:58:26.160]   Yes.
[00:58:26.160 --> 00:58:30.280]   So I posted a picture of my dog looking adorable waiting for the cable person.
[00:58:30.280 --> 00:58:31.280]   But...
[00:58:31.280 --> 00:58:32.280]   I'm going to start.
[00:58:32.280 --> 00:58:33.480]   We're sitting there today.
[00:58:33.480 --> 00:58:37.680]   He's installed the network and it was a 2.4 GHz and a 5 GHz network.
[00:58:37.680 --> 00:58:39.440]   And I was like, "Oh, are they the same?
[00:58:39.440 --> 00:58:41.200]   Do I need to have...
[00:58:41.200 --> 00:58:42.200]   Is it an extender?
[00:58:42.200 --> 00:58:44.240]   Do I have to have a separate 5 GHz network?"
[00:58:44.240 --> 00:58:45.400]   He's like, "Oh, no, no.
[00:58:45.400 --> 00:58:47.040]   This is a 5G network."
[00:58:47.040 --> 00:58:48.040]   And I was like...
[00:58:48.040 --> 00:58:49.040]   Oh, please.
[00:58:49.040 --> 00:58:50.040]   Oh, please.
[00:58:50.040 --> 00:58:51.040]   And he didn't...
[00:58:51.040 --> 00:58:52.040]   He didn't...
[00:58:52.040 --> 00:58:54.640]   He wasn't maliciously saying that.
[00:58:54.640 --> 00:58:55.640]   No, no.
[00:58:55.640 --> 00:58:57.760]   He's like, "You know, the 5G network's really fast."
[00:58:57.760 --> 00:58:59.000]   And I was like, "It's true.
[00:58:59.000 --> 00:59:01.000]   The 5 GHz network is really fast."
[00:59:01.000 --> 00:59:03.760]   He's like, "I don't know why people use the 2.4 network.
[00:59:03.760 --> 00:59:04.760]   It's silly."
[00:59:04.760 --> 00:59:08.440]   And I was like, "Well, it goes farther."
[00:59:08.440 --> 00:59:09.440]   But yeah.
[00:59:09.440 --> 00:59:10.960]   So he was all excited about 5G.
[00:59:10.960 --> 00:59:11.960]   He's like, "Harry, what's talking about it?"
[00:59:11.960 --> 00:59:13.960]   And I was like, "No."
[00:59:13.960 --> 00:59:18.160]   So, you know, there's a lot of confusion out there is the point.
[00:59:18.160 --> 00:59:20.320]   On all 5G, 6G, LTE.
[00:59:20.320 --> 00:59:21.320]   Well, giggerhoods.
[00:59:21.320 --> 00:59:23.360]   So I'm going to give you an I/O, by the way.
[00:59:23.360 --> 00:59:28.120]   I'm really interested to see that your dog wears Nike shower shoes, much like Mark Zuckerberg.
[00:59:28.120 --> 00:59:29.120]   Okay.
[00:59:29.120 --> 00:59:31.320]   Those are soccer shoes.
[00:59:31.320 --> 00:59:33.760]   Okay, sure.
[00:59:33.760 --> 00:59:36.320]   So your dog wears Nike soccer shoes.
[00:59:36.320 --> 00:59:37.320]   Adidas.
[00:59:37.320 --> 00:59:38.320]   Adidas, I mean.
[00:59:38.320 --> 00:59:39.320]   Not Nike.
[00:59:39.320 --> 00:59:40.320]   Yes.
[00:59:40.320 --> 00:59:41.320]   Yeah.
[00:59:41.320 --> 00:59:42.320]   Very important to get the brand right.
[00:59:42.320 --> 00:59:43.320]   It is.
[00:59:43.320 --> 00:59:44.320]   It is.
[00:59:44.320 --> 00:59:47.080]   She doesn't want to get a ringworm in the shower, I guess, or whatever it is that was
[00:59:47.080 --> 00:59:48.080]   prevent.
[00:59:48.080 --> 00:59:52.680]   Here is an IOT tip you can use in your newsletter.
[00:59:52.680 --> 00:59:56.520]   I had somebody call the radio show and say, you know, some of these IOT devices don't
[00:59:56.520 --> 01:00:03.800]   work with 5G, but, but, and this is the case of, for me at home, we have a 5G and 2.4G,
[01:00:03.800 --> 01:00:05.520]   but they're the same name.
[01:00:05.520 --> 01:00:07.440]   And some IOT devices are...
[01:00:07.440 --> 01:00:08.440]   I'm saying 5G.
[01:00:08.440 --> 01:00:09.440]   Did I say 5G?
[01:00:09.440 --> 01:00:10.440]   5G?
[01:00:10.440 --> 01:00:11.440]   I'm just saying.
[01:00:11.440 --> 01:00:12.440]   5G-hertz.
[01:00:12.440 --> 01:00:13.440]   5G-hertz.
[01:00:13.440 --> 01:00:14.440]   5G-hertz.
[01:00:14.440 --> 01:00:15.440]   You just did it.
[01:00:15.440 --> 01:00:17.240]   Leo is the cable installer.
[01:00:17.240 --> 01:00:22.120]   So, mate, lady, I'm telling you, you got your 2.4G.
[01:00:22.120 --> 01:00:23.680]   And you got your 5G.
[01:00:23.680 --> 01:00:24.680]   Perfect.
[01:00:24.680 --> 01:00:28.520]   Some of this stuff works with 5G.
[01:00:28.520 --> 01:00:29.520]   5G-hertz.
[01:00:29.520 --> 01:00:30.520]   You know what I'm saying?
[01:00:30.520 --> 01:00:31.520]   Oh, no, I'm saying.
[01:00:31.520 --> 01:00:34.320]   I have to say GHZ.
[01:00:34.320 --> 01:00:35.760]   All right.
[01:00:35.760 --> 01:00:38.760]   So 5G-hertz.
[01:00:38.760 --> 01:00:41.440]   Just call it AC.
[01:00:41.440 --> 01:00:44.000]   Well, AC is both.
[01:00:44.000 --> 01:00:46.640]   So anyway, 5G-hertz.
[01:00:46.640 --> 01:00:50.080]   These are not AC routers by the way, Karsten.
[01:00:50.080 --> 01:00:51.080]   But keep going.
[01:00:51.080 --> 01:00:58.360]   So sometimes the IOT device will join the 5G network, 5G's network, and not work.
[01:00:58.360 --> 01:01:01.120]   And so somebody called the radio show and said, "Well, what do I do about that?"
[01:01:01.120 --> 01:01:03.280]   Then a guy called in and said, "I got a solution for you.
[01:01:03.280 --> 01:01:04.880]   It works for me every time."
[01:01:04.880 --> 01:01:12.160]   You start walking and you get so far away that the 5G-hertz signal does not go as far
[01:01:12.160 --> 01:01:13.160]   as the 2.4, right?
[01:01:13.160 --> 01:01:17.400]   So you have so far away that the 5G-hertz signal is so attenuated, your phone joins
[01:01:17.400 --> 01:01:22.480]   the 2.4G-hertz, then you pair it up.
[01:01:22.480 --> 01:01:23.760]   That's actually a really good idea.
[01:01:23.760 --> 01:01:24.760]   Isn't it?
[01:01:24.760 --> 01:01:25.760]   Put it in your newsletter.
[01:01:25.760 --> 01:01:26.760]   I'm telling you.
[01:01:26.760 --> 01:01:28.680]   Kevin and I have talked about this.
[01:01:28.680 --> 01:01:30.440]   It's actually a very common question.
[01:01:30.440 --> 01:01:31.440]   It's a conundrum.
[01:01:31.440 --> 01:01:33.600]   It's not because...
[01:01:33.600 --> 01:01:37.440]   Well, there's a couple issues.
[01:01:37.440 --> 01:01:41.680]   Not every Google Wi-Fi is actually a huge perpetuator of this problem.
[01:01:41.680 --> 01:01:44.840]   My Eero stuff almost always works with every device you put in your account.
[01:01:44.840 --> 01:01:45.840]   Eero does it right.
[01:01:45.840 --> 01:01:46.840]   No, I agree with you.
[01:01:46.840 --> 01:01:47.840]   Eero does it right.
[01:01:47.840 --> 01:01:48.840]   Yeah.
[01:01:48.840 --> 01:01:49.840]   Yeah, Google Wi-Fi is funky and...
[01:01:49.840 --> 01:01:51.840]   A lot of them are funky.
[01:01:51.840 --> 01:01:52.840]   A lot of them don't do it.
[01:01:52.840 --> 01:01:54.840]   Orby, I think, is funky.
[01:01:54.840 --> 01:01:56.760]   Vellops isn't funky.
[01:01:56.760 --> 01:01:58.480]   But yeah, there is...
[01:01:58.480 --> 01:02:00.480]   And part of it is because...
[01:02:00.480 --> 01:02:02.040]   What?
[01:02:02.040 --> 01:02:03.800]   We should have a funky, not funky list.
[01:02:03.800 --> 01:02:06.840]   I think would be really good.
[01:02:06.840 --> 01:02:09.400]   So we could put that in your newsletter and smoke it.
[01:02:09.400 --> 01:02:10.400]   Pass.
[01:02:10.400 --> 01:02:11.400]   No pass.
[01:02:11.400 --> 01:02:12.400]   That's funky.
[01:02:12.400 --> 01:02:17.680]   So, yes, that is an excellent pro tip.
[01:02:17.680 --> 01:02:19.320]   And I will put that in the newsletter.
[01:02:19.320 --> 01:02:21.280]   Actually, we'll share it on the podcast.
[01:02:21.280 --> 01:02:22.280]   Pro tip, yeah.
[01:02:22.280 --> 01:02:24.080]   You don't have to give me credit.
[01:02:24.080 --> 01:02:25.920]   I always give credit.
[01:02:25.920 --> 01:02:27.160]   Credit where credit is due, man.
[01:02:27.160 --> 01:02:29.880]   Well, I should give credit to this listener calling.
[01:02:29.880 --> 01:02:31.480]   I don't know his name off the top of my head.
[01:02:31.480 --> 01:02:32.880]   Oh, I am really disappointed.
[01:02:32.880 --> 01:02:34.120]   We were talking for years.
[01:02:34.120 --> 01:02:38.200]   We've been talking about the laundry folding robot.
[01:02:38.200 --> 01:02:39.200]   Right.
[01:02:39.200 --> 01:02:40.200]   Right.
[01:02:40.200 --> 01:02:41.200]   Right.
[01:02:41.200 --> 01:02:42.200]   Sure.
[01:02:42.200 --> 01:02:43.200]   They just filed for bankruptcy.
[01:02:43.200 --> 01:02:44.200]   Oh, no.
[01:02:44.200 --> 01:02:45.200]   Laundroid.
[01:02:45.200 --> 01:02:53.240]   Oh, you know, there's a new laundry folding robot in the fun part about this one.
[01:02:53.240 --> 01:02:54.240]   Yeah.
[01:02:54.240 --> 01:02:58.480]   Is that it's the robot with hands.
[01:02:58.480 --> 01:03:03.120]   But when the time comes and you want to fold your laundry, you tell it and a person comes
[01:03:03.120 --> 01:03:05.120]   and operates the hands.
[01:03:05.120 --> 01:03:07.320]   That's called a maid.
[01:03:07.320 --> 01:03:11.760]   It's like a mechanical Turk that folds your laundry.
[01:03:11.760 --> 01:03:12.760]   It's $3,000.
[01:03:12.760 --> 01:03:14.200]   It's in Japan.
[01:03:14.200 --> 01:03:19.360]   And it's kind of an interesting concept because basically what I mean, it's still a robot,
[01:03:19.360 --> 01:03:20.360]   still folds laundry.
[01:03:20.360 --> 01:03:21.360]   So you don't have to.
[01:03:21.360 --> 01:03:26.040]   You're just it's laundry folding as a service backed by a human being somewhere in the world.
[01:03:26.040 --> 01:03:27.760]   The laundry folding is a service.
[01:03:27.760 --> 01:03:29.760]   We call that Washington fold.
[01:03:29.760 --> 01:03:32.240]   I mean, it's like labor arbitrage, basically.
[01:03:32.240 --> 01:03:36.880]   You could have someone in the Philippines come over and you don't fold your laundry without
[01:03:36.880 --> 01:03:37.880]   actually ever leaving their house.
[01:03:37.880 --> 01:03:43.760]   Anyway, if you've been waiting, if you've been waiting for the laundry folding robot,
[01:03:43.760 --> 01:03:46.520]   anxious to buy it, by the way, only $16,000.
[01:03:46.520 --> 01:03:51.120]   I think you see that almost that was the new ones.
[01:03:51.120 --> 01:03:52.920]   Three three dollars.
[01:03:52.920 --> 01:03:53.920]   No, 3000.
[01:03:53.920 --> 01:03:54.920]   3000.
[01:03:54.920 --> 01:03:55.920]   Wait a minute.
[01:03:55.920 --> 01:03:58.600]   And you still have to pay somebody to fold your laundry?
[01:03:58.600 --> 01:04:00.440]   No, no, that's part of the $3,000.
[01:04:00.440 --> 01:04:01.920]   Oh, it includes.
[01:04:01.920 --> 01:04:04.400]   Well, how many hours of laundry folding?
[01:04:04.400 --> 01:04:06.160]   You know, I did not investigate that.
[01:04:06.160 --> 01:04:07.840]   There's a person in the box.
[01:04:07.840 --> 01:04:08.840]   Where's this person?
[01:04:08.840 --> 01:04:11.680]   They can't have an electrical church.
[01:04:11.680 --> 01:04:12.680]   Yeah, they teleoperated.
[01:04:12.680 --> 01:04:13.680]   Oh, no.
[01:04:13.680 --> 01:04:14.680]   Oh, no.
[01:04:14.680 --> 01:04:20.600]   This must not succeed because then I see, I imagine an office building somewhere outside
[01:04:20.600 --> 01:04:26.280]   of Phoenix with thousands of people and little cubicles wearing visors, folding your laundry
[01:04:26.280 --> 01:04:27.280]   with joysticks.
[01:04:27.280 --> 01:04:28.840]   But that's not.
[01:04:28.840 --> 01:04:35.080]   But what if it was like the Japanese robot cafe that's operated by by.
[01:04:35.080 --> 01:04:36.080]   That was interesting.
[01:04:36.080 --> 01:04:37.080]   Handy cat people.
[01:04:37.080 --> 01:04:38.080]   Handy cat people.
[01:04:38.080 --> 01:04:41.280]   Yeah, so this is kind of like, let's see, where should I put this?
[01:04:41.280 --> 01:04:44.160]   I'm going to try to put this in the show notes somewhere, but I don't know where.
[01:04:44.160 --> 01:04:45.960]   The government's changelog, other other.
[01:04:45.960 --> 01:04:47.520]   Put it at the bottom.
[01:04:47.520 --> 01:04:48.520]   Put it at the bottom.
[01:04:48.520 --> 01:04:50.520]   All the good stuffs at the bottom.
[01:04:50.520 --> 01:04:51.520]   Oh, here.
[01:04:51.520 --> 01:04:52.520]   Laundry.
[01:04:52.520 --> 01:04:54.840]   I put it in the wrong place, Carson.
[01:04:54.840 --> 01:04:55.840]   I'm an idiot.
[01:04:55.840 --> 01:04:56.840]   There we go.
[01:04:56.840 --> 01:04:58.440]   Oh, this has been a fun show.
[01:04:58.440 --> 01:04:59.440]   I like it.
[01:04:59.440 --> 01:05:00.440]   That's the perfect place for the action.
[01:05:00.440 --> 01:05:02.280]   It's giving me a headache.
[01:05:02.280 --> 01:05:03.280]   Is this show?
[01:05:03.280 --> 01:05:04.280]   I'm sorry.
[01:05:04.280 --> 01:05:05.280]   Yeah.
[01:05:05.280 --> 01:05:11.640]   The reason this robot laundry is exciting, and I wrote about this a couple of weeks ago,
[01:05:11.640 --> 01:05:16.680]   is there's a big trend towards tele-operations of all kinds of devices, from autonomous cars
[01:05:16.680 --> 01:05:18.960]   to fairies to even drones.
[01:05:18.960 --> 01:05:22.800]   So like Google actually, just to bring it back to Google, because I love y'all like
[01:05:22.800 --> 01:05:24.600]   that.
[01:05:24.600 --> 01:05:25.600]   Their wing.
[01:05:25.600 --> 01:05:32.840]   Okay, first of all, there's a guy holding the plug, feeding out the wire to the robot
[01:05:32.840 --> 01:05:34.840]   so that it can walk over to the robot.
[01:05:34.840 --> 01:05:35.840]   Gaffers.
[01:05:35.840 --> 01:05:37.600]   You need a gaffer from the robot.
[01:05:37.600 --> 01:05:40.520]   So it can walk over to the washing machine.
[01:05:40.520 --> 01:05:45.640]   No, actually what we need is a robot, a tele-operated robot to gaff.
[01:05:45.640 --> 01:05:52.440]   And there's another guy holding a laundry basket, also following the robot.
[01:05:52.440 --> 01:05:53.440]   Carefully.
[01:05:53.440 --> 01:05:55.480]   The robot, by the way, was twitching his fingers.
[01:05:55.480 --> 01:05:57.920]   It's not going to work.
[01:05:57.920 --> 01:05:58.920]   The robot is...
[01:05:58.920 --> 01:06:02.920]   Both of those guys could be replaced by more robots.
[01:06:02.920 --> 01:06:04.720]   It's robots all the way down.
[01:06:04.720 --> 01:06:05.720]   All the way down.
[01:06:05.720 --> 01:06:09.680]   Why don't we just make a washing machine that folds your laundry?
[01:06:09.680 --> 01:06:11.920]   Another guy's bringing here a laundry...
[01:06:11.920 --> 01:06:12.920]   T-shirts.
[01:06:12.920 --> 01:06:13.920]   ...device.
[01:06:13.920 --> 01:06:14.920]   Oh my God.
[01:06:14.920 --> 01:06:16.920]   Well, this is a demo.
[01:06:16.920 --> 01:06:18.920]   This is the answer to the joke.
[01:06:18.920 --> 01:06:19.920]   This is the answer to the joke.
[01:06:19.920 --> 01:06:20.920]   You would set things up.
[01:06:20.920 --> 01:06:24.920]   How many engineers does it take to fold a t-shirt?
[01:06:24.920 --> 01:06:31.240]   Apparently, 3 plus a robot.
[01:06:31.240 --> 01:06:33.840]   So in this guy is your remote operator, I believe.
[01:06:33.840 --> 01:06:35.920]   Yes, I see him there, yes.
[01:06:35.920 --> 01:06:40.440]   Well, so you know the reason why laundry folding robots are so hard, right?
[01:06:40.440 --> 01:06:41.640]   It's so difficult to do.
[01:06:41.640 --> 01:06:42.640]   Why?
[01:06:42.640 --> 01:06:43.640]   Because it's very hard.
[01:06:43.640 --> 01:06:48.080]   A computer can be trained to do one specific object really well, right?
[01:06:48.080 --> 01:06:49.720]   Or one specific thing really well.
[01:06:49.720 --> 01:06:54.200]   So it can pick up an object at the same size and put it in the same place every time.
[01:06:54.200 --> 01:06:58.520]   What's hard is when you do laundry, there's different size things.
[01:06:58.520 --> 01:07:02.480]   So it's like, I don't know.
[01:07:02.480 --> 01:07:05.360]   It's finger grasping, can't always grasp the things correctly.
[01:07:05.360 --> 01:07:07.360]   And when you fold it, that's requiring...
[01:07:07.360 --> 01:07:08.360]   Oh, it's so long.
[01:07:08.360 --> 01:07:09.360]   Everything's different.
[01:07:09.360 --> 01:07:10.360]   There's no standard.
[01:07:10.360 --> 01:07:12.200]   I want to see this demo in Shark Tank.
[01:07:12.200 --> 01:07:14.280]   I think this would be an awesome demo.
[01:07:14.280 --> 01:07:20.320]   By the way, incidentally, I hope we don't get the UGO Corporation taking our video from
[01:07:20.320 --> 01:07:21.320]   today down.
[01:07:21.320 --> 01:07:28.880]   If our video is taken down by a laundry folding robot, that will in fact be the day I quit
[01:07:28.880 --> 01:07:29.880]   podcasting.
[01:07:29.880 --> 01:07:31.240]   Would that be your three strikes?
[01:07:31.240 --> 01:07:32.240]   That's the three strikes.
[01:07:32.240 --> 01:07:33.240]   No, no, we haven't.
[01:07:33.240 --> 01:07:36.040]   We don't have any strikes because we always appeal and they say, "Oh, yeah, yeah, we're
[01:07:36.040 --> 01:07:37.040]   right.
[01:07:37.040 --> 01:07:38.040]   You were right.
[01:07:38.040 --> 01:07:39.200]   We shouldn't have."
[01:07:39.200 --> 01:07:45.560]   So Google walkout organizers are saying they're facing retaliation for organizing those
[01:07:45.560 --> 01:07:47.880]   walkouts.
[01:07:47.880 --> 01:07:51.880]   Some of them have been demoted.
[01:07:51.880 --> 01:07:52.880]   Yeah, this is...
[01:07:52.880 --> 01:07:57.400]   Well, now let me ask you this question though.
[01:07:57.400 --> 01:08:02.080]   Maybe it's because we're in California, which is an at-will work state.
[01:08:02.080 --> 01:08:10.280]   But honestly, if our employees decided to hold a walkout, I would have the right to
[01:08:10.280 --> 01:08:12.280]   fire them.
[01:08:12.280 --> 01:08:14.760]   Well, I don't know what...
[01:08:14.760 --> 01:08:19.080]   Does that answer me this?
[01:08:19.080 --> 01:08:22.000]   If it is a labor organizing event or you can't...
[01:08:22.000 --> 01:08:23.000]   That's different.
[01:08:23.000 --> 01:08:26.000]   If it's a union event, that's protected.
[01:08:26.000 --> 01:08:27.000]   I can't...
[01:08:27.000 --> 01:08:29.040]   That'd be union busting, although there's plenty that goes on.
[01:08:29.040 --> 01:08:32.200]   What about an organizing event?
[01:08:32.200 --> 01:08:33.200]   Is it whistleblowing?
[01:08:33.200 --> 01:08:35.240]   Because that's protected as well.
[01:08:35.240 --> 01:08:38.120]   Sure, but then you'd have to be whistleblowing.
[01:08:38.120 --> 01:08:40.120]   They're not whistleblowing in the sense that they're saying...
[01:08:40.120 --> 01:08:41.200]   Carson, get back in your desk.
[01:08:41.200 --> 01:08:46.000]   They're not revealing anything illegal that the company's doing.
[01:08:46.000 --> 01:08:53.600]   They're just saying we don't want to work on AI that helps the military or whatever.
[01:08:53.600 --> 01:08:54.600]   Right, okay.
[01:08:54.600 --> 01:08:56.080]   It wasn't about their work conditions.
[01:08:56.080 --> 01:08:57.080]   It was a protest.
[01:08:57.080 --> 01:08:58.080]   No, that's true.
[01:08:58.080 --> 01:08:59.080]   Yeah, right.
[01:08:59.080 --> 01:09:04.320]   Meredith Whitaker, who leads Google Open Research, said that after the company disbanded
[01:09:04.320 --> 01:09:10.360]   its external AI ethics council on April 4th, she was told her role would be changed dramatically.
[01:09:10.360 --> 01:09:11.360]   She said, "In order to..."
[01:09:11.360 --> 01:09:15.200]   They told her, "In order to stay at the company at Google, you'd have to abandon your work
[01:09:15.200 --> 01:09:16.760]   on AI ethics.
[01:09:16.760 --> 01:09:21.800]   Your role at the AI Now Institute, the research center, she co-founded at NYU.
[01:09:21.800 --> 01:09:22.800]   Chris Stapleton...
[01:09:22.800 --> 01:09:27.080]   I'm sorry, Clara Stapleton, another walkout organizer and 12-year veteran of the company.
[01:09:27.080 --> 01:09:30.560]   Said that two months after the protest, she was demoted from her role as marketing manager
[01:09:30.560 --> 01:09:34.280]   at YouTube and would lose half her reports.
[01:09:34.280 --> 01:09:36.120]   But is that illegal, is my question?
[01:09:36.120 --> 01:09:40.080]   It may be reprehensible or moral or...
[01:09:40.080 --> 01:09:41.080]   It's not good PR.
[01:09:41.080 --> 01:09:43.480]   It's not good PR, but it's not illegal.
[01:09:43.480 --> 01:09:47.240]   I think the company has the right to retaliate against workers at...
[01:09:47.240 --> 01:09:49.760]   You see you serve it at the company as well.
[01:09:49.760 --> 01:09:50.960]   You need to say it.
[01:09:50.960 --> 01:09:52.960]   I don't want to sound like...
[01:09:52.960 --> 01:09:54.960]   Well, you do sound like a...
[01:09:54.960 --> 01:09:55.960]   John D. Rockefeller.
[01:09:55.960 --> 01:09:59.640]   Corporate apologists here, but you're right.
[01:09:59.640 --> 01:10:03.240]   No, he's a corporate mogul himself.
[01:10:03.240 --> 01:10:05.200]   Everything you say, Leo.
[01:10:05.200 --> 01:10:06.200]   Right, Carsten?
[01:10:06.200 --> 01:10:08.480]   You are absolutely positively right.
[01:10:08.480 --> 01:10:12.080]   There's nothing wrong with anything you have ever said.
[01:10:12.080 --> 01:10:13.080]   That's right.
[01:10:13.080 --> 01:10:16.280]   What do you want for review, Carsten?
[01:10:16.280 --> 01:10:17.280]   Well, but...
[01:10:17.280 --> 01:10:22.920]   I mean, there's James Daymore, who wrote that reprehensible memo and did get fired.
[01:10:22.920 --> 01:10:24.160]   But that's very different.
[01:10:24.160 --> 01:10:25.160]   That is a...
[01:10:25.160 --> 01:10:26.160]   That's different.
[01:10:26.160 --> 01:10:28.120]   That is a HR challenge, makes it difficult to work with him.
[01:10:28.120 --> 01:10:29.120]   I mean, there's a lot of...
[01:10:29.120 --> 01:10:34.840]   But if you wrote a memo, if you wrote an email to Larry Page or Sundar Pachay and said,
[01:10:34.840 --> 01:10:37.440]   "You know, I don't think we should be doing this kind of research."
[01:10:37.440 --> 01:10:41.680]   They're not going to demote you for that, or are they?
[01:10:41.680 --> 01:10:44.120]   I don't know.
[01:10:44.120 --> 01:10:46.120]   I don't know.
[01:10:46.120 --> 01:10:48.880]   Probably not, because they haven't been seen at the Googleplex in a while, so it would
[01:10:48.880 --> 01:10:51.080]   be someone else.
[01:10:51.080 --> 01:10:53.120]   Anyway, I bring this up.
[01:10:53.120 --> 01:10:59.920]   I think we've got one of these on Tech News Weekly on Thursday, so you can get more of
[01:10:59.920 --> 01:11:00.920]   the story.
[01:11:00.920 --> 01:11:01.920]   One of the demotees.
[01:11:01.920 --> 01:11:02.920]   Ah, okay.
[01:11:02.920 --> 01:11:06.960]   One of the organizers of the Google Walkout.
[01:11:06.960 --> 01:11:10.840]   One of these employees at Twitter who is no longer working here.
[01:11:10.840 --> 01:11:12.400]   No.
[01:11:12.400 --> 01:11:15.600]   I wouldn't fire a play for doing that.
[01:11:15.600 --> 01:11:16.600]   I might.
[01:11:16.600 --> 01:11:19.880]   Well, there you go.
[01:11:19.880 --> 01:11:21.120]   See?
[01:11:21.120 --> 01:11:22.920]   If Kevin's watching, he's on notice.
[01:11:22.920 --> 01:11:23.920]   There was...
[01:11:23.920 --> 01:11:28.520]   Remember when we had the total eclipse and everybody went out to see that?
[01:11:28.520 --> 01:11:30.320]   All three of you.
[01:11:30.320 --> 01:11:38.520]   It brought the operation to its knees for 30 minutes.
[01:11:38.520 --> 01:11:43.880]   So the Twitter CEO goes to the White House.
[01:11:43.880 --> 01:11:49.200]   Mr. Dorsey goes to the White House after sending an email to his employees apologizing
[01:11:49.200 --> 01:11:50.720]   in effect.
[01:11:50.720 --> 01:11:54.600]   He says, "We meet with a lot of heads of state.
[01:11:54.600 --> 01:11:56.800]   We think it's good to have a conversation."
[01:11:56.800 --> 01:11:58.040]   I know.
[01:11:58.040 --> 01:11:59.040]   Jack Dorsey wrote.
[01:11:59.040 --> 01:12:05.840]   All of you think I shouldn't, but in the long run, I think it's good to meet heads of state
[01:12:05.840 --> 01:12:10.600]   in order to listen, share our principles, and our ideas.
[01:12:10.600 --> 01:12:12.560]   Like, like, does anybody...
[01:12:12.560 --> 01:12:14.520]   Like he's going to listen?
[01:12:14.520 --> 01:12:16.240]   You know, what do you do?
[01:12:16.240 --> 01:12:20.800]   So apparently, according to people in the room, because it was a closed-door meeting,
[01:12:20.800 --> 01:12:26.360]   there was no press, but some people leaked that the primary topic of discussion for the
[01:12:26.360 --> 01:12:29.000]   half-hour was President Trump saying, "Why am I?"
[01:12:29.000 --> 01:12:31.000]   Twitter follower count going down.
[01:12:31.000 --> 01:12:34.280]   Can you do something like that?
[01:12:34.280 --> 01:12:38.600]   And Dorsey said, "Well, mine went down, too, because we're getting rid of bots.
[01:12:38.600 --> 01:12:39.600]   It's normal.
[01:12:39.600 --> 01:12:40.600]   It's not...
[01:12:40.600 --> 01:12:41.600]   Those are my Russian friends."
[01:12:41.600 --> 01:12:42.600]   You know, really.
[01:12:42.600 --> 01:12:43.600]   They're the ones who got me elected.
[01:12:43.600 --> 01:12:47.720]   You can't get rid of them.
[01:12:47.720 --> 01:12:48.720]   Twitter...
[01:12:48.720 --> 01:12:52.480]   Trump had tweeted right before the meeting that Twitter was playing political games tampering
[01:12:52.480 --> 01:12:54.920]   with his nearly 60 million followers.
[01:12:54.920 --> 01:12:58.840]   A significant... this is the Washington Post, a significant portion of the meeting focused
[01:12:58.840 --> 01:13:02.800]   on Trump's concern that Twitter quietly and deliberately has limited or removed some
[01:13:02.800 --> 01:13:05.000]   of his followers.
[01:13:05.000 --> 01:13:08.400]   Trump said he heard from fellow conservatives who had lost followers for unclear reasons
[01:13:08.400 --> 01:13:10.320]   as well.
[01:13:10.320 --> 01:13:12.120]   Twitter long explained follower figures.
[01:13:12.120 --> 01:13:15.320]   Fluctuate, I've lost several hundred thousand, I think.
[01:13:15.320 --> 01:13:16.960]   No, fifty thousand.
[01:13:16.960 --> 01:13:19.520]   You put your account.
[01:13:19.520 --> 01:13:23.320]   I reinstated it, but there's a long story there.
[01:13:23.320 --> 01:13:24.880]   I didn't kill it.
[01:13:24.880 --> 01:13:27.560]   I just took a break.
[01:13:27.560 --> 01:13:28.560]   Post it.
[01:13:28.560 --> 01:13:29.560]   Post it.
[01:13:29.560 --> 01:13:31.240]   And I recently reinstated it.
[01:13:31.240 --> 01:13:35.480]   Although I go back and forth as obviously, because every time I go on there, I go, "Well,
[01:13:35.480 --> 01:13:36.480]   it's not that valuable.
[01:13:36.480 --> 01:13:39.520]   I'm not sure I really need it."
[01:13:39.520 --> 01:13:43.520]   But I wanted to test... the reason I reinstated... I think I talked about this last week or
[01:13:43.520 --> 01:13:49.560]   the week before is Twitter had said, "We are not going to wait for abuse reports to get
[01:13:49.560 --> 01:13:50.920]   rid of abusive tweets.
[01:13:50.920 --> 01:13:53.200]   We are going to be proactively eliminating abusive tweets."
[01:13:53.200 --> 01:13:58.160]   And I thought, "What better way to test this policy than to paint a big target on my
[01:13:58.160 --> 01:14:00.280]   rump and go on Twitter?"
[01:14:00.280 --> 01:14:02.960]   And so far, I have to admit, I haven't seen any abusive tweets.
[01:14:02.960 --> 01:14:06.440]   That will change now, I'm sure.
[01:14:06.440 --> 01:14:12.040]   But I did notice when I came back that I probably lost about fifty thousand.
[01:14:12.040 --> 01:14:15.280]   Not because I wasn't there, I think, but mostly because one thing... another thing I've
[01:14:15.280 --> 01:14:19.800]   noticed, your follower count goes up as long as you don't tweet.
[01:14:19.800 --> 01:14:22.920]   The minute you tweet, that's going to cut your follower count, because there's going
[01:14:22.920 --> 01:14:25.000]   to be somebody who doesn't like what you say.
[01:14:25.000 --> 01:14:28.920]   So the best way to increase your numbers is just to have an account and publicize it,
[01:14:28.920 --> 01:14:30.480]   but not say anything.
[01:14:30.480 --> 01:14:31.720]   I don't have that problem at all.
[01:14:31.720 --> 01:14:34.000]   I know you and I have very different Twitter's.
[01:14:34.000 --> 01:14:36.400]   There's Stacey Twitter and there's Leo Twitter.
[01:14:36.400 --> 01:14:37.920]   I've got no problems.
[01:14:37.920 --> 01:14:38.920]   There's Stacey.
[01:14:38.920 --> 01:14:40.880]   Stacey Twitter is wonderful.
[01:14:40.880 --> 01:14:41.880]   I know.
[01:14:41.880 --> 01:14:43.720]   Leo, did you ever think the problem might be...
[01:14:43.720 --> 01:14:44.720]   What?
[01:14:44.720 --> 01:14:51.880]   Well, in fact, I've been thinking about this lately that we cover the internet the wrong
[01:14:51.880 --> 01:14:52.880]   way now.
[01:14:52.880 --> 01:14:57.600]   The internet has become so much ingrained in what we do that we should be covering the
[01:14:57.600 --> 01:15:03.520]   people who are on it, not the technology, whether it's Sri Lanka or whether it's anything
[01:15:03.520 --> 01:15:05.520]   else.
[01:15:05.520 --> 01:15:07.360]   We're covering Facebook as an entity.
[01:15:07.360 --> 01:15:09.960]   No, we're covering the activities of people.
[01:15:09.960 --> 01:15:13.600]   Well, that's the difference between you and me.
[01:15:13.600 --> 01:15:18.720]   Because the heritage of what I do has always been with the technologies.
[01:15:18.720 --> 01:15:22.560]   You, we were not specifically whether it was Tech TV or early days of Twitter.
[01:15:22.560 --> 01:15:26.360]   We were specifically not interested in people.
[01:15:26.360 --> 01:15:28.200]   But you're covering machines.
[01:15:28.200 --> 01:15:29.200]   You have no hope now.
[01:15:29.200 --> 01:15:30.200]   You have no hope now.
[01:15:30.200 --> 01:15:31.200]   That's the problem.
[01:15:31.200 --> 01:15:33.520]   That's what's ruined everything is people have gotten into the machines.
[01:15:33.520 --> 01:15:35.000]   People have entered your world.
[01:15:35.000 --> 01:15:36.000]   The people are in the machine.
[01:15:36.000 --> 01:15:39.680]   It's picked up in a beautiful galaxy fold and somebody put some people in it.
[01:15:39.680 --> 01:15:41.240]   Yeah.
[01:15:41.240 --> 01:15:42.240]   That's your problem.
[01:15:42.240 --> 01:15:43.240]   There's your problem, Leo.
[01:15:43.240 --> 01:15:44.240]   In a nutshell.
[01:15:44.240 --> 01:15:45.240]   Yes.
[01:15:45.240 --> 01:15:51.280]   But that's an incidentally, that is a long standing kind of evolutionary step in all
[01:15:51.280 --> 01:15:52.280]   technologies.
[01:15:52.280 --> 01:15:57.200]   I think I often compare it to the days and you remember this Jeff in the early days of
[01:15:57.200 --> 01:15:58.200]   high-fi.
[01:15:58.200 --> 01:16:02.200]   You had all these high-fi magazines and it was all about the hardware.
[01:16:02.200 --> 01:16:07.360]   It was all about how much power you could get without distortion and all this stuff.
[01:16:07.360 --> 01:16:11.120]   And then long around about the 60s when Rolling Stone came along and suddenly started becoming
[01:16:11.120 --> 01:16:15.360]   about the stuff you played on the hardware, not the hardware itself.
[01:16:15.360 --> 01:16:16.560]   High-fi magazines went away.
[01:16:16.560 --> 01:16:20.200]   Rolling Stone and other magazines took over.
[01:16:20.200 --> 01:16:24.080]   That's exactly the shift I think that's happened in technology where in the early days of technology
[01:16:24.080 --> 01:16:27.000]   we were talking about speeds and feeds and processors and hard-sized.
[01:16:27.000 --> 01:16:30.000]   It's even more than that because it's not the producers of the music.
[01:16:30.000 --> 01:16:31.960]   It's the listeners of the music.
[01:16:31.960 --> 01:16:32.960]   It's the audience.
[01:16:32.960 --> 01:16:33.960]   It's the public.
[01:16:33.960 --> 01:16:34.960]   That's interesting.
[01:16:34.960 --> 01:16:37.720]   Has now entered into your world.
[01:16:37.720 --> 01:16:41.760]   It's not the tool, what's being done with the tool, it needs the coverage.
[01:16:41.760 --> 01:16:42.760]   Right.
[01:16:42.760 --> 01:16:44.560]   So that is very true, Jeff.
[01:16:44.560 --> 01:16:45.560]   That is good.
[01:16:45.560 --> 01:16:47.640]   Can we mark this in the history books?
[01:16:47.640 --> 01:16:49.600]   Can you put that in the history books?
[01:16:49.600 --> 01:16:51.240]   Stacey, did you see what's true?
[01:16:51.240 --> 01:16:52.240]   It's true.
[01:16:52.240 --> 01:16:53.240]   On me.
[01:16:53.240 --> 01:16:54.240]   Yes, we are.
[01:16:54.240 --> 01:16:56.240]   There's a great post I put up on the one.
[01:16:56.240 --> 01:16:58.760]   One thing they agree on, Leo is the problem.
[01:16:58.760 --> 01:17:07.400]   I put a great post up on about the UK online harm report we were talking about last week
[01:17:07.400 --> 01:17:11.680]   which he said that basically what the UK is trying to do is to pass an omnibus law against
[01:17:11.680 --> 01:17:14.200]   bad behavior without defining bad.
[01:17:14.200 --> 01:17:15.200]   Right.
[01:17:15.200 --> 01:17:17.080]   Because people are thinking, "It's a technology."
[01:17:17.080 --> 01:17:18.080]   No, it's...
[01:17:18.080 --> 01:17:19.080]   No difference.
[01:17:19.080 --> 01:17:20.600]   It's "soilant internet."
[01:17:20.600 --> 01:17:21.600]   Yeah.
[01:17:21.600 --> 01:17:22.600]   It's right.
[01:17:22.600 --> 01:17:24.080]   And that's the problem.
[01:17:24.080 --> 01:17:25.080]   Yeah.
[01:17:25.080 --> 01:17:26.440]   And we're not thinking that way.
[01:17:26.440 --> 01:17:28.880]   Is this the Europe against the net post?
[01:17:28.880 --> 01:17:29.880]   No.
[01:17:29.880 --> 01:17:31.880]   No, that's an older one.
[01:17:31.880 --> 01:17:32.880]   That's much older.
[01:17:32.880 --> 01:17:33.880]   No, no, no, no.
[01:17:33.880 --> 01:17:35.920]   I think you should call it the "soilant internet."
[01:17:35.920 --> 01:17:36.920]   Gross.
[01:17:36.920 --> 01:17:37.920]   That's made of people.
[01:17:37.920 --> 01:17:42.040]   We're going to put on government, maybe, and government.
[01:17:42.040 --> 01:17:43.040]   But you're right.
[01:17:43.040 --> 01:17:44.040]   And this is the point.
[01:17:44.040 --> 01:17:45.040]   I mean, if you want to...
[01:17:45.040 --> 01:17:51.880]   This is why we need to have government regulation/a conversation about what this means for people.
[01:17:51.880 --> 01:17:55.240]   And this is why diversity is so important now in tech.
[01:17:55.240 --> 01:17:56.240]   Yes, yes, yes.
[01:17:56.240 --> 01:17:57.240]   And people still don't want...
[01:17:57.240 --> 01:18:02.320]   Because you're affecting so many different types of lives and you need to have those perspectives
[01:18:02.320 --> 01:18:03.640]   in here.
[01:18:03.640 --> 01:18:05.600]   Because it's reflect.
[01:18:05.600 --> 01:18:11.040]   You need to have your tools and services be reflective of the people that will use them.
[01:18:11.040 --> 01:18:15.560]   Otherwise, you build stupid stuff that either nobody likes or a small segment of the population
[01:18:15.560 --> 01:18:16.560]   likes.
[01:18:16.560 --> 01:18:21.400]   And you think it's all that in a piece of cake, aka Twitter.
[01:18:21.400 --> 01:18:23.280]   Did they just change everything about Twitter?
[01:18:23.280 --> 01:18:24.280]   Did that just happen?
[01:18:24.280 --> 01:18:25.280]   Yeah.
[01:18:25.280 --> 01:18:27.000]   You must have the new UI.
[01:18:27.000 --> 01:18:28.000]   Are you looking at it right now?
[01:18:28.000 --> 01:18:29.000]   Yeah, I have the new UI.
[01:18:29.000 --> 01:18:30.000]   Okay.
[01:18:30.000 --> 01:18:31.000]   That's what it's like.
[01:18:31.000 --> 01:18:32.000]   I'm like...
[01:18:32.000 --> 01:18:33.000]   They always have playing.
[01:18:33.000 --> 01:18:34.000]   Okay.
[01:18:34.000 --> 01:18:35.080]   Again, I've been moving.
[01:18:35.080 --> 01:18:36.080]   So...
[01:18:36.080 --> 01:18:39.080]   Did I just blink in Twitter changed?
[01:18:39.080 --> 01:18:40.080]   Yes.
[01:18:40.080 --> 01:18:41.080]   The answer is yes.
[01:18:41.080 --> 01:18:42.080]   Okay.
[01:18:42.080 --> 01:18:43.080]   They do that a lot.
[01:18:43.080 --> 01:18:44.080]   I don't know if that's really what you're saying.
[01:18:44.080 --> 01:18:46.800]   Someone was complaining about not being able to edit a photo.
[01:18:46.800 --> 01:18:48.760]   I didn't know you could edit a photo on Twitter.
[01:18:48.760 --> 01:18:49.760]   Oh, yeah.
[01:18:49.760 --> 01:18:50.760]   You can edit all kinds of photos.
[01:18:50.760 --> 01:18:51.760]   Why?
[01:18:51.760 --> 01:18:53.080]   I did not edit the photo of my dog, though.
[01:18:53.080 --> 01:18:56.400]   I should have cut out the slippers.
[01:18:56.400 --> 01:19:01.560]   So the Pew Research Center on Internet and Technology, they are constantly doing surveys.
[01:19:01.560 --> 01:19:10.000]   This one, 2791 US adult Twitter users who are willing to share their Twitter handles.
[01:19:10.000 --> 01:19:15.480]   Survey says US adult Twitter users are younger, not much of a surprise, and more likely to
[01:19:15.480 --> 01:19:18.160]   be Democrats, Democrats in the general public.
[01:19:18.160 --> 01:19:19.320]   Because they're younger.
[01:19:19.320 --> 01:19:20.560]   Because they're younger.
[01:19:20.560 --> 01:19:28.920]   Most users rarely tweet the most prolific 10% create 80% of the tweets from US adult users.
[01:19:28.920 --> 01:19:30.880]   Hello, journalists.
[01:19:30.880 --> 01:19:33.480]   Yeah, they're probably all journalists.
[01:19:33.480 --> 01:19:38.080]   A large majority of tweets come from a small minority of tweeters.
[01:19:38.080 --> 01:19:45.800]   Here's what's wrong with the logic behind the survey, if I may.
[01:19:45.800 --> 01:19:50.880]   It treats, of course, how do I say this?
[01:19:50.880 --> 01:19:55.400]   It's treating Twitter's creators, Twitter's people as if they're an audience, a mass audience
[01:19:55.400 --> 01:19:57.360]   that all see the same thing.
[01:19:57.360 --> 01:19:59.800]   So to say, it's what always drives me nuts when I see a headline.
[01:19:59.800 --> 01:20:02.640]   Twitter says, there's no such thing.
[01:20:02.640 --> 01:20:04.360]   Nobody sees the same Twitter.
[01:20:04.360 --> 01:20:06.080]   Nobody sees everybody on Twitter.
[01:20:06.080 --> 01:20:10.560]   The fact that the total makeup on Twitter is absolutely, is meaningless, utterly meaningless.
[01:20:10.560 --> 01:20:15.640]   Because there is no, everybody sees a different fellow, people in my age cohort.
[01:20:15.640 --> 01:20:20.200]   My Twitter would be, silver, Twitter, yeah, Twitter, silver.
[01:20:20.200 --> 01:20:21.600]   Yeah, right.
[01:20:21.600 --> 01:20:23.960]   So to say that, when can I get that?
[01:20:23.960 --> 01:20:24.960]   As a characterization.
[01:20:24.960 --> 01:20:25.960]   No, no, no, no.
[01:20:25.960 --> 01:20:27.960]   Those are the people causing the problems.
[01:20:27.960 --> 01:20:32.440]   It's the people causing the problems, the old for us.
[01:20:32.440 --> 01:20:35.240]   Most Twitter users engage modestly.
[01:20:35.240 --> 01:20:40.800]   The 10% of tweet most often focus more on politics and are, and here was a big surprise,
[01:20:40.800 --> 01:20:42.800]   mostly women.
[01:20:42.800 --> 01:20:43.800]   Oh.
[01:20:43.800 --> 01:20:48.600]   Now, this is not what you're talking about, Jeff, because this talks about the entire universe.
[01:20:48.600 --> 01:20:53.240]   Admittedly, if you don't follow any women, none of your tweets and your feed are women.
[01:20:53.240 --> 01:20:58.560]   But still, I think it's germane to say that most of the people who tweet are women.
[01:20:58.560 --> 01:21:02.600]   I wonder if that's because we can curate, and I mean, yes, people can come into our,
[01:21:02.600 --> 01:21:08.880]   they can slide into our DMs, but you can curate your experience to some effect on Twitter.
[01:21:08.880 --> 01:21:11.800]   And there's a lot of female journalists believe, or not.
[01:21:11.800 --> 01:21:16.040]   I think, well, I don't want to make too many generalizations because you'll punch me.
[01:21:16.040 --> 01:21:21.760]   65% of the top 10% of tweeters are women.
[01:21:21.760 --> 01:21:25.640]   So more than half that you would normally expense.
[01:21:25.640 --> 01:21:28.800]   Well, it's a significant, largely larger portion.
[01:21:28.800 --> 01:21:31.440]   That means 30% are men.
[01:21:31.440 --> 01:21:33.880]   That's a big deal, I think.
[01:21:33.880 --> 01:21:37.840]   But that makes sense because women, if you're going to make the generalization that's going
[01:21:37.840 --> 01:21:41.920]   to cause you to punch me, are more like to communicate.
[01:21:41.920 --> 01:21:44.320]   Actually, I wasn't going to make that generalization.
[01:21:44.320 --> 01:21:46.080]   You can still punch me.
[01:21:46.080 --> 01:21:52.040]   So I was going to say that because women don't have as many avenues, mainstream avenues,
[01:21:52.040 --> 01:21:55.440]   it's a good opportunity to share our thoughts on things.
[01:21:55.440 --> 01:21:56.800]   Ding, ding, ding, ding, ding, ding, ding.
[01:21:56.800 --> 01:21:59.520]   This is the one I had to fit with far, hard mongyoo for telling journalists to get off
[01:21:59.520 --> 01:22:03.360]   Twitter, people who had never been heard before in mainstream media who aren't in newsrooms
[01:22:03.360 --> 01:22:05.520]   finally have a voice and you're going to turn your back on them.
[01:22:05.520 --> 01:22:07.360]   Oh, that's really interesting.
[01:22:07.360 --> 01:22:10.880]   So just convince me to reactivate my Twitter account.
[01:22:10.880 --> 01:22:13.920]   Without Twitter, we wouldn't have had me to and living while black.
[01:22:13.920 --> 01:22:14.920]   Wow.
[01:22:14.920 --> 01:22:16.920]   That's a good point.
[01:22:16.920 --> 01:22:21.320]   And because those represent our noises, our more represented on Twitter.
[01:22:21.320 --> 01:22:22.320]   Exactly.
[01:22:22.320 --> 01:22:26.520]   And whether they're represented in the whole world or with each other, it doesn't matter.
[01:22:26.520 --> 01:22:27.520]   And you can...
[01:22:27.520 --> 01:22:32.800]   No, I was just going to say to build on that, you can follow those people.
[01:22:32.800 --> 01:22:41.000]   It gives them an extra resonance in the way, anytime there's a tragedy, for example, someone
[01:22:41.000 --> 01:22:45.960]   will always, by the way, follow so-and-so.
[01:22:45.960 --> 01:22:46.960]   Let's say it's a hurricane.
[01:22:46.960 --> 01:22:50.800]   There are meteorologists in the area or if it's Black Lives Matter.
[01:22:50.800 --> 01:22:55.440]   They're an activist who is on the ground who's been working in this community for a decade.
[01:22:55.440 --> 01:22:59.320]   So there's when things happen.
[01:22:59.320 --> 01:23:00.920]   And I actually think this is hugely valuable.
[01:23:00.920 --> 01:23:04.160]   I do it to a lesser extent for...
[01:23:04.160 --> 01:23:09.520]   I follow weird spectrum policy walks on Twitter that no one else follows because I want to
[01:23:09.520 --> 01:23:10.680]   understand what they're thinking.
[01:23:10.680 --> 01:23:14.360]   So and then as a journalist, you can...
[01:23:14.360 --> 01:23:20.280]   Those voices can get a say in the mainstream media that they haven't before.
[01:23:20.280 --> 01:23:21.280]   So I...
[01:23:21.280 --> 01:23:22.280]   Again.
[01:23:22.280 --> 01:23:23.280]   Okay.
[01:23:23.280 --> 01:23:24.280]   Yes.
[01:23:24.280 --> 01:23:31.320]   It's where alternative conversations happen.
[01:23:31.320 --> 01:23:32.320]   But they're not alternative.
[01:23:32.320 --> 01:23:35.200]   So I was going over this, so we have a Center for Community and Ethnic Media.
[01:23:35.200 --> 01:23:39.200]   We're trying to talk about the right name because community and ethnic are quite it.
[01:23:39.200 --> 01:23:42.280]   And I hope she won't mind.
[01:23:42.280 --> 01:23:45.840]   The head of it was emailing me today and talking about using the word everybody.
[01:23:45.840 --> 01:23:46.840]   And to me, everybody...
[01:23:46.840 --> 01:23:49.280]   No, it's not everybody either.
[01:23:49.280 --> 01:23:50.280]   It's not everybody either.
[01:23:50.280 --> 01:23:51.280]   Yeah.
[01:23:51.280 --> 01:23:52.280]   Right.
[01:23:52.280 --> 01:23:56.520]   But I know what she's saying is now finally it can be everybody.
[01:23:56.520 --> 01:23:57.520]   It's also...
[01:23:57.520 --> 01:23:58.520]   So it's not alternatives.
[01:23:58.520 --> 01:23:59.520]   It's not new voices.
[01:23:59.520 --> 01:24:00.520]   These voices have always been there.
[01:24:00.520 --> 01:24:03.600]   Nobody could hear them because they didn't have the outlet.
[01:24:03.600 --> 01:24:06.120]   And social media gives them that, which is part of...
[01:24:06.120 --> 01:24:08.160]   Well, I'll do that.
[01:24:08.160 --> 01:24:12.120]   But AOC who says social media is a health risk.
[01:24:12.120 --> 01:24:14.680]   A little bit of hypocrisy there.
[01:24:14.680 --> 01:24:15.680]   Is it made her?
[01:24:15.680 --> 01:24:16.680]   I...
[01:24:16.680 --> 01:24:17.680]   Okay.
[01:24:17.680 --> 01:24:18.680]   No.
[01:24:18.680 --> 01:24:19.680]   I think, again, both things can be true.
[01:24:19.680 --> 01:24:25.240]   I think there are enough documented studies where we talk about where we see that being
[01:24:25.240 --> 01:24:30.440]   online all the time, interacting with people, especially when she rise to prominence, especially
[01:24:30.440 --> 01:24:32.960]   as a woman, it is a health risk.
[01:24:32.960 --> 01:24:38.120]   You can only have so many people telling you constantly how crappy you are.
[01:24:38.120 --> 01:24:39.120]   That's going to affect you.
[01:24:39.120 --> 01:24:44.360]   That's the difference between Stacey Twitter and Leo Twitter, by the way.
[01:24:44.360 --> 01:24:47.400]   You have nice people talking to you on Twitter.
[01:24:47.400 --> 01:24:49.520]   I mean, I've had to block a couple of...
[01:24:49.520 --> 01:24:52.240]   I don't get as much hate as AOC does, obviously.
[01:24:52.240 --> 01:24:53.240]   Oh, no.
[01:24:53.240 --> 01:25:00.480]   But you can clearly see why somebody like AOC, somebody who is a public figure and also
[01:25:00.480 --> 01:25:08.320]   a hotspot is going to say, "Oh, this is not good."
[01:25:08.320 --> 01:25:12.680]   And yet, your argument, Jeff, is really important here.
[01:25:12.680 --> 01:25:17.640]   It is a chance for underrepresented voices to speak to her.
[01:25:17.640 --> 01:25:20.040]   So there's got to be a middle class here.
[01:25:20.040 --> 01:25:21.040]   Yes.
[01:25:21.040 --> 01:25:22.040]   Yes.
[01:25:22.040 --> 01:25:25.840]   And this goes back to the online harms thing in the UK.
[01:25:25.840 --> 01:25:32.120]   They would say in the bourgeois internet that Facebook should kill all the bad things.
[01:25:32.120 --> 01:25:35.200]   But I don't want them in that role, particularly.
[01:25:35.200 --> 01:25:37.680]   No, they should not be the arbiter of good and bad.
[01:25:37.680 --> 01:25:39.760]   Well, that's what the UK is telling them they should be.
[01:25:39.760 --> 01:25:40.760]   I know.
[01:25:40.760 --> 01:25:41.760]   I agree.
[01:25:41.760 --> 01:25:42.760]   Literally.
[01:25:42.760 --> 01:25:44.520]   Legal, legal, but harmful goes.
[01:25:44.520 --> 01:25:45.520]   Yeah.
[01:25:45.520 --> 01:25:46.520]   No, that's so bad.
[01:25:46.520 --> 01:25:47.520]   It's awful.
[01:25:47.520 --> 01:25:52.600]   So what if the problem is they don't give you the tools to mediate your own experience
[01:25:52.600 --> 01:25:54.200]   in a way that you can walk away from a conversation?
[01:25:54.200 --> 01:25:58.640]   That's why we like Google+ because you could very easily do that.
[01:25:58.640 --> 01:26:07.440]   And if we could come up with a...
[01:26:07.440 --> 01:26:12.000]   This is that thing that the guy doesn't like podcast hates, by the way.
[01:26:12.000 --> 01:26:14.120]   That's what bad life is whistling.
[01:26:14.120 --> 01:26:17.960]   He hates whistling, especially whistling.
[01:26:17.960 --> 01:26:24.920]   But actually having not legislating what they can and cannot do, but legislating or having
[01:26:24.920 --> 01:26:30.880]   the capability and technical acumen to say, "Here's the sorts of rules that you should
[01:26:30.880 --> 01:26:32.720]   implement to give users the tools."
[01:26:32.720 --> 01:26:36.920]   Now, one could say that in a capitalistic system that that wouldn't be necessary because
[01:26:36.920 --> 01:26:41.240]   you would just go to a platform where you aren't attacked, which then gets to maybe
[01:26:41.240 --> 01:26:47.320]   we should be looking at data gathering and network effects as part of our competition
[01:26:47.320 --> 01:26:49.680]   policy.
[01:26:49.680 --> 01:26:51.920]   This is one of those things where I'm like, "I don't know.
[01:26:51.920 --> 01:26:52.920]   That's possible."
[01:26:52.920 --> 01:26:56.040]   Doesn't Twitter have enough tools for you to...
[01:26:56.040 --> 01:26:58.360]   You can block keywords, right?
[01:26:58.360 --> 01:26:59.360]   You can block people.
[01:26:59.360 --> 01:27:02.680]   Yeah, but the problem is there are still talking about you.
[01:27:02.680 --> 01:27:03.680]   Yeah, there's still something...
[01:27:03.680 --> 01:27:05.520]   Oh, you're just blocking from you.
[01:27:05.520 --> 01:27:06.520]   Yeah, which...
[01:27:06.520 --> 01:27:07.520]   Yeah, but you know what?
[01:27:07.520 --> 01:27:13.840]   There should not be a tool for somebody like AOC to say, "Oh, nobody can say anything
[01:27:13.840 --> 01:27:15.320]   bad about me in public."
[01:27:15.320 --> 01:27:16.320]   That's not...
[01:27:16.320 --> 01:27:21.360]   That's not the sort of thing that they would be doing.
[01:27:21.360 --> 01:27:23.000]   It's policing violence speech.
[01:27:23.000 --> 01:27:24.000]   It's policing...
[01:27:24.000 --> 01:27:25.000]   Well, they should...
[01:27:25.000 --> 01:27:26.000]   Okay.
[01:27:26.000 --> 01:27:27.000]   And there are laws against that.
[01:27:27.000 --> 01:27:29.480]   So clearly they should be doing that.
[01:27:29.480 --> 01:27:34.360]   Twitter says, "They are accepting cases where they leave the tweet up so law enforcement
[01:27:34.360 --> 01:27:38.040]   can act, which makes no sense to me at all since they have a record."
[01:27:38.040 --> 01:27:39.440]   Dude, just document that.
[01:27:39.440 --> 01:27:40.440]   Just document that.
[01:27:40.440 --> 01:27:41.440]   Just document it.
[01:27:41.440 --> 01:27:42.440]   Make a print out and send it to the cops.
[01:27:42.440 --> 01:27:43.680]   You don't have to leave it up.
[01:27:43.680 --> 01:27:44.680]   Right.
[01:27:44.680 --> 01:27:45.680]   Yeah.
[01:27:45.680 --> 01:27:46.680]   Crazy.
[01:27:46.680 --> 01:27:50.040]   But this is also where I go back to and we're not going to go through it again.
[01:27:50.040 --> 01:27:54.800]   But my proposal that I wrote about, I think, two weeks ago about the internet court, if
[01:27:54.800 --> 01:27:59.640]   it is illegal, then courts should rule and you do it that way.
[01:27:59.640 --> 01:28:00.640]   Yes.
[01:28:00.640 --> 01:28:02.360]   If it's merely bad, then you go to Stacy's point.
[01:28:02.360 --> 01:28:03.720]   What's the quality of the experience?
[01:28:03.720 --> 01:28:05.200]   What tools are you giving people?
[01:28:05.200 --> 01:28:08.320]   What's the warranty you're giving people about how they can protect themselves and deal
[01:28:08.320 --> 01:28:09.320]   with themselves?
[01:28:09.320 --> 01:28:10.320]   Yeah.
[01:28:10.320 --> 01:28:11.320]   And what's...
[01:28:11.320 --> 01:28:12.320]   Wow.
[01:28:12.320 --> 01:28:13.320]   And for instance...
[01:28:13.320 --> 01:28:19.280]   What's the heat I get on Twitter is legitimate or illegitimate, but criticism of performance
[01:28:19.280 --> 01:28:21.160]   on the shows or the shows themselves.
[01:28:21.160 --> 01:28:22.160]   I don't...
[01:28:22.160 --> 01:28:23.160]   You know, I hurt my feelings.
[01:28:23.160 --> 01:28:24.160]   I don't like it.
[01:28:24.160 --> 01:28:27.680]   But that kind of thing is not problematic in any real way.
[01:28:27.680 --> 01:28:30.480]   It just hurts my feelings.
[01:28:30.480 --> 01:28:32.040]   And so...
[01:28:32.040 --> 01:28:37.000]   And the problem is from a personal point of view, humans are bad judge of whether this
[01:28:37.000 --> 01:28:41.320]   is damaging, threatening, or just insulting.
[01:28:41.320 --> 01:28:43.320]   And you know...
[01:28:43.320 --> 01:28:48.880]   I think you are missing a huge class or maybe you're willfully ignoring a huge class of
[01:28:48.880 --> 01:28:49.880]   comments directed...
[01:28:49.880 --> 01:28:55.480]   I mean, I'm sure they're directed towards men a lot, but they're directed towards women
[01:28:55.480 --> 01:28:57.040]   that are not just...
[01:28:57.040 --> 01:28:58.040]   I disagree with your opinion.
[01:28:58.040 --> 01:29:02.280]   Oh, I know about those because I have a wife who's on in public.
[01:29:02.280 --> 01:29:04.000]   And I am very well aware of those.
[01:29:04.000 --> 01:29:10.760]   And many of our female staffers and female contributors, I have seen the worst of humanity
[01:29:10.760 --> 01:29:13.560]   attacking them, as far worse for them than it is for me.
[01:29:13.560 --> 01:29:17.040]   In fact, I never have anything anywhere near that bad.
[01:29:17.040 --> 01:29:18.040]   And I wonder...
[01:29:18.040 --> 01:29:22.400]   So if you think about it as a public square problem, right?
[01:29:22.400 --> 01:29:27.960]   And if I think about my ability to walk outside and hang out in Austin, for example,
[01:29:27.960 --> 01:29:33.680]   and not have people walk up to me and yell hateful stuff at me, you know, is that protected
[01:29:33.680 --> 01:29:34.680]   by law?
[01:29:34.680 --> 01:29:36.680]   I probably could ask...
[01:29:36.680 --> 01:29:39.440]   Oh, no, it's actually not.
[01:29:39.440 --> 01:29:45.080]   So I could ask someone's like a cop and they'd probably be like, "Dude, stop harassing that
[01:29:45.080 --> 01:29:46.080]   lady.
[01:29:46.080 --> 01:29:47.080]   Just stop going straight."
[01:29:47.080 --> 01:29:49.360]   It's probably not illegal.
[01:29:49.360 --> 01:29:50.360]   Or other...
[01:29:50.360 --> 01:29:52.120]   Stacy, here's what happened.
[01:29:52.120 --> 01:29:54.360]   Other good people would come up and tell them to shut up.
[01:29:54.360 --> 01:29:55.360]   One would hope.
[01:29:55.360 --> 01:30:01.600]   But can you imagine the kind of infective that Alexandria Ocasio-Cortez faces in public?
[01:30:01.600 --> 01:30:02.600]   Not just...
[01:30:02.600 --> 01:30:05.880]   Okay, go ahead.
[01:30:05.880 --> 01:30:11.640]   And what's okay in Austin, let's say, you know, people running around topless.
[01:30:11.640 --> 01:30:13.320]   We had Leslie, the naked guy.
[01:30:13.320 --> 01:30:16.320]   He would just run around either naked or in a bikini, right?
[01:30:16.320 --> 01:30:17.760]   In Austin, that's totally okay.
[01:30:17.760 --> 01:30:22.280]   In rural Kansas, that would be not okay.
[01:30:22.280 --> 01:30:28.000]   And part of the problem here is we have these platforms that are international and they're
[01:30:28.000 --> 01:30:33.680]   trying to cover social mores for so many different communities that they really can't.
[01:30:33.680 --> 01:30:39.120]   And the tools have to be flexible, but you're also going to run into people from Kansas
[01:30:39.120 --> 01:30:41.840]   who are seeing people from Austin and get offended.
[01:30:41.840 --> 01:30:43.480]   And I'm not talking about hate speech.
[01:30:43.480 --> 01:30:46.320]   I'm just talking about people getting offended by things on this platform.
[01:30:46.320 --> 01:30:48.280]   This is why it's hard.
[01:30:48.280 --> 01:30:49.280]   Yes.
[01:30:49.280 --> 01:30:53.040]   Stacy, we go back to what I interrupted you and I apologize.
[01:30:53.040 --> 01:30:54.480]   I was trying to agree with you.
[01:30:54.480 --> 01:30:59.600]   But if you do that on the street, right, let's say it's a crowded street, one would hope
[01:30:59.600 --> 01:31:00.760]   Austin be in a nice place.
[01:31:00.760 --> 01:31:06.000]   If somebody walks up to you and just starts saying horrible things, forget the cop that
[01:31:06.000 --> 01:31:09.880]   other people would say, man, cool, chill, relax.
[01:31:09.880 --> 01:31:12.040]   We didn't luckily need to be alone.
[01:31:12.040 --> 01:31:13.800]   That's what we need in public.
[01:31:13.800 --> 01:31:15.120]   That's what we need online too.
[01:31:15.120 --> 01:31:17.600]   And we see too little of that.
[01:31:17.600 --> 01:31:21.440]   We don't see anyone reacting to the bad behavior because part of it is we say don't feed the
[01:31:21.440 --> 01:31:24.280]   troll, which is a proper tactic.
[01:31:24.280 --> 01:31:28.280]   But the other part of it is that I think we too often send people, hey, look, they're
[01:31:28.280 --> 01:31:30.080]   yelling at Stacy.
[01:31:30.080 --> 01:31:31.080]   Right?
[01:31:31.080 --> 01:31:34.040]   And we encourage the behavior.
[01:31:34.040 --> 01:31:42.520]   How do we get our norms recalibrated for all of us to have a responsibility for bad behavior?
[01:31:42.520 --> 01:31:45.480]   I think some of it people just don't see.
[01:31:45.480 --> 01:31:49.680]   I know that for a while there was a movement to retweet some of your hateful things so
[01:31:49.680 --> 01:31:51.760]   people were like, holy cow.
[01:31:51.760 --> 01:31:56.640]   And there's also, I don't actually want someone, sometimes people defend me on Twitter and
[01:31:56.640 --> 01:31:59.160]   I'm like, let's just ignore them.
[01:31:59.160 --> 01:32:06.440]   So I think it is hard to, I mean, to say you can't read the situation as well as you can
[01:32:06.440 --> 01:32:11.400]   online as you can in like a street, if that also makes sense.
[01:32:11.400 --> 01:32:17.400]   Though, but I do think in general, I rarely see people coming to the defense for someone.
[01:32:17.400 --> 01:32:21.160]   I rarely see someone shaming somebody for just bad behavior.
[01:32:21.160 --> 01:32:24.160]   I mean, I try to do it, but I come off like a nanny.
[01:32:24.160 --> 01:32:26.160]   But yeah.
[01:32:26.160 --> 01:32:27.160]   How about Sri Lanka?
[01:32:27.160 --> 01:32:33.560]   Let's bring in Sri Lanka into this because horrific, the horrific, horrific bombings in
[01:32:33.560 --> 01:32:36.840]   Sri Lanka and hundreds dead, very sad.
[01:32:36.840 --> 01:32:40.680]   One of the topics that came up though on Twitter on Sunday was the fact that the government
[01:32:40.680 --> 01:32:45.840]   in Sri Lanka shut down social media shortly thereafter saying it's problematic.
[01:32:45.840 --> 01:32:48.120]   It's inciting violence.
[01:32:48.120 --> 01:32:55.000]   And I at the time, uh, twisted it to serve my anti Facebook, techno valley politics.
[01:32:55.000 --> 01:32:57.480]   You want?
[01:32:57.480 --> 01:33:02.600]   And I'm quoting from a tweet by you and I, Jana, I can't pronounce it.
[01:33:02.600 --> 01:33:04.160]   This is a great threat.
[01:33:04.160 --> 01:33:08.360]   This is a killer for the love of whatever you hold holy, right?
[01:33:08.360 --> 01:33:12.280]   Stop twisting this incident to serve your anti Facebook, techno valley politics right
[01:33:12.280 --> 01:33:17.480]   now in a country with tight government controls on traditional media.
[01:33:17.480 --> 01:33:20.400]   Social media is a boon for us.
[01:33:20.400 --> 01:33:27.240]   Ah, to make an effort to understand violence in Sri Lanka began before social media, internet
[01:33:27.240 --> 01:33:32.200]   telephony to a large extent, much of race hatred is still fueled by print media in this
[01:33:32.200 --> 01:33:33.700]   country.
[01:33:33.700 --> 01:33:36.720]   You want to point fingers, at least do your research into a complex history.
[01:33:36.720 --> 01:33:39.280]   Take a look at statements by local politicians.
[01:33:39.280 --> 01:33:41.600]   See how they are reacting.
[01:33:41.600 --> 01:33:45.960]   Look, this is too much work for American journalists, but okay, look up the patterns of violence
[01:33:45.960 --> 01:33:50.640]   around elections, race issues, politics, leave your biases and baggage at the door.
[01:33:50.640 --> 01:33:56.640]   Keep in mind that some of the voices you hear you will hear pontificating or not from what
[01:33:56.640 --> 01:34:01.600]   is, what is hashtag I key K a, um, it was the bombing was I think, okay.
[01:34:01.600 --> 01:34:05.960]   And have not been for decades and understand the zeitgeist, the bad, zeitgeist about as
[01:34:05.960 --> 01:34:07.640]   much as my cat understands Twitter.
[01:34:07.640 --> 01:34:12.640]   Look for local journalists on the hashtag L K a tag.
[01:34:12.640 --> 01:34:14.120]   I get, well Sri Lanka.
[01:34:14.120 --> 01:34:15.120]   I don't know.
[01:34:15.120 --> 01:34:18.360]   Anyway, whether you like it or not, the overwhelming mind share your platforms have enabled you
[01:34:18.360 --> 01:34:22.000]   to set the global narrative around Sri Lanka, do your job be responsible.
[01:34:22.000 --> 01:34:29.240]   So what he's saying is, uh, in fact, social media is, is the truth teller in this case,
[01:34:29.240 --> 01:34:34.360]   not the mainstream media in Sri Lanka and the way that another, uh, thread which I think
[01:34:34.360 --> 01:34:37.960]   I put in right above there on the, on the, um, rundown, said it's also where you can
[01:34:37.960 --> 01:34:39.760]   find your family.
[01:34:39.760 --> 01:34:46.240]   Um, you know, there's a lot here, uh, but again, goes to the people who are not in power of
[01:34:46.240 --> 01:34:47.240]   communication.
[01:34:47.240 --> 01:34:48.240]   Do bad things happen there?
[01:34:48.240 --> 01:34:49.240]   Yes.
[01:34:49.240 --> 01:34:50.240]   Do the bad guys use it.
[01:34:50.240 --> 01:34:51.240]   But the truth is you close social media.
[01:34:51.240 --> 01:34:55.480]   The bad guys are going to plenty of platforms means they have.
[01:34:55.480 --> 01:34:56.480]   Yeah.
[01:34:56.480 --> 01:34:58.560]   It's the victims who don't have a means anymore.
[01:34:58.560 --> 01:34:59.560]   Yeah.
[01:34:59.560 --> 01:35:04.880]   So that is really important and interesting point and highlights how hard it is to judge
[01:35:04.880 --> 01:35:09.680]   this stuff because of cultural assumptions, whether it's Austin versus Kansas or the
[01:35:09.680 --> 01:35:13.760]   US versus Sri Lanka is Cara Swisher jumped on to it.
[01:35:13.760 --> 01:35:17.560]   She had a New York times column and jumped in quickly to say that, you know, her first
[01:35:17.560 --> 01:35:22.640]   reaction was she was happy about this and people, a lot of people sent her a lot, including
[01:35:22.640 --> 01:35:28.200]   uh, Rasmus, Klesha Nielsen at the Oxford, uh, Reuters Institute for the Society of Journalism,
[01:35:28.200 --> 01:35:33.360]   you know, I got researchers out who were talking about what the research is and what happens
[01:35:33.360 --> 01:35:35.320]   when you shut down these kinds of networks.
[01:35:35.320 --> 01:35:41.040]   It doesn't, by the quality of the research thus far, it doesn't end the violence.
[01:35:41.040 --> 01:35:45.240]   And so we've got to deal with the reality and evidence of what's really happening rather
[01:35:45.240 --> 01:35:47.160]   than these emotional responses.
[01:35:47.160 --> 01:35:49.200]   Is it still shut down?
[01:35:49.200 --> 01:35:50.200]   I don't know.
[01:35:50.200 --> 01:35:51.360]   I was wondering that myself.
[01:35:51.360 --> 01:35:52.360]   Yeah.
[01:35:52.360 --> 01:35:55.880]   So if it were, that would confirm this, which is basically it's the government saying we,
[01:35:55.880 --> 01:36:05.320]   we want to tightly control the story, probably in our favor, uh, as opposed to, uh, let,
[01:36:05.320 --> 01:36:08.320]   uh, original and honest voices come out.
[01:36:08.320 --> 01:36:12.560]   See in the United States, it's, it's just at, well at least we hope we feel like it's
[01:36:12.560 --> 01:36:22.480]   the opposite that social media is where amplification occurs for all the, you know, weird dystopian
[01:36:22.480 --> 01:36:24.280]   and, uh, and violent.
[01:36:24.280 --> 01:36:27.600]   But no, we just, we just in the media tend to focus on those things.
[01:36:27.600 --> 01:36:31.800]   I mean, again, you see some really good things, Black Lives Matter and all of that.
[01:36:31.800 --> 01:36:34.000]   Like this is very informative.
[01:36:34.000 --> 01:36:35.920]   This is hugely valuable.
[01:36:35.920 --> 01:36:37.400]   Uh, it's good.
[01:36:37.400 --> 01:36:38.400]   It's great.
[01:36:38.400 --> 01:36:41.560]   You know, so that's, that is the value of, uh, of Twitter.
[01:36:41.560 --> 01:36:43.560]   So there you go.
[01:36:43.560 --> 01:36:45.060]   Wait a second.
[01:36:45.060 --> 01:36:47.240]   Did I just hear you say that is the value?
[01:36:47.240 --> 01:36:51.560]   Well, it's one of the values of Twitter and I never just, uh, disagreed with that.
[01:36:51.560 --> 01:36:56.760]   It is a chance for, uh, all voices to be heard just as the internet is just as podcasting.
[01:36:56.760 --> 01:37:02.760]   It's just as YouTube is, uh, but we can't, we can't ignore the, you know, bailful.
[01:37:02.760 --> 01:37:09.720]   Uh, but even there, you know, again, your celebrity and Stacy, your female celebrity.
[01:37:09.720 --> 01:37:13.720]   And so that has an impact, but my Facebook feed is fine.
[01:37:13.720 --> 01:37:15.240]   There's nothing bad in it.
[01:37:15.240 --> 01:37:16.240]   Nothing.
[01:37:16.240 --> 01:37:17.240]   Yeah.
[01:37:17.240 --> 01:37:19.640]   Except did you read this thing that Facebook did?
[01:37:19.640 --> 01:37:20.640]   Okay.
[01:37:20.640 --> 01:37:23.120]   Well, now that day, you're going to a bit of a non sequitur here.
[01:37:23.120 --> 01:37:24.640]   You're going to complain about Facebook, which is fine.
[01:37:24.640 --> 01:37:26.120]   I was talking about the company.
[01:37:26.120 --> 01:37:27.120]   I'm not complaining.
[01:37:27.120 --> 01:37:28.120]   That's fine.
[01:37:28.120 --> 01:37:29.120]   That's a different thing.
[01:37:29.120 --> 01:37:35.800]   Uh, but remember the feed is system, the feed is managed by this company that is clearly
[01:37:35.800 --> 01:37:38.100]   immoral.
[01:37:38.100 --> 01:37:39.100]   So that's my closure.
[01:37:39.100 --> 01:37:40.920]   I raised money from my school from Facebook.
[01:37:40.920 --> 01:37:41.920]   I'm independent of Facebook.
[01:37:41.920 --> 01:37:43.800]   I don't think you're defending Facebook for that reason.
[01:37:43.800 --> 01:37:44.800]   No, no, no, I'm not.
[01:37:44.800 --> 01:37:48.240]   But I'm just going to let photos closure before we get into it.
[01:37:48.240 --> 01:37:55.120]   Facebook unintentionally, they say, uploaded email contacts of 1.5 million new Facebook
[01:37:55.120 --> 01:37:58.920]   users since May 2016.
[01:37:58.920 --> 01:38:06.000]   It turns out that when some people were joining Facebook, it was asking users to enter their
[01:38:06.000 --> 01:38:13.960]   email, not email address, email passwords to verify their identities.
[01:38:13.960 --> 01:38:14.960]   Facebook.
[01:38:14.960 --> 01:38:15.960]   That's ridiculous.
[01:38:15.960 --> 01:38:16.960]   That's ridiculous.
[01:38:16.960 --> 01:38:20.000]   Yes, for millions of people.
[01:38:20.000 --> 01:38:23.240]   Furthermore, no, so okay.
[01:38:23.240 --> 01:38:27.240]   So first of all, a normal site says, is this your email and then sends you an email and
[01:38:27.240 --> 01:38:29.360]   says, click to confirm.
[01:38:29.360 --> 01:38:30.440]   That would be sufficient.
[01:38:30.440 --> 01:38:32.520]   You control this email address.
[01:38:32.520 --> 01:38:40.840]   Facebook asked for email addresses and passwords to verify what they did was logged in and imported
[01:38:40.840 --> 01:38:46.400]   all your contacts without telling you.
[01:38:46.400 --> 01:38:50.440]   Facebook disclosed on Wednesday to Business Insider that 1.5 million people's contacts
[01:38:50.440 --> 01:38:56.680]   were collected this way, fed into Facebook's systems where they were used to improve Facebook's
[01:38:56.680 --> 01:38:58.800]   ad targeting.
[01:38:58.800 --> 01:39:03.360]   Yeah, maybe build Facebook's web of social connections, definitely.
[01:39:03.360 --> 01:39:05.760]   And oh, yeah, recommend friends to add.
[01:39:05.760 --> 01:39:10.360]   Yep, that's how they got those creepy things about your therapist and everybody else showing
[01:39:10.360 --> 01:39:11.360]   up on Facebook.
[01:39:11.360 --> 01:39:13.080]   No, no, yeah.
[01:39:13.080 --> 01:39:14.080]   We now know.
[01:39:14.080 --> 01:39:18.720]   It was exactly what everyone's worst case scenario was suspected.
[01:39:18.720 --> 01:39:20.200]   I mean, that's.
[01:39:20.200 --> 01:39:21.360]   And you cannot.
[01:39:21.360 --> 01:39:22.560]   Facebook grew up.
[01:39:22.560 --> 01:39:24.480]   This is not unintentional.
[01:39:24.480 --> 01:39:26.520]   Oh, well, wait a minute.
[01:39:26.520 --> 01:39:29.240]   How do you unintentionally ask for a password?
[01:39:29.240 --> 01:39:30.720]   Oh, again, asking.
[01:39:30.720 --> 01:39:31.720]   Download the content.
[01:39:31.720 --> 01:39:32.720]   What was it?
[01:39:32.720 --> 01:39:33.720]   What do you mean to do that?
[01:39:33.720 --> 01:39:34.720]   No, I agree.
[01:39:34.720 --> 01:39:35.720]   I agree.
[01:39:35.720 --> 01:39:36.720]   I was like, no, that's a feature.
[01:39:36.720 --> 01:39:38.560]   So you actually specifically had to build that.
[01:39:38.560 --> 01:39:39.560]   Yeah, you built it.
[01:39:39.560 --> 01:39:43.600]   Somebody coded that and improved it and it got into the code base.
[01:39:43.600 --> 01:39:47.360]   So my point is that, yeah, your feed might be fine, but the company that controls what
[01:39:47.360 --> 01:39:50.600]   you see in that feed does stuff like that.
[01:39:50.600 --> 01:39:51.600]   Yeah.
[01:39:51.600 --> 01:39:52.600]   Nothing.
[01:39:52.600 --> 01:40:00.680]   Facebook does surprises me.
[01:40:00.680 --> 01:40:03.400]   I mean, maybe I would be surprised.
[01:40:03.400 --> 01:40:05.840]   It's not a screw up.
[01:40:05.840 --> 01:40:06.840]   That is.
[01:40:06.840 --> 01:40:07.840]   No, that's.
[01:40:07.840 --> 01:40:10.800]   And by the way, I mean, I mean that I read the wrong thing.
[01:40:10.800 --> 01:40:12.680]   He created a new Instagram account.
[01:40:12.680 --> 01:40:13.760]   Please don't ask to follow me.
[01:40:13.760 --> 01:40:16.240]   It's private and you will not be allowed.
[01:40:16.240 --> 01:40:17.240]   Can you?
[01:40:17.240 --> 01:40:18.400]   Can I get in?
[01:40:18.400 --> 01:40:24.160]   No, for one reason, Lisa said, look, I post all of our vacation pictures on Instagram.
[01:40:24.160 --> 01:40:25.160]   Right.
[01:40:25.160 --> 01:40:26.160]   Create an account.
[01:40:26.160 --> 01:40:27.160]   Follow me.
[01:40:27.160 --> 01:40:28.400]   I only follow her and you will be able to see them.
[01:40:28.400 --> 01:40:29.640]   That made sense.
[01:40:29.640 --> 01:40:31.640]   I cannot tell you how many times I've lost count.
[01:40:31.640 --> 01:40:35.840]   How many times that Instagram account has asked for my contact list again and again and
[01:40:35.840 --> 01:40:38.480]   again and again and again.
[01:40:38.480 --> 01:40:41.160]   And it continues to do so.
[01:40:41.160 --> 01:40:42.360]   Growth hacking.
[01:40:42.360 --> 01:40:50.400]   Yeah, it's growth hacking, but honestly, please do not forget that when I give my contact
[01:40:50.400 --> 01:40:54.120]   list to a company, I am not selling myself out.
[01:40:54.120 --> 01:40:58.680]   I'm selling every single person I know out their name, their address, their phone number,
[01:40:58.680 --> 01:40:59.680]   their address.
[01:40:59.680 --> 01:41:01.520]   I am giving that to Facebook.
[01:41:01.520 --> 01:41:06.920]   Every single one of my 4,000 contacts, I would be giving every one of those 4,000 people
[01:41:06.920 --> 01:41:11.880]   personal information, your personal information, Stacy, yours, Jeff, to Facebook.
[01:41:11.880 --> 01:41:16.120]   I bet they ask for that again and again and again and again.
[01:41:16.120 --> 01:41:20.760]   And they pretend that it's just so we can tell you when your friend joins Instagram.
[01:41:20.760 --> 01:41:22.720]   Truth is they already have it.
[01:41:22.720 --> 01:41:26.480]   I'm so surprised that they, I remember.
[01:41:26.480 --> 01:41:30.760]   So I very early in my journalism career, my publisher, actually.
[01:41:30.760 --> 01:41:34.400]   So as a journalist, I have all my sources and contacts, right?
[01:41:34.400 --> 01:41:36.200]   It's a pretty great list, right?
[01:41:36.200 --> 01:41:41.280]   And our publisher really wanted it because they wanted to spam our contacts with ads or
[01:41:41.280 --> 01:41:43.440]   ad pitches basically, bye from us.
[01:41:43.440 --> 01:41:49.960]   And so we actually, she went into our shared server, so our Microsoft Exchange server, I
[01:41:49.960 --> 01:41:53.120]   think it was, and was trying to pull our contact.
[01:41:53.120 --> 01:41:57.280]   Oh my God, I hope she was so weird.
[01:41:57.280 --> 01:41:59.440]   This was, she thought this was awesome.
[01:41:59.440 --> 01:42:04.800]   So we actually had to pull all of our contacts off that and had to like keep our contacts
[01:42:04.800 --> 01:42:06.800]   completely separately.
[01:42:06.800 --> 01:42:11.360]   And to me, it is such, and this was, this was in 2000, right?
[01:42:11.360 --> 01:42:19.280]   So it is so weird to me how much that mindset has shifted for people because like, not just
[01:42:19.280 --> 01:42:22.320]   as a journalist, but again, like you said, it's your friends, it's your family.
[01:42:22.320 --> 01:42:24.080]   Why would you sell them out?
[01:42:24.080 --> 01:42:27.600]   Especially if they, you know, don't want you to share that.
[01:42:27.600 --> 01:42:30.880]   So yeah, it's nuts.
[01:42:30.880 --> 01:42:33.000]   I never tried that.
[01:42:33.000 --> 01:42:38.400]   What has evolved in our understanding of this is we, you used to be for, oh, they want it
[01:42:38.400 --> 01:42:39.760]   for a mailing list.
[01:42:39.760 --> 01:42:40.760]   That's valuable.
[01:42:40.760 --> 01:42:41.760]   You could sell those names.
[01:42:41.760 --> 01:42:42.760]   It's not about a mailing list.
[01:42:42.760 --> 01:42:47.800]   It's about building such something so much more sophisticated, a social graph, friends
[01:42:47.800 --> 01:42:53.680]   of friends, Facebook really deeply understood this early on.
[01:42:53.680 --> 01:42:56.360]   They understood how valuable a social graph was.
[01:42:56.360 --> 01:42:59.880]   Well, getting your email password is phishing.
[01:42:59.880 --> 01:43:01.760]   Oh, I would hope.
[01:43:01.760 --> 01:43:07.360]   I would hope that people faced with that would say no, but no, right.
[01:43:07.360 --> 01:43:10.240]   People do because it's their innocent people.
[01:43:10.240 --> 01:43:11.240]   They don't know this.
[01:43:11.240 --> 01:43:13.320]   They just, there are friends and family.
[01:43:13.320 --> 01:43:15.120]   They're not geeks.
[01:43:15.120 --> 01:43:19.160]   If you and I were asked for a password from Facebook, we wouldn't give it to them.
[01:43:19.160 --> 01:43:21.320]   We've been talking about it for an hour on the show.
[01:43:21.320 --> 01:43:22.320]   Yeah.
[01:43:22.320 --> 01:43:24.040]   You've worn people off of your radio.
[01:43:24.040 --> 01:43:25.040]   No, it is.
[01:43:25.040 --> 01:43:26.040]   We should make the point.
[01:43:26.040 --> 01:43:27.040]   It wasn't universal on Facebook.
[01:43:27.040 --> 01:43:29.120]   Maybe it was a pilot program.
[01:43:29.120 --> 01:43:30.520]   Maybe they were just, I don't know.
[01:43:30.520 --> 01:43:33.000]   I mean, it wasn't something everybody had.
[01:43:33.000 --> 01:43:39.680]   That's the problem with so much of this, all the platforms is there's a, there's a back
[01:43:39.680 --> 01:43:41.200]   to the early days.
[01:43:41.200 --> 01:43:43.560]   There's a very low tolerance for always just a test.
[01:43:43.560 --> 01:43:44.560]   Yeah.
[01:43:44.560 --> 01:43:45.560]   Move fast.
[01:43:45.560 --> 01:43:46.560]   And it doesn't represent policy.
[01:43:46.560 --> 01:43:47.840]   We were just trying something out.
[01:43:47.840 --> 01:43:52.640]   You can't use that excuse anymore because what you're doing is going to be seen and speaks
[01:43:52.640 --> 01:43:54.640]   for you as a company.
[01:43:54.640 --> 01:43:58.920]   But that's, that's, that's so historic and the way these cultures work.
[01:43:58.920 --> 01:44:03.200]   And that was, I had real trepidation about creating an Instagram account because I don't
[01:44:03.200 --> 01:44:06.880]   want to give Facebook any information.
[01:44:06.880 --> 01:44:12.200]   And just the fact that I have that Instagram on my, on my phone is, I know giving them a
[01:44:12.200 --> 01:44:14.600]   lot of information because there's location information.
[01:44:14.600 --> 01:44:18.920]   There's all sorts of information that is now leaking out to Facebook.
[01:44:18.920 --> 01:44:19.920]   It's good.
[01:44:19.920 --> 01:44:20.920]   We've become aware of that.
[01:44:20.920 --> 01:44:25.080]   We can now, we're now a little bit more at choice about how we do that.
[01:44:25.080 --> 01:44:29.240]   And of course I won't, we've talked about it, not on this show before, but about the Journal
[01:44:29.240 --> 01:44:35.000]   of American Medical Association, the article they published, they got 36 apps, mental health,
[01:44:35.000 --> 01:44:39.320]   they searched for them with two keywords, smoking cessation and depression.
[01:44:39.320 --> 01:44:47.120]   They got 36 mental health and smoking cessation apps of the 3633 sent information from those
[01:44:47.120 --> 01:44:49.640]   apps to other third parties.
[01:44:49.640 --> 01:44:52.200]   How did they, I couldn't forget from the story.
[01:44:52.200 --> 01:44:54.800]   They could do that or they didn't warn that they wouldn't do that.
[01:44:54.800 --> 01:44:57.160]   How did they know that they did do that?
[01:44:57.160 --> 01:45:01.960]   You put a, you put something like Wireshark on the actual network, and you can see the
[01:45:01.960 --> 01:45:03.960]   outbound traffic.
[01:45:03.960 --> 01:45:08.320]   So it's a simple device, you know, like a pineapple that you put on the wifi and you
[01:45:08.320 --> 01:45:09.320]   can watch.
[01:45:09.320 --> 01:45:10.960]   Well, but it's outbound to their server.
[01:45:10.960 --> 01:45:12.800]   How do you know it's then outbound in turn?
[01:45:12.800 --> 01:45:14.560]   It wasn't always to other parties.
[01:45:14.560 --> 01:45:16.520]   That's what, that's part of the problem.
[01:45:16.520 --> 01:45:19.760]   Well, wait, wait, wait, wait, wait, I'm just, again, I'm just, I'm not trying to defend
[01:45:19.760 --> 01:45:20.760]   anybody here.
[01:45:20.760 --> 01:45:21.760]   I'm just trying to understand.
[01:45:21.760 --> 01:45:28.320]   If I'm using an app that tracks my progress, clearly it's remembering me and my progress.
[01:45:28.320 --> 01:45:29.520]   It is going to the server.
[01:45:29.520 --> 01:45:31.400]   It is not just staying on my phone.
[01:45:31.400 --> 01:45:33.000]   Yeah, but it's on the side.
[01:45:33.000 --> 01:45:34.800]   How do they know?
[01:45:34.800 --> 01:45:40.800]   How did the authors of the, of the, of the, of the drama piece know that the company then
[01:45:40.800 --> 01:45:42.400]   sent that to other parties?
[01:45:42.400 --> 01:45:43.400]   Well, they wouldn't.
[01:45:43.400 --> 01:45:44.400]   So you send it.
[01:45:44.400 --> 01:45:45.400]   So I'm saying hold on.
[01:45:45.400 --> 01:45:50.480]   Just the story headline should be could do this, but they don't know that they did do
[01:45:50.480 --> 01:45:52.480]   this is what I couldn't find it from the story.
[01:45:52.480 --> 01:45:53.480]   You can, you can.
[01:45:53.480 --> 01:45:58.080]   So if you run, if you run something like wire shark or anything that shows where your traffic
[01:45:58.080 --> 01:46:02.400]   goes, what happens is some bits go to like a Facebook server or whoever server and then
[01:46:02.400 --> 01:46:06.080]   some will go to like an ad traffic or a third party server.
[01:46:06.080 --> 01:46:07.360]   So you can see all of that.
[01:46:07.360 --> 01:46:11.800]   It's not just one giant stream of traffic that's, you know, of course there's that
[01:46:11.800 --> 01:46:14.360]   in service, but that's, that's no surprise.
[01:46:14.360 --> 01:46:18.880]   Listen, I, I think there should be rules about anything that has this kind of personal
[01:46:18.880 --> 01:46:19.880]   information.
[01:46:19.880 --> 01:46:24.640]   I don't know if that at all, but this, but the journalism in the story, the headlines
[01:46:24.640 --> 01:46:26.920]   on some of them said did do this.
[01:46:26.920 --> 01:46:31.200]   And others said could do this big difference.
[01:46:31.200 --> 01:46:35.920]   And so what the actual policy and actions of the company are, in the fact that they
[01:46:35.920 --> 01:46:39.000]   didn't tell people that they had the data and more of them, all, all that.
[01:46:39.000 --> 01:46:40.000]   Yes, fix that.
[01:46:40.000 --> 01:46:42.640]   I'm not arguing with any of that, but I'm questioning.
[01:46:42.640 --> 01:46:48.080]   I'm just trying to question the conclusions of the story and how they reported this.
[01:46:48.080 --> 01:46:51.400]   I'm not sure that I get how they could have.
[01:46:51.400 --> 01:46:52.440]   Well, you're right.
[01:46:52.440 --> 01:47:00.200]   You cannot tell if the company, if the data is sent to the, first of all, it's probably
[01:47:00.200 --> 01:47:07.240]   the case that information about substance abuse, you know, measurements like that, it's
[01:47:07.240 --> 01:47:11.040]   probably the case they shouldn't be sent to the company at all, especially if the privacy
[01:47:11.040 --> 01:47:13.000]   policy doesn't say it's going to be.
[01:47:13.000 --> 01:47:16.520]   You can see if it's sent to a third party directly, which apparently in some cases it
[01:47:16.520 --> 01:47:17.520]   was, but you can't tell it.
[01:47:17.520 --> 01:47:21.680]   But that third party was an ad server.
[01:47:21.680 --> 01:47:22.960]   That's just the way to web up.
[01:47:22.960 --> 01:47:25.480]   Maybe it's an ad server, but why would they send the ad server?
[01:47:25.480 --> 01:47:26.920]   What I ate yesterday.
[01:47:26.920 --> 01:47:27.920]   Right.
[01:47:27.920 --> 01:47:31.160]   They're like, what data is getting sent there?
[01:47:31.160 --> 01:47:35.040]   Again, we just need to know more about this.
[01:47:35.040 --> 01:47:40.640]   And I agree with the basic premise of this is that when it comes to, especially matters
[01:47:40.640 --> 01:47:41.640]   of health and mental.
[01:47:41.640 --> 01:47:46.800]   Well, the other problem is there should be standards here that are, they found several
[01:47:46.800 --> 01:47:50.880]   cases where the apps lied about their privacy policy, other cases where they went.
[01:47:50.880 --> 01:47:51.880]   That's all bad.
[01:47:51.880 --> 01:47:52.880]   Yeah.
[01:47:52.880 --> 01:47:58.320]   And this is in JAMA because the American Medical Association because it's his aim that care
[01:47:58.320 --> 01:48:01.320]   providers, physicians, they don't know this.
[01:48:01.320 --> 01:48:04.320]   Oh, you guys, this happened to me.
[01:48:04.320 --> 01:48:07.680]   So I actually had a conversation with my daughter's pediatrician.
[01:48:07.680 --> 01:48:11.040]   So I went to their office and they were using a service.
[01:48:11.040 --> 01:48:13.760]   Oh gosh, what was it called?
[01:48:13.760 --> 01:48:19.200]   Off to look it up, but they were using a service and when I, to, to check in basically.
[01:48:19.200 --> 01:48:22.240]   And at first I was excited because I was, I always hate filling out the paperwork and
[01:48:22.240 --> 01:48:25.000]   every year you have to fill out the same paperwork and you're like, God, if this were
[01:48:25.000 --> 01:48:29.880]   online, you would just have this data and I'd change it if it changed.
[01:48:29.880 --> 01:48:30.880]   But it was the service.
[01:48:30.880 --> 01:48:35.560]   And as I was clicking through the, the end and the HIPAA compliance and all this, I came
[01:48:35.560 --> 01:48:41.000]   to a page that told me that my data on this particular form, which did ask for like my
[01:48:41.000 --> 01:48:45.480]   daughter's like, is she experiencing neurological things or these kinds of things that it would
[01:48:45.480 --> 01:48:50.360]   be shared with both the company providing the service and its third party advertisers
[01:48:50.360 --> 01:48:54.560]   and other other violation of HIPAA?
[01:48:54.560 --> 01:48:58.520]   No, because it's not actually her medical information apparently.
[01:48:58.520 --> 01:49:01.760]   It's just me filling out symptoms for my doctor.
[01:49:01.760 --> 01:49:03.440]   So it's not my doctor's data.
[01:49:03.440 --> 01:49:08.280]   It's me saying to my doctor what's wrong with her while we were while, why we were there.
[01:49:08.280 --> 01:49:14.440]   So I talked to my doctor about this because I refused to sign that part, but you know,
[01:49:14.440 --> 01:49:17.560]   I'd already filled in everything, right?
[01:49:17.560 --> 01:49:22.920]   And the physician was like, the doctor's admins were like, you need to, this is our
[01:49:22.920 --> 01:49:24.000]   check-in process now.
[01:49:24.000 --> 01:49:25.680]   This is the process.
[01:49:25.680 --> 01:49:28.640]   And so when I showed it to her, she's like, yeah, we'll just decline that.
[01:49:28.640 --> 01:49:32.480]   And I'm like, oh boy, but nobody does that.
[01:49:32.480 --> 01:49:33.680]   So I declined it.
[01:49:33.680 --> 01:49:34.680]   Yeah, nobody does.
[01:49:34.680 --> 01:49:35.680]   I mean, kids were filling this out.
[01:49:35.680 --> 01:49:36.680]   They were having a tablet.
[01:49:36.680 --> 01:49:37.680]   It was exciting.
[01:49:37.680 --> 01:49:41.840]   And then I think it was Frisia, maybe it was the name of the company.
[01:49:41.840 --> 01:49:46.120]   And then I talked to the doctor and she's like, I was like, look, it's not cool that
[01:49:46.120 --> 01:49:49.760]   you force your patients to give up this data.
[01:49:49.760 --> 01:49:54.880]   And she's using it because she's using it because she didn't realize that was the case.
[01:49:54.880 --> 01:49:58.480]   And she got it's a discount for her.
[01:49:58.480 --> 01:50:00.480]   Like it's cheaper and easier for her.
[01:50:00.480 --> 01:50:01.480]   Oh gee.
[01:50:01.480 --> 01:50:03.800]   I wonder why they gave her a discount.
[01:50:03.800 --> 01:50:04.800]   Right.
[01:50:04.800 --> 01:50:07.040]   Well, but I mean, I like our.
[01:50:07.040 --> 01:50:08.040]   I like our pediatric.
[01:50:08.040 --> 01:50:09.040]   Oh, no.
[01:50:09.040 --> 01:50:10.040]   He's a lovely person.
[01:50:10.040 --> 01:50:11.040]   She's a good person.
[01:50:11.040 --> 01:50:12.040]   She does not.
[01:50:12.040 --> 01:50:13.040]   Yeah.
[01:50:13.040 --> 01:50:17.440]   And this is scary and it's frustrating.
[01:50:17.440 --> 01:50:22.000]   And when you are a patient, for example, we were there for, I don't remember.
[01:50:22.000 --> 01:50:25.160]   We were there for something, but usually when you go to the doctor, you're sick and you
[01:50:25.160 --> 01:50:27.000]   need or it's an appointment.
[01:50:27.000 --> 01:50:29.360]   You're a little bit vulnerable.
[01:50:29.360 --> 01:50:30.360]   Yes.
[01:50:30.360 --> 01:50:33.680]   We're going to fall for this.
[01:50:33.680 --> 01:50:36.440]   You've got like no appointments for the next six weeks.
[01:50:36.440 --> 01:50:38.400]   So it's not like you can say, Hey, I'm not going to sign this.
[01:50:38.400 --> 01:50:39.400]   I refuse this.
[01:50:39.400 --> 01:50:41.360]   They're going to be like, Well, we can't see you.
[01:50:41.360 --> 01:50:43.680]   And then you're like, Yeah, I'll just take my tumor and go home.
[01:50:43.680 --> 01:50:48.000]   I mean, you should be very suspicious anytime you're filling something out on a tablet,
[01:50:48.000 --> 01:50:52.680]   not a piece of paper that immediately raises my suspicion.
[01:50:52.680 --> 01:50:54.720]   Well, it's Frigia.
[01:50:54.720 --> 01:50:58.120]   It's P H R E S I A.
[01:50:58.120 --> 01:51:01.840]   Well, we can do on for it.
[01:51:01.840 --> 01:51:03.320]   They don't ask for any personal information.
[01:51:03.320 --> 01:51:07.400]   I'm just going to ask for symptoms.
[01:51:07.400 --> 01:51:13.040]   So let me point you Jeff to the actual article on the JAMA Network.
[01:51:13.040 --> 01:51:14.040]   Okay, good.
[01:51:14.040 --> 01:51:15.040]   Thank you.
[01:51:15.040 --> 01:51:21.120]   Because I couldn't data transmission to one or more third parties was identified for 33
[01:51:21.120 --> 01:51:23.680]   of the 36 apps.
[01:51:23.680 --> 01:51:25.480]   Almost.
[01:51:25.480 --> 01:51:27.720]   Well, you can always tell third party because it's a different.
[01:51:27.720 --> 01:51:29.720]   The third party could be could be.
[01:51:29.720 --> 01:51:31.200]   Well, they talk about this.
[01:51:31.200 --> 01:51:38.080]   They say Google advertising services, Facebook, Google, they talk about where it went.
[01:51:38.080 --> 01:51:42.920]   Almost half the apps transmitted data to a third party, but lacked a privacy policy.
[01:51:42.920 --> 01:51:46.160]   Nine apps failed to disclose his transmission and policy text.
[01:51:46.160 --> 01:51:48.200]   Five apps or explicitly.
[01:51:48.200 --> 01:51:49.400]   This is the worst stated.
[01:51:49.400 --> 01:51:51.320]   The transmission would not occur.
[01:51:51.320 --> 01:51:53.320]   Three apps.
[01:51:53.320 --> 01:51:58.560]   81% transmitted data to analytics and advertising are marketing services operated by Google
[01:51:58.560 --> 01:51:59.560]   and Facebook.
[01:51:59.560 --> 01:52:00.560]   What?
[01:52:00.560 --> 01:52:01.560]   Data.
[01:52:01.560 --> 01:52:06.280]   The fact that this user is on this app and you can send them an ad the ads there.
[01:52:06.280 --> 01:52:07.280]   That's obvious.
[01:52:07.280 --> 01:52:08.280]   That's how it works.
[01:52:08.280 --> 01:52:09.600]   But if matter is better.
[01:52:09.600 --> 01:52:10.600]   If matter is better.
[01:52:10.600 --> 01:52:11.600]   If matter is better.
[01:52:11.600 --> 01:52:12.600]   If matter is better.
[01:52:12.600 --> 01:52:13.600]   If matter is great.
[01:52:13.600 --> 01:52:16.440]   No, matters greatly as to what the data is.
[01:52:16.440 --> 01:52:17.880]   Presence on the app and there's an ad there.
[01:52:17.880 --> 01:52:18.880]   Yeah.
[01:52:18.880 --> 01:52:19.880]   It's going to be sent to an answer.
[01:52:19.880 --> 01:52:20.880]   The.
[01:52:20.880 --> 01:52:21.880]   No, it's not.
[01:52:21.880 --> 01:52:24.240]   The question is did it send other data and use it another way?
[01:52:24.240 --> 01:52:25.240]   That matters.
[01:52:25.240 --> 01:52:26.240]   That's all I'm saying.
[01:52:26.240 --> 01:52:27.240]   No.
[01:52:27.240 --> 01:52:28.240]   It may be bad bad things.
[01:52:28.240 --> 01:52:29.320]   But I want to know the details.
[01:52:29.320 --> 01:52:34.560]   If you are getting if you are on a site or on an app that's for depression suffers,
[01:52:34.560 --> 01:52:41.920]   for example, and that data, the fact that you're on that app is damning in and of itself.
[01:52:41.920 --> 01:52:45.680]   Even the information there is damning.
[01:52:45.680 --> 01:52:48.120]   So just the fact.
[01:52:48.120 --> 01:52:49.120]   Yeah.
[01:52:49.120 --> 01:52:50.760]   It's like when someone sees you walking.
[01:52:50.760 --> 01:52:51.760]   I don't agree.
[01:52:51.760 --> 01:52:55.360]   But if you're on an ad supported site for depression.
[01:52:55.360 --> 01:52:56.360]   That's an ad.
[01:52:56.360 --> 01:52:59.120]   The ad has to come to a party.
[01:52:59.120 --> 01:53:00.120]   And there is.
[01:53:00.120 --> 01:53:05.360]   So it's an app and the challenge here is doctors are not being given this information about
[01:53:05.360 --> 01:53:07.360]   how these things are sharing data.
[01:53:07.360 --> 01:53:12.080]   And so they will tell their patients, Hey, use this app to help manage your depression
[01:53:12.080 --> 01:53:16.720]   and a patient thinking that doctor knows will say, Okay.
[01:53:16.720 --> 01:53:19.920]   And then their depression has been outed to Google.
[01:53:19.920 --> 01:53:20.920]   I understand.
[01:53:20.920 --> 01:53:22.280]   I absolutely agree.
[01:53:22.280 --> 01:53:23.280]   Wait, wait, wait, wait.
[01:53:23.280 --> 01:53:26.200]   I absolutely agree for the need for standards here.
[01:53:26.200 --> 01:53:28.120]   I agree that that's not going to be bad.
[01:53:28.120 --> 01:53:33.560]   But I'm questioning is very narrow here is the specific reporting in JAMA.
[01:53:33.560 --> 01:53:35.600]   What data about to get the answer?
[01:53:35.600 --> 01:53:37.320]   I'm going to answer.
[01:53:37.320 --> 01:53:39.000]   That's what I'm asking.
[01:53:39.000 --> 01:53:43.800]   So the conclusion of it is users should be aware their use of ostensibly standalone
[01:53:43.800 --> 01:53:45.760]   mental health apps and the health status.
[01:53:45.760 --> 01:53:51.040]   This implies maybe linked to other data for other purposes, such as marketing targeting
[01:53:51.040 --> 01:53:52.280]   mental illness.
[01:53:52.280 --> 01:53:56.760]   Critically, this may take place even if an app provides no visible clues like a Facebook
[01:53:56.760 --> 01:54:01.040]   login and even for users who do not have a Facebook account.
[01:54:01.040 --> 01:54:06.040]   This study was not designed to answer your question to identify was not designed to identify
[01:54:06.040 --> 01:54:09.840]   whether linkable information was actually being used by advertisers.
[01:54:09.840 --> 01:54:14.440]   For example, to subsequently drive tailored advertising, future work could consider looking
[01:54:14.440 --> 01:54:18.600]   for direct evidence of linkable information being used in this way by looking for changes
[01:54:18.600 --> 01:54:21.760]   in advertisement content, suggest that's critical.
[01:54:21.760 --> 01:54:25.760]   And all the headlines that then came out of the story jumped right to.
[01:54:25.760 --> 01:54:26.760]   I'm going to say it.
[01:54:26.760 --> 01:54:27.760]   I'm going to say it.
[01:54:27.760 --> 01:54:31.680]   The moral panic line of saying, Oh my God, all these places are telling the whole world
[01:54:31.680 --> 01:54:32.680]   that you're nuts.
[01:54:32.680 --> 01:54:40.280]   Well, I think it is a completely appropriate warning to health providers and people who
[01:54:40.280 --> 01:54:42.160]   might want to use these apps.
[01:54:42.160 --> 01:54:43.960]   I agree with that.
[01:54:43.960 --> 01:54:49.800]   But the journalism here, especially in those that we wrote it is shoddy.
[01:54:49.800 --> 01:54:51.760]   I would say it.
[01:54:51.760 --> 01:54:54.480]   No, you do headline.
[01:54:54.480 --> 01:54:55.480]   Everyone does this.
[01:54:55.480 --> 01:54:59.000]   Questions are designed to get people to read the story.
[01:54:59.000 --> 01:55:00.640]   That's a scare headline.
[01:55:00.640 --> 01:55:01.880]   That is a scary data.
[01:55:01.880 --> 01:55:03.880]   Not selling your data.
[01:55:03.880 --> 01:55:10.840]   Well, if it has an ad on it, but this is that's a scare headline.
[01:55:10.840 --> 01:55:11.840]   It's a scare headline.
[01:55:11.840 --> 01:55:13.480]   It's a scary word, but you know what?
[01:55:13.480 --> 01:55:16.000]   I'm sure that's a good.
[01:55:16.000 --> 01:55:17.000]   That's a good.
[01:55:17.000 --> 01:55:18.000]   That's a good.
[01:55:18.000 --> 01:55:19.480]   That mental health app might share your data without telling you.
[01:55:19.480 --> 01:55:20.480]   That's a good headline.
[01:55:20.480 --> 01:55:21.480]   That's the verge.
[01:55:21.480 --> 01:55:22.480]   I think.
[01:55:22.480 --> 01:55:24.880]   Yeah, I'm fine with that.
[01:55:24.880 --> 01:55:28.760]   I'm making a very narrow point here, but this is how these things get a stretch.
[01:55:28.760 --> 01:55:34.360]   And this is how we end up with the UK online harms report wanting to ban speech.
[01:55:34.360 --> 01:55:36.800]   That's what scares me.
[01:55:36.800 --> 01:55:38.200]   Should there be standards here?
[01:55:38.200 --> 01:55:39.200]   Absolutely.
[01:55:39.200 --> 01:55:40.200]   Are places doing some things wrong?
[01:55:40.200 --> 01:55:41.200]   Absolutely.
[01:55:41.200 --> 01:55:48.000]   I think the other conclusion is that these privacy policies lie and are not being there's
[01:55:48.000 --> 01:55:53.440]   no enforcement going on to make sure that they are accurate.
[01:55:53.440 --> 01:55:56.880]   Well, there's that FTC find that just happened against Facebook.
[01:55:56.880 --> 01:56:03.360]   Yeah, I mean, some of it's being enforced, but clearly on these apps, I mean, nobody
[01:56:03.360 --> 01:56:09.200]   has a nobody got time to read all those privacy policies and see if they're accurate.
[01:56:09.200 --> 01:56:11.280]   You think the FTC is doing studies like this?
[01:56:11.280 --> 01:56:13.000]   I don't think so.
[01:56:13.000 --> 01:56:19.920]   Well, I think that the JAMA is quite right to look at this and to lobby, to have standards
[01:56:19.920 --> 01:56:21.840]   in anything matter around health.
[01:56:21.840 --> 01:56:24.760]   I don't think this Washington Post headline is that inaccurate.
[01:56:24.760 --> 01:56:28.520]   Smoking and depression apps, depression apps are selling your data to Google and Facebook
[01:56:28.520 --> 01:56:29.520]   study finds.
[01:56:29.520 --> 01:56:30.520]   Wait, wait, wait, wait.
[01:56:30.520 --> 01:56:31.520]   That's accurate.
[01:56:31.520 --> 01:56:32.640]   No, there's two problems to headline number one.
[01:56:32.640 --> 01:56:36.600]   They're not necessarily selling the data per se.
[01:56:36.600 --> 01:56:42.480]   They are putting in that available in their way every damn site on the entire and app and
[01:56:42.480 --> 01:56:43.680]   the internet works.
[01:56:43.680 --> 01:56:45.400]   Number two, that's the worst.
[01:56:45.400 --> 01:56:46.320]   So we're sending.
[01:56:46.320 --> 01:56:47.320]   Okay.
[01:56:47.320 --> 01:56:49.520]   Let me, I said to let me get to number two.
[01:56:49.520 --> 01:56:54.400]   Number two is the implication of that is they are selling data about your depression and
[01:56:54.400 --> 01:56:55.680]   you're smoking to that.
[01:56:55.680 --> 01:56:58.760]   And they do not have the data to back that up.
[01:56:58.760 --> 01:56:59.760]   It's a matter of journalism.
[01:56:59.760 --> 01:57:00.760]   It's all I'm saying.
[01:57:00.760 --> 01:57:01.760]   Now you can.
[01:57:01.760 --> 01:57:02.760]   They have the data, block that up.
[01:57:02.760 --> 01:57:05.800]   They're saying this person's on a depression site.
[01:57:05.800 --> 01:57:09.480]   Or are you saying specifically just because I'm on a depression site, I might not be depressed.
[01:57:09.480 --> 01:57:14.520]   And that's the way that the stories were written in turn was that they're sharing your
[01:57:14.520 --> 01:57:16.480]   progress against this.
[01:57:16.480 --> 01:57:18.880]   They're sharing details where you are.
[01:57:18.880 --> 01:57:22.960]   And that's, and Gemma admits that that would be a fertile ground for the next study, which
[01:57:22.960 --> 01:57:25.520]   is to see if it changes the ads you get.
[01:57:25.520 --> 01:57:26.520]   Fine.
[01:57:26.520 --> 01:57:27.520]   Okay.
[01:57:27.520 --> 01:57:28.520]   What do you think?
[01:57:28.520 --> 01:57:29.520]   What do you think?
[01:57:29.520 --> 01:57:30.520]   What do you think the answer will be, Jeff?
[01:57:30.520 --> 01:57:31.520]   Do you think it'll change the answer?
[01:57:31.520 --> 01:57:32.520]   My story.
[01:57:32.520 --> 01:57:33.520]   What do you think?
[01:57:33.520 --> 01:57:34.520]   I was curious.
[01:57:34.520 --> 01:57:35.520]   What do you think the answer would be?
[01:57:35.520 --> 01:57:38.960]   Remember my story about how could you say with some certainty?
[01:57:38.960 --> 01:57:40.960]   Can you say with any certainty?
[01:57:40.960 --> 01:57:43.920]   No, it's not going to affect the ads that you went to a depression site.
[01:57:43.920 --> 01:57:45.960]   No, it's not going to affect your ads.
[01:57:45.960 --> 01:57:49.840]   There's Leo, can I answer?
[01:57:49.840 --> 01:57:51.840]   You also could jump to a conclusion there.
[01:57:51.840 --> 01:57:56.120]   Remember I talked to a journalist who talked on Facebook about falling down a lot, and
[01:57:56.120 --> 01:57:59.880]   he got an ad for multiple sclerosis and thus found out that's how he found out he had a
[01:57:59.880 --> 01:58:00.880]   mess.
[01:58:00.880 --> 01:58:01.880]   Right?
[01:58:01.880 --> 01:58:04.960]   And so the fact that someone is targeting a word doesn't mean that Facebook said, "Hey,
[01:58:04.960 --> 01:58:05.960]   we got one here.
[01:58:05.960 --> 01:58:06.960]   And that's patient.
[01:58:06.960 --> 01:58:07.960]   Give them an ad."
[01:58:07.960 --> 01:58:08.960]   Right?
[01:58:08.960 --> 01:58:09.960]   It didn't happen that way.
[01:58:09.960 --> 01:58:13.800]   There are triggers and keywords that are bought and that have implications.
[01:58:13.800 --> 01:58:19.360]   I agree with you guys, have implications, but the simplistic nature of that headline is
[01:58:19.360 --> 01:58:20.360]   misleading.
[01:58:20.360 --> 01:58:21.360]   All right.
[01:58:21.360 --> 01:58:24.720]   Well folks, if you want to get diagnosed for depression, make sure you download and use
[01:58:24.720 --> 01:58:28.520]   a depression app and that way your Facebook ads will offer you antidepressants.
[01:58:28.520 --> 01:58:29.920]   Well, here's the other problem.
[01:58:29.920 --> 01:58:31.920]   This is like a good benefit.
[01:58:31.920 --> 01:58:32.920]   That's a good thing.
[01:58:32.920 --> 01:58:35.800]   Because the problem is a lot of people are getting a lot of benefit out of a lot of these
[01:58:35.800 --> 01:58:36.800]   apps.
[01:58:36.800 --> 01:58:37.800]   No, I think that's...
[01:58:37.800 --> 01:58:42.800]   And no, Leo, I know people who are getting benefit out of these apps.
[01:58:42.800 --> 01:58:46.840]   And so if they are misused, that makes it a triple sin.
[01:58:46.840 --> 01:58:47.840]   I'm agreeing.
[01:58:47.840 --> 01:58:48.840]   Yes.
[01:58:48.840 --> 01:58:51.480]   But don't go to the point of saying, "Never use these apps."
[01:58:51.480 --> 01:58:52.480]   No, no, I'm not saying that.
[01:58:52.480 --> 01:58:53.480]   Because people can get better.
[01:58:53.480 --> 01:58:58.120]   I'm saying, "We ought to make sure these apps deal honestly with their users."
[01:58:58.120 --> 01:58:59.120]   I've agreed 100 times.
[01:58:59.120 --> 01:59:00.120]   Yeah.
[01:59:00.120 --> 01:59:04.120]   And Stacy, you're not going to blame me on your migraine you're getting right now, nor
[01:59:04.120 --> 01:59:06.600]   on the fact that we just told the world that you get them.
[01:59:06.600 --> 01:59:08.160]   And not my fault.
[01:59:08.160 --> 01:59:11.600]   I already know that you told the world because I got all bunch of tweets from people saying,
[01:59:11.600 --> 01:59:13.800]   "Hey, sorry you weren't on Twitter because of the migraines."
[01:59:13.800 --> 01:59:14.960]   No, I think it's the headset.
[01:59:14.960 --> 01:59:16.800]   It's kind of pushing a lot.
[01:59:16.800 --> 01:59:19.200]   We've got to get you a better headset.
[01:59:19.200 --> 01:59:23.080]   No, I just think it's the yelling in the headset.
[01:59:23.080 --> 01:59:24.240]   I'll be honest.
[01:59:24.240 --> 01:59:25.640]   Did you pack your stuff?
[01:59:25.640 --> 01:59:28.480]   Or you pack your nice thing, your microphone or your nice stuff?
[01:59:28.480 --> 01:59:29.480]   Yeah, my fancy mic.
[01:59:29.480 --> 01:59:30.480]   Well, I don't have...
[01:59:30.480 --> 01:59:31.920]   I'm in a two bedroom apartment.
[01:59:31.920 --> 01:59:34.040]   I'm working at a kitchen table right now.
[01:59:34.040 --> 01:59:35.040]   Nice.
[01:59:35.040 --> 01:59:36.040]   And yeah, it has...
[01:59:36.040 --> 01:59:37.040]   It has...
[01:59:37.040 --> 01:59:38.040]   It has...
[01:59:38.040 --> 01:59:39.040]   Much stuff.
[01:59:39.040 --> 01:59:40.040]   Mirrors.
[01:59:40.040 --> 01:59:41.040]   Oh, that's nice.
[01:59:41.040 --> 01:59:43.040]   Did you store a bunch of stuff?
[01:59:43.040 --> 01:59:44.040]   Yes.
[01:59:44.040 --> 01:59:45.040]   Well, pain.
[01:59:45.040 --> 01:59:46.040]   Yes.
[01:59:46.040 --> 01:59:47.640]   Just to separate your stuff.
[01:59:47.640 --> 01:59:52.040]   We should mention that that fine has not been applied yet, but this...
[01:59:52.040 --> 01:59:56.680]   This comes from Facebook's quarterly results, which came out today.
[01:59:56.680 --> 02:00:02.840]   The company has set aside $3 billion related to the FTC's investigation.
[02:00:02.840 --> 02:00:06.560]   They're predicting it could be a fine ranging from $3 to $5 billion.
[02:00:06.560 --> 02:00:07.560]   There's $2 billion.
[02:00:07.560 --> 02:00:08.560]   Yes.
[02:00:08.560 --> 02:00:10.600]   And so they're putting the money in the savings account.
[02:00:10.600 --> 02:00:15.160]   They're putting it in a sock so that if the fine should come through.
[02:00:15.160 --> 02:00:20.480]   But again, Facebook's stock, by the way, went up 10% in after hours trading.
[02:00:20.480 --> 02:00:24.360]   So clearly everything else in the quarterly result was positive.
[02:00:24.360 --> 02:00:25.360]   Yeah.
[02:00:25.360 --> 02:00:26.760]   There's good money in this depression.
[02:00:26.760 --> 02:00:27.760]   Yes.
[02:00:27.760 --> 02:00:33.160]   I think they did say that, "Portal, your favorite device, Leo, is "very valuable" to
[02:00:33.160 --> 02:00:34.160]   the company.
[02:00:34.160 --> 02:00:35.160]   Really?
[02:00:35.160 --> 02:00:36.160]   They're making a lot of money.
[02:00:36.160 --> 02:00:37.160]   Really?
[02:00:37.160 --> 02:00:38.160]   Really?
[02:00:38.160 --> 02:00:39.160]   You just said it was very valuable.
[02:00:39.160 --> 02:00:40.160]   Wow, that's interesting.
[02:00:40.160 --> 02:00:42.120]   Well, I'd be glad to sell anybody.
[02:00:42.120 --> 02:00:45.200]   I'd like to make them more valuable to me.
[02:00:45.200 --> 02:00:47.160]   Did you try to give one to your mother?
[02:00:47.160 --> 02:00:48.160]   Not yet.
[02:00:48.160 --> 02:00:50.280]   Actually, Burke asked if he could take one apart.
[02:00:50.280 --> 02:00:51.760]   I said, "Be my guest."
[02:00:51.760 --> 02:00:53.800]   So Burke's going to do something with one of them.
[02:00:53.800 --> 02:00:55.320]   You'll get a story out of it one day.
[02:00:55.320 --> 02:00:58.960]   Hey, see what data Facebook is gathering from that.
[02:00:58.960 --> 02:01:01.080]   Oh, you know, we could put a wire shark on it.
[02:01:01.080 --> 02:01:02.080]   That's an interesting idea.
[02:01:02.080 --> 02:01:03.080]   There you go.
[02:01:03.080 --> 02:01:04.080]   Yeah.
[02:01:04.080 --> 02:01:05.080]   All right.
[02:01:05.080 --> 02:01:06.080]   Let's do the change log and then we can go home.
[02:01:06.080 --> 02:01:07.080]   How about that?
[02:01:07.080 --> 02:01:15.080]   This is my podcast, Suck.
[02:01:15.080 --> 02:01:17.080]   The Google change log.
[02:01:17.080 --> 02:01:21.600]   Carson, I think that was hostile.
[02:01:21.600 --> 02:01:22.600]   You should walk out now.
[02:01:22.600 --> 02:01:24.560]   He would think he does.
[02:01:24.560 --> 02:01:25.560]   He's completely there.
[02:01:25.560 --> 02:01:30.440]   And I just heard about him, really, and Leo was right.
[02:01:30.440 --> 02:01:32.200]   We give Carson lots of props.
[02:01:32.200 --> 02:01:33.840]   He is a great producer.
[02:01:33.840 --> 02:01:34.840]   I couldn't be happier.
[02:01:34.840 --> 02:01:37.040]   We've had a string of great producers.
[02:01:37.040 --> 02:01:40.080]   And I know you guys, especially Jeff has loved our producers.
[02:01:40.080 --> 02:01:42.160]   Starting with Eileen, remember?
[02:01:42.160 --> 02:01:44.080]   And Chad, right?
[02:01:44.080 --> 02:01:45.080]   Right.
[02:01:45.080 --> 02:01:46.080]   Yeah.
[02:01:46.080 --> 02:01:47.080]   Where is Chad now?
[02:01:47.080 --> 02:01:49.400]   Chad moved back to Austin.
[02:01:49.400 --> 02:01:51.000]   I think, or maybe it's Houston.
[02:01:51.000 --> 02:01:53.000]   It's something with a stun in it.
[02:01:53.000 --> 02:01:58.760]   And he is doing the Gizwiz podcast with the Dick D. Bartolo every week.
[02:01:58.760 --> 02:02:04.400]   And he's still a great YouTuber making money on his Minecraft videos, as far as I know.
[02:02:04.400 --> 02:02:05.400]   Oh, Chad.
[02:02:05.400 --> 02:02:06.400]   Yeah.
[02:02:06.400 --> 02:02:09.000]   And Facebook dropped the price of the portal.
[02:02:09.000 --> 02:02:10.560]   Of course.
[02:02:10.560 --> 02:02:11.720]   And then who is before Carson?
[02:02:11.720 --> 02:02:13.880]   I'm forgetting a whole bunch of people in the middle.
[02:02:13.880 --> 02:02:14.880]   You are.
[02:02:14.880 --> 02:02:15.880]   Jason.
[02:02:15.880 --> 02:02:16.880]   Jason Howell.
[02:02:16.880 --> 02:02:17.880]   And Jason Clanthes.
[02:02:17.880 --> 02:02:18.880]   Oh, Jason Clanthes.
[02:02:18.880 --> 02:02:20.280]   Effin' done, of course.
[02:02:20.280 --> 02:02:22.760]   So we've had quite a few very, very good producers.
[02:02:22.760 --> 02:02:26.440]   Yeah, before Eileen was, what's his name?
[02:02:26.440 --> 02:02:28.160]   Before Eileen, was there some, Dane?
[02:02:28.160 --> 02:02:29.160]   Dane.
[02:02:29.160 --> 02:02:31.040]   Do you go back to the Dane era?
[02:02:31.040 --> 02:02:32.040]   I go back to Dane.
[02:02:32.040 --> 02:02:33.040]   Dane?
[02:02:33.040 --> 02:02:36.000]   Dane actually was the one who said we should do a podcast about Google.
[02:02:36.000 --> 02:02:37.520]   God bless you, Dane.
[02:02:37.520 --> 02:02:38.520]   Yeah.
[02:02:38.520 --> 02:02:40.020]   Thank you, Dane.
[02:02:40.020 --> 02:02:46.960]   So the Google Change Blogger, you can now get Google Fit on your Apple device and iOS.
[02:02:46.960 --> 02:02:49.040]   Competing with Apple Health, I guess.
[02:02:49.040 --> 02:02:50.960]   You're the latest news.
[02:02:50.960 --> 02:02:51.960]   Shoot.
[02:02:51.960 --> 02:02:54.000]   I actually triggered my...
[02:02:54.000 --> 02:02:58.120]   I should...
[02:02:58.120 --> 02:02:59.720]   Yeah.
[02:02:59.720 --> 02:03:04.120]   Google Fit now on iOS.
[02:03:04.120 --> 02:03:05.480]   I don't, you know, I don't...
[02:03:05.480 --> 02:03:06.480]   I like Google Fit.
[02:03:06.480 --> 02:03:08.760]   It's really good.
[02:03:08.760 --> 02:03:09.760]   It's not...
[02:03:09.760 --> 02:03:12.680]   I don't know if it's better than Apple's Health App and Apple's Health App ties into
[02:03:12.680 --> 02:03:14.960]   a lot of other hardware on your...
[02:03:14.960 --> 02:03:17.960]   Through your iPhone, including your Apple Watch, although Google Fit will work with
[02:03:17.960 --> 02:03:20.640]   the Apple Watch too, which is kind of nice.
[02:03:20.640 --> 02:03:24.240]   Google Assistant speakers are now getting free YouTube music streaming.
[02:03:24.240 --> 02:03:27.400]   It's ad supported, but that's nice.
[02:03:27.400 --> 02:03:29.200]   That's very nice.
[02:03:29.200 --> 02:03:32.320]   Google Maps has added EV chargers.
[02:03:32.320 --> 02:03:36.240]   So if you drive an electric vehicle, you can look at the map and find your next charge.
[02:03:36.240 --> 02:03:38.120]   Can I ask you a question, Leo?
[02:03:38.120 --> 02:03:39.120]   Yes.
[02:03:39.120 --> 02:03:41.600]   So if you don't have a Tesla...
[02:03:41.600 --> 02:03:42.600]   Yes.
[02:03:42.600 --> 02:03:43.600]   You have a...
[02:03:43.600 --> 02:03:46.600]   I feel the friend.
[02:03:46.600 --> 02:03:47.600]   Yes.
[02:03:47.600 --> 02:03:48.600]   You can't use...
[02:03:48.600 --> 02:03:49.600]   Tesla charging...
[02:03:49.600 --> 02:03:50.600]   No, no, no.
[02:03:50.600 --> 02:03:54.160]   The superchargers are only for Teslas, and Teslas even have a different connector.
[02:03:54.160 --> 02:03:57.600]   So a Tesla in turn can't use others.
[02:03:57.600 --> 02:03:59.280]   Teslas come with adapters so that you can't.
[02:03:59.280 --> 02:04:00.280]   They have an adapter, yeah.
[02:04:00.280 --> 02:04:01.280]   Well, I forgot.
[02:04:01.280 --> 02:04:02.520]   I forgot, Stacy, I can ask you that too.
[02:04:02.520 --> 02:04:03.520]   I forgot both on Teslas.
[02:04:03.520 --> 02:04:05.520]   You're the only one with a head of me.
[02:04:05.520 --> 02:04:06.520]   Here's my question.
[02:04:06.520 --> 02:04:12.800]   Given all the stuff Elon has said about the new self-driving capabilities of the next generation
[02:04:12.800 --> 02:04:16.000]   Teslas, any Tesla you buy from now on, we'll have that new chip.
[02:04:16.000 --> 02:04:20.040]   You want to talk about something scary?
[02:04:20.040 --> 02:04:23.800]   Are you going to talk about the part where he says, "And there's a setting where if you
[02:04:23.800 --> 02:04:26.600]   don't mind a few fender betters, you can turn it to aggressive mode?"
[02:04:26.600 --> 02:04:28.080]   That one?
[02:04:28.080 --> 02:04:29.080]   That's scary.
[02:04:29.080 --> 02:04:32.080]   That's why driving style.
[02:04:32.080 --> 02:04:33.520]   Yeah, he doesn't like Lidar.
[02:04:33.520 --> 02:04:35.560]   Would you turn on the aggressive mode?
[02:04:35.560 --> 02:04:40.680]   He says you can do it all with cameras radar as we do.
[02:04:40.680 --> 02:04:43.600]   He said that forever though.
[02:04:43.600 --> 02:04:44.600]   He sells...
[02:04:44.600 --> 02:04:45.600]   He's a good salesman.
[02:04:45.600 --> 02:04:46.680]   He sells the capabilities.
[02:04:46.680 --> 02:04:50.480]   So I'm on the cusp of deciding, "Should I get another Tesla or should I get another one?"
[02:04:50.480 --> 02:04:52.240]   Well, I was about to ask you what you're thinking was.
[02:04:52.240 --> 02:04:53.240]   Oh, okay.
[02:04:53.240 --> 02:04:54.840]   Last time you said no, done.
[02:04:54.840 --> 02:04:57.600]   Yeah, now I'm thinking maybe Elon is a good salesman.
[02:04:57.600 --> 02:04:58.600]   Maybe I should.
[02:04:58.600 --> 02:05:02.320]   Well, so this gets to, "Would you lease it or would you buy it?"
[02:05:02.320 --> 02:05:03.320]   It's a lease.
[02:05:03.320 --> 02:05:04.320]   Because I don't feel like...
[02:05:04.320 --> 02:05:06.400]   Yeah, you should not buy any new car.
[02:05:06.400 --> 02:05:07.400]   Yeah.
[02:05:07.400 --> 02:05:08.400]   I feel like you should always lease cars.
[02:05:08.400 --> 02:05:09.400]   As long as...
[02:05:09.400 --> 02:05:11.600]   Well, and we lease it through Twit.
[02:05:11.600 --> 02:05:13.600]   So as long as it's...
[02:05:13.600 --> 02:05:20.040]   As I have a company, I will do my executive vehicle through the company.
[02:05:20.040 --> 02:05:23.160]   So I'm interested in the Audi Electric.
[02:05:23.160 --> 02:05:25.640]   I drove the Jaguar I-PACE.
[02:05:25.640 --> 02:05:30.040]   I wasn't that crazy about it.
[02:05:30.040 --> 02:05:31.280]   I don't know.
[02:05:31.280 --> 02:05:33.200]   The Model 3 is pretty cool.
[02:05:33.200 --> 02:05:38.840]   He also says, "Next generation Model 3 won't have a steering wheel."
[02:05:38.840 --> 02:05:41.280]   I do not want a car that does not have a steering wheel.
[02:05:41.280 --> 02:05:42.280]   Not yet.
[02:05:42.280 --> 02:05:43.280]   Oh, Jesus.
[02:05:43.280 --> 02:05:44.280]   No, not yet.
[02:05:44.280 --> 02:05:45.280]   I'm not ready for that.
[02:05:45.280 --> 02:05:52.440]   Continuing with the Google Change Long Wing, which is Google's drone delivery service, is
[02:05:52.440 --> 02:05:58.360]   the first to get FAA approval.
[02:05:58.360 --> 02:06:00.280]   Pretty exciting.
[02:06:00.280 --> 02:06:05.080]   Wing now has the same certifications that smaller airlines get from the FAA.
[02:06:05.080 --> 02:06:10.600]   It plans to begin routine deliveries of small consumer items in two rural communities in
[02:06:10.600 --> 02:06:16.240]   Virginia, where there are apparently no sidewalks within months.
[02:06:16.240 --> 02:06:19.640]   So we are the only journalism school in New York that teaches drone reporting.
[02:06:19.640 --> 02:06:20.720]   Oh, wow.
[02:06:20.720 --> 02:06:21.960]   My colleague Travis Fox.
[02:06:21.960 --> 02:06:25.320]   I saw him going out the other day with this huge case he has and the students going to
[02:06:25.320 --> 02:06:26.820]   work.
[02:06:26.820 --> 02:06:28.080]   And I didn't realize this.
[02:06:28.080 --> 02:06:32.200]   He has on a G-Bjole, the yellow jacket.
[02:06:32.200 --> 02:06:33.960]   It's required by law.
[02:06:33.960 --> 02:06:36.760]   The back of us to say FAA licensed drone operator.
[02:06:36.760 --> 02:06:39.320]   So you can say, "Who is that Boseo doing that?"
[02:06:39.320 --> 02:06:40.320]   Wow.
[02:06:40.320 --> 02:06:41.320]   You can find it.
[02:06:41.320 --> 02:06:42.320]   I didn't know that.
[02:06:42.320 --> 02:06:43.320]   Yeah.
[02:06:43.320 --> 02:06:46.400]   I do know that you're not supposed to fly over people.
[02:06:46.400 --> 02:06:53.000]   And that's why Wing has to operate in a rural area of Virginia because you can't operate
[02:06:53.000 --> 02:06:55.200]   over crowds, which makes sense.
[02:06:55.200 --> 02:07:00.120]   Well, it might make more sense anyway because your density, your population density is
[02:07:00.120 --> 02:07:01.320]   harder to deliver.
[02:07:01.320 --> 02:07:05.720]   FAA required Wing to create extensive manuals, training routines, and a safety hierarchy,
[02:07:05.720 --> 02:07:08.680]   just like any air carrier.
[02:07:08.680 --> 02:07:13.720]   So it's interesting, but I don't think it's going to bring drone deliveries to a home near
[02:07:13.720 --> 02:07:15.720]   you yet.
[02:07:15.720 --> 02:07:16.720]   Damn.
[02:07:16.720 --> 02:07:21.720]   Do you wish to step into Childish Gambino's world?
[02:07:21.720 --> 02:07:23.320]   No.
[02:07:23.320 --> 02:07:31.240]   There is a Childish Gambino avatar in playground, but this is even more.
[02:07:31.240 --> 02:07:37.240]   You can go through an AR portal to explore an augmented cave where you can find and interact
[02:07:37.240 --> 02:07:42.280]   with hidden glyphs while still being able to see out into the real world, which is helpful
[02:07:42.280 --> 02:07:44.040]   and not getting run over.
[02:07:44.040 --> 02:07:52.360]   It's a AR app called Feros AR and I will get it and I will report back.
[02:07:52.360 --> 02:07:55.600]   A P-H-A-R-O-S-A-R.
[02:07:55.600 --> 02:07:59.560]   I'm much more excited about playing the Harry Potter Wizards Unite game in augmented reality
[02:07:59.560 --> 02:08:04.240]   where you can actually catch a wizard in the studio.
[02:08:04.240 --> 02:08:06.640]   There's still nothing happening in it though.
[02:08:06.640 --> 02:08:10.640]   Carson down side loaded it because it's available in New Zealand.
[02:08:10.640 --> 02:08:12.040]   Next will be Australia.
[02:08:12.040 --> 02:08:13.840]   That means you're getting close.
[02:08:13.840 --> 02:08:20.520]   I would think Google is testing a distraction free reader mode in Chrome Canary.
[02:08:20.520 --> 02:08:24.320]   That means it may be coming to Chrome soon.
[02:08:24.320 --> 02:08:29.160]   Nobody likes to be bombarded with ads says Google and general clutter when reading articles
[02:08:29.160 --> 02:08:30.160]   in the web.
[02:08:30.160 --> 02:08:34.560]   So reader mode for years we thought that Google an advertising company would never sign off
[02:08:34.560 --> 02:08:37.280]   on such a feature.
[02:08:37.280 --> 02:08:38.440]   Who's writing this?
[02:08:38.440 --> 02:08:39.440]   Thankfully.
[02:08:39.440 --> 02:08:41.080]   It looks like we underestimated.
[02:08:41.080 --> 02:08:47.800]   Now I see this is the reader mode version of Scott Scriven's article from Android Police.
[02:08:47.800 --> 02:08:49.640]   Nice.
[02:08:49.640 --> 02:08:55.080]   You select Distill Page from the menu.
[02:08:55.080 --> 02:09:00.160]   You can't apparently turn it on on other versions of the browser.
[02:09:00.160 --> 02:09:04.240]   So you have to hashtag enable-reader-mode.
[02:09:04.240 --> 02:09:05.760]   That's the flag.
[02:09:05.760 --> 02:09:09.720]   So go into Chrome Flags and look for reader mode.
[02:09:09.720 --> 02:09:10.720]   Enable it.
[02:09:10.720 --> 02:09:11.720]   Sorry has this.
[02:09:11.720 --> 02:09:12.720]   It's a really nice feature.
[02:09:12.720 --> 02:09:13.720]   I like it.
[02:09:13.720 --> 02:09:15.240]   Apple does it on its browsers.
[02:09:15.240 --> 02:09:19.080]   It's surprising though to think Chrome might have it.
[02:09:19.080 --> 02:09:26.960]   And finally, it's easier to find work from home jobs in Google search.
[02:09:26.960 --> 02:09:30.280]   I didn't know there were job search features in Google search.
[02:09:30.280 --> 02:09:32.160]   Yeah, you can find jobs there.
[02:09:32.160 --> 02:09:37.240]   And what's interesting is that this uses schema.org is something in my world of news
[02:09:37.240 --> 02:09:41.880]   that people are using a lot to try to put metadata on and Google's using schema.org
[02:09:41.880 --> 02:09:43.040]   stuff a lot.
[02:09:43.040 --> 02:09:49.480]   So job sites that in their schema.org structure label work from home jobs now in search.
[02:09:49.480 --> 02:09:51.680]   I don't really need.
[02:09:51.680 --> 02:09:52.680]   Go annotation.
[02:09:52.680 --> 02:09:54.480]   Blah, blah, blah, blah.
[02:09:54.480 --> 02:09:55.480]   Yeah.
[02:09:55.480 --> 02:09:59.400]   So yeah, this is non-visible annotations to web pages.
[02:09:59.400 --> 02:10:00.400]   Right.
[02:10:00.400 --> 02:10:01.760]   Hidden in the schema.
[02:10:01.760 --> 02:10:06.000]   And if you do the Google search, say you search for customer service jobs, click location.
[02:10:06.000 --> 02:10:09.960]   There will be a bunch of tags you can choose including work from home.
[02:10:09.960 --> 02:10:11.000]   I like it.
[02:10:11.000 --> 02:10:16.480]   And that's the Google change.
[02:10:16.480 --> 02:10:21.000]   And now I do the thing that I do every week, which is to ask you, Jeff Jarvis, Professor
[02:10:21.000 --> 02:10:27.280]   of Journalism at CUNY and Stacey Higginbotham, stay at home mom.
[02:10:27.280 --> 02:10:32.120]   No, IOT expert and just what?
[02:10:32.120 --> 02:10:33.280]   What?
[02:10:33.280 --> 02:10:36.880]   I mean, I do stay at home for part of the time.
[02:10:36.880 --> 02:10:38.560]   All the time.
[02:10:38.560 --> 02:10:39.560]   To work.
[02:10:39.560 --> 02:10:41.560]   I am a mom.
[02:10:41.560 --> 02:10:43.000]   I was just teasing you.
[02:10:43.000 --> 02:10:46.320]   But anyway, this is the time when I ask you if I've missed any stories that you would
[02:10:46.320 --> 02:10:47.320]   like to talk about.
[02:10:47.320 --> 02:10:48.320]   Oh, that.
[02:10:48.320 --> 02:10:49.760]   Oh, we have missed a ton of stories.
[02:10:49.760 --> 02:10:50.960]   However, there's a lot.
[02:10:50.960 --> 02:10:51.960]   I know.
[02:10:51.960 --> 02:10:53.920]   There's a lot, but I don't want to do any of them.
[02:10:53.920 --> 02:10:54.920]   TikTok.
[02:10:54.920 --> 02:10:57.280]   Band in India now back in India.
[02:10:57.280 --> 02:10:59.080]   I love TikTok.
[02:10:59.080 --> 02:11:00.080]   TikTok's cool.
[02:11:00.080 --> 02:11:02.200]   I bet your daughter uses it, right?
[02:11:02.200 --> 02:11:05.000]   She doesn't, but it's because she doesn't have a lot of friends on it.
[02:11:05.000 --> 02:11:08.680]   And she's actually very concerned about appearing on cameras.
[02:11:08.680 --> 02:11:10.600]   Oh, good for her.
[02:11:10.600 --> 02:11:13.760]   But we watch it a lot.
[02:11:13.760 --> 02:11:17.360]   The issue in India was pornography.
[02:11:17.360 --> 02:11:20.200]   Do you see a lot of porno on TikTok?
[02:11:20.200 --> 02:11:21.600]   My daughter and I do not.
[02:11:21.600 --> 02:11:22.760]   But we're mostly.
[02:11:22.760 --> 02:11:24.240]   You follow.
[02:11:24.240 --> 02:11:27.680]   It's another case of it's who you follow, right?
[02:11:27.680 --> 02:11:28.680]   I guess.
[02:11:28.680 --> 02:11:29.680]   I'm like.
[02:11:29.680 --> 02:11:31.360]   I didn't even know.
[02:11:31.360 --> 02:11:32.360]   She drives that.
[02:11:32.360 --> 02:11:34.120]   Well, of course, there's porn everywhere.
[02:11:34.120 --> 02:11:35.400]   You can't get away from it.
[02:11:35.400 --> 02:11:37.200]   That's right.
[02:11:37.200 --> 02:11:38.800]   Men men men.
[02:11:38.800 --> 02:11:43.200]   Anything I missed that you wanted to talk about, Jeffrey.
[02:11:43.200 --> 02:11:46.480]   No, I'm not funny.
[02:11:46.480 --> 02:11:51.480]   Okay, so let's get from Stacy.
[02:11:51.480 --> 02:11:52.480]   What?
[02:11:52.480 --> 02:11:54.400]   Wait, how about you, Carsten?
[02:11:54.400 --> 02:11:56.080]   Very last story.
[02:11:56.080 --> 02:11:57.680]   They've oh, yes.
[02:11:57.680 --> 02:11:59.840]   Actually Stacy was all excited about this.
[02:11:59.840 --> 02:12:00.760]   Oh, yeah.
[02:12:00.760 --> 02:12:04.560]   They asked artificial intelligence to create a game.
[02:12:04.560 --> 02:12:08.800]   Listen to the games it created first and then the game it finally did first.
[02:12:08.800 --> 02:12:16.600]   It created a game in which an exploding Frisbee relay race racers run on a track while discs
[02:12:16.600 --> 02:12:19.080]   that explode on impact are thrown at them.
[02:12:19.080 --> 02:12:22.840]   I actually think that sounds like a great game.
[02:12:22.840 --> 02:12:24.440]   But maybe not for the runners.
[02:12:24.440 --> 02:12:25.440]   How about this?
[02:12:25.440 --> 02:12:30.680]   A hot air balloon based sport where players balance on a line tethered between air,
[02:12:30.680 --> 02:12:33.720]   craft, and pass balls back and forth.
[02:12:33.720 --> 02:12:36.200]   Again, loads of fun to watch.
[02:12:36.200 --> 02:12:38.320]   Maybe not so much fun to play.
[02:12:38.320 --> 02:12:43.080]   How about pommel horse sawing two people sit on pommel horses on opposite sides of
[02:12:43.080 --> 02:12:47.880]   a giant log and rock back and forth with a saw.
[02:12:47.880 --> 02:12:49.880]   I've seen that at the lumberjack game.
[02:12:49.880 --> 02:12:50.880]   That is a game.
[02:12:50.880 --> 02:12:52.520]   I read this also goes to there.
[02:12:52.520 --> 02:12:53.880]   There is what is it in Sweden?
[02:12:53.880 --> 02:12:57.920]   I saw this girls on hobby horses.
[02:12:57.920 --> 02:13:01.160]   Oh, the New York Times did the hobby horse thing.
[02:13:01.160 --> 02:13:02.160]   But that is not a game.
[02:13:02.160 --> 02:13:03.160]   That is just something I know.
[02:13:03.160 --> 02:13:05.360]   But I have just seen a convergence here.
[02:13:05.360 --> 02:13:09.960]   It is all depends on what annual hobby horse championship.
[02:13:09.960 --> 02:13:13.800]   That probably the AI was fed out of saw.
[02:13:13.800 --> 02:13:14.800]   Yeah, out of saw.
[02:13:14.800 --> 02:13:15.800]   Making fun.
[02:13:15.800 --> 02:13:20.160]   Apparently, the AI is at least in think its peril is fun.
[02:13:20.160 --> 02:13:21.760]   Maybe they are getting that from what they ingest.
[02:13:21.760 --> 02:13:22.760]   I don't know.
[02:13:22.760 --> 02:13:25.040]   An underwater relay race.
[02:13:25.040 --> 02:13:26.560]   Look at football.
[02:13:26.560 --> 02:13:27.560]   That damages people.
[02:13:27.560 --> 02:13:31.800]   Baseball has really fast flying objects that if they hit you, it hurts.
[02:13:31.800 --> 02:13:34.920]   So the one they implemented actually does sound cool.
[02:13:34.920 --> 02:13:42.440]   A form of rugby with obstacles that require gymnastic style maneuvers to avoid.
[02:13:42.440 --> 02:13:45.840]   Here it is.
[02:13:45.840 --> 02:13:50.680]   It is kind of like parkour meets rugby.
[02:13:50.680 --> 02:13:52.240]   You said that you thought this looked like women.
[02:13:52.240 --> 02:13:53.240]   There is a drone.
[02:13:53.240 --> 02:13:54.240]   Oh, no.
[02:13:54.240 --> 02:13:55.240]   That is just.
[02:13:55.240 --> 02:13:56.240]   You require the gymnastic.
[02:13:56.240 --> 02:13:57.240]   Oh, this is a different one.
[02:13:57.240 --> 02:13:58.240]   This is speedgate.
[02:13:58.240 --> 02:13:59.240]   Yeah.
[02:13:59.240 --> 02:14:02.920]   Speedgate is the one they came up with that was actually viable and people could actually
[02:14:02.920 --> 02:14:03.920]   play.
[02:14:03.920 --> 02:14:05.840]   Looks like Quidditch, to be honest.
[02:14:05.840 --> 02:14:08.240]   Yeah, it is not super Quidditchy.
[02:14:08.240 --> 02:14:11.680]   You have to get your stuff in the center gate before you can unlock points at the other
[02:14:11.680 --> 02:14:12.680]   gate.
[02:14:12.680 --> 02:14:16.280]   So your first move is that and then there is rules about.
[02:14:16.280 --> 02:14:17.280]   Oh, that is interesting.
[02:14:17.280 --> 02:14:18.760]   Yeah, it is kind of a neat variation.
[02:14:18.760 --> 02:14:21.080]   You have to guard the gate and guard the goal.
[02:14:21.080 --> 02:14:22.080]   Yeah.
[02:14:22.080 --> 02:14:23.080]   Right.
[02:14:23.080 --> 02:14:25.080]   And then you can't go into the gate.
[02:14:25.080 --> 02:14:26.080]   I don't know.
[02:14:26.080 --> 02:14:27.080]   There is a ball.
[02:14:27.080 --> 02:14:28.080]   There is running.
[02:14:28.080 --> 02:14:29.080]   Oh, that is sports.
[02:14:29.080 --> 02:14:30.080]   I would be very bad at it.
[02:14:30.080 --> 02:14:31.480]   Whatever it is, I would be bad at it.
[02:14:31.480 --> 02:14:36.560]   So their goal is to get to the point where you will hear somebody say, "Oh, yeah, speedgate.
[02:14:36.560 --> 02:14:37.560]   My kids play that."
[02:14:37.560 --> 02:14:38.560]   Yeah.
[02:14:38.560 --> 02:14:39.560]   It could happen.
[02:14:39.560 --> 02:14:40.840]   I mean, it looks like a fun game.
[02:14:40.840 --> 02:14:45.480]   Oregon Sports Authority has recognized speedgate as the officials as an official sport in the
[02:14:45.480 --> 02:14:46.480]   state of Oregon.
[02:14:46.480 --> 02:14:47.480]   There you go.
[02:14:47.480 --> 02:14:50.640]   I mean, if we can play Ultimate Frisbee, we can certainly play speedgate.
[02:14:50.640 --> 02:14:53.200]   I am excited.
[02:14:53.200 --> 02:15:00.720]   So this was an effort of whom I am trying to...
[02:15:00.720 --> 02:15:02.280]   I think there is an NVIDIA.
[02:15:02.280 --> 02:15:03.280]   Okay.
[02:15:03.280 --> 02:15:06.320]   Or is it NVIDIA marketing it?
[02:15:06.320 --> 02:15:08.160]   I couldn't tell.
[02:15:08.160 --> 02:15:09.160]   Yeah.
[02:15:09.160 --> 02:15:12.840]   It is a design firm, AKQA.
[02:15:12.840 --> 02:15:18.160]   I bet they were engaged by NVIDIA or something like...
[02:15:18.160 --> 02:15:19.160]   Something along those lines.
[02:15:19.160 --> 02:15:23.160]   NVIDIA does a lot of artificial intelligence and machine learning stuff they make.
[02:15:23.160 --> 02:15:25.480]   They make the chips that do it.
[02:15:25.480 --> 02:15:26.480]   So that's cool.
[02:15:26.480 --> 02:15:29.800]   Stacy, do you have a thing this week?
[02:15:29.800 --> 02:15:34.080]   So I have two because I forgot that you guys probably would like this one more.
[02:15:34.080 --> 02:15:35.080]   So I'll tell you about this one.
[02:15:35.080 --> 02:15:36.520]   This is IoT Inspector, which...
[02:15:36.520 --> 02:15:38.320]   Did you guys talk about this while I was going?
[02:15:38.320 --> 02:15:41.360]   I made it my pick of the week on Mac Break Weekly last week.
[02:15:41.360 --> 02:15:42.360]   Okay.
[02:15:42.360 --> 02:15:44.840]   I'm really glad that you talked about this.
[02:15:44.840 --> 02:15:50.120]   It's not available for anything but Macs yet, but they're going to do something eventually.
[02:15:50.120 --> 02:15:51.120]   Yes.
[02:15:51.120 --> 02:15:54.160]   So I actually had one of the co-creators of it on the show.
[02:15:54.160 --> 02:15:55.160]   Oh, neat.
[02:15:55.160 --> 02:15:57.400]   Some things to know about it.
[02:15:57.400 --> 02:15:58.400]   What it does is...
[02:15:58.400 --> 02:15:59.400]   Sorry to pick.
[02:15:59.400 --> 02:16:00.400]   I've got another pick.
[02:16:00.400 --> 02:16:01.400]   I've had a backup pick.
[02:16:01.400 --> 02:16:02.400]   No, no, no.
[02:16:02.400 --> 02:16:03.400]   I want to know because you know more about it than I did.
[02:16:03.400 --> 02:16:04.560]   I installed it on my Mac.
[02:16:04.560 --> 02:16:05.560]   Oh, okay.
[02:16:05.560 --> 02:16:06.560]   Good.
[02:16:06.560 --> 02:16:07.760]   So yeah, this is a device.
[02:16:07.760 --> 02:16:08.800]   So you install it.
[02:16:08.800 --> 02:16:11.240]   If you have a Mac book, you can install it.
[02:16:11.240 --> 02:16:13.280]   And it runs in the background.
[02:16:13.280 --> 02:16:14.280]   What is it?
[02:16:14.280 --> 02:16:16.640]   It's doing traffic spoofing.
[02:16:16.640 --> 02:16:20.720]   So it sends everything that all the packets that your devices are sending.
[02:16:20.720 --> 02:16:25.520]   It's actually running those through your computer so IoT can inspect it and then sending
[02:16:25.520 --> 02:16:26.520]   it back out.
[02:16:26.520 --> 02:16:29.000]   So the first thing you learn is all the devices you have, phoning home.
[02:16:29.000 --> 02:16:30.560]   You know all the devices on your network.
[02:16:30.560 --> 02:16:34.960]   The second thing you learn is, are they encrypting this information?
[02:16:34.960 --> 02:16:38.800]   And many of them are not, by the way, in my house anyway.
[02:16:38.800 --> 02:16:41.120]   Yeah, there's three options.
[02:16:41.120 --> 02:16:46.760]   Weak encryption, no encryption, and insecure.
[02:16:46.760 --> 02:16:48.200]   So we talk about the differences there.
[02:16:48.200 --> 02:16:52.360]   It's not designed to run all the time and you'll also see where your traffic is going,
[02:16:52.360 --> 02:16:55.240]   just like with Wireshark or something like that.
[02:16:55.240 --> 02:16:56.920]   So it's pretty handy.
[02:16:56.920 --> 02:17:02.720]   Like I noticed that like my nest, for example, has, oh, I can't remember if it was weaker
[02:17:02.720 --> 02:17:03.720]   and secure.
[02:17:03.720 --> 02:17:04.720]   Yes, mine does too.
[02:17:04.720 --> 02:17:05.720]   Yeah, I was surprised.
[02:17:05.720 --> 02:17:06.720]   That's a camera.
[02:17:06.720 --> 02:17:08.640]   I don't want that to have weak encryption.
[02:17:08.640 --> 02:17:12.400]   Well, and he explains what the weak encryption is.
[02:17:12.400 --> 02:17:16.840]   So something with weak encryption is running an earlier version of TLS.
[02:17:16.840 --> 02:17:19.440]   The insecure encryption is a little bit different.
[02:17:19.440 --> 02:17:21.480]   Oh, wait.
[02:17:21.480 --> 02:17:28.240]   Yes, insecure is more like unknown because what it's using is using weak cipher suites
[02:17:28.240 --> 02:17:34.440]   for, and it's basically allowing it to connect with servers that use weak cipher suites.
[02:17:34.440 --> 02:17:37.160]   It doesn't necessarily mean that it is.
[02:17:37.160 --> 02:17:40.120]   It's just, it's just, I'm not surprised.
[02:17:40.120 --> 02:17:42.360]   That's a big issue on the net in general.
[02:17:42.360 --> 02:17:43.360]   Right.
[02:17:43.360 --> 02:17:45.240]   So that's what that is.
[02:17:45.240 --> 02:17:48.320]   And yeah, download it, see what you have.
[02:17:48.320 --> 02:17:50.440]   They're using this for research purposes.
[02:17:50.440 --> 02:17:52.080]   So you have to choose all of your devices.
[02:17:52.080 --> 02:17:53.280]   You want to monitor, monitor.
[02:17:53.280 --> 02:17:54.280]   You have to opt in.
[02:17:54.280 --> 02:17:57.960]   So think about that as a very progressive policy about data sharing.
[02:17:57.960 --> 02:17:59.880]   You are sharing your data with them.
[02:17:59.880 --> 02:18:03.480]   So if you have sensitive data, you may not want to share it with them.
[02:18:03.480 --> 02:18:05.240]   And you can delete it actually.
[02:18:05.240 --> 02:18:08.160]   So you can run all this, even your sensitive data and delete it.
[02:18:08.160 --> 02:18:10.920]   And they'll just be like, smell you later.
[02:18:10.920 --> 02:18:15.680]   It's a Mac only for now, but they are working on Windows and Linux versions.
[02:18:15.680 --> 02:18:17.320]   I think this is really cool.
[02:18:17.320 --> 02:18:21.400]   And you have to sign a fairly extensive consent form because the information is being sent
[02:18:21.400 --> 02:18:22.400]   to Princeton.
[02:18:22.400 --> 02:18:25.120]   But I think they're, I'm glad to know you talked to them.
[02:18:25.120 --> 02:18:26.120]   So they're trustworthy.
[02:18:26.120 --> 02:18:27.120]   I'm sure.
[02:18:27.120 --> 02:18:29.280]   Yeah, it's like an IRB kind of thing for your data.
[02:18:29.280 --> 02:18:30.280]   It was kind of fun.
[02:18:30.280 --> 02:18:31.280]   Yeah.
[02:18:31.280 --> 02:18:33.800]   Like, you know, just for this research data.
[02:18:33.800 --> 02:18:35.560]   So that's IoT inspector.
[02:18:35.560 --> 02:18:37.080]   There's a link.
[02:18:37.080 --> 02:18:38.640]   It's a little wonky to set up.
[02:18:38.640 --> 02:18:42.720]   You don't want to run it all the time because it will wreak havoc on your network.
[02:18:42.720 --> 02:18:45.120]   So you know, unless you're just up for that.
[02:18:45.120 --> 02:18:49.400]   So here's the screen from there, a video, just so you can see what kind of thing you're
[02:18:49.400 --> 02:18:50.840]   going to get.
[02:18:50.840 --> 02:18:53.480]   You get all the devices.
[02:18:53.480 --> 02:18:56.120]   You get a lot about how the devices are working.
[02:18:56.120 --> 02:18:57.840]   And so it's a pretty cool.
[02:18:57.840 --> 02:18:58.840]   I was really impressed.
[02:18:58.840 --> 02:19:00.040]   Yeah, and they want you to label your stuff.
[02:19:00.040 --> 02:19:04.120]   So like, I noticed some of the stuff was mislabeled, but you'll help them be able to identify different
[02:19:04.120 --> 02:19:06.800]   types of devices, different traffic patterns.
[02:19:06.800 --> 02:19:07.800]   Yeah.
[02:19:07.800 --> 02:19:11.400]   And then you can actually look at the network activity coming out of the device, which is
[02:19:11.400 --> 02:19:12.400]   pretty interesting.
[02:19:12.400 --> 02:19:16.320]   What's your other pick?
[02:19:16.320 --> 02:19:22.440]   OK, this was before I remembered that I had an actual tech pick because, but this is a
[02:19:22.440 --> 02:19:23.440]   really good book.
[02:19:23.440 --> 02:19:29.000]   If you have kids in your life who like, who are geeks who like Comic-Con's fan fiction,
[02:19:29.000 --> 02:19:36.320]   that sort of thing, this is a recently issued book by a 12 year old daughter loves it.
[02:19:36.320 --> 02:19:40.640]   And I don't really know what else to say about it other than it seems like a fun book.
[02:19:40.640 --> 02:19:43.280]   She really dreaded it appeared in her Easter basket.
[02:19:43.280 --> 02:19:48.400]   It's called the Princess and the Fan Girl, a geek-a-rella fairy tale.
[02:19:48.400 --> 02:19:49.400]   Once upon a time.
[02:19:49.400 --> 02:19:51.480]   I can't believe I forgot to say the name of the book.
[02:19:51.480 --> 02:19:52.480]   That's my job.
[02:19:52.480 --> 02:19:53.480]   Don't worry.
[02:19:53.480 --> 02:19:55.320]   Ashley Posten is the author.
[02:19:55.320 --> 02:19:56.320]   Yes.
[02:19:56.320 --> 02:20:01.880]   So it's really good if you have a teenage girl, maybe a boy, I don't know.
[02:20:01.880 --> 02:20:08.880]   But I'm always on the lookout for things that are fun and not good for that age group that
[02:20:08.880 --> 02:20:09.880]   have decent message.
[02:20:09.880 --> 02:20:11.880]   Is this cool to geek-a-rella?
[02:20:11.880 --> 02:20:14.760]   You know, I don't know.
[02:20:14.760 --> 02:20:17.080]   I guess so, maybe.
[02:20:17.080 --> 02:20:20.560]   The Magic Pumpkin is actually a vegan treats food truck.
[02:20:20.560 --> 02:20:22.800]   So oh, yeah, it's a book too.
[02:20:22.800 --> 02:20:23.800]   Who knew?
[02:20:23.800 --> 02:20:24.800]   Book two.
[02:20:24.800 --> 02:20:25.800]   You got more to read.
[02:20:25.800 --> 02:20:30.360]   Now I can get her something else.
[02:20:30.360 --> 02:20:33.040]   Jeff Jarvis, a number, my friend.
[02:20:33.040 --> 02:20:34.600]   Let's go with this one.
[02:20:34.600 --> 02:20:43.280]   Google has saved over 6 million pounds of food waste in its cafes in the last five years.
[02:20:43.280 --> 02:20:48.760]   And naturally a data-driven company they try to find all ways to brag just like this.
[02:20:48.760 --> 02:20:49.760]   They're being smart.
[02:20:49.760 --> 02:20:54.160]   So among the things they do, of course, is repurpose things, which we see this happen.
[02:20:54.160 --> 02:20:55.560]   We used to go to resort in the Poconos.
[02:20:55.560 --> 02:20:57.480]   We used to see tonight's dinner.
[02:20:57.480 --> 02:21:00.320]   We'd guess what it's going to be tomorrow at lunch.
[02:21:00.320 --> 02:21:03.400]   So leftover risotto becomes RNGV.
[02:21:03.400 --> 02:21:05.400]   Is that what you said?
[02:21:05.400 --> 02:21:06.400]   Ah-ha.
[02:21:06.400 --> 02:21:07.400]   RNGV.
[02:21:07.400 --> 02:21:08.400]   Oh, that's the fried risotto.
[02:21:08.400 --> 02:21:09.400]   Yeah, they're so good.
[02:21:09.400 --> 02:21:10.400]   Oh, I love them.
[02:21:10.400 --> 02:21:14.200]   You know, bananas go into banana bread, obviously.
[02:21:14.200 --> 02:21:18.640]   They also try to reduce waste earlier up the stream.
[02:21:18.640 --> 02:21:20.840]   So I didn't know this.
[02:21:20.840 --> 02:21:22.600]   Coffee cherries.
[02:21:22.600 --> 02:21:27.160]   You fruit around the coffee bean is normally wasted, but they're making a flour out of
[02:21:27.160 --> 02:21:31.480]   that and experimenting with it in brownies, tortillas, and other foods.
[02:21:31.480 --> 02:21:32.480]   Wow.
[02:21:32.480 --> 02:21:36.520]   They've void prepared too much food ahead of time.
[02:21:36.520 --> 02:21:40.880]   Near the end of the lunch, they might change the shallower pans at the salad bar to give
[02:21:40.880 --> 02:21:42.120]   an impression of abundance.
[02:21:42.120 --> 02:21:43.920]   It's all about fooling people.
[02:21:43.920 --> 02:21:47.720]   Obviously, we know that they went to smaller plates to get people to eat less some time
[02:21:47.720 --> 02:21:49.640]   ago than full meat.
[02:21:49.640 --> 02:21:51.120]   Mm-hmm.
[02:21:51.120 --> 02:21:54.200]   Take two of them, but smaller portions.
[02:21:54.200 --> 02:22:02.120]   And so 189 of Google Google's cafes and 26 countries, they use lean path systems to figure
[02:22:02.120 --> 02:22:03.120]   this out.
[02:22:03.120 --> 02:22:04.120]   Nice.
[02:22:04.120 --> 02:22:07.560]   There's one other number, which I just abused me, but the Boston Herald won't let me get
[02:22:07.560 --> 02:22:10.560]   to the story now because they were looked at it once.
[02:22:10.560 --> 02:22:17.080]   But a guy in Boston is wiggled by cubed guy in Boston, tried to get some money from the
[02:22:17.080 --> 02:22:22.360]   wiggle boss twins that they got from Zuckerberg and the court just said no.
[02:22:22.360 --> 02:22:23.360]   Oh my.
[02:22:23.360 --> 02:22:27.240]   Oh my.
[02:22:27.240 --> 02:22:30.440]   If you could have sued Zuckerberg, you would have sued Zuckerberg.
[02:22:30.440 --> 02:22:31.440]   [laughter]
[02:22:31.440 --> 02:22:35.280]   We'll joke there in the reference to the movie.
[02:22:35.280 --> 02:22:37.720]   You know, it's funny because I click one of your links and then I went to Boing Boing
[02:22:37.720 --> 02:22:43.760]   and then I found this and I have to say, you know my infatuation was Scottish accents.
[02:22:43.760 --> 02:22:44.760]   Yeah.
[02:22:44.760 --> 02:22:50.800]   For some reason, a couple of guys, Dylan Cassidy and Kelly Kraft have redubbed the Don
[02:22:50.800 --> 02:22:54.160]   Bluth movie The Land Before Time was Scottish Voices.
[02:22:54.160 --> 02:22:56.760]   It's not your mom's fault.
[02:22:56.760 --> 02:23:00.200]   Now, he's being mentioned that old boomer.
[02:23:00.200 --> 02:23:04.160]   And I don't know why, but it just tickles me.
[02:23:04.160 --> 02:23:06.000]   But his fault, the great...
[02:23:06.000 --> 02:23:10.680]   I'm sure it will be taken down pronto.
[02:23:10.680 --> 02:23:13.840]   So go get it while you can.
[02:23:13.840 --> 02:23:16.480]   But love you, Aladdin.
[02:23:16.480 --> 02:23:18.160]   I miss her so much.
[02:23:18.160 --> 02:23:21.080]   And you were always in love with me.
[02:23:21.080 --> 02:23:25.920]   I am hoping that this will spawn a slew of Scottish remakes.
[02:23:25.920 --> 02:23:26.920]   Scottish remakes.
[02:23:26.920 --> 02:23:31.360]   What movies would you like to see done in Scottish?
[02:23:31.360 --> 02:23:32.800]   A part of each other.
[02:23:32.800 --> 02:23:34.160]   I want to see everything done.
[02:23:34.160 --> 02:23:35.880]   Everything should be done in Scottish.
[02:23:35.880 --> 02:23:37.600]   I think The Lion King.
[02:23:37.600 --> 02:23:39.640]   Oh, The Lion King of Big Reet.
[02:23:39.640 --> 02:23:47.800]   Ladies and gentlemen, Basta, you have suffered through two minutes, two hours or 22 minutes
[02:23:47.800 --> 02:23:48.800]   of the most decisive...
[02:23:48.800 --> 02:23:49.800]   They've survived both.
[02:23:49.800 --> 02:23:51.720]   They've both had headaches now.
[02:23:51.720 --> 02:23:54.960]   Crazy podcast ever.
[02:23:54.960 --> 02:23:59.560]   This is why The Washington Post hates podcasts and you survived it.
[02:23:59.560 --> 02:24:00.560]   Congratulations.
[02:24:00.560 --> 02:24:01.560]   Thank you.
[02:24:01.560 --> 02:24:02.560]   Yay!
[02:24:02.560 --> 02:24:05.080]   Yes, Stacy Higginbotham is awesome.
[02:24:05.080 --> 02:24:07.640]   You should listen to the Stacy on IoT podcast.
[02:24:07.640 --> 02:24:09.680]   It's an actual good podcast with content.
[02:24:09.680 --> 02:24:11.040]   She does it with Kevin Toful.
[02:24:11.040 --> 02:24:14.480]   She also does a great newsletter at Stacy on IoT.
[02:24:14.480 --> 02:24:16.360]   Listen, watch, read.
[02:24:16.360 --> 02:24:18.400]   And I hope we see you again next week, Stacy.
[02:24:18.400 --> 02:24:19.400]   Thank you.
[02:24:19.400 --> 02:24:20.400]   Appreciate it.
[02:24:20.400 --> 02:24:22.680]   Glad the family's home.
[02:24:22.680 --> 02:24:26.920]   We also thank Jeff Jarvis, the director of the Townite Center for Entrepreneurial Journalism
[02:24:26.920 --> 02:24:31.320]   at the Craig Newmark Graduate School of Journalism at the City University of New York and former
[02:24:31.320 --> 02:24:34.680]   TV guide critic for joining us today.
[02:24:34.680 --> 02:24:37.920]   Buzzmachine.com is his blog at Jeff Jarvis on the Twitter.
[02:24:37.920 --> 02:24:42.480]   Stacy is @gigastacy on the Twitter.
[02:24:42.480 --> 02:24:44.560]   And I thank you both for being here.
[02:24:44.560 --> 02:24:45.560]   We do this week.
[02:24:45.560 --> 02:24:47.160]   Oh, it's such a great show.
[02:24:47.160 --> 02:24:48.160]   I love this show.
[02:24:48.160 --> 02:24:53.760]   And I admit to completely indulging myself, just having a good time with you guys.
[02:24:53.760 --> 02:24:55.800]   I apologize.
[02:24:55.800 --> 02:25:01.320]   If you want to watch us do this live with all the good parts left in, go to twit.tv/live.
[02:25:01.320 --> 02:25:08.920]   We do it about 130 Pacific on Wednesdays, 430 Eastern, 2030 UTC.
[02:25:08.920 --> 02:25:13.320]   You can also, and I encourage you to download copies.
[02:25:13.320 --> 02:25:18.400]   We have on-demand versions available of all of our shows at twit.tv, in this case twit.tv/twig,
[02:25:18.400 --> 02:25:19.560]   audio or video.
[02:25:19.560 --> 02:25:23.520]   Depending on your consumption preference, you can also subscribe, which is probably the
[02:25:23.520 --> 02:25:27.200]   best thing to do that way you get it automatically.
[02:25:27.200 --> 02:25:31.880]   Every week, the minute it's ready, just in time for your Thursday morning commute.
[02:25:31.880 --> 02:25:32.880]   Thanks everybody.
[02:25:32.880 --> 02:25:34.880]   We'll see you next time on This Week in Google.
[02:25:34.880 --> 02:25:34.880]   Bye bye.
[02:25:34.880 --> 02:25:44.880]   [MUSIC]

