;FFMETADATA1
title=Queso Morphines
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=482
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.000]   [MUSIC PLAYING]
[00:00:03.000 --> 00:00:05.040]   Netcast you love.
[00:00:05.040 --> 00:00:06.440]   From people you trust.
[00:00:06.440 --> 00:00:10.200]   [MUSIC PLAYING]
[00:00:10.200 --> 00:00:11.440]   This is Twig.
[00:00:11.440 --> 00:00:17.320]   This is Twig.
[00:00:17.320 --> 00:00:21.040]   This week in Google, episode 482, recorded Wednesday,
[00:00:21.040 --> 00:00:23.600]   November 14, 2018.
[00:00:23.600 --> 00:00:26.720]   K so, more fiends.
[00:00:26.720 --> 00:00:29.680]   This week in Google is brought to you by Kaptera,
[00:00:29.680 --> 00:00:31.880]   find software solutions for your business needs.
[00:00:31.880 --> 00:00:35.200]   Kaptera is a free website with over 600 categories
[00:00:35.200 --> 00:00:36.400]   of business software.
[00:00:36.400 --> 00:00:39.440]   Visit kaptera.com/twig and get your free copy
[00:00:39.440 --> 00:00:42.160]   of the big book of free software.
[00:00:42.160 --> 00:00:44.640]   And by ExpressVPN.
[00:00:44.640 --> 00:00:46.680]   Protect your online activity today.
[00:00:46.680 --> 00:00:49.480]   For three extra months free with a one year package,
[00:00:49.480 --> 00:00:52.920]   go to expressvpn.com/twig.
[00:00:52.920 --> 00:00:54.520]   And by CashFly.
[00:00:54.520 --> 00:00:57.880]   Make this the last month your CDN bill gives you a headache.
[00:00:57.880 --> 00:01:01.040]   Join the thousands of others who trust CashFly's reliable
[00:01:01.040 --> 00:01:01.800]   network.
[00:01:01.800 --> 00:01:05.560]   And just for Twit listeners, receive up to $2,000 in credits
[00:01:05.560 --> 00:01:08.240]   when you switch to CashFly before January 1.
[00:01:08.240 --> 00:01:14.280]   Learn more and get a complimentary quote at twit.cashfly.com.
[00:01:14.280 --> 00:01:16.080]   It's time for Twig this week in Google,
[00:01:16.080 --> 00:01:18.240]   the show where you cover the latest news.
[00:01:18.240 --> 00:01:20.360]   From Facebook and Google, Stacey Higginbotham
[00:01:20.360 --> 00:01:22.720]   is here from Stacey on IoT.
[00:01:22.720 --> 00:01:24.200]   Hi, Stace.
[00:01:24.200 --> 00:01:24.700]   Hello.
[00:01:24.700 --> 00:01:26.640]   Hello.
[00:01:26.640 --> 00:01:29.760]   Also here, Mr. Jeff Jarvis.
[00:01:29.760 --> 00:01:30.360]   He is.
[00:01:30.360 --> 00:01:34.640]   Well, I have that piece of paper here somewhere.
[00:01:34.640 --> 00:01:35.880]   Grab the book.
[00:01:35.880 --> 00:01:38.360]   Hey, the book is--
[00:01:38.360 --> 00:01:41.080]   The Town Knight Leonard Tao, Professor for Journalistic
[00:01:41.080 --> 00:01:43.560]   Innovation at Craig Newmark Graduate School of Journalism
[00:01:43.560 --> 00:01:45.040]   at the City University of New York
[00:01:45.040 --> 00:01:46.720]   and the Director of the Town Knight Center
[00:01:46.720 --> 00:01:49.280]   for Entrepreneurial Journalism at the Craig Newmark Graduate
[00:01:49.280 --> 00:01:52.280]   School of Journalism at the City University of New York.
[00:01:52.280 --> 00:01:55.080]   The largest-- now it really is a lower third.
[00:01:55.080 --> 00:01:58.880]   I really would like to see Jeff take his little hands
[00:01:58.880 --> 00:02:02.040]   and just kind of peek up over the lower third.
[00:02:02.040 --> 00:02:03.800]   I think it would be adorable.
[00:02:03.800 --> 00:02:04.800]   See if he can make a happy Jeff.
[00:02:04.800 --> 00:02:06.360]   Like, kill right.
[00:02:06.360 --> 00:02:06.960]   Oh, god.
[00:02:06.960 --> 00:02:07.880]   You're breaking something.
[00:02:07.880 --> 00:02:09.600]   Something's wet going-- oh.
[00:02:09.600 --> 00:02:10.720]   Oh, there it comes.
[00:02:10.720 --> 00:02:11.320]   There it comes.
[00:02:11.320 --> 00:02:13.280]   This is all for the visual audience.
[00:02:13.280 --> 00:02:13.880]   You're going to have to--
[00:02:13.880 --> 00:02:15.280]   She's hurting my boobs.
[00:02:15.280 --> 00:02:16.280]   Oh, shit.
[00:02:16.280 --> 00:02:16.780]   Hi.
[00:02:16.780 --> 00:02:18.560]   I'm doing a laugh.
[00:02:18.560 --> 00:02:19.920]   Hi.
[00:02:19.920 --> 00:02:22.400]   You need our standing desk where you can just move it up
[00:02:22.400 --> 00:02:22.880]   and down.
[00:02:22.880 --> 00:02:23.040]   Yeah.
[00:02:23.040 --> 00:02:23.680]   See for me.
[00:02:23.680 --> 00:02:24.680]   All right.
[00:02:24.680 --> 00:02:25.880]   You're already the Leo desk that can just move up.
[00:02:25.880 --> 00:02:25.880]   Yeah.
[00:02:25.880 --> 00:02:29.280]   Ooh.
[00:02:29.280 --> 00:02:30.920]   You're walking with that Leo.
[00:02:30.920 --> 00:02:35.000]   I know you're going to enjoy it during that.
[00:02:35.000 --> 00:02:37.160]   Where do you want to start today?
[00:02:37.160 --> 00:02:38.600]   Oh, I think Stacy has an idea.
[00:02:38.600 --> 00:02:39.440]   She has an idea.
[00:02:39.440 --> 00:02:41.800]   Well, technically this is this week at Google.
[00:02:41.800 --> 00:02:44.040]   So if we want to start with Google, I'm cool with that.
[00:02:44.040 --> 00:02:45.000]   Let's start with Google.
[00:02:45.000 --> 00:02:47.840]   It says it will make arbitration optional.
[00:02:47.840 --> 00:02:52.600]   In other words, not required for individual sexual harassment
[00:02:52.600 --> 00:02:58.440]   claims provide more info about investigations, expand training,
[00:02:58.440 --> 00:02:58.960]   and more.
[00:02:58.960 --> 00:03:01.440]   This is in response, of course, to the walk out last week
[00:03:01.440 --> 00:03:03.640]   of Google employees.
[00:03:03.640 --> 00:03:08.880]   Sundar Pichai sent this as an email to Google employees.
[00:03:08.880 --> 00:03:12.400]   This is kind of like the very least they could do.
[00:03:12.400 --> 00:03:13.400]   I mean--
[00:03:13.400 --> 00:03:14.960]   It would be the most they could do.
[00:03:14.960 --> 00:03:19.480]   Well, I would say the most would be diversifying its board
[00:03:19.480 --> 00:03:20.360]   further.
[00:03:20.360 --> 00:03:24.240]   I would say maybe putting in some--
[00:03:24.240 --> 00:03:25.560]   well, let's see.
[00:03:25.560 --> 00:03:27.400]   I'm like, what else?
[00:03:27.400 --> 00:03:30.520]   Putting in some goals around hiring maybe?
[00:03:30.520 --> 00:03:31.480]   Maybe not goals.
[00:03:31.480 --> 00:03:32.320]   Yeah, the first of the--
[00:03:32.320 --> 00:03:36.480]   Advanced recruitment efforts at different places.
[00:03:36.480 --> 00:03:37.000]   It does.
[00:03:37.000 --> 00:03:37.520]   It doesn't.
[00:03:37.520 --> 00:03:41.200]   It doesn't make sense that sexual harassment
[00:03:41.200 --> 00:03:44.240]   is less of a problem in a workplace that is 50% women,
[00:03:44.240 --> 00:03:45.240]   or is that--
[00:03:45.240 --> 00:03:47.240]   I would bet that's true.
[00:03:47.240 --> 00:03:47.840]   The more women--
[00:03:47.840 --> 00:03:49.720]   But it's also a question of the management Leo.
[00:03:49.720 --> 00:03:50.600]   Well, that's true.
[00:03:50.600 --> 00:03:50.600]   It is.
[00:03:50.600 --> 00:03:52.920]   It's mostly the culture.
[00:03:52.920 --> 00:03:55.000]   Because you can get a lot of harassment in places
[00:03:55.000 --> 00:03:57.440]   with more women.
[00:03:57.440 --> 00:04:00.680]   It's just women have to be in charge,
[00:04:00.680 --> 00:04:01.880]   or in management positions.
[00:04:01.880 --> 00:04:02.840]   Yeah, on the board.
[00:04:02.840 --> 00:04:05.240]   Yeah, that makes sense.
[00:04:05.240 --> 00:04:07.240]   Because, I mean, look at schools.
[00:04:07.240 --> 00:04:08.960]   There's a lot of female teachers,
[00:04:08.960 --> 00:04:12.120]   and there's still lots of sexual harassment happening there.
[00:04:12.120 --> 00:04:14.520]   That is, by the way, the reaction of the organizers
[00:04:14.520 --> 00:04:16.720]   of the walk out.
[00:04:16.720 --> 00:04:18.120]   They're happy.
[00:04:18.120 --> 00:04:19.560]   Yeah, they should be proud.
[00:04:19.560 --> 00:04:21.480]   They accomplished the first step.
[00:04:21.480 --> 00:04:22.160]   Yeah, they--
[00:04:22.160 --> 00:04:22.660]   Yes.
[00:04:22.660 --> 00:04:24.400]   What they showed is collective action works.
[00:04:24.400 --> 00:04:26.720]   And when we work together, we can make a change.
[00:04:26.720 --> 00:04:29.480]   However, here's what they wanted more.
[00:04:29.480 --> 00:04:31.440]   The response ignored several of the core demands,
[00:04:31.440 --> 00:04:34.280]   like elevating the diversity officer and employee
[00:04:34.280 --> 00:04:37.080]   representation on the board.
[00:04:37.080 --> 00:04:41.240]   And troublingly erased those focused on racism,
[00:04:41.240 --> 00:04:46.120]   discrimination, and structural inequality, which is--
[00:04:46.120 --> 00:04:48.160]   they said built into the class system that separates
[00:04:48.160 --> 00:04:50.920]   full-time employees from contract workers.
[00:04:50.920 --> 00:04:53.440]   This is actually going to be a larger problem for not only
[00:04:53.440 --> 00:04:55.560]   Google, but Silicon Valley, and general contract workers,
[00:04:55.560 --> 00:04:58.520]   make up more than half of Google's workforce,
[00:04:58.520 --> 00:05:02.000]   but receive few of the benefits associated with employment.
[00:05:02.000 --> 00:05:02.280]   And--
[00:05:02.280 --> 00:05:03.000]   It is true.
[00:05:03.000 --> 00:05:05.920]   They are largely people of color, immigrants, and people
[00:05:05.920 --> 00:05:07.440]   from working class backgrounds.
[00:05:07.440 --> 00:05:11.360]   Yeah.
[00:05:11.360 --> 00:05:14.280]   I mean, those are overarching labor issues.
[00:05:14.280 --> 00:05:18.480]   I mean, the fact that arbitration grew so much in companies,
[00:05:18.480 --> 00:05:21.120]   and you look at non-compete efforts that
[00:05:21.120 --> 00:05:24.200]   have been dismantled over the last few years in Silicon Valley,
[00:05:24.200 --> 00:05:29.840]   what's happening is we're seeing workers' employees
[00:05:29.840 --> 00:05:32.760]   start to take some of the same sort of things that
[00:05:32.760 --> 00:05:35.480]   happened back in the days when they started unions.
[00:05:35.480 --> 00:05:38.200]   So I don't know if we're ever going to get to actual unions,
[00:05:38.200 --> 00:05:43.480]   but it's good to see this kind of shift back to worker rights.
[00:05:43.480 --> 00:05:45.720]   Well, I mean, you can say there's a balance
[00:05:45.720 --> 00:05:49.320]   of power between employers and employees,
[00:05:49.320 --> 00:05:53.240]   except that there's no power at all among employees.
[00:05:53.240 --> 00:05:56.800]   In many cases, most states, including California,
[00:05:56.800 --> 00:05:58.960]   are at-will states.
[00:05:58.960 --> 00:06:02.280]   California is not a right to work state, but it's close to it.
[00:06:02.280 --> 00:06:05.640]   Although California law has been tilting in the direction
[00:06:05.640 --> 00:06:08.360]   of employees over the last few years,
[00:06:08.360 --> 00:06:10.800]   so at the state, at least, is trying
[00:06:10.800 --> 00:06:16.080]   to make it a little bit more favorable to employees.
[00:06:16.080 --> 00:06:21.920]   I mentioned that we have a forced arbitration clause
[00:06:21.920 --> 00:06:23.560]   in our contracts with Twitter employees,
[00:06:23.560 --> 00:06:26.600]   and the reason we do that, I'm told,
[00:06:26.600 --> 00:06:28.880]   is that because of California state law,
[00:06:28.880 --> 00:06:31.840]   it's very easy for an employee to sue us with no consequences
[00:06:31.840 --> 00:06:37.280]   to themselves, that basically to harass us back,
[00:06:37.280 --> 00:06:39.360]   whether there's merit in the case or not.
[00:06:39.360 --> 00:06:42.560]   And so arbitration kind of flattens that out a little bit.
[00:06:42.560 --> 00:06:44.520]   I would say the cost to bring a lawsuit
[00:06:44.520 --> 00:06:46.480]   and the effort to bring a lawsuit flattens that out.
[00:06:46.480 --> 00:06:50.720]   That's a very common argument for forced arbitration,
[00:06:50.720 --> 00:06:53.760]   but it also ignores that most people
[00:06:53.760 --> 00:06:56.960]   when are not going to file those types of suits.
[00:06:56.960 --> 00:06:59.680]   There's a significant barrier in finding someone
[00:06:59.680 --> 00:07:02.440]   to pursue those types of suits.
[00:07:02.440 --> 00:07:05.840]   Yeah, but many lawyers will do it and do it for--
[00:07:05.840 --> 00:07:10.840]   Contingency, but most people don't actually want to sue.
[00:07:10.840 --> 00:07:14.160]   Well, even the champions you're relying on the kindness
[00:07:14.160 --> 00:07:15.840]   of your employees rather than--
[00:07:15.840 --> 00:07:17.200]   Well, not necessarily.
[00:07:17.200 --> 00:07:20.400]   I mean, usually when someone is in that situation
[00:07:20.400 --> 00:07:23.400]   and they're stuck in arbitration, they're unhappy.
[00:07:23.400 --> 00:07:25.920]   And they feel wronged and aggrieved.
[00:07:25.920 --> 00:07:27.920]   But I could tell you from personal experience,
[00:07:27.920 --> 00:07:31.760]   employees are unhappy if they're terminated for any reason.
[00:07:31.760 --> 00:07:32.960]   That's not true.
[00:07:32.960 --> 00:07:35.080]   But many, many employees--
[00:07:35.080 --> 00:07:36.960]   Sorry, they are unhappy but they're not going to come out.
[00:07:36.960 --> 00:07:38.640]   They're unhappy to be terminated and some of them
[00:07:38.640 --> 00:07:41.640]   decide that they want to get exact revenge, whether there's
[00:07:41.640 --> 00:07:43.600]   merit or not in their case.
[00:07:43.600 --> 00:07:46.040]   Well, why shouldn't that go through the courts?
[00:07:46.040 --> 00:07:47.080]   That's what the courts are for.
[00:07:47.080 --> 00:07:49.360]   Because it's extremely expensive for the business
[00:07:49.360 --> 00:07:52.720]   at no cost for the employee.
[00:07:52.720 --> 00:07:53.800]   There can be a cost for--
[00:07:53.800 --> 00:07:55.320]   Minor cost.
[00:07:55.320 --> 00:07:57.400]   Trust me, I know.
[00:07:57.400 --> 00:07:59.680]   Minor cost to the employee.
[00:07:59.680 --> 00:08:03.600]   So if any, the cost is finding a lawyer who'll do it
[00:08:03.600 --> 00:08:04.880]   on a contingency basis.
[00:08:04.880 --> 00:08:08.200]   And then you just sit back and enjoy while the player twists
[00:08:08.200 --> 00:08:11.120]   in the wind.
[00:08:11.120 --> 00:08:15.760]   Look, believe me, I've considered suing and employers.
[00:08:15.760 --> 00:08:16.960]   And I never have.
[00:08:16.960 --> 00:08:18.520]   And often, you're right.
[00:08:18.520 --> 00:08:20.880]   There's a lot of reasons not to.
[00:08:20.880 --> 00:08:22.040]   It's not merely trouble.
[00:08:22.040 --> 00:08:24.640]   You don't want to get a bad reputation in the business
[00:08:24.640 --> 00:08:26.680]   you work in.
[00:08:26.680 --> 00:08:29.240]   And most people just aren't litigious.
[00:08:29.240 --> 00:08:31.320]   But there are some people who are litigious.
[00:08:31.320 --> 00:08:35.000]   And they can cost a company a lot of money and time.
[00:08:35.000 --> 00:08:36.080]   That is true.
[00:08:36.080 --> 00:08:37.640]   So I've seen a case of--
[00:08:37.640 --> 00:08:40.760]   Unless you think arbitration is unfair.
[00:08:40.760 --> 00:08:45.080]   I think arbitration tends to benefit the employer.
[00:08:45.080 --> 00:08:45.400]   Really?
[00:08:45.400 --> 00:08:50.400]   And it tends to hide issues, like larger cultural issues
[00:08:50.400 --> 00:08:52.960]   that can be a problem for a larger class.
[00:08:52.960 --> 00:08:55.480]   If I can-- I'll grant you.
[00:08:55.480 --> 00:08:58.480]   If you feel like arbitration is an unfair process,
[00:08:58.480 --> 00:09:00.840]   forcing employees to go through arbitration
[00:09:00.840 --> 00:09:02.200]   is inherently unfair.
[00:09:02.200 --> 00:09:03.640]   I don't think it's an unfair process.
[00:09:03.640 --> 00:09:06.960]   I think it's often in front of a judge.
[00:09:06.960 --> 00:09:09.480]   It's an extrajudicial proceeding.
[00:09:09.480 --> 00:09:10.920]   But there are rules.
[00:09:10.920 --> 00:09:13.400]   And it's designed to be fair.
[00:09:13.400 --> 00:09:16.840]   If you say it is unfair, then OK, then you might have a case.
[00:09:16.840 --> 00:09:19.200]   But I'm not sure I agree with you
[00:09:19.200 --> 00:09:22.440]   that arbitration is a terrible thing.
[00:09:22.440 --> 00:09:25.280]   I think arbitration, like any system,
[00:09:25.280 --> 00:09:27.720]   like the legal system we're talking about, can be abused.
[00:09:27.720 --> 00:09:31.560]   And if you are the company who's paying for the mediator,
[00:09:31.560 --> 00:09:36.680]   you are in a position of power and can either use it
[00:09:36.680 --> 00:09:39.760]   to extract settlements for people and not have some
[00:09:39.760 --> 00:09:41.280]   of your problems see the light of day.
[00:09:41.280 --> 00:09:42.920]   So that's the first issue.
[00:09:42.920 --> 00:09:45.760]   You can use it to hide continuing issues in your company.
[00:09:45.760 --> 00:09:50.600]   And then the second issue is that it can be unfair to employees.
[00:09:50.600 --> 00:09:52.280]   It's not always unfair.
[00:09:52.280 --> 00:09:55.480]   But the company does pay for it.
[00:09:55.480 --> 00:09:59.920]   And most people who-- unless it's egregious,
[00:09:59.920 --> 00:10:02.080]   they might not want to bite the hand that feeds them.
[00:10:02.080 --> 00:10:05.920]   Is doing what Google does, which is making it
[00:10:05.920 --> 00:10:08.520]   not for arbitration in the case of sexual harassment,
[00:10:08.520 --> 00:10:09.960]   a good step?
[00:10:09.960 --> 00:10:13.040]   Yes, because sexual harassment is one of the most common ways
[00:10:13.040 --> 00:10:17.800]   that people have used arbitration in the past to hide those problems.
[00:10:17.800 --> 00:10:21.280]   Look at Fox News or Fox.
[00:10:21.280 --> 00:10:22.840]   OK.
[00:10:22.840 --> 00:10:25.480]   So yeah, that's fair.
[00:10:25.480 --> 00:10:27.320]   Sorry, go ahead.
[00:10:27.320 --> 00:10:27.680]   I'm Jeff.
[00:10:27.680 --> 00:10:28.880]   I didn't mean to interrupt you.
[00:10:28.880 --> 00:10:30.480]   No, I wasn't saying anything.
[00:10:30.480 --> 00:10:31.720]   Well, you did say something.
[00:10:31.720 --> 00:10:33.160]   I just was-- I forget what it was.
[00:10:33.160 --> 00:10:33.880]   Yeah, I'm sorry.
[00:10:33.880 --> 00:10:34.720]   That's why I said--
[00:10:34.720 --> 00:10:39.720]   Well, I guess I was-- no, I was to Stacy.
[00:10:39.720 --> 00:10:44.520]   It's expensive for both sides to do suits and risky
[00:10:44.520 --> 00:10:46.360]   for both sides to do suits.
[00:10:46.360 --> 00:10:49.120]   I wonder if having some opening to do them
[00:10:49.120 --> 00:10:51.320]   itself has an impact on settlements.
[00:10:51.320 --> 00:10:54.200]   The fear that you can do the suit, right?
[00:10:54.200 --> 00:10:59.560]   So here's my experience has been almost always the opening
[00:10:59.560 --> 00:11:03.400]   assault is a lawsuit's going to cost you $100,000.
[00:11:03.400 --> 00:11:05.440]   Give us 10.
[00:11:05.440 --> 00:11:05.920]   Right.
[00:11:05.920 --> 00:11:07.960]   And almost always a company faced with that.
[00:11:07.960 --> 00:11:08.440]   We'll do that.
[00:11:08.440 --> 00:11:10.080]   It's the same thing as patent trolls.
[00:11:10.080 --> 00:11:11.120]   Patent, right.
[00:11:11.120 --> 00:11:12.520]   Almost always a company faced with that.
[00:11:12.520 --> 00:11:14.360]   We'll say, OK, here's 10.
[00:11:14.360 --> 00:11:17.120]   And employees know this, and these lawyers know this.
[00:11:17.120 --> 00:11:21.080]   And they push very strongly to get you to settle,
[00:11:21.080 --> 00:11:22.440]   because they don't want to go to court either.
[00:11:22.440 --> 00:11:23.240]   They don't want to waste time.
[00:11:23.240 --> 00:11:24.520]   They just want to write a bunch of letters
[00:11:24.520 --> 00:11:26.280]   and hope that you will say, well, it's cheaper just
[00:11:26.280 --> 00:11:29.640]   to give you 10 bucks to shut up, or whatever it is, right?
[00:11:29.640 --> 00:11:30.480]   And so--
[00:11:30.480 --> 00:11:32.040]   Do you wonder what the middle ground is?
[00:11:32.040 --> 00:11:32.840]   It's really hard to find.
[00:11:32.840 --> 00:11:34.120]   Yeah, what are they doing in Canada?
[00:11:34.120 --> 00:11:34.920]   I'm sure it's better.
[00:11:34.920 --> 00:11:41.160]   They just all apologize all the time.
[00:11:41.160 --> 00:11:41.720]   That's fine.
[00:11:41.720 --> 00:11:42.960]   Say, I don't really say something.
[00:11:42.960 --> 00:11:44.120]   I'm sorry, whatever it is, I'm sorry.
[00:11:44.120 --> 00:11:45.520]   There's some-- got to be some middle ground,
[00:11:45.520 --> 00:11:47.720]   because I understand that there's very frequently this.
[00:11:47.720 --> 00:11:50.200]   Oh, it's trial lawyers, and we should get them,
[00:11:50.200 --> 00:11:52.560]   because they're just making--
[00:11:52.560 --> 00:11:56.720]   but I also think the ability to sue is kind of critical.
[00:11:56.720 --> 00:11:58.640]   suing has taken on such a pejorative meaning,
[00:11:58.640 --> 00:12:00.360]   and merely says, going to the court
[00:12:00.360 --> 00:12:05.480]   to get an opinion, a fair opinion, on the situation,
[00:12:05.480 --> 00:12:08.240]   some sort of judgment and settlement on the situation.
[00:12:08.240 --> 00:12:09.680]   So it's actually--
[00:12:09.680 --> 00:12:12.000]   I don't think suing is a bad thing,
[00:12:12.000 --> 00:12:13.480]   but it has gotten very expensive,
[00:12:13.480 --> 00:12:15.400]   and it can be very expensive to defend this suit.
[00:12:15.400 --> 00:12:17.280]   And there are plenty of frivolous lawsuits
[00:12:17.280 --> 00:12:18.560]   out there floating around.
[00:12:18.560 --> 00:12:23.640]   Maybe you could do something where the person filing the complaint
[00:12:23.640 --> 00:12:27.120]   gets to hire and find the mediator
[00:12:27.120 --> 00:12:29.200]   to listen to the arbitration.
[00:12:29.200 --> 00:12:29.720]   OK.
[00:12:29.720 --> 00:12:32.920]   I think one thing that would be very handy,
[00:12:32.920 --> 00:12:35.200]   but it is not the case in these kinds of employment suits,
[00:12:35.200 --> 00:12:39.840]   is the ability to countersue for damages or legal fees.
[00:12:39.840 --> 00:12:43.400]   So if you win, I think this is what they do in Canada.
[00:12:43.400 --> 00:12:46.480]   If you file suit and lose, you have
[00:12:46.480 --> 00:12:50.840]   to pay the defendant's legal fees.
[00:12:50.840 --> 00:12:52.920]   It's like, well, your suit better have merit,
[00:12:52.920 --> 00:12:55.200]   otherwise it's not going to be without consequence
[00:12:55.200 --> 00:12:57.000]   that you file suit.
[00:12:57.000 --> 00:12:58.840]   That's a good way to do it, right?
[00:12:58.840 --> 00:13:00.800]   Mm-hmm.
[00:13:00.800 --> 00:13:03.280]   Yeah, this is not something we're probably going to solve today,
[00:13:03.280 --> 00:13:05.240]   although it'd be awesome if we did.
[00:13:05.240 --> 00:13:09.240]   But I think you did a good job articulating the other side,
[00:13:09.240 --> 00:13:12.560]   and I think that's a really important thing to think about.
[00:13:12.560 --> 00:13:14.360]   Nobody's seen that, so I don't want to imply
[00:13:14.360 --> 00:13:16.160]   that we're getting lawsuits out the door.
[00:13:16.160 --> 00:13:18.480]   No, but I mean, as a business, it is a legitimate--
[00:13:18.480 --> 00:13:19.560]   Some you have to think about.
[00:13:19.560 --> 00:13:21.080]   I don't have to think about it, fortunately.
[00:13:21.080 --> 00:13:27.280]   I've seen cases where someone with a grudge and with a problem
[00:13:27.280 --> 00:13:30.200]   uses the courts to go after--
[00:13:30.200 --> 00:13:33.400]   to Harris, someone they don't like at length.
[00:13:33.400 --> 00:13:35.800]   We all know litigious people.
[00:13:35.800 --> 00:13:37.280]   Yeah.
[00:13:37.280 --> 00:13:38.160]   I feel like I don't.
[00:13:38.160 --> 00:13:39.080]   I'm OK with that.
[00:13:39.080 --> 00:13:39.960]   Oh, you do.
[00:13:39.960 --> 00:13:40.920]   Oh, say the word.
[00:13:40.920 --> 00:13:41.280]   You do.
[00:13:41.280 --> 00:13:43.560]   His name is Peter Thiel.
[00:13:43.560 --> 00:13:45.760]   Oh, well, OK, yeah.
[00:13:45.760 --> 00:13:51.960]   And you argued vociferously that going after Gawker
[00:13:51.960 --> 00:13:55.000]   was intended to shut them down.
[00:13:55.000 --> 00:13:55.920]   Shut-- yes.
[00:13:55.920 --> 00:13:59.800]   Yeah.
[00:13:59.800 --> 00:14:01.640]   All right.
[00:14:01.640 --> 00:14:02.080]   OK.
[00:14:02.080 --> 00:14:04.520]   There's two sides, and it's a difficult one.
[00:14:04.520 --> 00:14:06.720]   And really, I think we do have a little bit of a problem
[00:14:06.720 --> 00:14:09.480]   in this country with lawsuits.
[00:14:09.480 --> 00:14:11.320]   And yet, at the same time, I think the right to sue
[00:14:11.320 --> 00:14:15.080]   is a very, very important part of our system.
[00:14:15.080 --> 00:14:17.960]   Well, and we don't have a lot of worker protections.
[00:14:17.960 --> 00:14:20.560]   I mean, if we're just thinking about employees, right?
[00:14:20.560 --> 00:14:22.560]   We don't have--
[00:14:22.560 --> 00:14:24.840]   and there's not even a lot of consumer protections,
[00:14:24.840 --> 00:14:28.440]   yet even in the software we buy in devices we have,
[00:14:28.440 --> 00:14:29.920]   those are all forced arbitration.
[00:14:29.920 --> 00:14:31.960]   So you're kind of like--
[00:14:31.960 --> 00:14:36.440]   I love Google CEO Ruth Porat's.
[00:14:36.440 --> 00:14:37.840]   CFO.
[00:14:37.840 --> 00:14:42.280]   CFO, Chief Financial Officer, Ruth Porat's question.
[00:14:42.280 --> 00:14:43.320]   Why can't we solve this?
[00:14:43.320 --> 00:14:44.920]   We should do better.
[00:14:44.920 --> 00:14:48.200]   Well, the full line is if we can make a car that drives itself,
[00:14:48.200 --> 00:14:49.560]   why can't we do better?
[00:14:49.560 --> 00:14:52.240]   It occurs to me that if we can make a car drive itself
[00:14:52.240 --> 00:14:53.920]   is the new, if we can go to the moon.
[00:14:53.920 --> 00:14:55.600]   It is, isn't it?
[00:14:55.600 --> 00:14:58.000]   But we haven't made a car drive itself, at least not
[00:14:58.000 --> 00:14:58.680]   properly yet.
[00:14:58.680 --> 00:14:59.600]   No.
[00:14:59.600 --> 00:15:02.560]   In fact, that's a very good point as well.
[00:15:02.560 --> 00:15:04.840]   Although Google's Waymo Division has--
[00:15:04.840 --> 00:15:09.120]   the CEO said we are about ready to launch a service this fall,
[00:15:09.120 --> 00:15:09.920]   or whatever.
[00:15:09.920 --> 00:15:10.640]   This is fall.
[00:15:10.640 --> 00:15:12.000]   It can't be this fall.
[00:15:12.000 --> 00:15:19.840]   Soon, as starting where they're already putting Waymo's,
[00:15:19.840 --> 00:15:25.000]   in the next few months, you'll be getting a driverless car
[00:15:25.000 --> 00:15:25.520]   service.
[00:15:25.520 --> 00:15:29.720]   Now, when they say driverless, is there no safety driver?
[00:15:29.720 --> 00:15:32.800]   I don't--
[00:15:32.800 --> 00:15:33.840]   Oh, I'm going to look that up.
[00:15:33.840 --> 00:15:34.160]   That's right.
[00:15:34.160 --> 00:15:35.240]   That's my question.
[00:15:35.240 --> 00:15:37.360]   Because that's a scary thing.
[00:15:37.360 --> 00:15:39.080]   This is a Wall Street Journal.
[00:15:39.080 --> 00:15:40.760]   Because I think so.
[00:15:40.760 --> 00:15:43.480]   Waymo plans to launch paid driverless car service.
[00:15:43.480 --> 00:15:50.120]   This is John Craftchick talking at the WSJD Tech Conference
[00:15:50.120 --> 00:15:52.400]   in Laguna Beach.
[00:15:52.400 --> 00:15:53.680]   He says they're going to launch it soon.
[00:15:53.680 --> 00:15:57.360]   They're already doing it in Phoenix area, right?
[00:15:57.360 --> 00:15:58.520]   But not for pay.
[00:15:58.520 --> 00:16:01.120]   So that's the difference, is they want to do basically
[00:16:01.120 --> 00:16:02.800]   a driverless Uber.
[00:16:02.800 --> 00:16:05.320]   They would be beating Uber, Lyft, and everybody else
[00:16:05.320 --> 00:16:07.680]   to the table on this one.
[00:16:07.680 --> 00:16:13.560]   Yeah.
[00:16:13.560 --> 00:16:15.160]   Would you get in that car?
[00:16:15.160 --> 00:16:16.160]   I would.
[00:16:16.160 --> 00:16:17.160]   Yeah, I would.
[00:16:17.160 --> 00:16:22.400]   I'd sit in the passenger seat and be ready to dive over and--
[00:16:22.400 --> 00:16:25.320]   Grab the wheel, the non-existent wheel.
[00:16:25.320 --> 00:16:27.720]   The non-existent-- there's got to be a brake pedal, right?
[00:16:27.720 --> 00:16:27.720]   No.
[00:16:27.720 --> 00:16:28.720]   Or a button.
[00:16:28.720 --> 00:16:30.480]   It's got to be something.
[00:16:30.480 --> 00:16:34.400]   These look like the Chrysler Pacificas that they bought,
[00:16:34.400 --> 00:16:36.560]   which I think do have steering wheels, brake pedals,
[00:16:36.560 --> 00:16:38.840]   and all of that, only vestigially,
[00:16:38.840 --> 00:16:43.040]   because there isn't anybody in the front seat.
[00:16:43.040 --> 00:16:44.360]   They already do this in Phoenix.
[00:16:44.360 --> 00:16:47.640]   Some cars, it says, will contain backup drivers who
[00:16:47.640 --> 00:16:48.600]   can take control.
[00:16:48.600 --> 00:16:53.400]   But some cars is not all cars.
[00:16:53.400 --> 00:16:57.600]   And the prices will be competitive with Uber and Lyft.
[00:16:57.600 --> 00:17:01.920]   So what I will say, having ridden in self-driving cars,
[00:17:01.920 --> 00:17:04.360]   they are slow.
[00:17:04.360 --> 00:17:07.360]   You can get stuck places for a while.
[00:17:07.360 --> 00:17:10.120]   So in some ways, I'm like, oh, that sounds great.
[00:17:10.120 --> 00:17:12.040]   But then you're like, if I really need to get somewhere,
[00:17:12.040 --> 00:17:16.080]   I would not necessarily hop into one of those.
[00:17:16.080 --> 00:17:20.440]   There would be humans in the remote operations center
[00:17:20.440 --> 00:17:22.880]   who could take control of the car by remote control.
[00:17:22.880 --> 00:17:24.160]   But that sounds worse.
[00:17:24.160 --> 00:17:27.640]   Dialing--
[00:17:27.640 --> 00:17:30.880]   It's over the joystick and a trailer going--
[00:17:30.880 --> 00:17:32.840]   I'd be-- that doesn't bother me.
[00:17:32.840 --> 00:17:35.440]   I mean, having remote control of cars.
[00:17:35.440 --> 00:17:36.640]   Card drone.
[00:17:36.640 --> 00:17:37.140]   Yeah.
[00:17:37.140 --> 00:17:38.480]   I'm fine with that, because then I
[00:17:38.480 --> 00:17:40.840]   could be like, hey, this car cannot turn left.
[00:17:40.840 --> 00:17:43.840]   Could you please help it out here?
[00:17:43.840 --> 00:17:45.360]   Slummy out now.
[00:17:45.360 --> 00:17:47.440]   I've got ice cream melting in the car here.
[00:17:47.440 --> 00:17:48.080]   Let's go.
[00:17:48.080 --> 00:17:48.920]   Yeah, that's the complaint.
[00:17:48.920 --> 00:17:50.240]   We've-- remember we talked about this.
[00:17:50.240 --> 00:17:53.200]   That was a complaint from some of the residents of Phoenix
[00:17:53.200 --> 00:17:55.480]   is that it's not so much that they're driving erratically,
[00:17:55.480 --> 00:17:57.200]   that they're driving way over cautiously,
[00:17:57.200 --> 00:17:58.560]   these self-driving cars.
[00:17:58.560 --> 00:18:01.280]   And so you get stuck behind them because they don't pick him.
[00:18:01.280 --> 00:18:03.520]   It's like when you're driving behind somebody
[00:18:03.520 --> 00:18:05.480]   learning to drive and they just can't make that left turn
[00:18:05.480 --> 00:18:06.480]   in it to him.
[00:18:06.480 --> 00:18:09.480]   Well, my daughter--
[00:18:09.480 --> 00:18:12.240]   I don't want to humiliate her, but she doesn't watch the show.
[00:18:12.240 --> 00:18:15.000]   She believes in the speed limit.
[00:18:15.000 --> 00:18:15.560]   Oh.
[00:18:15.560 --> 00:18:16.800]   I'm driving people crazy.
[00:18:16.800 --> 00:18:17.920]   I drive the speed limit.
[00:18:17.920 --> 00:18:20.520]   I'm just driving the left lane.
[00:18:20.520 --> 00:18:21.920]   No, she doesn't do that.
[00:18:21.920 --> 00:18:23.000]   No, but she drives--
[00:18:23.000 --> 00:18:24.840]   She's following the law, Jeff.
[00:18:24.840 --> 00:18:26.160]   She is following the law.
[00:18:26.160 --> 00:18:28.160]   But I just imagine--
[00:18:28.160 --> 00:18:30.160]   I get scared because of the people
[00:18:30.160 --> 00:18:32.680]   who are going to be going crazy around her.
[00:18:32.680 --> 00:18:36.520]   Tell her it's OK to drive seven miles over the speed limit.
[00:18:36.520 --> 00:18:37.520]   Tell her she tried something.
[00:18:37.520 --> 00:18:38.320]   Tell her she tried something.
[00:18:38.320 --> 00:18:39.320]   Tell her she tried something.
[00:18:39.320 --> 00:18:40.320]   You know what?
[00:18:40.320 --> 00:18:42.240]   In Austin, everyone drives the speed limit or below,
[00:18:42.240 --> 00:18:44.040]   and they drive in the left-hand lane.
[00:18:44.040 --> 00:18:45.800]   These people are terrible.
[00:18:45.800 --> 00:18:47.480]   Sounds like you're in Florida.
[00:18:47.480 --> 00:18:49.840]   So just tell her to come to Austin, she'll be fine.
[00:18:49.840 --> 00:18:52.920]   Turn signal, just blinking the whole way.
[00:18:52.920 --> 00:18:53.920]   Oh, my goodness.
[00:18:53.920 --> 00:18:55.200]   These people.
[00:18:55.200 --> 00:18:55.880]   Isn't it funny?
[00:18:55.880 --> 00:18:58.160]   What is the point of the speed limit?
[00:18:58.160 --> 00:19:00.640]   If nobody drives it.
[00:19:00.640 --> 00:19:01.280]   It Austin, they--
[00:19:01.280 --> 00:19:02.800]   You mean Germany?
[00:19:02.800 --> 00:19:06.360]   And man, I've been in cars in Germany with drivers.
[00:19:06.360 --> 00:19:07.600]   I'm not driving.
[00:19:07.600 --> 00:19:11.880]   I get pretty freaked at how fast they can go on the Autobahn.
[00:19:11.880 --> 00:19:13.960]   Yeah, there's no speed limit.
[00:19:13.960 --> 00:19:15.320]   No speed limit.
[00:19:15.320 --> 00:19:17.600]   And it's freaky.
[00:19:17.600 --> 00:19:21.200]   One time-- so I remember when the volcano came to Iceland
[00:19:21.200 --> 00:19:22.360]   and stopped flights.
[00:19:22.360 --> 00:19:22.840]   Yeah.
[00:19:22.840 --> 00:19:23.920]   Oh, yes.
[00:19:23.920 --> 00:19:25.160]   And so I made it.
[00:19:25.160 --> 00:19:26.960]   I was supposed to get a flight out of Berlin
[00:19:26.960 --> 00:19:29.360]   from Republic of that day.
[00:19:29.360 --> 00:19:30.760]   The flights were all canceled out of there.
[00:19:30.760 --> 00:19:32.800]   There was still a flight out of Munich.
[00:19:32.800 --> 00:19:36.920]   There was a location scouting company in line with me.
[00:19:36.920 --> 00:19:39.440]   They had a friend who got the last rental car,
[00:19:39.440 --> 00:19:42.200]   was going to drive to Munich.
[00:19:42.200 --> 00:19:44.920]   The guy said, oh, this is my old friend, Jeff.
[00:19:44.920 --> 00:19:46.200]   I didn't know me at all.
[00:19:46.200 --> 00:19:47.680]   Can you take him to Munich?
[00:19:47.680 --> 00:19:49.840]   So these two women took me to Munich.
[00:19:49.840 --> 00:19:51.280]   I'm in the back seat.
[00:19:51.280 --> 00:19:52.640]   God bless them.
[00:19:52.640 --> 00:19:55.120]   And she's hauling butt, which also--
[00:19:55.120 --> 00:19:57.080]   God bless because I was going to make the flight.
[00:19:57.080 --> 00:20:02.440]   But I'm forgetting the conversion from kilometers to miles.
[00:20:02.440 --> 00:20:04.600]   And I turn on Google Maps and see the speed,
[00:20:04.600 --> 00:20:06.120]   and I freak the turn it off.
[00:20:06.120 --> 00:20:10.240]   And I made the flight.
[00:20:10.240 --> 00:20:12.120]   I think it was the last flight out of Europe.
[00:20:12.120 --> 00:20:13.240]   And I made it.
[00:20:13.240 --> 00:20:14.400]   See?
[00:20:14.400 --> 00:20:15.080]   This is good.
[00:20:15.080 --> 00:20:16.760]   I had, apparently, two of the kids--
[00:20:16.760 --> 00:20:19.920]   I took a carpool with some people.
[00:20:19.920 --> 00:20:22.800]   And the kids that I carpooled, they told my daughter
[00:20:22.800 --> 00:20:24.600]   that they were afraid to ride in the car with me,
[00:20:24.600 --> 00:20:25.800]   because I drove too quickly.
[00:20:25.800 --> 00:20:26.300]   See?
[00:20:26.300 --> 00:20:27.480]   I thought so.
[00:20:27.480 --> 00:20:30.920]   As soon as somebody says, oh, I don't like it here at Austin,
[00:20:30.920 --> 00:20:33.360]   everybody's driving the speed limit.
[00:20:33.360 --> 00:20:36.120]   I started to worry.
[00:20:36.120 --> 00:20:37.840]   Yeah, I'm apparently--
[00:20:37.840 --> 00:20:39.960]   I drive too quickly.
[00:20:39.960 --> 00:20:40.880]   Keeps me on my toes.
[00:20:40.880 --> 00:20:42.480]   Otherwise, I get bored, and then I'm not paying attention.
[00:20:42.480 --> 00:20:42.960]   That's the problem.
[00:20:42.960 --> 00:20:44.440]   You're too smart.
[00:20:44.440 --> 00:20:44.840]   You're too smart.
[00:20:44.840 --> 00:20:46.160]   I don't think that's the problem.
[00:20:46.160 --> 00:20:46.640]   No, you are.
[00:20:46.640 --> 00:20:47.720]   You're bored.
[00:20:47.720 --> 00:20:50.800]   And you're trying to make things interesting.
[00:20:50.800 --> 00:20:53.520]   You want it to be more like a roller coaster.
[00:20:53.520 --> 00:20:54.120]   There we go.
[00:20:54.120 --> 00:20:56.920]   I'm trying to-- I'll tell the kids that when they're crying
[00:20:56.920 --> 00:20:57.480]   in the backseat.
[00:20:57.480 --> 00:20:59.400]   No, look, you've got an interesting, guys.
[00:20:59.400 --> 00:21:01.040]   Don't forget, she's got a Tesla.
[00:21:01.040 --> 00:21:04.360]   So not only can she go fast, she can pin the kids' heads
[00:21:04.360 --> 00:21:05.480]   against the seat.
[00:21:05.480 --> 00:21:06.240]   I can go fast.
[00:21:06.240 --> 00:21:07.240]   There's a speed of thought.
[00:21:07.240 --> 00:21:12.480]   OK, so--
[00:21:12.480 --> 00:21:14.720]   Lorny, Lorny.
[00:21:14.720 --> 00:21:15.220]   That's it.
[00:21:15.220 --> 00:21:16.040]   All right, I'm going to take a break,
[00:21:16.040 --> 00:21:20.520]   because if I don't start doing ads now, I may never do them.
[00:21:20.520 --> 00:21:22.080]   But we have lots more to talk about.
[00:21:22.080 --> 00:21:28.000]   We're going to do some stuff.
[00:21:28.000 --> 00:21:28.520]   I'm looking--
[00:21:28.520 --> 00:21:28.720]   Stuff.
[00:21:28.720 --> 00:21:30.440]   There's going to be lots of stuff.
[00:21:30.440 --> 00:21:34.480]   I like this article about the woman who's helping
[00:21:34.480 --> 00:21:36.880]   the Google design.
[00:21:36.880 --> 00:21:39.400]   I do too, especially because I don't love the design.
[00:21:39.400 --> 00:21:41.080]   So we'll talk about Google's design.
[00:21:41.080 --> 00:21:42.080]   Yeah.
[00:21:42.080 --> 00:21:44.920]   Ivy Ross, yeah.
[00:21:44.920 --> 00:21:47.480]   We will talk about other things too.
[00:21:47.480 --> 00:21:49.840]   Oh, and I added this just for you, Jeff.
[00:21:49.840 --> 00:21:51.720]   Google's NHS kidney data.
[00:21:51.720 --> 00:21:53.600]   They brought it in house, and people are upset.
[00:21:53.600 --> 00:21:55.120]   So you can be able to do it with that.
[00:21:55.120 --> 00:21:56.360]   Yep, yep, yep.
[00:21:56.360 --> 00:21:58.320]   Especially in England.
[00:21:58.320 --> 00:22:00.040]   Yeah, it's their data.
[00:22:00.040 --> 00:22:01.080]   It's their data.
[00:22:01.080 --> 00:22:02.160]   It's their data.
[00:22:02.160 --> 00:22:06.520]   And we've got a fairly beefy change log, I believe, this week.
[00:22:06.520 --> 00:22:07.800]   Yeah, I even added something to it.
[00:22:07.800 --> 00:22:08.320]   There you go.
[00:22:08.320 --> 00:22:09.000]   On there.
[00:22:09.000 --> 00:22:10.800]   So there's a lot to come.
[00:22:10.800 --> 00:22:12.360]   Our show today brought to you by Captera.
[00:22:12.360 --> 00:22:14.240]   If you're in business and you're looking for software--
[00:22:14.240 --> 00:22:16.480]   oh, there's a lot of good stuff out there.
[00:22:16.480 --> 00:22:20.840]   The best place to go, though, is captera.com/twig.
[00:22:20.840 --> 00:22:23.560]   C-A-P-T-E-D-E-R-A.
[00:22:23.560 --> 00:22:30.240]   Captera is a directory of thousands of business programs.
[00:22:30.240 --> 00:22:32.240]   600 categories.
[00:22:32.240 --> 00:22:38.400]   Everything from project management, e-commerce, email marketing.
[00:22:38.400 --> 00:22:41.520]   If you're a vet, a dentist, office software,
[00:22:41.520 --> 00:22:45.200]   to run a medical office, everything you'd ever want is in there.
[00:22:45.200 --> 00:22:49.960]   Plus, 600,000 plus verified user reviews
[00:22:49.960 --> 00:22:53.320]   from actual users of the software, people just like you.
[00:22:53.320 --> 00:22:57.120]   And a really great search engine makes it very easy to find software
[00:22:57.120 --> 00:22:59.680]   that has the features you require.
[00:22:59.680 --> 00:23:03.200]   You can even create a comparison table with links
[00:23:03.200 --> 00:23:04.680]   to their websites and everything else.
[00:23:04.680 --> 00:23:05.400]   And this is free.
[00:23:05.400 --> 00:23:07.680]   This is absolutely free.
[00:23:07.680 --> 00:23:09.160]   This is the year--
[00:23:09.160 --> 00:23:14.120]   12, it's about to be the year 2019, the year that you get your business
[00:23:14.120 --> 00:23:15.920]   going with Captera.
[00:23:15.920 --> 00:23:17.840]   Get the best software.
[00:23:17.840 --> 00:23:21.360]   Right now, if you go to captera.com/twig, you can download.
[00:23:21.360 --> 00:23:22.520]   This is an additional thing.
[00:23:22.520 --> 00:23:23.560]   That's really cool.
[00:23:23.560 --> 00:23:25.960]   They're a big book of free software.
[00:23:25.960 --> 00:23:29.600]   This is a free book that'll help you find the software you need for your business.
[00:23:29.600 --> 00:23:31.880]   300 different tools that are all free.
[00:23:31.880 --> 00:23:33.360]   And the book is free.
[00:23:33.360 --> 00:23:36.080]   So this is just a great thing to have.
[00:23:36.080 --> 00:23:39.880]   You put it on your Kindle or your iBooks or whatever.
[00:23:39.880 --> 00:23:41.080]   And it's all in there.
[00:23:41.080 --> 00:23:41.600]   It's kept here.
[00:23:41.600 --> 00:23:42.320]   And it's all free.
[00:23:42.320 --> 00:23:45.080]   It's kept here as a big book of free software.
[00:23:45.080 --> 00:23:50.600]   Just a little encouragement to get you to captera.com/twig.
[00:23:50.600 --> 00:23:53.000]   This is such a good, useful tool.
[00:23:53.000 --> 00:23:54.080]   People call me.
[00:23:54.080 --> 00:23:55.280]   They Google it.
[00:23:55.280 --> 00:23:57.160]   That's not the best way to find software.
[00:23:57.160 --> 00:23:58.040]   You'll list the stuff.
[00:23:58.040 --> 00:23:59.400]   You don't know what's good.
[00:23:59.400 --> 00:24:01.200]   Captera makes it easy.
[00:24:01.200 --> 00:24:07.000]   C-A-P-T-E-W-R-A.com/tweig.
[00:24:07.000 --> 00:24:11.440]   And we thank them for their support of this week in Google.
[00:24:11.440 --> 00:24:15.680]   So Ivy, I didn't even know about Ivy Ross until they mentioned her at the Pixel 3
[00:24:15.680 --> 00:24:17.320]   announcement.
[00:24:17.320 --> 00:24:20.480]   But she is the vice president of design at Google.
[00:24:20.480 --> 00:24:23.440]   She was responsible, I think, for material design.
[00:24:23.440 --> 00:24:27.240]   But really her specialty, she's the Johnny Ive of Google.
[00:24:27.240 --> 00:24:28.880]   Her specialty is hardware.
[00:24:28.880 --> 00:24:31.080]   She was responsible.
[00:24:31.080 --> 00:24:37.280]   She was a jewelry designer, still is, and was brought in to help fix Google
[00:24:37.280 --> 00:24:38.360]   Glass.
[00:24:38.360 --> 00:24:40.320]   [LAUGHTER]
[00:24:40.320 --> 00:24:42.280]   Never could make that one look good.
[00:24:42.280 --> 00:24:43.360]   That was exceeded there.
[00:24:43.360 --> 00:24:43.880]   No.
[00:24:43.880 --> 00:24:46.080]   But this was in 2014.
[00:24:46.080 --> 00:24:47.440]   Glass was already out.
[00:24:47.440 --> 00:24:48.840]   She was kind of stuck with that one.
[00:24:48.840 --> 00:24:52.120]   She's the one who put the orange button on the Pixel 2, which I really like.
[00:24:52.120 --> 00:24:52.640]   The paint.
[00:24:52.640 --> 00:24:53.520]   Oh, I did like that.
[00:24:53.520 --> 00:24:54.400]   Yeah.
[00:24:54.400 --> 00:24:55.960]   That's a designer, right?
[00:24:55.960 --> 00:24:58.200]   No, but no engineers are going to say, you know what would be cool is you
[00:24:58.200 --> 00:24:59.880]   have a black white phone with that orange button.
[00:24:59.880 --> 00:25:01.360]   Nobody's going to say that.
[00:25:01.360 --> 00:25:03.720]   But that's what she did.
[00:25:03.720 --> 00:25:06.000]   The Pixel 3, I think, is very good looking.
[00:25:06.000 --> 00:25:08.120]   The Google Home Hub.
[00:25:08.120 --> 00:25:10.320]   Very good looking.
[00:25:10.320 --> 00:25:11.280]   Well, it's better.
[00:25:11.280 --> 00:25:18.320]   I think it's better looking than, say, Amazon's Echo Show, which everybody widely criticized.
[00:25:18.320 --> 00:25:21.400]   Oh, well, that's not-- that's a very low bar.
[00:25:21.400 --> 00:25:23.920]   It's better looking than it died.
[00:25:23.920 --> 00:25:24.480]   It's better--
[00:25:24.480 --> 00:25:29.160]   I mean, basically, she was the one that put fabric on everything.
[00:25:29.160 --> 00:25:30.400]   It's the new fabric design.
[00:25:30.400 --> 00:25:31.880]   It's the one that came up with mint.
[00:25:31.880 --> 00:25:32.760]   The fabric is nice.
[00:25:32.760 --> 00:25:35.080]   Mint color, right?
[00:25:35.080 --> 00:25:38.080]   The pink one that looks like a donut.
[00:25:38.080 --> 00:25:39.560]   Yeah.
[00:25:39.560 --> 00:25:43.560]   Because I think they needed somebody to give it some human scale.
[00:25:43.560 --> 00:25:49.960]   Otherwise, it's like the Facebook portal, which, by the way, is on its way.
[00:25:49.960 --> 00:25:50.960]   Oh.
[00:25:50.960 --> 00:25:55.840]   I thought yesterday we had a meeting and Anthony said, "When's the portal coming?"
[00:25:55.840 --> 00:25:58.280]   I said, "Oh, let me check."
[00:25:58.280 --> 00:25:59.920]   And there was no evidence that I'd purchased it.
[00:25:59.920 --> 00:26:01.360]   I said, "I think I'm OK.
[00:26:01.360 --> 00:26:02.480]   I think I escaped."
[00:26:02.480 --> 00:26:05.440]   No, they just charged my card.
[00:26:05.440 --> 00:26:06.720]   Dang it.
[00:26:06.720 --> 00:26:09.280]   So we can review.
[00:26:09.280 --> 00:26:11.400]   That is an ugly thing, the Facebook portal.
[00:26:11.400 --> 00:26:15.680]   I mean, that's one of those things where Facebook must really be.
[00:26:15.680 --> 00:26:19.840]   They just-- let's get this thing off our books because it's not going anywhere.
[00:26:19.840 --> 00:26:23.440]   I've been seeing the ads because we've been watching some network TV and--
[00:26:23.440 --> 00:26:26.440]   On the NFL, they're buying some hefty expensive ads.
[00:26:26.440 --> 00:26:27.440]   Really?
[00:26:27.440 --> 00:26:28.440]   Yes.
[00:26:28.440 --> 00:26:30.440]   I've seen ads on the streets.
[00:26:30.440 --> 00:26:31.440]   On the good place.
[00:26:31.440 --> 00:26:34.960]   Does this-- I mean, does this look at all appealing?
[00:26:34.960 --> 00:26:36.560]   I mean, look at the design on this thing.
[00:26:36.560 --> 00:26:38.480]   It's hideous.
[00:26:38.480 --> 00:26:41.680]   It's not pretty, but the advertising is very effective.
[00:26:41.680 --> 00:26:44.880]   I mean, it's people basically dancing around and standing on their heads.
[00:26:44.880 --> 00:26:45.880]   It's very cute.
[00:26:45.880 --> 00:26:52.280]   It's what I think of as-- it's the same sort of things you see in all of the tech companies.
[00:26:52.280 --> 00:26:56.960]   It's cutesy people trying to be fun and mostly human with technology.
[00:26:56.960 --> 00:26:57.960]   Yeah.
[00:26:57.960 --> 00:26:59.960]   It's the exact opposite of what it looks like when you're actually planning--
[00:26:59.960 --> 00:27:02.960]   That's private by design.
[00:27:02.960 --> 00:27:04.960]   Acceptance not.
[00:27:04.960 --> 00:27:08.960]   I was waiting to hit line-- I had to line folks on this page.
[00:27:08.960 --> 00:27:10.960]   Ah, I was like, oh no.
[00:27:10.960 --> 00:27:15.960]   No, they say that because they got a clip you could put up the camera.
[00:27:15.960 --> 00:27:21.560]   I guess, you know, I suppose I can review this and then say, oh, it doesn't mean my needs
[00:27:21.560 --> 00:27:23.960]   and send it right back.
[00:27:23.960 --> 00:27:28.240]   Yes, or you could not review it and you could just tell people, I'm not reviewing this because
[00:27:28.240 --> 00:27:29.760]   this isn't something you should buy.
[00:27:29.760 --> 00:27:31.360]   I was going to do that.
[00:27:31.360 --> 00:27:34.360]   And then I found out I accidentally purchased one too.
[00:27:34.360 --> 00:27:36.240]   Can't still send.
[00:27:36.240 --> 00:27:38.240]   How did you accidentally purchase?
[00:27:38.240 --> 00:27:41.040]   Oh, chef, I do that all the time.
[00:27:41.040 --> 00:27:43.200]   I've got jewelry, gadgets.
[00:27:43.200 --> 00:27:44.560]   That's your excuse, yeah.
[00:27:44.560 --> 00:27:47.560]   A click of the button.
[00:27:47.560 --> 00:27:51.720]   I have a knee jerk reaction to new products from big companies that we should have them
[00:27:51.720 --> 00:27:53.480]   and review them.
[00:27:53.480 --> 00:27:58.440]   And I bought it really without thinking, gosh, really do we need to review this?
[00:27:58.440 --> 00:28:02.200]   Is there anybody who is actually seriously considering this?
[00:28:02.200 --> 00:28:06.560]   And then once I, you know, cooler heads prevail, I realized there's no one in the world considering
[00:28:06.560 --> 00:28:08.120]   to buying this, right?
[00:28:08.120 --> 00:28:10.440]   One day, she yelled at you.
[00:28:10.440 --> 00:28:15.680]   Well, even before, I mean, there's nobody saying, gosh, should I buy this?
[00:28:15.680 --> 00:28:17.880]   Let's see the twit review.
[00:28:17.880 --> 00:28:20.280]   Nobody is doing that.
[00:28:20.280 --> 00:28:23.960]   What do they think it's going to do uniquely versus this?
[00:28:23.960 --> 00:28:26.520]   I'm so clear that they just developed it.
[00:28:26.520 --> 00:28:27.840]   They got to ship it.
[00:28:27.840 --> 00:28:28.840]   That's it.
[00:28:28.840 --> 00:28:31.840]   That's the only thing that's out there is that it follows you.
[00:28:31.840 --> 00:28:33.960]   No, there's cameras that follow you.
[00:28:33.960 --> 00:28:35.640]   That's not a new technology.
[00:28:35.640 --> 00:28:38.920]   Well, that live in your home like that from the pig vendors?
[00:28:38.920 --> 00:28:42.720]   No, there's nothing like that right now that you can easily communicate with someone.
[00:28:42.720 --> 00:28:44.080]   Why did you text on that for years?
[00:28:44.080 --> 00:28:45.400]   I'm convinced of it.
[00:28:45.400 --> 00:28:47.960]   Oh, OK, like for video conferences.
[00:28:47.960 --> 00:28:50.080]   Yeah, remember that in the home?
[00:28:50.080 --> 00:28:51.080]   No, this was in the home.
[00:28:51.080 --> 00:28:52.080]   Remember the orbit camera?
[00:28:52.080 --> 00:28:53.560]   It was stood on a stalk.
[00:28:53.560 --> 00:28:55.400]   It was a ball.
[00:28:55.400 --> 00:28:57.240]   It would follow you around.
[00:28:57.240 --> 00:28:58.720]   This isn't hard to do.
[00:28:58.720 --> 00:29:01.920]   The Meevo camera, which is really your right, not a consumer product.
[00:29:01.920 --> 00:29:05.000]   It's designed for Facebook live streamers does this.
[00:29:05.000 --> 00:29:06.400]   It's actually easier to do now.
[00:29:06.400 --> 00:29:09.080]   The orbit actually had to move.
[00:29:09.080 --> 00:29:16.360]   But nowadays with a 4K sensor, you just can say, OK, I got 4K, I got wide angle.
[00:29:16.360 --> 00:29:19.680]   I'm covering this room and all I have to do is zoom in on that part of it and behind
[00:29:19.680 --> 00:29:20.680]   a resolution.
[00:29:20.680 --> 00:29:23.840]   And I'm pretty sure that's-- I don't think the camera moves.
[00:29:23.840 --> 00:29:28.080]   But I also mean the back end integration that allows you to just say who you want to talk
[00:29:28.080 --> 00:29:29.080]   to.
[00:29:29.080 --> 00:29:30.080]   It's easy like a phone.
[00:29:30.080 --> 00:29:32.600]   The Logitech camera, did that have like a tie-in?
[00:29:32.600 --> 00:29:34.240]   No, yeah, it was Skype.
[00:29:34.240 --> 00:29:35.880]   You had to use Skype or something.
[00:29:35.880 --> 00:29:36.880]   OK.
[00:29:36.880 --> 00:29:38.280]   OK, well Skype's pretty big.
[00:29:38.280 --> 00:29:39.880]   All right, I'll give it to you.
[00:29:39.880 --> 00:29:40.880]   I'll give it to you.
[00:29:40.880 --> 00:29:41.880]   I'll give it to you.
[00:29:41.880 --> 00:29:42.880]   I'll give it to you.
[00:29:42.880 --> 00:29:43.880]   I'll give it to you.
[00:29:43.880 --> 00:29:44.880]   The Kinect camera on the Xbox apparently followed you.
[00:29:44.880 --> 00:29:45.880]   I don't remember that.
[00:29:45.880 --> 00:29:47.360]   OK, yeah, but that was-- that's not for--
[00:29:47.360 --> 00:29:48.360]   That's a game.
[00:29:48.360 --> 00:29:49.880]   Talking to people.
[00:29:49.880 --> 00:29:50.880]   So you tell me.
[00:29:50.880 --> 00:29:55.600]   So I went to portal.facebook.com.
[00:29:55.600 --> 00:30:01.040]   And I actually, in order to do this, had to log in as me, which I don't do because remember
[00:30:01.040 --> 00:30:03.640]   I activate my Facebook account so I had to reactivate it.
[00:30:03.640 --> 00:30:04.640]   Oh, we know.
[00:30:04.640 --> 00:30:06.880]   And this is what I get.
[00:30:06.880 --> 00:30:10.400]   Your portal orders will show up here.
[00:30:10.400 --> 00:30:12.320]   Big blank white space.
[00:30:12.320 --> 00:30:14.880]   I thought, oh, hallelujah.
[00:30:14.880 --> 00:30:17.960]   I got no portal orders.
[00:30:17.960 --> 00:30:21.320]   Well, they charge my credit card, 600-some bucks because I bought two--
[00:30:21.320 --> 00:30:22.520]   What did you order?
[00:30:22.520 --> 00:30:23.520]   I bought two.
[00:30:23.520 --> 00:30:24.520]   Well, you can't--
[00:30:24.520 --> 00:30:25.520]   Why?
[00:30:25.520 --> 00:30:27.960]   OK, again, I must have been drunk.
[00:30:27.960 --> 00:30:29.920]   I thought, well, you have to have one on both ends.
[00:30:29.920 --> 00:30:30.920]   It turns out you don't.
[00:30:30.920 --> 00:30:34.000]   You could call somebody who has a Facebook messenger.
[00:30:34.000 --> 00:30:36.680]   But I just thought, well, if we're going to test this, we've got to have two.
[00:30:36.680 --> 00:30:38.520]   We have one on both ends.
[00:30:38.520 --> 00:30:39.520]   Yeah.
[00:30:39.520 --> 00:30:41.120]   I mean, that's true.
[00:30:41.120 --> 00:30:42.120]   Right?
[00:30:42.120 --> 00:30:43.120]   Yes.
[00:30:43.120 --> 00:30:44.120]   Yes.
[00:30:44.120 --> 00:30:45.120]   Thank you.
[00:30:45.120 --> 00:30:46.120]   You could be my editor.
[00:30:46.120 --> 00:30:47.120]   Yeah.
[00:30:47.120 --> 00:30:50.720]   Wait, do I get raised with that?
[00:30:50.720 --> 00:30:51.720]   No.
[00:30:51.720 --> 00:30:53.920]   You did not get a raise.
[00:30:53.920 --> 00:30:54.920]   Oh, yes.
[00:30:54.920 --> 00:30:55.920]   I'll send you.
[00:30:55.920 --> 00:30:56.920]   But here's the good news.
[00:30:56.920 --> 00:30:57.920]   You'll send me a portal.
[00:30:57.920 --> 00:30:58.920]   You'll get a portal.
[00:30:58.920 --> 00:30:59.920]   [LAUGHTER]
[00:30:59.920 --> 00:31:01.320]   I'm going to decline gracefully.
[00:31:01.320 --> 00:31:02.320]   You don't want that--
[00:31:02.320 --> 00:31:06.640]   You know, you can't talk to me because I don't have a Facebook messenger.
[00:31:06.640 --> 00:31:09.240]   I don't even exist for the portal.
[00:31:09.240 --> 00:31:10.920]   It's like, you want to talk to who?
[00:31:10.920 --> 00:31:11.920]   I don't think so.
[00:31:11.920 --> 00:31:14.680]   That means I can't talk to my mom either.
[00:31:14.680 --> 00:31:20.160]   By the way, I just saw a stat 44% of people 18 to 25 deleted, I think, mentions this last
[00:31:20.160 --> 00:31:23.720]   week that have deleted their Facebook on their phone.
[00:31:23.720 --> 00:31:25.120]   I think it's going to get harder and harder.
[00:31:25.120 --> 00:31:27.040]   I think this portal is going to be--
[00:31:27.040 --> 00:31:30.400]   I'll only be able to call other people's family members.
[00:31:30.400 --> 00:31:31.960]   Other people's family.
[00:31:31.960 --> 00:31:32.960]   [LAUGHTER]
[00:31:32.960 --> 00:31:33.960]   [LAUGHTER]
[00:31:33.960 --> 00:31:36.160]   Your high school boyfriend's grandmother.
[00:31:36.160 --> 00:31:37.160]   Exactly.
[00:31:37.160 --> 00:31:38.160]   Precisely.
[00:31:38.160 --> 00:31:39.160]   Precisely.
[00:31:39.160 --> 00:31:41.560]   People you may want to call.
[00:31:41.560 --> 00:31:42.560]   Yeah.
[00:31:42.560 --> 00:31:43.560]   Really?
[00:31:43.560 --> 00:31:44.560]   Nana?
[00:31:44.560 --> 00:31:45.560]   Nana?
[00:31:45.560 --> 00:31:46.560]   Yeah.
[00:31:46.560 --> 00:31:48.440]   I haven't seen Nana in 45 years.
[00:31:48.440 --> 00:31:51.360]   She might be dead.
[00:31:51.360 --> 00:31:54.960]   Google's AI is helping the New York Times.
[00:31:54.960 --> 00:31:55.960]   This is actually cool.
[00:31:55.960 --> 00:31:56.960]   This is cool.
[00:31:56.960 --> 00:31:57.960]   This is happy, right?
[00:31:57.960 --> 00:31:58.960]   This is cool.
[00:31:58.960 --> 00:32:00.920]   They have-- New York Times has 5 million photos.
[00:32:00.920 --> 00:32:06.160]   When I say photos, I mean physical photos, right?
[00:32:06.160 --> 00:32:07.160]   These aren't--
[00:32:07.160 --> 00:32:11.400]   So those are all in the basement of the building where I am talking to you from right now.
[00:32:11.400 --> 00:32:12.560]   What?
[00:32:12.560 --> 00:32:16.840]   We are next to the New York Times in the old headquarters of the New York Herald Tribune
[00:32:16.840 --> 00:32:19.200]   and the basement, the old real room.
[00:32:19.200 --> 00:32:20.200]   Sorry.
[00:32:20.200 --> 00:32:21.200]   [GRUNTING]
[00:32:21.200 --> 00:32:22.200]   Lights.
[00:32:22.200 --> 00:32:23.200]   I'm alive.
[00:32:23.200 --> 00:32:24.200]   I'm alive.
[00:32:24.200 --> 00:32:25.200]   Lights.
[00:32:25.200 --> 00:32:26.200]   [LAUGHTER]
[00:32:26.200 --> 00:32:27.200]   He's alive.
[00:32:27.200 --> 00:32:32.200]   So I'm alive.
[00:32:32.200 --> 00:32:36.440]   So the old real room was where the big, huge reels of paper were going up into the restroom,
[00:32:36.440 --> 00:32:37.960]   right, up onto the docks.
[00:32:37.960 --> 00:32:41.760]   And so the real room is a big, gigantic cavernous basement.
[00:32:41.760 --> 00:32:46.160]   And the New York Times uses that to store their photos, their old photo morgue.
[00:32:46.160 --> 00:32:52.760]   Now, as the story mentions, there was a leaky pipe about two, two and a half years ago.
[00:32:52.760 --> 00:32:56.260]   And that was through our very space here.
[00:32:56.260 --> 00:32:57.260]   Oh, where's the--
[00:32:57.260 --> 00:32:58.260]   Oh, that is--
[00:32:58.260 --> 00:32:59.260]   Oh, that is--
[00:32:59.260 --> 00:33:00.260]   That is not the morgue.
[00:33:00.260 --> 00:33:01.260]   [LAUGHTER]
[00:33:01.260 --> 00:33:02.260]   That's an impact.
[00:33:02.260 --> 00:33:04.080]   There he is in the basement.
[00:33:04.080 --> 00:33:07.600]   Oh, do you want to see it again?
[00:33:07.600 --> 00:33:10.440]   [MUSIC PLAYING]
[00:33:10.440 --> 00:33:11.440]   It's alive.
[00:33:11.440 --> 00:33:12.440]   It's alive.
[00:33:12.440 --> 00:33:13.440]   It's alive.
[00:33:13.440 --> 00:33:14.440]   It's alive.
[00:33:14.440 --> 00:33:15.440]   It's alive.
[00:33:15.440 --> 00:33:16.440]   It's alive.
[00:33:16.440 --> 00:33:17.440]   It's alive.
[00:33:17.440 --> 00:33:18.440]   It's alive.
[00:33:18.440 --> 00:33:19.440]   It's alive.
[00:33:19.440 --> 00:33:20.440]   It's alive.
[00:33:20.440 --> 00:33:21.440]   It's alive.
[00:33:21.440 --> 00:33:22.440]   It's alive.
[00:33:22.440 --> 00:33:23.440]   It's alive in the basement of the--
[00:33:23.440 --> 00:33:24.440]   Yes.
[00:33:24.440 --> 00:33:25.440]   Yes.
[00:33:25.440 --> 00:33:26.440]   So they are prints, right?
[00:33:26.440 --> 00:33:27.440]   They're not--
[00:33:27.440 --> 00:33:29.440]   Yeah, I'm going to get a leak.
[00:33:29.440 --> 00:33:36.040]   We had a leak above me that the massacre at our two floors of the school, and it got
[00:33:36.040 --> 00:33:37.040]   down to the basement.
[00:33:37.040 --> 00:33:40.280]   And there was a little bit of damage, not thankfully, not very much.
[00:33:40.280 --> 00:33:44.840]   But it could have just absolutely been disastrous for all that history.
[00:33:44.840 --> 00:33:48.960]   And so I think that's part of the reason that they hurried up digitizing more, and now
[00:33:48.960 --> 00:33:51.760]   use-- once it's digitized, guess what?
[00:33:51.760 --> 00:33:53.520]   Google Photos can do all kinds of neat things for them.
[00:33:53.520 --> 00:33:55.120]   Well, but it's more than just the front.
[00:33:55.120 --> 00:33:58.760]   So here's the back of a photo from the New York Times archive.
[00:33:58.760 --> 00:33:59.760]   Yeah.
[00:33:59.760 --> 00:34:01.240]   And so this has notes.
[00:34:01.240 --> 00:34:02.240]   It has received--
[00:34:02.240 --> 00:34:03.240]   Less pre-memory.
[00:34:03.240 --> 00:34:09.960]   --December 25, 1942, Art Department Files, Thursday early run, and then there is-- for some
[00:34:09.960 --> 00:34:13.520]   reason, glued to it, clips from the paper, pencils.
[00:34:13.520 --> 00:34:15.480]   They want to show when it ran.
[00:34:15.480 --> 00:34:16.480]   --where it ran.
[00:34:16.480 --> 00:34:17.480]   Yeah.
[00:34:17.480 --> 00:34:18.480]   Context on the--
[00:34:18.480 --> 00:34:19.480]   Yeah.
[00:34:19.480 --> 00:34:20.880]   --to publish the New York Times Long Island, February 28, 1988.
[00:34:20.880 --> 00:34:25.560]   So every time it's published, they have a stamp, and it's been used many times.
[00:34:25.560 --> 00:34:27.640]   I mean, this is really interesting.
[00:34:27.640 --> 00:34:29.120]   I used to have my photo wheel.
[00:34:29.120 --> 00:34:30.120]   So look at that.
[00:34:30.120 --> 00:34:32.320]   It's three and 15, 16 spies, something else, right?
[00:34:32.320 --> 00:34:35.200]   And you have to get the-- and these plastic things to play.
[00:34:35.200 --> 00:34:39.000]   This is what I did with Grease pens.
[00:34:39.000 --> 00:34:40.000]   It's really cool.
[00:34:40.000 --> 00:34:45.200]   And this is before you had your media files that are up in WordPress where you go to the
[00:34:45.200 --> 00:34:48.960]   media library and you're like, Google, and now you can Google for these things and find
[00:34:48.960 --> 00:34:49.960]   these old photos.
[00:34:49.960 --> 00:34:52.800]   And I mean, I remember-- and I was very young.
[00:34:52.800 --> 00:34:55.800]   Jeff is going to be way better at this, but we had--
[00:34:55.800 --> 00:34:56.800]   Because I'm old.
[00:34:56.800 --> 00:34:57.800]   But I am old.
[00:34:57.800 --> 00:34:58.800]   --as you're old.
[00:34:58.800 --> 00:34:59.800]   You are alive.
[00:34:59.800 --> 00:35:05.960]   But I remember my first journalism job, I was telling people about Google because it
[00:35:05.960 --> 00:35:08.720]   was new.
[00:35:08.720 --> 00:35:12.760]   But I didn't understand how they did.
[00:35:12.760 --> 00:35:15.280]   My first journalism job was in 2000.
[00:35:15.280 --> 00:35:16.280]   2000.
[00:35:16.280 --> 00:35:18.840]   At least you're in this millennium.
[00:35:18.840 --> 00:35:19.840]   Yeah.
[00:35:19.840 --> 00:35:23.240]   I did work in 1999, this week.
[00:35:23.240 --> 00:35:24.240]   2012.
[00:35:24.240 --> 00:35:25.240]   OK.
[00:35:25.240 --> 00:35:31.000]   But yeah, so they brought me into the morgue, which is a physical place with all these papers.
[00:35:31.000 --> 00:35:35.520]   And it was like a card catalog only times a million.
[00:35:35.520 --> 00:35:41.520]   And just all the-- like all the papers, and this is where I went to research stories.
[00:35:41.520 --> 00:35:42.520]   And I was like--
[00:35:42.520 --> 00:35:43.520]   That's amazing.
[00:35:43.520 --> 00:35:44.520]   --kitting me.
[00:35:44.520 --> 00:35:47.320]   I was like, there will be no context for my stories for the first few years.
[00:35:47.320 --> 00:35:48.320]   [LAUGHTER]
[00:35:48.320 --> 00:35:49.620]   They didn't even have microfiche.
[00:35:49.620 --> 00:35:51.000]   It wasn't even in microfiche.
[00:35:51.000 --> 00:35:52.000]   Was it?
[00:35:52.000 --> 00:35:54.400]   Some of the publications had microfiche.
[00:35:54.400 --> 00:35:55.880]   Some just had morks.
[00:35:55.880 --> 00:35:56.880]   Morks.
[00:35:56.880 --> 00:35:57.880]   Full of papers.
[00:35:57.880 --> 00:35:58.880]   Morks were great.
[00:35:58.880 --> 00:36:00.880]   And they folded them in a certain way.
[00:36:00.880 --> 00:36:01.880]   Yeah.
[00:36:01.880 --> 00:36:02.880]   And they were all basically like business-sized envelopes.
[00:36:02.880 --> 00:36:03.880]   Oh, really?
[00:36:03.880 --> 00:36:04.880]   All these clips in there.
[00:36:04.880 --> 00:36:05.880]   Yeah.
[00:36:05.880 --> 00:36:06.880]   And they're folded up.
[00:36:06.880 --> 00:36:07.880]   And--
[00:36:07.880 --> 00:36:09.320]   So they're not the whole newspaper.
[00:36:09.320 --> 00:36:10.320]   It's a clipping collection.
[00:36:10.320 --> 00:36:11.320]   It's clipping.
[00:36:11.320 --> 00:36:12.320]   Oh, yeah.
[00:36:12.320 --> 00:36:13.320]   No, it's clippings.
[00:36:13.320 --> 00:36:16.240]   And you had people who-- you had three or four people whose job it was every day to
[00:36:16.240 --> 00:36:19.720]   take multiple copies of the paper against multiple tags.
[00:36:19.720 --> 00:36:20.720]   Right?
[00:36:20.720 --> 00:36:22.640]   So this story has to-- is about five politicians.
[00:36:22.640 --> 00:36:24.440]   It's got to be in five folders.
[00:36:24.440 --> 00:36:28.160]   So it's five copies of the paper, cut out, and then marked.
[00:36:28.160 --> 00:36:29.160]   It's--
[00:36:29.160 --> 00:36:31.560]   It's been simply like if you printed Google.
[00:36:31.560 --> 00:36:32.560]   I mean, like that is--
[00:36:32.560 --> 00:36:33.560]   Yeah.
[00:36:33.560 --> 00:36:34.560]   That is the idea.
[00:36:34.560 --> 00:36:36.080]   And someone physically would do that.
[00:36:36.080 --> 00:36:40.160]   We actually used ours as a nursing room, too, which was a little awkward.
[00:36:40.160 --> 00:36:41.160]   But yeah.
[00:36:41.160 --> 00:36:43.760]   Oh, the morgue.
[00:36:43.760 --> 00:36:47.000]   I don't think papers have those anymore, do they?
[00:36:47.000 --> 00:36:48.000]   No, no, they don't.
[00:36:48.000 --> 00:36:49.000]   New York Times has one.
[00:36:49.000 --> 00:36:58.480]   Five to seven million photos going back to the 1970s, including prints and contact sheets.
[00:36:58.480 --> 00:36:59.960]   And they want to digitize it all.
[00:36:59.960 --> 00:37:05.240]   And Jeff, you got to get us in there.
[00:37:05.240 --> 00:37:06.240]   Oh, yeah, that'd be a good idea.
[00:37:06.240 --> 00:37:07.240]   Oh, yeah, we have a sense.
[00:37:07.240 --> 00:37:08.240]   And that would be interesting.
[00:37:08.240 --> 00:37:10.760]   If you visit-- if you let me know when you're coming to New York, you tell the Microsoft
[00:37:10.760 --> 00:37:13.000]   people coming to New York, you don't tell me.
[00:37:13.000 --> 00:37:16.800]   We're coming December 4th, 5th, and 6th.
[00:37:16.800 --> 00:37:18.280]   Or what?
[00:37:18.280 --> 00:37:21.120]   We're going to see Harry Potter and the Cursed Child.
[00:37:21.120 --> 00:37:22.840]   Oh, fun.
[00:37:22.840 --> 00:37:24.560]   And Bruce Springsteen on Broadway.
[00:37:24.560 --> 00:37:26.840]   Oh, wow, you got tickets.
[00:37:26.840 --> 00:37:27.840]   Oh.
[00:37:27.840 --> 00:37:28.840]   There was a lottery.
[00:37:28.840 --> 00:37:30.200]   You're a fancy man, Leo.
[00:37:30.200 --> 00:37:31.200]   I'm a fancy man.
[00:37:31.200 --> 00:37:33.960]   It's the last week of his performances.
[00:37:33.960 --> 00:37:35.240]   So we had to go.
[00:37:35.240 --> 00:37:40.440]   And then I really-- everybody who's seen Harry Potter and the Cursed Child said, this is
[00:37:40.440 --> 00:37:44.160]   the most amazing theatrical experience since "Angels in America."
[00:37:44.160 --> 00:37:45.720]   It's that long as well.
[00:37:45.720 --> 00:37:50.520]   You go to a matinee, then you escape for dinner and a nap, and then you come back for
[00:37:50.520 --> 00:37:53.000]   the evening show and see the rest of it.
[00:37:53.000 --> 00:37:54.000]   Wow.
[00:37:54.000 --> 00:37:55.800]   So, but if I have time, maybe we could figure this out.
[00:37:55.800 --> 00:37:56.800]   I'd love to get--
[00:37:56.800 --> 00:38:00.200]   And the third will have Alan Rusperger being interviewed by Margaret Sullivan here at
[00:38:00.200 --> 00:38:01.200]   the school.
[00:38:01.200 --> 00:38:02.200]   Oh, my oh.
[00:38:02.200 --> 00:38:03.200]   You do seem nice and stuff.
[00:38:03.200 --> 00:38:04.200]   Yeah, from the third.
[00:38:04.200 --> 00:38:06.400]   At that school of journalism of yours, at that little school of yours--
[00:38:06.400 --> 00:38:07.400]   Defraving Newmark.
[00:38:07.400 --> 00:38:08.400]   --gradual school of journalism.
[00:38:08.400 --> 00:38:10.320]   You do some very nice stuff.
[00:38:10.320 --> 00:38:11.320]   Thank you.
[00:38:11.320 --> 00:38:14.480]   Yes, you do.
[00:38:14.480 --> 00:38:21.320]   Google Open Sources, AI algorithms, its researchers can say can distinguish between voices with
[00:38:21.320 --> 00:38:24.800]   92% accuracy.
[00:38:24.800 --> 00:38:30.000]   I don't buy this because it still doesn't know who the heck I am, but that's just me.
[00:38:30.000 --> 00:38:32.000]   Well, you hope it doesn't.
[00:38:32.000 --> 00:38:33.880]   Well, no, my Google home.
[00:38:33.880 --> 00:38:34.880]   I've trained it.
[00:38:34.880 --> 00:38:38.120]   It still doesn't know who I am.
[00:38:38.120 --> 00:38:41.520]   We talk a lot about face recognition, but if you think about it, Google and Apple too
[00:38:41.520 --> 00:38:45.200]   have been collecting samples of people's voices for years now.
[00:38:45.200 --> 00:38:50.880]   As you train these devices, I presume they live somewhere, not in the basement of the
[00:38:50.880 --> 00:38:56.320]   Harold Tribune building, and that they can be used.
[00:38:56.320 --> 00:39:01.160]   And they are presumably what's part of been training all of this stuff.
[00:39:01.160 --> 00:39:09.080]   Google's AI research division has, according to Kyle Wiggers writing in VentureBeat, made
[00:39:09.080 --> 00:39:13.040]   promising progress toward a performance model.
[00:39:13.040 --> 00:39:15.600]   What the hell does that mean?
[00:39:15.600 --> 00:39:16.840]   Yeah, this story.
[00:39:16.840 --> 00:39:20.840]   So I read it thinking it was going to help me understand what was happening.
[00:39:20.840 --> 00:39:21.840]   But it didn't.
[00:39:21.840 --> 00:39:24.360]   But it has lots of pretty pictures.
[00:39:24.360 --> 00:39:28.440]   It has pretty pictures that has some nice things.
[00:39:28.440 --> 00:39:36.960]   It's talking about an unsupervised learning model that helps understand different strings
[00:39:36.960 --> 00:39:38.480]   of language together.
[00:39:38.480 --> 00:39:41.600]   And I don't understand how that helps identify people.
[00:39:41.600 --> 00:39:46.040]   So there was a lot about the story that I was like, I felt like maybe the writer didn't
[00:39:46.040 --> 00:39:47.640]   understand what Google was telling him.
[00:39:47.640 --> 00:39:48.640]   That's probably a case.
[00:39:48.640 --> 00:39:52.840]   Yeah, because I really wanted to understand it, but I couldn't.
[00:39:52.840 --> 00:39:56.960]   Well, this is the library, as everyone knows, for the unbounded interleaved state recurrent
[00:39:56.960 --> 00:39:59.040]   neural network algorithm.
[00:39:59.040 --> 00:40:01.480]   Well, I understand how recurrent neural networks work.
[00:40:01.480 --> 00:40:05.160]   So I felt like I'd get there, but maybe I'll just look up the original paper.
[00:40:05.160 --> 00:40:10.360]   It was originally proposed in a paper called fully supervised speaker diarization.
[00:40:10.360 --> 00:40:11.520]   Is it on archive?
[00:40:11.520 --> 00:40:16.880]   It is Cornell University Library.
[00:40:16.880 --> 00:40:19.480]   I don't know if you can download the whole text of it.
[00:40:19.480 --> 00:40:21.280]   Yes, you can.
[00:40:21.280 --> 00:40:26.080]   In this paper, we propose a fully supervised speaker diarization approach named unbounded
[00:40:26.080 --> 00:40:32.560]   interleaved state recurrent neural networks, given extracted speaker discriminative embeddings
[00:40:32.560 --> 00:40:34.240]   from input utterances.
[00:40:34.240 --> 00:40:39.800]   In other words, your speech, each individual speak was modeled by a parameter sharing
[00:40:39.800 --> 00:40:41.480]   RNN while the RNN said.
[00:40:41.480 --> 00:40:46.840]   So yeah, it sounds like they are distinguishing academics.
[00:40:46.840 --> 00:40:52.800]   The this RNN is naturally integrated with a distance dependent Chinese restaurant process
[00:40:52.800 --> 00:41:00.960]   or DDCRP to accommodate and unknown number speakers.
[00:41:00.960 --> 00:41:01.960]   I don't know.
[00:41:01.960 --> 00:41:04.520]   Really should it be called the distance dependent Chinese restaurant process?
[00:41:04.520 --> 00:41:05.840]   Yeah, well, I don't get that.
[00:41:05.840 --> 00:41:06.840]   Why is that?
[00:41:06.840 --> 00:41:07.920]   I'll make the name.
[00:41:07.920 --> 00:41:09.280]   Our system is what it is.
[00:41:09.280 --> 00:41:11.840]   That is what it is.
[00:41:11.840 --> 00:41:14.840]   I think it's like a problem set.
[00:41:14.840 --> 00:41:16.360]   It's noisy in Chinese restaurants.
[00:41:16.360 --> 00:41:18.240]   I don't know if you've ever been there.
[00:41:18.240 --> 00:41:23.160]   No, I think it's I'm trying to think of what it is.
[00:41:23.160 --> 00:41:24.440]   By the way, it is open source.
[00:41:24.440 --> 00:41:26.840]   We were looking at the GitHub.
[00:41:26.840 --> 00:41:33.800]   But the GitHub does not contain the data code or model because that's Google proprietary.
[00:41:33.800 --> 00:41:43.880]   So you can run a demo, but you won't have the key value to this, which is the data from
[00:41:43.880 --> 00:41:44.880]   Google.
[00:41:44.880 --> 00:41:52.040]   Yeah, I think it's speech recognition walkers would be interesting.
[00:41:52.040 --> 00:41:58.960]   I think one of the big deals was that it was going to be done on a device.
[00:41:58.960 --> 00:42:02.720]   So like it could be done at the edge.
[00:42:02.720 --> 00:42:05.400]   I think was one of the important things here.
[00:42:05.400 --> 00:42:07.200]   That's certainly something we would like.
[00:42:07.200 --> 00:42:09.160]   It's written in Python.
[00:42:09.160 --> 00:42:13.440]   So I imagine it's not too resource demanding.
[00:42:13.440 --> 00:42:15.160]   I want the paper.
[00:42:15.160 --> 00:42:16.800]   Give me the paper.
[00:42:16.800 --> 00:42:18.840]   Cornell has it.
[00:42:18.840 --> 00:42:23.040]   You have to go to the GitHub and then from the GitHub, you can get to the paper.
[00:42:23.040 --> 00:42:25.280]   Registration for .dev begins in January.
[00:42:25.280 --> 00:42:28.480]   Do you want Stacy.dev?
[00:42:28.480 --> 00:42:29.480]   I do not.
[00:42:29.480 --> 00:42:30.480]   Doesn't Google already own this?
[00:42:30.480 --> 00:42:33.480]   They do, but they're going to let you to write.
[00:42:33.480 --> 00:42:35.720]   What was the one we were talking about?
[00:42:35.720 --> 00:42:36.720]   .new.
[00:42:36.720 --> 00:42:37.720]   .new.
[00:42:37.720 --> 00:42:40.720]   So they use that to make your life easier.
[00:42:40.720 --> 00:42:43.640]   This one they're using to let devs do something.
[00:42:43.640 --> 00:42:47.600]   Well, you can let people you know you're a dev.
[00:42:47.600 --> 00:42:49.560]   Awesome.
[00:42:49.560 --> 00:42:55.520]   Corbin Davenport writing for Android police says many developers, including yours truly
[00:42:55.520 --> 00:43:02.640]   prefer to use IO domains for personal sites and projects because of course IO is the abbreviation
[00:43:02.640 --> 00:43:05.800]   for besides being Indian Ocean input output.
[00:43:05.800 --> 00:43:10.960]   Now Google is working on a new ending specifically for developers.dev.
[00:43:10.960 --> 00:43:14.320]   Say one all the IO for the hot startups.
[00:43:14.320 --> 00:43:16.760]   You're either .io or .ai.
[00:43:16.760 --> 00:43:22.840]   Now .app which Google has is now being is also available and you know it'll be in the
[00:43:22.840 --> 00:43:24.840]   change like but I can give it to you a little early.
[00:43:24.840 --> 00:43:28.120]   Squoosh.app is the first use I've seen of that.
[00:43:28.120 --> 00:43:29.120]   And it's a.
[00:43:29.120 --> 00:43:30.600]   I think Kevin owns one of those.
[00:43:30.600 --> 00:43:33.920]   I think it's intended to be PWA apps, right?
[00:43:33.920 --> 00:43:34.920]   Progressive web apps.
[00:43:34.920 --> 00:43:38.160]   This is a progressive web app.
[00:43:38.160 --> 00:43:43.600]   I think Kevin has smart home dot app or smart house dot app or something.
[00:43:43.600 --> 00:43:44.600]   Yeah, nice.
[00:43:44.600 --> 00:43:46.600]   I suppose.
[00:43:46.600 --> 00:43:47.600]   Yes.
[00:43:47.600 --> 00:43:51.440]   Can I'll talk about it in the change like you.
[00:43:51.440 --> 00:43:52.440]   Oh, you.
[00:43:52.440 --> 00:43:53.440]   Oh, oh, it's age.
[00:43:53.440 --> 00:43:55.080]   But you know it's funny.
[00:43:55.080 --> 00:43:57.280]   They had all the other domains they could have used.
[00:43:57.280 --> 00:44:00.880]   No, like since it's a word auto correct new.
[00:44:00.880 --> 00:44:01.880]   Squoosh.
[00:44:01.880 --> 00:44:02.880]   Squoosh.
[00:44:02.880 --> 00:44:07.840]   S Q U double O S H squoosh.
[00:44:07.840 --> 00:44:08.840]   What is a squoosh?
[00:44:08.840 --> 00:44:10.240]   Well, you'll find out state.
[00:44:10.240 --> 00:44:11.680]   Is it a verb?
[00:44:11.680 --> 00:44:13.440]   It is a verb.
[00:44:13.440 --> 00:44:14.440]   Okay.
[00:44:14.440 --> 00:44:16.480]   To squoosh something.
[00:44:16.480 --> 00:44:18.400]   You've been squoosh.
[00:44:18.400 --> 00:44:22.080]   Does that mean to flatten it out like smoosh squoosh it squish it.
[00:44:22.080 --> 00:44:23.080]   Okay.
[00:44:23.080 --> 00:44:24.080]   Yeah.
[00:44:24.080 --> 00:44:27.320]   It could be squished that app, but maybe that was taken.
[00:44:27.320 --> 00:44:30.480]   I don't even know.
[00:44:30.480 --> 00:44:31.480]   Google.
[00:44:31.480 --> 00:44:37.200]   So we all heard that Amazon's moving to now see.
[00:44:37.200 --> 00:44:38.600]   I read one place.
[00:44:38.600 --> 00:44:42.040]   It was Queens and one place it was New York City.
[00:44:42.040 --> 00:44:44.240]   Queens is in New York City.
[00:44:44.240 --> 00:44:45.240]   Is in New York City.
[00:44:45.240 --> 00:44:46.240]   Damn it.
[00:44:46.240 --> 00:44:47.240]   But is it in New York City?
[00:44:47.240 --> 00:44:48.240]   Is it in Manhattan?
[00:44:48.240 --> 00:44:53.560]   No, Queens is a borough outside of the five of the.
[00:44:53.560 --> 00:44:54.560]   Yeah.
[00:44:54.560 --> 00:44:56.480]   Staten Island is not New York City.
[00:44:56.480 --> 00:44:58.200]   The Bronx is in New York City.
[00:44:58.200 --> 00:45:00.200]   New York City is in Manhattan.
[00:45:00.200 --> 00:45:03.960]   The Bronx is probably the most of all the boroughs of New York City, I would think.
[00:45:03.960 --> 00:45:05.720]   No, Manhattan's the most.
[00:45:05.720 --> 00:45:06.720]   Okay, fine.
[00:45:06.720 --> 00:45:07.720]   Right.
[00:45:07.720 --> 00:45:08.720]   Brooklyn.
[00:45:08.720 --> 00:45:09.720]   So those are all boroughs.
[00:45:09.720 --> 00:45:12.760]   In fact, I think we've now gone through all five of them.
[00:45:12.760 --> 00:45:15.120]   If we left anything else.
[00:45:15.120 --> 00:45:17.720]   But they're all that's all greater New York City.
[00:45:17.720 --> 00:45:20.320]   No, it's New York City.
[00:45:20.320 --> 00:45:21.320]   Okay.
[00:45:21.320 --> 00:45:22.320]   I'm just.
[00:45:22.320 --> 00:45:23.320]   New York City.
[00:45:23.320 --> 00:45:24.320]   You live there.
[00:45:24.320 --> 00:45:25.320]   I'm going to defer to you.
[00:45:25.320 --> 00:45:26.320]   You live in Austin.
[00:45:26.320 --> 00:45:27.320]   I don't want to hear.
[00:45:27.320 --> 00:45:28.320]   I mean, if you talk to I used to live in New York.
[00:45:28.320 --> 00:45:29.320]   Oh, you did?
[00:45:29.320 --> 00:45:32.800]   And if you talk to someone who lives in New York, they'll tell you they live in New York.
[00:45:32.800 --> 00:45:35.560]   And then if they live in Manhattan, they won't say they live in New York.
[00:45:35.560 --> 00:45:37.720]   They say they live in Manhattan.
[00:45:37.720 --> 00:45:40.120]   And if you live in Queens, you say, I live in Queens.
[00:45:40.120 --> 00:45:41.280]   You don't say I live in New York City.
[00:45:41.280 --> 00:45:44.480]   No, we see I live in New Jersey and I say I live in New York.
[00:45:44.480 --> 00:45:45.480]   Yeah.
[00:45:45.480 --> 00:45:46.480]   You do not live in New York.
[00:45:46.480 --> 00:45:47.480]   Okay.
[00:45:47.480 --> 00:45:51.760]   How many how many people at this table were born in New York City?
[00:45:51.760 --> 00:45:52.760]   Okay.
[00:45:52.760 --> 00:45:55.400]   Just, just let's get that straight.
[00:45:55.400 --> 00:45:57.200]   You're just a bunch of newcomers.
[00:45:57.200 --> 00:45:58.200]   New Jersey.
[00:45:58.200 --> 00:45:59.200]   New York.
[00:45:59.200 --> 00:46:00.200]   All right.
[00:46:00.200 --> 00:46:05.480]   So, but I think if you live in Staten Island, you don't say I live in New York.
[00:46:05.480 --> 00:46:08.560]   Well, you might to somebody, Hey, I say I live in San Francisco when I'm in Spain.
[00:46:08.560 --> 00:46:10.080]   That's on how far away you are.
[00:46:10.080 --> 00:46:11.080]   Yeah.
[00:46:11.080 --> 00:46:12.080]   Bit well bingo.
[00:46:12.080 --> 00:46:13.080]   Exactly.
[00:46:13.080 --> 00:46:14.080]   Well, anyway, man.
[00:46:14.080 --> 00:46:20.160]   And so long island city chicken capital of the world.
[00:46:20.160 --> 00:46:24.440]   Long Island city is doesn't sound like New York City.
[00:46:24.440 --> 00:46:26.840]   It's on an island, not Manhattan.
[00:46:26.840 --> 00:46:35.840]   But that's where Amazon's going to be HQ to and Nova, North Virginia, right?
[00:46:35.840 --> 00:46:37.840]   A controversial move.
[00:46:37.840 --> 00:46:40.280]   There's actually Arlington and Arlington.
[00:46:40.280 --> 00:46:46.560]   And this is from the Google day one blog, Arlington National Landing in Arlington, Virginia.
[00:46:46.560 --> 00:46:50.400]   How many employees in each of those venues?
[00:46:50.400 --> 00:46:51.400]   A lot.
[00:46:51.400 --> 00:46:56.160]   I forget how many they adding to New York like 50,000.
[00:46:56.160 --> 00:46:57.160]   Let me see what Google says.
[00:46:57.160 --> 00:47:00.040]   They're getting huge tax breaks here, right?
[00:47:00.040 --> 00:47:01.040]   Oh, yeah.
[00:47:01.040 --> 00:47:03.000]   50,000 across the two headquarters.
[00:47:03.000 --> 00:47:04.880]   So I don't know how a New York it'll be.
[00:47:04.880 --> 00:47:07.200]   People are having 25,000.
[00:47:07.200 --> 00:47:14.480]   There was this rich company need all these tax breaks and the L subways already a mess
[00:47:14.480 --> 00:47:20.240]   and wait, there's more Google's the next day announced and we're going to double our employees
[00:47:20.240 --> 00:47:22.840]   in New York City.
[00:47:22.840 --> 00:47:29.120]   So if I were you, I'd get ready for the San Franciscoization of New York.
[00:47:29.120 --> 00:47:30.920]   It would never happen.
[00:47:30.920 --> 00:47:33.080]   New York is too resilient.
[00:47:33.080 --> 00:47:36.600]   I'm serious.
[00:47:36.600 --> 00:47:40.240]   You can bring in a, I mean, they've already got finance bros like they can't handle some
[00:47:40.240 --> 00:47:41.240]   tech bros, please.
[00:47:41.240 --> 00:47:42.240]   Right.
[00:47:42.240 --> 00:47:43.240]   Right.
[00:47:43.240 --> 00:47:44.240]   And what are we going to have?
[00:47:44.240 --> 00:47:46.680]   Are we going to have a special subway, you know, Google bus subway cars?
[00:47:46.680 --> 00:47:47.680]   Yep.
[00:47:47.680 --> 00:47:49.160]   No, you're on the subway with the rest of us.
[00:47:49.160 --> 00:47:55.440]   Elon's going to bore holes underneath the subway for a special sub subway right through
[00:47:55.440 --> 00:47:56.440]   the bedrock.
[00:47:56.440 --> 00:48:02.160]   Oh, I was going to say, we'll go up.
[00:48:02.160 --> 00:48:06.040]   That's 25,000 plus, I think Google's doubling.
[00:48:06.040 --> 00:48:09.440]   So that's another 7,000 32,000 new people.
[00:48:09.440 --> 00:48:11.080]   Most of them will want to live in Brooklyn.
[00:48:11.080 --> 00:48:12.840]   So get ready Brooklyn.
[00:48:12.840 --> 00:48:13.840]   Yep.
[00:48:13.840 --> 00:48:20.000]   I mean, don't, I guess, I mean, when you have a city of 8 million people, I guess another
[00:48:20.000 --> 00:48:21.000]   32,000.
[00:48:21.000 --> 00:48:22.000]   Yeah.
[00:48:22.000 --> 00:48:23.960]   I mean, San Francisco was seven miles by seven miles.
[00:48:23.960 --> 00:48:24.960]   No, it's tiny.
[00:48:24.960 --> 00:48:25.920]   And it can't, it can't grow.
[00:48:25.920 --> 00:48:28.680]   So okay.
[00:48:28.680 --> 00:48:30.240]   Are people excited in Manhattan?
[00:48:30.240 --> 00:48:32.440]   No, it's just another, it's another company.
[00:48:32.440 --> 00:48:33.440]   It's in Queens.
[00:48:33.440 --> 00:48:34.440]   It's a.
[00:48:34.440 --> 00:48:35.440]   That's where we get.
[00:48:35.440 --> 00:48:36.440]   Yes.
[00:48:36.440 --> 00:48:37.440]   That's where we get snobby.
[00:48:37.440 --> 00:48:38.440]   It's in Queens.
[00:48:38.440 --> 00:48:39.960]   You can't say it because you're in California.
[00:48:39.960 --> 00:48:41.960]   I can say it.
[00:48:41.960 --> 00:48:45.320]   It's in Queens.
[00:48:45.320 --> 00:48:46.320]   It's not New York.
[00:48:46.320 --> 00:48:48.760]   It's in Long Island City.
[00:48:48.760 --> 00:48:50.960]   Okay.
[00:48:50.960 --> 00:48:56.320]   And I bet you in Arlington, people are saying it's not in Washington, DC.
[00:48:56.320 --> 00:48:58.880]   It's in Arlington.
[00:48:58.880 --> 00:49:05.840]   I have friend, a startup guy, a founder who lives in Queens now with his wife and kid.
[00:49:05.840 --> 00:49:09.760]   And he was, he's ready now as he was getting ready to, he likes living there.
[00:49:09.760 --> 00:49:12.560]   He wanted to buy a place, but now the prices are going to go berserk.
[00:49:12.560 --> 00:49:13.560]   Yeah.
[00:49:13.560 --> 00:49:14.560]   He knows it.
[00:49:14.560 --> 00:49:17.120]   Isn't Arlington, I'm not as familiar with this area, but Arlington is just over the
[00:49:17.120 --> 00:49:19.400]   Potomac from DC.
[00:49:19.400 --> 00:49:22.920]   So isn't this really going to be affecting DC a little bit too, right?
[00:49:22.920 --> 00:49:23.920]   Oh, yeah.
[00:49:23.920 --> 00:49:24.920]   Yeah.
[00:49:24.920 --> 00:49:29.040]   And there's Amazon, New York and just over the East River for Midtown.
[00:49:29.040 --> 00:49:34.560]   So it's, yeah, it's, it's pretty much New York, right?
[00:49:34.560 --> 00:49:39.440]   So that's a, that's a, that's a, is this, is it, are the cities, all the little cities
[00:49:39.440 --> 00:49:43.880]   that put on the dog and pony show for Jeff Bezos or they pissed.
[00:49:43.880 --> 00:49:47.800]   Well, if you're going to do New York at DC, you should have just told us.
[00:49:47.800 --> 00:49:48.800]   Yeah.
[00:49:48.800 --> 00:49:49.800]   Yeah.
[00:49:49.800 --> 00:49:54.360]   But I don't, there's a lot of anger that it was, it was this bait and switch and they're
[00:49:54.360 --> 00:49:55.360]   getting data.
[00:49:55.360 --> 00:49:57.760]   I don't understand that really.
[00:49:57.760 --> 00:50:02.880]   So the bait and switches, they thought they were going for a headquarters, a second headquarters
[00:50:02.880 --> 00:50:04.520]   as it were.
[00:50:04.520 --> 00:50:10.600]   And then Amazon basically cut that in half and then through their operations facility
[00:50:10.600 --> 00:50:12.680]   thing in Tennessee.
[00:50:12.680 --> 00:50:16.680]   And as I think Dan Prumac put it really well in one of his newsletter, he's like, I'm just
[00:50:16.680 --> 00:50:20.920]   going to call these large satellite offices because you can only have one headquarters.
[00:50:20.920 --> 00:50:31.680]   So I think Amazon was doing a marketing sell for massive operations, basically.
[00:50:31.680 --> 00:50:32.960]   And people are like, yeah, I'll buy into that.
[00:50:32.960 --> 00:50:34.160]   We'll call it the second headquarters.
[00:50:34.160 --> 00:50:35.160]   It'll sound good.
[00:50:35.160 --> 00:50:39.040]   But then when Amazon split it, suddenly it looked a lot less appealing.
[00:50:39.040 --> 00:50:43.840]   So then everyone who tried for it looks kind of stupid, right?
[00:50:43.840 --> 00:50:49.680]   You know who's got who has a good job is the person who finds and builds facilities for
[00:50:49.680 --> 00:50:50.680]   Google.
[00:50:50.680 --> 00:50:53.480]   I think I used to know that guy.
[00:50:53.480 --> 00:50:54.480]   Yeah.
[00:50:54.480 --> 00:50:55.480]   Really?
[00:50:55.480 --> 00:50:56.480]   Yeah.
[00:50:56.480 --> 00:51:03.600]   They just bought the Bruce loose hanger, which is where Howard Hughes built his giant
[00:51:03.600 --> 00:51:06.280]   airplane that only flew once.
[00:51:06.280 --> 00:51:10.160]   It was the largest plane ever built in and it was built out of wood.
[00:51:10.160 --> 00:51:15.280]   And the hanger itself is a beautiful wooden beam structure.
[00:51:15.280 --> 00:51:18.720]   And that's going to be the new Los Angeles office for Google.
[00:51:18.720 --> 00:51:20.360]   Pretty amazing, huh?
[00:51:20.360 --> 00:51:21.360]   Pretty sweet.
[00:51:21.360 --> 00:51:23.440]   This is a drawing.
[00:51:23.440 --> 00:51:29.360]   This is the architects design because it's not built yet, but that is sweet.
[00:51:29.360 --> 00:51:30.600]   And it's beautiful.
[00:51:30.600 --> 00:51:31.600]   Isn't that?
[00:51:31.600 --> 00:51:34.400]   I mean, it looks just like my office, right?
[00:51:34.400 --> 00:51:35.400]   Yeah.
[00:51:35.400 --> 00:51:38.400]   What I love is the building inside the building.
[00:51:38.400 --> 00:51:39.400]   Yeah.
[00:51:39.400 --> 00:51:40.400]   Right.
[00:51:40.400 --> 00:51:41.400]   There's a huge hanger.
[00:51:41.400 --> 00:51:42.400]   You got to respect the hanger.
[00:51:42.400 --> 00:51:43.400]   Yeah.
[00:51:43.400 --> 00:51:44.400]   Yeah.
[00:51:44.400 --> 00:51:45.400]   The intensity.
[00:51:45.400 --> 00:51:46.400]   The heat.
[00:51:46.400 --> 00:51:47.400]   Yeah.
[00:51:47.400 --> 00:51:52.320]   Look, I mean, if these walls are intact, this is really a building inside of hanger.
[00:51:52.320 --> 00:51:53.800]   That's why.
[00:51:53.800 --> 00:51:55.720]   The actual plane is up where now?
[00:51:55.720 --> 00:51:58.480]   Oregon, the Evergreen Aviation Museum.
[00:51:58.480 --> 00:52:02.000]   Oh, I thought I was in the museum of flight.
[00:52:02.000 --> 00:52:04.280]   I don't know why I thought that it should be.
[00:52:04.280 --> 00:52:05.280]   Yeah.
[00:52:05.280 --> 00:52:06.280]   Yeah.
[00:52:06.280 --> 00:52:07.280]   That sucker up to Seattle.
[00:52:07.280 --> 00:52:08.280]   Yeah.
[00:52:08.280 --> 00:52:09.280]   It's a proof.
[00:52:09.280 --> 00:52:12.200]   Which, by the way, yes, best museum in the world.
[00:52:12.200 --> 00:52:13.200]   Maybe not in the world.
[00:52:13.200 --> 00:52:14.200]   Okay.
[00:52:14.200 --> 00:52:15.200]   But it's really an awesome thing.
[00:52:15.200 --> 00:52:17.200]   You haven't even moved yet and already you're becoming a booster.
[00:52:17.200 --> 00:52:18.200]   Oh, hush, hush.
[00:52:18.200 --> 00:52:22.840]   I don't need I've liked that museum for almost a decade.
[00:52:22.840 --> 00:52:24.240]   Anything else great about Seattle?
[00:52:24.240 --> 00:52:25.800]   You want to tell us?
[00:52:25.800 --> 00:52:26.800]   Yes.
[00:52:26.800 --> 00:52:27.800]   The coffee.
[00:52:27.800 --> 00:52:30.240]   I'll go for that.
[00:52:30.240 --> 00:52:32.240]   And I think, yeah.
[00:52:32.240 --> 00:52:36.560]   Oh, it's happening to you already, Stacy.
[00:52:36.560 --> 00:52:37.560]   Jeez.
[00:52:37.560 --> 00:52:39.200]   I have to love something.
[00:52:39.200 --> 00:52:42.920]   I'm so relentlessly positive and sincere about things.
[00:52:42.920 --> 00:52:43.920]   Come on.
[00:52:43.920 --> 00:52:46.320]   I'm going to have to find something I love and talk about it.
[00:52:46.320 --> 00:52:48.280]   Here it's barbecue.
[00:52:48.280 --> 00:52:49.280]   It's y'all.
[00:52:49.280 --> 00:52:50.280]   Okay.
[00:52:50.280 --> 00:52:51.280]   So, okay.
[00:52:51.280 --> 00:52:54.360]   So, will you still say y'all?
[00:52:54.360 --> 00:52:55.920]   Of course I will.
[00:52:55.920 --> 00:52:56.920]   Okay.
[00:52:56.920 --> 00:52:59.720]   What is the best word?
[00:52:59.720 --> 00:53:01.760]   It is the really good word.
[00:53:01.760 --> 00:53:03.640]   All right.
[00:53:03.640 --> 00:53:04.640]   Back to the good.
[00:53:04.640 --> 00:53:07.440]   This is Portlandia South.
[00:53:07.440 --> 00:53:10.640]   Oh, or north north.
[00:53:10.640 --> 00:53:11.640]   North.
[00:53:11.640 --> 00:53:12.640]   That's not that guy.
[00:53:12.640 --> 00:53:14.720]   Well, I think you're talking about awesome.
[00:53:14.720 --> 00:53:17.120]   Petaluma is Portlandia South.
[00:53:17.120 --> 00:53:18.120]   We are kind of like.
[00:53:18.120 --> 00:53:19.120]   Yeah.
[00:53:19.120 --> 00:53:20.120]   Yeah, yeah, yeah.
[00:53:20.120 --> 00:53:21.120]   That's true.
[00:53:21.120 --> 00:53:22.120]   Yeah, that's true.
[00:53:22.120 --> 00:53:26.520]   So, let's talk about the, so this is, people are very upset about this.
[00:53:26.520 --> 00:53:34.200]   And especially in the UK, Google bought a company called DeepMind.
[00:53:34.200 --> 00:53:41.800]   DeepMind is an AI company and apparently had a deal with the National Health Service
[00:53:41.800 --> 00:53:50.400]   in the UK and had promised, the DeepMind Health Division, has promised that the data
[00:53:50.400 --> 00:53:56.040]   that they had from the British Health Service would be kept private, would, you know, would.
[00:53:56.040 --> 00:54:01.400]   But once they were acquired by Google, all bets are off.
[00:54:01.400 --> 00:54:05.200]   The health app developed by DeepMind is going to be taken over by Google.
[00:54:05.200 --> 00:54:06.960]   It's called Streams.
[00:54:06.960 --> 00:54:11.160]   It was first used to send alerts in a London hospital.
[00:54:11.160 --> 00:54:13.320]   But you may remember the headlines.
[00:54:13.320 --> 00:54:18.520]   It had gathered data on 1.6 million patients without telling them.
[00:54:18.520 --> 00:54:23.320]   DeepMind now wants the app to become an AI assistant for nurses and doctors.
[00:54:23.320 --> 00:54:31.800]   One expert, according to the BBC, described the move as "trust demolition."
[00:54:31.800 --> 00:54:34.040]   Stacey, you posted the store.
[00:54:34.040 --> 00:54:37.240]   Actually, I'll give you Julia Poll's tweet.
[00:54:37.240 --> 00:54:38.840]   This is totally unacceptable.
[00:54:38.840 --> 00:54:44.920]   DeepMind repeatedly, unconditionally promised to never connect people's intimate, identifiable
[00:54:44.920 --> 00:54:46.520]   health data to Google.
[00:54:46.520 --> 00:54:49.240]   Now it is announced exactly that.
[00:54:49.240 --> 00:54:51.120]   This isn't transparency.
[00:54:51.120 --> 00:54:54.760]   It's trust demolition.
[00:54:54.760 --> 00:55:00.800]   I put this on here because Jeff and I had such a different reaction to the news in the
[00:55:00.800 --> 00:55:03.000]   first place.
[00:55:03.000 --> 00:55:07.400]   Jeff, you were very like, "Hey, this data could be used for good.
[00:55:07.400 --> 00:55:08.400]   Who needs consent?"
[00:55:08.400 --> 00:55:11.360]   Because the big debate was over at the lack of consent.
[00:55:11.360 --> 00:55:16.320]   I don't know if you recall this, Jeff.
[00:55:16.320 --> 00:55:22.600]   This is what I was worried about, is the idea that once that data is in there, you don't
[00:55:22.600 --> 00:55:24.720]   control where it goes.
[00:55:24.720 --> 00:55:28.240]   In this case, it's going back to Google.
[00:55:28.240 --> 00:55:34.280]   It's so important because you may even feel trust with the company that has the data.
[00:55:34.280 --> 00:55:38.960]   It often happens that these companies get sold and the policies change.
[00:55:38.960 --> 00:55:40.360]   They've got the data.
[00:55:40.360 --> 00:55:41.520]   You gave them the data.
[00:55:41.520 --> 00:55:46.680]   They're not bound often by the promises the previous owners made to you.
[00:55:46.680 --> 00:55:53.760]   Now under GDPR, you could actually get that data to strike the permissions from that data.
[00:55:53.760 --> 00:55:58.080]   Yes, but in this case, no.
[00:55:58.080 --> 00:56:01.680]   I'm curious how GDPR plays into this if it does it all.
[00:56:01.680 --> 00:56:02.680]   I don't know.
[00:56:02.680 --> 00:56:04.400]   I was trying to figure that out before the show.
[00:56:04.400 --> 00:56:05.720]   I don't know either.
[00:56:05.720 --> 00:56:06.720]   Sorry.
[00:56:06.720 --> 00:56:09.040]   That's one of the questions.
[00:56:09.040 --> 00:56:13.120]   This is the problem.
[00:56:13.120 --> 00:56:19.480]   I don't know how we solve for that because absent legislation, because companies have
[00:56:19.480 --> 00:56:23.760]   a vested interest in taking this and bringing this in-house.
[00:56:23.760 --> 00:56:26.520]   We've seen it happen again and again and again.
[00:56:26.520 --> 00:56:29.680]   It's not a question of Google's intentions.
[00:56:29.680 --> 00:56:33.680]   No, we promise we're going to be good with your data.
[00:56:33.680 --> 00:56:37.360]   It's just that the promise was made this data wouldn't be connected to Google and it has
[00:56:37.360 --> 00:56:38.360]   been.
[00:56:38.360 --> 00:56:39.360]   Right.
[00:56:39.360 --> 00:56:44.160]   Google, once it's connected, they have a fiduciary duty to make the best use of their data that
[00:56:44.160 --> 00:56:50.400]   benefits their shareholders, not to me as the person who maybe provided that data.
[00:56:50.400 --> 00:56:57.240]   Now according to the BBC, once this came out last year, the UK's Information Commissioner
[00:56:57.240 --> 00:57:03.760]   ruled the UK Hospital Trust, which was involved in the initial trial, had broken UK privacy
[00:57:03.760 --> 00:57:09.520]   law for failing to tell patients about the way their data was being used.
[00:57:09.520 --> 00:57:14.280]   The Information Commissioner told the BBC had expected that all the measures set out in
[00:57:14.280 --> 00:57:20.760]   its audit to remain in place after the DeepMind health moves to Google.
[00:57:20.760 --> 00:57:26.080]   An independent review panel set up to scrutinize DeepMind's relationship to the NHS was unlikely
[00:57:26.080 --> 00:57:31.560]   to continue in its current form, given the US takeover of the health division.
[00:57:31.560 --> 00:57:36.480]   In the words, they're deciding, "It doesn't matter anymore because Google's a American
[00:57:36.480 --> 00:57:37.480]   company."
[00:57:37.480 --> 00:57:40.400]   But hold on here.
[00:57:40.400 --> 00:57:47.560]   So the DeepMind response to the BBC, "Patient data remains under our NHS partner strict control
[00:57:47.560 --> 00:57:51.080]   and all decisions about its use will continue to lie with them.
[00:57:51.080 --> 00:57:53.760]   The move to Google does not affect this."
[00:57:53.760 --> 00:57:59.320]   So basically what they're complaining about is now that data is next to a company with
[00:57:59.320 --> 00:58:01.520]   kudos in their view.
[00:58:01.520 --> 00:58:04.560]   The issue here is what's the use and what's the harm.
[00:58:04.560 --> 00:58:11.040]   And just because it happens to be, then the issue is the terms of the use of the data,
[00:58:11.040 --> 00:58:13.280]   the issue is, "Oh, it's near Google.
[00:58:13.280 --> 00:58:15.720]   That's what I'm not understanding this story.
[00:58:15.720 --> 00:58:21.240]   I'm not understanding where the break of trust is there.
[00:58:21.240 --> 00:58:24.720]   I don't get it.
[00:58:24.720 --> 00:58:30.360]   The breach of trust is that this data wasn't supposed to go to Google, the DeepMind data,
[00:58:30.360 --> 00:58:35.800]   and also it's unclear what inferences were made from that data that could then be put
[00:58:35.800 --> 00:58:36.800]   back under Google."
[00:58:36.800 --> 00:58:41.520]   Jeff, where did you see them say, "Oh, it's still not going to be covered?"
[00:58:41.520 --> 00:58:50.160]   The BBC story, which you have, under the embedded tweet.
[00:58:50.160 --> 00:58:55.840]   Oh, in response, DeepMind told the BBC patient data is under our control.
[00:58:55.840 --> 00:59:00.920]   The issue here is not, I mean, DeepMind could have been bought by anybody.
[00:59:00.920 --> 00:59:02.000]   Could have bought by Amazon.
[00:59:02.000 --> 00:59:03.880]   Could have bought by anybody.
[00:59:03.880 --> 00:59:06.200]   It's not that, "Oh, it could have bought by Rupert Murdoch.
[00:59:06.200 --> 00:59:07.800]   Can I be screaming?"
[00:59:07.800 --> 00:59:11.600]   But the issue is not the ownership of the company.
[00:59:11.600 --> 00:59:14.440]   The issue is the terms on the data.
[00:59:14.440 --> 00:59:19.280]   And just because Google owns the company doesn't mean that evil has now occurred.
[00:59:19.280 --> 00:59:24.560]   But they're using that data in plan to do, they're planning to take the information and
[00:59:24.560 --> 00:59:31.640]   the inferences, derive from that information and put it into other products and practices.
[00:59:31.640 --> 00:59:33.120]   What's the harm of that?
[00:59:33.120 --> 00:59:36.880]   It's unclear when you take those inferences.
[00:59:36.880 --> 00:59:41.920]   It's unclear how divorced those really are from individuals' data.
[00:59:41.920 --> 00:59:42.920]   I got to do a time.
[00:59:42.920 --> 00:59:46.840]   And some people don't want their individual data in there at all, and that should be their
[00:59:46.840 --> 00:59:47.840]   right.
[00:59:47.840 --> 00:59:50.440]   This is not the same argument we had it before, Stacey.
[00:59:50.440 --> 00:59:52.440]   But this is knowledge.
[00:59:52.440 --> 00:59:55.920]   This is knowledge gained from information.
[00:59:55.920 --> 01:00:04.040]   And in a modern society, you do not control and own knowledge.
[01:00:04.040 --> 01:00:12.240]   And if I looked at that data with my eyes or with AI, no difference, I said, "Oh, wow,
[01:00:12.240 --> 01:00:20.960]   people who use hippie water bottles are 10% more likely to get kidney disease."
[01:00:20.960 --> 01:00:23.920]   So now you're saying, "Oh, you can't know that, Jeff.
[01:00:23.920 --> 01:00:24.920]   No, no, no, no.
[01:00:24.920 --> 01:00:30.120]   You're not going to warn people using hippie water bottles that they might be in danger
[01:00:30.120 --> 01:00:31.120]   here."
[01:00:31.120 --> 01:00:32.640]   No, no, no, because my data was in that pool.
[01:00:32.640 --> 01:00:34.520]   And I don't want you to use it for that.
[01:00:34.520 --> 01:00:39.480]   So you can't save anybody from getting kidney disease who uses hippie water bottles.
[01:00:39.480 --> 01:00:42.280]   And for those of you on audio, I reach for the nearest prop.
[01:00:42.280 --> 01:00:44.280]   He has a hippie water bottle.
[01:00:44.280 --> 01:00:45.280]   He does.
[01:00:45.280 --> 01:00:46.280]   He has a bottle.
[01:00:46.280 --> 01:00:48.280]   It's just not a plastic water bottle.
[01:00:48.280 --> 01:00:49.280]   I think it'd be a little different.
[01:00:49.280 --> 01:00:50.280]   I understand what you're saying, Jeff.
[01:00:50.280 --> 01:00:53.800]   It'd be a little different if the initial breach of trust hasn't occurred.
[01:00:53.800 --> 01:00:59.040]   In other words, the NSA had, without permission, handed over all this data in the first place
[01:00:59.040 --> 01:01:00.040]   to deep mine.
[01:01:00.040 --> 01:01:03.600]   And deep mine was now, has already been told that was illegal.
[01:01:03.600 --> 01:01:06.360]   And there already be under supervision.
[01:01:06.360 --> 01:01:09.600]   And so there is kind of this built-in, well, we're not sure we trust you.
[01:01:09.600 --> 01:01:15.080]   You already breached our trust once, although you might see it as the NHS's fault, not the
[01:01:15.080 --> 01:01:16.080]   insurance fault.
[01:01:16.080 --> 01:01:19.120]   But you could also flip your argument.
[01:01:19.120 --> 01:01:20.120]   Right.
[01:01:20.120 --> 01:01:24.280]   You could also flip your argument, though, instead of saving people from getting cancer
[01:01:24.280 --> 01:01:28.720]   from hippie water bottles, what if we say we're charging people who have the following
[01:01:28.720 --> 01:01:32.520]   characteristics 100% more for insurance?
[01:01:32.520 --> 01:01:35.360]   And then people might be like, "Well, I don't want to contribute to that.
[01:01:35.360 --> 01:01:36.960]   That's terrible."
[01:01:36.960 --> 01:01:43.520]   So there's always that argument that this will be used in a malign way as opposed to
[01:01:43.520 --> 01:01:44.520]   it anyway.
[01:01:44.520 --> 01:01:45.520]   Yes.
[01:01:45.520 --> 01:01:49.800]   I'm just pointing out that Jeff, your argument was made in the most, you know, I was just
[01:01:49.800 --> 01:01:52.360]   flipping your argument so people had to be opposite.
[01:01:52.360 --> 01:01:53.360]   Yeah, I'm still going to stay on my side.
[01:01:53.360 --> 01:01:54.360]   I know.
[01:01:54.360 --> 01:01:55.360]   I know.
[01:01:55.360 --> 01:01:56.360]   I was still staying on my side of the fence.
[01:01:56.360 --> 01:01:57.360]   I still believe.
[01:01:57.360 --> 01:01:58.360]   I'm over here.
[01:01:58.360 --> 01:01:59.360]   I'm over here with my little couple of people.
[01:01:59.360 --> 01:02:02.960]   It's going to make a difference in the lives of millions of patients around the world.
[01:02:02.960 --> 01:02:05.080]   Google said that.
[01:02:05.080 --> 01:02:06.080]   And you know what?
[01:02:06.080 --> 01:02:09.080]   I do believe that this kind of data is really useful.
[01:02:09.080 --> 01:02:12.720]   And that's why I'm so glad that there are a lot of people who are creating like data
[01:02:12.720 --> 01:02:18.280]   donation, like medical data donation kind of programs where like I can say, "Hey, this
[01:02:18.280 --> 01:02:19.280]   is my information.
[01:02:19.280 --> 01:02:22.800]   I want to share this with the program and make the world a better place."
[01:02:22.800 --> 01:02:31.520]   This is kind of the continuation of my continued complaint about Google, which is that nobody
[01:02:31.520 --> 01:02:36.600]   would have this complaint wouldn't be as vigorous if it weren't that Google is also
[01:02:36.600 --> 01:02:38.560]   using data to sell advertising.
[01:02:38.560 --> 01:02:44.600]   And so if it were a different company that had, you know, that wasn't trying to use data
[01:02:44.600 --> 01:02:46.880]   to sell advertising, it would be easier.
[01:02:46.880 --> 01:02:51.880]   But it's the same reason I don't like Google going buying YouTube because they do search
[01:02:51.880 --> 01:02:57.280]   and so there's this potential for them favoring people who use YouTube as opposed to people
[01:02:57.280 --> 01:02:58.360]   who don't.
[01:02:58.360 --> 01:03:06.240]   And that's where Google gets into this position by being so big and all encompassing.
[01:03:06.240 --> 01:03:08.280]   And people get nervous about them.
[01:03:08.280 --> 01:03:09.280]   Jeff, don't you?
[01:03:09.280 --> 01:03:11.840]   I mean, that's not an unreasonable thing to...
[01:03:11.840 --> 01:03:14.640]   But again, Leo, but nervousness is not...
[01:03:14.640 --> 01:03:16.760]   Yeah, I don't like big companies.
[01:03:16.760 --> 01:03:17.760]   I don't like American companies.
[01:03:17.760 --> 01:03:18.760]   No, it's not that I don't like big companies.
[01:03:18.760 --> 01:03:23.200]   That's not something that makes a company that makes a living selling personal data to
[01:03:23.200 --> 01:03:27.800]   advertisers to be the one store of this medical information.
[01:03:27.800 --> 01:03:33.640]   Give me the actual harm and balance that with the potential benefit.
[01:03:33.640 --> 01:03:35.640]   We can't just say we can't...
[01:03:35.640 --> 01:03:38.160]   I'm trying to protect that there's no benefit.
[01:03:38.160 --> 01:03:39.160]   Knowledge.
[01:03:39.160 --> 01:03:40.160]   I can't hear there's potential benefit.
[01:03:40.160 --> 01:03:44.160]   I'd be a lot more sanguine about it though, if it weren't a company that's selling our
[01:03:44.160 --> 01:03:48.040]   data for advertising purposes, that that's not their primary business.
[01:03:48.040 --> 01:03:51.160]   If it were some other company buying this, if we're IBM buying...
[01:03:51.160 --> 01:03:54.160]   NHS has control of the use of data.
[01:03:54.160 --> 01:03:58.080]   NHS ain't going to allow it to be used to sell you...
[01:03:58.080 --> 01:04:00.520]   Ah, I would not...
[01:04:00.520 --> 01:04:01.520]   It's you, guys.
[01:04:01.520 --> 01:04:06.440]   No, those people in hospitals, I mean, look at what's happening with Memorial Sloan Kettering
[01:04:06.440 --> 01:04:09.200]   and the way they sell their name to pharmaceuticals.
[01:04:09.200 --> 01:04:14.480]   Well, and it's the fact that the National Health Service in England gave this data without
[01:04:14.480 --> 01:04:17.280]   asking patients just gave this data.
[01:04:17.280 --> 01:04:19.280]   And I'm going to come back to...
[01:04:19.280 --> 01:04:20.280]   I'm going to come...
[01:04:20.280 --> 01:04:23.160]   Sorry, folks, I'm going to defend the enlightenment.
[01:04:23.160 --> 01:04:26.160]   Knowledge.
[01:04:26.160 --> 01:04:33.880]   The fact that if you start cutting off conclusions from information, and if you let everyone who
[01:04:33.880 --> 01:04:40.000]   has any part in producing any bit of data, you cut off the ability of conclusions to be
[01:04:40.000 --> 01:04:44.760]   made from that data, you're cutting off education, science, and the friggin enlightenment.
[01:04:44.760 --> 01:04:45.760]   Stop.
[01:04:45.760 --> 01:04:47.680]   It's not that we're against that, Jeff.
[01:04:47.680 --> 01:04:51.800]   It's we're against Google being the sole one doing this.
[01:04:51.800 --> 01:04:52.800]   Wait, wait.
[01:04:52.800 --> 01:04:53.800]   No, no, no, no, no.
[01:04:53.800 --> 01:04:54.800]   Okay, hold on, Jeff.
[01:04:54.800 --> 01:04:55.800]   This is showing data.
[01:04:55.800 --> 01:05:03.160]   And I would also argue it is our job to think about as we bring this, as we bring connectivity
[01:05:03.160 --> 01:05:08.560]   and scalability to like searches and digitize everybody's records, it is incumbent upon
[01:05:08.560 --> 01:05:13.640]   us to think about the harms that can come from that, not just the benefits.
[01:05:13.640 --> 01:05:18.320]   Because if we don't, someone else will, and we will find ourselves in the way that we
[01:05:18.320 --> 01:05:22.480]   have found ourselves with like Facebook and Cambridge Analytica, that is exactly why Facebook
[01:05:22.480 --> 01:05:27.720]   is on trial right now in public opinion, not in the actual legal systems.
[01:05:27.720 --> 01:05:33.840]   So I'm just saying that we have to, I mean, it may sound like techno panic, which I'm very
[01:05:33.840 --> 01:05:36.960]   proud of you that you have not said that once, but.
[01:05:36.960 --> 01:05:38.960]   Thank you for saying it for me.
[01:05:38.960 --> 01:05:39.960]   Because it does.
[01:05:39.960 --> 01:05:43.000]   I know, I know.
[01:05:43.000 --> 01:05:45.200]   But we have to, Jeff, that's our job.
[01:05:45.200 --> 01:05:46.200]   We can't.
[01:05:46.200 --> 01:05:49.240]   We can also have to balance that conversation with.
[01:05:49.240 --> 01:05:50.240]   We do.
[01:05:50.240 --> 01:05:51.240]   And that's why.
[01:05:51.240 --> 01:05:55.120]   And be rather than going to an emotional argument, I don't like Google, I trust Google, which
[01:05:55.120 --> 01:06:00.720]   is what Leo was saying, instead, then deal with the actual harm and the actual cures for
[01:06:00.720 --> 01:06:01.720]   that.
[01:06:01.720 --> 01:06:02.720]   So the potential.
[01:06:02.720 --> 01:06:03.720]   What are you trying to prevent?
[01:06:03.720 --> 01:06:04.720]   What are you?
[01:06:04.720 --> 01:06:08.800]   The potential for harm is that this data can be traced back to individuals in ways we don't
[01:06:08.800 --> 01:06:10.320]   understand now.
[01:06:10.320 --> 01:06:15.560]   There's also the potential for harm in that data being used to disadvantage certain classes
[01:06:15.560 --> 01:06:18.560]   of individuals just as much as it helps them.
[01:06:18.560 --> 01:06:21.520]   So those are the two largest potential harms I see.
[01:06:21.520 --> 01:06:24.920]   There's also potential for harm if it's not secured correctly and that sort of thing.
[01:06:24.920 --> 01:06:25.920]   But hopefully the NHS.
[01:06:25.920 --> 01:06:27.720]   There I absolutely agree.
[01:06:27.720 --> 01:06:33.120]   But what I am saying is the fact that this ends up in an app used in a hospital to help
[01:06:33.120 --> 01:06:35.600]   diagnose sick people.
[01:06:35.600 --> 01:06:39.280]   I have, I'm sorry, but I'm going to defend that.
[01:06:39.280 --> 01:06:43.160]   And that doesn't mean that the data that was contributed six steps back to the line of
[01:06:43.160 --> 01:06:44.960]   an AI is at risk.
[01:06:44.960 --> 01:06:51.800]   You could argue that the Tuskegee experiments helped diagnose and cure sick people.
[01:06:51.800 --> 01:06:54.880]   But those were morally reprehensible.
[01:06:54.880 --> 01:07:00.160]   And you could argue that when you start looking at data, hold on, when you start looking at
[01:07:00.160 --> 01:07:05.920]   data and pulling data in aggregate into something, it can be used in ways that are just as morally
[01:07:05.920 --> 01:07:07.400]   reprehensible.
[01:07:07.400 --> 01:07:10.080]   And that's what we have a duty to consider.
[01:07:10.080 --> 01:07:18.400]   So the problem there is, as Leo said, it's, it's, you know, who Tuskegee or NHS is at that
[01:07:18.400 --> 01:07:20.400]   point.
[01:07:20.400 --> 01:07:22.280]   And there are, I'm not arguing involved.
[01:07:22.280 --> 01:07:23.280]   There are protections needed.
[01:07:23.280 --> 01:07:24.280]   I'm not arguing involved.
[01:07:24.280 --> 01:07:28.800]   I mean, why do we have the systems we now have in universities and hospitals and around
[01:07:28.800 --> 01:07:30.920]   medicine around how experiments are done?
[01:07:30.920 --> 01:07:31.920]   Absolutely agreed.
[01:07:31.920 --> 01:07:32.920]   Those are issues.
[01:07:32.920 --> 01:07:35.600]   So maybe we need like an IRB for.
[01:07:35.600 --> 01:07:36.600]   Yeah.
[01:07:36.600 --> 01:07:38.760]   Well, I think this is showing us.
[01:07:38.760 --> 01:07:39.760]   Up rates under.
[01:07:39.760 --> 01:07:40.760]   Right.
[01:07:40.760 --> 01:07:46.040]   Which we should actually have updated IRBs and understand the implications from this.
[01:07:46.040 --> 01:07:47.040]   And that's what we're talking about.
[01:07:47.040 --> 01:07:48.040]   So IRB.
[01:07:48.040 --> 01:07:49.920]   Internal review board.
[01:07:49.920 --> 01:07:56.520]   So if you're doing experiments on people or human re thank you, you may have to do
[01:07:56.520 --> 01:07:57.520]   it for animals.
[01:07:57.520 --> 01:08:00.160]   You have to have a reviewed for impact and instead of.
[01:08:00.160 --> 01:08:04.800]   So why do you think Google bought DeepMind?
[01:08:04.800 --> 01:08:10.760]   Because they want unsupervised learning because they want to save the world.
[01:08:10.760 --> 01:08:11.760]   Did they buy it?
[01:08:11.760 --> 01:08:15.000]   Because they're a profit making institution that knows that data is.
[01:08:15.000 --> 01:08:18.040]   You can see you could keep both thoughts in your head at the same time.
[01:08:18.040 --> 01:08:21.640]   I don't think Google wants to save the world.
[01:08:21.640 --> 01:08:25.040]   Google's a business is a profit making enterprise.
[01:08:25.040 --> 01:08:28.840]   They have a fiduciary due to their shareholders that they will back those problems.
[01:08:28.840 --> 01:08:30.480]   Google does anything because they want to save the world.
[01:08:30.480 --> 01:08:34.520]   I think Larry and Sergey might have some, you know, projects they think will save the
[01:08:34.520 --> 01:08:35.520]   world.
[01:08:35.520 --> 01:08:39.280]   But honestly, the company I think organizing the world's knowledge and making it accessible
[01:08:39.280 --> 01:08:41.120]   is a version of saving the world.
[01:08:41.120 --> 01:08:42.120]   It is a good thing.
[01:08:42.120 --> 01:08:45.440]   That damn Gutenberg only wanted to do is make money.
[01:08:45.440 --> 01:08:46.440]   Cheese.
[01:08:46.440 --> 01:08:49.640]   Oh, you know, you can do both.
[01:08:49.640 --> 01:08:53.320]   You can do something that's good for the world and you can make money.
[01:08:53.320 --> 01:08:55.000]   That is true.
[01:08:55.000 --> 01:09:00.600]   But you can also find those at odds in certain situations and when they're at odds, do you
[01:09:00.600 --> 01:09:03.280]   go for profit making or do you go for saving the world?
[01:09:03.280 --> 01:09:09.240]   And a company you tend to go for profit making, which might be a good time to talk about Google
[01:09:09.240 --> 01:09:10.240]   in China.
[01:09:10.240 --> 01:09:14.440]   Oh, I thought you were going to say our final advertiser express VPN and then we'll talk
[01:09:14.440 --> 01:09:15.440]   about Google and China.
[01:09:15.440 --> 01:09:16.440]   Oh, OK.
[01:09:16.440 --> 01:09:18.240]   I'm going to make the money.
[01:09:18.240 --> 01:09:20.240]   I make the money.
[01:09:20.240 --> 01:09:22.240]   Speaking of profits.
[01:09:22.240 --> 01:09:25.040]   You see what the forward here.
[01:09:25.040 --> 01:09:26.040]   Huh?
[01:09:26.040 --> 01:09:28.040]   I'm the captain now.
[01:09:28.040 --> 01:09:33.320]   We'll get back to the conversation.
[01:09:33.320 --> 01:09:34.680]   It's always a great conversation.
[01:09:34.680 --> 01:09:39.520]   Jeff Jarvis from CUNY and Buzzmachine.com.
[01:09:39.520 --> 01:09:44.720]   And of course, the great Stacey Hingham, bathroomstacey.io.com our show today brought to you by a company
[01:09:44.720 --> 01:09:47.760]   that does want to protect your privacy.
[01:09:47.760 --> 01:09:49.360]   Express VPN.
[01:09:49.360 --> 01:09:55.680]   We talk all the time about security breaches, ISPs selling your data to the highest bidder.
[01:09:55.680 --> 01:09:56.840]   You are being tracked.
[01:09:56.840 --> 01:10:02.360]   We know you are by social media sites marketing marketing companies, your ISPs.
[01:10:02.360 --> 01:10:03.960]   They can record your browsing history.
[01:10:03.960 --> 01:10:07.400]   They often sell it to other corporations that want to profit free information.
[01:10:07.400 --> 01:10:12.200]   You can hide yourself online with Express VPN.
[01:10:12.200 --> 01:10:15.920]   Easy to use apps that run seamlessly in the background of your computer, your phone,
[01:10:15.920 --> 01:10:16.920]   your tablet.
[01:10:16.920 --> 01:10:19.440]   You turn them on with one click and boom, you're private.
[01:10:19.440 --> 01:10:20.840]   You're privatized.
[01:10:20.840 --> 01:10:27.560]   It secures and anonymizes your browsing by encrypting your data and hiding your personal
[01:10:27.560 --> 01:10:28.560]   IP address.
[01:10:28.560 --> 01:10:32.040]   Suddenly, you're emerging on the internet from Express VPN servers.
[01:10:32.040 --> 01:10:34.720]   They have servers all over the world, by the way.
[01:10:34.720 --> 01:10:40.680]   You could surf safely on public Wi-Fi without being hacked or having your data stolen.
[01:10:40.680 --> 01:10:44.720]   Number one VPN service, a credit tech radar, and it comes with a 30-day money-back guarantee
[01:10:44.720 --> 01:10:48.080]   that price is right, too, less than $7 a month.
[01:10:48.080 --> 01:10:49.960]   And of course, no logging.
[01:10:49.960 --> 01:10:51.120]   Read their privacy policy.
[01:10:51.120 --> 01:10:54.680]   They do everything right at Express VPN or we wouldn't mention them.
[01:10:54.680 --> 01:10:57.640]   If you don't want to hand over your online history to your internet service provider
[01:10:57.640 --> 01:11:06.400]   or data resellers, Express VPN, rock solid privacy at blazing speed, no compromises.
[01:11:06.400 --> 01:11:07.760]   Protect your online activity today.
[01:11:07.760 --> 01:11:12.160]   Find out how you can get three extra months free when you buy a one-year package at express
[01:11:12.160 --> 01:11:14.720]   vpn.com/twig.
[01:11:14.720 --> 01:11:16.920]   Express VPN.com/twig.
[01:11:16.920 --> 01:11:28.480]   We thank them for their support of this week in Google.
[01:11:28.480 --> 01:11:30.120]   What did you want to talk about, Stacey?
[01:11:30.120 --> 01:11:34.160]   What was the China, Google and China?
[01:11:34.160 --> 01:11:36.360]   Mr. Page, you're muted.
[01:11:36.360 --> 01:11:37.360]   Sorry.
[01:11:37.360 --> 01:11:39.080]   I didn't do it.
[01:11:39.080 --> 01:11:40.480]   I swear I didn't do it.
[01:11:40.480 --> 01:11:41.480]   I know.
[01:11:41.480 --> 01:11:42.480]   No one.
[01:11:42.480 --> 01:11:44.480]   There's no remote mute.
[01:11:44.480 --> 01:11:47.800]   No, it was just the justification for Google and China.
[01:11:47.800 --> 01:11:49.640]   I was just going with our theme.
[01:11:49.640 --> 01:11:53.000]   But we could also talk about other things, like the change log.
[01:11:53.000 --> 01:11:54.880]   Oh, it's time.
[01:11:54.880 --> 01:11:56.880]   Thank you for mentioning.
[01:11:56.880 --> 01:11:57.880]   It's time for the--
[01:11:57.880 --> 01:11:58.880]   There's just so much.
[01:11:58.880 --> 01:11:59.880]   There's so much.
[01:11:59.880 --> 01:12:05.880]   We better get to the Google change log.
[01:12:05.880 --> 01:12:11.480]   The log that's so big, it even leaked out into the other parts of the show.
[01:12:11.480 --> 01:12:14.960]   For instance, this was a story not in the change log.
[01:12:14.960 --> 01:12:18.120]   Android will automatically turn on your lights when you turn off your alarm.
[01:12:18.120 --> 01:12:23.600]   Two new features for the Google Assistant on your Android device.
[01:12:23.600 --> 01:12:28.360]   So the latest version of the clock app by Google has this, the latest version of the
[01:12:28.360 --> 01:12:30.640]   Google search app.
[01:12:30.640 --> 01:12:37.880]   And if you have, of course, Google Assistant, you can program the Assistant to report on
[01:12:37.880 --> 01:12:41.000]   your commute when you turn off the alarm, turn up the temperature.
[01:12:41.000 --> 01:12:46.840]   You can ask you to play a podcast when your alarm goes off some music, lights, thermostat,
[01:12:46.840 --> 01:12:49.200]   coffee maker, whatever it is.
[01:12:49.200 --> 01:12:51.360]   All of this from sub menus within the clock app.
[01:12:51.360 --> 01:12:54.560]   So look in the clock app and you'll see all of those.
[01:12:54.560 --> 01:12:59.640]   And if you have a Pixel 3 phone, you know you can set do not disturb the mute incoming
[01:12:59.640 --> 01:13:01.840]   calls or alerts.
[01:13:01.840 --> 01:13:05.080]   I actually have it set by schedule, so at 10 p.m. to 7 a.m.
[01:13:05.080 --> 01:13:06.680]   I don't want any calls.
[01:13:06.680 --> 01:13:10.960]   But soon you'll be able to tell your Assistant, "Hey, you know, I'm not going to tell you
[01:13:10.960 --> 01:13:16.160]   who's silenced my phone and do not disturb will be turned on."
[01:13:16.160 --> 01:13:18.400]   But this is not an all pixels.
[01:13:18.400 --> 01:13:20.200]   So I thought my phone was broken.
[01:13:20.200 --> 01:13:24.720]   And of course, a little bit of me was looking for an excuse to buy a Pixel 3 because I really
[01:13:24.720 --> 01:13:28.680]   was not too, but I thought, "Oh, no, my phone is broken.
[01:13:28.680 --> 01:13:30.400]   How disturbing."
[01:13:30.400 --> 01:13:34.240]   Because it wouldn't ring for like two weeks.
[01:13:34.240 --> 01:13:37.440]   And somehow I had turned on, do not disturb, but I don't even know how I turned it on.
[01:13:37.440 --> 01:13:40.560]   And finally, just as I was ready, I had to buy button.
[01:13:40.560 --> 01:13:41.880]   Oh, crap.
[01:13:41.880 --> 01:13:42.880]   It's not broken.
[01:13:42.880 --> 01:13:44.560]   I've actually got to stir it was on.
[01:13:44.560 --> 01:13:47.560]   Experimented with leaving, do not disturb on 24/7.
[01:13:47.560 --> 01:13:48.560]   It was kind of nice.
[01:13:48.560 --> 01:13:49.560]   I can't do that all the time.
[01:13:49.560 --> 01:13:54.360]   Because you can say, "I only want people in my contacts to call or I only want people
[01:13:54.360 --> 01:13:56.360]   in my favorites to call."
[01:13:56.360 --> 01:13:58.720]   And that's really the only calls you want to get, right?
[01:13:58.720 --> 01:14:00.880]   My wife was not happy with me.
[01:14:00.880 --> 01:14:01.880]   Yeah.
[01:14:01.880 --> 01:14:03.640]   So you need to add her to that.
[01:14:03.640 --> 01:14:04.640]   She can go.
[01:14:04.640 --> 01:14:06.800]   Well, I just, oh yeah, turn off to that stir.
[01:14:06.800 --> 01:14:09.640]   But I think that that's an interesting way to run.
[01:14:09.640 --> 01:14:10.640]   Then your phone doesn't bother you.
[01:14:10.640 --> 01:14:13.640]   I used to have mine on, do not disturb.
[01:14:13.640 --> 01:14:15.840]   Oh, no, never mind.
[01:14:15.840 --> 01:14:16.840]   Ignore me.
[01:14:16.840 --> 01:14:17.840]   Just ignore me.
[01:14:17.840 --> 01:14:18.840]   I wish I had it.
[01:14:18.840 --> 01:14:20.440]   I leave it on, do not disturb all the time.
[01:14:20.440 --> 01:14:22.640]   Yeah, mine's on, do not disturb right now.
[01:14:22.640 --> 01:14:23.640]   Yeah.
[01:14:23.640 --> 01:14:26.360]   The ticket bathroom is not available for comment.
[01:14:26.360 --> 01:14:27.360]   Yes.
[01:14:27.360 --> 01:14:30.640]   "Files by Google is being updated.
[01:14:30.640 --> 01:14:34.920]   The file management app for Android.
[01:14:34.920 --> 01:14:43.600]   Files Go came last year and they're now renaming it to just files.
[01:14:43.600 --> 01:14:45.160]   So Files Go is now files.
[01:14:45.160 --> 01:14:49.240]   They've redesigned the user interface.
[01:14:49.240 --> 01:14:53.760]   And there's a little celebration.
[01:14:53.760 --> 01:14:55.960]   When you clear out files, should I try it?
[01:14:55.960 --> 01:14:58.360]   I don't know if I have the newest one on here.
[01:14:58.360 --> 01:15:00.480]   Let's see if it's called files.
[01:15:00.480 --> 01:15:02.240]   Instead of files go.
[01:15:02.240 --> 01:15:06.560]   Yeah, I guess I have the new one.
[01:15:06.560 --> 01:15:08.360]   So is this it?
[01:15:08.360 --> 01:15:10.640]   So when you delete things, you get to celebrate it.
[01:15:10.640 --> 01:15:12.400]   Because Google hates to let me delete stuff.
[01:15:12.400 --> 01:15:13.400]   I'm always like, please, Google.
[01:15:13.400 --> 01:15:14.880]   I want to get rid of this.
[01:15:14.880 --> 01:15:15.880]   It's this files.
[01:15:15.880 --> 01:15:16.880]   Okay.
[01:15:16.880 --> 01:15:19.360]   Junk files, select and free up three or four hundred, three, four.
[01:15:19.360 --> 01:15:20.880]   Should I just delete these?
[01:15:20.880 --> 01:15:25.880]   Let's just delete these all, delete these junk files, clear them all out.
[01:15:25.880 --> 01:15:26.880]   Let's see the celebration.
[01:15:26.880 --> 01:15:28.880]   Well, that wasn't much.
[01:15:28.880 --> 01:15:31.080]   Well, that was very exciting.
[01:15:31.080 --> 01:15:32.080]   Let's do more.
[01:15:32.080 --> 01:15:34.640]   Let's do more duplicate files.
[01:15:34.640 --> 01:15:36.760]   Those are all you know, it didn't do nothing.
[01:15:36.760 --> 01:15:38.120]   Oh, there's a little guy.
[01:15:38.120 --> 01:15:39.760]   A little thumb came out.
[01:15:39.760 --> 01:15:40.760]   A little thumb.
[01:15:40.760 --> 01:15:41.760]   Even more.
[01:15:41.760 --> 01:15:42.760]   I can't stand the excitement.
[01:15:42.760 --> 01:15:43.760]   Oh, it's exciting.
[01:15:43.760 --> 01:15:45.320]   Hey, look, this is the change log.
[01:15:45.320 --> 01:15:48.120]   It doesn't have to be exciting to leading.
[01:15:48.120 --> 01:15:49.720]   It's got to be worthy of trumpets.
[01:15:49.720 --> 01:15:53.560]   We're going to say for you for.
[01:15:53.560 --> 01:15:58.320]   We'll now celebrate how much you save by telling you what you freed up room for.
[01:15:58.320 --> 01:16:00.240]   So I don't know what that means.
[01:16:00.240 --> 01:16:03.440]   Oh, I need that in my like calorie counting apps or my exercises.
[01:16:03.440 --> 01:16:05.000]   You lost weight.
[01:16:05.000 --> 01:16:06.000]   Congratulations.
[01:16:06.000 --> 01:16:07.400]   Now you can eat a piece of cake.
[01:16:07.400 --> 01:16:09.560]   Oh, that would be good.
[01:16:09.560 --> 01:16:11.720]   You've just earned a piece of cake.
[01:16:11.720 --> 01:16:15.240]   I think there used to be something like that out there.
[01:16:15.240 --> 01:16:16.520]   Project Fi is getting smart.
[01:16:16.520 --> 01:16:18.720]   You're a project Fi customer, Jeff.
[01:16:18.720 --> 01:16:19.720]   Yes, I am.
[01:16:19.720 --> 01:16:21.200]   Are you Stacey as well?
[01:16:21.200 --> 01:16:22.720]   Um, no.
[01:16:22.720 --> 01:16:25.200]   One of the things they're doing, we just did an express VPN ad.
[01:16:25.200 --> 01:16:33.320]   But if you're a Fi subscriber, you'll have access now to 2 million Wi-Fi hotspots that
[01:16:33.320 --> 01:16:35.960]   have VPN built in.
[01:16:35.960 --> 01:16:41.040]   And they're extending VPN now to all your connections, which is very cool.
[01:16:41.040 --> 01:16:47.040]   So I'm not sure how you turn that on.
[01:16:47.040 --> 01:16:50.560]   When you're on the go, you're likely connecting a new network.
[01:16:50.560 --> 01:16:52.880]   And the security of all the networks isn't made equal.
[01:16:52.880 --> 01:16:57.040]   Ah, when you enable our enhanced network, all of your mobile and Wi-Fi traffic will be
[01:16:57.040 --> 01:17:01.960]   encrypted and securely sent through our virtual private network on every network you connect
[01:17:01.960 --> 01:17:03.440]   to.
[01:17:03.440 --> 01:17:05.600]   That's a good thing.
[01:17:05.600 --> 01:17:06.680]   Faster connections now.
[01:17:06.680 --> 01:17:13.000]   In fact, they've, I always assumed that the way Fi worked was it went to the strongest
[01:17:13.000 --> 01:17:18.360]   signal, but apparently that that wasn't how it worked or it's something they're going
[01:17:18.360 --> 01:17:24.200]   to do that's a little bit differently, but they are now going to switch carriers based
[01:17:24.200 --> 01:17:29.000]   on the strength of your connection.
[01:17:29.000 --> 01:17:32.000]   I thought they were already doing that, but now they're going to do that.
[01:17:32.000 --> 01:17:35.960]   So there's some updates to project Fi, which by the way, will not be called project Fi
[01:17:35.960 --> 01:17:36.960]   in the future.
[01:17:36.960 --> 01:17:38.920]   It'll just be Google Fi.
[01:17:38.920 --> 01:17:39.920]   No more project.
[01:17:39.920 --> 01:17:41.280]   It's not a project anymore.
[01:17:41.280 --> 01:17:42.280]   It's a thing.
[01:17:42.280 --> 01:17:44.040]   It's a thing.
[01:17:44.040 --> 01:17:49.040]   It's a live.
[01:17:49.040 --> 01:17:50.040]   It's a live.
[01:17:50.040 --> 01:17:51.200]   Actually, Chrome 71's practically alive.
[01:17:51.200 --> 01:17:57.240]   It will warn you about unclear billing charges and subscription signups.
[01:17:57.240 --> 01:18:01.000]   It will show a warning before pages that don't provide sufficient clarity to the end user
[01:18:01.000 --> 01:18:03.280]   about a transaction.
[01:18:03.280 --> 01:18:07.920]   They've quantified best practices for making a visible and clearly explainable billing
[01:18:07.920 --> 01:18:09.040]   process.
[01:18:09.040 --> 01:18:11.480]   This is especially true on Chrome on mobile.
[01:18:11.480 --> 01:18:16.280]   Don't people realize when they have to give their credit card information?
[01:18:16.280 --> 01:18:20.880]   You were saying, "Here's you would think, here's you might get this warning.
[01:18:20.880 --> 01:18:25.200]   The page ahead may try to charge you money.
[01:18:25.200 --> 01:18:28.760]   If you don't want this, go back."
[01:18:28.760 --> 01:18:29.760]   Wow.
[01:18:29.760 --> 01:18:30.760]   Wow.
[01:18:30.760 --> 01:18:37.080]   Something like that would be really effective for privacy policies or data, non-encrypted
[01:18:37.080 --> 01:18:38.080]   data notification.
[01:18:38.080 --> 01:18:41.460]   They kind of do that now, right, where they warn you if you're on a page that's not
[01:18:41.460 --> 01:18:42.460]   necessarily fully encrypted.
[01:18:42.460 --> 01:18:48.400]   Which, by the way, I mean my Facebook portal portals.
[01:18:48.400 --> 01:18:49.400]   Yeah.
[01:18:49.400 --> 01:18:52.000]   Well, your IRC is not secure according to Chrome.
[01:18:52.000 --> 01:18:53.000]   It isn't.
[01:18:53.000 --> 01:18:54.840]   Is there some reason you want it to be?
[01:18:54.840 --> 01:18:57.160]   No, I'm just saying that they do that.
[01:18:57.160 --> 01:19:03.240]   It would be interesting if they scanned all of the Ulas for devices or something.
[01:19:03.240 --> 01:19:08.920]   If I were going to go buy something or googling a Nest thermostat and it said, "By the way,
[01:19:08.920 --> 01:19:12.300]   this does not follow traditional encryption practice."
[01:19:12.300 --> 01:19:15.180]   They do, by the way.
[01:19:15.180 --> 01:19:18.380]   They gave you a basic warning and plain English about something like that.
[01:19:18.380 --> 01:19:22.020]   Lots of reviews this morning of Google's new night site feature.
[01:19:22.020 --> 01:19:23.860]   It's finally rolling out to phones.
[01:19:23.860 --> 01:19:26.660]   This was something Google touted when they had asked a Pixel 3.
[01:19:26.660 --> 01:19:30.220]   But it didn't make available when you first got it.
[01:19:30.220 --> 01:19:33.180]   I haven't received the update yet.
[01:19:33.180 --> 01:19:35.620]   But apparently it just does an amazing job.
[01:19:35.620 --> 01:19:38.660]   All the reviewers are just blown away by how good stuff looks.
[01:19:38.660 --> 01:19:40.660]   Yes, I still don't want to.
[01:19:40.660 --> 01:19:42.500]   I keep going to looking.
[01:19:42.500 --> 01:19:44.800]   It is taking 15 images.
[01:19:44.800 --> 01:19:48.340]   Some of them are slow as one quarter of a second shutter speed.
[01:19:48.340 --> 01:19:51.580]   Obviously with motion, it's not going to be able to use that one if something's moving.
[01:19:51.580 --> 01:19:59.540]   But it can tell if you're on a tripod, it can get very low frame rates to get very high
[01:19:59.540 --> 01:20:01.580]   quality images late at night.
[01:20:01.580 --> 01:20:03.220]   Look at the difference there.
[01:20:03.220 --> 01:20:04.220]   So keep your eye peeled.
[01:20:04.220 --> 01:20:05.220]   That's awesome.
[01:20:05.220 --> 01:20:07.020]   If you have a Pixel 3, it will work best.
[01:20:07.020 --> 01:20:12.020]   But they are going to migrate this to the camera apps on both Pixel 2, Jeff and Pixel.
[01:20:12.020 --> 01:20:15.660]   But Pixel 3 is the one that is smartest about this.
[01:20:15.660 --> 01:20:20.100]   It's got the extra juice in there and the extra processor.
[01:20:20.100 --> 01:20:22.460]   So to say the dedicated processor?
[01:20:22.460 --> 01:20:23.460]   Yes.
[01:20:23.460 --> 01:20:24.940]   Well, but remember the Pixel 2 had that too.
[01:20:24.940 --> 01:20:27.100]   I guess it's just a better one.
[01:20:27.100 --> 01:20:28.900]   That's pretty amazing what it's doing.
[01:20:28.900 --> 01:20:29.900]   It does.
[01:20:29.900 --> 01:20:30.900]   Yeah.
[01:20:30.900 --> 01:20:33.620]   Lots of posts about this.
[01:20:33.620 --> 01:20:40.020]   But I think this is for Stacey, create your smart home assistant for the holidays.
[01:20:40.020 --> 01:20:42.180]   I don't know, making a great meal.
[01:20:42.180 --> 01:20:43.180]   I don't know.
[01:20:43.180 --> 01:20:44.980]   This is maybe just ad copy.
[01:20:44.980 --> 01:20:47.060]   Setting up daily habits as a family.
[01:20:47.060 --> 01:20:48.900]   Yeah, this is ad copy.
[01:20:48.900 --> 01:20:49.900]   Yeah.
[01:20:49.900 --> 01:20:50.900]   Turn your living room into a...
[01:20:50.900 --> 01:20:52.820]   You do all this already.
[01:20:52.820 --> 01:20:57.060]   I do this and it's not even my birthday, my holidays.
[01:20:57.060 --> 01:21:00.500]   Tomorrow, Stacey, the guys are coming to our house to put up the lights.
[01:21:00.500 --> 01:21:01.700]   I'm very excited.
[01:21:01.700 --> 01:21:03.020]   Oh, congratulations.
[01:21:03.020 --> 01:21:05.620]   Do you have your outdoor smart things or did you say that?
[01:21:05.620 --> 01:21:06.620]   I bought that one.
[01:21:06.620 --> 01:21:08.860]   I bought that one you from I, whatever the you mentioned.
[01:21:08.860 --> 01:21:09.860]   Oh, good.
[01:21:09.860 --> 01:21:10.860]   I devices.
[01:21:10.860 --> 01:21:11.860]   Excellent.
[01:21:11.860 --> 01:21:14.460]   I got six of them.
[01:21:14.460 --> 01:21:15.860]   So they're all over the place.
[01:21:15.860 --> 01:21:21.380]   And then I guess I gather, I'll be able to, with my phone or my voice, say, turn on quadrant
[01:21:21.380 --> 01:21:22.380]   one.
[01:21:22.380 --> 01:21:23.380]   Yes.
[01:21:23.380 --> 01:21:26.180]   So here's my pro tip for you and anyone else doing this.
[01:21:26.180 --> 01:21:32.260]   When you label your outlets, I label them with tape because if you move, you want to remove
[01:21:32.260 --> 01:21:37.580]   it, but label it whatever you call it in your Google or Madam A app.
[01:21:37.580 --> 01:21:42.860]   So I have upper balcony or balcony, roof, snow globe and whatever else I've got, Christmas
[01:21:42.860 --> 01:21:44.700]   tree.
[01:21:44.700 --> 01:21:49.460]   And that way you can only have to add them once and then they'll, when you take them
[01:21:49.460 --> 01:21:52.340]   out, you know, next year, you just plug them in and they work.
[01:21:52.340 --> 01:21:53.340]   Put them in the right spot.
[01:21:53.340 --> 01:21:54.340]   Yeah.
[01:21:54.340 --> 01:21:55.820]   I was like, but plug them in in this right spot.
[01:21:55.820 --> 01:21:56.820]   Yeah.
[01:21:56.820 --> 01:21:59.380]   No, I got my brother, label maker, ready to go.
[01:21:59.380 --> 01:22:00.380]   So there you go.
[01:22:00.380 --> 01:22:01.380]   Yep.
[01:22:01.380 --> 01:22:04.700]   I just don't know yet what those will be.
[01:22:04.700 --> 01:22:08.940]   I think, and then I also haven't yet paired them my phones, but I can go, they're Bluetooth.
[01:22:08.940 --> 01:22:11.340]   So I go next to it and it'll pair, go next to it.
[01:22:11.340 --> 01:22:15.100]   And so I can name them at that time and do the label maker and all that.
[01:22:15.100 --> 01:22:19.700]   And then I'll know what those like, we have, we have a giant Santa.
[01:22:19.700 --> 01:22:23.540]   We have no very Christmas for the garage.
[01:22:23.540 --> 01:22:24.540]   Test it.
[01:22:24.540 --> 01:22:26.900]   I forgot that they're Bluetooth because they work with home kit.
[01:22:26.900 --> 01:22:27.900]   Right.
[01:22:27.900 --> 01:22:28.900]   Yeah.
[01:22:28.900 --> 01:22:32.220]   Test it first and make sure the range isn't going to be a problem.
[01:22:32.220 --> 01:22:33.940]   Like they're too close to each other?
[01:22:33.940 --> 01:22:36.740]   No, they're too far away to be remote controlled.
[01:22:36.740 --> 01:22:39.260]   Oh, I don't know how big your yard is.
[01:22:39.260 --> 01:22:42.660]   That would be annoying if I had to go around the house to turn them on.
[01:22:42.660 --> 01:22:43.660]   Yes.
[01:22:43.660 --> 01:22:45.260]   That would be the entire purpose.
[01:22:45.260 --> 01:22:49.220]   But if so they're not Wi-Fi too, they're just Bluetooth.
[01:22:49.220 --> 01:22:50.940]   I guess so.
[01:22:50.940 --> 01:22:51.940]   I don't know.
[01:22:51.940 --> 01:22:52.940]   That's a bummer.
[01:22:52.940 --> 01:22:56.460]   I have Z-Wave ones, but I don't recommend those to normal people because nobody wants
[01:22:56.460 --> 01:22:58.620]   to buy a hub, which I totally get.
[01:22:58.620 --> 01:23:03.420]   It says I can do Siri, Echo or Google Assistant, but I guess it's my amplitude.
[01:23:03.420 --> 01:23:04.420]   Then their Wi-Fi.
[01:23:04.420 --> 01:23:06.100]   Well, I don't know.
[01:23:06.100 --> 01:23:07.100]   Hold on.
[01:23:07.100 --> 01:23:08.100]   I'll do it.
[01:23:08.100 --> 01:23:09.100]   It's Wi-Fi.
[01:23:09.100 --> 01:23:10.100]   It says Wi-Fi.
[01:23:10.100 --> 01:23:11.100]   Wi-Fi, good.
[01:23:11.100 --> 01:23:12.980]   So I was thinking it's Bluetooth when you pair, right?
[01:23:12.980 --> 01:23:16.300]   You pair it, but then you give it a name, you configure it.
[01:23:16.300 --> 01:23:17.300]   Yes.
[01:23:17.300 --> 01:23:18.540]   Wi-Fi offers smart blood.
[01:23:18.540 --> 01:23:19.540]   Oh, thank God.
[01:23:19.540 --> 01:23:24.500]   I was like, I bought 300 dollars of them, Stacey, on your recommendation.
[01:23:24.500 --> 01:23:26.300]   I was about to be very disappointed.
[01:23:26.300 --> 01:23:31.100]   Well, that one was like, wait, I've tried these a year and a half ago.
[01:23:31.100 --> 01:23:35.860]   So they worked a year and a half ago in my house, but I'm like, when you said Bluetooth,
[01:23:35.860 --> 01:23:37.700]   I'm like, wait, what?
[01:23:37.700 --> 01:23:41.940]   I'm going to say, hey, Echo, light up the garage.
[01:23:41.940 --> 01:23:43.980]   The garage is going to light up.
[01:23:43.980 --> 01:23:44.980]   Light up the portico.
[01:23:44.980 --> 01:23:45.980]   Yes, it's going to.
[01:23:45.980 --> 01:23:48.580]   Well, no, what you're going to do is you're going to group all of them.
[01:23:48.580 --> 01:23:50.180]   So then you're going to create a group.
[01:23:50.180 --> 01:23:51.180]   Yeah.
[01:23:51.180 --> 01:23:52.180]   And then turn on once.
[01:23:52.180 --> 01:23:53.180]   Turn on once.
[01:23:53.180 --> 01:23:56.940]   So you're going to do a particularly twig-ish with Twittish.
[01:23:56.940 --> 01:23:57.940]   Christmas.
[01:23:57.940 --> 01:23:58.940]   This one.
[01:23:58.940 --> 01:23:59.940]   It's Twittmas.
[01:23:59.940 --> 01:24:00.940]   Can I?
[01:24:00.940 --> 01:24:05.500]   Well, so I don't know why we're doing this.
[01:24:05.500 --> 01:24:10.020]   We've never decorated the house before, but I got the plans out for the house so I could
[01:24:10.020 --> 01:24:16.260]   figure out how much roofline I had, 413 feet.
[01:24:16.260 --> 01:24:19.460]   And then I doubled it just in case.
[01:24:19.460 --> 01:24:23.300]   And then Lisa then she said, and I wanted to go up the side of that thing and down the
[01:24:23.300 --> 01:24:26.820]   thing and around the bushes and over the river and through the woods.
[01:24:26.820 --> 01:24:29.100]   So we got more and we got big balls.
[01:24:29.100 --> 01:24:30.100]   We got stars.
[01:24:30.100 --> 01:24:31.100]   We got a sign.
[01:24:31.100 --> 01:24:34.180]   We got a Santa that goes, oh, oh, oh.
[01:24:34.180 --> 01:24:39.540]   And then all of, can I have all of that tied to one action?
[01:24:39.540 --> 01:24:44.540]   Or I say, turn on the celebration.
[01:24:44.540 --> 01:24:50.340]   If they're all, if they're all talk to echo, all the devices can talk to echo, then yes.
[01:24:50.340 --> 01:24:52.460]   They're all on those switches.
[01:24:52.460 --> 01:24:53.460]   Yes.
[01:24:53.460 --> 01:24:55.940]   So then all of those switches, you'll label them.
[01:24:55.940 --> 01:25:01.340]   You'll group each one into a room or into a, yeah, I would group it into a room.
[01:25:01.340 --> 01:25:05.620]   And if you group it into a room and the room's called like Christmas or Twittmas, then you
[01:25:05.620 --> 01:25:07.260]   just say, turn on Twittmas.
[01:25:07.260 --> 01:25:08.260]   Turn on Twittmas.
[01:25:08.260 --> 01:25:09.260]   Yeah.
[01:25:09.260 --> 01:25:10.260]   Yeah.
[01:25:10.260 --> 01:25:12.820]   You might want to test Twittmas to make sure she understands that.
[01:25:12.820 --> 01:25:14.820]   What are you saying?
[01:25:14.820 --> 01:25:16.940]   What are you saying here?
[01:25:16.940 --> 01:25:18.100]   I'm just, I'm just offering.
[01:25:18.100 --> 01:25:21.740]   I have, I have some poorly named things that I'm like, oh, I need to read.
[01:25:21.740 --> 01:25:25.740]   Because I, we have something called the nano leaves and I call it nano art.
[01:25:25.740 --> 01:25:26.740]   And she hates it.
[01:25:26.740 --> 01:25:28.740]   She keeps giving me definitions for a part.
[01:25:28.740 --> 01:25:31.900]   I swear to God.
[01:25:31.900 --> 01:25:34.820]   I'm like, please.
[01:25:34.820 --> 01:25:38.860]   So I need, I need to reach, I need to change.
[01:25:38.860 --> 01:25:48.540]   And Google Chrome Labs has released its open source browser based image optimization tool
[01:25:48.540 --> 01:25:49.540]   squoosh.
[01:25:49.540 --> 01:25:53.820]   Don't be a figure minute to figure it out, but it's kind of fun.
[01:25:53.820 --> 01:25:57.380]   It supports a wide variety of formats.
[01:25:57.380 --> 01:26:03.780]   It can, it can show you the original and the, if you've, so the key to go to install this
[01:26:03.780 --> 01:26:05.620]   is, and this is kind of new too.
[01:26:05.620 --> 01:26:10.740]   It's, you go to a website squoosh app and if you go on a mobile device, depends on the
[01:26:10.740 --> 01:26:15.100]   device, if you go on a mobile device, you can actually put this on your screen as if
[01:26:15.100 --> 01:26:16.860]   it were an app.
[01:26:16.860 --> 01:26:21.060]   The idea is you, you drag and drop or select an image.
[01:26:21.060 --> 01:26:24.540]   You can take a photo of you and look at the quality and they'll squish it.
[01:26:24.540 --> 01:26:28.100]   And then once it squishes it, it'll show you.
[01:26:28.100 --> 01:26:32.180]   So over the right, over on the right, Leo, go to quality and move that to the far to
[01:26:32.180 --> 01:26:33.180]   the left.
[01:26:33.180 --> 01:26:34.940]   Oh, you can degrade it.
[01:26:34.940 --> 01:26:35.940]   It's degrade the quality.
[01:26:35.940 --> 01:26:36.940]   Wait a minute.
[01:26:36.940 --> 01:26:37.940]   So it does it.
[01:26:37.940 --> 01:26:38.940]   Yeah.
[01:26:38.940 --> 01:26:41.260]   It's now you'll see degraded quality and not so much.
[01:26:41.260 --> 01:26:44.100]   Well, let's turn all the way down.
[01:26:44.100 --> 01:26:46.380]   Zero quality squoosh.
[01:26:46.380 --> 01:26:47.860]   It's 94% smaller.
[01:26:47.860 --> 01:26:50.540]   But see, this is why squoosh is so good.
[01:26:50.540 --> 01:26:51.700]   Yes, it degraded.
[01:26:51.700 --> 01:26:52.700]   I put a picture up.
[01:26:52.700 --> 01:26:53.700]   Can you see the degree?
[01:26:53.700 --> 01:26:54.700]   Yeah, I see that.
[01:26:54.700 --> 01:26:55.700]   Look at the whiskers.
[01:26:55.700 --> 01:27:00.140]   The way there's, it's more blocky, but still that's pretty good, right?
[01:27:00.140 --> 01:27:01.140]   It's pretty good.
[01:27:01.140 --> 01:27:02.140]   So that was great.
[01:27:02.140 --> 01:27:03.140]   Choose different compression.
[01:27:03.140 --> 01:27:04.140]   Bye.
[01:27:04.140 --> 01:27:06.900]   WebP browser ping.
[01:27:06.900 --> 01:27:08.860]   You can, this is a good tool to learn.
[01:27:08.860 --> 01:27:09.860]   Like what's going to do?
[01:27:09.860 --> 01:27:10.860]   What?
[01:27:10.860 --> 01:27:14.660]   I'm going to do browser ping resize reduce palette.
[01:27:14.660 --> 01:27:19.580]   Oh, it's big browser ping is big.
[01:27:19.580 --> 01:27:23.020]   Mo's JPEG was really small about browser JPEG.
[01:27:23.020 --> 01:27:26.300]   Now I can really let's get it down there.
[01:27:26.300 --> 01:27:27.300]   317k.
[01:27:27.300 --> 01:27:28.300]   That's pretty good.
[01:27:28.300 --> 01:27:29.620]   So, and then you could save the image out.
[01:27:29.620 --> 01:27:35.900]   So this is a really handy way to have an app on your mobile.
[01:27:35.900 --> 01:27:43.220]   This is completely cross platform or on your desktop to do this, which is cool.
[01:27:43.220 --> 01:27:46.260]   You like?
[01:27:46.260 --> 01:27:48.020]   This is squoosh.app.
[01:27:48.020 --> 01:27:49.660]   Squoosh.app.
[01:27:49.660 --> 01:27:55.700]   It'll also do a s it'll do.
[01:27:55.700 --> 01:27:57.220]   This is an SVG.
[01:27:57.220 --> 01:28:00.460]   I could do opti ping.
[01:28:00.460 --> 01:28:03.100]   Oh, it's bigger.
[01:28:03.100 --> 01:28:04.900]   So then I also went into the resize.
[01:28:04.900 --> 01:28:05.900]   Yeah.
[01:28:05.900 --> 01:28:07.060]   And I resized it down.
[01:28:07.060 --> 01:28:11.300]   I'm trying to understand why the quality went down so much.
[01:28:11.300 --> 01:28:12.300]   I don't get it.
[01:28:12.300 --> 01:28:18.060]   Well, if you make it smaller, you can't make a tiny little icon 20 by 28.
[01:28:18.060 --> 01:28:19.060]   Right.
[01:28:19.060 --> 01:28:21.180]   So I made it into a very small image.
[01:28:21.180 --> 01:28:22.180]   Yeah.
[01:28:22.180 --> 01:28:23.180]   And then it would vector image.
[01:28:23.180 --> 01:28:24.180]   That's kind of basically blown up.
[01:28:24.180 --> 01:28:32.300]   So teach you a little bit about images and image formats and quality and stuff.
[01:28:32.300 --> 01:28:33.500]   Is this cool?
[01:28:33.500 --> 01:28:37.460]   So is this mainly just a demo of .app?
[01:28:37.460 --> 01:28:42.700]   I would say it's a demo of progressive web apps because I saved it out as an icon onto
[01:28:42.700 --> 01:28:46.140]   my iPhone and Android phone and it works on both.
[01:28:46.140 --> 01:28:47.780]   It's pretty cool.
[01:28:47.780 --> 01:28:49.820]   Squoosh.
[01:28:49.820 --> 01:28:52.980]   And that I think unless what else, Carson, you put a lot of things in here.
[01:28:52.980 --> 01:28:56.140]   I'm not going to do them all because there's which one do you like?
[01:28:56.140 --> 01:28:58.140]   Any of them?
[01:28:58.140 --> 01:29:01.380]   She hates them all in a change lock.
[01:29:01.380 --> 01:29:02.380]   Yeah.
[01:29:02.380 --> 01:29:04.620]   Well, we put a lot in there just to make sure that we covered everything.
[01:29:04.620 --> 01:29:06.860]   But I think a lot of this we don't have to pick you got it.
[01:29:06.860 --> 01:29:07.860]   Yeah.
[01:29:07.860 --> 01:29:10.780]   I mean, you don't care about emojis on the G board, do you?
[01:29:10.780 --> 01:29:11.780]   No.
[01:29:11.780 --> 01:29:13.780]   Maybe we give you a suggestion.
[01:29:13.780 --> 01:29:14.780]   No, you got it.
[01:29:14.780 --> 01:29:15.780]   Got it.
[01:29:15.780 --> 01:29:16.780]   Cue the music, Carson.
[01:29:16.780 --> 01:29:17.780]   Cue the music.
[01:29:17.780 --> 01:29:18.780]   Wait.
[01:29:18.780 --> 01:29:19.780]   Wait.
[01:29:19.780 --> 01:29:24.020]   Oh, gosh, we came close to doing emojis.
[01:29:24.020 --> 01:29:25.580]   Phew, phew.
[01:29:25.580 --> 01:29:27.940]   Well, dodge the bullet right there.
[01:29:27.940 --> 01:29:31.980]   Hey, we mentioned that the times is digitizing, but you know, I have to give kudos to the art
[01:29:31.980 --> 01:29:34.140]   institute of Chicago.
[01:29:34.140 --> 01:29:41.180]   They have put a big chunk of their collection online at their website, 50,000 images and
[01:29:41.180 --> 01:29:42.180]   high res.
[01:29:42.180 --> 01:29:45.220]   I mean, you can see brush strokes.
[01:29:45.220 --> 01:29:47.980]   Ooh, that's what we should have squished.
[01:29:47.980 --> 01:29:49.580]   We should have squished those.
[01:29:49.580 --> 01:29:50.980]   Huh?
[01:29:50.980 --> 01:29:53.300]   This is really, really cool.
[01:29:53.300 --> 01:29:58.420]   So this is part of a new website for the art institute, Chicago.
[01:29:58.420 --> 01:30:04.700]   But if you like images, this is a good article actually at their website.
[01:30:04.700 --> 01:30:10.340]   It's A-R-T-I-C dot E-D-U art institute, Chicago.
[01:30:10.340 --> 01:30:14.460]   But if you want to, if you, and they're, and one of the things they're doing is highlighting
[01:30:14.460 --> 01:30:17.100]   lesser known, everybody knows about American Gothic, right?
[01:30:17.100 --> 01:30:21.540]   But they're also highlighting some lesser known, oh, actually, oh, I clicked the wrong
[01:30:21.540 --> 01:30:22.540]   one.
[01:30:22.540 --> 01:30:23.540]   I don't care about American Gothic.
[01:30:23.540 --> 01:30:24.980]   Let's get the bedroom by Vincent Van Gogh.
[01:30:24.980 --> 01:30:31.580]   Because, you know, every time I see a Van Gogh, I want to, I want to zoom in and really see
[01:30:31.580 --> 01:30:33.580]   it and really kind of get it.
[01:30:33.580 --> 01:30:34.900]   Check out those brush strokes.
[01:30:34.900 --> 01:30:37.820]   Check out those brush strokes, right?
[01:30:37.820 --> 01:30:39.980]   So this is really interesting.
[01:30:39.980 --> 01:30:44.140]   They talk about it for, you know, just to putting it as your desktop.
[01:30:44.140 --> 01:30:52.380]   But at, and this is a trend in the high end, like, CBIA home, home space, space, home electronic
[01:30:52.380 --> 01:30:59.260]   space is televisions that act as picture screens or devices like the mural.
[01:30:59.260 --> 01:31:05.540]   And so having access to this, because a lot of those have proprietary art that you subscribe
[01:31:05.540 --> 01:31:06.620]   to.
[01:31:06.620 --> 01:31:10.780]   But if you could just point your, you know, frame device, your digital frame device,
[01:31:10.780 --> 01:31:15.740]   that, you know, a rotating image of from the Chicago Museum of Art, that's actually really
[01:31:15.740 --> 01:31:17.940]   kind of a neat thing to have in your house.
[01:31:17.940 --> 01:31:18.940]   It turns.
[01:31:18.940 --> 01:31:23.180]   Do you think some of the, I mean, some museums are loathe to do this because they don't want
[01:31:23.180 --> 01:31:25.340]   people to steal their imagery, right?
[01:31:25.340 --> 01:31:29.340]   Like, you know, I think most museums are there for education.
[01:31:29.340 --> 01:31:36.460]   And there is something very different when you go in person to see art than just seeing
[01:31:36.460 --> 01:31:37.460]   a reproduction of it.
[01:31:37.460 --> 01:31:39.060]   And this is still a reproduction of it.
[01:31:39.060 --> 01:31:42.220]   So I don't think this, but it is nice.
[01:31:42.220 --> 01:31:46.020]   But not everybody can get to Chicago and really see this.
[01:31:46.020 --> 01:31:48.940]   And I love seeing, you know, Monet's brush strokes.
[01:31:48.940 --> 01:31:50.700]   I mean, I just love this stuff.
[01:31:50.700 --> 01:31:54.220]   And when I go to a museum, it's one of the things I immediately do is walk as close to
[01:31:54.220 --> 01:31:56.180]   the picture as Elle or sculpture.
[01:31:56.180 --> 01:31:57.180]   I'll let you.
[01:31:57.180 --> 01:32:02.420]   So let me and really kind of like scope it out and try to imagine standing there next
[01:32:02.420 --> 01:32:04.180]   to Toulouse-Lautrec as he paints it.
[01:32:04.180 --> 01:32:05.940]   I mean, that's part of the fun of it.
[01:32:05.940 --> 01:32:07.700]   And you can kind of almost do that with these.
[01:32:07.700 --> 01:32:08.980]   You really can zoom in.
[01:32:08.980 --> 01:32:10.740]   It's really neat.
[01:32:10.740 --> 01:32:12.180]   I wish you could zoom in a little bit more.
[01:32:12.180 --> 01:32:17.660]   Obviously, they used a probably pretty high quality camera, but I would love to go in
[01:32:17.660 --> 01:32:19.380]   two or three times more than this even.
[01:32:19.380 --> 01:32:21.100]   But still very nice.
[01:32:21.100 --> 01:32:22.980]   50,000 images.
[01:32:22.980 --> 01:32:29.420]   arctic.edu, our Institute of Chicago.
[01:32:29.420 --> 01:32:31.060]   I'm a little nervous.
[01:32:31.060 --> 01:32:32.580]   I don't know what I should be nervous about.
[01:32:32.580 --> 01:32:34.500]   This Pandora, you know, got started.
[01:32:34.500 --> 01:32:39.140]   Tim Westergren started Pandora because he did the music genome project in which he tried
[01:32:39.140 --> 01:32:45.300]   to categorize music by the, you know, not just by the instruments in it, but by the field,
[01:32:45.300 --> 01:32:46.460]   the style.
[01:32:46.460 --> 01:32:49.500]   And that's how Pandora got started with this music recommendation engine.
[01:32:49.500 --> 01:32:52.900]   If you like this song, you'll probably like this because they're in the same music genome.
[01:32:52.900 --> 01:32:55.660]   Now they want to do the same thing with podcasts.
[01:32:55.660 --> 01:33:01.220]   Oh, hey, I'm one of those.
[01:33:01.220 --> 01:33:03.980]   The Internet of Things podcast is one of those.
[01:33:03.980 --> 01:33:04.980]   Congratulations.
[01:33:04.980 --> 01:33:07.580]   I'm like, I'm part of the beta group.
[01:33:07.580 --> 01:33:09.180]   Oh, nice.
[01:33:09.180 --> 01:33:12.100]   So what genome are you in?
[01:33:12.100 --> 01:33:13.100]   The awesome genome.
[01:33:13.100 --> 01:33:15.300]   Do you think we're in the same genome?
[01:33:15.300 --> 01:33:16.300]   I think so.
[01:33:16.300 --> 01:33:19.220]   We're tech news shows.
[01:33:19.220 --> 01:33:24.060]   So it's in beta today's select Pandora users and iOS and Android devices.
[01:33:24.060 --> 01:33:28.020]   It'll expand to wider public in time.
[01:33:28.020 --> 01:33:34.020]   You should if you sign up for the beta, you should expect to see it sometime in December.
[01:33:34.020 --> 01:33:36.380]   Oh, you know all about this.
[01:33:36.380 --> 01:33:41.780]   Again, I thought that the genome wasn't as important as they thought it was and that
[01:33:41.780 --> 01:33:43.020]   it was really kind of a different.
[01:33:43.020 --> 01:33:47.620]   Well, I need to listen to a Pandora channel for any length of time.
[01:33:47.620 --> 01:33:50.740]   Might think that the genome is not exactly perfect.
[01:33:50.740 --> 01:33:53.860]   For some reason, I always get a Beatles song no matter what.
[01:33:53.860 --> 01:33:56.220]   That's very good.
[01:33:56.220 --> 01:34:03.140]   Well, so Spotify and Netflix are the next generation of recommendation engines and they
[01:34:03.140 --> 01:34:05.180]   use a different criteria.
[01:34:05.180 --> 01:34:11.060]   So I think what you may be referring to Jeff is just the idea that it's basically an older
[01:34:11.060 --> 01:34:14.180]   way of suggesting music.
[01:34:14.180 --> 01:34:19.460]   According to Pandora, the podcast genome project can evaluate content over 1500 attributes,
[01:34:19.460 --> 01:34:25.260]   MPAA ratings, production, style content type.
[01:34:25.260 --> 01:34:28.620]   Oh, they didn't ask about that.
[01:34:28.620 --> 01:34:35.420]   They asked what type of like we put a that we were tech new.
[01:34:35.420 --> 01:34:36.420]   We're news focused.
[01:34:36.420 --> 01:34:37.820]   We're focused on tech.
[01:34:37.820 --> 01:34:43.900]   We had improv slash witty banter, which may be giving heaven and I too much credit, but
[01:34:43.900 --> 01:34:50.340]   it's the style and other styles include like scripted interviews, storytelling.
[01:34:50.340 --> 01:34:51.660]   We are not those.
[01:34:51.660 --> 01:34:54.300]   So there were about 10 categories.
[01:34:54.300 --> 01:35:02.580]   And then it will use listener signals like thumbs, skips, replays to continue to refine
[01:35:02.580 --> 01:35:05.460]   their recommendations.
[01:35:05.460 --> 01:35:07.740]   This will be interesting, I think.
[01:35:07.740 --> 01:35:11.580]   I'll let you I'll let you know if I start seeing it rush new listeners.
[01:35:11.580 --> 01:35:14.180]   Anything that improves discovery is good.
[01:35:14.180 --> 01:35:17.260]   Yeah, I mean, yeah.
[01:35:17.260 --> 01:35:19.220]   Did you see the article?
[01:35:19.220 --> 01:35:23.220]   I talked about this on Twitter a little bit, but I wanted to do your thoughts as well.
[01:35:23.220 --> 01:35:29.060]   In the New York Times about Yuval, Nora, Noah Harari, he wrote that a couple of really
[01:35:29.060 --> 01:35:35.900]   good books, I thought, Sapiens, Homo Dios, and he's got a new one about lessons that
[01:35:35.900 --> 01:35:37.140]   the tech industry should learn.
[01:35:37.140 --> 01:35:47.140]   But basically, according to the Times, he's the principal doomsayer of Silicon Valley.
[01:35:47.140 --> 01:35:52.220]   He says that by creating powerful influence machines to control billions of minds, the
[01:35:52.220 --> 01:35:58.300]   big companies are destroying the idea of a sovereign individual with free will.
[01:35:58.300 --> 01:36:05.100]   He believes the Silicon Valley is creating a tiny ruling class above a teeming, furious,
[01:36:05.100 --> 01:36:07.380]   useless class, as he calls it.
[01:36:07.380 --> 01:36:13.820]   And he says in previous eras, autocrats needed the peasants because they grew all the food,
[01:36:13.820 --> 01:36:17.860]   but Silicon Valley doesn't really need the useless class.
[01:36:17.860 --> 01:36:22.380]   So they're much less likely to keep them around.
[01:36:22.380 --> 01:36:26.820]   One might argue that their entire business model is based on monetizing these class.
[01:36:26.820 --> 01:36:31.220]   I would say they need us to buy their crap.
[01:36:31.220 --> 01:36:44.460]   But yeah, he's embraced, apparently, by everybody from Google X to read Hastings of Netflix.
[01:36:44.460 --> 01:36:49.260]   When he came out to Silicon Valley, he was invited to dinner parties.
[01:36:49.260 --> 01:36:50.700]   My husband's read all of his books.
[01:36:50.700 --> 01:36:54.140]   I started Sapiens, but I was kind of like, "Oh."
[01:36:54.140 --> 01:36:55.780]   It's one of those you have to like...
[01:36:55.780 --> 01:36:56.780]   It's a slog.
[01:36:56.780 --> 01:36:59.420]   Oh, I didn't think so, really?
[01:36:59.420 --> 01:37:01.100]   I thought it was a slog.
[01:37:01.100 --> 01:37:04.340]   You have to like books like Freakonomics and the Malcolm Gladwell books.
[01:37:04.340 --> 01:37:06.460]   You're kind of a little Pat.
[01:37:06.460 --> 01:37:09.380]   He's a little sluggier than Malcolm Gladwell.
[01:37:09.380 --> 01:37:11.020]   And I don't particularly love.
[01:37:11.020 --> 01:37:16.100]   I'm kind of like, "Oh, how nice that your data that you selected lined up so appropriately
[01:37:16.100 --> 01:37:17.100]   for you."
[01:37:17.100 --> 01:37:18.100]   Yeah, a little Pat.
[01:37:18.100 --> 01:37:19.100]   Yeah.
[01:37:19.100 --> 01:37:20.900]   "Pack me some more insights, Malcolm."
[01:37:20.900 --> 01:37:29.340]   He writes in the essay he wrote for The Guardian, "If humans are hackable animals, and if
[01:37:29.340 --> 01:37:33.420]   our choices and opinions don't reflect our free will, what should the point of politics
[01:37:33.420 --> 01:37:34.620]   be?
[01:37:34.620 --> 01:37:38.740]   How do you live when you realize that your heart might be a government agent, that your
[01:37:38.740 --> 01:37:43.300]   amygdala might be working for Putin, that the legend thought that emerges in your mind
[01:37:43.300 --> 01:37:47.780]   might well be the result of some algorithm that knows you better than you know to yourselves?"
[01:37:47.780 --> 01:37:52.060]   Well, in some ways, that last bit is probably true.
[01:37:52.060 --> 01:37:59.180]   However, the rest, I'm kind of like, "Wait a year, you're traumatic license."
[01:37:59.180 --> 01:38:02.700]   See, he's trying to make it less of a slog for you.
[01:38:02.700 --> 01:38:07.140]   Okay, I read AI research.
[01:38:07.140 --> 01:38:08.140]   Yeah, that's slog.
[01:38:08.140 --> 01:38:11.140]   I mean, I'll slog if I think it's worth it.
[01:38:11.140 --> 01:38:12.940]   It's a deep mind paper.
[01:38:12.940 --> 01:38:16.460]   But I'm not going to slog for that, anyway.
[01:38:16.460 --> 01:38:21.660]   And then this article from the deputy governor of the central bank of Sweden.
[01:38:21.660 --> 01:38:27.060]   Why Sweden's cashless society is no longer a utopia, he points out that when people
[01:38:27.060 --> 01:38:32.980]   use private businesses to exchange value, as they have apparently in Sweden, the value
[01:38:32.980 --> 01:38:39.180]   of, the outstanding value of cash circulation in Sweden is 1% about a tenth of other developed
[01:38:39.180 --> 01:38:48.180]   nations because no one has cash and furthermore, they mostly use an app called SWISH.
[01:38:48.180 --> 01:38:56.940]   And yeah, and so as a result, he says, "This is problematic for governments.
[01:38:56.940 --> 01:39:00.900]   But this development raises some crucial issues regarding the state's role in the payment
[01:39:00.900 --> 01:39:05.300]   market for hundreds of years the public has been offered central banknotes and coins.
[01:39:05.300 --> 01:39:09.620]   If cash stops working, it would leave all individuals to rely on the private sector
[01:39:09.620 --> 01:39:12.500]   alone to get access to money and payment methods.
[01:39:12.500 --> 01:39:15.860]   This would be a historical change without precedent."
[01:39:15.860 --> 01:39:19.100]   This is actually, so I actually read about this a couple of years ago.
[01:39:19.100 --> 01:39:23.940]   It was brought up in relation to Sweden, actually.
[01:39:23.940 --> 01:39:31.260]   And there's going to be a regulating body.
[01:39:31.260 --> 01:39:34.540]   Either we'll have a crisis that's precipitated because of this.
[01:39:34.540 --> 01:39:42.940]   So somebody will go out of business or somebody will hack the private intermediary and change
[01:39:42.940 --> 01:39:46.140]   the value of money in a way that people are like, "Holy cow."
[01:39:46.140 --> 01:39:50.260]   So we're going to have either a reckoning or we're going to prevent the reckoning by
[01:39:50.260 --> 01:39:55.940]   recognizing that it's coming and build up some sort of governmental body to manage that.
[01:39:55.940 --> 01:40:00.940]   That is interesting because really, in most cases, the way you deal with money is all
[01:40:00.940 --> 01:40:04.860]   through private institutions, a bank, apps, whatever.
[01:40:04.860 --> 01:40:09.260]   But the money, the fundamental money you're dealing with is printed by the government
[01:40:09.260 --> 01:40:11.300]   and backed by the government.
[01:40:11.300 --> 01:40:13.860]   What if the government had no role in this at all?
[01:40:13.860 --> 01:40:15.020]   What would that look like?
[01:40:15.020 --> 01:40:17.220]   Well, it looks like what?
[01:40:17.220 --> 01:40:20.220]   What? Well, that also looks like...
[01:40:20.220 --> 01:40:22.100]   Don't leave me hanging.
[01:40:22.100 --> 01:40:23.100]   What?
[01:40:23.100 --> 01:40:24.100]   Bitcoin.
[01:40:24.100 --> 01:40:25.100]   Bitcoin.
[01:40:25.100 --> 01:40:26.100]   Well, that's, I think, probably the point.
[01:40:26.100 --> 01:40:32.580]   In fact, this guy's solution is to have an e-currency that's from the federal government.
[01:40:32.580 --> 01:40:34.700]   He wants to call him e-corona.
[01:40:34.700 --> 01:40:36.460]   But I don't see that that's any different than any other...
[01:40:36.460 --> 01:40:38.100]   What's the difference between e-corona and corona?
[01:40:38.100 --> 01:40:39.100]   Yeah, it's all a match.
[01:40:39.100 --> 01:40:40.100]   It's a fiat currency.
[01:40:40.100 --> 01:40:40.900]   It's a fiat currency.
[01:40:40.900 --> 01:40:46.980]   So you still have to pay your taxes.
[01:40:46.980 --> 01:40:50.820]   Well, yes, but...
[01:40:50.820 --> 01:40:57.500]   So you still have to exchange your virtual non-governmental currency for governmental
[01:40:57.500 --> 01:40:59.780]   currency at some point.
[01:40:59.780 --> 01:41:04.340]   Well, what the fear isn't that it...
[01:41:04.340 --> 01:41:10.540]   The fear is that if something happens in this case, that the currency suddenly...
[01:41:10.540 --> 01:41:13.740]   You've got worthless currency, basically, because there's this private company.
[01:41:13.740 --> 01:41:19.180]   So yes, I mean, you still pay the government in something that is converted back to corona,
[01:41:19.180 --> 01:41:20.180]   corona, whatever.
[01:41:20.180 --> 01:41:23.460]   I'm going to Stockholm tomorrow night?
[01:41:23.460 --> 01:41:24.460]   Ask that, will you?
[01:41:24.460 --> 01:41:25.460]   You can't...
[01:41:25.460 --> 01:41:26.860]   You can't, actually, this was really sad.
[01:41:26.860 --> 01:41:29.140]   When I was in Sweden, I could not download Swish.
[01:41:29.140 --> 01:41:31.540]   I was so excited to try it.
[01:41:31.540 --> 01:41:33.500]   But my credit cards are all tied to the US.
[01:41:33.500 --> 01:41:38.620]   So they were like, "No, you have to be one of the four banks."
[01:41:38.620 --> 01:41:47.740]   I think that back Swish, if your bank doesn't participate in the Swish banking network...
[01:41:47.740 --> 01:41:53.300]   Imagine in this country, if everything you did, you had to either use Apple Pay or Android
[01:41:53.300 --> 01:41:55.340]   Pay or Samsung Pay.
[01:41:55.340 --> 01:41:59.940]   Well, okay, but Swish is actually created by the banks.
[01:41:59.940 --> 01:42:05.020]   So the app was created by the banks who didn't want to get disintermediated by those things.
[01:42:05.020 --> 01:42:06.820]   So it is a little different.
[01:42:06.820 --> 01:42:09.420]   It's called Zell, and it's horrible.
[01:42:09.420 --> 01:42:13.700]   Yeah, well, Swish is not as bad apparently.
[01:42:13.700 --> 01:42:19.700]   Everybody uses Venmo or Cash.me, which is from Square Venmo's from PayPal.
[01:42:19.700 --> 01:42:27.740]   I mean, really, we're moving in that direction into a cashless society that's entirely exchanging...
[01:42:27.740 --> 01:42:30.100]   Well, in Sweden is very cashless.
[01:42:30.100 --> 01:42:31.100]   You can't...
[01:42:31.100 --> 01:42:34.020]   There's no place to use cash.
[01:42:34.020 --> 01:42:37.620]   You can actually put up a sign in your store that says "cash not accepted" here.
[01:42:37.620 --> 01:42:41.220]   There's tons of places that don't accept cash.
[01:42:41.220 --> 01:42:42.700]   There are tons of...
[01:42:42.700 --> 01:42:46.180]   The thing is taking over New York.
[01:42:46.180 --> 01:42:47.340]   Every city in America is the same.
[01:42:47.340 --> 01:42:48.900]   But it's just right around me.
[01:42:48.900 --> 01:42:52.220]   It's like 25 places where the food comes in bowls.
[01:42:52.220 --> 01:42:55.820]   Like we lost the ability to use plates.
[01:42:55.820 --> 01:42:57.700]   Pretty soon you won't get utensils either.
[01:42:57.700 --> 01:42:59.100]   We've got to take everything.
[01:42:59.100 --> 01:43:01.020]   Yeah, just slurping up.
[01:43:01.020 --> 01:43:02.020]   Plates are too flat.
[01:43:02.020 --> 01:43:03.780]   Bowls are protective.
[01:43:03.780 --> 01:43:07.820]   Every one of those places, a couple of places I go up and I try to get them a $20 bill.
[01:43:07.820 --> 01:43:08.820]   Oh, sorry, sir.
[01:43:08.820 --> 01:43:09.820]   No, we don't take cash.
[01:43:09.820 --> 01:43:11.820]   You fool you, you idiot.
[01:43:11.820 --> 01:43:13.580]   How long have you been asleep?
[01:43:13.580 --> 01:43:15.860]   We've worked so hard to protect you with our polls.
[01:43:15.860 --> 01:43:18.460]   And you try to use this dirty piece of paper.
[01:43:18.460 --> 01:43:20.660]   Shame on you.
[01:43:20.660 --> 01:43:23.460]   Well, it's beneficial to a vendor.
[01:43:23.460 --> 01:43:26.620]   Like a company, especially a small one because they don't have to worry so much about like
[01:43:26.620 --> 01:43:28.180]   embezzlement and things like that.
[01:43:28.180 --> 01:43:30.580]   Make me 3% of every transaction.
[01:43:30.580 --> 01:43:31.580]   Yeah.
[01:43:31.580 --> 01:43:36.580]   But it's worth it to them to deal with like to eliminate the registered person and all
[01:43:36.580 --> 01:43:37.580]   of this other thing.
[01:43:37.580 --> 01:43:38.580]   Really?
[01:43:38.580 --> 01:43:41.780]   It's just like the thing now is everything comes in bowls.
[01:43:41.780 --> 01:43:43.620]   Did Chipotle start that?
[01:43:43.620 --> 01:43:44.620]   It might be.
[01:43:44.620 --> 01:43:45.620]   It was a bit hot.
[01:43:45.620 --> 01:43:46.620]   Yeah.
[01:43:46.620 --> 01:43:47.620]   Plus the salad places.
[01:43:47.620 --> 01:43:52.420]   Yeah, I think it's kind of a fix of the salad in green places that just dump all this stuff
[01:43:52.420 --> 01:43:53.420]   in there.
[01:43:53.420 --> 01:43:54.420]   Like the Buddha bowls.
[01:43:54.420 --> 01:43:56.540]   Are we both focused here in Petaluma?
[01:43:56.540 --> 01:43:57.980]   I think we still use plates.
[01:43:57.980 --> 01:43:59.940]   No, it's it's bowls here.
[01:43:59.940 --> 01:44:05.100]   You're you're behind a couple of your even the salad places are on plates or no, they're
[01:44:05.100 --> 01:44:07.060]   in those plastic boxes.
[01:44:07.060 --> 01:44:08.060]   Styrifle?
[01:44:08.060 --> 01:44:10.380]   You do a lot of boxes.
[01:44:10.380 --> 01:44:14.660]   No clear plastic or paper if they're hip, their boxes.
[01:44:14.660 --> 01:44:17.060]   We're back in the blue food store still.
[01:44:17.060 --> 01:44:18.220]   Now we're bold here.
[01:44:18.220 --> 01:44:20.860]   And then you can get everything in bowls, right?
[01:44:20.860 --> 01:44:23.420]   And you go to there's the places I hate beats.
[01:44:23.420 --> 01:44:24.980]   I can't stand beats.
[01:44:24.980 --> 01:44:26.580]   I just can't abide them.
[01:44:26.580 --> 01:44:29.300]   There's a restaurant named like the beat.
[01:44:29.300 --> 01:44:30.300]   I'm not going in there.
[01:44:30.300 --> 01:44:31.300]   The beat.
[01:44:31.300 --> 01:44:32.300]   How about the people?
[01:44:32.300 --> 01:44:33.700]   Let's all go to the beat ball.
[01:44:33.700 --> 01:44:34.700]   Come on, Jeff.
[01:44:34.700 --> 01:44:37.500]   I'm buying beat ball for everyone.
[01:44:37.500 --> 01:44:42.660]   I think I'll have a little kale and some better mommy and some beats and oh yeah, some
[01:44:42.660 --> 01:44:44.660]   Brussels sprouts too.
[01:44:44.660 --> 01:44:48.540]   Jeff, we're too old for this world.
[01:44:48.540 --> 01:44:50.580]   This is I knew this would happen someday.
[01:44:50.580 --> 01:44:52.860]   I just didn't think it would happen today.
[01:44:52.860 --> 01:44:53.860]   I had that.
[01:44:53.860 --> 01:44:54.860]   Okay, today.
[01:44:54.860 --> 01:44:55.860]   Okay, in the bowl.
[01:44:55.860 --> 01:44:57.860]   Okay, poke balls are a thing.
[01:44:57.860 --> 01:44:59.460]   It's like how you go to New York.
[01:44:59.460 --> 01:45:01.460]   Everything is a thing is a thing here.
[01:45:01.460 --> 01:45:06.940]   Well, it starts in New York and I don't mean one island city.
[01:45:06.940 --> 01:45:10.700]   It started in Hawaii.
[01:45:10.700 --> 01:45:11.700]   That's where they're from.
[01:45:11.700 --> 01:45:12.700]   Not Japan.
[01:45:12.700 --> 01:45:13.700]   Pokey bowls are from Hawaii.
[01:45:13.700 --> 01:45:14.700]   Hawaii.
[01:45:14.700 --> 01:45:17.740]   We got Mexican bowls.
[01:45:17.740 --> 01:45:18.780]   We got salad bowls.
[01:45:18.780 --> 01:45:20.480]   We got barbecue bowls.
[01:45:20.480 --> 01:45:21.480]   We have poke bowls.
[01:45:21.480 --> 01:45:23.620]   It is kind of the infant infant.
[01:45:23.620 --> 01:45:25.580]   Inventilization of food.
[01:45:25.580 --> 01:45:26.580]   It is.
[01:45:26.580 --> 01:45:27.580]   It is.
[01:45:27.580 --> 01:45:29.120]   What'd you pick up with your balls?
[01:45:29.120 --> 01:45:30.280]   Oh, this is very important too.
[01:45:30.280 --> 01:45:34.520]   Did you see the trailer for the new Toy Story?
[01:45:34.520 --> 01:45:35.720]   No.
[01:45:35.720 --> 01:45:37.080]   Oh no, I missed it.
[01:45:37.080 --> 01:45:38.160]   There's a new character.
[01:45:38.160 --> 01:45:40.020]   And I think there's--
[01:45:40.020 --> 01:45:42.020]   The score, ball man.
[01:45:42.020 --> 01:45:44.280]   Well, thank Colin, spit on Caul.
[01:45:44.280 --> 01:45:45.320]   Then you ruined the punchline.
[01:45:45.320 --> 01:45:46.840]   Thank you very much Stacy.
[01:45:46.840 --> 01:45:48.140]   Sorry.
[01:45:48.140 --> 01:45:50.740]   Stacy.
[01:45:50.740 --> 01:45:51.780]   Stacy?
[01:45:51.780 --> 01:45:52.640]   What are back on you?
[01:45:52.640 --> 01:45:54.020]   Will you, there's all that.
[01:45:54.020 --> 01:45:56.200]   Oh, like the Toy Story.
[01:45:56.200 --> 01:45:57.200]   Right? Is that--
[01:45:57.200 --> 01:45:58.200]   Forky.
[01:45:58.200 --> 01:46:00.100]   They called the character Forky, which you're about to see.
[01:46:00.100 --> 01:46:01.140]   All right, watch this.
[01:46:01.140 --> 01:46:02.640]   Watch this.
[01:46:02.640 --> 01:46:04.340]   From Disney.
[01:46:04.340 --> 01:46:06.740]   The people who gave you--
[01:46:06.740 --> 01:46:07.740]   Walt.
[01:46:07.740 --> 01:46:09.740]   [MUSIC PLAYING]
[01:46:09.740 --> 01:46:12.240]    Those and flows are the angel 
[01:46:12.240 --> 01:46:14.140]   I hope you're going to get a new group of the songs.
[01:46:14.140 --> 01:46:15.300]   Is that Judy Collins?
[01:46:15.300 --> 01:46:17.780]   I'm going to get taken down because of the Judy Collins song.
[01:46:17.780 --> 01:46:18.820]   You can put it on.
[01:46:18.820 --> 01:46:19.780]   You're not going to--
[01:46:19.780 --> 01:46:21.220]   We're not adding anything to the overall.
[01:46:21.220 --> 01:46:22.060]   That was not.
[01:46:22.060 --> 01:46:24.660]   So you see all the cards in the--
[01:46:24.660 --> 01:46:26.660]   All happy in the sky.
[01:46:26.660 --> 01:46:30.320]   Cowgirl, Mr. Potato Head, the cowboy.
[01:46:30.320 --> 01:46:31.920]   I don't even know their names.
[01:46:31.920 --> 01:46:34.520]   I think what you would see today would be your head.
[01:46:34.520 --> 01:46:38.700]   And then the dinosaur that used to be Don Rickles, right?
[01:46:38.700 --> 01:46:40.000]   But he's the best away.
[01:46:40.000 --> 01:46:43.000]   The pig, little pig, little rubber.
[01:46:43.000 --> 01:46:45.200]   There's the alien with the three eyes.
[01:46:45.200 --> 01:46:46.200]   And he's just--
[01:46:46.200 --> 01:46:47.200]   After the dog.
[01:46:47.200 --> 01:46:48.200]   After the dog.
[01:46:48.200 --> 01:46:49.200]   And it's over.
[01:46:49.200 --> 01:46:50.640]   Next summer.
[01:46:50.640 --> 01:46:52.400]   You can turn this sound up.
[01:46:52.400 --> 01:46:53.200]   Next summer.
[01:46:54.200 --> 01:46:55.200]   I don't know.
[01:46:55.200 --> 01:46:56.200]   Climb.
[01:46:56.200 --> 01:46:57.200]   There's a--
[01:46:57.200 --> 01:46:58.200]   I don't belong here!
[01:46:58.200 --> 01:46:59.200]   Aaah!
[01:46:59.200 --> 01:47:00.200]   Aaah!
[01:47:00.200 --> 01:47:01.200]   Aaah!
[01:47:01.200 --> 01:47:05.200]   And they call the character Forky, but it's not a Fork.
[01:47:05.200 --> 01:47:06.200]   It is.
[01:47:06.200 --> 01:47:07.200]   A Spark.
[01:47:07.200 --> 01:47:08.200]   A Spark.
[01:47:08.200 --> 01:47:09.200]   So it should be called Sporky.
[01:47:09.200 --> 01:47:11.200]   Sporky's a better name.
[01:47:11.200 --> 01:47:12.200]   It is.
[01:47:12.200 --> 01:47:13.200]   I'm having a fit.
[01:47:13.200 --> 01:47:14.200]   I'm not a toy.
[01:47:14.200 --> 01:47:15.200]   Aaah!
[01:47:15.200 --> 01:47:18.200]   And he's got pipe cleaner hands.
[01:47:18.200 --> 01:47:21.200]   I don't even get it.
[01:47:21.200 --> 01:47:23.200]   He doesn't identify as a toy.
[01:47:23.200 --> 01:47:24.200]   I don't--
[01:47:24.200 --> 01:47:25.200]   Yeah, I don't--
[01:47:25.200 --> 01:47:28.200]   And other people misidentify him as a Fork?
[01:47:28.200 --> 01:47:29.200]   Yeah.
[01:47:29.200 --> 01:47:30.200]   Yeah.
[01:47:30.200 --> 01:47:36.200]   Okay, Kevin, who is an expert on juvenile here, has come in to explain.
[01:47:36.200 --> 01:47:38.200]   There's another teaser trailer.
[01:47:38.200 --> 01:47:39.200]   There's another teaser trailer.
[01:47:39.200 --> 01:47:40.200]   Oh.
[01:47:40.200 --> 01:47:41.200]   Is it?
[01:47:41.200 --> 01:47:43.200]   It's a key and peel teaser trailer for this?
[01:47:43.200 --> 01:47:47.200]   They're voicing key and peel are part of the toy collection.
[01:47:47.200 --> 01:47:48.200]   How exciting.
[01:47:48.200 --> 01:47:52.200]   Here's some serious news, and I'm really happy.
[01:47:52.200 --> 01:47:56.200]   Just in case you're thinking of swatting anybody.
[01:47:56.200 --> 01:48:03.200]   This guy, Tyler Barris, pleaded guilty on Tuesday to making a SWAT call.
[01:48:03.200 --> 01:48:08.200]   He made many, but one that actually caused the death of an innocent Kansas man.
[01:48:08.200 --> 01:48:16.200]   He was a dispute between two online gamers over a Buck-50 bet in a Call of Duty video game.
[01:48:16.200 --> 01:48:17.200]   He pled guilty.
[01:48:17.200 --> 01:48:22.200]   He'll be going to jail for between 20 and 25 years.
[01:48:22.200 --> 01:48:29.200]   51 charges, including federal charges, initially filed in California in the District of Columbia,
[01:48:29.200 --> 01:48:33.200]   related to other fake calls and threats.
[01:48:33.200 --> 01:48:36.200]   He's going to jail folks for 25 years.
[01:48:36.200 --> 01:48:39.200]   Oh, and it's a deadly weapon.
[01:48:39.200 --> 01:48:40.200]   That is not okay.
[01:48:40.200 --> 01:48:41.200]   Not okay.
[01:48:41.200 --> 01:48:43.200]   I'll be sentenced in January.
[01:48:43.200 --> 01:48:45.200]   We'll find out.
[01:48:45.200 --> 01:48:51.200]   Also charged as co-conspirators, an 18-year-old from Ohio and a 20-year-old from Kansas.
[01:48:51.200 --> 01:48:53.200]   They've pled not guilty.
[01:48:53.200 --> 01:48:56.200]   Their trial's coming up in January.
[01:48:56.200 --> 01:49:03.200]   We reported on this when it happened, a police officer responding to the phony SWAT call,
[01:49:03.200 --> 01:49:11.200]   fatally shot, went to a house that wasn't the right house and fatally shot a 20-year-old
[01:49:11.200 --> 01:49:12.200]   man.
[01:49:12.200 --> 01:49:20.200]   I didn't mean to bring you down from sporky and all that stuff, but I think it was time
[01:49:20.200 --> 01:49:27.200]   to get serious.
[01:49:27.200 --> 01:49:28.200]   Let's take a break.
[01:49:28.200 --> 01:49:31.200]   When we come back, you're a pixel a week because we're almost done.
[01:49:31.200 --> 01:49:33.200]   Oh, I'm sorry.
[01:49:33.200 --> 01:49:35.200]   She's so happy.
[01:49:35.200 --> 01:49:36.200]   Hagen-Botham.
[01:49:36.200 --> 01:49:38.200]   I'm so hungry.
[01:49:38.200 --> 01:49:41.200]   My husband Jess was like, "Hey, we're going to go out tonight because there's no water
[01:49:41.200 --> 01:49:42.200]   in our neighborhood."
[01:49:42.200 --> 01:49:43.200]   So I was like, "Kato."
[01:49:43.200 --> 01:49:45.200]   Still no water in your--
[01:49:45.200 --> 01:49:46.200]   Okay.
[01:49:46.200 --> 01:49:47.200]   This is just right now.
[01:49:47.200 --> 01:49:48.200]   Okay.
[01:49:48.200 --> 01:49:49.200]   This is not still leftover.
[01:49:49.200 --> 01:49:51.200]   This is not leftover from boiling the water.
[01:49:51.200 --> 01:49:53.200]   But basically, I'm excited because I'm getting queso.
[01:49:53.200 --> 01:49:54.200]   Tell them, shut down.
[01:49:54.200 --> 01:49:55.200]   At the end of this.
[01:49:55.200 --> 01:49:58.200]   Andrew, I'm coming for queso.
[01:49:58.200 --> 01:50:02.200]   You've got to have queso like every night before you go to Seattle.
[01:50:02.200 --> 01:50:04.200]   You've got a last chance for queso.
[01:50:04.200 --> 01:50:05.200]   Yeah.
[01:50:05.200 --> 01:50:07.200]   I am very concerned about the queso situation.
[01:50:07.200 --> 01:50:08.200]   Oh, you could imagine.
[01:50:08.200 --> 01:50:10.200]   They're going to put kale in it up there.
[01:50:10.200 --> 01:50:14.720]   I didn't know queso existed until I went to Austin a few years ago.
[01:50:14.720 --> 01:50:18.160]   Here I am, a grown man, a gourmet, a man of the world.
[01:50:18.160 --> 01:50:19.720]   I thought I knew everything.
[01:50:19.720 --> 01:50:25.800]   And then I went to Austin and they served me up this delicious, warm, cheesy substance.
[01:50:25.800 --> 01:50:26.800]   My life changed.
[01:50:26.800 --> 01:50:29.080]   Do you know why queso is so good?
[01:50:29.080 --> 01:50:30.920]   Because it's melted cheese.
[01:50:30.920 --> 01:50:32.960]   Oh, because it has opiate mixes.
[01:50:32.960 --> 01:50:34.600]   It's an opiate.
[01:50:34.600 --> 01:50:36.080]   Yes.
[01:50:36.080 --> 01:50:48.120]   We now know that there's an ingredient in cheese called cuisine that apparently attaches to
[01:50:48.120 --> 01:50:50.880]   the same brain.
[01:50:50.880 --> 01:50:51.880]   Receptors.
[01:50:51.880 --> 01:50:54.960]   Receptors as opiates.
[01:50:54.960 --> 01:51:04.880]   Cacine is actually dairy cheese, particularly is, contains, cacine also contains, cacine fragments
[01:51:04.880 --> 01:51:06.960]   called get this.
[01:51:06.960 --> 01:51:09.200]   Cazomorphins.
[01:51:09.200 --> 01:51:10.200]   A morphine like.
[01:51:10.200 --> 01:51:12.200]   It's actually pronounced queso-morphines.
[01:51:12.200 --> 01:51:14.200]   Oh my God.
[01:51:14.200 --> 01:51:15.840]   Oh my God.
[01:51:15.840 --> 01:51:19.680]   A queso-morphin.
[01:51:19.680 --> 01:51:23.880]   It's a morphine like compound, opiate molecules.
[01:51:23.880 --> 01:51:25.640]   There's a reason.
[01:51:25.640 --> 01:51:28.000]   So basically I'm high all the time.
[01:51:28.000 --> 01:51:29.080]   I like it.
[01:51:29.080 --> 01:51:31.720]   Well, not when you move to Seattle, you're going to be depressed all the time.
[01:51:31.720 --> 01:51:32.720]   It's always rain.
[01:51:32.720 --> 01:51:34.680]   Oh, they have other cheese up there.
[01:51:34.680 --> 01:51:35.680]   They do.
[01:51:35.680 --> 01:51:38.800]   You're trading in case sold for coffee.
[01:51:38.800 --> 01:51:40.920]   That's got its own special.
[01:51:40.920 --> 01:51:43.320]   What's happening?
[01:51:43.320 --> 01:51:45.280]   One substance for another.
[01:51:45.280 --> 01:51:46.280]   Why not?
[01:51:46.280 --> 01:51:47.440]   Why not?
[01:51:47.440 --> 01:51:51.760]   Our show today is quite literally brought to you by CashFly.
[01:51:51.760 --> 01:51:55.560]   CashFly is our content distribution network.
[01:51:55.560 --> 01:51:59.640]   When you download a show, whether your podcast, app downloads it or you go to our website
[01:51:59.640 --> 01:52:05.520]   and download it, you're connecting directly to CashFly.
[01:52:05.520 --> 01:52:09.360]   And I'll tell you what, we've been on CashFly practically since the beginning.
[01:52:09.360 --> 01:52:12.240]   And I have not once received that 3am phone call.
[01:52:12.240 --> 01:52:14.080]   Oh, the content distribution.
[01:52:14.080 --> 01:52:15.920]   The CDN is down.
[01:52:15.920 --> 01:52:19.800]   No, if you're in podcasting, if you're in software and you're in any business that
[01:52:19.800 --> 01:52:25.960]   relies on having your content available readily at any time to your consumers, you need a
[01:52:25.960 --> 01:52:26.960]   CDN.
[01:52:26.960 --> 01:52:29.640]   Here's the problem for a lot of us with CDNs.
[01:52:29.640 --> 01:52:31.960]   You can't always predict your bandwidth consumption.
[01:52:31.960 --> 01:52:34.440]   Twit's very spiky.
[01:52:34.440 --> 01:52:38.480]   We don't know from when to the next how much bandwidth we're going to use.
[01:52:38.480 --> 01:52:40.080]   Fortunately, CashFly makes it easy.
[01:52:40.080 --> 01:52:43.040]   They eliminate CDN outages.
[01:52:43.040 --> 01:52:46.320]   They help you reach new markets because they have servers all over the world.
[01:52:46.320 --> 01:52:52.720]   So no matter where our listeners are, they download it just as fast and you control costs
[01:52:52.720 --> 01:52:54.120]   with CashFly.
[01:52:54.120 --> 01:52:55.800]   I love the CashFly team.
[01:52:55.800 --> 01:52:58.360]   They need to talk to smart, agile techs.
[01:52:58.360 --> 01:53:00.040]   They can answer your questions the first time.
[01:53:00.040 --> 01:53:01.320]   They won't put you on hold.
[01:53:01.320 --> 01:53:03.480]   They won't fend you off to somebody else.
[01:53:03.480 --> 01:53:06.840]   They're really good because CashFly empowers them.
[01:53:06.840 --> 01:53:12.440]   100% availability with their Bulletproof 100% SLA.
[01:53:12.440 --> 01:53:13.440]   That's right.
[01:53:13.440 --> 01:53:14.440]   I said 100%.
[01:53:14.440 --> 01:53:18.960]   So you don't get angry emails or hundreds of letters from users because you're the CDN's
[01:53:18.960 --> 01:53:21.880]   down and you don't get that 3am phone call.
[01:53:21.880 --> 01:53:27.760]   I could tell you, we deliver petabytes of data every month without a hiccup ever thanks
[01:53:27.760 --> 01:53:29.120]   to CashFly.
[01:53:29.120 --> 01:53:30.960]   And I don't worry about costs.
[01:53:30.960 --> 01:53:36.480]   I don't log in multiple times a week or try to track my CDN usage because CashFly will
[01:53:36.480 --> 01:53:40.120]   tailor the plan to your yearly usage trend.
[01:53:40.120 --> 01:53:42.600]   So those spikes, they get all smoothed out.
[01:53:42.600 --> 01:53:44.680]   In fact, we save a lot of money.
[01:53:44.680 --> 01:53:48.840]   On average, customers who switch to CashFly save more than 20%.
[01:53:48.840 --> 01:53:50.720]   20%.
[01:53:50.720 --> 01:53:52.640]   Here's what ours Technica says.
[01:53:52.640 --> 01:53:56.840]   We use it, but ours Technica says CashFly has exceeded their expectations on every level
[01:53:56.840 --> 01:54:01.000]   from the technical operation of the actual service to the top notch support staff and
[01:54:01.000 --> 01:54:02.720]   their responsiveness.
[01:54:02.720 --> 01:54:07.600]   We literally would not exist if it weren't for CashFly.
[01:54:07.600 --> 01:54:10.440]   Like CashFly give you some sanity back to.
[01:54:10.440 --> 01:54:13.920]   And we have a new deal just for Twit listeners.
[01:54:13.920 --> 01:54:20.040]   Up to $2,000 in credits when you switch to CashFly before the end of the year.
[01:54:20.040 --> 01:54:24.680]   And more and get a complimentary quote at twit.cashfly.com.
[01:54:24.680 --> 01:54:29.360]   Up to $2,000 in credits rates are based off 30% off MRC.
[01:54:29.360 --> 01:54:31.840]   Bill credit applied for a 12 month period.
[01:54:31.840 --> 01:54:32.840]   And again, don't wait.
[01:54:32.840 --> 01:54:36.760]   This exclusive offer is available only until January 1st.
[01:54:36.760 --> 01:54:43.720]   I want you all to check it out twit.cashfly.com if you deliver content.
[01:54:43.720 --> 01:54:45.440]   You know, this was a big threat.
[01:54:45.440 --> 01:54:49.960]   When we first started Twit, we were doing, we were having, we were seeding BitTorrent
[01:54:49.960 --> 01:54:52.000]   seeds for listeners.
[01:54:52.000 --> 01:54:57.440]   They would become our quick, quick, you can put this and it was crazy.
[01:54:57.440 --> 01:54:58.440]   Thank God.
[01:54:58.440 --> 01:55:00.800]   CashFly, Matt Levine and his team came over and said, we can help.
[01:55:00.800 --> 01:55:04.640]   And they have for almost a decade now.
[01:55:04.640 --> 01:55:06.200]   Twit.cashfly.com.
[01:55:06.200 --> 01:55:07.200]   Thank you, CashFly.
[01:55:07.200 --> 01:55:08.360]   Bless you.
[01:55:08.360 --> 01:55:11.680]   Stacy, you have a pick of the week this week?
[01:55:11.680 --> 01:55:12.680]   I do.
[01:55:12.680 --> 01:55:16.720]   This is a fun one for anybody who's in the Amazon Echo ecosystem.
[01:55:16.720 --> 01:55:19.600]   So someone had asked us and we talked about it on the podcast.
[01:55:19.600 --> 01:55:22.960]   So if you listen to both of us, you've already heard this and you can feel superior to everyone
[01:55:22.960 --> 01:55:24.120]   else.
[01:55:24.120 --> 01:55:27.680]   This is a skill called notify me on the Echo.
[01:55:27.680 --> 01:55:32.480]   And so the links I put in here are for the skill itself.
[01:55:32.480 --> 01:55:36.400]   And then an article that Kevin wrote on how to use this.
[01:55:36.400 --> 01:55:38.360]   It's a little bit more involved.
[01:55:38.360 --> 01:55:41.680]   But what it does is you can take any device in your house.
[01:55:41.680 --> 01:55:49.560]   We did it with if so that talks to if and we basically made it so Amazon's when, when
[01:55:49.560 --> 01:55:56.080]   a motion was triggered by my wise camera, my echo would turn yellow in it with if I asked
[01:55:56.080 --> 01:55:59.160]   it to play my notifications, I would get the notification.
[01:55:59.160 --> 01:56:00.680]   I would say, hey, play my notifications.
[01:56:00.680 --> 01:56:04.520]   And it would say your wise cam saw movement.
[01:56:04.520 --> 01:56:12.640]   So it's a little, it's less useful than it automatically going, Bing Stacy, I saw movement.
[01:56:12.640 --> 01:56:13.800]   But that also might get annoying.
[01:56:13.800 --> 01:56:16.440]   So I understand why they did it this way.
[01:56:16.440 --> 01:56:18.560]   And it was just kind of a fun thing to do.
[01:56:18.560 --> 01:56:22.680]   You could use it for anything that's on if.
[01:56:22.680 --> 01:56:27.120]   So I actually thought it might be kind of cool if you are waiting for a certain email.
[01:56:27.120 --> 01:56:32.000]   You could get you type that in for your email thing.
[01:56:32.000 --> 01:56:36.280]   So if I get an email from this person, notify me on echo would be kind of the recipe, what
[01:56:36.280 --> 01:56:37.480]   it looks like.
[01:56:37.480 --> 01:56:43.080]   And then if that email comes in, your echo turns yellow and it's like, hey, someone
[01:56:43.080 --> 01:56:44.680]   so sent you the email.
[01:56:44.680 --> 01:56:45.680]   Nice.
[01:56:45.680 --> 01:56:46.680]   Have fun with this.
[01:56:46.680 --> 01:56:51.040]   It was actually kind of, I thought this was fun and it's actually somewhat useful.
[01:56:51.040 --> 01:56:53.760]   So awesome.
[01:56:53.760 --> 01:56:55.640]   And where should we go to read about this?
[01:56:55.640 --> 01:56:56.640]   Your blog?
[01:56:56.640 --> 01:57:03.960]   Yes, there's a link on Stacy and IOT how to trigger custom madam a notifications.
[01:57:03.960 --> 01:57:08.080]   And I bet in the chat rooms, scooter X has already found it.
[01:57:08.080 --> 01:57:09.080]   Has he?
[01:57:09.080 --> 01:57:10.080]   Yes, he has.
[01:57:10.080 --> 01:57:13.080]   I was like, oh, he's moving slowly.
[01:57:13.080 --> 01:57:16.520]   No, he it's already scrolled off the page.
[01:57:16.520 --> 01:57:17.520]   There we go.
[01:57:17.520 --> 01:57:19.280]   That's how fast that's how fast he is.
[01:57:19.280 --> 01:57:20.280]   Stacy on IOT.com.
[01:57:20.280 --> 01:57:25.640]   That's actually a good place to sign up for the newsletter and the podcast too.
[01:57:25.640 --> 01:57:26.640]   It is true.
[01:57:26.640 --> 01:57:30.880]   Jeff Jarvis, a number Stacy said such a high bar, somewhat useful.
[01:57:30.880 --> 01:57:31.880]   It's a very high.
[01:57:31.880 --> 01:57:35.120]   I can I can I can match that one.
[01:57:35.120 --> 01:57:36.120]   I don't know.
[01:57:36.120 --> 01:57:38.360]   Maybe tough as I'm pretty much useless.
[01:57:38.360 --> 01:57:41.880]   Oh, there's two here.
[01:57:41.880 --> 01:57:46.480]   One is that people have raised more than a billion dollars on Facebook since 2015 in
[01:57:46.480 --> 01:57:48.280]   their charities.
[01:57:48.280 --> 01:57:52.280]   And there's other Facebook news today, which we didn't get to, but let's put that aside.
[01:57:52.280 --> 01:57:56.560]   It's not really, you know, I feel guilty because we talked about before the show and
[01:57:56.560 --> 01:57:58.400]   then I realized I didn't get to it in the show.
[01:57:58.400 --> 01:57:59.400]   Isn't really news.
[01:57:59.400 --> 01:58:00.400]   It's one of those.
[01:58:00.400 --> 01:58:01.400]   It really does.
[01:58:01.400 --> 01:58:05.040]   Great times pieces about, you know, how behind the scenes at Facebook.
[01:58:05.040 --> 01:58:06.040]   Did not I?
[01:58:06.040 --> 01:58:07.040]   You should read it though.
[01:58:07.040 --> 01:58:08.040]   It is.
[01:58:08.040 --> 01:58:09.040]   Oh, it's good.
[01:58:09.040 --> 01:58:10.040]   Delays.
[01:58:10.040 --> 01:58:11.040]   It's the worst of it is.
[01:58:11.040 --> 01:58:15.320]   They hired a conservative company and they were trying to tie people who were criticizing
[01:58:15.320 --> 01:58:17.760]   Facebook with George Soros.
[01:58:17.760 --> 01:58:18.760]   Not a good thing.
[01:58:18.760 --> 01:58:21.240]   He's to blame the rest of us mainly just to tick talk and the personalities and the
[01:58:21.240 --> 01:58:22.240]   actions and whatnot.
[01:58:22.240 --> 01:58:26.120]   So anyway, that's you can read that news in the New York Times today about Facebook,
[01:58:26.120 --> 01:58:29.960]   but also on the good news side and full disclosure, of course, I raised money for the Craig New
[01:58:29.960 --> 01:58:32.920]   York Graduate School of Journalism from Facebook, but I am independent of them and
[01:58:32.920 --> 01:58:34.200]   none of them goes into my pocket.
[01:58:34.200 --> 01:58:35.200]   Right.
[01:58:35.200 --> 01:58:36.200]   That is a closure.
[01:58:36.200 --> 01:58:37.240]   So, a billion dollars.
[01:58:37.240 --> 01:58:41.800]   And so on Giving Tuesday, Facebook will match a total of up to $7 million.
[01:58:41.800 --> 01:58:42.800]   Wow.
[01:58:42.800 --> 01:58:50.320]   Facebook and PayPal will match donations of up to $250,000 per nonprofit and up to $20,000
[01:58:50.320 --> 01:58:51.320]   per donor.
[01:58:51.320 --> 01:58:52.760]   It's kind of cool when you think about it.
[01:58:52.760 --> 01:58:57.960]   If you could get all your friends to give $250,000, you could turn that into $500,000
[01:58:57.960 --> 01:59:00.040]   if you get in time quickly.
[01:59:00.040 --> 01:59:02.920]   So, a little gamification.
[01:59:02.920 --> 01:59:03.920]   And then the other.
[01:59:03.920 --> 01:59:04.920]   You should do your disclosure.
[01:59:04.920 --> 01:59:05.920]   I did it.
[01:59:05.920 --> 01:59:06.920]   I did it.
[01:59:06.920 --> 01:59:07.920]   You didn't hear it.
[01:59:07.920 --> 01:59:08.920]   You didn't hear it.
[01:59:08.920 --> 01:59:09.920]   It was awesome.
[01:59:09.920 --> 01:59:12.320]   I was thinking about K. So, I'm sorry, Jeff.
[01:59:12.320 --> 01:59:17.800]   You're, you're in door for the book.
[01:59:17.800 --> 01:59:24.440]   The other one is the Pixel book is going to be $699 at Best Buy for Black Friday.
[01:59:24.440 --> 01:59:25.440]   Awesome.
[01:59:25.440 --> 01:59:26.440]   It is awesome.
[01:59:26.440 --> 01:59:27.440]   Leo, do you have something?
[01:59:27.440 --> 01:59:31.240]   Because if you don't, I have a great story that everyone should read because it was awesome.
[01:59:31.240 --> 01:59:34.200]   I will just quickly say this and I'll throw it to you.
[01:59:34.200 --> 01:59:36.680]   And I don't know who put this link in the show notes, but I did want to make sure you
[01:59:36.680 --> 01:59:37.680]   saw it.
[01:59:37.680 --> 01:59:40.320]   It's from the Mozilla Foundation.
[01:59:40.320 --> 01:59:45.080]   It is a list of, it's just right for people who are about to buy say, oh, I don't know,
[01:59:45.080 --> 01:59:47.600]   a Facebook portal.
[01:59:47.600 --> 01:59:54.040]   It is, it is devices that you might not want to buy this holiday season.
[01:59:54.040 --> 01:59:55.800]   Or that I might happen to what do I?
[01:59:55.800 --> 01:59:57.480]   Or that Jeff might love.
[01:59:57.480 --> 02:00:00.160]   Because I, I want to live dangerously.
[02:00:00.160 --> 02:00:03.480]   I can't tell you how many of these I own and use every day.
[02:00:03.480 --> 02:00:04.480]   Me too.
[02:00:04.480 --> 02:00:08.080]   Well, if you, I will say it's nice if you dig in on them.
[02:00:08.080 --> 02:00:13.760]   They explain what makes them bad, which like for the ANOVA sous vide, one of the issues
[02:00:13.760 --> 02:00:15.320]   is encryption.
[02:00:15.320 --> 02:00:17.760]   Do you care if you're a sous vide cooker does that?
[02:00:17.760 --> 02:00:18.760]   I don't know.
[02:00:18.760 --> 02:00:19.760]   It's good.
[02:00:19.760 --> 02:00:24.000]   But things like, oh, here's the bike that I ride in a, you know, to stay in shape, the
[02:00:24.000 --> 02:00:26.560]   peloton bike and why is it creepy?
[02:00:26.560 --> 02:00:30.560]   Well, you know, because I don't know company.
[02:00:30.560 --> 02:00:31.560]   I don't know.
[02:00:31.560 --> 02:00:35.000]   And you actually can weigh into how creepy is.
[02:00:35.000 --> 02:00:36.000]   Yeah, it is.
[02:00:36.000 --> 02:00:37.000]   Because you can vote here.
[02:00:37.000 --> 02:00:38.000]   I'm going to vote.
[02:00:38.000 --> 02:00:39.520]   There's a scale on here that it says.
[02:00:39.520 --> 02:00:40.520]   I don't think it says.
[02:00:40.520 --> 02:00:46.680]   But, but I guess I'm all alone because according to voters, anyway, this is.
[02:00:46.680 --> 02:00:48.360]   So what is the, what is the creepiest thing?
[02:00:48.360 --> 02:00:50.240]   I think it's the Facebook portal.
[02:00:50.240 --> 02:00:52.120]   It's not even on here yet.
[02:00:52.120 --> 02:00:54.920]   Wait, what about the park or teddy bear?
[02:00:54.920 --> 02:00:56.840]   Oh, there's a good one.
[02:00:56.840 --> 02:00:57.840]   Is that, is that listening?
[02:00:57.840 --> 02:00:59.200]   No, it's actually not that creepy.
[02:00:59.200 --> 02:01:03.160]   It's a classic teddy bear, no built in Wi-Fi or Bluetooth, no batteries.
[02:01:03.160 --> 02:01:04.160]   But there is an app.
[02:01:04.160 --> 02:01:05.160]   The only reason.
[02:01:05.160 --> 02:01:08.960]   And when you point the app to a tummy, it starts to play.
[02:01:08.960 --> 02:01:13.920]   The only reason it's a problem is because it's privacy policy is written for people who,
[02:01:13.920 --> 02:01:16.160]   it's, it's a difficult to understand privacy policy.
[02:01:16.160 --> 02:01:17.160]   Right.
[02:01:17.160 --> 02:01:20.400]   That is really the only reason it's on here, which is why I'm kind of like, eh, Mozilla,
[02:01:20.400 --> 02:01:22.400]   I love you, but.
[02:01:22.400 --> 02:01:24.000]   So it's, it's things that are.
[02:01:24.000 --> 02:01:25.440]   Just go panic, perhaps.
[02:01:25.440 --> 02:01:31.760]   A little creepy to somewhat creepy to Lord, you don't want that house.
[02:01:31.760 --> 02:01:34.560]   Here's the wise cam, which we all know and love.
[02:01:34.560 --> 02:01:37.840]   It's on here because it's privacy level is difficult.
[02:01:37.840 --> 02:01:39.520]   So it's grade 13 privacy.
[02:01:39.520 --> 02:01:41.080]   It's you love, basically.
[02:01:41.080 --> 02:01:44.800]   But it also shares your information with third parties for unexpected reasons.
[02:01:44.800 --> 02:01:46.320]   Done, done, done.
[02:01:46.320 --> 02:01:53.320]   And I have read the wise cam privacy policy and I do not actually, I've read them and
[02:01:53.320 --> 02:01:54.480]   I don't recall that in there.
[02:01:54.480 --> 02:01:56.080]   And I've got to go back and I'm like, really?
[02:01:56.080 --> 02:01:59.040]   Well, this is for the tin foil hat crowd.
[02:01:59.040 --> 02:02:00.360]   Well, I don't know.
[02:02:00.360 --> 02:02:02.360]   I mean, it a grade 13 privacy policy.
[02:02:02.360 --> 02:02:03.560]   It's possible I missed it.
[02:02:03.560 --> 02:02:04.560]   Yeah.
[02:02:04.560 --> 02:02:07.520]   Because, you know, yeah, a little techno panic.
[02:02:07.520 --> 02:02:08.520]   Oh, yeah.
[02:02:08.520 --> 02:02:12.560]   So like I said, I have pretty much all of this stuff.
[02:02:12.560 --> 02:02:18.320]   So I'm like, I have nine out of 10 of those devices in my home right now.
[02:02:18.320 --> 02:02:19.320]   Yeah.
[02:02:19.320 --> 02:02:20.320]   It has toys.
[02:02:20.320 --> 02:02:21.320]   It has smart home.
[02:02:21.320 --> 02:02:23.920]   It has entertainment, wearables, health and exercise.
[02:02:23.920 --> 02:02:27.080]   It's pretty much everything that phones home.
[02:02:27.080 --> 02:02:29.720]   And some things that don't like that teddy bear.
[02:02:29.720 --> 02:02:33.600]   Ladies and gentlemen, was that the story you wanted to bring up?
[02:02:33.600 --> 02:02:37.960]   No, my story is the Virgis story on the kilogram being dead long lived.
[02:02:37.960 --> 02:02:39.840]   Wasn't that fascinating?
[02:02:39.840 --> 02:02:42.560]   That's just everyone should go read it because it's awesome.
[02:02:42.560 --> 02:02:51.680]   So until actually today, the term kilogram is defined by an actual piece of metal in a
[02:02:51.680 --> 02:02:56.320]   triple sealed vacuum dome in Paris.
[02:02:56.320 --> 02:03:00.120]   But there's a problem every time they bring it out to weigh it, which is like every 10
[02:03:00.120 --> 02:03:02.920]   years, it's a little it's a little bit less.
[02:03:02.920 --> 02:03:09.920]   It actually it's the last physical object that defines these key measurements.
[02:03:09.920 --> 02:03:13.520]   Like the meter is now defined by how long it takes light to travel a certain amount of
[02:03:13.520 --> 02:03:14.520]   time.
[02:03:14.520 --> 02:03:19.720]   This is the last one that's actually physical and they're going to change it to a based on
[02:03:19.720 --> 02:03:24.000]   physics instead of an actual thing.
[02:03:24.000 --> 02:03:25.480]   And it's very cool.
[02:03:25.480 --> 02:03:28.520]   And it gives the history of why we have the metric system.
[02:03:28.520 --> 02:03:30.520]   It's a really great article.
[02:03:30.520 --> 02:03:35.560]   And I feel like you guys who listen to the show are probably like the type of people
[02:03:35.560 --> 02:03:38.720]   who would be like, yeah, this is awesome.
[02:03:38.720 --> 02:03:45.000]   Well, and interestingly, even in the US where we don't use kilos, we still use this measurement.
[02:03:45.000 --> 02:03:47.000]   Because physics, because science.
[02:03:47.000 --> 02:03:49.000]   You know, because science, right?
[02:03:49.000 --> 02:03:52.280]   Because there are people who do use the metric system in the US.
[02:03:52.280 --> 02:03:53.280]   Strange.
[02:03:53.280 --> 02:03:55.200]   All the right thinking people of the world.
[02:03:55.200 --> 02:03:59.240]   Oh, come on, Stacy, you don't when you hear the weather, you don't say, well, what is
[02:03:59.240 --> 02:04:00.240]   that?
[02:04:00.240 --> 02:04:01.240]   And Celsius.
[02:04:01.240 --> 02:04:04.800]   No, but I do I would be a proponent of moving to the metric system.
[02:04:04.800 --> 02:04:05.800]   I really would.
[02:04:05.800 --> 02:04:07.800]   I mean, sensible person would be.
[02:04:07.800 --> 02:04:09.720]   See, they ain't going to happen.
[02:04:09.720 --> 02:04:10.720]   I know.
[02:04:10.720 --> 02:04:11.720]   All right.
[02:04:11.720 --> 02:04:17.880]   It'd be funny if how would you guys react if like President Trump said, you know, we're
[02:04:17.880 --> 02:04:19.760]   going to move to the metric system and everybody moved.
[02:04:19.760 --> 02:04:23.600]   It was like that was his that like, would you change your tune?
[02:04:23.600 --> 02:04:27.760]   You know, so he actually talked about reducing the number of guns out there.
[02:04:27.760 --> 02:04:29.680]   And I was like, you know what?
[02:04:29.680 --> 02:04:32.480]   That actually might be his Nixon in China kind of moment for me.
[02:04:32.480 --> 02:04:37.240]   I'd be like, yeah, okay, he's just crazy enough that, you know, he could pose anything.
[02:04:37.240 --> 02:04:38.240]   You don't know.
[02:04:38.240 --> 02:04:39.920]   He could end daylight saving time.
[02:04:39.920 --> 02:04:43.120]   And I mean, that'd be nice.
[02:04:43.120 --> 02:04:44.120]   Right.
[02:04:44.120 --> 02:04:45.120]   Right.
[02:04:45.120 --> 02:04:47.880]   I'd have to change my tune.
[02:04:47.880 --> 02:04:52.080]   Ladies and gentlemen, I just do that to Ryall Jeff.
[02:04:52.080 --> 02:04:56.120]   Ladies and gentlemen, that concludes this portion of the program.
[02:04:56.120 --> 02:05:02.440]   You may now go consume queso morphine's Stacy Higginbotham.
[02:05:02.440 --> 02:05:08.520]   She is at Stacy and IOT.com at gigastacey on Twitter, Jeff Jarvis, professor.
[02:05:08.520 --> 02:05:13.120]   The Leonard Tao, professor of journalistic innovation, the Craig Newmark graduate school
[02:05:13.120 --> 02:05:14.920]   of journalism at the city university in New York.
[02:05:14.920 --> 02:05:17.800]   And he's at buzz machine.com, the author of many fine.
[02:05:17.800 --> 02:05:19.120]   Are you working on a new book?
[02:05:19.120 --> 02:05:20.640]   You said you were, I thought.
[02:05:20.640 --> 02:05:22.960]   I think we're thinking Gutenberg.
[02:05:22.960 --> 02:05:24.440]   Nice.
[02:05:24.440 --> 02:05:26.680]   Like a bio.
[02:05:26.680 --> 02:05:28.480]   The end of the age.
[02:05:28.480 --> 02:05:30.680]   Oh, I would read that.
[02:05:30.680 --> 02:05:33.120]   First thing I write it.
[02:05:33.120 --> 02:05:34.640]   Get cracking, Jeff.
[02:05:34.640 --> 02:05:35.640]   Cracking.
[02:05:35.640 --> 02:05:37.120]   I'm doing my research test.
[02:05:37.120 --> 02:05:39.120]   I haven't read a book.
[02:05:39.120 --> 02:05:40.120]   Oh, yeah.
[02:05:40.120 --> 02:05:42.040]   I've been reading lots of stuff.
[02:05:42.040 --> 02:05:43.800]   It must be a good life being a professor.
[02:05:43.800 --> 02:05:44.800]   What do you think?
[02:05:44.800 --> 02:05:49.040]   I mean, I grew up, my dad's a professor.
[02:05:49.040 --> 02:05:51.240]   He was, I grew up my whole life in academia.
[02:05:51.240 --> 02:05:52.680]   I know, I know what you mean, Jeff.
[02:05:52.680 --> 02:05:55.000]   I know what the, means.
[02:05:55.000 --> 02:05:56.800]   Thank you, everybody, for joining us.
[02:05:56.800 --> 02:06:01.480]   We do this week in Google every Wednesday, 130 Pacific, 430 Eastern, 2030.
[02:06:01.480 --> 02:06:03.320]   I'm sorry, 2,130 now.
[02:06:03.320 --> 02:06:06.680]   UTC if you want to come by and say, hi, please do.
[02:06:06.680 --> 02:06:11.160]   We'll break out the kilogram scale for you.
[02:06:11.160 --> 02:06:13.360]   You can watch at twit.tv/live.
[02:06:13.360 --> 02:06:14.520]   You can listen there too.
[02:06:14.520 --> 02:06:19.880]   You can also chat at IRC.twit.tv with all the other people listening and watching live.
[02:06:19.880 --> 02:06:25.560]   On demand versions available at twit.tv/twig or use your favorite podcast application and
[02:06:25.560 --> 02:06:28.000]   subscribe that way you'll have the latest version.
[02:06:28.000 --> 02:06:29.880]   The minute it's available.
[02:06:29.880 --> 02:06:33.280]   Have a wonderful, wonderful evening.
[02:06:33.280 --> 02:06:38.560]   I'll be back here on Saturday.
[02:06:38.560 --> 02:06:41.480]   Actually you might want to tune in for triangulation this Friday.
[02:06:41.480 --> 02:06:47.360]   Kyle Wiens of I Fix It will join us to talk about the status of right to repair movement.
[02:06:47.360 --> 02:06:50.160]   It took a big setback recently from Apple.
[02:06:50.160 --> 02:06:51.160]   Wait, what?
[02:06:51.160 --> 02:06:54.040]   I thought we just had a big excitement.
[02:06:54.040 --> 02:06:55.040]   You got to listen?
[02:06:55.040 --> 02:06:57.760]   Yes, and Kyle was very involved in that.
[02:06:57.760 --> 02:06:59.280]   Kyle was very involved in that.
[02:06:59.280 --> 02:07:07.160]   We'll be talking about the Apple T2 chip which Apple has admitted will be used to break your
[02:07:07.160 --> 02:07:11.720]   machine if you dare to use third party repair.
[02:07:11.720 --> 02:07:14.760]   Some cases.
[02:07:14.760 --> 02:07:18.240]   That's Friday, 3 p.m. Pacific, 6 p.m. Eastern, 2300 U.S.D.C.
[02:07:18.240 --> 02:07:19.240]   Thank you everybody.
[02:07:19.240 --> 02:07:21.640]   We'll see you next time on This Week in Google.
[02:07:21.640 --> 02:07:22.400]   Bye bye.
[02:07:22.400 --> 02:07:32.400]   [Music]

