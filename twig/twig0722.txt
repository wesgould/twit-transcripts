;FFMETADATA1
title=The Rupert Murdoch of Hot Dogs
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2023-06-29
track=722
language=English
genre=Podcast
comment=<p>SCOTUS and cyberstalkers, AI books on Amazon, Pixel Tablet review</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf60.3.100

[00:00:00.000 --> 00:00:06.780]   It's time for Twig this week in Google Jeff Jarvis's here and Pruitt's in the house Stacy Higginbotham's here
[00:00:06.780 --> 00:00:15.280]   We'll talk about two Supreme Court cases one that's bad for cyberstalking victims one that's good for Google the AI battle
[00:00:15.280 --> 00:00:17.840]   centering on China now
[00:00:17.840 --> 00:00:24.260]   How a movie company plans to use AI to decide what films to make and what's happening to TCM
[00:00:24.260 --> 00:00:26.860]   It's all coming up next on twig
[00:00:29.360 --> 00:00:32.520]   Podcasts you love from people you trust
[00:00:32.520 --> 00:00:35.320]   This is twig
[00:00:35.320 --> 00:00:47.520]   This is twig this week in Google episode 722 recorded Wednesday June 28th
[00:00:47.520 --> 00:00:51.760]   2023 the Rupert Murdoch of hot dogs
[00:00:51.760 --> 00:00:58.400]   This episode of this week in Google is brought to you by the AWS insiders podcast
[00:00:58.560 --> 00:01:05.000]   search for AWS insiders in your podcast player or visit cloud fix.oria.com
[00:01:05.000 --> 00:01:12.240]   Slash podcast will also include a link in the show notes and our thanks to AWS insiders for their support and
[00:01:12.240 --> 00:01:13.880]   by
[00:01:13.880 --> 00:01:21.520]   Brooke linen summer is in full swing and Brooke linen is here to help you swap out winter warmth for easy breezy comfort
[00:01:21.520 --> 00:01:28.520]   Where their award-winning sheets and home essentials visit Brooke linen calm today and get $20 off plus free shipping on orders
[00:01:28.520 --> 00:01:31.840]   of $100 or more with the code twig and
[00:01:31.840 --> 00:01:33.620]   by
[00:01:33.620 --> 00:01:42.080]   ACI learning CIOs and CSOs agree that attracting and retaining talent is critical with an average completion rate of over
[00:01:42.080 --> 00:01:48.180]   80% your team deserves the entertaining and cutting-edge training that they want fill out the form at go.ac
[00:01:48.180 --> 00:01:53.840]   I learning comm slash twit for more information on a free two-week training trial for your team
[00:01:55.640 --> 00:01:59.520]   It's time for twig this week in Google the show where you can't talk about everything but Google
[00:01:59.520 --> 00:02:05.840]   They'll be a little Google in the show today. Hello everybody. I'm Leo LaPorte. Thank you very much for filling in for me
[00:02:05.840 --> 00:02:12.280]   Jason Howell last week. I ran out the door went to Disneyland and VidCon together
[00:02:12.280 --> 00:02:17.880]   Stacy had the week off too, but she's here today. Thank you Stacy. You think about them. You all left us
[00:02:17.880 --> 00:02:22.040]   I'm glad you're here Stacy because I had I have questions for you actually
[00:02:23.000 --> 00:02:29.560]   Oh, are they about chips or broadband they're about home automation, but I don't get to that. Yeah
[00:02:29.560 --> 00:02:36.240]   She is the host of the IOT show with Kevin Tofel also her website happens to be Stacy on IOT
[00:02:36.240 --> 00:02:41.800]   So I guess you might know something about I O T. I know a little something about that a little something something
[00:02:41.800 --> 00:02:45.720]   Also with us mr. Ant Pruitt hands-on photography. I'm sorry
[00:02:45.720 --> 00:02:49.960]   No longer a show, but Ant still with us. Thank God. He is the
[00:02:50.600 --> 00:02:55.600]   Manager of the still here still strong still strong damn it staying strong
[00:02:55.600 --> 00:03:00.280]   In fact, you're gonna big data. Mar you're gonna do Stacy's book Club at 9 a.m. Pacific
[00:03:00.280 --> 00:03:07.240]   Mm-hmm the new and I didn't know she's held book the Transformers. Yeah, then at one terra formers
[00:03:07.240 --> 00:03:10.240]   And that one percent one pimp Pacific you're gonna view
[00:03:10.240 --> 00:03:17.160]   Hugh Howie. Yes, who is the author silo? Yes? Well offer of wool wool, which is the silo
[00:03:17.160 --> 00:03:20.040]   TV series is based based on wool
[00:03:20.560 --> 00:03:22.480]   Wool mm-hmm
[00:03:22.480 --> 00:03:28.240]   That's gonna be great. I'm pretty pumped about that. Do you fit in the triangulation chair?
[00:03:28.240 --> 00:03:34.000]   Have you tried it? Yeah, I can sit in those chairs, but man, they're on the dad gum floor. I'm I so just sit on the floor
[00:03:34.000 --> 00:03:42.360]   Well, you do if you want to sit there you could take the dr. Evil chair. I still don't know how you sit in that chair either
[00:03:42.360 --> 00:03:48.480]   Well, what's wrong with this? That is not they all hate your that chair really. They'll say when you're going. Oh, you know why?
[00:03:49.240 --> 00:03:51.960]   After 15 years is sculpted to my figure
[00:03:51.960 --> 00:03:59.800]   I can't get comfortable in this thing. I don't understand
[00:03:59.800 --> 00:04:06.320]   That is Jeff Jarvis. He is the Leonard Ta professor for journalistic innovation at the Craig Newmark
[00:04:06.320 --> 00:04:14.440]   Graduate School of Journalism at the City University of New York may I be so craftous to plug down because I have the day has come
[00:04:16.280 --> 00:04:21.960]   And I just today got a discount code. Oh my friends here now that I've already bought it
[00:04:21.960 --> 00:04:24.700]   I know exactly but I just gave it to me. So it's it's
[00:04:24.700 --> 00:04:27.880]   TGP JJ 23
[00:04:27.880 --> 00:04:33.120]   US in my god Gutenberg breath is JJ 23 us or UK and
[00:04:33.120 --> 00:04:38.960]   Where do we use that? 25% at the blooms very you go to Gutenberg breath is calm
[00:04:38.960 --> 00:04:42.560]   Go to bloom is very very link
[00:04:44.360 --> 00:04:50.440]   Oh, it's actually on the website. So yeah, you don't have to even worry about it. Don't write it down
[00:04:50.440 --> 00:04:56.600]   It's right there. Just copy it. Oh, nice. Go pasted in. Okay, I sunded that good. Thank you 25% off
[00:04:56.600 --> 00:05:00.680]   But I've ever sent off well, I paid full price at Amazon, but that's okay
[00:05:00.680 --> 00:05:05.720]   Mine comes the 30th day after tomorrow. So good supporting the creators. Yeah
[00:05:05.720 --> 00:05:13.240]   Oh, thank you and there's and apparently there's already a New Yorker cartoon about Jeff Jarvis
[00:05:13.240 --> 00:05:15.240]   I
[00:05:15.240 --> 00:05:23.880]   You got this historical irony included with every ebook purchase it says I'm up there in the in the corner
[00:05:23.880 --> 00:05:30.360]   Strike straight in that's very good the twig the actor cartoon
[00:05:30.360 --> 00:05:34.520]   If you're not watching the video
[00:05:34.520 --> 00:05:36.680]   It's in the discord if we'll put it in the
[00:05:36.680 --> 00:05:41.560]   You know what let's make this the thumbnail that that's gotta be it. Yeah. Yeah
[00:05:42.840 --> 00:05:46.760]   Gutenberg prints is yeah, this is celebration week. Yeah, that's wonderful. Thank you
[00:05:46.760 --> 00:05:49.800]   Congratulations, Jeff. I will be in London
[00:05:49.800 --> 00:05:55.400]   Monday the third speaking with Alan rusperger prospect if you're around by all means come
[00:05:55.400 --> 00:05:59.720]   I'll put it up on my blog. I mean on my twitter feed so you can find the details nice
[00:05:59.720 --> 00:06:02.600]   And on the 8th
[00:06:02.600 --> 00:06:07.720]   1 o'clock saturday. I will be to museum imprinting and haverhill mass with clenn fleshman
[00:06:08.680 --> 00:06:16.280]   Marching with karaise and dug wilson the creator of the lana type film. Nice. This is exciting
[00:06:16.280 --> 00:06:20.440]   Yeah, that that might be the nerdiest group like
[00:06:20.440 --> 00:06:25.160]   Printing it nerd the nerdiest font print lino type
[00:06:25.160 --> 00:06:32.920]   I never heard I love it. It really is it's gonna be amazing. Yeah, of course glenn and marchin are the authors of shift happens
[00:06:33.560 --> 00:06:38.600]   kickstarter beautiful to volume book about keyboards typewriters
[00:06:38.600 --> 00:06:42.920]   So the reason we're doing it then is because glenn and marchin are on their way up to the press
[00:06:42.920 --> 00:06:46.840]   I think it's in new hamster or or main or somewhere
[00:06:46.840 --> 00:06:50.440]   But they're gonna they're stopping off at the museum printing and they're gonna go there
[00:06:50.440 --> 00:06:53.640]   They're gonna spend like a week with press checks as they print the books
[00:06:53.640 --> 00:06:56.040]   Wow
[00:06:56.040 --> 00:06:58.040]   That's quality control man. Wow
[00:06:58.040 --> 00:07:00.520]   uh
[00:07:00.520 --> 00:07:06.120]   Our top story this week has time to choose a top story. There was a lot of big stories, but this one I thought was
[00:07:06.120 --> 00:07:11.080]   Importance and we'll talk about it on sunday because the briano who has been
[00:07:11.080 --> 00:07:16.920]   horribly stalked in her life will be uh one of the panelists on this week in and tech
[00:07:16.920 --> 00:07:21.000]   supreme court this week decided seven to two
[00:07:21.000 --> 00:07:23.640]   to protect stalkers
[00:07:23.640 --> 00:07:27.080]   If they don't know that their threats
[00:07:27.080 --> 00:07:29.080]   are
[00:07:29.080 --> 00:07:30.440]   deadly
[00:07:31.400 --> 00:07:36.680]   This is a weird case the case countermen versus calorado
[00:07:36.680 --> 00:07:39.400]   Uh a guy named billy raman counterman
[00:07:39.400 --> 00:07:48.040]   He was convicted under a calorado anti stalking law because he sent a you know ream of threatening facebook messages
[00:07:48.040 --> 00:07:50.600]   To a woman he never met
[00:07:50.600 --> 00:07:53.000]   Uh the calorado law
[00:07:53.000 --> 00:07:57.640]   Did not require the court to consider his mental state when he sent the messages
[00:07:57.720 --> 00:08:02.040]   It only had to consider his behavior and how the recipient would feel
[00:08:02.040 --> 00:08:03.800]   threatened
[00:08:03.800 --> 00:08:06.120]   Right, which seems to make sense. Yeah, but uh
[00:08:06.120 --> 00:08:13.320]   whether he repeatedly caught according to the law repeatedly contacted followed or surveilled his target in a way that would cause a
[00:08:13.320 --> 00:08:17.480]   Person distress. It was of course found guilty because she was
[00:08:17.480 --> 00:08:24.200]   highly distressed and by the way, I've been you know harassed at anybody in the public eye has had this happen
[00:08:24.200 --> 00:08:27.960]   I've had it happen for 40 years, you know, I once had a young woman
[00:08:27.960 --> 00:08:30.840]   Uh praying for me on my stoop of my house
[00:08:30.840 --> 00:08:37.000]   Following me to grocery stores and commenting on my food and thing and I didn't blame her she was obviously mentally ill
[00:08:37.000 --> 00:08:41.080]   Um had she been others had she brought a knife
[00:08:41.080 --> 00:08:45.960]   Different story. I would have been felt threatened. Now. Here's the same the supreme court
[00:08:45.960 --> 00:08:50.440]   Said that if countermen
[00:08:50.440 --> 00:08:52.600]   didn't have any awareness
[00:08:52.840 --> 00:08:54.840]   of his threatening
[00:08:54.840 --> 00:08:58.040]   Of the threatening character of his statements. It would not
[00:08:58.040 --> 00:09:03.720]   be a problem and in fact any restriction in his speech would be a violation of his first amendment rights
[00:09:03.720 --> 00:09:07.000]   the majority opinion written by
[00:09:07.000 --> 00:09:09.000]   justice, uh, elana kegan
[00:09:09.000 --> 00:09:13.480]   Said the state prosecuted him in accordance with an objective standard
[00:09:13.480 --> 00:09:17.400]   Uh that didn't show any awareness
[00:09:17.400 --> 00:09:22.040]   Or did not have to show any awareness on his part of statements threatening character
[00:09:22.120 --> 00:09:25.880]   That's a violation of the first amendment, but bear us evidence the dissent
[00:09:25.880 --> 00:09:32.120]   Was did say that someone who's delusional can be dangerous. Yeah, and
[00:09:32.120 --> 00:09:37.720]   But by the way, both amy kony kony barrit and clarin stomens have probably had arris
[00:09:37.720 --> 00:09:42.120]   Nursers and people attacking them and i know they have in restaurants and so forth
[00:09:42.120 --> 00:09:43.080]   Well
[00:09:43.080 --> 00:09:45.080]   and then if it's a
[00:09:45.080 --> 00:09:47.560]   it's always been in the case in in
[00:09:49.560 --> 00:09:54.280]   the case of the law that if if somebody tweets at us
[00:09:54.280 --> 00:09:56.600]   something
[00:09:56.600 --> 00:09:58.600]   a bodily harm
[00:09:58.600 --> 00:10:03.160]   That that's considered a threat and prosecutable and i think it should be
[00:10:03.160 --> 00:10:06.120]   Uh
[00:10:06.120 --> 00:10:10.440]   This sounds like if if you're crazy enough not to know that that's a threat. Oh, you're all right then
[00:10:10.440 --> 00:10:15.880]   I think about let let's stay sego first because she would be a repertor go ahead
[00:10:16.600 --> 00:10:20.280]   Well, there's a couple things one is I know that like with
[00:10:20.280 --> 00:10:29.240]   Like protesters outside of an abortion clinic a lot of times they will justify their protests and getting very close in your face and screaming at you
[00:10:29.240 --> 00:10:37.320]   As a first amendment right but legally they're harassing you and it does count as harassment so
[00:10:37.320 --> 00:10:41.560]   I'm really struggling with this because
[00:10:43.960 --> 00:10:47.720]   Because it's terrifying being on the other end yes any of this and it's
[00:10:47.720 --> 00:10:53.960]   And it's not a first amendment right I mean we we talk about you know yelling fire in a crowded theater
[00:10:53.960 --> 00:10:57.560]   And so I I really I don't understand the legal distinction being made here
[00:10:57.560 --> 00:10:59.560]   So I'm trying to figure that out a little more
[00:10:59.560 --> 00:11:06.600]   Joseph would say that the fire the theater thing is actually i'm just no more but we'll live at the side probably yes
[00:11:06.600 --> 00:11:12.280]   Yes, so I i'm very torn in the state because i'm a big believer in the first federal freedom of expression
[00:11:12.520 --> 00:11:15.720]   I think we head to a doctrine
[00:11:15.720 --> 00:11:20.600]   Not just in the court now, but also you know with you on musk and company
[00:11:20.600 --> 00:11:27.960]   Is this belief that free speech means there's no consequence to speech? I have free speech. Yeah, but if I
[00:11:27.960 --> 00:11:31.640]   Lible you if I cause damage if I incite
[00:11:31.640 --> 00:11:33.560]   Violence
[00:11:33.560 --> 00:11:37.560]   There are consequences or simply if you think i'm a jerk and say so
[00:11:37.560 --> 00:11:40.520]   Or disagree with me. That's a consequence
[00:11:41.080 --> 00:11:44.360]   But we moved to consequence less speech and i'm not sure that's
[00:11:44.360 --> 00:11:48.280]   Um first I agree with you Stacy because i'm not sure that's first amendment really
[00:11:48.280 --> 00:11:51.960]   Is this well in put in place for you put it in this
[00:11:51.960 --> 00:11:56.840]   Antisty no no that's the point a del here here's uh what um
[00:11:56.840 --> 00:12:01.960]   uh professor at george washington university said in there amicus brief
[00:12:01.960 --> 00:12:06.840]   A delusional speaker may lack awareness of the threatening nature of her speech
[00:12:08.280 --> 00:12:15.080]   Uh a devious speaker may strategically disclaim such awareness right right that's a bad way
[00:12:15.080 --> 00:12:18.520]   Devious that's a common um
[00:12:18.520 --> 00:12:21.000]   uh strategy of the right
[00:12:21.000 --> 00:12:28.200]   Mm-hmm. Uh and which is oh, I'm not I didn't mean I get this a lot. Oh, I didn't mean harm
[00:12:28.200 --> 00:12:31.960]   Mm-hmm. You know you misinterpret you. I'm just speaking push it should be dead
[00:12:31.960 --> 00:12:37.160]   I was just pointing out that this is where your house is and that you don't exactly lock your door at night
[00:12:37.160 --> 00:12:42.120]   And there's an unlatch gate and you have a dog that might eat. I'm not advocating violence
[00:12:42.120 --> 00:12:44.200]   I'm just saying
[00:12:44.200 --> 00:12:48.200]   You know, I think you're exactly right stacey. That's exactly that how it's used
[00:12:48.200 --> 00:12:49.800]   um
[00:12:49.800 --> 00:12:52.200]   the eff and the aclu both
[00:12:52.200 --> 00:12:58.520]   Uh sided with countermen and thought that the you know free speech rest were abridged. I think this is insane
[00:12:58.520 --> 00:13:05.160]   Uh because you know the reason you and I stacey know about this is we've been harassed
[00:13:05.880 --> 00:13:11.560]   And we've had these oblique threats, which you know, I think don't rise to the test of countermen
[00:13:11.560 --> 00:13:17.720]   And uh, so they would be allowed as free speech. You absolutely have the right to say leo is a complete jerk
[00:13:17.720 --> 00:13:22.120]   You have the right to say that. Mm-hmm. And I would never disagree
[00:13:22.120 --> 00:13:26.280]   But if you said by the way leo you might disagree with that statement
[00:13:26.280 --> 00:13:31.560]   I don't even disagree with it. Uh if you say where I live and the door is unlocked
[00:13:32.440 --> 00:13:38.680]   That's an hour. That's uh a threatless threat and it's not nice and it should be absolutely
[00:13:38.680 --> 00:13:42.840]   You should be liable. There should be just as you said jeff consequences for what you say
[00:13:42.840 --> 00:13:47.160]   I understand that the the issue which is that you have the right to free speech
[00:13:47.160 --> 00:13:54.440]   and well in jeff with the consequences we're talking about two kind of different things right free speech is
[00:13:54.440 --> 00:13:57.480]   um
[00:13:58.280 --> 00:14:02.760]   You can have concept but you can't have free speech consequences from the government
[00:14:02.760 --> 00:14:05.160]   Government can't so but in this case
[00:14:05.160 --> 00:14:10.280]   Well the government's prosecuting. So that's why yeah, the government's prosecuting this guy
[00:14:10.280 --> 00:14:14.760]   But i'm also like this guy isn't it's my first moment. Yeah, but
[00:14:14.760 --> 00:14:19.160]   I don't yeah, I like it's not easy
[00:14:19.160 --> 00:14:22.280]   If someone's well it is someone's mentally ill and delusional
[00:14:22.280 --> 00:14:26.680]   I feel like you can take a case all the way through prosecute them and say I mean
[00:14:27.240 --> 00:14:33.080]   If their defense is indeed, I didn't know I was like genuinely I didn't realize I was threatening this person
[00:14:33.080 --> 00:14:35.000]   I feel like that's
[00:14:35.000 --> 00:14:36.920]   In sentencing that can come through
[00:14:36.920 --> 00:14:43.960]   But to strip the ability to prosecute someone for that and to have real consequences that is terrifying is
[00:14:43.960 --> 00:14:45.320]   I agree
[00:14:45.320 --> 00:14:50.840]   I mean aclu and eff argued that if anyone can be found guilty of making a threat based on how the
[00:14:50.840 --> 00:14:53.480]   threat is received
[00:14:54.120 --> 00:15:00.200]   There's not not how it was intended, but it's a reasonable person standard. It's not like just like
[00:15:00.200 --> 00:15:03.000]   You hurt my feelings. Yeah
[00:15:03.000 --> 00:15:14.200]   Uh, I just this you know, this was the concern that Barrett justices Barrett and uh thomas had which is that it opens the door to all sorts of harassment
[00:15:14.200 --> 00:15:17.560]   I wonder where catholic ellison mike bazna came out on it
[00:15:17.560 --> 00:15:23.720]   Oh, I don't know. I should look and see I have I didn't look at it. I didn't look at tectert and see what tectert has to say
[00:15:23.720 --> 00:15:29.800]   But it's a class in any event. This is it said and it was seven. It wasn't a it wasn't close. It was 72
[00:15:29.800 --> 00:15:32.920]   um
[00:15:32.920 --> 00:15:34.920]   He you know countermen is now
[00:15:34.920 --> 00:15:36.040]   uh
[00:15:36.040 --> 00:15:37.880]   off the hook
[00:15:37.880 --> 00:15:43.320]   And uh, so I think this opens the door in a social media era for all kinds of harassment
[00:15:43.320 --> 00:15:47.080]   I mean the harassers are smart. They know not to make a threat
[00:15:47.080 --> 00:15:51.240]   Of you know, and that's always been the case. Yeah, there are smart harassers
[00:15:51.320 --> 00:15:56.520]   And then those are the ones that are truly terrifying. Yes in all honesty the smartest harassers probably
[00:15:56.520 --> 00:16:01.000]   Don't actually want to physically harm you because they're too smart and they recognize the consequence
[00:16:01.000 --> 00:16:03.640]   What they want to do is make your life absolutely no, I didn't just
[00:16:03.640 --> 00:16:06.200]   Verify you will
[00:16:06.200 --> 00:16:07.720]   Yeah
[00:16:07.720 --> 00:16:11.880]   Anyway, the just uh, you know the supreme court has been very interesting this
[00:16:11.880 --> 00:16:16.440]   Full surprises. I think I was saying with it with the with the voting decision
[00:16:16.440 --> 00:16:20.680]   This week. I think that the public pressure on the court has an impact
[00:16:21.640 --> 00:16:24.120]   Are you gonna try to prove no? No, we're okay. We're all right
[00:16:24.120 --> 00:16:31.000]   We'll surprise you a number of people pointed out though that the way that decision went down could open the door to other
[00:16:31.000 --> 00:16:34.200]   Probability rights. Yes issues. Yes
[00:16:34.200 --> 00:16:38.760]   Yeah, it it opened a portal to a bad doctrine, right?
[00:16:38.760 --> 00:16:45.400]   So it's open to portals to a bad doctrine. Oh, you know how that is. It's like the guardian of the galaxy's ride. You're really
[00:16:45.400 --> 00:16:49.160]   You're really in trouble now good. So tight right there
[00:16:49.160 --> 00:16:51.160]   Yeah
[00:16:51.160 --> 00:16:53.240]   I love it. I'm like, oh
[00:16:53.240 --> 00:16:55.960]   Google google one in the supreme court this week
[00:16:55.960 --> 00:17:00.440]   It well one in a way the court rejected a lawsuit
[00:17:00.440 --> 00:17:06.120]   Uh, do you remember this? Uh, there was a site called rap genius later just genius
[00:17:06.120 --> 00:17:08.760]   That was a great place to go to get the lyrics wrong
[00:17:08.760 --> 00:17:13.320]   And they did a clever thing in the lyrics. They stuck some uh some hidden
[00:17:13.320 --> 00:17:17.720]   Secrets in there and they found that google was actually just lifting
[00:17:18.520 --> 00:17:22.120]   the lyrics from genius and putting them, you know in the google search results
[00:17:22.120 --> 00:17:29.720]   So the genius media group sued alphabet saying you stole millions of song lyrics
[00:17:29.720 --> 00:17:34.680]   Uh, they they lost in court because it turns out
[00:17:34.680 --> 00:17:38.040]   Genius didn't own the rights to any
[00:17:38.040 --> 00:17:42.040]   Exactly what I said at the time. That's not old
[00:17:42.040 --> 00:17:48.360]   That's the old news. So genius wasn't saying we have the copyright. They said google violated our contract by scrap
[00:17:48.680 --> 00:17:51.880]   lyrics and boosting them in google search without attribution
[00:17:51.880 --> 00:17:55.400]   Which caused millions of dollars and losses for the website
[00:17:55.400 --> 00:17:58.360]   It's lost it's been going on for years. Huh? Yeah, well
[00:17:58.360 --> 00:18:06.760]   Uh, well, this is the I think really that's the issue is these lyrics are floating around right the copyright holder is a song writer the publisher
[00:18:06.760 --> 00:18:09.160]   Right, not genius not google
[00:18:09.160 --> 00:18:13.800]   If that information is in the air, does it matter where google gets it from
[00:18:15.080 --> 00:18:18.440]   It now this is kind of goes back to link text because
[00:18:18.440 --> 00:18:23.800]   Google is eliminating a mo you know a click to genius to get that lyric
[00:18:23.800 --> 00:18:26.120]   By quoting the lyric on the search results
[00:18:26.120 --> 00:18:32.120]   You don't have to go to genius anymore and that's really what this came down to a lower court had ruled in favor of google
[00:18:32.120 --> 00:18:38.680]   The justices uh did not revive the lawsuit. They didn't give cert to uh to the appeal
[00:18:38.680 --> 00:18:42.840]   So it's over for that google's off the hook, but it's an it's an interesting
[00:18:43.320 --> 00:18:47.880]   I feel like if you're on the internet today if your core competency is just being a middleman
[00:18:47.880 --> 00:18:53.800]   Yeah, that is not that that is never going to be a tenable business model. Yeah, you were like the dropship of
[00:18:53.800 --> 00:18:58.040]   I don't think so I think newspapers can create real value
[00:18:58.040 --> 00:19:03.560]   They can but their past has been as middleman when it comes to advertising and to news
[00:19:03.560 --> 00:19:09.640]   Right, we run the ap you have to come to us to sell your car. Oh, you don't need to do either without us anymore
[00:19:10.120 --> 00:19:13.320]   I should point out as often the case with these kind of stories
[00:19:13.320 --> 00:19:16.200]   There is a deeper
[00:19:16.200 --> 00:19:19.240]   Subtle or legal issue at stake here than the obvious
[00:19:19.240 --> 00:19:22.840]   One genius was served suing
[00:19:22.840 --> 00:19:31.160]   Uh over contractual matter saying well, you know, our terms of service say google can't scrape the site and put the results up
[00:19:31.160 --> 00:19:34.760]   In fact, they say now the supreme court has opened the door to people
[00:19:34.760 --> 00:19:37.000]   uh, you know ignoring
[00:19:37.000 --> 00:19:38.840]   contractual uh
[00:19:38.840 --> 00:19:43.320]   That would be amazing actually. Yes, that's like oh, I I just clicked through or god help you
[00:19:43.320 --> 00:19:49.320]   You didn't actually even click through. Yeah, um, so the agreeing on the time that too to large language models
[00:19:49.320 --> 00:19:52.200]   Well before you do that because that's a whole nother
[00:19:52.200 --> 00:19:54.200]   Could for that
[00:19:54.200 --> 00:19:59.320]   Talking about a sidebar, but before you do that, let's just I just want to point out
[00:19:59.320 --> 00:20:01.560]   that uh
[00:20:01.560 --> 00:20:04.440]   Federal law this is a subtlety right federal law
[00:20:05.080 --> 00:20:08.520]   Pramp's lawsuits over issues that are similar to copyright
[00:20:08.520 --> 00:20:13.000]   So google was saying this genius is bringing a quasi copyright claim
[00:20:13.000 --> 00:20:16.040]   It's not really about the terms of contractor kind of saying hey
[00:20:16.040 --> 00:20:21.480]   We we kind of own the rights of these and and and I think that the the court and the lower court
[00:20:21.480 --> 00:20:28.840]   We're basically saying no to the law says very specifically you can't kind of infer some sort of protection because it's like copyright
[00:20:28.840 --> 00:20:32.440]   Uh, and that's why that's why they lost so yeah, wow
[00:20:34.280 --> 00:20:38.360]   By the way, the white house wanted uh wanted the court to skip the case
[00:20:38.360 --> 00:20:45.400]   Saying it was a poor vehicle to resolve the tension between copyright law and contractual rights
[00:20:45.400 --> 00:20:51.320]   That's an interesting just the copyright is just fascinating stuff. Sorry. Yeah, it's complicated
[00:20:51.320 --> 00:20:54.440]   I think it's the worst. Yeah, I agree. It's complicated
[00:20:54.440 --> 00:20:58.920]   All this stuff is complicated though, right lawyers lawyers and more
[00:20:59.720 --> 00:21:03.720]   Give me different give me complicated based on physics and hard science not
[00:21:03.720 --> 00:21:06.680]   Not imaginary legal concepts
[00:21:06.680 --> 00:21:13.640]   Then let's get into this AI thing that we were talking about stacy. You want a headache? I got a headache for you
[00:21:13.640 --> 00:21:16.680]   Um
[00:21:16.680 --> 00:21:19.320]   So we were ant
[00:21:19.320 --> 00:21:22.280]   This will give you a migraine
[00:21:22.280 --> 00:21:23.560]   Um
[00:21:23.560 --> 00:21:27.000]   We were talking about this before the show you probably heard a lot. Oh no
[00:21:27.400 --> 00:21:29.400]   Oh
[00:21:29.400 --> 00:21:34.520]   You did not see me. I'm like I cannot with this today. Uh, all right
[00:21:34.520 --> 00:21:40.680]   You're so interested in their bed about human consciousness. Let's do it. I won't do it if you don't want me to
[00:21:40.680 --> 00:21:48.200]   So we talk a lot about AI in fact, I'm really pleased Jason how all those shows also canceled
[00:21:48.200 --> 00:21:51.400]   I'm sorry both of you guys. It was just lack of uh
[00:21:51.400 --> 00:21:52.680]   Interestness
[00:21:52.680 --> 00:21:54.680]   Just bid this business. Um
[00:21:55.560 --> 00:21:58.440]   Well, and I'm not you know me. I don't I hate business
[00:21:58.440 --> 00:22:01.800]   but somebody's got to pay the bill
[00:22:01.800 --> 00:22:04.440]   and uh and unfortunately it's me so
[00:22:04.440 --> 00:22:05.720]   uh
[00:22:05.720 --> 00:22:09.160]   I kind of at least said I have to kind of pay attention to business and um, we just
[00:22:09.160 --> 00:22:15.560]   If shows don't get an audience and they don't get advertisers. We just can't keep doing them and I apologize
[00:22:15.560 --> 00:22:17.720]   The um and it's not you by the way, yeah, and it's not you by the way
[00:22:17.720 --> 00:22:22.120]   Yeah, and by you will like yeah, we want to keep everybody employed as best we can
[00:22:22.600 --> 00:22:25.960]   It's not you you're like the bob ross of photography. You are genius
[00:22:25.960 --> 00:22:30.040]   I remember the family had hair came here
[00:22:30.040 --> 00:22:36.520]   One of our first episodes of twigs you said that and it ended up being a thumbnail somebody made a weird
[00:22:36.520 --> 00:22:37.720]   With earlier
[00:22:37.720 --> 00:22:40.440]   This is really a bob ross with biceps or something. Yeah
[00:22:40.440 --> 00:22:43.400]   No, you are you have this channel
[00:22:43.400 --> 00:22:47.960]   Kind i'll never forget appreciation of the art and you're really good at it
[00:22:48.520 --> 00:22:51.240]   So but I think it's really more commentary on
[00:22:51.240 --> 00:22:54.520]   Because it wasn't the first time we've tried a photography right cast
[00:22:54.520 --> 00:23:00.920]   Catherine holland I did this weekend photography also failed because I just think I don't know what it is is your audience
[00:23:00.920 --> 00:23:04.280]   It's our audience. It's there are other photography shows
[00:23:04.280 --> 00:23:10.360]   Stop kelby does a bunch of them that maybe maybe they're doing better. I don't know I just don't know but for some reason
[00:23:10.360 --> 00:23:14.600]   We didn't attract an audience so and the same thing. I think android
[00:23:14.680 --> 00:23:20.200]   I hate to say it on the this weekend google, but I don't think I think android which is still the number one
[00:23:20.200 --> 00:23:23.000]   smartphone platform
[00:23:23.000 --> 00:23:26.680]   But phones in general aren't just aren't that interesting anymore?
[00:23:26.680 --> 00:23:30.520]   That's what we've been saying on this show a lot and and it was just reality that
[00:23:30.520 --> 00:23:35.560]   Jason company but the great thing was so I talked to Jason today and and he said
[00:23:35.560 --> 00:23:41.320]   The this title of the show dries them nuts because it's not about google change the name and leo likes it right
[00:23:42.120 --> 00:23:46.600]   Well, it's just it's not even a lot. I can't change it because I know it's stuck
[00:23:46.600 --> 00:23:51.720]   It's like Jason did I run with a name like shot by this it has to be good right? It's
[00:23:51.720 --> 00:24:00.440]   All right Jason had a show about android wasn't just about android right it was there was the relationships that he had on that show
[00:24:00.440 --> 00:24:07.800]   Oh, so it was as with any great podcast the people are what make a podcast great. Yeah, you know aunt Jason
[00:24:08.920 --> 00:24:14.120]   Win and and Ron and and previous hosts like florinced the never host like jr
[00:24:14.120 --> 00:24:16.760]   Michelle all these people are great. They had a great rapport
[00:24:16.760 --> 00:24:21.000]   But they just was a dwindling audience and I think it's just lack of interest in the subject manner
[00:24:21.000 --> 00:24:23.000]   I don't think it's as reflection on them. So
[00:24:23.000 --> 00:24:25.880]   And I didn't know that way
[00:24:25.880 --> 00:24:29.240]   It was a reflection on me. It's not no I did absolutely not
[00:24:29.240 --> 00:24:31.080]   so
[00:24:31.080 --> 00:24:35.880]   You know aunt's gonna very much be a part of the network going forward, especially in our club and here on twig
[00:24:36.520 --> 00:24:41.960]   Jason and jeff are working on a ai show, which means much needs to be done and the good news is
[00:24:41.960 --> 00:24:47.160]   Because we have the club. Thank god. We have the club. Thank you the club can subsidize
[00:24:47.160 --> 00:24:49.800]   an ai show even though there's
[00:24:49.800 --> 00:24:54.840]   You know it as any show when it starts there's a small audience and no advertising
[00:24:54.840 --> 00:24:58.280]   So this is going to be a club only exclusive for the time being
[00:24:58.280 --> 00:25:00.840]   And we're hoping to launch it soon
[00:25:00.840 --> 00:25:06.280]   But Jason spent 90 minutes yesterday in the and the discord talking to people about ideas
[00:25:06.440 --> 00:25:09.880]   But one of the ideas I thought was kind of interesting and I've been asking all our hosts this
[00:25:09.880 --> 00:25:13.400]   And stacy please recuse yourself if you want
[00:25:13.400 --> 00:25:22.600]   I'm ready. It's been my contention from the beginning that ai is very poorly named is nothing intelligent
[00:25:22.600 --> 00:25:28.440]   about chat gpt or stable diffusion all of these are
[00:25:28.440 --> 00:25:35.160]   computer programs that the distinction between these computer programs and traditional computer programs
[00:25:35.480 --> 00:25:37.160]   You know, I've written a lot of computer software
[00:25:37.160 --> 00:25:43.000]   You tell the in traditional programs you tell the computer exactly what to do if this happens do this if this happens do this
[00:25:43.000 --> 00:25:46.680]   And computers are really good at doing they only by the way
[00:25:46.680 --> 00:25:50.440]   even today a norm a computer can add
[00:25:50.440 --> 00:25:54.200]   Subtracts do division and multiplication in some cases
[00:25:54.200 --> 00:25:56.840]   Can move stuff around
[00:25:56.840 --> 00:26:01.800]   And can make decisions based on a condition. Mm-hmm. And it does all of that
[00:26:02.680 --> 00:26:10.760]   In concert so quickly that it looks like it's thinking or at least word processing or doing spreadsheet or playing a video game
[00:26:10.760 --> 00:26:13.000]   Because it's very very fast
[00:26:13.000 --> 00:26:14.920]   ai
[00:26:14.920 --> 00:26:19.640]   Is a little different and you I know you're an expert on this stacy because though if they've started using
[00:26:19.640 --> 00:26:22.280]   Data and training models
[00:26:22.280 --> 00:26:30.120]   Using generative adversarial networks or large language models or neural networks a variety of different well-known techniques
[00:26:31.080 --> 00:26:36.120]   And the difference is that instead of a human writing all the rules the computer can generate its own rules
[00:26:36.120 --> 00:26:41.080]   So alpha go is a good example, which is a google's uh deep mind
[00:26:41.080 --> 00:26:44.520]   Program that learned to play go by playing a lot of games
[00:26:44.520 --> 00:26:49.960]   They told it the rules of go which are very simple and then it played a billion games in four hours
[00:26:49.960 --> 00:26:55.400]   And actually taught itself to be better. That's pretty awesome. Some of those man's doing endless abt s yeah
[00:26:55.400 --> 00:26:56.440]   That's pretty awesome though
[00:26:56.440 --> 00:27:02.920]   But it generated in effect generated the same kind of thing that a human would write just a lot more of it a lot faster
[00:27:02.920 --> 00:27:05.560]   And so it created a ruleset
[00:27:05.560 --> 00:27:07.960]   in my opinion
[00:27:07.960 --> 00:27:09.960]   those things
[00:27:09.960 --> 00:27:11.720]   Are
[00:27:11.720 --> 00:27:15.480]   deterministic computer programs regardless of how it was created
[00:27:15.480 --> 00:27:19.560]   That don't approximate thinking in any way they just
[00:27:19.560 --> 00:27:25.400]   Are fast so it looks like it's writing it's regurgitating. It's but it's regurgitating. Oh wait
[00:27:26.200 --> 00:27:27.720]   so
[00:27:27.720 --> 00:27:31.080]   I you're not you're not wrong here except
[00:27:31.080 --> 00:27:36.840]   We're talking about intelligence and I don't think you're talking about two things
[00:27:36.840 --> 00:27:39.400]   We're talking about intelligence and then you're talking about thinking
[00:27:39.400 --> 00:27:43.000]   And I would definitely agree that a computer isn't thinking
[00:27:43.000 --> 00:27:47.560]   but I would say if you think of intelligence and you define it as being able to
[00:27:47.560 --> 00:27:51.400]   develop a system or a
[00:27:52.440 --> 00:27:56.200]   A way to react and adapt to your environment. Well, that's a good point
[00:27:56.200 --> 00:28:01.400]   And I would say that the AI is perfectly named because what a computer is doing is
[00:28:01.400 --> 00:28:06.760]   You're giving it a set of parameters and it's adapting and it's teaching itself
[00:28:06.760 --> 00:28:11.720]   Basically how to write it's okay. You're you're absolutely right. In fact. I'm looking at the definition of intelligence
[00:28:11.720 --> 00:28:17.720]   From oxford. It says the ability to acquire and apply knowledge and skills. That's pretty simple and you're right
[00:28:17.720 --> 00:28:21.880]   That's exactly what AI does but wait to guess the next step
[00:28:22.840 --> 00:28:24.840]   But is it thinking?
[00:28:24.840 --> 00:28:27.560]   There we go. But we're not arguing that it is conscious
[00:28:27.560 --> 00:28:33.240]   Well, okay, maybe your original point was maybe the term AI doesn't but
[00:28:33.240 --> 00:28:39.000]   Certainly a lot of the ways we describe the output of AI like it's hallucinating
[00:28:39.000 --> 00:28:42.920]   Uh, that's either more fick itself. That is right. It's more fick
[00:28:42.920 --> 00:28:47.080]   That yeah, but we do that for everything. Yeah, because we're people. Okay, so
[00:28:48.120 --> 00:28:54.040]   Regardless, so I'm not really arguing about terminology what I'm really saying is just be clear understand. It's not thinking
[00:28:54.040 --> 00:28:57.560]   Well, you would agree with that, right?
[00:28:57.560 --> 00:29:00.040]   Yeah, I don't think
[00:29:00.040 --> 00:29:07.160]   It is not thinking in the way that we think I do think it has developed an intelligence in the same way your dog
[00:29:07.160 --> 00:29:11.640]   Is has developed an intelligence? Well, I think a dog thinks allows it to react to
[00:29:15.240 --> 00:29:18.920]   Here's what I wrote on mass again. I guess I guess what do you mean by I mean like I think they
[00:29:18.920 --> 00:29:23.640]   Learn I think I'm machine. Yeah, don't learn. Okay. This is now. We're getting in the nut of it
[00:29:23.640 --> 00:29:26.120]   Okay, this is what I wrote on mass on see if you agree with this
[00:29:26.120 --> 00:29:29.000]   Don't kid yourself artificial intelligence
[00:29:29.000 --> 00:29:33.560]   And I put that in quotes as just big text way of saying hey, it's not our fault. It's the machines
[00:29:33.560 --> 00:29:41.240]   In point of fact every malign use of AI face recognition or you know is really just some human
[00:29:41.320 --> 00:29:47.880]   Composed algorithm doing something that maximizes profit or control at the expensive people
[00:29:47.880 --> 00:29:53.000]   I I say don't let these companies off the hook they're in charge not the machine
[00:29:53.000 --> 00:29:58.600]   Pay attention to the man behind the current doesn't have to be at the expense of people but sometimes it's well, it can be
[00:29:58.600 --> 00:30:05.720]   Yes, if you stuck if you stuck a dog let's stick with dogs because everyone loves dogs. Yes
[00:30:06.920 --> 00:30:11.000]   If I if I stuck a dog in an environment where they learned how to
[00:30:11.000 --> 00:30:16.600]   Bite everybody who was black. Don't I don't know why it's your fault like the dogs
[00:30:16.600 --> 00:30:18.840]   It is
[00:30:18.840 --> 00:30:19.800]   Well
[00:30:19.800 --> 00:30:25.160]   Yes, I mean and I then unleashed this dog upon the world and he started biting everybody
[00:30:25.160 --> 00:30:30.760]   It saw that it was who was black then yes, I have obviously trained this dog improperly
[00:30:30.760 --> 00:30:34.840]   And similarly if my tesla drives into a wall
[00:30:35.880 --> 00:30:39.800]   Under under full self-driving it's it's the software at fault
[00:30:39.800 --> 00:30:45.400]   I'm blaming the software, but I'm also blaming the driver for letting the software. Mm-hmm. Yeah
[00:30:45.400 --> 00:30:50.200]   Yeah, like if I if I do this about my dog if I inadvertently had trained it
[00:30:50.200 --> 00:30:54.600]   I could not just go wandering around saying oh sorry my dog is really racist
[00:30:54.600 --> 00:30:57.320]   Long as you
[00:30:57.320 --> 00:31:00.760]   Supreme court says as long as you didn't know the dog was racist you're okay
[00:31:03.240 --> 00:31:08.440]   Well, that's another case, but that's a different case because she said trained right training my dog
[00:31:08.440 --> 00:31:14.200]   the chat gpt lawyer and by the way the the I haven't put it in the rundown they the
[00:31:14.200 --> 00:31:17.080]   Judge decided the case about him
[00:31:17.080 --> 00:31:21.800]   He blamed the machine and he said well, I didn't think the machine would lie but she lied to me
[00:31:21.800 --> 00:31:24.680]   And the judge said no
[00:31:24.680 --> 00:31:29.560]   If you find you made a mistake use the word mistake you tried once
[00:31:30.280 --> 00:31:36.360]   Once there was doubt raised and you didn't do your job. Yeah, so look up the cases on google for god's sakes
[00:31:36.360 --> 00:31:40.680]   Then from then on out it wasn't as pretty double jab. What do we got?
[00:31:40.680 --> 00:31:46.680]   Yeah, regular folks understand that computers are only as good as what you put into them
[00:31:46.680 --> 00:31:51.640]   Yeah, I just I want regular folks to understand computer is actually in an animate box of rocks
[00:31:51.640 --> 00:31:56.520]   Right that we animate with electricity and and instruct with instructions written by humans
[00:31:56.840 --> 00:32:00.440]   Or it or generated at the request of humans from human data
[00:32:00.440 --> 00:32:06.840]   Stacey rolled us this computer did this I hear I used to hear it a lot
[00:32:06.840 --> 00:32:11.240]   Yeah, but we told the one was on the board. That was great. That was a great edit quick switch
[00:32:11.240 --> 00:32:14.760]   That was a great case either roller. What does Stacy just the right moment?
[00:32:14.760 --> 00:32:19.960]   Perfect. I'm like on the guardians of the galaxy. I got my eyes closed and I'm holding on to those
[00:32:22.360 --> 00:32:26.760]   So I think there's a couple things here. I I think that
[00:32:26.760 --> 00:32:30.840]   People should be aware that
[00:32:30.840 --> 00:32:34.120]   AI and these algorithms are
[00:32:34.120 --> 00:32:40.360]   Only as good as they're training and they are fallible. I think that's really important for everyone to realize
[00:32:40.360 --> 00:32:46.520]   I also think there is a link between the marketing done by tech firms
[00:32:46.520 --> 00:32:49.160]   and
[00:32:49.160 --> 00:32:55.160]   The people the end consumer who don't have knowledge of this and the tech firms are being quite disingenuous
[00:32:55.160 --> 00:33:00.040]   and I do think there's probably a case to be made for them being liable for
[00:33:00.040 --> 00:33:02.440]   their marketing
[00:33:02.440 --> 00:33:06.760]   and then once they have assessed an algorithm and know it's bad to
[00:33:06.760 --> 00:33:12.600]   So like that lawyer in his case because he's a professional deploying and using the algorithm
[00:33:12.600 --> 00:33:15.640]   He does have some liability here. I would argue that
[00:33:16.200 --> 00:33:21.400]   Whatever legal algorithm he used also has some liability because they're making claims
[00:33:21.400 --> 00:33:25.160]   Liabilities probably too short deceptive average responsibilities
[00:33:25.160 --> 00:33:30.920]   I argue this in my piece that when you you put a just put a box there that makes it look like google
[00:33:30.920 --> 00:33:35.480]   You think you're gonna get google like results back and microsoft associated with their search engine
[00:33:35.480 --> 00:33:42.440]   Is being irresponsible in giving that impression. I agree. I mean google results are terrible and most people know that now
[00:33:43.320 --> 00:33:48.280]   I mean part of it is we're really dumb as a species. We're just here we go. We used to be good
[00:33:48.280 --> 00:33:51.960]   That's the thing. I think we're gotten terrible. Yeah
[00:33:51.960 --> 00:33:59.640]   Yes, they they have but I mean we we're just not involved the other thing is these companies are throwing this out into the world willy nilly
[00:33:59.640 --> 00:34:01.640]   and
[00:34:01.640 --> 00:34:08.840]   It's it's all like a big science experiment that we think is magic because we still think of computers like calculators that are infallible
[00:34:08.840 --> 00:34:10.520]   and
[00:34:10.520 --> 00:34:12.520]   They're not
[00:34:12.920 --> 00:34:14.920]   so
[00:34:14.920 --> 00:34:19.800]   Supreme i guess no i just guess that you probably
[00:34:19.800 --> 00:34:26.760]   Are in a way saying that it's silly to worry about thinking versus not thinking
[00:34:26.760 --> 00:34:29.640]   Like they're not thinking I mean I
[00:34:29.640 --> 00:34:34.200]   Yeah, I think it's not germane to the issues that we're facing exactly yes
[00:34:34.200 --> 00:34:35.560]   Yeah
[00:34:35.560 --> 00:34:41.480]   It's a fun philosophical argument and if I were like drinking and we were on a patio and it was late at night
[00:34:41.880 --> 00:34:45.800]   We could have discussions why is everybody every time I bring this up talk about that
[00:34:45.800 --> 00:34:49.000]   Even steve said that we're not in college stand up all night
[00:34:49.000 --> 00:34:52.040]   on
[00:34:52.040 --> 00:34:57.560]   I mean like it is a college debate. It's a philosophy. It's a philosophical argument
[00:34:57.560 --> 00:35:00.440]   But it is but from a practical point of view
[00:35:00.440 --> 00:35:06.440]   I guess what I want to kind of emphasize is we don't want to get in the hype cycle with AI either
[00:35:06.680 --> 00:35:13.000]   Right and I think part of the hype cycle is the scientist writing letters saying it's a threat to the human species
[00:35:13.000 --> 00:35:19.480]   It's an extinction it goes both ways. I think that's hype. That's like saying that's just marketing. That's so hard to be like
[00:35:19.480 --> 00:35:21.640]   It's it's
[00:35:21.640 --> 00:35:27.080]   It helps them wash their hands of it too. I mean that was my point. Yes, there needs to be more middle ground
[00:35:27.080 --> 00:35:31.080]   Take some of the good
[00:35:31.480 --> 00:35:36.440]   Little had little cynicism like I don't even want to call it cynicism. Just skeptics skepticism skepticism
[00:35:36.440 --> 00:35:39.240]   Thank you. That's the word. Yeah, yeah
[00:35:39.240 --> 00:35:46.040]   Well, but but it also it does have larger implications because and I've said this in the show recently is I'm gonna get into the rat hole that is
[00:35:46.040 --> 00:35:48.600]   Long-termism that drives
[00:35:48.600 --> 00:35:54.120]   Sam Altman and Peter teal and Elon Musk and company and
[00:35:54.120 --> 00:35:57.560]   They argue that they're going to we're going to reach
[00:35:59.000 --> 00:36:06.920]   artificial general intelligence or super intelligence and that the machine will be basically as much as a human better in some ways
[00:36:06.920 --> 00:36:14.360]   And that we can build countless 10 to the 58th made up humans on computers on Mars
[00:36:14.360 --> 00:36:18.760]   They go why why is he not must want to put things in your head and go to Mars and have a lot of babies
[00:36:18.760 --> 00:36:23.480]   And so we owe to the future, but we can forget about the present
[00:36:25.240 --> 00:36:32.280]   This is also a great question. This is also a great curse. Well said the singularity is near and this is exactly if you believe that
[00:36:32.280 --> 00:36:37.640]   The singularity being the top point in time in which you can no longer distinguish a human and a machine
[00:36:37.640 --> 00:36:43.000]   Intelligence and if you believe that then of course once machines get that smart
[00:36:43.000 --> 00:36:46.440]   They will start designing better and better better machines at an exponential rate
[00:36:46.440 --> 00:36:51.400]   And yeah, maybe that is an extinction event. So the reason I bring that's why I bring this up is
[00:36:52.840 --> 00:36:57.720]   I don't think we know what makes a human and what makes consciousness
[00:36:57.720 --> 00:37:02.200]   And I think it's a lot to say that we're you know, we're approaching AGI
[00:37:02.200 --> 00:37:05.240]   Is that stacy? Does that?
[00:37:05.240 --> 00:37:11.000]   Is that right? I mean you you AGI being artificial generalized intelligence. Yes. Yes. Okay
[00:37:11.000 --> 00:37:14.120]   Not not the income whatever
[00:37:14.120 --> 00:37:17.480]   Not basic
[00:37:21.320 --> 00:37:24.040]   Intelligent machines intelligent machines thinking machines
[00:37:24.040 --> 00:37:31.000]   I feel okay. This is gonna sound really like a jerk thing to say. I think this is all a very
[00:37:31.000 --> 00:37:37.720]   Male, yeah, it's a boy problem. Yeah, I think it's a problem that is oblivious to like
[00:37:37.720 --> 00:37:43.720]   It's easy to think about this because the current work to fix things here and now is really hard
[00:37:43.720 --> 00:37:49.480]   So if you can keep building better toys or come up with like some cool philosophy
[00:37:49.480 --> 00:37:55.560]   That lets you kick the can down the road without actually acknowledging that you're kicking the can down the road then
[00:37:55.560 --> 00:38:03.240]   I agree 100% that's exactly what I'm saying. I agree 100% yes minus the boy part and we have
[00:38:03.240 --> 00:38:08.520]   No, no, you're right. Maybe it's like that. No, you agree
[00:38:08.520 --> 00:38:13.800]   That's why I say bros right I say the AI bros because it is kind of a bros because it is it's it's it's
[00:38:14.280 --> 00:38:19.160]   And then media don't quote the women who give it perspective like
[00:38:19.160 --> 00:38:22.360]   Mature yeah, Emily and Emily bender and Margaret Mitchell
[00:38:22.360 --> 00:38:25.480]   Um, and they never quote them. They only just quote the boys
[00:38:25.480 --> 00:38:29.880]   And maryday th wittaker. That's what I think that's the name of yeah. She's great too
[00:38:29.880 --> 00:38:35.960]   So I hope you'll have both on the AI show. That's that's the whole way. I think it's a great conversation
[00:38:35.960 --> 00:38:39.720]   I think it's very promise never to talk about consciousness. I'll visit it
[00:38:41.720 --> 00:38:45.400]   Jason is now running the anti rundown things to never talk about
[00:38:45.400 --> 00:38:51.240]   No, you should I mean we should I mean it's it's fun. I mean I like thinking about
[00:38:51.240 --> 00:38:57.480]   Consciousness, but I'm also like yeah, we got some real problems and the problem and it also includes
[00:38:57.480 --> 00:39:00.760]   Okay, stasis is going to hop up on a little soapbox for a second
[00:39:00.760 --> 00:39:03.320]   It also includes the real issue
[00:39:03.320 --> 00:39:10.360]   Which is everybody here wants to use AI for optimization for their own end goals, which i'm sorry
[00:39:11.160 --> 00:39:13.400]   Society is making as much money as possible
[00:39:13.400 --> 00:39:15.960]   You're building these machines
[00:39:15.960 --> 00:39:22.840]   To optimize for this in a way and to hide all these other issues, but really if we built these algorithms
[00:39:22.840 --> 00:39:27.080]   To take into account and optimize for maybe
[00:39:27.080 --> 00:39:32.200]   Not producing as much carbon but throwing as much carbon into the air
[00:39:32.200 --> 00:39:36.600]   Then we would have different algorithms that would not make us as much money
[00:39:36.600 --> 00:39:39.480]   And we would all as a society hate that because that's
[00:39:40.280 --> 00:39:43.320]   antithetical to how we are kind of currently wired
[00:39:43.320 --> 00:39:46.760]   Wired and going but the path we're going down
[00:39:46.760 --> 00:39:52.840]   Well, and this he don't get me started on the perverse incentives of of late stage capitalism because that's
[00:39:52.840 --> 00:39:58.920]   Well, but I think that's I mean that's a real issue here because that's a lot of the marketing hype around this stuff is like
[00:39:58.920 --> 00:40:06.040]   Oh, we're gonna solve all these issues that are actually caused by a lot of the late stage capitalism like the not accounting for externalities that matter
[00:40:06.520 --> 00:40:11.720]   And so then they're like, oh shoot. They might see it. Let's talk about it ending
[00:40:11.720 --> 00:40:14.600]   The world and this being a threat that way
[00:40:14.600 --> 00:40:20.680]   But really we just have to make a we have to make a collective decision to care about something that's not
[00:40:20.680 --> 00:40:24.840]   Money and we can use this technology in ways that would really actually be quite helpful
[00:40:24.840 --> 00:40:26.920]   And we might not be having these discussions
[00:40:26.920 --> 00:40:31.240]   Which is which is what i'm what Jason talks about too is let's talk about the utility
[00:40:31.240 --> 00:40:34.200]   We talked about today about good uses and bad uses
[00:40:35.000 --> 00:40:40.600]   And and stupid uses right and and malign uses, but there are good uses
[00:40:40.600 --> 00:40:44.120]   Uh, and and trying to figure out what that constrainer that and then your rights. Stacy
[00:40:44.120 --> 00:40:48.520]   This is what the stochastic parrots paper says is also pay attention to the environmental cost
[00:40:48.520 --> 00:40:52.360]   Pay attention to the human cost of people who are training your models for you and looking at horrible stuff
[00:40:52.360 --> 00:40:55.960]   Pay attention to the bias in in what you bring in
[00:40:55.960 --> 00:40:59.080]   Uh pay attention to try to get ever bigger
[00:40:59.640 --> 00:41:03.800]   Uh measurement and and losing sight of of the ability to manage what you have
[00:41:03.800 --> 00:41:08.200]   That's what they said in that paper and that's what gets ignored so often. Well, that's a perfect lead in
[00:41:08.200 --> 00:41:12.280]   To our ai segment which is coming up next
[00:41:12.280 --> 00:41:15.160]   We're gonna take a little break
[00:41:15.160 --> 00:41:21.880]   Thank you, stacy you you you you know you know because you're right you were dreading this you did very well
[00:41:21.880 --> 00:41:28.120]   Well, it and unfortunately it often goes right to that thing of and Steve gives his position was oh
[00:41:28.680 --> 00:41:34.520]   You know, it's just a matter of calculation calculating speed ram and you know just add as it gets faster for that
[00:41:34.520 --> 00:41:35.880]   at some point it's gonna
[00:41:35.880 --> 00:41:40.200]   It's consciousness is an emergent property and it's going to just start thinking and
[00:41:40.200 --> 00:41:43.320]   That's for a late night conversation
[00:41:43.320 --> 00:41:49.720]   I mean you could stick stuff in a petri dish. Yeah without any ram. Well, that's how that's how we
[00:41:49.720 --> 00:41:57.800]   But that's that's called us. Yeah, that's called us unless you believe god made us some do and i'm not criticizing that i don't happen to
[00:41:58.440 --> 00:42:03.000]   Uh, we're just the result of a late stage petri dish a lot of science. Yeah
[00:42:03.000 --> 00:42:06.200]   We're pretty random. It's just a random occurrence
[00:42:06.200 --> 00:42:12.040]   All right, uh, I see to me I want to stay up all night and talk about this, but we probably shouldn't because
[00:42:12.040 --> 00:42:15.560]   Let me know me. You'll get my waffles. Waffles awake. Go get some wine
[00:42:15.560 --> 00:42:20.680]   My water i'll smoke a dooby and we can talk but first no no
[00:42:20.680 --> 00:42:26.440]   first, uh, let's talk about our sponsor, right? We have a great sponsor the aws
[00:42:27.080 --> 00:42:28.440]   insiders
[00:42:28.440 --> 00:42:31.960]   podcast a fun fast pace entertaining
[00:42:31.960 --> 00:42:34.440]   insightful look behind the scenes of
[00:42:34.440 --> 00:42:37.720]   aws and cloud computing now
[00:42:37.720 --> 00:42:43.480]   This is not your typical talking heads tech podcast high production value high energy
[00:42:43.480 --> 00:42:49.560]   Unlike us and high entertainment full of captivating stories from the early days of aws
[00:42:49.560 --> 00:42:51.960]   To today and beyond the hosts
[00:42:51.960 --> 00:43:00.920]   Ruhull supermonium and hillary doil dig into the current state and the future of aws by talking with the people and companies that know it best
[00:43:00.920 --> 00:43:07.080]   Ruhull's very funny, but he's also a veteran aws pro with over 15 years experience managing one
[00:43:07.080 --> 00:43:10.360]   45,000 aws instances
[00:43:10.360 --> 00:43:18.040]   He is known for pushing aws products to their limits and for believing aws is truly the operating system of the future
[00:43:18.600 --> 00:43:23.800]   aws insiders is a show that's full of opinions takeaways and untold stories about the challenges
[00:43:23.800 --> 00:43:31.960]   Innovations in the mind-blowing promise of cloud computing take a look at this the brand new season just came out season two episode one
[00:43:31.960 --> 00:43:35.640]   filling the cloud talent gap
[00:43:35.640 --> 00:43:40.680]   Talk about staffing and optimizing your cloud team how it's a critical step right now
[00:43:40.680 --> 00:43:48.120]   Also possibly one of the most difficult ruhull hillary and their guests discuss solutions defining retaining and leveling up
[00:43:48.760 --> 00:43:50.120]   cloud
[00:43:50.120 --> 00:43:52.120]   Talent
[00:43:52.120 --> 00:43:56.360]   I was really interested in episode three. It's all about Moderna the mr
[00:43:56.360 --> 00:43:59.240]   And a vaccine and a ws. Did you know that?
[00:43:59.240 --> 00:44:03.480]   Ruhull hillary and moderna's director of data engineering cloud architecture
[00:44:03.480 --> 00:44:08.520]   I this was a revelation to me discuss how Moderna depends on aws
[00:44:08.520 --> 00:44:11.240]   and the cloud
[00:44:11.240 --> 00:44:15.560]   Search for aws insiders in your podcast player or visit cloud fix
[00:44:15.960 --> 00:44:19.800]   Dot aria dot com slash podcast that's cloud fix
[00:44:19.800 --> 00:44:22.600]   dot a u r e a dot com
[00:44:22.600 --> 00:44:24.840]   slash
[00:44:24.840 --> 00:44:29.960]   Podcast who will also include a link in the show notes and my thanks to ws insiders
[00:44:29.960 --> 00:44:32.680]   for their support
[00:44:32.680 --> 00:44:38.360]   And now we have no bumper music. We have no trumpets. We have no drummers, but it is time
[00:44:38.360 --> 00:44:41.560]   for the ai stories of the week
[00:44:41.560 --> 00:44:44.440]   Not yet anyway, but
[00:44:45.160 --> 00:44:47.160]   I can do
[00:44:47.160 --> 00:44:49.800]   Yeah, you know, let's what would ai sound like
[00:44:49.800 --> 00:44:53.080]   I don't know
[00:44:53.080 --> 00:44:55.400]   I'm sorry. I can't do that
[00:44:55.400 --> 00:44:57.480]   My son today
[00:44:57.480 --> 00:45:00.840]   Here's the ai generated books of nonsense
[00:45:00.840 --> 00:45:02.680]   so
[00:45:02.680 --> 00:45:04.680]   this from motherboard
[00:45:04.680 --> 00:45:09.160]   AI generated books of nonsense are all over amazon's best seller lists
[00:45:10.040 --> 00:45:16.920]   Amazon's kindle unlimited bestseller list full of books with titles like apricot barcord barcode architecture
[00:45:16.920 --> 00:45:23.240]   And jessica's attention. I amazon woke up to this and has pulled most of them become I don't know I'm not surprised
[00:45:23.240 --> 00:45:25.560]   They're there. How would they become a bestseller?
[00:45:25.560 --> 00:45:32.120]   Ah as if you advertise them on yeah. Oh, how did they do they know I don't know amazon's kindle
[00:45:32.120 --> 00:45:34.840]   I limited young adult romance bestseller list
[00:45:35.800 --> 00:45:40.360]   Was filled with dozens of ai generated books of nonsense on monday and tuesday
[00:45:40.360 --> 00:45:45.160]   As of this morning amazon appears to have taken action against the books rights mother boy's vice
[00:45:45.160 --> 00:45:47.640]   But the episode shows people are spamming
[00:45:47.640 --> 00:45:52.520]   ai generated nonsense to the platform and somehow finding a way to monetize it
[00:45:52.520 --> 00:45:59.240]   Oh, well, it could be with kindle unlimited if you download somebody's book on kindle if i's a kindle unlimited subscriber
[00:45:59.240 --> 00:46:05.560]   They paid for it and it costs you nothing. Yeah, so probably they have click farms, right?
[00:46:06.280 --> 00:46:11.400]   Uh who say okay, you know, here's a hundred bucks download a thousand car ten thousand or hundred thousand
[00:46:11.400 --> 00:46:17.880]   Progosian has to do something now. I was just gonna sing it was more of the idiocy of our society
[00:46:17.880 --> 00:46:24.280]   I was a bogey and a wise Stacy. Thank you Stacy. I don't even know what you're talking about. What are you talking about?
[00:46:24.280 --> 00:46:26.520]   Did you follow the news?
[00:46:26.520 --> 00:46:30.920]   The what oh, I know you know, I don't watch news. I apparently don't go jen
[00:46:30.920 --> 00:46:32.760]   ran the
[00:46:32.760 --> 00:46:39.720]   Progosian who who was going up against Putin ran the irc. Oh, I didn't know that's how you pronounced pergizizibuzian
[00:46:39.720 --> 00:46:42.360]   you pronounce that bagojian
[00:46:42.360 --> 00:46:44.600]   Progosian I put the extra
[00:46:44.600 --> 00:46:46.600]   pargosian
[00:46:46.600 --> 00:46:49.800]   You mean a second i was like i think he's talking about that the rebel guy
[00:46:49.800 --> 00:46:53.320]   The cool
[00:46:53.320 --> 00:46:56.680]   How do you pronounce hot dog salesman turned is that?
[00:46:57.640 --> 00:47:04.360]   Food and adversary right right prijogon prijogon. No, you haven't you prijogon?
[00:47:04.360 --> 00:47:11.720]   You've been you've been you pregosian. Okay, we should just we should stop right now
[00:47:11.720 --> 00:47:16.680]   To find like somebody who actually if his name comes up in an ad leo will get it 100 right
[00:47:16.680 --> 00:47:20.440]   I always get it right there. No, it's it's uh, I'm looking at the russian
[00:47:20.440 --> 00:47:23.160]   It's pregosian
[00:47:23.160 --> 00:47:28.200]   Progosian prijogon. What is that not? Pargosian is that okay? You said Pargosian. What are the extra?
[00:47:28.200 --> 00:47:35.000]   Yeah, yeah, I still understood where Jeff was going with that. So you mean the rebel. Oh gosh. Okay rebel guy
[00:47:35.000 --> 00:47:41.320]   That was Irish accent, but go ahead. That's no Irish is this I was using a Russian accent
[00:47:41.320 --> 00:47:46.200]   Yevany victorovich prijogon. Jameri. Jameri be shaking his head back there
[00:47:46.200 --> 00:47:48.520]   Where were we before we started?
[00:47:48.520 --> 00:47:56.280]   You were introing a story. You could sell it was rose to the house popular. He was gonna do the AI
[00:47:56.280 --> 00:48:01.560]   Um, here is one of Pragorgon's hot bugs. I save it for the show
[00:48:01.560 --> 00:48:08.840]   Dude, that was like a problem. That didn't look like why did you happen? That was a lunch
[00:48:08.840 --> 00:48:11.960]   That was because I knew you say Pargosian
[00:48:11.960 --> 00:48:15.400]   Okay, anyway
[00:48:16.280 --> 00:48:23.720]   So that's what's happening on amazon's unlimited. There are a lot of what are you surprised that people would then use AI to generate a bunch of
[00:48:23.720 --> 00:48:26.200]   junk nonsense
[00:48:26.200 --> 00:48:30.040]   Uh, let's talk about China because this is one of the things I mentioned to Jason and
[00:48:30.040 --> 00:48:35.800]   What you know, we're talking about AI is it's behind this now the new iron curtain
[00:48:35.800 --> 00:48:40.120]   Uh, but you've got to think the Chinese government is working hard on AI
[00:48:41.320 --> 00:48:46.040]   This is a story from bloomberg billionaires and bureaucrats mobilize china for AI race
[00:48:46.040 --> 00:48:49.160]   with the us now their position is
[00:48:49.160 --> 00:48:54.920]   Yes, of course the chinese tech sector is very interested in AI, but they're somewhat behind nevertheless
[00:48:54.920 --> 00:48:58.280]   Uh, there's a considerable amount of investment
[00:48:58.280 --> 00:49:02.520]   I'm not so much worried about private industry as the government
[00:49:02.520 --> 00:49:06.440]   Chinese government using AI and I have to think they've got some stuff going on
[00:49:07.000 --> 00:49:13.000]   Well, there I mean if we call it AI just AI that they're already using like face recognition and socials
[00:49:13.000 --> 00:49:15.720]   All kinds of other algorithms to like to track
[00:49:15.720 --> 00:49:22.520]   Lawbreaking and that sort of thing. Yeah, the top flight chinese talent and financing flowing into AI mirrors a wave of activity
[00:49:22.520 --> 00:49:30.760]   confulsing silicon valley which has deep implications for beijing's escalating conflict with washington writes a hot dog vendor
[00:49:30.760 --> 00:49:35.480]   so are are you saying or the people in this piece saying that the
[00:49:36.680 --> 00:49:41.160]   Chinese AI is a threat to us even though china probably
[00:49:41.160 --> 00:49:44.840]   Don't give a crap about us and you just want to use it for their own
[00:49:44.840 --> 00:49:49.480]   Their own yard. I you know what they're doing. That's a good question. Ants
[00:49:49.480 --> 00:49:55.960]   That is not clear from the article. I mean the article says the aggregate size of us deals and AI outpaces china's
[00:49:55.960 --> 00:50:02.840]   They're 26 billion in uh investments in 2023 for the us just 4 billion for china
[00:50:03.320 --> 00:50:07.160]   But it does it seems to be talking about private industry and I have to think that the
[00:50:07.160 --> 00:50:11.160]   We have no idea how much effort the chinese government is putting today
[00:50:11.160 --> 00:50:16.840]   I and we have a lot of anecdotal evidence that they care a lot about it. Don't they they probably care but again, I
[00:50:16.840 --> 00:50:21.800]   I think we're just being a little bit narcissistic and assuming that
[00:50:21.800 --> 00:50:28.360]   They care enough about AI to use it against us where we're they're probably just caring enough about AI to
[00:50:28.360 --> 00:50:30.920]   do something for their own
[00:50:31.480 --> 00:50:36.920]   This is like the 5g debate where we were like china's getting into 5g. We don't have our stuff yet
[00:50:36.920 --> 00:50:40.840]   This is both like uh, what does it call it when you're one country?
[00:50:40.840 --> 00:50:47.960]   Like nativist fear mongering about technology. Yeah, show venison. There we go. And then also
[00:50:47.960 --> 00:50:50.760]   idealizing a
[00:50:50.760 --> 00:50:53.560]   Hiped technology in this case AI. I mean
[00:50:53.560 --> 00:51:01.400]   We could equally throw in like material science and have something really to worry about because that's how you get like weird nerve gas
[00:51:01.560 --> 00:51:04.360]   And new bombs and all kinds of crazy stuff, but well
[00:51:04.360 --> 00:51:09.960]   This is a interesting thing that one of the reasons private investment in AI and china is slow
[00:51:09.960 --> 00:51:12.600]   Is because the chinese government
[00:51:12.600 --> 00:51:18.440]   Has lots of restrictions on what private industry can do with AI. Okay, they would like a monopoly
[00:51:18.440 --> 00:51:25.480]   I would imagine of AI in the government so well, it's very strict control and control. Yes. Yeah, yeah
[00:51:25.480 --> 00:51:28.520]   But I agree with you ant that I think if anything
[00:51:29.080 --> 00:51:35.400]   They're more interested in using AI to control their people. Mm-hmm. Uh, perhaps you know, especially if you have a central planned economy
[00:51:35.400 --> 00:51:42.440]   Uh, there's a lot of opportunity for AI to do a better job than the the you know everything to make things more efficient
[00:51:42.440 --> 00:51:43.640]   Well, yeah
[00:51:43.640 --> 00:51:48.120]   And there is a there is a case to be made that you I mean it would be kind of brutal again
[00:51:48.120 --> 00:51:53.240]   Based on how you train your algorithms, but it does cut out possibly some of the corruption right?
[00:51:53.960 --> 00:51:59.000]   With I mean you could theoretically I don't you I don't see that actually happening, but it could happen
[00:51:59.000 --> 00:52:01.880]   the uh, it's also the
[00:52:01.880 --> 00:52:07.080]   relevant to the us's policy towards AI us is considering new
[00:52:07.080 --> 00:52:11.000]   Curbs on AI chip exports to china. There's some
[00:52:11.000 --> 00:52:17.400]   You know serious concern that chinese might be using AI for weapon development for hacking using it against us
[00:52:17.400 --> 00:52:22.200]   And so while there are no restrictions yet there are perhaps some restriction
[00:52:22.760 --> 00:52:26.280]   Thoughts about restricting uh, AI chips from invidia and others
[00:52:26.280 --> 00:52:31.160]   And is it your is it your view that china's just not the threat that has made out to be
[00:52:31.160 --> 00:52:37.160]   My view is we worry a lot about china and how they run their country
[00:52:37.160 --> 00:52:41.960]   Their country and i'm not saying what they're doing is right or I agree with it
[00:52:41.960 --> 00:52:44.840]   But I am saying it is their house
[00:52:44.840 --> 00:52:51.240]   And their property if you will in that country and they can run their country however they see fit
[00:52:52.200 --> 00:52:55.080]   I agree and I think if china is a threat to the us
[00:52:55.080 --> 00:52:58.760]   It's an economic threat more than a military threat, right?
[00:52:58.760 --> 00:53:05.720]   Right, you know as this seems possible in the next few years china decides to take back taiwan
[00:53:05.720 --> 00:53:10.360]   That would be a threat to us in the west wouldn't it? I mean
[00:53:10.360 --> 00:53:18.280]   Yes, all the chips in our iPhones are making taiwan right, you know, but then that could maybe be a good thing for us in the long run and
[00:53:18.680 --> 00:53:23.960]   Create more jobs here in the us. Yeah, you know that because we always talk about we need more jobs
[00:53:23.960 --> 00:53:27.480]   Well, stop sinning our stuff to china to do
[00:53:27.480 --> 00:53:32.280]   Invidia is responding to a potential curb on uh, AI technology
[00:53:32.280 --> 00:53:35.480]   In uh sent to china by creating a special
[00:53:35.480 --> 00:53:44.680]   Dumb AI chip for the chinese market. It's the a 800. It has its its performances below the thresholds outlined by the commerce
[00:53:45.400 --> 00:53:46.760]   department
[00:53:46.760 --> 00:53:50.280]   So yeah, you can have this chip. It's a little dumber. This is this is for you
[00:53:50.280 --> 00:53:52.200]   Um
[00:53:52.200 --> 00:53:57.640]   The new restrictions that the commerce departments thinking about would ban the sale even of those chips without a license
[00:53:57.640 --> 00:54:01.480]   It's like this wall street journey
[00:54:01.480 --> 00:54:03.240]   Stacy you're gonna say the one
[00:54:03.240 --> 00:54:05.240]   Oh, I was gonna say I mean
[00:54:05.240 --> 00:54:11.240]   There's a couple ways that that could I mean china is no secret to corporate espionage
[00:54:11.320 --> 00:54:16.600]   They could totally steal what they need and then build it on their own it would take time but
[00:54:16.600 --> 00:54:19.320]   They could totally do it and two
[00:54:19.320 --> 00:54:25.000]   I don't want us to go. I mean the benefits of globalization have been pretty clear
[00:54:25.000 --> 00:54:29.640]   And I think it leads to less conflict over time because we're all like, oh, well
[00:54:29.640 --> 00:54:32.440]   This benefits me so I'll ignore that and
[00:54:32.440 --> 00:54:38.200]   So the way we're going about with this is scary personally to me
[00:54:40.760 --> 00:54:42.760]   I don't know I just
[00:54:42.760 --> 00:54:46.520]   Yeah
[00:54:46.520 --> 00:54:49.880]   We're not heading in the right direction. I'm like, oh, it's it's very depressing
[00:54:49.880 --> 00:54:54.760]   um also you can take dumb chips and do cool things with them and just
[00:54:54.760 --> 00:54:57.400]   engineer your way around
[00:54:57.400 --> 00:55:00.680]   Some of this stuff with limitations. I've always assumed that the chinese
[00:55:00.680 --> 00:55:01.720]   After networking
[00:55:01.720 --> 00:55:06.600]   Uh had a big head start on this kind of thing because they were way ahead of us in uh
[00:55:06.600 --> 00:55:12.120]   Text to speech because they they don't use a roman alphabet. They have a much more complicated difficult
[00:55:12.120 --> 00:55:18.520]   uh system and uh, you know, it doesn't work well with typewriters and things because there's hundreds of thousands of chinese characters
[00:55:18.520 --> 00:55:26.200]   And so I I've always heard for decades that they were way advanced in terms of text to speech and speech to text
[00:55:26.200 --> 00:55:29.240]   Which is a i it's kind of it's a kind of a i right
[00:55:29.240 --> 00:55:32.680]   Uh google's again just
[00:55:32.680 --> 00:55:33.480]   Sorry
[00:55:33.480 --> 00:55:37.080]   I was just having another bite of this. Yeah, we should just call it the master computers
[00:55:37.080 --> 00:55:44.920]   Faster computers and then specialized programs because I mean that's he actually bite that thing
[00:55:44.920 --> 00:55:52.280]   Uh, oh gosh. It's sausage. There's a lot of salt. It's gonna be fine. Leo's not gonna die on camera
[00:55:52.280 --> 00:55:54.760]   No, the thing is
[00:55:54.760 --> 00:55:59.640]   I'm looking at it now and it looks like a prop that hot dog does not look really
[00:55:59.640 --> 00:56:01.320]   It looks like yeah, how are you eating that?
[00:56:01.320 --> 00:56:08.440]   It looks so she that you get in the windows of the japanese sushi stores that are made of plastic at the gas station. Yes
[00:56:08.440 --> 00:56:13.880]   Nice one did what did you have for once? It was great today, but yours don't look so great
[00:56:13.880 --> 00:56:18.280]   It's not it looks like a drug
[00:56:18.280 --> 00:56:21.640]   I mean this is there for a year ago
[00:56:21.640 --> 00:56:26.440]   It was because of people gave a bottle of whiskey in their drawer. I keep a hot dog in my car
[00:56:28.920 --> 00:56:31.720]   I'm like I need I need a little waffle toaster advice
[00:56:31.720 --> 00:56:35.320]   Excuse me. I'm not saying there's anything wrong with that
[00:56:35.320 --> 00:56:39.160]   Google's deep mind. We were talking about deep mind now
[00:56:39.160 --> 00:56:44.120]   Google has mushed together the two AI companies that they had
[00:56:44.120 --> 00:56:46.520]   Into one
[00:56:46.520 --> 00:56:49.560]   Google's deep mind ceo de demi hasabi
[00:56:49.560 --> 00:56:54.360]   Says its next algorithm will be better than chat gpt
[00:56:55.240 --> 00:56:58.840]   Defined better. I mean death. There's this arms race here
[00:56:58.840 --> 00:57:02.120]   It's gonna be bigger. It's gonna be better
[00:57:02.120 --> 00:57:06.680]   To what I don't know. I don't understand what the metric is here. All right, so they did do
[00:57:06.680 --> 00:57:12.440]   Alpha go, which is amazing right right as we were talking about they see
[00:57:12.440 --> 00:57:16.200]   Playing go right I really want to stress
[00:57:16.200 --> 00:57:19.080]   Okay
[00:57:23.000 --> 00:57:28.760]   Can your dog play no, I know the rules, but I am not good. It's a very hard game
[00:57:28.760 --> 00:57:33.320]   Hasabi says engineers are using techniques from alpha go
[00:57:33.320 --> 00:57:40.920]   To make an AI system. They call it gemini that will be more capable than that behind open AI is chat gpt. So there
[00:57:40.920 --> 00:57:47.080]   People gemini is a large language model. It works with text like chat gpt for
[00:57:47.080 --> 00:57:49.800]   but hasabi says his
[00:57:50.520 --> 00:57:56.120]   His team will combine that technology with techniques used in alpha go aiming to give the system new capabilities
[00:57:56.120 --> 00:58:00.600]   Such as planning or the ability to solve problems
[00:58:00.600 --> 00:58:06.840]   Uh, yeah, he's not very specific, but of course it would be over my head if he was
[00:58:06.840 --> 00:58:12.440]   What I hear a lot these days is that they're saying the next phase here and and yon lacon said this
[00:58:12.440 --> 00:58:16.040]   Chats over the next phase is reasoning
[00:58:16.040 --> 00:58:19.240]   Oh, that doesn't say that would be actual thinking
[00:58:19.320 --> 00:58:22.120]   Yeah, that would be thinking then we come back to your yeah
[00:58:22.120 --> 00:58:29.480]   Alpha go was based on a technique deep. This is from wired. It was based on a technique deep minus pioneer called reinforcement learning
[00:58:29.480 --> 00:58:35.080]   Uh, and which software learns to take on tough problems that require choosing what actions to take as in go
[00:58:35.080 --> 00:58:38.200]   By making repeated attempts and getting feedback
[00:58:38.200 --> 00:58:44.520]   So as I said that the way alpha go learn to play chess and then later go was by playing a lot of games and
[00:58:44.520 --> 00:58:48.920]   Did it win? No, all right? Well, I did it. You know, uh, it's also using
[00:58:49.480 --> 00:58:51.720]   These things are prediction machines now
[00:58:51.720 --> 00:58:58.040]   They're they're predicting the next outcome and trying to do better at their predictions. Yeah, it's in this case
[00:58:58.040 --> 00:59:01.720]   So in this case with reinforcement learning what it does is it's like
[00:59:01.720 --> 00:59:04.200]   Uh, imagine
[00:59:04.200 --> 00:59:05.800]   Imagine amaze, right?
[00:59:05.800 --> 00:59:09.400]   So I turn left and then I hit a wall and like, okay, well that sucks
[00:59:09.400 --> 00:59:11.480]   I'm not gonna turn left anymore and then so
[00:59:11.480 --> 00:59:14.360]   Actually, that's a terrible terrible
[00:59:14.360 --> 00:59:17.080]   I'm trying different analogy. I'm like that
[00:59:17.880 --> 00:59:19.880]   Abort abort. No
[00:59:19.880 --> 00:59:25.160]   It's where you decide states these intelligence. I know
[00:59:25.160 --> 00:59:29.320]   See the algorithm work live before our very eyes ladies gentlemen
[00:59:29.320 --> 00:59:34.040]   It's basically when it's trying to it's trying to understand
[00:59:34.040 --> 00:59:38.760]   Its environment and what went wrong when it takes an action. So it's not actually predicting
[00:59:38.760 --> 00:59:45.400]   It's it's taking an action. It's going down that stream like in playing a video game or using synthetic data
[00:59:45.480 --> 00:59:50.600]   And then it's getting feedback that says oh like in a game the the game is probably the best like
[00:59:50.600 --> 00:59:54.440]   Oh, if I keep doing this I die quickly if my goal is to stay alive
[00:59:54.440 --> 00:59:59.960]   Then i'm gonna do this xy thing you couldn't argue it's working that that is the whole reason
[00:59:59.960 --> 01:00:02.520]   Open AI put out chat gpt
[01:00:02.520 --> 01:00:07.080]   Because it gets reinforcement from human users, right? That was the whole point
[01:00:07.080 --> 01:00:11.560]   Is to get in fact even ask for feedback. How do we do? How was that answer? Was that what you wanted?
[01:00:11.560 --> 01:00:17.320]   Right and that that's the that is the the reinforcement learning part of of chat gpt
[01:00:17.320 --> 01:00:22.360]   Um, but maybe gem and I will have some more
[01:00:22.360 --> 01:00:24.840]   efficient is
[01:00:24.840 --> 01:00:29.400]   For the large language model for chat gpt do they use reinforcement learning?
[01:00:29.400 --> 01:00:34.600]   Let's see. Well, one of the problems that chat gpt has is that it's frozen in time
[01:00:34.600 --> 01:00:41.480]   So it's september. What is it september 2020 now with a big plug-in? It will it could add in
[01:00:41.560 --> 01:00:45.640]   Information, but it doesn't use that as the training right they can use another it's it's easy
[01:00:45.640 --> 01:00:48.760]   I want to go back here, but I think it's hugely expensive to
[01:00:48.760 --> 01:00:54.520]   update your model on a continuous that's why they're too big fashion. Yeah, okay
[01:00:54.520 --> 01:00:57.800]   So stacey going back to your we're gonna get back to consciousness watch out
[01:00:57.800 --> 01:01:03.240]   So, um, I mentioned this book a few years ago called how history gets things wrong by Alexander rosenberg
[01:01:03.240 --> 01:01:04.760]   and
[01:01:04.760 --> 01:01:09.960]   Um, what he what he says to your point is that the theory of mind is false
[01:01:11.000 --> 01:01:15.960]   In fact, we don't come to things by having a desire and knowledge in the making decision
[01:01:15.960 --> 01:01:19.640]   And instead we are doing what you're saying. We are replaying
[01:01:19.640 --> 01:01:26.280]   Games video of life over and over and over again and we choose okay. I'll go down that path
[01:01:26.280 --> 01:01:31.240]   Now if you're an addict you're gonna go down the wrong path because it's the path that's well grooved
[01:01:31.240 --> 01:01:34.600]   if you're uh, you know trained and go
[01:01:34.600 --> 01:01:40.680]   Then you go it's the same thing. So in a way what you're saying is that the computers brain in that sense to add
[01:01:40.840 --> 01:01:47.960]   More fives it does operate according to rosenberg like our brain and in the end it's all about survival of the fittest
[01:01:47.960 --> 01:01:50.120]   and
[01:01:50.120 --> 01:01:54.360]   You don't know why the decision works. Why does it make you live longer than the next guy?
[01:01:54.360 --> 01:01:57.800]   But if it does you win you go on there's more of the same
[01:01:57.800 --> 01:02:05.880]   Yes, sorry, I'm I'm looking at chat gpt and reinforcement learning and yes
[01:02:05.880 --> 01:02:09.080]   They do reuse reinforcement learning and now I know how sorry
[01:02:09.560 --> 01:02:15.000]   I can only do two things that one no one thing at one time one thing at once. Yeah, I guess you could really argue that
[01:02:15.000 --> 01:02:19.720]   Evolution or the hot dog. Yeah, it's good. You can
[01:02:19.720 --> 01:02:27.560]   Lisa says that one of the reasons i'm healthier than normal people's because I eat a lot of bad food
[01:02:27.560 --> 01:02:32.040]   And that it's conditioned this is an example of reinforcement learning not working
[01:02:32.040 --> 01:02:38.040]   In shower less than other shower less. Well, I was gonna say the reinforcement probably is just
[01:02:38.440 --> 01:02:41.560]   So long term it can't change the behavior the ideal
[01:02:41.560 --> 01:02:47.800]   Like your negative your negative reinforcement is gonna. That's why evolution takes millions of years, right?
[01:02:47.800 --> 01:02:50.760]   Um, uh, we are a big computer
[01:02:50.760 --> 01:02:58.360]   But we've done it, you know because of survival not any of the fittest or of the that's a very it's really interesting that to me
[01:02:58.360 --> 01:03:00.840]   I was the most
[01:03:00.840 --> 01:03:05.720]   You know my father's an evolutionist. That's what he teaches taught Darwin courses for years
[01:03:05.800 --> 01:03:08.360]   He's a palean marine paleoecologist
[01:03:08.360 --> 01:03:14.360]   But uh one of the great and so I should have known this my whole life, but what are the great revelations for me later in life?
[01:03:14.360 --> 01:03:17.400]   Uh was this notion
[01:03:17.400 --> 01:03:22.120]   It comes from this Richard Dawkins itself and self is gene that really
[01:03:22.120 --> 01:03:25.400]   you know
[01:03:25.400 --> 01:03:31.160]   The universe tends towards entropy towards this organization, right?
[01:03:31.640 --> 01:03:36.520]   Except with us we are heading towards organization. How does that happen?
[01:03:36.520 --> 01:03:41.960]   How does it we get more organized when in fact entropy is what is a driving force in the universe?
[01:03:41.960 --> 01:03:46.600]   And it is the desire of a gene to reproduce and to succeed
[01:03:46.600 --> 01:03:51.720]   That that forces this organization. It's kind of an amazing thing if you imagine
[01:03:51.720 --> 01:03:57.080]   Uh, all you needed to create at one point was one thing
[01:03:57.800 --> 01:04:01.320]   That was capable of reproducing itself. That's all you needed
[01:04:01.320 --> 01:04:06.120]   Let's say you had two molecules and they somehow figured out how to make another of the same
[01:04:06.120 --> 01:04:12.840]   That's all you need to trigger this whole thing because then what happens is the molecules that are better at doing that
[01:04:12.840 --> 01:04:22.040]   Outpace the ones that are worse at it and they become better and then become better and suddenly you have instead of entropy you have an organizational
[01:04:22.680 --> 01:04:27.080]   Arrow you move in the direction of organization. John you disagree with this you're shaking your head
[01:04:27.080 --> 01:04:30.760]   Huh
[01:04:30.760 --> 01:04:33.960]   Well earth is a closed system
[01:04:33.960 --> 01:04:41.720]   It's the hole we're going towards disorder in the hole we are but in a close in a yes as a whole entropy will win
[01:04:41.720 --> 01:04:43.480]   but
[01:04:43.480 --> 01:04:49.080]   temporarily we have a system moving in the opposite direction because of evolution right because of
[01:04:49.480 --> 01:04:53.320]   survival of the fittest or really just really the drive to reproduce and the
[01:04:53.320 --> 01:04:59.640]   And reproducers succeed and as they succeed the more they succeed the more of them there are and and on and on and on
[01:04:59.640 --> 01:05:04.680]   And you get this bigger and bigger organization. I don't know what i'm talking about. Anyway, uh
[01:05:04.680 --> 01:05:07.000]   Warner brothers is tech. Huh?
[01:05:07.000 --> 01:05:10.280]   Warner brothers is just signed a deal for ai driven
[01:05:10.280 --> 01:05:15.000]   Film management. We'll just cut that part out. Can we just edit that out? Thank you
[01:05:15.000 --> 01:05:17.800]   I told you my brain
[01:05:17.800 --> 01:05:19.800]   management system
[01:05:19.800 --> 01:05:26.760]   Sinelitic is the company analytics they have a ai driven project management system
[01:05:26.760 --> 01:05:32.120]   Uh, and I love the hollywurp reporters lead on this resistance is futile
[01:05:32.120 --> 01:05:39.480]   Warner brothers has become the latest studio to publicly embrace artificial intelligence, but this is a perfect example of
[01:05:39.480 --> 01:05:41.720]   uh kind of
[01:05:41.720 --> 01:05:46.360]   Linkbaity I hate the headline, but but yeah, this is good for them
[01:05:46.440 --> 01:05:49.320]   This is sure and it's no threat to anybody right
[01:05:49.320 --> 01:05:52.520]   You just trying to make a job more efficient save some money
[01:05:52.520 --> 01:05:59.320]   Warner's will leverage the systems comprehensive data and predictive analytics to guide decision-making at the green light stage. Uh, oh
[01:05:59.320 --> 01:06:04.520]   Uh, oh the integrated online platform can assess the value of a star
[01:06:04.520 --> 01:06:10.280]   You know an actor in any territory. How much a film is expected to make in theaters?
[01:06:10.280 --> 01:06:13.880]   Or on other ancillary streams. You know, it becomes formulaic
[01:06:14.120 --> 01:06:22.040]   Well, we've always done this imperfectly voice tried to do this is humans are imperfect sometimes a good movie. We will still do it imperfectly because
[01:06:22.040 --> 01:06:27.400]   We're feeding the people like surprises. This gives back. I guess to your entropy. I mean
[01:06:27.400 --> 01:06:36.040]   AI can only build on what has historically been successful in the data it has and
[01:06:36.040 --> 01:06:41.800]   You even see this with hit pop songs. Yes, there will be millions of right also hit pop songs
[01:06:41.800 --> 01:06:46.280]   And then there will be the random news success that will then drive changes to that
[01:06:46.280 --> 01:06:49.720]   In fact, the hollywood reporter raises that exact example. They said
[01:06:49.720 --> 01:06:53.800]   Nobody thought joker that would be a hit
[01:06:53.800 --> 01:07:00.200]   Uh, but it was a billion dollar surprise and no and no executive could have predicted it
[01:07:00.200 --> 01:07:07.240]   But it's a huge issue. Hey, I I one of my students works at kpcc laist
[01:07:07.240 --> 01:07:10.840]   Um and and she was talking the other day with writer strike on
[01:07:11.560 --> 01:07:14.440]   AI is a sensitive topic in la
[01:07:14.440 --> 01:07:19.960]   Across the board. Yes, because it's a right use of a I scares the writers scares the directors scares the
[01:07:19.960 --> 01:07:22.440]   Illustrators and cinematographers
[01:07:22.440 --> 01:07:30.520]   And so it's gonna that's where it's gonna hit first. I think it scares the use cases mr. J. Yes. Yes. There's jobs
[01:07:30.520 --> 01:07:34.120]   So it is the next silicon valley versus hollywood
[01:07:34.120 --> 01:07:36.680]   But this story this
[01:07:37.480 --> 01:07:39.480]   Analytics
[01:07:39.480 --> 01:07:43.880]   Is this just project management in the sense that it's telling you like okay
[01:07:43.880 --> 01:07:49.960]   We need to get the following things and reminding people and then saying this is what we should prioritize and that sort of thing
[01:07:49.960 --> 01:07:52.120]   Yeah, because I mean my machine and edge
[01:07:52.120 --> 01:07:58.760]   Right. So the idea is instead of executive spending time trying to figure out what it stars worse
[01:07:58.760 --> 01:08:03.160]   And uh, what a market can deliver the machine will do it
[01:08:03.640 --> 01:08:08.680]   Uh, in fact the founder a Tobias quicer says artificial intelligence sounds scary
[01:08:08.680 --> 01:08:11.160]   But right now an AI cannot make any creative
[01:08:11.160 --> 01:08:18.760]   Decisions what it's good at is crunching numbers breaking down huge datasets and showing patterns that would not be visible to humans
[01:08:18.760 --> 01:08:24.120]   But for creative decision making he says you still need experience and gut instinct
[01:08:24.120 --> 01:08:30.360]   Yeah at the end of the day the executives can still say uh, we agree with this assessment right or disagree or not
[01:08:30.680 --> 01:08:36.440]   Maybe we'll make the judges some executive said, you know, we should make this right and he was right. I or she
[01:08:36.440 --> 01:08:41.560]   I think for most people gut instinct is actually pattern recognition. We just don't
[01:08:41.560 --> 01:08:45.560]   Totally that we're doing it. That's what our brain does is pattern recognition. Yes
[01:08:45.560 --> 01:08:50.120]   That's that that's that's that's what rosmer's point is too is where we're replaying video on our hands
[01:08:50.120 --> 01:08:51.560]   We see a pattern
[01:08:51.560 --> 01:08:55.880]   And the pattern may be good or bad by others judgments, but it's the pattern that we choose to go down
[01:08:56.360 --> 01:09:02.920]   There was a because we have theory about very famous, uh, chess family the polgar family
[01:09:02.920 --> 01:09:05.960]   Uh the the head of the family
[01:09:05.960 --> 01:09:10.200]   Uh who wasn't a great chess player himself the father
[01:09:10.200 --> 01:09:14.680]   Decided that the best way to become a great chess player would be see as many
[01:09:14.680 --> 01:09:18.360]   Positions as possible lazzlo polgar
[01:09:18.360 --> 01:09:21.480]   So as part of an educational experiment
[01:09:22.920 --> 01:09:28.360]   He wanted to prove that children could make exceptional achievements if trained in a specialist subject from an early age
[01:09:28.360 --> 01:09:31.480]   He taught his three dollars three daughters
[01:09:31.480 --> 01:09:37.880]   He taught his three daughters, uh, how to play chess. He they homeschooled him and this is in uh, hungry
[01:09:37.880 --> 01:09:42.520]   Uh, they also taught him esperana, which didn't work out so well, but he taught him chess
[01:09:42.520 --> 01:09:46.280]   and in fact the all three daughters became
[01:09:46.280 --> 01:09:52.840]   Chess grandmasters and one of them judit polgar is the woman's world was the woman's world champion
[01:09:53.560 --> 01:09:59.640]   Nice, uh, and it wasn't based on some sort of child, you know, uh, ability each other ability
[01:09:59.640 --> 01:10:02.840]   It was based on yes, it's reps and what and it was reps
[01:10:02.840 --> 01:10:08.360]   And it was in fact, uh, polgar put out a very thick book which I have
[01:10:08.360 --> 01:10:12.200]   That's just thousands of chess positions and you look at it. I say the move here is this
[01:10:12.200 --> 01:10:14.760]   the more you play
[01:10:14.760 --> 01:10:19.160]   The more you look at these positions the more you absorb the better your pattern recognition the better so so
[01:10:19.960 --> 01:10:25.000]   She learning is consciousness. It's just like well. What's interesting. Okay, but what's interesting is
[01:10:25.000 --> 01:10:30.040]   Inches at least it's the only thing I know anything about humans aren't capable of the kind of calculations
[01:10:30.040 --> 01:10:34.520]   Uh machine is so machine can in fact calculate 20 30 40 moves ahead
[01:10:34.520 --> 01:10:37.880]   Yeah, and and it does have to at some point assess a position
[01:10:37.880 --> 01:10:42.360]   But but it can do much more calculations so humans actually have to do more pattern recognition
[01:10:42.360 --> 01:10:47.800]   The machine because we can't calculate so we have to after four or five moves go. Well, that look that's the right
[01:10:47.960 --> 01:10:50.440]   That's that pattern. I recognize I want that
[01:10:50.440 --> 01:10:54.920]   And that's not quite as good. Unfortunately as a machine can do
[01:10:54.920 --> 01:10:58.680]   Machines now beat all the best human players
[01:10:58.680 --> 01:11:05.800]   Uh, you know frankly you could have a program on your phone now that beats the best human players, which is mind boggling
[01:11:05.800 --> 01:11:09.960]   So we still just yeah, I love chess. Yeah
[01:11:09.960 --> 01:11:14.920]   Um, I bet you've played it in years. I was I was much better as a young man, but
[01:11:15.560 --> 01:11:19.400]   Uh, but the more I play the better I get and the more positions I look at the better I am
[01:11:19.400 --> 01:11:21.800]   it's very much about
[01:11:21.800 --> 01:11:27.320]   Pattern recognition for humans not I think a little less so for machines actually
[01:11:27.320 --> 01:11:29.880]   They're able to calculate a lot better
[01:11:29.880 --> 01:11:33.080]   And that's our ai thing. Oh
[01:11:33.080 --> 01:11:36.280]   That was exhausting good sound Stacy do the sound
[01:11:43.800 --> 01:11:48.440]   Our show today. Oh, man. I just changed the sheets. I did not want to take the book linens off
[01:11:48.440 --> 01:11:54.040]   We only have one set of brook linens. Oh, and I just changed the seats tonight will be sleeping on something else
[01:11:54.040 --> 01:11:56.120]   I'm gonna have to buy some more is what I'll have to do
[01:11:56.120 --> 01:11:58.280]   brook linen
[01:11:58.280 --> 01:12:01.400]   Sleeping during the hot summer can be difficult to say the least
[01:12:01.400 --> 01:12:07.960]   Whether you're trying to nap after some fun in the sun or you're just struggling to stay cool at night brook linens award winning
[01:12:07.960 --> 01:12:09.080]   betting
[01:12:09.080 --> 01:12:10.520]   Is here to help
[01:12:10.520 --> 01:12:14.680]   brook linens mission is to provide you with hotel quality luxury betting
[01:12:14.680 --> 01:12:21.320]   Delivered directly to your door at a fair price found by husband and wife do a rich and vicky in 2014
[01:12:21.320 --> 01:12:25.480]   In brook lin, of course brook linen has everything you need to live your most
[01:12:25.480 --> 01:12:31.080]   comfortable life easily upgrade your home with quality products and curated designs that will leave your guests
[01:12:31.080 --> 01:12:33.480]   Swooning brook linen
[01:12:33.480 --> 01:12:39.240]   Not brook lin brook linen has been making dream spaces a reality for almost a decade
[01:12:39.720 --> 01:12:41.880]   So they're the obvious choice of making your house a home
[01:12:41.880 --> 01:12:46.440]   Oh, and if you're looking for a natural option brook linen features an organic collection
[01:12:46.440 --> 01:12:51.640]   Brook linen is the internet's favorite sheets and while there's no such thing as the perfect sleep
[01:12:51.640 --> 01:12:55.000]   There is the ideal fabric for every kind of sleeper
[01:12:55.000 --> 01:12:59.240]   mica chose the classic crisp percale weave
[01:12:59.240 --> 01:13:02.680]   Lisa and I are sleeping on their luxuries
[01:13:03.480 --> 01:13:09.400]   Bettery smooth best-selling looks setting sheets. Mm. Well, we got the sheets. We got the pillowcases
[01:13:09.400 --> 01:13:13.320]   We got the towels. We got the bath mats. We got the whole thing build your own indoor
[01:13:13.320 --> 01:13:17.080]   Oasis to escape the heat the options are endless
[01:13:17.080 --> 01:13:22.760]   Do yourself the favor of simplifying your shopping by bundling bed bath or both together
[01:13:22.760 --> 01:13:27.800]   You can save time and up to 25% when bundling your new favorite home essentials
[01:13:28.520 --> 01:13:33.560]   Wirecutter and good housekeeping both awarded brook linen for their outstanding bedding
[01:13:33.560 --> 01:13:37.400]   They have over a hundred thousand five star customer
[01:13:37.400 --> 01:13:40.280]   Reviews one reviewer said quote I seem to get that
[01:13:40.280 --> 01:13:43.000]   wonderful sleeping temperature very quickly
[01:13:43.000 --> 01:13:47.640]   And stay there throughout the night versus my older cotton sheet sets
[01:13:47.640 --> 01:13:50.920]   Another said quote best sheets in the world like butter
[01:13:50.920 --> 01:13:53.800]   brook lin uses only the highest quality
[01:13:53.800 --> 01:13:56.520]   materials for all their
[01:13:56.520 --> 01:13:58.520]   products long staple cotton
[01:13:58.520 --> 01:14:04.040]   So that everything they create is built to last and feels great. I can say
[01:14:04.040 --> 01:14:08.520]   It feels great shop and store online at brook linen.com
[01:14:08.520 --> 01:14:12.680]   Today to give yourself the cooling sleep you deserve this summer use twig
[01:14:12.680 --> 01:14:17.160]   For $20 off your online purchase of $100 or more plus free shipping
[01:14:17.160 --> 01:14:21.320]   on brook linen.com. That's b r o okay brook
[01:14:21.320 --> 01:14:25.080]   Linen l i n e n brook linen.com
[01:14:25.720 --> 01:14:29.320]   promo code is twig for $20 off plus
[01:14:29.320 --> 01:14:31.960]   free shipping
[01:14:31.960 --> 01:14:33.960]   Thank you brook linen
[01:14:33.960 --> 01:14:34.760]   uh
[01:14:34.760 --> 01:14:36.760]   oh boy
[01:14:36.760 --> 01:14:41.240]   Oh boy. What was the uh infrastructure bill 1.7 trillion
[01:14:41.240 --> 01:14:43.160]   Right
[01:14:43.160 --> 01:14:47.720]   Was this the inflation reduction act or no this was the jobs and infra whatever
[01:14:47.720 --> 01:14:51.960]   I can't remember was that last year? That's the problem right somebody
[01:14:52.520 --> 01:14:56.120]   They've got to do a better job of marketing was that last year? I'm okay last year
[01:14:56.120 --> 01:14:58.280]   Okay, uh
[01:14:58.280 --> 01:15:05.800]   Among other things a lot of money allocated to improve broadband biden has announced an additional 42 billion dollar high speed
[01:15:05.800 --> 01:15:08.520]   internet initiative
[01:15:08.520 --> 01:15:16.360]   Acknowledging finally the reality that if you're not connected you're kind of left out in the 21st century. Yeah
[01:15:16.360 --> 01:15:21.320]   Um, i've only been saying that for what 15 years. Oh, yeah
[01:15:22.120 --> 01:15:28.740]   The plan the the goal is to give every american household access to high speed and every american household should be a r
[01:15:28.740 --> 01:15:33.720]   Access by 2030. It's to be a right each state will receive a minimum of $107 million
[01:15:33.720 --> 01:15:36.600]   19 states over a billion texas
[01:15:36.600 --> 01:15:39.400]   3.3 billion
[01:15:39.400 --> 01:15:41.400]   under the program
[01:15:41.400 --> 01:15:43.400]   That's amazing just so i can censor it
[01:15:43.400 --> 01:15:49.880]   But yeah, I know more than 7 percent of the country more than 8 and a half million homes and small businesses are underserved
[01:15:50.360 --> 01:15:54.760]   The government's standard is and it's a pretty low standard 25 megabits download
[01:15:54.760 --> 01:16:00.280]   per second yeah, they last changed. I mean they it was a big deal when they upgraded it from
[01:16:00.280 --> 01:16:04.040]   Wasn't like five at one point. Yeah. Yeah, it was I mean
[01:16:04.040 --> 01:16:10.440]   The definition of broadband has always lagged by the actual need for broadband by about a decade
[01:16:10.440 --> 01:16:15.800]   What would you say now? I would say a hundred is should be like taping. I would say 250
[01:16:15.800 --> 01:16:17.960]   Actually, yeah
[01:16:17.960 --> 01:16:24.520]   Well, I mean think about what we're because it well not just streaming but also multiple people in a house
[01:16:24.520 --> 01:16:32.600]   What have I said? How about if I said this 50 megabits per person in the household? Okay or user not person per user in the household
[01:16:32.600 --> 01:16:34.760]   You and your 30 devices
[01:16:34.760 --> 01:16:41.160]   You're your 30 not devices because that I'd really be I need but I have gigabit at home and that's that serves us
[01:16:41.160 --> 01:16:47.320]   Yeah, we got that during covid when when lisa was zooming. I was zooming michael was going to school via zoom
[01:16:48.120 --> 01:16:52.440]   Uh, we needed a gigabit and we hardwired at all the workstations and all that stuff
[01:16:52.440 --> 01:16:56.920]   Um, anyway, a lot of people don't have that obviously and I I think
[01:16:56.920 --> 01:17:04.280]   Given that it's going to be government subsidy. It's reasonable say look 25 megabits per second for downloads three megabits for uploads
[01:17:04.280 --> 01:17:08.840]   Yes, that's that should be higher, but it's it's a good minimum
[01:17:08.840 --> 01:17:15.160]   Anyway, how much of this comes from satellite versus uh, that's a good question satellite can't do
[01:17:15.160 --> 01:17:17.160]   You
[01:17:17.160 --> 01:17:24.360]   This is this is the sad thing about this and of course you know that the big telecom companies are very active and they're lobbying
[01:17:24.360 --> 01:17:27.400]   And while a lot of this I think should go to rural
[01:17:27.400 --> 01:17:31.000]   You know, iSPs and municipalities
[01:17:31.000 --> 01:17:37.080]   Yep, the Comcast and Cox's at at AT&T's of the world are lobbying like hell
[01:17:37.080 --> 01:17:41.160]   And I think they're getting the lion's share of this money sad to say yep pretty sure of it
[01:17:42.440 --> 01:17:48.760]   Well, there's a couple things happening. Yes, they're trying the argument also
[01:17:48.760 --> 01:17:52.360]   If you are at a rural area without access to broadband
[01:17:52.360 --> 01:17:58.440]   One of the reasons you don't have it is because it's a broadband historically up until about 5g
[01:17:58.440 --> 01:18:01.240]   some 4g has
[01:18:01.240 --> 01:18:08.840]   You haven't been able to deliver it wirelessly at the speeds that even at 2530 or 2530 right and so
[01:18:10.360 --> 01:18:13.880]   You'd have these small rural isp's that would come up
[01:18:13.880 --> 01:18:19.400]   In try to deliver it, but then they also had to pay for backhaul back to Comcast
[01:18:19.400 --> 01:18:26.040]   In Comcast would be like oh he went you you want backhaul to the the fast real internet?
[01:18:26.040 --> 01:18:29.800]   Yes, we'll charge you a lot or they wouldn't even offer it so
[01:18:29.800 --> 01:18:38.920]   Wireless with 5g and Verizon's actually doing it. They're actually doing wireless broad like fixed
[01:18:39.320 --> 01:18:43.160]   Wireless broadband to homes with their wireless service
[01:18:43.160 --> 01:18:49.240]   Which allows you to do it economically without having to dig trenches, which is super super super expensive
[01:18:49.240 --> 01:18:52.040]   um, so
[01:18:52.040 --> 01:18:54.040]   I don't know what Comcast is doing
[01:18:54.040 --> 01:19:00.440]   They're not going to be laying cable in those areas. I can't think well if they do they better put shark repellent on the cables
[01:19:00.440 --> 01:19:03.880]   Did you see that first is the orc is attacking yachts?
[01:19:03.880 --> 01:19:06.920]   Now it's sharks attacking undersea cables
[01:19:08.360 --> 01:19:13.960]   Always attacked undersea cables like we all like every every few years everyone's like holy cow
[01:19:13.960 --> 01:19:21.240]   Our internet is usually reliant on these cables under the ocean and ships and earthquakes and sea animals all disrupted ah
[01:19:21.240 --> 01:19:24.280]   and then
[01:19:24.280 --> 01:19:25.800]   Yeah
[01:19:25.800 --> 01:19:27.800]   Yeah, sorry so there
[01:19:27.800 --> 01:19:30.680]   Take that sharks
[01:19:30.680 --> 01:19:37.400]   Actually, uh, the story was that google has puts a kevlar on its undersea cables to keep the sharks from biting them
[01:19:37.960 --> 01:19:41.000]   But they keep biting them, but now they just break a tooth. So that's good
[01:19:41.000 --> 01:19:46.600]   Yeah, it's like the ema isn't the frequency that they're like oh you they actually there's a buzzing or something
[01:19:46.600 --> 01:19:52.280]   I don't know if it's buzzing, but I mean think about how a lot of undersea creatures have
[01:19:52.280 --> 01:19:58.680]   I would call it extra sensory perception to deal with undersea. Yeah, like life under the sea they need different
[01:19:58.680 --> 01:20:05.480]   Magnetic sensing. I don't know but whatever it is you're right 1987 the New York Times reports
[01:20:05.960 --> 01:20:12.120]   Sharks have shown an inexplicable taste for the new fiber optic cables that are being struggled along the ocean floor
[01:20:12.120 --> 01:20:15.480]   Linking in the United States your own button or pad
[01:20:15.480 --> 01:20:21.560]   Now it seems google is biting back according to network world's bread dinner look you want to see a video
[01:20:21.560 --> 01:20:25.080]   This is uh, this is a shark biting another sea cable
[01:20:25.080 --> 01:20:27.400]   Here you go
[01:20:27.400 --> 01:20:33.080]   Uh, this is an old video like a cat. Yeah, here comes here comes watch out
[01:20:33.800 --> 01:20:43.720]   Turn him. Oh, it's like a taste to you. Oh, that's taste horrible. I don't want to eat that
[01:20:43.720 --> 01:20:45.880]   They've been about this one
[01:20:45.880 --> 01:20:56.840]   Do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do do. Oh, maybe not maybe he's not going to eat that one
[01:20:56.840 --> 01:21:02.600]   Wait, here's another one. Damn. Damn. Damn. Now he's gonna swim. Oh, you see the shark like a computer. We're learned he learns
[01:21:03.000 --> 01:21:07.160]   They use polyethylene protective yarn
[01:21:07.160 --> 01:21:12.360]   On monday google infrastructures are earls hodel
[01:21:12.360 --> 01:21:15.240]   I don't think that's like ers hodel
[01:21:15.240 --> 01:21:17.000]   Errs hodel
[01:21:17.000 --> 01:21:18.280]   Is that how you say it?
[01:21:18.280 --> 01:21:19.400]   Errs hodel
[01:21:19.400 --> 01:21:23.720]   And that's the company is helping to build a new trans specific cable system
[01:21:23.720 --> 01:21:27.400]   To connect the united states japan at get ready for the speeds of
[01:21:27.400 --> 01:21:30.120]   60 terabits a second
[01:21:31.880 --> 01:21:36.200]   Yeah, but that's like all the internet traffic from Japan. I know I know I know
[01:21:36.200 --> 01:21:44.920]   Why are sharks attracted undersea cables slate says unclear several outlets have pointed out that sharks can sense electromagnetic fields
[01:21:44.920 --> 01:21:48.520]   So perhaps they're attracted by the current
[01:21:48.520 --> 01:21:52.920]   Alternatively a shark expert from cal state long beach suggests is the wired
[01:21:52.920 --> 01:21:59.640]   Eh, they're just curious. I just want to know what happens when I bite this if you're a shark that happens a lot
[01:21:59.960 --> 01:22:01.960]   That's animal learning animals
[01:22:01.960 --> 01:22:05.740]   Yeah, anyone with a dual expertise and that's animal intelligence
[01:22:05.740 --> 01:22:14.440]   Condrich thin behavior and electrical engineering is warmly invited to offer a market-pelling explanation in the comments below says slate
[01:22:14.440 --> 01:22:17.480]   Then good good get on that word
[01:22:17.480 --> 01:22:20.280]   Condrich thin
[01:22:20.280 --> 01:22:22.600]   Condrich thin
[01:22:22.600 --> 01:22:26.760]   Uh, I guess that means shark. I don't know. I was gonna say does that mean shark. Yeah
[01:22:27.720 --> 01:22:31.000]   You could just say shark and I have to look at that. No
[01:22:31.000 --> 01:22:37.400]   Why use shark when you could say condrich thin? I mean if you know that word why not? Why not?
[01:22:37.400 --> 01:22:41.080]   Uh, otherwise you're just wasting your brain space
[01:22:41.080 --> 01:22:43.640]   Condrich theus is a classic. It tastes
[01:22:43.640 --> 01:22:51.640]   Cartesianus fishes that have skeletons primarily composed of cartilage. Well, I use that when i'm on jeopardy. That's for sure
[01:22:51.640 --> 01:22:54.120]   Sure, it's a cartilaginous fish
[01:22:56.120 --> 01:23:01.640]   Condrich tin. Why do they why do they drink the end? Yeah, thank you
[01:23:01.640 --> 01:23:05.560]   I'm probably mispronouncing it and uh, let's see what else
[01:23:05.560 --> 01:23:08.040]   Uh, do we do do do do do?
[01:23:08.040 --> 01:23:11.240]   Okay, now it's we're down to the seeds and stems. So let me take a break
[01:23:11.240 --> 01:23:14.680]   Seeds and stuff and we will
[01:23:14.680 --> 01:23:15.800]   We have a double
[01:23:15.800 --> 01:23:19.240]   We haven't done the changelog. I just want to break them out. We will do the changelog
[01:23:19.240 --> 01:23:23.960]   And then we will pick through we'll pick through the seeds and the stems for some
[01:23:25.480 --> 01:23:27.480]   Some juicy stories
[01:23:27.480 --> 01:23:30.600]   You never heard me say that before no
[01:23:30.600 --> 01:23:35.320]   But it just clicked. Oh bite of my hot talk. No, that's okay
[01:23:35.320 --> 01:23:40.200]   They look calm you down old hippie here. Oh, man
[01:23:40.200 --> 01:23:47.320]   He said see this it really does look like except for the bites now dude. It looks like a prop as i'm sitting right next to you
[01:23:47.320 --> 01:23:53.000]   Like just you not have any like condiments on it. I mean, that's I
[01:23:53.720 --> 01:23:55.720]   Least they brought it in
[01:23:55.720 --> 01:23:59.320]   But don't you put oh did it not make it into the drawer?
[01:23:59.320 --> 01:24:03.800]   Sorry the carpets clean
[01:24:03.800 --> 01:24:08.440]   And the ants won't eat it. No, I ordered it with the works. What does the works mean to you?
[01:24:08.440 --> 01:24:12.520]   Yeah, put everything on it, right? And then it came with nothing
[01:24:12.520 --> 01:24:19.400]   Just the answer like a separate packet of containers like with sauerkraut and everything nothing
[01:24:20.680 --> 01:24:24.520]   Maybe the they I don't know and then i think it's time to call out that vendor
[01:24:24.520 --> 01:24:30.360]   Berk hats have sort of weird explanation. He said well if they put the works on it would have been seven different things
[01:24:30.360 --> 01:24:34.440]   And I said, yeah, that's why you call it the works
[01:24:34.440 --> 01:24:37.960]   Come on mr. Bird because you don't want to taste the hot dog
[01:24:37.960 --> 01:24:42.520]   Hey, I have the pixel tablet right here in front of me. You didn't even notice
[01:24:42.520 --> 01:24:47.240]   Yeah, I will uh, I will be talking about that. We have a google changelog. There's still a lot more to come
[01:24:48.760 --> 01:24:54.280]   Although I do have an obligation to get the show over in the next six minutes and 45 seconds really
[01:24:54.280 --> 01:24:59.240]   Yeah, there's no way. I made a promise to my wife. I'm saying we've got that soon
[01:24:59.240 --> 01:25:05.960]   Hold on. How long have we been talking even stasis and now we're 23. She said can you get the show in under 90 minutes?
[01:25:05.960 --> 01:25:08.440]   And I said no problem notch. No
[01:25:08.440 --> 01:25:12.360]   Dude, you can't even finish your ad in that time
[01:25:15.720 --> 01:25:19.400]   I'm just gonna sit back and watch and our our sponsor appreciates that
[01:25:19.400 --> 01:25:29.560]   Actually, this is I want to give this guy a full drift because it's our studio sponsor the great folks at aci learning
[01:25:29.560 --> 01:25:35.560]   Thanks to aci learning the days of boring archaic training methods are finally over
[01:25:35.560 --> 01:25:42.920]   Lack of meaningful impact shows up as a low engagement that translates to suboptable performance and you don't want that
[01:25:44.120 --> 01:25:49.800]   You and your team to serve to be entertained while you train to be empowered to keep your organization safe
[01:25:49.800 --> 01:25:57.800]   And secure it's simple with your it training isn't raising your team to the level you aspire to you need aci learning
[01:25:57.800 --> 01:26:01.240]   With the training industry's completion rate just 30
[01:26:01.240 --> 01:26:05.880]   Aci learning blows its competitors out of the water with an over 80
[01:26:05.880 --> 01:26:14.040]   Completion rate now this is the format that it professionals want in today's it talent shortage whether you operate as your own department or your partner
[01:26:14.040 --> 01:26:18.040]   of a larger team your skills must be at the bare minimum up-to-date
[01:26:18.040 --> 01:26:25.240]   94% of cios and ceasos agree that attracting a retaining talent is increasingly critical to their roles
[01:26:25.240 --> 01:26:31.800]   ACi learning helps you retain your team and entrust them to thrive while investing in the security of your business
[01:26:31.800 --> 01:26:39.080]   Aci learning keeps your skills up-to-date with over 7,000 hours of content available and new episodes added daily
[01:26:39.560 --> 01:26:47.720]   Your enterprise needs cohesive cutting edge training to keep your team compliant and ahead of the pack choose an existing course or let aci learning
[01:26:47.720 --> 01:26:54.120]   Combined modules for a tailored solution. You can even let them custom design a course to address your specific needs
[01:26:54.120 --> 01:27:00.520]   aci learning's private bootcamp will train your team alongside the most passionate and best subject matter experts
[01:27:00.520 --> 01:27:03.720]   Certified in the latest versions of each certification
[01:27:04.120 --> 01:27:10.520]   Full access to advanced reporting via aci learning's pro portal. You could track and manage your team's results manage seats
[01:27:10.520 --> 01:27:18.040]   Assign and unassigned team members for customized courses relevant to their position and access monthly progress usage reports
[01:27:18.040 --> 01:27:23.800]   Visual reports provide immediate insight into your team's viewing patterns and progress over any period
[01:27:23.800 --> 01:27:29.320]   aci learning trains thousands of aspiring tech and cyber professionals annually
[01:27:29.320 --> 01:27:33.880]   Including providing scholarships to individuals from diverse backgrounds and those transitioning
[01:27:34.040 --> 01:27:36.200]   Out of military service into civilian careers
[01:27:36.200 --> 01:27:43.880]   Join the always-on tech training solution in a rapidly changing world of technology aci learning is in the studio every day
[01:27:43.880 --> 01:27:53.080]   To record and share relevant content that impacts your business be bold train smart learn more about aci learning's premium training options
[01:27:53.080 --> 01:27:58.840]   across audit it and cyber security readiness at go.aci learning.com
[01:27:59.640 --> 01:28:07.880]   Twitch for teams of two to one thousand volume discount start at five seats fell out the format go.aci learning.com
[01:28:07.880 --> 01:28:15.640]   Slash twit for more information on a free two week training trial for your team go.aci learning.com
[01:28:15.640 --> 01:28:18.280]   slash twit
[01:28:18.280 --> 01:28:22.520]   So I did not order the pixel fold, but I have to say
[01:28:24.200 --> 01:28:29.560]   Rana, Mario at ours technica who did his broken four days so that's not a really good
[01:28:29.560 --> 01:28:32.840]   Maybe he is the only one
[01:28:32.840 --> 01:28:34.840]   Didn't handle it rough right
[01:28:34.840 --> 01:28:40.840]   Nobody else really been griping about it people seem to like the aspect ratio of the screen because more square
[01:28:40.840 --> 01:28:44.040]   Than than maybe the galaxy fold, which is taller
[01:28:44.040 --> 01:28:47.640]   Paul Therat just got his and I asked him to do a little review
[01:28:47.640 --> 01:28:52.680]   He's tried he bought one he's trying to decide whether to keep it and he asked like sleeping in mexico in five days
[01:28:53.000 --> 01:28:56.280]   So he has to guess the side before he leaves from mexico pressure
[01:28:56.280 --> 01:29:00.040]   I did order you might remember during the google.io this one on sale the pixel
[01:29:00.040 --> 01:29:07.640]   Tablet because I liked the idea and goul had talked about this earlier of a tablet the docs
[01:29:07.640 --> 01:29:10.920]   on a little base
[01:29:10.920 --> 01:29:12.520]   pogo pins
[01:29:12.520 --> 01:29:18.600]   Via pogo pins and a magnet obviously this is really just like a regular tablet up unfortunately
[01:29:19.240 --> 01:29:23.640]   A number of people have said that it and I have to agree. It's just kind of an everyday
[01:29:23.640 --> 01:29:31.080]   Android utility. I'm not I have to say I'm not what did you expect from it? Yeah, well, right good question, right?
[01:29:31.080 --> 01:29:37.480]   We did talk to somebody on sunday who loves his samsung galaxy tab
[01:29:37.480 --> 01:29:41.960]   And I I've always been pretty nice. I think those are if you're going to get an android tablet
[01:29:41.960 --> 01:29:45.960]   That's probably but yeah, but samsung adds a lot of features multitasking stuff
[01:29:46.280 --> 01:29:51.800]   This isn't just a you know, what's an android tablet if you look at it. It looks just like an android tablet. It runs apps
[01:29:51.800 --> 01:29:56.200]   Google apps it's 500 bucks
[01:29:56.200 --> 01:30:00.200]   And I have to say if you put it side by side with the nest hub max
[01:30:00.200 --> 01:30:02.680]   Which I did not ask the tech guys on sunday
[01:30:02.680 --> 01:30:07.560]   It looks very similar and it functionality what at least when it's docked is very similar
[01:30:07.560 --> 01:30:10.280]   So the really the main thing this has to offer
[01:30:10.280 --> 01:30:12.760]   Is it is you can undock it?
[01:30:12.760 --> 01:30:17.400]   It's a it's a standalone tablet in addition to you know, okay
[01:30:17.400 --> 01:30:22.360]   It does not have it does not have a thread radio in it, right?
[01:30:22.360 --> 01:30:24.600]   Which is not
[01:30:24.600 --> 01:30:26.520]   Which the hub right?
[01:30:26.520 --> 01:30:29.480]   so it's not performing the functions of a
[01:30:29.480 --> 01:30:37.800]   Plugged in tablet in your home. Oh, that's a very important point. Yeah, google. It's not a smart home controller
[01:30:38.040 --> 01:30:42.520]   It's an android tablet as opposed to a fuchsia tablet with those smart home features. Yeah
[01:30:42.520 --> 01:30:47.160]   Right, so it's I mean it may look similar but it doesn't have the same functionality
[01:30:47.160 --> 01:30:52.680]   Like and if you bought it to actually control your smart home in a matter world. It's not going to it's twice as much
[01:30:52.680 --> 01:30:58.040]   Distinction it's twice as much as the nest up, but it is a tablet and it's a nice tablet
[01:30:58.040 --> 01:31:01.480]   I mean you don't buy this because you want a smart home hub
[01:31:01.480 --> 01:31:05.080]   I mean we we said that from the very beginning really this is not a smart home up
[01:31:05.080 --> 01:31:07.160]   You buy it because you want a tablet
[01:31:07.800 --> 01:31:13.480]   Yeah, and I imagine the use and this is certainly how I plan to use it. It's replacing the hub max
[01:31:13.480 --> 01:31:18.200]   In my kitchen, which we use as a kitchen timer. It still does that
[01:31:18.200 --> 01:31:24.040]   Because google assistance. Oh, no, no, don't do that. This is terrible because what happens when somebody walks by and takes your timer?
[01:31:24.040 --> 01:31:29.400]   Oh, yeah, like this is telling anybody they can do that. I only I can do
[01:31:29.400 --> 01:31:33.320]   That's a good point. This is not do not use it as a smart home hub
[01:31:33.400 --> 01:31:38.280]   This is not the intended use at all and I think it'll frustrate most people if you tell them to use it
[01:31:38.280 --> 01:31:46.040]   Because they're gonna encounter that. Yeah, can I see? Yeah, you want the tablet or the doctor both because uh, the doc is useless to me
[01:31:46.040 --> 01:31:52.280]   But it's nice because the doc gets a little base. Although the speakers in the tablet doc is useless. It's a hundred and twenty nine dollar
[01:31:52.280 --> 01:31:59.240]   Charger. Yeah, it doesn't have its own microphone. Now including the price. Yeah, it doesn't do anything by itself. Yeah. Yeah
[01:31:59.240 --> 01:32:02.760]   Basically, you know what google did here. What is it? Google was like, hey
[01:32:03.720 --> 01:32:07.480]   Let's make a really cool smart home tablet slash
[01:32:07.480 --> 01:32:09.480]   uh
[01:32:09.480 --> 01:32:11.880]   Smart home hub slash tablet. It'll be awesome
[01:32:11.880 --> 01:32:17.080]   And then they realized that the bomb was going to be way too much if they added all this radios and functionality and made that
[01:32:17.080 --> 01:32:23.400]   Doc anything more than just a charger like by adding a microphone or any sort of computing into it or radios
[01:32:23.400 --> 01:32:26.520]   So then they ended up with this weird
[01:32:26.520 --> 01:32:31.000]   Stupid product that doesn't that's a good tablet
[01:32:31.000 --> 01:32:35.560]   But has this weird docking thing and then they double down on it being like this is a real problem
[01:32:35.560 --> 01:32:40.920]   You've got this tablet and people leave it out and they don't charge it such a big problem
[01:32:40.920 --> 01:32:44.840]   Well, I wouldn't mind if apple did something like this and I could have an eye
[01:32:44.840 --> 01:32:48.920]   Whoops, he's watching david letterman for some reason
[01:32:48.920 --> 01:32:55.720]   Actually, I actually clicked on our replaza, but oh yeah, it's our replaza on david letterman. Yeah, okay
[01:32:55.720 --> 01:32:59.720]   Yeah, I mean it's got decent speaker. It's it's not an iPad and that's one of the problems
[01:32:59.800 --> 01:33:05.400]   I have in general with android tablets is is a lot of android apps don't really use this the screen realist
[01:33:05.400 --> 01:33:06.520]   it's
[01:33:06.520 --> 01:33:10.040]   Android is not as mature a tablet ecosystem as uh as apples
[01:33:10.040 --> 01:33:16.440]   I was I asked for it because just looking at it over there on the table here. I was like that sort of looks like a cheap
[01:33:16.440 --> 01:33:22.360]   Sort of it's a medium. Look at the camera. It's not a great camera
[01:33:22.360 --> 01:33:29.080]   I'm holding it in my hand. It feels fine. It feels substantial. It just looks screens decent the refresh rates
[01:33:29.320 --> 01:33:33.240]   Yeah, not as good as the samsung, but it's decent. This is totally decent
[01:33:33.240 --> 01:33:38.120]   How much was it? Was it 500 500 for the tablet right now for the tablet and the base?
[01:33:38.120 --> 01:33:44.840]   I don't know about 500 but yeah, that's what an iPad would cost. I like the idea. I do like the idea of it docking
[01:33:44.840 --> 01:33:47.880]   uh, when to dial
[01:33:47.880 --> 01:33:50.520]   Dump to the opposite gin and tonic on
[01:33:50.520 --> 01:33:54.680]   Android that was because it doesn't it's not it's not a super strong magnet
[01:33:54.680 --> 01:33:58.840]   But it shouldn't be because otherwise, right? I mean I can lift him the base to try to get it off with the yeah
[01:33:59.160 --> 01:34:05.000]   Yeah, but but what you said a minute ago, it's not that it's a tablet that docks. It's a
[01:34:05.000 --> 01:34:08.440]   um, whatever you call these
[01:34:08.440 --> 01:34:13.000]   Desk things that smart display separate smart display that separates. Yeah
[01:34:13.000 --> 01:34:20.520]   No, it's not smart. It is a tablet smart display as well. It's not a smart display. It's a tablet that docks in
[01:34:20.520 --> 01:34:27.640]   And it should have been a tablet smart display. I don't disagree. I think you're right actually
[01:34:28.520 --> 01:34:33.080]   Um, I mean you don't it's in a Kevin tried it and he's like hey look as a tablet
[01:34:33.080 --> 01:34:37.000]   It's fine and and I was really excited about it because I thought it could be
[01:34:37.000 --> 01:34:42.760]   A smart display that I could also use as a tablet. I was like, hey, that's pretty cool those bezels
[01:34:42.760 --> 01:34:45.640]   I'm looking at it from here. That's that's old school. Yeah
[01:34:45.640 --> 01:34:49.720]   Oh my god people in your bezels the bezel contingent
[01:34:49.720 --> 01:34:53.960]   The tablet needs bezels
[01:34:53.960 --> 01:34:55.800]   Hold up my phone to no, but it's
[01:34:57.400 --> 01:35:00.840]   Because you have to have somewhere to put your phone. Yeah, I know I know
[01:35:00.840 --> 01:35:06.600]   But that that just looks like a lot of lost screen real estate. They could have given you another
[01:35:06.600 --> 01:35:10.040]   I'll use it. I will use it in my
[01:35:10.040 --> 01:35:15.880]   Schading this tablet. I I have youtube tv so I'll put you know news on there
[01:35:15.880 --> 01:35:22.520]   Sports when I just don't think it's 500 dollars worth is all yeah. I think it's another google flop. I hate to say it
[01:35:22.520 --> 01:35:26.200]   And I wonder how well they'll do with the fold because
[01:35:26.840 --> 01:35:28.440]   honestly
[01:35:28.440 --> 01:35:32.680]   They better bring that on set category pretty good to bring that price down on it full
[01:35:32.680 --> 01:35:39.080]   I was thinking though stacey buying the next flip. It's good. We're gonna cover the Samsung event. It's I think july
[01:35:39.080 --> 01:35:42.680]   Uh, they're gonna announce a new fold and a new flip and I was thinking of buying the flip
[01:35:42.680 --> 01:35:45.640]   If I get tired of it in a month, can I send it to you?
[01:35:45.640 --> 01:35:47.880]   Sure
[01:35:47.880 --> 01:35:50.040]   I'll swap I'll swap them again with my kid
[01:35:50.040 --> 01:35:55.160]   They'll like the flip. I think I think it's a good phone for people who don't want to
[01:35:55.720 --> 01:36:00.840]   Who just want a thing that's relatively small to carry in their pocket or person. That's all you need, right?
[01:36:00.840 --> 01:36:04.760]   I don't know my kids like an old man. They just
[01:36:04.760 --> 01:36:08.840]   Another phone
[01:36:08.840 --> 01:36:13.400]   Oh, that's good. I don't need any more changes in my life. I love your kid
[01:36:13.400 --> 01:36:15.640]   That's uh
[01:36:15.640 --> 01:36:18.520]   Oh, I did okay. So I do want to ask you about a home
[01:36:18.520 --> 01:36:20.920]   automation thing
[01:36:20.920 --> 01:36:24.680]   All right matter uh, it is matter as a matter of fact
[01:36:25.240 --> 01:36:27.480]   Uh, tepee link has announced new
[01:36:27.480 --> 01:36:30.440]   switches light switches
[01:36:30.440 --> 01:36:34.120]   That are their wi-fi light switches, uh, but they are matter
[01:36:34.120 --> 01:36:37.880]   And I thought now they're 25 bucks each and I was thinking
[01:36:37.880 --> 01:36:41.560]   and I asked my wife I said if I uh
[01:36:41.560 --> 01:36:45.560]   Would you mind if I replaced all of the light switches in the house
[01:36:45.560 --> 01:36:51.480]   With these tp link switches and she said it was very right there. Well, she said an important thing. She says
[01:36:52.520 --> 01:36:54.520]   What happens when the wi-fi goes out?
[01:36:54.520 --> 01:36:57.880]   Yeah, don't do that
[01:36:57.880 --> 01:37:03.960]   Okay, if you were missing and I will tell you I've literally been doing this now for 10 years and experimenting with that
[01:37:03.960 --> 01:37:06.360]   That's why I wasn't gonna buy it till I talk to you. You bet
[01:37:06.360 --> 01:37:10.840]   The only way you should make your light switches smart
[01:37:10.840 --> 01:37:15.640]   Is be a lutron. I'm so sorry. You have to buy the lutron hub if you do it
[01:37:15.640 --> 01:37:21.400]   But pop those buggies babies in they don't support matter yet. I really hope they will one day
[01:37:21.880 --> 01:37:23.400]   most reliable
[01:37:23.400 --> 01:37:24.760]   most functional
[01:37:24.760 --> 01:37:26.440]   freakin expensive
[01:37:26.440 --> 01:37:30.520]   But they support because like these don't support three way switches for example
[01:37:30.520 --> 01:37:36.680]   Yeah, you know what that is they do have dimmers, but this is exactly what Jason's house. You have to buy a dimmer or an on off right don't
[01:37:36.680 --> 01:37:39.400]   I mean
[01:37:39.400 --> 01:37:45.000]   They're they're relatively inexpensive at like 25 bucks for the switch. I think it's 28 bucks for the dimmer. Yeah
[01:37:45.000 --> 01:37:47.400]   They're matter certified
[01:37:47.400 --> 01:37:51.240]   With Wi-Fi if your Wi-Fi goes out you can still use the the switch and go
[01:37:51.240 --> 01:37:56.920]   You're gonna have to double hit it probably to make sure you could turn the lights off
[01:37:56.920 --> 01:38:00.360]   Although Jason said with the cassette as the lutron cassette is
[01:38:00.360 --> 01:38:07.480]   They work just like a regular switch does yeah, that's why just go with lutron. It's so freaking reliable
[01:38:07.480 --> 01:38:12.120]   I mean, I wish I I'm still waiting for a technology that's better than that and so far
[01:38:12.120 --> 01:38:16.840]   No, just so the thing of course the thing that got my eye was matter
[01:38:17.320 --> 01:38:20.680]   Because then I don't need it to special hub it will work with other matter devices
[01:38:20.680 --> 01:38:25.560]   I think if i'm going to go home automation in the long run, I should have everything be matter compliant
[01:38:25.560 --> 01:38:32.040]   Yes, and I think lutron will eventually support matter if it becomes necessary
[01:38:32.040 --> 01:38:35.880]   But for the reliability for the ease of programming and use
[01:38:35.880 --> 01:38:41.640]   And everything works with lutron basically having google does amazon does home kit even
[01:38:41.720 --> 01:38:45.240]   Okay, even apple. Yeah has lutron responded to any
[01:38:45.240 --> 01:38:50.600]   Questions like every week in these days. Yes. Go quiet. Hey
[01:38:50.600 --> 01:38:56.920]   Could you tell me if you're going to matter? They're like no news on that friday yet stacy. Oh spin, huh? You'll be the first to know
[01:38:56.920 --> 01:38:59.480]   Like
[01:38:59.480 --> 01:39:03.000]   So I promise lisa I wouldn't do it until I talk to stacy
[01:39:03.000 --> 01:39:07.880]   And uh, and then Jason said no don't do it now it is the lutron switches
[01:39:07.880 --> 01:39:10.600]   Are a little more expensive
[01:39:11.400 --> 01:39:12.600]   the
[01:39:12.600 --> 01:39:14.600]   the tp link uh are
[01:39:14.600 --> 01:39:16.920]   25 bucks
[01:39:16.920 --> 01:39:21.240]   And they look like I bought mine for like 45. I think wow
[01:39:21.240 --> 01:39:27.560]   Um, they have a cregar variety of switches. Uh, this is this is the kind of switch I have
[01:39:27.560 --> 01:39:30.280]   in my house now
[01:39:30.280 --> 01:39:33.240]   Uh, oh, so many three. Yeah, it's a rocker with
[01:39:33.240 --> 01:39:37.480]   Okay. Yeah, don't buy them on amazon. Just go to home depot. Okay, and you can buy like a
[01:39:37.480 --> 01:39:41.000]   10 pack for more reasonable about that's what i'll do that
[01:39:41.800 --> 01:39:44.920]   Okay, while we're in need of hub and i'll need a hub
[01:39:44.920 --> 01:39:46.760]   But I don't mind having a I have a hu hub
[01:39:46.760 --> 01:39:51.240]   I don't mind. I the problem is right now. I have to hear hu lights in every thing
[01:39:51.240 --> 01:39:57.320]   I want to control with my voice what i'd like to do is be able to have all the switches be voice controllable
[01:39:57.320 --> 01:40:03.400]   Um, one of the things that you know, oh go ahead if you are using hu and you have
[01:40:03.400 --> 01:40:09.880]   Now lutron makes a device that you can like literally and I think I just threw mine away
[01:40:10.520 --> 01:40:15.800]   Um, you can put it on your toggle switch. So you have rockers. So that's not gonna work
[01:40:15.800 --> 01:40:20.040]   If you have a toggle switch, they make a device. No, I don't want to I don't want to root goldberg
[01:40:20.040 --> 01:40:25.000]   I want to unscrew and by the way, you need a ground wire, which I have I checked you need it
[01:40:25.000 --> 01:40:27.320]   So I have three wires
[01:40:27.320 --> 01:40:29.640]   Uh, so but what I want to do is I want to
[01:40:29.640 --> 01:40:32.200]   That you couldn't tell that i've done this
[01:40:32.200 --> 01:40:35.960]   Right, at least it could still go up to switch turn on turn it off
[01:40:35.960 --> 01:40:40.440]   She won't know but I will know that I can then open my phone and turn all the lights on or off
[01:40:40.440 --> 01:40:41.960]   in the house and that kind of thing
[01:40:41.960 --> 01:40:46.520]   Yeah, just get lutron and they actually have a device they actually have one that doesn't require ground
[01:40:46.520 --> 01:40:51.720]   I can't remember which like p64 or five or whatever version of it is but if you have
[01:40:51.720 --> 01:40:56.200]   I have ground I have neutral everywhere. I checked sorry not ground neutral wire. Sorry
[01:40:56.200 --> 01:40:59.160]   If you have a neutral wire just get the ones with the neutral wire, but yeah
[01:40:59.160 --> 01:41:01.800]   63 bucks compared to 25 bucks
[01:41:01.800 --> 01:41:04.840]   Okay, go to home depot. I'll go to home depot
[01:41:04.840 --> 01:41:09.400]   I don't need the wall. I don't need the wall plate. I just need to switch. I gotta walk
[01:41:09.400 --> 01:41:11.400]   I'll use the existing wall plate and then
[01:41:11.400 --> 01:41:14.440]   Put the uh rocker in the wall plate
[01:41:14.440 --> 01:41:17.240]   Okay
[01:41:17.240 --> 01:41:22.360]   All right, that's why you are here with leo shopping with leo and stacey question. Yes
[01:41:22.360 --> 01:41:25.880]   and buy them all at once
[01:41:25.880 --> 01:41:31.400]   And when you buy them all at once just get your electrician to come out and install. I mean
[01:41:31.400 --> 01:41:34.920]   If you're gonna do them all I could do it. Oh man. What?
[01:41:34.920 --> 01:41:39.240]   Your fingers do you have to turn off the power before?
[01:41:39.240 --> 01:41:41.880]   For you, uh, do this. Yes, please do not worry
[01:41:41.880 --> 01:41:45.880]   Oh
[01:41:45.880 --> 01:41:55.880]   This is 120 volts. I mean if I wear rubber sole shoes with that be okay. No, no, no go you go right ahead
[01:41:55.880 --> 01:41:58.360]   Let me know when you do it
[01:41:58.360 --> 01:42:04.520]   Okay
[01:42:05.400 --> 01:42:10.280]   And the other thing if you have three way switches yes, what's a three way switch?
[01:42:10.280 --> 01:42:15.720]   Like if you have a switch in your kitchen that on one side of your kitchen that controls your kitchen and on the other side
[01:42:15.720 --> 01:42:19.960]   You have a switch. Yeah. Yeah, yeah, we have lots of three way switches. Yeah, so for those
[01:42:19.960 --> 01:42:26.120]   Oh, so the tp link wouldn't even work for you because they don't actually support through switches. Oh, well, then forget it but with lutron
[01:42:26.120 --> 01:42:28.120]   you're gonna want to do the
[01:42:28.120 --> 01:42:35.080]   Smart switch in one spot and then you'll get the like little pico remote and you'll install that where the three way portion it's
[01:42:35.720 --> 01:42:40.920]   Uh, we already have some video about doing the install here and I think
[01:42:40.920 --> 01:42:44.920]   It might be on the video. Yeah flashing light warning
[01:42:44.920 --> 01:42:52.440]   Seal of approval from amp pro it. All right. Well, this is good. This is why you know, this is why I do the show so I can ask
[01:42:52.440 --> 01:42:54.920]   the smartest people in the room
[01:42:54.920 --> 01:42:57.880]   Uh big questions like that and me
[01:42:57.880 --> 01:43:01.560]   Big questions. Could you consult on my home lighting project?
[01:43:02.680 --> 01:43:07.080]   It sounds like I probably shouldn't do this. It sounds like trouble big time. Oh, no, it's great
[01:43:07.080 --> 01:43:11.400]   You know what we actually use lights in our house instead of shouting at my kid
[01:43:11.400 --> 01:43:16.760]   So in I programmed wait a minute wait a minute. Wait a minute. You better explain how that works light signals
[01:43:16.760 --> 01:43:19.740]   So like to get attention like downstairs
[01:43:19.740 --> 01:43:22.840]   Downstairs you blink their lights lights
[01:43:22.840 --> 01:43:25.240]   And
[01:43:25.240 --> 01:43:27.240]   No, she doesn't know
[01:43:27.240 --> 01:43:29.480]   Do you do morse? Yeah, you go on off on off off off?
[01:43:29.480 --> 01:43:32.520]   No, no, no, I just it's just a little on off and the
[01:43:32.520 --> 01:43:35.960]   And then they go. Oh mom wants me and they come downstairs. I've seen
[01:43:35.960 --> 01:43:42.120]   Well, they come upstairs. Yeah, and it's nice because we're not shouting yet. It's like come to dinner whatever and then
[01:43:42.120 --> 01:43:45.000]   You can do it. So I have a little remote
[01:43:45.000 --> 01:43:50.920]   That controls both the downstairs lights the lights in their room and then oh, that's cool
[01:43:50.920 --> 01:43:55.720]   Even set up one of my husband's room now. So now I have separate remotes to like call but various people
[01:43:56.440 --> 01:43:59.480]   Do you do blinks braid waffle three blinks?
[01:43:59.480 --> 01:44:02.920]   My mom when I was a kid because she got tired of shouting dinner
[01:44:02.920 --> 01:44:07.480]   Got a little bell and rings it and now every time I hear a little bell my mouth. Well, you're an animal
[01:44:07.480 --> 01:44:18.760]   Can I ask you a shopping question real quick since we're on this yeah
[01:44:18.760 --> 01:44:24.680]   Sunjake needs an air purifier. You guys were talking about what you use. What do you recommend? Oh get the co-wet open air mega
[01:44:25.160 --> 01:44:27.160]   Corbary say it again
[01:44:27.160 --> 01:44:33.480]   Co-way air mega for big rooms or just the co-way for not big rooms. Yeah, we have the air mega is expensive. It's uh
[01:44:33.480 --> 01:44:39.400]   It's a big, you know box. It's but does the whole so we have that in the house. Oh, that's not my air mega
[01:44:39.400 --> 01:44:43.480]   No, I know mine square. This is the new there. There. Oh mine's that guy. Yeah, the large space
[01:44:43.480 --> 01:44:51.160]   So uh, yeah, and then but they're expensive and then this is the one that I ordered I told you this story
[01:44:51.640 --> 01:44:56.120]   I ordered a refurbished on Amazon to save me a lot of money and then come with filters
[01:44:56.120 --> 01:44:59.240]   I used it for a year during covid without the filters
[01:44:59.240 --> 01:45:06.440]   Because I didn't know I didn't look I just got it open the box set up. Okay, honey. We're protected
[01:45:06.440 --> 01:45:10.760]   And then I protect you and then as usual
[01:45:10.760 --> 01:45:18.360]   Year later. I have the light comes on on the thing and says new filters. I said great. I ordered filters are up and up
[01:45:19.720 --> 01:45:24.600]   There's no filters in here at all. Oh, so basically I have fan blowing the air around
[01:45:24.600 --> 01:45:27.720]   For a year
[01:45:27.720 --> 01:45:30.680]   So tell jake get filters if it doesn't come with them
[01:45:30.680 --> 01:45:37.160]   Right, but you can get the little thin ones. We have those as well. Those are 51 square foot for the 200
[01:45:37.160 --> 01:45:42.840]   200m. That's what that says. Yeah, it's uh, it's a basically a simple hepa filter
[01:45:42.840 --> 01:45:46.120]   But for smoke like are you worried about uh, Canadian?
[01:45:46.440 --> 01:45:50.840]   Smuck fires is that no just allergies this time. Oh allergies. Yeah, it works great for that
[01:45:50.840 --> 01:45:54.360]   Yeah, run it. So close the door in your bedroom
[01:45:54.360 --> 01:45:59.240]   Run it for an hour on high before you go to sleep and then just put it back on sleep mode
[01:45:59.240 --> 01:46:03.000]   And then it's awesome. Oh you're good. Sounds like you know
[01:46:03.000 --> 01:46:06.680]   Yeah, now if we look at a cold way right now
[01:46:06.680 --> 01:46:11.720]   If you use the air purifiers water filters and biden and bidet toilet seats
[01:46:11.720 --> 01:46:13.400]   Ooh
[01:46:13.400 --> 01:46:18.680]   Let me tell you something until you get a bidet toilet seat. I don't know living you don't know living
[01:46:18.680 --> 01:46:22.040]   I'm telling you
[01:46:22.040 --> 01:46:27.400]   There's there's two things we miss where Disneyland we're staying at the crappy one of the crappy Disneyland hotels
[01:46:27.400 --> 01:46:31.640]   And we miss our eight sleep, you know thing that warms the cools of beds
[01:46:31.640 --> 01:46:37.560]   And we miss the thing that the toto toilets spritzes spritzes your rear keeps you fresh and clean
[01:46:37.560 --> 01:46:40.840]   Yeah, I've been in my search history. That's scary. Oh highly recommended
[01:46:41.880 --> 01:46:44.600]   They used to tell us about vidcon yet. You didn't say about vidcon
[01:46:44.600 --> 01:46:47.800]   Okay, so the big takeaway for me that I thought was
[01:46:47.800 --> 01:46:50.440]   So vidcon is
[01:46:50.440 --> 01:46:54.360]   John and Hank green started this now almost 15 years ago
[01:46:54.360 --> 01:46:58.360]   Uh, it was a 2011 jeff 2013 something like that like that
[01:46:58.360 --> 01:47:03.480]   Uh as a way to bring creators you at the time youtube creators on youtube
[01:47:03.480 --> 01:47:07.320]   Uh together and they're with their audience and so there's a big fan
[01:47:07.880 --> 01:47:11.800]   Contingent of you and you told me this jeff you warned me a lot of teenagers
[01:47:11.800 --> 01:47:18.760]   There but then there are a lot of creators there and now of course, uh, it's since been bought by via calm
[01:47:18.760 --> 01:47:24.040]   So it has a slight there's an industry track. That's what lisa was there for the industry track
[01:47:24.040 --> 01:47:26.280]   That's on the third floor well away from the children
[01:47:26.280 --> 01:47:31.880]   The second floor is the creator track and then on the ground floor is the conference and there's
[01:47:31.880 --> 01:47:37.000]   The fans the fans there's booths, you know, there was a guy there was a long line trailing around
[01:47:37.000 --> 01:47:42.040]   I thought oh this has got to be somebody big and it was some minecraft guy who wears a mask
[01:47:42.040 --> 01:47:47.800]   Uh, and he's standing there son in autographs in his mask or do actually more like
[01:47:47.800 --> 01:47:52.600]   Selfies in his mask and people were just all of twitter about this
[01:47:52.600 --> 01:47:56.280]   Well, you you get you get wristbands and you get a lottery
[01:47:56.280 --> 01:48:02.760]   As to you sign up and you ask for people and then you get voices and so you get like three
[01:48:02.760 --> 01:48:04.360]   uh
[01:48:04.360 --> 01:48:06.280]   guaranteed selfies
[01:48:06.280 --> 01:48:11.480]   Then and then there are uh some little kind of auditorium sections inside the conference
[01:48:11.480 --> 01:48:16.040]   Area there were two of them and they told maybe a couple hundred people
[01:48:16.040 --> 01:48:19.640]   There were two side by side running at the same time both filled
[01:48:19.640 --> 01:48:27.720]   One was some sort of ghost hunter youtube show and there was another show and there the people there are rabid fans of that
[01:48:27.720 --> 01:48:32.600]   Things in fact I was standing there watching the guy and there's four guys in a panel saying yeah
[01:48:32.600 --> 01:48:36.200]   You want to see the next video and there the fans are screaming
[01:48:36.200 --> 01:48:38.280]   ah
[01:48:38.280 --> 01:48:42.680]   And suddenly show the next video and they're all excited so what I really took away from it is how
[01:48:42.680 --> 01:48:44.520]   uh
[01:48:44.520 --> 01:48:50.520]   It's really it's it's programming has changed dramatically since my youth where we had three channels three networks
[01:48:50.520 --> 01:48:56.360]   And there was that and everybody watched one of the civil guns island now. There's a thousand niches. We're one of them
[01:48:56.360 --> 01:48:58.520]   Yes, we're a niche pro broadcaster
[01:48:58.520 --> 01:49:04.840]   Everybody's a niche broadcaster now and so this this you know ghost hunting show had its real solid
[01:49:04.840 --> 01:49:07.240]   constituency and to them
[01:49:07.240 --> 01:49:09.880]   To them they were superstars up there on that stage
[01:49:09.880 --> 01:49:15.160]   But the people on the next stage, I don't know who that is but this is my ob and the guy in the last
[01:49:15.160 --> 01:49:18.840]   He's got his things so each of these is as famous
[01:49:18.840 --> 01:49:24.840]   to that constituency as paul noumen would be to that to use the new scale of
[01:49:25.480 --> 01:49:28.200]   Everything that it's the it's the mass of niches
[01:49:28.200 --> 01:49:33.240]   And and things come down for this ridiculous scale that we all have to like the same thing
[01:49:33.240 --> 01:49:39.480]   To the smaller scale the other the other session. I I loved there. I presume this will have it is their mental health sessions
[01:49:39.480 --> 01:49:45.240]   Yeah, they have a lot of attention to mental health to inclusion. I mean, it's very conscious
[01:49:45.240 --> 01:49:52.360]   And of course that's probably because of john and hank initially right and uh because they were very
[01:49:53.960 --> 01:49:57.800]   Forward thinking in that regard. So yeah, it's good. You know, it's good clean fun
[01:49:57.800 --> 01:50:03.880]   It was very interesting. It feels very alien to a person my generation
[01:50:03.880 --> 01:50:08.360]   Uh because it's so different from the medial landscape I grew up in but it is
[01:50:08.360 --> 01:50:12.520]   I mean what's obvious is this is this is what twit is is
[01:50:12.520 --> 01:50:17.960]   You know, we have a small there's we have army. Yeah, we have 700 000 unique
[01:50:18.680 --> 01:50:22.120]   Listeners every month, right? That's our and that's actually pretty big
[01:50:22.120 --> 01:50:29.080]   Maybe not compared to a mark is brown layer of mr. Beast, but that's that's decent for a niche for new media
[01:50:29.080 --> 01:50:32.680]   And I think it's enough. It's certainly enough to make a living
[01:50:32.680 --> 01:50:40.600]   On there mr. Beast was not there because weirdly can lion was at the same time. What is what is can
[01:50:40.600 --> 01:50:46.680]   Lions, uh, it's a it got me. Yes. You got it's the international festival of creativity
[01:50:47.480 --> 01:50:51.240]   Okay, yeah, that's that's that's the advertising industry
[01:50:51.240 --> 01:50:55.560]   So that's where mr. Beast and Emma chamberlin and all the big shots were
[01:50:55.560 --> 01:50:58.280]   They this has always been a problem for vidcon
[01:50:58.280 --> 01:51:02.040]   Is that they occur just by the happenstance they occur at the same time
[01:51:02.040 --> 01:51:07.080]   And so vidcon's been tron because I was on the industrial the industry advisory board for a few years
[01:51:07.080 --> 01:51:10.600]   Trying to get the advertising executives to come and respect
[01:51:12.280 --> 01:51:19.000]   This kind of creator video is always the quest for them. Yeah, and of course here's she or lizar's pictures
[01:51:19.000 --> 01:51:23.480]   From the french Riviera, you know, she's she's at can lions
[01:51:23.480 --> 01:51:29.160]   Uh, so, you know the wall street journals at can lions, right? So, um
[01:51:29.160 --> 01:51:31.080]   This is
[01:51:31.080 --> 01:51:37.880]   This is the problem. This is you know, there's two sets of creators in their split and the ones who are most successful are probably in france
[01:51:37.880 --> 01:51:39.320]   I hate to tell you
[01:51:39.320 --> 01:51:43.160]   They're not because who would not go to the french Riviera as opposed to l.a
[01:51:43.160 --> 01:51:48.360]   Well, it's also where the money is, you know, the fans are at vidcon right but the money
[01:51:48.360 --> 01:51:52.600]   Was john green there? I didn't see him
[01:51:52.600 --> 01:51:55.320]   And as I mentioned before the show began
[01:51:55.320 --> 01:52:00.360]   I didn't know this but later read an article that said oh actually all the action wasn't at the conference center at the ani
[01:52:00.360 --> 01:52:01.480]   I'm a convention center
[01:52:01.480 --> 01:52:07.080]   All the action was over at the high at a few blocks away where they that's that was where the creators stayed
[01:52:07.480 --> 01:52:09.960]   And in fact when the ceo of tiktok came
[01:52:09.960 --> 01:52:12.520]   To to vidcon
[01:52:12.520 --> 01:52:18.600]   They sponsored it last year. They were the key key sponsor last year youtube's back again this year is sponsor
[01:52:18.600 --> 01:52:24.440]   But but when the ceo of tiktok came he didn't even go to anahime convention center. He went to the high yet
[01:52:24.440 --> 01:52:25.880]   so
[01:52:25.880 --> 01:52:27.880]   So really that I was
[01:52:27.880 --> 01:52:30.520]   Maybe we didn't see the best of vidcon
[01:52:30.520 --> 01:52:33.560]   That was not the case when I went no i think that's changed
[01:52:33.560 --> 01:52:38.440]   Hotels right there. We think it's correct and only 50,000 attendees. I think was the the final number
[01:52:38.440 --> 01:52:41.400]   It was small it's smaller than it's been in years past covid heard it
[01:52:41.400 --> 01:52:45.160]   I'm sure it's coming back. I don't or maybe not because one of the problems
[01:52:45.160 --> 01:52:49.640]   With being all these niche shows is there's no central place
[01:52:49.640 --> 01:52:51.880]   to go
[01:52:51.880 --> 01:52:53.880]   right
[01:52:53.880 --> 01:52:57.160]   You got hank tried he announced this vidcon some years ago
[01:52:57.160 --> 01:53:03.000]   where he wanted the creators guild he wanted to create a you know a union around creators and
[01:53:03.640 --> 01:53:10.040]   Um, it couldn't get it going on the negotiation side. Yeah, it's trying to bring together a critical mass of creators
[01:53:10.040 --> 01:53:13.160]   Yeah, and that's what vidcon really stands for
[01:53:13.160 --> 01:53:19.400]   It was very interesting. I mean, I don't feel like any obligation to go and certainly no one it was
[01:53:19.400 --> 01:53:25.640]   No one had any idea who lisa or i were I mean to nobody we are so old school
[01:53:25.640 --> 01:53:31.480]   Right. Well, we're doing well. That's it. Well because in some ways we we're part of the creator
[01:53:32.200 --> 01:53:36.920]   Yeah, you system our panel was lisa on yeah, she was on an industry panel on b2b
[01:53:36.920 --> 01:53:38.600]   Uh, you know
[01:53:38.600 --> 01:53:43.560]   B2b creators and of course the consensus there was linked in you got to be on linked in you got to do your stuff on linked in
[01:53:43.560 --> 01:53:46.840]   I was like, I mean you showed sure I was thinking now. I see her on linked in
[01:53:46.840 --> 01:53:51.800]   Is yeah, each other the b2b people linked in is probably a good place to be although
[01:53:51.800 --> 01:53:54.360]   I don't know
[01:53:54.360 --> 01:53:58.840]   All right quickly because no one cares. It's time for the google changelog
[01:53:58.840 --> 01:54:01.720]   Oh that I can really sell it
[01:54:02.280 --> 01:54:04.280]   There's a change
[01:54:04.280 --> 01:54:06.280]   I sold that good off
[01:54:06.280 --> 01:54:11.640]   YouTube music now automatically add songs to your last playlist
[01:54:11.640 --> 01:54:14.840]   crickets
[01:54:14.840 --> 01:54:20.360]   Google introduces a new shop tab for rentals and purchases on android tv. This is google's
[01:54:20.360 --> 01:54:24.760]   Response to the fact that they took away the google play store, right?
[01:54:24.760 --> 01:54:29.160]   You can't buy tv and movies on the google play store. So they are now going to add this
[01:54:29.720 --> 01:54:31.720]   slowly to android tv
[01:54:31.720 --> 01:54:34.680]   If you want to buy
[01:54:34.680 --> 01:54:37.400]   Movies or tv shows, that's where you're going to do it
[01:54:37.400 --> 01:54:42.360]   And it will also show anything you've already purchased or purchased in the past from google play movies and tv
[01:54:42.360 --> 01:54:45.560]   Or youtube or google tv or android tv
[01:54:45.560 --> 01:54:48.280]   I actually don't I really think that google
[01:54:48.280 --> 01:54:51.160]   You know crumbcast with android tv is quite good
[01:54:51.160 --> 01:54:55.320]   Or is it the google crumbcast with google tv? I can never get
[01:54:57.240 --> 01:55:00.120]   One of them is really good. I have no idea which
[01:55:00.120 --> 01:55:06.680]   Google search is rolling out a perspectives filter for more personal and human results
[01:55:06.680 --> 01:55:12.440]   Hmm. This is because this is part of sge right?
[01:55:12.440 --> 01:55:15.320]   I think it's also part of the reddit problem
[01:55:15.320 --> 01:55:18.840]   Oh, this is a response to the lack of input
[01:55:18.840 --> 01:55:25.000]   From reddit if reddit goes away because of course everybody knows the real way to do a good google search is to add the word
[01:55:25.000 --> 01:55:27.560]   Reddit to the end of whatever your sister. I never knew
[01:55:27.560 --> 01:55:29.720]   Yeah
[01:55:29.720 --> 01:55:31.720]   How could you not know this?
[01:55:31.720 --> 01:55:33.720]   I know how about the show?
[01:55:33.720 --> 01:55:39.080]   Like even when you google things you have to scroll through all the crap and eventually you'll find something that looks like it might work
[01:55:39.080 --> 01:55:41.000]   For you and it's usually on reddit
[01:55:41.000 --> 01:55:46.520]   I think it's really interesting stacy that you said and I agree with you the google search results have gone to hell
[01:55:46.520 --> 01:55:52.680]   Oh, there's terrible. They they really are and i've been looking for replacement, but they i don't know what it is. Is it duck duck?
[01:55:52.920 --> 01:55:56.040]   Google's fault it's the webs fault and it's going to get worse
[01:55:56.040 --> 01:56:01.160]   There's a story i put on the rundown that basically warns us that when the llms are going to fill up the web
[01:56:01.160 --> 01:56:03.880]   Yeah, there's a crap out there. Yeah. Yeah
[01:56:03.880 --> 01:56:11.800]   Uh, so it's i think it's both though. I think google also has decided that they want to feature i mean search results have gone below the fault
[01:56:11.800 --> 01:56:18.280]   The clickable. Yeah, I mean like if I click the top result for anything i search for just out of habit because i'm an idiot
[01:56:18.280 --> 01:56:21.320]   That that's just wrong. Don't ever do that. Don't ever do that. You end up
[01:56:22.520 --> 01:56:29.800]   Well, you'll just not end up where you want to be google is updating see we now have to cover android because of the no more all about android
[01:56:29.800 --> 01:56:35.320]   So here you go, uh google is updating the android logo with a 3d robot head
[01:56:35.320 --> 01:56:39.800]   Really
[01:56:39.800 --> 01:56:43.480]   They put a shadow on it. I've heard a freaking drop
[01:56:43.480 --> 01:56:46.440]   You put that we've got 90 minutes in this show and that's what you're
[01:56:46.440 --> 01:56:52.440]   All right. Let's move on google is giving fine my device a new logo. Okay. You know what that's it for the google
[01:56:52.440 --> 01:56:54.440]   changelog the hell with it
[01:56:54.440 --> 01:56:59.320]   All right last chance
[01:56:59.320 --> 01:57:01.160]   Last chance
[01:57:01.160 --> 01:57:03.160]   Uh, I'd like to wrap this up. Uh
[01:57:03.160 --> 01:57:05.480]   Anything
[01:57:05.480 --> 01:57:11.000]   I see a lot of uh stuff you added jeff is there anything? Was there some news about google and canada?
[01:57:11.000 --> 01:57:12.920]   Yeah
[01:57:12.920 --> 01:57:19.480]   Forces google and facebook to pay news outlets for linking to articles. This is the like song g
[01:57:20.360 --> 01:57:22.360]   Lifestyle
[01:57:22.360 --> 01:57:25.640]   I can never see that as a result meta says all right fine
[01:57:25.640 --> 01:57:28.840]   No more news for you can no news for you
[01:57:28.840 --> 01:57:34.360]   Um and meanwhile, it's it's just a mess. I put up a really good column
[01:57:34.360 --> 01:57:40.280]   Somebody wrote michael geys. She's been covering this all along c18 right after this is announced right after this announced
[01:57:40.280 --> 01:57:47.400]   Um the toronto star company formerly tor star and post media, which is the conglomerate that's taking everything up
[01:57:47.720 --> 01:57:51.480]   And now it's like an emerge basically taking away all competition in nudes
[01:57:51.480 --> 01:57:55.480]   They pushed for this law and there's going to be less news all around
[01:57:55.480 --> 01:57:59.720]   Facebook's not going to link to news or there's going to be no competition
[01:57:59.720 --> 01:58:02.600]   It's a blood the tv stations are now
[01:58:02.600 --> 01:58:09.000]   Trying to push hard to take away their requirement for local news or reduce it. It's a bloody mess
[01:58:09.000 --> 01:58:16.360]   And it all comes because the hedge fund owned big companies and investor companies went after
[01:58:16.840 --> 01:58:20.440]   They were political they bought their votes to get this protectionism
[01:58:20.440 --> 01:58:25.720]   And it's a disaster and and what's happening is I submitted a letter to the california senate
[01:58:25.720 --> 01:58:30.200]   Because the california journalism protection act or not protection
[01:58:30.200 --> 01:58:32.680]   um for the preservation act
[01:58:32.680 --> 01:58:35.800]   Is uh, I think coming up for debate this week
[01:58:35.800 --> 01:58:40.280]   And it's a disaster like the canina one like the jc pa, which is the one in congress
[01:58:40.280 --> 01:58:44.440]   Um, it's box sheesh for news companies full disclosure
[01:58:45.160 --> 01:58:48.120]   Google gives money to my school, but it's still stupid
[01:58:48.120 --> 01:58:52.520]   It's dangerous. It's as bad as that hot dog. It is
[01:58:52.520 --> 01:58:58.840]   Not bad. It's good. It's just I mean that hot dog could be dangerous. This is the rupert mordock of hot dogs. It just
[01:58:58.840 --> 01:59:01.960]   It's just will not die
[01:59:01.960 --> 01:59:04.200]   um
[01:59:04.200 --> 01:59:08.120]   What do I I would be remiss if I didn't cover
[01:59:08.120 --> 01:59:11.720]   What is turning out to be the worst?
[01:59:12.520 --> 01:59:15.720]   Corporate mismanagement Warner brothers discovery
[01:59:15.720 --> 01:59:21.480]   God what a mess they're making everything the latest is turn our classic movies, which is
[01:59:21.480 --> 01:59:26.440]   arguably the greatest gift to movie lovers ever especially in the marvel
[01:59:26.440 --> 01:59:30.040]   cinema star wars universe world of movies
[01:59:30.040 --> 01:59:32.680]   Uh, Warner has now fired
[01:59:32.680 --> 01:59:39.560]   Uh the five most senior executives at tcm through a mix of buyouts and pink slips
[01:59:40.200 --> 01:59:43.560]   Uh Warner brothers discovery. We didn't name for this the evil empire
[01:59:43.560 --> 01:59:49.800]   How about that as uh promised the viewers would no long would no see no change to ccm the management's all gone
[01:59:49.800 --> 01:59:53.320]   But there's no change the channel will remain free of ads
[01:59:53.320 --> 02:00:00.760]   But it's losing money like crazy. Uh, we remain fully committed to this business producers came for an emergency meeting with zazlove
[02:00:00.760 --> 02:00:04.600]   I swear. I think I saw the office on netflix again
[02:00:04.600 --> 02:00:08.840]   Recently no one no one knows if tcm
[02:00:09.800 --> 02:00:12.840]   Will survive or not. I pray that it continues
[02:00:12.840 --> 02:00:17.080]   Uh, Ryan Reynolds
[02:00:17.080 --> 02:00:20.040]   Ryan Reynolds
[02:00:20.040 --> 02:00:24.360]   Said uh, he told his 21 million followers on twitter tcm was a fixture in his life
[02:00:24.360 --> 02:00:31.240]   The channel is a holy corner of film history and a living breathing library for an entire art form right on ryan
[02:00:31.240 --> 02:00:33.560]   Um
[02:00:33.560 --> 02:00:39.240]   Mark harris a journalist and film historian called the cuts a catastrophic talent purge pet and azwald
[02:00:40.040 --> 02:00:42.040]   blame david zazlove
[02:00:42.040 --> 02:00:45.800]   Saying you couldn't just leave this one alone
[02:00:45.800 --> 02:00:48.360]   So
[02:00:48.360 --> 02:00:56.040]   So far, you know besides firing everybody no changes will see zazlove says i keep tcm playing in my office
[02:00:56.040 --> 02:01:01.480]   Okay, new rumor is that uh, comcast will buy
[02:01:01.480 --> 02:01:05.480]   Discovery mgm whatever whatever whatever
[02:01:06.200 --> 02:01:10.280]   No, i'm sorry mgm. That's that that's episode. But we'll buy this mess
[02:01:10.280 --> 02:01:14.840]   One of these just so sad or anti-trust imaginable
[02:01:14.840 --> 02:01:19.640]   Uh, you know, I guess it's the me. I don't know what's going on. This is this is the
[02:01:19.640 --> 02:01:22.920]   the uh other side of the vidcon story
[02:01:22.920 --> 02:01:27.960]   Which is you know media is changing dramatically and traditional media is in an in a tizzy
[02:01:27.960 --> 02:01:32.040]   Is it is the end of the mask my friends the end of the mask succession? Yeah
[02:01:32.840 --> 02:01:36.840]   Who will win the cage match between zuck and musk? Oh, gosh
[02:01:36.840 --> 02:01:47.080]   Oh, so I would have said that that zuck what did we say he did the the murphy? Yeah, uh, I mean that's a tough
[02:01:47.080 --> 02:01:51.880]   I would have put my money on on these guys. This kid besides the fact that he's like 10 years younger or more
[02:01:51.880 --> 02:01:54.120]   20 years younger
[02:01:54.120 --> 02:01:56.920]   This is quick. This is almost over Godzilla versus birth row
[02:01:58.360 --> 02:02:00.680]   I don't even want to talk about it except I think
[02:02:00.680 --> 02:02:04.760]   I think there's serious. I think they're serious about a cage match
[02:02:04.760 --> 02:02:11.560]   Just just yet the other day elan lex freedman the podcaster and mit genius
[02:02:11.560 --> 02:02:17.800]   Post okay. These people are all just getting in on this for the media cloud. I cannot believe we're this is the lowest
[02:02:17.800 --> 02:02:21.240]   Wait, i'm gonna give them their time. It was at the bottom. It's blow the fault
[02:02:21.240 --> 02:02:23.240]   Uh
[02:02:23.240 --> 02:02:25.240]   lex freedman posted video of him
[02:02:25.240 --> 02:02:27.240]   wrestling
[02:02:27.320 --> 02:02:29.320]   elan musk yeah
[02:02:29.320 --> 02:02:32.520]   What's wrong with these people
[02:02:32.520 --> 02:02:36.040]   As uh, have you not heard the update on this story? Oh, what's the latest?
[02:02:36.040 --> 02:02:41.080]   elan's mom came out and said like no, this isn't happy. Yeah, his mom. Oh, elan's ass would get
[02:02:41.080 --> 02:02:45.800]   Whooped but his mom came and saved him. I think his mom should come in and beat up zuck
[02:02:45.800 --> 02:02:50.600]   At least give his turn talking to right or zucks dead us dad
[02:02:50.600 --> 02:02:53.880]   Or i mean what's wrong with these people?
[02:02:54.040 --> 02:03:02.520]   Arguably the less the lesser of two evil here is definitely isn't that a sad statement look good when mark zucker brook looks good
[02:03:02.520 --> 02:03:05.720]   Because he doesn't look like he's just
[02:03:05.720 --> 02:03:14.280]   I mean he's just the lesser of two evils, but yeah, what what the hell no this is just boys ego stupid
[02:03:14.280 --> 02:03:17.000]   testosterone, you know skip
[02:03:17.000 --> 02:03:22.280]   What are we gonna do though boys are there's 50 of the world is boys?
[02:03:22.280 --> 02:03:24.280]   I
[02:03:24.280 --> 02:03:31.320]   Know how much the power is boys. Yeah, yeah fix that don't let's not get into this
[02:03:31.320 --> 02:03:34.760]   Stacey you'll never get to dinner
[02:03:34.760 --> 02:03:40.200]   Well, all right elan's musk mom can't stop the mark zucker bird cage fight
[02:03:40.200 --> 02:03:51.240]   Uh, let us do your pick of the week so that you can go on to things much more worthy of your time stacey
[02:03:52.200 --> 02:03:56.920]   Oh, well no, um, we can do well actually now, you know what yes
[02:03:56.920 --> 02:04:01.080]   It's your time now is your time
[02:04:01.080 --> 02:04:04.280]   um
[02:04:04.280 --> 02:04:06.840]   I'm sorry. Do you have anything?
[02:04:06.840 --> 02:04:15.000]   I brought two things ever on a pixel tablet. I brought those tp link
[02:04:15.720 --> 02:04:20.840]   Why the hot dog which is he brought the hot thing and bring the hot dog a plastic hot dog
[02:04:20.840 --> 02:04:23.480]   Within and I even ate it. I
[02:04:23.480 --> 02:04:26.360]   And nothing from you stacey. It's not
[02:04:26.360 --> 02:04:29.240]   Fun
[02:04:29.240 --> 02:04:35.480]   Yet so I don't want to show it until next week. Okay, save like wait, my thing of the week is this hot
[02:04:35.480 --> 02:04:40.680]   Thing just it doesn't look right does it no?
[02:04:41.400 --> 02:04:48.200]   Oh, it does not it's not right. It's just too perfect. I don't know if I'd say perfect
[02:04:48.200 --> 02:04:57.720]   And what did you have for lunch today? I had I had burgers
[02:04:57.720 --> 02:05:03.720]   I had no no hanging rings. I'm never ordering a hot dog again. That's oh man. That was good jeff
[02:05:03.720 --> 02:05:07.320]   What's your number of the week? Oh, oh, I know this a big one
[02:05:07.320 --> 02:05:10.360]   Oh, you want that one? Well, you give me anyone you want
[02:05:11.160 --> 02:05:14.120]   Well, that's fine. So so there was arguments about this
[02:05:14.120 --> 02:05:21.720]   The the it is said that the typical open AI engineer makes $925,000 now of course
[02:05:21.720 --> 02:05:24.520]   That's not salary
[02:05:24.520 --> 02:05:26.920]   It's a base of
[02:05:26.920 --> 02:05:35.800]   300 and 625 in stock based compensation for a stock that has no actual real set value in the real world yet
[02:05:35.800 --> 02:05:36.920]   Okay
[02:05:36.920 --> 02:05:41.560]   But be that as a may they're filthy rich these people three unless the company goes
[02:05:41.560 --> 02:05:46.200]   Okay, they're both you rich on paper. I've been filthy rich on paper
[02:05:46.200 --> 02:05:49.560]   Um or my husband has rather but we're
[02:05:49.560 --> 02:05:52.760]   Split our assets. Yeah
[02:05:52.760 --> 02:05:56.360]   You know what we ended up buying with that after after we had the dot-com bust
[02:05:56.360 --> 02:05:57.640]   What?
[02:05:57.640 --> 02:06:04.440]   We bought the tesla dining room chairs in a program that bought back the shares because they didn't want to have more than
[02:06:04.520 --> 02:06:10.040]   500 shareholders outstanding. Wow. So for a while we were millionaires. Oh, yeah, I remember
[02:06:10.040 --> 02:06:14.280]   Six dining room chairs. I remember going in we were grateful for those chairs
[02:06:14.280 --> 02:06:18.920]   Tech TV gave us shares and I remember going into the ceos office
[02:06:18.920 --> 02:06:22.120]   And I think this was the illegal and he was writing numbers
[02:06:22.120 --> 02:06:27.880]   He said if if the company's worth this much you're gonna be worth this I was like $10 million or something
[02:06:27.880 --> 02:06:30.440]   It was like the bell at dinnertime you started
[02:06:30.440 --> 02:06:33.000]   By salivating
[02:06:33.080 --> 02:06:35.080]   And it was all a lie
[02:06:35.080 --> 02:06:37.080]   You know
[02:06:37.080 --> 02:06:43.080]   It's just it's paper. It's paper value. Well what happened was and this happens a lot of times
[02:06:43.080 --> 02:06:44.840]   the
[02:06:44.840 --> 02:06:46.600]   lead investor
[02:06:46.600 --> 02:06:48.600]   Uh who was paul allen
[02:06:48.600 --> 02:06:52.200]   Gotta be made whole before anybody else saw any value of the stock
[02:06:52.200 --> 02:06:55.960]   So I had actual stock but paul got his because he lost his shirt
[02:06:55.960 --> 02:06:58.760]   He got his money first
[02:06:58.760 --> 02:07:00.760]   Which meant I got nothing
[02:07:00.760 --> 02:07:02.760]   I got diddly squat when cobcast
[02:07:02.760 --> 02:07:05.080]   Advisors shares
[02:07:05.080 --> 02:07:08.920]   Leo can I ask you to do more more through with this story? Can could you put the camera on your screen there or?
[02:07:08.920 --> 02:07:15.800]   Go to that story. Oh, I know my thing. Okay, but then y'all talk about okay. I'm on the story. I have a picture of sam altman
[02:07:15.800 --> 02:07:19.320]   Yeah, so now now just mouse over the twitter share
[02:07:19.320 --> 02:07:22.840]   Or click on the twitter share
[02:07:22.840 --> 02:07:24.120]   All right
[02:07:24.120 --> 02:07:28.600]   Purchase a link into this subscriber only article that has anybody to view the content for
[02:07:29.800 --> 02:07:31.800]   895 dollars
[02:07:31.800 --> 02:07:39.240]   Isn't that insane what is this you know biz journals calm? It's it's biz. It's my old employer's this new house
[02:07:39.240 --> 02:07:43.000]   It's it's my old employer too. Oh really?
[02:07:43.000 --> 02:07:50.680]   At 895 bucks. This is the family. Oh, yeah, it's a family. It's science
[02:07:50.680 --> 02:07:57.800]   So you give a free link a free link to a truncated article or you can buy it where anybody can read for 895
[02:07:57.800 --> 02:07:59.720]   Damn
[02:07:59.720 --> 02:08:03.880]   Do you think anybody buys that? No, I mean I might pay
[02:08:03.880 --> 02:08:10.520]   A dollar so 10 people could click on it or maybe $10 so 100 people could click on it
[02:08:10.520 --> 02:08:16.600]   Here's the free here by the way. I got the free link and it's got you know the lead and it's got a link
[02:08:16.600 --> 02:08:21.960]   But if I click that link will it be just say oh, sorry you can't be it'll be it'll be what you got
[02:08:21.960 --> 02:08:24.760]   It'll be what I gave you it's the first two paragraphs was all you need to know
[02:08:24.760 --> 02:08:27.320]   Oh, I didn't even know I wasn't getting the whole article
[02:08:29.320 --> 02:08:31.320]   You don't care. You didn't care
[02:08:31.320 --> 02:08:38.840]   These are articles for members. Oh soccer zone. I mean members only by the way one thing that drives me nuts about mastodon
[02:08:38.840 --> 02:08:40.520]   I love mastodon
[02:08:40.520 --> 02:08:46.520]   But whenever I put up a link to the washing potion the New York Times or whatever. Oh, yeah, I know I know
[02:08:46.520 --> 02:08:49.000]   If you go to if you do it on reddit
[02:08:49.000 --> 02:08:54.840]   They uh, they'll mastodon needs to do a deal like reddit where if you just log in you get like seven articles for free
[02:08:55.240 --> 02:08:58.840]   I just steal my articles by typing archive.ph in front
[02:08:58.840 --> 02:09:03.400]   No comment. I found my thing by the way. I remember okay
[02:09:03.400 --> 02:09:08.520]   I should probably put it in the things when I know what did you think doll before this? Give us your thing
[02:09:08.520 --> 02:09:11.800]   Oh my god. I am gonna smack you in
[02:09:11.800 --> 02:09:14.440]   I need you to do a punch
[02:09:14.440 --> 02:09:20.920]   We i'm sorry the ketamine just kicked in just go right ahead. Please hold a hot dog
[02:09:22.200 --> 02:09:25.000]   It's like rodo's hot dog. So yeah, didn't you see that?
[02:09:25.000 --> 02:09:27.560]   I would have said you were arguing that you were delusional when you
[02:09:27.560 --> 02:09:32.200]   He's not the supreme court says it's not his fault. He didn't know
[02:09:32.200 --> 02:09:35.160]   Because again, I think I'm gonna need that punch
[02:09:35.160 --> 02:09:41.480]   This punch mean I'll make her feel better. Here we go. I mean hit the button hit the button
[02:09:41.480 --> 02:09:44.440]   Hit the button. What was the book? Oh, I
[02:09:44.440 --> 02:09:50.520]   Thank you
[02:09:50.520 --> 02:09:52.520]   It's some serious negative
[02:09:52.520 --> 02:10:00.680]   All right, so my thing this week that I forgot completely about because I didn't put in the link my bad is
[02:10:00.680 --> 02:10:02.600]   nano leaf released
[02:10:02.600 --> 02:10:04.040]   Something that is
[02:10:04.040 --> 02:10:08.360]   Only cool for people who like are really into television or gaming but if you're into it
[02:10:08.360 --> 02:10:10.600]   It's actually a very affordable way to do ambient lighting
[02:10:10.600 --> 02:10:14.520]   So it's the nano leaf 40 screen mirror and light strip kit
[02:10:14.520 --> 02:10:16.280]   And this is a
[02:10:16.280 --> 02:10:22.440]   What this does is you stick this over your tv? It's got a little camera that's like processing the colors of what's on the television
[02:10:22.440 --> 02:10:24.680]   And then the light strip mimics. Oh, it's a bias light
[02:10:24.680 --> 02:10:28.840]   That's bias. Yes. Yes. Yeah, okay
[02:10:28.840 --> 02:10:35.160]   Um, so the huesink has done this for a while, but to get something like this you have to pay like
[02:10:35.160 --> 02:10:38.520]   It's like $400 to get the lighting and everything in the same box
[02:10:38.520 --> 02:10:42.520]   But this is actually for a 65 inch light strip in the little camera
[02:10:43.080 --> 02:10:46.760]   It is only $99 I or sorry $100
[02:10:46.760 --> 02:10:51.640]   It's pretty cool. And then an 85 inch light strip is like 120
[02:10:51.640 --> 02:10:53.640]   Oh, this is really did you have you done this?
[02:10:53.640 --> 02:10:56.760]   So we've I've done it with the hue
[02:10:56.760 --> 02:11:01.160]   Yeah, so I haven't tried this one out yet because it's not going to ship until july
[02:11:01.160 --> 02:11:05.240]   But if this is something you're interested in I love the nano leaf lighting
[02:11:05.240 --> 02:11:10.280]   The camera is pretty cool. They have a little privacy thing if you're worried about it taking
[02:11:11.000 --> 02:11:14.760]   So there's two ways they deal with privacy with this camera that like overlooks your tv
[02:11:14.760 --> 02:11:16.840]   Which admittedly is a little unattractive
[02:11:16.840 --> 02:11:18.520]   Um
[02:11:18.520 --> 02:11:20.280]   They let you
[02:11:20.280 --> 02:11:23.000]   Cover it or they also don't do any processing
[02:11:23.000 --> 02:11:29.560]   Except like in the cloud. It's all local on the device. I trust them. What is an antileaf gonna do with my information anyway
[02:11:29.560 --> 02:11:32.200]   Well, I mean
[02:11:32.200 --> 02:11:35.800]   Yeah, they're basically know what you're watching on tv, but
[02:11:37.080 --> 02:11:39.880]   They're they're they won't know that because none of that data goes to the cloud
[02:11:39.880 --> 02:11:44.200]   They're literally just saying these are the colors on the screen quickly mimicking to the thing
[02:11:44.200 --> 02:11:47.800]   I might actually do this. That's pretty cool. I've always wanted to do bias lighting
[02:11:47.800 --> 02:11:52.840]   And this is an inexpensive way to implement it. You do have to have that camera looking at your tv
[02:11:52.840 --> 02:11:54.840]   This is like dark mode. Yeah
[02:11:54.840 --> 02:11:57.960]   How is it like dark?
[02:11:57.960 --> 02:11:59.960]   Watch tv with all the lights on
[02:11:59.960 --> 02:12:03.800]   Yeah, he probably does
[02:12:06.760 --> 02:12:10.440]   The other cool thing they and I didn't know this was a thing because I'm not a gamer
[02:12:10.440 --> 02:12:12.920]   They actually synced their lights with
[02:12:12.920 --> 02:12:19.480]   Overwatch. Oh, which that's cool. You okay. Yeah, so when you get kills, it'll like turn
[02:12:19.480 --> 02:12:21.880]   Oh flood red. Yeah
[02:12:21.880 --> 02:12:26.680]   And if you die it'll flash it red and I'm like why don't we have like an api like this
[02:12:26.680 --> 02:12:32.680]   For sports. Yeah, you shouldn't need the camera. It should just be sending that information down the pipe
[02:12:33.240 --> 02:12:38.120]   You tried to build that it should light up like 10 years ago. That'd be awesome
[02:12:38.120 --> 02:12:41.560]   And what happened did the tv company didn't go along with it or
[02:12:41.560 --> 02:12:45.640]   It was a lot of it was a lot of integration on the media side
[02:12:45.640 --> 02:12:49.560]   And so it was very custom for it wasn't basically scalable
[02:12:49.560 --> 02:12:54.440]   But now you could do with I mean this is actually using computer vision and a AI to
[02:12:54.440 --> 02:13:00.600]   To quickly say oh these colors will work here go and they can do it rapidly enough. I'm a sucker
[02:13:00.600 --> 02:13:02.600]   I'm gonna buy this because um
[02:13:02.600 --> 02:13:08.360]   It's it's neat. Oh, yeah. I like this idea. I've spent a lot of money and they have four different layers
[02:13:08.360 --> 02:13:11.960]   They have yeah, yeah, I have two expensive
[02:13:11.960 --> 02:13:13.800]   Um
[02:13:13.800 --> 02:13:15.800]   All right, and I'm gonna get it for the bigger
[02:13:15.800 --> 02:13:18.520]   Up to 85 inches. I mean I
[02:13:18.520 --> 02:13:23.720]   I want to have other nanoleaf lights. It'll sync to those as well in the room
[02:13:23.720 --> 02:13:26.520]   I might be headed to ordana leaf heaven
[02:13:27.960 --> 02:13:32.680]   Yeah, so if you got a new media room or you got the new couch or what did you get you got reclining chairs?
[02:13:32.680 --> 02:13:37.320]   We got home theaters. See that's base man. I'm theater seating for our bedroom
[02:13:37.320 --> 02:13:43.480]   Oh, so you invited an audience. Well, that's living
[02:13:43.480 --> 02:13:46.840]   Come on over
[02:13:46.840 --> 02:13:53.320]   Watch little snore. It's got it's got their heated. It's got massagers
[02:13:53.320 --> 02:13:56.840]   Watch me snore
[02:13:57.640 --> 02:13:59.640]   That's hilarious
[02:13:59.640 --> 02:14:03.320]   It's a long story
[02:14:03.320 --> 02:14:09.000]   Lisa says there should we should do a reality show. Mm-hmm. Yeah. Oh, yeah
[02:14:09.000 --> 02:14:15.800]   Yeah, you should I had to take down the ceiling swing though because I didn't they were oh, I should probably
[02:14:15.800 --> 02:14:21.320]   Ant is like edging away. He's like no
[02:14:21.320 --> 02:14:23.320]   Yeah
[02:14:23.320 --> 02:14:34.520]   I'm busy buying these lights. Okay, Stacy. That's a good thing. Oh gosh, and I will uh, I'm gonna put these in the bedroom
[02:14:34.520 --> 02:14:39.000]   Jamerby is saying no, we shouldn't buy this for some reason. He's you shouldn't buy this
[02:14:39.000 --> 02:14:44.440]   I mean his I have a wage face going right now. Why should I not buy this? Not looking sin does not approve
[02:14:44.440 --> 02:14:48.600]   What is Scott? Well, cuz it told me about bias lakes
[02:14:50.520 --> 02:14:52.520]   They should not sink they should be 6500k
[02:14:52.520 --> 02:14:59.160]   Or the whole room will feel like immersive
[02:14:59.160 --> 02:15:02.120]   All right, I won't push the button
[02:15:02.120 --> 02:15:07.720]   I will ask Scott when he's talking about this home theater geeks like this last bias lady
[02:15:07.720 --> 02:15:11.560]   For him bias lighting is just a neutral light behind the tv
[02:15:11.560 --> 02:15:18.680]   Not attempting to match the content of the tv, which is what this does. Don't be distracted. He's right. No, man
[02:15:18.840 --> 02:15:20.840]   They do
[02:15:20.840 --> 02:15:25.640]   They have different levels of those so you could actually have what sounds to be bias lighting
[02:15:25.640 --> 02:15:28.680]   If the list level, it's okay because I'm sending it to my mother for some
[02:15:28.680 --> 02:15:32.360]   I don't know why
[02:15:32.360 --> 02:15:34.360]   Mom enjoyed the lighting
[02:15:34.360 --> 02:15:37.800]   Okay, you sent me a camera and lights
[02:15:37.800 --> 02:15:43.880]   Bob has a disco party Saturday night watching college football and the band strikes up when it touched down
[02:15:43.880 --> 02:15:46.360]   Wouldn't that be great? I think that would be pretty diagram
[02:15:46.360 --> 02:15:55.960]   I think if you need for nature documentaries, yeah, that's just me. Oh, yeah, you're all right. I'll ask Scott first
[02:15:55.960 --> 02:15:59.160]   You're right. Thank you. You stopped me from sending this to my mom
[02:15:59.160 --> 02:16:02.200]   And you do you have a pic?
[02:16:02.200 --> 02:16:06.200]   Uh, what was my pic? I know what it was Sony zv
[02:16:06.200 --> 02:16:10.280]   Oh, yes, Sony zv E1 there. Is this a vlogging camera?
[02:16:10.280 --> 02:16:14.040]   Yeah, that's their one of their latest vlog quote-unquote cameras
[02:16:14.040 --> 02:16:21.560]   But it's a full frame it recently you got a software update for a video where you can now shoot 4k 120 frames a second
[02:16:21.560 --> 02:16:27.480]   HD can shoot 240 frames a second which pretty much puts it on par with their
[02:16:27.480 --> 02:16:35.880]   With their fs cameras. Yeah, they're yeah, were there alpha z s3? Yeah, so you can get a decent video camera for a lot less money
[02:16:35.880 --> 02:16:37.880]   And it'll probably use my fx lenses, right?
[02:16:38.520 --> 02:16:41.640]   Yeah, they are all email usually so oh, that's cool
[02:16:41.640 --> 02:16:43.800]   That's that's a heck of a deal right there
[02:16:43.800 --> 02:16:48.520]   So this is a firmware update if you've already got it if you don't do you use this? Do you have it?
[02:16:48.520 --> 02:16:54.440]   I don't have it, but I don't have a problem with Sony cameras. Oh, I love my sonies. Yeah, they're pretty nice
[02:16:54.440 --> 02:17:00.120]   Yeah, but yeah, they have a firmware update if just in case you're curious, but my pick pick is
[02:17:00.120 --> 02:17:04.280]   Totally random. It's called beamer code beamer code
[02:17:05.480 --> 02:17:10.440]   Does it involve beamer's yeah involves BMW if you have a BMW or a mini
[02:17:10.440 --> 02:17:13.880]   And you want to play around which we do with your car
[02:17:13.880 --> 02:17:21.240]   Yeah, it's software beamer code allows you to connect to your obd with a particular obd dongle
[02:17:21.240 --> 02:17:25.160]   And you can play around with some of the things because with my car
[02:17:25.160 --> 02:17:28.520]   I recently had some issues with my
[02:17:28.520 --> 02:17:34.600]   daytime running lights and I got tired of seeing an error until I fixed the light and
[02:17:34.680 --> 02:17:38.120]   Found beamer code and it allowed me to go in there and turn that error off
[02:17:38.120 --> 02:17:43.960]   Nice. So I fixed the light. This is your new BMW that you just got. Yeah, I had a full little while
[02:17:43.960 --> 02:17:46.120]   It's nice. It looks very fancy
[02:17:46.120 --> 02:17:49.080]   Because of you and my wife who has a mini
[02:17:49.080 --> 02:17:51.880]   I'm thinking maybe my next EV will be a
[02:17:51.880 --> 02:17:55.080]   BMW is a shark
[02:17:55.080 --> 02:17:59.560]   I'm thinking with all that money I
[02:18:02.840 --> 02:18:07.800]   Time to upgrade it. No, that's what people think, isn't it? That's what they think. Yeah, that's what it is
[02:18:07.800 --> 02:18:09.880]   And I know I ain't the case
[02:18:09.880 --> 02:18:12.920]   But yeah beamer code lio. It's a it's a kia for
[02:18:12.920 --> 02:18:15.080]   a kia
[02:18:15.080 --> 02:18:16.600]   a kia
[02:18:16.600 --> 02:18:17.640]   a kia
[02:18:17.640 --> 02:18:18.680]   a kia
[02:18:18.680 --> 02:18:20.840]   I'm gonna drive an i kia car
[02:18:20.840 --> 02:18:23.560]   Do I have to assemble it myself? I would do that
[02:18:23.560 --> 02:18:27.880]   Yeah, I like my car ladies and gentlemen
[02:18:27.960 --> 02:18:34.920]   If you are not a member of the club you're missing out because tomorrow is a very big day stacy's book club is it nine am pacific
[02:18:34.920 --> 02:18:37.240]   noon eastern time
[02:18:37.240 --> 02:18:40.280]   And only new it's his latest the terra formers. Yep
[02:18:40.280 --> 02:18:47.720]   I hear it's a good book. I haven't read it. We'll find out tomorrow and can't like it or not. Did you have no comment?
[02:18:47.720 --> 02:18:51.320]   Oh, we're gonna it's gonna be no comment. We're it's gonna be surprised
[02:18:51.320 --> 02:18:54.040]   John loved it. We know that
[02:18:54.040 --> 02:18:58.280]   Uh then at one p.m. We know everybody loves silo the new apple tv plus
[02:18:58.280 --> 02:19:07.080]   Series which concludes friday. How about apple putting that first episode out to the public for every twitter? Yeah, that's crazy
[02:19:07.080 --> 02:19:11.480]   Is that wild? That's that's marketing here. Let me give you a taste of it
[02:19:11.480 --> 02:19:13.160]   Uh
[02:19:13.160 --> 02:19:15.160]   you what's funny is
[02:19:15.160 --> 02:19:21.720]   The first three episodes are nothing like the rest of the shit like there's a big thing happens the people who are in the first episode
[02:19:22.600 --> 02:19:25.720]   But I don't want to spoil it. Yeah, but it changes dramatically
[02:19:25.720 --> 02:19:30.120]   And then oh that's not they're not the stars. Oh, okay. Yeah
[02:19:30.120 --> 02:19:37.240]   Um, it's a it is good and the uh books that it's based on the wool series. Uh, will be the topic
[02:19:37.240 --> 02:19:39.160]   tomorrow
[02:19:39.160 --> 02:19:45.720]   As ant interviews the author Hugh howie that's exciting. It's so crazy one p.m. Pacific four p.m. Eastern
[02:19:45.720 --> 02:19:50.600]   That is a great get and I am thrilled to watch it. Uh, you should call it triangulation
[02:19:50.600 --> 02:19:55.080]   Should you should because that's really what it is. You're interviewing a
[02:19:55.080 --> 02:19:58.680]   I file anyway, yeah, I guess so tune in for that
[02:19:58.680 --> 02:20:05.400]   That's one of many things we do in the club. These are club exclusives because we want to make you feel bad that you're not in the club
[02:20:05.560 --> 02:20:12.520]   Really? Yes, I do want you to know that we want you to think gosh. I'm missing all the we still offer all these shows for free ad
[02:20:12.520 --> 02:20:15.240]   Supported but if you want ad-free versions of all the
[02:20:15.240 --> 02:20:20.840]   Existing shows if you want special content created just for the club and honestly the real reason they're in the club
[02:20:20.840 --> 02:20:22.760]   It's because club members are paying for it. Yep
[02:20:22.760 --> 02:20:30.120]   They're supporting it stuff that we can't sell ads to on because it's one off like that hu howie interview or it doesn't have a sufficient audience
[02:20:30.120 --> 02:20:32.920]   Uh home theater geeks. We had to cancel it and scott wilkinson
[02:20:33.160 --> 02:20:35.160]   But we could bring it back thanks to you
[02:20:35.160 --> 02:20:40.840]   Club members. Yep. So if and I think it's a very very fair seven dollars a month
[02:20:40.840 --> 02:20:43.640]   Gets you ad-free versions of all the shows gets you a bunch of shows
[02:20:43.640 --> 02:20:49.560]   We don't put out in public like hands-on mac-a-thush hands-on windows scott's home theater geeks the untitled linux show
[02:20:49.560 --> 02:20:52.120]   The gizfizz the windows
[02:20:52.120 --> 02:20:53.960]   Club. I mean it goes on on
[02:20:53.960 --> 02:20:57.800]   Uh, we're making the club a great place to believe you also get access to the discord
[02:20:57.800 --> 02:21:01.800]   Which is the best social network ever because it's filled with club members only
[02:21:02.520 --> 02:21:11.080]   And many of our hosts in there lots and lots of gifts. Yes, there's no gifts and but there are gifts great art animated
[02:21:11.080 --> 02:21:15.320]   Yes, joe has done some great stuff this episode
[02:21:15.320 --> 02:21:19.080]   Uh, anyway seven bucks a month put that tv slash club to it
[02:21:19.080 --> 02:21:25.880]   What's your thing doll is uh is now memorialized. Oh no did he really do that? Yeah, he's quick get it
[02:21:25.880 --> 02:21:28.440]   Such trouble
[02:21:28.440 --> 02:21:31.480]   Such trouble
[02:21:31.480 --> 02:21:33.480]   Okay, i'm going back here there it is
[02:21:33.480 --> 02:21:40.440]   So it's a poster says what's your thing doll
[02:21:40.440 --> 02:21:43.240]   And I have a black eye and some missing teeth
[02:21:43.240 --> 02:21:47.480]   Which is pretty much what stacy's dream come true
[02:21:47.480 --> 02:21:54.120]   I mean was I more like incredibly offended no you're used to it now you know
[02:21:54.120 --> 02:21:57.320]   Right on stacy. Yeah, no you were mortally offended
[02:21:58.200 --> 02:22:01.160]   Not mortally, but I was like mother plucker
[02:22:01.160 --> 02:22:05.880]   That dick
[02:22:05.880 --> 02:22:17.720]   Well, you've been you've been dissing the guys for for the whole show. I thought I could send one your way, but maybe I maybe I made a mistake
[02:22:17.720 --> 02:22:24.600]   I was doing it more in a 20s kind of gangster kind of a thing
[02:22:26.680 --> 02:22:28.680]   Let me just take that shovel from you now
[02:22:28.680 --> 02:22:33.240]   It's like let me just tell you something
[02:22:33.240 --> 02:22:37.000]   Join the club for more of this hilarity
[02:22:37.000 --> 02:22:38.520]   Uh
[02:22:38.520 --> 02:22:40.520]   Twitter.tv/club twit
[02:22:40.520 --> 02:22:46.360]   Thank you. We thank all of our members about 1% of our audiences joined we would like to get it to 5
[02:22:46.360 --> 02:22:49.400]   Yeah, yeah, 10 would if we had 10
[02:22:49.400 --> 02:22:54.680]   We would you know we would do more shows we would probably have no ads
[02:22:55.080 --> 02:22:56.840]   Do we have very different thing true?
[02:22:56.840 --> 02:23:00.440]   It's that's the kind of thing we need to keep going because it is getting really tough out here
[02:23:00.440 --> 02:23:05.640]   Especially because of me good at Disneyland as a as a company all together. Yeah, we'll take you all
[02:23:05.640 --> 02:23:13.400]   I worked for iHeart they had a Disneyland takeover. They did the whole oh yeah, and I didn't go what a moron
[02:23:13.400 --> 02:23:17.480]   I was thinking gosh. I could have I had a wait an hour for every single ride
[02:23:17.480 --> 02:23:22.920]   No, every single ride at least an hour. Anyway, thank you everybody. We really appreciate your support
[02:23:22.920 --> 02:23:28.840]   This show goes on every Tuesday 2 p.m. Civic 5. Sorry Wednesday 2 p.m. Pacific 5 p.m. Eastern
[02:23:28.840 --> 02:23:32.760]   2100 UTC watch it live at live dot twit dot tv
[02:23:32.760 --> 02:23:39.240]   There's audio or video so you could listen live 12 as well if you're watching or listening live chat live at irc
[02:23:39.240 --> 02:23:42.200]   Dot twit dot tv open to all you can use your browser to go there
[02:23:42.200 --> 02:23:49.400]   Uh after the fact on demand add supportive versions of the show at the website twit dot tv slash twig that website also has links
[02:23:49.880 --> 02:23:53.480]   to various podcast players so you can subscribe and
[02:23:53.480 --> 02:23:57.480]   You can also I think go to youtube. I think there's a full yeah
[02:23:57.480 --> 02:24:02.440]   There's a youtube channel dedicated to this week in google however you do it. Please come back each and every week
[02:24:02.440 --> 02:24:07.240]   And watch as I get slowly beaten to a pulp my stacey and aunt
[02:24:07.240 --> 02:24:13.160]   I apologize for anything. I might have said I blame the micro dosing. Thank you everybody and i'll see
[02:24:13.160 --> 02:24:17.000]   We'll see y'all next week on this week in google. Bye
[02:24:17.800 --> 02:24:21.640]   Yeah, and that was exactly 90 minutes. Yeah, yeah
[02:24:21.640 --> 02:24:24.920]   We take out
[02:24:24.920 --> 02:24:29.080]   It's midweek and you really want to know even more about the world of technology
[02:24:29.080 --> 02:24:34.840]   So you should check out tech news weekly the show where we talk to and about the people making and breaking the tech news
[02:24:34.840 --> 02:24:39.320]   It's the biggest news we talk with the uh people writing the stories that you're probably reading
[02:24:39.320 --> 02:24:44.600]   We also talk between ourselves about the stories that are getting us even more excited about tech news this week
[02:24:44.760 --> 02:24:50.360]   So if you're excited well, then join us head to twit.tv/tnw to subscribe
[02:24:50.360 --> 02:25:01.080]   [Music]

