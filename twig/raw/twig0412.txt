;FFMETADATA1
title=The Yodeling Pickle
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=412
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.400]   It's time for Twig this week in Google's Stacey Higginbotham's here, Jeff Jarvis is here.
[00:00:04.400 --> 00:00:13.400]   We're going to get into some raging debates coming up on privacy, on sexism, on yodeling pickles. It's all next!
[00:00:13.400 --> 00:00:14.600]   Twig.
[00:00:14.600 --> 00:00:19.400]   Netcast you love.
[00:00:19.400 --> 00:00:21.400]   From people you trust.
[00:00:21.400 --> 00:00:26.800]   This is Twig.
[00:00:27.500 --> 00:00:34.500]   Bandwidth for this week in Google is provided by CashFly, C-A-C-H-E-F-L-Y.com.
[00:00:34.500 --> 00:00:46.500]   This is Twig. This week in Google, episode 412, recorded Wednesday, July 5th, 2017. The yodeling pickle.
[00:00:46.500 --> 00:00:51.500]   It's time for Twig this week in Google's show where we talk about the latest news.
[00:00:52.000 --> 00:00:57.000]   From the Google verse, from the Facebook verse, the Twitter verse, from new media, old media.
[00:00:57.000 --> 00:01:04.500]   Really, it's a wide ranging conversation on a variety of topics with some of the smartest people I know starting with Stacey Higginbotham.
[00:01:04.500 --> 00:01:10.500]   You remember her from Gig Aome, she now is doing her own thing at Stacey on IOT.com.
[00:01:10.500 --> 00:01:16.000]   And subscribe to the newsletter there and she has a podcast with Kevin Tofel, IOT Podcast.com.
[00:01:16.000 --> 00:01:19.500]   Gig A Stacey, ladies and gentlemen. Hello Stacey!
[00:01:20.000 --> 00:01:23.000]   Hello, I cleaned up my study, you see? No boxes?
[00:01:23.000 --> 00:01:26.500]   Are you going to ever hang anything on the wall there?
[00:01:26.500 --> 00:01:28.500]   See, we're not going to stop shaming you.
[00:01:28.500 --> 00:01:30.500]   It looks like you just moved in.
[00:01:30.500 --> 00:01:32.500]   Look, see, there's stuff.
[00:01:32.500 --> 00:01:34.500]   Oh, I can't. This is the wrong camera.
[00:01:34.500 --> 00:01:37.000]   See, there's more stuff in the other place.
[00:01:37.000 --> 00:01:40.000]   But you're back. Oh, there's her whiteboard, which is very clean.
[00:01:40.000 --> 00:01:43.500]   A clean whiteboard is a clean mind.
[00:01:43.500 --> 00:01:46.000]   Did we shame you into taking the boxes away?
[00:01:47.000 --> 00:01:50.500]   No, no, I cleaned up. We had a party yesterday.
[00:01:50.500 --> 00:01:57.000]   Oh, fun. Fourth of July event. Did you do a barbecue? You are in Austin, Texas.
[00:01:57.000 --> 00:02:02.000]   No, no barbecue. Although I did make excellent ribs. I can share the rest of the rest of the day.
[00:02:02.000 --> 00:02:04.000]   I would love it.
[00:02:04.000 --> 00:02:07.000]   Stacey is an amazing resource in so many ways.
[00:02:07.000 --> 00:02:11.000]   Stacey on IOT and the Internet of Things podcast.
[00:02:12.000 --> 00:02:17.000]   So here Jeff Jarvis, professor of journalism at CUNY, the City University of New York.
[00:02:17.000 --> 00:02:21.000]   He's also a book author, written many books, including What Would Google Do?
[00:02:21.000 --> 00:02:24.000]   Geek Sparing Gifts about Reinventing the News Industry.
[00:02:24.000 --> 00:02:26.000]   He blogs at Buzzmachine.com.
[00:02:26.000 --> 00:02:29.000]   And he's @JeffJARVIS on Twitter. Hello.
[00:02:29.000 --> 00:02:34.000]   Hello. I have to share a little personal story with you.
[00:02:34.000 --> 00:02:41.000]   My son, who as you know, went to the new journalism program at Colorado, CU Boulder.
[00:02:42.000 --> 00:02:46.000]   Graduated in June, May, I guess I was out there.
[00:02:46.000 --> 00:02:48.000]   And he was desperately looking for an internship.
[00:02:48.000 --> 00:02:53.000]   And I did everything I could to connect him, emailed Lance, you went off at Mash and Off.
[00:02:53.000 --> 00:02:57.000]   Mashwall, all sorts of people, was not able to get him an internship.
[00:02:57.000 --> 00:03:03.000]   However, he did get an internship at the Center for Investigative Journalism.
[00:03:03.000 --> 00:03:06.000]   Oh, which is a pretty good internship.
[00:03:06.000 --> 00:03:08.000]   CIR is great.
[00:03:08.000 --> 00:03:10.000]   CIR is spectacular.
[00:03:10.000 --> 00:03:16.000]   They're 40 years old, the longest running independent investigative reporting outfit in the world.
[00:03:16.000 --> 00:03:19.000]   And one of the most advanced in...
[00:03:19.000 --> 00:03:22.000]   Well, that's why he's a visionary attitude.
[00:03:22.000 --> 00:03:24.000]   It's just they're just great.
[00:03:24.000 --> 00:03:26.000]   Because they've created a new site called RevealNews.org.
[00:03:26.000 --> 00:03:28.000]   That's new media.
[00:03:28.000 --> 00:03:30.000]   And he's got a work in there.
[00:03:30.000 --> 00:03:34.000]   But then it turned out, so he's become buddies with Phil Bronstein.
[00:03:34.000 --> 00:03:36.000]   I know you know Phil.
[00:03:36.000 --> 00:03:38.000]   Was he your boss when you were at the club?
[00:03:39.000 --> 00:03:40.000]   He's too young, Phil.
[00:03:40.000 --> 00:03:42.000]   He's too young.
[00:03:42.000 --> 00:03:53.000]   So Bronstein, who's a famous journalist, and apparently I didn't know this had filed dispatches from El Salvador during the revolution there.
[00:03:53.000 --> 00:04:00.000]   And so Henry's job this summer is to go through everything he's ever written.
[00:04:00.000 --> 00:04:02.000]   Phil Bronstein ever wrote at the examiner.
[00:04:02.000 --> 00:04:04.000]   He became editor and chief later.
[00:04:05.000 --> 00:04:09.000]   And prepare to do the research for a documentary about Phil Bronstein.
[00:04:09.000 --> 00:04:10.000]   Wow!
[00:04:10.000 --> 00:04:11.000]   Yeah!
[00:04:11.000 --> 00:04:13.000]   Does that include Sharon Stone stuff?
[00:04:13.000 --> 00:04:15.000]   I don't, but probably.
[00:04:15.000 --> 00:04:16.000]   I mean, that's the sad thing.
[00:04:16.000 --> 00:04:19.000]   In fact, Henry was saying, "Gosh darn it.
[00:04:19.000 --> 00:04:21.000]   Everything I could find mentions Sharon Stone.
[00:04:21.000 --> 00:04:22.000]   Can we just get this?"
[00:04:22.000 --> 00:04:24.000]   He married Sharon Stone briefly.
[00:04:24.000 --> 00:04:26.000]   And also famously got bit by a lizard.
[00:04:26.000 --> 00:04:27.000]   Bit by a...
[00:04:27.000 --> 00:04:29.000]   Well, not a lizard by a Komodo dragon, was it?
[00:04:29.000 --> 00:04:31.000]   A Komodo dragon, yeah.
[00:04:31.000 --> 00:04:33.000]   Seriously, but seriously, severely injured.
[00:04:33.000 --> 00:04:34.000]   But on his foot.
[00:04:34.000 --> 00:04:35.000]   Came through.
[00:04:35.000 --> 00:04:36.000]   Yeah, he came through.
[00:04:36.000 --> 00:04:40.000]   It's one of those horrible things that ended up being kind of comedic.
[00:04:40.000 --> 00:04:41.000]   Yeah.
[00:04:41.000 --> 00:04:44.000]   Wow, congratulations to Henry.
[00:04:44.000 --> 00:04:45.000]   I know, isn't that a...
[00:04:45.000 --> 00:04:46.000]   That's phenomenal.
[00:04:46.000 --> 00:04:47.000]   You know, I had nothing to do with it.
[00:04:47.000 --> 00:04:50.000]   He just, I don't know how, you know...
[00:04:50.000 --> 00:04:51.000]   All the better.
[00:04:51.000 --> 00:04:51.000]   Yeah.
[00:04:51.000 --> 00:04:54.000]   Actually, it goes back to his birth.
[00:04:54.000 --> 00:05:03.000]   Because, I think when he was a baby, or maybe when my other child, Abby, was a baby, I think was when Abby was a baby.
[00:05:04.000 --> 00:05:13.000]   My wife, after the baby was born, would go back to the Cal Pacific Medical Center for baby mommy and me classes where you learn how to nurse and all that stuff.
[00:05:13.000 --> 00:05:17.000]   And she met a very lovely lady who also had a baby at the same time.
[00:05:17.000 --> 00:05:23.000]   And she came back and said, "I just made great friends with this wonderful woman and her child.
[00:05:23.000 --> 00:05:26.000]   Her name is Maggie Hurst.
[00:05:26.000 --> 00:05:27.000]   Should I know that name?"
[00:05:27.000 --> 00:05:28.000]   And I said, "Yeah, you should."
[00:05:28.000 --> 00:05:29.000]   That's where the will to the chicle.
[00:05:29.000 --> 00:05:31.000]   So...
[00:05:32.000 --> 00:05:33.000]   Yeah.
[00:05:33.000 --> 00:05:34.000]   So it really goes back to that.
[00:05:34.000 --> 00:05:41.000]   Oh, I was just about to ask you how Maggie's married to Will Hurst, who ended up, the Hurst became great friends.
[00:05:41.000 --> 00:05:47.000]   And so I guess Henry got called Will and Will got him in the job.
[00:05:47.000 --> 00:05:49.000]   'Cause Will is a big benefactor to the CIR.
[00:05:49.000 --> 00:05:50.000]   You ready?
[00:05:50.000 --> 00:05:51.000]   Yeah.
[00:05:51.000 --> 00:05:52.000]   Yeah.
[00:05:52.000 --> 00:05:53.000]   Which is great.
[00:05:53.000 --> 00:05:54.000]   That's just great.
[00:05:54.000 --> 00:05:55.000]   Congratulations.
[00:05:55.000 --> 00:05:56.000]   But then tell him what he's done with the internship.
[00:05:56.000 --> 00:05:57.000]   You know what he's doing?
[00:05:57.000 --> 00:05:58.000]   What?
[00:05:58.000 --> 00:05:59.000]   Oh, he's getting a graduate degree.
[00:05:59.000 --> 00:06:00.000]   I agree.
[00:06:00.000 --> 00:06:01.000]   Yes, he is.
[00:06:01.000 --> 00:06:02.000]   Shouldn't he do...
[00:06:02.000 --> 00:06:03.000]   Shouldn't he get a journalism job first?
[00:06:03.000 --> 00:06:04.000]   He does have one.
[00:06:04.000 --> 00:06:05.000]   He does have one.
[00:06:05.000 --> 00:06:06.000]   Sorry.
[00:06:06.000 --> 00:06:07.000]   No.
[00:06:07.000 --> 00:06:10.000]   Actually, what I told him is this is an internship, but this is going to turn into a job.
[00:06:10.000 --> 00:06:11.000]   Okay.
[00:06:11.000 --> 00:06:12.000]   And probably for life, right?
[00:06:12.000 --> 00:06:13.000]   Stay at the CIR.
[00:06:13.000 --> 00:06:14.000]   Why not?
[00:06:14.000 --> 00:06:16.000]   That's a great place.
[00:06:16.000 --> 00:06:17.000]   They've done...
[00:06:17.000 --> 00:06:18.000]   They've done...
[00:06:18.000 --> 00:06:19.000]   They do some amazing things.
[00:06:19.000 --> 00:06:21.000]   They do news as comics.
[00:06:21.000 --> 00:06:26.000]   They do plays and dramas around to communicate news.
[00:06:26.000 --> 00:06:28.000]   You might have heard of an NPR.
[00:06:28.000 --> 00:06:30.000]   They do a lot of reports on NPR.
[00:06:30.000 --> 00:06:36.000]   But yeah, if you go to revealnews.org, this is their new kind of new media site.
[00:06:36.000 --> 00:06:37.000]   And it's really good.
[00:06:37.000 --> 00:06:38.000]   It's really good.
[00:06:38.000 --> 00:06:39.000]   So today...
[00:06:39.000 --> 00:06:40.000]   Go ahead.
[00:06:40.000 --> 00:06:41.000]   Oh, did they pay?
[00:06:41.000 --> 00:06:46.000]   Well, he's not getting paid right now, but they do have money because they are...
[00:06:46.000 --> 00:06:48.000]   I mean, it's not profit, but they are...
[00:06:48.000 --> 00:06:49.000]   So it's not a...
[00:06:49.000 --> 00:06:50.000]   Well, in doubt.
[00:06:50.000 --> 00:06:51.000]   Well, in doubt.
[00:06:51.000 --> 00:06:54.000]   There's a button that says donate on the page.
[00:06:54.000 --> 00:06:58.000]   I'm just curious, because like the more prestigious the journalism...
[00:06:58.000 --> 00:06:59.000]   Yes.
[00:06:59.000 --> 00:07:00.000]   So that's a paid...
[00:07:00.000 --> 00:07:01.000]   Yes.
[00:07:01.000 --> 00:07:05.000]   So I did my journalism internship at Texas Highway Patrol Magazine.
[00:07:05.000 --> 00:07:06.000]   See?
[00:07:06.000 --> 00:07:07.000]   You see?
[00:07:07.000 --> 00:07:08.000]   You see?
[00:07:08.000 --> 00:07:11.000]   Where I laid out all the stories edited and commissioned really.
[00:07:11.000 --> 00:07:12.000]   No, but that's how you learn.
[00:07:12.000 --> 00:07:13.000]   That's how you learn.
[00:07:13.000 --> 00:07:14.000]   But...
[00:07:14.000 --> 00:07:15.000]   That's how you learn.
[00:07:15.000 --> 00:07:16.000]   I did it because they paid.
[00:07:16.000 --> 00:07:19.000]   Oh, oh, oh, well, even better.
[00:07:19.000 --> 00:07:20.000]   So at the CUNY...
[00:07:20.000 --> 00:07:25.000]   At the CUNY Graduate School of Journalism, we require an internship after two semesters,
[00:07:25.000 --> 00:07:27.000]   going into the third and final.
[00:07:27.000 --> 00:07:30.000]   And if the employer does not pay, we pay.
[00:07:30.000 --> 00:07:34.000]   We raise money to do that so that every student can't afford to do the required internship.
[00:07:34.000 --> 00:07:35.000]   Yeah, and I told...
[00:07:35.000 --> 00:07:39.000]   It was one of my deals with Henry because I think internships are so important.
[00:07:39.000 --> 00:07:40.000]   Most of the more...
[00:07:40.000 --> 00:07:41.000]   Yeah.
[00:07:41.000 --> 00:07:42.000]   If not unpaid, mildly paid.
[00:07:42.000 --> 00:07:43.000]   Yes.
[00:07:43.000 --> 00:07:47.000]   I think we pay $5 a day or something.
[00:07:47.000 --> 00:07:49.000]   Those jobs are really how you get a real job.
[00:07:49.000 --> 00:07:50.000]   Those are the people you...
[00:07:50.000 --> 00:07:51.000]   Right.
[00:07:51.000 --> 00:07:52.000]   The skills you learn, but also the people you meet.
[00:07:52.000 --> 00:07:56.000]   And very often, working free for someone is a great way to get a paid job with somebody.
[00:07:56.000 --> 00:07:57.000]   Yes.
[00:07:57.000 --> 00:08:03.000]   So I told Henry, if you do go out of school and get an internship, I will continue to support
[00:08:03.000 --> 00:08:04.000]   you for a little while anyway.
[00:08:04.000 --> 00:08:05.000]   Is he living...
[00:08:05.000 --> 00:08:06.000]   He's there in Emeryville.
[00:08:06.000 --> 00:08:08.000]   He's living here in Petaluma, community.
[00:08:08.000 --> 00:08:09.000]   Yeah, so that's not too bad.
[00:08:09.000 --> 00:08:10.000]   Yeah.
[00:08:10.000 --> 00:08:16.000]   He goes to San Francisco Library every day because after the examiner was sold, they didn't
[00:08:16.000 --> 00:08:17.000]   preserve...
[00:08:17.000 --> 00:08:18.000]   They didn't digitize any of it.
[00:08:18.000 --> 00:08:19.000]   It's all in microfiche.
[00:08:19.000 --> 00:08:21.000]   So he's learning a really useful skill as a journalist.
[00:08:21.000 --> 00:08:23.000]   How do you use a microfiche?
[00:08:23.000 --> 00:08:24.000]   Microfiche.
[00:08:24.000 --> 00:08:29.000]   Well, when he runs across, if he runs it across, then he'll take five columns by jim carps.
[00:08:29.000 --> 00:08:30.000]   I know.
[00:08:30.000 --> 00:08:31.000]   I know.
[00:08:31.000 --> 00:08:32.000]   I know.
[00:08:32.000 --> 00:08:33.000]   I thought that was hysterical.
[00:08:33.000 --> 00:08:35.760]   So, and I told him, you got to read up on the history of the San Francisco examiner and
[00:08:35.760 --> 00:08:36.760]   the Chronicle.
[00:08:36.760 --> 00:08:41.000]   I had to tell him, in the old days, every town would have two papers, a morning paper,
[00:08:41.000 --> 00:08:43.200]   an afternoon paper, at least two.
[00:08:43.200 --> 00:08:46.160]   But in then over time, there wasn't enough money for both.
[00:08:46.160 --> 00:08:49.200]   So it ended up usually being one paper.
[00:08:49.200 --> 00:08:52.200]   And then in some towns, no paper.
[00:08:52.200 --> 00:08:55.040]   And what's paper would be the next question.
[00:08:55.040 --> 00:08:56.540]   Yeah.
[00:08:56.540 --> 00:08:58.600]   We live in an interesting age.
[00:08:58.600 --> 00:09:02.760]   So how long did you work for the Texas Highway Patrol?
[00:09:02.760 --> 00:09:03.760]   I think it was two semesters.
[00:09:03.760 --> 00:09:06.320]   See, that's a great internship.
[00:09:06.320 --> 00:09:09.560]   But I also was an RA at the time.
[00:09:09.560 --> 00:09:15.320]   And I was a substitute daycare teacher.
[00:09:15.320 --> 00:09:18.400]   So I could have actual, like, enough money.
[00:09:18.400 --> 00:09:19.400]   Good for you.
[00:09:19.400 --> 00:09:20.400]   Spending money.
[00:09:20.400 --> 00:09:21.400]   You worked your way through school.
[00:09:21.400 --> 00:09:23.400]   I had a yes, sort of.
[00:09:23.400 --> 00:09:24.600]   Good for you.
[00:09:24.600 --> 00:09:28.960]   I worked in the dining halls because that was the highest hourly wage, which I think
[00:09:28.960 --> 00:09:31.560]   of the time was like three, 15 hour.
[00:09:31.560 --> 00:09:32.680]   That's how long ago that was.
[00:09:32.680 --> 00:09:35.520]   But that's what I did all four years of school.
[00:09:35.520 --> 00:09:37.320]   Wow, all four years.
[00:09:37.320 --> 00:09:40.040]   Well, I didn't go for four years, but I did it for four years.
[00:09:40.040 --> 00:09:44.640]   I ended up driving out of school and keeping the dining halls.
[00:09:44.640 --> 00:09:47.200]   That was not a good career move.
[00:09:47.200 --> 00:09:49.600]   But life goals.
[00:09:49.600 --> 00:09:53.320]   But your parents were real proud of you.
[00:09:53.320 --> 00:09:57.880]   But I would drop it as a junior year so I could work at the campus radio station.
[00:09:57.880 --> 00:09:59.320]   And that paid.
[00:09:59.320 --> 00:10:03.560]   So I was able to do that in the dining hall.
[00:10:03.560 --> 00:10:07.920]   But I was there until my class graduated because I don't know why.
[00:10:07.920 --> 00:10:08.920]   I was loyal.
[00:10:08.920 --> 00:10:10.280]   I was loyal to them all.
[00:10:10.280 --> 00:10:15.920]   So we spent a lot of time on Sunday on Twitch on this week in tech.
[00:10:15.920 --> 00:10:20.560]   We had, I kind of rejiggered the panel because I want to make sure we had at least one woman.
[00:10:20.560 --> 00:10:25.360]   We called and got Katie Benner who wrote the New York Times article about harassment in
[00:10:25.360 --> 00:10:26.600]   Silicon Valley.
[00:10:26.600 --> 00:10:27.600]   Really blew the lid off.
[00:10:27.600 --> 00:10:31.840]   It all started and credit to Reed Albergotti over at the information who broke the story
[00:10:31.840 --> 00:10:38.560]   of Justin Callbeck and Callbeck, of course, a venture capitalist who was accused by at
[00:10:38.560 --> 00:10:48.000]   least six women of using his position of power to harass, sexually harass women who were
[00:10:48.000 --> 00:10:50.640]   women entrepreneurs who were trying to get funding.
[00:10:50.640 --> 00:10:53.680]   I mean, that is as bad as bad as you can get.
[00:10:53.680 --> 00:11:00.160]   And then Katie Benner and her article for the New York Times further the story by talking
[00:11:00.160 --> 00:11:05.720]   about Chris Saka, another very well-known venture capitalist, Dave McClure of another
[00:11:05.720 --> 00:11:11.680]   very famous entrepreneur, all three of whom apparently did something similar.
[00:11:11.680 --> 00:11:20.440]   In fact, Dave McClure in Medium shortly after the New York Times article wrote a maya culpa,
[00:11:20.440 --> 00:11:26.720]   a confessional piece in which he said, "I'm a jerk.
[00:11:26.720 --> 00:11:29.080]   You got me."
[00:11:29.080 --> 00:11:32.600]   And it stepped down from his position.
[00:11:32.600 --> 00:11:37.720]   Chris Saka has, I'm a creep, I'm sorry, it was the name of that story.
[00:11:37.720 --> 00:11:44.000]   Chris Saka has continued to kind of deny the allegations as has Mark Cantor.
[00:11:44.000 --> 00:11:49.120]   But nevertheless, these are well-known people in Silicon Valley, not so much Justin, but
[00:11:49.120 --> 00:11:51.160]   these two very well-
[00:11:51.160 --> 00:11:56.560]   Mark Cantor was much worse.
[00:11:56.560 --> 00:11:59.040]   He said that, "Well, I feel like a jerk to get rid of her."
[00:11:59.040 --> 00:12:00.040]   Yeah, he really denied.
[00:12:00.040 --> 00:12:03.480]   That was really awful.
[00:12:03.480 --> 00:12:05.240]   Yeah.
[00:12:05.240 --> 00:12:11.840]   So I mean, it's clear that at least some of these allegations are found in a fact.
[00:12:11.840 --> 00:12:16.960]   I mean, both Callbeck and McClure have left their positions.
[00:12:16.960 --> 00:12:19.640]   And in fact, the Callbeck's fund has been dissolved.
[00:12:19.640 --> 00:12:21.200]   Oh, really?
[00:12:21.200 --> 00:12:22.200]   Yeah.
[00:12:22.200 --> 00:12:23.200]   Yeah.
[00:12:23.200 --> 00:12:28.560]   McClure apparently had decided to quit earlier, but as it turns out, now there's still a
[00:12:28.560 --> 00:12:33.360]   lot of conversation about all this in Silicon Valley and elsewhere.
[00:12:33.360 --> 00:12:37.060]   He apparently, they knew about this and they didn't maybe act on it as quickly as they
[00:12:37.060 --> 00:12:38.060]   should.
[00:12:38.060 --> 00:12:45.080]   Christine Tsai, who is his partner, now runs the fund 500 startups, says I didn't know,
[00:12:45.080 --> 00:12:48.400]   but he said he was going to step down earlier this year anyway.
[00:12:48.400 --> 00:12:50.720]   And anyway, there's a lot more to this story.
[00:12:50.720 --> 00:12:55.880]   And I don't need to go into the deets of these specific story as I'm much more interested.
[00:12:55.880 --> 00:12:59.240]   And I know Stacy, you wanted to talk about this in the culture.
[00:12:59.240 --> 00:13:03.040]   I have to say though, the conversation is really important.
[00:13:03.040 --> 00:13:07.160]   And I have to say that as we've started talking about this, I've heard from everybody, I
[00:13:07.160 --> 00:13:11.880]   know, oh, yeah, I've been harassed and not just in the tech business, but every woman
[00:13:11.880 --> 00:13:15.440]   has been harassed at least once, if not many times.
[00:13:15.440 --> 00:13:18.120]   How much worse is the tech business?
[00:13:18.120 --> 00:13:19.120]   That's a good question.
[00:13:19.120 --> 00:13:20.120]   Yeah.
[00:13:20.120 --> 00:13:21.120]   That's that's.
[00:13:21.120 --> 00:13:22.120]   I don't think it is.
[00:13:22.120 --> 00:13:23.120]   Okay.
[00:13:23.120 --> 00:13:24.120]   Well, whoa, whoa.
[00:13:24.120 --> 00:13:25.120]   Okay.
[00:13:25.120 --> 00:13:29.960]   I've worked in finance, so I worked when I was very young.
[00:13:29.960 --> 00:13:33.560]   My first job was bond traders and working at the bonfire.
[00:13:33.560 --> 00:13:35.240]   That's a bunch of pros.
[00:13:35.240 --> 00:13:36.400]   It's a bunch of pros.
[00:13:36.400 --> 00:13:40.000]   It's a very big swing.
[00:13:40.000 --> 00:13:44.640]   You know what's yeah, I'm like, yeah, there's there's harassment there.
[00:13:44.640 --> 00:13:49.400]   Then I worked actually here in Austin, where I dealt with a lot of the real estate guys,
[00:13:49.400 --> 00:13:53.640]   and they were at the time, I guess you could get porn on your smartphones.
[00:13:53.640 --> 00:13:58.920]   And I vividly remember sitting at a table like at a gala with some real estate guys,
[00:13:58.920 --> 00:14:00.880]   that they were like showing me the porn on their smartphone.
[00:14:00.880 --> 00:14:02.720]   And I'm a 23 year old kid.
[00:14:02.720 --> 00:14:03.720]   Isn't that sweet?
[00:14:03.720 --> 00:14:04.720]   I'm like, ew.
[00:14:04.720 --> 00:14:07.520]   My wife worked in the construction industry similar.
[00:14:07.520 --> 00:14:08.520]   Oh, yeah.
[00:14:08.520 --> 00:14:09.520]   Similar.
[00:14:09.520 --> 00:14:13.680]   Very few women and very macho culture.
[00:14:13.680 --> 00:14:17.520]   And then yeah, tech guys are also bad.
[00:14:17.520 --> 00:14:25.600]   So honestly, I think let us say, oh, hey, I think the issue here is that the tech industry
[00:14:25.600 --> 00:14:27.960]   is not as enlightened as they may be thought they were.
[00:14:27.960 --> 00:14:30.720]   It's not as diverse as it needs to be.
[00:14:30.720 --> 00:14:32.480]   Well, we've known that forever.
[00:14:32.480 --> 00:14:37.120]   I mean, I was kind of getting to that from an economic diversity when I was asking about
[00:14:37.120 --> 00:14:40.120]   internships and journalism being paid, for example.
[00:14:40.120 --> 00:14:42.120]   That's a different kind of diversity.
[00:14:42.120 --> 00:14:44.120]   We're not talking about that here.
[00:14:44.120 --> 00:14:52.520]   I mean, these are conversations everyone should be having across the board, both economic,
[00:14:52.520 --> 00:14:58.280]   racial, gender, all of that needs to be if we want to fix where we are, right?
[00:14:58.280 --> 00:15:01.320]   So those are the big macro kind of questions.
[00:15:01.320 --> 00:15:03.120]   Boys will be boys.
[00:15:03.120 --> 00:15:04.120]   Wrong.
[00:15:04.120 --> 00:15:09.880]   If there aren't enough women around to say knock the heck out and stop it.
[00:15:09.880 --> 00:15:14.600]   So that's the I mean, most women aren't going to do it because boys still have the power.
[00:15:14.600 --> 00:15:16.680]   You don't want to be seen as.
[00:15:16.680 --> 00:15:20.200]   So you're equals in power.
[00:15:20.200 --> 00:15:24.640]   I would submit that's going to be a problem.
[00:15:24.640 --> 00:15:27.800]   You should not say the word boys will be boys.
[00:15:27.800 --> 00:15:31.800]   You as a person Leo should say, hey, there's not enough women in power.
[00:15:31.800 --> 00:15:33.480]   I'm going to say, hey, stop it.
[00:15:33.480 --> 00:15:34.880]   Don't do that.
[00:15:34.880 --> 00:15:41.200]   And I have worked for some real jerks and I've worked for people who are amazing.
[00:15:41.200 --> 00:15:46.960]   And I'm going to I'll call home for being really amazing because he said great culture.
[00:15:46.960 --> 00:15:47.960]   He did.
[00:15:47.960 --> 00:15:52.880]   And you know, I remember someone writing on the bottom of a blog post.
[00:15:52.880 --> 00:15:56.200]   It was a picture from one of our structure events and it was a picture of me sitting
[00:15:56.200 --> 00:15:59.720]   talking to some person at a cloud computing event.
[00:15:59.720 --> 00:16:02.240]   And the person wrote great legs.
[00:16:02.240 --> 00:16:05.720]   I saw this and I was like, oh my God, that's so embarrassing.
[00:16:05.720 --> 00:16:11.760]   And oh, just without without even thinking about it, just deleted it and was like, send
[00:16:11.760 --> 00:16:14.880]   a note to the person was like, screw you.
[00:16:14.880 --> 00:16:18.920]   We don't want your kind here on our site in good for home.
[00:16:18.920 --> 00:16:20.160]   I love that.
[00:16:20.160 --> 00:16:25.680]   I don't say boys will be boys to to to in any way condone it or even say it's okay.
[00:16:25.680 --> 00:16:29.320]   But just that this is what happens when you get a bunch of men together.
[00:16:29.320 --> 00:16:31.600]   You go you go in a locker room or just have to happen.
[00:16:31.600 --> 00:16:32.600]   I get a lot of it.
[00:16:32.600 --> 00:16:33.600]   But it does.
[00:16:33.600 --> 00:16:35.440]   I'm I agree it shouldn't happen, but it does.
[00:16:35.440 --> 00:16:38.520]   And somebody in the chat room saying I worked in construction.
[00:16:38.520 --> 00:16:42.480]   There's not a single construction office in the country doesn't have a adult calendar
[00:16:42.480 --> 00:16:43.800]   on the wall.
[00:16:43.800 --> 00:16:47.800]   Now if there were women executives that would stop immediately.
[00:16:47.800 --> 00:16:50.440]   It might not some women internalize a lot of this.
[00:16:50.440 --> 00:16:54.560]   I guess I'm looking at Google's diversity report, which just came out.
[00:16:54.560 --> 00:17:01.160]   This is overall 69% men, 39 31% women in in general.
[00:17:01.160 --> 00:17:05.960]   If I go to tech, which is what we really care about, it goes down to 20% women.
[00:17:05.960 --> 00:17:14.200]   The thing I see also very shameful, 1% black, 3% Hispanic, 53% white.
[00:17:14.200 --> 00:17:18.640]   The next largest ethnicity is Asian, 39%.
[00:17:18.640 --> 00:17:23.240]   So this is Google who's trying apparently trying very hard to create diversity.
[00:17:23.240 --> 00:17:29.760]   But if you are an environment like this, I think that's very hard on the 20 and the one
[00:17:29.760 --> 00:17:30.760]   in five women.
[00:17:30.760 --> 00:17:34.080]   Literally, although the yes, I think that's true.
[00:17:34.080 --> 00:17:40.960]   The watch frighten us more is that when one for whatever reason one person has the courage
[00:17:40.960 --> 00:17:44.040]   to come out and expose this, the number who then follow.
[00:17:44.040 --> 00:17:45.280]   Yeah, look what's happened.
[00:17:45.280 --> 00:17:48.960]   BC or whether it's Bill Cosby or any of these cases.
[00:17:48.960 --> 00:17:52.000]   So you know, it's not a numbers game.
[00:17:52.000 --> 00:17:53.160]   I think Stacy is quite right.
[00:17:53.160 --> 00:17:54.600]   It's a power game no matter what.
[00:17:54.600 --> 00:18:02.040]   And even if the numbers are 50/50, there's intimidation that goes beyond even that.
[00:18:02.040 --> 00:18:03.040]   Oh, absolutely.
[00:18:03.040 --> 00:18:04.960]   And you know, aggressions of all kinds.
[00:18:04.960 --> 00:18:08.760]   And we should, I agree with you Stacy, we should all be more like home.
[00:18:08.760 --> 00:18:12.040]   And even if there aren't women in the workplace, there should be men in the workplace who say
[00:18:12.040 --> 00:18:13.040]   knock it off.
[00:18:13.040 --> 00:18:14.040]   Exactly.
[00:18:14.040 --> 00:18:19.360]   And I would say, so here's my advice to guys in the workplace.
[00:18:19.360 --> 00:18:26.720]   Just basic rules that I can't believe someone they don't know, but if your dear, their mothers
[00:18:26.720 --> 00:18:30.640]   didn't or their fathers, I mean, yes, sorry.
[00:18:30.640 --> 00:18:34.080]   Sorry, I'm like, we are not putting this all on the women.
[00:18:34.080 --> 00:18:35.080]   Sorry, Jeff.
[00:18:35.080 --> 00:18:36.080]   No, no, no, no, no.
[00:18:36.080 --> 00:18:37.080]   There's a hot fight.
[00:18:37.080 --> 00:18:39.780]   But the real problem is that, man.
[00:18:39.780 --> 00:18:44.360]   Wait, Leo, please give you my rule.
[00:18:44.360 --> 00:18:45.360]   Okay.
[00:18:45.360 --> 00:18:49.100]   Rule is I should never know if you want to sleep with me.
[00:18:49.100 --> 00:18:53.200]   In a work environment, if you're dealing with a woman, that's a good rule.
[00:18:53.200 --> 00:18:54.360]   I like that.
[00:18:54.360 --> 00:18:57.520]   Any woman you're working with should never know if you want to sleep with her or not,
[00:18:57.520 --> 00:19:00.680]   because it's completely irrelevant to your professional relationship.
[00:19:00.680 --> 00:19:01.680]   Yes.
[00:19:01.680 --> 00:19:02.680]   Period.
[00:19:02.680 --> 00:19:04.560]   And if you follow that rule, you're great.
[00:19:04.560 --> 00:19:08.200]   That doesn't mean just not saying I want to sleep with you.
[00:19:08.200 --> 00:19:14.400]   No, it means all of the flirty, weird things that people do.
[00:19:14.400 --> 00:19:15.400]   Yeah.
[00:19:15.400 --> 00:19:19.140]   But I think that's a very good rule.
[00:19:19.140 --> 00:19:24.060]   And the thing I would say is that most men want to sleep with most women.
[00:19:24.060 --> 00:19:29.980]   So the difficulty and the reason men kind of gloss this over is, well, we're all thinking
[00:19:29.980 --> 00:19:32.980]   this anyway.
[00:19:32.980 --> 00:19:36.700]   So it really is, it's got to be at a behavioral level.
[00:19:36.700 --> 00:19:40.380]   You can't, I don't think you can teach men not to think that.
[00:19:40.380 --> 00:19:41.380]   Right.
[00:19:41.380 --> 00:19:42.700]   I don't care what they think.
[00:19:42.700 --> 00:19:44.540]   I just should never know.
[00:19:44.540 --> 00:19:46.540]   I love that rule.
[00:19:46.540 --> 00:19:55.220]   I mean, I've, I work with so many men and I work really closely with lots of men in, I
[00:19:55.220 --> 00:19:57.380]   honestly, I don't give it much thought.
[00:19:57.380 --> 00:20:00.040]   But when I was thinking about this rule, I was like, God, did any of them want to sleep
[00:20:00.040 --> 00:20:01.040]   with me?
[00:20:01.040 --> 00:20:02.040]   And I have no idea.
[00:20:02.040 --> 00:20:03.040]   Good.
[00:20:03.040 --> 00:20:04.040]   Zero.
[00:20:04.040 --> 00:20:05.040]   That's good.
[00:20:05.040 --> 00:20:06.040]   I like that a lot.
[00:20:06.040 --> 00:20:07.040]   So.
[00:20:07.040 --> 00:20:08.040]   And you can tell I don't want to sleep with you, right?
[00:20:08.040 --> 00:20:09.880]   That I don't even think about.
[00:20:09.880 --> 00:20:12.100]   No, it shouldn't come up.
[00:20:12.100 --> 00:20:13.580]   It shouldn't even be.
[00:20:13.580 --> 00:20:15.980]   No, I, that's a, that's such a great rule.
[00:20:15.980 --> 00:20:17.600]   I've never heard it articulated like that.
[00:20:17.600 --> 00:20:20.940]   And I think that that's, if it comes down to it, that's it.
[00:20:20.940 --> 00:20:25.380]   But, and, and, but you really got to say that doesn't just mean you say something.
[00:20:25.380 --> 00:20:27.740]   It means showing you porn at a dinner.
[00:20:27.740 --> 00:20:30.180]   It means having a sexy calendar even in your office.
[00:20:30.180 --> 00:20:31.980]   It's treating with respect.
[00:20:31.980 --> 00:20:32.980]   Yeah.
[00:20:32.980 --> 00:20:34.780]   It, it also stays easy.
[00:20:34.780 --> 00:20:37.180]   Stacey, let me say this if I may.
[00:20:37.180 --> 00:20:38.860]   Because I'm a, I'm a fender on this one.
[00:20:38.860 --> 00:20:39.860]   It means not interrupting.
[00:20:39.860 --> 00:20:44.860]   I just interrupted you.
[00:20:44.860 --> 00:20:46.860]   I mean, we're on a time show.
[00:20:46.860 --> 00:20:49.260]   I totally don't, I don't, I don't think anything of us when we interrupt.
[00:20:49.260 --> 00:20:53.060]   Well, and there's a technical reason for interrupting because we're on Skype and there's some delay
[00:20:53.060 --> 00:20:54.060]   between.
[00:20:54.060 --> 00:20:55.060]   Yeah.
[00:20:55.060 --> 00:20:59.540]   I mean, I'll, I'll take, I'll take the, the, the handicap points, but I agree with you.
[00:20:59.540 --> 00:21:01.020]   I agree with you.
[00:21:01.020 --> 00:21:06.780]   I watched the panel last week, or VidCon and there were two women, two men and one woman.
[00:21:06.780 --> 00:21:11.500]   And she tried like three times to break in and was interrupted and then finally sat back
[00:21:11.500 --> 00:21:17.140]   with that brand that said, yeah, okay, well, I'm normal and, and we're just not aware of
[00:21:17.140 --> 00:21:18.140]   it enough.
[00:21:18.140 --> 00:21:19.140]   I'm not aware of it.
[00:21:19.140 --> 00:21:20.140]   It's mansplaining.
[00:21:20.140 --> 00:21:21.140]   It's interrupting.
[00:21:21.140 --> 00:21:25.260]   It's, there's a lot of that too.
[00:21:25.260 --> 00:21:29.300]   So it does go to be even beyond that, not, not knowing that you.
[00:21:29.300 --> 00:21:33.540]   So Stacey, let me ask you this.
[00:21:33.540 --> 00:21:40.500]   Is African American men were being shot by police in America and we, and we saw the
[00:21:40.500 --> 00:21:43.980]   African Americans say that they had, including the president of the United States saying
[00:21:43.980 --> 00:21:48.620]   that they had to talk with their children as a woman.
[00:21:48.620 --> 00:21:55.660]   Did anyone ever have a talk with you, mother, coworker, mentor about what would happen
[00:21:55.660 --> 00:21:58.660]   to you in the workplace and how to deal with it?
[00:21:58.660 --> 00:21:59.660]   Let's see.
[00:21:59.660 --> 00:22:02.900]   I'm going to say no.
[00:22:02.900 --> 00:22:05.580]   I mean, my mom was a geophysicist.
[00:22:05.580 --> 00:22:10.100]   And so she was in the oil industry, which was hugely male dominated.
[00:22:10.100 --> 00:22:17.220]   And yeah, I don't mean, she met my dad on an oil boat though.
[00:22:17.220 --> 00:22:21.340]   So I'm kind of like, hmm, no, no, whatever had the talk.
[00:22:21.340 --> 00:22:29.500]   I will say I talk with my daughter about, I train her to notice things like we were, we
[00:22:29.500 --> 00:22:35.140]   were in Harry Potter world, and this is kind of silly, but some little boy who was probably
[00:22:35.140 --> 00:22:41.220]   her age got on the, we were in the train and he and his family got in and we were on our
[00:22:41.220 --> 00:22:42.220]   side.
[00:22:42.220 --> 00:22:48.420]   And a little boy just stopped, talked and talked and talked and talked and just, ah.
[00:22:48.420 --> 00:22:52.980]   And I finally, you know, I did not tell the boy to shut up, but I was like, man, I would
[00:22:52.980 --> 00:22:57.140]   tell my daughter to shut up at this point because it was really annoying.
[00:22:57.140 --> 00:23:02.580]   But the boy's family didn't tell him to shut up and Anna, my daughter, you know, she turns
[00:23:02.580 --> 00:23:05.260]   to me and she's like, man, he talked a lot.
[00:23:05.260 --> 00:23:06.580]   I was like, he did.
[00:23:06.580 --> 00:23:10.140]   And I bet, you know, if you had been talking, I would have told you to stop talking, but
[00:23:10.140 --> 00:23:11.300]   no one shut him up.
[00:23:11.300 --> 00:23:14.660]   In fact, they, they kind of were all grinning and like humoring him.
[00:23:14.660 --> 00:23:17.700]   And that's again, that could be a totally just different parenting style.
[00:23:17.700 --> 00:23:18.940]   They can go both ways too.
[00:23:18.940 --> 00:23:22.180]   Because my daughter was, would dominate dinner conversations.
[00:23:22.180 --> 00:23:27.820]   And my wife and I often talked about, um, how do we get Henry into the conversation
[00:23:27.820 --> 00:23:31.180]   that just never gets a word in edgewise?
[00:23:31.180 --> 00:23:33.860]   So it can go, that's a personality style too.
[00:23:33.860 --> 00:23:35.580]   It's not merely a gender style.
[00:23:35.580 --> 00:23:36.580]   That's true.
[00:23:36.580 --> 00:23:39.420]   Let me show you Apple's diversity report, just to be fair.
[00:23:39.420 --> 00:23:42.300]   Looks like Apple's doing a little bit of a better job with Google.
[00:23:42.300 --> 00:23:48.300]   Now they, they, they talk more about what we've done lately than the overall, but you
[00:23:48.300 --> 00:23:54.860]   see new hires, 37% in 2016, but here they break down tech versus store and such.
[00:23:54.860 --> 00:23:56.860]   They don't do, no, and I wish they did.
[00:23:56.860 --> 00:23:59.260]   Yes, store changes things, doesn't it?
[00:23:59.260 --> 00:24:06.260]   And I wish they did say, you know, among our tech employees, our engineers, they don't.
[00:24:06.260 --> 00:24:11.660]   At least not as far as I could see, but here is, um, the, the number you want to look at,
[00:24:11.660 --> 00:24:15.060]   to compare it to Google's is, is the bottom number here.
[00:24:15.060 --> 00:24:22.020]   So 19% Asian, this is totals, 9% black, 12% Hispanic.
[00:24:22.020 --> 00:24:27.700]   They don't, this is not, this is ethnicity is not about gender.
[00:24:27.700 --> 00:24:30.660]   And then I think the big story too is pay equity, right?
[00:24:30.660 --> 00:24:34.100]   We know that that doesn't happen still.
[00:24:34.100 --> 00:24:36.340]   Then Google's been dinged for not paying its females.
[00:24:36.340 --> 00:24:37.340]   Oh, yes.
[00:24:37.340 --> 00:24:38.340]   I was going to see it.
[00:24:38.340 --> 00:24:42.380]   Did they ever get that information to, uh, yeah, I don't know what the new department
[00:24:42.380 --> 00:24:43.380]   of labor.
[00:24:43.380 --> 00:24:45.980]   The latest is on that.
[00:24:45.980 --> 00:24:50.980]   Does the current department of labor give it to the care?
[00:24:50.980 --> 00:24:51.980]   That's a really interesting question.
[00:24:51.980 --> 00:24:55.940]   You know, that was a story we talked about a couple of months ago, but nothing Google
[00:24:55.940 --> 00:25:00.740]   refuses to hand over salary data after labor department accuses it of underpaying women.
[00:25:00.740 --> 00:25:04.100]   That's back in that's in June.
[00:25:04.100 --> 00:25:06.340]   So I guess they're continuing to do that.
[00:25:06.340 --> 00:25:09.540]   That's, that's, this is current.
[00:25:09.540 --> 00:25:13.020]   So Google says, of course we pay women fine, but we're not.
[00:25:13.020 --> 00:25:14.540]   And the numbers would be misleading.
[00:25:14.540 --> 00:25:15.540]   I don't know.
[00:25:15.540 --> 00:25:17.860]   Well, and that's two different issues.
[00:25:17.860 --> 00:25:20.500]   So harassment one issue, right?
[00:25:20.500 --> 00:25:27.420]   And then they're related, but I would say a bigger problem in Silicon Valley.
[00:25:27.420 --> 00:25:34.940]   So if you're dividing it into harassment and pay equality or discrimination based on gender.
[00:25:34.940 --> 00:25:39.260]   So separate from sexual harassment.
[00:25:39.260 --> 00:25:45.700]   There's more on the discrimination by gender that I think it's a different topic.
[00:25:45.700 --> 00:25:52.420]   And it's also part of like Silicon Valley's mythos of the incredibly hardworking entrepreneur
[00:25:52.420 --> 00:25:54.540]   with no family, etc, etc.
[00:25:54.540 --> 00:25:58.620]   So I think that dings women to.
[00:25:58.620 --> 00:26:00.260]   Well, it dings everybody.
[00:26:00.260 --> 00:26:02.900]   I mean, that's a terrible culture, right?
[00:26:02.900 --> 00:26:03.900]   That's it.
[00:26:03.900 --> 00:26:04.900]   It is.
[00:26:04.900 --> 00:26:08.540]   But, but women bear the brunt of that culture.
[00:26:08.540 --> 00:26:10.060]   And they bear it not just in the tech.
[00:26:10.060 --> 00:26:14.140]   They bear it in other industries like law and finance.
[00:26:14.140 --> 00:26:17.860]   I go home to this issue all the time because my wife runs our company.
[00:26:17.860 --> 00:26:18.860]   She's a CEO.
[00:26:18.860 --> 00:26:22.500]   As I said, she used to work in construction.
[00:26:22.500 --> 00:26:24.060]   And she's tough.
[00:26:24.060 --> 00:26:28.780]   She's really tough and she had to be tough, I think, to work as one of the sole females
[00:26:28.780 --> 00:26:31.580]   in the construction industry.
[00:26:31.580 --> 00:26:33.380]   And she's tough today.
[00:26:33.380 --> 00:26:38.660]   And so I'm and she's been harassed endlessly.
[00:26:38.660 --> 00:26:44.060]   So she's tough enough to stand up to it in most cases, I think, in every case.
[00:26:44.060 --> 00:26:48.260]   But I don't think not everybody is and not everybody should have to be tough.
[00:26:48.260 --> 00:26:51.380]   You shouldn't have to be tough and free.
[00:26:51.380 --> 00:26:56.500]   You don't know what your wife has internalized or where she could have focused her energies
[00:26:56.500 --> 00:26:59.300]   and what she could have done if she didn't have to deal with all that BS.
[00:26:59.300 --> 00:27:00.940]   Well, it's true.
[00:27:00.940 --> 00:27:02.940]   That's a good point.
[00:27:02.940 --> 00:27:03.940]   Right.
[00:27:03.940 --> 00:27:09.420]   Maybe she would have married someone, someone else and had an even better life.
[00:27:09.420 --> 00:27:11.580]   Oh, thanks a lot.
[00:27:11.580 --> 00:27:15.660]   She's she's what he do today.
[00:27:15.660 --> 00:27:18.660]   I'll just have you know, I keep my harassment to the bedroom.
[00:27:18.660 --> 00:27:21.660]   That's where your harassment belongs.
[00:27:21.660 --> 00:27:25.900]   Hey, honey, you're looking good today.
[00:27:25.900 --> 00:27:29.620]   Anyway, I'm glad I'm glad you Stacy said it before the show started.
[00:27:29.620 --> 00:27:34.140]   I want to talk about this and I we did make a point of talking about it on Sunday.
[00:27:34.140 --> 00:27:35.740]   It is a huge story.
[00:27:35.740 --> 00:27:42.380]   And I think the Silicon Valley story is a big deal because we think of ourselves tech
[00:27:42.380 --> 00:27:46.220]   people think of ourselves as enlightened, merit based.
[00:27:46.220 --> 00:27:52.260]   We think it's a meritocracy, but the numbers belie that and now the facts belie it.
[00:27:52.260 --> 00:27:59.660]   And I mean, it's again, credit to Susan Fowler, her blog that blew the lid off Uber.
[00:27:59.660 --> 00:28:04.220]   I think some I have to say, Jeff, now I'm going to get political.
[00:28:04.220 --> 00:28:06.420]   Some of this comes from the top, right?
[00:28:06.420 --> 00:28:10.220]   And the and the climate in this country and the tribalism in this country.
[00:28:10.220 --> 00:28:15.700]   And there is definitely it's almost feels like a retrenchment and move backwards in
[00:28:15.700 --> 00:28:22.500]   terms of what we used to call women's live in terms of empowering women.
[00:28:22.500 --> 00:28:28.620]   And I feel that's really unfortunate, but there's definitely same things happen with
[00:28:28.620 --> 00:28:29.820]   racism in this country.
[00:28:29.820 --> 00:28:37.300]   There are a group of people in this country who have suddenly become empowered.
[00:28:37.300 --> 00:28:42.580]   And because of the election of President Trump and kind of the general tenor of his
[00:28:42.580 --> 00:28:43.580]   tweets.
[00:28:43.580 --> 00:28:46.380]   By the way, you've come around to my point of view, haven't you?
[00:28:46.380 --> 00:28:48.180]   We let Trump tweet.
[00:28:48.180 --> 00:28:50.380]   Oh, oh, absolutely.
[00:28:50.380 --> 00:28:51.380]   I agree.
[00:28:51.380 --> 00:28:54.300]   I get so angry when journalists say he shouldn't tweet.
[00:28:54.300 --> 00:28:55.380]   It would be wrong for a guy.
[00:28:55.380 --> 00:28:56.380]   Why do you need something?
[00:28:56.380 --> 00:28:57.380]   No, let him implicate himself.
[00:28:57.380 --> 00:29:02.780]   Well, but it is bad for the country that he's tweeting, I suppose is bad for a reputation,
[00:29:02.780 --> 00:29:04.780]   bad for a real true view of where we're going.
[00:29:04.780 --> 00:29:08.380]   But at the same time, it's an insight into the mind of the man of the leader of the free
[00:29:08.380 --> 00:29:09.380]   world.
[00:29:09.380 --> 00:29:10.380]   I think that's a valuable tool.
[00:29:10.380 --> 00:29:13.180]   Back to your prayer point.
[00:29:13.180 --> 00:29:17.980]   I think you need to have data to say whether the delta, where's the delta going?
[00:29:17.980 --> 00:29:21.780]   Because certainly before the current administration, we had plenty of problems with racism in
[00:29:21.780 --> 00:29:22.780]   the region.
[00:29:22.780 --> 00:29:25.340]   It is purely anecdotal, I agree.
[00:29:25.340 --> 00:29:31.900]   But what you also see is that when a group is threatened, they respond as threatened
[00:29:31.900 --> 00:29:32.900]   animals.
[00:29:32.900 --> 00:29:33.900]   Right?
[00:29:33.900 --> 00:29:37.140]   And so that's what we're seeing when it comes, that's what the basis of the nationalism
[00:29:37.140 --> 00:29:38.580]   we're seeing is.
[00:29:38.580 --> 00:29:46.060]   But the white majority of America, the male majority of America is not the majority or
[00:29:46.060 --> 00:29:47.820]   not the powerful as much.
[00:29:47.820 --> 00:29:49.060]   And so then they react.
[00:29:49.060 --> 00:29:54.700]   So then you see things like the so called horribly named men's rights movement.
[00:29:54.700 --> 00:29:58.780]   You see gamer gate, you see all that kind of stuff emerging.
[00:29:58.780 --> 00:30:03.140]   And it's a reaction against a trend that is going against them.
[00:30:03.140 --> 00:30:06.060]   And it's kind of a last gasp of fight.
[00:30:06.060 --> 00:30:08.740]   And that's what I believe is going on right now.
[00:30:08.740 --> 00:30:11.900]   That's what I hope is going on.
[00:30:11.900 --> 00:30:15.340]   But it's boy, it's a painful, painful stage, isn't it?
[00:30:15.340 --> 00:30:18.980]   You know, I also think that the nation is challenged as an institution and thus nationalism
[00:30:18.980 --> 00:30:21.900]   is rising.
[00:30:21.900 --> 00:30:30.900]   I would also to be to point out that Lisa says this, she says, if I had been the woman
[00:30:30.900 --> 00:30:37.060]   I am in many other countries in the world, I would be dead by now.
[00:30:37.060 --> 00:30:41.540]   And we should point out that as, you know, we talk about how bad it is for women here,
[00:30:41.540 --> 00:30:45.500]   but it is far worse in many countries of the world.
[00:30:45.500 --> 00:30:49.900]   Well, so I was talking to the colleague today about, I was sorry, Stacy, go ahead.
[00:30:49.900 --> 00:30:54.100]   I was just going to say that kind of relativism does it.
[00:30:54.100 --> 00:30:55.100]   That is nothing.
[00:30:55.100 --> 00:30:56.100]   It is.
[00:30:56.100 --> 00:30:57.100]   I agree.
[00:30:57.100 --> 00:30:58.100]   It's so bad.
[00:30:58.100 --> 00:31:00.500]   I would say that we've made some progress.
[00:31:00.500 --> 00:31:02.900]   Well, let me try this one.
[00:31:02.900 --> 00:31:07.020]   So I was talking with my colleague who's going to run the news integrity initiative today
[00:31:07.020 --> 00:31:13.500]   about diversity is one of the pillars that we want to face because we think that news
[00:31:13.500 --> 00:31:18.060]   organizations that don't have diversity, don't understand the communities that they
[00:31:18.060 --> 00:31:22.020]   are to serve and reflect and having greater diversity will help them with that.
[00:31:22.020 --> 00:31:23.500]   It's pretty straightforward.
[00:31:23.500 --> 00:31:28.420]   We at CUNY take that as a major part of our mission.
[00:31:28.420 --> 00:31:30.980]   So then we were talking about, I said, well, how do we internationalize that topic as we're
[00:31:30.980 --> 00:31:31.980]   supposed to be international?
[00:31:31.980 --> 00:31:35.220]   And as I was thinking about it, let's be very, very clear.
[00:31:35.220 --> 00:31:38.140]   In the United States, we are bad at this.
[00:31:38.140 --> 00:31:41.500]   We still have a great long way to go with this.
[00:31:41.500 --> 00:31:45.580]   However, there is far more discussion of diversity in the United States because we've been
[00:31:45.580 --> 00:31:49.020]   bad at it, because we have a racial problem, because we had slavery, because of all these
[00:31:49.020 --> 00:31:53.340]   reasons that if you go to Europe, you hear far less discussion of diversity and the impacts
[00:31:53.340 --> 00:31:56.380]   of diversity on companies and on how they operate.
[00:31:56.380 --> 00:32:00.140]   So the weird thing is even though we're way behind and where we should be, we have a
[00:32:00.140 --> 00:32:04.740]   discussion here that I think could be also brought to other countries, could be brought
[00:32:04.740 --> 00:32:12.100]   to Europe and elsewhere in the world, that would be beneficial to never stand above and
[00:32:12.100 --> 00:32:16.140]   say that we're high in mighty and doing so well, we're not.
[00:32:16.140 --> 00:32:19.460]   But the discussion itself needs to be exported.
[00:32:19.460 --> 00:32:21.980]   Does that make any sense?
[00:32:21.980 --> 00:32:22.980]   It does.
[00:32:22.980 --> 00:32:26.620]   Because like, where's my gadgets?
[00:32:26.620 --> 00:32:29.100]   I don't know, not at all.
[00:32:29.100 --> 00:32:31.900]   I'm very, I'm very willing to talk about this.
[00:32:31.900 --> 00:32:34.940]   I think it's very important.
[00:32:34.940 --> 00:32:39.980]   Yes, and I think there's a couple things here.
[00:32:39.980 --> 00:32:46.020]   And it's hard to talk about right now because it feels like we're in kind of the nadir
[00:32:46.020 --> 00:32:53.260]   of our diversity experiments in some ways, but like immigration and opening our borders
[00:32:53.260 --> 00:32:58.580]   is unequivocally helped make America truly great, right?
[00:32:58.580 --> 00:33:04.900]   And many other countries, if you look at like Japan and parts of Europe where immigration
[00:33:04.900 --> 00:33:14.740]   is frowned upon and not, they don't welcome people in, they have problems associated with
[00:33:14.740 --> 00:33:15.740]   that.
[00:33:15.740 --> 00:33:22.060]   In a lot of ways, it is a conversation that everyone should be having, having.
[00:33:22.060 --> 00:33:24.500]   And so I agree.
[00:33:24.500 --> 00:33:25.740]   Long ways saying I agree.
[00:33:25.740 --> 00:33:30.100]   It's happening, you know, this is why it's kind of shocking to us in Silicon Valley because
[00:33:30.100 --> 00:33:32.180]   we really thought we were better than this.
[00:33:32.180 --> 00:33:36.140]   Look at Tesla, which you would assume is a very forward looking company.
[00:33:36.140 --> 00:33:42.300]   They've had some serious accusations and now they have a diversity panel on International
[00:33:42.300 --> 00:33:47.980]   Women's Day, they invited their female staff to a diversity panel.
[00:33:47.980 --> 00:33:54.220]   Actually, it was originally going to be an essential oils lunch and learn.
[00:33:54.220 --> 00:34:01.220]   But then people complained that, wait a minute, that's what you want us to talk about as ladies
[00:34:01.220 --> 00:34:02.460]   on International Women's Day?
[00:34:02.460 --> 00:34:05.860]   No, well, let's have a diversity panel.
[00:34:05.860 --> 00:34:14.340]   And it turned up even more issues, new allegations according to Engadget are going to make it
[00:34:14.340 --> 00:34:17.220]   hard for Tesla to downplay the accusations.
[00:34:17.220 --> 00:34:18.980]   It's the Guardian that they're reporting on this.
[00:34:18.980 --> 00:34:20.540]   Yes, actually, let's go to the Guardian.
[00:34:20.540 --> 00:34:21.900]   Sorry, I'm like, hold on.
[00:34:21.900 --> 00:34:23.540]   No, no, you're absolutely right.
[00:34:23.540 --> 00:34:27.620]   I read it and Engadget, but it's linking back to this.
[00:34:27.620 --> 00:34:29.220]   She took on Tesla for discrimination.
[00:34:29.220 --> 00:34:30.380]   Now others are speaking up.
[00:34:30.380 --> 00:34:31.380]   It's too big to deny.
[00:34:31.380 --> 00:34:35.780]   A female engineer who came forward with the claims of harassment says she was fired
[00:34:35.780 --> 00:34:39.220]   in retaliation, but now other women are stepping forward.
[00:34:39.220 --> 00:34:51.140]   So instead of discovering essential oils, they're maybe focusing a little bit more on
[00:34:51.140 --> 00:34:52.140]   the...
[00:34:52.140 --> 00:34:56.060]   Yeah, basically, when they did this open forum, instead of the essential oils, they heard
[00:34:56.060 --> 00:35:00.020]   about how women would avoid parts of the factory floor because they were getting cat
[00:35:00.020 --> 00:35:01.020]   called.
[00:35:01.020 --> 00:35:03.460]   They called them predator zones.
[00:35:03.460 --> 00:35:05.020]   Yes.
[00:35:05.020 --> 00:35:10.260]   Which kind of tells you a lot about what it's like to be a woman in some of these places
[00:35:10.260 --> 00:35:21.100]   and how the word inappropriate covers a multitude of sins, all of which are bad, and some of
[00:35:21.100 --> 00:35:22.820]   them are more inappropriate than others.
[00:35:22.820 --> 00:35:30.060]   I have to say, I think a lot of this is because we as men are not taught that it's bad.
[00:35:30.060 --> 00:35:31.060]   They should be.
[00:35:31.060 --> 00:35:35.020]   There's this kind of sense that, "Oh, boys will be boys.
[00:35:35.020 --> 00:35:38.500]   That's why you didn't like me to use that phrase, but that's the phrase that's used."
[00:35:38.500 --> 00:35:42.620]   And that's how guys are, and they're always going to hit on women and give them cat calls.
[00:35:42.620 --> 00:35:45.180]   The gals, they secretly like that.
[00:35:45.180 --> 00:35:47.980]   What about younger women, though?
[00:35:47.980 --> 00:35:48.980]   Or younger men?
[00:35:48.980 --> 00:35:49.980]   I don't know.
[00:35:49.980 --> 00:35:54.180]   I'm an old man, so I can't speak for younger men.
[00:35:54.180 --> 00:36:01.700]   I'm curious about what's happening generationally and how attitudes have shifted.
[00:36:01.700 --> 00:36:05.900]   Because like my generation, we got a lot of the "nice guys" which aren't nice.
[00:36:05.900 --> 00:36:09.340]   They're just like, "I'll pretend not to be a jerk on the surface so I get someone to
[00:36:09.340 --> 00:36:10.340]   sleep with me."
[00:36:10.340 --> 00:36:11.340]   But again, the-
[00:36:11.340 --> 00:36:12.340]   That's over.
[00:36:12.340 --> 00:36:13.340]   I've been using that for years.
[00:36:13.340 --> 00:36:14.340]   It's very effective.
[00:36:14.340 --> 00:36:17.340]   El Leo plays this cuddly thing.
[00:36:17.340 --> 00:36:18.340]   The nice guy thing.
[00:36:18.340 --> 00:36:19.340]   It's always a good one.
[00:36:19.340 --> 00:36:20.340]   Except that, never mind.
[00:36:20.340 --> 00:36:29.500]   I'm just going to get even worse to say that the girls always like the bad boys.
[00:36:29.500 --> 00:36:30.500]   That's the problem.
[00:36:30.500 --> 00:36:31.500]   Well, that's-
[00:36:31.500 --> 00:36:32.500]   I bought a leather jacket.
[00:36:32.500 --> 00:36:33.500]   I bought a leather jacket.
[00:36:33.500 --> 00:36:35.500]   I'm sorry, Stacey, go ahead.
[00:36:35.500 --> 00:36:39.060]   Jeff, you and I, we're going to go in the corner now.
[00:36:39.060 --> 00:36:41.500]   Where is my butt, El Leo?
[00:36:41.500 --> 00:36:44.500]   Where is that button?
[00:36:44.500 --> 00:36:46.500]   Yeah.
[00:36:46.500 --> 00:36:52.260]   So the point is, yes, boys should be taught better.
[00:36:52.260 --> 00:36:58.220]   I would encourage everyone, including you, Leo, to excise the phrase "boys will be boys"
[00:36:58.220 --> 00:37:03.460]   from your vocabulary and start making people acting as a role model and then also speaking
[00:37:03.460 --> 00:37:04.460]   up.
[00:37:04.460 --> 00:37:08.420]   I mean, guys can see when guys are behaving badly.
[00:37:08.420 --> 00:37:10.340]   You should just be like, "You're going to be on it."
[00:37:10.340 --> 00:37:11.340]   Dude, that's not cool.
[00:37:11.340 --> 00:37:12.340]   Yeah.
[00:37:12.340 --> 00:37:14.780]   Well, so here's the question to you about our social age.
[00:37:14.780 --> 00:37:21.900]   I'm fascinated by the Redditor who confessed to creating the-
[00:37:21.900 --> 00:37:22.900]   Wasn't that interesting.
[00:37:22.900 --> 00:37:28.100]   Trump beats up CN video and did an apology of sorts.
[00:37:28.100 --> 00:37:36.140]   And CNN decided that he was- that presumates he was a troll.
[00:37:36.140 --> 00:37:39.460]   Confessied enough that they wouldn't reveal him and wouldn't out him.
[00:37:39.460 --> 00:37:40.980]   There's some debate about that.
[00:37:40.980 --> 00:37:46.820]   But what strikes me so much is that how far overboard does an individual or a group or
[00:37:46.820 --> 00:37:51.180]   a society have to go to realize that bad behavior is bad?
[00:37:51.180 --> 00:37:55.700]   Do we have no hope of getting rid of trollish and misogynistic behavior unless it goes so
[00:37:55.700 --> 00:37:56.700]   far overboard?
[00:37:56.700 --> 00:37:59.260]   Unless the president of the United States grabs a woman by-
[00:37:59.260 --> 00:38:01.220]   Do you know why I will not say what-
[00:38:01.220 --> 00:38:03.220]   CNN in an auto play, sorry.
[00:38:03.220 --> 00:38:04.220]   All right, God bless you.
[00:38:04.220 --> 00:38:05.220]   Yeah.
[00:38:05.220 --> 00:38:08.580]   Well, CNN never learned that it has to go overboard without auto play before discovering
[00:38:08.580 --> 00:38:09.580]   everyone hates it.
[00:38:09.580 --> 00:38:16.180]   So, this is, this is, I don't know, what is it going to take to recapture civility and
[00:38:16.180 --> 00:38:21.060]   sense or to capture it in the first place if we have to go this far overboard?
[00:38:21.060 --> 00:38:26.860]   Somebody could post a whole mess of violence against media and anti-Semitic things to say,
[00:38:26.860 --> 00:38:28.100]   "Oh, right, I was just trolling now.
[00:38:28.100 --> 00:38:29.100]   I was going to-
[00:38:29.100 --> 00:38:30.100]   That's what was interesting.
[00:38:30.100 --> 00:38:35.500]   I wonder if that will often be the reaction or the result if you out a troll.
[00:38:35.500 --> 00:38:38.780]   They'll go, "Oh, you know, I don't really believe this anti-Semitic stuff.
[00:38:38.780 --> 00:38:41.260]   I just was trolling."
[00:38:41.260 --> 00:38:42.420]   I was just doing it for-
[00:38:42.420 --> 00:38:43.420]   I was doing it for the law.
[00:38:43.420 --> 00:38:50.380]   Or the venture capitalist who says, "Oh, okay, I confess I'm a jerk because I have to."
[00:38:50.380 --> 00:38:51.380]   Right.
[00:38:51.380 --> 00:38:52.900]   They might be a person who will be your dude.
[00:38:52.900 --> 00:38:53.900]   Yeah.
[00:38:53.900 --> 00:38:57.260]   Now, I'm not saying that there isn't a possibility to learn the lesson and to recant and that's
[00:38:57.260 --> 00:38:59.140]   fine and that's exactly what someone should do.
[00:38:59.140 --> 00:39:01.420]   So, I welcome that.
[00:39:01.420 --> 00:39:08.140]   But how do we teach the lesson before getting over the cliff?
[00:39:08.140 --> 00:39:09.140]   And I don't know-
[00:39:09.140 --> 00:39:10.140]   And by the way, the cliff-
[00:39:10.140 --> 00:39:11.420]   The cliff were right on the brink.
[00:39:11.420 --> 00:39:15.060]   They were a number of CNN journalists tweeted, "I'm afraid for my life.
[00:39:15.060 --> 00:39:18.220]   That feels like a call to violence by the president of the United States."
[00:39:18.220 --> 00:39:22.380]   And that actually, it puts me in fear of my safety.
[00:39:22.380 --> 00:39:25.380]   So this goes to Stacy's point that what you have-
[00:39:25.380 --> 00:39:30.100]   We all have to shun the bad behavior.
[00:39:30.100 --> 00:39:34.460]   You can't sit back silently and let it happen and then say, "Oh, yeah, I agree with that.
[00:39:34.460 --> 00:39:35.460]   That was awful.
[00:39:35.460 --> 00:39:37.140]   I hated it when it happened."
[00:39:37.140 --> 00:39:43.660]   We have to develop the norms and mores to show that we do not accept bad behavior, whether
[00:39:43.660 --> 00:39:46.500]   it's misogynistic or racist or whatever it is.
[00:39:46.500 --> 00:39:47.500]   Sorry, Stacy.
[00:39:47.500 --> 00:39:48.740]   Oh, no.
[00:39:48.740 --> 00:39:52.740]   We used to do that, although in America, and this is a real-
[00:39:52.740 --> 00:39:54.620]   I mean, this is attention in our society.
[00:39:54.620 --> 00:39:56.220]   We used to do that.
[00:39:56.220 --> 00:40:03.340]   And you've done to a certain way, you become a very censored and repressive society, right?
[00:40:03.340 --> 00:40:10.740]   And you could say, "Hey, being respectful of people is not awful and censoring and whatever."
[00:40:10.740 --> 00:40:14.220]   But it does require us to have some sort of common norms.
[00:40:14.220 --> 00:40:17.860]   And right now, we don't have common norms.
[00:40:17.860 --> 00:40:21.700]   We're fragmenting into all these crazy, different norms.
[00:40:21.700 --> 00:40:24.900]   And it's one thing to say, "Bye, guns, be, bye, guns."
[00:40:24.900 --> 00:40:32.500]   But we're also kind of shouting all our weird norms in these social media bubbles where other
[00:40:32.500 --> 00:40:34.900]   people can stumble across them.
[00:40:34.900 --> 00:40:41.020]   And I think this is really like structurally and from a societal perspective, really fascinating.
[00:40:41.020 --> 00:40:44.060]   And I'm not sure what we do with that.
[00:40:44.060 --> 00:40:51.500]   Because I mean, you could argue my making ribs is really offensive to someone who's a vegan.
[00:40:51.500 --> 00:40:52.500]   Right.
[00:40:52.500 --> 00:40:55.100]   And I'm not making fun of that.
[00:40:55.100 --> 00:40:58.020]   I mean, there are people who would feel very strongly about that.
[00:40:58.020 --> 00:41:03.300]   And if I were in certain cultures, making ribs would be a real problem.
[00:41:03.300 --> 00:41:08.380]   And we just, we're all living together in these bubbles where we see everybody else's
[00:41:08.380 --> 00:41:09.380]   bubbles.
[00:41:09.380 --> 00:41:10.380]   And it's...
[00:41:10.380 --> 00:41:12.220]   Here's an interesting question.
[00:41:12.220 --> 00:41:15.300]   Actually, the chat room kind of made me think of this.
[00:41:15.300 --> 00:41:21.700]   Because there's people saying, "See, and is threatening to dox this guy."
[00:41:21.700 --> 00:41:24.420]   And what about free speech?
[00:41:24.420 --> 00:41:27.020]   Does free speech protect anonymous speech?
[00:41:27.020 --> 00:41:31.380]   Free speech, this is the greatness gnomer.
[00:41:31.380 --> 00:41:39.420]   CNN, Reddit, Google, Facebook, Twitch, the New York Times all have the right to edit as
[00:41:39.420 --> 00:41:42.060]   a matter of their free speech.
[00:41:42.060 --> 00:41:47.420]   Allowing and enabling anyone's speech about anything in any manner is not the free speech
[00:41:47.420 --> 00:41:48.940]   that we're talking about.
[00:41:48.940 --> 00:41:53.100]   Government stopping you from speaking freely is the question of censorship.
[00:41:53.100 --> 00:41:57.060]   But CNN, Reddit, all those places have the right to edit.
[00:41:57.060 --> 00:41:58.820]   That's part of their right of free speech.
[00:41:58.820 --> 00:41:59.820]   So...
[00:41:59.820 --> 00:42:03.460]   XKCD's famous cartoon 1357.
[00:42:03.460 --> 00:42:06.740]   The right to free speech means that government can't arrest you for what you say.
[00:42:06.740 --> 00:42:11.300]   It doesn't mean anyone else has to listen to your BS or host you while you share it.
[00:42:11.300 --> 00:42:15.980]   The First Amendment doesn't shield you from criticism or consequences.
[00:42:15.980 --> 00:42:19.500]   If you're yelled at boycotted, have your show canceled, get banned from the internet community,
[00:42:19.500 --> 00:42:22.540]   your free speech rights are not being violated.
[00:42:22.540 --> 00:42:28.660]   It's just the people listening think you're an a-hole and they're showing you the door.
[00:42:28.660 --> 00:42:33.140]   So to your point.
[00:42:33.140 --> 00:42:35.740]   But okay.
[00:42:35.740 --> 00:42:39.980]   So we'll see it in one and doxing that guy.
[00:42:39.980 --> 00:42:40.980]   What do you think of that?
[00:42:40.980 --> 00:42:41.980]   They didn't doxing.
[00:42:41.980 --> 00:42:43.980]   They chose not to doxing.
[00:42:43.980 --> 00:42:48.660]   I mean, they said we know who it is, but they didn't reveal.
[00:42:48.660 --> 00:42:49.660]   Right.
[00:42:49.660 --> 00:42:50.660]   No.
[00:42:50.660 --> 00:42:51.660]   No.
[00:42:51.660 --> 00:42:52.660]   Right.
[00:42:52.660 --> 00:42:53.660]   Journalistically, is that the right thing to do?
[00:42:53.660 --> 00:42:58.020]   This is somebody who the president of the United States passed this on.
[00:42:58.020 --> 00:43:04.220]   This fed into his Twitter feed and hate of the media was a newsworthy event.
[00:43:04.220 --> 00:43:06.260]   It was newsworthy of who did it.
[00:43:06.260 --> 00:43:10.300]   People have a right to know who did the not a right to go back as journalistically.
[00:43:10.300 --> 00:43:12.500]   I think it's journalistically appropriate.
[00:43:12.500 --> 00:43:13.500]   Yeah.
[00:43:13.500 --> 00:43:14.500]   To what to not have doxed him?
[00:43:14.500 --> 00:43:15.660]   No, to dox him.
[00:43:15.660 --> 00:43:16.660]   I think it's right.
[00:43:16.660 --> 00:43:17.660]   I think it is to dox him.
[00:43:17.660 --> 00:43:21.460]   So, so what makes CNN think in this case, we're not going to reveal?
[00:43:21.460 --> 00:43:25.100]   Well, maybe as somebody in the chairman points out, the threat is if you don't do it anymore,
[00:43:25.100 --> 00:43:26.420]   we will reveal you.
[00:43:26.420 --> 00:43:27.700]   But is that CNN's role?
[00:43:27.700 --> 00:43:30.540]   Isn't that up to the public to decide?
[00:43:30.540 --> 00:43:31.540]   Right.
[00:43:31.540 --> 00:43:32.540]   No.
[00:43:32.540 --> 00:43:36.820]   I would say it's a natural consequence of throwing something like that out there with CNN as
[00:43:36.820 --> 00:43:37.820]   the target.
[00:43:37.820 --> 00:43:43.700]   Like that's the targeted individual speaking up and saying, "Hey, don't do that again."
[00:43:43.700 --> 00:43:44.700]   Yeah.
[00:43:44.700 --> 00:43:51.260]   I mean, yeah, I don't have a problem with it.
[00:43:51.260 --> 00:43:52.860]   Well, is it chilling?
[00:43:52.860 --> 00:43:58.700]   No, because free speech is about government suppression of speech, not about facing the
[00:43:58.700 --> 00:44:01.300]   consequences, right?
[00:44:01.300 --> 00:44:02.300]   Of your speech.
[00:44:02.300 --> 00:44:03.300]   Yeah.
[00:44:03.300 --> 00:44:06.420]   There's a lot of people coming back to this question of civility.
[00:44:06.420 --> 00:44:12.060]   The fake news, lack of facts, argument is civil, Jeff.
[00:44:12.060 --> 00:44:13.060]   Yeah.
[00:44:13.060 --> 00:44:16.580]   Well, that's kind of what I meant when I said boys, well, boys, and men are very much not
[00:44:16.580 --> 00:44:17.580]   civil.
[00:44:17.580 --> 00:44:23.500]   I mean, if it weren't for the civilizing force of the females, I think this would be a very
[00:44:23.500 --> 00:44:24.500]   different world.
[00:44:24.500 --> 00:44:25.500]   Oh, that is.
[00:44:25.500 --> 00:44:26.500]   Okay.
[00:44:26.500 --> 00:44:27.500]   That is crazy sexist.
[00:44:27.500 --> 00:44:28.500]   I think it's true.
[00:44:28.500 --> 00:44:29.500]   I think it's true.
[00:44:29.500 --> 00:44:31.380]   As a guy, I think it's true.
[00:44:31.380 --> 00:44:39.780]   I can tell you that I am not necessarily as civil as my husband is very civil.
[00:44:39.780 --> 00:44:40.780]   Okay.
[00:44:40.780 --> 00:44:43.140]   But Stacy put it another way on the terms of going overboard.
[00:44:43.140 --> 00:44:46.540]   How often have you run across a female troll?
[00:44:46.540 --> 00:44:52.220]   I know many women who are trollish in their behavior, who may not be trolling aggressively
[00:44:52.220 --> 00:44:55.020]   on Twitter, but that's a very different point.
[00:44:55.020 --> 00:44:56.020]   You're right.
[00:44:56.020 --> 00:44:57.020]   Okay.
[00:44:57.020 --> 00:44:58.020]   You're right.
[00:44:58.020 --> 00:45:02.820]   I mean, I have said very negative things about a woman.
[00:45:02.820 --> 00:45:07.620]   Just I have been known to use words that are really awful.
[00:45:07.620 --> 00:45:10.220]   I am not at all civilized.
[00:45:10.220 --> 00:45:14.100]   And if they see, if I know, see, but that's the point.
[00:45:14.100 --> 00:45:18.780]   The world is crumbling around me.
[00:45:18.780 --> 00:45:20.780]   We would have a veneer of civilization.
[00:45:20.780 --> 00:45:23.620]   We just, you know, we're not going there.
[00:45:23.620 --> 00:45:25.420]   We're not going there.
[00:45:25.420 --> 00:45:26.860]   No, no, no, you know what?
[00:45:26.860 --> 00:45:28.020]   Here's where I'll go.
[00:45:28.020 --> 00:45:30.420]   In general, humans are pretty bad.
[00:45:30.420 --> 00:45:32.420]   We are very flawed.
[00:45:32.420 --> 00:45:39.180]   We are flawed and one of the goals humanity should have is to be better.
[00:45:39.180 --> 00:45:41.300]   All of us.
[00:45:41.300 --> 00:45:45.260]   And that's called civilization, right?
[00:45:45.260 --> 00:45:47.980]   And I think as a civil society.
[00:45:47.980 --> 00:45:53.220]   And I think one of the things that you and I Jeff are upset about is the breakdown of
[00:45:53.220 --> 00:45:55.820]   civil society on places like Twitter.
[00:45:55.820 --> 00:45:57.820]   Hey, it's not as--
[00:45:57.820 --> 00:46:00.300]   It's just Ed Sheeran's unhappy too.
[00:46:00.300 --> 00:46:02.180]   You see, he's quitting Twitter because of--
[00:46:02.180 --> 00:46:03.180]   Yep.
[00:46:03.180 --> 00:46:04.660]   Because of the mean tweets.
[00:46:04.660 --> 00:46:10.940]   I don't think quitting Twitter stops the mean tweets, but at least you don't have to
[00:46:10.940 --> 00:46:11.940]   look at them.
[00:46:11.940 --> 00:46:12.940]   Yeah.
[00:46:12.940 --> 00:46:13.940]   Well, that's the Twitter rule.
[00:46:13.940 --> 00:46:16.100]   Twitter, oh, you can just mute these people.
[00:46:16.100 --> 00:46:18.700]   They're still going to be insulting you and saying horrible things about you and threatening
[00:46:18.700 --> 00:46:19.700]   you.
[00:46:19.700 --> 00:46:20.980]   But you don't have to see it.
[00:46:20.980 --> 00:46:22.420]   That's the Twitter rule.
[00:46:22.420 --> 00:46:23.420]   Yeah.
[00:46:23.420 --> 00:46:25.180]   That's the only rule.
[00:46:25.180 --> 00:46:27.100]   Twitter says, we'll publish anything.
[00:46:27.100 --> 00:46:28.100]   Just, you know.
[00:46:28.100 --> 00:46:29.100]   So, yeah.
[00:46:29.100 --> 00:46:34.620]   So I think that in a way, that's what this whole conversation could boil down to is are
[00:46:34.620 --> 00:46:42.220]   we moving forward as humans in becoming better or are we moving backwards and reverting to
[00:46:42.220 --> 00:46:44.700]   a less civilized state?
[00:46:44.700 --> 00:46:49.060]   I think it fits in sports because I think that you test the limits and then you pull
[00:46:49.060 --> 00:46:54.140]   back and renegotiate.
[00:46:54.140 --> 00:46:55.140]   Hope you're right.
[00:46:55.140 --> 00:46:58.860]   I hope I'm optimistic, but I'm too informed.
[00:46:58.860 --> 00:47:02.380]   You know, and the whole conversation may be moot when we get into a nuclear war with
[00:47:02.380 --> 00:47:03.380]   Korea.
[00:47:03.380 --> 00:47:04.380]   Yep.
[00:47:04.380 --> 00:47:05.380]   Yes.
[00:47:05.380 --> 00:47:07.500]   That will be very exciting.
[00:47:07.500 --> 00:47:11.180]   There are other fish being fried at this very moment.
[00:47:11.180 --> 00:47:18.100]   Hey, but meanwhile, I have a gadget that will spark, sparkle stuff in my skin.
[00:47:18.100 --> 00:47:22.940]   Mark goes to stuff in my refrigerator.
[00:47:22.940 --> 00:47:23.940]   Anybody?
[00:47:23.940 --> 00:47:24.940]   I got one.
[00:47:24.940 --> 00:47:27.140]   It's just a right.
[00:47:27.140 --> 00:47:29.580]   I thought we were going to go for a Google story.
[00:47:29.580 --> 00:47:31.500]   I often feel guilty.
[00:47:31.500 --> 00:47:37.820]   I think we've talked about this before because sometimes, you know, technology journalism
[00:47:37.820 --> 00:47:42.220]   and what we do it to is about the toy store.
[00:47:42.220 --> 00:47:46.780]   And there's lots of toys, but then sometimes I feel like we're using those toys to distract
[00:47:46.780 --> 00:47:53.260]   ourselves from things that actually have real meaning and purpose and consequences unlike
[00:47:53.260 --> 00:48:01.100]   the Amazon, low-skeleton wand, which has absolutely no meaning, no purpose and zero consequence.
[00:48:01.100 --> 00:48:03.900]   Is this the dash wand with Alexa?
[00:48:03.900 --> 00:48:04.900]   Yeah.
[00:48:04.900 --> 00:48:05.900]   Okay.
[00:48:05.900 --> 00:48:06.900]   Don't say her name.
[00:48:06.900 --> 00:48:07.900]   Yeah.
[00:48:07.900 --> 00:48:08.900]   It's the Amazon wand.
[00:48:08.900 --> 00:48:09.900]   So I'm still waiting.
[00:48:09.900 --> 00:48:11.900]   It's neither Stacy nor I or his fashion.
[00:48:11.900 --> 00:48:15.540]   He's still enough to get our invitation to buy the look.
[00:48:15.540 --> 00:48:17.540]   Ah, Amazon, come on.
[00:48:17.540 --> 00:48:18.540]   Come on.
[00:48:18.540 --> 00:48:19.540]   Look at her.
[00:48:19.540 --> 00:48:21.740]   She's a fashion plate.
[00:48:21.740 --> 00:48:23.820]   Wonderful things you could style for me.
[00:48:23.820 --> 00:48:24.820]   Wonderful clothes.
[00:48:24.820 --> 00:48:29.980]   And I would submit that I ain't some no slouching at the Pardonnay today.
[00:48:29.980 --> 00:48:30.980]   You own a fedora.
[00:48:30.980 --> 00:48:31.980]   My goodness.
[00:48:31.980 --> 00:48:32.980]   I own many fedoras.
[00:48:32.980 --> 00:48:34.980]   Oh, multiple fedoras.
[00:48:34.980 --> 00:48:36.420]   I buy hats.
[00:48:36.420 --> 00:48:38.380]   I buy jackets and clothing.
[00:48:38.380 --> 00:48:40.700]   I'm a very stylish fella.
[00:48:40.700 --> 00:48:45.100]   And I think frankly anybody who wants the look should be allowed to buy it.
[00:48:45.100 --> 00:48:46.100]   Isn't it?
[00:48:46.100 --> 00:48:48.780]   I think it actually, so I was thinking about it.
[00:48:48.780 --> 00:48:53.980]   It because a friend of mine wanted it because she does quilting and she wanted to show people
[00:48:53.980 --> 00:48:57.140]   like because her hands are busy when she's quilting.
[00:48:57.140 --> 00:49:01.100]   She liked the idea of telling it to snap a picture for her blogs or whatever.
[00:49:01.100 --> 00:49:02.100]   Oh, yeah.
[00:49:02.100 --> 00:49:05.740]   And I was thinking about that when I'm working like installing a gadget, you know, and I've
[00:49:05.740 --> 00:49:10.700]   got my hands, you know, I could have it take pictures while I'm installing.
[00:49:10.700 --> 00:49:14.740]   So you get a step by step of, you know, and if you're really lucky, maybe you get Stacy
[00:49:14.740 --> 00:49:16.780]   getting fried with the electrical circuit.
[00:49:16.780 --> 00:49:19.420]   So I thought I was okay with having cameras everywhere.
[00:49:19.420 --> 00:49:20.420]   I really did.
[00:49:20.420 --> 00:49:21.780]   In fact, remember Stacy?
[00:49:21.780 --> 00:49:24.740]   I ordered and you call me a doofus for doing it.
[00:49:24.740 --> 00:49:28.500]   The Nest IQ cams.
[00:49:28.500 --> 00:49:33.180]   So I put, I got two of them and I put them up in the house about a week ago.
[00:49:33.180 --> 00:49:36.660]   And the idea of the IQ cam is a couple of interesting things.
[00:49:36.660 --> 00:49:43.180]   One has a, I think a 4K camera, but it doesn't stream the full 4K, but it can zoom in.
[00:49:43.180 --> 00:49:46.020]   So if it sees faces or movement, it can zoom in on it.
[00:49:46.020 --> 00:49:51.260]   It does face recognition and in theory, although I thought it didn't really, well, I don't
[00:49:51.260 --> 00:49:53.980]   know what, how long it was going to take, but it would start to recognize people in your
[00:49:53.980 --> 00:49:56.460]   house and not notify you about them.
[00:49:56.460 --> 00:49:58.260]   Only notify you when a stranger was in the house.
[00:49:58.260 --> 00:50:00.620]   But of course, when you first put it in, everybody's a stranger.
[00:50:00.620 --> 00:50:02.060]   It was telling me about balloons.
[00:50:02.060 --> 00:50:05.860]   Don't you draw little faces on them?
[00:50:05.860 --> 00:50:07.500]   I didn't have to.
[00:50:07.500 --> 00:50:13.660]   So funny that there's somebody in your house and it zoomed in on these my alarm balloons.
[00:50:13.660 --> 00:50:15.620]   Well, not really.
[00:50:15.620 --> 00:50:20.260]   Anyway, I thought I was okay with cameras because, you know, I got cameras everywhere.
[00:50:20.260 --> 00:50:23.340]   I got all the Amazon stuff.
[00:50:23.340 --> 00:50:27.580]   And then, and then after a little while, I got, I got a little kind of creeped out by it
[00:50:27.580 --> 00:50:32.420]   and I ended up taking them out and they're here at work where there's some ignorance anywhere.
[00:50:32.420 --> 00:50:33.420]   Yeah.
[00:50:33.420 --> 00:50:35.180]   Anyway, it doesn't really matter.
[00:50:35.180 --> 00:50:37.580]   You don't pick your nose in your office, right?
[00:50:37.580 --> 00:50:41.060]   Well, that's the, I started to realize, I mean, that we have one in the living room and
[00:50:41.060 --> 00:50:44.460]   one in my office, the IQ, and that's like you.
[00:50:44.460 --> 00:50:48.780]   And I thought, you know, I know no one's seeing it but me unless they get hacked, but you
[00:50:48.780 --> 00:50:52.420]   know, you got to, and you've said this too, the nest has good security.
[00:50:52.420 --> 00:50:58.260]   I turned on two factor so nobody could, you know, hack my or make it hard to hack my account.
[00:50:58.260 --> 00:51:00.900]   But I still, the idea in this is little green glowing light.
[00:51:00.900 --> 00:51:04.260]   It has night vision, very good night vision.
[00:51:04.260 --> 00:51:06.660]   And I just kind of got creeped out by it.
[00:51:06.660 --> 00:51:07.660]   Ooh.
[00:51:07.660 --> 00:51:08.660]   Yeah.
[00:51:08.660 --> 00:51:12.100]   No one wants to see your like midnight refrigerator runs when you're at PJ.
[00:51:12.100 --> 00:51:13.100]   Exactly.
[00:51:13.100 --> 00:51:14.100]   Exactly.
[00:51:14.100 --> 00:51:17.620]   So I'm going to, I'm going to throw it out there again.
[00:51:17.620 --> 00:51:21.900]   I would try the net atmo because none of that goes to the cloud and it actually has better
[00:51:21.900 --> 00:51:26.420]   based on what you just told me, better facial recognition because you train it in your first,
[00:51:26.420 --> 00:51:27.620]   you know, initial setup.
[00:51:27.620 --> 00:51:29.580]   That's something that the nest should do.
[00:51:29.580 --> 00:51:31.420]   They should have training.
[00:51:31.420 --> 00:51:32.420]   So net atmo.
[00:51:32.420 --> 00:51:36.620]   And if it doesn't go to the cloud, where does it go?
[00:51:36.620 --> 00:51:40.980]   You can set it up on your own FTP server or you can actually, if you want, you can actually
[00:51:40.980 --> 00:51:44.260]   send it to Dropbox or you can save it to an SD card.
[00:51:44.260 --> 00:51:45.260]   It's French.
[00:51:45.260 --> 00:51:46.260]   That's why.
[00:51:46.260 --> 00:51:47.260]   Yes.
[00:51:47.260 --> 00:51:48.260]   It's European.
[00:51:48.260 --> 00:51:50.100]   They're more privacy focused.
[00:51:50.100 --> 00:51:51.100]   Well.
[00:51:51.100 --> 00:51:54.260]   To a fault, we'll get to that story in a minute.
[00:51:54.260 --> 00:51:56.300]   Yeah, we will.
[00:51:56.300 --> 00:51:57.900]   Yeah, we will.
[00:51:57.900 --> 00:51:58.900]   All right.
[00:51:58.900 --> 00:52:03.340]   Any TATMO and they have an outdoor and an indoor one, I've got the nests outdoors.
[00:52:03.340 --> 00:52:04.340]   Those I don't mind.
[00:52:04.340 --> 00:52:07.500]   I don't mind if critters get caught in the camera.
[00:52:07.500 --> 00:52:11.820]   The truth is also there was really no justification for me having cameras in the house.
[00:52:11.820 --> 00:52:16.540]   I don't, we're not worried about getting robbed or anything.
[00:52:16.540 --> 00:52:17.540]   So I was.
[00:52:17.540 --> 00:52:21.060]   A little bit of a little bit in there for pets because everybody, whenever I ask about
[00:52:21.060 --> 00:52:23.260]   this, they're like, I love seeing my pets.
[00:52:23.260 --> 00:52:24.260]   Yeah.
[00:52:24.260 --> 00:52:25.260]   And some people put it.
[00:52:25.260 --> 00:52:27.780]   I don't love my pets that much.
[00:52:27.780 --> 00:52:32.260]   Well, there's also the front door and okay, this is a very specific use case, but we have
[00:52:32.260 --> 00:52:36.580]   a fish tank and we bought snails and then we went out of town for a week and we were
[00:52:36.580 --> 00:52:40.380]   very concerned about the fate of the snails.
[00:52:40.380 --> 00:52:45.180]   The problem is, okay, so do you get motion activation from snails or no?
[00:52:45.180 --> 00:52:50.500]   Well, so I just thought, you know, we could just have a camera that we could check in
[00:52:50.500 --> 00:52:51.500]   on the live.
[00:52:51.500 --> 00:52:52.500]   I wasn't worried about motion.
[00:52:52.500 --> 00:52:53.500]   But you're right.
[00:52:53.500 --> 00:52:56.500]   You're still hasn't moved.
[00:52:56.500 --> 00:52:58.500]   Well, maybe it has.
[00:52:58.500 --> 00:53:00.500]   All the time laps.
[00:53:00.500 --> 00:53:05.260]   I wish I could get my money back in the next IQ, but I can't.
[00:53:05.260 --> 00:53:06.260]   It's really a great camera.
[00:53:06.260 --> 00:53:07.260]   I'll show you the camera.
[00:53:07.260 --> 00:53:08.260]   I don't know.
[00:53:08.260 --> 00:53:09.260]   Do we take them down?
[00:53:09.260 --> 00:53:10.260]   I don't see it.
[00:53:10.260 --> 00:53:12.020]   You can give them away.
[00:53:12.020 --> 00:53:13.020]   I could give them to the state.
[00:53:13.020 --> 00:53:14.020]   Or you can sell them on eBay.
[00:53:14.020 --> 00:53:17.020]   Oh, actually somebody unplugged it.
[00:53:17.020 --> 00:53:18.660]   Why was it unplugged?
[00:53:18.660 --> 00:53:22.500]   I guess even my staff doesn't like the idea.
[00:53:22.500 --> 00:53:23.940]   They unplug them.
[00:53:23.940 --> 00:53:27.140]   Well, they have a right to.
[00:53:27.140 --> 00:53:28.540]   They have a right to.
[00:53:28.540 --> 00:53:29.540]   But look at that.
[00:53:29.540 --> 00:53:30.740]   Do you want look at that?
[00:53:30.740 --> 00:53:32.180]   I blaring at you.
[00:53:32.180 --> 00:53:33.180]   Do you want that?
[00:53:33.180 --> 00:53:34.180]   It is.
[00:53:34.180 --> 00:53:36.980]   Doesn't that look like it's watching you?
[00:53:36.980 --> 00:53:37.980]   Did I show you the NetAppo?
[00:53:37.980 --> 00:53:40.020]   I think you look proud of mine.
[00:53:40.020 --> 00:53:41.220]   Well, I'm looking at the website.
[00:53:41.220 --> 00:53:44.580]   It looks like it looks more like a bar than a.
[00:53:44.580 --> 00:53:46.820]   Yeah, it's just a friendly cylinder.
[00:53:46.820 --> 00:53:48.620]   It's a friendly cylinder.
[00:53:48.620 --> 00:53:49.620]   Well, it is.
[00:53:49.620 --> 00:53:52.180]   It doesn't look like yours looks like an eye.
[00:53:52.180 --> 00:53:53.260]   Mine looks like this.
[00:53:53.260 --> 00:53:55.420]   It used to be on my desk, but I cleaned up.
[00:53:55.420 --> 00:53:56.940]   This is why I don't clean up you guys.
[00:53:56.940 --> 00:53:59.660]   So do you have it for besides nails, other pets?
[00:53:59.660 --> 00:54:02.500]   Or is it?
[00:54:02.500 --> 00:54:07.660]   I had it because it has the I really like the facial recognition feature and the way they
[00:54:07.660 --> 00:54:08.660]   implemented it.
[00:54:08.660 --> 00:54:10.740]   It seems like a good idea.
[00:54:10.740 --> 00:54:11.740]   Yeah.
[00:54:11.740 --> 00:54:16.300]   I keep waiting for companies to use that part of the API.
[00:54:16.300 --> 00:54:21.940]   So if it has NetAppo, but it doesn't use the facial recognition part of the API, it maybe
[00:54:21.940 --> 00:54:24.460]   that's a privacy setting because they're French.
[00:54:24.460 --> 00:54:29.860]   And I would love for it to be like, hey, I haven't seen Stacey, her daughter or her
[00:54:29.860 --> 00:54:31.660]   husband for 15 minutes.
[00:54:31.660 --> 00:54:33.460]   I'm going to arm the security system.
[00:54:33.460 --> 00:54:34.460]   Yeah.
[00:54:34.460 --> 00:54:38.260]   So it does that already, but that information, I would love for that information to be used
[00:54:38.260 --> 00:54:40.300]   for some of my other devices too.
[00:54:40.300 --> 00:54:43.780]   So the Nest does that, but it does it based on your smartphone.
[00:54:43.780 --> 00:54:46.900]   So it says, as soon as you move out of range with your smartphone.
[00:54:46.900 --> 00:54:48.700]   My daughter doesn't have a smartphone.
[00:54:48.700 --> 00:54:49.700]   Right.
[00:54:49.700 --> 00:54:50.700]   No, I like it.
[00:54:50.700 --> 00:54:51.700]   Why do that?
[00:54:51.700 --> 00:54:52.700]   Why not do the recognition?
[00:54:52.700 --> 00:54:55.300]   Here's the image from my studio, my studio is on right now.
[00:54:55.300 --> 00:54:57.500]   And it's a pretty detailed image.
[00:54:57.500 --> 00:54:59.060]   That's a lot of hats.
[00:54:59.060 --> 00:55:01.340]   Why do what play?
[00:55:01.340 --> 00:55:03.340]   That's our Leo.
[00:55:03.340 --> 00:55:05.340]   What is happening there?
[00:55:05.340 --> 00:55:06.340]   Oh my gosh.
[00:55:06.340 --> 00:55:08.460]   Is that the ostrich sleeping pillow?
[00:55:08.460 --> 00:55:09.980]   Yes, it is.
[00:55:09.980 --> 00:55:11.220]   How did you recognize that?
[00:55:11.220 --> 00:55:12.220]   That's exactly what it is.
[00:55:12.220 --> 00:55:13.220]   Because I am obsessed with that thing.
[00:55:13.220 --> 00:55:15.420]   Like somebody stealing my hat quick.
[00:55:15.420 --> 00:55:16.580]   Get him.
[00:55:16.580 --> 00:55:19.140]   Go get him, Nest.
[00:55:19.140 --> 00:55:24.580]   The other thing I like about this is both the Nest and the Ring Video Doorbell, I can
[00:55:24.580 --> 00:55:29.620]   pipe to the Echo Show, which is the Echo Show is the one, and you can see it on my desk
[00:55:29.620 --> 00:55:33.620]   right now with the screen on it.
[00:55:33.620 --> 00:55:40.220]   So you can say, Echo Show my front door or Echo Show the Studio Cam, and you can see it
[00:55:40.220 --> 00:55:42.220]   on the Echo, which is kind of neat.
[00:55:42.220 --> 00:55:44.380]   How was your latency?
[00:55:44.380 --> 00:55:49.620]   Because I did that with my August, and I was like, showed me my front door, and I counted
[00:55:49.620 --> 00:55:51.620]   to six seconds before it.
[00:55:51.620 --> 00:55:54.420]   Yeah, and that's a problem.
[00:55:54.420 --> 00:56:00.260]   With the Ring, the first time it says it's asleep, but presumably somebody's ringing
[00:56:00.260 --> 00:56:04.220]   the doorbell, or if there's motion, it will be awake.
[00:56:04.220 --> 00:56:07.380]   And there is, yes, six seconds, that sounds about right, which is a little bit of an issue
[00:56:07.380 --> 00:56:15.060]   because the person might have run the doorbell and laughed by the time you see them.
[00:56:15.060 --> 00:56:21.660]   I wanted to show, when we did the Fourth of July party yesterday, I had this great idea
[00:56:21.660 --> 00:56:25.820]   for a use case, and I don't know if it would be feasible, but getting the Arlo Outdoor
[00:56:25.820 --> 00:56:31.260]   cameras, or I guess the Logitech I know has an Echo Show integration, I'm not sure about
[00:56:31.260 --> 00:56:32.740]   the Arlo cameras.
[00:56:32.740 --> 00:56:37.700]   But being able to show the parents, their kids playing on the roof while we're all downstairs
[00:56:37.700 --> 00:56:41.980]   drinking, and the kids are all running around, that would have been actually kind of a cool
[00:56:41.980 --> 00:56:42.980]   use case.
[00:56:42.980 --> 00:56:43.980]   So.
[00:56:43.980 --> 00:56:44.980]   Yeah.
[00:56:44.980 --> 00:56:48.260]   I don't know.
[00:56:48.260 --> 00:56:52.380]   So to me, I like the Echo Show a lot.
[00:56:52.380 --> 00:56:54.420]   The IQ camera is a little less.
[00:56:54.420 --> 00:56:56.580]   I do like my doorbell camera.
[00:56:56.580 --> 00:56:58.500]   That makes, that's a use case that makes sense.
[00:56:58.500 --> 00:56:59.500]   That makes sense.
[00:56:59.500 --> 00:57:00.940]   And an outdoor camera makes sense.
[00:57:00.940 --> 00:57:04.620]   An indoor camera makes a little less sense to me.
[00:57:04.620 --> 00:57:05.620]   Yeah.
[00:57:05.620 --> 00:57:07.740]   Other than the snails.
[00:57:07.740 --> 00:57:12.820]   My husband, when we go out of town for a long time, he does like having an indoor camera.
[00:57:12.820 --> 00:57:15.460]   But people are pointing out that you can do this on the phone.
[00:57:15.460 --> 00:57:16.460]   Of course, you can't.
[00:57:16.460 --> 00:57:17.460]   All these apps.
[00:57:17.460 --> 00:57:19.700]   And I'm just sure you might ask camera on the web.
[00:57:19.700 --> 00:57:25.500]   But having it right there, by voice control, and to me, the Echo Show belongs on your desk.
[00:57:25.500 --> 00:57:26.500]   It's a desk.
[00:57:26.500 --> 00:57:29.100]   It's like, it's a great or kitchen, but desk.
[00:57:29.100 --> 00:57:31.140]   It's like a counter thing.
[00:57:31.140 --> 00:57:32.140]   It is.
[00:57:32.140 --> 00:57:34.180]   So we have ours on.
[00:57:34.180 --> 00:57:37.780]   This is my breakfast bar.
[00:57:37.780 --> 00:57:38.780]   That's what it's called.
[00:57:38.780 --> 00:57:42.020]   I always say my bar, but it's not a bar.
[00:57:42.020 --> 00:57:43.020]   Yeah.
[00:57:43.020 --> 00:57:48.460]   And we actually, we were actually watching quirky videos on YouTube the other day in the morning.
[00:57:48.460 --> 00:57:49.460]   Yeah.
[00:57:49.460 --> 00:57:50.460]   On your Echo Show.
[00:57:50.460 --> 00:57:51.460]   Yeah.
[00:57:51.460 --> 00:57:54.060]   And I like it that I could sit up my flash.
[00:57:54.060 --> 00:57:57.260]   I thought it was going to be very low quality because it's so small.
[00:57:57.260 --> 00:58:00.460]   10, 24 by 9 80 is fine.
[00:58:00.460 --> 00:58:04.860]   And I have, you can see the Tonight Show monologue from the night before CNN has videos.
[00:58:04.860 --> 00:58:06.860]   I think CNBC is doing video.
[00:58:06.860 --> 00:58:11.420]   I think we're going to inquire because we do, we have a flash briefing for Twit, which
[00:58:11.420 --> 00:58:14.060]   is Monday through Friday, our news techniques today.
[00:58:14.060 --> 00:58:16.740]   And then on Saturday, it's a new screensavers and Sunday, it's Twit.
[00:58:16.740 --> 00:58:17.740]   And it'd be great.
[00:58:17.740 --> 00:58:19.180]   We could put video in there.
[00:58:19.180 --> 00:58:20.660]   And I think that's a nice use for it.
[00:58:20.660 --> 00:58:23.100]   I love seeing lyrics in songs.
[00:58:23.100 --> 00:58:25.260]   Isn't that nice?
[00:58:25.260 --> 00:58:26.740]   Mm-hmm.
[00:58:26.740 --> 00:58:31.660]   I still think that karaoke is a missed opportunity.
[00:58:31.660 --> 00:58:35.300]   Well, you could do it and you could sing along with a real song.
[00:58:35.300 --> 00:58:39.620]   But then you have the, then you have the, the, the, then you sound bad next to the real person.
[00:58:39.620 --> 00:58:44.340]   Well, I believe, in fact, I know because I keep getting them by accident, there are karaoke
[00:58:44.340 --> 00:58:46.980]   tracks on most of the music services.
[00:58:46.980 --> 00:58:50.100]   If you ask, you know, used to be when you asked for the Beatles, you'd get Beatles karaoke
[00:58:50.100 --> 00:58:51.100]   because they didn't own the.
[00:58:51.100 --> 00:58:54.060]   But we did it last week and there was no, there were no lyrics.
[00:58:54.060 --> 00:58:58.140]   So yeah, if you get the real song, there may not be lyrics.
[00:58:58.140 --> 00:59:03.100]   But if you get the karaoke version in Amazon, you have to ask for the karaoke version.
[00:59:03.100 --> 00:59:06.300]   But Google constantly service me to karaoke version things.
[00:59:06.300 --> 00:59:07.780]   I'm like, God, stop.
[00:59:07.780 --> 00:59:08.780]   I don't want it.
[00:59:08.780 --> 00:59:09.780]   Yeah, take it out.
[00:59:09.780 --> 00:59:13.700]   And I don't understand why Google is so much worse because they're both pulling from Spotify.
[00:59:13.700 --> 00:59:17.740]   So I'm like, what is, what is happening on the back end there?
[00:59:17.740 --> 00:59:18.740]   That's interesting.
[00:59:18.740 --> 00:59:20.940]   So using Spotify as your service on both of those.
[00:59:20.940 --> 00:59:21.940]   Yeah.
[00:59:21.940 --> 00:59:28.860]   If I, and I actually, I shot a little video of this of asking for certain songs and Google
[00:59:28.860 --> 00:59:34.140]   will always feed me the karaoke version of these songs, whereas the echo will feed me
[00:59:34.140 --> 00:59:35.140]   the right version.
[00:59:35.140 --> 00:59:37.660]   I have no idea why this is different.
[00:59:37.660 --> 00:59:44.900]   So I get, I use for my music on the echo, use Amazon unlimited music as well as Google
[00:59:44.900 --> 00:59:45.900]   music.
[00:59:45.900 --> 00:59:51.740]   But I almost always get lyrics, but not always, but some, some, some for some reason I like
[00:59:51.740 --> 00:59:54.220]   to, there's some country artists that I don't get lyrics.
[00:59:54.220 --> 00:59:57.980]   And I just think they're not on rap genius or whatever it is that, that they use for
[00:59:57.980 --> 00:59:58.980]   lyrics, right?
[00:59:58.980 --> 01:00:01.020]   How many music services do you pay for?
[01:00:01.020 --> 01:00:04.220]   Google and Amazon, I used to pay for Spotify.
[01:00:04.220 --> 01:00:11.020]   I dumped it and actually Paul Therat on Windows weekly moments ago, showed me this great site
[01:00:11.020 --> 01:00:17.460]   that lets you import your playlists from one service to another, which is huge for me
[01:00:17.460 --> 01:00:21.340]   because I have all these great playlists that I miss on Spotify.
[01:00:21.340 --> 01:00:24.660]   It's called Stamp, it's a free-year music.com.
[01:00:24.660 --> 01:00:29.500]   So I can finally get my Spotify playlist moved over to my Google Play music.
[01:00:29.500 --> 01:00:35.500]   I might pay for Apple music too because they now have a $99 a year tier, which is almost
[01:00:35.500 --> 01:00:38.900]   was like eight bucks a month, a little less.
[01:00:38.900 --> 01:00:41.100]   That's a pretty good deal.
[01:00:41.100 --> 01:00:45.460]   You only need one obviously, but I only need one echo show, but I have three.
[01:00:45.460 --> 01:00:47.220]   It's because this is my job.
[01:00:47.220 --> 01:00:48.700]   That's my story and I'm sticking with it.
[01:00:48.700 --> 01:00:50.460]   Yeah, I know, I know.
[01:00:50.460 --> 01:00:51.780]   Sorry, I dropped my battery.
[01:00:51.780 --> 01:00:54.300]   I'm trying to figure out this is a bit of a puzzle.
[01:00:54.300 --> 01:00:55.860]   You pull it apart.
[01:00:55.860 --> 01:00:57.580]   I don't want to break it.
[01:00:57.580 --> 01:01:00.420]   I have the same thing, but you do pull it apart.
[01:01:00.420 --> 01:01:01.420]   Just like this?
[01:01:01.420 --> 01:01:02.420]   Yeah, right before the shot at.
[01:01:02.420 --> 01:01:04.220]   Like a party cracker?
[01:01:04.220 --> 01:01:05.220]   Yeah.
[01:01:05.220 --> 01:01:06.620]   It's quite hard to pull apart.
[01:01:06.620 --> 01:01:07.620]   It's also a string.
[01:01:07.620 --> 01:01:09.100]   I know, do you want me to go get my?
[01:01:09.100 --> 01:01:10.340]   Oh, I did it.
[01:01:10.340 --> 01:01:11.660]   I felt like I might break it.
[01:01:11.660 --> 01:01:15.660]   I wasn't, you know, and of course the last thing I'm going to do is look at a manual.
[01:01:15.660 --> 01:01:18.260]   All right, so put the batteries in.
[01:01:18.260 --> 01:01:20.420]   So this is the Amazon wall.
[01:01:20.420 --> 01:01:21.900]   Fashioned batteries.
[01:01:21.900 --> 01:01:22.900]   Triple A's.
[01:01:22.900 --> 01:01:30.140]   And I use rechargeables, so when I replace them, I'll have rechargeables.
[01:01:30.140 --> 01:01:36.100]   Problem with rechargeables is they tend to over time leak, which means you just lose
[01:01:36.100 --> 01:01:37.260]   the power if you don't use it.
[01:01:37.260 --> 01:01:39.460]   All right, so now what do I have to set it up?
[01:01:39.460 --> 01:01:40.460]   There's no screen.
[01:01:40.460 --> 01:01:41.620]   There's no screen.
[01:01:41.620 --> 01:01:42.780]   You need to get your phone.
[01:01:42.780 --> 01:01:46.300]   Oh, I set it up by my echo app.
[01:01:46.300 --> 01:01:49.300]   You go to www.amazon.com/dashwander.
[01:01:49.300 --> 01:01:53.260]   Oh, you have to do it by the web.
[01:01:53.260 --> 01:01:54.260]   Uh-huh.
[01:01:54.260 --> 01:01:55.700]   I can't do it with the echo apps.
[01:01:55.700 --> 01:01:57.420]   I bet you could do it by the see.
[01:01:57.420 --> 01:02:00.020]   I read the manual because there was your mistake.
[01:02:00.020 --> 01:02:02.420]   The Stacy's that kind of good.
[01:02:02.420 --> 01:02:03.420]   That was your mistake.
[01:02:03.420 --> 01:02:06.940]   So actually, most of the times I don't read the manual, but once I couldn't figure out
[01:02:06.940 --> 01:02:07.940]   how to put the batteries in.
[01:02:07.940 --> 01:02:08.940]   That's exactly.
[01:02:08.940 --> 01:02:10.540]   I was about to get the manual out.
[01:02:10.540 --> 01:02:15.180]   I'm saying I clearly need a manual here because it wasn't obvious.
[01:02:15.180 --> 01:02:17.340]   All right, so can I add a device?
[01:02:17.340 --> 01:02:18.340]   Set up a new device.
[01:02:18.340 --> 01:02:20.300]   Uh, yeah, you're right.
[01:02:20.300 --> 01:02:23.380]   You have to go to the web because they have echo tap and echo dot.
[01:02:23.380 --> 01:02:28.980]   They don't have any of the choose a device so you can't choose the wand.
[01:02:28.980 --> 01:02:32.340]   So do you want me to tell you that this is going to suck and you're going to hate it?
[01:02:32.340 --> 01:02:33.340]   Or do you want me to?
[01:02:33.340 --> 01:02:37.980]   I think I already know because I played with Megan Maroney's.
[01:02:37.980 --> 01:02:38.980]   It's terrible.
[01:02:38.980 --> 01:02:42.140]   Well, it's just it just hooks you into Amazon.
[01:02:42.140 --> 01:02:43.140]   Well, that's all right.
[01:02:43.140 --> 01:02:44.140]   That's too late.
[01:02:44.140 --> 01:02:45.140]   I've already.
[01:02:45.140 --> 01:02:48.740]   Well, I'm going to send it to Amazon, but like I didn't realize I wanted to be able to
[01:02:48.740 --> 01:02:54.220]   export stuff to a generic list for things I didn't want for Amazon, but I don't see
[01:02:54.220 --> 01:02:56.700]   how that's possible.
[01:02:56.700 --> 01:02:58.660]   And that saddens me.
[01:02:58.660 --> 01:03:01.220]   US dash one set up.
[01:03:01.220 --> 01:03:02.220]   Okay.
[01:03:02.220 --> 01:03:03.220]   Good.
[01:03:03.220 --> 01:03:07.140]   If you do the UCA, maybe you'll get cool spellings for all your words.
[01:03:07.140 --> 01:03:09.020]   And your arugula would be called rocket.
[01:03:09.020 --> 01:03:10.020]   Hello.
[01:03:10.020 --> 01:03:11.740]   Would you like some coriander?
[01:03:11.740 --> 01:03:14.740]   Uh, well, okay.
[01:03:14.740 --> 01:03:15.740]   Okay.
[01:03:15.740 --> 01:03:16.740]   Insert batteries.
[01:03:16.740 --> 01:03:17.740]   It shows you.
[01:03:17.740 --> 01:03:18.740]   That's good.
[01:03:18.740 --> 01:03:19.740]   How?
[01:03:19.740 --> 01:03:21.940]   So what don't you like about this?
[01:03:21.940 --> 01:03:28.040]   I just, I thought I'd be able to get, I thought I could use it as a generic grocery
[01:03:28.040 --> 01:03:30.740]   list generator to an app, but it really everything.
[01:03:30.740 --> 01:03:31.940]   Oh, it's only to Amazon.
[01:03:31.940 --> 01:03:32.940]   Yeah.
[01:03:32.940 --> 01:03:33.940]   It's only to Amazon.
[01:03:33.940 --> 01:03:39.140]   And I, so now I've got to still think before I scan something.
[01:03:39.140 --> 01:03:42.820]   Is this something I usually buy on Amazon or not?
[01:03:42.820 --> 01:03:44.700]   So it doesn't, it doesn't make my life as effortless.
[01:03:44.700 --> 01:03:47.380]   As I wanted it to be.
[01:03:47.380 --> 01:03:50.580]   Do you have it magnetically attached to your refrigerator?
[01:03:50.580 --> 01:03:51.580]   That's what you're supposed to do.
[01:03:51.580 --> 01:03:52.580]   Yeah.
[01:03:52.580 --> 01:03:56.540]   Well, but most of it also comes with a convenient hook you can attach to the wall.
[01:03:56.540 --> 01:03:58.060]   Is that hideously ugly hook?
[01:03:58.060 --> 01:04:00.060]   I would never want that on my wall.
[01:04:00.060 --> 01:04:03.500]   I was like, oh, it's just a hook.
[01:04:03.500 --> 01:04:08.060]   What do you mean hideously ugly hook is a hook is a little style to your hook?
[01:04:08.060 --> 01:04:10.220]   She has high hook standards.
[01:04:10.220 --> 01:04:11.220]   Apparently.
[01:04:11.220 --> 01:04:14.980]   All right, so, and then you're supposed to hold it to the ring turns orange, which it
[01:04:14.980 --> 01:04:16.260]   was stubbornly refusing to do.
[01:04:16.260 --> 01:04:18.260]   Maybe I put the batteries in wrong.
[01:04:18.260 --> 01:04:21.340]   They both go the same direction, which is unusual.
[01:04:21.340 --> 01:04:23.900]   It made noise.
[01:04:23.900 --> 01:04:30.340]   You know, if you're lucky enough to have Stacy on IoT helping you install your IoT device.
[01:04:30.340 --> 01:04:32.700]   I'm clearly not helping you enough.
[01:04:32.700 --> 01:04:37.260]   I'll see for what, you know, I kind of with you because I want to try Instacart before
[01:04:37.260 --> 01:04:38.260]   they go out of business.
[01:04:38.260 --> 01:04:40.460]   Oh, we ordered Instacart yesterday.
[01:04:40.460 --> 01:04:41.460]   It was great.
[01:04:41.460 --> 01:04:42.460]   I think it's great.
[01:04:42.460 --> 01:04:45.660]   I've been told by a number of people, because they're my son who's an Instacart user that
[01:04:45.660 --> 01:04:47.780]   it really, he, I said, do you get good produce?
[01:04:47.780 --> 01:04:49.780]   He said, yeah, everything works really well.
[01:04:49.780 --> 01:04:51.220]   Yeah, we got some produce.
[01:04:51.220 --> 01:04:57.140]   We got six bags of ice too, which was nice because our party, you know, all that see
[01:04:57.140 --> 01:04:58.140]   that.
[01:04:58.140 --> 01:04:59.140]   That's not it.
[01:04:59.140 --> 01:05:00.300]   So we got six bags of ice.
[01:05:00.300 --> 01:05:03.060]   We got some more brownie bites because they're delicious.
[01:05:03.060 --> 01:05:06.740]   Oh, so they went to Costco as well as they'll go to somewhere.
[01:05:06.740 --> 01:05:07.740]   They went to it's not just.
[01:05:07.740 --> 01:05:08.740]   It's not just.
[01:05:08.740 --> 01:05:09.740]   Okay.
[01:05:09.740 --> 01:05:10.740]   It's not just Whole Foods.
[01:05:10.740 --> 01:05:12.020]   Well, Instacart is HEB.
[01:05:12.020 --> 01:05:15.860]   He also told us they go to like pet stores for pet food.
[01:05:15.860 --> 01:05:19.980]   They'll go to CVS for like, if you're kid sick and, you know, you're a spouse is out
[01:05:19.980 --> 01:05:22.940]   of town or something, you can be like, help me.
[01:05:22.940 --> 01:05:25.940]   I need eight gallons of PDL like stat.
[01:05:25.940 --> 01:05:27.660]   Exactly.
[01:05:27.660 --> 01:05:31.900]   We always have PDL light in our pantry, just in case.
[01:05:31.900 --> 01:05:32.900]   Wow.
[01:05:32.900 --> 01:05:35.980]   That's expensive and nasty, nasty stuff.
[01:05:35.980 --> 01:05:36.980]   Yeah, we got it.
[01:05:36.980 --> 01:05:37.980]   Okay.
[01:05:37.980 --> 01:05:38.980]   Now, select your Wi-Fi network.
[01:05:38.980 --> 01:05:39.980]   Okay.
[01:05:39.980 --> 01:05:40.980]   Okay.
[01:05:40.980 --> 01:05:45.780]   And the idea is you're going to you're going to scan when you run out of stuff, you put
[01:05:45.780 --> 01:05:47.900]   it in the other refrigerator, right?
[01:05:47.900 --> 01:05:49.460]   Yeah.
[01:05:49.460 --> 01:05:50.460]   You can.
[01:05:50.460 --> 01:05:52.740]   I put mine on the refrigerator.
[01:05:52.740 --> 01:05:53.740]   Right.
[01:05:53.740 --> 01:05:57.940]   Because the idea is that you scan stuff like when you're when you're done, you're out
[01:05:57.940 --> 01:06:02.180]   of milk before you throw out the cart and you scan it.
[01:06:02.180 --> 01:06:04.380]   Is that kind of the whole premise?
[01:06:04.380 --> 01:06:07.380]   And it has a little tiny speaker and can talk like it could do all the other.
[01:06:07.380 --> 01:06:08.380]   Oh, good.
[01:06:08.380 --> 01:06:09.380]   Right.
[01:06:09.380 --> 01:06:12.220]   That means you can ask it what the weather is.
[01:06:12.220 --> 01:06:13.420]   You can.
[01:06:13.420 --> 01:06:14.420]   But you, why would you?
[01:06:14.420 --> 01:06:18.060]   You know, I have already have it echo and well, but what can you use only that?
[01:06:18.060 --> 01:06:20.060]   Can I just use that and nothing else?
[01:06:20.060 --> 01:06:21.060]   The Alexa family?
[01:06:21.060 --> 01:06:22.060]   Yeah.
[01:06:22.060 --> 01:06:23.060]   It's just got accepted.
[01:06:23.060 --> 01:06:24.780]   It doesn't play music.
[01:06:24.780 --> 01:06:25.780]   And thank you, Jack.
[01:06:25.780 --> 01:06:26.780]   That is what okay.
[01:06:26.780 --> 01:06:27.780]   Beep this.
[01:06:27.780 --> 01:06:28.780]   Everybody hold your ears.
[01:06:28.780 --> 01:06:29.780]   What's going on?
[01:06:29.780 --> 01:06:30.780]   What's the weather?
[01:06:30.780 --> 01:06:32.780]   Oh, I have to push the button.
[01:06:32.780 --> 01:06:34.780]   What's going on?
[01:06:34.780 --> 01:06:35.780]   What's the weather?
[01:06:35.780 --> 01:06:37.180]   So that's one different thing is not it's not.
[01:06:37.180 --> 01:06:38.180]   It's a Rosa.
[01:06:38.180 --> 01:06:40.540]   It's 79 degrees with mostly sunny skies.
[01:06:40.540 --> 01:06:41.540]   It's like a.
[01:06:41.540 --> 01:06:43.540]   You can look for intermittent clouds.
[01:06:43.540 --> 01:06:46.940]   It's like a real high of 79 degrees and a low of 50 degrees.
[01:06:46.940 --> 01:06:48.940]   It's like a really bad AM radio.
[01:06:48.940 --> 01:06:49.940]   It's really.
[01:06:49.940 --> 01:06:52.700]   Well, I mean, you press it because of battery life.
[01:06:52.700 --> 01:06:53.700]   Yeah.
[01:06:53.700 --> 01:06:54.700]   Yeah.
[01:06:54.700 --> 01:06:55.700]   And do you press to scan?
[01:06:55.700 --> 01:06:57.340]   It doesn't automatically see stuff, right?
[01:06:57.340 --> 01:06:59.340]   And you press to scan.
[01:06:59.340 --> 01:07:00.340]   Okay.
[01:07:00.340 --> 01:07:03.860]   Although for all I know, there's a giant camera in there and Jeff Bezos checks in every now
[01:07:03.860 --> 01:07:04.860]   and then.
[01:07:04.860 --> 01:07:05.860]   Hi, Jeff.
[01:07:05.860 --> 01:07:08.100]   Does it tell you other thing?
[01:07:08.100 --> 01:07:09.100]   Does it?
[01:07:09.100 --> 01:07:12.100]   Can you scan something and ask what ingredients are in it or how many calories or any of that
[01:07:12.100 --> 01:07:13.100]   kind of stuff?
[01:07:13.100 --> 01:07:14.100]   That would be really interesting.
[01:07:14.100 --> 01:07:15.100]   What is that?
[01:07:15.100 --> 01:07:17.700]   Somebody told me it's Dinty more hardy beef stew.
[01:07:17.700 --> 01:07:18.700]   Don't you?
[01:07:18.700 --> 01:07:20.820]   So I just pointed to the barcode.
[01:07:20.820 --> 01:07:21.820]   That's it.
[01:07:21.820 --> 01:07:24.060]   I don't think you could get that at Whole Foods.
[01:07:24.060 --> 01:07:26.380]   Oh, okay.
[01:07:26.380 --> 01:07:28.460]   Problem one, it doesn't tell you what it's scanned.
[01:07:28.460 --> 01:07:32.180]   Oh, yeah, you got to it's on your Amazon list.
[01:07:32.180 --> 01:07:35.620]   Oh, it should say I think that's Dinty more beef stew.
[01:07:35.620 --> 01:07:36.620]   Yeah, I'm struggling.
[01:07:36.620 --> 01:07:38.500]   I'm not right now.
[01:07:38.500 --> 01:07:39.500]   That's dopey.
[01:07:39.500 --> 01:07:41.060]   You don't know.
[01:07:41.060 --> 01:07:43.380]   That's one of the things Megan complained about.
[01:07:43.380 --> 01:07:52.300]   So you need to go to your Amazon shopping cart, which I don't even know where that is.
[01:07:52.300 --> 01:07:53.580]   Plus there's stupid stuff.
[01:07:53.580 --> 01:07:54.580]   Oh, yeah.
[01:07:54.580 --> 01:07:55.580]   Look.
[01:07:55.580 --> 01:07:58.660]   Dinty more beef stew scanned with dash.
[01:07:58.660 --> 01:07:59.660]   It did see it.
[01:07:59.660 --> 01:08:01.420]   Look at that.
[01:08:01.420 --> 01:08:05.980]   And I think you can tell it to buy stuff and it will actually buy it.
[01:08:05.980 --> 01:08:09.700]   Oh, wait a minute.
[01:08:09.700 --> 01:08:11.500]   Can I just point out what it's going to order?
[01:08:11.500 --> 01:08:14.460]   If I go ahead and order is a pack of 12.
[01:08:14.460 --> 01:08:17.460]   It's a lot of beef stew.
[01:08:17.460 --> 01:08:18.460]   Oh, oh, oh.
[01:08:18.460 --> 01:08:20.300]   That's a lot of beef stew.
[01:08:20.300 --> 01:08:23.220]   Usually ships within one to two months.
[01:08:23.220 --> 01:08:28.660]   So in September, I'm going to get, for reasons I really don't understand, a carton of 12
[01:08:28.660 --> 01:08:30.660]   did he more beef stews?
[01:08:30.660 --> 01:08:35.180]   Yeah, I would maybe not good.
[01:08:35.180 --> 01:08:37.420]   Maybe don't hit by.
[01:08:37.420 --> 01:08:39.420]   So and but that's the other bad thing is it is.
[01:08:39.420 --> 01:08:40.980]   Well, I guess that's good in this case.
[01:08:40.980 --> 01:08:42.180]   It's not going to do anything really.
[01:08:42.180 --> 01:08:43.420]   It just makes a list.
[01:08:43.420 --> 01:08:44.420]   Yeah.
[01:08:44.420 --> 01:08:50.420]   Well, no, if you say by when, not if you say by and scan, but if you say by, oh, you
[01:08:50.420 --> 01:08:53.980]   can still do that with you, which you would do with a regular one, right?
[01:08:53.980 --> 01:08:54.980]   Yes.
[01:08:54.980 --> 01:08:55.980]   Yeah.
[01:08:55.980 --> 01:08:58.980]   Underwear.
[01:08:58.980 --> 01:09:00.580]   I find anything for underwear.
[01:09:00.580 --> 01:09:03.660]   So I've added underwear to the shopping list.
[01:09:03.660 --> 01:09:06.060]   So it's just like a regular echo in that respect.
[01:09:06.060 --> 01:09:07.300]   Yes.
[01:09:07.300 --> 01:09:08.300]   So what does this do?
[01:09:08.300 --> 01:09:10.060]   I do a pile of pens.
[01:09:10.060 --> 01:09:16.300]   Amazon's choice for pilot pens is pilot precise V seven black ball pens.
[01:09:16.300 --> 01:09:20.060]   Doesn't box it's $12 and 16 cents plus tax.
[01:09:20.060 --> 01:09:21.740]   Would you like to buy it?
[01:09:21.740 --> 01:09:22.740]   Yes.
[01:09:22.740 --> 01:09:26.180]   Oh, I have to press the button.
[01:09:26.180 --> 01:09:28.020]   I have to press.
[01:09:28.020 --> 01:09:29.020]   Yes.
[01:09:29.020 --> 01:09:35.140]   Does that ding mean I bought it?
[01:09:35.140 --> 01:09:36.860]   Look at your Amazon app.
[01:09:36.860 --> 01:09:39.460]   Plus what did it what did it show for underwear?
[01:09:39.460 --> 01:09:40.460]   That's what I want to see.
[01:09:40.460 --> 01:09:41.900]   You put in sample underwear.
[01:09:41.900 --> 01:09:48.740]   Well, the last time last time I did this, it said it bought me some emergency underwear.
[01:09:48.740 --> 01:09:49.740]   I'm not going to see.
[01:09:49.740 --> 01:09:51.380]   Emergency underwear comes in handy.
[01:09:51.380 --> 01:09:59.860]   Well, you know, I now have a I have a it's just grab and go emergency underpants dispenser.
[01:09:59.860 --> 01:10:03.540]   And I but it's actually a great gag just to have around.
[01:10:03.540 --> 01:10:04.940]   So I just leave it lying around.
[01:10:04.940 --> 01:10:09.420]   But it bought me that by accident.
[01:10:09.420 --> 01:10:12.660]   So there's other kinds you can get instantly.
[01:10:12.660 --> 01:10:14.500]   Wait, wait, wait, wait, scroll down instant underpants.
[01:10:14.500 --> 01:10:17.500]   Yeah, just add water.
[01:10:17.500 --> 01:10:18.740]   Is it like a washcloth?
[01:10:18.740 --> 01:10:19.740]   One of those like instant wash.
[01:10:19.740 --> 01:10:20.740]   Oh, yeah.
[01:10:20.740 --> 01:10:25.100]   These underpants are conveniently compressed into a compact pellet.
[01:10:25.100 --> 01:10:27.540]   Just so that water you can't wear underpants.
[01:10:27.540 --> 01:10:34.380]   It says remember it's better to have damp underpants than no underpants at all.
[01:10:34.380 --> 01:10:37.100]   Oh, here's the bad.
[01:10:37.100 --> 01:10:39.020]   Here's the bad news right at the bottom fine print.
[01:10:39.020 --> 01:10:41.900]   Fence fits most children and small adults.
[01:10:41.900 --> 01:10:43.700]   I am not a small adult.
[01:10:43.700 --> 01:10:47.220]   So I guess this is just not your SLO.
[01:10:47.220 --> 01:10:52.820]   By the way, frequently bought together, Archie McPhee instant underpants just add water.
[01:10:52.820 --> 01:10:58.900]   A Kucharmont's Yodeling pickle and a bag of unicorn fart cotton candy.
[01:10:58.900 --> 01:11:00.980]   I set my friend to Yodeling pickle.
[01:11:00.980 --> 01:11:01.980]   It was awesome.
[01:11:01.980 --> 01:11:03.980]   What is the Yodeling pickle?
[01:11:03.980 --> 01:11:05.900]   Oh, you never see this?
[01:11:05.900 --> 01:11:07.140]   You have to have one.
[01:11:07.140 --> 01:11:08.900]   You press the button and it yodels.
[01:11:08.900 --> 01:11:10.300]   This is exactly look at it.
[01:11:10.300 --> 01:11:12.660]   It even looks like the Amazon wand.
[01:11:12.660 --> 01:11:18.540]   I think this is clearly the inspiration.
[01:11:18.540 --> 01:11:19.540]   Look at this Carson.
[01:11:19.540 --> 01:11:20.540]   Show my screen.
[01:11:20.540 --> 01:11:23.260]   I mean, show the show show the over the shoulder shot.
[01:11:23.260 --> 01:11:29.380]   I think this is clearly the inspiration for the one.
[01:11:29.380 --> 01:11:33.460]   Amazon I Jeff must have said, you know that Yodeling pickle really sells well.
[01:11:33.460 --> 01:11:36.620]   Let's make a Yodeling pickle echo.
[01:11:36.620 --> 01:11:40.460]   But the echo doesn't ask her to Yodel.
[01:11:40.460 --> 01:11:42.500]   Let's see if she'll Yodel for you.
[01:11:42.500 --> 01:11:43.500]   Yodel.
[01:11:43.500 --> 01:11:47.220]   You may have to say.
[01:11:47.220 --> 01:11:54.100]   You just bought some Yodeling pickles.
[01:11:54.100 --> 01:11:58.220]   Beatbox.
[01:11:58.220 --> 01:12:00.060]   You may have to say her name.
[01:12:00.060 --> 01:12:02.180]   I have to press the button and say her name.
[01:12:02.180 --> 01:12:03.500]   Well, I don't know.
[01:12:03.500 --> 01:12:04.500]   That's annoying.
[01:12:04.500 --> 01:12:06.340]   Because you might be adding that to the shot.
[01:12:06.340 --> 01:12:09.740]   It might be because that is a stopping request.
[01:12:09.740 --> 01:12:12.260]   Do you press and hold?
[01:12:12.260 --> 01:12:14.940]   I pressed and hold it held.
[01:12:14.940 --> 01:12:16.900]   Yodel.
[01:12:16.900 --> 01:12:25.100]   There's one by Yodeling pickle.
[01:12:25.100 --> 01:12:29.180]   What the Yodeling pickle button.
[01:12:29.180 --> 01:12:31.860]   Oh, Lord.
[01:12:31.860 --> 01:12:34.100]   Hours of mindless entertainment.
[01:12:34.100 --> 01:12:37.500]   It makes a great gag gift.
[01:12:37.500 --> 01:12:39.940]   I'm just very emphatic about my Yodeling pickle.
[01:12:39.940 --> 01:12:43.580]   I think it would be good for one of those white elephant parties.
[01:12:43.580 --> 01:12:44.580]   Yes.
[01:12:44.580 --> 01:12:47.660]   Yes, I may have purchased it for that too.
[01:12:47.660 --> 01:12:49.700]   Yes, those are very useful for that.
[01:12:49.700 --> 01:12:51.780]   OK, here's-- I should read the manual.
[01:12:51.780 --> 01:12:53.540]   Things to try.
[01:12:53.540 --> 01:12:58.180]   To add an item to your cart, say dark roast coffee or scan the item's barcode.
[01:12:58.180 --> 01:13:02.500]   To buy now, say buy in the thing.
[01:13:02.500 --> 01:13:05.220]   To track your order-- oh, here, let's try this.
[01:13:05.220 --> 01:13:07.220]   Where's my stuff?
[01:13:07.220 --> 01:13:14.660]   A shipment for Leo's order placed yesterday should arrive tomorrow.
[01:13:14.660 --> 01:13:19.140]   When it's in the Yodeling pickle, how many tablespoons?
[01:13:19.140 --> 01:13:20.700]   So you can do all the normal things.
[01:13:20.700 --> 01:13:21.700]   You're going to stick a butter.
[01:13:21.700 --> 01:13:24.300]   How many calories are in an apple?
[01:13:24.300 --> 01:13:27.940]   You could do the dim hue lights thing, calendar, sports.
[01:13:27.940 --> 01:13:29.100]   Well, this were Google.
[01:13:29.100 --> 01:13:30.740]   You would point to the dimmer's dew.
[01:13:30.740 --> 01:13:32.980]   You would say how many calories are in that?
[01:13:32.980 --> 01:13:33.980]   I know.
[01:13:33.980 --> 01:13:36.500]   They could really do much better.
[01:13:36.500 --> 01:13:42.460]   This is a promising start, but they really need to do more with this, I think.
[01:13:42.460 --> 01:13:44.700]   It's a yes.
[01:13:44.700 --> 01:13:46.420]   That was your reaction, too.
[01:13:46.420 --> 01:13:52.540]   Yes, although for controlling smart home stuff, I could put this in my guest room, for example,
[01:13:52.540 --> 01:13:57.020]   without spending a lot of money because it won't prime music for people, but they could
[01:13:57.020 --> 01:13:58.020]   be like--
[01:13:58.020 --> 01:14:00.140]   It's only $20, we should point out.
[01:14:00.140 --> 01:14:02.620]   This is the least expensive echo you can buy.
[01:14:02.620 --> 01:14:07.980]   Actually, if they did more like controlled the TV, this could be a great remote control.
[01:14:07.980 --> 01:14:12.140]   There's all sorts of things this could do if they would add some features to it.
[01:14:12.140 --> 01:14:13.140]   And Yodeling--
[01:14:13.140 --> 01:14:14.140]   That's $20.
[01:14:14.140 --> 01:14:15.140]   That's $20.
[01:14:15.140 --> 01:14:16.140]   That's great.
[01:14:16.140 --> 01:14:18.900]   I know, but if they had a feature, it probably wouldn't cost $20.
[01:14:18.900 --> 01:14:19.900]   Not necessarily.
[01:14:19.900 --> 01:14:22.060]   I mean, they just add echo features to it.
[01:14:22.060 --> 01:14:24.020]   They just design it.
[01:14:24.020 --> 01:14:27.060]   Somebody said this, and I think this is kind of true that maybe both this and the show
[01:14:27.060 --> 01:14:30.140]   were rushed a little bit.
[01:14:30.140 --> 01:14:37.060]   For instance, if you use the Echo app on Android, it's terrible for setting up the
[01:14:37.060 --> 01:14:39.420]   show.
[01:14:39.420 --> 01:14:45.340]   It looks like it has all the back end, but nobody did front end design on it.
[01:14:45.340 --> 01:14:47.460]   But on the iPhone, it looks fine.
[01:14:47.460 --> 01:14:48.460]   Looks good.
[01:14:48.460 --> 01:14:49.460]   Things like that.
[01:14:49.460 --> 01:14:57.940]   It feels like they just kind of had to rush it out the door.
[01:14:57.940 --> 01:14:58.940]   What else should we talk about?
[01:14:58.940 --> 01:15:02.100]   Oh, well, I think we need to talk about the Brits.
[01:15:02.100 --> 01:15:03.100]   The Brits?
[01:15:03.100 --> 01:15:04.940]   Are you talking about NHS?
[01:15:04.940 --> 01:15:05.940]   Yep.
[01:15:05.940 --> 01:15:06.940]   In DeepMind?
[01:15:06.940 --> 01:15:07.940]   In DeepMind.
[01:15:07.940 --> 01:15:08.940]   Yep.
[01:15:08.940 --> 01:15:09.940]   So it is illegal.
[01:15:09.940 --> 01:15:11.740]   It's illegal.
[01:15:11.740 --> 01:15:16.900]   Google has a partnership with the National Health Service in the UK, and it is illegal
[01:15:16.900 --> 01:15:18.940]   according today.
[01:15:18.940 --> 01:15:25.180]   So here it is 1.6 million patient records are shared with Google in an attempt to use
[01:15:25.180 --> 01:15:29.820]   AI to predict which patients would be risk from kidney damage.
[01:15:29.820 --> 01:15:35.580]   It was the ICO, which is the Information Commissioner's Office, the UK privacy body,
[01:15:35.580 --> 01:15:38.420]   has ruled that it was illegal.
[01:15:38.420 --> 01:15:39.940]   Today my office, this is the quote.
[01:15:39.940 --> 01:15:45.420]   Today my office has announced that the Royal Free NHS Foundation Trust did not comply with
[01:15:45.420 --> 01:15:50.820]   the Data Protection Act when it turned over the sensitive medical data of around 1.6
[01:15:50.820 --> 01:15:51.820]   million patients.
[01:15:51.820 --> 01:15:53.620]   Was it anonymized information or no?
[01:15:53.620 --> 01:15:54.620]   Yeah, of course it was.
[01:15:54.620 --> 01:15:56.700]   But here you go, the next two paragraphs down.
[01:15:56.700 --> 01:16:00.420]   The law says that patients are implied to have consented to data being shared for the
[01:16:00.420 --> 01:16:02.580]   purposes of their direct care.
[01:16:02.580 --> 01:16:06.780]   But as the aim here was to develop an app that would help future patients, no consent
[01:16:06.780 --> 01:16:08.860]   could be assumed.
[01:16:08.860 --> 01:16:10.620]   Isn't that just ludicrous?
[01:16:10.620 --> 01:16:15.340]   Now, in some small fairness, they said that there are ways they could have done it to protect
[01:16:15.340 --> 01:16:16.740]   privacy and so on.
[01:16:16.740 --> 01:16:18.300]   But it just shows the priority.
[01:16:18.300 --> 01:16:21.140]   The techno panic beats helping people.
[01:16:21.140 --> 01:16:22.140]   Well, hold it.
[01:16:22.140 --> 01:16:23.140]   Hold on.
[01:16:23.140 --> 01:16:28.660]   What if it shows that, hey, we're doing totally new stuff, let's have a conversation about
[01:16:28.660 --> 01:16:29.660]   that.
[01:16:29.660 --> 01:16:32.220]   We kind of agree with you.
[01:16:32.220 --> 01:16:33.900]   I mean, we kind of talked about this last week.
[01:16:33.900 --> 01:16:37.780]   Was it last week with regulation in this era?
[01:16:37.780 --> 01:16:43.060]   We're doing a lot of crazy stuff to say, hey, let's figure, let's just talk through this
[01:16:43.060 --> 01:16:44.540]   for a little bit as an IRB.
[01:16:44.540 --> 01:16:45.540]   Well, I'm sure they did.
[01:16:45.540 --> 01:16:47.100]   No, they didn't understand that.
[01:16:47.100 --> 01:16:48.900]   I think they did.
[01:16:48.900 --> 01:16:51.700]   And they did an effort to do this in a way that passed the law.
[01:16:51.700 --> 01:16:55.380]   And then the NHS comes back and it was the NHS, not Google, that was found to be at fault
[01:16:55.380 --> 01:16:56.380]   here.
[01:16:56.380 --> 01:16:59.340]   But it shows the priority that we have.
[01:16:59.340 --> 01:17:02.060]   You put privacy over lives.
[01:17:02.060 --> 01:17:03.060]   That's where we are right now.
[01:17:03.060 --> 01:17:04.060]   And this is dangerous.
[01:17:04.060 --> 01:17:10.140]   This is highly dangerous because it's insane that we're going to control knowledge.
[01:17:10.140 --> 01:17:13.580]   So privacy can also affect lives.
[01:17:13.580 --> 01:17:18.300]   I don't think they're saying that privacy is better than saving people's lives.
[01:17:18.300 --> 01:17:22.540]   They're saying privacy has a cost that can ruin people's lives.
[01:17:22.540 --> 01:17:23.980]   And I don't think that's wrong.
[01:17:23.980 --> 01:17:25.540]   But there's no privacy at all.
[01:17:25.540 --> 01:17:26.540]   This is about consent.
[01:17:26.540 --> 01:17:28.180]   There's no privacy about consent.
[01:17:28.180 --> 01:17:29.180]   But wait a minute.
[01:17:29.180 --> 01:17:32.260]   You might be making it more global than it really is.
[01:17:32.260 --> 01:17:38.260]   It might legitimately, first of all, we know that anonymization is not very effective, right?
[01:17:38.260 --> 01:17:45.180]   And so I think it's not unreasonable for the ICO to say, you know, as you did it, it's
[01:17:45.180 --> 01:17:52.420]   not legal because in those records, privatized, anonymized may not be truly anonymous.
[01:17:52.420 --> 01:17:54.260]   That's not the one.
[01:17:54.260 --> 01:17:57.980]   And as a result, you need, well, but you need more extensive, you need permission to do
[01:17:57.980 --> 01:18:03.580]   this particular, what it was was you need permission to do this particular data sharing.
[01:18:03.580 --> 01:18:04.580]   I agree.
[01:18:04.580 --> 01:18:08.340]   They probably shouldn't have brought up this future versus existing patient thing.
[01:18:08.340 --> 01:18:10.140]   But that is what they do.
[01:18:10.140 --> 01:18:12.540]   But their job is to protect privacy.
[01:18:12.540 --> 01:18:18.580]   But well, but doctors' jobs is to protect lives and to save lives.
[01:18:18.580 --> 01:18:19.900]   And this time, it's kidney damage.
[01:18:19.900 --> 01:18:22.700]   It could be other things that are highly fatal.
[01:18:22.700 --> 01:18:26.900]   And so this was a use that could not have been anticipated.
[01:18:26.900 --> 01:18:31.540]   The deep mind existed and AI could help predict kidney disease.
[01:18:31.540 --> 01:18:34.500]   But what if Stacey's couldn't have predicted that?
[01:18:34.500 --> 01:18:39.300]   So you're saying to all the past patients and all the past doctors and all the past bureaucrats
[01:18:39.300 --> 01:18:41.180]   should have thought about, sorry, it didn't.
[01:18:41.180 --> 01:18:42.180]   So we can't use that.
[01:18:42.180 --> 01:18:44.540]   No, but they'll just let some people get kidney diseases.
[01:18:44.540 --> 01:18:45.540]   Let's have the discussion.
[01:18:45.540 --> 01:18:49.100]   That's what Stacey's saying is that we can go back to the table and have that discussion
[01:18:49.100 --> 01:18:50.540]   and ask for permission.
[01:18:50.540 --> 01:18:52.920]   No, those are patients from the past.
[01:18:52.920 --> 01:18:55.140]   You can't go back and ask them permission over.
[01:18:55.140 --> 01:18:59.580]   So you've lost that data now and you said to people, you better go ahead, get kidney
[01:18:59.580 --> 01:19:00.580]   disease.
[01:19:00.580 --> 01:19:07.460]   So all the legal fine point wasn't done here for use that couldn't have been anticipated.
[01:19:07.460 --> 01:19:12.100]   This is the problem I have with the regulatory reflex, especially in Europe, where they think
[01:19:12.100 --> 01:19:13.420]   they know what the internet is.
[01:19:13.420 --> 01:19:15.380]   They think what they know what technology can do.
[01:19:15.380 --> 01:19:20.260]   And they want to constantly limit it to the past rather than anticipate the future and
[01:19:20.260 --> 01:19:21.260]   what can happen.
[01:19:21.260 --> 01:19:22.620]   And is there some risk?
[01:19:22.620 --> 01:19:29.060]   Yeah, but the risk to me is clearly greater that people will get now could get diseases
[01:19:29.060 --> 01:19:31.220]   that could have been prevented and treated.
[01:19:31.220 --> 01:19:36.580]   That is horrendous as the prioritization we're putting on society because of this techno
[01:19:36.580 --> 01:19:37.580]   panic.
[01:19:37.580 --> 01:19:41.820]   To be fair, we don't know that this would actually save lives.
[01:19:41.820 --> 01:19:43.820]   Yeah, Mr. Z.
[01:19:43.820 --> 01:19:44.820]   Assuming that it will.
[01:19:44.820 --> 01:19:48.740]   Stacey, Stacey, so make it breast cancer.
[01:19:48.740 --> 01:19:54.580]   I understand, but we're not sure, A, if we take this data and I'm just kind of falling
[01:19:54.580 --> 01:19:59.900]   down your arguments path, not necessarily where I'm thinking.
[01:19:59.900 --> 01:20:02.020]   I guess they want prevent disease.
[01:20:02.020 --> 01:20:03.020]   Make does it make right?
[01:20:03.020 --> 01:20:05.780]   So it makes diagnosing diseases easier.
[01:20:05.780 --> 01:20:07.780]   That's a good thing.
[01:20:07.780 --> 01:20:09.860]   Yes, it is.
[01:20:09.860 --> 01:20:13.700]   But when you do that using AI, hold on, Jeff, hold on, hold on.
[01:20:13.700 --> 01:20:18.220]   When you do that with AI, you're going to have to test that.
[01:20:18.220 --> 01:20:23.940]   And you're going to have to do peer reviewed studies to make sure the AI is actually doing
[01:20:23.940 --> 01:20:28.980]   it correctly and finding what you want it to find as opposed to what it thinks you want
[01:20:28.980 --> 01:20:29.980]   to find.
[01:20:29.980 --> 01:20:32.620]   And we can bring up many examples of all of that.
[01:20:32.620 --> 01:20:42.620]   So you're rushing to saving lives, but there's still time to get consent on data and still
[01:20:42.620 --> 01:20:46.380]   do this because it's going to be a long process before actual lives are saved.
[01:20:46.380 --> 01:20:47.380]   That's life.
[01:20:47.380 --> 01:20:52.860]   The problem I have with this aggregation of data thing is that the fight is goes on about
[01:20:52.860 --> 01:20:55.940]   whether women should be whether and how women should be tested for breast cancer, when and
[01:20:55.940 --> 01:20:59.060]   how men should be tested for prostate cancer.
[01:20:59.060 --> 01:21:02.700]   And the problem is that that's a discussion held in aggregate.
[01:21:02.700 --> 01:21:04.420]   And well, the percentages don't go out much.
[01:21:04.420 --> 01:21:11.100]   But in that in that delta, if there's even one life, it's affected, we have set the priority.
[01:21:11.100 --> 01:21:16.220]   And the priority here is that we're going to prioritize techno panic over data is a bad
[01:21:16.220 --> 01:21:19.900]   thing over helping people's health.
[01:21:19.900 --> 01:21:25.820]   And the and the greater picture here is we've said knowledge is dangerous.
[01:21:25.820 --> 01:21:30.020]   The greater knowledge is here is how to better diagnose people's ailments.
[01:21:30.020 --> 01:21:35.780]   The sacrifice of some small risk here, which is not even defined is to my mind, well frigging
[01:21:35.780 --> 01:21:37.140]   worth it.
[01:21:37.140 --> 01:21:42.440]   And we've said and techno panic has won the day and that and it's won the day not only
[01:21:42.440 --> 01:21:46.700]   over health, but over knowledge over the value of knowledge.
[01:21:46.700 --> 01:21:50.460]   And I find that horrendously offensive in a modern world.
[01:21:50.460 --> 01:21:51.460]   Sorry.
[01:21:51.460 --> 01:21:59.860]   I think that consent to get someone's information is far more important than the pursuit of knowledge.
[01:21:59.860 --> 01:22:05.220]   Because if we give that up, we have so much information that could be mined at any point
[01:22:05.220 --> 01:22:06.900]   in time then.
[01:22:06.900 --> 01:22:09.660]   And I understand the art.
[01:22:09.660 --> 01:22:13.900]   Okay, but also for things that might not be beneficial.
[01:22:13.900 --> 01:22:17.940]   >> So you're going to cut off all the good possibilities because of bad things.
[01:22:17.940 --> 01:22:19.580]   >> No, we're not going to cut it off.
[01:22:19.580 --> 01:22:23.300]   >> We're going to have conversations about it.
[01:22:23.300 --> 01:22:27.300]   >> But in that time, in that time when you have the conversation, somebody's life is
[01:22:27.300 --> 01:22:28.300]   affected.
[01:22:28.300 --> 01:22:32.420]   Somebody's mother, somebody's father, somebody's uncle is affected by that kidney disease.
[01:22:32.420 --> 01:22:34.500]   And the conversation can go on forever.
[01:22:34.500 --> 01:22:40.220]   The priority is always going to get hurt while we someone will always get hurt.
[01:22:40.220 --> 01:22:42.900]   >> Or okay, which is the greater harm?
[01:22:42.900 --> 01:22:46.740]   Which is the greater harm that someone gets a disease they may not have to have versus
[01:22:46.740 --> 01:22:50.780]   somebody's data sits in a repository they didn't give consent to.
[01:22:50.780 --> 01:22:56.060]   >> Well, what if that data gets, what if the 20 million people whose data is in there
[01:22:56.060 --> 01:22:59.620]   gets reversed, they get docked and it gets hacked?
[01:22:59.620 --> 01:23:00.620]   >> And what if?
[01:23:00.620 --> 01:23:02.620]   >> And then yes, what person do you find out that people-
[01:23:02.620 --> 01:23:06.540]   >> So you find out that 20,000 people are, their lives are irrevocably changed by their
[01:23:06.540 --> 01:23:07.540]   data getting out there.
[01:23:07.540 --> 01:23:09.460]   >> Because they thought out about where their kidneys.
[01:23:09.460 --> 01:23:11.380]   >> Well, we don't know.
[01:23:11.380 --> 01:23:15.380]   I mean, you have the right to have that be private, whatever you might say.
[01:23:15.380 --> 01:23:16.820]   >> I'm not sure you should.
[01:23:16.820 --> 01:23:18.100]   I'm not sure you should.
[01:23:18.100 --> 01:23:19.380]   I think the thing we've got to solve-
[01:23:19.380 --> 01:23:20.380]   >> Wow, wow.
[01:23:20.380 --> 01:23:22.420]   >> Let me finish, let me finish.
[01:23:22.420 --> 01:23:27.740]   The thing we've got to solve in society is about the bad use of data.
[01:23:27.740 --> 01:23:30.140]   The problem here, and I wrote a book about this, which I haven't sent you yet, which
[01:23:30.140 --> 01:23:31.140]   I need to.
[01:23:31.140 --> 01:23:36.300]   The problem here is that we still have a stigma around illness and disease.
[01:23:36.300 --> 01:23:38.260]   That's a larger problem in society.
[01:23:38.260 --> 01:23:43.100]   The fact that someone has, we have a problem that bad insurance companies and bad countries
[01:23:43.100 --> 01:23:45.900]   want to pull insurance from you because you have a disease.
[01:23:45.900 --> 01:23:46.900]   That's the problem.
[01:23:46.900 --> 01:23:49.860]   The fact that you have the disease and the fact that there's information about it is
[01:23:49.860 --> 01:23:50.860]   not the problem.
[01:23:50.860 --> 01:23:53.980]   The use of that data, the use of that information is where the problem lies.
[01:23:53.980 --> 01:24:01.940]   >> Well, and you do have a point that much of that information could be useful in preventing
[01:24:01.940 --> 01:24:02.940]   future disease.
[01:24:02.940 --> 01:24:04.340]   >> I was part of a-
[01:24:04.340 --> 01:24:07.940]   >> When you want to know if the BRCA, if the-
[01:24:07.940 --> 01:24:12.860]   I'm told you don't pronounce that burka, you pronounce it barka.
[01:24:12.860 --> 01:24:13.860]   >> That was a burka.
[01:24:13.860 --> 01:24:14.860]   >> Yeah.
[01:24:14.860 --> 01:24:15.860]   Well, it's BRCA.
[01:24:15.860 --> 01:24:16.860]   You pronounce it anywhere you want.
[01:24:16.860 --> 01:24:20.580]   I pronounce it burka, but apparently that's probably not the best pronunciation.
[01:24:20.580 --> 01:24:26.140]   But if that gene, if you had more information about how much that indicates a likelihood
[01:24:26.140 --> 01:24:31.660]   of breast cancer via statistical analysis of past information, I think that would be
[01:24:31.660 --> 01:24:32.820]   valuable.
[01:24:32.820 --> 01:24:39.540]   >> Actually, your doctor tells you when you go in for your annual, your doctor asks you
[01:24:39.540 --> 01:24:46.580]   a series of questions to see if you should be tested based on the utility of whether
[01:24:46.580 --> 01:24:52.780]   or not you'd even have it and then what you'd be able to do about it.
[01:24:52.780 --> 01:24:55.780]   There's a lot to unpack here.
[01:24:55.780 --> 01:24:58.860]   >> That would give the doctor more information though, right?
[01:24:58.860 --> 01:25:04.580]   Here she would now have a big statistical database that would say, well, actually after
[01:25:04.580 --> 01:25:09.140]   analysis we found that the presence of that gene only impacts you 20% and based on these
[01:25:09.140 --> 01:25:12.780]   other things, maybe you shouldn't do anything about it.
[01:25:12.780 --> 01:25:14.620]   But right now they don't have that kind of-
[01:25:14.620 --> 01:25:16.100]   >> They do have that data on brine.
[01:25:16.100 --> 01:25:17.100]   >> I have some of it.
[01:25:17.100 --> 01:25:18.100]   But, well, all right.
[01:25:18.100 --> 01:25:21.180]   But the other kidney disease might be a similar problem.
[01:25:21.180 --> 01:25:23.140]   >> So, but there-
[01:25:23.140 --> 01:25:31.620]   >> Okay, I think I understand your point, Jeff.
[01:25:31.620 --> 01:25:39.900]   And I am a big proponent of greater knowledge and sharing data and using it in these things.
[01:25:39.900 --> 01:25:47.460]   But I do think for reasons of privacy and how people's lives have changed and also too,
[01:25:47.460 --> 01:25:54.940]   for getting this sort of consent in the future, we should do it right and we should inspire
[01:25:54.940 --> 01:25:55.940]   patients.
[01:25:55.940 --> 01:25:57.700]   >> I know that people are going to die.
[01:25:57.700 --> 01:25:59.100]   Jeff, people are always dying.
[01:25:59.100 --> 01:26:00.900]   >> That's a great price.
[01:26:00.900 --> 01:26:01.900]   That's a huge-
[01:26:01.900 --> 01:26:02.900]   >> It is an huge price.
[01:26:02.900 --> 01:26:09.140]   >> So, 100 years from now you look back and as anthropologists you judge the priorities
[01:26:09.140 --> 01:26:10.140]   of society.
[01:26:10.140 --> 01:26:16.380]   And you say that there was a choice held in this year, 2017.
[01:26:16.380 --> 01:26:22.100]   Taking data that didn't have the right tick box next to the consent because we didn't
[01:26:22.100 --> 01:26:26.260]   even think to ask because we had no idea technology would advance and we could do these amazing
[01:26:26.260 --> 01:26:27.260]   things.
[01:26:27.260 --> 01:26:32.380]   And there was a choice between taking that data to have an impact on people's lives and
[01:26:32.380 --> 01:26:36.420]   not doing it because of protectionism around that data.
[01:26:36.420 --> 01:26:41.540]   I think that shows a skewed societal prioritization.
[01:26:41.540 --> 01:26:45.060]   I think that's an unethical prioritization.
[01:26:45.060 --> 01:26:51.740]   Maybe that's because you don't worry as much what happens when your data is leaked.
[01:26:51.740 --> 01:26:58.500]   Maybe you're not in a population that has to think or be worried about that that pays
[01:26:58.500 --> 01:27:01.820]   more for, I don't know, insurance or-
[01:27:01.820 --> 01:27:08.300]   >> I pay a lot for insurance because I have these preexisting conditions.
[01:27:08.300 --> 01:27:11.220]   >> So that is a for example, Jeff.
[01:27:11.220 --> 01:27:13.300]   That's for example.
[01:27:13.300 --> 01:27:18.620]   This feels like an argument I have with a lot of people who are very pro tech who are like,
[01:27:18.620 --> 01:27:23.820]   why doesn't everyone want to give this information or share this information?
[01:27:23.820 --> 01:27:29.900]   And if you're coming at it from a different perspective, maybe it becomes more clear.
[01:27:29.900 --> 01:27:32.180]   And I'm just saying you're part of a very-
[01:27:32.180 --> 01:27:35.940]   >> This is an attack perspective, Stacy.
[01:27:35.940 --> 01:27:37.500]   This is an attack perspective.
[01:27:37.500 --> 01:27:41.540]   This is the perspective of somebody who has two cancers.
[01:27:41.540 --> 01:27:44.980]   If there's more knowledge that could have prevented my prostate from being taken because
[01:27:44.980 --> 01:27:48.580]   they know more about it, I'm all for that.
[01:27:48.580 --> 01:27:51.540]   And I understand the point about vulnerability and privacy.
[01:27:51.540 --> 01:27:53.700]   I get that fully.
[01:27:53.700 --> 01:27:58.220]   But I do think that we've got to get our act together as a society and rather than going
[01:27:58.220 --> 01:28:03.060]   constantly protectionism, we've got to grab these new opportunities that are provided.
[01:28:03.060 --> 01:28:10.140]   >> And going forward, I'm sure the NHS will add a clause that says-
[01:28:10.140 --> 01:28:12.020]   >> We lose huge amounts of data.
[01:28:12.020 --> 01:28:13.020]   >> I do.
[01:28:13.020 --> 01:28:15.100]   >> They didn't ask in that case.
[01:28:15.100 --> 01:28:17.580]   >> Well, because we couldn't know it.
[01:28:17.580 --> 01:28:18.740]   >> So- >> And this is not a-
[01:28:18.740 --> 01:28:20.140]   >> I get people-
[01:28:20.140 --> 01:28:21.420]   >> No, I know.
[01:28:21.420 --> 01:28:22.420]   Commit people.
[01:28:22.420 --> 01:28:27.620]   So Google, instead of grabbing NHS, which granted would have been easier, now they should
[01:28:27.620 --> 01:28:32.300]   go out and advocate and advertise and have people donate their data.
[01:28:32.300 --> 01:28:33.820]   That's another way of doing this.
[01:28:33.820 --> 01:28:35.980]   >> In years and years forward to do that.
[01:28:35.980 --> 01:28:39.940]   There is tons of historical data that today could save lives.
[01:28:39.940 --> 01:28:41.220]   >> I understand.
[01:28:41.220 --> 01:28:45.740]   So you could have, you could probably, can you pull data from dead people?
[01:28:45.740 --> 01:28:46.740]   And I have no idea.
[01:28:46.740 --> 01:28:47.740]   >> Well, let me tell you about that.
[01:28:47.740 --> 01:28:48.740]   Let me tell you about that.
[01:28:48.740 --> 01:28:49.740]   I'll tell you exactly about that.
[01:28:49.740 --> 01:28:50.740]   >> Why?
[01:28:50.740 --> 01:28:53.580]   >> Because I was just at the funeral home on Monday with my mother's death certificates.
[01:28:53.580 --> 01:28:57.180]   And a new wrinkle here I found out since the last time we've gone, had to go through this
[01:28:57.180 --> 01:29:00.540]   in my family, is that we needed to get two sets of death certificates.
[01:29:00.540 --> 01:29:03.020]   We needed to get one that has no cause of death.
[01:29:03.020 --> 01:29:07.020]   Because HIPAA requires that banks and such receive a death certificate without cause
[01:29:07.020 --> 01:29:10.620]   of death, they're not allowed to have it.
[01:29:10.620 --> 01:29:15.780]   So even in death, yes, your privacy is protected in a way that's ridiculous.
[01:29:15.780 --> 01:29:16.780]   >> By the way, condolences.
[01:29:16.780 --> 01:29:18.980]   >> HIPAA, I apologize for not saying this up, Frank.
[01:29:18.980 --> 01:29:22.340]   >> No, no, no, I wasn't trying to go for that.
[01:29:22.340 --> 01:29:23.340]   >> Yes.
[01:29:23.340 --> 01:29:25.540]   >> But HIPAA, thank you very much.
[01:29:25.540 --> 01:29:29.740]   I'm not trying to, no, I'm trying to, you know, milk for that.
[01:29:29.740 --> 01:29:32.380]   HIPAA has gone to ridiculous extremes in a way.
[01:29:32.380 --> 01:29:36.300]   So I was part of a World Economic Forum group about privacy.
[01:29:36.300 --> 01:29:38.180]   And there was a great discussion there.
[01:29:38.180 --> 01:29:46.300]   And you had major people in the health field, doctors and hospitals and universities talking
[01:29:46.300 --> 01:29:50.260]   about what they lose by this over protection of privacy.
[01:29:50.260 --> 01:29:53.140]   That discussion's being held, it's being held in real terms.
[01:29:53.140 --> 01:29:55.620]   And as far as I'm concerned, the right side just lost one.
[01:29:55.620 --> 01:29:56.620]   That's all I'm saying.
[01:29:56.620 --> 01:30:03.220]   >> I'm right in the middle of this, I recognize both arguments.
[01:30:03.220 --> 01:30:06.940]   And I think because it's health data, it's particularly charged.
[01:30:06.940 --> 01:30:08.580]   Is it extra charged because it's Google?
[01:30:08.580 --> 01:30:12.660]   Had it been Watson, for instance, you think the same ruling probably, right?
[01:30:12.660 --> 01:30:14.460]   >> Yeah, probably.
[01:30:14.460 --> 01:30:18.780]   >> So it's not that it's Google, although that adds to the fear that somehow deep minds
[01:30:18.780 --> 01:30:19.780]   might be--
[01:30:19.780 --> 01:30:20.780]   >> Well, it's pointed out that Google did nothing wrong.
[01:30:20.780 --> 01:30:22.740]   The NHS was the one who was there.
[01:30:22.740 --> 01:30:23.740]   >> Yeah, okay.
[01:30:23.740 --> 01:30:24.740]   >> Though Google will still suffer.
[01:30:24.740 --> 01:30:26.740]   >> Yeah, well also some concern that maybe--
[01:30:26.740 --> 01:30:31.180]   >> And Guardian went after this whole thing like, "The Guardians which has become my beloved
[01:30:31.180 --> 01:30:34.140]   Guardian has become so anti-tech."
[01:30:34.140 --> 01:30:39.700]   So every time it's now reflexive that every time they can go against technology, they
[01:30:39.700 --> 01:30:40.700]   will.
[01:30:40.700 --> 01:30:44.100]   And so on this one too, we give too much data to Google.
[01:30:44.100 --> 01:30:48.180]   Well, you weren't giving it to Google, you were giving it to the NHS for good reasons.
[01:30:48.180 --> 01:30:50.660]   But to treat you.
[01:30:50.660 --> 01:30:51.660]   But anyway.
[01:30:51.660 --> 01:30:53.740]   Sorry, I just--
[01:30:53.740 --> 01:30:55.740]   >> Good talk, Jeff.
[01:30:55.740 --> 01:30:56.740]   High five.
[01:30:56.740 --> 01:31:02.500]   >> Yeah, and going forward, of course, they should ask you if it'd be okay to share your
[01:31:02.500 --> 01:31:04.500]   data, anonymize data with--
[01:31:04.500 --> 01:31:06.500]   >> See, that's going forward.
[01:31:06.500 --> 01:31:08.660]   >> Wait, wait, wait, wait, Jeff, I can save lives today.
[01:31:08.660 --> 01:31:12.300]   >> Jeff, did you read Henrietta Lacks, what's the book called?
[01:31:12.300 --> 01:31:13.300]   Something something like that?
[01:31:13.300 --> 01:31:14.300]   >> Yeah, yeah.
[01:31:14.300 --> 01:31:15.460]   >> Right, right.
[01:31:15.460 --> 01:31:21.140]   >> And it is sort of related here, but you know, just--
[01:31:21.140 --> 01:31:23.340]   >> What's the physical thing as opposed to the--
[01:31:23.340 --> 01:31:24.340]   >> I understand.
[01:31:24.340 --> 01:31:25.340]   >> Regional things.
[01:31:25.340 --> 01:31:33.620]   >> You know, it's just perspective and historical use of data slash physical.
[01:31:33.620 --> 01:31:34.620]   Anyway.
[01:31:34.620 --> 01:31:40.860]   >> The immortal life of Henrietta Lacks, her gene, which is scientist known as HELA or
[01:31:40.860 --> 01:31:48.740]   Hila, she died of cancer in 1951 and her gene cells were taken without her knowledge at
[01:31:48.740 --> 01:31:49.740]   that time.
[01:31:49.740 --> 01:31:50.740]   >> Which is a form of data.
[01:31:50.740 --> 01:31:51.740]   You're right, their gene is a form of data.
[01:31:51.740 --> 01:31:53.060]   >> And it's been bought and sold by the billions.
[01:31:53.060 --> 01:31:55.060]   I think it's more, this one's more about--
[01:31:55.060 --> 01:31:56.060]   >> Yeah.
[01:31:56.060 --> 01:31:57.540]   >> She was ripped off, that's her privacy.
[01:31:57.540 --> 01:31:58.540]   >> Exactly.
[01:31:58.540 --> 01:32:02.300]   >> That her privacy was impinged upon.
[01:32:02.300 --> 01:32:09.820]   >> But this is kind of, this is a consent issue because they actually didn't, they used
[01:32:09.820 --> 01:32:12.780]   her, was it her ovaries?
[01:32:12.780 --> 01:32:17.500]   I can't remember, her cervix cells, they used those and cultivated them without her
[01:32:17.500 --> 01:32:19.700]   permission or consent.
[01:32:19.700 --> 01:32:24.620]   >> So it's been used to develop the polio vaccine for cloning, gene mapping, and vitro
[01:32:24.620 --> 01:32:26.300]   fernalization.
[01:32:26.300 --> 01:32:32.060]   Her cells, the Hila cells are widely used and very valuable.
[01:32:32.060 --> 01:32:36.820]   Now the question is, I mean, they didn't have any value to her.
[01:32:36.820 --> 01:32:37.820]   >> I don't know.
[01:32:37.820 --> 01:32:38.820]   That's a really--
[01:32:38.820 --> 01:32:40.820]   >> They think they would have value in their current.
[01:32:40.820 --> 01:32:41.820]   >> Clearly has her family with--
[01:32:41.820 --> 01:32:42.820]   >> Her family with many people.
[01:32:42.820 --> 01:32:44.900]   >> Her family remains poor.
[01:32:44.900 --> 01:32:50.580]   And that's an issue because I don't know, was money made with her cells?
[01:32:50.580 --> 01:32:51.580]   I don't know.
[01:32:51.580 --> 01:32:52.580]   >> Yes.
[01:32:52.580 --> 01:32:53.580]   >> They were sold, huh?
[01:32:53.580 --> 01:32:56.140]   >> I believe yes.
[01:32:56.140 --> 01:32:58.820]   >> It's been like two or three years since I read the book.
[01:32:58.820 --> 01:32:59.820]   >> Yeah.
[01:32:59.820 --> 01:33:00.820]   >> But totally an awesome book.
[01:33:00.820 --> 01:33:01.820]   Everyone should read it.
[01:33:01.820 --> 01:33:04.860]   >> It certainly raises some very interesting ethical issues somewhat similar to what we're
[01:33:04.860 --> 01:33:07.700]   talking about right now, although as an individual.
[01:33:07.700 --> 01:33:14.180]   Well, see, these were all individuals whose data was borrowed by the NHS.
[01:33:14.180 --> 01:33:17.700]   There's also the issue of what rights you've given up by being a part of the National
[01:33:17.700 --> 01:33:19.780]   Health Service and so forth and so on.
[01:33:19.780 --> 01:33:23.540]   And the ICO's job is to protect you despite that.
[01:33:23.540 --> 01:33:27.620]   The NHS clearly felt like, no, we own this information.
[01:33:27.620 --> 01:33:31.260]   >> So let's say that you get the plague.
[01:33:31.260 --> 01:33:32.940]   >> Well, there were just two cases of the plague.
[01:33:32.940 --> 01:33:33.940]   >> I know.
[01:33:33.940 --> 01:33:35.940]   Fortunately, it's highly treatable in this year, right?
[01:33:35.940 --> 01:33:36.940]   >> Thank God.
[01:33:36.940 --> 01:33:37.940]   >> Thank goodness.
[01:33:37.940 --> 01:33:40.820]   But let's say you get the plague and you say, no, no, no, no, no, no, I don't want anybody
[01:33:40.820 --> 01:33:41.820]   to know that.
[01:33:41.820 --> 01:33:42.820]   Nobody should.
[01:33:42.820 --> 01:33:43.820]   That's private.
[01:33:43.820 --> 01:33:46.060]   >> You risk how the people get into play.
[01:33:46.060 --> 01:33:47.060]   >> Right.
[01:33:47.060 --> 01:33:48.580]   So we've said, we said, I agree with you.
[01:33:48.580 --> 01:33:49.580]   That's a good help.
[01:33:49.580 --> 01:33:50.580]   >> Public health information.
[01:33:50.580 --> 01:33:51.940]   >> In fact, yeah, we're there.
[01:33:51.940 --> 01:33:59.980]   >> When polio strikes or malaria or any communicable disease, that information goes right to the
[01:33:59.980 --> 01:34:00.980]   CDC.
[01:34:00.980 --> 01:34:03.820]   It's anonymized also, but it goes to the CDC.
[01:34:03.820 --> 01:34:07.900]   >> If you have certain strains of tuberculosis, actually, you are, but there are laws written
[01:34:07.900 --> 01:34:09.060]   about this.
[01:34:09.060 --> 01:34:12.220]   Your data is actually shared.
[01:34:12.220 --> 01:34:14.500]   But we wrote laws about that.
[01:34:14.500 --> 01:34:23.700]   So people have these conversations, not companies and the NHS, but lawmakers and presumably medical
[01:34:23.700 --> 01:34:26.980]   advocates and other people.
[01:34:26.980 --> 01:34:34.500]   So I see your point and I am not one to be like, yes, let people die.
[01:34:34.500 --> 01:34:40.740]   But I'm also a big proponent of privacy and I'm not willing to give it up without.
[01:34:40.740 --> 01:34:41.740]   >> Chairman says, if you show up, you're not going to be able to get a job.
[01:34:41.740 --> 01:34:45.620]   If you show up at work with a cold, is it your obligation to tell your coworkers?
[01:34:45.620 --> 01:34:53.140]   Or let's say something without symptoms that's even more deadly, Ebola.
[01:34:53.140 --> 01:34:54.140]   You, right?
[01:34:54.140 --> 01:34:56.180]   I mean, if you have Ebola, you should tell people.
[01:34:56.180 --> 01:34:57.180]   >> You should tell people.
[01:34:57.180 --> 01:34:58.180]   I think you're right.
[01:34:58.180 --> 01:34:59.180]   >> You should actually go.
[01:34:59.180 --> 01:35:00.180]   You might actually.
[01:35:00.180 --> 01:35:05.500]   >> How about if, well, what about more to the point and timely if you have AIDS?
[01:35:05.500 --> 01:35:07.140]   >> You have to tell your partners.
[01:35:07.140 --> 01:35:08.220]   There's laws about that.
[01:35:08.220 --> 01:35:10.380]   You can't be a, what are they called?
[01:35:10.380 --> 01:35:11.380]   >> Carrier.
[01:35:11.380 --> 01:35:12.380]   >> There's a word.
[01:35:12.380 --> 01:35:13.380]   >> Patient zero.
[01:35:13.380 --> 01:35:15.740]   >> There's a word for someone who.
[01:35:15.740 --> 01:35:19.140]   >> Yeah, but it's, there's, yes.
[01:35:19.140 --> 01:35:21.780]   But there are laws about not infecting your partners with it.
[01:35:21.780 --> 01:35:24.260]   If you infect someone with AIDS and you knew you had AIDS.
[01:35:24.260 --> 01:35:28.420]   >> What do you bet because that was at the time the gay cancer that that law got passed
[01:35:28.420 --> 01:35:29.420]   really fast?
[01:35:29.420 --> 01:35:30.420]   >> Yeah.
[01:35:30.420 --> 01:35:35.460]   >> That's the, I mean, part of the problem is, and Jeff, I understand this is what rankles
[01:35:35.460 --> 01:35:42.180]   you most, that there's a real mismatch between how society decides this stuff and implements
[01:35:42.180 --> 01:35:46.340]   it and the speed with which technology advances.
[01:35:46.340 --> 01:35:52.180]   And you're all, you're in favor of the side of this, of technology just advancing and,
[01:35:52.180 --> 01:35:55.700]   and Stacy's saying, but, but there needs to be a discussion about this.
[01:35:55.700 --> 01:35:56.700]   >> I agree with Stacy.
[01:35:56.700 --> 01:35:57.700]   >> This is government data.
[01:35:57.700 --> 01:35:59.220]   >> But I'm saying at some point.
[01:35:59.220 --> 01:36:00.220]   >> This isn't just.
[01:36:00.220 --> 01:36:01.220]   >> You're right, Stacy.
[01:36:01.220 --> 01:36:02.220]   It's government data.
[01:36:02.220 --> 01:36:08.060]   The privacy is more of a agree with all that, but I'm saying at some point you choose to,
[01:36:08.060 --> 01:36:12.460]   I would say we use a society should choose to violate the lack of consent of people in
[01:36:12.460 --> 01:36:14.060]   the past who would have no better.
[01:36:14.060 --> 01:36:17.740]   I think at some point if you said to Henrietta lacks, okay, we're going to try not to rip
[01:36:17.740 --> 01:36:18.740]   you off here.
[01:36:18.740 --> 01:36:23.540]   But you know, if we take these genes from you after you're gone, you could save countless,
[01:36:23.540 --> 01:36:24.540]   countless people.
[01:36:24.540 --> 01:36:30.020]   I bet Henrietta Lacks was a wonderful woman who would have said, well, of course, of
[01:36:30.020 --> 01:36:31.020]   course.
[01:36:31.020 --> 01:36:34.020]   >> What you said about Hila is about the fact that she wasn't compensated.
[01:36:34.020 --> 01:36:35.020]   >> Right.
[01:36:35.020 --> 01:36:36.020]   >> Not about privacy.
[01:36:36.020 --> 01:36:39.540]   >> Well, and she was, she did not get consent for herself.
[01:36:39.540 --> 01:36:45.820]   And they talk about how uncomfortable it makes, I think it may either, it made her or it made
[01:36:45.820 --> 01:36:53.980]   her children that parts of their mother in their, and again, scientifically this is not
[01:36:53.980 --> 01:37:00.620]   accurate, but they felt like parts of their mother were like floating around in labs everywhere.
[01:37:00.620 --> 01:37:01.620]   >> Right.
[01:37:01.620 --> 01:37:02.620]   >> Which, of course, is not.
[01:37:02.620 --> 01:37:03.620]   >> Of course, it's not.
[01:37:03.620 --> 01:37:04.620]   >> It's not.
[01:37:04.620 --> 01:37:05.620]   It's cells cultured from her cervical cells.
[01:37:05.620 --> 01:37:08.220]   >> But no one would explain that to them.
[01:37:08.220 --> 01:37:11.620]   And that's why the second point.
[01:37:11.620 --> 01:37:13.260]   So privacy is important.
[01:37:13.260 --> 01:37:20.140]   But two, if you don't get consent in explaining and educate people about this stuff, all kinds
[01:37:20.140 --> 01:37:24.540]   of weird crap and beliefs are going to blow up around this.
[01:37:24.540 --> 01:37:25.540]   And so.
[01:37:25.540 --> 01:37:31.940]   >> Okay, let's say that we find a lab that has 100 year old hair samples and we find that
[01:37:31.940 --> 01:37:34.500]   using that data, you know, gets us something.
[01:37:34.500 --> 01:37:37.940]   We have to go, we can't go back 100 years, but we find a new method.
[01:37:37.940 --> 01:37:39.540]   >> Those people are all dead and that's anonymized.
[01:37:39.540 --> 01:37:40.540]   >> Well.
[01:37:40.540 --> 01:37:41.540]   >> I mean, 100 year olds.
[01:37:41.540 --> 01:37:43.700]   >> No, it's not anonymized at all.
[01:37:43.700 --> 01:37:44.700]   There's there's been being.
[01:37:44.700 --> 01:37:48.340]   >> Was the issue that some of the data that the National Health Service shared was live
[01:37:48.340 --> 01:37:49.340]   people?
[01:37:49.340 --> 01:37:51.220]   >> No, it was the, it was the consent.
[01:37:51.220 --> 01:37:53.860]   It was the lack of the proper consent for that data.
[01:37:53.860 --> 01:37:56.940]   So they didn't stipulate whether they were dead or alive.
[01:37:56.940 --> 01:37:57.940]   Some of them weren't.
[01:37:57.940 --> 01:37:58.940]   >> Because.
[01:37:58.940 --> 01:38:02.620]   >> Yeah, and again, because that wasn't anticipated that you could ever predict kidney
[01:38:02.620 --> 01:38:04.900]   disease based on this data.
[01:38:04.900 --> 01:38:07.100]   So there was not a reason to ask.
[01:38:07.100 --> 01:38:11.100]   That's the problem I have with this kind of limiting the future by the knowledge of the
[01:38:11.100 --> 01:38:12.100]   past.
[01:38:12.100 --> 01:38:15.020]   Then we cut off tons.
[01:38:15.020 --> 01:38:18.740]   You know, this is not just, you know, a little bit of NHS recent data.
[01:38:18.740 --> 01:38:23.380]   And then the World Economic Forum project, they were upset about having, you know, 50
[01:38:23.380 --> 01:38:28.100]   years of data they couldn't get because of things like this that had, these are the
[01:38:28.100 --> 01:38:31.940]   experts saying we can have real impact from this.
[01:38:31.940 --> 01:38:39.460]   >> I think that Google, if it wants this data, it can go and get a pretty relevant subset
[01:38:39.460 --> 01:38:47.020]   of this type of data by talking to patients and asking them to donate, talking to doctors
[01:38:47.020 --> 01:38:49.300]   and having them talk to their patients.
[01:38:49.300 --> 01:38:52.540]   It's just, it's a more fragmented approach.
[01:38:52.540 --> 01:38:55.620]   It may be statistically not is awesome.
[01:38:55.620 --> 01:38:57.860]   >> Which may lead to more false diagnosis.
[01:38:57.860 --> 01:38:58.860]   >> Right.
[01:38:58.860 --> 01:38:59.860]   But Google.
[01:38:59.860 --> 01:39:00.860]   >> Which has an impact on people's lives.
[01:39:00.860 --> 01:39:01.860]   >> Smart.
[01:39:01.860 --> 01:39:04.180]   >> Yeah, but you're not going to get 1.6 million day patient records.
[01:39:04.180 --> 01:39:05.180]   >> No way.
[01:39:05.180 --> 01:39:07.300]   >> And the cost of that would be extreme.
[01:39:07.300 --> 01:39:14.780]   >> I'm just saying that there are ways around it just because in going forward if that they
[01:39:14.780 --> 01:39:16.140]   can get consent.
[01:39:16.140 --> 01:39:22.180]   I mean, you're basically saying I want it now and I don't like the rules that were in place
[01:39:22.180 --> 01:39:26.740]   when I got this stuff and now I want them changed because we can do more with them.
[01:39:26.740 --> 01:39:28.540]   That's not wrong.
[01:39:28.540 --> 01:39:33.380]   But what the courts are saying is we're not going to retroactively change the rules in
[01:39:33.380 --> 01:39:38.340]   this situation just because you, someone wants it now.
[01:39:38.340 --> 01:39:39.340]   >> Reality changes.
[01:39:39.340 --> 01:39:40.340]   >> Reality changes.
[01:39:40.340 --> 01:39:42.260]   >> I know, I know.
[01:39:42.260 --> 01:39:43.260]   But.
[01:39:43.260 --> 01:39:50.300]   >> Should, just out of curiosity, should we not have harvested and used those he/law cells?
[01:39:50.300 --> 01:39:55.860]   Are you asking me or both of us?
[01:39:55.860 --> 01:39:58.340]   >> I mean, there was huge benefit from that.
[01:39:58.340 --> 01:40:00.620]   >> No, we should.
[01:40:00.620 --> 01:40:01.620]   We did.
[01:40:01.620 --> 01:40:06.300]   And if I were the doctor, I mean, ideally we would have gotten consent.
[01:40:06.300 --> 01:40:07.300]   >> Right.
[01:40:07.300 --> 01:40:08.300]   >> We didn't.
[01:40:08.300 --> 01:40:09.300]   >> But she was dead.
[01:40:09.300 --> 01:40:13.020]   >> No, when they harvested them, they knew about them beforehand.
[01:40:13.020 --> 01:40:14.020]   They knew her.
[01:40:14.020 --> 01:40:17.660]   >> Well, they biopsied her cancerous cervix and those cells that they had.
[01:40:17.660 --> 01:40:19.260]   >> But she was still alive.
[01:40:19.260 --> 01:40:20.260]   >> Like some of them.
[01:40:20.260 --> 01:40:21.260]   >> She was alive, obviously.
[01:40:21.260 --> 01:40:24.180]   But I don't know if they knew the value of those cells.
[01:40:24.180 --> 01:40:26.780]   They just because they were cancerous continue to reproduce.
[01:40:26.780 --> 01:40:27.780]   >> I don't.
[01:40:27.780 --> 01:40:30.540]   >> Maybe later after she was dead, they said these are valuable.
[01:40:30.540 --> 01:40:33.780]   They got her consent probably to harvest them in the first place.
[01:40:33.780 --> 01:40:36.100]   >> No, they didn't.
[01:40:36.100 --> 01:40:37.780]   That was part of the treatment.
[01:40:37.780 --> 01:40:38.780]   >> Well, it's part of the treatment.
[01:40:38.780 --> 01:40:41.500]   >> She got a biopsy, right?
[01:40:41.500 --> 01:40:42.500]   I mean.
[01:40:42.500 --> 01:40:43.500]   >> Okay.
[01:40:43.500 --> 01:40:44.500]   >> I don't know.
[01:40:44.500 --> 01:40:46.900]   Like, I'm like, should they have not done this?
[01:40:46.900 --> 01:40:47.900]   I don't remember the facts.
[01:40:47.900 --> 01:40:50.460]   >> There's a question of the greater good.
[01:40:50.460 --> 01:40:51.460]   Do they do it wrong?
[01:40:51.460 --> 01:40:52.460]   Yes, but is there a greater good?
[01:40:52.460 --> 01:40:53.740]   I think they're clearly- >> She wasn't harmed.
[01:40:53.740 --> 01:40:55.380]   This was part of her cancer treatment.
[01:40:55.380 --> 01:40:56.380]   So- >> Right.
[01:40:56.380 --> 01:40:57.380]   >> She wasn't harmed.
[01:40:57.380 --> 01:41:00.700]   In fact, you could argue that this was with an intent to help her.
[01:41:00.700 --> 01:41:02.900]   It was later discovered the value of these cells.
[01:41:02.900 --> 01:41:08.580]   It would have been nice, of course, to compensate her at her estate for the billions of dollars.
[01:41:08.580 --> 01:41:09.980]   It's generated or whatever it is.
[01:41:09.980 --> 01:41:10.980]   Maybe it's a million dollars.
[01:41:10.980 --> 01:41:12.380]   I don't know.
[01:41:12.380 --> 01:41:19.260]   But there is clear benefit to society from having done this.
[01:41:19.260 --> 01:41:20.700]   No harm to her.
[01:41:20.700 --> 01:41:21.700]   >> Yeah, I did.
[01:41:21.700 --> 01:41:24.700]   >> I don't know if a court would say there's a problem.
[01:41:24.700 --> 01:41:30.380]   >> But a court would say they should compensate her in this case or compensate her heirs.
[01:41:30.380 --> 01:41:31.380]   >> There was no harm.
[01:41:31.380 --> 01:41:34.260]   So the interesting thing- >> Look, you know, even then, even then,
[01:41:34.260 --> 01:41:38.380]   we all donate data to things.
[01:41:38.380 --> 01:41:44.060]   So Four Square makes money on knowing that I've gone into that McDonald's and not that
[01:41:44.060 --> 01:41:45.060]   McDonald's.
[01:41:45.060 --> 01:41:46.060]   >> But you voluntarily gave that.
[01:41:46.060 --> 01:41:47.060]   >> They have that knowledge.
[01:41:47.060 --> 01:41:49.740]   I did voluntarily give it, but I didn't know that they were going to make money in a whole
[01:41:49.740 --> 01:41:50.740]   new model.
[01:41:50.740 --> 01:41:51.740]   >> That's true.
[01:41:51.740 --> 01:41:52.740]   They found a new model.
[01:41:52.740 --> 01:41:56.180]   >> And I'm, of course, finally with that.
[01:41:56.180 --> 01:42:02.540]   Knowledge, this is the problem we get down to in the end of saying from your own knowledge.
[01:42:02.540 --> 01:42:06.260]   And a free society and an advanced and modern society, the answer is no.
[01:42:06.260 --> 01:42:10.220]   You can't own it because you can't tell other people what they're not allowed to know.
[01:42:10.220 --> 01:42:12.500]   And in the end, it's about knowledge.
[01:42:12.500 --> 01:42:16.340]   >> Okay, no, there is a very big difference between knowledge and data.
[01:42:16.340 --> 01:42:18.100]   >> Oh, that's interesting.
[01:42:18.100 --> 01:42:20.700]   >> Data begets knowledge.
[01:42:20.700 --> 01:42:23.740]   >> And data is necessary to knowledge.
[01:42:23.740 --> 01:42:25.060]   Data is information.
[01:42:25.060 --> 01:42:27.940]   You can't- >> Data is a precursor to knowledge.
[01:42:27.940 --> 01:42:28.940]   Let's say that.
[01:42:28.940 --> 01:42:29.940]   >> Right.
[01:42:29.940 --> 01:42:30.940]   >> You can't own information.
[01:42:30.940 --> 01:42:31.940]   You can own the treatment of it, but you can't own information.
[01:42:31.940 --> 01:42:34.500]   That is the essence of copyright.
[01:42:34.500 --> 01:42:39.180]   You can only own the treatment of information, not the information itself.
[01:42:39.180 --> 01:42:43.260]   >> That's facts, but facts about an individual in person.
[01:42:43.260 --> 01:42:48.460]   That's different from data about their genetic history and that sort of thing.
[01:42:48.460 --> 01:42:53.540]   Unless we create a series of laws that protect people from the negative outcomes that come
[01:42:53.540 --> 01:42:54.540]   from having certain ideas.
[01:42:54.540 --> 01:42:55.940]   >> That's what I argued by book, exactly.
[01:42:55.940 --> 01:42:56.940]   We need to do that.
[01:42:56.940 --> 01:42:57.940]   I agree with that.
[01:42:57.940 --> 01:42:58.940]   We absolutely need to do that.
[01:42:58.940 --> 01:43:01.180]   And we need to- >> So, have some that there are no protections.
[01:43:01.180 --> 01:43:04.340]   There are no protections if your data.
[01:43:04.340 --> 01:43:05.340]   >> But people leave it.
[01:43:05.340 --> 01:43:06.340]   >> We're going to move on.
[01:43:06.340 --> 01:43:07.340]   >> Okay.
[01:43:07.340 --> 01:43:11.700]   I think we're going to move on because I think both points have been well made.
[01:43:11.700 --> 01:43:13.940]   It's an interesting- It's a fascinating question.
[01:43:13.940 --> 01:43:16.340]   I don't know if there's a clear answer to it.
[01:43:16.340 --> 01:43:22.980]   And the reason I mentioned harm is because that's often the measured courts use, the
[01:43:22.980 --> 01:43:28.100]   standard courts use for awards in lawsuits.
[01:43:28.100 --> 01:43:36.340]   When it comes up in a recent lawsuit, Facebook was being sued in the US over user tracking.
[01:43:36.340 --> 01:43:42.420]   Even after you're logged out of Facebook, they would continue to track your internet
[01:43:42.420 --> 01:43:44.220]   activity.
[01:43:44.220 --> 01:43:49.060]   And there was a suit saying that Facebook had violated federal and California privacy
[01:43:49.060 --> 01:43:54.180]   and wiretapping laws by storing cookies on their browsers that tracked where they visited
[01:43:54.180 --> 01:43:57.460]   outside websites containing Facebook-like buttons.
[01:43:57.460 --> 01:44:04.700]   The judge, Edward Davila and San Jose, district court judge threw it out.
[01:44:04.700 --> 01:44:11.060]   He said, "Nope, you failed to show that you, A, had a reasonable expectation of privacy,
[01:44:11.060 --> 01:44:16.660]   and more importantly, B, that you suffered any realistic economic harm or loss."
[01:44:16.660 --> 01:44:21.500]   In other words, the abstract notion that it was harmful wasn't sufficient for the judge
[01:44:21.500 --> 01:44:23.660]   to let the case move forward.
[01:44:23.660 --> 01:44:25.340]   You had to show actual harm.
[01:44:25.340 --> 01:44:27.020]   And I think a lot of these privacy-
[01:44:27.020 --> 01:44:28.020]   Amen.
[01:44:28.020 --> 01:44:31.060]   Issues are not, it's difficult to show actual harm.
[01:44:31.060 --> 01:44:34.380]   It's more potential or putative harm.
[01:44:34.380 --> 01:44:38.940]   Or it's the creepy line or it's the one, that's why I argue in the book.
[01:44:38.940 --> 01:44:41.980]   Actual harm is the proper standard.
[01:44:41.980 --> 01:44:47.780]   And even when there is actual harm, the solution to that is not necessarily withholding the
[01:44:47.780 --> 01:44:48.780]   information of the data.
[01:44:48.780 --> 01:44:51.340]   It is almost always how the data is used.
[01:44:51.340 --> 01:44:56.380]   And yet I bet a lot of people are upset over this decision and feel like the judge protected
[01:44:56.380 --> 01:44:57.380]   Facebook.
[01:44:57.380 --> 01:45:01.940]   And a lot of the stories I said, Facebook allowed to continue to track you even after
[01:45:01.940 --> 01:45:03.740]   you're logged out.
[01:45:03.740 --> 01:45:10.140]   Well, okay, so here's something that, and normally I'm pro in the harm category in the
[01:45:10.140 --> 01:45:20.300]   sense that you should have to show harm, except this privacy, there's been some really well-argued
[01:45:20.300 --> 01:45:21.300]   things.
[01:45:21.300 --> 01:45:24.180]   And I wish I were as erudite as the judges who wrote their descents here.
[01:45:24.180 --> 01:45:31.260]   But privacy is the ability to be your own person in a space and not be judged.
[01:45:31.260 --> 01:45:37.220]   And the question is, even if there's no harm, is the fact that that information is known
[01:45:37.220 --> 01:45:42.460]   and there is some judgment being made, is that a problem?
[01:45:42.460 --> 01:45:44.180]   And that is a real question.
[01:45:44.180 --> 01:45:50.420]   And that's where some of the privacy things about, gosh, my brain just stopped working.
[01:45:50.420 --> 01:45:51.420]   I'm sorry, you guys.
[01:45:51.420 --> 01:45:53.620]   >> You're too young for that spacing.
[01:45:53.620 --> 01:45:56.300]   >> I'm so tired.
[01:45:56.300 --> 01:46:01.820]   I blame the migraine medicine.
[01:46:01.820 --> 01:46:04.700]   >> You can't use that excuse every week, Stacey.
[01:46:04.700 --> 01:46:06.220]   >> It's topomax.
[01:46:06.220 --> 01:46:07.220]   It makes you stupid.
[01:46:07.220 --> 01:46:08.220]   It is what it does.
[01:46:08.220 --> 01:46:10.140]   >> Oh, can I get some?
[01:46:10.140 --> 01:46:12.260]   >> It makes you skinny and stupid.
[01:46:12.260 --> 01:46:13.260]   I call it the monotrot.
[01:46:13.260 --> 01:46:14.260]   >> Skinny and stupid.
[01:46:14.260 --> 01:46:15.260]   Oh, man, that's nice.
[01:46:15.260 --> 01:46:18.660]   Well, you know that that's how LSD was invented.
[01:46:18.660 --> 01:46:24.100]   They were playing with rye molds, air got, which is used as a migraine medicine.
[01:46:24.100 --> 01:46:25.100]   >> No, it did.
[01:46:25.100 --> 01:46:26.100]   >> Yeah.
[01:46:26.100 --> 01:46:31.380]   And Albert Hoffman was trying to synthesize migraine medicine and accidentally synthesized
[01:46:31.380 --> 01:46:32.980]   LSD and took the first acid trip.
[01:46:32.980 --> 01:46:33.980]   What was that?
[01:46:33.980 --> 01:46:34.980]   I think John will know this.
[01:46:34.980 --> 01:46:37.300]   Was this back in 1945, I want to say?
[01:46:37.300 --> 01:46:38.300]   It was in the '40s.
[01:46:38.300 --> 01:46:39.740]   It was a long time ago.
[01:46:39.740 --> 01:46:41.380]   Took a very famous bicycle ride.
[01:46:41.380 --> 01:46:45.660]   He accidentally touched some of it and got some of it in his mouth, got it in his bicycle
[01:46:45.660 --> 01:46:49.060]   and started tripping.
[01:46:49.060 --> 01:46:51.860]   And all of that to keep you from getting headaches.
[01:46:51.860 --> 01:46:52.860]   >> There we go.
[01:46:52.860 --> 01:46:53.860]   Well, thanks, man.
[01:46:53.860 --> 01:46:54.860]   [LAUGHTER]
[01:46:54.860 --> 01:46:58.300]   >> Anyway, I'm sorry that you're a little spacey.
[01:46:58.300 --> 01:46:59.300]   Stacey.
[01:46:59.300 --> 01:47:01.420]   >> I will be for like a month, but then I'll come back.
[01:47:01.420 --> 01:47:03.860]   >> We got a new name for it, Stupa Max.
[01:47:03.860 --> 01:47:04.860]   >> Stupa Max.
[01:47:04.860 --> 01:47:06.900]   Oh, there you go.
[01:47:06.900 --> 01:47:10.540]   >> So you're just, it's a month-long treatment, and then you don't have to do it anymore?
[01:47:10.540 --> 01:47:11.740]   >> No, it's forever.
[01:47:11.740 --> 01:47:15.900]   Well, it's as long as I can tolerate it and then I stop.
[01:47:15.900 --> 01:47:18.540]   I go off it on because it makes me lose my hair, too.
[01:47:18.540 --> 01:47:20.540]   I guess it's not the supermodel.
[01:47:20.540 --> 01:47:23.900]   >> It's a, no, but I know it because it's an anti-seizure medicine, too.
[01:47:23.900 --> 01:47:24.900]   >> It is.
[01:47:24.900 --> 01:47:25.900]   >> Yeah.
[01:47:25.900 --> 01:47:26.900]   >> Yeah.
[01:47:26.900 --> 01:47:29.540]   >> And in fact, the people I know who've been on it hate it.
[01:47:29.540 --> 01:47:30.540]   >> Yeah.
[01:47:30.540 --> 01:47:31.540]   >> Hate it.
[01:47:31.540 --> 01:47:32.540]   Because it makes you guys--
[01:47:32.540 --> 01:47:34.980]   >> Have you tried everything, I assume?
[01:47:34.980 --> 01:47:39.540]   >> Oh, Jeff, I have tried everything with the exception of LSD.
[01:47:39.540 --> 01:47:42.020]   I think you should try it.
[01:47:42.020 --> 01:47:47.420]   I've heard some good reports, and I want to say I've never had a migraine.
[01:47:47.420 --> 01:47:50.900]   And we want to do the show, well, we want to do the show with Stacey.
[01:47:50.900 --> 01:47:52.940]   Where'd she take it?
[01:47:52.940 --> 01:47:58.980]   >> I'm usually not as bad, but I'm struggling over this, who wrote this.
[01:47:58.980 --> 01:48:07.180]   It was an awesome essay on the harms of a lack of privacy that are just like self-sensoring
[01:48:07.180 --> 01:48:08.900]   is the closest I can get to it.
[01:48:08.900 --> 01:48:12.340]   I was trying to look it up, but I can't remember who wrote it.
[01:48:12.340 --> 01:48:15.740]   >> Well, yeah, I mean, so yeah, that makes sense.
[01:48:15.740 --> 01:48:20.460]   So in fact, it would have the opposite effect to the beneficial effect Jeff talks about,
[01:48:20.460 --> 01:48:24.900]   which is if you can't be assured that you control your data, you might be more reluctant
[01:48:24.900 --> 01:48:30.860]   to share valuable data, as Jeff often does about his medical condition and his life condition.
[01:48:30.860 --> 01:48:32.380]   And that is a valuable thing to share that.
[01:48:32.380 --> 01:48:36.060]   But if you can't be assured that it's going to be, you know, that you control where it
[01:48:36.060 --> 01:48:37.540]   goes.
[01:48:37.540 --> 01:48:43.540]   But I think Jeff's point is well taken, which is we don't need laws to protect privacy.
[01:48:43.540 --> 01:48:48.500]   We need laws to protect the misuse of that information, that there is value in the sharing
[01:48:48.500 --> 01:48:49.500]   of information.
[01:48:49.500 --> 01:48:56.180]   >> We need laws on privacy, but the primary way to do that is to protect against it.
[01:48:56.180 --> 01:48:59.980]   So Dana Boyd's great example of this, when she sat me down and taught me about this,
[01:48:59.980 --> 01:49:01.780]   she said, listen, Jeff, you come in and apply for a job.
[01:49:01.780 --> 01:49:06.220]   I can see the color of your hair, how old you are.
[01:49:06.220 --> 01:49:10.260]   I can guess all kinds of other things about you.
[01:49:10.260 --> 01:49:14.380]   I have the knowledge that you're an old fart.
[01:49:14.380 --> 01:49:16.940]   And I can use that to not give you the job.
[01:49:16.940 --> 01:49:18.420]   I'm not going to wait with that.
[01:49:18.420 --> 01:49:20.260]   And that's the way life goes.
[01:49:20.260 --> 01:49:24.020]   But if I do that 10 times in a row and you all get together and you share that knowledge
[01:49:24.020 --> 01:49:28.220]   that I've done that, then there are laws and you can get after me because of the use of
[01:49:28.220 --> 01:49:29.220]   that knowledge.
[01:49:29.220 --> 01:49:31.900]   You can't stop Dana from knowing that I'm an old fart.
[01:49:31.900 --> 01:49:33.380]   That's apparent.
[01:49:33.380 --> 01:49:38.340]   What you do with that knowledge and the pattern with which you do that is a different matter.
[01:49:38.340 --> 01:49:42.900]   And that's where we legislate the use of the knowledge rather than the knowledge itself.
[01:49:42.900 --> 01:49:46.460]   >> Boyd actually wrote, "Often privacy isn't about hiding.
[01:49:46.460 --> 01:49:48.540]   It's about creating space to open up."
[01:49:48.540 --> 01:49:50.900]   >> Oh, look, there it is.
[01:49:50.900 --> 01:49:52.940]   >> Oh, is that what you were looking for?
[01:49:52.940 --> 01:49:53.940]   >> Maybe.
[01:49:53.940 --> 01:49:55.060]   >> I didn't take any Topomax.
[01:49:55.060 --> 01:50:01.700]   This is a article about in an ethics blog, "Lost of Online Privacy, What's the Harm?"
[01:50:01.700 --> 01:50:05.140]   The Markle of Center for Applied Ethics at the University of Santa Clara.
[01:50:05.140 --> 01:50:08.020]   >> I keep scrolling when you show this on your screen.
[01:50:08.020 --> 01:50:09.020]   >> I know.
[01:50:09.020 --> 01:50:10.020]   Yes, that's it.
[01:50:10.020 --> 01:50:11.020]   That's it.
[01:50:11.020 --> 01:50:15.860]   >> They quote, and I love this at the beginning, Gabriel Garcia, Marquez, "All human beings have
[01:50:15.860 --> 01:50:20.940]   three lives, public, private, and secrets."
[01:50:20.940 --> 01:50:29.020]   This is actually a really good article that does address a lot of the things we've been
[01:50:29.020 --> 01:50:33.660]   talking about.
[01:50:33.660 --> 01:50:38.700]   Privacy is about much broader-- this is Jay Stanley, a senior policy analyst for the ACLU.
[01:50:38.700 --> 01:50:41.220]   Privacy is about much broader values than just hiding things.
[01:50:41.220 --> 01:50:46.460]   Ultimately, the fullest retort to the-- I have nothing that-- this is-- I thought it might
[01:50:46.460 --> 01:50:47.460]   be.
[01:50:47.460 --> 01:50:48.460]   I'm channeling you.
[01:50:48.460 --> 01:50:50.620]   That's because I've taken a lot of acid.
[01:50:50.620 --> 01:50:55.500]   Ultimately, the fullest retort to the nothing to hide impulse is a richer philosophical
[01:50:55.500 --> 01:51:01.540]   defense of privacy that articulates its importance to human life, the human need for a refuge
[01:51:01.540 --> 01:51:05.580]   from the eye of a community and from the self-monitoring that others-- that living with
[01:51:05.580 --> 01:51:09.740]   others entails the need for space in which to play and try out new ideas, identities,
[01:51:09.740 --> 01:51:14.700]   and behaviors without lasting consequences and the importance of maintaining the balance
[01:51:14.700 --> 01:51:17.100]   of power between individuals in the state.
[01:51:17.100 --> 01:51:21.420]   I would actually underscore that last one because I think that's for a lot of people
[01:51:21.420 --> 01:51:29.420]   who watch this show, the critical point of all this is maintaining the balance of power
[01:51:29.420 --> 01:51:31.140]   between individuals in the state.
[01:51:31.140 --> 01:51:32.140]   Yeah.
[01:51:32.140 --> 01:51:38.780]   Yeah, and I have long argued that the state presents itself as the protector of privacy.
[01:51:38.780 --> 01:51:40.980]   It is the worst threat to privacy.
[01:51:40.980 --> 01:51:41.980]   It is.
[01:51:41.980 --> 01:51:44.580]   It can use that data in ways that no one else can.
[01:51:44.580 --> 01:51:45.580]   Yeah.
[01:51:45.580 --> 01:51:50.620]   And this is also why I think I am very against spying on my daughter because I think people
[01:51:50.620 --> 01:51:55.260]   need that space to test things out and grow.
[01:51:55.260 --> 01:51:56.260]   Yes, yes.
[01:51:56.260 --> 01:52:00.100]   That's why you shouldn't read a kid's diary unless--
[01:52:00.100 --> 01:52:01.100]   Great.
[01:52:01.100 --> 01:52:03.580]   --and there are some mitigating circumstances.
[01:52:03.580 --> 01:52:09.420]   If you're afraid that they're doing something really dangerous and you need to protect them,
[01:52:09.420 --> 01:52:13.700]   there are some mitigating circumstances but generally invading their privacy is not a
[01:52:13.700 --> 01:52:14.700]   good thing.
[01:52:14.700 --> 01:52:16.700]   Everybody needs a private place.
[01:52:16.700 --> 01:52:24.260]   Well, so then the question is without getting consent, and I can't tell in this Facebook
[01:52:24.260 --> 01:52:30.100]   case if the issue-- I know he said it was harm, but if people thought they were exiting
[01:52:30.100 --> 01:52:36.060]   Facebook thus making their activity on the web, quote unquote, "private," then does that
[01:52:36.060 --> 01:52:39.860]   mean-- then there is kind of a harm from it.
[01:52:39.860 --> 01:52:41.260]   Except they say.
[01:52:41.260 --> 01:52:42.260]   Okay.
[01:52:42.260 --> 01:52:43.260]   Okay.
[01:52:43.260 --> 01:52:44.260]   But here's the but.
[01:52:44.260 --> 01:52:48.540]   Your ISP knows everywhere you've gone.
[01:52:48.540 --> 01:52:50.300]   So it's not private to that extent.
[01:52:50.300 --> 01:52:51.300]   Somebody else knows too.
[01:52:51.300 --> 01:52:52.300]   So this is a-- yes.
[01:52:52.300 --> 01:52:59.380]   And this is an issue of education among people because I know that when I log out of Facebook,
[01:52:59.380 --> 01:53:00.740]   Facebook is tracking me.
[01:53:00.740 --> 01:53:06.660]   I know that if I actually turn on certain apps in stores, their beacons are sharing my
[01:53:06.660 --> 01:53:11.580]   physical proximity data in interest in certain products with Facebook and Google because
[01:53:11.580 --> 01:53:16.020]   of retargeting via beacons, which is freaking crazy.
[01:53:16.020 --> 01:53:17.020]   So--
[01:53:17.020 --> 01:53:19.420]   But you are in a public place.
[01:53:19.420 --> 01:53:25.260]   That doesn't-- I mean, the issue is, am I aware as an individual that this is all being
[01:53:25.260 --> 01:53:26.420]   shared?
[01:53:26.420 --> 01:53:30.460]   And I think that's not a conversation we have a lot of.
[01:53:30.460 --> 01:53:32.740]   And people don't know what they're sharing in--
[01:53:32.740 --> 01:53:33.900]   Oh, we're having it like crazy.
[01:53:33.900 --> 01:53:35.900]   We have it here all the time.
[01:53:35.900 --> 01:53:36.900]   I know.
[01:53:36.900 --> 01:53:38.580]   But it's one of the few places I agree with you.
[01:53:38.580 --> 01:53:39.580]   I agree with you.
[01:53:39.580 --> 01:53:40.580]   I agree with you.
[01:53:40.580 --> 01:53:41.580]   Yeah.
[01:53:41.580 --> 01:53:45.540]   It's like, you know, somebody walking around with like, I don't know, they're fly-in zipped.
[01:53:45.540 --> 01:53:49.580]   We're all walking around with our fly-in zipped and we're not aware of it.
[01:53:49.580 --> 01:53:50.580]   I've done that.
[01:53:50.580 --> 01:53:55.660]   And you know, maybe they're someone decides it is a problem.
[01:53:55.660 --> 01:53:57.260]   People respect my privacy and don't tell me.
[01:53:57.260 --> 01:53:59.860]   I'd rather they tell me.
[01:53:59.860 --> 01:54:07.700]   Did you see the story about the Ticketmaster app that uses supersonic sound?
[01:54:07.700 --> 01:54:11.420]   It's a-- it's-- so there's a new-- Ticketmaster is often--
[01:54:11.420 --> 01:54:16.540]   offering a new way to authenticate at the gate when you buy a concert ticket.
[01:54:16.540 --> 01:54:23.940]   You put their app on the phone and the app emits tones at 19-- I think it's 19.5 kilohertz.
[01:54:23.940 --> 01:54:26.580]   Oh, God, are you going to play loud tones again?
[01:54:26.580 --> 01:54:27.580]   No, I won't.
[01:54:27.580 --> 01:54:31.380]   And-- but you probably are young enough that you could hear this.
[01:54:31.380 --> 01:54:32.700]   I am not.
[01:54:32.700 --> 01:54:34.260]   But most people are not.
[01:54:34.260 --> 01:54:37.260]   Very small percentage of people, mostly young years can hear it.
[01:54:37.260 --> 01:54:43.220]   But there's an additional feature to this app because if you've installed this app, which
[01:54:43.220 --> 01:54:49.580]   you have to, to get into the concert, it will continue to beacon at your location easily
[01:54:49.580 --> 01:54:55.900]   and expensively to receivers all over the venue so that Ticketmaster can track how much
[01:54:55.900 --> 01:54:57.780]   time do they spend at the beer counter?
[01:54:57.780 --> 01:54:59.460]   How much time do they spend in the mosh pit?
[01:54:59.460 --> 01:55:01.700]   How much time do they spend at their seat?
[01:55:01.700 --> 01:55:03.700]   That kind of thing.
[01:55:03.700 --> 01:55:09.220]   But now, I'm sure somewhere in the 15 pages up printed in tiny, tiny print on the back
[01:55:09.220 --> 01:55:12.980]   of your ticket, Ticketmaster tells you that.
[01:55:12.980 --> 01:55:14.860]   But you can't get in without it?
[01:55:14.860 --> 01:55:16.660]   I think you need to have that to get in.
[01:55:16.660 --> 01:55:17.660]   I think that's the point.
[01:55:17.660 --> 01:55:20.460]   So then you would have to get in and then delete the app if you wanted to preserve your
[01:55:20.460 --> 01:55:21.460]   privacy?
[01:55:21.460 --> 01:55:22.460]   Yeah.
[01:55:22.460 --> 01:55:23.460]   Ooh, that is prompt--
[01:55:23.460 --> 01:55:24.460]   That is prompt--
[01:55:24.460 --> 01:55:25.700]   That is a good printout of ticket.
[01:55:25.700 --> 01:55:26.940]   It's on the ticket, I'm sure.
[01:55:26.940 --> 01:55:28.980]   But it's in such fine print.
[01:55:28.980 --> 01:55:31.220]   So that's another thing.
[01:55:31.220 --> 01:55:34.260]   There is disclosure and then there's actual disclosure.
[01:55:34.260 --> 01:55:35.260]   Right.
[01:55:35.260 --> 01:55:38.340]   Well, in Europe, at least, hold on.
[01:55:38.340 --> 01:55:40.740]   I feel like I just saw a story about this this morning.
[01:55:40.740 --> 01:55:42.700]   I'm pulling it up.
[01:55:42.700 --> 01:55:47.580]   Okay, here we go.
[01:55:47.580 --> 01:55:49.100]   This was an article in Venture Beat.
[01:55:49.100 --> 01:55:52.140]   I think today, by the way, about this Ticketmaster.
[01:55:52.140 --> 01:55:53.140]   Okay.
[01:55:53.140 --> 01:55:57.420]   So this is from-- remember how I told you about my friend David, who started this connected
[01:55:57.420 --> 01:55:59.380]   writes newsletter?
[01:55:59.380 --> 01:56:03.020]   He was writing about this Facebook case, I think.
[01:56:03.020 --> 01:56:06.940]   But he said, "Consumer Watchdogs in Germany and Norway have recently been getting active
[01:56:06.940 --> 01:56:11.740]   in suing tech firms over lengthy and penetrable terms and conditions that make people sign
[01:56:11.740 --> 01:56:16.220]   away their privacy without being super clear about what will happen to their data."
[01:56:16.220 --> 01:56:19.460]   So it sounds like we need some of that action here.
[01:56:19.460 --> 01:56:20.460]   Yeah.
[01:56:20.460 --> 01:56:24.340]   I actually had a class-action lawyer a while back on my show who talked about terms and
[01:56:24.340 --> 01:56:27.620]   conditions and privacy rights.
[01:56:27.620 --> 01:56:33.660]   And the challenge is to do a class-action suit, which is one of the few ways to get people
[01:56:33.660 --> 01:56:37.580]   to pay attention to this, you have to prove harm.
[01:56:37.580 --> 01:56:41.980]   So without harm, which is the case that we're talking about, possibly what we're talking
[01:56:41.980 --> 01:56:45.420]   about with the Facebook thing, they don't really have a case.
[01:56:45.420 --> 01:56:52.740]   Now, a state attorney general, though, could come in sue on behalf of a population without--
[01:56:52.740 --> 01:56:54.700]   So that may be the future.
[01:56:54.700 --> 01:56:55.700]   I don't know.
[01:56:55.700 --> 01:57:01.460]   I mean, this is where our legal system isn't quite set up for what we're trying to make
[01:57:01.460 --> 01:57:03.180]   do with, I guess.
[01:57:03.180 --> 01:57:04.180]   That was--
[01:57:04.180 --> 01:57:09.180]   The company that Ticketmaster uses is Listener, L-I-S-N-R.
[01:57:09.180 --> 01:57:11.260]   And apparently does this for other companies.
[01:57:11.260 --> 01:57:13.900]   Land Rover uses it to have their cars communicate.
[01:57:13.900 --> 01:57:22.700]   Here's the exciting video that Ticketmaster created to share how wonderful this new Listener
[01:57:22.700 --> 01:57:25.100]   technology is going to be.
[01:57:25.100 --> 01:57:28.140]   Well, it's unfortunately doesn't-- it's all music.
[01:57:28.140 --> 01:57:31.180]   There's text.
[01:57:31.180 --> 01:57:36.220]   The ticket, an innovation that hasn't changed in 30 years.
[01:57:36.220 --> 01:57:37.220]   But it's about two.
[01:57:37.220 --> 01:57:42.420]   Two years ago, Ticketmaster set out to reimagine how a ticket could personalize the experience
[01:57:42.420 --> 01:57:44.820]   according to the identity of the fan.
[01:57:44.820 --> 01:57:51.020]   By the way, you can't pirate these tickets and it makes it hard to sell it in anything
[01:57:51.020 --> 01:57:58.340]   but an approved venue, which I think has a little bit to do with this as well.
[01:57:58.340 --> 01:58:01.620]   So replacing the paper ticket with sound.
[01:58:01.620 --> 01:58:05.740]   I wonder how easily you can hack it.
[01:58:05.740 --> 01:58:06.740]   Yeah.
[01:58:06.740 --> 01:58:07.740]   Wouldn't that be cool?
[01:58:07.740 --> 01:58:08.740]   That would be.
[01:58:08.740 --> 01:58:09.740]   Yeah.
[01:58:09.740 --> 01:58:11.660]   I mean, because if it's just a frequency, isn't it easy to replicate?
[01:58:11.660 --> 01:58:13.820]   I guess it's frequencies in a several tones.
[01:58:13.820 --> 01:58:14.820]   It's a code in that frequency.
[01:58:14.820 --> 01:58:15.820]   Yeah.
[01:58:15.820 --> 01:58:18.460]   But you copy the code.
[01:58:18.460 --> 01:58:21.300]   Ticketmaster says the rollout has already begun in some venues.
[01:58:21.300 --> 01:58:25.820]   It will take around four years to complete the process globally.
[01:58:25.820 --> 01:58:34.660]   And so just so you know, if you're a Ticketmaster user that there may be more going on, if
[01:58:34.660 --> 01:58:38.020]   you walk up to the door and it says, "Oh, hi, Leo.
[01:58:38.020 --> 01:58:39.100]   We've been waiting for you.
[01:58:39.100 --> 01:58:40.780]   Your seat is ready."
[01:58:40.780 --> 01:58:45.220]   Then you know you've got this new technology built in.
[01:58:45.220 --> 01:58:48.420]   Each smart tone carries its unique identifier meaning venues can install smart tone.
[01:58:48.420 --> 01:58:53.140]   Scanners around a space to track giggoers as they move around.
[01:58:53.140 --> 01:58:56.620]   It's not only arms venues with a vast swath of geolocation data.
[01:58:56.620 --> 01:59:02.940]   It lets them target individuals with tailored messages and "deeply personalized experiences."
[01:59:02.940 --> 01:59:04.780]   AKA ads.
[01:59:04.780 --> 01:59:05.780]   Deeply personalized.
[01:59:05.780 --> 01:59:06.780]   Ticket.
[01:59:06.780 --> 01:59:07.780]   I see you've been in the bathroom for 30 minutes.
[01:59:07.780 --> 01:59:17.220]   Maybe you should lay off the hot sauce.
[01:59:17.220 --> 01:59:20.140]   This means this is what EVP a product to Ticketmaster says.
[01:59:20.140 --> 01:59:24.300]   This means using identity to drive customized experiences based on who you are, where you
[01:59:24.300 --> 01:59:30.060]   are, eliminating fraud, resulting in a safer environment and delivering more personalization
[01:59:30.060 --> 01:59:32.140]   based on the specific event you're attending.
[01:59:32.140 --> 01:59:36.900]   I'm going to get a bet that doesn't mean the band is going to play more of your requests.
[01:59:36.900 --> 01:59:37.900]   Wow.
[01:59:37.900 --> 01:59:41.500]   So you're going to see more of the shopping malls, shoppers.
[01:59:41.500 --> 01:59:43.060]   It's an interesting idea.
[01:59:43.060 --> 01:59:46.260]   You don't have to use Bluetooth or be kidding.
[01:59:46.260 --> 01:59:47.860]   You can just do it.
[01:59:47.860 --> 01:59:48.860]   We know this already.
[01:59:48.860 --> 01:59:52.980]   The TV speaks sometimes to your app, right?
[01:59:52.980 --> 01:59:54.780]   Why is this better than a barcode?
[01:59:54.780 --> 01:59:59.820]   It doesn't require the user to interact or even know about it.
[01:59:59.820 --> 02:00:01.940]   You just walked through the dinner?
[02:00:01.940 --> 02:00:02.940]   That's why.
[02:00:02.940 --> 02:00:07.860]   If it's a connected door, it'll shut in your face.
[02:00:07.860 --> 02:00:11.460]   Like, "Eck."
[02:00:11.460 --> 02:00:12.460]   Google didn't Google use.
[02:00:12.460 --> 02:00:14.580]   I think Google has something called Nearby that's similar.
[02:00:14.580 --> 02:00:18.220]   In fact, remember, well, Google's trying stuff like this all the time.
[02:00:18.220 --> 02:00:23.180]   For a while, you could pay it at McDonald's near Google by just smiling.
[02:00:23.180 --> 02:00:24.980]   And it would recognize you and say, "Oh, yes.
[02:00:24.980 --> 02:00:27.180]   We'll take that out of your account right now."
[02:00:27.180 --> 02:00:28.180]   "Huh.
[02:00:28.180 --> 02:00:29.500]   You're right.
[02:00:29.500 --> 02:00:30.940]   Whatever happened to this?"
[02:00:30.940 --> 02:00:31.940]   "You remember that?"
[02:00:31.940 --> 02:00:34.300]   "Hey, I'm looking at it and I'm like, "Huh.
[02:00:34.300 --> 02:00:35.300]   This is cool.
[02:00:35.300 --> 02:00:36.940]   I don't remember this at all."
[02:00:36.940 --> 02:00:39.100]   Well, it was only in Silicon Valley.
[02:00:39.100 --> 02:00:40.100]   Oh.
[02:00:40.100 --> 02:00:43.580]   So, here's a, there's a little skills gap.
[02:00:43.580 --> 02:00:46.740]   I mean, Echo has mad skills.
[02:00:46.740 --> 02:00:53.180]   I guess if I change it and say, "I mean, doesn't mean it didn't activate your, you know,
[02:00:53.180 --> 02:00:54.180]   who?"
[02:00:54.180 --> 02:00:59.540]   15,000 skills now on the Amazon Echo.
[02:00:59.540 --> 02:01:06.780]   Compare that to the number of skills the Google Assistant has, 378.
[02:01:06.780 --> 02:01:09.500]   Or Cortana has 65.
[02:01:09.500 --> 02:01:13.820]   And you can see there's a fairly large skills app.
[02:01:13.820 --> 02:01:17.260]   So, let's ask, let's do a little informal survey.
[02:01:17.260 --> 02:01:20.780]   What, how many skills do you have activated in your Echo app?
[02:01:20.780 --> 02:01:22.620]   Is there a way to go look and see that?
[02:01:22.620 --> 02:01:23.620]   Yeah.
[02:01:23.620 --> 02:01:25.260]   Well, I think let me see if it's only smart.
[02:01:25.260 --> 02:01:27.660]   Because that's really the big problem, isn't it?
[02:01:27.660 --> 02:01:29.620]   Is that people don't know the skills exist?
[02:01:29.620 --> 02:01:31.780]   It's hard to figure out what the skills are.
[02:01:31.780 --> 02:01:33.740]   It's kind of like the App Store problem.
[02:01:33.740 --> 02:01:37.220]   One of the things I like about the Echo Show and it might be one of the reasons they created
[02:01:37.220 --> 02:01:42.820]   a screen is the screen will tell you about skills as part of the, you know, rotator.
[02:01:42.820 --> 02:01:44.580]   It tells you things you can do.
[02:01:44.580 --> 02:01:47.260]   So you can go into your skills.
[02:01:47.260 --> 02:01:51.580]   If you go to skills, there's a button called your skills and then you can see how many
[02:01:51.580 --> 02:01:52.580]   you have.
[02:01:52.580 --> 02:01:55.180]   All right, let's go into skills.
[02:01:55.180 --> 02:01:57.260]   How many skills do you have Stacy?
[02:01:57.260 --> 02:01:58.260]   I'm counting, 12.
[02:01:58.260 --> 02:02:00.020]   Oh, you have to count them.
[02:02:00.020 --> 02:02:01.020]   16.
[02:02:01.020 --> 02:02:04.060]   I have 25.
[02:02:04.060 --> 02:02:06.060]   I have 25.
[02:02:06.060 --> 02:02:15.460]   I have four of FART, 60 B, seven minute workout, which I have never used.
[02:02:15.460 --> 02:02:16.460]   I've used it.
[02:02:16.460 --> 02:02:18.140]   Not even for one, for seven minutes?
[02:02:18.140 --> 02:02:19.820]   Not even once.
[02:02:19.820 --> 02:02:27.820]   Abra, which I think is the one where it takes, your Echo can give your smart home commands
[02:02:27.820 --> 02:02:30.860]   using spells from Harry Potter, I think.
[02:02:30.860 --> 02:02:31.860]   Remember that one?
[02:02:31.860 --> 02:02:32.860]   Oh, yeah.
[02:02:32.860 --> 02:02:33.860]   I do remember.
[02:02:33.860 --> 02:02:34.860]   That one.
[02:02:34.860 --> 02:02:37.700]   Yeah, AccuWeather admirer.
[02:02:37.700 --> 02:02:39.820]   I can tell my admirer to make me smile.
[02:02:39.820 --> 02:02:40.820]   I don't know.
[02:02:40.820 --> 02:02:48.660]   Age calculator, things to try ambient noise of several kinds, thunder, rain and ocean.
[02:02:48.660 --> 02:02:50.780]   Any moat, I don't even know what that is.
[02:02:50.780 --> 02:02:53.140]   Oh, ask any moat to play my sonos.
[02:02:53.140 --> 02:02:56.140]   It's some sort of smart remote for sonos.
[02:02:56.140 --> 02:02:57.140]   Earthquakes.
[02:02:57.140 --> 02:02:58.860]   God, this is, I'm just in the A's.
[02:02:58.860 --> 02:02:59.860]   I'm not counting these.
[02:02:59.860 --> 02:03:00.860]   Okay.
[02:03:00.860 --> 02:03:01.860]   There's a lot of them.
[02:03:01.860 --> 02:03:02.860]   Yeah.
[02:03:02.860 --> 02:03:03.860]   There's a lot.
[02:03:03.860 --> 02:03:04.860]   Well, I try them.
[02:03:04.860 --> 02:03:05.860]   I try them.
[02:03:05.860 --> 02:03:06.860]   I try everything.
[02:03:06.860 --> 02:03:08.460]   But most people probably have a handful.
[02:03:08.460 --> 02:03:09.460]   No.
[02:03:09.460 --> 02:03:11.780]   If you go through your skills and delete skills, you don't use.
[02:03:11.780 --> 02:03:13.260]   I'm doing it right now.
[02:03:13.260 --> 02:03:14.260]   Why?
[02:03:14.260 --> 02:03:15.260]   I have a skill.
[02:03:15.260 --> 02:03:17.340]   Here's a resistor decoder skill.
[02:03:17.340 --> 02:03:21.860]   So you get a resistor and you read it, the colored rings and I'll tell you what kind
[02:03:21.860 --> 02:03:23.940]   of resistor it is.
[02:03:23.940 --> 02:03:24.940]   That's useful.
[02:03:24.940 --> 02:03:25.940]   Oh, yeah.
[02:03:25.940 --> 02:03:29.940]   That's a 43 ohm resistor there you got there.
[02:03:29.940 --> 02:03:30.940]   Petaluma patch.
[02:03:30.940 --> 02:03:32.260]   NPR story of the day.
[02:03:32.260 --> 02:03:34.460]   A lot of these are flash briefing.
[02:03:34.460 --> 02:03:37.100]   Math puzzles, magic eight ball.
[02:03:37.100 --> 02:03:38.620]   Do you have, would you rather?
[02:03:38.620 --> 02:03:40.260]   Oh, that might be my tip of the week.
[02:03:40.260 --> 02:03:41.660]   I hate would you rather.
[02:03:41.660 --> 02:03:42.660]   Oh my God.
[02:03:42.660 --> 02:03:43.660]   My daughter loves.
[02:03:43.660 --> 02:03:45.740]   I hate that game.
[02:03:45.740 --> 02:03:47.860]   It's all hypotheticals for one thing.
[02:03:47.860 --> 02:03:49.020]   Oh, it's perfect.
[02:03:49.020 --> 02:03:50.020]   You're a radio show host.
[02:03:50.020 --> 02:03:51.020]   What are you talking about?
[02:03:51.020 --> 02:03:52.540]   Oh, what do we do?
[02:03:52.540 --> 02:03:56.500]   We argued forever about hypothetical.
[02:03:56.500 --> 02:03:57.500]   That's a good point.
[02:03:57.500 --> 02:03:58.500]   This is my life.
[02:03:58.500 --> 02:04:00.620]   Hypotheticals are my life.
[02:04:00.620 --> 02:04:06.500]   Well, it's not hypothetical 15,069 skills at last count.
[02:04:06.500 --> 02:04:08.780]   I thought, the last thing I remember was 5,000.
[02:04:08.780 --> 02:04:10.460]   So it's just growing like Toxie.
[02:04:10.460 --> 02:04:12.860]   Yeah, they had 10,000 I think in February.
[02:04:12.860 --> 02:04:13.860]   Yeah.
[02:04:13.860 --> 02:04:15.540]   Nobody's making money on those, right?
[02:04:15.540 --> 02:04:16.540]   You can.
[02:04:16.540 --> 02:04:19.100]   Only on the game developer once.
[02:04:19.100 --> 02:04:20.700]   And it's unclear how that works.
[02:04:20.700 --> 02:04:23.660]   You have to apply and talk to the Amazon people.
[02:04:23.660 --> 02:04:29.740]   Well, speaking of which, remember that Prime Day is coming, the third annual.
[02:04:29.740 --> 02:04:31.020]   And there will be a skylight.
[02:04:31.020 --> 02:04:35.300]   I'm sorry, Echo voice deals for Prime Day.
[02:04:35.300 --> 02:04:36.300]   So get ready.
[02:04:36.300 --> 02:04:38.300]   Prime Day is July 11th.
[02:04:38.300 --> 02:04:40.140]   It's a week from West Hill.
[02:04:40.140 --> 02:04:41.140]   Huh?
[02:04:41.140 --> 02:04:43.940]   Well, you can ask Echo.
[02:04:43.940 --> 02:04:48.180]   God, you can ask Echo, what are your deals?
[02:04:48.180 --> 02:04:50.780]   And then they will give you some.
[02:04:50.780 --> 02:04:53.660]   And then you could say, order that.
[02:04:53.660 --> 02:04:55.300]   That's dangerous.
[02:04:55.300 --> 02:04:59.700]   And voice shoppers will get access to special deals two hours before the main event.
[02:04:59.700 --> 02:05:03.020]   It begins at 6 p.m. Pacific on July 11th.
[02:05:03.020 --> 02:05:04.740]   Did you buy anything in the last Prime Day?
[02:05:04.740 --> 02:05:05.740]   I didn't.
[02:05:05.740 --> 02:05:06.740]   You know, I really resisted.
[02:05:06.740 --> 02:05:07.740]   It's nothing there.
[02:05:07.740 --> 02:05:11.260]   I try to resist stuff like that because I feel like I'll buy things I don't need.
[02:05:11.260 --> 02:05:15.020]   Like I never do that.
[02:05:15.020 --> 02:05:20.300]   Like all of a sudden 16 Yodel pickles will arrive.
[02:05:20.300 --> 02:05:21.780]   That's happening, Leo.
[02:05:21.780 --> 02:05:26.260]   You don't realize it, but the Dash 1 does like, oh, Yodel pickles for everybody.
[02:05:26.260 --> 02:05:29.300]   Okay, but whatever you do, don't order the sugar-free gummy bears.
[02:05:29.300 --> 02:05:31.300]   Okay, I'm just saying.
[02:05:31.300 --> 02:05:32.300]   Right.
[02:05:32.300 --> 02:05:35.820]   No, Ted.
[02:05:35.820 --> 02:05:41.100]   And by the way, if you're not a Prime subscriber yet, this is a good time to become a subscriber
[02:05:41.100 --> 02:05:42.100]   to Prime.
[02:05:42.100 --> 02:05:46.860]   You get $20 off, $79 a year if you enable.
[02:05:46.860 --> 02:05:48.380]   And you can do that with your Echo as well.
[02:05:48.380 --> 02:05:49.700]   You just say, sign me up for that.
[02:05:49.700 --> 02:05:52.860]   Oh, then it's like Prime cost, what it costs when I first signed.
[02:05:52.860 --> 02:05:53.860]   Yeah, exactly.
[02:05:53.860 --> 02:05:54.860]   That's what I was talking about.
[02:05:54.860 --> 02:05:55.860]   Most many years ago.
[02:05:55.860 --> 02:05:56.860]   Yep.
[02:05:56.860 --> 02:06:01.660]   And $10 credit for any orders purchased before July 10th, if you've never used voice shopping
[02:06:01.660 --> 02:06:03.660]   before.
[02:06:03.660 --> 02:06:09.660]   So wow, this is just, they're just going to take over the world.
[02:06:09.660 --> 02:06:17.900]   They are just going to take this as the deals with, they call it deals with the A word.
[02:06:17.900 --> 02:06:20.420]   We have renamed her Madam A on our.
[02:06:20.420 --> 02:06:21.420]   Madam A.
[02:06:21.420 --> 02:06:22.420]   Good.
[02:06:22.420 --> 02:06:23.420]   Can we can we steal that?
[02:06:23.420 --> 02:06:24.420]   You're welcome to it.
[02:06:24.420 --> 02:06:27.780]   I'm going to let my plug Stacy and I out T you don't have to, I mean, you don't have
[02:06:27.780 --> 02:06:32.460]   to plug the show, but yeah, just it's a little easier because you want to have a person.
[02:06:32.460 --> 02:06:34.980]   Yeah, I call it Echo.
[02:06:34.980 --> 02:06:38.800]   I don't think that's too hard to do, but it's no, it's not.
[02:06:38.800 --> 02:06:39.800]   Yeah.
[02:06:39.800 --> 02:06:42.740]   You have seen this on a live Echo for old people, haven't you?
[02:06:42.740 --> 02:06:43.740]   Oh, love it.
[02:06:43.740 --> 02:06:44.740]   Oh, that was so great.
[02:06:44.740 --> 02:06:45.740]   We played it on the show.
[02:06:45.740 --> 02:06:46.740]   You must know, we did play on the show.
[02:06:46.740 --> 02:06:47.740]   That's right.
[02:06:47.740 --> 02:06:48.740]   It's just brilliant, just brilliant.
[02:06:48.740 --> 02:06:49.740]   I love it.
[02:06:49.740 --> 02:06:53.140]   You know what would have made it more perfect is if you were in the commercial Jeff.
[02:06:53.140 --> 02:06:55.140]   I'm not old.
[02:06:55.140 --> 02:06:56.140]   I'm cruising.
[02:06:56.140 --> 02:07:04.820]   You know that Jeff, pretty much the same age.
[02:07:04.820 --> 02:07:05.820]   You know that right.
[02:07:05.820 --> 02:07:10.940]   So anything you say bad about Jeff stick bounces off him and sticks to me.
[02:07:10.940 --> 02:07:14.780]   Well, that'll do it until I get my patch legal button.
[02:07:14.780 --> 02:07:17.060]   I get off my virtual porch.
[02:07:17.060 --> 02:07:20.460]   So Bixby, Bixby, the Bixby button.
[02:07:20.460 --> 02:07:21.860]   What a mess up this is.
[02:07:21.860 --> 02:07:26.860]   I have a dedicated button on my Samsung Galaxy S8 that not only doesn't do anything much
[02:07:26.860 --> 02:07:29.460]   now, it won't for the foreseeable future.
[02:07:29.460 --> 02:07:30.460]   They're having more.
[02:07:30.460 --> 02:07:34.060]   They're making a big voice product for, for who?
[02:07:34.060 --> 02:07:35.380]   Well, that's the for Korea.
[02:07:35.380 --> 02:07:36.380]   That's the problem.
[02:07:36.380 --> 02:07:39.140]   Bixby works fine in Korean.
[02:07:39.140 --> 02:07:41.100]   Bixby is the smart intelligence additional.
[02:07:41.100 --> 02:07:45.980]   Like I need an extra on any Android phone, anything besides Google Assistant.
[02:07:45.980 --> 02:07:47.660]   Bixby is Samsung's version.
[02:07:47.660 --> 02:07:49.940]   Like they do on all their, you know, they always do this.
[02:07:49.940 --> 02:07:52.300]   They add, you know, oh, it's not enough just to have Google voice.
[02:07:52.300 --> 02:07:54.020]   You have to have Samsung voice or Google pay.
[02:07:54.020 --> 02:07:56.900]   You have to have Samsung pay.
[02:07:56.900 --> 02:08:00.500]   The English version will be delayed again because it lacks enough big data to teach
[02:08:00.500 --> 02:08:01.500]   it to work.
[02:08:01.500 --> 02:08:03.540]   Nobody's using it.
[02:08:03.540 --> 02:08:04.540]   How are they going to?
[02:08:04.540 --> 02:08:08.220]   It was supposed to launch in April, then push to spring, then to June.
[02:08:08.220 --> 02:08:10.140]   They have a beta version out now.
[02:08:10.140 --> 02:08:12.700]   In fact, Florence Ion showed us on the new screens.
[02:08:12.700 --> 02:08:14.260]   There was a couple of weeks ago.
[02:08:14.260 --> 02:08:19.900]   It's mostly, at least in its current incarnation in English used to launch apps and you can
[02:08:19.900 --> 02:08:23.900]   actually do things like crop pictures and stuff with your voice.
[02:08:23.900 --> 02:08:28.020]   So it doesn't really overlap that much with Google Assistant.
[02:08:28.020 --> 02:08:32.660]   But it's just annoying that they put a physical button on there.
[02:08:32.660 --> 02:08:33.660]   That does nothing.
[02:08:33.660 --> 02:08:36.020]   Maybe they can make it yodel.
[02:08:36.020 --> 02:08:40.700]   That would make it a very expensive yodeling pickle.
[02:08:40.700 --> 02:08:42.180]   Exactly.
[02:08:42.180 --> 02:08:45.660]   And yeah, they're doing that a Bixby powered smart speaker.
[02:08:45.660 --> 02:08:50.340]   As is, by the way, I think Baidu is doing one for China, likely just released it.
[02:08:50.340 --> 02:08:51.820]   But these are for those markets.
[02:08:51.820 --> 02:08:54.980]   So the Korean Bixby does work.
[02:08:54.980 --> 02:08:55.980]   It's getting up a language.
[02:08:55.980 --> 02:08:58.340]   Okay, so the challenge is it doesn't speak English.
[02:08:58.340 --> 02:08:59.340]   It's getting up a language.
[02:08:59.340 --> 02:09:03.780]   And so I presume the Bixby will be Korean only.
[02:09:03.780 --> 02:09:05.100]   I didn't know this happened.
[02:09:05.100 --> 02:09:07.340]   That just happened, I guess.
[02:09:07.340 --> 02:09:12.780]   The laptop ban on passengers flying Eddy Hodd, Emirates and Turkish Airlines has now been
[02:09:12.780 --> 02:09:13.780]   lifted.
[02:09:13.780 --> 02:09:14.780]   Oh.
[02:09:14.780 --> 02:09:15.780]   Yeah.
[02:09:15.780 --> 02:09:20.540]   Six other airlines still are subject to the ban.
[02:09:20.540 --> 02:09:22.780]   Did they change their screening policies?
[02:09:22.780 --> 02:09:23.780]   Yeah.
[02:09:23.780 --> 02:09:24.780]   Okay.
[02:09:24.780 --> 02:09:27.380]   I forget what they said.
[02:09:27.380 --> 02:09:28.380]   Oh, I read the story.
[02:09:28.380 --> 02:09:30.700]   I can't remember what they're doing.
[02:09:30.700 --> 02:09:36.020]   So the ban remember was started in March covering flights to US destinations from 10 airports
[02:09:36.020 --> 02:09:38.300]   in the Middle East.
[02:09:38.300 --> 02:09:45.460]   And on only Middle Eastern airlines of all electronic devices, including laptops, tablets
[02:09:45.460 --> 02:09:54.380]   and e-readers, you couldn't bring your Kindle on either.
[02:09:54.380 --> 02:09:59.540]   The ban was obviously difficult to carry out.
[02:09:59.540 --> 02:10:03.820]   Individual boxing of every single electronic device traveling with a passenger at the gate,
[02:10:03.820 --> 02:10:06.220]   things like that.
[02:10:06.220 --> 02:10:16.140]   So anyway, so this article, TechCrunch doesn't say what changed.
[02:10:16.140 --> 02:10:17.220]   I forget.
[02:10:17.220 --> 02:10:18.220]   But that's good.
[02:10:18.220 --> 02:10:24.380]   Maybe they won't do the generalized laptop ban that they were talking about.
[02:10:24.380 --> 02:10:29.660]   The Afghanistan, this is a story that'll cheer you up.
[02:10:29.660 --> 02:10:34.660]   The all-girl robotics team, this is first, which is great.
[02:10:34.660 --> 02:10:40.900]   And for the first time ever, Afghanistan had fielded a first team.
[02:10:40.900 --> 02:10:43.500]   It was all women.
[02:10:43.500 --> 02:10:47.980]   They tried six Afghan inventors trying to come to the US for the first global challenge
[02:10:47.980 --> 02:10:50.100]   in DC mid-July.
[02:10:50.100 --> 02:10:56.540]   They couldn't get permission from the United States to interview for their visas.
[02:10:56.540 --> 02:10:59.780]   The girls risk a 500 mile trek cross country.
[02:10:59.780 --> 02:11:00.780]   Shh.
[02:11:00.780 --> 02:11:03.380]   The American Embassy in Kabul.
[02:11:03.380 --> 02:11:04.620]   That's a dangerous place.
[02:11:04.620 --> 02:11:07.780]   There've been a number of recent suicide attacks in a truck bomb.
[02:11:07.780 --> 02:11:10.060]   Yeah, it's dangerous just to be a woman there.
[02:11:10.060 --> 02:11:13.300]   It's dangerous to be a woman who's interested in getting educated.
[02:11:13.300 --> 02:11:17.900]   It was dangerous to travel 500 miles across the nation as a woman.
[02:11:17.900 --> 02:11:21.940]   And they did it not once, but twice.
[02:11:21.940 --> 02:11:27.020]   But we're denied the visa.
[02:11:27.020 --> 02:11:34.260]   So I think, though, that the teams from Iraq, Iran, and Sudan were able to secure travel
[02:11:34.260 --> 02:11:35.820]   visas.
[02:11:35.820 --> 02:11:43.620]   Only Afghanistan, Team Afghanistan, and Team Gambia have been denied visas so far.
[02:11:43.620 --> 02:11:47.820]   It's kind of too bad.
[02:11:47.820 --> 02:11:49.460]   Yeah.
[02:11:49.460 --> 02:11:54.260]   You know, we're going to, we're already seeing tourism down.
[02:11:54.260 --> 02:11:58.540]   What happens when we stop hosting events like this?
[02:11:58.540 --> 02:12:00.060]   Because people can't get in here.
[02:12:00.060 --> 02:12:01.860]   Because they can't travel here.
[02:12:01.860 --> 02:12:02.860]   Yeah.
[02:12:02.860 --> 02:12:08.700]   So I held a summit at CUNY right after the ban was announced.
[02:12:08.700 --> 02:12:13.900]   And there was the head of technology for the largest publisher in Scandinavia.
[02:12:13.900 --> 02:12:16.820]   It couldn't come because it was born and Iran.
[02:12:16.820 --> 02:12:17.820]   That's ridiculous.
[02:12:17.820 --> 02:12:19.780]   It is indeed.
[02:12:19.780 --> 02:12:20.780]   Yeah.
[02:12:20.780 --> 02:12:21.780]   This is our world now.
[02:12:21.780 --> 02:12:26.340]   We make strangers, the enemy, no matter who they are.
[02:12:26.340 --> 02:12:32.860]   So I'm disappointed to say that Instagram is now cracking down on fake influencers.
[02:12:32.860 --> 02:12:38.500]   So that's going to really reduce my ability to sell health teas and makeup.
[02:12:38.500 --> 02:12:42.100]   Oh, there goes my makeup blog.
[02:12:42.100 --> 02:12:43.100]   Yep.
[02:12:43.100 --> 02:12:44.100]   Sorry.
[02:12:44.100 --> 02:12:45.100]   Dang it.
[02:12:45.100 --> 02:12:46.100]   Sorry, Stace.
[02:12:46.100 --> 02:12:48.620]   That was worried.
[02:12:48.620 --> 02:12:50.100]   My van life blog.
[02:12:50.100 --> 02:12:51.700]   What is a fake influencer?
[02:12:51.700 --> 02:12:53.700]   I would like to know.
[02:12:53.700 --> 02:12:55.900]   What is a real influencer?
[02:12:55.900 --> 02:12:56.900]   I mean, really.
[02:12:56.900 --> 02:12:57.900]   A Kardashian.
[02:12:57.900 --> 02:12:59.900]   It's a real influencer.
[02:12:59.900 --> 02:13:04.180]   I got a name tag that called me an influencer.
[02:13:04.180 --> 02:13:05.700]   And I was like, I'm sorry.
[02:13:05.700 --> 02:13:06.700]   What?
[02:13:06.700 --> 02:13:07.700]   No, I'm not sure.
[02:13:07.700 --> 02:13:13.020]   Turns out there were sites like InstaGrace, InstaPlus and PureBoost, which third-party
[02:13:13.020 --> 02:13:18.580]   sites which help you boost the number of followers, not for real, obviously.
[02:13:18.580 --> 02:13:22.580]   And that violates Instagram's guidelines.
[02:13:22.580 --> 02:13:24.540]   They want to be influencers.
[02:13:24.540 --> 02:13:29.060]   Use these types of services to automate generic comments and likes.
[02:13:29.060 --> 02:13:33.380]   You pay for the number of likes and comments you want and a bot takes care of the rest.
[02:13:33.380 --> 02:13:40.340]   So brands, I guess, see all those likes and loves and comments and say, oh, this is an
[02:13:40.340 --> 02:13:41.340]   influencer.
[02:13:41.340 --> 02:13:47.180]   We ought to give her a free Bose speaker system or whatever it is.
[02:13:47.180 --> 02:13:48.180]   It was huge at it.
[02:13:48.180 --> 02:13:50.020]   You see the two nothing but this now.
[02:13:50.020 --> 02:13:51.020]   Yeah.
[02:13:51.020 --> 02:13:52.020]   Yeah.
[02:13:52.020 --> 02:13:59.060]   If I search for influencer in my inbox, I bet I have a bunch of crazy emails.
[02:13:59.060 --> 02:14:00.060]   So yeah.
[02:14:00.060 --> 02:14:01.780]   You're an influencer, I bet.
[02:14:01.780 --> 02:14:03.860]   How many Instagram followers do you have?
[02:14:03.860 --> 02:14:05.260]   I don't have an Instagram.
[02:14:05.260 --> 02:14:07.900]   I mean, I do have an Instagram, but I don't post anything on it.
[02:14:07.900 --> 02:14:09.900]   I'm ancient.
[02:14:09.900 --> 02:14:11.180]   You guys joke that you're old.
[02:14:11.180 --> 02:14:13.500]   I am like, oh no, we know old.
[02:14:13.500 --> 02:14:14.740]   You know, I'm old.
[02:14:14.740 --> 02:14:16.220]   I'm not social.
[02:14:16.220 --> 02:14:17.780]   So it's terrible.
[02:14:17.780 --> 02:14:20.260]   You may be cranky, but you're not old.
[02:14:20.260 --> 02:14:23.500]   Good article last year in Bloomberg by Max Chafkin.
[02:14:23.500 --> 02:14:24.500]   He says Instagram.
[02:14:24.500 --> 02:14:25.500]   Oh wait a minute.
[02:14:25.500 --> 02:14:28.660]   Let me pause this auto start video.
[02:14:28.660 --> 02:14:30.540]   Confessions of an Instagram influencer.
[02:14:30.540 --> 02:14:32.020]   I used to post cat photos.
[02:14:32.020 --> 02:14:35.900]   Then a marketing agency made me a star.
[02:14:35.900 --> 02:14:38.820]   Oh, that was a great story.
[02:14:38.820 --> 02:14:39.820]   I remember this.
[02:14:39.820 --> 02:14:40.820]   I know.
[02:14:40.820 --> 02:14:41.820]   Remember this?
[02:14:41.820 --> 02:14:42.820]   Yes.
[02:14:42.820 --> 02:14:49.100]   So you can get $10,000 for one Instagram post.
[02:14:49.100 --> 02:14:55.300]   Of course, the FTC says now you have to really use hashtag ad right at the front and stuff
[02:14:55.300 --> 02:14:56.300]   like that.
[02:14:56.300 --> 02:14:59.740]   Do you think that that diminishes the value because now people don't think you really
[02:14:59.740 --> 02:15:01.380]   like something you're just doing?
[02:15:01.380 --> 02:15:02.380]   Sure.
[02:15:02.380 --> 02:15:03.620]   I think so.
[02:15:03.620 --> 02:15:05.380]   Yeah.
[02:15:05.380 --> 02:15:07.100]   And that's kind of one of the pro.
[02:15:07.100 --> 02:15:15.140]   I mean, I remember when I was like probably in my teens reading a science fiction story
[02:15:15.140 --> 02:15:22.060]   where the predicted basically today, which was like this, the setting was a time when
[02:15:22.060 --> 02:15:27.860]   people would get credit for liking things like, Oh, I love this scarf by so and so.
[02:15:27.860 --> 02:15:30.940]   And no one actually liked anything anymore.
[02:15:30.940 --> 02:15:33.420]   So I follow a good friend.
[02:15:33.420 --> 02:15:34.820]   I just Dean love her.
[02:15:34.820 --> 02:15:36.460]   She's a definitely influencer.
[02:15:36.460 --> 02:15:39.620]   And I was following her Instagram and I saw this post.
[02:15:39.620 --> 02:15:41.660]   She's talking about, Oh, her amazing.
[02:15:41.660 --> 02:15:42.660]   Look at this TV.
[02:15:42.660 --> 02:15:46.620]   It's like a wallpaper on TV and it really is cool.
[02:15:46.620 --> 02:15:49.380]   And then I read the post and said this TV is so insane.
[02:15:49.380 --> 02:15:56.300]   Links in bio for unboxing and set up hashtag L G L L L L TV hashtag ad.
[02:15:56.300 --> 02:15:57.780]   And you're right.
[02:15:57.780 --> 02:16:00.620]   It kind of it kind of soured me on this.
[02:16:00.620 --> 02:16:02.900]   It's like, this isn't a real post.
[02:16:02.900 --> 02:16:05.860]   This is an ad in my Instagram feed.
[02:16:05.860 --> 02:16:10.340]   She got 79,000 views and I bet you she got a sides game, the TV for free.
[02:16:10.340 --> 02:16:12.260]   She probably got some good money out of that.
[02:16:12.260 --> 02:16:14.940]   No, I love just Dean and more power to her.
[02:16:14.940 --> 02:16:19.380]   I don't have a problem doing that at all because she's hashtag added and everything.
[02:16:19.380 --> 02:16:26.580]   But it does seem to me that the brand is getting less a little bit less because it's an ad.
[02:16:26.580 --> 02:16:32.380]   Yeah, but it's made to I mean, that's we talk about that in journalism all the time.
[02:16:32.380 --> 02:16:35.860]   The whole goal of advertisers right now is to provide something that looks as much like
[02:16:35.860 --> 02:16:37.020]   content as possible.
[02:16:37.020 --> 02:16:38.020]   Trick you.
[02:16:38.020 --> 02:16:39.020]   Yeah.
[02:16:39.020 --> 02:16:41.260]   Well, we've had a lot of fun and I am starving now.
[02:16:41.260 --> 02:16:46.140]   So I want to just go have this can of Dinty more hardy hardy beef stew.
[02:16:46.140 --> 02:16:49.660]   This is everything you want in a can of stew and more.
[02:16:49.660 --> 02:16:55.740]   I used to I used to get my mom fed me that.
[02:16:55.740 --> 02:16:56.740]   I've never.
[02:16:56.740 --> 02:16:57.740]   I've never.
[02:16:57.740 --> 02:16:58.740]   Can it is.
[02:16:58.740 --> 02:16:59.740]   It is nasty.
[02:16:59.740 --> 02:17:02.740]   Yeah, still.
[02:17:02.740 --> 02:17:05.500]   She's she was a working mom.
[02:17:05.500 --> 02:17:07.580]   So it's totally totally.
[02:17:07.580 --> 02:17:08.580]   Yeah.
[02:17:08.580 --> 02:17:09.580]   Yeah.
[02:17:09.580 --> 02:17:11.540]   Bless her for feeding me at all.
[02:17:11.540 --> 02:17:12.540]   She fed you.
[02:17:12.540 --> 02:17:16.420]   That's you know, she's still her obligation right there.
[02:17:16.420 --> 02:17:18.260]   And look at you.
[02:17:18.260 --> 02:17:19.580]   She you're strong.
[02:17:19.580 --> 02:17:20.580]   You're healthy.
[02:17:20.580 --> 02:17:22.580]   You have migraines is probably why.
[02:17:22.580 --> 02:17:26.060]   I blame the hamburger helper and did to more beef stew.
[02:17:26.060 --> 02:17:27.060]   I love hamburger.
[02:17:27.060 --> 02:17:29.420]   That's a healthy helper.
[02:17:29.420 --> 02:17:31.820]   That's still a tree.
[02:17:31.820 --> 02:17:33.060]   It is not good for you.
[02:17:33.060 --> 02:17:34.420]   It's a guilty pleasure.
[02:17:34.420 --> 02:17:37.180]   Hey, I eat a lot of stuff that's not good for me.
[02:17:37.180 --> 02:17:39.340]   It's county fair time.
[02:17:39.340 --> 02:17:42.420]   And and you know, I told dogs.
[02:17:42.420 --> 02:17:43.740]   Oh, exactly.
[02:17:43.740 --> 02:17:48.740]   You know, I told Lisa the only reason I go to the fair is for fair food.
[02:17:48.740 --> 02:17:51.860]   And I think I don't know if she posted it, but there's a picture of me somewhere with
[02:17:51.860 --> 02:17:54.300]   a with a corn dog this big.
[02:17:54.300 --> 02:17:56.900]   How do you feel about funny?
[02:17:56.900 --> 02:17:58.740]   The funnel cake.
[02:17:58.740 --> 02:17:59.740]   Love it.
[02:17:59.740 --> 02:18:00.740]   Love it.
[02:18:00.740 --> 02:18:01.740]   Yeah.
[02:18:01.740 --> 02:18:03.380]   And then there's a guy sells fudge there.
[02:18:03.380 --> 02:18:05.380]   That's just amazing.
[02:18:05.380 --> 02:18:07.660]   And you know, it's talk about guilty pleasure.
[02:18:07.660 --> 02:18:12.940]   I just I feel terrible for an Al Franken put up a link to the Minnesota State Fair.
[02:18:12.940 --> 02:18:17.820]   And they've gone all fancy dancing with the state fair food was amazing and truffles
[02:18:17.820 --> 02:18:18.820]   and God knows.
[02:18:18.820 --> 02:18:20.020]   It's funny because Minnesota State fairs.
[02:18:20.020 --> 02:18:25.660]   I think where I first learned of the deep fried Oreo and Snickers and all of that.
[02:18:25.660 --> 02:18:29.580]   Yeah, it looks like Lisa never posted the picture of me in the giant hot tub.
[02:18:29.580 --> 02:18:32.060]   It was a favor to you.
[02:18:32.060 --> 02:18:33.060]   Thank you.
[02:18:33.060 --> 02:18:35.700]   Probably because it really would have been compromising.
[02:18:35.700 --> 02:18:41.980]   Is that what you like at the county at the state fairs?
[02:18:41.980 --> 02:18:42.980]   Jeff is corn dogs.
[02:18:42.980 --> 02:18:45.020]   I used to when I was a kid.
[02:18:45.020 --> 02:18:46.020]   They're so good.
[02:18:46.020 --> 02:18:49.620]   Fulton County Fair and Fulton County, Illinois.
[02:18:49.620 --> 02:18:50.620]   So good.
[02:18:50.620 --> 02:18:52.740]   I do the turkey legs.
[02:18:52.740 --> 02:18:53.740]   Oh, yeah.
[02:18:53.740 --> 02:18:55.500]   The big Kenry the eighth turkey leg.
[02:18:55.500 --> 02:19:00.780]   And you have to you have to make noise and wave it around drunken, angry, big man.
[02:19:00.780 --> 02:19:07.900]   It is basically it is basically a club made of meat.
[02:19:07.900 --> 02:19:11.660]   Stacy share something cool and neat that you just got with us.
[02:19:11.660 --> 02:19:12.660]   Let's get Stacy.
[02:19:12.660 --> 02:19:18.240]   Oh, so instead of a pick, I was actually going to revisit some of my old picks after
[02:19:18.240 --> 02:19:19.740]   they've lived in my life.
[02:19:19.740 --> 02:19:22.140]   That's actually a good idea.
[02:19:22.140 --> 02:19:23.420]   Tell you what I think.
[02:19:23.420 --> 02:19:24.980]   What's worth it and what's not.
[02:19:24.980 --> 02:19:32.900]   So a couple of weeks ago, I told you about the Levitan light switch that was Madam A enabled
[02:19:32.900 --> 02:19:35.900]   and it's also Google Home enabled.
[02:19:35.900 --> 02:19:38.580]   After living with it, I have decided I do not like this light switch.
[02:19:38.580 --> 02:19:39.980]   It is.
[02:19:39.980 --> 02:19:45.420]   So the Levitan, it's 50 bucks Levitan decor a Wi-Fi light switch.
[02:19:45.420 --> 02:19:50.060]   I don't like the toggle mechanism and I don't like the dimmer slider.
[02:19:50.060 --> 02:19:51.340]   It's just not a fan.
[02:19:51.340 --> 02:19:55.460]   The thumbs down on the decor a smart switch with Z wave plus.
[02:19:55.460 --> 02:19:57.460]   You were so excited about it.
[02:19:57.460 --> 02:20:00.180]   But yeah, but you liked before you liked that other one.
[02:20:00.180 --> 02:20:01.180]   What's that other one?
[02:20:01.180 --> 02:20:02.180]   You still like that one?
[02:20:02.180 --> 02:20:03.180]   I love lutron.
[02:20:03.180 --> 02:20:05.860]   In fact, I added more lutrons to my house.
[02:20:05.860 --> 02:20:07.700]   There is one in.
[02:20:07.700 --> 02:20:10.300]   Oh, I'll post a video on it.
[02:20:10.300 --> 02:20:13.580]   Actually, I forgot that I have a video on it, but there's one that you can use without
[02:20:13.580 --> 02:20:17.580]   a neutral wire, but it's only on off and I love it.
[02:20:17.580 --> 02:20:18.580]   So it's the.
[02:20:18.580 --> 02:20:20.620]   That's the cassette.
[02:20:20.620 --> 02:20:23.220]   It's the cassette, but it's the.
[02:20:23.220 --> 02:20:26.140]   P hold on.
[02:20:26.140 --> 02:20:28.740]   X no five X.
[02:20:28.740 --> 02:20:30.420]   There's there's a lot of weird.
[02:20:30.420 --> 02:20:31.380]   So it doesn't have a dimmer.
[02:20:31.380 --> 02:20:32.940]   It's just on or off.
[02:20:32.940 --> 02:20:39.740]   It's just on or off, but you can use it to make your lights.
[02:20:39.740 --> 02:20:42.900]   What's the word I'm looking for?
[02:20:42.900 --> 02:20:43.900]   Five.
[02:20:43.900 --> 02:20:44.900]   Yes.
[02:20:44.900 --> 02:20:50.100]   No, I would like that because I don't.
[02:20:50.100 --> 02:20:54.100]   A lot of the lights now I use are LEDs and I don't even if they're dimmable, I don't like
[02:20:54.100 --> 02:20:55.740]   the way they dim.
[02:20:55.740 --> 02:20:56.740]   So I just.
[02:20:56.740 --> 02:20:57.740]   Oh, okay.
[02:20:57.740 --> 02:21:03.980]   So yeah, and you can put so the one without a neutral wires, the PD five WS.
[02:21:03.980 --> 02:21:11.820]   There is a one that is a six W and it's five amps versus six amps is what it's controlling.
[02:21:11.820 --> 02:21:15.820]   But if you don't need the neutral wire, you can use the six amp or if you have a neutral
[02:21:15.820 --> 02:21:18.780]   wire, you can use the six amp and I put mine on ceiling fans.
[02:21:18.780 --> 02:21:19.980]   So that was nice.
[02:21:19.980 --> 02:21:24.620]   And then the other thing I was going to tell you about is that works with home kit.
[02:21:24.620 --> 02:21:26.580]   It works with Echo.
[02:21:26.580 --> 02:21:27.660]   It works with everything.
[02:21:27.660 --> 02:21:31.300]   They have a, I mean, it just it's the smartest of the smart switches, right?
[02:21:31.300 --> 02:21:33.100]   Yes, but you do have to have a bridge.
[02:21:33.100 --> 02:21:34.100]   They have their own bridge.
[02:21:34.100 --> 02:21:37.420]   You have to use their proprietary bridge.
[02:21:37.420 --> 02:21:38.420]   So okay.
[02:21:38.420 --> 02:21:48.180]   And then I have an update on the snooze S and O Z, which was the crazy expensive fan white
[02:21:48.180 --> 02:21:49.380]   noise machine.
[02:21:49.380 --> 02:21:51.140]   I did not pay a full $80.
[02:21:51.140 --> 02:21:53.660]   I paid 60 for mine and I think it's awesome.
[02:21:53.660 --> 02:21:54.660]   This is a kickstart.
[02:21:54.660 --> 02:21:55.660]   It was.
[02:21:55.660 --> 02:21:56.660]   Yeah.
[02:21:56.660 --> 02:21:58.300]   I love it.
[02:21:58.300 --> 02:21:59.940]   My whole family fights over it.
[02:21:59.940 --> 02:22:05.020]   And apparently I snore gentle lady like snoring at night that may not be so gentle.
[02:22:05.020 --> 02:22:08.100]   Here's a guy who clearly snores not lady like.
[02:22:08.100 --> 02:22:10.020]   There you go.
[02:22:10.020 --> 02:22:14.500]   In this news video.
[02:22:14.500 --> 02:22:15.900]   My husband turns it on.
[02:22:15.900 --> 02:22:20.300]   Like sometimes if I forget that it's on my husband will like come over and to my side
[02:22:20.300 --> 02:22:21.300]   of the bed and turn it on.
[02:22:21.300 --> 02:22:24.260]   He says it helps reduce my snoring slash.
[02:22:24.260 --> 02:22:28.020]   He doesn't hear it.
[02:22:28.020 --> 02:22:32.420]   So there you go.
[02:22:32.420 --> 02:22:34.020]   Peaceful white noise from a real fan.
[02:22:34.020 --> 02:22:35.420]   But see, then you have a fan going.
[02:22:35.420 --> 02:22:37.060]   I don't know if I want a fan going.
[02:22:37.060 --> 02:22:38.060]   It's nice.
[02:22:38.060 --> 02:22:39.260]   Keep your laptop going.
[02:22:39.260 --> 02:22:40.260]   Yeah.
[02:22:40.260 --> 02:22:41.620]   Oh, walk, you could do that.
[02:22:41.620 --> 02:22:44.540]   But does a fan, does it blow air or is the fan the noise generator?
[02:22:44.540 --> 02:22:46.460]   It's the noise generator.
[02:22:46.460 --> 02:22:48.140]   So it doesn't it doesn't blow air.
[02:22:48.140 --> 02:22:50.060]   Oh, I don't want a fan on my face.
[02:22:50.060 --> 02:22:52.060]   OK, and you can schedule it.
[02:22:52.060 --> 02:22:55.140]   So the Bluetooth stuff is not like my favorite.
[02:22:55.140 --> 02:22:58.580]   I don't really spend a lot of time on the app personally.
[02:22:58.580 --> 02:23:04.060]   But my daughter likes when she steals it, she likes it to go off like at midnight or
[02:23:04.060 --> 02:23:06.540]   something so we can schedule it to go off at midnight.
[02:23:06.540 --> 02:23:08.100]   So what does it sound like?
[02:23:08.100 --> 02:23:10.020]   Oh, I played it for you guys.
[02:23:10.020 --> 02:23:11.020]   Do you want me to run and get it?
[02:23:11.020 --> 02:23:12.020]   No, no, that's OK.
[02:23:12.020 --> 02:23:14.820]   It's like is that whoosh sound?
[02:23:14.820 --> 02:23:18.340]   Sounds like a white noise generator.
[02:23:18.340 --> 02:23:21.140]   Let's see if he if he.
[02:23:21.140 --> 02:23:23.020]   Oh, yeah.
[02:23:23.020 --> 02:23:27.220]   You can hear it's like.
[02:23:27.220 --> 02:23:30.020]   There's too bad they have the cheesy Casio music on top of it.
[02:23:30.020 --> 02:23:31.660]   Exactly.
[02:23:31.660 --> 02:23:33.580]   It's also great for kids.
[02:23:33.580 --> 02:23:34.780]   Studies have shown that baby.
[02:23:34.780 --> 02:23:37.100]   Oh, there it is.
[02:23:37.100 --> 02:23:38.420]   There it is.
[02:23:38.420 --> 02:23:40.460]   And I think we've all been in that position.
[02:23:40.460 --> 02:23:44.220]   And customize the tone from life and mind does not look like that, which is like.
[02:23:44.220 --> 02:23:46.260]   Passes the original Kickstarter.
[02:23:46.260 --> 02:23:49.580]   That actually looked like it was made out of clay.
[02:23:49.580 --> 02:23:51.140]   Yeah.
[02:23:51.140 --> 02:23:56.500]   So it's probably the clay prototype and then some guy in the background going.
[02:23:56.500 --> 02:23:59.340]   All right.
[02:23:59.340 --> 02:23:59.580]   Yeah.
[02:23:59.580 --> 02:24:01.020]   So the cassette.
[02:24:01.020 --> 02:24:02.220]   Yes.
[02:24:02.220 --> 02:24:03.300]   No, yes.
[02:24:03.300 --> 02:24:04.700]   Well, because they get to Cora.
[02:24:04.700 --> 02:24:06.060]   No snooze.
[02:24:06.060 --> 02:24:06.940]   No snooze.
[02:24:06.940 --> 02:24:08.020]   Yes.
[02:24:08.020 --> 02:24:09.820]   Got it.
[02:24:09.820 --> 02:24:11.820]   Jeffrey, a number.
[02:24:11.820 --> 02:24:19.180]   So two Silicon Valley billionaires.
[02:24:19.180 --> 02:24:20.980]   That is to say that we're aware to go where to go.
[02:24:20.980 --> 02:24:22.740]   Here we go.
[02:24:22.740 --> 02:24:26.860]   It is to say Reid Hoppen and Mark Pinkus have started a project to reinvent the Democratic
[02:24:26.860 --> 02:24:30.420]   Party called WTF, which of course stands for win the future.
[02:24:30.420 --> 02:24:32.980]   Oh, God, Lord.
[02:24:32.980 --> 02:24:35.460]   I think Mark thought up the name.
[02:24:35.460 --> 02:24:39.700]   It's a modern people's lobby that empowers us all to choose our leaders and
[02:24:39.700 --> 02:24:41.500]   set our agenda set, Pinkus.
[02:24:41.500 --> 02:24:43.740]   Oh, just what we need another lobby.
[02:24:43.740 --> 02:24:44.740]   Yeah.
[02:24:44.740 --> 02:24:45.740]   Hmm.
[02:24:45.740 --> 02:24:50.420]   So people will vote on the policies and discuss them on Twitter.
[02:24:50.420 --> 02:24:53.180]   Oh, I can see nothing that will go with that.
[02:24:53.180 --> 02:24:54.420]   Oh, this no.
[02:24:54.420 --> 02:24:56.540]   See, I think Reid Hoppen is very smart.
[02:24:56.540 --> 02:25:00.460]   We talked about his decency pledge last week.
[02:25:00.460 --> 02:25:02.540]   Mark Pinkus, the founder of Zingan.
[02:25:02.540 --> 02:25:06.580]   Ah, so sure about him, but I don't do.
[02:25:06.580 --> 02:25:07.580]   Okay.
[02:25:07.580 --> 02:25:08.580]   Well.
[02:25:08.580 --> 02:25:11.860]   You know, I think it's pretty clear that we need to find new ideas somewhere.
[02:25:11.860 --> 02:25:15.180]   I just feel like Silicon Valley made me the place to lunch.
[02:25:15.180 --> 02:25:16.180]   Yeah.
[02:25:16.180 --> 02:25:17.180]   Yeah.
[02:25:17.180 --> 02:25:18.380]   Especially right now.
[02:25:18.380 --> 02:25:24.140]   Oh, by the way, Leo, did you order the sentio that thing that had turns your Amazon phone
[02:25:24.140 --> 02:25:25.580]   into kind of a laptop?
[02:25:25.580 --> 02:25:28.260]   Oh, did you get your stuff?
[02:25:28.260 --> 02:25:29.580]   Oh, no, no, no.
[02:25:29.580 --> 02:25:30.780]   It's now I tried to cancel.
[02:25:30.780 --> 02:25:32.220]   That's what I always do.
[02:25:32.220 --> 02:25:33.780]   I have second thoughts and then I could.
[02:25:33.780 --> 02:25:34.780]   Supermore.
[02:25:34.780 --> 02:25:41.380]   Yeah, they probably were very transparent about learning some things in the beta and making
[02:25:41.380 --> 02:25:45.340]   some changes, but now means the delay is delayed until like November.
[02:25:45.340 --> 02:25:50.940]   So I have the thing that Samsung shipped with their SA, the DEX, which does some of the
[02:25:50.940 --> 02:25:54.340]   same thing and Samsung stationary.
[02:25:54.340 --> 02:25:59.740]   The DEX is a little disk and you plug it into any available mouse monitoring keyboard.
[02:25:59.740 --> 02:26:05.300]   So unlike this, that's pretty small, but where you're going, you have to have a mouse monitoring
[02:26:05.300 --> 02:26:06.660]   keyboard, which is the issue.
[02:26:06.660 --> 02:26:09.580]   I don't normally just have a spare mouse monitoring keyboard anywhere.
[02:26:09.580 --> 02:26:13.100]   So this kind of makes sense, but I don't know.
[02:26:13.100 --> 02:26:15.220]   So you can now get a cancel.
[02:26:15.220 --> 02:26:18.980]   If it's too late, I'll let you think, should I hold on to it?
[02:26:18.980 --> 02:26:19.980]   Twenty bucks.
[02:26:19.980 --> 02:26:21.620]   I've never been overwhelmed by this.
[02:26:21.620 --> 02:26:26.540]   One of the reasons the Samsung makes sense is because they modified their version of Android
[02:26:26.540 --> 02:26:29.020]   to accommodate the screen.
[02:26:29.020 --> 02:26:33.460]   I don't know how Android's going to look on just a generic phone when it's hooked up
[02:26:33.460 --> 02:26:34.460]   to this.
[02:26:34.460 --> 02:26:37.660]   Well, there's an app you're on.
[02:26:37.660 --> 02:26:38.660]   Oh, there's an app.
[02:26:38.660 --> 02:26:39.660]   Okay.
[02:26:39.660 --> 02:26:40.660]   There's an app.
[02:26:40.660 --> 02:26:41.660]   Yeah.
[02:26:41.660 --> 02:26:44.060]   I mean, I have a number of these devices.
[02:26:44.060 --> 02:26:47.580]   And I think the principle makes sense.
[02:26:47.580 --> 02:26:49.100]   Execution, maybe not.
[02:26:49.100 --> 02:26:52.260]   You'd like something, but you travel so much, I can understand why you'd want something
[02:26:52.260 --> 02:26:53.260]   like that.
[02:26:53.260 --> 02:26:56.380]   I can have the side.
[02:26:56.380 --> 02:26:57.220]   I never can.
[02:26:57.220 --> 02:26:58.220]   How is it?
[02:26:58.220 --> 02:27:00.820]   How was $129 to you, Jeff?
[02:27:00.820 --> 02:27:02.700]   I mean, really that's actually it.
[02:27:02.700 --> 02:27:08.180]   Of course, I ordered the deluxe, which has the backlighting and the backlit keyboard.
[02:27:08.180 --> 02:27:10.300]   And so it's 190, I think.
[02:27:10.300 --> 02:27:12.100]   Oh, no.
[02:27:12.100 --> 02:27:13.940]   Kill it.
[02:27:13.940 --> 02:27:16.380]   This was a winner on Kickstarter.
[02:27:16.380 --> 02:27:22.660]   At the time, the biggest hardware startup of all time, $2.9 million they raised on this.
[02:27:22.660 --> 02:27:23.660]   Yeah.
[02:27:23.660 --> 02:27:25.660]   So what's the line to see?
[02:27:25.660 --> 02:27:30.260]   You know, so it's a good, you know, they're being very transparent and saying we started
[02:27:30.260 --> 02:27:33.180]   with this thing and we've learned some stuff.
[02:27:33.180 --> 02:27:40.620]   So the main challenges to you, basically, is a significant number of phones using non-standard
[02:27:40.620 --> 02:27:50.660]   communications protocols due to Android's fragmentation.
[02:27:50.660 --> 02:27:52.620]   And so they changed the firmware and stuff.
[02:27:52.620 --> 02:28:00.620]   So now it's going to ship late October to late in late November.
[02:28:00.620 --> 02:28:02.620]   I ordered the...
[02:28:02.620 --> 02:28:04.660]   I ordered the...
[02:28:04.660 --> 02:28:08.140]   Yeah.
[02:28:08.140 --> 02:28:09.580]   I have ordered...
[02:28:09.580 --> 02:28:13.140]   I got just got emailed today that my eco-reco is coming later in the month.
[02:28:13.140 --> 02:28:15.580]   You know about the eco-reco electric scooter?
[02:28:15.580 --> 02:28:16.580]   Oh!
[02:28:16.580 --> 02:28:17.580]   No, scooter?
[02:28:17.580 --> 02:28:18.580]   No.
[02:28:18.580 --> 02:28:21.820]   This is another way for me to break my neck.
[02:28:21.820 --> 02:28:28.300]   Actually, I got it for Michael because his bus stop is about... is like a mile from his
[02:28:28.300 --> 02:28:29.300]   school.
[02:28:29.300 --> 02:28:31.020]   So... or actually two miles.
[02:28:31.020 --> 02:28:37.300]   So we got him the eco-reco, the new model R. can go up to 20 miles an hour.
[02:28:37.300 --> 02:28:38.300]   Wow!
[02:28:38.300 --> 02:28:42.340]   It has a 10 to 40 mile range lithium ion.
[02:28:42.340 --> 02:28:48.540]   It's got a headlight, a tail light, an underlight turn signals, air tires, front shocks.
[02:28:48.540 --> 02:28:50.300]   It's a little heavy.
[02:28:50.300 --> 02:28:54.500]   It's 27 pounds, but it's light enough that he could... and it folds up that he could carry
[02:28:54.500 --> 02:29:02.900]   it as he gets off the bus, carry it under the bus and then unfold it and whiz off to school.
[02:29:02.900 --> 02:29:05.460]   Nine out of ten Americans commute to work.
[02:29:05.460 --> 02:29:10.780]   We've seen it because this is a going concern, which is nice.
[02:29:10.780 --> 02:29:12.860]   This company eco-reco makes other model scooters.
[02:29:12.860 --> 02:29:16.060]   We haven't seen the R yet, but they make other scooters.
[02:29:16.060 --> 02:29:17.060]   I don't know.
[02:29:17.060 --> 02:29:18.500]   I think this seems like a cool idea.
[02:29:18.500 --> 02:29:23.700]   So, I'm waiting on my... it's called Mighty.
[02:29:23.700 --> 02:29:26.420]   It is a Spotify player.
[02:29:26.420 --> 02:29:30.580]   And I wanted it when it first launched and I waited because I was like, "Oh, this does
[02:29:30.580 --> 02:29:32.940]   not look like it's going to go anywhere."
[02:29:32.940 --> 02:29:38.660]   But then I bought it like at the end of June, or no, the beginning of June because they
[02:29:38.660 --> 02:29:43.660]   had gotten finally all those Spotify approvals and they said it was shipping at the end of
[02:29:43.660 --> 02:29:44.740]   the month.
[02:29:44.740 --> 02:29:45.740]   Oh.
[02:29:45.740 --> 02:29:51.780]   So, I bought it and now it was supposed to ship by June 30th, but so far I got nothing
[02:29:51.780 --> 02:29:53.220]   and no communication from them.
[02:29:53.220 --> 02:29:56.460]   It's a new feature of the show, Kickstarter Nightmares.
[02:29:56.460 --> 02:30:01.420]   I thought I was being smart.
[02:30:01.420 --> 02:30:04.820]   No, it's never smart to buy something on Kickstarter.
[02:30:04.820 --> 02:30:05.820]   Never, ever.
[02:30:05.820 --> 02:30:07.220]   Run away.
[02:30:07.220 --> 02:30:08.220]   Exactly.
[02:30:08.220 --> 02:30:15.580]   So, which is exactly why I just bought Samsara, the world's first aluminum smart suitcase.
[02:30:15.580 --> 02:30:20.260]   Oh, you know, AT&T is putting stuff into me now.
[02:30:20.260 --> 02:30:21.260]   Did you see that?
[02:30:21.260 --> 02:30:22.260]   Oh, it's the only two of me.
[02:30:22.260 --> 02:30:23.260]   Those are crazy overpriced.
[02:30:23.260 --> 02:30:24.260]   Oh, of course you do.
[02:30:24.260 --> 02:30:25.260]   They're fancy.
[02:30:25.260 --> 02:30:26.260]   I've never owned it to me.
[02:30:26.260 --> 02:30:28.100]   It's one of my life bucket list goals.
[02:30:28.100 --> 02:30:30.060]   You should be a lot with your EBS chair.
[02:30:30.060 --> 02:30:32.500]   Yeah, my EBS chair, my two of me.
[02:30:32.500 --> 02:30:34.140]   So, let me see.
[02:30:34.140 --> 02:30:40.340]   Yeah, of the things I've ordered on... there's quite a few things I've ordered on Kickstarter
[02:30:40.340 --> 02:30:42.380]   that I never got.
[02:30:42.380 --> 02:30:43.380]   Really?
[02:30:43.380 --> 02:30:51.860]   Yeah, going back to 2011 when I ordered a pair of $100 dice that I still haven't gotten.
[02:30:51.860 --> 02:30:52.860]   What?
[02:30:52.860 --> 02:30:53.860]   Six years later.
[02:30:53.860 --> 02:30:55.860]   What kind of dice are these?
[02:30:55.860 --> 02:30:57.900]   They ate your grandpappy's dice.
[02:30:57.900 --> 02:30:59.300]   They're quantum dice.
[02:30:59.300 --> 02:31:00.300]   Quantum dice.
[02:31:00.300 --> 02:31:01.660]   I'm still waiting for my G-Bow.
[02:31:01.660 --> 02:31:02.860]   That was on Indiegogo.
[02:31:02.860 --> 02:31:04.660]   That was the robot that talks to you.
[02:31:04.660 --> 02:31:05.660]   Oh, yeah.
[02:31:05.660 --> 02:31:07.060]   They had some management shakeups.
[02:31:07.060 --> 02:31:08.060]   Did they?
[02:31:08.060 --> 02:31:09.060]   Yeah.
[02:31:09.060 --> 02:31:10.700]   Cynthia was taking it.
[02:31:10.700 --> 02:31:12.260]   Cynthia was left.
[02:31:12.260 --> 02:31:16.580]   She's still there, but she was taken down as CEO.
[02:31:16.580 --> 02:31:20.380]   So I think there was some...
[02:31:20.380 --> 02:31:23.740]   Here's the Unity Stand for IMAX and Apple displays.
[02:31:23.740 --> 02:31:25.540]   Never got that either.
[02:31:25.540 --> 02:31:27.060]   They canceled.
[02:31:27.060 --> 02:31:31.260]   So if you buy stuff on Kickstarter, let the buyer beware, I guess.
[02:31:31.260 --> 02:31:32.260]   Oh, yeah.
[02:31:32.260 --> 02:31:33.780]   Your money's back, right?
[02:31:33.780 --> 02:31:34.780]   Mmm.
[02:31:34.780 --> 02:31:36.860]   Not in a lot of cases, no.
[02:31:36.860 --> 02:31:37.860]   Oh.
[02:31:37.860 --> 02:31:39.300]   If you follow up, you might.
[02:31:39.300 --> 02:31:40.300]   No.
[02:31:40.300 --> 02:31:41.300]   They don't guarantee that.
[02:31:41.300 --> 02:31:42.660]   I'm not my floating bonsai tree.
[02:31:42.660 --> 02:31:43.660]   I told you that.
[02:31:43.660 --> 02:31:46.420]   I'm never going to use it.
[02:31:46.420 --> 02:31:47.420]   You want mine?
[02:31:47.420 --> 02:31:48.420]   Well, let me see.
[02:31:48.420 --> 02:31:49.780]   Is it an actual bonsai tree?
[02:31:49.780 --> 02:31:51.420]   No, you don't get the tree.
[02:31:51.420 --> 02:31:52.420]   Oh.
[02:31:52.420 --> 02:31:58.980]   You just get a piece of lava rock with a magnet in it and a base and then it floats on the
[02:31:58.980 --> 02:32:01.020]   magnet.
[02:32:01.020 --> 02:32:03.100]   I've found something I must buy.
[02:32:03.100 --> 02:32:04.100]   I must buy it.
[02:32:04.100 --> 02:32:05.100]   What's that?
[02:32:05.100 --> 02:32:06.100]   A floating bonsai tree?
[02:32:06.100 --> 02:32:07.100]   It's right now.
[02:32:07.100 --> 02:32:09.580]   It's a project we love on technology.
[02:32:09.580 --> 02:32:11.580]   Interactive LED eyelashes.
[02:32:11.580 --> 02:32:13.060]   Don't you think I'm looking at me?
[02:32:13.060 --> 02:32:14.060]   Oh, I actually want those.
[02:32:14.060 --> 02:32:15.260]   Don't make fun of me.
[02:32:15.260 --> 02:32:17.820]   There's a market for every product.
[02:32:17.820 --> 02:32:23.220]   Only reason I didn't buy them is because they're wired.
[02:32:23.220 --> 02:32:24.220]   They work.
[02:32:24.220 --> 02:32:25.220]   What?
[02:32:25.220 --> 02:32:26.700]   You have to have a wire coming out of your head?
[02:32:26.700 --> 02:32:31.460]   Well, so the lashes, if they're the ones I'm thinking of and I don't know how many flashing
[02:32:31.460 --> 02:32:33.820]   LED lights eyelashes there are.
[02:32:33.820 --> 02:32:35.220]   It's got to be the ones.
[02:32:35.220 --> 02:32:36.820]   Yep, those are it.
[02:32:36.820 --> 02:32:41.380]   So you stick the LEDs just like you're applying false eyelashes on your eyes and then there's
[02:32:41.380 --> 02:32:42.380]   a little wire.
[02:32:42.380 --> 02:32:44.780]   Because I don't do so well, yes.
[02:32:44.780 --> 02:32:46.940]   OK, it's not hard.
[02:32:46.940 --> 02:32:47.940]   You just glue.
[02:32:47.940 --> 02:32:51.340]   F. Oh, I like the new F dot lashes.
[02:32:51.340 --> 02:32:55.260]   Oh, I would order those.
[02:32:55.260 --> 02:32:58.380]   Those are creepy as hell.
[02:32:58.380 --> 02:32:59.780]   I think they'd be a mess.
[02:32:59.780 --> 02:33:00.780]   Oh, that's creepy.
[02:33:00.780 --> 02:33:01.780]   It's too bad.
[02:33:01.780 --> 02:33:02.780]   They're wired.
[02:33:02.780 --> 02:33:05.420]   They're tiny wires, but you can see the wire on the side.
[02:33:05.420 --> 02:33:09.700]   Oh, yeah, but you wear them to your rave or your next acid.
[02:33:09.700 --> 02:33:11.460]   No, it's going to see the wire.
[02:33:11.460 --> 02:33:13.260]   Oh, my dance party.
[02:33:13.260 --> 02:33:14.900]   Isn't that awesome?
[02:33:14.900 --> 02:33:18.460]   Oh, I want to look just like him.
[02:33:18.460 --> 02:33:20.780]   See, I already have blue in my hair.
[02:33:20.780 --> 02:33:21.780]   Look at that.
[02:33:21.780 --> 02:33:22.780]   So weird.
[02:33:22.780 --> 02:33:26.060]   If sprockets were allowed today, oh, I need these.
[02:33:26.060 --> 02:33:27.060]   How much were they?
[02:33:27.060 --> 02:33:28.060]   It wasn't.
[02:33:28.060 --> 02:33:31.580]   There were like four bucks for the pair.
[02:33:31.580 --> 02:33:32.740]   What if I just bought one?
[02:33:32.740 --> 02:33:35.380]   Oh, look, there's a little controller that you put in your pocket.
[02:33:35.380 --> 02:33:37.780]   No, it goes in the back of your head.
[02:33:37.780 --> 02:33:41.220]   Again, I have actually researched this.
[02:33:41.220 --> 02:33:45.060]   Oh, well, fortunately, you just put it under your wig and it's fine.
[02:33:45.060 --> 02:33:49.500]   This is clearly a drag queen product or Stacy.
[02:33:49.500 --> 02:33:54.420]   Not clearly a drag queen product.
[02:33:54.420 --> 02:33:55.420]   This is for people.
[02:33:55.420 --> 02:33:57.260]   OK, I'm wearing my wig on the show next to me.
[02:33:57.260 --> 02:33:58.260]   Oh, will you please?
[02:33:58.260 --> 02:33:59.260]   You have that wig?
[02:33:59.260 --> 02:34:01.740]   Of course, I have multiple wigs.
[02:34:01.740 --> 02:34:04.020]   See, I feel like there's so much about Stacy.
[02:34:04.020 --> 02:34:06.020]   You don't really know.
[02:34:06.020 --> 02:34:07.700]   No, it's just--
[02:34:07.700 --> 02:34:10.340]   Where are these when you take your LSD on the show, please?
[02:34:10.340 --> 02:34:12.420]   There we go.
[02:34:12.420 --> 02:34:13.860]   Man, oh, man.
[02:34:13.860 --> 02:34:19.020]   I love the concept of this sort of thing.
[02:34:19.020 --> 02:34:20.860]   I would be-- you know, in the Hunger Games,
[02:34:20.860 --> 02:34:22.660]   how they have the capital city people,
[02:34:22.660 --> 02:34:24.180]   I would be one of those people who
[02:34:24.180 --> 02:34:26.580]   is dying my skin crazy stuff.
[02:34:26.580 --> 02:34:30.380]   And I mean, I'd dye my hair.
[02:34:30.380 --> 02:34:31.380]   OK, I'll make you deal.
[02:34:31.380 --> 02:34:34.620]   If you order them, I'll order them.
[02:34:34.620 --> 02:34:36.220]   Oh, but I don't want the wire.
[02:34:36.220 --> 02:34:37.940]   I'm waiting for the battery powered.
[02:34:37.940 --> 02:34:40.220]   Because I don't want to wear them to the rave.
[02:34:40.220 --> 02:34:42.980]   I want to wear them out to a night--
[02:34:42.980 --> 02:34:43.740]   I like the night.
[02:34:43.740 --> 02:34:45.220]   There's a night rider one.
[02:34:45.220 --> 02:34:46.180]   Do you know what I mean?
[02:34:46.180 --> 02:34:48.100]   Now what I want is a little speaker.
[02:34:48.100 --> 02:34:49.900]   I could play that Spotify speaker
[02:34:49.900 --> 02:34:52.220]   and have the night rider theme going
[02:34:52.220 --> 02:34:54.620]   while my eyes are going back and forth.
[02:34:54.620 --> 02:34:55.140]   There you go.
[02:34:55.140 --> 02:34:56.460]   And I'd say just call me Kit.
[02:34:56.460 --> 02:35:00.700]   All right, we're going to wrap this up.
[02:35:00.700 --> 02:35:01.700]   I think we've done--
[02:35:01.700 --> 02:35:04.620]   sandwich by wet sleeve.
[02:35:04.620 --> 02:35:05.540]   What are you wearing?
[02:35:05.540 --> 02:35:06.460]   There was wet sleeve.
[02:35:06.460 --> 02:35:07.300]   No, no, no, stop.
[02:35:07.300 --> 02:35:08.780]   I already-- I just bought the lashes.
[02:35:08.780 --> 02:35:09.260]   Please.
[02:35:09.260 --> 02:35:10.180]   I've had enough.
[02:35:10.180 --> 02:35:10.780]   You really did.
[02:35:10.780 --> 02:35:11.620]   Oh, yeah, absolutely.
[02:35:11.620 --> 02:35:13.060]   I want to-- there was a plastic--
[02:35:13.060 --> 02:35:13.420]   Right.
[02:35:13.420 --> 02:35:16.180]   Liquid plastic welder.
[02:35:16.180 --> 02:35:17.100]   No, stop it.
[02:35:17.100 --> 02:35:18.100]   Like welding to it.
[02:35:18.100 --> 02:35:19.660]   You know I'm going to order it.
[02:35:19.660 --> 02:35:20.180]   Stop it.
[02:35:20.180 --> 02:35:20.740]   Sonnets.
[02:35:20.740 --> 02:35:21.260]   Stop it.
[02:35:21.260 --> 02:35:23.580]   You're his most advanced off-grid mobile mission.
[02:35:23.580 --> 02:35:24.860]   Stop it.
[02:35:24.860 --> 02:35:26.500]   This makes for a great gift right here.
[02:35:26.500 --> 02:35:30.100]   [LAUGHTER]
[02:35:30.100 --> 02:35:32.780]   Ladies and gentlemen, we do this week at Google every Wednesday.
[02:35:32.780 --> 02:35:36.860]   130 Pacific, 430 Eastern, 2030 UTC with Stacey Higginbotham
[02:35:36.860 --> 02:35:39.740]   of Stacey on IOT.
[02:35:39.740 --> 02:35:43.140]   IOTpodcast.com for the podcast at Gigastacey on Twitter.
[02:35:43.140 --> 02:35:45.700]   Thank you, Stacey.
[02:35:45.700 --> 02:35:46.540]   Oh, it's a pleasure.
[02:35:46.540 --> 02:35:48.100]   Oh, it's a pleasure.
[02:35:48.100 --> 02:35:49.620]   Oh, somebody's at your door.
[02:35:49.620 --> 02:35:50.900]   I was about to say yes.
[02:35:50.900 --> 02:35:53.140]   If my show was right here, I could see.
[02:35:53.140 --> 02:35:55.260]   Time for the pizza.
[02:35:55.260 --> 02:35:57.940]   We also have a fabulous time with Jeff Jarvis every week.
[02:35:57.940 --> 02:36:01.060]   He is at CUNY, and there are some very lucky journalism
[02:36:01.060 --> 02:36:03.220]   students who get to work with him.
[02:36:03.220 --> 02:36:05.660]   When does school start?
[02:36:05.660 --> 02:36:06.460]   August.
[02:36:06.460 --> 02:36:08.620]   You have to brainwash them.
[02:36:08.620 --> 02:36:09.220]   Have fun.
[02:36:09.220 --> 02:36:15.300]   We will also see him at buzzmachine.com.
[02:36:15.300 --> 02:36:17.300]   His blog, he writes on medium frequently,
[02:36:17.300 --> 02:36:21.180]   sometimes on politico.eu.
[02:36:21.180 --> 02:36:26.660]   I like the response from actual shpenga to your post
[02:36:26.660 --> 02:36:28.540]   on the politico.eu.
[02:36:28.540 --> 02:36:29.900]   The fight is on.
[02:36:29.900 --> 02:36:31.300]   You never watch like that, Jarvis.
[02:36:31.300 --> 02:36:32.020]   Yeah.
[02:36:32.020 --> 02:36:34.460]   Didn't like him and don't like him now.
[02:36:34.460 --> 02:36:35.860]   Oh, no.
[02:36:35.860 --> 02:36:38.020]   We do this show as a while we do that part.
[02:36:38.020 --> 02:36:40.100]   You can get a copy of it to a TV/twig.
[02:36:40.100 --> 02:36:41.740]   You can also get it if you subscribe
[02:36:41.740 --> 02:36:43.300]   in your favorite podcast appliance.
[02:36:43.300 --> 02:36:44.500]   Please do.
[02:36:44.500 --> 02:36:46.860]   We want you to be here for each and every episode.
[02:36:46.860 --> 02:36:48.260]   Thanks for joining us.
[02:36:48.260 --> 02:36:50.620]   And I will see you next time on This Week in Google.
[02:36:50.620 --> 02:36:51.620]   Bye-bye.
[02:36:51.620 --> 02:36:54.620]   [MUSIC PLAYING]
[02:36:54.620 --> 02:36:57.960]   [MUSIC PLAYING]
[02:36:57.960 --> 02:37:00.540]   (upbeat music)

