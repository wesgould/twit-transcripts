;FFMETADATA1
title=A Doctor in Industry
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=388
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.680]   It's time for Twig this week in Google. Jeff Jarvis joins us. Jeff Jarvis. I'm gonna call him that from Davos.
[00:00:05.680 --> 00:00:14.220]   He's at the World Economic Forum with some really big names. He'll have a report. Kevin Marks is also here from the indie web in Stacey
[00:00:14.220 --> 00:00:20.440]   Higginbotham from Stacey on IOT. We'll talk about the latest news from Google artificial intelligence and
[00:00:20.440 --> 00:00:25.120]   a visit to the Internet Archive. It's all come up next on Twig.
[00:00:28.920 --> 00:00:30.920]   Netcasts you love.
[00:00:30.920 --> 00:00:32.920]   From people you trust.
[00:00:32.920 --> 00:00:38.040]   This is Twig.
[00:00:38.040 --> 00:00:42.320]   Bandwidth for this week in Google is provided by cash fly.
[00:00:42.320 --> 00:00:45.160]   C-A-C-H-E-F-L-Y.com.
[00:00:45.160 --> 00:00:51.800]   Hi, this is Leo Leport and once again time for Twig's audience survey. We'd really like to hear from you.
[00:00:51.800 --> 00:00:54.640]   It's only going to take a couple of minutes really. That's all.
[00:00:54.920 --> 00:01:01.360]   Just go to twit.tv/survey and let us know what you think. Your anonymous feedback will help us make Twig even better.
[00:01:01.360 --> 00:01:03.360]   And thanks for your continued support.
[00:01:03.360 --> 00:01:15.760]   This is Twig. This week in Google episode 388 recorded Wednesday January 18th 2017. A doctor in industry.
[00:01:15.760 --> 00:01:23.040]   This week in Google is brought to you by Casper an online retailer of premium mattresses for a fraction of the price because
[00:01:23.360 --> 00:01:27.820]   everyone deserves a great night's sleep. Get $50 off any mattress purchased by visiting
[00:01:27.820 --> 00:01:32.360]   Casper.com/Twig and entering the promo code Twig.
[00:01:32.360 --> 00:01:34.200]   And by
[00:01:34.200 --> 00:01:41.000]   Eero. I settle for just a Wi-Fi router when you can have a brilliant hyper-fast super simple Wi-Fi system.
[00:01:41.000 --> 00:01:49.400]   No more buffering. No more dead zones. Finally Wi-Fi that works for free overnight shipping visit Eero.com and a checkout select overnight shipping
[00:01:49.400 --> 00:01:51.400]   and enter the code Twig.
[00:01:51.840 --> 00:01:58.200]   And by Audible to download a free audiobook of your choice go to Audible.com/Twig.
[00:01:58.200 --> 00:02:02.880]   It's time for Twig this week in Google the show where we cover the latest
[00:02:02.880 --> 00:02:10.680]   from the Google verse from the world about us. Jeff Jarvis is about in the world. He's in Davos, Switzerland
[00:02:10.680 --> 00:02:13.920]   for the world economic forum.
[00:02:13.920 --> 00:02:15.920]   Academic forum. Here's my
[00:02:15.920 --> 00:02:18.720]   official bag. That makes you a big shot.
[00:02:19.440 --> 00:02:24.680]   It does the weight badges. The thing that gets you in everywhere including God's living room. Yes.
[00:02:24.680 --> 00:02:28.680]   How many Davos for world economic forums has this is this for you?
[00:02:28.680 --> 00:02:34.080]   Eleven. Yeah, it seems like every as long as we've been doing the show you've been there this time of year.
[00:02:34.080 --> 00:02:36.560]   Yeah, I guess so. Yeah, you know
[00:02:36.560 --> 00:02:42.680]   It's under the press center where there's one two three four diligent journalists working and I'm disturbing them by talking.
[00:02:42.680 --> 00:02:46.320]   Oh, you're diligent. This is diligent journalism. Welcome to
[00:02:47.040 --> 00:02:49.680]   I'm responsive and responsible. Yeah.
[00:02:49.680 --> 00:02:54.480]   Also, also with us Kevin Marks, he's in a typhoon in his backyard.
[00:02:54.480 --> 00:02:58.880]   Hello there. The wind is starting to pick up. Yes, it is raining rather heavily here.
[00:02:58.880 --> 00:03:04.960]   What is it about you inside? You think you fly past my face. What is it about inside you don't like?
[00:03:04.960 --> 00:03:07.520]   Yeah, what have you ever done inside, Eddie?
[00:03:07.520 --> 00:03:11.360]   Um, there are four dogs in there that will join in. Ah,
[00:03:11.360 --> 00:03:14.480]   so
[00:03:14.480 --> 00:03:20.240]   Yeah, Stacy Haganbotham's also here from Austin, Texas. Stacy on IOT and she has a couple of
[00:03:20.240 --> 00:03:25.120]   a couple of dogs who just one just I have a name. Sounds like two. Okay, you can hear it. Yeah.
[00:03:25.120 --> 00:03:29.680]   Wait a minute. Your neighbor's dog is what you have play dates? Is he is he over right now? No, no, he's but
[00:03:29.680 --> 00:03:33.520]   I have like five feet from my neighbor. Um, so
[00:03:33.520 --> 00:03:37.680]   their backyard is like closer to me than my actual dog is right now.
[00:03:37.680 --> 00:03:42.480]   And he's a Bernie's mountain dog. So he's huge. Oh, that's the biggest. That's huge is right.
[00:03:43.120 --> 00:03:47.680]   He's so sweet if he would carry a barrel of brandy around his neck and be okay.
[00:03:47.680 --> 00:03:53.600]   Then you know he might. I wouldn't put it past him. Do they still do that in Davos?
[00:03:53.600 --> 00:04:02.560]   Yeah. Have you seen any shop? No, no. No, no, no. The premier of China spoke at Davos yesterday.
[00:04:02.560 --> 00:04:10.160]   That was a big deal. He did indeed. Uh, it's a he's taken over the world. He's now the unfree
[00:04:10.160 --> 00:04:16.800]   leader of the free world. Um, that's kind of where we are now. Uh, it's amazing here that the the the
[00:04:16.800 --> 00:04:24.880]   Trump, um, heartburn is felt throughout Joe Biden. I want to see Joe Biden speak today. And he gave
[00:04:24.880 --> 00:04:30.000]   basically a ben addiction to liberal democracy. Oh, it's not that bad.
[00:04:30.000 --> 00:04:35.920]   No, no, no, to the liberal world or world order of being open and real and free trade and
[00:04:35.920 --> 00:04:42.800]   globalism and all that liberal that definition. Uh, Xi Jinping, who's the president, not premier,
[00:04:42.800 --> 00:04:47.920]   but president of China. Uh, yes. One of the things he said, no one will emerge as a winner in a
[00:04:47.920 --> 00:04:53.840]   trade war. And that's specifically aimed at president like Trump. Uh, he quoted it was an
[00:04:53.840 --> 00:04:59.600]   amazing speech. His, his, his contention is that globalization is a good thing. It's powered
[00:04:59.600 --> 00:05:03.520]   growth worldwide. Obviously it's a good thing for China. Is it a good thing for the world?
[00:05:05.520 --> 00:05:09.920]   See, the view here, there's a lot of how people summarize the entire view of the elite.
[00:05:09.920 --> 00:05:17.760]   The elite say here or the elite say that globalization is an, is an alloy, good thing that maybe
[00:05:17.760 --> 00:05:22.880]   would have been done better, could have been done to have more concern about people's jobs and lives.
[00:05:22.880 --> 00:05:29.760]   Um, now what does that mean in practice? I have no idea. Um, I don't know how you protect jobs in a
[00:05:29.760 --> 00:05:35.440]   country without tariffs, uh, without all sorts of artificial restrictions without a trade
[00:05:35.440 --> 00:05:43.760]   war in effect. Uh, it doesn't calculate the, uh, added value of, of world trade. And I never see
[00:05:43.760 --> 00:05:49.760]   calculations of that. Number one, number two, it does not calculate very importantly the, um,
[00:05:49.760 --> 00:05:56.320]   job impact of technology, which is the real story that no one acknowledges here. However,
[00:05:56.320 --> 00:06:00.960]   I'll talk about this in the number of much later, but the Edelman PR company released its Edelman
[00:06:00.960 --> 00:06:10.160]   Trust barometer for, for 17th year. And for relevant to us at Twig and Twig is that there's an
[00:06:10.160 --> 00:06:17.120]   outright fear of innovation and thus technology now. There's a backlash coming to us folks, folks.
[00:06:17.120 --> 00:06:21.840]   I almost would prefer the globalization backlash to the technology backlash.
[00:06:21.840 --> 00:06:29.360]   Right. The Luddites coming to smash the looms. One of the things President Xi said is no one
[00:06:29.360 --> 00:06:34.400]   would emerge as a winner in a global trade war pursuing protectionism is like locking oneself
[00:06:34.400 --> 00:06:40.160]   in a dark room. And this, you'll appreciate this Kevin Marks wind and rain might be kept outside,
[00:06:40.160 --> 00:06:48.480]   but so are light in air. And dogs, the dogs of war are at the door. Yeah, it's really, I mean,
[00:06:48.480 --> 00:06:53.840]   the reason this is a tech, uh, subject is because almost all the products we use in technology are
[00:06:53.840 --> 00:07:01.360]   made outside the United States. Um, uh, and I think one of the, uh, you know,
[00:07:01.360 --> 00:07:07.520]   agendas for President Xi is to keep that though, whether it's jobs or jobs for robots to keep the
[00:07:07.520 --> 00:07:13.360]   cash flowing into China from Americans and other countries from the manufacturer there.
[00:07:13.360 --> 00:07:22.160]   And there's a vacuum here. Obviously, it's been inauguration week. Nobody here is up for Anthony
[00:07:22.160 --> 00:07:26.640]   Skarmucci is the only person from this world here Biden this year. Basically, Americans
[00:07:26.640 --> 00:07:32.880]   are much less representative than ever. And it's, it's symbolic of how Xi filled a vacuum.
[00:07:32.880 --> 00:07:38.800]   Miracles not here. Trugos not here. May is not here. So, gee is the biggest leader here by far,
[00:07:38.800 --> 00:07:44.080]   and he's, he's reveling in it. Sorry. Sorry, Stacy. No, no, that's, that's okay. That's more
[00:07:44.080 --> 00:07:50.000]   perspective than I was going to offer. Well, offer your perspective anyway. From my, from the,
[00:07:50.000 --> 00:07:56.080]   from the view from my porch in Austin, Texas is. Oh, well, so I was actually going to talk about
[00:07:56.080 --> 00:08:01.360]   protectionism and tariffs and what I think the nuance that gets lost is most of the time when
[00:08:01.360 --> 00:08:08.080]   we're talking about that it is, it's negotiated trade agreements. So the, the, the, the, you know,
[00:08:08.080 --> 00:08:12.320]   there's going to be a tariff. I'm going to tax this and you can tax that and it.
[00:08:12.320 --> 00:08:19.360]   The way we're talking about all of this is just silly. Well, it is interesting because, uh,
[00:08:20.160 --> 00:08:26.800]   technology fans, uh, have been a little upset with TPP, the Trans-Pacific Partnership,
[00:08:26.800 --> 00:08:31.760]   and frankly, a lot of the treaties that go on because very often American, uh,
[00:08:31.760 --> 00:08:39.280]   content companies interests, the copyright holders interests are in some, uh, in some's opinion,
[00:08:39.280 --> 00:08:44.400]   overrepresented in these treaties that a lot of what these treaties and the, are, are generated by,
[00:08:45.280 --> 00:08:51.360]   WIPO, the world intellectual property organization out of the UN, which is basically driven by Hollywood.
[00:08:51.360 --> 00:08:58.480]   Um, and so, uh, I think it's funny because the, the techno left has been against TPP from,
[00:08:58.480 --> 00:09:04.400]   for other reasons. Well, other reasons. Yeah. Yeah. Copyright over, over, over,
[00:09:04.400 --> 00:09:08.800]   over extension of copyright primarily, right? Right. Exactly. Yeah, exactly. Anyway,
[00:09:08.800 --> 00:09:14.960]   but, but also some of the, um, nation states and sue government stuff that that's off-putting.
[00:09:14.960 --> 00:09:19.040]   I mean, governments, sorry, corporations, corporations, corporations,
[00:09:19.040 --> 00:09:21.920]   consumer governments. So that's, that's the piece that's, that's a bit weird about it.
[00:09:21.920 --> 00:09:26.560]   And, and by the way, you could do anything you want, but ultimately this is not going to,
[00:09:26.560 --> 00:09:32.480]   globalization is a trend is not going to go away. The, uh, automation of jobs is not going to go away.
[00:09:32.480 --> 00:09:39.040]   Uh, and I think what she is saying, you know, you know, well, I'll paraphrase what she is saying is,
[00:09:39.040 --> 00:09:44.560]   it's not merely, uh, trade wars that could put us in that dark room. It's turning our back on,
[00:09:45.360 --> 00:09:50.080]   the, you know, the future somehow artificially holding back on the future. So what we really
[00:09:50.080 --> 00:09:56.080]   ought to do is plan for it, train for it, uh, consider it. And, and instead of saying,
[00:09:56.080 --> 00:10:03.040]   oh, we're going to go back to the manufacturing might of the 19 of 1950s America say, well,
[00:10:03.040 --> 00:10:07.520]   how do we deal with this inevitable freight train that's coming straight at us, right? Or no,
[00:10:07.520 --> 00:10:10.720]   I should car leader be in the water. Yes. I should gnarly just be in the one saying that.
[00:10:11.600 --> 00:10:18.160]   Oh, oh, oh, oh, oh, oh, oh, oh, oh, oh. Our leaders have, we have seen so many train wrecks coming in
[00:10:18.160 --> 00:10:24.480]   historically, we have decided not to deal with them until it's either too late or it's just
[00:10:24.480 --> 00:10:29.360]   an edible and then we stick a band-aid on it. I mean, in some ways, that's why Trump is
[00:10:29.360 --> 00:10:34.000]   in office now is because our government has become so right. We let it get to this point.
[00:10:34.000 --> 00:10:38.800]   Uh, I bet you could, but you could force Apple. I mean, and I'm sure he'll attempt to force Apple.
[00:10:38.800 --> 00:10:43.440]   That'd probably be the tip for TAT to repay trading Apple's 200 plus billion dollars
[00:10:43.440 --> 00:10:48.240]   that's held overseas would be. And by the way, let's get some manufacture back in the United
[00:10:48.240 --> 00:10:52.320]   States. But I don't know if those are going to be jobs that will make much of difference in the
[00:10:52.320 --> 00:10:57.360]   long run or maybe they will. I don't know. You know, when I talked to companies that are
[00:10:57.360 --> 00:11:03.440]   trying to do manufacturing in the US, one of the challenges is we just don't have the
[00:11:04.400 --> 00:11:09.280]   cluster of needed manufacturing. So like, if I'm going to make, let's say I'm going to make a
[00:11:09.280 --> 00:11:14.320]   tech toy, for example, I'm going to have the electronics assembled somewhere, some of the
[00:11:14.320 --> 00:11:19.120]   electronics, then I have to ship them someplace else to get like the screens put in that I'm going
[00:11:19.120 --> 00:11:25.680]   to have to have the plastics done elsewhere. And I have to ship it to that place, whereas in China,
[00:11:25.680 --> 00:11:32.160]   you can go like, down the street and get all that done. And that's not merely because you pay less.
[00:11:32.960 --> 00:11:38.400]   Right. Although that's part of it. But a lot of it is that that it's just logistically easier to
[00:11:38.400 --> 00:11:44.080]   make it in China and then bring it all over here and boats and planes. Well, I don't, yeah, we'll see.
[00:11:44.080 --> 00:11:48.720]   I mean, it's going to be it's interesting times ahead. What else have you seen at Davos that's of
[00:11:48.720 --> 00:11:56.240]   interest to the tech? Well, we mentioned this from Germany. I was in Germany beforehand at the
[00:11:56.240 --> 00:12:01.920]   DLD conference. And while I was there, the German government, an EU or other release to report,
[00:12:02.640 --> 00:12:09.840]   arguing that robots and us AI algorithms should have personhood and the potential to own
[00:12:09.840 --> 00:12:14.800]   intellectual property and be legal entities that you can sue.
[00:12:14.800 --> 00:12:25.120]   How's that for? So companies or people, robots or people, algorithms or people
[00:12:25.120 --> 00:12:27.280]   pays for robots. It's sued.
[00:12:31.920 --> 00:12:33.760]   Because if the robot, if the AI,
[00:12:33.760 --> 00:12:40.000]   invents something new, the argument in this European report is that the
[00:12:40.000 --> 00:12:46.400]   software should own the rights to that and license it and then gets assets.
[00:12:46.400 --> 00:12:53.120]   Oh, desk silly, because a robot in that case would be an employee of whatever company it works for.
[00:12:53.120 --> 00:12:57.840]   And that the same, I mean, people have those same rules already when you go to work for a tech
[00:12:57.840 --> 00:13:02.480]   company or anything. I don't know. Stacy, Stacy, you know, can you can you be a tough negotiator
[00:13:02.480 --> 00:13:07.520]   and get the robot to sign that contract? I don't know. We'll not sign. We'll not sign.
[00:13:07.520 --> 00:13:13.760]   Where is my robot union? That's one thing. The other thing I saw both in Munich and here at
[00:13:13.760 --> 00:13:17.600]   Davos is a great guy. I'm telling you at some point, you got to get him on
[00:13:17.600 --> 00:13:22.720]   triangulation named Jürgen Schmid Schuber. I just want to get him on for his name.
[00:13:23.760 --> 00:13:29.520]   Yeah, so SCHMIDHUBER. I'm telling you to look him up. He is one of the fathers. I didn't know
[00:13:29.520 --> 00:13:35.120]   this of AI. He has a big Swiss thing in AI. He is a geekskake. He's just wonderful.
[00:13:35.120 --> 00:13:43.680]   He starts riffing on the potential for AI and the speed that will come in the next 20 to 50 years.
[00:13:43.680 --> 00:13:52.400]   And how basically what he says is that there will be colonization of the universe by AI,
[00:13:53.200 --> 00:13:57.920]   by hardware, and that we're getting this guy on. We're getting this guy on.
[00:13:57.920 --> 00:14:04.320]   And intelligence will travel then indeed at the speed of light through radio waves from AI
[00:14:04.320 --> 00:14:08.400]   installation. And the whole thing about science fiction, he started reading science fiction
[00:14:08.400 --> 00:14:12.800]   when he was 10 years old and got so fascinated with it, he had to do it. He's a huge AI expert.
[00:14:12.800 --> 00:14:18.080]   And he's one of the inventors with his students of the, what is it called?
[00:14:19.360 --> 00:14:26.960]   Long-term short-term memory thing that enables so much of AI today. And yeah, so he's really
[00:14:26.960 --> 00:14:31.520]   quite amazing. And he's just, well, I got to get a video interview with this guy. He is wonderful.
[00:14:31.520 --> 00:14:36.720]   That was a huge discovery of this trip as Jürgen Schmidhuber. I'm telling you this.
[00:14:36.720 --> 00:14:42.560]   First line on his website says, since the age of 15 or so, the main goal of Professor Jürgen
[00:14:42.560 --> 00:14:48.800]   Schmidhuber has been to build a self-improving artificial intelligence smarter than himself and
[00:14:48.800 --> 00:14:56.880]   then retire. Sounds good to me. Wow. He's a neural network geek of major
[00:14:56.880 --> 00:15:04.320]   problems. No problem with Skynet at all. Oh, no, God, no, God, no. So I went to a panel discussion
[00:15:04.320 --> 00:15:09.360]   when he was on a day for a German newspaper insight. And he was on with David Kenny, he was the
[00:15:09.360 --> 00:15:18.640]   head of Watson at IBM. And I got, he runs another key company in AI technology, who's
[00:15:18.640 --> 00:15:27.200]   business card. I can't, I will find, no, I won't find. But yeah, he gleefully talks about how
[00:15:27.200 --> 00:15:35.520]   we're going to have this universe of AI entries that within 50 years, I think this 50 years,
[00:15:35.520 --> 00:15:42.480]   a small, cheap device will have more computing power than every brain in humanity.
[00:15:42.480 --> 00:15:47.760]   That makes sense. And the key is, and I'm certainly not one to argue with Professor
[00:15:47.760 --> 00:15:53.680]   Schmidhuber, but the key to this is there's a difference between how a computer thinks and how
[00:15:53.680 --> 00:15:59.440]   a human thinks there's a significant difference between the, you know, the von Neumann.
[00:15:59.440 --> 00:16:03.040]   But does that mean that the job? Well, and that's what Kurzwell always said, as well,
[00:16:03.040 --> 00:16:07.600]   if you can't tell the difference, it doesn't matter. And yet it's because of the apples and
[00:16:07.600 --> 00:16:12.000]   oranges, meaningless to say a computer that has all the intelligences of every human brain in
[00:16:12.000 --> 00:16:16.160]   existence, since they just don't work the same way. Or am I wrong Kevin Marx? What do you think?
[00:16:16.720 --> 00:16:19.760]   Well, that's, I mean, that part of the reason people are excited about recurrent neural networks
[00:16:19.760 --> 00:16:23.520]   is they do work more like brains or at least parts of brains. Certainly they work like the
[00:16:23.520 --> 00:16:26.640]   sort of vision and audio pressing parts of the brain. That's what we need to talk about.
[00:16:26.640 --> 00:16:30.480]   This is the guy who developed recurrent neural networks, by the way, so he's to be
[00:16:30.480 --> 00:16:33.520]   not not you, Kevin Schmidhuber. Yes, not me. Yeah.
[00:16:33.520 --> 00:16:40.320]   I don't know what we're doing about. Yeah. Good job. Well done. We blame you, Kevin. Deep
[00:16:40.320 --> 00:16:45.600]   mind was created by his lab's former students, two of the first four members of Deep Mind got
[00:16:45.600 --> 00:16:51.120]   their PhDs under Schmidhuber. So, so he's definitely, I would love to get
[00:16:51.120 --> 00:16:55.440]   a video of it. I don't know if there's any video of this guy, but I'm telling you, he is a gold
[00:16:55.440 --> 00:17:05.920]   line. I've discovered the geek God. He, he did it. Ask me anything on a year ago on Reddit.
[00:17:05.920 --> 00:17:09.440]   And by the way, his name is pronounced you again, Schmidhuber.
[00:17:11.920 --> 00:17:18.480]   According to him, hmm, interesting. Very interesting.
[00:17:18.480 --> 00:17:24.960]   The other interesting thing from here is a lot of fake news discussions, which we can get to
[00:17:24.960 --> 00:17:31.840]   in a minute, but also the the Edelman Trust Plan barometer, Edelman PR company for 17 years now
[00:17:31.840 --> 00:17:38.400]   has done a and you can look it up and get the graphics has done a survey of trust across the
[00:17:38.400 --> 00:17:44.400]   world, across institutions. Media's trust has fallen hugely, but worse than anything else.
[00:17:44.400 --> 00:17:49.520]   All trust levels of all institutions have fallen. Media is now down as low as government.
[00:17:49.520 --> 00:18:00.160]   Trust is what we're seeing is here in Davos, is that it truly is the populism war against the elite.
[00:18:00.160 --> 00:18:04.240]   The elite's here, but the the world out there is angry.
[00:18:05.440 --> 00:18:10.720]   Here's the inversion of influence authorities. I actually argue with them on this because they
[00:18:10.720 --> 00:18:15.520]   talk about how influence authority comes to the bottom of the pyramid, but they still talk about
[00:18:15.520 --> 00:18:20.240]   a mass. And of course, I argue with that. What is the mass? Yeah, the trust gap wide.
[00:18:20.240 --> 00:18:25.440]   So otherwise, I've been all kinds of fake news sessions talking about that. And that's pretty
[00:18:25.440 --> 00:18:38.240]   much my office report thus far. I love I'm looking at you against AMA. Somebody asked him, what
[00:18:38.240 --> 00:18:41.920]   what is something that's true, but nobody agrees with you on. And Schmidhover says,
[00:18:41.920 --> 00:18:48.560]   many think intelligence to our point to our conversation in a minute ago is is this awesome,
[00:18:48.560 --> 00:18:52.480]   infinitely complex thing. I think it's just the product of a few principles that will be
[00:18:52.480 --> 00:18:57.680]   considered very simple in hindsight. So simple, even kids will be able to understand and build
[00:18:57.680 --> 00:19:02.720]   intelligent, continually learning more and more general problem solvers. Partial justification of
[00:19:02.720 --> 00:19:06.720]   this belief, there already exists blueprints of universal problem solvers developed in my lab.
[00:19:06.720 --> 00:19:11.360]   And the principles of our less universal, but still rather general, very practical program
[00:19:11.360 --> 00:19:15.440]   learning, the kind we have right now recurrent neural networks can also be described by just a
[00:19:15.440 --> 00:19:21.760]   few lines of pseudo code. He also says general purpose quantum computation won't work.
[00:19:21.760 --> 00:19:29.520]   And so far he's been proven right. Interesting. I will. Well, let's let's, I maybe it'll be an
[00:19:29.520 --> 00:19:35.120]   impossible get carstable work on getting him. I think that would be very, very interesting.
[00:19:35.120 --> 00:19:41.040]   It's what I find interesting is the diversity of opinion about artificial intelligence among
[00:19:41.040 --> 00:19:46.160]   experts. Oh boy. Yeah. No one seems to agree what they can or can't do.
[00:19:46.160 --> 00:19:51.040]   Well, we don't even know what human consciousness really is. Right. That's part problem number one.
[00:19:51.040 --> 00:19:53.680]   We don't know how we again. I think that's the wrong decision.
[00:19:53.680 --> 00:20:01.200]   Whether it's Watson today is helping doctors do a better job of diagnosing tumors and
[00:20:01.200 --> 00:20:08.160]   recommending treatments. It's not a gut feeling like the human brain has, but it tries to do a
[00:20:08.160 --> 00:20:13.840]   better job of making a predictive calculation. That's all. That's all it is. So it's a predictive
[00:20:13.840 --> 00:20:19.280]   calculation. This is interesting because MIT actually, MIT Tech Review had a story.
[00:20:19.840 --> 00:20:25.360]   I can't remember. He was MIT scientists, but an AI defeated someone at poker.
[00:20:25.360 --> 00:20:31.680]   And again, poker is one of those games where people feel like they're interacting with someone
[00:20:31.680 --> 00:20:35.840]   based on gut feelings. Yeah, we would have a huge advantage in poker one would expect.
[00:20:35.840 --> 00:20:43.600]   But really our gut feelings in a lot of cases are just us assessing subconsciously a lot of
[00:20:43.600 --> 00:20:50.560]   minute tells and information. So if a computer, you could easily train a computer to see those
[00:20:50.560 --> 00:20:55.760]   things like micro expressions or I don't know, my hair is blowing this direction and the wind,
[00:20:55.760 --> 00:21:02.080]   thus this must be true. I mean, that's actually probably easier to program than other things.
[00:21:02.080 --> 00:21:08.560]   Like emotions are true intelligence. Kevin has sent us a link to a
[00:21:08.560 --> 00:21:14.400]   Kathy O'Neill site. Kevin may not know that both Stacy and I are big Kathy O'Neill fans.
[00:21:14.400 --> 00:21:21.920]   And which she jibunks this whole idea, right? Well, she jibunks a chunk of it, which is to say
[00:21:21.920 --> 00:21:28.160]   there is a problem where people over rely on this. And you say, oh, we can solve this with an
[00:21:28.160 --> 00:21:34.720]   algorithm or with a big data or whichever. And that, you know, or AI is what we call it now.
[00:21:34.720 --> 00:21:39.120]   It's been called a lot of things over the years. And the point is that if you're not actually
[00:21:39.120 --> 00:21:43.840]   checking what these models are doing, then you've just replaced one opaque process with another opaque
[00:21:43.840 --> 00:21:50.480]   process. And you may be encoding in your people's biases into the system. And if you don't
[00:21:50.480 --> 00:21:54.560]   especially check for that, you're very likely to be because it depends a lot on what you give
[00:21:54.560 --> 00:21:58.880]   it as training data as well. So it's the herb I very much recommend her, her book, Weapons of
[00:21:58.880 --> 00:22:03.440]   Math Destruction, because it's got lots of concrete examples of where people have done this. And
[00:22:03.440 --> 00:22:09.360]   there's someone who need to think about as they start applying these tools. Because the challenge,
[00:22:09.360 --> 00:22:14.880]   the challenge is you're basically teaching computers to sort of do things through hunches and got
[00:22:14.880 --> 00:22:19.040]   instinct. And you can't always introspect what they've done. We try and look inside these neural
[00:22:19.040 --> 00:22:24.480]   networks and you, you know, you find stuff like deep dream and things like that. And it's hard to
[00:22:24.480 --> 00:22:28.080]   know exactly what's going on and how it reached that conclusion.
[00:22:29.680 --> 00:22:37.360]   But Kevin, David Kenny from IBM Watson said today that there's a necessity of transparency
[00:22:37.360 --> 00:22:43.360]   in these systems. So when Watson says to doctors, here is why we think this tumor is X.
[00:22:43.360 --> 00:22:49.200]   It will say here's the five reasons why we say that based on the percentage,
[00:22:49.200 --> 00:22:54.800]   guess here's the treatment that we recommend. Here's why. So he believes that the transparency
[00:22:54.800 --> 00:22:59.440]   is a principle of AI systems, otherwise you can't manage them.
[00:22:59.440 --> 00:23:05.040]   So that's actually Watson giving the probability of how confident he is in certain examples.
[00:23:05.040 --> 00:23:05.040]   Yes.
[00:23:05.040 --> 00:23:13.360]   What what I think is the other issue that people are thinking about isn't it's how does Watson
[00:23:13.360 --> 00:23:20.080]   get to that probability? So it's not it's not just it's the showing your work as opposed to showing
[00:23:20.080 --> 00:23:24.640]   the results of your work. Does that make sense? Yeah, it does. So okay.
[00:23:24.640 --> 00:23:28.240]   Yeah, I mean, I don't know if I'm putting words in Kevin's mouth.
[00:23:28.240 --> 00:23:31.600]   Yeah, that's right. But the other thing that's slightly worrisome about this is that
[00:23:31.600 --> 00:23:36.800]   we people can't necessarily do this either. We can't always give you reasons for how we choose
[00:23:36.800 --> 00:23:44.720]   things. And what we are good at is making up narratives that are plausible. So we will choose
[00:23:44.720 --> 00:23:47.760]   something and then we'll come up with a justification for that. And there's lots of research, this
[00:23:47.760 --> 00:23:52.560]   confabulation of if you are somebody, why they did something, they'll tell you why. And they've done
[00:23:52.560 --> 00:23:59.520]   experiments with showing different things to the two different halves of the brain and having the
[00:23:59.520 --> 00:24:03.200]   left brain narrate the reason why the right brains is something even though we couldn't actually see
[00:24:03.200 --> 00:24:07.680]   that. There's there's and your body will make up your brain will make up something plausible to
[00:24:07.680 --> 00:24:14.480]   justify what you just did. And my worry is that we're going to instead of actually getting the
[00:24:14.480 --> 00:24:21.920]   AIs to give us a you know, an actual explanation for what's going on, we're going to start training
[00:24:21.920 --> 00:24:27.360]   to make up plausible seeming ones like we do as well. And that's and that's that's you know,
[00:24:27.360 --> 00:24:31.360]   that's more worrisome. That that is the real sort of, you know, post truth problem.
[00:24:31.360 --> 00:24:37.200]   What about Jorgen's contention that really all this comes down to us not really understanding
[00:24:37.200 --> 00:24:42.320]   how the mind works and that once we really understand it, it comes down to a few simple
[00:24:42.320 --> 00:24:47.280]   principles that any even a child could understand. If that were the case, it would explain the
[00:24:47.280 --> 00:24:54.880]   situation we're in right now, but it also would kind of imply that we'll that soon this conundrum
[00:24:54.880 --> 00:24:58.880]   will be solved and we will be able to create true artificial intelligence.
[00:24:58.880 --> 00:25:04.560]   Right. But but the thing is that you know, a lot of our artificial intelligence is sort of
[00:25:04.560 --> 00:25:08.880]   post-oxitification. You know, we make up a narrative that's humans convincing for others.
[00:25:09.920 --> 00:25:16.720]   And that's that's the bit that the machines are going to be people are starting to teach the
[00:25:16.720 --> 00:25:21.840]   machines to do that too. There are new story writing robots and. Right. But that's that is a
[00:25:21.840 --> 00:25:26.480]   human feeling. I mean, Kahneman talks about that. We had Jonathan Haidon talking about that,
[00:25:26.480 --> 00:25:34.000]   which is what we think of as reason. Haid uses analogy that that reason is the driver
[00:25:34.800 --> 00:25:40.560]   of an elephant called intuition and reason only thinks it's telling the elephant where to go.
[00:25:40.560 --> 00:25:44.720]   Right. Much more often the elephant decides and then reason comes up with post talk,
[00:25:44.720 --> 00:25:48.880]   as you say, post talk justifications. Yeah. And I should be over makes two arguments.
[00:25:48.880 --> 00:25:56.560]   One is that science fiction, which he loved and read as a child had to invent wormholes
[00:25:56.560 --> 00:26:01.520]   because of the limitations of the human body and so on. But when he says when artificial
[00:26:01.520 --> 00:26:06.880]   intelligence becomes the being that is transmitted by radio waves across all these instances,
[00:26:06.880 --> 00:26:11.760]   the limitations of the human body are meaningless. The second is when he goes to Asimov's laws,
[00:26:11.760 --> 00:26:17.200]   said you fail at the first law because it gets good enough that you can't tell
[00:26:17.200 --> 00:26:22.640]   what's the robot versus what's the human. And when it comes to say that you can't harm the human,
[00:26:22.640 --> 00:26:27.280]   you don't know what the human is. What does that even mean? Yeah. Right. And so as it goes back to
[00:26:27.280 --> 00:26:32.240]   this notion that we I started off with earlier about the EU and personhood for robots,
[00:26:32.240 --> 00:26:40.400]   it dilutes humanity. Right. It says, oh, yeah. Okay. Well, we're also ran. We're also there.
[00:26:40.400 --> 00:26:45.280]   But the intelligence is not human intelligence. We had this argument with the
[00:26:45.280 --> 00:26:49.680]   AI. Well, you know, Schmitt over is kind of saying this. I can't wait to retire. He's essentially
[00:26:49.680 --> 00:26:55.280]   saying I can't wait to retire as a human and let the machines take over. Yeah. I mean, this is
[00:26:55.280 --> 00:27:00.800]   that's a grim future, not necessarily humans. I don't know. I mean,
[00:27:00.800 --> 00:27:08.160]   what if you look at it as humans do what we do best? Presumably those things will make us happy
[00:27:08.160 --> 00:27:14.240]   and do what they do best. That area of expertise is rapidly shrinking. Yeah. What do we do best
[00:27:14.240 --> 00:27:22.000]   besides, you know, drink slurpies and watch TV and kill and create technological means of
[00:27:22.000 --> 00:27:28.400]   self destruction, exactly. I don't know. I feel like there's room for both. Maybe
[00:27:28.400 --> 00:27:35.600]   I'm a was it trans humanist, post humanist? I don't even know. I mean, I've read I've read
[00:27:35.600 --> 00:27:43.840]   thousands of science fiction stories. Sure. There we go. Where humans and robots, you know,
[00:27:43.840 --> 00:27:49.600]   they're interchangeable almost and it's it's there's a lot of parallels there between rights for
[00:27:49.600 --> 00:27:56.000]   robots and rights for disenfranchised people. I don't know. I'm in since we haven't really met
[00:27:56.000 --> 00:28:01.280]   any self aware robots. I'm kind of like, yeah, there's a debate that we can have then, maybe.
[00:28:01.280 --> 00:28:07.120]   So what's this? So what's this Amazon? The Amazon prize that went out, I think yesterday,
[00:28:07.120 --> 00:28:13.920]   I did it's on the rundown. Do you see this? Let's talk about offer prize for being able to come up
[00:28:13.920 --> 00:28:22.080]   with a 20 minute it's it's it's Turing test, but it's the Jeff test, not me Bezos. The Bezos test
[00:28:22.080 --> 00:28:30.080]   for conversation is clearly taking echo to its next generation. Was this was this the 2.5
[00:28:30.080 --> 00:28:33.840]   million dollar prize from a wild back or is this someone do a lot of prizes? So I don't know.
[00:28:33.840 --> 00:28:37.680]   This must be I just saw coverage of it's the yesterday. I don't know. I don't know. Sissy,
[00:28:37.680 --> 00:28:45.760]   I just saw it go in the class. Yeah. Keep on thinking that the the the the the the test isn't necessarily
[00:28:45.760 --> 00:28:50.160]   the human brain because the human brain will be exceeded. It depends on what the task is.
[00:28:50.160 --> 00:28:56.800]   If the task is to colonize the universe and not be limited by human bodies,
[00:28:56.800 --> 00:29:04.080]   to what end I don't know, but Professor Schmittuber proposes this
[00:29:06.560 --> 00:29:12.800]   then using our intelligence becomes rather meaningless. So what what is what is the role of
[00:29:12.800 --> 00:29:19.040]   human beings in the host AI future? What do we do?
[00:29:19.040 --> 00:29:26.320]   That's that's that's that's that's you think about that is what are you good at? What would
[00:29:26.320 --> 00:29:31.200]   you like to do like a cockroach saying, you know, thank God the humans won because now I can just
[00:29:31.200 --> 00:29:40.160]   skittle around and eat snacks because I mean, really, I guess you're right though, we won't care,
[00:29:40.160 --> 00:29:44.960]   right? We won't care. We'll just we'll the robots will tell us what to do.
[00:29:44.960 --> 00:29:50.720]   And I don't know why you guys have like it doesn't have to be this negative.
[00:29:50.720 --> 00:29:57.040]   Why don't you why don't you think of robots as something that I mean they could be programmed
[00:29:57.040 --> 00:30:03.680]   to be nurturing and help us and I mean, like it doesn't have the reason it's first of all that
[00:30:03.680 --> 00:30:08.880]   that we only program them initially at some point and this is what Schmittuber is saying,
[00:30:08.880 --> 00:30:14.720]   they take over both the manufacturer and the programming of themselves because they do it so
[00:30:14.720 --> 00:30:19.920]   much better and so much faster at which point we are now completely superfluous to the cycle.
[00:30:19.920 --> 00:30:23.920]   I mean, this is really five. They're freezing their children. I mean,
[00:30:23.920 --> 00:30:27.840]   sci-fi. Well, that's in fact funny you say that because Ray Kurzweil has Ray Kurzweil
[00:30:27.840 --> 00:30:31.840]   and he said, you don't have to worry about the machines because we're their parents so they will
[00:30:31.840 --> 00:30:40.400]   honor and revere us. Well, it depends on how you feel. He must look at it. He had a nice dad.
[00:30:40.400 --> 00:30:44.960]   That's all I can say. Well, I've seen his daughter interview and that was interesting.
[00:30:44.960 --> 00:30:53.600]   So did you see matches talk stroke essay thing about this about super intelligence?
[00:30:54.080 --> 00:30:54.600]   No.
[00:30:54.600 --> 00:30:59.520]   I just put the link in the chat. I love him. I just really do.
[00:30:59.520 --> 00:31:04.320]   I did see a link to it, but I haven't read it.
[00:31:04.320 --> 00:31:11.520]   And I don't do it just by reading it here or playing the whole thing, but he's basic. I read it,
[00:31:11.520 --> 00:31:15.840]   go and watch that after the show or watch the video because the video works too.
[00:31:15.840 --> 00:31:19.120]   And he's addressing a guy we interviewed on the triangulation, Nick Bostrom.
[00:31:20.080 --> 00:31:24.880]   And his book Super Intelligence and Bostrom is very is afraid, very afraid, right?
[00:31:24.880 --> 00:31:29.840]   So Mache now has some what an answer to that, I guess.
[00:31:29.840 --> 00:31:37.440]   So he's emailed you a link to a Schmidt-Huber video that I haven't watched yet, but it looked
[00:31:37.440 --> 00:31:37.920]   enticing.
[00:31:37.920 --> 00:31:44.720]   So here's a series of different arguments about why AI isn't a threat to humanity.
[00:31:46.800 --> 00:31:50.560]   And there's one that I like is the argument from my roommate.
[00:31:50.560 --> 00:31:54.080]   And he says, "My roommate was the smartest person I ever met in my life.
[00:31:54.080 --> 00:31:57.040]   He was incredibly brilliant. And all he did was lay around and play a word of warcraft between
[00:31:57.040 --> 00:32:01.120]   bong rits. The assumption that any intelligent agent will want to recursively self-improve,
[00:32:01.120 --> 00:32:04.880]   better learn conquer the galaxy to better achieve its goals makes unwarranted assumptions about
[00:32:04.880 --> 00:32:10.720]   the nature of motivation." So he's basically saying, if we build human-like intelligence,
[00:32:10.720 --> 00:32:14.000]   we'll probably end up with a bunch of layabout computers that are trying to describe what it is.
[00:32:14.000 --> 00:32:17.520]   It's perfectly possible and AI won't do much of anything except he's used its
[00:32:17.520 --> 00:32:20.240]   powers of hyper persuasion to get us to bring it brownies.
[00:32:20.240 --> 00:32:29.600]   This is well worth reading this or watching this video.
[00:32:29.600 --> 00:32:32.320]   This is great. Mache is a brilliant guy. That's really neat.
[00:32:32.320 --> 00:32:35.120]   So he made a video out of this?
[00:32:35.120 --> 00:32:39.440]   He did it. This is originally a talk. He has this way of presenting his talks that there's
[00:32:39.440 --> 00:32:43.520]   like a slide and a transcript like this. So there's a link to the video at the top of the
[00:32:43.520 --> 00:32:48.560]   page. Oh yeah, he did a web camp zagreb. All right. Nice.
[00:32:48.560 --> 00:32:56.800]   So you all read, we talked about the Google AI translate AI, New York Times article about
[00:32:56.800 --> 00:33:03.600]   three weeks ago? Yes. Yeah, this is where the Google translate had created a kind of intermediate
[00:33:03.600 --> 00:33:08.320]   language that only it spoke. It created, if it affected new language that it could translate,
[00:33:08.320 --> 00:33:13.440]   but we didn't understand. And part of what Schmidt-Huber said is basically that's
[00:33:13.440 --> 00:33:17.440]   like 10 year old technology. It's just changing. Yeah.
[00:33:17.440 --> 00:33:23.440]   Translate right now. But it took getting the speed and scale of computers to be able to handle
[00:33:23.440 --> 00:33:28.320]   the technology from 10 years ago. So God knows what's happening in the labs now, what's possible now.
[00:33:28.320 --> 00:33:33.840]   Yeah. Right. But it's still hitting up actual like, there's actual hardware.
[00:33:33.840 --> 00:33:39.280]   We're still developing hardware that's going to be fast enough and good enough for real AI.
[00:33:39.280 --> 00:33:44.400]   So, you know, don't worry just yet. So what's the way that we are here?
[00:33:44.400 --> 00:33:51.040]   Because we're building new chips, wasn't it? So isn't that what Jeff Hawkins is up to?
[00:33:51.040 --> 00:33:56.000]   At Numenta, we've also interviewed him. He's an interesting fellow. He was a creator of Palm and
[00:33:56.000 --> 00:34:02.560]   graffiti and his took his billions and finally did what he wanted to do because he's a trained
[00:34:02.560 --> 00:34:07.200]   neuroscientist, which is to create a chip company, Numenta, that was designing chips that worked
[00:34:07.200 --> 00:34:14.000]   like the human mind, massively parallel chips. There are lots of efforts right now to create
[00:34:14.000 --> 00:34:20.800]   the best chip for AI. But Google this has this tensor processing units that they've
[00:34:20.800 --> 00:34:25.120]   go into their data centers and they'll rent you those as part of Google Cloud.
[00:34:25.120 --> 00:34:30.480]   That's one of the most amazing things. Amazon's doing this too. You can just use their research
[00:34:30.480 --> 00:34:42.800]   and their intelligence for your own stuff. The next generation of garage inventors
[00:34:42.800 --> 00:34:48.720]   aren't going to be working with AI clearly. There's all kinds of new tools and mechanisms we
[00:34:48.720 --> 00:34:54.160]   went through that video a few weeks ago from what Google enables you to do just using their
[00:34:54.160 --> 00:34:58.480]   API. They give it to you. They give you the tensor flow and say, "You're voice breaking the shit and all that."
[00:34:58.480 --> 00:35:06.640]   So Google has that with the TPUs plus GPUs. Amazon has it with they have, oh, what are they?
[00:35:06.640 --> 00:35:11.200]   They didn't open source. They, oh, I cannot remember. Anyway, they've got--
[00:35:11.200 --> 00:35:16.480]   It's the Alexa back bar for Alexa, right? Yes. And they also have something else that I'm
[00:35:16.480 --> 00:35:21.520]   blanking on. And then Intel bought a company called Nirvana that also offers AI.
[00:35:21.520 --> 00:35:26.240]   Everybody's doing this. Everybody's doing it. Have you heard of Mesosphere,
[00:35:26.240 --> 00:35:31.360]   M-E-S-O, Sphere? Yes, but that's a distributed operating system.
[00:35:31.360 --> 00:35:35.840]   Is that what it is? Okay. I got him Florian Library. I saw it today.
[00:35:35.840 --> 00:35:40.480]   I know Florian. Sorry. We know Florian's great guy. He is.
[00:35:40.480 --> 00:35:46.160]   I love this Gosh darn show because we get the smartest people on here.
[00:35:47.120 --> 00:35:53.200]   And so Mesosphere is part of how you've got big data and rapid analytics. They have something
[00:35:53.200 --> 00:35:58.320]   called the Smackstack in Mesos, is the M in the Smackstack. And I'm trying to remember
[00:35:58.320 --> 00:36:06.560]   Cloud Native Tech, Hybrid Cloud Freedom, Power Monument Applications, Cloud Native Data Services.
[00:36:06.560 --> 00:36:11.760]   But they have an AI thing as well, huh? They are getting into that direction because you kind of
[00:36:11.760 --> 00:36:18.880]   have to. But it was essentially an operating system to manage highly distributed multiple
[00:36:18.880 --> 00:36:24.640]   data centers. So how long, Kevin, before I have, I've got a nice Synology Nest. How long before I
[00:36:24.640 --> 00:36:30.080]   have something the size of that box in my house that is my own personal AI? Will that, is that in
[00:36:30.080 --> 00:36:36.480]   the future? Well, the thing is, I mean, you need the big chips and the stuff to train the network.
[00:36:36.480 --> 00:36:40.000]   So better to train the cloud. Once you've trained it, you can have it
[00:36:40.000 --> 00:36:45.840]   run it locally. So the training happens in the cloud, but then you can put it in your phone.
[00:36:45.840 --> 00:36:51.280]   So you look at how the Google Translate stuff works. It'll download, it'll translate things to
[00:36:51.280 --> 00:36:56.560]   run locally that will run your phone and do offline translation. So that's classic machine
[00:36:56.560 --> 00:37:02.080]   intelligence where you have big data data sets, you throw them at neural networks, and over time
[00:37:02.080 --> 00:37:07.920]   it learns. But you're saying once those patterns are learned, you can then put them in a smaller
[00:37:07.920 --> 00:37:11.440]   device. You don't need to continue to throw data at it. And those are models.
[00:37:11.440 --> 00:37:18.560]   Models. I love this. I want this. And even if we are going to end up being cockroaches
[00:37:18.560 --> 00:37:22.080]   in the general scheme of things, I would love to have that.
[00:37:22.080 --> 00:37:25.440]   The next thing you need is data, right? You need large.
[00:37:25.440 --> 00:37:28.720]   But see, that's what's going on right now. That's what Google's doing. What do you think Google
[00:37:28.720 --> 00:37:33.520]   gives you free photo storage? What do you think? I mean, this is Facebook, Google, Microsoft,
[00:37:33.520 --> 00:37:39.760]   Apple, all we're all doing this. It turns out these phones that we have in our pockets are
[00:37:39.760 --> 00:37:46.400]   marvelous data collection entities. So we say for them, did you work once that data becomes
[00:37:46.400 --> 00:37:52.800]   the commoditized element as the analysis that because the value? Yes. The insights become the
[00:37:52.800 --> 00:37:58.240]   value. And the reason I say that is because you can do, you can take the exact same data,
[00:37:58.800 --> 00:38:04.400]   like let's say your Facebook, you can take the exact same training data and the way you run that
[00:38:04.400 --> 00:38:09.360]   through how you tweak your algorithm is going to dictate. So if you're if you're
[00:38:09.360 --> 00:38:13.840]   tweaking for recognition, you're going to get one algorithm. If you're tweaking for,
[00:38:13.840 --> 00:38:18.320]   I don't know, likes, you'll get another Facebook's a terrible example.
[00:38:18.320 --> 00:38:28.320]   So it's what people don't seem to realize is it's important to have data. But the harder part
[00:38:28.320 --> 00:38:33.840]   is figuring out how to tweak the algorithm to get what you want from that data, if that makes sense.
[00:38:33.840 --> 00:38:40.400]   Yes. So you need the data, but you also need, often you need it to be labeled as well. There
[00:38:40.400 --> 00:38:45.600]   are doing some networks that will auto extract things. But for a lot of these things, you often
[00:38:45.600 --> 00:38:51.600]   need to have either, for language translation, you parallel texts for image recognition,
[00:38:51.600 --> 00:38:58.080]   you need labeled images that you know, which ones have dogs in or whatever. And you could,
[00:38:58.080 --> 00:39:03.360]   there's time to do more where it will automatically extract features and common things it sees.
[00:39:03.360 --> 00:39:05.600]   So that's not.
[00:39:05.600 --> 00:39:08.400]   And mapping those something else is still all work.
[00:39:08.400 --> 00:39:13.920]   That's that's not quite true. So, I mean, one of the big breakthroughs with, you know,
[00:39:13.920 --> 00:39:19.200]   CNN's convolutional neural nets or whatever is that they can actually find,
[00:39:20.160 --> 00:39:24.080]   they can actually start to decipher things without having labeled data.
[00:39:24.080 --> 00:39:29.280]   That's like really exciting. That's that's the, oh, what's the word?
[00:39:29.280 --> 00:39:30.240]   How do they do that?
[00:39:30.240 --> 00:39:35.280]   They don't want them. They don't want them. They don't want them.
[00:39:35.280 --> 00:39:38.400]   They feel high up for higher order structures. There's a good.
[00:39:38.400 --> 00:39:41.920]   So they still need data to do that though, right? Oh, yeah.
[00:39:41.920 --> 00:39:42.480]   Yeah.
[00:39:42.480 --> 00:39:43.920]   They're going to have a bunch of problems.
[00:39:43.920 --> 00:39:45.120]   But just not as much data.
[00:39:46.080 --> 00:39:50.480]   Well, in the same way that children learn to see by looking at the world a lot and realizing
[00:39:50.480 --> 00:39:52.560]   that there are some things that are there persistently in.
[00:39:52.560 --> 00:39:55.280]   I thought that was how these neural networks work as well.
[00:39:55.280 --> 00:39:56.160]   Isn't that the same?
[00:39:56.160 --> 00:39:58.880]   So you have to optimize for a feature that you want it.
[00:39:58.880 --> 00:40:02.080]   And when you see it doing something that you you reward it.
[00:40:02.080 --> 00:40:05.280]   I the other day, I was just kind of blown away. I use Google Photos.
[00:40:05.280 --> 00:40:10.880]   It's easier and it would seem not so difficult if I typed in Paris to figure out which photos
[00:40:10.880 --> 00:40:13.280]   are of Paris because landmarks, etc.
[00:40:14.000 --> 00:40:18.560]   And it wouldn't be so hard if I typed in Burmese Mountain Dog to find a Burmese Mountain Dog.
[00:40:18.560 --> 00:40:23.920]   But I thought about this. I can type in dogs and Google Photos will see
[00:40:23.920 --> 00:40:31.200]   dogs of a variety of breeds that if you think about it, a Burmese Mountain Dog is completely
[00:40:31.200 --> 00:40:38.640]   different than the Chihuahua. How does Google Photos know that they're both dogs?
[00:40:38.640 --> 00:40:42.480]   That's pretty. I mean, maybe I'm missing something, but that's pretty darn important.
[00:40:42.480 --> 00:40:46.320]   Or does it stop at the high order and say a dog has these characteristics,
[00:40:46.320 --> 00:40:49.280]   but then go to the lower order and say this is a doxor.
[00:40:49.280 --> 00:40:52.800]   Well, notice there are some cat pictures in amongst my dog pictures.
[00:40:52.800 --> 00:40:59.680]   Here's even a metal dog, a dog statue that's in there.
[00:40:59.680 --> 00:41:05.760]   It clearly knows here's dogs, here's a bulldog, here's my papiana, here's a cat,
[00:41:05.760 --> 00:41:11.280]   but it's predominantly dog. Well, here's a llama, but it's predominantly dogs.
[00:41:11.280 --> 00:41:13.520]   Here's a gorilla. I don't know how that got in there, but
[00:41:13.520 --> 00:41:20.480]   but it's doing a surprisingly good job. I guess that's my point in figuring out what makes a dog
[00:41:20.480 --> 00:41:25.600]   a dog and it's more than four legs and ears, eyes and the nose.
[00:41:25.600 --> 00:41:28.480]   And Keith, all of this is forgiving in perfection.
[00:41:28.480 --> 00:41:32.640]   How does it know? How did it know, for instance, that this was a picture of a dog?
[00:41:32.640 --> 00:41:39.360]   That is as undoggy as, I mean, he's looking over his shoulder. It's not the whole body.
[00:41:40.240 --> 00:41:44.960]   But that's your wouldn't make it. That's the, I mean, that's the algorithm.
[00:41:44.960 --> 00:41:48.000]   They've trained the model and tweaked for
[00:41:48.000 --> 00:41:52.720]   in the computer, in this case, picks what it focuses on.
[00:41:52.720 --> 00:41:57.760]   So, or in some models, but it got these by there's my sweet dog,
[00:41:57.760 --> 00:42:04.880]   Ozzy, they got these. I know, he fought a long hard battle, but we decided that it
[00:42:04.880 --> 00:42:10.400]   would come to the, it's conclusion last week. He's a sweetheart. That's why I was searching for
[00:42:10.400 --> 00:42:17.200]   dogs. I want a picture of Ozzy. It doesn't know yet to the search by dog name. That would scare me.
[00:42:17.200 --> 00:42:24.640]   Here's, I think with the final, this was the final blow for Ozzy was when we forced him to wear
[00:42:24.640 --> 00:42:31.360]   the Santa suit. That was, he said, I'm not, I'm leaving. Goodbye.
[00:42:32.160 --> 00:42:38.880]   But Ozzy was the get along dog of get along. He was a get along dog. Oh, I miss him. Anyway,
[00:42:38.880 --> 00:42:45.200]   I didn't want to bring that up, but, uh, sorry, but no, no, no, no. I'm, you know,
[00:42:45.200 --> 00:42:52.480]   I just try, I'm trying not to cry. So, um, anyway, that's, so that's an example of the kind of
[00:42:52.480 --> 00:42:56.560]   apps level of abstraction Google's already doing and doing it thanks to the fact that we're all
[00:42:56.560 --> 00:43:01.840]   giving it so much data. And I think that's, so here's my question. What's the cost of training?
[00:43:01.840 --> 00:43:05.040]   Nobody ever talks about that. How much manual and human effort do you have to have
[00:43:05.040 --> 00:43:09.040]   to train that system to know all those dogs? Is that a high cost or a low cost?
[00:43:09.040 --> 00:43:16.880]   It's computing power. Um, like, like your man, Jurgen said, this has been research that's been
[00:43:16.880 --> 00:43:23.760]   available for a while. Like, image recognition, for example, they came up, like, Jeff Hinton came
[00:43:23.760 --> 00:43:28.800]   up with the algorithms, like, I want to say like over a decade ago, but it wasn't until, or even
[00:43:28.800 --> 00:43:34.240]   longer actually 20 years ago, but it wasn't until like 2012 that he had the compute power to actually
[00:43:34.240 --> 00:43:39.120]   make it powerful enough. Yeah. Same thing with self-driving cars and a whole bunch of stuff, right?
[00:43:39.120 --> 00:43:44.400]   If I searched for cat, I got a lot of my dogs as well. Yeah. Cats and dogs. Look, I didn't say it
[00:43:44.400 --> 00:43:52.080]   was perfect, but I'm just impressed that it could see a variety of dogs. No, you have that you're not,
[00:43:52.080 --> 00:43:57.200]   so you're, you're finding you don't get as many false positives. Right. It's gotten better and better
[00:43:57.200 --> 00:44:03.040]   too, by the way. Yeah, but so much of this is acceptance of, of, of approximation. So I,
[00:44:03.040 --> 00:44:09.040]   I, last week I was at an event at Lufthansa speaking before the CEO, Karsten Spore, and he got up and
[00:44:09.040 --> 00:44:15.600]   talked about how he's very happy that the computers on airplanes are 20 years old, because they work,
[00:44:15.600 --> 00:44:20.800]   because the phone is too unreliable. 20 years is good for an airplane. For some things, he sounds
[00:44:20.800 --> 00:44:25.280]   like Steve Gibson. Yeah, because there's a, there's a precision there. Yeah. But for, for finding out
[00:44:25.280 --> 00:44:28.480]   whether it's a dog or a cat, there's no mission critical issue.
[00:44:28.480 --> 00:44:35.040]   We used to say that about voice. Remember how we like cell phones used to a suck, but b, we were like,
[00:44:35.040 --> 00:44:39.600]   well, I could never talk to someone on a cell phone. It's too unreliable. Where are my five
[00:44:39.600 --> 00:44:44.000]   nines? But everything gets more reliable over time. Let's take a break. This is a good conversation,
[00:44:44.000 --> 00:44:49.360]   a conversation I suspect we will be having over and over again in a more refined fashion over the
[00:44:49.360 --> 00:44:55.840]   next decade until finally we can retire and let the machines take over the show. And, and, you know,
[00:44:55.840 --> 00:45:00.720]   by then they'll have enough. How many hours of footage Patrick Delahaney figured it out? Many
[00:45:00.720 --> 00:45:06.080]   tens of thousands of hours of footage of a, of all, of me particularly, but of all of you too now,
[00:45:06.080 --> 00:45:10.480]   congratulations. We'll make it possible for us to do this show without our intervention. We could
[00:45:10.480 --> 00:45:15.760]   sit back, make brownies for the robots, because they will be wanting those brownies. Our show
[00:45:15.760 --> 00:45:23.280]   today, taking a break. Our show today brought to you by Casper Batteras is an online retailer of
[00:45:23.280 --> 00:45:29.440]   premium mattresses made in the US of a for a fraction of the price because you're buying directly
[00:45:29.440 --> 00:45:34.480]   from the factory. Casper is revolutionizing the mattress industry, cutting out the middleman,
[00:45:34.480 --> 00:45:40.080]   the resellers, the showrooms. And that means you save your effectively, you're buying at the same
[00:45:40.080 --> 00:45:44.960]   price the showroom does, which is a lot less than you've been paying. Now, you may say, but I need
[00:45:44.960 --> 00:45:49.600]   the showroom Leo. I need to lie on a mattress before I buy it. Well, how about if you lie on it
[00:45:49.600 --> 00:45:55.360]   for 100 nights first? Yes, you can return your Casper mattress anytime in the first 100 nights.
[00:45:55.360 --> 00:46:00.720]   And you'll get every penny back a cargo come and get the mattress. You don't have to pack it back up
[00:46:00.720 --> 00:46:06.000]   and whisk it away. And they will return you every cent. You're not going to want to return it though.
[00:46:06.000 --> 00:46:11.840]   You're going to love this obsessively engineered mattress supportive memory foams for a sleep surface
[00:46:13.040 --> 00:46:19.920]   with precisely the right sink, just the right bounce and it's breathable. So it sleeps cool,
[00:46:19.920 --> 00:46:25.600]   fresh. So one of the things you don't get too hot or too cold throughout the night,
[00:46:25.600 --> 00:46:32.800]   it's interesting. I love this page on the Casper site. These are the six things. They did hundreds
[00:46:32.800 --> 00:46:38.560]   of prototypes to test firmness, density, rebound speed, airflow, transition temperatures,
[00:46:38.560 --> 00:46:43.280]   odor too, because you want a mattress to smell clean and fresh. The Casper mattress straight out of the
[00:46:43.280 --> 00:46:50.240]   box, I was waiting because I thought, maybe, nope, it doesn't smell like the factory. It smells
[00:46:50.240 --> 00:46:58.400]   like fresh air. So free delivery to US and Canada and a surprisingly compact box, which is also good.
[00:46:58.400 --> 00:47:04.080]   We sent this to my son when he was a freshman. He's a senior now. Actually, I think he was a
[00:47:04.080 --> 00:47:10.400]   sophomore because he gets tired of the mattress at the dorm. We sent him a Casper mattress and he
[00:47:10.400 --> 00:47:14.800]   was able to, because it's a box to just bring it upstairs. He was on the third floor, cut up in the
[00:47:14.800 --> 00:47:22.240]   box. The mattress is there. He loves it. Statistically, lying on a bed in a showroom has no correlation
[00:47:22.240 --> 00:47:26.960]   to whether the bed is right for you. Why don't you try it in your house, in your own environment,
[00:47:26.960 --> 00:47:32.720]   using it the way you would use it. Free shipping and returns at the Casper mattress. By the way,
[00:47:32.720 --> 00:47:36.080]   you could save an additional $50 off their really low price already.
[00:47:36.080 --> 00:47:42.000]   Towards any mattress purchase, when you go to Casper, C-A-S-P-E-R.com/tweig, use the promo code
[00:47:42.000 --> 00:47:49.200]   toig, $50 off your mattress purchase at Casper.com/tweig with the promo code toigterms and conditions
[00:47:49.200 --> 00:47:55.680]   apply. Casper.com/tweig, we thank you so much for giving me a good night's sleep and my son
[00:47:56.800 --> 00:48:05.760]   and our guestroom and our nephew. Maybe I feel bad. I never did get
[00:48:05.760 --> 00:48:09.200]   Aussie that Casper dog bed. They have dog beds too. I never did get them that.
[00:48:09.200 --> 00:48:12.880]   Oh, I'm sorry we did this to you. Why'd you bring that up?
[00:48:12.880 --> 00:48:16.720]   No, it's okay. This is what happens sometimes. I was talking to our neighbor,
[00:48:16.720 --> 00:48:22.000]   had a very similar situation. Now, our good friend, Evan Dunn, Jason Clanthes,
[00:48:23.200 --> 00:48:29.360]   he just had to put his dog down. Same very similar situation. That's the problem when you get a pet.
[00:48:29.360 --> 00:48:32.720]   You outlive them in the best case anyway. You don't want them to outlive.
[00:48:32.720 --> 00:48:36.880]   Yeah, I knew a brilliant, brilliant blind man who went through juicy and
[00:48:36.880 --> 00:48:42.400]   our dogs and he said, "I can't do it anymore. I can't give up this relationship I have so he
[00:48:42.400 --> 00:48:48.400]   gave up his dog." It's hard. You really do fall in love with your pet.
[00:48:48.400 --> 00:49:00.880]   Let's see. Lots of little things here. Leo, can I ask what is on your desk? It looks like a raccoon.
[00:49:00.880 --> 00:49:05.280]   Oh, that's my new little friend. It is a raccoon. Okay.
[00:49:09.040 --> 00:49:12.480]   I've been looking at that. I'm like, what is under Leo's computer there?
[00:49:12.480 --> 00:49:17.680]   Yeah, there's a little raccoon. There's a long story about this raccoon,
[00:49:17.680 --> 00:49:25.680]   which goes back to Kevin Rose in the famous raccoon throwing incident of 1987. Now, when was
[00:49:25.680 --> 00:49:33.040]   that? That was 2007, probably 2008. Maybe even more recently. And then ever since Patrick Dela
[00:49:33.040 --> 00:49:39.200]   Hante has managed to sneak this little raccoon into many of the shots on our show.
[00:49:39.200 --> 00:49:45.840]   And I noted that the raccoon was back. Okay. Yeah, I have never seen the raccoon before.
[00:49:45.840 --> 00:49:51.120]   I do. I love him. I think raccoons are great. He's a little chill. He's very chill. It doesn't
[00:49:51.120 --> 00:49:58.000]   do much. Just relaxing on the surface studio. You've got windows, windows screen longer than
[00:49:58.000 --> 00:50:02.400]   most things you keep. Oh, I can't give this up. You're going to see this on my desk for a long
[00:50:02.400 --> 00:50:08.000]   time. This is just great. It's a very limited. I don't think everybody would benefit from this.
[00:50:08.000 --> 00:50:12.160]   It shouldn't be. It's not a general purpose computer for most people. It could turn into an
[00:50:12.160 --> 00:50:19.600]   all in one, right? But really, for me, the great use is the 20 degrees using this for our show.
[00:50:19.600 --> 00:50:23.760]   It's out of my way. It doesn't block my shot. And I have this nice big thing. And you probably
[00:50:23.760 --> 00:50:29.920]   noticed I'll tell us straight. I'll zoom in on a story and all of that like this story,
[00:50:29.920 --> 00:50:37.280]   which I'm very excited about. I'm wearing my my Moto 360, which has been discontinued. I love my
[00:50:37.280 --> 00:50:45.840]   Moto 360, but Google is doing Android Wear 2.0. And according to Evan Blass, Evleaks,
[00:50:45.840 --> 00:50:56.000]   it's coming soon, February 9th. It will include a digital crown. Now, this is a mockup. So,
[00:50:56.000 --> 00:51:02.160]   don't look at this picture. This is from Android Police, the mockup. But Venture Beat has also a
[00:51:02.160 --> 00:51:08.000]   story that says that at the same time as Android Wear 2.0 comes out February 9th, Google will
[00:51:08.000 --> 00:51:13.040]   announce a or launch a new smartwatch, which I will be quick to get. I've been very happy with
[00:51:13.040 --> 00:51:20.240]   the Pixel. It really is a phone that really grows on you. And when I wear the Android watch,
[00:51:20.240 --> 00:51:27.840]   it's a nice pair. Blass says the Pixel branded watches will be made by LG.
[00:51:27.840 --> 00:51:34.480]   Oh, they're not going to be Pixel. I'm sorry. They will be branded the LG watch sport and the
[00:51:34.480 --> 00:51:41.120]   LB LG watch style. LG's been making Android Wear 1.0 watches. In fact, I have one of their watches.
[00:51:41.120 --> 00:51:53.760]   It's a little big. Blass says that the Android police render is very close. The larger sport
[00:51:53.760 --> 00:52:03.040]   watch 14.2 millimeters thick, 480 by 40 circular P OLED display, 768 megs of RAM, 4 gigs of storage,
[00:52:03.040 --> 00:52:10.320]   420 milliamp hour battery, and a waterproof rating of IP68 plus heart rate sensor,
[00:52:10.320 --> 00:52:20.240]   Wi-Fi, Bluetooth, this is the one that's interesting, 3G and LTE data. So it will have its own data.
[00:52:20.240 --> 00:52:26.080]   It won't have to be tied to the phone. GPS will be built in and NFC, which confirms kind of the
[00:52:26.080 --> 00:52:31.840]   other rumor that Android pay would come to the watch, just as Apple Watch has Apple pay using
[00:52:31.840 --> 00:52:37.840]   NFC. Smaller watch will be a little thinner, a little lighter, a little smaller battery,
[00:52:37.840 --> 00:52:44.000]   worse IP67 rating, and will lack the cellular GPS NFC and heart rate monitor. So you can get a
[00:52:44.000 --> 00:52:49.680]   thin small watch or a big watch with all the features. Both will have a digital crown.
[00:52:49.680 --> 00:52:58.320]   So right now there is a crown on the Android Wear watch, but it's just a button. A crown implies
[00:52:58.320 --> 00:53:02.480]   that you could twirl it. And that's what you can do on the Apple Watch, right? And apparently
[00:53:02.480 --> 00:53:09.840]   that's what Android Wear 2.0 is going to have this crown that you can turn to navigate. And
[00:53:09.840 --> 00:53:14.320]   there isn't really any navigation in Android Wear. They're just swiping right now. All you can do
[00:53:14.320 --> 00:53:20.320]   is swipe. So navigation would give you more capability. I have to say that's at your peril.
[00:53:20.320 --> 00:53:27.760]   One of the flaws with the Apple Watch is that it is very confusing. It's got a button.
[00:53:27.760 --> 00:53:33.840]   It's got a crown that you can twirl and a crown you can push. Plus it's got force touch,
[00:53:33.840 --> 00:53:39.360]   plus it's got swiping. So it's kind of overloaded with capabilities. And I have to say I've liked
[00:53:39.360 --> 00:53:44.640]   the Android Wear watch because it's a lot simpler. In fact, you don't even need to touch it to do
[00:53:44.640 --> 00:53:49.680]   most of the functions because you can flick your wrist to get the notifications to pop up.
[00:53:49.680 --> 00:53:57.840]   But it's not as capable. So February 9th. And Ron Amadio wrote this article for
[00:53:57.840 --> 00:54:04.400]   us. Technica says, I expect Google to have an event around it. I will buy one immediately.
[00:54:04.400 --> 00:54:10.960]   I like I'm not a big. Is there a price? No. It hasn't launched yet.
[00:54:10.960 --> 00:54:18.640]   If you if you I would, you know, the Apple Watch series ones now down to 250 bucks, 350 for the
[00:54:18.640 --> 00:54:23.600]   series two is the least expensive. And of course you could spend a lot of hour. I would guess that
[00:54:23.600 --> 00:54:30.240]   Google anything like that would be in the I would guess the $300 range, but I might be wrong.
[00:54:30.240 --> 00:54:35.280]   Be nice to see it down below 200. They'd sell a lot of them at that price. So I don't think they
[00:54:35.280 --> 00:54:41.520]   can make them at that price. Now they said this the big one had GPS and 3G. Yeah. And
[00:54:41.520 --> 00:54:47.680]   no, no, no, no, no, no, that's that's 300. How much is Samsung? Samsung has watched with similar
[00:54:47.680 --> 00:54:51.680]   capability. Actually, pretty nice. Watch those gears to watch is those those are around that,
[00:54:51.680 --> 00:54:58.320]   right? 350. They were they were given away free with your your return. Your return to note seven.
[00:54:58.320 --> 00:55:04.880]   By the way, Stacey, several return there's yes. Yes. Yes. And that good thing because Verizon is
[00:55:04.880 --> 00:55:10.640]   now is turning off the ability to make calls with it. Yes. They're there being slowly. Yeah.
[00:55:10.640 --> 00:55:16.560]   They realize that they were hanging on too long. Yeah. But they did manage to get quite a few.
[00:55:16.560 --> 00:55:23.840]   There are 90 90 what 97% 98% now returned. I think the rest of them are in drawers somewhere.
[00:55:23.840 --> 00:55:32.400]   People forgot they bought them. There's so Indonesia, you know, well, also air FDFAA is now
[00:55:32.400 --> 00:55:38.880]   stopped requiring airlines to make announcements. So I know it's getting a little annoying.
[00:55:38.880 --> 00:55:45.680]   This is the Samsung Gear S3, the latest one from Samsung. I think that's a pretty good looking
[00:55:45.680 --> 00:55:52.160]   watch. No longer requires having a Samsung phone either works with other Android devices.
[00:55:52.160 --> 00:55:57.120]   Oh, I didn't know that. Oh, that's nice. Yeah. And they use the crowns as one of the
[00:55:57.680 --> 00:56:05.680]   as a UI thing. I feel like people, you know, you just you got to pick what you want to do on your
[00:56:05.680 --> 00:56:11.200]   wrist. You know, it's it's kind of like I'm like, people are like, I need this functionality and
[00:56:11.200 --> 00:56:17.040]   this and this and this. And I'm like, God, guys, it's a watch. It's not a smartphone. And I just feel
[00:56:17.040 --> 00:56:25.440]   like people just want too much. I guess I'm I'm constantly amazed at like complaints I see about
[00:56:25.440 --> 00:56:31.360]   different smartwatches. Some of this is also the company's trying to figure out what works. I mean,
[00:56:31.360 --> 00:56:36.960]   this category has not taken off. And they're just experimenting at this point to try to figure out
[00:56:36.960 --> 00:56:42.640]   what is it people want on their wrist? They've sort of absorbed adjacent categories. So the
[00:56:42.640 --> 00:56:46.960]   Apple one is basically mostly health tracking. They didn't have to find that a lot. Yeah, Apple
[00:56:46.960 --> 00:56:51.600]   realized Apple decided this is the sweet spot for them is health. Yeah. But that basically means
[00:56:51.600 --> 00:56:55.520]   they've taken the they've gone after a Fitbit and the other ones that were like that. Right.
[00:56:55.520 --> 00:56:57.840]   And basically even the high end of that market as well.
[00:56:57.840 --> 00:57:05.920]   So there's a list of phones that will work with the S3 and it's quite long at HTC Huawei Lenovo
[00:57:05.920 --> 00:57:13.120]   LG Motorola Oppo, Pantech, Sharp, Sony, Vivo, Xiaomi, Shkircera, and Asus. No Google phones on
[00:57:13.120 --> 00:57:19.760]   that list though. No way. Yeah, I'm disappointed that the Pixel's not on that list. Yeah. Yeah.
[00:57:19.760 --> 00:57:24.480]   How long is the as the watch as the Google watch put out?
[00:57:24.480 --> 00:57:32.960]   Feels like two years. When did Android where it was of you know, 350 dollar device that basically
[00:57:32.960 --> 00:57:38.880]   is is completely obsolete now. Yeah, well, we know it wasn't a big seller because Motorola,
[00:57:38.880 --> 00:57:42.560]   which made the best one the Moto 360. That's what I'm wearing has gotten out of the business.
[00:57:42.560 --> 00:57:46.960]   Is there a way I can make any more of those? Yeah. Yeah. When did Android where the watch?
[00:57:46.960 --> 00:57:51.760]   Well, I broke. I stopped working to watch, which is which most, you know, young hip people just
[00:57:51.760 --> 00:57:56.960]   carry their phones. I still miss my watch. So I'm waiting to get a new one. I'll get this as a
[00:57:56.960 --> 00:58:03.280]   miss mark. You should get a different one with the Leo. So that way we can talk about the
[00:58:03.280 --> 00:58:10.480]   differences, the pros and cons. Okay. Okay. It's coming up for three years. It was 2014.
[00:58:10.480 --> 00:58:14.560]   Is it that long? Wow. Wow.
[00:58:14.560 --> 00:58:22.000]   Apparently, we are preparing for payments through Google Assistant as well. So your watch will be
[00:58:22.000 --> 00:58:30.080]   able to pay things. But according to nine to five Google Ben Shun, an APK actually comes from an APK
[00:58:30.080 --> 00:58:35.840]   teardown by XDA developers. The latest version of the Google app contains strings of code, which
[00:58:35.840 --> 00:58:44.320]   at payments coming to the service soon. So you might be able to say, hey, Google, send Stacey $100.
[00:58:44.320 --> 00:58:50.480]   That would be great. Yes. You can do that with Venmo. You could Venmo be late.
[00:58:50.480 --> 00:58:56.480]   Can I talk to Venmo? No. No, because that would be really like if my friend came over, they'd be like,
[00:58:56.480 --> 00:59:01.280]   hey, Google, send this on, you know, send me a hundred bucks.
[00:59:01.280 --> 00:59:06.000]   Well, that's the risk, isn't it, of giving you voice activated payments?
[00:59:06.000 --> 00:59:09.440]   Indeed. Yeah. I'd want some more authentication there.
[00:59:09.440 --> 00:59:16.960]   Well, you know what Apple does that solves the problem is you have to authenticate the watch
[00:59:16.960 --> 00:59:23.440]   through the phone or use a code. And then if you take the watch off, if it breaks its connection
[00:59:23.440 --> 00:59:29.120]   with your wrist, it stops it. The authentication is undone and you have to re authenticate while
[00:59:29.120 --> 00:59:34.800]   it's on your wrist again, which is a very smart way to do that, I think. The watch itself has to be
[00:59:34.800 --> 00:59:40.240]   on your wrist and authenticated while it's on your wrist for it to be used for payments.
[00:59:40.240 --> 00:59:46.240]   Google is about to buy fabric. They've got weave.
[00:59:46.240 --> 00:59:55.840]   This is actually a Twitter. They have thread and weave. Now they have fabric. I think Google's
[00:59:55.840 --> 01:00:00.960]   turning into a textile manufacturer. This is a Twitter tool for developers to make mobile apps.
[01:00:00.960 --> 01:00:02.880]   Obviously Twitter doesn't need that anymore.
[01:00:02.880 --> 01:00:08.560]   They also get crash, crash, let it's in the rest of those, those, those tool sets as well.
[01:00:08.560 --> 01:00:17.120]   Ah, interesting. And they're going to fit that into the Google, the Google network problems.
[01:00:17.120 --> 01:00:20.880]   So Twitter's really stripping down for speed.
[01:00:22.640 --> 01:00:26.880]   Really at some point, I think you just delete all the accounts except real Donald Trump and your
[01:00:26.880 --> 01:00:37.840]   and your golden. Just make it a megaphone for by the way, at noon Eastern on Friday at Potis
[01:00:37.840 --> 01:00:47.200]   will be renamed at Potis 44 and locked. And will be handed over the keys to the account will be
[01:00:47.200 --> 01:00:54.960]   handed over to President Trump. Trump says he's going to continue to prefer real Donald Trump.
[01:00:54.960 --> 01:01:02.400]   He's already got what 22 million people following him there. So but I imagine Potis will be used for
[01:01:02.400 --> 01:01:08.000]   something. The official president of the United States Twitter account. Maybe that'll be the
[01:01:08.000 --> 01:01:13.760]   one who's start to be the staff with the risk and right or Apple to see who's saying it's the
[01:01:13.760 --> 01:01:16.960]   election. There've been a couple of staff tweets, but almost all the tweets have been
[01:01:16.960 --> 01:01:21.520]   Donald Trump tweets, right? He's really that's become his own personal bully pulpit.
[01:01:21.520 --> 01:01:26.560]   Well, he said he said that he dictates a lot of them in any case. He also said that he doesn't
[01:01:26.560 --> 01:01:30.640]   like tweeting, which I don't believe for a second. He just does it because the lying media is after
[01:01:30.640 --> 01:01:36.720]   Well, every president starting with FDR and the fireside chats has looked for a way
[01:01:36.720 --> 01:01:43.120]   to disintermediate, to go directly to the voter. That's a valuable tool for him.
[01:01:43.760 --> 01:01:50.320]   It also I think fits his style because it's brief. Although Trump for some reason thought that was a
[01:01:50.320 --> 01:01:58.320]   280 word limit and it's 100 obviously it's 140 words, but he said in interview characters
[01:01:58.320 --> 01:02:03.280]   in the interview, I have about 280 characters. So maybe that does kind of argue that he doesn't
[01:02:03.280 --> 01:02:07.840]   that he does dictate it that he doesn't know how long. In fact, lately his tweets have been
[01:02:07.840 --> 01:02:12.960]   spanning, haven't they? Instead of being a single tree. But it also fits his style like this. It's a
[01:02:12.960 --> 01:02:17.440]   short sweet to the point. I mean, if ever there was anybody made for Twitter or never a service
[01:02:17.440 --> 01:02:22.240]   made for a guy, it's it's Trump and Twitter. They go together like soup and sandwich.
[01:02:22.240 --> 01:02:35.920]   No comment. Wow. I have to press to recognize. Yeah, let's go somewhere else. I can't do it.
[01:02:35.920 --> 01:02:42.160]   I just even I can't do it. Can't do it. Google plus is back, baby. I am so glad to say
[01:02:42.160 --> 01:02:49.760]   to report that Google's pouring energy and effort engineers are working on Google plus
[01:02:49.760 --> 01:02:56.160]   comment filters. Jesus. Defense or back?
[01:02:56.160 --> 01:03:05.760]   As bizarre. Google. I don't know what's going on. Comment filters needed to be there. Now you have
[01:03:06.480 --> 01:03:12.320]   besides reporting abuse, you can by default low quality, whatever that is, low quality comments
[01:03:12.320 --> 01:03:18.880]   are hidden by default. And but you can as the poster see what was hidden and unhide. That's a
[01:03:18.880 --> 01:03:24.080]   good thing if they do if they do it well. That would help with spam, right? Comment spam has become
[01:03:24.080 --> 01:03:27.280]   a just a real bane of Google plus.
[01:03:29.840 --> 01:03:39.280]   Advance it. It's not a ghost town. I visit it all the time. And the few last remain. Okay, as it goes
[01:03:39.280 --> 01:03:49.600]   town, but there are a few people still wandering among the facades and the muddy streets posting
[01:03:49.600 --> 01:03:55.280]   and those few people, some of them are quite good. Lauren, Lauren Weinstein still posts, right?
[01:03:55.840 --> 01:04:04.160]   Is great. I love his stuff. I have to say it's been going on for about a year that Google plus
[01:04:04.160 --> 01:04:10.720]   has for some reason surfaced as and I wish I could turn this off and tell me if I can,
[01:04:10.720 --> 01:04:18.640]   you know, recommended for you. A lot of arch right wing, you know, racist hate speech posts,
[01:04:18.640 --> 01:04:25.120]   which don't have a lot of likes or comments. So I'm not sure how the algorithm is working, but
[01:04:25.120 --> 01:04:30.240]   you know, I maybe you're recommending those to me because you think I'm so much of a lefty.
[01:04:30.240 --> 01:04:35.280]   I should read more of those. I don't know. But I could stop doing that now.
[01:04:35.280 --> 01:04:42.880]   But see, yeah, look, look, there's some good posters. Casey McKinnon, Alex Steen. There's some humor.
[01:04:42.880 --> 01:04:46.800]   I don't know what's humorous about two found dead, but
[01:04:46.800 --> 01:04:54.480]   oh, I get it. I get it too dead. You'll like this. This you can add this to your stand up.
[01:04:55.440 --> 01:05:06.240]   Gallery. Jeff Jarvis, too dead. Found dead. Thank God. That's a relief. Also,
[01:05:06.240 --> 01:05:09.920]   if you liked the old way of doing things on Google plus, you know, the classic,
[01:05:09.920 --> 01:05:14.480]   say bye bye to your little friend that's going away as well. So they're doing I
[01:05:14.480 --> 01:05:20.400]   I want them to keep this alive. This may be the last place the Donald Trump has a discovered.
[01:05:21.200 --> 01:05:28.400]   Huh? Oh, no. He can't figure out Snapchat. I guarantee you. Oh, I'd love to see some snaps from the Oval Office.
[01:05:28.400 --> 01:05:33.040]   Oval Office or one of Trump's gilded palaces.
[01:05:33.040 --> 01:05:39.680]   Either would be fine. See? He says, Peter's, but palace. Yeah.
[01:05:39.680 --> 01:05:47.040]   What did Putin say? We have better prostitutes in the West. What did he say?
[01:05:47.840 --> 01:05:51.120]   Let me say it was the strangest thing ever.
[01:05:51.120 --> 01:06:00.080]   Putin denying the dossier and saying, but we do have the best prostitutes.
[01:06:00.080 --> 01:06:07.040]   What? What? What is going on? I am I maybe I'm maybe I'm in my
[01:06:07.040 --> 01:06:09.280]   doe-dage and I just don't know it. I don't know.
[01:06:09.280 --> 01:06:13.360]   Nope, then I'm that I'm there too. Because yeah, I was like, huh?
[01:06:13.360 --> 01:06:17.040]   No, but see, but your figment of my imagination, Stacy. So
[01:06:17.680 --> 01:06:20.000]   that doesn't help. There we go.
[01:06:20.000 --> 01:06:23.840]   I was going to
[01:06:23.840 --> 01:06:29.440]   point us to the story about a new wake word for the Amazon Echo.
[01:06:29.440 --> 01:06:35.360]   Oh, please. I haven't seen this on mine and I'm actually curious if any of you guys have seen it.
[01:06:35.360 --> 01:06:37.280]   Really? What is the new new wake word?
[01:06:37.280 --> 01:06:42.720]   The story is that Amazon Echo and Amazon Dot added computer as a wake word.
[01:06:42.720 --> 01:06:44.320]   Oh, that's a good wake word.
[01:06:45.120 --> 01:06:50.960]   Hey, computer. Let me see. They're saying that can I get it in my I'm looking in my Echo app,
[01:06:50.960 --> 01:06:54.400]   which is not called the Echo app, as you know, but we're not supposed to use the A word.
[01:06:54.400 --> 01:07:00.320]   So yes, don't use the A word. So it's under settings. Click your particular device and you'll be.
[01:07:00.320 --> 01:07:04.400]   Oh, I would I would do that because then it'd be like Star Trek computer.
[01:07:04.400 --> 01:07:05.600]   Exactly what it is. Yeah.
[01:07:05.600 --> 01:07:11.280]   When does the wearable echo shaped like a meanie come?
[01:07:11.280 --> 01:07:13.440]   Like a what?
[01:07:13.440 --> 01:07:17.360]   That's what that's what what shape like the thing on your computer.
[01:07:17.360 --> 01:07:21.840]   The indicator that you tap. Oh, yeah. Yeah. Why do you do that?
[01:07:21.840 --> 01:07:26.400]   I heard shape like a weenie come and I'm like, what? What is wrong with you?
[01:07:26.400 --> 01:07:33.360]   No, my wake words are still the two A words in Echo, not not as a computer, but that would be nice.
[01:07:33.360 --> 01:07:36.400]   Computer. Yeah, I would be excited.
[01:07:36.400 --> 01:07:40.000]   It's good. I was the twin shows nuts.
[01:07:40.720 --> 01:07:44.160]   Computer would not be a good trigger word in this in this context. You probably should have
[01:07:44.160 --> 01:07:51.040]   said that. Yeah, Los Angeles has a an echo skill. Speaking of
[01:07:51.040 --> 01:07:55.120]   one of the first major cities with its own skill.
[01:07:55.120 --> 01:08:01.280]   The city of Los Angeles echo skill launched last month with basic information about holiday
[01:08:01.280 --> 01:08:06.640]   events around LA next month. It will include information about reading times at local libraries.
[01:08:06.640 --> 01:08:13.600]   Hey, echo is the local library open for reading information about city council members? Hey,
[01:08:13.600 --> 01:08:19.840]   echo. I know I'm not going to go on with that one. And upcoming council sessions.
[01:08:19.840 --> 01:08:25.280]   I like that. Oh, it would be needed. What if you could do so they're talking about three
[01:08:25.280 --> 01:08:30.880]   one one services? It'd be awesome if I if you could say, Hey, echo, I'd like to report a pothole.
[01:08:30.880 --> 01:08:34.480]   Yes, you could have a conversational interface. Yes.
[01:08:35.600 --> 01:08:40.160]   Whenever I want to report a pothole or figure something out, it's not a recording thing. It's
[01:08:40.160 --> 01:08:44.720]   basically like you have to wait online forever on the phone forever. That'd be terrible. That's
[01:08:44.720 --> 01:08:49.040]   it. That'd be awesome. A phone like thing because then you can actually be where the pothole is
[01:08:49.040 --> 01:08:54.160]   and have the GPS coordinates where is the next thing that I say. It's somewhere down the street
[01:08:54.160 --> 01:08:58.400]   on the corner of this city street. Well, ways is very good at that kind of thing. I you know,
[01:08:58.400 --> 01:09:04.720]   if you're if you're a serious wazer. Yeah, you don't want to be just the Leo. What? Is it five
[01:09:04.720 --> 01:09:09.760]   by you know, it's in the middle of of winter in New Jersey. So five miles from those a pothole.
[01:09:09.760 --> 01:09:16.160]   No poop. Of course there is this New Jersey in the winter. Yeah, yeah, try it. Yeah. Can you narrow
[01:09:16.160 --> 01:09:22.480]   it down a little? Driven on some roads where some some bozo the life has gone through and
[01:09:22.480 --> 01:09:28.080]   reported everything. It's all it's all to be every day and bottle. Yeah, yeah, yeah, yeah, I know.
[01:09:29.280 --> 01:09:34.960]   Why is it the only the and it was a one brief shining moment. The motorola the
[01:09:34.960 --> 01:09:40.720]   Moto X allowed you to have any wake phrase you wanted and nobody else has done that since then.
[01:09:40.720 --> 01:09:46.400]   It can't be that hard if the Moto X did it. Why do you have to be stuck with those three? Can't
[01:09:46.400 --> 01:09:50.080]   we just I mean, there's enough horsepower in that thing that it could record. Charlie
[01:09:50.080 --> 01:09:55.200]   Charlie Kindle came on actually my podcast and we talked about that because it's like
[01:09:55.200 --> 01:09:58.320]   probably the third most common question I get about the event the Kindle.
[01:09:59.200 --> 01:10:05.440]   Sorry, no, he did not. His name is spelled quote the wrong way for Amazon is Kindle E L.
[01:10:05.440 --> 01:10:15.840]   So he said he tried to get Jeff to change it. No. He says that is actually very hard to get it to
[01:10:15.840 --> 01:10:21.040]   you have to train the computer to only hear that word. They're looking for a certain
[01:10:21.040 --> 01:10:27.120]   combination of things that don't occur naturally or as naturally. They apparently also looked at
[01:10:27.120 --> 01:10:34.720]   like how often people's how common a name the a name is. No, but that is that if they could let
[01:10:34.720 --> 01:10:39.680]   you customize it, they wouldn't have to worry about that because I know what's common in my life.
[01:10:39.680 --> 01:10:45.920]   And if I have a kid named a L E X a, that's a very poor choice. And if I happen to have kids
[01:10:45.920 --> 01:10:52.640]   named a L E X a echo and Amazon, I'd be in deep trouble. I wouldn't have it. But on my
[01:10:52.640 --> 01:10:57.920]   end, Moto X, I used to my trigger was I swear to God in a worked great help me Obi-Wan Kenobi.
[01:10:57.920 --> 01:11:03.600]   That is not a phrase that came up a lot in my life. And it was a perfect trigger phrase.
[01:11:03.600 --> 01:11:09.280]   Let me give you I've you heard about the Watson conference call app on GitHub. No.
[01:11:09.280 --> 01:11:16.400]   So I heard about this. So somebody used the API and they wrote an app so that you put it on for
[01:11:16.400 --> 01:11:22.240]   your conference call and it listens for your name. Yeah, Jeff, Jeff wakes you up and then what it
[01:11:22.240 --> 01:11:26.960]   does. Yeah, then it says you attacks to say you got and then what it does is it waits and
[01:11:26.960 --> 01:11:32.000]   sort of on time that it comes back and it says, Oh, sorry was on mute and you have 32 seconds.
[01:11:32.000 --> 01:11:36.560]   It gives you and it gives you the prior two sentences to your name.
[01:11:36.560 --> 01:11:46.720]   This guy works for Splunk. He lives in Shanghai as a trigger. Josh Newlin, this software is
[01:11:46.720 --> 01:11:54.000]   called say what it's a Python script that runs when the meeting begins and the computer microphone
[01:11:54.000 --> 01:12:06.000]   listens in the background. And then oh, m g. Now why why he admits this? Well, he works for Splunk,
[01:12:06.000 --> 01:12:14.240]   which is an AI company. That's great. Isn't that is that you're on that. Thank you Splunk.
[01:12:16.000 --> 01:12:19.680]   What about this? I'm going to this talk. Let's get some conspiracy theories going because, you
[01:12:19.680 --> 01:12:25.520]   know, there's a podcast isn't a podcast without a few good conspiracy theories. Amazon has sought
[01:12:25.520 --> 01:12:32.720]   permission from the state of Washington to run mystery wireless tests.
[01:12:32.720 --> 01:12:42.880]   Oh, yes, I was kind of look at see in rural Washington and Seattle. They don't say what the
[01:12:42.880 --> 01:12:47.120]   test would be before, but the filing hints a new type of technology or wireless service.
[01:12:47.120 --> 01:12:53.200]   The project would involve prototypes designed to support quote innovative communications capabilities
[01:12:53.200 --> 01:12:58.000]   and functionalities. Maybe the woods will be listening. You'll be able to say, Hey,
[01:12:58.000 --> 01:13:05.120]   echo in the woods. The test could involve maybe some kind of communication system to control
[01:13:05.120 --> 01:13:11.520]   delivery drones. It is the same. It's the Amazon listed Neil Woodward is the main contact and
[01:13:11.520 --> 01:13:17.280]   he's a retired astronaut who joined Amazon in 2008 and his senior manager for prime air.
[01:13:17.280 --> 01:13:21.600]   The team in charge of the team. Oh, no, wait, wait, wait. That's so cool.
[01:13:21.600 --> 01:13:28.880]   But very Bezos hired an astronaut. And last year. Yeah. All right. Tell me
[01:13:28.880 --> 01:13:32.800]   the problem. Tell me the frequencies again because I'm on the spectrum. Okay,
[01:13:32.800 --> 01:13:40.560]   you have the most frequencies. Seven is allowed in the 700 megahertz band and the 800 then 1,800
[01:13:41.280 --> 01:13:48.320]   and 90 is downlink 1900 and 2,100. Oh, that's sprint. That's mobile phones. Okay. So that's
[01:13:48.320 --> 01:13:57.440]   it's for mobile phones, fixed wireless or broadcast. So boy, you're good. Boy, you're good.
[01:13:57.440 --> 01:14:04.000]   And I'm glad we hired this woman. She's just what an asset you are. You even like just know
[01:14:04.000 --> 01:14:11.200]   things like that. Thank you. I mean, that's that's so that's what the US government
[01:14:11.200 --> 01:14:16.240]   says you can do in those bands. Let's look at 1900. You said 1900?
[01:14:16.240 --> 01:14:23.680]   It's four. Okay. So there's two bands. There's the base or downlink band. That'll be 1930 to 1990.
[01:14:23.680 --> 01:14:28.560]   And then then mobile uplink band 1920 to 1980.
[01:14:28.560 --> 01:14:34.000]   Okay. Those are again fixed in mobile. I think I feel like that makes sense.
[01:14:34.000 --> 01:14:38.720]   A lot of that. Yeah. So that's what this isn't. But it's not something like
[01:14:39.600 --> 01:14:43.760]   there's some for satellite bands, for example, which then you might be like, oh, look, they're
[01:14:43.760 --> 01:14:46.480]   going to use satellite, you know, broadband. So this is
[01:14:46.480 --> 01:14:53.120]   it is under the FCC's experimental authority. And that's why those frequencies.
[01:14:53.120 --> 01:15:00.400]   Low power temporary fixed based transmitters and associated mobile units indoors at and near
[01:15:00.400 --> 01:15:04.880]   its company facilities in Seattle. Those facilities, by the way, are in the back of a retail mall
[01:15:04.880 --> 01:15:09.280]   featuring a hobby lobby store in a walk King restaurant. If you really want to start a conspiracy
[01:15:09.280 --> 01:15:15.840]   theory, I think the hobby lobby has is up to something. No, I'm just kidding. Obviously.
[01:15:15.840 --> 01:15:23.680]   You know, this is probably a non-story, but I just like it. I just like I like to imply that
[01:15:23.680 --> 01:15:31.120]   Amazon is up to secret, interesting stuff. Well, they I mean, you could do some cool stuff with this.
[01:15:31.120 --> 01:15:37.680]   This spectrum, especially at the lower end is actually really good spectrum.
[01:15:38.800 --> 01:15:42.880]   Yeah, no, it's yeah. Yeah, that's the stuff that goes through walls. Yeah. Yeah, it goes through
[01:15:42.880 --> 01:15:46.640]   walls. So it doesn't feel like like I would be like, Oh, maybe they're doing something crazy in
[01:15:46.640 --> 01:15:52.400]   their warehouses. But this feels like something that could be I think it's for that floating drone
[01:15:52.400 --> 01:15:56.640]   drone control warehouse sky. And remember how they had the patent for the warehouse in the sky?
[01:15:56.640 --> 01:16:02.400]   That would make sense if they were testing drone delivery, but what in in the woods, I guess,
[01:16:02.400 --> 01:16:04.880]   but I mean, it would make sense that you would have to have something that would go
[01:16:05.440 --> 01:16:13.440]   from the warehouse to the drone. Anyway, who is it that's test is a grub hub? One of the food delivery
[01:16:13.440 --> 01:16:20.320]   services is testing a little robots that go around street on the street delivering food. Yeah.
[01:16:20.320 --> 01:16:26.720]   How long before somebody says, Oh, there's food in those things. So my friend Bill Gross was here
[01:16:26.720 --> 01:16:33.440]   with me. Did you see? So he said he saw a UPS kind of truck was shown at at XES where the truck
[01:16:33.440 --> 01:16:37.520]   drives to a neighborhood and then in the truck is all bunch of drones and the drones go out from
[01:16:37.520 --> 01:16:42.400]   there. Then the truck goes to the next stop of the neighborhood. So it's kind of Postmates
[01:16:42.400 --> 01:16:47.680]   last mile. I drove. Yeah. Is bill there? I say hi to Bill. Love Bill. He is here. Well,
[01:16:47.680 --> 01:16:52.880]   we were just talking about X one the other day with Jordan Rittner. Love Bill. Yep.
[01:16:52.880 --> 01:16:59.040]   Fleet greatest. I'm so greatest fleet of delivery robots rollout in DC. This is a partnership
[01:16:59.040 --> 01:17:07.920]   with the Postmates and Starship technologies. The food at this point, a Postmates employee
[01:17:07.920 --> 01:17:14.960]   walks behind the drone. The drones are like little vans controlled by a remote pilot. They travel
[01:17:14.960 --> 01:17:20.320]   on sidewalks to within a two mile radius at about four miles an hour that which is kind of fast
[01:17:20.320 --> 01:17:24.560]   can carry up to a 40 pound delivery. Don't you would not want to get hit in the shins by a
[01:17:24.560 --> 01:17:30.160]   a four mile an hour robot carrying 40 pounds of stuff. I imagine it's got sensors that would make
[01:17:30.160 --> 01:17:35.360]   it stop. I hope so. But you could have so much fun messing with it. You could be like, well,
[01:17:35.360 --> 01:17:39.440]   you did you push a button on the back. It opens up and there's your food. I mean, come on.
[01:17:39.440 --> 01:17:49.760]   This I mean, this may something like this seems really cool for you somewhere, but this is
[01:17:49.760 --> 01:17:56.000]   probably not the right thing. Not in a city. I feel like why I did a story like a year ago
[01:17:56.000 --> 01:18:03.200]   about those similar types of drones wandering through hospitals. I think it was dispensing.
[01:18:03.200 --> 01:18:08.240]   Oh, yeah. No, that's that's actually being used right now. Yeah. Yeah. And there's a hotel in San
[01:18:08.240 --> 01:18:14.720]   Francisco. There's a couple of hotels that use robots to like you forget your toothbrush,
[01:18:14.720 --> 01:18:21.840]   you call down and a robot comes to your door with a toothbrush. Those are but those environments
[01:18:21.840 --> 01:18:26.000]   are much more constrained. The robot knows how to go up and down the elevator and go down a hall
[01:18:26.000 --> 01:18:31.680]   and stuff like that. And then you can get greeted by a robot in at the W in San Francisco,
[01:18:31.680 --> 01:18:35.280]   but he but it doesn't move. It just well, you saw that at CES. You were talking about that, right?
[01:18:35.280 --> 01:18:38.560]   Yeah. Yeah. I wasn't talking about that, but I have seen it. I was going to say, it's not as
[01:18:38.560 --> 01:18:42.960]   cool as the Japanese robot with the chis the dinosaur wearing the fez because that thing is
[01:18:42.960 --> 01:18:49.520]   awesome. Why don't all robots look like dinosaurs wearing fez? That would I don't know. What is
[01:18:49.520 --> 01:18:58.000]   the plural of fez? Is it fez is? Fez is? Fez is? Fez is? Fez is? Kevin's like, I'll just tell you,
[01:18:58.000 --> 01:19:02.720]   dumb Americans, dumb Americans don't know a thing, do you? Oh, I'm not sure what it is in the original
[01:19:02.720 --> 01:19:11.120]   Arabic, but definitely is. We're going to take a break. When we come back, Samsung says that the
[01:19:11.840 --> 01:19:18.880]   problem with the battery and the Note 7 was the battery. So that's that's that's very informative.
[01:19:18.880 --> 01:19:25.120]   Actually, on the 23rd, they will release their detailed findings, but they are they are saying now
[01:19:25.120 --> 01:19:31.520]   the battery was the main cause of the Note 7 battery fire.
[01:19:31.520 --> 01:19:40.960]   Don't blame anybody else. And what's going on with Samsung? Isn't this isn't the CEO now wanted
[01:19:40.960 --> 01:19:44.160]   a wanted man? The air apparent. I think it's the air.
[01:19:44.160 --> 01:19:49.280]   Okay. I think so. The court court, I think, blockchain for now. Okay.
[01:19:49.280 --> 01:19:53.440]   Thanks. Wow. Jay Y Lee.
[01:19:53.440 --> 01:20:02.560]   And the Samsung court has rejected arrest of the Samsung air for bribery, perjury, and embezzlement.
[01:20:02.560 --> 01:20:10.240]   So he can continue. He's the vice chairman of Samsung Electronics and they say the
[01:20:10.240 --> 01:20:17.440]   D blumberg says the de facto head of Samsung group. His grandfather founded it.
[01:20:17.440 --> 01:20:24.960]   Let us take a break and talk about something that you know very well, Stacy.
[01:20:24.960 --> 01:20:34.240]   You use it all the time. Food glorious. Hot cupcakes and mustard. It is no, it is the
[01:20:34.240 --> 01:20:41.760]   hero and I've got. Oh, yeah. Here it is. I got three of them right here in my hands. The hero is a
[01:20:41.760 --> 01:20:46.800]   better Wi-Fi. Not just a Wi-Fi router. It's a Wi-Fi system that blankets your whole home
[01:20:46.800 --> 01:20:56.880]   in fast, reliable Wi-Fi. No buffering, no dead zones. Wi-Fi that works. And it's enterprise grade
[01:20:56.880 --> 01:21:01.200]   Wi-Fi. Now with the next generation of mesh technology, they've updated one of the things I love
[01:21:01.200 --> 01:21:06.880]   about Euro that you can't say about most routers out there is it's getting firmware updates on a
[01:21:06.880 --> 01:21:11.200]   regular basis. They you don't even have to do anything. They push the firmware updates. I just
[01:21:11.200 --> 01:21:18.400]   got 2.1 on my hero, which adds true mesh, which makes it possible for Euro to use the fastest,
[01:21:18.400 --> 01:21:24.640]   most reliable path for Wi-Fi automatically. You don't have to do anything. It adapts to network
[01:21:24.640 --> 01:21:30.720]   demand, your home's layout. So the way it works, you put the Euro app on your phone. You install the
[01:21:30.720 --> 01:21:37.120]   first Euro. It's got two ethernet ports, one to connect to your cable modem or your DSL modem or
[01:21:37.120 --> 01:21:42.400]   the internet. The other one you can use to and I do on a 24 port switch. You can use you don't
[01:21:42.400 --> 01:21:47.920]   have to have 24 ports, but I have a lot of stuff. You could connect it to a switch for wired connections.
[01:21:47.920 --> 01:21:55.440]   It is a really cool device then. Once you got that up and running, you go to the where you
[01:21:55.440 --> 01:22:02.400]   haven't bad Wi-Fi. You go and you place the second or third Euro where you haven't bad Wi-Fi and
[01:22:02.400 --> 01:22:08.400]   the app will help you place it properly. And you're just slowly spreading the Wi-Fi throughout the
[01:22:08.400 --> 01:22:14.160]   house. This new true mesh makes the system even more expandable. So you can go well beyond three.
[01:22:14.160 --> 01:22:20.480]   I have five cover the largest homes and most complex layouts and two times the land speed
[01:22:20.480 --> 01:22:26.640]   between euros now. You want a pro tip? Yeah, pro tip. It makes it even better. Euro pro tip.
[01:22:26.640 --> 01:22:34.880]   Euro pro tip. They have an echo skill and you can actually tell the echo to turn off the light,
[01:22:34.880 --> 01:22:40.240]   the LED light on certain Euro routers. It's a good time to turn off the line. Yep.
[01:22:40.240 --> 01:22:48.400]   You can also they also know this is really interesting. Your phone pairs to a particular
[01:22:48.400 --> 01:22:53.680]   Euro or your device. So you can ask echo, where's my phone? And they'll say, well,
[01:22:53.680 --> 01:22:58.080]   it's paired to the dining room. So you can kind of know, oh good. Well, at least I know it's in that
[01:22:58.080 --> 01:23:06.400]   area. And the thing we use all the time with Euro, Euro is actually kind of cool. You can reserve IP
[01:23:06.400 --> 01:23:10.720]   addresses. You can do port forwarding, but you can also see all the connected devices wired and
[01:23:10.720 --> 01:23:18.400]   unwired and assign them to a household member. So we went through and assigned our 14 year olds
[01:23:18.400 --> 01:23:25.920]   iPad, iPhone, computer, Xbox. We said, these are all Michaels. And now I can tell echo echo,
[01:23:25.920 --> 01:23:37.680]   pause Michaels internet. And his and all of a sudden he's offline and has to go to bed or come to
[01:23:37.680 --> 01:23:42.960]   dinner. And by the way, no, he can't say echo, unpause Michaels internet. You have to do that in the app,
[01:23:42.960 --> 01:23:50.000]   which is password protected. I love euros. I this is such a great solution. And so much better than
[01:23:50.000 --> 01:23:56.560]   the old school Wi-Fi router. If you're suffering with bad Wi-Fi, go to E-E-R-O dot com and get free
[01:23:56.560 --> 01:24:01.280]   shipping. If you're in a hurry, which you ought to be, if you're suffering, get this end the suffering
[01:24:01.280 --> 01:24:06.080]   faster with free overnight shipping when you use the offer code twigs. So go to euro.com order
[01:24:06.080 --> 01:24:11.440]   euro. You get one, you can get two, you can get three, you can get more. In fact, I would start
[01:24:11.440 --> 01:24:15.760]   with three probably in most homes and then expand beyond that if you have, if you know, you're spread
[01:24:15.760 --> 01:24:23.120]   out more. And when you use the offer code twig, after ordering, select overnight shipping, use the
[01:24:23.120 --> 01:24:27.840]   offer code twig and they'll that'll be zero cost to you. I said, I told you before I set it up at my
[01:24:27.840 --> 01:24:34.000]   mom's house because she couldn't, she has a studio out back where she weaves and does art. And she
[01:24:34.000 --> 01:24:38.480]   stopped using it because she couldn't get her iPad wouldn't work out there was too far away.
[01:24:38.480 --> 01:24:45.360]   So when I went out there after Christmas, I brought a set of euros put one next to the router,
[01:24:45.360 --> 01:24:49.840]   which is in her office, one in the kitchen and then one in the studio. I actually put another one
[01:24:49.840 --> 01:24:55.840]   in the background. I think I added one more and man, it's great. She's loving it. I also gave her an
[01:24:55.840 --> 01:25:00.960]   echo out in the studio. So she's super happy now. She can listen to classical music and knit. It's
[01:25:00.960 --> 01:25:08.880]   wonderful. E-E-R-O dot com. Don't forget the offer code twig for free overnight shipping.
[01:25:08.880 --> 01:25:19.760]   January 23rd, just announced just moments ago, we will find out that we'll see the full report on
[01:25:19.760 --> 01:25:27.120]   what went wrong with the note seven. They do say something was with the battery. You know, we have
[01:25:27.120 --> 01:25:32.960]   a lot of theories. We've talked a lot about this, what it could be. And of course, the note seven's
[01:25:32.960 --> 01:25:39.520]   been fully recalled. Samsung will probably host a conference call. I doubt they'll do a live event.
[01:25:39.520 --> 01:25:46.000]   Of course, there will be an apology. And the main thing they've got to do at this point is
[01:25:46.000 --> 01:25:49.920]   explain why they how they know it's happening and why it's never going to happen again.
[01:25:55.120 --> 01:26:02.000]   We shall see lots of theories about why it happened. Generally thought that the company tried to put
[01:26:02.000 --> 01:26:08.880]   too much battery in too small a case that the pressure exerted on the battery caused it to leak.
[01:26:08.880 --> 01:26:17.600]   And the leaks got plated and eventually short circuited and caused the fire. And that would
[01:26:17.600 --> 01:26:29.040]   make sense. But you know, only Samsung really knows. So yeah, just leading my hat on your Davos.
[01:26:29.040 --> 01:26:35.440]   I've got to get out of here, hat. Yeah. All right, Jeff Jarvis from Davos been great having you. You
[01:26:35.440 --> 01:26:43.040]   let a panel. Go catch the go catch the bus. Just saw to know is Bono there this year?
[01:26:44.560 --> 01:26:50.080]   No, will I am? Okay. And Shakira's the new Bono. Yeah.
[01:26:50.080 --> 01:26:53.440]   Hips don't lie. Have a great day. Great night.
[01:26:53.440 --> 01:27:01.520]   Who is Sergey? Sergey for the first time in the years. Oh, nice. So how much longer?
[01:27:01.520 --> 01:27:06.640]   Till Friday night and then I fly back Saturday. All right. Then I got a Tokyo.
[01:27:06.640 --> 01:27:12.640]   And I'll call in early next whatever it is Thursday morning or whatever Tokyo. Oh my God,
[01:27:12.640 --> 01:27:17.360]   you are crazy. So you're going to be there during the inauguration. Are they going to have a big
[01:27:17.360 --> 01:27:21.200]   screen and everybody's going to gather around. I'm glad we shot at the country and you know,
[01:27:21.200 --> 01:27:27.600]   I went to the mall and I'm going to take this off. Hold on. It's very hot. I went to the mall.
[01:27:27.600 --> 01:27:33.360]   And I should have brought it for proper purposes, but it'll piss off your audience.
[01:27:33.360 --> 01:27:39.280]   I got a hat made a black hat that just says resist. Oh, I think I'll read it. I saw that. Yeah.
[01:27:39.280 --> 01:27:45.760]   What is the general tenor there? I mean, is it concern? Is it is it? I mean, what's what is the
[01:27:45.760 --> 01:27:51.040]   goal? Yeah, it's it's a lot of concern. And I would say that that that you know, I can make
[01:27:51.040 --> 01:27:57.200]   crumb jokes in any room here because these are not crumb fans. So the law were there very quiet
[01:27:57.200 --> 01:28:01.440]   about it. So this is the elite versus the populist the populist saying here. There's
[01:28:01.440 --> 01:28:04.640]   somewhere there's a populist problem. Yeah, but it ain't here. No, no.
[01:28:05.920 --> 01:28:10.240]   If one of the great lines when when Edelman did his trust barometer, they had a panel discussion
[01:28:10.240 --> 01:28:16.800]   in a banker from Deutsche Bank set up there to the whole assembled room of executives from all
[01:28:16.800 --> 01:28:20.960]   over. He said, you're all bankers now, which is to say nobody trusts any of us.
[01:28:20.960 --> 01:28:27.680]   It's very interesting because I mean, we could be very good for especially for the
[01:28:27.680 --> 01:28:35.440]   Davos crowd. No, because this is a crowd that believes strongly in free free trading globalization.
[01:28:35.440 --> 01:28:40.000]   Oh, yeah, that's probably not. And it's a crowd to believe strongly and things like NATO and the
[01:28:40.000 --> 01:28:48.000]   United Nations. Oh, yeah, that. Yeah. This is crowd that believe strongly in technology.
[01:28:48.000 --> 01:28:54.720]   Yeah, so no, no. Yeah, well, they make some more money and be able to handle it off to their
[01:28:54.720 --> 01:29:00.320]   kids easier. Yeah, but I think beyond that, no, I'm thrilled about that. My tax rate is going to
[01:29:00.320 --> 01:29:08.080]   plummet. So I couldn't be happier. You just disappoint. And I got my and I got my health care
[01:29:08.080 --> 01:29:15.040]   locked in. So screw you guys. Anyway, have fun on Friday and for the rest of the weekend.
[01:29:15.040 --> 01:29:19.280]   We really work. Sorry, sorry, leave you early, but I'm going to go and get the bus.
[01:29:20.320 --> 01:29:25.280]   Thanks at the bike. I can only imagine that Jeff is in the lower tier where the dorms are way out.
[01:29:25.280 --> 01:29:32.320]   The outskirts of there's another. I am I am in the farthest reach of Davos, but there's another
[01:29:32.320 --> 01:29:37.680]   town called Postures, which is far away. I'm very lucky. Very grateful not to be there. So no, it's
[01:29:37.680 --> 01:29:44.320]   an easy bus ride for me. I think clusters is where all the hot shots stay. If they have cars
[01:29:44.320 --> 01:29:49.520]   that drive them in, some some of the people, yes, have cars that they have on call all day. Yeah.
[01:29:49.520 --> 01:29:55.040]   Yeah. Those black cars all kind of clusters. Thank you, Jeff. Have a good night. All right.
[01:29:55.040 --> 01:29:59.280]   Thanks for being here. We really appreciate it. Can you guys hear the dogs barking or the dog
[01:29:59.280 --> 01:30:06.000]   barking? Nope. Nope. Good. That mic. That's why we sent you that mic. That mic is amazing
[01:30:06.000 --> 01:30:12.880]   rejection of background noise. Why is it more of an only mic, which is why you get such a
[01:30:12.880 --> 01:30:17.280]   Yeah, we're going to see. You know what? We should really send Kevin except this mic does not do well
[01:30:17.280 --> 01:30:25.200]   in the in the elements. Yeah. We need to get you a studio. That's what we need. Yeah.
[01:30:25.200 --> 01:30:31.520]   A dog free studio. Oh, you know, I put a story on the list. I should have brought it up when Jeff
[01:30:31.520 --> 01:30:36.320]   was here, but Kevin is here because this feels like a Jeff and Kevin trifecta kind of story,
[01:30:36.320 --> 01:30:40.080]   which is why I put it on there. And for those of you guys who feel like there's not enough
[01:30:40.080 --> 01:30:45.440]   Google angle, it involves the Android Play Store. So it's all the way down at the bottom. It's from
[01:30:45.440 --> 01:30:50.960]   the New York Times. It's Farhad Manju's column about the new censorship thanks to Tech, which is
[01:30:50.960 --> 01:30:58.960]   rejecting app stores. So China shutting down Apple or forcing Apple to take the New York Times
[01:30:58.960 --> 01:31:05.120]   app off of its app stores and basically not just by the way, not just the New York Times,
[01:31:05.120 --> 01:31:12.480]   but Pokemon go. Oh, yes. Yes. I'm like, oh, the freedom of information. But more people are
[01:31:12.480 --> 01:31:21.520]   probably like, I want to play Pokemon go. The article does go into the idea of how we've stopped with
[01:31:21.520 --> 01:31:28.080]   like a decentralized web. And I actually got into a conversation with someone today related to
[01:31:28.080 --> 01:31:33.680]   IoT and how we're kind of replicating the thin client brain in the cloud model with IoT.
[01:31:33.680 --> 01:31:39.040]   And we really need to move towards a distributed decentralized model for all kinds of reasons,
[01:31:39.040 --> 01:31:43.360]   resiliency being one of them. So I was like, oh, that's a good topic for you guys.
[01:31:43.360 --> 01:31:49.360]   Yeah. I don't know. So Kevin, this is this is your thing. So I'm very curious, like,
[01:31:49.360 --> 01:31:57.600]   how do we make the open web a viable business model and attractive to other people?
[01:31:57.600 --> 01:32:02.000]   Well, I know that's to this. There's a class action lawsuit just to prove to go forward against
[01:32:02.000 --> 01:32:08.960]   Apple for restricting apps to the Apple app store on the Apple platform. Now the lawyers for the
[01:32:08.960 --> 01:32:15.040]   class action lawsuit want to make money. So they're asking for damages because they say Apple is
[01:32:15.040 --> 01:32:20.000]   price fixing, which is clearly not the app store. In fact, has had the effect of lowering the value
[01:32:20.000 --> 01:32:25.760]   of applications across the board. But I think you could make a better kit. Well, not as lucrative,
[01:32:25.760 --> 01:32:30.720]   but a better case that the app store is a form of censorship, right? Apple won't allow
[01:32:30.720 --> 01:32:36.560]   politically controversial apps in. Of course, one of that allow adult apps in and will not
[01:32:36.560 --> 01:32:42.240]   allow competitive apps in many cases to protect their monopoly. So I think that's another issue
[01:32:42.240 --> 01:32:47.440]   with app stores. App stores, as sensors can be censored easily by governments, but already
[01:32:47.440 --> 01:32:51.440]   are censoring these companies are already well, that's so good. Apple is Google.
[01:32:51.440 --> 01:32:56.560]   Google has to do any of that. Yeah. Yeah. This is less of a I think that I think that
[01:32:56.560 --> 01:33:02.800]   an Android is you don't have to buy it from the app store. Yeah. The bad thing about iOS is you do.
[01:33:02.800 --> 01:33:08.560]   Yes. So which gives Apple veto power of everything and therefore gives governments a
[01:33:08.560 --> 01:33:14.960]   good. Yes. But I mean, but the other half of this is that the number of apps people installed is
[01:33:14.960 --> 01:33:21.680]   going going down. You know, that people are in many ways over apps and the average number of apps
[01:33:21.680 --> 01:33:24.880]   installed per month is is under one now. So,
[01:33:24.880 --> 01:33:33.840]   Chanel, how are you skills? Wow. What? Yeah. So basically the web is is is is coming back
[01:33:33.840 --> 01:33:38.480]   for this, which is good because it means that you and with the sort of advent of progressive
[01:33:38.480 --> 01:33:43.120]   web apps and the ability to have notifications and persistent stuff and work offline and so on
[01:33:43.120 --> 01:33:47.360]   means that that you can do a lot of the things that you have to build an app for the fall with
[01:33:47.360 --> 01:33:52.160]   web stuff now. So that is an advantage. That means that you can just use a URL to rather
[01:33:53.360 --> 01:33:58.400]   have to make something for the app store. And that's not all those features of their
[01:33:58.400 --> 01:34:04.320]   iOS yet, but they are there on Android and they are there on Firefox and Chrome on desktop as well.
[01:34:04.320 --> 01:34:09.040]   Well, you have the great firewall of China to which blocks the web stuff.
[01:34:09.040 --> 01:34:16.800]   Yeah. So China certainly has a, you know, a better way to just block all of this stuff.
[01:34:16.800 --> 01:34:20.160]   The app store gives it a nice convenient way to prevent, you know.
[01:34:20.160 --> 01:34:25.040]   Yeah. The fire was a lot harder and difficult. I mean, you can sneak the stuff.
[01:34:25.040 --> 01:34:29.840]   Right. But it's the stuff that signal does where they do web fronting where they
[01:34:29.840 --> 01:34:34.320]   they brute messages through other other domains so that to try and block signal,
[01:34:34.320 --> 01:34:38.480]   you basically have to try and block Google as well. Good. Let's do that.
[01:34:38.480 --> 01:34:44.720]   Yeah. You do this at your apparel, of course, that, you know, this is how a first world country
[01:34:44.720 --> 01:34:49.280]   can quickly become a third world country by blocking things like Google.
[01:34:50.080 --> 01:34:54.240]   Signal is a really good example because of course that's the message of the,
[01:34:54.240 --> 01:35:01.200]   the known secure messaging app widely accepted as secure messaging app that is used by dissidents
[01:35:01.200 --> 01:35:07.200]   and others to protect their privacy from overarching governments.
[01:35:07.200 --> 01:35:12.480]   There was a story that WhatsApp was insecure. We talked a lot about that on security now.
[01:35:12.480 --> 01:35:20.640]   Yesterday, I'd refer you to that piece. Bottom line is there's some implementation decisions
[01:35:20.640 --> 01:35:26.320]   in WhatsApp made that do increase your risk, but they're easily fixed in the settings of WhatsApp.
[01:35:26.320 --> 01:35:32.400]   And you should probably use signal. That's, you know, it just works. Right.
[01:35:32.400 --> 01:35:40.960]   I love it. The Farhad brought, dragged out the 1996 declaration of independence of
[01:35:40.960 --> 01:35:46.240]   cyber space space that John's Perry Barlow wrote and has since by the way, somewhat disavowed,
[01:35:46.240 --> 01:35:52.000]   ours is a, and Jeff's been trying to get me to get John Perry Barlow to read, do a dramatic reading
[01:35:52.000 --> 01:35:58.640]   of that for a long time. One of the lines ours is a world that is both everywhere and nowhere.
[01:35:58.640 --> 01:36:05.680]   And as a result, world's governments will never be able to limit or lasso the digital realm,
[01:36:05.680 --> 01:36:12.480]   but they're going to try. And then he quotes John Gilmore, cyberpunk John Gilmore from 1993
[01:36:12.480 --> 01:36:16.960]   and founder of Sun, the net interpret censorship is damaged and roots around it.
[01:36:16.960 --> 01:36:22.800]   Oh, everyone was so naive. Oh, those were the days, weren't they?
[01:36:22.800 --> 01:36:30.480]   Just, well, yeah, well, I think that it's, you still, I think you still can make the case. In fact,
[01:36:30.480 --> 01:36:38.400]   I would make strongly the case that, and I made this case 15 years ago in a documentary that
[01:36:38.400 --> 01:36:44.400]   Kevin Spacey was making about hackers, that hackers are going to be the freedom fighters of the future,
[01:36:44.400 --> 01:36:50.640]   that the people who know how to use the internet and route around government intervention
[01:36:50.640 --> 01:36:56.240]   are going to be the ones who help us preserve our freedoms. And I think you're seeing that
[01:36:56.240 --> 01:37:00.000]   already with people like Moxie Marlin Spike and Signal.
[01:37:00.000 --> 01:37:03.760]   Right, but that's why I want to go ahead Kevin.
[01:37:03.760 --> 01:37:10.160]   So, but also the value that the signal rises is actually usable, unlike PGP and a lot of the
[01:37:10.160 --> 01:37:15.600]   previous tools where, you know, it takes, it takes about a week's worth to get the damn things set up,
[01:37:15.600 --> 01:37:20.640]   and then you have to read it that every year. Signal basically just works. But in order to do
[01:37:20.640 --> 01:37:26.160]   that, they may be dependent on phone numbers, which has, it has other issues, which means you've
[01:37:26.160 --> 01:37:31.760]   got to have a phone number, and B, if you change phones, then you have to re-invite everyone and
[01:37:31.760 --> 01:37:35.040]   redo that. But that was their way of getting around the public key problem, which actually
[01:37:35.040 --> 01:37:40.240]   distributes the kids. Sometimes there's a balance between security and convenience,
[01:37:40.240 --> 01:37:46.880]   and you give up, the more you have one, the less you have the other, unfortunately. But
[01:37:46.880 --> 01:37:53.840]   sophisticated users will know the difference. The most common passwords of 2016.
[01:37:53.840 --> 01:38:01.520]   From Dan, who does on here? Yeah. Aaron Guccione, who is at Kipkeeper, which is one of the, one of the
[01:38:01.520 --> 01:38:07.760]   well-known password, very good password, vault keeper security. They went through 10 million
[01:38:07.760 --> 01:38:13.280]   passwords that became public through data breaches in 2016, and came up with a list.
[01:38:14.320 --> 01:38:20.880]   Are you ready? The 25 most common passwords of 2016, they haven't really changed much. Number
[01:38:20.880 --> 01:38:27.120]   one, one, two, three, four, five, six. Number two, one, two, three, four, five, six, seven, eight, nine.
[01:38:27.120 --> 01:38:35.840]   It's three, PETA. Number three, QWERTY. Number four, one, two, three, four, five, six, seven, eight.
[01:38:37.360 --> 01:38:45.760]   Number five, one, one, one, one, one, one, one. I had a chief engineer who used that as a password,
[01:38:45.760 --> 01:38:50.400]   as I remember. In fact, I think at tech TV, that was the default password. It was one, one, one, one,
[01:38:50.400 --> 01:38:54.720]   one. Number six. Really? What you should want to do. One, two, three, four, five, six, seven, eight,
[01:38:54.720 --> 01:38:59.600]   nine, zero, one, number seven, one, two, three, four, five, six, seven. Number eight, password.
[01:39:00.960 --> 01:39:06.800]   I feel like that's sinking. It's going down. People are learning. It used to be like number one.
[01:39:06.800 --> 01:39:10.960]   It was number one. No, I think one, two, three, four, five, six has always been number nine,
[01:39:10.960 --> 01:39:17.120]   one, two, three, one, two, three, oh, clever. And number 10, obfuscated by doing it backwards,
[01:39:17.120 --> 01:39:25.360]   nine, eight, seven, six, five, four, three, two, one. Why is my noob a password, number 12 on the list?
[01:39:28.000 --> 01:39:37.520]   Number 21, Google. What's your Google password? Google? Wow. Anyway.
[01:39:37.520 --> 01:39:42.080]   Oh, I'm trying to, I was trying to figure out the one Q2W3 and then I'm like, oh, look at your
[01:39:42.080 --> 01:39:48.000]   keyboard, right? Yeah. I'm like, so what's 18? ATCSK. What's that?
[01:39:48.000 --> 01:39:52.000]   Oh, that's it. Number 15 is eight. This is good. This would be a great test.
[01:39:53.200 --> 01:40:00.400]   Right for people to like, okay, you have five seconds to figure out why number 15, what is
[01:40:00.400 --> 01:40:04.480]   one eight ATCSKD2W?
[01:40:04.480 --> 01:40:08.880]   ATC.
[01:40:08.880 --> 01:40:13.920]   18. It doesn't seem like it's related to the keyboard layout, does it?
[01:40:13.920 --> 01:40:16.880]   No, I was typing it in. I'm like, I don't see it.
[01:40:16.880 --> 01:40:21.680]   What is what? But if this is, I mean, apparently this is multiple people.
[01:40:21.680 --> 01:40:27.040]   A lot of people doing it. A lot of people doing it. What does it mean? Come on, chatroom.
[01:40:27.040 --> 01:40:34.400]   It's stuck right between 666, 666, and 777, 777.
[01:40:34.400 --> 01:40:42.240]   So it's got to be pretty obvious. What could it be?
[01:40:42.240 --> 01:40:47.600]   This is the only one you're right, Stacey. This is the only one that's not immediately.
[01:40:47.600 --> 01:40:49.760]   I feel like once we know it's going to be, oh.
[01:40:50.480 --> 01:40:54.640]   I've got three smart people here.
[01:40:54.640 --> 01:40:56.800]   Three RJ.
[01:40:56.800 --> 01:41:00.800]   No, some of these are weird. Like, look at the, look at 20. That's...
[01:41:00.800 --> 01:41:06.000]   I'm sure they all have some meaning somewhere.
[01:41:06.000 --> 01:41:12.640]   Number 20 is three RJ S, one L A seven Q E.
[01:41:12.640 --> 01:41:15.680]   Is it backwards? Is it anything?
[01:41:18.080 --> 01:41:22.000]   I would be a terrible hacker. I'm looking at here for like, I feel like I'm reading license
[01:41:22.000 --> 01:41:25.120]   plates. You know, where you're trying to figure out what are they trying to do.
[01:41:25.120 --> 01:41:30.960]   These are obviously used on sites where it requires a letter and a number, right?
[01:41:30.960 --> 01:41:34.720]   Is shift, is shift helpful?
[01:41:34.720 --> 01:41:40.400]   No, no one, oh, ASCII? No? Okay, so...
[01:41:40.400 --> 01:41:44.560]   Oh, wait a minute. Here's a link to the, in the short room.
[01:41:44.560 --> 01:41:47.280]   It's a hard-coded password for bots.
[01:41:47.280 --> 01:41:49.840]   Oh, this is...
[01:41:49.840 --> 01:41:50.880]   Spambot, yes.
[01:41:50.880 --> 01:41:53.360]   This is a box of spam bots.
[01:41:53.360 --> 01:41:56.400]   It's spam bots, use it. So it's not...
[01:41:56.400 --> 01:41:57.760]   It's like, it's my doe saying, yeah.
[01:41:57.760 --> 01:42:03.120]   This is Graham Cluely, not aptly named, writing at tripwire.com.
[01:42:03.120 --> 01:42:08.480]   The reason is, uh, bots.
[01:42:08.480 --> 01:42:10.240]   Oh!
[01:42:10.240 --> 01:42:10.560]   Huh.
[01:42:13.760 --> 01:42:18.400]   People are not choosing those passwords. Same with 3RJS1LA7QE.
[01:42:18.400 --> 01:42:20.960]   Ooh, I spotted the bot passwords!
[01:42:20.960 --> 01:42:21.520]   Woo!
[01:42:21.520 --> 01:42:23.600]   You are an anti-bot.
[01:42:23.600 --> 01:42:25.600]   I'm a human!
[01:42:25.600 --> 01:42:26.400]   You're a human!
[01:42:26.400 --> 01:42:32.240]   Wow, that's really interesting. It must be hard-coded in some malware or something, right?
[01:42:32.240 --> 01:42:37.120]   Yeah, that's, that's pretty cool.
[01:42:37.120 --> 01:42:37.360]   Yeah.
[01:42:37.360 --> 01:42:39.120]   All right.
[01:42:39.120 --> 01:42:40.960]   We've all learned something here today.
[01:42:42.080 --> 01:42:42.960]   History solved.
[01:42:42.960 --> 01:42:43.680]   Thanks, internet.
[01:42:43.680 --> 01:42:46.000]   You're so smart, internet.
[01:42:46.000 --> 01:42:49.120]   How do people survive without the internet?
[01:42:49.120 --> 01:42:50.560]   Oh, my gosh.
[01:42:50.560 --> 01:42:54.480]   I remember being a reporter in my early, early, early, early days.
[01:42:54.480 --> 01:42:54.880]   Have to go to the library.
[01:42:54.880 --> 01:42:55.760]   Like the first year.
[01:42:55.760 --> 01:42:58.720]   Yeah, you had to use, like, microfreshing.
[01:42:58.720 --> 01:43:00.480]   Your paper had a thing called the Whorekin.
[01:43:00.480 --> 01:43:01.840]   You had to go look up stuff, like...
[01:43:01.840 --> 01:43:02.720]   I remember that.
[01:43:02.720 --> 01:43:03.280]   I remember...
[01:43:03.280 --> 01:43:03.680]   Yeah.
[01:43:03.680 --> 01:43:07.040]   Yeah, you'd go to the microfiche reader and then have like a little spinner,
[01:43:07.040 --> 01:43:08.560]   and you'd go going through the...
[01:43:08.560 --> 01:43:10.480]   Oh, man, those were the days.
[01:43:10.480 --> 01:43:11.280]   Yeah.
[01:43:11.280 --> 01:43:13.760]   It took you like all day to write one story.
[01:43:13.760 --> 01:43:14.240]   Yeah.
[01:43:14.240 --> 01:43:14.960]   It was crazy.
[01:43:14.960 --> 01:43:17.600]   Yeah, but it's crazier than that,
[01:43:17.600 --> 01:43:19.760]   or having to write 50 stories a day because...
[01:43:19.760 --> 01:43:21.200]   Okay, that's also great.
[01:43:21.200 --> 01:43:22.080]   Because it's expected of you.
[01:43:22.080 --> 01:43:25.040]   Because, hey, well, it doesn't take any time to research this.
[01:43:25.040 --> 01:43:28.400]   HTC has a new series of phone, the U-series.
[01:43:28.400 --> 01:43:30.160]   No more headphone jacks for seeing.
[01:43:30.160 --> 01:43:34.480]   And I called it Android companies would copy Apple slavishly,
[01:43:34.480 --> 01:43:37.360]   even though there's no reason to do so,
[01:43:37.360 --> 01:43:38.000]   and eliminate headphone jacks.
[01:43:38.000 --> 01:43:39.520]   Isn't the big Samsung?
[01:43:39.520 --> 01:43:41.280]   Samsung's gonna have the eight.
[01:43:41.280 --> 01:43:41.920]   Is it the eight?
[01:43:41.920 --> 01:43:44.000]   Yeah, the eight would have headphone jacks.
[01:43:44.000 --> 01:43:45.200]   No, I thought they said it would.
[01:43:45.200 --> 01:43:49.120]   Because they were the ones who said with the Note 7 announcement,
[01:43:49.120 --> 01:43:51.280]   and look, it comes with a headphone jack.
[01:43:51.280 --> 01:43:54.400]   But I think the rumor has been that the eight won't.
[01:43:54.400 --> 01:43:54.960]   Courage.
[01:43:54.960 --> 01:43:56.800]   The courage to include the headphone jack.
[01:43:56.800 --> 01:43:58.160]   I hate them.
[01:43:58.160 --> 01:44:00.400]   I hate all of them if they take away my headphone jack.
[01:44:00.400 --> 01:44:00.720]   Me too.
[01:44:00.720 --> 01:44:02.560]   Especially without something.
[01:44:02.560 --> 01:44:05.440]   I mean, apparently people really like their ear pods.
[01:44:05.440 --> 01:44:07.040]   So I don't have them.
[01:44:07.040 --> 01:44:07.760]   I don't know.
[01:44:07.760 --> 01:44:08.240]   But...
[01:44:08.240 --> 01:44:08.960]   I do like the...
[01:44:08.960 --> 01:44:10.320]   You mean the ear pods, the wireless?
[01:44:10.320 --> 01:44:11.440]   I'm sorry, the ear pods.
[01:44:11.440 --> 01:44:12.160]   Yeah, I like the ear pods.
[01:44:12.160 --> 01:44:13.680]   I do like the ear pods.
[01:44:13.680 --> 01:44:15.520]   But are you an iPhone user?
[01:44:15.520 --> 01:44:16.960]   Me?
[01:44:16.960 --> 01:44:17.360]   Yeah.
[01:44:17.360 --> 01:44:18.160]   No, not usually.
[01:44:18.160 --> 01:44:18.880]   I have one.
[01:44:18.880 --> 01:44:21.120]   It makes no sense to have ear pods unless you have an iPhone.
[01:44:21.120 --> 01:44:22.480]   You can pair them to your...
[01:44:22.480 --> 01:44:23.760]   I've paired it on my Pixel.
[01:44:23.760 --> 01:44:27.920]   But they're no better than any other Bluetooth headset in that case, right?
[01:44:27.920 --> 01:44:30.560]   Yeah, so if you take a microphone with it or not?
[01:44:30.560 --> 01:44:31.520]   Yeah, it's a good microphone.
[01:44:31.520 --> 01:44:35.360]   But Apple's doing all this stuff with them that the third parties don't do.
[01:44:35.360 --> 01:44:39.040]   Like it's figuring out which microphone to use, for instance.
[01:44:39.040 --> 01:44:42.000]   But I've been told when I'm on calls with it, it sounds very good.
[01:44:42.000 --> 01:44:43.440]   So...
[01:44:43.440 --> 01:44:46.160]   And it goes down a little bit lower than the ones that are...
[01:44:46.160 --> 01:44:48.320]   At least has a little bit of a...
[01:44:48.320 --> 01:44:50.320]   thing there.
[01:44:50.320 --> 01:44:52.320]   Is that the technical term?
[01:44:52.320 --> 01:44:55.280]   Yeah, the thing, the drip, the little icicle that comes off your...
[01:44:55.280 --> 01:44:57.120]   Comes off your ear.
[01:44:57.120 --> 01:44:59.120]   The wedge.
[01:44:59.120 --> 01:44:59.680]   The wedge.
[01:44:59.680 --> 01:45:00.400]   Right, it's...
[01:45:00.400 --> 01:45:00.400]   It's...
[01:45:00.400 --> 01:45:02.080]   I mean, it's the microphones.
[01:45:02.080 --> 01:45:02.640]   That's what it is.
[01:45:02.640 --> 01:45:03.120]   Yeah.
[01:45:03.120 --> 01:45:03.760]   It's basically...
[01:45:03.760 --> 01:45:04.720]   That's where the microphone is.
[01:45:04.720 --> 01:45:06.400]   Yeah, that's where the microphone lives.
[01:45:06.400 --> 01:45:08.640]   And they could do noise canceling...
[01:45:08.640 --> 01:45:09.440]   I mean, you could do...
[01:45:09.440 --> 01:45:13.120]   One point too is a composition that it can work out where your mouth isn't correlated out.
[01:45:13.120 --> 01:45:14.320]   Exactly what it does.
[01:45:14.320 --> 01:45:16.080]   You're so smart.
[01:45:16.080 --> 01:45:17.280]   There's the...
[01:45:17.280 --> 01:45:23.040]   The other security thing that happened this week was Google Key Transparency.
[01:45:23.040 --> 01:45:24.000]   Did you see that?
[01:45:24.000 --> 01:45:24.880]   No.
[01:45:24.880 --> 01:45:26.480]   Let me find that.
[01:45:26.480 --> 01:45:28.640]   So this is talking about PGP.
[01:45:28.640 --> 01:45:30.160]   It's a key transparency that all,
[01:45:30.160 --> 01:45:33.760]   which I think takes you to the open source thing.
[01:45:33.760 --> 01:45:35.040]   And then they take you to the...
[01:45:35.040 --> 01:45:37.280]   And there's a blog post about it.
[01:45:37.280 --> 01:45:38.560]   So what does that mean?
[01:45:38.560 --> 01:45:39.600]   Key transparency.
[01:45:39.600 --> 01:45:44.880]   So the goal of this is to come up with a way of doing public key mapping.
[01:45:44.880 --> 01:45:52.880]   So that you can map from your accounts to public keys and check that they do actually belong to you.
[01:45:52.880 --> 01:45:56.960]   So it's trying to solve the how do I find somebody's public key thing?
[01:45:56.960 --> 01:45:59.440]   What we've done in the past, of course, is use key servers.
[01:45:59.440 --> 01:46:01.920]   MIT runs one, PGP runs one.
[01:46:01.920 --> 01:46:05.280]   You're supposed to upload your public key there and searchable by it.
[01:46:05.280 --> 01:46:07.760]   Something that they've got a mapping and you can verify it.
[01:46:07.760 --> 01:46:08.080]   Okay.
[01:46:08.080 --> 01:46:09.040]   So you can see...
[01:46:09.040 --> 01:46:11.440]   Well, this is what Keybase.io does.
[01:46:11.440 --> 01:46:13.920]   And I really like Keybase.io, but it's a private...
[01:46:13.920 --> 01:46:14.560]   I think I just lost it.
[01:46:14.560 --> 01:46:16.080]   Oh, you don't hear me?
[01:46:16.080 --> 01:46:17.680]   Hello?
[01:46:17.680 --> 01:46:20.160]   So I think I just had to drop out briefly, sorry.
[01:46:20.160 --> 01:46:20.480]   Okay.
[01:46:20.480 --> 01:46:23.200]   So I didn't quite hear what you said.
[01:46:23.200 --> 01:46:24.560]   I was just saying Keybase does this.
[01:46:24.560 --> 01:46:26.000]   Yes.
[01:46:26.000 --> 01:46:28.640]   So the point of this is it's an open source thing.
[01:46:28.640 --> 01:46:33.280]   It's secured by Merkel hashes, so you can call it a blockchain if you really want to.
[01:46:33.280 --> 01:46:40.400]   And the idea is a verifiable mapping between email address and public key.
[01:46:40.400 --> 01:46:44.160]   And that's the idea behind it.
[01:46:44.160 --> 01:46:48.080]   But so far, it's a bunch of code and they're still working on
[01:46:48.080 --> 01:46:49.280]   integrated as UI.
[01:46:49.280 --> 01:46:52.240]   But they wanted to publish it as code and get code review on it.
[01:46:52.240 --> 01:46:56.720]   Is the idea that at some point Google will run a server that you can use,
[01:46:56.720 --> 01:47:02.240]   that you can log into or upload a key to or upload your email address to and see what's going on?
[01:47:02.240 --> 01:47:03.360]   Is that the idea or...
[01:47:03.360 --> 01:47:09.120]   The idea is that it shouldn't just be one server.
[01:47:09.120 --> 01:47:10.160]   It should be a module service.
[01:47:10.160 --> 01:47:10.400]   Yeah.
[01:47:10.400 --> 01:47:16.400]   And they can basically, you can attach things to this sort of chain of signed keys to say.
[01:47:16.400 --> 01:47:18.400]   Look at this one as well.
[01:47:18.400 --> 01:47:24.480]   I use PGP keys when I download open install open source software to verify that the software
[01:47:25.840 --> 01:47:27.760]   is legitimate.
[01:47:27.760 --> 01:47:31.280]   Keys are routinely used or other ways of doing it.
[01:47:31.280 --> 01:47:33.920]   But PGP is routinely used in open source for that.
[01:47:33.920 --> 01:47:37.760]   And that's, you know, corrupt stores and things like that.
[01:47:37.760 --> 01:47:40.000]   And we use key signing there.
[01:47:40.000 --> 01:47:46.160]   And effectively, when you're using signal, you're doing that.
[01:47:46.160 --> 01:47:47.920]   Signal has a different name for it.
[01:47:47.920 --> 01:47:49.440]   They don't call it a key.
[01:47:49.440 --> 01:47:51.280]   They call it a magic number or something.
[01:47:51.280 --> 01:47:52.000]   Right.
[01:47:52.000 --> 01:47:54.800]   Well, in fact, Open Whisper Systems is one of the teams working on
[01:47:55.680 --> 01:47:58.320]   this as well as teams at Yahoo and Google.
[01:47:58.320 --> 01:48:00.640]   Here's what they say in their blog post at Google.
[01:48:00.640 --> 01:48:05.200]   Our goal is to evolve key transparency into an open source, generic, scalable,
[01:48:05.200 --> 01:48:10.640]   and interoperable directory of public keys with an ecosystem of mutually auditing directors,
[01:48:10.640 --> 01:48:12.240]   directories rather.
[01:48:12.240 --> 01:48:16.320]   So this is kind of, I guess, to replace the PGP key server.
[01:48:16.320 --> 01:48:16.720]   Yes.
[01:48:16.720 --> 01:48:16.880]   Yeah.
[01:48:16.880 --> 01:48:18.240]   That's the idea.
[01:48:18.240 --> 01:48:20.480]   And it's that they already have certificate transparency,
[01:48:21.440 --> 01:48:26.960]   which is where they keep track of the certificates for every website
[01:48:26.960 --> 01:48:29.360]   and put them in a database that's kind of like this.
[01:48:29.360 --> 01:48:31.840]   So they're trying to extend this from what they do for that,
[01:48:31.840 --> 01:48:34.160]   which is every useful service they provide.
[01:48:34.160 --> 01:48:36.080]   And obviously, it's easy for them to do so crawling away,
[01:48:36.080 --> 01:48:38.320]   so they can fetch instead of sort of time.
[01:48:38.320 --> 01:48:43.920]   And they're trying to do this for to do PKI.
[01:48:43.920 --> 01:48:45.920]   So it's interesting, but as I say, it's early days.
[01:48:45.920 --> 01:48:50.560]   And it's not, I'm not sure it's baked into any usable products yet.
[01:48:50.560 --> 01:48:51.760]   We'll watch with interest.
[01:48:51.760 --> 01:48:54.000]   I like what Keybase has done, but it's of course a private
[01:48:54.000 --> 01:49:00.160]   attempt by some very well-known crypto folk, Keybase.io.
[01:49:00.160 --> 01:49:01.120]   I keep a key there.
[01:49:01.120 --> 01:49:02.640]   That's actually where I keep my public key.
[01:49:02.640 --> 01:49:08.400]   And you can use it to track, I'll track Kevin Mark's public key.
[01:49:08.400 --> 01:49:10.560]   And if it changes, I'll be notified, things like that.
[01:49:10.560 --> 01:49:13.360]   Those are all, that's what we need because PGP's
[01:49:13.360 --> 01:49:17.760]   Key server infrastructure is kind of primitive.
[01:49:20.320 --> 01:49:23.440]   We didn't mention it, but Chelsea Manning's sentence will be
[01:49:23.440 --> 01:49:28.560]   commuted, President Obama commuted the bulk of her 35-year sentence,
[01:49:28.560 --> 01:49:32.000]   which was really kind of disproportionate.
[01:49:32.000 --> 01:49:35.280]   People accused of similar or a convicted of similar crimes
[01:49:35.280 --> 01:49:37.120]   typically would have received seven or eight years.
[01:49:37.120 --> 01:49:38.720]   She has served seven.
[01:49:38.720 --> 01:49:39.520]   It'll be commuted.
[01:49:39.520 --> 01:49:42.160]   She'll be released on May 17th.
[01:49:42.160 --> 01:49:47.280]   I think mostly hailed by people.
[01:49:47.280 --> 01:49:54.160]   There was a petition circulating, which got more than 100,000
[01:49:54.160 --> 01:49:56.720]   signatures urging the president to do that.
[01:49:56.720 --> 01:50:02.240]   So I think most of the tech community thinks this is a good idea.
[01:50:02.240 --> 01:50:07.200]   She's attempted to commit suicide twice, has gone on a hunger strike.
[01:50:07.200 --> 01:50:10.640]   Seems like a good idea to commute this sentence.
[01:50:10.640 --> 01:50:13.680]   There are those who say, "Ah, she's a traitor, and
[01:50:13.680 --> 01:50:16.720]   she should serve all 35 years."
[01:50:16.720 --> 01:50:18.880]   But I think for humanitarian reasons her sentence.
[01:50:18.880 --> 01:50:20.160]   She's not been pardoned, by the way.
[01:50:20.160 --> 01:50:25.280]   She's been sentences commuted, which tells me that it's highly unlikely
[01:50:25.280 --> 01:50:28.000]   in the next couple of days Edward Snowden will receive a pardon.
[01:50:28.000 --> 01:50:30.160]   He, of course, never turned himself in.
[01:50:30.160 --> 01:50:32.960]   Was never tried and never received a sentence.
[01:50:32.960 --> 01:50:34.560]   There's no commutation possible.
[01:50:34.560 --> 01:50:36.960]   And if Chelsea Manning hasn't been pardoned,
[01:50:36.960 --> 01:50:40.480]   it seems unlikely that Edward Snowden would be pardoned.
[01:50:40.480 --> 01:50:45.120]   Mark Zuckerberg testified in a suit.
[01:50:46.160 --> 01:50:47.280]   Yes, we know.
[01:50:47.280 --> 01:50:47.760]   No, no, no.
[01:50:47.760 --> 01:50:52.400]   Mark Zuckerberg visited one of the 50 states he wants to visit.
[01:50:52.400 --> 01:50:56.640]   He has now visited one of the 50.
[01:50:56.640 --> 01:50:59.200]   Which state was it?
[01:50:59.200 --> 01:50:59.920]   Where was the trial?
[01:50:59.920 --> 01:51:00.720]   Texas.
[01:51:00.720 --> 01:51:01.920]   Oh, it's one of them.
[01:51:01.920 --> 01:51:02.640]   He's Texas.
[01:51:02.640 --> 01:51:03.440]   No, it wasn't.
[01:51:03.440 --> 01:51:03.920]   No, it wasn't.
[01:51:03.920 --> 01:51:04.480]   It was Dallas.
[01:51:04.480 --> 01:51:06.320]   Dallas is one of those schools.
[01:51:06.320 --> 01:51:08.720]   This is, of course, the Oculus Rift trial.
[01:51:08.720 --> 01:51:15.280]   A small company claims that Oculus stole all of its technology from ZeniMax.
[01:51:16.080 --> 01:51:20.960]   And Zuckerberg says, I never even heard of ZeniMax before this lawsuit.
[01:51:20.960 --> 01:51:23.840]   Okay.
[01:51:23.840 --> 01:51:30.080]   What else?
[01:51:30.080 --> 01:51:33.040]   Oh, I'm like, do you want more insights?
[01:51:33.040 --> 01:51:33.920]   I don't think we need.
[01:51:33.920 --> 01:51:36.400]   He actually, he, I was like, he appeared to be a good adult.
[01:51:36.400 --> 01:51:37.600]   What if you want some?
[01:51:37.600 --> 01:51:39.600]   Well, he's going to run for president in three years.
[01:51:39.600 --> 01:51:41.680]   I was going to say if you want conspiracy theories,
[01:51:41.680 --> 01:51:43.760]   he's totally running for president.
[01:51:43.760 --> 01:51:46.800]   In fact, mark my words.
[01:51:46.800 --> 01:51:52.480]   We are going to see the techno, the crowd, the techno elite,
[01:51:52.480 --> 01:51:56.000]   suddenly gain a great interest in government.
[01:51:56.000 --> 01:51:59.760]   And Peter Teal is going to run for governor of California.
[01:51:59.760 --> 01:52:03.360]   I think Jeff Bezos has ambitions.
[01:52:03.360 --> 01:52:05.200]   He's moved to Washington, D.C.
[01:52:05.200 --> 01:52:09.040]   And not just because of the Washington Post.
[01:52:09.040 --> 01:52:10.000]   He's got a big house.
[01:52:10.000 --> 01:52:10.560]   Did he move?
[01:52:10.560 --> 01:52:11.280]   Did he move there?
[01:52:11.280 --> 01:52:16.400]   Well, he just bought the most expensive house in the most prestigious district of
[01:52:16.400 --> 01:52:20.960]   Washington, D.C., where the Obamas will be his neighbors and many others.
[01:52:20.960 --> 01:52:27.360]   I think there's a, you don't buy a house in D.C., a 23 bedroom house in D.C. for fun.
[01:52:27.360 --> 01:52:31.920]   Well, you don't buy, I mean, if you're going to run for office,
[01:52:31.920 --> 01:52:33.440]   you wouldn't run in D.C., though.
[01:52:33.440 --> 01:52:36.560]   No, but you, but you, you know, may end up living there.
[01:52:36.560 --> 01:52:37.040]   Who knows?
[01:52:37.040 --> 01:52:40.640]   I guess they give you a house, though, right?
[01:52:40.640 --> 01:52:41.600]   They do give you a house.
[01:52:41.600 --> 01:52:46.720]   Wait, I think Zuckerberg is totally, look, he's,
[01:52:46.720 --> 01:52:47.280]   did we talk about this last week?
[01:52:47.280 --> 01:52:48.640]   Is he going to be old enough?
[01:52:48.640 --> 01:52:49.680]   Yes, just barely.
[01:52:49.680 --> 01:52:50.720]   Okay.
[01:52:50.720 --> 01:52:55.600]   His birthday is like the month before the filing deadline, like August.
[01:52:55.600 --> 01:52:58.080]   Because yeah, oh, you only have to be 35, right?
[01:52:58.080 --> 01:52:58.640]   35.
[01:52:58.640 --> 01:52:59.440]   Okay.
[01:52:59.440 --> 01:53:02.080]   I thought you had to be 40 and I was like, there is no way.
[01:53:02.080 --> 01:53:03.680]   Constitutionally, he has to be 35.
[01:53:03.680 --> 01:53:04.480]   He'll be 35.
[01:53:04.480 --> 01:53:05.280]   I think it's August.
[01:53:05.280 --> 01:53:06.480]   What is it?
[01:53:06.480 --> 01:53:08.960]   2018, something like 2019.
[01:53:10.160 --> 01:53:11.200]   So he will make it.
[01:53:11.200 --> 01:53:15.600]   He also has now proclaimed that he has some religious feeling.
[01:53:15.600 --> 01:53:19.120]   At least he's not an atheist anymore, or I don't know.
[01:53:19.120 --> 01:53:21.760]   He's going to visit all 50 states this year.
[01:53:21.760 --> 01:53:23.600]   Maybe you have a town hall.
[01:53:23.600 --> 01:53:29.280]   And most, I think the most important factoid, or fact, I guess,
[01:53:29.280 --> 01:53:33.920]   it's kind of a factoid, is that when they restructured Facebook's
[01:53:33.920 --> 01:53:38.800]   stocks so that he and Priscilla could give away the bulk of it to the foundation,
[01:53:40.000 --> 01:53:44.640]   among the change in bylaws was a change that said he could continue to run Facebook,
[01:53:44.640 --> 01:53:46.640]   even if he held government office.
[01:53:46.640 --> 01:53:49.680]   Now, why do you do that unless you intend to hold government office?
[01:53:49.680 --> 01:53:55.040]   To be president, you can't run a company.
[01:53:55.040 --> 01:53:59.040]   That's why they're watching with great interest to see if Trump skirts that.
[01:53:59.040 --> 01:53:59.280]   No.
[01:53:59.280 --> 01:54:01.280]   He can't skirt that.
[01:54:01.280 --> 01:54:04.480]   No, there is no conflict of interest.
[01:54:04.480 --> 01:54:05.360]   He's the president.
[01:54:09.760 --> 01:54:10.400]   We'll see.
[01:54:10.400 --> 01:54:11.120]   I don't know.
[01:54:11.120 --> 01:54:16.000]   Look, I think from an emotional perspective, from an ethical perspective,
[01:54:16.000 --> 01:54:20.640]   that's the case, but from a purely legal perspective, it may not be the case, right?
[01:54:20.640 --> 01:54:22.320]   He may be exempt, the president's exempt.
[01:54:22.320 --> 01:54:25.600]   In any event, clearly, Zuck is watching with interest.
[01:54:25.600 --> 01:54:32.400]   And I think that this most recent election was a cattle prod to these already massively ambitious
[01:54:32.400 --> 01:54:35.920]   people to perhaps seek public office.
[01:54:35.920 --> 01:54:41.680]   Not because being president of the United States isn't any way better than being
[01:54:41.680 --> 01:54:48.320]   chairman and president of Facebook or Amazon, but because they want to change the world.
[01:54:48.320 --> 01:54:49.920]   Right?
[01:54:49.920 --> 01:54:51.760]   I don't know.
[01:54:51.760 --> 01:54:53.360]   And it looks good on the resume.
[01:54:53.360 --> 01:54:56.720]   I mean, there is the question of like, hey,
[01:54:56.720 --> 01:54:59.680]   once you've been president, what do you do next?
[01:54:59.680 --> 01:55:03.680]   And once you've been the CEO of a massive company that you've grown from scratch,
[01:55:03.680 --> 01:55:04.560]   what do you do next?
[01:55:04.560 --> 01:55:05.040]   What is next?
[01:55:05.040 --> 01:55:05.840]   You go to space.
[01:55:05.840 --> 01:55:07.200]   That's been the historic route.
[01:55:07.200 --> 01:55:10.080]   So it's kind of, you know what?
[01:55:10.080 --> 01:55:11.760]   It turns out space, not.
[01:55:11.760 --> 01:55:15.280]   I really fully believe that we are going to see Silicon Valley
[01:55:15.280 --> 01:55:21.040]   in many ways attempt to get involved in politics.
[01:55:21.040 --> 01:55:25.440]   I don't know why, but I just feel like that seems, the technocrats are, you know,
[01:55:25.440 --> 01:55:26.880]   this is exactly the mindset.
[01:55:26.880 --> 01:55:27.840]   I need it though.
[01:55:27.840 --> 01:55:33.200]   They're going to hate it, but the mindset of a technocrat is these idiots don't know how to
[01:55:33.200 --> 01:55:34.320]   run anything.
[01:55:34.320 --> 01:55:36.240]   We're the smartest people in the room.
[01:55:36.240 --> 01:55:37.920]   We know how to do it right.
[01:55:37.920 --> 01:55:40.320]   Get out of the way and let us just take over.
[01:55:40.320 --> 01:55:41.440]   That's their.
[01:55:41.440 --> 01:55:43.280]   And then they're going to encounter.
[01:55:43.280 --> 01:55:46.160]   Remember when Zuckerberg donated so much money to the schools?
[01:55:46.160 --> 01:55:47.040]   Was it New Jersey?
[01:55:47.040 --> 01:55:48.400]   New Jersey wasted money.
[01:55:48.400 --> 01:55:48.400]   Yeah.
[01:55:48.400 --> 01:55:51.520]   They're going to run into the real world.
[01:55:51.520 --> 01:55:55.840]   They're not well equipped to deal with it because nothing works in the real world,
[01:55:55.840 --> 01:55:57.440]   especially in politics that way.
[01:55:57.440 --> 01:56:01.280]   And what is it when you meet an immovable force?
[01:56:01.280 --> 01:56:01.920]   I feel like there's.
[01:56:02.960 --> 01:56:05.680]   An irresistible force meets an immovable object.
[01:56:05.680 --> 01:56:08.000]   Oh, is that what I want to say?
[01:56:08.000 --> 01:56:11.120]   And then you have what they call a Mexican standoff.
[01:56:11.120 --> 01:56:13.600]   It's not going to be good.
[01:56:13.600 --> 01:56:16.720]   I would like to.
[01:56:16.720 --> 01:56:16.880]   Do it.
[01:56:16.880 --> 01:56:19.840]   So I take it you would not vote for Mark Zuckerberg for president.
[01:56:19.840 --> 01:56:22.080]   I don't know.
[01:56:22.080 --> 01:56:24.000]   I mean, the man doesn't have a platform yet.
[01:56:24.000 --> 01:56:27.360]   I want people to run for office who are tech savvy,
[01:56:27.360 --> 01:56:32.640]   but who genuinely care about fixing our problems and also
[01:56:32.640 --> 01:56:34.240]   genuinely care about people.
[01:56:34.240 --> 01:56:36.400]   I think Mark cares about fixing our problems.
[01:56:36.400 --> 01:56:37.760]   I don't know how it feels about people though.
[01:56:37.760 --> 01:56:42.480]   I don't think he feels because you can't look at the problems we have
[01:56:42.480 --> 01:56:44.800]   divorced from the people.
[01:56:44.800 --> 01:56:47.600]   I think so.
[01:56:47.600 --> 01:56:58.320]   Every idea buys into that of late expressed notion that empathy is not useful in politics.
[01:57:01.040 --> 01:57:02.320]   Good luck to them then.
[01:57:02.320 --> 01:57:11.600]   It was a New Yorker article that I thought was really interesting.
[01:57:11.600 --> 01:57:12.160]   I can't remember.
[01:57:12.160 --> 01:57:13.440]   I've referred to it before.
[01:57:13.440 --> 01:57:19.840]   Empathy in fact can get in the way, right?
[01:57:19.840 --> 01:57:21.680]   It can.
[01:57:21.680 --> 01:57:23.040]   That's what it is.
[01:57:23.040 --> 01:57:29.680]   Yale University psychologist Paul Bloom says that empathy is an overrated trait for a leader.
[01:57:29.680 --> 01:57:31.920]   It can inspire altruism, but it can also be
[01:57:31.920 --> 01:57:34.160]   contribute to bias.
[01:57:34.160 --> 01:57:37.840]   And it does not necessarily translate to moral decision making.
[01:57:37.840 --> 01:57:41.680]   And they did a lot of research to indicate this.
[01:57:41.680 --> 01:57:48.240]   But there is that and you shouldn't have the type of like right now
[01:57:48.240 --> 01:57:53.120]   our politics is run very much on knee jerk reactions to moral outrages.
[01:57:53.120 --> 01:57:53.600]   Right.
[01:57:53.600 --> 01:57:57.920]   So you get highly that's not the way to govern, but you can't.
[01:57:57.920 --> 01:58:03.120]   I don't know if you can be an effective president, especially because it's such.
[01:58:03.120 --> 01:58:07.440]   Bloom talks about the child in the well story.
[01:58:07.440 --> 01:58:07.440]   Yes.
[01:58:07.440 --> 01:58:08.240]   Remember that?
[01:58:08.240 --> 01:58:08.960]   Baby Jessica.
[01:58:08.960 --> 01:58:14.240]   Baby Jessica in Texas, of course, where else if she fell into a well and the whole nation focused on
[01:58:14.240 --> 01:58:18.800]   that and because it's easier for us to care about one person and to help one person,
[01:58:18.800 --> 01:58:21.440]   than to sympathize with a large group.
[01:58:21.440 --> 01:58:26.480]   But you're not going to get elected if you don't know how to connect with people.
[01:58:27.360 --> 01:58:33.440]   It's I mean, Donald Trump has zero policy efforts, it seems.
[01:58:33.440 --> 01:58:37.760]   But he connects with people, certain people very well.
[01:58:37.760 --> 01:58:38.480]   He's empathetic.
[01:58:38.480 --> 01:58:40.320]   He's not empathetic.
[01:58:40.320 --> 01:58:40.720]   He just.
[01:58:40.720 --> 01:58:42.960]   So he's not.
[01:58:42.960 --> 01:58:43.520]   I don't know.
[01:58:43.520 --> 01:58:44.560]   I don't know.
[01:58:44.560 --> 01:58:45.200]   How do you know?
[01:58:45.200 --> 01:58:48.240]   People who meet with him report that he's quite a nice guy.
[01:58:48.240 --> 01:58:50.160]   I'm not saying he's mean.
[01:58:50.160 --> 01:58:53.040]   I just he's not empathetic.
[01:58:54.480 --> 01:58:58.960]   He doesn't seek to feel other people's pain or do we know that?
[01:58:58.960 --> 01:58:59.680]   I don't understand.
[01:58:59.680 --> 01:59:02.800]   You know, I mean, Bill Clinton always said, I feel your pain.
[01:59:02.800 --> 01:59:03.360]   I feel your pain.
[01:59:03.360 --> 01:59:04.640]   He was the king of empathy, right?
[01:59:04.640 --> 01:59:05.440]   I feel lucky.
[01:59:05.440 --> 01:59:06.800]   Looky in the eyes.
[01:59:06.800 --> 01:59:10.480]   He'd feel your pain and then he'd go around and screw you.
[01:59:10.480 --> 01:59:12.320]   So I don't know.
[01:59:12.320 --> 01:59:14.320]   I'm not convinced that I don't know.
[01:59:14.320 --> 01:59:16.240]   Maybe I'm cynical.
[01:59:16.240 --> 01:59:20.240]   Our show today brought to you by the best audio bookstore in the world.
[01:59:20.240 --> 01:59:21.840]   Audible.
[01:59:23.120 --> 01:59:24.640]   I'm such a huge fan.
[01:59:24.640 --> 01:59:28.720]   As you know, I became an Audible customer 16 years ago now.
[01:59:28.720 --> 01:59:31.600]   And I have hundreds of books in my Audible library.
[01:59:31.600 --> 01:59:35.600]   That's one of the things I like about Audible is that unlike the, you know,
[01:59:35.600 --> 01:59:41.520]   my previous books on tape, I get to keep these books and I can download them and listen to them
[01:59:41.520 --> 01:59:42.400]   again at any time.
[01:59:42.400 --> 01:59:46.640]   Oh, look, Neil Gaiman has a new book on Norse mythology.
[01:59:46.640 --> 01:59:49.920]   Oh, that would be great.
[01:59:49.920 --> 01:59:54.960]   Of course, he wrote American Gods, which is kind of based on Norse mythology.
[01:59:54.960 --> 01:59:56.720]   This is not out yet.
[01:59:56.720 --> 01:59:57.760]   It won't be until February.
[01:59:57.760 --> 01:59:58.800]   But so you can get this.
[01:59:58.800 --> 02:00:05.680]   What I like about Audible is that they get all the best sellers when they come out
[02:00:05.680 --> 02:00:08.640]   day in date as the hardcover.
[02:00:08.640 --> 02:00:10.000]   So you can listen to anything.
[02:00:10.000 --> 02:00:11.840]   They got Charlie's stress.
[02:00:11.840 --> 02:00:14.160]   Yeah, they got a ton of Charlie.
[02:00:14.160 --> 02:00:15.360]   So what do you like about his stuff?
[02:00:17.840 --> 02:00:22.640]   He writes, well, he writes, it's good science fiction stuff, but he's also,
[02:00:22.640 --> 02:00:23.440]   he's a techy.
[02:00:23.440 --> 02:00:24.800]   So he writes the tech right.
[02:00:24.800 --> 02:00:26.240]   I look for that.
[02:00:26.240 --> 02:00:30.480]   He does a lot of interesting, he's got multiple different parallel series.
[02:00:30.480 --> 02:00:34.400]   I mean, he, but the Empire Games is the new one in the merchant,
[02:00:34.400 --> 02:00:36.240]   the merchant series.
[02:00:36.240 --> 02:00:38.320]   So they have up to book six, the trade of queens.
[02:00:38.320 --> 02:00:39.760]   So I guess book seven is due.
[02:00:39.760 --> 02:00:46.560]   Yeah, well, he just reissued them as three large books, rather than six more books.
[02:00:47.200 --> 02:00:52.880]   Um, the previous series, and then this is the first, um, the second trilogy,
[02:00:52.880 --> 02:00:55.600]   which is so twice as long as those and it's it.
[02:00:55.600 --> 02:00:57.280]   It's a six ology.
[02:00:57.280 --> 02:01:05.440]   Well, look, here's the thing, no matter what you're into, especially science fiction,
[02:01:05.440 --> 02:01:08.640]   when I first joined Audible in 2000, they didn't have a lot of science fiction,
[02:01:08.640 --> 02:01:11.520]   but they really made a concerted effort to beef up the sci-fi.
[02:01:11.520 --> 02:01:16.400]   In fact, they created an Audible frontiers imprint that
[02:01:16.400 --> 02:01:20.080]   re-records some of the classics from Heinlein and Asimov.
[02:01:20.080 --> 02:01:22.720]   So really there's, this is a great place.
[02:01:22.720 --> 02:01:28.560]   Oh, Carrie Fisher's, uh, uh, diary narrated by Carrie Fisher.
[02:01:28.560 --> 02:01:32.480]   Yeah, the princess, the diarist and Billy Lord is, uh, is available in Audible.
[02:01:32.480 --> 02:01:34.960]   I would love to listen to that.
[02:01:34.960 --> 02:01:38.560]   I've heard her do that made the, make the rounds, heard many interviews with her.
[02:01:38.560 --> 02:01:41.600]   Cut to 2013 about this.
[02:01:42.080 --> 02:01:46.960]   Much the same kinds of things were happening only faster and more intensely.
[02:01:46.960 --> 02:01:50.480]   This, this, uh, of course is the diary that she took.
[02:01:50.480 --> 02:01:57.840]   She kept while she was, uh, making the original Star Wars movie, uh, in 1977 and reveals,
[02:01:57.840 --> 02:02:03.120]   well, I don't want to know spoilers, but reveals us as kind of a shocking secret.
[02:02:03.120 --> 02:02:05.200]   Okay. It was so not shocking.
[02:02:05.200 --> 02:02:08.400]   Come on now.
[02:02:08.400 --> 02:02:09.920]   You knew really you could tell.
[02:02:10.960 --> 02:02:12.560]   Oh, okay.
[02:02:12.560 --> 02:02:13.840]   I didn't know.
[02:02:13.840 --> 02:02:14.320]   I didn't know.
[02:02:14.320 --> 02:02:15.440]   I was shocked.
[02:02:15.440 --> 02:02:16.000]   I didn't know.
[02:02:16.000 --> 02:02:19.280]   Uh, Michael Lewis is newest.
[02:02:19.280 --> 02:02:23.440]   I read every, if you haven't read a Michael Lewis book, he wrote the big short.
[02:02:23.440 --> 02:02:26.000]   He wrote, uh, Moneyball.
[02:02:26.000 --> 02:02:30.000]   He is one of our best, most insightful writers of nonfiction.
[02:02:30.000 --> 02:02:34.560]   His latest, the undoing project is about Daniel Kahneman and Amos Dversky.
[02:02:34.560 --> 02:02:40.400]   Uh, and their articles and studies on how humans make decisions.
[02:02:41.040 --> 02:02:46.480]   Uh, actually Kahneman's book is one of my favorites on Audible.
[02:02:46.480 --> 02:02:47.840]   Let me see if I can find that.
[02:02:47.840 --> 02:02:49.200]   That's another one I would recommend.
[02:02:49.200 --> 02:02:53.200]   Uh, Daniel Kahneman wrote, uh, thinking fast and slow.
[02:02:53.200 --> 02:03:00.160]   Um, and it's, it's about how we, we were just talking about the elephant and the guy
[02:03:00.160 --> 02:03:01.520]   riding the elephant and all that stuff.
[02:03:01.520 --> 02:03:06.960]   Um, he says we have two systems that drive the way we think system one is fast,
[02:03:07.520 --> 02:03:12.320]   it's intuitive, it's emotional system two is slower, more deliberate, more logical.
[02:03:12.320 --> 02:03:15.680]   And he, and he's really done the studies on how we think.
[02:03:15.680 --> 02:03:19.200]   I'll tell you what, Audible is such a great resource.
[02:03:19.200 --> 02:03:20.320]   Let me get you a book free.
[02:03:20.320 --> 02:03:20.880]   How about that?
[02:03:20.880 --> 02:03:23.520]   Here's the new one, an Audible original.
[02:03:23.520 --> 02:03:24.960]   Ponzi supernova.
[02:03:24.960 --> 02:03:26.560]   Bernie Madoff.
[02:03:26.560 --> 02:03:29.840]   On the world's largest con.
[02:03:29.840 --> 02:03:30.880]   Hmm.
[02:03:30.880 --> 02:03:36.000]   This is called, uh, this is, uh, uh, interesting.
[02:03:36.000 --> 02:03:38.480]   I didn't know they, this is something maybe Audible's doing now.
[02:03:38.480 --> 02:03:39.920]   A little different.
[02:03:39.920 --> 02:03:41.040]   I, they're doing originals.
[02:03:41.040 --> 02:03:45.920]   So here's the deal.
[02:03:45.920 --> 02:03:48.720]   Go to audible.com/twig.
[02:03:48.720 --> 02:03:52.320]   That'll bring you to the gold sign up.
[02:03:52.320 --> 02:03:56.320]   That's the book a month's account, which also get the daily digest of the Wall Street Journal
[02:03:56.320 --> 02:03:58.000]   and the New York Times.
[02:03:58.000 --> 02:04:03.680]   You pick any book, you get a credit toward a book, keep the book no matter what happens.
[02:04:03.680 --> 02:04:06.640]   But if you're not happy, you can cancel anytime in the first 30 days and pay zip.
[02:04:06.640 --> 02:04:09.280]   But I don't think you'll cancel.
[02:04:09.280 --> 02:04:12.400]   It's been, for me, one of the great pleasures of my life.
[02:04:12.400 --> 02:04:14.400]   I listen all the time at work.
[02:04:14.400 --> 02:04:16.480]   If I had a commute, I would listen.
[02:04:16.480 --> 02:04:17.280]   That's how I started.
[02:04:17.280 --> 02:04:27.120]   I listen when I'm on the treadmill, when I'm washing dishes, Audible.com/twig for your first book.
[02:04:27.120 --> 02:04:28.400]   Free.
[02:04:28.400 --> 02:04:30.640]   And we thank Audible for their support.
[02:04:30.640 --> 02:04:31.920]   Here's another Audible original.
[02:04:31.920 --> 02:04:36.480]   They must be doing a bunch more genius dialogues with MacArthur Fellows.
[02:04:36.480 --> 02:04:38.080]   Wow.
[02:04:38.080 --> 02:04:41.280]   Amy Smith of MIT's D-Lab.
[02:04:41.280 --> 02:04:43.520]   Oh, this sounds great.
[02:04:43.520 --> 02:04:46.720]   They're doing a bunch of original stuff now.
[02:04:46.720 --> 02:04:47.360]   This is new.
[02:04:47.360 --> 02:04:51.840]   All right.
[02:04:51.840 --> 02:04:52.400]   Moving on.
[02:04:52.400 --> 02:04:57.920]   Google says it's verified apps feature.
[02:04:57.920 --> 02:05:00.400]   You know, where it scans those apps has identified.
[02:05:00.400 --> 02:05:00.960]   Get this.
[02:05:00.960 --> 02:05:08.720]   25,000 potentially dangerous apps and kept them out of the play store.
[02:05:08.720 --> 02:05:10.880]   Wow.
[02:05:10.880 --> 02:05:12.480]   Thanks, Google.
[02:05:12.480 --> 02:05:13.360]   Thanks, Google.
[02:05:13.360 --> 02:05:16.560]   Thank you, Google.
[02:05:16.560 --> 02:05:18.480]   And that's actually on your Android device.
[02:05:18.480 --> 02:05:21.200]   Make sure it's turned on Google, I warn you.
[02:05:21.200 --> 02:05:22.560]   And here I love this.
[02:05:22.560 --> 02:05:25.040]   This is Google for you.
[02:05:25.040 --> 02:05:26.080]   This is the formula.
[02:05:29.120 --> 02:05:30.240]   The Z score.
[02:05:30.240 --> 02:05:32.400]   So that way you can have a Python script do this.
[02:05:32.400 --> 02:05:35.760]   And look at the--
[02:05:35.760 --> 02:05:37.680]   And is the number of devices downloaded the app.
[02:05:37.680 --> 02:05:39.920]   X is the number of retained devices that downloaded the app.
[02:05:39.920 --> 02:05:43.200]   And P is a probability of device downloading any app that will be retained.
[02:05:43.200 --> 02:05:44.320]   And there you go.
[02:05:44.320 --> 02:05:47.120]   If an app has an extremely poor Z score,
[02:05:47.120 --> 02:05:51.680]   Google immediately classifies it as a PHA.
[02:05:51.680 --> 02:05:55.840]   And then we'll kill installs.
[02:05:55.840 --> 02:05:56.480]   So there.
[02:05:56.480 --> 02:05:57.520]   What is a PHA?
[02:05:57.520 --> 02:05:58.880]   I don't know.
[02:05:58.880 --> 02:06:00.160]   But I'm sure it's a bad thing.
[02:06:00.160 --> 02:06:00.560]   OK.
[02:06:00.560 --> 02:06:02.720]   Potentially harmful app.
[02:06:02.720 --> 02:06:03.840]   Potentially harmful app.
[02:06:03.840 --> 02:06:06.080]   Seriously, they did that?
[02:06:06.080 --> 02:06:07.360]   Of course they did.
[02:06:07.360 --> 02:06:08.320]   Oh, gosh.
[02:06:08.320 --> 02:06:10.160]   Data driven, baby.
[02:06:10.160 --> 02:06:11.280]   It's data driven.
[02:06:11.280 --> 02:06:13.200]   Maybe they will work out.
[02:06:13.200 --> 02:06:15.200]   The technocrats will work out in government.
[02:06:15.200 --> 02:06:15.680]   It's--
[02:06:15.680 --> 02:06:16.160]   This is--
[02:06:16.160 --> 02:06:17.440]   But I mean, that's the point.
[02:06:17.440 --> 02:06:24.080]   Is that the mindset that says, well, it's all can be solved with a formula.
[02:06:24.080 --> 02:06:27.600]   Is exactly the mindset that says, get out of the way, let us take over.
[02:06:27.600 --> 02:06:28.640]   We know how to do these.
[02:06:28.640 --> 02:06:30.000]   We know how to do the math.
[02:06:30.000 --> 02:06:31.520]   I don't--
[02:06:31.520 --> 02:06:31.920]   Yes.
[02:06:31.920 --> 02:06:33.520]   I totally understand.
[02:06:33.520 --> 02:06:35.440]   For a while, I used to be that way.
[02:06:35.440 --> 02:06:36.320]   But now I feel--
[02:06:36.320 --> 02:06:36.880]   Oh, there you go.
[02:06:36.880 --> 02:06:38.480]   You become empathetic.
[02:06:38.480 --> 02:06:39.760]   I know I just--
[02:06:39.760 --> 02:06:40.400]   I'm a fan.
[02:06:40.400 --> 02:06:43.280]   I dealt with the real world.
[02:06:43.280 --> 02:06:44.400]   Yeah.
[02:06:44.400 --> 02:06:45.520]   No.
[02:06:45.520 --> 02:06:50.560]   The thing is, the scientific method is a powerful thing.
[02:06:50.560 --> 02:06:52.640]   I love the scientific method.
[02:06:52.640 --> 02:06:54.400]   You will never convince me that scientific method
[02:06:54.400 --> 02:06:57.520]   is not the most important thing humans have come up with.
[02:06:58.160 --> 02:06:58.880]   Ever.
[02:06:58.880 --> 02:06:59.920]   Right.
[02:06:59.920 --> 02:07:01.920]   So there is that.
[02:07:01.920 --> 02:07:05.040]   But it does rely on you actually modeling what you think you're modeling.
[02:07:05.040 --> 02:07:05.360]   Yeah.
[02:07:05.360 --> 02:07:06.400]   And if you--
[02:07:06.400 --> 02:07:08.000]   Well, there's confirmation--
[02:07:08.000 --> 02:07:10.960]   There's a lot of human failings like confirmation bias and so forth.
[02:07:10.960 --> 02:07:12.240]   Yeah.
[02:07:12.240 --> 02:07:13.280]   And there's also a lot of--
[02:07:13.280 --> 02:07:19.680]   You know, there's an entire field of scientific papers that are now being questioned
[02:07:19.680 --> 02:07:21.840]   because they've basically spent too long
[02:07:21.840 --> 02:07:25.840]   gaming the statistics rather than actually doing productive judicial experiments.
[02:07:25.840 --> 02:07:29.440]   So the challenge is you've got to use the scientific method,
[02:07:29.440 --> 02:07:34.480]   but you've also got to make sure that you haven't got models with gaps in it,
[02:07:34.480 --> 02:07:35.440]   which is the--
[02:07:35.440 --> 02:07:38.480]   The--
[02:07:38.480 --> 02:07:41.360]   And the tech industry has incredible gaps.
[02:07:41.360 --> 02:07:42.400]   Yes.
[02:07:42.400 --> 02:07:43.360]   Incredible.
[02:07:43.360 --> 02:07:43.920]   Oh, it's all--
[02:07:43.920 --> 02:07:46.320]   And has a blind spot as to where its gaps are.
[02:07:46.320 --> 02:07:47.280]   Yes.
[02:07:47.280 --> 02:07:48.000]   Yes.
[02:07:48.000 --> 02:07:49.840]   I was going to point you to--
[02:07:49.840 --> 02:07:51.120]   Let me see if I can find the link.
[02:07:51.120 --> 02:07:52.640]   There was this great article.
[02:07:52.640 --> 02:07:54.800]   And it was on.
[02:07:55.680 --> 02:07:56.880]   It was about--
[02:07:56.880 --> 02:08:05.200]   It was the 69th African American woman to graduate with a PhD in physics
[02:08:05.200 --> 02:08:07.040]   in the United States ever.
[02:08:07.040 --> 02:08:12.800]   I think it was Gizmodo interviewed her about hidden figures, the movie.
[02:08:12.800 --> 02:08:13.520]   Oh, yeah.
[02:08:13.520 --> 02:08:19.920]   And the story, which I'm desperately searching for the link because it was a great interview.
[02:08:19.920 --> 02:08:20.160]   From--
[02:08:20.160 --> 02:08:20.640]   Did you find it?
[02:08:20.640 --> 02:08:21.520]   From Huffpo.
[02:08:21.520 --> 02:08:22.800]   This is--
[02:08:22.800 --> 02:08:23.520]   Oh, is it Huffpo?
[02:08:23.520 --> 02:08:24.240]   A year ago.
[02:08:24.720 --> 02:08:26.800]   Dr. Tonda Prescott Weinstein.
[02:08:26.800 --> 02:08:27.760]   So that's her.
[02:08:27.760 --> 02:08:30.880]   The 63rd Black woman in American history with a physics PhD.
[02:08:30.880 --> 02:08:34.320]   That's kind of hard to believe that in all this time,
[02:08:34.320 --> 02:08:35.360]   there've only been 63.
[02:08:35.360 --> 02:08:40.560]   But remember, women weren't allowed to go to school to college 100 years ago.
[02:08:40.560 --> 02:08:40.800]   So--
[02:08:40.800 --> 02:08:42.480]   You know.
[02:08:42.480 --> 02:08:43.520]   Well, yes.
[02:08:43.520 --> 02:08:47.440]   So this is-- I think it's a more recent article because it's tied to the release of the movie.
[02:08:47.440 --> 02:08:48.480]   I wish I could find it.
[02:08:48.480 --> 02:08:49.120]   I'll find it.
[02:08:49.120 --> 02:08:50.880]   I have the means.
[02:08:50.880 --> 02:08:51.440]   What you do--
[02:08:51.440 --> 02:08:52.960]   So let me show you a little tip here.
[02:08:52.960 --> 02:08:56.000]   You go to tools in your Google search and you say,
[02:08:56.000 --> 02:08:59.760]   let's find the results to this search from the last month or week.
[02:08:59.760 --> 02:09:00.480]   Which should we do?
[02:09:00.480 --> 02:09:01.040]   Month?
[02:09:01.040 --> 02:09:01.600]   Look at that.
[02:09:01.600 --> 02:09:02.560]   Carson already did it.
[02:09:02.560 --> 02:09:04.480]   Oh, Carson.
[02:09:04.480 --> 02:09:06.880]   This is from Gizmodo, Australia.
[02:09:06.880 --> 02:09:12.560]   A black female astrophysicist explains why hidden figures isn't just about history.
[02:09:12.560 --> 02:09:14.240]   That's definitely the way you get on it.
[02:09:14.240 --> 02:09:16.080]   Read that article.
[02:09:16.080 --> 02:09:22.880]   It made me cry because I have been in places where I am one of the only women
[02:09:22.880 --> 02:09:23.520]   in the room.
[02:09:23.520 --> 02:09:23.840]   Yeah.
[02:09:23.840 --> 02:09:31.200]   I can't imagine being one of only women and black being a black woman.
[02:09:31.200 --> 02:09:31.760]   You imagine.
[02:09:31.760 --> 02:09:35.920]   In the things people said to her that wasn't even a jerky--
[02:09:35.920 --> 02:09:39.600]   it may not have even been meant as a jerk thing.
[02:09:39.600 --> 02:09:42.160]   Like the assumption that she couldn't handle the coursework.
[02:09:42.160 --> 02:09:44.640]   Actually, I think that is kind of a jerky thing,
[02:09:44.640 --> 02:09:46.400]   but I know at the time that people--
[02:09:46.400 --> 02:09:49.680]   Let's not forget Larry Summers.
[02:09:49.680 --> 02:09:51.520]   Yeah.
[02:09:51.520 --> 02:09:55.280]   Who was president of Harvard when he said, "Well, women just can't do physics."
[02:09:55.280 --> 02:09:57.440]   A couple of years ago.
[02:09:57.440 --> 02:10:04.000]   I mean, my mom was a geophysicist because she was convinced she couldn't be a physicist.
[02:10:04.000 --> 02:10:08.560]   And she was one of seven women in her program at like Rinsselaer Polytech back in the 70s.
[02:10:08.560 --> 02:10:13.200]   For her, I like it that your mom--
[02:10:13.200 --> 02:10:17.040]   See, I didn't realize you came from a good--
[02:10:17.040 --> 02:10:19.360]   Oh, they are so disappointed that I'm a writer.
[02:10:19.360 --> 02:10:22.320]   They're like, "My dad's a double E. My mom's a geophysicist."
[02:10:22.320 --> 02:10:24.080]   That's where all this brains come from.
[02:10:24.080 --> 02:10:26.320]   Now I get it.
[02:10:26.320 --> 02:10:35.920]   Larry Summers implied there was an innate difference between men and women that made women bad at science.
[02:10:35.920 --> 02:10:39.040]   You jerk.
[02:10:39.040 --> 02:10:44.160]   By the way, he then went on to a prominent role in--
[02:10:45.840 --> 02:10:50.080]   Who was it? I think it was a Obama administration.
[02:10:50.080 --> 02:10:53.200]   Please.
[02:10:53.200 --> 02:10:54.960]   He got sanitized.
[02:10:54.960 --> 02:11:01.600]   So read that article because it's a great example of how tech and science has these gaps in putting them in charge.
[02:11:01.600 --> 02:11:02.880]   Do you want to get really depressed?
[02:11:02.880 --> 02:11:07.600]   I just finished the chapter in the People's History of the United States by Howard Zinn
[02:11:07.600 --> 02:11:12.000]   on women's role in American history. It's so depressing.
[02:11:13.520 --> 02:11:14.960]   It's like, "Oh my God."
[02:11:14.960 --> 02:11:20.320]   On behalf of men everywhere, I apologize.
[02:11:20.320 --> 02:11:26.400]   And it's just like, "Well, we can do better. Let's do better. Why don't we do better?"
[02:11:26.400 --> 02:11:30.240]   And while we're doing better, let's get your pick of the week!
[02:11:30.240 --> 02:11:33.200]   Ladies and gentlemen, it's the time of the show towards the end here,
[02:11:33.200 --> 02:11:35.200]   where we get to do the picks of the week.
[02:11:35.200 --> 02:11:36.880]   Normally, Jeff would do a number, but he's gone.
[02:11:36.880 --> 02:11:42.320]   He's gone, so I'll search for a number while Stacy does her thing.
[02:11:43.200 --> 02:11:44.160]   I like your thing.
[02:11:44.160 --> 02:11:48.960]   My thing is a thing because, "Oh, did I freeze?"
[02:11:48.960 --> 02:11:52.240]   No. No. Me might have frozen, but yours still moving.
[02:11:52.240 --> 02:11:56.240]   I was like, "My Skype just went someplace else."
[02:11:56.240 --> 02:11:57.760]   It knew you were going to do this thing.
[02:11:57.760 --> 02:12:01.760]   All right. So my app of the week is Stringify.
[02:12:01.760 --> 02:12:08.560]   And this is a way to connect. It's like, if this than that, but instead of just doing one
[02:12:08.560 --> 02:12:12.560]   end statement, you can do many end statements.
[02:12:12.560 --> 02:12:13.600]   Oh, I've been wanting this.
[02:12:13.600 --> 02:12:20.400]   Yes. So the iOS app is much better. I've got to tell you this right now.
[02:12:20.400 --> 02:12:23.360]   The Android app is out and bait it. It just came out like a week and a half ago,
[02:12:23.360 --> 02:12:25.440]   maybe two weeks ago.
[02:12:25.440 --> 02:12:29.680]   And so if you have the option, iOS is the way to go.
[02:12:29.680 --> 02:12:32.480]   So what you do is you manually add all your devices.
[02:12:32.480 --> 02:12:36.320]   They support an astonishing number of things.
[02:12:36.880 --> 02:12:42.240]   So Hue lights, Arlo cameras, NetAppmo, Nest, the Wink, TechCrunch.
[02:12:42.240 --> 02:12:44.880]   New York Times.
[02:12:44.880 --> 02:12:45.840]   Automatix.
[02:12:45.840 --> 02:12:46.640]   Our sponsor.
[02:12:46.640 --> 02:12:51.040]   Oh, yeah. So lots of physical and digital things.
[02:12:51.040 --> 02:12:54.240]   Yay. The Echo. I should put that in there because people love talking to that.
[02:12:54.240 --> 02:12:54.800]   Yes. Got it.
[02:12:54.800 --> 02:13:00.960]   So it lets you build these flows in your best bet, actually, if you can.
[02:13:00.960 --> 02:13:03.520]   By the way, it even supports if this than that.
[02:13:04.240 --> 02:13:07.200]   Yes, it also supports this than that, which is really cool.
[02:13:07.200 --> 02:13:08.960]   You can change these things.
[02:13:08.960 --> 02:13:10.880]   That's exactly what you're going to do.
[02:13:10.880 --> 02:13:13.040]   So if you go to the site, it'll show you a flow.
[02:13:13.040 --> 02:13:15.600]   I don't want to show you my flows because they're kind of messy.
[02:13:15.600 --> 02:13:20.400]   I will say it is tough to navigate the first time around.
[02:13:20.400 --> 02:13:22.160]   This is programming at this point.
[02:13:22.160 --> 02:13:28.320]   It is. So click on a flow. Pick one that you like 30 minute timer.
[02:13:28.320 --> 02:13:32.560]   That's the most popular current, popular flow.
[02:13:33.120 --> 02:13:34.160]   So you see what you do.
[02:13:34.160 --> 02:13:35.120]   Here's a better one.
[02:13:35.120 --> 02:13:36.880]   Party time light cascade.
[02:13:36.880 --> 02:13:37.680]   Oh, do that one.
[02:13:37.680 --> 02:13:38.400]   Yeah.
[02:13:38.400 --> 02:13:42.080]   Because this requires Instion and Hue lights and Alexa.
[02:13:42.080 --> 02:13:42.720]   I mean, Echo.
[02:13:42.720 --> 02:13:48.480]   So what you do is you ask Amazon Echo to tell Stringify its party time.
[02:13:48.480 --> 02:13:52.320]   It turns on your Instion keyboard, starts your timer for seven seconds,
[02:13:52.320 --> 02:13:54.960]   then turns on Hue bulb A19 to purple.
[02:13:54.960 --> 02:13:56.960]   Yes.
[02:13:56.960 --> 02:13:59.200]   That's something you can't do with if this than that.
[02:13:59.200 --> 02:14:01.520]   Right. And some of them are even better.
[02:14:01.520 --> 02:14:04.400]   Like you can have multiple triggers.
[02:14:04.400 --> 02:14:06.480]   So if this is happening and this is happening.
[02:14:06.480 --> 02:14:07.840]   Here's an example.
[02:14:07.840 --> 02:14:09.600]   Your pivot power genius.
[02:14:09.600 --> 02:14:10.400]   There you go.
[02:14:10.400 --> 02:14:17.520]   When the Nest Camtex motion and when you're away, it sends an email.
[02:14:17.520 --> 02:14:22.800]   Then it turns on your Wink pivot power genius and sends you an email.
[02:14:22.800 --> 02:14:26.960]   So you've got and statements in here, then sets a timer for 10 minutes,
[02:14:26.960 --> 02:14:29.840]   then turns off your pivot power pivot genius.
[02:14:29.840 --> 02:14:30.800]   Huh.
[02:14:31.600 --> 02:14:35.200]   So it's just a power strip.
[02:14:35.200 --> 02:14:36.160]   Okay.
[02:14:36.160 --> 02:14:36.720]   Yeah.
[02:14:36.720 --> 02:14:38.080]   It's a smart power strip.
[02:14:38.080 --> 02:14:38.400]   Yeah.
[02:14:38.400 --> 02:14:41.360]   So yeah, you can see how this could be a really powerful tool.
[02:14:41.360 --> 02:14:41.360]   Cool.
[02:14:41.360 --> 02:14:42.080]   Yeah.
[02:14:42.080 --> 02:14:43.520]   It is tough.
[02:14:43.520 --> 02:14:47.520]   Like my recommendation, if you download this, it's not tough.
[02:14:47.520 --> 02:14:48.640]   I shouldn't say that.
[02:14:48.640 --> 02:14:51.200]   Watch the video as to how to do it.
[02:14:51.200 --> 02:14:53.360]   Because otherwise, if you should like start playing around,
[02:14:53.360 --> 02:14:54.080]   you're kind of like,
[02:14:54.080 --> 02:14:57.280]   If you know Boolean logic, you should be able to figure this out though, right?
[02:14:57.280 --> 02:15:02.160]   The logic is how they have things wrapped on their stuck on their canvas.
[02:15:02.160 --> 02:15:03.600]   It's purely a mechanical thing.
[02:15:03.600 --> 02:15:04.560]   It's not a logical thing.
[02:15:04.560 --> 02:15:05.040]   Oh, I got it.
[02:15:05.040 --> 02:15:05.280]   Okay.
[02:15:05.280 --> 02:15:08.000]   I like it that you could do Boolean logic.
[02:15:08.000 --> 02:15:12.240]   So there's one that it will turn on your spotlights when your Nest Camtex motion,
[02:15:12.240 --> 02:15:13.360]   but only if it's night.
[02:15:13.360 --> 02:15:14.480]   Yeah.
[02:15:14.480 --> 02:15:15.680]   Nice.
[02:15:15.680 --> 02:15:16.400]   Stuff like that.
[02:15:16.400 --> 02:15:18.720]   So have fun, you guys.
[02:15:18.720 --> 02:15:19.520]   Stringify.
[02:15:19.520 --> 02:15:20.240]   Is it free?
[02:15:20.240 --> 02:15:21.520]   It is free.
[02:15:21.520 --> 02:15:25.040]   They pay for this based on hold on.
[02:15:25.920 --> 02:15:29.280]   Oh, enterprises pay for this.
[02:15:29.280 --> 02:15:29.840]   Oh, okay.
[02:15:29.840 --> 02:15:30.880]   Somebody's got to pay for it.
[02:15:30.880 --> 02:15:31.360]   Okay.
[02:15:31.360 --> 02:15:32.480]   Well, if someone's paying for it,
[02:15:32.480 --> 02:15:36.640]   I would have thought they might get referral fees if you buy new gadgets as well.
[02:15:36.640 --> 02:15:37.200]   Yeah, they should.
[02:15:37.200 --> 02:15:37.280]   They should.
[02:15:37.280 --> 02:15:37.760]   They should.
[02:15:37.760 --> 02:15:38.240]   They should.
[02:15:38.240 --> 02:15:38.640]   Definitely.
[02:15:38.640 --> 02:15:40.800]   No, I didn't ask them about that, but no.
[02:15:40.800 --> 02:15:41.200]   Nice.
[02:15:41.200 --> 02:15:41.680]   Very good.
[02:15:41.680 --> 02:15:42.000]   Fine.
[02:15:42.000 --> 02:15:42.880]   Thank you, Stacy.
[02:15:42.880 --> 02:15:44.160]   Hello.
[02:15:44.160 --> 02:15:46.640]   Kevin, you've got a few things you want to give a plug to.
[02:15:46.640 --> 02:15:50.400]   So, like these are my indie web things.
[02:15:50.400 --> 02:15:54.320]   The usual thing is if you go to indieweb.org,
[02:15:54.320 --> 02:15:56.960]   we've got our regular Homebrew website club next week
[02:15:56.960 --> 02:16:02.080]   in Modell City, San Francisco, London, Brighton, and so others.
[02:16:02.080 --> 02:16:03.600]   Then as well, I think.
[02:16:03.600 --> 02:16:10.560]   The other indie web thing is that web mention is now a W3C recommendation.
[02:16:10.560 --> 02:16:11.120]   I saw that.
[02:16:11.120 --> 02:16:11.920]   Congratulations.
[02:16:11.920 --> 02:16:12.480]   That's huge.
[02:16:12.480 --> 02:16:16.320]   That's actually been finalized and we've been working through this for a while now,
[02:16:16.320 --> 02:16:17.680]   and it's been fully published.
[02:16:17.680 --> 02:16:18.160]   Nice.
[02:16:18.720 --> 02:16:21.680]   At this point, go out and implement this.
[02:16:21.680 --> 02:16:26.240]   It's official and it's stable now.
[02:16:26.240 --> 02:16:28.800]   So, web mention, we discussed before,
[02:16:28.800 --> 02:16:33.520]   but it's basically a very simple protocol that when one website links to another,
[02:16:33.520 --> 02:16:35.360]   it can send a web mesh to say they've done that.
[02:16:35.360 --> 02:16:40.400]   And then you can then the receiving site can interpret that as a different thing,
[02:16:40.400 --> 02:16:44.000]   depending on what kind of thing it's been done.
[02:16:44.000 --> 02:16:44.480]   Mine can't.
[02:16:44.480 --> 02:16:45.440]   It's desperate to get to it.
[02:16:45.440 --> 02:16:46.000]   Wow.
[02:16:46.000 --> 02:16:46.960]   I was like, oh.
[02:16:46.960 --> 02:16:51.600]   [Laughter]
[02:16:51.600 --> 02:16:54.480]   So cats are harmed in the making of this twit.
[02:16:54.480 --> 02:16:57.200]   I said the dogs are in the house.
[02:16:57.200 --> 02:16:58.880]   Obviously, the cat has been the one.
[02:16:58.880 --> 02:17:00.320]   [Laughter]
[02:17:00.320 --> 02:17:06.720]   So you can use it to do likes and reposts and RSVPs and threaded conversations
[02:17:06.720 --> 02:17:07.920]   and other things like that.
[02:17:07.920 --> 02:17:13.280]   So that's out there and implementations are growing all the time.
[02:17:13.920 --> 02:17:17.440]   And then the other thing I wanted to mention was micro.blog,
[02:17:17.440 --> 02:17:24.160]   which is a planning to be a new microblogging tool and reader.
[02:17:24.160 --> 02:17:24.720]   Oh, good.
[02:17:24.720 --> 02:17:25.680]   We need one of those.
[02:17:25.680 --> 02:17:28.960]   And this is Manton Reese's doing this.
[02:17:28.960 --> 02:17:30.320]   Yes, Manton Reese's doing this.
[02:17:30.320 --> 02:17:30.880]   And there's a kickstarter for it.
[02:17:30.880 --> 02:17:31.920]   First of all, my screen.
[02:17:31.920 --> 02:17:33.280]   There you go.
[02:17:33.280 --> 02:17:34.800]   Yes.
[02:17:34.800 --> 02:17:37.200]   So what's it going to look like?
[02:17:37.200 --> 02:17:38.960]   You hosted?
[02:17:40.240 --> 02:17:44.960]   He's going to host stuff himself or let you redirect your domain to it.
[02:17:44.960 --> 02:17:47.680]   But yes, he says about it.
[02:17:47.680 --> 02:17:48.160]   Yeah.
[02:17:48.160 --> 02:17:49.280]   We'll turn on his voice.
[02:17:49.280 --> 02:17:51.040]   I turned off his voice.
[02:17:51.040 --> 02:17:51.840]   Go ahead, Manton.
[02:17:51.840 --> 02:17:57.280]   Oh, I'm building a new social network for independent micro blogs.
[02:17:57.280 --> 02:17:58.960]   It's called micro.blog.
[02:17:58.960 --> 02:18:00.080]   They'll be federated by pursuit.
[02:18:00.080 --> 02:18:03.280]   So we'll add free blogging platform for short form content.
[02:18:03.280 --> 02:18:04.560]   I like it.
[02:18:04.560 --> 02:18:05.840]   This is what we need.
[02:18:06.880 --> 02:18:10.560]   So how is this different from say, GNU Social?
[02:18:10.560 --> 02:18:16.400]   He's trying to make it so that it will integrate things.
[02:18:16.400 --> 02:18:18.240]   So in some ways, it's more like friend feed was,
[02:18:18.240 --> 02:18:19.440]   or bring stuff in from outside.
[02:18:19.440 --> 02:18:20.800]   It's also a friend feeder.
[02:18:20.800 --> 02:18:22.000]   And then you can reply there.
[02:18:22.000 --> 02:18:22.400]   I understand.
[02:18:22.400 --> 02:18:28.160]   He said he was going to adopt the web mention and micro pub stuff
[02:18:28.160 --> 02:18:29.680]   so that he can plug into the internet.
[02:18:29.680 --> 02:18:31.440]   Manton's big into indie web, right?
[02:18:31.440 --> 02:18:35.200]   So he can expect to be fully indie web compliant.
[02:18:35.200 --> 02:18:36.480]   So that looks promising.
[02:18:36.480 --> 02:18:38.480]   He raised $45,000.
[02:18:38.480 --> 02:18:39.680]   His goal was 10.
[02:18:39.680 --> 02:18:41.440]   And there's still two weeks.
[02:18:41.440 --> 02:18:47.600]   So everybody go to Kickstarter and search for micro.blog.
[02:18:47.600 --> 02:18:48.080]   Probably.
[02:18:48.080 --> 02:18:50.480]   Yeah, we'll go to micro.blog and there's a link there.
[02:18:50.480 --> 02:18:51.360]   Nice.
[02:18:51.360 --> 02:18:52.480]   That's a domain.
[02:18:52.480 --> 02:18:54.800]   People have a domain, obviously, from.
[02:18:54.800 --> 02:18:55.760]   And he's in Austin.
[02:18:55.760 --> 02:18:57.520]   So if you want to go over and say hi, Stacey.
[02:18:57.520 --> 02:18:59.280]   Oh, what's up?
[02:18:59.280 --> 02:18:59.920]   What's up?
[02:18:59.920 --> 02:19:02.400]   I'm like an indie media publication,
[02:19:02.400 --> 02:19:05.120]   although I'm not on actual indie web platforms.
[02:19:05.120 --> 02:19:07.120]   I'm going to give him some money.
[02:19:07.120 --> 02:19:10.720]   You can make your platforms you're on indie more indie.
[02:19:10.720 --> 02:19:15.920]   We've seen so many attempts to do an open federated replacement
[02:19:15.920 --> 02:19:19.040]   for these siloed solutions like Twitter.
[02:19:19.040 --> 02:19:20.960]   And they never succeed.
[02:19:20.960 --> 02:19:22.400]   In fact, app.net just.
[02:19:22.400 --> 02:19:23.600]   Ad don't it shut down this week.
[02:19:23.600 --> 02:19:24.240]   Hold the plug.
[02:19:24.240 --> 02:19:27.440]   What is it going to take to make one of these work?
[02:19:27.440 --> 02:19:31.840]   Well, the thing is that my sense of it is they have to go with the web.
[02:19:31.840 --> 02:19:34.640]   They have to be part of the web and behave in that way.
[02:19:34.640 --> 02:19:38.320]   And a lot of them, they said, OK, we're going to make our own Twitter
[02:19:38.320 --> 02:19:40.080]   and it'll be our own special thing.
[02:19:40.080 --> 02:19:42.960]   That was a problem with the aspirants, a problem with app.net, to some extent,
[02:19:42.960 --> 02:19:47.280]   in that they're trying to replicate the whole thing in one place.
[02:19:47.280 --> 02:19:50.320]   Whereas what we've been trying to do with indie web
[02:19:50.320 --> 02:19:53.040]   is build it as a set of protocols so that you can wire lots of pieces together
[02:19:53.040 --> 02:19:53.840]   and have it still work.
[02:19:53.840 --> 02:19:57.280]   And that's why I'm encouraged by microblog
[02:19:57.280 --> 02:19:59.360]   because he's taking that idea on board.
[02:19:59.360 --> 02:20:03.920]   And I'm hoping in what we're trying to do with indie web is to encourage other places to drop that too.
[02:20:03.920 --> 02:20:12.160]   So working with new social and other implementations like that to implement this stuff as well.
[02:20:12.160 --> 02:20:14.720]   So that you're able to not.
[02:20:14.720 --> 02:20:17.680]   The problem is there's a lot of monoculture.
[02:20:17.680 --> 02:20:20.640]   There's only one piece of software that you'll have to be running.
[02:20:20.640 --> 02:20:22.480]   And if you're all running that, then it federates.
[02:20:22.480 --> 02:20:23.440]   And if you're not, it doesn't.
[02:20:23.440 --> 02:20:26.960]   And we're trying to make it more like protocols so that you can doesn't matter
[02:20:26.960 --> 02:20:29.920]   what software you're running. If you've got a simple protocol like with Mention,
[02:20:29.920 --> 02:20:32.160]   you can implement that and wire different things together.
[02:20:32.160 --> 02:20:35.760]   Someday everyone will have a web server in their house.
[02:20:35.760 --> 02:20:39.600]   You've already had most web servers in your house.
[02:20:39.600 --> 02:20:40.720]   Yes.
[02:20:40.720 --> 02:20:41.040]   Yes.
[02:20:41.040 --> 02:20:41.840]   That's the problem.
[02:20:41.840 --> 02:20:42.640]   Oh, yeah.
[02:20:42.640 --> 02:20:44.160]   We didn't speed in for example.
[02:20:44.160 --> 02:20:49.680]   We didn't, for example, talk about the new Raspberry Pi that's out that is super powered.
[02:20:49.680 --> 02:20:50.240]   Yeah.
[02:20:50.240 --> 02:20:50.720]   We'll talk about it.
[02:20:50.720 --> 02:20:52.000]   That could have been someone's pick.
[02:20:52.000 --> 02:20:52.240]   Yeah.
[02:20:52.240 --> 02:20:56.640]   Mine is a very happy pick because I think we all need to some cheering up
[02:20:57.600 --> 02:21:00.720]   I found three interesting websites this week.
[02:21:00.720 --> 02:21:06.720]   I'm going to open a incognito tab because I don't want it to be polluted by my choices.
[02:21:06.720 --> 02:21:11.600]   It's the happy or sorry, the nicest place on the inter.net.
[02:21:11.600 --> 02:21:23.520]   And what it does, it just picks a few seconds from a bunch of YouTube videos that have zero views.
[02:21:24.960 --> 02:21:30.000]   And you just, it's kind of like chat roulette with nice, wait a minute.
[02:21:30.000 --> 02:21:31.440]   This is the hug, I'm sorry.
[02:21:31.440 --> 02:21:32.560]   This is a different one.
[02:21:32.560 --> 02:21:33.520]   This is the hug one.
[02:21:33.520 --> 02:21:37.600]   So what you do is you give a hug, I'm sorry.
[02:21:37.600 --> 02:21:39.360]   I have another one that's what I was just saying.
[02:21:39.360 --> 02:21:44.080]   And so they invite you to record a video and you give it a hug.
[02:21:44.080 --> 02:21:53.200]   And then your hug will be on the internet and you will get a hug if you want to go to the nicest place
[02:21:54.240 --> 02:21:56.000]   on the inter.net.
[02:21:56.000 --> 02:21:58.960]   So that's nice.
[02:21:58.960 --> 02:22:00.480]   That's actually not the one I wanted.
[02:22:00.480 --> 02:22:02.160]   And it looks like the one that I wanted,
[02:22:02.160 --> 02:22:07.680]   astronaut.io has gone down and I'm going to be sad about that.
[02:22:07.680 --> 02:22:11.120]   This is the Alkraen a new incognito tab and try it once more.
[02:22:11.120 --> 02:22:13.520]   astronaut.io.
[02:22:13.520 --> 02:22:16.640]   S-T-R-O-N-A-U-T.io.
[02:22:16.640 --> 02:22:20.880]   The idea was this is the one where the guy wrote a script to find
[02:22:21.680 --> 02:22:26.720]   un-viewed videos on YouTube and just fire them up a few seconds at a time.
[02:22:26.720 --> 02:22:27.280]   Is it gone?
[02:22:27.280 --> 02:22:29.440]   Oh crap.
[02:22:29.440 --> 02:22:32.240]   I guess it's gone.
[02:22:32.240 --> 02:22:33.120]   It was really cool.
[02:22:33.120 --> 02:22:36.160]   It was like chat roulette only like fun stuff.
[02:22:36.160 --> 02:22:37.600]   Is it, what does it say, Carson?
[02:22:37.600 --> 02:22:38.720]   Can you make that bigger?
[02:22:38.720 --> 02:22:40.880]   I can't read the little print.
[02:22:40.880 --> 02:22:41.920]   And you got to the site.
[02:22:41.920 --> 02:22:42.880]   I can't get to the site.
[02:22:42.880 --> 02:22:44.880]   Today you are an astronaut.
[02:22:44.880 --> 02:22:49.200]   You are floating in inner space 100 miles above the surface of the earth.
[02:22:49.840 --> 02:22:52.400]   You peer through your window and this is what you see.
[02:22:52.400 --> 02:22:54.000]   You are people watching.
[02:22:54.000 --> 02:22:56.160]   These are fleeting moments.
[02:22:56.160 --> 02:22:58.000]   So go ahead, press go.
[02:22:58.000 --> 02:22:59.840]   You can't.
[02:22:59.840 --> 02:23:03.760]   The site is, I don't know if the site is down or if it's D-dossed.
[02:23:03.760 --> 02:23:06.400]   I saw it on Hacker News.
[02:23:06.400 --> 02:23:07.200]   It's probably down.
[02:23:07.200 --> 02:23:09.280]   It's not letting for me.
[02:23:09.280 --> 02:23:10.160]   Yeah, me neither.
[02:23:10.160 --> 02:23:12.640]   Well, I think it's probably made it worse by pressing.
[02:23:12.640 --> 02:23:13.280]   I didn't help.
[02:23:13.280 --> 02:23:15.280]   I did not help.
[02:23:15.280 --> 02:23:18.800]   It was such, it was actually really interesting.
[02:23:19.360 --> 02:23:25.360]   Because you see very strange, disjointed, unrelated clips.
[02:23:25.360 --> 02:23:31.360]   But it was like, it really was like you were seeing the universe of people
[02:23:31.360 --> 02:23:33.840]   in all their varied forms and their clips from all over the world.
[02:23:33.840 --> 02:23:35.840]   And it's YouTube.
[02:23:35.840 --> 02:23:38.480]   So there was nothing, you know, mostly not inappropriate.
[02:23:38.480 --> 02:23:39.760]   I didn't see anything inappropriate.
[02:23:39.760 --> 02:23:43.120]   Anyway, I guess you can't see it now.
[02:23:43.120 --> 02:23:43.840]   Keep it around.
[02:23:43.840 --> 02:23:44.480]   Astronaut.
[02:23:44.480 --> 02:23:47.280]   Astro.net.
[02:23:47.280 --> 02:23:48.000]   Right? What was it?
[02:23:48.800 --> 02:23:49.680]   Astronaut.
[02:23:49.680 --> 02:23:50.160]   I-O.
[02:23:50.160 --> 02:23:55.120]   It was, I'm coming up with more of these like the planet radio things,
[02:23:55.120 --> 02:23:57.760]   where more of these like interesting uses of the web.
[02:23:57.760 --> 02:23:59.680]   They're back.
[02:23:59.680 --> 02:24:03.040]   Kids were, we're done for the day.
[02:24:03.040 --> 02:24:07.680]   Wanna thank you so much for joining us, Kevin Marks from the indie web,
[02:24:07.680 --> 02:24:10.720]   it would be webcamp, everybody or kevinlex.com.
[02:24:10.720 --> 02:24:12.880]   Always a pleasure.
[02:24:12.880 --> 02:24:14.800]   Hello, Kitty.
[02:24:14.800 --> 02:24:15.440]   And your kitties.
[02:24:15.440 --> 02:24:16.800]   What's your kitties name?
[02:24:16.800 --> 02:24:17.840]   This is Kelly.
[02:24:17.840 --> 02:24:20.000]   Kelly is really quite a piebald.
[02:24:20.000 --> 02:24:21.760]   She's got a little bit of everything in there.
[02:24:21.760 --> 02:24:24.560]   She's a calico, yes, she's a calico.
[02:24:24.560 --> 02:24:26.000]   Calico, is that what you call it?
[02:24:26.000 --> 02:24:27.680]   And-
[02:24:27.680 --> 02:24:29.200]   I think piebald's horses.
[02:24:29.200 --> 02:24:30.080]   Oh, is it really?
[02:24:30.080 --> 02:24:31.280]   I think so.
[02:24:31.280 --> 02:24:32.240]   It's moving like camera.
[02:24:32.240 --> 02:24:32.720]   Still not.
[02:24:32.720 --> 02:24:34.080]   You can't have a piebald cat?
[02:24:34.080 --> 02:24:35.280]   I don't know.
[02:24:35.280 --> 02:24:36.720]   This is a good question.
[02:24:36.720 --> 02:24:38.720]   This is a really weird word.
[02:24:38.720 --> 02:24:41.600]   No, you're gonna have a piebald anything.
[02:24:41.600 --> 02:24:42.720]   Oh!
[02:24:42.720 --> 02:24:43.280]   A pie-
[02:24:43.280 --> 02:24:46.880]   A piebald or pie in animals when it has a pattern of pigmented spots
[02:24:46.880 --> 02:24:50.320]   on an unpigmented white background of hair feathers or scales.
[02:24:50.320 --> 02:24:52.560]   So you can barely have a piebald snake.
[02:24:52.560 --> 02:24:52.960]   Piebald snake!
[02:24:52.960 --> 02:24:58.080]   I mean, I think that cat qualifies as piebald.
[02:24:58.080 --> 02:25:00.240]   The spots are pigmented in shades of black and/or yellow
[02:25:00.240 --> 02:25:02.480]   as determined by the genotype control and the color of the animal.
[02:25:02.480 --> 02:25:06.240]   Same as calico, I don't know.
[02:25:06.240 --> 02:25:14.400]   Stacey Higginbotham covers IoT as an independent and highly talented journalist.
[02:25:15.120 --> 02:25:18.160]   StaceyOnIOT.com and IoTpodcast.com.
[02:25:18.160 --> 02:25:19.680]   The podcast she does with Kevin Dofel.
[02:25:19.680 --> 02:25:21.920]   Oh, I should have said something.
[02:25:21.920 --> 02:25:23.120]   I actually put...
[02:25:23.120 --> 02:25:26.880]   So now Stacey on IoT is not just a sign-up page for the newsletter.
[02:25:26.880 --> 02:25:28.160]   There's actual content there.
[02:25:28.160 --> 02:25:30.960]   But do sign up for the newsletter.
[02:25:30.960 --> 02:25:31.440]   Oh, yes.
[02:25:31.440 --> 02:25:32.320]   Please do that too.
[02:25:32.320 --> 02:25:32.480]   But...
[02:25:32.480 --> 02:25:35.200]   I'm gonna get in trouble.
[02:25:35.200 --> 02:25:36.240]   I'm not gonna say anything more.
[02:25:36.240 --> 02:25:37.280]   What?
[02:25:37.280 --> 02:25:38.320]   How would you get in trouble?
[02:25:38.320 --> 02:25:40.320]   The people who manage me.
[02:25:40.320 --> 02:25:42.800]   My boss will tell me...
[02:25:42.800 --> 02:25:44.960]   You may make people sign up for the newsletter.
[02:25:44.960 --> 02:25:46.160]   You have a boss?
[02:25:46.160 --> 02:25:49.040]   I have a partner, a business partner.
[02:25:49.040 --> 02:25:50.960]   He sells my ads.
[02:25:50.960 --> 02:25:55.840]   He does all the gory work, like contracts and advertising and...
[02:25:55.840 --> 02:26:01.600]   So whatever you do, if you go to StaceyOnIOT and it is just a dynamite site,
[02:26:01.600 --> 02:26:02.880]   don't read the articles.
[02:26:02.880 --> 02:26:05.920]   Just sign up for the newsletter.
[02:26:05.920 --> 02:26:10.400]   Sign up for the newsletter.
[02:26:10.400 --> 02:26:13.440]   We want to keep Stacey afloat.
[02:26:13.440 --> 02:26:16.960]   She's doing such a great job.
[02:26:16.960 --> 02:26:19.040]   Also, follow her on Twitter.
[02:26:19.040 --> 02:26:20.480]   She's @gigastacey.
[02:26:20.480 --> 02:26:23.440]   And the podcast is there as well.
[02:26:23.440 --> 02:26:25.680]   Up to 94 episodes now.
[02:26:25.680 --> 02:26:28.400]   Yep, 95 comes out tomorrow.
[02:26:28.400 --> 02:26:31.120]   That's the real deal when you get to 100.
[02:26:31.120 --> 02:26:38.560]   Well, we lost like two years' worth because we were like 115 or something
[02:26:38.560 --> 02:26:40.880]   when Kevin and I did a giga-ome.
[02:26:40.880 --> 02:26:42.560]   But all of those have disappeared into the...
[02:26:42.560 --> 02:26:43.760]   Why didn't you know that?
[02:26:43.760 --> 02:26:47.120]   So this is a continuation of a giga-ome podcast.
[02:26:47.120 --> 02:26:50.480]   Except it has a totally different name because...
[02:26:50.480 --> 02:26:51.600]   What was it?
[02:26:51.600 --> 02:26:55.280]   So I won't tell anybody, but what was the name of the giga-ome podcast?
[02:26:55.280 --> 02:26:57.680]   The Giga-ome Internet of Things podcast.
[02:26:57.680 --> 02:26:58.400]   Oh, okay.
[02:26:58.400 --> 02:27:01.440]   And you didn't have copies of the shows?
[02:27:01.440 --> 02:27:05.040]   They uploaded everything onto SoundCloud.
[02:27:05.040 --> 02:27:09.680]   Because we recorded them and then sent them to our producer guy.
[02:27:09.680 --> 02:27:11.680]   They uploaded them on SoundCloud and then...
[02:27:11.680 --> 02:27:12.800]   deleted them.
[02:27:12.800 --> 02:27:15.760]   They stopped paying their SoundCloud bill.
[02:27:15.760 --> 02:27:18.720]   I suppose they have the rights to it so you probably couldn't just put them somewhere.
[02:27:18.720 --> 02:27:20.320]   Yeah, it's okay.
[02:27:20.320 --> 02:27:22.800]   I am okay with the web being ephemeral.
[02:27:22.800 --> 02:27:23.440]   I never...
[02:27:23.440 --> 02:27:24.880]   I'm not a hoarder.
[02:27:24.880 --> 02:27:25.360]   I know.
[02:27:25.360 --> 02:27:27.040]   I don't even count on it being ephemeral.
[02:27:27.040 --> 02:27:28.400]   I wish it were in some cases.
[02:27:28.400 --> 02:27:29.360]   Well, yeah.
[02:27:32.400 --> 02:27:36.080]   My other tip is Internet Archive will host your audio for free.
[02:27:36.080 --> 02:27:36.400]   Yes.
[02:27:36.400 --> 02:27:38.640]   If you see it.
[02:27:38.640 --> 02:27:40.720]   Archive.org.
[02:27:40.720 --> 02:27:42.080]   We did that in the early days.
[02:27:42.080 --> 02:27:43.760]   That was one of our distribution methods.
[02:27:43.760 --> 02:27:50.960]   The archive is awesome, Brewster Kale and his team of Jolly Pirates.
[02:27:50.960 --> 02:27:56.560]   They actually hosted all of the giga-ome old archives because when it shut down,
[02:27:56.560 --> 02:27:59.440]   we had no idea what was going to happen and they did a rush job.
[02:28:00.880 --> 02:28:05.040]   They have 2,457 old-time radio shows.
[02:28:05.040 --> 02:28:08.320]   They have a library of sound.
[02:28:08.320 --> 02:28:09.360]   Folks send me.
[02:28:09.360 --> 02:28:11.040]   They have games.
[02:28:11.040 --> 02:28:13.200]   They have 10,000 grateful dead tracks.
[02:28:13.200 --> 02:28:14.800]   They have...
[02:28:14.800 --> 02:28:18.480]   I mean, it's just a wonderful, wonderful collection of...
[02:28:18.480 --> 02:28:23.280]   And this is thanks to Brewster, the Internet Archive,
[02:28:23.280 --> 02:28:26.720]   make sure that things that would otherwise disappear don't.
[02:28:26.720 --> 02:28:30.480]   There's lots of game ROMs up there.
[02:28:30.480 --> 02:28:33.280]   Oh, and one of my favorites, the Prelinger Archives.
[02:28:33.280 --> 02:28:37.760]   This archive of old film footage.
[02:28:37.760 --> 02:28:41.680]   Amazing.
[02:28:41.680 --> 02:28:42.640]   Amazing.
[02:28:42.640 --> 02:28:47.680]   He collected newsreels, found footage, all sorts of stuff.
[02:28:47.680 --> 02:28:50.800]   Just really a great resource.
[02:28:50.800 --> 02:28:52.320]   General Motors presents...
[02:28:52.320 --> 02:28:57.040]   Oh, I wonder if this is where Alamo Draft House gets all of its sound.
[02:28:57.040 --> 02:28:59.040]   I bet ya.
[02:28:59.040 --> 02:29:00.640]   A doctor in industry.
[02:29:00.640 --> 02:29:10.160]   The story of Kenneth W. Randall, M.D.
[02:29:10.160 --> 02:29:15.840]   The times were slower, they take a long time to turn the page on this.
[02:29:15.840 --> 02:29:24.800]   In debt tribute to the members of our medical staff from the men and women in general motors.
[02:29:26.880 --> 02:29:28.240]   Oh, wow.
[02:29:28.240 --> 02:29:32.160]   This is, I think, William Post Jr.'s greatest work.
[02:29:32.160 --> 02:29:34.560]   Worthy of an Academy Award.
[02:29:34.560 --> 02:29:37.360]   Ladies and gentlemen, keep the music going.
[02:29:37.360 --> 02:29:38.160]   I love the music.
[02:29:38.160 --> 02:29:42.240]   Ladies and gentlemen, we thank you for joining us on This Week in Google,
[02:29:42.240 --> 02:29:44.000]   and wish you a happy week.
[02:29:44.000 --> 02:29:51.760]   We'll be back Wednesdays at 130 Pacific, 430 Eastern, 2130, on most of these same...
[02:29:52.320 --> 02:29:57.600]   podcast stations. Goodbye and have a pleasant tomorrow.
[02:29:57.600 --> 02:30:03.600]   I think we found our new theme music.
[02:30:03.600 --> 02:30:06.800]   Very nice.

