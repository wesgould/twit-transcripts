;FFMETADATA1
title=Move Fast and Break Everything
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=433
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.240]   It's time for Twig this week in Google. I'm Jason Howell joining me today will be Stacy Higginbotham and
[00:00:05.240 --> 00:00:10.100]   Matthew Ingram we're gonna talk about all sorts of things and right 8.1 final release
[00:00:10.100 --> 00:00:16.520]   Almost pushing out to pixel devices everywhere. We've got Facebook's photo capture that they're rolling out
[00:00:16.520 --> 00:00:19.940]   also Amazon and by extension Jeff Bezos is
[00:00:19.940 --> 00:00:24.760]   Incredibly rich this holiday season all that more coming up next on this week in Google
[00:00:24.760 --> 00:00:29.840]   Netcasts you love
[00:00:29.840 --> 00:00:31.840]   From people you trust
[00:00:31.840 --> 00:00:43.780]   This is twig bandwidth for this week in Google is provided by cash fly C A C A G F L Y
[00:00:43.780 --> 00:00:46.280]   .com
[00:00:46.280 --> 00:00:51.280]   This is twig this week in Google episode
[00:00:51.280 --> 00:00:55.240]   433 recorded on Wednesday November 29th
[00:00:55.240 --> 00:00:58.840]   2017 move fast and break everything
[00:00:59.600 --> 00:01:06.460]   This week in Google is brought to you by Casper a sleep brand that continues to revolutionize its line of products to create an
[00:01:06.460 --> 00:01:08.760]   exceptionally comfortable sleep
[00:01:08.760 --> 00:01:18.000]   Experience one night at a time get $50 toward any mattress purchase by visiting Casper.com/twig and use promo code twig at checkout
[00:01:18.000 --> 00:01:27.120]   And by Lighthouse Lighthouse is the only security camera powered by the same technology that's in self-driving cars and just for Twit fans
[00:01:27.320 --> 00:01:36.560]   Visit www.light.house/twit to get 40% off your very own Lighthouse with the Lighthouse Intelligence Service plan
[00:01:36.560 --> 00:01:38.760]   Supplies are limited
[00:01:38.760 --> 00:01:42.920]   And by Sonic Twits 10 gig fiber internet service provider
[00:01:42.920 --> 00:01:51.040]   Join Sonics internet revolution as they bring fast affordable internet phone and TV to homes and businesses all over California
[00:01:51.040 --> 00:01:56.320]   Visit sonic.com/twit to sign up for service and receive your first month free
[00:01:57.120 --> 00:02:02.760]   It's time for twig this week in Google. I'm Jason how Leo Le Port is out this week
[00:02:02.760 --> 00:02:04.200]   I believe he's back next week
[00:02:04.200 --> 00:02:09.080]   So you don't have the way too long for the triumphant return of Leo, but I'm super stoked to be here and
[00:02:09.080 --> 00:02:11.520]   Jeff Jarvis is also out by the way
[00:02:11.520 --> 00:02:14.920]   Remember exactly why I know it was in an email somewhere
[00:02:14.920 --> 00:02:20.720]   But he's in Eastern Europe. There we go. Really live Europe. What a typical excuse for Jeff
[00:02:21.520 --> 00:02:27.520]   Traveling whatever but as you as you hear Stacy, hey, come on. Is joining us today. Welcome Stacy
[00:02:27.520 --> 00:02:32.720]   Hello, hello. It's good to talk with you again. Good to see you saw you in person not too long ago
[00:02:32.720 --> 00:02:34.200]   That was that was awesome
[00:02:34.200 --> 00:02:40.480]   It's always kind of an interesting experience to kind of break the internet wall and finally meet face-to-face with people
[00:02:40.480 --> 00:02:44.680]   They've been talking to you for so long so and my first words were oh my god. You're so tall
[00:02:44.680 --> 00:02:48.880]   It's a pretty standard first word that I hear from people
[00:02:49.520 --> 00:02:55.040]   That I meet after after doing all these shows, but and it's great to talk with you also Matthew Ingram
[00:02:55.040 --> 00:03:01.960]   Now with a Columbia journalism review. How's it going Matthew? Yes good. Yeah, yeah, that's brand new
[00:03:01.960 --> 00:03:06.120]   So I'm still getting used to it got my Columbia student card
[00:03:06.120 --> 00:03:12.880]   So I get to use the library and stuff now you get to shop at the bookstore. Yeah, and yeah get some sweaters, maybe
[00:03:12.880 --> 00:03:19.320]   But he's good. That's that's obviously the biggest perk. That's that's the reason that you took the job
[00:03:19.320 --> 00:03:21.320]   Is that you can shop at the gift shop?
[00:03:21.320 --> 00:03:29.480]   Well right on so what exactly what would you say explain your your beat a little bit as far as Columbia journalism reviews concerned?
[00:03:29.480 --> 00:03:34.920]   So I guess my focus is I'm the chief digital writer
[00:03:34.920 --> 00:03:38.280]   So but my focus is mostly on
[00:03:38.280 --> 00:03:46.880]   The power of the platforms as it turns out of which Google is one yeah Google Facebook Twitter Instagram Snapchat
[00:03:48.280 --> 00:03:49.640]   just the
[00:03:49.640 --> 00:03:55.200]   the disruption that they're causing for journalism and the media both economically and sort of
[00:03:55.200 --> 00:03:58.440]   Socially if you will or politically
[00:03:58.440 --> 00:04:02.000]   and how that how media companies should be
[00:04:02.000 --> 00:04:05.040]   Thinking about that and and how they should be dealing with it
[00:04:05.040 --> 00:04:11.600]   Right on well congratulations on that. I know that was a pretty pretty recent. That's pretty exciting last couple of months
[00:04:11.600 --> 00:04:14.520]   Yeah, November 1 actually my first day I
[00:04:15.240 --> 00:04:20.000]   Took the train to Washington and sat through eight hours of congressional hearings. So
[00:04:20.000 --> 00:04:22.680]   Fantastic welcome
[00:04:22.680 --> 00:04:24.800]   Welcome to your new digs
[00:04:24.800 --> 00:04:30.400]   No, it's awesome big big congratulations to you and obviously for this show
[00:04:30.400 --> 00:04:34.120]   I mean that's a lot of what what we talked about on this week in Google and
[00:04:34.120 --> 00:04:39.280]   I was looking through the Google news this week as we were kind of putting things together Carson
[00:04:39.280 --> 00:04:43.200]   I have a list of things and I feel like this is maybe a light
[00:04:43.640 --> 00:04:46.080]   Google news week, but I think we've got enough in here
[00:04:46.080 --> 00:04:52.600]   We've got plenty of other other topics that we can dive into starting with probably the most important news of the week
[00:04:52.600 --> 00:04:58.740]   No doubt people are very excited about the fact that the hamburger emoji is now
[00:04:58.740 --> 00:05:04.160]   Correct the right way around the cheese is no longer on the bun. It's now on the burger
[00:05:04.160 --> 00:05:08.800]   We can all sleep at night. Let's not let's not deny this is a crucial
[00:05:08.800 --> 00:05:11.520]   even global issue like
[00:05:12.000 --> 00:05:17.000]   I for one I'm glad that this has been dealt with because it's been hanging over us for
[00:05:17.000 --> 00:05:19.400]   low these many days
[00:05:19.400 --> 00:05:24.880]   How many hamburger emojis have you been putting out there they've been or is it just that they've been being sent to you?
[00:05:24.880 --> 00:05:28.640]   Because I don't know if I've ever used a hamburger emoji. I actually had neither
[00:05:28.640 --> 00:05:31.040]   I like to eat them, but the emoji
[00:05:31.040 --> 00:05:35.080]   I don't feel that strongly about but I have to admit I got into the
[00:05:35.080 --> 00:05:38.560]   discussion over what is the appropriate order for
[00:05:39.720 --> 00:05:46.760]   The lettuce the tomato the cheese, etc. People feel very passionately about this topic. They really do
[00:05:46.760 --> 00:05:49.920]   I don't know. Look. I mean it looks fine either way
[00:05:49.920 --> 00:05:52.760]   No
[00:05:52.760 --> 00:05:59.560]   From off the microphone. I mean it's still cheese in this in a hamburger, right?
[00:05:59.560 --> 00:06:05.800]   Are we rehashing this now? We don't need every hash. Are we going there? No, no, we don't really need to
[00:06:06.720 --> 00:06:09.720]   Sorry, I didn't mean to take it there although I do want to point out
[00:06:09.720 --> 00:06:15.800]   Another part of this emoji gate if you if it does qualify as a gate is this whole beer thing
[00:06:15.800 --> 00:06:21.640]   Which I hadn't noticed but once I noticed the correction that they made to the beer emoji
[00:06:21.640 --> 00:06:24.040]   I realized like what were they thinking?
[00:06:24.040 --> 00:06:30.000]   How do you have foam on the top of a beer stein a beer mug with no beer up to the top of the
[00:06:30.000 --> 00:06:34.280]   Airport I mean, how do you do that? It looks like a hat like a like a
[00:06:34.920 --> 00:06:39.640]   I don't know a raccoon hat or something really worth what this says to me is that Google
[00:06:39.640 --> 00:06:44.840]   Maybe doesn't invest a lot in its emojis. Yeah, like maybe they've got better things to do
[00:06:44.840 --> 00:06:50.760]   Maybe of their mini messaging platforms. They still are kind of like yeah, let's work on those instead
[00:06:50.760 --> 00:06:56.680]   Let's fix things that are broken instead of fixed little pictures that were drawn by somebody
[00:06:56.680 --> 00:07:02.920]   Although if I remember correctly soon our patch I did say when all this was kind of hitting the fan
[00:07:03.160 --> 00:07:07.920]   That this was gonna be high priority that he was gonna make a change any any help to if they were gonna get on it
[00:07:07.920 --> 00:07:14.320]   Yeah, so well just imagine if it was beer and burgers imagine all the other emojis that that
[00:07:14.320 --> 00:07:17.400]   I seem to be frozen for some reason
[00:07:17.400 --> 00:07:21.600]   I am actually moving my lips, but the beer and
[00:07:21.600 --> 00:07:24.600]   What else what other emojis are they ignoring?
[00:07:24.600 --> 00:07:30.400]   That's true, you know, what have we not discovered yet in their gift emojis are on point?
[00:07:30.960 --> 00:07:36.560]   Okay, so I'm pro those, you know, I have never used an eggplant emoji, but that seems to be a popular one
[00:07:36.560 --> 00:07:41.520]   What about the dumpling emoji? Has anybody looked at the dumpling emoji? So that's brand new
[00:07:41.520 --> 00:07:48.020]   So Jenny Lee who's a friend who used to work at the New York Times wrote about emojis and actually
[00:07:48.020 --> 00:07:50.560]   pursued a personal quest
[00:07:50.560 --> 00:07:53.320]   to get the international emoji
[00:07:53.320 --> 00:07:55.960]   Federation or whatever it is to add the dumpling and
[00:07:55.960 --> 00:07:58.320]   and succeeded so
[00:07:58.320 --> 00:07:59.520]   good for her
[00:07:59.520 --> 00:08:03.640]   Man apples dumpling emoji looks just like a photograph or something that they are
[00:08:03.640 --> 00:08:06.280]   This terrible course
[00:08:06.280 --> 00:08:11.120]   Wow, that's a lot of I didn't realize there was so much variation in how you could do a dumpling
[00:08:11.120 --> 00:08:13.560]   But then I also didn't realize the same about a hamburger, so
[00:08:13.560 --> 00:08:19.360]   Were you not aware that there was a dumpling emoji because I wasn't I wasn't no, you know, I'm honestly
[00:08:19.360 --> 00:08:25.080]   I'm super clueless about emoji. I rarely ever use it unless it's like placed in front of me and super obvious
[00:08:25.080 --> 00:08:29.680]   Like I never go hunting like I see people's emoji language, you know filled
[00:08:29.680 --> 00:08:35.080]   posts on Twitter or whatever and I'm kind of amazed that they took the time to do that
[00:08:35.080 --> 00:08:38.400]   I just want to like you know, I know I know it's it's
[00:08:38.400 --> 00:08:45.600]   You go ahead, Steve. Oh, I was gonna say Google will auto suggest things as emojis when you're typing
[00:08:45.600 --> 00:08:48.240]   So I get a lot of emoji exposure that way
[00:08:48.240 --> 00:08:53.720]   Maybe it's just because I have teenagers and 20 some things, but I
[00:08:54.360 --> 00:08:56.480]   mean emojis are are funny, but
[00:08:56.480 --> 00:08:58.280]   there are
[00:08:58.280 --> 00:09:00.280]   there are people I know and
[00:09:00.280 --> 00:09:06.120]   related to who they conduct entire conversations in emojis or the emojis are a
[00:09:06.120 --> 00:09:10.800]   significant part of the conversation. Yeah, and it's it's interesting to me as a
[00:09:10.800 --> 00:09:18.880]   as a sort of student of language how how much you can convey with teeny tiny little cartoon
[00:09:19.720 --> 00:09:26.800]   Figures and I know when we used to use slack Stacy and I at way back in the gig home days, which I think was a hundred years ago
[00:09:26.800 --> 00:09:36.480]   It was amazing how much you could convey was just a single emoji where it would take you, you know sentences to kind of get across what you were
[00:09:36.480 --> 00:09:40.560]   It's just interesting to me that they're very emotionally
[00:09:40.560 --> 00:09:43.400]   laden but
[00:09:43.400 --> 00:09:49.000]   But so simple and obviously dumplings don't actually mean that much they just mean dumplings
[00:09:49.240 --> 00:09:50.440]   but
[00:09:50.440 --> 00:09:53.880]   You know you can get across if you ever looked at shares tweets
[00:09:53.880 --> 00:10:00.160]   Sometimes I have to spend like 20 minutes trying to understand what she's saying because it's literally all emoji
[00:10:00.160 --> 00:10:05.200]   But I don't know if it's only emojis. I mean share shares lived a hard life
[00:10:05.200 --> 00:10:09.360]   So true and she conveys emotion in lots of ways obviously
[00:10:09.360 --> 00:10:15.720]   But I just find it fascinating that I thought they were dumb and I wished I thought they were a sign of the dumbing down of
[00:10:16.000 --> 00:10:22.880]   Society in general in my kids in particular, but I think they they actually do convey things that are hard to convey in words
[00:10:22.880 --> 00:10:25.480]   So yeah, they they also soften things
[00:10:25.480 --> 00:10:31.360]   I I use emojis a lot of times as a as yeah to add emotion
[00:10:31.360 --> 00:10:36.000]   And I think you know people market is being a preteen girl thing and I have a preteen girl
[00:10:36.000 --> 00:10:41.160]   And she definitely uses emojis and stickers. Oh my god the stickers. Yes, but I
[00:10:42.720 --> 00:10:49.880]   Actually think it's it's useful and fun and so and who doesn't like getting like the little sparkly champagne plus the fire
[00:10:49.880 --> 00:10:55.960]   Where it's plus everything when you like you like get something that's way better than like yay good job
[00:10:55.960 --> 00:10:58.400]   You know the burger
[00:10:58.400 --> 00:11:01.760]   Fixing the burger is great, but what I wish as an old person
[00:11:01.760 --> 00:11:06.880]   Someone would do is is figure out a way to incorporate a magnifying glass
[00:11:07.400 --> 00:11:13.240]   To figure out because sometimes I will see emojis and I literally have no idea what they are like I so there
[00:11:13.240 --> 00:11:19.160]   There's so much going on in the emoji like a burger. I can figure out obviously, but some of them
[00:11:19.160 --> 00:11:24.680]   I'm squinting out them. I don't know what it is. Am I supposed to be a happy about it?
[00:11:24.680 --> 00:11:27.240]   It's like your
[00:11:27.240 --> 00:11:33.480]   Language with different like with some covering some of my emojis mean something and with other people my emojis mean totally different thing
[00:11:34.000 --> 00:11:37.280]   So it's kind of like whoa as a student translation
[00:11:37.280 --> 00:11:42.320]   It's like you need a two times magnification button exactly two times
[00:11:42.320 --> 00:11:47.840]   Yeah, and then you need a click to translate. Oh, yeah Google well, you know, that shouldn't be that hard, right?
[00:11:47.840 --> 00:11:55.080]   Yeah, that should be really easy actually doesn't each make emoji like I know I've seen texts where they strip out the emoji
[00:11:55.080 --> 00:11:57.640]   But they replace it with a description of what the emoji actually is
[00:11:57.640 --> 00:12:03.800]   They can translate welch, you know, they can probably translate emojis
[00:12:04.760 --> 00:12:11.160]   Well, I would say no because not everything correlates exactly to what it is true. Yeah
[00:12:11.160 --> 00:12:15.240]   But in a couple of hundred years when emoji is an official language
[00:12:15.240 --> 00:12:19.480]   That people live and exist there that you know in their lives
[00:12:19.480 --> 00:12:25.240]   100% of the time this is gonna be a you know do the research now we need to figure this out now
[00:12:25.240 --> 00:12:32.920]   It's important for the future generations for society absolutely. These are not these are these are not you know petty topics here
[00:12:33.400 --> 00:12:37.200]   I'm like I'm so old school. I still do the the emoticons
[00:12:37.200 --> 00:12:41.680]   I it's like it's habit. It's total habit to do the colon
[00:12:41.680 --> 00:12:43.920]   parentheses
[00:12:43.920 --> 00:12:51.080]   Question the smile smiley face. Yeah, do you do a do you do a colon and then a period and then the bracket?
[00:12:51.080 --> 00:12:53.240]   We're just a colon and the bracket
[00:12:53.240 --> 00:12:58.000]   Well, I thought you were gonna ask do you do the bracket and then the colon which is the like opposite way
[00:12:58.000 --> 00:13:01.660]   But no, I do the I do the colon and then the bracket. I don't put it
[00:13:01.660 --> 00:13:03.660]   So it doesn't really
[00:13:03.660 --> 00:13:07.200]   Because my daughter told me that I'm
[00:13:07.200 --> 00:13:10.900]   She made fun of me because I make the smiley face with a nose
[00:13:10.900 --> 00:13:18.780]   Yeah, really you said that's like it's like I'm from the the 16th century or something
[00:13:18.780 --> 00:13:25.220]   It's like you use the Oxford comma in a text. Yeah, yeah, exactly. Yeah, I probably so now I just use it to bugger
[00:13:25.220 --> 00:13:27.620]   I probably do that too actually thing it
[00:13:27.620 --> 00:13:30.500]   Dang it. I need to get with the times
[00:13:31.140 --> 00:13:37.220]   Keep it up, you know, I have I have good news, but it's not on the emoji front cool. Take it away
[00:13:37.220 --> 00:13:41.820]   Alright, as you guys know, I have a pixel little a little pixel
[00:13:41.820 --> 00:13:48.800]   I want to call it the pixel mini, but I won't and I had clicking and I updated it and low and behold it
[00:13:48.800 --> 00:13:50.800]   The update came through yesterday
[00:13:50.800 --> 00:13:53.020]   The clicking has stopped so
[00:13:53.020 --> 00:13:55.660]   There was clicking
[00:13:55.660 --> 00:14:01.120]   Yeah, sorry. I missed that. Yeah, no no there was clicking and I had heard some reports of this
[00:14:01.120 --> 00:14:05.360]   So explain the clicking issue because I have not experienced this with my two Excel
[00:14:05.360 --> 00:14:08.960]   So it was a so it's only it was only the small one
[00:14:08.960 --> 00:14:12.000]   That is that the pixel - buzzing?
[00:14:12.000 --> 00:14:19.400]   So it was clicking and it was a radio issue was the NFC radio. So the solution Google gave me
[00:14:19.400 --> 00:14:24.760]   Originally was we'll turn the NFC off and wait for your update. It's a great and I was like
[00:14:24.760 --> 00:14:30.080]   Can you just push me the update now and they're like no no we cannot do so
[00:14:30.720 --> 00:14:35.760]   I did get the update. I've now turned NFC back on so I can use payments. It's very exciting
[00:14:35.760 --> 00:14:40.280]   But sweet instant. So that was it a lot of people had this issue
[00:14:40.280 --> 00:14:45.360]   Some people actually got new phones, but most people just are waiting for the update
[00:14:45.360 --> 00:14:50.200]   Right before Google knew what to do with it. They were probably like oh, I guess so you got to give you a replacement
[00:14:50.200 --> 00:14:54.960]   Then they realized hey, we can probably fix that was an update. It was really irritating because it was on
[00:14:54.960 --> 00:14:59.480]   Oh, I heard about was the burden. Did you get any bread in? That's the two Excel
[00:15:00.320 --> 00:15:05.800]   Yeah, because there's the two different sizes by two different manufacturers even though they're both released but with Google's name
[00:15:05.800 --> 00:15:09.920]   It's two different problem two different entire sets of problems on both sides. Yeah
[00:15:09.920 --> 00:15:13.480]   Yeah, and then we also have the story of
[00:15:13.480 --> 00:15:18.360]   Random reboots that are apparently happening on some pixel two devices
[00:15:18.360 --> 00:15:23.280]   In this case apparently the issue might have something to do with the LTE modem
[00:15:23.280 --> 00:15:29.680]   Some some users on Reddit were pointing that this seemed to happen when they were connected to a low signal
[00:15:29.680 --> 00:15:36.000]   LTE connection and that the modem might be causing a kernel panic of some sort when it's an LTE mode
[00:15:36.000 --> 00:15:38.200]   Those are the best kernel panics or the best
[00:15:38.200 --> 00:15:41.880]   Your kernel panics nice I
[00:15:41.880 --> 00:15:45.160]   Have a kernel panic on my oven
[00:15:45.160 --> 00:15:48.640]   That sounds dangerous
[00:15:48.640 --> 00:15:52.400]   Constantial kernel panic
[00:15:52.400 --> 00:15:59.200]   That is true. That's a that's a given in today's day and age. I thought of you Stacy. I was watching
[00:15:59.840 --> 00:16:01.840]   Mr. Robot and
[00:16:01.840 --> 00:16:08.360]   They hacked into someone's house and turned the heat down and made the lights flash and made the alarm go off
[00:16:08.360 --> 00:16:10.760]   And I forget what else he did and I thought of you
[00:16:10.760 --> 00:16:16.080]   Yes, that that has happened in my house. They made a blender go off. It was a door unlock
[00:16:16.080 --> 00:16:20.480]   That's when they that's when they hacked your house. That is when they hacked my house the best part
[00:16:20.480 --> 00:16:22.680]   but the best part was they you had to
[00:16:23.480 --> 00:16:28.800]   We can your password deliberately so they could hack your house because they actually couldn't hack it
[00:16:28.800 --> 00:16:32.640]   That was the best part that was the best part TV journalism
[00:16:32.640 --> 00:16:39.760]   Let's pretend we hacked your house in a magical world where your house actually gets hacked by somebody
[00:16:39.760 --> 00:16:45.880]   It I mean so Matthew do you have the the pixel 2 or the 2xl?
[00:16:45.880 --> 00:16:51.080]   Are you on the picture? I am I am embarrassed to say that I do not you have the first generation or what phone do you have?
[00:16:51.080 --> 00:16:54.520]   I have a Samsung Galaxy S7
[00:16:54.520 --> 00:17:00.800]   With a cracked screen. Oh, yeah, so I'm I'm kind of a
[00:17:00.800 --> 00:17:05.840]   Hobo when it comes to phones as compared to you guys. I don't have the latest every time I come on
[00:17:05.840 --> 00:17:09.080]   You know Leo has bought five of whatever the new
[00:17:09.080 --> 00:17:14.200]   Phone is and and I'm and he's like how's your phone? I'm like I'm fine. I'm still with them
[00:17:16.440 --> 00:17:21.600]   So I feel like I'm you know in a different century, but I would love the pixel
[00:17:21.600 --> 00:17:24.760]   I just haven't been able to convince my
[00:17:24.760 --> 00:17:30.920]   But chief financial officer to let me buy one I would say there's no shame whatsoever in holding on to your phone
[00:17:30.920 --> 00:17:37.000]   Sometimes I wish that my life was a little simpler when it comes to phones because yeah because you're right
[00:17:37.000 --> 00:17:41.000]   We're constantly getting new phones. It's kind of part of what we do here. It's great play the phone
[00:17:42.280 --> 00:17:47.240]   Yeah, well, yes, so do I where I go swimming, which is my other favorite thing to do and
[00:17:47.240 --> 00:17:54.480]   So I think my wife is pretty much just wants me to have whatever phone was popular five years ago
[00:17:54.480 --> 00:17:57.280]   Because then when I drop it, it's not as big a deal
[00:17:57.280 --> 00:18:05.320]   Yeah, I can't remember the yes, the real question was what you Matthew is are you unmasked at on because as a social media reporter
[00:18:05.320 --> 00:18:08.760]   I would expect our platform reporter. I am unmasked at
[00:18:09.320 --> 00:18:12.600]   Which is a great name, which is a great name for a bunch of reasons
[00:18:12.600 --> 00:18:15.000]   It's the name of a heavy metal band. Yes, it is
[00:18:15.000 --> 00:18:19.120]   I think it's where it came from but it's also just a great name masterton. It's
[00:18:19.120 --> 00:18:25.720]   If there was social media, you know in the ice age, I think masterton would have been a great a great name
[00:18:25.720 --> 00:18:28.520]   I am on there, but I don't
[00:18:28.520 --> 00:18:36.520]   Do a lot I am interested in it though. It's I mean I've tried them all I remember to remember app.net
[00:18:36.960 --> 00:18:41.800]   Yes, oh, yes, that one I actually thought had some potential
[00:18:41.800 --> 00:18:46.040]   But it just never never really took off masterton is
[00:18:46.040 --> 00:18:48.840]   Interesting because it's distributed you can run your own
[00:18:48.840 --> 00:18:55.960]   Mastodon server if you want or you can join one that someone else is already running. So it's it's sort of federated in a way
[00:18:55.960 --> 00:18:57.960]   which I think is interesting but
[00:18:57.960 --> 00:19:05.800]   You know the biggest problem is the same one with every service the network effects that keep you on Twitter are
[00:19:06.480 --> 00:19:08.480]   almost impossible they create a
[00:19:08.480 --> 00:19:14.800]   A moat. It's almost impossible to cross so one or two people you know might go on masterton
[00:19:14.800 --> 00:19:18.800]   But even they're probably still spending most of their time on Twitter because that's where everyone is
[00:19:18.800 --> 00:19:24.120]   That's where everyone moans about how much they hate Twitter. So it's
[00:19:24.120 --> 00:19:27.960]   But everybody stays there. Yeah, it's a totally dysfunctional
[00:19:27.960 --> 00:19:32.240]   Environment and yet it's the people there are what make it
[00:19:32.920 --> 00:19:38.040]   Useful and that's that was reinforced for me when Twitter first started having problems way back when
[00:19:38.040 --> 00:19:44.200]   We kept going down all the time and everybody would jump back on Twitter as soon as it was up and bitch about how it went down
[00:19:44.200 --> 00:19:51.560]   They never left right, you know the Twitter could have been put together with you know bungee cords and gum and
[00:19:51.560 --> 00:19:54.760]   Twine and whatever and probably was for all I know
[00:19:54.760 --> 00:19:57.360]   But people just kept using it
[00:19:57.760 --> 00:20:02.400]   Even while they were moaning about it and that that was a sign of how strongly they felt about it
[00:20:02.400 --> 00:20:07.560]   And how strongly they were tied to it. I don't know how something like Mastodon. How do you get?
[00:20:07.560 --> 00:20:10.200]   Get beyond that I
[00:20:10.200 --> 00:20:16.320]   Had not heard a master on before this is interesting. I wondered I wondered when I saw you go straight to the computer
[00:20:16.320 --> 00:20:21.600]   Like what I was like I know Mastodon the band. I know the band, but I've never heard of a social network
[00:20:21.600 --> 00:20:26.160]   It's sort of low key
[00:20:27.040 --> 00:20:32.160]   It's a and it's it's it's almost a straight out of clone, but
[00:20:32.160 --> 00:20:38.960]   You know, it's very if you're familiar with Twitter. He looks it's easy to figure out how fast it works. Yeah
[00:20:38.960 --> 00:20:46.320]   Yeah, yeah, yeah, it's interesting with these social networks, you know that have been around what it really does feel like you're talking about that moat is
[00:20:46.320 --> 00:20:48.680]   so
[00:20:48.680 --> 00:20:51.600]   Uncrossable at this point when you're talking about Facebook you're talking about Twitter
[00:20:51.800 --> 00:20:58.640]   It really makes very little room for anyone to come in and expect that they can command enough
[00:20:58.640 --> 00:21:05.200]   And yeah enough crossover from anyone that anyone's going to pull all their contacts over there
[00:21:05.200 --> 00:21:07.720]   I mean there will be outlying cases
[00:21:07.720 --> 00:21:12.240]   But by and large as a social network you need to continue growing and that's just hard to do in this day
[00:21:12.240 --> 00:21:14.240]   And we kind of have the means that work now
[00:21:14.240 --> 00:21:21.040]   Yeah, and the barriers to entry are like you said you have to find all the people who you followed on Twitter and follow them on Mastodon
[00:21:21.560 --> 00:21:23.560]   I'm frozen again
[00:21:23.560 --> 00:21:29.480]   I blame my network provider, but then and lots of people are just not gonna do that. It's so
[00:21:29.480 --> 00:21:32.080]   So you might get the odd
[00:21:32.080 --> 00:21:39.880]   nerdy friend who'll go and use something new, but you can't reproduce that social graph that that brings all the power to that
[00:21:39.880 --> 00:21:47.800]   Network and so no matter how bad Facebook and Twitter are you're gonna keep using them and Instagram probably because you've built up this huge
[00:21:48.000 --> 00:21:52.160]   It actually reminds me of a friend who used to play World of Warcraft
[00:21:52.160 --> 00:22:00.560]   I think still does and he got married and had a few kids and he thought I should probably stop playing World of Warcraft for three hours every night and
[00:22:00.560 --> 00:22:06.320]   Then he told me I found that he was back on World of Warcraft and he said I couldn't stop
[00:22:06.320 --> 00:22:09.920]   I spent literally tens of thousands of hours
[00:22:09.920 --> 00:22:17.520]   Creating my character and building this network of friends and you know other players. I he said I couldn't quit
[00:22:18.360 --> 00:22:19.800]   hmm, I
[00:22:19.800 --> 00:22:23.200]   Can't quit you Facebook. I can't quit you World of Warcraft
[00:22:23.200 --> 00:22:29.880]   So I feel about Facebook. I'm now what am I what is it? I'm almost a year off of Facebook
[00:22:29.880 --> 00:22:35.440]   Although I did I did go on there a couple of weeks ago first login in almost a year
[00:22:35.440 --> 00:22:42.920]   But that was just to post one thing and then get right out. I didn't read any comments. I didn't want to get sucked into the Facebook universe
[00:22:42.920 --> 00:22:43.960]   I
[00:22:43.960 --> 00:22:45.960]   Don't know if my life has improved
[00:22:46.080 --> 00:22:48.080]   But to quit
[00:22:48.080 --> 00:22:50.080]   Well this time last year
[00:22:50.080 --> 00:22:58.040]   I think we were starting to hear a lot about kind of how Facebook was was being you know potentially used to influence a lot of people
[00:22:58.040 --> 00:22:59.440]   you know
[00:22:59.440 --> 00:23:06.760]   Thought the stories that were being presented to them how Facebook was you know its algorithms were kind of architecting what you see
[00:23:06.760 --> 00:23:11.920]   And I started to really kind of consider wow, you know in a in a world is very uncertain right now
[00:23:12.200 --> 00:23:18.800]   This is like the the best propaganda platform that there is because because of the way it's set up
[00:23:18.800 --> 00:23:20.800]   And I just I didn't want to be
[00:23:20.800 --> 00:23:26.240]   Facing that the the the kind of weird aspect of that though is that I stayed on Twitter
[00:23:26.240 --> 00:23:31.000]   And it's not like Twitter's any better, but I felt really weirded out by Zuckerberg's
[00:23:31.000 --> 00:23:36.600]   Response about the election and being like well, I think that's really crazy
[00:23:36.600 --> 00:23:39.640]   I can't remember his his direct quote when I heard it was a crazy idea
[00:23:39.640 --> 00:23:43.240]   It was a crazy idea and when I heard that I just thought that was so
[00:23:43.240 --> 00:23:50.400]   So wrong for someone with that much influencing control over that company to say something so irresponsible
[00:23:50.400 --> 00:23:52.800]   That was so obviously not true
[00:23:52.800 --> 00:23:59.080]   And I thought if you're thinking and saying that in the public then you really don't care about the influence
[00:23:59.080 --> 00:24:01.520]   I think history has kind of shown at least over the past
[00:24:01.520 --> 00:24:06.680]   many months that Zuckerberg is at least to some degree come around and
[00:24:07.040 --> 00:24:11.200]   Started to really take it seriously, but then at that moment I thought wow, that's really irresponsible
[00:24:11.200 --> 00:24:13.960]   And I don't want to be a part of it. I think definitely
[00:24:13.960 --> 00:24:16.760]   Congressional hearings will get your attention in that way
[00:24:16.760 --> 00:24:22.200]   So I think that's helped kind of convince him that it's an important problem. Yeah, absolutely
[00:24:22.200 --> 00:24:25.960]   So I don't know if I end up I don't know
[00:24:25.960 --> 00:24:29.560]   It's it's hard because I've been away for a year now and I'm like, okay
[00:24:29.560 --> 00:24:33.580]   So it was difficult in the beginning because that's where all my friends and my fan
[00:24:33.580 --> 00:24:38.220]   You know if I have any family extended family that's online and involved in anything
[00:24:38.220 --> 00:24:39.780]   They're probably involved in Facebook
[00:24:39.780 --> 00:24:44.340]   So that's a good way to really kind of keep people in the know of my life and follow them
[00:24:44.340 --> 00:24:46.340]   But I also just do you feel left out?
[00:24:46.340 --> 00:24:48.900]   Not at this point. I did in the beginning
[00:24:48.900 --> 00:24:53.500]   It did in the big my daughter my my 19 year old daughter quit
[00:24:53.500 --> 00:24:58.420]   deleted her account and she went I think almost a year
[00:24:59.180 --> 00:25:06.580]   And then she got back on and I said why and she said I I was missing things like I didn't know what was happening with people
[00:25:06.580 --> 00:25:13.180]   Yeah, you know family members like who was getting married or who had a baby or and then the baby photos show up there and then
[00:25:13.180 --> 00:25:16.540]   People say oh, did you see the baby photos? You're like what baby?
[00:25:16.540 --> 00:25:28.180]   Hey congratulations, yeah, yeah, and so you miss like she was definitely philosophically opposed
[00:25:29.180 --> 00:25:35.780]   But but she just got pulls back in because everybody kept saying did you see this or did you hear from so and so or did you?
[00:25:35.780 --> 00:25:41.300]   Yeah, I just I just confess that I have not and then I move on
[00:25:41.300 --> 00:25:45.860]   But I'm also kind of a missing profit individual. So I'm like, yeah, who cares? It's just a baby
[00:25:45.860 --> 00:25:49.100]   It's just a baby
[00:25:52.860 --> 00:25:59.140]   Parents or no well, I say I said it when I had a baby. I'm like, it's just a baby. It's just a baby
[00:25:59.140 --> 00:26:02.140]   Baby for thousands of years exactly
[00:26:02.140 --> 00:26:08.020]   I care when they do something interesting. Yes. I care about my baby a lot more than you probably do
[00:26:08.020 --> 00:26:15.140]   I'm very excited. I don't expect that you are equally excited. They babies are like opinions. Everybody thinks there's is the best
[00:26:15.140 --> 00:26:16.740]   It's true
[00:26:16.740 --> 00:26:18.740]   It's so true
[00:26:19.140 --> 00:26:25.540]   Yeah, I don't know I did kind of recently get back into the Instagram thing because everybody was telling
[00:26:25.540 --> 00:26:28.500]   Well, if you don't like Facebook, but you still want to see what people are up to
[00:26:28.500 --> 00:26:34.940]   Instagram is great because then it kind of removes true like another part of Facebook that I never really cared about was that
[00:26:34.940 --> 00:26:39.900]   It seems like such an open inviting place to share your opinions and people do it all the time
[00:26:39.900 --> 00:26:44.260]   But then when you do the way it amplifies that message across the network
[00:26:44.260 --> 00:26:48.780]   Suddenly you're getting into these heated conversations with people you don't know
[00:26:48.780 --> 00:26:51.060]   And that just doesn't make me feel good
[00:26:51.060 --> 00:26:56.900]   So then it makes me not want to share my opinions at all because it's just yeah your own headache and sorrow is not worth it
[00:26:56.900 --> 00:27:02.220]   Yet I'm on Twitter. I don't even know just which is a hundred times worse for that
[00:27:02.220 --> 00:27:08.420]   But I could get dragged into someone I was just talking to someone yesterday and they said yeah, I finally gave up
[00:27:08.420 --> 00:27:13.700]   I found I was in a heated debate with someone named. I think it was jalapeno poop
[00:27:13.700 --> 00:27:16.460]   She said I thought to myself
[00:27:16.940 --> 00:27:22.140]   What am I doing doing with my life? I was gonna say time to reevaluate your life
[00:27:22.140 --> 00:27:29.060]   And they spilled jalapeno rock so you know what I do feel like there is a little bit of a difference though on Twitter
[00:27:29.060 --> 00:27:35.260]   I feel like I can be a little bit more passive in that regard now. I don't really share a whole lot of very
[00:27:35.260 --> 00:27:40.780]   divisive, you know opinions on on modern modern topics
[00:27:40.780 --> 00:27:42.820]   You know things that are really bubbling up to the surface right now
[00:27:43.100 --> 00:27:49.020]   Because I know the reaction that it could get and it's just don't I don't have that room in my life for it
[00:27:49.020 --> 00:27:54.540]   But Twitter feels a little bit like I can just be an observer and not get involved in it
[00:27:54.540 --> 00:28:00.740]   And still kind of learn things from it and use it for what I want to where's Facebook feels way more personal to me
[00:28:00.740 --> 00:28:04.300]   Like because I'm on a network filled with friends and family
[00:28:04.300 --> 00:28:06.500]   I feel like I can say these things without judgment
[00:28:06.500 --> 00:28:12.540]   But then when I do people outside of that network still get pulled in somehow and then that enrages
[00:28:12.620 --> 00:28:18.460]   My friends and family who then they like go to battle, you know with each other and they all line up and
[00:28:18.460 --> 00:28:23.580]   It's a war of words and I just don't like being exposed to all of that. It just doesn't feel good
[00:28:23.580 --> 00:28:28.180]   It's I will say you have multiple social networks and
[00:28:28.180 --> 00:28:32.100]   Ways and that is anathema to
[00:28:32.100 --> 00:28:34.540]   Mark Zuckerberg or was for so long
[00:28:34.540 --> 00:28:42.460]   Yeah, and actually there's a whole sociological sort of area of research that that looks at what's called the collapse of context
[00:28:42.700 --> 00:28:44.700]   so you
[00:28:44.700 --> 00:28:49.860]   you have these different networks whether it's family and work friends and personal friends and
[00:28:49.860 --> 00:28:56.420]   people your friends with who are you know trans or gay and then you have other friends who they would never
[00:28:56.420 --> 00:29:02.940]   those two groups would never mix and you speak in different ways with different people and you talk about different things and and
[00:29:02.940 --> 00:29:08.900]   and social media often tends to just remove any sense of context. Yes, so
[00:29:08.900 --> 00:29:11.340]   it's the thing you
[00:29:11.580 --> 00:29:20.060]   You might say to someone in a bar after a couple of years as a joke or you know a sarcastic remark gets amplified and
[00:29:20.060 --> 00:29:22.980]   distributed to like millions of people
[00:29:22.980 --> 00:29:24.860]   it's just
[00:29:24.860 --> 00:29:32.340]   It creates a whole different phenomenon. Yeah, and not at all not at all what you may have expected would happen when you dropped that
[00:29:32.340 --> 00:29:34.660]   seemingly, you know in
[00:29:34.660 --> 00:29:36.000]   innocent
[00:29:36.000 --> 00:29:40.480]   comments at one point and now suddenly now suddenly it's a meme or
[00:29:40.480 --> 00:29:42.680]   Suddenly it's making you know
[00:29:42.680 --> 00:29:47.000]   It's making the news for whatever reason because it ties into some greater sentiment or whatever
[00:29:47.000 --> 00:29:54.720]   Well and social like Facebook is this too and Instagram a lot of it is is almost performance social performance. Yes
[00:29:54.720 --> 00:29:56.720]   so you're performing for
[00:29:56.720 --> 00:30:00.980]   your friends or people you a tribe that you want to belong to or group you want to
[00:30:00.980 --> 00:30:04.240]   impress and so you in some cases people
[00:30:04.920 --> 00:30:06.920]   exaggerate their reaction because they want to
[00:30:06.920 --> 00:30:12.000]   They're signaling to a group that they are part of that by attacking someone else
[00:30:12.000 --> 00:30:15.920]   Absolutely, I never do that obviously other people never
[00:30:15.920 --> 00:30:18.760]   I think we've all probably done that once or twice
[00:30:18.760 --> 00:30:21.480]   with whether we realize it or not and
[00:30:21.480 --> 00:30:27.800]   Yeah, I don't know the answers and I don't even know if Facebook is is necessarily the you know
[00:30:27.800 --> 00:30:30.320]   the air quotes evil
[00:30:30.320 --> 00:30:33.480]   That I thought that it was a year ago when I stepped away
[00:30:33.480 --> 00:30:35.800]   I just I just know that for my own quality of life
[00:30:35.800 --> 00:30:40.600]   I needed to step away and honestly in retrospect. It was a really good thing
[00:30:40.600 --> 00:30:43.360]   I became incredibly productive as a result of it
[00:30:43.360 --> 00:30:49.480]   And I can I can totally point to that switch is being the time when I started to focus on things that I'd been putting off for a
[00:30:49.480 --> 00:30:54.160]   Long time the alternative to that was me sitting on the couch for hours at the end of my day
[00:30:54.160 --> 00:30:58.880]   Oh, I've got a couple of hours. I could watch something I could read something instead. I'm gonna scroll
[00:30:59.600 --> 00:31:06.920]   continuously eyes wide open at this Facebook feed and just get angry or get like just feel bad and that didn't seem like a good use of my
[00:31:06.920 --> 00:31:12.640]   Time, so are you gonna get rid of your Netflix subscription next in your quest for productivity? Well?
[00:31:12.640 --> 00:31:15.120]   No, probably not
[00:31:15.120 --> 00:31:18.600]   There's some really good stuff on Netflix Stacy. There's
[00:31:18.600 --> 00:31:24.760]   I'm not advocating it. I'm like, how far are you gonna take this? I don't know I don't know how far I take it
[00:31:25.080 --> 00:31:30.500]   Sometimes I told somebody yesterday that Twitter for me is like television like a lot of people watch
[00:31:30.500 --> 00:31:35.480]   garbage TV shows because they're funny, you know, they watch the biggest loser
[00:31:35.480 --> 00:31:37.880]   I don't want to name shows because people probably love them
[00:31:37.880 --> 00:31:42.640]   But everyone has some and or they watch dancing with the stars or you know
[00:31:42.640 --> 00:31:48.840]   America's got weirdos or whatever show they love and and for me, it's you know Twitter is that and
[00:31:48.840 --> 00:31:51.240]   The only problem is the show seems to be
[00:31:52.160 --> 00:31:59.920]   Going to a dark place a lot and so I've actually started questioning whether that's good for my kind of sanity to just
[00:31:59.920 --> 00:32:03.600]   continually be subjected to the worst of
[00:32:03.600 --> 00:32:08.600]   Humanity and in many cases that's coming from the president of the United States
[00:32:08.600 --> 00:32:15.440]   So that's and that's also disturbing in a whole different way, but it's I am trying to limit it
[00:32:15.440 --> 00:32:20.680]   You know, I try to stop but it pulls you back in it does every time you think about
[00:32:21.440 --> 00:32:25.240]   Twitter pulls me back in you need to follow more puppy and
[00:32:25.240 --> 00:32:31.800]   Face faces and things and real scientists other accounts where you're like we're scrolling question like how delightful
[00:32:31.800 --> 00:32:39.040]   So true, I have a bunch actually that I followed just for that like they're like a pala cleanser, you know part way
[00:32:39.040 --> 00:32:44.200]   My daughter will grab them because I'll show her the best ones and she's like
[00:32:44.200 --> 00:32:47.000]   How come you haven't liked this more?
[00:32:50.960 --> 00:32:54.320]   It was my daughter we I said are you following this one and they were all
[00:32:54.320 --> 00:32:58.520]   You know humor accounts or just weird accounts or
[00:32:58.520 --> 00:33:04.880]   Funny pictures and and we actually exchanged lists. So I found a whole bunch of new ones that
[00:33:04.880 --> 00:33:12.560]   And it isn't it's a it's a nice feeling you're starting to feel bad about your life in the world and then you see something funny
[00:33:12.560 --> 00:33:14.920]   Yeah
[00:33:14.920 --> 00:33:17.500]   Yeah, someone in chat was it was a robot
[00:33:17.760 --> 00:33:22.520]   Robot says my Twitter is nice your feed is what you make it and I know this
[00:33:22.520 --> 00:33:28.840]   I know that like if I really wanted to I could eliminate a lot of the stuff that like makes me upset or makes me not feel
[00:33:28.840 --> 00:33:31.200]   super happy about you know
[00:33:31.200 --> 00:33:37.480]   About a life at this very moment in my Twitter feel but I but I refuse to remove people that I'm following that
[00:33:37.480 --> 00:33:42.800]   Post things like that because I also feel the need to be informed about what's going on
[00:33:42.800 --> 00:33:46.520]   Not to be clueless and disconnected and that's a good really difficult job
[00:33:46.520 --> 00:33:53.200]   I kind of have to be on there because it's a great source of news and I'm connected to all kinds of really smart people
[00:33:53.200 --> 00:33:57.800]   Who share things that I would never find otherwise and that's a massive massive?
[00:33:57.800 --> 00:34:02.240]   benefit it just comes along with you know an ocean of of
[00:34:02.240 --> 00:34:07.120]   Felt or and it's it's hard sometimes to
[00:34:07.120 --> 00:34:10.420]   to kind of push aside the
[00:34:10.420 --> 00:34:12.200]   depressing or
[00:34:12.200 --> 00:34:16.120]   Enraging stuff and and find the jewels in there
[00:34:16.120 --> 00:34:19.040]   Yep
[00:34:19.040 --> 00:34:23.480]   Agreed well, that's delightful. I'm happy we lifted lifted your spirits everyone
[00:34:23.480 --> 00:34:27.760]   You're gonna take a nap after that discussion and I'm gonna help you out
[00:34:27.760 --> 00:34:32.000]   We're gonna thank the sponsor of today's episode and then get back to talking about Google this episode of this week
[00:34:32.000 --> 00:34:34.000]   And Google is brought to you by Casper
[00:34:34.000 --> 00:34:40.080]   Casper is a sleep brand that I'm sure you've heard of Casper that continue to revolutionize
[00:34:40.720 --> 00:34:44.200]   With all sorts of I mean their mattresses are amazing
[00:34:44.200 --> 00:34:47.280]   We have a Casper mattress in two of our three bedrooms
[00:34:47.280 --> 00:34:54.120]   We keep meaning to swap out the third one and no kidding our daughter whose room has the non
[00:34:54.120 --> 00:34:56.720]   Casper mattress never wants to sleep in a room
[00:34:56.720 --> 00:35:01.400]   She always wants to sleep in the other two rooms partially because kids do that they want to sleep with their parents
[00:35:01.400 --> 00:35:05.280]   But also because the other mattresses are way more comfortable
[00:35:05.280 --> 00:35:09.320]   And I know this because I've experienced her mattress and it's not that comfortable by comparison
[00:35:09.480 --> 00:35:13.080]   The online retailer of premium mattresses for a fraction of the cost
[00:35:13.080 --> 00:35:18.320]   Casper is revolutionizing the mattress industry by cutting the cost of dealing with
[00:35:18.320 --> 00:35:23.440]   Resellers and showrooms and then they're passing that savings directly on to the consumer
[00:35:23.440 --> 00:35:28.340]   Casper's mattress is an obsessively engineered mattress at a fair price
[00:35:28.340 --> 00:35:36.280]   Casper actually combines multiple supportive memory foams for sleep surface with just the right sink just the right bounce
[00:35:36.280 --> 00:35:42.000]   You're going to notice it immediately also has a breathable design that sleeps cool to help you regulate your temperature
[00:35:42.000 --> 00:35:43.880]   Throughout the night
[00:35:43.880 --> 00:35:50.360]   Casper mattress provides long lasting comfort and support and you can buy it super easy like anything else that you buy online
[00:35:50.360 --> 00:35:53.720]   You just go online order it. It's completely risk-free
[00:35:53.720 --> 00:35:59.280]   You know like I said, I've had a Casper mattress now for a couple of years now
[00:35:59.280 --> 00:36:02.840]   love going to bed on the Casper mattress and
[00:36:03.760 --> 00:36:08.960]   Yeah, we're gonna we're gonna be upgrading our third room as well to the Casper very soon Casper understands
[00:36:08.960 --> 00:36:11.880]   probably the most important part of this is that
[00:36:11.880 --> 00:36:17.000]   When you're trying out a mattress in like a sterile, you know mattress
[00:36:17.000 --> 00:36:19.680]   mattress store
[00:36:19.680 --> 00:36:25.360]   Environment you're not really getting the full picture you lay down on the mattress. You say oh, yeah, this is great
[00:36:25.360 --> 00:36:27.360]   That's what we did with our first daughters
[00:36:27.360 --> 00:36:33.560]   Our oldest daughters mattress when we bought it initially when in there lay down and all this feels really nice
[00:36:34.400 --> 00:36:37.360]   But really, you know you spend so much of your time on that mattress
[00:36:37.360 --> 00:36:42.840]   You're only really gonna get a good idea of how that mattress actually is when you bring it into the home
[00:36:42.840 --> 00:36:49.000]   You sleep on it you test it out over time and they don't really let you do that with this the mattresses that you purchase from the store
[00:36:49.000 --> 00:36:55.400]   Quite so easily Casper actually offers free delivery and painless returns within a 100 day period
[00:36:55.400 --> 00:37:01.960]   So you don't have the light down in the showroom you can just bring in that mattress put it where it's going to exist
[00:37:01.960 --> 00:37:07.200]   And you know where it's gonna be for forever because you're not gonna end up sending it back by by the way
[00:37:07.200 --> 00:37:08.800]   And you can sleep on it
[00:37:08.800 --> 00:37:13.380]   You can test it out over time and see what you think and if you really have to return it you got a hundred days
[00:37:13.380 --> 00:37:15.600]   But you're not gonna do that
[00:37:15.600 --> 00:37:20.760]   Casper mattress upholds the highest environmental product production standards made in the USA
[00:37:20.760 --> 00:37:27.720]   Free shipping and returns to the US Canada and the UK and Casper recently introduced the Casper wave mattress
[00:37:27.720 --> 00:37:33.760]   Which features a natural geometry support system and a new top layer for added comfort
[00:37:33.760 --> 00:37:36.600]   Get a Casper mattress today check it out for yourself
[00:37:36.600 --> 00:37:45.160]   You can actually save an additional $50 towards a mattress purchase by going to Casper calm slash twig and enter promo code twig
[00:37:45.160 --> 00:37:51.120]   TWIG at checkout you'll get that $50 towards that mattress purchase. That's Casper
[00:37:51.600 --> 00:37:56.760]   dot-com slash twig with promo code twig terms and conditions do apply
[00:37:56.760 --> 00:38:03.400]   But check it out Casper dot com slash twig and promo code twig and I think you'll be happy that you did
[00:38:03.400 --> 00:38:08.280]   Alright, let's see here. There's got to be more Google stuff here
[00:38:08.280 --> 00:38:10.840]   We kind of started to talk about it with the emoji
[00:38:10.840 --> 00:38:16.240]   But I don't even think of that I even mentioned that eight point one on Android the beta anyways
[00:38:16.720 --> 00:38:24.300]   Is rolling out? I think it's the final beta before the official release of 8.1 and one interesting feature of 8.1 that's coming
[00:38:24.300 --> 00:38:30.720]   With that update now with this new update. Well, there's a few things the neural networks API is on there
[00:38:30.720 --> 00:38:33.740]   And that's going to allow for better on device
[00:38:33.740 --> 00:38:37.280]   Machine learning frameworks
[00:38:37.280 --> 00:38:41.760]   Operations we've heard of tensor flow light on Android devices
[00:38:41.760 --> 00:38:45.680]   So this is going to kind of take stuff that Google's been doing a lot in the cloud and
[00:38:45.680 --> 00:38:52.280]   power that on the device and you can think of some really interesting things that that opens the doors for one of those
[00:38:52.280 --> 00:38:53.560]   I'll talk about in a second
[00:38:53.560 --> 00:38:58.920]   But then another thing which is the pixel visual core which is Google's very first
[00:38:58.920 --> 00:39:04.840]   processor that they developed that exists within the new pixel devices, so this is
[00:39:04.840 --> 00:39:09.280]   finally getting the chance to be activated and
[00:39:09.600 --> 00:39:14.960]   Essentially what it's going to when it's going to allow for at least in the short term is for third-party developers who?
[00:39:14.960 --> 00:39:16.760]   You know, let's say Instagram for example
[00:39:16.760 --> 00:39:22.800]   I think I've heard of like Instagram and Snapchat that the pictures that you take through those in-app cameras
[00:39:22.800 --> 00:39:28.840]   Aren't as good as the pictures that you could take if you did it on your device in the standard camera app on a pixel
[00:39:28.840 --> 00:39:32.000]   That's because they don't have the same
[00:39:32.000 --> 00:39:36.480]   Access to the camera API that Google is granting to its own camera
[00:39:36.480 --> 00:39:42.920]   Well, thanks to pixel visual core processing that Android camera API is now being shared to third parties
[00:39:42.920 --> 00:39:48.200]   So basically, you know as long as developers do a little bit of work to provide that compatibility
[00:39:48.200 --> 00:39:54.080]   All the pictures that you take depending on what app you're using are gonna look really nice and
[00:39:54.080 --> 00:39:56.600]   Who knows what else they're gonna use that?
[00:39:56.600 --> 00:40:03.880]   Dedicated chip for down the line, but I think it's I think it's cool that like Google's thinking about this kind of stuff moving
[00:40:04.080 --> 00:40:09.480]   moving some of that extra power dedicated devoted to a certain thing on to the phone and
[00:40:09.480 --> 00:40:11.120]   Maybe it you know
[00:40:11.120 --> 00:40:16.760]   Maybe it provides a hint of of what their you know what their direction is as they go forward with it with their devices
[00:40:16.760 --> 00:40:18.800]   Certainly helps them to stand out
[00:40:18.800 --> 00:40:26.400]   So they they kind of have to do this so I am I am a huge as you guys probably know on our sick of by now
[00:40:26.400 --> 00:40:28.640]   I am a huge fan of semiconductors
[00:40:28.640 --> 00:40:34.480]   And this is a trend that Apple probably started way back in the day with their motion co-processor
[00:40:34.480 --> 00:40:39.720]   I don't know if y'all remember that but the idea of offloading more dedicated tasks to a dedicated chip makes sense
[00:40:39.720 --> 00:40:41.320]   and
[00:40:41.320 --> 00:40:43.320]   then for Google's perspective
[00:40:43.320 --> 00:40:53.680]   Your hardware is becoming your differentiator on consumer devices once you achieve enough scale and everyone's following this Apple vertically in a graded model and
[00:40:55.320 --> 00:40:59.160]   That makes a lot of sense. It'll help this particular one doesn't help
[00:40:59.160 --> 00:41:03.760]   But if Google can continue making its own silicon for things it could help with you know
[00:41:03.760 --> 00:41:09.120]   Ensuring security over a longer time period of ownership of the device for example because they don't
[00:41:09.120 --> 00:41:13.520]   They don't have to work so much around the device maker schedule
[00:41:13.520 --> 00:41:16.120]   and
[00:41:16.120 --> 00:41:19.040]   It also allows them to build really awesome features
[00:41:19.480 --> 00:41:25.880]   That are software but needs certain kinds of hardware into the phone and give you something that another vendor can't provide
[00:41:25.880 --> 00:41:28.160]   So it makes a ton of sense
[00:41:28.160 --> 00:41:31.840]   Yeah, I agree and it seems obvious. That's the way Google's going
[00:41:31.840 --> 00:41:36.920]   You know they want to control more of the hardware and it makes sense. I mean I think that's smart for them
[00:41:36.920 --> 00:41:41.920]   Yeah, I mean not only the the pixel
[00:41:41.920 --> 00:41:44.280]   visual core which
[00:41:44.280 --> 00:41:47.640]   Has kind of a dedicated camera focus right now
[00:41:47.640 --> 00:41:51.880]   I guess it's entirely possible that they tap into that for other reasons down the line
[00:41:51.880 --> 00:41:56.640]   But I also am very intrigued maybe even a little more intrigued with the idea of
[00:41:56.640 --> 00:42:03.960]   Bringing this this TensorFlow light this artificial intelligence processing onto the device to pull it out of the cloud
[00:42:03.960 --> 00:42:11.120]   I have been so excited about this Jason. Yeah, I mean this is like I we're talking about this a little bit on on all about Android last night
[00:42:11.440 --> 00:42:18.680]   But I'm curious to see what this results in over the course of the next couple of years because I feel like by and large phones
[00:42:18.680 --> 00:42:20.280]   At this point, you know
[00:42:20.280 --> 00:42:24.600]   We've seen what we've seen and there've been little kind of eye-opening moments where you're like
[00:42:24.600 --> 00:42:26.080]   Oh, wow, I didn't know if phone could do that
[00:42:26.080 --> 00:42:32.440]   But it's really hard to surprise us nowadays where a lot of the really interesting stuff has been happening has been an artificial intelligence
[00:42:32.440 --> 00:42:36.520]   But much as that has had to happen in the cloud a couple of years ago, you know
[00:42:36.520 --> 00:42:40.560]   That changes to Google photos to process auto awesom and state, you know
[00:42:40.560 --> 00:42:45.760]   Sequence together photos to make a video and do do all these things that all happen in the cloud now
[00:42:45.760 --> 00:42:48.600]   It's happening on the device. You don't have to send, you know
[00:42:48.600 --> 00:42:52.720]   All of your stuff into the clouds and maybe there's a security or privacy benefit there
[00:42:52.720 --> 00:42:55.760]   But it results in things like like this story
[00:42:55.760 --> 00:43:02.640]   apparently some Google researchers plan to show off a new kind of new piece of research around
[00:43:02.640 --> 00:43:05.560]   protection built
[00:43:05.760 --> 00:43:07.760]   The option to
[00:43:07.760 --> 00:43:14.080]   Use the front-facing camera to see if I can explain this properly use the front-facing camera to detect whether there are two
[00:43:14.080 --> 00:43:21.760]   Sets of eyes looking at the phone at once so say you're on the train and you're checking your your phone your text messages or whatever
[00:43:21.760 --> 00:43:28.320]   You don't want someone looking over your shoulder using artificial intelligence or the machine learning kind of capabilities on the device
[00:43:28.320 --> 00:43:34.640]   Instead of having to send it up into the cloud it can analyze the data that's coming in from the front-facing camera and detect that
[00:43:34.640 --> 00:43:37.720]   There is another set of eyeballs that's actually pointed at the screen
[00:43:37.720 --> 00:43:41.920]   I think if you skip like a third of the way into here a Carson or maybe a half of the way
[00:43:41.920 --> 00:43:45.160]   It ends up showing a little bit of the interface
[00:43:45.160 --> 00:43:51.520]   But and then once that happens see it cuts right back and it shows you the pass through Cameron says oh they're looking
[00:43:51.520 --> 00:43:57.000]   You know hide the content and little things like this that I mean this is this is just a one example
[00:43:57.000 --> 00:44:02.760]   I don't know if this is necessarily, you know ready for prime time necessarily the way it is right now, but
[00:44:03.520 --> 00:44:11.200]   I'm sure we'll see more interesting kind of uses of that AI and that added smarts on the device to do new things that might actually
[00:44:11.200 --> 00:44:13.720]   Wow us and surprise us again. What's that hope so?
[00:44:13.720 --> 00:44:19.320]   So I think this will be great. You're right although the privacy element here is key
[00:44:19.320 --> 00:44:24.560]   So not any like having my phone look at me all the time and yeah, you know sending that scary
[00:44:24.560 --> 00:44:29.640]   But when we think about what's happening the next big phase in computing
[00:44:30.240 --> 00:44:37.280]   You know is contextual awareness and being having our devices derive context and exchange that information
[00:44:37.280 --> 00:44:41.000]   And that all has to happen at what we like to call the edge
[00:44:41.000 --> 00:44:42.600]   I don't know what else you want to call it
[00:44:42.600 --> 00:44:48.320]   So that you need like local processing for that and I think we're gonna see our phones become
[00:44:48.320 --> 00:44:54.320]   Like they are like the country like they became the controller for the smart home and people are like this kind of sucks
[00:44:54.320 --> 00:44:56.160]   But it's got a cool and then voice change things
[00:44:56.160 --> 00:45:01.960]   I think we're gonna come back to the phone being a hub to process contextual information
[00:45:01.960 --> 00:45:05.280]   So like hey Stacy's in her car right now
[00:45:05.280 --> 00:45:10.680]   I should only offer her certain options or you know not offer her other options
[00:45:10.680 --> 00:45:14.320]   So that's it would be learning how to do that and that's really awesome
[00:45:14.320 --> 00:45:20.840]   And I think there's there's a huge amount of potential to do stuff like that like I notice you're driving
[00:45:20.840 --> 00:45:23.520]   so I'll you know read out texts instead of
[00:45:24.240 --> 00:45:30.640]   Instead of making you pick up your phone the the problem is the more context you get the creepier it gets
[00:45:30.640 --> 00:45:34.680]   So the more your phone knows about you the more Google knows about you
[00:45:34.680 --> 00:45:39.680]   Where you are what you're doing what you look like who's standing behind you?
[00:45:39.680 --> 00:45:42.040]   What speed you're going?
[00:45:42.040 --> 00:45:47.520]   You know, there's I know there are people that are gonna get creeped out by it that the
[00:45:47.520 --> 00:45:49.800]   Report I don't know if it was on the lineup
[00:45:49.800 --> 00:45:54.720]   But that Google phones were basically collecting data about your location
[00:45:54.720 --> 00:45:59.360]   Even if you had location turned off and I mean all they were doing was collecting
[00:45:59.360 --> 00:46:04.520]   The location of a nearby cell tower presumably to make it easier to show you
[00:46:04.520 --> 00:46:10.200]   Things or to know where you are and that's and that's good like that being able to send you
[00:46:10.200 --> 00:46:16.880]   Location based things is good and and being able to reach you or whatever it was they were doing is good
[00:46:16.880 --> 00:46:19.440]   but it requires things that are gonna
[00:46:19.440 --> 00:46:22.200]   Make people uncomfortable. I think
[00:46:22.200 --> 00:46:25.680]   Yeah
[00:46:25.680 --> 00:46:32.480]   So yes, so apparently South Korean regulators are actually really kind of digging into the story that you're talking about
[00:46:32.480 --> 00:46:34.280]   It's against the law there
[00:46:34.280 --> 00:46:41.200]   Yeah, then I mean 80% of the market there is on Android and so when you consider and I think really around that
[00:46:41.200 --> 00:46:43.200]   That's like awareness from the consumer
[00:46:43.400 --> 00:46:50.640]   Level is that when I turn off location on you know that that location button in my settings on my Android phone
[00:46:50.640 --> 00:46:55.080]   I mean my expectation is that you're not doing anything with that location data anymore
[00:46:55.080 --> 00:46:59.200]   Right, so Google wasn't doing anything with it
[00:46:59.200 --> 00:47:04.520]   It was kind of an accidental setting and to even when you turn off your location settings
[00:47:04.520 --> 00:47:11.280]   The physical hardware in the carrier know where you are. Yes, so it makes sense that your operating system
[00:47:11.800 --> 00:47:16.960]   would also come into that data now they shouldn't if they
[00:47:16.960 --> 00:47:21.800]   The question then is should they be and what should they do with it?
[00:47:21.800 --> 00:47:25.840]   They certainly shouldn't store it if you've turned off your location services because that's an implicit
[00:47:25.840 --> 00:47:30.080]   Explicit kind of don't track me and Google wasn't storing it, right?
[00:47:30.080 --> 00:47:37.960]   Notivica to improve notifications and message delivery. Yeah, they said don't know what that means but
[00:47:38.800 --> 00:47:42.800]   Well, and they didn't ultimately use it either. I don't remember if they stored it or not. I
[00:47:42.800 --> 00:47:49.160]   Don't think it did. Yeah. No, Google says the data was not stored on its servers and they don't do it anymore
[00:47:49.160 --> 00:47:53.840]   But still right and yes, so that's that's something where I'm sure Google thought it was innocuous
[00:47:53.840 --> 00:47:57.360]   Maybe they didn't even know that was happening. They didn't store it
[00:47:57.360 --> 00:48:04.920]   No one's gonna pay any attention to those details all they're gonna think is Google's tracking me and my location when I told them not to
[00:48:04.920 --> 00:48:12.120]   Yeah, right and in the phone world. It's obvious that you have to know where the phone is because that's how some of its work
[00:48:12.120 --> 00:48:15.840]   So if you're if you're a technologist, you're like well
[00:48:15.840 --> 00:48:25.640]   Well, and so maybe people are comfortable with Verizon knowing so they can send a phone call but Google knowing
[00:48:25.640 --> 00:48:28.080]   I think it's a different is a different animal
[00:48:28.080 --> 00:48:34.600]   Yeah, inside Google. I'm saying the people who worked on that. I can surely see them thinking yeah, and you know, honestly
[00:48:34.600 --> 00:48:42.080]   I would rather have Google in many cases have that information than Verizon or AT&T which have in the past given
[00:48:42.080 --> 00:48:46.280]   The government that data cart like her blanche access to it. So
[00:48:46.280 --> 00:48:51.840]   Right, but that's how they find the killer on all those like mystery murder shows
[00:48:51.840 --> 00:48:55.040]   They always try angulate to sell signal
[00:48:55.040 --> 00:49:02.080]   Enhance we need that we need that so they can catch a serial killer every week. Yeah. Yeah for our entertainment
[00:49:03.520 --> 00:49:06.920]   And and kind of what you were talking about Stacy's far as
[00:49:06.920 --> 00:49:09.520]   artificial intelligence
[00:49:09.520 --> 00:49:14.240]   The power of that but also the privacy implications of having to send this Matthew
[00:49:14.240 --> 00:49:19.400]   I think you were touching on this as well having to send that up to a server in order for it to happen that reminded me of
[00:49:19.400 --> 00:49:26.920]   kind of I feel like we're ending we're we're ending up in a new phase with Google where they're realizing that too sure
[00:49:26.920 --> 00:49:30.920]   They stand to benefit from having that information, but there is a public pushback
[00:49:30.920 --> 00:49:35.680]   We heard about you know Google clips, which is still on a wait list. That was the AI
[00:49:35.680 --> 00:49:44.600]   empowered camera that they announced during their pickle launch and that has artificial all of its artificial intelligence
[00:49:44.600 --> 00:49:49.360]   Processing happening on the device none of it hits the servers
[00:49:49.360 --> 00:49:57.400]   I see Google potentially doing more of that on the phone as well so that you can do some of these really interesting things without feeling like
[00:49:58.120 --> 00:50:02.920]   Great in order to get this yet another really amazing feature. I had to send them even more
[00:50:02.920 --> 00:50:07.560]   Granular information about how I live my life and who I talk with instead
[00:50:07.560 --> 00:50:11.760]   Hopefully, you know, it stays on the device that that at least makes the consumer happy
[00:50:11.760 --> 00:50:15.240]   I don't know if that makes Google very happy because they stand to benefit from all that information
[00:50:15.240 --> 00:50:20.400]   But but I mean the trade-offs like I when Google photos when I started using it
[00:50:20.400 --> 00:50:26.560]   You know in a really intensive way like uploaded literally every photo I've ever taken in my entire life. Yeah
[00:50:26.720 --> 00:50:28.720]   I
[00:50:28.720 --> 00:50:34.840]   Instantly saw the benefits when I connected it to my to my Google account
[00:50:34.840 --> 00:50:39.580]   Then I was able to search those photos for my kids for
[00:50:39.580 --> 00:50:43.320]   You know the family cat or anything like any keyword
[00:50:43.320 --> 00:50:51.120]   Yep, and it was mind-boggling like this is the kind of stuff like Google Earth or or that you you think we're living in the future
[00:50:51.120 --> 00:50:56.400]   This is this is so useful and so amazing and all I had to do
[00:50:56.400 --> 00:51:01.120]   was give Google access to all of my photos and now it knows what all of my kids look like and
[00:51:01.120 --> 00:51:06.800]   and where I live and presumably all sorts of other things and so but
[00:51:06.800 --> 00:51:12.400]   And that's and that concerns me, but I'm also amazed by how useful it is. Yeah
[00:51:12.400 --> 00:51:17.760]   Yeah, and you have to kind of go back and forth and figure out where where you sit on that
[00:51:17.760 --> 00:51:21.400]   I use the the keyword search and photos all the time and every time
[00:51:21.640 --> 00:51:29.680]   I'm amazed at how accurate or how that one random photo from so many years ago that had you know a tree in it
[00:51:29.680 --> 00:51:34.360]   And so I do a search for tree and you know out of my thousands upon thousands of photos
[00:51:34.360 --> 00:51:39.440]   I'm able to find it in seconds. It's just like it that kind of makes you go. Okay
[00:51:39.440 --> 00:51:42.600]   I'm okay with it here have my stuff because that's super useful. I
[00:51:42.600 --> 00:51:49.080]   Will say it is useful, but I noticed when I was looking for a particular picture of my dog
[00:51:49.800 --> 00:51:54.600]   I actually did a search for dog and I know it's in there and Google did not find it
[00:51:54.600 --> 00:51:57.320]   And also so then I had to go back and look at all my photos
[00:51:57.320 --> 00:52:02.320]   And I noticed that it didn't pull up a couple dog photos where my dog still looks like my dog
[00:52:02.320 --> 00:52:08.080]   So there is also when you're doing something like that, especially if you have the number of photos that many of us have
[00:52:08.080 --> 00:52:10.160]   There's kind of like oh, this is so cool
[00:52:10.160 --> 00:52:14.480]   But what about all the trees that it didn't pull up that it didn't get yeah, yeah
[00:52:14.480 --> 00:52:17.960]   We made it. I mean that is one of the issues like we've all got
[00:52:18.560 --> 00:52:25.320]   Gigabytes worth of photos that we're never going to look at because how could you you literally wouldn't have time?
[00:52:25.320 --> 00:52:27.680]   And yet I don't want to find the ones
[00:52:27.680 --> 00:52:33.200]   What I do I delete like I keep one one or two like
[00:52:33.200 --> 00:52:40.640]   Like photos of an event so for Christmas 2017 there will be like four photos max
[00:52:40.640 --> 00:52:43.600]   Wow, I am weird. Do you do that right afterwards?
[00:52:44.240 --> 00:52:49.800]   I usually just go through after and I don't take a lot of photos all the time. You pick one or two. Yeah
[00:52:49.800 --> 00:52:52.200]   Because otherwise it's just crap
[00:52:52.200 --> 00:52:59.480]   It is but you're so you're so disciplined like I know I literally have a thousand photos of every
[00:52:59.480 --> 00:53:05.080]   Event or everything I've been to or everywhere I go because I don't know which one is gonna be good
[00:53:05.080 --> 00:53:08.920]   And then I don't have the discipline to delete them. So now I have
[00:53:08.920 --> 00:53:10.880]   thousands of
[00:53:10.880 --> 00:53:18.640]   Quasai half decent photos and that's what you know a few good ones that I can't find right delete the worst photos
[00:53:18.640 --> 00:53:26.040]   Automatically delete things with fingers in them or things like that, but yeah, well, I'm not on Facebook
[00:53:26.040 --> 00:53:29.720]   So maybe that's where I get all my time to delete pictures and I don't go for babies
[00:53:29.720 --> 00:53:32.960]   I mean, yeah, that kind of manicuring your
[00:53:32.960 --> 00:53:39.240]   Your photo reel like that that takes a lot of time like I've tried to do that too at a certain point
[00:53:39.240 --> 00:53:41.240]   I just gave up and figured
[00:53:41.240 --> 00:53:47.720]   Photo search is good enough if I need to find something I will Google's making more enhancements to you know
[00:53:47.720 --> 00:53:50.880]   Help you clean up bad photos or duplicate photos or whatever
[00:53:50.880 --> 00:53:55.360]   It's not 100% but it's something and you know, I don't even really use those
[00:53:55.360 --> 00:54:02.360]   I just let it all panorama features and stuff like that are really great like the auto awesome stuff. Yeah, sometimes having all those photos
[00:54:02.360 --> 00:54:05.040]   They're able to do some cool stuff really to that's true
[00:54:05.040 --> 00:54:11.720]   I took a picture just the other night and I took three pictures in a row of a truck and it's now like a little gift
[00:54:11.720 --> 00:54:13.720]   It just moved right across the
[00:54:13.720 --> 00:54:17.720]   And you can frame that on the note no you can't frame that
[00:54:17.720 --> 00:54:20.880]   But you'll have it forever
[00:54:20.880 --> 00:54:27.320]   What else what we're talking about photos we can talk a little bit about lens lens is the
[00:54:27.320 --> 00:54:32.800]   Capability that we heard about at Google I/O that I think actually ended up in Google photos
[00:54:33.520 --> 00:54:40.480]   Initially where it can analyze a photo or analyze something on the screen and tell you what it's looking at
[00:54:40.480 --> 00:54:45.280]   So if you happen to take for whatever reason a picture of a box of cap and crunch cereal
[00:54:45.280 --> 00:54:49.120]   You know you run that through lens and it will know that it's cap and crunch
[00:54:49.120 --> 00:54:52.400]   You know give you a link to buy or give you a Wikipedia or whatever
[00:54:52.400 --> 00:54:58.040]   It's basically like now on tap but for any you know any of your pictures or whatever
[00:54:58.040 --> 00:55:00.920]   Is it because I don't have it on mine yet, and I just did the update
[00:55:01.760 --> 00:55:05.080]   So okay, so first of all initially you could find it in photos
[00:55:05.080 --> 00:55:11.520]   I think that's for all devices or maybe it was a pixel only thing it is okay because I'm on a one plus five T
[00:55:11.520 --> 00:55:14.260]   Oh, I just found it and I don't
[00:55:14.260 --> 00:55:23.000]   Okay, so you have it so you have it on your on your pixel in the so you have it looks like you have to take a photo
[00:55:23.000 --> 00:55:25.680]   Though you can't just do it from the camera well
[00:55:25.680 --> 00:55:30.760]   So yeah initially it was just in the photos app now is starting to roll out to Google Assistant
[00:55:31.080 --> 00:55:34.520]   So you can use it and it will open up the camera and
[00:55:34.520 --> 00:55:38.280]   Oh, I see it. Oh hey nice
[00:55:38.280 --> 00:55:41.080]   lucky
[00:55:41.080 --> 00:55:43.800]   All right now, let's see this is pixel only
[00:55:43.800 --> 00:55:49.320]   This is pixel sorry plus poor galaxy S7 owners are left out in the cold
[00:55:49.320 --> 00:55:55.560]   Wow, wow, wow, well, I've been using the one plus five T and I need to switch back over. Oh
[00:55:56.400 --> 00:56:02.840]   Yeah, I actually really do the one plus five T of course is the the
[00:56:02.840 --> 00:56:09.400]   Recent update that one plus did on their phone and I'm liking it a lot. It's a really great upgrade
[00:56:09.400 --> 00:56:12.400]   They move the fingerprint sensor to the back which is nice
[00:56:12.400 --> 00:56:20.560]   Because that that frees up more space on the display so it's a little bit more bezel lists or bezel reduced on the on the bottom and top
[00:56:20.560 --> 00:56:24.360]   You can see a little sliver there used to be a fingerprint sensor down there
[00:56:24.840 --> 00:56:31.320]   But yeah, it's a nice looking phone. This is a really zoomed in shot. So I'll just show it sideways, but
[00:56:31.320 --> 00:56:34.120]   So yeah, so I've been using that
[00:56:34.120 --> 00:56:40.200]   But I got to go back to my pixel to excel and I think I'm about ready to go back to the pixel to excel at this point
[00:56:40.200 --> 00:56:43.120]   But for 500 bucks you kind of can't go wrong with
[00:56:43.120 --> 00:56:53.440]   With the price. Yeah, they do the the reasonably priced high, you know production value device is really well. I think
[00:56:54.520 --> 00:56:56.200]   But I'm interested in the lens
[00:56:56.200 --> 00:57:02.280]   I was just gonna say that that to me like the first I think Google goggles was the first time I had that
[00:57:02.280 --> 00:57:06.440]   kind of functionality in a phone on the Nexus. I think it was and and
[00:57:06.440 --> 00:57:11.440]   Even though it didn't actually work sometimes it would just
[00:57:11.440 --> 00:57:16.520]   Crap out and say goggles has quent or something when it did it was really useful
[00:57:16.520 --> 00:57:21.800]   Like especially when I was traveling and I would take a picture of something and be like oh, it's a cool pile of rocks
[00:57:21.800 --> 00:57:27.200]   And then goggles would be like oh, that's uh, this incredibly historically significant thing
[00:57:27.200 --> 00:57:34.560]   It would tell me what it was and I actually found that really useful. Yeah. Yeah goggles was cool for its time now
[00:57:34.560 --> 00:57:39.200]   They've really I mean, it's just crazy how far that functionality has come
[00:57:39.200 --> 00:57:44.480]   Then it was kind of a unique thing that you could do here and there and not necessarily depend on but now
[00:57:44.480 --> 00:57:47.920]   I mean, it's pretty you know, it's pretty dependable
[00:57:48.920 --> 00:57:52.680]   Everything that all the changes that they've made it's nice to see that it's it's lasted
[00:57:52.680 --> 00:57:58.640]   Let's see here what else is there
[00:57:58.640 --> 00:58:04.720]   Well, this is just patent news and it's not even really news actually what I do when I dug into this
[00:58:04.720 --> 00:58:08.380]   I was like wait a minute. This is a patent from 2013, but apparently Google
[00:58:08.380 --> 00:58:17.120]   Might someday bring motorized lid to the pixel book. So the thought that's what I thought so it opens and closes on its own
[00:58:17.120 --> 00:58:20.680]   Uh-huh, so you just tap the case and it opens and
[00:58:20.680 --> 00:58:25.300]   Like the trunk on my Tesla. Yes. Yeah, exactly. It's like the Tesla book
[00:58:25.300 --> 00:58:30.000]   And it would open to just the right angle
[00:58:30.000 --> 00:58:36.880]   So it used the camera to know where your face is and open to the writing. Do we really need help opening our laptops?
[00:58:36.880 --> 00:58:45.960]   I like the the writing and the verge piece speaks to a magical world where laptop users are freed from the tyranny of having to engage in
[00:58:45.960 --> 00:58:47.960]   physical effort to use their
[00:58:47.960 --> 00:58:50.360]   At all. Yes
[00:58:50.360 --> 00:58:54.520]   Like I said, this is a pad filed in 2013
[00:58:54.520 --> 00:58:59.760]   So that kind of if they haven't done anything with that yet, yeah, maybe they will at some point
[00:58:59.760 --> 00:59:02.920]   I don't actually the drawing looks like a computer from
[00:59:02.920 --> 00:59:09.040]   1995 it looks like an old, you know, think pad or something. Yeah
[00:59:09.040 --> 00:59:14.080]   So do we want to talk about Facebook's new capture test?
[00:59:14.920 --> 00:59:16.920]   Yeah, yeah, absolutely
[00:59:16.920 --> 00:59:20.080]   Let's see here. So
[00:59:20.080 --> 00:59:27.880]   Apparently your face. Yeah, you upload your face. So you try see try and log into your Facebook account and
[00:59:27.880 --> 00:59:30.720]   They really want your pictures. Yes
[00:59:30.720 --> 00:59:37.400]   Apparently they won't let you in until you send them a picture of your face and of course they're doing it to
[00:59:37.400 --> 00:59:43.720]   You know prevent spamming bots and all that sort of stuff. They want to make sure you're not a robot
[00:59:44.440 --> 00:59:51.320]   I think it's pretty smart. I mean, I don't care like Facebook presumably has thousands of photos of my face already
[00:59:51.320 --> 00:59:53.840]   Because I put photos on there actually think that's true
[00:59:53.840 --> 00:59:56.360]   It's a good not every effort
[00:59:56.360 --> 00:59:59.960]   No, and so they're in there promising to delete it
[00:59:59.960 --> 01:00:04.560]   But Facebook's promises are worth like like Twitter or a brainstorm
[01:00:04.560 --> 01:00:11.160]   So I mean, I'm just saying like I don't like I don't put pictures
[01:00:11.160 --> 01:00:17.080]   I put some pictures or people put pictures of me, but like this just feels like you know
[01:00:17.080 --> 01:00:19.520]   What if we had an awesome shot of everybody's face?
[01:00:19.520 --> 01:00:22.560]   What we do this time they're really gonna delete it
[01:00:22.560 --> 01:00:25.320]   Bull because time for sure
[01:00:25.320 --> 01:00:34.440]   One it is impossible the way Facebook is set up to permanently delete anything from their servers because it's scattered all across the world
[01:00:34.440 --> 01:00:36.700]   All of them. Well, yeah all across the world actually
[01:00:40.700 --> 01:00:42.940]   So unless they figure you'll change your mind
[01:00:42.940 --> 01:00:44.140]   Yeah
[01:00:44.140 --> 01:00:49.180]   So unless they're putting it in a dedicated server that all of this stuff is going to and then they're freaking like
[01:00:49.180 --> 01:00:55.360]   magnetizing that server and it never touches or goes across anything else and they're dedicated places all
[01:00:55.360 --> 01:00:59.680]   Over the world for this that just get dumped. It's not it's a lie
[01:00:59.680 --> 01:01:07.240]   So just think of how useful that could be for like the NSA or you know Homeland Security. Yes
[01:01:08.940 --> 01:01:14.600]   I mean, it's in and so I'm I'm part of this is kind of a ship that sailed because like
[01:01:14.600 --> 01:01:20.880]   There is a face database for everybody who's got who's state licenses are
[01:01:20.880 --> 01:01:24.480]   Yeah, they have the new licenses
[01:01:24.480 --> 01:01:28.460]   so, you know, these already exist in
[01:01:28.460 --> 01:01:36.560]   Most people do put pictures on Facebook. So Facebook has this but there's definitely people who like if if you're an abused spouse
[01:01:36.640 --> 01:01:40.740]   You're running away from someone those kind of things. You don't want that information
[01:01:40.740 --> 01:01:48.220]   available accessible to anyone really I agree and I think the you know the biggest issue with
[01:01:48.220 --> 01:01:53.800]   Facebook on that score as opposed to say Twitter is Facebook continually is
[01:01:53.800 --> 01:02:01.540]   Trying to get you to reveal as much information about your real self as possible use your real name
[01:02:02.100 --> 01:02:07.440]   Upload a photo identify yourself if you want to be verified you have to submit a document
[01:02:07.440 --> 01:02:12.200]   And all of that can can backfire in a whole bunch of different ways
[01:02:12.200 --> 01:02:19.160]   I was interested in the at the congressional hearings. I forget if it was the Senate or or
[01:02:19.160 --> 01:02:21.720]   the House, but someone
[01:02:21.720 --> 01:02:26.600]   kept going after Twitter and effectively suggesting that allowing anonymity was
[01:02:26.600 --> 01:02:29.760]   Was wrong what that people shouldn't be allowed?
[01:02:30.360 --> 01:02:33.520]   To be online and do things and say things
[01:02:33.520 --> 01:02:39.720]   anonymously, which is kind of a creepy I mean, yes, there's downside stand in the many but
[01:02:39.720 --> 01:02:43.800]   The idea that everybody has to identify themselves at all times
[01:02:43.800 --> 01:02:48.800]   Can be very very dangerous for certain people in certain situations absolutely
[01:02:48.800 --> 01:02:50.760]   do you
[01:02:50.760 --> 01:02:54.280]   Is that a direction that you think things are are heading in?
[01:02:54.280 --> 01:02:59.280]   I mean, can you see can you foresee a future where the Internet is so locked down compared to today that
[01:02:59.520 --> 01:03:01.520]   That's a requirement. I don't know how that's possible
[01:03:01.520 --> 01:03:07.720]   No, but I know that that there are definitely people pushing for that to happen and it's you know
[01:03:07.720 --> 01:03:15.840]   Certainly the Senate people in the Senate and and the House are among those people and they're because the belief is that anonymity is
[01:03:15.840 --> 01:03:18.000]   what causes
[01:03:18.000 --> 01:03:23.880]   The problems so right it's what makes trolls do what they do. It's what makes it hard to track
[01:03:23.880 --> 01:03:26.320]   You know whether that was a Russian ad or not
[01:03:27.360 --> 01:03:30.880]   but I think there's there's some significant downsides to
[01:03:30.880 --> 01:03:36.840]   To requiring that everybody you know be their authentic selves or whatever
[01:03:36.840 --> 01:03:42.640]   Right, and that's I mean if you want to stay anonymous. I think it's probably
[01:03:42.640 --> 01:03:47.840]   Well, no, I'm not even gonna go there. I don't think about no
[01:03:47.840 --> 01:03:51.840]   I'm just like oh the mental work to think about anonymity and getting people
[01:03:52.200 --> 01:03:58.400]   Making it easy enough for people who have a legitimate reason to stay anonymous to still participate
[01:03:58.400 --> 01:04:00.600]   while also
[01:04:00.600 --> 01:04:03.240]   Like people will perform certain hurdles
[01:04:03.240 --> 01:04:08.600]   Maybe sending a photo isn't the right hurdle because it's it's smacks it
[01:04:08.600 --> 01:04:14.560]   But you can get a real person who wants to participate to do some things that a bot wouldn't do so
[01:04:14.560 --> 01:04:20.960]   The question is figuring out how to do that and as computers get better. That's just gonna be harder and harder. So
[01:04:21.440 --> 01:04:24.000]   Right, well the bots are getting smarter and smarter
[01:04:24.000 --> 01:04:33.520]   Reading things clicking things filling in forms, you know, they're more human than lots of the people I know who are online. Oh
[01:04:33.520 --> 01:04:36.760]   And that brings up a really cool article
[01:04:36.760 --> 01:04:40.600]   And this is not on the rundown, but I'll put it on the rundown. Oh
[01:04:40.600 --> 01:04:44.320]   No, it lost. Okay. It's from fast code design
[01:04:44.320 --> 01:04:48.840]   Copy I'm pasting hold on you guys. I don't know where to
[01:04:50.040 --> 01:04:54.080]   And other so this was an awesome article that talked about
[01:04:54.080 --> 01:04:58.220]   Oh, I see why it's not
[01:04:58.220 --> 01:05:01.480]   Oh
[01:05:01.480 --> 01:05:08.000]   Clean it up there you go. So click that it's about how the future of user interfaces is
[01:05:08.000 --> 01:05:16.160]   Going to be designing for a mix of humans and ais so bots plus people and that's kind of what we're talking about
[01:05:16.800 --> 01:05:21.080]   So this is a really fun article that kind of ties into some of this stuff
[01:05:21.080 --> 01:05:27.920]   And I thought it was actually pretty thought provoking myself. So we did it. Well, and I actually think we're probably doing that already
[01:05:27.920 --> 01:05:34.080]   I mean people at Google. I'm sure are designing for for bots or automated
[01:05:34.080 --> 01:05:40.360]   Processes almost as much as they're designing for human beings the the difficult part is when
[01:05:40.360 --> 01:05:46.400]   So I don't you guys probably saw the YouTube video stories about the creepy
[01:05:47.040 --> 01:05:50.600]   videos and so on and yeah, a lot of them are
[01:05:50.600 --> 01:05:52.720]   are effectively
[01:05:52.720 --> 01:05:54.720]   produced or generated
[01:05:54.720 --> 01:05:59.780]   solely to fit a kind of algorithmic or or kind of
[01:05:59.780 --> 01:06:02.480]   facilitate search
[01:06:02.480 --> 01:06:08.560]   keywords, you know combinations of things that that algorithms have determined are
[01:06:08.560 --> 01:06:14.520]   interesting or will surface easily those are that's that's content created by
[01:06:15.200 --> 01:06:20.120]   Machines for machines and then people see them and they're like, oh my god. This is horrible
[01:06:20.120 --> 01:06:21.960]   and
[01:06:21.960 --> 01:06:23.960]   Yet, I'm sure the machines don't care
[01:06:23.960 --> 01:06:28.200]   and and some of it might actually turn out to be
[01:06:28.200 --> 01:06:34.400]   Not useful but creative or interesting and then some of it is just going to be bizarre and disturbing
[01:06:34.400 --> 01:06:43.260]   Yeah, yeah machine see that in the in the way of a you know a recommend recommended next thing to watch or maybe it
[01:06:43.260 --> 01:06:47.080]   Follow you know exactly but and then the challenge of course then becomes you know
[01:06:47.080 --> 01:06:50.540]   This story has been bubbling up for a while, you know YouTube kids
[01:06:50.540 --> 01:06:56.980]   Which is an app that my kids are very familiar with just present presenting the next video
[01:06:56.980 --> 01:07:04.220]   To someone who you know to my children is one example who aren't going to question the next video
[01:07:04.220 --> 01:07:09.280]   They're just going to follow along and be along for the ride and end up being exposed to these really crazy videos
[01:07:09.280 --> 01:07:15.960]   Yeah, and I'm so conflicted on YouTube right now all these this continuing outpouring of of stories about you know
[01:07:15.960 --> 01:07:17.960]   These these horrible
[01:07:17.960 --> 01:07:21.780]   highly offensive and objectionable you know videos
[01:07:21.780 --> 01:07:29.020]   involving children targeted at children I mean I always knew YouTube was it was basically a minefield
[01:07:29.020 --> 01:07:33.920]   You know between the comments and the amount of content that's being uploaded to it
[01:07:33.920 --> 01:07:39.000]   I just feel like YouTube we're seeing how YouTube is kind of buckling under its own weight
[01:07:39.400 --> 01:07:46.200]   There's so much video being uploaded. How do you possibly police at all and in a way Google or YouTube is very much like Facebook
[01:07:46.200 --> 01:07:49.520]   In that sense, let's say it's all user-generated content or most of it
[01:07:49.520 --> 01:07:52.920]   It's there's massive volumes like the volume is
[01:07:52.920 --> 01:07:56.320]   Almost impossible to contemplate yeah
[01:07:56.320 --> 01:08:03.360]   Or to understand on a sort of human level the amount of content that gets uploaded and so and yet algorithms fail
[01:08:04.720 --> 01:08:08.400]   spectacularly at sifting through a lot of that because they don't
[01:08:08.400 --> 01:08:15.680]   Have the kind of human context that human beings do you can you and I can look at a video and say well
[01:08:15.680 --> 01:08:19.400]   That's just creepy people shouldn't watch that algorithms are like
[01:08:19.400 --> 01:08:21.680]   it has
[01:08:21.680 --> 01:08:22.840]   sex and
[01:08:22.840 --> 01:08:29.500]   bananas and robots and it's funny and you like funny things with bananas and robots and so you know what I mean like it
[01:08:29.500 --> 01:08:36.740]   meets the criteria, but you and I would both it would trigger something and be like no one wants to watch that or that's bad
[01:08:36.740 --> 01:08:43.380]   We're not there yet where algorithms can kind of make those kind of decisions reliably is this comment
[01:08:43.380 --> 01:08:46.260]   trolling or is it acceptable is this
[01:08:46.260 --> 01:08:51.220]   video offensive or is it creative or is it interesting?
[01:08:51.220 --> 01:08:58.660]   Yeah, it's like challenge. I'm sorry. Go. No, I was gonna say and the challenge with kids stuff less so with more adults
[01:08:58.660 --> 01:09:06.700]   But I don't know if kids can see one or one image and have it stick with them for like the rest of their life
[01:09:06.700 --> 01:09:11.100]   Like I vivid vividly remember walking in on my parents watching
[01:09:11.100 --> 01:09:14.540]   Star Trek the wrath of Khan and the scene where they stick to the bug
[01:09:14.540 --> 01:09:23.740]   Not that scene that scene was the only thing I saw for like months afterwards, but I still vividly remember I mean like
[01:09:23.740 --> 01:09:26.740]   Oh, man
[01:09:27.340 --> 01:09:29.340]   Young yeah
[01:09:29.340 --> 01:09:37.700]   That traumatized me in ways that like I mean, I'm clearly a self-actualized person, but yeah, it stuck with me
[01:09:37.700 --> 01:09:44.540]   Yeah, but I wonder about I wonder someone actually mentioned this while the YouTube thing was going on and it made me think I
[01:09:44.540 --> 01:09:46.900]   Mean some of these videos are clearly
[01:09:46.900 --> 01:09:53.460]   Over the top or in and go into dark areas and you you know all your kids watching them, but there are also
[01:09:53.460 --> 01:09:56.500]   there probably is room for
[01:09:56.940 --> 01:09:58.260]   auto-generated
[01:09:58.260 --> 01:10:05.380]   Creativity of some kind where things are merged together or blended together and you get something new and you and sometimes it is creepy or weird
[01:10:05.380 --> 01:10:09.260]   but bugs bunny cartoons were pretty creepy and weird and you know
[01:10:09.260 --> 01:10:15.980]   I grew up on those some of the people who designed those were probably on drugs or they were very strange individuals
[01:10:15.980 --> 01:10:16.980]   some of those
[01:10:16.980 --> 01:10:22.780]   Episodes where you look back at them now and you're like that is like way out there and
[01:10:22.780 --> 01:10:26.220]   But I mean it was also
[01:10:27.100 --> 01:10:33.460]   Funny and so there there probably are things that are in that gray area and I wouldn't want to say
[01:10:33.460 --> 01:10:40.740]   You know, I don't want my kids to watch any YouTube videos or I only want them to watch this channel and that channel
[01:10:40.740 --> 01:10:47.540]   Maybe there are things that they would find funny or interesting that I wouldn't or that I would never pick for them
[01:10:47.540 --> 01:10:56.140]   I imagine a world where an algorithmically generated motion picture wins best movie of the year at the Academy Awards
[01:10:56.620 --> 01:11:01.060]   Not entirely outside the realm of possibility, right? What's these computers get smart enough?
[01:11:01.060 --> 01:11:03.780]   They could absolutely create a narrative. That's easy to follow
[01:11:03.780 --> 01:11:08.700]   Just by analyzing everything that's already out there. I mean AI is
[01:11:08.700 --> 01:11:14.620]   There is music being created by computers entirely and journalism is being written
[01:11:14.620 --> 01:11:17.580]   Journalist is being written great great example
[01:11:17.580 --> 01:11:23.180]   They're they're processing imagery and yes, it ends up looking very kind of psychedelic or whatever
[01:11:23.180 --> 01:11:29.100]   But if a human had had processed that or created that someone would be like wow, that's really creative. That's interesting
[01:11:29.100 --> 01:11:30.620]   I haven't seen that before
[01:11:30.620 --> 01:11:35.060]   Think about it. We're like in we're in the first inning not even in the first in yeah
[01:11:35.060 --> 01:11:40.420]   I mean the speed with which this is going to improve is going to be mind-boggling and so
[01:11:40.420 --> 01:11:45.860]   Five years from now we could be seeing things that like now it's sort of Stone Age
[01:11:45.860 --> 01:11:50.900]   You know and it's very crude and you think yeah computers aren't that good at that
[01:11:51.060 --> 01:11:55.620]   But they've gotten pretty good at some other things. They weren't very good at either. So
[01:11:55.620 --> 01:11:58.700]   Absolutely
[01:11:58.700 --> 01:11:59.620]   Wow, okay
[01:11:59.620 --> 01:12:02.860]   I meant to I meant to take a break for the sponsor about 10 minutes ago
[01:12:02.860 --> 01:12:07.700]   But I didn't want to interrupt any of that so we're gonna thank the sponsor then let's talk a little bit about Amazon and
[01:12:07.700 --> 01:12:10.700]   Kind of the sales that happened
[01:12:10.700 --> 01:12:16.260]   I'm sure some of you out there watching listening maybe both of you that that here on the show today
[01:12:16.780 --> 01:12:20.340]   Bots some stuff. I didn't buy anything really this year. We'll talk about that anyways
[01:12:20.340 --> 01:12:24.140]   But I want to thank the sponsor of this episode and that is
[01:12:24.140 --> 01:12:28.780]   Lighthouse sponsoring this week in Google lighthouse is more than a security camera
[01:12:28.780 --> 01:12:32.900]   It's actually really tall security camera. This thing is really cool. It's an interactive
[01:12:32.900 --> 01:12:40.180]   Security assistant and it's powered by the same technology that you find that you would find in self-driving cars
[01:12:40.180 --> 01:12:42.300]   There are 3d time of flight sensor
[01:12:43.260 --> 01:12:49.900]   It's a game changer for people who are looking for the best security camera a lot of people are doing that in this world of the Internet of things
[01:12:49.900 --> 01:12:54.380]   This is just an excellent security camera at the top of the device
[01:12:54.380 --> 01:12:58.260]   Lighthouse actually emits invisible light into the room that it's in
[01:12:58.260 --> 01:13:04.460]   The 3d sensor measures the time that it takes for the light to bounce off objects to create a 3d
[01:13:04.460 --> 01:13:08.320]   Structure of the room and then using advanced AI
[01:13:08.740 --> 01:13:13.460]   Lighthouse continuously analyzes the 3d structure to detect actual movement
[01:13:13.460 --> 01:13:20.100]   To know whether it's a person pet an object whatever it happens to be that's within its view
[01:13:20.100 --> 01:13:28.580]   And unlike other cameras you you won't get security alerts every time something moves like it doesn't just let you know every time there's movement
[01:13:28.580 --> 01:13:35.540]   So because it can identify this stuff. So there's no more alerts for pets or sunlight shadows
[01:13:35.540 --> 01:13:41.860]   It recognizes that thanks to the AI that's happening on the device and kind of doesn't show those things to you
[01:13:41.860 --> 01:13:48.580]   But it shows you the important things lighthouse learns who you know and who you don't and to eliminate false security alerts
[01:13:48.580 --> 01:13:53.460]   Lighthouse lets you invite people to the app maybe family members the dog walker the nanny
[01:13:53.460 --> 01:13:59.900]   House cleaners whoever while restricting access to your video giving you total privacy
[01:13:59.900 --> 01:14:03.740]   And when you're alerted to a possible security threat you can sound an alarm
[01:14:04.060 --> 01:14:08.300]   You can speak through the camera or call 911 directly from the app
[01:14:08.300 --> 01:14:14.460]   It's basically it's like the search engine for your life at home when you want to know what happened
[01:14:14.460 --> 01:14:20.100]   You can ask lighthouse and it'll actually show you what did the kids do while I was out yesterday?
[01:14:20.100 --> 01:14:28.420]   If you don't see Michael by 4 p.m. On weekdays, let me know kids can wave to say hello and open up two-way talk
[01:14:28.420 --> 01:14:31.580]   So you're always in contact with your children when they're home
[01:14:31.700 --> 01:14:38.020]   It's got everything you want in the security camera 1080p live stream two-way talk night vision
[01:14:38.020 --> 01:14:40.580]   30-day video history and
[01:14:40.580 --> 01:14:46.660]   Three different permission settings with the option to turn off the camera when the owner is home
[01:14:46.660 --> 01:14:53.220]   Super smart lighthouse has an exclusive offer just for twit fans just in time for the holiday season visit
[01:14:53.860 --> 01:15:02.100]   www.light.house/twit and you'll get 40% off your very own lighthouse with the lighthouse intelligence
[01:15:02.100 --> 01:15:08.000]   Service plan right now you can't even purchase lighthouse on their main website lighthouse has been sold out of
[01:15:08.000 --> 01:15:16.260]   Pre-orders, but they're building more lighthouse cameras as we speak. They've set aside a special allotment just for you just for twit fans
[01:15:16.260 --> 01:15:23.260]   So go to www.light.house/twit to get 40% off
[01:15:23.260 --> 01:15:30.340]   It's a total savings of up to 200 dollars and you won't want to wait this offer is only good while supplies last
[01:15:30.340 --> 01:15:35.260]   if you act now you'll get it in time for the holidays it's a great holiday gift and
[01:15:35.260 --> 01:15:45.380]   We think you'll like it. So we thank lighthouse for their support of this week in Google. That's www.light.house/twit for 40% off
[01:15:45.380 --> 01:15:51.940]   So first and foremost we should all know that Jeff Bezos is ridiculously rich
[01:15:52.940 --> 01:16:00.220]   He topped 100.3 billion dollars over the weekend as Amazon comm rose 2% last Friday
[01:16:00.220 --> 01:16:06.740]   Thanks to black Friday sales, of course. So his net worth tops 100 billion dollars
[01:16:06.740 --> 01:16:12.540]   We can all strive to be third or second place behind Bezos
[01:16:12.540 --> 01:16:15.260]   someday, right
[01:16:15.260 --> 01:16:21.020]   We could be second place with a very small amount of money the way the world is you know working out
[01:16:21.340 --> 01:16:27.460]   How much how much in Bitcoin do we have to own in order to beat Bezos at this point not very much right?
[01:16:27.460 --> 01:16:32.420]   Not as many as I mean, no, we're at 11 now. I believe or we okay
[01:16:32.420 --> 01:16:40.340]   That is unbelievable. When does the bottom call out on that one? I had about a hundred at one point
[01:16:40.340 --> 01:16:45.660]   And then I got rid of them because they were worth pennies. I was like this is dumb
[01:16:46.860 --> 01:16:52.820]   Even for on how do you feel how does how does that feel because that sounds pretty painful?
[01:16:52.820 --> 01:16:56.140]   It feels a lot like not buying Apple when it was five dollars
[01:16:56.140 --> 01:17:02.820]   Pretty you suit. I mean that's the thing you don't know at the time. So it's yeah always hindsight, right?
[01:17:02.820 --> 01:17:08.460]   Like hindsight, but it does remind me that no matter how dumb something seems or no matter how much you think
[01:17:08.460 --> 01:17:12.140]   Company is gonna go bankrupt. Why not buy a couple hundred chairs anyway?
[01:17:12.500 --> 01:17:18.540]   You never know. Yeah, you might as well. You've got a couple hundred bucks to burn however much it is at the time
[01:17:18.540 --> 01:17:25.100]   Measurement the guy who the older guy who sold his Apple shares not long after
[01:17:25.100 --> 01:17:28.380]   Do you remember that story? I can't remember his name now
[01:17:28.380 --> 01:17:32.980]   But he was an older fellow and and he got 10% of the company
[01:17:32.980 --> 01:17:40.460]   And he sold it for 800 bucks or something and think that worked didn't we we did we interview him on triangulation?
[01:17:40.460 --> 01:17:45.940]   Wow, he was their first CEO. Oh, man. Somebody somebody in chats can remember his name
[01:17:45.940 --> 01:17:50.760]   And it's he thought it's just too risky and so you know, I might as well sell it and
[01:17:50.760 --> 01:17:56.780]   Someone there was a sign somewhere. It was probably photoshopped, but it said are you having a bad day?
[01:17:56.780 --> 01:18:04.060]   So-and-so if he had hung on to it, it'd be worth, you know, 300 billion dollars or whatever
[01:18:04.060 --> 01:18:09.420]   I don't know my 10% of Ronald Wayne is his name on all the way and that's right. Thank you, babe
[01:18:09.420 --> 01:18:12.580]   Bro, thank you, Blake and the best part is he doesn't regret it at all
[01:18:12.580 --> 01:18:15.500]   He said it was the right decision to make at the time, right?
[01:18:15.500 --> 01:18:21.060]   You don't mean to kick back. Yeah, you really don't know at the time if I'd have done this instead of that
[01:18:21.060 --> 01:18:22.540]   I'd be over there instead of here
[01:18:22.540 --> 01:18:28.260]   That's a dad-ism if I ever heard one my dad used to tell you that all the time. That's still done. I'm it
[01:18:28.260 --> 01:18:34.100]   I'm probably gonna tell it to my kids too. It's it's like a wherever you go. There you are. Yes, exactly
[01:18:34.100 --> 01:18:36.780]   So
[01:18:36.780 --> 01:18:43.820]   Yes, Black Friday then Cyber Monday was a big year Cyber Monday at least was the biggest shopping day ever
[01:18:43.820 --> 01:18:46.740]   For Amazon anyways
[01:18:46.740 --> 01:18:53.940]   They never give you numbers they just really big mush words around so it sounds like
[01:18:53.940 --> 01:19:01.940]   Really big really big even bigger than the last really big one than the last really yes, which was prime
[01:19:01.940 --> 01:19:08.980]   I want to know I want to know Ali Baba. I think singles day was 25 billion. Whoa, that was one day
[01:19:08.980 --> 01:19:13.300]   25 billion I think they did five billion in the first 15 minutes
[01:19:13.300 --> 01:19:19.660]   So maybe Amazon just doesn't want to be you know measure up. Yeah, yeah, that's right
[01:19:19.660 --> 01:19:25.900]   They have a much smaller market. Well, I guess globally. No, okay never mind. Yeah
[01:19:25.900 --> 01:19:31.180]   I was like wait. Nope. Nope. They sell stuff elsewhere in the world. Yeah, Ali Express
[01:19:32.140 --> 01:19:35.120]   I have not bought anything from there Florence. I am
[01:19:35.120 --> 01:19:43.060]   Big fan. I have bought stuff on Ali Express. I bought an Elsa costume for Halloween one year and it arrived and it was awesome
[01:19:43.060 --> 01:19:49.380]   It was good cheap cheap. I was gonna say cheap cheap quality. Well, I mean it was what it was needed to be
[01:19:49.380 --> 01:19:53.420]   Yeah, it was a Halloween. Yeah, one day. Yeah, you just need it for that one time
[01:19:53.420 --> 01:19:57.060]   You might as well get it as inexpensive as long as you're ordering far enough out
[01:19:57.060 --> 01:19:59.900]   Cyber Monday was the first
[01:20:00.100 --> 01:20:05.380]   Well saw 81 million Americans shopping online to the tune of two billion dollars in mobile shopping
[01:20:05.380 --> 01:20:11.180]   The most popular shopping app apparently was wish which I actually used wish this time
[01:20:11.180 --> 01:20:16.300]   I've never ordered from wish before but wish kind of reminds me of Ali Bob. It's kind of the similar
[01:20:16.300 --> 01:20:19.020]   Kind of approach never heard of it
[01:20:19.020 --> 01:20:23.580]   It's a similar type of marketplace. You find a lot of the same type of stuff on wish
[01:20:23.580 --> 01:20:28.220]   You got to be okay with the fact that it's gonna take 30 to 60 days to ship
[01:20:28.620 --> 01:20:32.820]   Okay, because you know a lot of it's coming from China and you know
[01:20:32.820 --> 01:20:36.540]   They take the slow route on and that's why it's ridiculously inexpensive
[01:20:36.540 --> 01:20:48.340]   So yeah, if you if you plan far enough in advance
[01:20:48.340 --> 01:20:50.420]   You can you can buy stuff for dirt cheap
[01:20:50.420 --> 01:20:51.980]   You just have to be able to wait for it
[01:20:51.980 --> 01:20:56.700]   And I'm guessing that cyber Monday or black Friday was not the right day to order from
[01:20:57.180 --> 01:21:01.380]   From wish because everything tastes more than 30 days and you're not gonna have it in time for Christmas
[01:21:01.380 --> 01:21:03.940]   I'm gonna have to say I didn't buy anything
[01:21:03.940 --> 01:21:09.780]   Like you I didn't I didn't participate in the cyber Monday or black Friday
[01:21:09.780 --> 01:21:14.260]   I mean, I'm Canadian so our Thanksgiving was some time ago, but we still have
[01:21:14.260 --> 01:21:18.020]   Cyber Monday and black Friday. I don't know why because why not
[01:21:18.020 --> 01:21:23.100]   Yeah, I mean the US is doing it. Why don't why shouldn't we do it?
[01:21:23.100 --> 01:21:29.260]   And there were some great sales, but I just to be honest, I don't I don't need that much stuff
[01:21:29.260 --> 01:21:31.260]   I have a lot of stuff already
[01:21:31.260 --> 01:21:39.220]   I thought about buying some USB drives like you can get 256 gig USB drives, which I find fascinating
[01:21:39.220 --> 01:21:41.780]   Wow, what would I do with it? I don't know, but it's pretty cool. I
[01:21:41.780 --> 01:21:49.820]   Got 256 gigs in my pocket and I am happy to see you. No, it was 40 bucks
[01:21:50.140 --> 01:21:54.380]   Like I love tech that yeah, I'm like thanks more saw
[01:21:54.380 --> 01:21:56.420]   Yeah, no kidding
[01:21:56.420 --> 01:22:02.940]   Mine boggling for anyone who's old enough to remember, you know, my first hard drive was 50 mags or something
[01:22:02.940 --> 01:22:08.580]   And I thought that was more data than anybody could possibly accumulate in their lifetime at the time it was
[01:22:08.580 --> 01:22:13.100]   But my how that changes that's crazy
[01:22:13.100 --> 01:22:17.820]   Did you guys read this article this mirror article was an investigative report
[01:22:18.940 --> 01:22:22.380]   reporter went to work for the Amazon
[01:22:22.380 --> 01:22:26.060]   Where have foreign Amazon warehouse in Essex and
[01:22:26.060 --> 01:22:28.420]   spent
[01:22:28.420 --> 01:22:33.540]   Five weeks working in the warehouse final shift was on black Friday and just kind of reported
[01:22:33.540 --> 01:22:36.260]   experience from this warehouse and
[01:22:36.260 --> 01:22:39.740]   Didn't sound like a very fun place to work
[01:22:39.740 --> 01:22:44.620]   You know, mother Jones did an article like that a couple years ago
[01:22:44.940 --> 01:22:51.900]   Very exact exact same experience. So this again makes sense. I mean what Amazon's trying to do
[01:22:51.900 --> 01:23:00.780]   Does it will exhaust people? Yeah, I will say I I worked in warehouse once long before the internet
[01:23:00.780 --> 01:23:04.260]   So there was no cyber Monday, but it's a terrible job
[01:23:04.260 --> 01:23:10.820]   I mean, there are there are probably warehouses that are nice and clean and where everyone's you know
[01:23:11.980 --> 01:23:20.620]   Abides by some code of but the warehouse I worked in was it was a nightmare. So I I don't I'm not you know
[01:23:20.620 --> 01:23:23.300]   absolving Amazon of any it's just
[01:23:23.300 --> 01:23:25.860]   those jobs are inherently
[01:23:25.860 --> 01:23:31.900]   Dredgery I mean repetitive drudgery. There's no way to kind of make them not that right?
[01:23:31.900 --> 01:23:38.340]   I think we could expect a certain amount of humanity, but I mean Amazon's got
[01:23:39.180 --> 01:23:43.420]   Just such massive volumes of stuff. I'm not an article
[01:23:43.420 --> 01:23:48.860]   I don't know if you guys read this a while back that looked at retirees who work in Amazon warehouses
[01:23:48.860 --> 01:23:56.740]   Because they're obviously they need the money or they're looking for something to do and they're traveling around their RVs
[01:23:56.740 --> 01:24:00.540]   so they the RVs park in the parking lot of the Amazon
[01:24:00.540 --> 01:24:03.980]   warehouse and and they work in the warehouse and
[01:24:03.980 --> 01:24:08.860]   it it was a slightly better story than and the
[01:24:08.940 --> 01:24:14.580]   Mirror one, but you know, it's hard work standing up for hours at a time. I think Amazon had
[01:24:14.580 --> 01:24:20.660]   Dispensers on the wall for like paying medication and and arthritis
[01:24:20.660 --> 01:24:23.940]   Medication and stuff that you could just help yourself to
[01:24:23.940 --> 01:24:27.660]   Yeah
[01:24:27.660 --> 01:24:30.540]   Tylenol some time at all
[01:24:30.540 --> 01:24:33.220]   Get it delivered
[01:24:33.540 --> 01:24:40.180]   Yeah, no, it's it sounds like so just just a little bit of insight into you know what the workers experience workers
[01:24:40.180 --> 01:24:45.700]   We're expected to stuff a package every 30 seconds. So I mean, this is you know, it's total
[01:24:45.700 --> 01:24:51.780]   When you're talking about the volume like you like you say Matthew when you're talking about the kind of volume and the service as Amazon
[01:24:51.780 --> 01:25:00.420]   Prime customers kind of expect this quick turnaround and you just think of the massive amount of products that are being ordered every second
[01:25:00.420 --> 01:25:04.300]   All that needs to be bought you know located boxed shipped out
[01:25:04.300 --> 01:25:10.580]   So stuffing a package every 30 seconds some workers are falling asleep while they were standing 10 10 hour shifts
[01:25:10.580 --> 01:25:15.620]   Standing they maybe get a couple of like 30 minute breaks. I think throughout that time
[01:25:15.620 --> 01:25:20.300]   We're told they could not sit down even when things slowed down
[01:25:20.300 --> 01:25:24.100]   They have a screen that shows them their units per hour
[01:25:24.660 --> 01:25:31.980]   How long each unit has taken so obviously they're further incentivized to keep the boat rolling here keep the ship rolling
[01:25:31.980 --> 01:25:39.460]   You know the reporter noticed a couple of colleagues were taken out on an ambulance from exhaustion
[01:25:39.460 --> 01:25:42.580]   Like it's just kind of it's crazy
[01:25:42.580 --> 01:25:48.580]   But I mean like like you say is that more indicative of this type of work or is this that Amazon?
[01:25:48.860 --> 01:25:54.540]   Goes above and beyond it, you know as far as pushing their employees to the limits. I don't know
[01:25:54.540 --> 01:26:01.120]   Well, it certainly seems like the expectations have increased like you said everybody expects the package to show up
[01:26:01.120 --> 01:26:06.840]   You know an hour after they ordered it so Amazon is trying to meet those expectations
[01:26:06.840 --> 01:26:12.420]   But my favorite line was at every turn it felt like the human staff were reduced to livestock
[01:26:12.420 --> 01:26:14.940]   Existing only to service the machines
[01:26:14.940 --> 01:26:16.940]   Oh
[01:26:16.940 --> 01:26:19.340]   Believe that yeah
[01:26:19.340 --> 01:26:21.340]   That's a few right
[01:26:21.340 --> 01:26:23.740]   machines are
[01:26:23.740 --> 01:26:27.180]   better better and faster and the
[01:26:27.180 --> 01:26:30.700]   at this I mean that's that's one of the challenges of
[01:26:30.700 --> 01:26:35.260]   Low-skilled workers and you know because I write about IOT a lot
[01:26:35.260 --> 01:26:40.340]   I will say that on the employment side in factories that get automated so not warehousing
[01:26:40.340 --> 01:26:44.620]   But in factories the staff now has to be
[01:26:44.620 --> 01:26:50.860]   More trustworthy more educated and able to make decisions based on the information the robots offer to them
[01:26:50.860 --> 01:26:56.300]   So I mean what we're seeing is a hollowing out of kind of the unskilled labor force
[01:26:56.300 --> 01:27:00.100]   And that's you know, we've talked about this before with things like
[01:27:00.100 --> 01:27:04.580]   The Amazon store where you don't have to go to a cashier
[01:27:04.580 --> 01:27:06.860]   What happens to all the people who?
[01:27:07.100 --> 01:27:11.860]   Cashier was a pretty good job and that was the way they not just entered the workforce
[01:27:11.860 --> 01:27:15.980]   But you know raised their kids or put them through school and what happens?
[01:27:15.980 --> 01:27:19.020]   When there aren't anymore cashiers what happens when?
[01:27:19.020 --> 01:27:22.460]   Long-haul truck driving is not a thing that you can do
[01:27:22.460 --> 01:27:25.020]   and the jobs
[01:27:25.020 --> 01:27:27.460]   servicing robots are
[01:27:27.460 --> 01:27:29.860]   Require skills that you don't have
[01:27:29.860 --> 01:27:32.340]   So it's good in a way that
[01:27:32.340 --> 01:27:34.180]   You know warehouse jobs are terrible
[01:27:34.180 --> 01:27:39.780]   And so if it was all automated that would be good because people shouldn't be doing those jobs there it's horrible
[01:27:39.780 --> 01:27:48.100]   But what if that's the only job you can get or what if that's you know, then then what do we do with the people who that was their job?
[01:27:48.100 --> 01:27:49.300]   Right
[01:27:49.300 --> 01:27:55.940]   Yeah, I mean because you're talking a lot of people here obviously Amazon's been increasing the amount, you know, they're their warehouses
[01:27:55.940 --> 01:28:00.980]   Doing you know massive hiring days to fill these warehouses with workers
[01:28:00.980 --> 01:28:08.820]   So one hand they're providing jobs to a lot of people on the other hand the conditions. Yeah, the conditions are miserable
[01:28:08.820 --> 01:28:11.900]   And you know someone I
[01:28:11.900 --> 01:28:16.340]   Didn't see the name because it scrolled out of you, but made a it made a good point, you know Amazon's
[01:28:16.340 --> 01:28:20.980]   Amazon will be criticized because of the working conditions of the workers
[01:28:20.980 --> 01:28:27.700]   But then once Amazon replaces the workers with robots and machines Amazon will be criticized for replacing
[01:28:27.700 --> 01:28:31.580]   You know real human jobs with robots and putting them out of work
[01:28:31.580 --> 01:28:37.260]   So it's kind of a no-win situation from Amazon's and this reminds me a lot of the controversies over
[01:28:37.260 --> 01:28:41.140]   You know sneaker companies like Nike
[01:28:41.140 --> 01:28:42.940]   using
[01:28:42.940 --> 01:28:51.820]   poor people in other countries children in some cases to make their products that and paying them what what we would see as as
[01:28:51.820 --> 01:28:55.220]   a barely living wage and yet in
[01:28:55.820 --> 01:29:00.020]   Those countries those jobs are actually pretty good and they're making a lot of money
[01:29:00.020 --> 01:29:07.660]   So it it seems horrible and yet it might actually be better than the alternative. Yeah, so it's hard to make a judgment
[01:29:07.660 --> 01:29:10.300]   I'm sure if you ask some of those
[01:29:10.300 --> 01:29:16.820]   Warehouse workers they'd be like, you know, this is horrible, but I have a job and I can put food on the table, right?
[01:29:16.820 --> 01:29:21.020]   Yeah, that's true
[01:29:21.020 --> 01:29:27.660]   Yeah, good good point you owe do is if that is your real name says it's actually a win-win for Amazon
[01:29:27.660 --> 01:29:31.260]   Which is true? It's only a lose lose for Amazon in the court of public opinion
[01:29:31.260 --> 01:29:35.540]   But Amazon still wins and Bezos truly wins
[01:29:35.540 --> 01:29:42.220]   No matter what actually Amazon's the number of areas they're gonna win keep increasing
[01:29:42.220 --> 01:29:44.940]   You know, they I mean retail is a whole
[01:29:44.940 --> 01:29:48.780]   Now they're they're talking about getting into pharmacy
[01:29:49.580 --> 01:29:51.580]   That's a that's a
[01:29:51.580 --> 01:29:57.340]   Hundreds of billions of dollars that market and if Amazon brings the same kinds of things that brought to
[01:29:57.340 --> 01:30:06.460]   buying, you know, USB thumb drives and books to that market that's gonna I will get counterfeit pills so much easier so much
[01:30:06.460 --> 01:30:10.940]   That's a service
[01:30:10.940 --> 01:30:19.100]   And then I think we're about at wrap-up time before the the last
[01:30:19.580 --> 01:30:25.020]   The last ad but before we do that I realized we didn't talk at all about Waymo have you guys been
[01:30:25.020 --> 01:30:28.380]   Following the latest developments in the uber Waymo case
[01:30:28.380 --> 01:30:34.540]   Things are getting a little is that gonna be like a novella on TV a telenovela good lord. It's totally
[01:30:34.540 --> 01:30:42.160]   Definitely has some some nuggets in there. There was some testimony yesterday from a former uber security
[01:30:42.160 --> 01:30:44.540]   analysts attorney Richard Jacobs
[01:30:44.540 --> 01:30:47.860]   who said that uber reportedly went to
[01:30:48.460 --> 01:30:53.660]   Great lengths to cover its tracks to keep quote unlawful schemes from seeing the light of day
[01:30:53.660 --> 01:30:57.820]   That testimony also showed that the company was urging employees to use
[01:30:57.820 --> 01:31:00.420]   disappearing chat apps for
[01:31:00.420 --> 01:31:07.700]   For their conversations so that nothing was, you know held in a permanent record also that employees were trained to quote impede
[01:31:07.700 --> 01:31:12.000]   Investigations none of this is sounding very good ultimately judge
[01:31:12.000 --> 01:31:17.760]   Alsop said it would be a huge injustice to force the trial to begin on December 4th
[01:31:17.760 --> 01:31:23.400]   Which is I think what it was scheduled to begin in light of this letter that reveals a lot of this information now Waymo's
[01:31:23.400 --> 01:31:28.880]   Who has discovered this letter is now starting to dive in so they're postponing the trial date?
[01:31:28.880 --> 01:31:31.940]   It's being pushed back a little bit while they dive into this
[01:31:31.940 --> 01:31:34.880]   Things not looking so good for uber in this case
[01:31:34.880 --> 01:31:38.840]   And it's amazing how that company
[01:31:39.200 --> 01:31:43.840]   Well, what was this the just before that the most recent?
[01:31:43.840 --> 01:31:50.480]   news was that they had this massive security breach that they paid people to cover up
[01:31:50.480 --> 01:31:53.740]   I mean, I don't know how uber can get through a week
[01:31:53.740 --> 01:31:55.400]   without
[01:31:55.400 --> 01:31:57.400]   You know some massive
[01:31:57.400 --> 01:32:01.040]   Revelation about how they did something dodgy. It's like it's like
[01:32:01.040 --> 01:32:03.640]   so that I remember
[01:32:03.640 --> 01:32:08.720]   Having an argument with somebody about whether uber was actually innovative or whether they were just taking advantage of
[01:32:09.160 --> 01:32:14.520]   Legal loopholes and trying to force laws to change and that and my argument was that's good
[01:32:14.520 --> 01:32:20.880]   They actually have force change in things like the taxi market, but it it almost feels like their big innovation was
[01:32:20.880 --> 01:32:25.280]   Finding ways around doing the right thing instead of actually
[01:32:25.280 --> 01:32:29.560]   Innovating to do a better thing if you know what I mean, absolutely
[01:32:29.560 --> 01:32:32.840]   I mean, it was it was a part of their culture
[01:32:32.840 --> 01:32:41.080]   You know hands down that that was a part of their culture and drive to success was how can we side step all these things and now the new
[01:32:41.080 --> 01:32:43.520]   CEO Dara Kazrashahi
[01:32:43.520 --> 01:32:45.520]   has vowed to
[01:32:45.520 --> 01:32:49.900]   kind of you know, make a copa to a certain degree and
[01:32:49.900 --> 01:32:57.560]   Say things are changed. He reportedly knew about that that data breach. So now well, he told people about the data breach
[01:32:58.200 --> 01:33:04.240]   That Dara is why the data breach was out there. He found out. I was like, we gotta tell people
[01:33:04.240 --> 01:33:06.240]   When did he find out?
[01:33:06.240 --> 01:33:09.080]   Well, he's only been there for like a month
[01:33:09.080 --> 01:33:12.160]   Yeah, I'm just saying
[01:33:12.160 --> 01:33:18.520]   You know if you discover something like that on the first day you probably don't I mean this poor guy think about
[01:33:18.520 --> 01:33:24.280]   I know think about what else is under the bed, you know, he's like he's like a maid at a hotel room
[01:33:24.280 --> 01:33:28.200]   God, there's gonna be so much nastiness running around there
[01:33:28.200 --> 01:33:33.560]   But it does it reinforces for me that there's a very kind of gray area
[01:33:33.560 --> 01:33:36.000]   loose
[01:33:36.000 --> 01:33:38.000]   hard to define line between
[01:33:38.000 --> 01:33:41.760]   pushing for change and
[01:33:41.760 --> 01:33:49.320]   Innovation and that sort of mentality that we need to move fast and break things and and this goes for Facebook to and
[01:33:49.320 --> 01:33:51.880]   and that between that and
[01:33:52.680 --> 01:33:57.240]   Actually breaking things that are kind of important like laws or you know
[01:33:57.240 --> 01:34:04.280]   Moving fast and breaking things was fine when Facebook was, you know a bunch of guys in their hoodies and flip-flops
[01:34:04.280 --> 01:34:09.800]   But moving fast and breaking democracy or moving fast and breaking, you know
[01:34:09.800 --> 01:34:17.280]   Society or information is that's bad. Yeah, so move fast and break everything's
[01:34:17.280 --> 01:34:22.640]   Yeah, and so we're looking sure break break things and then fix that
[01:34:22.880 --> 01:34:25.600]   That's cool and that's good if it's if they're just gadgets
[01:34:25.600 --> 01:34:29.440]   But if they're laws or their actual, you know things that
[01:34:29.440 --> 01:34:34.560]   Allow society to function then maybe moving fast and breaking them is not the right approach. Yeah
[01:34:34.560 --> 01:34:39.400]   Yes indeed
[01:34:39.400 --> 01:34:46.600]   All right, so I guess we'll we'll find out when that court date is pushed off to and I'm sure
[01:34:46.600 --> 01:34:49.440]   She has to talk about it on this show when that does happen
[01:34:50.080 --> 01:34:52.320]   Have I missed anything else before we
[01:34:52.320 --> 01:34:55.600]   Start to kind of wrap things
[01:34:55.600 --> 01:34:57.920]   Well, well, we're not quite at wrapping things up yet
[01:34:57.920 --> 01:35:01.040]   But have I missed any stories that you guys really want to talk about before we do the break
[01:35:01.040 --> 01:35:04.800]   Then we can do the stuff
[01:35:04.800 --> 01:35:11.360]   Well Facebook
[01:35:11.360 --> 01:35:13.800]   Stepped it again with the racial targeting
[01:35:14.400 --> 01:35:18.760]   So that they said that wasn't gonna happen anymore and then someone found it
[01:35:18.760 --> 01:35:25.000]   Continued to happen. That's another for me at least algorithmic kind of problem or more like a yes
[01:35:25.000 --> 01:35:27.680]   You can target using all sorts of offensive terms
[01:35:27.680 --> 01:35:33.920]   That doesn't mean Facebook wants to advertise to racists just that
[01:35:33.920 --> 01:35:40.920]   The system permits it gives that it gives people the tools to become do ever be racist
[01:35:40.920 --> 01:35:43.280]   so it feels more like a bug than a
[01:35:43.880 --> 01:35:51.880]   Yeah, well, I would say no if they're their whole goal is to get you to share as much of yourself on there as possible
[01:35:51.880 --> 01:36:00.440]   So it can create finely tuned demographic profiles to sell to advertisers. That is literally what Facebook does all that crap about
[01:36:00.440 --> 01:36:02.200]   the world so
[01:36:02.200 --> 01:36:04.200]   That it's able to do that
[01:36:04.200 --> 01:36:06.640]   in doing this accurately
[01:36:06.640 --> 01:36:09.160]   and harmfully is not
[01:36:09.160 --> 01:36:12.040]   That's not surprising
[01:36:12.040 --> 01:36:19.400]   No, but in these cases at least my understanding is people just typed in racist terms where you're supposed to put the name of your company
[01:36:19.400 --> 01:36:24.600]   Or where you're supposed to put what business you're in and they just typed in things like Jew-hating
[01:36:24.600 --> 01:36:30.440]   And so that and so then the system didn't automatically flag that as not an occupation or
[01:36:30.440 --> 01:36:38.000]   Not a thing that that you should be able to fill into a form where you're gonna do an automated ad campaign
[01:36:38.000 --> 01:36:44.240]   I think if nothing else we should know in the great the great year of our lord
[01:36:44.240 --> 01:36:46.880]   2017 is that people
[01:36:46.880 --> 01:36:54.480]   Will be racist a-holes sure the internet and they will do that kind of stuff. So for Facebook to say oh
[01:36:54.480 --> 01:37:00.520]   You know you have to know people do that. Yeah, you have to build this into your platform. You have to say hey
[01:37:00.520 --> 01:37:05.800]   What's that racist a-hole who hangs out on my reddit Nazi channel gonna do here?
[01:37:05.960 --> 01:37:07.960]   That should just be run over your QA
[01:37:07.960 --> 01:37:13.000]   And that's why things like Microsoft's chatbot that went instantly racist
[01:37:13.000 --> 01:37:16.080]   I think in like half an hour that shouldn't come as a surprise
[01:37:16.080 --> 01:37:24.640]   I mean if you've ever been to reddit or 4chan or anything like that your you should you should know going in
[01:37:24.640 --> 01:37:28.000]   Yeah, that like you said the worst aspects of humanity
[01:37:28.000 --> 01:37:34.680]   Will take control of your platform or your service or your thing right and you should figure out how to make that happen
[01:37:35.400 --> 01:37:39.320]   How can your thing or how will your thing be abused and
[01:37:39.320 --> 01:37:42.200]   Make sure yeah cuz it will
[01:37:42.200 --> 01:37:47.560]   Plenty of people have have have reason to you know or feel like they have reason to do that
[01:37:47.560 --> 01:37:53.600]   And really quick actually this I think I saw breaking this morning maybe maybe with
[01:37:53.600 --> 01:37:55.640]   Andy Rubin
[01:37:55.640 --> 01:38:01.800]   So Andy Rubin godfather of Android founder of essential is now apparently on leave from the company
[01:38:02.440 --> 01:38:07.680]   essential after a report of you guessed it an appropriate relationship at Google and
[01:38:07.680 --> 01:38:15.000]   You know, yeah, something something smells wrong about that because you I don't see
[01:38:15.000 --> 01:38:19.880]   That just a relationship with a co-worker is
[01:38:19.880 --> 01:38:21.640]   going to
[01:38:21.640 --> 01:38:27.040]   Cause Google to get rid of any ribbon. There's got to be more well Google didn't get rid of him and you're
[01:38:28.040 --> 01:38:32.000]   Sorry, yeah, but there's got to be more going on like there's
[01:38:32.000 --> 01:38:37.640]   You know than just we were co-workers and had a relationship. Yeah, I don't seem
[01:38:37.640 --> 01:38:41.360]   Like a firing offense or a quitting offense or a
[01:38:41.360 --> 01:38:46.800]   Yeah, well, they're saying that this happened while Rubin worked in the Android division
[01:38:46.800 --> 01:38:49.600]   and
[01:38:49.600 --> 01:38:54.120]   There was a report. Oh, sorry. I just disappeared there was a report to HR
[01:38:55.080 --> 01:39:01.760]   After that report to HR, he moved to the robotics division apparently within Google the Verge pointed out that Google's guidelines
[01:39:01.760 --> 01:39:08.040]   Would have required if he was working in Android at the time of this whatever this is happened. He would have been required
[01:39:08.040 --> 01:39:15.680]   To move or him or the other person affected to move to a different division to split them up
[01:39:15.680 --> 01:39:18.600]   So that would make sense that he would move to the robotics division
[01:39:18.600 --> 01:39:23.840]   It would also have required the company to disclose that relationship. I don't know if there was any disclosure
[01:39:23.840 --> 01:39:25.840]   I mean there certainly wasn't publicly
[01:39:25.840 --> 01:39:31.440]   Rubin spokesperson says relationship was consensual whatever it was
[01:39:31.440 --> 01:39:38.880]   But that Google's internal investigation said that Rubens behavior was quote improper and showed bad judgment
[01:39:38.880 --> 01:39:46.240]   So yeah, so you could you could throw him under the sexual harasser bus given the times because his phone is terrible
[01:39:46.240 --> 01:39:51.640]   In or he could have done something again, it essential
[01:39:52.080 --> 01:40:00.960]   How crazy would it be to be branded as a sexual harasser is better than being branded as the failed CEO of a smartphone company in 2017
[01:40:00.960 --> 01:40:03.640]   Wow
[01:40:03.640 --> 01:40:09.840]   That's like choosing between being shot and being stabbed like I don't it's
[01:40:09.840 --> 01:40:16.880]   Well, he apparently chose sexual harassment if that's okay, but you know he may have yeah, I mean I've met Andy Rubin
[01:40:16.880 --> 01:40:18.000]   he is
[01:40:18.000 --> 01:40:22.020]   The worst I could say about him is he was kind of condescending to me and
[01:40:22.020 --> 01:40:30.840]   He has he has a little bit of a quality though even when you see him speaking like publicly is he's got that that aura to him
[01:40:30.840 --> 01:40:38.920]   But if that was a firing offense there'd be nobody working at any of the major platforms. It's all like on Valley. Yeah, yeah, right
[01:40:38.920 --> 01:40:42.000]   So I'm not but I will say that Google
[01:40:42.000 --> 01:40:45.560]   Google if we
[01:40:45.800 --> 01:40:50.600]   The sexual harassment that goes on at Google or even things like dating your coworkers
[01:40:50.600 --> 01:40:53.700]   That is not uncommon at Google. That's not uncommon at a lot of tech places
[01:40:53.700 --> 01:41:00.080]   especially places that want you to live where you work into be right social at work, so
[01:41:00.080 --> 01:41:08.960]   you know this and that is an unintended consequence you force people to live in dormitories nearby and socialize with each other and you
[01:41:08.960 --> 01:41:11.200]   provide all sorts of you know
[01:41:11.560 --> 01:41:16.000]   Chances for them to do that at work and you blur the lines between work and your social life
[01:41:16.000 --> 01:41:18.000]   And then it shouldn't be that surprising
[01:41:18.000 --> 01:41:20.960]   People are gonna form relationships. Yeah
[01:41:20.960 --> 01:41:25.480]   All right, I think we've done it
[01:41:25.480 --> 01:41:30.240]   I think there is not one possible news story left for us to discuss we've covered it all
[01:41:30.240 --> 01:41:34.080]   Everything except for the things that we didn't talk about from the doc
[01:41:34.080 --> 01:41:37.360]   But before we get into tips tricks and all that kind of stuff
[01:41:37.880 --> 01:41:44.040]   To close out the show. Let's thank the sponsor of today's episode of this week in Google and that is sonic sonic provides
[01:41:44.040 --> 01:41:50.600]   My internet at home and also provides twits 10 gig fiber internet here at the studio
[01:41:50.600 --> 01:41:52.680]   They are that they are
[01:41:52.680 --> 01:41:59.880]   Twits service provider the internet infrastructure in the US as you may know needs fixing too many people are paying too much
[01:41:59.880 --> 01:42:02.040]   For unsatisfactory cable internet
[01:42:02.760 --> 01:42:09.280]   Sonic actually delivers fast affordable internet phone and TV to homes and businesses all over California
[01:42:09.280 --> 01:42:14.840]   We love sonic sonic's mission is to bring internet freedom to all with unlimited and
[01:42:14.840 --> 01:42:21.040]   Uncapped internet we didn't even talk today about kind of this this whole march up to the net neutrality
[01:42:21.040 --> 01:42:25.400]   Stuff is happening with Ajit pie a little bit later in December
[01:42:25.400 --> 01:42:32.660]   Sonic goes counter to a lot of what a lot of the other ISPs are doing they're dedicated there firmly dedicated to this
[01:42:32.740 --> 01:42:34.740]   Idea of unlimited
[01:42:34.740 --> 01:42:40.700]   Uncapped privacy focused internet sonic delivers residential and business fiber to the premise
[01:42:40.700 --> 01:42:46.720]   Networks with gigabit connectivity in San Francisco the north Bay area and the east Bay area
[01:42:46.720 --> 01:42:51.060]   Internet service includes 15 email accounts and one gig of storage
[01:42:51.060 --> 01:42:56.900]   Personal web hosting with a new domain and fax line service from only $40 per month
[01:42:57.300 --> 01:43:06.860]   Sonic offers download speeds up to 1000 megabits per second a home phone connection with unlimited local and long distance calling is included in all of their services
[01:43:06.860 --> 01:43:13.860]   Switching from your current carrier is actually very easy. You can keep your existing phone number thankfully and by standing up for privacy
[01:43:13.860 --> 01:43:19.800]   Friendly and local customer support uncapped bandwidth and affordable pricing for all its products
[01:43:20.180 --> 01:43:29.080]   Well some sonic's customer advocacy is paving the way for a better state of internet access in America and really sets a standard
[01:43:29.080 --> 01:43:32.660]   I feel for how ISPs should be and
[01:43:32.660 --> 01:43:38.360]   You know increasingly are not sonic is steadfast in their commitment to
[01:43:38.360 --> 01:43:40.620]   privacy and
[01:43:40.620 --> 01:43:43.600]   You know just being very pro customer pro consumer
[01:43:44.020 --> 01:43:51.780]   Join the internet revolution visit sonic comm slash twit you'll receive your first month of sonic internet and phone service for free
[01:43:51.780 --> 01:43:59.900]   Plus bundle with dish and you'll save a hundred twenty dollars on your sonic bill visit sonic comm slash twit
[01:43:59.900 --> 01:44:04.620]   That's sonic comm slash twit we thank sonic for their support
[01:44:04.620 --> 01:44:07.420]   All right
[01:44:07.420 --> 01:44:13.260]   Stacy it looks like you have a thing in the dog. What's the thing? I did so this actually was a tip from a twit
[01:44:13.980 --> 01:44:14.740]   a
[01:44:14.740 --> 01:44:15.900]   Twit
[01:44:15.900 --> 01:44:18.880]   Reader listener listener watcher whatever
[01:44:18.880 --> 01:44:25.820]   Fair like a person who watches and listens to twit on Bluetooth headsets
[01:44:25.820 --> 01:44:28.940]   So this oh I didn't put I just put Bluetooth headset
[01:44:28.940 --> 01:44:35.340]   So this is the sense so I was looking for cheap Bluetooth headsets to go with my phone that I could just wear
[01:44:35.340 --> 01:44:41.340]   You know running around and just I lose headsets like nobody's business. So I don't want to spend a lot of money on them
[01:44:42.220 --> 01:44:44.220]   So these are
[01:44:44.220 --> 01:44:48.700]   Okay, and they're comfortable for me with small ears. There is I still
[01:44:48.700 --> 01:44:51.900]   Feel like the sound quality is good
[01:44:51.900 --> 01:44:58.980]   When I talk to people on them there is if I'm outside there's noise, but if I'm inside people can hear me just fine
[01:44:58.980 --> 01:45:03.060]   So and I did go do a workout
[01:45:03.060 --> 01:45:10.460]   They did not fall out of my ears while I was running nor did the ones the one sweat session that I did that did not short them out
[01:45:10.460 --> 01:45:16.060]   So I felt good about that. So these are a $30 pair of Bluetooth headphones and
[01:45:16.060 --> 01:45:18.860]   You know, they're all right
[01:45:18.860 --> 01:45:20.060]   nice
[01:45:20.060 --> 01:45:26.300]   I actually need to move into the Bluetooth headphone world because Google is basically forcing me to because of
[01:45:26.300 --> 01:45:32.660]   I'm living the dongle life now with my wired headphones and the pixel - I'm still not happy about it
[01:45:32.660 --> 01:45:37.500]   But I figure it's probably time for me to check out Bluetooth headphones finally and see if I really
[01:45:38.020 --> 01:45:42.540]   Care either way about them my biggest issue and I don't know if this is
[01:45:42.540 --> 01:45:45.420]   Headphones specific yet because I haven't tried enough of them
[01:45:45.420 --> 01:45:49.380]   The others were the pixel buds and those are terrible. Yeah
[01:45:49.380 --> 01:45:59.140]   So I'm not gonna put this but there is interruption on them every now and then and it I don't know if it's just my phone like
[01:45:59.140 --> 01:46:01.500]   things notifications going in but
[01:46:01.500 --> 01:46:04.380]   man
[01:46:05.500 --> 01:46:10.460]   So so every now and then yeah, so every now and then they're not like there's still Bluetooth headsets
[01:46:10.460 --> 01:46:17.660]   I guess is the best way to do it. They they're a great pair of $30 Bluetooth headsets with all the caveats at that implies
[01:46:17.660 --> 01:46:20.460]   Got it. Have you tried the pixel buds?
[01:46:20.460 --> 01:46:26.220]   Yeah, they're awful. I wanted to claw them at they felt like something clawing at my ears
[01:46:26.220 --> 01:46:32.220]   I hated them and then someone said in order to pair them with something or to put them back in the case
[01:46:32.420 --> 01:46:39.660]   Reset them and then pair them suit to move from like one device to another. That just seems insane. I don't understand how
[01:46:39.660 --> 01:46:44.780]   They could have made it out the door with with that being the case. Wow
[01:46:44.780 --> 01:46:47.980]   I'm not I'm not rushing
[01:46:47.980 --> 01:46:51.300]   Emoji yeah
[01:46:51.300 --> 01:46:54.740]   In the right place
[01:46:54.740 --> 01:46:58.380]   According to the Cinderpitch I
[01:46:58.540 --> 01:47:03.540]   Matthew what about you? Do you have anything stuff thing tool tip anything that you want to mention?
[01:47:03.540 --> 01:47:09.900]   So I guess I'm interested in there's this thing called Bitcoin. I don't know if you guys remember her that a little bit
[01:47:09.900 --> 01:47:12.700]   pretty kind of a big deal I
[01:47:12.700 --> 01:47:20.340]   Just I find it fascinating. I said it was saying to this to someone the other day it watching
[01:47:20.340 --> 01:47:25.340]   Bitcoin over the last year or cryptocurrency in blockchain is
[01:47:25.900 --> 01:47:33.780]   the closest thing I can remember to when I first saw the internet so a mosaic browser and
[01:47:33.780 --> 01:47:39.700]   hyperlinks and then the early early days of the internet because I'm an old person I
[01:47:39.700 --> 01:47:43.100]   Just it was
[01:47:43.100 --> 01:47:45.380]   simultaneously
[01:47:45.380 --> 01:47:48.180]   Incredibly nerdy
[01:47:48.180 --> 01:47:50.180]   really dumb looking
[01:47:50.180 --> 01:47:52.620]   hard to use
[01:47:52.620 --> 01:47:54.620]   confusing and yet
[01:47:54.940 --> 01:47:56.940]   incredibly fascinating and
[01:47:56.940 --> 01:48:03.180]   You just had a feeling that this was gonna change not just one or two things but everything
[01:48:03.180 --> 01:48:07.860]   Eventually, which I think has proven true and I feel that way
[01:48:07.860 --> 01:48:10.380]   I felt the same when Napster showed up and peer-to-peer
[01:48:10.380 --> 01:48:13.260]   networking but
[01:48:13.260 --> 01:48:17.060]   Bitcoin feels and cryptocurrency and blockchain feel
[01:48:17.060 --> 01:48:22.140]   So there it's it's just nerds and people trying to make quick money at the moment
[01:48:22.380 --> 01:48:26.180]   But that's you know the early Internet was like that too. I
[01:48:26.180 --> 01:48:29.180]   Just feel like there's something
[01:48:29.180 --> 01:48:33.380]   This is gonna change a lot of stuff a lot of incredibly important
[01:48:33.380 --> 01:48:36.380]   things whether it's insurance or banking or
[01:48:36.380 --> 01:48:39.500]   Government or you you name it and yet right now
[01:48:39.500 --> 01:48:44.660]   It's a it's a wild west side show and both of those aspects are kind of fascinating to me. So
[01:48:44.660 --> 01:48:47.580]   Anyway, some you should look into it
[01:48:48.220 --> 01:48:54.580]   So you're kicking yourself for having sold the Bitcoin way back when yeah, does that mean you're
[01:48:54.580 --> 01:48:56.700]   Have you gotten back on that train now?
[01:48:56.700 --> 01:49:00.220]   I mean it's too expensive to get back on that train now if you want a single Bitcoin anyway
[01:49:00.220 --> 01:49:03.780]   But there's so many different cryptocurrencies now and ethereum
[01:49:03.780 --> 01:49:06.900]   I think is way more interesting than Bitcoin like Bitcoin
[01:49:06.900 --> 01:49:12.620]   I think is was a proof of concept in it and it it led the way for a lot of things
[01:49:12.620 --> 01:49:16.940]   But there are way more interesting things being done with other types of cryptocurrencies
[01:49:16.940 --> 01:49:24.020]   I find the financial aspect fascinating with companies doing ICOs, which are yeah kind of equity issues, but kind of not
[01:49:24.020 --> 01:49:29.820]   The SEC's scrambling to catch up. I talked to a guy who works for the Bank of Canada
[01:49:29.820 --> 01:49:31.660]   and
[01:49:31.660 --> 01:49:36.140]   He they're thinking about launching their own cryptocurrency the Bank of Canada
[01:49:36.140 --> 01:49:42.100]   So the central bank of a country Wow because they're gonna have they're gonna have to do something
[01:49:42.660 --> 01:49:49.220]   Otherwise they will cease to be relevant because currency and markets will move elsewhere
[01:49:49.220 --> 01:49:51.220]   So it's just a fascinating area
[01:49:51.220 --> 01:49:58.700]   Yeah, hey one of the bigger threats to banks is the fact that you can no longer do time delayed based arbitrage with
[01:49:58.700 --> 01:50:07.820]   cryptocurrencies because you automatically transfer the data it goes as fast as the data so things like the wires for
[01:50:07.820 --> 01:50:10.820]   traditional payments
[01:50:11.460 --> 01:50:14.860]   Yes, it's gotten faster, but you still had like an overnight time frame
[01:50:14.860 --> 01:50:18.900]   Those weren't hit people's accounts and you could actually trade on that and people did
[01:50:18.900 --> 01:50:22.540]   So well not not yet not everybody
[01:50:22.540 --> 01:50:27.220]   Some financial players are like dang this is gonna kill us
[01:50:27.220 --> 01:50:29.860]   So it's very
[01:50:29.860 --> 01:50:34.620]   Yeah, interesting. Yeah, there's gonna be fortunes made and lost
[01:50:34.620 --> 01:50:39.820]   For sure absolutely. Oh, really are my my biggest rule for any
[01:50:40.420 --> 01:50:42.420]   ICO any
[01:50:42.420 --> 01:50:48.140]   Bitcoin slash blockchain tech is first of all, why do you need a distributed ledger?
[01:50:48.140 --> 01:50:51.420]   Because if you don't if you can't justify that it's just yeah, it's just
[01:50:51.420 --> 01:50:55.340]   Yeah, that's it
[01:50:55.340 --> 01:51:00.660]   All right, what do I have let's see here? I think I have a tip for you
[01:51:00.660 --> 01:51:02.100]   We talked about it a little bit earlier
[01:51:02.100 --> 01:51:09.540]   We talked about the pixel to having the pixel visual core and with the new 8.1 beta update rolling out
[01:51:09.660 --> 01:51:15.660]   It's gonna activate that but just because you got the update on your pixel 2 doesn't mean that it's activated
[01:51:15.660 --> 01:51:24.220]   So if you want the benefits of pixel visual core which is and primarily the benefits right now or that you get all of the camera quality
[01:51:24.220 --> 01:51:28.780]   The HDR plus processing that you're used to getting in the stock camera app
[01:51:28.780 --> 01:51:36.820]   But in all camera apps on third-party apps like Instagram or whatever when you use those cameras you get the better improved quality
[01:51:36.820 --> 01:51:42.660]   That's a big part of what this allows for you're gonna have to go into settings and your pixel 2 device and activate this
[01:51:42.660 --> 01:51:45.820]   And I'll show you how to do it basically you have to have here we go
[01:51:45.820 --> 01:51:51.340]   You have to have access to the developer settings if you don't know how if you don't have access to developer settings
[01:51:51.340 --> 01:51:55.780]   You'll need to activate that first and foremost. It's kind of a hidden thing if you go into system
[01:51:55.780 --> 01:51:58.260]   and then you go into about phone and
[01:51:58.260 --> 01:52:04.340]   You double tap or you tap the build number which I'm trying to kind of hide a little bit here a bunch of times
[01:52:04.340 --> 01:52:09.300]   It'll open up the menu for you and then you'll find it in here system settings developer options
[01:52:09.300 --> 01:52:14.780]   I'll go ahead and go in there and then when you go down to the debugging section down at the very bottom of debugging
[01:52:14.780 --> 01:52:17.300]   his camera how HDR plus
[01:52:17.300 --> 01:52:23.900]   Go ahead and turn that on and then in order to actually apply it you have to reboot your device once you do that
[01:52:23.900 --> 01:52:30.260]   Then you will hopefully experience the benefits of pixel visual core in 8.1
[01:52:30.260 --> 01:52:36.420]   And some of your third-party camera apps that you use will see an improved bump in quality free pictures
[01:52:36.420 --> 01:52:40.980]   So now you know and knowing is half the battle as
[01:52:40.980 --> 01:52:53.020]   That's it. I think we've we've got a bunch of magical title suggestion ideas that came from this episode
[01:52:53.020 --> 01:52:55.020]   Which tells me this was an awesome episode
[01:52:55.020 --> 01:52:58.860]   Usually the best episodes have like tons of title options
[01:52:59.940 --> 01:53:05.500]   So but I knew it was going to be because you guys are both awesome and I really enjoyed doing shows with you guys
[01:53:05.500 --> 01:53:10.220]   So thanks for inviting to the home. Thanks, man. Thank you Matthew
[01:53:10.220 --> 01:53:19.740]   Matthew Ingram Columbia journalism review Matthew Ingram calm Matthew you're awesome. Tell people where they can follow all your awesome work
[01:53:19.740 --> 01:53:28.780]   They can go to CJ or org or they can go to add Matthew. I was one tea on Twitter right on very important one tea
[01:53:28.780 --> 01:53:31.100]   You're gonna want to put the second tea there. Don't do it
[01:53:31.100 --> 01:53:37.980]   Entirely different Matthew Ingram. That's the people need to I only need one. That's right. Yes, you make one work
[01:53:37.980 --> 01:53:41.460]   Thank you Matthew. It's a lot of fun. Appreciate it and
[01:53:41.460 --> 01:53:46.860]   Stacey it's always a pleasure to get the chance to podcast with you. Thank you so much
[01:53:46.860 --> 01:53:52.340]   Creator of the IOT podcast IOT podcast calm. What else do you want people to know?
[01:53:53.260 --> 01:54:00.780]   You can go to Stacy on IOT you'll find stories about IOT from me and Kevin TOEFL and you'll find the podcast and
[01:54:00.780 --> 01:54:03.020]   Can sign up for the newsletter?
[01:54:03.020 --> 01:54:05.740]   Fantastic do it. Can I connect with you on Facebook?
[01:54:05.740 --> 01:54:09.220]   You can I actually
[01:54:09.220 --> 01:54:15.540]   Sometimes even answer but I don't go on it for so it's only work. Okay, okay
[01:54:15.540 --> 01:54:19.740]   If I do decide to like continue like get back on to Facebook and everything
[01:54:19.740 --> 01:54:22.860]   I think I'm still gonna keep the app off my phone, but I don't even know if I'm gonna do that
[01:54:23.820 --> 01:54:28.100]   I feel a little bit of the poll, but I need to I need to resist the temptation
[01:54:28.100 --> 01:54:30.300]   I don't know if I have time for Facebook in my life right now
[01:54:30.300 --> 01:54:35.740]   What if you just stick a thing like after you've been on Facebook for more than 10 minutes? It shuts off
[01:54:35.740 --> 01:54:38.380]   Screen turns off. Oh, that's a good idea
[01:54:38.380 --> 01:54:44.420]   And you like there there's the app for you so you put it on during certain hours and it logs off
[01:54:44.420 --> 01:54:48.100]   Automatic cuz a lot of the times like even I do this on Twitter. I'm like
[01:54:48.820 --> 01:54:52.220]   She got a bed. I should go to bed. Jesus. Why am I still schooling?
[01:54:52.220 --> 01:54:57.620]   Exactly exactly whoever invented pull to refresh. It was an evil genius
[01:54:57.620 --> 01:55:05.380]   Absolutely, it really it really is an addictive kind of impulsive thing the more you do it the more you want to
[01:55:05.380 --> 01:55:07.420]   Thank you guys
[01:55:07.420 --> 01:55:12.200]   You can find me at well y'all gold music calm if you go there actually right now
[01:55:12.200 --> 01:55:17.460]   I have some new album that I'm working on releasing and I've got a little kickstarter
[01:55:17.460 --> 01:55:21.700]   You don't have to give to the kickstarter if you want to you can the album is gonna get released anyways
[01:55:21.700 --> 01:55:25.900]   But spent the last three years on it. So I thought why not? We'll see how this goes
[01:55:25.900 --> 01:55:28.100]   So yellow gold music calm and
[01:55:28.100 --> 01:55:33.700]   You can there's a link on there to go to the kickstarter if you want to help support the release of the album
[01:55:33.700 --> 01:55:38.860]   But you know music's the thing I do in my past time and I'm happy to share it with you guys as well
[01:55:38.860 --> 01:55:44.620]   But that's it for me if you want to find this show, it's pretty easy to do twit.tv
[01:55:45.060 --> 01:55:51.140]   Slash T W I G for this week in Google you go there. You're gonna find all past episodes
[01:55:51.140 --> 01:55:57.320]   You also find that we have a live link up at the top twit.tv slash live if you want to go there
[01:55:57.320 --> 01:55:59.900]   You can watch us record live every Wednesday 4 p.m
[01:55:59.900 --> 01:56:03.860]   4 p.m. Eastern. Sorry 1 p.m. Pacific
[01:56:03.860 --> 01:56:06.500]   2100 UTC and
[01:56:06.500 --> 01:56:09.820]   I think that's about it
[01:56:09.860 --> 01:56:15.140]   You know all the information that you need to know until next week when Leo report returns the big
[01:56:15.140 --> 01:56:19.060]   Thank you to Carsten producer of the show for putting all the links together and helping
[01:56:19.060 --> 01:56:22.100]   produce the show during the live show and
[01:56:22.100 --> 01:56:27.820]   That's all we got. Thank you everyone. We'll see you next week on another episode of this week in Google. Bye
[01:56:27.820 --> 01:56:29.820]   You
[01:56:29.820 --> 01:56:32.400]   (upbeat music)
[01:56:32.400 --> 01:56:34.980]   (upbeat music)
[01:56:34.980 --> 01:56:36.980]   [Music]

