;FFMETADATA1
title=Warm and Defeated
artist=TWiT
album_artist=TWiT
album=This Week in Google
track=476
genre=Podcast
comment=http://twit.tv/twig
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.600]   It's time for Twig this week in Google.
[00:00:02.600 --> 00:00:05.520]   Jeff and Stacy are off this week, but good news.
[00:00:05.520 --> 00:00:10.240]   Matthew Ingram and Mike Elgin are here and we have a lot to talk about,
[00:00:10.240 --> 00:00:14.560]   including that Burger King commercial written with artificial intelligence.
[00:00:14.560 --> 00:00:16.560]   We've got an Olive Garden commercial.
[00:00:16.560 --> 00:00:19.280]   We'll act out written with artificial intelligence.
[00:00:19.280 --> 00:00:21.120]   We might even show some real intelligence.
[00:00:21.120 --> 00:00:21.800]   It's all next.
[00:00:21.800 --> 00:00:22.800]   On Twig.
[00:00:22.800 --> 00:00:26.200]   [MUSIC]
[00:00:26.200 --> 00:00:28.000]   Netcasts you love.
[00:00:28.000 --> 00:00:30.000]   From people you trust.
[00:00:30.000 --> 00:00:33.280]   [MUSIC]
[00:00:33.280 --> 00:00:35.080]   This is Twig.
[00:00:35.080 --> 00:00:39.200]   [MUSIC]
[00:00:39.200 --> 00:00:40.720]   This is Twig.
[00:00:40.720 --> 00:00:47.280]   This week in Google, episode 476 recorded Wednesday, October 3rd, 2018.
[00:00:47.280 --> 00:00:50.160]   Warm and defeated.
[00:00:50.160 --> 00:00:53.120]   This week in Google is brought to you by DigitalOcean,
[00:00:53.120 --> 00:00:57.360]   the easiest cloud platform to deploy, manage, and scale applications.
[00:00:57.360 --> 00:01:02.560]   Over 150,000 businesses rely on DigitalOcean to remove infrastructure friction
[00:01:02.560 --> 00:01:04.880]   and deliver industry leading price performance.
[00:01:04.880 --> 00:01:11.280]   Sign up today and receive a free $100 credit at dio.co/twig.
[00:01:11.280 --> 00:01:13.040]   And by Capterra.
[00:01:13.040 --> 00:01:15.280]   Find software solutions for your business needs.
[00:01:15.280 --> 00:01:20.000]   Capterra is a free website with over 600 categories of business software.
[00:01:20.000 --> 00:01:26.480]   And now visit capterra.com/twig to get your free copy of the big book of free software.
[00:01:26.480 --> 00:01:28.320]   And by WordPress.
[00:01:28.320 --> 00:01:32.960]   Reach more customers when you build your business website on WordPress.com.
[00:01:32.960 --> 00:01:41.520]   Plan start at just $4 a month and get 15% off any new plan at WordPress.com/twig.
[00:01:41.520 --> 00:01:45.520]   It's time for Twig this week in Google this week.
[00:01:45.520 --> 00:01:46.560]   Everybody's out.
[00:01:46.560 --> 00:01:48.240]   Stacey Higginbotham is out.
[00:01:48.240 --> 00:01:49.200]   She's doing something.
[00:01:49.200 --> 00:01:50.720]   She's in Dallas or something.
[00:01:50.720 --> 00:01:54.720]   And then Jeff Jarvis is, is he still in Milan or he's traveling?
[00:01:54.720 --> 00:01:55.920]   I believe he's in London.
[00:01:55.920 --> 00:01:58.240]   Oh, well, he's in London.
[00:01:58.240 --> 00:02:03.120]   The good news is we've replaced them with amazing simulations.
[00:02:03.120 --> 00:02:06.480]   Matthew Ingram is here from the Columbia Review of Journalism.
[00:02:06.480 --> 00:02:09.200]   He's their chief digital writer, cjr.org.
[00:02:09.200 --> 00:02:09.840]   I'm Matthew.
[00:02:09.840 --> 00:02:11.200]   Hey.
[00:02:11.200 --> 00:02:12.000]   Great to see you.
[00:02:12.000 --> 00:02:16.320]   He's a nice Canadian fella, so be gentle.
[00:02:16.320 --> 00:02:18.560]   And Mike Elgin is here.
[00:02:18.560 --> 00:02:20.960]   He has yet to leave, but he's about to leave for Italy.
[00:02:20.960 --> 00:02:22.640]   So he's still in the Bay Area.
[00:02:22.640 --> 00:02:23.120]   Hi, Mike.
[00:02:23.120 --> 00:02:23.920]   I'll be pl-- Hi.
[00:02:23.920 --> 00:02:25.440]   I'll be playing Stacey today.
[00:02:25.440 --> 00:02:26.880]   My oven is smarter than my dog.
[00:02:26.880 --> 00:02:32.800]   Mike is at digital is--
[00:02:32.800 --> 00:02:34.080]   I always get this wrong.
[00:02:34.080 --> 00:02:35.120]   So Gastronomat.
[00:02:35.120 --> 00:02:35.920]   Gastronomat.
[00:02:35.920 --> 00:02:36.640]   I always want to say digital.
[00:02:36.640 --> 00:02:37.040]   I don't know why.
[00:02:37.040 --> 00:02:38.720]   Gastronomat.net.
[00:02:38.720 --> 00:02:39.760]   And Elgin.com.
[00:02:39.760 --> 00:02:41.120]   Elgin.com for Mike's list.
[00:02:41.120 --> 00:02:46.160]   I brought Matthew on because of this article.
[00:02:46.160 --> 00:02:48.720]   Explain yourself, Matthew.
[00:02:48.720 --> 00:02:50.960]   Explain yourself in this Columbia Journalism.
[00:02:50.960 --> 00:02:52.800]   Is the podcast bubble bursting?
[00:02:54.720 --> 00:02:56.400]   It was a bubble, actually, wasn't it?
[00:02:56.400 --> 00:03:03.280]   You-- I admit-- I admire you, Matthew, because a lot of mainstream media coverage of podcasting
[00:03:03.280 --> 00:03:06.080]   for the last couple of years, acted like it was this brand new thing.
[00:03:06.080 --> 00:03:07.680]   It was just so hot.
[00:03:07.680 --> 00:03:10.560]   And that's part of the reason why the bubble burst, right?
[00:03:10.560 --> 00:03:13.040]   Because a lot of people jumped on a bandwagon.
[00:03:13.040 --> 00:03:17.520]   You point out the panoply, which was set up by Slate, and really was a well-known network.
[00:03:17.520 --> 00:03:19.520]   Recently laid off most of its staff.
[00:03:19.520 --> 00:03:23.280]   They're not going to create any new shows, just distribute other people's shows.
[00:03:23.280 --> 00:03:26.880]   And Buzzfeed laid off the staff at its podcasting unit.
[00:03:26.880 --> 00:03:29.920]   Yeah, it's interesting.
[00:03:29.920 --> 00:03:35.600]   Buzzfeed, in particular, they said they're going to more like the model they use for TV.
[00:03:35.600 --> 00:03:39.840]   So they're still going to do podcasts, but they're going to mostly use freelancers.
[00:03:39.840 --> 00:03:43.680]   So obviously, they-- I think they wanted to be a bit more flexible,
[00:03:43.680 --> 00:03:45.520]   not to get out of podcasting completely.
[00:03:45.520 --> 00:03:51.440]   Panoply clearly just decided the money's in kind of selling picks and shovels,
[00:03:51.440 --> 00:03:53.600]   not making the actual content.
[00:03:53.600 --> 00:03:57.440]   That's a play in the old line that the only company,
[00:03:57.440 --> 00:04:01.360]   the only people that made money during the gold rush in San Francisco was Levi's,
[00:04:01.360 --> 00:04:04.080]   because they made the jeans with the gold miners.
[00:04:04.080 --> 00:04:10.560]   But I do think a lot of companies were-- let's face it, desperate for new
[00:04:10.560 --> 00:04:15.360]   sources of revenue, advertisements, declining, podcasts like cereal came along,
[00:04:15.360 --> 00:04:19.120]   got a lot of attention. A lot of companies thought, great.
[00:04:19.120 --> 00:04:23.840]   It's-- you just take a mic in front of a couple of guys and boom, you've got a podcast.
[00:04:23.840 --> 00:04:25.040]   It's just that easy.
[00:04:25.040 --> 00:04:28.960]   And as you know, it takes a lot more work than that.
[00:04:28.960 --> 00:04:31.200]   It takes a lot more resources.
[00:04:31.200 --> 00:04:36.480]   It takes an investment of time, and you have to really think about what you're doing.
[00:04:36.480 --> 00:04:42.400]   And if it's just a kind of side thing you're doing to try and make some extra revenue,
[00:04:42.400 --> 00:04:43.920]   it's probably not going to work very well.
[00:04:43.920 --> 00:04:44.560]   You--
[00:04:44.560 --> 00:04:46.080]   It's also-- go ahead.
[00:04:46.080 --> 00:04:46.880]   Go ahead.
[00:04:46.880 --> 00:04:49.680]   Well, I was just going to say, it's very similar to what happened with blogging,
[00:04:49.680 --> 00:04:55.360]   because really, the people who jumped on the bandwagon didn't really get what it was and
[00:04:55.360 --> 00:04:59.120]   didn't understand it. They overspent, just like in podcasting.
[00:04:59.120 --> 00:05:06.000]   And for example, my favorite definition of a blog comes from Dave Weiner,
[00:05:06.000 --> 00:05:09.680]   who said that a blog is the unedited voice of a single person.
[00:05:09.680 --> 00:05:10.160]   Yes.
[00:05:10.160 --> 00:05:11.120]   That's all it is.
[00:05:11.120 --> 00:05:11.520]   Yeah.
[00:05:11.520 --> 00:05:16.160]   And so all these publication companies emerge with professional editors,
[00:05:16.160 --> 00:05:18.240]   and all this kind of stuff, which is very expensive.
[00:05:18.240 --> 00:05:20.960]   You need lots of revenue for that sort of thing.
[00:05:20.960 --> 00:05:25.520]   And a podcast, to a certain extent-- I don't know if it's certainly not the unedited voice
[00:05:25.520 --> 00:05:30.080]   necessarily, but it's something that's more authentic than radio or television.
[00:05:30.080 --> 00:05:31.360]   It's more real.
[00:05:31.360 --> 00:05:34.240]   And the bandwagon jumpers didn't get that.
[00:05:34.240 --> 00:05:40.640]   They did the slick, heavily produced, pretty expensive shows, which then had to be paid for
[00:05:42.400 --> 00:05:47.680]   by the tiny number of companies that tend to advertise on podcasts.
[00:05:47.680 --> 00:05:51.280]   And that's a really tough way to go.
[00:05:51.280 --> 00:05:57.440]   And they didn't find the pot of gold they're looking for, just like in blogging.
[00:05:57.440 --> 00:06:01.680]   And podcasts will be here forever, despite what Matthew Ingram says.
[00:06:01.680 --> 00:06:03.840]   No, he says it too.
[00:06:03.840 --> 00:06:05.760]   And blogging will be here forever.
[00:06:05.760 --> 00:06:10.240]   It's just not a get-rich, quick scheme by any stretch of the imagination.
[00:06:10.960 --> 00:06:13.520]   You embed a tweet from Rose Evelath.
[00:06:13.520 --> 00:06:17.440]   I don't know who she is, but she says, "The BuzzFeed pivot away from podcasts makes sense."
[00:06:17.440 --> 00:06:22.000]   When you remember that BuzzFeed lives and dies as a business on detailed analytics,
[00:06:22.000 --> 00:06:24.480]   web content has them, video has them.
[00:06:24.480 --> 00:06:25.840]   I'm not sure I'd agree with that.
[00:06:25.840 --> 00:06:26.880]   Podcasts still don't.
[00:06:26.880 --> 00:06:31.920]   Is that-- do you think that that's-- I don't think that's actually the problem, but--
[00:06:31.920 --> 00:06:37.200]   I think it's maybe one of them, especially if you're a company like BuzzFeed.
[00:06:37.840 --> 00:06:41.680]   You know, you-- Well, but that's my problem with companies like BuzzFeed in general,
[00:06:41.680 --> 00:06:44.400]   is they're chasing hits or chasing views.
[00:06:44.400 --> 00:06:46.960]   And that means it's basically demand media.
[00:06:46.960 --> 00:06:49.920]   You create content because it's going to generate views.
[00:06:49.920 --> 00:06:53.680]   And in my opinion, as a content creator, that's the worst reason to create content.
[00:06:53.680 --> 00:06:54.320]   Absolutely.
[00:06:54.320 --> 00:07:02.400]   One of the things that makes podcasts so-- I think so successful when they work well,
[00:07:02.400 --> 00:07:06.960]   is also one of the things that makes them less appealing to companies like BuzzFeed.
[00:07:06.960 --> 00:07:13.280]   And that is-- it's difficult to sort of browse or snack them in a way.
[00:07:13.280 --> 00:07:16.080]   You have to really devote some time to them in order to--
[00:07:16.080 --> 00:07:17.040]   You can't snack on our stuff.
[00:07:17.040 --> 00:07:18.480]   [LAUGHTER]
[00:07:18.480 --> 00:07:19.840]   That's a whole meal.
[00:07:19.840 --> 00:07:26.000]   They really require people to invest a certain amount of attention and time to it.
[00:07:26.000 --> 00:07:31.040]   And that pays off when a podcast does the same, when the person who's doing it
[00:07:31.040 --> 00:07:36.000]   pays attention to-- or is really passionate about their topic.
[00:07:36.000 --> 00:07:40.160]   But that makes it less likely to be the kind of thing that a company like BuzzFeed is going to be
[00:07:40.160 --> 00:07:40.640]   interested in.
[00:07:40.640 --> 00:07:45.760]   It's the way that BuzzFeed should do podcasts is they should think of it as a kind of
[00:07:45.760 --> 00:07:51.760]   crowdsourcing, fan feedback, reader feedback.
[00:07:51.760 --> 00:07:55.520]   They should be in a conversation constantly with their readers.
[00:07:55.520 --> 00:08:02.960]   And the goal of it should be both to cement the brand in the minds of their biggest fans.
[00:08:02.960 --> 00:08:06.560]   And also to improve the quality of their journalism.
[00:08:06.560 --> 00:08:09.840]   Because really, that's-- I mean, you see what happens on the Twit Network.
[00:08:09.840 --> 00:08:13.200]   You have all these shows that are all about tech.
[00:08:13.200 --> 00:08:16.000]   And everybody's learning from all the shows all the time.
[00:08:16.000 --> 00:08:21.360]   And so there's a lot of well-informed people talking to you simply because it's a network
[00:08:21.360 --> 00:08:22.480]   all on the same topic.
[00:08:22.480 --> 00:08:24.080]   So there's a lot of learning, a lot of interaction.
[00:08:24.080 --> 00:08:25.680]   That chat room never stops.
[00:08:25.680 --> 00:08:30.960]   There's a lot of information that comes into the chat room and other sources of media.
[00:08:30.960 --> 00:08:37.440]   And that's really-- to me, that's the thing that's so great about podcasting is the
[00:08:37.440 --> 00:08:39.680]   interaction with the audience, potentially.
[00:08:39.680 --> 00:08:40.240]   Yeah.
[00:08:40.240 --> 00:08:48.480]   What do you-- I think that if we lump all this stuff in a new media, so blogs, podcasts,
[00:08:48.480 --> 00:08:52.800]   YouTube, it does really still seem very early days.
[00:08:52.800 --> 00:08:57.120]   And it seems like we don't really understand a lot of what's going on.
[00:08:57.120 --> 00:09:05.280]   Well, it's really interesting is that audio is the most mobile medium there is, by far.
[00:09:05.280 --> 00:09:10.080]   You can be running a marathon and consume audio content.
[00:09:10.080 --> 00:09:15.200]   And this is one of the reasons why, yes, it's not snackable, it's not skimmable.
[00:09:15.200 --> 00:09:18.080]   But at the same time, people have-- people are doing other things.
[00:09:18.080 --> 00:09:20.560]   They're driving, they're doing dishes, stuff like that.
[00:09:20.560 --> 00:09:23.680]   And so podcasts-- audio podcasts can rush into film.
[00:09:23.680 --> 00:09:28.240]   And video podcasts are great too. But audio podcasts-- and this is going to have a great
[00:09:28.240 --> 00:09:33.360]   future because when we get into the world of smart glasses and bone conduction glasses,
[00:09:33.360 --> 00:09:39.120]   where you can just stream audio all the time, the number of opportunities for listening to
[00:09:39.120 --> 00:09:45.200]   podcasts will just grow and grow. So it's really a medium that's well-- audio podcasting is a
[00:09:45.200 --> 00:09:47.600]   medium that's well suited for what's coming in the future.
[00:09:47.600 --> 00:09:50.240]   Well, I'm glad to hear you say that.
[00:09:50.240 --> 00:09:55.040]   All you need is one earbud in one ear and you can consume audio podcasts.
[00:09:55.040 --> 00:10:00.640]   Well, and I mean smart assistants, I think there's a huge amount of potential for people to
[00:10:00.640 --> 00:10:06.960]   listen to podcasts over their Google Home or start it on their phone and continue it on their Google
[00:10:06.960 --> 00:10:12.480]   Home or their Alexa. And I do think in a way, you're rightly. It is still very early if you think
[00:10:12.480 --> 00:10:19.600]   about it. Podcasts haven't been around really that long. And so I think we're still figuring out what
[00:10:19.600 --> 00:10:28.240]   makes a good one and how do audiences engage within, then what does that generate in terms of--
[00:10:28.240 --> 00:10:33.520]   and obviously you're further along than just about everybody in terms of learning those lessons.
[00:10:33.520 --> 00:10:41.680]   I would say it's more than podcasts, more than new media. A relevant quote from Cara Swisher's
[00:10:41.680 --> 00:10:48.480]   opinion piece about Kevin's system leaving Facebook. She was of the opinion. This is interesting that
[00:10:48.480 --> 00:10:53.040]   this article became an opinion piece in The New York Times. She's of the opinion that Mark Zuckerberg,
[00:10:53.040 --> 00:10:58.000]   the system was an annoyance to Mark Zuckerberg. He was forced out of Facebook when instead he
[00:10:58.000 --> 00:11:02.960]   should have become the next CEO of Facebook because his depth of knowledge and his understanding,
[00:11:02.960 --> 00:11:08.720]   he was a perfect CEO, etc., etc. But at the very end, she quotes system, the creator of Instagram,
[00:11:08.720 --> 00:11:13.360]   saying something that I think could apply to what we were just talking about. Social media
[00:11:13.360 --> 00:11:21.200]   is in a pre-Newtonian moment. Instead of social media, let's say the internet is in a pre-Newtonian
[00:11:21.200 --> 00:11:28.240]   moment. We all understand that it works, but not how it works. Just as before Newton, we knew gravity
[00:11:28.240 --> 00:11:31.920]   existed, we just didn't know how it worked. There are certain rules that govern it and we have to
[00:11:31.920 --> 00:11:36.000]   make it our priority to understand the rules or we cannot control it. I'm not sure I agree with
[00:11:36.000 --> 00:11:41.200]   the control it thing. I think that would be a misguided mission to control it. But I think it
[00:11:41.200 --> 00:11:44.000]   is useful to understand the rules if you want to make it.
[00:11:44.000 --> 00:11:50.640]   When you said pre-Newtonian, I think you've talked about the Apple Newton or Fig Newton,
[00:11:50.640 --> 00:11:56.000]   but no, Sir Isaac, I think was the... Wow, I remember reading that quote too, and I think he's
[00:11:56.000 --> 00:12:04.480]   right. I think we get so carried away with how much has changed in the last 10, 15 years, but
[00:12:05.520 --> 00:12:13.360]   really it's a fraction of a second humanity-wise in terms of since the internet, let alone
[00:12:13.360 --> 00:12:21.600]   since the web or digital. It seems like it's always been here. By the way, we have in studio
[00:12:21.600 --> 00:12:27.280]   today, 47, 16-year-olds from Australia who are on a field trip to the United States. Hello, kids.
[00:12:27.280 --> 00:12:32.640]   They grew up with this stuff. It's been their whole lives podcasting from their point of view
[00:12:32.640 --> 00:12:37.280]   has probably been there the whole lives. It's about 14 years old. So since they were two,
[00:12:37.280 --> 00:12:43.760]   and certainly the internet has, so it might feel to them as if, oh man, I mean, what was... I don't
[00:12:43.760 --> 00:12:48.320]   even know what life was like before the internet, but for people, an oldster like me, it's pretty... I
[00:12:48.320 --> 00:12:52.560]   do remember pre-internet days, and I don't think we understand it very well. I do think we're in a
[00:12:52.560 --> 00:12:57.520]   pre-Newtonian moment. I think it was one of the interesting conversations we should be having
[00:12:57.520 --> 00:13:02.960]   is instead of saying, oh my gosh, is podcasting have a future? What's going to happen podcasting?
[00:13:02.960 --> 00:13:08.000]   I think we should all be talking about why, given the existence of podcasting, do we need
[00:13:08.000 --> 00:13:14.240]   television and radio? Because the whole model of television and radio is based on artificial
[00:13:14.240 --> 00:13:21.920]   scarcity and all these other barriers that enrich the controllers of those things. Everything
[00:13:21.920 --> 00:13:29.280]   should be a podcast. Everything. I agree. Or YouTube. I'll bet you, you guys probably don't watch TV
[00:13:29.280 --> 00:13:34.080]   very much. You probably watch a lot of YouTube though, right? Yeah, they're nodding. I know even
[00:13:34.080 --> 00:13:40.080]   with my older kids, my son is 23. YouTube is the source for everything for him. He wanted to learn
[00:13:40.080 --> 00:13:45.120]   how to cook. He watched YouTube videos. Yeah, my daughter's the same. Yeah. Who needs knowledge?
[00:13:47.280 --> 00:13:53.920]   Well, but that does raise another issue, which is it's these, part of the reason we like them is
[00:13:53.920 --> 00:13:59.920]   just as you were describing, Mike, they're unfiltered, unedited. They're very personal. They're very
[00:13:59.920 --> 00:14:06.000]   passionate, but they are, and there's no gatekeeper. Nothing wrong. But for that reason,
[00:14:06.000 --> 00:14:12.880]   we have this argument with Jeff all the time because Jeff Jarvis will blow out gasket, say,
[00:14:12.880 --> 00:14:16.400]   well, you couldn't trust the, you know, how could you? Where are you saying you could trust the New
[00:14:16.400 --> 00:14:21.600]   York Times 20 years ago? It's just it appeared to be trustworthy. They're appeared because they're
[00:14:21.600 --> 00:14:25.840]   gatekeepers. They're appeared to be some. There were more filters. There are more filters. It doesn't
[00:14:25.840 --> 00:14:30.400]   mean it wasn't wrong from time to time. I also think there's, and Jeff and I have talked about
[00:14:30.400 --> 00:14:38.880]   this too, there's, there isn't just the occasional wacko who has a YouTube channel peddling conspiracy
[00:14:38.880 --> 00:14:44.720]   theories. There's also YouTube, the engine, the algorithmic driven recommendation engine that is
[00:14:44.720 --> 00:14:53.280]   feeding those videos to users, to audiences. And that's an element that we haven't had before,
[00:14:53.280 --> 00:14:55.520]   one that we don't really understand very well, I don't think.
[00:14:55.520 --> 00:14:59.840]   Yeah. So I would, I would quibble a little bit with Kevin's system about the need to control it,
[00:14:59.840 --> 00:15:07.520]   but it's true that what we don't understand is how it works for, I mean, and when you look at
[00:15:07.520 --> 00:15:12.800]   something like Twitter, which kind of like many social sites descended into a dumpster fire,
[00:15:12.800 --> 00:15:18.800]   how do you keep social sites from doing that? We don't really understand very much of that.
[00:15:18.800 --> 00:15:24.080]   I don't know. I think if you, if you think about Twitter, or even Facebook for that matter,
[00:15:24.080 --> 00:15:30.080]   there's literally nothing that has ever existed in history that is like that. So there's nothing
[00:15:30.720 --> 00:15:37.840]   that has ever connected that many people in that way. So there is no like metaphor, you can say,
[00:15:37.840 --> 00:15:43.040]   oh, well, people didn't trust newspapers or people didn't trust, you know, TV anchors. Well,
[00:15:43.040 --> 00:15:47.920]   that's, they're really not comparable. There's never been a network that connected two billion
[00:15:47.920 --> 00:15:54.640]   people on a daily basis. There's never been the ability to share your thoughts instantaneously with
[00:15:54.640 --> 00:16:01.760]   potentially hundreds of millions of people. And we just don't know how to behave, like how that
[00:16:01.760 --> 00:16:07.600]   works. So you post something on Twitter, is that like you publishing it in a newspaper article,
[00:16:07.600 --> 00:16:12.800]   or is it like you stapling something to a telephone poll, or is it like you standing in a part or
[00:16:12.800 --> 00:16:19.360]   a public square yelling? It's like all of those things. And also things that we don't even understand.
[00:16:19.360 --> 00:16:23.760]   Yeah. Well, and there's also this, the truth about Twitter is very solipsistic, that it's a
[00:16:23.760 --> 00:16:27.760]   bunch of journalists on Twitter. How many of you young people use Twitter?
[00:16:27.760 --> 00:16:33.760]   How many of you are so about not many? How about Facebook? How many of you are on Facebook?
[00:16:33.760 --> 00:16:39.280]   A little more. How about Snapchat? How many of you are in almost all of you? Instagram?
[00:16:39.280 --> 00:16:45.200]   Instagram? Yeah, that's the most, right? So that's the other thing is,
[00:16:45.200 --> 00:16:49.760]   Twitter gets, I think a disproportionate amount of attention, especially in the news.
[00:16:49.760 --> 00:16:56.000]   Agreed. Yeah, agreed. It's not really, it's a small,
[00:16:56.000 --> 00:17:02.800]   selecting point for people. But this is the interesting thing about Twitter is,
[00:17:02.800 --> 00:17:10.160]   that's not an inconsequential demographic. All journalists. And journalists get enormous amounts of
[00:17:10.160 --> 00:17:16.480]   stories and interactions, stuff like that from Twitter. So really, I see Twitter,
[00:17:17.840 --> 00:17:24.720]   if I'm going to pick a metaphor that's off of Matthew's list, it's almost like an adjunct
[00:17:24.720 --> 00:17:28.080]   to the world of journalism, to a certain extent. I mean, you were talking about-
[00:17:28.080 --> 00:17:31.920]   It's one of your sources. Right. You were talking on Twitter, making the observation that everybody's
[00:17:31.920 --> 00:17:36.320]   like, "Oh my gosh, you know, President Trump is so fantastic at using Twitter. He's like,
[00:17:36.320 --> 00:17:41.920]   uses Twitter to, no, he uses Twitter to play the media like a fiddle."
[00:17:43.040 --> 00:17:47.760]   And if you look at actual- Right. But if you look at actual tweets during the election,
[00:17:47.760 --> 00:17:53.840]   it seemed to be all Trump all the time. Other politicians, including Barack Obama and others,
[00:17:53.840 --> 00:17:59.600]   were getting far more retweets, far more engagement, far more comments than Trump's tweets. But the
[00:17:59.600 --> 00:18:05.360]   media was picking up on those highly newsworthy, crazy tweets from Trump. And every day he was
[00:18:05.360 --> 00:18:09.280]   using Twitter to get in the New York Times to get in all these other publications. So really,
[00:18:09.280 --> 00:18:14.480]   Twitter, I think the biggest impact is its total access to almost all journalists.
[00:18:14.480 --> 00:18:20.640]   Isn't it the case, though, that all of this hand-ringing over Facebook and Twitter only
[00:18:20.640 --> 00:18:25.440]   started when Trump won the election? That before then- Yeah. But-
[00:18:25.440 --> 00:18:30.160]   And this is what we were talking about on Twitter, Mike. I know you, it sounds like you listened.
[00:18:30.160 --> 00:18:38.800]   Is I would submit that that's because he figured out how to use both to great effect.
[00:18:39.120 --> 00:18:42.800]   He's a troll. He's a troll. He's been a troll for years.
[00:18:42.800 --> 00:18:49.680]   He's the pro- with a name. But- I mean, the fact of the matter is he or that guy Pascal or whatever,
[00:18:49.680 --> 00:18:54.640]   he figured out Parscale, how to use it better than anybody.
[00:18:54.640 --> 00:18:59.680]   Yeah. But it's interesting if you look at- This is something I think about often.
[00:18:59.680 --> 00:19:07.440]   When it comes to Facebook and how to use it, Barack Obama's campaign, there were a number of
[00:19:08.000 --> 00:19:13.840]   really laudatory articles written about how well they were using digital media, in particular,
[00:19:13.840 --> 00:19:20.400]   Facebook, and how they had figured out how to kind of mobilize their followers by using Facebook and
[00:19:20.400 --> 00:19:25.920]   so on, had all these digital smart guys figuring it out. And then- And it was all very positive.
[00:19:25.920 --> 00:19:32.000]   And then as soon as Trump and the right wing did exactly the same thing, it became-
[00:19:32.000 --> 00:19:34.800]   It was not exactly the same thing. It wasn't exactly the same thing.
[00:19:34.800 --> 00:19:41.600]   No, it was different. But to be fair, what Obama did in 2008 was create a great digital
[00:19:41.600 --> 00:19:46.000]   group that understood these things. They collected a lot of numbers. I know they got mine
[00:19:46.000 --> 00:19:51.200]   because they said they want updates or whatever. And then they were able to use those numbers
[00:19:51.200 --> 00:19:57.200]   and those Facebook accounts in 2012 in the re-election effectively.
[00:19:57.200 --> 00:19:58.960]   But the message is targeting.
[00:19:58.960 --> 00:19:59.920]   But even what they did was-
[00:19:59.920 --> 00:20:00.720]   Which is targeting.
[00:20:00.720 --> 00:20:05.920]   They used targeting, but what they did was nothing compared with what Brad Parscale and the Trump
[00:20:05.920 --> 00:20:07.040]   team did. I mean-
[00:20:07.040 --> 00:20:07.840]   The biggest difference-
[00:20:07.840 --> 00:20:08.720]   I just thought it was interesting that-
[00:20:08.720 --> 00:20:12.000]   Go ahead, Mike, and then, and then, Matt.
[00:20:12.000 --> 00:20:13.920]   Yeah, the biggest difference is that
[00:20:13.920 --> 00:20:18.880]   Obama organized self-selected supporters.
[00:20:18.880 --> 00:20:19.280]   That's right.
[00:20:19.280 --> 00:20:24.000]   People voluntarily sign up to be part of this moment. And they organized that brilliantly.
[00:20:24.960 --> 00:20:32.080]   What the Trump campaign did was they used targeting to manipulate people who never even
[00:20:32.080 --> 00:20:35.120]   had any idea they were involved in anybody's political campaign.
[00:20:35.120 --> 00:20:38.880]   Right. Well, and you could say, and I'm not sure if this is true, but we're looking into it,
[00:20:38.880 --> 00:20:47.600]   there is an interesting correlation between the Russian activities on Facebook, which very closely
[00:20:47.600 --> 00:20:52.640]   mirrored Trump's own strategies down to the state and the localities. Whether there was
[00:20:52.640 --> 00:20:56.400]   communication between the two or not is yet to be determined.
[00:20:56.400 --> 00:21:01.520]   But maybe just the Russians are smart enough to see what Trump was going for and to support it.
[00:21:01.520 --> 00:21:05.600]   But it wasn't Trump's campaign by itself. He got a lot of help.
[00:21:05.600 --> 00:21:12.640]   And we're still debating how much of an effect that had on the election.
[00:21:12.640 --> 00:21:18.000]   Did it actually influence the outcome? This is something that even sociologists and experts in
[00:21:18.000 --> 00:21:20.080]   this type of stuff are not clear on.
[00:21:20.080 --> 00:21:24.080]   I have to say, Matthew, it seems like the needle is moving more in the direction of it did.
[00:21:24.080 --> 00:21:29.280]   I don't know. I thought so too. There was one particular
[00:21:29.280 --> 00:21:32.560]   academic whose name escapes me now who just came out with a book.
[00:21:32.560 --> 00:21:35.280]   Yes. That's who we were talking about on the.
[00:21:35.280 --> 00:21:36.320]   Believe. Yeah.
[00:21:36.320 --> 00:21:42.000]   Yeah. That it actually did tip things over. So I've spoken to several sociologists who study
[00:21:42.000 --> 00:21:48.560]   this area and they disagree. They don't think that there is as much evidence.
[00:21:49.840 --> 00:21:53.120]   But obviously there's still a debate. The big question is, did it
[00:21:53.120 --> 00:21:59.120]   push people in a new direction or did it just sort of reinforce
[00:21:59.120 --> 00:22:02.960]   things that they were already thinking or planning to do?
[00:22:02.960 --> 00:22:05.520]   Well, it takes a village to lose an election.
[00:22:05.520 --> 00:22:08.800]   There's somebody who ran for president recently.
[00:22:08.800 --> 00:22:15.680]   And we're not talking about whether the needle moved nationwide.
[00:22:15.680 --> 00:22:17.760]   There was no way Trump was ever going to run in California.
[00:22:18.320 --> 00:22:20.560]   We're talking about a couple of states with three states.
[00:22:20.560 --> 00:22:21.680]   Yeah. That's how you win.
[00:22:21.680 --> 00:22:24.880]   You do. Not only were those electro college to your advantage
[00:22:24.880 --> 00:22:28.080]   and within 80,000 folk margin, he was able to win.
[00:22:28.080 --> 00:22:31.520]   Those are the things that were killer claim failed to go and press on those states.
[00:22:31.520 --> 00:22:32.160]   Yeah. She didn't do it.
[00:22:32.160 --> 00:22:32.640]   Yeah.
[00:22:32.640 --> 00:22:33.120]   Okay.
[00:22:33.120 --> 00:22:36.080]   It was a combination of both of those things.
[00:22:36.080 --> 00:22:39.840]   And so, you know, it's hard. You know, at the end of the day, it's hard to lay blame.
[00:22:39.840 --> 00:22:42.320]   Even if you don't blame it all.
[00:22:42.320 --> 00:22:43.040]   All right.
[00:22:43.040 --> 00:22:46.560]   But I only bring this up in the context of if somebody does,
[00:22:46.560 --> 00:22:51.200]   if we get into that post-Newtonian moment, when somebody writes the, you know, the
[00:22:51.200 --> 00:22:58.320]   Prince Chippia social network, and maybe it's Brad Parscale, that's going to be a scary moment.
[00:22:58.320 --> 00:23:02.640]   Because I think actually we might be seeing that moment where people have figured out,
[00:23:02.640 --> 00:23:09.840]   here's how you use the internet to sway opinion to.
[00:23:09.840 --> 00:23:12.640]   I mean, it's not just elections.
[00:23:12.640 --> 00:23:18.800]   The biggest problem is not the persuasion of people in one political direction or another.
[00:23:18.800 --> 00:23:23.200]   The thing that the Soviet Union, the KGB discovered during the Soviet Union,
[00:23:23.200 --> 00:23:28.080]   was dysinformatsia, which is where we get the word disinformation.
[00:23:28.080 --> 00:23:31.440]   The goal is not to persuade about something or another.
[00:23:31.440 --> 00:23:36.800]   The goal is to reduce and eliminate trust in any source of information.
[00:23:36.800 --> 00:23:39.760]   And so nobody knows what's true. Everybody's like, well, who knows?
[00:23:39.760 --> 00:23:46.480]   It's, and that's the thing that's being exported by Putin, former KGB officer.
[00:23:46.480 --> 00:23:51.120]   And the thing that kind of Trump is kind of picking up on as well, it works.
[00:23:51.120 --> 00:23:55.920]   To, if you can get everybody both fearful and also confused about what's true,
[00:23:55.920 --> 00:23:58.080]   you can, you can do all kinds of things.
[00:23:58.080 --> 00:24:05.440]   I actually, I remember reading Facebook, even before kind of the election, and there was all the
[00:24:05.440 --> 00:24:10.640]   attention on the Russian internet research agency, Facebook had an internal security
[00:24:10.640 --> 00:24:17.440]   report in which they discussed the actions of what appeared to be government actors or
[00:24:17.440 --> 00:24:23.040]   government influence actors. And they didn't specifically say Russia, which caused a bunch
[00:24:23.040 --> 00:24:26.640]   of controversy in the head of security left partly as a result.
[00:24:26.640 --> 00:24:32.160]   But in the report, I remember them mentioning that it was interesting to see how much of the
[00:24:32.160 --> 00:24:39.680]   activity wasn't actually designed to be ideologically, you know, balanced in one way or the other,
[00:24:39.680 --> 00:24:46.400]   ideologically sort of rigged to right or left. It was simply designed to confuse people and
[00:24:46.400 --> 00:24:52.800]   get them doubting even their own senses, doubting what was true, doubting who to believe.
[00:24:52.800 --> 00:24:58.800]   And, you know, that that was enough to kind of just create chaos and kind of get people
[00:24:58.800 --> 00:25:04.320]   paralyzed within decision. And ended, I think the most interesting fact about this, which
[00:25:04.320 --> 00:25:11.760]   is underappreciated is the fact that the Russian trolls actually organized street marches
[00:25:11.760 --> 00:25:14.400]   successfully in the United States. People are marching in the streets.
[00:25:14.400 --> 00:25:15.120]   Not amazing.
[00:25:15.120 --> 00:25:18.160]   Because Putin organized those protests.
[00:25:18.160 --> 00:25:18.880]   Yeah.
[00:25:18.880 --> 00:25:19.520]   Totally amazing.
[00:25:19.520 --> 00:25:20.640]   Astonishing. Yeah.
[00:25:20.640 --> 00:25:26.240]   Matthew Ingram is here from the Columbia Journalism Review, and this all started because you said
[00:25:26.240 --> 00:25:30.640]   the bubble hit person podcasting. You see what Matthew is. See, you see what you've done.
[00:25:30.640 --> 00:25:34.000]   You never should have said it. No, no, I completely agree. In fact,
[00:25:34.000 --> 00:25:37.200]   I've been hoping the pop the bubble would burst. It's been also crowded in here.
[00:25:37.200 --> 00:25:38.320]   You use a bubble.
[00:25:38.320 --> 00:25:48.080]   I want to buy my tool at bulbs in peace. Also, Mike Elgin, he is at Elgin.com.
[00:25:48.080 --> 00:25:54.080]   There's a lot to talk about. In fact, let's when we come back, do the Google change log.
[00:25:54.960 --> 00:25:58.240]   And you guys are all going to get up and walk out on me when soon?
[00:25:58.240 --> 00:26:01.200]   No, shoot. 255.
[00:26:01.200 --> 00:26:05.760]   255. Okay. Remind me to say something nasty about Australia, a 254.
[00:26:05.760 --> 00:26:10.640]   And then we can stage the greatest walk out. It'd be such a great moment. I might even,
[00:26:10.640 --> 00:26:14.160]   we might even get on Twitter if we do this right, kids.
[00:26:14.160 --> 00:26:16.080]   It's called disinformation.
[00:26:16.080 --> 00:26:16.800]   This information.
[00:26:16.800 --> 00:26:17.280]   This information.
[00:26:21.040 --> 00:26:28.960]   d o dot c o slash twig digital ocean. And I am such a fan of digital ocean.
[00:26:28.960 --> 00:26:34.240]   Digital ocean makes it the easiest cloud platform you can use.
[00:26:34.240 --> 00:26:41.280]   If you're a developer, if you're setting up a web service, application service of any kind,
[00:26:41.280 --> 00:26:45.200]   you can deploy, manage and scale applications so easily on digital ocean.
[00:26:45.200 --> 00:26:50.160]   And I love it. They call them droplets. Droplets are scalable.
[00:26:50.400 --> 00:26:52.880]   Virtual machines.
[00:26:52.880 --> 00:27:00.320]   With that on storage, you can add storage, you can add security, you can add monitoring capabilities,
[00:27:00.320 --> 00:27:01.360]   you can add backup.
[00:27:01.360 --> 00:27:06.480]   You could choose from standard or CPU's optimized droplets and customize from there.
[00:27:06.480 --> 00:27:12.560]   You get an easy to use control panel, an API that lets you spend more time coding,
[00:27:12.560 --> 00:27:14.400]   less time managing infrastructure.
[00:27:14.400 --> 00:27:19.520]   Access to the compute resources you need at the lowest rates.
[00:27:20.480 --> 00:27:26.560]   In fact, I can't believe I've been running a blog on there for like five bucks a month.
[00:27:26.560 --> 00:27:34.880]   I love it because once I created droplet in digital ocean, I can use my SSH public key to log in.
[00:27:34.880 --> 00:27:41.440]   So it's very secure. They've really done a nice job of locking it down 99.99% uptime.
[00:27:41.440 --> 00:27:48.000]   And it's an SLA. So they guarantee it. Cloud firewalls. Monitoring and alerting.
[00:27:48.640 --> 00:27:51.680]   Full DNS management. Global data centers.
[00:27:51.680 --> 00:27:56.880]   Enterprise SSDs. And as I mentioned, that easy to use API.
[00:27:56.880 --> 00:28:02.400]   So let me think of an example we can do with this here.
[00:28:02.400 --> 00:28:08.640]   Let's say you're a 16 year old in Australia and you like to write Python code.
[00:28:08.640 --> 00:28:14.000]   And you have this great idea for an online game that you'd like to create.
[00:28:14.000 --> 00:28:22.160]   That sounds good, right? And maybe you'd like to do it for a couple of bucks.
[00:28:22.160 --> 00:28:26.000]   You don't have a lot of money, right? But you want to do a proof of concept.
[00:28:26.000 --> 00:28:29.280]   And what if it became popular? What do you like to be able to scale it up?
[00:28:29.280 --> 00:28:31.840]   Let's create a new droplet. I'll show you how easy this is.
[00:28:31.840 --> 00:28:36.800]   You see, by the way, with Create, I can also add a lot of other things.
[00:28:36.800 --> 00:28:41.440]   Cloud firewalls, floating IPs, load balancers, all sorts of stuff.
[00:28:41.440 --> 00:28:46.720]   Spaces is for storing and serving static assets. So if you could have a space for an image.
[00:28:46.720 --> 00:28:50.080]   Now, a couple of ways to do this, I can choose from Ubuntu, free BSD,
[00:28:50.080 --> 00:28:55.120]   Fedora, Debian, CentOS. But really, maybe I want to use a container distribution.
[00:28:55.120 --> 00:29:00.880]   Or how about a one click app? Discourse, doku, lamp. Let's set up a lamp stack or a MySQL stack.
[00:29:00.880 --> 00:29:05.200]   Ruby on rails. WordPress, GitLab, PHP, My Admin.
[00:29:05.200 --> 00:29:09.040]   You want to use Python? Let's use Django, right? That's the Python framework.
[00:29:09.040 --> 00:29:15.520]   So I'm going to click on Django. Now I have to choose the droplet size, everything from a
[00:29:15.520 --> 00:29:21.360]   $5 a month, one gigabyte memory, one virtual CPU, 25 gigabytes space and a terabyte.
[00:29:21.360 --> 00:29:26.720]   That's actually quite a bit of storage. You can even pay for backup, if you want, automatically.
[00:29:26.720 --> 00:29:32.640]   20% of the droplet price. So that's like a buck block storage. Let's pass on that.
[00:29:32.640 --> 00:29:37.120]   Now, where's your server? Amsterdam, Singapore, London, Frankfort, Toronto, Bangalore. Let's do,
[00:29:37.120 --> 00:29:42.800]   I always choose San Francisco too. I like to add my SSH key to it. That way I can log in.
[00:29:42.800 --> 00:29:52.720]   And let's name this Leo's Django. So now this is going to be the Django framework running.
[00:29:52.720 --> 00:29:58.160]   Now let's see how long it takes to set Leo's Django up. Basically, I'm provisioning a server
[00:29:58.160 --> 00:30:03.760]   that I can use, that I can write code on, that could post code on. It's public if I want it to be.
[00:30:04.560 --> 00:30:10.720]   I can add resources. If I write the ultimate tool, I can say, hey, it's a huge success.
[00:30:10.720 --> 00:30:15.440]   Yes, I spelled Django wrong. DJA, NOG, it's Leo's JNOG.
[00:30:15.440 --> 00:30:20.320]   Within a couple of minutes. I mean, what's the last time you set up a server?
[00:30:20.320 --> 00:30:25.120]   Last time you set up a server, you probably had to wait a day or two. I will provision that.
[00:30:25.120 --> 00:30:30.080]   We'll let you know. We'll get back to you. Over 150,000 businesses, including some of the world's
[00:30:30.080 --> 00:30:34.800]   fastest growing startups use digital ocean to remove infrastructure friction,
[00:30:34.800 --> 00:30:44.160]   deliver industry leading price performance. And boom, by the way, at 104.248.65.30, I've got a
[00:30:44.160 --> 00:30:52.560]   server running. It's running Django on Ubuntu. It's awesome. I can log in right here in a console.
[00:30:52.560 --> 00:30:59.040]   I can use SSH. I can program on it. That's how fast and easy it is. And I'll tell you what,
[00:30:59.040 --> 00:31:02.960]   I'm going to make it even more affordable because if you sign up today, you'll get a free $100 credit
[00:31:02.960 --> 00:31:13.440]   at dio.co/twig digital ocean, dio.co/twig for a free $100 credit. I've used this for a long time,
[00:31:13.440 --> 00:31:19.600]   for projects, for ideas, for blogging. It's all sorts of great things. Set up a wiki. Anything you
[00:31:19.600 --> 00:31:25.680]   can do on the internet, you can do a digital ocean and you can get $100 credit right now.
[00:31:25.680 --> 00:31:28.800]   dio.co/twig.
[00:31:28.800 --> 00:31:37.280]   Leo report with Matthew Ingram and Mike Elgin, Stacy and Jeff have the week off, but you know,
[00:31:37.280 --> 00:31:41.600]   it doesn't get the week off ever. The Google change log.
[00:31:41.600 --> 00:31:45.600]   The Google change log.
[00:31:45.600 --> 00:31:54.880]   We brought this back for the 20th anniversary of Google. Are you laughing?
[00:31:54.880 --> 00:31:56.560]   I love the drums. I love the drums.
[00:31:56.560 --> 00:32:04.960]   Some user, somebody made that for me some time ago. 20 years of Google, they celebrated their
[00:32:04.960 --> 00:32:10.000]   anniversary. It's unclear when exactly Google's actual anniversary is.
[00:32:10.000 --> 00:32:19.680]   Sometime in September, probably, there's when Google first incorporated, there's various dates
[00:32:19.680 --> 00:32:26.080]   in here, but Google has. Here's some of the numbers over 20 years. Google Maps has helped
[00:32:26.080 --> 00:32:31.040]   people find their way with driving directions in 240 plus countries, spanning 40 million miles of
[00:32:31.040 --> 00:32:37.360]   road, 83 plus trips to the moon and back, connecting people to 150 million places around the world.
[00:32:37.360 --> 00:32:44.800]   Google Assistant now in 20 languages. Translate lets you say thank you across 100 languages.
[00:32:44.800 --> 00:32:50.640]   So did you guys from Australia, did you use translate when you got here so that people would understand?
[00:32:50.640 --> 00:32:55.680]   Yeah. 143 billion words a day translated.
[00:32:55.680 --> 00:33:03.600]   Is that all? It always blows me away when I see these stats from Google and Facebook and
[00:33:03.600 --> 00:33:10.080]   companies like that, just the volume of traffic they can handle. Half a billion people use Google
[00:33:10.080 --> 00:33:19.520]   photos every month, more than 1.2 billion photos and videos a day. Photos is freed up, 410 petabytes
[00:33:19.520 --> 00:33:31.360]   worth of space on people's phones. Autocomplete and search estimates, they estimate people save 200
[00:33:31.360 --> 00:33:39.440]   years of typing a day, 200 years of typing a day without a complete. But it gets people to,
[00:33:39.440 --> 00:33:44.560]   it forces them to correct the word duck all the time. I know what you mean.
[00:33:44.560 --> 00:33:49.600]   1 billion people. I almost never want to say duck.
[00:33:49.600 --> 00:34:00.080]   You must have said duck once. That's why it's your Google Assistant got a makeover this week.
[00:34:00.080 --> 00:34:04.000]   I don't know if you saw it on the phones. I didn't we talk about this last week. I feel like we
[00:34:04.000 --> 00:34:10.960]   talked, we didn't. So this is the new, the new look of the Google Assistant. It's a lot better,
[00:34:10.960 --> 00:34:16.720]   isn't it? Bigger visuals, new controls. It's more compatible with two coming devices,
[00:34:16.720 --> 00:34:23.920]   one of which is the Pixel 3, which has a wireless dock that holds the phone up at a viewing angle.
[00:34:23.920 --> 00:34:30.720]   And so it looks like the Pixel 3 will be usable in its dock as a smart display. And then of course,
[00:34:30.720 --> 00:34:36.640]   the Google Smart Display, which is forthcoming. It's basically a Google Home device with a screen.
[00:34:36.640 --> 00:34:41.920]   And so we're adding more visual elements and touch screen elements to the Assistant and moving
[00:34:41.920 --> 00:34:47.120]   away from just text, text, question, text answer. So it's very cool.
[00:34:48.560 --> 00:34:53.760]   I quickly want to buy Leo dot page, but I'm sure it's already too late.
[00:34:53.760 --> 00:34:58.800]   That's the new top level domain from Google registry dot page.
[00:34:58.800 --> 00:35:06.160]   I'm sure Larry dot page is taken. Larry's got one. Yeah. So does Ellen. Ellen page. Home dot page.
[00:35:06.160 --> 00:35:10.880]   Christopher Neiman dot page, web dot page. I see all the good pages are gone.
[00:35:12.240 --> 00:35:20.720]   Turn the page. Word is image looks available. How do I get it? Quick, quick. I have no idea.
[00:35:20.720 --> 00:35:26.240]   Google registry. Oh, I know there's people out there doing it right now.
[00:35:26.240 --> 00:35:34.880]   It's probably very very very. Hurry. Hurry. Hurry. Hurry. I'd like to add.
[00:35:34.880 --> 00:35:40.800]   If you're interested, registered, follow registered. Oh, I have to go through somebody. Oh, man. Oh,
[00:35:40.800 --> 00:35:45.200]   forget it. I have lost it already. Dang it. Why did I?
[00:35:45.200 --> 00:35:53.360]   I wasn't thinking they also have dot how and dot. I don't understand Japanese.
[00:35:53.360 --> 00:35:58.240]   I don't know what that means. What is that? Anybody speak Japanese? What does that mean?
[00:35:58.240 --> 00:36:02.320]   Everyone dot everyone. That's awesome.
[00:36:02.320 --> 00:36:08.000]   Everyone's got that page. How do I type that in though?
[00:36:08.880 --> 00:36:22.560]   Try your best. You don't have to put that on my business card. I can do it. Yeah.
[00:36:22.560 --> 00:36:27.520]   I'm putting that on my business card. Leo dot squiggles. Leo dot everyone. How do you say
[00:36:27.520 --> 00:36:36.400]   me not Japanese mean not. Leo, I love that. But I think I'll take page instead. If you don't.
[00:36:37.600 --> 00:36:41.920]   I'm going to I'm going to miss out on page. Can we hold the release of this podcast for an extra
[00:36:41.920 --> 00:36:51.840]   hour while I'm too late? We do it live. There is a new mini in town. Looks like a mint.
[00:36:51.840 --> 00:37:01.040]   It's a new aqua Google Home Mini. How exciting is that? Look, I did say it would all be good.
[00:37:02.400 --> 00:37:07.920]   Just new stuff from Google. New Android app. Actually, yeah, it actually might be cool. The
[00:37:07.920 --> 00:37:14.640]   Google Home thing might be might connect to a greater hub that is like controls the home and
[00:37:14.640 --> 00:37:18.560]   stuff like that could be the beginning. Remember, does anybody remember Android? What was it?
[00:37:18.560 --> 00:37:24.320]   Android home or something like that? Google Google like five years ago introduced this new home
[00:37:24.320 --> 00:37:30.960]   operating system. What? And then yeah, at Google I/O, Android at home or something like that.
[00:37:30.960 --> 00:37:36.480]   And then it just fizzled and went away and nobody ever heard from it again. But they were like
[00:37:36.480 --> 00:37:45.840]   demoing stuff and pretty amazing. And so I think they're finally getting around to that.
[00:37:45.840 --> 00:37:49.840]   And they have to to compete with Amazon, which is definitely wants to control your home.
[00:37:49.840 --> 00:37:56.480]   But yeah, we'll see. I think it's going to be a lot more capable than the current one. It's
[00:37:56.480 --> 00:38:01.600]   going to look exactly the same, but it's going to be much more capable. Is it a software platform?
[00:38:01.600 --> 00:38:07.360]   Is it a hardware platform? It's similar, except it's going to support both Wi-Fi and Bluetooth.
[00:38:07.360 --> 00:38:12.880]   And so it seems like that the addition of Bluetooth is going to be really interesting
[00:38:12.880 --> 00:38:18.240]   for the platform because it's like, what are they connecting to right now? We really don't know.
[00:38:18.240 --> 00:38:23.200]   But it's, you know, we're what a week away from finding out. So
[00:38:24.400 --> 00:38:30.160]   it should be. Yeah, it should be cool. I think that product is going to surprise people
[00:38:30.160 --> 00:38:33.360]   quite a bit. I just don't know how.
[00:38:33.360 --> 00:38:41.840]   We will be streaming the event. It starts 8 a.m. Pacific, 11 a.m. Eastern time on Tuesday,
[00:38:41.840 --> 00:38:50.320]   October 9th. And we hope to learn some very interesting things. Certainly new Pixel phone.
[00:38:50.320 --> 00:38:54.080]   And you said there'll be a new charging stand or holding a grasping stand?
[00:38:54.080 --> 00:38:57.920]   Yeah, it's a cradle for wireless charging.
[00:38:57.920 --> 00:39:05.440]   How do you know about this? There's leaked renders that they're going to show at the
[00:39:05.440 --> 00:39:10.800]   event on the 9th. And they got out there. They look perfectly legitimate. But it's basically
[00:39:10.800 --> 00:39:17.920]   around disk plugs in with probably USB-C at the bottom. And then it holds the phone at an angle.
[00:39:19.520 --> 00:39:28.880]   Yeah, it looks cool. And again, I think they're really shooting for expanding the usage of the
[00:39:28.880 --> 00:39:33.120]   smart display by actually turning Google phones into a smart display. And hopefully...
[00:39:33.120 --> 00:39:38.640]   Actually, I got a charger for my phone that would do that, that would hold it up at an angle
[00:39:38.640 --> 00:39:46.000]   like that specifically because I wanted to see it. It actually just rotates photos, but still,
[00:39:46.000 --> 00:39:50.720]   it's another display you could use for different things. The reports on this one are that there's
[00:39:50.720 --> 00:39:55.440]   going to be a completely different display for when it's in that mode. So when you're like in,
[00:39:55.440 --> 00:40:00.720]   you're in smart, and it'll probably be very similar to the Google smart display product that
[00:40:00.720 --> 00:40:05.760]   they're going to be announcing. It'll essentially probably be a Google smart display. So it's just
[00:40:05.760 --> 00:40:08.400]   completely convert over to that when you plug it into this device.
[00:40:10.320 --> 00:40:14.480]   By the way, I've just been trying to register Leo.page. I hope you would keep talking.
[00:40:14.480 --> 00:40:19.840]   Well, I went to Google domains and said, "You can't do that till October 9th." So this is going to be
[00:40:19.840 --> 00:40:26.240]   a big day after that. Can you grab Jimmy.page? We did see. I did get an email because I'm a Google
[00:40:26.240 --> 00:40:31.360]   5 customer saying that pre-orders will begin immediately after the event for whatever it is they
[00:40:31.360 --> 00:40:36.880]   announce. Nice. So I presumably, that's the Pixel 3 that they're talking about.
[00:40:36.880 --> 00:40:41.360]   I'm torn because I really want an Apple watch, but I would rather have the Pixel 3.
[00:40:41.360 --> 00:40:46.080]   So I'm kind of on the fence right now whether I go with the watch or the Pixel 3.
[00:40:46.080 --> 00:40:52.640]   For people who were hoping to keep their old Pixel alive, good news. Google has launched a repair
[00:40:52.640 --> 00:40:59.600]   service for older pixels. This is from Android Police. Eventually, we'll cover all four Pixel phones.
[00:41:03.120 --> 00:41:08.000]   That's a good thing because basically, if you have one of the old pixels, good luck trying to get it
[00:41:08.000 --> 00:41:17.280]   fixed. That's good. I think some corporate responsibility, it's really a shame when you get
[00:41:17.280 --> 00:41:22.000]   these things and after two years they're obsolete and you can't get them fixed and you have to throw
[00:41:22.000 --> 00:41:25.680]   them out and they're perfectly good. They just want to sell new stuff.
[00:41:26.240 --> 00:41:33.840]   I bought my Pixel book at Best Buy and I was kind of in a rush and I went to buy it and I'm like,
[00:41:33.840 --> 00:41:39.040]   oh, I'll go sign up for the Google Care or whatever it's called later. And then when I went to sign
[00:41:39.040 --> 00:41:42.880]   up for it, they're like, nope, you can only get that if you buy it from the Google directly.
[00:41:42.880 --> 00:41:48.000]   Now I have this Pixel book that's really, I got that high end one, so it's pretty expensive.
[00:41:48.000 --> 00:41:54.000]   I have no coverage. To the best of my knowledge, there's no plan to repair it, I guess other than
[00:41:54.000 --> 00:41:57.680]   bringing it back to Best Buy, which is like, wow, I'm not sure I want to do that.
[00:41:57.680 --> 00:42:02.320]   So they really need to get their act together if they're going to compete in any way against
[00:42:02.320 --> 00:42:06.800]   Apple because Apple's just, everybody knows, you can just take it to your friendly neighborhood
[00:42:06.800 --> 00:42:14.320]   Apple store, go to the Genius bar, they'll probably fix it. Apple's light years ahead of Google on
[00:42:14.320 --> 00:42:20.960]   and it's not about the repairs as much as it is about getting people to purchase in the first
[00:42:20.960 --> 00:42:27.680]   place. The knowledge that they're going to have this service is going to really facilitate people
[00:42:27.680 --> 00:42:34.080]   buying pixels, I think. Have you used the new Google Maps with the special commute tab enabled?
[00:42:34.080 --> 00:42:45.920]   No. I work from home, so I commute it. There's no tab for your cross home commute.
[00:42:46.880 --> 00:42:51.840]   Well, I don't really have much of a commute either, but what's really good, and this is something I'm
[00:42:51.840 --> 00:42:57.120]   really glad they added you, you do mixed mode commutes. So you could drive, walk, and take mass
[00:42:57.120 --> 00:43:03.520]   transit and it will put them all together for you, which is great, including a arrival time based
[00:43:03.520 --> 00:43:08.960]   on all three of those. So that's coming to Google Maps and iOS and Android and an E-minute now.
[00:43:08.960 --> 00:43:15.760]   Pay attention to which mode you're in because I had an experience where I got directions when we
[00:43:15.760 --> 00:43:24.160]   were in Italy and almost drove down a staircase because I would then walk mode as opposed to
[00:43:24.160 --> 00:43:29.840]   driving them. Probably a marble staircase. Don't do that. Isn't that what happened in the Italian
[00:43:29.840 --> 00:43:33.520]   Italy? You're allowed to do that, I think? No, you are. No, I saw the movie.
[00:43:33.520 --> 00:43:40.000]   By the way, it's not just Google. Nobody's offering the dot page domain until October 9th,
[00:43:40.000 --> 00:43:46.880]   but hey, thank you, GoDaddy. I can pre-register it at GoDaddy, which they said increases my chance,
[00:43:46.880 --> 00:43:53.680]   maximizing my chance of getting it for a meal $669.99. That seems worth it.
[00:43:53.680 --> 00:43:59.920]   If I want priority pre-registration, which lines up ahead of everyone waiting for general
[00:43:59.920 --> 00:44:07.360]   availability and increases my chance of getting this domain, it's a mere $819.99.
[00:44:08.400 --> 00:44:13.840]   So thank you, GoDaddy. I think Google said we're going to charge $12.
[00:44:13.840 --> 00:44:22.080]   And that's why you won't actually find Leo dot page. That's why I'll never find it because
[00:44:22.080 --> 00:44:27.280]   somebody will pay $100 to get it. Right. I'd pay that much for Larry dot page.
[00:44:27.280 --> 00:44:33.440]   What? You better think Larry's got that locked in. I don't know. How did Ellen Page get it?
[00:44:33.440 --> 00:44:38.720]   How did these? You got to know somebody else. She pays somebody to do that.
[00:44:38.720 --> 00:44:45.040]   I don't know what this means, but I like the name. Reimagining the commercial break with
[00:44:45.040 --> 00:44:52.800]   advanced TV solutions under the right. What can possibly be wrong with that? This is Google Ad
[00:44:52.800 --> 00:45:00.640]   Manager delivering seamless experiences and all screens dynamic ad insertion. What everybody
[00:45:00.640 --> 00:45:06.480]   wants this smarter TV ad breaks optimize your commercial breaks for the highest revenue based
[00:45:06.480 --> 00:45:11.680]   on the most relevant combinations of ads customized for each viewer. Well, how would they customize
[00:45:11.680 --> 00:45:21.040]   them for me? How do they know what I want to see? Oh, they know. That's what I love about
[00:45:21.040 --> 00:45:25.840]   Google, you know, on one hand, they say, you know, we respect your privacy. And then
[00:45:25.840 --> 00:45:32.480]   the hand, if you go to the ad blogs, this, Hey, you want to know what that guy did with his
[00:45:32.480 --> 00:45:38.160]   credit card the other day, we can tell you. We respect your privacy because that's our whole
[00:45:38.160 --> 00:45:45.520]   business plan is selling your privacy. That's why we respect it. Yeah. So this is a, I see this is a
[00:45:45.520 --> 00:45:52.240]   a trend that is mostly associated with Amazon. So for example, Amazon was the alternative to
[00:45:52.240 --> 00:45:56.480]   brick and mortar stores. And now they're talking about launching thousands of brick and mortar
[00:45:56.480 --> 00:46:00.880]   stores. Everybody's confused. Wait, I thought they were the anti brick and mortar, but that's never
[00:46:00.880 --> 00:46:07.120]   been the case. What Amazon represents is commerce that's algorithmically driven. That's, you know,
[00:46:07.120 --> 00:46:12.240]   has all this tracking, all this kind of stuff. This is another example of that. I thought YouTube
[00:46:12.240 --> 00:46:18.880]   was the anti TV. No, not really. It's basically algorithmically driven customized advertising
[00:46:18.880 --> 00:46:23.840]   theoretically. I actually think Google is not very good at producing customized ads,
[00:46:23.840 --> 00:46:31.040]   not as good as Facebook has become recently. But I think we're going to see Google breaking
[00:46:31.040 --> 00:46:37.200]   into TV in one way or another. And, you know, there are already everybody's TV set under the age
[00:46:37.200 --> 00:46:43.040]   of whatever it is 20. And so it's really not about TV versus online. It's really about,
[00:46:43.040 --> 00:46:50.480]   is it are you just sending out mass ads to everybody advertising beer and tires for the sports fans
[00:46:50.480 --> 00:46:56.560]   and, you know, etc. Or are you customizing based on purchase history, web activity and all that
[00:46:56.560 --> 00:47:00.480]   kind of stuff? And that's the business they're in. It doesn't matter if it's TV or not. So I
[00:47:00.480 --> 00:47:04.240]   think we're going to see a lot of that where these algorithmically generated, you know,
[00:47:04.240 --> 00:47:11.200]   that live and die by algorithms are going to break into, I guess, the real world or the analog world,
[00:47:11.200 --> 00:47:16.240]   whatever you want to call it, in any way they can, because that's really their model is to use
[00:47:16.240 --> 00:47:23.440]   algorithms and tracking and personalization to sell more stuff. So my only fear is that this
[00:47:23.440 --> 00:47:31.680]   future will be much like the web is now for me where I go to a site and it says, would you like
[00:47:31.680 --> 00:47:36.800]   to buy the pair of shoes that you just finished buying? Yeah, I love those recommendations.
[00:47:36.800 --> 00:47:42.160]   I mean, Amazon does it all the time. Yeah, the things I just literally just bought it.
[00:47:42.160 --> 00:47:47.040]   No, I don't need another one. It's enough. Thanks. I read that book already.
[00:47:47.040 --> 00:47:54.560]   What I'm confused about and maybe somebody has a handle on this, the best contextual advertising
[00:47:54.560 --> 00:47:59.200]   by far is on Instagram. I literally want every single thing to advertise to me.
[00:47:59.200 --> 00:48:02.560]   Is that amazing? And there's no other site. All these kids are not.
[00:48:03.360 --> 00:48:07.680]   How many of you have bought something on an Instagram ad? And I should hold up both hands.
[00:48:07.680 --> 00:48:13.600]   I used to. I took Instagram off my FFA, just the activity I can. I used to wake up in the middle
[00:48:13.600 --> 00:48:17.920]   of the night and I'd be bored. I'd flip through Instagram invariably. I'd buy something at three
[00:48:17.920 --> 00:48:23.120]   in the morning invariably. Yeah. The underwear I'm wearing right now, I bought on Instagram.
[00:48:23.120 --> 00:48:29.360]   You want to send it to you in the morning? Why did you get rid of your, that's why.
[00:48:30.240 --> 00:48:33.920]   What do you, why did I get rid of it? Because I was waking up at three AM going on Instagram
[00:48:33.920 --> 00:48:39.680]   and buying crap. It's like worse than the home shopping channel. So is that or hitting the
[00:48:39.680 --> 00:48:45.760]   ice cream or impulse control? Well, I guess you could say it's my fault or you could say,
[00:48:45.760 --> 00:48:50.640]   as Mike just did, they really nailed the algorithm. Yeah. Yeah, they are pretty good at it. I have
[00:48:50.640 --> 00:48:57.840]   to say I don't want anything on Instagram. I have come close, but every time before I do,
[00:48:58.480 --> 00:49:02.800]   I do a Google search for whoever's selling it. And in almost every case,
[00:49:02.800 --> 00:49:09.200]   it looks like it wouldn't be a great deal. Yeah. See, that's the thing. It works on the phone.
[00:49:09.200 --> 00:49:13.520]   You're not likely to do anything, but just, and then the other thing that they've really made too
[00:49:13.520 --> 00:49:19.360]   easy is between Apple Pay and Amazon Pay, I don't have to fill out a credit card,
[00:49:19.360 --> 00:49:22.320]   it just press a button and it's on its way. Yeah. Right.
[00:49:24.400 --> 00:49:30.000]   I did something really weird. I'm sorry to tell you. I did something really weird,
[00:49:30.000 --> 00:49:37.280]   which I think says a lot about online commerce. So I saw this really cool thing on Instagram.
[00:49:37.280 --> 00:49:41.440]   I had to have it. It was basically this orange, you probably seen it too. It's this orange hockey
[00:49:41.440 --> 00:49:46.560]   puck thing that gives you a Wi-Fi hotspot. I bought it. Yeah. Well, you shouldn't have because
[00:49:46.560 --> 00:49:51.360]   you have five of them. It's like a form of showrooming, except there's no showroom involved at all.
[00:49:51.360 --> 00:49:56.400]   So I went to Amazon to check the reviews, because we're also going to get reviews that are that
[00:49:56.400 --> 00:50:01.760]   clear and that numerous. So I went to Amazon, read the reviews and decided not to buy it,
[00:50:01.760 --> 00:50:09.120]   which I think is a really interesting phenomenon. Amazon reviews are actually a really good guide
[00:50:09.120 --> 00:50:13.280]   for certain products about whether or not you should buy them, because not only you can go
[00:50:13.280 --> 00:50:17.040]   straight to the one star reviews and find out what the, and if you see a common thread,
[00:50:17.040 --> 00:50:21.840]   oh, this really, everybody was saying, oh, it slows way down after a trivial amount of traffic,
[00:50:21.840 --> 00:50:25.440]   it slows way down. That's exactly what's wrong with it. Yep. Yes.
[00:50:25.440 --> 00:50:31.600]   And I would be recommending that. Recommendation wise, I think Amazon's recommendations are
[00:50:31.600 --> 00:50:36.960]   probably the best I've ever seen and have been for a long time, not just people who bought this,
[00:50:36.960 --> 00:50:42.800]   also bought this, but things that it recommends based on your browsing history,
[00:50:42.800 --> 00:50:49.600]   they're almost always things I'm either interested in buying or thinking about buying or like they're
[00:50:49.600 --> 00:50:56.240]   just extremely good at that stuff. Yeah. I actually like this Skyrim solace, but yeah, the 500 megabyte.
[00:50:56.240 --> 00:51:00.720]   But see, it was too late. I'd already bought it. Yeah, we just use it on our last trip,
[00:51:00.720 --> 00:51:03.840]   and that's where the 500 megabyte limit daily limit really bid us.
[00:51:03.840 --> 00:51:10.640]   It's 90. We paid 99 bucks. You can have five, five devices connected because it's Wi-Fi.
[00:51:10.640 --> 00:51:15.920]   We'd carry it around as we'd walk around. But I think it's less necessary now because of Google
[00:51:15.920 --> 00:51:22.560]   Fi and T-Mobile. We got just as fast on our phones and we could hotspot them. And we already had it.
[00:51:22.560 --> 00:51:27.440]   See, my carrier has a Rome-like home thing. So you pay 10 bucks a day.
[00:51:27.440 --> 00:51:31.520]   You're basically using your account like you were at home.
[00:51:31.520 --> 00:51:35.520]   So in a way, I think this was a good idea, but I think the time has
[00:51:36.160 --> 00:51:43.680]   changed a lot. It is interesting, though. I wonder whether Instagram, are they better at
[00:51:43.680 --> 00:51:50.240]   feeding you things that you are actually interested in because it's a specifically image-based.
[00:51:50.240 --> 00:51:55.360]   You're literally just scrolling through images and liking things.
[00:51:55.360 --> 00:52:04.000]   That provides a huge amount of data on what visually being tracked.
[00:52:04.000 --> 00:52:10.000]   Let's not forget what is it? Eighth post is an ad. But all the posts in between are often ads.
[00:52:10.000 --> 00:52:16.880]   Did you read this article in today's New York Times? A penthouse made for Instagram. $15,000
[00:52:16.880 --> 00:52:22.800]   a month in New York City, 2400 square feet free of the clutter of everyday life because no one
[00:52:22.800 --> 00:52:29.520]   lives there. Instagram influencers rent it by the day. It's booked through the month of October
[00:52:29.520 --> 00:52:34.720]   already to post pictures. So here's the bedroom, by the way, furniture from Wayfair.
[00:52:34.720 --> 00:52:43.280]   And here is an Instagram post from an Instagram influencer advertising apparently her bra.
[00:52:43.280 --> 00:52:51.200]   Recently, I discovered be tempted bras. I haven't been happier. This is an Instagram post.
[00:52:51.200 --> 00:52:55.680]   This should be illegal. Well, it does say ad in there, but I mean, there are more ads on
[00:52:55.680 --> 00:52:58.720]   Instagram than their regular posts. Talk about advertiser supported.
[00:52:59.520 --> 00:53:03.520]   There was a piece recently, I can't remember who did it, but it was all about
[00:53:03.520 --> 00:53:09.840]   kids basically having a summer job that just consists of posting sponsored content.
[00:53:09.840 --> 00:53:14.000]   And if you want to be a real Instagram influencer, have a baby. That's really useful.
[00:53:14.000 --> 00:53:16.000]   Yeah, it helps. It really helps.
[00:53:16.000 --> 00:53:22.000]   Cheryl Lee Lyle, Soho Manhattan selling, what do you think she's selling there?
[00:53:22.000 --> 00:53:23.360]   Cupcakes.
[00:53:23.360 --> 00:53:28.000]   Nope. That's what I thought. It's that soft Roma t-shirt or sweatshirt she's wearing.
[00:53:28.000 --> 00:53:33.520]   More on my cozy new sweatshirt and this adorable interior by Wearfair on my stories now.
[00:53:33.520 --> 00:53:34.960]   You want to see more?
[00:53:34.960 --> 00:53:36.800]   Oh, does it add?
[00:53:36.800 --> 00:53:40.880]   Oh, she's in trouble with the FTC. They're coming for you, Cheryl.
[00:53:40.880 --> 00:53:42.160]   No, they're not.
[00:53:42.160 --> 00:53:48.960]   So this pen has a mirror of a company that's existed in Russia for a while since at least a year,
[00:53:48.960 --> 00:53:53.840]   which will rent a private jet so you can take pictures of yourself on a private jet.
[00:53:53.840 --> 00:53:55.200]   It never leaves as the tarmac.
[00:53:55.200 --> 00:54:02.400]   Oh, I think Dan Bilzerian uses that. I think that's, yeah, he's always going somewhere in his
[00:54:02.400 --> 00:54:03.280]   private jet.
[00:54:03.280 --> 00:54:08.800]   But something occurred, something disturbing occurred to me about Instagram influencers.
[00:54:08.800 --> 00:54:12.720]   They often mock everyday situations even though they're staged.
[00:54:12.720 --> 00:54:17.680]   So for example, one common theme of a lot of influencers is they show them getting up in the
[00:54:17.680 --> 00:54:20.800]   morning or going to bed at night, especially getting up in the morning. But they get up in
[00:54:20.800 --> 00:54:24.960]   the morning. Their makeup is on, their hair is done, they look perfect, the whole room is
[00:54:24.960 --> 00:54:30.240]   perfect, everything's wonderful. How is this different from the king of France
[00:54:30.240 --> 00:54:34.560]   in at Versailles who staged a going to bed and getting up ceremony every day and all the
[00:54:34.560 --> 00:54:38.720]   sycophants would gather around and then they, after everybody would tip toe away, they'd go
[00:54:38.720 --> 00:54:40.720]   into their real bedroom and actually go bed.
[00:54:40.720 --> 00:54:42.720]   You would have loved Instagram.
[00:54:42.720 --> 00:54:44.480]   You would have loved Instagram.
[00:54:44.480 --> 00:54:45.840]   What an influencer.
[00:54:45.840 --> 00:54:46.640]   You would have.
[00:54:46.640 --> 00:54:48.320]   Appre moi la Instagram.
[00:54:48.320 --> 00:54:54.640]   The one thing I missed on Instagram, speaking of these staged things, is Kirby Jenner.
[00:54:54.640 --> 00:55:02.320]   This is actually a very funny comic who poses as the fraternal twin of Kendall Jenner.
[00:55:02.320 --> 00:55:11.120]   He'll take Kendall Jenner's real pictures and Photoshop him into them in hysterical ways.
[00:55:11.120 --> 00:55:16.640]   He's not in this picture. He gets dressed up to match.
[00:55:18.480 --> 00:55:26.160]   Yeah. This is a really great send up of everything that's wrong with Instagram,
[00:55:26.160 --> 00:55:31.600]   so that's a real Kirby Jenner post with him added.
[00:55:31.600 --> 00:55:36.240]   I'm sorry if you're listening and you're not seeing these pictures, but just do me a
[00:55:36.240 --> 00:55:38.480]   favor, follow them on Instagram, Kirby Jenner.
[00:55:38.480 --> 00:55:45.440]   There was quite a disturbing piece, actually, earlier the tier about a girl who basically
[00:55:45.440 --> 00:55:51.040]   lives her life for Instagram. I think she was 18 at the time, I think, and not really,
[00:55:51.040 --> 00:55:58.320]   nobody famous, but her parents were wealthy enough that she went to some pretty good parties,
[00:55:58.320 --> 00:56:04.560]   I guess. She got marketers interested in promoting their products through her Instagram.
[00:56:04.560 --> 00:56:12.240]   It was depressing in a way because she had no real life. Her entire life consisted of
[00:56:13.440 --> 00:56:17.280]   faking a life in order to promote products on Instagram.
[00:56:17.280 --> 00:56:21.840]   It's really sad. I mean, where is she now? You got to wonder, you know, this poor girl,
[00:56:21.840 --> 00:56:25.600]   her value has been completely warped, right? Yeah, can you go back and pre?
[00:56:25.600 --> 00:56:29.600]   Yes. Which one would you like? Keep going, one more.
[00:56:29.600 --> 00:56:33.040]   Okay, look at that's his sleeve. Look at how long his sleeve is. He says,
[00:56:33.040 --> 00:56:37.440]   "Whoa, I won the award for longest sleeves at this Met Jingle Ball thing,
[00:56:37.440 --> 00:56:42.240]   and this is a real big moment for me. Shout out to Reggie at Men's Warehouse for hooking it up,
[00:56:42.240 --> 00:56:47.200]   like always my Uber driver for her patients and Janet the caterer who fed me snacks all night
[00:56:47.200 --> 00:56:54.320]   because I couldn't find my own hands." That's a great look. So this is the real Kendall Jenner at
[00:56:54.320 --> 00:56:59.840]   the Met Ball Met Gala with him, but he's been some, I mean, he doesn't do a whole lot of posts
[00:56:59.840 --> 00:57:07.840]   because it takes forever. He's got to get the outfit. He's got to, oh wait a minute, here's a video.
[00:57:08.400 --> 00:57:13.040]   Kylie said, "If this video gets over 50 million views, she'll let me have my garage sale at her house."
[00:57:13.040 --> 00:57:17.040]   Kirby, Kylie.
[00:57:17.040 --> 00:57:27.440]   He is, I think, the most talented person I have ever seen. Certainly the most talented of
[00:57:27.440 --> 00:57:32.880]   all the Kardashian. He is the best Kardashian. Oh, he is. Yeah, yeah, he's definitely the best.
[00:57:33.760 --> 00:57:39.120]   All right, we hadn't finished the Google change slide, but we got really distracted here. I have
[00:57:39.120 --> 00:57:43.840]   one more thing that you're going to very much like, and I bet you all of the young people here
[00:57:43.840 --> 00:57:49.120]   in the audience will be excited about this. Do you like Assassin's Creed? Fun game, right?
[00:57:49.120 --> 00:57:55.680]   Can you imagine? Your school stuck you with Chromebooks. You can't play any games, but wait.
[00:57:55.680 --> 00:58:02.240]   Google Project Stream is bringing Assassin's Creed Odyssey to your Chrome browser.
[00:58:03.600 --> 00:58:08.560]   Oh, but wait, I'm sorry. The fine print. Your internet connection has to be capable of 25
[00:58:08.560 --> 00:58:12.400]   megabits a second. You must be 17 years or older and live in the US to participate.
[00:58:12.400 --> 00:58:24.000]   In your browser, this is really, I mean, it makes sense. You have to get an invite.
[00:58:24.000 --> 00:58:29.680]   You could apply on the website right now if you meet all those criteria.
[00:58:30.240 --> 00:58:33.840]   25 megabits isn't that much nowadays. I think there's a lot of people with that.
[00:58:33.840 --> 00:58:40.560]   I'll be interested to see how this works, like in real life.
[00:58:40.560 --> 00:58:45.680]   They don't, you know, I'm looking at the sign up. They don't say specifically what kind of hardware
[00:58:45.680 --> 00:58:51.280]   you need, but I'm thinking this is you got to have a pixel book or you're using Chrome on a fairly
[00:58:51.280 --> 00:58:56.160]   heavy PC. Yeah, I'm totally doing this. Isn't that awesome? I got a pixel book. Yeah, it's pretty cool.
[00:58:57.040 --> 00:59:02.320]   I think by the end of the October 9th announcement, everybody's going to think that pixel books
[00:59:02.320 --> 00:59:09.200]   and high-end Chromebooks are like the most capable devices. I believe the Windows 10
[00:59:09.200 --> 00:59:14.880]   rumors, for example. The other rumor is there'll be a tablet. I think a fairly good rumor, Kevin
[00:59:14.880 --> 00:59:19.360]   Tofel was talking about this from about Chromebooks.com last week. Fairly good rumor that there will be a
[00:59:19.360 --> 00:59:26.080]   new tablet called not code named Nocturn or Pixel Slate that doesn't have a keyboard but has their
[00:59:26.080 --> 00:59:29.760]   third-party keyboards like the bridge. That's how we learned about it, a keyboard that can be
[00:59:29.760 --> 00:59:37.680]   attached to it. It'll be a fairly high resolution, a 3x2 screen, a run Android, and that's the rumor
[00:59:37.680 --> 00:59:45.040]   that's most interesting that it will also run Windows 10. The good money is on the idea that the
[00:59:45.040 --> 00:59:52.240]   Nocturn tablet will get it immediately or soon enough, and that the pixel book could get it immediately.
[00:59:53.360 --> 00:59:59.280]   And it's only a matter of time, six months or whatever, where there'll be a half a dozen to a
[00:59:59.280 --> 01:00:06.000]   dozen devices that will run this Windows 10. So pretty cool for people.
[01:00:06.000 --> 01:00:13.120]   Are you all worried about the idea of a tablet version of Chrome OS or does that sound good to
[01:00:13.120 --> 01:00:16.160]   you? Because I've only seen one. It was from Acer and it was unimpressive.
[01:00:16.160 --> 01:00:22.720]   Well, the thing is that the pixel book folds into a tablet, but it's irritating to use because the
[01:00:22.720 --> 01:00:26.640]   back is a keyboard. And even though the keyboard is disabled in the tablet mode,
[01:00:26.640 --> 01:00:32.320]   it's still super annoying to use it with a keyboard in the back. And so a tablet mode is going to
[01:00:32.320 --> 01:00:37.200]   need an entirely different interface. I've used my pixel book a lot in tablet mode and just,
[01:00:37.200 --> 01:00:42.160]   you know, just tapping around and using touch for the browser and stuff like that does not work.
[01:00:42.160 --> 01:00:48.800]   A lot of missed pokes. And so they really have to change the interface and I hope they do.
[01:00:49.360 --> 01:00:55.920]   Essentially, when you put any Chromium or Chrome OS device into tablet mode, or if it's a tablet,
[01:00:55.920 --> 01:01:01.520]   it should just kick into a completely different interface that's a lot more like iOS or Android
[01:01:01.520 --> 01:01:07.440]   interface. So I think it's a great idea. It's a great device. Right now Google does some things
[01:01:07.440 --> 01:01:12.000]   that are interesting. For example, there are certain things that you need a keyboard to do.
[01:01:12.000 --> 01:01:17.600]   And as soon as you put it in tablet mode, those things go down and become selectable on the bottom
[01:01:17.600 --> 01:01:22.640]   display thing. They just need to do a lot more of that kind of thing in order to make it viable.
[01:01:22.640 --> 01:01:26.240]   But I think it's doable and I think it would be great if they could do that. Because again,
[01:01:26.240 --> 01:01:32.320]   you know, you add a Bluetooth keyboard to a tablet. That's how I have used my iPad since 2010.
[01:01:32.320 --> 01:01:40.560]   It's great to have a separate, completely separate keyboard from a tablet that's a dedicated tablet.
[01:01:40.560 --> 01:01:43.360]   Yeah, but is it covered in leather?
[01:01:44.880 --> 01:01:51.440]   I hope so. Did you see this? The HP Spectre folio, the first laptop. This isn't a leather case,
[01:01:51.440 --> 01:01:59.520]   a leather cover. This is actually a leather laptop for you leather enthusiasts. You want leather.
[01:01:59.520 --> 01:02:06.240]   And the reason I thought of this is because it does have a kind of a keyboard does not detach,
[01:02:06.240 --> 01:02:11.280]   but it has, you know, kind of it's a convertible. So kind of, and you can see as it floats down,
[01:02:11.280 --> 01:02:17.120]   there's a one position where the trackpad is still accessible. I'm not sure exactly what the idea
[01:02:17.120 --> 01:02:23.840]   there is, but it's running Windows 10. So anyway, I just thought I mentioned that because
[01:02:23.840 --> 01:02:31.840]   this has been the extended edition of the Google change log. Yeah, thank you very much.
[01:02:31.840 --> 01:02:37.440]   Thank you. Thank you very much. Our show today brought to you by Capeterra.
[01:02:38.880 --> 01:02:44.080]   If you're looking for business software for your business, whether you're a, I don't know,
[01:02:44.080 --> 01:02:48.240]   you have a window washing business, you're a veterinarian, you're a dentist,
[01:02:48.240 --> 01:02:52.560]   you want to, you know, you do sales and you want to keep track of customers,
[01:02:52.560 --> 01:02:57.120]   there are really only a couple of ways to do it. You could Google it and get a bunch of just kind
[01:02:57.120 --> 01:03:01.360]   of undifferentiated results and then have to comb through it and figure out what's good, what's not.
[01:03:01.360 --> 01:03:07.280]   You could ask a friend, but, you know, your friend better be pretty well connected. Let me give you
[01:03:07.280 --> 01:03:16.000]   the best way. Capeterra, C-A-P-T-E-R-R-A dot com slash twig. We all face unexpected hurdles at work,
[01:03:16.000 --> 01:03:22.160]   but don't let finding the right software for your business be one of them. 2019 is fast approaching.
[01:03:22.160 --> 01:03:26.640]   Man, I didn't think I'd be saying that anytime soon, but it is. You don't have time for unexpected
[01:03:26.640 --> 01:03:31.600]   hurdles to derail. You find software for your business fast with Capeterra. You can use the
[01:03:31.600 --> 01:03:37.680]   site, but they've got a new free ebook. You might want to check out the big book of free software.
[01:03:37.680 --> 01:03:42.160]   It's free. It's a free resource to help you find the software you need for your business.
[01:03:42.160 --> 01:03:48.160]   300 different software tools. It'll help you find a completely free tool to test today.
[01:03:48.160 --> 01:03:53.360]   Yeah, everything in here is free. And even the book is free. So you don't even have to ask your
[01:03:53.360 --> 01:03:58.320]   boss for any money. Just go out and get it. Capeterra dot com slash twig. Whether you're looking for a
[01:03:58.320 --> 01:04:02.160]   new project management tool, e-commerce software, an email marketing solution,
[01:04:02.160 --> 01:04:08.720]   Capeterra's big book of free software. Did I mention it's free? Is there for you? Visit
[01:04:08.720 --> 01:04:13.280]   capeterra dot com slash twig and get your copy of the big book of free software.
[01:04:13.280 --> 01:04:18.800]   Absolutely free. You couldn't really charge for a big book of free software. So they're giving it away.
[01:04:18.800 --> 01:04:24.320]   Capeterra is a great resource for finding the business software you need. It makes it easy.
[01:04:24.320 --> 01:04:30.960]   And now you get free. The big book of free software capeterra dot com slash twig. Join the millions of
[01:04:30.960 --> 01:04:35.360]   smart people use capeterra every single week.
[01:04:35.360 --> 01:04:42.800]   Are they talking about this in Canada? California has enacted enacted a net neutrality law joining a
[01:04:42.800 --> 01:04:48.480]   number of other states. It seemed like, you know, when I read it seemed like, you know, not a big
[01:04:48.480 --> 01:04:55.360]   deal. The net neutrality law basically reinstates the old FCC rule that internet service providers
[01:04:55.360 --> 01:05:01.680]   can't block sites, slow down sites, can't zero rate sites. In other words, let some sites be free
[01:05:01.680 --> 01:05:08.720]   on their service while others are not immediately the department just as sued and many other ISPs.
[01:05:08.720 --> 01:05:13.280]   Because apparently, despite the fact that the FCC says, we don't need this regulation
[01:05:13.280 --> 01:05:19.200]   because nobody really wants to do this. Everybody wants to do it. I mean, isn't this an emission
[01:05:19.200 --> 01:05:23.760]   basically by the United States government that the whole point of eliminating net neutrality so
[01:05:23.760 --> 01:05:28.080]   that the internet service providers can slow down sites, block sites and zero rate sites?
[01:05:28.080 --> 01:05:40.560]   Pretty much. The Justice Department wants to basically rule that it's unconstitutional for a
[01:05:40.560 --> 01:05:44.640]   state to regulate interstate commerce. That's actually in the Constitution that the federal
[01:05:44.640 --> 01:05:48.800]   government regulates interstate commerce. Exactly. The problem with that line of argument has been
[01:05:48.800 --> 01:05:56.080]   tested in court before. And what was determined in the existing precedent is that in fact,
[01:05:56.080 --> 01:06:05.200]   states can do that. And the reason they can do that is because the Ajit Pai FCC in their reversal of
[01:06:05.200 --> 01:06:15.120]   the Obama era ruling for net neutrality essentially argued that the internet should not be or will
[01:06:15.120 --> 01:06:19.280]   not be regulated by the federal government. And therefore, private companies can do pretty much
[01:06:19.280 --> 01:06:25.840]   whatever they want. And so what is likely to happen according to legal experts is that
[01:06:25.840 --> 01:06:29.520]   when this goes into court, they're saying, no, you guys ruled that you cannot regulate this stuff.
[01:06:29.520 --> 01:06:34.320]   If you can't regulate it, then you can't tell states they can't regulate it. And therefore,
[01:06:34.320 --> 01:06:38.640]   states are allowed to regulate it. And this is going to happen. I think that California's net
[01:06:38.640 --> 01:06:46.640]   neutrality law is going to prevail in court. And it'll be another shining example of
[01:06:46.640 --> 01:06:53.200]   competence and so on by the current FCC. I hope you're right. I understand the constitutional
[01:06:53.200 --> 01:06:58.720]   objection, but the California legislatures who created this law say we specifically wrote a law
[01:06:59.440 --> 01:07:04.560]   that would not infringe on the Constitution. I think you made an excellent point. The federal
[01:07:04.560 --> 01:07:07.920]   government has already said we're not going to regulate it. So how could this be competing
[01:07:07.920 --> 01:07:15.520]   with the federal government? And when I read what this law does, I wonder how an internet service
[01:07:15.520 --> 01:07:21.040]   provider or the Department of Justice, all of whom are suing the state, explain the fact that
[01:07:21.040 --> 01:07:26.160]   they've said, well, we don't want to do this. They're trying to preserve the right to do this.
[01:07:26.640 --> 01:07:31.440]   The new California law prohibits. See, what do you think about this? You think it should be
[01:07:31.440 --> 01:07:38.000]   against the law for internet service providers to block or throttle lawful traffic? Should it be
[01:07:38.000 --> 01:07:43.520]   against the law from them for them to require fees from websites or online services like Twit
[01:07:43.520 --> 01:07:51.680]   to deliver or prioritize traffic to consumers? Should the ISPs be allowed to zero rate?
[01:07:52.560 --> 01:07:58.000]   Should they be allowed to evade net neutrality protections by slowing down traffic at network
[01:07:58.000 --> 01:08:02.080]   interconnect points? All of this seems sensible. No, they shouldn't be able to do that.
[01:08:02.080 --> 01:08:08.720]   A part of the argument is that they need to do some of those things or want to in certain cases. So
[01:08:08.720 --> 01:08:16.480]   throttling, for example, for network congestion purposes. So their argument would be, yes, we don't
[01:08:16.480 --> 01:08:23.280]   want to throttle all of this company's traffic. But we might want to do what they call in Canada
[01:08:23.280 --> 01:08:30.720]   ban with shaping for specific purposes. And that might be misconstrued as I'm not saying
[01:08:30.720 --> 01:08:36.720]   I agree with this argument, but I think that's part of what the reason they're complaining is that
[01:08:36.720 --> 01:08:41.360]   they feel it would this would criminalize things that they already do.
[01:08:41.360 --> 01:08:46.480]   Yes, it would. Lots of companies do criminalize things they already do.
[01:08:46.480 --> 01:08:53.520]   Exactly my point. The trouble is that it's much more complicated because really from the
[01:08:53.520 --> 01:08:59.120]   ISP's point of view, there are two internets. There's the internet that goes from the world
[01:08:59.120 --> 01:09:03.920]   to the ISP and then the internet that goes from the ISP to the customer of the ISP.
[01:09:04.480 --> 01:09:10.880]   And if you're throttling the latter one, the connection to the customer,
[01:09:10.880 --> 01:09:16.320]   based on if you're prioritizing different services based on any deals you made or whether you happen
[01:09:16.320 --> 01:09:23.840]   to own those sources of information, then that is clearly a violation. That's akin to a company
[01:09:23.840 --> 01:09:29.280]   building a toll road and then blocking traffic on the actual road and routing different people
[01:09:29.280 --> 01:09:34.240]   for different reasons and so on, clearly a violation. However, there are certain things like
[01:09:34.240 --> 01:09:39.920]   huge percentage of the internet's traffic is Netflix and YouTube. And so there's all kinds
[01:09:39.920 --> 01:09:43.520]   of things that they have to do between Netflix and YouTube to give prioritization,
[01:09:43.520 --> 01:09:50.640]   huge investment, YouTube and Netflix pay a lot to the ISP's for some of this activity.
[01:09:50.640 --> 01:09:52.480]   And that's an entirely different kettle of fish.
[01:09:52.480 --> 01:09:53.440]   Hearing agreements.
[01:09:53.440 --> 01:09:59.280]   Desirable for exactly, hearing agreements, etc. And so it's very, very complicated.
[01:09:59.280 --> 01:10:07.920]   But it's essentially what we have here as a conflict between a state administration,
[01:10:07.920 --> 01:10:14.400]   the Jerry Brown administration, which is acting on behalf of consumers and enacting a clear consumer
[01:10:14.400 --> 01:10:23.040]   protection bill and the FCC, which is acting clearly against consumers in favor of companies.
[01:10:24.240 --> 01:10:30.800]   And this is essentially how Washington tends to work anyway. And it couldn't be more clear cut
[01:10:30.800 --> 01:10:35.920]   in that regard. I mean, the devil is in the details and the California bill is not perfect.
[01:10:35.920 --> 01:10:42.400]   But this is what we're talking about. Does freedom mean that consumers have the freedom to get
[01:10:42.400 --> 01:10:49.120]   unbiased access to everything on the internet or does freedom mean that Comcast has the right to
[01:10:49.120 --> 01:10:54.000]   charge and nickel and dime and throttle and play games with a consumer in order to make more money?
[01:10:54.000 --> 01:10:58.080]   What, how do you define freedom? It's clear how the FCC defines it.
[01:10:58.080 --> 01:11:02.800]   Well, the Department of Justice does not always win. According to Reuters,
[01:11:02.800 --> 01:11:09.200]   the US, which is trying to force Facebook to wiretap messenger calls and their investigation
[01:11:09.200 --> 01:11:19.680]   of the Fresno MS-13 gang has failed. The proceedings were sealed. So we only know that the judge at
[01:11:19.680 --> 01:11:25.920]   the US District Court of Fresno ruled in Facebook's favor. It said Facebook does not have to decrypt
[01:11:25.920 --> 01:11:31.680]   those messages. We don't know why, but Reuters has two people briefed on the sealed ruling,
[01:11:31.680 --> 01:11:36.240]   which who are saying that Facebook in fact has won this case.
[01:11:38.960 --> 01:11:51.600]   Yeah. Whenever the generation has its unassailable bad, bad people,
[01:11:51.600 --> 01:11:59.680]   whether a few years ago is Al Qaeda, it's always child pornography now, MS-13 is something everybody's
[01:11:59.680 --> 01:12:05.520]   afraid of. And the typical playbook is that you law enforcement or some other government agency
[01:12:05.520 --> 01:12:14.640]   wants to be able to do certain things and judges or some other group in the government says,
[01:12:14.640 --> 01:12:20.320]   no, you can't do that because of annoying things like the Constitution or the existing statutes.
[01:12:20.320 --> 01:12:25.840]   And then they always come back with, oh, but the child pornography, we have to stop the,
[01:12:25.840 --> 01:12:32.080]   whatever it is that nobody could argue is a good thing to stop MS-13 in this case.
[01:12:32.080 --> 01:12:36.960]   You always have to be, everybody should be wary of these examples because they're all,
[01:12:36.960 --> 01:12:42.160]   they will always come back with that, that example, that reason why they need to do something
[01:12:42.160 --> 01:12:47.920]   generally that they've always wanted to do. And it's a great way to get voters to the polls is to scare
[01:12:47.920 --> 01:12:57.440]   them. Yeah. That's exactly right. That's why, that's why, what's his name book is called Fear.
[01:12:57.440 --> 01:13:02.560]   What's his Bob, Bob, what's book? The reason it's called Fear is that it's based on this idea,
[01:13:02.560 --> 01:13:08.640]   this political idea that scaring the daylights out of everybody gives you a lot of power.
[01:13:08.640 --> 01:13:13.600]   And so lots and lots, I wrote a piece on this, I call it the Fear Industrial Complex. And
[01:13:13.600 --> 01:13:19.200]   you know, lots and lots of different companies have discovered if you can make people scared
[01:13:19.200 --> 01:13:24.640]   enough, they will, they will give you either power or money or both. And so, yeah, that's,
[01:13:24.640 --> 01:13:29.680]   that's essentially the sign of the time. So we need to learn to, to recognize those things
[01:13:29.680 --> 01:13:33.520]   when they're coming at us. I mean, Hitler used, it's been used to time immemorial to create
[01:13:33.520 --> 01:13:38.160]   escape, go to make people scared of them and then say, well, I have the solution, only I can
[01:13:38.160 --> 01:13:46.160]   solve this problem. What's changed is the ability of people to, as we began the conversation, to use
[01:13:46.160 --> 01:13:50.640]   modern technologies, internet technologies to spread this. And we can really, we could scare
[01:13:50.640 --> 01:13:57.040]   people really good now. We got it down, man, we could terrify you. Just my mother-in-law got a phone
[01:13:57.040 --> 01:14:03.520]   call out of the blue that said, oh, we've recognized that your so-and-so activity, we're going to be,
[01:14:03.520 --> 01:14:08.560]   we're going to be coming to your place of business or your home to forfeit, you're going to have to
[01:14:08.560 --> 01:14:12.560]   forfeit your home and all this kind of stuff. And just scared the daylights out of her. I mean,
[01:14:12.560 --> 01:14:17.520]   and I looked it up and did some research and it turns out that this is a scam that starts in
[01:14:17.520 --> 01:14:24.800]   India. They tunnel into like a US VoIP and then they make phone calls and leave messages. And
[01:14:24.800 --> 01:14:29.440]   it's just like, there's no attempt to persuade or con or trick. It's just like scare people and the
[01:14:29.440 --> 01:14:35.040]   money flows. And it's just a really awful thing that, you know, they're essentially trolling for
[01:14:35.040 --> 01:14:38.000]   people like my mother-in-law who don't really understand what's happening. Works better with
[01:14:38.000 --> 01:14:41.520]   older people, that's for sure. Yeah. Now that I'm an older person, I'm getting a lot more of those.
[01:14:42.640 --> 01:14:47.840]   I think last week when the new iPhone came out, I got a call during all the shows. They call
[01:14:47.840 --> 01:14:52.800]   me a total of 14 times in one day saying your iCloud account has been hacked. This is, hi,
[01:14:52.800 --> 01:14:59.280]   this is Olivia from Apple's Apple support. Your iCloud account has been hacked. It was a robo call.
[01:14:59.280 --> 01:15:04.800]   Please call us at this number. And whatever you do, don't use any Apple services until you do.
[01:15:04.800 --> 01:15:10.720]   Of course, immediately I went to iCloud to see if I've been hacked and I hadn't. But, you know,
[01:15:10.720 --> 01:15:13.440]   those things are scams almost immediately. It doesn't. Yeah.
[01:15:13.440 --> 01:15:20.080]   Yeah. And I think it's- Yeah, exactly. I know my friend, her mother, was on the way when she called
[01:15:20.080 --> 01:15:27.360]   her. She was on her way to the bank to take money out, to send a money order to someone
[01:15:27.360 --> 01:15:33.120]   who claimed that her network had been hacked. And this was the only way to get- she was literally
[01:15:33.120 --> 01:15:38.400]   on the way to the bank to take money out, to give to some complete stranger. And if her daughter
[01:15:38.400 --> 01:15:42.640]   hadn't called, you know, she probably would have. Well, I know for a fact, and maybe this is something
[01:15:42.640 --> 01:15:46.960]   I don't know a lot of people have talked about, but that all of these scammers, all these call,
[01:15:46.960 --> 01:15:51.600]   are coming from Australia. This is actually- Yeah, it's Australia. Many people don't know this.
[01:15:51.600 --> 01:15:56.800]   You know, they call us Russians, but it's Australia. They're all coming- The worst.
[01:15:56.800 --> 01:16:03.600]   It's- Yeah. Oh, up. They're gonna leave. I think we have to play them a special song as they-
[01:16:03.600 --> 01:16:09.920]   Thank you. Thank you, young people. Play the song out, will you, Carsten, as the procession-
[01:16:09.920 --> 01:16:11.040]   Is it the land down under?
[01:16:11.040 --> 01:16:23.120]   Australians, let us rejoice, for we are young and free. We've golden soil and wealth for toil.
[01:16:23.120 --> 01:16:28.160]   Our home is Gert. I see what else Gert-
[01:16:28.160 --> 01:16:30.960]   Gert, you know. Gert, you know. Gert, you know, we're in a lot anymore.
[01:16:32.560 --> 01:16:42.560]   Thank you guys. Have a great day. Bye-bye. Bye-bye. So long. Catch you later, mate. Bye-bye.
[01:17:00.240 --> 01:17:03.760]   Probably never in a minute. Man, with eight hours. Sturring.
[01:17:03.760 --> 01:17:13.760]   Oh. Hey, you know, it'd be a great idea for a podcast because I know you don't have enough
[01:17:13.760 --> 01:17:21.440]   podcasts to deal with, Leo. A podcast that focuses on scams, that identifies as a major scam,
[01:17:21.440 --> 01:17:24.880]   and then follows through on it. Like, the podcast can be like- Where does this call from? Who is
[01:17:24.880 --> 01:17:29.600]   doing this? Yeah. That's a great idea. Right, interact with the Nigerian, like,
[01:17:29.600 --> 01:17:32.880]   scammer and like, you know, go back and forth with them and like, I don't know.
[01:17:32.880 --> 01:17:37.840]   That's the next cereal. Because what you do, every, you know, if you did the Nigerian,
[01:17:37.840 --> 01:17:42.880]   what do they call it? The 4/4/4 scam or whatever. The way if you did then, yeah, if you did it,
[01:17:42.880 --> 01:17:46.240]   it would take a long period of time because, you know, you have to write, they write, they
[01:17:46.240 --> 01:17:52.560]   be really next week. Yeah. We'll talk to the comedian who does that. No.
[01:17:52.560 --> 01:17:59.760]   Who responds to it's fantastic. There's a whole, there's a whole routine where he basically responds
[01:17:59.760 --> 01:18:07.600]   to a Nigerian scammer and pretends to be interested and then asks for photos of him and his family,
[01:18:07.600 --> 01:18:13.040]   and it's quite funny. There used to be a website full of things like that where people would say,
[01:18:13.040 --> 01:18:17.920]   well, we need you to prove that you're legit. So we want you to put a fish on your head and like,
[01:18:17.920 --> 01:18:26.080]   stand on one day. Okay, I'll do that. I hope that those nice children from Australia who are going
[01:18:26.080 --> 01:18:31.280]   home are not going to go through New Zealand because a new law has been passed in New Zealand,
[01:18:31.280 --> 01:18:38.640]   went into effect October 1st, the Customs and Exercise Act allows New Zealand customs to carry out
[01:18:38.640 --> 01:18:44.960]   digital strip searches. They previously customs in New Zealand could stop anyone at the border
[01:18:44.960 --> 01:18:50.800]   in demand to see their electronic devices. The updated law says travelers must provide the
[01:18:50.800 --> 01:18:58.080]   password, the pin code, the fingerprint to unlock that device or be subject to a fine up to $5,000.
[01:18:58.080 --> 01:19:02.320]   And the device will be seized and forensically searched anyway.
[01:19:02.320 --> 01:19:09.600]   The, the New Zealand Customs spokesperson, Terry Brown said, it's a file by file search on your
[01:19:09.600 --> 01:19:14.160]   phone. We're not going into the cloud. We'll examine your phone while it's in flight mode.
[01:19:14.160 --> 01:19:19.360]   And basically that says, so if you're a bad guy, just store everything in Google Docs.
[01:19:19.360 --> 01:19:21.600]   In the cloud, yeah. Yeah. And you're cool.
[01:19:21.600 --> 01:19:27.920]   Just to convert. Yeah, which is why they'll amend the rule in like a year to include the cloud
[01:19:27.920 --> 01:19:32.240]   and social networks. You have to provide your social networking passwords, etc. It's a slippery
[01:19:32.240 --> 01:19:37.440]   slope for sure. They're worried about. I assume the, I assume that ICE and, you know,
[01:19:39.200 --> 01:19:45.840]   border patrol, we're doing this, not New Zealand. You never know where it's coming from. And maybe
[01:19:45.840 --> 01:19:51.360]   they'll say, Oh, that's a good idea. You know, you do know that the FBI actually is getting now,
[01:19:51.360 --> 01:19:57.680]   it's routine and warrants. And it's recently happened in an FBI search warrant, the right to
[01:19:57.680 --> 01:20:02.000]   hold the phone up to a perpetrator's face to unlock it. Oh, really? Interesting.
[01:20:02.000 --> 01:20:08.240]   The Feds, FBI was able to force a suspect to unlock his Apple iPhone 10 with his face. And it
[01:20:08.240 --> 01:20:11.440]   is apparently being written into search warrants on a regular basis now.
[01:20:11.440 --> 01:20:17.680]   So as far as I understand the legal president so far, they can compel you to do things that
[01:20:17.680 --> 01:20:24.720]   involve using your body. So your fingerprint, your eye scan, your face. But yes, because for
[01:20:24.720 --> 01:20:28.480]   years, they've been able to take a fingerprint or take care for. But they're not allowed,
[01:20:28.480 --> 01:20:33.120]   they're not allowed to force you to reveal a code. Yeah, because that's the contents of your brain.
[01:20:34.080 --> 01:20:39.600]   Exactly. A lot because it's speech. All of this relies on courts that continue to uphold that
[01:20:39.600 --> 01:20:43.120]   principle. And there is absolutely no guarantee that that's going to happen.
[01:20:43.120 --> 01:20:47.760]   Well, and the advice advice that I've read is, yes, technically, you can say,
[01:20:47.760 --> 01:20:54.080]   court president doesn't give you the right to force me to reveal a code. And they can say,
[01:20:54.080 --> 01:20:58.960]   fine, you can sit in this lammer for a couple of weeks. Yeah, you get to sit in airplane jail.
[01:20:58.960 --> 01:21:05.360]   Believe me, airport jail is the worst jail. You don't want to be an airport jail. I've been there.
[01:21:05.360 --> 01:21:10.960]   This is one of the things I love about a Chromebook, because you can just power wash it. And there's
[01:21:10.960 --> 01:21:16.320]   literally nothing on the device, nothing that can be even extracted from the hard drive and so on.
[01:21:16.320 --> 01:21:20.800]   But there are a lot of these things that changes in what happens as you travel around the world
[01:21:20.800 --> 01:21:24.320]   in terms of your security and privacy. A lot of things are like stories like New Zealand.
[01:21:25.280 --> 01:21:30.000]   And in almost every case, I'm like, well, that's an interesting story as a journalist. I need to
[01:21:30.000 --> 01:21:35.440]   cover that be aware of that. But for me personally, traveling, it doesn't bother me at all.
[01:21:35.440 --> 01:21:39.600]   Oh, white man. Yeah, exactly. We're a turbans see what happens, Mike.
[01:21:39.600 --> 01:21:46.000]   OK, right. It's true. But there's a story that came out today that actually did freak me out.
[01:21:46.000 --> 01:21:52.720]   And that story is that a German man was detained in Turkey. Yes.
[01:21:53.280 --> 01:22:00.720]   For defaming Erdogan on social media years ago, I have personally defamed Erdogan myself
[01:22:00.720 --> 01:22:05.360]   and the Cuban government and lots of, you know, you name it. So I'm thinking, this is the first
[01:22:05.360 --> 01:22:08.560]   time I've ever thought, you know what? What do we need to solve? I'm just going to delete all
[01:22:08.560 --> 01:22:14.400]   my old posts because at some point, this thing is going to snowball to the point where I'm not
[01:22:14.400 --> 01:22:19.760]   going to get into some of these countries anymore. The first thing before I deactivated my Twitter
[01:22:19.760 --> 01:22:23.200]   account, that's the first thing I did is I ran a script that deleted all my old tweets.
[01:22:23.520 --> 01:22:28.480]   Every one of them. Fantastic. And I think that that's probably a good idea, although
[01:22:28.480 --> 01:22:32.720]   tweets live forever regardless of what you might do because there's plenty of, there's stored in
[01:22:32.720 --> 01:22:38.880]   lots of places. But yeah, I agree with you. I think this is, this is not, this is a point,
[01:22:38.880 --> 01:22:43.040]   but I would stay out of Turkey. You know, we were in Italy, Mike, there were,
[01:22:43.040 --> 01:22:47.920]   we could, you couldn't move and a mall for your positano was jammed with tourists,
[01:22:47.920 --> 01:22:52.160]   of course, it's high season in September. But our guide said it's gotten much worse because
[01:22:52.160 --> 01:22:57.440]   people aren't going to Turkey and Egypt. Right. Interesting. So this is why the places like Italy
[01:22:57.440 --> 01:23:03.920]   and Provence and are getting so crowded in Barcelona because the number of places that people,
[01:23:03.920 --> 01:23:10.080]   the tourists want to go is narrowing. There's one idea that I think is kind of an interesting idea
[01:23:10.080 --> 01:23:14.960]   that I think Matt Drudge, either you see the Drudge report on Twitter or Matt Drudge on Twitter,
[01:23:14.960 --> 01:23:19.680]   they post one tweet, they delete, there's always one tweet there. And when they do a new tweet,
[01:23:19.680 --> 01:23:23.520]   they delete the existing tweet and then tweet another one. So you can go to the side. I think
[01:23:23.520 --> 01:23:30.000]   it's Matt Drudge, always exactly one tweet. And that sounds like a good idea to me.
[01:23:30.000 --> 01:23:37.440]   Yeah, but again, you know, you can, it may not be on Twitter, but it's still there. I mean,
[01:23:37.440 --> 01:23:41.760]   idea activity in my account, but it doesn't mean it's not. I know I've thought about going,
[01:23:41.760 --> 01:23:48.640]   you know, crossing the border to get into the US and having someone check my Twitter feed or
[01:23:49.440 --> 01:23:54.480]   and see things that I've said about Donald Trump and decide that, you know, I'm not welcome in the
[01:23:54.480 --> 01:24:01.040]   country. Yeah. And then there's always Lindsay Lohan, who could punch you in the airport in France.
[01:24:01.040 --> 01:24:05.440]   So and might, apparently she might, apparently, will.
[01:24:05.440 --> 01:24:13.920]   Yeah, I have not had any boy, you know, we, we traveled, we travel a lot. I have never had
[01:24:13.920 --> 01:24:18.800]   problems coming into the United States, more trouble going into Canada than ever getting into
[01:24:18.800 --> 01:24:23.120]   the United States because I'm a US citizen. But the Canadians are tough.
[01:24:23.120 --> 01:24:28.000]   You guys, you guys are tough, but fair, tough. You don't want to mess with a
[01:24:28.000 --> 01:24:35.120]   Mountie. I'm telling those guys. They're scary. Did I ever tell you about coming back from San
[01:24:35.120 --> 01:24:44.080]   Francisco and going through Canadian customs in Pearson and this large, you know, marine looking
[01:24:44.080 --> 01:24:51.440]   guy with the haircut and weapons and whatnot was very serious. And he said, you, Matthew Ingram,
[01:24:51.440 --> 01:24:58.640]   and I was like, I'm pretty sure I am. Yeah. But why? And he said, like, the Matthew Ingram,
[01:24:58.640 --> 01:25:04.160]   and I'm like, am I on a list or something? Is there a T Matthew Ingram? And he said, are you on
[01:25:04.160 --> 01:25:11.280]   that show with Leo Laport this week in Google? And I'm like, yeah, I love that job. I love that.
[01:25:12.240 --> 01:25:16.880]   Yeah. And then we had this great chat about whatever we were discussing that week.
[01:25:16.880 --> 01:25:24.160]   I'm always very aware of the fact that we have listeners in every walk. And that's why I really
[01:25:24.160 --> 01:25:31.760]   try to be fair. I'm sure that was a was a guy, a security guy. Yeah. And he said, I have some
[01:25:31.760 --> 01:25:36.800]   different views when it comes to like encryption and I can understand that they're right on the
[01:25:36.800 --> 01:25:41.920]   front line there. You should like totally get in the chat room or something because we would
[01:25:41.920 --> 01:25:45.600]   love to hear from you. Yeah. I mean, and I always ask law enforcement when they come in,
[01:25:45.600 --> 01:25:49.840]   we get a lot of law enforcement visiting. And I always ask them, and they often do come down on
[01:25:49.840 --> 01:25:52.960]   the other side, but then remember, they're on the front lines and they're looking for every tool
[01:25:52.960 --> 01:25:57.600]   they can get. But it isn't, but I think most of them understand that there's a natural tension
[01:25:57.600 --> 01:26:04.480]   between the need to police and the right to freedom. That's why we have a constitution.
[01:26:04.480 --> 01:26:09.840]   And that's, you know, I mean, a lot of the things in the constitution make it harder for police.
[01:26:11.360 --> 01:26:16.320]   Privacy makes it harder for police. You know, the fourth amendment, the fifth amendment,
[01:26:16.320 --> 01:26:20.160]   these all make it harder. And they're intended to because we don't want to.
[01:26:20.160 --> 01:26:22.160]   Yeah, luckily we don't have any of that in Canada.
[01:26:22.160 --> 01:26:29.440]   I always think that if the United States enacted Canada's immigration rules, it would be such a
[01:26:29.440 --> 01:26:37.600]   scandal. Canada, it's like you pretty much like they're much heavy, much more heavily into
[01:26:38.240 --> 01:26:44.000]   what we would call an H1B visa type of situation. You have to come in with skills and all that
[01:26:44.000 --> 01:26:50.560]   kind of stuff. Whereas the US is kind of works against that partly because the corporate powers
[01:26:50.560 --> 01:26:55.680]   that have influence in Washington want lots and lots and lots of cheap workers. And so they're not,
[01:26:55.680 --> 01:27:02.720]   they're not looking for competitors. They're looking for consumers, workers. And so, but if,
[01:27:02.720 --> 01:27:09.280]   like let's say President Trump suddenly enacted Canada's law to the letter, it would be the most
[01:27:09.280 --> 01:27:13.520]   outrageous scandal imaginable. People would be running in the streets going, oh my God,
[01:27:13.520 --> 01:27:19.440]   they're just a leadist, blah, blah, blah. Mexico too. I mean, all of our neighbors have this rational
[01:27:19.440 --> 01:27:25.760]   policy of saying, you know what, we could really use more doctors. And so we'll prioritize professionals
[01:27:25.760 --> 01:27:30.000]   in one way or another. So I just think it's kind of an interesting thing that people,
[01:27:30.000 --> 01:27:34.640]   your average American would assume, oh yeah, Canada's probably got a really sort of liberal
[01:27:34.640 --> 01:27:40.240]   policy on immigration. Nope. On refugees, yes, but not not.
[01:27:40.240 --> 01:27:45.920]   Yeah, for sure. Well, Canada did though. Remember, and I know Matthew knows this, but there was a
[01:27:45.920 --> 01:27:52.880]   why it was very wide open and when was that the 70s? And it was a lot of people came to Canada and
[01:27:53.440 --> 01:28:02.560]   it's a wonderfully diverse nation because of it. Well, I would say, you know, we have had problems
[01:28:02.560 --> 01:28:10.320]   for sure. And I would never claim that Canada was some kind of utopian society, but nothing gets me
[01:28:10.320 --> 01:28:17.440]   thinking about how well things work here as a multicultural society, like going to the US.
[01:28:17.440 --> 01:28:24.880]   I mean, there's just things you take for granted in a place like Toronto or Vancouver or even Ottawa,
[01:28:24.880 --> 01:28:30.400]   just the way culture works, different people speaking different languages, doing different things.
[01:28:30.400 --> 01:28:34.560]   It's so wonderful. It's so wonderful when you go to Toronto.
[01:28:34.560 --> 01:28:38.560]   Toronto has two Chinatowns. Yeah, and they're both excellent.
[01:28:38.560 --> 01:28:43.760]   They're both excellent. I mean, it's just wonderful the diversity there. And when I worked at Rogers,
[01:28:44.880 --> 01:28:50.000]   the staff was incredibly diverse partly it's because we were in the ethnic channel building and they
[01:28:50.000 --> 01:28:57.040]   do broadcast, newscaster, like 50 different languages. But it's really a model of what could
[01:28:57.040 --> 01:29:01.360]   happen. And it's a shame that it's become less open. Well, one of the things that I like to say
[01:29:01.360 --> 01:29:06.400]   to confuse and arrange Canadians is that Canada is like the United States, but the United States
[01:29:06.400 --> 01:29:11.760]   is nothing like Canada, by which I mean that there are many pockets of the United States that
[01:29:11.760 --> 01:29:17.920]   are a lot like Canada in every way, including diversity. I went to the most diverse university
[01:29:17.920 --> 01:29:24.880]   in the world, which is UCLA. And it was fantastic because of the diversity.
[01:29:24.880 --> 01:29:32.960]   There are parts of the Northeast, New York is a wonderful example of great diversity.
[01:29:32.960 --> 01:29:38.400]   So a lot of the cultural elements that you find in Canada, you'll also find in the United States.
[01:29:38.400 --> 01:29:44.240]   However, you won't find a Texas in Canada, you won't find a South, you won't find,
[01:29:44.240 --> 01:29:53.920]   so there's the problem with absolutely everything in the United States besides extremism in every
[01:29:53.920 --> 01:30:05.520]   direction is that the lack of evenly distributed culture of laws of norms and etc. So it's interesting.
[01:30:06.640 --> 01:30:14.320]   And I don't know what this has to do with Google, but it's going to help.
[01:30:14.320 --> 01:30:16.320]   We do have a Texas. It's called Alberta.
[01:30:16.320 --> 01:30:20.400]   It's true. Actually, there are Texases in the world.
[01:30:20.400 --> 01:30:26.640]   Texas is a friend of mine. Alberta is no Texas, I tell you.
[01:30:26.640 --> 01:30:29.680]   It's like a nice, original Texas.
[01:30:29.680 --> 01:30:34.720]   When we went into Barcelona, I was just blown away. And I guess it's probably because we were
[01:30:36.000 --> 01:30:41.760]   affluent white people coming from the United States. But remember that once you're in Barcelona,
[01:30:41.760 --> 01:30:46.400]   you go anywhere in Europe without any customs controls. Any port of entry in the EU,
[01:30:46.400 --> 01:30:51.840]   basically they barely looked at us. They didn't look at our bags,
[01:30:51.840 --> 01:30:54.480]   they didn't look at anything. They barely looked at the passports again.
[01:30:54.480 --> 01:30:55.680]   It's very unnerving.
[01:30:55.680 --> 01:30:57.360]   That's the best part about going to Europe.
[01:30:57.360 --> 01:30:59.920]   And I know anywhere now, I'm in Europe. I'm going to Europe.
[01:30:59.920 --> 01:31:02.080]   So, you guys say hello, you show him your passport.
[01:31:02.960 --> 01:31:07.520]   It was almost as easy getting back in the US, frankly. It wasn't. In fact, Johnny Jett,
[01:31:07.520 --> 01:31:10.960]   our travel guys, always say, you got to get global entry, Leo. You got to get global entry,
[01:31:10.960 --> 01:31:18.480]   really speed it up. The line for global entry, when we get off the plane and JFK was around the block,
[01:31:18.480 --> 01:31:21.680]   we just walked right up, showed him our passport, walked right in.
[01:31:21.680 --> 01:31:27.200]   Everybody's got global entry now. So, the smart thing to do is not have it.
[01:31:30.400 --> 01:31:36.080]   That's what I think anyway. Here is an interesting story about Burger King
[01:31:36.080 --> 01:31:41.280]   and their new ads created by artificial intelligence.
[01:31:41.280 --> 01:31:47.440]   I don't think they actually were, although some people might consider ad agencies artificially
[01:31:47.440 --> 01:31:53.040]   intelligent. Gender reveal, bad, tender reveal, yum. It is a boy bird with crispy
[01:31:53.040 --> 01:31:57.440]   chicken tenders from burger thing, like 11, but minus one. Math is tasty.
[01:31:57.440 --> 01:32:01.840]   Tender plus sauce equals romantic dinner. Eat them out. Pick two of these for six money bucks.
[01:32:01.840 --> 01:32:07.520]   BK logo appears. I don't like it. Yeah, it's excellent.
[01:32:07.520 --> 01:32:14.480]   So natural and authentic. Burger King and it pressfully says the video is the work of a new
[01:32:14.480 --> 01:32:18.160]   deep learning algorithm, but an article from ad age says humans.
[01:32:19.200 --> 01:32:27.600]   Obviously. Have you seen there's a guy on Twitter who constantly posts,
[01:32:27.600 --> 01:32:36.480]   I made an artificial intelligence algorithm watch 100 episodes of Seinfeld and then it wrote one?
[01:32:36.480 --> 01:32:44.000]   He likes different things. And they're hilarious, but they're jokes. I mean, he makes them up.
[01:32:44.000 --> 01:32:47.280]   Okay, so this is after, this is an Olive Garden commercial after.
[01:32:48.080 --> 01:32:53.280]   That's a really good after watching Seinfeld. No, so he made it watch a whole bunch of Olive
[01:32:53.280 --> 01:32:59.440]   Garden commercial a group of friends laughs at a dinner table. Oh, click the first one.
[01:32:59.440 --> 01:33:05.680]   There you go. A group of friends laughs at the dinner table. A waitress comes to deliver what
[01:33:05.680 --> 01:33:11.760]   could be considered food waitress pasta nachos for you. We see the pasta nachos. They're warm.
[01:33:11.760 --> 01:33:17.680]   They're warm to feed it friend one. The menu is here. Waitress lasagna wings with
[01:33:17.680 --> 01:33:24.240]   extra Italy. We see the lasagna wings. There's more Italy than necessary friend two. I shall eat
[01:33:24.240 --> 01:33:31.840]   Italian citizens waitress unlimited stick. We see the unlimited stick. It is infinite. It is all.
[01:33:31.840 --> 01:33:38.720]   Yeah, they are warm and defeated. Friends three leave without me. I'm home waitress gluten
[01:33:38.720 --> 01:33:44.480]   classical from the kitchen. We see the gluten classical. We believe the waitress that it is
[01:33:44.480 --> 01:33:49.760]   from the kitchen. We have no reason not to believe friend four says nothing friend one.
[01:33:49.760 --> 01:33:54.080]   What is wrong friend four friend four says nothing friend two friend four. What is wrong
[01:33:54.080 --> 01:34:01.520]   friend four friend four smiles wide. Her mouth is full of secret soup. Olive Garden when you're here
[01:34:01.520 --> 01:34:08.960]   you're here says the wet voice. They're a great parody.
[01:34:10.560 --> 01:34:16.320]   Just to be clear he's writing these. Yes, but I think what this represents and the Burger King
[01:34:16.320 --> 01:34:23.680]   thing represents is the creeping realization that things that are presented as AI are not very smart
[01:34:23.680 --> 01:34:30.640]   by how we consider smartness. In fact human intelligence is quite an amazing thing that we are
[01:34:30.640 --> 01:34:36.560]   years and years and years from replicating in any convincing way. I mean best example I've ever seen
[01:34:36.560 --> 01:34:41.920]   is that the day after Bill Cosby was handcuffed and hauled off to prison.
[01:34:41.920 --> 01:34:45.760]   Twitter recommended that I follow him on Twitter. Oh my god.
[01:34:45.760 --> 01:34:49.760]   I don't think he'll be tweeting a lot. That's going to be too.
[01:34:49.760 --> 01:34:56.640]   And for them to realize that it come to the conclusion that I wanted to follow Bill Cosby
[01:34:56.640 --> 01:35:01.120]   because I mentioned him. I made some crack about Jell-O putting or something like that.
[01:35:01.120 --> 01:35:07.600]   And now all of a sudden it's like you should follow him. If a person recommended that I would say
[01:35:07.600 --> 01:35:10.000]   you're an idiot. You're one of the tricks from him.
[01:35:10.000 --> 01:35:17.520]   Right. It's funny though that that article points out that it's become part of marketing that
[01:35:17.520 --> 01:35:23.360]   you want to pretend that artificial intelligence is involved because it makes you sound really
[01:35:23.360 --> 01:35:28.800]   smart or that what you're doing is really and yet do you remember when Facebook came out with
[01:35:29.840 --> 01:35:34.080]   it was a called M or something it was its assistant that was going to help you
[01:35:34.080 --> 01:35:40.720]   became racist. And they pretended that it was AI even though it was human beings.
[01:35:40.720 --> 01:35:45.040]   Yeah. This is what happened with the law.
[01:35:45.040 --> 01:35:49.040]   A lot of companies are doing this and I think this is the big lesson for AI. I'm sorry to cut
[01:35:49.040 --> 01:35:57.200]   you off Matthew. Anytime somebody comes out with AI that has human assistance, their intention
[01:35:57.200 --> 01:36:02.480]   is to say okay. When there is a mistake, when the computers can't figure something out, we have
[01:36:02.480 --> 01:36:06.640]   this team that will figure it out and they will also train the algorithm so that over time
[01:36:06.640 --> 01:36:12.960]   will gradually phase out the humans. They've tried this with M as you mentioned. There's a
[01:36:12.960 --> 01:36:22.880]   great AI calendar scheduling meeting app called Amy from AI.No it's x.ai I believe is a company.
[01:36:22.880 --> 01:36:27.840]   Oh yeah. And I'm pretty sure they still use human assistance. The biggest
[01:36:27.840 --> 01:36:33.280]   area of this is driverless cars. What we were told was that they're developing these driverless
[01:36:33.280 --> 01:36:38.720]   cars and then they'll just turn them loose on the highways and they'll just flawlessly drive around.
[01:36:38.720 --> 01:36:46.160]   What they've come to realize though is that all of these actual examples of self-driving cars
[01:36:46.160 --> 01:36:51.120]   like in Arizona and so on, they have a NASA-like control room of people who are monitoring everything
[01:36:51.120 --> 01:36:58.160]   who can remotely take control. Because AI just cannot function like a human can,
[01:36:58.160 --> 01:37:04.400]   as Jeff would say, full stop. And so this idea that you're going to have control rooms of humans
[01:37:04.400 --> 01:37:09.680]   monitoring self-driving cars is going to be here for decades I think. And if you look at some of the
[01:37:09.680 --> 01:37:14.480]   in Silicon Valley, some of the self-driving delivery systems, those are heavily monitored
[01:37:14.480 --> 01:37:20.480]   remotely. The little ice cooler that goes down the sidewalk, completely monitored by a control room.
[01:37:20.480 --> 01:37:27.200]   And so the idea that AI is anywhere near being able to just function on its own without human
[01:37:27.200 --> 01:37:33.200]   input is a delusion. Is this not going to happen? I would argue that this also extends to social
[01:37:33.200 --> 01:37:38.640]   media. I mean, if you look at the problems that Facebook in particular has had in Twitter for that
[01:37:38.640 --> 01:37:47.440]   matter, trying to algorithmically or through machine learning recognize trolls or spam or
[01:37:47.440 --> 01:37:54.480]   harassment, there are nuances of human behavior that even some human beings aren't that great at
[01:37:54.480 --> 01:37:59.840]   recognizing, let alone being able to train an algorithm to do it. And that you just cannot do it
[01:37:59.840 --> 01:38:03.520]   at the kind of scale that Facebook is trying to do it.
[01:38:03.520 --> 01:38:09.600]   One of the most frustrating things about Zuckerberg's testimony in front of that Senate hearing
[01:38:09.600 --> 01:38:15.040]   was that one of the only satisfying answers that he gave was that, "Oh, we're on the brink of
[01:38:15.040 --> 01:38:19.760]   developing AI that will just handle this real time." And everybody's like, "Hmm, great. That's
[01:38:19.760 --> 01:38:26.880]   fantastic." In fact, that was the worst possible scenario. Essentially, Facebook is saying that we
[01:38:26.880 --> 01:38:34.720]   kind of view AI as this rug where we can sweep, under which we can sweep these things that we have
[01:38:34.720 --> 01:38:41.280]   to eventually apologize for. It'll all happen behind the scenes if we are deleting false positives
[01:38:41.280 --> 01:38:45.760]   because our algorithm falsely thinks they're fake news. No one will ever know because it's
[01:38:45.760 --> 01:38:51.680]   instantaneous in real time. And so that really bothered me. That the AI is going to handle it
[01:38:51.680 --> 01:38:56.000]   is not a good answer to any of these problems. It's the perfect answer.
[01:38:56.000 --> 01:38:59.920]   It's the perfect answer. Because A, a lot of people go, "That's great."
[01:38:59.920 --> 01:39:05.200]   And B, if it fails, who gets the blame? The AI, right? It's a perfect answer.
[01:39:06.240 --> 01:39:12.800]   This guy's no fool. By the way, that Olive Garden Parity is written by comedian Keaton
[01:39:12.800 --> 01:39:24.160]   Patti. You could follow him on Twitter @KEATONPATTI. And I, for one, was on your wings with more
[01:39:24.160 --> 01:39:28.640]   Italy than ever before. I think warm and defeated makes a nice show time.
[01:39:28.640 --> 01:39:35.360]   Warm and defeated. I oversee the pasto nachos. Pasta nachos. They're warm and defeated.
[01:39:36.240 --> 01:39:47.200]   Speaking of defeated, a sad day coming our way. March 2019, GeoCities will pass into internet
[01:39:47.200 --> 01:39:54.960]   history. I had a GeoCities page. Did you really? Yep. I'm trying to think. I don't think I ever did.
[01:39:54.960 --> 01:40:02.800]   Yep. The internet in 1990s was filled to the brim. Filled to the brim, right,
[01:40:02.800 --> 01:40:10.080]   seen it with flashy text art, pixelated graphics, neon colors, and broken HTML. Those were simply
[01:40:10.080 --> 01:40:18.000]   times. There's actually the best discovery in this article. Yeah, right. And then a mailbox
[01:40:18.000 --> 01:40:25.680]   it opens and closes. The best discovery here is the tumble log. One terabyte of kilobyte age photo op.
[01:40:27.040 --> 01:40:35.520]   Sounds like a title written, frankly, by AI. But it's a full of GeoCities pages.
[01:40:35.520 --> 01:40:41.200]   Nice. I don't know if you can still go to the Turkish Angora Breeders Union page? No.
[01:40:41.200 --> 01:40:48.320]   Who won't? Is this? Yeah, I guess Yahoo owns. Yeah, who Japan owns? Oh, yeah.
[01:40:48.320 --> 01:40:51.600]   Good times.
[01:40:53.280 --> 01:41:01.680]   Here's one. Something's wrong with this one. Physician assisted suicide. There is a page.
[01:41:01.680 --> 01:41:05.760]   That's a good looking page. What's really good about this site is they're all rendered in
[01:41:05.760 --> 01:41:11.040]   1990s era internet explorer. Nice. Yeah. Really authentic.
[01:41:11.040 --> 01:41:19.840]   One of the most shocking things about this GeoCities news is that Yahoo bought it in 1999
[01:41:20.640 --> 01:41:30.240]   for $3.6 billion. What unheard of back then for a value of price like that. That's just an outrageous
[01:41:30.240 --> 01:41:34.480]   amount for back then. I mean, back then. I was actually, if I remember Fred Wilson of
[01:41:34.480 --> 01:41:40.480]   Union Square Ventures, I think famous VC is invested in all sorts of startups. GeoCities was one of
[01:41:40.480 --> 01:41:47.040]   his first big payoffs. Oh, really? Yeah. Yeah. He took the money and run.
[01:41:47.040 --> 01:41:51.760]   Tont have money. Where can I find stupid money like that? It was going to be the new AOL.
[01:41:51.760 --> 01:41:54.880]   With Yahoo gone, it's really harder to find that kind of... You know,
[01:41:54.880 --> 01:42:00.240]   ironically, the idea behind GeoCities is actually quite forward thinking. It's like an open thing
[01:42:00.240 --> 01:42:04.640]   where people can post their own. It's a combination of social networking blogging, essentially.
[01:42:04.640 --> 01:42:11.920]   And also, you know, nothing like WordPress and Tumblr. If you were to hear a description of that
[01:42:11.920 --> 01:42:15.520]   and say that this was something that was available in the 90s, you'd think, wow, they were way ahead
[01:42:15.520 --> 01:42:20.880]   of their time. But now it's just viewed as the opposite of the head of their time.
[01:42:20.880 --> 01:42:26.480]   So, yeah, I do sort of kill that and Tumblr. That's really some good work by Yahoo.
[01:42:26.480 --> 01:42:34.000]   Yahoo Japan is different than the other Yahoo. Do tumblers are going to be killed?
[01:42:34.000 --> 01:42:39.520]   No, not killed so much as it's a fact. I've actually been spending more time on there.
[01:42:39.520 --> 01:42:45.280]   What is going on? This is owned by Verizon now, right? This is part of the OAS.
[01:42:45.840 --> 01:42:53.680]   Family of companies. Yeah. I wonder what Verizon has in mind for Tumblr because
[01:42:53.680 --> 01:42:59.360]   Tumblr is a pretty much wild west part of the net, right? Yeah. It's great software.
[01:42:59.360 --> 01:43:02.720]   Yeah, which is part of what I like about it. And it's wonderful software.
[01:43:02.720 --> 01:43:11.360]   It's also the sort of... I know there's a lot of porn and so on, but there's also... There's a lot of
[01:43:12.480 --> 01:43:18.880]   people who... I don't think would fit anywhere else. There's a lot of art. There's a lot of just
[01:43:18.880 --> 01:43:24.240]   odd people with things they're interested in. I find that type of stuff fast.
[01:43:24.240 --> 01:43:29.440]   Oh, yeah. Yeah. It's an easy way to make a blog. And it's kind of the original concept of a blog,
[01:43:29.440 --> 01:43:38.400]   which is a stream of stuff. I got it as my daughter started using it. And they almost exclusively
[01:43:38.400 --> 01:43:48.240]   consumed media in the form of animated GIFs from anime TV shows. So literally, that was 98% of
[01:43:48.240 --> 01:43:53.920]   their media consumption at one point, which I just found fastly. I had a very prime Tumblr address,
[01:43:53.920 --> 01:43:58.000]   which I constantly got emails people wanted, which is leo.tumbler.com.
[01:43:58.000 --> 01:44:04.400]   But I killed it. So if somebody's been looking for that, go ahead and get it.
[01:44:05.440 --> 01:44:08.480]   You know what? I don't think... I didn't really need to kill Tumblr.
[01:44:08.480 --> 01:44:16.400]   But getting off Instagram, Tumblr, Twitter, and Facebook was part of a larger thing. Stop
[01:44:16.400 --> 01:44:23.360]   looking at news, like Google News and Microsoft News and the news. Stop watching cable news to
[01:44:23.360 --> 01:44:28.400]   the degree that I can. I mean, I got to keep up on what's going on. But all that stuff was not
[01:44:28.400 --> 01:44:34.400]   helping my mental state. And now I've noticed a really... And I wonder if maybe it's just
[01:44:34.400 --> 01:44:40.720]   people I know, but a really negative trend. It's mostly young people. I should ask these
[01:44:40.720 --> 01:44:45.680]   Australians before they left. I notice it because I no longer looking at these things.
[01:44:45.680 --> 01:44:51.600]   People instead of conversation will show you stuff from the... Like hold up memes,
[01:44:51.600 --> 01:44:56.880]   like you're a dinner. And they go, "Look at that." Instead of conversing, have you seen this?
[01:44:56.880 --> 01:45:02.400]   I've actually done that. I mean, I do it with my daughters. Did you see this on Instagram?
[01:45:03.360 --> 01:45:10.320]   It's actually... Maybe I'm too deep into it, but I actually find it starts conversation.
[01:45:10.320 --> 01:45:17.200]   It is... Oh, yeah. I'm sure it does. I mean, and it is me trying to communicate with them
[01:45:17.200 --> 01:45:24.320]   in a way that they feel comfortable. Yeah. I guess if I had younger kids, I might. But
[01:45:24.320 --> 01:45:29.680]   now because I don't see Instagram or Facebook or anything, I kind of want to almost shield my
[01:45:29.680 --> 01:45:34.960]   eyes. Like, "No, no, don't show me that." I've actually been trying to do less. I mean,
[01:45:34.960 --> 01:45:41.520]   I have to be on Twitter for my job. I have to read the news. I have to know what's going on
[01:45:41.520 --> 01:45:46.160]   and what people are talking about. But I agree with you that I don't think it's fundamentally a
[01:45:46.160 --> 01:45:54.000]   healthy way to live all the time. So I'm trying to scale it back to the point where I do other
[01:45:54.000 --> 01:46:00.080]   things outside of kind of work or I try and put a limit on it and then go and look at
[01:46:00.080 --> 01:46:05.760]   funny cat videos or something. Although I guess you could make the case that that's all that this
[01:46:05.760 --> 01:46:13.040]   network is, is us finding stuff to show people on the internet. See, this... It is in a way. It is in a way.
[01:46:13.040 --> 01:46:20.400]   Sorry, my dad. The world needs to be shown things. I don't think that podcasts are addictive,
[01:46:20.400 --> 01:46:23.920]   though. I don't think I've ever heard anybody say that. The problem with social media is that it's
[01:46:23.920 --> 01:46:29.200]   designed and increasingly evolving to become more distracting and addictive and to gobble up more
[01:46:29.200 --> 01:46:32.800]   and more over time. Whereas I think podcasts gallop us up more over your time because there's
[01:46:32.800 --> 01:46:38.080]   lots of information and stories and stuff like that. Anyway, I'm the biggest podcast fan in the
[01:46:38.080 --> 01:46:43.280]   universe. Thank you for promoting podcasts. Yes. But I've been doing it for... I've been the biggest
[01:46:43.280 --> 01:46:46.800]   podcast fan in the world for a long, long time. But one of the things that
[01:46:49.040 --> 01:46:54.880]   is interesting and I think it's the best tip I can give people for avoiding the kinds of problems
[01:46:54.880 --> 01:47:03.040]   you're talking about is to not consume the feed that you get. So I have two Twitter accounts.
[01:47:03.040 --> 01:47:09.760]   One is for incoming and one is for outgoing. So my Galgan is where I connect to people that I know,
[01:47:09.760 --> 01:47:13.920]   that I connect to people that are interesting, that I connect for all the normal reasons you connect.
[01:47:13.920 --> 01:47:20.880]   But 99% of the time I don't look at that the stream. I have another stream which is cherry
[01:47:20.880 --> 01:47:27.760]   picked for news, for specific types of information that I like to get from Twitter. And it's completely
[01:47:27.760 --> 01:47:36.160]   devoid of rancor, of BS, of the kind of politics. I use a lot of mute words, a muted words feature,
[01:47:36.160 --> 01:47:43.680]   so I don't have to hear about the political stuff of the moment. And so I have one for input and
[01:47:43.680 --> 01:47:48.240]   one for output. Every once in a while I'll look at the output for the mic output account. But I
[01:47:48.240 --> 01:47:55.120]   recommend that it's not being on social media, it's consuming social media as a source of information.
[01:47:55.120 --> 01:48:02.640]   It's good for outgoing. It's not good for incoming. I spent a lot of time trying to come up with
[01:48:02.640 --> 01:48:11.840]   sort of ways of correcting that problem. So Twitter lists was one way and I never actually look at my
[01:48:11.840 --> 01:48:21.280]   stream. I only look at lists. And so theoretically it kind of focuses the stream on specific topics,
[01:48:21.280 --> 01:48:25.440]   although Trump infects everything eventually. Yeah.
[01:48:25.440 --> 01:48:31.840]   Yeah, I do exactly what you do, Mike, because of you actually. I killed my outbound account
[01:48:31.840 --> 01:48:35.760]   completely, but I have a links for Twitter account that is just news.
[01:48:35.760 --> 01:48:38.800]   Yeah, it's a great source of news. Yeah, I don't.
[01:48:38.800 --> 01:48:43.280]   Yeah, well, I don't consider a source for news, but I realize that the feed reader is kind of dead.
[01:48:43.280 --> 01:48:48.640]   RSS is kind of dead. Twitter is kind of supplanted it. And the one thing I find Twitter very useful
[01:48:48.640 --> 01:48:55.520]   for is like hot breaking news. So yes, if I hear a rumor, some celebrities passed aware there was a
[01:48:55.520 --> 01:49:00.160]   big earthquake or something like that, that seems to me better the fastest way to get that
[01:49:00.160 --> 01:49:04.320]   in for at least get the shape of that information, if not accurate news about it.
[01:49:05.120 --> 01:49:12.000]   They used to say journalism was the first rough draft of history. And I think Twitter in a way has
[01:49:12.000 --> 01:49:18.160]   taken over that even rougher. Yeah, even rougher. Well, one of the things that I did with my
[01:49:18.160 --> 01:49:22.800]   my incoming account is called the new news. And I've been using it as an incoming source of news
[01:49:22.800 --> 01:49:28.640]   for a long time. But a few months ago, I turned it into a breaking news feature. Now just retweet
[01:49:28.640 --> 01:49:34.160]   the very most breaking news. This isn't an ad. I don't care if people follow it or not. It's just
[01:49:34.160 --> 01:49:39.680]   something that I do impulsively because I got to just I got to share certain types of information.
[01:49:39.680 --> 01:49:44.720]   So it's political tech, stuff like that. And I just reshare it because it's so easy to do.
[01:49:44.720 --> 01:49:48.960]   But one of the things that I add to my feed in addition to sources that I add people who are
[01:49:48.960 --> 01:49:54.480]   super good at retweeting and the best retweeter on the internet, I guarantee you is Matthew Ingram.
[01:49:54.480 --> 01:49:55.440]   Really?
[01:49:55.440 --> 01:50:02.000]   So if yeah, if you follow one person on on Twitter, follow Matthew because you like you like retweet
[01:50:02.000 --> 01:50:10.800]   the best content. And it's like, if you get like, most people are retweeting guard like you get
[01:50:10.800 --> 01:50:14.880]   you look at your streaming like, why am I getting that on my stream? And then you realize somebody
[01:50:14.880 --> 01:50:19.680]   you know, somebody's retweeting somebody I followed for tech is retweeting sports ball. And I'm like,
[01:50:19.680 --> 01:50:24.640]   Oh my God, I don't want that. But but yeah, Matthew Ingram is like the guy to follow because
[01:50:24.640 --> 01:50:29.520]   you just have and this is this goes back to what a newspaper is you have intelligent journalists who
[01:50:29.520 --> 01:50:35.680]   were curating sources of information and information itself for you. And that's really what you
[01:50:35.680 --> 01:50:38.000]   everybody should be looking for on social.
[01:50:38.000 --> 01:50:42.880]   That is like the old days of Twitter five, six years ago, where there were people
[01:50:42.880 --> 01:50:44.400]   like those days.
[01:50:44.400 --> 01:50:51.040]   Yeah, who were who? Who was the guy? It was great. He became the Twitter journalist. He used it
[01:50:51.040 --> 01:50:56.560]   exclusively. Andy Carvin. Andy Carvin. Right. And it was great. You could follow Andy and you knew
[01:50:56.560 --> 01:50:59.440]   that you were going to be in the audience. Yeah. What happened? Is he still?
[01:50:59.440 --> 01:51:07.760]   Yeah, he's still so he's teaching actually at UBC. He made it a full time job at one point.
[01:51:07.760 --> 01:51:14.800]   He did. Yeah. He was at NPR originally, I think, and then yeah, he started
[01:51:14.800 --> 01:51:22.400]   reportedly, which was effectively a journalistic enterprise devoted to just doing that.
[01:51:23.440 --> 01:51:30.960]   And it was brilliant. And then the Omidyar network stopped funding it. And then he moved to now this.
[01:51:30.960 --> 01:51:40.960]   Yeah, his work was seminal in terms of using social media properly for journalism and turning.
[01:51:40.960 --> 01:51:46.480]   I interviewed him a number of times about it. And he said he looked at it as
[01:51:47.680 --> 01:51:55.440]   Twitter was effectively his newsroom. He did crowdsourcing journalism that was just unparalleled.
[01:51:55.440 --> 01:51:58.400]   He was the guy to read during the Arab Spring. Yeah.
[01:51:58.400 --> 01:52:05.200]   Another brilliant example is Nick Bilton for his wonderful book, "Hatching Twitter." He has all
[01:52:05.200 --> 01:52:10.640]   these very sort of storytelling approaches to things. Oh, it was a beautiful sunny day in Sunnyvale in
[01:52:10.640 --> 01:52:16.080]   2000, you know, October, whatever, 2006. And he got all that information by going back in Twitter
[01:52:16.080 --> 01:52:20.160]   and seeing what people are posting. He got little details about what people were wearing and what
[01:52:20.160 --> 01:52:25.200]   kind of muffin they got at Starbucks and all this kind of detail because people post that kind of
[01:52:25.200 --> 01:52:31.520]   stuff. And he was able to go back and use it as a sort of historic snapshot of what individual
[01:52:31.520 --> 01:52:37.760]   specific people were saying and doing and thinking and feeling and etc. And so as a journalistic
[01:52:37.760 --> 01:52:44.000]   storytelling research, if you're telling history, it's a wonderful, it's wonderful. Because of
[01:52:44.000 --> 01:52:49.520]   course, you can search and find all those details. Speaking of retweeting, I actually remember,
[01:52:49.520 --> 01:52:56.560]   believe it or not, in the early, early days, remember when retweeting was like brand new and
[01:52:56.560 --> 01:53:01.920]   you would hype the letters RT or the, yeah, there was no facility to do it. Yeah.
[01:53:01.920 --> 01:53:07.520]   And you all tweet. Yeah. I had a friend who got really, really mad at me for retweeting. He's like,
[01:53:07.520 --> 01:53:12.800]   why are you doing that? You should be posting your own thoughts. You shouldn't just be
[01:53:12.800 --> 01:53:18.160]   retweeting other people. And I was like, okay, I mean, sure, I have my own thoughts.
[01:53:18.160 --> 01:53:22.000]   That's one point of view. People say smart things and I want to like,
[01:53:22.000 --> 01:53:26.720]   cheer them on by. He just was outraged that this was not how you using Twitter.
[01:53:26.720 --> 01:53:31.120]   It's really a shame those of us who've been with Twitter for a long time to have watched its
[01:53:31.120 --> 01:53:37.840]   rise and fall, I think. I'm thinking back every time I see a hashtag and you see them all. I mean,
[01:53:37.840 --> 01:53:42.960]   Google's event coming up on Tuesday is hashtag made by Google. I think about Chris Messina,
[01:53:42.960 --> 01:53:48.000]   who invented it. And it was just kind of, you know, I know, Chris, you know, you just kind of a
[01:53:48.000 --> 01:53:52.400]   nice guy and he just said, you know, it'd be kind of cool if you put a hashtag in front of something
[01:53:52.400 --> 01:54:00.640]   and be like a kid. Chris, you got that from IRC. I think. Did he? Yeah. Yeah. Hashtags were
[01:54:00.640 --> 01:54:06.960]   already in use. They were already in use. Okay. You just used the idea. Well, it's, and then what
[01:54:06.960 --> 01:54:12.080]   Twitter was smart because they took these ideas that were really homegrown and they incorporated
[01:54:12.080 --> 01:54:19.600]   them into the system. Yeah. Yeah. I mentioned all that. I love how Instagram lets you follow a hashtag
[01:54:19.600 --> 01:54:25.280]   and it just shuffles it into your stream. That's new. And I do like that feature. Yeah. Yeah. Yeah.
[01:54:25.280 --> 01:54:32.960]   It is kind of, in a way, Twitter is like a live sort of research lab into what,
[01:54:33.760 --> 01:54:40.640]   into the kind of downsides and upsides of just mass kind of real time
[01:54:40.640 --> 01:54:47.840]   communication, social communication. It's like a conversation with millions of people,
[01:54:47.840 --> 01:54:53.680]   all of whom have different kind of competing agendas, some of whom are mentally disturbed.
[01:54:53.680 --> 01:54:59.600]   And I mean, it just it's a. It's just a common spot. You're talking about Kanye West now, right?
[01:54:59.600 --> 01:55:06.400]   Yeah. He's pretty good at getting attention for himself on Twitter as well.
[01:55:06.400 --> 01:55:12.240]   A little bit. A little bit. Yeah. I think Twitter is like a narcissist's dream.
[01:55:12.240 --> 01:55:14.320]   And I think that narcissists make the best trolls.
[01:55:14.320 --> 01:55:19.280]   Quite strong this whole point. Exactly. Exactly. I think that,
[01:55:19.280 --> 01:55:26.400]   to me personally, I like Twitter a lot. And I've been harping on this for years and years,
[01:55:26.400 --> 01:55:31.200]   especially on the show, that they just need to make one change. Let me delete other people's
[01:55:31.200 --> 01:55:37.440]   comments to my tweet. And then I can start conversations and I can moderate those conversations.
[01:55:37.440 --> 01:55:43.280]   And then beyond that, I don't need any other censorship or filtering or anything. Just let
[01:55:43.280 --> 01:55:50.000]   me start conversations and let them not be hijacked by people who are out to disrupt conversations.
[01:55:50.000 --> 01:55:53.520]   And that's it. That's all they have to do. It would be fantastic.
[01:55:53.520 --> 01:55:59.280]   It is pretty unprecedented that you can be having a conversation with people.
[01:55:59.280 --> 01:56:06.720]   And some rando can just, from across the world, can jump into your conversation and torpedo the
[01:56:06.720 --> 01:56:12.720]   whole thing. Well, what happens is I'm harping away at the president all the time. And sometimes
[01:56:12.720 --> 01:56:18.800]   you hit a nerve, you put on some list. And so it's distributed to all the trolls. And they all come
[01:56:18.800 --> 01:56:25.200]   after you. And it's just, you know, my comments here, my @ mentions looking at people that are
[01:56:25.200 --> 01:56:31.920]   at mentioning me, just becomes unusable for like a month because it's just flooded with Russian
[01:56:31.920 --> 01:56:37.520]   bots and all this garbage. And I just would love some control over that. And I really don't need
[01:56:37.520 --> 01:56:43.520]   any additional filtering beyond that. I think Twitter's fine the way it is, except for that.
[01:56:44.080 --> 01:56:52.160]   Well, here's some our last story. It's a very good news. Twitter has finally allowed you to opt
[01:56:52.160 --> 01:56:59.120]   out of curated Tom time lines and go back to the chronological tweet. They phased him out two years
[01:56:59.120 --> 01:57:07.120]   ago. Hallelujah. Yeah. Go into the settings. And I don't know. Have you guys done it yet? I don't
[01:57:07.120 --> 01:57:12.640]   know. I did actually. And this might seem like heresy. You like the curated.
[01:57:13.600 --> 01:57:20.160]   So I've been trying to spend less time on Twitter. And I actually find the when I come back. Stuff
[01:57:20.160 --> 01:57:26.320]   you might have missed. The stuff you missed is actually helpful. Yeah. I don't care about stuff
[01:57:26.320 --> 01:57:33.120]   I miss. Honestly, I would like to miss more stuff. I'm worried about missing things. It bugs
[01:57:33.120 --> 01:57:39.440]   the hell on me when I see a two day old tweet at the top of my. Really? Yes. Yeah. You too. I don't
[01:57:39.440 --> 01:57:44.880]   know. I'm of two minds about it. Like I said, if I was on it all the time, like I used to be,
[01:57:44.880 --> 01:57:52.240]   then I would have definitely much more preferred the chronological. But I do find it useful to get
[01:57:52.240 --> 01:57:56.560]   kind of I'm actually looking at Twitter discover or whatever it's called now,
[01:57:56.560 --> 01:58:02.080]   which I never used to do because it also used to be filled with garbage. But just to try and find out,
[01:58:02.080 --> 01:58:07.680]   you know, is there a Twitter moment about something that's going on? That's interesting. Is there
[01:58:07.680 --> 01:58:12.640]   something? Is there a conversation going on somewhere that I should know about? Because I am trying to
[01:58:12.640 --> 01:58:16.320]   dip in and out more often instead of just having it on all the time. The thing that continually
[01:58:16.320 --> 01:58:22.080]   comes up for me when we talk about Twitter is why do we let this company take something that is
[01:58:22.080 --> 01:58:27.360]   really of potentially of a lot of value? Why do we let them be in charge of this?
[01:58:27.360 --> 01:58:33.360]   Seriously. Why don't why don't all the journalists, all the people who are good and interesting on
[01:58:33.360 --> 01:58:38.560]   Twitter? Why don't they do something different? Why don't they go somewhere? Why doesn't somebody
[01:58:38.560 --> 01:58:44.720]   create something because Twitter is the worst possible steward of this thing that is valuable?
[01:58:44.720 --> 01:58:48.720]   But people have created things and no one goes. No, I used to mess it up briefly.
[01:58:48.720 --> 01:58:53.120]   Yeah, I know. Yeah, I messed it on. What's the other one? The app.net?
[01:58:53.120 --> 01:58:59.600]   That's gone. That was going to be paid for and mess it on is great, but there's nobody there.
[01:58:59.600 --> 01:59:03.760]   There's that's the problem. And that's why everybody who does good content on Twitter
[01:59:03.760 --> 01:59:08.560]   should move to Mastodon. And then we could finally leave this hell hole behind.
[01:59:08.560 --> 01:59:13.760]   We just need to convince people to do it somehow. People are sheep.
[01:59:13.760 --> 01:59:20.240]   Leo, I try to get everybody to go to Google plus and nobody. I remember that.
[01:59:20.240 --> 01:59:26.240]   I went. I was there. Thank you. You gave me a most powerful human phenomenon.
[01:59:26.880 --> 01:59:34.800]   Imagineable like Twitter should have failed so many times because its infrastructure was terrible.
[01:59:34.800 --> 01:59:41.920]   The company management was ridiculous. It's one of the probably worst managed companies in history.
[01:59:41.920 --> 01:59:51.600]   And yet it became this massive success because it was so compelling to use and addicting.
[01:59:52.160 --> 01:59:57.040]   And people just couldn't help themselves no matter how remember how people used to complain
[01:59:57.040 --> 02:00:01.200]   to have this network goes down all the time and they would complain on Twitter about how
[02:00:01.200 --> 02:00:04.720]   painful it was to use Twitter and then they would just come back and keep doing it was
[02:00:04.720 --> 02:00:10.720]   they could not fail. They tried so many times to fail and they failed to fail.
[02:00:10.720 --> 02:00:18.640]   Yep. Facebook too though. I mean Facebook how many times did they apologize for this
[02:00:18.640 --> 02:00:23.200]   that and the other thing and the list of failed like ideas that they came out with
[02:00:23.200 --> 02:00:27.680]   mostly copying other companies just blatantly. Some of the things that they
[02:00:27.680 --> 02:00:31.600]   copied from Snapchat and other places have actually succeeded for them.
[02:00:31.600 --> 02:00:36.160]   But for the most part it's been one failure after another but those network effects.
[02:00:36.160 --> 02:00:40.480]   Everybody's on Facebook. So that's why you got Facebook. It's not because Facebook is such a
[02:00:40.480 --> 02:00:46.720]   great one. They're terrible stewards again. But it's like actually my youngest daughter
[02:00:48.000 --> 02:00:53.760]   deleted her account on Facebook and then eventually she was forced to sign up again
[02:00:53.760 --> 02:00:59.920]   because everyone was there. All her family members all her extended family. There was no way to know
[02:00:59.920 --> 02:01:05.760]   what was going on or to see the photos of the baby or the wedding or whatever.
[02:01:05.760 --> 02:01:11.120]   So she was basically sucked back into it in the same way we all have to use email even though
[02:01:11.120 --> 02:01:20.560]   they don't like it. Yep. By the way we were talking on Sunday about Google's 20th anniversary and
[02:01:20.560 --> 02:01:25.920]   what did we use before Google and there was a lot of speculation. Thanks to Jessica Condett we now know
[02:01:25.920 --> 02:01:32.320]   three dead search engines remembering dog pile, ass, jeeves and alta vista. We all agree.
[02:01:32.320 --> 02:01:36.080]   Alta vista. She forgot a few like. James was actually pretty good. She wasn't even born
[02:01:36.560 --> 02:01:42.960]   practically. But what about excite right? There were a few others. But this is the fact or fact
[02:01:42.960 --> 02:01:49.440]   actually I loved from her article. The first use of the word Google as a transitive verb
[02:01:49.440 --> 02:01:55.440]   is on America television was in 2002 Buffy the vampire slayer. And of course Jessica would know
[02:01:55.440 --> 02:01:59.760]   this fourth episode of the seventh and final season. Jessica's gonna be on Twitter on Sunday right?
[02:02:00.560 --> 02:02:06.320]   Fast enough Buffy will. Oh, Xander in the gang sounds like when I'm at Kavanaugh's parties.
[02:02:06.320 --> 02:02:11.200]   Buffy will. Oh, Xander in the gang are trying to help. Cassie a high school student
[02:02:11.200 --> 02:02:16.800]   who's cryptically saying she's going to die next week in Buffy's dining room. They search through
[02:02:16.800 --> 02:02:21.920]   hard copies of Cassie's medical records to find nothing noteworthy. Willow tapping away on a
[02:02:21.920 --> 02:02:28.560]   thick white. I book turns to says I book. Have you coogled her yet? Xander replies jokingly. Will
[02:02:28.560 --> 02:02:35.280]   wow. She's 17. That's amazing. You know, to another example of the power of Google
[02:02:35.280 --> 02:02:43.120]   emerged today or recently. You we've all heard about the New York Times expose on the Trump empire
[02:02:43.120 --> 02:02:48.480]   and Fred Trump and the family and the real estate deals. Donald Trump was a millionaire by age eight.
[02:02:48.480 --> 02:02:54.640]   He was making $200,000 a year by the time he was three. All this kind of stuff. That entire
[02:02:54.640 --> 02:03:00.080]   investigative report began with a Google search. What? And they talked about the Google search in
[02:03:00.080 --> 02:03:04.400]   the article or the follow up articles where there was this they had this
[02:03:04.400 --> 02:03:13.040]   with odd phrase that Fred Trump Donald Trump's father used for recycling money back to the kids
[02:03:13.040 --> 02:03:19.120]   or something like that. It's called I don't know something mortgage was like a preemptive
[02:03:19.120 --> 02:03:24.080]   mortgage or I don't know some some weird phrase that is not used by anybody else. And they simply
[02:03:24.080 --> 02:03:29.360]   searched Google for that phrase and the word Trump and they found some documents and they're like,
[02:03:29.360 --> 02:03:34.960]   wait a minute, this document appears and they started pulling that thread and this entire expose
[02:03:34.960 --> 02:03:41.680]   came from that thread pulling. There you go. All the world's information at your fingertips.
[02:03:42.400 --> 02:03:49.360]   Yep. Let's take a break here. Picks of the week. Mike Elgin. This has been a great show.
[02:03:49.360 --> 02:03:54.160]   We're going to fire Jeff and Stacy. Yeah. Fire on you guys. Elgin.com
[02:03:54.160 --> 02:04:03.760]   and becoming nomad.net. Matthew Ingram with one T. He's on the Twitter at Matthew.i. M-A-T-H-E-W-I
[02:04:03.760 --> 02:04:10.720]   chief digital writer at the Columbia journalism review. And I show today brought to you by my home
[02:04:10.720 --> 02:04:17.040]   on the internet WordPress. Do you use WordPress? You ought to use WordPress. I've used WordPress
[02:04:17.040 --> 02:04:21.600]   practically since it's started since Matt Mullenweg invented it for a long time. I hosted it myself
[02:04:21.600 --> 02:04:26.880]   until I realized, wait a minute. What am I doing spending money for a server and spending a lot of
[02:04:26.880 --> 02:04:31.840]   time running it when I could just have WordPress do the work? That's when I went and moved. Leo
[02:04:31.840 --> 02:04:38.320]   Laport.com to WordPress.com. And man am I glad it's hassle free WordPress takes care of the hosting,
[02:04:38.320 --> 02:04:47.040]   the security, the software updates. You focus on your content. And I also admit that for a while
[02:04:47.040 --> 02:04:54.240]   I went through a winter of blogging winter, nuclear winter as I started using Twitter and
[02:04:54.240 --> 02:05:00.800]   Facebook. But now that I'm back, it feels so good to post my content on my site where it belongs.
[02:05:00.800 --> 02:05:06.880]   You know, I posted some stuff from the trip and I got 102 followers from WordPress.
[02:05:06.880 --> 02:05:13.200]   See WordPress.com has its own follow system. I love that. And WordPress.com, you can choose
[02:05:13.200 --> 02:05:17.840]   from hundreds of designs, match your brand or your vision. It's great for a personal blog,
[02:05:17.840 --> 02:05:23.520]   but it's really good for a business blog too. You really ought to have a website for your business.
[02:05:23.520 --> 02:05:29.280]   That Facebook page isn't owned by you. It's owned by a guy named Zuckerberg. That Twitter page
[02:05:29.280 --> 02:05:35.360]   isn't owned by you. I don't know who it's owned by, but it ain't you. And you can't control it,
[02:05:35.360 --> 02:05:41.680]   right? But you can control your website. The Akismet, Annie Spam Engine for comments is the best ever.
[02:05:41.680 --> 02:05:47.600]   I love the way comments work on WordPress. And I use them all the time. I just think it's fantastic.
[02:05:47.600 --> 02:05:52.640]   Upload the images right to your site so you're hosting them, audio and video as well.
[02:05:52.640 --> 02:05:58.000]   Not on YouTube, not somewhere else where it could be taken down or dinged.
[02:05:58.000 --> 02:06:03.760]   Import X and export content to and from your site. Nothing's ever trapped there. It's your site,
[02:06:03.760 --> 02:06:10.080]   your home, your content. WordPress knows that. They support it. Great tools for search engine
[02:06:10.080 --> 02:06:16.720]   optimizations, social media linking. So your followers, your readers can share your site with
[02:06:16.720 --> 02:06:21.280]   their Twitter feed and their Facebook. Great marketing tools too. And I love the WordPress
[02:06:21.280 --> 02:06:26.000]   app. I use that to post all through my vacation on my phone and my tablet was great.
[02:06:26.000 --> 02:06:32.480]   You get great support to 24/7, even with the $4 a month plan. Yep, that's right.
[02:06:32.480 --> 02:06:39.040]   WordPress plans started $4 a month. When you add it all up, it's no surprise. 31% of all the
[02:06:39.040 --> 02:06:47.040]   websites in the world, 31% run on WordPress. But don't do it yourself. Go to WordPress.com,
[02:06:47.040 --> 02:06:55.920]   15% off any new plan purchase at WordPress.com/swig. Create your website. WordPress.com/swig.
[02:06:55.920 --> 02:07:01.440]   I just added a news feed because I realized now that I don't use Twitter, I don't have a way of
[02:07:01.440 --> 02:07:08.000]   putting stories that we're going to cover on the shows on my Twitter feed. So I've now put
[02:07:08.000 --> 02:07:12.560]   them on my blog on a site, just an RSS feed. It makes it so easy to do that. It took me three
[02:07:12.560 --> 02:07:21.760]   seconds. WordPress.com/swig. Time for pics. I'm going to start because Lisa and I watched a
[02:07:21.760 --> 02:07:27.120]   great documentary. Want to be my neighbor. Last night calls an invitation. Be my neighbor.
[02:07:27.120 --> 02:07:33.040]   Have you guys seen this yet? No, I want to. Oh my God, I was balling my eyes out. It's now
[02:07:33.040 --> 02:07:38.720]   available on iTunes. It's a riddle or you can buy it. I bought it for 10 bucks because I want to
[02:07:38.720 --> 02:07:46.400]   own this. I want to show this to anybody. It is the most moving story of Mr. Rogers, Fred Rogers,
[02:07:46.400 --> 02:07:51.760]   who created a kids show in the 50s that everybody or actually I guess it was the 60s. I thought it was
[02:07:51.760 --> 02:07:57.120]   kind of boring. They interviewed one producer who said everything you know about good TV,
[02:07:57.120 --> 02:08:07.520]   Mr. Rogers did the opposite. It was slow. The sets were bad. And it was brilliant because he
[02:08:07.520 --> 02:08:14.560]   spoke to children as equals. And he changed the world. And we need a little bit more of Mr.
[02:08:14.560 --> 02:08:20.400]   Rogers in the world. So if you haven't seen this, I know you've seen the show, Mr. Rogers,
[02:08:20.400 --> 02:08:26.160]   but there is a great documentary called Won't You Be in My Neighbor? And I just can't recommend
[02:08:26.160 --> 02:08:31.760]   it more highly. It's beautiful. It's really. I was in tears the whole time.
[02:08:31.760 --> 02:08:40.080]   In addition to what he achieved for children, he entered into the public sphere with his children
[02:08:40.080 --> 02:08:44.880]   show during the Vietnam War, where kids were watching the war on TV. He was aware of this.
[02:08:45.440 --> 02:08:50.960]   A lot of social strife and so on. And he tried to do something about it. But in addition to
[02:08:50.960 --> 02:08:58.960]   all that good work that he did, it's also so gratifying to see a media visionary create his
[02:08:58.960 --> 02:09:06.400]   entirely new world from scratch by himself. Well, not by himself entirely, but like
[02:09:06.400 --> 02:09:14.000]   he invented a language for how to talk to children. He demystified things like superstition. For
[02:09:14.000 --> 02:09:20.480]   example, the king of the land of make believe was Friday the 13th. And every time they had a
[02:09:20.480 --> 02:09:26.400]   show that was actually Friday the 13th, it was the king's birthday. And he was deliberately trying
[02:09:26.400 --> 02:09:33.760]   to demystify the superstition around Friday the 13th. Little stuff like that. Pretty unbelievable.
[02:09:33.760 --> 02:09:38.640]   It was a huge champion, obviously, public television too. I mean, he gave a
[02:09:38.640 --> 02:09:46.160]   safety speech about the value of public television. It's in the movie. And it's an incredible moment.
[02:09:46.160 --> 02:09:51.200]   And they got it all in the documentary. It's incredible. And they address all of this stuff.
[02:09:51.200 --> 02:09:55.840]   Vietnam, he did a show about debt. He did a whole week about debt. Race. Race.
[02:09:55.840 --> 02:10:03.680]   There was a big nationwide scandal at some point where Black people were being banned from swimming
[02:10:03.680 --> 02:10:11.040]   pools. So I think he's had a Black mailman on the show. And they said, and what they did was they
[02:10:11.040 --> 02:10:17.440]   had a little foot bath. And he invited, he was using it to soak his feet. And he invited a
[02:10:17.440 --> 02:10:21.440]   mailman to soak his feet. And they soak their feet together. Like little things like that.
[02:10:21.440 --> 02:10:28.880]   Not overtly political, but just bringing a level of humanitarials.
[02:10:30.080 --> 02:10:36.560]   That scene is also, and they talked to the actor who played the cop about it. And they
[02:10:36.560 --> 02:10:42.480]   showed that scene. It is one of the best documentaries I've seen in a long time. And I love documentaries.
[02:10:42.480 --> 02:10:46.880]   Won't you be my neighbor? And it's on iTunes now. I'm sure that means it's also on Google Play.
[02:10:46.880 --> 02:10:52.720]   And the Tom Hanks movie is coming out. That's right. Tom Hanks plays Mr. Rogers.
[02:10:52.720 --> 02:10:57.760]   Yes, Mr. Rogers. Wow. The first photo from the set came out. And there he is in the red sweater,
[02:10:57.760 --> 02:11:01.280]   and the sneakers and all that stuff. You know, there's a perfect guy to play Mr. Rogers.
[02:11:01.280 --> 02:11:04.000]   Yeah, definitely. Mike, what's your pick this week?
[02:11:04.000 --> 02:11:10.160]   Nothing is consequential or important as that, for sure. But something that's kind of nice for
[02:11:10.160 --> 02:11:15.840]   everybody. It's a website called Light Apps List. And it's exactly what it sounds like. Somebody
[02:11:15.840 --> 02:11:22.000]   is curating all the light versions of apps that use less battery, less storage, less memory.
[02:11:22.000 --> 02:11:27.520]   For those of us who want to like lighten the load, the collective load of what's happening on our
[02:11:27.520 --> 02:11:32.480]   smartphones. So far, all of the, there's a brand new site, everything is for Android. They're going
[02:11:32.480 --> 02:11:38.400]   to introduce iPhone versions as well. And they're inviting their crowdsourcing and stuff like that.
[02:11:38.400 --> 02:11:42.480]   Going to be adding things to it. But I love the idea. So there are light versions of Twitter,
[02:11:42.480 --> 02:11:49.920]   light versions of Instagram, of Google Maps, of Google Go, of Gmail, all these like
[02:11:49.920 --> 02:11:55.360]   things that people might have heard about or maybe haven't heard about. But you can,
[02:11:55.360 --> 02:12:00.080]   by replacing the full featured apps with light versions of those apps, you can like,
[02:12:00.080 --> 02:12:07.440]   radically extend your battery life. Probably the biggest benefit. So check it out, lightappslist.com.
[02:12:07.440 --> 02:12:11.840]   And it'll click you right to the Play Store to download.
[02:12:11.840 --> 02:12:13.920]   And light is spelled L-I-T-E.
[02:12:13.920 --> 02:12:15.280]   Yes.
[02:12:15.280 --> 02:12:20.880]   Lightappslist.com. Matthew, you got a pick or something you're interested in?
[02:12:21.840 --> 02:12:26.800]   Yeah, this was actually on the rundown, but we didn't get to it.
[02:12:26.800 --> 02:12:27.520]   So good.
[02:12:27.520 --> 02:12:32.480]   This is something I'm interested in, Sir Tim Berners-Lee, who you might have heard of.
[02:12:32.480 --> 02:12:36.720]   Oh, yeah. We talked about this on Twitter. I want you to explain what this is all about.
[02:12:36.720 --> 02:12:43.920]   I didn't understand a word. I was like, so what it sounds like to me is Sir Tim
[02:12:44.480 --> 02:12:52.960]   wants to effectively decentralize, re-decentralize the web. So I think for some time, he's been concerned
[02:12:52.960 --> 02:12:59.440]   about kind of centralization around platforms like Facebook and even Google for that matter.
[02:12:59.440 --> 02:13:09.760]   And I think, so he started something called Solid. And Solid is effectively a protocol or a way of
[02:13:09.760 --> 02:13:17.120]   constructing apps that will function in a kind of distributed and decentralized way.
[02:13:17.120 --> 02:13:22.000]   That's the closest I can get to understanding it. He wrote something ironically, in a way,
[02:13:22.000 --> 02:13:28.400]   on Medium about it, because Medium is a bit of a wall garden. But the idea is to try and,
[02:13:28.400 --> 02:13:38.400]   in his words, capture kind of what the web could have been, something that where users have a lot
[02:13:38.400 --> 02:13:45.360]   more control and there's a lot more sort of back and forth and interaction and interoperability
[02:13:45.360 --> 02:13:51.920]   and control over data, things that we've kind of been prevented from having in a way,
[02:13:51.920 --> 02:13:57.680]   because of the way the web has developed. Now, I have to say I looked a little bit at Solid. I
[02:13:57.680 --> 02:14:05.120]   installed a sort of, I signed up for an iteration of it. I looked at some of the apps that are
[02:14:05.120 --> 02:14:11.600]   available. There's like a notepad sort of thing. It's very, very early. It's like looking at,
[02:14:11.600 --> 02:14:21.760]   you know, a web browser in 1994 or whatever. So it's, but it's, the philosophy is fascinating.
[02:14:21.760 --> 02:14:26.960]   And certainly if anybody can kind of push this idea forward, it's Tim.
[02:14:27.680 --> 02:14:38.000]   Tim has a company called inruptinrupt.com. Solid is open source inrupt is a company designed to,
[02:14:38.000 --> 02:14:44.880]   I somehow use Solid. It isn't all clear. He's taken a leave from the World Wide Web Consortium
[02:14:44.880 --> 02:14:51.680]   to do this. So I think it's, he's pretty darn serious about it. And I've been reading up on Solid
[02:14:51.680 --> 02:14:59.200]   and you can make Solid apps if you're familiar with HTML and JavaScript, typically React,
[02:14:59.200 --> 02:15:06.480]   Angular, or jQuery, if you as well. So there is some, I want to see people make some apps.
[02:15:06.480 --> 02:15:13.920]   Yeah. There aren't many yet. I mean, it's still pretty early. But the idea is,
[02:15:13.920 --> 02:15:20.560]   you know, the gives more power to individual users and sort of more power to distributed
[02:15:20.560 --> 02:15:26.160]   iterations of things as opposed to centralizing everything behind one company.
[02:15:26.160 --> 02:15:33.040]   Maybe this is the solution to that network effect problem. If you get enough people to use Solid,
[02:15:33.040 --> 02:15:38.400]   then you could create apps that take advantage of that with that invading people's privacy.
[02:15:38.400 --> 02:15:43.840]   You could have a solid Facebook or Twitter and you would actually have the network effect
[02:15:43.840 --> 02:15:50.000]   immediately. I don't know. It's a really great idea and I wish him success. But I don't know if I
[02:15:50.000 --> 02:15:58.080]   really get it. I think it's going to take a while to see what it's actually capable of.
[02:15:58.080 --> 02:16:03.280]   Matthew Ingram, we love reading your stuff. And I was just giving you a hard time about the
[02:16:03.280 --> 02:16:07.440]   podcast bubble bursting because you know what? You were absolutely, everything you wrote in that
[02:16:07.440 --> 02:16:13.360]   article was absolutely right on and accurate. Thanks. Yeah. Follow Matthew Effect. You taught
[02:16:13.360 --> 02:16:19.360]   me a thing or two. Follow Matthew at cjr.org where he's chief digital writer or on the Twitter
[02:16:19.360 --> 02:16:27.360]   at Matthew, I M A T H E W one T I. Thank you, Matthew. Thanks for having me. You'll find Mike
[02:16:27.360 --> 02:16:33.120]   Elgin at his page Elgin.com and find out more about his amazing travel adventures that Mike
[02:16:33.120 --> 02:16:40.800]   and the Miradue do some amazing stuff. Guess from nomad.net. Join us abroad. We're doing Italy,
[02:16:40.800 --> 02:16:47.920]   France, Morocco, Mexico City and Barcelona. And you can come and drink with Mike all week. Oh, man.
[02:16:47.920 --> 02:16:54.720]   Oh, man. That's nomad.net. I can't wait to do this. Yeah. Mexico City's coming up. You sold out the
[02:16:54.720 --> 02:16:59.360]   Prosecco experience coming up in a couple of ways, but Mexico, Mexico City's Morocco. That's
[02:16:59.360 --> 02:17:04.720]   might be the one I want to do. We had one day in Tangier and we're blown away. I can't. I
[02:17:04.720 --> 02:17:10.000]   would really like to go back. So good. And we're going to be on a camel just like you were,
[02:17:10.000 --> 02:17:14.960]   except we're going to actually enter into the Sahara Desert. What? Yeah, we did the tourist camel.
[02:17:14.960 --> 02:17:19.840]   He's going to circle. That was more than enough, by the way.
[02:17:19.840 --> 02:17:28.000]   Thank you, Mike. Thank you, Matthew. We do this week in Google every Wednesday, 130 Pacific, 430
[02:17:28.000 --> 02:17:32.320]   Eastern. That's 2030 UTC. If you want to come by say, hi, we'd love to see you.
[02:17:32.320 --> 02:17:37.440]   You can also watch the live stream. If you want to be like those Australian kids,
[02:17:37.440 --> 02:17:42.080]   just email tickets and Twitter TV. We'll put a chair out for you. We got 47. We now know
[02:17:42.080 --> 02:17:48.080]   you can also watch live from the convenience of your home at Twitter TV slash live. If you do
[02:17:48.080 --> 02:17:54.320]   that, join us in the chatroom, IRC dot Twitter dot TV. And of course, you can always subscribe.
[02:17:54.880 --> 02:17:59.760]   Your favorite podcatcher will have a copy or go to twit.tv/twig for more information.
[02:17:59.760 --> 02:18:06.960]   Are Jeff and Stacy back? No, I think next week they're gone again. So we will.
[02:18:06.960 --> 02:18:12.960]   What? Yes, hard to believe that pretty soon we're going to forget that they even existed.
[02:18:12.960 --> 02:18:17.760]   This was a lot. I have to say, this is a great conversation. So I thank you.
[02:18:17.760 --> 02:18:23.200]   The only record will be Twitter. We'll see you next time on this week in Google. Bye.
[02:18:23.200 --> 02:18:24.200]   Bye-bye!
[02:18:24.200 --> 02:18:33.200]   [Music]

