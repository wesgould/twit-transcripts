;FFMETADATA1
title=You Can't Shoot Their Feet
artist=Leo Laporte, Jeff Jarvis, Joan Donovan
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-03-31
track=657
language=English
genre=Podcast
comment=Russian propaganda, how platforms fight disinformation, Eric Schmidt
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:03.760]   It's time for Twig this week in Google and Stacey are out.
[00:00:03.760 --> 00:00:06.720]   Jeff Jarvis is here and we brought in Joan Donovan,
[00:00:06.720 --> 00:00:12.800]   who's a professor at Harvard's Kennedy Center and an expert on disinformation.
[00:00:12.800 --> 00:00:15.280]   We'll talk about the methods of disinformation,
[00:00:15.280 --> 00:00:17.920]   how Russia has used disinformation,
[00:00:17.920 --> 00:00:22.960]   and who's responsible for keeping disinformation out of the ecosystem.
[00:00:22.960 --> 00:00:25.040]   It's all coming up next on Twig.
[00:00:28.240 --> 00:00:30.000]   Podcasts you love.
[00:00:30.000 --> 00:00:31.280]   From people you trust.
[00:00:31.280 --> 00:00:34.000]   This is Twig.
[00:00:34.000 --> 00:00:46.800]   This is Twig. This week in Google, episode 657, recorded Wednesday, March 30th, 2022.
[00:00:46.800 --> 00:00:48.960]   You can't shoot their feet.
[00:00:48.960 --> 00:00:53.920]   This week in Google is brought to you by 8 Sleep.
[00:00:53.920 --> 00:00:57.840]   Good sleep is the ultimate game changer and nature's best medicine.
[00:00:58.160 --> 00:01:05.200]   Go to 8sleep.com/twig to check out the PodPro cover and save a $150 check out.
[00:01:05.200 --> 00:01:08.640]   8 sleep currently ships within the US, Canada, and the UK.
[00:01:08.640 --> 00:01:11.520]   And by Blue Land.
[00:01:11.520 --> 00:01:16.560]   Stop wasting water and throwing out more plastic and get Blue Land's revolutionary
[00:01:16.560 --> 00:01:18.640]   refill cleaning system instead.
[00:01:18.640 --> 00:01:24.320]   Right now you can get 20% off your first order when you go to bluland.com/twig.
[00:01:25.280 --> 00:01:32.320]   And by Our Crowd. Our Crowd helps accredited investors invest early in pre-IPO companies,
[00:01:32.320 --> 00:01:34.640]   alongside professional venture capitalists.
[00:01:34.640 --> 00:01:41.200]   Join the fastest growing venture capital investment community at ourcrowd.com/twig.
[00:01:41.200 --> 00:01:45.280]   It's time for Twig this weekend.
[00:01:45.280 --> 00:01:46.720]   Google!
[00:01:46.720 --> 00:01:50.880]   Hard to do a show without two of our stalwarts.
[00:01:50.880 --> 00:01:54.080]   Stays Higginbotham and Anteproop both have the week off.
[00:01:54.080 --> 00:01:55.200]   Jeff Jarvis is here.
[00:01:55.200 --> 00:01:56.880]   But you can't get rid of me, man.
[00:01:56.880 --> 00:01:58.400]   You try and try everything.
[00:01:58.400 --> 00:02:00.000]   No, I'm still here.
[00:02:00.000 --> 00:02:00.960]   Oh my goodness.
[00:02:00.960 --> 00:02:06.480]   He's the Leonard Taap professor for journalistic innovation at the Great New York.
[00:02:06.480 --> 00:02:10.960]   Graduate School of Journalism at the City University of New York.
[00:02:10.960 --> 00:02:13.280]   Welcome.
[00:02:13.280 --> 00:02:16.160]   Question that has been burning in my soul.
[00:02:16.160 --> 00:02:16.880]   Yes.
[00:02:16.880 --> 00:02:17.680]   For a week?
[00:02:17.680 --> 00:02:17.920]   Yes.
[00:02:17.920 --> 00:02:20.080]   What are you sitting on today?
[00:02:20.880 --> 00:02:24.400]   I'm sitting basically it's a bicycle seat on a stick.
[00:02:24.400 --> 00:02:26.560]   You're still sitting on that damn thing?
[00:02:26.560 --> 00:02:30.400]   No, I'm getting used to it.
[00:02:30.400 --> 00:02:36.160]   My butt's numb, but my core strength is amazing.
[00:02:36.160 --> 00:02:41.920]   So he sat on this last week and couldn't sit still because his rear end is dead.
[00:02:41.920 --> 00:02:43.440]   You're not supposed to sit still.
[00:02:43.440 --> 00:02:44.720]   You're supposed to be dead.
[00:02:44.720 --> 00:02:46.080]   Constantly moving.
[00:02:46.080 --> 00:02:51.040]   Hey, that's so we brought in the big guns to take over for Stacy and Ant.
[00:02:51.040 --> 00:02:54.960]   This week, Joan Donovan is here from the Shorenstein Center.
[00:02:54.960 --> 00:02:56.640]   We should get Carol Shorenstein a song.
[00:02:56.640 --> 00:03:01.680]   Carol Shorenstein Center on media policy,
[00:03:01.680 --> 00:03:03.680]   media politics and public policy.
[00:03:03.680 --> 00:03:04.880]   It's great to have you Joan.
[00:03:04.880 --> 00:03:06.400]   We've had her on many times before.
[00:03:06.400 --> 00:03:10.720]   She is the research director there and an expert on disinformation,
[00:03:10.720 --> 00:03:13.280]   which has kept you pretty busy.
[00:03:13.280 --> 00:03:14.400]   I'm thinking of late.
[00:03:15.040 --> 00:03:16.720]   Oh, I don't even know if you heard of it.
[00:03:16.720 --> 00:03:20.640]   It's like such a niche, you know, research area these days.
[00:03:20.640 --> 00:03:22.400]   Oh my God.
[00:03:22.400 --> 00:03:23.840]   It's all Joan's fault.
[00:03:23.840 --> 00:03:28.400]   Nobody cared about this before and she came along and suddenly, you know,
[00:03:28.400 --> 00:03:29.600]   they were a few people.
[00:03:29.600 --> 00:03:31.920]   There were a few people out there.
[00:03:31.920 --> 00:03:35.760]   And Jeff, I remember meeting you at the beginning of all of this and being like,
[00:03:35.760 --> 00:03:37.520]   oh, we're in trouble.
[00:03:37.520 --> 00:03:39.440]   You should have all run away.
[00:03:39.440 --> 00:03:40.720]   When was that?
[00:03:40.720 --> 00:03:41.520]   When was the beginning?
[00:03:41.520 --> 00:03:42.240]   You just had a curious.
[00:03:42.240 --> 00:03:48.160]   Back around, like when Google News Lab was bringing together people trying to figure out,
[00:03:48.160 --> 00:03:51.520]   you know, did this fake news thing have anything to do with journalism?
[00:03:51.520 --> 00:03:54.400]   And I was just starting up at Data and Society and Jeff,
[00:03:54.400 --> 00:03:59.200]   of course, had a longer relationship with them, Data and Society and Dana Boyd.
[00:03:59.200 --> 00:04:03.360]   And I was just getting my sea legs, but I really relied on Jeff for a lot of
[00:04:03.360 --> 00:04:09.280]   input and insight into how this, you know, whole thing came together and how the field
[00:04:09.280 --> 00:04:10.480]   is going to mature.
[00:04:10.480 --> 00:04:13.200]   Well, I think we learned from each other, maybe.
[00:04:13.200 --> 00:04:15.040]   I think, you know, you've been around.
[00:04:15.040 --> 00:04:21.680]   You get like there's history in those in your, you know, just body in your experience.
[00:04:21.680 --> 00:04:21.920]   Yeah.
[00:04:21.920 --> 00:04:24.720]   That's what they say after a while, Jeff.
[00:04:24.720 --> 00:04:27.760]   There's history embodied in that right there.
[00:04:27.760 --> 00:04:28.080]   Yeah.
[00:04:28.080 --> 00:04:33.520]   But it's like, you know, I, you know, I can't, you came on the scene as an internet researcher
[00:04:33.520 --> 00:04:37.680]   that knew a lot about social movements and knew a lot about online coordination.
[00:04:37.680 --> 00:04:41.600]   And I was always interested in pranking and digital shenanigans.
[00:04:41.600 --> 00:04:45.680]   And then I'd done research on people who hide their identities online,
[00:04:45.680 --> 00:04:47.280]   particularly white supremacists.
[00:04:47.280 --> 00:04:51.920]   And so I was like showing up at a moment when everybody was like, wait,
[00:04:51.920 --> 00:04:55.520]   the internet isn't just a force for social good.
[00:04:55.520 --> 00:04:57.280]   There are baddies on the internet.
[00:04:57.280 --> 00:04:57.680]   Yeah.
[00:04:57.680 --> 00:04:58.080]   Where?
[00:04:58.080 --> 00:05:00.720]   And, and then it just.
[00:05:00.720 --> 00:05:04.000]   Joan was doing very brave things from beginning.
[00:05:04.000 --> 00:05:04.560]   It is brave.
[00:05:04.560 --> 00:05:06.000]   It isn't in the bad group.
[00:05:06.000 --> 00:05:06.720]   So really.
[00:05:06.720 --> 00:05:07.280]   Yeah, it is.
[00:05:07.280 --> 00:05:07.680]   Very.
[00:05:07.680 --> 00:05:08.640]   Yeah, those people are.
[00:05:08.640 --> 00:05:12.880]   And bringing under understanding of what they were doing, why they were doing it.
[00:05:12.880 --> 00:05:14.160]   Nobody else really had.
[00:05:14.160 --> 00:05:16.960]   Actually, since we have you on, Joan, this is a story from last month,
[00:05:16.960 --> 00:05:22.080]   but I'm curious what you think about the linguistic analysis of Q.
[00:05:22.080 --> 00:05:25.040]   Did you see this?
[00:05:25.040 --> 00:05:31.440]   I did, you know, and there's just been, there's been a lot of analysis and, you know,
[00:05:32.000 --> 00:05:38.160]   until like, until the day that the person who was Q that had access to the keys of Achan
[00:05:38.160 --> 00:05:44.320]   stands up and says, it was me, ha, ha, you know, I think we're all going to have our doubts.
[00:05:44.320 --> 00:05:46.960]   But I think this is a pretty decent approximation.
[00:05:46.960 --> 00:05:50.480]   I also don't think it was just one or two people.
[00:05:50.480 --> 00:05:59.760]   I think that there was always a, a Q was like a, one of those, you know, names that was
[00:05:59.760 --> 00:06:01.840]   like a moniker for a collective.
[00:06:01.840 --> 00:06:02.400]   Yeah.
[00:06:02.400 --> 00:06:06.880]   And so when you do look closely at some of the posts, there's some really weirdo, like,
[00:06:06.880 --> 00:06:11.920]   you know, yoga, spiritual music stuff that's in Q post.
[00:06:11.920 --> 00:06:14.160]   And you're like, where did, where did that come from?
[00:06:14.160 --> 00:06:18.240]   And, and then there's like, you know, this very bombastic, militaristic language.
[00:06:18.240 --> 00:06:20.800]   And you're like, that just seems like a whole different vibe.
[00:06:20.800 --> 00:06:28.240]   And I think the story was, I think that, yeah, no, I, I, I think personally, like,
[00:06:28.240 --> 00:06:32.480]   this is not anything I can sustain with any research is I think it was something that was
[00:06:32.480 --> 00:06:42.480]   tested and got out of hand and then was reigned in by the Watkins on Achan who saw a, a marketing
[00:06:42.480 --> 00:06:47.840]   opportunity, a way to drive people to their website, a way to drive people to their political
[00:06:47.840 --> 00:06:49.520]   convictions. And they were a bit of a.
[00:06:49.520 --> 00:06:49.840]   Was it a.
[00:06:49.840 --> 00:06:53.760]   Was it, was it sincere or was it a hoot from the beginning?
[00:06:53.760 --> 00:07:02.320]   I think it actually came out of this, you know, on, on 4chan and whatnot, there'd been a lot of talk
[00:07:02.320 --> 00:07:05.920]   about psychological influence operations and if you could really do it.
[00:07:05.920 --> 00:07:12.800]   And I think it was people playing with some military strategy, playing with psychological
[00:07:12.800 --> 00:07:18.160]   theories about repetition and, and facts and evidence.
[00:07:18.160 --> 00:07:23.440]   And so I actually think it was plotted out probably not as strategically all the way to the end
[00:07:23.440 --> 00:07:27.600]   because they could have never known, you know, what was, what was going to be around the corner.
[00:07:27.600 --> 00:07:32.960]   But what they did strategically that I thought was always really interesting is they took
[00:07:32.960 --> 00:07:37.520]   breaking news and made it relevant to the conspiracy.
[00:07:37.520 --> 00:07:43.280]   And so there was always something new to discover and to be part of.
[00:07:43.280 --> 00:07:49.120]   And so it generated all of this participation and excitement and, you know, there are all these
[00:07:49.120 --> 00:07:55.440]   like Easter egg hunts. And so in that sense, I do think that the Watkins saw that there was.
[00:07:55.440 --> 00:08:00.480]   An advantage here that their website could really.
[00:08:00.480 --> 00:08:07.360]   Bring people together and could be used as a place to launder information out to other platforms,
[00:08:07.360 --> 00:08:13.760]   but because it was always bigger than the messages from Q itself, there are all of these, you know,
[00:08:14.880 --> 00:08:22.000]   sort of centrifugal rings of influencers around Q that it was always more than just
[00:08:22.000 --> 00:08:26.560]   one layer of people. And it's hard to tell if they were even coordinating at all,
[00:08:26.560 --> 00:08:33.520]   more so than they were just trying to keep it alive because it made money, it generated
[00:08:33.520 --> 00:08:39.440]   attention and it got political things done. Yeah. So really it's of interest, I guess,
[00:08:39.440 --> 00:08:44.800]   to think of who the original creators were just like, it's of interest to figure out who's
[00:08:44.800 --> 00:08:50.240]   Satoshi Nakamoto is, but not really of real import because both Bitcoin.
[00:08:50.240 --> 00:08:57.200]   Yeah, because you could study what it did without knowing who it was. But there is, you know,
[00:08:57.200 --> 00:09:04.640]   there's that great series on HBO about this. Well, that's what got me really interested. Yeah.
[00:09:04.640 --> 00:09:12.880]   Yeah. And that does talk to some of the major players that would really know who was behind it
[00:09:12.880 --> 00:09:18.160]   and what, you know, because eventually, you know, if you run a website that has user generated
[00:09:18.160 --> 00:09:24.240]   content and someone's demanding that much attention and causing that much chaos, you're going to
[00:09:24.240 --> 00:09:30.400]   want to know who they are. Right. Like, you know, in the Watkins, especially if this had been
[00:09:30.400 --> 00:09:39.040]   some kind of government op would have been, you know, intensely suspicious of FBI, CIA, DHS
[00:09:39.600 --> 00:09:44.800]   involvement. And so it's just like it strikes me that the people who ran the website would have
[00:09:44.800 --> 00:09:52.400]   to know a little bit more about the subversive intentions of Q, if not be, you know, the ones
[00:09:52.400 --> 00:10:00.640]   behind it themselves. The documentary is called Into the Storm and the creator of a Colin Hopkins
[00:10:00.640 --> 00:10:06.560]   Hoskins. I think pretty much concluded that was Ron Watkins at the end. And it was very
[00:10:06.560 --> 00:10:11.280]   conclusive for me as I watched the documentary. But again, that's not that's not as much
[00:10:11.280 --> 00:10:16.960]   important. Yeah, I think Ron played a significant role. But again, it's like, if you look at the
[00:10:16.960 --> 00:10:23.760]   multi level marketing around Q and on and just the like, you know, the spirituality angle a
[00:10:23.760 --> 00:10:30.880]   little bit, you know, I mean, it's it's t-shirts, it's keychains. It's your main only in one aspect,
[00:10:30.880 --> 00:10:37.200]   which is that Ron Watkins is running for Congress. So yes. So it does, I guess, in that, in that
[00:10:37.200 --> 00:10:42.320]   respect, have some, have some important I always hope it was like, it was like a washed up comic
[00:10:42.320 --> 00:10:47.440]   like Gallagher. Wouldn't that have been better? Like, carrot top shows up. Yeah, carrot top.
[00:10:47.440 --> 00:10:55.520]   That's better. Yeah, or Andy or Andy. What's Andy? Andy Kaufman. It's a perfect
[00:10:55.520 --> 00:10:59.600]   Andy Kaufman. It would be an Andy Kaufman. Yeah. Perfect Andy Kaufman. All right. But there is
[00:10:59.600 --> 00:11:04.720]   stuff that is more up to date today to talk about. So I guess we should get to the news of the week.
[00:11:04.720 --> 00:11:10.080]   Although when you have Joan Donovan on, you could talk about it. It's very tempting to go down a lot
[00:11:10.080 --> 00:11:15.680]   of different rabbit holes. That's where she lives. Yeah. In those rabbit holes. Yeah. I know you guys
[00:11:15.680 --> 00:11:20.240]   don't even want to know. Like I often tell people they're like, what are you been looking at? And I'm
[00:11:20.240 --> 00:11:26.320]   like, no. No, you don't want to know. It's like asking a Facebook moderator. Hey, had a
[00:11:26.320 --> 00:11:32.560]   good day. How was work? It was work. No, no, not the conversation that we had here.
[00:11:32.560 --> 00:11:38.080]   Actually, let's talk a little bit about disinformation in Ukraine. It was a very interesting strategy.
[00:11:38.080 --> 00:11:47.440]   The US information agencies pursued to defuse what they saw as the coming war and the coming
[00:11:47.440 --> 00:11:54.400]   disinformation from Putin, for instance, to say, no, no, no, those stories about the chemical
[00:11:54.400 --> 00:11:58.240]   weapons bunkers the US has in Ukraine are really just
[00:11:58.240 --> 00:12:05.440]   to a straw man to distract you from the fact that they're going to use chemical weapons
[00:12:05.440 --> 00:12:12.080]   to attack Ukraine. Russians are going to or pay no attention to. So the Russians who have
[00:12:13.520 --> 00:12:20.720]   historically been kind of kings of disinformation, I think, have been added again considerably.
[00:12:20.720 --> 00:12:24.800]   Yes. And do you think that was a good strategy on the part of the information agencies to?
[00:12:24.800 --> 00:12:31.440]   Yeah, I think it in the evidence points, it points right back to what happened in Syria.
[00:12:31.440 --> 00:12:36.800]   And so we know that this is part of the strategy is to say this horrible things about to happen,
[00:12:37.680 --> 00:12:45.600]   blame this other person. And that kind of attack and deflect strategy is something that we've seen
[00:12:45.600 --> 00:12:50.400]   from Russia in the past. Gary Kasparov says that Putin's strategy has always been
[00:12:50.400 --> 00:12:56.240]   sue for peace talks so he can rearm and reload and then attack with great vigor again.
[00:12:56.240 --> 00:13:05.200]   It's like we know the playbook now. Yeah. And I think that the US and NATO understand that
[00:13:05.200 --> 00:13:10.720]   there's a common enemy of foot. And that's why I think news media was making a pretty big deal
[00:13:10.720 --> 00:13:17.520]   about the end of Biden's very cinematic speech on Saturday with the castle and the dark lighting
[00:13:17.520 --> 00:13:23.840]   and everything. Very ominous. But I was like, this feels like the moon landing in a way.
[00:13:23.840 --> 00:13:28.640]   Do you think that was intentional all of that staging? Oh, yeah. Oh, yeah. Everybody loves a good
[00:13:28.640 --> 00:13:37.920]   stage. Yeah. Sorry. I'm not hearing you sometimes. Have you seen a up ticker down tick
[00:13:37.920 --> 00:13:45.920]   in Russian disinformation? Are they a little busy now paying attention to or in terms of
[00:13:45.920 --> 00:13:52.080]   America or has an impact on the flow? It's interesting. Yeah, the kind of stuff that we see
[00:13:52.080 --> 00:13:56.080]   like that used to be pumped through Russian state media, of course,
[00:13:56.880 --> 00:14:01.440]   you know, the platform companies have shut all of that down. And so there's a lot less
[00:14:01.440 --> 00:14:08.560]   RT. But what we're seeing then is clips of those shows show up on TikTok and in Telegram much more
[00:14:08.560 --> 00:14:15.440]   frequently. And so the media out of Russia is coming back. But the Russian focus on the US and
[00:14:15.440 --> 00:14:20.640]   I thought it was really interesting love to know, Jeff, your opinion on, you know, when Putin started
[00:14:20.640 --> 00:14:26.960]   to mention JK Rowling and cancel culture. It reminded me a lot of this kind of dog whistling
[00:14:26.960 --> 00:14:31.120]   where you're like, well, the US media is going to have to cover this because we mentioned, you
[00:14:31.120 --> 00:14:38.960]   know, Harry Potter, right? And so that's why. Oh, that's interesting. A way to snake back into
[00:14:38.960 --> 00:14:45.360]   the American, you know, it is interesting. Good theory. I like that. Yeah. They must have
[00:14:45.360 --> 00:14:53.680]   a fairly big operation. You know, I noted, for instance, we had a story yesterday that Verizon
[00:14:53.680 --> 00:15:00.880]   customers are getting spam text messages from their own number. They said, click a link to receive
[00:15:00.880 --> 00:15:07.200]   a gift. And in many cases, the link led to a Russian news site. And I thought, wow, that's
[00:15:07.200 --> 00:15:13.520]   interesting. Is that their strategy? I mean, yes, spoofing, you know, to try to get people to
[00:15:14.240 --> 00:15:20.160]   see things from their perspective. And I think that, you know, the use of spam is something
[00:15:20.160 --> 00:15:25.120]   that's also been interesting on the activist side. I have a colleague, Julia Minson at the
[00:15:25.120 --> 00:15:30.720]   university that's been working with some hackers on building repositories of messages that can
[00:15:30.720 --> 00:15:37.440]   be spent spammed to Russian emails so that they can get information one on how to install things
[00:15:37.440 --> 00:15:44.800]   like Tor and Circumvents, Russian suppression of the internet, and then also to messages about
[00:15:44.800 --> 00:15:51.360]   from independent media that make people think twice about the media that they're ingesting. But
[00:15:51.360 --> 00:15:56.080]   who knows? Like, are they opening the spam? We don't know. But it does seem to me that there's
[00:15:56.080 --> 00:16:03.440]   going to be some like low grade information warfare tactic playing out as people experiment with
[00:16:04.000 --> 00:16:07.440]   different gateways into other people's networks.
[00:16:07.440 --> 00:16:12.880]   Yeah. You do it all, right? And it's cheap and easy to do thanks to technology. So as long as
[00:16:12.880 --> 00:16:16.960]   you have the manpower, which Russia does to do it, you do it.
[00:16:16.960 --> 00:16:25.040]   I put in the story about the the bot farms that were shut down in Ukraine. Yeah. And the great
[00:16:25.040 --> 00:16:32.240]   thing about it is pictures of the racks used to create all the different identities to go to
[00:16:32.240 --> 00:16:38.160]   spin off and other things. 10,000 SIM cards, 100 sets of GSM gateways.
[00:16:38.160 --> 00:16:46.480]   All this was just a spam us. Yeah. Well, and in fact, yeah, that's used in Florida as well.
[00:16:46.480 --> 00:16:53.360]   It's all over the world. These spam farms in Asia too. Yeah, here's some of the pictures. These are
[00:16:53.360 --> 00:17:00.880]   almost 10,000 SIMs of various operators. The idea being it looks like these are genuine users.
[00:17:00.880 --> 00:17:07.600]   It's been used for click fraud in advertising. I guess this was for disinformation. I don't know
[00:17:07.600 --> 00:17:13.920]   what they were. I would assume so. Yeah. The the bot farms operated in Kharkiv,
[00:17:13.920 --> 00:17:20.720]   Charkazi, Tarnopo, Potava, Zakar, Piti, Patiya, regions, fake accounts. Ah, they would use these
[00:17:20.720 --> 00:17:26.480]   to create fake accounts on social media. Right. Yeah, that makes sense. Yeah. And then post fake
[00:17:26.480 --> 00:17:31.680]   news about the Russian war. I think I've known a few of them. I wonder, Joan, if this I mean this,
[00:17:31.680 --> 00:17:38.400]   is this a how new is this phenomenon is social media really allowing? This has kind of been
[00:17:38.400 --> 00:17:42.320]   something that poor Jeff has to hear me say all the time, allowing this to be weaponized.
[00:17:42.320 --> 00:17:51.200]   Yeah, we've been here. You know, if you look at the sort of now canonical case of the Russian
[00:17:51.200 --> 00:17:57.760]   Internet Research Agency, when they were creating fake accounts, it was, you know, they were some of
[00:17:57.760 --> 00:18:03.280]   them were originally lifestyle accounts, things that were, you know, on Twitter that were very spammy
[00:18:03.280 --> 00:18:11.600]   until they were turned into, you know, mostly anti-dem or anti-Hillary accounts. They weren't
[00:18:11.600 --> 00:18:18.960]   exactly pro-Trump. And so yeah, like we've seen this happen time and time again. We were also
[00:18:18.960 --> 00:18:27.200]   looking at a bunch of Facebook pages that were being operated and were selling identities to support
[00:18:27.200 --> 00:18:36.240]   the convoy protests that were happening in Ottawa. And then, you know, started to roll out, you know,
[00:18:36.240 --> 00:18:41.840]   in the US and there was a, you know, nobody really paid any attention to it, but there were a couple
[00:18:41.840 --> 00:18:48.800]   of convoy protests in D.C. over the past few weeks. And but yeah, it's just, you know, part of the
[00:18:48.800 --> 00:18:56.480]   nature of the design of these platforms themselves is this, you know, engagement farming and building
[00:18:56.480 --> 00:19:03.200]   out these fake profiles. Because the goal here is to have an impact on the algorithm, not just
[00:19:03.200 --> 00:19:09.280]   an impact on public conversation, but you want to be able to put information out there and then
[00:19:09.280 --> 00:19:15.520]   have it be discoverable. And so it's like, if a single person tweets and nobody retweets it,
[00:19:15.520 --> 00:19:22.320]   you know, did anybody really see it, you know, and so having a bot network or, you know, fake
[00:19:22.320 --> 00:19:28.080]   influencers or fake accounts that follow you, that job is to reshare that information,
[00:19:28.080 --> 00:19:36.000]   it's just part of the industry. It's part of social media is like, it's just what it is is,
[00:19:36.560 --> 00:19:42.480]   is, you know, it's a communications medium. And of course, whatever the communications medium,
[00:19:42.480 --> 00:19:46.880]   you're going to use it. And if it's, you know, more friction-free, you're going to use it more,
[00:19:46.880 --> 00:19:53.200]   it's easier to take advantage of. Not all this information comes from Russia, surprisingly.
[00:19:53.200 --> 00:20:03.440]   Facebook, for instance, paid a firm called targeted victory to push the message that TikTok
[00:20:03.440 --> 00:20:12.080]   was bad for kids. How stupid. Employees from the firm targeted victory works to undermine
[00:20:12.080 --> 00:20:18.720]   TikTok through a nationwide media and lobbying campaign portraying TikTok, which was an existential
[00:20:18.720 --> 00:20:24.000]   threat to a Facebook, I guess, as a danger to American children and society. This is
[00:20:24.000 --> 00:20:29.840]   according to the Washington Post, which saw internal emails to this effect. Here's one quote,
[00:20:30.640 --> 00:20:36.960]   quote, get the message out that that while meta is the current punching bag, TikTok is the real
[00:20:36.960 --> 00:20:43.840]   threat, especially as a foreign owned app that is number one in sharing data that young teens
[00:20:43.840 --> 00:20:50.400]   are using end quote, bonus point. Here's the best one. Bonus points. If we can fit this into a
[00:20:50.400 --> 00:20:55.360]   broader message that the current bills aren't where members of Congress should be focused.
[00:20:57.120 --> 00:21:04.400]   Pay no attention to that meta problem. Look at TikTok. Well, you know, I, this had echoes of
[00:21:04.400 --> 00:21:12.400]   a campaign that Facebook had also run in 2018, where they used a similar firm called Definers that
[00:21:12.400 --> 00:21:18.880]   had attacked Apple and Google, and they had come up with some anti-Semitic messaging that
[00:21:18.880 --> 00:21:25.360]   also attacked George Soros. And so Facebook, in response to that New York Times story,
[00:21:25.360 --> 00:21:32.800]   that was it was a really big deal when it came out. And it was a share of Frankl Cecilia King
[00:21:32.800 --> 00:21:40.720]   piece. And so this is nothing new that Facebook has done here. This is just a different firm doing
[00:21:40.720 --> 00:21:46.560]   the same media. Do you think all companies do this? This is like, they usually hire,
[00:21:46.560 --> 00:21:52.320]   what they create a fake organization and fund that and let that organization, right? Microsoft
[00:21:52.320 --> 00:21:57.920]   has been doing this for years going after Google and company. And so it's usually,
[00:21:57.920 --> 00:22:03.040]   they're smart enough to create a separation. Actually, Microsoft, for responsible
[00:22:03.040 --> 00:22:09.040]   adolescents, Microsoft and given money. Did sponsor ads and videos about Gmail man
[00:22:09.040 --> 00:22:14.720]   reading mail? So they didn't try too hard to obfuscate the source of that information.
[00:22:14.720 --> 00:22:20.480]   Yeah. Targeted victory does not deny it. They say, quote, they didn't say what they did, but we
[00:22:20.480 --> 00:22:24.400]   are proud of the work we've done. You could hire us with with men.
[00:22:24.400 --> 00:22:27.520]   With your enemies too. And meta spokesperson Andy Stone,
[00:22:27.520 --> 00:22:31.760]   defended the campaign saying, quote, we believe all platforms, including TikTok,
[00:22:31.760 --> 00:22:36.640]   should face a level scrutiny of a level of scrutiny consistent with their growing success.
[00:22:36.640 --> 00:22:43.360]   TikTok says it's deeply concerned. Well, it's hard, you know, you're out here as,
[00:22:43.360 --> 00:22:47.840]   you know, if you're out here as an academic, you're like, wait, you know, we're trying to work
[00:22:47.840 --> 00:22:54.400]   with these companies to battle this information. And here they are paying PR firms to blast each
[00:22:54.400 --> 00:23:00.720]   other. And it's like, how do we get a handle on like authentic debate and discourse, especially
[00:23:00.720 --> 00:23:05.200]   if you look in that article, Taylor Lorenz and Drew Harwell do a really great job because
[00:23:05.200 --> 00:23:10.880]   there were a couple viral campaigns. We were sitting on a case study about the slap your teacher
[00:23:10.880 --> 00:23:17.200]   phenomenon. And we were just like, we just can't figure out how this thing became so popular
[00:23:17.200 --> 00:23:22.800]   when it's not actually that viral. And there's a few things on TikTok and there's a few things on
[00:23:22.800 --> 00:23:28.640]   Facebook. And this really connected the dots for us is that there was a paid media campaign
[00:23:28.640 --> 00:23:32.640]   to get out that, you know, TikTok was hosting these viral challenges.
[00:23:32.640 --> 00:23:38.720]   Oh, yeah. And so really, we were just like sitting here reading it this morning. My whole
[00:23:38.720 --> 00:23:45.200]   team was on on signal just like, oh, man, we can finish the case study now because we know how
[00:23:45.200 --> 00:23:52.000]   it became popular in the media. So how much of moral panic is seated in this way?
[00:23:52.000 --> 00:23:59.520]   Yeah. And so we had to just be really careful when we start to read these stories. And it employs
[00:23:59.520 --> 00:24:04.960]   a classic media manipulation technique called trading up the chain. So it's well known. There's
[00:24:04.960 --> 00:24:09.360]   a really great book by Ryan Holiday called Trust Me I'm Lying where he's the chapter on
[00:24:09.360 --> 00:24:14.800]   describing. Yeah, read this chapter again because it'll just, but it's you plan a story in local
[00:24:14.800 --> 00:24:20.960]   news or blog that doesn't have a lot of over its head, like oversight in terms of editorial and
[00:24:20.960 --> 00:24:26.320]   you get that story and that establishes the facts. Here's a perfect thing can be built on.
[00:24:26.320 --> 00:24:30.640]   Here's a perfect example that happened in Northern California a couple of months ago.
[00:24:30.640 --> 00:24:33.200]   This is from Oakland's channel two news.
[00:24:33.200 --> 00:24:39.040]   Sweeping TikTok has law enforcement watching. It's the door kicking challenge in which young
[00:24:39.040 --> 00:24:45.440]   people kick someone's door to beat out of to the beat of a song then run as KTVU's Deborah V
[00:24:45.440 --> 00:24:50.720]   alone reports causing damage and safety conditions. This sparked a series of
[00:24:50.720 --> 00:24:59.200]   this part is series on TikTok of prank that began a college news paper articles in Petaluma.
[00:24:59.200 --> 00:25:05.200]   Doors are being damaged. Plus we're worried when this happens somebody's going to get shot.
[00:25:06.320 --> 00:25:12.560]   If you think you're being broken in on here's the Petaluma police community engagement officer.
[00:25:12.560 --> 00:25:21.760]   Oh my god. This is this as far as I know was localized to our small town. I asked our 19 year
[00:25:21.760 --> 00:25:27.440]   old and I asked my son who's a fairly well-known TikToker. Are you aware of this challenge? No.
[00:25:27.440 --> 00:25:30.960]   Leo, along this one.
[00:25:32.960 --> 00:25:46.320]   Line 109. Line 109. Kick the door when you hear it. 16% of British toddlers are on TikTok.
[00:25:46.320 --> 00:25:58.000]   I mean like as like their parents are. No, they serve. Did they survey them? It may be affecting
[00:25:58.000 --> 00:26:06.240]   their attention span. Maybe. Like I said, mommy, mommy. Please have the TikTok please.
[00:26:06.240 --> 00:26:11.120]   British toddlers are increasingly likely to be users of TikTok with a substantial number of
[00:26:11.120 --> 00:26:17.200]   parents saying their preschool children use the video service despite the app supposedly
[00:26:17.200 --> 00:26:22.320]   being restricted to those ages 13 and older. Excuse me. Excuse me. A little bit of a little
[00:26:22.320 --> 00:26:25.520]   asylum here. I know you're only two years old, but did you fill out that age thing?
[00:26:25.520 --> 00:26:34.000]   Did you go down to the pub Sally? Get your age thing. I don't even know how you get statistics on that.
[00:26:34.000 --> 00:26:38.160]   This is from OS.com. That's the funniest thing. This is the British.
[00:26:38.160 --> 00:26:43.680]   Offcome. The British enforcement media radio. They're the ones who are going to be enforcing
[00:26:43.680 --> 00:26:48.480]   the new regulations against the internet. Offcome. These are the folks who are going to protect us
[00:26:48.480 --> 00:26:54.640]   from ourselves. This rises to a third of all children in the five to seven year old age group.
[00:26:54.640 --> 00:27:01.120]   Oh my god. I wonder if they just asked like parents, do you let your children use TikTok on
[00:27:01.120 --> 00:27:05.120]   your phone and they're just like, yeah, show them funny cat videos or something. Honestly,
[00:27:05.120 --> 00:27:10.880]   the whole school I get my dinner down. The 13 year old age limit is about collecting information
[00:27:10.880 --> 00:27:15.840]   on kids. It means you can't make an account on TikTok, which means you can't post on TikTok.
[00:27:15.840 --> 00:27:21.680]   But I don't think in any respect, it limits you from using viewing video on YouTube, TikTok,
[00:27:21.680 --> 00:27:24.080]   or anyhow. How does a five year old get access to this?
[00:27:24.080 --> 00:27:28.640]   Mommy. Mommy. Can I use the phone, mommy?
[00:27:28.640 --> 00:27:35.600]   Especially three year olds, it's mom's mom's going, shut up and watch TikTok.
[00:27:35.600 --> 00:27:41.200]   Yeah. That's how I sell soothed it. I'm like, oh, I'm just going to kick my axis.
[00:27:41.200 --> 00:27:44.720]   Why shouldn't three year olds be able to do them scroll under the rest of us?
[00:27:44.720 --> 00:27:47.760]   Yeah. Totally. That's hysterical.
[00:27:47.760 --> 00:27:53.840]   That's weird. I mean, if we're all out here trying to get headlines, it's one thing, but
[00:27:53.840 --> 00:28:00.480]   you really want to tie that to some really linear analysis of facts.
[00:28:00.480 --> 00:28:09.280]   I think there is a bit of a moral technopanic going on in the world these days around
[00:28:10.640 --> 00:28:18.480]   access to technology, meaning exactly that this child or this person has been corrupted in some
[00:28:18.480 --> 00:28:24.320]   way. I don't know. I hate to hold myself out as an example of anything, but I consume more
[00:28:24.320 --> 00:28:28.240]   horrible media than most people on the planet.
[00:28:28.240 --> 00:28:31.600]   I think I'm okay.
[00:28:31.600 --> 00:28:37.040]   You mean you're watching? You're not your binge bingeing Bridgerton. Is that what you're talking
[00:28:37.040 --> 00:28:41.440]   about? Yeah. But I guess I just don't know. Maybe I don't believe in anything. So I'm like,
[00:28:41.440 --> 00:28:42.960]   yeah, flat earth, let's watch it.
[00:28:42.960 --> 00:28:46.400]   So where are you in the third person effect, Joan? And the first person?
[00:28:46.400 --> 00:28:53.120]   What's that? Is that core? The third person effect says that everybody else is influenced by
[00:28:53.120 --> 00:28:57.760]   this horrible stuff, but not me. And the first person effect says that I'm better than everybody
[00:28:57.760 --> 00:29:02.480]   else. Right? Not so much of what goes on here is that there's a big assumption that everybody
[00:29:02.480 --> 00:29:07.840]   else is connected. Yeah, I can handle it. But I really worry about the mush minds in my community.
[00:29:07.840 --> 00:29:13.840]   There are things that I am totally doopable on, and I need to know what those things are.
[00:29:13.840 --> 00:29:22.080]   I want to believe probably the worst about Bitcoin. And so I'm like doopable about it.
[00:29:22.080 --> 00:29:28.960]   People, I was just talking to a colleague the other day. And I had shared that New York City
[00:29:28.960 --> 00:29:35.600]   scammer sells Chuck E Cheese tokens in Times Square as Bitcoin makes a million dollars.
[00:29:35.600 --> 00:29:44.000]   I was like, that's funny. I would believe Bitcoin was real. And so you just kind of know
[00:29:44.000 --> 00:29:50.880]   what you might be susceptible to where your biases are and then show up with sort of a relentless
[00:29:50.880 --> 00:29:58.720]   skepticism towards everything. But I have criteria by which makes it so that I believe certain things
[00:29:58.720 --> 00:30:03.520]   more than others, especially when it comes to long form investigative journalism.
[00:30:03.520 --> 00:30:14.880]   But it's crazy to think that there's a great book if I divulge my book choice early.
[00:30:14.880 --> 00:30:19.920]   But there is this great book from Kelly Wheel about flat earthers that has recently come out.
[00:30:19.920 --> 00:30:24.960]   Oh, yeah, I've ordered that. Yeah. And one of the things that goes to this talk about
[00:30:24.960 --> 00:30:31.120]   does TikTok corrupt children is there's this line from one of her respondents in there.
[00:30:31.120 --> 00:30:36.560]   He says, I didn't find flat earth. It found me. And what he's talking about is he was watching a
[00:30:36.560 --> 00:30:42.080]   bunch of conspiracy videos. But then flat earth conspiracy videos came up in his recommendations
[00:30:42.080 --> 00:30:49.120]   and he got into it. And so I think part of that underlying fear about what the algorithms are
[00:30:49.120 --> 00:30:55.440]   doing to people is animating people's fears about what's under the hood at TikTok.
[00:30:55.440 --> 00:31:02.080]   And are they Chinese propagandists that are trying to make us feel better about US-China relations?
[00:31:02.080 --> 00:31:10.560]   Is that part of TikTok's adoption in the US? And so it's important for us to be skeptical of
[00:31:10.560 --> 00:31:17.120]   things. But also we can't just get into this belief that these companies can do all this
[00:31:17.120 --> 00:31:22.000]   stuff that is just not actually possible. And how much people are affected?
[00:31:22.000 --> 00:31:24.480]   Joan, can you explain this to me? If anybody can, you can.
[00:31:24.480 --> 00:31:33.600]   Flat earth is clearly just wrong and dumb. And all you do is take a ride in Jeff Bezos' penis ship
[00:31:33.600 --> 00:31:41.120]   and you'll know. Right. Right. I'm dead. I'm dead. That's right. That one tells me.
[00:31:41.120 --> 00:31:47.280]   Just take a ride, man. And you'll see. It's curved. It's curved. It is.
[00:31:47.280 --> 00:31:52.400]   I'm all stubborn. And so I feel like flat earthers might be the our birds real crowd.
[00:31:52.400 --> 00:31:57.760]   But is it performative? Is it just a permanent piss off people? And that's why I'm doing that.
[00:31:57.760 --> 00:32:06.800]   It's not as performative as our birds are real, which is definitely got a bit of showmanship
[00:32:06.800 --> 00:32:14.160]   and a bit of, you know, a bit of pranksterism behind it. Whereas the flat earthers, like,
[00:32:14.160 --> 00:32:20.960]   they do believe it. They do. They have like a lot of work into it. You can fly or sail around the
[00:32:20.960 --> 00:32:26.720]   world and never fall off. Yeah, but you can't fall off because it's really, really far to the ends
[00:32:26.720 --> 00:32:33.360]   where the ice wall exists that keeps all of the thing on the disc inside the disc. It's like,
[00:32:33.360 --> 00:32:39.120]   it's like to get to the crust of the pizza. You just, it's really, really far. And so you
[00:32:39.120 --> 00:32:45.520]   sail for a really long time. No, you know, that makes no sense. I'm not explaining my position.
[00:32:45.520 --> 00:32:50.080]   I'm just explaining the position. I think John would do a river of invention in a minute.
[00:32:50.080 --> 00:32:56.160]   So let me, so it's crazy. So, but we should ask, John, really, the nut of the whole thing,
[00:32:56.160 --> 00:32:59.520]   this whole conversation that we've debate we've been having for years,
[00:33:00.640 --> 00:33:08.000]   is it the platform's responsibility to somehow curb this? Is there anything they can do?
[00:33:08.000 --> 00:33:13.760]   Should they do something about it? Yeah, I mean, it's their products. This is how they're going to
[00:33:13.760 --> 00:33:21.680]   make money and build their product and build their design. And, you know, eventually, you know,
[00:33:21.680 --> 00:33:26.320]   different products are going to come along that do better curation, that give people more of what
[00:33:26.320 --> 00:33:32.800]   they want. And we're going to move away from these platforms that are everything to everyone and
[00:33:32.800 --> 00:33:40.320]   more into platforms, like, for instance, Twitch that serves gamer communities. It knows what the
[00:33:40.320 --> 00:33:47.280]   feelings and the ethics of the fans are. And it works really hard to remove problematic content
[00:33:47.280 --> 00:33:57.520]   from its platform. And I think the model of Facebook and Twitter and YouTube has always been,
[00:33:57.520 --> 00:34:03.280]   well, we'll take anything and we're structures here and we build infrastructure.
[00:34:03.280 --> 00:34:10.480]   But that just doesn't hold anymore. It just doesn't make sense. You're continuously negotiating
[00:34:10.480 --> 00:34:16.880]   in on the national media stage, like, if you should keep all of these horrible people on your
[00:34:17.440 --> 00:34:23.280]   services, and it's just a drain on resources. And so it's better to try to fix some of these
[00:34:23.280 --> 00:34:29.760]   problems upfront. And, you know, Twitter is doing, you know, different than Facebook is in terms of
[00:34:29.760 --> 00:34:35.600]   trying to clean up these messes that are left behind by, frankly, like, the biggest messes are
[00:34:35.600 --> 00:34:42.640]   left behind by the biggest influencers. And so we are going to revisit the, you know, presidential
[00:34:42.640 --> 00:34:49.680]   question around Trump returning to social media. And these companies are going to have to decide
[00:34:49.680 --> 00:34:56.160]   which communities they're going to serve. And if they're going to play on this national stage,
[00:34:56.160 --> 00:35:02.480]   and if they do, they're, you know, there's a really good opportunity for more innovation
[00:35:02.480 --> 00:35:07.520]   that does more niche platforming work. And you can kind of see it with the
[00:35:08.240 --> 00:35:15.520]   creep of young people away from Facebook products and towards, you know, things that are more
[00:35:15.520 --> 00:35:22.080]   things that are less social networking and a little bit more ephemeral.
[00:35:22.080 --> 00:35:25.680]   Is TikTok one of those things? Definitely. Yeah.
[00:35:25.680 --> 00:35:28.880]   It's hard to find things on TikTok.
[00:35:28.880 --> 00:35:33.680]   You can't I can't I can't find my sons to the things on TikTok. It's crazy. And I follow
[00:35:33.680 --> 00:35:41.440]   them and I can't find I have to search for Leo. I agree with Joan, as always, that they have to
[00:35:41.440 --> 00:35:46.640]   make choices and they are allergic to choices. Choices are expensive. And I think that we will
[00:35:46.640 --> 00:35:51.920]   see more things serving communities all that good. But what if they're not allowed to make the choices?
[00:35:51.920 --> 00:35:59.360]   I had an amazing encounter this week on Twitter with with Brendan Carr, an FCC commissioner,
[00:36:00.160 --> 00:36:05.600]   who was going crazy on it was going after me and others and I was having back channel
[00:36:05.600 --> 00:36:10.080]   chats. I didn't see this. But let me Adam, Jeff. I'm like, it's on. It's on.
[00:36:10.080 --> 00:36:14.960]   It comes after you. Mine 67. Yeah. I'm Joe. Jeff. Protect me.
[00:36:14.960 --> 00:36:19.760]   I'm coming in. I might be like three days late, though. I don't know what that means about me, but
[00:36:19.760 --> 00:36:26.320]   so we wonder why he was doing this. And then he had his testimony before the
[00:36:27.600 --> 00:36:33.680]   Commerce Committee in the house in which he at the end of this whole long boring thing
[00:36:33.680 --> 00:36:40.240]   with with thickly veiled references to basically saying big tech has to allow our speech.
[00:36:40.240 --> 00:36:45.360]   And so they're not going to be allowed to take things down if certain people get in power.
[00:36:45.360 --> 00:36:49.920]   And so all the all the yelling we do about you better be responsible platforms. You better
[00:36:49.920 --> 00:36:53.600]   take this stuff down. You better care about your product. You better care about society.
[00:36:53.600 --> 00:36:58.800]   It could well turn around on a dime and. Oh, I can't do this. Sorry.
[00:36:58.800 --> 00:37:07.440]   Yeah, we're not there, but we could be. But if it happens like that, you know, who the public
[00:37:07.440 --> 00:37:14.720]   attention and the conversation then turns to is to Congress to do something. Oh, oh, like,
[00:37:14.720 --> 00:37:19.360]   yeah, exactly. I haven't really done anything. But there's going to be much more accountability
[00:37:19.360 --> 00:37:27.040]   on political accounts, you know, in the public ire and the journalism that beat is is going to
[00:37:27.040 --> 00:37:32.800]   is going to come back to bite them. And so, you know, and I'm, you know, Daniel Citron's,
[00:37:32.800 --> 00:37:38.640]   you know, someone who's thought very deeply about she's a professor of law. She's thought very
[00:37:38.640 --> 00:37:46.640]   deeply about the codes and rules of internet speech. And, you know, I come down with her is that
[00:37:46.640 --> 00:37:53.520]   the companies have a duty of care to do content moderation in a way that is
[00:37:53.520 --> 00:38:04.560]   responsible ethical, you know, has public interest at its core. And, you know, we're in this moment
[00:38:04.560 --> 00:38:14.240]   where you have these very, very different political views on what the internet is good for and how
[00:38:14.240 --> 00:38:17.360]   it should be governed. And this point of view, unfortunately,
[00:38:17.360 --> 00:38:25.680]   I don't know, like, you might think wrong, think, think I'm wrong on this. But I kind of feel like
[00:38:25.680 --> 00:38:31.920]   Facebook changing its name to meta signals, a really big transition where Facebook is probably
[00:38:31.920 --> 00:38:38.880]   going to go away in the form that we know it currently, slowly over time and be replaced with
[00:38:38.880 --> 00:38:44.160]   something else. I have a feeling that Twitter is actually going to turn into a kind of banking
[00:38:44.160 --> 00:38:50.480]   system with, you know, like with messages attached. But it's, you know, like the things that these
[00:38:50.480 --> 00:38:59.280]   people become interested in, the same rails that move information also move money. And so, I'm not,
[00:38:59.280 --> 00:39:05.200]   I don't feel like we've really seen our final form in terms of social media here. And so,
[00:39:06.160 --> 00:39:12.720]   legislation might accelerate legislation like that, that would say, you know, hands off,
[00:39:12.720 --> 00:39:19.440]   no moderation allowed would probably accelerate the death of social media pretty quickly.
[00:39:19.440 --> 00:39:23.200]   You know how low it will go. You've seen it, Jim.
[00:39:23.200 --> 00:39:30.400]   Elon Musk actually is, I think in hand in hand with Brennan Carr, when he suggested that Twitter,
[00:39:31.600 --> 00:39:36.400]   maybe he should create his own Twitter, or the Twitter, he says serves as the defect of public
[00:39:36.400 --> 00:39:42.800]   town square. So it should support true free speech. And maybe we should do something about that.
[00:39:42.800 --> 00:39:48.640]   Carr, I know it's funny because if you're on the left, you might have might agree with Carr when
[00:39:48.640 --> 00:39:52.880]   it comes to things like corporate media, you might say, well, yeah, there's too much corporate
[00:39:52.880 --> 00:39:55.680]   dominance of media. So we don't get a really,
[00:39:56.560 --> 00:40:03.120]   no, when here is what Carr is arguing, and I find it extremely offensive and white supremacist
[00:40:03.120 --> 00:40:10.080]   racist in the end, is that anti discrimination laws should be applied to right with speech.
[00:40:10.080 --> 00:40:15.360]   And so I just simply said, well, no, it's a private, you know, you're used to be a Republican.
[00:40:15.360 --> 00:40:20.480]   It's a private company. And like a bar can refuse you service for any reason.
[00:40:21.920 --> 00:40:28.080]   And he hated that. And he's trying to argue that this is a matter of civil rights akin to laws
[00:40:28.080 --> 00:40:35.520]   around racism, to that that he says it to the house, that anti discrimination law should be
[00:40:35.520 --> 00:40:41.040]   should be applied to their speech. That's just offensive, stupid. And it's against the First
[00:40:41.040 --> 00:40:46.720]   Amendment. But hey, look at our Supreme Court, look where the Congress we could get soon.
[00:40:48.000 --> 00:40:53.600]   We could be there. Yeah. Let's take a little break. Joan Donovan is here from the Shorrentine
[00:40:53.600 --> 00:41:00.000]   Shorenston School, the Center on Media, Politics and Public Policy at Harvard University. Always a
[00:41:00.000 --> 00:41:07.760]   welcome visitor to these shores. Jeff Jarvis also Stacy and Ant have the week off our show
[00:41:07.760 --> 00:41:12.800]   today brought to you by now I've talked about mattresses before, but I want to talk to you about
[00:41:12.800 --> 00:41:21.840]   something that's on top of my mattress. It's the eight sleep pod pro cover. And it is very cool.
[00:41:21.840 --> 00:41:28.880]   Also very hot. It can be either. For you know, Lisa and I have toyed with electric blankets,
[00:41:28.880 --> 00:41:34.000]   electric mattress pads, various ways of, you know, warming up the bed at night and so forth.
[00:41:34.000 --> 00:41:37.120]   I used to have one of those things they have in colonial homes. You put coals in it and you
[00:41:37.120 --> 00:41:43.520]   rub it between the blankets. But that's not what this is. This is so much better. This is a high
[00:41:43.520 --> 00:41:52.480]   tech answer to good sleep. It turns out evolutionarily, you might go to bed, the fire you just put
[00:41:52.480 --> 00:42:00.160]   out the fire, the house is still warm over the night, the room cools down. Your body is tuned to
[00:42:00.160 --> 00:42:04.880]   that. It cools down. And then in the morning, the sun comes out, it warms up and you get up. And
[00:42:04.880 --> 00:42:10.480]   this is kind of the natural sleep cycle, a cycle that we frankly don't get anymore because we have
[00:42:10.480 --> 00:42:17.040]   climate controlled homes and lots of blankets. Eight sleep is bringing that sleep cycle back. Now,
[00:42:17.040 --> 00:42:21.840]   I'm not going to dictate to you how you use your eight sleep. But I'll tell you how I have it. And
[00:42:21.840 --> 00:42:27.600]   by the way, eight sleep, which is cool. It's a cover that goes over your mattress. It's computer
[00:42:27.600 --> 00:42:33.120]   controlled, internet controlled. You have a app that you can choose temperatures and so forth. But
[00:42:33.120 --> 00:42:37.520]   it also observes your sleeping, how you're restless, how you're not when you get up, when you go to bed.
[00:42:37.520 --> 00:42:42.800]   And it then programs the bed. So when I get in bed, it's nice toasty, cozy and warm. It slowly
[00:42:42.800 --> 00:42:49.520]   cools off as the night goes by and then warms up again to wake me up around eight in the morning.
[00:42:49.520 --> 00:42:54.000]   And I have never slept better. Kevin Rose was the first to tell me about this. Amy Webb and
[00:42:54.000 --> 00:42:58.480]   Kevin both have the eight sleep and we're saying, you got to get this the best thing that ever happened.
[00:42:59.200 --> 00:43:04.800]   30% of Americans struggle with sleep. And one of the main reasons is temperature regulation.
[00:43:04.800 --> 00:43:10.880]   Night sweats we sleep too hot or we're freezing cold. This is the solution. Eight sleeps pond pro
[00:43:10.880 --> 00:43:18.160]   cover. It pairs dynamic cooling and heating. It does both with biometric tracking. In fact,
[00:43:18.160 --> 00:43:22.000]   you can even get your sleep score and all sorts of information about how you slept from it.
[00:43:22.000 --> 00:43:28.160]   You add the cover to any mattress. Temperature range is huge. 55 degrees to 110 degrees.
[00:43:28.800 --> 00:43:34.800]   You can control it. Lisa and by the way, we have a double bed. Lisa has her side. I have my side.
[00:43:34.800 --> 00:43:39.440]   She likes to have it really warm all night long. I like that cycle. You can control that.
[00:43:39.440 --> 00:43:44.800]   Each side of the bed is adjusted based on your sleep stages, your biometrics. It actually even
[00:43:44.800 --> 00:43:51.040]   tests the temperature of the bedroom and reacts intelligently to create the optimal sleeping environment.
[00:43:51.040 --> 00:43:56.240]   I know this sounds a little bit like hocus pocus, but believe me, when you do it, you will never go back.
[00:43:56.800 --> 00:44:02.640]   I'm totally hooked on this. Eight sleep users on average fall asleep up to 32 percent faster,
[00:44:02.640 --> 00:44:10.480]   reduce sleep interruptions by 40 percent. Overall, get a more restful night's sleep. This is the ultimate
[00:44:10.480 --> 00:44:16.160]   in sleep. I have to tell you, they have mattresses as well. If you want to get a mattress with the
[00:44:16.160 --> 00:44:21.680]   pod built in, but I have the pod pro cover. That's the most affordable easiest way to do this.
[00:44:22.800 --> 00:44:27.280]   And I'll tell you, I'll tell you, it makes a huge difference. We've been doing it now, I think for
[00:44:27.280 --> 00:44:32.160]   about six months, ever since Kevin told us about it. And I've become a big fan. Eight sleep,
[00:44:32.160 --> 00:44:39.520]   eight, spell it out, eightsleep.com/twig. Check out the pod pro cover. Read up about it.
[00:44:39.520 --> 00:44:44.240]   I think you got to try it. You can save $150 a check out, by the way. Eight sleep currently
[00:44:44.240 --> 00:44:53.040]   ships within the US, Canada, and the UK get that $150 credit if you go to eightsleep.com/twig.
[00:44:53.040 --> 00:45:01.680]   Unbelievable. Unbelievable. I don't know how to explain it better. I wish I could. It's
[00:45:01.680 --> 00:45:11.600]   fantastic. And it's cozy. I really like it. Eight sleep and cool as well. Eightsleep.com/twig.
[00:45:11.600 --> 00:45:15.360]   Thank you. Eight sleep for supporting this week in Google.
[00:45:15.360 --> 00:45:21.920]   Just a tip to you, customers out there who buy it using the twig thing. Make sure that
[00:45:21.920 --> 00:45:26.400]   unlikely you read the instructions first. Oh, yes. This is the thing. We were talking about
[00:45:26.400 --> 00:45:32.080]   this when I got it. I put it on wrong because I thought you have the cover and then you have
[00:45:32.080 --> 00:45:36.400]   there's a bottom part. You put that underneath and you'd talk. And no, I should have watched it.
[00:45:36.400 --> 00:45:41.120]   There's a very good video explains it. It's very straightforward. I wasn't paying attention.
[00:45:41.120 --> 00:45:47.040]   It actually uses water in the pad. So it's very quick when it heats up, it heats up fast,
[00:45:47.040 --> 00:45:52.000]   when it cools off, it cools off, because water is very conductive. So really, you know immediately,
[00:45:52.000 --> 00:45:56.960]   it's really great. It's really amazing. I have a hard time explaining it.
[00:45:56.960 --> 00:46:01.520]   Can I hit rewind for one quick second? Yes. I was thinking about in the break,
[00:46:01.520 --> 00:46:06.880]   the story you told where you said, oh, finally we can, I have known them reum old, so that's just
[00:46:06.880 --> 00:46:13.280]   what's happening to me. Finally, we can close that case because we heard who had paid for it.
[00:46:13.280 --> 00:46:17.920]   What was that story again? Yeah, I was just like understanding, you know, where,
[00:46:17.920 --> 00:46:24.000]   you know, we had seen this slap of Facebook, slap a teacher story,
[00:46:24.000 --> 00:46:29.120]   and it was being pumped out. Like this was this viral TikTok challenge. And, you know,
[00:46:29.120 --> 00:46:33.600]   amongst researchers were like, we don't see it. Like, we don't see it on TikTok. There's a little
[00:46:33.600 --> 00:46:39.440]   bit of stuff about slap a teacher, but it's not viral like this. And so we had kind of worried
[00:46:39.440 --> 00:46:44.800]   wondered why there was media pickup of something that hadn't really happened. And we were kind of,
[00:46:44.800 --> 00:46:50.400]   you know, we will half write a bunch of case studies here and there just because we want to
[00:46:50.400 --> 00:46:56.240]   save that data and we want to hold on to it. And so what this story allowed us to connect the dots
[00:46:56.240 --> 00:47:03.440]   between is that there was actually a media PR campaign behind, you know, getting people to
[00:47:03.440 --> 00:47:09.200]   think that TikTok was allowing these abhorrent, you know, challenges to happen on their platform
[00:47:09.200 --> 00:47:17.280]   when in fact the op ed or the letter to the editor that was written was written by this targeted
[00:47:17.280 --> 00:47:22.480]   victory organization. And did you find out who was behind it? Who funded it?
[00:47:23.920 --> 00:47:29.120]   In this case, it was Facebook that had been funding the decision. This is one of the Facebook cases.
[00:47:29.120 --> 00:47:32.480]   This is one of the Facebook's cases. Yeah. That's one of the things that was discovered.
[00:47:32.480 --> 00:47:41.200]   Yeah. Yeah. No. And yeah, and they had done this in the past in 2018 with Google and Apple,
[00:47:41.200 --> 00:47:46.480]   they had placed they had paid another PR agency to place stories saying that Google and Apple are
[00:47:46.480 --> 00:47:53.440]   not as privacy for it as we may think. Oh, really? Yeah. Surprise.
[00:47:53.440 --> 00:47:59.200]   You did a Facebook. Oh, I wanted to put it together. Yeah. I mean, you know, I just, my mind becomes
[00:47:59.200 --> 00:48:04.800]   a repository for all of these like corporate shenanigans. And it's just like, the end of the day,
[00:48:04.800 --> 00:48:11.280]   you just realize, you know, the there can be so many books written about this stuff. But getting
[00:48:11.280 --> 00:48:16.720]   accountability for massively resourced companies is going to be impossible without
[00:48:16.720 --> 00:48:26.160]   some kind of legislation that, you know, makes transparency, a priority. But we're in this,
[00:48:26.160 --> 00:48:30.800]   you know, space where, of course, yeah, Microsoft has been up to this stuff for a long time.
[00:48:30.800 --> 00:48:35.280]   Is that the best solution? Just to say, you have to report, if you're doing a campaign like this,
[00:48:35.280 --> 00:48:38.720]   you have to report it like you just have to be transparent about it. Yeah, I think it's just
[00:48:38.720 --> 00:48:43.200]   be transparent about it. Could you not? You don't want to outline it's a fine line between that
[00:48:43.200 --> 00:48:48.320]   matter. It's speech. It's speech. It's every time it's speech. Yeah, but it's also just this, um,
[00:48:48.320 --> 00:48:55.360]   like we use it when we're studying this, we look at that kind of framework as a true cost of
[00:48:55.360 --> 00:49:02.880]   misinformation. So getting the Petaluma police to comment on door knocking in the dormitories
[00:49:02.880 --> 00:49:08.640]   is a waste of somebody's time that is getting paid. And so we have to wonder then, where did
[00:49:08.640 --> 00:49:15.760]   these Petaluma stories really generate from? And if they were a target of this PR campaign and they
[00:49:15.760 --> 00:49:20.640]   were like, look, the doors are getting injured. Who knows what's going to happen next?
[00:49:20.640 --> 00:49:27.440]   I think of the doors. You know, like the cats will get out. Who knows? Who knows what's going to
[00:49:27.440 --> 00:49:33.600]   happen? Only time will tell. Yeah. And so we often wonder with these made up stories that, uh,
[00:49:34.320 --> 00:49:40.160]   you know, how do you make someone responsible then for that? If, uh, you know, the punch of teacher
[00:49:40.160 --> 00:49:47.760]   challenges causing hysteria in a high school that has, you know, some violence and already and like
[00:49:47.760 --> 00:49:53.920]   teachers get afraid of this. And then it's not really even a thing. It reminds me too of the Tide
[00:49:53.920 --> 00:50:01.120]   Pods challenge that nobody was eating Tide Pods. Yeah. You know, and it just never lasts. Where did
[00:50:01.120 --> 00:50:07.120]   that come from? It's it because it's it that one. I think I remember that there had been a rumor
[00:50:07.120 --> 00:50:11.760]   where someone said, you know, kids are eating Tide Pods. They look like candy and then
[00:50:11.760 --> 00:50:16.880]   like people on TikTok and other social media did pretend like they were going to about to
[00:50:16.880 --> 00:50:23.840]   budge into a Tide Pod, like saying looks delicious, right? And, uh, and then it kind of fueled that,
[00:50:23.840 --> 00:50:29.760]   that panic for a while. And, you know, poison control centers and other places were responding
[00:50:29.760 --> 00:50:35.040]   on social media being like, if you do eat detergent, you know, go to your local hospital or are you way?
[00:50:35.040 --> 00:50:38.640]   You just like, it's like,
[00:50:38.640 --> 00:50:44.160]   late in your veins and drink bleach about all that the present. Yeah. I know. Yeah. This is,
[00:50:44.160 --> 00:50:48.080]   it's called public relations. Public relations is spas, spas, spas, spas spinning. It's about
[00:50:48.080 --> 00:50:52.720]   spinning things in your favor and in your competitors dis favor. And that's, I don't, well, I think
[00:50:52.720 --> 00:50:57.520]   you want to outlaw that. I mean, the responsibility for not using public funds for this is for the
[00:50:57.520 --> 00:51:02.960]   pedal. I'm a police to say, no, when channel two comes calling and says we'd like to interview you
[00:51:02.960 --> 00:51:09.840]   about people kicking in doors, just say, no, I have, I have things I need to do now. And I'm not
[00:51:09.840 --> 00:51:15.520]   going to do that. And for channel two, when they get the PR call from targeted, whoever,
[00:51:15.520 --> 00:51:19.200]   to say, yeah, we're not interested in that. The problem is you get, you know, complicity all down
[00:51:19.200 --> 00:51:25.120]   the line, you know, channel channel two is using, I'm sure all of the, you know, they use these
[00:51:25.120 --> 00:51:32.720]   EPKs, these electronic press kits from companies, run these videos produced by companies all the
[00:51:32.720 --> 00:51:39.200]   time without disclosure. And the problem is, is that the kids are crazy and teenagers will never
[00:51:39.200 --> 00:51:44.080]   listen. Yeah. And that story, that's a good story. That's a good story. As old as time is,
[00:51:44.080 --> 00:51:51.200]   you know, and, uh, but you do have to worry, like in the long run, the acceleration of these and
[00:51:51.200 --> 00:51:55.920]   how many of these are we going to have to go through until is it foolish to say that
[00:51:55.920 --> 00:52:01.760]   rather than holding the platforms responsible, and I know you said that we have to, but to say
[00:52:01.760 --> 00:52:07.520]   that we should inoculate people and just kind of transparency is good. And then teach people
[00:52:07.520 --> 00:52:13.280]   look for the label that says it's made up. I mean, shouldn't that shouldn't that be how we
[00:52:13.280 --> 00:52:23.040]   handle this? I would love it if that was possible. But the way content is displayed on social media
[00:52:23.040 --> 00:52:29.840]   is a play of appearances. We had a whole rash of, uh, recontextualized media, just repurposed video
[00:52:29.840 --> 00:52:37.120]   and JPEGs and, uh, to say, you know, this is images from the war in Ukraine. And it was just,
[00:52:37.120 --> 00:52:42.480]   it was an overwhelming volume of content that had been served up very early on. Jeff
[00:52:42.480 --> 00:52:47.120]   has been in the vision. He said, watch the video you're seeing on MSNBC and CNN. It comes from Russia.
[00:52:47.120 --> 00:52:54.560]   Mm hmm. And then some of them were video game simulations, like that whole ghost of Kiev story.
[00:52:54.560 --> 00:52:59.840]   I mean, we all like, like the X files, we all wanted to believe that, you know,
[00:52:59.840 --> 00:53:06.080]   it, well, but we wanted to believe the ghost of Kiev was going to save Ukraine from Russian,
[00:53:06.080 --> 00:53:12.560]   you know, air, you know, supremacy. And it was, it was nuts, you know, and it was a, what was
[00:53:12.560 --> 00:53:19.440]   circulating was a video game simulation of a fighter pilot downing, uh, other warplanes. And,
[00:53:19.440 --> 00:53:28.000]   and people were sharing it as if, you know, Ukraine could mount a defense, um, which we, we, we, we
[00:53:28.000 --> 00:53:33.920]   know they couldn't. And so the problem here is what ends up getting in the way of people's truth
[00:53:33.920 --> 00:53:38.800]   seeking behaviors. And so if you're looking for truth and you're, and you're saying Ukraine
[00:53:38.800 --> 00:53:45.360]   air support and you're getting a bunch of like disconnected videos, some of them real,
[00:53:45.360 --> 00:53:51.360]   some of them fake, whose responsibility is it then to put those labels on to make sure that the
[00:53:51.360 --> 00:53:57.120]   truthful stuff comes up first and, and to sort and rank. Like if you, if you break these platforms
[00:53:57.120 --> 00:54:06.240]   down to their base function, their function is to discover, rank and represent information. They
[00:54:06.240 --> 00:54:12.240]   just, that's what they do. Right. And so, um, it's the ranking. It's the ranking. Yeah. Yeah.
[00:54:12.240 --> 00:54:19.520]   It's the rank question. That problem. Yeah. Um, because I, I, one of my theories, I have a whole
[00:54:19.520 --> 00:54:23.520]   bunch of crazy theories, but you know, at least none of them are about the earth being flat, um,
[00:54:24.400 --> 00:54:30.160]   is that we're concentrating still too much on the wrong end. Now don't take any insult to this,
[00:54:30.160 --> 00:54:34.080]   because at the end you could concentrate on, but is, is people want to say to the platforms,
[00:54:34.080 --> 00:54:37.680]   play, welcome, we'll get rid of all the bad stuff and everything will be okay. And then
[00:54:37.680 --> 00:54:43.280]   instead the job has to be, as it has been since printing, sorry, Gutenberg moment, that, um,
[00:54:43.280 --> 00:54:48.160]   we need institutions to find the good stuff. The bad stuff's always going to be there. We can't
[00:54:48.160 --> 00:54:52.560]   get rid of it, but, but too much attention is paid to it because we don't have the mechanisms,
[00:54:52.560 --> 00:54:59.200]   I think, to find that, which is credible and relevant and, and certified and, and so on. And, and, and
[00:54:59.200 --> 00:55:04.800]   we need to invent, and the institutions we have, newspapers, editors, publishers, are inadequate
[00:55:04.800 --> 00:55:10.400]   to the scale that we have. And so I just think we, we, we need to kind of shift our thinking a
[00:55:10.400 --> 00:55:14.720]   little bit and realize that there's always going to be stupid stuff there. And the trick is to find
[00:55:14.720 --> 00:55:19.520]   that way is to ignore it better. Does that make any sense? Well, yeah. And that's part of the
[00:55:19.520 --> 00:55:25.920]   discoverability ranking stuff. If it's, if the platforms are continuously going to serve
[00:55:25.920 --> 00:55:33.920]   things that are outrageous and nearly unbelievable, um, then that's what we're going to get. I don't
[00:55:33.920 --> 00:55:42.720]   think there's a deficit of true and correct content, uh, or factual content. The, the, the issue mainly
[00:55:42.720 --> 00:55:49.760]   is that there's an industry behind producing disinformation or misinformation at scale that
[00:55:49.760 --> 00:55:55.920]   has to be reckoned with in order to make sure that these algorithms are not being game, uh,
[00:55:55.920 --> 00:56:00.720]   and the repeat offenders are removed from these platforms so that they can't just keep
[00:56:00.720 --> 00:56:07.680]   reinventing new ways to do the same old thing. Um, and so I think there's a, a bunch of interventions
[00:56:07.680 --> 00:56:13.120]   that need to happen. And, and you know, and the hard part is, is you have, you were talking about
[00:56:13.120 --> 00:56:20.160]   this in the context of America, which is, uh, you know, anti institutional at its core in a lot of
[00:56:20.160 --> 00:56:25.680]   ways. You know, we have institutions, but the people don't love them. Uh, and so, you know,
[00:56:25.680 --> 00:56:32.960]   that's why you see people who are anti institutionalist rising up on YouTube as these like very,
[00:56:32.960 --> 00:56:39.760]   and you know, very well known creators, right? Because people will engage with their content
[00:56:39.760 --> 00:56:45.680]   because they're like, look what Google's doing now. They're partnering with the WHO. This is terrible.
[00:56:45.680 --> 00:56:51.680]   And you're like, well, it's better than partnering with nobody or, you know, letting the stuff from,
[00:56:51.680 --> 00:56:57.600]   you know, the conspiracy theorists reign supreme. And so I do think you're right in the sense that
[00:56:57.600 --> 00:57:05.680]   there needs to be better, uh, in more stable linking on the internet to things that have these, uh,
[00:57:05.680 --> 00:57:11.920]   that things that are wedge issues, things that, you know, kind of make people, uh, think twice about
[00:57:11.920 --> 00:57:18.640]   other people's humanity, right? Racism, sexism, transphobia, you know, homophobia, uh,
[00:57:18.640 --> 00:57:27.120]   religious bigotry. Um, and so there does need to be, uh, clear content rules that are enforced.
[00:57:27.120 --> 00:57:32.000]   But I agree with you that the also the institutions are going to have to, um,
[00:57:32.000 --> 00:57:38.000]   uh, step in in a different way. And I'd love to get you guys's opinion on, uh,
[00:57:38.000 --> 00:57:43.840]   White House briefing tick talkers on how to talk about Ukraine. Like, did you guys follow that story?
[00:57:43.840 --> 00:57:48.080]   Oh, yeah. Oh, yeah. We talked about it. And we said, yeah, that's fine. That's appropriate.
[00:57:48.080 --> 00:57:54.480]   They're, they're thought make, you know, leaders, thought makers, taste makers, they auto, uh,
[00:57:55.040 --> 00:57:59.360]   I mean, it's pretty clear the White House's main goal was to kind of feed them the line.
[00:57:59.360 --> 00:58:06.240]   And, uh, and I, I had some question about, um, you know, whether these people were kind of
[00:58:06.240 --> 00:58:11.840]   trained and journalists enough to know that they were being spun. But you know, this is,
[00:58:11.840 --> 00:58:17.600]   see, Maya, they weren't trained. No, I know they weren't. They didn't know. Easy to spin. And,
[00:58:17.600 --> 00:58:20.800]   you know, that's why they were there, but also because they were very influential.
[00:58:21.680 --> 00:58:25.360]   I'm kind of more laissez-faire. I really feel like, uh,
[00:58:25.360 --> 00:58:31.600]   more information is good information. We know more about what's going on because of that,
[00:58:31.600 --> 00:58:37.840]   for one thing. You know, I'm sure of disinformation as old as this, you know, dirt, but, uh, we know
[00:58:37.840 --> 00:58:45.280]   now more about how it spreads and stuff. This is your, this is what you study, but that I don't,
[00:58:45.280 --> 00:58:50.160]   I don't think that that's necessarily a bad thing. Do you? I mean, do you, so, you know,
[00:58:51.280 --> 00:58:57.680]   do you want a judgment on this? I'm two minds about it. One, one point is that, yeah, like,
[00:58:57.680 --> 00:59:04.240]   there was a heavy hand in the NATO messaging, right? So it wasn't the voices of Ukraine. It wasn't the
[00:59:04.240 --> 00:59:09.200]   oppressed of Russia that were meant to be represented here. It was very much, you know,
[00:59:09.200 --> 00:59:17.760]   US NATO party line here, uh, which was that, you know, Ukraine is, is an underdog, but a strong
[00:59:17.760 --> 00:59:23.840]   contender for survival, right? Um, which I thought was really problematic in the sense that I couldn't
[00:59:23.840 --> 00:59:29.680]   tell you if any of these tick talkers actually feel that way. And what it reminded me of is the
[00:59:29.680 --> 00:59:35.760]   youth, uh, Bloomberg paid a bunch of tick talkers in the 2020 primaries to make him relevant
[00:59:35.760 --> 00:59:42.000]   and to name him into existence. I didn't feel like any of those tick talkers or Instagrammers or
[00:59:42.000 --> 00:59:49.520]   whoever were going to, um, you know, vote for him, right? And so you know, you know, that's where
[00:59:49.520 --> 00:59:56.080]   this stuff becomes dangerous territory where you, we've had advertising, we've had coercive
[00:59:56.080 --> 01:00:02.560]   advertising as long as that's, we've had campaigns. So it's just, we haven't liked it. Yeah, but it's
[01:00:02.560 --> 01:00:08.800]   just another form of it. It is, but it's also this sponsored advertising, this entire industry that's
[01:00:08.800 --> 01:00:15.200]   not really regulated in any way and not, there's no, you know, serious effort to get disclaimers out
[01:00:15.200 --> 01:00:21.040]   there that, you know, this information was provided by, you know, Bloomberg or whoever, um,
[01:00:21.040 --> 01:00:25.360]   or sponsored by like, you know, I think there's also a difference if you're getting paid to do
[01:00:25.360 --> 01:00:31.440]   something versus like the White House's good faith efforts to, uh, make young people aware about
[01:00:31.440 --> 01:00:37.120]   what was happening, uh, in Ukraine, but it's just, uh, it's hard to tell. Like we're headed down this
[01:00:37.120 --> 01:00:43.600]   road, we have to start to, uh, you know, we'd be mad. We'd be mad if they ignored it.
[01:00:43.600 --> 01:00:52.080]   Right? We get mad all the time. We only, if they only briefed mainstream, uh, you know, networks,
[01:00:52.080 --> 01:00:57.040]   the bloggers, we'd like the bloggers and we get mad about that. So I mean, I think this is
[01:00:57.040 --> 01:01:02.800]   democratizing information. My only qualms as, as I said, that, you know, they were probably
[01:01:02.800 --> 01:01:07.520]   were spinning them in a way that, but you know, that's life, you know, you, that's how you learn.
[01:01:07.520 --> 01:01:14.720]   Next, next time they'll be wiser about getting spun. Uh, and I think it's the White House job to,
[01:01:14.720 --> 01:01:19.920]   to do that. But, and, and, and, and, you know, the problem is that the too much skepticism, and
[01:01:19.920 --> 01:01:26.160]   this is a data void point, that if we teach our young people to, to not trust anything or anyone,
[01:01:26.160 --> 01:01:30.720]   then that's where we end up here. I, I just was going through Twitter because I, I violate the rules
[01:01:30.720 --> 01:01:34.720]   and I do that on the show sometimes. I'm doing it too right now. So it's okay. And I, and I saw this
[01:01:34.720 --> 01:01:40.720]   amazing one rumor is that Pfizer, who sponsored the Oscars, needed a big moment around Alopecia
[01:01:40.720 --> 01:01:45.440]   because they have a drug coming to work. Oh, please. That's a conspiracy, right? Right. Right.
[01:01:45.440 --> 01:01:49.040]   Right. You know, so that's the kind of stuff that people who are, who has to gravitate to.
[01:01:49.040 --> 01:01:53.440]   They want to. But the other thing Leo, I think is important is a survey after survey after
[01:01:53.440 --> 01:01:57.680]   survey has the majority of people saying they don't trust what they read on social media.
[01:01:57.680 --> 01:02:01.680]   That's really good news. That's great news. Yeah. Right.
[01:02:01.680 --> 01:02:06.400]   This is called growing up. Yeah. You know, learning what to trust, what not to trust,
[01:02:06.400 --> 01:02:09.200]   what to believe, what not to believe choosing. I mean,
[01:02:09.200 --> 01:02:16.320]   every human, one of the things every human has to do, many don't do, is figuring out what their
[01:02:16.320 --> 01:02:21.680]   values are, what their core beliefs are, what they care about in, in kind of in a vacuum,
[01:02:21.680 --> 01:02:26.560]   having taken in all this information as they grew up, now really thinking about it,
[01:02:26.560 --> 01:02:29.840]   and then acting from then on, according to their values,
[01:02:29.840 --> 01:02:39.600]   something everybody has always had to do. So I think more, I don't think more information or
[01:02:39.600 --> 01:02:45.920]   information flooding is necessarily a bad thing. People, it's, you know, it's just life you have to
[01:02:45.920 --> 01:02:51.680]   learn. But it comes down to quality. And like if you, so there's always been crap.
[01:02:52.480 --> 01:02:57.440]   Yeah. But one of the big conundrums with misinformation research, I grew up with Gilligan's Island.
[01:02:57.440 --> 01:03:05.120]   There's always been crap. Oh, don't I know it. You know, but I think, you know, the moment that
[01:03:05.120 --> 01:03:10.320]   you're searching for something online and you're, you know, is the moment where you're most susceptible
[01:03:10.320 --> 01:03:15.200]   to misinformation because you're in, you're in an information seeking mode and you know that
[01:03:15.200 --> 01:03:23.200]   you don't know. And that's where the companies play an incredibly important role. Whereas, you
[01:03:23.200 --> 01:03:29.280]   know, maybe it was harder back in the day before, you know, internet search, he had to go to a
[01:03:29.280 --> 01:03:35.200]   library to kind of figure these things out. Because you had your neighbors and your friends and family.
[01:03:35.200 --> 01:03:41.040]   You talk to your neighbors. And I think that this is when you go out to your peers and you say,
[01:03:41.040 --> 01:03:45.920]   you know, do you think it's Alopecia that became a story because Pfizer wanted to,
[01:03:45.920 --> 01:03:50.640]   and then they go, what are you nuts? And then you go, yeah, I guess that's probably not,
[01:03:50.640 --> 01:03:55.520]   not really happening. And you saw you. This is, I feel like this is a normal process that we're
[01:03:55.520 --> 01:04:01.520]   pathologizing. I don't think so though, because if I were to search online and because that story
[01:04:01.520 --> 01:04:07.600]   about Alopecia is, you know, ranked really highly because a lot of people are sharing it,
[01:04:07.600 --> 01:04:14.400]   then that's a story that I probably wouldn't have run into if not by virtue of internet ranking
[01:04:14.400 --> 01:04:19.680]   systems or listening to this podcast or listening to this podcast. But it's it is going to reach
[01:04:19.680 --> 01:04:26.080]   many more people, especially in moments where they're seeking information. If they were just like,
[01:04:26.080 --> 01:04:29.680]   well, what is Alopecia? Are you more vulnerable because you're seeking information?
[01:04:29.680 --> 01:04:34.880]   Yeah, you are because you're asking a lot of people. I would say you're less vulnerable.
[01:04:34.880 --> 01:04:39.200]   Questions now. Because now you're in the, I think you're, I think you're most vulnerable
[01:04:39.200 --> 01:04:45.600]   when you know the answer and you don't question it. I think you're, you're less vulnerable when
[01:04:45.600 --> 01:04:50.640]   you're saying, I want to know what the answer is. And you're weighing facts. That's when you're
[01:04:50.640 --> 01:04:56.480]   best capable of saying what, you know, let's, let's look at the, the truth. Yeah, my problem is,
[01:04:56.480 --> 01:05:02.400]   is it's not all facts that you're weighing things against. You're looking at a, you know,
[01:05:02.400 --> 01:05:08.640]   you're looking at a social media return. Joe, you would agree that there's no way to create a
[01:05:08.640 --> 01:05:14.400]   perfect stream of factual information. There's always going to be misinformation. Yes. I think
[01:05:14.400 --> 01:05:21.040]   there will always be misinformation in spaces that have a preponderance of user generated content.
[01:05:21.040 --> 01:05:26.880]   Right. Humans are really good at it. Yeah. You know, but I do think that to Jeff's point earlier,
[01:05:26.880 --> 01:05:31.600]   that there is a way to create streams of more factual or public interest information that have
[01:05:31.600 --> 01:05:38.000]   not been explored yet. And that aren't prioritized by these companies. And maybe it shouldn't be the
[01:05:38.000 --> 01:05:42.720]   role at, you know, and maybe you got me here, Leo, on this is maybe it's not the role of social
[01:05:42.720 --> 01:05:50.160]   media companies to do that. Maybe, you know, I mean, I maybe they tell you that you're in a,
[01:05:50.160 --> 01:05:58.160]   you're in a landfill of words. Yes. Their job is to file it on. Yeah. And so, but you want it,
[01:05:58.160 --> 01:06:04.000]   you want to demand more from companies that are in the I don't want them to be arbitrator of what
[01:06:04.000 --> 01:06:08.960]   is good and not good, what is true and not true. I don't they already are. Well, they already are.
[01:06:08.960 --> 01:06:13.680]   But that's what I always say against, you know, when Twitter says, well, you don't really want
[01:06:13.680 --> 01:06:19.600]   a chronological feed, I say, no, yes, I do. I don't want you telling me what what's important.
[01:06:19.600 --> 01:06:24.720]   I don't do Facebook because I don't want them just, you know, giving me information based on their
[01:06:24.720 --> 01:06:32.160]   values. So, but I don't want them to filter out stuff. I want them to give it give it all to me.
[01:06:32.160 --> 01:06:36.000]   Let me know the judge. You really don't want that. Yes.
[01:06:36.000 --> 01:06:40.720]   Then you're going to get the alopecia stuff right next to an equal to the New York Times,
[01:06:40.720 --> 01:06:47.040]   which is what that's, that's an old media point of view that there's somebody, the New York Times,
[01:06:47.040 --> 01:06:52.400]   or somebody who should be judging and that you show you them. It's next to you that the
[01:06:52.400 --> 01:06:57.840]   alopecia stupid theory is right next to Twitch and is fine. You don't really want that.
[01:06:57.840 --> 01:07:02.320]   Well, who do you want to say not? Who do you want to? Who's in charge?
[01:07:02.320 --> 01:07:09.120]   This is why we created the institutions that are now outmoded. But, but, you know, in 1470,
[01:07:09.120 --> 01:07:14.320]   I'll be going to Gutenberg here 1470, Niccolo Potarotti said to the pope, you got to fix this
[01:07:14.320 --> 01:07:19.200]   print stuff. It's all messed up. Got a censoring. And what he was really asking for was an editor.
[01:07:19.200 --> 01:07:24.160]   No, what he was asking for was not censorship. What he was asking for was the institution
[01:07:24.160 --> 01:07:29.360]   of editing and publishing. I would submit, and this is kind of a change of pace for me,
[01:07:29.360 --> 01:07:36.480]   that all judgment is editorial judgment is censorship of some kind. It's deciding what's good
[01:07:36.480 --> 01:07:40.720]   and what's not good. And I don't, I want to get all, give me all the information.
[01:07:40.720 --> 01:07:43.600]   Gee, who decides what stories we talk about in the rundown?
[01:07:43.600 --> 01:07:50.160]   Well, no, but no, but at a meta level, you decide what podcast you listen to. You decide who you
[01:07:50.160 --> 01:07:57.440]   want to hear talking about compute consumer choice. Right? I'm saying, I understand that what
[01:07:57.440 --> 01:08:03.280]   has happened over the intervening thousands of years is the amount and quantity of information has
[01:08:03.280 --> 01:08:10.480]   gotten unmanageably huge. And everybody who who encounters the internet, and I'm sensitive to it
[01:08:10.480 --> 01:08:17.520]   because I didn't grow up with it, goes, whoa, there's a barrage of information. But you quickly,
[01:08:17.520 --> 01:08:25.360]   but the truth is that's life. Your censarium is barraged, even the minute you come out of the womb,
[01:08:25.360 --> 01:08:32.320]   and you learn how to go on TikTok. You learn how to triage it. Internally,
[01:08:33.120 --> 01:08:40.800]   using inputs from friends, neighbors, family, mother, father, and your own judgment. And that's
[01:08:40.800 --> 01:08:45.040]   up. It's on you. And I don't want anybody telling me what I should know or not know.
[01:08:45.040 --> 01:08:49.840]   I want all the information and I will decide. I think anything else is sensitive.
[01:08:49.840 --> 01:08:57.360]   But you're, yeah, but here, let's go back to the example then where defineers is or like a
[01:08:57.360 --> 01:09:03.120]   targeted victory. The PR firm that's working on behalf of Facebook is getting, you know,
[01:09:03.120 --> 01:09:09.760]   ghost writers to plant letters to the editor. And you're just like, don't you feel like there
[01:09:09.760 --> 01:09:16.000]   should be some some truth in like, yes, the answer to that is more information, not less
[01:09:16.000 --> 01:09:22.720]   information. So the just as you said, transparency. So I want to know, all metapaid for that letter,
[01:09:23.520 --> 01:09:27.600]   that will help me judge the value of that letter. That's more information, not less.
[01:09:27.600 --> 01:09:37.360]   Yeah, but there's also just this, you know, it, I hear you and I don't agree that all editing is
[01:09:37.360 --> 01:09:42.240]   censorship because like, to be honest with you, like some words got to go.
[01:09:42.240 --> 01:09:47.840]   No, I'm not saying you. Yes, of course. You and I and everybody else should edit.
[01:09:48.720 --> 01:09:53.520]   But I don't want to, I don't want to put somebody you don't want to computer an algorithm doing it.
[01:09:53.520 --> 01:09:58.240]   Or just or or has even one human being now, you don't even want the editor of the New York Times
[01:09:58.240 --> 01:10:02.800]   down. That's what you're saying. You want to know Dave Myers view of the river. No, they can. No,
[01:10:02.800 --> 01:10:07.040]   they can. And then if I choose the New York Times, I'm choosing to read their viewpoint.
[01:10:07.040 --> 01:10:11.360]   It needs to be clear in my time. I'm saying, if you need, if you're listening, you're listening
[01:10:11.360 --> 01:10:17.520]   a viewpoint, but I should be able to choose. I shouldn't, I shouldn't, there should not be.
[01:10:17.520 --> 01:10:23.040]   And this is where a big tech can be gatekeepers because they control the faucets.
[01:10:23.040 --> 01:10:29.120]   They open them up, open them up. But beware, you're going to sound like car in a minute and say
[01:10:29.120 --> 01:10:35.760]   you are required to carry everything. And no, still my boy with you, you get
[01:10:35.760 --> 01:10:42.560]   to agree with you. I choose to follow who Twitter or even people on Twitter. So I get the choice
[01:10:42.560 --> 01:10:47.440]   and Twitter gets the choice. And if I want to files, if I want to go on truth, social or
[01:10:47.440 --> 01:10:53.760]   GAB, I can. And so that's the way it should be. That's the right. And I think that's what the
[01:10:53.760 --> 01:10:58.080]   first man first amendment is about is that there shouldn't be some higher authority saying you,
[01:10:58.080 --> 01:11:02.800]   you, you, but no, not you. Well, the no, like,
[01:11:03.520 --> 01:11:12.000]   griffs, scams, hoaxes, like, you know, so the way that this field developed around this information,
[01:11:12.000 --> 01:11:18.640]   I had been in research about white supremacists and white supremacists using social media and
[01:11:18.640 --> 01:11:25.120]   other places to raise money and to recruit new followers and, you know, and the entire scene
[01:11:25.120 --> 01:11:31.520]   of these people on social media was about hiding who they were about using coded phrases.
[01:11:32.640 --> 01:11:37.600]   So that, you know, it was hard for content moderators to figure out what they were doing and planning.
[01:11:37.600 --> 01:11:42.240]   And these are some of the same jokers that eventually planned Unite the Right and Charlottesville
[01:11:42.240 --> 01:11:46.960]   using apps. And so it's, it's not the case. You wouldn't have stopped and dry. You wouldn't have
[01:11:46.960 --> 01:11:51.120]   stopped the Klan from marching through Skokie, though. Oh, I would have. You would have. I would
[01:11:51.120 --> 01:11:56.000]   have done it with my own two hands. I'm not a legislator. I would have, you know, I would have
[01:11:56.000 --> 01:12:01.040]   put this body on the line. That's the classic free speech. ACL use story, though, right? Is
[01:12:01.600 --> 01:12:06.560]   to defend the right. We can also protest. Joan can show up on your test and shoot the,
[01:12:06.560 --> 01:12:10.400]   shoot their feet and get them out of there. Yeah. No, you can't shoot their feet. Well,
[01:12:10.400 --> 01:12:17.040]   with, sorry, I did not, I do not own any firearms. No, no, no, but what I'm saying to you,
[01:12:17.040 --> 01:12:19.760]   you can have a dialogue. I'm just going to the top of your lungs.
[01:12:19.760 --> 01:12:25.120]   You're going to have a dialogue with the KKK any day of the week. I know where I stand.
[01:12:25.120 --> 01:12:31.520]   And I want to be that bicyclist in Washington, who was in the front of the truck of the convo.
[01:12:32.320 --> 01:12:39.200]   Just going real slow. Yeah. But the thing about it is, is, you know, as you start to move between
[01:12:39.200 --> 01:12:45.280]   the wires and the weeds, and you go from these like online examples to these real life examples,
[01:12:45.280 --> 01:12:50.880]   you have to also think about, well, what is the community that the KKK is routinely trying to
[01:12:50.880 --> 01:12:57.360]   march on? They're routinely trying to march on black communities. They're provoking fear,
[01:12:58.320 --> 01:13:06.400]   causing in many cases, riots and chaos in their wake. And so, you know, it's in the planning of that
[01:13:06.400 --> 01:13:12.560]   happens on social media, right? And so I do think that there should be more, more accountability
[01:13:12.560 --> 01:13:18.080]   for these systems and these mechanisms of coordination. That's a tricky, that's a tricky gray area.
[01:13:18.080 --> 01:13:23.760]   It's tricky. It's illegal for them to burn across. It's, is it illegal for them to go on
[01:13:23.760 --> 01:13:28.240]   telegram and say, let's go burn across. And who should be responsible? I would submit,
[01:13:28.240 --> 01:13:33.840]   it's not really telegram's job. It's maybe law enforcement's job. But, but you can keep going
[01:13:33.840 --> 01:13:37.200]   down that government. But well, there's a point, but that's what I'm saying. This is the tricky
[01:13:37.200 --> 01:13:44.560]   gray area where there's clearly illegal acts. And, and you know, that they should be arrested for
[01:13:44.560 --> 01:13:49.920]   that and put in jail. And, and then there's this gray, it gets grayer and grayers and go back and
[01:13:49.920 --> 01:13:58.640]   back, back, you know, to imagine this, Leo, you're the owner of the Baskin Robbins, you know, and the
[01:13:58.640 --> 01:14:04.960]   KKK is using your business as a place to have their Tuesday night meetings so that they can plan
[01:14:04.960 --> 01:14:08.560]   their own. You're allowed as the owner to say, get the hell out of here. I don't like what you're
[01:14:08.560 --> 01:14:12.880]   saying. Then, then all we're asking is the owners of Facebook to be like, get the hell out of you.
[01:14:12.880 --> 01:14:16.880]   No, it's a garden party. They can. It's a garden party. No one's saying they shouldn't be able to.
[01:14:16.880 --> 01:14:21.440]   I'm not saying that. No, well, let's just chairman the this is FCC commissioner is, but I just I
[01:14:21.440 --> 01:14:26.320]   agree with you. Did you have I disagree with them? No one should say to any, but that's what I'm saying.
[01:14:26.320 --> 01:14:31.120]   It's not the platform's responsibility. They can do what they want. They should do what we as a
[01:14:31.120 --> 01:14:35.840]   society would like them to do. But that's up to them. But, you know, if you don't like it, get your
[01:14:35.840 --> 01:14:40.080]   bunch. If you don't like the basket, if the Baskin Robbins owner says, good, I love it. These guys
[01:14:40.080 --> 01:14:45.440]   eat a lot of ice cream. Then I reserve the right to go and march outside and say, let's never go to
[01:14:45.440 --> 01:14:51.440]   this Baskin and Robbins because the KKK organizes there. That's all of that's appropriate.
[01:14:51.440 --> 01:14:56.880]   Yeah. That's the kind of public pressure that's been put on like Twitter, for instance. So,
[01:14:56.880 --> 01:15:02.720]   Twitter had blue checked a bunch of Charlottesville organizers and people were like, why are you
[01:15:02.720 --> 01:15:07.360]   blue check and white supremacists over here, Twitter? And they were like, we, I don't know,
[01:15:07.360 --> 01:15:13.120]   we're going to suspend all blue checks from here for a while and reevaluate our system.
[01:15:13.120 --> 01:15:14.320]   Fair enough. That's their right.
[01:15:14.320 --> 01:15:17.360]   They started removing blue checks and.
[01:15:17.360 --> 01:15:20.960]   That's their right. I just don't want Congress or the FCC to tell them what to do.
[01:15:20.960 --> 01:15:23.680]   Amen. Can I ask Joan a question here?
[01:15:23.680 --> 01:15:26.160]   I think that's the line we all agree with.
[01:15:26.160 --> 01:15:34.000]   So there was a paper from Cambridge, Dan Williams called the marketplace of rationalizations that
[01:15:34.000 --> 01:15:40.320]   came out this month. And what he argues is that there's a marketplace of belief. And then people
[01:15:40.320 --> 01:15:45.920]   have a belief they go for a rationalization. And he said there are agents that
[01:15:45.920 --> 01:15:51.040]   basically sell these rationalizations. Yeah, this is.
[01:15:51.040 --> 01:15:52.000]   This is what we see.
[01:15:52.000 --> 01:15:57.280]   This is condiment as well, which is we decide what we want to believe based on emotion and then
[01:15:57.280 --> 01:16:02.320]   find reason to support it. But what it said to me was the journalism is set up. As you said,
[01:16:02.320 --> 01:16:10.080]   a few minutes ago, Leo, in a marketplace of facts, when facts are insufficient and people are
[01:16:10.080 --> 01:16:14.080]   going to have these beliefs and I'm not buying filter bubbles and echo chambers,
[01:16:14.080 --> 01:16:17.760]   but they do have beliefs. And what they're seeking is not necessarily facts.
[01:16:17.760 --> 01:16:21.600]   They're seeking a rationalization help me believe in the truth.
[01:16:21.600 --> 01:16:28.640]   So we're set up wrong. So if we judge journalists as to whether or not people understand they
[01:16:28.640 --> 01:16:37.200]   should wear masks in a pandemic, then we become educators propagandists.
[01:16:39.040 --> 01:16:43.360]   What's our role in a belief based market rather than a fact based market?
[01:16:43.360 --> 01:16:51.120]   How do we in journalism deal with managing to serve the marketplace sensibly?
[01:16:51.120 --> 01:16:56.640]   Before Joan answers, I'm going to turn this on its head and say, as a consumer of journalism,
[01:16:56.640 --> 01:17:03.520]   I would behoove me to understand the mixed motives of a journalistic entity because
[01:17:03.520 --> 01:17:07.360]   one of the fairs clear motives is profit.
[01:17:08.640 --> 01:17:14.800]   And I don't know of any journalistic entity that doesn't consider what its readers or listeners
[01:17:14.800 --> 01:17:24.640]   or viewers want. And to some degree or more or less, pander to that. So as a consumer of journalism,
[01:17:24.640 --> 01:17:33.200]   it's important to know that. Right? And I would say you act as a journalist, you act as you think best.
[01:17:33.200 --> 01:17:35.920]   And I as a consumer of journalism are going to act as a judge.
[01:17:35.920 --> 01:17:39.440]   Best. We'll judge it. And by the way, fascinating. Here's a good,
[01:17:39.440 --> 01:17:42.080]   this is kind of a good example. Then I'm going to let Joan talk, sorry, Joan.
[01:17:42.080 --> 01:17:46.160]   But here's a good example. We've been talking about masks a lot. Somebody in the
[01:17:46.160 --> 01:17:50.240]   Twitter community and I love this said, you know, Leo, there has been a study because
[01:17:50.240 --> 01:17:55.920]   somebody sent me an email saying he was actually a prisoner at Lompoke. And he sent me
[01:17:55.920 --> 01:18:01.600]   saying, among other things, masks don't work. Everybody knows that they don't block the virus.
[01:18:01.600 --> 01:18:06.720]   And I said, well, tell that to the surgeon next time he's operating on you. And somebody posted
[01:18:06.720 --> 01:18:13.520]   this fascinating article, I think from science saying that actually that's more of a widely held
[01:18:13.520 --> 01:18:19.440]   belief without any scientific evidence that every study that they've attempted to do on this
[01:18:19.440 --> 01:18:24.560]   shows that in fact, masks in the surgical theater don't necessarily do anything at all.
[01:18:24.560 --> 01:18:31.200]   We do have studies on masks and viruses. Okay. But my point, my point being is I,
[01:18:31.600 --> 01:18:35.360]   I've always used that as a retort. Well, to tell that to your surgeon, it's not the best
[01:18:35.360 --> 01:18:41.200]   retort as it turns out, given more information. And what's great is, and I'm proud of myself,
[01:18:41.200 --> 01:18:45.680]   because instead of rejecting that information outright, I read it and said, Oh, that's kind of
[01:18:45.680 --> 01:18:50.320]   interesting. I might, Oh, and this is the hardest thing. I might have been wrong about that.
[01:18:50.320 --> 01:18:56.320]   I think that's healthy. That's good. And that's because of more information and more information
[01:18:56.320 --> 01:19:00.080]   exchange. All right. If you even remember what Jeff's question was, Joan,
[01:19:01.040 --> 01:19:04.160]   you may get to leave versus fact marketplaces. Go ahead.
[01:19:04.160 --> 01:19:11.760]   I mean, you know, when it comes down to the way in which people make decisions, you know,
[01:19:11.760 --> 01:19:17.680]   there's been a couple of things that have been sort of thorns in the side of this research field
[01:19:17.680 --> 01:19:23.760]   for a long time, which is essentially like, once people adopt a political party, it's very rare
[01:19:23.760 --> 01:19:29.440]   that they switch it. And in most elections, you're talking about the shift from, you know, party to
[01:19:29.440 --> 01:19:34.720]   party is being less than 5% of people. And so that people become very locked into their beliefs.
[01:19:34.720 --> 01:19:41.760]   And then it does become a shortcut to action. That's who wins elections is the swing voters,
[01:19:41.760 --> 01:19:46.080]   right? Because the ones who are willing to vote against the last time they vote.
[01:19:46.080 --> 01:19:52.720]   Yeah, but it or the people who win elections are the people who vote for the candidate, right? And
[01:19:54.240 --> 01:20:00.880]   but the swing elections, the idea that there is this like a small group of people that can make
[01:20:00.880 --> 01:20:06.640]   this big difference. So deciding if you're going to be one of those people or not, of course, is
[01:20:06.640 --> 01:20:16.240]   is actually fairly rare compared to consistently following a party or in this case, I would imagine
[01:20:16.240 --> 01:20:21.600]   it would also carry to a set of beliefs. And, you know, there's not a lot of beliefs that we end
[01:20:21.600 --> 01:20:27.440]   up taking action on. And, you know, of course, some of us can be more hypocritical than others,
[01:20:27.440 --> 01:20:32.720]   like, you know, and so it's it but it's hard to say, you know, in terms of a fact-based market
[01:20:32.720 --> 01:20:41.200]   versus a belief-based market, if you have settled on something, it is very hard to do what Leo did,
[01:20:41.200 --> 01:20:47.920]   which is take a step back and say, well, do I really know how the question I ask myself all the
[01:20:47.920 --> 01:20:53.120]   time is how do I know that? And I have to, you know, I kind of have to remind myself that
[01:20:53.120 --> 01:21:01.040]   if I don't think about how I know something, then I am trafficking and belief. And it's more
[01:21:01.040 --> 01:21:07.120]   likely than not. And so, you know, for people that are trying to think about, you know,
[01:21:07.120 --> 01:21:12.720]   their positions on certain issues, and if they do have an anecdote or story that's really compelling
[01:21:12.720 --> 01:21:19.200]   to them, they should ask themselves, well, how do I know that? And then, and then Dick, and to try
[01:21:19.200 --> 01:21:24.560]   to figure out if if that was part of some kind of propaganda or that was part of some PR campaign
[01:21:24.560 --> 01:21:32.000]   or it got to you because you were, you know, a voter in a swing state that was being heavily
[01:21:32.000 --> 01:21:38.000]   targeted by marketing, you know, and so I think it's important that we do that, especially around
[01:21:38.000 --> 01:21:43.680]   really hot button political issues. Like I was reading some of the fallout around the
[01:21:43.680 --> 01:21:49.680]   quote unquote, "Don't say gay bill." And it's a pretty nuanced bill, but a lot of these--
[01:21:49.680 --> 01:21:52.960]   It doesn't, by the way, it doesn't say "don't say gay."
[01:21:52.960 --> 01:21:54.480]   But it was a meme. Yeah.
[01:21:54.480 --> 01:21:55.200]   You know, it's a meme.
[01:21:55.200 --> 01:21:57.280]   Yes, so we think that's what the bill was.
[01:21:57.280 --> 01:22:01.680]   Well, death panels, what are you, fight fire with fire? Get, you know, the robo is good with death
[01:22:01.680 --> 01:22:03.680]   battle. Well, just because they do it doesn't mean we have to do it.
[01:22:04.640 --> 01:22:09.520]   Yeah, but now you've got like teenagers screaming the word gay in their high school and gay people.
[01:22:09.520 --> 01:22:10.880]   They didn't have any Oscars.
[01:22:10.880 --> 01:22:11.360]   I like that.
[01:22:11.360 --> 01:22:12.480]   You're like, "You're leaving?"
[01:22:12.480 --> 01:22:15.600]   You even know, "Oh no, I'm just trying to live out here."
[01:22:15.600 --> 01:22:20.160]   You know, and so it can like, but at the end of the day, like, you want to have
[01:22:20.160 --> 01:22:26.160]   conversations with representatives from communities that actually have a point of view
[01:22:26.160 --> 01:22:29.520]   and are affected by the issues, and that's what journalism is great at.
[01:22:30.880 --> 01:22:36.000]   And then there are these other times when you have these PR folks or these political operatives
[01:22:36.000 --> 01:22:44.480]   slide in and really cloud the field or muddy the waters with very catchy, non-nuance takes.
[01:22:44.480 --> 01:22:49.600]   And so from our point of view, it's like that kind of stuff, especially when we see it happen
[01:22:49.600 --> 01:22:53.840]   memetically, like something like "don't say gay" is like really easy to remember.
[01:22:53.840 --> 01:23:00.080]   And so all nuance is gone. I'm not defending this bill by any means, of course.
[01:23:00.080 --> 01:23:09.040]   No, but to be clear, the bill says that you cannot teach content about sexual orientation
[01:23:09.040 --> 01:23:14.000]   or gender identity to kindergarteners through third grade. It doesn't say anything about not
[01:23:14.000 --> 01:23:21.040]   saying the word gay. I understand, but you know, believe me, I understand the discussion, but
[01:23:21.040 --> 01:23:26.960]   I think it doesn't help in the discussion to misrepresent stuff.
[01:23:28.160 --> 01:23:30.080]   But the intent is pretty much the same.
[01:23:30.080 --> 01:23:38.560]   If you get down to it, the intent of that bill was about transgender
[01:23:38.560 --> 01:23:48.240]   library, transgender story, drag queen story hour, right? And it was about young people.
[01:23:48.240 --> 01:23:54.000]   There's a whole other moral panic happening in pre-teens where a lot of teens are now
[01:23:54.000 --> 01:23:58.240]   opting to be called "they them." And parents are like, "What?"
[01:23:58.240 --> 01:23:59.680]   Yeah, it's throwing parents.
[01:23:59.680 --> 01:24:03.360]   It's like "gay." Like, "What is this hat?" And you know, most parents are like, "Whatever,
[01:24:03.360 --> 01:24:07.840]   I'll call you whatever as long as you're not getting too bad in school." But a lot of parents
[01:24:07.840 --> 01:24:12.160]   are alarmed by this. And so they're like, "Oh, they must be getting indoctrinated when they're
[01:24:12.160 --> 01:24:19.360]   young." And you know, I live through a period where you weren't allowed to talk about being gay in
[01:24:19.360 --> 01:24:20.720]   high school. Or like, you know, like-
[01:24:20.720 --> 01:24:22.480]   Wasn't that long ago, frankly?
[01:24:22.480 --> 01:24:27.920]   It wasn't that long ago. And so this kind of legislation, of course, is intensely problematic,
[01:24:27.920 --> 01:24:34.880]   especially if you have kids that are asking, "Why do I have two aunties? And why do they live
[01:24:34.880 --> 01:24:39.440]   together? And why do they only have one bedroom? And why do they share a dog?" And you know,
[01:24:39.440 --> 01:24:39.840]   it's like-
[01:24:39.840 --> 01:24:42.880]   No, and then this is the conversation we should be having.
[01:24:43.920 --> 01:24:50.160]   Not don't say gay, because it's not actually what the bill was about. And so now,
[01:24:50.160 --> 01:24:55.520]   I don't want to be paternalistic. I think that the natural thing is to say, "Well,
[01:24:55.520 --> 01:25:00.720]   we're smart enough to figure this out."
[01:25:00.720 --> 01:25:01.680]   Third person effect.
[01:25:01.680 --> 01:25:05.120]   Yeah, the third person effect. And I think-
[01:25:05.120 --> 01:25:10.560]   I mean, what do you say about that? That's paternalistic.
[01:25:10.560 --> 01:25:14.640]   Most people don't have a lot of time to figure that kind of stuff out, right?
[01:25:14.640 --> 01:25:19.520]   We want- And this is like the difference between that kind of fact-based or belief-based
[01:25:19.520 --> 01:25:24.400]   marketplace of ideas. If you have fact-based, then people are going to, you know, when their
[01:25:24.400 --> 01:25:29.600]   stories write about the legislation, and they're going to tell you what's in the legislation.
[01:25:29.600 --> 01:25:33.360]   If it's more belief-based, then it's really a battle of good and evil.
[01:25:33.360 --> 01:25:39.520]   And, you know, I personally might come down on the idea that like not being able to talk
[01:25:39.520 --> 01:25:47.280]   openly with children about gender identity and LGBTQ issues is like, you know, discriminatory
[01:25:47.280 --> 01:25:54.000]   and shouldn't be made into law. But at the same time, like, I'd like that debate to be- or that
[01:25:54.000 --> 01:26:00.640]   policy to be represented in a way in the media, especially to the people that we would count
[01:26:00.640 --> 01:26:05.200]   as allies in a way that makes them sound smarter when they're talking about it,
[01:26:05.200 --> 01:26:12.480]   makes them sound more informed, and it can lead to like more constructive, potentially,
[01:26:12.480 --> 01:26:18.560]   you know, more constructive policy making in the future that, you know, either eradicates this,
[01:26:18.560 --> 01:26:22.000]   or undermines it in some other way.
[01:26:22.000 --> 01:26:29.440]   So I don't know how long a variety has been doing its charts on engagement on Twitter,
[01:26:30.240 --> 01:26:38.720]   or the trending TV charts. But I think it's interesting that the Academy Awards
[01:26:38.720 --> 01:26:47.600]   got more engagements than any TV show since the charts began with 32.8 million engagements
[01:26:47.600 --> 01:26:52.240]   compared to the next- the number two on the charts for the week, Bridgerton, which was less than
[01:26:52.240 --> 01:27:00.160]   the- which about 20,95,000. I think that's information that's interesting. I don't- I think that
[01:27:00.160 --> 01:27:04.160]   it's- I have to say I was watching the Academy Awards with Twitter open, and I thought that was-
[01:27:04.160 --> 01:27:11.680]   I almost wanted to close the lid because I- the Twitter went crazy after the slap heard
[01:27:11.680 --> 01:27:16.240]   around the world. But I think it's also very- it ended up being a really good, powerful
[01:27:16.800 --> 01:27:23.680]   discussion, a valuable discussion, don't you think? I thought the discussion was all over the place.
[01:27:23.680 --> 01:27:29.040]   It was. It was. I was like- I couldn't figure out how people were going to come down on it. I even
[01:27:29.040 --> 01:27:36.000]   was texting with my- my dear old mom, and she was like, "Well, you know, I always thought
[01:27:36.000 --> 01:27:41.280]   good about Will Smith, but if he's willing to do that in public, imagine how he treats his family
[01:27:41.280 --> 01:27:46.240]   and staff." And I was like, "Oh, that's- that's a nuance point I hadn't thought about."
[01:27:46.240 --> 01:27:50.320]   Is that great? See? More information. It's interesting. Yeah.
[01:27:50.320 --> 01:27:55.520]   But I was just- yeah. I mean, I was- I was astounded that it happened, and then also just,
[01:27:55.520 --> 01:28:02.080]   you know, the way in which you saw it spun in different circles around like red feed,
[01:28:02.080 --> 01:28:09.920]   blue feed. It was just, you know, you could really tell, like, in some instances, there was just
[01:28:09.920 --> 01:28:15.280]   blatant racism that were just like- I think there were jokes from the right that was like about
[01:28:15.280 --> 01:28:21.360]   black on black crime. And, you know, this is quote-unquote, "thuggish behavior," which is code for the
[01:28:21.360 --> 01:28:29.200]   N word. And so it was, yeah. And so it was like- it was not good in that sense of the word. And then
[01:28:29.200 --> 01:28:35.200]   on top of it, on the left, you have this sort of mealy-mouthed approach of like, you know,
[01:28:35.200 --> 01:28:43.280]   like, you know, the echoes of the "should you punch a Nazi" debate that played out on Twitter.
[01:28:43.280 --> 01:28:47.760]   And like, people were saying, "Well, you know, sometimes people just need to get smacked." And
[01:28:47.760 --> 01:28:53.120]   I'm just like, I just don't think that that's what we need either. I think in though, in the final
[01:28:53.120 --> 01:28:59.280]   analysis, having had that national discussion was better than each person going home alone to think
[01:28:59.280 --> 01:29:04.880]   about it. Oh, good point. Oh, by themselves. Yeah. Because it brought out nuances. It brought out
[01:29:04.880 --> 01:29:10.880]   all that nuance. Yeah, in fact, I loved what your mom said. I think that's- to me, that's the
[01:29:10.880 --> 01:29:15.520]   benefit of us- of us- of us- But the other thing that's true is it's also- information place like
[01:29:15.520 --> 01:29:21.200]   Twitter. Go ahead. Sorry, Leo. No. It's also about white Twitter and black Twitter.
[01:29:21.200 --> 01:29:25.040]   Yeah, yeah. I mean, you can do all the tribalism and all that, too. No, no, no, no, I'm not saying
[01:29:25.040 --> 01:29:29.280]   that at all. That's interesting information. Yeah. Let me, let me give you an example. So I can't
[01:29:29.280 --> 01:29:34.400]   find it now, but there was a chart today. Somebody did an analysis of Twitter of which states were pro-
[01:29:34.400 --> 01:29:42.000]   rocker-pros. Right? And a black Twitter expert came in and said, they don't understand the
[01:29:42.000 --> 01:29:45.600]   language of black Twitter. So they're counting some people here and some people there, because
[01:29:45.600 --> 01:29:49.440]   they just don't understand because they're- it's done by the people who don't know what a great
[01:29:49.440 --> 01:29:56.800]   opportunity to learn the language. Yes, but it's also a case where, you know, I've talked about
[01:29:56.800 --> 01:30:02.000]   Andre Brock Jr. on this show before who wrote distributed blackness. And he kind of says,
[01:30:02.000 --> 01:30:07.680]   at some point, we don't need or want you in our Twitter. You've had all the media.
[01:30:07.680 --> 01:30:11.120]   Well, you've got all of this stuff. Let us have ours for God's sakes and stay out.
[01:30:11.120 --> 01:30:18.240]   Can I follow you? Oh, yeah. Yeah. But I don't tweet. I don't want to get in any conversations on
[01:30:18.240 --> 01:30:23.920]   Twitter, but I like to read it. Oh, I do too. I do that. But there's times when, you know,
[01:30:23.920 --> 01:30:28.720]   I've seen people on black Twitter say that they really don't want to be watched. And so maybe
[01:30:28.720 --> 01:30:33.680]   don't do it on Twitter then or, you know, they don't do it on Trump social. Nobody will be there.
[01:30:33.680 --> 01:30:40.400]   Nonetheless, but my point is only that there were also racial nuances beyond the racism.
[01:30:40.400 --> 01:30:47.360]   There were nuances about black women in hair, about black men, about lots of things that
[01:30:47.360 --> 01:30:51.520]   people felt they could comment on, and maybe they weren't the best to comment, and maybe it was
[01:30:51.520 --> 01:30:55.440]   better to sit back and watch and learn. Yeah, that's fine too. That's it.
[01:30:56.800 --> 01:31:03.520]   I welcome reading that as well. I just I feel like we're better off with all of that
[01:31:03.520 --> 01:31:08.080]   information flowing through our heads. If and boy, you really that was perfect Joan,
[01:31:08.080 --> 01:31:14.400]   you continually remind yourself of why do I believe what I believe? Why I know this? Yeah,
[01:31:14.400 --> 01:31:19.520]   why do I know this? Where is this coming from? And our open, traditional information.
[01:31:19.520 --> 01:31:26.160]   Then the opportunity is fantastic. It's fantastic. We've never had a better opportunity
[01:31:26.160 --> 01:31:32.560]   for understanding each other. Well, yeah, and you know, sometimes I think about it and I'm like,
[01:31:32.560 --> 01:31:43.040]   okay, here we have this problem of social media. And we also at this, you know, it starts to arise
[01:31:43.040 --> 01:31:49.120]   right at the same exact time that a movement like Black Lives Matter starts to make a big difference
[01:31:50.320 --> 01:31:58.720]   in the world. And Black Lives Matter is using a lot of social apps and connectivity in order to drive
[01:31:58.720 --> 01:32:11.200]   change about police policies and really is able to activate within hours major networks of people
[01:32:11.200 --> 01:32:17.120]   and change public conversation if there's some kind of crisis happening. At the exact same time
[01:32:17.120 --> 01:32:23.600]   that that starts to happen. You start to get all this worry about social media being probably bad
[01:32:23.600 --> 01:32:29.120]   for the world. But then what you start to look at when you realize, okay, well, what are people
[01:32:29.120 --> 01:32:34.720]   saying is bad about social media? It's not Leo that people are saying, oh my God, it's so great
[01:32:34.720 --> 01:32:39.920]   that we're all connected and we can see so many more opinions and we can have so much more discussion.
[01:32:39.920 --> 01:32:46.800]   That's not the problem. For me, the problem is when Russia pretends to be Black Lives Matter.
[01:32:46.800 --> 01:32:53.520]   And intentionally inflames the conversation and uses the affordances of these tech platforms to do it
[01:32:53.520 --> 01:32:59.760]   to impersonate certain social movements in certain groups of people. And not just Russia,
[01:32:59.760 --> 01:33:07.280]   but now it's, you know, it's kind of like the imposterness of these fake accounts that represent
[01:33:07.280 --> 01:33:13.440]   people that they don't actually know or aren't willing to say that's a problem. Yeah, I'm willing
[01:33:13.440 --> 01:33:18.320]   to say Twitter, there should be a law. Twitter should ban inauthentic accounts or something.
[01:33:18.320 --> 01:33:22.240]   I mean, I'm not against that idea. Twitter can kill all the bots at once.
[01:33:22.240 --> 01:33:30.160]   Yeah, it's not just bots. It's like, or inauthentic accounts. Yeah, in the inauthentic accounts,
[01:33:30.160 --> 01:33:34.560]   when you go back and you look sort of historically, when people were first starting to toy around
[01:33:34.560 --> 01:33:39.440]   with the notion of impersonating a whole community. So beyond just one or two accounts,
[01:33:39.440 --> 01:33:44.960]   we're talking hundreds of accounts at a time pretending to be Black people online.
[01:33:44.960 --> 01:33:49.040]   Yeah. And so, and so that's why the stakes here, I think, are actually,
[01:33:49.040 --> 01:33:56.800]   for me and you are so important where we want to be able to preserve a lot of that conversation,
[01:33:56.800 --> 01:34:04.880]   a lot of that, you know, genuine conversation where society is moving towards more pro-social
[01:34:04.880 --> 01:34:12.160]   goals, more democratic goals. Let's go there. It's the bad actors or politicians, you know,
[01:34:12.160 --> 01:34:20.320]   like PR or political operatives disguised as journalists. You know, these are the people that are really
[01:34:20.320 --> 01:34:28.480]   making a mess of, you know, these platforms. I would submit that one of your most important
[01:34:28.480 --> 01:34:33.840]   skills you have, and certainly I have, is to detect inauthenticity, to have a really good radar
[01:34:34.400 --> 01:34:40.000]   for BS. Except for when it comes to Bitcoin, I'll believe anything about those tricks.
[01:34:40.000 --> 01:34:46.000]   I mean, yeah, like, that's something everybody should work on. I mean, that's a, that's a great
[01:34:46.000 --> 01:34:52.800]   thing, whether it's spam coming in saying, click this link to win a prize or, or a Russian saying,
[01:34:52.800 --> 01:35:00.880]   you know, let's kill Whitey. You should really work on that. That's a good thing to have is that
[01:35:00.880 --> 01:35:06.560]   inauthenticity radar. Let's take it. I think that can be taught too. I think so. And I think we
[01:35:06.560 --> 01:35:11.360]   should. I think these are the things we should work on. I know I'm not the one that says, oh,
[01:35:11.360 --> 01:35:15.040]   you got to teach critical thinking in school. It's much more difficult than that. But I think we
[01:35:15.040 --> 01:35:22.400]   should work on really improving, to understand the opportunity that all this flood of information
[01:35:22.400 --> 01:35:27.920]   provides and improving our ways personally of dealing with it. I mean, maybe that's because
[01:35:27.920 --> 01:35:35.600]   that's what I've embarked on for the last 20 years of my life. In this job is I read a fantastic
[01:35:35.600 --> 01:35:42.480]   amount of news constantly, as I'm sure you do, Joan, and I know you do, Jeff, constant inputs,
[01:35:42.480 --> 01:35:49.200]   and then really trying to use it to massage it so that it's a value to me. That's my job.
[01:35:49.200 --> 01:35:53.200]   And then I can spout it out to you. But you offer that to your audience as well.
[01:35:53.200 --> 01:35:59.040]   That's what I do. That's what I consider my job. Yeah. So you are acting as that institution.
[01:35:59.040 --> 01:36:04.720]   But also I would absolutely think that if anybody does, it always baffles me that people
[01:36:04.720 --> 01:36:09.680]   send me notes saying, you shouldn't say that. Or why do you think that? It's like, all you do is
[01:36:09.680 --> 01:36:16.000]   turn it off. There's a button. Press the stop button and go to another podcast.
[01:36:16.560 --> 01:36:23.600]   There's an infinite. That's what I like. There's an infinitive choice. And maybe open yourself
[01:36:23.600 --> 01:36:28.160]   up to hearing things you don't want to hear or don't like or you disagree with just in case.
[01:36:28.160 --> 01:36:34.960]   Just in case. Anyway, I don't know. I don't know how I got all Pollyanna on this because
[01:36:34.960 --> 01:36:41.040]   it's terrible. It's a horrible world. And we've got to fix it. You know what I believe in? I'll
[01:36:41.040 --> 01:36:47.920]   tell you what I do believe in getting rid of single use plastics. Bad, bad. Let's not fill
[01:36:47.920 --> 01:36:54.240]   the ocean with bottles that we only use once. And let's not deplete the ocean by shipping water
[01:36:54.240 --> 01:36:59.760]   coast to coast when a little tablet will do. I'm talking about Blue Land. I am a
[01:36:59.760 --> 01:37:07.680]   I am a Blue Land fan. Did you know that an estimated five billion with a B plastic hand soap
[01:37:07.680 --> 01:37:14.240]   and cleaning bottles are thrown away each year? Almost all of them used once. And if that's not
[01:37:14.240 --> 01:37:20.080]   bad enough, the liquids in these things are 90% water being shipped around, being stored.
[01:37:20.080 --> 01:37:25.680]   It just doesn't make sense. It's bad for the planet. It's bad for your pocketbook. It's bad for you.
[01:37:25.680 --> 01:37:32.320]   You can stop wasting water. Stop throwing out plastic with Blue Land's revolutionary refill
[01:37:32.320 --> 01:37:37.520]   cleaning system instead. I'm a Blue Land fan. Around the house, we've got Blue Land hand soap
[01:37:37.520 --> 01:37:43.600]   dispensers. I love it. They're made of beautiful solid. They call it Instagramable, but they're
[01:37:43.600 --> 01:37:48.560]   really heavy glass, beautiful glass bottles with a pump on it. And then when it runs low,
[01:37:48.560 --> 01:37:54.720]   there's these little tablets. They send me. I put them in, I had warm water. My water
[01:37:54.720 --> 01:38:01.840]   instead of shipping it across country. And it makes a fantastic hand cleaner, which I have
[01:38:01.840 --> 01:38:07.760]   at every sink in the house. And by the way, I got I probably got too many of them during Christmas.
[01:38:07.760 --> 01:38:13.040]   I bought a bunch of Blue Land Christmas cents. So I smell like gingerbread every night when I go
[01:38:13.040 --> 01:38:19.040]   to bed. But I don't think that's a bad thing. I really don't powder dish soap. It's a great
[01:38:19.040 --> 01:38:23.920]   dish soap. It works really well. It's got baking soda in it as well as detergents. It cleans like
[01:38:23.920 --> 01:38:30.240]   crazy. They've got cleaners for the bathroom, cleaners for the kitchen, incredible sense,
[01:38:30.240 --> 01:38:38.240]   not just gingerbread iris agave, perein lemon, lavender eucalyptus. And they have, I can go on and
[01:38:38.240 --> 01:38:44.000]   on the toilet. This actually they sell out of these the Blue Land toilet tablet cleaners. Just a tablet,
[01:38:44.000 --> 01:38:49.280]   like a little fizzy throw in the toilet and you walk away, go do some other chores. Five minutes
[01:38:49.280 --> 01:38:56.320]   later, you scrub it out, you're done. It is fantastic. It works so well. No waste, no plastic at all.
[01:38:56.320 --> 01:39:04.720]   Comes a little paper packet. They've got hand get the hand soap duo or the clean essentials. This
[01:39:04.720 --> 01:39:09.840]   is a great house warming gift. Great for yourself. Plastic free. We use their laundry and dishwasher
[01:39:09.840 --> 01:39:15.520]   tablets. By the way, I did an experiment. We had the tide pods. We had the cow gun pods.
[01:39:17.040 --> 01:39:21.760]   Said, okay, today, this week, we're only going to use the Blue Land pods. This was some time ago.
[01:39:21.760 --> 01:39:27.440]   Better work better. Love it. Smelled great. Everything was clean. No more cow gun.
[01:39:27.440 --> 01:39:33.600]   No more or whatever those are called. Is it Calgon? What are those? The dishwasher pods? No more
[01:39:33.600 --> 01:39:41.360]   tide pod cascade. Calgons the bath. Bath beads cascade. That's it. I love this stuff.
[01:39:42.480 --> 01:39:47.120]   Okay, I'm sorry. I'm getting a little excited. It's good for the environment. It's good for you.
[01:39:47.120 --> 01:39:51.600]   You buy the bottle once, you refill it forever. That's it on plastic waste.
[01:39:51.600 --> 01:40:02.560]   Eco-friendly products do not work less well. These work better. So don't forget that notion
[01:40:02.560 --> 01:40:06.000]   that, oh, yeah, I'm going to get an eco-friendly detergent and I'm going to walk around with dirty
[01:40:06.000 --> 01:40:12.480]   clothes. No less expensive, more effective. What could possibly be wrong? Look at Blue Land's
[01:40:12.480 --> 01:40:17.440]   stunning high quality forever bottle started just $10. I say buy the kit, get the whole kit and
[01:40:17.440 --> 01:40:22.320]   caboodle, just Blue Land your house. I did that. And now I'm giving it to everybody I know,
[01:40:22.320 --> 01:40:27.040]   we're just giving them way Blue Land because everybody should be using this money saving
[01:40:27.040 --> 01:40:35.120]   refill tablets started $2, $2 and there's no plastics. Once you buy that bottle, it lasts forever
[01:40:35.120 --> 01:40:39.120]   and they're beautiful. By the way, they look actually better without all that advertising and stuff on
[01:40:39.120 --> 01:40:43.680]   it. Try Blue Land today. You'll love it. The planet will thank you. You'll thank you. You'll be glad.
[01:40:43.680 --> 01:40:51.920]   Get 20% off your first order. So make it a big one. Blueland.com/twig. 20% off your first order of any
[01:40:51.920 --> 01:41:07.280]   Blue Land products, blueland.com/twig. Big fan. Big fan. We use all the Blue Land products.
[01:41:07.280 --> 01:41:13.360]   All of them. I'm sorry. I get excited. But I feel good about this one.
[01:41:13.360 --> 01:41:20.800]   I don't know. I think this is fine. Eric Schmidt apparently is basically endowing
[01:41:20.800 --> 01:41:27.600]   President Biden's science office. Is this influence peddling or just more money out of the billionaires
[01:41:27.600 --> 01:41:32.720]   and he gives them some money. Yeah. Schmidt's Foundation helped cover official salaries.
[01:41:32.720 --> 01:41:39.360]   There's ethical flags raised. I don't think there was a quid pro quo. It's the White House
[01:41:39.360 --> 01:41:45.600]   Office of Science and Technology Policy. Many of the people in there come from Google or have
[01:41:45.600 --> 01:41:50.720]   worked with Schmidt. It's 140% office, according to Politico. If he had too much influence,
[01:41:50.720 --> 01:41:56.000]   on who gets paid, that's an issue. By paying. There's a sense on how it was run in the administration.
[01:41:56.000 --> 01:42:03.600]   So it's from the Eric Schmidt's charity, Arms Schmidt Futures. They paid the salaries of two
[01:42:03.600 --> 01:42:08.400]   science office employees, including for six weeks that of the current chief of staff,
[01:42:08.400 --> 01:42:10.480]   who's now one of the most senior officials in the office.
[01:42:10.480 --> 01:42:17.840]   You know, I mean, it's appropriate to look into it. But I think God bless you, Eric Schmidt. Thank
[01:42:17.840 --> 01:42:24.000]   you. Science, nothing wrong with it. We need more information in the White House and government.
[01:42:24.000 --> 01:42:27.600]   Yes, or am I wrong?
[01:42:27.600 --> 01:42:37.520]   So AI policy though, right? Like we got to wonder where Schmidt's interests are and what parts of
[01:42:37.520 --> 01:42:41.680]   the government he's going to fund and what kinds of things that he wants to see
[01:42:42.960 --> 01:42:52.480]   greased as the saying goes. And so here, there's an issue like beyond Schmidt, there's a revolving
[01:42:52.480 --> 01:43:00.160]   door between tech companies and governments. And we see that every time there's a turnover
[01:43:00.160 --> 01:43:04.320]   of president, we see people go into these companies and go back into government. And
[01:43:04.320 --> 01:43:09.520]   there's nowhere else to watch that happen more closely than at the Kennedy School where
[01:43:10.160 --> 01:43:16.000]   like half the faculty didn't hear anytime a Democrat gets elected. And you're just out here being like,
[01:43:16.000 --> 01:43:22.480]   we were supposed to have lunch. And like they're like, I'm moving to DC. And you know, it's, it's,
[01:43:22.480 --> 01:43:26.800]   you know, and so you just you kind of get a mind for what's happening here. But, you know,
[01:43:26.800 --> 01:43:32.640]   with Schmidt, there's been a lot of debate about AI weapons in the future of AI weapons and
[01:43:32.640 --> 01:43:39.600]   where that's all going to go. And we do know that one of his interests is in national security
[01:43:39.600 --> 01:43:46.480]   policy and artificial intelligence. And so while I don't think that this is enough, you know,
[01:43:46.480 --> 01:43:52.000]   pay to play to really make any kind of dent, you know, a couple of employees, you do have to kind,
[01:43:52.000 --> 01:43:59.840]   you do have to ask yourself like, why this particular, you know, agency and not another one. And then
[01:43:59.840 --> 01:44:06.480]   also, you know, who was, who was the one that approached him for the funding and under what
[01:44:06.480 --> 01:44:12.480]   auspicious. And so I wish there was way more transparency and clarity in that sense.
[01:44:12.480 --> 01:44:18.080]   There is a statement on the Schmidt Futures website, which says that US government and the office of
[01:44:18.080 --> 01:44:22.960]   science and technology policy have used pooled philanthropic funding to ensure staffing across
[01:44:22.960 --> 01:44:30.560]   agencies for 25 years. So this is not new. And they said, you know, hey, we're disappointed by the
[01:44:30.560 --> 01:44:34.880]   reporting. We have great respect for the media and for the journalists to report stories, even
[01:44:34.880 --> 01:44:42.000]   those with which we may disagree. But we believe there's no, it's an unsubstantiated claim that we
[01:44:42.000 --> 01:44:50.560]   had any influence. We don't. We're just funding it. So, you know, the best way to fund all this stuff
[01:44:50.560 --> 01:44:58.160]   to tax billionaires appropriately. And then, then you can fund a lot more. So it shouldn't be
[01:44:58.160 --> 01:45:03.680]   voluntary giving. I'm I'm with you in that regard. And it should definitely not be used for lobbying.
[01:45:04.560 --> 01:45:09.680]   We shouldn't we shouldn't. It's a shame that we have to have a bake sale to keep the government's
[01:45:09.680 --> 01:45:15.760]   office of technology and science policy alive. So maybe just tax some of these guys a little bit.
[01:45:15.760 --> 01:45:22.560]   You know, and I should also mention that, you know, the current head of OSTP is a long time mentor of
[01:45:22.560 --> 01:45:28.560]   mine, a long time lesson and someone who I deeply, deeply admire and respect and have I've read all
[01:45:28.560 --> 01:45:35.280]   her books. And she's a sociologist. So she's, you know, one of my type of people.
[01:45:35.280 --> 01:45:43.520]   And very acutely aware of the role that philanthropy she was head of the Social Science Research
[01:45:43.520 --> 01:45:49.360]   Council prior to this appointment. And so she's acutely aware of the influence of philanthropy on
[01:45:49.360 --> 01:45:58.960]   policy and on research. And so as a buttress to that, I'm also very happy that Alondra is,
[01:45:58.960 --> 01:46:05.760]   or I should say Dr. Nelson is in the position that she is in, because I do know that when she's
[01:46:05.760 --> 01:46:12.560]   involved in big projects and and, you know, getting the work done, it's always done with an eye towards
[01:46:12.560 --> 01:46:18.000]   the research and not influence. Yeah. I mean, if I had billions, I don't know what Eric's
[01:46:18.640 --> 01:46:22.800]   wealth is, is something like 20 billion. If I had billions of dollars, I probably would want to,
[01:46:22.800 --> 01:46:28.720]   you know, donate a considerable amount of it towards science policy. That's a good area.
[01:46:28.720 --> 01:46:35.840]   But at the same time, maybe we just tax him instead. And then he doesn't get any say in where the money
[01:46:35.840 --> 01:46:42.400]   goes or in the policies that are generated. It's appropriate for Politico to raise the questions.
[01:46:43.280 --> 01:46:49.120]   I don't feel like there's any evidence of it's not as it's not an open lobbying
[01:46:49.120 --> 01:46:54.000]   effort, which Schmidt also engaged in when he was CEO of Google. So
[01:46:54.000 --> 01:47:00.560]   I mean, billionaires going to billionaire, right? Yeah. People with money are going to buy
[01:47:00.560 --> 01:47:04.560]   influence. They're going to and but they're going to have the thing about philanthropy that I've
[01:47:04.560 --> 01:47:12.720]   in Jeff knows this very well too, is that philanthropists have different motivations. Every one of them
[01:47:12.720 --> 01:47:18.640]   is very unique and there's case scenario and there's and there's sometimes strings and sometimes
[01:47:18.640 --> 01:47:24.000]   they're written down, which is nice. The one of the big benefits being at Harvard is it's really
[01:47:24.000 --> 01:47:28.480]   hard for anybody to lean on me for a little extra work because I'm like, well, we have a contract.
[01:47:28.480 --> 01:47:34.400]   And so look at that. But when it comes to this kind of giving and when people are,
[01:47:34.400 --> 01:47:41.520]   you know, making decisions between do I fund, you know, outside research, do I fund government
[01:47:41.520 --> 01:47:48.400]   research, do I fund this that, you know, there there are concerns about them really trying to fund or
[01:47:48.400 --> 01:47:56.080]   supercharge things that mean a lot to them. And as a result, yeah, you do see deficits that appear
[01:47:56.080 --> 01:48:04.160]   in other areas because philanthropy we get so used to philanthropy that we don't have mechanisms
[01:48:04.160 --> 01:48:10.160]   anymore for relying on our own institutions to make up those gaps and governments and academia
[01:48:10.160 --> 01:48:17.840]   are ones that unfortunately have started to really lean on philanthropy for those, you know, the
[01:48:17.840 --> 01:48:24.400]   missing resources and that comes a lot of it comes with a lot of conversations. And then you
[01:48:24.400 --> 01:48:28.080]   have other funders that are like, I'm going to send you money and like you might never hear
[01:48:28.080 --> 01:48:33.440]   for me again. And that's fine too. And there's also let's face it, government funding also carries
[01:48:33.440 --> 01:48:38.880]   strings. So, but Jones, right, there's good funders and they don't. Yeah, Craig, Craig,
[01:48:38.880 --> 01:48:44.960]   Newmark is a great funder because he really leaves it's clear that you have freedom to do that.
[01:48:44.960 --> 01:48:51.600]   And there's other funders who don't so much. I've been lucky. I haven't had that. But it can happen.
[01:48:51.600 --> 01:49:00.720]   Yeah. And that's why we don't take money from corporations or good. We don't take money or data
[01:49:00.720 --> 01:49:05.360]   from any of the big platform companies. Like, I shouldn't say corporations because if Delta
[01:49:05.360 --> 01:49:10.000]   was over here being like, Joan, we want to fund your research and be like, great, there's no
[01:49:10.000 --> 01:49:14.480]   conflict of interest here. And if you know any air conditioner companies that want to fund
[01:49:14.480 --> 01:49:20.960]   disinfo research, I'm looking. But when it comes to creating conflict of interest with platforms
[01:49:20.960 --> 01:49:27.040]   or someone like Schmidt who was in the tech industry and has a lot of vested interests in the tech
[01:49:27.040 --> 01:49:34.880]   industry moving beyond this moment of AI scrutiny, that's when we have to be, you know, a little wary
[01:49:34.880 --> 01:49:41.440]   is, you know, like, if he made his money and, you know, but if he made his money like, you know,
[01:49:41.440 --> 01:49:46.320]   make an ambulances and like, you know, he was just that guy that's like, well, that's the guy that
[01:49:46.320 --> 01:49:52.240]   made his money build an ambulances. And then you wouldn't question if he was funding OSTP or not,
[01:49:52.240 --> 01:49:59.760]   right? Because it just wouldn't, it wouldn't really matter. So yeah, so that's why I, it raises
[01:49:59.760 --> 01:50:04.960]   my concern level a little bit. But that's tempered by knowing who's at OSTP.
[01:50:04.960 --> 01:50:12.640]   This can bear that there's there's at places like yours in Colombia and Yale that have big
[01:50:12.640 --> 01:50:16.640]   endowments. That's one way to look at the world. And then when you're a public university that's
[01:50:16.640 --> 01:50:20.960]   being starved by the day, it's another way to look at the world. Yeah. Yeah. But I don't get any
[01:50:20.960 --> 01:50:25.920]   money from any of these endowments. Those are building those are for building buildings and like
[01:50:25.920 --> 01:50:32.720]   making the making the handles on the door, the door is shiny, right? Like the doorknobs are
[01:50:32.720 --> 01:50:39.360]   all gold at Harvard. And I'm just like, how do they get so shiny? So they shine. I know, right?
[01:50:39.360 --> 01:50:43.680]   And that's where the poverty of research is actually something that like I've been very
[01:50:43.680 --> 01:50:50.080]   interested in because you're right about public universities even more, more so being in a position
[01:50:50.080 --> 01:50:55.600]   of starving the beast and being unable to raise because there's just not a lot of
[01:50:55.600 --> 01:51:03.760]   infrastructural support for them to do their work anyway. But I was a child of UC San Diego and UCLA.
[01:51:03.760 --> 01:51:13.040]   And so no stranger to that problem as well. Waymo's going fully driverless. Somebody said,
[01:51:13.040 --> 01:51:19.840]   I just imagine there's a ghost driving the car in San Francisco. They will you will be able to
[01:51:19.840 --> 01:51:25.520]   hail a waymo. This is the Google self driving. Wait, can I cancel that? How do we get that to
[01:51:25.520 --> 01:51:32.800]   start? I don't want to go to San Francisco. No, no, no, no, I don't know. I'm going to be like
[01:51:32.800 --> 01:51:37.680]   the first person that's taken out by a zombie car because I step out from behind a trash can or
[01:51:37.680 --> 01:51:45.360]   something. Zombie cars in San Francisco. Leo, you remember Christine? Yeah. Yeah. Okay.
[01:51:45.360 --> 01:51:49.360]   Could be. Could be. Could be. She was a mean car.
[01:51:49.360 --> 01:51:54.400]   Harder haunted car. They don't. The good news is they don't let him drive downtown. Nobody can
[01:51:54.400 --> 01:52:00.320]   drive downtown. Wait, what? They can get on the highway? No. Well, no, they know they just only
[01:52:00.320 --> 01:52:07.840]   can go out in the avenues where it's very nice and orderly. So this sounds so dangerous. Why
[01:52:07.840 --> 01:52:14.160]   do I pay like a 12 year old to drive a golf cart? Probably save who goes from one place to another
[01:52:14.160 --> 01:52:21.600]   in the Richmond. Yeah, exactly. Yeah. I don't know. I don't know. Wemo provided in November
[01:52:21.600 --> 01:52:29.680]   through January. It's not a high volume business. 1,503 rides in that six month period or three
[01:52:29.680 --> 01:52:33.920]   months period, four months period. Doesn't it feel like being on a roller coaster? Can you imagine
[01:52:33.920 --> 01:52:38.240]   sitting in the backseat? I think it's not that exciting. I think they're actually very slow and
[01:52:38.240 --> 01:52:45.280]   patient. They wait a lot. Okay. Yeah, I think I just I just I wouldn't get in. I agree with you.
[01:52:45.280 --> 01:52:51.200]   I'm not makes me nervous. Makes me nervous. How do I know that though? I know that because I watched
[01:52:51.200 --> 01:52:58.160]   a Stephen King movie and I'm like Christine. That's the that's the first cultural arrest
[01:52:58.160 --> 01:53:05.440]   that reference I have to. Let's see. Let's do the change vlog with John. Play those play those
[01:53:05.440 --> 01:53:12.640]   instruments that you have next to you right there. The Google change vlog. Very nice. It's getting
[01:53:12.640 --> 01:53:17.360]   better at the tempehty. Do you like that? That role. That's good. John is so talented.
[01:53:17.360 --> 01:53:30.880]   So talented. Google Docs getting more markdown support. Woo hoo. Woo hoo. Google is going to
[01:53:30.880 --> 01:53:37.440]   let you I don't know do bullets with an asterisk and I like markdown but I don't know if this is
[01:53:37.440 --> 01:53:44.320]   going to be a huge huge. There's no admin control for this feature. The feature is off by default
[01:53:44.320 --> 01:53:50.240]   but you can enable it going to tools preferences turn on markdown. Okay. You wanted the change
[01:53:50.240 --> 01:53:57.680]   vlog. You got the change vlog. Create externally friendly booking pages with appointment scheduling
[01:53:58.880 --> 01:54:04.400]   Google calendar. Don't isn't that externally friendly?
[01:54:04.400 --> 01:54:13.600]   I don't. Okay. Look how pretty that looks. That's great. Going after Calendly.
[01:54:13.600 --> 01:54:19.200]   Yeah. Actually, Lisa just signed up from Calendly. Everybody uses it. Everybody.
[01:54:19.200 --> 01:54:25.280]   So screw it up all the time. Do you? Do you have an account or she's trying to say she has to pay
[01:54:25.280 --> 01:54:31.360]   for an account or not? Yeah. No, I'm not. No. I play hard to get for meetings. Yeah. She has.
[01:54:31.360 --> 01:54:37.520]   She does a lot of meetings. And what Calendly does and what this will do, I guess, is help you
[01:54:37.520 --> 01:54:43.600]   two parties or multiple parties figure out a mutual time for a meeting or a vet or a hangout
[01:54:43.600 --> 01:54:48.000]   or whatever. And I think that that's very much needed in business because people are busy.
[01:54:48.640 --> 01:54:54.480]   So look for that new feature in Google calendar. Nest Hub UI is getting redesigned.
[01:54:54.480 --> 01:55:02.800]   Be careful. This I have one of these. You might note a boot loop on yours. Some displays are getting
[01:55:02.800 --> 01:55:12.320]   into a boot loop stuck. I don't know if let's see if there's a way to fix this. Some Nest Hub
[01:55:12.320 --> 01:55:17.680]   owners today are waking up to a UI redesign. Although the update looks to a boot loop to the
[01:55:17.680 --> 01:55:22.080]   assistant smart display for others is 9 to 5 Google. What do you have the Nest Hub do anymore?
[01:55:22.080 --> 01:55:28.720]   Oh, I use it all the time. You really do. Oh, yeah. Well, it's by my well, I have them in every room.
[01:55:28.720 --> 01:55:37.120]   So I will say, hey, you know who turn off the lights or turn on the lights because we have some
[01:55:37.120 --> 01:55:42.720]   Nest Hue lights or not Nest Hub. He likes it will announce when there's somebody at the door.
[01:55:42.720 --> 01:55:47.360]   They're supposed to say Jeff Jarvis is at the front door so I can pretend no one's home.
[01:55:47.360 --> 01:55:52.240]   But instead it just says someone's at the front door. So but at least I don't miss people at the
[01:55:52.240 --> 01:55:56.480]   front door and then you get a picture on it of the person standing there at the front door.
[01:55:56.480 --> 01:56:03.600]   What else does it do? I use it for alarms. Sometimes I have it play me a particular song to wake me
[01:56:03.600 --> 01:56:09.120]   up, which is nice. Timers, of course, everybody loves timers. You know what I love about it?
[01:56:09.120 --> 01:56:14.960]   I can watch because I am a YouTube TV subscriber when like the hearings are going on
[01:56:16.400 --> 01:56:24.640]   for Katanji. I was I could say, hey, YouTube or I'd say, hey, play CNN on
[01:56:24.640 --> 01:56:31.520]   YouTube TV. And then it would put the video on the audio. It just did a little screen in the
[01:56:31.520 --> 01:56:36.960]   background. That's very handy. There's lots of stuff. There's lots of stuff. I like it.
[01:56:36.960 --> 01:56:42.160]   Line is a very expensive clock. Well, that's the thing. You got to kind of force yourself to
[01:56:42.160 --> 01:56:48.240]   use it. There's a new magic wand button on the Google G board keyboard that will let you
[01:56:48.240 --> 01:56:57.520]   emojify what you're typing. So type the emojify key to add emojis within a sentence or replace
[01:56:57.520 --> 01:57:03.360]   all text with emojis. See the magic sparkle wand? Oh, God, I better that accidentally.
[01:57:03.360 --> 01:57:09.360]   Yeah. And then you send a completely unintelligible emoji laden message.
[01:57:10.240 --> 01:57:15.360]   I think that's cool. It inserts emojis that are relevant to your message.
[01:57:15.360 --> 01:57:23.840]   We'll see. We'll just see. You know, the kids are so good about just getting the emojis in there.
[01:57:23.840 --> 01:57:29.200]   I bet you Jones real good to use emojis, John. I invented an emoji. Did I tell you this? The
[01:57:29.200 --> 01:57:34.880]   beaver. I'm the beaver emoji. That's that's me and a few other folks who are co-creators. And
[01:57:34.880 --> 01:57:40.880]   it was a very strange process. But here we are. I made an emoji and it's in everybody's telephone,
[01:57:40.880 --> 01:57:45.200]   the beaver. So from is it with pride? Is this in your Wikipedia profile?
[01:57:45.200 --> 01:57:52.480]   It's in funny. It's on my Twitter bio. But I don't I never like, you know, I don't know anybody that
[01:57:52.480 --> 01:57:58.960]   edits my Wikipedia. So it's like I need someone to go in there and add it, you know. So Jeff Jarvis,
[01:57:58.960 --> 01:58:04.800]   why don't you go in and get me get me some respect of the beaver emoji. And from now on,
[01:58:04.800 --> 01:58:09.520]   when I tell Gboard beaver beaver beaver, it's going to put your beaver in there.
[01:58:09.520 --> 01:58:16.880]   It is. It is. I'm excited. All that implies. Okay. So, you know, people don't know that I'm like,
[01:58:16.880 --> 01:58:22.880]   I'm like a crazy joker. Like I love like a good prank. I like, I love all the double.
[01:58:22.880 --> 01:58:28.960]   Well aware of the double entendre. Okay. Oh, yeah, I was here for this. Like, I was like, this is
[01:58:28.960 --> 01:58:37.120]   emoji is made from me. And so, yeah, so I just think it's hilarious. I want to make, like,
[01:58:37.120 --> 01:58:41.920]   really fancy polos with it with the little beaver emoji on the side, you know, like the
[01:58:41.920 --> 01:58:48.320]   alligator or polo shirts and, you know, and get people to wear them. I think I think some PJs
[01:58:48.320 --> 01:58:52.400]   with it all around would be nice too. Yeah, yeah, that would be it too. If I was like an
[01:58:52.400 --> 01:58:57.600]   entrepreneur, like if I really had my stuff together, or a Chan, you'd be making the buttons now.
[01:58:57.600 --> 01:59:01.840]   Yeah, I know I'd be doing it. I'd be all over this stuff. But unfortunately, I've got this like
[01:59:01.840 --> 01:59:07.760]   job that just doesn't let me go to bed. And so, but if I were in the merch game,
[01:59:07.760 --> 01:59:09.520]   the beaver emoji would be on everything.
[01:59:09.520 --> 01:59:19.600]   The Google Chrome version 100 came out with a bunch of fixes fireworks. Yeah, for zero days
[01:59:19.600 --> 01:59:26.640]   and stuff. They also a new redesign for the launcher rolling out to Chromebooks. Jeff, you should
[01:59:26.640 --> 01:59:31.600]   look for this, a floating pane at the left that doesn't take up your entire screen. So you have
[01:59:31.600 --> 01:59:37.120]   more space. So it's kind of more like these menus you see on Windows and other operations.
[01:59:37.120 --> 01:59:42.960]   It's changed? I don't know if I like change. I know. I know. Voice dictation will support
[01:59:42.960 --> 01:59:50.240]   edit commands. That's really great. So you can say delete, which will delete the last letter.
[01:59:50.240 --> 01:59:55.040]   You can say move to next character, which moves the cursor along. You can say help.
[01:59:55.040 --> 01:59:58.640]   So play with some of those commands and let us know how you like those.
[01:59:58.640 --> 02:00:04.960]   I added a few things in there. A return to the office. I'm having a feeling you added this one.
[02:00:04.960 --> 02:00:14.480]   Is a pain in the ass. Google is eliminating the bidets in their bathrooms much to the dismay of
[02:00:14.480 --> 02:00:22.560]   employees. They had those toto toilets. But it turns out they're not in compliance with
[02:00:22.560 --> 02:00:26.640]   California state code for commercial buildings. California is no fun.
[02:00:26.640 --> 02:00:36.240]   Yeah, that's weird. One of the reasons evidently is they're going to go to recycled water and you
[02:00:36.240 --> 02:00:43.520]   don't want that being shot up. Oh, no, no, no, no, no, no, no. When an employee challenged the bidet
[02:00:43.520 --> 02:00:50.640]   removal decision, citing environmental and hygienic benefits of bidets, facilities professional,
[02:00:51.600 --> 02:00:58.240]   Edgar Tovar responded, Google's not going to replace the bidet. The bidet seat removal has the added
[02:00:58.240 --> 02:01:03.440]   benefit of supporting Google's water sustainable commitments, recycled water systems, which don't
[02:01:03.440 --> 02:01:11.120]   work well in bidets. You know, once you get used to these heated seats and the little squirt of water
[02:01:11.120 --> 02:01:18.480]   to clean your beavers, she about. Yeah, you don't want to go back. You don't want to go back.
[02:01:19.360 --> 02:01:25.920]   Google Docs can write your meta descriptions. You can, but this is kind of interesting. Well,
[02:01:25.920 --> 02:01:31.520]   I don't know how you could use this for your papers to do the abstract. Oh, it would be if it was
[02:01:31.520 --> 02:01:42.240]   written at a, you know, I had like a six grade reading level. No, come on, Jeff. Come on. Come on.
[02:01:42.240 --> 02:01:47.520]   I'm least obscure. You're not mine, of course, of course. Oh, no, no, no. But I'll tell you,
[02:01:48.880 --> 02:01:55.040]   I can imagine students using this and it creating lots of hilarious mistakes.
[02:01:55.040 --> 02:02:02.000]   And so I bring this on with pleasure. Like, let's, let's get this done.
[02:02:02.000 --> 02:02:05.760]   Danny, good one writing in search engine. Lamb gave some examples. And actually, these are
[02:02:05.760 --> 02:02:13.200]   pretty good. So Danny took this article, Facebook warns publishers to avoid watch bait tactics,
[02:02:13.200 --> 02:02:17.600]   these tactics will result in fewer recommendations. He took it, ran it through the Google,
[02:02:18.560 --> 02:02:24.400]   uh, Synopsizer. And Google said this article is about how to avoid watch bait in your videos.
[02:02:24.400 --> 02:02:28.800]   It's important to know what Facebook considers watch bait and what you should do to avoid it.
[02:02:28.800 --> 02:02:35.120]   Little tautological, but okay. Circular. Yeah. Yeah. The second one was better.
[02:02:35.120 --> 02:02:45.520]   TikTok testing search ads. Um, and Google's sub, uh, Synopsize that with this is a big deal.
[02:02:45.520 --> 02:02:51.280]   TikTok is testing search ads and it's a big opportunity for brands to attract qualified traffic.
[02:02:51.280 --> 02:02:58.480]   shouldn't use big twice. It's kind of colloquial. This is a big deal. Yeah. Yeah. Kind of colloquial.
[02:02:58.480 --> 02:03:02.800]   Kind of cute. All right. I'll be playing with that. I just found that.
[02:03:02.800 --> 02:03:10.000]   This, this would get you like a C in my class. Yeah. Like, you can't use big deal. This is a big deal.
[02:03:10.000 --> 02:03:16.960]   It's just come on. Hey, Professor, this is a big deal. Yeah. Oh, yeah. Well, you know,
[02:03:16.960 --> 02:03:22.800]   I'll tell you what I taught at UC San Diego. Students would come to class like in board shorts with
[02:03:22.800 --> 02:03:28.800]   their, of course, literally their surfboard flip flops and professor Donovan. Hey,
[02:03:28.800 --> 02:03:34.720]   Hey, I just got back from the beach and I'm just like, can you at least pretend that you were
[02:03:34.720 --> 02:03:42.080]   like one dude. I'm studying, dude. I got this. Don't worry. I got this, dude.
[02:03:42.080 --> 02:03:50.240]   Can I have an extension? I heard my list. Oh man. The shark bit me. The shark.
[02:03:50.240 --> 02:03:57.520]   I'm like, Oh man. I mean, I long for those days though, to be honest with you. Like, I like looking
[02:03:57.520 --> 02:04:03.360]   back. I'm like, Oh my God. This was like, like, this could have been good TV. Like with the way
[02:04:03.360 --> 02:04:08.400]   students would come into class and just like, you know, kind of like wind swept and just like,
[02:04:08.400 --> 02:04:13.440]   Oh, this like, I'm just coming in from the beach. And it was kind of like being in Save by the Bell.
[02:04:13.440 --> 02:04:18.160]   I think they did that show. Yeah. Save by the Bell or welcome back. Kata. Yeah.
[02:04:18.160 --> 02:04:25.200]   Vinnie Barberino. Kana. That was like New York City though. Yeah. Oh, oh, Mr. Kata. Mr. Kata.
[02:04:25.200 --> 02:04:28.880]   That's the Google change law.
[02:04:32.320 --> 02:04:34.400]   We're going to take a break and your picks of the week.
[02:04:34.400 --> 02:04:39.040]   Joan, you already used your book. So you don't have to if you don't want to, but your picks of
[02:04:39.040 --> 02:04:44.400]   the week are coming up. If you want to think of something else, I have a very cool pick. And
[02:04:44.400 --> 02:04:48.640]   actually, we didn't do any of the silly stuff that I was going to do. Because there's a whole
[02:04:48.640 --> 02:04:52.720]   bunch of silly stuff. What appealed to you with the silly stuff? Did you put the one in there with
[02:04:52.720 --> 02:05:00.320]   a guy created the Feed Me machine? Yeah. Is that? Yeah, I did. Yeah. I like that. We can do that one.
[02:05:00.320 --> 02:05:03.200]   Yeah. Should we just do that before we go to the break? Sure.
[02:05:03.200 --> 02:05:07.120]   We just some crazy stuff like. You got to describe it for the folks on
[02:05:07.120 --> 02:05:11.840]   like Dyson making air purifying headphones like that.
[02:05:11.840 --> 02:05:20.640]   Look, we're going to cost like a thousand dollars or Scottie GB who invented this conveyor belt that
[02:05:20.640 --> 02:05:29.200]   feeds me a five meal course. So it's through a napkin onto it with it with a plastic toy shovel,
[02:05:29.200 --> 02:05:35.600]   sandbox shovel. Now it's feeding him a bowl of soup by opening a book, which is kind of interesting.
[02:05:35.600 --> 02:05:41.360]   And then he's got a little rubber hand with a napkin on it and a hairdryer that's going to get
[02:05:41.360 --> 02:05:48.640]   this is my favorite one. It blows salad into his mouth as it goes by as it goes by. It's actually
[02:05:48.640 --> 02:05:54.000]   quite well designed. Notice he's got these little structures that go up and down to let things come
[02:05:54.000 --> 02:06:01.120]   in here. Okay, now I don't know what is Oh, I don't know what that was at a zucchini.
[02:06:01.120 --> 02:06:04.720]   Something got fired into his mouth. This is one of my favorite. Here comes corn in the cob.
[02:06:04.720 --> 02:06:09.680]   He's got a little toy truck rolled in the butter rolled in the butter and then it's feeding it to
[02:06:09.680 --> 02:06:18.160]   spinning around. But the best is yet to come dessert dessert. By the way, he's got to chew mighty
[02:06:18.160 --> 02:06:24.640]   fast because this conveyor belt. It's like working for a Ford in the 1920s. It moves fast. Oh, slicing up.
[02:06:24.640 --> 02:06:32.320]   And what is that dog? I don't know what this is. It's got sliced up. And now he's little pieces of
[02:06:32.320 --> 02:06:39.920]   something. He's going to die. This looks dangerous. What are the best is yet to come. Oh my God,
[02:06:39.920 --> 02:06:45.680]   more napkins at least he's keeping his mouth clean. What are you making me watch? And now,
[02:06:46.640 --> 02:06:52.880]   Joan, this is what the kids are doing. This is an eclair. This is a grown in man. This is not a
[02:06:52.880 --> 02:06:59.440]   child. It's a grown ass man getting the views on the beer. Yeah, he's a liar. He's a liar. Yeah,
[02:06:59.440 --> 02:07:05.120]   with a caboose. Oh my God. Now he puts his glasses on. How did he even enjoy that? He didn't.
[02:07:05.120 --> 02:07:10.000]   And he's reading his newspaper also on the conveyor belt.
[02:07:12.240 --> 02:07:16.960]   That is ridiculous. Joseph machines credit credit to where credits do.
[02:07:16.960 --> 02:07:23.040]   Joseph machines. The kids. The the the the toddlers are going to watch that and they're and their
[02:07:23.040 --> 02:07:28.240]   dining habits are going to go just downhill. Then they're going to start kicking on doors.
[02:07:28.240 --> 02:07:32.240]   It's just a matter of time. Yeah, I know, right? I just want I'm just curious.
[02:07:32.240 --> 02:07:38.160]   He's got eight hundred and five thousand followers, eleven million likes that got 4.7
[02:07:38.160 --> 02:07:43.920]   million views. So that's a that's probably his biggest video ever. Very good. He's got a mask
[02:07:43.920 --> 02:07:49.760]   one. He's got all kinds. He's got one on the five to the right. Yeah. Yeah. Who else gets their
[02:07:49.760 --> 02:07:55.680]   mask? He's look at his house is loaded with these gidge muzzin gadgets. Oh, geez. He's obviously
[02:07:55.680 --> 02:08:01.280]   so he's got a lot of you. Oh my goodness. It's a little mask. That's good. Master's hanging there
[02:08:01.280 --> 02:08:06.480]   a little mask machine. Couple clips and just goes on to his bones. He walks out. Love it. See?
[02:08:07.040 --> 02:08:11.840]   See, this is learnings from TikTok. Things you'll learn things you'll learn on TikTok.
[02:08:11.840 --> 02:08:21.120]   This should go in the crazy file. ExxonMobil is going to get into Bitcoin mining with its ex
[02:08:21.120 --> 02:08:26.560]   there. There you go, Joan. Excess methane. There we go. It's got to be fake news. This got to be
[02:08:26.560 --> 02:08:32.560]   this is too perfect. Oil Bitcoin mining. Yeah. Come on. How do I know that? Yeah.
[02:08:33.120 --> 02:08:37.680]   Like, come on. It's from the New Republic. It's from the New Republic because oil drilling isn't
[02:08:37.680 --> 02:08:44.080]   destructive enough. ExxonMobil is getting into Bitcoin mining to cryptocurrency climate and Russia's
[02:08:44.080 --> 02:08:51.840]   invasion of Ukraine starting to intersect in strange ways. It's right outside the Bakken
[02:08:51.840 --> 02:08:57.520]   shale formation in North Dakota. I'm thinking they have a lot of excess natural gas and they're
[02:08:57.520 --> 02:09:02.160]   just instead of burning it, which is not a very good thing to do. They're going to burn it and
[02:09:02.160 --> 02:09:09.360]   make Bitcoin, which is not much better. No. Come to think of it. So what you're telling me is this
[02:09:09.360 --> 02:09:18.800]   money, which is made out of computers, is like worth something in this world. Like, I can't wrap my
[02:09:18.800 --> 02:09:23.760]   head around. I also can't wrap my head around how a piece of paper that is green and has dead
[02:09:23.760 --> 02:09:28.560]   presidents on it is also. Yeah. It's exactly the same. There's some kind of magic happening here,
[02:09:28.560 --> 02:09:37.040]   but also at least I can hold money in my hands. No. You go down to Times Square. There's a guy who
[02:09:37.040 --> 02:09:41.360]   will sell you Bitcoin. There's a guy who will sell you. It's just as Chuck E. Cheese's Bitcoin.
[02:09:41.360 --> 02:09:44.240]   Yeah. You can hold it in your hand and everything. You're going to love it. Yeah.
[02:09:44.240 --> 02:09:52.160]   Tangibles. It's just tangible. Our show today brought you, actually, this is relevant by Our
[02:09:52.160 --> 02:10:00.080]   Crowd, which is a venture capital firm that decided to open the gates, let you peek inside and get
[02:10:00.080 --> 02:10:05.760]   involved if you want to all around the world. They're amazing tech companies doing incredible
[02:10:05.760 --> 02:10:10.880]   things, driving returns for investors. But you know, you might say, well, but yeah, but you got to
[02:10:10.880 --> 02:10:18.800]   be a billionaire to do it. Nope. Nope. From personalized medicine to cybersecurity to higher education,
[02:10:19.600 --> 02:10:26.960]   where students sacrifice $3.8 billion in earnings every year by dropping out. Our crowd is identifying
[02:10:26.960 --> 02:10:32.320]   innovators that want to make the world a better place and that you can invest in. And you can
[02:10:32.320 --> 02:10:36.160]   invest in when the potential growth is the greatest, not after everybody knows about them,
[02:10:36.160 --> 02:10:43.680]   not after the IPO, no before the exit when they're just getting started. Our crowd is the fastest
[02:10:43.680 --> 02:10:49.520]   growing venture capital investment community. You do have to be an accredited investor to do this.
[02:10:49.600 --> 02:10:54.240]   And that's for your protection. They don't want you betting the rent money. So what you should do
[02:10:54.240 --> 02:11:01.600]   right now is go to our crowd.com/twig. And you tell them what country you're in and then you'll
[02:11:01.600 --> 02:11:06.320]   see what the requirements are in that country. It's different in every country to be at what they
[02:11:06.320 --> 02:11:12.400]   call an accredited investor. Our crowd's accredited investors have already invested over a billion
[02:11:12.400 --> 02:11:16.800]   dollars in growing tech companies. Twenty one of their portfolio companies are unicorns,
[02:11:17.680 --> 02:11:25.120]   40 IPOs or exit sales of portfolio companies have met some nice returns for investors.
[02:11:25.120 --> 02:11:30.800]   Now you can do it a couple of different ways. They have a fund. Actually, they have several funds.
[02:11:30.800 --> 02:11:35.760]   You could put it as, again, those for as little as $50,000. You could participate in single company
[02:11:35.760 --> 02:11:41.200]   deals for $10,000. So that's the minimum that you'll need to invest. This is all again, you know,
[02:11:41.200 --> 02:11:47.920]   this shouldn't be your first thing that you do to grow your wealth. This should be something
[02:11:47.920 --> 02:11:52.800]   after you've done all the other things and you want to just try a little bit of venture investing.
[02:11:52.800 --> 02:11:57.920]   Investment terms will vary depending on where you invest. So do go to the site, input the country
[02:11:57.920 --> 02:12:01.760]   you're investing from. But we were talking about education. In fact, it's right at the top there,
[02:12:01.760 --> 02:12:07.520]   John, on the website, you can invest in something called edu-nav, whose patented technology uses
[02:12:07.520 --> 02:12:14.080]   machine learning and combinatorial algorithms to guide students along the optimal path to graduation.
[02:12:14.080 --> 02:12:21.040]   Edu-nav is already in use by 20 colleges and universities. In fact, one college doubled their
[02:12:21.040 --> 02:12:28.640]   graduation rate, thanks to it. And you can invest in it today, early stage at our crowd. It's just
[02:12:28.640 --> 02:12:34.400]   an example of the kinds of deals our crowd brings their accredited investors. Now here's the beauty
[02:12:34.400 --> 02:12:42.880]   part, it costs you nothing to join. Invest in edu-nav at our crowd.com/tweg that's O-U-R-C-R-O-W-D
[02:12:42.880 --> 02:12:49.440]   our crowd.com/tweg. By the way, you can join for free, get all this information. It's fascinating.
[02:12:49.440 --> 02:12:59.440]   Invest if and when you're ready at O-U-R-C-R-O-W-D.com/tweg. Our crowd. Join the fastest growing venture
[02:12:59.440 --> 02:13:06.640]   capital investment community, our crowd.com/tweg. Thank you, our crowd, for supporting this week in
[02:13:06.640 --> 02:13:12.880]   Google. I like this because it democratizes deal flow, something that normally normal people just
[02:13:12.880 --> 02:13:18.080]   don't get access to. You have to be in the know, you have to be on Sandhill Road to know about these
[02:13:18.080 --> 02:13:23.600]   early stage companies. Of course, the risks involved absolutely read all about it at the website.
[02:13:24.160 --> 02:13:29.760]   But I think it's a very interesting way to invest that extra that you might have.
[02:13:29.760 --> 02:13:36.720]   Our crowd.com/tweg. We thank you so much for supporting this week in Google.
[02:13:36.720 --> 02:13:42.480]   So, Joan, you were very generous. You gave away your pick of the week earlier. Do you have
[02:13:42.480 --> 02:13:48.800]   something to replace it or do you want to recuse yourself? You can skip. You can skip. No, I want to
[02:13:50.400 --> 02:13:58.080]   mention the correct title of the book, which is Off the Edge, Flat Earther's Conspiracy Culture,
[02:13:58.080 --> 02:14:05.280]   and Why People Will Believe Anything. Kelly Wheel is a reporter at The Daily Beast.
[02:14:05.280 --> 02:14:12.000]   It's been someone that has been on the conspiracy beat, the internet beat, as well as the white
[02:14:12.000 --> 02:14:17.840]   supremacy beat for many, many years. So, when she told me a while back that she was going to write
[02:14:17.840 --> 02:14:22.160]   a book on Flat Earther, I just thought, "This is going to be a wild ride." This is for you.
[02:14:22.160 --> 02:14:26.480]   Yeah, it was like, I felt like she wrote a book for me. I was like, "This is great. This is
[02:14:26.480 --> 02:14:32.960]   stuff I want to know about, but I don't have time to look into." And it really is, she treats people
[02:14:32.960 --> 02:14:39.680]   with respect, but she is very skeptical. She never shows up to these conferences with Flat Earther's
[02:14:39.680 --> 02:14:46.480]   pretending to believe, which is good. And I think that people really enjoy the book. So, I just want
[02:14:46.480 --> 02:14:53.280]   to make sure I got the title right. Flat Earther's Conspiracy Culture, and Why People Will Believe
[02:14:53.280 --> 02:15:00.000]   Anything. Certainly worth knowing Kelly Wheel, W-E-I-L-L, Off the Edge is the name of the book,
[02:15:00.000 --> 02:15:05.840]   Off the Edge. I have had it on my wish list for some time on Audible, and I was kind of on the
[02:15:05.840 --> 02:15:10.160]   fence, so now I will buy it. Absolutely great. Listen, especially if you have a car trip,
[02:15:10.160 --> 02:15:16.720]   yeah, getting the copy of the car. It's in Cajun, that kind of stuff. Jeff, do you have a number?
[02:15:16.720 --> 02:15:22.000]   I do. I'm going to save one of these. I'm going to try to test it out before next week's show,
[02:15:22.000 --> 02:15:28.400]   so I'll give you this one instead. Netflix just did a celebratory post that five years ago,
[02:15:28.400 --> 02:15:35.920]   they came up with the Skip Intro button. They invented that, huh? They did. They found that 15%
[02:15:36.720 --> 02:15:41.520]   of the time members were manually advancing the series within the first five minutes.
[02:15:41.520 --> 02:15:46.000]   I've told them people didn't want the intro. It's just the theme song, and who's in this,
[02:15:46.000 --> 02:15:50.480]   and all that stuff. The credits, and the big models of the product. I always watch it the first
[02:15:50.480 --> 02:15:57.680]   episode, and there's some I love. Severance has an amazing open, and then the next time I skip it,
[02:15:57.680 --> 02:16:05.920]   because I also skip the previously, I skip that too. Go right to the me. So, in the net,
[02:16:05.920 --> 02:16:11.520]   in five years, the button has been pressed 136 million times, saving members and astonishing
[02:16:11.520 --> 02:16:17.520]   195 years in cumulative time. See, it's a little thing.
[02:16:17.520 --> 02:16:24.240]   Technology. It makes a big difference. Everybody does it now, right? In fact, I get mad when I
[02:16:24.240 --> 02:16:28.960]   don't have a Skip Intro button. It makes me mad. I want to get right to the show.
[02:16:28.960 --> 02:16:30.960]   But when you're bingeing-- But did you a book you want to?
[02:16:31.520 --> 02:16:35.040]   Do you remember when you had to wait till like Tuesday at nine o'clock?
[02:16:35.040 --> 02:16:43.120]   Can you even imagine? But you know, the one thing I miss, all this net culture, all this instant
[02:16:43.120 --> 02:16:48.480]   access, I don't browse nothing no more. I really miss that. I used to love the jazz.
[02:16:48.480 --> 02:16:51.440]   I wouldn't even know how to browse. Yeah.
[02:16:51.440 --> 02:16:55.840]   Magazines were browsable medium, right? And then the internet came all killed browsing,
[02:16:55.840 --> 02:17:00.000]   and then everybody abandoned browsing, and we're now left with the stores. I used to go to bookstores,
[02:17:00.000 --> 02:17:04.000]   record stores to browse. That's all gone. Browsing is dead. I guess--
[02:17:04.000 --> 02:17:04.960]   Yeah.
[02:17:04.960 --> 02:17:08.080]   I guess the irony, of course, they call them browsers.
[02:17:08.080 --> 02:17:13.200]   When I go to like a hacker news or something, that's in a way it's browsing. It's a collection of links.
[02:17:13.200 --> 02:17:16.400]   It's kind of like browsing, right? Yeah.
[02:17:16.400 --> 02:17:20.880]   Well, Joan, I think in the beginning, we did browse to the very, very beginning. We could go
[02:17:20.880 --> 02:17:24.480]   through all these rat holes. Look, I spent all time following links.
[02:17:24.480 --> 02:17:27.680]   Yeah, but is that where your entire specialty started?
[02:17:27.680 --> 02:17:30.800]   But nobody links out. It's where the trouble came. It drives me crazy.
[02:17:30.800 --> 02:17:35.520]   You'll be reading the Verge or something. There's a link that says Bitcoin.
[02:17:35.520 --> 02:17:41.360]   You click it. It's just another Verge article. That's not browsing. That's staying at home.
[02:17:41.360 --> 02:17:48.320]   I'll tell you, though, people do record shop these days, and my friend runs a record store
[02:17:48.320 --> 02:17:53.680]   up in Gloucester, Massachusetts. And they're doing better than ever. And in the pandemic,
[02:17:53.680 --> 02:17:58.720]   put a little dent in the business. But people do want material now.
[02:17:58.720 --> 02:18:02.880]   And I often wonder, how are you going to know? You go to somebody's house and you're looking
[02:18:02.880 --> 02:18:07.200]   around at their place and they got no DVDs, they got no records, they got no CDs.
[02:18:07.200 --> 02:18:10.960]   How do you know that they're normal? How do you know who they are?
[02:18:10.960 --> 02:18:15.280]   How do you look at to figure out? What do you look at? It's weird.
[02:18:15.280 --> 02:18:19.840]   Vinyl record sales were at their highest last year in 30 years.
[02:18:20.880 --> 02:18:28.160]   Huge comeback. 23% of all albums bought were on vinyl, not CD. Do you want to know what the
[02:18:28.160 --> 02:18:35.040]   number one vinyl album was? No, let me guess. Let me guess. I bet it was probably a Fleetwood
[02:18:35.040 --> 02:18:39.920]   Mac reissue. You're close. It was Abba's comeback record.
[02:18:39.920 --> 02:18:43.040]   Whoa, my. There we go. And there they are, Abba.
[02:18:43.040 --> 02:18:49.120]   They're back, baby. That's it. That's it.
[02:18:50.480 --> 02:18:54.320]   I have a pick that I don't usually let you guys do the picks, but this one
[02:18:54.320 --> 02:18:57.600]   just got me so excited. You actually used to do them too.
[02:18:57.600 --> 02:19:03.440]   And then you got lazy Leo. I have picks I could do, but I don't want to make the show go on any
[02:19:03.440 --> 02:19:08.240]   longer than it has to. Remember, I'm sitting on a stick. And it's the end of the week.
[02:19:08.240 --> 02:19:13.280]   And it's the end of the week. This is the Museum of Endangered Sounds.
[02:19:13.280 --> 02:19:19.760]   SavetheSounds.info. I love this idea. You would. You would. Pick a sound.
[02:19:19.760 --> 02:19:23.040]   There's all these sounds out of a telephone rotary dial going down.
[02:19:23.040 --> 02:19:26.560]   Oh my god, please. Ring that phone. That's my ring.
[02:19:26.560 --> 02:19:31.040]   God. Yeah. I'm going to spend all night on this website. You just ruined my night, Leo.
[02:19:31.040 --> 02:19:35.360]   Typewriter. Yeah. Yeah. Yeah. That one got matrix printer.
[02:19:35.360 --> 02:19:40.320]   Oh. Punching that paper. Good afternoon.
[02:19:40.320 --> 02:19:45.920]   Afternoon. This day, lifetime will be close. There's making like techno music.
[02:19:45.920 --> 02:19:50.720]   You can actually loop it all. So it will.
[02:19:50.720 --> 02:19:56.800]   This day, lifetime will be 12, 30, and 26.
[02:19:56.800 --> 02:19:57.680]   Please give me a while.
[02:19:57.680 --> 02:19:58.880]   Okay. Now my brand is.
[02:19:58.880 --> 02:20:06.880]   Where's the actual modem sound that's like that's wrong?
[02:20:06.880 --> 02:20:09.520]   That's here. Wait, let me stop all this other stuff.
[02:20:09.520 --> 02:20:10.640]   Yeah, stop all that.
[02:20:10.640 --> 02:20:12.640]   There is a modem sound. Mainline me.
[02:20:12.640 --> 02:20:17.840]   And modem. Yeah. No, that's that's euro signal. I don't know.
[02:20:17.840 --> 02:20:20.720]   I'm not a euro person. So I don't know that one. Let's see.
[02:20:20.720 --> 02:20:23.120]   I want the like, I want this.
[02:20:23.120 --> 02:20:29.520]   To modem. You know, 56 K. That's it. Up up. Go up to that AOL logo.
[02:20:29.520 --> 02:20:32.000]   I'm sure that's it. This. That's your middle.
[02:20:32.000 --> 02:20:34.480]   Oh, no, you're right.
[02:20:34.480 --> 02:20:37.520]   No, this is it. I remember that screen from when I was a kid.
[02:20:37.520 --> 02:20:39.680]   No. Fine.
[02:20:39.680 --> 02:20:42.960]   I mean, act.
[02:20:42.960 --> 02:20:45.520]   Comfort. Comfort.
[02:20:45.520 --> 02:20:46.880]   You heard that last bit of.
[02:20:46.880 --> 02:20:50.480]   Oh, yeah. It's gone through. It's gone through.
[02:20:50.480 --> 02:20:56.000]   We're sorry. Your call cannot be completed as dialed from the phone.
[02:20:56.000 --> 02:20:58.640]   Here's one I bet you you had, Joe, when you were a kid.
[02:20:58.640 --> 02:20:59.680]   A Tamagotchi.
[02:20:59.680 --> 02:21:01.280]   Oh, yeah.
[02:21:01.280 --> 02:21:05.520]   I let that thing die.
[02:21:05.520 --> 02:21:06.240]   We love the erudary house.
[02:21:06.240 --> 02:21:07.120]   Thousand deaths.
[02:21:07.120 --> 02:21:07.600]   The bottom.
[02:21:08.640 --> 02:21:11.440]   Now we're all we're going to do this for hours. I'm sorry.
[02:21:11.440 --> 02:21:14.000]   Well, unfortunately, there aren't that many. I wish there were more.
[02:21:14.000 --> 02:21:19.920]   This guy is a freshman at Chattanooga State Community College, which means he doesn't remember
[02:21:19.920 --> 02:21:24.800]   any of these sounds, but you know, he's trying to keep them alive forever.
[02:21:24.800 --> 02:21:25.760]   Good for him.
[02:21:25.760 --> 02:21:26.720]   Good for him.
[02:21:26.720 --> 02:21:27.760]   Him as your employees.
[02:21:27.760 --> 02:21:28.800]   I love everything.
[02:21:28.800 --> 02:21:30.800]   Where's the cassette? You want to hear the cassette?
[02:21:30.800 --> 02:21:31.760]   Oh, yes, I do.
[02:21:31.760 --> 02:21:33.040]   Music of volume two.
[02:21:33.040 --> 02:21:36.160]   Oh, yeah.
[02:21:37.760 --> 02:21:40.400]   That's a I guess it's a blank cassette.
[02:21:40.400 --> 02:21:42.560]   It's one year. I think it's rewinding.
[02:21:42.560 --> 02:21:44.560]   Oh, yes. Rewinding.
[02:21:44.560 --> 02:21:49.680]   There's also a turntable for those who don't remember turntables somewhere in here.
[02:21:49.680 --> 02:21:53.520]   Oh, well, here's a floppy disk.
[02:21:53.520 --> 02:21:56.560]   Oh, yeah.
[02:21:56.560 --> 02:21:57.360]   See you're on.
[02:21:57.360 --> 02:21:57.760]   That's subtle.
[02:21:57.760 --> 02:21:58.960]   That's subtle.
[02:21:58.960 --> 02:22:01.840]   Oh, when a CD skips, remember that?
[02:22:01.840 --> 02:22:03.040]   Oh, my God.
[02:22:03.040 --> 02:22:04.000]   Oh, the worst.
[02:22:04.000 --> 02:22:04.560]   The worst.
[02:22:04.560 --> 02:22:10.880]   So the other day I was listening to a CD that I had made into MP3s when I was way younger,
[02:22:10.880 --> 02:22:12.880]   maybe 20 years ago at this point.
[02:22:12.880 --> 02:22:17.360]   And there was a skip in the CD, and I hadn't listened to these MP3s in forever.
[02:22:17.360 --> 02:22:20.480]   And there's a skip in the MP3, and I like couldn't believe it.
[02:22:20.480 --> 02:22:23.680]   In my mind, I was like, and then I remember I had-
[02:22:23.680 --> 02:22:24.000]   That was right.
[02:22:24.000 --> 02:22:24.160]   I remember I had-
[02:22:24.160 --> 02:22:24.640]   That was impossible.
[02:22:24.640 --> 02:22:26.080]   I had one of these-
[02:22:26.080 --> 02:22:29.920]   I was one of those people that would throw the CDs around the car and, you know,
[02:22:29.920 --> 02:22:30.480]   it was-
[02:22:30.480 --> 02:22:30.880]   Yeah.
[02:22:30.880 --> 02:22:33.120]   I remember when when CDs first came out,
[02:22:33.120 --> 02:22:34.240]   I was working at a radio station.
[02:22:34.240 --> 02:22:35.520]   Our program director said,
[02:22:35.520 --> 02:22:38.160]   "These are indestructible and throw them across the table."
[02:22:38.160 --> 02:22:39.120]   And they all scratched.
[02:22:39.120 --> 02:22:40.800]   That's it.
[02:22:40.800 --> 02:22:44.560]   Joe Donovan, you are a blessing.
[02:22:44.560 --> 02:22:46.560]   You are amazing.
[02:22:46.560 --> 02:22:48.240]   Come back anytime.
[02:22:48.240 --> 02:22:48.240]   Yeah.
[02:22:48.240 --> 02:22:48.800]   I'm a thank you.
[02:22:48.800 --> 02:22:49.360]   I'm a thank you.
[02:22:49.360 --> 02:22:50.640]   We love having you on.
[02:22:50.640 --> 02:22:54.640]   So smart research director at the Shorenstein Center on Media, Politics,
[02:22:54.640 --> 02:22:57.440]   and Public Policy at Harvard's Kennedy School.
[02:22:57.440 --> 02:23:01.440]   Your students are lucky, and we are so glad you could be here.
[02:23:01.440 --> 02:23:03.760]   Do you have a book that you want to plug?
[02:23:03.760 --> 02:23:08.480]   Well, September, I'll come back, but Memoir's coming out.
[02:23:08.480 --> 02:23:10.320]   We got the book, you know, we got the-
[02:23:10.320 --> 02:23:10.480]   Yeah.
[02:23:10.480 --> 02:23:14.400]   We're getting the galleys next week, and we're going to get half a book.
[02:23:14.400 --> 02:23:15.360]   Who had the good-
[02:23:15.360 --> 02:23:16.640]   Bloomsbury.
[02:23:16.640 --> 02:23:18.080]   Bloomsbury.
[02:23:18.080 --> 02:23:18.080]   Oh.
[02:23:18.080 --> 02:23:22.240]   And it's me and Emily Dreyfus and Brian Friedberg are my co-authors.
[02:23:22.240 --> 02:23:22.800]   And-
[02:23:22.800 --> 02:23:23.600]   And I'm going to get-
[02:23:23.600 --> 02:23:24.160]   I'm going to get it on-
[02:23:24.160 --> 02:23:25.280]   I'm going to get it on Target.
[02:23:25.280 --> 02:23:25.360]   From Target.
[02:23:25.360 --> 02:23:28.080]   Apparently Target has it for $28, so-
[02:23:28.080 --> 02:23:30.320]   Everything from Occupy to the Insurrection.
[02:23:30.320 --> 02:23:30.800]   Oh wow.
[02:23:30.800 --> 02:23:34.880]   Over the last 10 years of net war, and it is a wild ride.
[02:23:34.880 --> 02:23:35.840]   Oh, I can't.
[02:23:35.840 --> 02:23:36.160]   It's a way-
[02:23:36.160 --> 02:23:36.720]   It's a great academic.
[02:23:36.720 --> 02:23:39.440]   Ah, no, just Bloomsbury.
[02:23:39.440 --> 02:23:41.120]   But we keep the sources light.
[02:23:41.120 --> 02:23:43.200]   There's only about 750 of them.
[02:23:43.200 --> 02:23:46.080]   Pre-order now on Amazon.
[02:23:46.080 --> 02:23:48.480]   Memoir's the untold story of the online battles
[02:23:48.480 --> 02:23:50.400]   upending democracy in America.
[02:23:50.400 --> 02:23:54.640]   Yes, when that comes out, if Emily wants to come on and Brian as well,
[02:23:54.640 --> 02:23:56.720]   we'll have all three of you on, it's up to you, but-
[02:23:56.720 --> 02:23:58.400]   Oh, that would be a treat.
[02:23:58.400 --> 02:23:59.520]   We would love to do that.
[02:23:59.520 --> 02:24:00.560]   That sounds so good.
[02:24:00.560 --> 02:24:01.120]   Okay.
[02:24:01.120 --> 02:24:01.520]   So good.
[02:24:01.520 --> 02:24:01.760]   Sounds good.
[02:24:01.760 --> 02:24:03.360]   We'll do a special episode.
[02:24:03.360 --> 02:24:04.080]   Book it.
[02:24:04.080 --> 02:24:04.800]   Book it.
[02:24:04.800 --> 02:24:05.600]   Book it, Dano.
[02:24:05.600 --> 02:24:06.160]   Yeah.
[02:24:06.160 --> 02:24:06.640]   Yeah.
[02:24:06.640 --> 02:24:07.360]   Thank you, Joan.
[02:24:07.360 --> 02:24:08.960]   Thank you so much.
[02:24:08.960 --> 02:24:09.760]   Appreciate it.
[02:24:09.760 --> 02:24:14.320]   Jeff Jarvis, director of the townite center for entrepreneurial journalism at Craig.
[02:24:14.320 --> 02:24:14.720]   Newmark.
[02:24:14.720 --> 02:24:22.160]   Graduate School of Journalism at the City University of New York.
[02:24:22.160 --> 02:24:23.280]   Buzzmachine.com.
[02:24:23.280 --> 02:24:24.240]   Thank you, bless you.
[02:24:24.240 --> 02:24:24.960]   Great to see you.
[02:24:24.960 --> 02:24:25.680]   Your students are lucky too.
[02:24:25.680 --> 02:24:26.080]   You always do.
[02:24:26.080 --> 02:24:27.200]   To see you, boss.
[02:24:27.200 --> 02:24:27.520]   Yeah.
[02:24:27.520 --> 02:24:29.840]   This was a fun show.
[02:24:29.840 --> 02:24:32.160]   Unlike the others, I loyally show up.
[02:24:32.160 --> 02:24:33.760]   I don't just disappear.
[02:24:33.760 --> 02:24:35.280]   I missed Ant and Stacey.
[02:24:35.280 --> 02:24:36.480]   I like wrapping with them too.
[02:24:36.480 --> 02:24:36.480]   I do too.
[02:24:36.480 --> 02:24:37.680]   No, they're great too.
[02:24:37.680 --> 02:24:38.640]   Have me on again.
[02:24:38.640 --> 02:24:39.120]   We will.
[02:24:39.120 --> 02:24:40.880]   Yeah, we need to have you on with them, yes.
[02:24:40.880 --> 02:24:46.160]   We do twigs this showed this week in Google or whatever the hell we want to talk about.
[02:24:46.160 --> 02:24:50.480]   Every Wednesday, 2 p.m. Pacific, 5 p.m. Eastern 2100 UTC.
[02:24:50.480 --> 02:24:52.000]   You can watch us do it live if you want.
[02:24:52.000 --> 02:24:56.240]   We do a live stream, audio and video at twit.tv/live.
[02:24:56.240 --> 02:24:58.480]   If you're watching live, chat live.
[02:24:59.280 --> 02:25:02.400]   The chatroom is irc.twit.tv.
[02:25:02.400 --> 02:25:06.480]   And of course, club twit members have their very own chatroom.
[02:25:06.480 --> 02:25:14.000]   Look at their, this is this is RTFM's LP collection with his ex-wife as the model.
[02:25:14.000 --> 02:25:14.560]   I love it.
[02:25:14.560 --> 02:25:17.440]   So this is what you do when you go in somebody's house.
[02:25:17.440 --> 02:25:21.360]   And you know what he has the requisite cinder blocks holding up boards.
[02:25:21.360 --> 02:25:23.360]   That is the real deal.
[02:25:23.360 --> 02:25:24.640]   That is a real vinyl collection.
[02:25:24.640 --> 02:25:26.080]   He said 1200 albums.
[02:25:26.080 --> 02:25:28.000]   Anyway, that's the kind of thing you see.
[02:25:28.000 --> 02:25:30.320]   If you're in our Discord server, please.
[02:25:30.320 --> 02:25:31.040]   Oh, look at this.
[02:25:31.040 --> 02:25:36.000]   Galia has quite a good collection of science fiction books and comics.
[02:25:36.000 --> 02:25:38.240]   She says, that's how you know I'm not normal.
[02:25:38.240 --> 02:25:42.960]   So the reason you'd want to be in here, well, there are lots of them.
[02:25:42.960 --> 02:25:45.760]   First of all, you get ad-free versions of all of our shows because you're paying us
[02:25:45.760 --> 02:25:47.840]   seven bucks a month to be in the club.
[02:25:47.840 --> 02:25:49.760]   So we don't need to monetize you.
[02:25:49.760 --> 02:25:52.800]   You also get access to this wonderful discord.
[02:25:52.800 --> 02:25:57.200]   It's not just chat about the shows, but about anything a geek might be into.
[02:25:57.200 --> 02:26:03.520]   Comics, anime, data science, hacking, ham radio, hardware, sci-fi.
[02:26:03.520 --> 02:26:07.600]   We have Stacy's book club in there, the untitled Linux show,
[02:26:07.600 --> 02:26:11.920]   the Gizfiz, and tomorrow we've got a special event.
[02:26:11.920 --> 02:26:15.360]   Paul Therat is going to do an AMA, 9AM Pacific.
[02:26:15.360 --> 02:26:21.440]   Members of the club can join us live and then listen after the fact on our Twit+ feed.
[02:26:21.440 --> 02:26:26.080]   April 14th, some guy named Jeff Jarvis will do a fireside chat as well.
[02:26:26.080 --> 02:26:28.800]   Yep, they'll be burning something.
[02:26:28.800 --> 02:26:29.200]   Yes.
[02:26:29.200 --> 02:26:31.920]   So the question really is, why aren't you a member?
[02:26:31.920 --> 02:26:32.880]   My goodness.
[02:26:32.880 --> 02:26:37.040]   It really helps us in our efforts, and I think we've made something that has a lot of great stuff
[02:26:37.040 --> 02:26:37.760]   going on.
[02:26:37.760 --> 02:26:41.360]   All you have to do is go to twit.tv/club, twit $7 a month.
[02:26:41.360 --> 02:26:42.480]   And guess what?
[02:26:42.480 --> 02:26:45.040]   If you're a member club twit, you didn't even hear this ad.
[02:26:45.040 --> 02:26:46.320]   You just know ads at all.
[02:26:46.320 --> 02:26:47.680]   So please join.
[02:26:47.680 --> 02:26:50.960]   Thank you all for joining us, each and every week on this week in Google.
[02:26:50.960 --> 02:26:51.760]   We'll see you next time.
[02:26:51.760 --> 02:26:52.720]   Bye-bye.
[02:26:53.840 --> 02:26:56.400]   Hey, I'm Rod Pyle, editor of Ad Astra Magazine.
[02:26:56.400 --> 02:27:00.640]   And each week I'm joined by Tariq Malek, the editor in chief over at space.com.
[02:27:00.640 --> 02:27:05.760]   In our new This Week in Space podcast, every Friday, Tariq and I take a deep dive into the
[02:27:05.760 --> 02:27:07.600]   stories that define the new space age.
[02:27:07.600 --> 02:27:08.800]   What's NASA up to?
[02:27:08.800 --> 02:27:11.520]   Windwell Americans once again set foot on the moon.
[02:27:11.520 --> 02:27:13.680]   And how about those samples from the Perseverance rover?
[02:27:13.680 --> 02:27:15.040]   One of those coming home.
[02:27:15.040 --> 02:27:17.280]   What the heck is Elon Musk done now?
[02:27:17.280 --> 02:27:21.680]   In addition to all the latest and greatest in space exploration, we'll take an occasional look
[02:27:21.680 --> 02:27:24.800]   at bits of spaceflight history that you probably never heard of
[02:27:24.800 --> 02:27:27.520]   and all with an eye towards having a good time along the way.
[02:27:27.520 --> 02:27:30.000]   Check us out on your favorite podcatcher.
[02:27:30.000 --> 02:27:40.160]   [Music]

