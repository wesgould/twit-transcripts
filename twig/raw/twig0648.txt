;FFMETADATA1
title=Band Saws and Butt Joints
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2022-01-27
track=648
language=English
genre=Podcast
comment=FLoC is dead, ID.me and facial scanning, Neil Young gone from Spotify
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:04.560]   It's time for Twig this week in Google Stacey Higginbotham and Pruitt Jeff Jarvis.
[00:00:04.560 --> 00:00:10.560]   The whole gang is here. We'll talk about Google's flock. They're replacing it, but there's the replacement any better.
[00:00:10.560 --> 00:00:16.400]   The IRS wants to use id.me for your face scans. Is that a good idea?
[00:00:16.400 --> 00:00:22.240]   And why is Amazon getting behind the legalization of marijuana? It's all coming up next on Twig.
[00:00:25.280 --> 00:00:31.040]   Podcasts you love. From people you trust. This is Twig.
[00:00:31.040 --> 00:00:44.160]   This is Twig. This week in Google, episode 648, recorded Wednesday, January 26th, 2022.
[00:00:44.160 --> 00:00:49.840]   Band saws and butt joints. This week in Google is brought to you by Wealthfront.
[00:00:49.840 --> 00:00:58.640]   To start building your wealth and get your first $5,000 managed for free for life, go to wealthfront.com/twig.
[00:00:58.640 --> 00:01:05.840]   And buy imperfect foods. Imperfect foods is catching the food that's falling through the cracks of our food system
[00:01:05.840 --> 00:01:11.280]   by sourcing quirky yet delicious foods. Right now imperfect foods is offering our listeners
[00:01:11.280 --> 00:01:17.520]   20% off your first four orders when you go to imperfectfoods.com and use the promo code Twig.
[00:01:18.400 --> 00:01:24.560]   And buy compiler. An original podcast from Red Hat discussing tech topics big, small and
[00:01:24.560 --> 00:01:29.760]   strange. Listen to the compiler on Apple Podcasts or anywhere you listen to podcasts.
[00:01:29.760 --> 00:01:37.680]   It's time for Twig this week in Google. The show where we get the ladies from the world around us.
[00:01:37.680 --> 00:01:42.560]   How about that? That's really what we're doing. Taking a look at the world around us
[00:01:43.440 --> 00:01:50.080]   with Waldorf and Sadler. Here's my Sadler ladies and gentlemen. Mr. Jeff Jarvis. Hello Jeff. Hello.
[00:01:50.080 --> 00:01:56.000]   Is that the grumpy one? I think they're both grumpy. Both grumpy. Well, I'm fine. That's
[00:01:56.000 --> 00:02:06.160]   the Leonard Taill professor for journalistic innovation at the graduate school of journalism at the
[00:02:06.160 --> 00:02:12.800]   city university of New York. Looking good to the wonderful my family thinks outside the door there.
[00:02:13.680 --> 00:02:26.320]   They hear you making that racket. Also here and put put grit from hands on photography.
[00:02:26.320 --> 00:02:31.680]   He's the community manager at our wonderful discord and you got you did a bunch of stuff this week.
[00:02:31.680 --> 00:02:37.760]   Yeah, been a little busy. We got an event coming up with Mr. Georgia Dow. It's going to be a lot
[00:02:37.760 --> 00:02:45.520]   of fun. Yeah. We got Mike Elgin coming up in a couple of weeks. It's going to be good. It's
[00:02:45.520 --> 00:02:49.840]   going to be good. It's going to be good. Discord is back up. Discord was down today. The API was
[00:02:49.840 --> 00:02:56.240]   broken. I think it's not happening. It's distributed. It's not distributed. That's the illusion that
[00:02:56.240 --> 00:03:02.800]   you have. Everybody has a server but they all run on Discord. Discord owns hardware.
[00:03:02.800 --> 00:03:07.040]   So it's really it's just I never understood that. Yeah, I never understood either till I got one
[00:03:07.040 --> 00:03:12.080]   and then I realized, well, it's just, you know, it's all running on their hardware. Yeah. So it's
[00:03:12.080 --> 00:03:15.600]   like any other year. I was just like Twitter. Yeah, but it's just like Twitter, really.
[00:03:15.600 --> 00:03:21.680]   Anything else, right? You just can have a closed little club, which is great. Also with this,
[00:03:21.680 --> 00:03:26.480]   she's back. Stacey, well, she wasn't gone last week, but she is. I was here yesterday or last week.
[00:03:26.480 --> 00:03:33.520]   Last week, host of Stacey's book club, Stacey Higginbotham from Stacey on IOT.com and the IOT podcast
[00:03:33.520 --> 00:03:39.920]   with Kevin Tofol. Hello, Stacey. Hello, y'all. Half of your eyes look one way. Half the eyes
[00:03:39.920 --> 00:03:49.200]   look the other. Her husband really? Did her right eye today. In a beautiful moment of togetherness.
[00:03:49.200 --> 00:03:53.680]   That's really sweet. I should get Lisa to do my makeup sometime. I think that's a nice idea.
[00:03:53.680 --> 00:03:59.440]   Do you do I make up? No, of course not. I did used to for you. I was just saying you did TV.
[00:03:59.440 --> 00:04:05.200]   Yeah. I am my own makeup kit with, I know what my shades are and all that and my contour and fill
[00:04:05.200 --> 00:04:12.160]   and the minute we started doing these podcasts in video, I said, I am never, I'm not wearing makeup.
[00:04:12.160 --> 00:04:20.800]   Sorry. I did my eye makeup today, too. You. Fabulous. Did you paint on those eyebrows? I think you did.
[00:04:22.400 --> 00:04:24.720]   No. You painted them. That's just focused peaking. You did.
[00:04:24.720 --> 00:04:30.000]   Were you around Leo when when HGTV came in and started doing the era? Yeah.
[00:04:30.000 --> 00:04:36.880]   Brush thing. I was doing live with Regis and Kelly and the makeup artist, you go there,
[00:04:36.880 --> 00:04:41.200]   you get made up before the show. The makeup artist said we're using airbrushes today
[00:04:41.200 --> 00:04:44.720]   and she painted my face with an airbrush.
[00:04:44.720 --> 00:04:51.280]   Because if you just use regular foundation and powder, you can see it. There's enough detail.
[00:04:51.280 --> 00:04:52.880]   You can see the powder. Yeah. So they.
[00:04:52.880 --> 00:04:59.120]   HD and HD. They pay. But apparently Kelly had brought that on. She had a TV show. I came
[00:04:59.120 --> 00:05:03.520]   remember their name of it, but she had a prime time TV show and they were using it there. So
[00:05:03.520 --> 00:05:07.760]   they wanted to try it on. They didn't stick with it. It was back to the. No, no, I would say it won't.
[00:05:07.760 --> 00:05:13.360]   Yeah. You don't need that. I had my makeup done for Fox TV once in. Oh my.
[00:05:13.360 --> 00:05:19.920]   Did they paint you right? I looked at they did, but they I walked out looking almost the same as I
[00:05:19.920 --> 00:05:23.520]   normally do, but a little better. Yeah. That's what you want.
[00:05:23.520 --> 00:05:28.880]   Wait, you took like 45 minutes. I still don't understand what all they were doing with my face.
[00:05:28.880 --> 00:05:32.800]   Oh, we used to have hair and makeup and tech TV. It would literally take you 45 minutes. You
[00:05:32.800 --> 00:05:37.760]   have to come an hour early to get hair done and makeup done. And then they've and then and they do
[00:05:37.760 --> 00:05:42.560]   your nails. They do the whole thing. Nails? Nails. Well, we were showing. Yeah, that's
[00:05:42.560 --> 00:05:49.840]   showing products. All I see. You're a hand model, George. Yeah. George, no, it was Kramer. Kramer
[00:05:49.840 --> 00:05:55.040]   was a hand model. And yeah, Kramer is too lotion is in. So I got good at going, you know, rub, you
[00:05:55.040 --> 00:05:59.200]   rub the outline. If I were showing this phone, you would you because you don't want to just go,
[00:05:59.200 --> 00:06:02.640]   look, there's the phone. You go like this. That's right. That's right.
[00:06:02.640 --> 00:06:07.200]   That looks my. It's your van. Those like the prices, right? Yeah.
[00:06:08.720 --> 00:06:16.400]   You just slowly stroking it. No idea how many times Queen Pruitt gave me crap for that.
[00:06:16.400 --> 00:06:21.920]   Fingernails. Yeah. If I did a product demo and, you know, we do to be roll of me showing the
[00:06:21.920 --> 00:06:27.520]   product, then my, I know the manicure done. Okay. You want a tip? She let me know. You want a tip?
[00:06:27.520 --> 00:06:30.480]   I'll give you a tip. Everybody. This is going to be your tip of the week.
[00:06:30.480 --> 00:06:35.600]   Because we, I for a while with tech TV, I would go and I would get my nails done.
[00:06:35.600 --> 00:06:39.360]   And I went to a place, it was 20 bucks. And eventually they stopped. They said,
[00:06:39.360 --> 00:06:44.400]   you're just paying too much. You need to go to the five dollar nail plies. And I'm not going to the
[00:06:44.400 --> 00:06:50.240]   five dollar nails. I'm sorry. So fungus is and stuff. Well, I just, you know, I said really 20
[00:06:50.240 --> 00:06:55.520]   bucks a week. You're not anyway, I stopped. But I did it. My, and I asked and they said, look,
[00:06:55.520 --> 00:07:01.200]   all you need to do the manicure said, get a good nail brush every, every day in the, when you shower,
[00:07:01.200 --> 00:07:06.560]   whatever you use the nail brush, scrubs, scrub and then you put a nice little cuticle oil on your
[00:07:06.560 --> 00:07:11.360]   cuticle oil. And you're good. And you know, you can cut obviously, but you're good. That means
[00:07:11.360 --> 00:07:14.960]   your nails will look good enough that you can do the hand modeling. Have you maintained this
[00:07:14.960 --> 00:07:21.600]   toilet? I still do this. Yes. Every morning. Oh, yeah. I get lax on it. And she lets me know,
[00:07:21.600 --> 00:07:24.960]   because we have all of that stuff here with that being her preferred. Oh, that's your job.
[00:07:24.960 --> 00:07:29.600]   She does manicures. That's your job. I remember when I first met her, she was like,
[00:07:30.160 --> 00:07:33.040]   she looked first at my nails and commented on that before anything.
[00:07:33.040 --> 00:07:36.240]   Grateful. I had just had them done.
[00:07:36.240 --> 00:07:38.720]   Have my hands on my pockets.
[00:07:38.720 --> 00:07:42.800]   He's done my nails, actually. I went when she was at the manicure shop down here and went down,
[00:07:42.800 --> 00:07:47.200]   got my nails done. Now, who would think who would think this would be our topic?
[00:07:47.200 --> 00:07:50.480]   The lead story today. I don't know how we got here.
[00:07:50.480 --> 00:07:53.440]   Our lead. You know what our lead story should be? Goodbye flock.
[00:07:54.320 --> 00:08:01.200]   flock you. We got to say hello to flock. Yeah, Google says, and I think this is really an
[00:08:01.200 --> 00:08:06.400]   interesting story said yesterday, they were going to replace, as you remember, they had announced,
[00:08:06.400 --> 00:08:10.960]   no more third party cookies. Everybody hates them. Good. That's fine. But we do need some way to
[00:08:10.960 --> 00:08:16.560]   sell targeted advertising. So we're going to have the federated learning of cohorts,
[00:08:17.360 --> 00:08:24.400]   which was a kind of retro name, a awkward retro name, because everything in this area had birds
[00:08:24.400 --> 00:08:33.680]   themed. So this flock would essentially watch what you do, just the same way as Google always did.
[00:08:33.680 --> 00:08:39.280]   But instead of having a unique identifier for you, they would put you in a cohort of people with
[00:08:39.280 --> 00:08:44.320]   similar interests. They never really explained how big this cohort would be.
[00:08:44.880 --> 00:08:51.280]   But the thing that I think upset people was then when you go to a website, your flock cohort would
[00:08:51.280 --> 00:09:00.880]   be sent to the website. Like, you know, hi, here's Leo. He's 3994-7. And people didn't really like
[00:09:00.880 --> 00:09:06.000]   that. It wasn't clear how big the cohort would be. I think there's also, it's funny because they're
[00:09:06.000 --> 00:09:15.280]   in trouble with the EU. The Oxford Springer, our favorite heavy, has sued them for ending cookies.
[00:09:15.280 --> 00:09:21.440]   Yeah. Yeah. Springer is the one who led the fight against Google, used their political clout,
[00:09:21.440 --> 00:09:25.760]   got the EU to go after Google. Everybody's screaming privacy, privacy dot, dot, dot,
[00:09:25.760 --> 00:09:29.200]   insurance, get rid of cookies. They're evil. And then, and then after the spreader says,
[00:09:29.200 --> 00:09:34.320]   Oh, we can't target any ads. No, don't get rid of cookies. That's very confusing.
[00:09:34.320 --> 00:09:41.280]   The EFF did not like flock because there was this concern that, as always, with these things that
[00:09:41.280 --> 00:09:47.520]   are so called anonymized, it's possible to de-anonymize. And they were a little worried that if people
[00:09:47.520 --> 00:09:53.440]   started to combine your flock ID with other profile information, they might actually be able to figure
[00:09:53.440 --> 00:10:00.240]   out who's who and all of that. So Google has, I think in response to the EU, I don't know, really,
[00:10:00.240 --> 00:10:07.280]   in response to privacy advocates. I think there was a lot of reasons because a lot of people were
[00:10:07.280 --> 00:10:15.600]   unhappy with flock. Yeah. So they're coming up with topics. Topics. It was just a proposal. I mean,
[00:10:15.600 --> 00:10:22.640]   they never implemented it. It was only in beta. So now it's topics. Topics.
[00:10:22.640 --> 00:10:30.480]   So Ben Galbraith, who's direct senior director for product, acknowledged privacy concerns with
[00:10:30.480 --> 00:10:35.840]   flock said topics was easier for users to understand. It would also make it easier for Google to
[00:10:35.840 --> 00:10:45.920]   remove sensitive topics from being targeted. So then to topics, let's advertisers target online
[00:10:45.920 --> 00:10:55.360]   users based on a topic they might be interested in, fitness or travel. Chrome does this automatically.
[00:10:55.360 --> 00:11:01.760]   Now, it's not, by the way, this is all proposal not implemented yet. Chrome will generate five
[00:11:01.760 --> 00:11:08.160]   topics based on your browser history with, and this is important, participating websites.
[00:11:08.160 --> 00:11:13.760]   It then sends three topics, one from each of the past three weeks, to participating sites,
[00:11:13.760 --> 00:11:19.600]   to share with their advertising partners. Topics like flock are not permanent. In this case,
[00:11:19.600 --> 00:11:25.920]   they're stored for three weeks, then deleted. There are 350 topics. But Google's saying, by doing
[00:11:25.920 --> 00:11:30.560]   this, instead of saying what your interests are in flock, we're saying what your interests are,
[00:11:30.560 --> 00:11:36.800]   but we could also just ignore things like, I don't know, whatever is, you know, mental health issues,
[00:11:36.800 --> 00:11:42.640]   searches for mental health resources. Guns, that kind of thing. So topic data is stored on your
[00:11:42.640 --> 00:11:50.160]   device. manicures manicures. They know, I feel like a good test. If I get a bunch of manicure
[00:11:50.160 --> 00:11:57.600]   information in my Instagram feed, I think we'll know. So it's never stored on its Google, unlike
[00:11:57.600 --> 00:12:01.360]   third party tracking cookies, which allow people companies to use tracking pixels.
[00:12:01.360 --> 00:12:07.360]   So they can go cross site. This can't really go cross site. You will be unlike flock, you'll be
[00:12:07.360 --> 00:12:14.960]   able to see what topics you are being, you know, flogged with that are being sent to sites.
[00:12:14.960 --> 00:12:21.120]   If you don't like a topic, I like this. You can remove it. This is great. Yeah. You can say,
[00:12:21.120 --> 00:12:26.560]   you know, I'm not interested in tennis rackets. I am interested once for my wife, and I don't
[00:12:26.560 --> 00:12:34.880]   play tennis, right? Yeah, I'm too old. I play pickleball. And yeah. And Google says users will be able
[00:12:34.880 --> 00:12:42.240]   to disable the feature completely, which is really interesting. I think Google does Google wants
[00:12:42.240 --> 00:12:46.960]   to maintain its ad model, but they really do want to respond to people's privacy concerns.
[00:12:46.960 --> 00:12:54.000]   It's a way to add is a value advice. Go ahead. I'm sorry. You said this is stored on the device.
[00:12:54.000 --> 00:12:59.360]   And yeah, Chrome is just reading it. Yes. That way. Well, Chrome generates it and then stores it in its,
[00:12:59.360 --> 00:13:04.640]   you know, store, stores it in wherever it stores cookies. Back pocket. Yes.
[00:13:04.640 --> 00:13:10.240]   Back pocket is that but it's stored locally on your device. It's not sent up to the cloud,
[00:13:10.240 --> 00:13:14.800]   like associate with your IP address. Correct. So they announced. Yeah, that's right. That's the
[00:13:14.800 --> 00:13:19.760]   key, right? It's anonymized. Yeah. Although it is in a way associated with your IP address,
[00:13:19.760 --> 00:13:27.120]   because when you go, let's say you go to what's the, you know, some bass fishing place.
[00:13:27.120 --> 00:13:34.400]   And and it sends out, let's, you know, you go to Dick's Sporting Goods website. And it's going to
[00:13:34.400 --> 00:13:40.640]   say, well, here's the three things. Here's the three topics Leo's interested in. But but Dick's is
[00:13:40.640 --> 00:13:46.560]   getting that plus my IP address. So that's why they have to expire them on a regular basis, right?
[00:13:46.560 --> 00:13:50.240]   So they can't start building a picture of you. The other issue thing here,
[00:13:50.240 --> 00:13:54.640]   what's going to be done? Go ahead, please. I'm sorry. Oh, I was going to say Google says they
[00:13:54.640 --> 00:14:00.320]   also are going to send fake topics to about 5% of the websites to make sure the topic generation
[00:14:00.320 --> 00:14:05.120]   is actually random, which I thought was kind of like, does that mean like 5% of your ads aren't
[00:14:05.120 --> 00:14:10.880]   are they going to give people a discount of 5% like that? Oh, from the get where they're coming from,
[00:14:10.880 --> 00:14:19.520]   but it's kind of weird. I have. I mean, I'm a little, I'm a, of course, I'm in a little skeptical
[00:14:19.520 --> 00:14:25.600]   because Google has its own way of tracking who you are and all of that. But I think that Google is
[00:14:25.600 --> 00:14:30.880]   really is, I'm going to give them credit for trying to find a system that users can tolerate
[00:14:30.880 --> 00:14:35.520]   that gives the advertisers some of the information, which might actually give you more relevant
[00:14:35.520 --> 00:14:39.920]   advertising. Here's a question. Do you, did you receive anything, whether, whether an advertiser
[00:14:39.920 --> 00:14:45.680]   can do a not? That's how it got abused on Facebook, right? If I decide I want to advertise this thing,
[00:14:45.680 --> 00:14:52.880]   but not to people who like hip hop as a way, you know, this job as a way to kind of redlining
[00:14:52.880 --> 00:14:59.040]   in other words, right? And the not was an issue there. I don't think it was the primary one.
[00:14:59.040 --> 00:15:03.200]   I don't think that Google can control the not because they're going to send here's a,
[00:15:03.200 --> 00:15:07.600]   here's on the Google search blog sample. They're going to send out, you know, this, you know,
[00:15:07.600 --> 00:15:11.440]   Jeff's interest in autos and vehicles, books and literature, comics and animation, rock music,
[00:15:11.440 --> 00:15:17.120]   team sports. There's no reason that Dick's sports couldn't say, oh, rock, I don't want anybody's
[00:15:17.120 --> 00:15:23.040]   interest in rock and just say, you know, we won't advertise to that person. The point that Google's
[00:15:23.040 --> 00:15:28.400]   making on this graph diagram is you don't know, you can't tell what's in that cookie. It's, you
[00:15:28.400 --> 00:15:34.080]   know, it's code, but you can see very clearly, because Chrome will show this what your, what interest
[00:15:34.080 --> 00:15:39.360]   is sending to advertisers. And you see that X, you can delete them. So I think this is, I like
[00:15:39.360 --> 00:15:48.400]   this solution. Let me here's Google's one minute video to describe this here. Are you getting
[00:15:48.400 --> 00:15:59.920]   audio? You didn't test your audio, Leo. That's right, right. Right. Right. Right. Yeah. So maybe
[00:15:59.920 --> 00:16:07.360]   there's no audio in it. We heard the tick tock. Okay. Hmm. Introducing topics. Thank you.
[00:16:07.360 --> 00:16:11.920]   Obviously, Sandbox's newest proposal to help you preserve your privacy online while enabling sites
[00:16:11.920 --> 00:16:15.920]   to show about puzzle with topics your browser will know topics related to participate inside you
[00:16:15.920 --> 00:16:21.120]   visit. Let me do this. I'm a professional. For example, if you've recently visited sites about
[00:16:21.120 --> 00:16:27.360]   sports, the browser may note that sports is one of your topics. Your topics will help determine the
[00:16:27.360 --> 00:16:34.400]   ads you see. And as I won't need to know who you are to show you an ad about sports. They just know
[00:16:34.400 --> 00:16:38.960]   you like it. You have no control. Oh, sorry. You do have control over your topics and you can
[00:16:38.960 --> 00:16:43.840]   run for topics in the browser or you can turn them off altogether. I'm cold reading it, man.
[00:16:43.840 --> 00:16:50.480]   With topics, the specific sites you visited are no longer shared across the web, like they might
[00:16:50.480 --> 00:16:56.080]   have been with third party cookers, party evil cooking. We're testing topics soon to get feedback
[00:16:56.080 --> 00:17:01.360]   from the industry so that it works together with all of the other privacy sandbox proposals
[00:17:01.360 --> 00:17:05.760]   to protect your privacy and keep the internet a valuable source of free information
[00:17:05.760 --> 00:17:09.440]   for now in the future. And that's the balancing act. They want to support advertising, which
[00:17:09.440 --> 00:17:14.240]   is what supports free internet content. But they also want you to feel comfortable with the
[00:17:14.240 --> 00:17:18.000]   information they share. I don't know negatives. Stacy, can you see a negative here?
[00:17:18.000 --> 00:17:24.400]   Um, no, I mean, I think it's mostly positive, especially if they make it easy for you to look
[00:17:24.400 --> 00:17:28.640]   because then you can also, I mean, the negative for an advertiser is I could go in and I could be
[00:17:28.640 --> 00:17:33.920]   like, I'm so sick of seeing all these high price coats because I keep buying them. So then I'm like,
[00:17:33.920 --> 00:17:40.400]   I'm not interested in high fashion. And then I would stop buying high fashion because of advertising.
[00:17:40.400 --> 00:17:44.880]   Google has really not a problem. Published interestingly on GitHub,
[00:17:44.880 --> 00:17:51.680]   the details about the API, if you want to, if you want to start getting ready as a, as a web
[00:17:52.160 --> 00:17:59.520]   site designer to support this. I think it's, um, I think there's a full opportunity is to let me
[00:17:59.520 --> 00:18:06.720]   pick my topics. Oh, isn't that interesting? Tell you I want a car. But you might, I mean,
[00:18:06.720 --> 00:18:12.000]   I will say that as I, so especially because it's short range, like three weeks, let's just go with
[00:18:12.000 --> 00:18:17.520]   every time I binge watch a show, I start googling like mad to hear like the actors. Where have I
[00:18:17.520 --> 00:18:23.120]   seen this person? Yeah, me too. So it captures, it captures that, which is a very fleeting interest,
[00:18:23.120 --> 00:18:29.280]   but might be interesting for an advertiser. And then it goes away when I'm probably no longer
[00:18:29.280 --> 00:18:33.280]   interested in it. If I had to manage that, I'd just be like, I'm interested in IOT and clothes.
[00:18:33.280 --> 00:18:40.160]   But the interesting thing too here is Google's always, always sussed out topicality,
[00:18:40.160 --> 00:18:44.800]   right from the very, very beginning of advertising. So how are they going to know an expensive coat
[00:18:44.800 --> 00:18:48.560]   site? I guess that's pretty obvious. They're coats and they're expensive. But you know,
[00:18:48.560 --> 00:18:54.480]   what, how does the topicality, uh, determined is going to be interesting to see because we've
[00:18:54.480 --> 00:19:01.680]   never had transparency on that before. Why this ad appeared on this page. If you are,
[00:19:01.680 --> 00:19:06.160]   almost makes it look like they have access to the history, even though they say they're
[00:19:06.160 --> 00:19:12.880]   not looking at your history of which the user or the site and the user. But they, they are very
[00:19:12.880 --> 00:19:17.200]   obviously looking at your history. I mean, if I look at like my Google news feed, for example,
[00:19:17.200 --> 00:19:24.640]   I mean, all I have to do is search like one freaking, I don't know, like hot tub chemicals. And for
[00:19:24.640 --> 00:19:31.280]   the rest of my life, I'm reading stories about hot tubs. So I assume this is going to be built on,
[00:19:31.280 --> 00:19:38.400]   I mean, that's easy for Google, right? Yeah. Yeah, it may not be accurate. So I can go in and be
[00:19:38.400 --> 00:19:42.720]   like, please stop showing me ads for hot tubs. I mean, Google, look, Google's not going to make
[00:19:42.720 --> 00:19:47.840]   privacy advocates and advertisers happy. So it's a, it's like any compromise. There's,
[00:19:47.840 --> 00:19:54.560]   it's not going to be perfect. I think as a user, I'm satisfied with this. Will advertisers feel
[00:19:54.560 --> 00:19:59.280]   like they're getting enough information? I mean, honestly, if, if you're Dick's sporting goods and
[00:19:59.280 --> 00:20:03.200]   it says you're interested, I don't know, pickleball is probably too granular. That's the problem.
[00:20:03.200 --> 00:20:08.880]   So it might say you're interested in racket sports. Then then, I mean, maybe they'd like
[00:20:08.880 --> 00:20:11.680]   more granular. I don't know. I don't know. I heard from advertisers. I don't know.
[00:20:11.680 --> 00:20:18.160]   The other interesting piece here is, is the correlations that occur. I remember early on,
[00:20:18.160 --> 00:20:22.880]   I'm forgetting the name of the company that was bought by a well long ago, they found, for example,
[00:20:22.880 --> 00:20:32.560]   that service of people in the army and navy and so on correlated very highly with big screen TVs.
[00:20:34.240 --> 00:20:41.200]   So you might be wanting to advertise a big screen TV, but you go toward military interest or vice versa,
[00:20:41.200 --> 00:20:46.000]   right? And that's because you have your own data as an advertiser that says these are,
[00:20:46.000 --> 00:20:49.840]   these are the useful correlations that we have. Yeah.
[00:20:49.840 --> 00:20:54.800]   But that's okay. I mean, that's been happening for a while. So I don't have issues with that.
[00:20:54.800 --> 00:21:01.120]   I mean, I think we're cool with that, right? Right. We, we are, but we got to think about,
[00:21:02.240 --> 00:21:08.000]   you know, how does this, let's, let's, here's the test, I think. If I'm a job site and I'm
[00:21:08.000 --> 00:21:15.680]   advertising jobs, am I using some topics to get rid of, oh, people who may be less desirable
[00:21:15.680 --> 00:21:22.800]   for my employer? People who are on a culture fit? Yeah. Right. And, and so that's going to,
[00:21:22.800 --> 00:21:26.240]   Google's going to have to test out, and they're good at this, right? Going back to Matt Cuts,
[00:21:26.240 --> 00:21:30.800]   they're good at sussing out bad uses of things, but far better than Facebook ever was.
[00:21:31.600 --> 00:21:38.400]   I've seen how could this system get misused? And, and it will be it surely will be. No doubt about it.
[00:21:38.400 --> 00:21:47.040]   Okay. Anyway, I commend them for trying to find something that makes people happy.
[00:21:47.040 --> 00:21:52.640]   And I'm sure the, I haven't seen the EFF yet. Let me see if the EFF has already happened.
[00:21:52.640 --> 00:21:57.680]   I'm sorry for this. Well, but also the Washington Post, The New York Times, I put it in this tweet.
[00:21:58.640 --> 00:22:02.160]   Rather than say it a new way to target ads, a new way to track you.
[00:22:02.160 --> 00:22:05.840]   Yeah. But remember there.
[00:22:05.840 --> 00:22:09.200]   I don't think that's a problem. I think it's important to explain to people that these
[00:22:09.200 --> 00:22:13.520]   companies are tracking them. I have a really interesting tracking story, but it's not other
[00:22:13.520 --> 00:22:18.240]   thing. But let me just say this talk about it. And then let's do that. But it's not tracking.
[00:22:18.240 --> 00:22:23.680]   The common person's understanding. It's not tracking with you on the New York Times headline
[00:22:23.680 --> 00:22:28.960]   scaring. Well, but is it Leo defend me? I'm going to defend you because it's so
[00:22:28.960 --> 00:22:34.880]   third. So cookies as Google pointed out, track you in the sense that you, the Facebook like button
[00:22:34.880 --> 00:22:41.360]   sees multiple sites that you've been on. This is a very different system where the Chrome browser,
[00:22:41.360 --> 00:22:46.880]   which sees those sites anyway, then extracts from that an interest, five interests, which you
[00:22:46.880 --> 00:22:52.480]   can see what they are. You can turn them off and sends that to the website. That is not
[00:22:53.600 --> 00:22:57.280]   the same thing at all. That's not really tracking you, in my opinion. Now, maybe
[00:22:57.280 --> 00:23:02.880]   do you think Stacy consumers will see that mechanism that the browser then says this is what
[00:23:02.880 --> 00:23:08.000]   media you're interested in? Is that will that be seen as tracking? I don't feel like that's
[00:23:08.000 --> 00:23:16.480]   tracking. It's not following you in any way. Anytime something pops up and tells the average person,
[00:23:16.480 --> 00:23:21.040]   Hey, I think you like this, or I know you like this, right? And they're correct. They don't like
[00:23:21.040 --> 00:23:25.760]   creeps people out. They don't care that it follows them other places. That's a real subtle
[00:23:25.760 --> 00:23:31.440]   distinction. Most people don't care about they should because that's the problem. That's why we
[00:23:31.440 --> 00:23:37.440]   are. That's why we don't like third party cookies. But how sophisticated does someone have to be to
[00:23:37.440 --> 00:23:42.720]   so? Okay, can I tell you my tracking story? Because I think this is interesting. And it ties to this.
[00:23:42.720 --> 00:23:48.240]   So I read it. My daughter is a sophomore in high school. So she's looking, we're about to start
[00:23:48.240 --> 00:23:53.280]   the whole college thing. So we got a book by a Wall Street Journal reporter about like
[00:23:53.280 --> 00:23:58.640]   getting money for college. And in that book, he talked about how colleges are now using pixel
[00:23:58.640 --> 00:24:04.560]   trackers on emails to track when applicants open their emails, how quickly they respond,
[00:24:04.560 --> 00:24:08.880]   how many times they open it. And they use that information. I thought that was always.
[00:24:08.880 --> 00:24:15.520]   Okay, but they use that information for into getting a sense of how interested someone is
[00:24:16.320 --> 00:24:24.800]   in the school and for allocating scholarship money. And I was like, oh crap, I gotta get my
[00:24:24.800 --> 00:24:32.000]   daughter proton mail because I don't want her casual searches or interests in something to
[00:24:32.000 --> 00:24:39.760]   cost us money. And so I think it's old news that yes, there are pixel trackers in email.
[00:24:39.760 --> 00:24:44.080]   We might all be like, yeah, yeah, I know. But when you think about how those are being used,
[00:24:44.080 --> 00:24:49.360]   it's kind of like, holy crap. It might surprise you.
[00:24:49.360 --> 00:24:54.240]   Well, and that's what I mean. Like people are not sophisticated or it's not even not
[00:24:54.240 --> 00:24:58.720]   sophisticated. We just don't think everything through to that level because my God, we'd have,
[00:24:58.720 --> 00:25:04.800]   we sound like paranoid crazy folk. I guess. But anyway, that's why I've been thinking.
[00:25:04.800 --> 00:25:11.440]   Yeah, I think you got a normal air quotes and normals. We do sound paranoid when we start
[00:25:11.440 --> 00:25:15.600]   talking about privacy and security, even though it's pretty blatant, what's going on.
[00:25:15.600 --> 00:25:24.560]   I think Google has an opportunity here. And this is me just speculating to your point, Leo,
[00:25:24.560 --> 00:25:32.320]   about about the awareness of people under your points, Stacy and aunt about the nervousness
[00:25:32.320 --> 00:25:38.560]   is to be very strong of saying control your advertising, control what topics you have,
[00:25:38.560 --> 00:25:43.360]   control what topics you don't get. I've always argued, I wanted on the sites that I ran when I
[00:25:43.360 --> 00:25:47.120]   ran them, I wanted the opportunity for somebody to say, don't show me this advertiser and sell that
[00:25:47.120 --> 00:25:53.360]   data to the advertiser. These people don't want to see you. You figure out why, but that's valuable.
[00:25:53.360 --> 00:25:56.960]   You're wasting your money trying to get to these people. So stop going to those people, right?
[00:25:56.960 --> 00:26:04.560]   I think there's an opportunity to become, I hate these in this word, very proactive and allow
[00:26:05.760 --> 00:26:12.320]   users to, in fact, not just feel in control, but be in control. It's going to generate more data
[00:26:12.320 --> 00:26:16.560]   that is actually valuable because it comes from, it's a permission to system that says,
[00:26:16.560 --> 00:26:20.560]   yeah, I am looking for a car right now. No, I was looking for my daughters. You already bought
[00:26:20.560 --> 00:26:25.520]   the car, stop with the car ads, please stop wasting your money and my eye space. There's a lot of
[00:26:25.520 --> 00:26:33.520]   opportunity there, I think they won't do it. Well, and there's always people who are going to be like,
[00:26:34.240 --> 00:26:38.160]   not everybody wants to be proactive with advertisers. Many people might be like,
[00:26:38.160 --> 00:26:44.160]   I'm going to tell it, I want Dita about that. My thought is that really the people object to
[00:26:44.160 --> 00:26:48.480]   this is not about tracking. It's not about cookies, it's not about any of this. They object to advertising
[00:26:48.480 --> 00:26:52.560]   and they just, they don't want advertising that knows anything about them,
[00:26:52.560 --> 00:26:57.120]   which is never big. By the way, never been the case. Advertising is always
[00:26:57.120 --> 00:27:02.640]   targeted to some degree. If you buy better homes and gardens, you're targeting the readers of
[00:27:02.640 --> 00:27:07.040]   better homes and gardens. If you buy ads on this show, you're targeting them based on their interest
[00:27:07.040 --> 00:27:14.320]   in this show. I think where people got upset is they just don't like advertising.
[00:27:14.320 --> 00:27:17.440]   I think advertising has gotten more manipulative.
[00:27:17.440 --> 00:27:21.840]   Oh, it's not more manipulative. Think about the advertising.
[00:27:21.840 --> 00:27:28.400]   In TV ads where your co-worker put a bottle of scope on your desk,
[00:27:28.400 --> 00:27:32.480]   because they didn't want to tell you you had bad breath. I mean, it's been manipulative
[00:27:32.480 --> 00:27:39.280]   since four out of five doctors recommend bacon for breakfast. I mean, it's always been manipulative.
[00:27:39.280 --> 00:27:40.880]   I think it's maybe less manipulative now.
[00:27:40.880 --> 00:27:44.000]   Sorry, Stacey.
[00:27:44.000 --> 00:27:45.840]   No, go ahead.
[00:27:45.840 --> 00:27:51.760]   But more than half the ads in newspapers in the early days before the line of type and the
[00:27:51.760 --> 00:27:56.960]   explosion, more than half in major newspapers were for patent medicines. Advertising was
[00:27:56.960 --> 00:28:03.920]   crap. Let me get it. Feeling tired? Well, you need snake oil. I mean, I understand what
[00:28:03.920 --> 00:28:10.240]   people hate ads. That's probably part of the reason people hate ads. But ads, we even do ads.
[00:28:10.240 --> 00:28:15.120]   Marketing is important to businesses. Somehow we have to get the word out.
[00:28:15.120 --> 00:28:22.000]   It's not the ads. So advertising, yes, is manipulative. But I think the fact that it's coming at you
[00:28:22.000 --> 00:28:29.040]   in so many places in ways might feel more manipulative. I say that because,
[00:28:29.040 --> 00:28:34.880]   again, I'll just use my fancy coats as an example. I can watch the fancy coats on television.
[00:28:34.880 --> 00:28:40.080]   But when I'm watching them on the internet, they're chasing me over every freaking story there is.
[00:28:40.080 --> 00:28:48.880]   So the ratio of time I spend looking at fancy coats is pretty high. Maybe that's what feels so
[00:28:48.880 --> 00:28:54.240]   important. I don't. I think, well, so really the debate is how sophisticated are people when they
[00:28:54.240 --> 00:28:59.680]   say, "I don't want to be tracked." And I think the people who listen to this show are the privacy
[00:28:59.680 --> 00:29:09.680]   advocates who are in technology are really saying, "I don't want cookies tracking me from site to
[00:29:09.680 --> 00:29:16.400]   site. I don't want my location information to be available. I don't want you putting little sub
[00:29:16.400 --> 00:29:21.440]   pixels in my email so that you know I've opened it." That kind of thing is the intrusive tracking
[00:29:21.440 --> 00:29:28.000]   people are talking about. And I don't think this is not that. They may still not like it, but this
[00:29:28.000 --> 00:29:33.680]   is much more benign. Will they do a good job of explaining this to the public or will the public
[00:29:33.680 --> 00:29:38.080]   and media will be? Again, as I go back to, the narrative is going to come from the Washington
[00:29:38.080 --> 00:29:43.360]   Post and New York Times saying, "They found a new way to track you, evil Google." And it's going to
[00:29:43.360 --> 00:29:47.520]   be real hard to say, "We're actually trying to improve advertising." And if we don't,
[00:29:47.520 --> 00:29:53.040]   media are warped. Folks. So what if we say a less intrusive way to tell?
[00:29:53.040 --> 00:29:58.880]   Well, here's the headline in the Times. "Google introduces a new system for tracking Chrome browser
[00:29:58.880 --> 00:30:03.520]   users." Washington Post is worse. And if you, well, but even if you just read the headline, that's...
[00:30:03.520 --> 00:30:09.600]   Yeah, that sounds so good. Oh, right. Yeah. And it blocks so-called cookies.
[00:30:10.160 --> 00:30:15.200]   Yeah. Oh, why don't watch which scare quotes around cookies?
[00:30:15.200 --> 00:30:20.320]   Okay. I will tell you though, when I ask normal people every now and then, I'm like, "Hey,
[00:30:20.320 --> 00:30:24.960]   you know what a cookie is?" They're like, "Oh, yeah. It's that thing that it's on the web. It follows you."
[00:30:24.960 --> 00:30:26.320]   He's popping up on the screen.
[00:30:26.320 --> 00:30:33.040]   Here's the... Seriously. Yeah. Now, here's the Washington Post. Google Pro is a new way
[00:30:33.040 --> 00:30:37.200]   to track people around the web again. That's not even accurate.
[00:30:38.000 --> 00:30:41.680]   Wow. Yeah. It's got a little clickbait right there. Yeah.
[00:30:41.680 --> 00:30:47.120]   Yep. Moral panic. I gave you the warning. It was time for the moral panic.
[00:30:47.120 --> 00:30:49.760]   You fell apart. Geez.
[00:30:49.760 --> 00:30:52.080]   It's producing the show, Jeff. Come on, man.
[00:30:52.080 --> 00:30:53.840]   I'm not producing today.
[00:30:53.840 --> 00:30:55.840]   Oh, he's not. He had plenty of time to do it.
[00:30:55.840 --> 00:30:58.240]   He pushed the button. Never mind. I was trying to help you, Ant.
[00:30:58.240 --> 00:31:02.560]   There was a changing of the guard at the Tri-caster, and they didn't catch it. So,
[00:31:02.560 --> 00:31:07.840]   Ant, would you push the button again and the folks who pushed the buttons on the other
[00:31:07.840 --> 00:31:11.520]   side of the room, maybe they'll push the button at the same. There we go.
[00:31:11.520 --> 00:31:13.600]   There we go. I feel so good. Thank you, Ant.
[00:31:13.600 --> 00:31:17.200]   That's a priority. That is a blow on the top of the discussion.
[00:31:17.200 --> 00:31:20.400]   It does need some effect, because remember, most of the audience is not looking. They're
[00:31:20.400 --> 00:31:23.280]   only listening. Yeah. So, they only know we have to work on that.
[00:31:23.280 --> 00:31:27.280]   That would be the... It's always... It's gotta be real.
[00:31:27.280 --> 00:31:31.200]   No, it's more like... It's Charles.
[00:31:31.200 --> 00:31:38.720]   Yes. I'll dig through my library of sounds.
[00:31:38.720 --> 00:31:42.000]   Here, turn that up, Burke.
[00:31:42.000 --> 00:31:51.120]   Is that sinister enough? Do it again here.
[00:31:51.120 --> 00:31:52.560]   I don't hear it.
[00:31:52.560 --> 00:31:57.040]   Moral panic. Oh, yeah. That's good.
[00:31:57.040 --> 00:32:02.320]   Yeah. I'll buy you. I'll buy you that. Here's another one. Let's try this here.
[00:32:02.320 --> 00:32:03.680]   One more time.
[00:32:03.680 --> 00:32:10.400]   Moral panic. I think you need a little more panic in your voice.
[00:32:10.400 --> 00:32:14.640]   I think you need the more of your notes here.
[00:32:14.640 --> 00:32:16.960]   And then the tone. Oh, time it better.
[00:32:16.960 --> 00:32:18.800]   I think... Yeah, time it.
[00:32:18.800 --> 00:32:23.280]   Moral panic.
[00:32:24.960 --> 00:32:34.720]   Moral panic. Moral panic. We'll work on that. We'll workshop that.
[00:32:34.720 --> 00:32:36.880]   I like giving Leo notes. That's good.
[00:32:36.880 --> 00:32:38.800]   We have way too much fun on this show.
[00:32:38.800 --> 00:32:41.440]   We have way too much time in our lives.
[00:32:41.440 --> 00:32:43.440]   Moral panic. I was gonna say I think it's too...
[00:32:43.440 --> 00:32:47.760]   Panic. Moral panic.
[00:32:47.760 --> 00:32:53.120]   Moral panic. Oh, this is making me so happy.
[00:32:54.400 --> 00:32:57.600]   I think we need to cut the sound a little short.
[00:32:57.600 --> 00:33:00.480]   And then we'll get there.
[00:33:00.480 --> 00:33:02.960]   I will fix it on my end.
[00:33:02.960 --> 00:33:06.480]   Which will we three meet again in five.
[00:33:06.480 --> 00:33:07.840]   Moral panic.
[00:33:07.840 --> 00:33:09.920]   Moral panic.
[00:33:09.920 --> 00:33:15.760]   Okay. You want a heartbeat and a shark sound and...
[00:33:15.760 --> 00:33:17.600]   Yeah. All right.
[00:33:17.600 --> 00:33:21.120]   We can... I can work on that sound.
[00:33:21.120 --> 00:33:23.440]   I'm as a radio production director for many years.
[00:33:23.440 --> 00:33:25.280]   I could put something together.
[00:33:25.280 --> 00:33:27.840]   We get that. We get those notes from the morning guy.
[00:33:27.840 --> 00:33:29.200]   I need a moral panic sound.
[00:33:29.200 --> 00:33:32.000]   Got it for you. Yeah.
[00:33:32.000 --> 00:33:33.360]   Yeah. I mean, I feel like...
[00:33:33.360 --> 00:33:36.400]   You know, a lot of people listen to the show.
[00:33:36.400 --> 00:33:39.760]   We should be aware and try to give them some fun as well.
[00:33:39.760 --> 00:33:42.960]   By the way, the other Washington Post headline that caught my eye,
[00:33:42.960 --> 00:33:47.520]   "Why Amazon is ramping up its push for legalizing marijuana."
[00:33:47.520 --> 00:33:48.720]   So there...
[00:33:48.720 --> 00:33:52.160]   Now that's link bang.
[00:33:52.160 --> 00:33:52.880]   I see this.
[00:33:52.880 --> 00:33:54.160]   I clicked on that one right away.
[00:33:54.160 --> 00:33:55.840]   It also explains,
[00:33:55.840 --> 00:34:00.240]   I think this story explains why they stopped testing their employees for marijuana.
[00:34:00.240 --> 00:34:01.920]   Well, I know why they stopped testing them.
[00:34:01.920 --> 00:34:05.040]   Who cares if they're high?
[00:34:05.040 --> 00:34:06.400]   Nice angle.
[00:34:06.400 --> 00:34:07.360]   Let them be high.
[00:34:07.360 --> 00:34:08.000]   It means why?
[00:34:08.000 --> 00:34:09.040]   Well, there's a business angle.
[00:34:09.040 --> 00:34:10.000]   What's it go...
[00:34:10.000 --> 00:34:11.600]   Explain that story to the facey.
[00:34:11.600 --> 00:34:12.000]   Okay.
[00:34:12.000 --> 00:34:18.160]   Legalizing pot could also open the door to a lucrative new market for the online retailer.
[00:34:18.160 --> 00:34:21.680]   But Amazon says it's not interested in selling pot.
[00:34:22.320 --> 00:34:25.120]   They just want to remove hiring impediments,
[00:34:25.120 --> 00:34:28.240]   which disproportionately impact individuals of color.
[00:34:28.240 --> 00:34:30.960]   So...
[00:34:30.960 --> 00:34:32.560]   And they want to sell a few bongs.
[00:34:32.560 --> 00:34:33.600]   And a few bongs.
[00:34:33.600 --> 00:34:35.760]   Yeah, there are more...
[00:34:35.760 --> 00:34:36.240]   We can't...
[00:34:36.240 --> 00:34:36.720]   They can't...
[00:34:36.720 --> 00:34:40.560]   Petaluma is not allowed legal...
[00:34:40.560 --> 00:34:41.920]   Marijuana is legal in California,
[00:34:41.920 --> 00:34:46.640]   but they've not allowed stores to open selling pot in town.
[00:34:46.640 --> 00:34:48.560]   But there are about 800 bong stores.
[00:34:48.560 --> 00:34:49.440]   I don't know.
[00:34:49.440 --> 00:34:49.680]   Yeah.
[00:34:49.680 --> 00:34:50.080]   It's like...
[00:34:50.080 --> 00:34:50.400]   It's like...
[00:34:50.400 --> 00:34:51.280]   It's like...
[00:34:51.280 --> 00:34:53.520]   Go out of town, get your dope,
[00:34:53.520 --> 00:34:55.440]   but come back to buy the pipe.
[00:34:55.440 --> 00:34:58.720]   Yeah, go 10 minutes away and then come back to town.
[00:34:58.720 --> 00:35:00.960]   I'm deeply...
[00:35:00.960 --> 00:35:01.760]   Elizabeth Warren.
[00:35:01.760 --> 00:35:05.680]   I'm deeply skeptical that Amazon's lobbying is anything more than a self-interested
[00:35:05.680 --> 00:35:08.000]   move to monopolize another market.
[00:35:08.000 --> 00:35:10.240]   Wait a minute.
[00:35:10.240 --> 00:35:10.880]   Wait a minute.
[00:35:10.880 --> 00:35:14.480]   Potentially blocking black and Latino entrepreneurs
[00:35:14.480 --> 00:35:16.000]   from an emerging industry.
[00:35:16.000 --> 00:35:17.600]   Oh, God.
[00:35:17.600 --> 00:35:19.920]   Why are people associating race
[00:35:19.920 --> 00:35:21.520]   with marijuana use?
[00:35:21.520 --> 00:35:23.680]   That seems not so good.
[00:35:23.680 --> 00:35:25.920]   Well, it certainly has been happening in the justice system
[00:35:25.920 --> 00:35:28.880]   where people who are throwing a jail for marijuana sales
[00:35:28.880 --> 00:35:30.560]   are disproportionately people of color.
[00:35:30.560 --> 00:35:31.600]   Sure.
[00:35:31.600 --> 00:35:33.440]   For instances...
[00:35:33.440 --> 00:35:35.120]   I feel like, I mean, in Washington state,
[00:35:35.120 --> 00:35:38.560]   many of the owners of the marijuana dispensaries
[00:35:38.560 --> 00:35:40.320]   are not people of color.
[00:35:40.320 --> 00:35:43.280]   Yeah, and that's the issue is people suffered
[00:35:43.280 --> 00:35:46.320]   from this industry when it was underground.
[00:35:46.320 --> 00:35:49.600]   And now the opportunity should be given to those communities
[00:35:49.600 --> 00:35:51.360]   that were made to suffer by the justice system.
[00:35:51.360 --> 00:35:52.400]   Oh, that's interesting.
[00:35:52.400 --> 00:35:53.920]   That's interesting.
[00:35:53.920 --> 00:35:54.800]   Okay.
[00:35:54.800 --> 00:35:55.360]   That seems...
[00:35:55.360 --> 00:35:55.600]   Yeah.
[00:35:55.600 --> 00:35:56.800]   It's a little...
[00:35:56.800 --> 00:35:58.320]   So I think that's what she's not saying.
[00:35:58.320 --> 00:35:58.880]   It's a little bit...
[00:35:58.880 --> 00:35:59.760]   That's what your point is.
[00:35:59.760 --> 00:36:00.160]   Yeah, it is.
[00:36:00.160 --> 00:36:00.960]   It is a little bit.
[00:36:00.960 --> 00:36:02.080]   What do you think, Ant?
[00:36:02.080 --> 00:36:02.400]   Yeah, I...
[00:36:02.400 --> 00:36:05.040]   I mean, you are a person of color.
[00:36:05.040 --> 00:36:05.360]   Sorry.
[00:36:05.360 --> 00:36:06.480]   Seems like I mute it myself.
[00:36:06.480 --> 00:36:07.840]   Yeah, were you swearing?
[00:36:07.840 --> 00:36:08.560]   Okay, that's good.
[00:36:08.560 --> 00:36:09.920]   Go on.
[00:36:09.920 --> 00:36:10.480]   Go on.
[00:36:10.480 --> 00:36:10.640]   Go on.
[00:36:10.640 --> 00:36:11.040]   Go on.
[00:36:11.040 --> 00:36:11.440]   Work on.
[00:36:11.440 --> 00:36:12.720]   No, I agree.
[00:36:12.720 --> 00:36:14.800]   It is totally disproportionate.
[00:36:14.800 --> 00:36:18.720]   I want to say I mentioned this the other week where
[00:36:19.520 --> 00:36:24.720]   if somebody gets in trouble for a marijuana charge or what have you,
[00:36:24.720 --> 00:36:27.200]   they literally get the book on it.
[00:36:27.200 --> 00:36:28.880]   I mean, they get these resentful things.
[00:36:28.880 --> 00:36:29.760]   It's a disproportionate...
[00:36:29.760 --> 00:36:31.520]   You know, that's the thing.
[00:36:31.520 --> 00:36:32.720]   Well, prosecution too.
[00:36:32.720 --> 00:36:33.040]   Yeah.
[00:36:33.040 --> 00:36:36.000]   Okay.
[00:36:36.000 --> 00:36:39.280]   Flock, here's one from Protocol.
[00:36:39.280 --> 00:36:43.360]   Flock is dead, but topics won't fix Google's ad targeting problems.
[00:36:43.360 --> 00:36:47.760]   It just seems like rearranging deck chairs on the sinking ship
[00:36:47.760 --> 00:36:48.880]   of targeted ads.
[00:36:49.440 --> 00:36:51.280]   I like that headline better.
[00:36:51.280 --> 00:36:52.560]   Yeah, it's not as bad.
[00:36:52.560 --> 00:36:55.920]   Is the ship sinking of targeted ads?
[00:36:55.920 --> 00:36:57.200]   I guess maybe it is.
[00:36:57.200 --> 00:37:00.000]   Well, it's listing to port, heavily.
[00:37:00.000 --> 00:37:00.320]   Yeah.
[00:37:00.320 --> 00:37:01.360]   This.
[00:37:01.360 --> 00:37:02.800]   I can use my number now.
[00:37:02.800 --> 00:37:03.040]   Especially with more regulation.
[00:37:03.040 --> 00:37:04.960]   Oh, go for it.
[00:37:04.960 --> 00:37:06.880]   Okay.
[00:37:06.880 --> 00:37:07.280]   All right.
[00:37:07.280 --> 00:37:07.680]   Thank you.
[00:37:07.680 --> 00:37:10.240]   So, and I've talked about this before,
[00:37:10.240 --> 00:37:15.920]   but Instacart under Fiji CMO, former head of the news feed at Facebook,
[00:37:15.920 --> 00:37:18.960]   the new CEO there, is going to go heavily into advertising.
[00:37:18.960 --> 00:37:20.480]   Makes sense, right?
[00:37:20.480 --> 00:37:23.120]   It's a low margin business, gross, and we can push you.
[00:37:23.120 --> 00:37:26.080]   No, buy the Pepsi net this week, right?
[00:37:26.080 --> 00:37:27.920]   So advertising there, makes sense.
[00:37:27.920 --> 00:37:31.680]   Amazon has built tremendous, quietly,
[00:37:31.680 --> 00:37:34.720]   built a tremendous advertising business that's challenging Google.
[00:37:34.720 --> 00:37:37.920]   Best Buy now sells floor space,
[00:37:37.920 --> 00:37:39.600]   Brent's not floor space, as marketing.
[00:37:39.600 --> 00:37:41.600]   Best Buy is an advertising medium.
[00:37:41.600 --> 00:37:42.640]   They've always done that.
[00:37:42.640 --> 00:37:43.760]   Trust me, sirs.
[00:37:43.760 --> 00:37:48.800]   Have always made more money by pricing product to iLevel or in NCAPs.
[00:37:48.800 --> 00:37:48.960]   Right.
[00:37:48.960 --> 00:37:50.320]   Now there's new players.
[00:37:50.320 --> 00:37:51.840]   Instacart is a new player.
[00:37:51.840 --> 00:37:53.920]   Amazon is a new player in advertising.
[00:37:53.920 --> 00:37:55.760]   Best Buy is a new player in this way.
[00:37:55.760 --> 00:37:59.120]   TV set makers, we talked about that in the show.
[00:37:59.120 --> 00:37:59.360]   Yeah.
[00:37:59.360 --> 00:38:01.280]   What finally occurred to me today in Twitter
[00:38:01.280 --> 00:38:04.800]   was that all these industries are going into advertising,
[00:38:04.800 --> 00:38:06.800]   as media is leaving advertising.
[00:38:06.800 --> 00:38:08.240]   And media said, "Oh, we give up.
[00:38:08.240 --> 00:38:09.120]   We're going to put payables."
[00:38:09.120 --> 00:38:10.640]   I'll give you a different perspective.
[00:38:10.640 --> 00:38:13.600]   All these industries are going into it as margins
[00:38:13.600 --> 00:38:15.360]   from their primary business drop.
[00:38:15.360 --> 00:38:16.720]   Yes.
[00:38:16.720 --> 00:38:18.080]   And what they're doing is they're saying,
[00:38:18.080 --> 00:38:20.240]   "Well, look, people will buy more TVs
[00:38:20.240 --> 00:38:22.880]   if we could charge a third what we used to charge.
[00:38:22.880 --> 00:38:25.040]   We'll make up the difference in putting ads on."
[00:38:25.040 --> 00:38:26.640]   And I understand that.
[00:38:26.640 --> 00:38:29.440]   And it's kind of, in a way, consumers are telling them that.
[00:38:29.440 --> 00:38:30.400]   Drop the price.
[00:38:30.400 --> 00:38:31.520]   We'll put up with the ads.
[00:38:31.520 --> 00:38:35.200]   And they're meeting consumers where they're shopping.
[00:38:35.200 --> 00:38:37.440]   I mean, if I'm no longer, if Instacart,
[00:38:37.440 --> 00:38:38.480]   if I'm using Instacart,
[00:38:38.480 --> 00:38:42.080]   I'm not seeing what the NCAPs that people paid so much money to
[00:38:43.280 --> 00:38:45.600]   to Kroger for getting their stuff there.
[00:38:45.600 --> 00:38:46.640]   How much of this?
[00:38:46.640 --> 00:38:49.520]   How much of this also comes from, to your point exactly, Stacey.
[00:38:49.520 --> 00:38:52.720]   People under 30 don't watch TV anymore.
[00:38:52.720 --> 00:38:54.000]   They don't read newspapers.
[00:38:54.000 --> 00:38:58.720]   We get advertisers because there's no E-Week or PC Week
[00:38:58.720 --> 00:39:01.760]   for enterprise companies to buy ads in.
[00:39:01.760 --> 00:39:03.040]   They're trying desperately to figure out,
[00:39:03.040 --> 00:39:04.560]   "How do we reach our customers?"
[00:39:04.560 --> 00:39:06.240]   I understand that.
[00:39:06.240 --> 00:39:07.280]   That makes sense.
[00:39:07.280 --> 00:39:09.760]   So it's coming in both directions.
[00:39:09.760 --> 00:39:11.440]   Companies trying to lower prices,
[00:39:11.440 --> 00:39:12.960]   find another way to make money.
[00:39:12.960 --> 00:39:15.600]   And advertisers desperate to reach an audience.
[00:39:15.600 --> 00:39:16.480]   That's the same.
[00:39:16.480 --> 00:39:17.280]   Right.
[00:39:17.280 --> 00:39:18.240]   Rather than fixing,
[00:39:18.240 --> 00:39:20.640]   rather than coming up with new models that make advertising work,
[00:39:20.640 --> 00:39:23.680]   the media business is basically giving up.
[00:39:23.680 --> 00:39:24.640]   They're all acting like Apple.
[00:39:24.640 --> 00:39:25.840]   Ah, screw it.
[00:39:25.840 --> 00:39:30.560]   And so you have Wall Street Journal and companies like that
[00:39:30.560 --> 00:39:32.560]   trying to make it more difficult around advertising.
[00:39:32.560 --> 00:39:36.400]   That's why they're ganging up with odd bedfellows here.
[00:39:36.400 --> 00:39:38.720]   And media should be in the business and say,
[00:39:38.720 --> 00:39:42.800]   "No, let's find new services for marketing to hold on to that subsidy."
[00:39:42.800 --> 00:39:45.280]   But they've just surrendered.
[00:39:45.280 --> 00:39:51.200]   So it's an interesting time we live in.
[00:39:51.200 --> 00:39:54.640]   We've always been ad supported.
[00:39:54.640 --> 00:39:58.320]   But as advertisers have kind of,
[00:39:58.320 --> 00:40:00.240]   COVID kind of drove advertising down,
[00:40:00.240 --> 00:40:05.280]   diversity and podcasts drove audience numbers down.
[00:40:05.280 --> 00:40:06.560]   We've seen Dwin and Ling revenue.
[00:40:06.560 --> 00:40:09.360]   That's why we started Club Twit as another way to support us.
[00:40:10.400 --> 00:40:13.680]   And it's interesting because we kind of consciously said,
[00:40:13.680 --> 00:40:15.520]   "Well, if you don't like ads,
[00:40:15.520 --> 00:40:19.280]   would you be willing to pay seven bucks a month for ad-free content?"
[00:40:19.280 --> 00:40:23.600]   And to this day, it's still only a few percentage,
[00:40:23.600 --> 00:40:29.200]   maybe 2% of our total audience that is willing to pay not to have ads.
[00:40:29.200 --> 00:40:32.320]   So, and I'm not sure if that's a reflection of any--
[00:40:32.320 --> 00:40:35.280]   What's even funnier is those that signed up,
[00:40:35.280 --> 00:40:37.600]   we still get a large percentage of them to say,
[00:40:37.600 --> 00:40:38.480]   "We want the ads."
[00:40:38.480 --> 00:40:39.280]   I love the ads.
[00:40:39.280 --> 00:40:40.160]   That's the funniest thing.
[00:40:40.160 --> 00:40:42.160]   I know that cracks me up.
[00:40:42.160 --> 00:40:43.200]   I love the ads.
[00:40:43.200 --> 00:40:44.560]   You're just like, "We want more ads."
[00:40:44.560 --> 00:40:45.840]   What was that the one more?
[00:40:45.840 --> 00:40:47.120]   So some people want to support us.
[00:40:47.120 --> 00:40:47.840]   Well, the pitch is--
[00:40:47.840 --> 00:40:49.280]   To support us.
[00:40:49.280 --> 00:40:51.760]   To support us as well as if you don't want ads.
[00:40:51.760 --> 00:40:53.440]   If you give us seven bucks a month,
[00:40:53.440 --> 00:40:54.800]   we don't need to play ads for you
[00:40:54.800 --> 00:40:56.320]   because you're giving us what we would make
[00:40:56.320 --> 00:40:59.440]   for you from an advertiser.
[00:40:59.440 --> 00:41:00.240]   So that's, you know.
[00:41:00.240 --> 00:41:03.040]   But it's such a small percentage.
[00:41:03.040 --> 00:41:04.240]   I have to think--
[00:41:04.240 --> 00:41:07.680]   It may also reflect the fact that people have figured out how to skip ads.
[00:41:08.400 --> 00:41:09.040]   People are pretty--
[00:41:09.040 --> 00:41:11.520]   Well, it's also not seeing or hearing ads.
[00:41:11.520 --> 00:41:14.800]   NPR stations get 6% to 12% of their audience
[00:41:14.800 --> 00:41:15.920]   giving money to the station.
[00:41:15.920 --> 00:41:19.680]   That took a long time to visit.
[00:41:19.680 --> 00:41:23.200]   And they only do it by bugging the hell out of you a lot.
[00:41:23.200 --> 00:41:24.000]   Exactly.
[00:41:24.000 --> 00:41:26.240]   So could you get to 5%?
[00:41:26.240 --> 00:41:28.320]   It's the frequency of the free to take.
[00:41:28.320 --> 00:41:30.480]   You know what?
[00:41:30.480 --> 00:41:31.840]   I want a sweater vest.
[00:41:31.840 --> 00:41:34.000]   I want a twin sweater vest.
[00:41:34.000 --> 00:41:35.360]   Then I'll give money to you.
[00:41:35.360 --> 00:41:37.600]   Trust me.
[00:41:37.600 --> 00:41:38.720]   You don't want a walk around.
[00:41:38.720 --> 00:41:39.520]   Why gosh, could we all get--
[00:41:39.520 --> 00:41:40.960]   You what?
[00:41:40.960 --> 00:41:41.840]   Twin sweater vest.
[00:41:41.840 --> 00:41:43.760]   Sweater vest and bow ties.
[00:41:43.760 --> 00:41:44.560]   I'm bow ties.
[00:41:44.560 --> 00:41:44.960]   Oh, yes.
[00:41:44.960 --> 00:41:46.960]   It could be so much a horrible--
[00:41:46.960 --> 00:41:48.880]   I'd say it might be me actually on the horn.
[00:41:48.880 --> 00:41:52.960]   I mean, how cute would that be?
[00:41:52.960 --> 00:41:54.720]   When I first started working in radio,
[00:41:54.720 --> 00:41:56.720]   I worked for a funky little station
[00:41:56.720 --> 00:41:59.920]   called K-L-O-K, clock radio.
[00:41:59.920 --> 00:42:00.240]   Right?
[00:42:00.240 --> 00:42:01.200]   Clock radio.
[00:42:01.200 --> 00:42:02.240]   And it was all men.
[00:42:02.240 --> 00:42:04.160]   This is, you know, it was all men.
[00:42:04.160 --> 00:42:07.440]   And so the men from clock--
[00:42:07.920 --> 00:42:10.880]   and they all had mustard yellow jackets
[00:42:10.880 --> 00:42:12.160]   with the clock clock on it.
[00:42:12.160 --> 00:42:12.640]   No.
[00:42:12.640 --> 00:42:13.440]   [LAUGHTER]
[00:42:13.440 --> 00:42:15.520]   Oh, no.
[00:42:15.520 --> 00:42:16.480]   Oh, no.
[00:42:16.480 --> 00:42:17.120]   Eliteus.
[00:42:17.120 --> 00:42:18.080]   And there'd be billboards.
[00:42:18.080 --> 00:42:19.120]   They're men from clock,
[00:42:19.120 --> 00:42:21.760]   and they're mustard colored blazers.
[00:42:21.760 --> 00:42:23.040]   So--
[00:42:23.040 --> 00:42:25.200]   I'm going to say no to mustard.
[00:42:25.200 --> 00:42:27.760]   If that is an unfliring color on a lot of people.
[00:42:27.760 --> 00:42:28.000]   Say.
[00:42:28.000 --> 00:42:30.640]   But I'll wear a blazer.
[00:42:30.640 --> 00:42:31.200]   Thank you, Missy.
[00:42:31.200 --> 00:42:31.680]   Fezes.
[00:42:31.680 --> 00:42:33.600]   We could all wear fezes, if you want.
[00:42:33.600 --> 00:42:37.280]   I feel like that could be culturally appropriate.
[00:42:37.280 --> 00:42:37.840]   [LAUGHTER]
[00:42:37.840 --> 00:42:39.200]   We'd have to investigate that.
[00:42:39.200 --> 00:42:41.360]   Even--
[00:42:41.360 --> 00:42:42.080]   I don't know.
[00:42:42.080 --> 00:42:43.840]   After Attiterk said, we don't want them.
[00:42:43.840 --> 00:42:45.760]   I think we can-- anybody there ever--
[00:42:45.760 --> 00:42:46.400]   open all.
[00:42:46.400 --> 00:42:47.120]   Well, yeah.
[00:42:47.120 --> 00:42:47.600]   Open all.
[00:42:47.600 --> 00:42:48.560]   [LAUGHTER]
[00:42:48.560 --> 00:42:50.560]   I want to take a little break.
[00:42:50.560 --> 00:42:52.240]   Come back with more with this.
[00:42:52.240 --> 00:42:54.880]   In fact, I am going to do something unheard of.
[00:42:54.880 --> 00:42:58.320]   Stacy picks the next story.
[00:42:58.320 --> 00:42:59.840]   Wait a minute.
[00:42:59.840 --> 00:43:00.400]   Yeah.
[00:43:00.400 --> 00:43:01.360]   Oh, you can do it later.
[00:43:01.360 --> 00:43:02.000]   Is this a democracy?
[00:43:02.000 --> 00:43:04.000]   [LAUGHTER]
[00:43:04.000 --> 00:43:06.000]   [LAUGHTER]
[00:43:06.000 --> 00:43:08.480]   Maybe it's a-- a stacyocracy.
[00:43:08.480 --> 00:43:10.320]   It's a stacyocracy.
[00:43:10.320 --> 00:43:11.920]   That's the best kind ofocracy.
[00:43:11.920 --> 00:43:12.400]   [LAUGHTER]
[00:43:12.400 --> 00:43:12.640]   Yeah.
[00:43:12.640 --> 00:43:15.520]   Stacyocracy coming up next.
[00:43:15.520 --> 00:43:17.520]   But first, a word from our sponsor.
[00:43:17.520 --> 00:43:21.280]   And the other thing is, I like to choose sponsors that we like,
[00:43:21.280 --> 00:43:23.040]   that we have a relationship with.
[00:43:23.040 --> 00:43:26.960]   And I feel like it's not an ad to kind of twist your arm.
[00:43:26.960 --> 00:43:29.520]   It's more like, I'd like to introduce you to something.
[00:43:29.520 --> 00:43:32.800]   And I think this is one thing I tell my kids this.
[00:43:35.040 --> 00:43:36.720]   I want to introduce you to Wealthfront.
[00:43:36.720 --> 00:43:42.960]   So, you know, when you're young, you want to do the latest thing.
[00:43:42.960 --> 00:43:44.160]   I want to do NFTs.
[00:43:44.160 --> 00:43:45.200]   I want to buy Bitcoin.
[00:43:45.200 --> 00:43:47.280]   My daughter wanted to buy Dogecoin.
[00:43:47.280 --> 00:43:48.400]   I said, honey, it's fine.
[00:43:48.400 --> 00:43:50.080]   You know, you can play with that.
[00:43:50.080 --> 00:43:51.680]   But please, would you put the bulk--
[00:43:51.680 --> 00:43:54.160]   first of all, are you saving for your retirement?
[00:43:54.160 --> 00:43:56.240]   When you're 25, you don't think about that.
[00:43:56.240 --> 00:44:00.880]   And if only I had, I'd have so much more if I'd started young.
[00:44:00.880 --> 00:44:03.360]   Start now, put some money aside,
[00:44:03.360 --> 00:44:06.560]   save a little bit for playtime, for day trading, whatever.
[00:44:06.560 --> 00:44:09.120]   This week at Google is brought to you by Wealthfront.
[00:44:09.120 --> 00:44:12.800]   It makes it easy to invest, easy to grow your savings.
[00:44:12.800 --> 00:44:14.400]   With what every expert tells you,
[00:44:14.400 --> 00:44:21.120]   you need a diversified portfolio that balances your other riskier/more fun bits.
[00:44:21.120 --> 00:44:25.200]   You can start investing in no time with Wealthfront's classic portfolio.
[00:44:25.200 --> 00:44:27.760]   But you can also make it your own with the things you care about.
[00:44:27.760 --> 00:44:29.360]   I'd be really like this.
[00:44:29.360 --> 00:44:31.280]   Social responsible funds.
[00:44:32.240 --> 00:44:33.600]   You could do a technology fund.
[00:44:33.600 --> 00:44:35.360]   I think a lot of you might be interested in this.
[00:44:35.360 --> 00:44:38.240]   Yeah, if you want to do crypto, they've got crypto trusts.
[00:44:38.240 --> 00:44:40.560]   There are hundreds of other investments.
[00:44:40.560 --> 00:44:44.640]   Wealthfront was designed by financial experts to do the right thing,
[00:44:44.640 --> 00:44:47.200]   to help you turn your ideas into great investments.
[00:44:47.200 --> 00:44:51.360]   But without having you have to do everything yourself.
[00:44:51.360 --> 00:44:53.440]   You'll see, if you just look at the Wealthfront site,
[00:44:53.440 --> 00:44:55.680]   you could pick the category you want to invest in.
[00:44:55.680 --> 00:44:57.520]   But then they'll do it right.
[00:44:57.520 --> 00:45:01.760]   For instance, they'll do tax loss harvesting, lower your tax bill.
[00:45:01.760 --> 00:45:02.880]   That's complicated.
[00:45:02.880 --> 00:45:03.840]   They help you do that.
[00:45:03.840 --> 00:45:06.560]   They will rebalance your portfolio.
[00:45:06.560 --> 00:45:07.920]   Do you know what rebalancing is?
[00:45:07.920 --> 00:45:10.400]   Did you know you're supposed to do that at least quarterly?
[00:45:10.400 --> 00:45:13.600]   Well, Front does it for you automatically.
[00:45:13.600 --> 00:45:15.280]   It's doing the right thing.
[00:45:15.280 --> 00:45:17.280]   That's the experts at work there.
[00:45:17.280 --> 00:45:21.760]   Now today, since we started talking about Wealthfront some years ago,
[00:45:21.760 --> 00:45:27.200]   $28 billion in assets, half a million people building their Wealth at Wealthfront.
[00:45:27.200 --> 00:45:28.480]   It's easy to get started.
[00:45:29.200 --> 00:45:32.480]   $500 to get started, grow your Wealth the easy way,
[00:45:32.480 --> 00:45:35.680]   prepare for retirement or buy in that house or the kids in college,
[00:45:35.680 --> 00:45:38.160]   all of the stuff you should be saving for.
[00:45:38.160 --> 00:45:43.120]   If you want to have fun investing to the moon with your diamond hands, that's fine.
[00:45:43.120 --> 00:45:45.280]   But Wealthfront is so simple, so powerful.
[00:45:45.280 --> 00:45:46.720]   It's got an app.
[00:45:46.720 --> 00:45:48.880]   The App Store app is great.
[00:45:48.880 --> 00:45:51.440]   4.9 out of 5 stars in the Apple App Store.
[00:45:51.440 --> 00:45:52.960]   So start building your Wealth.
[00:45:52.960 --> 00:45:56.880]   By the way, we've got a nice little enticement for you.
[00:45:56.880 --> 00:46:02.160]   Your first $5,000 will be managed free for life forever.
[00:46:02.160 --> 00:46:03.920]   What a great way to start that nest egg.
[00:46:03.920 --> 00:46:07.360]   Wealthfront.com/twig.
[00:46:07.360 --> 00:46:10.560]   It's just the easy way to do it, the right way to do it.
[00:46:10.560 --> 00:46:14.800]   W-e-a-l-t-h-f-r-o-n-t.com/twig.
[00:46:14.800 --> 00:46:16.320]   Start building your Wealthfront.
[00:46:16.320 --> 00:46:18.560]   .com/twig.
[00:46:18.560 --> 00:46:23.120]   We thank them for supporting Twig and you're supporting us by using that address.
[00:46:23.120 --> 00:46:24.960]   So please get the /twig in there.
[00:46:24.960 --> 00:46:28.720]   Wealthfront.com/twig.
[00:46:28.720 --> 00:46:32.800]   There were so many good things to choose from Stacy.
[00:46:32.800 --> 00:46:34.880]   What did you, what caught your-
[00:46:34.880 --> 00:46:38.080]   I want to talk about IDME and the IRS.
[00:46:38.080 --> 00:46:40.480]   Perfect. That's exactly what I was going to talk about.
[00:46:40.480 --> 00:46:41.840]   Oh good.
[00:46:41.840 --> 00:46:43.760]   Because it's really confusing.
[00:46:43.760 --> 00:46:48.480]   So the IRS, and they've kind of backtracked a little bit on this,
[00:46:48.480 --> 00:46:50.000]   but I don't think the backtrack is real.
[00:46:50.000 --> 00:46:51.600]   It's a little confusing.
[00:46:52.320 --> 00:46:53.440]   Yes.
[00:46:53.440 --> 00:46:56.880]   I've seen on the site even, if you go to irs.gov,
[00:46:56.880 --> 00:46:59.120]   they say coming this summer,
[00:46:59.120 --> 00:47:03.280]   if you wanted, there's certain things they want to extra security, right?
[00:47:03.280 --> 00:47:05.680]   So they've caught, this is the troubling thing.
[00:47:05.680 --> 00:47:08.000]   They've contracted with the third party, a company.
[00:47:08.000 --> 00:47:12.400]   A lot of states are working with, called ID.me.
[00:47:12.400 --> 00:47:17.760]   The states are doing it to try to eliminate fraud in things like COVID loans
[00:47:17.760 --> 00:47:21.120]   and, you know, unemployment.
[00:47:21.680 --> 00:47:24.800]   And ID.me is a way of authenticating yourself.
[00:47:24.800 --> 00:47:27.600]   Okay. So there's the kind of the synopsis.
[00:47:27.600 --> 00:47:30.480]   Tell me, Stacy, what you think of this story.
[00:47:30.480 --> 00:47:34.320]   Well, originally I was like, what the what?
[00:47:34.320 --> 00:47:37.680]   Because at first it sounded like the IRS was going to make all of us upload our
[00:47:37.680 --> 00:47:38.880]   biometric information.
[00:47:38.880 --> 00:47:42.800]   And I was like, this is a bad plan for so many reasons, right?
[00:47:42.800 --> 00:47:43.200]   What can I do?
[00:47:43.200 --> 00:47:44.320]   And now it's coming in.
[00:47:44.320 --> 00:47:46.320]   Yeah, I'm like, oh.
[00:47:47.280 --> 00:47:52.720]   And then now they're saying, no, you can still pay your taxes and do things.
[00:47:52.720 --> 00:47:57.360]   It's going to be a little bit more onerous, but you can still not use this.
[00:47:57.360 --> 00:48:02.480]   But in totally separate news, and I'm still trying to figure out what the heck
[00:48:02.480 --> 00:48:04.240]   ID.me is actually using.
[00:48:04.240 --> 00:48:13.120]   So they're saying they're not using facial recognition, but they are.
[00:48:13.120 --> 00:48:16.000]   And this is this is where I'm like, this is why we're talking about this.
[00:48:16.000 --> 00:48:20.160]   Because I am like trying to figure out things they could be doing with the IRS.
[00:48:20.160 --> 00:48:25.200]   You know, they're saying you take a selfies and sometimes they want to be moving, by the way.
[00:48:25.200 --> 00:48:27.280]   So it's not just a picture of a picture.
[00:48:27.280 --> 00:48:28.000]   So a selfie.
[00:48:28.000 --> 00:48:28.800]   Hi, it's me.
[00:48:28.800 --> 00:48:33.600]   And we will use that picture view to verify that it's you.
[00:48:33.600 --> 00:48:36.240]   But then they were accused.
[00:48:36.240 --> 00:48:39.120]   That's called one to one face, face recognition, right?
[00:48:39.120 --> 00:48:43.040]   But then they were accused of doing something that is much more disturbing,
[00:48:43.040 --> 00:48:49.120]   which is called one to many face matching steps, which means
[00:48:49.120 --> 00:48:59.040]   it's the kind of thing that Clearview AI was using, where I take a picture of Stacy,
[00:48:59.040 --> 00:49:03.200]   and then I try to match it in a database of many people.
[00:49:03.200 --> 00:49:08.400]   And that's a lot more concerning because it's not then just about authentication.
[00:49:08.400 --> 00:49:10.240]   You're being added to a large database.
[00:49:12.320 --> 00:49:17.920]   So in his LinkedIn post, Blake Hall, who, uh, the CEO of ID,
[00:49:17.920 --> 00:49:22.320]   the CEO says, he made it clear as mud.
[00:49:22.320 --> 00:49:27.120]   We avoid disclosing methods we use to stop identity theft and organize crime,
[00:49:27.120 --> 00:49:29.120]   because it jeopardizes their effectiveness.
[00:49:29.120 --> 00:49:29.520]   Okay.
[00:49:29.520 --> 00:49:32.960]   Criminals are constantly a lot of excuses.
[00:49:32.960 --> 00:49:39.280]   ID.me uses a specific one to many check on selfies tied to government programs,
[00:49:39.280 --> 00:49:42.880]   targeted by organized crime to prevent prolific identity thieves.
[00:49:42.880 --> 00:49:45.920]   In other words, they're going to match your picture against a database.
[00:49:45.920 --> 00:49:49.040]   It's almost like going to the police department and looking through the mug sheet
[00:49:49.040 --> 00:49:51.840]   to see if you're a member of organized crime.
[00:49:51.840 --> 00:49:58.640]   He says this step is internal to ID.me and does not involve any external or government database.
[00:49:58.640 --> 00:50:04.320]   It occurs once during enrollment and exists to make sure a single attacker is not registering
[00:50:04.320 --> 00:50:05.360]   multiple identities.
[00:50:05.360 --> 00:50:06.240]   That kind of makes sense.
[00:50:06.240 --> 00:50:08.640]   The one to one would not register that.
[00:50:09.280 --> 00:50:11.280]   It wouldn't know if you've done this 12 times.
[00:50:11.280 --> 00:50:15.120]   Here's his, here's his out.
[00:50:15.120 --> 00:50:18.480]   This step is not tied to identity verification.
[00:50:18.480 --> 00:50:22.800]   It does not block legitimate users from verifying their identity,
[00:50:22.800 --> 00:50:25.600]   nor is it used for any other purpose than to prevent identity theft.
[00:50:25.600 --> 00:50:30.800]   Data shows that removing this control would immediately lead to significant
[00:50:30.800 --> 00:50:32.800]   identity theft and organized crime.
[00:50:32.800 --> 00:50:37.760]   The one to one face match step is the only step we use to verify identity.
[00:50:38.560 --> 00:50:40.400]   [laughter]
[00:50:40.400 --> 00:50:44.000]   But it sounds to me like he's also saying, but we're also going to do the one to many.
[00:50:44.000 --> 00:50:50.080]   So it sounds, here's what I'm wondering, because Brian Krebs went through the process to
[00:50:50.080 --> 00:50:51.040]   register his face.
[00:50:51.040 --> 00:50:56.720]   And he said, so as part of that, there was a person who actually registered.
[00:50:56.720 --> 00:50:58.000]   So that's the one to one.
[00:50:58.000 --> 00:51:02.400]   And now Brian Krebs has his IRS ID.me profile.
[00:51:02.400 --> 00:51:02.640]   Right.
[00:51:02.640 --> 00:51:07.680]   What I'm guessing is if Brian Krebs were a fraudster and tried to do it again under the name
[00:51:07.680 --> 00:51:15.920]   of Pete Johnson, Pete Johnson, when he went to try to do this, that would be a one to one.
[00:51:15.920 --> 00:51:22.080]   But they would have to check the face of Pete Johnson against their one to many database.
[00:51:22.080 --> 00:51:22.160]   Right.
[00:51:22.160 --> 00:51:24.800]   And they would recognize that Pete Johnson is also Brian Krebs.
[00:51:24.800 --> 00:51:25.120]   Right.
[00:51:25.120 --> 00:51:27.440]   That's what I'm hoping they're trying to say.
[00:51:27.440 --> 00:51:28.480]   I think that's what they are saying.
[00:51:28.480 --> 00:51:29.280]   I don't know.
[00:51:29.280 --> 00:51:29.520]   Yeah.
[00:51:29.520 --> 00:51:31.040]   Which is okay.
[00:51:31.040 --> 00:51:33.440]   That's not terrible.
[00:51:33.440 --> 00:51:36.720]   But that means every single, so this is my problem.
[00:51:36.720 --> 00:51:41.920]   Every single time I set up an ID me account, which I'm going to need to log into the IRS
[00:51:41.920 --> 00:51:45.680]   for certain things like paying your taxes or getting your refund.
[00:51:45.680 --> 00:51:46.800]   Well, you don't have to.
[00:51:46.800 --> 00:51:49.680]   That's what the IRS has said that you don't have to.
[00:51:49.680 --> 00:51:52.880]   But you're going to be more of me too shortly.
[00:51:52.880 --> 00:51:56.160]   This is confusing unless they've changed your policy.
[00:51:56.160 --> 00:51:58.640]   Because here's the page Brian Krebs publishes.
[00:51:58.640 --> 00:52:03.600]   If you have an existing IRS username, please create a new ID me account as soon as possible.
[00:52:04.320 --> 00:52:09.120]   You won't be able to log in with your existing IRS name and password starting summer 2022.
[00:52:09.120 --> 00:52:11.520]   Now, maybe they've changed that.
[00:52:11.520 --> 00:52:18.800]   Gizmodo's story has an editor's note saying that you can still file and pay taxes without
[00:52:18.800 --> 00:52:24.000]   logging into an IRS account or providing biometric data, which contradicts information
[00:52:24.000 --> 00:52:27.200]   an IRS spokesperson previously provided.
[00:52:27.200 --> 00:52:31.680]   They also pointed out that this was a really frustrating correction.
[00:52:32.640 --> 00:52:36.160]   And if you go down to...
[00:52:36.160 --> 00:52:37.360]   I think the thing is,
[00:52:37.360 --> 00:52:40.560]   pay taxes.
[00:52:40.560 --> 00:52:40.960]   Fine.
[00:52:40.960 --> 00:52:43.520]   If you're going to give us money, fine.
[00:52:43.520 --> 00:52:46.640]   You want to claim a refund, you're going to need to use ID.me.
[00:52:46.640 --> 00:52:50.800]   I think this is the weasel words.
[00:52:50.800 --> 00:52:56.000]   There are some things you don't need to use ID.me for, but there are also some things you
[00:52:56.000 --> 00:52:57.760]   must use ID.me for.
[00:52:57.760 --> 00:52:59.680]   So my problem, there are a couple.
[00:52:59.680 --> 00:53:01.040]   One is it's a third party.
[00:53:01.040 --> 00:53:07.360]   I guess, I mean, but they're going to have a lot of face recognition information because
[00:53:07.360 --> 00:53:10.640]   with a one to many system, they have to store everybody's face.
[00:53:10.640 --> 00:53:12.400]   With a one to one system, they don't.
[00:53:12.400 --> 00:53:15.200]   With a one to many, they have to store everybody's face.
[00:53:15.200 --> 00:53:19.520]   That means they're going to have a database of every single person who logs into the IRS
[00:53:19.520 --> 00:53:20.400]   using the system.
[00:53:20.400 --> 00:53:23.760]   ID me has that, not the IRS.
[00:53:23.760 --> 00:53:24.480]   No, an ID.
[00:53:24.480 --> 00:53:25.520]   Well, that's my problem.
[00:53:27.360 --> 00:53:29.040]   Okay. Well, no, no, no, no, I get it.
[00:53:29.040 --> 00:53:29.440]   I get it.
[00:53:29.440 --> 00:53:34.880]   But ID me already has this database because it already has many people needed a base.
[00:53:34.880 --> 00:53:35.520]   And then that means.
[00:53:35.520 --> 00:53:36.720]   Yeah, right.
[00:53:36.720 --> 00:53:38.560]   Well, until you get your.
[00:53:38.560 --> 00:53:40.000]   Okay.
[00:53:40.000 --> 00:53:42.560]   So your issue is you don't want to use to buy the way.
[00:53:42.560 --> 00:53:45.600]   The IRS used to use an Equifax to do this.
[00:53:45.600 --> 00:53:48.400]   So that was a problem.
[00:53:48.400 --> 00:53:51.360]   They got rid of that because and here's the problem.
[00:53:51.360 --> 00:53:52.640]   Why do they get rid of Equifax?
[00:53:52.640 --> 00:53:54.480]   Because they had a massive data breach.
[00:53:56.080 --> 00:54:03.040]   So now they got ID me and a data breach here would be 10 times worse because it wouldn't just be
[00:54:03.040 --> 00:54:06.960]   your social security number, it'd be your social security number and your face.
[00:54:06.960 --> 00:54:10.080]   And you're right.
[00:54:10.080 --> 00:54:14.880]   Like it's still unclear if the IR, I think what's happening is there's a lot of backlash
[00:54:14.880 --> 00:54:19.040]   and the IRS is suddenly like, uh, I don't know if we can force everybody.
[00:54:19.040 --> 00:54:20.640]   I mean, because you have to pay your taxes.
[00:54:20.640 --> 00:54:21.600]   You can't.
[00:54:21.600 --> 00:54:23.360]   I mean, basically you're legally now.
[00:54:23.360 --> 00:54:25.360]   That's why they're saying you can pay your taxes with that.
[00:54:25.360 --> 00:54:28.240]   They never implied that you couldn't pay your taxes.
[00:54:28.240 --> 00:54:31.680]   It's if you want to hit there's some things that you have to do it for.
[00:54:31.680 --> 00:54:32.080]   And so.
[00:54:32.080 --> 00:54:34.800]   What is a good system for verified it?
[00:54:34.800 --> 00:54:35.840]   Well, this is the problem.
[00:54:35.840 --> 00:54:36.880]   This is a good system.
[00:54:36.880 --> 00:54:42.960]   Ideally you would have.
[00:54:42.960 --> 00:54:46.880]   I mean, would you make the underlying security aspects of it?
[00:54:46.880 --> 00:54:53.280]   Not open source, but like, I mean, yes, you need a database of faces and information.
[00:54:53.280 --> 00:54:55.520]   If you're going to use faces for security.
[00:54:55.520 --> 00:55:00.000]   Cribs, Cribs looked at their security and he said, you know, they do all the security in depth,
[00:55:00.000 --> 00:55:00.960]   stuff, blah, blah, blah, blah.
[00:55:00.960 --> 00:55:02.480]   So maybe it is secure.
[00:55:02.480 --> 00:55:03.520]   Here's the real problem.
[00:55:03.520 --> 00:55:08.720]   There are, you can't change your face really or fingerprints.
[00:55:08.720 --> 00:55:12.720]   When you're giving people biometrics, you're giving them something permanent.
[00:55:12.720 --> 00:55:15.200]   Even a social security number can be changed.
[00:55:15.200 --> 00:55:17.440]   But when you're given biometrics, that's it.
[00:55:17.440 --> 00:55:18.240]   It's over.
[00:55:18.240 --> 00:55:21.840]   And if that if that leaks out, that's becomes.
[00:55:21.840 --> 00:55:24.240]   And if it's used as a basis for.
[00:55:24.240 --> 00:55:25.040]   Unfixable problem.
[00:55:25.040 --> 00:55:26.480]   Yes, an unfixable problem.
[00:55:26.480 --> 00:55:28.880]   The problem is that is not the data.
[00:55:28.880 --> 00:55:30.160]   Because everything is breached.
[00:55:30.160 --> 00:55:33.680]   The problem is using bad structures for transactions.
[00:55:33.680 --> 00:55:36.400]   And then that's the problem.
[00:55:36.400 --> 00:55:39.120]   It's like the social security number wasn't supposed to be used for everything, right?
[00:55:39.120 --> 00:55:40.960]   But it became used for everything.
[00:55:40.960 --> 00:55:44.960]   So what if the face ends up being used for every banking thing and then it gets misused?
[00:55:44.960 --> 00:55:46.000]   And that's where the problem is.
[00:55:46.000 --> 00:55:47.360]   There's a further issue because.
[00:55:48.320 --> 00:55:53.200]   ID.me looks like they've not been fully transparent on this.
[00:55:53.200 --> 00:55:54.800]   At first they say, oh, no, it's one to one.
[00:55:54.800 --> 00:55:56.240]   And they said, but we have to use one to many.
[00:55:56.240 --> 00:55:59.360]   Oh, now we're not going to tell you because we don't got crips to know.
[00:55:59.360 --> 00:56:04.400]   And this is what Caitlin Sealy George, who's the campaign director for Fight for the Future,
[00:56:04.400 --> 00:56:09.360]   says, we already know this company is willing to say anything in order to get more government
[00:56:09.360 --> 00:56:10.400]   contracts.
[00:56:10.400 --> 00:56:15.680]   The CEO of ID.me has been peddling erroneous numbers about unemployment benefit fraud.
[00:56:15.680 --> 00:56:18.800]   But the fact that the IRS knew about this crips discrepancy is a big problem.
[00:56:18.800 --> 00:56:24.400]   The only responsible thing for the IRS and any other state or federal agency using ID.me
[00:56:24.400 --> 00:56:27.600]   to do is to stop these contracts immediately.
[00:56:27.600 --> 00:56:36.400]   So there's a lot of the buyers like, well, here's my, here's my big worry and issue.
[00:56:36.400 --> 00:56:38.320]   And we're already getting there.
[00:56:38.320 --> 00:56:44.000]   But if you have a government entity, sure, this is the kind of services side of the government.
[00:56:44.000 --> 00:56:48.080]   But having a database of your face and all the relevant information about you.
[00:56:48.080 --> 00:56:54.080]   In that it becomes centralized, that can be used across.
[00:56:54.080 --> 00:57:00.400]   My worries, it can be used across all government agencies to find you or to like,
[00:57:00.400 --> 00:57:03.200]   technically, I guess it already can be.
[00:57:03.200 --> 00:57:06.480]   And we're already seeing that with like the nationwide ID programs.
[00:57:06.480 --> 00:57:13.760]   But understanding who has access to your face data and all the personally identifiable
[00:57:13.760 --> 00:57:18.160]   information associated with that and what they can use that for.
[00:57:18.160 --> 00:57:22.720]   Can they use it for tracking you if you haven't paid a parking ticket using like city cameras?
[00:57:22.720 --> 00:57:30.000]   Can they sell that to a private company to track like what stores you going?
[00:57:30.000 --> 00:57:35.280]   I mean, like my concern is this is really rich, valuable data.
[00:57:35.280 --> 00:57:42.720]   And I want to see really hard rules about how it's stored, secured, and then limited in use.
[00:57:42.720 --> 00:57:44.720]   Okay, but Stacy, it's not going to be secure.
[00:57:44.720 --> 00:57:46.480]   There's going to be a breach.
[00:57:46.480 --> 00:57:49.040]   So then the next question becomes, like you're right over there.
[00:57:49.040 --> 00:57:49.840]   That's problematic.
[00:57:49.840 --> 00:57:53.680]   The next question is, if it is breached, what could be done with it?
[00:57:53.680 --> 00:57:57.120]   What else would depend upon the same data that could then be misused?
[00:57:57.120 --> 00:58:03.920]   Well, I think, I mean, I actually don't think being breached is the worst case scenario.
[00:58:03.920 --> 00:58:08.880]   I think the worst case scenario is the government has given all this valuable data to a private
[00:58:08.880 --> 00:58:14.880]   company who could then turn around and sell that data to, I don't know, somebody who
[00:58:14.880 --> 00:58:23.920]   makes traffic cameras and then you suddenly have or parking lot cameras.
[00:58:23.920 --> 00:58:29.760]   So you know where Stacy Higginbotham is based on camera data wherever I am in my day, for example.
[00:58:29.760 --> 00:58:36.080]   And I mean, it's kind of like location data from phones, which we might talk about later,
[00:58:36.080 --> 00:58:40.880]   because there's a lawsuit about that. That's more my concern.
[00:58:40.880 --> 00:58:45.040]   I mean, yes, it probably will get breached, but yeah.
[00:58:45.040 --> 00:58:51.040]   Yeah, and will the government's database, would that be any more secure than ID.me?
[00:58:51.040 --> 00:58:54.480]   I mean, I think a lot of this comes.
[00:58:54.480 --> 00:58:54.640]   No.
[00:58:54.640 --> 00:58:56.800]   Yeah, I mean, that's part of the problem.
[00:58:56.800 --> 00:58:57.360]   Yeah.
[00:58:57.360 --> 00:59:02.320]   This is, I don't, I don't.
[00:59:02.320 --> 00:59:07.520]   Assembling all those valuable stuff and we're like, yeah, we know that security is not 100%,
[00:59:07.520 --> 00:59:12.320]   but hey, let's take all of our gold and stick it someplace right here.
[00:59:12.320 --> 00:59:16.480]   Throw the government a bone. They may secure it. I mean, we just had the whole
[00:59:16.480 --> 00:59:23.440]   COVID testing site go up that was hosted by the government that didn't crash as soon as we
[00:59:23.440 --> 00:59:26.320]   launched it. So maybe there's hope on security too.
[00:59:27.360 --> 00:59:33.600]   Well, and the government may already have your photo because they use DMV photos.
[00:59:33.600 --> 00:59:35.040]   So, oh yeah.
[00:59:35.040 --> 00:59:35.520]   Oh, yeah.
[00:59:35.520 --> 00:59:36.000]   Oh, yeah.
[00:59:36.000 --> 00:59:36.480]   Oh, yeah.
[00:59:36.480 --> 00:59:42.960]   We started that with, they started now taking your DMV photos and creating the nationwide database.
[00:59:42.960 --> 00:59:48.000]   Remember, initially it was statewide and then several states signed up to
[00:59:48.000 --> 00:59:49.680]   participate in the nationwide and now everybody.
[00:59:49.680 --> 00:59:56.240]   641 million drivers license and ID photos from 21 states went to federal law enforcement databases.
[00:59:56.960 --> 00:59:59.760]   And then we shared with other 130 million people.
[00:59:59.760 --> 01:00:00.560]   Yeah.
[01:00:00.560 --> 01:00:02.720]   Why do we have so many driver's licenses?
[01:00:02.720 --> 01:00:06.000]   That's a good point.
[01:00:06.000 --> 01:00:08.480]   There's more driver's licenses than there are people.
[01:00:08.480 --> 01:00:10.720]   Uh, I don't know.
[01:00:10.720 --> 01:00:11.680]   That's not right.
[01:00:11.680 --> 01:00:13.040]   This is consumer reports.
[01:00:13.040 --> 01:00:13.680]   I trust them.
[01:00:13.680 --> 01:00:14.240]   I don't know.
[01:00:14.240 --> 01:00:15.680]   Federal agencies.
[01:00:15.680 --> 01:00:16.160]   Okay.
[01:00:16.160 --> 01:00:16.320]   Sorry.
[01:00:16.320 --> 01:00:18.000]   No, I'm just like, wait, we don't have that population.
[01:00:18.000 --> 01:00:19.520]   Thanks for the sanity check.
[01:00:19.520 --> 01:00:20.000]   Yeah.
[01:00:20.000 --> 01:00:20.480]   I don't know.
[01:00:20.480 --> 01:00:24.800]   Well, I mean, how many pictures have you had taken over the years?
[01:00:24.800 --> 01:00:28.720]   A lot.
[01:00:28.720 --> 01:00:34.240]   About 2016, the Center on Privacy and Technology reported about half of all American adult faces
[01:00:34.240 --> 01:00:37.520]   are already in police facial recognition databases.
[01:00:37.520 --> 01:00:42.400]   So maybe this, maybe it's silly to worry about this because this, this ship has sailed.
[01:00:42.400 --> 01:00:44.160]   Federal.
[01:00:44.160 --> 01:00:52.000]   Then we have to figure out rules for how they can use that in, you know, FBI uses DMV photos.
[01:00:53.360 --> 01:00:55.520]   ICE uses DMV photos.
[01:00:55.520 --> 01:00:57.600]   Uh, I mean, it's not.
[01:00:57.600 --> 01:00:58.880]   Oh boy.
[01:00:58.880 --> 01:00:59.200]   Yeah.
[01:00:59.200 --> 01:01:02.400]   Yeah.
[01:01:02.400 --> 01:01:06.880]   And actually this is an article in the Consumer Reports from three years ago,
[01:01:06.880 --> 01:01:11.600]   but they even addressed this thing about the difference between a phone face ID on a phone,
[01:01:11.600 --> 01:01:14.640]   which is one to one and one to many.
[01:01:14.640 --> 01:01:18.240]   When we do, what we do on a phone is authentication.
[01:01:18.240 --> 01:01:21.680]   We're simply comparing two face images, but when law enforcement is searching
[01:01:21.680 --> 01:01:25.040]   for a person of interest, the system has to search against a large database
[01:01:25.040 --> 01:01:27.680]   of millions or even a hundred million photos.
[01:01:27.680 --> 01:01:30.080]   And that introduces this problem.
[01:01:30.080 --> 01:01:35.600]   The accuracy of facial recognition goes down as the size of a database increases.
[01:01:35.600 --> 01:01:38.320]   That's one of the concerns about using it for criminal law enforcement.
[01:01:38.320 --> 01:01:44.800]   Uh, is it's not, you know, and we've seen this, people going to jail incorrectly because
[01:01:44.800 --> 01:01:47.680]   they were identified in a digital lineup.
[01:01:47.680 --> 01:01:49.440]   So.
[01:01:51.200 --> 01:01:55.520]   Anyway, it's a, I'm glad you brought it up because that, that is exactly one of the topics I wanted
[01:01:55.520 --> 01:01:56.000]   to talk about.
[01:01:56.000 --> 01:01:58.080]   And I don't know what I think I want.
[01:01:58.080 --> 01:02:00.080]   I mean, we certainly want it to be more secure.
[01:02:00.080 --> 01:02:02.640]   Maybe the government already does have all those photos anyway.
[01:02:02.640 --> 01:02:07.760]   So, but should this third party ID dot maybe allowed to collect all of these.
[01:02:07.760 --> 01:02:08.880]   How much fraud is there now?
[01:02:08.880 --> 01:02:10.480]   What's the problem they're trying to solve?
[01:02:10.480 --> 01:02:16.480]   Big apparently a big, according to, well, and this is the other thing ID.me CEO says,
[01:02:16.480 --> 01:02:18.000]   Oh, it's big.
[01:02:18.000 --> 01:02:19.360]   Hey, probably we're here.
[01:02:19.360 --> 01:02:20.320]   We're just here to solve.
[01:02:20.400 --> 01:02:20.720]   Yeah.
[01:02:20.720 --> 01:02:27.600]   There's a lot of fraud for parents committing tax fraud against their kids.
[01:02:27.600 --> 01:02:30.720]   So they actually like, but this wouldn't solve that.
[01:02:30.720 --> 01:02:34.880]   Cause if you're the first person to authenticate is like, if I authenticated as my daughter,
[01:02:34.880 --> 01:02:38.960]   for example, then I become the canonical image for her.
[01:02:38.960 --> 01:02:43.360]   ID.me has said there's been massive unemployment benefit fraud.
[01:02:43.360 --> 01:02:48.320]   They say 30% of states have seen what is the number they said.
[01:02:48.320 --> 01:02:50.720]   Washington lost like a billion dollars in.
[01:02:50.720 --> 01:02:57.920]   Many states, this is from last year, are seeing a 30% fraud rate, according to ID.me.
[01:02:57.920 --> 01:03:02.400]   And of course, this is self serving information.
[01:03:02.400 --> 01:03:08.720]   And that's one of the things that she was referring to when she said that he's been,
[01:03:08.720 --> 01:03:11.200]   you know, kind of misleading about this information.
[01:03:11.200 --> 01:03:14.640]   We did get a lot of fraud.
[01:03:14.640 --> 01:03:17.680]   I mean, there is, there was a lot of unemployment insurance fraud, but
[01:03:18.560 --> 01:03:22.160]   I don't know how much IRS fraud versus unemployment.
[01:03:22.160 --> 01:03:26.880]   ID.me says $400 billion in pandemic unemployment fraud.
[01:03:26.880 --> 01:03:29.680]   Unproven.
[01:03:29.680 --> 01:03:31.040]   That's, that's another case.
[01:03:31.040 --> 01:03:31.200]   Yeah.
[01:03:31.200 --> 01:03:32.640]   Well, that's another case.
[01:03:32.640 --> 01:03:36.960]   Unproven, but that's the kind of stuff that the states are using them for.
[01:03:36.960 --> 01:03:40.240]   That was at least a billion.
[01:03:40.240 --> 01:03:47.680]   Gave access. Yeah. $400 billion is a lot of money by cyber criminal gangs.
[01:03:47.680 --> 01:03:56.960]   And then that number, by the way, then parroted by the top Republican on the
[01:03:56.960 --> 01:03:58.800]   House Ways and Means Committee, Kevin Brady.
[01:03:58.800 --> 01:04:04.320]   So it suddenly becomes, you know, truth.
[01:04:05.760 --> 01:04:10.400]   Oregon paid only 24 million in fraudulent unemployment benefits during the pandemic.
[01:04:10.400 --> 01:04:13.440]   The total amount of government payouts was 800 billion.
[01:04:13.440 --> 01:04:15.680]   So they're saying half of that money was fraudulent.
[01:04:15.680 --> 01:04:16.880]   It seems like a large number.
[01:04:16.880 --> 01:04:20.960]   California says 20 billion.
[01:04:20.960 --> 01:04:25.760]   20 billion is what they paid out in fraud.
[01:04:25.760 --> 01:04:28.960]   Fraudulent unemployment payments during the pandemic.
[01:04:28.960 --> 01:04:31.840]   11% of what it paid out overall.
[01:04:31.840 --> 01:04:33.200]   Yeah.
[01:04:33.200 --> 01:04:36.400]   We had 650 million in Washington.
[01:04:36.400 --> 01:04:37.040]   So that's a lot.
[01:04:37.040 --> 01:04:44.560]   Hall Hall, by the way, says to this day, it's accurate according to Bloomberg Business Week.
[01:04:44.560 --> 01:04:49.200]   He said it's confirmed and patterns the data company, the data the company has gathered through
[01:04:49.200 --> 01:04:53.360]   its work for California, Florida, New York, Pennsylvania, Texas and 22 other states.
[01:04:53.360 --> 01:04:58.720]   He says if state agencies are reporting far lower levels of fraud publicly, well, they're wrong.
[01:04:58.720 --> 01:05:02.560]   Okay.
[01:05:02.560 --> 01:05:06.400]   He says this is the largest cyber attack in terms of fraud in American history.
[01:05:06.400 --> 01:05:17.440]   And that's what's giving some people a little bit of qualms about ID.me.
[01:05:17.440 --> 01:05:23.120]   Are they overstating this to get this government contract?
[01:05:23.120 --> 01:05:25.360]   Yes.
[01:05:25.360 --> 01:05:27.760]   Would be surprised.
[01:05:27.760 --> 01:05:28.800]   I know, but we don't know.
[01:05:29.520 --> 01:05:31.840]   We don't know. That's marketing, right? That's sales.
[01:05:31.840 --> 01:05:38.960]   ID.me began 12 years ago as TroopSwap, a Craigslist knockoff for the military community.
[01:05:38.960 --> 01:05:41.600]   That's a pivot.
[01:05:41.600 --> 01:05:45.200]   Then they turned it into a group on style discount service.
[01:05:45.200 --> 01:05:50.240]   Then they realized the real asset of TroopSwap was the software which allowed veterans
[01:05:50.240 --> 01:05:54.320]   to prove their eligibility without presenting documents, bearing social security numbers.
[01:05:54.320 --> 01:05:57.120]   They said, hmm, hmm.
[01:05:57.280 --> 01:05:57.840]   Hmm.
[01:05:57.840 --> 01:06:08.160]   Hall and Thompson, both ex-military, but neither of whom had a tech background, saw an opportunity.
[01:06:08.160 --> 01:06:15.200]   I always anybody who does a lot of deals with the government, I'm always like,
[01:06:15.200 --> 01:06:16.400]   "Mmm, your sketch."
[01:06:16.400 --> 01:06:20.000]   The Obama administration, this is all from Bloomberg Business Week.
[01:06:20.000 --> 01:06:24.400]   The Obama administration announced the national strategy for trusted identities in
[01:06:24.400 --> 01:06:29.920]   cyberspace, a push to get private sector companies to develop ID verification technologies.
[01:06:29.920 --> 01:06:36.880]   The TroopSwap team bought the ID.me domain, rebranded, and won a $2.6 million grant from
[01:06:36.880 --> 01:06:40.800]   the Commerce Department's National Institutes of Standards and Technology.
[01:06:40.800 --> 01:06:53.920]   Anyway, there's more to this. This is a deeper story. I'm sure we'll be reading more of this.
[01:06:54.560 --> 01:07:03.120]   It's interesting. There you go. There you have it.
[01:07:03.120 --> 01:07:10.800]   What do we think about the end of the line for Google Cloud's G Suite Free Edition?
[01:07:10.800 --> 01:07:17.920]   If you have a G Suite Free Edition, you're going to have to upgrade to a paid version
[01:07:17.920 --> 01:07:20.400]   starting July 1st.
[01:07:21.040 --> 01:07:25.120]   They should have long ago. This is useful for me because I pay already.
[01:07:25.120 --> 01:07:26.960]   I'm going to hear me complainable all the time.
[01:07:26.960 --> 01:07:35.840]   G Suite is now built for big companies. They should have a light individual or family
[01:07:35.840 --> 01:07:40.800]   edition of it that says, "All you want is your own domain. Some email. Here's the deal."
[01:07:40.800 --> 01:07:43.200]   And that's what it was used for, right?
[01:07:43.200 --> 01:07:44.480]   That's why I'm there.
[01:07:44.480 --> 01:07:45.760]   Yeah, you have a custom domain.
[01:07:45.760 --> 01:07:49.440]   They've overcomplicated the hell out of it on the high end. They've taken away features
[01:07:49.440 --> 01:07:52.560]   because it's so complicated. We can't get as Google users.
[01:07:52.560 --> 01:08:00.480]   They shouldn't treat the individual who was grandfather now like a company.
[01:08:00.480 --> 01:08:02.560]   It's only six bucks a month. It's not that bad.
[01:08:02.560 --> 01:08:07.200]   They stopped offering the Free Tier 10 years ago.
[01:08:07.200 --> 01:08:07.680]   I know.
[01:08:07.680 --> 01:08:08.720]   Yeah, 2012.
[01:08:08.720 --> 01:08:09.920]   But you were grandfather.
[01:08:09.920 --> 01:08:12.800]   But you still had it. So there's people who have been all along.
[01:08:12.800 --> 01:08:14.720]   They got a domain name. It's just them.
[01:08:14.720 --> 01:08:15.760]   That's my wife.
[01:08:15.760 --> 01:08:19.120]   They got a smaller version of G Suite. So she's going to have to start paying in July.
[01:08:19.600 --> 01:08:20.160]   Yeah.
[01:08:20.160 --> 01:08:20.720]   Yeah.
[01:08:20.720 --> 01:08:22.400]   How does she feel about that?
[01:08:22.400 --> 01:08:24.960]   She doesn't know yet. She'll know about that.
[01:08:24.960 --> 01:08:26.080]   She'll ask me, "What did you get me into?"
[01:08:26.080 --> 01:08:27.600]   "Not me in Jake."
[01:08:27.600 --> 01:08:31.920]   No, I'm kind of glad I didn't do that.
[01:08:31.920 --> 01:08:33.920]   But we, Twit, pays the...
[01:08:33.920 --> 01:08:36.560]   Because you run servers and things.
[01:08:36.560 --> 01:08:38.720]   Yeah. So we pay the six bucks per seat.
[01:08:38.720 --> 01:08:40.400]   And everybody's got an ad.
[01:08:40.400 --> 01:08:42.960]   And twit.tv is through that. That's the email.
[01:08:42.960 --> 01:08:45.200]   Not the website, but the email.
[01:08:45.200 --> 01:08:46.400]   Yeah.
[01:08:46.400 --> 01:08:48.720]   Yeah. And we use G Suite...
[01:08:48.720 --> 01:08:50.240]   I'm sorry, workspace.
[01:08:50.240 --> 01:08:52.880]   I'm using it right now as a Google Sheet that we use to
[01:08:52.880 --> 01:08:57.360]   share the stories with all the hosts and the follow stories.
[01:08:57.360 --> 01:09:02.880]   So Simon Bodger sent me a thing from GitHub,
[01:09:02.880 --> 01:09:08.160]   a G Suite migration plan that goes into some detail about this.
[01:09:08.160 --> 01:09:12.400]   What line is that?
[01:09:12.400 --> 01:09:13.840]   It's not yet. It will be in a second.
[01:09:13.840 --> 01:09:15.840]   It will be...
[01:09:15.840 --> 01:09:17.200]   What good are you?
[01:09:17.200 --> 01:09:18.960]   150.
[01:09:18.960 --> 01:09:21.920]   So I'm going to make you scroll.
[01:09:21.920 --> 01:09:24.080]   Yeah. Why you make me scroll?
[01:09:24.080 --> 01:09:24.400]   Okay.
[01:09:24.400 --> 01:09:25.600]   It's fun to watch that.
[01:09:25.600 --> 01:09:27.200]   GitHub G Suite migration.
[01:09:27.200 --> 01:09:30.640]   So this is a software that will just move you over to a page.
[01:09:30.640 --> 01:09:31.200]   No, I don't think...
[01:09:31.200 --> 01:09:32.240]   No, this is the process.
[01:09:32.240 --> 01:09:33.920]   Oh, God.
[01:09:33.920 --> 01:09:35.120]   Yeah.
[01:09:35.120 --> 01:09:36.400]   It's not...
[01:09:36.400 --> 01:09:37.360]   Doesn't look simple.
[01:09:37.360 --> 01:09:39.280]   It's a typical Google process.
[01:09:39.280 --> 01:09:39.680]   Wow.
[01:09:41.280 --> 01:09:43.120]   Gmail, my gradable. Yes.
[01:09:43.120 --> 01:09:44.400]   Gmail rules. Yes.
[01:09:44.400 --> 01:09:48.640]   Google talk hangouts. No.
[01:09:48.640 --> 01:09:49.440]   This is sad.
[01:09:49.440 --> 01:09:50.960]   Well, that's because they killed it.
[01:09:50.960 --> 01:09:51.440]   Right.
[01:09:51.440 --> 01:09:52.720]   Google Drive. Yes.
[01:09:52.720 --> 01:09:56.560]   But my gradable to what?
[01:09:56.560 --> 01:09:56.880]   Right.
[01:09:56.880 --> 01:09:59.440]   Well, you get Google Drive if you pay the same.
[01:09:59.440 --> 01:10:01.680]   Oh, this is if you don't want to pay six bucks.
[01:10:01.680 --> 01:10:02.160]   Yeah.
[01:10:02.160 --> 01:10:02.800]   Yeah, I think so.
[01:10:02.800 --> 01:10:05.680]   If a G Suite account remains unpaid,
[01:10:05.680 --> 01:10:08.800]   Google will convert the account to a Google identity.
[01:10:08.800 --> 01:10:10.480]   This is an incomplete Google account,
[01:10:10.480 --> 01:10:12.720]   which does not include Gmail and Calendar.
[01:10:12.720 --> 01:10:14.400]   But for some reason, it includes Google Drive.
[01:10:14.400 --> 01:10:14.960]   I'll tell you why.
[01:10:14.960 --> 01:10:16.080]   They don't want to delete your data.
[01:10:16.080 --> 01:10:19.520]   While many Google services work with Google identity,
[01:10:19.520 --> 01:10:20.800]   its usage is inconvenient.
[01:10:20.800 --> 01:10:24.320]   And one will be required to be logged into several accounts
[01:10:24.320 --> 01:10:25.360]   to use free Gmail.
[01:10:25.360 --> 01:10:27.840]   So you're going to have all sorts of identities.
[01:10:27.840 --> 01:10:31.440]   So here's...
[01:10:31.440 --> 01:10:35.600]   This is how to get out of G Suite Google to free Google accounts.
[01:10:35.600 --> 01:10:38.000]   And what tools you'll need.
[01:10:38.560 --> 01:10:39.440]   And how you will do it.
[01:10:39.440 --> 01:10:40.560]   Ah, I see.
[01:10:40.560 --> 01:10:40.880]   Okay.
[01:10:40.880 --> 01:10:41.520]   It's quite a few.
[01:10:41.520 --> 01:10:43.520]   A lot of steps there.
[01:10:43.520 --> 01:10:44.720]   A few steps.
[01:10:44.720 --> 01:10:45.280]   Yeah.
[01:10:45.280 --> 01:10:46.400]   So six bucks a month.
[01:10:46.400 --> 01:10:49.680]   I think I'm missing the opportunity just to do a different offer.
[01:10:49.680 --> 01:10:51.280]   Fine.
[01:10:51.280 --> 01:10:52.320]   Charges for the services.
[01:10:52.320 --> 01:10:53.440]   Are you giving it for a buck?
[01:10:53.440 --> 01:10:57.840]   If you only have one person using it, you know, a buck.
[01:10:57.840 --> 01:10:59.360]   Wait, I mean, we pay, what?
[01:10:59.360 --> 01:11:01.200]   We pay $6 a person?
[01:11:01.200 --> 01:11:03.040]   I mean, you can't pay $6.
[01:11:03.040 --> 01:11:04.560]   Or, I mean, you're...
[01:11:04.560 --> 01:11:06.560]   You get a lot for that money, by the way.
[01:11:06.560 --> 01:11:06.800]   I should say.
[01:11:06.800 --> 01:11:07.520]   Yeah.
[01:11:07.520 --> 01:11:09.680]   You get like 15 gigs.
[01:11:09.680 --> 01:11:10.320]   So...
[01:11:10.320 --> 01:11:10.880]   Yeah.
[01:11:10.880 --> 01:11:11.280]   So I...
[01:11:11.280 --> 01:11:12.400]   Like if you're sad...
[01:11:12.400 --> 01:11:13.680]   You also get complications.
[01:11:13.680 --> 01:11:14.960]   Now, that's my problem.
[01:11:14.960 --> 01:11:16.640]   It's because you entered in...
[01:11:16.640 --> 01:11:19.440]   Because then when I use this, I guess my Chromebook.
[01:11:19.440 --> 01:11:20.640]   And I want to add something.
[01:11:20.640 --> 01:11:21.120]   Oh, no, no.
[01:11:21.120 --> 01:11:23.920]   You must get the administrator to do this.
[01:11:23.920 --> 01:11:26.560]   And then I start screaming on Twitter and people have to call me down.
[01:11:26.560 --> 01:11:27.360]   I'm the administrator.
[01:11:27.360 --> 01:11:28.480]   Yeah.
[01:11:28.480 --> 01:11:29.520]   I am the administrator.
[01:11:29.520 --> 01:11:30.320]   I mean...
[01:11:30.320 --> 01:11:31.280]   I'm an idiot.
[01:11:31.280 --> 01:11:32.560]   What are you doing to me?
[01:11:32.560 --> 01:11:35.680]   And so that's the problem, Stacey, is that it's...
[01:11:35.680 --> 01:11:36.000]   Yes.
[01:11:36.000 --> 01:11:36.880]   Six bucks a month.
[01:11:36.880 --> 01:11:38.160]   And you get a whole lot of the services.
[01:11:38.160 --> 01:11:38.640]   Absolutely.
[01:11:38.640 --> 01:11:39.760]   But you also get headaches.
[01:11:39.760 --> 01:11:41.520]   Whatever Google introduces something new.
[01:11:41.520 --> 01:11:42.800]   And you can't...
[01:11:42.800 --> 01:11:43.600]   Well, you could...
[01:11:43.600 --> 01:11:43.920]   Most...
[01:11:43.920 --> 01:11:44.720]   I mean, like,
[01:11:44.720 --> 01:11:48.640]   I'm our Google G Suite administrator for Stacey on IoT.
[01:11:48.640 --> 01:11:50.480]   So there's four licenses out.
[01:11:50.480 --> 01:11:52.720]   And I am the person.
[01:11:52.720 --> 01:11:53.280]   Your jeans.
[01:11:53.280 --> 01:11:56.960]   And I have the same frustrations you have, Jeff.
[01:11:56.960 --> 01:11:58.640]   I don't yell as loudly on Twitter.
[01:11:58.640 --> 01:11:59.360]   But...
[01:11:59.360 --> 01:11:59.840]   But...
[01:11:59.840 --> 01:12:00.640]   I should point out that...
[01:12:00.640 --> 01:12:03.600]   I should point out that there are still...
[01:12:03.600 --> 01:12:05.200]   This is not changing the free...
[01:12:06.080 --> 01:12:08.240]   Gmail, the free Google Drive.
[01:12:08.240 --> 01:12:08.240]   Right.
[01:12:08.240 --> 01:12:08.720]   I mean...
[01:12:08.720 --> 01:12:08.720]   Yeah.
[01:12:08.720 --> 01:12:09.280]   Right.
[01:12:09.280 --> 01:12:10.880]   This is only if you want to use a custom domain.
[01:12:10.880 --> 01:12:11.920]   It's only if you want your own domain.
[01:12:11.920 --> 01:12:14.400]   So I have my name at gmail.com.
[01:12:14.400 --> 01:12:16.240]   I have a Google Drive under that name.
[01:12:16.240 --> 01:12:16.640]   Which they have for...
[01:12:16.640 --> 01:12:19.760]   Which you should pay for.
[01:12:19.760 --> 01:12:20.480]   No, it's free.
[01:12:20.480 --> 01:12:23.600]   No, I'm saying people wanting to go custom.
[01:12:23.600 --> 01:12:24.720]   Well, actually...
[01:12:24.720 --> 01:12:28.400]   I do pay for it because I have a Google One plan.
[01:12:28.400 --> 01:12:29.280]   Which means I...
[01:12:29.280 --> 01:12:32.000]   So I have two terabytes of Google Drive and Blum.
[01:12:32.000 --> 01:12:32.480]   Because...
[01:12:32.480 --> 01:12:33.200]   What do they give you?
[01:12:33.200 --> 01:12:34.320]   50 gigabytes or something?
[01:12:34.320 --> 01:12:37.360]   It's not usually you can run through that if you have Gmail on.
[01:12:37.360 --> 01:12:40.560]   So Google probably just feels like if you're sophisticated enough to run...
[01:12:40.560 --> 01:12:43.760]   To own your own domain and to watch the run that through them.
[01:12:43.760 --> 01:12:43.840]   That's pretty sophisticated.
[01:12:43.840 --> 01:12:44.240]   Yeah.
[01:12:44.240 --> 01:12:44.720]   Yeah.
[01:12:44.720 --> 01:12:45.040]   Yeah.
[01:12:45.040 --> 01:12:47.360]   I mean, because you also have to have a server.
[01:12:47.360 --> 01:12:50.960]   So you're either running your own server or you're going to hire someone to do it.
[01:12:50.960 --> 01:12:52.560]   You know, so that's pretty sophisticated.
[01:12:52.560 --> 01:12:54.480]   They figure you could be your own IT administrator.
[01:12:54.480 --> 01:12:55.840]   If you want...
[01:12:55.840 --> 01:13:00.080]   There are a lot cheaper ways than six bucks a person per month.
[01:13:00.080 --> 01:13:02.480]   If you just want email at your own domain.
[01:13:03.040 --> 01:13:03.680]   That's...
[01:13:03.680 --> 01:13:04.160]   I don't want my Gmail.
[01:13:04.160 --> 01:13:05.040]   I like to do it.
[01:13:05.040 --> 01:13:05.840]   Or if you want to use...
[01:13:05.840 --> 01:13:07.360]   Well, I can still use Gmail.
[01:13:07.360 --> 01:13:10.800]   I just have it forward to Gmail and then log into my Gmail account.
[01:13:10.800 --> 01:13:12.000]   You can still use that.
[01:13:12.000 --> 01:13:14.560]   So I have my own custom domains.
[01:13:14.560 --> 01:13:17.920]   For a long time, email to me would go to my Gmail account.
[01:13:17.920 --> 01:13:18.960]   And actually...
[01:13:18.960 --> 01:13:23.040]   I had a really complicated set because I did that because Gmail was good at spam.
[01:13:23.040 --> 01:13:27.120]   And then I would have it forwarded from my Gmail account to another mail server.
[01:13:27.120 --> 01:13:30.320]   So I was kind of running my mail through Gmail for the spam filtering.
[01:13:30.880 --> 01:13:33.840]   You and Dvorak was insisting that he had better spam.
[01:13:33.840 --> 01:13:36.800]   And I never had any spam and you said, "No, I got Gmail and it works fine."
[01:13:36.800 --> 01:13:40.000]   He used some guy service.
[01:13:40.000 --> 01:13:42.880]   I got no spam.
[01:13:42.880 --> 01:13:44.800]   I think we even had a song.
[01:13:44.800 --> 01:13:47.760]   I guess no spam.
[01:13:47.760 --> 01:13:52.000]   Yeah, he was famous for that.
[01:13:52.000 --> 01:13:55.520]   And I'm sure he must have gotten spam, but who knows?
[01:13:55.520 --> 01:13:56.880]   Anyway, Gmail was very good.
[01:13:56.880 --> 01:14:00.240]   Actually, I stopped doing that because Gmail's spam filtering is not that good.
[01:14:00.240 --> 01:14:00.880]   It's no good.
[01:14:00.880 --> 01:14:01.200]   Yeah.
[01:14:01.200 --> 01:14:01.760]   Oh, it's not?
[01:14:01.760 --> 01:14:02.400]   You don't think so?
[01:14:02.400 --> 01:14:02.880]   No.
[01:14:02.880 --> 01:14:04.560]   I get so much spam.
[01:14:04.560 --> 01:14:08.000]   My biggest problem is that it was "la port at Gmail."
[01:14:08.000 --> 01:14:10.560]   And that sounds French.
[01:14:10.560 --> 01:14:13.040]   So I was getting not only spam, but I was getting a lot of French spam.
[01:14:13.040 --> 01:14:14.240]   Oh, right.
[01:14:14.240 --> 01:14:15.120]   Oh, classy of you.
[01:14:15.120 --> 01:14:16.320]   Right.
[01:14:16.320 --> 01:14:18.400]   Oh.
[01:14:18.400 --> 01:14:22.240]   Actually, when did I get the other day in French?
[01:14:22.240 --> 01:14:24.000]   Oh, from my employer.
[01:14:24.000 --> 01:14:27.040]   I heart sent me an email.
[01:14:27.040 --> 01:14:29.840]   They do every six months, even though this is no longer the
[01:14:29.840 --> 01:14:33.680]   recommendation, they change your password.
[01:14:33.680 --> 01:14:37.040]   They expire your password every six months.
[01:14:37.040 --> 01:14:38.080]   So annoying.
[01:14:38.080 --> 01:14:40.240]   Because I only log in once every six months.
[01:14:40.240 --> 01:14:44.400]   They sent me an email, but where is they sent me in English and French?
[01:14:44.400 --> 01:14:48.320]   And I was trying to figure out, do they think I'm French?
[01:14:48.320 --> 01:14:49.760]   Clearly they do.
[01:14:49.760 --> 01:14:52.240]   Clearly they do.
[01:14:52.240 --> 01:14:55.120]   You got to make sure you got to know all of that.
[01:14:55.120 --> 01:14:55.360]   Yeah.
[01:14:55.360 --> 01:14:58.400]   Or is it there's some sort of Canadian thing, but they're not a Canadian company.
[01:14:58.400 --> 01:14:59.760]   I was a Canadian.
[01:14:59.760 --> 01:15:01.840]   They're not a Canadian company.
[01:15:01.840 --> 01:15:02.160]   No.
[01:15:02.160 --> 01:15:04.240]   It was an odd thing.
[01:15:04.240 --> 01:15:08.000]   I have to ask my boss, why did you send me this in French?
[01:15:08.000 --> 01:15:11.520]   "Shange Le Pas world."
[01:15:11.520 --> 01:15:15.200]   How about this?
[01:15:15.200 --> 01:15:18.720]   Actually, let's take a break and then I have a lot of how about this is coming up,
[01:15:18.720 --> 01:15:21.120]   but I want to talk a little bit about my food.
[01:15:21.120 --> 01:15:24.080]   Tomorrow our imperfect food box comes.
[01:15:24.080 --> 01:15:27.280]   You said, Stacey, were you somebody was using imperfect foods?
[01:15:27.280 --> 01:15:28.400]   I love imperfect foods.
[01:15:28.400 --> 01:15:29.120]   I have done it in the past.
[01:15:29.120 --> 01:15:29.520]   Yeah.
[01:15:29.520 --> 01:15:30.640]   I just blew me away.
[01:15:30.640 --> 01:15:33.920]   So imperfect, you can put the lower third up.
[01:15:33.920 --> 01:15:34.720]   It's okay.
[01:15:34.720 --> 01:15:35.760]   Imperfect foods.
[01:15:35.760 --> 01:15:36.800]   This isn't bad.
[01:15:36.800 --> 01:15:37.360]   There you go.
[01:15:37.360 --> 01:15:38.160]   That's it.
[01:15:38.160 --> 01:15:38.880]   This is an ad.
[01:15:38.880 --> 01:15:47.120]   Imperfect foods was started because they noticed that a lot of foods were thrown away
[01:15:47.120 --> 01:15:52.480]   because the grocer wouldn't put because they want that beautiful display of perfect apples.
[01:15:52.480 --> 01:15:55.840]   But as you may know, if you've ever gone to an apple orchard,
[01:15:55.840 --> 01:15:57.440]   most apples aren't perfect.
[01:15:57.440 --> 01:15:59.040]   And in fact, I'll be honest with you.
[01:15:59.040 --> 01:16:02.320]   The small ones, maybe those are the sweetest.
[01:16:02.320 --> 01:16:04.480]   Those are the best apples because we used to go.
[01:16:04.480 --> 01:16:07.040]   We used to pick our apples and I knew which ones to get.
[01:16:07.040 --> 01:16:12.240]   Not the perfect ones that have been stored in a refrigerator and bread
[01:16:12.240 --> 01:16:15.520]   so that they could be stored in a freezer six months before they go on the grocery shelf.
[01:16:15.520 --> 01:16:19.200]   So imperfect food said, we should really solve this problem because it turns out,
[01:16:19.200 --> 01:16:22.000]   I didn't realize this, about a third, more than a third.
[01:16:22.000 --> 01:16:30.320]   35% of the food supply in this country goes unsold, uneaten, and because it's not perfect.
[01:16:30.320 --> 01:16:33.680]   So imperfect foods was started as a delivery service.
[01:16:33.680 --> 01:16:37.760]   So you could kind of be a good person and get delicious foods.
[01:16:37.760 --> 01:16:42.080]   I have to tell you, we've been subscribing imperfect foods for some time now.
[01:16:42.080 --> 01:16:44.160]   I've never had anything I wouldn't go.
[01:16:44.160 --> 01:16:44.720]   That's great.
[01:16:44.720 --> 01:16:48.320]   Sometimes I thought we were going to get a lot of food that looked like Richard Nixon,
[01:16:48.320 --> 01:16:52.320]   you know, like carrots with a big nose and stuff. But no, there's a few sometimes.
[01:16:52.320 --> 01:16:55.360]   Or the pomegranates aren't as big as your head.
[01:16:55.360 --> 01:16:57.440]   They're only as big as your fist, things like that.
[01:16:57.440 --> 01:16:59.360]   I think those are sweeter anyway.
[01:16:59.360 --> 01:17:01.760]   Now imperfect foods does a whole lot more.
[01:17:01.760 --> 01:17:02.720]   They do meats.
[01:17:02.720 --> 01:17:03.840]   They do dairy.
[01:17:03.840 --> 01:17:04.880]   They do snacks.
[01:17:04.880 --> 01:17:11.280]   All of it, food that the grocers, they overproduced it or they didn't want it.
[01:17:11.280 --> 01:17:12.720]   It's great.
[01:17:12.720 --> 01:17:14.400]   You'll reduce food waste.
[01:17:14.400 --> 01:17:17.920]   It'll save you time in your grocery shopping and you will eat more fresh
[01:17:18.480 --> 01:17:19.920]   delicious food.
[01:17:19.920 --> 01:17:25.120]   Plus you're really helping the environment.
[01:17:25.120 --> 01:17:28.080]   So not only does that food not get wasted, right?
[01:17:28.080 --> 01:17:31.040]   But you're also because instead of going to the grocery store,
[01:17:31.040 --> 01:17:36.080]   the imperfect foods delivers, they bunch all the livers in your neighborhood together
[01:17:36.080 --> 01:17:36.880]   on the single day.
[01:17:36.880 --> 01:17:40.160]   So everybody in our area is Thursday, one truck.
[01:17:40.160 --> 01:17:46.320]   That means there's 25% to 75% fewer emissions than if we were all going to the grocery store.
[01:17:46.320 --> 01:17:47.520]   That's awesome.
[01:17:48.000 --> 01:17:52.400]   On average, imperfect foods customers save six to eight pounds of food with every order.
[01:17:52.400 --> 01:17:55.600]   But I don't want to emphasize that.
[01:17:55.600 --> 01:18:00.080]   You're going to feel good about doing good, but you're going to get great food too.
[01:18:00.080 --> 01:18:02.000]   Look, go to imperfectfoods.com.
[01:18:02.000 --> 01:18:04.000]   Check first to see if they deliver in your area.
[01:18:04.000 --> 01:18:05.440]   I don't want to oversell this.
[01:18:05.440 --> 01:18:07.440]   If you can't get it, I don't want to make you feel bad.
[01:18:07.440 --> 01:18:09.840]   But if you can, trust me, you should do it.
[01:18:09.840 --> 01:18:10.560]   Sign up.
[01:18:10.560 --> 01:18:16.080]   You could personalize your weekly grocery order with fresh seasonal produce, pantry staples,
[01:18:16.080 --> 01:18:17.280]   yummy snacks.
[01:18:17.280 --> 01:18:18.960]   It'll arrive on the same day every week.
[01:18:18.960 --> 01:18:20.240]   Won't necessarily be Thursday.
[01:18:20.240 --> 01:18:21.280]   That's just my neighborhood.
[01:18:21.280 --> 01:18:24.320]   But it's nice because you know that box is going to be there.
[01:18:24.320 --> 01:18:26.240]   They're really good on packaging too.
[01:18:26.240 --> 01:18:27.200]   That's the other thing.
[01:18:27.200 --> 01:18:31.840]   Unlike some grocery delivery services, you get 100 plastic bags.
[01:18:31.840 --> 01:18:36.960]   You could say goodbye to packaging, guilt, imperfect foods is the only national grocery delivery
[01:18:36.960 --> 01:18:41.200]   company that makes it easy to return your packaging after every order.
[01:18:41.200 --> 01:18:42.080]   We do that.
[01:18:42.080 --> 01:18:44.000]   They don't do a lot of plastic bags.
[01:18:44.720 --> 01:18:47.040]   It's just, you can tell it's thoughtful.
[01:18:47.040 --> 01:18:47.680]   It's thoughtful.
[01:18:47.680 --> 01:18:49.280]   And that's why I really like it.
[01:18:49.280 --> 01:18:53.680]   It is a great grocery delivery service that you will love.
[01:18:53.680 --> 01:18:54.560]   And I want you to try.
[01:18:54.560 --> 01:18:56.400]   In fact, we've got a deal for you.
[01:18:56.400 --> 01:19:04.320]   Get your first four orders, 20% off, not just for the first one, but for the first four orders.
[01:19:04.320 --> 01:19:07.440]   When you go to imperfectfoods.com, but you have to do this for me,
[01:19:07.440 --> 01:19:10.240]   use the promo code twig to get that offer.
[01:19:10.240 --> 01:19:11.280]   TWIG.
[01:19:11.280 --> 01:19:13.120]   That way they know what you saw it here too.
[01:19:13.120 --> 01:19:19.120]   20% off your first four orders, that's up to an $80 value at imperfectfoods.com
[01:19:19.120 --> 01:19:20.560]   when you use a promo code twig.
[01:19:20.560 --> 01:19:22.400]   We love it.
[01:19:22.400 --> 01:19:25.920]   From my family to yours, I think you'll love it too.
[01:19:25.920 --> 01:19:29.920]   Imperfectfoods.com, promo code twig.
[01:19:29.920 --> 01:19:35.920]   Good way to really feel good about what you're eating and has helped the environment and get
[01:19:35.920 --> 01:19:36.400]   delicious.
[01:19:36.400 --> 01:19:38.800]   I think it's perfect personally.
[01:19:38.800 --> 01:19:39.120]   Food.
[01:19:40.720 --> 01:19:43.600]   I think he hanked to an imperfect foods challenge.
[01:19:43.600 --> 01:19:43.600]   TWIG.
[01:19:43.600 --> 01:19:44.320]   Oh, he should.
[01:19:44.320 --> 01:19:44.640]   TWIG.
[01:19:44.640 --> 01:19:45.200]   You're hanked.
[01:19:45.200 --> 01:19:46.240]   You should.
[01:19:46.240 --> 01:19:48.160]   What can you make with this?
[01:19:48.160 --> 01:19:49.200]   Exactly.
[01:19:49.200 --> 01:19:49.680]   TWIG.
[01:19:49.680 --> 01:19:52.720]   I'll tell you the one thing I really love, they have a heritage chicken,
[01:19:52.720 --> 01:19:55.360]   which is a little smaller because it's not, you know, they have the big breasts.
[01:19:55.360 --> 01:19:57.600]   It's not the dolly part of their chicken.
[01:19:57.600 --> 01:19:58.800]   It's not all hormones.
[01:19:58.800 --> 01:19:59.360]   It's not all hormones.
[01:19:59.360 --> 01:20:03.760]   It's like a traditional small chicken and I made it last night.
[01:20:03.760 --> 01:20:05.120]   We have, it's so good.
[01:20:05.120 --> 01:20:10.000]   I roasted it high heat using the cafe zuni roasting recipe.
[01:20:10.000 --> 01:20:11.520]   And it is so good.
[01:20:11.520 --> 01:20:14.240]   It's just, it's my favorite thing.
[01:20:14.240 --> 01:20:17.760]   Anyway, that's, that's, that's free.
[01:20:17.760 --> 01:20:18.800]   I'll give that to you for free.
[01:20:18.800 --> 01:20:25.440]   So this was a weird one from zippy.com was as an SEO company.
[01:20:25.440 --> 01:20:28.000]   We studied 81,000 page titles.
[01:20:28.000 --> 01:20:31.840]   Google rewrote 61% of them.
[01:20:31.840 --> 01:20:37.360]   For yes, in the search results.
[01:20:37.360 --> 01:20:38.400]   So here's an example.
[01:20:38.400 --> 01:20:39.120]   This at all.
[01:20:39.120 --> 01:20:39.920]   Here's an example.
[01:20:39.920 --> 01:20:44.000]   Zippy product guides is the, you know, there's a HREF title.
[01:20:44.000 --> 01:20:46.880]   That's how you're, that's your title for your tags.
[01:20:46.880 --> 01:20:49.440]   Yeah, it's a tag in your HTML.
[01:20:49.440 --> 01:20:52.400]   They rewrote it to support and product guides, Zippy, SEO,
[01:20:52.400 --> 01:20:56.160]   which probably is more to what you want, but they're just doing it.
[01:20:56.160 --> 01:20:56.800]   They're not asking.
[01:20:56.800 --> 01:20:57.600]   They're just doing it.
[01:20:57.600 --> 01:21:03.200]   So apparently if it's too short, they'll rewrite it.
[01:21:03.200 --> 01:21:07.120]   If it's too long, they'll obviously truncate it.
[01:21:08.160 --> 01:21:12.160]   They said, no, this is in the search results search results.
[01:21:12.160 --> 01:21:12.560]   Okay.
[01:21:12.560 --> 01:21:15.280]   Ask Andrew about this.
[01:21:15.280 --> 01:21:16.640]   He must know about this.
[01:21:16.640 --> 01:21:21.760]   We mapped 80,000 titles by the number of characters to determine the likelihood of
[01:21:21.760 --> 01:21:23.680]   Google rewriting each one.
[01:21:23.680 --> 01:21:28.320]   Google rewrites both long titles and extremely short titles over 95% of the time.
[01:21:28.320 --> 01:21:31.440]   Titles, where's the data from?
[01:21:31.440 --> 01:21:33.680]   Is it other, is it other metadata in the page?
[01:21:33.680 --> 01:21:35.520]   Did they just grab instead of?
[01:21:35.520 --> 01:21:36.880]   That's an interesting question.
[01:21:37.680 --> 01:21:38.240]   They must.
[01:21:38.240 --> 01:21:40.640]   Yeah, it's a computer doing it.
[01:21:40.640 --> 01:21:41.280]   Not a human.
[01:21:41.280 --> 01:21:43.280]   I don't want to hell of a job.
[01:21:43.280 --> 01:21:44.400]   They rewrite that headline.
[01:21:44.400 --> 01:21:49.680]   They said, we found a number of common scenarios in which it became more likely for
[01:21:49.680 --> 01:21:50.880]   Google to rewrite titles.
[01:21:50.880 --> 01:21:54.240]   I guess the really, the moral of this is check.
[01:21:54.240 --> 01:21:55.520]   Look at your search results.
[01:21:55.520 --> 01:21:59.360]   Length, if it's too long or too short, using the same keyword more than once.
[01:21:59.360 --> 01:22:02.160]   So they'll just take that second version out.
[01:22:02.160 --> 01:22:05.040]   Because people were trying to do that to be pushed up.
[01:22:05.040 --> 01:22:05.040]   Yeah.
[01:22:05.040 --> 01:22:06.480]   That's probably where it started.
[01:22:06.480 --> 01:22:10.400]   Is they were trying to produce the SEO by repetition?
[01:22:10.400 --> 01:22:13.040]   They don't like title separators like dashes or pipes.
[01:22:13.040 --> 01:22:14.320]   They'll take those out.
[01:22:14.320 --> 01:22:16.160]   Titles with brackets or parenthesis.
[01:22:16.160 --> 01:22:19.680]   Identical bluerler plate used across many titles.
[01:22:19.680 --> 01:22:24.000]   Missing or superfluous brand names and more.
[01:22:24.000 --> 01:22:26.320]   So very interesting.
[01:22:26.320 --> 01:22:27.200]   That's actually helpful.
[01:22:27.200 --> 01:22:29.360]   Yeah, that's I'm sure the goal.
[01:22:29.360 --> 01:22:30.080]   Absolutely.
[01:22:30.080 --> 01:22:33.360]   It's helping you out, but at the same time,
[01:22:34.240 --> 01:22:37.600]   who gave them the right to just go in and change my stuff like that?
[01:22:37.600 --> 01:22:45.120]   Here's another one where they took bracket updated 2022 and replaced it with parenthesis 2022 update.
[01:22:45.120 --> 01:22:49.280]   Well, but yeah, it's weird that they're doing this automatically.
[01:22:49.280 --> 01:22:50.800]   So this is a good article.
[01:22:50.800 --> 01:22:52.640]   Z Y P P Y dot com.
[01:22:52.640 --> 01:22:53.280]   You could read it.
[01:22:53.280 --> 01:22:54.000]   It's in their blog.
[01:22:54.000 --> 01:22:57.280]   And how you could use H one tags.
[01:22:57.280 --> 01:23:04.160]   Google considers other HTML elements when crafting page titles beyond title tags.
[01:23:04.160 --> 01:23:06.480]   Chief among these in the H one tags.
[01:23:06.480 --> 01:23:11.280]   So strategic use of H one tags might solve that as well.
[01:23:11.280 --> 01:23:13.280]   So interesting.
[01:23:13.280 --> 01:23:18.320]   I just thought that was an interesting insight into some of the things Google.
[01:23:18.320 --> 01:23:21.120]   Can you imagine the scale that Google does this that they're doing that?
[01:23:21.120 --> 01:23:21.760]   It's amazing.
[01:23:21.760 --> 01:23:24.960]   And yeah, how quick is this done?
[01:23:24.960 --> 01:23:27.040]   Yeah, but instantly it's published and stuff.
[01:23:27.040 --> 01:23:29.360]   How fast does this go through their process?
[01:23:29.360 --> 01:23:30.080]   Oh, it's instant.
[01:23:30.080 --> 01:23:32.880]   Has to be because they're constantly updating.
[01:23:33.520 --> 01:23:34.000]   Hmm.
[01:23:34.000 --> 01:23:34.320]   Yeah.
[01:23:34.320 --> 01:23:36.480]   Let's see.
[01:23:36.480 --> 01:23:36.960]   What else?
[01:23:36.960 --> 01:23:39.040]   Can I raise one?
[01:23:39.040 --> 01:23:40.000]   Oh, I'm sorry.
[01:23:40.000 --> 01:23:42.400]   There's only just a state secrecy.
[01:23:42.400 --> 01:23:44.160]   It can be a Jeffocracy as well.
[01:23:44.160 --> 01:23:45.760]   What would you like to tell me?
[01:23:45.760 --> 01:23:48.240]   I meant to put these two stories together in the rundown.
[01:23:48.240 --> 01:23:49.440]   And I somehow screwed this up.
[01:23:49.440 --> 01:23:56.240]   I'm fascinated by by auto driving cars to two stories of different views.
[01:23:56.240 --> 01:24:01.280]   One is that as I think we might have mentioned this last week.
[01:24:01.280 --> 01:24:04.320]   I can't remember that a driver was charged.
[01:24:04.320 --> 01:24:05.040]   Yes, we did.
[01:24:05.040 --> 01:24:06.240]   Right.
[01:24:06.240 --> 01:24:12.720]   But then tie that in a British law group now recommends immunity for drivers of self-driving cars.
[01:24:12.720 --> 01:24:16.720]   I, you know, I think I said this when we talked about the story last week.
[01:24:16.720 --> 01:24:23.120]   The guy who was charged with, I think, vehicular manslaughter,
[01:24:23.120 --> 01:24:25.600]   was using Tesla's self-driving.
[01:24:25.600 --> 01:24:28.720]   And this was some years ago that he did this.
[01:24:28.720 --> 01:24:31.360]   And it went through a red light and killed a couple of pedestrians.
[01:24:31.360 --> 01:24:39.040]   But at the time he was using it, it never, even though Tesla said it's full self-driving,
[01:24:39.040 --> 01:24:40.320]   it didn't stop at lights.
[01:24:40.320 --> 01:24:44.240]   When I, the whole time I had my Model X, it wasn't supposed to stop at lights
[01:24:44.240 --> 01:24:45.760]   or stop signs or anything else.
[01:24:45.760 --> 01:24:50.800]   The mistake, I mean, if Tesla's to blame, it's by calling it full self-driving.
[01:24:50.800 --> 01:24:51.040]   Right.
[01:24:51.040 --> 01:24:54.400]   It should be really, you know, a driver or a sister, you know,
[01:24:54.400 --> 01:24:55.680]   because on the highway, it's great.
[01:24:55.680 --> 01:24:57.520]   Where there's no stop lights, it's great.
[01:24:57.520 --> 01:24:59.040]   And my Ford does the same thing.
[01:24:59.040 --> 01:25:03.840]   My Mustang does the same thing, but I would never take it on a city street because it wouldn't stop.
[01:25:03.840 --> 01:25:06.880]   So I think he's right to charge this guy.
[01:25:06.880 --> 01:25:10.880]   He was misusing the product, but Tesla does bear some culpability, I think.
[01:25:10.880 --> 01:25:14.880]   Oh, now the reason he wasn't using self-driving.
[01:25:14.880 --> 01:25:18.880]   It wasn't full self-driving in the, I'm sorry, it wasn't full self-driving.
[01:25:18.880 --> 01:25:21.760]   It was auto, Tesla called it auto pilot in those days.
[01:25:21.760 --> 01:25:22.160]   Yeah.
[01:25:22.160 --> 01:25:23.280]   But Tesla still implies it.
[01:25:23.280 --> 01:25:25.600]   It's something, you know, going to stop.
[01:25:25.600 --> 01:25:25.920]   It does.
[01:25:25.920 --> 01:25:28.480]   It Tesla really oversells what they're doing.
[01:25:28.480 --> 01:25:29.360]   Yeah. I apologize.
[01:25:29.360 --> 01:25:29.840]   Yeah.
[01:25:29.840 --> 01:25:30.880]   Not full self-driving.
[01:25:30.880 --> 01:25:32.160]   That's what Tesla's doing now.
[01:25:32.160 --> 01:25:34.560]   And it does stop theoretically at stop lights.
[01:25:34.560 --> 01:25:37.200]   So that's the bigger question for, then this guy is saying,
[01:25:37.200 --> 01:25:42.720]   indemnity, the bigger question is, if you trust what full self-driving
[01:25:42.720 --> 01:25:46.400]   that claims to be full self-driving, and it isn't, and it does something stupid,
[01:25:46.400 --> 01:25:48.000]   are you liable or is it liable?
[01:25:48.000 --> 01:25:49.680]   That's an interesting question.
[01:25:49.680 --> 01:25:49.840]   Right.
[01:25:49.840 --> 01:25:54.560]   So the Brits, and here's my, get ready, my moral panic analysis of this,
[01:25:55.360 --> 01:25:58.000]   is that they love trying to blame technology for everything.
[01:25:58.000 --> 01:26:05.520]   So, and so, I remember my own punchline there, but, but,
[01:26:05.520 --> 01:26:07.840]   should I get this?
[01:26:07.840 --> 01:26:10.560]   It's a reflex to save us blame the technology company
[01:26:10.560 --> 01:26:12.080]   and make the driver blameless.
[01:26:12.080 --> 01:26:16.960]   Well, no, there's got to be some shared blame here in terms of,
[01:26:16.960 --> 01:26:19.680]   if you have the ability to override, you see something going on,
[01:26:19.680 --> 01:26:21.440]   you're still responsible, you're a car.
[01:26:21.440 --> 01:26:24.640]   And for the Brits to come on and recommend the law commission,
[01:26:24.640 --> 01:26:29.760]   commissions, multiple, to recommend that drivers should not face regulatory legal sanctions,
[01:26:29.760 --> 01:26:30.720]   if something goes wrong.
[01:26:30.720 --> 01:26:37.680]   Okay, there's liability on the software that screws up,
[01:26:37.680 --> 01:26:39.920]   but there should be on the driver as well.
[01:26:39.920 --> 01:26:43.200]   And this is a really odd thing to come out with.
[01:26:43.200 --> 01:26:47.840]   Well, it depends on how we view self-driving.
[01:26:47.840 --> 01:26:52.320]   If I'm in a, I mean, especially because, you know, the auto industry is pitching
[01:26:52.960 --> 01:26:56.720]   level five self-driving is something that won't even have a steering wheel, right?
[01:26:56.720 --> 01:27:00.320]   We're all just hanging out even how much you don't have, clearly,
[01:27:00.320 --> 01:27:01.840]   like upload on that.
[01:27:01.840 --> 01:27:01.840]   Right.
[01:27:01.840 --> 01:27:02.320]   But,
[01:27:02.320 --> 01:27:03.280]   I agree.
[01:27:03.280 --> 01:27:04.640]   So, I feel like,
[01:27:04.640 --> 01:27:10.640]   and people are people, and this is why Tesla,
[01:27:10.640 --> 01:27:14.800]   I have so much ire for them for the way they've pitched autopilot,
[01:27:14.800 --> 01:27:19.040]   because people, if you, if you give them the opportunity not to pay attention,
[01:27:19.040 --> 01:27:19.520]   that's the concern.
[01:27:19.520 --> 01:27:20.240]   That's the concern.
[01:27:20.240 --> 01:27:21.680]   They're going to take it.
[01:27:21.680 --> 01:27:24.400]   Can we say that's okay, or do we say no?
[01:27:24.400 --> 01:27:25.600]   No, it's not okay.
[01:27:25.600 --> 01:27:26.080]   You still responsible.
[01:27:26.080 --> 01:27:27.760]   That's why that guy should be charged.
[01:27:27.760 --> 01:27:29.200]   That's why the Brits are saying,
[01:27:29.200 --> 01:27:33.280]   the Brits are saying is unrealistic to respect someone who is not paying attention
[01:27:33.280 --> 01:27:36.160]   to the road to deal with, for example, a tire blowout,
[01:27:36.160 --> 01:27:37.680]   TYRE, I don't know if they do that.
[01:27:37.680 --> 01:27:38.560]   So pay attention.
[01:27:38.560 --> 01:27:38.960]   Loads.
[01:27:38.960 --> 01:27:41.360]   So, so Stacy.
[01:27:41.360 --> 01:27:44.800]   But what they're saying is they're saying, they're putting this there,
[01:27:44.800 --> 01:27:51.280]   because then the automotive people have to be much more realistic about what they're promising,
[01:27:51.280 --> 01:27:52.160]   I think.
[01:27:52.160 --> 01:27:52.640]   Here's where.
[01:27:52.640 --> 01:27:55.440]   They're saying, look, the blame is not going to be on consumers.
[01:27:55.440 --> 01:28:00.080]   It's going to be on you guys, then they can't go to the consumers and sell them something that
[01:28:00.080 --> 01:28:01.520]   isn't actually okay.
[01:28:01.520 --> 01:28:03.520]   I think I'll just say, here's what the market's like.
[01:28:03.520 --> 01:28:05.360]   Here's what Pete Buttigieg says.
[01:28:05.360 --> 01:28:07.280]   He's the secretary of transportation.
[01:28:07.280 --> 01:28:08.640]   He told the Verge this,
[01:28:08.640 --> 01:28:11.360]   I keep saying this until I'm blue in the face.
[01:28:11.360 --> 01:28:16.720]   Anything on the market today that you can buy is a driver assistance technology,
[01:28:16.720 --> 01:28:19.440]   not a driver replacement technology.
[01:28:19.440 --> 01:28:20.960]   I don't care what it's called.
[01:28:20.960 --> 01:28:24.480]   We need to make sure that we're crystal clear about that, even if companies
[01:28:24.480 --> 01:28:24.880]   are not.
[01:28:24.880 --> 01:28:28.400]   And actually, this is part of an article from the Verge talking about how the
[01:28:28.400 --> 01:28:32.240]   self-driving car industry is abandoning the term self-driving.
[01:28:32.240 --> 01:28:33.600]   They don't want to use it anymore.
[01:28:33.600 --> 01:28:40.400]   Waymo, Ford, Lyft, Uber, and Volvo are saying, we're not going to use it.
[01:28:40.400 --> 01:28:44.000]   The problem is Tesla's going to keep using it.
[01:28:44.000 --> 01:28:50.160]   And there's some concern that Tesla kind of let Tesla own that market,
[01:28:50.160 --> 01:28:52.080]   even if it's just a marketing term.
[01:28:52.080 --> 01:28:57.040]   But if Tesla is legally liable, then they might start
[01:28:57.040 --> 01:29:02.880]   and know either they're not selling it that way, or they'll actually deliver real,
[01:29:02.880 --> 01:29:07.280]   high quality stuff because they don't want to be, and they can't right now because it's not.
[01:29:07.280 --> 01:29:11.760]   It's really up to, and I'm trying to remember if it's the NTSB or NHTSA,
[01:29:11.760 --> 01:29:16.160]   but it's up to whatever that federal aid, there is one of those two federal agencies.
[01:29:16.160 --> 01:29:20.960]   I think it's NHTSA that says what is full self-driving, what's not.
[01:29:20.960 --> 01:29:23.440]   It's up to them to say that you can't use that term.
[01:29:23.440 --> 01:29:24.560]   They could do that, by the way.
[01:29:24.560 --> 01:29:26.880]   But federal regulation doesn't require a law.
[01:29:26.880 --> 01:29:27.760]   But that's like level five.
[01:29:27.760 --> 01:29:29.440]   That's not like normal language.
[01:29:29.440 --> 01:29:32.880]   So like level five is us sitting in our car as a living room.
[01:29:32.880 --> 01:29:36.400]   Level four is everything up to level four as drivers.
[01:29:36.400 --> 01:29:38.320]   But there's no such thing as level five right now.
[01:29:38.320 --> 01:29:39.840]   Right.
[01:29:39.840 --> 01:29:43.840]   And so to call it level five and apply full self-driving is level five.
[01:29:43.840 --> 01:29:47.920]   But Tesla doesn't call it level five, they call it autopilot.
[01:29:47.920 --> 01:29:51.680]   No, they call it full self-driving, FSD.
[01:29:51.680 --> 01:29:52.960]   That's what they're selling.
[01:29:52.960 --> 01:29:57.680]   Okay, yeah, but they don't call it quote unquote level five,
[01:29:57.680 --> 01:29:59.600]   which is the actual words.
[01:29:59.600 --> 01:30:02.720]   Okay, but doesn't full self-driving and apply level five?
[01:30:02.720 --> 01:30:04.240]   If it's synonymous.
[01:30:04.240 --> 01:30:07.280]   Well, I would argue that, but a lawyer can get away saying,
[01:30:07.280 --> 01:30:09.920]   I mean, that's the issue.
[01:30:09.920 --> 01:30:10.800]   So you've got to really-
[01:30:10.800 --> 01:30:13.280]   It's something like way more several years ago,
[01:30:13.280 --> 01:30:14.400]   this is from the Verge article,
[01:30:14.400 --> 01:30:18.080]   way more considered developing an advanced driver assist system
[01:30:18.080 --> 01:30:20.960]   like Tesla's full self-driving version of autopilot.
[01:30:20.960 --> 01:30:23.920]   But ultimately decided against it having become alarmed
[01:30:23.920 --> 01:30:29.840]   by the negative effects on the driver drivers would zone out or fall asleep at the wheel.
[01:30:29.840 --> 01:30:34.320]   Is there something in the bill of sales that's like fine print that says,
[01:30:34.320 --> 01:30:36.000]   hey, you still need to be alert.
[01:30:36.000 --> 01:30:36.480]   Oh, yeah.
[01:30:36.480 --> 01:30:37.440]   Engage in these.
[01:30:37.440 --> 01:30:39.520]   Well, they said it.
[01:30:39.520 --> 01:30:42.880]   Do they still have the thing on the drivers?
[01:30:42.880 --> 01:30:45.600]   So you have to touch the steering wheel every moment.
[01:30:45.600 --> 01:30:46.800]   Is that still a thing?
[01:30:46.800 --> 01:30:48.080]   This is what's really interesting.
[01:30:48.080 --> 01:30:50.560]   I can't remember what somebody in the chat will tell us.
[01:30:50.560 --> 01:30:56.800]   We don't have FSDrief, Tesla's, but on the autopilot,
[01:30:56.800 --> 01:30:58.720]   yeah, you had to torque the wheel a little bit.
[01:30:58.720 --> 01:31:03.200]   Four GM and others are coming out with their new self-driving,
[01:31:03.200 --> 01:31:05.280]   where you don't have to have your hands on the wheel.
[01:31:05.280 --> 01:31:09.040]   But in all those cases, there's a camera that's watching your eyes
[01:31:09.040 --> 01:31:10.800]   to see that you're paying attention that you're there.
[01:31:10.800 --> 01:31:14.320]   I don't believe Tesla's doing that.
[01:31:14.320 --> 01:31:17.120]   So I'm not sure how Tesla's verifying that the driver is there.
[01:31:17.120 --> 01:31:19.440]   Let's say they're not doing it.
[01:31:19.440 --> 01:31:25.680]   I think that's even in the sign-in of the contract that you allegedly read your contract
[01:31:25.680 --> 01:31:29.200]   and it says, hey, what's I engaged is I am responsible for paying--
[01:31:29.200 --> 01:31:30.560]   Oh, they pop up things on the--
[01:31:30.560 --> 01:31:32.160]   They tell you.
[01:31:32.160 --> 01:31:37.520]   I mean, I don't think it's enough to put it in the contract or the owner's manual.
[01:31:37.520 --> 01:31:40.240]   You got to pop it up on the screen, but they could do that.
[01:31:40.240 --> 01:31:47.360]   This gets to a problem we have, which is we're making driving more boring by doing this.
[01:31:47.360 --> 01:31:53.520]   This little mid-level where highway driving is relaxed with a Tesla and highway.
[01:31:53.520 --> 01:31:55.600]   I've rented cars that have this and it's fine.
[01:31:55.600 --> 01:32:01.200]   But as a person who I need to be really stimulated when I'm doing anything
[01:32:01.200 --> 01:32:06.240]   to keep paying attention to it, so it has a real problem for me to actually zone out.
[01:32:06.240 --> 01:32:11.920]   It's much easier for me to zone out if I'm in a half-self-driving thing.
[01:32:11.920 --> 01:32:12.720]   So I get it.
[01:32:12.720 --> 01:32:15.840]   I think it's a really dangerous time to be driving cars.
[01:32:15.840 --> 01:32:18.080]   Here's what Tesla says right now.
[01:32:18.080 --> 01:32:23.360]   Autopilot and full self-driving capability are intended for use with a fully-attentive driver
[01:32:23.360 --> 01:32:27.360]   who has their hands on the wheel and is prepared to take over at any moment.
[01:32:27.360 --> 01:32:31.840]   While these features are designed to become more capable over time,
[01:32:31.840 --> 01:32:36.000]   the currently-enabled features do not make the vehicle autonomous.
[01:32:36.000 --> 01:32:41.360]   So that documentation doesn't cover them legally from a liability standpoint?
[01:32:41.360 --> 01:32:43.360]   I don't know.
[01:32:43.360 --> 01:32:44.240]   It's not in the UK.
[01:32:44.240 --> 01:32:47.360]   I mean, it says it right there.
[01:32:47.360 --> 01:32:52.320]   If you're ever going to want to regulate technology, this is the one to regulate.
[01:32:52.320 --> 01:32:54.000]   This is the one to set guidelines about.
[01:32:54.000 --> 01:32:54.880]   Oh, I'm in the people, yeah.
[01:32:54.880 --> 01:32:56.960]   Yeah, that's amazing.
[01:32:56.960 --> 01:33:00.240]   I don't know if you have to touch the wheel or not.
[01:33:00.240 --> 01:33:05.040]   Is anybody have a FSD Tesla in the chat room?
[01:33:05.040 --> 01:33:06.720]   I'm curious.
[01:33:06.720 --> 01:33:13.120]   I get the sense that you do, but I don't know.
[01:33:13.120 --> 01:33:15.360]   I guess nobody we know has an FSD.
[01:33:15.360 --> 01:33:15.840]   Not so many answers.
[01:33:15.840 --> 01:33:17.440]   Yeah, nobody we know has an FSD.
[01:33:17.440 --> 01:33:19.760]   Redcon 5 has it in the Discord.
[01:33:19.760 --> 01:33:20.320]   Oh, does he?
[01:33:20.320 --> 01:33:21.600]   Yeah, take a look.
[01:33:21.600 --> 01:33:23.520]   Nobody just read car.
[01:33:23.520 --> 01:33:25.200]   Yeah, here he is driving his car.
[01:33:25.200 --> 01:33:28.560]   That's the automated traffic.
[01:33:28.560 --> 01:33:31.520]   [laughter]
[01:33:31.520 --> 01:33:32.880]   Nobody tries to write car.
[01:33:32.880 --> 01:33:35.440]   There's a system right there.
[01:33:35.440 --> 01:33:35.840]   Right there.
[01:33:35.840 --> 01:33:37.120]   Dog tired.
[01:33:37.120 --> 01:33:37.600]   Good night.
[01:33:37.600 --> 01:33:40.880]   I don't know.
[01:33:40.880 --> 01:33:41.440]   I don't know.
[01:33:41.440 --> 01:33:43.600]   I would hope there's some sort of mechanism.
[01:33:43.600 --> 01:33:49.280]   There is in the GM and Ford's Blue Cruise and GM's something.
[01:33:49.280 --> 01:33:50.960]   There's a camera.
[01:33:50.960 --> 01:33:57.440]   I would hope that too, but I'm just thinking about our legal system and loopholes and all of that stuff.
[01:33:57.440 --> 01:34:01.200]   And Tesla can just put something in writing just like that.
[01:34:01.200 --> 01:34:04.800]   It says, "Hey, make sure you're doing the right thing when you're engaging this."
[01:34:04.800 --> 01:34:05.840]   We warned you.
[01:34:05.840 --> 01:34:12.240]   Out of sync says Tesla uses the interior camera for FSD beta to watch the drivers and will turn off
[01:34:12.240 --> 01:34:15.120]   FSD if they catch you, not paying attention.
[01:34:15.120 --> 01:34:17.120]   That could catch you.
[01:34:17.120 --> 01:34:17.760]   Oh, well, there you go.
[01:34:17.760 --> 01:34:20.000]   You have to earn FSD at least right now.
[01:34:20.000 --> 01:34:21.680]   You have to have a good driving record.
[01:34:21.680 --> 01:34:23.600]   You have to drive perfectly and all that stuff.
[01:34:26.560 --> 01:34:30.800]   And the other thing is both GM and Ford will only do it on City Streets.
[01:34:30.800 --> 01:34:32.160]   They'll only do it on streets.
[01:34:32.160 --> 01:34:33.520]   They've mapped out highways.
[01:34:33.520 --> 01:34:36.080]   How do you know then?
[01:34:36.080 --> 01:34:39.520]   Does it look a little like, come on, say, and you can use it now or it goes off your feet?
[01:34:39.520 --> 01:34:40.240]   Yeah, exactly.
[01:34:40.240 --> 01:34:40.800]   Oh, yeah.
[01:34:40.800 --> 01:34:41.280]   Interesting.
[01:34:41.280 --> 01:34:41.680]   Yeah.
[01:34:41.680 --> 01:34:43.440]   You know what's fascinating?
[01:34:43.440 --> 01:34:46.640]   I'm just going to throw this out here because it was in one of the short stories
[01:34:46.640 --> 01:34:49.920]   that I read in that book that I read about AI in the future.
[01:34:49.920 --> 01:34:56.320]   This actually does happen already with automated fairies.
[01:34:56.320 --> 01:34:58.000]   So we'll throw that out there too.
[01:34:58.000 --> 01:35:04.320]   One of the premises was that for self-driving vehicles, when they hit weird situations,
[01:35:04.320 --> 01:35:07.200]   they automatically go to a remote person driver.
[01:35:07.200 --> 01:35:08.720]   So like a remote drone driver, right?
[01:35:08.720 --> 01:35:10.160]   Yeah, we talked about that, right?
[01:35:10.160 --> 01:35:12.160]   Did we-- oh, no, that was un-swit.
[01:35:12.160 --> 01:35:15.200]   That was the one where it's like a video game.
[01:35:15.200 --> 01:35:16.480]   Yeah.
[01:35:16.480 --> 01:35:16.880]   Yeah.
[01:35:16.880 --> 01:35:17.920]   So it was kind of like,
[01:35:17.920 --> 01:35:22.720]   yeah, the kids, in some cases, it was actual kids doing a video game.
[01:35:22.720 --> 01:35:28.400]   So the car had the ability to recognize when it no longer could handle a situation,
[01:35:28.400 --> 01:35:31.200]   it would flip over to a remote control operator.
[01:35:31.200 --> 01:35:34.320]   So then the person driving the car still doesn't have to pay attention,
[01:35:34.320 --> 01:35:38.880]   but we don't have to plan for every externality, which would be impossible.
[01:35:38.880 --> 01:35:42.240]   Yeah, this is-- they showed this as CES, the Halo,
[01:35:42.240 --> 01:35:45.440]   HALO self-driving car.
[01:35:45.440 --> 01:35:49.760]   And if the Halo gets confused, like, I don't know what's going on,
[01:35:49.760 --> 01:35:54.320]   a remote driver can take over, which is kind of cool.
[01:35:54.320 --> 01:35:58.160]   Yeah, we talked about that on Twitch, halo.car,
[01:35:58.160 --> 01:36:02.480]   the electric car that drives to you, but there's human driving it.
[01:36:02.480 --> 01:36:08.560]   And there's a company called Eynride that's doing it for trucks in Norway,
[01:36:08.560 --> 01:36:12.880]   and then there's another company that's doing some ferries up there,
[01:36:12.880 --> 01:36:15.680]   where everything's remote controlled until it isn't.
[01:36:15.680 --> 01:36:20.080]   And then it's-- See, I told you I could make a living playing video games.
[01:36:20.080 --> 01:36:29.280]   I told you, mom, what do you think of this? W-B-E-Z in Chicago, public radio.
[01:36:29.280 --> 01:36:33.760]   We were taught-- you said about four, what is it, five or six percent of people who listen to public?
[01:36:33.760 --> 01:36:34.400]   Six to 12.
[01:36:34.400 --> 01:36:35.600]   Six to 12, I'd say.
[01:36:35.600 --> 01:36:37.120]   Six to 12, depending on the station.
[01:36:37.120 --> 01:36:41.440]   That never was enough for them to be rich, but you know what? Change that podcasting,
[01:36:41.440 --> 01:36:48.160]   because they could sell ads on podcasts, and public broadcasting ads on podcasts are very,
[01:36:48.160 --> 01:36:51.440]   very much more expensive than Twitch. They're very, very expensive.
[01:36:51.440 --> 01:36:52.080]   Really?
[01:36:52.080 --> 01:36:52.720]   Oh, God.
[01:36:52.720 --> 01:36:54.720]   Yeah, and their minimums are huge.
[01:36:54.720 --> 01:37:00.400]   I don't know if you remember this, but in the days of public broadcasting,
[01:37:00.400 --> 01:37:01.600]   you couldn't have a call to action.
[01:37:01.600 --> 01:37:07.360]   When you were buying it, you were supporting public broadcasting.
[01:37:07.360 --> 01:37:13.280]   You know, this broadcast of "Downton Abbey" is brought to you by Archer Daniels Midland,
[01:37:13.280 --> 01:37:16.640]   making the world safe for, you know--
[01:37:16.640 --> 01:37:19.200]   We like calling Brits too.
[01:37:19.200 --> 01:37:25.280]   Yeah, but it never could say call 1-800-- Archer Daniels, but that all changed with podcasts.
[01:37:25.280 --> 01:37:30.480]   And you'll note all of the public radio broadcasts, podcasts have lots of ads in them.
[01:37:30.480 --> 01:37:32.720]   So suddenly, I think they're flush with cash.
[01:37:33.280 --> 01:37:39.200]   So that's the lead up to this. WBEZ just bought the Chicago Sun Times.
[01:37:39.200 --> 01:37:43.920]   Obviously, I'm going to put this in the rundown. And it so happens last night,
[01:37:43.920 --> 01:37:49.520]   I talked to Matt Mugh as a Mugh synthesizer who is the CEO of WBEZ.
[01:37:49.520 --> 01:37:53.040]   And so now he's in charge of a newspaper. We had a fascinating conversation about all this.
[01:37:53.040 --> 01:37:55.440]   Now, there's a no cash deal, right?
[01:37:55.440 --> 01:37:56.400]   Right.
[01:37:56.400 --> 01:37:56.400]   Right.
[01:37:56.400 --> 01:38:01.760]   So it's not their fat wallet that's buying this, but what are they giving--
[01:38:01.760 --> 01:38:02.400]   No, no, no.
[01:38:03.120 --> 01:38:05.360]   Well, it's going to keep the Sun Times alive.
[01:38:05.360 --> 01:38:09.920]   The Sun Times-- they wanted to stop the Sun Times being bought by one of the evil players.
[01:38:09.920 --> 01:38:11.360]   And kind of the employees did that.
[01:38:11.360 --> 01:38:11.840]   Like the trough.
[01:38:11.840 --> 01:38:13.280]   And this was safe home for it.
[01:38:13.280 --> 01:38:18.640]   Well, trough across the street, my old stomping grounds, the Tribune is now owned by Alden,
[01:38:18.640 --> 01:38:22.560]   which is the worst hedge fund. And by the full disclosure, I used to be an advisor to Digital
[01:38:22.560 --> 01:38:28.080]   First, which was also owned by Alden and left long ago.
[01:38:28.880 --> 01:38:34.720]   And so Tribune is pretty much hated now. So there's an opportunity now.
[01:38:34.720 --> 01:38:36.080]   So I'm wrong in this Sun Times.
[01:38:36.080 --> 01:38:40.720]   Well, we're saying this is a sign of the times that these public broadcasting is now
[01:38:40.720 --> 01:38:42.880]   so flush they're buying newspapers. They didn't buy it.
[01:38:42.880 --> 01:38:43.840]   No, they didn't buy it.
[01:38:43.840 --> 01:38:44.800]   They just took it over.
[01:38:44.800 --> 01:38:46.400]   They're going to run-- they're going to run-- they're going to run-- they have a large
[01:38:46.400 --> 01:38:47.760]   newsroom. It's twice the size of Tribune.
[01:38:47.760 --> 01:38:49.920]   So I apologize, B E C. I apologize.
[01:38:49.920 --> 01:38:54.800]   And well, B E C does really good stuff, and they make a lot of money from podcasts.
[01:38:54.800 --> 01:38:55.280]   Right.
[01:38:55.280 --> 01:38:57.600]   This American life being primary there.
[01:38:57.600 --> 01:38:58.160]   Right.
[01:38:58.160 --> 01:39:03.440]   But the point is, the opportunity is to undercut Tribune and create a free news,
[01:39:03.440 --> 01:39:08.400]   equality ecosystem across Chicago land, as we Chicagoans call it,
[01:39:08.400 --> 01:39:13.120]   and undercut the Tribune and undercut Alden.
[01:39:13.120 --> 01:39:16.960]   So it's going to be fascinating to watch in this one-to-one city.
[01:39:16.960 --> 01:39:21.280]   And I broke-- what are you-- you bought your teeth?
[01:39:21.280 --> 01:39:22.480]   No, what are you doing? You learned something.
[01:39:22.480 --> 01:39:23.840]   You bit your tongue.
[01:39:23.840 --> 01:39:25.440]   No, I didn't do that.
[01:39:25.440 --> 01:39:27.040]   That's when you don't do the comments.
[01:39:27.040 --> 01:39:27.680]   Cut into--
[01:39:27.680 --> 01:39:28.560]   I cut my teeth.
[01:39:28.560 --> 01:39:31.280]   I cut my teeth in Chicago journalists.
[01:39:31.280 --> 01:39:34.080]   I worked for Chicago today in the paper that had no tomorrow,
[01:39:34.080 --> 01:39:35.440]   owned by Chicago Tribune.
[01:39:35.440 --> 01:39:37.200]   And when I was there, there were four papers.
[01:39:37.200 --> 01:39:39.760]   Chicago Daily News sometimes Tribune today.
[01:39:39.760 --> 01:39:41.200]   And now it's two.
[01:39:41.200 --> 01:39:43.680]   It's going to be a newspaper war of a whole new--
[01:39:43.680 --> 01:39:45.520]   The Sun Times was storied.
[01:39:45.520 --> 01:39:46.720]   It's Andy and I co-work there.
[01:39:46.720 --> 01:39:48.160]   So did Roger Ebert.
[01:39:48.160 --> 01:39:49.680]   It was a storied newspaper.
[01:39:49.680 --> 01:39:55.680]   But hard times-- I remember they fired their entire photography staff.
[01:39:55.680 --> 01:39:56.800]   It's like a murder-coded--
[01:39:56.800 --> 01:39:58.560]   Yeah, it said Murdoch owned it for a while.
[01:39:58.560 --> 01:39:59.040]   Yeah.
[01:39:59.040 --> 01:39:59.600]   Told them all.
[01:39:59.600 --> 01:40:00.800]   He used the smartphones.
[01:40:00.800 --> 01:40:01.280]   Is that--
[01:40:01.280 --> 01:40:01.760]   Yeah.
[01:40:01.760 --> 01:40:02.160]   Is that just--
[01:40:02.160 --> 01:40:02.560]   Yeah, they said--
[01:40:02.560 --> 01:40:04.240]   Well, those were the smartphones, even--
[01:40:04.240 --> 01:40:04.560]   Oh.
[01:40:04.560 --> 01:40:04.720]   It was even--
[01:40:04.720 --> 01:40:05.200]   It was--
[01:40:05.200 --> 01:40:05.680]   It was--
[01:40:05.680 --> 01:40:08.720]   There were equipping reporters who were pretty stupid about cameras
[01:40:08.720 --> 01:40:12.320]   because I'm one with cameras to go out and throw stuff.
[01:40:12.320 --> 01:40:16.320]   Like the one-- one man band so-called in TV were credit--
[01:40:16.320 --> 01:40:17.120]   The producer.
[01:40:17.120 --> 01:40:17.840]   --they called that.
[01:40:17.840 --> 01:40:18.160]   The producer.
[01:40:18.160 --> 01:40:19.360]   The car the other day was for--
[01:40:19.360 --> 01:40:20.320]   He was post.
[01:40:20.320 --> 01:40:22.240]   Her producer or her camera person or everything.
[01:40:23.280 --> 01:40:26.080]   So yes, it's going to be really interesting to watch.
[01:40:26.080 --> 01:40:26.640]   She's OK.
[01:40:26.640 --> 01:40:27.680]   On the right.
[01:40:27.680 --> 01:40:28.480]   Yeah, she was OK.
[01:40:28.480 --> 01:40:29.280]   Yes, she's fine.
[01:40:29.280 --> 01:40:32.640]   But I didn't realize that was because she had to do it all.
[01:40:32.640 --> 01:40:35.600]   Yeah, she set up the camera.
[01:40:35.600 --> 01:40:36.320]   She did everything.
[01:40:36.320 --> 01:40:37.200]   So sad.
[01:40:37.200 --> 01:40:40.000]   I see that all the time and I say, I'm so sorry.
[01:40:40.000 --> 01:40:43.120]   You had to work in media after this--
[01:40:43.120 --> 01:40:44.480]   these tough times.
[01:40:44.480 --> 01:40:52.720]   Philadelphia Inquirer was donated to a nonprofit by its owner in 2016.
[01:40:52.720 --> 01:40:56.080]   Salt Lake Tribune announced plans to be coming on profit in 2019.
[01:40:56.080 --> 01:40:58.960]   The problem with journalists though is they think, oh good,
[01:40:58.960 --> 01:40:59.840]   we're not for profit now.
[01:40:59.840 --> 01:41:01.440]   We can-- we have no problems.
[01:41:01.440 --> 01:41:04.000]   No, every person I know who runs it not for profit,
[01:41:04.000 --> 01:41:07.600]   Matt Mughatwbez, his predecessor,
[01:41:07.600 --> 01:41:09.680]   Cooley Schickenslami, who just left WNYC,
[01:41:09.680 --> 01:41:12.880]   these people are the smartest business people around
[01:41:12.880 --> 01:41:15.360]   because they've got to do it under more difficult circumstances
[01:41:15.360 --> 01:41:16.880]   and they've got to become sustainable.
[01:41:16.880 --> 01:41:19.920]   So you can't just say, oh, who cares about capitalism?
[01:41:19.920 --> 01:41:20.800]   Uh-oh.
[01:41:20.800 --> 01:41:22.240]   No, well, you have to pay the bills.
[01:41:22.240 --> 01:41:24.000]   You have to run the presses.
[01:41:24.000 --> 01:41:25.840]   Some has got to buy the paper and the ink.
[01:41:25.840 --> 01:41:28.400]   Well, or well, actually not that.
[01:41:28.400 --> 01:41:30.080]   My recommendation would stop doing that as soon as--
[01:41:30.080 --> 01:41:32.640]   Someone has to pay the journalists to go out and hunt stores.
[01:41:32.640 --> 01:41:33.040]   Oh, that.
[01:41:33.040 --> 01:41:33.840]   Mm-hmm.
[01:41:33.840 --> 01:41:34.240]   Yes.
[01:41:34.240 --> 01:41:36.160]   We have to eat too.
[01:41:36.160 --> 01:41:40.240]   The Chicago Sun Times is a dollar in the city,
[01:41:40.240 --> 01:41:41.600]   $2 in the burbs.
[01:41:41.600 --> 01:41:44.000]   That's weird.
[01:41:44.000 --> 01:41:44.800]   I don't.
[01:41:44.800 --> 01:41:46.960]   The hardest working paper in America.
[01:41:46.960 --> 01:41:48.160]   That's--
[01:41:48.160 --> 01:41:49.200]   [laughs]
[01:41:49.200 --> 01:41:50.000]   It's kind of a weird--
[01:41:50.000 --> 01:41:53.360]   Maybe it's more expensive because you have to pay for distribution--
[01:41:53.360 --> 01:41:55.120]   Yeah, they have to drive a truck out there.
[01:41:55.120 --> 01:41:57.040]   [laughs]
[01:41:57.040 --> 01:41:58.800]   That's fascinating.
[01:41:58.800 --> 01:42:00.080]   So I didn't realize it was--
[01:42:00.080 --> 01:42:02.240]   I just thought, oh, they just had a lot of extra money.
[01:42:02.240 --> 01:42:04.800]   They thought, what can we do with this American life money?
[01:42:04.800 --> 01:42:05.840]   It's a rescue thing.
[01:42:05.840 --> 01:42:06.240]   Good.
[01:42:06.240 --> 01:42:06.800]   Yeah.
[01:42:06.800 --> 01:42:07.120]   Good.
[01:42:07.120 --> 01:42:08.160]   And I think that's a good thing.
[01:42:08.160 --> 01:42:10.560]   I mean, it's not the end of the line by any means.
[01:42:10.560 --> 01:42:12.000]   You still have to make it work.
[01:42:12.000 --> 01:42:15.040]   Uh...
[01:42:15.040 --> 01:42:17.760]   There's a couple of Spotify stories that are interesting.
[01:42:17.760 --> 01:42:18.720]   Yeah, Spotify--
[01:42:18.720 --> 01:42:19.680]   Mm-hmm.
[01:42:19.680 --> 01:42:24.400]   In a lot of trouble because they're $100 million golden boy,
[01:42:24.400 --> 01:42:28.000]   Joe Rogan keeps putting people on who say, like,
[01:42:28.000 --> 01:42:33.040]   Eric Clapton who says you're being hypnotized into taking the vaccine.
[01:42:33.040 --> 01:42:36.240]   And so there's a lot of heat to it.
[01:42:36.240 --> 01:42:37.920]   Ask them about the vaccine.
[01:42:37.920 --> 01:42:38.880]   So this is--
[01:42:38.880 --> 01:42:42.320]   He gets these people on that, you know, Eric Clapton,
[01:42:42.320 --> 01:42:44.560]   maybe you don't ask Eric Clapton knowing his views.
[01:42:44.560 --> 01:42:48.320]   No, no, the guy he brought on that's controversial was December 31st
[01:42:48.320 --> 01:42:53.920]   is the doctor who claims he invented the mRNA vaccine,
[01:42:53.920 --> 01:42:57.360]   who is saying you're being hypnotized with subliminal messages.
[01:42:57.360 --> 01:42:59.440]   Clapton must have heard that episode.
[01:42:59.440 --> 01:43:00.080]   That's what--
[01:43:00.080 --> 01:43:03.360]   So Clapton is one of the people influenced by Joe Rogan.
[01:43:03.360 --> 01:43:05.120]   Yeah, oh, okay.
[01:43:05.120 --> 01:43:05.840]   I was like--
[01:43:05.840 --> 01:43:07.920]   I was like, he could just not ask them about that.
[01:43:07.920 --> 01:43:09.120]   No, no, no, he wants to.
[01:43:09.120 --> 01:43:10.000]   No, that's--
[01:43:10.000 --> 01:43:12.480]   And in fact, Neil Young, who is ProVax,
[01:43:12.480 --> 01:43:15.440]   has asked Spotify to remove his music.
[01:43:15.440 --> 01:43:17.520]   And Spotify just announced they would.
[01:43:17.520 --> 01:43:18.080]   Wow.
[01:43:18.080 --> 01:43:18.960]   So they made a choice.
[01:43:18.960 --> 01:43:19.840]   They called his bluff.
[01:43:19.840 --> 01:43:21.200]   No, no, no.
[01:43:21.200 --> 01:43:22.160]   They called his bluff.
[01:43:22.160 --> 01:43:23.120]   No, my dear.
[01:43:23.120 --> 01:43:24.080]   They called his bluff.
[01:43:24.080 --> 01:43:24.400]   This is--
[01:43:24.400 --> 01:43:26.560]   Sure, Neil, you don't want the money fine.
[01:43:26.560 --> 01:43:28.880]   Yeah, I think that's what it was more of.
[01:43:28.880 --> 01:43:29.360]   Yeah, sure.
[01:43:29.360 --> 01:43:31.120]   That's one less check for us to sign.
[01:43:31.120 --> 01:43:33.760]   270 physicians wrote--
[01:43:33.760 --> 01:43:34.560]   They wrote all that data.
[01:43:34.560 --> 01:43:37.840]   So it's not like Spotify is like--
[01:43:37.840 --> 01:43:40.160]   I mean, if Taylor Swift said she--
[01:43:40.160 --> 01:43:42.080]   if Taylor Swift said this, they may have changed--
[01:43:42.080 --> 01:43:42.880]   I like that idea.
[01:43:42.880 --> 01:43:43.760]   That's a different story.
[01:43:43.760 --> 01:43:44.800]   Come on, Tay Tay.
[01:43:44.800 --> 01:43:46.800]   It's at balls in your court now.
[01:43:46.800 --> 01:43:47.280]   Up to you.
[01:43:47.280 --> 01:43:48.240]   Yeah.
[01:43:48.240 --> 01:43:52.160]   270 doctors, physicians, and science educators.
[01:43:52.160 --> 01:43:52.720]   I like that.
[01:43:52.720 --> 01:43:53.680]   Educators.
[01:43:53.680 --> 01:43:54.160]   Edicators.
[01:43:54.160 --> 01:43:55.360]   Sign an open letter calling on--
[01:43:55.360 --> 01:43:56.480]   You're an educated man.
[01:43:56.480 --> 01:43:57.200]   I can tell.
[01:43:57.200 --> 01:44:02.000]   --an open letter calling on Spotify to take action
[01:44:02.000 --> 01:44:03.120]   against misinformation.
[01:44:03.120 --> 01:44:08.480]   This interview with a doctor--
[01:44:08.480 --> 01:44:10.000]   and I put that in quotes--
[01:44:10.000 --> 01:44:16.080]   named Malone--
[01:44:16.080 --> 01:44:17.360]   Robert Malone.
[01:44:17.360 --> 01:44:18.800]   He's a virologist, says,
[01:44:18.800 --> 01:44:21.600]   "I'm one of the architects of mRNA,
[01:44:21.600 --> 01:44:24.560]   and you are being hypnotized."
[01:44:24.560 --> 01:44:29.360]   Yeah, I think Joe says, "I'm a comedian."
[01:44:29.360 --> 01:44:31.840]   This is entertainment.
[01:44:31.840 --> 01:44:32.880]   It's the same thing, Hannity says.
[01:44:32.880 --> 01:44:34.160]   No, but he's not doing it.
[01:44:34.160 --> 01:44:35.680]   He's doing it seriously.
[01:44:35.680 --> 01:44:36.960]   He knows he's influencing people.
[01:44:36.960 --> 01:44:37.680]   Well, he's a troll.
[01:44:37.680 --> 01:44:40.800]   A lot of people under 30, including my son, are big fans.
[01:44:40.800 --> 01:44:43.600]   And probably do believe this when they hear it.
[01:44:45.120 --> 01:44:45.920]   We know--
[01:44:45.920 --> 01:44:50.960]   the Green Bay Packers quarterback.
[01:44:50.960 --> 01:44:53.120]   Aaron Rodgers.
[01:44:53.120 --> 01:44:54.080]   Aaron Rodgers.
[01:44:54.080 --> 01:44:57.760]   Well, believed the vaccine information he got from Joe Rogan.
[01:44:57.760 --> 01:44:59.120]   And parroted it back.
[01:44:59.120 --> 01:45:01.280]   Apparently, so does Eric Clapton.
[01:45:01.280 --> 01:45:03.440]   And millions of other young men.
[01:45:03.440 --> 01:45:07.920]   So I understand why they're upset by this.
[01:45:07.920 --> 01:45:08.640]   On the other hand, it's--
[01:45:08.640 --> 01:45:10.720]   didn't we talk about this last week,
[01:45:10.720 --> 01:45:11.600]   or maybe we did on Twitter?
[01:45:11.600 --> 01:45:12.960]   That's the problem I confused the two.
[01:45:13.840 --> 01:45:16.160]   I mean, Spotify is going to say,
[01:45:16.160 --> 01:45:17.280]   "Well, it's free speech.
[01:45:17.280 --> 01:45:18.880]   Rogan can say what he wants, right?"
[01:45:18.880 --> 01:45:21.040]   How responsible are they?
[01:45:21.040 --> 01:45:22.480]   No, we talked about this last week.
[01:45:22.480 --> 01:45:25.600]   But Spotify hired him and pays him.
[01:45:25.600 --> 01:45:29.840]   And so, where I normally say that Facebook is not a medium,
[01:45:29.840 --> 01:45:30.640]   Spotify is.
[01:45:30.640 --> 01:45:33.760]   April 23rd, 2021.
[01:45:33.760 --> 01:45:37.760]   He Rogan said, "If you're like 21 years old and you say to me,
[01:45:37.760 --> 01:45:39.520]   should I get vaccinated, I'll go, no."
[01:45:39.520 --> 01:45:43.360]   He also promoted Iver Mechton, the D-Wermer.
[01:45:44.320 --> 01:45:46.160]   As a treatment.
[01:45:46.160 --> 01:45:51.680]   I remember that Iver Mechton controversy.
[01:45:51.680 --> 01:45:52.240]   Yeah.
[01:45:52.240 --> 01:45:56.720]   Anyway, I don't know what's so Spotify said, "Okay,
[01:45:56.720 --> 01:45:58.160]   bye, Neil."
[01:45:58.160 --> 01:45:59.520]   That's interesting.
[01:45:59.520 --> 01:46:02.480]   I didn't see that they had agreed.
[01:46:02.480 --> 01:46:06.320]   Neil Young said, "You can have Joe Rogan or Neil Young.
[01:46:06.320 --> 01:46:07.120]   You can't have both."
[01:46:07.120 --> 01:46:10.320]   You might find also this interesting down--
[01:46:10.320 --> 01:46:13.200]   Now all the way down to line 137,
[01:46:13.200 --> 01:46:15.280]   I found this slightly interesting,
[01:46:15.280 --> 01:46:18.720]   a list of podcasts that Spotify trumpeted heavily,
[01:46:18.720 --> 01:46:19.840]   that haven't been heard of since.
[01:46:19.840 --> 01:46:20.800]   Yeah.
[01:46:20.800 --> 01:46:21.600]   Yeah.
[01:46:21.600 --> 01:46:22.160]   Yeah.
[01:46:22.160 --> 01:46:24.320]   Spotify just emailed me again,
[01:46:24.320 --> 01:46:26.640]   asking for information on my podcast.
[01:46:26.640 --> 01:46:27.360]   Good for you.
[01:46:27.360 --> 01:46:28.640]   For you, Petriod or something.
[01:46:28.640 --> 01:46:31.760]   Well, no, it was weird just because Spotify and Amazon Music
[01:46:31.760 --> 01:46:33.600]   both came at me on the same day.
[01:46:33.600 --> 01:46:33.840]   Right.
[01:46:33.840 --> 01:46:34.800]   And I was like, "What's happening?"
[01:46:34.800 --> 01:46:35.920]   But you're already on both.
[01:46:35.920 --> 01:46:36.720]   I'm sure we are.
[01:46:36.720 --> 01:46:37.680]   I mean, any--
[01:46:37.680 --> 01:46:37.840]   Yeah.
[01:46:37.840 --> 01:46:38.000]   Yeah.
[01:46:38.000 --> 01:46:40.160]   We're on any platform that takes RSS feeds.
[01:46:40.160 --> 01:46:41.040]   We'd--
[01:46:41.040 --> 01:46:43.040]   That's not an endorsement of Spotify.
[01:46:43.040 --> 01:46:45.920]   Should we say, "No, you can't have our--
[01:46:45.920 --> 01:46:47.840]   You can't have Neil Young and you can't have Twit."
[01:46:47.840 --> 01:46:50.080]   Oh, no, no.
[01:46:50.080 --> 01:46:50.480]   I just--
[01:46:50.480 --> 01:46:52.240]   I think it's interesting that they're pushed--
[01:46:52.240 --> 01:46:53.280]   that they've clearly--
[01:46:53.280 --> 01:46:55.440]   Here's--
[01:46:55.440 --> 01:46:57.280]   I don't want to hear you scraping the bottom of the barrel
[01:46:57.280 --> 01:46:58.720]   because I'm not the bottom of the barrel.
[01:46:58.720 --> 01:46:59.280]   There--
[01:46:59.280 --> 01:46:59.680]   There--
[01:46:59.680 --> 01:47:00.160]   Clearly--
[01:47:00.160 --> 01:47:00.880]   No, but you're--
[01:47:00.880 --> 01:47:02.880]   You're not Megan and Harry.
[01:47:02.880 --> 01:47:03.760]   Let's put it that way.
[01:47:03.760 --> 01:47:04.080]   You're not--
[01:47:04.080 --> 01:47:04.560]   Right.
[01:47:04.560 --> 01:47:04.560]   Yeah.
[01:47:04.560 --> 01:47:05.520]   I mean--
[01:47:05.520 --> 01:47:06.640]   Megan and Harry signed up--
[01:47:06.640 --> 01:47:07.200]   Let's be real.
[01:47:07.200 --> 01:47:08.320]   --$ $1 million--
[01:47:08.320 --> 01:47:09.360]   $1 pound--
[01:47:09.360 --> 01:47:09.760]   Sorry.
[01:47:09.760 --> 01:47:12.720]   Deal with Spotify in December of 2020.
[01:47:12.720 --> 01:47:15.040]   They did one show and that's it.
[01:47:15.040 --> 01:47:19.840]   Kim Kardashian West announced to potify--
[01:47:19.840 --> 01:47:21.600]   Potify podcast.
[01:47:21.600 --> 01:47:24.000]   [laughter]
[01:47:24.000 --> 01:47:24.400]   You're--
[01:47:24.400 --> 01:47:25.360]   You're the professional, right?
[01:47:25.360 --> 01:47:27.440]   Did you take the good drugs, Leo?
[01:47:27.440 --> 01:47:28.000]   Yeah.
[01:47:28.000 --> 01:47:29.280]   June 2020.
[01:47:29.280 --> 01:47:33.920]   In March 2021, she hinted she was ready to release her episodes.
[01:47:33.920 --> 01:47:35.040]   Nothing.
[01:47:35.040 --> 01:47:36.400]   This is from Pod News.
[01:47:36.400 --> 01:47:37.760]   PodNews.net.
[01:47:37.760 --> 01:47:40.320]   Anna DuVernay's array signed in the agreement
[01:47:40.320 --> 01:47:41.840]   to produce a scripted and unscripted podcast
[01:47:41.840 --> 01:47:42.960]   January 2021.
[01:47:42.960 --> 01:47:46.400]   Stay tuned yet to be released.
[01:47:46.400 --> 01:47:49.520]   Warner Brothers in DC signed a deal with Spotify
[01:47:49.520 --> 01:47:52.320]   for a set of original podcast June 2020.
[01:47:52.320 --> 01:47:56.000]   Batman Unburied announced casting June 2021.
[01:47:56.000 --> 01:47:58.960]   No show.
[01:47:58.960 --> 01:48:00.720]   Takes a long time to make a podcast.
[01:48:00.720 --> 01:48:02.480]   I will be the first to tell you.
[01:48:02.480 --> 01:48:05.840]   This show, which we are recording in 2022,
[01:48:05.840 --> 01:48:07.600]   won't be out for three or four years.
[01:48:07.600 --> 01:48:10.000]   We got it.
[01:48:10.000 --> 01:48:11.760]   So much for post-production.
[01:48:11.760 --> 01:48:14.640]   A lot of-- there's a whole bunch of them.
[01:48:14.640 --> 01:48:15.120]   Wow.
[01:48:15.120 --> 01:48:15.840]   Is it funny?
[01:48:15.840 --> 01:48:16.800]   Mark and Jay do plot--
[01:48:16.800 --> 01:48:17.760]   A lot of dollars.
[01:48:17.760 --> 01:48:22.320]   Did they pay all these people or did they just make an announcement?
[01:48:22.320 --> 01:48:23.920]   I would hope they have better lawyers than that.
[01:48:23.920 --> 01:48:26.080]   Yeah.
[01:48:26.080 --> 01:48:28.800]   Maybe they did an advance and so if they didn't--
[01:48:28.800 --> 01:48:34.400]   Well, the interesting thing is Spotify's stock value
[01:48:34.400 --> 01:48:39.600]   went up when Megan and Harry announced $836 million.
[01:48:39.600 --> 01:48:42.960]   So they're getting the benefit of this.
[01:48:42.960 --> 01:48:49.680]   This is but churning signed a deal in September 2020,
[01:48:49.680 --> 01:48:54.000]   promising more than 250 shows to transform into podcasts.
[01:48:54.000 --> 01:48:54.480]   Yeah.
[01:48:54.480 --> 01:48:55.760]   They've yet to release any shows.
[01:48:55.760 --> 01:48:56.320]   No shows.
[01:48:56.320 --> 01:48:59.360]   The Russo brothers--
[01:48:59.360 --> 01:49:01.680]   I hope they get all the winners in the building though.
[01:49:01.680 --> 01:49:02.400]   Oh, wait, wait.
[01:49:02.400 --> 01:49:04.640]   Variety just published a story saying
[01:49:04.640 --> 01:49:07.760]   Gimlet Studio is hiring in-house producers for Harry and Megan
[01:49:07.760 --> 01:49:09.040]   Markle's weekly podcast.
[01:49:09.040 --> 01:49:13.280]   So we'll hear about it.
[01:49:13.280 --> 01:49:13.760]   No shows.
[01:49:13.760 --> 01:49:15.040]   So basically they're like,
[01:49:15.040 --> 01:49:17.440]   clearly we can't rely on them to do this.
[01:49:17.440 --> 01:49:20.080]   And of course Gimlet is owned by Spotify.
[01:49:20.080 --> 01:49:21.920]   We should point out.
[01:49:21.920 --> 01:49:22.720]   Speaking of podcasts--
[01:49:22.720 --> 01:49:23.920]   So if you want to produce it,
[01:49:23.920 --> 01:49:28.080]   they're looking for two producers for six-month contract positions.
[01:49:28.080 --> 01:49:29.120]   Six-month contract.
[01:49:29.120 --> 01:49:30.560]   That doesn't sound very problematic.
[01:49:30.560 --> 01:49:31.600]   I'm like, oh.
[01:49:31.600 --> 01:49:32.480]   See how it goes.
[01:49:32.480 --> 01:49:35.760]   Did you watch Only Murders in the Building?
[01:49:35.760 --> 01:49:36.560]   I have watched it.
[01:49:36.560 --> 01:49:36.880]   Yes.
[01:49:36.880 --> 01:49:38.160]   That was very good.
[01:49:38.160 --> 01:49:39.440]   Well, this started in.
[01:49:39.440 --> 01:49:40.240]   Yeah, and then finish it.
[01:49:40.240 --> 01:49:41.200]   Wait, don't spoil it.
[01:49:41.200 --> 01:49:42.080]   I might come back to it.
[01:49:42.080 --> 01:49:42.640]   Don't tell Steve.
[01:49:42.640 --> 01:49:43.440]   It's fun.
[01:49:43.440 --> 01:49:44.160]   It's OK.
[01:49:44.160 --> 01:49:44.640]   It's fine.
[01:49:44.640 --> 01:49:45.520]   It's fun.
[01:49:45.520 --> 01:49:47.360]   It was about podcasting, which I like.
[01:49:47.360 --> 01:49:47.760]   What was I saying?
[01:49:47.760 --> 01:49:48.640]   It was about podcast.
[01:49:48.640 --> 01:49:49.520]   About who?
[01:49:49.520 --> 01:49:51.440]   Crime, true crime podcasts.
[01:49:51.440 --> 01:49:51.920]   Mm.
[01:49:51.920 --> 01:49:52.480]   A lot of good.
[01:49:52.480 --> 01:49:53.520]   Are they still, by the way,
[01:49:53.520 --> 01:49:56.080]   that was the all the rage last year, true crime.
[01:49:56.080 --> 01:49:57.040]   Is that still a thing?
[01:49:57.040 --> 01:49:58.400]   I don't know.
[01:49:58.400 --> 01:49:59.040]   True crime.
[01:49:59.040 --> 01:50:00.480]   Still loves true crime.
[01:50:00.480 --> 01:50:02.000]   All the people we talked to said,
[01:50:02.000 --> 01:50:03.760]   what, where's your true crime show?
[01:50:03.760 --> 01:50:04.960]   I said, we're a tech network.
[01:50:04.960 --> 01:50:05.520]   We don't have--
[01:50:05.520 --> 01:50:06.080]   You're right.
[01:50:06.080 --> 01:50:07.680]   We don't have--
[01:50:07.680 --> 01:50:09.920]   There's some crazy kind of crime world.
[01:50:09.920 --> 01:50:10.880]   No, actually, I came--
[01:50:10.880 --> 01:50:11.360]   I literally--
[01:50:11.360 --> 01:50:11.520]   I literally--
[01:50:11.520 --> 01:50:12.320]   I'm a sci-fi angle.
[01:50:12.320 --> 01:50:14.640]   Came up with a treatment for a true crime podcast
[01:50:14.640 --> 01:50:16.400]   we could produce that is right down our house.
[01:50:16.400 --> 01:50:16.880]   Wow.
[01:50:16.880 --> 01:50:20.000]   But then I thought, I'm not going to pander to these bozos.
[01:50:20.000 --> 01:50:20.960]   So I didn't do it.
[01:50:20.960 --> 01:50:23.520]   Was it the Broadcom sex dungeon?
[01:50:23.520 --> 01:50:26.640]   No, but that's intriguing.
[01:50:28.960 --> 01:50:29.600]   No, it was--
[01:50:29.600 --> 01:50:30.800]   He used the dungeon sounds.
[01:50:30.800 --> 01:50:31.520]   It was about--
[01:50:31.520 --> 01:50:33.520]   And I kind of almost don't want to even--
[01:50:33.520 --> 01:50:35.680]   Talking about Adrien Lamo, who was a hacker,
[01:50:35.680 --> 01:50:38.080]   I knew him pretty well in the day.
[01:50:38.080 --> 01:50:38.880]   He was autistic.
[01:50:38.880 --> 01:50:41.040]   He was kind of a genius hacker.
[01:50:41.040 --> 01:50:43.200]   Died was--
[01:50:43.200 --> 01:50:45.840]   I think it was ruled a suicide.
[01:50:45.840 --> 01:50:48.960]   But there were some very suspicious circumstances around it,
[01:50:48.960 --> 01:50:51.920]   including the fact that he had taped onto his thigh
[01:50:51.920 --> 01:50:53.840]   a phone number and a name.
[01:50:53.840 --> 01:50:57.040]   There were some very suspicious circumstances.
[01:50:57.040 --> 01:50:59.680]   So I thought that'd be a good true crime podcast
[01:50:59.680 --> 01:51:00.960]   who killed Adrien Lamo.
[01:51:00.960 --> 01:51:02.080]   And it's right up our alley.
[01:51:02.080 --> 01:51:05.280]   We could talk about his hacking skills and all that.
[01:51:05.280 --> 01:51:07.120]   So there, I just gave it away.
[01:51:07.120 --> 01:51:07.680]   Somebody should--
[01:51:07.680 --> 01:51:08.240]   There you go.
[01:51:08.240 --> 01:51:09.040]   Somebody just got it there.
[01:51:09.040 --> 01:51:10.000]   Megan and Harry should do it.
[01:51:10.000 --> 01:51:10.560]   It'd be great.
[01:51:10.560 --> 01:51:16.320]   Is that 256 encryption?
[01:51:16.320 --> 01:51:18.800]   Can you imagine that conversation?
[01:51:18.800 --> 01:51:20.400]   There's--
[01:51:20.400 --> 01:51:21.200]   Yeah, I just--
[01:51:21.200 --> 01:51:22.240]   I don't know.
[01:51:22.240 --> 01:51:26.400]   We are going to cover Jason Hall has told me he will get up at 7
[01:51:26.400 --> 01:51:28.080]   in the morning because that's when it is--
[01:51:28.080 --> 01:51:29.120]   Pacific Times--
[01:51:29.120 --> 01:51:32.880]   February 9th, the launch of the Galaxy S22.
[01:51:32.880 --> 01:51:36.320]   Samsung Unpack it, or whatever they call it.
[01:51:36.320 --> 01:51:37.600]   Samsung Unsomething.
[01:51:37.600 --> 01:51:38.800]   Unpack.
[01:51:38.800 --> 01:51:39.360]   Unpack?
[01:51:39.360 --> 01:51:40.320]   Is it Unpacked again?
[01:51:40.320 --> 01:51:41.280]   Yes, sir.
[01:51:41.280 --> 01:51:44.400]   S22, 22 plus, Galaxy S22 Ultra.
[01:51:44.400 --> 01:51:45.600]   It's interesting in the--
[01:51:45.600 --> 01:51:48.240]   Yeah, Galaxy Unpacked.
[01:51:48.240 --> 01:51:50.720]   The epic standard.
[01:51:50.720 --> 01:51:52.240]   S.
[01:51:52.240 --> 01:51:55.840]   Then in one of the announcements they said,
[01:51:55.840 --> 01:51:59.680]   it's a notable product announcement.
[01:51:59.680 --> 01:52:01.920]   And of course, they are killing the note.
[01:52:01.920 --> 01:52:03.840]   So we think this will be a note-like phone,
[01:52:03.840 --> 01:52:05.200]   maybe with a stylus and a lot.
[01:52:05.200 --> 01:52:05.760]   Oh.
[01:52:05.760 --> 01:52:06.320]   Oh.
[01:52:06.320 --> 01:52:06.720]   Oh.
[01:52:06.720 --> 01:52:07.200]   Oh.
[01:52:07.200 --> 01:52:08.160]   Oh.
[01:52:08.160 --> 01:52:09.120]   Oh.
[01:52:09.120 --> 01:52:10.960]   Eight minutes later, Stacey was like, oh, I can't.
[01:52:10.960 --> 01:52:11.360]   I can't.
[01:52:11.360 --> 01:52:11.920]   I can't.
[01:52:11.920 --> 01:52:12.560]   I can't.
[01:52:12.560 --> 01:52:13.360]   I can't get it.
[01:52:13.360 --> 01:52:15.360]   [LAUGHTER]
[01:52:15.360 --> 01:52:16.320]   And the--
[01:52:16.320 --> 01:52:17.520]   Hey, did you all see--
[01:52:17.520 --> 01:52:18.640]   What's the deal?
[01:52:18.640 --> 01:52:21.520]   Because I read some rumors about Google doing a foldable phone.
[01:52:21.520 --> 01:52:23.040]   And you know how I'm obsessed with both of them.
[01:52:23.040 --> 01:52:25.440]   Yes, we've been reading these rumors for a long time.
[01:52:25.440 --> 01:52:26.080]   Yep.
[01:52:26.080 --> 01:52:26.560]   I agree.
[01:52:26.560 --> 01:52:27.760]   But is there anything new?
[01:52:27.760 --> 01:52:27.760]   No.
[01:52:27.760 --> 01:52:29.120]   I thought I saw something new.
[01:52:29.120 --> 01:52:29.440]   So--
[01:52:29.440 --> 01:52:30.960]   Yeah, there was a rumor that they might
[01:52:30.960 --> 01:52:34.720]   announce it this year along with a 6A, maybe at the same time.
[01:52:34.720 --> 01:52:37.200]   But that's just rumor.
[01:52:37.200 --> 01:52:39.760]   I think somebody saw something in an FCC database.
[01:52:39.760 --> 01:52:41.840]   I think that's what kind of led to--
[01:52:41.840 --> 01:52:42.320]   Prominent.
[01:52:42.320 --> 01:52:43.120]   All right.
[01:52:43.120 --> 01:52:43.360]   Yeah.
[01:52:43.360 --> 01:52:46.720]   I think I mentioned it in the rundown.
[01:52:46.720 --> 01:52:49.520]   I produced a couple of weeks ago and we skipped it.
[01:52:49.520 --> 01:52:50.240]   Oh.
[01:52:50.240 --> 01:52:51.200]   Because it was a--
[01:52:51.200 --> 01:52:54.400]   Because I wasn't here to be obsessed about the phones.
[01:52:54.400 --> 01:52:54.880]   See.
[01:52:54.880 --> 01:52:57.200]   There's always so much good stuff in the rundown.
[01:52:57.200 --> 01:52:58.080]   I don't get to it all.
[01:52:58.080 --> 01:52:59.360]   I try it, but I don't.
[01:52:59.360 --> 01:52:59.920]   North Korean.
[01:52:59.920 --> 01:53:01.520]   There's so much good stuff in the rundown.
[01:53:01.520 --> 01:53:01.760]   Yeah.
[01:53:01.760 --> 01:53:03.600]   And we always talk about Facebook.
[01:53:03.600 --> 01:53:04.400]   I haven't yet.
[01:53:04.400 --> 01:53:06.000]   You know what we haven't talked about Facebook in a week.
[01:53:06.000 --> 01:53:06.160]   I know.
[01:53:06.160 --> 01:53:06.960]   Today's not.
[01:53:06.960 --> 01:53:07.520]   It's not.
[01:53:07.520 --> 01:53:08.400]   Have you noticed that?
[01:53:08.400 --> 01:53:10.560]   That I am being influenced by somebody.
[01:53:10.560 --> 01:53:12.160]   No, no, that's not true.
[01:53:12.160 --> 01:53:12.800]   That's not fair.
[01:53:12.800 --> 01:53:13.360]   It's not true.
[01:53:13.360 --> 01:53:15.120]   There's just not much story there.
[01:53:15.120 --> 01:53:16.400]   It's kind of boring.
[01:53:16.400 --> 01:53:17.120]   North Korean--
[01:53:17.120 --> 01:53:18.000]   We do have--
[01:53:18.000 --> 01:53:20.240]   Taken down by cyber attackers.
[01:53:20.240 --> 01:53:23.840]   D-dossed right after their missile launch.
[01:53:23.840 --> 01:53:26.960]   All of a sudden, the internet went down for six hours,
[01:53:26.960 --> 01:53:27.760]   Wednesday morning.
[01:53:27.760 --> 01:53:30.560]   It came a day after North Korea
[01:53:30.560 --> 01:53:32.160]   conducted its fifth missile test.
[01:53:32.160 --> 01:53:34.720]   Not sure if it's related.
[01:53:34.720 --> 01:53:39.840]   Hackers also took the Bella Roussen railway system offline
[01:53:39.840 --> 01:53:43.120]   in an attempt to slow down the Russian invasion of Ukraine.
[01:53:43.120 --> 01:53:43.680]   And they said it.
[01:53:43.680 --> 01:53:44.240]   They admitted it.
[01:53:44.240 --> 01:53:44.480]   Yep.
[01:53:44.480 --> 01:53:49.120]   We infected them with ransomware to slow them down.
[01:53:49.120 --> 01:53:52.560]   This is a new kind of activist hacking.
[01:53:52.560 --> 01:53:53.440]   May I call this?
[01:53:53.440 --> 01:53:55.200]   This is this is armies without countries.
[01:53:55.200 --> 01:53:55.520]   Yeah.
[01:53:55.520 --> 01:53:58.480]   Without gunpowder.
[01:53:58.480 --> 01:53:58.720]   Yeah.
[01:53:58.720 --> 01:54:01.280]   Very interesting.
[01:54:01.280 --> 01:54:02.560]   And good guys and bad guys,
[01:54:02.560 --> 01:54:05.360]   both don't we know from our election hacking and all that?
[01:54:05.360 --> 01:54:10.080]   Seoul-based NK Pro, a new site that monitors North Korea,
[01:54:10.080 --> 01:54:13.360]   reported that log files and network records showed websites
[01:54:13.360 --> 01:54:16.400]   and North Korean web domains largely unreachable,
[01:54:16.400 --> 01:54:18.240]   because North Korea's DNS system
[01:54:18.240 --> 01:54:22.320]   stopped communicating the routes the packets should take.
[01:54:22.320 --> 01:54:25.600]   A DDoS attack.
[01:54:25.600 --> 01:54:28.080]   That us?
[01:54:28.080 --> 01:54:29.520]   Yeah.
[01:54:29.520 --> 01:54:30.720]   I mean, I'm not surprised.
[01:54:30.720 --> 01:54:32.080]   I've thought this for years.
[01:54:32.080 --> 01:54:35.680]   I think I gave an interview like 15 years ago saying hackers
[01:54:35.680 --> 01:54:37.600]   are going to be the next, you know,
[01:54:37.600 --> 01:54:39.200]   frontline of cyber warfare,
[01:54:39.200 --> 01:54:40.560]   because they've got the skills
[01:54:40.560 --> 01:54:44.240]   and either nation states will use them as they are
[01:54:44.240 --> 01:54:46.320]   or they will act on their own.
[01:54:46.320 --> 01:54:48.480]   That's groups like Anonymous have done.
[01:54:48.480 --> 01:54:49.040]   I think it's very-
[01:54:49.040 --> 01:54:52.000]   It clearly did not read my book pick from several episodes ago.
[01:54:52.000 --> 01:54:53.360]   Annalie, No Wits?
[01:54:53.360 --> 01:54:55.040]   This is how they know autonomous?
[01:54:55.040 --> 01:54:58.560]   No, this is and this is how they tell me the world ends by Nicole Perla.
[01:54:58.560 --> 01:54:58.560]   Oh.
[01:54:58.560 --> 01:54:59.920]   That was literally what it was.
[01:54:59.920 --> 01:55:00.640]   That was a good book.
[01:55:00.640 --> 01:55:01.840]   Yes, yes, yes, yeah.
[01:55:01.840 --> 01:55:04.240]   It's like, oh no, no, I did read it.
[01:55:04.240 --> 01:55:04.960]   It stays-
[01:55:04.960 --> 01:55:07.760]   I'm just parroting her point of view.
[01:55:07.760 --> 01:55:14.560]   Manufacturers have less than five days supply of some computer chips
[01:55:14.560 --> 01:55:16.800]   according to the US Department of Commerce.
[01:55:16.800 --> 01:55:20.880]   This just in time thing is not working out so well.
[01:55:20.880 --> 01:55:22.400]   Wait for thin inventories,
[01:55:22.400 --> 01:55:24.880]   the factories vulnerable to shutdowns
[01:55:24.880 --> 01:55:29.440]   if shift deliveries are interrupted by anything like weather or COVID-19.
[01:55:29.440 --> 01:55:34.320]   And I've also seen that production seems to be back up on chips.
[01:55:34.320 --> 01:55:35.360]   Is that what you're hearing?
[01:55:35.360 --> 01:55:38.480]   What's happening is they're building fabs like crazy.
[01:55:38.480 --> 01:55:40.560]   Intel's got a big one, $20 billion fab.
[01:55:40.560 --> 01:55:41.840]   They're building in Ohio,
[01:55:41.840 --> 01:55:43.680]   TSMC is building in Arizona.
[01:55:43.680 --> 01:55:46.400]   But the problem is it takes a while,
[01:55:46.400 --> 01:55:48.320]   like a year or two to build a fab.
[01:55:48.320 --> 01:55:50.400]   So according to the Commerce Department-
[01:55:50.400 --> 01:55:51.440]   another problem.
[01:55:51.440 --> 01:55:51.760]   Yes.
[01:55:51.760 --> 01:55:57.440]   So the second problem is not only do they take a long time to come online,
[01:55:57.440 --> 01:56:01.120]   but a lot of the shortages aren't on the most advanced chips,
[01:56:01.120 --> 01:56:02.560]   which is what these fabs are going to make.
[01:56:02.560 --> 01:56:03.840]   The legacy knows, right?
[01:56:03.840 --> 01:56:05.920]   Yeah, they're not even saying that problem.
[01:56:05.920 --> 01:56:10.000]   So if you all care about this, I'm just going to pitch.
[01:56:10.000 --> 01:56:10.640]   I'm sorry.
[01:56:10.640 --> 01:56:10.960]   Sorry.
[01:56:10.960 --> 01:56:13.120]   For our show, we talked for like 15 minutes about it.
[01:56:13.120 --> 01:56:14.080]   Oh good.
[01:56:14.080 --> 01:56:15.120]   Oh good.
[01:56:15.120 --> 01:56:19.680]   No, that's a great pitch for the IoT show with Stacy
[01:56:19.680 --> 01:56:20.640]   and Kevin.
[01:56:20.640 --> 01:56:25.600]   Survey respondents to the Washington Post said they didn't see shortage
[01:56:25.600 --> 01:56:26.960]   just going away in the next six months.
[01:56:26.960 --> 01:56:30.240]   Some say it could last into well into 2023.
[01:56:30.240 --> 01:56:33.600]   Yeah.
[01:56:33.600 --> 01:56:36.640]   And so the Commerce Department is also they.
[01:56:36.640 --> 01:56:38.880]   So some of this is a little self-serving.
[01:56:38.880 --> 01:56:40.000]   I know.
[01:56:40.000 --> 01:56:40.800]   Surprise.
[01:56:40.800 --> 01:56:42.000]   Politics self-serving.
[01:56:42.000 --> 01:56:46.720]   There's the Biden administration is pushing the CHIPS Act,
[01:56:46.720 --> 01:56:51.280]   which is a $52 billion allocation for funds for CHIP
[01:56:51.280 --> 01:56:53.200]   development to happen here in the US.
[01:56:53.200 --> 01:56:56.480]   And so that's part of what they're doing.
[01:56:56.480 --> 01:57:00.320]   Some of this like some chip inventories,
[01:57:00.320 --> 01:57:04.160]   like historically they have kept pretty limited inventories
[01:57:04.160 --> 01:57:05.840]   because the supply chains haven't been messed up.
[01:57:05.840 --> 01:57:07.760]   So in some ways, this is not a story.
[01:57:07.760 --> 01:57:14.320]   And the solutions that we're proposing aren't quite the best solutions.
[01:57:15.520 --> 01:57:18.240]   Just simply because, like I said, we don't need new fab.
[01:57:18.240 --> 01:57:22.240]   I mean, yes, we do need new fabs, but what we really need are more
[01:57:22.240 --> 01:57:23.840]   transparent supply chains.
[01:57:23.840 --> 01:57:28.640]   And what we're undergoing is a shift in the way the supply chain for CHIPS
[01:57:28.640 --> 01:57:32.720]   is going to be managed, I guess, is the way to think about it.
[01:57:32.720 --> 01:57:35.280]   So instead of just like going to the store and being like,
[01:57:35.280 --> 01:57:37.920]   oh, I want all these chips.
[01:57:37.920 --> 01:57:40.400]   Now we're having to like order our chips in advance.
[01:57:40.400 --> 01:57:41.120]   Does that make sense?
[01:57:41.120 --> 01:57:42.960]   Yeah.
[01:57:43.920 --> 01:57:45.200]   I was like, did y'all go away?
[01:57:45.200 --> 01:57:45.680]   Oh my God.
[01:57:45.680 --> 01:57:50.800]   This happens all the time at parties when I talk about semiconductors.
[01:57:50.800 --> 01:57:51.280]   What did you say?
[01:57:51.280 --> 01:57:55.280]   People just leave.
[01:57:55.280 --> 01:57:58.240]   Oh, do you need a drink?
[01:57:58.240 --> 01:57:58.800]   Yeah, yeah, yeah.
[01:57:58.800 --> 01:57:59.120]   Thanks.
[01:57:59.120 --> 01:57:59.520]   Oh, sorry.
[01:57:59.520 --> 01:58:01.040]   See, I'll be right back.
[01:58:01.040 --> 01:58:02.800]   No, no, no, no, no.
[01:58:02.800 --> 01:58:03.440]   Keep your spot.
[01:58:03.440 --> 01:58:04.080]   Keep your space.
[01:58:04.080 --> 01:58:05.920]   Don't don't change.
[01:58:05.920 --> 01:58:11.520]   I should do a change log.
[01:58:12.720 --> 01:58:14.320]   Can I do a tease?
[01:58:14.320 --> 01:58:15.200]   Can I do a tease?
[01:58:15.200 --> 01:58:16.880]   A tease before the change log.
[01:58:16.880 --> 01:58:17.600]   Yeah.
[01:58:17.600 --> 01:58:20.000]   When we come back from the change log.
[01:58:20.000 --> 01:58:20.320]   Yes.
[01:58:20.320 --> 01:58:22.480]   And probably a commercial too.
[01:58:22.480 --> 01:58:22.880]   Yes.
[01:58:22.880 --> 01:58:23.840]   Wait this long.
[01:58:23.840 --> 01:58:24.320]   Yes.
[01:58:24.320 --> 01:58:27.360]   We're going to find a musical that Ant actually likes.
[01:58:27.360 --> 01:58:27.440]   No.
[01:58:27.440 --> 01:58:34.800]   And now it's time for the console.
[01:58:34.800 --> 01:58:35.200]   It's time to talk.
[01:58:35.200 --> 01:58:36.560]   Change log.
[01:58:41.600 --> 01:58:46.240]   You no longer have to say, hey, goog, to get the assistant to shut up.
[01:58:46.240 --> 01:58:47.600]   This is an old story.
[01:58:47.600 --> 01:58:48.240]   I don't know why.
[01:58:48.240 --> 01:58:49.120]   Yes, I don't know why.
[01:58:49.120 --> 01:58:49.840]   I just discovered this.
[01:58:49.840 --> 01:58:53.200]   Because I think that they did a, they just did a blog post say,
[01:58:53.200 --> 01:58:53.920]   I almost put it in.
[01:58:53.920 --> 01:58:55.040]   I said, no, this is old.
[01:58:55.040 --> 01:58:56.720]   They did a blog post that says, by the way,
[01:58:56.720 --> 01:58:58.000]   remind people, hey, you can do that.
[01:58:58.000 --> 01:58:59.840]   And then saying, stop forever.
[01:58:59.840 --> 01:59:00.560]   That's new.
[01:59:00.560 --> 01:59:01.440]   No, it's not.
[01:59:01.440 --> 01:59:03.760]   You can just know it's when the alarm's going off
[01:59:03.760 --> 01:59:05.120]   or a timer's going off.
[01:59:05.120 --> 01:59:06.240]   You just say, stop.
[01:59:06.240 --> 01:59:08.800]   So, okay.
[01:59:08.800 --> 01:59:10.640]   Ah, I see the difference.
[01:59:11.520 --> 01:59:15.280]   Google's smart home devices have had a version of this feature for years.
[01:59:15.280 --> 01:59:19.040]   Oh, no, no, this is the verge just not using this thing.
[01:59:19.040 --> 01:59:22.000]   Because no, you've always been able to say.
[01:59:22.000 --> 01:59:22.720]   You know what?
[01:59:22.720 --> 01:59:23.200]   Yeah.
[01:59:23.200 --> 01:59:25.280]   A lot of people don't know all the features.
[01:59:25.280 --> 01:59:29.120]   So there's always an audience for when you discover something to be like,
[01:59:29.120 --> 01:59:29.920]   so nice of you.
[01:59:29.920 --> 01:59:32.400]   So you say, it's true.
[01:59:32.400 --> 01:59:35.840]   That's the problem with a change log is sometimes,
[01:59:35.840 --> 01:59:37.120]   there's no change.
[01:59:37.120 --> 01:59:38.240]   There's no change.
[01:59:38.240 --> 01:59:39.520]   Pixel's big, add a glaze.
[01:59:39.520 --> 01:59:40.880]   It's finished.
[01:59:40.880 --> 01:59:42.320]   There's no new Google.
[01:59:42.320 --> 01:59:42.800]   It's over.
[01:59:42.800 --> 01:59:44.080]   It's not going to change anymore.
[01:59:44.080 --> 01:59:44.640]   It's over.
[01:59:44.640 --> 01:59:45.440]   Rename this.
[01:59:45.440 --> 01:59:47.760]   It is what it is.
[01:59:47.760 --> 01:59:48.080]   Okay.
[01:59:48.080 --> 01:59:49.920]   The stopped worked for timers.
[01:59:49.920 --> 01:59:54.160]   Scooter X says it is new because now it works for other things.
[01:59:54.160 --> 01:59:56.320]   Okay.
[01:59:56.320 --> 01:59:57.440]   It's still really important.
[01:59:57.440 --> 01:59:59.040]   It always worked for alarms.
[01:59:59.040 --> 02:00:01.680]   Not always, but it's been working for a while because I've been shouting it by.
[02:00:01.680 --> 02:00:03.760]   So like when I asked Google, like,
[02:00:03.760 --> 02:00:05.520]   tell me about Abraham Lincoln and people like,
[02:00:05.520 --> 02:00:07.840]   Abraham Lincoln was born and you're like,
[02:00:07.840 --> 02:00:08.240]   okay, stop.
[02:00:08.240 --> 02:00:09.680]   If you're bored, stop.
[02:00:09.680 --> 02:00:10.240]   Yeah.
[02:00:10.240 --> 02:00:13.520]   You can, for weather, okay, that's the biggest difference is it.
[02:00:13.520 --> 02:00:17.360]   Stop has been expanded to stop it in more situations.
[02:00:17.360 --> 02:00:19.200]   Okay.
[02:00:19.200 --> 02:00:22.400]   And they've been rolling this out on the Pixel 6 with quick phrases.
[02:00:22.400 --> 02:00:26.720]   Um, quick phrases let you do certain things without saying,
[02:00:26.720 --> 02:00:27.680]   hey, you know who.
[02:00:27.680 --> 02:00:31.600]   The feature that's rolled out,
[02:00:31.600 --> 02:00:33.520]   according to the Verge on Google's smart home devices,
[02:00:33.520 --> 02:00:37.920]   is in a universal stop making noise command for your smart speaker.
[02:00:37.920 --> 02:00:40.560]   You still have to say, hey, Google, if you want to stop a song,
[02:00:40.560 --> 02:00:43.360]   you can't just stop.
[02:00:43.360 --> 02:00:44.240]   Okay.
[02:00:44.240 --> 02:00:45.760]   All right.
[02:00:45.760 --> 02:00:46.960]   Would stop all of you.
[02:00:46.960 --> 02:00:47.520]   Maybe we're better.
[02:00:47.520 --> 02:00:48.080]   If you were.
[02:00:48.080 --> 02:00:50.000]   I like stop.
[02:00:50.000 --> 02:00:50.320]   What?
[02:00:50.320 --> 02:00:52.960]   You know, Lisa swears at it.
[02:00:52.960 --> 02:00:57.040]   In fact, she says, and she wasn't able to demonstrate it for me.
[02:00:57.040 --> 02:00:58.400]   So it might have been a one off,
[02:00:58.400 --> 02:01:00.720]   but she swore at the Google and the Google said,
[02:01:00.720 --> 02:01:04.000]   that's not nice and you shouldn't swear at me at which point.
[02:01:05.520 --> 02:01:09.120]   She swore even louder and it said, do you want to file a report?
[02:01:09.120 --> 02:01:11.600]   And she said, yes, I do.
[02:01:11.600 --> 02:01:14.960]   And she said, I want to tell you guys back at Google,
[02:01:14.960 --> 02:01:16.160]   it's a machine.
[02:01:16.160 --> 02:01:17.600]   It's okay to yell at it.
[02:01:17.600 --> 02:01:21.920]   You don't have to be polite.
[02:01:21.920 --> 02:01:22.320]   Hey, sir.
[02:01:22.320 --> 02:01:23.680]   No, you good day, sir.
[02:01:23.680 --> 02:01:24.720]   You agree with that, right?
[02:01:24.720 --> 02:01:27.520]   You don't have to be polite to a virtual assistant.
[02:01:27.520 --> 02:01:28.240]   It's just a.
[02:01:28.240 --> 02:01:29.520]   I am polite to like.
[02:01:29.520 --> 02:01:29.680]   Yeah, he has.
[02:01:29.680 --> 02:01:31.360]   He's coming after her.
[02:01:31.360 --> 02:01:32.560]   You're polite.
[02:01:32.560 --> 02:01:33.120]   No, no, I.
[02:01:33.120 --> 02:01:34.800]   Well, I.
[02:01:34.800 --> 02:01:36.320]   You want to get bad habits.
[02:01:36.320 --> 02:01:37.200]   Yeah, yeah, yeah.
[02:01:37.200 --> 02:01:38.160]   Because practice is.
[02:01:38.160 --> 02:01:39.120]   Yeah, I'm polite to normal people.
[02:01:39.120 --> 02:01:39.760]   Yeah.
[02:01:39.760 --> 02:01:40.400]   Yeah.
[02:01:40.400 --> 02:01:41.760]   So people like to your robot.
[02:01:41.760 --> 02:01:46.160]   Well, basically, if I'm talking, and I've yelled at Madam A,
[02:01:46.160 --> 02:01:49.760]   I, and sometimes I yell at Google when it goes on and on,
[02:01:49.760 --> 02:01:51.440]   so stop is very helpful.
[02:01:51.440 --> 02:01:54.480]   But yeah, in general, like if I'm defaulting.
[02:01:54.480 --> 02:01:55.600]   Yeah, just, yeah.
[02:01:55.600 --> 02:01:56.880]   I'm like, thanks.
[02:01:56.880 --> 02:01:57.760]   I'm talking to somebody.
[02:01:57.760 --> 02:02:00.000]   Yeah, I'm a nice person.
[02:02:00.000 --> 02:02:01.280]   I'm nice to my little bots.
[02:02:03.440 --> 02:02:04.560]   But if you know what, if you say.
[02:02:04.560 --> 02:02:05.920]   I'm not going to donate my kidney or anything.
[02:02:05.920 --> 02:02:08.320]   No, and if you say if you, it's not the end of the world either.
[02:02:08.320 --> 02:02:09.040]   So don't.
[02:02:09.040 --> 02:02:10.320]   No, no, when that means.
[02:02:10.320 --> 02:02:11.200]   Time Google, right.
[02:02:11.200 --> 02:02:14.880]   Big pixels, big, at a glance upgrade.
[02:02:14.880 --> 02:02:16.800]   Are we still talking about the Pixel six?
[02:02:16.800 --> 02:02:18.320]   Good Lord.
[02:02:18.320 --> 02:02:23.120]   Starts rolling out with more Google clock and fitness integrations.
[02:02:23.120 --> 02:02:24.560]   Okay.
[02:02:24.560 --> 02:02:25.760]   Which fitness integrations?
[02:02:25.760 --> 02:02:30.720]   Your assistant shows you what you need right when you need it on your home screen and lock screen.
[02:02:31.360 --> 02:02:34.160]   Bedtime, your upcoming bedtime.
[02:02:34.160 --> 02:02:39.600]   Timer and stopwatch info from clock activity info from your fitness app.
[02:02:39.600 --> 02:02:46.000]   So go into your at a glance settings and you'll see at least three new settings.
[02:02:46.000 --> 02:02:53.280]   Google could, should be adding several more capabilities to at a glance.
[02:02:53.280 --> 02:02:56.560]   Add a store shopping list in Google Pay Rewards cards.
[02:02:56.560 --> 02:03:01.280]   Connected devices, connection, status and battery info for your Bluetooth devices.
[02:03:01.280 --> 02:03:04.320]   Apple does that nicely with their AirPods doorbells.
[02:03:04.320 --> 02:03:07.440]   So as you, oh, I will like that when I can see who's at the door.
[02:03:07.440 --> 02:03:09.600]   I think we talked about that last week.
[02:03:09.600 --> 02:03:10.640]   Flashlight.
[02:03:10.640 --> 02:03:11.600]   This is good.
[02:03:11.600 --> 02:03:14.640]   Do you really need a reminder when your flashlight is on?
[02:03:14.640 --> 02:03:17.840]   I guess you actually do turn it off.
[02:03:17.840 --> 02:03:18.880]   Yes, actually.
[02:03:18.880 --> 02:03:24.160]   And a safety check countdown from the personal safety app.
[02:03:24.160 --> 02:03:27.440]   So get ready.
[02:03:27.440 --> 02:03:27.920]   Look at this.
[02:03:27.920 --> 02:03:33.520]   Here's a QR code with your flight information on the at a glance.
[02:03:33.520 --> 02:03:34.320]   That's kind of neat.
[02:03:34.320 --> 02:03:38.320]   So nine to five Google says more coming.
[02:03:38.320 --> 02:03:41.760]   Check your at a glance settings.
[02:03:41.760 --> 02:03:43.520]   Google chat.
[02:03:43.520 --> 02:03:49.120]   Yes, there is such a thing adds rich text formatting on the web rolling out now.
[02:03:49.120 --> 02:03:52.160]   Who uses Google chat?
[02:03:52.160 --> 02:03:53.200]   Oh, just somebody.
[02:03:53.200 --> 02:03:54.480]   I use Google chat.
[02:03:54.480 --> 02:03:55.120]   Do you?
[02:03:55.120 --> 02:03:55.760]   I'm always like that.
[02:03:55.760 --> 02:03:56.800]   Just take the G people, right?
[02:03:56.800 --> 02:04:02.000]   You can make a bold bold word or an italic word or underline it or that kind of thing.
[02:04:02.000 --> 02:04:03.440]   That's cool.
[02:04:03.440 --> 02:04:05.520]   That'll be everyone.
[02:04:05.520 --> 02:04:11.440]   I'll get it in the next 15 days turned on by default with no opt out settings for admins.
[02:04:11.440 --> 02:04:13.040]   So stick it.
[02:04:13.040 --> 02:04:20.240]   YouTube music will soon work for supervised kid accounts enrolled with the family link.
[02:04:20.240 --> 02:04:24.800]   Do you use family links, Stacy?
[02:04:24.800 --> 02:04:25.520]   No, probably not.
[02:04:26.400 --> 02:04:28.480]   Kids under 13 YouTube music.
[02:04:28.480 --> 02:04:29.840]   Yeah.
[02:04:29.840 --> 02:04:31.040]   We don't use YouTube music.
[02:04:31.040 --> 02:04:34.240]   So this was a feature that Google Play music had.
[02:04:34.240 --> 02:04:36.160]   If you were under 13, you could have your own library.
[02:04:36.160 --> 02:04:38.560]   Now YouTube music will be adding this.
[02:04:38.560 --> 02:04:41.120]   No, we just have to have that.
[02:04:41.120 --> 02:04:41.600]   Not bad.
[02:04:41.600 --> 02:04:41.840]   Yeah.
[02:04:41.840 --> 02:04:45.680]   And that's the Google change line.
[02:04:45.680 --> 02:04:54.320]   Anything else before we get to our picks of the week we're getting on.
[02:04:54.320 --> 02:04:55.760]   Did you see Mark Cuban?
[02:04:55.760 --> 02:04:57.840]   I don't know how long he's going to stay with this.
[02:04:57.840 --> 02:04:59.680]   Because he doesn't tend to stay with.
[02:04:59.680 --> 02:05:01.600]   Is it a thing?
[02:05:01.600 --> 02:05:04.560]   It's yeah, he started his own online pharmacy.
[02:05:04.560 --> 02:05:09.440]   Pharmacy thing where he's just he's charging basically for generic drugs, a penny or two
[02:05:09.440 --> 02:05:10.720]   above the cost.
[02:05:10.720 --> 02:05:14.160]   I think he's going to trouble with the name the cost plus drug company.
[02:05:14.160 --> 02:05:16.560]   There is a business called cost plus, but okay.
[02:05:16.560 --> 02:05:18.640]   That's the world market.
[02:05:18.640 --> 02:05:21.680]   Well, yeah, but it is a market.
[02:05:22.240 --> 02:05:23.920]   Anyway, I looked it up.
[02:05:23.920 --> 02:05:27.760]   There's no insulin, which would be a real benefit to the world.
[02:05:27.760 --> 02:05:30.880]   Yeah, because the price differentials aren't that great.
[02:05:30.880 --> 02:05:31.920]   And most of what I looked at.
[02:05:31.920 --> 02:05:33.280]   Okay.
[02:05:33.280 --> 02:05:33.840]   Here's one.
[02:05:33.840 --> 02:05:39.200]   It's 15% above generics plus the price of shipping.
[02:05:39.200 --> 02:05:43.520]   And it may not be that the price is much greater in many cases.
[02:05:43.520 --> 02:05:50.480]   Because like a lot of like Walgreens and many of the supermarket chains as part of their like pharmacy
[02:05:50.480 --> 02:05:54.160]   efforts, they've been doing what he's doing basically.
[02:05:54.160 --> 02:05:55.600]   They've just been doing it for their first.
[02:05:55.600 --> 02:05:56.400]   Yes, right.
[02:05:56.400 --> 02:05:56.720]   Okay.
[02:05:56.720 --> 02:05:57.360]   Go on.
[02:05:57.360 --> 02:05:57.840]   Right.
[02:05:57.840 --> 02:05:59.680]   So yeah, so I don't know.
[02:05:59.680 --> 02:06:04.400]   I mean, he's making it like say not like a business.
[02:06:04.400 --> 02:06:06.800]   Like I'm going to he says, I'm not going to make any money at this.
[02:06:06.800 --> 02:06:09.200]   I'm just a good person.
[02:06:09.200 --> 02:06:10.160]   I'm saving the world.
[02:06:10.160 --> 02:06:10.400]   Yeah.
[02:06:10.400 --> 02:06:12.000]   You know what?
[02:06:12.000 --> 02:06:13.040]   More power to him.
[02:06:13.040 --> 02:06:15.360]   It drives people to think about this issue.
[02:06:16.400 --> 02:06:22.960]   There are a couple drugs that are actually like one of my mom's drugs is actually like $50 cheaper
[02:06:22.960 --> 02:06:26.000]   per month, which is really I mean, that's great for her.
[02:06:26.000 --> 02:06:27.280]   That's like going out to dinner.
[02:06:27.280 --> 02:06:27.520]   So.
[02:06:27.520 --> 02:06:34.880]   So I understand if you don't have insurance that covers this stuff, then it can be very expensive.
[02:06:34.880 --> 02:06:38.960]   Does your mom not have insurance or is it cheaper for her to buy it?
[02:06:38.960 --> 02:06:40.880]   Sometimes it's expensive with insurance.
[02:06:40.880 --> 02:06:43.840]   Sometimes it's cheaper to buy it out.
[02:06:43.840 --> 02:06:44.560]   Okay.
[02:06:44.560 --> 02:06:51.200]   And it's also just easier in some case, like sometimes insurance doesn't pay for certain drugs.
[02:06:51.200 --> 02:06:51.440]   Right.
[02:06:51.440 --> 02:06:51.840]   Yeah.
[02:06:51.840 --> 02:06:52.400]   Right.
[02:06:52.400 --> 02:06:52.720]   Okay.
[02:06:52.720 --> 02:06:52.880]   Sure.
[02:06:52.880 --> 02:06:54.000]   It's going to be interesting.
[02:06:54.000 --> 02:06:56.080]   Interesting.
[02:06:56.080 --> 02:06:58.800]   You want to laugh at Facebook?
[02:06:58.800 --> 02:06:59.920]   You want to laugh at Facebook?
[02:06:59.920 --> 02:07:02.240]   Uh, you want to do a face book show?
[02:07:02.240 --> 02:07:03.680]   Oh, okay.
[02:07:03.680 --> 02:07:04.720]   No, the meta the meta.
[02:07:04.720 --> 02:07:05.200]   No, Facebook.
[02:07:05.200 --> 02:07:06.160]   The meta ad is for horizon.
[02:07:06.160 --> 02:07:08.160]   The meta ad is for funny.
[02:07:08.160 --> 02:07:08.560]   I thought.
[02:07:08.560 --> 02:07:10.400]   For horizon?
[02:07:10.400 --> 02:07:11.920]   Horizon.
[02:07:12.720 --> 02:07:15.120]   Not meta has built an AI supercomputer or.
[02:07:15.120 --> 02:07:16.960]   It's killing team.
[02:07:16.960 --> 02:07:18.640]   Line 48.
[02:07:18.640 --> 02:07:21.600]   You know, we do have a Facebook section.
[02:07:21.600 --> 02:07:22.320]   Here it is.
[02:07:22.320 --> 02:07:25.520]   The meta ad for horizon.
[02:07:25.520 --> 02:07:26.560]   You started the quick takes.
[02:07:26.560 --> 02:07:28.400]   It's very confusing to know where to put this.
[02:07:28.400 --> 02:07:29.360]   What is horizon?
[02:07:29.360 --> 02:07:29.920]   So we know.
[02:07:29.920 --> 02:07:30.400]   Oh, I've.
[02:07:30.400 --> 02:07:30.960]   That's their.
[02:07:30.960 --> 02:07:34.000]   We saw this ad when this came in ages ago.
[02:07:34.000 --> 02:07:34.560]   We did.
[02:07:34.560 --> 02:07:34.880]   Yeah.
[02:07:34.880 --> 02:07:36.400]   No legs.
[02:07:36.400 --> 02:07:37.520]   No legs.
[02:07:37.520 --> 02:07:38.640]   What is it with the legs?
[02:07:38.640 --> 02:07:39.360]   No legs.
[02:07:39.360 --> 02:07:43.200]   We I think the more modern versions of this have legs, don't they?
[02:07:43.200 --> 02:07:46.000]   The one Mark showed us had legs.
[02:07:46.000 --> 02:07:48.800]   I don't think this whole idea has legs.
[02:07:48.800 --> 02:07:54.080]   This isn't Christian Bell.
[02:07:54.080 --> 02:07:55.200]   Is it old?
[02:07:55.200 --> 02:07:56.480]   It's a fake Kristen Bell.
[02:07:56.480 --> 02:07:57.360]   Is it Kristen Bell?
[02:07:57.360 --> 02:07:59.120]   Yeah, this is offensive on so many levels.
[02:07:59.120 --> 02:08:00.640]   Let's go to the end of the show.
[02:08:00.640 --> 02:08:00.960]   Okay.
[02:08:00.960 --> 02:08:02.640]   Stacy's offended.
[02:08:02.640 --> 02:08:05.600]   If we're not going to talk about Facebook's super computing,
[02:08:05.600 --> 02:08:06.800]   I'm not here for it.
[02:08:06.800 --> 02:08:08.160]   Well, you may talk about that if you like.
[02:08:08.160 --> 02:08:10.000]   Why are they building a super computer?
[02:08:10.000 --> 02:08:14.080]   They're building the world's fastest computer for AI.
[02:08:14.080 --> 02:08:15.280]   Why are they doing that?
[02:08:15.280 --> 02:08:15.520]   Why?
[02:08:15.520 --> 02:08:17.920]   You're fault Stacy.
[02:08:17.920 --> 02:08:18.800]   Why Stacy?
[02:08:18.800 --> 02:08:19.600]   Why?
[02:08:19.600 --> 02:08:21.520]   We'll be right back with more in just a little bit.
[02:08:21.520 --> 02:08:24.560]   I showed you they brought to you by compiler.
[02:08:24.560 --> 02:08:30.720]   It's a podcast, original podcast from Red Hat discusses tech topics, big, small, and
[02:08:30.720 --> 02:08:34.240]   yeah, I'm in it a little bit weird, a little bit strange.
[02:08:34.240 --> 02:08:41.280]   You may be familiar with Red Hat's other podcast, Command Line Heroes and other responses,
[02:08:41.280 --> 02:08:42.080]   which we love.
[02:08:42.080 --> 02:08:50.240]   This one hosted by Angela Andrews and Brent Simino is, I think, a brilliant idea for a podcast.
[02:08:50.240 --> 02:08:53.280]   And I say that because I'm very jealous that they thought of it first.
[02:08:53.280 --> 02:08:58.960]   What they've done is they've actually started to look at the things that Red Hatters are asking
[02:08:58.960 --> 02:09:05.360]   in their messaging platform, whatever it is that they use, probably Slack or something like that.
[02:09:05.360 --> 02:09:07.920]   It's questions that they go, "That's interesting.
[02:09:07.920 --> 02:09:09.040]   What do you think?
[02:09:09.040 --> 02:09:15.680]   Technology can be big, bold, bizarre, complicated compiler, unravels, industry topics, trends,
[02:09:15.680 --> 02:09:20.560]   and the things you've always wanted to know about tech through interviews with people who know it best.
[02:09:20.560 --> 02:09:28.160]   On the show, you'll hear a chorus of perspectives from diverse communities beyond the code.
[02:09:29.040 --> 02:09:33.040]   Red Hatters, tackling big questions like, "What is technical debt?
[02:09:33.040 --> 02:09:36.480]   What are tech hiring managers actually looking for?"
[02:09:36.480 --> 02:09:39.280]   Which I thought was quite interesting.
[02:09:39.280 --> 02:09:42.960]   Episode two, "What can video games teach us about edge computing?"
[02:09:42.960 --> 02:09:47.760]   The internet is a patchwork of international agreements and varying infrastructure, but
[02:09:47.760 --> 02:09:50.800]   there's something coming to change the way we connect.
[02:09:50.800 --> 02:09:56.000]   In this episode of Compiler hosts explore what edge computing could mean for people who enjoy
[02:09:56.000 --> 02:10:00.080]   video games and what this form of entertainment could teach us about the technology.
[02:10:00.080 --> 02:10:02.800]   Episode nine, "Our tech hubs changing."
[02:10:02.800 --> 02:10:09.120]   Usually you go to Silicon Valley or San Francisco if you want to get up to career in tech,
[02:10:09.120 --> 02:10:11.440]   but that's definitely starting to change.
[02:10:11.440 --> 02:10:15.280]   The hosts of Compiler speak to a few of the changemakers who are thinking outside
[02:10:15.280 --> 02:10:19.280]   of the physical and social dimensions we've come to associate with innovation.
[02:10:19.280 --> 02:10:21.920]   You may not need legs in the future, right?
[02:10:23.440 --> 02:10:29.520]   To learn more about Compiler, just go to reed.ht/twit.
[02:10:29.520 --> 02:10:34.800]   That's their URL shortener, reed.ht/twit.
[02:10:34.800 --> 02:10:36.560]   It's really well produced.
[02:10:36.560 --> 02:10:39.760]   I love it that it has a variety of voices talking about the topic,
[02:10:39.760 --> 02:10:41.040]   not all in agreement.
[02:10:41.040 --> 02:10:43.520]   It's a very nice survey.
[02:10:43.520 --> 02:10:45.360]   New episodes just came out.
[02:10:45.360 --> 02:10:46.640]   You can download them at any time.
[02:10:46.640 --> 02:10:50.800]   So if you've heard the early episodes as I have, maybe it's time to go back and listen to some of the newer ones.
[02:10:51.520 --> 02:10:55.120]   Listen to Compiler on Apple Podcasts or anywhere you listen to your podcast.
[02:10:55.120 --> 02:11:00.880]   I will put a link on the show's notes page there so you can click that as well at twit.tv.
[02:11:00.880 --> 02:11:02.400]   We thank Compiler for their support.
[02:11:02.400 --> 02:11:02.880]   It's a nice one.
[02:11:02.880 --> 02:11:05.600]   One podcast can help another podcast along.
[02:11:05.600 --> 02:11:10.240]   Learn more at reed.ht/twit.
[02:11:10.240 --> 02:11:14.480]   You can listen there as well and we'll put a link to the latest episode up on the show notes.
[02:11:14.480 --> 02:11:15.280]   Thank you, Red Hat.
[02:11:15.280 --> 02:11:16.240]   Thank you, Compiler.
[02:11:16.240 --> 02:11:20.800]   Welcome to the family of podcasters.
[02:11:21.440 --> 02:11:28.400]   Oh, here's the picture of Elizabeth from Knoxville holding an onion
[02:11:28.400 --> 02:11:34.720]   in her cloth while she was claiming to have been maced.
[02:11:34.720 --> 02:11:36.400]   I was amazed.
[02:11:36.400 --> 02:11:37.440]   I was amazed.
[02:11:37.440 --> 02:11:39.040]   Yeah.
[02:11:39.040 --> 02:11:41.760]   Can we just mention the keyboard scarf, which?
[02:11:41.760 --> 02:11:43.360]   I like the scarf, Elizabeth.
[02:11:43.360 --> 02:11:44.080]   Nice choice.
[02:11:44.080 --> 02:11:46.880]   She's kind of funny.
[02:11:46.880 --> 02:11:49.360]   She looks at the onion.
[02:11:50.160 --> 02:11:52.720]   Dabs her-- by the way, you can see it in the towel.
[02:11:52.720 --> 02:11:54.640]   Dabs her eye with it.
[02:11:54.640 --> 02:12:02.080]   That says I was maced with onions.
[02:12:02.080 --> 02:12:07.920]   And you know what?
[02:12:07.920 --> 02:12:09.360]   I'm insecure for being maced.
[02:12:09.360 --> 02:12:13.920]   To be honest, even if she was maced while storming the Capitol,
[02:12:13.920 --> 02:12:15.440]   I don't think that deserves it.
[02:12:15.440 --> 02:12:17.120]   I don't think that's such a bad thing.
[02:12:18.000 --> 02:12:18.960]   Because of a shot.
[02:12:18.960 --> 02:12:20.640]   I mean, the law.
[02:12:20.640 --> 02:12:22.800]   You're breaking into the federal building.
[02:12:22.800 --> 02:12:25.440]   If you were a black man, you'd be shot.
[02:12:25.440 --> 02:12:25.920]   Oh, yeah.
[02:12:25.920 --> 02:12:26.640]   You'd be gone.
[02:12:26.640 --> 02:12:27.200]   Long gone.
[02:12:27.200 --> 02:12:29.600]   No onions involved.
[02:12:29.600 --> 02:12:29.920]   All right.
[02:12:29.920 --> 02:12:33.760]   Let us start with Stacy's thing.
[02:12:33.760 --> 02:12:34.800]   You want to talk about your range?
[02:12:34.800 --> 02:12:37.200]   Because you did-- actually, I'm going to give you a plug.
[02:12:37.200 --> 02:12:40.160]   Because you did actually do a good article about your brand new range
[02:12:40.160 --> 02:12:42.480]   that you talked about a couple of weeks ago on the show.
[02:12:42.480 --> 02:12:43.520]   Actually, maybe it was last week.
[02:12:43.520 --> 02:12:44.560]   It was your pick of the week.
[02:12:44.560 --> 02:12:45.040]   Last week.
[02:12:45.040 --> 02:12:46.080]   Your smart range.
[02:12:46.080 --> 02:12:47.120]   I move quickly.
[02:12:47.120 --> 02:12:47.840]   Yeah, you do.
[02:12:47.840 --> 02:12:52.000]   I'm really jealous because I really want an induction cooktop.
[02:12:52.000 --> 02:12:53.040]   So--
[02:12:53.040 --> 02:12:53.840]   Ooh, it is so good.
[02:12:53.840 --> 02:12:57.040]   But this is actually about the pan you use with your new range.
[02:12:57.040 --> 02:12:57.360]   Yeah.
[02:12:57.360 --> 02:13:00.240]   So today's thing of the week--
[02:13:00.240 --> 02:13:02.240]   She can't lift it because she's got a dead arm.
[02:13:02.240 --> 02:13:06.800]   I'm only using my left hand and it's a heavy pot.
[02:13:06.800 --> 02:13:07.920]   Oh, that is the big one.
[02:13:07.920 --> 02:13:08.560]   Oh, that.
[02:13:08.560 --> 02:13:09.520]   I have the whole set.
[02:13:09.520 --> 02:13:11.440]   You're strong, Stacy.
[02:13:11.440 --> 02:13:12.080]   Look at that.
[02:13:12.080 --> 02:13:12.400]   Woo.
[02:13:12.400 --> 02:13:13.760]   All right.
[02:13:13.760 --> 02:13:14.240]   All right.
[02:13:14.240 --> 02:13:16.320]   It's lighter without the lid, I got to tell you.
[02:13:17.120 --> 02:13:18.080]   The lid is--
[02:13:18.080 --> 02:13:19.040]   Well, it's just a heavy--
[02:13:19.040 --> 02:13:20.240]   So it's just a stainless pot.
[02:13:20.240 --> 02:13:20.720]   It's a nice pan.
[02:13:20.720 --> 02:13:21.280]   It's a good pan.
[02:13:21.280 --> 02:13:22.320]   It looks like a pot.
[02:13:22.320 --> 02:13:25.200]   Pestin actually makes really good just regular pans.
[02:13:25.200 --> 02:13:26.480]   But these are the new--
[02:13:26.480 --> 02:13:28.320]   This is what we pay attention to.
[02:13:28.320 --> 02:13:29.120]   Yeah.
[02:13:29.120 --> 02:13:29.920]   Oh, I can't.
[02:13:29.920 --> 02:13:31.840]   I might not be able to hold it in an un-trit.
[02:13:31.840 --> 02:13:34.640]   So this guy right here is where the battery goes.
[02:13:34.640 --> 02:13:35.360]   Yes, my pan--
[02:13:35.360 --> 02:13:35.760]   Battery?
[02:13:35.760 --> 02:13:36.160]   --the battery.
[02:13:36.160 --> 02:13:37.120]   In your pan?
[02:13:37.120 --> 02:13:38.080]   What?
[02:13:38.080 --> 02:13:39.120]   What?
[02:13:39.120 --> 02:13:45.600]   So this pan connects to my phone and my range via Bluetooth.
[02:13:46.160 --> 02:13:49.600]   And what it does is on its--
[02:13:49.600 --> 02:13:51.600]   Well, on its own it doesn't do anything.
[02:13:51.600 --> 02:13:53.840]   But on the proper induction cooktop,
[02:13:53.840 --> 02:13:57.200]   what it will do is it measures the--
[02:13:57.200 --> 02:13:59.440]   It talks to the range, the cooktop,
[02:13:59.440 --> 02:14:03.920]   and it has sensors in the pan that measure the temperature
[02:14:03.920 --> 02:14:05.680]   that the pan has reached.
[02:14:05.680 --> 02:14:08.400]   So what you can do is you can tell it either A,
[02:14:08.400 --> 02:14:14.160]   I want to maintain a 240-degree pan for clarifying butter.
[02:14:15.040 --> 02:14:16.800]   And it will do that for you.
[02:14:16.800 --> 02:14:19.760]   Or you can alternatively say,
[02:14:19.760 --> 02:14:21.440]   "I want to cook chicken."
[02:14:21.440 --> 02:14:24.080]   And there are pre-programmed recipes
[02:14:24.080 --> 02:14:26.240]   that it will just cook the chicken for you.
[02:14:26.240 --> 02:14:27.600]   So we have made--
[02:14:27.600 --> 02:14:28.480]   Jeff, this is for you.
[02:14:28.480 --> 02:14:29.680]   Catch-o-e-pepe.
[02:14:29.680 --> 02:14:30.480]   Oh, yeah.
[02:14:30.480 --> 02:14:31.280]   Oh, yeah.
[02:14:31.280 --> 02:14:32.640]   Ooh.
[02:14:32.640 --> 02:14:34.800]   Their recipe was a little rich for my blood,
[02:14:34.800 --> 02:14:36.480]   but it turned out perfectly.
[02:14:36.480 --> 02:14:39.120]   And we did have some snafus.
[02:14:39.120 --> 02:14:41.280]   So the pan only connects to one account,
[02:14:41.280 --> 02:14:42.400]   which I think is a problem,
[02:14:42.400 --> 02:14:45.280]   because in my family, we all cook.
[02:14:45.280 --> 02:14:45.760]   Right.
[02:14:45.760 --> 02:14:48.240]   So we need to connect it to our iPad,
[02:14:48.240 --> 02:14:50.560]   and we need to connect the range to our iPad.
[02:14:50.560 --> 02:14:53.040]   So if you don't have a dedicated home computer,
[02:14:53.040 --> 02:14:54.640]   it's kind of an issue.
[02:14:54.640 --> 02:14:58.800]   So that's the thing, one, that we learned.
[02:14:58.800 --> 02:15:01.520]   But my husband freaking loves it.
[02:15:01.520 --> 02:15:03.600]   He loves it because he is the most--
[02:15:03.600 --> 02:15:06.960]   I cook by throwing stuff in and smell.
[02:15:06.960 --> 02:15:10.160]   He cooks by following the recipe exactly.
[02:15:11.360 --> 02:15:12.080]   Engineer.
[02:15:12.080 --> 02:15:13.280]   Sometimes his--
[02:15:13.280 --> 02:15:16.000]   Yeah, sometimes his recipes don't turn out
[02:15:16.000 --> 02:15:18.000]   always so great simply just because,
[02:15:18.000 --> 02:15:18.960]   you know, he's--
[02:15:18.960 --> 02:15:21.280]   it's like they only said to cook it for 10 minutes.
[02:15:21.280 --> 02:15:22.960]   I'm like, but it clearly isn't done.
[02:15:22.960 --> 02:15:24.640]   And he's like, but it said 10 minutes.
[02:15:24.640 --> 02:15:26.720]   So this is helpful for him.
[02:15:26.720 --> 02:15:29.040]   Stubborn engineer.
[02:15:29.040 --> 02:15:30.560]   Yeah.
[02:15:30.560 --> 02:15:32.400]   Oh, the computer told me it's done.
[02:15:32.400 --> 02:15:33.120]   That's fine.
[02:15:33.120 --> 02:15:33.760]   I'll take it off.
[02:15:33.760 --> 02:15:35.280]   So Stacey, I don't understand one thing.
[02:15:35.280 --> 02:15:38.560]   The pot is--
[02:15:40.080 --> 02:15:40.720]   Cheese.
[02:15:40.720 --> 02:15:42.560]   No, there's a lot going on here.
[02:15:42.560 --> 02:15:43.920]   That wasn't like a--
[02:15:43.920 --> 02:15:44.640]   you're stupid.
[02:15:44.640 --> 02:15:47.520]   It was like there is a lot happening in this situation.
[02:15:47.520 --> 02:15:48.320]   Oh, why?
[02:15:48.320 --> 02:15:50.240]   Why?
[02:15:50.240 --> 02:15:51.040]   No, no, sorry.
[02:15:51.040 --> 02:15:53.280]   That was not meant to be an assault on you.
[02:15:53.280 --> 02:15:53.600]   No.
[02:15:53.600 --> 02:15:54.400]   So this time--
[02:15:54.400 --> 02:15:56.800]   You've probably gone through it, but I--
[02:15:56.800 --> 02:15:58.320]   but hey, I'm on Twitter the whole time.
[02:15:58.320 --> 02:16:03.920]   So why would the pan need to be smart?
[02:16:03.920 --> 02:16:05.920]   Isn't just a stove all you need to be smart?
[02:16:05.920 --> 02:16:07.840]   No.
[02:16:07.840 --> 02:16:08.640]   No, it needs to be--
[02:16:08.640 --> 02:16:12.720]   Do you want the--
[02:16:12.720 --> 02:16:13.280]   No, I have the O.
[02:16:13.280 --> 02:16:16.240]   No, you want the pan knows how hot it is.
[02:16:16.240 --> 02:16:17.440]   The stove doesn't know how hot.
[02:16:17.440 --> 02:16:18.240]   As soon as the stove--
[02:16:18.240 --> 02:16:22.560]   The stove is like an extra insurance policy,
[02:16:22.560 --> 02:16:25.280]   but every pan is a little bit different, right?
[02:16:25.280 --> 02:16:27.760]   So you're dealing with both the pan,
[02:16:27.760 --> 02:16:29.520]   the environment, the food that's in it.
[02:16:29.520 --> 02:16:33.840]   The pan has to be connected to the stove, right?
[02:16:33.840 --> 02:16:36.480]   You have a pan compatible stove for this.
[02:16:36.480 --> 02:16:37.280]   That's the keys.
[02:16:37.280 --> 02:16:39.040]   You have to have a stove that's smart enough.
[02:16:39.040 --> 02:16:40.560]   Hestan-- the Hestan queues originally
[02:16:40.560 --> 02:16:43.680]   came with a smart single burner cooktop.
[02:16:43.680 --> 02:16:44.560]   Right, right.
[02:16:44.560 --> 02:16:45.920]   And you could still buy that.
[02:16:45.920 --> 02:16:47.200]   Yeah, yeah, I have that.
[02:16:47.200 --> 02:16:48.000]   So like--
[02:16:48.000 --> 02:16:48.480]   I'm not the--
[02:16:48.480 --> 02:16:49.680]   I'm not the whole question yet.
[02:16:49.680 --> 02:16:50.160]   Yes.
[02:16:50.160 --> 02:16:51.040]   Oh, OK.
[02:16:51.040 --> 02:16:52.320]   So tell me the question.
[02:16:52.320 --> 02:16:53.280]   Well, here, let me answer.
[02:16:53.280 --> 02:16:54.640]   So this is a good use case.
[02:16:54.640 --> 02:16:57.600]   When you're heating oil to fry anything,
[02:16:57.600 --> 02:17:00.320]   let's say chicken, you've got your oil at a set temperature,
[02:17:00.320 --> 02:17:01.840]   but when you add the chicken,
[02:17:01.840 --> 02:17:04.800]   it's going to lower the temperature of the oil a little bit.
[02:17:04.800 --> 02:17:08.480]   And the pan can tell that and have the oven compensate.
[02:17:08.480 --> 02:17:10.160]   So that's why--
[02:17:10.160 --> 02:17:11.040]   That's cool.
[02:17:11.040 --> 02:17:12.720]   That's cool.
[02:17:12.720 --> 02:17:13.120]   Go, Jack.
[02:17:13.120 --> 02:17:14.320]   The oven or the cooktop?
[02:17:14.320 --> 02:17:16.320]   I'm sorry, the cooktop.
[02:17:16.320 --> 02:17:16.800]   I'm sorry.
[02:17:16.800 --> 02:17:17.200]   It's difficult.
[02:17:17.200 --> 02:17:18.000]   OK.
[02:17:18.000 --> 02:17:20.080]   You threw that in to test me.
[02:17:20.080 --> 02:17:20.320]   I don't--
[02:17:20.320 --> 02:17:21.840]   I know what you're doing.
[02:17:21.840 --> 02:17:22.640]   I get it.
[02:17:22.640 --> 02:17:25.120]   You're not on Twitter.
[02:17:25.120 --> 02:17:25.760]   You pass.
[02:17:25.760 --> 02:17:29.040]   So what sensors are in the pan?
[02:17:29.040 --> 02:17:30.880]   What is it measuring?
[02:17:30.880 --> 02:17:32.960]   Just temperature alone or in here?
[02:17:32.960 --> 02:17:34.880]   I believe it's just temperature.
[02:17:34.880 --> 02:17:36.400]   OK.
[02:17:36.400 --> 02:17:37.200]   But don't--
[02:17:37.200 --> 02:17:40.160]   That's all that I have seen it measuring.
[02:17:40.160 --> 02:17:42.320]   But if someone else knows--
[02:17:42.320 --> 02:17:42.800]   Like I--
[02:17:42.800 --> 02:17:44.400]   And then last question, which is a lie.
[02:17:44.400 --> 02:17:45.360]   I'll do one more.
[02:17:45.360 --> 02:17:46.080]   So how do you--
[02:17:46.080 --> 02:17:49.040]   Is there a way to check its progress, right?
[02:17:49.040 --> 02:17:51.920]   Because your microwave tells you something on the screen on it.
[02:17:51.920 --> 02:17:54.640]   Does your stove have a screen that tells you
[02:17:54.640 --> 02:17:56.160]   the chicken is almost done?
[02:17:56.160 --> 02:17:58.640]   Or does your iPad do that?
[02:17:58.640 --> 02:18:00.080]   Or how do you know kind of what that is?
[02:18:00.080 --> 02:18:00.720]   But now--
[02:18:00.720 --> 02:18:04.240]   Your microwave just tells you how much time is left that you set.
[02:18:04.240 --> 02:18:06.320]   Well, it's something for the machine is done.
[02:18:06.320 --> 02:18:07.040]   It's something.
[02:18:07.040 --> 02:18:12.320]   Well, if you want to do that, just turn on your stopwatch
[02:18:12.320 --> 02:18:13.840]   when you throw something on the screen.
[02:18:13.840 --> 02:18:16.240]   But the microwave tells me when it's done.
[02:18:16.240 --> 02:18:18.800]   How does your hands remain in it?
[02:18:18.800 --> 02:18:19.840]   Tell me when it's done.
[02:18:19.840 --> 02:18:21.360]   It's finished.
[02:18:21.360 --> 02:18:23.920]   All I did was what you told me to do,
[02:18:23.920 --> 02:18:25.280]   and you're in control.
[02:18:25.280 --> 02:18:28.400]   Now the pan and the stove have taken over from you.
[02:18:28.400 --> 02:18:28.960]   You're now--
[02:18:28.960 --> 02:18:30.560]   I'm so sorry, Mr. LaPorit.
[02:18:30.560 --> 02:18:35.440]   I'm so sorry, Miss Stacy.
[02:18:35.440 --> 02:18:38.560]   How do you know when the chicken is done?
[02:18:38.560 --> 02:18:42.320]   There's a Heston Q app, and it will tell you.
[02:18:42.320 --> 02:18:42.560]   Uh-huh.
[02:18:42.560 --> 02:18:45.440]   OK, that's probably the simple question.
[02:18:45.440 --> 02:18:45.840]   And--
[02:18:45.840 --> 02:18:47.200]   Yeah, don't I know.
[02:18:47.200 --> 02:18:49.200]   It was just really killing you, man.
[02:18:49.200 --> 02:18:50.880]   I was just going to let it happen.
[02:18:50.880 --> 02:18:52.640]   I'm like, I'll just answer later.
[02:18:52.640 --> 02:18:54.160]   It happened.
[02:18:54.160 --> 02:18:57.040]   OK, thank you, Stacy.
[02:18:57.040 --> 02:18:59.840]   Are you all done now?
[02:19:00.640 --> 02:19:01.440]   OK.
[02:19:01.440 --> 02:19:02.320]   Oh, wait. I should tell you.
[02:19:02.320 --> 02:19:03.680]   So this pan-- sorry.
[02:19:03.680 --> 02:19:04.640]   One more thing.
[02:19:04.640 --> 02:19:06.560]   This pan is, I think, $2.99.
[02:19:06.560 --> 02:19:08.560]   I got it on sale for $2.55.
[02:19:08.560 --> 02:19:09.520]   This is a chef's pan.
[02:19:09.520 --> 02:19:12.720]   There are five different pan options you can choose,
[02:19:12.720 --> 02:19:14.400]   based on the sorts of things you want to cook.
[02:19:14.400 --> 02:19:17.280]   It's not a nonstick chef's pan,
[02:19:17.280 --> 02:19:18.960]   which I'm feeling kind of sad about.
[02:19:18.960 --> 02:19:19.600]   But--
[02:19:19.600 --> 02:19:20.080]   What?
[02:19:20.080 --> 02:19:20.880]   Yeah, I don't think they have--
[02:19:20.880 --> 02:19:21.840]   It's my cook.
[02:19:21.840 --> 02:19:22.400]   As I remember.
[02:19:22.400 --> 02:19:24.160]   They have one nonstick fry pan now.
[02:19:24.160 --> 02:19:24.640]   Do they have a--
[02:19:24.640 --> 02:19:26.160]   It's a 11-inch fry pan that's nonstick.
[02:19:26.160 --> 02:19:27.120]   That's our next purchase.
[02:19:28.880 --> 02:19:30.480]   Yeah, I have those testing cues,
[02:19:30.480 --> 02:19:31.920]   and I used them a few times,
[02:19:31.920 --> 02:19:32.800]   and then I just thought,
[02:19:32.800 --> 02:19:34.400]   I don't know how to cook.
[02:19:34.400 --> 02:19:40.320]   I am not as excited about this as my husband did.
[02:19:40.320 --> 02:19:42.800]   Yeah, he likes to be told what to do, probably.
[02:19:42.800 --> 02:19:44.640]   It's like the June oven.
[02:19:44.640 --> 02:19:45.440]   He loves that thing.
[02:19:45.440 --> 02:19:46.240]   I mean, I love it, too.
[02:19:46.240 --> 02:19:46.640]   Yeah.
[02:19:46.640 --> 02:19:49.760]   Lisa came around on the June--
[02:19:49.760 --> 02:19:51.040]   she doesn't like it that you have to tap
[02:19:51.040 --> 02:19:52.800]   like three times to cook something.
[02:19:52.800 --> 02:19:54.960]   But it does such a good job on the Brussels sprouts.
[02:19:54.960 --> 02:19:57.440]   She kind of came around on the June.
[02:19:57.440 --> 02:20:00.400]   Does it kill them beyond a shadow of recognition?
[02:20:00.400 --> 02:20:01.360]   No, it could be what it could--
[02:20:01.360 --> 02:20:01.920]   Chris--
[02:20:01.920 --> 02:20:02.640]   No, yes.
[02:20:02.640 --> 02:20:04.080]   That's so perfect for that.
[02:20:04.080 --> 02:20:06.720]   She grew up with boiled Brussels sprouts,
[02:20:06.720 --> 02:20:08.960]   and told me I will never eat a Brussels sprout.
[02:20:08.960 --> 02:20:09.520]   Oh.
[02:20:09.520 --> 02:20:12.560]   And then once we tried this nice roasting thing
[02:20:12.560 --> 02:20:14.160]   and they're crispy and they're delicious,
[02:20:14.160 --> 02:20:15.040]   she said, "Oh, I love--"
[02:20:15.040 --> 02:20:16.000]   She makes it all the time,
[02:20:16.000 --> 02:20:19.280]   which is too bad because it really stinks up there.
[02:20:19.280 --> 02:20:20.240]   Yes.
[02:20:20.240 --> 02:20:24.400]   It's really cruciferous.
[02:20:24.400 --> 02:20:25.840]   It's cruciferous--
[02:20:25.840 --> 02:20:26.960]   it's very large.
[02:20:26.960 --> 02:20:30.880]   Jeff, do you have a number of the week?
[02:20:30.880 --> 02:20:32.240]   Well, I kind of gave it before,
[02:20:32.240 --> 02:20:32.960]   but I'll do another one.
[02:20:32.960 --> 02:20:34.640]   I'm going to continue a rant from last week.
[02:20:34.640 --> 02:20:35.760]   Yes.
[02:20:35.760 --> 02:20:36.960]   It is an wordle.
[02:20:36.960 --> 02:20:38.160]   I hate wordle.
[02:20:38.160 --> 02:20:39.520]   Oh, I'm really enjoying it.
[02:20:39.520 --> 02:20:40.400]   Why don't you play it?
[02:20:40.400 --> 02:20:41.120]   Then you would hate it.
[02:20:41.120 --> 02:20:42.080]   It's bams, everybody.
[02:20:42.080 --> 02:20:43.040]   No, I'm not doing that.
[02:20:43.040 --> 02:20:44.720]   You're not going to pull me into your cult.
[02:20:44.720 --> 02:20:45.360]   No.
[02:20:45.360 --> 02:20:46.960]   Is this primally?
[02:20:46.960 --> 02:20:47.920]   So tell me it's primally.
[02:20:47.920 --> 02:20:49.360]   No.
[02:20:49.360 --> 02:20:49.840]   No.
[02:20:49.840 --> 02:20:51.680]   So a bot came along to say--
[02:20:51.680 --> 02:20:53.040]   Oh, you're a wordle.
[02:20:53.040 --> 02:20:54.240]   You're a wordle later.
[02:20:54.240 --> 02:20:55.040]   Yeah.
[02:20:55.040 --> 02:20:56.240]   To turn a wordle.
[02:20:56.240 --> 02:20:56.720]   Yes, wordle.
[02:20:56.720 --> 02:20:57.600]   Said the bot.
[02:20:57.600 --> 02:21:00.880]   People don't care about your mediocre linguistic escapades.
[02:21:00.880 --> 02:21:02.080]   To teach you a lesson,
[02:21:02.080 --> 02:21:03.680]   tomorrow's word is--
[02:21:03.680 --> 02:21:04.160]   Ooh.
[02:21:04.160 --> 02:21:05.760]   Bad.
[02:21:05.760 --> 02:21:07.120]   And Twitter killed the bot.
[02:21:07.120 --> 02:21:08.480]   Not all the damn smell.
[02:21:08.480 --> 02:21:09.520]   No, they should, because that's cheating.
[02:21:09.520 --> 02:21:11.120]   Whether you got that completely wrong--
[02:21:11.120 --> 02:21:11.680]   And by the way,
[02:21:11.680 --> 02:21:14.240]   how did it know what the word for tomorrow was?
[02:21:14.240 --> 02:21:14.800]   That's what everybody else--
[02:21:14.800 --> 02:21:16.160]   Because an algorithm can do it.
[02:21:16.160 --> 02:21:16.400]   No.
[02:21:16.400 --> 02:21:18.880]   It found the list, probably.
[02:21:18.880 --> 02:21:18.960]   OK.
[02:21:18.960 --> 02:21:23.920]   Well, Kim Zetter said that you could just--
[02:21:23.920 --> 02:21:25.840]   Because it's written in JavaScript,
[02:21:25.840 --> 02:21:26.480]   you can just--
[02:21:26.480 --> 02:21:27.200]   Just see it.
[02:21:27.200 --> 02:21:28.000]   You can show the--
[02:21:28.000 --> 02:21:29.600]   You can cheat.
[02:21:29.600 --> 02:21:30.640]   But why would you do that?
[02:21:30.640 --> 02:21:31.760]   You're only cheating yourself.
[02:21:31.760 --> 02:21:34.160]   Because it's a dumb game that spams everybody.
[02:21:34.160 --> 02:21:35.040]   That's why you would.
[02:21:35.040 --> 02:21:36.240]   It's--
[02:21:36.240 --> 02:21:38.160]   Look, games don't spam.
[02:21:38.160 --> 02:21:40.000]   Players spam people.
[02:21:40.000 --> 02:21:40.720]   Yeah.
[02:21:40.720 --> 02:21:41.280]   Yeah.
[02:21:41.280 --> 02:21:41.760]   Yeah.
[02:21:41.760 --> 02:21:42.240]   Yeah.
[02:21:42.240 --> 02:21:44.240]   The game is not spamming nobody.
[02:21:44.240 --> 02:21:45.440]   You've got to talk to the people
[02:21:45.440 --> 02:21:47.760]   on your Twitter stream who keep spamming their wordle.
[02:21:47.760 --> 02:21:48.480]   I do.
[02:21:48.480 --> 02:21:49.520]   They talk back to me--
[02:21:49.520 --> 02:21:50.480]   You've never seen my wordle score.
[02:21:50.480 --> 02:21:51.920]   You've never seen my wordle score.
[02:21:51.920 --> 02:21:54.240]   John's never tweeted his wordle score.
[02:21:54.240 --> 02:21:56.160]   I've never tweeted my wordle score.
[02:21:56.160 --> 02:21:57.680]   No, I've never heard of it.
[02:21:57.680 --> 02:21:59.360]   I got yesterday's in two.
[02:21:59.360 --> 02:22:00.160]   Ooh.
[02:22:00.160 --> 02:22:00.800]   Whoa.
[02:22:00.800 --> 02:22:01.840]   That's worthy of a tweet.
[02:22:01.840 --> 02:22:02.800]   You just did it, John.
[02:22:02.800 --> 02:22:04.320]   You just did it anyway.
[02:22:04.320 --> 02:22:06.560]   That was a live spam, wordle spam.
[02:22:06.560 --> 02:22:09.120]   Yeah, but I got it in two.
[02:22:09.120 --> 02:22:10.080]   That's pretty impressive.
[02:22:10.080 --> 02:22:10.560]   But you know what I have to think--
[02:22:10.560 --> 02:22:11.520]   That's pretty nice.
[02:22:11.520 --> 02:22:13.040]   When you get it in two, that's just luck.
[02:22:13.040 --> 02:22:14.160]   Let's be honest.
[02:22:14.160 --> 02:22:15.520]   What's your first word, John?
[02:22:15.520 --> 02:22:16.400]   What's the wordle of your story?
[02:22:16.400 --> 02:22:17.920]   Do you have a regular word you start with?
[02:22:17.920 --> 02:22:19.600]   Well, it was from Twitter.
[02:22:20.800 --> 02:22:22.400]   What did Ashley Esquetha say?
[02:22:22.400 --> 02:22:24.080]   Audio.
[02:22:24.080 --> 02:22:25.200]   Audio was just weird.
[02:22:25.200 --> 02:22:25.680]   I always use audio.
[02:22:25.680 --> 02:22:26.720]   They used a lot of vowels.
[02:22:26.720 --> 02:22:28.800]   And the word yesterday was sugar.
[02:22:28.800 --> 02:22:31.680]   So I got the A and the U, and the U is in the right place.
[02:22:31.680 --> 02:22:32.880]   Oh.
[02:22:32.880 --> 02:22:34.720]   So I couldn't put the A anywhere else.
[02:22:34.720 --> 02:22:37.440]   I put the A where it goes and sugar was obvious.
[02:22:37.440 --> 02:22:39.040]   I got sugar, but it took me four hours.
[02:22:39.040 --> 02:22:40.000]   Oh, what did I start?
[02:22:40.000 --> 02:22:40.960]   What did I start with?
[02:22:40.960 --> 02:22:41.520]   What's your start?
[02:22:41.520 --> 02:22:43.200]   What word do you start with Stacy?
[02:22:43.200 --> 02:22:44.320]   What's the first word you use?
[02:22:44.320 --> 02:22:44.400]   Oh, Lord.
[02:22:44.400 --> 02:22:47.120]   So I have to play a wordle.
[02:22:47.120 --> 02:22:47.840]   Oh.
[02:22:47.840 --> 02:22:48.960]   Yay, Stacy.
[02:22:48.960 --> 02:22:50.240]   I know I like you.
[02:22:50.240 --> 02:22:51.920]   I just-- I don't--
[02:22:51.920 --> 02:22:54.480]   If you're a member of Club Twit, you may be happy to know
[02:22:54.480 --> 02:22:58.400]   that there is a wordle group in our Club Twit,
[02:22:58.400 --> 02:23:00.800]   where you can post your wordle scores.
[02:23:00.800 --> 02:23:02.560]   And what have you done?
[02:23:02.560 --> 02:23:03.200]   Oh, it's OK.
[02:23:03.200 --> 02:23:03.680]   You know what?
[02:23:03.680 --> 02:23:04.800]   Oh, it's really there.
[02:23:04.800 --> 02:23:05.360]   You put it there.
[02:23:05.360 --> 02:23:05.680]   Fine.
[02:23:05.680 --> 02:23:06.640]   Put that there.
[02:23:06.640 --> 02:23:08.000]   That's good.
[02:23:08.000 --> 02:23:09.120]   Post up there.
[02:23:09.120 --> 02:23:10.800]   I'm happy with that.
[02:23:10.800 --> 02:23:11.600]   Yeah.
[02:23:11.600 --> 02:23:12.960]   Yeah.
[02:23:12.960 --> 02:23:13.920]   It's actually kind of--
[02:23:13.920 --> 02:23:15.840]   Did you all see my feed?
[02:23:15.840 --> 02:23:16.400]   There's sugar.
[02:23:16.400 --> 02:23:17.040]   What's primal?
[02:23:17.040 --> 02:23:19.200]   Oh.
[02:23:19.200 --> 02:23:21.200]   OK, Google Primal PRIMEL.
[02:23:21.200 --> 02:23:23.200]   I'm-- oh, god.
[02:23:23.200 --> 02:23:24.400]   Shut up, Google.
[02:23:24.400 --> 02:23:25.840]   It's prime numbers.
[02:23:25.840 --> 02:23:27.280]   It's prime numbers.
[02:23:27.280 --> 02:23:28.720]   Stop.
[02:23:28.720 --> 02:23:29.600]   OK, here we go.
[02:23:29.600 --> 02:23:30.720]   Primal.
[02:23:30.720 --> 02:23:32.960]   So you have to, what, come up with a prime number?
[02:23:32.960 --> 02:23:34.640]   Yep.
[02:23:34.640 --> 02:23:36.080]   Every guess has to be prime.
[02:23:36.080 --> 02:23:38.480]   Every guess has to be prime,
[02:23:38.480 --> 02:23:39.680]   and you have to find the prime number.
[02:23:39.680 --> 02:23:40.720]   That's hard.
[02:23:40.720 --> 02:23:42.240]   This is-- oh, yeah.
[02:23:42.240 --> 02:23:43.520]   No, it's for like the hard--
[02:23:43.520 --> 02:23:44.080]   But you know what?
[02:23:44.080 --> 02:23:45.040]   Our audience--
[02:23:45.040 --> 02:23:46.400]   I can see people as audience.
[02:23:46.400 --> 02:23:47.360]   Guess the prime and six tries.
[02:23:47.360 --> 02:23:48.400]   [Sighs]
[02:23:48.400 --> 02:23:49.040]   Yep.
[02:23:49.040 --> 02:23:50.160]   You can't do that.
[02:23:50.160 --> 02:23:51.520]   You won't see that, tweet it.
[02:23:51.520 --> 02:23:53.360]   But I could write a program to do it.
[02:23:53.360 --> 02:23:54.560]   Maybe I'll do that.
[02:23:54.560 --> 02:23:55.040]   Yeah.
[02:23:55.040 --> 02:23:55.680]   Oh, look at you.
[02:23:55.680 --> 02:23:57.760]   Well, some people have generated--
[02:23:57.760 --> 02:23:59.760]   you could use a list of prime numbers.
[02:23:59.760 --> 02:24:01.200]   I just thought, you know,
[02:24:01.200 --> 02:24:02.960]   it feels very engineering, Matthew.
[02:24:02.960 --> 02:24:04.800]   Oh, I can see next week's show Leo
[02:24:04.800 --> 02:24:06.480]   is going to be showing us the code he does.
[02:24:06.480 --> 02:24:06.880]   I am.
[02:24:06.880 --> 02:24:07.920]   I'm going to do a primal song.
[02:24:07.920 --> 02:24:08.320]   Do it.
[02:24:08.320 --> 02:24:08.880]   That's easy.
[02:24:08.880 --> 02:24:09.440]   Do it.
[02:24:09.440 --> 02:24:09.920]   Trivial.
[02:24:09.920 --> 02:24:12.640]   Trivial.
[02:24:12.640 --> 02:24:13.840]   Just generate that--
[02:24:13.840 --> 02:24:14.800]   That's your sign for next week's--
[02:24:14.800 --> 02:24:17.040]   You could probably very easily generate
[02:24:17.040 --> 02:24:19.280]   the entire set of five digit prime numbers.
[02:24:19.280 --> 02:24:19.840]   Oh, yeah.
[02:24:19.840 --> 02:24:22.880]   And the solver would be a lot easier than--
[02:24:22.880 --> 02:24:25.120]   Yeah, solver between your head is hard.
[02:24:25.120 --> 02:24:26.560]   Yeah, I'm going to--
[02:24:26.560 --> 02:24:28.720]   There's a lot more words than there are
[02:24:28.720 --> 02:24:29.840]   five digit prime numbers.
[02:24:29.840 --> 02:24:31.440]   [Laughs]
[02:24:31.440 --> 02:24:32.560]   And if you're looking for it, y'all,
[02:24:32.560 --> 02:24:34.560]   it's prime EL.
[02:24:34.560 --> 02:24:36.080]   So, prime EL.
[02:24:36.080 --> 02:24:36.560]   Yeah, like--
[02:24:36.560 --> 02:24:37.200]   Not primal.
[02:24:37.200 --> 02:24:38.240]   Prime numbers.
[02:24:38.240 --> 02:24:39.200]   It's a convergent--
[02:24:39.200 --> 02:24:39.840]   Or how old?
[02:24:39.840 --> 02:24:40.560]   Converged.
[02:24:40.560 --> 02:24:43.440]   Dot YT slash primal.
[02:24:43.440 --> 02:24:45.600]   Yeah, primal solver.
[02:24:45.600 --> 02:24:46.160]   That'd be fun.
[02:24:46.960 --> 02:24:50.480]   Uh, thank you, Ed, Jeff, for whatever the hell that was.
[02:24:50.480 --> 02:24:51.360]   And--
[02:24:51.360 --> 02:24:51.680]   Grant.
[02:24:51.680 --> 02:24:53.760]   [Laughs]
[02:24:53.760 --> 02:24:54.320]   It's five.
[02:24:54.320 --> 02:24:56.080]   The number was five for five letters.
[02:24:56.080 --> 02:24:57.600]   Five because of Wordel.
[02:24:57.600 --> 02:24:59.360]   And troll is a five letter word.
[02:24:59.360 --> 02:25:00.560]   That's what everybody who sends me
[02:25:00.560 --> 02:25:00.880]   to this--
[02:25:00.880 --> 02:25:00.880]   Yes.
[02:25:00.880 --> 02:25:02.800]   This Wordel crap is doing to me.
[02:25:02.800 --> 02:25:03.360]   Yes.
[02:25:03.360 --> 02:25:04.960]   I like Wordel.
[02:25:04.960 --> 02:25:05.840]   It's fun.
[02:25:05.840 --> 02:25:06.320]   I will--
[02:25:06.320 --> 02:25:07.680]   I will do it first thing in the morning
[02:25:07.680 --> 02:25:08.800]   just to get my brain started.
[02:25:08.800 --> 02:25:09.920]   I think it's a good way to start.
[02:25:09.920 --> 02:25:11.560]   Uh, Ant-
[02:25:11.560 --> 02:25:13.600]   [Laughs]
[02:25:13.600 --> 02:25:14.960]   What's your pick of the week this week?
[02:25:15.760 --> 02:25:18.800]   My pick of the week is a podcast I was a guest on.
[02:25:18.800 --> 02:25:20.640]   It's called Because We Make.
[02:25:20.640 --> 02:25:22.160]   Hosted by a friend of mine.
[02:25:22.160 --> 02:25:24.160]   I've known for quite a while now.
[02:25:24.160 --> 02:25:27.840]   Um, his name is Vincent Ferrari.
[02:25:27.840 --> 02:25:28.400]   Oh, yeah.
[02:25:28.400 --> 02:25:30.240]   It's co-host Ethan Carter.
[02:25:30.240 --> 02:25:33.280]   And Vincent says to you, Mr. Jarvis,
[02:25:33.280 --> 02:25:34.400]   he says hello.
[02:25:34.400 --> 02:25:34.960]   Because he--
[02:25:34.960 --> 02:25:35.600]   Hello, Vincent.
[02:25:35.600 --> 02:25:36.080]   --joked about.
[02:25:36.080 --> 02:25:38.800]   Checking in on you with some
[02:25:38.800 --> 02:25:42.000]   funny Twitter conversations of your,
[02:25:42.000 --> 02:25:43.840]   if you will, but he says to tell you hello.
[02:25:44.640 --> 02:25:45.840]   Um, but yeah, I was a guest on that
[02:25:45.840 --> 02:25:47.360]   and we just talked about, um,
[02:25:47.360 --> 02:25:50.320]   some of the stuff I do here at Twitch and my show and just--
[02:25:50.320 --> 02:25:51.200]   You're in a maker.
[02:25:51.200 --> 02:25:52.480]   Content creator stuff.
[02:25:52.480 --> 02:25:52.960]   Yeah.
[02:25:52.960 --> 02:25:55.440]   I love their show because it's, it's, it's--
[02:25:55.440 --> 02:25:57.840]   Yes, it's geared towards makers.
[02:25:57.840 --> 02:25:58.720]   I'm not a maker.
[02:25:58.720 --> 02:26:01.680]   I can tell you the first thing about a,
[02:26:01.680 --> 02:26:04.720]   I don't know, bandsaw and butt joints and not--
[02:26:04.720 --> 02:26:05.200]   none of that.
[02:26:05.200 --> 02:26:06.160]   But I still listen to it.
[02:26:06.160 --> 02:26:08.400]   It's just good conversations that they have every week.
[02:26:08.400 --> 02:26:09.120]   So does--
[02:26:09.120 --> 02:26:12.160]   So if you want more bandsaws and butt joints,
[02:26:12.160 --> 02:26:13.680]   because we make.com.
[02:26:13.680 --> 02:26:15.120]   [LAUGHTER]
[02:26:15.120 --> 02:26:15.760]   That's the way.
[02:26:15.760 --> 02:26:17.760]   The fact that you even know the word butt joint,
[02:26:17.760 --> 02:26:19.280]   you're way ahead of me.
[02:26:19.280 --> 02:26:20.880]   I heard them mention it on that show.
[02:26:20.880 --> 02:26:22.320]   I'm like, what are you talking about?
[02:26:22.320 --> 02:26:23.360]   [LAUGHTER]
[02:26:23.360 --> 02:26:24.080]   Joints and--
[02:26:24.080 --> 02:26:24.960]   [LAUGHTER]
[02:26:24.960 --> 02:26:25.520]   Huh?
[02:26:25.520 --> 02:26:26.240]   Huh?
[02:26:26.240 --> 02:26:27.600]   Tap and die.
[02:26:27.600 --> 02:26:28.800]   I don't know any of that stuff.
[02:26:28.800 --> 02:26:29.840]   Tap and die, baby.
[02:26:29.840 --> 02:26:30.640]   Tap and die.
[02:26:30.640 --> 02:26:31.840]   And then here's the picture.
[02:26:31.840 --> 02:26:35.360]   Here's you put this up of the reporter getting the poor predator.
[02:26:35.360 --> 02:26:37.680]   She's talking about the news, folks,
[02:26:37.680 --> 02:26:40.720]   that, um, earlier today and the dedication.
[02:26:40.720 --> 02:26:43.760]   They're producers, they're editors,
[02:26:43.760 --> 02:26:46.880]   and they're reporters all at the same time.
[02:26:46.880 --> 02:26:48.240]   So she sets up her camera.
[02:26:48.240 --> 02:26:51.120]   She turns on her lights.
[02:26:51.120 --> 02:26:52.080]   She gets the microphone.
[02:26:52.080 --> 02:26:53.520]   She goes on the other side of the camera.
[02:26:53.520 --> 02:26:54.800]   Camera's recording.
[02:26:54.800 --> 02:26:55.600]   Or is she live?
[02:26:55.600 --> 02:26:56.640]   I guess it's just on the live.
[02:26:56.640 --> 02:26:57.280]   It was a water-based live.
[02:26:57.280 --> 02:26:57.840]   It was water-based.
[02:26:57.840 --> 02:26:58.240]   Water-based.
[02:26:58.240 --> 02:26:58.720]   Water-based.
[02:26:58.720 --> 02:26:59.600]   Cold temperatures.
[02:26:59.600 --> 02:27:04.320]   So she's out, poor woman at WSAZ, Channel 3, and Dunbar.
[02:27:04.320 --> 02:27:06.560]   West Virginia, she's out there.
[02:27:06.560 --> 02:27:07.360]   It's her last week.
[02:27:07.360 --> 02:27:09.440]   She's about to move up to a bigger station.
[02:27:09.440 --> 02:27:10.000]   About time.
[02:27:10.000 --> 02:27:12.480]   And there's a car.
[02:27:12.480 --> 02:27:13.760]   It just runs right into her.
[02:27:13.760 --> 02:27:15.280]   She runs right into her camera.
[02:27:15.280 --> 02:27:16.320]   I shouldn't laugh.
[02:27:16.320 --> 02:27:17.120]   That's horrible.
[02:27:17.120 --> 02:27:19.280]   I just got hit by a car, but I'm okay, Tim.
[02:27:19.280 --> 02:27:20.880]   The camera's on its side.
[02:27:20.880 --> 02:27:22.320]   We're all good.
[02:27:22.320 --> 02:27:26.880]   And of course, being local news, they stay right with the shot.
[02:27:26.880 --> 02:27:30.000]   We got hit by a car in college too, just like that.
[02:27:30.000 --> 02:27:30.800]   Wow.
[02:27:30.800 --> 02:27:32.480]   So it's not her first time.
[02:27:32.480 --> 02:27:33.600]   Oh, it was funny.
[02:27:33.600 --> 02:27:35.040]   She's even more than once.
[02:27:35.040 --> 02:27:37.520]   This is all this poor woman.
[02:27:37.520 --> 02:27:38.560]   Isn't this awful?
[02:27:38.560 --> 02:27:40.240]   And then she has to set this up again.
[02:27:40.240 --> 02:27:41.120]   She's so glad.
[02:27:41.120 --> 02:27:42.720]   Wait, wait what she says to the driver.
[02:27:42.720 --> 02:27:44.560]   You are so sweet and you're okay.
[02:27:44.560 --> 02:27:46.240]   It is all good.
[02:27:46.240 --> 02:27:48.640]   Isn't that nice of her?
[02:27:48.640 --> 02:27:50.160]   If she forgives the driver immediately,
[02:27:50.160 --> 02:27:51.200]   of course, she's standing in the road.
[02:27:51.200 --> 02:27:52.640]   But that was a dumb little lie.
[02:27:52.640 --> 02:27:55.120]   She'll learn tomorrow morning when she wakes up.
[02:27:55.120 --> 02:27:56.160]   Yeah, she's going to know.
[02:27:56.160 --> 02:27:56.400]   Yeah.
[02:27:56.400 --> 02:27:58.720]   Oh, gosh.
[02:27:58.720 --> 02:28:02.240]   It's, yeah, I hope she gets a better job.
[02:28:02.240 --> 02:28:03.760]   There you see she's moving the camera.
[02:28:03.760 --> 02:28:05.200]   I hope she gets a better job.
[02:28:05.200 --> 02:28:06.000]   She did already.
[02:28:06.000 --> 02:28:06.400]   Yeah.
[02:28:06.400 --> 02:28:07.440]   She got tons of attention.
[02:28:07.440 --> 02:28:09.520]   Well, but this is, but this is,
[02:28:09.520 --> 02:28:11.920]   I mean, one of the things I thought was interesting
[02:28:11.920 --> 02:28:14.000]   about this story is this is so common now.
[02:28:14.000 --> 02:28:15.120]   And they were talking about like,
[02:28:15.120 --> 02:28:19.600]   some female reporter didn't want to go out at like 11 o'clock
[02:28:19.600 --> 02:28:21.200]   to report on her rape in that area.
[02:28:21.200 --> 02:28:21.680]   Yeah.
[02:28:21.680 --> 02:28:24.400]   And I'm like, yeah, you shouldn't have to do that.
[02:28:24.400 --> 02:28:27.440]   This is notorious.
[02:28:27.440 --> 02:28:29.280]   You know, you get a hurricane.
[02:28:29.280 --> 02:28:31.520]   And who has to stand out there in the hurricane?
[02:28:31.520 --> 02:28:33.600]   The weather guy from Channel 4.
[02:28:33.600 --> 02:28:35.600]   And location shots are worthless.
[02:28:35.600 --> 02:28:36.640]   Yeah, we've talked about that.
[02:28:36.640 --> 02:28:37.840]   There's no reporting there.
[02:28:37.840 --> 02:28:39.440]   It drives me nuts.
[02:28:39.440 --> 02:28:41.200]   Nothing there.
[02:28:41.200 --> 02:28:42.240]   It's just good pictures.
[02:28:42.240 --> 02:28:42.720]   That's all.
[02:28:42.720 --> 02:28:45.200]   Well, it's a visual medium.
[02:28:45.200 --> 02:28:48.480]   One last plug for the hard head.
[02:28:48.480 --> 02:28:49.360]   Yep.
[02:28:49.360 --> 02:28:53.760]   I just wanted to give him a shout out and say good luck.
[02:28:53.760 --> 02:28:56.320]   This past weekend was supposed to be open weekend
[02:28:56.320 --> 02:28:59.040]   for track and field indoor season for him.
[02:28:59.040 --> 02:29:03.040]   And he sent me a picture, actually sent the family
[02:29:03.040 --> 02:29:04.560]   a group of pictures when I saw it.
[02:29:04.560 --> 02:29:05.760]   It just made me cry.
[02:29:05.760 --> 02:29:07.760]   He's running for the University of the Pacific.
[02:29:07.760 --> 02:29:08.560]   Oh my God.
[02:29:08.560 --> 02:29:09.440]   Dang, man.
[02:29:09.440 --> 02:29:10.560]   That's my boy.
[02:29:10.560 --> 02:29:11.360]   Great school.
[02:29:11.360 --> 02:29:14.960]   But yeah, I'm just wanting to wish him well and say, hey,
[02:29:14.960 --> 02:29:16.160]   keep kicking ass, man.
[02:29:16.160 --> 02:29:18.640]   UOP up there in weed California.
[02:29:18.640 --> 02:29:21.760]   No, they're not weed.
[02:29:21.760 --> 02:29:22.640]   Okay.
[02:29:22.640 --> 02:29:23.680]   They're in Oregon.
[02:29:23.680 --> 02:29:24.000]   Okay.
[02:29:24.000 --> 02:29:26.160]   Forest Grove, Oregon.
[02:29:26.160 --> 02:29:26.800]   Why didn't I?
[02:29:26.800 --> 02:29:27.120]   Yes.
[02:29:27.120 --> 02:29:28.880]   Didn't you go to weed for some of it?
[02:29:28.880 --> 02:29:30.400]   Yeah, we stopped on the way.
[02:29:30.400 --> 02:29:30.800]   Okay.
[02:29:30.800 --> 02:29:31.200]   Okay.
[02:29:33.360 --> 02:29:35.680]   Because that town fascinated us.
[02:29:35.680 --> 02:29:36.080]   Yes.
[02:29:36.080 --> 02:29:36.880]   Because it should.
[02:29:36.880 --> 02:29:40.240]   Thank you, Ann Pruitt,
[02:29:40.240 --> 02:29:44.000]   twitter.tv/handsonphotography/hop for his latest.
[02:29:44.000 --> 02:29:46.000]   What are you covering this week on HOP?
[02:29:46.000 --> 02:29:48.800]   This week, I have a special guest,
[02:29:48.800 --> 02:29:51.040]   the one and only Mr. Scott Born.
[02:29:51.040 --> 02:29:54.240]   My buddy from McPherack in the day.
[02:29:54.240 --> 02:29:55.200]   Back in the day.
[02:29:55.200 --> 02:29:56.160]   And he knows Scott Born.
[02:29:56.160 --> 02:29:56.880]   What's he doing?
[02:29:56.880 --> 02:30:01.120]   Well, you know, he's an amazing bird photographer,
[02:30:01.120 --> 02:30:03.200]   but he's pivoting into some other things.
[02:30:03.200 --> 02:30:04.000]   Now.
[02:30:04.000 --> 02:30:07.440]   And I'll just say grab your iPhones and get ready.
[02:30:07.440 --> 02:30:07.840]   Okay.
[02:30:07.840 --> 02:30:08.880]   How fun.
[02:30:08.880 --> 02:30:10.000]   Scott Born.
[02:30:10.000 --> 02:30:10.800]   Hey, Ann.
[02:30:10.800 --> 02:30:12.560]   Can we have a book?
[02:30:12.560 --> 02:30:14.320]   Yes, we do.
[02:30:14.320 --> 02:30:16.240]   It's the unauthorized bread.
[02:30:16.240 --> 02:30:17.680]   That's the winning vote.
[02:30:17.680 --> 02:30:18.400]   Oh, great.
[02:30:18.400 --> 02:30:21.680]   Cory Doctor Rose unauthorized bread will be the book
[02:30:21.680 --> 02:30:23.680]   of the month for Stacey's book club.
[02:30:23.680 --> 02:30:24.480]   When is the event?
[02:30:24.480 --> 02:30:27.200]   We'll figure that out tomorrow.
[02:30:27.200 --> 02:30:28.080]   Don't know yet.
[02:30:28.080 --> 02:30:28.320]   Okay.
[02:30:28.320 --> 02:30:29.920]   Stay tuned.
[02:30:29.920 --> 02:30:31.120]   We're just like leading you on.
[02:30:31.120 --> 02:30:31.760]   But now you have it.
[02:30:31.760 --> 02:30:33.200]   Now that we know it's a novella.
[02:30:33.200 --> 02:30:36.480]   So whether it's Wirtle or the Stacey's book club,
[02:30:36.480 --> 02:30:40.000]   there's plenty of reasons to join club twit.
[02:30:40.000 --> 02:30:43.840]   Best of all, your community manager, Mr. Ant,
[02:30:43.840 --> 02:30:46.640]   Pruitt club twitted seven bucks a month, as I mentioned,
[02:30:46.640 --> 02:30:48.720]   but you get ad free versions of all the show.
[02:30:48.720 --> 02:30:49.920]   You get all the fun.
[02:30:49.920 --> 02:30:51.040]   You get the twit plus twit.
[02:30:51.040 --> 02:30:53.680]   Twit.tv/club twit.
[02:30:53.680 --> 02:30:55.680]   Thank you for giving me a chance to plug it.
[02:30:55.680 --> 02:30:59.200]   And thank you, Ant, for being here.
[02:30:59.200 --> 02:31:01.520]   Jeff Jarvis is the director, ladies and gentlemen,
[02:31:01.520 --> 02:31:04.480]   of the Townite Center for Entrepreneurial Journalism at the...
[02:31:04.480 --> 02:31:08.640]   Craig Newmark Graduate School of Journalism.
[02:31:08.640 --> 02:31:13.200]   At the City University of New York.
[02:31:13.200 --> 02:31:16.320]   Soon to be a mini musical coming to a stage near you.
[02:31:16.320 --> 02:31:17.040]   Yeah, we don't need it.
[02:31:17.040 --> 02:31:17.600]   We got it.
[02:31:17.600 --> 02:31:19.040]   We got it.
[02:31:19.040 --> 02:31:20.080]   Outstanding.
[02:31:20.080 --> 02:31:23.520]   And thanks, of course, to Stacey Higginbotham.
[02:31:23.520 --> 02:31:27.040]   Stacey on IOT.com is her website, her podcast,
[02:31:27.040 --> 02:31:29.120]   the IOT podcast with Kevin Tofel.
[02:31:29.120 --> 02:31:30.080]   Sign up for the newsletter.
[02:31:30.080 --> 02:31:30.560]   It's free.
[02:31:31.120 --> 02:31:32.160]   Sign up for events.
[02:31:32.160 --> 02:31:34.400]   They're free to thank you, Stacey.
[02:31:34.400 --> 02:31:36.880]   Thank you so much.
[02:31:36.880 --> 02:31:38.800]   Thank you all for being here.
[02:31:38.800 --> 02:31:41.440]   We do twig every Wednesday, 130...
[02:31:41.440 --> 02:31:43.760]   I'm sorry, 2 o'clock Pacific, 5 p.m. Eastern,
[02:31:43.760 --> 02:31:45.760]   2200 UTC.
[02:31:45.760 --> 02:31:50.800]   And you can watch us do it live if you want it live.twit.tv.
[02:31:50.800 --> 02:31:53.920]   If you're watching live chat live at irc.twit.tv
[02:31:53.920 --> 02:31:57.760]   or in the Discord, there's live chat there for every show as well.
[02:31:57.760 --> 02:32:00.480]   You can also get on-demand shows after the fact
[02:32:00.480 --> 02:32:02.720]   at the website, twit.tv/twig.
[02:32:02.720 --> 02:32:06.080]   Or on YouTube, there's a dedicated channel
[02:32:06.080 --> 02:32:07.600]   or in your favorite podcast player.
[02:32:07.600 --> 02:32:10.080]   In fact, if your podcast player supports reviews,
[02:32:10.080 --> 02:32:12.640]   please give us a five-star review so we can share the word.
[02:32:12.640 --> 02:32:16.640]   Tell the world, tell the people about twig.
[02:32:16.640 --> 02:32:20.400]   I'll see you on our Twit community forums,
[02:32:20.400 --> 02:32:21.760]   twit.community.
[02:32:21.760 --> 02:32:24.480]   On our Mastodon instance, twit.social.
[02:32:24.480 --> 02:32:26.400]   I'll see you in Discord and I'll see you next week.
[02:32:26.400 --> 02:32:27.280]   We all will.
[02:32:27.280 --> 02:32:28.160]   On this week in Google.
[02:32:28.160 --> 02:32:28.960]   Bye-bye.
[02:32:29.840 --> 02:32:31.840]   (upbeat music)
[02:32:31.840 --> 02:32:34.240]   Don't miss all about Android every week.
[02:32:34.240 --> 02:32:37.200]   We talk about the latest news, hardware, apps,
[02:32:37.200 --> 02:32:41.680]   and now all the developer-y goodness happening in the Android ecosystem.
[02:32:41.680 --> 02:32:45.040]   I'm Jason Howell, also joined by Ron Richards, Florence Ion,
[02:32:45.040 --> 02:32:47.200]   and our newest co-host on the panel,
[02:32:47.200 --> 02:32:49.680]   when to now who brings her developer chops.
[02:32:49.680 --> 02:32:51.120]   Really great stuff.
[02:32:51.120 --> 02:32:54.160]   We also invite people from all over the Android ecosystem
[02:32:54.160 --> 02:32:57.520]   to talk about this mobile platform we love so much.
[02:32:57.520 --> 02:33:01.360]   Join us every Tuesday, all about Android, on twit.tv.
[02:33:01.360 --> 02:33:11.360]   (upbeat music)

