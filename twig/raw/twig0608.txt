;FFMETADATA1
title=Real Men Don't Use Allen Wrenches
artist=Leo Laporte, Jeff Jarvis, Stacey Higginbotham, Ant Pruitt
album_artist=TWiT
publisher=TWiT
album=This Week in Google
TRDA=2021-04-22
track=608
language=English
genre=Podcast
comment=Nobody likes FLoC, Google is most trusted brand, Daily Mail sues Google, EU's prospective AI ban
encoded_by=Uniblab 5.1
date=2021
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:06.240]   It's time for Twig this week in Google Stacey Higginbotham's here, Jeff Jarvis, and Prout will talk about Google's flock.
[00:00:06.240 --> 00:00:09.600]   Looks like nobody wants to use it, but is it really all that
[00:00:09.600 --> 00:00:15.720]   brat bad? That turns out Google is one of the most trusted brands globally. We'll talk about
[00:00:15.720 --> 00:00:20.100]   jeans, the daily mail suing Google and
[00:00:20.100 --> 00:00:26.200]   OS 9 is out. Lots more to talk about in our Google change log all coming up next.
[00:00:29.720 --> 00:00:32.520]   Podcasts you love from people you trust.
[00:00:32.520 --> 00:00:35.560]   This is Twig.
[00:00:35.560 --> 00:00:48.120]   This is Twig. This week in Google, episode 6 hundred eight, recorded Wednesday April 21st, 2021.
[00:00:48.120 --> 00:00:51.120]   Real men don't use Allen wrenches.
[00:00:51.120 --> 00:00:56.560]   It's time for Twig this week in Google to show we cover the latest from, you know, whatever.
[00:00:57.440 --> 00:01:03.800]   Whatever's on our minds. Stacey Higginbotham is here from Stacy on IOT.com in the IOT podcast.
[00:01:03.800 --> 00:01:11.880]   Hello, Stacey. Hello, everyone. Nice to see you. Stacey is both in the chat and the discord chat.
[00:01:11.880 --> 00:01:18.040]   So you're you're just like a type and fool today. Oh, John, get ready to mute me. I'm so sorry.
[00:01:18.040 --> 00:01:19.440]   I love the banging.
[00:01:19.440 --> 00:01:25.800]   And then it's like, and then like two words come out.
[00:01:26.040 --> 00:01:28.640]   I know it's a lot of it's a lot of work to type.
[00:01:28.640 --> 00:01:32.040]   We got to have something. I'm going to have a voice dictation, something like that.
[00:01:32.040 --> 00:01:34.040]   Who could invent that?
[00:01:34.040 --> 00:01:39.360]   Jeff Jarvis is also here, Leonard Tao, professor for journalistic innovation at the Craig
[00:01:39.360 --> 00:01:44.920]   Newmark Graduate School of Journalism at the City University of New York, buzzer
[00:01:44.920 --> 00:01:47.680]   machine.com is your log home.
[00:01:47.680 --> 00:01:49.960]   New York, New York.
[00:01:49.960 --> 00:01:53.520]   I just was reading that 25 to 35 percent of New Yorkers.
[00:01:54.560 --> 00:01:57.160]   Many of them unwittingly had COVID over the last year.
[00:01:57.160 --> 00:02:00.000]   Kind of a stunning over.
[00:02:00.000 --> 00:02:03.560]   Yeah. Well, it's a small city where there was jammed in.
[00:02:03.560 --> 00:02:05.880]   It hit. Well, it hit New York first and worst, right?
[00:02:05.880 --> 00:02:13.000]   Well, actually not first was saddles first, but it was badly badly, badly, crappy stats for your city.
[00:02:13.000 --> 00:02:15.240]   Yeah, I'm depressing.
[00:02:15.240 --> 00:02:21.240]   And and Pruitt from Hands on Photography, our great friend. Hello, aunt.
[00:02:21.240 --> 00:02:24.280]   Hello, Mr. LaPorte. Good to see you.
[00:02:24.800 --> 00:02:30.720]   You too. Before we get into the news, we have some news of our own.
[00:02:30.720 --> 00:02:33.960]   We announced this on Sunday and the words getting out.
[00:02:33.960 --> 00:02:37.760]   We have announced something we call club twit.
[00:02:37.760 --> 00:02:40.560]   It's a paid tier for twit.
[00:02:40.560 --> 00:02:46.800]   And so the genesis of this is when we first started twit 2005, I thought, Oh, it'd be great
[00:02:46.800 --> 00:02:53.720]   if the we didn't have to worry about advertisers and the implicit conflict of interest and all that.
[00:02:53.720 --> 00:02:55.200]   And if we just let the listeners pay.
[00:02:55.200 --> 00:02:57.520]   So we had a tip chart back in those days.
[00:02:57.520 --> 00:02:59.760]   It was it was early days.
[00:02:59.760 --> 00:03:02.000]   I don't think people really knew what we were doing.
[00:03:02.000 --> 00:03:03.960]   And so, you know, some people contributed.
[00:03:03.960 --> 00:03:05.000]   I paid you way back.
[00:03:05.000 --> 00:03:08.320]   Did you thank you, Jeff, my my son, my son insisted.
[00:03:08.320 --> 00:03:09.320]   Yeah, Jake insisted.
[00:03:09.320 --> 00:03:11.280]   Jake was a big fan. I know. Yeah.
[00:03:11.280 --> 00:03:17.600]   So we we took we took in about at the peak 9,000 a month, which is good money.
[00:03:17.600 --> 00:03:23.040]   I'm not complaining, but it wasn't enough to build a podcast empire by any means or even do a second show.
[00:03:23.600 --> 00:03:28.440]   So at some point, I think about a year and a half in, we started taking advertising.
[00:03:28.440 --> 00:03:32.400]   That's my background, you know, ad supported media, both radio and TV.
[00:03:32.400 --> 00:03:38.640]   So we decided to take ads and that's been very good for us.
[00:03:38.640 --> 00:03:44.200]   It allowed us to build, you know, two new studios, the brick house and now this with the help of our audience
[00:03:44.200 --> 00:03:46.720]   through the bricks that we sold at the brick house.
[00:03:46.720 --> 00:03:49.640]   And of course, grow to many shows and so forth.
[00:03:50.040 --> 00:03:54.000]   But then lately after COVID, advertising started to dry up.
[00:03:54.000 --> 00:03:58.760]   Audience started to get more interested in news podcasts.
[00:03:58.760 --> 00:04:01.640]   And to be honest, there were a lot more podcasts to listen to.
[00:04:01.640 --> 00:04:08.160]   And we had a, you know, our revenue shrank by about, I don't know, at first it was about 30%, which meant we
[00:04:08.160 --> 00:04:10.240]   because we don't have investors.
[00:04:10.240 --> 00:04:12.520]   We had to shrink our staff and shrink our.
[00:04:12.520 --> 00:04:15.200]   And this is a hard thing to do, but we had to do it.
[00:04:15.200 --> 00:04:17.720]   I know a lot of companies had to during the crisis.
[00:04:18.720 --> 00:04:20.920]   Produce the number shows we did and so forth.
[00:04:20.920 --> 00:04:23.040]   Advertising has not come back yet.
[00:04:23.040 --> 00:04:28.800]   It seems like it might start to come back and I'm hopeful for fourth quarter as, as we kind of climb out of this quarantine
[00:04:28.800 --> 00:04:33.840]   hole. But that's not the only headwind audiences.
[00:04:33.840 --> 00:04:36.760]   As I said, there've been, you know, there's many more choices.
[00:04:36.760 --> 00:04:42.240]   Actually, a big issue for us is thanks to things like Spotify and
[00:04:42.240 --> 00:04:49.160]   Heart by Heart Media, big companies who can force you to listen on their platform and thereby contract you.
[00:04:49.160 --> 00:04:55.320]   I mean, they gave Joe Rogan a hundred million dollars to be exclusive because if you listen on Spotify, they know who you
[00:04:55.320 --> 00:04:57.320]   are. They know your home address.
[00:04:57.320 --> 00:04:59.080]   They know your credit card number.
[00:04:59.080 --> 00:05:03.560]   They know everything about you, including when you listen, how often you listen, how long you listen.
[00:05:03.560 --> 00:05:09.760]   And advertisers are addicted to that kind of information as we've talked about many times on this show.
[00:05:10.240 --> 00:05:12.160]   And so we're just seeing it.
[00:05:12.160 --> 00:05:17.160]   We see it as it's going to be likely to get more and more difficult to sell advertising.
[00:05:17.160 --> 00:05:20.880]   In fact, advertisers have already started to say, Hey, we want more information.
[00:05:20.880 --> 00:05:23.320]   We want more tracking and we're very reluctant to do that.
[00:05:23.320 --> 00:05:29.960]   So the idea is, well, maybe if we could have half, I don't want to get rid of the free stuff.
[00:05:29.960 --> 00:05:37.680]   I feel like our free tier, our ad supported tier is so important because not everybody wants to spend money on a podcast and not
[00:05:37.680 --> 00:05:42.440]   everybody can. And I really want to make sure everybody wants to listen to a twig, can hear twig.
[00:05:42.440 --> 00:05:45.160]   So I you have this promise for me.
[00:05:45.160 --> 00:05:47.600]   We are not going to change anything.
[00:05:47.600 --> 00:05:49.920]   Everything is the same.
[00:05:49.920 --> 00:05:56.200]   Same podcast, same places, IRC, the Twitch forums, the Twitter community, the Mastodon, a Twit.
[00:05:56.200 --> 00:05:58.480]   Dot social, all of that will stay and continue to be free.
[00:05:58.480 --> 00:06:02.200]   We're adding something, which is a new paid tier.
[00:06:02.200 --> 00:06:03.160]   It's called Club Twit.
[00:06:03.600 --> 00:06:10.280]   Seven dollars a month gives you ad free versions of all of our shows, both audio and video.
[00:06:10.280 --> 00:06:12.440]   You'll get a custom feed when you join.
[00:06:12.440 --> 00:06:13.880]   That's just for you.
[00:06:13.880 --> 00:06:15.240]   And that is the ad free stuff.
[00:06:15.240 --> 00:06:18.360]   Then there'll be a new feed of ad on video bonus content.
[00:06:18.360 --> 00:06:19.640]   We call that Twit Plus.
[00:06:19.640 --> 00:06:28.280]   Plus we have a discord where you can participate and we'll be using the discord a little later on to take audio questions from our audience.
[00:06:28.280 --> 00:06:33.360]   So, but the main offer I want to say is really the ad free podcast, seven bucks a month.
[00:06:33.600 --> 00:06:35.640]   That's kind of the trade off instead of listening to ads.
[00:06:35.640 --> 00:06:41.560]   You give us the revenue we would generate from your listening to the shows and we'll call it even.
[00:06:41.560 --> 00:06:45.040]   I think it's a fair price and I hope you do too.
[00:06:45.040 --> 00:06:47.600]   But again, you don't have to subscribe.
[00:06:47.600 --> 00:06:49.040]   Everything else is going to stay the same.
[00:06:49.040 --> 00:06:55.880]   So if you want to support us, twit.tv/club twit, twit.tv/club twit.
[00:06:55.880 --> 00:06:57.000]   All the details are there.
[00:06:57.000 --> 00:07:00.400]   The full explanation that you just heard is there.
[00:07:01.400 --> 00:07:04.040]   And you can see exactly how it works.
[00:07:04.040 --> 00:07:07.440]   We use a member full, which is a Patreon company.
[00:07:07.440 --> 00:07:10.200]   They do, I think, a really good job.
[00:07:10.200 --> 00:07:14.960]   We really spent some time, Lisa, especially spent some time researching this,
[00:07:14.960 --> 00:07:19.840]   interviewed a lot of different platforms that we decided member full was the best for what we want to do.
[00:07:19.840 --> 00:07:24.160]   They use Stripe as a payment provider, which means you can use Apple Pay.
[00:07:24.160 --> 00:07:25.120]   You can use a credit card.
[00:07:25.120 --> 00:07:27.320]   You could use pretty much any kind of payment method.
[00:07:27.320 --> 00:07:30.720]   Any national audience members can also use Stripe.
[00:07:31.000 --> 00:07:33.120]   So if you've ever used Stripe before, you know, it's very easy.
[00:07:33.120 --> 00:07:37.480]   Somebody was saying it's actually easier to sign up for club twit than it was to get a pizza the other night.
[00:07:37.480 --> 00:07:39.680]   So I think that's probably the case.
[00:07:39.680 --> 00:07:43.480]   So anyway, that's the story.
[00:07:43.480 --> 00:07:44.680]   We are answering questions.
[00:07:44.680 --> 00:07:45.440]   A lot.
[00:07:45.440 --> 00:07:50.800]   I know there are a lot of questions in the discord once you join or you can email us or ask in the chatroom, IRC.
[00:07:50.800 --> 00:07:52.320]   And we'll answer those two.
[00:07:52.320 --> 00:07:57.320]   No PayPal, long dog, because Stripe is a PayPal competitor.
[00:07:57.320 --> 00:07:59.000]   And that's what member full uses.
[00:07:59.640 --> 00:08:02.520]   And they're really, I think, Stripe is a little bit better.
[00:08:02.520 --> 00:08:05.800]   We used to use PayPal and I'm actually very happy with a with Stripe.
[00:08:05.800 --> 00:08:08.640]   And no, you can't pay me with a theory and retcon five.
[00:08:08.640 --> 00:08:12.400]   How about Dogecoin?
[00:08:12.400 --> 00:08:13.840]   Surely you'll get those.
[00:08:13.840 --> 00:08:14.880]   You sense.
[00:08:14.880 --> 00:08:16.400]   Hey, Rick, can I take a theory?
[00:08:16.400 --> 00:08:20.760]   I have a bad, bad experience.
[00:08:20.760 --> 00:08:23.360]   I have bad experiences with crypto wallets, as you know.
[00:08:23.360 --> 00:08:26.280]   So yeah, we won't we won't remind you.
[00:08:26.280 --> 00:08:26.840]   Yeah, I have.
[00:08:26.840 --> 00:08:30.800]   Actually, that's one of the places the tip chart went was that Bitcoin wallet.
[00:08:30.800 --> 00:08:33.840]   That's why there's so no state bitcoins in there.
[00:08:33.840 --> 00:08:39.320]   Well, to be fair, the last contribution was in 2015 when Bitcoin was pennies.
[00:08:39.320 --> 00:08:41.520]   So it wasn't like people were giving me big bucks.
[00:08:41.520 --> 00:08:43.240]   All right.
[00:08:43.240 --> 00:08:44.720]   We don't use that Bitcoin.
[00:08:44.720 --> 00:08:53.720]   Well, no, nobody is flying to join Google's flock says the verge.
[00:08:54.240 --> 00:08:57.280]   Firefox has already said we're not going to do it.
[00:08:57.280 --> 00:08:59.320]   Apple Safari won't do it.
[00:08:59.320 --> 00:09:06.360]   Mozilla, Vivaldi, a brave all out in this article by Deeter Boney says edge.
[00:09:06.360 --> 00:09:14.280]   But I have to say, when they asked Microsoft about edge, their response was very
[00:09:14.280 --> 00:09:16.320]   Equivocal.
[00:09:16.320 --> 00:09:21.120]   In fact, to the point where I feel like Microsoft was hedging their bets and
[00:09:21.120 --> 00:09:22.640]   said, yeah, we're going to wait and see it.
[00:09:22.640 --> 00:09:23.800]   We might well use flock.
[00:09:23.800 --> 00:09:31.240]   Just as a point of information, Steve Gibson did a very about half an hour on
[00:09:31.240 --> 00:09:34.080]   flock yesterday on security now, which I encourage people to listen to.
[00:09:34.080 --> 00:09:35.080]   It's at the end of the show.
[00:09:35.080 --> 00:09:41.800]   He's he says it's a good idea and it works as advertised that it is privacy
[00:09:41.800 --> 00:09:45.720]   protecting and it's very cleverly implemented to do that.
[00:09:45.720 --> 00:09:51.720]   I pointed out it's, of course, Google, you know, has to make advertising work.
[00:09:52.640 --> 00:09:57.520]   So Google is for itself and many, many others.
[00:09:57.520 --> 00:10:02.680]   So Google's kind of got to do the best it can to reassure people on privacy, but
[00:10:02.680 --> 00:10:04.880]   still they have to give something to advertisers.
[00:10:04.880 --> 00:10:06.120]   That's what I was just talking about.
[00:10:06.120 --> 00:10:11.600]   And what Google did point out is that there are a number of categories that they
[00:10:11.600 --> 00:10:18.240]   don't sell ads against, including things like your mental state, your previous
[00:10:18.240 --> 00:10:22.200]   incarceration, your whether you're pregnant, your health, any health conditions.
[00:10:22.440 --> 00:10:26.320]   They don't sell ads against those in flock will not add those features to any of
[00:10:26.320 --> 00:10:27.000]   its cohorts.
[00:10:27.000 --> 00:10:30.480]   So that might have been a little bit reassuring.
[00:10:30.480 --> 00:10:33.960]   Precious is interesting because that's highly lucrative.
[00:10:33.960 --> 00:10:38.640]   Well, if you look at what they won't sell, I mean, liquor, right?
[00:10:38.640 --> 00:10:44.160]   The stuff that Google won't do ads for is actually a list of stuff that people buy ads.
[00:10:44.160 --> 00:10:45.600]   Ads for.
[00:10:45.600 --> 00:10:52.080]   So I think this is why I feel like Google might be a little bit sincere in that
[00:10:52.080 --> 00:10:54.200]   they're doing to reinvent advertised.
[00:10:54.200 --> 00:10:54.400]   Yes.
[00:10:54.400 --> 00:10:56.840]   We need to get past what it was.
[00:10:56.840 --> 00:11:01.080]   And if we're going to shoot down everything that comes up, just because it came from Google.
[00:11:01.080 --> 00:11:05.520]   No, they're not shooting it down because it came from Google originally.
[00:11:05.520 --> 00:11:12.800]   And E F F shot it down originally because they want to stop the web being built on
[00:11:12.800 --> 00:11:17.640]   an ad supported model that I mean, that that's their ideas that your data should
[00:11:17.640 --> 00:11:19.480]   not be your payment for serving the web.
[00:11:19.480 --> 00:11:20.920]   We should come up with something better.
[00:11:20.920 --> 00:11:23.680]   And it might be something like, I mean, what we're doing.
[00:11:23.680 --> 00:11:25.440]   Well, I'm not Twitter.
[00:11:25.440 --> 00:11:28.040]   OK, so we have a world where everything's behind a paywall.
[00:11:28.040 --> 00:11:33.040]   I wrote a tweet about this last week, but my dystopia is that everything goes
[00:11:33.040 --> 00:11:37.880]   behind a paywall and major media and that everybody has a sub stack.
[00:11:37.880 --> 00:11:40.040]   And the only thing that's free is disinformation.
[00:11:40.040 --> 00:11:42.400]   With crappy.
[00:11:42.400 --> 00:11:44.360]   I don't think I think that's a little crazy.
[00:11:44.360 --> 00:11:47.600]   I think what's going to I mean, like I tweeted just the other day.
[00:11:47.600 --> 00:11:50.960]   I realized I subscribed to 11 publications, which is insane.
[00:11:50.960 --> 00:11:52.880]   I spend about $1,000 a year.
[00:11:52.880 --> 00:11:53.440]   Thank you.
[00:11:53.440 --> 00:11:54.400]   Very, thank you.
[00:11:54.400 --> 00:11:55.600]   Yeah, but yeah, thank you.
[00:11:55.600 --> 00:11:56.160]   No, I know.
[00:11:56.160 --> 00:11:57.960]   I mean, it's it's my job.
[00:11:57.960 --> 00:12:02.880]   I mean, I do it because I run into paywalls all the time and but it is frustrating also
[00:12:02.880 --> 00:12:03.680]   to try to.
[00:12:03.680 --> 00:12:04.400]   I'm like you.
[00:12:04.400 --> 00:12:06.960]   Like, yeah, I pay for stuff to people to content.
[00:12:06.960 --> 00:12:08.400]   Yeah, I pay for more stuff.
[00:12:08.400 --> 00:12:12.040]   I know they won't be able to read Bloomberg any money, except that I have to use
[00:12:12.040 --> 00:12:14.240]   Bloomberg's materials on our shows.
[00:12:14.240 --> 00:12:15.880]   I I give I give.
[00:12:15.880 --> 00:12:19.240]   It's 35 months and they have the cash darn Bloomberg terminal.
[00:12:19.240 --> 00:12:23.520]   They make a lot of money on Reuters is going to charge that much, which is ridiculous.
[00:12:23.520 --> 00:12:24.000]   Yeah.
[00:12:24.000 --> 00:12:25.000]   Sorry, Reuters.
[00:12:25.000 --> 00:12:27.800]   I like Reuters, but so so go ahead.
[00:12:27.800 --> 00:12:27.960]   Stay.
[00:12:27.960 --> 00:12:31.680]   But I think what's going to happen is we're going to reach a point where people
[00:12:31.680 --> 00:12:35.800]   start hitting all these paywalls and they experience the pain of that.
[00:12:35.800 --> 00:12:39.680]   And we're going to come up with new models because they're going to be forced.
[00:12:39.680 --> 00:12:44.720]   Remember, I mean, we always this is what competition does.
[00:12:44.720 --> 00:12:48.920]   It creates you get this like pain point and then people will come out with like
[00:12:48.920 --> 00:12:53.840]   aggregators that are like of some sort and we'll get to that point.
[00:12:53.840 --> 00:12:56.560]   It's constantly evolving and that's good.
[00:12:56.560 --> 00:12:57.720]   That's what the web is.
[00:12:57.720 --> 00:13:02.040]   But that's exactly why we never want to get rid of the free tier because I don't
[00:13:02.040 --> 00:13:05.200]   want to put everything I don't I didn't want to do a paywall per se.
[00:13:05.200 --> 00:13:08.720]   Now, maybe that's crazy because this is this is patronage.
[00:13:08.720 --> 00:13:09.560]   This is support.
[00:13:09.560 --> 00:13:10.320]   This is help.
[00:13:10.320 --> 00:13:10.840]   Yeah.
[00:13:10.840 --> 00:13:11.720]   Fans.
[00:13:11.720 --> 00:13:14.600]   But I know as people will be involved, financial times, for instance,
[00:13:14.600 --> 00:13:17.600]   you can't read their content, unless you pay Wall Street Journal.
[00:13:17.600 --> 00:13:22.320]   You can read some limited content, but not all of it, unless you pay new, you know,
[00:13:22.320 --> 00:13:25.360]   paywalls are definitely a thing.
[00:13:25.360 --> 00:13:31.280]   Is that because this idea of of freemium doesn't work?
[00:13:31.280 --> 00:13:34.480]   No, because it's a winner to call market.
[00:13:34.480 --> 00:13:38.240]   One of my numbers below, which I will now sacrifice for the good of the show.
[00:13:38.240 --> 00:13:38.880]   Thank you, sir.
[00:13:38.880 --> 00:13:46.160]   Is a view of all of the news sites that have over a hundred thousand subscribers.
[00:13:46.160 --> 00:13:51.160]   There's 28 of them adding up to 23 million worldwide, the English speaking world.
[00:13:51.160 --> 00:13:53.000]   That's a fraction, though, of the total.
[00:13:53.000 --> 00:13:53.680]   It's nothing.
[00:13:53.680 --> 00:13:56.520]   It's nothing in the US.
[00:13:56.520 --> 00:14:02.280]   63% of all subs go to three, count them three publications.
[00:14:02.280 --> 00:14:03.120]   You know what they are.
[00:14:03.120 --> 00:14:03.800]   You're not your time.
[00:14:03.800 --> 00:14:04.080]   You're both.
[00:14:04.080 --> 00:14:04.680]   Yeah.
[00:14:04.680 --> 00:14:05.080]   Yeah.
[00:14:05.080 --> 00:14:05.160]   Yeah.
[00:14:05.160 --> 00:14:06.480]   And I am so it's a winner.
[00:14:06.480 --> 00:14:07.280]   Take all model.
[00:14:07.280 --> 00:14:08.160]   What was the third one?
[00:14:08.160 --> 00:14:10.760]   Wall Street Journal, the first times in Washington Post.
[00:14:10.760 --> 00:14:11.520]   Yeah.
[00:14:11.520 --> 00:14:12.240]   Oh, I have so.
[00:14:12.240 --> 00:14:12.920]   Yeah.
[00:14:12.920 --> 00:14:13.440]   So do I.
[00:14:13.440 --> 00:14:13.680]   Yeah.
[00:14:13.680 --> 00:14:13.960]   Yeah.
[00:14:13.960 --> 00:14:14.440]   Of course.
[00:14:14.440 --> 00:14:14.880]   Yeah.
[00:14:14.880 --> 00:14:17.440]   So what I don't subscribe to any of those.
[00:14:17.440 --> 00:14:19.680]   You're the con of man.
[00:14:19.680 --> 00:14:20.800]   You're the real man.
[00:14:20.800 --> 00:14:27.840]   Well, but see, the thing is I totally get having these paywalls up because ideally
[00:14:27.840 --> 00:14:31.360]   they're going to have legit content out there and legit information.
[00:14:31.360 --> 00:14:35.160]   But there's a lot of times that I've been able to click on some of these
[00:14:35.160 --> 00:14:40.400]   stories and get a partial view of whatever they're showing before hitting the paywall.
[00:14:40.400 --> 00:14:42.200]   And I'm like, this is bad.
[00:14:42.200 --> 00:14:43.520]   And it just doesn't.
[00:14:43.520 --> 00:14:46.880]   But and I'll bring up this.
[00:14:46.880 --> 00:14:47.840]   There is a hazard.
[00:14:47.840 --> 00:14:50.480]   Knox Harrington and our IRC chat brings us up.
[00:14:50.480 --> 00:14:56.600]   If all the good content sites are paywalled, what will be left for for normal
[00:14:56.600 --> 00:15:00.640]   unpaying people to pay to breed is link bait and crap.
[00:15:00.640 --> 00:15:01.280]   Exactly.
[00:15:01.480 --> 00:15:06.160]   But I'm seeing I'm seeing link bait from some of these reputable sources, too.
[00:15:06.160 --> 00:15:06.520]   Oh, yeah.
[00:15:06.520 --> 00:15:10.120]   And that's why I'm like, you know, why are you bother?
[00:15:10.120 --> 00:15:12.360]   It's not all crap.
[00:15:12.360 --> 00:15:13.520]   I mean, mine's free.
[00:15:13.520 --> 00:15:14.680]   It's more trade though.
[00:15:14.680 --> 00:15:19.120]   I get a lot of people who are cranky that my stuff is too technical for them.
[00:15:19.120 --> 00:15:20.600]   Well, too bad.
[00:15:20.600 --> 00:15:22.800]   I mean, that's what you do.
[00:15:22.800 --> 00:15:23.520]   What is the point?
[00:15:23.520 --> 00:15:24.560]   What today is the point?
[00:15:24.560 --> 00:15:26.480]   People are just cranky.
[00:15:26.480 --> 00:15:28.240]   I got to say, people are cranky about anything.
[00:15:28.240 --> 00:15:29.080]   They'll complain about anything.
[00:15:29.080 --> 00:15:31.440]   If you're not text, you know, not you'd get just as maybe
[00:15:31.440 --> 00:15:33.360]   people saying you need to be more technical.
[00:15:33.360 --> 00:15:36.720]   Yes, I get each week.
[00:15:36.720 --> 00:15:40.120]   I get someone telling me I'm too technical and I get someone saying, thank
[00:15:40.120 --> 00:15:41.280]   you for explaining things so clear.
[00:15:41.280 --> 00:15:41.800]   Exactly.
[00:15:41.800 --> 00:15:43.360]   So I take it all with a.
[00:15:43.360 --> 00:15:45.760]   OK, you can't please everyone.
[00:15:45.760 --> 00:15:51.720]   But I will say there will always be I do think there's always room for I'll
[00:15:51.720 --> 00:15:59.720]   call it a trade like publication because the people like my advertisers are happy
[00:15:59.720 --> 00:16:05.120]   to reach the people who are legitimately focused on my area.
[00:16:05.120 --> 00:16:08.240]   And I think there's areas where that model will work.
[00:16:08.240 --> 00:16:12.200]   I think the narrower that it breaks down, the narrower your niche and you
[00:16:12.200 --> 00:16:13.440]   have a very narrow niche.
[00:16:13.440 --> 00:16:15.480]   The easier it is to do that.
[00:16:15.480 --> 00:16:20.880]   I think if you're general, it's harder, right?
[00:16:20.880 --> 00:16:23.720]   Because you don't have a natural audience.
[00:16:23.720 --> 00:16:25.520]   Yeah.
[00:16:25.520 --> 00:16:27.240]   That's how you have a natural audience.
[00:16:27.240 --> 00:16:28.680]   It's just they're not lucrative.
[00:16:28.680 --> 00:16:30.720]   They're just like the everybody.
[00:16:30.720 --> 00:16:31.240]   Yeah.
[00:16:31.240 --> 00:16:31.720]   Yeah.
[00:16:31.720 --> 00:16:31.960]   Right.
[00:16:31.960 --> 00:16:35.520]   Well, consider what is considered a frequent user of the New York Times.
[00:16:35.520 --> 00:16:39.720]   They had to lower it from 20 pages a month to 10 pages a month to five or three.
[00:16:39.720 --> 00:16:40.680]   Wow.
[00:16:40.680 --> 00:16:42.640]   That means that means people who are paying.
[00:16:42.640 --> 00:16:45.760]   You know, they can't get them to convert.
[00:16:45.760 --> 00:16:51.200]   And they only come three to five times a month, three to five articles a month.
[00:16:51.200 --> 00:16:52.040]   Yeah.
[00:16:52.040 --> 00:16:53.320]   I'm reading.
[00:16:53.320 --> 00:16:54.000]   I'm stunned.
[00:16:54.000 --> 00:16:54.680]   There's all I can do.
[00:16:54.680 --> 00:16:56.880]   I happen to have in my hand.
[00:16:56.880 --> 00:16:58.480]   Oh, what is this?
[00:16:58.480 --> 00:17:00.200]   Professor, Senator McCarthy.
[00:17:00.200 --> 00:17:00.680]   Go ahead.
[00:17:00.680 --> 00:17:01.440]   Excellent paper.
[00:17:01.440 --> 00:17:01.800]   Yeah.
[00:17:01.800 --> 00:17:08.600]   Three great researchers Duncan Watts, David Rothschild and Marcus Mobius just published.
[00:17:08.600 --> 00:17:11.760]   And it's about fake news.
[00:17:11.760 --> 00:17:17.960]   And they found that from 2017, there were 5,000 English language publications,
[00:17:17.960 --> 00:17:19.800]   academic papers about fake news.
[00:17:19.800 --> 00:17:24.040]   So what they're saying is, I think it's maybe a little overblown and they and what
[00:17:24.040 --> 00:17:24.960]   the research we need.
[00:17:24.960 --> 00:17:26.400]   But here's some data in this.
[00:17:26.920 --> 00:17:30.120]   News consumption is a small fraction of overall media consumption.
[00:17:30.120 --> 00:17:36.960]   14% of overall news, media consumption is dedicated to news.
[00:17:36.960 --> 00:17:37.720]   Right.
[00:17:37.720 --> 00:17:38.400]   14%.
[00:17:38.400 --> 00:17:46.080]   Less than 1% of regular news consumption and less than one tenth of 1% of overall
[00:17:46.080 --> 00:17:48.000]   media consumption can be considered fake.
[00:17:48.000 --> 00:17:53.960]   Three and four Americans spend less than 30 seconds a day reading news online,
[00:17:54.040 --> 00:17:58.120]   while almost half consumed no online news whatsoever.
[00:17:58.120 --> 00:17:59.080]   Mm.
[00:17:59.080 --> 00:17:59.880]   So we're weird.
[00:17:59.880 --> 00:18:02.120]   We are really, we are.
[00:18:02.120 --> 00:18:04.960]   Well, it's interesting because, well, as you mentioned, Reuters just went
[00:18:04.960 --> 00:18:05.960]   behind a paywall.
[00:18:05.960 --> 00:18:10.600]   Their plan is to give you five free articles, then it's 35 bucks a month,
[00:18:10.600 --> 00:18:14.640]   but they want to quote court business professionals.
[00:18:14.640 --> 00:18:17.440]   So that's the key, I think.
[00:18:17.440 --> 00:18:21.080]   You if you're going to go behind a paywall, you have to find.
[00:18:21.080 --> 00:18:23.880]   And that's why the Wall Street Journal, which I think was the first to do it.
[00:18:24.240 --> 00:18:27.880]   And financial times have done it successfully because people who read those
[00:18:27.880 --> 00:18:30.000]   papers could sit afforded it.
[00:18:30.000 --> 00:18:33.600]   They can well, not only can they afford it, they consider money well spent.
[00:18:33.600 --> 00:18:36.720]   The information they get helps them make money, right?
[00:18:36.720 --> 00:18:37.360]   Yeah.
[00:18:37.360 --> 00:18:41.360]   So that it's just a logical thing to spend 38 bucks a month or 39 bucks a month.
[00:18:41.360 --> 00:18:43.480]   And they're not buying content and they're buying the speed.
[00:18:43.480 --> 00:18:47.120]   They're buying speed and I got this before somebody also knowledge.
[00:18:47.120 --> 00:18:47.480]   Yeah.
[00:18:47.480 --> 00:18:48.080]   Right.
[00:18:48.120 --> 00:18:54.960]   And for me, they're also buying, I think the knowledge and I would say the insights.
[00:18:54.960 --> 00:18:56.880]   I mean, like I read CIO journal.
[00:18:56.880 --> 00:18:59.200]   I don't read a lot of the Wall Street Journal main stories, but I do.
[00:18:59.200 --> 00:19:00.280]   I'm a fun person.
[00:19:00.280 --> 00:19:03.480]   I am a fun person.
[00:19:03.480 --> 00:19:03.960]   You are.
[00:19:03.960 --> 00:19:05.200]   Well, and I bet you don't know.
[00:19:05.200 --> 00:19:06.680]   This is fun.
[00:19:06.680 --> 00:19:09.840]   Is CIO journal a closed-circ publication?
[00:19:09.840 --> 00:19:10.680]   I bet it is.
[00:19:10.680 --> 00:19:11.880]   Well, no, sorry.
[00:19:11.880 --> 00:19:14.080]   That's the CIO journal as part of the Wall Street Journal.
[00:19:14.080 --> 00:19:16.400]   Oh, because they have a little segment.
[00:19:16.400 --> 00:19:22.000]   It would be this category that you had to qualify by showing that you were, you know, a C-level
[00:19:22.000 --> 00:19:26.320]   executive or an IT professional and then you would get these closed circulation things
[00:19:26.320 --> 00:19:30.240]   like information weekly information week is a perfect example.
[00:19:30.240 --> 00:19:30.760]   Yeah.
[00:19:30.760 --> 00:19:33.520]   I don't know if those even still around because that was another.
[00:19:33.520 --> 00:19:35.200]   They're on my own.
[00:19:35.200 --> 00:19:36.600]   They're still around.
[00:19:36.600 --> 00:19:37.080]   OK.
[00:19:37.080 --> 00:19:38.800]   So that was another model.
[00:19:38.800 --> 00:19:38.960]   Yeah.
[00:19:38.960 --> 00:19:40.920]   And you could get it because you're an IT.
[00:19:40.920 --> 00:19:42.840]   Yeah, I did enjoy that one.
[00:19:42.840 --> 00:19:43.120]   Yeah.
[00:19:43.120 --> 00:19:43.840]   I got PC.
[00:19:43.840 --> 00:19:45.760]   Those were ads supported.
[00:19:46.120 --> 00:19:47.560]   Because they wanted to.
[00:19:47.560 --> 00:19:49.760]   They were closed.
[00:19:49.760 --> 00:19:51.560]   Yeah, well, because but it was the right.
[00:19:51.560 --> 00:19:56.160]   Yeah, because they could tell people like, yeah, if you're right in front of all the
[00:19:56.160 --> 00:19:56.680]   IT pros, right?
[00:19:56.680 --> 00:19:57.480]   Just like to a can.
[00:19:57.480 --> 00:20:00.520]   If you're buying an ad on to it, you're no, you're speaking to tech enthusiasts.
[00:20:00.520 --> 00:20:02.440]   But here's one more number here.
[00:20:02.440 --> 00:20:08.280]   So the all the publications they could find that had 100,000 or more subscribers was 28
[00:20:08.280 --> 00:20:11.200]   publications in the entire English speaking world adding up to 23 million
[00:20:11.200 --> 00:20:12.360]   subscribe subscriptions.
[00:20:12.360 --> 00:20:14.480]   Amazon Prime 200 million.
[00:20:15.720 --> 00:20:16.720]   I know.
[00:20:16.720 --> 00:20:18.080]   Well, one's clearly more important.
[00:20:18.080 --> 00:20:21.720]   But someone is someone in the chat is explaining my free dose.
[00:20:21.720 --> 00:20:27.720]   If you if you have Amazon Prime, you get the Washington Post at only three
[00:20:27.720 --> 00:20:30.440]   ninety nine a month, right, which is a great deal.
[00:20:30.440 --> 00:20:33.840]   And you get a lot of free content on Amazon Prime video.
[00:20:33.840 --> 00:20:38.160]   You get, of course, overnight to deliver rather than that's and that's the initial
[00:20:38.160 --> 00:20:39.240]   reason for Amazon Prime.
[00:20:39.240 --> 00:20:42.400]   They've managed to make that a pretty valuable thing, but it's also expensive.
[00:20:42.400 --> 00:20:43.800]   It's it's 10 bucks a month.
[00:20:44.680 --> 00:20:48.320]   It's not just a normal person, as you were saying earlier.
[00:20:48.320 --> 00:20:54.520]   The problem I have with a lot of these different big time news publications is
[00:20:54.520 --> 00:20:55.560]   trust level.
[00:20:55.560 --> 00:21:00.200]   They all tend to come off having their own agendas at times.
[00:21:00.200 --> 00:21:03.160]   That's why you have to buy all of them.
[00:21:03.160 --> 00:21:04.040]   Right.
[00:21:04.040 --> 00:21:07.760]   And then that's the thing you end up going back and forth between three or four
[00:21:07.760 --> 00:21:10.640]   different providers of the same story.
[00:21:10.640 --> 00:21:13.720]   And it's like, she's why am I paying for all of this?
[00:21:13.720 --> 00:21:18.000]   And neither one of them are saying the same thing or or or even getting all
[00:21:18.000 --> 00:21:18.800]   of the truth out.
[00:21:18.800 --> 00:21:20.200]   You know, it's it's pretty bad.
[00:21:20.200 --> 00:21:20.920]   I'm frustrated.
[00:21:20.920 --> 00:21:25.000]   That's because a lot of times why I just I don't even bother with news
[00:21:25.000 --> 00:21:29.440]   consumption at all, because it's probably a bunch of crap that I'm going to get.
[00:21:29.440 --> 00:21:33.760]   This is this is why I say you're the normal person.
[00:21:33.760 --> 00:21:35.000]   That's I think the normal.
[00:21:35.000 --> 00:21:36.360]   In a democracy.
[00:21:36.360 --> 00:21:39.240]   You should have to.
[00:21:39.240 --> 00:21:41.160]   I mean, you have to read things.
[00:21:41.160 --> 00:21:44.520]   You have to absorb knowledge and decide where you stand on.
[00:21:44.520 --> 00:21:47.360]   I mean, that to me sounds like what you should do as a citizen.
[00:21:47.360 --> 00:21:48.440]   You and I agree.
[00:21:48.440 --> 00:21:50.160]   You and I agree.
[00:21:50.160 --> 00:21:56.360]   But I think that what if you if you hear what I'm saying, you hear that from
[00:21:56.360 --> 00:21:58.920]   every of the vast majority of people, I know.
[00:21:58.920 --> 00:22:01.880]   And I mean, maybe that is a problem.
[00:22:01.880 --> 00:22:05.280]   I think there's it's a problem on this.
[00:22:05.280 --> 00:22:09.160]   Also, the creation guys who create the content as much as it is.
[00:22:09.160 --> 00:22:11.200]   It's what makes us elites is that we.
[00:22:11.200 --> 00:22:13.320]   Yeah, we're elites because we read newspapers.
[00:22:13.320 --> 00:22:16.040]   I'm going to say something that's not going to be popular.
[00:22:16.040 --> 00:22:23.000]   I'm going to say we've had this this stuff going on with Chauvin and the other
[00:22:23.000 --> 00:22:25.600]   police killings here recently.
[00:22:25.600 --> 00:22:30.920]   But fact of the matter, there's just as many white people being killed by
[00:22:30.920 --> 00:22:34.600]   police officers that shouldn't be and none of that stuff gets reported.
[00:22:34.600 --> 00:22:36.320]   You know, that's that's still news.
[00:22:36.320 --> 00:22:42.320]   The news story should be that we have some poorly trained police officers.
[00:22:42.320 --> 00:22:45.400]   Yeah. Yeah. Yeah, I agree.
[00:22:45.400 --> 00:22:49.440]   I mean, there's a higher percentage who New Jersey people getting killed,
[00:22:49.440 --> 00:22:51.920]   arrest kids for not having bicycle licenses.
[00:22:51.920 --> 00:22:52.880]   Yeah. Right.
[00:22:52.880 --> 00:22:56.120]   Yeah. Who pull over somebody for an expired tag.
[00:22:56.120 --> 00:22:58.040]   All these reasons people are pulled over.
[00:22:58.040 --> 00:23:01.400]   But every story, OK, this is our change.
[00:23:01.400 --> 00:23:04.680]   But I also should point out and this is another problem.
[00:23:04.680 --> 00:23:06.320]   Every story has multiple angles.
[00:23:06.320 --> 00:23:07.480]   There's another angle.
[00:23:07.480 --> 00:23:12.640]   90% whatever of police officers are hardworking, care a lot,
[00:23:12.640 --> 00:23:15.920]   try to be fair and honest and they're not killing people.
[00:23:15.920 --> 00:23:20.760]   So so my point is not being to defend police officers or not,
[00:23:20.760 --> 00:23:25.800]   but that there are angles into all of these stories and there's a and that's
[00:23:25.800 --> 00:23:29.880]   why and I think Stacy and I try to re I'm spend.
[00:23:29.880 --> 00:23:34.120]   Probably five hours a day consuming news content.
[00:23:34.520 --> 00:23:38.120]   Some of it general, most of it, a lot of it for the shows, but still,
[00:23:38.120 --> 00:23:42.520]   it's a job to keep up on this stuff.
[00:23:42.520 --> 00:23:45.320]   And I you don't go home and watch the news in the evening.
[00:23:45.320 --> 00:23:52.600]   No, I don't watch the news, but I do try to check check out things like tech
[00:23:52.600 --> 00:23:56.040]   mean, it's going to give me a nice aggregate and yeah, that's part of my
[00:23:56.040 --> 00:23:58.320]   do try to check stuff out like that. Yeah.
[00:23:58.320 --> 00:24:00.400]   Yeah. Because it is my job.
[00:24:00.400 --> 00:24:03.920]   I do want to come on to this show each and every Wednesday and
[00:24:03.920 --> 00:24:06.960]   in Italy sound like I know what the hell I'm talking about.
[00:24:06.960 --> 00:24:11.800]   But a lot of times it's still quite challenging and frustrating to see the
[00:24:11.800 --> 00:24:17.400]   different ridiculous headlines that have nothing to do with the story or vice versa.
[00:24:17.400 --> 00:24:18.720]   Yeah. Yeah.
[00:24:18.720 --> 00:24:20.640]   It's a it's a good.
[00:24:20.640 --> 00:24:22.040]   I just got this book.
[00:24:22.040 --> 00:24:24.480]   And what is this for?
[00:24:24.480 --> 00:24:26.320]   We're prime minister of Australia. Kevin Rotten.
[00:24:26.320 --> 00:24:28.080]   Yeah. Called the case for courage.
[00:24:28.080 --> 00:24:31.360]   I spent 40 bucks to get it delivered to me to Australia.
[00:24:31.360 --> 00:24:33.120]   But I really want to know.
[00:24:33.120 --> 00:24:35.840]   Is that pay now? This is this is about his campaign against
[00:24:35.840 --> 00:24:38.000]   against Murdoch particularly. That's a paywall. Yeah.
[00:24:38.000 --> 00:24:41.440]   I guess Murdoch and he's talking about Murdoch and news
[00:24:41.440 --> 00:24:45.040]   corporal specifically in Australia where they have a monopoly in some places on news.
[00:24:45.040 --> 00:24:47.200]   But he gives a description of what news is right.
[00:24:47.200 --> 00:24:48.880]   I think is what Anne's talking about here.
[00:24:48.880 --> 00:24:52.960]   He said, I worry because we're becoming a nation driven by anxiety, fear and anger
[00:24:52.960 --> 00:24:54.960]   at times bordering on an uncontrollable rage.
[00:24:54.960 --> 00:24:59.200]   These are deeply disabling emotions that crowd out the space in our nation's mind
[00:24:59.200 --> 00:25:00.880]   and imagination for almost anything else.
[00:25:01.440 --> 00:25:04.320]   They overwhelm our natural sense of optimism, enterprise, generosity,
[00:25:04.320 --> 00:25:07.920]   a spirit, transform in us instead into a frightened and fractured people,
[00:25:07.920 --> 00:25:11.360]   a nation of us versus them and everybody else.
[00:25:11.360 --> 00:25:17.040]   And what he the point he makes about Murdoch, but it's true of all media,
[00:25:17.040 --> 00:25:20.480]   all big media is that they traffic in fear.
[00:25:20.480 --> 00:25:24.080]   The attention based business model, which is why we have to change advertising,
[00:25:24.080 --> 00:25:25.120]   getting back to that.
[00:25:25.120 --> 00:25:26.720]   If all you're going to do is sell attention.
[00:25:27.680 --> 00:25:32.480]   And then the problem is that the way you get attention is by scarier than a lot of people.
[00:25:32.480 --> 00:25:36.240]   And that's not good for the country, either one.
[00:25:36.240 --> 00:25:41.920]   Do you know the interesting thing is I think sometimes it's thought that Murdoch is
[00:25:41.920 --> 00:25:44.560]   doing it for political reasons.
[00:25:44.560 --> 00:25:49.280]   And I think probably he would do whatever makes him the most money.
[00:25:49.280 --> 00:25:50.160]   Do you think, Jeff?
[00:25:50.160 --> 00:25:52.160]   Yeah, it's money for him.
[00:25:52.160 --> 00:25:55.120]   It's not about politics for him.
[00:25:56.480 --> 00:25:59.840]   But fear and conservatism, which is one of Rod's points kind of going together.
[00:25:59.840 --> 00:26:03.600]   Fear and conservative conservatism.
[00:26:03.600 --> 00:26:09.600]   But I also think it is incumbent upon the journalist and the papers to make things matter to people.
[00:26:09.600 --> 00:26:15.440]   I mean, our job is one to like, try to deeply understand issues and explain them clearly to
[00:26:15.440 --> 00:26:16.080]   people.
[00:26:16.080 --> 00:26:18.800]   But it's also to make them care about something.
[00:26:18.800 --> 00:26:21.760]   Fear is the easiest way to make someone care about something, right?
[00:26:22.560 --> 00:26:28.480]   But there's also, if you look at good healthcare reporting, that is an arcane system that just
[00:26:28.480 --> 00:26:29.840]   grinds people up like meat.
[00:26:29.840 --> 00:26:37.040]   You always have to find the reason to make someone care about it.
[00:26:37.040 --> 00:26:39.600]   And usually it's someone injured or say,
[00:26:39.600 --> 00:26:42.960]   There's a fine line between that and being people.
[00:26:42.960 --> 00:26:44.000]   Isn't that a fine line?
[00:26:44.000 --> 00:26:44.320]   Yeah.
[00:26:44.320 --> 00:26:44.880]   That's the thing.
[00:26:44.880 --> 00:26:47.760]   I mean, you're exactly right, Stacy.
[00:26:47.760 --> 00:26:51.760]   I know whenever you write a story, that's probably a big part of what you're considering
[00:26:51.760 --> 00:26:57.200]   as you write it is how do I communicate, this is important and you should pay attention to this.
[00:26:57.200 --> 00:26:58.000]   Why this matters?
[00:26:58.000 --> 00:26:58.960]   Why this matters.
[00:26:58.960 --> 00:27:04.720]   But there's a fine line between that and saying, here's 10 things you have to do before you go to
[00:27:04.720 --> 00:27:05.200]   bed tonight.
[00:27:05.200 --> 00:27:07.920]   So it's interesting.
[00:27:07.920 --> 00:27:12.560]   I can see how people might accidentally really veer into link bait simply because they just
[00:27:12.560 --> 00:27:14.480]   want to get people to care about what they're writing about.
[00:27:14.480 --> 00:27:15.120]   Yep.
[00:27:15.120 --> 00:27:16.480]   If people don't care, they're not going to click.
[00:27:16.480 --> 00:27:18.800]   It's very challenging.
[00:27:18.800 --> 00:27:20.320]   Say the same thing about thumbnails.
[00:27:20.880 --> 00:27:21.280]   Yeah.
[00:27:21.280 --> 00:27:22.400]   For people who don't have a good thumbnail.
[00:27:22.400 --> 00:27:23.120]   Yeah.
[00:27:23.120 --> 00:27:26.240]   And by the way, what are your thumbnails doing?
[00:27:26.240 --> 00:27:32.000]   That, by the way, is why if you go through YouTube, all the thumbnails right now are reactions.
[00:27:32.000 --> 00:27:35.040]   They look the same and I can't stand it.
[00:27:35.040 --> 00:27:36.000]   I can't either.
[00:27:36.000 --> 00:27:36.640]   But it works.
[00:27:36.640 --> 00:27:37.600]   It's working.
[00:27:37.600 --> 00:27:39.520]   You know, that's what people want to click on.
[00:27:39.520 --> 00:27:40.080]   So really?
[00:27:40.080 --> 00:27:41.200]   We don't have any of that stuff.
[00:27:41.200 --> 00:27:42.560]   The fault is in our stars.
[00:27:42.560 --> 00:27:48.480]   The fault is in who we are as human beings and the things that grab our attention.
[00:27:49.600 --> 00:27:57.200]   And we're kind of a fundamentally kind of basic system.
[00:27:57.200 --> 00:27:58.160]   That is easy to game.
[00:27:58.160 --> 00:28:04.720]   Well, funny thing, when Eli Parrazier, author of The Filter Bubble, which I argue is not the
[00:28:04.720 --> 00:28:08.240]   case that we did really not talk about Filter Bubbles, but he started up worthy.
[00:28:08.240 --> 00:28:09.680]   It was with a good motive.
[00:28:09.680 --> 00:28:12.880]   It was saying, "I want to draw people's attention to news that matters."
[00:28:12.880 --> 00:28:16.800]   But by going with the data, what they ended up inventing was the,
[00:28:16.800 --> 00:28:18.720]   and you won't believe what happened next to them.
[00:28:18.720 --> 00:28:19.520]   Yeah.
[00:28:19.520 --> 00:28:19.840]   Right.
[00:28:19.840 --> 00:28:25.680]   And they taught all of media then how to do that for not good purposes until we all got sick of it.
[00:28:25.680 --> 00:28:26.720]   Of course, then it goes away.
[00:28:26.720 --> 00:28:33.280]   But even in his good motive, he was trying to use the tools that then get misused.
[00:28:33.280 --> 00:28:35.040]   So let me show you these.
[00:28:35.040 --> 00:28:36.640]   And this might reassure you, Stacey.
[00:28:36.640 --> 00:28:38.320]   These are, we're talking about flock.
[00:28:38.320 --> 00:28:44.160]   And one of the things Google has said is, one of the concerns that Vivaldi came up with, for
[00:28:44.160 --> 00:28:49.040]   instance, is that the way flock works is based on one week of your browsing.
[00:28:49.040 --> 00:28:51.040]   It changes every week.
[00:28:51.040 --> 00:28:56.400]   You're put into a category of interests with other people who share those interests.
[00:28:56.400 --> 00:29:00.560]   That category, which is never identified by a name, it's just numerically identified,
[00:29:00.560 --> 00:29:04.240]   then can be used by advertisers who can say, "You know, it's interesting.
[00:29:04.240 --> 00:29:10.240]   Group 13974 was very successful for us when 13221 wasn't.
[00:29:10.240 --> 00:29:12.400]   So let's buy more ads to 13974."
[00:29:12.400 --> 00:29:13.840]   That's the theory behind it.
[00:29:13.840 --> 00:29:14.960]   It's anonymized.
[00:29:14.960 --> 00:29:19.760]   And the advertiser really doesn't even know what segment they're buying.
[00:29:19.760 --> 00:29:27.440]   But Vivaldi's concern was, yeah, but in a way you do know, because if all the people who did a search
[00:29:27.440 --> 00:29:35.360]   for ketoacidosis on the web are in this group, that tells you that those people in that group
[00:29:35.360 --> 00:29:36.800]   had a medical condition.
[00:29:36.800 --> 00:29:41.680]   Google is saying no, very specifically about medical conditions.
[00:29:41.680 --> 00:29:47.680]   But in general, we don't sell advertising and will not have flock categories based on this content.
[00:29:47.680 --> 00:29:48.800]   I have the page here.
[00:29:48.800 --> 00:29:57.120]   It's in their advertising policies help, prohibited content, but also prohibited practices.
[00:29:57.120 --> 00:30:02.320]   And then finally, this is content that they will not sell ads against and will not be in flock.
[00:30:02.320 --> 00:30:08.880]   Sexual orientation, sexual content, alcohol.
[00:30:09.440 --> 00:30:13.920]   A lot of this stuff gambling and games, you think, boy, they're missing out on a whole category.
[00:30:13.920 --> 00:30:15.200]   You mean no draft kings?
[00:30:15.200 --> 00:30:17.120]   Healthcare and medicines.
[00:30:17.120 --> 00:30:21.360]   Some healthcare related content cannot be advertised at all.
[00:30:21.360 --> 00:30:25.200]   Political content, financial services.
[00:30:25.200 --> 00:30:31.600]   So there's a significantly, and I think admittedly the list is Google.
[00:30:31.600 --> 00:30:34.080]   It still means the shoes are going to follow me everywhere.
[00:30:34.080 --> 00:30:35.200]   Shoes will follow you.
[00:30:35.200 --> 00:30:37.200]   So that's really what it is.
[00:30:37.200 --> 00:30:39.520]   And also the flocks change every week too.
[00:30:39.520 --> 00:30:40.480]   They change every week.
[00:30:40.480 --> 00:30:40.800]   That's right.
[00:30:40.800 --> 00:30:41.920]   Based on what I'm going to say.
[00:30:41.920 --> 00:30:42.960]   That's what I was going to say.
[00:30:42.960 --> 00:30:45.200]   These are weekly, redone every week.
[00:30:45.200 --> 00:30:52.400]   So I mean, again, I'm not sure I fully agree with Steve, but he did a very, he did analysis from the point of view
[00:30:52.400 --> 00:30:59.920]   of statistics and whether what they propose, this K distribution actually makes sense.
[00:30:59.920 --> 00:31:01.040]   And he says it does.
[00:31:01.040 --> 00:31:05.680]   And what they're doing is privacy forward and does protect people.
[00:31:06.720 --> 00:31:09.680]   But it is more privacy forward, but it can be.
[00:31:09.680 --> 00:31:14.320]   So it is more privacy forward, but it also can create links.
[00:31:14.320 --> 00:31:17.360]   And I don't know if Steve addressed this or not, but it.
[00:31:17.360 --> 00:31:25.520]   If you're on other Google properties like Gmail, it is possible for Google to tie your actual identity
[00:31:25.520 --> 00:31:27.120]   to those statistics.
[00:31:27.120 --> 00:31:28.560]   They say they're not doing that though, right?
[00:31:28.560 --> 00:31:29.360]   Absolutely.
[00:31:29.360 --> 00:31:31.200]   Well, I believe.
[00:31:31.200 --> 00:31:31.520]   Yeah.
[00:31:31.520 --> 00:31:34.480]   I'm like, getting caught doing this would be a high concept.
[00:31:34.480 --> 00:31:40.080]   Yeah, I mean, they could be a considerable liability once they make that warrant,
[00:31:40.080 --> 00:31:42.800]   the Federal Trade Commission can map them and say prove to us you don't.
[00:31:42.800 --> 00:31:45.760]   And if they do, they can be in deep trouble.
[00:31:45.760 --> 00:31:49.040]   So having made that promise, there is some accountability and impossible,
[00:31:49.040 --> 00:31:50.080]   whether the government does it or not.
[00:31:50.080 --> 00:31:52.320]   There's the threat of accountability.
[00:31:52.320 --> 00:31:52.880]   I will give you.
[00:31:52.880 --> 00:31:53.840]   There's a threat of accountability.
[00:31:53.840 --> 00:31:59.760]   Well, in a company as big and value was Google is going to be really reluctant to fall into that.
[00:31:59.760 --> 00:32:00.160]   I don't know.
[00:32:00.160 --> 00:32:04.000]   Remember how they were snarfing up everybody's Wi-Fi SSIDs.
[00:32:04.000 --> 00:32:05.440]   Again, when they were doing it.
[00:32:05.440 --> 00:32:06.880]   Yeah, we talked about it.
[00:32:06.880 --> 00:32:12.000]   And I think it was very clear that that was not an intentional policy on theirs.
[00:32:12.000 --> 00:32:14.720]   It was just the way open Wi-Fi works.
[00:32:14.720 --> 00:32:19.600]   And they, as soon as they, you know, people said, "Hey, you're doing that," they stopped.
[00:32:19.600 --> 00:32:26.080]   I don't look, I have mixed feelings about Google, especially of late, but I don't think
[00:32:26.080 --> 00:32:27.360]   they're actively malicious.
[00:32:27.360 --> 00:32:31.760]   I think what they're trying to do here is exactly what you'd expect a company that makes
[00:32:31.760 --> 00:32:34.560]   its living on advertising would try to do, which is to find it.
[00:32:34.560 --> 00:32:36.080]   Right. They're trying to make it more palpable.
[00:32:36.080 --> 00:32:36.560]   Right.
[00:32:36.560 --> 00:32:36.960]   Yeah.
[00:32:36.960 --> 00:32:43.120]   So to respond to people's privacy concerns, while still not eliminating the ad tracking
[00:32:43.120 --> 00:32:46.160]   of some kind, because advertisers really want to know more.
[00:32:46.160 --> 00:32:51.760]   Look, there's that old saying in advertising that, you know, I know half of the money I spent
[00:32:51.760 --> 00:32:53.360]   on advertising works.
[00:32:53.360 --> 00:32:54.560]   I just don't know which half.
[00:32:54.560 --> 00:32:58.720]   There's always that, you know, advertisers always feel like they're throwing money
[00:32:59.440 --> 00:33:00.720]   into the unknown.
[00:33:00.720 --> 00:33:04.640]   And they would, the more they can say, well, at least we're advertising people who might buy our
[00:33:04.640 --> 00:33:05.360]   product.
[00:33:05.360 --> 00:33:06.800]   That's really all they want to know.
[00:33:06.800 --> 00:33:12.400]   Some of our advertisers actually get angry because they say, "Well, you, you know,
[00:33:12.400 --> 00:33:15.600]   you drove a thousand hits to our website, but none of them bought the product."
[00:33:15.600 --> 00:33:17.680]   I can't help you with that.
[00:33:17.680 --> 00:33:19.280]   Right.
[00:33:19.280 --> 00:33:20.560]   I can't help you with that.
[00:33:20.560 --> 00:33:24.320]   All I can do is get them to be interested enough to go see it.
[00:33:24.320 --> 00:33:27.200]   If they don't buy you did the last mile, now you got to close it.
[00:33:27.200 --> 00:33:28.320]   You got to close it, man.
[00:33:28.320 --> 00:33:29.040]   Yeah.
[00:33:29.040 --> 00:33:31.840]   And so I shouldn't say that advertisers don't care because they do.
[00:33:31.840 --> 00:33:37.440]   Every advertiser wants their dollar to have a $2 return on interest investment.
[00:33:37.440 --> 00:33:42.640]   So I did some research this week by chance and came across interesting things
[00:33:42.640 --> 00:33:46.080]   in the early bit of the last century.
[00:33:46.080 --> 00:33:54.240]   Newspapers, I didn't realize this, would advertise that they censored the classified ads.
[00:33:54.240 --> 00:33:56.480]   Because newspapers themselves had really bad advertising.
[00:33:56.480 --> 00:33:58.240]   They were selling patent medicines on the consumer.
[00:33:58.240 --> 00:33:58.800]   Yeah, that's right.
[00:33:58.800 --> 00:34:00.400]   But also their advertisers were really bad.
[00:34:00.400 --> 00:34:08.080]   So the Postal Department threatened to regulate newspapers and declare them to be a public utility
[00:34:08.080 --> 00:34:10.800]   along with various states.
[00:34:10.800 --> 00:34:12.720]   So this is just like what's going on now?
[00:34:12.720 --> 00:34:14.000]   Just like before, just like before.
[00:34:14.000 --> 00:34:17.680]   Then the other thing that happened was newspapers have always been anti-competitive as hell with this today.
[00:34:17.680 --> 00:34:24.320]   They also fought hard against the billboard industry and tried to basically get an outlaw
[00:34:24.320 --> 00:34:25.280]   because this was junk.
[00:34:25.280 --> 00:34:28.560]   We shouldn't be advertising this.
[00:34:28.560 --> 00:34:31.680]   It's messing up the public just like they tried to kill radio news years later.
[00:34:31.680 --> 00:34:36.400]   And so we could go on through this again and again and again and again and eight new.
[00:34:36.400 --> 00:34:38.080]   Here is an interesting statistic.
[00:34:38.080 --> 00:34:44.640]   This is from Morning Consult brands based on their average net trust rating,
[00:34:44.640 --> 00:34:49.760]   the share of people who they polled who say they trust a brand to do the right thing.
[00:34:51.120 --> 00:34:56.160]   This is rankings conducted between March 1st and 30th 2021 in Brazil, Canada, China,
[00:34:56.160 --> 00:34:59.840]   France, Germany, Italy, Spain, the UK and the US.
[00:34:59.840 --> 00:35:04.480]   Number one, most trusted brand, Google.
[00:35:04.480 --> 00:35:09.680]   Number two, PayPal, which is bizarre.
[00:35:09.680 --> 00:35:12.080]   Yeah, but people who's doing this survey?
[00:35:12.080 --> 00:35:12.720]   It's getting their money.
[00:35:12.720 --> 00:35:16.720]   Microsoft, YouTube, Amazon, Sony, Adidas, Netflix, Visa and Samsung.
[00:35:16.720 --> 00:35:17.840]   You know who's not in the top 10?
[00:35:17.840 --> 00:35:18.240]   Apple.
[00:35:18.240 --> 00:35:20.560]   This is--
[00:35:20.560 --> 00:35:21.360]   Facebook.
[00:35:21.360 --> 00:35:22.160]   Facebook.
[00:35:22.160 --> 00:35:23.440]   Well, if I wouldn't expect Facebook--
[00:35:23.440 --> 00:35:24.400]   Facebook.
[00:35:24.400 --> 00:35:24.960]   Oh, that's less.
[00:35:24.960 --> 00:35:25.520]   But I would say--
[00:35:25.520 --> 00:35:28.480]   Google for all the-- oh my god, everybody hates the internet.
[00:35:28.480 --> 00:35:29.680]   Everybody saw the internet.
[00:35:29.680 --> 00:35:34.480]   What you were talking about beforehand in terms of media whipping up stops,
[00:35:34.480 --> 00:35:37.280]   this is my moral panic argument and I'm writing a book proposal,
[00:35:37.280 --> 00:35:42.480]   so watch out and be more of it, is media would give us the impression that everybody hates Google.
[00:35:42.480 --> 00:35:44.640]   I should point out that YouTube is also Google,
[00:35:44.640 --> 00:35:47.120]   so really Google has two out of the top five positions.
[00:35:49.120 --> 00:35:50.080]   Google's trusted.
[00:35:50.080 --> 00:35:55.440]   And Amazon, which people are saying after the various labor messes they've had,
[00:35:55.440 --> 00:35:56.720]   number five.
[00:35:56.720 --> 00:35:58.960]   That's very interesting.
[00:35:58.960 --> 00:36:00.000]   People like these services.
[00:36:00.000 --> 00:36:04.720]   Maybe that's what this is measuring more than anything else is how they like it.
[00:36:04.720 --> 00:36:09.600]   And I think people like Google because they use Google and Google search and for many people,
[00:36:09.600 --> 00:36:11.440]   Gmail are--
[00:36:11.440 --> 00:36:15.200]   So this indicates trust too, Leo, because PayPal, the reason PayPal is number two
[00:36:15.200 --> 00:36:16.560]   is because people are trusting their money.
[00:36:16.560 --> 00:36:17.440]   They have money, yeah.
[00:36:17.440 --> 00:36:18.000]   Yeah.
[00:36:18.000 --> 00:36:18.880]   Yeah, that's true.
[00:36:19.760 --> 00:36:24.640]   All right, we can move on a little bit, but can I go to a slightly related--
[00:36:24.640 --> 00:36:28.640]   You can if you let me pause briefly for station identification.
[00:36:28.640 --> 00:36:30.720]   All right, now Jeff Jarrf.
[00:36:30.720 --> 00:36:37.200]   I just thought it interesting that the temptation to mess you up with those moments is--
[00:36:37.200 --> 00:36:38.880]   I know you love it, I know, I know.
[00:36:38.880 --> 00:36:39.440]   I know, I know.
[00:36:39.440 --> 00:36:39.920]   I know, I know.
[00:36:39.920 --> 00:36:40.160]   We--
[00:36:40.160 --> 00:36:41.840]   I should just point out just real quickly.
[00:36:41.840 --> 00:36:45.120]   Nobody wanted to buy ads on this show.
[00:36:45.120 --> 00:36:45.760]   I don't know why.
[00:36:45.760 --> 00:36:48.080]   We don't do enough link bait.
[00:36:48.080 --> 00:36:50.320]   I don't know why, but this show is ad-free.
[00:36:50.320 --> 00:36:51.840]   Even if you didn't pay for it--
[00:36:51.840 --> 00:36:52.880]   That's not about hers.
[00:36:52.880 --> 00:36:56.160]   Please, I beg of you, pay for it.
[00:36:56.160 --> 00:36:56.560]   Oh, yes.
[00:36:56.560 --> 00:36:57.280]   No, I understand.
[00:36:57.280 --> 00:37:00.480]   In fact, I don't expect more than about 10% of our audience to pay for it.
[00:37:00.480 --> 00:37:03.920]   That, by the way, that's our stretch goal.
[00:37:03.920 --> 00:37:09.440]   Public broadcasting, you know, with all the pledge nights and stuff, they do 5%.
[00:37:09.440 --> 00:37:11.200]   So if we got--
[00:37:11.200 --> 00:37:11.760]   6 to 12.
[00:37:11.760 --> 00:37:12.400]   Yeah, if we--
[00:37:12.400 --> 00:37:14.080]   Okay, 6 to 12 is a little more than it used to be.
[00:37:14.080 --> 00:37:18.960]   So if we got 10, I would be over the moon and it would be a huge success.
[00:37:18.960 --> 00:37:23.760]   So that means 90% of our audience is not paying for it.
[00:37:23.760 --> 00:37:24.800]   And I understand that.
[00:37:24.800 --> 00:37:25.680]   They're like you, Aunt.
[00:37:25.680 --> 00:37:27.360]   They just say, "I'm not going to pay for that."
[00:37:27.360 --> 00:37:30.720]   I don't blame you.
[00:37:30.720 --> 00:37:32.080]   But that's the thing, though.
[00:37:32.080 --> 00:37:34.000]   There are things that I will pay for.
[00:37:34.000 --> 00:37:36.240]   I do pay for Amazon Prime.
[00:37:36.240 --> 00:37:40.000]   I do pay for Netflix because I see value there.
[00:37:40.000 --> 00:37:42.720]   But there's a lot of stuff out there.
[00:37:42.720 --> 00:37:44.160]   I'm like, "Do you pay for any content?
[00:37:44.160 --> 00:37:46.240]   Do you pay for Netflix, I guess, is content?"
[00:37:46.240 --> 00:37:48.480]   So anything else besides Netflix?
[00:37:48.480 --> 00:37:53.200]   And now that I heard that there's a discount for the Washington Post,
[00:37:53.200 --> 00:37:56.560]   I will at least look at the Washington Post now for that discount rate.
[00:37:56.560 --> 00:37:58.480]   I think it's just a seafood.
[00:37:58.480 --> 00:38:02.240]   Just to see if it's worth it, you know, I don't know.
[00:38:02.240 --> 00:38:04.400]   The New York Times has become a problem at--
[00:38:04.400 --> 00:38:07.760]   You don't have to buy a newspaper subscription, Aunt.
[00:38:07.760 --> 00:38:08.240]   Really?
[00:38:08.240 --> 00:38:08.880]   No, I'm--
[00:38:08.880 --> 00:38:10.160]   Well, I'd like to--
[00:38:10.160 --> 00:38:12.960]   I'd like to be informed and I would like to see--
[00:38:12.960 --> 00:38:14.400]   That's the same thing.
[00:38:14.400 --> 00:38:16.400]   Made him feel bad, I'm sorry.
[00:38:16.400 --> 00:38:17.360]   But no, I--
[00:38:17.360 --> 00:38:22.080]   But I do think journalists should still be paid, so yeah.
[00:38:22.080 --> 00:38:26.000]   What is the best way to get informed these days?
[00:38:26.000 --> 00:38:27.680]   That seems like a pretty imperfect way to--
[00:38:27.680 --> 00:38:28.720]   That's what we have to be in this.
[00:38:28.720 --> 00:38:29.680]   --find newspapers.
[00:38:29.680 --> 00:38:30.160]   I don't know.
[00:38:30.160 --> 00:38:34.320]   I think what you're talking to me is probably asking Mr. Burke at the studio.
[00:38:34.320 --> 00:38:35.040]   He seems annoying.
[00:38:35.040 --> 00:38:35.840]   He knows everything.
[00:38:37.680 --> 00:38:41.680]   Yeah, we were talking on Windows Weekly about a crappy Microsoft news is--
[00:38:41.680 --> 00:38:44.800]   Apple News just is bad.
[00:38:44.800 --> 00:38:46.000]   I saw it as a news product?
[00:38:46.000 --> 00:38:47.040]   Yeah, and it's bad.
[00:38:47.040 --> 00:38:48.240]   If you used Windows, you would--
[00:38:48.240 --> 00:38:49.360]   They actually pay the papers.
[00:38:49.360 --> 00:38:50.320]   They have since the beginning.
[00:38:50.320 --> 00:38:51.040]   It's awful.
[00:38:51.040 --> 00:38:54.320]   And Apple News is also awful.
[00:38:54.320 --> 00:38:56.160]   Google News is of the three.
[00:38:56.160 --> 00:38:57.600]   It's the better of the three.
[00:38:57.600 --> 00:38:59.920]   But even then, it has a lot of junk in it.
[00:38:59.920 --> 00:39:00.480]   So--
[00:39:00.480 --> 00:39:03.200]   Couldn't build news keeps feeding me sports, and I keep telling it.
[00:39:03.200 --> 00:39:03.680]   Exactly.
[00:39:03.680 --> 00:39:06.800]   Now, I've given up literally on all the--
[00:39:06.800 --> 00:39:08.320]   I'm like, don't show me basketball.
[00:39:08.320 --> 00:39:09.520]   Don't show me baseball.
[00:39:09.520 --> 00:39:11.120]   Now, it's showing me cricket.
[00:39:11.120 --> 00:39:12.080]   It just really--
[00:39:12.080 --> 00:39:14.080]   No, you want sports, Stacy.
[00:39:14.080 --> 00:39:15.200]   We know you want sports.
[00:39:15.200 --> 00:39:15.760]   You want sports.
[00:39:15.760 --> 00:39:17.600]   That's hysterical.
[00:39:17.600 --> 00:39:18.720]   It's like, OK.
[00:39:18.720 --> 00:39:20.000]   That's early next.
[00:39:20.000 --> 00:39:20.640]   That's it.
[00:39:20.640 --> 00:39:24.160]   I can't wait to see what new sport I will discover.
[00:39:24.160 --> 00:39:25.360]   But I'm just curious.
[00:39:25.360 --> 00:39:29.120]   I mean, if anybody should know, we should know.
[00:39:29.120 --> 00:39:30.080]   How do you--
[00:39:30.080 --> 00:39:31.760]   How should some--
[00:39:31.760 --> 00:39:33.440]   Why should somebody behave to be well-informed?
[00:39:33.440 --> 00:39:34.960]   What is it?
[00:39:34.960 --> 00:39:35.520]   What do you do?
[00:39:36.160 --> 00:39:39.680]   Like, John Oliver, I think some of the more comic--
[00:39:39.680 --> 00:39:43.360]   That's why the Daily Show worked, right?
[00:39:43.360 --> 00:39:46.320]   Because there was a little bit of sugar with your medicine.
[00:39:46.320 --> 00:39:47.920]   The politicized--
[00:39:47.920 --> 00:39:48.720]   Yeah, because you--
[00:39:48.720 --> 00:39:50.400]   But only if you're liberal.
[00:39:50.400 --> 00:39:51.920]   People on the right hate.
[00:39:51.920 --> 00:39:52.400]   Hate.
[00:39:52.400 --> 00:39:52.800]   Hate.
[00:39:52.800 --> 00:39:54.560]   John Stewart and John Oliver.
[00:39:54.560 --> 00:39:55.360]   They hate him.
[00:39:55.360 --> 00:39:57.120]   And Greg Gutman is no John Stewart.
[00:39:57.120 --> 00:40:03.520]   I think it's hard because in Ant really drives home.
[00:40:03.520 --> 00:40:07.680]   I think we feel like things should be black and white.
[00:40:07.680 --> 00:40:11.680]   And when you talk about the real issues that we face in society--
[00:40:11.680 --> 00:40:17.120]   and I'm not-- I'm talking about everything from systemic racism to police
[00:40:17.120 --> 00:40:21.600]   brutality or to issues with our police departments
[00:40:21.600 --> 00:40:23.200]   and even things like health care.
[00:40:23.200 --> 00:40:26.240]   These are complicated, highly sophisticated things
[00:40:26.240 --> 00:40:31.120]   where you have to know a lot and you have to be willing to accept that there are trade-offs.
[00:40:31.680 --> 00:40:38.800]   And you can't-- people don't want to think about trade-offs because it's hard.
[00:40:38.800 --> 00:40:41.520]   And you have to have facts and it feels like debate club.
[00:40:41.520 --> 00:40:46.080]   And news people are all the people who like debate club, right?
[00:40:46.080 --> 00:40:48.720]   We're all the people who are like, "Yeah, let's research the hell out of this."
[00:40:48.720 --> 00:40:51.280]   And pummel people with the information.
[00:40:51.280 --> 00:41:00.080]   There's no easy way to solve this without recognizing and getting everybody,
[00:41:00.080 --> 00:41:04.400]   like citizens to recognize that there are trade-offs associated with everything.
[00:41:04.400 --> 00:41:06.720]   And I don't know how to do that.
[00:41:06.720 --> 00:41:08.080]   You have to start with education.
[00:41:08.080 --> 00:41:12.800]   Like we do it with my daughter who is like, "You know, I hate capitalism."
[00:41:12.800 --> 00:41:15.120]   And I'm like, "Well, let's talk about that."
[00:41:15.120 --> 00:41:16.400]   Yeah, let's explain.
[00:41:16.400 --> 00:41:20.400]   Yeah, we should not be deforesting the rainforest.
[00:41:20.400 --> 00:41:22.800]   And I'm like, "Well, let's talk about why that's happening."
[00:41:22.800 --> 00:41:24.800]   And then she's like, "Oh, this sucks."
[00:41:24.800 --> 00:41:28.560]   It's a lot easier to do a bumper sticker than it is to do it.
[00:41:28.560 --> 00:41:29.360]   Yeah, so...
[00:41:29.360 --> 00:41:30.320]   Do a deep dive.
[00:41:30.320 --> 00:41:32.960]   I don't think this is a new problem.
[00:41:32.960 --> 00:41:34.560]   It's human.
[00:41:34.560 --> 00:41:35.840]   Why you have, quote, "elage."
[00:41:35.840 --> 00:41:36.400]   Humans.
[00:41:36.400 --> 00:41:39.760]   You have the people who dig into this and want to fight it out.
[00:41:39.760 --> 00:41:43.440]   But then we get pissed off at them because they've put the effort
[00:41:43.440 --> 00:41:45.840]   and they usually put the effort in because they have an agenda.
[00:41:45.840 --> 00:41:47.520]   I was listening to a podcast the other day
[00:41:47.520 --> 00:41:51.680]   of people that you all know very well.
[00:41:51.680 --> 00:41:56.960]   And they were talking about COVID-19 and remedies and vaccines.
[00:41:57.680 --> 00:42:00.880]   And they said something like, "Don't listen to the medical elites."
[00:42:00.880 --> 00:42:03.280]   And it was like, "Wait a minute.
[00:42:03.280 --> 00:42:04.080]   You mean doctors?
[00:42:04.080 --> 00:42:04.640]   Why not?"
[00:42:04.640 --> 00:42:06.400]   You mean epidemiologists?
[00:42:06.400 --> 00:42:07.200]   You mean scientists?
[00:42:07.200 --> 00:42:08.080]   Don't listen.
[00:42:08.080 --> 00:42:11.760]   But you see how they twisted that by calling them elites?
[00:42:11.760 --> 00:42:12.240]   Right.
[00:42:12.240 --> 00:42:13.760]   Right.
[00:42:13.760 --> 00:42:17.280]   I'm going to list some idiot podcaster for my...
[00:42:17.280 --> 00:42:18.240]   I'm going to get my elven there.
[00:42:18.240 --> 00:42:18.640]   Or elite.
[00:42:18.640 --> 00:42:20.320]   From some idiot podcaster.
[00:42:20.320 --> 00:42:22.240]   Well, but we always...
[00:42:22.240 --> 00:42:23.920]   We go back to my COVID list.
[00:42:23.920 --> 00:42:27.040]   That's where I find the best information.
[00:42:27.040 --> 00:42:30.960]   And so I want systems where I can find experts in multiple areas.
[00:42:30.960 --> 00:42:36.640]   Americans have historically hated eggheads.
[00:42:36.640 --> 00:42:38.560]   Going back to Adlays Stevenson, right?
[00:42:38.560 --> 00:42:39.200]   That was...
[00:42:39.200 --> 00:42:41.360]   When he was running for president, he was an egghead.
[00:42:41.360 --> 00:42:45.520]   A.K.A. an expert or a professor.
[00:42:45.520 --> 00:42:48.160]   Why is it we're so anti-elite?
[00:42:48.160 --> 00:42:48.800]   What is that?
[00:42:48.800 --> 00:42:51.120]   No idea.
[00:42:51.120 --> 00:42:52.640]   I will say that I am.
[00:42:52.640 --> 00:42:57.680]   We're a culture founded on rugged individualism.
[00:42:57.680 --> 00:42:59.600]   Andy Galatarianism, right?
[00:42:59.600 --> 00:43:02.560]   Well, I will say Galatarianism.
[00:43:02.560 --> 00:43:03.120]   Right.
[00:43:03.120 --> 00:43:03.760]   Yeah.
[00:43:03.760 --> 00:43:06.240]   But we're also a culture of people who were like...
[00:43:06.240 --> 00:43:09.600]   I mean, super individualistic, super independent.
[00:43:09.600 --> 00:43:11.040]   We're people who literally,
[00:43:11.040 --> 00:43:12.640]   you know,
[00:43:12.640 --> 00:43:15.680]   overthrew the government when they decided to tax us.
[00:43:15.680 --> 00:43:15.840]   Right.
[00:43:15.840 --> 00:43:16.000]   You know,
[00:43:16.000 --> 00:43:18.320]   that is our founding mythology.
[00:43:18.320 --> 00:43:18.800]   It's our history.
[00:43:18.800 --> 00:43:19.680]   Yeah, yeah, yeah.
[00:43:19.680 --> 00:43:20.160]   Yeah.
[00:43:21.360 --> 00:43:26.000]   You know, I will say that I am privileged and I recognize that because
[00:43:26.000 --> 00:43:29.440]   you say, "Well, where do you get your news from?"
[00:43:29.440 --> 00:43:34.480]   I have the privilege of knowing you folks and can come straight to you.
[00:43:34.480 --> 00:43:37.760]   I have the privilege of knowing someone like Dan Patterson.
[00:43:37.760 --> 00:43:39.680]   And he's super smart.
[00:43:39.680 --> 00:43:40.160]   Yeah.
[00:43:40.160 --> 00:43:41.840]   He ever pops up in my head.
[00:43:41.840 --> 00:43:42.400]   I go...
[00:43:42.400 --> 00:43:46.320]   I pull him right up on the phone and just go from there.
[00:43:46.320 --> 00:43:50.160]   I have a couple different people in my circle that I can trust for
[00:43:51.040 --> 00:43:52.800]   news because I go to him.
[00:43:52.800 --> 00:43:53.440]   I say, "You know what?
[00:43:53.440 --> 00:43:55.280]   I don't know what's true out there.
[00:43:55.280 --> 00:43:56.400]   Can you help me?"
[00:43:56.400 --> 00:43:58.000]   And they can answer.
[00:43:58.000 --> 00:43:59.920]   You all can answer for me and it helps.
[00:43:59.920 --> 00:44:02.160]   But everybody doesn't have that privilege.
[00:44:02.160 --> 00:44:02.640]   I know that.
[00:44:02.640 --> 00:44:07.920]   And there's also understanding the implications if you go to a primary source.
[00:44:07.920 --> 00:44:11.600]   Like Georgia's election law is a great example, actually.
[00:44:11.600 --> 00:44:11.760]   Yeah.
[00:44:11.760 --> 00:44:14.080]   Like that was 98 pages.
[00:44:14.080 --> 00:44:15.120]   And if you read the whole...
[00:44:15.120 --> 00:44:16.960]   Like my husband read the whole law
[00:44:16.960 --> 00:44:19.920]   because he really wanted to understand what was happening.
[00:44:19.920 --> 00:44:23.200]   And he comes away and he's like, "Stacey, I don't think it's that bad."
[00:44:23.200 --> 00:44:28.720]   And the reason he didn't is because he didn't understand all of the historical
[00:44:28.720 --> 00:44:32.800]   precedents, that particular aspects of the law set.
[00:44:32.800 --> 00:44:33.200]   And that's...
[00:44:33.200 --> 00:44:38.720]   So it's also because we, in some areas,
[00:44:38.720 --> 00:44:41.360]   include the meaning and the intent of things,
[00:44:41.360 --> 00:44:45.360]   intentionally, which makes it also difficult for people to understand
[00:44:45.360 --> 00:44:47.120]   and also engenders more distrust.
[00:44:47.120 --> 00:44:49.120]   So...
[00:44:50.080 --> 00:44:50.720]   I don't know.
[00:44:50.720 --> 00:44:51.680]   We're not going to solve this.
[00:44:51.680 --> 00:44:52.560]   This is not a solvable problem.
[00:44:52.560 --> 00:44:53.920]   It's fascinating.
[00:44:53.920 --> 00:44:56.640]   But I think we do have entirely new opportunities, right?
[00:44:56.640 --> 00:44:58.080]   And it starts with...
[00:44:58.080 --> 00:45:02.240]   I remember Dave Weiner talking about the early days of blogs.
[00:45:02.240 --> 00:45:03.920]   And you could have...
[00:45:03.920 --> 00:45:08.800]   Economists could now speak directly to the public, right?
[00:45:08.800 --> 00:45:09.760]   And you could go to the experts,
[00:45:09.760 --> 00:45:10.640]   you could listen to the economists,
[00:45:10.640 --> 00:45:12.640]   whatever you think they think of economists.
[00:45:12.640 --> 00:45:12.880]   But...
[00:45:12.880 --> 00:45:15.360]   I was going to say, economists, as experts?
[00:45:15.360 --> 00:45:15.760]   Jeff.
[00:45:15.760 --> 00:45:16.160]   I know.
[00:45:16.160 --> 00:45:18.560]   But I'm going to go to my COVID list.
[00:45:18.560 --> 00:45:19.440]   Economists...
[00:45:19.440 --> 00:45:22.560]   Advocacy is a very good source of information.
[00:45:22.560 --> 00:45:23.600]   It's just too expensive.
[00:45:23.600 --> 00:45:24.800]   Yes, it is.
[00:45:24.800 --> 00:45:25.440]   I'd subscribe.
[00:45:25.440 --> 00:45:29.600]   Stacey, I'm just going to be like, "I've subscribed everything to me."
[00:45:29.600 --> 00:45:33.440]   I think economists is quite good, don't you?
[00:45:33.440 --> 00:45:34.160]   But I just can't...
[00:45:34.160 --> 00:45:35.600]   Pile will go for a $150.
[00:45:35.600 --> 00:45:37.440]   It's so expensive.
[00:45:37.440 --> 00:45:38.880]   I know, I can't do it.
[00:45:38.880 --> 00:45:40.320]   I also subscribed to the New Yorker,
[00:45:40.320 --> 00:45:41.280]   and I can't keep up with it,
[00:45:41.280 --> 00:45:41.920]   but it's just cheap.
[00:45:41.920 --> 00:45:42.240]   Me too?
[00:45:42.240 --> 00:45:43.120]   Another pile of guilt.
[00:45:43.120 --> 00:45:43.440]   Yeah.
[00:45:43.440 --> 00:45:44.160]   Pile of guilt.
[00:45:44.160 --> 00:45:46.080]   You know what?
[00:45:46.080 --> 00:45:48.480]   I thought this might be a good time to go to our state.
[00:45:48.480 --> 00:45:56.080]   And ask our listeners how they keep up with what's going on in the world around us.
[00:45:56.080 --> 00:46:00.160]   So if you're in the stage room on our Discord server,
[00:46:00.160 --> 00:46:05.120]   it's called Twig Live, raise your hand if you have a thought about all this.
[00:46:05.120 --> 00:46:05.920]   Let's just try it.
[00:46:05.920 --> 00:46:07.280]   Let's just see.
[00:46:07.280 --> 00:46:08.240]   We may regret this.
[00:46:08.240 --> 00:46:15.600]   Especially because the first person up on the stage is named Captain Jack.
[00:46:16.480 --> 00:46:20.720]   Mr. Sparrow, you may jump right in.
[00:46:20.720 --> 00:46:23.520]   Go ahead and pot this up, John, on the stage.
[00:46:23.520 --> 00:46:24.320]   Are you there, Jack?
[00:46:24.320 --> 00:46:26.080]   Hey Leo.
[00:46:26.080 --> 00:46:26.640]   Hey.
[00:46:26.640 --> 00:46:27.920]   Hey, Group, how's it going?
[00:46:27.920 --> 00:46:28.640]   It's going great.
[00:46:28.640 --> 00:46:29.600]   Thanks for listening to the show.
[00:46:29.600 --> 00:46:30.720]   Thanks for joining Club2It.
[00:46:30.720 --> 00:46:33.040]   Absolutely.
[00:46:33.040 --> 00:46:36.240]   So I can't show you on screen here right now,
[00:46:36.240 --> 00:46:38.000]   because obviously we don't do video yet.
[00:46:38.000 --> 00:46:38.400]   Not yet.
[00:46:38.400 --> 00:46:39.280]   We're working on that.
[00:46:39.280 --> 00:46:45.360]   I keep up four different browsers with YouTube TV,
[00:46:45.360 --> 00:46:51.600]   and they've got my BBC, Al Jazeera, and NBC and Fox.
[00:46:51.600 --> 00:46:54.560]   And whenever they synced up on something, I know it's important.
[00:46:54.560 --> 00:46:55.360]   Wow.
[00:46:55.360 --> 00:46:56.560]   That actually is pretty good.
[00:46:56.560 --> 00:46:57.120]   Otherwise, I'm pretty good at changing.
[00:46:57.120 --> 00:46:57.280]   Wow.
[00:46:57.280 --> 00:46:59.680]   Fox and Al Jazeera agree on anything.
[00:46:59.680 --> 00:47:03.600]   I'll be amazed, but that's a reasonable algorithm, actually.
[00:47:03.600 --> 00:47:04.080]   Wow.
[00:47:04.080 --> 00:47:04.320]   Yeah.
[00:47:04.320 --> 00:47:05.760]   There's an algorithm.
[00:47:05.760 --> 00:47:08.160]   How do you watch all four of them at the same time?
[00:47:08.160 --> 00:47:13.200]   I'll put it in the chat, but four different YouTube TV
[00:47:14.160 --> 00:47:15.760]   windows on a TV TV.
[00:47:15.760 --> 00:47:20.000]   But you must have one mute, a mall, except one.
[00:47:20.000 --> 00:47:22.400]   You must rotate through them or something.
[00:47:22.400 --> 00:47:26.240]   It's kind of like the NFL red zone.
[00:47:26.240 --> 00:47:27.760]   Yeah.
[00:47:27.760 --> 00:47:29.280]   Well, the game's on.
[00:47:29.280 --> 00:47:31.360]   And if something interesting happens, you unmute that one.
[00:47:31.360 --> 00:47:32.400]   Is that kind of how you're doing it?
[00:47:32.400 --> 00:47:34.080]   You can see the lower thirds.
[00:47:34.080 --> 00:47:35.280]   Close captions.
[00:47:35.280 --> 00:47:36.400]   Close captions.
[00:47:36.400 --> 00:47:36.880]   Close captions.
[00:47:36.880 --> 00:47:37.440]   Close captions.
[00:47:37.440 --> 00:47:38.480]   I don't have audio on them.
[00:47:38.480 --> 00:47:42.000]   You could probably read faster than they can speak all of them.
[00:47:42.000 --> 00:47:43.360]   Yeah, that makes sense.
[00:47:43.360 --> 00:47:44.960]   That's a good plan, guys.
[00:47:44.960 --> 00:47:45.360]   Nice, heck.
[00:47:45.360 --> 00:47:45.760]   Yeah.
[00:47:45.760 --> 00:47:48.160]   Thanks for sharing with us.
[00:47:48.160 --> 00:47:48.640]   That's great.
[00:47:48.640 --> 00:47:50.160]   How about that?
[00:47:50.160 --> 00:47:54.160]   Don't judge these people based on their chat handles.
[00:47:54.160 --> 00:47:54.480]   Okay.
[00:47:54.480 --> 00:47:55.440]   I'm just saying.
[00:47:55.440 --> 00:47:56.080]   Oh, I am.
[00:47:56.080 --> 00:47:58.080]   King Wing 27.
[00:47:58.080 --> 00:47:58.720]   What do you think?
[00:47:58.720 --> 00:48:06.160]   Oh, I mainly just use Google News, which is not great,
[00:48:06.160 --> 00:48:07.440]   but instead of the old map.
[00:48:07.440 --> 00:48:12.080]   So the main thing is those top five stories for you.
[00:48:12.800 --> 00:48:16.400]   And I actually find that those are usually pretty close to what I'm interested in.
[00:48:16.400 --> 00:48:23.840]   Well, like lately, I haven't been using it just because it's all about COVID.
[00:48:23.840 --> 00:48:25.600]   And I'm just kind of don't want to hear it.
[00:48:25.600 --> 00:48:26.720]   You're all done with that one, aren't we?
[00:48:26.720 --> 00:48:27.440]   Yeah.
[00:48:27.440 --> 00:48:30.560]   Yeah, like, well, unfortunately, where I live, we're not.
[00:48:30.560 --> 00:48:32.560]   Where are you calling from?
[00:48:32.560 --> 00:48:35.600]   I'm calling from Ontario Canada.
[00:48:35.600 --> 00:48:36.080]   Oh, yeah.
[00:48:36.080 --> 00:48:38.640]   We're not shortly walked down.
[00:48:38.640 --> 00:48:39.760]   Oh, I'm sorry.
[00:48:39.760 --> 00:48:40.320]   She's.
[00:48:40.320 --> 00:48:40.800]   Yeah, sorry.
[00:48:40.800 --> 00:48:42.320]   We're deeply in it.
[00:48:42.320 --> 00:48:45.200]   So yeah, so we're so I just, but I use that.
[00:48:45.200 --> 00:48:48.080]   And then I use I follow mainly tech news.
[00:48:48.080 --> 00:48:49.520]   So I go to main, like,
[00:48:49.520 --> 00:48:50.960]   another place where.
[00:48:50.960 --> 00:48:51.920]   Yeah, that's a lot easier.
[00:48:51.920 --> 00:48:53.920]   That's what most of my news consumption is that.
[00:48:53.920 --> 00:48:56.480]   And that's a lot easier as I had said, a tech meme and nuzzle.
[00:48:56.480 --> 00:48:59.440]   There's lots of ways to kind of keep on on tech news.
[00:48:59.440 --> 00:49:03.120]   And there's a few enough voices that you can kind of get a consensus opinion.
[00:49:03.120 --> 00:49:03.680]   Right.
[00:49:03.680 --> 00:49:04.800]   On one subject.
[00:49:04.800 --> 00:49:07.200]   So that is easier than politics.
[00:49:07.200 --> 00:49:08.640]   Politics is a little harder, I think.
[00:49:08.640 --> 00:49:11.120]   King Wing.
[00:49:11.120 --> 00:49:12.160]   Don't call it politics.
[00:49:12.160 --> 00:49:13.680]   Call it policy policy.
[00:49:13.680 --> 00:49:14.640]   Yeah.
[00:49:14.640 --> 00:49:15.440]   King Wing, thank you.
[00:49:15.440 --> 00:49:18.400]   I'm going to let you wander from the stage.
[00:49:18.400 --> 00:49:19.360]   I appreciate.
[00:49:19.360 --> 00:49:21.280]   I want to do one more before we move on.
[00:49:21.280 --> 00:49:24.960]   And that's Joe, Joe, I'm inviting you up onto the stage.
[00:49:24.960 --> 00:49:30.640]   We're finding out what people really think and education for this panel.
[00:49:30.640 --> 00:49:31.120]   Hey, Joe.
[00:49:31.120 --> 00:49:36.960]   Yeah, I like Apple News Plus more than I thought I would.
[00:49:36.960 --> 00:49:38.800]   They have quite a few sources.
[00:49:39.360 --> 00:49:41.040]   The Wall Street Journal is on there.
[00:49:41.040 --> 00:49:42.480]   The National Review is on there.
[00:49:42.480 --> 00:49:45.200]   Obviously, they have magazines on there like Wired.
[00:49:45.200 --> 00:49:49.920]   I know that they don't necessarily offer all of the Wall Street Journal, for example.
[00:49:49.920 --> 00:49:54.480]   But it seems to have a good cross-section of a lot of good content from them.
[00:49:54.480 --> 00:49:55.920]   It's a lot cheaper way to get the journal.
[00:49:55.920 --> 00:49:56.240]   Yeah.
[00:49:56.240 --> 00:49:57.200]   Yeah.
[00:49:57.200 --> 00:50:02.160]   And they seem to have a number of respected sources across the political spectrum.
[00:50:02.160 --> 00:50:02.400]   Right.
[00:50:02.400 --> 00:50:09.200]   I mean, I think the National Review and NPR don't necessarily agree on
[00:50:09.200 --> 00:50:09.920]   that.
[00:50:09.920 --> 00:50:12.560]   I've actually enjoyed reading the National Review.
[00:50:12.560 --> 00:50:15.600]   And I also read a libertarian magazine called Reason.
[00:50:15.600 --> 00:50:23.920]   And while I rarely agree with either, they at least are thoughtful and well thought out and
[00:50:23.920 --> 00:50:24.880]   intelligent.
[00:50:24.880 --> 00:50:27.200]   And you lose good to see a little reason?
[00:50:27.200 --> 00:50:29.280]   Yes, they're reason.
[00:50:29.280 --> 00:50:30.560]   I'm not a libertarian.
[00:50:30.560 --> 00:50:34.560]   But at least there are some intelligence behind it.
[00:50:34.560 --> 00:50:37.040]   Hey, Joe, good suggestion.
[00:50:37.040 --> 00:50:37.440]   Thank you.
[00:50:37.440 --> 00:50:38.400]   I appreciate it.
[00:50:38.400 --> 00:50:38.880]   Thank you.
[00:50:38.880 --> 00:50:39.200]   Yeah.
[00:50:39.200 --> 00:50:41.040]   I tried to get a ton of good content.
[00:50:41.040 --> 00:50:41.440]   I'm really curious.
[00:50:41.440 --> 00:50:41.440]   I'm really curious.
[00:50:41.440 --> 00:50:42.000]   I'm really curious.
[00:50:42.000 --> 00:50:42.400]   I'm really curious.
[00:50:42.400 --> 00:50:43.680]   Do you like your voices to go with people's avatars and names?
[00:50:43.680 --> 00:50:44.240]   Isn't that nice?
[00:50:44.240 --> 00:50:44.880]   That's kind of fun.
[00:50:44.880 --> 00:50:45.360]   That's awesome.
[00:50:45.360 --> 00:50:46.960]   Yeah, that's good.
[00:50:46.960 --> 00:50:47.200]   Yeah.
[00:50:47.200 --> 00:50:51.840]   So I think we'll just do this from time to time, especially in something where we can't really
[00:50:51.840 --> 00:50:52.960]   decide what the answer is.
[00:50:52.960 --> 00:50:55.440]   We can choose to go to them too.
[00:50:55.440 --> 00:50:57.760]   We have a smart, I have to say, we have a smart audience.
[00:50:57.760 --> 00:51:02.400]   And one of the reasons we've always used the chat, the IRC chat,
[00:51:02.400 --> 00:51:07.200]   and now we're using Discord, is I've always wanted to make everything we do more interactive.
[00:51:07.200 --> 00:51:09.520]   I don't want to be sitting up here preaching.
[00:51:09.520 --> 00:51:13.360]   I think it's a conversation.
[00:51:13.360 --> 00:51:17.520]   And everything we do is somewhat of a conversation, but the more voices we can get in the better.
[00:51:17.520 --> 00:51:19.040]   I really appreciate that.
[00:51:19.040 --> 00:51:24.320]   Thank you for those of all for you and all of you who've joined the Twit Club.
[00:51:24.320 --> 00:51:33.360]   Oh, by the way, I was so I created a new tab in Firefox and Firefox uses pocket for news suggestions.
[00:51:33.360 --> 00:51:34.720]   I just want to give you an example.
[00:51:36.240 --> 00:51:39.120]   So this is a magazine you read, Ion Stacy.
[00:51:39.120 --> 00:51:40.160]   I know you like Ion.
[00:51:40.160 --> 00:51:45.680]   The obesity era is American people get fatter, so did marmosets, vervet, monkeys, and mice.
[00:51:45.680 --> 00:51:47.360]   That's one of them from domino.com.
[00:51:47.360 --> 00:51:50.320]   My boyfriend and I sleep in separate rooms and love it.
[00:51:50.320 --> 00:51:54.880]   So far, I would click on all of these stories.
[00:51:54.880 --> 00:51:55.680]   They're good, right?
[00:51:55.680 --> 00:51:57.680]   Slate, really though.
[00:51:57.680 --> 00:51:59.680]   What genes are in style now?
[00:51:59.680 --> 00:52:01.280]   I read that story.
[00:52:01.280 --> 00:52:02.480]   I have so many thoughts.
[00:52:03.680 --> 00:52:05.840]   This is a, I hear, you're so long ago.
[00:52:05.840 --> 00:52:08.640]   I love it.
[00:52:08.640 --> 00:52:09.600]   It's great.
[00:52:09.600 --> 00:52:16.080]   Finally from ink.com, 35 smart habits that will train other people to treat you with respect.
[00:52:16.080 --> 00:52:19.280]   We have completely reinvent media.
[00:52:19.280 --> 00:52:26.320]   Clearly, there were two news stories in here, but we're actually a few more, but those are the top
[00:52:26.320 --> 00:52:26.720]   stories.
[00:52:26.720 --> 00:52:28.000]   Oh my God.
[00:52:28.000 --> 00:52:31.440]   Google, when it's not feeding me, sports actually does a pretty good job.
[00:52:31.440 --> 00:52:35.280]   I mean, if it just didn't have that weird hang up, like you must have a well-rounded
[00:52:35.280 --> 00:52:36.960]   news profile, must have sports.
[00:52:36.960 --> 00:52:38.960]   I'm just looking through it all about AI.
[00:52:38.960 --> 00:52:41.840]   It took a while for me too.
[00:52:41.840 --> 00:52:48.240]   It learned as you use celebrity news and weddings and stuff, as if I really care who's
[00:52:48.240 --> 00:52:51.920]   having a baby and I kept denying it.
[00:52:51.920 --> 00:52:56.000]   Didn't it finally started giving me all of my sports and tech stuff that, but it was a little
[00:52:56.000 --> 00:52:56.480]   weird to see.
[00:52:56.480 --> 00:52:58.160]   Yeah, but one bad click.
[00:52:58.160 --> 00:52:59.040]   That's the problem.
[00:53:00.000 --> 00:53:04.960]   Okay, I'm going to confess, Meghan Markle, around the Oprah interview, I was like,
[00:53:04.960 --> 00:53:07.600]   I am here for this at the end of the pandemic.
[00:53:07.600 --> 00:53:10.560]   I'm the one person that wasn't interested in it.
[00:53:10.560 --> 00:53:16.080]   So I clicked and oh, it screwed up my algorithm.
[00:53:16.080 --> 00:53:16.480]   It does.
[00:53:16.480 --> 00:53:16.960]   Yeah.
[00:53:16.960 --> 00:53:22.960]   There's so there's so there's so there's so desperate and needy for signals that you actually
[00:53:22.960 --> 00:53:26.880]   don't I actually am aware of this and don't read articles because I don't want to give
[00:53:26.880 --> 00:53:29.440]   them any signals that I might want to read on these.
[00:53:29.440 --> 00:53:33.760]   Yeah, if it has real housewives in it, I don't care how link baby.
[00:53:33.760 --> 00:53:34.560]   I'm not clicking on it.
[00:53:34.560 --> 00:53:36.640]   Not doing it.
[00:53:36.640 --> 00:53:40.320]   If I read that gene story, I'd get ads for mom jeans for the rest of my life.
[00:53:40.320 --> 00:53:41.840]   I just don't want it.
[00:53:41.840 --> 00:53:44.640]   It is a real issue, Leo.
[00:53:44.640 --> 00:53:48.080]   This is a very big.
[00:53:48.080 --> 00:53:49.520]   Pain is real.
[00:53:49.520 --> 00:53:56.080]   Here's here's the this is the transition from 32 to 64 bit.
[00:53:56.080 --> 00:53:57.360]   So what's happening here?
[00:53:57.360 --> 00:54:02.400]   We're going to have to re architect our entire wardrobe.
[00:54:02.400 --> 00:54:04.480]   I need a 64 bit pant.
[00:54:04.480 --> 00:54:10.560]   But instead of 64 bit, it's like we don't actually know what kind of instruction set
[00:54:10.560 --> 00:54:11.520]   we're going for yet.
[00:54:11.520 --> 00:54:14.400]   Everyone's like, well, maybe we'll use it or not instruction sets.
[00:54:14.400 --> 00:54:14.640]   Sorry.
[00:54:14.640 --> 00:54:17.120]   Can I ask you a question actually as long as we're on jeans?
[00:54:17.120 --> 00:54:22.320]   So fashion moment with Stacy.
[00:54:22.320 --> 00:54:25.040]   I don't know if it's because I'm fat, you know, heavy around the middle.
[00:54:25.840 --> 00:54:31.040]   But all the pants sold for young people, this is a suit I got from Taylor
[00:54:31.040 --> 00:54:31.920]   Store, which I love.
[00:54:31.920 --> 00:54:36.720]   And it's tailored to me, but they're all low rise kind of hip hugger pants.
[00:54:36.720 --> 00:54:40.080]   And I know that because maybe it's because I'm old.
[00:54:40.080 --> 00:54:42.400]   I want to have my pants up to my nipples.
[00:54:42.400 --> 00:54:42.800]   I don't know.
[00:54:42.800 --> 00:54:46.320]   But I just I just feel like they're they're too low.
[00:54:46.320 --> 00:54:51.280]   And and I can't bring them higher because my, you know, the end of my legs,
[00:54:51.280 --> 00:54:52.960]   they won't that's where the pant meets.
[00:54:52.960 --> 00:54:54.160]   They won't that kick.
[00:54:54.160 --> 00:54:56.800]   But they seem so low that they're always about to fall off.
[00:54:56.800 --> 00:54:59.120]   What do you think, Dr. Higginbotham?
[00:54:59.120 --> 00:55:00.480]   Should I see a specialist?
[00:55:00.480 --> 00:55:05.040]   Well, what high rise pants?
[00:55:05.040 --> 00:55:06.800]   Do you?
[00:55:06.800 --> 00:55:08.880]   Well, I'm like, okay, I'm not a man.
[00:55:08.880 --> 00:55:08.880]   I'm not a man.
[00:55:08.880 --> 00:55:09.440]   I'm not a man.
[00:55:09.440 --> 00:55:10.400]   As I walk around.
[00:55:10.400 --> 00:55:12.160]   Man, her face.
[00:55:12.160 --> 00:55:13.280]   Spenders, a belt.
[00:55:13.280 --> 00:55:15.200]   I well, I think that's the choice.
[00:55:15.200 --> 00:55:17.680]   But I just feel like, why would you make pants?
[00:55:17.680 --> 00:55:20.320]   Is it that young people's hips are wider?
[00:55:20.320 --> 00:55:21.840]   That their guts are smaller?
[00:55:21.840 --> 00:55:22.480]   Why would you make--
[00:55:22.480 --> 00:55:23.360]   No, what happens is,
[00:55:23.920 --> 00:55:29.120]   well, I don't know if men ever had an option of a super high rise.
[00:55:29.120 --> 00:55:32.720]   Like right now in women's pants, you can get like an even an 11 inch rise,
[00:55:32.720 --> 00:55:33.520]   which is huge.
[00:55:33.520 --> 00:55:37.280]   Like that's the seam from your crotch, basically up to the waistband.
[00:55:37.280 --> 00:55:39.520]   That's the mom pants.
[00:55:39.520 --> 00:55:41.360]   I've been thinking of they go really high.
[00:55:41.360 --> 00:55:41.760]   Yeah.
[00:55:41.760 --> 00:55:45.760]   So I don't know guys like in the 40s, I think guys had that silhouette,
[00:55:45.760 --> 00:55:47.440]   actually, with like the suit pants.
[00:55:47.440 --> 00:55:47.760]   Yeah.
[00:55:47.760 --> 00:55:50.480]   Why can't we bring that back?
[00:55:50.480 --> 00:55:51.920]   I want plants to go over my belly.
[00:55:51.920 --> 00:55:52.800]   Probably fun.
[00:55:52.800 --> 00:55:55.520]   Just Google high rise pants for men.
[00:55:55.520 --> 00:55:56.160]   OK.
[00:55:56.160 --> 00:55:56.960]   So you're looking for--
[00:55:56.960 --> 00:56:00.240]   Yeah, that your internet feeds are going to be doomed forever.
[00:56:00.240 --> 00:56:00.800]   [LAUGHTER]
[00:56:00.800 --> 00:56:01.840]   Missed you with that in court.
[00:56:01.840 --> 00:56:04.480]   I bet you grab your wallet like this.
[00:56:04.480 --> 00:56:06.960]   [LAUGHTER]
[00:56:06.960 --> 00:56:07.680]   Over my back.
[00:56:07.680 --> 00:56:08.240]   Here's your issue.
[00:56:08.240 --> 00:56:10.880]   The problem is, is you get older and you get your belly?
[00:56:10.880 --> 00:56:12.800]   That's really the problem is the belly, isn't it?
[00:56:12.800 --> 00:56:12.960]   Yeah.
[00:56:12.960 --> 00:56:15.840]   Your shape gets messed up.
[00:56:15.840 --> 00:56:15.840]   Yeah.
[00:56:15.840 --> 00:56:16.320]   And so--
[00:56:16.320 --> 00:56:16.880]   And stuff--
[00:56:16.880 --> 00:56:17.680]   It's like a funnel.
[00:56:17.680 --> 00:56:18.880]   The pants just slide right down.
[00:56:18.880 --> 00:56:20.800]   Yeah, it's going to fall down unless you have elastic,
[00:56:20.800 --> 00:56:22.480]   which is kind of not what you want.
[00:56:22.480 --> 00:56:23.120]   No, I really want sweats.
[00:56:23.120 --> 00:56:24.720]   And then it's down to your suspenders.
[00:56:24.720 --> 00:56:26.000]   Yeah, sweats-- I think suspenders,
[00:56:26.000 --> 00:56:28.240]   that's what I did as a convert to suspenders.
[00:56:28.240 --> 00:56:30.320]   That's pretty much why I wear sweats most of the time.
[00:56:30.320 --> 00:56:32.720]   Sweats, you have no belly, sir.
[00:56:32.720 --> 00:56:33.680]   You are ripped.
[00:56:33.680 --> 00:56:34.640]   No, but but but but but but but but but but but but but but,
[00:56:34.640 --> 00:56:37.040]   but britches are a pain in the butt for me to find.
[00:56:37.040 --> 00:56:39.680]   They do because they're they don't fit right in the way.
[00:56:39.680 --> 00:56:41.040]   I love that you just called them britches.
[00:56:41.040 --> 00:56:41.520]   [LAUGHTER]
[00:56:41.520 --> 00:56:42.560]   And oh, sorry.
[00:56:42.560 --> 00:56:44.640]   [LAUGHTER]
[00:56:44.640 --> 00:56:45.440]   Oh, sorry.
[00:56:45.440 --> 00:56:46.280]   I just caught--
[00:56:46.280 --> 00:56:47.680]   [LAUGHTER]
[00:56:47.680 --> 00:56:48.800]   [LAUGHTER]
[00:56:48.800 --> 00:56:50.400]   I'm like, dad, is that you?
[00:56:50.400 --> 00:56:53.120]   Like, we're talking to George Washington here.
[00:56:53.120 --> 00:56:55.280]   [LAUGHTER]
[00:56:55.280 --> 00:56:56.480]   Oh, man, I'm sorry.
[00:56:56.480 --> 00:56:57.840]   I'm a suddener for life.
[00:56:57.840 --> 00:56:59.680]   [LAUGHTER]
[00:56:59.680 --> 00:57:00.280]   I call them--
[00:57:00.280 --> 00:57:00.760]   That was awesome.
[00:57:00.760 --> 00:57:02.400]   I call them knicker-bockers, my son.
[00:57:02.400 --> 00:57:05.760]   [LAUGHTER]
[00:57:05.760 --> 00:57:08.640]   But yeah, they're hard to buy for me because either the waste
[00:57:08.640 --> 00:57:12.960]   is too-- is not quite right or the thighs are not quite right.
[00:57:12.960 --> 00:57:15.240]   And it's been like that for ever in a day.
[00:57:15.240 --> 00:57:17.760]   So I just grabbed sweatpants and gym pants or--
[00:57:17.760 --> 00:57:20.000]   You just need a larger-sized pant.
[00:57:20.000 --> 00:57:21.560]   So you get your thighs circumference
[00:57:21.560 --> 00:57:22.960]   and then tailorm in at the waist.
[00:57:22.960 --> 00:57:23.440]   That's--
[00:57:23.440 --> 00:57:24.280]   There you go.
[00:57:24.280 --> 00:57:25.280]   Yeah, we're a lot of belts.
[00:57:25.280 --> 00:57:26.760]   All right.
[00:57:26.760 --> 00:57:27.800]   And we're a lot of belts.
[00:57:27.800 --> 00:57:30.000]   The ratchet belt is awesome.
[00:57:30.000 --> 00:57:36.120]   Why is the Daily Mail suing Google?
[00:57:36.120 --> 00:57:37.960]   I found this amusing.
[00:57:37.960 --> 00:57:39.360]   So the Daily Mail is--
[00:57:39.360 --> 00:57:40.360]   The Daily Mail is a notorious--
[00:57:40.360 --> 00:57:42.800]   It's a notorious trash sheet of trash sheet.
[00:57:42.800 --> 00:57:44.120]   Yeah, it's awful.
[00:57:44.120 --> 00:57:46.680]   So they're suing saying that Google
[00:57:46.680 --> 00:57:49.800]   doesn't favor them in the algorithm,
[00:57:49.800 --> 00:57:52.240]   which is what we pick up playing about.
[00:57:52.240 --> 00:57:55.320]   I don't want to see Daily Mail stories in my Google News.
[00:57:55.320 --> 00:57:56.560]   Exactly.
[00:57:56.560 --> 00:57:58.200]   That's why it abused me.
[00:57:58.200 --> 00:57:59.040]   It fits right through.
[00:57:59.040 --> 00:58:00.080]   It's Google's doing its job.
[00:58:00.080 --> 00:58:00.720]   Yeah.
[00:58:00.720 --> 00:58:01.200]   That's good.
[00:58:01.200 --> 00:58:01.720]   If there's no need--
[00:58:01.720 --> 00:58:04.160]   I'm going to file it under the court brief.
[00:58:04.160 --> 00:58:06.200]   Can we talk about something that I'm really excited about,
[00:58:06.200 --> 00:58:06.720]   or should we not?
[00:58:06.720 --> 00:58:08.280]   It's even a highlighted green story.
[00:58:08.280 --> 00:58:09.360]   We can.
[00:58:09.360 --> 00:58:10.560]   And it's not jeans.
[00:58:10.560 --> 00:58:13.680]   After this station identification,
[00:58:13.680 --> 00:58:17.040]   you're listening to the Twit podcast network.
[00:58:17.040 --> 00:58:22.000]   Stacey Higginbotham has a story for us.
[00:58:22.000 --> 00:58:22.920]   Go ahead, Stacey.
[00:58:22.920 --> 00:58:23.400]   OK.
[00:58:23.400 --> 00:58:27.040]   So I'm really excited because this morning, the EU
[00:58:27.040 --> 00:58:33.000]   issued its proposal for legislation around AI and AI apps.
[00:58:33.000 --> 00:58:34.440]   No, Jeff.
[00:58:34.440 --> 00:58:36.880]   This is actually a really good legislation.
[00:58:36.880 --> 00:58:37.800]   You read it.
[00:58:37.800 --> 00:58:38.760]   But you don't think this is good?
[00:58:38.760 --> 00:58:39.760]   I was impressed.
[00:58:39.760 --> 00:58:42.560]   I thought I was good.
[00:58:42.560 --> 00:58:46.080]   As we know, Jeff doesn't like any regulation of technology.
[00:58:46.080 --> 00:58:47.520]   No, it's not true.
[00:58:47.520 --> 00:58:47.880]   I don't like--
[00:58:47.880 --> 00:58:49.560]   I'm going to let Jeff talk to him in a minute.
[00:58:49.560 --> 00:58:50.520]   Do you explain it first?
[00:58:50.520 --> 00:58:51.920]   I've got to explain it first, yes.
[00:58:51.920 --> 00:58:53.560]   I'm going to tell you what they're going to do,
[00:58:53.560 --> 00:58:55.400]   or what they want to do.
[00:58:55.400 --> 00:58:58.480]   They're basically trying to classify
[00:58:58.480 --> 00:59:03.160]   the outcomes of your AI algorithm on what it is intended--
[00:59:03.160 --> 00:59:07.360]   or what your AI is intending to do and on the outcomes.
[00:59:07.360 --> 00:59:11.360]   So the risk levels are unacceptable for--
[00:59:11.360 --> 00:59:14.120]   they talk about things like coercion of minors
[00:59:14.120 --> 00:59:15.920]   and things like that.
[00:59:15.920 --> 00:59:18.920]   They also have the high risk areas.
[00:59:18.920 --> 00:59:21.000]   And this is where things like biometrics,
[00:59:21.000 --> 00:59:23.200]   that would be DNA scans, facial recognition,
[00:59:23.200 --> 00:59:26.720]   and iris scans all fall into high risk.
[00:59:26.720 --> 00:59:29.280]   And in the high risk category, what they're
[00:59:29.280 --> 00:59:32.480]   asking for companies to do is basically document
[00:59:32.480 --> 00:59:37.240]   their process of how their algorithm was built,
[00:59:37.240 --> 00:59:39.840]   the annotated, the data that goes into it,
[00:59:39.840 --> 00:59:44.800]   and then to actually test it when tested--
[00:59:44.800 --> 00:59:47.800]   I guess in a lab function-- to see what the implications are,
[00:59:47.800 --> 00:59:50.440]   and then to come back and continue to do that.
[00:59:50.440 --> 00:59:55.640]   So to me, I look at that, and I think, OK, that makes sense.
[00:59:55.640 --> 00:59:57.240]   It's fundamentally changing the way
[00:59:57.240 --> 01:00:00.000]   the software industry has worked, which is like, hey,
[01:00:00.000 --> 01:00:03.360]   let's throw some beta out there and see what happens.
[01:00:03.360 --> 01:00:05.680]   But I think the EU is trying to make a distinction
[01:00:05.680 --> 01:00:09.760]   in areas where it's not just a cool feature for your email
[01:00:09.760 --> 01:00:12.720]   provider, but this is like I'm setting bail for someone
[01:00:12.720 --> 01:00:17.800]   or a real life-changing kind of thing.
[01:00:17.800 --> 01:00:21.320]   They're like, hey, let's have you show your work
[01:00:21.320 --> 01:00:23.200]   and test it out a little bit before you release it
[01:00:23.200 --> 01:00:23.960]   to the public.
[01:00:23.960 --> 01:00:24.360]   I think that's--
[01:00:24.360 --> 01:00:25.280]   So that's my take.
[01:00:25.280 --> 01:00:26.160]   Very sensible.
[01:00:26.160 --> 01:00:27.960]   But Jeff, what's wrong with that?
[01:00:27.960 --> 01:00:31.160]   Well, and I think as it stands, Jaycee, you're right.
[01:00:31.160 --> 01:00:34.200]   This is not bad, but I can see danger at every bullet
[01:00:34.200 --> 01:00:35.000]   in the last year.
[01:00:35.000 --> 01:00:41.000]   So I said this the other day, on the other week on the show,
[01:00:41.000 --> 01:00:42.880]   I think that the biggest mistake the technology industry
[01:00:42.880 --> 01:00:44.200]   made in the last couple of decades
[01:00:44.200 --> 01:00:46.280]   was naming it artificial intelligence,
[01:00:46.280 --> 01:00:48.440]   because it became scare words.
[01:00:48.440 --> 01:00:50.920]   And even algorithm is a scare word.
[01:00:50.920 --> 01:00:53.520]   Data is a scare word now.
[01:00:53.520 --> 01:00:54.920]   Well, math is a scare word.
[01:00:54.920 --> 01:00:56.800]   What is math?
[01:00:56.800 --> 01:00:58.560]   What is math?
[01:00:58.560 --> 01:01:00.240]   Math.
[01:01:00.240 --> 01:01:01.240]   Oh, math, yes.
[01:01:01.240 --> 01:01:01.880]   That's true.
[01:01:01.880 --> 01:01:02.400]   That's true.
[01:01:02.400 --> 01:01:02.920]   That's true.
[01:01:02.920 --> 01:01:06.600]   What they say in England, maths.
[01:01:06.600 --> 01:01:07.280]   Which I never got.
[01:01:07.280 --> 01:01:08.680]   Why is it plural, math?
[01:01:08.680 --> 01:01:09.400]   Well, it's funny.
[01:01:09.400 --> 01:01:10.400]   It's a math.
[01:01:10.400 --> 01:01:10.960]   But so--
[01:01:10.960 --> 01:01:12.240]   Sport is singular.
[01:01:12.240 --> 01:01:13.000]   Oh, no, no, no.
[01:01:13.000 --> 01:01:13.640]   We're not going to go there.
[01:01:13.640 --> 01:01:14.640]   OK.
[01:01:14.640 --> 01:01:16.640]   [LAUGHTER]
[01:01:16.640 --> 01:01:17.880]   Keep going, Jeff.
[01:01:17.880 --> 01:01:19.880]   We'll never get back.
[01:01:19.880 --> 01:01:22.520]   Hank is down here slippery slope, man.
[01:01:22.520 --> 01:01:23.800]   That's called link bait.
[01:01:23.800 --> 01:01:24.440]   Where?
[01:01:24.440 --> 01:01:29.360]   You got me in your-- right by the-- oh, you got me right there.
[01:01:29.360 --> 01:01:31.720]   Like a hook and then fish in the mouth.
[01:01:31.720 --> 01:01:33.320]   So there's a side film.
[01:01:33.320 --> 01:01:34.680]   Well, yeah.
[01:01:34.680 --> 01:01:35.480]   Why is it that?
[01:01:35.480 --> 01:01:37.280]   Why is sports plural?
[01:01:37.280 --> 01:01:38.600]   But so singular.
[01:01:38.600 --> 01:01:41.360]   [LAUGHTER]
[01:01:41.360 --> 01:01:43.600]   But look, OK, I'm going to go through these bullet points.
[01:01:43.600 --> 01:01:44.200]   You could tell me--
[01:01:44.200 --> 01:01:45.640]   In a hospital.
[01:01:45.640 --> 01:01:46.560]   Why is it in a hospital?
[01:01:46.560 --> 01:01:47.080]   I'm sorry.
[01:01:47.080 --> 01:01:47.600]   Yeah.
[01:01:47.600 --> 01:01:48.520]   So for example--
[01:01:48.520 --> 01:01:49.600]   Jeff, to make his point--
[01:01:49.600 --> 01:01:52.440]   How do people create credit scores now?
[01:01:52.440 --> 01:01:54.400]   How do people create credit scores now?
[01:01:54.400 --> 01:01:56.640]   There are algorithms now.
[01:01:56.640 --> 01:01:58.960]   When you go to the bank, they have formula
[01:01:58.960 --> 01:02:01.840]   based on their experience that say people who do this
[01:02:01.840 --> 01:02:04.360]   are good credit risk, people who do that are bad credit risk.
[01:02:04.360 --> 01:02:05.520]   They've always done that.
[01:02:05.520 --> 01:02:09.280]   And they did it with scriveners and pens long ago.
[01:02:09.280 --> 01:02:11.040]   And then they did it more and more with formula.
[01:02:11.040 --> 01:02:12.360]   And they did the same thing with insurance.
[01:02:12.360 --> 01:02:13.200]   Now there's some moral--
[01:02:13.200 --> 01:02:16.600]   It says, oh, it's artificial intelligence.
[01:02:16.600 --> 01:02:17.080]   Right.
[01:02:17.080 --> 01:02:18.960]   Well, now it's an evil thing.
[01:02:18.960 --> 01:02:19.680]   Yeah, no.
[01:02:19.680 --> 01:02:20.200]   That's--
[01:02:20.200 --> 01:02:24.360]   That's why this isn't saying evil.
[01:02:24.360 --> 01:02:25.480]   Any algorithm can be--
[01:02:25.480 --> 01:02:27.840]   I'm going to say it's something that you conflated with AI
[01:02:27.840 --> 01:02:28.920]   and they're not the same thing.
[01:02:28.920 --> 01:02:30.560]   But I want to go through these bullet points.
[01:02:30.560 --> 01:02:32.040]   Because I agree with you, Stacey.
[01:02:32.040 --> 01:02:34.760]   This doesn't seem like a bridge too far.
[01:02:34.760 --> 01:02:38.560]   For instance, a ban on AI for indiscriminate surveillance,
[01:02:38.560 --> 01:02:40.880]   including systems that directly track individuals
[01:02:40.880 --> 01:02:41.440]   in physical environments.
[01:02:41.440 --> 01:02:41.640]   Oh, no.
[01:02:41.640 --> 01:02:43.600]   Please go to the actual law.
[01:02:43.600 --> 01:02:43.880]   OK.
[01:02:43.880 --> 01:02:48.680]   So this is-- the problem is the law has been leaked.
[01:02:48.680 --> 01:02:50.800]   It's a proposal that's circulating online.
[01:02:50.800 --> 01:02:52.760]   Is the draft out?
[01:02:52.760 --> 01:02:54.120]   No, no, it's the proposal.
[01:02:54.120 --> 01:02:54.960]   Yeah, here.
[01:02:54.960 --> 01:02:56.520]   Where do you want me to drop this?
[01:02:56.520 --> 01:02:58.120]   Just tell me where to find it.
[01:02:58.120 --> 01:02:59.600]   Because I'm reading the Verge article,
[01:02:59.600 --> 01:03:01.360]   but it was from a couple of days ago.
[01:03:01.360 --> 01:03:04.320]   Go to the ec.europa.eu.
[01:03:04.320 --> 01:03:04.820]   OK.
[01:03:04.820 --> 01:03:07.440]   And if you go to the commission's press page.
[01:03:07.440 --> 01:03:09.800]   Europe fit for the digital age.
[01:03:09.800 --> 01:03:12.240]   Commission proposes new rules and actions
[01:03:12.240 --> 01:03:16.640]   for excellence and trust in artificial intelligence.
[01:03:16.640 --> 01:03:17.640]   And I agree--
[01:03:17.640 --> 01:03:20.320]   Yeah, you can scroll down to where they start classically.
[01:03:20.320 --> 01:03:23.280]   There is high risk, critical infrastructures
[01:03:23.280 --> 01:03:25.520]   that could put the life and health of citizens at risk.
[01:03:25.520 --> 01:03:28.080]   I think this must be self-driving vehicles, things
[01:03:28.080 --> 01:03:29.360]   like that, right?
[01:03:29.360 --> 01:03:31.280]   Educational or vocational training
[01:03:31.280 --> 01:03:33.320]   that may determine access to education.
[01:03:33.320 --> 01:03:37.280]   Oh, AI systems should not be used in--
[01:03:37.280 --> 01:03:39.480]   dot, dot, dot, critical infrastructures.
[01:03:39.480 --> 01:03:41.360]   Educational or vocational training.
[01:03:41.360 --> 01:03:43.000]   They're not saying they shouldn't be used.
[01:03:43.000 --> 01:03:46.080]   And they're saying that if they're in these categories,
[01:03:46.080 --> 01:03:49.760]   you have to follow rules about basically explaining--
[01:03:49.760 --> 01:03:52.960]   It's basically showing your work and then auditing your work.
[01:03:52.960 --> 01:03:53.880]   And here is the--
[01:03:53.880 --> 01:03:54.880]   Sharing.
[01:03:54.880 --> 01:03:57.320]   Here are the obligations.
[01:03:57.320 --> 01:03:59.760]   Adequate risk assessment and mitigation systems,
[01:03:59.760 --> 01:04:01.360]   high quality of the data sets.
[01:04:01.360 --> 01:04:03.440]   In other words, not just a bunch of white people
[01:04:03.440 --> 01:04:06.160]   for face recognition, logging of activity
[01:04:06.160 --> 01:04:08.880]   to ensure traceability of results.
[01:04:08.880 --> 01:04:11.440]   This seems fair detailed documentation,
[01:04:11.440 --> 01:04:13.680]   providing all information necessary on the system
[01:04:13.680 --> 01:04:18.480]   and its purpose for authorities to assess its compliance.
[01:04:18.480 --> 01:04:21.280]   Clear and adequate information to the user.
[01:04:21.280 --> 01:04:22.200]   Yes.
[01:04:22.200 --> 01:04:24.040]   Appropriate human oversight measures
[01:04:24.040 --> 01:04:29.440]   to minimize risk, CF Robocop, and a high level
[01:04:29.440 --> 01:04:33.080]   of robustness, security, and accuracy.
[01:04:33.080 --> 01:04:36.920]   So if you look at Jeff, like your credit card scoring,
[01:04:36.920 --> 01:04:40.920]   you're like your FICO score, this is actually really--
[01:04:40.920 --> 01:04:45.240]   I think this is useful because it gives transparency
[01:04:45.240 --> 01:04:46.520]   to how those things are assessed.
[01:04:46.520 --> 01:04:48.160]   How did you calculate this?
[01:04:48.160 --> 01:04:49.920]   Not that you can't calculate it.
[01:04:49.920 --> 01:04:51.920]   But how did you get to this?
[01:04:51.920 --> 01:04:55.000]   And I think that's fair.
[01:04:55.000 --> 01:04:57.760]   Like we saw this with Stanford, actually,
[01:04:57.760 --> 01:05:01.160]   in their COVID vaccine algorithm for flowchart,
[01:05:01.160 --> 01:05:03.720]   however you want to call it, when they decided
[01:05:03.720 --> 01:05:06.520]   who got the vaccine and then suddenly they were like,
[01:05:06.520 --> 01:05:11.760]   oh, this prioritized, older people who were not
[01:05:11.760 --> 01:05:12.560]   on the front lines.
[01:05:12.560 --> 01:05:15.360]   And they Stanford published it.
[01:05:15.360 --> 01:05:17.960]   And then everyone said, oh, here's your issue.
[01:05:17.960 --> 01:05:20.920]   That's the process that we want these things to go through.
[01:05:20.920 --> 01:05:24.560]   And this law says these following classes
[01:05:24.560 --> 01:05:25.840]   will have to do this.
[01:05:25.840 --> 01:05:27.080]   So you can't just shrug it off.
[01:05:27.080 --> 01:05:30.880]   It's basically doing what Stanford did.
[01:05:30.880 --> 01:05:33.600]   And companies don't want this because they
[01:05:33.600 --> 01:05:35.160]   don't want that transparency.
[01:05:35.160 --> 01:05:37.000]   Some of them may feel it's competitive advantage.
[01:05:37.000 --> 01:05:40.560]   And it does change the way that we release products
[01:05:40.560 --> 01:05:42.720]   and will slow things down somewhat.
[01:05:42.720 --> 01:05:43.760]   I think it's also--
[01:05:43.760 --> 01:05:45.920]   Well, actually, the big companies may be in favor of it
[01:05:45.920 --> 01:05:49.080]   because they can afford to do this for us.
[01:05:49.080 --> 01:05:50.600]   Startups can't.
[01:05:50.600 --> 01:05:51.520]   They do distinguish between--
[01:05:51.520 --> 01:05:54.320]   Well, but I don't think you should be a startup if you can't--
[01:05:54.320 --> 01:05:56.600]   if you're going to put out an AI algorithm that
[01:05:56.600 --> 01:06:00.520]   tracks someone's bail and says, I think you are morally
[01:06:00.520 --> 01:06:01.240]   responsible.
[01:06:01.240 --> 01:06:03.120]   You have to be responsible for those actions.
[01:06:03.120 --> 01:06:04.080]   I don't think that's a problem.
[01:06:04.080 --> 01:06:05.640]   We want to know how they calculate that.
[01:06:05.640 --> 01:06:06.480]   I agree with you.
[01:06:06.480 --> 01:06:10.440]   It's like you have to have a license to be a lawyer or a doctor.
[01:06:10.440 --> 01:06:13.000]   I'm not saying all licensing is good.
[01:06:13.000 --> 01:06:15.080]   Reading hair seems a bit excessive.
[01:06:15.080 --> 01:06:16.560]   I think quite importantly, they make
[01:06:16.560 --> 01:06:20.160]   distinction between uses of AI with limited risk
[01:06:20.160 --> 01:06:21.640]   and minimal risk.
[01:06:21.640 --> 01:06:23.840]   They say, for instance, the proposal
[01:06:23.840 --> 01:06:26.800]   allows the free uses of applications such as AI-enabled
[01:06:26.800 --> 01:06:29.280]   video games or spam filters.
[01:06:29.280 --> 01:06:32.920]   The vast majority of AI systems fall into this category
[01:06:32.920 --> 01:06:35.520]   and the draft regulation does not intervene here.
[01:06:35.520 --> 01:06:38.960]   So they do at least say, well, these are--
[01:06:38.960 --> 01:06:41.640]   this is uses of AI that are de minimis.
[01:06:41.640 --> 01:06:43.960]   And these are the ones that might be more significant.
[01:06:43.960 --> 01:06:44.880]   And I think the--
[01:06:44.880 --> 01:06:50.400]   But I guess part of my question is, why is it just AI though?
[01:06:50.400 --> 01:06:51.800]   It should be algorithms.
[01:06:51.800 --> 01:06:52.840]   But you know, this is--
[01:06:52.840 --> 01:06:53.840]   This is algorithms.
[01:06:53.840 --> 01:06:55.640]   And that's really--
[01:06:55.640 --> 01:06:58.640]   I think they say AI because AI is understood and well known
[01:06:58.640 --> 01:06:59.160]   and all that.
[01:06:59.160 --> 01:07:01.240]   But they really should just say computer algorithms.
[01:07:01.240 --> 01:07:02.240]   Because that's what--
[01:07:02.240 --> 01:07:04.240]   Well, and they define that.
[01:07:04.240 --> 01:07:06.760]   So they talk about AI systems.
[01:07:06.760 --> 01:07:09.480]   They talk about apps.
[01:07:09.480 --> 01:07:12.280]   I think they talk about the definition somewhere in here.
[01:07:12.280 --> 01:07:15.040]   But I can't remember.
[01:07:15.040 --> 01:07:18.640]   But ideally--
[01:07:18.640 --> 01:07:21.240]   This stuff is not clearly AI.
[01:07:21.240 --> 01:07:24.840]   For instance, they say biometric identification systems.
[01:07:24.840 --> 01:07:25.840]   That's not--
[01:07:25.840 --> 01:07:28.600]   They're basically saying, when you put a computer in charge
[01:07:28.600 --> 01:07:32.320]   of making a decision about a person or a thing--
[01:07:32.320 --> 01:07:32.840]   This--
[01:07:32.840 --> 01:07:33.440]   That's right.
[01:07:33.440 --> 01:07:35.080]   --here are the cases where we probably
[01:07:35.080 --> 01:07:36.040]   need a little more oversight.
[01:07:36.040 --> 01:07:36.960]   Right.
[01:07:36.960 --> 01:07:37.760]   Yeah.
[01:07:37.760 --> 01:07:41.040]   I think this is completely appropriate, to be honest with you.
[01:07:41.040 --> 01:07:43.880]   And at this level, I won't disagree.
[01:07:43.880 --> 01:07:48.040]   But the funny thing is, I fear, fear.
[01:07:48.040 --> 01:07:49.040]   You know, I understand.
[01:07:49.040 --> 01:07:49.440]   I fear--
[01:07:49.440 --> 01:07:50.560]   I completely agree.
[01:07:50.560 --> 01:07:53.560]   I fear them going--
[01:07:53.560 --> 01:07:56.240]   But at the same time, your regulation has worked out to date.
[01:07:56.240 --> 01:07:57.960]   And it's pretty crappy.
[01:07:57.960 --> 01:07:58.760]   I don't like that cooking.
[01:07:58.760 --> 01:08:01.320]   That's a gay, isn't awful log.
[01:08:01.320 --> 01:08:03.680]   Because it's out of fear.
[01:08:03.680 --> 01:08:08.800]   But Jeff, you would, I think, agree that it is prudent and reasonable
[01:08:08.800 --> 01:08:13.560]   to consider AI and its use and to put some limits on it,
[01:08:13.560 --> 01:08:15.560]   or at least some requirements on it, right?
[01:08:15.560 --> 01:08:19.120]   I tend to think that if you want to regulate surgery,
[01:08:19.120 --> 01:08:20.600]   you regulate surgery.
[01:08:20.600 --> 01:08:22.840]   And whether it's AI or not, doesn't really matter.
[01:08:22.840 --> 01:08:25.000]   It's how you regulate that activity.
[01:08:25.000 --> 01:08:25.760]   So don't call it AI.
[01:08:25.760 --> 01:08:26.840]   Call it decision--
[01:08:26.840 --> 01:08:28.280]   Or a computer decision making.
[01:08:28.280 --> 01:08:28.880]   Either way.
[01:08:28.880 --> 01:08:30.760]   Well, even why is a computer system making?
[01:08:30.760 --> 01:08:33.080]   Humans make a lot of bad decisions, too.
[01:08:33.080 --> 01:08:35.880]   Yeah, but it's a little harder to investigate their algorithm.
[01:08:35.880 --> 01:08:37.360]   But when a human makes a decision--
[01:08:37.360 --> 01:08:39.400]   Yeah, you can ask them--
[01:08:39.400 --> 01:08:40.480]   Like, if you know what's a--
[01:08:40.480 --> 01:08:42.240]   I mean, that's why people hate bureaucracies.
[01:08:42.240 --> 01:08:47.360]   But you can go to a person and say, stop doing that, right?
[01:08:47.360 --> 01:08:50.080]   You have the chance to say, let me talk to your manager.
[01:08:50.080 --> 01:08:53.000]   When you go to Facebook because you disagree with their algorithm
[01:08:53.000 --> 01:08:55.560]   or you find it harms people, you're stuck in this--
[01:08:55.560 --> 01:08:57.760]   and there was actually a really good article about getting stuck
[01:08:57.760 --> 01:09:01.000]   in the Facebook bureaucracy where they've intentioned--
[01:09:01.000 --> 01:09:02.160]   I don't know if it's intentional or not.
[01:09:02.160 --> 01:09:06.680]   They built systems that when you're badly affected by it,
[01:09:06.680 --> 01:09:08.880]   there's no where to go.
[01:09:08.880 --> 01:09:10.760]   There's no way to really address it,
[01:09:10.760 --> 01:09:14.200]   unless you can get media attention and then people call it.
[01:09:14.200 --> 01:09:14.480]   Right.
[01:09:14.480 --> 01:09:17.960]   So I'll say regulate the companies for any behavior,
[01:09:17.960 --> 01:09:20.200]   not just AI.
[01:09:20.200 --> 01:09:22.640]   Then whatever behavior you find difficult and dangerous,
[01:09:22.640 --> 01:09:24.680]   whatever the cause on it--
[01:09:24.680 --> 01:09:26.000]   OK, that's fine.
[01:09:26.000 --> 01:09:27.040]   Of course, that's fine.
[01:09:27.040 --> 01:09:29.760]   You need to regulate at a technology level here,
[01:09:29.760 --> 01:09:35.000]   because if you regulate by the company's individual deployment
[01:09:35.000 --> 01:09:38.800]   of something, it's too one off.
[01:09:38.800 --> 01:09:42.080]   And these are broad, general rules of a big class.
[01:09:42.080 --> 01:09:44.040]   But every company will have their own algorithms,
[01:09:44.040 --> 01:09:46.000]   and they'll all be different, and they'll be proprietary.
[01:09:46.000 --> 01:09:47.480]   I know, but we can see them.
[01:09:47.480 --> 01:09:48.240]   That's the beauty.
[01:09:48.240 --> 01:09:50.000]   When you go to Facebook and you say,
[01:09:50.000 --> 01:09:52.320]   hey, our presidential election is all screwed up,
[01:09:52.320 --> 01:09:54.240]   and we have anti-vaxxers everywhere,
[01:09:54.240 --> 01:09:57.760]   we would like to understand why this is happening.
[01:09:57.760 --> 01:10:00.640]   Facebook can say, here's the process
[01:10:00.640 --> 01:10:03.240]   by which we're feeding information to people.
[01:10:03.240 --> 01:10:05.680]   And here's why it's doing that.
[01:10:05.680 --> 01:10:08.320]   In companies, just so you know, Jeff, companies already have--
[01:10:08.320 --> 01:10:11.280]   there's a field called MLOps.
[01:10:11.280 --> 01:10:14.440]   They already have the things in place to document
[01:10:14.440 --> 01:10:17.360]   their data science, like how they built the algorithm,
[01:10:17.360 --> 01:10:19.320]   they document their data sets.
[01:10:19.320 --> 01:10:22.120]   That's already in existence at any company building this,
[01:10:22.120 --> 01:10:25.680]   because if their data scientist kills over,
[01:10:25.680 --> 01:10:28.800]   they need to be able to say, oh, well, they're gone.
[01:10:28.800 --> 01:10:31.400]   Now we have to solve-- we have to do this ourselves, right?
[01:10:31.400 --> 01:10:34.440]   So it's not a new burden for anybody
[01:10:34.440 --> 01:10:37.560]   who's doing data science at any level.
[01:10:37.560 --> 01:10:40.240]   It's the audit in the accountability.
[01:10:40.240 --> 01:10:41.240]   Two weeks ago--
[01:10:41.240 --> 01:10:42.880]   I think I may go ahead.
[01:10:42.880 --> 01:10:46.000]   Two weeks ago on this weekend, a price tech, Trevini--
[01:10:46.000 --> 01:10:48.560]   Ms. Trevini Gandhi, I believe was her name.
[01:10:48.560 --> 01:10:51.440]   She was pretty much talking about all of this.
[01:10:51.440 --> 01:10:55.760]   She's a data science senior, data scientist, I believe,
[01:10:55.760 --> 01:10:56.760]   for data crew.
[01:10:56.760 --> 01:11:00.520]   And they talk all about this MLOps
[01:11:00.520 --> 01:11:04.800]   and all of the issues with ethics behind it.
[01:11:04.800 --> 01:11:07.520]   And I believe we even got into a discussion
[01:11:07.520 --> 01:11:12.080]   about looking at it the way Asimov had the robotics law.
[01:11:12.080 --> 01:11:12.720]   Was it the--
[01:11:12.720 --> 01:11:13.720]   Yeah.
[01:11:13.720 --> 01:11:14.480]   Uh-huh.
[01:11:14.480 --> 01:11:19.160]   That's widely considered to be bogus, but OK.
[01:11:19.160 --> 01:11:20.800]   The laws of robotics are making sense.
[01:11:20.800 --> 01:11:23.000]   Laws of robotics are inherently consistent,
[01:11:23.000 --> 01:11:24.720]   but that's fine.
[01:11:24.720 --> 01:11:27.280]   My only other point, Stacey, is this
[01:11:27.280 --> 01:11:29.280]   will go to your favorite thing for me?
[01:11:29.280 --> 01:11:33.720]   Is what leads to a fear regime that leads to moral panic?
[01:11:33.720 --> 01:11:35.520]   I can give you an example from the US.
[01:11:35.520 --> 01:11:38.560]   Representative Tom Mowalski, who is my representative
[01:11:38.560 --> 01:11:40.120]   in New Jersey, who I think is great.
[01:11:40.120 --> 01:11:42.000]   And I've supported him full disclosure and give it
[01:11:42.000 --> 01:11:42.360]   him money.
[01:11:42.360 --> 01:11:44.800]   And when we could knock on doors, I'd knock on doors from him.
[01:11:44.800 --> 01:11:47.040]   He's come up with probably the best effort
[01:11:47.040 --> 01:11:49.200]   to slice off 230.
[01:11:49.200 --> 01:11:50.200]   I still think it's bad.
[01:11:50.200 --> 01:11:51.840]   I still think there's a lot of problems with it.
[01:11:51.840 --> 01:11:54.200]   And I attended a session with him
[01:11:54.200 --> 01:11:58.040]   and then talked to a few people on Capitol Hill about this.
[01:11:58.040 --> 01:12:00.840]   One of my biggest complaints is the nickname of the bill.
[01:12:00.840 --> 01:12:03.360]   The dangerous algorithm is built.
[01:12:03.360 --> 01:12:07.320]   And so when we start figuring out
[01:12:07.320 --> 01:12:09.960]   artificial intelligence and dangerous algorithms,
[01:12:09.960 --> 01:12:13.800]   we have a larger impact on the adoption of technology
[01:12:13.800 --> 01:12:16.800]   and the use and some of the stupid regulation that
[01:12:16.800 --> 01:12:17.880]   can then follow.
[01:12:17.880 --> 01:12:21.960]   So I'm always sensitive to that and how it's positioned.
[01:12:21.960 --> 01:12:23.240]   That's part of my problem here.
[01:12:23.240 --> 01:12:26.080]   So this is about artificial intelligence.
[01:12:26.080 --> 01:12:28.280]   If we've never named an artificial intelligence,
[01:12:28.280 --> 01:12:30.640]   I don't know that this law would exist.
[01:12:30.640 --> 01:12:31.920]   There might have been something else.
[01:12:31.920 --> 01:12:32.920]   I think it--
[01:12:32.920 --> 01:12:34.600]   You had an ML operation.
[01:12:34.600 --> 01:12:35.880]   There were other things to know.
[01:12:35.880 --> 01:12:38.800]   But by putting it up in that kind of labeling,
[01:12:38.800 --> 01:12:42.720]   it often leads to problems.
[01:12:42.720 --> 01:12:45.240]   I am talking-- well, OK.
[01:12:45.240 --> 01:12:48.000]   I'm just going to go with-- I think you agree with me
[01:12:48.000 --> 01:12:52.800]   and you're mad at the fact that it uses AI.
[01:12:52.800 --> 01:12:56.240]   No, I worry about how they'll f this up.
[01:12:56.240 --> 01:13:01.240]   Because I saw how they f'd up privacy in a lot of ways.
[01:13:01.240 --> 01:13:02.240]   Not as bad as other reality--
[01:13:02.240 --> 01:13:05.480]   GDPR is actually not a terrible--
[01:13:05.480 --> 01:13:08.920]   GDPR in the EU is actually a decent law.
[01:13:08.920 --> 01:13:11.600]   GDPR is an OK law, but it has problems.
[01:13:11.600 --> 01:13:12.960]   It has some wackiness in there.
[01:13:12.960 --> 01:13:14.440]   Well, every law has problems.
[01:13:14.440 --> 01:13:15.440]   It's a BII.
[01:13:15.440 --> 01:13:15.960]   I mean--
[01:13:15.960 --> 01:13:17.480]   There is no such thing as perfection.
[01:13:17.480 --> 01:13:21.960]   We're making a tempting to make progress towards perfection.
[01:13:21.960 --> 01:13:23.600]   And I think you've got to take a step.
[01:13:23.600 --> 01:13:26.440]   You can't take no steps saying, well, we can't get it right.
[01:13:26.440 --> 01:13:27.320]   Let's not do anything.
[01:13:27.320 --> 01:13:29.480]   But you also need a voice of caution about this, which I am.
[01:13:29.480 --> 01:13:30.480]   I'm glad you are.
[01:13:30.480 --> 01:13:31.680]   And you're absolutely right.
[01:13:31.680 --> 01:13:33.520]   And sure, they have screwed things up sometimes.
[01:13:33.520 --> 01:13:35.720]   As you say, Stacey, we have to have a conversation about it.
[01:13:35.720 --> 01:13:37.200]   And this is that conversation.
[01:13:37.200 --> 01:13:39.000]   What could go wrong with theory and everything that
[01:13:39.000 --> 01:13:40.880]   could go wrong is the conversation.
[01:13:40.880 --> 01:13:42.520]   Well, and one thing I note in the--
[01:13:42.520 --> 01:13:44.800]   at least, and this isn't the law, but the proposal
[01:13:44.800 --> 01:13:46.880]   is they don't actually define AI.
[01:13:46.880 --> 01:13:47.960]   They talk about the benefits of AI.
[01:13:47.960 --> 01:13:49.440]   Yeah, they don't have the definition here.
[01:13:49.440 --> 01:13:50.320]   They need a definition.
[01:13:50.320 --> 01:13:53.640]   And I think that's part of the-- that is actually a problem.
[01:13:53.640 --> 01:13:55.840]   Because that's what you're regulating.
[01:13:55.840 --> 01:13:56.480]   What's the line?
[01:13:56.480 --> 01:13:59.600]   But computer algorithms that impact human behavior
[01:13:59.600 --> 01:14:02.760]   should probably be regulated to some degree.
[01:14:02.760 --> 01:14:05.200]   At the very least, requiring transparency.
[01:14:05.200 --> 01:14:08.840]   That's kind of minimum.
[01:14:08.840 --> 01:14:10.600]   So what the Mellon-Ospiel does?
[01:14:10.600 --> 01:14:12.360]   Well, they have--
[01:14:12.360 --> 01:14:13.320]   Go ahead.
[01:14:13.320 --> 01:14:14.360]   Go ahead.
[01:14:14.360 --> 01:14:16.000]   Go ahead.
[01:14:16.000 --> 01:14:20.160]   I was going to say they kind of have divided this up into--
[01:14:20.160 --> 01:14:26.120]   they call it the machinery products, which is the hardware.
[01:14:26.120 --> 01:14:30.480]   And then the AI is basically pretty much the software side
[01:14:30.480 --> 01:14:33.360]   is really kind of what it looks like they're doing here.
[01:14:33.360 --> 01:14:35.760]   And they have new machinery regulation.
[01:14:35.760 --> 01:14:36.880]   So they're using--
[01:14:36.880 --> 01:14:38.720]   I mean, could they say software?
[01:14:38.720 --> 01:14:40.000]   Sure.
[01:14:40.000 --> 01:14:41.600]   But you know how politicians--
[01:14:41.600 --> 01:14:45.760]   And that kind of supports a little bit Jeff's argument.
[01:14:45.760 --> 01:14:49.080]   Because until you say what you're regulating in a way--
[01:14:49.080 --> 01:14:49.580]   Right.
[01:14:49.580 --> 01:14:50.080]   Clear.
[01:14:50.080 --> 01:14:51.080]   --that are said that I did.
[01:14:51.080 --> 01:14:51.840]   Yeah.
[01:14:51.840 --> 01:14:52.600]   Then I think maybe--
[01:14:52.600 --> 01:14:53.160]   Then it could be any of you.
[01:14:53.160 --> 01:14:55.040]   --you need to go back to the table and say, well,
[01:14:55.040 --> 01:14:56.840]   exactly, you're regulating here.
[01:14:56.840 --> 01:15:00.720]   And if you're placed AI with algorithms, computer algorithms,
[01:15:00.720 --> 01:15:03.760]   I'd be comfortable with what they're proposing here.
[01:15:03.760 --> 01:15:07.400]   You can't say it's AI that TikTok uses or Facebook
[01:15:07.400 --> 01:15:09.800]   uses to cultivate the newsfeed or TikTok
[01:15:09.800 --> 01:15:12.840]   uses to put on the front page.
[01:15:12.840 --> 01:15:13.760]   That's not AI.
[01:15:13.760 --> 01:15:15.400]   That's an algorithm.
[01:15:15.400 --> 01:15:15.760]   But I think--
[01:15:15.760 --> 01:15:17.440]   Even then, they should be transparent.
[01:15:17.440 --> 01:15:18.120]   I think it totally--
[01:15:18.120 --> 01:15:22.160]   The Mellon-Ospiel says basically, algorithms are bad.
[01:15:22.160 --> 01:15:24.640]   But if you do things in chronological order,
[01:15:24.640 --> 01:15:25.140]   that's OK.
[01:15:25.140 --> 01:15:26.960]   Well, that can get gained if that is anything.
[01:15:26.960 --> 01:15:27.880]   That's not true.
[01:15:27.880 --> 01:15:29.360]   Yeah.
[01:15:29.360 --> 01:15:31.080]   Algorithms aren't per se bad.
[01:15:31.080 --> 01:15:34.000]   We should be transparent.
[01:15:34.000 --> 01:15:36.520]   Because if the algorithm says, no fat people
[01:15:36.520 --> 01:15:39.320]   on the front page, you'd want to know that.
[01:15:39.320 --> 01:15:42.480]   But the issue is what you want to know is the impact of it.
[01:15:42.480 --> 01:15:44.280]   Whoever made that algorithm, whatever made it a bit--
[01:15:44.280 --> 01:15:45.080]   They may not know it.
[01:15:45.080 --> 01:15:46.200]   It's more about studying--
[01:15:46.200 --> 01:15:47.680]   That's what this is calling for.
[01:15:47.680 --> 01:15:51.840]   This is saying when you put out an algorithm before you
[01:15:51.840 --> 01:15:53.200]   put out the algorithm, you have
[01:15:53.200 --> 01:15:55.880]   to assess how it will affect people.
[01:15:55.880 --> 01:15:56.360]   Yes.
[01:15:56.360 --> 01:15:58.280]   You may not know until it's out.
[01:15:58.280 --> 01:15:59.120]   Well, do your best.
[01:15:59.120 --> 01:15:59.960]   And then you need to check--
[01:15:59.960 --> 01:16:01.640]   Well, you have to do it.
[01:16:01.640 --> 01:16:03.920]   And then there's presumably-- this
[01:16:03.920 --> 01:16:07.160]   is what the appropriate human oversight offers
[01:16:07.160 --> 01:16:12.560]   is saying that oversight will be used to minimize risk.
[01:16:12.560 --> 01:16:15.680]   So that's saying, hey, when we see that this algorithm is
[01:16:15.680 --> 01:16:21.960]   causing harm, we can go back and say, yo, this is happening.
[01:16:21.960 --> 01:16:23.000]   Fix it.
[01:16:23.000 --> 01:16:24.480]   Google's-- How did you get to this?
[01:16:24.480 --> 01:16:28.680]   Again, they're moving profits offshore.
[01:16:28.680 --> 01:16:31.840]   Google's subsidiary moved $75 billion in profits
[01:16:31.840 --> 01:16:36.640]   through Ireland 2019 using the double Irish.
[01:16:36.640 --> 01:16:38.840]   I thought that we--
[01:16:38.840 --> 01:16:39.400]   No.
[01:16:39.400 --> 01:16:40.560]   Close that loophole.
[01:16:40.560 --> 01:16:43.720]   The big change is the Biden administration,
[01:16:43.720 --> 01:16:46.720]   the Treasury Department, is finally suggesting
[01:16:46.720 --> 01:16:49.040]   international agreements on how to tax these companies.
[01:16:49.040 --> 01:16:52.160]   That's what you need is international agreements
[01:16:52.160 --> 01:16:54.280]   to crack down on this stuff.
[01:16:54.280 --> 01:16:57.680]   But right now, they're just doing what their stockholders want.
[01:16:57.680 --> 01:16:59.240]   And no, it hasn't gone away.
[01:16:59.240 --> 01:17:01.760]   This is about law and diplomacy.
[01:17:01.760 --> 01:17:05.840]   So the way they're avoiding taxes on that $75 billion
[01:17:05.840 --> 01:17:09.320]   in profits is moving it through a subsidiary of Google,
[01:17:09.320 --> 01:17:14.840]   incorporated in Ireland, and then registered--
[01:17:14.840 --> 01:17:17.640]   I'm sorry, for the purposes of paying taxes,
[01:17:17.640 --> 01:17:23.480]   domiciled in Bermuda where the tax is 0%.
[01:17:23.480 --> 01:17:25.920]   It's called the double Irish.
[01:17:25.920 --> 01:17:28.120]   The company that these profits move through
[01:17:28.120 --> 01:17:31.080]   has zero employees.
[01:17:31.080 --> 01:17:32.520]   So it's not really a company.
[01:17:32.520 --> 01:17:36.640]   It's an entity created specifically to avoid taxes.
[01:17:36.640 --> 01:17:40.320]   And I guess it's not illegal, or Google wouldn't be doing it.
[01:17:40.320 --> 01:17:41.880]   And of course, that's Google's defenses.
[01:17:41.880 --> 01:17:44.360]   We're just doing what we are legally allowed to do.
[01:17:44.360 --> 01:17:45.720]   But of course, we have to remember
[01:17:45.720 --> 01:17:48.880]   that the reason that loopholes like this are in the law
[01:17:48.880 --> 01:17:51.640]   is because lobbyists from Google and other companies
[01:17:51.640 --> 01:17:54.560]   went to legislators and said, by the way,
[01:17:54.560 --> 01:17:57.440]   Google's not the only business doing this stuff.
[01:17:57.440 --> 01:18:00.360]   No, not Google by any means, absolutely.
[01:18:00.360 --> 01:18:01.840]   This is the same longer.
[01:18:01.840 --> 01:18:03.360]   Yeah.
[01:18:03.360 --> 01:18:07.280]   I mean, how many of y'all protest your tax
[01:18:07.280 --> 01:18:09.520]   of property tax appraisals?
[01:18:09.520 --> 01:18:10.160]   I don't.
[01:18:10.160 --> 01:18:10.840]   Can I?
[01:18:10.840 --> 01:18:11.680]   Am I allowed to?
[01:18:14.840 --> 01:18:16.280]   Yes, you can dispute.
[01:18:16.280 --> 01:18:18.800]   Well, in Texas, you could dispute your tax appraisals.
[01:18:18.800 --> 01:18:20.720]   You could dispute anything in Texas.
[01:18:20.720 --> 01:18:23.240]   Your arm sort of scared of it.
[01:18:23.240 --> 01:18:23.720]   There you go.
[01:18:23.720 --> 01:18:25.880]   Well, I'm just saying that the--
[01:18:25.880 --> 01:18:26.640]   I'm sure I could--
[01:18:26.640 --> 01:18:27.160]   --the more I could--
[01:18:27.160 --> 01:18:28.480]   Yeah, I'm sure I could go and say--
[01:18:28.480 --> 01:18:29.400]   I think I have to take advantage of it.
[01:18:29.400 --> 01:18:31.280]   Hey, my house isn't worth that much.
[01:18:31.280 --> 01:18:32.080]   What are you talking about?
[01:18:32.080 --> 01:18:33.160]   I did ask one time, Stacey.
[01:18:33.160 --> 01:18:34.520]   I did ask one time my guy.
[01:18:34.520 --> 01:18:39.040]   And he was incredibly knowledgeable about every house
[01:18:39.040 --> 01:18:40.360]   of the block and what was different.
[01:18:40.360 --> 01:18:41.240]   And he beat me down.
[01:18:41.240 --> 01:18:42.760]   I was even trying to fight so much to say,
[01:18:42.760 --> 01:18:43.800]   I don't understand this.
[01:18:43.800 --> 01:18:44.800]   But he was unbelievable.
[01:18:44.800 --> 01:18:47.040]   Let me explain to you, Mr. Jarvis.
[01:18:47.040 --> 01:18:48.000]   Yes, and he did.
[01:18:48.000 --> 01:18:48.680]   He did.
[01:18:48.680 --> 01:18:50.560]   He asked you want me to do my job.
[01:18:50.560 --> 01:18:52.240]   Even I was left speechless.
[01:18:52.240 --> 01:18:54.560]   This was, by the way, the last year, 2019,
[01:18:54.560 --> 01:18:57.040]   that Google was taking advantage of the double Irish
[01:18:57.040 --> 01:18:59.400]   with the Bermuda Rach around.
[01:18:59.400 --> 01:19:01.960]   But it changed its tax structure that year.
[01:19:01.960 --> 01:19:04.720]   And Google was quick to point that out.
[01:19:04.720 --> 01:19:09.160]   In December 2019, in line with the OECD's BEPS conclusions
[01:19:09.160 --> 01:19:11.200]   and changes to US and Irish tax laws,
[01:19:11.200 --> 01:19:13.160]   we simplified our corporate structure
[01:19:13.160 --> 01:19:18.800]   and started licensing our IP from the US, not Bermuda,
[01:19:18.800 --> 01:19:21.560]   including all annual and one-time income taxes
[01:19:21.560 --> 01:19:22.840]   over the past 10 years.
[01:19:22.840 --> 01:19:26.280]   Our global effects of tax rate has been over 20%.
[01:19:26.280 --> 01:19:29.320]   What is our global effective tax rate, honey?
[01:19:29.320 --> 01:19:31.040]   30-- what? 48%.
[01:19:31.040 --> 01:19:32.240]   OK, he's checking.
[01:19:32.240 --> 01:19:34.920]   Google's only paying 20.
[01:19:34.920 --> 01:19:36.520]   Just checking.
[01:19:36.520 --> 01:19:37.760]   Well, that's the thing about the fight
[01:19:37.760 --> 01:19:40.160]   over the infrastructure bill and the tax rate.
[01:19:40.160 --> 01:19:43.040]   Tax rate is show. It's meaningless.
[01:19:43.040 --> 01:19:45.240]   It's the effective tax rate you want to get to.
[01:19:45.240 --> 01:19:48.480]   Yeah. 25, 28%, 21%, whatever.
[01:19:48.480 --> 01:19:51.120]   That's meaningless.
[01:19:51.120 --> 01:19:53.720]   Well, I'll have to have you explain that sometime to me, Jeff.
[01:19:53.720 --> 01:19:56.760]   But for now, I'm going to have to appeal my property tax.
[01:19:56.760 --> 01:19:57.120]   So--
[01:19:57.120 --> 01:19:57.880]   Yeah.
[01:19:57.880 --> 01:19:59.800]   [LAUGHTER]
[01:19:59.800 --> 01:20:00.560]   Stay tuned.
[01:20:00.560 --> 01:20:02.520]   I don't know.
[01:20:02.520 --> 01:20:04.720]   No, I think it's probably something--
[01:20:04.720 --> 01:20:05.840]   I'm sure I could do it.
[01:20:05.840 --> 01:20:08.360]   But I suspect that something's done more often
[01:20:08.360 --> 01:20:10.760]   in the great state of Texas than here in.
[01:20:10.760 --> 01:20:12.760]   Well, if you hire people to do it for you--
[01:20:12.760 --> 01:20:14.880]   Now, people's a public of California.
[01:20:14.880 --> 01:20:17.040]   We don't do those kinds of things.
[01:20:17.040 --> 01:20:17.560]   Yeah.
[01:20:17.560 --> 01:20:19.440]   Well, I was always against it.
[01:20:19.440 --> 01:20:21.000]   But my husband was--
[01:20:21.000 --> 01:20:22.120]   And did you win?
[01:20:22.120 --> 01:20:24.160]   Were you able to get a rebate?
[01:20:24.160 --> 01:20:27.800]   Yeah, you usually get-- you usually get your value--
[01:20:27.800 --> 01:20:29.840]   you're protesting the value of your house.
[01:20:29.840 --> 01:20:30.320]   Yeah.
[01:20:30.320 --> 01:20:30.800]   It's not--
[01:20:30.800 --> 01:20:31.600]   You're saying it should be--
[01:20:31.600 --> 01:20:34.560]   Well, California is weird because of Prop 13's Harvest Camp.
[01:20:34.560 --> 01:20:35.520]   Right.
[01:20:35.520 --> 01:20:37.080]   We still get assessed.
[01:20:37.080 --> 01:20:38.640]   You get assessed on sale.
[01:20:38.640 --> 01:20:39.640]   There's still plenty of--
[01:20:39.640 --> 01:20:42.040]   But you're assessed on sale, but then you're only allowed
[01:20:42.040 --> 01:20:43.320]   to go up 1% a year.
[01:20:43.320 --> 01:20:43.840]   Right.
[01:20:43.840 --> 01:20:44.280]   So last--
[01:20:44.280 --> 01:20:44.780]   Yeah.
[01:20:44.780 --> 01:20:45.680]   Basically, don't pretend--
[01:20:45.680 --> 01:20:47.520]   But in Texas, you can go up--
[01:20:47.520 --> 01:20:48.020]   Right.
[01:20:48.020 --> 01:20:49.720]   If you're assessed at a huge amount,
[01:20:49.720 --> 01:20:51.680]   your property tax will suddenly go--
[01:20:51.680 --> 01:20:53.160]   Yeah, but you're not paying income tax.
[01:20:53.160 --> 01:20:55.520]   You should just pay your darn tax and shut up.
[01:20:55.520 --> 01:20:55.880]   Yeah.
[01:20:55.880 --> 01:20:57.520]   That was my argument to my husband.
[01:20:57.520 --> 01:21:00.920]   But he was like, that's why this is here.
[01:21:00.920 --> 01:21:02.520]   And that's kind of the Google argument.
[01:21:02.520 --> 01:21:03.760]   I mean, it is the Google's argument.
[01:21:03.760 --> 01:21:04.680]   No, of course it is.
[01:21:04.680 --> 01:21:05.180]   It is.
[01:21:05.180 --> 01:21:06.400]   They wrote the law that way.
[01:21:06.400 --> 01:21:09.040]   Legislators job to say what to define it.
[01:21:09.040 --> 01:21:10.800]   We wrote the law that way.
[01:21:10.800 --> 01:21:11.960]   We bought our legislature.
[01:21:11.960 --> 01:21:13.320]   We paid good money for them.
[01:21:13.320 --> 01:21:15.400]   And we should do whatever they say.
[01:21:15.400 --> 01:21:17.480]   I want to hear what you have to say about Facebook,
[01:21:17.480 --> 01:21:18.400]   put the link in the audio.
[01:21:18.400 --> 01:21:19.120]   I'm a 90 here.
[01:21:19.120 --> 01:21:20.120]   What do you think about it?
[01:21:20.120 --> 01:21:23.840]   So I heard on Sunday that Mark Zuckerberg is going
[01:21:23.840 --> 01:21:25.960]   to make some big announcement on Monday.
[01:21:25.960 --> 01:21:28.080]   Then I didn't see anything about the announcement.
[01:21:28.080 --> 01:21:30.800]   What was it?
[01:21:30.800 --> 01:21:32.760]   He talked to-- was it Casey?
[01:21:32.760 --> 01:21:34.720]   Yeah.
[01:21:34.720 --> 01:21:36.160]   I always want to say Casey Affleck.
[01:21:36.160 --> 01:21:37.360]   But it's Casey Newton.
[01:21:37.360 --> 01:21:37.880]   Newton.
[01:21:37.880 --> 01:21:40.560]   Newton.
[01:21:40.560 --> 01:21:43.400]   A suite of audio products-- a clubhouse, of course,
[01:21:43.400 --> 01:21:46.080]   because all social networks do, and all social networks do--
[01:21:46.080 --> 01:21:47.800]   a push into podcasting.
[01:21:47.800 --> 01:21:50.280]   An argument that he now favors content creators,
[01:21:50.280 --> 01:21:52.480]   because also on the news side, they've also
[01:21:52.480 --> 01:21:56.160]   announced a slew of products to help news people
[01:21:56.160 --> 01:21:59.400]   with external blogs, things off of Facebook, by the way.
[01:21:59.400 --> 01:22:02.080]   We talked about that a few weeks ago.
[01:22:02.080 --> 01:22:07.240]   A plan to integrate Spotify's music player into Facebook.
[01:22:07.240 --> 01:22:08.560]   It's so funny.
[01:22:08.560 --> 01:22:11.760]   This almost tells me that Facebook has not--
[01:22:11.760 --> 01:22:13.120]   you know, I've for a long time thought
[01:22:13.120 --> 01:22:16.280]   that Zuckerberg was a smart guy making all the right moves,
[01:22:16.280 --> 01:22:17.840]   and maybe he was for a while.
[01:22:17.840 --> 01:22:22.200]   But lately, it's just like, what are you guys doing?
[01:22:22.200 --> 01:22:24.640]   Oh, we'll do that.
[01:22:24.640 --> 01:22:26.120]   What's the hot thing now, clubhouse?
[01:22:26.120 --> 01:22:27.680]   Oh, we can do that.
[01:22:27.680 --> 01:22:29.480]   By the way, I should point out the technique,
[01:22:29.480 --> 01:22:32.880]   the technology that we used earlier to bring people
[01:22:32.880 --> 01:22:37.640]   on into Discord was like an exact duplicate of clubhouse.
[01:22:37.640 --> 01:22:40.400]   Apparently, there's nothing to stop anybody from doing that.
[01:22:40.400 --> 01:22:42.320]   So everybody and their brothers doing it.
[01:22:42.320 --> 01:22:45.120]   I think even Slack is going to be doing it.
[01:22:45.120 --> 01:22:46.440]   Every guy's going to do it.
[01:22:46.440 --> 01:22:49.960]   So what it also goes against the monopoly argument is--
[01:22:49.960 --> 01:22:50.680]   everybody's doing it.
[01:22:50.680 --> 01:22:52.680]   Yeah, there's no monopoly.
[01:22:52.680 --> 01:22:55.440]   Facebook users can record brief voice messages
[01:22:55.440 --> 01:22:57.280]   and post them in their news feeds.
[01:22:57.280 --> 01:22:58.120]   I'm surprised you couldn't.
[01:22:58.120 --> 01:22:59.040]   You could do videos.
[01:22:59.040 --> 01:23:01.800]   So now you can do audios.
[01:23:01.800 --> 01:23:03.160]   Just weird.
[01:23:03.160 --> 01:23:05.880]   Do a video and people can-- I don't understand that.
[01:23:05.880 --> 01:23:09.520]   And then the podcast discovery product--
[01:23:09.520 --> 01:23:13.800]   so Apple announced also a big podcast push on Tuesday,
[01:23:13.800 --> 01:23:16.960]   yesterday, including subscriptions very much like what
[01:23:16.960 --> 01:23:18.320]   we do with Sub-Twit.
[01:23:18.320 --> 01:23:20.160]   We wouldn't do it with Apple because it would be--
[01:23:20.160 --> 01:23:21.920]   you'd have to use Apple Podcast to listen.
[01:23:21.920 --> 01:23:23.040]   It'd be Apple-centric.
[01:23:23.040 --> 01:23:24.640]   So that's silly.
[01:23:24.640 --> 01:23:25.840]   Why would we do that?
[01:23:25.840 --> 01:23:30.880]   But I think a lot of podcasters who are just starting
[01:23:30.880 --> 01:23:31.720]   would be thrilled.
[01:23:31.720 --> 01:23:36.360]   Apple does take 30% compared to memberfuls 4.9%.
[01:23:36.360 --> 01:23:39.760]   That's a significant difference.
[01:23:39.760 --> 01:23:43.480]   But they also said they were going to update their podcast
[01:23:43.480 --> 01:23:44.920]   discovery mechanisms.
[01:23:44.920 --> 01:23:48.920]   It has always been the case for any podcast, including ours
[01:23:48.920 --> 01:23:53.200]   throughout the ages, that if you were highlighted on a platform,
[01:23:53.200 --> 01:23:54.960]   especially on iTunes--
[01:23:54.960 --> 01:23:56.400]   Apple Podcast.
[01:23:56.400 --> 01:23:58.400]   Or if you were highlighted on Spotify,
[01:23:58.400 --> 01:24:01.120]   or you were highlighted on Stitcher, or any one of these
[01:24:01.120 --> 01:24:03.880]   places, it makes a huge difference.
[01:24:03.880 --> 01:24:06.840]   We'd see all of a sudden, I'd see numbers on Stitcher
[01:24:06.840 --> 01:24:09.480]   go through the roof and say, Lisa, what happened on Stitcher?
[01:24:09.480 --> 01:24:11.520]   Oh, they featured us this week.
[01:24:11.520 --> 01:24:13.320]   But the problem is it goes way up,
[01:24:13.320 --> 01:24:14.840]   and then it goes way back down.
[01:24:14.840 --> 01:24:17.160]   So it doesn't make a big difference in the overall.
[01:24:17.160 --> 01:24:20.680]   But it's certainly something you want.
[01:24:20.680 --> 01:24:22.960]   The fact that Facebook's going to use Spotify to do this
[01:24:22.960 --> 01:24:26.040]   is, I guess, good.
[01:24:26.040 --> 01:24:27.600]   It sounds like Facebook's actually going
[01:24:27.600 --> 01:24:31.720]   to be sending them to Spotify, which is interesting.
[01:24:31.720 --> 01:24:35.400]   It can't be that hard to create a podcast product on Facebook.
[01:24:35.400 --> 01:24:40.640]   Rooms is their version of Clubhouse.
[01:24:40.640 --> 01:24:42.960]   Did I even know there were rooms on Facebook?
[01:24:42.960 --> 01:24:45.920]   I don't know.
[01:24:45.920 --> 01:24:49.400]   I still don't have spaces on Twitter.
[01:24:49.400 --> 01:24:51.760]   I don't have rooms because I don't have Facebook.
[01:24:51.760 --> 01:24:54.360]   I don't have spaces because I don't know why.
[01:24:54.360 --> 01:24:55.360]   I don't know why.
[01:24:55.360 --> 01:24:55.960]   I have spaces.
[01:24:55.960 --> 01:24:56.960]   Do you?
[01:24:56.960 --> 01:24:57.560]   I should check.
[01:24:57.560 --> 01:24:58.080]   I do.
[01:24:58.080 --> 01:25:00.080]   I have to show everybody.
[01:25:00.080 --> 01:25:02.440]   I check my Twitter client to see if I've got spaces.
[01:25:02.440 --> 01:25:04.880]   It's only on your mobile client.
[01:25:04.880 --> 01:25:05.880]   Yeah.
[01:25:05.880 --> 01:25:08.200]   But they supposedly rolled it out to everybody
[01:25:08.200 --> 01:25:09.640]   on the Android side now, right?
[01:25:09.640 --> 01:25:10.520]   Oh, on Android.
[01:25:10.520 --> 01:25:13.840]   Yeah, so if you hit the plus to start a new Twitter,
[01:25:13.840 --> 01:25:16.120]   you don't get space's photos gift or text.
[01:25:16.120 --> 01:25:16.560]   Oh, yeah.
[01:25:16.560 --> 01:25:18.560]   I can do a hit record.
[01:25:18.560 --> 01:25:20.920]   I can do an audio of what's happening.
[01:25:20.920 --> 01:25:22.880]   Well, I'm just sitting here right now.
[01:25:22.880 --> 01:25:23.720]   You don't get that out?
[01:25:23.720 --> 01:25:24.640]   I just cast.
[01:25:24.640 --> 01:25:25.400]   No, sir.
[01:25:25.400 --> 01:25:26.280]   And then I'm done.
[01:25:26.280 --> 01:25:26.600]   And what is that?
[01:25:26.600 --> 01:25:28.120]   We hit the plus.
[01:25:28.120 --> 01:25:28.400]   Right.
[01:25:28.400 --> 01:25:29.480]   So I don't even--
[01:25:29.480 --> 01:25:32.960]   I even saw you supposed to do the long press or whatever.
[01:25:32.960 --> 01:25:33.640]   I get none of that.
[01:25:33.640 --> 01:25:35.120]   Oh.
[01:25:35.120 --> 01:25:37.320]   Man, I can't do the voice record, though.
[01:25:37.320 --> 01:25:39.040]   I don't get stages.
[01:25:39.040 --> 01:25:41.160]   Yes, this is what's so weird, spaces.
[01:25:41.160 --> 01:25:43.200]   Just so much weird about these platforms
[01:25:43.200 --> 01:25:45.560]   is that everybody has a different experience.
[01:25:45.560 --> 01:25:46.560]   Yeah.
[01:25:46.560 --> 01:25:47.600]   So if I hit this--
[01:25:47.600 --> 01:25:49.800]   I can view a space from someone.
[01:25:49.800 --> 01:25:51.320]   That's the audio thing.
[01:25:51.320 --> 01:25:53.360]   You don't have that, Stacey.
[01:25:53.360 --> 01:25:53.880]   That's me.
[01:25:53.880 --> 01:25:54.320]   I don't have it.
[01:25:54.320 --> 01:25:54.680]   Let me see.
[01:25:54.680 --> 01:25:55.560]   Oh, you don't have it again?
[01:25:55.560 --> 01:25:57.720]   Oh, I don't know what's going on.
[01:25:57.720 --> 01:26:02.280]   Oh, that little blue purple little squiggle, that's audio.
[01:26:02.280 --> 01:26:03.280]   Yeah, I don't have that.
[01:26:03.280 --> 01:26:04.280]   Then there's a picture.
[01:26:04.280 --> 01:26:05.160]   I could post a picture.
[01:26:05.160 --> 01:26:11.840]   Yeah, I don't see anything that looks spaces like this.
[01:26:11.840 --> 01:26:12.360]   What about this?
[01:26:12.360 --> 01:26:14.200]   What does this do?
[01:26:14.200 --> 01:26:15.200]   That's a story, right?
[01:26:15.200 --> 01:26:15.880]   Maybe if I just tweet.
[01:26:15.880 --> 01:26:16.800]   Oh, that's a thread.
[01:26:16.800 --> 01:26:19.160]   I could just many more tweets.
[01:26:19.160 --> 01:26:22.520]   Cancel, and I'll send them to tweet.
[01:26:22.520 --> 01:26:23.600]   All right, I don't know.
[01:26:23.600 --> 01:26:24.160]   I don't know.
[01:26:24.160 --> 01:26:24.920]   I don't know what I can do.
[01:26:24.920 --> 01:26:25.420]   All right.
[01:26:25.420 --> 01:26:25.760]   No, I can't do audio.
[01:26:25.760 --> 01:26:27.200]   I wasn't super interested in it,
[01:26:27.200 --> 01:26:29.120]   but I wasn't going to try it one Saturday
[01:26:29.120 --> 01:26:31.880]   while I was just sitting out there in the backyard,
[01:26:31.880 --> 01:26:35.480]   because people have added me to their spaces on Twitter.
[01:26:35.480 --> 01:26:38.640]   And I was like, I never got the notification
[01:26:38.640 --> 01:26:40.560]   that I have that capability.
[01:26:40.560 --> 01:26:44.840]   But when I saw in one of those tech stories that I read,
[01:26:44.840 --> 01:26:48.080]   they said, yeah, Android people should have it by now.
[01:26:48.080 --> 01:26:48.920]   Here's Messenger.
[01:26:48.920 --> 01:26:51.320]   They haven't heard your Dulset tones.
[01:26:51.320 --> 01:26:52.640]   Here's rooms on Facebook.
[01:26:52.640 --> 01:26:54.600]   It's in Messenger, Jeff.
[01:26:54.600 --> 01:26:56.600]   So it's Messenger rooms, a new way
[01:26:56.600 --> 01:26:59.800]   to hang out with your favorite people on video chat.
[01:26:59.800 --> 01:27:01.600]   It's not Facebook proper.
[01:27:01.600 --> 01:27:02.440]   It's Facebook Messenger.
[01:27:02.440 --> 01:27:03.360]   Oh, I see.
[01:27:03.360 --> 01:27:05.000]   So it's like a video call from multiple people.
[01:27:05.000 --> 01:27:06.960]   Oh, look, you don't have to have a Facebook account
[01:27:06.960 --> 01:27:08.440]   to join the room.
[01:27:08.440 --> 01:27:09.040]   Oh, boy.
[01:27:09.040 --> 01:27:09.880]   Oh, my.
[01:27:09.880 --> 01:27:11.240]   Otherwise, nobody can call anybody.
[01:27:11.240 --> 01:27:12.080]   Yeah, right.
[01:27:12.080 --> 01:27:12.720]   Like a word calling you.
[01:27:12.720 --> 01:27:15.760]   Do you have to have Messenger?
[01:27:15.760 --> 01:27:18.320]   Yeah, I guess that means you have to have a Facebook account.
[01:27:18.320 --> 01:27:20.680]   No, it says you can click a link.
[01:27:20.680 --> 01:27:23.560]   So if somebody sends you a link, you'll just join right in.
[01:27:23.560 --> 01:27:24.560]   Let me create a room.
[01:27:24.560 --> 01:27:26.640]   That's actually good, because there are a lot of people
[01:27:26.640 --> 01:27:29.880]   who hate Facebook.
[01:27:29.880 --> 01:27:31.320]   I can't create this in Firefox.
[01:27:31.320 --> 01:27:32.400]   I have to do it in Chrome.
[01:27:32.400 --> 01:27:37.520]   It's all flocked up.
[01:27:37.520 --> 01:27:40.040]   Why does it have to be terminal?
[01:27:40.040 --> 01:27:41.280]   It's good ones, Daisy.
[01:27:41.280 --> 01:27:44.160]   Why doesn't Facebook have its own browser?
[01:27:44.160 --> 01:27:45.640]   Shouldn't they just have a Facebook--
[01:27:45.640 --> 01:27:48.280]   How does a Facebook offer computing as a service?
[01:27:48.280 --> 01:27:49.600]   I have so many questions about Facebook.
[01:27:49.600 --> 01:27:50.400]   Bingbo.
[01:27:50.400 --> 01:27:52.400]   Mm.
[01:27:52.400 --> 01:27:53.480]   They have the resources.
[01:27:53.480 --> 01:27:54.840]   They could do it.
[01:27:54.840 --> 01:27:55.880]   They do.
[01:27:55.880 --> 01:27:57.720]   They could offer some really compelling things,
[01:27:57.720 --> 01:27:59.080]   especially around--
[01:27:59.080 --> 01:28:00.520]   I'll hate to say it-- AI.
[01:28:00.520 --> 01:28:08.880]   Good story on Axios about all everybody cloning clubhouse.
[01:28:08.880 --> 01:28:14.000]   Dollars flow to live audio as moderation problems loom.
[01:28:14.000 --> 01:28:16.480]   Here are the people rolling out products
[01:28:16.480 --> 01:28:19.320]   that are suspiciously like clubhouse.
[01:28:19.320 --> 01:28:20.880]   Plus, add Reddit to this list.
[01:28:20.880 --> 01:28:22.400]   It's brand new since then.
[01:28:22.400 --> 01:28:25.040]   Facebook's Spotify, Twitter, Discord.
[01:28:25.040 --> 01:28:26.000]   We just used it.
[01:28:26.000 --> 01:28:27.280]   Apple.
[01:28:27.280 --> 01:28:29.840]   It's going to do its own audio news,
[01:28:29.840 --> 01:28:33.200]   but I don't know if they're going to do that.
[01:28:33.200 --> 01:28:34.320]   Let's think about this story.
[01:28:34.320 --> 01:28:35.480]   Is this moderation problem?
[01:28:35.480 --> 01:28:37.760]   How do you moderate audio?
[01:28:37.760 --> 01:28:39.160]   Because it's fleeting.
[01:28:39.160 --> 01:28:39.640]   Yeah.
[01:28:39.640 --> 01:28:40.920]   It's a ephemeral, right?
[01:28:40.920 --> 01:28:43.440]   Yeah, it's a lot harder to moderate it.
[01:28:43.440 --> 01:28:44.320]   You don't know--
[01:28:44.320 --> 01:28:47.160]   There's going to be awful stuff going on.
[01:28:47.160 --> 01:28:48.440]   Yeah.
[01:28:48.440 --> 01:28:51.440]   And it's a clubhouse that has shut down rooms
[01:28:51.440 --> 01:28:53.840]   discussing Jewish white privilege.
[01:28:53.840 --> 01:28:54.960]   Oh.
[01:28:54.960 --> 01:28:56.080]   Yeah.
[01:28:56.080 --> 01:28:58.400]   In other words, anti-Semitic rooms.
[01:28:58.400 --> 01:28:59.240]   Right.
[01:28:59.240 --> 01:28:59.800]   Right.
[01:28:59.800 --> 01:29:10.880]   Well, a higher tone version of anest
[01:29:10.880 --> 01:29:13.080]   Facebook's oversight board has decided
[01:29:13.080 --> 01:29:16.960]   to delay its decision for a few more weeks on the Trump ban
[01:29:16.960 --> 01:29:19.240]   so they can take comments.
[01:29:19.240 --> 01:29:20.480]   They've only been taking them.
[01:29:20.480 --> 01:29:22.480]   They can read all 9,000 of them.
[01:29:22.480 --> 01:29:26.360]   It doesn't take weeks to read 9,000 comments, but OK.
[01:29:26.360 --> 01:29:28.000]   You have people to do that for you.
[01:29:28.000 --> 01:29:32.200]   Do I sense a little nervousness about their decision?
[01:29:32.200 --> 01:29:33.640]   What do you think they're going to decide?
[01:29:33.640 --> 01:29:36.840]   So this is all about President Trump being banned
[01:29:36.840 --> 01:29:40.120]   from Facebook, the Facebook oversight board, which
[01:29:40.120 --> 01:29:42.880]   is a high level group of how many?
[01:29:42.880 --> 01:29:44.320]   It's like 30 people.
[01:29:44.320 --> 01:29:44.840]   It's 20.
[01:29:44.840 --> 01:29:45.920]   20.
[01:29:45.920 --> 01:29:47.640]   But prestige is high.
[01:29:47.640 --> 01:29:48.640]   I totally agree.
[01:29:48.640 --> 01:29:49.120]   I agree.
[01:29:49.120 --> 01:29:53.040]   Who can make binding decisions on Facebook's bans
[01:29:53.040 --> 01:29:54.160]   and other things.
[01:29:54.160 --> 01:29:56.360]   They took on the Trump ban.
[01:29:56.360 --> 01:29:58.840]   If they, for instance, announced that no, you
[01:29:58.840 --> 01:30:01.360]   have to reinstate them, Facebook would have to reinstate
[01:30:01.360 --> 01:30:03.760]   the form of President.
[01:30:03.760 --> 01:30:06.160]   Meanwhile, Apple's putting parlour back
[01:30:06.160 --> 01:30:07.880]   under political pressure.
[01:30:07.880 --> 01:30:12.160]   But they're saying with some stipulations, right?
[01:30:12.160 --> 01:30:14.560]   Yeah, yeah, yeah.
[01:30:14.560 --> 01:30:19.280]   That's really-- you think they cave to political pressure?
[01:30:19.280 --> 01:30:19.960]   Apple?
[01:30:19.960 --> 01:30:24.720]   Who wants parlour back?
[01:30:24.720 --> 01:30:26.920]   Well, they had the letters from the senators who demanded it.
[01:30:26.920 --> 01:30:28.360]   Ted Cruz.
[01:30:28.360 --> 01:30:30.840]   I'm going to say they have a huge following.
[01:30:30.840 --> 01:30:34.640]   So somebody's going to yell loud enough at the right people
[01:30:34.640 --> 01:30:37.280]   and clearly they did.
[01:30:37.280 --> 01:30:38.520]   Interesting.
[01:30:38.520 --> 01:30:43.080]   Should Twitter reinstate the President, former President?
[01:30:43.080 --> 01:30:43.600]   No.
[01:30:43.600 --> 01:30:45.040]   It's so nice.
[01:30:45.040 --> 01:30:46.000]   It's been so quiet.
[01:30:46.000 --> 01:30:47.440]   It's been quiet, hasn't it?
[01:30:47.440 --> 01:30:48.240]   It's been so good.
[01:30:48.240 --> 01:30:49.640]   Should Facebook reinstate him?
[01:30:49.640 --> 01:30:51.320]   It's not quite the same.
[01:30:51.320 --> 01:30:53.320]   If he's not the President anymore,
[01:30:53.320 --> 01:30:54.960]   it kind of is an interesting question.
[01:30:54.960 --> 01:30:55.160]   Yes.
[01:30:55.160 --> 01:30:56.560]   Just because--
[01:30:56.560 --> 01:30:59.160]   He's no longer a national leader.
[01:30:59.160 --> 01:31:02.360]   So he's not protected.
[01:31:02.360 --> 01:31:06.240]   If he says something that others would be banned for,
[01:31:06.240 --> 01:31:07.840]   I think you ban him, right?
[01:31:07.840 --> 01:31:08.320]   Yeah.
[01:31:08.320 --> 01:31:09.800]   You don't give him special privilege.
[01:31:09.800 --> 01:31:13.040]   Maybe when he's the President you did, they certainly did.
[01:31:13.040 --> 01:31:16.680]   But once you're not the President,
[01:31:16.680 --> 01:31:19.920]   if you say something that would anybody else
[01:31:19.920 --> 01:31:21.800]   be banned for, why wouldn't you be?
[01:31:21.800 --> 01:31:25.200]   Which would bring the argument that the stuff that was said
[01:31:25.200 --> 01:31:27.520]   was during the presidency.
[01:31:27.520 --> 01:31:32.640]   Shouldn't his slate move white clean now?
[01:31:32.640 --> 01:31:35.760]   Or a sin's a sin and it's still on his record.
[01:31:35.760 --> 01:31:39.120]   His permanent record, as we said in high school.
[01:31:39.120 --> 01:31:41.320]   Well, if the issue isn't that he's sinned,
[01:31:41.320 --> 01:31:44.280]   but the issue is that he's a powerful person deliberately
[01:31:44.280 --> 01:31:48.440]   spreading misinformation that has real world implications,
[01:31:48.440 --> 01:31:52.600]   then you could say he's no longer president.
[01:31:52.600 --> 01:31:54.200]   So he can be as crazy as you would stay.
[01:31:54.200 --> 01:31:56.440]   He has chop says he's still a national leader,
[01:31:56.440 --> 01:31:58.080]   even if he's not in office.
[01:31:58.080 --> 01:31:59.120]   You know, he's--
[01:31:59.120 --> 01:32:00.040]   He has Secret Service.
[01:32:00.040 --> 01:32:02.880]   I mean-- He's the titular leader of the Republican Party.
[01:32:02.880 --> 01:32:03.800]   Party, yeah.
[01:32:03.800 --> 01:32:04.320]   Yeah.
[01:32:04.320 --> 01:32:07.040]   So I think that's how you have to judge this call.
[01:32:07.040 --> 01:32:08.680]   We can't just judge it based on the fact
[01:32:08.680 --> 01:32:13.040]   that he's a crazy racist old man who's spewing
[01:32:13.040 --> 01:32:14.760]   vitriol all over Twitter.
[01:32:14.760 --> 01:32:19.400]   We have to say, OK, is that any different from the other crazy
[01:32:19.400 --> 01:32:21.440]   racist old men's been spilling the truth?
[01:32:21.440 --> 01:32:23.980]   When you took the Twitter quiz, we
[01:32:23.980 --> 01:32:26.080]   also took the Twitter quiz together.
[01:32:26.080 --> 01:32:27.320]   I think I remember whether you're
[01:32:27.320 --> 01:32:29.280]   saying that prominent people should
[01:32:29.280 --> 01:32:32.920]   be held to a higher standard than the whole boy.
[01:32:32.920 --> 01:32:33.920]   OK?
[01:32:33.920 --> 01:32:36.880]   Higher standard or lower standard?
[01:32:36.880 --> 01:32:37.880]   My argument was--
[01:32:37.880 --> 01:32:38.880]   They get some protection.
[01:32:38.880 --> 01:32:39.880]   Higher standard.
[01:32:39.880 --> 01:32:40.400]   I did.
[01:32:40.400 --> 01:32:40.880]   OK.
[01:32:40.880 --> 01:32:44.000]   Powerful people should tell people the truth, I think,
[01:32:44.000 --> 01:32:46.160]   is kind of where I was.
[01:32:46.160 --> 01:32:47.760]   I don't-- I do remember.
[01:32:47.760 --> 01:32:48.320]   It is hard.
[01:32:48.320 --> 01:32:50.160]   Well, it is a politics.
[01:32:50.160 --> 01:32:50.760]   No, I know.
[01:32:50.760 --> 01:32:52.200]   It's a really interesting question.
[01:32:52.200 --> 01:32:54.600]   That was silly, Stacey.
[01:32:54.600 --> 01:32:56.800]   I've read it on.
[01:32:56.800 --> 01:33:01.080]   I vote that they're nervous about the decision.
[01:33:01.080 --> 01:33:03.080]   I think they don't receive their way.
[01:33:03.080 --> 01:33:04.280]   Of course.
[01:33:04.280 --> 01:33:04.800]   Right.
[01:33:04.800 --> 01:33:06.400]   It's a no win.
[01:33:06.400 --> 01:33:09.920]   I mean, I'll-- I'll-- I think the oversight board's decision--
[01:33:09.920 --> 01:33:13.440]   I thought the oversight board was a decent idea well done.
[01:33:13.440 --> 01:33:15.400]   And then the oversight board's decision is to date, I think,
[01:33:15.400 --> 01:33:17.920]   have been awful all at all.
[01:33:17.920 --> 01:33:19.560]   And so I'm ready to-- you know, we'll
[01:33:19.560 --> 01:33:20.640]   see how they do on this one.
[01:33:20.640 --> 01:33:24.600]   Amazon is opening a hair salon.
[01:33:24.600 --> 01:33:25.360]   You sound thrilled.
[01:33:25.360 --> 01:33:29.400]   Lexa, a little more off the top, please.
[01:33:29.400 --> 01:33:32.560]   In partnership with the prominent UK stylist,
[01:33:32.560 --> 01:33:35.640]   the salon will include tablets at every chair, a screen--
[01:33:35.640 --> 01:33:37.000]   we laughed at Amazon.
[01:33:37.000 --> 01:33:39.600]   And Amazon said, we're going to open a grocery store.
[01:33:39.600 --> 01:33:44.040]   We're laughing at it on the other side of our code desk now.
[01:33:44.040 --> 01:33:46.120]   The salon will include tablets at every chair,
[01:33:46.120 --> 01:33:48.840]   a screen to virtually try on hair colors
[01:33:48.840 --> 01:33:51.600]   and a station to display information about products
[01:33:51.600 --> 01:33:54.240]   when a consumer physically points at them.
[01:33:54.240 --> 01:33:55.000]   That one.
[01:33:55.000 --> 01:33:57.600]   I want that one, Mommy.
[01:33:57.600 --> 01:33:59.560]   This is a marketing and technology thing.
[01:33:59.560 --> 01:34:01.960]   They said they didn't want to do more salons.
[01:34:01.960 --> 01:34:02.640]   But--
[01:34:02.640 --> 01:34:05.880]   It's only open to employees at first.
[01:34:05.880 --> 01:34:06.720]   Yeah, they want to get--
[01:34:06.720 --> 01:34:07.760]   --just like Amazon Go.
[01:34:07.760 --> 01:34:09.160]   --gonna get in fashion.
[01:34:09.160 --> 01:34:12.400]   Didn't they do the same exact thing with Amazon Go?
[01:34:12.400 --> 01:34:14.800]   No, they bought whole foods, and then they launched Amazon.
[01:34:14.800 --> 01:34:15.800]   Oh.
[01:34:15.800 --> 01:34:17.480]   [LAUGHTER]
[01:34:17.480 --> 01:34:20.080]   Well, maybe they'll buy Vidal Sess--
[01:34:20.080 --> 01:34:22.320]   No, Stacy, does anything in this sound
[01:34:22.320 --> 01:34:24.520]   since you're the most fashionable among us?
[01:34:24.520 --> 01:34:26.680]   And Anne has no hair?
[01:34:26.680 --> 01:34:29.800]   Does this sound like at all appealing?
[01:34:29.800 --> 01:34:30.320]   Sorry.
[01:34:30.320 --> 01:34:33.800]   It's sad that I am the most fashionable one.
[01:34:33.800 --> 01:34:35.120]   You are.
[01:34:35.120 --> 01:34:36.120]   I would--
[01:34:36.120 --> 01:34:36.920]   You use that blue hair.
[01:34:36.920 --> 01:34:38.560]   I would be interested in--
[01:34:38.560 --> 01:34:40.640]   Well, I would still have it except for the pandemic.
[01:34:40.640 --> 01:34:44.320]   But I would like to see things like being
[01:34:44.320 --> 01:34:46.880]   able to try on makeup in different hairstyles.
[01:34:46.880 --> 01:34:48.800]   I think that's super fun.
[01:34:48.800 --> 01:34:49.320]   Yeah.
[01:34:49.320 --> 01:34:51.760]   I don't see why Amazon would want to do that.
[01:34:51.760 --> 01:34:52.520]   Well, they would--
[01:34:52.520 --> 01:34:54.120]   I mean, they would want to sell me the makeup so sure.
[01:34:54.120 --> 01:34:55.120]   They could very interesting.
[01:34:55.120 --> 01:34:56.000]   --data harvesting.
[01:34:56.000 --> 01:34:57.920]   But they always have been interested in fashion.
[01:34:57.920 --> 01:34:59.360]   That Amazon look device?
[01:34:59.360 --> 01:35:00.720]   Yeah, it's hard to crack, though.
[01:35:00.720 --> 01:35:02.520]   That's why they have a hard time there.
[01:35:02.520 --> 01:35:03.960]   Yeah.
[01:35:03.960 --> 01:35:05.120]   Yeah, because they don't-- you know what?
[01:35:05.120 --> 01:35:06.400]   They're not an upscale brand.
[01:35:06.400 --> 01:35:07.920]   It's not--
[01:35:07.920 --> 01:35:10.560]   Nobody wants to get the Amazon haircut.
[01:35:10.560 --> 01:35:13.720]   It just doesn't sound to feel like--
[01:35:13.720 --> 01:35:15.200]   I don't know if that looks good people would.
[01:35:15.200 --> 01:35:15.880]   But it asks.
[01:35:15.880 --> 01:35:16.880]   Really?
[01:35:16.880 --> 01:35:17.440]   OK.
[01:35:17.440 --> 01:35:18.800]   I want to look like Jeff Bezos.
[01:35:18.800 --> 01:35:22.160]   Can you make the upscale brands or--
[01:35:22.160 --> 01:35:25.280]   What's happening to the upscale brands?
[01:35:25.280 --> 01:35:27.760]   They've been scared of the internet from beginning.
[01:35:27.760 --> 01:35:30.280]   And they were scared of retailers
[01:35:30.280 --> 01:35:31.520]   trying to sell their stuff.
[01:35:31.520 --> 01:35:33.080]   So it may be-- Stacy's right.
[01:35:33.080 --> 01:35:37.440]   It may be a way in this where these higher end brands.
[01:35:37.440 --> 01:35:40.680]   Oh, I can totally see why a high end brand would not
[01:35:40.680 --> 01:35:42.760]   want to be on Amazon.
[01:35:42.760 --> 01:35:44.040]   There's so many good reasons.
[01:35:44.040 --> 01:35:44.560]   Yeah.
[01:35:44.560 --> 01:35:45.960]   There's counterfeiting.
[01:35:45.960 --> 01:35:49.480]   There's Amazon knocking off their gear for lower price.
[01:35:49.480 --> 01:35:51.480]   And association, yeah.
[01:35:51.480 --> 01:35:52.040]   All of the other--
[01:35:52.040 --> 01:35:53.640]   They're selling it to someone who they don't
[01:35:53.640 --> 01:35:55.920]   want wearing their brand because these brands are
[01:35:55.920 --> 01:35:56.320]   like--
[01:35:56.320 --> 01:35:56.680]   Right.
[01:35:56.680 --> 01:35:57.680]   --that feel they don't exist.
[01:35:57.680 --> 01:35:58.520]   It's down market.
[01:35:58.520 --> 01:36:00.880]   You don't want that.
[01:36:00.880 --> 01:36:03.840]   Amazon, this, I think they will make money on.
[01:36:03.840 --> 01:36:09.040]   They are planning a furniture assembly service.
[01:36:09.040 --> 01:36:11.600]   I actually hired somebody-- OK, I'm embarrassed to admit this.
[01:36:11.600 --> 01:36:14.080]   But when I got some Ikea furniture some years ago,
[01:36:14.080 --> 01:36:16.680]   I hired somebody to assemble it.
[01:36:16.680 --> 01:36:17.240]   I can't--
[01:36:17.240 --> 01:36:18.960]   I was a partnership with like a tax grab.
[01:36:18.960 --> 01:36:19.400]   I could do it.
[01:36:19.400 --> 01:36:20.680]   Yeah, a tax grab it now.
[01:36:20.680 --> 01:36:21.880]   This is before a tax grab it.
[01:36:21.880 --> 01:36:23.040]   Now you can get a tax grab it.
[01:36:23.040 --> 01:36:24.040]   I got my brother-in-law.
[01:36:24.040 --> 01:36:25.360]   I gave him $50.
[01:36:25.360 --> 01:36:27.800]   Depending on the day, I would have done the same thing.
[01:36:27.800 --> 01:36:28.480]   No judgment.
[01:36:28.480 --> 01:36:31.360]   I do not want to assemble Ikea furniture.
[01:36:31.360 --> 01:36:32.160]   Well, I enjoyed it.
[01:36:32.160 --> 01:36:33.680]   It was me living very curious.
[01:36:33.680 --> 01:36:35.520]   It was like I was a real man.
[01:36:35.520 --> 01:36:36.040]   Yeah.
[01:36:36.040 --> 01:36:37.280]   I hate that.
[01:36:37.280 --> 01:36:38.760]   Real men don't use Allen Rinsch.
[01:36:38.760 --> 01:36:40.360]   I twisted this Allen Rinsch.
[01:36:40.360 --> 01:36:40.880]   Yeah.
[01:36:40.880 --> 01:36:42.320]   I'm sorry.
[01:36:42.320 --> 01:36:44.240]   Not manly.
[01:36:44.240 --> 01:36:46.960]   For me, my standard, it was pretty good.
[01:36:46.960 --> 01:36:47.760]   Apparently, wait--
[01:36:47.760 --> 01:36:51.240]   The matter is like mechanical Turk for everyday tasks.
[01:36:51.240 --> 01:36:52.200]   That's a task grab.
[01:36:52.200 --> 01:36:53.040]   Mechanical Jeff.
[01:36:53.040 --> 01:36:55.600]   Since task grab it was found, I've used--
[01:36:55.600 --> 01:36:57.440]   First of all, I interviewed the founder.
[01:36:57.440 --> 01:37:00.520]   And then after that, I started using task grab it like crazy.
[01:37:00.520 --> 01:37:02.640]   I use it all the time.
[01:37:02.640 --> 01:37:07.320]   Instead of hiring movers now, I just get task grab it.
[01:37:07.320 --> 01:37:09.160]   Now I keep that in mind.
[01:37:09.160 --> 01:37:09.800]   Oh, yeah.
[01:37:09.800 --> 01:37:10.440]   They're nice.
[01:37:10.440 --> 01:37:11.720]   They come over.
[01:37:11.720 --> 01:37:14.360]   They got a truck.
[01:37:14.360 --> 01:37:20.720]   Would you like to meet Amarronothra's Twitter?
[01:37:20.720 --> 01:37:24.160]   A new bug.
[01:37:24.160 --> 01:37:25.400]   There is a kind of mite.
[01:37:25.400 --> 01:37:27.800]   They're not very big, but 0.7 millimeters long.
[01:37:27.800 --> 01:37:29.480]   They're harmless to people.
[01:37:29.480 --> 01:37:32.280]   They're a coastal mite that eats algae, lichen,
[01:37:32.280 --> 01:37:34.360]   and lives in a group.
[01:37:34.360 --> 01:37:36.520]   Oh, now some paywalls popped up.
[01:37:36.520 --> 01:37:37.880]   And they got discovered--
[01:37:37.880 --> 01:37:38.960]   On your Twitter.
[01:37:38.960 --> 01:37:40.640]   Twitter.
[01:37:40.640 --> 01:37:42.720]   Call it the mite of Twitter.
[01:37:42.720 --> 01:37:44.160]   That's the story.
[01:37:44.160 --> 01:37:48.760]   They spotted the bug in a tweet.
[01:37:48.760 --> 01:37:55.080]   Japanese and Austrian researchers spotted the bug in a tweet
[01:37:55.080 --> 01:37:57.520]   posted by an amateur photographer.
[01:37:57.520 --> 01:38:01.240]   The pictures showed a group of the tiny arachnids.
[01:38:01.240 --> 01:38:03.320]   I don't know, mites were arachnids.
[01:38:03.320 --> 01:38:06.040]   Mashing in a crack in a concrete wort
[01:38:06.040 --> 01:38:07.320]   at the fishing port of Chosey--
[01:38:07.320 --> 01:38:08.400]   Oh, eight mammals.
[01:38:08.400 --> 01:38:11.560]   --southeast of the capital.
[01:38:11.560 --> 01:38:17.280]   And a professor at Tokyo's Jose University, Satoshi Shimano
[01:38:17.280 --> 01:38:18.520]   saw it.
[01:38:18.520 --> 01:38:20.800]   And he said immediately after I saw them,
[01:38:20.800 --> 01:38:23.560]   I thought these are different from others I knew.
[01:38:23.560 --> 01:38:26.040]   And I thought they could be a new species.
[01:38:26.040 --> 01:38:26.500]   Conti--
[01:38:26.500 --> 01:38:27.880]   Because I know my mites so well.
[01:38:27.880 --> 01:38:29.760]   I know my mites.
[01:38:29.760 --> 01:38:32.360]   And this mite is mighty.
[01:38:32.360 --> 01:38:35.760]   And so they named it.
[01:38:35.760 --> 01:38:38.800]   Thanks to the suggestion of Professor Shimano.
[01:38:38.800 --> 01:38:41.440]   After Twitter.
[01:38:41.440 --> 01:38:43.840]   After Twitter, the Twitter part's easy.
[01:38:43.840 --> 01:38:44.680]   It's the first part.
[01:38:44.680 --> 01:38:49.440]   Amaranathris-- Amaranathris Twitter.
[01:38:49.440 --> 01:38:51.520]   I'll take your word for it.
[01:38:51.520 --> 01:38:55.200]   If you ever see one of these Amaranathis Twitters,
[01:38:55.200 --> 01:38:57.840]   you'll say, oh, yeah, I saw that on Twitter.
[01:38:57.840 --> 01:38:59.400]   I recommend that.
[01:38:59.400 --> 01:39:00.520]   By a tweet.
[01:39:00.520 --> 01:39:03.480]   So Jeff, this just proves that you have always said Twitter
[01:39:03.480 --> 01:39:05.800]   is so useful in the world.
[01:39:05.800 --> 01:39:08.200]   See, it's useful to science.
[01:39:08.200 --> 01:39:10.880]   Useful to everything.
[01:39:10.880 --> 01:39:14.640]   But can it let you shapeshift?
[01:39:14.640 --> 01:39:17.040]   [LAUGHTER]
[01:39:17.040 --> 01:39:20.560]   TikTok is a new filter that shapeshifts you
[01:39:20.560 --> 01:39:22.680]   into a movie character.
[01:39:22.680 --> 01:39:23.480]   Press the button.
[01:39:23.480 --> 01:39:23.980]   Let's see.
[01:39:23.980 --> 01:39:24.640]   I love you so first.
[01:39:24.640 --> 01:39:25.920]   Finish on my witch Disney princess.
[01:39:25.920 --> 01:39:27.040]   I look like.
[01:39:27.040 --> 01:39:30.160]   Three, two, one.
[01:39:30.160 --> 01:39:32.200]   And she's turned into--
[01:39:32.200 --> 01:39:32.920]   OK, Mulan.
[01:39:32.920 --> 01:39:33.400]   OK.
[01:39:33.400 --> 01:39:33.900]   Mulan.
[01:39:33.900 --> 01:39:35.840]   Next to the Marvel character.
[01:39:35.840 --> 01:39:38.320]   Now three, two, one.
[01:39:38.320 --> 01:39:39.400]   She's turned into--
[01:39:39.400 --> 01:39:41.000]   [LAUGHTER]
[01:39:41.000 --> 01:39:42.080]   Wakanda Forever.
[01:39:42.080 --> 01:39:43.160]   I really love that.
[01:39:43.160 --> 01:39:45.200]   That's my favorite movie.
[01:39:45.200 --> 01:39:46.640]   Next, which Harry Potter character.
[01:39:46.640 --> 01:39:48.440]   I think this is vaguely racist.
[01:39:48.440 --> 01:39:50.840]   But OK, let's see what happens here.
[01:39:50.840 --> 01:39:54.280]   She is a Severus Snape.
[01:39:54.280 --> 01:39:57.320]   That's mean.
[01:39:57.320 --> 01:39:58.320]   How about Lord of the Rings?
[01:39:58.320 --> 01:39:59.360]   Can you do Lord of the Rings?
[01:39:59.360 --> 01:40:00.040]   I'll take it.
[01:40:00.040 --> 01:40:02.360]   We'll just move on to the next one.
[01:40:02.360 --> 01:40:05.360]   And finally, which Lord of the Rings character?
[01:40:05.360 --> 01:40:06.080]   He's a big alum.
[01:40:06.080 --> 01:40:06.800]   He's a big alum.
[01:40:06.800 --> 01:40:07.880]   He's a big alum.
[01:40:07.880 --> 01:40:08.960]   Gollum.
[01:40:08.960 --> 01:40:09.640]   Gollum.
[01:40:09.640 --> 01:40:10.600]   No.
[01:40:10.600 --> 01:40:11.400]   She's not.
[01:40:11.400 --> 01:40:13.600]   She's another guy.
[01:40:13.600 --> 01:40:14.120]   So--
[01:40:14.120 --> 01:40:14.600]   Something else.
[01:40:14.600 --> 01:40:16.600]   This is stolen directly from Snapchat.
[01:40:16.600 --> 01:40:20.120]   And for their like, hey, check out this filter.
[01:40:20.120 --> 01:40:20.720]   And you'll--
[01:40:20.720 --> 01:40:22.120]   And then you'll share it with the volume.
[01:40:22.120 --> 01:40:24.120]   But they do the Snapchat.
[01:40:24.120 --> 01:40:25.160]   Tell me if they do.
[01:40:25.160 --> 01:40:27.320]   But do Snapchat do the thing that I think tiktiktiktiktikt
[01:40:27.320 --> 01:40:31.840]   does is very smart, which is a melding of the filter
[01:40:31.840 --> 01:40:36.360]   with the Facebook, which Disney character are you.
[01:40:36.360 --> 01:40:39.360]   And then it does the thing, and it tells you the answer.
[01:40:39.360 --> 01:40:40.240]   Does that--
[01:40:40.240 --> 01:40:40.920]   Oh, no.
[01:40:40.920 --> 01:40:41.280]   No.
[01:40:41.280 --> 01:40:42.400]   That's BuzzFeed Plus.
[01:40:42.400 --> 01:40:42.920]   Yeah.
[01:40:42.920 --> 01:40:44.520]   I think that's pretty good.
[01:40:44.520 --> 01:40:45.240]   Yeah.
[01:40:45.240 --> 01:40:47.320]   I think it's pretty good to grand stories.
[01:40:47.320 --> 01:40:50.000]   Which Harry Potter kit-- should I do it?
[01:40:50.000 --> 01:40:50.840]   No.
[01:40:50.840 --> 01:40:53.320]   OK.
[01:40:53.320 --> 01:40:57.120]   Saved by the ant.
[01:40:57.120 --> 01:40:59.760]   Hey, should we call you Ant Twitter?
[01:40:59.760 --> 01:41:02.480]   Like we discovered you on Twitter, and as Twitter is--
[01:41:02.480 --> 01:41:03.800]   [LAUGHTER]
[01:41:03.800 --> 01:41:05.160]   No.
[01:41:05.160 --> 01:41:06.120]   That would be no.
[01:41:06.120 --> 01:41:07.360]   No.
[01:41:07.360 --> 01:41:08.360]   Your own genius.
[01:41:08.360 --> 01:41:08.800]   I know.
[01:41:08.800 --> 01:41:13.680]   Prouitus Twitter is the latest food fat on tiktiktikt.
[01:41:13.680 --> 01:41:14.960]   It's our tiktik segment.
[01:41:14.960 --> 01:41:17.680]   I think this is so much more fun than change log.
[01:41:17.680 --> 01:41:19.880]   Twist-- oh, now I have to do some puzzle piece.
[01:41:19.880 --> 01:41:21.000]   Oh, look at that.
[01:41:21.000 --> 01:41:22.640]   I hate it when they have me do that.
[01:41:22.640 --> 01:41:23.640]   Oh, I didn't hate it.
[01:41:23.640 --> 01:41:25.680]   Puzzle pieces are way better than the verify.
[01:41:25.680 --> 01:41:26.480]   I hate the thing.
[01:41:26.480 --> 01:41:27.240]   Is this a drug?
[01:41:27.240 --> 01:41:27.720]   What's a drug?
[01:41:27.720 --> 01:41:28.240]   A drug.
[01:41:28.240 --> 01:41:28.740]   What's a drug?
[01:41:28.740 --> 01:41:31.600]   So look, you can twist your bacon.
[01:41:31.600 --> 01:41:31.960]   That's--
[01:41:31.960 --> 01:41:32.720]   Bacon twist.
[01:41:32.720 --> 01:41:34.480]   OK.
[01:41:34.480 --> 01:41:35.760]   OK, how do you do it?
[01:41:35.760 --> 01:41:36.260]   OK.
[01:41:36.260 --> 01:41:39.000]   It's the newest thing that bacon twisty bacon on Twitter.
[01:41:39.000 --> 01:41:40.520]   That I've seen.
[01:41:40.520 --> 01:41:43.520]   But let's do it barbecue sauce, which should be.
[01:41:43.520 --> 01:41:46.360]   There is nothing better than smoked bacon on the grill.
[01:41:46.360 --> 01:41:47.960]   It twirling around.
[01:41:47.960 --> 01:41:48.600]   So it looks like--
[01:41:48.600 --> 01:41:49.600]   I'm not stealing anyone's idea.
[01:41:49.600 --> 01:41:50.160]   That noodle.
[01:41:50.160 --> 01:41:52.680]   I'm just adding some smoke and barbecue sauce and bacon.
[01:41:52.680 --> 01:41:54.080]   All right, I'm so sick of this.
[01:41:54.080 --> 01:41:56.600]   How does that not set his grill on fire?
[01:41:56.600 --> 01:41:57.520]   That much fat falling down that--
[01:41:57.520 --> 01:41:59.080]   The fat dripping right into it?
[01:41:59.080 --> 01:42:02.520]   That much fat on that little thin tin grill.
[01:42:02.520 --> 01:42:03.520]   Yeah.
[01:42:03.520 --> 01:42:05.400]   That's not right.
[01:42:05.400 --> 01:42:06.400]   Twisted bacon.
[01:42:06.400 --> 01:42:07.520]   Isn't that weird how--
[01:42:07.520 --> 01:42:10.080]   I mean, it's the most trivial of things.
[01:42:10.080 --> 01:42:13.440]   But isn't that weird how, like, that suddenly is a thing?
[01:42:13.440 --> 01:42:14.360]   There's a craze.
[01:42:14.360 --> 01:42:15.960]   Yeah.
[01:42:15.960 --> 01:42:16.800]   That's people.
[01:42:16.800 --> 01:42:19.240]   Weird, fueled by novelty and easy.
[01:42:19.240 --> 01:42:20.160]   That's what TikTok.
[01:42:20.160 --> 01:42:21.200]   That's what TikTok is.
[01:42:21.200 --> 01:42:23.400]   It's just like novelty.
[01:42:23.400 --> 01:42:25.200]   What I don't like-- the one thing I don't like--
[01:42:25.200 --> 01:42:28.560]   and he was exhibiting this, which is the leaning over
[01:42:28.560 --> 01:42:31.160]   to look into the camera thing.
[01:42:31.160 --> 01:42:32.520]   Everybody on TikTok does that.
[01:42:32.520 --> 01:42:33.400]   I fast can't quite.
[01:42:33.400 --> 01:42:34.960]   People do that.
[01:42:34.960 --> 01:42:36.200]   I don't like it.
[01:42:36.200 --> 01:42:37.200]   I think that that's true.
[01:42:37.200 --> 01:42:38.720]   But how do you feel like they're right there?
[01:42:38.720 --> 01:42:39.720]   It's probably--
[01:42:39.720 --> 01:42:41.680]   I was going to say that's the engagement tactic.
[01:42:41.680 --> 01:42:43.880]   Oh, you're engaging and deep in--
[01:42:43.880 --> 01:42:44.800]   Is this better?
[01:42:44.800 --> 01:42:45.800]   Yeah.
[01:42:45.800 --> 01:42:49.760]   Let's all lean into the camera.
[01:42:49.760 --> 01:42:50.800]   Sorry, I can't lean.
[01:42:50.800 --> 01:42:52.960]   My camera's way over there.
[01:42:52.960 --> 01:42:54.280]   Me too.
[01:42:54.280 --> 01:42:57.320]   I'm leaning-- reminds me of--
[01:42:57.320 --> 01:42:57.820]   Yeah.
[01:42:57.820 --> 01:42:58.320]   Oh, see you.
[01:42:58.320 --> 01:42:59.240]   There you go.
[01:42:59.240 --> 01:43:00.720]   That's 3D movie time.
[01:43:04.120 --> 01:43:05.120]   All right, let's take a little break.
[01:43:05.120 --> 01:43:09.800]   Then it's time for the Google Change Log.
[01:43:09.800 --> 01:43:11.160]   Change Log Time.
[01:43:11.160 --> 01:43:13.760]   Play the drums slowly, my friend.
[01:43:13.760 --> 01:43:15.600]   The Google Change Log.
[01:43:15.600 --> 01:43:24.560]   This is something Lisa discovered and was horrified--
[01:43:24.560 --> 01:43:27.720]   horrified to find Google Earth's historical 3D time
[01:43:27.720 --> 01:43:31.920]   lapses showing the ravages of climate change.
[01:43:31.920 --> 01:43:34.480]   You need Google Earth to do this.
[01:43:34.480 --> 01:43:37.720]   And then click on the Voyager tab
[01:43:37.720 --> 01:43:39.720]   and search for a place of interest.
[01:43:39.720 --> 01:43:43.440]   Or you can check out one of Google's five guided tours
[01:43:43.440 --> 01:43:48.040]   about the Earth's fragile beauty.
[01:43:48.040 --> 01:43:49.160]   Should I try it?
[01:43:49.160 --> 01:43:50.600]   Let me launch Google Earth.
[01:43:50.600 --> 01:43:54.000]   Have you done-- have anybody done this yet?
[01:43:54.000 --> 01:43:55.000]   No.
[01:43:55.000 --> 01:43:55.640]   No.
[01:43:55.640 --> 01:43:56.640]   Yes.
[01:43:56.640 --> 01:43:58.200]   And were you horrified?
[01:44:00.960 --> 01:44:03.400]   Not so much, because I kind of--
[01:44:03.400 --> 01:44:03.960]   You kind of knew.
[01:44:03.960 --> 01:44:06.160]   I mean, it does definitely paint a picture.
[01:44:06.160 --> 01:44:06.760]   Yeah.
[01:44:06.760 --> 01:44:10.080]   Fragile beauty.
[01:44:10.080 --> 01:44:12.640]   So what you're going to see is a timeline
[01:44:12.640 --> 01:44:19.360]   and evolution of, say in this case, meandering rivers
[01:44:19.360 --> 01:44:22.760]   in Bolivia and how they have changed--
[01:44:22.760 --> 01:44:26.480]   well, this doesn't look like the change is dramatic.
[01:44:26.480 --> 01:44:28.760]   This is rivers in their natural state
[01:44:28.760 --> 01:44:32.400]   unchanged by dams or other human interventions.
[01:44:32.400 --> 01:44:33.240]   So that's all.
[01:44:33.240 --> 01:44:36.560]   If you go look at like Phoenix or Las Vegas
[01:44:36.560 --> 01:44:39.840]   is another good one, you'll see like--
[01:44:39.840 --> 01:44:43.240]   well, Las Vegas is not great, but Phoenix is good.
[01:44:43.240 --> 01:44:45.200]   Yeah.
[01:44:45.200 --> 01:44:46.920]   Let's see.
[01:44:46.920 --> 01:44:48.000]   All right.
[01:44:48.000 --> 01:44:51.040]   Changing forests, warming planet,
[01:44:51.040 --> 01:44:52.120]   the effect of rising temperature--
[01:44:52.120 --> 01:44:54.040]   I think it's under like farmlands.
[01:44:54.040 --> 01:44:55.160]   The Columbia glacier.
[01:44:55.160 --> 01:44:59.360]   Do you want to see a glacier disappear in 10 years?
[01:44:59.360 --> 01:45:02.840]   It's retreated more than 20 kilometers.
[01:45:02.840 --> 01:45:05.080]   I mean, Google Earth is just fun to play with.
[01:45:05.080 --> 01:45:07.640]   But maybe some of these are a little bit depressing
[01:45:07.640 --> 01:45:10.600]   as this glacier just basically goes away.
[01:45:10.600 --> 01:45:12.280]   It's gone.
[01:45:12.280 --> 01:45:12.780]   Gone.
[01:45:12.780 --> 01:45:14.480]   And then when you say, oh, global warming,
[01:45:14.480 --> 01:45:15.920]   it's not real.
[01:45:15.920 --> 01:45:18.360]   That's pretty real.
[01:45:18.360 --> 01:45:19.440]   That kind of brings it home.
[01:45:23.560 --> 01:45:27.120]   Yeah, at least it was kind of like, oh, she actually
[01:45:27.120 --> 01:45:31.480]   was viscerally affected by this urban expansion.
[01:45:31.480 --> 01:45:34.840]   I bet we can see the Las Vegas timeline.
[01:45:34.840 --> 01:45:36.760]   Let's head to Las Vegas.
[01:45:36.760 --> 01:45:38.280]   War.
[01:45:38.280 --> 01:45:39.120]   Down for that.
[01:45:39.120 --> 01:45:39.840]   Yeah.
[01:45:39.840 --> 01:45:43.560]   Going back to 1987, Vegas is going
[01:45:43.560 --> 01:45:45.880]   to spread out over the desert.
[01:45:45.880 --> 01:45:48.200]   It's this is one of the fastest growing regions
[01:45:48.200 --> 01:45:48.800]   of the country.
[01:45:48.800 --> 01:45:53.000]   Kind of cool.
[01:45:53.000 --> 01:45:55.680]   Yeah, kind of cool.
[01:45:55.680 --> 01:45:58.760]   All right, let's continue on with the change log.
[01:45:58.760 --> 01:46:04.960]   Google Meet is getting a UI refresh and smarter meeting
[01:46:04.960 --> 01:46:05.720]   features.
[01:46:05.720 --> 01:46:09.000]   Starting next month, you'll be able to do background.
[01:46:09.000 --> 01:46:10.880]   Google Meet has been a little bit laggard
[01:46:10.880 --> 01:46:12.120]   compared to some of the other.
[01:46:12.120 --> 01:46:13.320]   Yeah, surprisingly.
[01:46:13.320 --> 01:46:14.320]   Yeah.
[01:46:14.320 --> 01:46:16.440]   You're going to be able to get background video, automatic
[01:46:16.440 --> 01:46:19.880]   light adjustments, and a whole lot more.
[01:46:19.880 --> 01:46:21.440]   You can adjust your own.
[01:46:21.440 --> 01:46:23.200]   This I've wanted for a long time.
[01:46:23.200 --> 01:46:25.040]   Video feed position.
[01:46:25.040 --> 01:46:27.280]   Whenever I'm on a meet, I'm always
[01:46:27.280 --> 01:46:29.960]   in the little tiny thumbnail in the upper right hand corner.
[01:46:29.960 --> 01:46:31.840]   You know, like you can't even see it.
[01:46:31.840 --> 01:46:35.920]   I want to be one of the big, big boys.
[01:46:35.920 --> 01:46:36.800]   Now you can--
[01:46:36.800 --> 01:46:37.840]   You want to be in the grid.
[01:46:37.840 --> 01:46:40.160]   I want to be in the grid, man.
[01:46:40.160 --> 01:46:44.600]   Google Meet can improve webcam brightness and visibility.
[01:46:44.600 --> 01:46:48.120]   And just for you, Jeff Jarvis paid subscribers
[01:46:48.120 --> 01:46:51.840]   to get access to a new auto zoom feature, which will follow you
[01:46:51.840 --> 01:46:56.000]   and position you in front of the cameras you move around.
[01:46:56.000 --> 01:46:57.720]   That's something Apple actually is added to.
[01:46:57.720 --> 01:46:59.560]   It's a camera on the new iMacs.
[01:46:59.560 --> 01:47:02.720]   And it's something that Google Nest Hub Max does.
[01:47:02.720 --> 01:47:04.600]   If you're in a duo conversation, it'll
[01:47:04.600 --> 01:47:05.600]   follow you around the room.
[01:47:05.600 --> 01:47:07.400]   This stuff is kind of cool.
[01:47:07.400 --> 01:47:09.160]   Google, you may remember, bought Fitbit.
[01:47:09.160 --> 01:47:11.520]   That acquisition cleared.
[01:47:11.520 --> 01:47:16.000]   And now they've announced a new Fitbit for $150.
[01:47:16.000 --> 01:47:17.920]   Fitbit's looks or looks.
[01:47:17.920 --> 01:47:20.960]   It's a fashion focused fitness band.
[01:47:20.960 --> 01:47:22.520]   Five days battery life.
[01:47:22.520 --> 01:47:23.480]   It's pretty, isn't it?
[01:47:23.480 --> 01:47:23.960]   You like it?
[01:47:23.960 --> 01:47:24.680]   That's the one we want.
[01:47:24.680 --> 01:47:28.320]   We just talked about the OnePlus and that price point.
[01:47:28.320 --> 01:47:31.320]   But yet it pretty much fails as a smartwatch.
[01:47:31.320 --> 01:47:32.800]   This is probably the one we want,
[01:47:32.800 --> 01:47:36.520]   because I know Ms. Stacy loves that OS.
[01:47:36.520 --> 01:47:40.880]   So I find this product so puzzling.
[01:47:40.880 --> 01:47:44.000]   I don't understand why it exists.
[01:47:44.000 --> 01:47:45.760]   I know that sounds like a jerk thing to say.
[01:47:45.760 --> 01:47:50.160]   But Fitbit has bands that can make something prettier.
[01:47:50.160 --> 01:47:53.840]   Like right now, I've got the quote unquote, fancy rose gold
[01:47:53.840 --> 01:47:55.280]   version and a cloth band.
[01:47:55.280 --> 01:47:57.800]   That looks like the looks pretty much.
[01:47:57.800 --> 01:47:58.640]   Right, but it's not.
[01:47:58.640 --> 01:48:00.360]   It's a charge three.
[01:48:00.360 --> 01:48:02.720]   The Lux has a skinnier this thing.
[01:48:02.720 --> 01:48:02.720]   OK.
[01:48:02.720 --> 01:48:03.360]   What's this called?
[01:48:03.360 --> 01:48:04.960]   Display.
[01:48:04.960 --> 01:48:05.840]   This thing.
[01:48:05.840 --> 01:48:07.840]   This thing.
[01:48:07.840 --> 01:48:10.560]   The little display computing thing.
[01:48:10.560 --> 01:48:13.360]   It doesn't have all of the features of the four,
[01:48:13.360 --> 01:48:15.600]   like the charge four.
[01:48:15.600 --> 01:48:19.360]   You still need your smartphone, for example, for GPS tracking.
[01:48:19.360 --> 01:48:24.280]   But it does have more features than the Fitbit Charge 3 has.
[01:48:24.280 --> 01:48:27.360]   But I'm still kind of like, eh, it's kind of a weird product.
[01:48:27.360 --> 01:48:30.480]   How about with this special edition,
[01:48:30.480 --> 01:48:34.840]   Gorgiana soft gold stainless steel Parker link bracelet?
[01:48:34.840 --> 01:48:36.520]   That's $200.
[01:48:36.520 --> 01:48:37.880]   Oh, OK.
[01:48:37.880 --> 01:48:39.600]   That's a no.
[01:48:39.600 --> 01:48:39.920]   How about--
[01:48:39.920 --> 01:48:40.440]   Other stage--
[01:48:40.440 --> 01:48:42.400]   --we're coming up.
[01:48:42.400 --> 01:48:42.920]   No.
[01:48:42.920 --> 01:48:44.080]   Black rubber.
[01:48:44.080 --> 01:48:44.800]   How about--
[01:48:44.800 --> 01:48:46.120]   That's the nicest one.
[01:48:46.120 --> 01:48:49.160]   But a white band is going to turn into grossness.
[01:48:49.160 --> 01:48:49.800]   Yeah.
[01:48:49.800 --> 01:48:51.520]   I've been there, done that.
[01:48:51.520 --> 01:48:52.320]   Let's take a look.
[01:48:52.320 --> 01:48:54.680]   Let's watch-- let's go to the tape.
[01:48:54.680 --> 01:48:56.800]   The screen is nice.
[01:48:56.800 --> 01:48:58.560]   It's more than a bracelet.
[01:48:58.560 --> 01:49:00.920]   So they're really-- this is-- they're going for the fashion,
[01:49:00.920 --> 01:49:02.280]   right?
[01:49:02.280 --> 01:49:05.000]   Yeah, that's what they're-- but this is not fashion.
[01:49:05.000 --> 01:49:06.480]   No, no smart watches.
[01:49:06.480 --> 01:49:09.120]   It's clunky, right?
[01:49:09.120 --> 01:49:12.080]   It levels up your workshops with active zone mints,
[01:49:12.080 --> 01:49:14.440]   helps improve your nights for more energized days.
[01:49:14.440 --> 01:49:16.760]   So yeah, you can do all the things that--
[01:49:16.760 --> 01:49:17.480]   Is it Android?
[01:49:17.480 --> 01:49:18.000]   No.
[01:49:18.000 --> 01:49:18.480]   It's Fitbit.
[01:49:18.480 --> 01:49:20.040]   I think having the curve--
[01:49:20.040 --> 01:49:21.280]   It's good.
[01:49:21.280 --> 01:49:24.000]   The curve and dust for design makes it a little bit more
[01:49:24.000 --> 01:49:25.520]   fashionable.
[01:49:25.520 --> 01:49:26.040]   It does.
[01:49:26.040 --> 01:49:26.640]   It makes it--
[01:49:26.640 --> 01:49:29.080]   --that levels curve off on it.
[01:49:29.080 --> 01:49:29.480]   Yep.
[01:49:29.480 --> 01:49:29.960]   You're right.
[01:49:29.960 --> 01:49:32.440]   I mean, like this guy--
[01:49:32.440 --> 01:49:32.800]   Right.
[01:49:32.800 --> 01:49:33.760]   So that's challenging.
[01:49:33.760 --> 01:49:34.280]   Not as well.
[01:49:34.280 --> 01:49:35.800]   Love their Fitbits, though, don't they?
[01:49:35.800 --> 01:49:36.840]   There's--
[01:49:36.840 --> 01:49:37.600]   I love my Fitbit.
[01:49:37.600 --> 01:49:38.320]   Yeah.
[01:49:38.320 --> 01:49:39.920]   It's interesting.
[01:49:39.920 --> 01:49:40.400]   There's real--
[01:49:40.400 --> 01:49:43.720]   I've had one since 2011.
[01:49:43.720 --> 01:49:44.720]   And I've tried others.
[01:49:44.720 --> 01:49:46.840]   I mean, don't get me wrong.
[01:49:46.840 --> 01:49:50.320]   But yeah, I had the little one that grew as a flower.
[01:49:50.320 --> 01:49:51.200]   Do you all remember the flower?
[01:49:51.200 --> 01:49:52.600]   Yeah, I do remember the flower.
[01:49:52.600 --> 01:49:53.040]   Yeah.
[01:49:53.040 --> 01:49:55.720]   I remember the one that I lost every other month.
[01:49:55.720 --> 01:49:58.560]   Oh, that was the one that went in my saggy pant band.
[01:49:58.560 --> 01:49:59.040]   Yeah.
[01:49:59.040 --> 01:49:59.560]   Yeah.
[01:49:59.560 --> 01:50:04.400]   I clipped onto my bra strap or into my bra strap right there.
[01:50:04.400 --> 01:50:06.640]   And it would always end up in the wash every time.
[01:50:06.640 --> 01:50:08.200]   I went through so many of them.
[01:50:08.200 --> 01:50:08.720]   Yep.
[01:50:08.720 --> 01:50:10.040]   That's before it was a wash.
[01:50:10.040 --> 01:50:10.520]   It was went out.
[01:50:10.520 --> 01:50:12.920]   Actually, work gave up on him because work
[01:50:12.920 --> 01:50:14.120]   was provided to me.
[01:50:14.120 --> 01:50:15.880]   Oh, that was nice of work.
[01:50:15.880 --> 01:50:17.880]   And yeah, I was like, I'll forget it.
[01:50:17.880 --> 01:50:19.880]   Just stop giving him some.
[01:50:19.880 --> 01:50:21.400]   [LAUGHING]
[01:50:21.400 --> 01:50:23.360]   No more gear for Ant.
[01:50:23.360 --> 01:50:28.400]   Chrome OS90 is here, live captioning built-in,
[01:50:28.400 --> 01:50:32.360]   diagnostic app, and more.
[01:50:32.360 --> 01:50:34.360]   And for some of you, not all of you,
[01:50:34.360 --> 01:50:35.880]   a small percentage of you flock.
[01:50:35.880 --> 01:50:39.680]   No, it's all--
[01:50:39.680 --> 01:50:42.240]   It's all immobile flockets.
[01:50:42.240 --> 01:50:44.480]   Oh, well, is it?
[01:50:44.480 --> 01:50:45.400]   I don't know.
[01:50:45.400 --> 01:50:46.360]   It's only mobile.
[01:50:46.360 --> 01:50:47.360]   Yeah, I think so.
[01:50:47.360 --> 01:50:48.160]   Oh, I didn't know that.
[01:50:48.160 --> 01:50:50.080]   OK.
[01:50:50.080 --> 01:50:53.360]   By the way, I touted a week ago that we had this wonderful thing.
[01:50:53.360 --> 01:50:54.160]   I was so happy.
[01:50:54.160 --> 01:50:56.480]   And Google heard it was happy and took it away.
[01:50:56.480 --> 01:50:57.480]   Oh, oh.
[01:50:57.480 --> 01:50:59.560]   That you could do the multiple cuts and pastes
[01:50:59.560 --> 01:51:00.720]   and it would show you all of them.
[01:51:00.720 --> 01:51:02.320]   I now went through that for me anymore.
[01:51:02.320 --> 01:51:04.720]   Oh, really?
[01:51:04.720 --> 01:51:05.560]   Was it maybe an extension?
[01:51:05.560 --> 01:51:08.160]   Oh, they realized I have a workspace account.
[01:51:08.160 --> 01:51:10.320]   And said, no, you can't have that one.
[01:51:10.320 --> 01:51:12.280]   [LAUGHTER]
[01:51:12.280 --> 01:51:14.520]   It's got a scanner app.
[01:51:14.520 --> 01:51:17.560]   Google Maps and YouTube now open a standalone app
[01:51:17.560 --> 01:51:20.120]   Windows rather than tabs.
[01:51:20.120 --> 01:51:23.640]   You can enable offline access for Docs, Sheets, and Slides
[01:51:23.640 --> 01:51:26.240]   in Google Drive.
[01:51:26.240 --> 01:51:28.160]   They have deprecated a few codecs
[01:51:28.160 --> 01:51:30.800]   that you weren't using anyway.
[01:51:30.800 --> 01:51:32.600]   And a redesigned account manager
[01:51:32.600 --> 01:51:34.760]   is available under accounts instead of people
[01:51:34.760 --> 01:51:37.520]   to help users better understand how accounts work.
[01:51:37.520 --> 01:51:39.480]   Thank you, Google.
[01:51:39.480 --> 01:51:44.440]   Google's adding a price tracking tool to Chrome for Android.
[01:51:44.440 --> 01:51:49.120]   This is something that really freaked me out on Microsoft Edge.
[01:51:49.120 --> 01:51:52.040]   When you started buying something on Edge,
[01:51:52.040 --> 01:51:53.920]   you would go out and see if it could buy a better price,
[01:51:53.920 --> 01:51:55.680]   find a better price without even asking you.
[01:51:55.680 --> 01:51:57.400]   Kind of like Honey or one of those.
[01:51:57.400 --> 01:51:58.400]   Yeah.
[01:51:58.400 --> 01:52:00.600]   It's built into Microsoft's Edge.
[01:52:00.600 --> 01:52:02.800]   But I thought Honey, you had to opt in for that.
[01:52:02.800 --> 01:52:03.120]   You do.
[01:52:03.120 --> 01:52:04.640]   Microsoft just does it.
[01:52:04.640 --> 01:52:05.080]   Oh.
[01:52:05.080 --> 01:52:07.440]   And actually, they saved me a little bit of money
[01:52:07.440 --> 01:52:09.800]   on a computer I was buying.
[01:52:09.800 --> 01:52:12.400]   Price tracking is now in Chrome.
[01:52:12.400 --> 01:52:14.760]   You can track prices on tabs.
[01:52:14.760 --> 01:52:16.520]   See, price drops on your tabs.
[01:52:16.520 --> 01:52:17.600]   Get price drop alerts.
[01:52:17.600 --> 01:52:20.840]   Yeah.
[01:52:20.840 --> 01:52:22.920]   It shows you how much people do shopping on mobile.
[01:52:22.920 --> 01:52:24.680]   That's for sure.
[01:52:24.680 --> 01:52:26.000]   Wow.
[01:52:26.000 --> 01:52:27.800]   Still not in the Chrome stable release, though.
[01:52:27.800 --> 01:52:36.240]   This is for users of the Canary update of version 9.0.
[01:52:36.240 --> 01:52:41.400]   Google's rolling out Android 12 developer preview 3
[01:52:41.400 --> 01:52:46.880]   for Pixel phones as we inch toward the release of Android 12.
[01:52:46.880 --> 01:52:50.560]   It's an incremental update for stability and performance.
[01:52:50.560 --> 01:52:52.360]   Developers are advised to, quote,
[01:52:52.360 --> 01:52:58.440]   "get apps ready for the consumer beta because that's coming."
[01:52:58.440 --> 01:53:02.080]   I would-- you know, they did this at Google I/O last time
[01:53:02.080 --> 01:53:03.480]   a couple of years ago.
[01:53:03.480 --> 01:53:05.040]   I wonder if that's what'll happen, though.
[01:53:05.040 --> 01:53:07.120]   That's actually pretty soon.
[01:53:07.120 --> 01:53:09.640]   I bet you that's what they're planning.
[01:53:09.640 --> 01:53:10.000]   May--
[01:53:10.000 --> 01:53:12.080]   Well, of course, rolling out the developer preview.
[01:53:12.080 --> 01:53:12.960]   No, the public beta.
[01:53:12.960 --> 01:53:14.520]   Developer previews are out.
[01:53:14.520 --> 01:53:18.880]   Public beta release in May, final release sometime
[01:53:18.880 --> 01:53:22.520]   in the fall for Android 12.
[01:53:22.520 --> 01:53:31.960]   And Google search has added an AR Pac-Man 3D Hello Kitty,
[01:53:31.960 --> 01:53:34.760]   Gundam, and other Chinese--
[01:53:34.760 --> 01:53:39.160]   I'm sorry, Japanese anime characters.
[01:53:39.160 --> 01:53:40.000]   How does it work?
[01:53:40.000 --> 01:53:41.600]   Do I have to-- do I search for it?
[01:53:41.600 --> 01:53:43.560]   What do I do to get that to work?
[01:53:43.560 --> 01:53:46.240]   So I-- so I search.
[01:53:46.240 --> 01:53:47.400]   Here, I'll show you a video.
[01:53:47.400 --> 01:53:49.560]   That'll be easier than me doing it for you.
[01:53:49.560 --> 01:53:51.120]   Here's a kitty cat.
[01:53:51.120 --> 01:53:56.920]   Your favorite hungry character Pac-Man shows up in your phone
[01:53:56.920 --> 01:54:00.240]   and then you can watch a Pac-Man game in AR.
[01:54:00.240 --> 01:54:01.360]   You've done this before, right?
[01:54:01.360 --> 01:54:02.760]   You've done various little characters
[01:54:02.760 --> 01:54:03.640]   you could project into.
[01:54:03.640 --> 01:54:04.800]   I have a life.
[01:54:04.800 --> 01:54:06.440]   Yeah.
[01:54:06.440 --> 01:54:07.960]   This doesn't look like that.
[01:54:07.960 --> 01:54:08.640]   It's cute.
[01:54:08.640 --> 01:54:09.720]   I like it.
[01:54:09.720 --> 01:54:12.960]   I like it cute Pac-Man wandering around,
[01:54:12.960 --> 01:54:14.240]   eating dots on your table.
[01:54:14.240 --> 01:54:16.000]   This is as far as AR has ever gotten.
[01:54:16.000 --> 01:54:16.280]   Is this--
[01:54:16.280 --> 01:54:17.640]   Yeah, it's a toy.
[01:54:17.640 --> 01:54:18.760]   Yeah, I agree.
[01:54:18.760 --> 01:54:22.400]   Even with Apple, and they have spent so much money on AR.
[01:54:22.400 --> 01:54:22.800]   We do.
[01:54:22.800 --> 01:54:23.680]   That's not true.
[01:54:23.680 --> 01:54:24.920]   That's not true.
[01:54:24.920 --> 01:54:28.160]   Yo, I used AR when I was by-- so when we bought the house,
[01:54:28.160 --> 01:54:30.440]   we obviously had to get some furniture and stuff.
[01:54:30.440 --> 01:54:32.200]   Did you have an AR walk through?
[01:54:32.200 --> 01:54:33.320]   Oh, you boxed up.
[01:54:33.320 --> 01:54:34.840]   Yeah, well, I did use--
[01:54:34.840 --> 01:54:38.960]   I used a couple stores actually have the tools.
[01:54:38.960 --> 01:54:41.320]   So you just-- you would bring it up in your room.
[01:54:41.320 --> 01:54:41.880]   And Amazon does.
[01:54:41.880 --> 01:54:44.080]   And it was actually really kind of neat.
[01:54:44.080 --> 01:54:44.640]   That's really neat.
[01:54:44.640 --> 01:54:47.840]   Amazon does that too for a lot of their devices and products.
[01:54:47.840 --> 01:54:49.960]   You can place it in the room.
[01:54:49.960 --> 01:54:53.240]   We've done that a couple of times with lamps.
[01:54:53.240 --> 01:54:53.760]   Hey.
[01:54:53.760 --> 01:54:55.440]   Yeah, I did it for lamps.
[01:54:55.440 --> 01:54:56.640]   And that's--
[01:54:56.640 --> 01:54:59.440]   Google Chimes.
[01:54:59.440 --> 01:55:01.520]   I know, Stacey, you're from Austin,
[01:55:01.520 --> 01:55:04.960]   and you knew what a hardship it was when COVID-19 shut down
[01:55:04.960 --> 01:55:06.920]   by Southwest last year.
[01:55:06.920 --> 01:55:09.320]   It was shut down again this year.
[01:55:09.320 --> 01:55:15.360]   They have sold a 50% stake to the publishers of Rolling Stone
[01:55:15.360 --> 01:55:16.920]   magazine, Penske Media Corp.
[01:55:16.920 --> 01:55:19.440]   What another way they got to-- they got some bailout money
[01:55:19.440 --> 01:55:21.040]   and then gave away 50% of the credit.
[01:55:21.040 --> 01:55:21.520]   What is it?
[01:55:21.520 --> 01:55:23.000]   Yeah, that's how I would look at it.
[01:55:23.000 --> 01:55:23.520]   All right.
[01:55:23.520 --> 01:55:24.080]   Yeah, because--
[01:55:24.080 --> 01:55:27.320]   Rolling Stone finally got their claws into Southwest.
[01:55:27.320 --> 01:55:28.960]   Rolling Stone, Billboard and Variety.
[01:55:28.960 --> 01:55:31.240]   So those are really three of the big entertainment
[01:55:31.240 --> 01:55:33.720]   magazines.
[01:55:33.720 --> 01:55:36.640]   The problem was that the cancellations to the pandemic
[01:55:36.640 --> 01:55:38.520]   were not covered by insurance.
[01:55:38.520 --> 01:55:40.600]   So it's thought, according to the Wall Street Journal,
[01:55:40.600 --> 01:55:44.360]   a cancellation cost millions.
[01:55:44.360 --> 01:55:45.400]   Last I knew--
[01:55:45.400 --> 01:55:46.880]   I don't know if this is told in case.
[01:55:46.880 --> 01:55:49.560]   Or last I knew Penske was a Saudi money.
[01:55:49.560 --> 01:55:51.360]   Oh, interesting.
[01:55:51.360 --> 01:55:54.960]   Yeah, how long has Vince's corner media owned it?
[01:55:54.960 --> 01:55:56.080]   Oh, a few years.
[01:55:56.080 --> 01:55:58.520]   [LAUGHTER]
[01:55:58.520 --> 01:56:03.920]   Penske is interesting.
[01:56:03.920 --> 01:56:06.440]   It's run by Jay Pinsky.
[01:56:06.440 --> 01:56:08.400]   Jay Pinsky, yeah, son of--
[01:56:08.400 --> 01:56:09.080]   founder Penske.
[01:56:09.080 --> 01:56:10.920]   Do they make the vans?
[01:56:10.920 --> 01:56:12.600]   That's the best dad.
[01:56:12.600 --> 01:56:13.600]   It's the Penske--
[01:56:13.600 --> 01:56:14.080]   That's the race car.
[01:56:14.080 --> 01:56:15.880]   It's the race car, Penske's.
[01:56:15.880 --> 01:56:16.840]   Of the race car?
[01:56:16.840 --> 01:56:18.640]   Penske's.
[01:56:18.640 --> 01:56:23.760]   The Penske Corporation, Roger Penske, former race car driver.
[01:56:23.760 --> 01:56:26.800]   His son now runs it.
[01:56:26.800 --> 01:56:28.800]   Oh, they founded Firefly, which
[01:56:28.800 --> 01:56:30.840]   is a wireless company for children.
[01:56:30.840 --> 01:56:32.760]   Remember that?
[01:56:32.760 --> 01:56:34.880]   In the mid-2000s.
[01:56:34.880 --> 01:56:36.040]   He's an interesting entrepreneur.
[01:56:36.040 --> 01:56:37.080]   Yeah.
[01:56:37.080 --> 01:56:40.160]   Co-owner of the IndyCar Series racing team, Dragon Racing.
[01:56:40.160 --> 01:56:46.960]   And now, you know, and Billboard variety and Rolling Stone.
[01:56:46.960 --> 01:56:50.320]   And now, they're half owners of South By.
[01:56:50.320 --> 01:56:52.840]   I think that's probably as good as you're going to get in terms--
[01:56:52.840 --> 01:56:53.440]   I think it is.
[01:56:53.440 --> 01:56:54.360]   I think it is.
[01:56:54.360 --> 01:56:58.080]   It'll probably bring in some synergy, as we like to say.
[01:56:58.080 --> 01:56:59.080]   Yeah.
[01:56:59.080 --> 01:57:00.080]   Showbiz.
[01:57:00.080 --> 01:57:02.080]   Yeah.
[01:57:02.080 --> 01:57:03.080]   Can I--
[01:57:03.080 --> 01:57:05.080]   It's--
[01:57:05.080 --> 01:57:07.320]   Elon's Vegas tube.
[01:57:07.320 --> 01:57:09.520]   Oh, the boring tube.
[01:57:09.520 --> 01:57:10.080]   It's so boring.
[01:57:10.080 --> 01:57:11.600]   The boring tube.
[01:57:11.600 --> 01:57:17.520]   The video I put in there tears it to shreds.
[01:57:17.520 --> 01:57:18.400]   To shreds.
[01:57:18.400 --> 01:57:20.960]   And this is the man we're going to trust to go to the moon.
[01:57:21.960 --> 01:57:25.800]   Um, well, let's be fair.
[01:57:25.800 --> 01:57:29.800]   It's just a hole in the ground with a car in it.
[01:57:29.800 --> 01:57:30.800]   Well, let's--
[01:57:30.800 --> 01:57:31.800]   Well, it's a bit of a car.
[01:57:31.800 --> 01:57:33.200]   It was supposed to be, you know, like a people mover thing.
[01:57:33.200 --> 01:57:34.200]   Yeah, no.
[01:57:34.200 --> 01:57:35.880]   And maybe it will be someday.
[01:57:35.880 --> 01:57:37.640]   Now, it's a Tesla with a driver.
[01:57:37.640 --> 01:57:39.280]   And it goes 35 miles an hour.
[01:57:39.280 --> 01:57:40.280]   Here we go.
[01:57:40.280 --> 01:57:43.040]   Debunking Vegas Loop.
[01:57:43.040 --> 01:57:44.360]   It's 50 miles.
[01:57:44.360 --> 01:57:45.680]   But it's amazing.
[01:57:45.680 --> 01:57:49.320]   Well, the video of the cars moving through the loop
[01:57:49.320 --> 01:57:52.960]   is hysterical because they're not moving in exactly--
[01:57:52.960 --> 01:57:55.920]   It's original bird at 5 minutes and 50 seconds.
[01:57:55.920 --> 01:57:57.560]   This was a complete map of the track
[01:57:57.560 --> 01:57:59.680]   without stops, without passengers.
[01:57:59.680 --> 01:58:01.640]   And it took 5 minutes and 35 seconds.
[01:58:01.640 --> 01:58:02.640]   That's the plan.
[01:58:02.640 --> 01:58:04.240]   It takes people a minute to load up
[01:58:04.240 --> 01:58:06.520]   and another minute to leave the vehicle on arrival
[01:58:06.520 --> 01:58:08.840]   as new passengers are climbing into the car.
[01:58:08.840 --> 01:58:11.400]   That gives us about 7 and 1/2 minutes per round trip
[01:58:11.400 --> 01:58:12.560]   as an estimate.
[01:58:12.560 --> 01:58:14.640]   And as it turns out, we found that exact number
[01:58:14.640 --> 01:58:16.640]   on some of the projects related documents,
[01:58:16.640 --> 01:58:19.040]   especially in regards to fire code.
[01:58:19.040 --> 01:58:22.480]   The claim is this system can carry 4400 people per hour
[01:58:22.480 --> 01:58:24.360]   and there are 62 cars.
[01:58:24.360 --> 01:58:27.560]   One of these cars can carry three, possibly four people,
[01:58:27.560 --> 01:58:30.680]   and do a lap every 7 and 1/2 minutes.
[01:58:30.680 --> 01:58:33.880]   Add it all up, and the max output on this system
[01:58:33.880 --> 01:58:37.360]   is 1,488 people per hour if there's
[01:58:37.360 --> 01:58:38.960]   three people per vehicle.
[01:58:38.960 --> 01:58:41.520]   1,984 if they cram four--
[01:58:41.520 --> 01:58:42.720]   This is a take down there.
[01:58:42.720 --> 01:58:44.120]   Isn't there even enough room in the car?
[01:58:44.120 --> 01:58:44.600]   No, the other one.
[01:58:44.600 --> 01:58:45.440]   This is the--
[01:58:45.440 --> 01:58:46.280]   What's in the car?
[01:58:46.280 --> 01:58:47.200]   It's just a hole.
[01:58:47.200 --> 01:58:47.680]   Second.
[01:58:47.680 --> 01:58:49.120]   There's no way to get out of the vehicle.
[01:58:49.120 --> 01:58:51.320]   Keep people out of danger from approaching vehicles
[01:58:51.320 --> 01:58:52.960]   when they're out of the car.
[01:58:52.960 --> 01:58:54.360]   But what if the car breaks down?
[01:58:54.360 --> 01:58:55.280]   It's the exit.
[01:58:55.280 --> 01:58:55.680]   Yeah.
[01:58:55.680 --> 01:58:56.680]   Signs.
[01:58:56.680 --> 01:58:58.160]   Directional arrows.
[01:58:58.160 --> 01:59:00.160]   How about reflective tape?
[01:59:00.160 --> 01:59:02.080]   Nope on all counts.
[01:59:02.080 --> 01:59:04.560]   What if it catches fire as Tesla's doer?
[01:59:04.560 --> 01:59:05.920]   There's no way for a truck.
[01:59:05.920 --> 01:59:06.720]   You get in there.
[01:59:06.720 --> 01:59:08.560]   Fire alarm pull stations?
[01:59:08.560 --> 01:59:09.840]   You won't spot any.
[01:59:09.840 --> 01:59:10.760]   Ventilation checks.
[01:59:10.760 --> 01:59:11.120]   I'm not--
[01:59:11.120 --> 01:59:13.360]   I would have assumed all of that was on a road map
[01:59:13.360 --> 01:59:14.160]   form anyway.
[01:59:14.160 --> 01:59:15.680]   None of those either.
[01:59:15.680 --> 01:59:18.040]   In fact, the paperwork outlining this project
[01:59:18.040 --> 01:59:20.520]   very clearly states that this mass transport tunnel
[01:59:20.520 --> 01:59:24.040]   system has no fire sprinklers, smoke detectors,
[01:59:24.040 --> 01:59:24.880]   or fire detectors.
[01:59:24.880 --> 01:59:25.560]   Oh, Lord.
[01:59:25.560 --> 01:59:26.520]   I'm not getting on there.
[01:59:26.520 --> 01:59:27.520]   Well, that's a problem.
[01:59:27.520 --> 01:59:29.520]   OK.
[01:59:29.520 --> 01:59:30.040]   All right.
[01:59:30.040 --> 01:59:30.880]   And you can't get out.
[01:59:30.880 --> 01:59:31.880]   I'm not getting out.
[01:59:31.880 --> 01:59:35.120]   There's not room to go around the cars to get out.
[01:59:35.120 --> 01:59:36.880]   Well, here's the good news.
[01:59:36.880 --> 01:59:40.320]   SpaceX has won the contract with NASA
[01:59:40.320 --> 01:59:43.680]   to take people to the moon.
[01:59:43.680 --> 01:59:46.240]   And there is somewhere to get out on the moon.
[01:59:46.240 --> 01:59:48.400]   There's a lot of places you can get out.
[01:59:48.400 --> 01:59:51.960]   You don't need fire suppression because there's no air.
[01:59:51.960 --> 01:59:55.640]   This is actually a big deal.
[01:59:55.640 --> 02:00:00.240]   NASA was not funded, really, for the moon mission,
[02:00:00.240 --> 02:00:03.040]   even though President Trump was a big supporter of a Congress
[02:00:03.040 --> 02:00:04.520]   never allocated the money.
[02:00:04.520 --> 02:00:06.280]   So they just didn't have the money.
[02:00:06.280 --> 02:00:12.280]   The bids were out to the Jeff Bezos' blue origin
[02:00:12.280 --> 02:00:15.000]   and to a space company called Dynetics.
[02:00:15.000 --> 02:00:17.840]   But SpaceX dramatically underbid those two.
[02:00:17.840 --> 02:00:20.160]   And since NASA didn't have any money,
[02:00:20.160 --> 02:00:25.440]   they gave the contract the $2.9 billion contract to SpaceX.
[02:00:25.440 --> 02:00:31.520]   Because NASA already has a launch system in progress,
[02:00:31.520 --> 02:00:34.760]   even though the Starship Musk's building
[02:00:34.760 --> 02:00:37.680]   is capable of launching the astronauts from a launching
[02:00:37.680 --> 02:00:40.360]   pad and taking them to the moon and taking them back,
[02:00:40.360 --> 02:00:45.520]   they won't be doing that because NASA has to use its SLS Orion
[02:00:45.520 --> 02:00:48.440]   project, which they have spent crazy billions of dollars
[02:00:48.440 --> 02:00:49.360]   building.
[02:00:49.360 --> 02:00:51.560]   So the astronauts will get in the Orion.
[02:00:51.560 --> 02:00:55.720]   They'll fly to the SpaceX Starship,
[02:00:55.720 --> 02:00:57.640]   which will then take them the rest of the way of the moon.
[02:00:57.640 --> 02:00:59.320]   So there's a transfer.
[02:00:59.320 --> 02:01:00.440]   And then--
[02:01:00.440 --> 02:01:01.720]   Well, I just hate that on the subway.
[02:01:01.720 --> 02:01:02.240]   I know.
[02:01:02.240 --> 02:01:02.560]   I know.
[02:01:02.560 --> 02:01:04.680]   I just hate that one to make a transfer.
[02:01:04.680 --> 02:01:05.120]   Yeah.
[02:01:05.120 --> 02:01:07.880]   And then what's cool about the Starship
[02:01:07.880 --> 02:01:10.880]   is there's no lunar lander.
[02:01:10.880 --> 02:01:13.200]   The other company's bids were much more
[02:01:13.200 --> 02:01:16.040]   conventional with a separate lunar lander.
[02:01:16.040 --> 02:01:17.640]   As you may know, because we've seen
[02:01:17.640 --> 02:01:20.320]   the video of the Starship landing on Earth,
[02:01:20.320 --> 02:01:22.360]   that's how Starship's going to land on the moon, just
[02:01:22.360 --> 02:01:24.720]   like a 1950s rocket mosh.
[02:01:24.720 --> 02:01:27.560]   It's going to turn around.
[02:01:27.560 --> 02:01:30.680]   It's going to fire its thrusters and slowly lower the moon.
[02:01:30.680 --> 02:01:33.520]   And then-- and you can kind of see in this picture--
[02:01:33.520 --> 02:01:35.520]   the astronauts-- let's see if I can get
[02:01:35.520 --> 02:01:37.360]   zoom in a little bit on this picture--
[02:01:37.360 --> 02:01:43.120]   are going to get into a crane mechanism for window washers.
[02:01:43.120 --> 02:01:45.840]   And they're going to be lowered from--
[02:01:45.840 --> 02:01:47.320]   I say, I could go to the moon.
[02:01:47.320 --> 02:01:48.720]   And that was-- I'd say, I'm not leaving it.
[02:01:48.720 --> 02:01:49.720]   I'm staying in.
[02:01:49.720 --> 02:01:50.440]   I'm not doing that.
[02:01:50.440 --> 02:01:51.720]   It doesn't look like fun.
[02:01:51.720 --> 02:01:55.200]   Then lowered onto the surface of the moon.
[02:01:55.200 --> 02:01:55.800]   But it will--
[02:01:55.800 --> 02:01:59.400]   There's been people that signed up to go on this trip, right?
[02:01:59.400 --> 02:02:00.160]   I'd go.
[02:02:00.160 --> 02:02:02.640]   I don't care about the window washer maneuver.
[02:02:02.640 --> 02:02:03.280]   I'm at it.
[02:02:03.280 --> 02:02:04.360]   So I'm at it.
[02:02:04.360 --> 02:02:05.240]   Yeah, I don't care about that.
[02:02:05.240 --> 02:02:08.120]   If my daughter was older, I'd go.
[02:02:08.120 --> 02:02:08.720]   Oh, yeah.
[02:02:08.720 --> 02:02:09.960]   Well, don't worry.
[02:02:09.960 --> 02:02:11.160]   Your daughter will be older.
[02:02:11.160 --> 02:02:11.320]   You'll be older.
[02:02:11.320 --> 02:02:14.160]   Well, yeah.
[02:02:14.160 --> 02:02:17.320]   They're hoping for 2026.
[02:02:17.320 --> 02:02:18.480]   I think 2025.
[02:02:18.480 --> 02:02:20.040]   They're hoping to do it pretty soon.
[02:02:20.040 --> 02:02:24.040]   NASA originally-- Trump originally promised, I think, 2024.
[02:02:24.040 --> 02:02:25.800]   But I think we're thinking maybe it's
[02:02:25.800 --> 02:02:29.120]   going to take a little longer than that.
[02:02:29.120 --> 02:02:30.040]   So you got time.
[02:02:30.040 --> 02:02:32.520]   Your daughter might be grown by the time you can go.
[02:02:32.520 --> 02:02:34.640]   So really, you would go to the moon because so would I.
[02:02:34.640 --> 02:02:37.080]   I think it sounds like fun.
[02:02:37.080 --> 02:02:38.920]   I want to see the Earth from space.
[02:02:38.920 --> 02:02:39.720]   Yeah.
[02:02:39.720 --> 02:02:40.720]   I just--
[02:02:40.720 --> 02:02:40.720]   Yeah.
[02:02:40.720 --> 02:02:45.720]   Yeah, dude, too great.
[02:02:45.720 --> 02:02:46.720]   I don't know why.
[02:02:46.720 --> 02:02:47.720]   It's like a spirit.
[02:02:47.720 --> 02:02:48.680]   I'm like--
[02:02:48.680 --> 02:02:50.520]   I do too.
[02:02:50.520 --> 02:02:53.520]   Well, I mean, I don't want to do it today.
[02:02:53.520 --> 02:02:58.640]   I want to do it in maybe 10, 20 years I would want to do it.
[02:02:58.640 --> 02:03:00.640]   I mean, when I'm closer to the end of my life,
[02:03:00.640 --> 02:03:03.240]   but I still want to appreciate things, just in case
[02:03:03.240 --> 02:03:05.320]   I don't come back.
[02:03:05.320 --> 02:03:06.280]   I want to do it, too.
[02:03:06.280 --> 02:03:11.800]   But I want to do it along the lines of how we have private charter
[02:03:11.800 --> 02:03:13.720]   for airplanes.
[02:03:13.720 --> 02:03:14.680]   They really take--
[02:03:14.680 --> 02:03:16.640]   Well, Virgin Galactic is going to do that.
[02:03:16.640 --> 02:03:17.720]   They won't take you to the moon.
[02:03:17.720 --> 02:03:19.080]   They'll just take you to the lower orbit.
[02:03:19.080 --> 02:03:22.880]   But you could see the Earth as a planet come back.
[02:03:22.880 --> 02:03:23.880]   And then bring me back.
[02:03:23.880 --> 02:03:26.040]   Just a couple hundred thousand.
[02:03:26.040 --> 02:03:29.520]   So if you're really rich in this country,
[02:03:29.520 --> 02:03:32.360]   you create something called a family office
[02:03:32.360 --> 02:03:34.640]   to do your investing.
[02:03:34.640 --> 02:03:37.600]   The Yamaguchi family is a founding family--
[02:03:37.600 --> 02:03:42.040]   or Yamauchi family is a founding family of Nintendo.
[02:03:42.040 --> 02:03:43.760]   Their family office has a website.
[02:03:43.760 --> 02:03:45.920]   Turn on the sound for this one.
[02:03:45.920 --> 02:03:48.480]   John, did you post this?
[02:03:48.480 --> 02:03:49.040]   I did.
[02:03:49.040 --> 02:03:52.600]   Yeah, I figured it was a Jarvis.
[02:03:52.600 --> 02:03:59.520]   Yamauchi number 10, family office is basically a little--
[02:03:59.520 --> 02:04:02.520]   so move over the nav at the left and sound.
[02:04:02.520 --> 02:04:07.760]   It's a little--
[02:04:07.760 --> 02:04:08.260]   I don't want to--
[02:04:08.260 --> 02:04:09.840]   It's a little game.
[02:04:09.840 --> 02:04:10.640]   Philosophy.
[02:04:10.640 --> 02:04:12.800]   Ooh.
[02:04:12.800 --> 02:04:14.200]   Look at that.
[02:04:14.200 --> 02:04:15.200]   This is so cool.
[02:04:15.200 --> 02:04:15.700]   OK.
[02:04:15.700 --> 02:04:18.480]   We're putting some of the family money into the website,
[02:04:18.480 --> 02:04:20.840]   actually.
[02:04:20.840 --> 02:04:22.920]   OK.
[02:04:22.920 --> 02:04:26.720]   It's in Japanese, but I'm sure it's very exciting.
[02:04:26.720 --> 02:04:28.600]   I wish I could play a game on this, though.
[02:04:28.600 --> 02:04:30.720]   It feels like I should be able to use the arrow keys
[02:04:30.720 --> 02:04:34.000]   to set something.
[02:04:34.000 --> 02:04:38.960]   Where is mine in Nintendo emulator?
[02:04:38.960 --> 02:04:41.720]   I think that's pretty cool.
[02:04:41.720 --> 02:04:42.960]   Pretty cool.
[02:04:42.960 --> 02:04:44.000]   Thank you, Jeff, for that.
[02:04:44.000 --> 02:04:45.440]   That's a good one.
[02:04:45.440 --> 02:04:47.240]   It's cool.
[02:04:47.240 --> 02:04:48.000]   Anything else?
[02:04:48.000 --> 02:04:48.760]   What have I missed?
[02:04:48.760 --> 02:04:50.120]   There seems like a lot more stories,
[02:04:50.120 --> 02:04:52.280]   but I just don't know if I've missed anything important.
[02:04:55.920 --> 02:05:00.080]   So will you put your palm over the machine
[02:05:00.080 --> 02:05:01.760]   to get into Whole Foods and pay?
[02:05:01.760 --> 02:05:02.120]   Sure.
[02:05:02.120 --> 02:05:03.440]   Why wouldn't I?
[02:05:03.440 --> 02:05:03.960]   Right.
[02:05:03.960 --> 02:05:04.480]   Why not?
[02:05:04.480 --> 02:05:05.320]   Why not?
[02:05:05.320 --> 02:05:07.480]   Well, here's the argument against it.
[02:05:07.480 --> 02:05:12.440]   That means Whole Foods will now have the biometrics of my palm
[02:05:12.440 --> 02:05:15.480]   and who knows how well they will be--
[02:05:15.480 --> 02:05:16.560]   Well, it's Amazon.
[02:05:16.560 --> 02:05:17.840]   Whole Foods is owned by Amazon.
[02:05:17.840 --> 02:05:18.800]   Amazon.
[02:05:18.800 --> 02:05:22.240]   And I can't exactly change my palm print.
[02:05:22.240 --> 02:05:24.840]   So that kind of gives them biometric--
[02:05:24.840 --> 02:05:26.040]   When you rob the bank.
[02:05:26.040 --> 02:05:27.400]   Yeah, about me.
[02:05:27.400 --> 02:05:29.120]   Maybe I don't want them to have.
[02:05:29.120 --> 02:05:31.200]   Disney used to do that, a fingerprint,
[02:05:31.200 --> 02:05:34.680]   to get into and get on the rides at Disney Parks.
[02:05:34.680 --> 02:05:37.800]   And we talked about this many years ago on Steam years ago,
[02:05:37.800 --> 02:05:41.360]   suggesting people use their knuckle instead.
[02:05:41.360 --> 02:05:42.000]   Years ago.
[02:05:42.000 --> 02:05:44.840]   Do you have your name with knuckle?
[02:05:44.840 --> 02:05:47.360]   I don't know, but maybe it would fool the Disney.
[02:05:47.360 --> 02:05:49.680]   We thought it maybe wasn't the best plan to give Disney
[02:05:49.680 --> 02:05:52.760]   your fingerprint, because you can't change it.
[02:05:52.760 --> 02:05:53.480]   You can't change it.
[02:05:53.480 --> 02:05:56.480]   But on the other hand, you could.
[02:05:56.480 --> 02:05:58.120]   It would just be incredibly painful.
[02:05:58.120 --> 02:05:58.640]   And/or.
[02:05:58.640 --> 02:06:03.280]   [LAUGHTER]
[02:06:03.280 --> 02:06:05.880]   But sometimes privacy is worth it.
[02:06:05.880 --> 02:06:06.800]   All right, let's--
[02:06:06.800 --> 02:06:08.840]   You know, we've all seen the Bourne identity.
[02:06:08.840 --> 02:06:11.040]   Or is it the Bourne ones where he brings up bigger hips?
[02:06:11.040 --> 02:06:11.360]   He changes.
[02:06:11.360 --> 02:06:12.480]   Yeah, we have.
[02:06:12.480 --> 02:06:13.960]   Yeah.
[02:06:13.960 --> 02:06:16.280]   Let's finish this thing.
[02:06:16.280 --> 02:06:19.600]   Let's wrap this thing up with some picks and tips and numbers.
[02:06:19.600 --> 02:06:22.960]   Next, starting with Stacey Higginbotham's pick
[02:06:22.960 --> 02:06:23.960]   of the Wii.
[02:06:23.960 --> 02:06:24.560]   Oh, it's start with me.
[02:06:24.560 --> 02:06:27.680]   Well, that's because you're all the way on the left.
[02:06:27.680 --> 02:06:30.440]   But I can start with somebody else.
[02:06:30.440 --> 02:06:31.560]   So she's going.
[02:06:31.560 --> 02:06:33.600]   She's taking-- Stacey's got a little ride.
[02:06:33.600 --> 02:06:38.600]   By the way, no fire escapes, no alarms, no fire extinguishers
[02:06:38.600 --> 02:06:40.240]   on that ride.
[02:06:40.240 --> 02:06:41.240]   That's Stacey just talking.
[02:06:41.240 --> 02:06:44.440]   Oh, I was going to say, I am within 100 feet
[02:06:44.440 --> 02:06:46.240]   from a fire extinguisher.
[02:06:46.240 --> 02:06:48.080]   No, I'm within 30 feet of fire extinguisher.
[02:06:48.080 --> 02:06:48.880]   OK, good.
[02:06:48.880 --> 02:06:50.880]   I have one in the house.
[02:06:50.880 --> 02:06:52.080]   Just in case I have to--
[02:06:52.080 --> 02:06:53.120]   Three.
[02:06:53.120 --> 02:06:55.360]   Good for you.
[02:06:55.360 --> 02:06:56.360]   We have a fire extinguisher.
[02:06:56.360 --> 02:06:57.280]   We're not going to watch me jazz.
[02:06:57.280 --> 02:06:58.160]   Do we have a different--
[02:06:58.160 --> 02:06:59.920]   That doesn't look like an Allen wrench.
[02:06:59.920 --> 02:07:00.440]   We don't.
[02:07:00.440 --> 02:07:02.000]   OK, just checking.
[02:07:02.000 --> 02:07:03.000]   She's planning a heart attack.
[02:07:03.000 --> 02:07:05.320]   I don't have a-- I don't have a--
[02:07:05.320 --> 02:07:06.520]   What are those things called?
[02:07:06.520 --> 02:07:07.640]   Box cutter.
[02:07:07.640 --> 02:07:09.520]   You need a box cutter?
[02:07:09.520 --> 02:07:10.520]   I need a box cutter.
[02:07:10.520 --> 02:07:12.720]   And what's your pick of the week this week?
[02:07:12.720 --> 02:07:19.600]   Do you have one?
[02:07:19.600 --> 02:07:21.160]   I could go if she's not ready.
[02:07:21.160 --> 02:07:22.160]   Go.
[02:07:22.160 --> 02:07:23.160]   Mine is pretty--
[02:07:23.160 --> 02:07:24.160]   What a rotary.
[02:07:24.160 --> 02:07:25.800]   Quick this time.
[02:07:25.800 --> 02:07:28.960]   There's a podcast out there called Relatively Normal.
[02:07:28.960 --> 02:07:31.480]   It's hosted by Mr. Mark--
[02:07:31.480 --> 02:07:33.600]   I can never say his last name.
[02:07:33.600 --> 02:07:37.680]   But he used to write for Clemson Sports Network.
[02:07:37.680 --> 02:07:39.680]   And he started this podcast because he's
[02:07:39.680 --> 02:07:43.240]   focusing on mental health and mental health awareness.
[02:07:43.240 --> 02:07:46.840]   And he's been able to have conversations
[02:07:46.840 --> 02:07:53.040]   with other doctors and other psychiatrists and psychologists.
[02:07:53.040 --> 02:07:57.240]   And this particular episode that I highlighted today deals more
[02:07:57.240 --> 02:08:02.800]   so with men and men of color regarding mental health
[02:08:02.800 --> 02:08:05.520]   and mental health awareness and all of the stigma around it
[02:08:05.520 --> 02:08:09.120]   and some of the things that we have heard as we grew up
[02:08:09.120 --> 02:08:13.200]   and things that some people just tend to believe.
[02:08:13.200 --> 02:08:18.440]   And mental health issues are just someone being weak or whatever.
[02:08:18.440 --> 02:08:22.640]   And the doctor on there goes through the ins and outs of it
[02:08:22.640 --> 02:08:27.000]   being just as important as someone that has diabetes.
[02:08:27.000 --> 02:08:27.960]   It's an illness.
[02:08:27.960 --> 02:08:29.400]   Just like any other illness.
[02:08:29.400 --> 02:08:29.900]   Yeah.
[02:08:29.900 --> 02:08:31.080]   It's just like any other illness.
[02:08:31.080 --> 02:08:35.560]   And they have a lot of interesting illustrations
[02:08:35.560 --> 02:08:35.920]   in that.
[02:08:35.920 --> 02:08:37.640]   I think it was worth the listen.
[02:08:37.640 --> 02:08:38.240]   No stigma.
[02:08:38.240 --> 02:08:41.320]   We don't have a stigma if you get a heart attack
[02:08:41.320 --> 02:08:42.320]   or you break your thumb.
[02:08:42.320 --> 02:08:44.880]   It's just just an illness.
[02:08:44.880 --> 02:08:45.320]   Yeah.
[02:08:45.320 --> 02:08:45.920]   But check it out.
[02:08:45.920 --> 02:08:49.640]   It even gets into suicide and does the taste
[02:08:49.640 --> 02:08:50.040]   that's there.
[02:08:50.040 --> 02:08:53.920]   And it's pretty interesting and pretty scary all at the same time.
[02:08:53.920 --> 02:08:56.640]   Relatively normal podcast by Mark Peissant
[02:08:56.640 --> 02:09:00.240]   and the latest episode, season three, episode 69
[02:09:00.240 --> 02:09:02.760]   is there a doctor in the house.
[02:09:02.760 --> 02:09:04.240]   That's good stuff.
[02:09:04.240 --> 02:09:07.400]   And then lastly, just watch my video on YouTube.
[02:09:07.400 --> 02:09:08.320]   You have a short film.
[02:09:08.320 --> 02:09:08.680]   You have a short film.
[02:09:08.680 --> 02:09:09.520]   You have another one.
[02:09:09.520 --> 02:09:11.200]   You have a short film.
[02:09:11.200 --> 02:09:13.760]   Worth the Queen Pruitt.
[02:09:13.760 --> 02:09:16.680]   But I hadn't had time to shoot anything else like that.
[02:09:16.680 --> 02:09:17.640]   But I want to do it again.
[02:09:17.640 --> 02:09:19.360]   That's because we keep you busy with the reviews.
[02:09:19.360 --> 02:09:22.560]   You're doing the Sony A1 next, right?
[02:09:22.560 --> 02:09:25.480]   Yeah, working on the A1 right now.
[02:09:25.480 --> 02:09:27.280]   And I believe we'll have that ready next week.
[02:09:27.280 --> 02:09:28.120]   Nice.
[02:09:28.120 --> 02:09:28.920]   Nice.
[02:09:28.920 --> 02:09:34.160]   I was recording a review of my new Mach-E, my month-in review
[02:09:34.160 --> 02:09:39.160]   of the Ford Electric Mach-E, which I mean short--
[02:09:39.160 --> 02:09:40.600]   short, long story short.
[02:09:40.600 --> 02:09:42.760]   I love it.
[02:09:42.760 --> 02:09:46.120]   But because of COVID, I couldn't have a camera crew or anything.
[02:09:46.120 --> 02:09:49.800]   So I shot the whole thing on a 360 degree camera
[02:09:49.800 --> 02:09:52.840]   and it's taking a long time to figure out
[02:09:52.840 --> 02:09:54.320]   what the hell they'll look at.
[02:09:54.320 --> 02:09:55.480]   He's got to do it.
[02:09:55.480 --> 02:09:56.400]   He's got to do it.
[02:09:56.400 --> 02:09:57.720]   Oh, he's got a lot of work.
[02:09:57.720 --> 02:10:00.040]   So this week, we're going to do--
[02:10:00.040 --> 02:10:02.760]   actually, it's a great bit from an Apple accessories,
[02:10:02.760 --> 02:10:08.800]   Apple iOS accessories with the hosts of iOS today,
[02:10:08.800 --> 02:10:11.720]   the wonderful Russ Murray-Archerd and Micah Sargent.
[02:10:11.720 --> 02:10:13.880]   So that will be actually coming up right after the show
[02:10:13.880 --> 02:10:14.400]   hands-on tech.
[02:10:14.400 --> 02:10:19.280]   And then next week, my review of the Ford Mach-E.
[02:10:19.280 --> 02:10:21.880]   And then maybe we'll get to your Sony A1 review the week
[02:10:21.880 --> 02:10:22.400]   afterwards.
[02:10:22.400 --> 02:10:25.680]   Got to kind of start and get a little bit behind on the reviews.
[02:10:25.680 --> 02:10:27.560]   Jeff, you have a number for us?
[02:10:27.560 --> 02:10:28.360]   Yeah, I do.
[02:10:28.360 --> 02:10:28.960]   Yeah, I do.
[02:10:28.960 --> 02:10:29.920]   No, it's going to slide it again.
[02:10:29.920 --> 02:10:31.320]   You prepare so many numbers.
[02:10:31.320 --> 02:10:31.760]   It's good.
[02:10:31.760 --> 02:10:32.240]   You're smart.
[02:10:32.240 --> 02:10:32.840]   Because I never know.
[02:10:32.840 --> 02:10:33.960]   I see I gave up a number.
[02:10:33.960 --> 02:10:34.960]   I know.
[02:10:34.960 --> 02:10:35.480]   Yeah.
[02:10:35.480 --> 02:10:38.760]   For the good of the show, I'm willing to give up anything.
[02:10:38.760 --> 02:10:45.800]   So WordPress is an amazing company and organization.
[02:10:45.800 --> 02:10:47.880]   And it's share of--
[02:10:47.880 --> 02:10:49.600]   I'm not sure what the methodology here is,
[02:10:49.600 --> 02:10:53.720]   but this came from Ben Evans' email newsletter.
[02:10:53.720 --> 02:10:58.640]   It's share of content management systems is now 41.1%.
[02:10:58.640 --> 02:11:00.000]   Yikes.
[02:11:00.000 --> 02:11:00.560]   Bigger than not.
[02:11:00.560 --> 02:11:02.400]   That's pretty dominant.
[02:11:02.400 --> 02:11:05.120]   Next down is Shopify at 3 and 1/2.
[02:11:05.120 --> 02:11:08.280]   Joomla, 2/1.
[02:11:08.280 --> 02:11:13.600]   And Squarespace and Wix are tied at 1.6%.
[02:11:13.600 --> 02:11:15.080]   Huge travel there.
[02:11:15.080 --> 02:11:17.000]   WordPress is dominant.
[02:11:17.000 --> 02:11:20.200]   Blogger, Bless It's Little Survive and Heart until Google
[02:11:20.200 --> 02:11:21.960]   realizes we have that still.
[02:11:21.960 --> 02:11:24.320]   Why did we kill that?
[02:11:24.320 --> 02:11:27.640]   Is hanging in there at 1%?
[02:11:27.640 --> 02:11:30.720]   And then it falls off from there.
[02:11:30.720 --> 02:11:36.240]   And I'm always impressed with Matt and with WordPress
[02:11:36.240 --> 02:11:40.160]   and the vision he had for an open source base and a company
[02:11:40.160 --> 02:11:41.760]   built on top and it worked.
[02:11:41.760 --> 02:11:43.240]   Yeah.
[02:11:43.240 --> 02:11:45.280]   They say 41% of all websites.
[02:11:45.280 --> 02:11:47.200]   I think that that's actually just as accurate
[02:11:47.200 --> 02:11:49.520]   because most websites are content management systems
[02:11:49.520 --> 02:11:50.080]   of some kind.
[02:11:50.080 --> 02:11:55.040]   So 41% of the entire web runs on WordPress.
[02:11:55.040 --> 02:11:58.920]   When we were doing the ads, it was about 30%.
[02:11:58.920 --> 02:12:02.080]   So I take credit for the final 11%.
[02:12:02.080 --> 02:12:02.800]   Yes, you should.
[02:12:02.800 --> 02:12:03.840]   Good job, Mr. LePorke.
[02:12:03.840 --> 02:12:04.680]   Good job.
[02:12:04.680 --> 02:12:08.480]   10 years before, in 2011, it was 13.1%.
[02:12:08.480 --> 02:12:08.840]   Yeah.
[02:12:08.840 --> 02:12:09.840]   It's been growing.
[02:12:09.840 --> 02:12:10.880]   None.
[02:12:10.880 --> 02:12:11.280]   Stop.
[02:12:11.280 --> 02:12:12.520]   It's impressive.
[02:12:12.520 --> 02:12:13.960]   Matt Mullen-Wig is amazing.
[02:12:13.960 --> 02:12:16.240]   Good job, WordPress.
[02:12:16.240 --> 02:12:17.600]   And finally--
[02:12:17.600 --> 02:12:18.480]   Are you ready, Stacey?
[02:12:18.480 --> 02:12:19.960]   Are you ready?
[02:12:19.960 --> 02:12:20.720]   If Stacey's--
[02:12:20.720 --> 02:12:22.160]   I did my homework now.
[02:12:22.160 --> 02:12:23.320]   He's a box cutter.
[02:12:23.320 --> 02:12:25.760]   I'm going to be upset.
[02:12:25.760 --> 02:12:26.280]   Whoa.
[02:12:26.280 --> 02:12:27.440]   My pick of the week.
[02:12:27.440 --> 02:12:28.400]   It's an Alan Rich.
[02:12:28.400 --> 02:12:29.400]   Awesome.
[02:12:29.400 --> 02:12:30.400]   Ooh.
[02:12:30.400 --> 02:12:30.400]   It is--
[02:12:30.400 --> 02:12:31.400]   A power supply.
[02:12:31.400 --> 02:12:33.640]   --a Pbron Outdoor Outlet.
[02:12:33.640 --> 02:12:35.640]   This is IPV-- or IPV.
[02:12:35.640 --> 02:12:36.640]   Sorry.
[02:12:36.640 --> 02:12:39.400]   This is IP65 rated.
[02:12:39.400 --> 02:12:41.360]   But if you want the P outside, it'd
[02:12:41.360 --> 02:12:43.880]   be a very handy thing to have.
[02:12:43.880 --> 02:12:46.360]   I was going to say it is not IPV6.
[02:12:46.360 --> 02:12:48.520]   Is this a cassette-a?
[02:12:48.520 --> 02:12:50.080]   This is a cassette-a.
[02:12:50.080 --> 02:12:52.520]   So what you need to know about this,
[02:12:52.520 --> 02:12:54.440]   this is a connected outdoor outlet.
[02:12:54.440 --> 02:12:57.560]   It is expensive AF, y'all.
[02:12:57.560 --> 02:12:59.240]   It is $79.95.
[02:12:59.240 --> 02:13:00.240]   Wow.
[02:13:00.240 --> 02:13:03.000]   And you might be like, wow, I can buy two outlets that are home
[02:13:03.000 --> 02:13:05.320]   kit certified for like $50.
[02:13:05.320 --> 02:13:06.920]   And you can.
[02:13:06.920 --> 02:13:09.240]   And I recommend you do that if that's your jam.
[02:13:09.240 --> 02:13:12.160]   But this is good because it is--
[02:13:12.160 --> 02:13:12.560]   If that's true.
[02:13:12.560 --> 02:13:14.320]   --Lutron.
[02:13:14.320 --> 02:13:15.120]   It's also a good--
[02:13:15.120 --> 02:13:15.640]   --it is.
[02:13:15.640 --> 02:13:17.680]   --ci, which I like.
[02:13:17.680 --> 02:13:22.160]   But this is temperature rated to below 20 degrees
[02:13:22.160 --> 02:13:24.960]   up to 50 degrees Celsius.
[02:13:24.960 --> 02:13:27.680]   So this is for your Christmas lights.
[02:13:27.680 --> 02:13:28.200]   You're--
[02:13:28.200 --> 02:13:29.600]   This is for like Minnesota.
[02:13:29.600 --> 02:13:31.240]   Minnesota.
[02:13:31.240 --> 02:13:34.720]   There's Minnesota Christmas lights.
[02:13:34.720 --> 02:13:40.400]   And I think most of the outdoor outlets are not IP65 rated.
[02:13:40.400 --> 02:13:45.320]   They're usually IP64 rated, which is a little less water
[02:13:45.320 --> 02:13:48.000]   resistant.
[02:13:48.000 --> 02:13:50.560]   This guy, I have not tried this one,
[02:13:50.560 --> 02:13:51.880]   but I have one outside.
[02:13:51.880 --> 02:13:54.240]   I went out and bought one because I live in Seattle.
[02:13:54.240 --> 02:13:56.120]   And there's tons of rain.
[02:13:56.120 --> 02:13:59.480]   And my string lights, it would be more convenient to have them
[02:13:59.480 --> 02:14:02.360]   not under an eave for that lit.
[02:14:02.360 --> 02:14:06.720]   So they've been outside for four weeks in Seattle
[02:14:06.720 --> 02:14:09.400]   weather, granted the last week, has been very, very nice
[02:14:09.400 --> 02:14:09.960]   sale.
[02:14:09.960 --> 02:14:12.080]   But prior to that, super rainy.
[02:14:12.080 --> 02:14:16.720]   And so far, it hasn't shorted or done anything terrible.
[02:14:16.720 --> 02:14:20.080]   Because it's cassette, it links back
[02:14:20.080 --> 02:14:24.480]   to any cassette abridge.
[02:14:24.480 --> 02:14:26.280]   If you have the cassette abridge in your home,
[02:14:26.280 --> 02:14:28.240]   it's so easy to set up.
[02:14:28.240 --> 02:14:29.960]   And this is why I love all Lutron products.
[02:14:29.960 --> 02:14:31.600]   One, Lutron will back you up.
[02:14:31.600 --> 02:14:33.600]   And they have the greatest technical support ever.
[02:14:33.600 --> 02:14:36.320]   But B, to set it up, you literally plug this in.
[02:14:36.320 --> 02:14:39.200]   You go to the app and you're like, I want to add a device.
[02:14:39.200 --> 02:14:41.080]   It's like, what device do you want?
[02:14:41.080 --> 02:14:42.280]   And you pick this device.
[02:14:42.280 --> 02:14:44.440]   And then you hold this button right here down.
[02:14:44.440 --> 02:14:47.280]   In 10 seconds later, it's linked to the app and you're done.
[02:14:47.280 --> 02:14:48.680]   It's so nice.
[02:14:48.680 --> 02:14:49.560]   Is it pictures?
[02:14:49.560 --> 02:14:51.320]   Is that how they're doing it?
[02:14:51.320 --> 02:14:53.760]   No, it is-- oh, it is radio.
[02:14:53.760 --> 02:14:54.640]   It's not radio around.
[02:14:54.640 --> 02:14:55.360]   I'm so sorry.
[02:14:55.360 --> 02:14:58.320]   It is their cassette a clear connect is the protocol.
[02:14:58.320 --> 02:14:59.400]   And that's that really works.
[02:14:59.400 --> 02:15:00.840]   So this is 1,000 feet.
[02:15:00.840 --> 02:15:04.040]   Yeah, it's a great protocol.
[02:15:04.040 --> 02:15:07.560]   So if I were going to get a lighting automation,
[02:15:07.560 --> 02:15:10.960]   you'd think Lutron would be the way to go.
[02:15:10.960 --> 02:15:13.600]   If you were putting smart lighting in your home,
[02:15:13.600 --> 02:15:16.440]   Lutron, it's bar none my favorite.
[02:15:16.440 --> 02:15:16.960]   King of the hill.
[02:15:16.960 --> 02:15:19.360]   Hands down the best.
[02:15:19.360 --> 02:15:21.000]   And it works with the Google Assistant.
[02:15:21.000 --> 02:15:21.840]   It works with HomeKit.
[02:15:21.840 --> 02:15:22.760]   It works with Amazon Echo.
[02:15:22.760 --> 02:15:24.280]   You buy a hub.
[02:15:24.280 --> 02:15:27.000]   Yeah, you have to have a hub to connect it to Wi-Fi,
[02:15:27.000 --> 02:15:31.040]   because remember, it's the clear connect.
[02:15:31.040 --> 02:15:32.160]   Sorry.
[02:15:32.160 --> 02:15:33.280]   They have a fancier version.
[02:15:33.280 --> 02:15:35.080]   It's important to know that Lutron has cassette,
[02:15:35.080 --> 02:15:36.800]   which is their DIY smart home stuff.
[02:15:36.800 --> 02:15:39.160]   And then they have fancy Lutron that
[02:15:39.160 --> 02:15:41.720]   is like a control four vendors or someone will say.
[02:15:41.720 --> 02:15:42.600]   Yeah, yeah, yeah.
[02:15:42.600 --> 02:15:44.360]   That's expensive too.
[02:15:44.360 --> 02:15:45.400]   That stuff.
[02:15:45.400 --> 02:15:46.560]   Yeah, that's radio rods.
[02:15:46.560 --> 02:15:52.400]   I have outdoor Lutron lights that are very expensive.
[02:15:52.400 --> 02:15:54.840]   Yeah.
[02:15:54.840 --> 02:15:59.280]   So yeah, and this is, again, this is expensive,
[02:15:59.280 --> 02:16:01.320]   but-- and I'll tell you in a year--
[02:16:01.320 --> 02:16:03.600]   but it's a much-- you could feel it.
[02:16:03.600 --> 02:16:06.920]   It's a much higher quality build.
[02:16:06.920 --> 02:16:10.920]   It looks powerful.
[02:16:10.920 --> 02:16:12.120]   I don't know if it's powerful.
[02:16:12.120 --> 02:16:13.400]   It looks powerful.
[02:16:13.400 --> 02:16:18.440]   Looks like the kind of light plug Darth Vader might have.
[02:16:18.440 --> 02:16:18.920]   Yeah.
[02:16:18.920 --> 02:16:21.840]   It's very glossy, black, and powerful looking.
[02:16:21.840 --> 02:16:22.800]   Well, I haven't.
[02:16:22.800 --> 02:16:24.320]   There's plastic on this.
[02:16:24.320 --> 02:16:26.240]   Oh, no, it is glossy.
[02:16:26.240 --> 02:16:27.960]   I never take the plastic off of anything.
[02:16:27.960 --> 02:16:29.800]   Anyway.
[02:16:29.800 --> 02:16:31.760]   It's like to watch it slowly drop.
[02:16:31.760 --> 02:16:32.240]   It feels good to do, though.
[02:16:32.240 --> 02:16:33.960]   Yeah, I love to take the plastic off.
[02:16:33.960 --> 02:16:35.560]   I like taking the plastic off.
[02:16:35.560 --> 02:16:37.320]   I don't-- I wait to.
[02:16:37.320 --> 02:16:38.320]   I hate gadgets.
[02:16:38.320 --> 02:16:40.040]   That way this all just comes off.
[02:16:40.040 --> 02:16:43.080]   Yeah, I don't like the mess with them.
[02:16:43.080 --> 02:16:44.120]   Yeah, it gets yucky.
[02:16:44.120 --> 02:16:45.360]   I just don't want to mess with them.
[02:16:45.360 --> 02:16:46.360]   Plus, for a long time.
[02:16:46.360 --> 02:16:47.520]   I'm like, you set it up again.
[02:16:47.520 --> 02:16:50.080]   I had a-- I think it was the essential phone.
[02:16:50.080 --> 02:16:52.160]   I say, boy, the pictures are terrible on this.
[02:16:52.160 --> 02:16:54.400]   I finally realized, like, a month later,
[02:16:54.400 --> 02:16:57.720]   I'm never taking the plastic off the lens.
[02:16:57.720 --> 02:17:00.360]   Suddenly, they cleared up.
[02:17:00.360 --> 02:17:03.800]   Trust me, that was just a prelude to the problems
[02:17:03.800 --> 02:17:04.400]   of that phone.
[02:17:04.400 --> 02:17:07.560]   That thing never was very good, but at least, you know,
[02:17:07.560 --> 02:17:10.200]   once you took the plastic off, it looked a little bit better.
[02:17:10.200 --> 02:17:12.640]   So now I make a religious fetish of it
[02:17:12.640 --> 02:17:15.760]   to go through and make sure all plastic is removed
[02:17:15.760 --> 02:17:17.920]   at time of use.
[02:17:17.920 --> 02:17:23.200]   And it is-- it's very satisfying to peel the plastic off.
[02:17:23.200 --> 02:17:26.600]   Although I found a plastic covering on my PC, I guess,
[02:17:26.600 --> 02:17:29.800]   when stuff's made in China, it's just their habit
[02:17:29.800 --> 02:17:34.040]   to cover every glossy surface with a film of plastic.
[02:17:34.040 --> 02:17:36.200]   So I just-- I didn't even realize,
[02:17:36.200 --> 02:17:39.200]   month after I was getting the PC, there's a very thin film
[02:17:39.200 --> 02:17:43.800]   of plastic on the front of that.
[02:17:43.800 --> 02:17:45.160]   Thank you, Stacy.
[02:17:45.160 --> 02:17:48.000]   I'm glad you found the box cutter.
[02:17:48.000 --> 02:17:49.400]   Yes, sorry, y'all.
[02:17:49.400 --> 02:17:51.280]   Oh, I'm just teasing you.
[02:17:51.280 --> 02:17:52.240]   No problem.
[02:17:52.240 --> 02:17:53.680]   Why do we always start with Stacy?
[02:17:53.680 --> 02:17:55.200]   We should start with Ann once in a while.
[02:17:55.200 --> 02:17:56.320]   It just seems--
[02:17:56.320 --> 02:17:59.160]   No, I'll start with me.
[02:17:59.160 --> 02:18:02.240]   It's like, I don't do my homework until Stacy's doing her home.
[02:18:02.240 --> 02:18:05.880]   Right, I'm waiting on what he wants to be first.
[02:18:05.880 --> 02:18:07.400]   Nobody wants to be first.
[02:18:07.400 --> 02:18:09.560]   I got a pick of the week, Club Twit.
[02:18:09.560 --> 02:18:12.040]   Go to twit.tv/clubtwit.
[02:18:12.040 --> 02:18:13.480]   We're having so much fun in there.
[02:18:13.480 --> 02:18:15.840]   Reminds me so much of the early days of Twit
[02:18:15.840 --> 02:18:19.040]   where that great community is hanging out and enjoying
[02:18:19.040 --> 02:18:19.560]   each other.
[02:18:19.560 --> 02:18:21.800]   And, man, I'm loving it.
[02:18:21.800 --> 02:18:24.160]   Aunt Pruitt is at Hands-On Photography.
[02:18:24.160 --> 02:18:27.280]   Another good reason to join Club Twit
[02:18:27.280 --> 02:18:30.000]   is because it supports shows like Hands-On Photography.
[02:18:30.000 --> 02:18:32.880]   And we want to do more photography shows.
[02:18:32.880 --> 02:18:34.800]   But for some reason, I don't know why they're really hard
[02:18:34.800 --> 02:18:36.160]   to sell ads for.
[02:18:36.160 --> 02:18:40.440]   I thought, Canon, Sony, Nikon, that they'd all just come running.
[02:18:40.440 --> 02:18:41.960]   Epson, no.
[02:18:41.960 --> 02:18:43.120]   No, they don't.
[02:18:43.120 --> 02:18:44.160]   So I don't know why--
[02:18:44.160 --> 02:18:45.520]   And I know they're PR.
[02:18:45.520 --> 02:18:46.040]   I know.
[02:18:46.040 --> 02:18:46.560]   I know.
[02:18:46.560 --> 02:18:48.040]   We thought--
[02:18:48.040 --> 02:18:51.040]   not just for you when we did the Tans--
[02:18:51.040 --> 02:18:53.240]   this week in photo stuff.
[02:18:53.240 --> 02:18:54.400]   Oh, yeah, tour it.
[02:18:54.400 --> 02:18:56.240]   With what's her name?
[02:18:56.240 --> 02:18:56.960]   With the Catherine?
[02:18:56.960 --> 02:18:57.960]   It was--
[02:18:57.960 --> 02:19:00.520]   Catherine Hall, we thought, oh, this is going to be easy.
[02:19:00.520 --> 02:19:01.400]   She thought it was easy.
[02:19:01.400 --> 02:19:03.000]   Oh, I know everybody in the--
[02:19:03.000 --> 02:19:04.360]   no, nobody wanted to buy ads.
[02:19:04.360 --> 02:19:08.000]   So if you want photography stuff, join Club Twit.
[02:19:08.000 --> 02:19:10.280]   Because that's how you support it.
[02:19:10.280 --> 02:19:13.040]   That's how you show that there's an interest in it.
[02:19:13.040 --> 02:19:14.360]   Maybe advertisers don't care.
[02:19:14.360 --> 02:19:16.160]   But we know the audience cares.
[02:19:16.160 --> 02:19:21.080]   All you have to do is go to twit.tv/clubtwit.
[02:19:21.080 --> 02:19:24.760]   And focus on photography is a twit.tv/--
[02:19:24.760 --> 02:19:26.440]   I'm sorry, Hands-On.
[02:19:26.440 --> 02:19:27.560]   Focus there is an example.
[02:19:27.560 --> 02:19:28.920]   That was a show we were doing.
[02:19:28.920 --> 02:19:30.920]   We stopped doing hands-on photography.
[02:19:30.920 --> 02:19:32.160]   Save Hands-On Photography.
[02:19:32.160 --> 02:19:34.400]   Now that it's in trouble, but just
[02:19:34.400 --> 02:19:35.600]   let me use it for the moment.
[02:19:35.600 --> 02:19:37.320]   Save Hands-On Photography.
[02:19:37.320 --> 02:19:39.960]   Twit.tv/hop.
[02:19:39.960 --> 02:19:41.960]   Mr. Jeff Jarvis, ladies and gentlemen,
[02:19:41.960 --> 02:19:46.160]   I give you the director of the Townite Center
[02:19:46.160 --> 02:19:48.640]   for Entrepreneurial Journalism at the Craig Newmark Graduate
[02:19:48.640 --> 02:19:52.960]   School at the City University of New York.
[02:19:52.960 --> 02:19:55.440]   Google may pay him, but we love him.
[02:19:55.440 --> 02:19:57.440]   Google does not pay me.
[02:19:57.440 --> 02:19:59.640]   I just want to give you the chance to say that very clearly.
[02:19:59.640 --> 02:20:00.800]   Google does not.
[02:20:00.800 --> 02:20:03.720]   There was a moment today when somebody slandered me.
[02:20:03.720 --> 02:20:06.040]   Who is this clown from the New York Times?
[02:20:06.040 --> 02:20:08.640]   Is he somebody I should know?
[02:20:08.640 --> 02:20:09.320]   Is he slandered?
[02:20:09.320 --> 02:20:10.800]   Or you are liable here.
[02:20:10.800 --> 02:20:11.240]   Was it--
[02:20:11.240 --> 02:20:12.920]   He labeled him.
[02:20:12.920 --> 02:20:13.440]   But the--
[02:20:13.440 --> 02:20:15.000]   Or liable them, which is-- which is--
[02:20:15.000 --> 02:20:16.440]   Libel's in print, slanders in speech.
[02:20:16.440 --> 02:20:17.600]   I don't know what I'm pissed.
[02:20:17.600 --> 02:20:18.120]   It was in speech.
[02:20:18.120 --> 02:20:19.120]   Yes, Nander is spoken.
[02:20:19.120 --> 02:20:20.760]   I was like, was it on TV or was it on the--
[02:20:20.760 --> 02:20:21.880]   It was on TV.
[02:20:21.880 --> 02:20:23.800]   And he said-- he didn't just slandered him.
[02:20:23.800 --> 02:20:25.240]   He slandered us.
[02:20:25.240 --> 02:20:26.480]   Because he said, oh, if tech--
[02:20:26.480 --> 02:20:28.800]   if you listen to the tech journalists talking
[02:20:28.800 --> 02:20:33.760]   about technology, they're all paid by Google and Facebook.
[02:20:33.760 --> 02:20:34.320]   Yep.
[02:20:34.320 --> 02:20:35.680]   What the hell?
[02:20:35.680 --> 02:20:36.520]   Yeah, I saw it.
[02:20:36.520 --> 02:20:37.160]   It was like--
[02:20:37.160 --> 02:20:37.960]   Who was this?
[02:20:37.960 --> 02:20:40.280]   And then they said, give us names.
[02:20:40.280 --> 02:20:41.960]   And the guy said, well, Jeff Jarvis.
[02:20:41.960 --> 02:20:43.800]   I was like, I almost hit the roof, Jeff.
[02:20:43.800 --> 02:20:45.080]   I was furious.
[02:20:45.080 --> 02:20:46.960]   And I know you didn't want to bring it up.
[02:20:46.960 --> 02:20:47.560]   But it's cool.
[02:20:47.560 --> 02:20:50.360]   Yeah, because the person who shared it
[02:20:50.360 --> 02:20:51.800]   is a horrible troll.
[02:20:51.800 --> 02:20:53.680]   Well, yeah, the person shared whatever.
[02:20:53.680 --> 02:20:55.800]   But this was on--
[02:20:55.800 --> 02:20:56.440]   Some like--
[02:20:56.440 --> 02:20:57.680]   Well, I've done it enough.
[02:20:57.680 --> 02:21:00.480]   It was on a panel on an organization that
[02:21:00.480 --> 02:21:03.320]   is the anti-tech lobbying organization that--
[02:21:03.320 --> 02:21:05.840]   irony doesn't reveal its own fund.
[02:21:05.840 --> 02:21:06.760]   Oh.
[02:21:06.760 --> 02:21:07.160]   What a surprise.
[02:21:07.160 --> 02:21:10.360]   By the way, I get not one penny of my salary
[02:21:10.360 --> 02:21:12.480]   from Google or Facebook or any technology company.
[02:21:12.480 --> 02:21:12.800]   No.
[02:21:12.800 --> 02:21:16.280]   But Ben Smith's salary at the New York Times
[02:21:16.280 --> 02:21:18.080]   is well subsidized by Google and Facebook.
[02:21:18.080 --> 02:21:20.080]   Because they get tons of money from Google and Facebook.
[02:21:20.080 --> 02:21:20.920]   Oh, interesting.
[02:21:20.920 --> 02:21:21.320]   Isn't that--
[02:21:21.320 --> 02:21:22.240]   Facts or facts?
[02:21:22.240 --> 02:21:23.160]   Isn't that interesting?
[02:21:23.160 --> 02:21:25.440]   He's the media columnist at the Times.
[02:21:25.440 --> 02:21:28.200]   And clearly, a first class twit.
[02:21:28.200 --> 02:21:30.040]   And he didn't just-- he named you.
[02:21:30.040 --> 02:21:32.240]   But he basically said all tech journalists.
[02:21:32.240 --> 02:21:34.560]   If you hear them defending big tech,
[02:21:34.560 --> 02:21:37.120]   it's because they're paid by Google and Facebook.
[02:21:37.120 --> 02:21:37.680]   And that was--
[02:21:37.680 --> 02:21:38.840]   I don't know a single soul.
[02:21:38.840 --> 02:21:39.600]   Furiating.
[02:21:39.600 --> 02:21:40.560]   Who's out there in that world?
[02:21:40.560 --> 02:21:42.840]   Such BS.
[02:21:42.840 --> 02:21:45.240]   We bent over backwards.
[02:21:45.240 --> 02:21:47.240]   Not to take money from these companies.
[02:21:47.240 --> 02:21:52.000]   I don't own stocks in any company that we cover.
[02:21:52.000 --> 02:21:56.520]   We get yelled at for giving Google a hard time.
[02:21:56.520 --> 02:21:57.800]   So annoying.
[02:21:57.800 --> 02:21:59.360]   That's how you know you're doing it right.
[02:21:59.360 --> 02:22:02.160]   It's when they yell at you for giving you a better.
[02:22:02.160 --> 02:22:03.640]   Guy's a bozo.
[02:22:03.640 --> 02:22:05.360]   So he worked for the New York Observer.
[02:22:05.360 --> 02:22:07.000]   That should tell you something.
[02:22:07.000 --> 02:22:07.880]   No, he didn't.
[02:22:07.880 --> 02:22:09.320]   Yeah.
[02:22:09.320 --> 02:22:11.680]   He covered politics for Politico, the New York Daily News,
[02:22:11.680 --> 02:22:13.840]   the New York Observer, and the New York Sun.
[02:22:13.840 --> 02:22:14.240]   Oh.
[02:22:14.240 --> 02:22:15.240]   Oh.
[02:22:15.240 --> 02:22:15.740]   Oh.
[02:22:15.740 --> 02:22:16.240]   Oh, Mr.--
[02:22:16.240 --> 02:22:16.960]   He was the other for BuzzFeed.
[02:22:16.960 --> 02:22:21.040]   Yeah, Mr. Ed's-- oh, he was a founding editor of BuzzFeed.
[02:22:21.040 --> 02:22:24.640]   Well, there's a journalistic enterprise.
[02:22:24.640 --> 02:22:28.480]   Hey, BuzzFeed News is actually a journalistic enterprise.
[02:22:28.480 --> 02:22:29.440]   Yeah, it is.
[02:22:29.440 --> 02:22:29.960]   Yeah.
[02:22:29.960 --> 02:22:31.120]   It's not slander and tire publications.
[02:22:31.120 --> 02:22:32.640]   It's hard to hear what he's not fair to me.
[02:22:32.640 --> 02:22:35.600]   A lot of BuzzFeed is Linkbait, though, right?
[02:22:35.600 --> 02:22:36.880]   The name kind of says all.
[02:22:36.880 --> 02:22:39.680]   There's two-- yes, BuzzFeed is one part.
[02:22:39.680 --> 02:22:40.200]   Not the news.
[02:22:40.200 --> 02:22:41.200]   The news site is actually big.
[02:22:41.200 --> 02:22:42.280]   OK.
[02:22:42.280 --> 02:22:42.880]   Well, I don't care.
[02:22:42.880 --> 02:22:43.760]   I don't like him anyway.
[02:22:43.760 --> 02:22:49.320]   He's a jerky jerk.
[02:22:49.320 --> 02:22:50.360]   And that's that.
[02:22:50.360 --> 02:22:54.360]   And I couldn't let my good friend Jeff Jarvis go on defend.
[02:22:54.360 --> 02:22:57.240]   Well, it was actually gratifying today, because I thought, oh, no, here we go.
[02:22:57.240 --> 02:22:58.640]   It's going to be a troll day.
[02:22:58.640 --> 02:23:00.360]   And I got tons of support.
[02:23:00.360 --> 02:23:03.880]   People said that was over the line, that was wrong, it's not true.
[02:23:03.880 --> 02:23:05.280]   It was ridiculous.
[02:23:05.280 --> 02:23:08.080]   But he slanted every tech journalist.
[02:23:08.080 --> 02:23:08.960]   That's the thing.
[02:23:08.960 --> 02:23:10.840]   I mean, the guy's a clown.
[02:23:10.840 --> 02:23:16.000]   If he thinks people in our business are paid by Google and Facebook,
[02:23:16.000 --> 02:23:19.600]   that's like a fourth grader's impression of how things work.
[02:23:19.600 --> 02:23:24.040]   Now I can't get Google to do what I want to with the workplace accounts.
[02:23:24.040 --> 02:23:26.880]   That's freaking nuts.
[02:23:26.880 --> 02:23:31.960]   The truth is, I pay Google a fortune between all my little services.
[02:23:31.960 --> 02:23:32.600]   I pay them.
[02:23:32.600 --> 02:23:34.520]   They don't pay me.
[02:23:34.520 --> 02:23:37.400]   And Google doesn't go around offering--
[02:23:37.400 --> 02:23:42.120]   are you asserting that these companies are going on bribing journalists?
[02:23:42.120 --> 02:23:43.520]   That's horrific.
[02:23:43.520 --> 02:23:46.720]   Google and Facebook are going around giving money to journalists
[02:23:46.720 --> 02:23:47.840]   for favorable coverage.
[02:23:47.840 --> 02:23:51.920]   If they're doing that, if any tech company is doing that, that's horrific.
[02:23:51.920 --> 02:23:56.600]   I'd like to see the evidence, Mr. Ben Smith, you moron.
[02:23:56.600 --> 02:23:58.480]   And the money would be nice.
[02:23:58.480 --> 02:23:59.800]   [LAUGHTER]
[02:23:59.800 --> 02:24:00.800]   Yeah.
[02:24:00.800 --> 02:24:01.320]   Yeah.
[02:24:01.320 --> 02:24:03.200]   If we missed that email, let us know.
[02:24:03.200 --> 02:24:05.160]   We'll let more of our spam and CP.
[02:24:05.160 --> 02:24:06.480]   Where is that--
[02:24:06.480 --> 02:24:07.000]   Yeah.
[02:24:07.000 --> 02:24:07.800]   Where is that--
[02:24:07.800 --> 02:24:09.920]   That bag of money that they're bringing around.
[02:24:09.920 --> 02:24:11.160]   What a moron to say that.
[02:24:11.160 --> 02:24:12.440]   That's appalling.
[02:24:12.440 --> 02:24:12.920]   It really was.
[02:24:12.920 --> 02:24:13.960]   Just appalling.
[02:24:13.960 --> 02:24:15.960]   And he's a media columnist at the New York Times.
[02:24:15.960 --> 02:24:17.520]   This is what I mean when I say the Times.
[02:24:17.520 --> 02:24:22.840]   It's occasionally more and more lately problematic.
[02:24:22.840 --> 02:24:26.920]   So maybe, yeah, maybe I should really stick with The Washington Post.
[02:24:26.920 --> 02:24:32.400]   Well, and it indicates the agenda of news organizations.
[02:24:32.400 --> 02:24:37.920]   Yeah, they hate big tech because they see big tech as the enterprise
[02:24:37.920 --> 02:24:39.600]   that put them out of business.
[02:24:39.600 --> 02:24:41.840]   And that's the next book proposal I'm working on right now.
[02:24:41.840 --> 02:24:42.800]   Oh, good.
[02:24:42.800 --> 02:24:43.800]   All right.
[02:24:43.800 --> 02:24:44.800]   Thank you, Jeff.
[02:24:44.800 --> 02:24:45.680]   Thank you, Stacey.
[02:24:45.680 --> 02:24:48.120]   Stacey's is Stacey on IOT.com.
[02:24:48.120 --> 02:24:51.560]   Our newsletter's free because Google paid for it.
[02:24:51.560 --> 02:24:52.480]   What is this?
[02:24:52.480 --> 02:24:54.280]   I/O-- I/O-ZAR.
[02:24:54.280 --> 02:24:55.280]   Is that your new title?
[02:24:55.280 --> 02:24:57.680]   You're the I/O-ZAR.
[02:24:57.680 --> 02:24:58.240]   What?
[02:24:58.240 --> 02:24:59.400]   The I/O-T-ZAR.
[02:24:59.400 --> 02:25:00.920]   I/O-T-ZAR.
[02:25:00.920 --> 02:25:02.440]   Somebody's changed your lower third.
[02:25:02.440 --> 02:25:04.080]   John, was it changed your lower third?
[02:25:04.080 --> 02:25:04.640]   Lower third?
[02:25:04.640 --> 02:25:05.480]   They're I/O-T-ZAR.
[02:25:05.480 --> 02:25:06.720]   Oh, oh, I'm--
[02:25:06.720 --> 02:25:07.200]   I'm--
[02:25:07.200 --> 02:25:07.720]   Oh.
[02:25:07.720 --> 02:25:08.520]   The I/O-T.
[02:25:08.520 --> 02:25:09.720]   It's spelled with the T-S-A-R.
[02:25:09.720 --> 02:25:10.240]   Yeah.
[02:25:10.240 --> 02:25:11.160]   Yeah.
[02:25:11.160 --> 02:25:13.840]   Well, Stacey on IOT.com's a place to go.
[02:25:13.840 --> 02:25:18.160]   There's also a great podcast she does with Kevin Tofel.
[02:25:18.160 --> 02:25:19.920]   He of the aboutchromebooks.com.
[02:25:19.920 --> 02:25:20.920]   Kevin!
[02:25:20.920 --> 02:25:21.920]   And Kevin.
[02:25:21.920 --> 02:25:24.320]   And it's called the IOT podcast.
[02:25:24.320 --> 02:25:25.720]   Thank you, everybody, for joining us.
[02:25:25.720 --> 02:25:29.080]   We do this week in Google of a Wednesday afternoon,
[02:25:29.080 --> 02:25:32.240]   about 2 o'clock Pacific, 5 p.m. Eastern.
[02:25:32.240 --> 02:25:35.520]   That's 2,100 UTC.
[02:25:35.520 --> 02:25:38.160]   Or whenever Google sends us a check, we'll do it then.
[02:25:38.160 --> 02:25:41.720]   [LAUGHTER]
[02:25:41.720 --> 02:25:45.160]   So crazy.
[02:25:45.160 --> 02:25:47.440]   If you want to watch us do it live,
[02:25:47.440 --> 02:25:50.360]   ads included, go to twit.tv/live.
[02:25:50.360 --> 02:25:52.440]   A couple of people said, hey, if I join the club,
[02:25:52.440 --> 02:25:54.920]   will there be no ads in the live feed?
[02:25:54.920 --> 02:25:56.440]   No.
[02:25:56.440 --> 02:25:57.880]   Because I got it.
[02:25:57.880 --> 02:25:58.920]   Well, actually, you know what?
[02:25:58.920 --> 02:26:00.800]   Today, just for you.
[02:26:00.800 --> 02:26:02.640]   No ads.
[02:26:02.640 --> 02:26:03.280]   Just for--
[02:26:03.280 --> 02:26:04.600]   I love that we can hear Jim--
[02:26:04.600 --> 02:26:06.400]   That was a real problem, actually.
[02:26:06.400 --> 02:26:07.080]   Yeah.
[02:26:07.080 --> 02:26:07.580]   Yeah.
[02:26:07.580 --> 02:26:08.080]   No ads.
[02:26:08.080 --> 02:26:08.580]   No ads.
[02:26:08.580 --> 02:26:09.080]   That was--
[02:26:09.080 --> 02:26:11.600]   You just go to sleep for two minutes.
[02:26:11.600 --> 02:26:12.280]   And then come back.
[02:26:12.280 --> 02:26:14.200]   I could play some lounge music.
[02:26:14.200 --> 02:26:15.320]   Da-da-da-da.
[02:26:15.320 --> 02:26:20.000]   I mean, I had to go in the middle of the Facebook convo.
[02:26:20.000 --> 02:26:21.880]   I had to leave to go get my waffles,
[02:26:21.880 --> 02:26:23.720]   because there wasn't an ad.
[02:26:23.720 --> 02:26:28.040]   And you didn't share waffles with the rest of the crew?
[02:26:28.040 --> 02:26:30.400]   I told everybody in Club Twitch that I was--
[02:26:30.400 --> 02:26:30.920]   She told Discord.
[02:26:30.920 --> 02:26:32.120]   --in this waffle time.
[02:26:32.120 --> 02:26:34.360]   [LAUGHTER]
[02:26:34.360 --> 02:26:36.400]   I saw her say it in Discord.
[02:26:36.400 --> 02:26:38.720]   What do you put on your waffles?
[02:26:38.720 --> 02:26:40.960]   I just see there's sweet enough as it is.
[02:26:40.960 --> 02:26:44.960]   And can I just do a shout out to John, who never--
[02:26:44.960 --> 02:26:46.160]   when I'm chomping on the waffles--
[02:26:46.160 --> 02:26:46.680]   Oh, we know not to--
[02:26:46.680 --> 02:26:48.120]   --is so gracious and never to show me.
[02:26:48.120 --> 02:26:50.360]   Never cut to Stacy when she's eating.
[02:26:50.360 --> 02:26:51.760]   No, we know that much.
[02:26:51.760 --> 02:26:53.000]   We're going to have Google News.
[02:26:53.000 --> 02:26:53.600]   I'm using my own--
[02:26:53.600 --> 02:26:54.600]   I'm like, ah!
[02:26:54.600 --> 02:26:57.320]   [LAUGHTER]
[02:26:57.320 --> 02:26:58.960]   Little secret to Aunt Wally's drinking.
[02:26:58.960 --> 02:27:00.600]   Little secret every once in a while.
[02:27:00.600 --> 02:27:02.360]   I drink a lot of water in the middle of the show.
[02:27:02.360 --> 02:27:03.760]   I got to go.
[02:27:03.760 --> 02:27:04.880]   I just leave.
[02:27:04.880 --> 02:27:06.360]   You guys talk.
[02:27:06.360 --> 02:27:07.360]   He never cuts the--
[02:27:07.360 --> 02:27:10.640]   if you ever see the wide shot, you'll never know.
[02:27:10.640 --> 02:27:11.880]   I'm gone.
[02:27:11.880 --> 02:27:12.480]   That's true.
[02:27:12.480 --> 02:27:13.680]   Then when I come, I sit back down.
[02:27:13.680 --> 02:27:14.680]   I say something.
[02:27:14.680 --> 02:27:15.920]   It's like I never left.
[02:27:15.920 --> 02:27:17.600]   It's pretty much the only time you get a word
[02:27:17.600 --> 02:27:19.000]   and edge-wise, actually.
[02:27:19.000 --> 02:27:20.880]   [LAUGHTER]
[02:27:20.880 --> 02:27:23.240]   On to my versions of the show available at the website,
[02:27:23.240 --> 02:27:25.320]   twit.tv/twig.
[02:27:25.320 --> 02:27:26.600]   While you're there on the page, you'll
[02:27:26.600 --> 02:27:29.000]   see a link to our YouTube channel--
[02:27:29.000 --> 02:27:32.560]   so everything we do is on the YouTube channel there.
[02:27:32.560 --> 02:27:35.080]   You can also see links to various podcast clients
[02:27:35.080 --> 02:27:37.720]   where you can subscribe to the show automatically.
[02:27:37.720 --> 02:27:39.320]   If you do subscribe, leave us a review.
[02:27:39.320 --> 02:27:41.560]   A five-star review would very much help.
[02:27:41.560 --> 02:27:44.560]   We'd really appreciate that.
[02:27:44.560 --> 02:27:47.640]   And of course, there are wonderful ways
[02:27:47.640 --> 02:27:50.800]   to interact with us that are still absolutely free,
[02:27:50.800 --> 02:27:53.960]   and supported in effect by the wonderful advertisers
[02:27:53.960 --> 02:27:55.120]   who support the network.
[02:27:55.120 --> 02:27:58.600]   That's our IRC at IRC.twit.tv in real time.
[02:27:58.600 --> 02:28:01.200]   We have a forum at twit.community.
[02:28:01.200 --> 02:28:03.200]   That's a great interactive space.
[02:28:03.200 --> 02:28:04.760]   And of course, a Mastodon instance,
[02:28:04.760 --> 02:28:08.480]   if you want to join the Fediverse at twit.social.
[02:28:08.480 --> 02:28:11.320]   Thanks for being here.
[02:28:11.320 --> 02:28:13.520]   We'll see you next time on Twig.
[02:28:13.520 --> 02:28:13.920]   Bye-bye.
[02:28:13.920 --> 02:28:20.000]   You know what's fun, Android.
[02:28:20.000 --> 02:28:21.520]   You know what's even more fun, though?
[02:28:21.520 --> 02:28:22.440]   All about Android.
[02:28:22.440 --> 02:28:25.000]   That's my show, Jason Howell, along with my co-hosts
[02:28:25.000 --> 02:28:26.800]   Ron Richards, Florence Ion.
[02:28:26.800 --> 02:28:29.080]   And we welcome guests on each and every week
[02:28:29.080 --> 02:28:32.720]   from throughout the Android ecosystem, developers,
[02:28:32.720 --> 02:28:36.040]   Googlers, journalists, people who are all geeked out
[02:28:36.040 --> 02:28:37.960]   about the Android operating system.
[02:28:37.960 --> 02:28:41.840]   We tell you everything you need to know, twit.tv/AAA
[02:28:41.840 --> 02:28:42.720]   every Tuesday.
[02:28:42.720 --> 02:28:43.800]   We'll see you there.
[02:28:43.800 --> 02:28:47.040]   [MUSIC PLAYING]
[02:28:47.040 --> 02:28:50.400]   [MUSIC PLAYING]
[02:28:50.400 --> 02:28:52.980]   (upbeat music)
[02:28:52.980 --> 02:28:55.980]   (suspenseful music)

